question_id,title,body,tags
3798971,Is there an algorithm to find the edge vectors of a polytope? [duplicate],"This question already has an answer here : How to algorithmically find only the edges of a high dimensional convex hull? (1 answer) Closed 3 years ago . Question: Given some points $x^1,\dots,x^m \in \mathbb R^n$ , is there an algorithm that finds the vectors along the edges of the polytope $$P = \mathrm{conv}(x^1,\dots,x^n)$$ formed by the convex hull of these points? Definitions: A polytope is the convex hull of finitely many points in $\mathbb R^n$ . Let $P \subset \mathbb R^n$ be a polytope. A face of $P$ is a a subset $F \subset P$ of the form $$F = \arg \max\{c^Tx : x \in P\}$$ for some $c \in \mathbb R^n$ . The dimension of a face of $P$ is the dimension of its affine hull. A vertex is a face of dimension zero and an edge is a face of dimension one. If $E$ is an edge of $P$ then $E = \mathrm{conv}(x, y)$ for some vertices $x$ , $y$ . An edge vector of a polytope $P$ is a vector $v \in \mathbb R^n$ such that $v=x-y$ for some vertices $x$ and $y$ for which $\mathrm{conv}(x,y)$ is an edge. Example: Let $x^1=(0,0), x^2=(1,0), x^3=(0,1), x^4=(1,1)$ . The convex hull of these points is the square $$P=\mathrm{conv}(x^1, x^2, x^3, x^4)$$ whose edge vectors are given by $$\{(1,0), (0,1), (-1, 0), (0, -1)\}.$$ Note that $(1,1)=x^4-x^1$ is not an edge vector although it is the difference of two vertices.","['polytopes', 'geometry', 'linear-programming', 'linear-algebra', 'discrete-mathematics']"
3798977,"Volume of the solid where it is enclosed from $2x + y + z = 4$ and the planes $x = 0$, $y = 0$, $z = 0$","Calculate the volume of the solid where it is enclosed from $2x + y + z = 4$ and the planes $x = 0$ , $y = 0$ , $z = 0$ . My approach: For $z = 0$ : \begin{equation*}
2x + y = 4 \implies y = -2x + 4
\end{equation*} and for $y = 0$ : \begin{equation*}
2x = 4 \implies x = 2
\end{equation*} So the integral for the volume are: \begin{equation*}
  \int_0^2 \int_0^{-2x +4} \left(4 - 2x - y\right)\,dy \,dx
      = \frac{16}{3}
\end{equation*} Is my approach correct? If not, can you provide the correct answer?","['integration', 'multivariable-calculus', 'solution-verification', 'volume']"
3799026,Does there exist an absolutely continuous probability measure on every measure space?,"Let $(\Omega,\mathcal F,\mu)$ be an arbitrary measure space, where $\mu$ is non-zero but does not need to be $\sigma$ -finite or semi-finite. Does there necessarily exist a probability measure $P$ on $(\Omega,\mathcal F)$ such that $P$ is absolutely continuous with respect to $\mu$ ?","['measure-theory', 'absolute-continuity', 'probability-theory', 'radon-nikodym']"
3799048,Outer Measure of Cartesian Product with Interval,"(Apologises in advance if this has already been asked, but I looked around and couldn’t find anything that answered my question). Let $\lambda_m^*$ denote the Lebesgue outer measure on $\mathbb{R}^m$ , and $[a,b]$ be an interval of $\mathbb{R}$ . If $A$ is a (not necessarily Lebesgue measurable) subset of $\mathbb{R}^n$ , is it possible to say that: $\lambda_{n+1}^*(A \times [a,b]) = \lambda_n^*(A) (b - a)$ ? It is pretty straight forward to see that the left hand side is less than or equal to the right hand side (that’s true for arbitrary Cartesian products), and that equality holds if $A$ is Lebesgue measurable. But what about the general case? I’m not sure what the best way to find either a proof or a counterexample is, so some help would be much appreciated.","['measure-theory', 'lebesgue-measure', 'outer-measure', 'products']"
3799051,Limit Behavior of Differential Equation,"I have a differential equation of the form $$r(t)f(t,x) + \frac{\partial}{\partial t}f(t,x) = g(t,x)$$ where $r(t),g(t,x)$ are given and I want to solve for $f(\cdot,\cdot)$ . I also know that there exist a constant $r$ and a function $g(x)$ such that $\lim_{t \to \infty} r(t) = r^{\ast}$ and $\lim_{t \to \infty} g(t,x) = g^{\ast}(x)$ uniformly over $x$ . In the limit as $t \to \infty$ , the differential equation would become $r^{\ast} f(x) = g^{\ast}(x)$ , so that $f(x) = g^{\ast}(x)/r^{\ast}$ . If $f(t,x)$ is a solution to the original differential equation, under what conditions do we have that $$\lim_{t \to \infty} f(t,x) = \frac{g^{\ast}(x)}{r^{\ast}}$$","['functional-analysis', 'ordinary-differential-equations']"
3799122,What can be said of Neumann eigenfunctions with non-vanishing derivative?,"There is a theorem which states that if $f$ is an eigenfunction of the Dirichlet problem of the Laplacian: $$-\Delta f =\lambda f$$ $$f|_{\partial \Omega }=0$$ Then if $f$ has a single nodal domain (meaning, the set { $x\in \Omega|f(x)\neq 0$ } has a single connected component), then the corresponding eigenvalue $\lambda$ is the first eigenvalue of the Laplacian, and it is a simple eigenvalue. I was wondering if there is a similar result for the case where instead of the Dirichlet problem we're looking at the Neumann problem (where the normal derivative of $f$ vanishes at the boundary), and we replace the assumption that $f$ has a single nodal domain with the assumption that the derivative of $f$ does not vanish in $\Omega$ (only at the boundary). Can I say anything 'interesting' about the eigenfunction/eigenvalue in this case? The proof I know of the theorem above (which uses orthogonality of the eigenfunctions) cannot be applied for this case since $f$ does not have a constant sign in $\Omega$ . I know that in the Neumann case, the first eigenvalue must be $0$ , so unless $f$ is constant (and we assume it's not, since it's derivative does not vanish) then the eigenvalue $\lambda$ will definitely not be the first eigenvalue. But maybe it has to be the second? Or maybe it still needs to be simple or something? If anyone knows of a relevant result (or an interesting counter example) - please let me know. I'm looking for anything which can be said about $f$ or $\lambda$ based on only the given assumptions. I'm interested mainly in results for manifolds and metric graphs, but feel free to share anything related. Thanks a lot in advance.","['operator-theory', 'laplacian', 'functional-analysis', 'partial-differential-equations', 'boundary-value-problem']"
3799132,Prove that $\tan(x)\tan(x+\frac{\pi}{3})+\tan(x)\tan(\frac{\pi}{3}-x)+\tan(x+\frac{\pi}{3})\tan(x-\frac{\pi}{3}) = -3$,"Let's assume that $\tan(x) = y$ . So, $\tan\Big(x+\dfrac{\pi}{3}\Big) = \dfrac{\tan(x) + \tan\Big(\dfrac{\pi}{3}\Big)}{1-\tan(x)\tan\Big(\dfrac{\pi}{3}\Big)} = \dfrac{y+\sqrt{3}}{1-\sqrt{3}y}$ Similarly, $\tan\Big(x-\dfrac{\pi}{3}\Big) = \dfrac{y-\sqrt{3}}{1+\sqrt{3}y}$ Also, $\tan\Big(\dfrac{\pi}{3}-x\Big) = -\tan\Big(x-\dfrac{\pi}{3}\Big) = \dfrac{\sqrt{3}-y}{1+\sqrt{3}y}$ Now, $\tan(x)\tan\Big(x+\dfrac{\pi}{3}\Big)+\tan(x)\tan\Big(\dfrac{\pi}{3}-x\Big)+\tan\Big(x+\dfrac{\pi}{3}\Big)\tan\Big(x-\dfrac{\pi}{3}\Big)$ $$ = y\Big(\dfrac{y+\sqrt{3}}{1-\sqrt{3}y}\Big)+y\Big(\dfrac{\sqrt{3}-y}{1+\sqrt{3}y}\Big)+\Big(\dfrac{y+\sqrt{3}}{1-\sqrt{3}y}\Big)\Big(\dfrac{y-\sqrt{3}}{1+\sqrt{3}y}\Big)$$ $$ = y\Big(\dfrac{(y+\sqrt{3})(1+\sqrt{3}y)+(1-\sqrt{3}y)(\sqrt{3}-y)}{1-3y^2}\Big)+\Big(\dfrac{y^2-3}{1-3y^2}\Big)$$ $$ = y\Big(\dfrac{y+\sqrt{3}y^2+\sqrt{3}+3y+\sqrt{3}-y-3y+\sqrt{3}y^2}{1-3y^2}\Big)+\Big(\dfrac{y^2-3}{1-3y^2}\Big)$$ $$ = \dfrac{2\sqrt{3}y+2\sqrt{3}y^3+y^2-3}{1-3y^2}$$ This is how much I've been able to simplify the expression but I'm unable to continue. I am familiar with the values of trigonometric functions at multiples and sub multiples of angles and I think the solution would involve their use (as the question has been taken from that very chapter). Thanks!",['trigonometry']
3799151,"Determine all functions $f:\mathbb{R}\to\mathbb{R}$ such that $\alpha\,f(yz)+\beta\,f(zx)+\gamma\,f(xy)\geq f(x+y+z)$ for all $x,y,z\in\mathbb{R}$.","Let $\alpha,\beta,\gamma$ be three real numbers.  Determine all functions $f:\mathbb{R}\to\mathbb{R}$ such that $$\alpha\,f(yz)+\beta\,f(zx)+\gamma\,f(xy)\geq f(x+y+z)$$ for all $x,y,z\in\mathbb{R}$ . Remarks. If $\alpha=\beta=\gamma=0$ , then any function $f:\mathbb{R}\to\mathbb{R}_{\leq0}$ is a solution. If $\alpha+\beta+\gamma=1$ , then any constant function $f$ satisfies the functional inequality. If $\alpha+\beta+\gamma>1$ , then any nonnegative constant function $f$ satisfies the functional inequality. If $\alpha+\beta+\gamma<1$ , then any nonpositive constant function $f$ satisfies the functional inequality. Special Case. (The idea was borrowed from this answer , which is a particular case with $\alpha=\beta=\gamma=\dfrac13$ .) Suppose that $\alpha+\beta+\gamma=1$ . Without loss of generality, $\alpha\leq \beta\leq \gamma$ . Plugging in $y:=0$ and $z:=0$ , we get $$f(x)\leq f(0)$$ for all $x\in\mathbb{R}$ .  Note that $\gamma>0$ .  Plugging in $y:=-x$ and $z:=0$ , we have $$f(0)\geq f(-x^2)\geq f(0)$$ for all $x\in\mathbb{R}$ .  This means $f(x)=f(0)$ for all $x\leq 0$ .  Now, by letting $y:=x$ and $z:=-2x$ , we have $$f(0)\geq f(+x^2)\geq f(0)$$ for all $x\in\mathbb{R}$ .  This means $f(x)=f(0)$ for all $x\geq 0$ .  Therefore, $f(x)=f(0)$ for every $x\in\mathbb{R}$ .  Hence, if $\alpha,\beta,\gamma\in\mathbb{R}$ are such that $\alpha+\beta+\gamma=1$ , then the only solutions $f$ are constant functions. What are solutions for other parameters $(\alpha,\beta,\gamma)$ ?  Can we hope to find all solutions $f$ for an arbitrary triple $(\alpha,\beta,\gamma)$ ?  If this is too difficult, can we at least hope for a characterization of all solutions $f$ if $\alpha=\beta=\gamma$ ? Postscript. If we have an equality instead, that is, $$\alpha\,f(yz)+\beta\,f(zx)+\gamma\,f(xy)=f(x+y+z)$$ for all $x,y,z\in\mathbb{R}$ , then by plugging $y,z:=0$ , we get $f(x)=(\alpha+\beta+\gamma)\,f(0)$ .  In particular, when $x:=0$ , we have $$(\alpha+\beta+\gamma-1)\,f(0)=0\,.$$ Thus, $\alpha+\beta+\gamma=1$ or $f(0)=0$ . If $\alpha+\beta+\gamma=1$ , then using the same argument as the proof of the ""Special Case"" above, we can see that $f(x)=f(0)$ for all $x\in\mathbb{R}$ .  That is, $f$ is a constant function. If $\alpha+\beta+\gamma\neq 1$ , then $f(0)=0$ .  Using the same argument as the proof of the ""Special Case"" above, we can see that $f(x)=0$ for all $x\in\mathbb{R}$ .  That is, $f$ is the zero function.","['contest-math', 'functional-equations', 'functions', 'functional-inequalities']"
3799251,Example of delta method for iid random variables,"Suppose $X_i \stackrel{\text{iid}}{\sim} N(\mu, \sigma^2)$ .  I want to analyze the limiting distribution of $\frac{1}{\bar{X}}$ in a few cases: (1) If $\mu \ne 0$ , then we purely apply the delta method to the function $g(x) = 1/x$ to state that $$\sqrt{n}\left(\frac{1}{\bar{X}} - \frac{1}{\mu}\right) \rightarrow N(0, \sigma^2/\mu^4) $$ in distribution. My question is what happens when $\mu = 0$ ?  Specifically what is is the limiting distribution of $\frac{\sqrt{n}}{\bar{X}}$ ?  I'm a TA for undergraduate probability and was asked this by a student and gave them the following answer, though I said i wasn't sure. I think there is no limiting distribution, but I'm guessing based on the following:  By the strong law of large numbers, $\bar{X} \rightarrow 0$ almost surely.  Therefore, $$\frac{\sqrt{n}}{|\bar{X}|} \rightarrow \infty \quad \text{almost surely} $$ so there is no limiting distribution (at least for real valued limits). Is there a limiting distribution if we assume our limit can take values in the extended real line?  Define $Y_n :=  \sqrt{n}/\bar{X}$ .  Since $-\bar{X}$ is equal in distribution to $\bar{X}$ , we have $\lim_n Y_n \in \{-\infty, \infty\}$ if the limit exists (I wouldn't know how to show this), so that $P(\lim_n Y_n = \infty) = P(\lim_n Y_n = -\infty) = 1/2$ assuming that the limit exists almost surely.  I don't think this is correct though.  Am I right?","['probability-theory', 'weak-convergence']"
3799281,Uniform bound for integral in terms of $\left\lVert f' \right\rVert_4^4$,"Show that there is a constant $C>0$ such that for any compactly supported $C^1$ function $f: \mathbb{R} \to \mathbb{R}$ , we have $$\int_{\mathbb{R}} \left(\frac{f(x)-f(y)}{x-y}\right)^4dy \le C \left\lVert f' \right\rVert_4^4\qquad\text{for all }x \in \mathbb{R}.$$ This is an old quals problem that I don't know how to do. One hint is that I may use integration by parts, but I don't know how to apply the hint either. Any approach would be much appreciated.","['integration', 'lp-spaces', 'derivatives', 'real-analysis']"
3799333,I am looking for a book focused on Euclidean Geometry,"So I look for a book that has geometric approaches to (quote unquote) everything there is to know about Euclidian geometry. I am not a bookworm so I am basically wondering blindly in a dark room but the books I checked thus far did not satisfy me because they seem either to just be partially euclidean partially everything else (analytical/calculus/algebra/ other types of geometries etc) Basically I would like a book like the elements but better written/written as a textbook (because the books I found on Elements are more focused on translating the ancient script rather than mentioning the concepts in a modern and easy to understand way and with an order that makes sense in nowadays geometric applications they also luck examples/conclusions/ exercises ) After reading and understanding this book I would like to be able to know everything there is about e.g how to measure lines using circles and tangents in order to construct a shape or what not how to compare circles how to construct vertices using circles how to cut shapes or distances in equal parts etc etc etc Actually what I described above is somehow misleading but it's hard for me to articulate what I am looking for....  think of Euclidea the game, I want to be able to solve everything there (<-- this is not my target but I think it gives a good idea of what I am looking for) The purpose of wanting this is that in many math problems I encounter there is an intuitive geometrical approach that I miss because my geometry knowledge is limited but my teacher or fellow students point out. I thank you in advance for your time. P.S No children's books with basic stuff, I need it to be as comprehensive as it can get.","['book-recommendation', 'geometry']"
3799335,A curve in $\Bbb R^2$ with non zero curvature is characterized by it's curvature. Is there a contradiction through this example?,"A curve in $\Bbb R^2$ with non zero curvature is characterized by it's curvature i.e.  Let $f: \Bbb R \rightarrow \Bbb R^2 ~\& ~g:\Bbb R \rightarrow \Bbb R^2$ be two $2$ times differentiable path-length parametrizations of curves $C_f$ and $C_g $ in $\Bbb R^3$ . If $k_f(s)=k_g(s) \ne 0 ~\forall ~s \in \Bbb R,$ then the two curves are identical except for probably their position in $\Bbb R^2$ Let us consider an example. Let $C_f$ be the curve parametrized by the function $f :\Bbb R \rightarrow \Bbb R^2~|~f(t)=(t,t^3) $ and $C_g$ be the curve parametrized by the function $g: \Bbb R \rightarrow \Bbb R^2~|~g(t)=(t,|t^3|).$ Then, I was able to compute that the curvatures $k_{C_f}(f(t))=k_{C_g}((g(t))=\dfrac {6t}{\sqrt {1+9t^4}}, t \in \Bbb R$ . But, if we sketch the images $\{(t,t^3): t \in \Bbb R \}$ and $\{(t,|t^3|): t \in \Bbb R \}$ , then they are not identical. Why does this seem to contradict the above theorem in bold?","['curves', 'multivariable-calculus', 'parametrization', 'differential-geometry']"
3799343,Find $\sum_{r=1}^{20} (-1)^r\frac{r^2+r+1}{r!}$.,"Calculate $$\sum_{r=1}^{20} (-1)^r\frac{r^2+r+1}{r!}\,.$$ I broke the sum into partial fractions and after writing 3-4 terms of the sequence I could see that it cancels but I wasn't able to arrive at the exact expression. I understand that it's trivial but for whatever reason I just couldn't get it so just need a little help.","['summation', 'factorial', 'telescopic-series', 'sequences-and-series', 'algebra-precalculus']"
3799358,The natural map from the tensor algebra to the symmetric algebra has a section?,"If $V$ is a finite-dimensional vector space over a field of characteristic zero, and $\pi$ is the projection from $T^n(V)$ (the $n$ -fold tensor product) to $S^n(V)$ the $n$ -fold symmetric product, I'm asked to show that $\pi$ has a section. My initial thought was that since $T^n(V)$ is a finite-dimensional vector space, and the projection $\pi$ is a surjective linear transformation, we have that there exists a right inverse of $\pi$ (i.e. a section). This seemed too simple to be true and didn't require any knowledge about tensor algebras. Am I missing something?","['linear-algebra', 'tensor-products', 'commutative-algebra']"
3799365,Found this identity by accident,"I found the following identity when trying to prove something else: $$\sum\limits_{n=1}^\infty\frac{(n-1)!x}{\prod\limits_{k=1}^n (k+x)}\equiv 1,~ \forall x>0$$ I'm sure there's a name for it (or for a more general identity for which this is a special case). Does anyone know the name?","['ramanujan-summation', 'sequences-and-series']"
3799373,Why don't these ODEs produce the same result?,"I am relatively new to differential equations, and the following problem is confusing me. Consider, for example, the ODE $x'+x=0$ such that $x(0)=1$ . This has solution $x(t)=e^{-t}$ . But consider an $\epsilon>0$ and the ODE. $$\epsilon x'' + x' + x = 0$$ subject to $x(0)=1$ . In the limit as $\epsilon\to 0$ the above equation becomes $x'+x=0$ . But no matter how small we make epsilon, the solution will never look like $e^{-t}$ , and in fact, is unique up to a constant. While mathematically I think I can see what is going on - the first equation only has one linearly independent solution, while the second has two - physically I cannot understand how this can be.","['stability-in-odes', 'boundary-layer', 'ordinary-differential-equations']"
3799396,Making $\sin(2k\pi x)$ an odd function about $x=\frac{1}{4}$,"I want to ask whether k should be even or odd   in order to make the function $$f(x)=\sin(2k\pi x)$$ to be an odd function about $x=\frac{1}{4}$ . I think that if it is an odd function, then, $$f(x+\frac{1}{4})=-f(x-\frac{1}{4})$$ .
I sub it in; but it turns out I get to: $$\sin(2k\pi x)\cos(\frac{1}{2}k\pi)x=0.$$ Then, I don’t know how to proceed. Thank you so much.","['even-and-odd-functions', 'algebra-precalculus', 'trigonometry']"
3799401,Proving that $ \sum_{k=0}^\infty\frac1{2k+1}{2k \choose k}^{-1}=\frac {2\pi}{3\sqrt{3}} $,Mathematica gives away the interesting sum: $$\sum_{k=0}^\infty\frac1{2k+1}{2k \choose k}^{-1}=\frac{2\pi}{3\sqrt{3}}$$ The question is: How to prove it by hand? Remark. This question is self-answered (whence the OP provided an effort).  See the answer below .,"['summation', 'calculus', 'binomial-coefficients', 'sequences-and-series', 'binomial-theorem']"
3799411,"Prove that the line, containing the segments with lengths the max and min distances from the origin to a circle, contains the center of the circle","( https://www.desmos.com/calculator/nwdvygfw1r for reference) I have a problem trying to prove what my intuition is telling me. I was trying to find the maximum and minimum distances from the origin of the 2D plane to a circumference, and I thought that maybe the distance's extrema formed line segments that when lied up together, were contained in the line formed with the origin and the circumference's center. My strategy then was to find the equation of the line passing through the origin and the circumference center, find the points at which it cut the circumference, and thus calculate the distances from those points to the origin, taking the larger one as the maximum distance and vice versa. I was stumped when I asked myself why exactly my thoughts led me to blindly believe that those two distances should be contained in that line. When I tried to prove it, I thought of brute-forcing it, by calculating the distance formula ( d (O,P), with P being a point on C ), differentiating it, and then setting it equal to zero, but ended up with a hot mess. My attempt for a circumference with the origin inside it proved successful, but I was unable to extend the rationale of the triangle inequality to the case where the origin is external to the circumference, much less when it lies on the circumference. Any help is appreciated! This is my first time posting here and I'm hoping I can get somewhere with this problem. (in Desmos link: how can I prove that if OA and OB are the min and max distances (A,B points on the ciruference), from O to the circumference respectively, then line AB contains C?)","['analytic-geometry', 'euclidean-geometry', 'calculus', 'algebra-precalculus']"
3799432,Books for vector analysis,"In the beginning of Griffiths electrodynamics there is a section for Vector analysis. All that was taught in very brief and I would like to read it in detail from a mathematical perspective rather than a Physics. I want a book on VECTOR ANALYSIS which has all topics such as Dirac delta function vector fields theory, Spherical coordinates, curl,divergence, gradient, potentials etc and basically has all topics. I have a good understanding on single variable calculus. Please help","['multivariable-calculus', 'book-recommendation', 'vector-analysis']"
3799451,Prove: A basis $G$ is a Grobner basis of an ideal $\iff$ for every element $S$ in a homogeneous basis for the syzygies $S(G)$ we have $S.G \to_{G}0. $,"A basis $G=(g_1,...,g_t)$ for an ideal I is a Grobner basis $\iff$ for every element $S=(h_1,...h_t)$ in a homogeneous basis for the syzygies $S(G)$ we have $S.G = \Sigma_{i=1}^{t} h_ig_i \to_{G}0. $ This is theorem 9 in chapter 2.9 of Ideals, Varieties and Algorithms by Cox, Little and O'Shea. The proof given there is not clear to me. Definitions in the text: Let $F=(f_1,...,f_s)$ . A syzygy on the on the leading terms $LT(f_1),...,LT(f_s)$ of $F$ is an s-tuple of polynomials $S=(h_1,...,h_s)\in (k[x_1,...,k_n])^s$ such that $\Sigma_{i-1}^{s} h_iLT(f_i)=0$ . $S(F)$ is a collection of all syzygies on $F$ . An element $S \in S(F)$ is homogeneous of multidegree $\alpha$ where $\alpha \in Z^{n}_{\geq0}$ provided that $S=(c_1x^{\alpha_1},...,c_sx^{\alpha_s})$ where $c_i\in k$ and $\alpha_i + \ multideg(f_i) = \alpha$ whenever $c_i \neq 0$ We say that $f \in [x_1,..,x_n]$ reduces to zero modulo $G = \{g_1,...,g_t\} \subset k[x_1,...,x_n]$ i.e. $f \to_{G} 0$ if $f$ can be written in the form $f = a_1g_1+...+a_tg_t$ such that whenever $a_ig_i \neq 0$ , we have multideg(f) $\geq$ multideg( $a_ig_i$ ). My Attempt: ( $\implies$ ) Given $G$ to be a Grobner basis and $S=(h_1,...,h_t)$ a syzygy of $G$ that is homogeneous we know that $\Sigma_{i=1}^t h_iLT(g_i) = 0$ and that $LT(h_i).LT(g_i)$ have the same multidegree for all $i$ . To prove RHS we need to show that multideg( $\Sigma h_ig_i$ ) $\geq$ multideg( $h_ig_i$ ) for all $i$ which is equivalent to showing $\Sigma_{i=1}^t LT(h_i)LT(g_i) \neq 0 $ . But the above would imply that $\Sigma_{i=1}^t LT(h_i)LT(g_i)=0$ . What is wrong here ? Help also in proving the other direction.","['groebner-basis', 'multivariate-polynomial', 'algebraic-geometry', 'ideals', 'commutative-algebra']"
3799479,"Suppose that $G$ is a transitive permutation group of degree 12 and order $95,040 = 12\times 11\times10\times9\times8$. Prove that $G$ is simple.","I've question on the problem 8C.3 from Isaacs's Finite Group Theory: Suppose that $G$ is a transitive permutation group of degree 12 and
order $95,040 = 12\times 11\times10\times9\times8$ . Prove that G is
simple. The hint on the book says: Show that $G$ must be 2-transitive, and that a nonidentity proper normal
subgroup would have to be regular. Observe that a group of order 12
can never be a minimal normal subgroup of any group. I've shown that $G$ is 2-transitive. So the key question now is that how to show that a nonidentity proper normal subgroup of $G$ is regular. If $G$ has two distinct minimal normal subgroups, then I know that they're regular.(See 8B.3 from Isaacs's Finite Group Theory). So we may assume that $G$ has a unique minimal normal subgroup $N$ . Since $C_G(N)$ is semiregular, hence we may also assume that $N$ is non-abelian. But I have no idea now.",['group-theory']
3799492,How to solve this equation manually: $(x^2+100)^2=(x^3-100)^3$?,"Well, I was given a problem,
find $x$, if: $$(x^2+100)^2=(x^3-100)^3$$ I tried everything that I could, I even opened up the brackets which gave an ugly degree 9 equation, I also tried to plot the curves $y=\left(x^2+100\right)^2$ and $y=\left(x^3-100\right)^3$ and locate their point of intersection but it couldn't be done manually. So, in the end I was forced to use hit and trial after doing which I got the answer, is their any way to solve this algebraically??",['algebra-precalculus']
3799508,"Maximum of $f(x)=\frac{2x\sqrt{(x+1)}}{(9x^2+3)^{\frac{1}{4}}}+\frac{(1-2x)\sqrt{2-2x}}{(9(1-2x)^2+3)^{\frac{1}{4}}}$ on the interval $[0,1/2]$","I would like to find the maxima of the following function in one variable : $$f(x)=\frac{2x\sqrt{(x+1)}}{(9x^2+3)^{\frac{1}{4}}}+\frac{(1-2x)\sqrt{2-2x}}{(9(1-2x)^2+3)^{\frac{1}{4}}}$$ on the interval $[0,1/2]$ . I have already checked that it must occur at $1/3$ , but could someone give me a proper method of proving it (ideally without using computation engines ?). The problem here is obviously that the derivative of this function is not nice-looking at all and i do not wish to ""play"" with it.
Thanks in advance !","['maxima-minima', 'calculus', 'cauchy-schwarz-inequality', 'optimization', 'derivatives']"
3799519,Qualitative study of a second order Cauchy problem,"I need a check on the following exercise: Consider the following Cauchy problem: \begin{cases}
 y''(x)=y'(x)^2 - 2 \\
 y(0)=0 \\
 y'(0) = 1
 \end{cases} i) Show the solution is defined for all $x \in \mathbb{R}$ ii)  Compute $\lim_{x \rightarrow +\infty} y'(x)$ and $\lim_{x \rightarrow +\infty} y(x)$ My attempt: i) I recast everything to the first order, hence I define the vector function $$F(t,y,y')=[y'^2-2,y']^T$$ I would like to prove sublinearity in order to show that the solution is globally defined: $$||F(t,y,y')|| \leq h + k ||[y,y']||$$ Using the expression for $F$ : $$y'^4 - 3 y'^2 +4$$ but I don't know how to find a sublinearity condition here: I should bound the latter expression with $y^2 + y'^2$ So, I notice that the function $F=[F_1,F_2]$ is such that $\partial_y F_1 = \partial_y F_2 = 0$ and $\partial_{y'}F_1 = 1$ and $\partial_{y'}F_2 = 2y'$ . This means that $F$ il globally Lipschitz , so in principle, existence and uniqueness could be applied iteratively and define a solution for every $x \in \mathbb{R}$ . IS THERE A WAY TO SHOW IT WITH SUBLINEARITY? ii) Here I note that, after the reduction to the first order, I have (call $y'=z$ ) the ODE $$z' = z^2-2$$ with $z(0)=1$ . By existence and uniquess, and using the stationary solutions $y=\pm \sqrt{2}$ , I have that $z$ starts from $1$ and then it decreases. The limit must exists, since the solution is defined on the whole $\mathbb{R}$ and it's monotone. Then $$\lim_{x \rightarrow \infty} z(x)= \lim_{x \rightarrow \infty} y'(x)= -\sqrt{2}$$ To compute $$\lim_{x \rightarrow \infty} y(x)$$ I note that $y'(x)=z(x)$ , and if it would be finite , then $$\lim_{x \rightarrow \infty} y'(x) = 0$$ But this limit is precisely the one I have just copmuted, i.e. $-\sqrt{2}$ , therefore this limit must be $+\infty$ or $-\infty$ . Since $y'(x)=z(x)$ and $z(x)$ is monotonically decreasing, then this limit must be $-\infty$ . Is eveything okay?","['cauchy-problem', 'solution-verification', 'ordinary-differential-equations', 'real-analysis']"
3799552,Tennis game with two different conventions for serving: show that the probability of win does not depend on the convention adopted.,"You are playing a match against an opponent in which at each point either you or your opponent serves. If you serve you win the point with probability $p_1$ , but if your opponent serves you win the point with probability $p_2$ . There are two possible conventions for serving: (i) serves alternate; (ii) the player serving continues to serve until she loses a point. You serve first and the first player to reach n points wins the match. Show that your probability of winning the match does not depend on the serving convention adopted. I guess I must find a relation between the alternate way of serving and the other way, showing that the probability of win is the same without necessarily calculate it, for example dividing the combinations of serving in the second way in different sets with the same resulting probability equal to that of the first way. Edit: I finally solved the problem, for the solution see my answer.","['probability-theory', 'probability']"
3799601,Why are effective divisors on $\mathbb{P}^n$ positive?,"I'm reading Griffiths and Harris, Principles of algebraic geometry , and it's written that ""any effective nonzero divisor on $\mathbb{P}^n$ is positive"" on page 159. Is that obvious? There doesn't seem to be any explanation in the book. Here, a divisor is called positive if the associated line bundle has a hermitian metric whose curvature form is positive. I know that hyperplane divisors are positive since they induce the Fubini-Study metric, but I'm not sure how to deal with a general irreducible hypersurface.","['complex-geometry', 'divisors-algebraic-geometry', 'algebraic-geometry', 'differential-geometry']"
3799608,The wrong way of finding the average distance between two points on a circle,I was trying to find the average distance between two points on a circle and got the following result. Why is my method wrong?,"['analytic-geometry', 'circles', 'geometry', 'calculus', 'algebra-precalculus']"
3799623,"Computing the quotient of SL(2,Z) by its commutator subgroup","So far, I know the matrices $$S=\left( \begin{matrix} 0&-1\\ 1&0 \end{matrix} \right), \quad T=\left( \begin{matrix} 1&1\\ 0&1 \end{matrix} \right) $$ generate SL $(2,\mathbb{Z})$ . $S^2=(ST)^3=-I$ . I also know the matrices $$X=\left( \begin{matrix} 1&1\\ 1&2 \end{matrix} \right), \quad Y=\left( \begin{matrix} 2&1\\ 1&1 \end{matrix} \right) $$ with their inverses $X^{-1},Y^{-1}$ generate SL $(2,\mathbb{Z})'$ , the commutator subgroup. I'm interested in what the quotient $SL(2,\mathbb{Z})/SL(2,\mathbb{Z})'$ is. In general, how would I go about computing the quotient of two matrix groups? I'm trying to use GAP but I'm having a lot of trouble!","['matrices', 'group-theory', 'quotient-group', 'modular-group']"
3799672,Reduce quadratic form of $3$ variables to sum of 3 squares,"I am dealing with following quadratic form: $q:\mathbb{R}^3\rightarrow \mathbb{R}$ $$
q(x,y,z)=xy+yz+xz
$$ and I am trying to reduce it to sum of squares. I approach this by trying to eliminate one variable. Since it is all symmetric, let's say we eliminate $x$ . I proceed: $$
q=x(y+z)+yz=\bigg(x^2+x(y+z)+\bigg(\frac{y+z}{2}\bigg)^2
\bigg)-x^2-\bigg(\frac{y+z}{2}\bigg)^2+yz$$ which ... doesn't work, since there's stil all of $x,y,z$ . Simply, for any variable $a$ there is no two terms: $a^2$ and $ab$ (for some other variable $b$ ) so i am not able to simplify it to a square this way. How to approach this?","['algebra-precalculus', 'quadratics', 'linear-algebra', 'quadratic-forms']"
3799686,"In triangle ABC,angle A=90, AK is vertical to BC, IHGK and DEFK are squares inscribe in triangle ABK and triangle CAK. Proof that AH=AE.:","In triangle $ABC$ , $\angle{A} = 90$ , $AK$ is vertical to $BC$ , $IHGK$ and $DEFK$ are squares inscribe in $\Delta ABK$ and $\Delta CAK$ . Prove that $AH=AE$ . I’ve tried proving $\Delta{AHG}$ is identical to $\Delta{AEF}$ and proving $\angle{AHE} = 45$ . Any hint is appreciated. Thank you. Now I can finish the question here:) Suppose angle HAG=t, angle EAF=90-t AK=AG+GK=AG+GH=AH*(cos(t)+sin(t)) AK=AF+FK=AF+FE=AE*(cos(90-t)+sin(t))=AE*(sin(t)+cos(t)) So AH*(cos(t)+sin(t))=AE*(cos(t)+sin(t)) =>>AH=AE It is more beautiful than I thought ^^","['euclidean-geometry', 'geometry']"
3799697,Is there a triangulation of a closed surface with each vertex incident to $n\ge 7$ triangles?,"Fix some $n\ge 7$ . Is there a finite 2-dimensional simplicial complex with each vertex is incident to exactly $n$ triangles, and the complex forms a closed surface . If the surface is not closed , then uniform tilings of the hyperbolic plane do the trick. But I am specifically interested in the case of finite closed complexes.
So is this possible? Since I want a surface, I made some quick computations with the Euler characteristic $\chi$ of that surface.
I found that the complex must have $-6\chi/(n-6)$ vertices.
I came not much farther than that.","['surfaces', 'graph-theory', 'geometry', 'geometric-topology', 'simplicial-complex']"
3799708,Prove or disprove formally: $(\forall x F) \lor G \vDash \forall x (F \lor G)$,"I want to formally prove that this statement is either false or true. I think it is false, because $G$ could contain the variable $x$ unbound and for some interpretation $\mathcal A$ , $G$ could be true for some $x$ while $(\forall x G)$ is false and $(\forall x F)$ is false. So $(\forall x F) \lor G$ evaluates to true, while $\forall x (F \lor G)$ evaluates to false, so $(\forall x F) \lor G \nvDash \forall x (F \lor G)$ for the same interpretation $\mathcal A$ . However, I am in doubt if my reasoning is correct and I do not know how to prove this formally.","['predicate-logic', 'first-order-logic', 'proof-writing', 'logic', 'discrete-mathematics']"
3799725,Partial sums over rational functions,"I recently came across the result that $$\sum_{n=2}^\infty \frac{n^4-n^3+n+1}{n^6-1} = \frac{1}{2}$$ I am wondering how one could proof this, generally how one could evaluate a sum over rational functions. If I plug the sum into Wolfram Alpha it gives $$\frac{3k^4-k-2}{6k(k+1)(k^2+k+1)}$$ as the $k$ -th partial sum. Taking the limit as $n \to \infty$ , this would in fact proof the upper equality. Sadly, I could not wrap my head around how to get to Wolfram Alphas partial sum result. If anyone has an idea let me know. Any tips are appreciated.","['summation', 'sequences-and-series']"
3799760,"Question about the proof that $(D(f),\mathcal{O}_{\operatorname{Spec}A}|_{D(f)})\cong (\operatorname{Spec}A_f,\mathcal{O}_{\operatorname{Spec}A_f})$","I am trying to prove/understand why $(D(f),\mathcal{O}_{\operatorname{Spec}A}|_{D(f)})\cong (\operatorname{Spec}A_f,\mathcal{O}_{\operatorname{Spec}A_f})$ . This problem appears in Vakil's algebraic geometry notes as problem 4.3.B. I know that since $D(f)=\{P\in\operatorname{Spec}A\mid f\not\in P\}$ , we can identify $D(f)$ and $\operatorname{Spec}A_f$ . So let $\pi:D(f)\rightarrow \operatorname{Spec}A$ be the natural map. I'd now like to show that $\mathcal{O}_{\operatorname{Spec}A_f}\rightarrow \pi^*\mathcal{O}_{\operatorname{Spec}A}|_{D(f)}$ is an isomorphism of sheaves. The hint given is to notice that distinguished open sets of $\operatorname{Spec}A_f$ are already distinguished open sets in $\operatorname{Spec}A$ . If we consider $D(g/f^n)=\{P\in\operatorname{Spec}A_f\mid g/f^n\not\in P\}$ , then how can we think of this as a distinguished open set in $\operatorname{Spec}A$ ? It doesn't make sense to ask if $g/f^n$ is not in a prime ideal of $A$ . Is really saying that the corresponding prime ideal of $A$ doesn't contain $g$ ? Further, I know that $\mathcal{O}_{\operatorname{Spec}A_f}(D(g/1))$ is the localization of $A_f$ is the localization of $A_f$ at all elements that do not vanish outside of $V(g/1)$ . That is, the localization of $A_f$ at $\{a/f^n\in A_f\mid D(g/1)\subset D(g/f^n)\}$ . And how do we describe $\mathcal{O}_{\operatorname{Spec}A}|_{D(f)}(D(g))$ ? How can I go about finish this problem/seeing the isomorphism?","['algebraic-geometry', 'abstract-algebra']"
3799775,Generalization of two formulae and an alternative proof of Bretschneider's formula,"Below I present two seemingly unknown identities that I then use to provide an alternative proof of Bretschneider's formula. Made the necessary adjustments, the identities can also be used to provide alternative proofs of Brahmagupta's Formula as well as Heron's Formula (see for example https://geometriadominicana.blogspot.com/2020/07/killing-three-birds-with-one-stone.html ). For implications in a triangle see also https://geometriadominicana.blogspot.com/2020/06/another-proof-for-two-well-known.html . Here, $a$ , $b$ , $c$ , $d$ are the sides of a general convex quadrilateral, $s$ is the semiperimeter, and $\alpha$ and $\gamma$ are two opposite angles. Then $$\sin^2{\frac{\alpha}{2}}=\frac{(s-a)(s-d)-bc\cos^2{\frac{\gamma}{2}}}{ad}\quad and \quad \cos^2{\frac{\alpha}{2}}=\frac{(s-b)(s-c)-bc\sin^2{\frac{\gamma}{2}}}{ad}\tag{1}$$ Proof . By the Law of Cosines, $$a^2+d^2-2ad\cos{\alpha}=b^2+c^2-2bc\cos{\gamma}\tag{2}$$ Yielding $\cos{\alpha}=\frac{a^2+d^2-b^2-c^2+2bc\cos{\gamma}}{2ad}$ . Now, making use of the half angle formula for cosine, $$\begin{align*} \cos^2{\frac{\alpha}{2}}&=\frac{a^2+d^2+2ad-b^2-c^2+2bc\cos{\gamma}}{4ad}\tag{3}\\ &=\frac{a^2+d^2+2ad-b^2-c^2+2bc(1-2\sin^2{\frac{\gamma}{2}})}{4ad}\tag{4}\\&=\frac{(a+d)^2-(b-c)^2-4bc\sin^2{\frac{\gamma}{2}}}{4ad}\tag{5}\\&=\frac{(a+d+b-c)(a+d-b+c)-4bc\sin^2{\frac{\gamma}{2}}}{4ad}\tag{6}\\&=\frac{1}{ad}\left(\frac{a+b+c+d}{2}-c\right)\left(\frac{a+b+c+d}{2}-b\right)-\frac{bc\sin^2{\frac{\gamma}{2}}}{ad}\tag{7}\\&=\frac{(s-b)(s-c)-bc\sin^2{\frac{\gamma}{2}}}{ad}\tag{8}\end{align*}$$ $\square$ The other formula can be obtained similarly by replacing $\cos^2{\frac{\alpha}{2}}$ by $1 - \sin^2{\frac{\alpha}{2}}$ in $(3)$ . A proof of Bretschneider's formula The formulae in $(1)$ can be rewritten as follows $$ad\sin^2{\frac{\alpha}{2}}+bc\cos^2{\frac{\gamma}{2}}=(s-a)(s-d)\tag{9}$$ and $$bc\sin^2{\frac{\gamma}{2}}+ad\cos^2{\frac{\alpha}{2}}=(s-b)(s-c)\tag{10}$$ Multiplying $(9)$ and $(10)$ we get $$\begin{align*}\left(ad\sin^2{\frac{\alpha}{2}}+bc\cos^2{\frac{\gamma}{2}}\right)\left(bc\sin^2{\frac{\gamma}{2}}+ad\cos^2{\frac{\alpha}{2}}\right) &= (s-a)(s-b)(s-c)(s-d)\tag{11}\end{align*}$$ Expanding, factorizing, completing the squares and keeping in mind some well-known trigonometric identities, $$\begin{align*}abcd\cos^2\left({\frac{\alpha+\gamma}{2}}\right)+\left(ad\sin{\frac{\alpha}{2}}\cos{\frac{\alpha}{2}}+bc\sin{\frac{\gamma}{2}}\cos{\frac{\gamma}{2}}\right)^2 &=(s-a)(s-b)(s-c)(s-d)\tag{12}\\abcd\cos^2\left({\frac{\alpha+\gamma}{2}}\right)+\left(\frac{ad\sin{\alpha}}{2}+\frac{bc\sin{\gamma}}{2}\right)^2 &=(s-a)(s-b)(s-c)(s-d)\tag{13}
\end{align*}$$ Since the area of $ABCD$ can be expressed as the sum of the areas of $\triangle{ABD}$ and $\triangle{CBD}$ , which in turn can be written as $\frac{ad\sin{\alpha}}{2}+\frac{bc\sin{\gamma}}{2}$ , then we are done. $\square$ Are the identities (9) and (10) known? Is this proof of Bretschneider's formula known? Thanks in advance. Edited : I have added a concept map of identities (9) and (10) so you can see clearly what's going on here.","['quadrilateral', 'area', 'geometry', 'reference-request']"
3799777,Computing $\sum_{n=1}^\infty\frac{(-1)^nH_n^2}{2n+1}$ in an alternative way,The following equality $$\sum_{n=1}^\infty(-1)^{n-1}\frac{H_n^2}{2n+1}=\frac{3}{16}\pi^3-\frac34\ln^2(2)\pi-8\Im\left\{\text{Li}_3\left(\frac{1+i}{2}\right)\right\}$$ can be proved if we are allowed to use the generating function ( see Eq $(3)$ ) $$\sum_{n=1}^\infty x^nH_n^2=\frac{\ln^2(1-x)+\text{Li}_2(x)}{1-x}$$ But the problem-proposer mentioned that the sum to be calculated without using this generating function. I have no clue how to approach it with such restriction. any idea ? Thanks in advance.,"['integration', 'real-analysis', 'alternative-proof', 'harmonic-numbers', 'sequences-and-series']"
3799808,Quadratic in one,"Let $a+b+c=0$ . Does $1^2a+1b+c=0 \implies b^2 \geq 4ac \ $ ? Obviously we know the inequality holds in general, but does considering a ""quadratic in one"" imply this?",['algebra-precalculus']
3799866,"$f$ is analytic on $D$, prove that $f$ is a constant","I've been stuck on this for a while, it is a decade old qualifying exam problem from my university: Let $f$ be a analytic function in the open unit disk $D$ such that $|f(z)|\leq 1$ for all $z\in D$ . Let $g$ be the restriction of $f$ to the real interval $(0,1)$ and assume $\lim_{r\rightarrow 1}g(r)=1$ and $\lim_{r\rightarrow 1}g'(r)=0$ . Prove that $f$ is a constant. Can anyone please give me some hints? I've really tried everything now, but nothing seems to work. The condition given seems to be very localized and I'm not sure how to use them. Thanks in advance!",['complex-analysis']
3799881,"Let $f: [a, b]\rightarrow R$ be differentiable at each point of $[a, b ]$ and $f'(a)=f'(b)$, prove that there's a line passing to $a$ tangent to $f$","Let $f: [a, b]\rightarrow R$ be differentiable at each point of $[a, b ]$ , and suppose
that $f'(a) = f'(b)$ . Prove that there is at least one point $c$ in $(a,b)$ such that $$
f'(c) = \dfrac{f(c)-f(a)}{c-a}
$$ My attempt: define $h(x) = \dfrac{f(x)-f(a)}{x-a}$ on $(a,b]$ and $h(a) = f'(a)$ . Notice that $h$ is continuous on $[a,b]$ . Now $$h'(x) = \dfrac{f'(x)}{x-a}-\dfrac{f(x)-f(a)}{(x-a)^2}$$ Note that we define $h'$ on $(a,b]$ Our goal is to show that an extremum point of $h(x)$ lies in $(a,b)$ so we can claim $h'(c)=0$ for some $c\in (a,b)$ . Moving things around we see that $f'(x) = h'(x)(x-a)+h(x)$ on $(a,b]$ . We observe that if $h(x)$ is strictly increasing (or strictly decreasing), then $f'(x)$ is also strictly increasing (or strictly decreasing). Hence a contradiction to $f'(a)=f'(b)$ and so there's an extremum $c$ for $h(x)$ . Here, we obtain a contradiction because if we were to avoid a contradiction then $f'(a)>d>f'(d+\epsilon)$ (assuming $f'$ is increasing) for any $\epsilon>0$ . Applying an intermidate-value-theorem type lemma to $f'$ we contradict monotonicity. Hence, $f(a)<f(a+\epsilon)$ for any $\epsilon>0$ . Therefore, $h'(c)=0$ implies $$\dfrac{f(c)-f(a)}{c-a}=f'(c)$$ I am only looking for proof verifications. If my proof is wrong, please $\textbf{only respond with hints}$ .","['extreme-value-theorem', 'proof-writing', 'derivatives', 'solution-verification']"
3799916,IMO 1998 - Combinatorics,"I am trying this IMO Combinatorics problem $1998$ P2 that goes like this: In a competition, there are $m$ contestants and $n$ judges, where $n \geq 3$ is an odd integer. Each judge rates each contestant as either “pass” or “fail”. Suppose $k$ is a number such that, for any two judges, their ratings coincide for at most $k$ contestants. Prove that $$\frac{k}{m}\geq \frac{n-1}{2n}$$ I am completely puzzled on how to start, could you give me any hints?","['contest-math', 'combinatorics']"
3800006,Proving a sum of a strange series $ \sum_{i=1}^{n} 11i^{10}-55i^9+165i^8-330i^7+462i^6 -462i^5+330i^4-165i^3+55i^2-11i+1 = n^{11} $,"While messing around with zeta functions I encountered a strange sum: $$
\sum_{i=1}^{n} 11i^{10}-55i^9+165i^8-330i^7+462i^6 -462i^5+330i^4-165i^3+55i^2-11i+1 = n^{11}
$$ How should I approach proving that this equality is true for all $n,i \in \mathbb{N}$ ? I ran this for a few values on some engines and it does work for every number I tried.","['telescopic-series', 'harmonic-analysis', 'binomial-coefficients', 'sequences-and-series', 'binomial-theorem']"
3800063,Two versions of the spectral Theorem?,"I'm studying the spectral Theorem (for bounded self-adjoint operators) by myself and I'm following Nik Weaver's nice book. Let me first introduce some notations first. Notations: If $\mathcal{H}$ is a Hilbert space, $\mathcal{B}(\mathcal{H})$ is the (Banach space) of all bounded linear operators $A: \mathcal{H} \to \mathcal{H}$ . If $A \in \mathcal{B}(\mathcal{H})$ , $\mbox{sp}(A)$ is the spectrum of $A$ . Now, let $(X, \mathcal{F},\mu)$ be a $\sigma$ -finite measure space. A measurable Hilbert bundle over $X$ is a disjoint union: $$\mathcal{X} = \bigcup_{n\in \mathbb{N}}(X_{n}\times \mathcal{H}_{n}) $$ where $\{X_{n}\}_{n\in \mathbb{N}}$ is a measurable partition of $X$ and, for each $0 \le n \le \infty$ , $\mathcal{H}_{n}$ is a Hilbert space with dimension $n$ . Finally, $f: X \to \mathcal{H}$ is weakly-measurable if the function $x \mapsto \langle f(x),v\rangle$ is measurable for every $v \in \mathcal{H}$ . We denote $L^{2}(X;\mathcal{H})$ the set of all weakly measurable functions $f: X \to \mathcal{H}$ such that: $$||f|| := \int_{x}||f(x)||^{2}d\mu(x) < +\infty $$ modulo functions which are zero almost everywhere. This is a Hibert space with inner product: $$\langle f,g\rangle := \int_{x}\langle f(x),g(x)\rangle d\mu(x) $$ If $f \in L^{2}(X;\mathcal{H})$ , $M_{f}$ is the operator multiplication by $f$ . Also, $L^{2}(X;\mathcal{X}) := \bigoplus_{n\in \mathbb{N}}L^{2}(\mathcal{X}_{n};\mathcal{H}_{n})$ . Now, the statement of the spectral Theorem in this reference is as follows. Theorem: Let $\mathcal{B}(\mathcal{H})$ be self-adjoint. Then there exits a probability measure $\mu$ on $\mbox{sp}(A)$ , a measurable Hilbert bundle $\mathcal{X}$ over $\mbox{sp}(A)$ and an isometric isomorphism $U: L^{2}(\mbox{sp}(A);\mathcal{X}) \to \mathcal{H}$ such that $A = UM_{x}U^{-1}$ . However, I'm more interested in another version of this Theorem, which is stated in Dimock's book and goes like (with adapted notation) Theorem: Let $A \in \mathcal{B}(\mathcal{H})$ be self-adjoint. Then, there exists a measure space $(\mathcal{M},\mathcal{\Omega},\mu)$ , a bounded measurable function $\tau: \mathcal{M}\to \mathbb{R}$ and a unitary operator $U: \mathcal{H}\to L^{2}(\mathcal{M},\mu)$ such that $A = UM_{\tau}U^{-1}$ . Question: How can I obtain Dimock's version of the spectral Theorem from Weaver's version of it?","['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
3800076,Riemann Zeta function's analytic continuation relationship with simple closed formulas [duplicate],"This question already has answers here : Connection between Faulhauber's formula and Riemann zeta function (2 answers) Closed 3 years ago . Consider the famous output of the analytic continuation of the Riemann zeta function at negative values : $$
\zeta(-1) = 1+2+3+4... \hspace{3mm} =  \dfrac{-1}{12}
$$ Now consider the closed formula which Gauss discovered for the sum of the first $n$ integers( $i,n \in \mathbb{N}$ ): $$
\sum_{i=1}^{n}i = 1+2+3+4...+n =  \dfrac{n(n+1)}{2} = \dfrac{n^2}{2} + \dfrac{n}{2}
$$ We Find the roots of the closed form ( using my special analytic Theorem which is based on the principal : "" I decide the function is continuous now "") : $$
n_1 = -1
$$ $$
n_2 = 0
$$ Now we find the right to left definite integral between the two roots (by leaning on the same analytic method I described above): $$
\int_{0}^{-1} \dfrac{n^2}{2}+\dfrac{n}{2} dn  =  \dfrac{n^3}{6}+\dfrac{n^2}{4}\Biggr|_{0}^{-1} = 0 - \left(\dfrac{-1}{6}+\dfrac{1}{4}\right) = \dfrac{-1}{12}
$$ I can't prove it for all negative values of $\zeta$ , I have verified the same relationship is true for $\zeta(-3)$ and the closed form of $n^3$ which integrates into $\dfrac{1}{120}$ if we use my ""special analytic method"". Now on a more serious note, can anybody please explain this to me? When I look at the functional equation for the analytic continuation of the zeta function, it seems like an impenetrable fortress, yet it turns out it is possible to produce these results with simple (illogical) assumptions regarding continuity of discrete functions?","['complex-analysis', 'combinatorics', 'analytic-number-theory']"
3800082,"Finding out if a sequence of digits is a ""numpad path""","I noticed that several of my old credit cards have verification codes which have an interesting property which I'll call ""numpathable"". A numpad is a graph that looks like this: 7 - 8 - 9
|   |   |
4 - 5 - 6
|   |   |
1 - 2 - 3 A number $n$ is a numpad path (or numpath , if you like) if its digits can be produced by moving along the edges of this graph, with at most one edge in between consecutive digits in $n$ . For example, $4556$ is a numpad path, as are $1$ , $12$ , and $123$ and $12321$ . $987456321$ is the largest number that's a numpad path without repeating any digits. However $4553$ isn't a numpad path, because you must traverse more than one edge between $5$ and $3$ . $1234$ is also not a numpad path for the same reason. Similarly, any number containing $0$ isn't a numpad path since $0$ doesn't appear on this graph. Q: What is the probability $P(d)$ that a random $d$ -digit number ( $d > 0$ ) is a numpad path? (Clearly, all 1 digit-numbers except 0 are numpad paths, so $P(1) = 0.9$ .) Edit: I previously called these ""numpad tours"", but a commenter pointed out that tours typically touch every node in a graph, so I've renamed it to numpad paths .",['probability']
3800090,How do you find the following multivariable limit? [duplicate],"This question already has answers here : Multivariable limit proof: $\lim\limits_{(x,y)\rightarrow (0,0)}\frac{\left|x\right|^a\left|y\right|^b}{\left|x\right|^c + \left|y\right|^d} = 0$ (4 answers) Closed 3 years ago . How do you find the following limit: $$\lim_{x,y\to 0} \frac{|x||y|^4}{|x|^4+|y|^5}$$ I've tried to apply the squeeze theorem, but unlike a lot of other questions I encounter, the degree of $x$ in the numerator is too small for me to write the function as a constant times a function that goes to zero as $x,y \rightarrow 0$ .",['multivariable-calculus']
3800093,"Short proof that if a and b of a group G commute then an element of order lcm(o(a),o(b) exists in G [duplicate]","This question already has answers here : Order of elements is lcm-closed in abelian groups (6 answers) Closed last year . I've been looking for short proofs for a few days now, but could not find one. Today I was finally able to write my own proof and I'd be happy if someone could verify if it's true. General assumption: We are talking in a group with a multiplicative operation. First we use the following theorem(1) as a given: If $ab=ba$ and $gcd(o(a),o(b))=1$ => $o(ab)=o(a)o(b)$ Statement: If $ab=ba$ => $\exists{c} $ : o(c)=lcm(o(a),o(b)) Proof: Let:
d= $gcd(o(a),o(b))$ Then: $lcm(o(a),o(b))=\frac{o(a)o(b)}{d}$ $gcd(\frac{o(a)}{d},o(b))=1$ Lets now look at: $a^d$ => $o(a^d)=\frac{o(a)}{d}$ Finally we arrive at: $a^{d}b$ . Since $gcd(o(a^d),o(b))=gcd(\frac{o(a)}{d},o(b))=1$ => By theorem(1) $o(a^db)$ must be equal to $o(a^d)o(b)$ . $o(a^d)o(b)=\frac{o(a)}{d}o(b)=lcm(o(a),o(b))$ Conclusion: $o(a^{gcd(a,b)}b)=lcm(o(a),o(b))$ Long proofs I found: https://yutsumura.com/the-existence-of-an-element-in-an-abelian-group-of-order-the-least-common-multiple-of-two-elements/ Proofs I could not understand: Order of element equal to least common multiple","['group-theory', 'abstract-algebra']"
3800187,"Arrange 6 fruits from three identical watermelons, three identical bananas and three identical strawberries.","The Question Marcus stumbles upon 3 identical watermelons, 3 identical bananas and 3 identical
strawberries. Marcus wishes to create an arrangement in a line using precisely 6 of these fruits. How many ways can he do this? (BWW is different to WBW). My Attempt We can choose 0 watermelons, 1 watermelon... 3 watermelons, so 4 possible ways.
Similarly, 4 ways to choose a number of bananas.
Similarly, 4 ways to choose a number of strawberries. So 64 ways. But we must then remove combinations such as 0 watermelons, 0 bananas and 0 strawberries which have less than 6 fruits in the arrangement. So we subtract 25 (I counted). Answer = 39? Post-Note I'm not sure if I'm right, I'm not sure if I accounted for the permutations correctly, and I definitely don't think this is the best way. Help appreciated! Thanks!","['permutations', 'contest-math', 'combinations', 'combinatorics', 'problem-solving']"
3800201,"If $f:\mathbb R^2 \to \mathbb R$ continuous on straight lines and $f(\text{compact})= \text{compact}$, then $f$ continuous?","Let $f:\mathbb{R}^2$ $\rightarrow$ $\mathbb{R}$ be a map such that $f$ is continuous over all segments (namely, for all $a,b$ $\in$ $\mathbb{R}^2$ , $t$ $\mapsto$ $f(at+b)$ is continuous), and If $K$ $\subset$ $\mathbb{R}^2$ is compact, then its image $f(K)$ is compact. Then $f$ is continuous. I tried to prove this, but I could not do it. Can you give me a hint?","['general-topology', 'real-analysis']"
3800203,Contravariant functor taking every finite group to an isomorphic group,"Every finite abelian group $A$ is isomorphic to its dual group $A^*:=\operatorname{Hom}(A,\mathbb{C}^\times)$ . The isomorphism of $A$ with $A^*$ is non-canonical, and one way to make this precise is to say that the functor $A\mapsto A^*$ is contravariant, so this functor cannot be naturally isomorphic to the (covariant) identity functor. I wonder if there is an analogous construction that works for all finite groups. Specifically: Does there exist a contravariant functor $F$ from the category of finite groups to itself, such that $F(G)$ is isomorphic to $G$ for every group $G$ ? The imprecise question I have in mind is: for an arbitrary finite $G$ , can one construct a group $G'$ that is non-canonically isomorphic to $G$ ? The existence of a contravariant functor $F$ as above would be one precise way to answer the imprecise question.","['group-theory', 'abstract-algebra', 'category-theory']"
3800240,How to interpret the definition of injectivity,"I am reading Terence Tao's Analysis . In section 3.3, he introduces the definition of injectivity as: A function f is one-to-one
(or injective) if different elements map to different elements: $$x \neq x' \Longrightarrow f(x) \neq f(x') $$ Equivalently, a function is one-to-one if $$ f(x) = f(x') \Longrightarrow x = x'$$ The language is not hard to understand. When I was doing the Exercise 3.3.3, however, I found that it is not very rigorous and different interpretations for the definition result in different conclusions. For example, if we interpret it as (Suppose the domain is $X$ ) $$ \forall x \forall x'(x \in X \wedge x' \in X \wedge (x \neq x' \Longrightarrow f(x) \neq f(x')))$$ ,
then the empty function is not injective since $x \in \varnothing$ is always a false statement. On the other hand, if we interpret it as $$\forall x \forall x'((x \in X \wedge x' \in X \wedge x \neq x') \Longrightarrow (f(x) \neq f(x'))) $$ ,
or $$ \forall x \forall x'((x \in X \wedge x' \in X) \Longrightarrow( x \neq x' \Rightarrow (f(x) \neq f(x'))) $$ ,
then the empty function is always injective for $$(x \in \varnothing \wedge x' \in \varnothing \wedge x \neq x') \Longrightarrow (f(x) \neq f(x'))$$ and $$(x \in \varnothing \wedge x' \in \varnothing) \Longrightarrow( x \neq x' \Rightarrow (f(x) \neq f(x'))$$ are vacuously true. Which of the interpretations is right, or can there be different interpretations for a definition?","['elementary-set-theory', 'functions', 'logic']"
3800250,Verify my proof that e is irrational,"I just want some confirmation that my proof that $e$ is irrational is valid. Please offer any comments on what I could do better/formatting tips, and point out any logical errors, no matter how small. Thanks! We need to prove that $e$ is irrational. We proceed to prove by contradiction. Assume for the sake of contradiction that there exist integers $p$ , $q$ such that $p$ and $q$ are integers. We first prove that $e$ cannot be an integer by using the fact that $$e=\sum_{n=0}^{\infty} \frac{1}{n!}$$ We see that $$1+\frac{1}{1!}<1+\frac{1}{1!}+\frac{1}{2!}+\ldots<1+\sum_{n=0}^{\infty}\frac{1}{2^n}$$ since it can be shown that $n!>2^n$ for all $n>3$ . This means that $2<e<3$ and thus $e$ cannot be an integer. This means that $q\neq 1$ . Since $e=p/q$ , we can write $$\frac{p}{q}=\sum_{n=0}^{\infty}\frac{1}{n!}$$ If we multiply both sides by $q!$ , we notice that we get $$p(q-1)!=q!\sum_{n=0}^{\infty} \frac{1}{n!}=\sum_{n=0}^{q}\frac{q!}{n!}+\sum_{n=q+1}^{\infty}\frac{q!}{n!}$$ We notice that since $p$ and $q$ are integers, the left side must be an integer. Now, looking at the right side, we see that the first term is an integer, since every factor in the denominator cancels out with some factor in $q!$ , since $n\leq q$ in the first term. However, the right sum simplifies to $$\sum_{n=q+1}^{\infty}\frac{q!}{n!}=\frac{1}{q+1}+\frac{1}{(q+1)(q+2)}+\cdots<\sum_{k=1}^{\infty}\frac{1}{(q+1)^k}$$ We see that this is an infinite geometric series with sum $$\frac{\frac{1}{q+1}}{1-\frac{1}{q+1}}=\frac{1}{q}$$ and since $q\neq 1$ , we see that this sum must be less than $1$ , so it cannot be an integer. This means we have an integer and a non-integer summing to an integer, a contradiction. This means our assumption must have been wrong, and $e$ must be irrational, completing the proof.","['power-series', 'solution-verification', 'sequences-and-series']"
3800259,How do I determine sample size for a test?,"Say you have a die with $n$ number of sides.  Assume the die is weighted properly and each side has an equal chance of coming up. How do I determine the minimum number of rolls needed so that results show an equal distribution, within an expected margin of error? I assume there is a formula for this, but I am not a math person, so I don't know what to look for.  I have been searching online, but haven't found the right thing.","['statistics', 'probability']"
3800263,Directional derivative and gradient of a differentiable function,"Let $u = f(x,y,z)$ be a differentiable function in $\mathbb{R^3}$ Given the function satisfies: $f(x,y,x^2 + y^2) = 2x+y$ for all $x,y$ And the directional derivative of the point $(0,2,4)$ in the direction: $(-2,1,2)$ is equal to $-\frac{5}{3}$ . Calculate: $\nabla f(0,2,4)$ My try so far: I first normalized the vector: $\frac{(-2,1,2)}{||(-2,1,2)||} = (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3})$ we know that $f(0,2,4) = f(x,y,x^2 + y^2)$ because $x=0, y=2 , x^2+y^2 = 4$ and so: $f(0,2,4) = 2 \cdot 0 + 2 = 2$ . By the definition: $\frac{\partial f}{\partial (-2,1,2)}(0,2,4) = -\frac{5}{3} \Rightarrow \nabla f(0,2,4) \cdot (-\frac{2}{3}, \frac{1}{3}, \frac{2}{3}) = -\frac{5}{3}$ Even by the definition I am stuck: $\nabla f(0,2,4) = \lim_{h \rightarrow 0} \frac{f( (0,2,4) + h(-2,1,2)) - f(0,2,4)}{h} = \lim_{h \rightarrow 0} \frac{f(-2h,2+h, 4+2h) - 2}{h}$ Because the point $(-2h,2+h, 4+2h)$ does not satisfy that $x^2 + y^2 = z$ (coordinates) ... The problem is that I don't know how to find the gradient of that point, because the function is not given in its explicit form.. I would appreciate your help, thank you!","['multivariable-calculus', 'calculus', 'vector-analysis']"
3800294,Why this $G$ is non empty?,"Consider the following gradient system on a doubly connected planar domain $\Omega$ : $$\begin{cases}
\frac{dx}{dt}&= - u_x;\\ 
\frac{dy}{dt}&= - u_y,
\end{cases}$$ where $u$ is the first eigenfunction of Dirichlet laplacian. i.e. if $\Gamma_0,\Gamma_1$ are respectively inner and outer boundary of $\Omega$ , $u$ satisfies the following: $$\begin{cases}-\Delta u=\lambda_1 u & \text{ in }\Omega, \\
u=0 & \text{ on }\partial \Omega=\Gamma_0\cup\Gamma_1;\end{cases}$$ where $\lambda_1$ is the first eigenvalue. Define, $$G=\{x\in \Omega: \text{the solution curve of the gradient system starting from $x$ touches $\Gamma_0$} \}$$ Why $G$ is non-empty? $\textbf{Note:}$ It is well known that $u>0$ inside $\Omega$ and $\frac{\partial u}{\partial\eta}<0$ on $\Gamma_0$ .","['gradient-flows', 'ordinary-differential-equations', 'partial-differential-equations']"
3800327,Boolean operator in Queen problem,"In this paper , (page 28) I see the rule for there must be a queen in each row . My question is shouldn't it be: Xi1 & Xi2 &....XiN ...instead of: Xi1 or Xi2 or....XiN","['binary-operations', 'boolean-algebra', 'satisfiability', 'discrete-mathematics']"
3800330,Is there a different way of splitting numbers into digits?,"I was looking at a graph visualizing the Euler–Mascheroni constant ( $\gamma$ ), like that below, and an interesting question emerged. Background: The Euler-Mascheroni constant, to take the definition directly from the above-linked Wikipedia page, is the limiting difference between the harmonic series and the natural logarithm . Basically, the ""natural log of infinity"" (not quite so rigorous), or $\lim_{x \to \infty} \ln(x)$ , is infinite, and so is the harmonic series, or $\sum_{n=1}^{\infty} \frac{1}{n}$ . But if you subtract this infinite natural log from the harmonic series, you get a finite number around $0.57721$ , called the Euler-Mascheroni constant. Question: As the harmonic series is a step function , $\gamma$ is the sum of ""contributions"" from infinitely many sections, shown below as the first purple section covering $x \in [1, 2)$ , the second purple section covering $x \in [2, 3)$ , the third covering $x \in [3, 4)$ , etc. It occurred to me that this is fairly similar to the notion of a number being the sum of its digits, like the number 123 expressed as follows: It could be really useful to be able to express, operate on, and reason about a number with each ""digit"" representing a different term of a series, beyond the one canonical series in which we currently express all numbers: $$\textrm{number}=\textrm{digit}_1*\textrm{base}^{n-1}\ +\ \textrm{digit}_2*\textrm{base}^{n-2}\ +\ \textrm{digit}_3*\textrm{base}^{n-3}\ +\ ...\ +\ \textrm{digit}_n*\textrm{base}^0$$ TL;DR: Does there exist an area of study within mathematics that generalizes the notion of ""digits of a number"", allowing for them to be defined by something other than the series directly above, and with its own rules and operations for manipulating such a number? What are its rules and operations?","['number-theory', 'reference-request', 'real-analysis', 'sequences-and-series', 'soft-question']"
3800356,Showing for $g\in C^\infty$ that $g^{(n)}(0)=0$ given a vanishing property [duplicate],"This question already has an answer here : Show $g^{(n)}(0)=0$ for all $n$ (1 answer) Closed 3 years ago . Given an infinitely differentiable function $g: \mathbb{R} \rightarrow \mathbb{R}$ with the property that for every index $n$ there are positive numbers $c_{n}$ and $\delta_{n}$ such that $$|g(x)| \leq c_{n}|x|^{n} \quad      \text{if}     \quad |x|< \delta_{n}$$ Show that for each natural number $n,g^{(n)}(0)=0$ My attempt: By taking $x=\frac{1}{k}$ itself, we obtain $$\left|\frac{g(\frac{1}{k})}{\frac{1}{k}}\right| \leq  \frac{c_n}{k^{n-1}}$$ whenever $1/k < \delta_n,~n \geq2$ . Now
Let $k \rightarrow \infty $ to get $g'(0)=0$ . But what about higher derivatives of $g$ at $0$ ?","['calculus', 'derivatives', 'real-analysis']"
3800358,Finding if these function is bijective within given domain/codomain,"Wanting to know my reasons to the questions below are justified for it to be bijective/not Please correct me if I am wrong as I am trying to learn bijectivty and solving problems $f(x)=x^4+2x^2+1$ with $f: [0,\infty) \to [0,\infty)$ We know that for a function to be bijective it must satisfy both injectivity (one to one) and subjectivity (onto) For this 1) I believe that this is not bijective although the domain lets this graph satisfy injective properties since it is a one to one function, the image of this graph is not equal to the codomain it is not sujective. 2. I believe that this function with the given domain and codomain is bijective since it satifies one to one and also surjectivity because the image of this graph corresponds to the codomain. Since whatever even value x is inputted the function is always push out odd integers. Is there a bijective function within this domain and codomain? $f:[-1,1] \to [-10000,10000]$ if $f(x)=10000x$ I believe that this would be a bijective function because first of all it is a linear function and it satisfies one to one and within this domain the image of the function and the codomain is equal. I am not entirely sure of my answers and would like some clarification if wrong :) Thank you","['elementary-set-theory', 'functions', 'graphing-functions']"
3800372,Forming a subset of $\mathbb{R}$ by coin tossing,"We form a subset $X$ of $\mathbb{R}$ as follows: for every $x\in\mathbb{R}$ we toss a coin. if heads then we put it in $X$ . Assume two tosses are independent of each other. What is the probability that $X$ is a (Lebesgue) measurable subset of $\mathbb{R}$ ? I just started learning probability theory so I am not sure how to formulate this properly, I mean I get uncountably many random variables ... . Will really appreciate any help with this. Feel free to use any advanced theorems, I will look them up. EDIT: Thanks to users angryavian & StephenMontgomery-Smith for pointing out that it makes sense only to talk about probability of $X$ being measurable. I have edited the same. Thanks","['measure-theory', 'lebesgue-measure', 'probability-theory', 'real-analysis']"
3800380,Solving $\sqrt[3]{x+1} - \sqrt[3]{x-1} = \sqrt[3]{x^2-1}$ for real $x$,"Solve the equation in the Real number system: $$\sqrt[3]{x+1} - \sqrt[3]{x-1} = \sqrt[3]{x^2-1}$$ I have attempted using $(A-B)^3 = A^3 - B^3 - 3.A.B.(A-B)$ with $A = \sqrt[3]{x+1}$ , $B = \sqrt[3]{x-1}$ and $(A-B) = \sqrt[3]{x^2-1}$ , however I end up getting $-3.(\sqrt[3]{x^2-1})^2 = x^2-3$ which is not equivalent to the original one since 0 is solution only of $-3.(\sqrt[3]{x^2-1})^2 = x^2-3$ . Could someone explain to me why does it not work here? By the way, the answer according to the book is $\left\lbrace \frac{\sqrt 5}{2},-\frac{\sqrt 5}{2} \right\rbrace$ but I could not get there.","['radical-equations', 'algebra-precalculus', 'roots']"
3800387,Proving $\int_{0}^{1} \frac{\tanh^{-1}\sqrt{x(1-x)}}{\sqrt{x(1-x)}}dx=\frac{1}{3}(8C-\pi\ln(2+\sqrt{3}))$ for an identity of Srinivasa Ramanujan,"Ramanujan is supposed to  have given more than five thousand elegant results, a good number of them are yet to be proved or disproved. Yesterday in the comment section of Proving that $ \sum_{k=0}^\infty\frac1{2k+1}{2k \choose k}^{-1}=\frac {2\pi}{3\sqrt{3}} $ A wonderful Ramanujan identity $$S=\sum_{k=0}^{\infty}  \frac{1}{(2k+1)^2}{2k \choose k}^{-1}=\frac{1}{3}(8C-\pi\ln(2+\sqrt{3}))~~~~(1)$$ was showcased, Mathematica also gives this out. My Attempt to prove (1) by hand: Note the integral representation of the reciprocal of the binomial co-efficient: $${n \choose j}^{-1}=(n+1)\int_{0}^{1} x^j (1-x)^{n-j}~ dx~~~~(2)$$ $$S=\sum_{k=0}^{\infty}  \frac{1}{(2k+1)^2}{2k \choose k}^{-1}= \int_{0}^{1} \sum_{k=0}^{\infty} \frac{[x(1-x)]^{k}}{(2k+1)} dx= \int_{0}^{1} \frac{\tanh^{-1}\sqrt{x(1-x)}}{\sqrt{x(1-x)}} dx~~~~(3)$$ The question is:  How to get this integral (3) by hand ?","['integration', 'definite-integrals', 'ramanujan-summation', 'binomial-coefficients', 'binomial-theorem']"
3800398,"Closed $[a,b]⊆\mathbb{R}$ is not a countable union of $≥2$ disjoint closed intervals?",Show that in real line closed intervals cannot be written as a union of countable disjoint closed intervals. Honestly I cannot see how to show that. If it was given that about a closed sets i know some of the example and Cantor set is also a good example but i dont find any examples to show that a closed interval cannot be written as a union of countable disjoint closed intervals. Although if i consider the trivial representation by singletone sets then i can see that it is uncountable but satisfies given other properties. But i dont know any example. So i want help. Thank you.,"['real-numbers', 'metric-spaces', 'real-analysis']"
3800412,Coloring Julia Sets using Distance Estimation Relative to Zoom Depth,"Using the distance estimation coloring algorithm learned from here , I was able to color Julia Sets projected on a Riemann sphere, as with this video. However, once I started displaying polynomial matings of Julia Sets , using this coloring algorithm provided inconsistent results, as shown in the first image below. Certain parts are clearer than others, and this is because the polynomial mating brings out deeper parts of the fractal without zooming in , and so using the same distance adjustment makes those zoomed in parts more ""blurry"" than the rest. As such, what I think I need is some sort of algorithm to detect how ""zoomed in"" I am in the Julia set, so I can adjust the distance accordingly. I tried basing it off of how many iterations it takes for the orbit to escape (as the deeper you get, the more iterations it takes for the orbit to escape) but that didn't quite get the intended effect (second image below - certain parts are barely visible). Here is my code for the coloring: for (iter = currentMatingIteration + 1; iter < maxIterations && (w.x * w.x + w.y * w.y < bailout*bailout); iter++)
{
    d2 *= 4.0 * w2;
    
    // Julia Set algorithm
    w = c_2(w) + c;

    w2 = w.x * w.x + w.y * w.y;

    // Distance checker
    if(w2 > maxDist)
        break;
}

float fineness = 7;     // the higher, the less ""blurry""
//float fineness = 15;  // this is used for the second picture below

float d = sqrt(w2 / d2) * log(w2);  // this is the distance estimation
float dist = clamp(sqrt(d * pow(fineness, 2)), 0, 1);   // this is the adjustments I make for coloring

//float dist = clamp(sqrt(d * pow(fineness * (float(iter) / maxIterations), 2)), 0, 1);     // This is my attempt to solve this problem, used in the second picture below My project is here for testing. Edit: While this probably isn't a general solution to figuring out how deeply zoomed one is, what worked for this issue is calculating the derivative during the pull-back part of the mating algorithm, and using that as the initial value for calculating the distance estimation for each Julia Set (thanks to Claude in the comments). The successful result is below: Riemann Sphere Adjustment Without adjustment: With adjustment:","['complex-analysis', 'derivatives', 'coloring', 'fractals']"
3800420,Orientability of a submanifold which is a preimage of a submanifold,"I am reading Milnor & Stacheff, Characteristic Classes, Chapter 18. There is a short review of smooth manifolds, and there is a following statement: Suppose $f:M\to N$ is a smooth map between smooth manifolds, and suppose $f$ is transverse to a submanifold $Y\subset N$ (so that $f^{-1}(Y)$ is a submanifold of $M$ ). If $\nu^k$ is the normal bundle of $Y$ in $N$ , then the induce bundle over $f^{-1}(Y)$ from $\nu^k$ by $f$ can be idenfied with the normal bundle of $f^{-1}(Y)$ in $M$ . In particular, if $\nu^k$ is an oriented vector bundle, and if $M$ is an oriented manifold, then $f^{-1}(Y)$ is also an oriented manifold. I see that (assuming that $M,N$ are Riemannian) the normal bundle of $f^{-1}(Y)$ can be identified with the induced bundle $f^*\nu^k$ , but I can't see how the last statement follows. Am I missing a elementary theorem or something?","['riemannian-geometry', 'transversality', 'smooth-manifolds', 'vector-bundles', 'differential-geometry']"
3800422,How do the distance spectra of imaginary and real roots of uniform tilings relate?,"Consider a tiling of the hyperbolic plane defined by a vertex configuration $\Gamma$ . Let in the following be $\Gamma = (3.4)^3$ to have a specific example at hand: Let the distance spectrum of  a ""imaginary root"" be the sequence of numbers $f(n)$ giving the numbers of vertices on the border of the following sequence $\mathcal{C}_n$ of graphs which are the vertices at some obvious distance $n$ away from the center of the graphs (which is not a vertex), i.e. the vertices on the $n$ -th upright square around the center. The sequence starts with $\langle 8, 28, 104,\dots \rangle$ . $\mathcal{C}_1$ $\mathcal{C}_2$ $\mathcal{C}_3$ Compare this spectrum with the distance spectrum of a ""real root"", i.e. an arbitrary vertex (arbitrary, because the tiling is vertex transitive). The sequence $g(n)$ of numbers of vertices at graph distance $n$ away from a real root starts with $\langle 6, 21, 72 \dots \rangle$ . My question is: How do these two sequences relate? How can one be derived from the
other?","['graph-theory', 'combinatorial-geometry', 'combinatorics', 'generating-functions']"
3800436,"If a matrix $A \in \mathbb{R}^{N\times N}$ is both row and column diagonally dominant, will it satisfy $(x^{2p-1})^T A x \geq 0, p \geq 1$?","If a matrix $A = \{a_{i,j}\} \in \mathbb{R}^{N\times N}$ is both  row and column diagonally dominant with non-negative diagonal entries, i.e. $a_{i,i} \geq  0$ , $\forall i = 1, \cdots, N$ $a_{i,i} \geq \sum_{j = 1,\cdots, N; j\neq i} |a_{i,j}|$ , $\forall i = 1, \cdots, N$ $a_{i,i} \geq \sum_{l = 1,\cdots, N; l\neq i} |a_{l, i}|$ , $\forall i = 1, \cdots, N$ will it satisfy $x^T A x \geq 0, \forall \mathbf{x} \in \mathbb{R}^N$ ? EDIT True, answered by Minus One-Twelfth $(\mathbf{x}^{(2p-1)})^T A \mathbf{x} \geq 0$ , where $p \geq 2$ is an interger? EDIT : $\mathbf x^{2p-1} = [x_1^{2p-1}, x_2^{2p-1}, \cdots, x_N^{2p-1}]^T$ . Thank you very much! I wrote a short matlab code to verify this: N = 5;
for i = 1:100000
    A = 2*rand(N, N) - 1; % random value in [-1, 1]
    rowsum = sum(abs(A), 2) - abs(diag(A));
    columnsum = sum(abs(A), 1)' - abs(diag(A));
    v = max(rowsum, columnsum);
    A = A - diag(diag(A)) + diag(v); % column/row diagonally dominant
    xv = 4*rand(N, 100000) - 2; % random vector in [-2, 2]
    p = 1;
    minvalue = min(dot((xv.^(2*p-1)),  A * xv))
    if minvalue < 0
        fprintf('wrong!\n');
        pause;
    end
end","['linear-algebra', 'numerical-linear-algebra', 'inequality', 'positive-definite', 'quadratic-forms']"
3800448,Very hard variation of handshake problem,"Here is the problem: There are 1000 people in a hall. Initially one person had his hand painted. Every second everyone shakes their hand with someone else (in the sense that every second 500 couples form and the two people in the same couple shake hands with each other). In addition no two people can ever shake hands more than once. Of course whenever someone with a painted hand shakes the hand of someone who has it clean, it gets it painted. How much time, at most, is needed to paint all the hands? Prove it. Clarification: we are only considering games that run the full length, i.e. the game has to be able to get to the last round after which all possible handshakes have occurred, no dead ends allowed. So the question is posed within the framework of such games. My considerations: I've tried pretty hard to get the answer for a general n people game, or even for this 1000 people game, but there really seems to be nothing helpful to prove it or even guess it or find it easily for large n, especially given the fact that I have manually bashed the first cases for n = 2,4,6,8,10,12 (the answers being 1,2,3,5,6,8 rounds respectively) which look to have no useful relationship whatsoever between eachother or with n. I think the greedy algorithm is optimal, but I haven't even bothered proving that, since it doesn't really help to find the answer to the problem and prove it, so at times I've just tried to assume it, but even then it didn't quite get me anywhere. Also I don't think there is some beautifully simple symmetry argument to get an answer here, because that should hopefully be reflected in the cases for the first few n, but maybe I am missing it, I couldn't think of anything of that kind. What I am thinking now is that the answer might be some really complicated non closed form/non elementary function of n, or possibly some not even expressible function of n (this last statement in the sense that it's some function who's values for each given n are defined to be the ones given by such a game like this one, or some isomorphic problem, and there definitely are such kind of functions out there, so this could be a possibility). But if any of these last options I've given are correct, how could one possibly prove that? Thank you very much for the help, I hope there is someone who can solve this.","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
3800475,"Difference between random variables, random samples, and observations","Here there is my (difficult to be phrased without looking silly) question: is there an actual difference between a random variable, a random sample, and an observation? [For what below, the books I am reading are De Groot and Schervish's ""Probability and Statistics"" and Spiegel's ""Statistics"" (Schaum's)] Before jumping to the conclusion that I am missing some very basic stuff (which could be, of course), give me a second to explain my problem and read below the example, which could lead to a slightly more precise formulation of the question. Example Given a population of 100 students in a campus, we want to gather some information concerning their heights, such as mean, variance, etc. Thus, on Monday we get a sample of 5 students, on Tuesday we get a sample of other 5 students, and so on until Sunday: we end up having 7 samples, with 5 observations each. In the example above, for what would we use the notation $X_i$ ? I know that notation is just notation, but my point is that $X_i$ is used for many things in statistics and sometime I feel I lose sight of the actual object behind the notation. What I mean is that, by taking the example, my first feeling is that the r.v. is the height, every student is an observation, and we have the 7 samples (this would be my feeling after reading Spiegel). But then, after De Groot and Schervish, I get the feeling that a student is a r.v. in its own right, call it, the $X_i$ random variable. What above becomes even muddier to me when we move to flipping coins (a favourite example of books): if we flip one coin $n$ times and we do it $k$ times to get if it is fair, what is the r.v.? I would say that we have just one r.v., which is the behavior of the coin; also, we have $k$ samples of the behavior of this one coin, with $n$ observations each. Thus, $X_i$ would be reserved for the sample? However, it seems to me that quite often (always), one observation (say the $i$ th) of a head or tail of that one coin is considered a r.v. in itself, thus getting the $X_i$ notation. If you are wondering why this question, simply, I want to get this, because I have the feeling the confusion I have is hampering my possibility of gettings some things right in statistics. Below, some links that - unfortunately - did not help me in getting this right: Random Variable vs data vs random sample https://stats.stackexchange.com/questions/239500/what-is-the-difference-between-random-variable-and-random-sample Concerning the second link, the comment of hanugm to the accepted answer (which has been left unanswered) really summarizes what my problem is all about.","['statistics', 'sampling', 'probability', 'random-variables']"
3800547,How do we calculate the directional derivative of a vector field? (If there is such a thing.),"So, for a scalar field $T(x,y,z)$ , the derivative along $d\vec l$ is given by $$\frac {dT}{|d\vec l|} = |\vec \nabla T| \cos\theta$$ where $\theta$ is the angle between $\vec \nabla T$ and $d\vec l$ For a vector field $\vec V (x,y,z)$ , I understand that $\vec \nabla . \vec V$ and $\vec \nabla \times \vec V$ give the Divergence and the Curl respectively. But, is there a way in which $\vec \nabla$ can act on $\vec V$ to give an expression for $\frac {d \vec V}{|d\vec l|}$ , the directional derivative of $\vec V$ along $d\vec l$ ? PS: I've only just started to learn vector calculus, so pardon me if this question comes out as silly.","['field-theory', 'vector-fields', 'derivatives', 'vector-analysis']"
3800584,Number of combinations of two numbers from a list with repeating numbers? [duplicate],"This question already has answers here : Extended stars-and-bars problem(where the upper limit of the variable is bounded) (3 answers) Closed 3 years ago . I've tried googling it and looking it up on this website but since I don't know the technical term for this calculation I ran out of luck. Basically, if I have a collection of numbers (each of which may have duplicates) how many unique combinations of $n$ numbers can I make by picking from that collection? ( This question addresses the same issue ) For example: $C = \{ 1, 2, 2, 3, 3, 3 \}$ and I want to know how many combinations of $2$ numbers I can make. Glancing over the collection, I can quickly see I can only make the following pairs: $P = \{ (1,2),(1,3),(2,2),(2,3),(3,3) \}$ Which gives me the answer $|P|=5$ . But if I want to find the number of combinations of $4$ numbers, I can't just enumerate all the possible $4$ -tuples because there's no way to make $(1,1,1,1)$ or $(1,2,2,2)$ , for example. Is there a way to calculate this in general using combinatorics?","['combinations', 'combinatorics', 'multisets']"
3800605,Is this proof of $\tan \frac{x}{2} = \frac{1-\cos x}{\sin x}$ incomplete?,"So, for any angle $\alpha$ : $$\cos(2\alpha) = \cos^2\alpha - \sin^2\alpha = \dfrac{\cos^2\alpha-\sin^2\alpha}{\cos^2\alpha+\sin^2\alpha} = \dfrac{\dfrac{\cos^2\alpha-\sin^2\alpha}{\cos^2\alpha}}{\dfrac{\cos^2\alpha+\sin^2\alpha}{\cos^2\alpha}}= \dfrac{1-\tan^2\alpha}{1+\tan^2\alpha}$$ Now, $\cos\alpha = \cos\Big(2\cdot\dfrac{\alpha}{2}\Big) = \dfrac{1-\tan^2\dfrac{\alpha}{2}}{1+\tan^2\dfrac{\alpha}{2}}$ Now, using the componendo and dividendo rule, we get : $$\dfrac{\cos\alpha+1}{\cos\alpha-1} = \dfrac{2}{-2\tan^2\dfrac{\alpha}{2}} = \dfrac{-1}{\tan^2\dfrac{\alpha}{2}} \implies \tan^2\dfrac{\alpha}{2} = \dfrac{1-\cos\alpha}{1+\cos\alpha}$$ $$\implies \tan^2\dfrac{\alpha}{2} = \dfrac{(1-\cos\alpha)(1-\cos\alpha)}{(1+\cos\alpha)(1-\cos\alpha)} = \Big(\dfrac{1-\cos\alpha}{\sin\alpha}\Big)^2$$ $$\implies \Bigg|\tan\Big(\dfrac{\alpha}{2}\Big)\Bigg| = \Bigg|\dfrac{1-\cos\alpha}{\sin\alpha}\Bigg|$$ Now, only if $\mathrm{sign}\Big(\tan\dfrac{\alpha}{2}\Big) = \mathrm{sign}\Big(\dfrac{1-\cos\alpha}{\sin\alpha}\Big)$ is true, we can say that $\tan\dfrac{\alpha}{2} = \dfrac{1-\cos\alpha}{\sin\alpha}$ So, I think that without proving that, the proof will be incomplete but my Math textbook doesn't prove it. So, is it necessary to prove it? If not, why not? Thanks!","['trigonometry', 'solution-verification']"
3800664,"If $ f\geq0 $ and $ \intop_{0}^{\infty}f\left(x\right) $ converge, and $ \intop_{0}^{\infty}f'\left(x\right) $ converge, does it mean that:","Let $ f $ be non negative function, and differntiable such that $ f' $ is continious and $ \intop_{0}^{\infty}f\left(x\right),\intop_{0}^{\infty}f'\left(x\right) $ both converge. Is it true that $ \lim_{x\to\infty}f'\left(x\right)=0 $ ? I know that $ \lim_{x\to\infty}f\left(x\right)=0 $ for sure in this conditions. But Im not sure how to tell something about the limit of the deriviative. Thanks in advance","['calculus', 'improper-integrals']"
3800705,About the Elliptical Function $\mathrm{sn}(z)$,"In the picture below, the symmetry principle was used to show that the rectangle of Figure $36$ a is mapped onto Figure $36$ b. I did that. Shortly thereafter the function $\mathrm{sn}( \alpha \cdot z)$ is considered, where $\alpha$ is such that $\mathrm{sn}(\alpha \cdot K) = 1$ . Then, it claims that with this condition on $\alpha$ , the function $\mathrm{sn}(\alpha \cdot z)$ maps Figure $36$ a onto the upper half-plane. I can't see why this statement is true. I had some ideas but none with success, if someone could give me a light, I would be grateful. Idea $1$ : Since the function $\mathrm{sn}(z)$ is analytical, in particular, continuous, then it maps boundary to boundary, and connected set to connected set. Thus, it is enough to show that the boundary of the interior of the rectangle is mapped to the real axis, but I couldn't do it. The other idea was to remember that, by definition, the function $\mathrm{sn}(z)$ maps the interior of the rectangle with vertices $-K, K, K+iK' \hspace{1mm} \mbox{and} \hspace{1mm} -K+iK'$ onto the upper half-plane. Thus, if the function $\mathrm{sn}(\alpha \cdot z)$ is considered, it maps the inside of the rectangle with vertices $- \dfrac{K}{\alpha}, \dfrac{K}{\alpha}, \dfrac{K+iK'}{\alpha} \hspace{1mm} \mbox{and} \hspace{1mm} -\dfrac{K+iK'}{\alpha}$ onto the upper half-plane. My idea was to show that this last rectangle and the rectangle of Figure 36a are the same. I think I should use the condition $\mathrm{sn}(\alpha \cdot K) = 1$ to conclude this fact but I couldn't do it. Applying the symmetry principle to an inversion of the rectangle in Fig $34$ a with respect to its upper side, we find that the function $w=\mathrm{sn}(z;q)$ maps the rectangle of Fig. $36$ a onto the full $w$ -plane which is furnished with a slit as indicated in Fig. $36$ b. Consider now the function $\mathrm{sn}(\alpha z)$ (where $\alpha$ is such that $\mathrm{sn}(\alpha K=1)$ ) which maps the rectangle in Fig. $36$ a onto the upper half-plane.",['complex-analysis']
3800725,Spivak Calculus Chapter 5-33d: $\lim_{x \to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2}$,"Find the following limit: 33d) $$\lim_{x\to\infty}\frac{x^2(1+\sin^2(x))}{(x+\sin(x))^2}$$ I am 99.9% sure this limit doesn't exist because as $x \rightarrow \infty$ , $1+\sin^2(x)$ periodically oscillates between 1 and 2, which means so does the limit. Is this a trick question or am I losing my mind?","['limits', 'calculus', 'trigonometry']"
3800777,"Give a regular expression for the set of strings over {a, b, c} such that the number of a's equals the number of b's and is equal to 2","How would I describe from finite automata to regular expressions? I know how I would describe it if there were to be just one or more number of a's and b's using + but I'm not sure how to go about it to make it exactly 2. So far my thought process has been: These are the possibilities of the strings with 2 a's and 2 b's: aabb, abab, bbaa, abab, abba, baab. Now I need to add in the c's, but it doesn't matter where it is placed - so it would be c*?","['automata', 'regular-expressions', 'discrete-mathematics', 'regular-language']"
3800793,What is a function called that maps the minimum of the domain to the minimum of the codomain?,"Is there a standard name for a function that maps the minimum of the domain to the minimum of the codomain? In greater detail, let $\mathbf{X} = (X,\leq)$ and $\mathbf{Y} = (Y, \leq)$ be partially-ordered spaces. Suppose $\mathbf{X}$ and $\mathbf{Y}$ each has a minimum. Let $f:X\rightarrow Y$ . Is there a name, known in the mathematical literature, for the following property of $f$ : $ f(\min \mathbf{X}) = \min\mathbf{Y}$ ?","['maxima-minima', 'order-theory', 'functions', 'terminology']"
3800806,Using Fundamental Theorem of Algebra to find $z_0$ such that $|p(z_0)| < |p(0)|$,"A question from the book Hubbard's Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach (5th edition): I was able to easily find some point $z_0 = i/3$ (just by trying out points), but am unsure how to use their equation in their proof to find this point. Consider the polynomial $p(z) = z^8 + z^4 + z^2 + 1$ where $z \in \mathbb{C}$ , use the construction in the proof of the fundamental theorem of algebra, using equation 1.6.28 to find a point $z_0$ such that $|p(z_0)| < |p(0)| = 1$ . Here is a rough idea of how they prove The Fundamental Theorem of Algebra in their book: First, they showed that for any monic polynomial of degree $k > 0$ with complex coefficients $p(z) = z^k + a_{k-1} z^{k-1} + \dots + a_0$ , that $|p(z)|$ always has a global minimum at $z_0 \in \mathbb{C}$ for some $R>0$ with $|z_0| \leq R$ . Afterwards to show $p(z_0) = 0$ , they assume instead $p(z_0) \neq 0$ to find a point $z$ such that $|p(z)| < |p(z_0)|$ , which would lead to a  contradiction. To do this, they let $z = z_0 + u$ so that: $$\begin{align} p(z) &= (z_0 + u)^k + a_{k-1} (z_0 + u)^{k-1} + \dots + a_0 \\ &= u^k + b_{k-1}u^{k-1} + \dots + b_0 \\ &= q(u) \end{align}$$ where it can be shown $b_0 = z_0^k + a_{k-1}z_0^{k-1} + \dots + a_0 = p(z_0) \neq 0$ If we let $j>0$ be the smallest power of $q(u)$ with a non-zero coefficient, then we have equation 1.6.28 : $$\boxed{ q(u) = b_0 + b_j u^j + (b_{j+1} u^{j+1} + \dots + u^k) = p(z) = p(z_0 + u)}$$ Noting $u$ can be written as $\rho e^{i\theta}$ , it can be imagined $b_0 + b_j u^j$ is travelling around a circle with center $b_0 = p(z_0)$ . It can then be shown there exists a $\rho$ (I am not detailing its value here) such that $|b_j|\rho^j < |b_0|$ ,  so that for some values of $\theta$ we have $|b_0 + b_j u^j| < |b_0|$ (i.e. visually it will be a point on the line segment between $0$ and $b_0$ )  and $|b_{j+1} u^{j+1} + \dots + u^k| < |b_j|\rho^j$ (the distance between the points $b_0 + b_ju^j$ and $b_0$ ). This leads to the contradiction $|p(z)| = |p(z_0 + u)| < |b_0| = |p(z_0)|$ , since $|p(z_0)|$ is the minimum of the modulus of polynomial $p$ . I detailed the proof if it is needed (if you think I have left something out from the book let me know), but now I am uncertain how to use the equation 1.6.28 (boxed equation above) in their proof to find this point $z_0$ . Any hints or ideas would be greatly appreciated.","['complex-analysis', 'complex-numbers']"
3800838,A fast solution of $\frac{\left|x^2-1\right|-3}{1-2x}<\:x$,"If I have this easy inequality $$\frac{|x^2-1|-3}{1-2x}<\:x$$ your solutions, step by steps are $]-1,\frac{1}{2}[\,\cup\, ]\frac{4}{3},+\infty[$ , considering the signs of $|x^2-1|$ , i.e. $x^2-1\geq 0 \iff x\leq 1 \vee x\geq 1$ and $x^2-1<0 \iff -1<x<1$ and solving a simple fracture inequality with $1-2x\neq 0$ . I have five options: \begin{array} {|r|r|}\hline  A=]- 1; 1[\,\cup \,] \frac{4}{3} ; 2[ \\ \hline B=] -\infty;-1[ \cup ] \frac{1}{2} ; \frac{4}{3} [ \\ \hline C=]-1;\frac{1}{2}[ \cup ]  ; ]\frac{4}{3} +\infty[  \\ \hline D=] -\infty;-1[ \cup [1; \frac{4}{3} [ \\ \hline \text{None of the previous answers are correct}\\ \hline  \end{array} I've seen, without to do the calculus, that the $0$ satisfies the inequality and it is not are into the sets $B$ and $D$ . It not $A$ because $x\neq \frac 12$ . Hence I have $50 \%$ to find the correct answer: or it is into $C$ (exact answer) or $E$ (none of the previous answers are correct). Any of you users see something else in your minds?",['algebra-precalculus']
3800848,"For vector spaces $V,W$ over $\mathbf{k}$, is every additive $\phi: V \to W$ also $\mathbf{k}$-linear?","Since every map of vector spaces is a map of abelian groups, I was wondering if the converse also holds: Given an additive map $\phi: V \to W$ between two vector spaces, does it follow that $\phi$ is also $\mathbf{k}-$ linear? I'm interested in the case of $\mathbf{k}$ having characteristic zero, specially if $\mathbf{k}$ is a famous field like rational or real or complex numbers. I'm guessing it's false, but I tried to come up with counter-examples for $\mathbf{k} = \mathbf{Q}, \mathbf{R}$ and couldn't find any. Finding a counter-example in characteristic $p>0$ may not be that hard, for instance since taking $p-$ th power is additive. However that's not the case I care most about. Appreciate any help!","['abstract-algebra', 'linear-algebra', 'vector-spaces']"
3800852,"Prove existence of a function $f:\Bbb Z^+ \to \Bbb Z^+$ such that for any $a,b,c$ there exists $n$ such that $f(an+b)=c$","I'm trying to solve the following exercise from Velleman's ""How to Prove it"" book: Prove that there is a function $f:\Bbb Z^+ \to \Bbb Z^+$ such that for all positive integers $a, b$ and $c$ there is some positive integer $n$ such that $f(an+b) = c$ . I have been trying to use the equinumerous theorems between family of functions, but without success. Could anyone give me a hint on how to approach this problem?",['elementary-set-theory']
3800892,"What does ""antisymmetric"" mean for the adjoint map of a Lie algebra?","A theorem about Lie algebras says that: Fix a Lie group $G$ , endowed with a left invariant riemannian metric $g$ , let $\nabla$ be the Levi Civita connection. Let the Lie algebra of $G$ be $\mathfrak{g}$ then the following are equivalent: The adjoint map of $\mathfrak{g}$ is such that $ad(X)$ is antisymmetric $\forall X\in\mathfrak{g}$ The one parameter subgroups of $G$ are precisely the (Levi-Civita) geodetics of $G$ Now my question is not about the theorem, but about the meaniing of the first condition. To my understanding $ad$ is a linear map $X\in\mathfrak{g}\mapsto ad(\mathfrak{g})\in \mathfrak{gl}(\mathfrak{g})\cong Hom(\mathfrak{g},\mathfrak{g})$ where $ad(X): Y\in\mathfrak{g}\mapsto [X,Y]\in \mathfrak{g}$ . So it is a linear map, and not multinear map: how can it be antisymmetric? Wat do we mean by this term here? What am I missing? Thanks in advance","['geodesic', 'lie-algebras', 'riemannian-geometry', 'lie-groups', 'differential-geometry']"
3800911,Prime numbers which divide $n^3-3n+1$,"Let $f(n)=n^3-3n+1$ . It can be proved that for any prime $p$ and integer $n$ such that $p\mid f(n)$ we have either $p=3$ or $p\equiv\pm1\pmod 9$ (see below). Indeed, suppose that for prime number $p$ and integer $n$ we have $p\mid n^3-3n+1$ .
Firstly, note that if $x=t+\frac{1}{t}$ , then $$
f(x)=t^3+\frac{1}{t^3}+1=\frac{t^6+t^3+1}{t^3}.
$$ Now we will consider two cases: Case 1. In $\mathbb{F}_p\backslash\{0\}$ there is a $a$ such that $n\equiv a+\frac{1}{a}\pmod p$ . Then, we have $$
a^6+a^3+1\equiv 0\pmod p,
$$ so $a^9\equiv 1\pmod p$ . Moreover, $x^{p-1}\equiv 1\pmod p$ . Thus, the order $d$ of $a$ in $\mathbb{F}_p^{\times}$ must divide $\gcd(p-1,9)$ . If $d\in\{1,3\}$ , then $a^3\equiv 1\pmod p$ , so $p=3$ . Otherwise, $d=9$ and $9\mid p-1$ , as desired. Case 2. There is no $a\in\mathbb{F}_p$ such that $n\equiv a+\frac{1}{a}$ . Then, we can consider the extension $\mathbb{F}_p(a)$ , where $a$ is a root of the polynomial $x^2-nx+1=0$ (which is irreducible in $\mathbb{F}_p$ due to our assumption). Note that $|\mathbb{F}_p(a)|=p^2$ since the degree of this extension is 2. Similarly, as in the first case we deduce that in $\mathbb{F}_p(a)$ $$
a^6+a^3+1=0,~\text{so}~a^9=1.
$$ If $d$ is the order of $a$ in $\mathbb{F}_p(a)^{\times}$ , then $d\mid\gcd(p^2-1,9)$ . As in the first case, if $d\in\{1,3\}$ , then $a^3=1$ in $\mathbb{F}_p(a)$ and $p=3$ . Otherwise, $d=9$ , so $9\mid p^2-1$ . Therefore, $p\equiv\pm 1\pmod 9$ , as desired. I am interested in elementary proof of this fact (without using field extensions, groups, etc.). Is this possible?","['number-theory', 'polynomials', 'prime-numbers']"
3800918,Proof of Weyl's criterion not using essential spectrum,"Consider the following theorem. Theorem: Let $A$ be a bounded self-adjoint operator on a Hilbert space $\mathcal{H}$ . Then $\lambda \in \sigma(A)$ if, and only if there exists a sequence $\{\psi_{n}\}_{n\in \mathbb{N}}$ such that $||\psi_{n}|| = 1$ for all $n$ and $\lim_{n\to \infty}||(A-\lambda)\psi_{n}|| \to 0$ . This is a 'part' of the so-called Weyl's criterion . Usually, this result arises when studying spectral theory of Fredholm operators , where the essential spectrum plays a central role. In general, the above theorem comes together with some statement(s) concerning the essential spectrum of $A$ too, so that the above result is proved by using properties of the essential spectrum. However, I'm not interested in Fredholm operators, but rather in bounded self-adjoint operators alone. Question: How can I prove (or where can I find the proof) of the above Theorem using just the usual spectral theory of bounded self-adjoint operators and not using essential spectrum arguments?","['hilbert-spaces', 'spectral-theory', 'functional-analysis']"
3800991,Why can the irreducible representations of $SU(N)$ be obtained in this way?,"Let us consider the group $SU(N)$ . Recently watching a recorded lecture in group theory for physicists I've heard about the following. Take the definining representation of $SU(N)$ , namely, $(\rho,V)$ where $V=\mathbb{C}^N$ and where $\rho : SU(N)\to \operatorname{GL}(\mathbb{C}^N)$ is just the inclusion map, i.e., for every $g\in SU(N)$ we have $\rho(g)$ act upon $v\in \mathbb{C}^N$ by usual matrix multiplication. Now take the $r$ -fold tensor product $V^{\otimes r}$ . The representation $\rho$ naturally induces one representation on $V^{\otimes r}$ which is characterized on decomposable tensors as $$[\rho^{\otimes r}(g)](v_1\otimes\cdots \otimes v_r)=(\rho(g)v_1)\otimes\cdots \otimes (\rho(g)v_r).\tag{1}$$ On the other hand, the symmetric group $S_r$ naturally acts on $V^{\otimes r}$ . Its action on decomposable tensors is $$\pi(\sigma)(v_1\otimes\cdots\otimes v_r) = v_{\sigma(1)}\otimes\cdots\otimes v_{\sigma(r)}.\tag{2}$$ We may further show that the two actions commute. This is clear on decomposable tensors. We just apply $\pi(\sigma)$ to (1) and apply $\rho^{\otimes r}(g)$ to (2) and compare. Since decomposable tensors span $V^{\otimes r}$ and since $\rho^{\otimes r}(g)$ and $\pi(\sigma)$ are linear, the two operations must commute applied to anything. Now I've heard that because of this, if we decompose $V^{\otimes r}$ in a direct sum of irreducible representations of $S_r$ the irreducible factors also give rise to irreducible representations of $SU(N)$ . In other words, decompose $$V^{\otimes r}=\bigoplus_\lambda {\cal W}_\lambda,$$ where ${\cal W}_\lambda$ are irreducible representations of $S_r$ , then ${\cal W}_{\lambda}$ is invariant under $\rho^{\otimes r}$ and, moreover, the subrepresentation of $\rho^{\otimes r}$ so obtained is irreducible. Why is that the case? Why the two actions commuting imply that the irreducible $S_r$ factors of $V^{\otimes r}$ with respect to the $\pi$ are also irreducible factors of $SU(N)$ with respect to $\rho^{\otimes r}$ ? Can all irreducible representations of $SU(N)$ be found in this way? Namely, by taking the definining representation, taking one sufficiently big $r\in \mathbb{N}$ and decomposing $V^{\otimes r}$ into irreducible factors of the symmetric group?","['unitary-matrices', 'representation-theory', 'linear-algebra', 'group-theory', 'mathematical-physics']"
3800995,Support of a probability measure vs support of its margins,"Let $\mu$ be a Borel probability measure on the product of two metric spaces $(X, d) $ and $(Y, h) $ . Assume its marginal probability measures $\mu_X$ on $X $ and $\mu_Y$ on $Y$ have compact supports. Can we conclude that $\mu$ is compactly supported as well?","['measure-theory', 'probability-distributions', 'probability-theory']"
3801019,Calculating $\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx$ without using Beta function and Euler sum.,"Is it possible to show that $$\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=-\frac12\zeta(4)$$ without using the Beta function $$\text{B}(a,b)=\int_0^1 x^{a-1}(1-x)^{b-1}dx=\frac{\Gamma(a)\Gamma(b)}{\Gamma(a+b)}$$ and the generalized Euler sum $$\sum_{n=1}^\infty \frac{H_n}{n^q}= \left(1+\frac{q}{2} \right)\zeta(q+1)-\frac{1}{2}\sum_{k=1}^{q-2}\zeta(k+1)\zeta(q-k),\quad q\ge 2\ ?$$ By integration by parts I found $$\int_0^1\frac{\ln^2x\ln(1-x)}{1-x}dx=\color{blue}{\int_0^1\frac{\ln^2(1-x)\ln x}{x}dx}$$ Setting $1-x\to x$ gives the blue integral again. This integral seems tough under such restrictions. All approaches are welcome. thanks.","['integration', 'logarithms', 'real-analysis', 'complex-analysis', 'alternative-proof']"
3801021,"Which integers, satisfying a condition, satisfy the equation $a^2+b^2+16c^2=9k^2+1$","I found the following question, on a past international competition: Find all prime numbers $a, b, c$ and all positive integers k, which satisfy the equation: $a^2+b^2+16c^2=9k^2+1$ I solved it with the following laborious and tedious method: $a^2+b^2+c^2mod3\equiv 1$ However $x^2\equiv 0 or 1 mod3$ , for all integers x. So we have that either $a\equiv b\equiv 0mod3$ , $c\equiv 1or2mod3$ or any of its permutations hold true. If $a\equiv bmod3$ Then $a=3$ , $b=3$ Then $18+16c^2=9k^2+1$ $17+16c^2=9k^2$ I create a function $f$ , such that $f(x)=x^2-(x-1)^2$ $f'(x)=2$ So $f$ is increasing and 1-1. For $c>2$ (from the above) we have that $16c^2+17$ can't be a square of an integer, since $16^2$ is a square and hence since for c>2, $(4c+1)^2-16c^2>17$ , which is true from the $f$ function created above. So we have that $c=2$ is the only solution, with $c=2, a=3, b=3, k=3$ . If $a\equiv c\equiv 0mod3$ , $b\equiv 1or2mod3$ $a=c=3$ So $9+b^2+16*9=9k^2+1$ $b^2+153=9k^2+1$ $b^2+152=9k^2$ So we have $b^2+152\equiv 0mod9$ So $b\equiv 1 or 8 mod9$ If $x=81$ , then $f(x)=81^2-80^2=161>152$ , so from the conclusions we derived earlier on about the function $f$ , we have that b<80. Now we break it up into sub-parts: $b=1$ : $153=9k^2$ false $b=8$ : $216=9k^2$ false $b=10$ : $252=9k^2$ false $b=17$ : $441=9k^2$ So $k=7$ $b=19$ : $513=9k^2$ false $b=26$ : $828=9k^2$ false $b=28$ : $936=9k^2$ false $b=35$ : $1377=9k^2$ false $b=37$ : $1521=9k^2$ $k=13$ $b=44$ : $2088=9k^2$ false $b=46$ : $2268=9k^2$ false $b=53$ : $3113=9k^2$ false $b=55$ : $3177=9k^2$ false $b=62$ : $3996=9k^2$ false $b=64$ : $4248=9k^2$ false $b=71:$ $5193=9k^2$ false $b=73$ : $5481=9k^2$ false So from these sub-parts we have the following solutions: $b=17, a=3, c=3, k=7$ $b=37, a=3, c=3, k=13$ If $b\equiv c\equiv 0mod3$ , $a\equiv 1or2mod3$ Then it is symmetrical to the previous case: So the solutions are: $a=17, b=3, c=3, k=7$ $a=37, b=3, c=3, k=13$ So in total we have the following solutions: $a=17, b=3, c=3, k=7$ $a=37, b=3, c=3, k=13$ $b=17, a=3, c=3, k=7$ $b=37, a=3, c=3, k=13$ $c=2, a=3, b=3, k=3$ My solution to the question is extremely long and tedious. Are there better approaches to the question? And if so could you please post them up?","['contest-math', 'number-theory', 'elementary-number-theory', 'calculus', 'functions']"
3801038,Why does the Ratio Test prove absolute convergence when the limit test does not?,"I've been taught that the ratio test proves absolute convergence when the limit test does not but I've been confused about why is that.
The limit test, in my mind, gives the final value which is being continually added to the series. For example (Since I can't use the limit notation for the life of me, I'll type the values) Limit of $1/n^2$ , as $x$ approaches infinity, is $0$ . I'd assume the sum to be something like this. $$\sum_{i=1}^\infty\left(\frac1{i^2}\right) = 1 + \ldots+ 0 + 0 + 0 + \ldots$$ So basically the value has converged and therefore there is a limit. Of course, this doesn't work for some cases like the harmonic series and I'd like to get an idea of whether thinking of them this way is correct or not and get an intuition for how to think about such series whose limit approaches $0$ . However,
There is also the Ratio Test defines absolute convergence if the limit is less than 1. That is basically telling us that the series is getting smaller and smaller and thereby telling us that the limit Limit of $f(x)$ , as x approaches infinity, is 0 and basically giving the limit test. Why does this test give proof of absolute convergence then when the limit test can't give the proof? Proof would be appreciated but intuition as to how to understand them better would be appreciated much more.","['limits', 'sequences-and-series']"
3801046,"Evaluating Multivariable Limit $\lim\limits_{(x,y) \to (0,2)} \frac{\sin(xy)}{x}$","Question : Evaluate the limit $$\lim\limits_{(x,y) \to (0,2)} \frac{\sin(xy)}{x}$$ My first thought is that the limit looks a lot like the single variable $\lim\limits_{x \to 0} \frac{\sin(x)}{x} = 1$ . Regardless of what $y$ is (as long as it is real) $xy \to 0$ . Hence I am wrongly concluding that the entire limit evaluates to $1$ . I guess the ratio of the convergence is not the same as in the single variable case, hence it may not be 1. However, I am unsure how to evaluate it properly.","['limits', 'multivariable-calculus']"
3801057,Complex normal coordinates in Kähler manifolds,"Let $(M, g, J, \omega)$ be a Kähler manifold. That is, $(M, J)$ is a complex manifold, $g$ is a Hermitian metric on $M$ and $$\omega (X, Y) = g(JX, Y)$$ is a closed two form. As a Riemannian manifold $(M, g)$ , for each $x\in M$ , one can find a geodesic normal coordinates around each $x$ . In the case of Kähler metric, one actually have more: Proposition: (Complex Normal Coordinates on Kähler manifolds) For each $x\in M$ , there is a local holomorphic coordinates around $x$ so that the metric $g = g_{i\bar j}$ satisfies $$g_{i\bar j}(x) = \delta_{ij},  \ \ d g_{i\bar j} (x) = 0, \ \ \ \frac{\partial^2 g_{i\bar j}}{\partial z_k \partial z_l} (x) = 0.$$ While the first two conditions are similar to what we have for geodesic normal coordinates in Riemannian geometry, there is no corresponding analogue for the last condition. Also, even in a Kähler manifold, the geodesic normal coordinates might not be holomorphic. I am looking for a proof of this proposition.","['complex-geometry', 'kahler-manifolds', 'differential-geometry']"
3801095,Why some operations on tensors don't give a tensor?,"The gradient is a tensor $\nabla f:\mathbf{V} \to \mathbf{R}$ where the partial derivatives are evaluated at some point $(x_0, y_0, z_0)$ . And evaluation of this linear form at some vector $v=(v_1,v_2,v_3)$ gives $$
(\nabla f)(\mathbf{v}) = \partial_x f v_1 + \partial_y f v_2 + \partial_z f v_3
$$ Furthermore in going to a new coordinate system these partial derivatives transform in the expected way. But what about a function $g:\mathbf{V} \to \mathbf{R}$ which is defined only using the partial derivative of $f$ in the $x$ direction. $$
g(\mathbf{v}) = \partial_x f v_1 + \partial_x f v_2 + \partial_x f v_3
$$ As I understand this is not considered a tensor because in moving to a new coordinate system it does not transform correctly. This has confused me endlessly. The strict definition considers a map such as $f:\mathbf{V} \to \mathbf{R}$ a tensor if linearity holds in each parameter. The function $g$ above certainly satisfies that. It seems to me that this definition is not used and that the definition of a tensor that is actually used consists of two parts. linearity in each parameter (i.e. multilinear form), and the algebraic structure of the coefficients is maintained in coordinate transformation Because once we have calculated $\partial_x f$ it is just a scalar and we just hit $(\partial_x f,\partial_x f,\partial_x f)$ with the usual transformation for a covariant vector to get the new coefficients for $g$ in the new coordinate system. That these new coefficients don't have the right algebraic structure doesn't make multilinearity of $g$ go away. Is this at all correct?","['tensors', 'linear-algebra', 'multilinear-algebra']"
3801125,Showing $K(\sqrt \alpha)/F$ is Galois if and only if $\sigma(\alpha)/\alpha$ is a unit and a square.,"I would like help solving is the following problem: Assume that $K/F$ is a finite Galois extension and $\text{char} F \neq 2$ . Let $G:= \text{Gal}(K/F)$ be its Galois group and let $\alpha \in K^\times$ . Show that $K(\sqrt{\alpha})/F$ is a Galois extension if and only if $\frac{\sigma(\alpha)}{\alpha} \in K^{\times 2}$ for all $\sigma \in G$ , where $K^{\times 2} := \{x^2 \mid x \in K^\times\}$ . I have part of a solution for the reverse implication, but I am unsure where I use the hypothesis, so I am not confident of its validity. My argument goes as follows: if $\alpha$ is a perfect square, then $K(\sqrt{\alpha}) = K$ and the solution is trivial. Suppose $\alpha$ is not a perfect square. Then, the minimal polynomial of $\sqrt \alpha$ over $K$ is $x^2 - \alpha$ . This means $[K(\sqrt\alpha) : K] = 2$ . By the tower law, we have $[K(\sqrt{\alpha}), F] = [K(\sqrt{\alpha}): K] [K : F] = 2 |G|$ . Given any $\sigma \in G$ , we can extend it to an automorphism of $K(\sqrt \alpha)$ by choosing whether sigma will send $\sqrt \alpha$ to $+\sqrt{\sigma(\alpha)}$ or $-\sqrt{\sigma(\alpha)}$ (SEE EDIT BELOW). As $\text{char} F \neq 2$ , this gives 2 choices for every $\sigma \in G$ , hence we can have $2 |G|$ automorphisms, constructed in this way. As $|\text{Gal}(K(\sqrt \alpha), F)|$ is bounded above by $[K(\sqrt \alpha): F] = 2|G|$ , we have constructed every possible automorphism and $|\text{Gal}(K(\sqrt \alpha), F)| = [K(\sqrt \alpha): F]$ , so the extension is Galois. As far as I can tell, this doesn't use the hypothesis on $\frac{\sigma(\alpha)}{\alpha}$ , so I am sceptical. Help with both directions of the proof would be greatly appreciated. Edit: Following the comments from Μάρκος Καραμέρης, as $\sigma(\alpha) =  \alpha . k^2$ , $\sigma(\sqrt(\alpha)) = \pm k \sqrt \alpha$ , for some fixed $k \in K^\times$ . This gives us our extensions from $\sigma \in \text{Gal}(K/F)$ to some pair $\sigma_+, \sigma_- \in K(\sqrt \alpha)$ , where $\sigma_\pm (\sqrt(\alpha)) = \pm k \sqrt \alpha$ . This completes the reverse implication.","['field-theory', 'galois-theory', 'galois-extensions', 'abstract-algebra']"
3801135,Test for a continuous function,"Let $f$ be a function defined in $[0, 6]$ , continuous in $[0, 6]$ and it is provided of a third derivative in $]0, 6[.$ Which of the following assertions is false ? $$\fbox{A}\quad f \text{ has no asymptotes; }$$ $$\fbox{B}\quad f \text{ may have no critical points; }$$ $$\fbox{C}\quad f \text{ has a relative maximum or has a minimum
relative; }$$ $$\fbox{D}\quad f'' \text{ is continuous in } ]0; 6[;$$ $$\fbox{E}\quad \text{If } f'(5) = f''(5) = 0 \text{ and } f'''(5) = 7, \text{then } f \text{ has an inflection point with 
a horizontal tangent at } x = 5$$ Below there is the original question in Italian Language. Above there is the translation. My attempt of resolution for to find the correct answer. The $\fbox{A}$ is true being $f$ is continuous in $[0,6]$ .  The $\fbox{B}$ is true for the Weierstrass' theorem: remark that $[0,6]$ is closed set. If I think to the polynomial $\deg(p(x))=6$ and $\fbox{C}$ for me it is true. For the $\fbox{D}$ I have thought that if $f$ and it is provided of a third derivative in $]0,6[$ , almost for $f''$ is continuous in $]0,6[$ . I'd say the $\fbox{E}$ is false , but I can't justify it. I ask if my reasoning is correct or there are incongruities.","['calculus', 'derivatives', 'real-analysis']"
3801137,Trouble with the proof of Cauchy convergence criterion,"While reading the Analysis 1 textbook by Vladimir A. Zorich I encountered a proof which has this one conclusion I fail to understand. The theorem and proof : (Cauchy’s convergence criterion) A numerical sequence converges if and only if it is a Cauchy sequence. Proof. $\implies$ :(I skipped this part of the proof since I have no issues with it.) $\impliedby$ :
Let ${x_k}$ be a fundamental sequence. Given $\epsilon > 0$ , we find an index $N$ such that $|x_m − x_k| < \frac{\epsilon}{3}$ when $m ≥ N$ and $k ≥ N$ . Fixing $m = N$ , we find that for any $k >N$ $$x_N − \frac{\epsilon}{3}<x_k <x_N + \frac{\epsilon}{3}\ \text{,} \ \ \ \ \ \text{(3.1)}$$ but since only a finite number of terms of the sequence have indices not larger than $N$ , we have shown that a fundamental sequence is bounded. $$\text{For}\ n \in \mathbb{N}\ \text{we now set } a_n := \inf_{k≥n} x_k ,\ \text{and }\ b_n := \sup_{k≥n} x_k \ \text{.}$$ It is clear from these definitions that $a_n ≤ a_{n+1} ≤ b_{n+1} ≤ b_n$ (since the greatest lower bound does not decrease and the least upper bound does not increase when we pass to a smaller set). By the nested interval principle, there is a point A common to all of the closed intervals $[a_n, b_n]$ . Since $$a_n ≤ A ≤ b_n$$ for any $n \in \mathbb{N}$ and $$a_n = \inf_{k≥n} x_k ≤ x_k ≤ \sup_{k≥n} x_k = b_n$$ for $k ≥ n$ , it follows that $$|A − x_k| ≤ b_n − a_n\ \text{.}\ \ \ \ \ \text{(3.2)}$$ But it follows from Eq. $\text{(3.1)}$ that $$\underbrace{x_N − \frac{\epsilon}{3}≤ \inf_{k≥n} x_k = a_n ≤ b_n = \sup_{k≥n} x_k ≤ x_N + \frac{\epsilon}{3}}_{\text{The problematic part}}$$ for $n>N$ , and therefore $$b_n − a_n ≤ \frac{2\epsilon}{3} < \epsilon \ \ \ \ \ \text{(3.3)}$$ for $n>m$ . Comparing Eqs. $\text{(3.2)}$ and $\text{(3.3)}$ , we find that $|A −x_k| < \epsilon$ , for any $k > N$ , and we have proved that $\lim_{k \to \infty}x_k = A$ . End of proof. The underbraced part doesn't make sense to me, because from the stated it could happen that $$x_N − \frac{\epsilon}{3} = a_n $$ and since $a_n≤x_k$ it is possible that $a_n=x_k$ and if those equalities hold, then $x_N − \frac{\epsilon}{3} = a_n=x_k$ , but this contradicts what was stated before in $\text{(3.1)}$ , $x_N − \frac{\epsilon}{3}<x_k $ . Why does the problematic part hold despite this ?
Thanks","['cauchy-sequences', 'proof-explanation', 'proof-writing', 'real-analysis', 'limits']"
3801139,"Detail on the the choice of sign when computing $\int_{-1}^1\sqrt{1-x^2} \, dx$ by residues","Say I want to compute $\int_{-1}^1\sqrt{1-x^2}\,dx$ by residues. I use the traditional dog bone contour. The small circuits around $-1$ and $1$ do not contribute, while the segments in the middle add up to twice the integral I want. The total result is the residue at infinity. So far, so good. My problem is that when I want to compute the reside at infinity of $\sqrt{1-z^2}$ I write $z=1/w$ , $dz=-dw/w^2$ so $$\sqrt{1-z^2} \, dz=-\frac{1}{w^2}\sqrt{1-\frac{1}{w^2}} \, dw=-\frac{1}{w^3} \sqrt{w^2-1} \, dw$$ and I need to expand $\sqrt{w^2-1}$ around $w=0$ . This is done by writing $$\sqrt{w^2-1}=\sqrt{(-1)(1-w^2)}=\pm i\sqrt{1-w^2}$$ and then expanding $\sqrt{1-w^2}$ . My question is very specific: how do I decide whether to take $+i$ or $-i$ above?","['complex-analysis', 'contour-integration', 'residue-calculus']"
3801141,Prove or disprove Average rate of change,"Prove or disprove
For any Real valued function $f$ whose domain is all real numbers and continuous on $\mathbb{R}$ there is no function $f$ exist satisfy the following condition:
If $[a,b]$ and $[c,d]$ are two distinict intervals then the average rate of change on $[a,b]$ which is defined as $\frac{f(b)-f(a)}{b-a} $ doesn't equal average rate of change on $[c,d]$ . i.e. a function $f$ is continuous on $\mathbb{R}$ whose average rates of change are all distinct","['calculus', 'derivatives', 'real-analysis']"
3801180,Updates of Serge Lang — Differential manifolds,"I have Serge Lang — differential manifolds. An interesting read. But the book is 50 years old. Are there newer books that give a better and more comprehensive treatment of the material, or this the best of its kind?","['manifolds', 'book-recommendation', 'differential-geometry']"
3801181,Understanding subset terminology,"I'm having a hard time wrapping my head around subsets/element problems. For example: Is $\{2\} \subseteq \{2,3\}$ ? I thought it was because every element in the left set is also in the set on the right. Furthermore is $\{2\} \subseteq \mathcal{P}(\{1,2\})$ ? I also thought this was true because the set on the left contains one or more of the elements of the set on the right.","['elementary-set-theory', 'logic']"
3801182,Median of triangle and tangents,"Let $AD$ be an altitude of any triangle $ABC$ . Consider a circle with $AD$ as its diameter, cutting $AB$ and $AC$ at $P$ and $Q$ respectively. Let the tangents at $P$ and $Q$ meet at $X$ . Prove that $AX$ bisects $BC$ . I have attempted the proof, using angles in the alternate segment, and Apollonius’ Theorem, but to no avail. Are there some other theorems that I should be using?","['euclidean-geometry', 'geometry']"
