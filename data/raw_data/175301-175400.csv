question_id,title,body,tags
3136107,Is the additive group of integers a rational group?,"A group $\mathbb{G}$ is called rational ( https://groupprops.subwiki.org/wiki/Rational_group ) if $g,g' \in \mathbb{G}, \langle g \rangle = \langle g' \rangle \Rightarrow \exists x \in \mathbb{G}: xgx^{-1} = g'$ . I followed, that $\mathbb{Z}$ is not rational, because e.g. $\langle 2 \rangle = \langle -2 \rangle$ , but $x+2+(-x)=2 \neq -2$ . On the other hand, I am reading a paper, saying 'a group X is called rational if it is isomorphic to a subgroup of the group of rationals $\mathbb{Q}$ ', and this would imply that $\mathbb{Z}$ is rational. Are the defintions misleading or is something wrong with my conclusions?","['group-theory', 'definition', 'rational-numbers']"
3136158,Proving $G \ast_A$ finitely presented $\Leftrightarrow$ $A$ finitely generated,"I want to prove the HNN extension $G \ast_A$ is finitely presented $\Leftrightarrow$ $A$ is finitely generated, given that $G$ is a finitely presented group, say $G = \langle S \mid R \rangle$ The $\Leftarrow$ direction is easy I think, since if $A = \langle T \rangle$ for $T$ finite, then a finite presentation for $G \ast_A$ is given by $\langle S \cup \{ t \} \mid R \cup \{ tat^{-1} = \theta(a) : a \in T \} \rangle$ where $\theta : A \hookrightarrow G$ is the monomorphism defining the HNN extension. The other direction is where I have trouble. If I suppose $G \ast_A = \langle T \mid R' \rangle$ is finitely generated, then I know $G \leq G \ast_A$ , so I can write the $S$ generating $G$ in the $T$ generating the HNN extension and add in the $R$ relations: $G \ast_A = \langle S \cup T \mid R \cup R' \cup S_= \rangle$ where $S_=$ are the relations defining $S$ in terms of the $T$ . This is now a finite presentation that looks kind of similar to the probably infinite presentation $\langle S \cup \{ t \} \mid R \cup \{ tat^{-1} = \theta(a) : a \in A \} \rangle$ . I feel like I should do something involving Tietze transformations, but I'm not sure what. Any help proving this would be appreciated. I have seen some references to this theorem on this site e.g. Examples of non-finitely presented groups , so hopefully it is well known and easy to prove, I've just not seen how to do it.","['finitely-generated', 'group-theory']"
3136177,$d_p$ and $d_\infty$ in $\mathbb{C}^n$ are uniformly equivalent,"I need to prove this Show 
with the metrics $d_p$ and $d_{\infty}$ in $\mathbb{C}^n$ are uniformly equivalents, with $p \in [1, \infty)$ . So, I have in my book two definitions about equivalence metrics in a metric space $(X,d)$ .
1) Two metrics $d_1$ and $d_2$ are uniformly equivalents if there are constants $a, b >0$ such that $ad_1(x,y) \leq d_2(x,y) \leq bd_1(x,y), \forall x, y \in \mathbb{C}^n$ or equivalently, if $a \leq \dfrac{d_2(x,y)}{d_1(x,y)}\leq b $ for all $x \neq y$ .
2) The second definition is about topology equivalence. We say that two metrics $d_1$ and $d_2$ are topology equivalents if any sequence convergent in space $X$ with the metric $d_1$ also converge in the metric $d_2$ and for the same limit point. I have already proved two facts: a) If two metrics $d_1$ and $d_2$ are uniformly equivalent in $X$ , then a subset $M$ of $X$ is bounded with respect to the metric $d_1$ if, and only if, $M$ is bounded with respect to the metric $d_2$ . b) If two metrics are uniformly equivalent, then they are topologically equivalent. But my problem above continues. 
I could this:
By definition we know that $$
d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \mbox{e}\;\;
d_\infty (x,y) = \sup_{i=1,..., n}{ |x_i - y_i| }.
$$ So, by Minkowski's inequality, we have $$
d_p (x,y) = \left(  \sum_{i=1}^{n} \ |x_i -y_i|^{p} \right)^{1/p} \leq 
\left(  \sum_{i=1}^{n} \ |x_i|^{p} \right)^{1/p} +
\left(  \sum_{i=1}^{n} \ |y_i|^{p} \right)^{1/p} \leq M_1 + M_2 = M
$$ and $$
d_\infty = \sup_{i=1,..., n}{ |x_i - y_i| } \leq N.
$$ How $0 < d_p(x,y)$ and $0 < d_\infty (x,y)$ for all $x \neq y$ , we have with statements above that $$
0 \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)} \leq \dfrac{M}{N}=b, b>0.
$$ My problem here is how can I to prove with there is a constant positive $a$ such that $a \leq \dfrac{d_p(x,y)}{d_{\infty}(x,y)}$ .
Thanks.","['functional-analysis', 'analysis', 'metric-spaces']"
3136185,How should we think of the commutator of two permutations?,"I have a fairly good intuitive concept of what the conjugate of $x$ with $y$ does: $yxy^{-1}$ . It “applies the transformation $y$ to the transformation $x$ ” rather than to the underlying set. For example, if $y$ is 45 degree rotation of the plane, and $x$ is reflection along the vertical-axis, then the conjugate of $x$ with $y$ will give you a rotated reflection, i.e. a reflection along the diagonal. (Hence, the $y$ -conjugate operation rotates the operation $x$ itself, rather than the plane on which $x$ is acting). I do not have a similar intuitive understanding of what the commutator does in a permutation group. I went through a number of examples, and couldn’t find the common pattern. Is there a similarly intuitive concept of what the commutator does?","['permutations', 'group-theory']"
3136202,Log (uniform) continuous functions,"I am interested in functions $f: \mathbb R_+\to \mathbb R_+$ such that $\log \circ f \circ \exp$ is uniformly continuous. In other words \begin{align}
   \forall_{c}\, \exists_{c'}, \forall_{ x,x'\in[x/c',xc']}\, f(x')\in[f(x)/c, f(x)c].
\end{align} This definition includes functions such as $x\mapsto x^2$ and $x\mapsto x^{-1}$ which are not uniformly continuous. I'm wondering what the right term for such functions is? I have also looked for ""multiplicatively uniformly continuous"", also with no luck. If the range of $f$ is compact, can we say that $f^{-1}$ is also log uniformly continuous?
Can we say that if $f$ and $g$ are Log uniform continuous, then so is $fg$ ?","['continuity', 'uniform-continuity', 'analysis', 'reference-request']"
3136208,How to find infimum and supremum of an inequality?,"Given $A=\{x\in \mathbb{R}: (2x^2+x-21)(x^2+2x)<0\}$ , I want to find $\inf(A)$ and $\sup(A)$ and to if it admits minimum and/or maximum. First of all, I've solved the inequality, which gives me that for $]-2:-\frac{7}{2}[$ and $]0:3[$ is negative, whereas the other parts of the domain are positive. I think the maximum of the function, in this case, are indeed $-2;-\frac{7}{2};0;3$ , but I don't know how to find $\inf(A)$ . Moreover, how do I know if it admits minimum and/or maximum?","['calculus', 'functional-analysis', 'real-analysis']"
3136209,Shock and mass conservation law derivation (Rankine-Hugoniot),"Suppose we are looking at the non-linear system $u_t+uu_x=0$ . Some of the waves from this system are drawn below where I included $a$ and $b$ as interval-bounds around where the most happens. There is a vertical green line drawn at some point of the shock wave such that the area in yellow and orange are equal, this line is called $\sigma(t)$ . The three points where $\sigma(t)$ intersects $u(t,x)$ are $u_+(t),u_0(t)$ and $u_-(t)$ from up to down respectively. We define $$M(t)=M_{a,b}(t)=\int_a^bu(t,x)dx,$$ the area enclosed by the shock wave, the $t$ -axis and the vertival lines $x=a$ and $x=b$ ; from this $M(t)$ we have already derived a law that $$\frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right).$$ We also now define $u(0,x)=f(x)$ and $x=X(t,u)=g(u)+tu$ , so $X(0,u)=g(u)$ (and $u=u(t,x)$ ). The goal is to show that the following expression for $M(t)$ also satisfies the law for $\frac{dM}{dt}$ as written above: $$M(t)=\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b).$$ I find this latter expression for $M$ very disturbing and do not know exactly how to differentiate this w.r.t. $t$ . The Leibniz integral rule would probably be the way to go, but I do not see how we could get $\frac{dM}{dt}=\frac{1}{2}\left(u(t,a)^2-u(t,b)^2\right)$ out of this. My problem is that there is a $du$ after the integral and the Leibniz rule is for $\frac{d}{dx}\int u(x,t) dt$ or something. Suggestions and hints for a proper approach to this is very much appreciated. Thanks for the time and help! I tried, but failed miserably: $$\frac{d}{dt}\int_{u(t,b)}^{u(t,a)}\left(X(t,u)-a\right)du+(b-a)u(t,b)=\int_{\frac{du(t,b)}{dt}}^{\frac{du(t,a)}{dt}}\frac{dX(t,u)}{dt}\frac{du}{dt}+(b-a)\frac{du(t,b)}{dt}?$$ Edit:(why did I not use chain rule anywhere? I have no idea how to properly differentiate this integral.)","['integration', 'derivatives', 'wave-equation', 'partial-differential-equations']"
3136211,How do I prove/disprove this formula of infinite sums of derivatives.,"I am a first year student who wants to learn more about math, specifically how to think like a mathematicians. For that, I came up with a simple exercise to find functions f(x) that satisfy $$f(x) = \sum_{i=1}^\infty \frac{d^i}{dx^i}f(x)$$ or show that the trivial solution f(x) = 0 is the only function that satisfies the infinite sum. I have no idea or pointers on how to solve this. I don't necessarily want a solution to this, I would love to get some hints because appart from basic induction (which I learned from my first year at college) I can't come up with a tool or method that might help me to solve this puzzle (I tried Taylor expansion but it led to nothing usefull). I specifically want to grasp how mathematicians approach such questions. Where do I start, what should I apply to solve this sum, what is the standart tool to approach this, etc? Trial and error did work for f(x) = 0, but I wan't to be able to solve such tasks in a more academic sense.","['ordinary-differential-equations', 'sequences-and-series']"
3136254,Differentiability of the Schatten $p$-norm on positive definite matrices,"Let $V$ be the vector space of symmetric matrices in $\Bbb R^{n\times n}$ . For $p\in (1,\infty)$ , the Schatten $p$ -norm of $M\in V$ is defined as $\|M\|_p =(\sum_{i=1}^n \sigma_i(M)^p)^{1/p}$ where $\sigma_1(M),\ldots,\sigma_n(M)$ are the singular values of $M$ . Now, let $C\subset V$ be the cone of positive semi-definite matrices. It follows from this old post that $$\nabla \|M\|_p = \|M\|_p^{1-p}M^{p-1}\qquad \forall M\in C.$$ Where is a reference for the above statement?","['derivatives', 'linear-algebra', 'positive-semidefinite', 'reference-request']"
3136264,The number of polynomial functions $f:A\to A$ is $|A|^2$ if and only if $x^2=x$ for all $x\in A$.,"Let $A$ be a commutative ring with $n$ elements, $n\ge2$ . Prove that the next statements are equivalent: $(\forall x\in A)(x^2=x)$ . The number of polynomial functions $f:A\to A$ is $n^2$ . I managed to do only the implication $a\implies b)$ : $f(x)=a_kx^k+a_{k-1}x^{k-1}+\cdots+a_1x+a_0=(a_k+\cdots+a_1)x+a_0=bx+a_0$ $b$ and $a_0$ are arbitrary in $A$ , so the number of polynomial functions is $n^2$ . Can somebody give me some ideas for $b)\implies a)$ ?","['finite-rings', 'ring-theory', 'functions', 'polynomials', 'commutative-algebra']"
3136265,Find $\lim\limits_{n\rightarrow\infty} \sum\limits_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n}$,"Let $(a_n)_{n\geq1}$ a sequence strictly increasing of real positive numbers such that $\lim\limits_{n\to\infty} \frac{a_{n+1}}{a_n}=1$ . Find $$\lim_{n\to\infty} \sum_{k=1}^{n} \frac{a_k}{a_k+a_1+a_2+...+a_n}$$ I know this should be solved using Riemann integration, but my only significant progress was the finding of the partition $$0\leq\frac{a_1}{a_1+...+a_n}\leq\frac{a_1+a_2}{a_1+...+a_n}\leq...\leq\frac{a_1+...+a_n}{a_1+...+a_n}=1$$ for the interval $[0,1]$ .","['limits', 'sequences-and-series']"
3136272,Evaluate the limit: $\lim\limits_{n\to\infty} \frac{4^nn!}{(3n)^n}$,"Evaluate the following limit $$
\lim_{n\to\infty} \frac{4^nn!}{(3n)^n}
$$ I've shown the limit is equal to $0$ . One may for example use the ration test by which: $$
\begin{align}
\frac{x_{n+1}}{x_n} &= \left({4\over 3}\right)^{n+1}\frac{(n+1)!}{(n+1)^{n+1}} \cdot \left({3\over 4}\right)^{n}\frac{n^n}{n!}\\
&= \frac{4}{3}\frac{n^n}{(n+1)^n}
\end{align}
$$ Now taking the limit of the fraction: $$
{4\over 3}\lim_{n\to\infty} \left(n\over n+1\right)^n = {4\over 3e} < 1
$$ By this the sequence is converging to $0$ . Another way could be Stirling's approximatiom, by which: $$
\frac{4^nn!}{(3n)^n} \sim \frac{4^n}{(3n)^n}\cdot \sqrt{2\pi n}\cdot\left({n\over e}\right)^n = \left({4\over 3e}\right)^n\sqrt{2\pi n}
$$ Applying ration test after Stirling's approximation yields the same result. The problem is that Stirling's approximation has not been introduced yet. Also this limit comes right after the problems on proving some specific statements, among which are: $$
\begin{align*}
\lim_{n\to\infty} x_n = x &\implies \lim_{n\to\infty}\frac{x_1 + x_2 + \cdots +x_n}{n} = x \tag 1\\
\lim_{n\to\infty} x_n = x &\implies \lim_{n\to\infty} \sqrt[n]{x_1x_2\dots x_n} = x\tag 2\\
\lim_{n\to\infty}\frac{x_{n+1}}{x_n} = x &\implies \lim_{n\to\infty} \sqrt[n]{x_n} = x\tag 3
\end{align*}
$$ Right before the problem from question section the book is asking to find the limit of: $$
\lim_{n\to\infty} {1\over n}\sqrt[n]{(n+1)(n+2)\dots(2n)}
$$ This may be easily handled by applying $(3)$ . My assumption is that the author expects me to use one of those proofs I've done before, however I don't see how any of them may be applied. Also please note that proving Cesaro-Stolz is following this limit. I would appreciate if someone could point me to a way to use $(1), (2)$ or $(3)$ for evaluating the limit. Or possibly suggest other approaches to find that limit from question section. Thank you!","['limits', 'sequences-and-series', 'real-analysis']"
3136317,"Prove if $F\in\mathcal{A}$, then we have $\bigcap\mathcal{A}\subseteq F \subseteq \bigcup\mathcal{A}$","Prove that if $F\in\mathcal{A}$ , then we have $$\bigcap\mathcal{A}\subseteq F \subseteq \bigcup\mathcal{A}$$ . Note: $\bigcap\mathcal{A}=\bigcap_{U\in\mathcal{A}}U$ and $\bigcup\mathcal{A}=\bigcup_{U\in\mathcal{A}}U$ . My proof. Let $F\in\mathcal{A}.$ $(\subseteq)$ . Let $x\in\bigcap\mathcal{A}=\bigcap_{U\in\mathcal{A}}U$ . It means that $x\in U$ for all $U\in\mathcal{A}$ , so $x\in F$ because $F\in\mathcal{A}$ . Hence $$\bigcap\mathcal{A}\subseteq F.$$ $(\supseteq).$ $x\in\bigcup U$ iff there is a some $U\in \mathcal{A}$ such that $x\in A$ . But $F$ is an exactly such an $U$ . Can you check my proof? Thankss...","['elementary-set-theory', 'proof-verification']"
3136331,"What ""tools"" are available within pure mathematics to visualize more advanced topics? [closed]","Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 years ago . Improve this question What ""tools"" exist within mathematics to visualize concepts for more advanced areas of mathematics, particularly within Analysis, Topology and Algebra ? Furthermore, can one rigorously integrate such ""visual tools"" within proofs? As an example here are some that I've encountered so far:, I've seen commutative diagrams for compositions of maps. Another example is the following: If $X, W \subset \mathbf{R}^2$ are subsets of the plane, we can draw a picture of a map $f : X \to W$ by colouring $X$ with some pattern, and then colouring each point $(u,v)= f(x,y)$ by the same colour as $(x,y)$ as shown in the free book Stokes’s Theorem (Benjamin McKay) (on pages 3-4). Edit: By visualization, I mean anything that conveys information diagrammatically or without the use of words, which hold true for the examples I provided.","['visualization', 'soft-question', 'analysis']"
3136404,What are autonomous and non-autonomous systems??,What are autonomous and non-autonomous systems and how are they different from each other. What are the differences between the types of systems they are describing? Do both autonomous and non-autonomous system describe dynamical systems or are dynamical systems something different?,"['multivariable-calculus', 'ordinary-differential-equations', 'dynamical-systems']"
3136494,Inequality in triangle: $2\cos(A)\le\cos(B-C)$,"Let $ABC$ be an acute-angled triangle. If $A \geq B \geq C$ , show that $$2 \cos(A) \leq \cos(B-C).$$ I've tried all sorts of trigonometric relationships to get this to a nicer form, but I couldn't arrive at anything manageable. I have observed that equality is acquired when $ABC$ is equilateral. Any tips?","['trigonometry', 'geometry', 'inequality']"
3136503,How to evaluate the limit where something is raised to a power of $x$?,"I am attempting to evaluate the following limit: $$\lim_{x\to \infty} \Biggl(\frac{x+3}{x+8}\Biggl)^x$$ I was wondering if anyone could share some strategies for evaluating limits raised to a power of $x$ , as I have never encountered these before. I have found the answer to be $\frac{1}{e^5}$ , but I am unsure how to arrive at this answer.","['limits', 'calculus']"
3136509,"Is it true that $(x,y,z)=((x,y),z)=(x,(y,z))$?","Is there any difference between $\mathbb{R} \times \mathbb{R} \times \mathbb{R}$ , $\mathbb{R} \times \mathbb{R}^2$ and $\mathbb{R}^2 \times \mathbb{R}$ ? Two of them have two coordinates, a scalar and a vector. The first one is clearly the 3-dimensional space $\mathbb{R}^3$ . But what about the other two? Is there any topological difference?","['elementary-set-theory', 'coordinate-systems', 'general-topology']"
3136552,Prove that $X\times Y=\emptyset$ $\iff$ $X=\emptyset$ or $Y=\emptyset$.,"I am looking for verification of an attempt that I have made to prove the following claim: Prove that $X\times Y=\emptyset$ $\iff$ $X=\emptyset$ or $Y=\emptyset$ . Proof I will prove the contrapositive. LHS: Assume $X\neq\emptyset$ and $Y\neq\emptyset$ , so there is $a\in X$ and $b\in Y$ such that $(a,b)\in X\times Y$ . Thus, $X\times Y\neq\emptyset.$ RHS: Assume $X\times Y\neq\emptyset.$ Then, there is a $(a,b)\in X\times Y$ such that $a\in X$ and $b\in Y$ , hence $X\neq \emptyset$ and $Y\neq\emptyset$ . $\quad \Box$ Can you check and critique my proof attempt above? I would be grateful for any feedback or other ways in which the problem could be appraoched.
Thanks","['elementary-set-theory', 'proof-writing', 'solution-verification']"
3136569,Levi-Civita & Kronecker delta identity,"I am trying to prove the following identity: $\epsilon^{ijk}\epsilon_{pqk}=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i$ Starting from the following identity: $\epsilon^{ijk}\epsilon_{pqr}=\begin{vmatrix}
\delta^i_p & \delta^i_q & \delta^i_r \\
\delta^j_p & \delta^j_q & \delta^j_r \\
\delta^k_p & \delta^k_q & \delta^k_r \\
\end{vmatrix}$ But, when I expand out the matrix, I end up with: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_r^k-\delta_q^k\delta_r^j)-\delta_q^i(\delta_p^j\delta_r^k-\delta_p^k\delta_r^j)+\delta_r^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j)$ Contracting by setting r=k, I obtained: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j\delta_k^k-\delta_q^k\delta_k^j)-\delta_q^i(\delta_p^j\delta_k^k-\delta_p^k\delta_k^j)+\delta_k^i(\delta_p^j\delta_q^k-\delta_p^k\delta_q^j)$ Then: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(\delta_q^j-\delta_q^j)-\delta_q^i(\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j)$ So finally: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^j\delta_q^i-\delta_p^i\delta_q^j$ Which is the identity I'm trying to prove, except that the right side is multiplied by -1 for some reason. I must be doing something wrong along the way, but I haven't been able to find it. Any ideas? EDIT: A similar question has been answered elsewhere , but it is only when expanding the determinant that I arrive at this problem. For that reason, I decided to make a separate post about this. EDIT 2: Added intermediate steps. FINAL EDIT: As Travis pointed out, I incorrectly replaced $\delta_k^k$ by $1$ instead of $3$ . After correcting this, I arrived at the correct solution: $\epsilon^{ijk}\epsilon_{pqr}=\delta_p^i(3\delta_q^j-\delta_q^j)-\delta_q^i(3\delta_p^j-\delta_p^j)+(\delta_p^j\delta_q^i-\delta_p^i\delta_q^j)=\delta_p^i\delta_q^j-\delta_p^j\delta_q^i$","['tensors', 'linear-algebra']"
3136570,"No non-negative continuous function on $[a,b]$ such that $\int_a^b f(t)dt=1, \int_a^b tf(t)dt=c, \int_a^b t^2f(t)dt=c^2$ for $c\in\mathbb{R}$.","Show that there exists no non-negative continuous function $f$ defined on the interval $[a,b]$ such that it satisfies the following conditions: $$\int_a^b f(t)dt=1 \quad \int_a^b tf(t)dt = c \quad \int_a^b t^2f(t)dt = c^2,$$ for some $c\in\mathbb{R}$ . I was given a hint that I need to use Cauchy-Schwarz inequality, and I am familiar with Cauchy-Schwarz, I just do not see how to apply it to this problem. Cauchy-Schwarz inequality states that if $u$ and $v$ are elements of an inner product space then, $\| \langle u,v \rangle \| \leq \| u \| \| v \|$ . So I guess here I have the inner product space of $C([a,b])$ , continuous functions on $[a,b]$ , and I have that $$\langle f, 1 \rangle = 1 \quad \langle f,t\rangle = c \quad \langle f,t^2 \rangle = c^2$$ I don't know how to piece it together though. Any help would be appreciated!","['hilbert-spaces', 'functional-analysis', 'analysis', 'real-analysis']"
3136603,Convergence of probability measure,"Let $X_1,X_2,...$ be iid random variables with density $(1-\cos x)/\pi x^2$ . How do we show that $$\lim\limits_{n\to\infty}\mathbb{P}\left(\frac{X_1+...+X_n}{n}\leq x\right)=\frac{1}{2}+\pi^{-1}\arctan x?$$ Maybe using the characteristic functions? Then $$\phi_X(u)=\int e^{iux}(1-\cos x)/\pi x^2dx.$$ But how do we proceed?","['measure-theory', 'convergence-divergence', 'probability']"
3136637,Showing injective property of derivative map over vector space of polynomials,"From S.L Linear Algebra: (1) Let $P_n$ be the vector space of polynomials of degree $ \leq n$ . (2) Then
  the derivative $D: P_n \rightarrow P_n$ is a linear map of $P_n$ into
  itself. (3) Let $I$ be the identity mapping. Prove that the following linear maps are invertible: (a) $I - D^2$ ... My observation (long): (1) I find a first sentence very interesting: Let $P_n$ be the vector space of polynomials of degree $ \leq n$ . But in order for $P_n$ to be a vector space, it must contain a zero vector, which in vector space of polynomials is given by a zero polynomial which has a very confusing degree . But it is argued that it makes most sense for zero polynomial to have $-\infty$ degree. Hence wouldn't it be more precise to say that $P_n$ is vector space of polynomials that have a degree of $\leq -\infty$ ? In which case $n= -\infty$ ? I don't believe $n$ is associated in any way with dimension of vector space, because in this case this assertion would break rule of cardinality. (2) Then the derivative $D: P_n \rightarrow P_n$ is a linear map of $P_n$ into
  itself. This is another interesting assertion. We know that exponent property of derivative will change degree of $p_g \in P_n$ to $g-1$ . In this case, if our polynomial is something like $x^{-\infty + 1}$ , we would get a derivative $\frac{d}{dx} x^{-\infty + 1} = -\infty x^{-\infty}$ which suggests that kernel is not trivial for $D$ and hence our linear derivative map over $P_n$ to $P_n$ is not injective ... And therefore not invertible. I definitely am wrong with this assertion, since it is assumed that $D$ is invertible over $P_n \rightarrow P_n$ (3) Let $I$ be the identity mapping. It is very easy to prove that identity map is both injective and surjective therefore invertible. Since considering that: $I(v) = v$ We can easily see that $Im(I) = P_n$ and $Ker(I) = {0}$ . Polynomials being scalars : Considering that polynomials are scalars, isn't basis of vector space zero-dimensional? Therefore by rank-nullity theorem: $$\textrm{dim} \, P_n = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n)$$ $$0 = \textrm{dim} \, Im(P_n) + \textrm{dim}  \, Ker(P_n)$$ $$0 = 0 + 0$$ Therefore by this assumption, kernel is trivial and linear map must be invertible. Question: In this case, I'm trying to find that there exists a matrix $A$ such that: $$(I - D^2)A = I$$ I know that identity map is invertible (as mentioned in my observation), but I'm not so sure about $D^2$ . In fact I'm unable to find a matrix associated with $D$ in order to prove my assertion. What could be the simple solution? Is my observation very incorrect? Thank you!","['matrices', 'derivatives', 'linear-algebra', 'polynomials']"
3136642,Why can't I link these three loops together?,"Today is Mardi Gras, so I decided to try to connect two of my friend's drawer handles together with a necklace. It is easy to connect the necklace to one drawer handle. (See picture for what I mean by ""connect."") However, at this stage I am unable to connect the necklace to a second drawer handle. Is this impossible? My thinking is that the necklace and the two handles represent a total of 3 loops in $\mathbb R^3$ . If you can connect two of these loops together, why can't you do the same procedure to connect the third one? Would the resulting configuration be a nontrivial link?","['knot-theory', 'general-topology']"
3136755,Notation: Using $\prod A_i$ for noncommuting $A_i$,"I am writing up some math and realized that I do not know whether $\prod_{i=1}^m A_i=A_mA_{m-1}\cdots A_1$ , OR whether $\prod_{i=1}^m A_i=A_1A_2\cdots A_m$ . Is there accepted consensus on this ordering?","['notation', 'algebra-precalculus', 'products']"
3136763,Example of two homotopically equivalent manifolds such that one admits a symplectic structure and the other does not,"A smooth manifold $M$ admits a symplectic structure if there is an alternating
non degenerate $2$ -form $\omega \in \Lambda^2(M)$ that is also closed i.e. $d\omega = 0.$ Usually we can express obstructions to the existence of certain tensors in terms of vanishing of some cohomology classes.
On the other hand, due to the ""integrability"" condition $d\omega = 0$ one should expect that a set of necessary and sufficient condition for $M$ to admit a symplectic structure cannot be expressed just in homological terms. In order to support this guess I  am thus looking for  a concrete example: Find $M_1, M_2 $ (possibly) compact smooth manifolds (of the same dimension) such that $M_1$ is symplectic $M_2$ does not admit a symplectic structure $M_1$ is homotopically equivalent to $M_2$","['symplectic-geometry', 'smooth-manifolds', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3136778,Proving a function is a fixed point,"I'm taking a class in University which involves proving the correctness of computer programs and I'm really bad a proofs, I don't really understand them at all. Can anyone tell me if my proof actually makes any sense, I frequently think I've written a proof but usually I've proved nothing. Given this (T1 is a function that takes another function (F) as one of its parameters): T1(F,x,y) == if y = 0 then x else F(x−1,y−1) I had to suggest a fixed point solution, I suggested: f1(x,y) == x - y Because T1 is a recursive function that implements subtraction. And below I had to prove my suggestion was correct. To show T1(f1) = f1

T1(f1, x, y) == if y = 0 then x else f1(x−1,y−1)

if y = 0 then x
else (x-1) - (y-1)

if y = 0 then x
else x - y - 2

x - y - 2 = f1(x-1,y-1)

x - y = f1(x,y) Does my proof have some obvious hole in it that I'm not seeing or is there some mistake I can't see, I think it works but I'm not really sure why it proves anything?","['proof-writing', 'functions', 'fixed-point-theorems']"
3136795,Interpreting Doob's Up-Crossing Inequality,"Doob's upcrosssing inequality states that, for martingale $X_n$ , if $U_n(a,b)$ counts the number of upcrossings through $(a,b)$ up to time $n$ , then $E[U_n(a,b)]\leq \frac{1}{b-a}E[(a-X_n)_+]$ . We get this result by looking at stopping times that correspond to each up-crossing: namely, $\tau_{2k-1}$ corresponds to the beginning of the $k$ th up-crossing, where $X_{\tau_{2k-1}} \leq a$ and $\tau_{2k}$ corresponds to the end of that up-crossing, so that it's the first time after $\tau_{2k-1}$ where $X_{\tau_{2k}}\geq b$ (and we cap all the stopping times by time $n$ ). If we sum up the corresponding jumps: $\sum_k X_{\tau_{2k}}-X_{\tau_{2k-1}}$ , then of course by design, we will have $U_n(a,b)$ jumps of size at least $b-a$ . We might additionally get a ""partial jump"" at the end, where $\tau_{2k-1}<n$ but we get to $n$ before the system makes it past $b$ . Taking expectations, we see that by Doob's optional stopping theorem, each of the full jumps has expectation zero, so we end up with a bound that's only dependent on the potential partial jump. I'm eliding some details here, since they  don't pertain directly to my question, but this is the general idea. My question is really about intuitions. There's something very weird about the fact that we designed our stopping times specifically so that the jumps $X_{\tau_{2k}}-X_{\tau_{2k-1}}$ are of size at least b-a, but in expectation they are of size zero. This feels very odd to me. One explanation might be that really what this saying is that actually in expectation these jumps just don't occur, but if they don't occur in expectation, then shouldn't our expected number of up-crossings be zero, since that's precisely what that statement means? Is there a better way to understand how jumps that are certainly positive end up being zero in expectation? I really want an intuitions-based answer; I understand the application of the optional stopping theorem, I'm just struggling to make sense of what is happening morally.","['martingales', 'probability-theory']"
3136831,"A finite dimensional extension of a field is direct product of local rings, the Fulton way.","Theorem. If $k$ is a field, $R$ a ring containing $k$ as a subring and $R$ finite dimensional over $k$ , then $R$ is isomorphic to a direct product of local rings. I know the usual proof using the Chinese remainder theorem, but I want to understand the proof by Fulton in his 'Algebraic curves', only valid for algebraically closed $k$ . Fulton shows in an entertaining way that, if $k$ is algebraically closed and if $I$ is an ideal in $k[X_1, \ldots, X_n]$ with finite vanishing set $V(I)=\left \{  P \in k^n \mid F(P)=0, 
 \forall F\in I \right \}$ then $k[X_1, \ldots, X_n]/I$ is isomorphic to a direct product of local rings. Then he claims that the above Theorem can be deduced. I know how to get $R$ as a quotient $k[X_1, \ldots, X_n]/I$ , using the fact that $R$ is finite dimensional over $k$ , but I don't know how to assure that $I$ has a finite vanishing set. Any ideas?","['algebraic-geometry', 'abstract-algebra']"
3136877,Stopped random walk is not uniformly integrable,"I know that in general Doob's Optional Stopping Theorem doesn't hold for unbounded stopping times, but that it does when the system up to the stopping time is uniformly integrable. One counter example in the unbounded stopping time case is given by $X_n=\eta_1+\eta_2+\cdots+\eta_n$ , where the $\eta_i$ are i.i.d. and take values $\pm 1$ each with probability 1/2. Let $\tau = \inf\{n: X_n=1\}$ . In this case, $E[X_\tau] = 1\neq 0 = E[X_0]$ . However, I'm struggling to see why the martingale in this case is not UI. I have some intuition here: even though our stopping time is a.s. finite, it isn't uniformly bounded in $\omega$ , so for any $M$ there are always a nontrivial set of paths $\omega$ for which $\tau>M$ . This gives us more time and opportunity to have $X_n$ with nonzero probability of having $|X_n|>M$ before time $\tau$ (clearly, not possibly when $n<M$ ). Still I'd like to see a more formal run-down of what's going on here, and why this case is not uniformly integrable, if possible.","['random-walk', 'uniform-integrability', 'martingales', 'stopping-times', 'probability-theory']"
3136889,"General expression for $n$-th derivative of $x^{\alpha}$, $\alpha > 0$.","Assume $\alpha > 0$ is fixed, but not necessarily integer. I'm looking for a general expression of $$A(n)=\frac{d^n}{d x^n} x^{\alpha} \tag{1}$$ If I'm not messing anything up, with $\alpha > n$ , $A(n)=(\alpha)_{n}~x^{\alpha - n} = \frac{\Gamma(\alpha +1)}{\Gamma(n + 1)}x^{\alpha-n}$ . On the other hand, $\alpha < n$ , it seems that at a certain point $t$ , such that $\alpha - t \geq 0 \geq \alpha - t - 1$ , a $(-1)^j$ term pops up for the remaining $n-t$ terms. 
Here for me it's not clear whether we can still use a similar expression through $\Gamma$ functions, and how would it look like? My question: is there a general expression of (1) for any $\alpha$ and $n$ ? Sorry if this is a trivial question. I think I could work out the expressions for all 3 cases (the two mentioned and one with $\alpha$ being an integer), but it somehow feels like there should be a general expression, something involving $\Gamma(n - \alpha ) / \Gamma(n - 1)$ , but can't seem to reach it.","['calculus', 'derivatives', 'real-analysis']"
3136903,"Continuous function and limit $ f(x,y)=\left\{\begin{matrix} (x-y)\sin\frac{1}{x}\sin\frac{1}{y} & , xy\neq 0 \\ 0 &, x=y=0 \end{matrix}\right. $","I have this function: $$ f(x,y)=\left\{\begin{matrix}
(x-y)\sin\frac{1}{x}\sin\frac{1}{y} & ,  xy\neq 0 \\ 0
 &, x=y=0
\end{matrix}\right. $$ a) Show that $ \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)] $ does not exist. b) Show that $ f(x,y) $ is continuous at $ (0,0) $ . For (a) I took $ \lim_{y\rightarrow 0} f(x,y) $ and I got that it's equal to $$ x\sin\frac{1}{x}\lim_{y\rightarrow 0}\sin\frac{1}{y} - \sin\frac{1}{x}\lim_{y\rightarrow 0}y \sin\frac{1}{y} $$ , but $$ \lim_{y\rightarrow 0}\sin\frac{1}{y} $$ does not exist, so $ \lim_{y\rightarrow 0} f(x,y) $ does not exist and $ \lim_{x\rightarrow 0}[\lim_{y\rightarrow 0} f(x,y)] $ does not exist. Am I right? For (b) I guess that I have to show that $$ \lim_{(x,y)\rightarrow (0,0)} f(x,y) = 0 $$ How can I do this ?","['continued-fractions', 'limits', 'trigonometry', 'functions']"
3136914,Picard Group of a Blowup,Let $\pi : \tilde{\mathbb{P}^{3}} \longrightarrow \mathbb{P}^{3}$ be the blowup of $\mathbb{P}^{3}$ along a smooth algebraic curve $C$ with exceptional divisor $E$ We know that $\text{Pic}(\mathbb{P}^{3}) \simeq \mathbb{Z}$ . How do I determine the Picard Group of $\tilde{\mathbb{P}^{3}}$ ? I thank you for your help.,['algebraic-geometry']
3136956,"For any eight points on an equilateral triangle of side $1$, there's at least one pair of those points at most $1/3$ apart","Let's say we have an equilateral triangle of side length $1$ . Show that for any configuration of eight points on this triangle (on the sides or in the interior), there is at least one pair of from these eight points such that the distance between the two points in the pair is less than or equal to $1/3$ cm. My idea was to draw out the triangle and put three points on each edge, such that each point lies on the trisection of a side, but I'm not quite sure if this is rigorous enough. Also, as an extension question, can we say anything for the more generalized version of this problem with an equilateral triangle of side length $n$ and $m$ points being chosen?","['pigeonhole-principle', 'geometry']"
3136958,Showing that a recurrence relation is increasing,"Suppose $(x_n)_{n\in\mathbb{N}}\in\mathbb{R}^{\mathbb{N}}$ satisfies $x_{n+1}=\sqrt{x_n+1}-1$ and that for all $n\in\mathbb{N}$ , $x_n\in(-1,0).$ How would I show that the sequence described by this recurrence relation is increasing?","['discrete-mathematics', 'sequences-and-series']"
3136961,PDF of $Y=g(X)$ when $X$ is absolutely continuous and $g$ is strictly increasing and continuously differentiable,"Let $X$ be an absolutely continuous random variable with density function $f_{X}$ . Suppose that $g$ is strictly increasing and continuously differentiable. Derive the PDF of the random variable $Y=g(X).$ It is given as hint to find the CDF of $Y$ first and then differentiate it to find the PDF. So I try that for a start \begin{align*}
F_{Y}(y) = \mathsf{P}(Y\leq y) &=\mathsf{P}(g(X)\leq y) \\
& = \mathsf P(X \leq g^{-1}(y)) \\
& = F_{X}(g^{-1}(y)).
\end{align*} And differentiating with respect to $y$ on both sides yields \begin{align*}
f_{Y}(y)=\frac{d}{dy}F_{Y}(y) &=\frac{d}{dy}F_{X}(g^{-1}(y)) \\
&=f_{X}(g^{-1}(y))\cdot\frac{1}{g'(y)},
\end{align*} where in the final step I used the inverse function theorem for $\mathbb{R}$ . Is this the right approach to the problem? If it is not, could you please provide a hint to put me in the right direction? Any feedback is much appreciated. Thank you for your time.","['probability-distributions', 'probability-theory', 'probability']"
3136994,Space of lipschitz functions form a Banach space,"Let $X$ be a Banach space. Show that $L=\{f:X\to\mathbb{R}: f \mbox{ is Lipschitz}, f(0) = 0\}$ with the norm $$||f||_{Lip_0} = \sup\left\{\frac{|f(x)-f(y)|}{||x-y||}, x\neq y\in X\right\}$$ is a Banach space. I've found Banach space of p-Lipschitz functions but I did not understand the proof given. I have a few questions first. Which norm is $||x-y||$ ? So I need to prove that every Cauchy sequence in $L$ converges to an element of $L$ , right? In other words, $\forall \epsilon>0$ there exists $n_0$ such that $m,n>n_0\implies ||f_m-f_n||_{Lip_0}<\epsilon$ $$ ||f_m-f_n||_{Lip_0} = \sup\left\{\frac{|(f_m-f_n)(x)-(f_m-f_n)(y)|}{||x-y||}, x\neq y\in X\right\} = \sup\left\{\frac{|f_m(x)-f_m(y)|}{||x-y||}+\frac{f_n(y)-f_n(x)}{||x-y||}, x\neq y\in X\right\}$$ both $f_m$ and $f_n$ are Lipschitz so they're continous, which means something I don't know what.","['banach-spaces', 'functional-analysis', 'metric-spaces']"
3137004,Calculating Hyperbolic Sin faster than using a standard power series,"Using $$ \sinh x = x + \tfrac{x^3}{3!}+ \tfrac{x^5}{5!} + \tfrac{x^7}{7!}+ \cdots$$ as the Standard Power Series. This series takes a very long time to run. Can it be written  without using the exponentials divided by a huge factorial. The example functions in Is there a way to get trig functions without a calculator? using the ""Tailored Taylor"" series representation for sin and cosine are very fast and give the same answers.  I want to use it within my calculator program. Thank you very much.","['trigonometry', 'sequences-and-series']"
3137015,Formal way of writing the Binomial Expansion for Derivatives,"I'm currently in an Advanced Calculus class where we are currently talking about derivatives of functions of several variables, and I came across the following question: Find the tenth differential, $d^{10}f$ , of $f(x,y)=\ln(x+y)$ . After finding a few of the early differentials, I noticed that they followed a similar pattern of the Binomial Theorem for polynomials, e.g., $$d^4f=\frac{-(3!)}{(x+y)^4}(dx^4+4\,dx^3dy+6\,dx^2dy^2+4dx\,dy^3+dy^4),\tag{$\star$}$$ where everything in the parentheses is the fourth row of Pascal's Triangle. So my question is: $\textbf{Is there a formal shorthand for writing such differentials?}$ Am I okay to write my fourth differential as I did in $(\star)$ ? Could I write an arbitrary $n$ th differential as $\frac{(-1)^{n+1}(n-1)!}{(x+y)^n} \sum\limits_{k=0}^n\!{n \choose k}dx^{n-k}\,dy^{k}$ ? (E.g., $(\star)$ could be written $\frac{(-1)^{5}(3)!}{(x+y)^4} \sum\limits_{k=0}^4\!{4 \choose k}dx^{4-k}\,dy^{k}$ ) Or are both just informal nonsense? Note: [I am assuming that $dx\,dy=dy\,dx$ since $x+y=0$ is excluded in the domain of $f$ , which means all partial derivatives will be continuous on the same domain.]","['analysis', 'calculus', 'combinatorics', 'partial-derivative', 'derivatives']"
3137017,How do you calculate the weighting of a point inside of an equilateral triangle compared to its vertices?,"In an equilateral triangle that contains a point, how do you calculate 3 weights that sum to 100% and indicate how much influence each vertex has on the point. When the point is in the center all the weights are 33%: And if it's on one edge they should be split between the vertices that share that edge: This is similar to how an HSL color wheel works:","['triangles', 'geometry']"
3137113,How to transfer a metric to the orthonormal coordinate?,"Suppose in Cartesian coordinate system a Minkowski metric for flat spacetime can be written as : $$ ds^2 = -[1 + 2ψ(t,x)]dt^2 + a^2(t) [1 - 2ψ(t,x)]dx^2 $$ This is a diagonal metric. How can I transform this metric to the orthonormal frame? For that case, what will be the components of this metric in the orthonormal frame?","['coordinate-systems', 'tensors', 'matrices', 'general-relativity', 'differential-geometry']"
3137124,Open sets in $\mathbb{R}^d$ written as union of partially open cubes,"We know that an open set in $\mathbb{R}^d$ can be written as union of disjoint closed cubes ( here for example).
In Zygmund's book it is left to reader the fact that the same open set can be written as a countable union of disjoint partly open cubes . I don't know how to proceed in this case. I believe one should follow the idea in 1-dimensional case, taking intervals like $$ [a_1,b_1) \cup [a_2,b_2) \cup \ldots$$ How does the proof linked above change? Thanks in advance!!","['general-topology', 'measure-theory', 'real-analysis']"
3137140,"$\lim_{x\to 2}f(x)$ (if it exists), where $f(x)=\begin{cases}x-[x],&x<2\\4,&x=2\\3x-5,&x>2\end{cases}$","Question : Evaluate $\lim_{x\to 2}f(x)$ (if it exists), where $f(x)=\begin{cases}x-[x],&x<2\\[2ex]4,&x=2\\[2ex]3x-5,&x>2\end{cases}$ [.] denotes the greatest integer function . My attempt: $$\lim_{x \to 2^+} f(x) = 3(2)-5=1$$ $$\lim_{x \to 2^-} f(x) = x - [x] = 2-[2]$$ From this step onwards I am stuck and do not know how to continue. Please help. Thank you!","['limits', 'calculus']"
3137170,Show that $\max_{1\leq i\leq n}|X_i-\bar{X}| < \frac{(n-1)S}{\sqrt{n}}$,"Let $X_1, X_2,...,X_n$ be sample from some population. Show that $$\max_{1\leq i\leq n}|X_i-\bar{X}| < \dfrac{(n-1)S}{\sqrt{n}}$$ Simplifying the inequality, I found, to prove: $$n\max_{1\leq i\leq n}(X_i-\bar{X})^2 < (n-1)\sum_{i=1}^n (X_i-\bar{X})^2$$ or more generally, we need to prove, $n\cdot\max_{1\leq i\leq n}Y_i^2 < (n-1) \sum_{i=1}^n Y_i^2$ I am stuck here. Any help appreciated.","['statistics', 'inequality']"
3137203,Integral ${\frac{1}{\pi^2}} \int_{0}^{\infty}\frac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x$,$${\dfrac{1}{\pi^2}}\int_{0}^{\infty}\dfrac{{(\ln{x}})^2}{\sqrt{x}{(1-x)^2}} \mathrm d x$$ I tried substituting $1/x$ for $x$ but the the only change in the integral is that the $\sqrt{x}$ moves in the numerator from the denominator. I don't understand what to substitute. $\tan{x}$ doesn't seem to work.,"['integration', 'calculus', 'improper-integrals']"
3137257,A functional equation defined on the real axis.,"$$
f: \mathbb{R} \to \mathbb{R}\qquad \frac{f(x+y)}{x+y} = \frac{f(x)-f(y)}{x-y}, \qquad \forall x,y\in \mathbb{R}, \left|x\right| \neq \left|y\right|
$$ Can I prove anything interesting about this function? I need to find it.","['functional-equations', 'functions']"
3137269,Is there a sequence of natural numbers with these properties?,Is there a sequence of natural numbers (except 1) which every consecutive term is $\textbf{not}$ relatively prime and each natural number appears $\textbf{exactly once}$ ?,"['elementary-number-theory', 'sequences-and-series']"
3137277,Why do I only get one answer when evaluating $x^2=2x$?,"What this question is not about I know to expect two answers $\{2,0\}$ , (I can test them, by plugging them into the equation). I know how to get them graphically. I know how to get the algebraically: $x^2 = 2x$ $x^2 - 2x = 0$ $(x-2)(x-0)=0$ $(x-2)(x)=0$ $x=0, x=2$ However My first attempt got only one answer, and I see nothing else wrong with it. $x^2 = 2x$ $x = 2$ (by dividing by $x$ ) What am I doing wrong? How can I detect this error early? I can catch this error at the end, as I expect two answers. In this case there is little difference between the point of error and the end, but in more complex problems there may be a long time between them, so I would like to catch this earlier. How do I know where I went wrong?","['algebra-precalculus', 'roots', 'polynomials']"
3137282,A Chernoff bound variation,"I see the following variation of Chernoff Bound in one of the papers of a very good conference. The Chernoff Bound is given in the paper is as follows: Let $X_1,..., X_T$ be independent random variables with $E[X_i] = p_i$ and $X_i \in [0, 1]$ . Let $X = \sum_{i=1}^T X_i$ and $\mu = \sum_{i=1}^T p_i = E[X]$ . Then, for all $\delta\geq 0$ $P[X \geq (1 + \delta)\mu] \leq e^{-\frac{\delta^2}{2+\delta}\mu}$ (1) $P[X \leq (1-\delta)\mu] \leq e^{-\frac{\delta^2}{2+\delta}\mu}$ (2) I do not have a problem with (1) but I have not seen (2) before. Is this one of the variations of the Chernoff bound?","['probability-theory', 'upper-lower-bounds']"
3137295,Calculate the sum $S_n = \sum\limits_{k=1}^{\infty}\left\lfloor \frac{n}{2^k} + \frac{1}{2}\right\rfloor $,"I am doing tasks from Concrete Mathematics by Knuth, Graham, Patashnik for trainning, but there are a lot of really tricky sums like that: Calculate sum $$S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor  $$ My idea I had the idea to check when $$\frac{n}{2^k} < \frac{1}{2}$$ because then $$ \forall_{k_0 \le k} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor=0$$ It should be $$ k_0 = \log_2(2n) $$ but I don't know how it helps me with this task (because I need not only ""stop moment"" but also sum of considered elements Book idea Let $$S_n = \sum_{k=1}^{\infty} \left\lfloor \frac{n}{2^k} + \frac{1}{2} \right\rfloor $$ then $$ S_n-S_{n-1} = 1$$ and then solve this recursion. But I write $S_n - S_{n-1}$ and  I don't see how it can be $1$ , especially that is an infinite sum.","['summation', 'ceiling-and-floor-functions', 'discrete-mathematics', 'real-analysis']"
3137319,How in general does one construct a cycle graph for a group?,"I think I know how to interpret a cycle graph for a group, but I don’t know how to construct one. In particular, I don’t know a general rule of how to find the “basic elements” which to take the powers of. Is there a general algorithm for constructing the cycle graph of a finite group, which always ensures that you get the correct unique cycle graph?","['graph-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
3137341,Proving there is no plane tangent to a graph,"Define $$f(x, y) = \begin{cases} 
\sin(y^2/x)\sqrt{x^2 + y^2} & \text{ if } x \neq 0 \\
0 & \text{ if } x = 0.
\end{cases}$$ (a) Show $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ has directional derivatives in every direction at $(0, 0)$ . (b) Show there is no plane that is tangent to the graph of $f : \mathbb{R}^{2} \rightarrow \mathbb{R}$ at the point $(0, 0, f(0,0))$ . Geometrically, part $(b)$ makes sense because it's a really sharp point (but still continuous), meaning that there should be no tangent plane there. I don't fully understand how I'm supposed to show there's no tangent at $(0, 0, f(0,0))$ though. Isn't this a point in $\mathbb{R}^{3}$ ? And we have a function in $\mathbb{R}^{2}$ ? I'm also not too sure about how to do $(a)$ . I know how to compute the directional derivatives. I don't think that's enough to show existence, though. I'm guessing that it's going to be some sort of limit. I would really appreciate some sort of help with this exercise.","['multivariable-calculus', 'derivatives', 'tangent-line', 'real-analysis']"
3137362,Integrate a top form over a surface without partition of unity,"Suppose we are given a compact Riemann surface $M$ , an open cover $\mathscr{U}=\{U_1,U_2,\dots\}$ of $M$ , charts $\{(U_1,\phi_1),(U_2,\phi_2),\dots\}$ , holomorphic coordinates, $\phi_m:p\in U_m\mapsto z_m$ , holomorphic transition functions $z_m=f_{mn}(z_n)$ on patch overlaps $U_m\cap U_n$ (with appropriate cocycle relations, $f_{mn}\circ f_{n\ell}\circ f_{\ell m}=1$ on triple overlaps $U_m\cap U_n\cap U_\ell$ ), and a globally defined top form, $\omega=\omega(z,\bar{z})dz\wedge d\bar{z}$ . I want to integrate $\omega$ over $M$ , $$
I= \int_M\omega,
$$ using the above data (without using a partition of unity), and I'm wondering whether my reasoning is correct. In particular, suppose we construct non-overlapping sets $\{V_1,V_2,\dots\}$ such as those shown in the figure: My question is whether I can compute $I$ via a sum of integrals over the $\{V_1,V_2,\dots\}$ (to guarantee no overcounting) without using a partition of unity; i.e., is it true that: \begin{equation}
\begin{aligned}
I &= \sum_m\int_{V_m}\omega\\
&=\sum_m\int_{V_m}\omega_m(z_m,\bar{z}_m)dz_m\wedge d\bar{z}_m
\end{aligned}
\end{equation} In the examples I have considered this gives the right answer but I would like to know if it is true in general. Thanks!","['riemann-surfaces', 'riemannian-geometry', 'complex-geometry', 'manifolds', 'differential-geometry']"
3137364,How to prove this subsets of $\mathbb{R}^n$ are homeomorphic to $\mathbb{R}^n$?,"Let $f:\mathbb{R}^n\to\mathbb{R}$ be a nonzero linear map. Let $\alpha \in \mathbb{R}$ e define $A=\{x\in \mathbb{R}^n:f(x)<\alpha\}$ . By continuity of $f$ , $A$ is open in $\mathbb{R}^n$ . How can I prove (rigorously) that $A$ and $\mathbb{R}^n$ are homeomorphic ? I have some thoughts but I'm not able to make them rigorous. Intuitively I see that $\{x\in \mathbb{R}^n:f(x)=\alpha\}$ is an hyperplane in $\mathbb{R}^n$ which divides $\mathbb{R}^n$ into to ""open rectangles"" homeomorphics between them, and one of this ""rectangles"" is $A$ . For example, if $n=2$ , I think I can show that $A$ is homeomorphic with $\mathbb{R}\times(-\infty,\alpha)$ maybe using some rotation, and then I easily can show that $\mathbb{R}\times(-\infty,\alpha)$ is homeomrphic to $\mathbb{R}^2$ since I know that $(-\infty,\alpha)$ is homeomrphic to $\mathbb{R}$ . For example if $(a,b)\in \mathbb{R}^2$ is the vector such that $f(x,y)=ax+by$ , such homeomorphism could be the linear map $\mathbb{R}\times(-\infty,\alpha)\to A$ represented by the matrix \begin{pmatrix}
\cos(b/a)& -\sin(b/a)
\\\sin(p/a) & \cos(b/a)
\end{pmatrix} What is a rigorous (and possibly elegant) way to prove that $A$ and $\mathbb{R}^n$ are homeomorphic for general $n$ ? Suppose also that $\beta\in \mathbb{R}$ and $\beta<\alpha$ . Let $B=\{x\in \mathbb{R}^n:f(x)>\beta\}$ . How can I prove that $A \cap B$ is non empty and homoemorphic to $\mathbb{R}^n$ ?","['multivariable-calculus', 'calculus', 'general-topology']"
3137417,Find range of $x$ satisfying $\left \lfloor \frac{3}{x} \right \rfloor+\left \lfloor \frac{4}{x} \right \rfloor=5$,"Find range of $x$ satisfying $$\left \lfloor \frac{3}{x} \right \rfloor +\left \lfloor \frac{4}{x} \right \rfloor=5$$ Where $\lfloor\cdot\rfloor$ is the floor function My try: As far as domain of LHS is concerned we have $x \ne 0$ and since RHS is positive, we have $x \gt 0$ Now since LHS is sum of two positive integers, let us suppose: $$\left \lfloor \frac{3}{x} \right \rfloor=m$$ and $$\left \lfloor \frac{4}{x} \right \rfloor=5-m$$ Thus we have: $$ m \le \frac{3}{x} \lt m+1$$ $$5-m \le \frac{4}{x} \lt 6-m$$ Adding both we get: $$5 \le \frac{7}{x} \lt 7$$ $\implies$ $$1 \lt x \le \frac{7}{5}$$ Hence $$x \in (1, 1.4]$$ But answer in book is given as $$x \in (1,\frac{4}{3})$$ What went wrong?","['algebra-precalculus', 'proof-verification', 'ceiling-and-floor-functions', 'inequality']"
3137423,Prove that $\int_{0}^{1}f(x)\arctan x dx=\frac{π}{8}\int_{0}^{1}f(x)dx$,"For $f:[0,1]\rightarrow\mathbb{R}$ a continuous function with the property that $2f(\frac{1-x}{1+x})=(1+x)^2$ . Prove that $\int_{0}^{1}f(x)\arctan xdx=\frac{π}{8}\int_{0}^{1}f(x)$ . However, I obtained that $$\int_{0}^{1}f(x)\arctan xdx=\int_{0}^{1} 2f\left(\frac{1-t}{1+t}\right)\frac{1}{(1+t)^2}\arctan\left(\frac{1-t}{1+t}\right)dt=\frac{π}{4}-\int_{0}^{1}\arctan tdt$$ Which is $\frac{\ln 2}{2}$ . However $\int_{0}^{1}f(x)dx=1$ after similar changes in variable. The results don't match, in conclusion. Edit: I copied the exercise correctly. I post a photo of it, sorry for being in Romanian.","['integration', 'definite-integrals']"
3137440,Finding the area of inner triangle constructed by three cevian lines of a large triangle,"QUESTION : In a triangle $ABC$ , $AD, BE$ and $CF$ are three cevian lines such that $BD:DC = CE:AC = AF:FB = 3:1$ . The area of $\triangle ABC$ is $100$ unit $^2$ . Find the area of $\triangle HIG$ Attempt : First, I thought that it can be done through applying Menelaus's Theorem . Despite having few knowledge about that theorem, I started like that: As a consequence of Menelaus's theorem , if two cevian $BH$ and $GF$ of $ABG$ meet at $I$ then: $$\frac{HI}{IB} = \frac{BF}{AB}.\frac{AH}{HG}$$ $\implies \frac{HI}{IB} = \frac{AH}{4HG}......(i)$ And $$\frac{FI}{IG} = \frac{3HG}{AG}......(ii)$$ Similarly from $\triangle ACI$ , I got two more equations and that is: $\frac{CG}{GI} = \frac{4GH}{AE}.......(iii)$ $\frac{EH}{HI} = \frac{IG}{4IC}........(iv)$ And likewise, from $\triangle BHC$ , $\frac{DG}{HG} = \frac{3HI}{BH}........(v)$ $\frac{IG}{GC} = \frac{BI}{4IH}.........(vi)$ But later on, I thought that it wouldn't be so good for relating with so many sides or segments and also be ineffective instead of relating the area. Then, how could I relate the area using Menelaus's theorem or in any other way like pure geometry? Thanks in advance.","['contest-math', 'area', 'geometry', 'triangles', 'plane-geometry']"
3137446,Almost sure convergence of positive random variables,"I have a sequence of random variables $X_1, X_2, \ldots : \Omega \rightarrow \mathbb{R}$ and $(\Omega, \mathcal{F}, \mathbb{P})$ is a probability space. The variables are known to be non-negative, i.e., $X_n \ge 0$ for all $n$ . If we assume that $$\liminf_{n \to \infty} \: \mathbb{E}[X_n] = 0,$$ then, can we show that $$\liminf_{n \to \infty} \: X_n = 0$$ almost surely? My tentative proof is as follows. The expected value of $X_n$ is $$\mathbb{E}[X_n] = \int_{\Omega} X_n(\omega) \mathrm{d}\mathbb{P}(\omega).$$ If we apply Fatou's Lemma we conclude that $$\int_{\Omega} \liminf_{n \to \infty} X_n(\omega) \mathrm{d}\mathbb{P}(\omega) \le \liminf_{n \to \infty} \int_{\Omega} X_n(\omega) \mathrm{d}\mathbb{P}(\omega) = 0,$$ where the right-hand side is 0 by assumption. Since each $X_n \ge 0$ , then $\liminf_{n \to \infty} X_n(\omega) \ge 0$ for all $\omega$ . Therefore, the left-hand side is also $\ge 0$ and, thus, it is $=0$ . Therefore we can conclude that $$\liminf_{n \to \infty} X_n = 0$$ almost surely.","['statistics', 'probability-theory', 'random-variables']"
3137453,Showing that $\arctan x = \arcsin\left(\frac{x}{\sqrt{1 + x^2}}\right)$,"Since I don't have the answer to this one, I want to make sure I've done this correctly. Show that $$\arctan x = \arcsin\left(\frac{x}{\sqrt{1 + x^2}}\right)$$ Since $$\arctan x = \arcsin\left(\frac{x}{\sqrt{1 + x^2}}\right)$$ we have $$\tan\left(\arcsin\left(\frac{x}{\sqrt{1 + x^2}}\right)\right) = x$$ Applying the Pythagorean theorem, we learn that $b = 1$ , so we have $\tan(x/1) = x$ . It may be a bit short, but I really want to be sure I have this right before I continue on. :)","['algebra-precalculus', 'trigonometry']"
3137485,"Find all positive integers $a, b, c$ such that $21^a+ 28^b= 35^c$ .","Find all positive integers $a, b, c$ such that $21^a+ 28^b= 35^c$ . It is clear that the equation can be rewritten as follows: $$
(3 \times 7)^a+(4 \times 7)^b=(5 \times 7)^c
$$ If $a=b=c=2$ then this is the first possible answer to this issue. It is also obvious that the sum of $(3*7)^a+(4*7)^b$ must end and be divisible by $5$ . Since $21^a$ always ends at $1$ , then $28^b$ should end at $4$ . Defined $b$ as $b=2+4k$ -- even positive integer.","['algebra-precalculus', 'perfect-powers', 'exponential-sum', 'diophantine-equations']"
3137504,von Mangoldt's formula for Chebyshev $\psi$ function,"Chebyshev's $\psi$ function is defined for primes $p$ as $$\psi(x)=\sum _{p^k\leq x} \log (p)$$ von Mangoldt found an explicit formula for this, with the exception that the function takes half-values at each 'step': $${\psi}^{}_{0} (x)=x-\frac{\zeta '(x)}{\zeta(x)}-\frac{1}{2}\ln\bigl(1-x^2 \bigr)-\sum_{\rho}\frac{x^{\rho}}{\rho}$$ where $\rho$ denotes the non-trivial zeroes of the zeta function. I have two questions: Does this formula assume that the Riemann hypothesis is correct, or does it remain valid if instances of $\rho$ exist that lie away from the critical line but still within the critical strip? Given that $\frac{\zeta '(x)}{\zeta(x)}=\ln(2\pi)$ , and given that $\frac{1}{2}\ln\bigl(1-x^2 \bigr)$ is defined only in the half-plane $\ge 1$ , is always negative with a pole at $x=1$ , and rapidly converges towards $0$ from below, the expression $x-\frac{\zeta '(x)}{\zeta(x)}-\frac{1}{2}\ln\bigl(1-x^2 \bigr)$ is asymptotic to $x-\ln(2\pi)$ . Is it therefore possible to write von Mangoldt's formula using big or little O notation for the expression $x-\frac{\zeta '(x)}{\zeta(x)}-\frac{1}{2}\ln\bigl(1-x^2 \bigr)$ ? If so, how?","['riemann-zeta', 'number-theory', 'asymptotics', 'chebyshev-function']"
3137525,How to prove that a function is non-negative definite?,The function I am trying to prove is $\exp(-2\lvert j-k\rvert)$ . Here is what I  have tried; $\sum_{j=1}^n \sum_{k=1}^n\ a_j \bar{a_k}\exp(-2\lvert j-k\rvert)$ = $\sum_{j=1}^n \sum_{k=1}^n\ a_j \bar{a_k}\exp(-2\lvert j-k\rvert)$ = $\sum_{j>k} \ a_j \bar{a_k}\exp(-2j)\exp(2k)+\sum_{k>j} \ a_j \bar{a_k}\exp(2j)\exp(-2k) + \sum_{j=k} \lvert a_j\rvert^2$ but I am not sure if it is the right path or not.,"['functions', 'positive-semidefinite']"
3137545,Examples of non trivial vector bundles,"Once you see the notion of vector bundle, next thing you want to see are examples of   non trivial vector bundles. Here, I want to collect such examples with justification of one or two lines saying why this vector bundle is non trivial. Please add one per answer.","['vector-bundles', 'line-bundles', 'examples-counterexamples', 'differential-geometry']"
3137554,At how many points is this piecewise function discontinuous?,"$$f(x) =
\begin{cases}
\frac{3x}{x^2-16}  & \text{when $x\leq 2$} \\[2ex]
\frac{x-2}{x^2-5x+6} & \text{when $x\gt 2$}
\end{cases}
$$ At what points is this piecewise function discontinuous? My solution: $x=-4, 4, 2, 3$ are the points  at which the denominator is zero, but $x=4\not\leq2$ and $x=2\not\gt2$ . Therefore, the points are $x=-4$ and $x=3$ . But the answer is given as $3$ . What am I doing wrong?","['calculus', 'functions']"
3137639,"If $f:X\rightarrow Y$ is an order isomorphism , then $f^{-1}:Y\rightarrow X$ is an order isomorphism .","Let $(X,\leq)$ , $(Y,\leq ')$ be partially ordered sets. If $f:X\rightarrow Y$ is an order isomorphism between $X$ and $Y$ , then the inverse function $f^{-1}:Y\rightarrow X$ is an order isomorphism between $Y$ and $X.$ Proof.  Assume $f:X\rightarrow Y$ is an order isomorphism between $X$ and $Y$ that is $$x_1\leq x_2\iff f(x_1)\leq ' f(x_2)$$ for all $x_1,x_2\in\mathbb{X}.$ So  since $f$ bijective, we have $$x_1\leq x_2 =f^{-1}(y_1)\leq f^{-1}(y_2) \iff f(x_1)\leq'f(x_2)=y_1\leq 'y_2,$$ for all $y_1,y_2 \in Y,$ that is $$f^{-1}(y_1)\leq f^{-1}(y_2) \iff y_1\leq 'y_2$$ for all $y_1,y_2 \in Y$ . So we are done. Can you check my proof? Thankss...","['elementary-set-theory', 'order-theory', 'proof-writing']"
3137667,"Why does the ""solution"" not satisfy the PDE?","I'm trying to solve the following PDE: $$x z_x -xyz_y = z \quad with\quad z(x,x) =x^2 e^x \quad (x,y) \in \mathbb{R}^2.$$ I've proceeded as, $$\frac{dx}{ x} = \frac{dy }{-xy }  \Rightarrow y = C e^{-x}, $$ so $$\frac{dz}{ dx}  = z \Rightarrow z = G(C) e^x .$$ Using the given curve, $$
\begin{split}
z(x,x) &= G\left(\frac{ x}{ e^{-x}}\right) e^x = x^2 e^x\\
\\
& \Rightarrow G(t) = t^2 e^{2x} (\text{ I am not sure this is the only possible form of }G).
\end{split}
$$ Hence, $$z(x,y) = y^2 e^x,$$ but the thing is, this does not satisfy the given PDE: $$
x (y^2 e^x) - xy (2y e^x) = -xy^2e^x \not = z.
$$ I'm assuming that there is a problem while finding the characteristic curve since I cancelled $x$ s there, but not sure why would that cause a problem, so I'm looking for both a solution and an explanation what is wrong in my wrong solution.","['ordinary-differential-equations', 'partial-differential-equations']"
3137704,A non-composite sequences,"Can you provide a counterexample for a claim given below? Inspired by Puzzle 937 I have formulated the following claim: For any $n > 0$ let $B = p_1 \cdot p_2 \cdot .... \cdot p_n$ be the product of the first $n$ primes. Let $X$ be the smallest number, bigger than $B^k/p_{n+1}$ and coprime to $B^k$ , where $k$ is a fixed positive integer. Define the number $m_n$ as $X \cdot p_{n+1}-B^k$ , then $m_n$ is either $1$ or prime. Try it for yourself! I was searching for counterexample using the following PARI/GP code: CE(lb,ub,k)={
for(n=lb,ub,
B=prod(i=1,n,prime(i));
X=ceil((B^k)/prime(n+1));
while(gcd(X,B^k)!=1,
X++);
m=X*prime(n+1)-B^k;
if(!(ispseudoprime(m) || m==1),print(m)))
}","['elementary-number-theory', 'examples-counterexamples', 'sequences-and-series', 'recreational-mathematics', 'prime-numbers']"
3137716,probability of infinite intersection of events,"given a sequence of mutually independent events $\{A_n\}_{n \in \mathbb{N}}$ , I am trying to prove that: $$P\left(\bigcap_{i=n}^{\infty}A_i\right) = \lim \limits_{n \to \infty} \prod_{i=n}^{n}P(A_i)$$ It is easy to prove by induction that $P(\bigcap_{i=n}^{m}A_i) = \prod_{i=n}^{m}P(A_i)$ for all $m\in \mathbb{N}$ . Now, my problem is due to the limit. In particular: Can I write $\bigcap_{i=n}^{\infty}A_i =  \lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i$ ?  If yes, is this the traditional way to interpret intersections -
unions of countable sets? Can I write $P (\lim \limits_{m \to \infty} \bigcap_{i=n}^{m}A_i) = \lim \limits_{m \to \infty} P(\bigcap_{i=n}^{m}A_i)$ ? If yes, why and does this property hold in any measurable space? For example, we know that countable additivity is an axiom of measures, but there are no axioms for the one I wrote. So in case it is correct there must be a way to prove it. Maybe applying De Morgan's laws? Thanks a lot.","['limits', 'measure-theory', 'probability']"
3137724,Laplacian is the trace of Hessian in local coordinates,"I am working in the riemannian setting with Levi-Civita connection and
I would like to prove using local coordinates that the laplacian is the trace of the Hessian i.e. $$ \text{tr}_g(\nabla^2 f)=\sum_{i,j} g^{ij} \bigl ( f_{ij} - \sum_k\Gamma_{ij}^k f_k \bigr ) = \sum_{i,k} \bigl ( \partial_i (g^{ki} f_k) + \sum_{j}g^{kj} \Gamma_{ji}^i f_k \bigr )= \Delta f$$ Expanding summations and using Liebniz I obtain $$ \sum_{i,j} g^{ij} f_{ij} - \sum_{i,j,k}g^{ij}\Gamma_{ij}^k f_k = \sum_{i,k} g^{ki}f_{ik} + \sum_{i,k} \partial_i(g^{ki})f_k+ \sum_{i,j,k} g^{kj}\Gamma_{ji}^if_k$$ and then to prove the equality it is enough to show $$ -\sum_{i,j,k}g^{ij}\Gamma_{ij}^kf_k = \sum_{i,k} \partial_i(g^{ki})f_k+ \sum_{i,j,k}g^{kj}\Gamma_{ji}^if_k$$ i.e. that for every $k$ it holds $$ -\sum_{i,j}g^{ij} \Gamma_{ij}^k = \sum_{i}\partial_i (g^{ki}) + \sum_{i,j}\Gamma_{ji}^ig^{kj}$$ I do not know how to proceed from this point.","['riemannian-geometry', 'differential-geometry']"
3137805,Uniform sampling of convex polytopes: why is it hard?,"There is a surprising number of posts asking about the uniform sampling of convex polytopes (e.g. 1 , 2 , 3 ), and an equally surprising number of non-answers (e.g. a , b ). From what I have read, this seems to be a difficult problem; and known solutions consist in partitioning the polytope into a collection of simplices, select one randomly, and sample a uniformly random point from it. That is assuming that we know the vertices of the polytope in the first place (otherwise the problem is apparently NP-hard ?). From what I know, sampling uniformly random points from a d-dimensional simplex with d+1 known vertices is done in two steps: Sample d+1 weights from a Dirichlet distribution with parameters $(\alpha_k = 1)_{k=0..d}$ Compute the corresponding weighted sum of vertices Assuming this is correct, my question is: why can we not apply the same method to sample any convex polytope uniformly randomly? I.e. by computing a weighted sum of its vertices, with weights sampled from a Dirichlet distribution. Is it because it has not been proven that this would result in a uniform distribution, or because there are known cases where it does not?","['convex-geometry', 'geometry', 'sampling']"
3137813,"How to evaluate $\sum\limits_{n=0}^\infty\frac{1}{x^n+y^n}$ for $x,y>1$?","Is there an analytical expression for $\sum\limits_{n=0}^\infty\frac{1}{x^n+y^n}$ when $x,y>1$ ? If so, how do you solve it?","['convergence-divergence', 'taylor-expansion', 'sequences-and-series']"
3137818,Understanding that a hypothesis test does not depend on a certain parameter,"I have a very big doubt regarding a test not depending on $\theta_1$ . Suppose I have: $$(X_i)_{i=1}^{n}, X_i \stackrel{iid}{\sim} N(\theta, 1), \hbox{ that is } X_i \sim g(x_i, \theta) = (2\pi)^{-1/2} \exp[\frac{-1}{2}(x_i - \theta)^2]$$ The joint distribution is given by $f(x;\theta) = (2\pi)^{-n/2} \exp[\frac{-1}{2}\sum_{i = 1}^{n}(x_i - \theta)^2] $ Suppose I have the following hypothesis test with $0<\theta_1$ , where $\theta_1$ is fixed but arbitrary (remark: always positive) $$H_0: \theta = 0\quad \hbox{vs}\quad H_1: \theta = \theta_1$$ By Neyman–Pearson theorem, there is some $k>0$ such that $$\phi(x) = \left\{
  \begin{array}{lr}
    1 & : f(x;0)k  < f(x;\theta_1)  \\
    0 & : f(x;0)k  > f(x;\theta_1)
  \end{array}
\right.$$ is a UMP test in the class of tests with size $\alpha = E_0[\phi(X)]$ . $$\hbox{I want to understand why this test does not depend on }\theta_1?$$ $remark:$ $\theta_1$ is positive and fixed but arbitrary. That is, I want to understand if I want to change the parameter $\theta_1$ , I will still get the same test exactly. Notice that: $$k < \frac{f(x;\theta_1)}{f(x;0)} \Longleftrightarrow \ln(k) < \frac{n}{2}(2 \theta_1 \bar{x} - \theta_1^{2}) $$ In addition, we can see that \begin{equation} \label{eq1}
\begin{split}
k < \frac{f(x;\theta_1)}{f(x;0)} & \Longleftrightarrow  \frac{\ln(k)}{n} < \frac{1}{2}(2 \theta_1 \bar{x} - \theta_1^{2}) = \theta_1\bar{x} - \frac{\theta_1^{2}}{2} \\
 & \Longleftrightarrow \frac{\ln(k)}{n} + \frac{\theta_1^{2}}{2} < \theta_1\bar{x}\\
& \stackrel{\theta_1 > 0}{\Longleftrightarrow } \frac{\ln(k)}{n\theta_1} + \frac{\theta_1}{2} < \bar{x}\\
&\Longleftrightarrow \sqrt{n}\left[\frac{\ln(k)}{n\theta_1} + \frac{\theta_1}{2}\right] < \sqrt{n}\bar{x}\\
& \Longleftrightarrow \tilde{k}(\theta_1) < \sqrt{n}\bar{x}
\end{split}
\end{equation} Then $\phi(x) = \left\{
  \begin{array}{lr}
    1 & : \tilde{k}(\theta_1) < \sqrt{n}\bar{x}  \\
    0 & : \tilde{k}(\theta_1) > \sqrt{n}\bar{x}
  \end{array}
\right.$ , with $\alpha = E_0[\phi(X)]$ . Notice that $Z = \sqrt{n} \bar{X} \sim N(0,1)$ . So, to determine the test well, we have to determine the constant $\tilde{k}(\theta_1)$ . We will determine the constant $\tilde{k}(\theta_1)$ forcing the test to be of size $\alpha$ . Under the null hypothesis, we have: \begin{equation} 
\begin{split}
\alpha = E_{0}[\phi(X)] & \Longleftrightarrow  P_{0}[\tilde{k}(\theta_1) < Z] = \alpha \\
 & \Longleftrightarrow P_{0}[Z \leq \tilde{k}(\theta_1) ] = 1 -\alpha \\
& \Longleftrightarrow F_Z (\tilde{k}(\theta_1)| \theta = 0) = 1 -\alpha\\
& \Longleftrightarrow \tilde{k}(\theta_1) = F_{Z}^{-1} ( 1- \alpha| \theta = 0)
\end{split}
\end{equation} And here is my problem, because the $\tilde{k}(\theta_1)$ depends on the parameter $\theta_1$ . For example, suppose $\alpha = 0.01$ , we have $$\tilde{k}(\theta_1) = \sqrt{n}\left[\frac{\ln(k)}{n\theta_1} + \frac{\theta_1}{2}\right] = 2.33$$ In other words, if a take some other $\theta_1^{'}>0$ , we have other $\tilde{k}(\theta_1^{'})$ . And consequently, I will have another test $\phi^{'}(x) = \left\{
  \begin{array}{lr}
    1 & : \tilde{k}(\theta_1^{'}) < \sqrt{n}\bar{x}  \\
    0 & : \tilde{k}(\theta_1^{'}) > \sqrt{n}\bar{x}
  \end{array}
\right.$ For this purpose, can I adjust the $\tilde{k}(\theta_1)$ by resizing the sample $n$ ? that is, if I want the $\tilde{k}(\theta_1)$ not to depend on the parameter $\theta_1$ , I should just change $n$ ? but this does not seem to make much sense. Why am I asking this? because in other problems, I really need to vary the parameter $\theta_1 > 0$ and ensure that the test does not depend on $\theta_1$ . For example: $$H_0: \theta \leq 0\quad \hbox{vs}\quad H_1: \theta > 0.$$","['statistics', 'hypothesis-testing']"
3137828,"Proving $y-x^2, z-xy$ generate the ideal of the twisted cubic","This question comes from Hartshorne's exercise 1.2. He defines $Y:=\{(t,t^2,t^3)\in \mathbb{A}^3\mid t\in k\}$ and asks us, among other things, to find generators for the ideal $I(Y):=\{f\in k[x,y,z]\mid f(p)=0\,\,\,\forall p\in Y\}$ . To me it's obvious that the natural candidates are $y-x^2, z-xy$ . I've already verified that $Y=Z(J)$ , where $J:=(y-x^2, z-xy)$ . By the Nullstellensatz, I could only conclude that $I(Y)=\sqrt{J}$ . I guess it's true that $J$ is a radical ideal (and this would solve the problem), but I don't know how to prove it. Using the mere definition of radical didn't work for me. Is there some other way? Obs.: here $k$ is an algebraically closed field.","['algebraic-curves', 'affine-varieties', 'algebraic-geometry', 'ideals', 'commutative-algebra']"
3137925,Question about weak law,"This is from Durrett. Let $X_i$ be $iid$ with $P\{X_i = (-1)^k k\} = \frac{C}{k^2\log k}, k\geq 2$ and $C = [\sum_{k=2}^{\infty} \frac{1}{k^2 \log k}]^{-1}$ . Show that $E|X_1|=\infty$ but there is a finite constant $\mu$ s.t. $\frac{S_n}{n}\xrightarrow{p} \mu $ . My attempt: Step 1) Show $nP(|X_1|>n)\rightarrow 0$ Step 2) This means $\frac{S_n-E\tilde{S_n}}{n}\xrightarrow{p} 0$ where $\tilde{S_n} = \sum_{k=1}^{n}\tilde{X_k}$ and $\tilde{X_k} = X_k .1_{(|X_k|<n)}$ which means I have to show $lim_{n\rightarrow \infty} \frac{E\tilde{S_n}}{n}$ is a constant. Step 1) $$P(|X_{1}|>n) = \sum_{k=n}^{\infty} \frac{C}{k^2 \log k}\leq \int_{n}^{\infty}\frac{C}{x^2 \log x}dx\leq \frac{C}{\log n}\int_{n}^{\infty}\frac{1}{x^2}dx = \frac{C}{n\log n}$$ $$\therefore nP(|X_{1}|>n) = \frac{C}{\log n}\xrightarrow{n\rightarrow \infty} 0$$ Step 2) I am stuck here. Since $X_i$ are $iid$ : $$E\tilde{S_n} = nE\tilde{X_1} = n E[X_1 .1_{(|X_1|<n)}] = n\sum_{k=2}^{n} (-1)^k k.\frac{C}{k^2\log k} = nC\sum_{k=2}^{n} (-1)^k.\frac{1}{k\log k}$$ So for $lim_{n\rightarrow \infty} \frac{E\tilde{S_n}}{n}$ to be a constant, have to show: $$lim_{n\rightarrow \infty} \sum_{k=2}^{n} (-1)^k.\frac{1}{k\log k}<\infty$$","['statistics', 'probability-theory', 'real-analysis']"
3137936,A question about Integral Closures from Hartshorne Chapter 2 Exercise 6.4,"The exercise that I'm having trouble with is the following. Hartshorne II.6.4: Let $k$ be a field of characteristic $\neq 2$ . Let $f \in k[x_1, \dots x_n]$ be a square free nonconstant polynomial, i.e. in the unique factorization of $f$ into irreducible polynomials, there are no repeated factors. Let $A=k[x_1 \dots x_n,z]/(z^2-f)$ . Show that $A$ is an integrally closed ring. [Hint: The quotient field $K$ of $A$ is just $k(x_1, \dots x_n)[z]/(z^2-f)$ . It is a Galois extension of $k(x_1, \dots x_n)$ with Galois group $\mathbb{Z}/2\mathbb{Z}$ generated by $z \mapsto -z$ . If $\alpha=g+hz \in K$ , where $g,h \in k(x_1, \dots x_n)$ , then the minimal polynomial of $\alpha$ is $X^2-2gX+(g^2-h^2f)$ . Now show that $\alpha$ is integral over $k[x_1, \dots x_n]$ if and only if $g,h \in k[x_1, \dots x_n]$ . Conclude that $A$ is the integral closure of $k[x_1, \dots x_n]$ in $K$ .] Even with the hint I have two questions. 1) Why do I require $f$ to be square-free? I know that I do not want $f$ to be a square, because then $(z^2-f)$ would not be a prime ideal in $k[x_1, \dots x_n,z]$ . I cannot see where square-free is used though. 2) For $\alpha=g+hz$ with $g,h \in k(x_1, \dots x_n)$ , the minimal polynomial over $k(x_1, \dots x_n)[z]$ is exactly the one given above. However, why couldn't there be some OTHER monic polynomial $s \in k[x_1, \dots x_n][z]$ such that $s(\alpha)=0$ ? I don't understand why $\alpha$ integral over $k[x_1, \dots x_n]$ forces $g,h \in k[x_1, \dots x_n]$ .","['function-fields', 'minimal-polynomials', 'algebraic-geometry', 'commutative-algebra']"
3137997,"If there exists $m,n\geq2$ such that $x^{m}y^{n}=yx$, for any $x,y\in M$, then prove that the operation is commutative [duplicate]","This question already has an answer here : Prove that a semigroup satisfying $a^pb^q=ba$ is commutative (1 answer) Closed 5 years ago . Let $M$ be a set, not null, and $*$ an operation that is associative. If there exist $m,n\geq2$ such that $x^{m}y^{n}=yx$ , for any $x,y\in M$ , then prove that the operation is commutative. Now, I obtained a result by $x{\rightarrow}yxy^{-1}$ such that $yx=(yxy^{-1})^my^n=yx^my^{n-1}=yx^my^ny^{-1}=yyxy^{-1}$ and from here $yx=yyxy^{-1}{\Rightarrow}xy=yx$ . However, this works only if my set is a group. How can I adapt this to the form of the set?","['group-theory', 'abstract-algebra', 'semigroups']"
3138081,"If $D$ is a diagonal matrix, when is the commutator $DA - AD$ full rank?","Suppose $A \in \mathbb{C}^{n \times n}$ is full rank.   I'm looking for a sufficient condition on $A$ such that for some diagonal matrix $D \in \mathbb{C}^{n \times n}$ , the commutator $D A - A D$ is full rank. I've worked out some necessary conditions- $A$ and $D$ cannot share an eigenvector, so no column of $D$ can have $n-1$ zero indices (ie,  no column of $A$ is a scaled column of the identity matrix). For $n=2$ , \begin{align}
DA - A D &= 
\begin{bmatrix} 
0 & (d_1 - d_2) a_{1,2} \\
(d_2 - d_1) a_{2, 1} & 0
\end{bmatrix}\\ 
\det(DA - AD) &= (d_1-d_2)^2 a_{1,2} a_{2,1},
\end{align} so a $a_{1,2}, a_{2,1} \neq 0$ is both necessary and sufficient. But for $n=3$ , \begin{align}
DA - A D &= 
\begin{bmatrix} 
0 & (d_1 - d_2) a_{1,2} & (d_1 - d_3) a_{1, 3} \\
(d_2 - d_1) a_{2, 1} & 0 & (d_2 - d_3) a_{2, 3} \\
(d_3 - d_1) a_{3, 1} & (d_3 - d_2) a_{3, 2} & 0
\end{bmatrix}\\ 
\det(DA - AD) &= (d_1-d_2)(d_1-d_3)(d_2-d_3) (a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}).
\end{align} So if $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0$ , $DA - AD$ is rank deficient regardless of the choice of $D$ . Q1 : Is there a geometric interpretation for the constraint $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2}) \neq 0$ ? Q2 :  I don't get a factored form of the determinant (one term depending on $D$ , one term on $A$ ) for $n > 3$ .  Is $n=3$ a special case? Q3 : Is there a sufficient condition on $A$ such that $DA - AD$ is full rank for $n>3$ ? I'd prefer to not assume $A$ is positive definite. Edit :  If $A$ is symmetric, $DA - AD$ is skew-symmetric and thus rank deficient if $n$ is odd. Q4 : $A$ symmetric is one way to make $(a_{1,2} a_{2,3} a_{3,1} - a_{1,3}a_{2,1}a_{3,2})=0$ , but clearly other choices result in a rank-deficient commutator.  Is there a clean way to express this for $n > 3$ ?","['matrices', 'linear-algebra']"
3138099,Find explicit formula for $y_{n}=\frac{1}{n (\sqrt 2)^n}+\frac{n-1}{n} y_{n-2}$,"Is it possible to find a explicit formula here ? This is the reduction formula of $$\int_{0}^{\frac{\pi}{4}} (\cos x)^n dx$$ . If the limits are from $0$ to $\frac{\pi}{2}$ then the nice walli's formula can be obtained,however i have no idea regarding solving this kind of recurrences. Any help will be appreciated.","['integration', 'definite-integrals', 'recurrence-relations', 'discrete-mathematics']"
3138125,Can the smallest solution of $\varphi(k)=n$ be an even positive integer?,"Suppose, $n$ is a positive integer and the equation $$\varphi(k)=n$$ has a solution. Upto $n=20\ 000$ , the smallest solution $k$ (if existent) is an odd positive integer. Do positive integers $n$ exist, such that the smallest solution of $\varphi(k)=n$ is an even integer ? If yes, which is the smallest such $n$ ?","['number-theory', 'totient-function', 'elementary-number-theory']"
3138129,Maclaurin Series from sin(x) to cos(x) using derivative,"I understand how to find the MacLaurin series for $\sin(x)$ . $$\sum_{n=1}^\infty \frac{x^{2n-1}\cdot\!(-1)^{n-1}}{(2n-1)!}$$ Now I am trying to find the MacLaurin series for $\cos(x)$ by taking the derivative of the above sum with respect to $x$ . Using power rule, I got the following series: $$\cos(x) = \sum_{n=1}^\infty \frac{x^{2n-2}\cdot\!\mathrm{(-1)}^{n-1}}{(2n-2)!}$$ However, the MacLaurin series is: $$\cos(x) = \sum_{n=0}^\infty \frac{x^{2n}\cdot\!\mathrm{(-1)}^{n}}{(2n)!}$$ How are these two $\cos(x)$ MacLaurin series equal? What makes the second one more correct than the series I got by taking the derivative of the $\sin(x)$ series. A sort of related question: if you choose to start at $n=1$ vs $n=0$ , how would you change the terms of the $\sin(x)$ Maclaurin series?","['power-series', 'derivatives', 'taylor-expansion']"
3138150,A nifty triangle inequality trick,"I was reading the proof of a theorem and at a point of the argument the author just posed an inequality which I was unable to derive, but is probably just a smart application of the triangle inequality. The problem is as follows. Let $f,g:\mathbb{R} \to \mathbb{R}$ be uniformly continuous and $\epsilon, \delta >0$ such that if $x,y\in \mathbb{R}$ are st $|x-y| < \delta$ , that then $|f(x)-f(y)|, |g(x)-g(y)| <\epsilon$ . Let $I \subset \mathbb{R}$ be an interval with $diam(I) < \delta$ The author then states without proof that $\sup_{x,y\in I} |f(x)-g(y)|^p < |f(t)-g(t)+2\epsilon|^p$ for all $t \in I$ ., $p\geq 1$ It feels as if this should be easy to prove, but I have been stuck at it for quite some time now. Any help would be greatly appreciated.","['inequality', 'analysis']"
3138187,A peculiar integral identity,"I was trying to solve this integral $$\int_0^\pi e^{v \cos \theta \cos t} \cosh(v \sin \theta \sin t) dt \, .$$ I checked on WolframAlpha and Gradshteyn and Ryzhik but didn't find anything useful. To get a first impression I plotted the integrand for $v=1$ . So, for $f_{\theta}(t) = e^{\cos \theta \cos t} \cosh(\sin \theta \sin t)$ we get the following plots: So far, nothing too weird about the plot. I proceeded by trying to numerically evaluate the integral itself. Turns out, the integral is invariant under $\theta$ ! Even better, since the integral is independent of $\theta$ we can just set $\theta=0$ and obtain $$\int_0^\pi e^{v \cos \theta \cos t} \cosh(v \sin \theta \sin t) dt = \int_0^\pi e^{v \cos t} dt = \pi I_0(v)\quad \forall \theta \in [-\pi,\pi]\; ,$$ where the second equality is a known identity of the modified Bessel function of the first kind . Since I only stumbled upon this identity numerically, I was wondering if someone has some analytical insights into this. Put into a question: Does someone know, why this identity holds? Bonus I now face the same integral but with an additional linear term, that is $$\int_0^{\pi} tf_{\theta}(t)dt \; ,$$ with $f_{\theta}$ as defined above. I am hoping that the techniques that illuminate the identity above will also shed some light at this new integral, which by the way is not independent in $\theta$ anymore.","['integration', 'trigonometry', 'definite-integrals', 'bessel-functions']"
3138191,Primes with digits only 1,"Let $Y(k)$ be the number consisting of $1$ , repeated $k$ times. We know that $Y(2) =11$ is prime. It so happens that $Y(19)$ and $Y(23)$ are also prime. Are there any more? Regards, David","['number-theory', 'repunit-numbers', 'prime-numbers', 'decimal-expansion']"
3138203,Prove that there is a bijection between permutations written in canonical cycle notation and one line notation,"Suppose $\sigma\in S_{n}$ is written in cycle notation, with its fixed points included. We say $\sigma$ is written in canonical cycle notation if each cycle is rotated such that its smallest element is last, and the cycles are arranged in ascending order of their last elements. For example, $(351)(462)(87)(9)$ is in canonical form. Define the function $f:S_{n}\to S_{n}$ by removal of parentheses, with the image interpreted in one-line notation. Prove that $f$ is bijective. This is one of those things that seems pretty obvious but I'm having trouble formalising it. It's Foata's fundamental transformation, and the only proof I can find of it is his original one, which is in french. NB. I realise that this is not the typical definition of canonical form.","['permutations', 'combinatorics']"
3138298,$\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx$ is not the Fourier series of a Riemann integrable function.,"Show that $$\sum_\limits{n\ge 2}\frac{1}{\log n}\sin nx$$ is not the Fourier series of a Riemann integrable function. I think this comes from the low increasing speed of $\log$ function. Analogously, $$\sum_\limits{n\ge 1}\frac{1}{n^\alpha}\sin nx$$ is not the Fourier series of a Riemann integrable function when $0<\alpha<1$ . And for the boundary situation, $$\sum_\limits{n\ge 1}\frac{1}{n}\sin nx$$ is a Fourier series of a Riemann integrable function, the sawtooth function.","['fourier-series', 'fourier-analysis', 'analysis']"
3138407,Axiom of choice and dual of a tensor product,"EDIT : This question (and other related questions) was also asked on mathoverflow : here . Let $V$ , and $W$ be vector spaces. By the universal property of the tensor product, 
there is a canonical map from $V^*\otimes W^*$ into $(V\otimes W)^*$ (since the map $(\omega_1,\omega_2)\mapsto \omega_1\otimes\omega_2$ is bilinear from $V^*\times W^*$ into $(V\otimes W)^*$ . I have read that this map is actually injective by using some basis on $V$ and $W$ . Since the existence of basis for any arbitrary vector space relies on the axiom of choice, my question is : is the axiom of choice necessary to prove the injectivity of the canonical map $V^*\otimes W^*\to(V\otimes W)^*$ ?","['axiom-of-choice', 'linear-algebra', 'tensor-products']"
3138417,Help finding the derivative of $f(x) = \cos{\left(\sqrt{e^{x^5} \sin{x}}\right)}$,"I am trying to find the derivative of $f(x) = \cos(\sqrt{(e^{x^5} \sin(x)})$ . I keep getting the wrong answer, and I'm not sure what I'm doing wrong. $$\frac{d}{dx} e^{x^5} = e^{x^5} \cdot 5x^4$$ $$\frac{d}{dx} \sin(x) = \cos(x)$$ $$\frac{d}{dx} (e^{x ^5} \cdot \sin(x)) = [(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]$$ $$\frac{d}{dx} \sqrt{x} = \frac{1}{2\sqrt{x}}$$ $$\frac{d}{dx} \sqrt{(e^{x^5} \sin(x))} = \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}}$$ Therefore, since $\frac{d}{dx} \cos(x) = -\sin(x)$ , I have $$ f'(x) = -\sin(\sqrt{e^{x^5} \sin(x)}) \cdot \frac{[(e^{x^5} \cdot 5x^4) \cdot \sin(x)] + [\cos(x) \cdot e^{x^5}]}{2 \sqrt{e^{x^5} \sin(x)}}$$ However, the website I'm using, ""WeBWorK"", says this is incorrect.","['calculus', 'derivatives', 'chain-rule']"
3138443,Evaluate :$\int_{-1}^{1} 2\sqrt{1-x^2} dx $,"Evaluate: $$\int_{-1}^{1} 2\sqrt{1-x^2} dx $$ The answer is $\pi$ My attempt $x = \sin(u), dx = \cos(u)du$ $$\int_{-1}^{1} 2 \sqrt{1-\sin^2(u)}\cos(u)du = \int_{-1}^{1} 2 \cos^2(u)du =\int_{-1}^{1} \frac{1}{2}(1+\cos(2u))du = \bigg(\frac{u}{2} + \frac{1}{2}\sin(2u) \bigg)\Bigg|_{-1}^{1}$$ confused how to proceed ?","['integration', 'calculus']"
3138503,Uniqueness of geodesics that join two points in exponential neighbourhoods,"I'm currently working through some notes on surfaces in $\mathbb{R}^3$ and their geodesics, with the following definitions (it's kind of lengthy, I will highlight the key parts): For a sufficiently smooth surface $S$ parametrized by $\varphi  : D \to \mathbb{R}^3$ , a geodesic is a curve $\alpha : I \to S$ to have acceleration orthogonal to the tangent spaces, i.e. $\alpha$ is a geodesic if and only if $$
\alpha''(t) \perp T_{\alpha(t)}S \quad (\forall t \in I).
$$ It is proven that fixing $p \in S$ and $v \in T_pS$ there is a unique geodesic $\gamma_{p,v} : I_{p,v} \to S$ such that $\gamma(0) = p$ and $\gamma'(0) = v$ , with $I_{p,v}$ the maximal interval of definition. That is, geodesics that stem from $p$ with velocity $v$ are (locally) unique, because two different ones give the same solution to an ODE in a sufficiently small interval of $p$ . It is also claimed that, since these curves and their definition intervals depend smoothly on $p$ and $v$ and $\gamma_{p,0}$ is defined on $\mathbb{R}$ , by continuity there exists some open ball $B_R(0_p) \subset T_pS$ where $1 \in I_{p,v}$ for $v \in B_R(0_p)$ . Hence it is possible to define the Riemannian exponential as $$
\begin{align}
\exp_p : &B_R(0_p) \to S \\
& v \longmapsto \gamma_{p,v}(1) 
\end{align}
$$ Shortly after it is proved that $\exp_p$ sends lines through $0$ to geodesics , since $\gamma_{s,v}(t) = \gamma_{s,tv}(1)$ , and that $D(\exp_p)_0 = Id$ which says that in a neighbourhood of $0$ , the exponential is a diffeomorphism . We assume from now on that $R$ is small enough for this to hold. From the previous definitions and results, it is then claimed that one can see that if $q = \exp_p(v) \in \exp(B_R(0_p))$ then there is a unique geodesic that joins $p$ and $q$ , namely $$
\gamma(t) := \exp_p(tv)
$$ with $[0,1] \subset Dom(\gamma)$ . I do not see why local uniqueness is derived just from the previous results. How can this be proven? I am aware that (given sufficient regularity hypotheses), any curve $\alpha \subset S$ in $\exp_p(B_R(0_p))$ is a lift $\alpha(t) = \exp_p(\beta(t))$ with $\beta$ a curve in $B_R(0_p)$ (that is, in it's identification with a ball of the plane) but I haven't been able to prove much with that. I also presume a strong usage of locality is needed, as for example in the sphere any two points there are two geodesics joining them.","['geodesic', 'surfaces', 'differential-geometry']"
3138523,Morrey's Inequality in 1D,"Morrey's Inequality in 1D for $p=2$ : There exists a constant $C$ such that $\|u\|_{C^{0,1/2}(\mathbb{R})} \leqslant C \|u\|_{H^{1}(\mathbb{R})}$ for all $u \in C^{1}(\mathbb{R})$ . Of course, for any $u \in C^{1}$ we have by the fundamental theorem of calculus and Holder's inequality: $|u(x)-u(y)| \leqslant \int_{y}^{x} |u'(t)|\;dt \leqslant \|u'\|_{L^{2}(\mathbb{R})} |x-y|^{1/2}$ . This implies $[u]_{C^{0,1/2}(\mathbb{R})} \leqslant \|u\|_{H^{1}(\mathbb{R})}$ . To conclude, we need to show that $|u(x)| \leqslant C\|u\|_{H^{1}(\mathbb{R})}$ for all $x \in \mathbb{R}$ . How can I go about proving this? I know Evans has a proof for $\mathbb{R}^{n}$ but I would like to know if there is a simpler approach in 1D.","['sobolev-spaces', 'analysis']"
3138550,"For $0 < \alpha < 1$, construct an open set $E \subset [0,1]$ which is dense in $[0,1]$ and $m(E) = \alpha$.","Q: For $0 < \alpha < 1$ , construct an open set $E \subset [0,1]$ which is dense in $[0,1]$ and $m(E) = \alpha$ . My first guess is an open covering of the rationals with each rational covered by an open interval of length $\alpha/2^k$ : such as $E = \cup_k (r_k, r_k + \alpha/2^k)$ . Then $m(E) \le \sum \alpha/2^k = \alpha$ . But that gives $m(E) \le \alpha$ not $m(E) = \alpha$ . What is a better answer that gives $m(E) = \alpha$ ?","['measure-theory', 'real-analysis']"
3138566,There are lower bounds worked out for the length of nontrivial Collatz-cycles. How can *upper bounds for the disproof* be determined?,"There have been lower bounds estimated for the length $N$ of (odd) steps of a nontrivial cycle in the collatz-problem. Such estimates have been based on knowledge of upper bounds $\chi$ for any number $a_1$ , where it has been empirically determined that they decrease to $1$ and enter the so-called ""trivial cycle"" by the iterated Collatz-transformation. One well known estimate for such length $N$ has been based on the value $\chi_{\Tiny \text{TOdS}} = 5 \cdot 2^{60}$ (due to Tomas Oliveira da Silva, for reference see wikipedia). Recently a new estimate has been published based on $\chi_{\Tiny \text{yoyo}}= 87 \cdot 2^{60} $ such that all $a_1 \lt \chi_{\Tiny \text{yoyo}}$ run into the trivial cycle. Inspired by another question here (but which seem to have really meant to ask for the highest lower bound for $N$ instead) I've got the somehow reciprocal question: Q1: How can we estimate an upper bound for the length $N$ , for which we can disprove a nontrivial ""general cycle"" , based on the knowledge of convergence of all $a_1 \lt \chi_{\Tiny \text{yoyo}}= 87 \cdot 2^{60} $ Remark: Question Q1 includes the question for the most meaningful/rigorous method to arrive at a large number for $N$ . There are more and less restrictive methods available, but like require accordingly more or less computational effort. Q2: What would be that ""champion"" number $N_\max$ ? Remark: The concept of ""m-cycles"" as defined by R. Steiner, J. Simons and B. de Weger is relevant here. That concept describes somehow the ""hilliness"" of a trajectory in an assumed cycle, such that an ""1-cycle"" has one local peak, a ""2-cycle"" has two local peaks and so on. It has been proven by the named authors that for ""m-cycles"" with few peaks ( $m<72$ ) there are no nontrivial cycles at all and so any length $N \gt 1$ is already disproved (and this question is answered for such types of cycles). Thus I have introduced in the above the qualifier ""general cycle"" to emphasize that I look for a estimate for an upper bound for $N$ without the respect to the solution for the ""m-cycle"" with small $m$ . For being able to simply translate the notions and notations of the Simons/deWeger-paper into mine here (as requested by @reuns) I extract from [pg 51]: ""m-cycle"" is the same They use the $(3x+1)/2$ and $x/2$ convention for one step;I use ${3a+1\over 2^A}$ (=""Syracuse""-) convention. The number of odd steps in S/dW-text is $K$ , in my text is $N$ , meaning $K=N$ . The number of even steps in S/dW-text is $L$ , in my text is $S=\text{sum-of-exponents } A_k$ ; the discrepancy seeming obvious by the definitions of the steps such that $K+L=S$ . Their criterion $\Lambda = (K+L) \log(2) - K \log (3)$ use I unnamed
so far as $ S \log (2) - N \log(3)$ In the S/dW-text they use $x_\min$ where I use $a_1$ as
minimal/ initial element of a cycle/1-cycle/m-cycle. They use $x_\min>X_0=301 \cdot 2^{50}$ where I use the newer value $\chi_{\Tiny \text{yoyo}}=87 \cdot 2^{60}$ We apply the same idea for using upper bounds/lower bounds
estimates in general. However in the current or forthcoming answers on Estimation-method 1 and Estimation-method 2 I don't use more knowledge about the
structure of a trajectory than that there must be at least one
element $a_1$ smaller than some sort of average of all elements. In
S/dW text for analyzing the possibility of ""1-cycle"" and ""2-cycle""
they use the knowledge of the structure of the trajectory having
roughly geometric growthrate leading to the disproof of the ""1-cycle"", then of the ""2-cycle"" and then of the ""m-cycle"" for $m\le72$ . (Final remark: the reason for the difference of notations is that I'd developed my ideas initially not being aware of the Steiner/Simons/deWeger work. Accidentally my line of thoughts has been essentially the same, but I feel my choices for notation being more coherent and fluent and so kept it. A translation of the famous ""1-cycle"" disproof into my language/notation can moreover be found here . This might help to understand my arguing here even better)","['collatz-conjecture', 'number-theory', 'numerical-methods']"
3138599,Equilateral and almost equiangular polygons in 3D,"This is a generalization of the following question. Posssible pentagons in 3D Let $P$ be an $n$ -gon in $\mathbb{R}^3$ . Assume that $P$ has equal side lengths and $(n-1)$ interior angles are $\theta$ , where we assume that $0<\theta<\pi$ .
Then what is the possible value for $\theta$ ? We can formulate the question as follows. Let $v_1, \dots,v_n \in \mathbb{R}^3$ denote the sides of $P$ .
We set $v_{i,j}:=\left<v_i,v_j\right>$ , where $\left<,\right>$ denotes the inner product on $\mathbb{R}^3$ .
Assume that the following three equations hold. \begin{align}
&v_1+v_2+\dots+v_n=0. \qquad (1)\\
&v_{i,i}=1 \quad \text{for $1 \leq i \leq n$}.\qquad (2)\\
&v_{i,i+1}=-\cos \theta \quad \text{for $1 \leq i \leq n-1$}. \qquad (3)
\end{align} Here the minus sign to $\cos \theta$ is due to the fact that $\theta$ is an interior angle. I would like to know for what $\theta$ , the equations (1), (2) and (3) have a solution.
I guess that the answer is: (i) If $n$ is odd, then $\frac{\pi}{n} \leq \theta \leq \frac{n-2}{n} \pi$ . (ii) If $n$ is even, then $0<\theta\leq \frac{n-2}{n}\pi$ . Note that $\frac{n-2}{n} \pi$ and $\frac{\pi}{n}$ are the regular polygon and a star polygon in $\mathbb{R}^2$ , respectively. 
Is my answer true?","['geometry', 'discrete-mathematics']"
3138634,How to count all the solutions for $\sum\limits_{i=1}^{n} \frac{1}{2^{k_i}}= 1$ for $k_i\in \Bbb{N}$ and $n$ a fixed positive integer?,"After reading this question , I would like to just count all solutions for: $$\frac{1}{2^{k_1}} + \frac{1}{2^{k_2}} + \frac{1}{2^{k_3}} + \dots + \frac{1}{2^{k_n}}=1$$ for $k_i\in \Bbb{N}$ (we can include $0$ ) with $n$ a fixed positive integer. I noticed that if we denote with $f(k)$ the number of times the value $k$ appears in the sequence $k_i$ then: $$2^n=\sum_{k=0}^{n}{2^{n-k}f(k)}$$ and also $$n=\sum_{k=0}^{n}{f(k)}$$ So the problem is equivalent to count all $f(0), ... ,f(n)$ solutions to the system of the last two equations. I tried to apply the star and bars method and inclusion-exclusion principle, but with no success so far. EDIT 2019-08-14 After reading this answer to another question, I found that the number of solutions is the coefficient of $x^ny^{2^n}$ of the generating function: $$\frac{1}{(1-xy)(1-xy^2)(1-xy^4)\ldots(1-xy^{2^{n-1}})(1-xy^{2^n})}$$ but is it possible to get a formula?","['integer-partitions', 'elementary-number-theory', 'inclusion-exclusion', 'combinatorics']"
3138663,discrete version of normal distribution?,"Just what the title says. I want to model probability of number of sales on a given day. I can't use poisson/binomial, because there are indications that the standard deviation of the sales might differ between two products with the same average number of sales. Is there a discrete version of normal distribution, or something like poisson distribution in which you can independently set the variance?","['statistics', 'probability-distributions']"
3138678,Limits of exponentials: $\lim_{x \to 1} {x^x - x^{x^x} \over {(1-x)^2}}$,"It is a $\frac00$ limit, but I can't seem to figure it out. I tried writing ${x^x}$ as ${e^{x\ln x}} $ and going with L'Hospital from there but I got stuck. Any help is greatly appreciated.","['limits-without-lhopital', 'analysis', 'taylor-expansion', 'limits', 'exponential-function']"
3138695,"Can two monic irreducible polynomials over $\mathbb{Z}$, of coprime degrees, have the same splitting field?","Let $f,g \in \mathbb{Z}[X]$ be monic polynomials. It is possible for distinct monic polynomials over $\mathbb{Z}$ to have the same splitting field. For example $f = x^4 - 2$ and $g= x^4+2$ both have splitting field $\mathbb{Q}(\sqrt[\leftroot{-2}\uproot{2}4]{2}, i)$ . Here $f,g$ are both irreducible with equal degrees. Another example is $f=x^n - 1$ and $g=\Phi_n(x)$ , the $n$ 'th cyclotomic polynomial. The splitting field of both is the $n$ 'th cyclotomic field. In this case, $g$ is irreducible and divides $f$ . The degree of $g$ is of course $\varphi(n)$ . Question: Suppose $f,g$ are monic and irreducible, with coprime degrees. Say $\deg f = n$ and $\deg g = k$ . Let us write $K_f$ and $K_g$ for their splitting fields. Is it possible for these splitting fields to coincide, $K_f = K_g$ ? I am particularly interested in the case $k=n-1$ . I have attempted to approach the question using group theory. If we write $G$ for their common Galois group, then via the action of $G$ on the roots of $f$ and $g$ respectively, we would realize $G$ as a transitive subgroup of both $S_n$ and $S_k$ . Therefore, as $k$ and $n$ are coprime $nk \mid |G|$ . $|G| \leq k!$ $[S_n : G] \geq n(n-1)\cdots(k+1)$ . Furthermore, if $\alpha$ is a root of $f$ and $\beta$ is a root of $g$ , then their stabilizers $Stab(\alpha)$ and $Stab(\beta)$ are subgroups of index $n$ and $k$ respectively. As these are coprime by assumption we would have $G = Stab(\alpha)Stab(\beta)$ . My conjecture is that this is not possible, but I am not able to show it. I seem to remember that, if $l < q$ then $S_l$ is never a transitive subgroup of $S_q$ , except the case $S_5 \leq S_6$ . Here $S_5$ acts transitively on it's $5$ -Sylow subgroups by conjugation, and there are $6$ of them. However, I might be wrong as I am unable to recover a reference for this. Of course, if this was true, we would simply strengthen 2. and 3. above to strict inequalities, unless $n=6$ and $k=5$ . I am also unaware whether there exists such a $f$ with degree $6$ and Galois group $S_5$ acting on the roots in this exotic manner.","['algebraic-number-theory', 'irreducible-polynomials', 'galois-theory', 'polynomials', 'group-theory']"
3138703,Proving concavity of derivative,"Let $f(x)$ be  defined and continuous and derivable for $x>-1$ , $f(0)=1$ , $f’(0)=0$ and $$f''(x) = \frac {1+x}{1+f(x)}.$$ Prove that $f’(x)$ is concave up for all $x>-1$ . My attempt: 
I tried to integrate by multiplying both sides by $dy/dx$ but could not proceed further.","['derivatives', 'monotone-functions', 'ordinary-differential-equations']"
3138710,A binomial identity related to Lucas polynomials.,"I’ve come across  the following identity which seems to be related to the Lucas polynomials $\sum_{j=0}^{\lfloor{\frac{n}{2}}\rfloor}\frac{n}{n-j}\binom{n-j}{j}x^{n-2j}$ : $${\sum_{j=0}^{\lfloor{\frac{n}{2}}\rfloor}\binom{i+j+x}{i-j+1}\frac{n}{n-j}\binom{n-j}{j}}=\binom{x+n+i}{i+1}$$ for $i\ge0$ and $n\ge{i+2}$ .
Is there a simple proof?","['binomial-coefficients', 'discrete-mathematics']"
3138719,"Solution of the recurrence $a_n-3a_{n-1}=2^n$, $a_0=5$","What is the solution of the recurrence $a_n-3a_{n-1}=2^n$ , $a_0=5$ . I think the solution is $$a_n=3^n(5)+\sum_{i=1}^{n-1}3^{n-i}2^i+2^n.$$ Am I right? Can it be simplified further? Thanks beforehand.","['algebra-precalculus', 'recurrence-relations', 'discrete-mathematics']"
