question_id,title,body,tags
2276760,Divisors of the form $4n+1$,"I read a question Number of divisors of the form $(4n+1)$ . In the soultion Any positive divisor of $2^2\cdot 3^3\cdot 5^3\cdot 7^5$ of the form
  $4k+1$ is a number of the form: $$3^a\cdot 5^b\cdot 7^c$$ with $0\leq
> a\leq 3,0\leq b\leq 3,0\leq c\leq 5$ and $\color{red}{a+c}$ being even. There are:
  $$\frac{4\cdot 4\cdot 6}{2}=\color{red}{48}$$ why a+c should be even? Plz use easy language. I don't know the meaning of mod....","['algebra-precalculus', 'elementary-number-theory']"
2276835,"Process bounded at stopped times, constant between.","I have a positive pure jump  process $Y_t$, which is progressive w.r.t. the filtration ($F_t$) and has jump-times $(\tau_j)$. I know that $\sup_{j\in \mathbb{N}}\mathbb{E} Y_{\tau_j}<\infty$ and $\tau_j\rightarrow \infty$ for $j\rightarrow \infty$ almost surely. Does it hold that $\sup_{t<\infty}\mathbb{E}Y_t$ is bounded as well? If not, does there exist a simple counter-example?","['stochastic-processes', 'probability-theory', 'stopping-times', 'probability', 'martingales']"
2276837,Einstein Notation,"Evaluate $$\delta^{i}_{j}\delta^{j}_{i}$$ when $1\leq i,j \leq n$ Simplify $$\delta^{a}_{b}g_{ca}g^{bd}\delta^{c}_{d}$$ when $a,b,c,d\in \{1,2,...,n\}$ in Einstein notation a matrix as a linear transformation is written as $$A=a^{i}_{j}$$ So $$\delta^{i}_{j}\delta^{j}_{i}=I$$ when I is the identity matrix.
But on the other hand the index $j$ is used for summation so the answer will be $$\delta^{i}_{j}\delta^{j}_{i}=\delta^{i}_{i}+\delta^{i}_{i}+...+\delta^{i}_{i}(\text{n times})=1+1+...+1=n$$ What is the correct answer? $$\delta^{a}_{b}g_{ca}g^{bd}\delta^{c}_{a}=\delta_{b}g_{c}g^{b}\delta^{c}$$ How should I continue?","['tensors', 'index-notation', 'differential-geometry']"
2276877,Sum of entries of a matrix,"For a matrix $A \in \mathbb{R}^{n \times n}$, it is clear that the sum of all the entries of $A$ can be expressed as
$$\vec{1}^{T} A \vec{1} = \sum \limits_{i,j} A_{i,j}$$ Now suppose $A,B \in \mathbb{R}^{n \times n}$ are symmetric matrices. Then by the above expression, it is clear that the sum of the entries of the product $AB$ is the same as that of $BA$, even though the two are distinct as matrices.
So 
$$\vec{1}^{T} (AB+BA) \vec{1}=2\vec{1}^{T} AB \vec{1}$$ Do we have any such expression for higher degrees? That is, suppose we form the sum of all possible permutations of a product of $n$ repetitions of $A$ and $m$ repetitions of $B$, and let $Symm(A^nB^m)$ denote this sum. For example, when $n=3$ and $m=2$, the expression has $\binom{5}{2}$ terms as follows
$$Symm(A^3B^2)=A^3B^2 + A^2BAB + A^2B^2A+ABA^2B+ABABA+AB^2A^2+BA^3B+BA^2BA +BABA^2+B^2A^3$$ Can we say anything useful about
$$\vec{1}^{T} Symm(A^nB^m) \vec{1}$$
in terms of $A,B$? This came up while working on a larger problem, so I've skipped the context here as of now. I apologize if the question is a bit vague and open-ended, and will update it promptly based on any feedback. Thanks.","['matrices', 'noncommutative-algebra', 'linear-algebra']"
2276902,Linear matrix equation $AXA^T=B$,"What methods are there to solve the following linear matrix equation for $X$ $$AXA^T=B$$ where $X$ and $B$ are real square matrices, $X$ is symmetric and $A$ might not be square. OBS : I could reduce the problem to a more special case in which $B$ is diagonal.","['matrices', 'matrix-equations', 'linear-algebra']"
2276908,Understanding the distance metric for a 3D space of uniform positive curvature.,"I am reading a cosmology textbook, and the distance metrics for three dimensional spaces exhibiting various curvatures are being presented. My question is about their treatment of a three dimensional space under unifom positive curvature : In polar coordinates, on the two dimensional surface of a sphere, we can express the distance $d\ell$ between two points as a function in their separation in the radial coordinate $r$, and $d\ell^2 = dr^2 + R^2\sin^2(r/R)d\theta^2$ In three dimensions, this extends to $d\ell^2 = dr^2 + R^2\sin^2(r/R)[d\theta^2 + \sin^2\theta d\phi^2]$. Now, my texbook asserts that when the two points whose separation we are measuring are at antipolar locations, we have $r = \pi R \rightarrow r/R=\pi$, which gives $sin^2(r/R) = 0\rightarrow d\ell^2 = dr^2$. But this makes no sense to me. This isn't how spherical coordinates work at all, right...? If I have two point at antipolar points on a sphere, and I measure each of their $r$ coordinates (the length to the point along a line forming angles $\theta$ and $\phi$ from the $z$ and $x$ axes, respectively) as $r_1$ and $r_2$, then shoudln't their separation $dr = |r_1-r_2|$ simply be $2R$? This would mean that $r/R = 2 \neq \pi$ Claiming that $r/R = \pi$ implies that $r$ refers to the actual path length between the points along the surface of the sphere, which is not how spherical coordinates work. Is my error in applying these spherical coordinates to a two dimensional sphere surface, when I am supposed to be thinking about the three dimensional surface under uniform positive curvature? I.e. the image I have in my head of how spherical coordinates work in this context is all wrong, or at least my placement of the points? I am picturing a ""three dimensional space under uniform positive curvature"" as a sphere, like the Earth. But that isn't right, is it? That is just me imagining a 2D surface being curved into a third dimension, when the more accurate analog is to somehow imagine a 3D space being ""curved"" into a fourth dimension?","['physics', 'differential-geometry', 'curvature']"
2276965,What is the Maximum-Likelihood Estimator of this strange distribution?,"Suppose there is a probability distribution for values of $x$ greater than $0$: $$p(x) \propto \frac{m}{(x+1)^{m+1}}$$ And we select from a sample of $\{X_1, X_2, \ldots ,X_n\}$ with all $X_i$ having this distribution.
What is the maximum likelihood estimator of $m$? I tried to do this using the log-likelihood function method but it doesn't work because the log-likelihood function is not well behaved so I ended up concluding that the MLE of $m$ is $m=\max(X_n)$, similar to the continuous uniform distribution. Is this correct?","['parameter-estimation', 'statistics', 'probability', 'probability-distributions']"
2276971,Modulus problem (Complex Number),"If complex number $z(z\neq2)$ satisfies the equation : $z^2=4z+\lvert z\rvert^2+\frac{16}{\lvert z\rvert^3}$, then what is the value of $\lvert z\rvert^4$? My try: I tried to take $z=x+iy$ and solved for the value of $\lvert z\rvert^4$ but everytime ended up getting a value greater than $9$. Hint - The answer lies between $0$ and $9$ , both included.","['complex-analysis', 'complex-numbers']"
2276989,Rotation Invariance of measure,"i know that lebesgue measure is rotation invariant but can anyone please tell me that why $\sigma$ on $S^{n-1}$ is invariant under rotation where $$\sigma (E)=n\mu_n(E_1)$$ where $\mu_n$ is the lebesgue measure on $\mathbb R^n$ and $E_1=\{rx' : ~r\in (0,1] ~\And~x'\in E\}$ and $E\in \mathbb B(S^{n-1})$. as i was able to think it may be due to dependency of $\sigma$ on $\mu_n$ and the property that $\mu_n$ is rotation invariant but i am not sure about my argument. Any type of help will be appreciated.Thanks in advance.",['measure-theory']
2277029,Probability Distribution (solve for k),"I don't know how to get this question started so a push in the right direction would be a great help. Here is the question; Let $X$ be a discrete random variable with probability distribution:
  $$\begin{array}{c|c|c|c|c}
x & 1 & 2 &3 & 4\\\hline
P(X=x) & 1k & 2k & 3k & 4k
\end{array}$$ (a) Solve for k ... so how would I use the formula to input this information in to find $k$? You guys do not have to solve for all the $k$'s. Just show me how to solve for one of the $k$'s, and then I think I can do the rest. Thank you!","['statistics', 'probability', 'probability-distributions']"
2277056,In Trouble by the Sunny Side of Mercury,"I have the following homework problem, and have been able to work out parts a) and b), but not part c). Captain Ralph is in trouble near the sunny side of Mercury. The temperature of the ship's hull when he is at location $(x, y, z)$ will be given by $T (x, y, z) = e^{−x^2 − 2y^2 − 3z^2}$, where $x$, $y$, and $z$ are measured in meters. He is currently at $(1, 1, 1)$. a) In what direction should he proceed in order to decrease the temperature most rapidly? I got: $\left(2e^{-6},4e^{-6},6e^{-6}\right)$ b) If the ship travels at $e^9$ meters per second, how fast (in degrees per second) will the temperature decrease if he proceeds in that direction? I got: $\sqrt{56}e^3$ c) Unfortunately, the metal of the hull will crack if cooled at a rate greater than $\sqrt{{{17}}}e^3$ degrees per second. Describe the set of possible directions in which he may proceed to bring the temperature down at no more than that rate. I'm lost here. I found this other math.slackexchange post , but I'm lost following him. I've tried recreating his steps to no avail for my own problem. I also found this Caltech homework . The problem I'm doing is exactly the same as their last problem (just different numbers), I don't understand how they got to their answer. Also, I am submitting to a computer, so even though the answer might be correct, it could be that the answer isn't the one they're looking for. I submitted to my instructors for more verification on this problem last night, but have still not heard back from them. Any clarification/help would be much appreciated! Edit: I also tried $\sqrt{56}e^3\cos \left(\theta \right)$, which wasn't accepted either. I still haven't heard back from either of the course professors. Edit 2: I found this website where they go through and solve for theta. Additionally, my answer needs to be something in the form of this (whatever it means, please explain it to me if you understand it).
$$\{ai+bj+ck \mid a^2+b^2+c^2=1, 0<[answer]\leq \sqrt{17}e^3\}$$","['multivariable-calculus', 'vectors', 'calculus']"
2277082,Minimizing the condition number of a block matrix,"Let $$A = \left[
    \begin{array}{ccc}
       & B_{(n(m+1)-m) \times (n(m+1))} & \\
      \hline
      Z_{m \times (nm)} & | & S_{m \times n}
    \end{array}
\right]$$ be a block matrix where $B_{(n(m+1)-m) \times (n(m+1))}$ is full rank, $Z_{m \times (nm)}$ is a ""zero matrix"", and $S_{m \times n}$ is some sort of ""choice"" matrix such that there is only one $1$ in each row and each column (the rest being zeros), e.g. $\bigl( \begin{smallmatrix} 0 & 0 & 1 & 0 \\ 1 & 0 & 0 & 0 \\ 0 & 0 & 0 & 1\end{smallmatrix} \bigr)$. More importantly, $n \geq m$. I want to minimize the condition number of $A$ w.r.t. $S$, i.e. $\min_{S} \kappa(A)$. It seems to me that one approach is to use some sort of branch-and-bound technique to cope with the combinatorial explosition, since there are $\frac{n!}{(n-m)!}$ possible ways $S$ can be.","['matrices', 'combinatorics', 'linear-algebra', 'condition-number']"
2277084,Regular Tetrahedron rotation problem,"Say that I have a regular tetrahedron with vertices $a$, $b$, $c$ and $d$ where $a$, $b$ and $c$ sit on a plane. The height of $d$, above the plane, is trivial to obtain given the length of any edge. If I were now to re-orient the tetrahedron such that vertex $a$ remains in contact with the plane but $b$ and $c$ are raised from it, how can I now obtain the height of $d$ above the plane, given the elevations of $b$ and $c$? Presumably this is relatively trivial, but it is outside my area of expertise. One thing to note is that I need to be able calculate the final algorithm fairly quickly in real time using the 'C' language on a fairly low power processor.","['trigonometry', 'geometry']"
2277121,Are eigenvectors preserved by conjugating by a diagonal matrix?,"Let $A$ be a real symmetric matrix and $D$ a real diagonal matrix with non-zero entries along the diagonal. Is it true that the eigenvectors of $D^{-1}AD$ are the same as those of $A$? If not, can anything be said relating the eigenvectors/eigenvalues of these two matrices?","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2277146,Conditions for universal map to profinite group to be surjective?,"Let $G$ be a profinite group, written as the projective limit $\operatorname{lim}(G_i)$ of a diagram of finite groups $G_i$. Let $H$ be a group which is equipped with maps $f_i : H \to G_i$, compatible with the diagram. This induces a universal map $u : H \to G$. What are conditions on $H$ or $f_i$ which ensure the map $u$ is surjective? In particular, I am not sure if $u$ is surjective when the $f_i$ are surjective. Under this assumption, I can certainly find for each finite segment of an element of $G$---viewing elements of $G$ as sequences of elements from each $G_i$ which are sent to each other via the maps in the diagram---an element of $H$ which is sent to it by $u$. But it seems like one would need extra assumptions to find preimages for a general element of $G$.","['profinite-groups', 'group-theory']"
2277181,Product of $\min$ and $\max$ of $n$ i.i.d. random variables.,"Let $X_1, \ldots, X_n$ be i.i.d. continuous uniform random variables on $\left[\vartheta-\frac{1}{2},\vartheta+\frac{1}{2}\right]$ for some $\vartheta \in \mathbb R$. I am trying to solve $\mathbb E[(T-\vartheta)^2]$ where $T(X) = \frac{1}{2}\left(\min_{1 \le i \le n} X_i + \max_{1 \le i \le n} X_i\right)$ or simplify it such that it becomes clear that it is independent of $\vartheta$. I have tried to approach this by a variety of conversion but always end up with a term that involes $\mathbb E\left[\left(\min_{1 \le i \le n} X_i\right)\left(\max_{1 \le i \le n} X_i\right)\right]$. I can't find a way to actually calculate it. Do you have a hint or another way of approaching the task?","['probability-theory', 'probability', 'statistics']"
2277211,Advice on how to overcome obstacles in studying Abstract Algebra [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I am studying math as an undergraduate, and I have run into a problem that seems to common among students- I enjoy analysis, followed by topology, and find the topics intuitive. However, the road has been a bit bumpy with Abstract Algebra.Unfortunately, I would also like to take higher level classes in Abstract Algebra; because when I do get something (after a lot of hard work), it's very rewarding. I also don't want to feel apprehensive toward topics that involve algebra in the future. So, while my undergrad Abstract Algebra sequence had some of my favorite classes as an undergrad, it was also frustrating, extremely time consuming, and nerve wracking. I figured that there had to be something wrong with my approach to studying; and I asked a lot of peers and TAs for tips, but didn't find advice that works for me. I try to  I have noticed a few key things that are preventing me from processing concepts more quickly: I seem process information better through symbolic notation, as opposed to words. For example, if I read ""2 plus 2 equals 4,"" I automatically have to translate this into ""2 + 2 = 4."" Symbolic notation is used a lot in analysis, logic, etc., but most algebra books are very, very wordy. Most of the time, when I read abstract algebra books, I honestly feel like I am reading something in a different language. I feel like I don't know what the words mean conceptually, that I'm not getting the semantics. Definitions in Algebra tend to be high level from the beginning; as in each definition has a bunch of embedded definitions. I feel like I struggle to see the whole picture because of this, especially when definitions, properties and theorems sometimes take up pages of writing. I don't know how to compress these theorems into pieces that are more manageable, and keep track of the overarching theme of definitions and theorems. Examples- Often, I only begin to understand definitions when I have an example that I can play with. I really enjoyed learning about modules upon discovering that abelian groups are modules over the integers. This is opposed to other math classes, where I often get a sense of the implications of a definition/theorem before seeing an example. Conceptualizing structure- At the stage I am at, it seems like Abstract Algebra is all about defining different kinds of structure. The problem is, I often just can't do this, without examples with objects that are ""nice,"" like the integers, complex numbers, fields of characteristic p, etc. Because of this, I tend to think of different structures in algebra as categories of different properties, without really putting these properties together into an intuitive structure. 
The best analogy I can make is this: if you are a very visual person, and need to see a concrete visual representation of a structure to understand it. Then, you probably translate information into a visual form. But there are some structures that are complex, and very hard to ""see"" everything in one picture. So, instead, you create a picture that represents each property of the structure, but this means that it is easy to miss how all those pictures work together to build the overall structure. So understanding is lost in translation. If anyone has had a similar experiences and has found ways to overcome these problems, I would be really grateful for suggestions. I tried to be specific with the problems I am facing. Also, if I need to make improvements to this question, I would also appreciate any advice!","['abstract-algebra', 'book-recommendation', 'advice']"
2277216,Norm of a skew symmetric unitary matrix,"Let $U \in \mathbb{R}^{n \times n}$ be a unitary matrix, $U$ can be nonsymmetric, its eigenvalues can be complex numbers and all have modulus $1$. Is there an upper bound for the maximum singular value of its skew symmetric part (which is not necessarily unitary) depending on its eigenvalues? i.e.: Is there an $f$ such that
$\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) \le  f\left(\lambda_i\left(U\right)\right)$ ? More details: Observe that if $U=I$ (eigenvalues are real) $\Rightarrow \left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 0$, and if $U$ is skew-symmetric (eigenvalues purely imaginary) $\Rightarrow\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right) = 1$. Therefore there is a relationship between the norm $\left\|\frac{U - U^T}{2} \right\|_2 = \sigma_\text{max}\left(\frac{U - U^T}{2}\right)$ and the argument of the eigenvalues of $U$, i.e. $f\left(\lambda_i\left(U\right)\right) = f\left(\text{arg}(\lambda_i\left(U\right))\right)$. Further notes: in my work $U$ is the unitary factor of the polar decomposition of an M-matrix, but this may be irrelevant.","['matrices', 'linear-algebra', 'operator-theory']"
2277334,computing Buffalo numbers,"The Question (with details and generality removed) With proper capitalization, how many grammatical English sentences could ""buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo buffalo"" possibly represent? Motivation The English word 'buffalo' has several definitions: ( noun ) any of several wild bovids: such as water buffalo or cape buffalo, or [long sciencey definition] ( verb, transitive ) see also: bewilder, baffle, bamboozle ['intimidates' is also a common] ( proper noun ) A city in New York. ( adjective ) A demonym for something from Buffalo, New York. The first three are directly from Merriam-Webster; the last one I actually can't find a reputable source for, but I've picked it up by cultural osmosis. This has led to the observation that, ignoring capitalization, you can put any number $n$ of copies of the word 'buffalo' to make a valid English sentence. (or, to use the internal source ...) Examples The usual example here is the $n=8$ case, which is usually capitalized Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo. and means The New York bovids that New York bovids intimidate, themselves intimdate New York bovids. However, $n=8$ can also be capitalized differently: Buffalo buffalo buffalo Buffalo buffalo Buffalo buffalo buffalo which means New York boivds intimidate the New York bovids that New York bovids intimidate. This is not only a different grammatical parsing, but is in fact a different semantic meaning. In the second sentence, there could be three groups of bovids, one of which is intimidated by the other two. This is impossible with the first sentence— if there are three groups of bovids, they intimidate each other sequentially. There is yet another interpretation: Buffalo buffalo buffalo buffalo Buffalo buffalo buffalo buffalo. which can mean The bovids that bovids intimidate, themselves intimidate the New York bovids that bovids intimidate. This sentence is clearly different, since there could be as many as four groups of bovids, and only one of them need be from New York. However, this is not what I meant when I wrote the sentence. I meant that The New York bovids— namely, those that the bovids who intimidate New York (!) do intimidate— intimidate bovids. This is also trivially different from any of the other sentences above; never before have we encountered bovids that intimidate an entire city! This example shows that even with the same capitalization, we can get different semantic meaning. The Question Using the four definitions from the motivation, and assuming that you are allowed to capitalize at your leisure, how many grammatical English sentences are given by simply writing down the word 'buffalo' $n$ times (and a period)? Feel free to call this number the Buffalo number $B(n)$, and say it counts the number of valid parsings :) ... or alternatively, that it counts Buffalo functions , i.e. $f:\{1,2,\cdots, n\}\to\{1,2,3,4\}$ satisfying... whatever they're supposed to. Caveats and Variations The Wikipedia page that I cited above permits the sentence ""Buffalo."", a command meaning ""Hey, you! Intimidate them!"". English permits this kind of obnoxiousness, where you have both an implied subject by using the imperative mood, and an implied direct object for a transitive verb. Such sentences should be allowed by this count, but I wouldn't mind too much if you removed this restriction. ""Buffalo buffalo buffalo?"" either means either ""Do bovids actually intimidate other bovids?"" (i.e., a reasonable complaint about this entire setup) or ""Do New York bovids actually intimidate things?"". These are both different from the plausible meanings of ""Buffalo buffalo buffalo."" ""Buffalo, buffalo buffalo."" is another possibility (in the imperative mood) but with different punctuation, in which the governor of New York is suggesting that the city should intimidate the bovids in their midst, perhaps to avoid having to come up with ways to describe these unlikely scenarios. Such sentences should not be allowed by this count, but I would be impressed if you had a method of counting under which it was easier to count them. I would be rather surprised if there is a nice answer to this question which doesn't essentially give a generating function (on how many copies of each buffalo definition are being used) and certainly wouldn't mind if you expressed the answer in that language. Tiny Computations $B(1)=1$. The example above shows $B(1)\geq 1$, and any English sentence requires a verb. QED. $B(2)\geq 2$. One of the two words must be a verb. The other cannot be an adjective, since any adjective requires a noun. Therefore $B(2)\leq 4$, and the following are valid parsings: Hey, you! Intimidate bovids! Hey, you! Intimidate New York! Bovids intimidate things. Therefore $B(2)=3$, since the verb in ""Buffalo$^3$ buffalo$^2$ is not correctly conjugated (""Buffalo buffalos."" would work, but it's not what we're counting...) $B(3)\geq 4$, and I think I understand a proof that equality holds, but I'm having a hard time writing it. Hey you! Intimidate New York bovids! Bovids intimidate bovids. Bovids intimidate New York. New York bovids intimidate things. (Some thoughts on the proof: I am pretty sure that there cannot be two verbs in the sentence; the only one that I'm not sure of is if you can't do something funky with implied objects on ""buffalo$^X$ buffalo$^2$ buffalo$^2$. New York cannot intimidate anything, since the conjugation is wrong.) Disclaimer: In case you've not figured this out by now, this is all for entertainment value only. I believe that the question is now (mostly) unambiguous, although of course I'm happy to specify something if need be. I am aware that, even if it is properly disambiguated, the question is maybe too hard, and maybe should be split into a couple different questions. But I would be happy with asymptotics, or even nontrivial one-sided bounds.","['generating-functions', 'combinatorics', 'recreational-mathematics']"
2277358,How do you derive the quadratic formula using calculus?,"The quadratic formula: $$f(x)=ax^2+bx+c=0$$ $$x=\frac{-b \pm \sqrt{b^2-4ac}}{2a}$$ I remember a tutor once showing me a method for deriving the quadratic formula using calculus somehow. This was around 20 years ago and I can't even remember the tutor's name. I'd really like to learn this method. Just to clarify, I do know how to derive it using the ""Completing the square"" method. I was linked to the solution here: https://www.google.com/amp/s/threesixty360.wordpress.com/2008/10/19/using-calculus-to-generate-the-quadratic-formula/amp/ But I am stuck at one step. Start with: $$f(x)=ax^2+bx+c$$ We want: $$f(x)=0$$ The first derivative gives: $$f'(x)=2ax+b$$ Which leads to this: $$f(x)=c+\int_0^x (2at+b)dt$$ I can't see why the $t's$ were introduced here. If anyone has any other methods I'd really like to see them also.","['linear-algebra', 'calculus', 'quadratics']"
2277416,How do we evaluate the closed form for $\int_{-\infty}^{+\infty}{(-1)^{n+1}x^{2n}+2n+1\over (1+x^2)^2}\cdot e^{-x^2}\mathrm dx?$,"Proposed: $$\int_{-\infty}^{+\infty}{(-1)^{n+1}x^{2n}+2n+1\over (1+x^2)^2}\cdot e^{-x^2}\mathrm dx={\sqrt{\pi}\over 2^{n-2}}\cdot F(n)\tag1$$
  Where is n integer, $n\ge1$ I am struggled to find the closed form for $(1)$ Where $F(1)=1, F(2)=3, F(3)=9, F(4)=21, F(5)=63, ...$ How can we find the closed form for $(1)?$","['gaussian-integral', 'calculus', 'closed-form', 'integration', 'definite-integrals']"
2277425,Largest Power of $11$ that can divide $^{2n}C_n$,"I need help answering this question. If $f(n) =$ the greatest integer $k$ for which $11^k$ divides
$\binom{2n}{n}$, what is the maximum value of $f(n)$ for all integers $1 \le n \le 10000$?
$${2n \choose n}=\frac{(2n)!}{(n!)^2}$$ I have figured out that $2$ is the greatest power of $11$ which divides $\binom{10000}{5000}$. I don't know how to show that this is/isn't the greatest value that $f(n)$ can take on for $\binom{2n}{n}$.","['number-theory', 'combinatorics', 'divisibility']"
2277453,a contour integral and residue,Evaluate : $\oint_C (1+z+z^2)[e^{2/z}+e^{2/(z-2)}+e^{2/(z-3)}]dz$ over the circle $C:|z|=\frac{1}{4}$. It is clear that terms that involve $e^{2/(z-2)}+e^{2/(z-3)}$ need not to be considered here as they have singularities outside the given region. But I have problem to calculate the residue (at the pole $0$) within the region of the other remaining terms here. Then we should apply Cauchy's residue theorem to evaluate that integral. So how to calculate the residue at the pole $0$?,"['complex-analysis', 'contour-integration']"
2277466,comparing the smallest positive roots of $\cos(\sin x)=x$ and $\sin(\cos x)=x$,"Given $x_1$ is a root of $$\sin(\cos x)=x$$ and $x_2$ is a root of $$\cos(\sin x)=x$$ when $x_1,x_2 \in [0 \:\: \frac{\pi}{2}]$ Then which  is greater $x_1$ or $x_2$ we have first equation as $$\arcsin x=\cos x$$ and by graphs it has one solution $x_1 \lt 1$  and similarly from second equation $$\arccos x=\sin x$$ and by graph  it also has $x_2 \lt 1$ as a solution. but any clue to decide which is greater","['algebra-precalculus', 'trigonometry', 'inverse-function']"
2277500,"Calculate $\int_{0}^{1} (x-f(x))^{2016} dx$, given $f(f(x))=x$.","This question was asked in an entrance test for an undergraduate mathematics program today, held all over India. Question: $f$ is a differentiable function in $[0,1]$ such that $f(f(x))=x$ and $f(0)=1$. Find the value of $\int_{0}^{1} (x-f(x))^{2016} dx$. I tried to solve it by substituting $f(f(x))$ in place of $x$, but could not proceed much further. Any hints or solutions will be appreciated.",['definite-integrals']
2277573,Show that $L_p =\frac{1^p+2^p+3^p+...+(p-1)^p}{p}$ is an integer and generalize the integer for all odd integers p,"proof the integer: From Fermat's little theorem, if $\gcd(a,p) = 1$, then we have: $a^p \equiv a \pmod p$. Since $p$ is only divisible by itself and 1, so we have:
$1^p+2^p+3^p+...+(p-1)^p \equiv 1+2 + ...+ p-1 \pmod p$ $\equiv p(p-1)/2$. Since p is odd prime, $p -1 = 2k$. So we have: $p(p-1) = kp\equiv 0 \pmod p$. Therefore $L_p$ is an integer. The question is how can I generalize the integer $L_p$ for all odd integers $p$. That is, in the expression for $L_p$ replace the exponent $p$ with another non-constant quantity depend on $p$ in a way that $L_p$ remains an integer for all odd integer $p$, and justify the choice of exponent.","['number-theory', 'divisibility', 'elementary-number-theory']"
2277621,"If $a_n$ is sequence of positive numbers such that $\sum a_n$ converges, then does $\sum \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}}$ converge?","Failed attempts : Let $x_n= \frac {{(a_n)}^{\frac 14}}{n^{\frac 45}}$. 1) By limit comparison test - Taking $y_n=a_n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 45} {a_n}^{\frac 34}}$ (Leads nowhere.) Taking $y_n=\frac 1n$, $\lim \frac {x_n}{y_n}=\lim \frac 1{n^{\frac 15} {a_n}^{\frac 14}}$ (Leads nowhere). Similarly taking $y_n={a_n}^2,\frac 1{n^2}$ doesn't work. Still searching for $y_n$ that works. 2) By comparison test - $\frac {{(a_n)}^{\frac 14}}{n^{\frac 45}} \le {(a_n)}^{\frac 14}$ (Leads nowhere since we can't say whether $\sum{a_n}^{\frac 14}$ converges). 3) By ratio test - $\lim\frac {{(a_{n+1})}^{\frac 14}}{{n+1}^{\frac 45}} \frac {n^{\frac 45}}{{a_n}^{\frac 14}}=\lim \frac {{a_{n+1}}^{\frac 14}}{{a_n}^{\frac 14}}$ (Leads nowhere since we don't know whether the last limit is less than $1$) Where am I making mistakes? Can you provide some hints?","['real-analysis', 'limits', 'sequences-and-series', 'proof-verification', 'convergence-divergence']"
2277632,What is the least value of $k$ for which $B^k = I$?,"I found the following question in a textbook, and I wasn't able to solve it. Suppose $A$ and $B$ are two non-singular matrices such that $B \neq I$, $A^6 = I$, and $AB^2 = BA$, then what is the least value of $k$ for which $B^k = I$? My attempt We can see that $A^6 = I$, so, I tried to manipulate the second equation to involve $A^6$, so that we can get the identity matrix. But this didn't work, and I ended up getting something of the form, $B^{-1}AB^2 = A$. I wasn't able to reduce it further. Answer The answer is $k = 127$.","['matrices', 'linear-algebra']"
2277648,Relation between Jacobi field and Killing field,"Let $M$ be a manifold, $X$ a vector field on $M$. 
Question: If for every point $p\in M$, there exist a neighborhood $U$, such that for any radial geodesic $\gamma$ in $U$, $X(\gamma)$ is a Jacobi field, then, must $X$ be a Killing field?","['riemannian-geometry', 'differential-geometry']"
2277650,Bieberbach theorem,"Bieberbach theorem say that every discrete cocompact subgroup in $Isom(\mathbb{R}^n )$ contains a translational subgroup of finite index.
(the translations forming an abelian normal subgroup of finite index) 1) Why this subgroup is a lattice in $ \mathbb{R}^n$ ? 2) Can you give me a simple proof in dimension 2 ?.","['differential-geometry', 'group-theory', 'integer-lattices']"
2277706,How to prove $\sum\limits_{i=0}^{\lfloor\frac{r}{2}\rfloor}\binom{r}{i}\binom{r-i}{r-2i}2^{r-2i}=\binom{2r}{r}$,Please help me to prove $\sum\limits_{i=0}^{\lfloor r/2\rfloor}\binom{r}{i}\binom{r-i}{r-2i}2^{r-2i}=\binom{2r}{r}$. By computer search I have found these for r varies from 0 to 10000. How to prove this for general $r\in N.$,"['number-theory', 'combinatorics', 'elementary-number-theory']"
2277739,Why is the denominator $N-p-1$ in estimation of variance?,"I was recently going through the book Elements of Statistical Learning by Tibshirani et.al. In this book, while explaining the ordinary least squares model, the authors state that assume that $y_i \epsilon \mathbb{R}$ represents the observed variables, $\hat{y_i}$ represents the model output and $\mathbf{x_i} \epsilon \mathbf{R}^{p+1}$ represent the inputs. If the $y_i$s are assumed to be uncorrelated and have constant with variance $\sigma$, then the unbiased estimate of variance is $\hat{\sigma} = \frac{1}{\left (N-p-1 \right)}\sum\left( y_i - \hat{y_i} \right)^2$, summation being done from $i=1$ to $i=N$. Note that $p$ has been used here to denote the dimensionality of $\mathbf{x_i}$s. My question is why is the factor in the denominator $N-p-1$ while estimating the variance of $y_i$s i.e. $\hat{\sigma}$ ? From my understanding if the $y_s$s are real numbers that have constant variance, the factor should be equal to $N-1$.","['statistics', 'variance', 'least-squares']"
2277783,group generated by matrices of finite order,"Let $K$ be a field, and suppose $A,B$ are $n{\,\times\,}n$ matrices with coefficients in $K$ such that 
$$A^p =I\;\;\;\text{and}\;\;\;B^q=I$$
for some positive integers $p,q$. Let $G$ be the multiplicative group of matrices generated by $\{A,B\}$. Two questions: $\;$Must $G$ be finite? $\;$Assuming $G$ is finite, must $G$ be isomorphic to a subgroup of $S_m$, where $m = \text{lcm}(p,q)$? Since it appears that questions $(1)$ and $(2)$ both got quick ""no"" answers, I'll add one more question, a variant of question $(1)$: $\;\;\;$3. What if $A,B$ also satisfy some multiplicative identity not algebraically derivable from $A^p=I$ and $B^q=I$. In other words, some word in $A,B,A^{-1},B^{-1}$ is equal to $I$. Now must $G$ be finite? Based on the answers already given for questions $(1)$ and $(2)$, I'm not optimistic about the chances for a ""yes"" answer to question $(3)$. Thanks for the responses so far.","['matrices', 'permutations', 'group-theory']"
2277795,"Prove that $\forall a\in \mathbb{R\smallsetminus Q}$, there exist infinitely many $n\in \mathbb N$ such that $\lfloor{an^2}\rfloor$ is even.","Question: Prove that  $\forall a\in \mathbb{R\smallsetminus Q}$, there exist infinitely many $n\in \mathbb N$ such that $\lfloor{an^2}\rfloor$ is even. If $a=\sqrt2$ and $x^2-2y^2=1(x,y\in\mathbb N)$ then $x-\sqrt2y=\dfrac{1}{x+\sqrt2y},$
$$9xy-9\sqrt{2}y^2=\dfrac{9y}{x+\sqrt2y}\in(3,4)$$
so $\lfloor{(3y)^2\sqrt2}\rfloor=9xy-4$ is even since $y$ is even. If $k\in\mathbb N,\sqrt k \notin \mathbb N$ then we can prove it for $a=u+v\sqrt k,u,v\in\mathbb Q$ as above. I know that if $a$ is an counterexample and the continued fraction representations of$a=[a_0; a_1,a_2,....]$ then $a_{k},a_{k+2},a_{k+4},\cdots$ must be all even for some $k\in \mathbb N$.","['number-theory', 'equidistribution', 'continued-fractions', 'ceiling-and-floor-functions']"
2277843,Kollo Skewness - Calculation and meaning,"I would like to calculate the Kollo Skewness which is defined here in Definition 2. $b(x) = E\left [ \sum (Y_{i}Y_{j})Y \right ]$
where $Y = \Sigma ^{-1/2} (X-\mu)$ with $E[X] = \mu$ and $\Sigma$ is the dispersion matrix (Covariance Matrix). I am strugling to grasp the intuition behind the Kollo Skewness. A good first step would be understanding the thinking behind $Y$. What is the effect of premultiplying with the inverse of the cholesky decomposition of the Covariance Matrix? I have also written a Matlab function to calculate the sample Kollo Skewness.
$\widehat{b(X)} = \frac{1}{n}1_{pxp}\star\sum_{n}^{i=1}(y_{i}y_{i}'\otimes y_{i})$ function [ B ] = KolloSkew( X )
%sample estimate for Kollo Skewness
%   
%   Calculation according to Def. 3 Kollo (2008)
%
%   Input:
%   X: mxn of sample data
%   
%   Output:
%   B: Kollo Skewness Vector

[m, n] = size(X);
S = cov(X);

%%  Y
Y = zeros(size(X,2));
for j=1:n
    % yi = S^(-1/2)*(xi-x.mean)
    Y(:,j) = (chol(S)')^(-1)*(X(1,:)'-mean(X(:,1)));
end

%% B = 1/n 1pxp star sum
temp1 = 1/m * ones(n); %temp1: 1/n 1pxp
temp2 = zeros(4,2); %temp2: sum yi*yi' kron yi*yi'
for j=1:n
    temp2 = temp2 + kron(Y(:,j)*Y(:,j)',Y(:,j)); 
end


B = star(temp1, temp2);

end

%% star product
function [ starprod ] = star( A, B )
% star product according to Def. 1 Kollo
r = size(B,1) / size(A,1);
s = size(B,2) / size(A,2);

n = size(A,2);
m = size(A,1);

temp3 = zeros(n,1);
for i=1:m
   for j=1:n
       temp3 = temp3 + A(i,j) * B(1+(i-1)*r:i*r, 1+(j-1)*s:j*s);
   end 
end

starprod = temp3;
end","['statistics', 'linear-algebra']"
2277844,"Prove that subset $S$ of $[0, 1]$ of total length greater than $0.6$ contains two elements such that their difference is excatly $0.1$","Let $S$ be a subset of $[0, 1]$ consisting of a finite number of intervals. How to prove that if the total length of intervals from $S$ is greater than $0.6$ then $S$ contains two numbers such that their difference is exactly $0.1$?","['number-theory', 'combinatorics']"
2277852,How to appreciate Riemannian geometry,"I'm currently following an introduction to Riemannian geometry i.e. connections, curvature and isometric immersions (the Gauss, Codazi and Ricci equations). I find the introduction to Riemannian geometry interesting, but whenever I look at some theorems beyond the introductory topics they seem quite artificial and not intuitive. Also I can't see why they are interesting for us? There are many examples, one of them is Schur's lemma which goes as follows: Let $M$ be a Riemannian manifold of dimension $n \geq 3$. Suppose that for every plane $\pi$ in $T_pM$, $K(\pi)$ (the sectional curvature) has the same value $c(p)$. Then $c(p)$ is a constant function. First the theorem only works in $n\geq 3$, but my main problem is that my intuition lives in surfaces in $\mathbb{R}^3$ where there is only one plane $\pi $ in  $T_pM$. Hence what is the intuition behind this lemma? How can one see the beauty of such theorems? This is however unfortunate since the theory of Riemannian geometry is a popular branch of mathematics which implies many people are interested in it (and probably see the beauty of such theorems and problems). The purpose of my question is to get some intuition or feeling for it so I can appreciate such theorems. EDIT 1: The very general question, has more concrete subquestions: Why are we interested in the relation between curvature and the shape of manifolds, what is the importance of this? How does one intuitively see which relations (in 1)) one can expect and which not? (for example if the sectional curvatures are $\leq 0$ then for what properties of $M$ can one hope for?) Some theorems hold only in specific higher dimensions, for example Schur's lemma above. How does a mathematician find such theorems and proofs? EDIT 2: As suggested in a comment, maybe these questions can be answered by giving interesting examples of the uses of Riemannian geometry.","['riemannian-geometry', 'soft-question', 'motivation', 'geometry', 'differential-geometry']"
2277858,"Representing real numbers in $[0,1]$ with a binary tree","All we know Cantor's famous argument about counting real numbers in [0,1]. He basically suppose that we list every real number in [0,1] with a binary representation, then we use the diagonal argument to find a real number which is not listed there. Let's think of an infinite binary tree. Root node is 0, and following every node has 0 as its left child and 1 as its right child. So, as far as I can see every branch of this tree corresponds to real number in [0,1] with a binary representation, which means that we can find every element in Cantor's list in this tree. Moreover, even the element constructed by diagonal argument can be found there. If we count the nodes of this infinite binary tree, we have countable set since countable union of countable sets is countable. However, the set of infinite binary sequences is uncountable. What is going wrong here?","['cardinals', 'elementary-set-theory']"
2277865,"How to prove that there could be only finite or countable set of non-intersecting intervals in $[0,1]$?","Suppose we have $\{I_{\alpha}, \alpha \in \mathbf{A}\ - arbitrary\ set\ of\ indexes \}$ - a family of non-intersecting intervals, $ I_{\alpha} \subset [0,1]$. How to prove that its cardinality is not more than countable? UPD : I concluded, that, in fact, there is a more common statement with quite the same proof as proposed in my own answer. Concretely, Let $\mathbb{H}$ be a separable metric space, B = $\{B_{\alpha}, 
\alpha \in \mathbf{A}\}$ - family of open balls in this space. Than 
the cardinality of B is not more than countable. As I stated, the proof is quite the same, but instead of $\mathbb{Q}$ one must consider constructing a map from B to S , where S is a countable dense set in $\mathbb{H}$.","['real-analysis', 'metric-spaces', 'infinity', 'elementary-set-theory']"
2277928,Convergence of a monotonic sequence,"Assume $a_n\geq 0$ is a sequence of positive real numbers which satisfy the following inequality: for each $n,m\in\mathbb{N}$, we have $$(n+m)a_{n+m}\leq na_n+ma_m.$$ I can't show the convergence of this seemingly well-behaved sequence (I am guessing it does converge, and that one should use a monotonic trick?). Any hint is appreciated. I found this in a list of exercises in sequences and convergence, and I believe it should be elementary because so are the rest of the problems in that list. Thanks in advance! For completeness, my -failed- attempts so far: Using induction, one proves esily that for any $p,n\in\mathbb{N}$ the inequality $$a_{pn}\leq a_n.$$ From here one might conclude convergence of subsequences of the form $\{a_{{p^k}n}\}_k$ for any $p,n\in\mathbb{N}$. But unfortunately, I can't get an argument from here. Another promising-looking inequality is $$a_{n+1}\leq \frac{n}{n+1}a_n+\frac{a_1}{n},$$ but again this does not suffice. Finally, I tried playing with the sequence (for some $r$ fixed) $$A_n=\min\left\{\frac{nx_n+x_1}{n+1},\frac{(n-1)x_{n-1}+x_2}{n+1},\dots,\frac{(n-1)x_{n-r}+x_{r+1}}{n+1}\right\},$$ becase its a trick I have seen elsewhere when solving exercises on convergence of bounded sequences. I don't think this is the way, because I believe this would only work if you had an estimate of $a_{n+1}$ as a ""convex"" combination of the $r$-tail. EDIT: It seems like everyone can prove this result one way or another. A friend of mine immediately suggested to apply Fekete's Subadditive Lemma here to the sequence $b_n=n a_n$.",['sequences-and-series']
2277978,Show $\lim_{|a|\to\infty}\int_a^{a+\epsilon} f(u(x)) d x = 0$,"I have the following problem: let $u\in L^\infty(\mathbb R) \cap L^1(\mathbb R)$ and $f\in C^\infty(\mathbb R)$ with $f(0)=0$. Show that: $$ \lim_{|a|\to\infty}\int_a^{a+\epsilon} f(u(x)) d x = 0 \qquad\text{for all }\epsilon>0$$ Now although $u\in L^1(\mathbb R)$ does not imply $u(x)\to 0$ as $|x|\to\infty$, it does imply that the average of $u$ goes to to zero in the sense that $$ \lim_{|a|\to\infty}\int_a^{a+\epsilon} |u(x)|dx =0 \qquad\text{for all }\epsilon>0$$ So an estimate of the form
$$ \int_a^{a+\epsilon} f(u(x)) d x \approx f\Big(\int_a^{a+\epsilon} u(x) d x\Big) $$ is needed, which seems reasonable but I can't find a rigorous proof.","['lp-spaces', 'integration', 'integral-inequality']"
2278009,"Alternative Proof for a Lemma in Barry Simon's ""Operator Theory""","I've been working on a presentation based on chapters $3.6$, $3.7$ of Barry Simon's book ""Operator Theory"". Due to having only a limited amount of time to present the topic, I cannot introduce all the notions from the previous chapters of his book. This results in problems with some of the proofs in the book, which I had to change so as not to include some of these notions.  There is one lemma at the beginning of chapter $3.6$, which I can't prove though: $\mathbf{3.6.3.Lemma:}$ Let $A\in L(H)$ (where L(H) is the set of BLOs over a Hilbert space H) satisfy for all Orthonormal Systems $(\phi_n)_{n\in\mathbb{N}},(\psi_n)_{n\in\mathbb{N}}$: $${\langle\psi_n,A\phi_n\rangle}\overset{n\rightarrow +\infty}{\longrightarrow}0$$
  Then $A$ is compact. Barry Simon uses the concept of the Polar Decomposition to prove this with the help of another Lemma from a previous chapter ($3.1.14$) and his proof is pretty straightforward. However, the people attending the presentation don't know what a Polar Decomposition is, nor do they have the prerequisite knowledge for me to quickly introduce the concept (This would require them to know about projections and partial isometries). Additionally, Lemma $3.1.14$ which is used in this proof seems to be a special case of what we are trying to prove. Thus I'm trying to prove this using only basic properties of compact operators, such as Schauder's Theorem, the fact that the limit of a sequence of finite rank compact operators is compact and the equivalent definitions for compactness. I'm also allowed to use the canonical decomposition and singular values. What I can also use is the property that the Eigenvalues of compact operators converge to zero. The basic difficulty I'm facing is that this lemma is unrelated to the topic I'm presenting and it requires knowledge, with which I'm not very familiar. I have attempted to prove it by approximating it by a sequence of finite rank operators, the problem is that I can't think of a sequence, which takes advantage of the assumption. Using the canonical decomposition does not work since it requires that the operator be compact (so I can only use it in a proof by contradiction). So I don't have any idea how to start. Another approach I have tried is proving the contraposition using the property of the Eigenvalues. Basically, assuming that the Eigenvalues (so also the singular values) converge to something other than $0$ (which would imply that $A$ is not compact) then the inner product with $\phi$ and $\psi$ does not always converge to zero, but I haven't got far. So I would appreciate it if any of you could give me some assistance in proving this (hints, ideas etc.) -Thanks in advance!","['functional-analysis', 'compact-operators', 'operator-theory', 'hilbert-spaces']"
2278023,Quotient of the linear group by the subgroup of matrices with positive determinant,Let $G=GL_n(\mathbb{R})$ be the group of invertible $n \times n$ matrices. Let $H$ be the subgroup of the matrices with positive determinant. It is obviously a normal subgroup. What can be said about the quotient $G/H$?,"['abstract-algebra', 'group-theory']"
2278063,Calculate the number of ways to paint $5$ buildings with $4$ colours such that all $4$ colours must be used,"A developer has recently completed a condominium project in a valley. There are    blocks of buildings $A$, $B$, $C$, $D$ and $E$ as shown in the diagram below. The developer has    colours available to paint the buildings. Each block can only be painted using a single colour. Find the number of ways to paint all the    blocks if all $4$ colours must be used. My attempt: We have $5$ ways to choose $4$ buildings with $4$ different colours. Among the $4$ buildings, we have $4!$ ways to paint them using $4$ different colours. So my answer is $120$. However, the answer given is $240$. What is my mistake?","['number-theory', 'combinatorics', 'permutations']"
2278071,Composition of bounded operator and compact operators,"In Hilbert space $H$, is it true that the composition of operators  $ST$ and $TS$ of the bounded operator $S$ and the compact operator $T$ are compact?","['functional-analysis', 'compact-operators', 'operator-theory', 'hilbert-spaces']"
2278087,Maximum value of a complex polynomial on the unit disk,"The polynomial is $p(z)=\sum^n_{k=0} a_kz^k$. And I want to prove the following inequality on the unit disk$$\max_{B_1(0)}|p(z)|\geq |a_n|+|a_0|$$ By the maximum modulus principle, the maximum must be on the unit circle and greater than $|a_0|$ by considering $p(0)$. However, I cannot make further conclusions from this, since any attempt of using the triangle inequality will result in the opposite direction of the wanted result. I have also seen a similar problem , although I can conclude $\max_{|z|=1}|p(z)|$is greater than any of the two on RHS, but since there is no relation of $\max_{k\in\{0,\ldots,n\}}|a_k|\geq|a_0|+|a_k|$, a tighter bound is needed. I also tried expanding it into trig functions, and consider the roots, but it didn't work as expected.","['complex-analysis', 'maximum-principle']"
2278106,"Examples of Groups (resp. Rings, Fields, etc.) Which Are Isomorphic to a Proper Subgroup (resp. Subring, Subfield, etc.)","Was just reading this question When is a group isomorphic to a proper subgroup of itself? and was wondering not about the conditions for being isomorphic to a proper subgroup, subring, etc., but about examples of these things happening. (Certainly one necessary condition is that our algebraic object be infinite as a set. Otherwise we cannot have a bijection between the initial set and a proper subset.) I believe one example was given in the above link using even powers of a polynomial ring $R[x]$ and the ring $R[x]$ itself. I believe another example is in Dummit and Foote with the roots of unity or something like this. Anyway, have at it! (Here is another related post: Rings with isomorphic proper subrings ) (Feel free to also post answers with maybe manifolds which are homeomorphic/diffeomorphic/biholomorphic to proper submanifolds or things like that if you have some favorites!) (The only category which I know excludes this business is algebraic geometry...but only kinda if you deal with incomplete intersections...)","['abstract-algebra', 'ring-theory', 'infinite-groups', 'group-theory']"
2278247,Proving that SSE and SSR are independent [duplicate],"This question already has answers here : How to prove SSE and SSR are independent (2 answers) Closed 2 years ago . I'm trying to show that SSE and SSR are independent (conditionally on X) but I have to use the following steps/hint.
[Hint: Notice you have to consider SSE and SSR as random variables, so be careful how you define them. You may want to use the result that two linear forms U = AX and V = BX, with A and B being constant matrices and X is Normal, are independent iff Cov(U, V) = 0]. I know that the question was posted before but I'm not finding how to prove it using this hint. Any help is much appreciated!","['statistics', 'linear-regression', 'least-squares', 'discrete-mathematics']"
2278253,Prove the convergence or divergence of $\sum_{n=0}^\infty \left( (n+1)^\frac1n-(n)^\frac1n\right)$,"Let $$u_n:=(n+1)^\frac{1}{n}-(n)^\frac1n$$ How to prove the convergence  or divergence of the series $\sum u_n$ ?
I tried the usual criteria but failed.","['convergence-divergence', 'sequences-and-series', 'calculus']"
2278276,How can we extend the Itō isometry for elementary processes to the class of square-integrable processes?,"Let $\lambda^1$ denote the Lebesgue measure on $\mathbb R$ $T>0$ $I:=(0,T]$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space $(\mathcal F_t)_{t\in\overline I}$ be a filtration of $\mathcal A$ $(W_t)_{t\in\overline I}$ be a $\mathcal F$-Brownian motion on $(\Omega,\mathcal A,\operatorname P)$ $\Phi:\Omega\times\overline I\to\mathbb R$ is called elementary $\mathcal F$-predictable $:\Leftrightarrow$ $$\Phi_t=\sum_{i=1}^kX_i1_{(t_{i-1},\:t_i]}\;\;\;\text{for all }t\in\overline I\tag1$$ for some $k\in\mathbb N$, $\mathcal F_{t_{i-1}}$-measurable $X_i$ with $\left|X_i(\Omega)\right|\in\mathbb N$ and $0\le t_0<\cdots<t_k\le T$. Let $$\mathcal E:=\left\{\Phi:\Omega\times\overline I\to\mathbb R\mid\Phi\text{ is elementary }\mathcal F\text{-predictable}\right\}\;.$$ Let $\Phi\in\mathcal E$ with $(1)$ $\Rightarrow$ $$\int_0^T\Phi_t\:{\rm d}W_t:=\sum_{i=1}^kX_i\left(W_{t_i}-W_{t_{i-1}}\right)$$ is called Itō integral of $\Phi$ with respect to $W$ and $$\int_a^b\Phi_t\:{\rm d}W_t:=\int_0^T\Phi_t1_{(a,b]}(t)\:{\rm d}W_t$$ is called Itō integral of $\Phi$ with respect to $W$ from $a\in[0,T]$ up to $b\in[a,T]$ . Let $\Phi,\Psi\in\mathcal E$ and $0\le a\le b\le T$ $\Rightarrow$ $$\operatorname E\left[\int_a^b\Phi_t\:{\rm d}W_t\int_a^b\Psi_t\:{\rm d}W_t\mid\mathcal F_a\right]=\operatorname E\left[\int_a^b\Phi_t\Psi_t\:{\rm d}t\mid\mathcal F_a\right]\tag2\;.$$ Let $$\mathcal R:=\bigcup_{F\in\mathcal F_0}F\times\left\{0\right\}\cup\bigcup_{0\le s<t\le T}\bigcup_{F\in\mathcal F_s}F\times(s,t]$$ and $$\mathcal P:=\sigma(\mathcal R)\;.$$ $\Phi:\Omega\times\overline I\to\mathbb R$ is called $\mathcal F$-predictable $:\Leftrightarrow$ $\Phi$ is $\mathcal P$-measurable. Note that $\mathcal E$ is a dense subspace of $$\mathcal I^2:=\left\{\Phi\in\mathcal L^2\left(\operatorname P\otimes\left.\lambda^1\right|_{\overline I}\right):\Phi\text{ is }\mathcal F\text{-predictable}\right\}$$ and $\mathcal I^2$ is a closed subspace of $\mathcal L^2\left(\operatorname P\otimes\left.\lambda^1\right|_{\overline I}\right)$. Question : How can we extend $(2)$ to $\mathcal I^2$? Let $\Phi,\Psi\in\mathcal I^2$, $0\le a\le b\le T$ and $A\in\mathcal F_a$. We need to show that $$\operatorname E\left[1_A\int_a^b\Phi_t\:{\rm d}W_t\int_a^b\Psi_t\:{\rm d}W_t\right]=\operatorname E\left[1_A\int_a^b\Phi_t\Psi_t\:{\rm d}t\right]\tag3\;.$$ Note that $1_A$ is $\mathcal F_a$-measurable and hence $$1_{A\times(a,\:b]}\in\mathcal I^2\tag4$$ with $$1_A\int_a^b\Phi_t\:{\rm d}W_t\int_a^b\Psi_t\:{\rm d}W_t=\int_a^b1_A\Phi_t\:{\rm d}W_t\int_a^b1_A\Psi_t\:{\rm d}W_t\tag5\;.$$ Moreover, note that $\mathcal I^2$ equipped with the inner product inherited from $\mathcal L^2\left(\operatorname P\otimes\left.\lambda^1\right|_{\overline I}\right)$ is a $\mathbb R$-Hilbert space.","['stochastic-processes', 'probability-theory', 'stochastic-integrals', 'stochastic-analysis', 'stochastic-calculus']"
2278338,Proving Ramanujan's Nested Cube Root,"Ramanujan's Nested Cube: If $\alpha,\beta$ and $\gamma$ are the roots of the cubic equation $$x^3-ax^2+bx-1=0\tag{1}$$ then, they satisfy $$\alpha^{1/3}+\beta^{1/3}+\gamma^{1/3}=(a+6+3t)^{1/3}\tag{2.1}$$ $$(\alpha\beta)^{1/3}+(\beta\gamma)^{1/3}+(\gamma\alpha)^{1/3}=(b+6+3t)^{1/3}\tag{2.2}$$ where $$t^3-3(a+b+3)t-(ab+6(a+b)+9)=0\tag3$$ The formula (2.1) is what Ramanujan used to get $$\sqrt[3]{\cos\tfrac {2\pi}7}+\sqrt[3]{\cos\tfrac {4\pi}7}+\sqrt[3]{\cos\tfrac {8\pi}7}=\sqrt[3]{\tfrac 12\left(5-3\sqrt[3]7\right)}$$ by starting with $x^3+x^2-2x-1=0$ along with its trigonometric roots $\cos\frac {2\pi}7,\>\cos\frac {4\pi}7, \>\cos\frac {8\pi}7$ on LHS, and then getting RHS with $a=-1,b=-2$ . Question: How to prove the formulas (2.1) and (2.2)? Is there a standard procedure to find trigonometric roots of a polynomial? I first started off with a function $x^3-px^2+qx-1=0$ and assumed that the roots were $\alpha^{1/3},\beta^{1/3},\gamma^{1/3}$ . That way, by Vieta's formula, we have $$\alpha^{1/3}+\beta^{1/3}+\gamma^{1/3}=p$$ $$(\alpha\beta)^{1/3}+(\beta\gamma)^{1/3}+(\gamma\alpha)^{1/3}=-q$$ However, I'm not sure how to represent the RHS in $(2.1)$ or $(2.2)$ EDIT: I found a proof, but something doesn't match up. I have posted another question here","['radicals', 'polynomials', 'trigonometry', 'cubics', 'nested-radicals']"
2278354,"Differential equation with homoegeneous coefficient, solution other than in book","I have a  differential equation: $$ x \frac{dy}{dx} - y - x\sin\left(\frac{y}{x}\right) = 0. $$ I'm multiplying both sides by $dx$ and I'm obtaining: $$ x\,dy - y\,dx - x \sin\left(\frac{y}{x}\right)\, dx = 0. $$ Next, after simplification I have: $$ x\,dy - \left(y+\sin\left(\frac{y}{x}\right)\right)\,dx = 0.$$ This is a homogeneous differential equation with homogeneous functions of order $1$ right? So I use substitution: $$ y = ux, dy = u\,dx + x\,du $$ and I'm obtaining the equation: $$ x(u\,dx + x\,du ) -\left(ux + \sin\left(\frac{y}{x}\right)\right)\,dx = 0.$$ After simplification I'm obtaining: $$ x^{2}\, du - \sin(u)\, dx = 0.$$ So, next I'm dividing equation boths sides by: $\sin(u)x^{2}$: $$ \frac{du}{\sin(u)} - \frac{dx}{x^{2}}  = 0.$$ Because : $$ \int \frac{dx}{x^{2}} = \frac{-1}{x} + C $$ and $$ \int\frac{du} {\sin(u)} = \ln \left| \tan\left(\frac{u}{2}\right)\right| + C. $$ So: $$ \ln \left|\tan\left(\frac{u}{2}\right)\right| +  \frac{1}{x}  = C.$$ Next: $$ \ln \left| \tan\left(\frac{y}{2x}\right)\right| = C - \frac{1}{x}$$ $$ e^{C-\frac{1}{x}} = \left|\tan\left(\frac{y}{2x}\right)\right|  $$ $$ \pm e^{c} e^{\frac{-1}{x}} = \tan\left(\frac{y}{2x}\right). $$   Now I'm substituting $d = \pm e^{e^{c}} $ and in consequence I have: $$ de^{\frac{-1}{x}} = \tan\left(\frac{y}{2x}\right) $$ $$ \arctan\left(d e^{\frac{-1}{x}} \right) = \frac{y}{2x} $$ $$ y = 2x \cdot \arctan\left(de^{\frac{-1}{x}}\right).$$ When I look on the answer from the book there is: $$ y = 2x \cdot \arctan(cx).$$ Why here is $ x $ instead $e^{\frac{-1}{x}} $ ? I don't know. Is my answer wrong? I will be greatfull for help.
Best regards.","['homogeneous-equation', 'ordinary-differential-equations']"
2278381,Subgroups of $GL_n$ and group actions,"For my abstract algebra class I have to do some exercises concerning group actions of $GL_n(K)$ on the set of flags $$F_n = \{0 \subseteq V_1 \subseteq V_2 \subseteq \ldots \subseteq V_r = K^n \,\vert\, \text{dim}_K V_i = n_1 + \ldots + n_i\}$$ where $n_1 + \ldots + n_r = n$. Although I got to know the basic things such as orbit and stabilizer, I don't have any practice on how to apply these concepts and, concretely, for these exercise I don't know how to proceed. The question is threefold and as follows. $\textit{First}$, show that the parabolic subgroup 
$$P_n = \Bigg\{ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\ 
                                0 & \ddots & \ddots & \vdots \\ 
                               \vdots & \ddots & \ddots & B_{r-1r} \\ 
                                 0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a subgroup of $GL_{n}$ by using the group action of $GL_n(K)$ on $F_n$.
$\textit{Secondly}$, with the help of this result, show that the corresponding Levi subgroup $$M_n = \Bigg\{ \begin{pmatrix} A_1 & 0 \ldots & 0 \\ 
                                0 & \ddots & \ddots & \vdots \\ 
                               \vdots & \ddots & \ddots & 0 \\ 
                                 0     & \ldots & 0    & A_r \end{pmatrix}\Bigg\}$$ where $$A_i \in GL_{n_i}$$ is in turn a subgroup of $P_n$. And $\textit{finally}$, show that the unipotent radical $$U_n = \Bigg\{ \begin{pmatrix} I_1 & B_{12} \ldots & B_{1r} \\ 
                                0 & \ddots & \ddots & \vdots \\ 
                               \vdots & \ddots & \ddots & B_{r-1r} \\ 
                                 0     & \ldots & 0    & I_r \end{pmatrix}\Bigg\}$$ where $$I_i = Id_{GL_{n}}, \, 1\leq i \leq r, B_{ij}\in \text{Mat}(n_{i} \times n_{j}, K) \, 1\leq i < j \leq r $$ is a normal subgroup in $P_n$. With that, also prove that $P_n/U_n \cong M_n$ What I know is that the stabilizer for a group G and a set M is the set $G_x = \{g \in G\vert g\cdot x = x\}$ with $x \in M$ and a subgroup of $G$. I would merely guess that in the first or second part I would have to show that along these lines. In short, I do not have a clear conceptual picture and, most of all, I wouldn't know how to write it down properly even in case I had it. EDIT : Due to Derek Holt's comment, I tried to figure out how to show that $P_n$ is the stabilizer of the flags in which $V_i$ is the subspace spanned by the first $n_1 + \ldots + n_i$ basis vectors. Although I can somehow imagine it with simple block matrices of the kind of $$\begin{pmatrix} A & B \\ C & D \end{pmatrix} $$ I do not know how to put it on paper. What I tried is to rewrite each vector $x \in K^n$ for all $V_i$ where $i \in \{1, \ldots, n\}$ to $$ x = \begin{pmatrix} x_1 \\ \vdots \\ \vdots \\ x_n\end{pmatrix} = \begin{pmatrix} x_{V_i} \\ x'_{V_i} \end{pmatrix}$$ where $$ x \in V_i \Leftrightarrow x'_{V_i} = 0$$
Put informally, I think I 'see' that if I left-multiply some vector $x \in V_i \subset F_n$ with some matrix $p \in P_n$, then I should still get some vector $x^* \in V_i \subset F_n$. But how to write that down in a correct way? Also, for illustration purposes I tried to left-multiply the vector $$ x = \begin{pmatrix} x_1 \\ x_2 \\ x_3 \\ 0 \\ \vdots \\ 0 \end{pmatrix}$$ where $x \in \mathbb{R}^n$ with the given matrix $$ \begin{pmatrix} A_1 & B_{12} \ldots & B_{1r} \\ 
                                0 & \ddots & \ddots & \vdots \\ 
                               \vdots & \ddots & \ddots & B_{r-1r} \\ 
                                 0     & \ldots & 0    & A_r \end{pmatrix}$$ in order to show that this really affects the resulting vector $x^*$ only in the first 3 coordinates, i.e. that it still lies in the subspace $V_i \in F_n$ it was before. But I got in some trouble regarding how to actually multiply it with the given block matrices. For example, $A_1$ would be in $GL_{n_1}(K)$, but how would I know what $n_1$ is? With regard to the third part, I know what a normal subgroup is and how to check it, i.e. $xVx^{-1} \subset V \, \forall x \in P_n$. I already did that with triangular matrices and the triangular matrices where $1$s are on the diagonal. However, here are entire matrices within other matrices, so I assume this complicates things. Hence, what's the idea?","['group-actions', 'abstract-algebra', 'group-theory']"
2278409,I am unsure what this question is asking - bounded linear functionals!,"There has been a question very similar to this in a different post, but the person who asked the question put a disclaimer that he only wanted hints. I would prefer a more full answer to what the question is actually asking for. Let $(X,\left\|\cdot\right\|)=(l^1,\left\|\cdot\right\|_1)$ and let $f \in X^*$. Prove that there exists a bounded sequence $(v_1,v_2,v_3,...,v_n...)$ of real numbers such that $$f(x)=f(a_1,a_2,a_3,...,a_n,...)= \sum^{\infty}_{n=1}a_n v_n$$ for all $x= (a_1,a_2,a_3,...,a_n,...)\in l^1$. So my thoughts are as $x\in l^1$, we must have $\sum^{\infty}_{n=1}|a_n| < \infty$. As the sequence $(v_1,v_2,v_3,...,v_n...)$ is bounded, we must be able to say that $$\sum^{\infty}_{n=1}a_nv_n \leq \sum^{\infty}_{n=1}|a_n|v_n \leq \|v_n\|_{\infty} \sum^{\infty}_{n=1}|a_n| \leq \infty.$$ However, I am not too sure if this answers the existence question! Link to previous question: Existence of $\{a_n\}$ s.t $f(\vec{v})=\sum_{n \in \mathbb{N}} a_n v_n$ for continuous linear functions.","['functional-analysis', 'lp-spaces', 'sequences-and-series', 'functions']"
2278426,Coordinates of the point of an ellipsoid closest to the origin.,"Given the ellipse $\frac{(x-r)^2}{a^2}  +  \frac{(y-s)^2}{b^2} =1$ 
$(a^2 > b^2)$. Find the coordinates of the point of the ellipsoid closest to the origin in terms of a,b,r and s. I tried three different method so far but all lead to highly complicated and lengthy computations. Anyone has a shortcut? Thanks","['multivariable-calculus', 'vector-analysis']"
2278430,Osculating circle,"Compute the radius of osculating circle of the hyperbola $$\frac{x^2}{a^2}-\frac{y^2}{b^2}=1$$ at one of its vertices and give a geometrical method for the construction of this osculating circle. Consider the particular case of equilateral hyperbola $x^2-y^2=a^2$ I computed the radius on the vertex $(a,0)$ by finding the reciprocal of the curvature of hyperbola. Which is $r=\frac{1}{|\kappa|}=\frac {b^2}{a}. \ $ Now how can I solve the next part? ""give a geometrical method for the construction of this osculating circle""","['conic-sections', 'differential-geometry', 'calculus']"
2278436,"Does there exist integers $a, n > 1$ such that $1 + \frac{1}{1 + a} + \frac{1}{1 + 2a} + ... + \frac{1}{1 + na}$ is an integer?","Does there exist integers $a, n > 1$ such that $1 + \frac{1}{1 + a} + \frac{1}{1 + 2a} + ... + \frac{1}{1 + na}$ is an integer? I have no clue how to begin. I've tried to simplify this somehow, but with no effect.","['number-theory', 'combinatorics', 'integers']"
2278550,Let $H ≤ G$ and $Hg$ be a right coset of $H$ in $G$. Prove that the set $\{k^{-1}|k \in Hg\}$ is a left coset of $H$ in $G$.,"I currently and studying for my exams and this is one of the warm up questions, however I am struggling to understand how to approach it, my current attempt has been: $k=hg$ $\implies k^{-1}=g^{-1}h^{-1}$ $\implies k^{-1}hg=1$ $\implies k^{-1}hgg_1h_1=g_1h_1 \in gH$ But this feels wrong, I am not sure how to approach this as I have been struggling with it for quite awhile now, any help would be appreciated. It also follows on by saying "" deduce that there is a bijection between the left and right cosets of H in G "" so any tips for that would be appreciated. Edit: Thank you for the help, most of this has been resolved in a way that has allowed me to figure it out for myself.","['finite-groups', 'group-theory']"
2278618,Eigenvalues of self-adjoint extension of the Laplacian,"Consider the Laplace operator $-\Delta$ on $L^2(\mathbb{R}^3)$ with domain $C_0^{\infty}(\mathbb{R}^3 \backslash \{ 0 \})$, the smooth functions with compact support not near 0. I have had a look at this Solving Poisson Equation with domain $L^2(\mathbb{R}^3 \backslash \{ 0 \})$. But feel that what was missing here was that since we are away from zero, we can't simply solve $(\Delta^{\ast} \pm i)u =0$, it is better to solve ($\Delta^{\ast} \pm i)u =\delta$ for some $\delta$ function. In so doing, taking the Fourier transform yields that $$(4\pi^2 \left| \xi \right|^2 \pm i) \hat{u}(\xi) = 1,$$ and therefore, $$u = \frac{1}{4\pi} \frac{e^{i\sqrt{\pm i} \ |x|}}{|x|}. $$ From this, I want to show that all self-adjoint extensions of $-\Delta$ have a negative eigenvalue, except possibly one. I'm fairly sure we need to use the above formula that I've obtained for $u$, but am unsure. Also, how would we obtain the Friedrich's extension from this?","['functional-analysis', 'laplacian', 'operator-theory', 'partial-differential-equations']"
2278638,Summation of the Sine Function [duplicate],"This question already has answers here : How can we sum up $\sin$ and $\cos$ series when the angles are in arithmetic progression? (8 answers) Closed 7 years ago . I was messing around on Wolfram Alpha's summation calculator and when I plugged in the summation
$$\sum_{i=1}^n\sin\frac{i\pi}{180}$$
and it gave me the value
$$\frac12\left(\cot\frac\pi{360}-\csc\frac\pi{360}\cos\frac{(2n+1)\pi}{360}\right)$$
I don't understand... how does it arrive at this formula? And how do I verify this? If I wanted to, how could I find similar formulas in the future?","['summation', 'trigonometry']"
2278640,Proving equality of two functions,"Lef $f, g$ be two $\mathbb{N}\rightarrow \mathbb{N}$ functions satisfying the following conditions. $f(g(n))=g(n)+1$ $g(f(n))=f(n)+1$ Show that $f = g$ . I have tried a lot of things and got lots of results, none of which look promising.","['functional-equations', 'functions', 'discrete-mathematics']"
2278713,"Let $f$ be nonconstant analytic in a domain $D$ and continuous on its closure. If $|f|$ is constant on $\partial D$, then $f$ has a zero in $D$. [duplicate]","This question already has answers here : If $f$ is a non-constant analytic function on $B$ such that $|f|$ is a constant on $\partial B$, then $f$ must have a zero in $B$ [duplicate] (2 answers) Closed 7 years ago . This exercise has no assumption on the boundedness of $D$, hence $\bar{D}$ is not guaranteed to be compact. Would this be essential for the assertion to be true? Suppose the nonconstant function $f(z)$ is analytic in a domain $D$ and continuous on its closure. If $|f(z)|$ is constant on the boundary of $D$, prove that $f(z)$ has a zero in $D$. The obvious approach is to assume by contradiction that $f$ has no zero in $D$. Then we can define a nonconstant analytic function $1/f$ in $D$, which gives us by the Maximum Modulus Theorem that $1/|f|$ does not attain a maximum in $D$. But how can I use this to reach a contradiction?
I am stuck here and I would greatly appreciate any help.","['complex-analysis', 'maximum-principle', 'analysis']"
2278733,"Integral: $\int_{|z|\mathop=2}\sqrt{z^4-z}\,dz$ - finding Residues / Laurent Series","I wish to calculate the integral
  $$\int_{|z|\mathop=2}\sqrt{z^4-z}\,dz$$ $$z^4-z=z(z-1)(z-e^{2i\pi/3})(z-e^{-2i\pi/3})$$
We must make branch cuts which go through the branch points. The branch points of $\sqrt{z^4-z}$ are $z=0,1,e^{2i\pi/3},e^{-2i\pi/3}$. All of these lie inside this contour, so I first thought that I must work out the residue at each of these points, and then use the residue theorem. However there is no nice way that I could think of to calculate these residues. Then it occurred to me that I am not even sure if they exist - these are not poles, they are branch points with branch cuts through them. I then saw that the question has a hint saying to use Laurent Series. Most of the Laurent series I have ever seen have been functions with (sort of) Taylor series, but with some change of variables, or multiplied by some factor of $1/z^n$. The point is - I don't know any (efficient) way of finding the Laurent Series for this function. My questions : How can I Define these branch cuts, Find the different Laurent Series, and find the value of the coefficient of $1/z$, thus finding value of the integral? Also: I thought Laurent Series existed within an annulus of the point about which they exist - how is this possible for a function with branch cuts? Is there something I am confusing here? These two ideas I had seem to contradict. If the Laurent Series do exist, then is it possible to calculate the residues directly? (Also assuming these exist)","['branch-points', 'laurent-series', 'complex-analysis', 'contour-integration', 'branch-cuts']"
2278765,Marginal distribution for correlated uniform variables,"Question Let $X$ and $Y$ be two uniformly distributed random variables with bounds $x_\text{low}$, $x_\text{up}$, $y_\text{low}$ and $y_\text{up}.$ $X$ and $Y$ are correlated with a correlation coefficient of $R$. Given an observed outcome $x$ from the variable $X$ and given the correlation coefficient $R$, how can one calculate the probability of a particular outcome $y$ from variable $Y$. In other words, how can one calculate $$P(Y=y \mid X=x, R) = \text{?}$$ Extreme cases The extreme cases are easy. If $R=0$ ($X$ and $Y$ are independent), then $$P(Y=y \mid X=x, R) = \frac 1 {Y_\text{up} - Y_\text{low}}$$ If $R = 1$, then $$P(Y=y \mid X=x, R) =
\begin{cases}
1,  & \text{if} \space y = \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}} \\
0, & \text{if} \space y ≠ \frac{x - x_\text{low}}{x_\text{up} - x_\text{low}}
\end{cases}$$ Goal In case it is of interest, my goal when asking this question is to write a short algorithm that sample points from this bivariate uniform distribution with specified correlation coefficient.","['correlation', 'probability-theory', 'probability-distributions', 'statistics', 'probability']"
2278799,Equivalence of Cauchy integral with Riemann integral,"There has already been some discussion on this topic . However my question is about a specific solution to this problem and for the benefit of readers I think it is better to add some context (even though it means repetition of some stuff mentioned in the linked question). In what follows $f$ is a function of type $f:[a, b]\to\mathbb{R}$ and $f$ is bounded. A partition $P$ of $[a, b]$ is a set of type $$P = \{x_{0}, x_{1}, x_{2}, \dots, x_{n}\}$$ where $$a = x_{0} < x_{1} < x_{2} < \dots < x_{n} = b$$ The norm $||P||$ of partition $P$ is defined by $||P|| = \max_{i = 1}^{n}(x_{i} - x_{i - 1})$. We define the following sums for $f$ over $P$
\begin{align}
C(f, P) &= \sum_{i = 1}^{n}f(x_{i - 1})(x_{i} - x_{i - 1})\notag\\
S(f, P) &= \sum_{i = 1}^{n}f(t_{i})(x_{i} - x_{i - 1})\notag\\
U(f, P) &= \sum_{i = 1}^{n}M_{i}(x_{i} - x_{i - 1})\notag\\
L(f, P) &= \sum_{i = 1}^{n}m_{i}(x_{i} - x_{i - 1})\notag
\end{align}
where $t_{i}$ are arbitrary points in $[x_{i - 1}, x_{i}]$ and $$M_{i} = \sup\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\},\,m_{i} = \inf\,\{f(x)\mid x\in [x_{i - 1}, x_{i}]\}$$ The sum $C(f, P)$ is called (left) Cauchy sum for $f$ over $P$. The Riemann sum $S(f, P)$ depends on choice of tags $t_{i}$ but this dependence in not shown in the notation and should be evident from the context. And finally $U(f, P), L(f, P)$ are upper and lower Darboux sums for $f$ over $P$. Cauchy Integral : The function $f$ is said be said to be Cauchy integrable over $[a, b] $ with Cauchy integral $I$ if for every $\epsilon >0$ there is a number $\delta > 0$ such that $|C(f, P) - I| < \epsilon$ whenever $P$ is a partition of $[a, b]$ with $||P|| < \delta$. A similar definition is available for Riemann integral if $C(f, P)$ is replaced by $S(f, P)$. Both these notions are equivalent and since every Cauchy sum is also a Riemann sum, the inference from Riemann to Cauchy is trivial. The converse appears to be hard and perhaps not popular enough to be seen in textbooks. User Tony Piccolo in his answer gives three references for the proof that Cauchy integrability implies Riemann integrability. It is the second proof from that answer which I want to discuss here (as other two proofs use somewhat complicated ideas and some very non-obvious tricks). This is provided as a hint that Given any partition $P$ of $[a, b]$ and a number $\epsilon > 0$ there is a partition $Q\supseteq P$ of $[a, b]$ such that $C(f, Q) > U(f, P) - \epsilon$. Using the counterpart equation $C(f, P) < L(f, P) + \epsilon$ we can easily show that difference $U(f, P) - L(f, P)$ can be made small if sums $C(f, P)$ tend to a finite limit and thus we get Riemann integrability (via Darboux integrability, also this link between Darboux and Riemann integral is popular and available in good textbooks). Here are my questions: It is easy to prove that we can choose tags $t_{i}$ such that $S(f, P) > U(f, P) - \epsilon$. We just have to choose tags so that $f(t_{i})$ is sufficiently near $M_{i}$. My hunch is that if we add the tags $t_{i}$ to $P$ we get a partition $Q\supseteq P$ and that is the needed partition which ensures $C(f, Q) > U(f, P) - k\epsilon$ where $k$ is some fixed positive constant. Is this correct? And if so how do we go about proving this? Another doubt is whether the relation between $C(f, P)$ and $U(f, P)$ is valid in general? Or does it hold only for Cauchy integrable functions? My guess is that it holds only for Cauchy integrable functions. Is this correct?","['real-analysis', 'riemann-integration']"
2278827,Evaluate $\int (1-x^{2008})^{\frac{1}{2007}} (1-x^{2007})^{\frac{1}{2008}} dx$,"Evaluate the given integral
$$\int (1-x^{2008})^{\frac{1}{2007}} (1-x^{2007})^{\frac{1}{2008}} dx$$ Using integration by parts is out of equation because we can't integrate any of two brackets. I also cannot think of any substitution that can lead to integration. Could someone help me with this?","['indefinite-integrals', 'integration', 'calculus']"
2278835,Let $a$ and $b$ be positive real numbers. If $x^2 + y^2 \le1$ then the largest $ax + by$ is?,"The problem: The solution: -Why is the equation $y=\frac{a}{b}x+\frac{c}{b}$  a line if $\frac{c}{b}$ is not constant?  That is, $c$ varies as either $y$ or $x$ varies, but if this equation is a line, then the constant term $\frac{c}{b}$ can't vary, precisely because it's constant , which is not possible if $c$ is varying.  Clearly, I'm missing a detail. -A tangent line to a disc isn't necessary perpendicular to the line segment formed by the center of the disc and the point of interception between the line and the disc.  The tangent line can be tilted up or down (so not perpendicular) and still be ''touching'' the disc, so why in this case this line is necessarily perpendicular?","['algebra-precalculus', 'plane-geometry']"
2278906,Random simulations of the distribution of genders in a classroom,"For my AP statistic class, we had to design an experiment (usually a survey of what brand of soda people prefer), so me being an overachiever decided to study how boys and girls distribute themselves among other genders. For instance, girls will sit near each other normally. My way of doing this would be to find the average percentage of people guys sit around that are the same gender for both genders independently. I'd expect to see these numbers be $50$% if they sat randomly, but I'm willing to bet they're around $75$%-$85$%. How can I produce random simulations of this test? It's a requirement for the assignment.",['statistics']
2278912,"Show that $\lim_{n\to\infty}\int_{[0,1]}\frac{nx}{1+n^2x^2}=0$.","$\lim_{n\to\infty}\int_{[0,1]}\frac{nx}{1+n^2x^2}=0$ is to be shown. This should be done using the Lebesgue Dominated Convergence theorem. I can see that the sequence of functions $(f_n)$, where $f_n(x)=\frac{nx}{1+n^2x^2}$ converges pointwise to zero on $[0,1]$. I need help in figuring out a dominating function to apply the aforementioned theorem, i.e. a function $g$ such that it is integrable on $[0,1]$ and $|f_n|\leq g$ a.e. on $[0,1]$ for all $n$. Someone please give me a hint. Thanks.","['self-learning', 'lebesgue-integral', 'measure-theory']"
2278951,Functions and Sequences Problem,"The function $F(k)$ is defined for positive integers as $F(1) = 1$,
  $F(2) = 1$, $F(3) = -1$ and $F(2k) = F(k)$, $F(2k + 1) = F(k)$ for $k \geq
 2$. Then $$F(1) + F(2) + \dotsb + F(63)$$ equals $\begin{array}{lr}
 (\text{A}) & 1 \\
 (\text{B}) & -1 \\
 (\text{C}) & -32 \\ 
 (\text{D}) & 32 \\
\end{array}$ My approach: $F(4)=1$, $F(5)=1$, $F(6)=-1$, $F(7)=-1$ (i.e., all values of $F$ are either $1$ or $-1$). I tried to find a pattern for which $F(x)$ repeats after a certain integer but tried till $F(30)$ and cannot find a solution.  Where am I going wrong?","['sequences-and-series', 'functions']"
2278956,How can I find an example to illustrate that the lower box dimension may not be finitely stable?,"Here the lower box dimension and the upper box dimension are exactly what Falconer talks about in his book ""Fractal Geometry"". I already know that the upper box dimension is finitely stable since $N_\delta(A\cup B)\le N_\delta (A)+N_\delta(B)$, where $N_\delta(A)$ is defined as usual, and Falconer gives an example to show the box dimension may not be countably stable. (We know that, however, the Hausdorff dimension is countably stable). But now I want to find the example to show the lower box dimension may not even be finitely stable.It seems that we can use von Koch curve to illustrate this, but what is the explicit explanation for this? Since the lower box dimension is monotonic, we need to find set A and B and show that   $\underline{\dim}_B(A\cup B)>\max\{\underline{\dim}_B(A),\underline{\dim}_B(B)\}$.","['dimension-theory-analysis', 'measure-theory', 'fractals', 'geometry']"
2278964,Binomial coefficients equality or maybe probability,"Let $m,n$ be positive integers. 
Evaluate the following expression:
$$
F(m,n) = \sum\limits_{i=0}^n\frac{\binom{m+i}{i}}{2^{m+i+1}}+
\sum\limits_{i=0}^m\frac{\binom{n+i}{i}}{2^{n+i+1}}.
$$ Calclulations give the hypothesis that $$F(m,n)=1,$$ for all positive integers $m,n$.
Also if $m=n$, then
$$
F(m,m) = \sum\limits_{i=0}^m\frac{\binom{m+i}{i}}{2^{m+i}} = \sum\limits_{i=0}^m\frac{\binom{m+i}{m}}{2^{m+i}}. 
$$
The numerator of every summand is equal to the number of $m$-subsets in $m+i$-set and denominator is equal to the number of subsets in $m+i$-set. So, I think it maybe the key to solution.","['combinatorics', 'binomial-coefficients', 'probability']"
2278973,To prove $\frac{1}{2} \times \frac{3}{4} \times \frac{5}{6} \cdots \frac{(2n-1)}{2n} \le \frac{1}{\sqrt{3n+1}}$ [duplicate],This question already has an answer here : How does one prove that $\frac{1}{2}\cdot\frac{3}{4}\cdots \frac{2n-1}{2n}\leq \frac{1}{\sqrt{3n+1}}?$ (1 answer) Closed 7 years ago . To prove $$P=\frac{1}{2} \times \frac{3}{4} \times \frac{5}{6} \cdots  \frac{(2n-1)}{2n}\le \frac{1}{\sqrt{3n+1}}$$ i have written $P$ as $$P=\frac{(2n)!}{2^{2n}(n!)^2}=\frac{(2n)!}{4^{n}(n!)^2}=\frac{\binom{2n}{n}}{4^n}$$ Now $$P=\frac{\binom{2n}{n}}{(1+3)^n} \lt \frac{\binom{2n}{n}}{1+3n}$$  since $$(1+3)^n=1+3n+\binom{n}{2}3^2+\cdots$$ Any help here..,"['radicals', 'inequality', 'binomial-coefficients', 'induction', 'combinatorics']"
2279039,Proving Plancherel's identity.,"I want to prove that $$\Arrowvert \hat{f} \Arrowvert_{l^2(\mathbb{Z})} = \Arrowvert f \Arrowvert _{L^2([-L,L])},$$ Could anyone give me a hint please?","['real-analysis', 'fourier-series', 'fourier-analysis', 'measure-theory', 'fourier-transform']"
2279057,What is a characterisation function,"I am reading a paper: ""Probability of Backtest Overfitting"" , and page 13 defines the relative frequency: $$f(\lambda) = \sum_{c \in C_S} \frac{\chi_{\{\lambda\}} \left( \lambda_c\right)}{|\{C_S\}|}$$ the authors say that $\mathcal{X}$ is the characterization function. I would like to know what exactly that is since I have never come across that term before. EDIT: I emailed the author of the paper, and according to him, $f(\lambda)$ is simply a PDF.","['probability', 'measure-theory']"
2279130,How do I go about solving this equation? $3^x + 10^x = 4^x + 9^x$,"How do I go about solving this equation?
$$3^x + 10^x = 4^x + 9^x.$$ I noticed that $1$ and $0$ are solutions, so maybe a way to prove that they are the unique solutions. Taking the derivative does not seem to lead anywhere...","['inequality', 'transcendental-equations', 'algebra-precalculus', 'karamata-inequality', 'exponentiation']"
2279132,Are dependent variables random?,"A linear regression model can be described as:
$$ y = \beta_0 + \beta_1 X + \epsilon $$
where $\epsilon$ is the zero mean normal error. My question is: Is $X$ random variable?
If no, then how can we define
$$ \mathbb{E}[y\mid X]$$
as $X$ is a deterministic. If yes, then what do we mean by this (found in the wikipedia page) ""we want to find how changing the value of $X$, changes $typical/expected$ value of $y$?"" Are we talking about an instance of random X or expected value of X?
link for wiki page: https://en.wikipedia.org/wiki/Regression_analysis","['regression', 'linear-regression', 'normal-distribution', 'statistics', 'conditional-expectation']"
2279135,“Geometric” problems on the Jordan normal form of a particular operator,"Assume you have a class of students more or less familiar with the notion of the matrix of a linear operator. They have seen and calculated lots of examples in various context: geometric transformations (rotations, reflections, scaling along axes, ...), operators on polynomials (derivation), number-theoretic ($\mathbb{C}^n\to\mathbb{C}^n$, linear over $\mathbb{R}$ but not over $\mathbb{C}$). In the study of the Jordan normal form the basic problem is to find the canonical form and a Jordan basis of an operator. The algorithm one usually gives to the students starts with the line “pick a basis and find the matrix of the given operator with respect to this basis”. But then we give the students a problem of the form “given a matrix , find its canonical form and a Jordan basis”. Now I would very much like to force the students to calculate the Jordan form of an operator , so they would pick a basis themselves, find the corresponding matrix, find the Jordan basis and then express it not as a set of columns of numbers, but as elements of the vector space in question. This needs a couple of examples, here they are: $V=\mathbb{C}^2$, the operator is $A\colon\begin{pmatrix}x\\y\end{pmatrix}\mapsto\begin{pmatrix}\overline{x}-\operatorname{Re}(y)\\(1+i)\cdot\operatorname{Im}(x)-y\end{pmatrix}$. The natural $\mathbb{R}$-basis is $$\begin{pmatrix}1\\0\end{pmatrix},\ \begin{pmatrix}i\\0\end{pmatrix},\ \begin{pmatrix}0\\1\end{pmatrix},\ \begin{pmatrix}0\\i\end{pmatrix},$$
the matrix of $A$ is
$$\begin{pmatrix}1&0&-1&0\\0&-1&0&0\\0&1&-1&0\\0&1&0&-1\end{pmatrix},$$
the JNF is $\operatorname{diag}(1,J_2(-1),-1)$, and the Jordan basis is, for example,
$$\begin{pmatrix}-1\\0\end{pmatrix},\ \begin{pmatrix}2\\4+4i\end{pmatrix},\ \begin{pmatrix}1+4i\\4i\end{pmatrix},\ \begin{pmatrix}0\\4i\end{pmatrix}.$$ $V=\mathbb{R}[t]_{\leqslant4}$, the space of polynomials of degree at most 4, and the operator if $f\mapsto f'+f(0)+f'(0)$. The Jordan basis in this case is a set of polynomials. The two examples above are not very interesting in terms of the calculating the JNF (few small blocks, distinct eigenvalues), but this can be easily fixed. But I find it pretty hard to invent a problem of this sort which have a geometric origin (transformations in, say 4- or 5-dimensional Euclidean space). Most of the transformations I can describe in simple geometric terms (rotations, reflections, projections) are either diagonalizable, or have imaginary eigenvalues (so it is impossible to get back form the coordinate columns to points in space), or both. Is there a way to construct a “geometric” problem on the computation of the JNF? Since there must be other contexts similar to the three described above, what are the interesting problems on the computation of the JNF of a particular operator? To clarify this second question, I am well-aware of the problems of the sort “one knows the characteristic and minimal polynomials, the rank of the square and the maximal number of linearly independent eigenvectors, find the JNF”. Apart from the use in the class in order for the students to recall the notion of the matrix of an operator, this can also be very useful in an online course with automated assignment check.","['education', 'jordan-normal-form', 'linear-algebra']"
2279149,Having trouble using my usual method of partial fraction decomposition for $\frac{9 + 3s}{s^3 + 2s^2 - s - 2}$.,"I'm having trouble using my usual method of partial fraction decomposition for $\dfrac{9 + 3s}{s^3 + 2s^2 - s - 2}$ . We can factor such that $$\dfrac{9 + 3s}{s^3 + 2s^2 - s - 2} = \dfrac{A}{s - 1} + \dfrac{B}{s + 1} + \dfrac{C}{s + 2}$$ Therefore, $$ 9 + 3s = A(s + 1)(s + 2) + B(s - 1)(s + 2) + C(s + 1)(s - 1)$$ And we have that $s \not = 1, -1, -2$ . From here I usually plug in values to find $A$ , $B$ , and $C$ . $s = 0:$ $$9 = 2A - 2B - C \implies C = 2A - 2B - 9$$ $s = 2:$ $$15 = 12A + 4B + 3C \implies A = \dfrac{15 - 4B - 3C}{12}$$ $s = 3:$ $$18 = 20A + 10B + 8C \implies 10B = 18 - 20A - 8C \implies B = \dfrac{9}{5} - 2A - \dfrac{4C}{5}$$ We now have equations for $A$ , $B$ , and $C$ . But if I try to substitute them into each other, this will result in an infinite loop of substitution. This method for partial fraction decomposition have always worked for me in the past, so I don't understand why it isn't working in this situation. I would greatly appreciate it if people could please take the time to explain why my method for partial fraction decomposition is not working in this case and what I should do.","['algebra-precalculus', 'factoring', 'partial-fractions']"
2279155,Period for a Rubik's cube repeated manipulation,"A standard Rubik's cube is initially unscrambled (say per picture below, green facing observer) The same manipulation is repeated: rotation of front face by 1/4 turn (say clockwise) rotation of the whole cube 1/4 turn around vertical axis (say anticlockwise seen from top) (with the proposed orientations, the center of the rotated face at 1 will cyclically be green, orange, blue, red; in standard notation these moves cycle between F L B R; the center of the upper face always remain white). After how many manipulations will the cube be first unscrambled again? (note: by symmetry, this is independent of the direction of the rotations, as long as they remain the same across manipulations). Is there a simple argument to tell which face is facing the observer at that point (equivalently, to determine the answer modulo 4)? Note: I'm interested in the reasoning to solve that kind of problems, rather than in the answer for that particular manipulation.","['finite-groups', 'rubiks-cube', 'group-theory']"
2279165,Can an 8×8 square be tiled with these smaller squares?,"Number of square pieces , Question C Sal has two $4\times 4$ squares, three $3\times 3$ squares, four $2\times 2$ squares and four $1\times 1$ squares. Draw a diagram showing how she can place all or some of these squares together without gaps or overlaps to make the largest square possible. Explain why she cannot construct a larger square. I came across the fact that I couldn't work out how to make an $8\times8$ square. In theory, I should be able to make one, as the area of all of the squares combined is $79$ unit squares, and by taking away a few squares, I should be able to get a $8\times8$ square, however everyone seems to be forgetting that you can't change the shape of a square in order to get it to fill a gap. So, can we really make an $8\times8$ square, and if no, prove why. Also, if we can't make a $8\times8$ square, how can we make a $7\times7$ square?","['combinatorics', 'combinatorial-geometry']"
2279174,Local connectedness in the boundary implies local connectedness in the closure,"Given a locally connected topological space $X$, and $A \subset X$ such that the boundary of $A$ is locally connected, is true that the closure of $A $ is also locally connected?
I tried hard to prove this without success. Any help?
Thank you!","['general-topology', 'connectedness']"
2279205,Example of filtration in probability theory,"I'm studying Martingales and before them filtrations. Given a probability space $(\Omega, F, P)$ I define a filter $(F_n)$ as a increasing sequence of $\sigma$ -algebras of $F$ , such that $F_t \subset F$ and $t_1 \leq t_2 \Longrightarrow F_{t_1} \subset F_{t_2}$ . Here comes my question:
How can the $F_t$ 's be $\sigma$ -algebras and subsets of $F$ without being exactly equal to $F$ ? I suppose that $F_t$ 's being $\sigma$ -algebras mean that they are $\sigma$ -algebras with respect to the measure space $(\Omega, F)$ .
Can anyone explain why they are not necessarily equal to $F$ and give an example where this is obviously false?","['martingales', 'probability-theory', 'measure-theory', 'filtrations']"
2279211,weighted sum of independent indicator random variables,"I am trying to prove the following claim: Let $v \in \mathbb R_{\ge 0}^n $ such that $||v||_1 = 1$ (i.e. $\sum_{i=1}^n
 v_i = 1$) and let $w \in \{0, 1\}^n $ be a random vector so that
  each $w_i$ is $1$ with probability $ \frac{1}{3} $ and $0$ with
  probability $\frac{2}{3}$ with all choices being independent. Then $\mathrm {Pr}(w \cdot v \ge 1/3) \ge \frac{1}{3}$ I know by linearity of expectation that $E[w \cdot v] = 1/3$, so it seems very intuitive. The result is obvious for $v=(1, 0, 0, \ldots ,0)$ but I am not sure how to prove it for the general case.","['probability', 'probability-distributions']"
2279232,ANOVA F-test results in p-value being 1,"In one of the statistics exercises I have tried to solve, I have to show that the means of the observations in four groups aren't equal. I read that a t-test can only be used to compare means for two groups and that a solution would be to make an ANOVA which includes an F-test. The result of my F-test ended up being approximately 62. Since this is far away from 1, it indicates there is a difference in the means. However, I would like to know whether 62 sounds legit? (I know you haven't seen the exercise). Furthermore, how can I calculate the p-value by myself? I tried to calculate the p-value in Excel but that results in value 1 and I don't know if that is correct. Of course, 1 is much bigger than my statistical significance 0,05.","['means', 'descriptive-statistics', 'statistics', 'variance']"
2279242,Tensor product of $L^2$ spaces,"For Tensor-products on $L^2$ spaces I am Aware of the following property: I know that $L^2(X,\mathbb{R})\otimes L^2(X,\mathbb{R}) \simeq L^2(X^2,\mathbb{R}).$ I was wondering if this is also true for Hilbert-space valued function functions, i.e.
$L^2(X,H) \otimes L^2(X,H) \simeq L^2(X^2,H)$? If anything is unclear, please let me know. The problem seems to be that $L^2(X,H) \simeq L^2(X,\mathbb{C}) \otimes H.$ Thus,
$L^2(X^2,H) \simeq L^2(X^2,\mathbb{C}) \otimes H.$ The left-hand side however is $L^2(X,\mathbb{C}) \otimes H\otimes L^2(X,\mathbb{C}) \otimes H.$","['real-analysis', 'partial-differential-equations', 'functional-analysis', 'measure-theory', 'analysis']"
2279270,"On average, where is the lift?","This started as a computing problem with several variables, and I'd like to know if there's a closed form formula for the average position of the lift. Context: there's a building with $N$ floors and $m$ people distributed randomly across them. If a person is on the floor where they live, they will take the lift to go straight down (no stopping) to the ground floor, and vice-versa. At each step, one person is randomly selected and makes their move. At each step, then, the lift moves either once (already on the floor it's called from) or twice (goes to pick up the person first, then makes the requested move). Each new position of the lift is recorded. On randomness: since this was first a question about a script in Python, the random.randint() method was used, where the documentation states Almost all module functions depend on the basic function random() , which generates a random float uniformly in the semi-open range $[0.0, 1.0)$. Python uses the Mersenne Twister as the core generator. It produces 53-bit precision floats and has a period of $2^{19937}-1$. Question: after $k$ iterations, the algorithm stops and returns the average $\mathcal{A}$ of the recorded positions of the lift. Is it possible to predict either $\lim\limits_{k\to\infty}\mathcal{A}(N,m)$ or $\mathcal{A}(N,m,k)$?","['expectation', 'statistics', 'algorithms', 'probability', 'python']"
2279308,Is the interior of the closure of the interior of the closure of a set equal to the interior of the closure of that set?,"Let $S$ be a subset of a topological space. I want to prove or disprove the following claim: $\left(\overline{\left( \overline{S} \right)^\circ}\right)^\circ=\left( \overline{S} \right)^\circ$ Setting $A=\left( \overline{S} \right)^\circ$ , we have: $A=\left( \overline{A} \right)^\circ$ . I know counterexamples where $A$ is open and this does not hold (for example: $(-1,0)\cup(0,1) $ in R), but I cannot find $S$ such that $A=\left( \overline{S} \right)^\circ$ . Thus, I guess the statement is true, and I am trying to prove it. I proved that $A\subseteq\left( \overline{A} \right)^\circ$ , but I did not manage to proof the other implication yet.",['general-topology']
2279312,Osculating plane,"Consider the curve: $$x=p \sqrt{p^2-q^2}\cos t; \ \ \ y=q\sqrt{p^2-q^2}\ \ (1+\sin t) \ \ \ z=(p^2-q^2)(1+\sin t) \ while \ p>q$$ Check that the osculating plane to the curve is one of the planes of circular section in the paraboloid: $$\frac{x^2}{p^2}+\frac{y^2}{q^2}=2z $$ I have tried to solve this question by first finding the osculating plane but I couldnt because the point on which I am supposed to find the osculating plane is not given. So, do I have to take an arbitrary point on the curve ? Also I did compute the unit normal, unit tangent and binormal vectors, which are $$ T(t)=(-\sin t,\frac {q}{p}\cos t,\frac{\sqrt{p^2-q^2}}{p}\cos t) $$ $$N(t)=(-\cos t,-\frac{q}{p}\sin t,\frac{\sqrt{p^2-q^2}}{p}\sin t)$$ $$ B(t)=(0,-\frac{\sqrt{p^2-q^2}}{p},\frac{q}{p}\ )$$ Can someone please tell me what should I do next ?","['multivariable-calculus', 'differential-geometry', 'geometry']"
2279324,Showing that $\sum_{k=N+1}^\infty \frac {1}{k!} < \frac {1}{N!}$ [duplicate],"This question already has answers here : How to show $\sum_{k=n}^\infty{\frac{1}{k!}} \leq \frac{2}{n!}$ (3 answers) Closed 7 years ago . I was constructing a proof through inequalities, but I am having a bit of problem showing the following step:  $$\sum_{k=N+1}^\infty \frac {1}{k!} < \frac {1}{N!}$$ Is there any quick way to show this?","['real-analysis', 'inequality', 'factorial', 'summation', 'sequences-and-series']"
2279329,Prove curves are the geodesics,"Prove that the ellipsoid $\frac{x^{2}}{a^{2}}+\frac{y^{2}}{b^{2}}+\frac{z^{2}}{c^{2}}=1$ always has at least three geodesics. I think these three geodesics should be cross sections of ${(x,y,0)}$, ${(x,0,z)}$ and ${(0,y,z)}$ with our ellipsoid. And I want prove that the geodesic curvature is zero on these curves. The formula I want to use is $s'k_{g}=(T\times T')N$. So all I have to prove is that $T'//N$. And $N=({\frac{2x}{a^{2}}},\frac{2y}{b^{2}},\frac{2z}{c^{2}})$. But $T'$ is not very easy to get.",['differential-geometry']
2279372,Minimize $m+n$ given $\frac{2016}{2017}<\frac mn<\frac{2017}{2018}$,"Given: $$\dfrac{2016}{2017}<\dfrac mn<\dfrac{2017}{2018}$$ Find the smallest value possible of the sum of the denominator and the numerator, i.e. $m+n$. I don't know how to spot the very peculiar fraction with the minimum values of $m$ and $n$ in the domain $\left[\dfrac{2016}{2017},\dfrac{2017}{2018}\right]$. Edit: thank you everyone for the answers! 
how can one prove that using the median operator the result that is strictly between the two fractions is the one with the minimized value possible for m+n. I've done some work, I realized that I need to count the number of digits in a decimal number c, given x < c < y , let's say N, c= c*10^(N-1)/10^(N-1)
m+n= c*10^(N-1)/(gcd(c*10^(N-1),10^(N-1))) + 10^(N-1)/(gcd(c*10^(N-1),10^(N-1)))
m+n= (c+1)(  10^(N-1) / (gcd(c*10^(N-1),10^(N-1)) ) c is a variable that changes on the domain (x,y) and N is dependent on c, so N is also a variable, and then (m+n) is the last variable that is dependent on N and the gcd, which means on both N and c, to write N in terms of c for natural numbers, it's pretty easy and straight-forward: N= ceiling(log(c)) or N= floor(log(c))+1, now, since x and y are at least consecutive numbers, the variable c won't be a natural number and its length isn't easily given. 1) how can you be determined of the number of digits in any decimal number including the fractional part?
2) is it possible to have a function with two variables f(x,y), from which we can obtain the minimum value of m+n? thanks a ton! I look forward for your answers.","['algebra-precalculus', 'contest-math']"
2279379,What is the latest verified research on the 3x+1 Problem? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question Wikipedia : Collatz Conjecture Take any positive integer n. If n is even, divide it by $2$ to get $n / 2$ . If n is odd, multiply it by $3$ and add $1$ to obtain $3n + 1$ . Repeat the process (which has been called ""Half Or Triple Plus One"", or HOTPO) indefinitely. The conjecture is that no matter what number you start with, you will always eventually reach $1$ . To quote: Steiner (1977) proved that there is no 1-cycle other than
the trivial (1;2). Simons (2004) used Steiner's method to prove that
there is no 2-cycle. Simons & de Weger (2003) extended this proof up
to 68-cycles: there is no k-cycle up to k = 68. Beyond 68, this method
gives upper bounds for the elements in such a cycle: for example, if
there is a 75-cycle, then at least one element of the cycle is less
than $2385\times 2^{50}$ . Has there been any legitimate progress since then, in terms of cycles or anything else?","['number-theory', 'conjectures', 'collatz-conjecture']"
2279393,Decomposition of a unitary representation into a sum of finite dimensional ones,"Let $\pi$ be a unitary representation of a countable discrete group $\Gamma$ on a Hilbert space $H$. Suppose that there is a dense set $D$ of vectors in $H$ such that for every $x\in D$ the cyclic subrepresentation $\pi_x$ of $\pi$ given by $x$ contains a finite dimensional subrepresentation. Does it follow that $\pi$ is a sum of finite dimensional representations? Notice that when we strengthen the requirement from ""densely many"" to ""all"", i.e. we require that every cyclic subrepresentation contains a finite dimensional subrepresentation, then the conclusion above follows easily by the Zorn's lemma. I am curious if ""densely many"" is sufficient in the requirement. But perhaps there is some counterexample.","['functional-analysis', 'representation-theory', 'group-theory', 'hilbert-spaces']"
2279409,Prove that $10 \times 10$ grid filled with positive integers contains two elements sharing a side such that their difference is at least $6$.,"I have a $10 \times 10$ grid filled with different positive integers and want to prove that some two numbers sharing a side in a grid differ by at least $6$. My solution is: Let $m$ be the smallest and $M$ the greatest integer from the grid, then $M - m \ge 99$ (because every number in the grid is different). Consider the shortest path from $m$ to $M$. It consists of at most $19$ numbers (including $M$ and $m$). Let $x_1, x_2, ... x_k$ be the numbers on the path. Assuming that $|x_i - x_{i + 1}| \le 5$ we have $|M - m| = |x_1 - x_2| + ... + |x_{k - 1} - x_k| \le 5 * 18 = 90 < 99$, hence contradiction. It isn't very nice or clever or the one I would come up with quickly solution. Can you think of anything better? Maybe easier or shorter one?",['combinatorics']
2279417,Evaluating a definite integral involving $ \tan^{-1}$,The question is to evaluate $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} \cos x}}dx$$ I tried to take idea from the graph of $\tan^{-1} \tan x$ and rewrite the integral as $$\int_{\pi/2}^{5\pi/2} \frac{e^{\tan^{-1} \sin x}}{e^{\tan^{-1} \sin x}+e^{\tan^{-1} -\cos x}}dx$$.I couldn't proceed from here.Any ideas?Thanks.,"['definite-integrals', 'calculus']"
2279427,Convert a differential equation into an algebraic equation?,"The book I used (Calculus with Analytic Geometry by Thurman S. Peterson (printed in 1960)) says that: ""A relation among the variables which reduces a differential equation to an algebraic identity is called a solution of the equation."" Does it mean that we just convert a differential equation into its algebraic equation form and to verify that this algebraic equation comes from a certain differential equation, we called this algebraic equation the solution to a certain differential equation? Seems that the word ""solution"" in differential equations is not a traditional one. I wanna be enlightened. Thanks!",['ordinary-differential-equations']
2279442,"What is the value of following integral: $\int_{1/2014}^{2014}\frac{\tan^{-1}x} x \, dx$?","What is the value of following integral? 
  $$\int_{1/2014}^{2014}\frac{\tan^{-1}x} x \, dx$$ I am having problem evaluating this.","['integration', 'calculus']"
2279457,Why does $|e^{ix}| = 1$ when $x$ is a real number,I'm halfway through a question on Complex Analysis and part of the solution says that $|e^{ix}|=1$ when $x$ is real. But I cannot seem to find an explanation anywhere for this? I tried expanding it into trigonometric functions but got nowhere...,"['complex-analysis', 'exponential-function', 'complex-numbers']"
2279458,Solving $\frac{\partial^2 f}{\partial x^2}+\frac1x\frac{\partial f}{\partial x}+\frac1{x^2}\frac{\partial^2 f}{\partial t^2}=-a$,"$$\frac{\partial^2 f}{\partial x^2}+\frac1x\frac{\partial f}{\partial x}+\frac1{x^2}\frac{\partial^2 f}{\partial t^2}=-a$$
  $$\frac{\partial f}{\partial x}\big|_{x=0}=0, \frac{\partial f}{\partial x}\big|_{x=b}=l^2$$
  ($l\in\Bbb R$). I am trying to solve this linear differential equation but I am struggling to come up with a correct technique to solve this. Separation of variables is not effective due to the $-a$ term on the RHS. I haven't tried Fourier transforms yet, because I feel like there may be an easier way to do this. There is also the option of series solutions, but of course a closed form solution (if available) would be preferable. I have tried Wolfram Alpha since it could give an indication of what the solution should look like, but it didn't solve the equation. Any ideas?","['ordinary-differential-equations', 'partial-differential-equations']"
