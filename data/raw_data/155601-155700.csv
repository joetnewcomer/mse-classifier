question_id,title,body,tags
2636451,Can one compute the dot product knowing only the basis expansions of vectors v and w?,"Scenario: Suppose that $V$ is a finite dimensional vector space and that $\{e_1,...,e_n\}$ is a basis for $V$. Let $v,w\in V$ and suppose that $v=\sum_{i=1}^n \alpha _i e_i$ and $w=\sum_{i=1}^n \beta _i e_j$ where $\alpha_i$, $i=1,..,n$ and $\beta_j$, $j=1,...,n$. 
Can we compute $\langle v,w\rangle $ knowing only the basis expansions of $v$ and $w$ and the values of $\{\langle e_i,e_j\rangle :i,j=1,...,n\}$? What I understand: I know that $\langle v,w\rangle =\langle \sum_{i=1}^n \alpha _i e_i, \sum_{i=1}^n \beta _i e_j\rangle$ and that $\langle e_i,e_j\rangle=0$. The basis expansion would be any $\alpha_1 e_1 +\alpha_2 e_2+...+ \alpha_n e_n$ and similarly for $\beta_j e_j$. 
Is the problem asking whether the dot product is simply $\sum_{i=1}^n \alpha_i \beta_j$?","['matrices', 'inner-products', 'linear-algebra', 'vector-spaces']"
2636488,money changing problem,"Given $M\geq 1$ coins of denominations $a_1, \ldots, a_M$, I want to determine the number of ways to make change to obtain a value of $n\geq 0$ coins. We will assume that we have an unrestricted number of coins of each denomination. For example, if I have only pennies and nickels, the number of ways to obtain $10$ cents is 3 (two nickels, a nickel and five pennies, or 10 pennies). Now say I only have one kind of coin and its denomination is $a_1$. Then with my unlimited number of coins I can only form change that consists of multiples of $a_1$. I can easily create a generating function whose coefficients tell us the number of ways to make change for $a_1k$ coins for all $k \geq 0$, namely
$$\sum_{k=0}^\infty (x^{a_1})^k=\frac{1}{1-x^{a_1}}.$$
All the coefficients are $1$ because there is only one way to make change for $a_1k$ if I only have coins of denomination $a_1$. Now what I am trying to determine is the generating function whose coefficients give me the number of ways to make change for $n$ when I have more than one denomination. I believe that the answer is
$$\prod_{i=1}^M\left(\sum_{k=0}^\infty (x^{a_i})^k \right)=\prod_{i=1}^M \frac{1}{1-x^{a_i}}, $$
but I do not know how to prove this. Could someone provide some insight or reference a good proof? Thank you!","['generating-functions', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
2636511,"Find $P(\max(X_1,X_2,X_3)=1)$ of Poisson random variable.","Let $X_1 ,X_2,X_3$ be independent Poisson random variables with mean $1$  . Then $P(\max(X_1,X_2,X_3)=1)$  equals ? $(A)1-e^{-3}$ $(B) e^{-3}$ $(C)1-8e^{-3}$ $(D)7e^{-3}$ I usually solve these types of problem when we are given $P(\max(X_1,X_2,X_3)\leq1)$(say) then i proceed in this way if they are independent $P(X_1\leq1)P( X_2\leq1)P(X_3\leq 1)$. I have reasoning in my head behind this $P(\max(X_1,X_2,X_3)\leq1)$ is that if maximum order statistic is  $\leq1$ then other two random variables are also $\leq 1$ . Thus in this question $P(X_1\leq1)P( X_2\leq1)P(X_3\leq 1)$ changes as $P(X_1=1)P( X_2=1)P(X_3= 1)=e^{-3}$ Someone tell me if I am on right track and give me some more knowledge on this topic.","['statistics', 'probability', 'order-statistics', 'probability-distributions']"
2636558,Polynomial such that $f''(x) \rightarrow2$ as $x\rightarrow\infty$ given some values what is $f(1)$?,"Let $f$ be a polynomial such that $f''(x) \rightarrow2$ as $x\rightarrow\infty$, the minimum of f is attained at $3$, and $f(0)=3$, Then $f(1)$ equals. $(A) \ 1$ $(B) \ 2$ $(C) -1$ $(D) -2$ I am not sure how to deal with $f''(x) \rightarrow2$ as $x\rightarrow\infty$ . Give me a hint to try. EDIT : Work after hint Let $f(x)=ax^2+bx +c  $ $f''(x)=2\implies 2=2a \implies a=1 $ $f(0)=3=c$ $f(x)=x^2+bx +3  $ Using the fact that minima attained at 3 we have 
$f(3)=12+3b=3 \implies b=-3$ $f(x) = x^2-3x+3$ $f(1) = 1-3+3 = 1$","['derivatives', 'maxima-minima']"
2636605,What are all the unramified extensions of $\mathbb{Q}_p$?,"We know that for any $n$ relatively prime to $p$, $\mathbb{Q}_p(\zeta_n)$ is an unramified extension over $\mathbb{Q}$. On the other hand, we know that finite unramified extensions of a field complete with respect to a discrete valuation corresponds to finite extensions of its residue field. The residue field of $\mathbb{Q}_p$ is $\mathbb{F}_p$, and we know the finite extensions of $\mathbb{F}_p$ are $\mathbb{F}_{p^r} = \mathbb{F}_p(\zeta_{p^r-1})$, so the finite unramified extensions of $\mathbb{Q}_p$ should be $\mathbb{Q}_p(\zeta_{p^r-1})$. Is it true that for any $n$ such that $(n,p) = 1$, there is an $r$ such that $\mathbb{Q}_p(\zeta_n) = \mathbb{Q}_p(\zeta_{p^r-1})$?","['number-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
2636610,Proving AM-GM with the method of Lagrange multipliers,"In my calculus book, there is a question that basically says ""use the method of Lagrange multipliers on $f(x,y,z)=xyz$  with constraint $g(x,y,z)=x+y+z=C$, $C$ being a constant, and use this to prove AM-GM for three variables."" The next question asks to generalize this result. It is very easy to see that the only possible location for an extremum of f along the constraint is when all three variables equal to each other, and that an other value of f along the constraint is less than this, but I couldn't think of a way to show that f actually has a maximum along the constraint short of independently proving AM-GM and applying it here.",['multivariable-calculus']
2636627,Divisiblity of binomial coefficients,Find the number of binomial coefficients in the expansion of $(1+x)^{2018}$ that are divisible by $13$ On searching the internet for a long time I found that these type of questions are relevantly solved using either Kummer 's theorem or the Luca's theorem. But I haven't studied them yet. So can someone please explain some method to solve these problem without using those theorems.,"['number-theory', 'binomial-coefficients']"
2636634,Manipulate the series and find limit.,"Let $$ L= \lim_{n\to \infty} \sum_{r=0}^n \frac{2^r}{5^{2^r}+1},$$
 then find $L$. I tried various ways to manipulate it to difference series, but failed to do so.
Please help Thankyou","['sequences-and-series', 'limits']"
2636715,Is it possible to partition a topological space into nonempty disjoint closed subsets?,"Let $X$ be a topological space containing at least 2 elements. Is it true, in general, that there exist 2 disjoint closed nonempty subsets $A$ and $B$ of $X$, such that $$X = A \cup B?$$ 
Is it false, in general?",['general-topology']
2636766,When Burnside's Lemma does not apply,"I have a finite group G which acts on a set X . I want to establish the number of distinct members of a subset Y $\subset$ X , however Y is not closed under G (I think that the correct terminology here is that Y is not an invariant subset of X ?), so we can't use Burnside's Lemma. The alternative, which is to count all the orbits of Y under G is ruled out because it is computationally out of reach: | G | = 3,359,232, while | Y | = 554,191,840,696. My general question: is there any way we can reduce the complexity of the orbit-counting problem? Supplementary question: I see this in the Wikipedia entry for ""Burnside's Lemma"": The first step in the proof of the lemma is to re-express the sum over
  the group elements g $\in$ G as an equivalent sum over the set of
  elements x $\in$ X : $\sum_{g \in G}|X^g| = |\{(g,x)\in G\times X :  g.x = x\}| = \sum_{x \in X} |G_x|.$ Does this equivalence hold in general, ie would it apply to my subset Y , or does it only apply for invariant subsets, ie: when the Lemma itself is applicable? My hope is that somehow I can still express the orbit-counting problem in terms of fixed points, which are computationally more accessible.","['finite-groups', 'group-theory']"
2636777,Why are some non elementary integrals defined and others are not?,"Why are some integrals that cannot be integrated in elementary terms defined and given names, while others aren’t? Based on what criteria are they chosen? Applicability to real life? And what is the point if we cannot solve them? For example: $$\int\frac{\sin(x)}{x}\,dx=\text{Si}(x), \quad -\int_{-x}^\infty \frac{e^{-t}}{t}\,dt=\text{Ei}(x), \quad \int \cos\left(x^2\right)\,dx = \sqrt{\frac\pi2} \text{C}\left( \sqrt{\frac2\pi}x\right)$$
while others like 
$$\int x^x \,dx$$
are not defined.",['integration']
2636820,"Let $A,B$ be some sets such that $|A|=a, |B|=b$. Prove $\binom{a}{b}=|P_b(A)|$ is a well defined expression.","Let $A,B$ be some sets such that $|A|=a, |B|=b$. Prove $\binom{a}{b}=|P_b(A)|$(=the set of all subsets of A of cardinality $b$) is a well defined expression. Hello all. In this question I need to show that the equality $\binom{a}{b}=|P_b(A)|$ holds for all sets $A\neq A' \land |A|=|A'|=a, B\neq B'\land |B|=|B'|=b $. I don't really know how to show it, I thought about doing some tricks with the binomial theorem but I guess it has nothing to do with that. Would love to get your help... thanks in advance :)","['cardinals', 'elementary-set-theory', 'discrete-mathematics']"
2636834,Proving that the inverse of a matrix $H$ is itself,"Let $u$ be a column vector in $\mathbb{R}^n$, and let $H = I_n - uu^T$ where $I_n$ is the $n \times n$ identity matrix. Also, it is known that $H$ is symmetric. Prove that if $u^Tu = 2$, then the inverse of $H$ is itself. Theoretically speaking, I know the easiest way to approach this is to show that $HH = I_n$. Currently I'm stuck at the following and can't get past the last equation: $$HH = (I_n - uu^T)(I_n - uu^T) = I_n^2 - 2uu^T + (uu^T)^2 = \cdots$$ How can I solve this equation using $u^Tu = 2$? I just can't see where this piece of information can assist me in proving the desired equation. I can't manipulate the last part to suit my needs. Tips and tricks are greatly appreciated!","['matrices', 'linear-algebra', 'vectors', 'inverse']"
2636845,"Check differentiability of $f(x,y)=y\sin\frac{1}{x}$ at $(0,0)$","Given , $f(x,y)=y\sin\frac{1}{x}$ when $x\ne0$ and $f(0,0)=0$ . Investigate differentiability at $(0,0)$ . I've found that it is continuous at $(0,0)$ and the partial derivatives $f_x=0$  $\forall (x,y)$ and $f_y=\begin{cases}
\sin\frac1x & \text{ if } x\ne0\\ 
1 & \text{ if } x= 0
\end{cases}.$ For differentiability it is sufficient to show that both partial derivatives exist and one of them(confused between ""both continuous"" Or ""one of them"") is continuous about some neighbourhood of $(0,0)$ . Now, I'm stuck. Please help how to think. EDIT $f(0,y)=y$","['multivariable-calculus', 'derivatives']"
2636870,Deciding monotonic nature of $h(x)$ given another function $f(x)$,"Question: If $f(x)$ is a strictly increasing function, then $h(x)=f(x)-a(f(x))^2+a(f(x))^3$ is a non-monotonic function for what set of values of $a$? Here both $f(x)$ and $h(x)$ take only real values. My attempt: For $h(x)$ to be monotonic, it's derivative should either be increasing or decreasing for all $x$. So, by putting the derivative of the function as positive for all $x$ and considering the fact that $f'(x)$ is also positive, I got the set of values of a between $(0,3)$. However the answer excludes all of these values. Please provide a detailed solution.
All help is appreciated.","['derivatives', 'calculus']"
2637003,Intuitive meaning of attitude error function $\Psi$ defined over $SO(3)$. Is $\Psi$ a metric?,"We define the attitude error function $\Psi(R, R_d)$ over $SO(3)$ as :
$$ \Psi(R, R_d) = \frac{1}{2}tr(I - R_d^{\top}R)$$
This acts as a metric to define distance between two rotation matrices which otherwise can't be calculated using euclidean vector space subtraction. But we also know that $\Psi = \mathbb{cos(\phi)}$, where $\mathbb{\phi}$ is the rotation angle about the rotation axis as obtained through the axis-angle representation of rotation matrices. I wanted to ask if $\Psi$ is actually a metric (in a formal sense) or not? Meaning does it satisfy the triangle inequality :
$$ \Psi(I, R) \le \Psi(I,R_d) + \Psi(R_d,R)$$ I tried working it out, also did brute force expansion, but couldn't really come to a conclusion. Since $\Psi$ corresponds to the rotation angle, I feel like this should be true as we can see by keeping an axis fixed, say $e_1$ and perform 2 successive rotations about the same axis corresponding to $R_d$ and $R$ respectively. I have very elementary knowledge regarding the above. Any help is appreciated.","['manifolds', 'general-topology', 'differential-geometry', 'error-function']"
2637033,Strong consistency of sample variance,"An unbiased estimator for a population's variance is: $$s^2=\frac{1}{n-1}\sum_{i=1}^{n} \left( X_i - \bar{X} \right)^2$$ where $$\bar{X} = \frac{1}{n}\sum_{j=1}^{n} X_j$$ Now, it is widely known that this sample variance estimator is simply consistent (convergence in probability). I wonder, is it also true that it is strongly consistent, i.e. it converges to population variance almost surely? And if yes, are there any additional requirements for $\{X_n\}_{n\geq 1}$?","['statistics', 'parameter-estimation']"
2637034,Show that limit of $e^{e^{-xy}}$ as $ x^2+y^2 \to \infty$ does not exist,"The problem consists of showing that $\lim\limits_{x^2+y^2 \to \infty}e^{e^{-xy}}$ does not exist. My initial approach was to set $x = t, y = 0$ and show that the function converges to different limits when I let $t \to \infty$ compared to when $t \to -\infty$. From that, I concluded that the function does not have a limit. However, I am not sure whether it is a valid approach to set $y=0$ in this case. Is this the right way to go about it, or should I try some other strategy? I tried using polar coordinates but that does not get me far. Thanks,","['multivariable-calculus', 'limits']"
2637059,Prove or disprove that every Holomorphic function preserving unboundedness is a polynomial.,Roughly speaking as the title says: Prove or disprove that every holomorphic function preserving unboundedness is a polynomial. Let $f : \Bbb C \to \Bbb C$ be a holomorphic function such that for every unbounded set $ V \subset \Bbb C$ the image $f(V)$ is also unbounded. Prove or disprove that $f$ must be a polynomial.,"['complex-analysis', 'analytic-functions', 'holomorphic-functions']"
2637061,A limit exists iff and only the left limit and the right limit exist and are equal to each other,"It is well known that $$\lim_{x \to a} f(x) = L \iff \lim_{x \to a+}f(x) = L = \lim_{x \to
 a-}f(x)$$ Consider the function $\sqrt{.}: \mathbb{R}^+ \to \mathbb{R}$ Now, consider $\lim_{x \to 0} \sqrt{x}$ We can prove this limit is equal to $0$. Indeed, let $\epsilon > 0$. Choose $\delta = \epsilon^2$. Then, for $x \in \mathbb{R}^+$ satysfying $0 < |x| < \delta$ or equivalently $0 < x < \delta$, we have $\sqrt{x} < \sqrt{\delta} = \epsilon$, which establishes the result. However, my confusion lies in the following: the limit from the left does not seem to exist, making the above theorem untrue. Where lies my mistake?","['real-analysis', 'limits']"
2637063,"Holomorphic function mapping unit disc to the ""pacman"" $U = \{|z|<1,\ \mathrm{Arg}z \notin [-\frac{\pi}{4},\frac{\pi}{4}]\}$","Find an injective and surjective holomorphic function, that transfers the unit disc $\Bbb D = \{|z|<1\}$ to the domain $U = \{|z|<1,\ \mathrm{Arg}z \notin [-\frac{\pi}{4},\frac{\pi}{4}]$}. I tried to begin with a branch of $\log$, and after applying $z \mapsto z- \frac{\pi}{4}$ and $z \mapsto \frac{2}{3}z$ I got to the domain $\{z 
\in \Bbb C :\mathrm{Re}(z)<0, 0<\mathrm{Im}(z)<\pi \}$. Next I thought about getting to half a plane, and using Caley transform $z \mapsto \frac{z-i}{z+i}$ and be done, but I'm having trouble with this part. Any ideas of such a mapping/something else?","['complex-analysis', 'holomorphic-functions', 'mobius-transformation']"
2637081,Estimating the measure of a sum of subsets,"Let $A,B \subset \mathbb R^n$ be compact measurable subsets and let $A + B = \{ a+b \mid a \in A, b \in B \}$. If $m$ is the Lebesgue measure, is it possible to relate $m(A + B)$ to $m(A)$ and $m(B)$? In $\mathbb R$, for instance, $m([a,b] + [c,d]) = m([a+c, b+d]) = (b-a) + (d-c) = m([a,b]) + m([c,d])$. I would be happy, in general, with something like $m(A+B) \le m(A) + m(B)$ but I do not know whether this is possible.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2637083,Where am I going wrong in interpreting this problem as a gambler's ruin problem?,"I was trying to solve this problem (Strategic Practice Week 3, Homework problem 4 in Harvard's Stat 110 class) , by framing it as a gambler's ruin problem: Calvin and Hobbes play a match consisting of a series of games, where
  Calvin has probability $p$ of winning each game (independently) and $q = 1-p$. They
  play with a “win by two” rule: the first player to win two games more
  than his opponent wins the match. Find the probability that Calvin
  wins the match (in terms of $p$), by interpreting the problem as a
  gambler's ruin problem. Here's how I approached the problem: Let, $W:$ Calvin wins the match; $D_i:$ (Wins by Calvin) $-$ (Wins by Hobbes) $= i$ $p_i:$ $\Pr($W | $D_i$$)$ Now, by conditioning on the first game, and using the law of total probability, we get: $p_i = p p_{i+1} + qp_{i-1}$, with $p_2 = 1$ (Calvin wins with certainty if the difference is $2$) and $p_{-2}  = 0$ (Calvin loses with certainty if the difference is $-2$). Solving this recurrence relation (which I omit here, since it's mostly algebra gymnastics), we get: $p_i = \dfrac{p^6}{p^8 - q^4}p^i - \dfrac{p^2q^2}{p^8-q^4}(\dfrac{q}{p})^i$ Since both Calvin and Hobbes start with a $0$ difference in wins, what we need to find is $p_0 = \dfrac{p^2(p^4-q^2)}{p^8-q^4}$. However, the answer turns out to be $\dfrac{p^2}{p^2+q^2}$, which can be easily obtained by using the law of total probability and conditioning on the number of wins in the first 2 matches (0 win, 1 win or 2 wins, with the number of wins $X \sim $ Bin($2,p$)). Where am I going wrong with my interpretation of the problem as a gambler's ruin problem?","['probability', 'gambling']"
2637138,Compose the function $f$ $n$-times recurrently,"The function $f$ is defined as $f(x) = 3x+2$.
If I were to compose the function $f$ $n$-times recurrently I would start doing it $n = 1$ times, $n = 2$ times etc., until I can find a pattern between the result and $n$. So far I have: $n=1$: $(f∘f) = f(3x+2) = 3(3x+2)+2 = 9x+8$ $n=2$: $f(f∘f) = f(9x+8) = 3(9x+8)+2 = 27x+26$ $n=3$: $f(f∘f∘f) = f(27x+26) = 3(27x+26)+2 = 81x+80$ Do I need to find some sort of correlation between the result in square brackets and n to solve this particular problem?
Any help will be appreciated and this is my first post so I apologize if I haven't done it right :)","['functions', 'discrete-mathematics']"
2637221,Show that $A+B$ contains at least $m+n-1$ elements.,"Let $A,B \subset \mathbb Z$ such that $|A|=m$ and $|B|=n$. Then show that $|A+B| \geq m+n-1$. How can I proceed? I have tried to proceed by using law of trichotomy but I only managed to find $\mathrm {max} (m,n)$ elements in $|A+B|$. How should I proceed? Thank you in advance.","['combinatorics', 'additive-combinatorics']"
2637235,"Evaluate $\int_0^\infty\frac{\sin(\varphi_1x)}x\frac{\sin\varphi_2x}x\cdots\frac{\sin\varphi_nx}x\frac{\sin(ax)}x\cos(a_1x)\cdots\cos(a_mx) \, dx$","How to evaluate $$ 
\int_0^\infty \frac{\sin(\varphi_1x)}{x}\frac{\sin\varphi_2x}{x} \cdots \frac{\sin\varphi_nx}{x} \frac{\sin(ax)}{x}\cos(a_1x) \cdots \cos(a_mx) \, dx  \text{ ?}
$$ For small $n$ and $m$ it's simple (setting $\sin(kx)=\dfrac{e^{ikx}-e^{-ikx}}{2i}$ and using Jordan's lemma, but for arbitrary $n$ the calculation is too tedious. Surely there must be some nice trick here? Thanks","['residue-calculus', 'calculus', 'complex-analysis', 'improper-integrals', 'definite-integrals']"
2637240,"Prove the following by two different methods, one combinatorial and one algebraic","Reading through my textbook I came across the following problem, and I am looking for some help solving it. I am asked to prove the following by two different methods, one combinatorial and one algebraic. If I could get help with either or both it would be great, thanks! Prove that this identity is true, $$\binom{n}{k} -\binom{n-3}{k} =\binom{n-1}{k-1} + \binom{n-2}{k-1} + \binom{n-3}{k-1}$$","['combinatorics', 'proof-writing', 'binomial-coefficients']"
2637260,Conversion of Surface integral to a suitable Volume integral.,"While deriving the Euler's equations of motion in case of Fluid dynamics, I came across this part - Here $p$ denotes the hydrostatic pressure(scalar function)
I am unable to understand how it transformed this - $\iint _{\Delta S} - p\hat{n}~ds = -\iiint_{\Delta V} \nabla p~dv$ It says its a consequence of Gauss Divergence theorem but I could try only the below - $\iint _{\Delta S} - p\hat{n}~ds = \iiint _{\Delta V} \nabla\cdot(-p)~dv$ , but this seems wrong as how can i take the divergence of the scalar function? Reference - Textbook of Fluid Dynamics by F. Chorlton, page 96, Euler's equation of motion","['fluid-dynamics', 'surface-integrals', 'multivariable-calculus', 'divergence-operator', 'vector-analysis']"
2637357,"A smooth map from $Gr_{\mathbb{C}} (1,2)$ to $Gr_{\mathbb{R}} (2,4)$","$\mathbf{Problem}$: Let $Gr_{\mathbb{C}} (1,2)$ be the complex Grassmannian manifold that consists of all the complex lines going through the origin in $\mathbb{C}^2$, and $Gr_{\mathbb{R}} (2,4)$ be all the 2-dimensional linear subspaces of $\mathbb{R}^4$. Show that the function $f: Gr_{\mathbb{C}}(1,2) \to Gr_{\mathbb{R}}(2,4)$ is smooth, where f maps a complex line to itself regarded as a 2-dimensional linear subspace of $\mathbb{R}^4$. $\mathbf{Attempt}$: 1) $\mathbf{Setup}$: Proving the map $f$ smooth is to prove that $\psi_j \circ f \circ \phi^{-1}_i$ is smooth when the composition makes sense. The charts I am using for the Grassmannians are the standard ones: for $Gr_{\mathbb{C}} (1,2)$, define $I = \{1,2\}$, $I_i = \{i\}$, $\mathbb{C}^{I_i} = \{(z_1, z_2): z_j = 0,  \forall j \in I - I_i \}$, and $U_{I_i} = \{E \subset \mathbb{C}^2: E \cap \mathbb{C}^{I - I_i} = \{0\} \}$. Define the function $\phi_i: U_{I_i} \to Hom(\mathbb{C}^{I_i}, \mathbb{C}^{I - I_i})$  by $\phi_i(E) = l_E$, the linear map from $\mathbb{C}^{I_i}$ to $\mathbb{C}^{I - I_i}$ such that $E = \{z + l_e(z): z \in \mathbb{C}^{I_i}\}$, i.e., E is the graph of it. Similar definition holds for $Gr_{\mathbb{R}} (2,4)$. 2) Geometrically the problem seems straightforward as I am not really changing anything about the line itself, but I am stuck in proving smoothness of the composition of the functions. In class we defined the smoothness of a function between manifolds by showing  $\psi_j \circ f \circ \phi^{-1}_i$ is a smooth function mapping from a Euclidean space to a Euclidean space, which in our case is not exactly so, since $\psi_j \circ f \circ \phi^{-1}_i: Hom(\mathbb{C}^{I_i}, \mathbb{C}^{I - I_i}) \to Hom(\mathbb{R}^{J_j}, \mathbb{R}^{J - J_j})$, so we seem to be mapping linear functions to linear functions. I know in general $Hom(\mathbb{R}^{I'}, \mathbb{R}^{I - I'})$ is isomorphic to $\mathbb{R}^{k(n-k)}$ for $|I'| = k, |I| = n$ (similar case for $Hom(\mathbb{C}^{I'}, \mathbb{C}^{I - I'})$), but how does this work in our context exactly? 3) Another question I have is of more general context. For functions that maps between Euclidean spaces, if the functions is smooth over the Euclidean space, then it must be smooth mapping between the manifolds right? Any help is appreciated!","['grassmannian', 'smooth-manifolds', 'differential-geometry']"
2637358,Computing the product $A_1 A_2 \cdots A_n$ with $A_n =\left(\displaystyle\begin{smallmatrix} n^2&1\\-1&n^2\end{smallmatrix}\right)$,"Let $n \in \Bbb N$ and consider the square matrix $$A_n =\begin{pmatrix} n^2 & 1\\-1 & n^2\end{pmatrix}.$$ Prove that there are sequence, $x_n,y_n$ such that 
  $$A_1A_2\cdots A_n =\begin{pmatrix} x_n&y_n\\-y_n& x_n\end{pmatrix}.$$ Find the explicit expression of $x_n$ and $y_n$. What can we say about the convergence of  $x_n$ and $y_n$? I have shown the existence of  $x_n$ and $y_n$ by induction and it turn out after identification that they satisfy the relations, 
$$ x_{n+1} =(n+1)^2x_n-y_n, \qquad\qquad y_{n+1} = x_n +(n+1)^2y_n$$ Can someone help to solve this recursive formula in other to get into the two last questions? Is there a more elegant way to overcome this problem?","['matrices', 'contest-math', 'sequences-and-series', 'closed-form']"
2637366,Bijection from the irrationals to the reals,"Since the irrationals and the reals have the same cardinality, there must be a bijection between them. Somewhere on this forum I found something like this: Map all of the numbers of the form $q + n \sqrt2$ where $q \in \mathbb
 Q$ and $n \in \mathbb N$ into $q + (n+1) \sqrt2$ and map the rest of
  these numbers into themselves. The problem is that I can't see how this function could possibly work. For example, we will never get the value of $2$, since $2$ is not an irrational number and adding multiples of $\sqrt 2$ will never make a number rational. Could you please explain to me why this function works?","['elementary-set-theory', 'real-numbers', 'functions']"
2637428,Does having closed forms of two generating functions guarantee that one can find the closed form of their term-by-term product?,"I want to find the closed form of $$ G_n(k) = \sum_{k=0}^n k! \bigg\lbrace {n \atop k}\bigg\rbrace x^k $$ Suppose the closed form of $$ E_n(k) = \sum_{k=0}^n k!\, x^k $$ is known; call it $ A_n(k)$. And suppose also that the closed form of $$ F_n(k) = \sum_{k=0}^n \bigg\lbrace {n \atop k}\bigg\rbrace x^k $$ is known; call it $B_n(k) $. Using these generating functions and their closed forms, can I acquire a closed form for $G_n(k)$ ? I know I can't just multiply the two functions because of the powers of $x$, but I don't think a typical convolution will give a congenial result.","['generating-functions', 'combinatorics', 'summation', 'sequences-and-series']"
2637430,"Limits with two variables, finding k such that limit exists","Find the biggest number $k$, such that the limit $$
\lim_{(x,y)\to(0.0)} \frac{x^{15}y^{23}}{(x^2 +y^2)^p} $$ exists for all $p < k $ I was thinking that if we're left with x's and/or y's in the numerator and denominator, we have an expression in an undetermined form, $0 \over 0$ so i though that $k = 7.5$ would be correct, seeing as that would give a $0\over 0 $ expression, atleast from what I've calculated.","['multivariable-calculus', 'limits']"
2637518,Piecewise integration: find the 75th percentile of this continuous random variable.,"The problem from a quiz of mine is as follows: In general, I know how to find the $p$th percentile of a random variable $X$, given its probability density function $f(x)$. You simply solve for the value $x_p$ that satisfies the following equation: $\int^{x_p}_{-\infty}{f(x)dx} = \frac{p}{100}$ When given just a single function that isn't broken up piecewise like this, I know how to proceed. But here's my issue with this problem: How do I split up the integral into multiple pieces, considering the upper limit of integration is a variable?","['statistics', 'integration', 'percentile']"
2637546,"What is the name of the conjecture related to what Matt Parker has called ""The Square-Sum Problem""?","I read a book called Things to Make and Do in the Fourth Dimension , written by Australian mathe-matician and comedian, Matt Parker. In one part of the book, I remember him explaining about a conjecture such that for all $n\geqslant 89$, you can arrange the elements of the set $\{1, 2,\ldots,n\}$ in a certain way where each adjacent pair of elements sums to a squared number. For example, let $n = 17$. Al-though $17<89$, below is a good example to demonstrate what I mean: We have the set $\{1, 2, 3,\ldots, 17\}$ which can also be written as $\mathbb{N}_{\leqslant 17}$. How can we order each number in a certain way such that every adjacent pair of numbers in the ordered sequence add up to a squa-red number? Well, we order it like so: Let $S_{17} = [\ldots]$ be our sequence that orders the elements from $\{1, 2, 3,\ldots, 17\}$ in a certain way as mentioned in the foregoing then, $$S_{17} := \big[17, 8, 1, 15, 10, 6, 3, 13, 12, 4, 5, 11, 14, 2, 7, 9, 16\big].$$ Here, $17 + 8 = 5^2$, $8 + 1 = 3^2$, $1 + 15 = 4^2,\ldots$ The sequence in the sandbox above is a special case where it has $17$ elements and begins with $17$. Take the sequence $S_{16}$ then this sequence ends with $16$. In fact, it is exactly the same as $S_{17}$ except it does not start from $17$, but $8$ instead. However, the sequence $S_{18}$ does not exist, and there are many sequences with this property that do not exist. The conjecture is interesting because if true, it will prove that there are only finitely many sequences $S_n$ that do not exist, also proving the contrapo-sitive. I did some research and it seems like this is true for $89\leqslant n \leqslant 300$ thus far, but I do not know the name of this conjecture. Does it even have a name? I also haven’t stumbled across any attempts of proving this conjecture. Can it be done? I guess that is two questions, then. Thank you in advance.","['conjectures', 'sequences-and-series']"
2637577,Proof of the 'Infinite' DeMorgan's Law.,"Suppose I wanted to prove $\Big(\bigcup_{n=1}^{\infty} B_n \Big)^c = \Big(\bigcap_{n=1}^{\infty} B_n^c\Big)$. I've written out a proof of this, and was hoping someone could tell me whether it appears to be on the right track. I've proved several types of theorems of this kind, but only with a finite number of sets. Take the set $X \in \Big(\bigcup_{n=1}^{\infty} B_n \Big)^c$. Then, $X \not \in \bigcup_{n=1}^{\infty} B_n$, and so for $n \in [1, \infty)$, $X \in B_k^c$, equivalent to $\Big(\bigcap_{n=1}^{\infty} B_n^c\Big)$. So $\Big(\bigcup_{n=1}^{\infty} B_n \Big)^c \subset \Big(\bigcap_{n=1}^{\infty} B_n^c\Big)$. (The last step seems like a jump to me, but I don't quite know another way other than to say $X \not \in B_1, X \not \in B_2, \ldots$ and thus $X \in B_1^c$, $X \in B_2^c$, $X \in B_c^x, \ldots$, but that doesn't seem convincing without some sort of inductive argument.) Now take $X \in\Big(\bigcap_{n=1}^{\infty} B_i^c\Big)$. This requires that $X \not \in B_i$ for $i \in [1, \infty)$, and $x \not \in \bigcup_{n=1}^{\infty} B_n$, which implies $X \in \Big(\bigcup_{n=1}^{\infty} B_n\Big)^c$, and thus $\Big(\bigcap_{n=1}^{\infty} B_n^c\Big) \subset \Big(\bigcup_{n=1}^{\infty} B_n\Big)^c$. From $\Big(\bigcup_{n=1}^{\infty} B_n \Big)^c \subset \Big(\bigcap_{n=1}^{\infty} B_n^c\Big)$ and $\Big(\bigcap_{n=1}^{\infty} B_n^c\Big) \subset \Big(\bigcup_{n=1}^{\infty} B_n\Big)^c$, we conclude $\Big(\bigcup_{n=1}^{\infty} B_n \Big)^c = \Big(\bigcap_{n=1}^{\infty} B_n^c\Big)$. How does this proof look? Are there jumps in my logic? Thanks in advance.","['elementary-set-theory', 'proof-verification']"
2637583,A naive conjecture about Taylor series convergence,"Conjecture. Let $f$ be a real (or complex) analytic function defined on some open subset $U$ of real (complex) numbers and assume that $p,q,x\in U$ are such that $x\in B(p,r_p)\cap B(q, r_q)$, where $r_p$ and $r_q$ are radiuses of convergence of $f$ expanded at $p$ and $q$ respectively and where $B$ stands for an open ball. If
  $$\frac{|x-p|}{r_p}<\frac{|x-q|}{r_q},$$
  then $P_n = o(Q_n)$, where
  $$P_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(p)}{i!}(x-p)^i\right|\quad\text{and}\quad  Q_n = \left|f(x) - \sum_{i=0}^n\frac{f^{(i)}(q)}{i!}(x-q)^i\right|$$ I came up with this conjecture while I was playing with an approximation of the natural logarithm using Taylor series. So, below I present the path of reasoning which lead me to this conjecture. I put a more detailed anaysis in this jupyter notebook . If $p>0$ and $x\in (0, 2p)$, then
$$\ln(x)=\ln(p) - \sum_{i=1}^\infty\frac{1}{i}\left(\frac{p-x}{p}\right)^i.$$ By looking at the above formula, we notice that the very first term is $\ln(p)$. Let us say that we want to approximate $\ln(x)$, so we forbid ourselves to use logarithm in this series. Hence, we constrain ourselves to $p$ of the form $e^K$, where $K$ can be any integer. Problem. For a given point $x>0$, find $p=e^K$ such that Taylor series of $\ln$ expanded at $p$ converges to $\ln(x)$ in the fastest way. We know that if $x$ close to an edge of the interval of convergence, then the Taylor series converges very slowly. Thus, using this basic fact, I found the answer heuristically. In a nutshell, the idea is to compare distance from $x$ to $p$ but also take into account the size of the interval of convergence. Thus, we just look for best $p$ such that
$$\frac{|x-p|}{r_p}$$
is the smallest, where $r_p$ is the radius of the convergence of $\ln$ a $p$. So, if $x\in (e^K, 2e^K)$, then we should compare 
$$\frac{x-e^K}{e^K}\quad\text{with}\quad\frac{e^{K+1}-x}{e^{K+1}}$$
and put $p = e^K$ if the first one is smaller and put $p = e^{K+1}$ otherwise. For $x = C_0e^K$, where $C_0 = \frac{2e}{1+e}$, we have the equality of the above two numbers. Thus, in order to compute $\ln(x)$ using Taylor series, it is best to start with $p=e^K$, where $K$ is such that $x\in[C_0e^{K−1},C_0e^K)$. I have done some numerical tests whether this $C_0$ is the optimal one and it looks like it is. You are welcome to see the details in this already mentioned jupyter notebook . So, I wonder if this property generalizes to an arbitrarty analytic function and the conjecture is a manifestation of my wonderings.","['real-analysis', 'taylor-expansion', 'asymptotics', 'complex-analysis', 'numerical-methods']"
2637601,Finding the maximum value without using derivatives,"Find the maximum value of $$f(x)=2\sqrt{x}-\sqrt{x+1}-\sqrt{x-1}$$ without using derivatives. The domain of $f(x)$ is $x \in [1,\infty)$. Then, using derivatives, I can prove that the function decreases for all $x$ from $D(f)$ and the maximum value is $f(1)= 2 - \sqrt{2}$. However, this uses derivatives.","['radicals', 'optimization', 'algebra-precalculus', 'maxima-minima', 'fractions']"
2637618,Is this chaining property of conditional expectation correct?,"Using the general measure-theoretic definition of conditional expectation, let $X$ be a random variable measurable with respect to the $\sigma$-algebra $\mathscr{F}$, and let $\mathcal{G} \subset \mathscr{F}$ and $\mathcal{H} \subset \mathscr{F}$ be two $\sigma$-algebras contained in the larger $\sigma$-algebra $\mathscr{F}$. Note that the intersection of $\sigma$-algebras is again a $\sigma$-algebra. Question: Is it true in general that $$\mathbb{E}[\mathbb{E}[X|\mathcal{G}]|\mathcal{H}] = \mathbb{E}[X | \mathcal{G} \cap \mathcal{H}] = \mathbb{E}[\mathbb{E}[X|\mathcal{H}]|\mathcal{G}]\,? $$ Note: A pointer to a reference would be more than sufficient for an accepted answer. If you know of a brief and simple counterexample, of course, feel free to post it. /Note I know that it is true in special cases. For example, the tower property, the case when $\mathcal{G} \subset \mathcal{H}$, is an example. Also the property that $\mathbb{E}[X|\mathcal{A}]=\mathbb{E}[X]$ when $\sigma(X)$ and $\mathcal{A}$ are independent, because their independence implies that their intersection is the trivial $\sigma$-algebra, with respect to which the conditional expectation of any random variable is just its expectation in the conventional sense. This question and this one seem to discuss what appears to  also be a special case, which is according to Billingsley is apparently true. See also this related question . It almost seems to follow from the definition of conditional expectation, but the definition one usually sees, e.g. on Wikipedia , does not seem to be applicable to the case when $\mathcal{G} \not\subset \mathcal{H}$ and $\mathcal{H} \not\subset\mathcal{G}$. Useless comments: It would be helpful to know whether or not this is the case, so as to better be able to avoid false identities which contradict this property (in case the property is actually, in fact, true). E.g. if one denotes the join of two $\sigma$-algebras, $\sigma(\mathcal{G} \cup \mathcal{H})$ (the smallest $\sigma$-algebra containing both $\mathcal{G}$ and $\mathcal{H}$) by $\mathcal{G} \lor \mathcal{H}$, it is extremely tempting for me to write stupid things like $$""\mathbb{E}[\mathbb{E}[X|G]|\mathcal{H}] = \mathbb{E}[X| \mathcal{G} \lor \mathcal{H}] = \mathbb{E}[\mathbb{E}[X|\mathcal{H}]|\mathcal{G}] "" \,, $$ since in the case that $\mathcal{G} = \sigma(Y)$ and $\mathcal{H} = \sigma(Z)$, so that $\mathcal{G}\lor \mathcal{H} = \sigma(Y,Z)$, it corresponds to the very tempting (in general) false identity $$""\mathbb{E}[\mathbb{E}[X|Y]|Z] = \mathbb{E}[X|Y,Z] = \mathbb{E}[\mathbb{E}[X|Z]|Y] ]"" \,. $$","['reference-request', 'probability-theory', 'conditional-expectation', 'measure-theory']"
2637677,Why is this binomial equation not an identity between polynomials?,"In the book Concrete Mathematics , the following identity appears:
$$(r-k){r\choose k} = r {r-1\choose k}$$
It is followed by a proof that it holds for all real r. This proof makes use of the property that both sides of [the identity] are polynomials in r of degree k+1. So far so good. However, the passage goes on by claiming that the following identity cannot be extended to real values for r this way, because it is not and identity between polynomials.
$${n \choose k} = {n \choose n-k}$$ How can the factors (r-k) and r make the first identity polynomial when the second is not; am I missing some other difference that is critical?","['binomial-coefficients', 'discrete-mathematics']"
2637690,Is there a formula to calculate the area of a trapezoid knowing the length of all its sides?,"If all sides: $a, b, c, d$ are known, is there a formula that can calculate the area of a trapezoid? I know this formula for calculating the area of a trapezoid from its two bases and its height: $$S=\frac {a+b}{2}×h$$ And I know a well-known formula for finding the area of a triangle, called Heron's formula: $$S=\sqrt {p(p-a)(p-b)(p-c)}$$ $$p=\frac{a+b+c}{2}$$ But I could not a formula for finding the area of a trapezoid in the books.","['quadrilateral', 'euclidean-geometry', 'area', 'geometry']"
2637739,The partial derivative of the complex quadratic statement,"Assume a complex-valued function $ h\left( {a,\theta } \right) = f\left( {a,\theta } \right) + ig\left( {a,\theta } \right) $  where $ a = \left[ {\begin{array}{*{20}{c}}
  {{a_1}}& \cdots &{{a_n}} 
\end{array}} \right] $  and $ \theta  = \left[ {\begin{array}{*{20}{c}}
  {{\theta _1}}& \cdots &{{\theta _n}} 
\end{array}} \right] $  has been represented by a quadratic matrix form
$$ h\left( {a,\theta } \right) = {x^H}Ax = \left[ {\begin{array}{*{20}{c}}
  {{a_1}{e^{ - j{\theta _1}}}}& \cdots &{{a_n}{e^{ - j{\theta _n}}}} 
\end{array}} \right]A\left[ {\begin{array}{*{20}{c}}
  {{a_1}{e^{j{\theta _1}}}} \\ 
   \vdots  \\ 
  {{a_n}{e^{j{\theta _n}}}} 
\end{array}} \right] $$
where $A$ is a complex n by n matrix, neither Hermitian nor skew-Hermitian so the value of $ {x^H}Ax $ is a complex number. 
What are the closed forms of $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial \theta }} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial \theta }} = \frac{{\partial {x^H}Ax}}{{\partial \theta }}$ and $\frac{{\partial f\left( {a,\theta } \right)}}{{\partial a}} + i\frac{{\partial g\left( {a,\theta } \right)}}{{\partial a}} = \frac{{\partial {x^H}Ax}}{{\partial a }}$ in terms of the matrix $A$ and vector $x$. Actually, I could not use the Wirtinger derivatives because I do not know how to use them in polar from.","['multivariable-calculus', 'partial-derivative', 'quadratic-forms', 'complex-numbers']"
2637758,Finding partition with equal sum,"Given a positive integer $n$. Your friend selects numbers $x_1,x_2,\ldots,x_n\ge 0$, not necessarily distinct. You are allowed to ask him the sum of any subset of numbers that you want. At the end, you should answer whether the numbers can be partitioned into two subsets $A,B$ (that is, $A\cup B$ is the set of all numbers, and $A\cap B$ is empty) such that the sum of the numbers in $A$ is the same as in $B$. What is the minimum number of questions that always suffice? If you ask for every number separately, this takes $n$ questions. Is it the best you can do?","['puzzle', 'combinatorics']"
2637801,Is there a standard name for functions whose fibers are finite on every element in their image?,"What would you call a function $f$ with the property that $\forall y\in \text{img}(f)\left(\left|f^{-1}[\{y\}]\right|\in \mathbb{N}\right)$? What about a function whose fibers all have the same cardinality? Do either of these have a standard name? I'm trying to formalize a certain kind of map between objects, that should satisfy this and I don't want to make up notation if possible. It sorta reminds me of covering maps between topological spaces, though I can't think of the right word for this.","['terminology', 'covering-spaces', 'functions']"
2637819,Set-theoretic equality of splitting fields within a fixed algebraic closure,"Let $F$ be a field and let $f(x)\in F[x]$ be a polynomial. Recall the following two facts: (1) algebraic closures are unique up to isomorphism (2) splitting fields are unique up to isomorphism Fix an algebraic closure $\overline F$ of $F$. Is it true that any two (necessarily isomorphic) splitting fields $E,E'\subseteq \overline F$ for $f$ are equal $as$ $sets$ in addition to being isomorphic? My intuition tells me that this should be the case: fixing an algebraic closure allows us to fix roots of $f$ within this particular ambient field, and since any splitting field is the smallest field containing these roots, any two such fields must actually be equal as sets, in addition to being isomorphic. Does this sound about right? Is this totally trivial?","['abstract-algebra', 'splitting-field', 'field-theory', 'elementary-set-theory']"
2637836,$B$-admissible p-adic representations definitions and implications (Berger article),"I am currently reading this article by Laurent Berger about $p-$adic Galois representations and I have some question regarding its section $4$. I will go through some of its lines and ask my questions in what follows. Let $K$ be a finite extension of $\mathbb Q_p$ and $G_K$ denote its absolute Galois group. We start with a $\mathbb Q_p$-algebra $B$, endowed with a $G_K$ action. The author also assumes that (1) $B$ is a domain such that $\text{Frac}(B)^{G_K}=B^{G_K}$ (2) If $y\in B$ is such that the line $\mathbb Q_p\cdot y$ is stable under $G_K$, then $y\in B^{\times}$. I am not exactly sure in (1) what $G_K$-action are we assuming on $\text{Frac}(B)$, but I suppose it is expected one by declaring $\sigma\left(\dfrac{a}{b}\right):=\dfrac{\sigma(a)}{\sigma(b)}$.
To me it seems that condition $(1)$ means that $B^{G_K}\subset B^{\times}$.
The other condition is clear. Next, if $V$ is $p-$adic representation of $G_K$, we let $D_B(V):=(B\otimes_{\mathbb Q_p}V)^{G_K}$ and this is a $B^{G_K}$ vector space.
Then the author continues: There is a natural map 
  $$
\alpha: B\otimes_{B^{G_K}} D_B(V)\rightarrow B\otimes_{\mathbb Q_p}V
$$ By condition (1) above the map is injective. I assume this map should be given by $\alpha(b\otimes v)=bv$. But I don't understand how does condition $(1)$ imply its injectivity. Finally, the above injectivity implies that $\dim_{B^{G_K}} D_B(V)\le \dim_{\mathbb Q_p}V$ and if equality holds, the author calls $V$ to be $B$-admissible.
He claims: $V$ is $B-$admissible iff $\alpha$ is surjective. I do understand the converse direction, but I still can't prove the opposite. Hope someone can help.","['number-theory', 'galois-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
2637837,Link between average and integrals,"Inspired by the definition of an integral: $\int^b_af(x)dx=\lim_{n\to∞} \Delta x \sum^{n}_{i=1} f(x^*_i)$. I wanted to create an equation that finds the average height for all the points on an interval ""a b"" of a function. So I came up with this equation: $$\lim_{n\to∞}\frac{1}{n}\sum^{n}_{i=1}f(a+i\Delta x)$$
Where $\Delta x=\frac{b-a}{n}$. I noticed that the ""average equation"" is very similar to the definition of an integral. In fact, it is equal to $\frac{\int^b_a f(x)dx}{b-a}$ or the integral divided by distance between a and b. I was wondering conceptually why these two ideas are related in this way.","['integration', 'average']"
2637912,Proof by Mathematical Induction for Inequality,"Prove, by mathematical induction, that $$\sum_{i=1}^n\frac{i}{i+1}\leq \frac{n^2}{n+1}$$
(When $n$ is a natural number.) So I went through all the typical steps you'd go through with mathematical induction, and after assuming that the statement at $n+1$ was true, I ended up with the following: $$\sum_{i=1}^{n+1}\frac{i}{i+1}\leq \frac{(n+1)^2}{n+2}$$
... (algebra) ...
$$\sum_{i=1}^n\frac{i}{i+1}\leq \frac{n^2+n}{n+2}$$ But since the right hand side of the inequality I ended up with is greater than the original right hand side of the inequality I wanted to end up with, this doesn't help me prove the original statement.  So I've either done something wrong with my algebra or I'm not thinking far enough outside the box; how do I use induction to prove this?","['inequality', 'induction', 'proof-writing', 'summation', 'discrete-mathematics']"
2637913,Limit at Infinity of Maclaurin Series,"Let $f(x) = \sum_{n=1}^\infty a_n x^n$. What is $\lim_{x\rightarrow \infty} f(x)$ in terms of the $a_i$? That question may be too broad, so here are some restrictions: Assume f(x) is continuous (and therefore well-defined) on all of $\mathbb{R}^+$ and the limit exists in the extended reals. For example, if I were given the sequence $$\{a_i\}_{i=0}^\infty = \{1,-1,1/2,-1/6,1/24,\cdots, (-1)^i / i!, \cdots \}$$, I would have no idea what its limit at infinity is, but as soon I knew it was $e^{-x}$, taking the limit would be easy ($\lim_{x \rightarrow \infty} e^{-x} = 0$). The same is true for $e^x$. Any help is appreciated. Thanks edit:
To make the question simpler, I would be happy with determining if the limit is finite or infinite.","['real-analysis', 'sequences-and-series', 'limits']"
2637920,Formula to find a particular solution of a specific linear ODE,"I'm currently following the book ""Differential Equations with Applications and Historical Notes"" by George F.Simmons. When he goes on to talk about particular solutions of linear ODEs and how to find them, one of the methods he proposes as a last resort is ""Variation of parameters"". One of the problems related to that method states the following: Prove that the method of variation of parameters applied to the equation $y''+y=f(x)$ leads to the particular solution:
  $$ y_p(x)=\int_{0}^x f(t)\sin(x-t)dt  \tag{1} $$ This is what I've tried to do. I start from the fact that $$ y_p(x)=y_1v_1 + y_2v_2 \tag{2}$$
Where $y_1$ and $y_2$ are solutions to the homogeneous ODE, while $v_1$ and $v_2$ are functions of $x$ to be determined by the following formulas:
$$v_1 = \int \frac{-y_2f(x)}{W(y_1,y_2)}dx \tag{3}$$
$$v_2 = \int \frac{y_1f(x)}{W(y_1,y_2)}dx \tag{4}$$ $W(y_1,y_2)$ is the wronskian. In my case I've chosen $y_1=\sin(x)$ and $y_2=\cos(x)$, so that $W(y_1,y_2)=-1$. Using (3) and (4), and substituting into (2), I get: $$ y_p(x)=\sin(x) \int \cos(x)f(x)dx\ - \cos(x)\int \sin(x)f(x) dx \ \ $$ But I can't find a way to get to (1). I've tried integration by parts. Also, I'm not familiar with the concept of convolution yet.","['integration', 'ordinary-differential-equations']"
2637960,Milnor's octahedron,"If $V$ is a vector space, then a curvaturelike tensor in $V$ is a quadrilinear map $F:V \times V \times V \times V \to \Bbb R$ satisfying 1) $F(x,y,z,w) = -F(y,x,z,w) = -F(x,y,w,z)$; 2) $F(x,y,z,w) = F(z,w,x,y)$; 3) $F(x,y,z,w) + F(y,z,x,w) + F(z,x,y,w) = 0$, for all $x,y,z,w \in V$. It turns out that the Bianchi identity (item 3) actually implies item 2, assuming 1. This implication is called "" Milnor's octahedron argument "". I'd like to know where this appeared for the first time. Thanks!","['math-history', 'riemannian-geometry', 'reference-request', 'tensors', 'linear-algebra']"
2637983,Can algebraic numbers be compared using only rational arithmetic?,"I was working on a program to carry out some computations, and ran into an issue of needing to compare some algebraic numbers, but not having enough precision to do it without exact arithmetic, and not knowing how to do it with exact arithmetic. A little algebra shows that the statement
$$a+b\sqrt{n}>0$$
is equivalent to asking that either $a^2>nb^2$ and $a>0$ or $nb^2>a^2$ and $b>0$. In particular, this means that we can easily compute the order on $\mathbb Q(\sqrt{5})$ using only rational arithmetic on the coefficients of polynomials in $\sqrt{5}$. However, it seems not so clear how to generalize this reasoning even to an example like deciding whether $a+b\sqrt[3]{n}+c\sqrt[3]{n}^2$ is positive. In general, suppose that $f$ is an irreducible polynomial in $\mathbb Q[x]$ and has some real root $\alpha$. Let $F=\mathbb Q[x]/(f)\cong \mathbb Q(\alpha)$ be the corresponding field extension. This field clearly can be ordered, as it is identified with a subfield of $\mathbb R$. Is it possible to compute an explicit order* on $F$ using only rational arithmetic? I feel that this must be possible, but can't figure out how. I'm most interested in whether, for each fixed field extension $F$, there exists an algorithm taking as input a polynomial in $\alpha$ of degree less than $\deg f$ and deciding whether it is positive or not, using a bounded number of operations. I want this primarily for field extensions of low degree, so I'm less interested in how the complexity grows as $F$ becomes more complex than in how algorithms tailored to a single $F$ fare. (*Obviously, I'm most interested in being able to compute the order on $\mathbb Q(\alpha)$ inherited from $\mathbb R$, but given that this field is isomorphic to $\mathbb Q(\alpha')$ for any other root of $f$, there are probably multiple orders - any of which would be interesting to compute)","['abstract-algebra', 'algorithms', 'field-theory', 'ordered-fields']"
2638073,How to solve this summation without taylor?,"The summation in question  is $$\sum_{n=0}^\infty  \frac{n(n+1)(n+2)}{n! + (n+1)! + (n+2)!}$$ The sum can be simplified further into $$\sum_{n=0}^\infty  \frac{n(n+1)^2}{(n+2)!}$$ With Taylor expansion allowed, I don't think it's hard to derive it from expansion of $e^x$.",['sequences-and-series']
2638080,Does Mathematica have a function for the composite numbers not divisible by a squared prime?,"I'm looking for a formula in Mathematica that gives me the sequence of integers whose prime decomposition is p1*p2*...*pn, that is, not divisible by any squared prime.
Does it even exist?","['number-theory', 'prime-numbers', 'mathematica']"
2638081,Integration by parts twice yields $0=0$?,"I had this question:
$$\int 3x\sec^2(4x) dx$$ After doing integration by parts for the first time, setting $u=3x$ and $dv=\sec^2(4x)dx$ and doing derivative and integral, I got
$$\int 3x\sec^2(4x) dx = \frac{3}{4}x\tan(4x)-\frac{3}{4}\int \tan(4x) dx$$ At this point, I realize I can solve for the $\tan(4x)$ using $u$-substitution, but I continue doing integration by parts. For
$$\int \tan(4x) dx$$
I substitute $u=\tan(4x)$ and $dv=1dx$ getting
$$\int \tan(4x) dx = \tan(4x)\cdot x - \int x\cdot 4\sec^2(4x)dx$$ Substituting everything back in, I get
$$\int 3x\sec^2(4x) dx = \frac{3}{4}x\tan(4x) - \frac{3}{4}\left(\tan(4x)\cdot x -\int x\cdot 4\sec^2(4x)dx\right)$$ Distributing the $3/4$ and simplifying, I get $0=0$ — that's not the answer. I don't believe I broke any rules using integration by parts, yet the answer is invalid. Why doesn't this work? Thanks.","['indefinite-integrals', 'integration', 'calculus']"
2638101,"How can we show that $ (a_n), (b_n), (c_n) $ are convergent and have the same limit?","We have the following for $a \le b \le c >0$: $A(a,b,c)=\frac{a+b+c}{3}, B(a,b,c)= (abc)^{1/3}, C(a,b,c)=\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}  $. Then we define the sequences $(a_n),(b_n), (c_n)$ by $a_1=a, b_1=b, c_1=c,$ $a_{n+1}=A(a_n,b_n,c_n), b_{n+1}=B(a_n,b_n,c_n), c_{n+1}=C(a_n,b_n,c_n)$. How can we show that $(a_n),(b_n), (c_n)$ are convergent and have the same limit? I understand that $A(a,b,c)\ge B(a,b,c)$ and $B(a,b,c)\ge C(a,b,c)$","['inequality', 'sequences-and-series', 'convergence-divergence', 'limits']"
2638117,Probability inequality via symmetrization,"Let $X$ and $X'$ be independent and identically distributed random variables. Define the symmetrized version of $X$ as $X^s=X-X'$. If $a \geq 0$ is such that $P(X \leq -a) \leq 1-p$ and $P(X \geq a) \leq 1-p$ then I have to show that, for every $\varepsilon>0$,
$$P(\left|X^s\right| \geq \varepsilon) \geq P(\left|X\right|>a+\varepsilon)$$ I do not understand what role does $p$ play in the whole problem. I'd appreciate if anyone can tell me how to proceed. Thank you.","['inequality', 'probability-theory', 'probability-distributions', 'symmetry', 'random-variables']"
2638121,"Homomorphisms from $(\Bbb Q,+)$ to $(\Bbb Q,+)$ and from $(\Bbb Q^*,\cdot)$ to $(\Bbb Q^*,\cdot)$","Hope this isn't a duplicate. Considering homomorphisms from $(\Bbb Q,+)$ to itself, I notice that all the homomorphisms are of the form $\phi(x) = ax$ , where $a=\phi(1)$ . So there are exactly $\aleph_0$ number of homomorphisms from $(\Bbb Q,+)$ to itself and except the trivial 0-homomorphism, all others are onto-homomorphism . Is it correct? On the other hand, considering homomorphisms from $(\Bbb Q^*,\cdot)$ to itself, I get that $\phi(1) =1$ and also get some additional restrictions like if $x \in \Bbb Q^*$ and $x$ is a square number then $\phi(x) \neq y$ , where y is either (negative) or (positive but does not admit a rational square root) . Are there some other restrictions on a general homomorphism  from $(\Bbb Q^*,\cdot)$ to itself ? The following maps,(i) $ x \mapsto x$ and (ii) $x \mapsto x^{-1}$ (since, $(\Bbb Q^*,\cdot)$ is abelian) are automorphisms of $(\Bbb Q^*,\cdot)$ . How can one find the complete list of $Aut(\Bbb Q^*,\cdot)$ ? Thanks in advance for help.","['abstract-algebra', 'group-theory', 'group-homomorphism']"
2638151,Is the style of _Scott 1967_ outdated in discussing continuum hypothesis in a probability space?,"Here is the article I am considering: Scott, D. Math. Systems Theory (1967) 1: 89 (see here ), an article named A proof of the independence of the continuum hypothesis . He shows that CH is not true in a model of set theory--something like a special probability measurable space $(\Omega,\Sigma, P)$ where $\Omega=[0,1]^I, I>\omega_1$. Of course on that time the methods such as forcing was not written the same as that of today and therefore the style of the article may be out of fashion. So here are my 3 questions Is this presentation of this article out of fashion? Some people I know thought this article was quite confusing and is not helpful. Is it really a confusing and strange article? If so, why is it strange? Is the such an article misleading and inappropriate for beginners of set theory to study? Just give me some ideas so I would be assured.","['cardinals', 'meta-math', 'set-theory', 'measure-theory']"
2638152,How to find the shaded area,How to find the shaded area crossed by semi-circle of radius 2 and quarter-circle of radius 4?,"['circles', 'quadrilateral', 'euclidean-geometry', 'geometry', 'area']"
2638190,"Find limit of $\lim_{x\to 0}\frac{\sin^{200}(x)}{x^{199}\sin(4x)}$, if it exists","I'm practising solving limits and the one I'm currently struggling with is the following: $$\ell =\lim_{x\to 0}\frac{\sin^{200}(x)}{x^{199}\sin(4x)}$$ What I've done: Since this is an obvious $0/0$ , I tried using de L'Hospital's Rule consecutively only to see both the numerator and the denominator grow so much in size that each couldn't fit in one row.
$$
\begin{align}
l
& =\lim_{x→0}{{\sin^{200}(x)}\over{x^{199}\sin(4x)}}\\
& = \lim_{x→0}{{200\sin^{199}(x)\cos(x)}\over{x^{198}\left(199\sin\left(4x\right)+4x\cos\left(4x\right)\right)}}\\
& = \lim_{x→0}{{39800\cos^2\left(x\right)\sin^{198}\left(x\right)-200\sin^{200}\left(x\right)}\over{x^{198}\left(800\cos\left(4x\right)-16x\sin\left(4x\right)\right)+198x^{197}\left(199\sin\left(4x\right)+4x\cos\left(4x\right)\right)}}
\end{align}
$$ Another solution I tried was through manipulation and the use of trigonometric identities and formulae but to no avail. I tried substituting: $\color{red}{\sin(4x)}$ with $\color{blue}{4\sin(x)\cos(x) - 8\sin(3x)\cos(x)}$ and then $\color{red}{8\sin(3x)\cos(x)}$ with $\color{blue}{4\sin(4x)+4\sin(2x)}$.
$$
\begin{align}
l
& =\lim_{x→0}{{\sin^{200}(x)}\over{x^{199}\sin(4x)}}\\
& =\lim_{x→0}{{\sin^{200}(x)}\over{x^{199}(4\sin(x)\cos(x) - 8\sin(3x)\cos(x))}}\\
& =\lim_{x→0}{{\sin^{200}(x)}\over{x^{199}(4\sin(x)\cos(x) - 4\sin(4x)+4\sin(2x))}}\\
\end{align}
$$ No matter what I try, the limit remains $0/0$. Question: Does the above limit exist? If so, what I path should I follow to work out a solution?","['derivatives', 'real-analysis', 'limits', 'trigonometry', 'calculus']"
2638220,Weak convergence and convergence of norms imply strong convergence in Hilbert space [duplicate],"This question already has an answer here : Weak convergence, together with convergence of norms, implies strong convergence in a Hilbert space. (1 answer) Closed 4 months ago . Let $(x_n)$ be a sequence in a Hilbert space $H$ which weakly converges to $x$ . If $\|x_n\| \rightarrow \|x\|$ also, show that $x_n$ converges strongly to $x$ . So, this statement seems to be true. I was wondering how to show it. I tried with that: Since $x_n \overset{w}{\rightarrow} x$ weakly, that means that $|\langle x_n - x, y\rangle| < \epsilon$ for every $y$ . Since its true for every $y$ , I can pick $y = x_n - x$ , which proves the strong convergence is implied by only the weak. 
Why is that wrong? And how to fix it?","['functional-analysis', 'weak-convergence', 'hilbert-spaces']"
2638226,$\cos(\frac{2 \pi}{n}) = 1 \ \forall n \geq 1$? Need help in finding my mistake.,"I am going to ""prove"" that $\cos\left(\frac{2\pi}{n}\right) = 1$ for all $n \geq 1.$ While this is definitely not true, I have a chain of arguments that seemingly shows exactly that. Please help me understand where the error creeped in. Here we go. We start with recalling a number of identities involving the complex exponential function. First, recall that $$e^{ix} = \cos{x} + i\sin{x}.$$
By trigonometry, it follows that $$e^{-ix} = \cos(-x) + i\sin(-x)= \cos{x} - i\sin{x}.$$
By these two identities,  we have $$\cos{x} = \frac{e^{ix} + e^{-ix}}{2}.\tag{1}$$ Next, recall Euler's identity: $$e^{i\pi} = -1.$$ Squaring both sides, we obtain $$e^{2i\pi} = 1.\tag{2}$$ And now to the meat of the ""argument"". Letting $x = \frac{2\pi}{n}$ in (1), we get \begin{align} \large \cos\left(\frac{2\pi}{n}\right) &= \large \frac{e^{i\frac{2\pi}{n}} + e^{-i\frac{2\pi}{n}}}{2} \\ &= \large \frac{(e^{2 \pi i})^\frac1n + (e^{2 \pi i})^{-\frac1n}}{2}  \\ & = \large \frac{1^\frac1n + 1^{-\frac1n}}{2} = \large \frac{1 + 1}{2} = 1.
\end{align} My guess is that I messed up the factoring of the exponents in step 2 of the above, but it's not clear to me why what I did is not allowed.","['fake-proofs', 'trigonometry', 'complex-numbers', 'pi']"
2638228,A high school Olympiad problem,"Let $p(n)$ denote the largest prime factor of $n$. Prove that there are infinitely many $n$ such that $$p(n)<p(n+1)<p(n+2).$$ Edit: My solution: Choose $n=\prod_{i=1}^{k}p_i$, product of the first $k$ primes. This means that $n+1=\prod_{i=1}^{k}p_i+1$ is either a larger prime or has a prime factor larger than that of all factors of $n$. Still not sure about $n+2$.","['number-theory', 'elementary-number-theory']"
2638295,power logsine integral $\int_0^\frac{\pi}{2}\ln^n{(\sin{x})}dx$,"It is not hard to find that $\int_0^\frac{\pi}{2}\ln{(\sin{x})}dx=-\frac{\pi}{2}\ln2$. https://socratic.org/questions/how-do-you-prove-that-the-integral-of-ln-sin-x-on-the-interval-0-pi-2-is-converg Moreover wolfram can compute that 
 $$\int_0^\frac{\pi}{2}\ln^2{(\sin{x})}dx=\frac{\pi^3+3\pi\ln^24}{24}$$
 $$\int_0^\frac{\pi}{2}\ln^3{(\sin{x})}dx=-\frac{\pi}{16}(12\zeta(3)+\ln^34+\pi^2\ln4)$$
and  $$\int_0^\frac{\pi}{2}\ln^4{(\sin{x})}dx=\pi\zeta(3)\ln{8}+\frac{19\pi^5}{480}+\frac{\pi}{2}\ln^42+\frac{\pi^3}{4}\ln^22$$ How to prove these formulas?
In general, How to evaluate the integral $\displaystyle\int_0^\frac{\pi}{2}\ln^n{(\sin{x})}dx$ ? Thanks in advance.","['integration', 'definite-integrals']"
2638302,"Find all pairs $(n,k)$","The question is as it was officially written: Find all pairs of positive integers $(n,k)$ such that $n! + 8 = 2^k$. I started by writing a list of $n!$ and $2^k$ and found that two valid solutions were $(4,5)$ and $(5,7)$. To continue, I did \begin{align}
n! & = 2^k-8 \\
& = 2^k-2^3 \\
n\times (n-1) \times \cdots \times 2 \times1& = 2^3(2^k-1) \\
\end{align} I noticed that every number in $n!$ divides $2^3$ and $2^k-1$. Then I set out to find some solutions. My assumption - which is a big one - is that every number in $n!$ could be made by the prime numbers already in $n!$. For example, $3$ and $7$ might divide $2^k-1$ so hence $21$ divides $2^k-1$. If you want to answer the question, stop reading . Below is how I finished the question, and I know I got it wrong. $\underline {\text{Proof}}$: We have $$n!=2^3(2^k-1)$$ And we want to prove that any prime $p$ divides $2^3(2^k-1) \implies p|2^k-1$ . Hence assume \begin{align}
2^k-1 & \equiv 0 \pmod p \\
2^k & \equiv 1 \pmod p \\
\end{align} By Fermat's Little Theorem we know that $k=p-1$. Since there are infinitely many primes and there can be infinitely many primes in $n!$, the solution set $(n,k)$ is countably infinite. I'm pretty sure this is wrong but I would like to know the correct approach to a multi-variable question like this.","['algebra-precalculus', 'contest-math', 'number-theory']"
2638310,Connect any three points using two parabolas,"Let us say we have 3 points $(x_1,y_1),(x_2,y_2),(x_3,y_3)$. We want to build exactly 2 parabolas connecting theese points in a smooth way. For the first parabola we can write system of two equations using known points
$$y_1 = a_1 x_1^2 + b_1 x_1 + c_1$$
$$y_2 = a_1 x_2^2 + b_1 x_2 + c_1$$
Similarly for the second parabola we can do the same:
$$y_2 = a_2 x_2^2 + b_2 x_2 + c_2$$
$$y_3 = a_2 x_3^2 + b_2 x_3 + c_2$$
But now we have 4 equations and 6 unknowns $( a_i,b_i,c_i)$, where $i=1,2$.
We can write down an equation that meets the condition of the ""smooth connection"", i.e. derivative at the point $(x_2,y_2)$ is the same for parabola 1 and parabola 2:
$$y_2^{'} = 2a_1 x_2 + b_1$$
$$y_2^{'} = 2a_2 x_2 + b_2$$
which leads us to the fifth equation: $2a_1 x_2 + b_1=2a_2 x_2 + b_2$ I can not find another one assumption that will make a number of unknowns even to a number of equations. See an example of how it may look like.",['geometry']
2638315,What is the value of $f(100)$?,"We have $f:\Bbb R\to \Bbb R^*$ , a function that admits primitives and admits the relations $$\cos \left(f(x)\right)=1,\ ∀x\in \Bbb R, \quad\text{and}\quad|f(\pi )−\pi |≤\pi .$$ What  is the value of $f(100)$ ? My thought. We obviously have $$\cos (f(100)) =1\overset{?}{\implies} f(100) =\arccos (1),$$ but this seems not to make any sense at all. How can I use the provided inequality $|f(\pi)−\pi|≤\pi$ ?","['real-analysis', 'functional-equations', 'calculus', 'functions', 'contest-math']"
2638371,"If $A$ is a set with $m$ elements and $C \subseteq A$ is a set with $1$ element, then prove that $A \setminus C$ is a set with $m-1$ elements","Definition: $N_m$ = $\{1,2,\ldots,m\}$ My Attempt: Since $A$ is a set with $m \in \mathbb{N}$ elements, there exists a bijection $f:N_m \to A$. We know that $C \subseteq A$ contains just $1$ element, say $a$ ( Note that $a \in A$ as well ). Let $k \in N_m$ be the element that maps to $a$. We know $k$ is unique because $f$ is also an injection. We define:
$$h(q)=
\begin{cases}
f(q),&q=1,2,\ldots,k-1 \\[2ex]
f(q+1),&q=k,k+1,\ldots,m-1
\end{cases}$$ It is evident that $h:N_{m-1} \to A\setminus C$. All we have to do now is to prove that $h$ is a bijection . Injectivity: There are only two possible scenarios for any member of the domain. Either it belongs to $N_{k-1}$ or it belongs to $N_{m-1} \setminus N_{k-1}$. For any $x_1,x_2 \in N_{k-1}$ such that $x_1 \neq x_2$, $h(x_1),h(x_2) \in f(N_{k-1})$. Since $f$ is injective over $N_{k-1}$, $h(x_1) \neq h(x_2)$. For any $x_1,x_2 \in N_{m-1} \setminus N_{k-1}$ such that $x_1 \neq x_2$, $h(x_1),h(x_2) \in f(N_{m-1} \setminus N_{k-1})$. Since $f$ is injective over $N_{m-1} \setminus N_{k-1}$, $h(x_1) \neq h(x_2)$. Surjectivity: This is the part I cannot figure out. I would like if someone could point me in the right direction.",['elementary-set-theory']
2638472,Let $f(x)=\big(|x+a|-|x+2|+b|x+5| \big)$ be an odd function then find $a+b$,Let $f(x)=\big(|x+a|-|x+2|+b|x+5| \big)$ be an odd function then find $a+b$. My try : $$\begin{align}f(-x)&=|-x+a|-|-x+2|+b|-x+5| \\ &=-f(x) \\&= -\big(|x+a|-|x+2|+b|x+5| \big)  \end{align}$$ $$|-x+a|-|-x+2|+b|-x+5|+ \big(|x+a|-|x+2|+b|x+5|) =0$$ Now what do I do?,"['absolute-value', 'calculus', 'functions']"
2638504,How to calculate the limit $\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k}\sin(\frac{2k\pi }{n})$? Is it a Riemann sum?,"I just came across this limit and I suppose it can be computed using a Riemann sum but I can't get it right.
$$\lim _{n\to \infty }\sum _{k=1}^n\sqrt{n^4+k}\sin\left(\frac{2k\pi }{n}\right)$$
Any ideas?","['riemann-sum', 'sequences-and-series', 'limits']"
2638536,Position operator,"I'm learning about unbounded operator densely defined on Hilbert spaces from ""Reed & Simon, Functional Analysis"". The first example was the position operator , which is defined as follows. Let $D(T) = \{ \varphi \in L^2(\mathbb R) : \int_\mathbb R x^2|\varphi(x)|^2 {\rm d}x < \infty \}$ and $T\colon D(T)\to L^2(\mathbb R)$ be given by $T\varphi(x) = x\varphi(x)$. Well, the domain $D(T)$ of $T$ that was chosen is the largest one for which the range of $T$ is in $L^2(\mathbb R)$. I am wondering: Why such operator is called position operator? What is its meaning for physicists? What is the intuition in defining such operator like this? Thanks in advance!","['intuition', 'reference-request', 'physics', 'mathematical-physics', 'functional-analysis']"
2638638,"Dedekind domain with finite prime ideals is PID (a corollary on Serre's ""Local fields"" book).","In the aforementioned book in p. 12 right under the approximation lemma there is a corollary stating that: ""A Dedeking domain $A$ with only finitely many prime ideals is principal"". I can find different proofs elsewhere but I am really interested in the way that it can be proved with what Serre has already proved. The proof goes like this: ""It is enough to show that every prime ideal is principal. Let $p$ be one, there exists an $x$ in $A$ with $v_p(x)=1$ and $v_q(x)=0$ for $q\neq p$. Hence $xA=p$."" Obviously the problem is how can such an $x$ be found, since the approx. lemma only gives inequalities. The statement of the approximation lemma can be found on a similar question, here . I would like to mention that up to now, Serre hasn't mentioned the so-called ""Strict triangle inequality"", neither the fact that in the approximation lemma the inequalities can be taken to be equalities.","['local-field', 'abstract-algebra', 'algebraic-number-theory', 'commutative-algebra']"
2638663,Can anyone suggest well written books for learning basic statistics/ probability?,"I'm in final year high-school, (which is grade 13 where I live), and, as the title suggests, I'm seeking a book which introduces statistic/ probability with little prerequisites other than mastery of basic arithmetic and a good command and understanding of basic Algebra. I will be self studying this, so ideally, the book should elucidate rather well written explanations for basic statistical notions.","['self-learning', 'statistics', 'probability']"
2638666,If $A$ is $2\times2 $ matrix such that $\operatorname{tr} A =\det A=3$ then trace of $A^{-1}=$,"If $A$ is $2\times2 $ matrix such that $\operatorname{tr} A=\det A=3$ then trace of $A^{-1}$ is? $(A) \quad 1   \qquad
 (B) \quad \dfrac{1}{3} \qquad
 (C) \quad \dfrac{1}{6} \qquad
 (D) \quad\dfrac{1}{2}$ I did it in this way: $$\lambda_1+\lambda_2 = 3 $$
$$\lambda_1\cdot\lambda_2=3$$
$$\frac{1}{\lambda_1}+\frac{1}{\lambda_2} \implies
\frac{\lambda_1+\lambda_2}{\lambda_1\lambda_2} \implies
\frac{3}{3}=1$$ I am practicing these types of question and after $5$ minutes of digging I came up with this answer. I wanna know if there is any alternative approach to solve this problem.",['linear-algebra']
2638687,Second Order ODE $y'' - 2y' + y = e^{2x}$ by Method of Variation of Parameters,"I'm having some trouble with solving this second-order ODE by the method of variation of parameters. The problem is: $y'' - 2y' + y = e^{2x}$. Here's what I've done thus far. The homogeneous component has an auxiliary equation of $r^2 - 2 r + 1$, which has a single root of $r = 1$, so our complimentary equation is $y_c = c_1 e^{x}+ c_2 xe^{x}$. So, we take $y_1 = y_2 = e^{x}$, and guess that our particular solution takes the form $y_p = u_1(x) y_1 + u_2 (x) y_2$. Differentiating in $x$ gives us 
\begin{align*}
y'_p = \frac{d}{dx}[u_1 e^x+u_2 xe^x] = (u_1' e^x + u_2' xe^x) + u_1e^x+u_2e^x+u_2xe^x
\end{align*}
Then, we can assume that $u_1' e^x + u_2' xe^x = 0$, so $y_p' = u_1e^x+u_2e^x+u_2xe^x$. Differentiating in $x$ once more and using our earlier assumption gives us
\begin{align*}
y''_p = (u_1' e^x + u_2'xe^x)+u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x
\end{align*}
Substituting back into our differential equation gives us (pre-simplification)
\begin{align*}
y'' - 2y' + y = u_1 e^x+2u_2e^x+u'_2e^x+u_2xe^x - 2(u_1e^x+u_2e^x+u_2xe^x) + u_1 e^x + u_2 xe^x = e^{2x}
\end{align*}
After simplifying, I get $2u'_2 e^x = e^{2x}$, so $2u'_2 = e^x$, $u'_2 = \frac{1}{2} e^x$, and $u_2 = \frac{1}{2} e^x$. Since $u_1'e^x + u_2'xe^x = 0$ by assumption, and $u'_2 = \frac{1}{2}e^x$, we have $u_1'+ \frac{1}{2}e^x = 0$, and $u'_1 = - \frac{1}{2}e^x$, so we get $$y_p = \Big(-\frac{1}{2}e^x\Big)e^x + \Big(\frac{1}{2}e^x\Big)xe^x = - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ Therefore, our general solution is $$y = c_1 e^{x}+ c_2 xe^{x} + - \frac{1}{2} e^{2x} + \frac{1}{2} xe^{2x}$$ This answer, however, is not correct. The right answer should be $y = e^x(c_1 + c_2x) + e^{2x}$, but I can't figure out where I went wrong. Some help on this would be greatly appreciated.",['ordinary-differential-equations']
2638697,Localization of finite direct product of ring at an element,"Given a finite direct product of rings $A_1\times A_2\times\cdots\times A_n$, what is the localization of the product ring at an element $f:=(f_1,f_2,...,f_n)$. 
There is a similar question Localization of a direct product , but the subtlety is the multiplicative set is not $S_{f_1}\times S_{f_2}\times \cdots\times S_{f_n}$. This problem comes form my try to prove the isomorphism of structure sheaves by checking the isomophism on distinguished opens.
$$
\mathcal{O}_{\operatorname{Spec} \prod_i A_i}(D(f))\cong \mathcal{O}_{\coprod_i \operatorname{Spec} A_i} \left( \coprod_i D(f_i) \right)
$$ These two sheaves should be isomorphic and implies
$$
\left(\prod_i^n A_i\right)_f \cong \coprod_i (A_i)_{f_i}.
$$
Is this true or did I make some mistakes in this derivation?","['localization', 'algebraic-geometry', 'commutative-algebra']"
2638737,Generalizing Odom's construction of the golden ratio,"The artist and amateur mathematician George Odom found this nice construction for the golden ratio $\phi$ using an equilateral triangle and its circumcircle , $\hskip2.3in$ $\hskip3.3in$ Fig. 1 Let $A$ and $B$ be the midpoints. The ratio of the line segments $|AB|$ and $|BC|$ is, 
$$R_3 = \frac{|AB|}{|BC|} =\phi$$ We can easily generalize the above figure using a square , $\hskip2.3in$ $\hskip3.3in$ Fig. 2 Let the (sadly invisible) $A,B,C$ of Fig.2 be analogous to that of Fig.1. What is then its, $$R_4=\frac{|AB|}{|BC|} =\,?$$ Q: In general, is $R_n$ for $n>3$ algebraic? If it is, does its minimal polynomial have a closed-form?","['golden-ratio', 'polygons', 'geometry']"
2638783,Players tournament,"In the final phase of a television game show, the $N$ finalists competed exactly once with each other, receiving $10$ points for each win, $5$ points for each draw, and no points for losses. At the final ranking, the total number of points won by each of the players in positions $1$ to $N-10,$ were received as follows: $50\%$ from games played against the players in positions $N-9$ to $N$ and the rest of them in games with all others. Similarly, $50\%$ of points won by each of the players in positions $N-9$ to $N,$ were received from games against the players that were ranked in positions $1$ to $N-10.$ Find the number of players. My humble attempt: All games played were $N(N-1)/2.$ Assuming there were $W$ wins, there were equally $L$ losses and $W=L,$ so there were $N(N-1)/2-2W$ draws. The total points won by all players were $10W+5(N(N-1)/2-2W) = 5(N(N-1)/2).$ Total points won by each of the players in positions $1$ to $N-10:$ Each player played $N-1$ games, of which $10$ were with the players in the last $10$ positions (clearly different players from them). So he gained $50\%$ of his points from $N-1-10$ games and $50\%$ of his points from $10$ games... and this is where I am stuck! Any help is highly appreciated!",['combinatorics']
2638840,differential equation contains definite integral,"I am stuck on solving the following differential equation which contains a definite integral that I don't know how to deal with: $$ f^{\prime\prime} + a^2 f - b\int_0^L f(t) \, dt = c$$ The boundary condition is $f(0)=0$ and $f(L)=R$. Anyone help me out of here? Thank you in advance.","['definite-integrals', 'integro-differential-equations', 'ordinary-differential-equations']"
2638850,Short inequality involving determinants,"Let $A, B \in M_n(\mathbb{C})$ such that $$|\det(A+zB)|\leq1$$ for any $z\in\mathbb{C}$  with $|z|=1$. Prove that $$|\det(A)|\le1$$ I assumed that $|\det A|\ge1$. Then there exists $A^{-1}$ and $|\det(A^{-1})|\le1$. I multiplied the relation and obtained $$|\det(I_n+zBA^{-1})|\le|\det(A^{-1})|\le1$$ so $|\det(I_n+zC)|\le1$ for any $z$ with $|z|=1$, where $C=BA^{-1}$. I don't think that this is true but I don't know how to prove it.","['matrices', 'linear-algebra', 'complex-numbers', 'determinant']"
2638856,Omar Khayyam and the tribonacci constant,"While trying to find the tribonacci cousin of this post , I came across this nice short article A Geometric Problem of Omar Khayyam and its Cubic by Wolfdieter Lang. Given the figure, $\hskip1.7in$ The problem is to find the point $P$ on a circle with radius $R$ such that, $$\frac{x}{R} = \frac{R-h}{h}$$ By the Pythagorean theorem, we have $R^2 =h^2+x^2$. Eliminating $R$, we then get, $$x^3-2hx^2+2h^2x-2h^3=0$$ If $h=10$, then $x^3+200x=20x^2+2000$, the cubic mentioned in Khayyam's MacTutor biography and this MSE post . However, if $h=1$, then, $$x^3-2x^2+2x-2=0$$ which, if this Wikipedia section is to be believed, was also solved by Khayyam using a circle and a hyperbola, $\hskip2.0in$ Remarkably, this real root is $\displaystyle x= \frac1T+1$ with tribonacci constant $T$. So Khayyam was playing with one of the $n$-nacci ratios before Fibonacci. Q: This is the fourth geometric context I've seen where the tribonacci constant plays a role, after the snub cube , the hard hexagon model , and this short note . Are there other geometric contexts that the tribonacci constant appears in?","['cubics', 'math-history', 'constants', 'geometry']"
2638899,Logical equivalences/proof,"So I am working on logical equivalences for the first time and it was all making sense, until I was given the exercise: Verify the following equivalence by writing an equivalence proof. That
  is, start on one side and use known equivalences to get to the other side.
  $(p \to q) \land (p \lor q) ≡ q$. I am aware of the various laws, however in what order do I apply these laws? Is there an order? If there is not an order then what are the steps to take in order to prove the equivalence?","['propositional-calculus', 'logic', 'discrete-mathematics']"
2638962,What type of differential equation is it and how can it be solved?,"The equation is:
$$\frac{dA(t)}{dt}=aA(t)\cdot k^{2}(t)+b\cdot k(t)$$
If not the last part, I would use the separation of variables, but how to solve it in this form?",['ordinary-differential-equations']
2639023,Monodromy representation.,"In a scientific article on the net, i find without further details, the following paragraph : Let $ X \subset \mathbb{C} $ be un open connected subset, and $ x \in X $. A linear differential equation of order $ n $ : $$ y^{(n)} + a_1 y^{(n-1)} + \dots + a_{n-1} y' + a_n = 0 $$
 define a monodromy representation : $ \rho \ : \ \pi_1 (X,x) \to \mathrm{GL}_{n} ( \mathbb{C} ) $. By the Cauchy existence theorem, the local solutions in a neighbourhood of $ x $ form a $ \mathbb{C} $ - vector space of dimension n endowed with monodromy action of $ \pi_1 (X,x) $. My questions are to know : How is the monodromy representation $ \rho \ : \ \pi_1 (X,x) \to \mathrm{GL}_{n} ( \mathbb{C} ) $ defined, and, what is the intuition and the use of $ \rho $ compared to the linear differential equation : $ y^{(n)} + a_1 y^{(n-1)} + \dots + a_{n-1} y' + a_n = 0 $ ? N.B. : The paragraph above is from the following pdf : http://www.galois.ihp.fr/wp-content/uploads/2011/12/T.-Szamuely.pdf , page : $ 48 $. Thanks in advance for your help.","['algebraic-topology', 'galois-theory', 'ordinary-differential-equations', 'fundamental-groups']"
2639037,Let $T:\mathbb{R}_n\rightarrow\mathbb{R}_n$ the linear operator defined by $T(p(x))=p(x)+p'(x)$ Calculate $det(T)$,"Let $T:\mathbb{R}_n\rightarrow\mathbb{R}_n$ the linear operator defined by $T(p(x))=p(x)+p'(x)$ Calculate $det(T)$ My work: Let $B=\{1,x,x^2,...,x^n\}$ a basis for $\mathbb{R}_n$. Then, $T(1)=1$ $T(x)=x+1$ $T(x^2)=x^2+2x$ . . . $T(x^n)=x^n+nx^{n-1}$ This implies, $[T_{BB}]=\begin{pmatrix}
1 && 1 && 0 && 0 && ... && 0\\
0 && 1 && 2 && 0 && ... && 0\\
0 && 0 && 1 && 3 && ... && 0\\
.\\
.\\
.\\
0 && 0 && 0 && 0 && ... && n\\
0 && 0 && 0 && 0 && ... &&1\\
\end{pmatrix}$ As $det(T)=det([T_B])$ and $det([T_B])=det\begin{pmatrix}
1 && 1 && 0 && 0 && ... && 0\\
0 && 1 && 2 && 0 && ... && 0\\
0 && 0 && 1 && 3 && ... && 0\\
.\\
.\\
.\\
0 && 0 && 0 && 0 && ... && n\\
0 && 0 && 0 && 0 && ... &&1\\
\end{pmatrix}=1$ then $det(T)=1$. I don't sure of my result. I think i have a mistake. Can someone help me?","['linear-algebra', 'determinant']"
2639057,Differential equation: find fundamental set of solutions,"We have $y' = Ay$ with $A = \begin{pmatrix}
2 & 1 \\
0 & 1
\end{pmatrix}
$ First I found out the eigenvalues which are $e_1 =2$ and $e_2 = 1$. Both of them are $α = 1$ which means they both have the algebraic multiplicity $1$. Then I found out the geometric multiplicity. For both eigenvalues it is $γ =1$. Now I got troubles to set up the fundamental set of solution. I tried to use the following rule: Every funtion(=column of the fundamental matrix) of the fundametal set of solutions is a linear combination of the following functions: 
$x^ve^{e_jx}$ with $1 \leq j \le 2$, $0 \le v \le α_j-γ_j$ The theorem works when I have only one eigenvalue but this time I have two different eigenvalues and it is not working. I dont see where my mistake is. I get: $$φ = \begin{pmatrix}
a_1e^{2x} \\
a_2e^x
\end{pmatrix}
$$ afterwards I would usually do a compare of coefficients of $Aφ$ and $φ'$ to find $a_1$ and $a_2$ but the  $φ$ seems to be wrong. Alternatively I could solve this differential equation with ""Jordan-Matrix way"" but I want to know where my mistake is.","['real-analysis', 'ordinary-differential-equations']"
2639062,$B^n/S^{n-1}$ is homeomorphic to $S^n$,"Let $B^n$ be a n-dimensional disc (ball) with boundary $S^{n-1}$. Prove that  $B^n/S^{n-1}$ is homeomorphic to $S^n$. Could someone check my proving, please? Let  put the ball $B^n=\{x\in\mathbb R^n\mid |x|\leq 1\}$ into $\mathbb R^{n+1}$ using $x\mapsto (x,0)$. Then we can use homeomorphism $(x,0)\mapsto (x,\sqrt{1-|x|})$. We fall into the half of the sphere $S^n$. The boundary of the ball after this will turn just into points of the form $(*,0)$. If you pull it to the point then we will get suspension $S^{n-1}$ or $S^n$.","['general-topology', 'proof-verification']"
2639071,The system of exponential equations,"How do you solve:
$$\begin{cases}
x\cdot2^{x-y}+3y\cdot2^{2x+y-1}=1 \hspace{0.1cm},\\ x\cdot2^{2x+y+1}+3y\cdot8^{x+y}=1\\
\end{cases}$$
I subtracted the equations, factorized by grouping, and got two terms equal zero. When I tried to use one of the terms, by expressing one variable through another and put it back in the first equation, it became complicated, so I gave up. Result is: $ (x=1, \hspace{0.1cm}y=-1) $","['algebra-precalculus', 'exponential-function', 'systems-of-equations']"
2639125,Beginner question about representations and Jordan forms,"I have just started studying representation theory of finite groups in finite dimensional vector spaces. I have question about writing a representation as direct sum of subrepresentations. Now assume we have a representation $$G\rightarrow GL(V)$$$$p:g\rightarrow p_g$$ and let $V=W_1\oplus W_2...\oplus W_r$ and $W_i$ is $p_g$ invariant for all $i,g$ and $W_i=span\{v_{i1},v_{i2}...,v_{id_i}\}$ ($dim(W_i)=d_i$). If we set this base, $B=\{v_{11},...,v_{1d_1},...,v_{i1}...v_{id_i},...,v_{r1}...,v_{rdr}\}$ for $V$ then the matrix form of $p_g$ becomes a block diagonal matrix, say $A_g=diag\{A_1,A_2,...,A_r\}$ and each $A_i$ is subrepresentation, $A_g=A_1\oplus A_2\oplus ...\oplus A_n$ So my  question is can i do the reverse? I mean if i have a representation $A_g$ -i know i can write it's Jordan form wihch is block diagonal and they are isomorphic representations since they are similar- can i always find or construct such bases as in example to make each subspace $A_g$ invariant? For example let $A_g=diag\{A_1,...A_r\}$ and $A_1$ is $d_1\times d_1$ matrix, pick $W_1=span\{(1_F,0,...,0),(0,1_F,0,...,0),...(0,0,...,1_F\text{($d_1-th$ entry)},0,...,0)\}$ would not this representation be $W_i$ invariant?
 If so this should mean every finite dimensional representation which is not in form of a jordan block can be written as direct some of subrepresentations where those are the jordan blocks, right? Thanks.","['finite-groups', 'group-theory', 'representation-theory', 'linear-algebra']"
2639132,Finitely presented groups with no proper subgroups of finite index,Is there an infinite finitely presented group with NO nontrivial normal subgroup of finite index?,['group-theory']
2639166,What is the largest possible number of moves that can be taken to color the whole grid?,"Consider a $10\times 10$ grid. On every move, we color $4$ unit squares that lie in the intersection of some two rows and two columns. A move is allowed if at least one of the $4$ squares is previously uncolored. What is the largest possible number of moves that can be taken to color the whole grid? Attempt: It is easy to get $81$ moves: By always choosing the first line, the first
column and a square of the remaining $9\times 9$ grid as the lower
right square, the whole grid can be colored in $81$ moves. But is this a maximum number of moves?","['combinatorics', 'algorithms', 'linear-algebra', 'contest-math']"
2639192,Let $A$ be a finite set such that $|A|=n+1$ for some $2\le n$ and let $R\subseteq A\times A$ be some reflexive relation in $A$. Prove the following:,"Let $A$ be a finite set such that $|A|=n+1$ for some $2\le n$ and let $R\subseteq A\times A$ be some reflexive relation in $A$ . We'll denote $R^{(k)}=R\circ R\circ .... \circ R$ ( $k\in\mathbb{N^+}$ times) Prove $R^{(n)}$ is a transitive relation. Hey everyone. I had already proven that for all $1\le k\le m \Rightarrow R^{(k)}\subseteq R^{(m)}$ and proved that if there exists some $1\le k$ such that $R^{(k)}=R^{(k+1)}$ then for all $k \le m \Rightarrow R^{(m)}=R^{(k)}$ by induction. Now, a relation $S$ is transitive iff $S\circ S \subseteq S$ . So I need to show that $R^{(2n)}\subseteq R^{(n)}$ . According to the first claim I've proven, $R^{(n)}\subseteq R^{(2n)}$ so basically I need to prove that $R^{(2n)}=R^{(n)}$ . According to the second claim I had proven, it is enough to show that $R^{(n)}=R^{(n+1)}$ and that would indicate $R^{(2n)}=R^{(n)}$ as desired.
How can I show that $R^{(n)}=R^{(n+1)}$ ? Thanks in advance.","['relations', 'elementary-set-theory', 'discrete-mathematics']"
2639219,Why denote the spectral decomposition of a bounded operator as an integral?,"In the spectral theorem for bounded self-adjoint operators we get, for each self-adjoint (bounded) operator $A$ in a Hilbert space $\mathcal{H}$, and each bounded Borel-measurable function $f \in \mathcal{B}_0(\sigma(A))$ (where I denote by $\mathcal{B}_0(\sigma (A))$ the set of all Borel-measurable functions in the spectrum $\sigma(A)$ of $A$) a bounded operator $f(A)$ defined as $$\langle \psi, f(A) \psi \rangle = \int_{\sigma(A)} f(\lambda) d\mu_{\psi}(\lambda) \ ,$$ where $\mu_{\psi}$ is a measure defined by a family of ""projection-valued measures"" $(P_B)_{B \in \mathcal{B}(\sigma(A))}$ which are just $\chi_B(A)$ in the sense of the functional calculus, for each Borel subset $B$ of $\sigma(A)$. 
Given all this information, the operator $A$ is denoted as $$ A = \int_{\sigma(A)} \lambda dP_\lambda \ ,$$ and this decomposition is unique in the sense that any other projection-valued measure satisfying this equality should be equal to the first. 
My questions are: Is there some other reason for this notation for the operator $A$? How this integral should be interpreted, as it isn't actually constructed as a Lebesgue integral? The only way I know to obtain the operator in this case is to use the somewhat more general expression above, and use the polarization identity + Riesz lemma, as usual. Sorry for the long question, my doubt is quite simple but there was a lot of context before I could express it.","['functional-analysis', 'spectral-theory', 'operator-theory']"
2639224,Maximum likelihood estimate of gaussian given rounded values,"Suppose there is a hidden gaussian with mean $\mu$ and variance $\sigma^2$, and that $X_i \sim \mathcal{N}(\mu,\sigma^2)$ where the $X_i$ are i.i.d.  If I can only oberve the rounded value of $X_i$, i.e. $Y_i = \lfloor X_i + 1/2 \rfloor$, is there an effective means of computing the maximum likelihood estimates of $\mu$ and $\sigma$?","['maximum-likelihood', 'statistics', 'parameter-estimation']"
2639243,What length would the sides of a triangle over Earth's surface be for the sum of its angles to be 180.1°,"For simplicity's sake, let the Earth be a perfect sphere. Imagine you are drawing an equilateral triangle over its surface. How long should its sides be, for the sum of its angles to be 180.1 degrees?","['spheres', 'spherical-geometry', 'triangles', 'geometry']"
2639292,Totally ramified extensions of $\mathbb{Q}_p$,"There's a theorem that gives you all unramified extensions of $\mathbb{Q}_p$: they correspond to the finite extensions of the residue field $\mathbb{F}_p$. Is there a similar result for totally ramified extensions? I have a more specific question - I'm trying to find all degree three extensions of $\mathbb{Q}_2$. The ramification index is either 1 (in which case the extension is unramified), or 3 (in which case it's totally ramified). I know exactly how to construct a unramified extension of degree 3 - it's $\mathbb{Q}(\zeta_{2^3-1}) = \mathbb{Q}(\zeta_7)$. But I'm not sure what to do with the totally ramified ones. I know that $\mathbb{Q}_{\zeta_{2^m}}$ are totally ramified extensions of $\mathbb{Q}_2$, but none of such field has degree 3 because $[\mathbb{Q}_{\zeta_{2^m}}:\mathbb{Q}_2]=2^{m-1}$. I know that $\mathbb{Q}_2$ adjoint a root of an Eisenstein polynomial would be totally ramified, so I can adjoint a root of $X^3-2$,  which gives me a totally ramified degree 3 extension. Is that all of it? Edit: I also know that all totally ramified extensions are obtained by adjoinint a root of an Eisenstein polynomial, but that still doesn't tell me how to find all of them.","['number-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
2639304,Looking for a function...,"I'm looking for a monotonic increasing function $f(x)$ defined on $[0,\infty)$ so that $f(0)=0$ and with $f'(x)$ approaching $0$ as $x$ approaches $0$, and which approaches 1 as $x \rightarrow \infty$. The function might look roughly like the figure below, but that particular function is just a shifted and scaled $s$-function.  In particular, $f(0)\neq 0$, nor does $f'(x)$ approach $0$ at the origin, which are two properties I'd like to have. Any ideas?","['derivatives', 'calculus', 'limits']"
2639312,Addition of $2$ Events,"Let $X$ and $Y$ be independent, each uniformly distributed on $\{1, 2, ..., n\}$. Find $P(X + Y = k)$ for $2 \le k \le 2n$. \begin{align}P(X + Y = k) &= \sum_{(x,y)\,:\,x+y=k} P(x, y) \\
&= \sum_{(x,y)\,:\,x+y=k} \frac{1}{n^2} \\
&= (k - 1)\frac{1}{n^2}  \\
&= \frac{k-1}{n^2} \end{align} When $k = 2: (1, 1)$ When $k = 3: (1, 2), (2, 1)$ When $k = 4: (1, 3), (3, 1), (2, 2)$ When $k = 5: (1, 4), (4, 1), (2, 3), (3, 2)$ $$\#(x, y) = k - 1$$ Textbook Answer: $\frac{k-1}{n^2}\,\,\,$ for $\,\,\,2 \le k \le n+1$ $\frac{2n-k+1}{n^2}\,\,\,$ for $\,\,\,n+2 \le k \le 2n$ Why are there $2$ intervals being considered?","['uniform-distribution', 'statistics', 'probability']"
2639333,Find a recurrence relation for the number of strings in ternary that do not contain any repeated character.,"Find a recurrence relation for the number of strings in $\left \{ 0,1,2\right \}^{n}$ that do not contain any repeated character. (So, no 00, 11, or 22.) I found a similar question here , but this seems a opposite question as mine. what's the beginning step to find out the recurrence relation, they are too many possible answers, I don't know how to get start.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
