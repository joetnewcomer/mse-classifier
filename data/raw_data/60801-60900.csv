question_id,title,body,tags
681175,Radon-Nikod√Ωm (write the density as a limit),"Let $\mu$ be a probability measure and $\nu$ a $\sigma$-finite measure on $(\mathbb{R},\mathcal{B})$ with $\nu\ll\mu$. Show that it is $\mu$-a.s. $$
\lim_{h\to 0}\frac{\nu [x-h,x+h)}{\mu [x-h,x+h)}=\frac{d\nu}{d\mu}(x).
$$ I do not have many ideas... Set $f:=\frac{d\nu}{d\mu}$. Then $\nu [x-h,x+h)=\int_{[x-h,x+h)}f\, d\mu$ and $$
\lim_{h\to 0}\frac{\nu [x-h,x+h)}{\mu [x-h,x+h)}=\lim_{h\to 0}\frac{\int_{[x-h,x+h)}f\, d\mu}{\mu [x-h,x+h)}.
$$ But that does not really help. Can you help me, please? Miro","['measure-theory', 'functional-analysis']"
681183,Prove that a series is $O(t^a)$.,"Consider the series
$$ u(t,x) = \sum_{i \geq 1} {u_i(x) t_1^i } + \sum_{i+2j \geq k+2, j\geq 1} {\varphi_{i,j,k}(x) t_1^i t_2^j y^k} $$
where $t \in \tilde{\mathbb{C} \setminus \{ 0 \}}$, $x$ is complex $n$-dimensional, and 
$$t_1 = t, t_2 = t^{\rho(x)}, y = \log t.$$
$\rho (x) $ is some given function, $\log$ is the natural logarithm, and $\tilde{\mathbb{C} \setminus \{ 0 \}}$ is the universal covering space of $\mathbb{C} \setminus \{0\}$. Assume that the coefficients are holomorphic on the same disk. It is known that $u$ converges on the region
$$ \{(t,x) : |t_1| < \epsilon, |t_2 | <\epsilon , |yt_1|<\epsilon, |y^2 t_2| <\epsilon, |x_i| < r\ (i =1,...,n) \}.$$ I want to show that 
$$ \max_{x \in K} |u(t,x)| = O(t^a)$$
as $t$ tends to zero in $S_\theta := \{t : |\arg t|<\theta \}.$ What I have so far: Take $r_1 > 0, r_2 > 0, r_3 > 0,$ and $r_4 > 0$ so that $r_1 < \epsilon, r_2 < \epsilon, r_3 r_1 < \epsilon, r_3^2 r_2 <\epsilon$ and $r_4 < r$ hold. Then $u$ is now convergent on the region
$$ \{(t_1, t_2 , x, y) : |t_1| \leq r_1 , |t_2| \leq r_2, |y| \leq r_3, |x_i| \leq r_4\ (i=1,...,n) \}.$$
We can get Cauchy's estimates for the coefficients as follows: For some $M$,
$$ |u_i (x) | \leq \dfrac{M}{r_1^i} \qquad \text{and} \qquad |\varphi_{i,j,k} (x) | \leq \dfrac{M}{r_1^i r_2^j r_3^k}$$
on $D(r_4) := \{x : |x_i| \leq r_4\ (i=1,...,n) \}.$ There, any help - hints, suggestions on how to proceed, results that can be used - would be appreciated. Thank you!","['asymptotics', 'self-learning', 'complex-analysis', 'taylor-expansion']"
681205,Inverse of the Joukowski map $\phi(z) = z + \frac{1}{z}$,"We know the Joukowski map $$\phi(z) = z + \frac{1}{z}$$ which maps the upper semidisc of radius $1$ in the lower half plane, and the lower semidisc of radius $1$ in the upper half plane. What is the inverse of this function ? We obtain $z ^{2}-zy + 1 = 0$ and this equation has $2$ solutions, which is the right one ?","['complex-analysis', 'analysis']"
681209,What is the difference between standard derivative and partial derivative?,"This is actually a very old question that now I have to face it again and look for answer of it. Suppose $f:\mathbb{R}^n\to \mathbb{R}, f(x_1,x_2,...,x_n)=y$ is a function. What is the difference between: $\frac{\partial f}{\partial x_i}$ $\frac{df}{dx_i}$ if this means anything at all? I am reading this book and the following passage is part of the book: ...it would be appropriate to introduce a scaled time $\tau$ via $$\tau=\epsilon^2 t$$ and regard $u$ as depending both on $t$ and $\tau$, and having no
  explicit dependence on $\epsilon$; $t$ and $\tau$ will be treated as
  mutually independent. Correspondingly, the time differentiation should
  be transformed as $$\frac{d}{dt}\to\frac{\partial}{\partial t}+\epsilon^2
 \frac{\partial}{\partial \tau} $$ Where $u=X-X_0$ and $X_0$ is a stable answer for following differential equation:
$$\frac{dX}{dt}=F(X)$$","['calculus', 'derivatives']"
681226,Correlation Coefficient Distribution Function: An Apparent Discrepancy?,"I'd like to explain an apparent discrepancy between: (1) The sample correlation distribution function between sample vectors for a bivariate, correlated random variable (correlation coefficient = $\rho$) and (2) The sample correlation distribution function for two normally distributed random vectors that each contain a given signal with additive Gaussian noise. In this latter case, the correlation coefficient between these vectors $\rho$. While (1) gives the standard Pearson Correlation Coefficient, (2) is more consistent with time series analysis and match filtering/correlation detection. I'd like to use a distribution function for (2) to identify the detection performance of such a correlation detector. Please read on below. First sample correlation computation: The ""correct"" way to view the correlation distribution function is as follows: Suppose pairs $(x_{1}, s_{1})$, $(x_{2}, s_{2})$, $(x_{3}, s_{3})$, $\cdots$, $(x_{n}, s_{n})$ are a sample of size $n$ drawn from the distribution of the random vector $(S, \, X)^{\prime}$, which has a bivariate normal distribution with mean vector $(\mu_{S}, \, \mu_{X})^{\prime}$ and covariance: \begin{equation} 
\Sigma =  
\begin{bmatrix}
       \sigma_{S}^{2} & \sigma_{S\,X} \\[0.3em]
       \sigma_{S\,X} & \sigma_{X}^{2}            
     \end{bmatrix},
\end{equation} where the population correlation coefficient is: \begin{equation} 
\rho = \cfrac{\sigma_{S\,X}}{\sigma_{X}\, \sigma_{S}}
\end{equation} The estimator for $\rho$, call it $\hat{\rho}$, is: \begin{equation}
\hat{\rho}_{1}  
= \frac{ \boldsymbol{x}^{\text{T}} \boldsymbol{s} } { \left|\left|\, \boldsymbol{x} \,\right|\right| \,\left|\left| \,\boldsymbol{s} \,\right|\right| }
\end{equation} where $\boldsymbol{x}$ and $\boldsymbol{s}$ are concatenated $n$-dim vectors formed from from the samples $(x_{k}, s_{k})$ above ($k$ $=$ $1,2,\cdots,n$). Coefficient $\hat{\rho}$ has a Pearson's correlation coefficient distribution function, that I will call $p_{R}\left( r \,; \rho, n, \,\mathcal{H}_{1}\right)$. The expression $\mathcal{H}_{1}$ is borrowed from hypothesis decision theory. It indicates that the true correlation is nonzero, e.g., $\rho$ $\ne$ $0$. Second sample correlation computation: OK: Now take two vectors, $\boldsymbol{x}$ and $\boldsymbol{s}$. This time, assume: $\boldsymbol{x}$ $\sim$ $\mathcal{N}(\boldsymbol{\mu}_{X}, \sigma_{X}^{2}\mathbf{I})$ and: $\boldsymbol{s}$ $\sim$ $\mathcal{N}(\boldsymbol{\mu}_{S}, \sigma_{S}^{2}\mathbf{I})$ Again, compute the sample correlation distribution function: \begin{equation}
\hat{\rho}_{2}  
= \frac{ \boldsymbol{x}^{\text{T}} \boldsymbol{s} } { \left|\left|\, \boldsymbol{x} \,\right|\right| \,\left|\left| \,\boldsymbol{s} \,\right|\right| }
\end{equation} Suppose now that $\rho$ is the same in both cases . Inconsistency: These coefficients DO NOT have the same distribution functions. That is, the distribution function for $\rho_{1}$ and $\rho_{2}$ do not overlap. In fact, the latter is lower variance, and shifted to the right of the first. The following figure illustrates that while  $\rho_{1}$ has the ""expected"" Pearson correlation coefficient distribution (left panel), the second coefficient $\rho_{2}$ behaves as though it has more samples, and a higher correlation coefficient (right panel). A parameter fit for $\rho$ and $n$ using $p_{R}\left( r \,; \rho, n, \,\mathcal{H}_{1}\right)$ illustrates that the distribution for $\rho_{2}$ is parametrized differently (blue curve, right panel). Both $\rho$ and $n$ are larger than expected. My Question : Can we develop the correct distribution function for $\rho_{2}$? Or, can we ""correct"" $\rho$ and $n$ and simply use  $p_{R}\left( r \,; \rho, n, \,\mathcal{H}_{1}\right)$? To generate these figures, Matlab Code follows below: %NOTE! Function corrdist.m is commented out at bottom of code. Save
%the function as it's own .m file and comment out to run this code.

%STEP 0: 
%PROVIDE PRELIMINARY DEFINITIONS.

clear h;
%length of time series/dimension of vector (make N even for convenience)
N       = 1e2;
%number of samples drawn from population (to make histogram). Choose a very
%large sample to make the histogram smooth.
Nsim    = 2e4;

%the true correlation coefficient (||n||^2 = sigma^2*(N-1) = 1*(N-1) ):
r0  = 1/( 1 + (N-1)./(s'*s));

%now add N(0,1) noise to signal to make Nsim sample-vectors from same
%distribution. Call result x.

%signal definition
s   = sqrt(2)*randn(N,1);
x   = repmat(s,1,Nsim) + randn(N, Nsim);

%STEP 1: 
%COMPUTE THE HISTOGRAM FOR THE CORRELATION COEFFICIENT BETWEEN TWO
%NORMALLY DISTRIBUTED VECTORS WITH SPECIFIED CORRELATION r0.

%Define a bivariate, normally distributed random variable z.
R   = chol([1,r0;r0,1]);
mu  = [0,0];

%Compute a correlation coefficient between each random vector
%This is not efficient, but it shows the reader (you) what I am doing:
c   = [];
for k = 1:Nsim,

    z       = (repmat(mu,N,1) +  randn(N,2)*R);
    c       = cat(1, c, z(:,1)'*z(:,2)/(norm(z(:,1))*norm(z(:,2))));    
end;

subplot(1,2,1);
[Nb, b]     = hist(c, floor(sqrt(length(cc))));
Nb          = Nb./trapz(b, Nb);

h(1) = bar(b, Nb,'facecolor','k','edgecolor','none');
hold on;
r       = linspace(0,1,1e4);
h(2)    = plot(r, corrdist(r, r0, N),'-r','linewidth',4);
f1      = gcf;
a1      = gca;

legend(h,{'Sample Correl. Hist.','Theor. Distr.'},'Location','Best');
legend boxoff;
title('Using Correlated Normal Random Variables');
xlabel('Coefficient');
ylabel('Normalized Density');

%STEP 2: 
%COMPUTE THE HISTOGRAM FOR THE CORRELATION COEFFICIENT BETWEEN TWO
%NORMALLY DISTRIBUTED VECTORS THAT CONSIST OF A GIVEN SIGNAL, PLUS ADDITIVE
%GAUSSIAN NOISE.

%normalize data vectors (columns) for correlation coefficient computation. 
%Take care not to include norm-computations that are below eps to 
%avoid numerical blow-up (divide by zero).
n           = sqrt( sum( x.*conj(x),1) );
m           = n(n>eps);
x(:,n<eps)  = [];
Nsim        = length(m);
temp        = repmat(m.^(-1),size(x,1),1);

%normalize x = signal + noise for correlation computation. 
x           = x.*temp;

%now compute the sample correlation between the first sample vector and the
%following independent and identically distributed column vectors in x.
cc      = x(:,1)'*x(:,2:end);

%compute histogram, and normalized histogram.
[Nb, b]     = hist(cc, floor(sqrt(length(cc))));
Nb          = Nb./trapz(b, Nb);

%NOTE: The correlation distribution function cc and the histogram do not
%agree in this case.
subplot(1,2,2);
h(3) = bar(b, Nb,'facecolor','k','edgecolor','none');
hold on;
r   = linspace(0,1,1e4);
h(4) = plot(r, corrdist(r, r0, N),'-r','linewidth',4);

%BUT: The EFFECTIVE distribution function for cc behaves as though it has
%many more degrees of freedom, and a different true correlation:

%Find the best fit correlation value and sample number to fit histogram.
dof     = fminsearch( @(p) norm( corrdist(b,p(1),p(2)) - Nb), [r0, N]);
h(5)    = plot(r, corrdist(r, dof(1), dof(2)),'-b','linewidth',4);
f2      = gcf;
a2      = gca;
ylimRef = ylim;
ylim(a1,ylimRef);
set(a2,'ytick',[]);
set(a1,'xlim',[0.27, 0.92]);
set(a2,'xlim',[0.27, 0.92]);

legend(h(3:end),{'Correl. Histogram','Theor. Distr.','Best-Fit Distr.'},'Location','Best');
legend boxoff;
title('Using Signal + Noise Vectors');
%Note that the latter,  best-fit correlation distribution fits the 
%histogram MUCH better than the correlation distribution function using the
%assumed parameters, r0 and N.

mydir   = pwd;

set(gcf,'units','normalized','outerposition',[0,1,1,1]);
set(gcf,'Units','points')
set(gcf,'PaperUnits','points')

sizeX= get(gcf,'Position');
sizeX= sizeX(3:4);
set(gcf,'PaperSize',sizeX)

set(gcf,'PaperPosition',[0,0,sizeX(1),sizeX(2)])

print -dpdf  -r300 CorrelationParadox;

> %CORRDIST.M IS DOWN HERE! 
%CORRDIST.M IS DOWN HERE!
%
% function y = corrdist(r, ro, n) 
% %This function computes the probability density function for the
% %correlation coefficient of a bivariate random variable.
% %
% % USAGES
% % y = corrdist(r, ro, n)
% %
% % INPUT
% % r:    Vector of possible correlation random variables, i.e. the values at
% %       which the pdf is evaluated at.
% % ro:   The given (true) correlation coefficient, i.e. the population
% %       correlation coefficient. length(ro) > 1 supported.
% % n:    The number of samples in the correlated data. Only length(n) = 1
% %       supported.
% % 
% % OUTPUT
% % y:    The probability density function for r, given ro, for n data
% %       samples of a bivariate normal distribution.
% %
% %-----------------------------------------------------------------------
% % Latest Edit: 11.June.2012
% % Joshua D Carmichael
% % [email¬†protected] % %
% % Original Author: Xu Cui, Stanford University (retrieved 11.June.2012)
% %-----------------------------------------------------------------------
% 
% %accept vectorized inputs.
% if(length(ro)> 1.5),
%     r   = repmat(r(:),1,length(ro));
%     ro  = repmat(ro(:)', length(r),1);
% end;
% 
% if( n < 120 ),
%     
%     y = (n-2) * gamma(n-1) * ((1-ro.^2).^((n-1)/2)).* (1-r.^2).^((n-4)/2);
%     y = y./ (sqrt(2*pi) * gamma(n-1/2) * (1-ro.*r).^(n-3/2));
%     y = y.* (1+ 1/4*(ro.*r+1)/(2*n-1) + 9/16*(ro.*r+1).^2 / (2*n-1)/(2*n+1));
%     
% else
%     
%     y = (n-2) * (1-ro.^2)^((n-1)/2) * (1-r.^2).^((n-4)/2);
%     y = y./ (sqrt(2*pi) * (1-ro.*r).^(n-3/2)) * n.^(-1/2);
%     y = y.* (1+ 1/4*(ro.*r+1)/(2*n-1) + 9/16*(ro.*r+1).^2 / (2*n-1)/(2*n+1));
%     
% end;
% 
% y(r>1)              = 0;
% y(r<-1)             = 0;
% y(~isfinite(y))     = 0;
% 
% return;","['statistics', 'correlation', 'probability-distributions', 'hypothesis-testing']"
681252,Notation for rounding in equation,"I'm wondering if there is a symbol or notation for Round to the nearest 10th For example, the area of a circle with a radius of 45 feet, rounded to the nearest square foot, could be written as, A = œÄ45¬≤sym Where sym is some symbol that means round to the nearest sq foot",['algebra-precalculus']
681290,"Divisible groups, exercise from Rotman's theory of groups","The following exercise is from Rotman, An Introduction to the theory of groups, 4th ed, p324.
""The following conditions on a group G are equivalent:
(i) G is divisible,
(ii) Every nonzero quotient of G is infinite,
(iii) G has no maximal subgroups."" I can prove that (i) implies (ii) and that (ii) is equivalent to (iii) but I am having trouble with showing that (iii) implies (i) and (ii) implies (i). Any suggestions?","['divisible-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
681296,What is $\lim_{n \to \infty} \sum_{x=0}^{n-1} \frac{n-x}{n+x}$?,"These are two little questions that came to mind while I was looking at this problem . What is $\displaystyle \lim_{n \to \infty} \sum_{x=0}^{n-1} \frac{n-x}{n+x}$? I am fairly certain that the answer is $\infty$ because as $n$ gets closer to $\infty$ there are more terms that are very close to $1$ (if $n = 1,000,000$ then all the terms until $x = 5026$ are greater than or equal to $0.99$, and if $n = 1,000,000,000$ then you have to get to $x = 5025126$ for the terms to drop below $0.99$), but I don't know how to prove it. I also checked the partial differences (i.e. between $n = 1$ and $n = 2$, between $n = 2$ and $n = 3$, and so forth) and noticed that they all tend to some number around $0.386294$. Is there a name for this number, and what's its significance? WolframAlpha seems to suggest it has something to do with the Digamma function but I'm not sure what it's all about.","['fractions', 'summation', 'limits']"
681324,Solving $ \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) $,"I found this problem in a collection of contest problems of a Russian competition in 1995 and wasn't able to solve it. Solve for real $x$:
  $$ \cos (\cos (\cos (\cos(x))))=\sin (\sin (\sin (\sin (x)))) $$ My guess is that there is no solution, but how do I prove it? I tried to estimate $LHS\ge \cos (1) \ge \cos(\pi/3)=1/2 $ and RHS similarly but the ranges overlap.. Do you have a better idea?","['calculus', 'contest-math', 'trigonometry', 'real-analysis', 'analysis']"
681326,Is the Center of G the same as the Centralizer of g in G?,"Is the center, $Z(G)$, of a group $G$ the same as the centralizer, $C(g)$, of an element $g\in G$?
I have proven that $C(g)\leq G\forall g\in G$ but my homework, in a later problem, asks me to prove that $Z(G)\leq G$. This confuses me, because I thought $C(g)$ is the same as $Z(G)$.","['group-theory', 'abstract-algebra']"
681364,Curvature and the Arrow Pratt Absolute Risk Coefficient,"So I'm in my first year of grad school, and I'm taking a decision analysis course. One of the topics we're covering is risk aversion, and with that comes discussion of the Arrow Pratt Absolute Risk Aversion coefficient. I know that this coefficient is supposed to be a measure of the curvature of an individual's utility function; however, using the little bit I remember from differential geometry, the Arrow Pratt coefficient definitely is not equal to the standard geometric definition of curvature. My question is: how are the two definitions related? function definitions $u(x)$: this functions takes in a vector $x \in R^{n}$ and spits out a value $u(x) \in R$ $r(x)= \frac{-u''(x)}{u'(x)}$  (risk coefficient) $k(x) = \frac{|u'(x)|}{(1+(u'(x))^{2})^{3/2}}$ In class, we've been using utility functions that have a constant absolute risk version coefficient. So, I used the one given in class when toying around in Matlab. This function is: $u(x)=(\frac 4 3)(1-(1/2)^{x/50})$ Thus, $u'(x)= \frac {-(2*(1/2)^{x/50}\ln(1/2))}{75}$ $u''(x)= \frac{-((1/2)^{x/50}log(1/2)^2}{1875}$ And we have: $r(x) = \frac{-\ln(1/2)}{50}$ However, curvature is: $k(x) = \frac{1}{((4(\frac12)^{x/25}\frac{log(1/2)^2}{5625} + 1)^{3/2}} = \frac{1}{(.00034166(\frac12)^{x/50}+1)^{3/2}}$ So, clearly this line does not have constant curvature by the geometric definition. This is also obvious when looking at a graph of $u(x)$. So, I'm struggling to relate the two measures. Is Arrow-Pratt discussing the relationship between slope and curvature? Could someone help elucidate this. Thanks!","['economics', 'geometry', 'curvature', 'utility']"
681373,Global maxima of a function subject to a constraint.,"I am trying to prove that the global maxima of $f(x_1,x_2,...,x_n)=(x_1x_2¬∑¬∑¬∑x_n)^2$, subject to $\lVert(x_1,x_2,...,x_n)\rVert_2=r$ is  $(r^2/n)^n$ I know I have to find the critical points of the Lagrange function, that is $$F(x_1,x_2,...,x_n,\lambda)=(x_1x_2¬∑¬∑¬∑x_n)^2+\lambda(\sqrt{\sum_{i=1}^nx_i^2}-r)$$
In order to do that, I've built the system in which all partial derivatives are null, so I have: $$
D_1F(x_1,x_2,...,x_n,\lambda)=2(x_1x_2¬∑¬∑¬∑x_n)(x_2x_3¬∑¬∑¬∑x_n)+\frac{\lambda x_1}{\sqrt{\sum_{i=1}^nx_i^2}}=0
$$
$$
...
$$
$$
D_jF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2¬∑¬∑¬∑x_n)(x_1x_2¬∑¬∑¬∑x_{j-1}x_{j+1}¬∑¬∑¬∑x_n)+\frac{\lambda x_j}{\sqrt{\sum_{i=1}^nx_i^2}}=0
$$
$$
...
$$
$$
D_nF(x_1,x_2,...,x_n,\lambda)=2(x_1x_2¬∑¬∑¬∑x_n)(x_1x_2¬∑¬∑¬∑x_{n-1})+\frac{\lambda x_n}{\sqrt{\sum_{i=1}^nx_i^2}}=0
$$
$$
D_{n+1}F(x_1,x_2,...,x_n,\lambda)=\sqrt{\sum_{i=1}^n}x_i^2-r=0
$$ I think, because I have tried to solve reduced forms of this system in a mathematical software, that the solutions are like the following:
$$
(x_1,x_2,...,x_n)=(\pm\frac{r}{\sqrt{n}},\pm\frac{r}{\sqrt{n}},...\pm\frac{r}{\sqrt{n}})
$$
where all the $x_i$ take the positive and negative sign, so there are $2^n$ solutions. However, I didn't manage to solve the system. I have tried to replace the summation with $r$ in all equations, to sum all the equations, to reduce dividing by $x_1¬∑¬∑¬∑x_n$... but I didn't get to any interesting end. Could you help me, please, giving me any hint about how can I solve this system? Thank you very much.",['multivariable-calculus']
681376,Texture mapping from a camera image (knowing the camera pose),"I'm not sure if I should ask this question here or on stackoverflow, so forgive me if I'm wrong. I want to apply a texture (taken from a camera) on a 3D surface, let me explain my problem: I have acquire a 3D surface through a Kinect camera. At some intervals I have saved the camera position (A rotation|translation matrix) as well as a 2D image of what the camera is looking at. Now, what I'm trying to do is to find which 3D points (X,Y,Z) correspond to the 2D point (u,v) in my image, if I know that, I can apply the color of my 2D point on the corresponding 3D point. I guess I have to start from this formula to solve this issue ? (taken from here ) The first matrix represents the intrinsic parameters of my camera while the second matrix represents my Rotation|Translation matrix. Both matrices are known. Here is a picture representing what I'm trying to achieve (sorry for my poor drawing skills) As you can see, the 3D surface is represented with triangles, each point of the triangle is stored in a .obj file, so I can also have access to them. From the formula, I just have to take the 3D point interesting me, apply the formula and find the corresponding 2D point. However, I don't have any ideas of how to find the 3D points that are in my camera range. If I take a point that is not in my camera range, the corresponding 2D point would not make any sense and so the texture mapping will be wrong. Any idea of how I can solve this issue ? Thanks !","['matrices', '3d', 'computer-science', 'image-processing']"
681394,"Convergence almost everywhere implies convergence in measure, the proof thereof","Let $(E, \mathcal{E}, \mu)$ be a measure space, and $(f_n)_{n\in\mathbb{N}}$ and $f$ be measurable functions $(E, \mathcal{E}, \mu)\longrightarrow (\mathbb{R}, \mathcal{B})$. The first part of Theorem 2.10 on page 20 of Stefan Grosskinsky's lecture notes says ""assume that $\mu(E) < \infty$, then if $f_n \rightarrow f$ almost everywhere then $f_n \rightarrow f$ in measure"". I don't quite understand why we need $\mu(E) < \infty$. The proof given in there does indeed use $\mu(E) < \infty$. However, I seem to be able to find another proof that doesn't need to use it. I will first present (a slightly modified version of) the proof given by Prof. Grosskinsky below. Assume $f_n \rightarrow f$ almost everywhere and let $g_n = f_n - f$ for each $n$, we want to show that $g_n \rightarrow 0$ in measure. We need to show that for every $\epsilon > 0$, we have $\mu(|g_n| \geq \epsilon) \rightarrow 0$, or equivalently in the case $\mu(E) < \infty$, $\mu(|g_n| < \epsilon) \rightarrow \mu(E)$. Indeed, \begin{equation*} \begin{split} \mu(|g_n| < \epsilon) &\geq \mu\big(\bigcap_{m \geq n} \{|g_m| < \epsilon\}\big) \\ &\nearrow \mu\big(\bigcup_{N \in \mathbb{N}} \bigcap_{m > N} \{|g_m| < \epsilon\}\big) \\ &\geq \mu \big(\bigcap_{q \in \mathbb{Q},\, q > 0}\bigcup_{N \in \mathbb{N}} \bigcap_{m > N} \{|g_m| < \epsilon\}\big) \\ &= \mu(g_m \rightarrow 0) \\ &= \mu(E). \end{split} \end{equation*} I understand the proof above, but below I will present what seems to be a proof without resorting to $\mu(E) < \infty$. This proof directly shows, for all $\epsilon > 0$, that $\mu(|g_n| \geq \epsilon) \rightarrow 0$, as opposed to the proof above where we showed $\mu(|g_n| < \epsilon) \rightarrow \mu(E)$. \begin{equation*} \begin{split} \mu(|g_n| \geq \epsilon) & \leq \mu\big(\bigcup_{m \geq n} \{|g_m| \geq \epsilon\}\big)\\ &\searrow \mu \big(\bigcap_{N \in \mathbb{N}} \bigcup_{m>N} \{|g_m| \geq \epsilon\}\big)\\ &\leq \mu\big(\bigcup_{q\in\mathbb{Q},\, q>0} \bigcap_{N \in \mathbb{N}} \bigcup_{m>N} \{|g_m| \geq \epsilon\}\big)\\ &= \mu(g_m \nrightarrow 0)\\ &= 0. \end{split} \end{equation*} This 'proof' doesn't resort to $\mu(E) < \infty$, so it seems to me. Am I doing something wrong? Thank you in anticipation for your help!!!","['measure-theory', 'convergence-divergence', 'probability']"
681433,An inequality for sides of a triangle,"Let $ a, b, c $ be sides of a triangle and $ ab+bc+ca=1 $. Show
$$(a+1)(b+1)(c+1)<4 $$ I tried Ravi substitution and got a close bound, but don't know how to make it all the way to $4 $.
I am looking for a non-calculus solution (no Lagrange multipliers). Do you know how to do it?","['geometry', 'inequality', 'contest-math', 'geometric-inequalities', 'triangles']"
681445,Show that the right half-open topology on $\mathbb R$ is not metrisable.,"The right half-open topology on the real line $\mathbb R$ is the topology generated by the right half-open intervals $[a,b)$ for $ -\infty < a < b < \infty$. How to show it does not arise from a metric? I know it's a Hausdorff space so this approach doesn't work. Any idea is appreciated.","['general-topology', 'real-analysis']"
681447,Is the universal hyperplane section the blowup of the baselocus?,"I think I've heard this statement before but I'd like to make sure it's true. Let $X$ be a variety and $L$ a line bundle on it.
Take $S < P\left(H^0(X,L)\right)$ to be a linear subspace of the projective space of sections.
Such a thing is called a linear system and produces a (rational) map $X \to P\left( H^0(X,L)^\vee \right)$ to the dual projective space, sending $x$ to $\epsilon_x \colon H^0(X,L) \to k$ which takes $s$ to $\epsilon_x(s) = s(x)$. The baselocus $A$ of the linear system $S$ is given by $\{x \in X | s(x)=0, \forall s \in S\}$.
The universal family is $H$, given by $\{(s,x) \in S \times X | s(x) = 0 \}$. My question is: is $H = Bl_A X$?
If so, why?","['algebraic-geometry', 'projective-geometry']"
681454,Topology on the space of compatible almost complex structures in symplectic geometry,"I have a few fairly generic questions, with a specific application to symplectic geometry in mind. Let me pose the specific problem first: Let a symplectic manifold $(M,\omega)$ be given. One is naturally led to consider the ""space"" $\mathcal{J}$ of almost complex structures compatible with $\omega.$ One then shows that this space is non-empty and contractible. Of course, this is really just a set until one equips it with a topology. My basic question is: what is this topology? I can think of one possibility, which rests on the apparent fact (stated without proof in Audin-Lafontaine) that one has a bundle $\mathcal{J}(\omega) \to M$ of compatible almost complex structures, with fiber $\mathcal{J}_p = \{ \text{complex structures of } T_pM \text{ compatible with } \omega_p \}.$ Our space $\mathcal{J}$ is then the space of sections of said bundle. Working fiberwise, we can see that we have a bijection $\mathcal{J}_p \cong Sp_{2n}/U(n),$ so we could let $\mathcal{J}_p$ inherit the topology of that homogeneous space. Is there a natural way, then, to construct a topology on $\mathcal{J}$ using the topologies on the fibers $\mathcal{J}_p?$ More generally, is there a natural way to topologize the space of sections of a vector bundle that applies to this scenario? Finally, how would one construct the bundle $\mathcal{J}(\omega)$ from the fibers and the base space? I suppose one could use the fiber bundle construction theorem, but then one would need specific transition maps.","['almost-complex', 'riemannian-geometry', 'symplectic-geometry', 'differential-geometry']"
681456,"Why can Bessel sequences be defined by the condition $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}<\infty$?","A sequence $\{f_{k}\}_{k=1}^{\infty}$ is called a Bessel sequence in a Hilbert space $H$, if there exists $B>0$ such that 
$$\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}\leq B\|f\|^{2}$$
for all $f\in H$. But my question is: is this an equivalent definition for if $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}<\infty$ for all $f\in H$, then $\{f_{k}\}_{k=1}^{\infty}$ is a Bessel sequence. If yes, how to show that if $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}<\infty$ for all $f\in H$, then $\{f_{k}\}_{k=1}^{\infty}$ is a Bessel sequence. How to relate the infinity with $||f||^2$? I was thinking can we use method of contradiction? Suppose $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}> B\|f\|^{2}$, so we have $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}=\infty$ (is this conclusion right?), so a contradiction. So we must have $\sum_{k=1}^{\infty}|\langle f,f_{k}\rangle|^{2}\leq B\|f\|^{2}$. Does this make sense? Thanks in advance.","['wavelets', 'sequences-and-series', 'functional-analysis']"
681470,Decide whether a function has an elementary indefinite integral without determining it!,"Risch , who developed the algorithm in 1968, called it a decision procedure, because it is a method for deciding whether a function has an elementary function as an indefinite integral; and also, if it does, determining it . Is there a way to only decide whether a function has an elementary function as an indefinite integral without determining it?","['algorithms', 'functions', 'decision-theory']"
681473,Convergence in law of point processes implies pointwise convergence?,"Let me first set up what I mean by convergence in law of a point process. I'm interested here in simple, locally finite point processes on a complete, separable metric space $X$. And by a point process I mean a random, discrete measure on $X$. Let $M$ be the space of locally finite, discrete, simple measures on $X$ and give it the topology of vague convergence. Ie.,  $\mu_n \to \mu$ if for every continuous $f: X \to \mathbb{R}$ with compact support,
$$
\int f(x) d\mu_n(x) \to \int f(x) d\mu(x).
$$ Now say the sequence of random point processes $\mu_n$ converges in law to $\mu$ if they converge weakly with respect to the vague topology. Ie., for every continuous bounded function $F: M \to \mathbb{R}$,
$$
\mathbb{E} F(\mu_n) \to \mathbb{E}F(\mu).
$$ Finally, to my question. If $\mu_n$ converges in law to $\mu$, is it true that there exists a realization of these point processes on a probability space such that on compact sets the point processes converge almost surely? So if $P_n$ are the points in $X$ corresponding to $\mu_n$, $P$ are the points in $X$ corresponding to $\mu$, and $K \subset X$ is compact, is it true that
$$
\lim d( P_n \cap K, P \cap K) = 0 \, a.s., 
$$
where $d$ is the usual notion of distance between two sets? This seems like a Skorokhod type of result so I expect that it's true but I can't find a reference or prove it myself.",['probability-theory']
681477,Using the cocycle condition to glue sheaves,"Given a cover $\{U_i\}$ of a space $X$ and for each $U_i$ a sheaf $\mathcal{F}_i$ and isomorphisms $\phi_{ij}:\mathcal{F_j}|_{U_i \cap U_j} \rightarrow \mathcal{F_i}|_{U_i \cap U_j}$ satisfying the cocycle condition $\phi_{ij}\phi_{jk}\phi_{ki} = id$, I want to show that there is a sheaf $\mathcal{F}$ on $X$ whose restriction to each $U_i$ is isomorphic to $\mathcal{F_i}$. I understand that $\mathcal{F}(U)$ should consist of tuples $(s_i)$ of sections from the various sheaves, then I need to define restriction maps, prove the presheaf axioms and then the sheaf axioms. I have at least a vague idea of how the proof should go. Unfortunately, on my way to proving the result I get utterly lost in an unholy mess of details, so that even once I ""finished"" my proof I was nowhere near certain my proof was correct. I've searched around for proofs to examine, but any proof I can find is either lacking many details or required as an exercise. Here are my questions: 1) Can someone show me a proof of this result? I am interested to see both a direct proof checking all the details and also a more intuitive proof, possibly appealing to results about gluing morphisms or whatnot. 2) Should the tuples $(s_i)$ be leaving in the product or the disjoint union of the $\mathcal{F_i}(U\cap U_i)$? I thought it would be product, but I saw union on Google Books in Introduction to Singularities and Deformations by Greuel, Lossen, and Shustin. 3) Where does the cocycle condition come into play? 4) How can I prevent all these messy sheaves from deterring me from the beautiful subject of algebraic geometry? Thanks in advance.","['sheaf-theory', 'algebraic-geometry']"
681489,Variance of the sum of Bernoulli Random variables?,"$\newcommand{\var}{\operatorname{var}}$ Let $X_{i}$ be a Bernoulli random variable with paramater $p_{i}$ where $p_i$ itself is a random variable that ranges from $0$ to $1$. The expectation of $p_{i}$ is $\rho$, and $X_{i}$ is independent of $X_{j}$ where $i\neq j$ Let $Y=\sum_{i=1}^{n} X_{i}$ Show that $\var(Y)>n\rho(1-\rho)$ This is what I have done so far: $$\var(Y)=\var\left(\sum_{i=1}^n X_{i}\right) = \sum_{i=1}^n \var(X_{i})$$
$$\var(X_{i})=E[\var(X_{i}\mid p_{i})]+\var(E[X_i \mid p_i])$$
$$=E[p_i-p_i^2]+\var(p_i)$$
$$=\rho-E[p_i^2]+E[p_i^2]-E[p_i]^2$$
$$=\rho-\rho^2$$ So,
$$\var(Y)=\sum_{i=1}^n (\rho-\rho^{2})=n\rho(1-\rho)$$ What am I doing wrong? This is the variance if $p_i$ were a constant. When $p_i$ varies, surely that should increase the variance of $Y$. Any insight would be appreciated!","['probability', 'random-variables']"
681492,What is this cycle on the Jacobian of a curve?,"Let $C$ be a curve and $J$ it's Jacobian. There is the standard Abel-Jacobi map $a:C\rightarrow J$ which is given by $Q\rightarrow Q-P$ for some fixed point $P$ (here I am regarding $J$ as the degree 0 divisors up to equivalence on $C$). Let the image of this map in $Ch_1(J)$ be denoted as [C.] Instead of the standard Abel-Jacobi map, I'm interested in ""doubling"" it. Consider the map $b:C\rightarrow J$ given by $Q\rightarrow 2Q-2P.$ Notice that this map arises as the composition $$C\rightarrow J\times J\rightarrow J$$ where the first map is just $a$ in each component and the latter map is multiplication. We can denote the image of $b$ by the cycle $[W]$ . Question: Is $[C]\sim [W]$ in the chow ring? I don't have a particular reason to believe that this is true, but as a student that is fairly new to working with cycles, it would be good to see an argument or counterexample. Thanks!","['abelian-varieties', 'algebraic-geometry', 'algebraic-curves']"
681529,Eigenvalue problems for matrices over finite fields,"Suppose I have a symmetric matrix $A$ with entries in a finite field. In particular, I have the case in mind where $A \in \{0,1\}^{n \times n}$ and want to treat the entries as elements of $\mbox{GF}(2)$ . How much is known about the eigenvalue problem in this case? Is there a spectral theorem? Are there fast algorithms for computing eigenvectors?","['matrices', 'finite-fields', 'linear-algebra', 'eigenvalues-eigenvectors']"
681577,Intuition behind (statistical) completeness,"I was wondering if any of the members of the MSE community would like to share his/her intuition about completeness in statistics. For the sake of ""completeness"", here's the definition, taken from Wikipedia : Consider a random variable $X$ whose probability distribution belongs to a parametric family of probability distributions $P_\theta$ parametrized by $\theta$ . The statistic $T$ is said to be complete for the distribution of $X$ if for every measurable function $g$ (which must be independent of $\theta$ ) the following implication holds: $$E(g(T(X))) = 0 \mbox{ for all }\theta \mbox{ implies that }P_\theta(g(T(X)) = 0) = 1\mbox{ for all }\theta.$$ Now, to give you an idea of what kind of answer I am looking for, here's how I think about (minimal) sufficiency: I think of a statistic as a partition of the sample space. In that context, a statistic is sufficient for $\theta$ if this partition does not result in a loss of ""information"" about $\theta$ ; it is minimal sufficient if it is the coarsest partition which does not result in a loss of information (superlatives carry a uniqueness connotation, which I am ignoring here).","['statistics', 'intuition', 'random-variables']"
681580,What exactly is a modulus of continuity?,"This is my first post on here, so forgive me if I am ignorant of certain customs. I am currently reading Courant & John's Introduction to Calculus and Analysis Volume I . Unfortunately, I have stumbled upon an ambiguous notation on page 41. It begins: Our definition of continuity of the function $f(x)$ at $x_0$ requires that for every degree of precision $\epsilon > 0$ there exists quantities $\delta \gt 0$ (so-called moduli of continuity) such that $|f(x)-f(x_0)| < \epsilon$ for all $x$ in the domain of $f$ for which $|x-x_0| < \delta$ . He then goes on to say that (for ""regular continuity""): In general, as our example show, this $\delta = \delta(\epsilon)$ depends not only on $\epsilon$ but also on the value of $x_0$ . What exactly is meant by a modulus of continuity and how is it related to $\delta(\epsilon)$ ? Edit - Upon further inspection I have noted that the ""modulus of continuity"" depends on both $x_0$ and $\epsilon$ for ""regular continuity"", however only depends on $\epsilon$ for uniform continuity. I believe that some of the confusion has been cleared by the notation of $\delta(\epsilon)$ . However, he states that moduli of continuity are simply quantities $\delta > 0$ . Is this ""it"", or is there further discussion to be had?","['continuity', 'real-analysis']"
681586,The Derivative of a General Linear Map,"This question is somewhat abstract compared to the things we've discussed in class, so I'm just making sure I've got the right idea. I'd appreciate any help/suggestions; I'm pretty sure I've got the right answer, but I may be totally incorrect in what I think the question is asking. The question is:
Suppose $f: \mathbb{R}^n \rightarrow \mathbb{R}^m$ is a linear map. What is the derivative of $f$ ? My answer is: Let $f: A \subset \mathbb{R}^n \rightarrow \mathbb{R}^m$ be a linear map where $A$ is an open set. Let $x,y \in \mathbb{R}^n$ and $\alpha \in \mathbb{R}$ . From the def. of a linear map, we know that $f(\alpha x) = \alpha f(x)$ and $f(x + y) = f(x) + f(y)$ . Thus, to be a linear map, no element in the image of $f$ can contain the product of elements in the domain of $f$ (otherwise, $f(x_1,x_2) = x_1x_2 \implies f(\alpha_1x_1,\alpha_2x_2) = \alpha^2x_1x_2 \ne \alpha f(x_1,x_2)$ ). Given the above statement, all linear maps from $\mathbb{R}^n$ to $\mathbb{R}^m$ must have the form... $f(x_1,...,x_n) =$ $\begin{pmatrix} 
a_{1,1} x_1 ~ + ...+ ~ a_{1,n} x_n \\
\vdots \\
a_{m,1}x_1 ~ + ... + ~ a_{m,n} x_n
\end{pmatrix}$ Where $a_{i,j}$ is the coefficient of the $x_{j}$ term in the $i^{th}$ row. The derivative of this function is the matrix of partial derivatives of $f(x_1,...,x_n)$ . Note, though, that the partial derivative of any row with respect to $x_i$ is just the coefficient of the $x_i$ term (since no other term in the row depends on $x_i$ , thus their partial derivatives are zero). So our matrix of partial derivatives becomes: $\frac{df}{dx} = \begin{pmatrix} 
a_{1,1} & ... & a_{1,n} \\
\vdots & \ddots\\
a_{m,1} & ... & a_{m,n}
\end{pmatrix}$ (That is, the matrix of coefficients where the $i^{th}$ column contains the $m$ coefficients of each $x_i$ term). Can anyone verify whether or not a) This is actually what the question is looking for (I think the simplicity is making me skeptical) and b) Whether or not I've the correct answer if it is, in fact, what the question is looking for?","['vector-spaces', 'multivariable-calculus']"
681608,Hartshorne II Prop. 6.9,"Prop. 6.9: Let $X \to Y$ be a finite morphism of non-singular curves, then for any divisor $D$ on $Y$ we have $\deg f^*D=\deg f\deg D$. I can not understand two points in the proof: (1) (Line 9) Now $A'$ is torsion free, and has rank equal to $r=[K(X):K(Y)]$. Since it is a torsion-free module over PID $O_Q$, I see it is free, but how to calculate its rank? (2) (Line 15) Clearly $tA'=\bigcap(tA'_{\mathfrak m_i}\cap A')$ so ... I don't know how to show the claim ?","['modules', 'commutative-algebra', 'algebraic-geometry', 'proof-explanation']"
681619,What does $\prod_{n\geq2}\frac{n^4-1}{n^4+1}$ converge to?,"What does $\prod_{n\geq2}\frac{n^4-1}{n^4+1}$ converge to? As far as I can tell, this has no closed-form solution (not saying much, I don't know much math), but a friend of mine swears he saw a closed-form solution to this in some text he doesn't remember. Running it through WolframAlpha gives me an approximation which, entered through an inverse symbolic calculator, gets no results. This is satisfactory enough for me to believe there is no closed-form solution, but my friend really does insist there is one. Edit Okay, not meaning to sound greedy, but I'd also be interested in seeing how I'd derive a closed-form solution algebraically. I do appreciate the fast answers, though.","['sequences-and-series', 'calculus', 'products', 'analysis', 'limits']"
681623,$\arcsin(\sin x)$ explanation?,"First off, I know this is a duplicate of this question . I'm asking this because I still don't quite understand the answer given there. But first, some graphs! $y=\sin(x)$ $y=\arcsin(x)$ $y=\sin(\arcsin(x))$ $y=\arcsin(\sin (x))$ What I don't understand is why $f(x) = \arcsin(\sin(x))$ looks like it does. As stated in the previously linked question, $f(x)=\arcsin(x)$ is only defined between $[-1,1]$. So, how does it effect the rest of the graph? Shouldn't the domain be only values of $\sin x$ within the domain of $\arcsin x$? Thanks!","['trigonometry', 'algebra-precalculus']"
681631,A product of minimal normal subgroups is the product of some subcollection,"I would appreciate a hint on how to go about solving the following problem. Let $X$ be a finite set of minimal normal subgroups of $G$ and write $K:=\prod X$ (the product of all members of $X$). a) Show that $K$ is the direct product of some subcollection $Y \subseteq X$. b) If $Y$ is as in (a), show that all members of $X \setminus Y$ are contained in $Z(K)$.  Conclude that $Y$ contains all nonabelian members of $X$. Let $X=\{N_1,...,N_k\}$.  I have the result that each $N_i$ is the direct product of isomorphic (specifically, conjugate in $G$) simple subgroups.  Since the center of a direct product is the direct product of the centers, this tells us that for all $N_i$, either $Z(N_i)=1$ or $Z(N_i)=N_i$.  This plus part (b) seems to indicate that we can leave out the abelian subgroups, but I certainly do not see why. If the $N_i$ are finite, then we must have $Y=X$ because otherwise $\prod Y$ must have smaller order and cannot equal $K$, so I think I should be working in the fully general case of infinite minimal normal subgroups in infinite groups. Is there another property of minimal normal subgroups that I'm not thinking of but should be considering?","['group-theory', 'abstract-algebra']"
681638,"For the binomial distribution, why does no unbiased estimator exist for $1/p$?","Suppose that $X \sim \mathrm{Binomial}(n,p)$ for $0 < p < 1$ Why does no unbiased estimator exist for $1/p$? My approach: We try to find the structure of $E_p(U(x))$, where $U(x)$ is any estimator of $1/p$. Now, we will have: $$\sum{U(x)\binom{n}{x}\theta^x(1-\theta)^{n-x}}<\sum{U(x)\binom{n}{x}}<M(n)<\infty$$ so that the expectation is bounded above. So this is supposed to mean that if $\theta < 1/M(n)$, then the expectation cannot attain $1/\theta$ but I am not sure why the above argument even makes sense and what being bounded means for the expectation. Thanks!",['statistics']
681676,Flip 98 fair coins and 1 HH coin and 1 TT coin,"Flip $98$ fair coins and $1 \ HH$ coin and $1 \ TT$ coin. Given that you see an $H$ , what is the probability that it was the $HH$ coin. Applying Bayes Theorem, : $$P(HH|H) = \frac{P(H|HH) * P(HH)}{P(H)}$$ $P(H|HH) = 1$ $P(HH) = \frac{1}{100}$ $P(H) = \frac{100}{200} = \frac{1}{2}$ So I get $P(HH|H) = \frac{1}{50}$ 1) Is this the correct answer? 2) What's wrong with the 'intuitive' answer? I.e. You see $1\ H$ , so we only have $99$ possibilities remaining.  Of these $99$ , only $1$ of them is $HH => 1/99$","['bayes-theorem', 'probability']"
681692,$f(f(x))=f(x)$ question,I am wondering what is the class of functions $f: \mathbb{R}\rightarrow\mathbb{R}$ such that $f(f(x))=f(x)$? I think it should be: Constant Value functions the identity function absolute value function $|x|$ But I don't know if this is right or how to show it rigorously. Any suggestions?,"['function-and-relation-composition', 'functional-equations', 'analysis']"
681707,Direct sum of compact operators is compact,"I have that $T_n$ are bounded operators on $H_n$ ($n\geq 1$) and that $\sup ||T_i||<\infty$. Define $T=\oplus T_n$ and $H=\oplus H_n$. I want to show that $T$ is compact iff $T_n$ is compact for all $n$ and $||T_n||\rightarrow 0$. Here is what I have so far: Assume that $T$ is compact, and let $B_n$ be the unit ball in $H_n$. Then we have that $\overline{T_n(B_n)}$ is a closed subset of $\overline{T(B)}$ (the unit ball in $H$), so we get compactness of $T_n$, and to see that $|T_n|\rightarrow 0$ just note that if the limit didn't go to zero, then for some $\epsilon>0$ there is an infinite subsequence $\{n_i\}$ such that $|T_{n_i}|>\epsilon$. Pick $m$ large, and let $h_{n_i}\in H_{n_i}$ be such that $|T_{n_i}(h_{n_i})|\geq \epsilon$ for $i=1,...,m$. Let $h\in H$ be equal to $h_{n_i}$ in the $n_i$-position and $0$ elsewehere. Then, $|h|=\sqrt{m}$ so $|T(h)/\sqrt{m}|\geq \epsilon \sqrt{m}$, so letting $m\rightarrow\infty$ we get that $T$ is unbounded, a contradiction. For the other direction I am a little stuck, I was thinking of using a theorem that says that for a bounded operator $S$, we have that $S$ is compact iff there is a sequence $S_n$ of operators of finite rank such that $|S-S_n|\rightarrow 0$. Maybe call $S_i$ to be $T_1\oplus...\oplus T_i$, and arguing that $S_i$ has finite rank? I can see that $|T-S_n|\rightarrow 0$ for if $h$ is a unit vector, then 
$$|(T-S_n)(h)|=|\sum_{n+1}^\infty T_n(h_n)|\leq \sup_{i\geq n+1}|T_i|\rightarrow 0$$, but I don't know where to use the hypothesis that $\sup |T_n|<\infty$ and how to show that $S_n$ has finite rank.",['functional-analysis']
681732,Homomorphism from C* into itself having ker = R>0,"So I feel like I must be missing an easy example here... I'm trying to find a homomorphism from the multiplicative group $\mathbb{C}-\{0\}$ into itself such that its kernel is the positive reals. Obvious near-misses are $\phi(z)=\phi(r\text{e}^{i\theta})=\theta$ (it's a homomorphism into the $\mathbb{R}$ under addition) and $\psi(z)=\psi(r\text{e}^{i\theta})=r$ (homomorphism into the multiplicative group $\mathbb{R}$, and therefore into the multiplicative group $\mathbb{C}-\{0\}$, but $\text{ker}\psi=\{0\} \ne \mathbb{R} \ge 0$). And I have written down about a dozen other types of function that I thought I would try (e.g. permutations of $\phi(z)=\phi(a+ib)=\text{sgn}(a)(+/*)b$ ...), but I can't seem to find a homomorphism such that $\varphi(z_{0})=\varphi(a+i0)=1$. Any suggestions?",['group-theory']
681734,Calculus 2: Volume of a Solid of Revolution,"From Rogawski ET 2e section 6.3, exercise 22. Find the volume of the solid obtained by rotating the region enclosed by the curves $x=\sqrt{8\sin y}, x=0$ about the $y$ -axis over the interval $1.05\le y\le1.57$ Volume = $\boxed{50.2654}$ I have been stumped on this question for almost two hours now, and I have tried changing the axises from x to y and I then attempted to integrate for dy. However, the answer I have entered and have been repeatedly getting was $16\pi$ or $50.2654$ , and sadly this is not the right answer. If anyone could offer any help I would be tremendously grateful. Thank you for your time.","['calculus', 'integration']"
681737,"Which is greater, $\cos(\cos(1))$ or $\cos(\cos(\cos(1)))$?","What is the simplest way we can find which one of $\cos(\cos(1))$ and $\cos(\cos(\cos(1)))$ [in radians] is greater without using a calculator [pen and paper approach]? I thought of using some inequality relating $\cos(x)$ and $x$, but do not know anything helpful.
We can use basic calculus. Please help.",['trigonometry']
681758,Superior limit of integrals of entire functions,"Let $f$ be an entire function on $\mathbb{C}$. If $f$ is not constant, then I want to prove \begin{equation}
\limsup_{R\to\infty}\int_{\lvert z\rvert=R}\lvert f(z)\rvert\,\lvert dz\rvert=\infty.
\end{equation}
Is it correct or not? How to prove? Note that $|f(z)|$ is not bounded does not imply that the limit of the integral goes to infinity. We need to consider $\mu\big(\big\{\lvert z\rvert=R: f(z)>M\big\}\big)$. Thanks.","['calculus', 'integration', 'complex-analysis', 'analysis']"
681783,Maclaurin Series of $\frac{1}{e^x -1}$,"I want to find the MacLaurin Series for the function $f(x) = \frac{1}{e^x -1}$. But when I compute the first derivative of $f(x)$:
$$
\frac{d}{dx}\frac{1}{e^x -1} = -\frac{e^x}{(e^x-1)^2}
$$ A the point $x=0$, I get an indeterminate expansion:
$$
f'(0)=-\frac{e^0}{(e^0-1)^2}
$$ So how can I compute the series for this function $f(x)$?","['power-series', 'calculus']"
681795,Lower limit topology doesn't arise from metric; proof by contradiction.,"This problem is in Tao's Epsilon of the room, 1.6.11. Show that the half-open topology (Example 1.6.22) is Hausdorff, but does not arise from a metric. (Hint: assume for contradiction that the half-open topology did arise from a metric; then show that for every real number x there exists a rational number q and a positive integer n such that the ball of radius 1/n centred at q has infimum x.) And I proved it in below way; I think this proof has some missing points; Could you verify this proof? Clearly, Let $F$ be half-open interval topology and $x,y \in \mathbb{R}$ and without loss of generality, $x<y$. Then $[x,y)$ and $[y,y+1)$ is clearly open neighborhood of $x,y$ without nonempty intersection. Hence this topology is hausdorff. Now we want to show that it doesn't arise from metric space. For contradiction, suppose it is arise from metric space. By definition of metrizable topology, for any $[x,y) \in F$ can be represented as union of open balls $B(x_0, r) = \{ x \in X : d(x_0 , x) < r\}$. We know that $\mathbb{Q}$ is dense in $\mathbb{R}$, so for any open sets $[a,b) \in F$, $\mathbb{Q} \cap [a,b) \neq \emptyset$. Hence $F$ is separable. With the fact that $\mathbb{Q} \cap [a,b)$ is dense in $F$ and $F$ is metrizable, there exists $x \in \mathbb{R}$ such that $x= \inf\{B(q,\frac{1}{n})\}$. However this is contradiction when $x \notin \mathbb{Q}$ since $ \inf\{B(q,\frac{1}{n})\} = (q-\frac{1}{n},q+\frac{1}{n})$ has infimum as $q-\frac{1}{n}$, which is rational number. Therefore, $F$ is not metrizable.","['general-topology', 'functional-analysis']"
681809,Calculus 2: Cylindrical Shells,"I have essentially tried using the integration formula for finding the volume of two different functions, which is pi times the integration of a to b of (R^2-r^2). In this case I used ((8-x^2)^2-(x^2)^2). However, I have struggled to determine the boundaries of the integral even though it is going to be rotated about the y-axis. Any assistance will be very helpful in my quest to conquer cylindrical shells. Thank you for your time.","['calculus', 'integration']"
681810,one-dimensional inverse square laws,"I suddenly became curious about the following differential equation:
\begin{align*} 
\frac{d^2x}{dt^2} = \frac{k}{x(t)^2} && x(0) = x_0 > 0 && \frac{dx}{dt}(0) = v_0
\end{align*}
the ""1D inverse square law problem"". It's a pretty natural thing to think about. You're a positive distance $x_0$ away from a point particle which is exerting an inverse square force on you. You're moving only radially with respect to that particle with initial velocity $v_0$. What happens? Do all solutions have a ""closed form""? Do some of them? Obviously the behaviour can be quite different depending on the intial conditions. For example, if $k <0$, you're either going to fall into the ""black hole"", or you're going to escape to infinity (or is it even possible to asympotically approach some finite maximum distance from below as $t \to +\infty$?). Edit: I thought of one solution: letting $x(t) = a t^{2/3}$, one gets $\frac{d^2x}{dt^2} = \frac{-2a}{9} t^{-4/3}$ so that $x^2 \cdot  \frac{d^2 x}{dt^2} = \frac{-2a^3}{9}$ whence, taking $a = - \left( \frac{9k}{2} \right)^{1/3}$, we get a solution. We could also shift the time variable to get some more solutions. Edit2: The preceding solution is simple enough that I figured this was the ""minimal escape curve"" i.e. the one where you have precisely enough energy to escape to infinity. Indeed, it's not hard to check this by calculuating the kinetic and potential energy when $t= 1$ i.e. $x=a$. But my friend pointed out a much nicer way to see this! Notice $\frac{dx}{dt} = \frac{2a}{3} t^{-1/3} \to 0$ as $t \to \infty$. Since the veclocity goes to zero, the kinetic energy goes to zero. So, by conservation of energy, this is the minimal escape curve (if you had any spare energy left at $x = \infty$, you wouldn't have slowed to velocity zero). Edit3: In the end I found out how to get the solutions implicitly. See my answer here .","['ordinary-differential-equations', 'physics']"
681825,Abstract definition of Projection,"I'm coming from the programming domain and is searching the mathematical domain for a better understanding of the term Projection sometimes used in programming as well. After reading a lot of Wikipedia sources I think I have finally gotten a good picture of how the domain looks here even if the details are way over my head. The Wikipedia article about Projection (mathematics) gives a good example with the dot and the shadow, but I'm having problem with this statement: The restriction to a subspace of a projection is also called a
  projection, even if the idempotence property is lost. What does ""The restriction"" refer to? Any restriction? Any subset of a projection is an projection in itself? The function that limits the projection set is itself a projection of the projection? In the example with the dot and the shadow, how would this subset of the shadow that implies non-idempotency be achieved?",['elementary-set-theory']
681838,What went wrong? [One-dimensional-inverse-square-law],"Intrigued by this question, one-dimensional inverse square laws ,
I started to try to find an answer and came up with what follows. However, I calculated the derivatives to double check myself, and this does not work. However, it seems straightforward enough that I do not see my mistake. Question: Is there a closed form solution to all $$ \ddot x = \frac{k}{x^2}  \quad (x(0)=x_0), \  \dot{x}(0) = v_0, \ k \in \mathbb{R}.$$ Here is my very naive two cents. Multiplying both sides by $\dot{x}$ and obtain, $$ \dot{x} \ddot{x} =  \frac{k \dot{x}}{x^2}.$$ Integrating both sides,  we obtain
\begin{equation} \tag{1} \frac{ (\dot{x})^2}{2} = -\frac{k}{x} + C. \end{equation} Here it seems natural to re-write this as,
\begin{equation} \tag{2} \dot{x} = \sqrt{2} \sqrt{C - \frac{k}{x}}. \end{equation}
I do not know if that is fully acceptable. However, if it is we then have via integration one more time, or rather letting WolframAlpha integrate one side, $$x(t) = \sqrt{2} x \sqrt{C - \frac{k}{x}} - \frac{ k \log \left( 2 \sqrt{C} x \sqrt{C - \frac{k}{x}} + 2Cx - k \right)}{\sqrt{2C}} + \tilde{C}. $$ Note, that both $C$ and $\tilde{C}$ are determined by initial conditions. Using the given initial conditions and (1), we can solve for $C$, to find that $C = \frac{v_0^2}{2} + \frac{k}{x_0}.$ Similarly we could solve for $\tilde{C}$, but before I did that I discovered my solution was wrong. If you substitute my ""solution"" back into the differential equation, you find (much to my dismay) $$ \ddot{x} = \frac{k}{x^2 \sqrt{2c - \frac{k}{x}}}. $$ Insight on my  mistake and/or a proper way to solve this is greatly appreciated.",['ordinary-differential-equations']
681850,"Which one is greater, $\sin(\sin(\sin(1)))$ or $\cos(\cos(\cos(1)))$","I know I asked a similar question sometime before, and the thing is I need them for a proof. So, please help, I promise this is the last one. What is the simplest way we can find which one of $\sin(\sin(\sin(1)))$ and $\cos(\cos(\cos(1)))$ [in radians] is greater without using a calculator [pen and paper approach]? We can use basic calculus. Any approximation which can be reasonably done with paper and pen is also welcome. EDIT: Background: What I did was that I proved that the minimum of $f=\cos (\cos (\cos (\cos(x))))$ is $\cos(\cos(\cos(1)))$ and maximum of $g=\sin (\sin (\sin (\sin (x))))$ is $\sin(\sin(\sin(1)))$ . So, I just needed this to prove that the maximum of $g$ is smaller than the minimum of $f$ , to show that there is no root of the equation $f=g$ .",['trigonometry']
681861,Random Triangle Inscribed in a Circular Sector,"Lately, I have been thinking about expected area and perimeter of a triangle inscribed in a 'partial' circle or circular sector with radius $r$ and truth be told, I couldn't answer these questions. I hope someone here could give me a good answer. Let's say circular sector as $\frac{2\pi}{n}$ circle for $n\in\mathbb{N}$. My questions are: What is expected area and perimeter of a triangle inscribed in a circular sector with radius $r$, where one of its vertex lies on center of the circular sector and the other vertexs are uniformly, independently, and randomly drawn inside the circular sector. What is expected area and perimeter of a triangle inscribed in a circular sector with radius $r$, where one of its vertex lies on center of the circular sector and the other vertexs are uniformly, independently, and randomly drawn along the circumference/ perimeter circular sector. My approach for the first one is (based on Wolfram MathWorld ):
$$
\begin{align}
\text{E}[A]&=\int_0^r\int_0^r\int_0^{\frac{2\pi}{n}}\int_0^{\theta_1}A(r_1,r_2,\theta_1,\theta_2)\cdot f(r_1,r_2,\theta_1,\theta_2)\,d\theta_1d\theta_2dr_1dr_2\\
&=\int_0^r\int_0^r\int_0^{\frac{2\pi}{n}}\int_0^{\theta_1}\frac{1}{2}\left|\sqrt{r_1r_2}\sin(\theta_1-\theta_2)\right|\cdot \left(\frac{n}{2\pi r}\right)^2\,d\theta_1d\theta_2dr_1dr_2\\
&=\frac{1}{8}\left(\frac{n}{\pi r}\right)^2\int_0^r\int_0^r\int_0^{\frac{2\pi}{n}}\int_0^{\theta_1}\left|\sqrt{r_1r_2}\sin(\theta_1-\theta_2)\right|\,d\theta_1d\theta_2dr_1dr_2,\\
\end{align}
$$
where
$$
\begin{align}
f(r_1,r_2,\theta_1,\theta_2)&=f(r_1)\cdot f(r_2)\cdot f(\theta_1)\cdot f(\theta_2)\\
&=\frac{1}{(r-0)}\cdot\frac{1}{(r-0)}\cdot\frac{1}{\left(\frac{2\pi}{n}-0\right)}\cdot\frac{1}{\left(\frac{2\pi}{n}-0\right)}\\
&=\left(\frac{n}{2\pi r}\right)^2.
\end{align}
$$
But I didn't quite sure about that especially for the integral limits and the pdf $f(r_1,r_2,\theta_1,\theta_2)$. For the expected perimeter and the second one question, I don't have any idea. Could someone here provide me answers about these problems? I'd be grateful for any help you are able to provide.","['geometry', 'geometric-probability', 'probability', 'expectation']"
681862,number of zeros in a disk of a holomorphic function,"Let $f$ be a holomorphic function defined in a beighborhood of $\overline{D(0,R)}$ which has no zeros on $\partial D(0,R)$. Let $N$ be the number of zeros of $f$ inside $D(0,R)$. Prove that
\begin{equation}
\max _{|z|=R}\text{Re}\left(z\frac{f'(z)}{f(z)}\right)\geq N.
\end{equation} How to prove? My idea: I write $f(z)=(z-z_1)\cdots (z-z_N)g(z)$ and calculated out that
\begin{equation}
\int_{|z|=R}\left(z\frac{f'(z)}{f(z)}\right)dz=2\pi i\sum_{k=1}^Nz_k.
\end{equation}
I also know by Argument Principle,
 \begin{equation}
2\pi iN=\int_{|z|=R}\frac{f'(z)}{f(z)}dz.
\end{equation}
But I do not know how to continue...","['calculus', 'complex-analysis']"
681873,Rationalizing the denominator 3,"It is a very difficult question. How can we Rationalizing the denominator?
$$\frac{2^{1/2}}{5+3*(4^{1/3})-7*(2^{1/3})}$$","['fractions', 'radicals', 'arithmetic', 'algebra-precalculus']"
681880,Lower semicontinuous functional,"Consider the space $A=(C^1([0,1],\mathbb R),\|.\|_{L^\infty})$ norm and look at the functional
$$
\mathcal F: A\to\mathbb R_+, f\mapsto \int_{0}^1\left|f'(t)\right|~\mathrm dt.
$$
This functional is not continuous. My question: Is it lower semi-continuous?
And could there be a meaningful extension of $\mathcal F$ to the closure of $A$ under the $L^\infty$-norm?","['measure-theory', 'functional-analysis', 'real-analysis']"
681893,How to integrate $ \int \frac{x}{\sqrt{x^4+10x^2-96x-71}}dx$?,I read about $ \int \dfrac{x}{\sqrt{x^4+10x^2-96x-71}}dx$ on the Wikipedia Risch algorithm page . They gave an answer but I don't understand how they got it.,"['elliptic-integrals', 'calculus', 'integration', 'algorithms']"
681949,Prove that $\sqrt{8}=1+\dfrac34+\dfrac{3\cdot5}{4\cdot8}+\dfrac{3\cdot5\cdot7}{4\cdot8\cdot12}+\ldots$,"Prove that $\sqrt{8}=1+\dfrac34+\dfrac{3\cdot5}{4\cdot8}+\dfrac{3\cdot5\cdot7}{4\cdot8\cdot12}+\ldots$ My work: $\sqrt8=\bigg(1-\dfrac12\bigg)^{-\frac32}$ Now, I suppose there is some ""binomial expansion with rational co-efficients"" or something similar for this, which I do not know. Please help. N.B.: Any other solution is also acceptable, I do not have any restriction regarding the solution.","['binomial-coefficients', 'exponentiation', 'sequences-and-series', 'algebra-precalculus']"
681954,Interval Notation and Infinity- Closed or Open?,"I was wondering whether I should use closed $[-\infty, \infty ]$ or open $(-\infty, \infty )$ notation when representing the infinity sign in interval notation. My teacher says to use open symbols because infinity has no end, but I was also taught that the open signs $(-\infty, \infty )$ are equivelant to $\lt$ and $\gt$ respectively, meaning it would have a value less than infinity...?",['algebra-precalculus']
682003,$2\sin x=3\cot x$ from $0^\circ$ to $360^\circ$,"I used two slightly different approaches to solve this.  First approach gives 2 correct solutions,  second approach gives 4 solutions of which 2 are correct and 2 wrong,  I just cannot figure out why I'm getting 2 extra wrong answers with second approach.","['trigonometry', 'algebra-precalculus']"
682004,Gradient of a function restricted to a submanifold,Let $f$ be a sufficiently smooth function on a manifold $S$. Let $M$ be a sub manifold of $S$. Can someone show how is it then true that $(\text{grad}f|_M)_p$ at a point $p$ (gradient of the mapping $f|_M$ restricted to $M$ at a point $p\in M$) is the orthogonal projection of $(\text{grad}f)_p$ onto $T_p(M)$?,"['riemannian-geometry', 'differential-geometry']"
682025,How prove this limit $\lim\limits_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a$ given two other limits related to $f_n$,"Let $(f_n)$- real sequence such that $$ \lim_{n\rightarrow \infty} \frac{f_{n+1}f_n-f_{n-1}f_{n+2}}{f_{n+1}^2-f_nf_{n+2}}=a+b, $$ and $$ \lim_{n\rightarrow \infty} \frac{f_{n}^2-f_{n-1}f_{n+1}}{f_{n+1}^2-f_nf_{n+2}}=ab   \quad    (|a|<|b|). $$ Prove that:$$\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}}=a $$ I think we must prove $\displaystyle\lim_{n\rightarrow \infty} \frac{f_n}{f_{n+1}} $ exists,and we prove this limit is $a$,But I can't prove this limit exists. My idea: since
$$\lim_{n\to\infty}\dfrac{\dfrac{f_{n}}{f_{n+1}}-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=a+b$$
and 
$$\lim_{n\to\infty}\dfrac{\left(\dfrac{f_{n}}{f_{n+1}}\right)^2-\dfrac{f_{n-1}}{f_{n}}\dfrac{f_{n}}{f_{n+1}}}{1-\dfrac{f_{n}}{f_{n+1}}\dfrac{f_{n+2}}{f_{n+1}}}=ab$$
But I felt this deal is not useful, Other idea: I want to take the 
Fibonacci sequence  to solve this problem, But I can't, Thank you",['limits']
682045,"series from one of Coffey's papers involving digamma, $\gamma$, and binomial","I was looking over one of Coffey's papers where is shows the following series, but with no evaluation. I am just wondering if anyone would know how to evaluate this series: $$\sum_{n=1}^{\infty}(-1)^{n}\left[1+\frac{2}{n+1}\right]\binom{x}{n+2}=\frac{1}{2}(2-x)(x-1)-x\left(1-2\gamma+x-2\psi(x+1)\right)$$ It is related to the derivation of the integral $$\int_{0}^{1}\left(\frac{1}{\ln(x)}+\frac{1}{1-x}\right)^{2}dx$$. It is in his paper entitled, ""certain log integrals, zeta values, and the Stieltjes constant"".",['sequences-and-series']
682061,Flat morphism of finite type,"Let $f:X \rightarrow Y$ be a finite dominant morphism of integral locally noetherian schemes. If f is flat over a point $y$, is it true that we can find an open nbhd V of Y ontaining y such that $f^{-1}(v) \rightarrow V$ is flat? I think this should follow from local freeness, but I am having some problems showing it. Any help would be appreciated (or hints).",['algebraic-geometry']
682067,General and particular solution of differential equation,"1) I need to find, in implicit form, the general solution of the differential equation 
$$\frac{dy}{dx}=\frac{2y^4e^{2x}}{3(e^{2x}+7)^2}$$ 2) I then need to find the corresponding particular solution (in implicit form) that satisfies the initial condition $y=2$ and $x=0$. 3) I then need to find the explicit form of this particular solution. For the first part I came up with $$-\frac{3}{y^4}\frac{ dy}{dx}= \frac{-2e^{2x}}{(e^{2x}+7)^2}$$
which is $$\frac{d}{dx} (y^{-3})=\frac{d}{dx}\left(\frac{1}{e^{2x}+7}\right)$$
then $y^{-3}=\frac{1}{e^{2x}+7}  +c$ For part 2) i got $c=0$ so the particular solution would be $y^{-3}=\frac{1}{e^{2x}+7}.$ However I am confused as to how to do the 3rd part as the answer I got for part 2 seems to be in explicit form. I am not sure if I did the first part correctly even so need quite a bit of help!","['implicit-differentiation', 'ordinary-differential-equations', 'calculus', 'integration']"
682072,How show this $\Delta ABC\sim\Delta A'B'C'$,"Assmue that:if $\Delta ABC$ and $\Delta A'B'C'$ are not  right triangle,and such $$\sin{(2A)}:\sin{(2A')}=\sin{(2B)}:\sin{(2B')}=\sin{(2C)}:\sin{(2C')}$$ show that
$$\Delta ABC\sim\Delta A'B'C'$$ My idea: if 
$$\sin{A}:\sin{A'}=\sin{B}:\sin{B'}=\sin{C}:\sin{C'}$$
then  Law of sines we have
$$a:a'=b:b'=c:c'$$
then $$\Delta ABC\sim\Delta A'B'C'$$ But for this ,$$\sin{(2A)}:\sin{(2A')}=\sin{(2B)}:\sin{(2B')}=\sin{(2C)}:\sin{(2C')}$$
I can't prove it ,Thank you",['trigonometry']
682093,Upper Bound for $p(1-p) $ where $0 \le p \le 1$,"In a book on statistics, I've seen the upper bound of $p(1-p) $ to be $$ p(1-p) \le \frac{1}{4} $$ for $0 \le p \le 1$ which seemed correct. I tried to duplicate this with a simple derivation, $$p(1-p) = C$$ $$p-p^2 = C$$ Taking derivatives of both sides, gives $$1-2p=0$$ $$p=\frac{1}{2}$$ which is the value of $p$ at the maximum, and $1/2*1/2 = 1/4$. I obtained this without using the constraint $0 \le p \le 1$ though. How would I include this constraint? Through Lagrange? Is that overkill?","['inequality', 'quadratics', 'calculus', 'probability']"
682105,canonical bundle of relative Grassmannian,"Let $X$ be a smooth projective variety and $\mathcal{E}$ a locally free sheaf of rank $r$ on it. Now we consider the relative Grassmannian $\mathrm{Grass}_X(l,\mathcal{E})$. What is the canonical bundle of the relative Grassmannian?","['sheaf-theory', 'algebraic-geometry', 'grassmannian']"
682157,Strong Markov property of Brownian motion,"I was able to understand Brownian Motion $\{B(t):t\geq0\}$ has Strong Markov Property i.e. For any stopping time $\tau$, $P(B(t+\tau)\leq y | \mathcal{F}_{\tau})=P(B(t+\tau)\leq y|B(\tau))$ a.s. , $y \in \mathbb{R}$. I want to prove the following assertions $\cdot$ $\hat{B}(t):=B(\tau+t)-B(\tau)$, $t\geq0$ is independent of $\mathcal{F}_{\tau}$ $\cdot$ $0\leq \forall s < \forall t$,  $\hat{B}(t)-\hat{B}(s)$ has the normal distribution of $N(0, t-s)$. $\cdot$ $0\leq \forall s < \forall t$,  $\hat{B}(t)-\hat{B}(s)$ is independent of $\hat{B}(u),\,u\in[0,s]$ How do I use Strong Markov property? Please teach me. My Brownian motion definition is as follows: $\cdot$ $\forall \omega \in \Omega$, $t \mapsto B(t,\omega)$ is continuous. $\cdot$ $0\leq \forall s < \forall t$,  $B(t)-B(s)$ has the normal distribution of $N(0, t-s)$. $\cdot$ $0\leq \forall s < \forall t$,  $B(t)-B(s)$ is independ of $B(u),\,u\in[0,s]$.","['stochastic-processes', 'markov-process', 'probability', 'brownian-motion']"
682202,Calculus 2 Shell Method,"I have tried to use the standard formula for shell method and when I used this formula, the answer that I produced was incorrect. I do not know if it was a mathematical error or a formulaic error. If anybody could help with this problem, it would be greatly appreciated.","['calculus', 'integration']"
682210,Proof of closed form of recursively defined sequence,"Let $f:\mathbb{N} \rightarrow \mathbb{N}$ be defined by $f(1) = 5, f(2) = 13$, and for $n \ge 2, f(n) = 2f(n - 2) + f(n - 1)$. Prove that $f(n) = 3\cdot 2^n + (-1)^n$ for all $n \in N$ So far I've proved that $f(n)$ is true when $n = 1, 2$.
For $k \ge 3$, assume that $p(j)$ is true for all $j \in N, j < k$ Now I want to prove that $p(k)$ is true for all $k \in N$ How would I go about doing that?","['sequences-and-series', 'real-analysis', 'analysis']"
682230,Exercise 2.3 from Hartshorne's algebraic Geometry.,"2.3) A scheme $(X,\mathcal{O}_X)$ is reduced if for every open set $U\subset X$, the ring $\mathcal{O}_(U)$ has no nilpotent element. b) Let $(X,\mathcal{O}_X)$ be a scheme. Let $(\mathcal{O}_X)_{red}$ be the sheaf associated to the prescheaf $U\rightarrow \mathcal{O}_X(U)_{red}$, where for any ring $A$, we denote by $A_{red}$ the quotient of $A$ by its ideal of nilpotent elements. Show that $(X,(\mathcal{O}_X)_{red})$ is a scheme. P.S. I have been able to show that it is a locally ringed space. How to proceed further.","['algebraic-geometry', 'schemes']"
682251,Is it possible that $\sum n a_n^2$ converges but $\sum a_n$ diverges?,"Yesterday I was thinking about a problem, when an interesting question appeared: Does there exist a sequence $a_n \geq 0$ of non-negative real numbers such that $\sum_{n \geq 1} n a_n^2 < \infty$ and $\sum_{n \geq 1} a_n = \infty$? After thinking about it for a while, I couldn't come up with either an example of this or a proof that this is impossible. Is this known (I didn't manage to find it online)? Comments :
One thing I noticed is that, if $p > 1$ is fixed, we may assume without loss of generality that $$\frac{1}{n^p} <a_n < \frac{1}{n} \forall n.$$ The reason the following: Assume there exists $a_n$ that satisfies the above two conditions, and let $S = \left\{n \in \mathbb{N}, n \geq 1 | a_n < \frac{1}{n}\right\}$. Then for every $n \geq 1, n \notin S$, we have $a_n \leq na_n^2$, thus $$\sum_{n \notin S} a_n \leq \sum_{n \geq 1} n a_n^2 < \infty $$ Therefore, since $\sum_{n \geq 1} a_n = \infty$, it follows that $\sum_{n \in S} a_n = \infty$. If we take $b_n = \begin{cases} a_n, &n \in S \\ 0, &n \notin S  \end{cases}$, $b_n$ is a non-negative sequence also satisfying the above two properties. Therefore, we may assume wlog that $a_n < \frac{1}{n} \forall n$. Now, if $p>1$ is fixed we may also assume wlog that $a_n > \frac{1}{n^p}$, because if $a_n$ satisfies the two properties, then $$b_n = \begin{cases} a_n, &a_n > \frac{1}{n^p} \\ \frac{1}{n^{(p+1)/2}}, &a_n \leq \frac{1}{n^p}  \end{cases}$$
also satisfies them.","['absolute-convergence', 'convergence-divergence', 'sequences-and-series', 'limits']"
682271,Concave function divided by a convex function. What is the result?,"Let us say that I have a function $f(x)$ that we know is a concave. And let us also say that we have another function $g(x)$ that is a convex. If I make a new function, $h(x) = \frac{f(x)}{g(x)}$, will this new function be convex or concave? I was wondering is there is an easy way to determine this, instead of deriving it from scratch, (like maybe there is a property or rules relating to operations on con-vex/cave functions or something). Thanks.","['optimization', 'convex-analysis', 'convex-optimization', 'functions']"
682273,How to prove this $A$ is an invertible matrix,"let Symmetric matrix $A=(a_{ij})_{n\times n},n\ge 2$,and 
$$\begin{cases}
a_{jk}=j+k\cdot i&j< k\\
a_{jj}=2j\cdot(i+1)
\end{cases}$$
where $i^2=-1$ show that :$A$ is Invertible matrix My idea: I want to  find this value $\det(A)=?$, or maybe  don't have closed form? when $n=2$,then 
$$A=\begin{bmatrix}
2(i+1)&1+2i\\
1+2i&4(i+1)
\end{bmatrix}$$ so
$$det(A)=8(i+1)^2-(1+2i)^2=16i-(4i^2+4i+1)=3+12i?$$ and  this problem is from china linear algebra problem book ,and this book most of problem is very hard.and this problem is last at this book. Thank you for you help me","['matrices', 'linear-algebra', 'determinant']"
682290,Is $GL(n;\mathbb{C})$ algebraic or not?,"The set of $n\times n$ matrices can be identified with $\mathbb{C}^{n^2}$. 1) Consider the subset $V$ of the affine space $\mathbb{A}^{n^2+1}$ (note plus one) given by
$$V:=\{(x_{ij},t): \det(x_{ij})\cdot t-1=0\}$$
This is an algebraic subset, indeed $V=V(f)$ where $f=\det(x_{ij})\cdot t-1\in\mathbb{C}[x_{ij},t]$. Then consider the map $\phi:GL(n;\mathbb{C})\longrightarrow V$ sending the matrix $(a_{ij})$ to the $(n^2+1)$-tuple $(a_{ij},\frac{1}{\det(a_{ij})})$. This is a bijection, making $GL(n;\mathbb{C})$ also algebraic . 2) $GL(n;\mathbb{C})$ is the complement of the algebraic variety in $\mathbb{C}^{n^2}$ defined by the vanishing of the determinant polynomial, and so is open in the Euclidean topology. Since every affine algebraic variety in $\mathbb{C}^n$ is closed in the Euclidean topology, we can conclude that the general linear group is not an algebraic variety. So what should I argue from the above reasoning? 1) $GL(n;\mathbb{C})$ is algebraic 2) $GL(n;\mathbb{C})$ is not algebraic 3) It is a common fact that some subsets can be algebraic in one affine space and not algebraic in an affine space of different dimension, so both 1) and 2) can be correct.","['matrices', 'algebraic-geometry']"
682306,Motivation for Conjugate transpose of a matrix,I'am currently going through a self study of Linear algebra . I'am finding it difficult to grasp the intuition behind the concept of Conjugate transpose of a matrix .Why take the complex conjugate of each entry after a transpose for a complex matrix ?Can someone help me fill that gap ?,"['matrices', 'linear-algebra', 'intuition']"
682328,Equivalence of categories of vector bundles and locally free sheaves,"Let $X$ be a scheme. It is known that the category $\mbox{Vec}_r(X)$ of vector bundles of rank $r$ on $X$ and the category $\mbox{Loc}_r(X)$ of locally free sheaves of rank $r$ on $X$ are equivalent (The equivalence is given by the functor $F\colon \mbox{Loc}_r(X)\rightarrow \mbox{Vec}_r(X)$ such that $F(\mathcal{E})=\textbf{Spec}(\mbox{Sym}(\check{\mathcal{E}}))$, where $\mbox{Sym}(\mathcal{F})$ is the symmetric algebra of $\mathcal{F}$ and $\textbf{Spec}(.)$ is as defined in Hartshorne p.128. My confusion is the following. In many books it is written that a locally free subsheaf $\mathcal{E}'$ of a locally free sheaf $\mathcal{E}$ does not always correspond to a sub-vector bundle of the vector bundle $F(\mathcal{E})$. However, since the categories are equivalent, the monomorphism $\iota\colon\mathcal{E}'\rightarrow\mathcal{E}$ maps to a monomorphism $F(\iota)\colon F(\mathcal{E}')\rightarrow F(\mathcal{E})$ in $\mbox{Vec}_r(X)$. So, I would expect to get a sub-vector bundle of the vector bundle $F(\mathcal{E})$. A monomorphism in a category is not always an injective map in the usual sense. Is it maybe the point that $F(\iota)$ might not be an injective map, although it is a monomorphism? PS: While I was trying to sort out this confusion, I looked in Hartshorne and was even more confused, because he does not define what a morphism of vector bundles is, although he defines what an isomorphism is (again on page 128). So my second question is what is the definition of a morphism of vector bundles in algebraic geometry?",['algebraic-geometry']
682336,Stone-Weierstrass theorem with $p(1/x)$,"I am trying to prove that for a continuous $f\colon[1,\infty)\rightarrow\mathbb R$ and $f(x)\to a$ as $x\to\infty$ it could be approximated by $g(x)=p(1/x)$ where $p$ is a polynomial.","['polynomials', 'approximation-theory', 'approximation', 'real-analysis', 'complex-analysis']"
682340,Is every convergent limit of an iteration a fixed point as well?,Let $f(x)$ be a function and suppose $\lim_{n \to \infty}f^n(a)=L$ for some $a$ in the domain of $f$. What are the sufficient conditions for $L$ being a fixed point of $f$? Is the continuity of $f$ enough?,"['fixed-point-theorems', 'limits']"
682352,Arc Length from chord and tangent angle,"This is for a rubberband-powered car competition. In the diagram above, I will be given the length from points A to B, as well as angle a . The car will need to go from A to B, positioned at a to follow the curved path from A to B. I know that this describes a unique circle, but I can't figure out how to calculate it. Everything I've found requires the radius of the circle or the sagitta, both of which I will not know. What I need to determine is the length of the arc (so I can know how much tension to put on the rubber bands) and the angle at which to set the car tires to follow this path. Regardless, I think trial and error with the car itself will give me the best chance of success with the competition. However, I'm at a point where the math problem itself is driving me nuts, regardless of how helpful it will be with the physical car.","['trigonometry', 'circles']"
682358,Is it true that $\det(A + I) = \operatorname{trace} (A) + 1$?,"Let $A$ be an $n\times n$ matrix such that $\operatorname{rank} A = 1$ and that $n- 2$ rows of $A$
  are the zero rows. Is it true that $\det(A + I) = \operatorname{trace} (A) + 1$? I already have the answer which can be viewed here . I don't understand how they expanded the determinant and got: $\begin{vmatrix}
a_i+1 & a_k  \\
\beta a_i & \beta a_k+1
\end{vmatrix}$
. I saw another answer where he divided the determinant to two upper diagonals matrices but I didn't get how he did that. An explaination of these two answers and/or another way to solve it would be appreciated. NOTE: we haven't covered eigenvalues.",['linear-algebra']
682399,How to prove a set is open,"The problem is to prove $\{(x,y) \mid 2 \lt x^2 + y^2 \lt 4\}$ is open. So I have an arbitrary circle in this set, with a radius greater than $2$ and less than 4 (as given in the problem) and an arbitrary point $(a,b)$ in this arbitrary circle. I want to show that this arbitrary point is in the set, so I have that $2 \lt |a - x| \lt 4$ and $2 \lt |b - y| \lt 4$ since $2 \lt |a - x| \lt x^2 + y^2 \lt 4$ and $2 \lt |b - y| \lt x^2 + y^2 \lt 4$ (I think?). But after some time algebraically manipulating these inequalities, I cannot come to the conclusion that $2 \lt a \lt 4$ and $2 \lt b \lt 4$ which is what I think we want.","['general-topology', 'calculus']"
682400,How I figure what (theoretical) dice are needed to achieve a certain curve?,"I am posting here by suggestion of RPG.se I want to make character stats that fit within certain bell curves depending on choices during character creation (for example race, gender, class, sprokets amount, whatever). And I am wondering how I figure how I calculate what dice I need to attain the curve I want, I will use for each stat a different way of rolling it, whatever one I find most appropriate... For example I might want a bell curve more accentuated (ie: everyone is almost certainly a the center), or one that is not only accentuated, but skewed to one side or another (for example, a random number between 1 and 100, but that most of the times rolls 70 instead of 50) Or maybe bowl shaped curves, or slopes, or senoidal (dunno what would be the use of that though :P) So, how I can learn more about this? (I suspect the subject is big enough that you cannot fit only in one answer here)",['statistics']
682416,Matrix diagonalization theorems and counterexamples: reference-request.,"I'm looking for exhaustive list of diagonalization theorems and counterexamples in linear algebra. In this question I understand the question of matrix diagonalization very broadly:
suppose we have some group $G$ of matrices, where $G$ is one of $\operatorname{Gl}(n,\mathbb{R})$ - real invertible matrices, $\operatorname{Gl}(n,\mathbb{C})$ - complex invertible matrices, $\operatorname{O}(n)$ - orthogonal matrices, $\operatorname{SO}(n)$ - special orthogonal matrices, $\operatorname{U}(n)$, $\operatorname{SU}(n)$ - unitary and special unitary matrices, generalization of 3-5: matrices with real or complex coefficients, satisfying either of $M C M^{-1} = C$ or $M C M^* = C$ for some fixed matrix $C$. This includes e.g. Lorentz group, group of symplectic matrices. Also we have some set $S$ of matrices we want to diagonalize, where $S$ is one of $M^n(\mathbb{R})$ - all real valued matrices, $M^n(\mathbb{C})$ - all complex-valued matrices, self-adjoint matrices over $\mathbb{R}$ or $\mathbb{C}$, anti-symmetric matrices over $\mathbb{R}$, normal matrices over $\mathbb{R}$ or $\mathbb{C}$. I'm interested in the orbits of action of $G$ on $S$, where action is one of $s \mapsto gsg^{-1}$ ($s$ is interpreted as an endomorphism) and $s\mapsto gsg^*$ ($s$ is interpreted as a bilinear form): what is the ""nicest"" form of matrix, which we can find on any orbit? May be its not exactly diagonal, but Jordan, or made from $2\times 2$ blocks on the diagonal - all of these are examples of ""nice"" forms. The typical questions I would ask are Is it true, that for a real matrix $m$, its diagonizability by $\operatorname{Gl}(n,\mathbb{C})$ (with the obtained diagonal matrix being real) implies the diagonizability by $\operatorname{Gl}(n,\mathbb{R})$ (as an endomorphism)? (Yes) Is every (real) symmetric matrix diagonizable as a bilinear form by Lorentz transformations (in $\mathbb{R}^4$)? Can every real antisymmetric form be transformed by the same Lorentz group to a matrix with blocks of the form $\left(\begin{smallmatrix} 0 & -\lambda \\ \lambda & 0 \end{smallmatrix}\right)$ on the diagonal? Can any real matrix $m$ be transformed to a matrix with $1\times1$ and $2\times 2$ blocks on the diagonal by $SO(n)$? ( No ) What if you are allowed to have Jordan-like form, i.e. whenever two blocks on the diagonal are the same, one can have identity matrix on the diagonal next to it? (Still no by the dimension counting argument) Ok, what is the simplest shape one can hope for? Note that two transformation types coincide for $SO(n)$, $O(n)$, $U(n)$, $SU(n)$, so I don't need to specify one in the question #3. Is there any extensive reference I can look up all these questions and future similar questions?","['matrices', 'linear-algebra', 'reference-request', 'block-matrices', 'diagonalization']"
682476,"Have averages, need variance","I have a large spreadsheet, generated by a colleague, that contains the results of $E$ experiments. For each experiment, he calculated the average of $M$ measurements. I need the calculate the average and variance of all measurements, but due to the complex structure of the spreadsheet, I cannot make the calculation directly in the spreadsheet, nor copy the data to another spreadsheet. How can use the current data, i.e., the averages per experiment and the number of measurements per experiment, to calculate the average and variance of all measurements? Here is what I did so far. Let $x_{ij}$ be measurement $j$ in experiment $i$. Let $a_i$ be the average of experiment $i$: $$a_i = \frac{1}{M} \sum_{j=1}^{M}{ x_{ij} } $$ Let $A$ be the total average: $$A = \frac{1}{EM} \sum_{i=1}^{E}{ \sum_{j=1}^{M}{ x_{ij} } } = \frac{1}{E} \sum_{i=1}^E {a_i}$$ So, I CAN calculate the total average by averaging the averages per experiment $a_i$. However, this trick doesn't seem to work for the variance. I need to calculate $V$: $$V = \frac{1}{EM} \sum_{i=1}^{E}{ \sum_{j=1}^{M}{ (x_{ij} - A)^2 } }$$ But if I calculate the variance of the $a_i$'s, I get: $$V' = \frac{1}{E} \sum_{i=1}^{E}{ (a_{i} - A)^2 }$$ These don't seem similar: $$V - V' 
  = \frac{1}{EM} \sum_{i=1}^{E}{ 
   \sum_{j=1}^{M}{ [(x_{ij} - A)^2  
   - (a_{i} - A)^2]
  } }$$
$$  = \frac{1}{EM} \sum_{i=1}^{E}{ 
   \sum_{j=1}^{M}{ [(x_{ij} + a_{i} - 2A)\cdot(x_{ij}-a_{i})]
  } }
$$ Can I use the existing data to get, at least, an approximation to $V$?","['statistics', 'standard-deviation', 'average']"
682532,Visual explanation of $\pi$ series definition,"Can you visually explain why the following is true: $$
\frac{\pi}{4} = \sum\limits_{k=0}^\infty \frac{(-1)^k}{2k + 1} = \frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\frac{1}{9}\ldots\approx 78.5\%
$$ By visually I mean use a circle and square to help me understand rather than some calculus that I won't understand. I have recently been very intrigued by $\pi /4$ as it seems to make geometry simpler. For example, the area of a circle is $\pi /4$ times the area of its circumscribed square, and the circle's perimeter is $\pi /4$ times the perimeter of its circumscribed square. See more here: Geometry with $\pi /4$","['pi', 'summation', 'sequences-and-series', 'visualization']"
682544,Assistance Solving A Second Order Nonlinear ODE (Converted into a First Order),"I am trying to find the solution to $y''=y+y^2$ I noticed that if I multiplied by $y'$ on both sides and integrated, the result would be $\frac{1}{2}(y')^2=\frac{1}{2}y^2+\frac{1}{3}y^3+c$ I have almost no experience with nonlinear ODEs (they have this as a sort of bonus problem in the book I'm reading on linear ODEs) so I am not sure how to progress from here (I'm satisfied with solution where c=0, but would also like to see it where c is left there if possible)",['ordinary-differential-equations']
682551,Conditioning versus subscripting for a Markov process,"I tried to see if this question had been asked anywhere, but it's a bit hard to set up the search terms properly. I want to confirm my a priori notion that the notations $P_{X_\tau}[G]$ and $P[G|X_\tau]$ do indeed denote different things, where $X = (X_t)_{t \in \mathbb{R}}$ is a Markov process and $\tau$ is a stopping time. I'm having to question myself just because I've recently come across some papers which cast my notion into doubt. To say a bit more, I interpret $P[G|X_\tau]$ in the classical sense as just a r.v. measurable w.r.t. $\sigma(X_\tau)$ and having the property
$$
  \int_H P[G|X_\tau] dP
= \int_H I_G dP
= P[G \cap H]
$$
for all $\sigma(X_\tau)$-measurable sets $H$, and
$P_{X_\tau}[G]$ as the composition of the function 
$$
   f(x) 
:= P[G|X_0 = x]
$$
with the r.v. $X_\tau$. Although $P_{X_\tau}[G]$ certainly satisfies the measurability condition, the integral condition seems not to hold generally: 
$$
  \int_H P_{X_\tau}[G] dP
= \mathbb{E}[I_H (\mathbb{E}[G|X_0 = \cdot]\circ X_\tau)]
= \overset{?}{\dots}
= P[G \cap H].
$$
And of course, when I say that I'd thought they denote different things, I really mean non-equivalent entities. Thanks a hundred!","['probability-theory', 'stochastic-processes', 'notation']"
682617,Prove that $(B - A) \cup (C - A) = (B \cup C) - A$ by showing that each side is a subset of the opposite side.,"A big problem is that I never even know where to start with proofs. Then I panic and get absolutely nowhere. To reiterate: Prove that $(B - A) \cup (C - A) = (B \cup C) - A$ by showing that each side is a subset of the opposite side. $$(B - A) \cup (C - A) = (B \cup C) - A$$ I thought to use the Distributive Laws, but I'm not sure if that would take me in the right direction. I'm also supposed to prove it using membership tables, which I haven't even a basic understanding of. Blegh. Edit: Here's a rough proof I attempted. How off the mark am I? And how can I rewrite it cleaner? \ Let $x \in (B - A) \cup (C - A)$. \ Then $x \in (B - A)$ or $x \in (C - A)$. \ Assume that $x \in (B - A)$. \ Thus $x \in B$ and $x \notin A$. \ Therefore $x \in (B \cup C)$. \ Because $x \notin A$, $x \in (B \cup C) - A$ \ $(B - A) \cup (C - A) \subseteq (B \cup C) - A. \quad \quad$ (Distributive Law) \ If $x \in (B \cup C) - A$, then $x \notin A$, and $x \in B$ or $x \in C$. \ Therefore $x \in (B - A)$ or $x \in (C-A)$ \ $x \in (B - A) \cup (C - A)$ \ $(B \cup C) - A \subseteq (B - A) \cup (C - A)$","['discrete-mathematics', 'elementary-set-theory']"
682619,"Laplace transform of and impulse sampled function using ""frequency"" convolution","This is a long question, but assume we have this: The book uses the frequency convolution theorem to solve this problem. To solve the integral, it uses a contour + residue theorem to solve it. The only problem is you can form this path to the left or right: Now, the book says if you evaluation the path on the left side, then: My first question is why does the integral along TL vanish for this side? For the right side, the book says: My second question is why do we have to consider two cases for this side? It seems as if we can apply the logic for the first case here and assume the integral along TL vanishes. 
Anyway, for the first case the book says: My third question is why do they use the initial value theorem to justify the path integral is zero? Lastly, for the second case: My fourth question is how do they obtain that the path integral along TL for this case is $-1/2*x(0+)$? My thanks to anyone who took the time to read all this! Furthermore if there seems to be a consistent misunderstanding of some basic concept it would be great to get resources to read on this subject. EDIT: Btw, this is taken from a text book called Discrete Time Control Systems by Ogata.","['calculus', 'laplace-transform', 'complex-numbers', 'complex-analysis', 'contour-integration']"
682646,How do I prove an inequality of a strict concave function?,"The function $f(x)$ is strict concave, strict increasing, and $f(0) = 0$. $a, b \in \mathbf R \; and \; a < b$, how can I get that $\frac{a}{b} < \frac{f(a)}{f(b)}$? Thank you! Oh sorry I forgot to mention that  $f:\mathbf R_{+} \rightarrow \mathbf R_{+}$ is continuous; and $a, b$ are positive...",['functions']
682668,Why a closed subscheme give rise to a closed immersion,"G\""ortz and Wedhorn, in their book ``Algebraic Geometry I'' at page 84, give the following definitions.
Let $(X, \mathscr{O}_X)$ be a scheme. (1) A closed subscheme of $X$ is given by a closed subset $Z \subseteq X$ (let $i : Z \longrightarrow X$ be the inclusion)
and a sheaf $\mathscr{O}_Z$ on $Z$, such that $(Z, \mathscr{O}_Z)$ is a scheme, and such that the sheaf $i_*\mathscr{O}_Z$ is isomorphic to $\mathscr{O}_X/\mathscr{J}$
for a sheaf of ideals $\mathscr{J} \subseteq \mathscr{O}_X$. (2) A morphism $i : Z \longrightarrow X$ of schemes is called a closed immersion, if the underlying continuous map is a homeomorphism between $Z$ and a closed subset of $X$,
and the sheaf homomorphism $i^\flat : \mathscr{O}_X \longrightarrow i_*\mathscr{O}_Z$ is surjective. Then they wrote~: if $Z \subseteq X$ is closed subscheme as in (1), then the morphism $(i, i^\flat)$ is a closed immersion. I think that the morphism $i^\flat$ is the composition of $\mathscr{O}_X \longrightarrow \mathscr{O}_X/\mathscr{J}$ and the isomorphism $\mathscr{O}_X/\mathscr{J} \simeq i_*\mathscr{O}_Z$.
But, I don't see why $(i, i^\flat)$ is a morphism of schemes.",['algebraic-geometry']
682692,Show that E|Z| = 0,"Let Z($\omega$) = {$0 \le t \le 1 $ |$W_t(\omega) = 0$} , the zero-set of the Brownian motion path up to 1 . Then Z is a Borel set and has the measure |Z| = $\int_{[0,1]}I_Z(t)dt$ , Show that E|Z| = 0. My attempt is using $E|Z| =  \int_R \int I_Z(t)dtdP(\omega) $ , however , it seems this is not correct.","['probability-theory', 'measure-theory']"
682705,Does the series converge? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 10 years ago . Improve this question Let $a_n$ be a sequence of positive reals satisfying 
$$ a_n\le a_{2n}+a_{2n+1} $$ What can we say about the convergence of $\sum_{n=1}^\infty a_n $ ?","['sequences-and-series', 'real-analysis', 'analysis']"
682707,Is the group of invertible elements in $\mathbb{Z}/8\mathbb{Z}$ isomorphic to $\mathbb{Z}/4\mathbb{Z}$ or to the symmetry group of the rectangle?,"Problem: Is the group $g(8) = \{[1], [3], [5], [7]\}$ isomorphic to $\mathbb{Z}_4$ or to the symmetry group of the rectangle? Attempt: I know that $g(8)$ is isomorphic to $\mathbb{Z}_4$ because I compare the multiplication tables, and they look the same. I know that $g(8)$ is not cyclic, and I know that the symmetry group of the rectangle is not cyclic since every non-identity element has order 2. Does that mean they are not isomorphic? Can anyone help me verify the last part.
Thank you.","['group-isomorphism', 'finite-groups', 'group-theory', 'abstract-algebra']"
682725,Find an invertible matrix $P$ and a diagonal matrix $D$ such that $D=P^{‚àí1}AP$?,"I have a matrix $A=\begin{bmatrix} -5 & -1 & 2\\ 2 & 0 & -2\\ -6 & -1 & 3\end{bmatrix}$, and I need to find an invertible matrix P and a diagonal matrix D such that $D = P^{-1}AP$.
I've found the eigenvalues for the matrix and they are $-3, 1, 0$, so I know the D matrix, but I can't seem to figure out the eigenvectors for the P matrix.",['linear-algebra']
682732,Unitary group and unit circle,"Let $U(n)$ denote the group of complex unitary matrices, let $S^1$ be the unit circle in the complex plane. Then the map $$f:U(n)\to S^1,\quad f(A)=det(A)$$ is a group homomorphism and a submersion. It is straight forward to show that $f$ is a group homomorphism. For the second part, we have the derivative of $f$ as follows: $$df_A(X)=det(A)tr(A^{-1}X)$$ I wonder how can we show that $df_A:T_AU(n)\to T_{f(A)}S^1$ is surjective. Please help","['manifolds', 'lie-groups', 'differential-geometry']"
682750,Given $\omega_i \in \Omega_X(U_i)$ can I find $f\in {\cal O}_X(\cap U_i)$ so that $df = \omega_1 - \omega_2$,"As per this question: Duality in algebraic de Rham cohomology I am trying to show that the map $H^1(X,\Omega_X) \rightarrow H^2_{\text dR}(X/k)$, where $X$ is a projective algebraic curve over an algebraically closed field $k$, characteristic $p \geq 0$. I had already computed the ƒåech cohomology, and was looking for a shortcut. However, it was suggested that ƒåech cohomology is the best way of doing this, and to be honest it still gives rise to an interesting question (at least in my opinion). To be concrete, I will fix the cover I am using: I have a natural projection $\pi\colon X \rightarrow \mathbb P_k^1$, and I let $U_\infty = X \backslash \pi^{-1}(\infty)$, and similarly for $U_0$. Then
$$ H^1(X,\Omega_X) = \frac{\Omega_X(U_0 \cap U_\infty)}{\{\omega_0 - \omega_\infty|\omega_i \in \Omega_X(U_i)\}}$$
and
$$H^2_{\text {dR}}(X) = \frac{\Omega_X(U_0 \cap U_\infty)}{\{df - \omega_\infty -\omega_0|\omega_i \in \Omega(U_i), f\in{\cal O}_X(U_1\cap U_2)\}}.$$
(Note that I have taken the restrictions as given). Hence showing that $H^1(X,\Omega_X) \rightarrow H^2_{\text{dR}}(X)$ is surjective is equivalent to showing that the subspaces we are quotienting by are the same, which is equivalent to saying that given any $f \in \mathcal O_X(U_0 \cap U_\infty)$ we can split $df$ in to two differentials, regular on $U_0$ and $U_\infty$ respectively. So to emphasise, my question is Why, given any $f \in \mathcal O_X(U_0 \cap U_\infty)$, can we split $df$ in to two differentials, regular on $U_0$ and $U_\infty$ respectively?","['algebraic-geometry', 'algebraic-curves']"
682762,Thinking of mathematics in terms of analogs,"I think the way that I've come to think about mathematics is becoming problematic and I'm wondering if I should abandon it. When I study mathematics, I find myself trying to compare the mathematical constructs, operations, entities, and even the basic terminology (which I have come to understand is incredibly elegant, precise, and deliberate) to real world, physical, even visible phenomena. I think under the pretense that the things I do in the mathematical world represent real, fundamental structures in this Universe. For example, the fact that terms can 'cancel' out in an equation has profound implications on the workings of the Universe and should be heeded and studied as such. In other words, I try to make sense of the things I learn in math classes by finding their analogs in the real word, because I assume they must have at least one. Thinking with this frame of mind has led me to appreciate mathematics in a deeply profound and beautiful way, and it's the mindset that I try to share with other people when explaining why mathematics should be studied and why people describe it as beautiful. When I learn something new in a math class, I try to understand and remember that these are not simply tedious equations and formulas that mean nothing and come from nowhere, but that they have real physical and, mostly, intuitive meaning. All that being said, I'm taking my first liner algebra course this term, and it's becoming harder to utilize this mentality, not simply because linear algebra deals with such things as infinite dimensionality which we obviously have no intuitive way of grasping or visualizing, but really just because the class seems more about computation and calculation than concept and philosophy. I worry that my thinking has led me astray, primarily because it becomes hard to focus on just doing sheer, brute force calculation without wondering and worrying about what these constructs really mean. This leads me to fall behind in lecture, take hours longer than is probably necessary on the homework, and add to an overall level of frustration that has been building for some time now because of it, which only clouds my understanding even more. My question is really more of a plea for advice. Should I abandon my way of thinking about mathematics as though it will become increasingly unhelpful in future courses and topics, or is linear algebra truly more about numerical gymnastics than tangible interpretation? Should I focus, currently, on simply learning the algorithms for computation now assuming that the philosophical groundwork will be exposed later on, after which the conceptual work that I'm looking for will yield itself? I'd really appreciate responses from the people that frequent this site. I've been nothing but overwhelmed at the level of quality, thought, and sincerity in the answers I've read here and throughout the conversations I've eavesdropped so far. Also, please direct me to similar questions if you know of any, and help me with the tagging of this question, as it is the first one I've ever asked on this site.","['education', 'advice', 'linear-algebra', 'soft-question']"
682776,$Z\subset \mathbb{P}^n$ irreducible iff its pre-image in $\mathbb{A}^{n+1}-\{0\}$ irreducible,"I'm having trouble with this question (it's a homework question).
If $p:\mathbb{A}^{n+1}-\{0\} \rightarrow \mathbb{P}^n$ is the canonical projection and $Z\subset \mathbb{P}^n$ is closed, then $Z$ is irreducible if and only if $p^{-1}(Z)$ is irreducible. I think I've got one direction ""$\Leftarrow$"": suppose $Z=C_{1}\cup C_{2}$, is a decomposition. Then $C_{i}=Z(I_{i})$ for some homogenous ideals $I_{i}$. Then $p^{-1}(C_{i})$ is the zero locus (in $\mathbb{A}^{n+1}$) of the ideal generated by all homogenous elements in $I_{i}$ without the origin. And this is closed in $\mathbb{A}^{n+1}-\{0\}$. Since $p^{-1}(Z)$ is irreducible then $p^{-1}(Z)=p^{-1}(C_{1})$, say. Then $Z=C_{1}$ is irreducible. For the other implication I get stuck: suppose $p^{-1}(Z)=D_{1}\cup D_{2}$. Since $p$ is surjective $Z=p(D_{1})\cup p(D_{2})$. But I don't think $p(D_{i})$ are necessarily closed. I'm not sure how else to show irreducibility other than start with a decomposition, though. Any help or hint is appreciated. Thanks in advance!",['algebraic-geometry']
682777,Is an abelian group characterized by its localizations?,Let $G$ and $H$ be countable abelian groups. Assume that for every prime number $p$ there is an isomorphism $G\otimes_{\mathbb Z} \mathbb Z[\frac{1}{p}]\cong H\otimes_{\mathbb Z} \mathbb Z[\frac{1}{p}]$. Does it follow that $G$ and $H$ are isomorphic as abelian groups? (Note that this is certainly true for finitely generated groups. Moreover it also holds if all isomorphisms $G\otimes_{\mathbb Z} \mathbb Z[\frac{1}{p}]\cong H\otimes_{\mathbb Z} \mathbb Z[\frac{1}{p}]$ are induced by one fixed group homomorphism $G\to H$. In both cases it is enough to consider two different prime numbers.),"['commutative-algebra', 'abstract-algebra']"
682779,Prime factors of $2^{2^2}+3^{3^3}+5^{5^5}+7^{7^7}$,"Are there any useful restrictions to the prime factors of the number $$2^{2^2}+3^{3^3}+5^{5^5}+7^{7^7}?$$ The two smallest are $6771419$ and $72153167$ , which I found by trial division. The number is small enough for ECM, but this is quite slow for numbers
of this magnitude. Is there anything better than trial division or ECM ?","['prime-factorization', 'tetration', 'number-theory']"
682825,Distribution of time spent above $0$ by a Brownian Bridge.,"Let's say I have a Brownian motion, such that I know its value at time 0 (0) and time T (also 0). I am trying to evaluate the time spent above 0 between time 0 and T. Obviously I know that the average of this value is 1/2, but is there a way to know the full law of that duration? Or are there other known theoretical results for this? I am guessing this is a fairly common problem but I am unable to find references for this. Thanks!","['stochastic-processes', 'simulation', 'probability']"
682842,Scaling the normal distribution?,"I might just be slow (or too drunk), but I'm seeing a conflict in the equations for adding  two normals and scaling a normal. According to page 2 of this , if $X_1 \sim N(\mu_1,\sigma_1^2)$ and $X_2 \sim N(\mu_2,\sigma_2^2)$, then $X_1 + X_2 \sim N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2)$, and for some $c \in \mathbb{R}$, $cX_1 = N(c\mu_1, c^2\sigma_1^2)$. Then for $X \sim N(\mu,\sigma^2)$, we have $X + X = N(\mu + \mu,\sigma^2 + \sigma^2) = N(2\mu,2\sigma^2)$, but also $X + X = 2X = N(2\mu,2^2\sigma^2) = N(2\mu,4\sigma^2)$ ? Ie, the variances disagree. edit: Oh, am I mistaken in saying that $2X = X + X$? Is the former ""rolling"" $X$ just once and doubling it while the latter ""rolls"" twice and adds them?","['statistics', 'normal-distribution', 'probability']"
