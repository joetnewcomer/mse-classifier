question_id,title,body,tags
3763675,Derivation of PCA using Multivariable Calculus,"I've been unable to solve $(12.11)$ to $(12.13)$ for the derivation of $z_{n i}$ and $b_j$ I've tried solving this for the last 3 hours, and I always end up with a completely different expression which includes the terms $z_{n i}$ and $b_j$ . This is taken from Bishop's Pattern Recognition and machine learning book. Specifically I am trying to obtain the term below, i.e step 12.13. $$
 b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}$$ I know I need to use the chain rule, but I am not getting the correct result! $$ J=\frac{1}{N}
 \sum_{n=1}^{N}\left\|\mathbf{x}_{n}-\tilde{\mathbf{x}}_{n}\right\|^{2}
 \tag{12.11} $$ Consider first of all the minimization with respect to
the quantities $\left\{z_{n i}\right\} .$ Substituting for $\tilde{\mathbf{x}}_{n},$ setting the derivative with respect to $z_{n
 j}$ to zero, and making use of the orthonormality conditions, we
obtain $$ z_{n j}=\mathbf{x}_{n}^{\mathrm{T}} \mathbf{u}_{j}
 \tag{12.12}$$ where $j=1, \ldots, M .$ Similarly, setting the
derivative of $J$ with respect to $b_{i}$ to zero, and again making
use of the orthonormality relations, gives $$
 b_{j}=\overline{\mathbf{x}}^{\mathrm{T}} \mathbf{u}_{j} \tag{12.13}$$","['machine-learning', 'multivariable-calculus', 'linear-algebra', 'principal-component-analysis']"
3763681,Any linear operator $T$ satisfies $\lvert \lvert T x \rvert \rvert = \lvert \lvert T \rvert \rvert \cdot \lvert \lvert x \rvert \rvert$,"Let $(X,\lvert \lvert \cdot \rvert \rvert_{X})$ and $(Y,\lvert \lvert \cdot \rvert \rvert _{Y})$ and $X$ be finite dimensional, then show that any linear operator $T$ satisfies $\lvert \lvert T x \rvert \rvert = \lvert \lvert T \rvert \rvert \cdot \lvert \lvert x \rvert \rvert$ for some $x \neq 0$ . My idea: I have shown that $T$ is bounded, now let us assume that for any $x \neq 0$ we have $\lvert \lvert Tx\rvert \rvert < \lvert \lvert T\rvert \rvert \cdot \lvert \lvert x \rvert \rvert$ , then: we have $\lvert \lvert T(\frac{x}{\lvert \lvert x \rvert \rvert})\rvert \rvert < \lvert \lvert T \rvert \rvert$ for all $x \neq 0$ . But at the same time $\{ \frac{x}{\lvert \lvert x \rvert \rvert}: x \neq 0\}=\partial B_{1}(0)$ and $\lvert \lvert T \rvert \rvert=\sup\limits_{ x \in \partial B_{1}(0)}\lvert \lvert Tx\rvert \rvert_{Y}$ So we can construct a sequence $(x_{n})_{n}\subseteq \partial B_{1}(0)$ such that $\lim\limits_{n \to \infty}\lvert \lvert Tx_{n}\rvert \rvert _{Y}=\lvert \lvert T\rvert \rvert$ Since $\dim X<\infty$ , we know that the closed unit ball $\overline{B_{1}^{X}(0)}$ is compact and hence $\partial B_{1}(0)$ is compact since it is closed and a subset. Thus there exists a convergent subsequence $(x_{n(k)})_{k \in \mathbb N}$ of $(x_{n})_{n \in \mathbb N}$ and $x \in \partial B_{1}(0)$ such that $\lim\limits_{k \to \infty}x_{n(k)}=x$ . By the boundedness of $T$ and thus the continuity, it must follow that $\lim\limits_{k \to \infty}\lvert \lvert Tx_{n(k)}\rvert\rvert_{Y}=\lvert\lvert Tx\rvert\rvert_{Y}$ and thus $$\lvert \lvert Tx\rvert \rvert _{Y}=\lim\limits_{k \to \infty}\lvert \lvert Tx_{n(k)}\rvert \rvert _{Y}=\lim\limits_{n \to \infty}\lvert \lvert Tx_{n}\rvert \rvert _{Y}=\lvert \lvert T\rvert \rvert$$ which is a contradiction since $x \in \partial B_{1}(0)$","['solution-verification', 'sequences-and-series', 'functional-analysis', 'real-analysis']"
3763725,Show that $\lim\limits_{t\to\infty}\frac{1-F(\eta t)}{1-F(t)}=0\implies E[X^m]<\infty$,"Let $X\ge0$ be a random variable with distribution function $F(t)$ such that $F(t)<1$ for all $t\in\mathbb{R}$ and, for some $\eta\in (1,\infty)$ , \begin{align*}
\lim_{t\to\infty}\frac{1-F(\eta t)}{1-F(t)}=0.
\end{align*} Show that $E[X^m]<\infty$ for any $m\in(0,\infty)$ . I have shown that $EX<\infty$ , as done below: Note that our assumption is that: $\lim\limits_{t\to\infty}\frac{P(X>\eta t)}{P(X>t)}\stackrel{(*)}{=}0$ , we first show that $(*)\implies E[X]<\infty$ . By $(*)$ $\exists$ an $s\in\mathbb{N}$ such that: \begin{align*}
\frac{P(X>\eta t)}{P(X>t)}<\frac{1}{2\eta}\,\,\text{for all $t\ge s$}
\end{align*} Now note that: \begin{align*}
\int_{s\eta^n}^{s\eta^{n+1}}P(X>t)\,dt&\le P(X>s\eta^n)(s\eta^{n+1}-s\eta^{n})\\
&=s\eta^n(\eta-1)P(X>s\eta^n)\\
&=s\eta^n(\eta-1)\frac{P(X>s\eta^{n})}{P(X>s\eta^{n-1})}\frac{P(X>s\eta^{n-1})}{P(X>s\eta^{n-2})}...\frac{P(X>s\eta)}{P(X>s)}P(X>s)\\
&\le s\eta^n(\eta-1)\frac{1}{2\eta}\frac{1}{2\eta}...\frac{1}{2\eta}P(X>s)\,\,\text{since:}\\
&\text{$\bigg|\frac{s\eta^{n-k}}{s\eta^{n-k-1}}\bigg|=\eta$ and $s\eta^{n-k}=\eta(s\eta^{n-k-1})$ where $s\eta^{n-k-1}\ge s$ as $\eta>1$}\\
&\le s(\eta-1)\eta^n\frac{1}{(2\eta)^n}\quad\text{as $P(X>s)\le1$}\\
&=\frac{s(\eta-1)}{2^n}\\
\end{align*} Thus, \begin{align*}
&\int_{s\eta}^{\infty}P(X>t)\,dt=\sum_{n\ge1}\int_{s\eta^n}^{s\eta^{n+1}}P(X>t)\,dt\le s(\eta-1)\sum_{n\ge1}\frac{1}{2^n}=s(\eta-1)<\infty
\end{align*} Hence, \begin{align*}
EX=\int_{0}^{\infty}P(X>t)\,dt=\int_{0}^{s\eta}P(X>t)\,dt+\int_{s\eta}^{\infty}P(X>t)\,dt\le s\eta+s(\eta-1)<\infty\,\,\text{as we wished to show}.
\end{align*} However, I cannot figure out how to extend this result to $EX^m<\infty$ , any help here would be greatly appreciated. My thoughts on an extension are as follows, if we can show that \begin{align*}
\lim_{t\to\infty}\frac{P(X^m>\eta t)}{P(X^m>t)}=0
\end{align*} Then replacing $X$ with $X^m$ in our above argument finishes the proof, but I cannot show that this limit is zero. Here is what I have \begin{align*}
\lim_{t\to\infty}\frac{P(X^m>\eta t)}{P(X^m>t)}&=\lim_{t\to\infty}\frac{P(X>(\eta t)^{1/m})}{P(X>t^{1/m})}\\
&=\lim_{t\to\infty}\frac{P(X>{\eta}^{1/m} t^{1/m})}{P(X>t^{1/m})}\\
&=\lim_{z\to\infty}\frac{P(X>{\eta}^{1/m}\cdot z)}{P(X>z)}\quad\text{since $z=t^{1/m}\to\infty$ as $t\to\infty$}
\end{align*} But now $\eta>1$ implies that $\eta^{1/m}<\eta$ and so \begin{align}
X>\eta z\implies X>\eta^{1/m}z
\end{align} And so $P(X>\eta z)\le P(X>\eta^{1/m}z)$ , hence \begin{align*}
\lim_{t\to\infty}\frac{P(X^m>\eta t)}{P(X^m>t)}=\lim_{z\to\infty}\frac{P(X>{\eta}^{1/m}\cdot z)}{P(X>z)}\ge \lim_{z\to\infty}\frac{P(X>\eta z)}{P(X>z)}=0
\end{align*} and so the inequality is going to the wrong way.","['expected-value', 'self-learning', 'probability-theory', 'probability']"
3763730,Regular singular point of non-linear ODE,"Consider a system of ordinary differential equations of the form $$
\dot{x}(t) + \frac{1}{t}Ax(t) = B(x(t))
$$ where $x(t) \in \mathbb{R}^n$ , $A \in \mathrm{Mat}_{n\times n}(\mathbb{R})$ is a constant matrix, and $B: \mathbb{R}^n \to \mathbb{R}^n$ is homogeneous of degree $2$ , i.e. $B(\lambda x) = \lambda^2 B(x)$ for $\lambda \in \mathbb{R}$ . What is known about existence of solutions near $t = 0$ ? If it were not for the quadratic term, the point $t = 0$ would be a regular singular point of the ODE and then we could use the Frobenius method. But in all the books I have, regular singular points are only discussed for linear systems.","['matrices', 'dynamical-systems', 'ordinary-differential-equations', 'real-analysis']"
3763737,calculate the Integer solutions of an equation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question How does anyone calculate the ""integer solutions"" of an equation in this example: $$
x(x+1) + 62 = y^2
$$ https://www.wolframalpha.com/input/?i=x*%28x%2B1%29%2B62%3Dy%5E2 here the Integer solutions are : $x = -62$ , $y= 62$ $x = -2$ , $y= 8$ $x = 1$ , $y= 8$ $x = 61$ , $y= 62$ How do they get $8$ ?","['elementary-number-theory', 'algebra-precalculus', 'diophantine-equations']"
3763744,Is there a generalization of the helix from $\mathbb{R}^3$ to $\mathbb{R}^4$?,"The helix is a curve $x(t) \in \mathbb{R}^3$ defined by: $$
x(t) = \begin{bmatrix}
\sin(t) \\
\cos(t) \\
t
\end{bmatrix}
$$ and it takes the classic shape: Does this have a natural extension from $\mathbb{R}^3$ to $\mathbb{R}^4$ ?  (Or even $\mathbb{R}^n$ ?) What I've tried so far: The classic $\mathbb{R}^3$ helix curve above has two nice properties: $x(t)$ has constant distance from the axis of propagation $\hat{e}_3$ , where $\hat{e}_3 = \begin{bmatrix} 0 \\ 0 \\ 1 \end{bmatrix}$ $x(t)$ has constant angular velocity when projected onto the plane normal to $\hat{e}_3$ .  i.e. the vector $(x_1(t), x_2(t))$ has polar coordinates $(r, \theta) = (1, t)$ , so $\dot{\theta} \equiv 1$ . The classic helix can be viewed as a parametric walk of a circle in $\mathbb{R}^2$ , with the parameter $t$ added as the third dimension.  A natural extension to a helix in $\mathbb{R}^n$ would be a parametric walk of a curve on a hypersphere in $\mathbb{R}^{n-1}$ , with parameter $t$ added as the nth dimension.  So for $\mathbb{R}^4$ , one could choose a spherical spiral to walk the sphere in $\mathbb{R}^3$ , and use parameter t as the 4th dimension: $$
x(t) = \begin{bmatrix}
\sin(t) \cos(ct) \\
\sin(t) \sin(ct) \\
\cos(t) \\
t
\end{bmatrix}
$$ The first three components are rendered on wikipedia as: This construction matches the two properties I listed: $x(t)$ has constant distance from the axis of propagation $\hat{e}_4$ , where $\hat{e}_4 = \begin{bmatrix} 0 \\ 0 \\ 0 \\ 1 \end{bmatrix}$ When $c=1$ , $x(t)$ has constant angular velocity when projected onto the 3-plane normal to $\hat{e}_4$ .  i.e. the vector $(x_1(t), x_2(t), x_3(t))$ has spherical coordinates $(r, \theta, \phi) = (1, t, t)$ , so $\dot{\theta} = \dot{\phi} \equiv 1$ . It's technically a direct extension of the $\mathbb{R}^3$ helix, since $c=0$ induces an identical curve (up to a projection.)  But it still feels a little arbitrary, and the closed form will be quite ugly in higher dimensions. Is there a generally accepted extension of the classical circular helix in $\mathbb{R}^3$ to $\mathbb{R}^4$ ?  (Or even $\mathbb{R}^n$ ?)  And do its properties or construction at all resemble the above? After some research, I've learned that there are interesting generalizations of helices in $\mathbb{R}^n$ , defined in terms of derivative constraints, Frenet frames, etc. such that even polynomial curves can behave as helices. [ Altunkaya and Kula 2018 ].  However, that's much more general than I'm seeking, since those are aperiodic, and may have unbounded distance from the axis of propagation.  But the existence of such work is promising - I just don't know how to search this space well.","['curves', 'differential-geometry']"
3763765,"Properties of the linear operator $T(f)=\chi_{[0,\frac{1}{2}]}f(2x)$ where $T:L^1[0,1] \to L^1[0,1]$","Properties of the linear operator $T(f)=\chi_{[0,\frac{1}{2}]}f(2x)$ where $T:L^1[0,1] \to L^1[0,1]$ I am supposed to find the operator norm, check whether it is one to one or onto and if it is compact. I think that the norm is $1/2$ as $\int_{0}^{1/2}f(2x)=1/2\int_{0}^{1}f(x)$ . It is $1-1$ as $T(f)=0$ iff $f(x)=0$ for all $x$ . It is clearly not surjective. I also think that it cannot be compact. I think that the range is closed and not finite dimensional and so $T$ is not compact. IS that true?It is closed beceasue passing into subsequence we get a function that is $0$ on $[1/2,1]$","['compact-operators', 'functional-analysis']"
3763783,${f(x) = x^2 + x + 1}$ is continuous on ${\mathbb{R}}$,I need help finding the limit of the function ${f(x) = x^2 + x + 1}$ as ${x\rightarrow c}$ for any ${c \in \mathbb{R}}$ . So far I have used the limit definition and triangle inequality up to this point. $${|(x^2+x+1)-(c^2+c+1)|=|x^2-c^2+x-c|\leq|x^2-c^2|+|x-c|}$$ I do not know how to handle the $|x^2-c^2|$ part within the limit definition.,"['epsilon-delta', 'limits', 'continuity', 'real-analysis']"
3763798,Writing out a (set) relation in terms of predicate logic,"Revisiting some elementary binary relations on $\mathbb{R}$ while studying predicate logic (at a graduate level), I wanted to receive some criticism on the following thoughts I have. Lets take a look at this binary relation (that is a function) on $\mathbb{R}$ such that $x,y \in \mathbb{R}$ : $f :=$ { $(x,y)| y = x^{2}$ }. Since we know that a relation is just a predicate of two variables (or ""subjects"", if you will) that produces a truth value, couldn't we write the  relation $f$ as the following  assuming I was a student who had never taken set theory (but knew that $x$ and $y$ are real numbers in the proposition $f(x,y)$ )? That is, knowing that “is the square of” is the predicate $f$ where $x$ and $y$ are the subjects: $f(x,y) :=$ $y$ is the square of $x$ Since a course in the logic of propositions and predicates is usually taken before set theory at my university, I feel like this would occur naturally as an example. Of course, reoccurring later in a set theory course as $f :=$ { $(x,y)| y = x^{2}$ } where the proposition $f(x,y)$ is rewritten (set-theoretically) as $y = f(x):= x^{2}$ .","['predicate-logic', 'logic', 'relations', 'solution-verification', 'elementary-set-theory']"
3763803,Bayes classifier for binary decision problem with Reject option,"Consider the decision problem where three decisions are valid: $0, 1$ and a third option $reject$ . An optimal rule has the lowest probability of error at a fixed ""reject"" probability. More formally, given a context $X \in \mathcal{X}$ and a target $Y \in \{0, 1\}$ , a decision rule $g: \mathcal{X} \rightarrow \{0, 1, reject\}$ is optimal if for any decision rule $g'$ with $\mathbb{P}(g'(X) = reject) \leq \mathbb{P}(g(X) = reject)$ we have that the error probability of $g'$ is at least as large as that of $g$ : $$\mathbb{P}(g'(X) \neq Y | g'(X) \neq reject) \geq \mathbb{P}(g(X) \neq Y | g(X) \neq reject)$$ Show that the following decision function is optimal for $c \in (0, 1/2)$ : $$
g_c(X) = 
\begin{cases}
1,  & \eta(X) > 1/2 + c \\
0, & \eta(X) \leq 1/2 - c \\
reject, & \text{otherwise}
\end{cases}
$$ where $\eta(x) = \mathbb{P}(Y = 1 | X=x)$ . What I've tried I've tried showing this in the same way as showing that $\eta(x)$ is the Bayes optimal predictor in the case without the rejection option: Using the identity: $$
\mathbb{P}(g(X) \neq Y | X = x) = 1- (\mathbb{I}(g(X) = 1)\eta(x) + \mathbb{I}(g(X) = 0)(1-\eta(x)))
$$ We then have: $$
\begin{align}
&\mathbb{P}(g(X) \neq Y | X = x) - \mathbb{P}(g_c(X) \neq Y | X = x)\\
& = (\mathbb{I}[g_c(X) = 1]\eta(x) + \mathbb{I}[g_c(X) = 0](1-\eta(x))) -(\mathbb{I}[g(X) = 1]\eta(x) + \mathbb{I}[g(X) = 0](1-\eta(x))) \\
&= (\eta(x)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1)] + (1-\eta(x))(\mathbb{I}[g_c(X) = 0] - \mathbb{I}[g(X) = 0])) \\
&= (2\eta(x) -1)(\mathbb{I}[g_c(X) = 1] - \mathbb{I}[g(X) = 1]) + (1-\eta(x))(\mathbb{I}[g_c(X) = reject] - \mathbb{I}[g(X) = reject])
\end{align}
$$ The last equality follows from considering the complement of the events in the indicators in the second term. In the case of the Bayes optimal rule, the first term is always nonnegative. In our case this isn't clear to me since the decision boundary is no longer $1/2$ . We also have an extra term involving the rejection probability. I have tried a few other manipulations but I think there is a simpler way that I am missing.","['machine-learning', 'statistics', 'probability']"
3763805,Let $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ an application $f(x)=|x|^2x$,"Let $f: \mathbb{R}^n\rightarrow \mathbb{R}^n$ an application defined by the rule $f(x)=|x|^2x$ , where $|\cdot|$ and the euclidean norm, $| x | = \langle x,x \rangle.~$ Show that $f$ is of class $C^1$ (i) and carry a unit ball $B(0,1)$ over herself injectively. (ii) Show, ask, that your inverse is not differentiable at the origin (iii). (i) To prove that $f$ $\in$ $C^\infty$ , I did it like this: I know that the standard function is class $C^\infty$ and also the identity function is class $C^\infty$ . Logo $f$ is a product of functions that are class $C^\infty$ So $f$ is class $C^\infty$ . (ii) To show that $f$ takes an open ball of radius 1 centered injecting itself on the origin ... I don't know how to prove it ... looking at the function of the impression of actually being a normalized ball. Any help in that part?
How do I show that you have a ball in a ball. Did you understand the reasoning of this. (iii) Finally, prove that $f^{-1}$ is not differentiable in origin. I thought of assuming that $f^{-1}$ is differentiable in origin, I thought of using the trick $f^{-1} \circ f = x$ , so if we derive, $Df^{-1}(f(x)) Df(x)=1$ . Then I don't know what I can do.","['multivariable-calculus', 'derivatives', 'real-analysis']"
3763812,Prove the equivalence between some set $C$ be closed under some operation and de restriction of this operation be a operation in $C$,"The question is stated as: Consider a binary operation on a set $A$ $$F:A \times A \rightarrow A$$ Assume $C \subseteq A$ . By the restriction of the operaction $F$ to $C$ , we mean the function $G$ such that $Dom(G)=C \times C$ and $G(x,y) = F(x,y)$ for all $x$ and $y$ in $C$ . a. Prove that the restriction of $G$ of $F$ to $C$ is an operation on $C$ if and only if $C$ is closed under $F$ . Here is my atempt: First assume $C$ is closed under $F$ , thus we have $(\forall x)(\forall y)([x \in C \land y \in C] \Rightarrow F(x,y) \in C)$ As $F(x,y) = G(x,y)$ for any $x$ and $y$ and $Dom(G) = C \times C \subseteq Dom(F)$ , we have that $(\forall x)(\forall y)([x \in C \land y \in C] \Rightarrow G(x,y) \in C)$ , and therefore we have $G:C \times C \rightarrow C$ , then $G$ is a binary operation on $C$ . On other hand assume that the restriction $G$ is a binary opeation on $C$ , thus we have $G:C \times C \rightarrow C$ We have $C \subseteq A$ and $F(x,y) = G(x,y)$ for any $x$ and $y$ , thus $(\forall x)(\forall y)([x \in C] \land y \in C] \Rightarrow G(x,y)=F(x,y) \in C)$ , then $C$ is closed under $F$ . As both sides implies in eachother I conclude that he restriction of $G$ of $F$ to $C$ is an operation on $C$ if and only if $C$ is closed under $C$ . This atempt is correct? If not, how it should be proved?","['elementary-set-theory', 'functions', 'solution-verification']"
3763813,"What is the difference between the derivative (the Jacobian), and the differential?","Let $f:M \subset \mathbb{R}^2 \rightarrow N \subset \mathbb{R}^3$ . The function $f$ is a vector function. Its differential $\mathrm{d}f \in \mathbb{R}^3$ represents the infinitesimal change in the function, where by $\mathrm{d}f$ , I mean $\mathrm{d}f(x)$ . Its Jacobian (matrix) $J \in \mathbb{R}^{3 \times 2}$ maps vectors between tangent spaces $T_x M$ and $T_{f(x)} N$ . The relation between the two is $\mathrm{d}f = J dx$ , where $\mathrm{d}x \in \mathbb{R}^2$ . However, if $f$ is considered a ""mapping"", then is the differential of the mapping $\mathrm{d}f$ equal to the Jacobian $J$ ? From some of the answers, it seems that I took some things for granted (common knowledge or agreed by all). Moreover, there seems to be a confusion between differential, derivative, and their notation. So first, let's agree that the differential (total derivative) and the derivative (Jacobian) are not the same thing: Difference between differential and derivative https://en.wikipedia.org/wiki/Differential_of_a_function Next, as per Wikipedia , let's agree on notation. Each of $f'(x)$ , $D f(x)$ , and $\frac{\mathrm{d} f}{\mathrm{d} x}$ , and $J$ refers to the derivative.  The notation $\mathrm{d}f$ is reserved to denote the differential. Now, back to my question. The derivative of $f$ is the Jacobian matrix $f'(x)=Df=J \in \mathbb{R}^{3 \times 2}$ . The differential of $f$ is the 3D vector $\mathrm{d}f = J \mathrm{d}x$ . For some reason, there are people who confusingly use the term ""differential of a mapping"" to refer to the derivative, as if they don't distinguish between the derivative and the differential: https://en.wikipedia.org/wiki/Pushforward_(differential)#The_differential_of_a_smooth_map Differential of a Map My question is: What's up with that, and what am I missing? Why is that important: for a long time, I wasn't clear about what exactly the differential is. It became an issue when I used matrix calculus to calculate the Hessian of a matrix function. The book Matrix Differential Calculus with Applications in Statistics and Econometrics cleared it all up for me. It properly and distinctively defines the Jacobian, gradient, Hessian, derivative, and differential. The distinction between the Jacobian and differential is crucial for the matrix function differentiation process and the identification of the Jacobian (e.g. the first identification table in the book). At this point, I am mildly annoyed (with myself) that previously I wrote things (which are too late to fix now) and blindly (relying on previous work) used the term ""differential of a mapping"". So, currently, I either look for some justification for this misnomer or otherwise suggest to the community to reconsider it. I tried to track down the culprit for this ""weird fashion"", and I went as far as the differential geometry bible. Looking at do Carmo , definition 1 in chapter 2 appendix, pg. 128 (pg. 127 in the first edition), the definition of $dF_p$ is fine (grammar aside): it's a linear map that is associated with each point in the domain. But then, in example 10 (pg. 130), he uses the same notation to denote both Jacobian and differential. (This is probably what Ulrich meant by almost the same thing.)
More specifically, he ""applies it twice"": once to get the Jacobian and once to get the differential. He uses $df(\cdot)$ to denote the Jacobian, a non-linear map into a matrix target, and $df_{(\cdot)}(\cdot)$ to denote the differential, a linear map into a vector target, and he calls both a differential. Another point why I find it confusing is that for me the Jacobian is a matrix of partial derivatives and the differential is an operator. For example, to differentiate the matrix function $f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}$ : $f(X) = tr AX$ I would use the differential operator: $df(X; dX) = tr AdX$ And from the Jacobian identification table (Magnus19), I'll get: $Df(X) = A'$ Note that the differential isn't a trivial linear map anymore. It also leads to another point. The differential has a linear approximation meaning. Basically, it denotes the change in the function. If it's a scalar value function, the change would be scalar, and thus the differential (would map to a scalar). If the domain is matrices, then the Jacobian is a matrix (a non-linear map from matrices to matrices). I definitely would find it confusing if someone would treat them the same. Let's do another example, $f:\mathbb{R}^{2 \times 2} \rightarrow \mathbb{R}^{2 \times 2}$ : $f(X) = AX$ Using the differential operator: $df(X; dX) = AdX$ $vec\ df(X; dX) = (I_2 \otimes A) vec\ dX$ From the Jacobian identification table: $Df(X) = I_2 \otimes A$ In this case, I'm not sure I'd consider the differential $df$ and Jacobian $Df$ almost the same thing (I'm not so good with tensors). This is the root of my issue. It's not always a simple matrix multiplication, and one needs to be mindful about the difference between the differential and Jacobian. Not to mention the second order differential and the Hessian identification. I corresponded with a couple of Caltech guys who settled it for me, and I can live with that. To paraphrase: Math is a living language like any other, it evolves and changes. As long as we clearly define the terms in the context, there shouldn't be a problem--call it whatever you want.","['calculus', 'differential-geometry']"
3763851,Existence of a vector field on a Riemannian manifold,"Let $(M,g,\nabla)$ be a Riemannian manifold with metric $g$ and Riemannian connection $\nabla$ . Let $f$ be a positive function on $M$ . Does there exist a vector field $Z$ such $$\frac{1}{f}\nabla_X\operatorname{grad} f=\nabla_XZ$$ I am looking for a complete existence result or non-existence result. I tried $Z=\operatorname{grad}\operatorname{log} f$ and I got $$\nabla_XZ=\nabla_X (\operatorname{grad}  \operatorname{log} f)=\nabla_X (\frac1f\operatorname{grad}  f)=\frac1f\nabla_X (\operatorname{grad}  f)-X(f) \frac1{f^2}(\operatorname{grad}  f)$$ Thanks in advance.","['riemannian-geometry', 'differential-geometry']"
3763852,How many five letter words can be made from the letters in SUCCESSFUL?,"Question from this video: https://www.youtube.com/watch?v=nU2NrXOCdwk Actually the word there was SUCCESSES, I will just change it to SUCCESSFUL. So let me partially do the solution because I don't have a problem doing exactly the entire thing. From my understanding, you have to create integer partitions and those are represented by the cases. SUCCESSFUL Letter Count S - 3 U - 2 C - 2 E - 1 F - 1 L - 1 All - 10 Integer Partition Cases Case 1: {1, 1, 1, 1, 1} Case 2: {2, 1, 1, 1} Case 3: {2, 2, 1} Case 4: {3, 1, 1} Case 5: {3, 2} To give an explanation, the integer partitions can be interpreted as Case 1 has all letters different from each other Case 2 has 2 letters identical to each other while the rest are all different Remember that each set has to be taken from a pool of more letters. In all honesty, I only know how to do well probably Case 1 and 2 and even then I'm not sure. Case 1: {1, 1, 1, 1, 1} - 5! Case 2: {2, 1, 1, 1} - (5C2 * 3C1) * 3! Case 1 is pretty straight forward.
For case 2, I have used 5C2 to take two positions. Because those position will be occupied by identical letters, the order does not really matter. But I had to multiply by 3C1 because there are three possible letters.
Yeah the tricky part is that you don't consider just letters with two repetitions but those with at least 2, so S is included.
I'm at a loss on how to do Case 3 to 5. I have an idea but still not sure about it. If anyone can comment on my solution if anything is wrong or suggest a better way then they may feel free to do so. Thank you.",['combinatorics']
3763854,Does the probability of guessing a perfect multiple choice test score increase by taking the test multiple times?,"Me and my brother are currently trying to solve a debate. The question is: Does the probability of a student guessing a perfect score on a multiple choice test go up if the student takes the test multiple times over a set period of time. This started by us discussing the probability of guessing every question on the SAT, resulting in a perfect score. He theorizes that by taking the SAT the max number of times he can (48) before a set date (graduation), his chances of guessing a perfect score go up, due to the fact that he has taken it 48 times. We both recognize that each test attempt is an independent event and doesn't affect the outcome of another, so each individual test attempt still has the same probability of guessing a perfect score. However, he believes that by increasing the number of trials, the  chance of getting a perfect score is greater. I say that because each event is independent, each time is the same exact probability and does not change the more that you take it. The way that makes the most sense for me to think of it is imagining 100 locks and 100 keys. You pick 1 key and 1 lock (1 test, and 1 correct set of answers). After each attempt the locks are randomized again, just like each subsequent test and correct set of answers would become random each time. Each time you try the locks (or the test), you would have a 1:100 chance regardless of how many times you try. Which one of us (if either) is correct? And how do we go about solving this problem? Thanks in advance for the help!","['statistics', 'probability', 'recreational-mathematics']"
3763900,What should I read to understand the math behind waves?,"I'm learning differential equations and waves - following online courses and reading some textbooks - and I find that quite often, the use of Phasors, equations combining sinusoidal waves of different amplitudes and frequencies (the derivation of beats)...etc...they're explained really briefly, without a lot of diving deeper into them, and then applied. Is there a specific ""math"" which I could study, or textbook I could read, that presents these subjects more explicitly? The more geometric and intuitive the math book is, the better! I tried looking for ""wave mathematics"" but...found mostly really advanced books. Complex Analysis didn't seem to fit either... All and any recommendations are welcome, thank you so much! Edit: Books on the differential equations of waves are helpful...but, I think I'd like to start simpler. What led me to asking this question is that it took me a long time to understand the geometric intuition for how to combine waves of the same amplitude but different frequencies (for which this - https://www.jstor.org/stable/27965328?seq=1 - helped quite a bit) and I still don't understand at all how to combine them if they have different amplitudes! Is there not anything simpler than a differential equations approach, or pherhaps a book that DOES go into the differential equations, but also does so explaining each step, instead of skipping over the ""basic"" math of waves? Thanks again.","['ordinary-differential-equations', 'book-recommendation', 'reference-request', 'wave-equation', 'partial-differential-equations']"
3763946,Is Analysis I by Terence Tao incomplete?,"I am trying self-study real analysis using Analysis I by Terence Tao .
I came across some of the following: Chapter 1: Introduction, page 8: Limit interchange is always untrustworthy? (See Proposition $11.15 .3$ for an answer. But there is no Proposition 11.15.3 . Chapter 11 has only 10 sections. Another example from chapter1 page 10: Since $1 \neq 0,$ we thus seem to have shown that interchange of
derivatives is untrustworthy. But are there any other circumstances in
which the interchange of derivatives is legitimate? (See Theorem $11.37 .4$ and Exercise $11.37 .1$ for some answers.) Again I could find neither Theorem 11.37.4 nor Exercise 11.37.1 . Am I missing something or is there is some mistake in the book?","['limits', 'book-recommendation', 'reference-request', 'real-analysis']"
3763992,about prisoners and selection of numbers,"Each of the three prisoners had a natural number written on their foreheads: 1, 2 or 3. Numbers can be repeated.  The prisoners see all numbers except their own.  After that, everyone tries to guess their number.  If someone succeeds, the prisoners will be released, otherwise they will be executed.  Before the trial, the prisoners can agree.  How can they get out? (prisoners cannot hear each other during the trial) I thought that 1st prisoner can do this.
if he sees two identical numbers in front of him, then he will say the same number and thus one of the prisoners will definitely be right if all the numbers are the same, and if the first prisoner sees different numbers, then he will say the third number which was not there and thus the prisoners will be  right in the case of three different digits.  but after that I don't know how the other 2 prisoners should act, they already know that in the case with three different numbers and the case with three identical numbers, the first one has already taken over, so it is necessary to choose one number from the two that they see.  But how can they choose to be right in all cases?","['logic', 'probability']"
3764034,Turning on a nuclear briefcase with the smallest possible number of keystrokes,"On the front panel of the ""nuclear briefcase"" there are $12$ buttons. Each button controls its own switch: pressing it toggles it from ON to OFF and back. The initial position of the switches is unknown. The nuclear case triggers an inaudible (ultrasonic) frequency alarm when at least eight switches are in the ON position. Find the shortest way using as few keystrokes as possible to ensure that the suitcase will sound an alarm. I tried to do this with examples, but actually I've not idea how can I determine which button to press.","['logic', 'combinatorics', 'extremal-combinatorics', 'puzzle']"
3764066,"How do I use structural induction to show that for all $(a,b) \in S$ that $(a+b) = 4k$ for some $k \in \Bbb Z$?","I'm given that: Let $S$ be the subset of the set of ordered pairs of integers defined recursively by: Base case: $(0,0) \in S$ Recursive step: If $(a,b) \in S$ , then $(a+1, b+3) \in S$ and $(a+3, b+1) \in S$ How do I use structural induction to show that for all $(a,b) \in S$ that $(a+b) = 4k$ for some $k \in \Bbb Z$ ? Essentially, I believe I'm supposed to show that $(a+b)$ is divisible by $4$ , but I'm at a bit of a loss in figuring out what steps I'm supposed to take here. Any help is greatly appreciated!","['elementary-number-theory', 'induction', 'divisibility', 'discrete-mathematics']"
3764125,Integrals related to $\int_0^{\pi} \left(\frac{\sin(\alpha u)^\alpha \sin((1-\alpha) u)^{1-\alpha}}{\sin u} \right)^{\rho/\alpha}du$,"I meet the following integral when I am reading materials regarding the stable distribution: $$
\frac{1}{\pi}\int_{0}^{\pi}\left\{%
\frac{\sin^{\alpha}\left(\alpha u\right)\
\sin^{1-\alpha}\,\left(\,{\left[1 - \alpha\right]u}\,\right)\ }{\sin\left(u\right)} \right\}^{\rho/\alpha}\,\mathrm{d}u,
$$ where $\alpha \in \left(0,1\right)$ . $$
\mbox{ It looks the result shall be}\quad
\frac{\Gamma\left(1 - \rho/\alpha\right)}
{\Gamma(1 - \rho/\alpha + \rho)\,\Gamma(1-\rho)},
$$ where $-1 < \Re\left(\rho\right) < \alpha$ . However, I have no idea how to solve it. Wondering someone can help me on this. Thanks in advance 13/11/2020 I would like to thanks Pisco for the kind and patient help. Now I can understand his proof perfectly. There is a related integral, $$\int _0^{\infty }\frac{1}{\pi }\int _0^{\pi }\exp\left\{-q x^{-\frac{1-\alpha }{\alpha }}  \left(\frac{(\sin[\alpha
 u])^{\alpha }(\sin[(1-\alpha )u])^{1-\alpha }}{\sin [u]}\right)^{\frac{1}{\alpha }}\right\}du e^{-x}dx=e^{-q^{\alpha }}, \quad \alpha \in (0,1),\ (*)$$ Further, since for the unilateral stable distribution, $S_\alpha$ , $\alpha \in (0,1)$ , the density function is $$f_{S_\alpha}(x)=\frac{1}{x}\sum_{k=1}^\infty\frac{(-x^\alpha)^{-k}}{k!\Gamma(-k\alpha)}, \ x>0,$$ and we know the Laplace transform of the above function is $e^{-q^\alpha}$ . For example, see ""2016A new family of tempered distributions"". If we take the Laplace for $f_{S_\alpha}$ and leave the requirement for the exchange of the integral and sum for a while, we have $$\mathcal{L}_{S_\alpha}(q)=\sum_{k=1}^{\infty}\frac{(-q^\alpha)^k}{k!\Gamma(-k \alpha)}\Gamma(-k\alpha)=e^{-q^\alpha}-1,$$ which is not what we are expecting. Therefore I am also wondering how to find $f_{S_\alpha}$ from $(*)$ . Looking for your help.","['integration', 'probability-distributions', 'complex-analysis', 'gamma-function', 'beta-function']"
3764176,Cardinality of set of $a_r$?,"Question So I conjectured a formula which was proven : Let $b_r = \sum_{d \mid r} a_d\mu(\frac{m}{d})$ . We prove that if the $b_r$ 's are small enough, the result is true. Claim: If $\lim_{n \to \infty} \frac{\log^2(n)}{n}\sum_{r=1}^n |b_r| = 0$ and $f$ is smooth, then $$\lim_{k \to \infty} \lim_{n \to \infty} \sum_{r=1}^n a_rf\left(\frac{kr}{n}\right)\frac{k}{n} = \left(\lim_{s \to 1} \frac{1}{\zeta(s)}\sum_{r=1}^\infty \frac{a_r}{r^s}\right)\int_0^\infty f(x)dx.$$ My question is what is the cardinality of the set of $a_r$ ? Reason for confusion Focusing on the L.H.S This seems to say for every point on the curve can be mapped to $f(x)$ which in turn can be mapped to a coefficient $a_r$ . $$ x \to f(x) \to a_r $$ Hence, the set has cardinality $ 2^{\aleph_0} $ Focusing on the R.H.S This seems to say the number of $a_r$ must must be the same as that of the natural numbers. Hence, the set has cardinality $ \aleph_0 $","['integration', 'measure-theory', 'elementary-set-theory', 'intuition']"
3764198,Best approximation of sum of unit vectors by a smaller subset,"Let $v_1,\ldots,v_N$ be linear independent unit vectors in $\mathbb{R}^N$ and denote their scaled sum by $s_N = \frac{1}{N}\sum_{k=1}^N v_k.$ I would like to find a small subset of size $n$ among those vectors such that their scaled sum approximates $s_N$ well. In other words find $$ J = \underset{J\in\mathscr{J}}{\operatorname{argmin}} \bigg\lVert s_N - \frac{1}{n}\sum_{k=1}^n v_{J_k}\bigg\rVert$$ where $J$ runs over the set $\mathscr{J}$ of all subsets of $\{1,\ldots,N\}$ with size $n$ and $\lVert \cdot \rVert$ is the euclidean norm. The set of vectors can be considered an iid sample drawn uniformly from the sphere. And, of course, in my case $N$ and $n$ are too large ( $N$ will be of the order of 10'000 or 100'000 and $n$ maybe one or two magnitudes smaller) to just try all subsets. So  I am looking for something more clever. My approach so far I tried Repeated random subsampling , i.e. drawing many, many subsets of size $n$ in an iid fashion, calculating the approximation for each instance and retaining the best. Greedy approach, starting with a single vector, and then increasing the set in steps every time by a single vector. The vector is that single vector which gives the best approximation for the enlarged set. Questions Is this a known problem with a proper name? Is it hard (as in NP-hard for example) or are clever solutions known? Are there better heuristic approaches? Are there theoretic results/performance guarantees for the two heuristics I used? Note : I edited the question to include scaling. Some of the answers/comments refer to the older version where vectors were not scaled.","['linear-algebra', 'combinatorics', 'integer-programming', 'discrete-optimization', 'optimization']"
3764230,Learn how to sketch functions intuitively,"A professor told us that it is better to have an idea of the graph of a function before starting to apply the techniques of differential calculus in order to sketch it rigorously. He was able to sketch an approximate graph of functions like: $$e^{|x^2-1|+x}$$ $$\sqrt[3]{x^2 (x-1)}$$ $$e^{-x} \sqrt[3]{ (x^2-4)}$$ It is easy to understand the process when guided, however I can't seem to be able to build the same kind of intuition alone. Are there methods/books that help you to have a general idea on the behavior of a function on its domain before using differential calculus? I believe it should be a set of techniques more advanced than the horizontal/vertical shifting/flipping/scaling that is learned in precalculus but less advanced than differential calculus.","['calculus', 'graphing-functions']"
3764317,What is the geometric interpretation of the transpose?,"I can follow the definition of the transpose algebraically, i.e. as a reflection of a matrix across its diagonal, or in terms of dual spaces, but I lack any sort of geometric understanding of the transpose, or even symmetric matrices. For example, if I have a linear transformation, say on the plane, my intuition is to visualize it as some linear distortion of the plane via scaling and rotation. I do not know how this distortion compares to the distortion that results from applying the transpose, or what one can say if the linear transformation is symmetric. Geometrically, why might we expect orthogonal matrices to be combinations of rotations and reflections?","['matrices', 'orthogonal-matrices', 'linear-algebra', 'geometric-interpretation', 'transpose']"
3764335,"Unusual ways of summing well-known series -- for example, this unusual summation of the geometric series","The typical way of summing $S_g(n,x) = 1+x+x^2+\cdots+x^n$ by multiplying by $(1-x)$ is well known. The arithmetico-geometric series $S_{ag}(n,x) = 1+2x+3x^2+4x^3+\cdots+(n+1)x^n$ can be summed in one of two ways: 1) apply $(1-x)$ twice, or 2) notice that the $n$ th term is the derivative of $x^n$ , and thus that $\frac{d}{dx}S_g(n,x) = S_{ag}(n,x)$ . Let's call the first method the ""multiplication"" method and the second method the ""differentiation"" method. $S_g$ can be summed using the multiplication method, and $S_{ag}$ can be summed using both the multiplication and the differentiation methods (assuming that you know the sum of $S_g$ ). The natural question is whether or not $S_g$ can also be summed using the differentiation method or something like it. This led me to the following summation of the geometric series: Multiply by $e^{yx}$ (which is never zero) on both sides to get $$
S_g(n,x)e^{yx} = e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx}. 
$$ Noting that $xe^{yx} = \frac{\partial}{\partial y}e^{yx}$ , it follows that \begin{align}
e^{yx} + xe^{yx}+x^2e^{yx}+\cdots + x^ne^{yx} &= e^{yx} +  \frac{\partial}{\partial y}e^{yx} + x\frac{\partial}{\partial y}e^{yx}+\cdots+x^{n-1}\frac{\partial}{\partial y}e^{yx} \\
%
&=e^{yx}+ \frac{\partial}{\partial y}S_g(n-1,x)e^{yx} \\
%
&=e^{yx}+ \frac{\partial}{\partial y}[S_g(n,x)e^{yx}-x^ne^{yx}] \\
%
&=e^{yx}+\frac{\partial}{\partial y}S_g(n,x)e^{yx}-x^{n+1}e^{yx}, 
\end{align} and thus, $$
S_g(n,x)e^{yx}-\frac{\partial}{\partial y}S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx}.
$$ Finally, since $\frac{\partial}{\partial y}S_g(n,x)e^{yx} = xS_g(n,x)e^{yx}$ , it follows that $$
(1-x)S_g(n,x)e^{yx} = (1-x^{n+1})e^{yx},
$$ and hence, for all $x\neq 1$ , it follows that $$
S_g(n,x) = \frac{1-x^{n+1}}{1-x}.
$$ This method is clearly unusual in that, although it achieves the correct result, it uses more machinery (calculus, exponentials), and is a bit more complicated than the typical way. Nonetheless, I also think there's something fascinating about seeing all the different ways a series can be summed. My general question is what other ""unusual"" ways of summing well-known series do people know? My specific question is whether or not this particular way of summing the geometric series is known? My guess for the second question is 'yes', because the tools are still pretty basic and the manipulations not that complicated, but I've only ever seen the typical way.","['summation', 'real-analysis', 'alternative-proof', 'calculus', 'sequences-and-series']"
3764342,"If a sequence $\langle a_n\rangle$ is such that $a_1a_2=1, a_2a_3=2, \ldots$ and $\lim\frac{a_n}{a_{n+1}}=1.$ Then find $|a_1|.$","Here since $\lim \frac{a_n}{a_{n+1}}=1.$ So no definite conclusion can be made about the nature of the sequence $\langle a_n\rangle$ . So how can I can proceed to find the value of $a_1$ from the relation: $a_ka_{k+1}=k,$ for any $k\in\mathbb N$ ? Please suggest something..","['sequences-and-series', 'real-analysis']"
3764366,plot of $\sin(x) + \sin(y)= \cos(x) + \cos(y)$,"I was playing arround with implicit plots of the form $f(x,y) = g(x,y)$ , and I noticed that if you plot in the plane the following equation: $\sin(x) + \sin(y)= \cos(x) + \cos(y)$ you get the following graph: My question is why does this trigonometric functions give us this squares spanning the entire plane?","['implicit-function', 'trigonometry', 'graphing-functions']"
3764400,What can be concluded about an equilibrium point that has finitely many trajectories that converge to it?,"If you have a dynamical system $\dot{x}=f(x)$ and $f(x)$ is smooth , given that $f(x_0)=0$ and only finitely many trajectories converge to $x_0$ . I think if $x_0$ is isolated then the stable manifold is one dimensional since if it was more than one dimensional it would have infinitely many . I also think that the number of trajectories can be at most two . My questions are : 1) If $x_0$ is isolated , is the stable manifold of $x_0$ only one dimensional and If yes is the smoothness condition needed ? 2) If $x_0$ is not isolated can other conditions be imposed so that the same conclusion can be said ? Edit : I think I could use the linearization of the system at $x_0$ as $\dot{x}=Ax$ , so if the system has one stable eigenvector $v_1$ ,   since trajectories cannot intersect it will have only two stable trajectories $v_1$ and $-v_1$ , if there is another stable eigenvector then the whole plane containing the two vectors will be stable implying there is an infinite number of trajectories . However , this doesn't include the cases where the linearization fails or non-isolated points case .","['ordinary-differential-equations', 'control-theory', 'stability-in-odes', 'differential-topology', 'dynamical-systems']"
3764420,change of variables and partial derivatives in thermodynamics,"In thermodynamics one works with state functions, e.g., the energy function $U(Y_1,\dotsc,Y_n)$ , where $Y_i>0$ are the so called extensive variables. This function is $1$ st order homogeneous. Sometimes one uses the specific energy function $$u(y_1,\dots,y_{n-1})=U(y_1,\dots,y_{n-1},1),$$ where $y_i=\frac{Y_i}{Y_n}$ . The relation between $u$ and $U$ is $$Y_n\cdot u(y_1,\dotsc,y_n)=U(Y_1,\dots,Y_n).$$ I want to understand how to express second order partial derivatives $\frac{\partial^2u}{\partial y_i\partial y_j}$ in terms of $\frac{\partial^2U}{\partial Y_i\partial Y_j}$ and $Y_i$ . Let's start with the first order derivatives. Using the chain rule we can write $$\frac{\partial u}{\partial y_i}=\sum_{k=1}^n \frac{\partial U}{\partial Y_k}\frac{\partial Y_k}{\partial y_i}.$$ But now I get stuck. What is $\frac{\partial Y_k}{\partial y_i}$ ? Can we simply write it as $\frac{\partial Y_k}{\partial y_i}=1/\frac{\partial y_i}{\partial Y_k}$ ? Apparently not as otherwise, e.g., $\frac{\partial y_1}{\partial Y_2}=0$ and $\frac{\partial Y_2}{\partial y_1}=\infty$ . How can we deal with that? Technically, it must hold that $\frac{\partial U}{\partial Y_i}=\frac{\partial u}{\partial y_i}$ because the first order partial derivatives of a $1$ st order homogeneous function are $0$ th order homogeneous. But I do not see how to show that formally using mathematical arguments.","['mathematical-modeling', 'multivariable-calculus', 'change-of-variable', 'partial-derivative', 'derivatives']"
3764426,Matrix-vector multiplication/cross product problem,"How can I generally solve equations of the form $\mathbf{A} \mathbf{w} =
\begin{pmatrix} x \\ y \\ z \end{pmatrix}
\times \mathbf{w}$ for the matrix $\mathbf{A},$ where $\mathbf{w}$ can be any vector? I recognize that you could just set $\mathbf{w}$ to a vector with simple values, such as $\begin{pmatrix} 1 \\ 2 \\ 1 \end{pmatrix}$ , but doing so still isn't helpful. Also, $x,$ $y,$ and $z$ are entirely independent variables.","['matrices', 'algebra-precalculus', 'cross-product', 'vectors']"
3764430,Find for which $\alpha$ the integral $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ converges,"Find for which $\alpha$ the integral $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ converges. My Attempt: suppose $f(x) = \frac{1-x^{\alpha}}{1-x}$ . I think that the integral converges for $\alpha > -1$ . First I've tried to use the linearity of integrals, such that: $$\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx = \int_{0}^{1} \frac{1}{1-x}dx - \int_{0}^{1} \frac{x^{\alpha}}{1-x}dx$$ as $$\int_{0}^{1} \frac{1}{1-x}dx = |_{0}^{1} \ln(1-x) $$ but because the first part of the integral diverges and is not dependent on $\alpha$ , then it's not helpful. the reason why I was foucsed in the point $x=1$ is because for $x=0$ , I'll apply the $\int_{0}^{1} \frac{1-x^{\alpha}}{1-x}dx$ limit comparison test with the function $g(x) = \frac{1}{1-x}$ , and lim $_{x\to 0} \frac{f(x)}{g(x)} = $ lim $_{x\to 0} 1-x^{\alpha} = 1$ , then for every $0<t<1: \ \int_{0}^{t}f(x)$ converges. I've checked different values of $\alpha$ and I'm wondering if the answer is connected to the fact that given $b>0$ , the integral $\int_{0}^{b} \frac{1}{x^\alpha}dx$ converges if and only if $\alpha < 1$ . I suppose that the easiest way to prove for which $\alpha$ the integral converges is by using the limit comparison test for improper integrals, but I can't find a function that will prove/disprove my hypothesis.","['integration', 'convergence-divergence', 'improper-integrals', 'real-analysis']"
3764465,Definition and intuition of a tubular neighborhood of a submanifold,"Let $d\in\mathbb N$ , $k\in\{1,\ldots,d\}$ and $M$ be a $k$ -dimensional embedded $C^1$ -submanifold of $\mathbb R^d$ with boundary $^1$ . Now let $$T_xM:=\left\{v\in\mathbb R^d\mid\exists\varepsilon>0,\gamma\in C^1((-\varepsilon,\varepsilon),M):\gamma(0)=x,\gamma'(0)=v\right\}$$ denote the tangent space of $M$ at $x$ and $N_xM:=(T_xM)^\perp$ for $x\in M$ . I'm trying to understand the definition of a tubular neighborhood of $M$ . I was only able to find definitions of this notion in a way more general setting which built up on several concepts I'm not familiar with. So, I'd like to find a simplified, but equivalent, definition for my present setting. What I've understood is that one considers the space $$N(M):=\{(x,v):x\in M\text{ and }v\in N_xM\}$$ and the map $$E(x,v):=x+v\;\;\;\text{for }(x,v)\in N(M).$$ Now the usual definition of a tubular neighborhood $U$ of $M$ is that it is a (open?) neighborhood of $M$ in $\mathbb R^d$ (really a neighborhood of all of $M$ ?) such that $U$ is the diffeomorphic image under $E$ of an open subset $V$ of $N(M)$ with $$V=\{(x,v)\in N(M):\left\|v\right\|<\delta(x)\}\tag1$$ for some continuous function $\delta:M\to(0,\infty)$ . I really got trouble to understand this. The vector $E(x,v)$ is simply a vector originating from $x$ and pointing in the direction $v$ . If $M$ is a circle in $\mathbb R^3$ , I guess the intuition is that for each point $x$ on the circle all the $v\in N_xM$ built a ring around $x$ by ""rotating"" the normals around $x$ . Doing so for all $x$ on the circle, we obtain a torus. Is this correct so far? All the pictures I've found have confused me. And how would a tubular neighborhood of a sphere in $\mathbb R^3$ look like? For a sphere, the normal spaces are 1-dimensional ... $^1$ i.e. each point of $M$ is locally $C^1$ -diffeomorphic to $\mathbb H^k:=\mathbb R^{k-1}\times[0,\infty)$ . If $E_i$ is a $\mathbb R$ -Banach space and $B_i\subseteq E_i$ , then $f:B_1\to E_2$ is called $C^1$ -differentiable if $f=\left.\tilde f\right|_{B_1}$ for some $E_1$ -open neighborhood $\Omega_1$ of $B_1$ and some $\tilde f\in C^1(\Omega_1,E_2)$ and $g:B_1\to B_2$ is called $C^1$ -diffeomorphism if $g$ is a homeomorphism from $B_1$ onto $B_2$ and $g$ and $g^{-1}$ are $C^1$ -differentiable.","['surfaces', 'manifolds-with-boundary', 'smooth-manifolds', 'differential-topology', 'differential-geometry']"
3764483,Reading Atiyah's article,"I'm reading Atiyah's article about his homonymous theorem but cannot figure out some of the notations he used. I'm not very familiar with algebraic geometry, so I thought I could ask here for some help: the article to which I refer is ( ""Convexity and commuting hamiltonians"" ). There, he talks about ""Negative normal bundle"" (Proof of Lemma 2.1). I know what a normal bundle is, but I don't know what a negative normal bundle actually is; at the very beginning he defined ""almost periodic vector fields"" as those that generate a torus action (and I didn't understand in which sense can a vectorial field generate an action (probably by exponential?)); referring to that, in (Lemma 2.2) he said that the almost periodic vector field "" $X_\phi$ is the set of the fixed points"" of the action generated by $X_\phi$ (How can a vector field be a set of points?); At page 6, he used the following notation: "" $\pm\phi|N$ "" and "" $\pm f_{n+1}|N$ "" where $\phi$ and $f_{n+1}$ are real valued functions. What does it mean (he said that $\pm\phi|N$ - for example - has $Z\cap N$ as a non-degenerate critical manifold, so I guess they both are subsets of $N$ )?","['vector-fields', 'vector-bundles', 'lie-groups', 'differential-geometry']"
3764552,Is the minimum of this functional $C^{\infty}$?,"The problem: Let us define $$
\mathscr{F}(u)=\int_0^1(u'(x))^4-e^x\sin (u(x))\, \mathrm{d}x
$$ for $u \in W^{1,1}([0,1])$ such that $u(0)=A$ and $u(1)=B$ .
It don't matters what $A$ or $B$ are, because I am interested in the reasoning. I am asked to study if the minimum $u$ is such that $u \in C^{\infty}([0,1])$ or eventually $u \in C^{\infty}([0,1]-E)$ where $E$ is a closed and negligible subset of $[0,1]$ , i.e. it is closed and $\mu(E)=0$ . An attempt: First of all, thanks to Ioffe's theorem, we know that $\mathscr{F}$ is sequentially weakly lower semi continuous in $W^{1,1}([0,1])$ . Further more, because $(u'(x))^4-e^x\sin (u(x)) \geq (u'(x))^4-e$ , a minimum $u \in W^{1,1}$ exists. But what can be said about the regularity of $u$ ? I know the Tonelli’s partial regularity theorem but I cannot directly apply it here because $F_{pp}$ is not defined positive but we only have $F_{pp} \geq 0$ .
I know that if $u'(x) \neq 0$ then $u$ is $C^{\infty}$ in a neighborhood of $x$ . My question is: $u \in C^{\infty}([0,1])$ ? Further more I would like to understand when can we apply Tonelli’s theorem if $F_{pp} \geq 0$ but not $F_{pp} > 0$ i.e. if $F_{pp}$ is positive semidefinite but not positive definite. Remark: I tried first solving the related Question .","['calculus-of-variations', 'real-analysis', 'maxima-minima', 'functional-analysis', 'optimization']"
3764570,"Proof: For $n \in \mathbb{Z_+}$, the set of all functions $f: \{1,...,n\} \to \mathbb{Z_+}$ is countable","(citation: Munkres Topology 7.5 b) Backgound (from Munkres Topology Chapter 7): Theorem 7.1: $B$ is a nonempty countable set $\Leftarrow\Rightarrow$ there is an injective function $g: B \to \mathbb{Z_+}$ Theorem 7.6: a finite product of countable sets is countable PROOF Let $B_n = \{f | f: \{1,...n\} \to \mathbb{Z_+}\}$ and let $\mathbb{Z_+^n} = \{(z_1, z_2,...,z_n)|z_1, z_2,...,z_n\in \mathbb{Z_+}\}$ . Define the function $g_n: B_n \to \mathbb{Z_+^n}$ such that $g_n(f) = (f(1), f(2),...,f(n))$ , where $f\in B_n$ . Let $z = z' \in \mathbb{Z_+^n}$ , such that $z = g_n(f), z' = g_n(f')$ for $f,f'\in B_n$ . Then $z = (f(1),f(2),...,f(n)) = (f'(1),f'(2),...,f'(n)) = z'$ , which holds iff $f(1)=f'(1), f(2)=f'(2),...,f(n)=f'(n)$ . This implies that $f = f'$ , hence $g_n$ is an injection from $B_n$ into $\mathbb{Z_+^n}$ . $\mathbb{Z_+^n}$ is countable by Theorem 7.6, as $\mathbb{Z_+}$ is countable.  Thus, by Theorem 7.1, $\exists$ an injective function $h: \mathbb{Z_+^n}\to \mathbb{Z_+}$ .  Then, $h \circ g_n: B_n\to \mathbb{Z_+}$ is injective. Therefore, by Theorem 7.1, $B_n$ is countable. I believe this is sufficient, but please let me know, if it may be corrected or improved.","['functions', 'solution-verification']"
3764576,Is the product of two Cesaro convergent series Cesaro convergent?,"Let $\{a_n \}_{n \geq 1}$ and $\{b_n \}_{n \geq 1}$ be two sequences of real numbers such that the infinite series $\sum\limits_{n=1}^{\infty} a_n$ and $\sum\limits_{n=1}^{\infty} b_n$ are both convergent in the Cesaro sense i.e. \begin{align*} \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} s_k & < + \infty \\  \lim\limits_{n \to \infty} \frac 1 n \sum\limits_{k=1}^{n} t_k & < + \infty \end{align*} where $\{s_k \}_{k \geq 1}$ and $\{t_k\}_{k \geq 1}$ are sequences of partial sums of the series $\sum\limits_{n=1}^{\infty} a_n$ and $\sum\limits_{n=1}^{\infty} b_n$ respectively. Can I say that $\sum\limits_{n=1}^{\infty} a_n b_n$ is convergent in the Cesaro sense? If ""yes"" then what can I say about it's limit in terms of the limits of the given two series?","['proof-writing', 'real-analysis', 'sequences-and-series', 'convergence-divergence', 'cesaro-summable']"
3764584,Reference for a bilinear form lemma,"I encountered the result below in a paper of Claude Viterbo ( Intersection de sous-variétés lagrangiennes, fonctionnelles d’action et indice des systèmes
hamiltoniens , p. 379) that I was reading, and it does not have a reference. If anyone could provide me a reference, it would be very helpful. Lemma: Let $Q^t$ be a $C^1$ family of bilinear forms defined in a Hilbert space. Let $Q^t$ be nondegenerate for $t\neq 0$ and $Q^t = U_t + C_t$ , where $U_t$ is positive definite with continuous inverse and $C_t$ is compact.
If $\left.\frac{d}{dt}Q^t\right|_{t=0}$ when restricted to $\ker(Q^0)$ has signature $\sigma$ and nullity $\mu$ we have \begin{equation}
\sigma - \mu \leq index(Q^{-1}) - index(Q^{+1}) \leq \sigma + \mu.
\end{equation} In fact, I am trying to understand the proof of proposition 8. Thank you very much!","['bilinear-form', 'linear-algebra', 'functional-analysis', 'reference-request']"
3764596,How to differentiate $g(X)=\operatorname{tr}\left(X^{-1}\right)$? [duplicate],"This question already has an answer here : Gradient of ${\mathrm X} \mapsto \mbox{tr} \left(\mathrm A \mathrm X^{-1} \mathrm B \right)$ (1 answer) Closed 3 years ago . Let $X$ be a square invertible $n \times n$ matrix. Calculate the derivative of the following function with respect to X. $$
g(X)=\operatorname{tr}\left(X^{-1}\right)
$$ I'm stumped with this. As when I work through it I use  these two identities. $$\frac{\partial}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}=-\boldsymbol{f}(\boldsymbol{X})^{-1} \frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}} \boldsymbol{f}(\boldsymbol{X})^{-1}$$ and 2. $$
\frac{\partial}{\partial \boldsymbol{X}} \operatorname{tr}(\boldsymbol{f}(\boldsymbol{X}))=\operatorname{tr}\left(\frac{\partial \boldsymbol{f}(\boldsymbol{X})}{\partial \boldsymbol{X}}\right)
$$ I should arrive at the solution. using 1. I get $$d/dX(X^{-1}) = -X^{-1}\otimes X^{-1}$$ . So the answer should be the trace of that right? which = $$tr(-X^{-1})tr(X^{-1}).$$ but the solution seems to be $$-X^{-2T}$$ ? which I can't see","['scalar-fields', 'matrices', 'matrix-calculus', 'inverse', 'derivatives']"
3764630,Find two-index Inverse relations,"Solve the following system of $ mn$ equations $$
a_{p,q}=\sum_{i=0}^m \sum_{j=0}^n i^p j^q b_{i,j}, p=0 \ldots m, q=0 \ldots n. 
$$ where $b_{i,j}$ are unknowns. Of course, for small $m,n$ , it is possible to solve it by hand by using Gauss elimination, but what about the general case? I hope there exists a nice close expression for $b_{p,q}$ something like to $$
b_{p,q}=\sum_{i=0}^n \sum_{j=0}^m X^{(p,q)}_{i,j} \, a_{i,j}, p=0 \ldots m, q=0 \ldots n. 
$$ Numerical experiments for small $n,m$ show that it is very likely that the Stirling numbers of the first kind should be involved in the expression for $X^{(p,q)}_{i,j}$ EDIT. I checked for some small $n,m$ that the following holds $$
b_{0,0}=- \frac{1}{n! m! } \sum_{i=0}^m \sum_{j=0}^n s(m+1,i+1) s(n+1,j+1) a_{i,j}, 
$$ where $s(n,i)$ is the signed Stirling numbers of the first kind.
For other cases it seems that we are dealing with some generalizations of the Stirling numbers.","['combinatorics', 'stirling-numbers', 'generating-functions']"
3764643,Elementary block matrix operations to block-triangularize block tridiagonal matrix,"For some flow modeling purpose, the system $(S)$ defined as $MX=L$ is solved, where $M$ is a tridiagonal block matrix defined as $$ M= \begin{pmatrix}B_1& C_1 &0 &0  \\ A_2 & B_2 &  C_2 & 0 \\ 0& A_3 & B_3 & C_3 \\ 0 & 0 & A_4 & B_4\end{pmatrix} $$ and $L$ is a block vector defined as $$ L = \begin{pmatrix} L_1 \\ L_2 \\ L_3 \\ L_4 \end{pmatrix}$$ For some physical constraints, I need to get rid of the block $A_4$ and have the last line as $$\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$$ where $X$ is the resultant block after transforming $M$ . Note that $A_i$ blocks are not invertible. Here are two ideas I thought about using my humble matrices background, but I'm not sure they're right ( I'm assuming the same properties of simple matrices are available for block matrices, which is not always true.. ) : Change the third column $Col_3$ as follows : $Col_3 <- Col_3 - A_4B_4^{-1}Col_4 $ . The results retrieved were wrong... Multiplying $M$ on the left by the matrix $$ \begin{pmatrix}I& 0 &0 &0  \\ 0 & I &  0 & 0 \\ 0& 0 & I & 0 \\ Z_1 & Z_2 & Z_3 & I\end{pmatrix} $$ and try to find $Z_i$ as a function of the blocks $A_i, B_i$ and $C_i$ so I'd have the last line in a $\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$ format. The computation was a bit complicated and didn't give good results. How can I transform M to have its last line in a $\begin{pmatrix} 0 & 0 & 0 & X \end{pmatrix}$ format and have the same solution as the $(S)$ system ? Thanks!","['matrices', 'systems-of-equations', 'tridiagonal-matrices', 'block-matrices']"
3764648,Regular Expression describing language accepted by Finite State Automata,Hopefully I am including the image correctly or this won't make any sense.  I am trying to figure out a regular expression for what this FSA accepts.  From what I can tell it accepts any combination of 01 or 10 repeated.  Wouldn't the expression then be ((01)* | (10)*) * I believe I have what it should be as correct I just don't know if I am representing that answer as a regular expression in the correct form.  Any help would be appreciated,"['automata', 'regular-expressions', 'discrete-mathematics']"
3764653,"Equivalence of Classical Nullstellensatz to ""Affine schemes have points""","The nLab page on the Nullstellensatz states: For $k$ an algebraically closed field and $I$ a proper ideal in the polynomial ring $k[X_1, \dots, X_n]$ , the set $V(I)$ (of $n$ -tuples $\vec x \in k^n$ such that all polynomials in $I$ vanish when evaluated on these $\vec x$ ) is an inhabited set. we remark that an element of $V(I)$ is just a $k$ -algebra homomorphism of the form $k[X_1, \dots X_n]/I \rightarrow k$ . I believe the map we are discussing is to consider each element $\vec x \in V(I)$ as the evaluation homomorphism , $\phi_{\vec x}: K[X_1, X_2, \dots X_n]/I \rightarrow k$ is the evaluation map which maps $\phi(p) = p(\vec x)$ . I don't understand why we need to quotient by the ideal $I$ . Even without the quotient, it continues to be a homomorphism? We have that: $$
\forall p, q \in K[X_1, \dots, X_n], \\
(p + q)(\vec x) = p(\vec x) + q(\vec x) \\
(p \cdot q)(\vec x) = p(\vec x) \cdot q(\vec x) \\
$$ So, why do we bother quotienting with $I$ ? What am I missing here? They go on to say: Dually this is a morphism of affine schemes (ring spectra) of the form $\operatorname{Spec}(k) \rightarrow \operatorname{Spec}(k[X_1, \dots X_n] / I)$ . Moreover since $\operatorname{Spec}(k)$ is the terminal object in this context, such a map is the same as a ""point"", a global element of $\operatorname{Spec}(k[X_1, \dots X_n] / I)$ . Hence in this form the Nullstellensatz simply says that (for $k$ algebraically closed) affine schemes have points I am quite lost at this stage. I understand that $Spec(k)$ contains only the zero ideal $(0)$ since $k$ is a field, and hence the morphism is as good as singling out a single point. However, in what way is this a ""terminal object""? In what category even are we discussing the above? How is this equivalent to the (only) Nullstellensatz that I know, which states: Nullstellensatz, statement 1: maximal ideals of $\mathbb C[X_1, \dots X_n]$ are in bijection with points in $\mathbb C^n$ ? Nullstellensatz, statement 2: All maximal ideals of $\mathbb C[X_1, \dots X_n]$ are of the form $(x - C_1, x - C_2, \dots x- C_n)$ for $c_i \in \mathbb C$ I want to understand this form of the Nullstellensatz since it is the one that is used in "" Yuri Manin, Introduction into theory of
schemes "". The textbook contains this as an exercise. I'm unfortunately unable to make sense of this.","['ring-theory', 'algebraic-geometry', 'commutative-algebra']"
3764727,Does $g(v_n) \longrightarrow g(0)$ for all $v_n \text{s.t.} ||v_{n+1}|| \leq ||v_n||$ imply $g$ continuos at $0$?,"The question is pretty much summed up in the title: Let $V,W$ be normed vector spaces and $$ g: V \to W$$ . Suppose g fulfills $$g(v_n) \longrightarrow g(0) $$ for all sequences $v_n$ such that $$v_n \longrightarrow 0$$ and $$||v_{n+1}|| \leq ||v_n||.$$ Does this imply continuity of $g$ at $0$ ? Intuititively, this should hold true, but I am looking for a proof (or counterexample). Of course, for any sequence $v'_n \longrightarrow 0$ we can select a subsequence that fulfills the above requirement and will have the limit $g(0)$ . But is this enough to ensure the convergence? Do we need additional restrictions on the spaces $V,W$ for this to hold?","['continuity', 'convergence-divergence', 'normed-spaces', 'functional-analysis']"
3764736,"Solving for positive reals: $abcd=1$, $a+b+c+d=28$, $ac+bc+cd+da+ac+bd=82/3$","$$a,b,c,d \in \mathbb{R}^{+}$$ $$ a+b+c+d=28$$ $$ ab+bc+cd+da+ac+bd=\frac{82}{3} $$ $$ abcd = 1 $$ One can also look for the roots of polynomial $$\begin{align}
f(x) &= (x-a)(x-b)(x-c)(x-d) \\[4pt]
&= x^4 - 28x^3 + \frac{82}{3}x^2 - (abc+abd+acd+bcd)x + 1
\end{align}$$ and $f(x)$ has no negative roots... but how else do I proceed? There is a trivial solution $\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, 27$ . We just need to prove it's unique.","['contest-math', 'inequality', 'roots', 'polynomials', 'algebra-precalculus']"
3764743,Why this $\sigma \pi \sigma^{-1}$ keeps appearing in my group theory book? (cycle decomposition),"I'm studying cycle decomposition in group theory. The exercises on my book keep saying things like: Find a permutation $\sigma$ such that $\sigma (123) \sigma^{-1} = (456)$ Prove that there is no permutation $\sigma$ such that $\sigma (1 2) \sigma^{-1} = (123)$ Show that $\pi(x_1 x_2 \cdots x_n)\pi^{-1} = (\pi(x_1)\cdots \pi(x_n))$ Why this  $\sigma \pi \sigma^{-1}$ appears every time in this book? Is this important for something? Could somebody give me a clear picture of how we arrive at this? Also, would be nice to know how to prove the third one, if somebody could help me.","['permutations', 'group-theory', 'abstract-algebra', 'finite-groups']"
3764744,"Let $G$ be a finite group such that if $A, B\le G$ then $AB\le G$. Prove $G$ is a solvable.","Let $G$ be a finite group satisfying the following property: (*) If $A, B$ are subgroups of $G$ then $AB$ is a subgroup of $G$ . Prove $G$ is solvable. So I felt like a good place to start is to let $|G| = p_1^{a_1}p_2^{a^2}...p_r^{a_r}$ such that each $p_i$ is a distinct prime, and then Let for each $i = 1, 2, ..., r$ Let $H_i$ be a Sylow $_{p_i}$ subgroup. So $G = H_1H_2..H_r$ and each $H_i$ is solvable since all $p$ -groups are solvable. I was then thinking that a good idea since I know by Burnside's Theorem that $H_iH_j$ is solvable, to try and use induction and prove that if $H_1H_2...H_m$ is solvable then $H_1H_2...H_mH_{m+ 1}$ is sovable, but I can't figure it out. Any help will be greatly appreciated.","['group-theory', 'abstract-algebra', 'solvable-groups']"
3764764,Example of a flow that does not preserve volume measure (autonomous ode),"Consider the autonomous differential equation in $\mathcal{U} = \mathbb{R} \times (0, +\infty)$ given by $$x' = \dfrac{x^2}{1+x^2y^2}, y' = 0.$$ Justify that the respective flow is complete (i.é, defined for all $t \in \mathbb{R}$ ). Show that given $T>0$ , there exist limited open sets $X \subset \mathcal{U}$ such that $X_T = \{f^t(x,y); (x,y) \in X \hspace{0.1cm} \text{and} \hspace{0.1cm} t \in \left[0,T\right]\}$ has infinite volume measure. Attempt: Consider the initial condition of the equation as $\gamma (0) = (x_0,y_0)$ . Since $y' = 0$ , I know that $y = y_0$ , with $y_0 > 0$ . I was able to show that the flow of this autonomous equation is $$f^t(x_0,y_0) = \left( \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) \overline{+} \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}, y_0 \right),$$ which is defined for every $t \in \mathbb{R}$ . Hence, the respective flow is complete. Given $T >0$ , my initial idea was to consider $X = (a_1, a_2) \times (0,y_0) \subset \mathcal{U}$ , with $$a_1 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) - \left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}$$ $$ \hspace{0.1cm} \text{and} \hspace{0.1cm}$$ $$ a_2 = \frac{\left(-\left(t - \frac{1}{x_0} + y_0 x _0 \right) +\left[\left(t - \frac{1}{x_0} + y_0 x_0 \right) + 4y_0 \right]^{\frac{1}{2}}  \right)}{2y_0}, $$ which is an open and limited set. But I'm not sure if $X_T$ is a set with infinite volume measure. Can anyone help me conclude or even help me construct a different limited open set? Any help would be appreciated!","['measure-theory', 'ordinary-differential-equations', 'dynamical-systems']"
3764769,"To calculate $\nabla f$ and $\nabla f(0,0)$","I need to calculate $\nabla f$ and $\nabla f(0,0)$ , where $$f(x,y)=10x^3-5x^2+5xy+5y^2+8.$$ My work so far: Using the formula $$\nabla f = \left(\frac{\partial f}{\partial x},\frac{\partial f}{\partial y}\right)$$ I got \begin{align*}
\frac{\partial }{\partial x}(10x^3-5x^2+5xy+5y^2+8)&=30 x^{2} - 10 x + 5 y,\\
\frac{\partial }{\partial y}(10x^3-5x^2+5xy+5y^2+8)&=5x + 10 y.
\end{align*} Thus $$\nabla f \left(x,y\right)=\left(30 x^{2} - 10 x + 5 y,5 x + 10 y\right)$$ Is my process correct so far? Also, to calculate $\nabla f(0,0)$ , would $$\nabla f \left(0,0\right)=\left(0,0\right)$$ as $(0,0)$ would provide no values?","['partial-derivative', 'multivariable-calculus']"
3764773,"If $X_n\thicksim\text{Unif}\left\{\frac{1}{n},\frac{2}{n},\dots,\frac{n-1}{n},1\right\}$ then $X_n\overset{d}\to Z$ where $Z\thicksim\text{Unif}[0,1]$","The Problem: Let $Z\thicksim\text{Unif}[0,1]$ . $\textbf{(a)}$ Find the moment generating function of $M_Z(t)$ of $Z$ . $\textbf{(b)}$ For $n\in\mathbb N$ , let $X_n$ be a uniform random variable on the set $\left\{\frac{1}{n},\frac{2}{n},\dots,\frac{n-1}{n},1\right\}$ , by which we mean that $P\left(X_n=\frac{k}{n}\right)=\frac{1}{n}$ for each $k\in\{1,2,\dots,n\}$ . Prove a limit in distribution $X_n\overset{d}\longrightarrow X$ using moment generating functions and identify the limit. My Attempt: (a) We have for $t\ne0$ , $$M_Z(t)=E\left[e^{tZ}\right]=\int_0^1 e^{tz}\,dz=\frac{e^t-1}{t}.$$ If $t=0$ then we see from the integral above that $M_Z(t)=1.$ (b) Observe that \begin{align}M_{X_n}(t)&=E\left[e^{tX_n}\right]=\sum_{k=1}^{n}\frac{e^{tk/n}}{n}\\
&=\frac{1}{n}\sum_{k=1}^n \left(e^{t/n}\right)^k=\frac{1}{n}\cdot\frac{e^{t/n}-e^{t(n+1)/n}}{1-e^{t/n}}\\
&=\frac{1/n(1-e^t)}{e^{-t/n}-1},
\end{align} for $t\ne0$ and $M_{X_n}(0)=1.$ For $t\in(-1,1)\setminus\{0\}$ , $$M_{X_n}(t)=\frac{1/n(1-e^t)}{e^{-t/n}-1}\longrightarrow\frac{0}{0}\quad\text{as }n\to\infty.$$ So we may apply L'Hopital's rule to the function $$f(x)=\frac{1/x(1-e^t)}{e^{-t/x}-1}\quad\text{for }x\in\mathbb R\setminus\{0\}.$$ Hence, $$\lim_{x\to\infty}f(x)=\lim_{x\to\infty}\frac{e^t-1}{te^{-t/n}}=\frac{e^t-1}{t},$$ whence for all $t\in(-1,1)$ , $$\lim_{n\to\infty}M_{X_n}(t)=\frac{e^t-1}{t}.$$ Since $M_Z(t)$ is finite in the interval $(-1,1)$ and the limit above holds for all $t\in(-1,1)$ , the continuity theorem for moment generating functions implies that $X_n\overset{d}\longrightarrow Z.$ What do you think about my proof? Any feedback is most welcomed and appreciated. Thank you for your time.","['probability-distributions', 'solution-verification', 'probability-theory']"
3764798,Showing the support of a sheaf may not be closed (Liu 2.5),"This is question 2.5 of Qing Liu. I am new in algebraic geometry and really stuck on it and can't do anything to solve it. The question:
Let $F$ be a sheaf on $X$ . Let $\operatorname{Supp} F=\{x\in X:F_x\neq 0\}$ . We want to show that in general, $\operatorname{Supp} F$ is not a closed subset of $X$ . Let us fix a sheaf $G$ on $X$ and a closed point $x_0\in X$ . Let us define a pre-sheaf $F$ by $F(U)=G(U)$ if $x_0\notin U$ and $F(U)= \{s\in G(U):s_{x_0}=0\}$ otherwise. Show that $F$ is a sheaf and that $\operatorname{Supp} F = \operatorname{Supp}G\setminus \{x_0\}$ . I don't know how to solve this question:
To show a pre-sheaf is a sheaf I need to check the ""uniqueness"" and ""gluing local sections"". For the uniqueness: Let $U$ be an open subset of $X$ , $s\in F(U)$ , if $x_0\notin U$ , then since $G$ is a sheaf, I don't see a problem for $F$ to be a sheaf. If $s\in F(U)$ and $x_0 \in U$ and $\{U_i\}_i$ be an open covering of $U$ ,then there exists an $i_0$ such that $x_0\in U_{i_0}$ . the image of $s$ in the stalk $F_{x_0}$ is $s_{x_0}$ . $F(U_{i_0})=\{s\in G(U):s_{x_0}=0\}$ by definition. I don't know what to do now? (so sorry and I know this is an easy question...)",['algebraic-geometry']
3764802,"If $ \lim_{x \to +\infty}f(x) = A $ and $ \lim_{x \to +\infty}f'(x) = B $, prove that $B = 0$","Problem: Let $ f: \mathbb{R} \to \mathbb{R} $ be a function of class $ C^1 $ such that $ \lim_{x \to +\infty}f(x) = A $ and $ \lim_{x \to +\infty}f'(x) = B $ for $ A, B \in \mathbb{R} $ . Prove that $B = 0$ . I need help in validating my proof. Here it goes: Suppose that $ B \neq 0 $ . Take $ \epsilon = B+1 $ . From the definition of limit to infinity, the following holds: $ (\exists M > 0)(\forall x > M) | f'(x) - B | < B+1 $ From there, $ 1 < f'(x) < 2B + 1 $ (for all x greater than M). Now, take an interval $ [M+1, M+2] $ . $ f $ is continuous on that segment, so it's bounded and reaches it's maximum and minimum. Function $ f $ is also differentiable on that segment, and if we apply Fermat's theorem (on local maximum/minimum), we'll get a contradiction, since $ f'(x) > 0 $ for all $ x \in [M+1, M+2] $ . Therefore $ B = 0 $ .","['limits', 'solution-verification', 'derivatives', 'real-analysis']"
3764811,Constants in matrix integration,"Suppose you have an integral of a matrix-valued function of the form: $$\int_a^b B A(t) C dt$$ In this case, the notation $A(t)$ is to denote a matrix that depends on the integrated variable $t$ (for example $A(t) = e^{tA}$ ), where the matrices $B$ and $C$ are independent of $t$ . Is the following necessarily true as it would be for the analogous scalar problem? $$\int_a^b BA(t)Cdt = B\int_a^bA(t)Cdt = \int_a^b BA(t)d t C = B\int_a^b A(t)dt C$$","['matrices', 'matrix-calculus', 'linear-algebra']"
3764847,"Prime ideals of $\Bbb C[x, y]$","In the exercise 3.2.E of Vakil's ""Foundations of Algebraic Geometry"", it is asked to prove that all the prime ideals of $\Bbb C[x, y]$ are of the form $(0)$ , $(x-a, y-b)$ or $(f(x, y))$ , where $f$ is an irreducible polynomial. In order to do so, it is suggested to consider a non-principal prime ideal $\mathcal{p}$ along with $f, g \in \mathcal{p}$ with no common factor. Then, dividing $g$ by $f$ in $\Bbb C(x)[y]$ , one is supposed to find $h(x) \in (f, g)$ - so some factor of the form $x-a$ is in $\mathcal{p}$ . But I can't seem to figure out how to find such $h(x)$ , because the expression of the division is something of the form: $$g(x, y) = f(x,y) \left (\frac{p_0(x)}{q_0(x)} + \cdots + \frac{p_n(x)}{q_n(x)}y^n \right) + \left(\frac{r_0(x)}{a_0(x)} + \cdots + \frac{r_m(x)}{a_m(x)}y^m \right)$$ And I don't know how to relate the fact that $f, g$ have no common factor with this expression. My guess is that it has something to do with the term $\frac{r_0(x)}{a_0(x)}$ , but I don't know what it is. Can anyone shed some light?","['algebraic-geometry', 'commutative-algebra']"
3764884,Determine the lie algebra of the subgroup of SO(4),"Let $G\subset SO(4)$ be the subgroup given below: $$G=\left\{
\begin{pmatrix}
a & -b & -c &-d\\
b & a & -d & c\\
c & d & a & -b \\
d &-c & b &a
\end{pmatrix} : a,b,c,d\in \mathbb{R}, a^2+b^2+c^2+d^2=1\right\}$$ Find the lie algebra $\mathfrak{g}$ . I know that if $X\in \mathfrak{so}_4$ , then $X\in \mathfrak{g} \iff e^{tX}\in G$ $\forall$ $t\in \mathbb{R}$ . However, I am not able to use it here, as the given group is a bit complicated. Any help is appreciated. Thanks","['lie-algebras', 'lie-groups', 'differential-geometry']"
3764941,How to express birational equivalence of Diophantine equation $x^4+y^4=z^2$ and elliptic curve?,"I have a series of change of variables to go from the Diophantine equation $x^4 + y^4 = z^2$ to the elliptic curve $y^2 = x^3 - 4x$ that is supposedly a bijection (bar a finite number of trivial solutions): $$
\begin{align}
x^4+y^4=z^2 \\
v^2 = u^4+1 && (u, v) &= (y/x, z/x^2) \\
r^2 + 2rs^2 = 1 && (r, s) &= (v-u^2, u) \\
a^3 + 2b^2 = a && (a, b) &= (r, rs) \\
y_1^2 = x_1^3 - 4x_1 && (x_1, y_1) &= (-2a, 4b)
\end{align}
$$ My question is, if I'm going in the inverse direction, how do I find $x$ in terms of $x_1, y_1$ if I only have two variables in the elliptic curve? Further, how could I write this into one singular change of variables? I can write the forward change as one: $$
(x, y, z) \rightarrow \left(-2 \frac{z-y^2}{x^2}, 4 \frac yx \left(\frac{z-y^2}{x^2}\right)\right)
$$ For the reverse I can make it up to $v^2 = u^4 + 1$ with the map: $$
(x_1, y_1) \rightarrow \left(-\frac{y_1^2-2x_1^3}{4x_1^2}, -\frac{y_1}{2x_1}\right)
$$ How would I notate going from the 2nd equation to the first, would $x$ just be a free variable and I multiply each side of $v^2=u^4+1$ by $x^4$ ?","['number-theory', 'elliptic-curves', 'diophantine-equations']"
3764942,How to solve these kind of Modular Arithmetics situations?,"Helloo...
Im kinda dumb in Discrete Mathematics since I just started studying it currently and I'm trying to figure out studying material to get remainders from operations like $11^{14^{4578369}}\pmod {41}$ or $11^{3928454} \pmod {3293}$ . Can someone gimme hints or even the path to follow for their solutions? I've tried using fermat's little theorem but I couldn't solve them anyway , cuz in the first case it's kinda power from power situation and the other one the mod isn't prime. Note: Here's a transcription of the exercises that uses these operations that im talking about. Thanks, though. Considering $a=11^{3928454}$ , $b=3293$ and $x$ as the
remainder from division of $a$ by $b$ . Which is the addition of all $x$ digits? a. 1 b. 2 c. 4 d. 3 Considering $a=11^{14^{4578369}}$ and $b=41$ . Which is the remainder from division
of $a$ by $b$ . a.40 b.38 c.37 d.39 EXTRA NOTE: I don't want the correct alternatives, I only want to figure out how to use Fermat's Little theorem in these situations !!!!","['modular-arithmetic', 'discrete-mathematics', 'computer-science']"
3764950,Bounded linear map which is not continuous,"Definition: A subset $B$ of the TVS $E$ is said to be bounded if to every neighborhood of zero $U$ in $E$ there is a number $\lambda >0$ such that $B \subset \lambda U.$ Definition: Let $E,F$ be two TVS, and $u$ a linear map of $E$ into $F$ . Let us say that $u$ is bounded if, for every bounded subset $B$ of $E$ , $u(B)$ is a bounded subset of $F$ . We have the following result: Theorem: Let $E$ be a metrizable space TVS. If a linear map of $E$ into a TVS $F$ is bounded, it is continuous. My question: Is there a counterexample of a bounded linear map which is not continuous? If this example exists, the space $E$ cannot be metrizable.","['continuity', 'topological-vector-spaces', 'functional-analysis']"
3764956,"Is $(\mathbb{Z}, \times)$ also a group?","Addition $+$ is a closure operation for set of integers ( $\mathbb{Z}$ ) The identity element for set of integers is $0$ Definition of group: Each group is a set of elements with one operation $*$ and  is closed under $*$ . Each element in the group has an inverse. Each element combine with its inverse gives the identity element $e$ . So, $(\mathbb{Z}, +)$ is a group Is $(\mathbb{Z}, \times)$ also a group?","['abstract-algebra', 'discrete-mathematics', 'arithmetic', 'group-theory', 'computer-science']"
3764959,Evaluating $I=\oint \frac{\cos(z)}{z(e^{z}-1)}dz$ along the unit circle,Evaluate the following along the unit circle: $$I=\oint \frac{\cos(z)}{z(e^{z}-1)}dz$$ I tried doing it by $$f(z)=\frac{\cos(z)}{e^{z}-1}$$ Then the integral would be: $$I=2\pi if(0)$$ The problem is that $f(0)$ gives me $\frac{1}{0}$ . So how do I solve it?,"['complex-analysis', 'calculus', 'contour-integration', 'complex-integration']"
3764966,Can $y=10^{-x}$ be converted into an equivalent $y=\mathrm{e}^{-kx}$?,"I was dealing with the values: | Digits | Expression | Value                 |
|--------|------------|-----------------------|
| 1      | 10⁻¹       | 0.1                   |
| 2      | 10⁻²       | 0.01                  |
| 3      | 10⁻³       | 0.001                 |
| 4      | 10⁻⁴       | 0.0001                |
| 5      | 10⁻⁵       | 0.00001               |
| 6      | 10⁻⁶       | 0.000001              |
| 7      | 10⁻⁷       | 0.0000001             |
| 8      | 10⁻⁸       | 0.00000001            |
| 9      | 10⁻⁹       | 0.000000001           |
| 10     | 10⁻¹⁰      | 0.0000000001          |
| 11     | 10⁻¹¹      | 0.00000000001         |
| 12     | 10⁻¹²      | 0.000000000001        |
| 13     | 10⁻¹³      | 0.0000000000001       |
| 14     | 10⁻¹⁴      | 0.00000000000001      |
| 15     | 10⁻¹⁵      | 0.000000000000001     | And then I plotted the results in Excel on a log scale: Now, I already know the formula for this graph, it's: $$ y = 10^{-x} $$ But was curious to see how well an ""exponential"" trendline would fit, and it fits very well : The $R^2$ is $1$ , even for $15$ decimal places. So it seems that: $$y = 10^{-x} ↔ y = e^{-2.30258509299405x} $$ The question So I have to wonder: is there an algebraic transformation of: $$y = 10^{-x} → y = e^{-kx} $$ Where does the constant $k$ come from? Does it have an expression? Or is this all a very interesting coincidence?",['linear-algebra']
3764989,Canonical connection form on 2-dim surface and Gaussian Curvature,"Let M be a 2-dimensional Riemannian manifold. Let $X_1,X_2$ be an orthonormal frame (w.r.t Riemannian metric) in an open set $U$ . Let $(\omega_1,\omega_2)$ be the dual coframe to $(X_1,X_2)$ . a) Prove that there exists the unique 1-form $\omega_2^1$ such that $$d\omega_1=\omega_2^1\wedge \omega_2 \text{ and } d\omega_2=-\omega_2^1\wedge \omega_1$$ The form $\omega^1_2$ is called the canonical connection form of the frame $(X_1, X_2)$ . b) Let $\tilde{X_1},\tilde{X_2}$ be another orthonormal frame defining the same orientation on $U$ . Let $\tilde{\omega_1}, \tilde{\omega_2}$ be the corresponding dual coframe. Let $\tilde{\omega}_2^1$ be the canonical connection form of the frame ( $\tilde{X}_1,\tilde{X}_2$ ). Prove that $d\omega_2^1=d\tilde{\omega}_2^1$ c) Let $K$ and $\tilde{K}$ be two functions on U such that $$d\omega_2^1=K\omega_1\wedge \omega_2 \text{ and } d\tilde{\omega_2^1}=\tilde{K} \tilde{\omega}_1\wedge \tilde{\omega}_2$$ Show that $K=\tilde{K}$ . Solution: a) Since $\omega_1, \omega_2$ form a coframe, $$\omega_2^1=f_1\omega_1+f_2\omega_2, \text{ where } f_1=\omega_2^1(X_1), f_2=\omega_2^1(X_2)$$ $d\omega_1=\omega_2^1\wedge\omega_2 \implies d\omega_1(X_1,X_2)=\omega_2^1(X_1)$ $d\omega_2=-\omega_2^1\wedge\omega_2 \implies d\omega_2(X_1,X_2)=\omega_2^1(X_2)$ Therefore $\omega_2^1=d\omega_1(X_1,X_2)\omega_1+d\omega_2(X_1,X_2)\omega_2$ . This shows the existence and uniqueness of the canonical connection form. Further, it is easily seen that the conditions are satisfied. b) Since $X_1,X_2$ is a frame $$\tilde{X}_1=a_1X_1+a_2X_2 \text{ and } \tilde{X}_2=b_1X_1+b_2X_2$$ By orthonormality, $\langle\tilde{X_i},\tilde{X}_j\rangle=\delta_{i,j}$ . We get $a_1^2+a_2^2=1=b_1^2+b_2^2$ and $a_1b_1+a_2b_2=0$ . Assume $a_1=\cos\theta$ $a_2=\sin\theta$ and $b_1=\sin\phi$ $b_2=\cos\phi$ . We get $\theta=-\phi$ .  So, $$\tilde{X}_1=\cos\theta X_1+\sin\theta X_2 \text{ and } \tilde{X}_2=-\sin \theta X_1+\cos\theta X_2; \text{ where } \theta \text { is a smooth function on U}$$ Similarly, $\tilde{\omega_i}(\tilde{X}_j)=\delta_{i,j}$ gives $$\tilde{\omega}_1=\cos\theta \omega_1+\sin\theta \omega_2 \text{ and } \tilde{\omega}_2=-\sin \theta \omega_1+\cos\theta \omega_2; $$ I don't know how to continue further. Any help is appreciated. Thanks","['curvature', 'riemannian-geometry', 'differential-geometry']"
3764995,"Looking for the proof of theorem 5.2.11 of Casella, Berger, Statistical Inference","Theorem 5.2.11 Suppose $X_1,\dots, X_n$ is a random sample from a pdf or pmf $f(x\mid \theta)=h(x)c(\theta)\exp(\sum_{i=1}^kw_i(\theta)T_i(x))$ is in exponential family. Define statistics $T_i=\sum_it_i(X_j)$ where $i=1,\dots, k$ . If the set $\{(w_1(\theta),\dots,w_k(\theta))\}$ contains some open subset of $\mathbb{R}^k$ , then the distribution of $(T_1,\dots, T_k)$ is an exponential family of the form $g(u_1,\dots, u_k\mid\theta)=H(u_1,\dots, u_k)c(\theta)^n \exp(\sum_iw_i(\theta)u_i)$ Q: I am looking for a proof of the theorem or reference of proof. How is containing open set for $(w_1(\theta),\dots,w_k(\theta))$ used to derive transformation? All I could see is that somehow Jacobian between $(T_i)$ 's and $(X_i)$ 's is accounted by part of $H$ but this may not be 1-1.","['statistical-inference', 'statistics', 'probability']"
3765027,Moduli Space of Tori,"I'm looking at an exercise that reads: Problem. Let $A = \begin{bmatrix}a & b\\c & d\end{bmatrix} \in GL(2, \mathbb{C})$ and $\Lambda = \langle z \mapsto z + \omega_1, z \mapsto z + \omega_2\rangle$ with $\omega_1, \omega_2$ linearly independent over $\mathbb{R}$ . Prove that if $\Lambda = \langle z \mapsto z + \omega_1', z \mapsto z + \omega_2'\rangle$ where $$
\begin{bmatrix}\omega_1'\\\omega_2'\end{bmatrix} = A\begin{bmatrix}\omega_1\\\omega_2\end{bmatrix}
$$ then $A \in GL(2, \mathbb{Z})$ with $\det(A) = \pm1$ . Now show that the moduli space of tori, defined to be the space of all conformal equivalence classes of tori, is $\mathbb{H}/PSL(2, \mathbb{Z})$ . I've done the first bit, but not sure how to proceed with the second. In particular, I'm not sure how to visualise $\mathbb{H}/PSL(2, \mathbb{Z})$ and hence find a bijection between that and the equivalence classes.","['complex-analysis', 'moduli-space']"
3765051,Prove $\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3)$,How to prove that $$\lim_{n\mapsto 0}[(\psi(n)+\gamma)\psi^{(1)}(n)-\frac12\psi^{(2)}(n)]=2\zeta(3)\ ?$$ I encountered this limit while I was trying to solve $\int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx$ using the derivative of beta function but I have no idea how to tackle this limit. We know that this integral is very simple : $$\int_0^1\frac{\ln x\ln(1-x)}{x(1-x)}dx=\int_0^1\frac{\ln x\ln(1-x)}{x}dx+\underbrace{\int_0^1\frac{\ln x\ln(1-x)}{1-x}dx}_{1-x\to x}$$ $$=2\int_0^1\frac{\ln x\ln(1-x)}{x}dx=2\zeta(3)$$ but using integration does not always work for high-power log integrals and beta function would be the right tool but my problem is only when $n\to 0$ . Any help would be appreciated. Note: No solutions using asymptotic expansion please.,"['polygamma', 'beta-function', 'gamma-function', 'harmonic-numbers', 'limits']"
3765093,Definite integral using properties,"Find the value of the following integral. $$\int_{0}^{4\pi} \ln|13\sin x+3\sqrt3 
\cos x|\;\rm dx$$ My attempt : Using the properties of definite integral, I converted it to this integral. $$4\int_{0}^{\pi} \ln|14\sin(x+\arctan(\frac{3\sqrt3}{13}))|\;\rm dx$$ Please tell me how to proceed further.","['integration', 'definite-integrals', 'inverse-function', 'calculus', 'functions']"
3765152,Solve $x^2+3y = u^2$ and $y^2+3x=v^2$ in positive integers.,"The question is from the pg - 59 from ' An Introduction to Diophantine Equations ' by Titu Andreescu , Dorin Andrica , Ion Cucurezeanu. Example 1 : Solve in positive system of equations in positive integers $$\begin{cases} x^2+3y = u^2 \\ y^2 + 3x = v^2 \end{cases}$$ $\;\;\;\;\;\;\;\;\;\,\,\,\,\,\,\,\,\,\,\,\,\text{(Titu Andreescu)}$ Solution. The inequality $x^2 + 3y ≥ (x + 2)^2 , y^2 + 3x ≥ (y + 2)^2$ cannot both be true, because adding them would yield a contradiction. So at least one of the inequalities $x^2 + 3y < (x + 2)^2$ and $y^2 + 3x < (y + 2)^2$ is true. Without loss of generality, assume
that $x^2 + 3y < (x + 2)^2$ . Then $$x^2 < x^2 + 3y < (x + 2)^2 \implies
x^2 + 3y = (x+1)^2$$ or, $3y = 2x+ 1$ . We obtain $x = 3k + 1, y = 2k + 1$ for some nonnegative integer $k$ and $y^2 + 3x = 4k^2 + 13k + 4$ . For $k > 5, (2k+ 3)^2 < 4k^2 + 13k+ 4 < (2k+ 4)^2$ ; hence $y^2 + 3x$ cannot be
a perfect square. Thus we need only consider $k ∈ {\{0, 1, 2, 3, 4\}}$ . Only $k = 0$ makes $y^2 + 3x$ a perfect square; hence the unique solution is $$x = y = 1,\;\;\;\;\;\; u = v = 2.$$ But if we take  , $$4k^2+13k + 4 = v^2$$ $$\implies k = \dfrac{-13 \pm\sqrt{105+16v^2}}{8}$$ Since $105+16v^2 = a^2 \implies 105 = (a-4v)(a+4v)$ which gives $a \in \{\pm11 , \pm13 , \pm19 ,\pm53\}$ .
Out of these , only $a \in \{ \pm13 , \pm53\}$ works which gives $k=0,5$ , And so the the answer should be $$(x,y,u,v) = (1,1,2,2)\;\;\;,(16,11,17,13)\;\;\;\;,(11,16,13,17)$$ Who is correct here?","['contest-math', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'algebra-precalculus']"
3765202,Calculation of $\left(\frac{1}{\cos^2x}\right)^{\frac{1}{2}}$,"Shouldn't $\left(\frac{1}{\cos^2x}\right)^{\frac{1}{2}} = |\sec(x)|$ ?
Why does Symbolab as well as my professor (page one, also below) claim that $\left(\frac{1}{\cos^2x}\right)^{\frac{1}{2}} = \sec(x)$ , which can be negative? Also, the length of a vector cannot be negative... isn't it?",['trigonometry']
3765211,Showing an infinite sequence is constant under some condition,"Let $a_1,a_2,...$ be an infinite sequence of positive real numbers such that for each positive integer $n$ we have $$
\frac{a_1+a_2+..+a_n}n\ge\sqrt{\frac{a^2_1+a^2_2+...+a^2_{n+1}}{n+1}}.
$$ Prove that the sequence $a_1,a_2,...\ $ is constant. MY ATTEMPT/THOUGHTS: My initial plan is to show that the sequence is bounded and then proving it is constant. For that I considered the following. Let $m_n=\min\{a_1,a_2,...,a_n\}$ , $M_n=\max\{a_1,a_2,...,a_n\}$ , and $$S_n=\frac{a^2_1+a^2_2+...+a^2_n}{n}.$$ Then we have $$m^2_n\le S_{n+1} \le M^2_n.$$ Also from the given inequality, we have, on squaring, $$\frac{1}{n}S_n+2\frac{a_1a_2+a_1a_3+....+a_{n-1}a_n}{n^2}\ge S^2_{n+1}.$$ I have no idea how to proceed after this or even if I am moving in the right direction! Do you have any suggestions? Thanks for your time.","['contest-math', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
3765253,How to prove that these functions do not intersect?,"I want to prove that these two functions $f(x)$ and $g(x)$ do not intersect for $x>1$ : $$f(x)=\cosh \left(\frac{2 \sqrt{2} \pi  x \left(x^2-1\right) \cosh (\pi  x)}{\sqrt{x^4+6 x^2+\left(x^2-1\right)^2 \cosh (2 \pi  x)+1}}\right)$$ $$g(x)=\frac{4 x^2+\left(x^2-1\right)^2 \cosh (2 \pi  x)}{\left(x^2+1\right)^2}$$ Both functions are greater than one and strictly increasing. Subtraction and taking derivative do not work, the problem becomes more complicated. Does anyone have an idea to prove it by assuming false assumption? Or to prove that $f(x)-g(x)$ has no real root? Any hints or suggestions are really appreciated.","['alternative-proof', 'functions', 'roots', 'monotone-functions']"
3765261,Show that the number of maximum fresh lines thus introduced is $\displaystyle\frac{n(n-1)(n-2)(n-3)}{8}$. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question There are $n$ straight lines in a plane in which no two are parallel and no three pass through the same point. Their points of intersection are joined. Show that the number of maximum fresh lines thus introduced is $\displaystyle\frac{n(n-1)(n-2)(n-3)}{8}$ . Though I tried this questions many times but I'm kinda stuck","['combinatorics', 'discrete-mathematics']"
3765262,Critical Points of $dy/dx=0.2x^2\left(1-x/3\right)$,"I am trying to determine the critical points of the ODE $$\frac{dy}{dx}=0.2x^2\left(1-\frac{x}{3}\right).$$ Setting the right-hand-side to zero gives two solutions, namely $x=0$ and $x=3$ . I was wondering if the $x^2$ means that there's two critical points at $x=0$ . That is, there are two critical points at $x=0$ and one critical point at $x=3$ .",['ordinary-differential-equations']
3765294,"Find the dimension of $V = \{f \in C^k [0, 1] : a_n f^{(n)}(t)+\cdots+a_0 f (t) = 0 \textrm{ for all } t \in [0, 1]\}$.","Using the additive properties of differentiation I proved that $V$ is a subspace. The fact that $\overline{O} \in V$ is also trivial. However, I am stuck as to how to use the property of the continuity and differentiability of $f \in V$ to find out the dimension of $V$ . Any help would be appreciated. EDIT: The $a_i$ from $i = 0$ to $i = n $ are fixed and are $\in \Bbb{R}$ .","['linear-algebra', 'vector-spaces', 'ordinary-differential-equations', 'real-analysis']"
3765302,A right triangle has a certain angle twice of another angle in the triangle. Find the maximum number of integer side lengths it has.,"A right triangle has a certain angle twice of another angle in the triangle. Find the maximum number of integer side lengths it has. How I tried working on the problem: There are $2$ possible triangular angles that satisfy this, $30, 60, 90$ triangle $45, 45, 90$ triangle For $30, 60, 90$ triangle, the ratio is $1:\sqrt{3}:2$ .
For $45, 45, 90$ triangle, the ratio is $1:1:\sqrt{2}$ . How should I continue working on this problem?",['geometry']
3765394,Is this series $\sum_{n=0}^{\infty} 2^{(-1)^n - n} = 3 $ or $ \approx 3$,"Is this series $\sum_{n=0}^{\infty} 2^{(-1)^n - n} = 3 $ or $ \approx 3$ Hello all.
In Ross' elementary Analysis Chapter 14 there is an example of the above series.
It simply says it converges.
I have computed that it converges to 3.
The way I did it is that I thought: $$2 + \frac{1}{4} + \frac{1}{2} + + \frac{1}{16} + + \frac{1}{8} + + \frac{1}{64} + + \frac{1}{32}....$$ can be broken up to $$ 2 + \sum_{n=1}^{\infty} (\frac{1}{4})^n + \frac{1}{2}\sum_{n=0}^{\infty} (\frac{1}{4})^n$$ Which is 2 + 2 geometric series that I computed to $\frac{1}{3} $ and $ \frac{2}{3}$ respectively getting exactly 3.
Using the formula I learned first term divided by 1 - ratio. Now wolfram alpha says this is only approximately 3.
Did I go wrong somewhere above?
Or could it be that there is some rule that makes me not be able to give an exact result?
I apologize if this is very basic. I have been thinking about this since yesterday. Thanks a lot!","['sequences-and-series', 'real-analysis']"
3765401,Find all functions $f:\mathbb{R}^+\to \mathbb{R}$ such that $xf(xf(x)-4)-1=4x$,"Find all functions $f:\mathbb{R}^+\to \mathbb{R}$ such that for all $x\in\mathbb{R}^+$ the following is valid: $$xf\big(xf(x)-4\big)-1=4x$$ All I could do is: $f(x)> {4\over x}$ for all $x$ so $f(x)>0$ for all $x$ . $(4,\infty )\subseteq {\rm Range}(f)$ , since $$f(xf(x)-4)={4x+1\over x} >4$$ Function $g(x)=xf(x)-4$ is injective: \begin{align}g(x_1)=g(x_2) &\implies  f(g(x_1))=f(g(x_2))\\&\implies {4x_1+1\over x_1}={4x_2+1\over x_2} \\&\implies x_1=x_2\end{align} Function $g$ satisfies $$\boxed{xg(g(x)) -(4x+1)g(x)+4x=0}$$","['contest-math', 'functional-equations', 'real-analysis']"
3765458,Fourier transform of $1/ \sqrt{m^2+p_1^2+p_2^2+p_3^2}$,"Let $m>0$ and consider the function $f:\mathbb R^3\to\mathbb C$ defined through $$ f(p_1,p_2,p_3) = \frac{1}{\sqrt{m^2+p_1^2+p_2^2+p_3^2}}.$$ I would like to compute the Fourier transform of $f$ . This particular function is of interest as one which naturally appears in some problems of special relativity. What I already know: Although $f$ is neither integrable nor square integrable, the Fourier transform of $f$ is well defined as the Fourier transform of a tempered distribution. Using symbolic calculus software, I expect that $\int_{-\infty}^{+\infty}f(p_1,p_2,p_3) e^{-i x_1p_1} dp_1 = 2K_0(x_1 \sqrt{m^2+p_2^2+p_3^2})$ , where $K_0$ is a modified Bessel function of the second kind. Questions: Is the Fourier transform of $f$ explicitly computable? If it is, how could I compute it?","['multivariable-calculus', 'multiple-integral', 'definite-integrals', 'fourier-transform']"
3765477,Prove that $xg(x)<\int_{0}^xg(x)dx$,Let $f(x)$ satisfy the differential equation $$\frac{d(f(x))}{dx}+f(x)=g(x)$$ where $f(x)$ and $g(x)$ are continuous functions. Also it is known that $f(x)$ is a decreasing function of $x$ for all positive x. Prove that $$xg(x)<\int_{0}^xg(x)dx$$ My Attempt: Let $H(x)=xg(x)-\int_{0}^xg(x)dx$ $H'(x)=xg'(x)$ What can we say about $g(x)$ and $g'(x)$ . I think some information seems to be missing. If it is given that $f(0)\geq 0$ something may be worked out.,"['integration', 'calculus', 'ordinary-differential-equations', 'real-analysis']"
3765491,Question from J. Milnor paper from 1968 about diffeomorphic manifolds,"In the article "" A note on curvature and fundamental group ""(1968)  by J. Milnor  the following side question arises: where $G$ and $H$ are continuous (over $\mathbb{R}$ ) and discrete (over $\mathbb{Z}$ ) Heisenberg $3\times 3$ matrix group. The fundamental group of orbit space G/H is isomorphic to the nilpotent group $H$ (and it follows from introductory facts about algebraic topology, e.g "" Algebraic Topology - A First Course "" W. Fulton, Corollary 13.16). But I guess the author's doubt may not easy to answer, but the paper is known and maybe somebody knows the answer and could lighten up the problem. *I supply an above entry with another statement (from "" Treatise on Analysis "" Volume III, Chapter XVI by J. Dieudonne, e.g statement 16.10.3) related to a unique differentiable structure on $G/H$ : Let $G$ be a Lie transformation group of a manifold $M$ , $M/G$ the orbit space topologized by the finest topology for which the natural mapping $\pi: M\to M/G$ is continuous. Let \begin{equation*}
D=\{(p,q)\in M\times M: \exists_{g\in G}\,\, p=g\cdot q\}
\end{equation*} Then the following statements are true: (i) $M/G$ is a closed Hausdorff space if and only if the subset $D\subset M\times M$ is closed. (ii) There exists a differentiable structure on the topological space $M/G$ such that $\pi:M\to M/G$ is a submersion if and only if the topological subspace $D\subset M\times M$ is a closed submanifold. In this case, the differentiable structure is unique and all $G$ -orbits in $M$ have the same dimension.","['heisenberg-group', 'differential-topology', 'algebraic-geometry', 'algebraic-topology']"
3765506,"Fourier-like family total in $L^2(-\pi,\pi)$","Consider the Hilbert space $H=L^2(-\pi,\pi)$ . A subset of $H$ is said to be total in $H$ if the closure of its span is the whole $H$ . For instance, the Fourier basis $\{e^{in x}\}_{n\in\mathbb{Z}}$ is a Hilbert basis for $H$ and in particular it is total in $H$ . I am interested in families of the form $\{e^{is_n x}\}_{n\in\mathbb{Z}}$ , where $\mathcal{S}=\{s_n\}_{n\in\mathbb{Z}}$ is a strictly increasing sequence. Is there any easy condition on $\mathcal{S}$ which guarantees that such a family is total in $H$ ? I would guess that $S$ should at least be diverging, in order to capture arbitrarily high frequencies, but I'm not sure that it is actually a necessary condition. Of course not every increasing sequence $\mathcal{S}$ has this property. An easy counter example is any $\mathcal{S}\subsetneq \mathbb{Z}$ , since the obtained family would be a proper subset of the Fourier basis. However I guess there must be some easy and quite general criterion to check whether a sequence gives rise to a family total in $H$ .","['hilbert-spaces', 'linear-algebra', 'functional-analysis', 'fourier-analysis']"
3765508,Infinite Burnside lemma,"Burnside's lemma says that if a finite group $G$ acts on a finite set $X$ then the number of orbits equals the average number of fixed points: $$|X/G| = \frac{1}{|G|} \sum_{g \in G} |X^g|.$$ I was wondering if there's anything that can be said in general, say in terms of measures: $$\# X/G = \frac{1}{\mu(G)} \int_G \#X^g d\mu?$$ Here are two examples to show that something like this could work under certain hypotheses (as yet unknown to me). Example 1. Let $G$ be the group of invertible affine linear transformations $x \mapsto ax+b$ of $\mathbf{R}$ and let $X = \mathbf{R}$ . Then the natural action of $G$ on $X$ is transitive (just translate), so $\#X/G = 1$ .
Now since $$ax + b = x \iff x = \frac{b}{1-a} \quad (a \ne 1)$$ we see that $\#X^g = 1$ generically, so the ""average"" number of fixed points is 1 also. Example 2. Let $G = \langle -1 \rangle$ this time and keep $X = \mathbf{R}$ . There are infinitely many orbits, namely $\{0\}$ and $\{x, -x\}$ for all $x \ne 0$ . Here, $1$ fixes everything while $-1$ fixes only 0. Under the usual conventions surrounding arithmetic with $\infty$ , the average number of fixed points is $$\frac{1}{2}\big(1 + \infty\big) = \infty$$ which is, again, the number of orbits.","['group-theory', 'infinite-groups', 'measure-theory']"
3765513,Can we recover left-hand derivative from right-hand derivative,"Consider a convex function $f(x)$ on interval $(a,b) \subseteq \mathbb{R}$ . According to ""A user's guide to measure theoretic probability"" by Pollard (see Appendix C), its right-hand $D_{+}(x)$ and left-hand $D_{-}(x)$ derivatives $D_{-}(x_0)
=
\lim_{x \rightarrow x_0^{-}}
\frac{f(x) - f(x_0)}{x - x_0}
,
\quad
D_{+}(x_0)
=
\lim_{x \rightarrow x_0^{+}}
\frac{f(x) - f(x_0)}{x - x_0}$ exist at any $x_0 \in (a,b)$ . Further, we know that $D_{+}(x)$ is increasing and right-continuous, and $D_{-}(x)$ is increasing and left-continuous, w.r.t. domain $(a,b)$ . My question is following: given $D_{+}(x)$ , can we recover $D_{-}(x)$ ? If no, then under what additional conditions it is possible? My current guess is to do the recovery as: $D_{-}(x_0) = \lim_{x \rightarrow x_0^{-}} D_{+}(x)$ Yet, I'm not sure if the left-hand limit exist at $D_{+}(x_0)$ . Is the above statement correct? How can we prove it?","['limits', 'convex-analysis', 'derivatives', 'functional-analysis']"
3765546,Brouwer's fixed point theorem and the one-point topology,"In the 2-dimensional case, Brouwer's fixed point theorem (BFPT) says that every continuous function $D^2\to D^2$ has a fixed point, where $D^2$ is the disk. Now fix a particular topology: pick some point $x_0\in D^2$ and use it to define the one-point topology $\cal T_0$ on $D^2$ : it includes all sets $A$ with $x_0\in A$ , and the empty set. (This is indeed a topology, see for example https://en.wikipedia.org/wiki/Particular_point_topology ). With respect to $\cal T_0$ , a self map $D^2\to D^2$ is continuous if and only if it is constant or has $x_0$ as a fixed point. So, for every self map on $D^2$ , continuity with respect to $\cal T_0$ means that a fixed point exists. Hence the BFPT is trivially true, by definition of $\cal T_0$ . In conclusion, there are topologies where BFPT is a theorem that requires proof, and there is a topology $\cal T_0$ where BFPT is true simply by definition. This gives $\cal T_0$ a special place among all possible topologies on $D^2$ : it is the topology that makes BFPT trivial. Does such a situation or property have a name? Does it have a category theory interpretation (maybe like ""universal property"")? I feel there is a certain equivalence between BFPT and $\cal T_0$ here. They characterize each other in a certain way: $\cal T_0$ makes BFPT trivially true by definition, and BFPT links continuity and fixed points (like $\cal T_0$ does). Can this sense of equivalence be expressed rigorously? EDIT: I want to thank you for the comments which were very helpful for me. I still seem to struggle with the question in my head, so I edited my question with the intention to make it more precise.","['fixed-point-theorems', 'general-topology', 'logic', 'category-theory']"
3765564,Eight points in a fixed circle,"In the below figure, the point $P$ is inside the ellipse, $A, B, C, D$ are points on the ellipse, and segment $AB$ is perpendicular and intersects with segment $CD$ at $P$ . The line $PE$ is perpendicular and intersects with segment $AC$ at $E$ , and the line $PE$ intersects with the opposite segment $BD$ at $F$ . The line $PG$ is perpendicular and intersects with segment $AD$ at $G$ , and the line $PG$ intersects with the opposite segment $BC$ at $H$ . The line $PI$ is perpendicular and intersects with segment $BD$ at $I$ , and the line $PI$ intersects with the opposite segment $AC$ at $J$ . The line $PK$ is perpendicular and intersects with segment $BC$ at $K$ , and the line $PK$ intersects with the opposite segment $AD$ at $L$ . How to prove that the eight points $E, F, G, H, I, J, K, L$ are in the same circle and this circle's center and radius are only determined by point $P$ 's position, in other words, the positions of $A, B, C, D$ don't influence this circle's center and radius. I have investigated this problem when the ellipse is converted to a circle, wolfram mathworld and Eight points in a circle , but I don't know whether it can help to solve the ellipse problem. If the ellipse function is $\frac{x^2}{a^2} + \frac{y^2}{b^2} = 1$ , the coordinates of point $P$ is $(s, t)$ and guarantees $\frac{s^2}{a^2} + \frac{t^2}{b^2} < 1$ , give this circle's equation.",['geometry']
3765584,Proper flat morphism with geometrically connected and smooth generic fiber,"I am trying to understand the proof of the following statement in http://virtualmath1.stanford.edu/~conrad/mordellsem/Notes/L13.pdf Lemma 7.1 . Let $f:X\to S$ be a proper flat surjective map to a noetherian scheme $S$ , and assume  that $f$ has  geometrically  connected  and  smooth  generic  fibers.   Then  all  fibers  are geometrically connected. Proof . We may and do assume that $S$ is reduced and irreducible (by base change to irreducible  components  of $S$ ,  equipped  with  the  reduced  structure).   For  a  non-generic  point $s\in S$ there  is  a  discrete  valuation  on  the  function  field  of $S$ that  dominates $O_{S,s}$ [EGA,II, 7.1.7], so by base change to such a ring we can assume that $S=\operatorname{Spec} R$ for a discrete valuation ring $R$ .  Let $K=\operatorname{Frac}(R)$ . By $R$ -flatness of $X$ and smoothness and geometric connectedness of the generic fiber, the $R$ -finite $H^0(X,O_X)$ injects into $H^0(X_K,O_{X_K})=K$ .  Thus, $R=H^0(X,O_X)$ by the normality of $R$ .  That is, $X\to\operatorname{Spec} R$ is its own Stein factorization.  But Stein factorizations always have geometrically connected fibers [EGA, III1, 4.3.4]. So I am not sure where the smoothness condition is needed exactly, it looks like we only need that the generic fiber is geometrically reduced.
Indeed, the injection $H^0(X,O_X) \to H^0(X_K,O_{X_K})$ only require the flatness of $X$ : the restriction to $X_K$ is injective if $X_K$ contains all associated primes, but since $X$ is flat over $R$ all associated primes live in the generic fiber. And $H^0(X_K,O_{X_K})=K$ only requires $X_K$ to be geometrically connected and geometrically reduced. So it looks like the following statement is true: Lemma . Let $f:X\to S$ be a proper flat surjective map to a noetherian scheme $S$ , and assume  that $f$ has  geometrically  connected  and  reduced  generic fibers.   Then  all  fibers  are geometrically connected. Am I missing something?","['algebraic-geometry', 'schemes']"
3765589,"How to prove that $\int_0^1 f(x)\,dx = f(0) + \frac{1}{2}f'(c)$ for some $ c \in [0,1]$?","Given that ${f}$ is differentiable on the interval $[0,1]$ I need to prove that $\int_0^1 f(x)dx = f(0) + \frac{1}{2} f'(c)$ for some $ c \in [0,1]$ . I'm aware of integral mean value theorem, which gives us the following: Exists point $c \in [0,1]: {f}(c) = \frac{1}{1} \int_0^1 f(x)\,\mathrm{d}x$ But I can't get further and it looks like that's not the right way at all. I'll be happy to get any tips or key statements that will lead me to the solution, please.","['calculus', 'definite-integrals']"
3765590,Solve second order DE: $\sin^2 x\frac{d^2y}{dx^2} = 2y$,"Solve: $$\sin^2 x \frac{d^2y}{dx^2} = 2y $$ So what I did was separation of variables, it got me $$\frac{y''} {2y}= \csc^2 x$$ and integrating both sides will give $$\frac{y'}{2y}= -\cot x+ C $$ On another integration we will get $$\frac{\ln y}{2} = -\ln(\sin x) + Cx + K$$ then we get $$\ln y = -2\ln(\sin x) + 2Cx + 2K$$ and so my answer is $$y = e^{-2\ln(\sin x) + 2Cx}$$ but I believe it is wrong.","['calculus', 'ordinary-differential-equations']"
3765594,How to calculate the $p$-torsion points of an elliptic curve?,"How to calculate the $p$ -torsion points of an elliptic curve? Consider the elliptic curve $E: \ y^2=x^3-5$ over $\mathbb{Q}$ . Then it is given that $E[2]=\{0,~(\sqrt[3]{5},0),~(\zeta_2 \sqrt[3]{5},0),~(\zeta_3^2 \sqrt[3]{5},0) \}$ . see for instance Page $2$ here Clearly these points satisfy the elliptic curve though they do not belong to $\mathbb{Q}$ . But I didn't see how these are $2$ -torsion points. Can you help me to explain? If $P=(x,y)$ be a $2$ -torsion point of $E: \ y^2=x^3-5$ , then $2P=0$ . Also, What are the $3$ -torsion points ? Do Pari/gp  calculate torsion points ?","['number-theory', 'elliptic-curves']"
3765595,Spiral equation,"Considering concentric arcs, of equal developed length, whose start point is aligned: I am looking for the equation of the spiral passing through the end points. Some help to solve this problem will be welcome! Edit: The result",['geometry']
3765597,"If $\exists$ a surjection from nonempty set $A$ onto $\mathcal{P}(\mathbb{Z_+})$, then $A$ is uncountable","I wanted to show this to use it in some other proofs.  I believe it holds.  My questions are, if this proposition is true and if the proof sufficiently shows it holds. Background From a previous proof, it uses the result that $\mathcal{P}(\mathbb{Z_+})$ , the power set of the positive integers, is uncountable. Theorem 7.1 nonempty set A is countable $\iff\exists$ a surjective function $h: \mathbb{Z_+} \to A$ (citation: Munkres Topology Chapter 7) Proof by Contradiction: Let $A$ be a nonempty set and suppose $A$ is countable. Assume $\exists$ a surjective function $h: A \to \mathcal{P}(\mathbb{Z_+})$ . Since $A$ is countable, $\exists$ a surjective function $f: \mathbb{Z_+} \to A$ , by Theorem 7.1. Consider the composition $h\circ f: \mathbb{Z_+} \to \mathcal{P}(\mathbb{Z_+})$ .  Since $f$ and $h$ are surjections, it follows that $h\circ f$ is a surjection ( $c\in \mathcal{P}(\mathbb{Z_+}) \Rightarrow \exists a\in A$ , such that $c = h(a) \Rightarrow \exists n\in \mathbb{Z_+}$ , such that $c = h(f(n))$ . This implies that $\mathcal{P}(\mathbb{Z_+})$ is countable, by Theorem 7.1, a contradiction.  Hence, $A$ is uncountable.","['elementary-set-theory', 'functions', 'solution-verification']"
3765610,"For a projection $\Pi$, is $\text{tr}(\Pi X)\leq \text{tr}(X)$?","All matrices are finite dimensional symmetric positive semidefinite matrices in this question. Let $\Pi$ be projection i.e. in its eigenbasis, it is the the identity matrix with some diagonal elements replaced by $0$ . Let $X$ be an arbitrary symmetric positive definite matrix. Is it true that $$\text{tr}(\Pi X)\leq \text{tr}(X)$$ Using the answer here , I see that it is indeed true that $\text{tr}(\Pi X)\leq \text{rank}(\Pi)\text{tr}(X)$ but I was hoping the rank term could also be dropped.","['matrices', 'trace', 'projection-matrices', 'linear-algebra']"
3765640,Let $G$ be a finite nilpotent group and $G'$ its commutator subgroup. Show that if $G/G'$ is cyclic then $G$ is cyclic.,"So I thought the cleanest way to do this was to simply prove $G' = 1$ since if $G$ is cyclic $G' = 1$ and then $G \cong G/G'$ , but I got no where with this. My next idea was since $G$ is nilpotent I know it's the direct product of normal Sylow subgroups which commute with one another, so it suffices to show that each Sylow subgroup is cyclic. If $P$ is a Sylow $p$ subgroup then $PG'/G'$ is cyclic so by the second isomorphism theorem so is $P/P\cap G'$ . So if $P$ intersects $G'$ trivial y then $P$ is cyclic. But I don't know what to do with the case where $P\cap G' \neq 1$ .","['cyclic-groups', 'finite-groups', 'nilpotent-groups', 'sylow-theory', 'group-theory']"
3765655,How to reduce this Riccati ODE to a $1^{st}$ order linear ODE: $y'=1+x-(1+2x)y+xy^2$?,"I am trying to solve this differential equation: $$ y'=1+x-(1+2x)y+xy^2 \quad (E_y)$$ that has a given partial solution $y_1(x)=1$ This equation is apparently Riccati form. The theory states that the following substitution will reduce the ODE to a linear first order form: $$(y(x)\neq 1): u(x) = \frac{1}{y(x)-y_1(x)} = \frac{1}{y(x)-1} \iff $$ $$ \bbox[15px,#ffd,border:1px solid blue]{y(x)=1 + \frac{1}{u(x)} \quad(1)} \quad \bbox[15px,#ffd,border:1px solid blue]{\text{and}\quad  y'(x) =-\frac{u'(x)}{u^2(x)} \quad (2)}$$ Hence, plugging $(1),(2)$ in $(E_y)$ : $$ u'(x) = u^2(x)(2+x)+u(x)(2x+1)-x \quad (E_u)$$ Apparently, the substitution didn't change the form of the differential equation. We notice that $(E_u)$ is still Riccati form. Why did this happen and what is a proper way to solve this?","['calculus', 'ordinary-differential-equations']"
3765686,Understanding the multidimensional chain rule,"I have some trouble in understanding the multidimensional chain rule. For differentiable functions $f,g$ , defined by $f: U \to V$ , $g:V\to\mathbb K^n$ where $U \subseteq \mathbb R^d$ , $V \subseteq \mathbb R^\nu$ are open, $$\Big(\mathrm D(g \circ f)\Big)(x)=\Big(\mathrm D g(f(x))\Big)\cdot\Big(\mathrm Df(x)\Big).$$ I have rather accepted this identity than understood it. I already saw some examples and understood the fact that I did not understand something completely. By differentiating a multidimensional function, one apparentely differentiates the components  separately and somehow adds everything? Unfortunately I don't know exactly how... any explanation would be highly appreciated.","['multivariable-calculus', 'chain-rule']"
3765689,Number of possible n-towers,"This question is inspired by Q5 from the 2018 MAT, but it is not the same. An $n$ -brick is defined as a rectangle with height $1$ and width $n$ . A $1$ -tower is defined as a $1$ -brick.
An $n$ -tower for $n$ greater than or equal to two is an $n$ -brick atop which any number of towers are stacked. The $n$ -bricks directly above the first one should sum to $n$ (so if you had a $4$ -tower, you could put a $3$ -brick and a $1$ -brick on it, or two $2$ -bricks on it. The $3$ -brick would then have either three $1$ -bricks on it, or one $2$ -brick and one $1$ -brick; whereas the $2$ -brick would always have two $1$ -bricks).  Below is a picture of a $4$ -tower If $s_n$ is the number of n -towers, can you write $s_n$ in terms of $s_k$ where $k<n$ ?","['combinatorics', 'recurrence-relations', 'discrete-mathematics', 'recursion']"
3765705,"Evaluating S depending upon following condition: Calculate the sum $S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k},$","Suppose that $x_{1}, x_{2}, \ldots, x_{n}(n>2)$ are real numbers such that $x_{i}=x_{n-i+1}$ for $1 \leq i \leq n .$ Consider the sum $S=\Sigma \Sigma \Sigma x_{i} x_{j} x_{k},$ where summations are taken over all i, $j, k: 1 \leq i, j, k \leq n$ and $i, j, k$ are all distinct. Then S equals $S=\sum \Sigma x_{i} x_{j}\left(L-x_{i}-x_{j}\right) i \neq j$ where $L=x_{1}+x_{2}+\ldots+x_{n}$ $=L \sum \Sigma x_{i} x_{j}-\sum \Sigma x_{i}^{2} x_{j}-\sum \Sigma x_{i} x_{j}^{2}$ $=\mathrm{L} \sum \mathrm{x}_{\mathrm{i}}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}^{2}\left(\mathrm{L}-\mathrm{x}_{\mathrm{i}}\right)-\Sigma \mathrm{x}_{\mathrm{i}}\left(\mathrm{M}-\mathrm{x}_{\mathrm{i}}^{2}\right)$ where $\mathrm{M}=\mathrm{x}_{1}^{2}+\mathrm{x}_{2}^{2}+\ldots .+\mathrm{x}_{\mathrm{n}}^{2}$ $=\mathrm{L}^{3}-3 \mathrm{LM}+2 \Sigma \mathrm{x}_{\mathrm{i}}^{3}$ What to do next?","['measure-theory', 'summation', 'solution-verification', 'sequences-and-series', 'problem-solving']"
3765725,Exists $t^*\in \mathbb{R}$ such that $y(t^*)=-1$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $y'=y^2-3y+2, y(0) = \frac{3}{2}$ Exists $t^*\in \mathbb{R}$ such that $y(t^*)=-1$ ?. How to prove without solving the ode? Any hint?",['ordinary-differential-equations']
3765751,What is the Fourier transform of $|x|$?,"I am trying to find the Fourier transform of $|x|$ in the sense of distributions in its simplest form. Here is what I have done so far: Let $$f(x)=|x|=\lim_{a\rightarrow 0}\frac{1-e^{-a|x|}}{a},$$ then the Fourier transform is given by $$\begin{aligned}
\hat{f}(\xi)&=\int_{-\infty}^\infty f(x)e^{-2\pi i x \xi}dx \\
&=\lim_{a\rightarrow 0}\frac{1}{a}\left(\delta(\xi)-\frac{2a}{a^2+4\pi^2\xi^2}\right).
\end{aligned}$$ Using the identity (see here) , $$\delta(\xi)=\lim_{a\rightarrow 0}\frac{1}{\pi}\frac{a}{a^2+\xi^2},$$ we know that $$2\pi\delta(2\pi\xi)=\lim_{a\rightarrow0}\frac{2a}{a^2+4\pi^2\xi^2}.$$ Hence, using the identity, $$\delta(b x)=\frac{1}{|b|}\delta(x),$$ we know that $$\hat{f}(\xi)\stackrel{?}{=}\lim_{a\rightarrow0}\frac{1}{a}[\delta(\xi) - \delta(\xi)].$$ This doesn't seem right... Can you see where I have gone wrong and do you know how to calculate $\hat{f}(\xi)$ in its simplest form?","['calculus', 'fourier-transform', 'dirac-delta']"
3765765,Proving $\log\left(\frac{4^n}{\sqrt{2n+1}{2n\choose n+m}}\right)\geq \frac{m^2}{n}$,"I have tried doing this exercise, Let $m,n\in\mathbb{N}, m\leq n$ , prove that $$\log\left(\frac{4^n}{\displaystyle\sqrt{2n+1}{2n\choose n+m}}\right)\geq \frac{m^2}{n}$$ I achieved some results like for example, $$\displaystyle\sum_{i=0}^n 2^i\binom{2n-i}{n} = 4^n$$ and $$\displaystyle {{2n}\choose{n}} > \frac{4^n}{2n}$$ trying to find a relationship but it doesn't work for me. Any idea?","['inequality', 'combinatorics', 'combinatorial-proofs', 'logarithms']"
