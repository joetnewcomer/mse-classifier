question_id,title,body,tags
2420727,How do I evaluate the limit $\lim _{ x\to -\infty } \frac { 2x-3 }{ \sqrt { x^{ 2 }+7x-2 } } $ with a radical?,"I'm trying to evaluate $$\lim _{ x\to -\infty  } \frac { 2x-3 }{ \sqrt { x^{ 2 }+7x-2 }  } $$ by rationalizing the denominator, but I am not getting anywhere. Can someone please help me with this? Thanks","['calculus', 'limits']"
2420759,Help with marble problem,"In a box, there are red and blue marbles. If you take away one red marble,
  1/4 of all marbles are red. If, on the other hand, you add a red one, 1/3 of all marbles are red. How many blue marbles are in the box? If we say: x = Amount of red marbles y = Amount of blue marbles n = Total amount of marbles Then, what I get from the statement above is: x + y = n (x - 1) = n/4 (x + 1) = n/3 But I dont get any further from there. I do not want the solution, but a hint would be helpful. Source: http://monoid.mathematik.uni-mainz.de/monoid127.pdf (Page 21, Exercise 1163)",['functions']
2420786,"Let $K$ be a number field, and $R$ a $\overline{K}$-algebra, equipped with a continuous action of $G_K$. Is $R$ defined over $K$?","Let $K$ be a number field, and $R$ a $\overline{K}$-algebra, so we are given an inclusion $i : \overline{K}\hookrightarrow R$. Suppose there is an action of $G_K = \text{Gal}(\overline{K}/K)$ on $R$ satisfying: The stabilizer of any $r\in R$ has finite index (ie, the action is continuous when $R$ is given the discrete topology, and For $\sigma\in G_K$, $\sigma\circ i = i\circ\sigma$. Then must there exist a $K$-subalgebra $R_K\subset R$ on which $G_K$ acts as the identity, and such that $R \cong R_K\otimes_K \overline{K}$?","['algebraic-geometry', 'commutative-algebra']"
2420807,Lorentz Transformations Vs Coordinate Transformations,"I'm really confused about Lorentz transformations at the moment. In most books on QFT, Special Relativity or Electrodynamics, people talk about Lorentz transformations as some kind of special coordinate transformation that leaves the metric invariant and then they define what they call the Lorentz scalars. But from my point of view (which is somehow grounded in a background from differential geometry), scalars and the metric, which is a tensor, are invariant under any ""good"" coordinate transformation and that's a lot more than just Lorentz transformations, so I don't see why there's a special role for the Lorentz transformations in special relativity. Saying that the metric is invariant under Lorentz transformations is non-sense to me, because indeed it should be under any type of coordinate transformation if it's a well defined metric on a Minkowski manifold. It seems to me that Lorentz transformations should be relating observers (frames) and not coordinate systems - that would make more sense to me, but usually people mix both concepts as if they were exactly the same. I'd like to understand what it means when one says that some scalar is Lorentz invariant. If someone could clarify me this conceptual confusion, I would be really grateful.","['coordinate-systems', 'semi-riemannian-geometry', 'differential-geometry', 'general-relativity']"
2420813,A cocountable topology is not first countable,"I'm trying to understand this example: Let $X$ be any uncountable set and declare $U\subset X$ to be open if $X\setminus U$ is countable. Then $X$ is not first countable. Here is my reasoning: Suppose for a contradiction every $x\in X$ has a countable neighborhood basis. So let $x\in X$ and let $\{N_i\mid i\in\mathbb{N}\}$ be a countable neighborhood basis at $x$. Each $N_i$ contains an open neighborhood $U$, and since $U$ is open, $X\setminus U$ is countable, so $U$ must be uncountable. But $U\subseteq N_i$, so $N_i$ is uncountable. Hence every $N_i$ is uncountable. But how does that give a contradiction? I'm also struggling to understand the next statement: if $A$ is a proper uncountable subset of $X$. then $\overline{A}\neq A$. 
Clearly if $A$ is uncountable, then $A$ is not open, so $X\setminus A$ is non-empty and not closed, but then what? I understood that $X$ is not first countable and $A\neq\overline{A}$. Now the claim is that $\overline{A}\neq\{x\in X\mid$ there is a sequence $(x_n)_{n\in\mathbb{N}}$ in $A$ which converges to $x\}$. It suffices to show that $S:=\{x\in X\mid$ there is a sequence $(x_n)_{n\in\mathbb{N}}$ in $A$ which converges to $x\}=A$. Clearly $A\subseteq S$, by setting $x_n=x$ for each $n\in\mathbb{N}$. But How do I prove $S\subseteq A$?",['general-topology']
2420850,How to find the distinct equivalence classes for the set of all bit strings of length 5,"Let B denote the set of all bit strings of length 5,
  $b_1,b_2,b_3,b_4,b_5$. Define a relation R on B by: two bit strings
  are related by R if and only if they both have bits $b_1$ the same and both have
  the bits $b_5$ the same. (a) List all the elements of the equivalence class [10010]. (b) How many distinct equivalence classes are there? List them. So we never went over bit strings in class and I'm trying to apply the same concepts as a similar question that had integers and ordered pairs. For part (a), the elements of the equivalence class [10010] is just 0 and 1? For part (b), the distinct equivalence classes are all of the variations of the 5 bit strings where bits $b_1$ and $b_5$ are the same? for example, [10010] and [11110]? How would I go about determining the exact number of distinct equivalent classes? Any help is appreciated, thanks in advance","['bit-strings', 'equivalence-relations', 'discrete-mathematics']"
2420872,Does order matter in set intersection?,"I am asked to prove the following: $$A\setminus(B\cup C)=(A\setminus B)\cap(A\setminus C)$$ To do so, I must show that all the elements on the left-hand side exist on the right-hand side and vice versa. That is, $$\forall x\in A\setminus(B\cup C),\, x\in(A\setminus B)\cap(A\setminus C),$$ and, $$\forall x\in(A\setminus B)\cap(A\setminus C),\, x\in A\setminus(B\cup C).$$ I attempted to prove the former first. Let $x\in A\setminus(B\cup C)$. Therefore, $x\in A\cap(B\cup C)'$. By De Morgan's laws, this equates to $A\cap(B'\cap C')$. At this point, I am tempted to do the following. Given that $A\cap A=A$, I would like to rewrite $A\cap(B'\cap C')$ as $A\cap A\cap B'\cap C'$, then rearrange this expression into $A\cap B'\cap A\cap C'$, such that $x\in (A\setminus B)\cap(A\setminus C)$. I am not sure however, if $A\cap A\cap B'\cap C$ is equivalent to $A\cap B'\cap A\cap C'$. Hence the question: Does order matter in the intersection of sets? Does $A\cap B\cap C=C\cap A\cap B=B\cap A\cap C$, and so on?",['elementary-set-theory']
2420875,"Completion of the space of piecewise-constant functions on $[0,1]$","Let $L$ be the space of piecewise-constant functions on $[0,1]\subset \mathbb{R}$ equipped with the supremum norm (i.e. step functions). What is the completion of this space? We discussed in my class that all metric spaces have (unique) completion, but the proof of existence using equivalence classes doesn't give much mechanism to actually compute the completion. Intuitively, the space should contain all continuous functions, but it should be strictly larger as we can have a sequence $f_n \in L$ which is just the constant sequence of a function which is discontinuous. My guess would be $C[0,1] \cup \{f |  f $  is piecewise constant$ \}$, and I could probably show that $L$ is dense in this space, but I'm not sure how I'd go about showing that it is complete. Notably these functions are all integrable, so perhaps that's the completion - but we haven't discussed integrals yet in our course which makes me a bit hesitant.",['real-analysis']
2420878,Is there a name for the property of a function that maps the empty set to zero?,"We almost can define a measure in a concise and neat way by stating that a measure on a $\sigma$-algebra $\Sigma$ is a countably additive function $ \mu : \Sigma \to [0, \infty] $ such that $ \mu(\emptyset) = 0 $. Except for the last property that I couldn't find a name for. On my mind, such an essential should be given a name. So far the best I could come up with is to call $\mu$ an empty-to-zero function. References and ideas are welcome.","['notation', 'functions', 'soft-question', 'measure-theory', 'definition']"
2420882,Iterated Derivatives of function combinations?,"NOTE: I will use $f^{(n)}$ to represent the $n$th derivative of the function $f$. While working with with Taylor Series, I decided to try and find formulas to make multiple differentiation of a function easier for myself. For the sum of two functions, the iterated derivative is easy if you know the iterated derivatives of each of the two functions:
$$(f+g)^{(n)}(x)=(f^{(n)}+g^{(n)})(x)$$
The formula for the product was a little bit trickier, but I found a formula that resembles slightly the binomial theorem:
$$(f\cdot g)^{(n)}(x)=\sum_{k=0}^n \binom{n}{k}(f^{(k)}\cdot g^{(n-k)})(x)$$
However, there are three that I was not able to find a formula for:
$$(1/f)^{(n)}(x)$$
$$(f\circ g)^{(n)}(x)$$
$$(f^{-1})^{(n)}(x)$$
Does anybody know corresponding formulas for these?","['derivatives', 'taylor-expansion', 'inverse-function', 'function-and-relation-composition', 'functions']"
2420922,To prove that $B\subset A\rightarrow A=A\cup B$ is it necessary to consider the cases $A\cup B=\emptyset$ and $A\cup B\neq \emptyset$?,"For what I know, to prove $B\subset A\rightarrow A=A\cup B$ it suffices to prove that $A\subset (A\cup B)$ and $(A\cup B)\subset A$. Prove that $A\subset (A\cup B)$ is trivial, because $A\cup B=\{x:x\in A\vee x\in B\}$. My teacher in proving that $(A\cup B)\subset A$ separated the proof into two parts: $A\cup B=\emptyset$ and $A\cup B\neq \emptyset$. But I didn't understand why the need to separate this proof in two cases since $x\in (A\cup B)\to x\in A \vee x\in B$ We know, by hypothesis, that $B\subset A$, so we can conclude that $(x\in A\vee x\in B)\leftrightarrow(x\in A)$ is tautology(using truth table). So saying $x\in A\vee x\in B$ is equivalent to saying $x\in A$ which implies $x\in (A\cup B)\to x\in A \vee x\in B\equiv x\in A\Rightarrow (A\cup B)\subset A$ (note: the implication $p\to q$ is true whenever $p$ is false). In my opinion in the proof that $(A\cup B)\subset A$ there was no need to consider two cases, but as my teacher has great knowledge about set theory I feel that I am making a mistake by not considering the two cases. That is why I ask for help in this doubt. Obs.: If I used some logical symbol wrongly let me know, please, since I still know few things about propositional logic.",['elementary-set-theory']
2420944,A-branes on the mirror to the projective line,"I am trying to understand the definition for the category of A-branes for the mirror to the projective line described in this paper by Ballard, Meet homological mirror symmetry (the construction is in section 3.2) and I'm starting to think that I'm missing something. The construction starts like this: the mirror to $\mathbb P ^1$ can be taken to be the Landau-Ginzburg model \begin{align} W:& \mathbb C^* \to \mathbb C \\ & z\mapsto z+\frac1z.  \end{align} We can put a symplectic form on $\mathbb C^*$, defined by $i\frac{dz\wedge d\bar z}{z\bar z}$, and look at the general fiber which, if I understand correctly, should be two points, while the critical points of W are $\pm 1$, and its critical values are $\pm 2$. It is claimed at this point that we can use the symplectic form to split the tangent space to the domain into the tangent space of the fiber (which has rank 0?) and the symplectic orthogonal (the whole tangent bundle?). From here on I have a very hard time understanding the construction and the notation (up until Definition 3.7 excluded). The upshot is that the category of A-branes on this LG model is equivalent to the category of modules over the path algebra of a certain quiver $Q$, which should appear from considering two Lagrangian submanifolds (called vanishing thimbles associated with the critical points). Can someone help me understand how these Lagrangian are constructed, and/or give me a good pointer to other references on this?","['symplectic-geometry', 'algebraic-geometry']"
2421092,matrix differentiation of lp norm,"I'm trying to figure out what is the gradient [matrix differentiation] of the lp norm. I saw a few other posts regarding the specific case when for example when p=2 so I think that: $$\frac{\partial d}{\partial x} ||Ax - b||_2  = A$$ but what about for different arbitrary values of p? In that case, what is$\frac{\partial d}{\partial x} ||Ax - b||_p $, where subscript _p means lp-norm?","['derivatives', 'least-squares', 'matrix-calculus', 'normed-spaces', 'partial-derivative']"
2421126,$n^2+1$ and $n$ are coprime for all $n\in\mathbb{N}$,"Prove for each $n\in\mathbb{N}$ that $\,n^2+1\,$ and $\,n\,$ are coprime. My attempt: We have to prove that $\,n^2+1\,$ and $\,n\,$ are coprime, i.e., we have to prove $\gcd(n^2+1,n)=1$. Suppose that $\gcd(n^2+1,n)=d$. Then $d|(n^2+1)$ and $d|n$, then $d|(n^2+1-n\cdot n) = d|(n^2+1-n^2) = d|1$ This means $d=1$. Is my approach correct?","['number-theory', 'elementary-number-theory']"
2421209,Kalman tuning rules on States process noises,"experts on Estimation, on Kalman, I'm considering a temporal linear KF estimation problem, the whole system is as follows: 
$$Y_{2n*1}(t) = [Y1;Y2]$$
$$Y1 = Ax +\epsilon_{1}$$
$$Y2 = Ax +Bb+\epsilon_{2}$$
where the size of measurements $Y1$ and $Y2$ is the same $(n*1)$, the corresponding measurement noises $\epsilon_{1}$ and $\epsilon_{2}$, independent from each other, are both Gaussian White noise, 
$$\epsilon_1 \sim N(0, Q_1)$$
$$\epsilon_2 \sim N(0, Q_2)$$
and $Q_2$ is 100 times samller than $Q_1$, $A$, of size $(n*m), m<n$, is the geometry matrix related to state vector $x$, of size $(m*1)$, $B$, of size $(n*n)$, is the geometry matrix related to state vector $b$, of size $(n*1)$, Is there any tuning rules, for example on the state process noise covariance $Q_x, Q_b$, that I should follow, to avoid the case that the state $x$ is biased-estimated. However, according to the state nature, $b$ is a un-known constant vector to be estimated. The resonable process noise uncertainty $Q_b$ is of order,i.e. $1e-4$. The biased estimation already happened, it seems that too much confidence has been put on the estimates $\hat b$. As the measurement covariance matrix is unchangeable, either are the geometry matrices, so I'm wondering maybe I should start from the tuning of states process noises uncertainty. Could any expert on estimation, KF tuning, help me, please?","['kalman-filter', 'estimation', 'discrete-mathematics']"
2421220,Prove that $10101\ldots01$ can't be a perfect square.,"Prove that $10101\ldots01$, where the sequence $""01""$ is repeated $k$ times can't be a perfect square for any value of $k$. All I managed to prove is that the general formula is $1+\frac {100}{99}(100^k-1)$ but I don't know how to continue.",['number-theory']
2421234,Why do $A$ and $A^TA$ have the same row space?,Theorem: Let $A$ be an $m \times n$ real matrix. Then $A$ and $A^T A$ have the same row space. I am trying to derive and intuitively understand why this theorem holds. Would appreciate any help. Thanks.,"['matrices', 'linear-algebra']"
2421277,Solving Problems In Consecutive Days,"During a common year a student solves at least $1$ problem in combinatorics, each day. But, so he would not get too tired, he solves at most $12$ problems per week. Prove that there exist several consecutive days during which student has solved, exactly, $20$ problems. What I have tried is dividing $365$ days in $19$ $``boxes""$ . Hence $365 > 19*19$ there must be a $``box""$ with $20$ consecutive days. Also, hence $19 = 2*7+5$ he can't have solved more than $2*12+5$ problems.
But from this I cannot infer how I get there is no $21$ days in the $``box""$ . Is my approach even good? Thanks in advance","['pigeonhole-principle', 'combinatorics', 'contest-math']"
2421280,"If $W$ is a standard Brownian motion, does $W(1)$ take every real number?","Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $W:[0,\infty)\times \Omega\rightarrow\mathbb{R}$ be a standard Brownian motion. Is it true that for all $x\in\mathbb{R}$ there exists $\omega\in\Omega$ such that $W(1,\omega)=x$? I do not know if the question has a positive or negative answer. I know that $P(W(1)\in [a,b])>0$ for all real numbers $a<b$, but I do not think that suffices. I have no idea on how to prove the existence of such an $\omega$, since it seems to me that the proof may use an explicit form of $\Omega$. EDIT : As I read in a comment below, this statement is not true. My doubt came from reading the proof of Lemma 5.22 in An Introduction to Computational Stochastic PDEs . The step I do not completely understand is the following: from $E[W(t)|W(1)]=t \,W(1)$, where $W$ is a Brownian motion and $0\leq t\leq 1$, it is stated that $E[W(t)|W(1)=0]=0$. One of the possible definitions that I studied for $E[W(t)|W(1)]$, and in general for $E[X|Y]$, where $X$ and $Y$ random variables, is $E[X|Y](\omega):=E[X|Y=Y(\omega)]$, where $E[X|Y=y]=\int_{\mathbb{R}} x\,P_{X|Y=y}(dx)$, being $P_{X|Y=y}$ the $P_Y$-unique probability satisfying $P(X\in A,Y\in B)=\int_B P_{X|Y=y}(A)\,P_Y(dy)$. In this example, $E[W(t)|W(1)=W(1)(\omega)]=E[W(t)|W(1)](\omega)=t\,W(1)(\omega)$. I understand that, if $W(1)(\omega)=0$ for some $\omega\in\Omega$, then $E[W(t)|W(1)=0]=t\cdot 0=0$. But if for our particular probability space $(\Omega,\mathcal{F},P)$ and our particular Brownian motion $W$ there is no $\omega$ satisfying $W(1)(\omega)=0$, then I do not see that $E[W(t)|W(1)]=t\,W(1)$ implies that $E[W(t)|W(1)=0]=0$. Could you explain this to me using probability theory, and not just ""intuition""?","['stochastic-processes', 'probability-theory', 'probability', 'brownian-motion', 'stochastic-calculus']"
2421293,Is ${\sqrt2^{\sqrt2^{\sqrt2^{\sqrt2^\sqrt2}}}}^{...}=4$ correct? [duplicate],"This question already has answers here : Are these solutions of $2 = x^{x^{x^{\:\cdot^{\:\cdot^{\:\cdot}}}}}$ correct? (4 answers) Closed 6 years ago . My teacher asked me this question. But I  think this is wrong. Anyone found this, before me? I do not know. Anyway, Is my solution correct? ${x^{x^{x^{x^x}}}}^{...}=2$ $x^2=2$ $x=\sqrt 2$ ${\sqrt2^{\sqrt2^{\sqrt2^{\sqrt2^\sqrt2}}}}^{...}=2$ Now, Let, ${x^{x^{x^{x^x}}}}^{...}=4$ $x^4=4$ $x=\sqrt2$ ${\sqrt2^{\sqrt2^{\sqrt2^{\sqrt2^\sqrt2}}}}^{...}=4$ Contradiction!","['fake-proofs', 'proof-verification', 'number-theory', 'infinity', 'sequences-and-series']"
2421338,"Does the sequence $f_1=x^2+1$ , $f_{n+1}=(f_n)^2+1$ contain only irreducible polynomials?","Consider the sequence $$f_1=x^2+1$$ $$f_{n+1}=(f_n)^2+1$$ of polynomials over the integers. Is $f_n$ irreducible over $\mathbb Q[x]$ for all $n\ge 1$ ? With PARI/GP, I found out that upto degree $4\ 096$, all the polynomials are irreducible over $\mathbb Q[x]$. Obviously , the polynomials don't have real roots and are all even. Any ideas ?","['irreducible-polynomials', 'dynamical-systems', 'polynomials', 'sequences-and-series']"
2421370,Reference: solved problems and exercises on PDEs,"I'm looking for books/lecture notes that contain solved problems and exercises on PDE. More specifically, I'm interested in basic 'computational' exercises; more theoretical/advanced problems (also with some functional analytical flavor); problems that are more ""numerical"" in nature (maybe based on software like Matlab or Mathematica). Some references can be found  at Supplemental reference request-Graduate level PDE problems and solutions book , but I'm looking for something more both basic and advanced.","['real-analysis', 'partial-differential-equations', 'reference-request', 'soft-question', 'ordinary-differential-equations']"
2421376,Integrate $\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}$,"Evaluate $$I=\int \frac{dx}{(x^2-x+1)\sqrt{x^2+x+1}}$$ My Try: we have $x^2-x+1=(x+w)(x+w^2)$ where $w$ is complex cube root of unity I have splitted $I$ as $$I=AI_1+BI_2$$ where $A,B$ are some constants $$I_1=\int \frac{dx}{(x+w)\sqrt{x^2+x+1}}$$ By taylor's series $$x^2+x+1=P(x+w)^2+Q(x+w)+R=(x+w)^2 \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right)$$ for some constants complex $P,Q,R$ hence $$I_1=\int \frac{\frac{dx}{(x+w)^2}}{\sqrt{ \left(P+\frac{Q}{x+w}+\frac{R}{(x+w)^2}\right)}}=\int \frac{dt}{\sqrt{Rt^2+Qt+P}}$$ which is a standard Integral. Similar analysis for $I_2$ . Any other approach?","['complex-numbers', 'calculus', 'functions', 'indefinite-integrals', 'integration']"
2421399,$n$-th derivative of $y$ in $x^y=e^{x-y}$,"Question: If $x^y=e^{x-y}$, find a general formula for $\displaystyle\frac{d^ny}{dx^n}$. My Attempt: $$x^y=e^{x-y}$$
$$\Rightarrow\ \ \ \ \ y\ln x=x-y$$
$$\Rightarrow\ \ \ \ \ y=\frac{x}{\ln x+1}$$
So
$$\begin{align}\frac{dy}{dx}&=\frac{\ln x+1-1}{(\ln x+1)^2}\\
&=\frac{\ln x}{(\ln x+1)^2}
\end{align}$$
Then
$$\begin{align}\frac{d^2y}{dx^2}&=\frac{\frac{1}{x}(\ln x+1)^2+\frac{2\ln x}{x}(\ln x+1)}{(\ln x+1)^4}
\end{align}$$
Now, I can see that the denominator satisfies a straight-forward pattern $(\ln x+1)^{2^n}$. So the general formula for $\displaystyle\frac{d^ny}{dx^n}$ is
$$\frac{d^ny}{dx^n}=\frac{g(n)}{(\ln x+1)^{2^n}}$$But I can't derive any formula for the numerator $g(n)$, nor can I see any regular pattern in the structure. Should I differentiate the initial expression $x^y=e^{x-y}$ implicity? Can anyone help me in that?","['derivatives', 'calculus']"
2421402,Eigenvalues of a $2 \times 2$ block matrix where every block is an identity matrix,"I want to consider the following matrix: \begin{bmatrix}\boldsymbol{I}_n & \boldsymbol{I}_n \\\boldsymbol{I}_n & \boldsymbol{I}_n\end{bmatrix} By doing several numerical examples, I recognized that this matrix has $n$ eigenvalues equal to zero and $n$ eigenvalues equal to $2$. Is there any way to prove this for an arbitrary number $n$?","['matrices', 'eigenvalues-eigenvectors', 'block-matrices']"
2421406,Proof that a strictly diagonally dominant matrix is invertible [duplicate],"This question already has answers here : Strictly diagonally dominant matrices are non singular (4 answers) Closed 4 years ago . Let $A$ be a strictly diagonally dominant matrix of dimensions $n \times n$ . (""Strictly diagonally dominant"" means that $\left|a_{i,i}\right| > \sum\limits_{j \neq i} \left|a_{i,j}\right|$ for all $i \in \left\{1,2,\ldots,n\right\}$ , where $a_{u,v}$ denotes the $\left(u,v\right)$ -th entry of $A$ .) Prove that $A$ is invertible. My attempt builds on the proof of Gershgorin's circle theorem, given in the Wikipedia article https://en.wikipedia.org/wiki/Gershgorin_circle_theorem Let $\lambda$ be an eigenvalue of $A$ , and scale its corresponding eigenvector $x$ so that $x_{i} = 1$ and $|x_{j}| \leq 1$ for $j \neq i$ Then $Ax = \lambda x$ , and in particular $\sum_{i\neq j}a_{i,j}x_{j} = \lambda - a_{ii}$ (1) Now because $A$ is strictly diagonally dominant it holds for every $i$ that, $\sum_{j \neq i}|a_{i,j}| < |a_{i,i}|$ and since $|x_{i}| \leq 1$ the following should hold: $\sum_{i\neq j}|a_{i,j}x_{j}| \leq \sum_{j \neq i}|a_{i,j}| < |a_{i,i}|$ But here I get stuck, feel like I want to use (1) in some way to complete the proof and put $\lambda = 0$ to get a contradiction, thus proving that if $A$ is strictly diagonally dominant, it has non-zero eigenvalues which should imply invertibility.. Am I in the right direction?","['matrices', 'inequality', 'linear-algebra', 'proof-verification']"
2421413,Banach Space Adjoint of Bounded Operator on $(\ell_1)^*$ acting on $\ell_\infty$,"My textbook has this example: Let $T$ be the operator on $\ell_1$ with $T(a_1,a_2,..) = (a_2,a_3,..)$. If $T'$ denotes the Banach space adjoint of $T$, $T'$ acts on $\ell_\infty$ (isometric to the $(\ell_1)^*$) by $T'(x_1,x_2..) = (0,x_1,x_2,..)$. Can you help me see the details of how to derive this? The isometry I know of between $(\ell_1)^*$ and $\ell_\infty$ is: $\phi: \ell_\infty \to (\ell_1)^*$; $\phi(x) = \phi_x$ s.t $\phi_x(a_1,a_2,..) = \sum_{i=1}^{\infty}x_ia_i$, but I can't see how to use this to formalise that statement.","['functional-analysis', 'sequences-and-series']"
2421443,Probability that a point is closer to a side than a diagonal,So I have a rectangle in which a point is randomly chosen. One side is $a$ and the other is $b=a\sqrt3$. I am supposed to find the probability that a point is closer to a side than to the closest diagonal. I have found the probability that the point is closer to $a$ (0.71) and to $b$ (0.24). Now I was wondering how I can put these two probabilities together to form the asked-for probability. Thanks,"['probability-theory', 'probability', 'geometry']"
2421448,"Prove that for any sets $A$, $B$, and $C$, $A×(B\setminus C) = (A×B)\setminus (A×C)$.","This is Velleman's exercise 4.1.8 (""$×$"" means the Cartesian product): Prove that for any sets $A$, $B$, and $C$, $A×(B\setminus C) = (A×B)\setminus (A×C)$. Cartesian product of $A$ and $B$, denoted $A × B$ is equal to $\{(a, b) | a ∈ A and b ∈ B\}$ (""$(a, b)$"" is an ordered pair). And here's my proof of it: Proof. ($\rightarrow$) Let $(x, y)$ be an arbitrary element of $A×(B\setminus C)$, then $x ∈ A$ and $y ∈ (B\setminus C)$ which means that $y ∈ B$ and $y ∉ C$. From $x ∈ A$ and $y ∈ B$, we get $(x, y) ∈ (A×B)$. From $x ∈ A$ and $y ∉ C$, we get $(x, y) ∉ (A×C)$. Thus $(x, y) ∈ (A×B)\setminus (A×C)$. Therefore $A×(B\setminus C) ⊆ (A×B)\setminus (A×C)$. ($\leftarrow$) Let $(x, y)$ be an arbitrary element of $(A×B)\setminus (A×C)$, then $(x, y) ∈ (A×B)$ which means that $x ∈ A$ and $y ∈ B$ and $(x, y) ∉ (A×C)$ which means that either $x$ is not an element of $A$ or $y$ is not an element of $C$ or both. But since we saw that $x ∈ A$, therefore $y ∉ C$. From $y ∈ B$ and $y ∉ C$, we get $y ∈ (B\setminus C)$. From $x ∈ A$ and $y ∈ (B\setminus C)$, we get $(x, y) ∈ A×(B\setminus C)$. Therefore $(A×B)\setminus (A×C) ⊆ A×(B\setminus C)$. From ($\rightarrow$) and ($\leftarrow$), we get $A×(B\setminus C) = (A×B)\setminus (A×C)$. Is my proof correct (particularly ""From $x ∈ A$ and $y ∉ C$, we get $(x, y) ∉ (A×C)$"" in the forward direction)?","['logic', 'elementary-set-theory', 'proof-verification']"
2421457,Can I always find a smooth bundle map relating two arbitrary smooth vector fields defined on the same smooth manifold?,"Suppose to consider two arbitrary smooth vector fields $X: M\rightarrow TM$ and $Y: M\rightarrow TM$ defined on the same differentiable manifold $M$. Is there always a smooth map $A: TM\rightarrow TM$ (or a composition of smooth maps, none of them necessarily induced by a diffeomorphism $\varphi: M\rightarrow M$), such that $Y=A(X)$? If the answer is no, what are the conditions under which I can find such a smooth map $A$? If you know the answer, could you please suggest me a reference where this problem is treated?","['tangent-bundle', 'vector-fields', 'smooth-manifolds', 'differential-geometry']"
2421512,Can we always replace $\sin x$ with $x$ in limit as $x\to 0$,"Let $D\subset \mathbb{R}^2$, $0$ be a limit point of $D$, and $f:D\to \mathbb{R}$ be a function. How to prove or disprove that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? If it is not true, what are the sufficient and necessary conditions such that $\displaystyle\lim_{x\to 0}f(x,\sin x)=\lim_{x\to 0}f(x, x)$ ? Thanks in advances.","['multivariable-calculus', 'calculus', 'limits']"
2421529,Is probability a well-established field without room to grow? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. This question is not about mathematics, within the scope defined in the help center . Closed 6 years ago . Improve this question As I pursue my PhD in statistical science, I have grown very fond of probability theory.  I was curious if it was worth furthering my background in probability past more advanced topics such as: Brownian Motion, Donsker's invariance principle, etc. in hopes of somehow offering something to the field.  I also would like to find areas of intersection such as statistical physics or network theory/reinforcement learning. Is probability advancing anymore or is it a well-established field without much room to grow?","['probability-theory', 'research', 'probability']"
2421545,Matrix Linear Least Squares Problem with Diagonal Matrix Constraint,"How could one solve the following least-squares problem with Frobenius Norm and diagonal matrix constraint? $$\hat{S} := \arg \min_{S} \left\| Y - XUSV^T \right\|_{F}^{2}$$ where the $S$ is a diagonal matrix and $U,V$ are column-orthogonal matrix. Is there any fast algorithm?","['least-squares', 'optimization', 'svd', 'convex-optimization', 'linear-algebra']"
2421547,Can $10101\dots1$ be a perfect square in any base?,"Inpired by another question and it's answer I started to wonder if it's true in other bases as well. I've at least not found any base $b$ where $10101\dots1 = (b^{2k}-1)/(b^2-1)$ (where $k>1$) is a perfect square anytime. From the answers we can conclude that $8|b^2$ which means that the base must be a multiple of $4$. This is because we must have $b^2/8$ to be an integer. Also I think/guess that $b^k+1$ must be a perfect square for some $k$. This is because $${b^{2k}-1\over b^2-1} = {(b^k-1)(b^k+1)\over (b-1)(b+1)}$$ has to be a square. I've also found that the question is equivalent to if $111\cdots1$ can be a perfect square in base $b^2$. I've seen that it can be a perfect square in base $8$, but $8$ is no perfect square.",['number-theory']
2421566,"Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?""","Why is the exterior algebra called the ""exterior algebra?"" What makes it ""exterior?"" Is it just because a module can be universally embedded into its exterior algebra, so one could view the exterior algebra as surrounding the module? Why is it not just called the ""alternating algebra?""","['terminology', 'abstract-algebra', 'math-history', 'differential-geometry']"
2421612,Intuition why a combined inscribed and circumscribed polygon converge faster to $\pi$?,"I was playing around a little with approximating $\pi$ by calculating the perimeter of a regular polygon both inscribing and circumscribing a circle. When using trigonometry then it can be shown that $\pi$ can be approximated using the perimeters of these polygons with $n$ sides as follows $$
\pi_i(n) = n\,\sin\left(\frac{180^\circ}{n}\right),
$$ $$
\pi_c(n) = n\,\tan\left(\frac{180^\circ}{n}\right),
$$ with $\pi_i(n)$ and $\pi_c(n)$ the approximation of $\pi$ from an inscribing and circumscribing polygon respectively. But $\pi_i(n)$ and $\pi_c(n)$ are always a lower and upper bound respectively for $\pi$. Therefore I thought that taking the average should give a better result, which indeed reduced the error. However the rate at which the error gets smaller as $n$ increases was the same as the previous two. When I started using different weights I found the following expression gives the best result $$
\pi_w(n) = \frac23\,\pi_i(n) + \frac13\,\pi_c(n).
$$ Graphing the errors of these approximations gives: The rate at which the error of $\pi_i(n)$ and $\pi_c(n)$ goes down seems to be proportional to $n^{-2}$, while the rate at which the error of $\pi_w(n)$ goes down seems to be proportional to $n^{-4}$. These converges rates can be explained by looking at the Taylor series of each expression. However I am curious if this could also be explained with pure geometry.","['pi', 'approximation', 'geometry']"
2421627,Does positive Ricci curvature tensor on an orthonormal frame imply positive Ricci curvature tensor?,"Let $(M,g)$ be a Riemannian manifold and $Ric$ its Ricci curvature. Let $\{e_1,\cdots, e_n\}$ be specific orthonormal frame and in this frame we have:
$$Ric(e_i,e_i)>0\quad \forall i=1,...,n$$ My question is Does this condition imply positive Ricci curvature tensor?i.e., $Ric(X,X)>0$ for all vector field $X$?","['riemannian-geometry', 'differential-geometry']"
2421675,"If $Y$ is a nonnegative absolutely continuous random variable and $E[X|Y]=Y/2$, is $E[X|Y=-1]=-1/2$? Is $E[X|Y=2]=1$?","One of the definitions I learned for $E[X|Y=y]$ is the following:
$$ E[X|Y=y]=\int_{\mathbb{R}} x\,P_{X|Y=y}(dx), $$
where $P_{X|Y=y}$ a probability verifying 
$$ P(X\in A, Y\in B)=\int_B P_{X|Y=y}(A)\,P_Y(dy), \;\;(*)$$
for all $A,B\in\mathcal{B}(\mathbb{R})$. This probability $P_{X|Y=y}$ is unique in the following sense: if $Q_{X|Y=y}$ is another probability satisfying $(*)$, then $P_{X|Y=y}(C)=Q_{X|Y=y}(C)$ for all $C\in\mathcal{B}(\mathbb{R})$ and $y\notin N$, with $P_Y(N)=0$. Let $Y$ be a nonnegative absolutely continuous random variable with $E[X|Y]=Y/2$. Let $N=(-\infty,0)\cup \{2\}$. We have $P_Y(N)=0$, so in principle $P_{X|Y=y}$ may be any probability on $(\mathbb{R},\mathcal{B}(\mathbb{R}))$ for $y\in N$, right? Then $E[X|Y=y]$ could be any value for $y\in N$, right? I am totally confused. Intuitively, $E[X|Y=2]$ should be $1$ and $E[X|Y=-1]$ should not be defined, since $Y$ is nonnegative. As you can see in this question , in Lemma 5.22 of the book An Introduction to Computational Stochastic PDEs , it is stated that $E[W(t)|W(1)]=t\,W(1)$ implies that $E[W(t)|W(1)=0]=0$, where $W$ is a Brownian motion and $0\leq t\leq 1$. But $P(W(1)=0)$, so $E[W(t)|W(1)=0]$ is not uniqueley defined, it may be any value. I mean, $y\mapsto E[W(t)|W(1)=y]$ is defined $P_{W(1)}$-a.s., so at $y=0$ there is not a unique definition. It is like stating something about $f(0)$ when $f$ is a real function defined a.e.","['probability-theory', 'conditional-expectation', 'probability', 'measure-theory']"
2421685,Existence of a sequence $\{\epsilon_n\}_{n\ge 1}$ such that $\sum\limits_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} $ converges,I am trying to find a sequence $\{\varepsilon_n\}_{n\ge 1}$  such that $$\lim_{n\to \infty}\varepsilon_n=1~~~~~~~~~~~~~\text{and}~~~~~~~~~\sum_{n=1}^{\infty}\frac{1}{n^{\varepsilon_n}} <\infty.$$ In case such sequence $\{\varepsilon_n\}_{n\ge 1}$ exists I would like to have an explicit example. Remark: This is an interesting problem since we know that for the case where  $\varepsilon_n = 1$ we have $$\sum_{n= 1}^{\infty}  \frac{1}{n^1}=\infty$$,"['real-analysis', 'sequences-and-series', 'harmonic-numbers', 'convergence-divergence', 'analysis']"
2421689,"Do there exist two spaces such that there is no continuous, non-constant function between them?","Do there exist two topological spaces $(X,\tau_X)$ and $(Y,\tau_Y)$, each with more than one point, so that no continuous non-constant function $f:X\to Y$ exists? Edit: As Krish commented, an example of this would be to take $X$ connected and $Y$ discrete with more than one point, so now I propose the question: Do there exist two topological spaces $(X,\tau_X)$ and $(Y,\tau_Y)$, each with more than one point, so that no continuous non-constant function $f:X\to Y$ or $f:Y\to X$ exists? Edit 2: I added the assumption that $X$ and $Y$ have at least two points.","['continuity', 'general-topology']"
2421735,Test whether the function $\frac{x}{2}+\frac{1}{x}$ is contraction or NOT,"Let , $X=\{x\in \Bbb R: x\ge 1\}$. Define a map $f:X\to X$ by $\displaystyle f(x)=\frac{x}{2}+\frac{1}{x}$ for all $x\in X$. Examine if $f$ is contraction map or not w.r.t. usual metric on $\Bbb R$. Find the smallest $\lambda$, as contraction constant. We have $|f(x)-f(y)|=\left |\left(\frac{1}{2}-\frac{1}{xy}\right)(x-y)\right |\le |x-y|.\left(\frac{1}{2}+\frac{1}{|xy|}\right)\le \frac{3}{2}|x-y|$ From here how I can say that $f$ is a contraction map ? As, $x,y\ge 1$ so , $1/xy \le 1$. If we take $x,y\in X$ such that $1/xy=1/4$ then the constant $\lambda=1/2+1/4=3/4<1$. But $f$ to be contraction we have to show for all $x,y\in X$ , $|f(x)-f(y)|\le \lambda|x-y|$. So why I will take $x,y$ such that $1/xy=1/4$ ?","['functional-analysis', 'real-analysis', 'metric-spaces', 'analysis']"
2421778,Sum of areas of triangles [duplicate],"This question already has answers here : Inside an equilateral triangle $ABC$,an arbitrary point $P$ is taken from which the perpendiculars $PD,PE$ and $PF$ are dropped onto the sides... (2 answers) Closed 6 years ago . Let  $ABC$ an  equilateral   triangle  and  $O$ a  point in  interior of $ABC$. Consider $M, N, P$  the  projections on  the  sides $AB, BC, AC$.  Then  the  sum of  areas of  triangles $ AOM, BON, COP$ is  the same like  the  sum  of  areas of  triangles $BOM, CON, AOP$. How  to  start? I  try  to  find  an  elementary  solution.",['geometry']
2421876,What is the number of segments in the picture,"i have no Idea how i can do this. I guess it is 8*8 = 64, but that isn't the right answer: What is the number of segments in the picture below? Each segment joins two circles.","['algebra-precalculus', 'combinatorics', 'graph-theory']"
2421885,Why does an integral extension over a ring has the same Krull dimension as the ring?,"The Krull dimension of a ring R is defined as the supremum of the lengths of chains of prime ideals contained in R. I heard that an integral extension over a ring R has the same Krull dimension as R, however, I don't really see why this is true.","['abstract-algebra', 'integral-extensions', 'commutative-algebra']"
2421892,In how many ways can 10 people be split into five groups of 2?,"My reasoning for this is as follows: you first pick two from 10, then two from 8, et cetera until nobody is left. The order doesn't matter, so you also divide by the number of permutations of the 5 groups: $$
\frac{C_2^{10} \cdot C_2^8 \cdot C_2^6 \cdot C_2^4 \cdot C_2^2}{5!}
$$ I'm not at all confident with combinatorics though; is this formula correct?",['combinatorics']
2421901,Alternating series and reciprocal Fibonacci constant,We know that reciprocal Fibonacci constant $$\sum_{n=1}^{\infty} \frac{1}{F_n} = \frac{1}{1} + \frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{5} + \frac{1}{8} + \frac{1}{13} + \frac{1}{21} + \cdots \approx 3.3598856662 \dots .$$ Evaluate:  $$\sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{F_n} $$,"['fibonacci-numbers', 'sequences-and-series']"
2421951,Why in some problems we round up and in some problems we round down?,"The following was written in the book I am reading. Suppose you’re organizing an outing for $100$ people, and you're renting minibuses that can hold $15$ people each. How many minibuses do you need? Basically you need to calculate $$100 \,÷\,15 \approx 6.7$$ But then you have to take the context into account: you can’t book $0.7$ of a minibus, so you have to round up to $7$ minibuses. Now consider a different context. You want to send a friend some chocolates in the mail, and a first-class stamp is valid for up to $100$ g. The chocolates weigh $15$ g each, so how many chocolates can you send? You still need to start with the same calculation $$100 \,÷\,15 \approx 6.7$$ But this time the context gives a different answer: since you can’t send $0.7$ of a chocolate, you'll need to round down to $6$ chocolates. I can't help but sense that there's a deeper underlying phenomenon going on here. So I have a few questions. What is actually the deeper thing going on here? What might be some exercises worth thinking about (i.e. struggling with) to better grasp what is actually going on here, for someone who has virtually zero proof experience in math?","['algebra-precalculus', 'intuition']"
2421990,Fourier transform of modified Bessel function of the second kind,Does anyone have any suggestions for performing the following Fourier transform: I = $\int_{-\infty}^{\infty} K_0(a \sqrt{x^2+1}) e^{i kx} dx $ where $K_0$ is the modified Bessel function of the second kind.,"['special-functions', 'integration', 'fourier-transform', 'bessel-functions']"
2421998,Overcounting in combinatorics problem,"Disclaimer: this is a homework exercise, please don't give me the full solution! I'd just like a hint in the right direction. A box contains 30 red balls, 30 white balls, and 30 blue
balls. If 10 balls are selected at random, without replacement,
what is the probability that at least one color will be
missing from the selection? My reasoning so far has been to look at the event that every colour is present, and then just do $1 - $ that value. We can think of it as selecting 3 balls, one from each colour, and then filling the remaining 7 spaces with any other balls. Selecting 1 ball from 30 three times is $30^3$ combinations. Then, selecting 7 balls from 87 is $C_7^{87}$ combinations, and the total combinations is their product. However, this turns out to be larger than $C_{10}^{90}$, which is the total amount of ways 10 balls can be selected from 90 balls. Clearly, I've overcounted somewhere, but I have no idea where! I've been stuck on this for hours and I can't see where I'm going wrong.","['combinatorics', 'probability']"
2422000,"If X is separable, then ball $X^*$ is weak-star metrizable.",Here is a proof in Conway's book. But there is something I don't understand. Why choose $\{x_n\}$ in ball $X$? Why not in $X$? I didn't see where the condition $\{x_n\}$ is dense using there. Without $\{x_n\}$ dense I can still get $x^*-y^*=0$. Thank you!,"['functional-analysis', 'normed-spaces', 'general-topology', 'banach-spaces']"
2422096,Expected value of a marginal distribution is a function of $x$?,"Let $f$ be the joint PDF of the random vector $(X, Y)$. $f(x, y) = \displaystyle\frac{x(x-y)}{8}$ for $0 < x < 2$ and $-x < y < x$, otherwise it's zero. Calculate the correlation between $X$ and $Y$. The problem I'm struggling with here is that to compute the correlation, I need $E(X)$, $E(Y)$, $E(XY)$, etc.
I already calculated the $E(X)$, but when trying to calculate $E(Y)$, I get: $E(Y) = \displaystyle\int_{-x}^xyf_y(y)dy$ Where $f_y(y)$ is the marginal distribution of y, which I computed as: $f_y(y) = \displaystyle\int_0^2\frac{x(x-y)}{8}dx = \frac{1}{3}-\frac{1}{4}y$ Because of the limits of integration when computing $E(Y)$, I assumed that the integral was going to be in terms of $x$, and in fact, I computed it and got $E(Y) = -\displaystyle\frac{x^3}{6}$. Why is my expected value in terms of $x$? Is this the proper way to calculate this? Shouldn't be Y a random variable of it's own?",['probability']
2422132,Show that $\lfloor(1+\sqrt{3})^{2n-1}\rfloor$ is divisible by $2^n$,Show that $\lfloor(1+\sqrt{3})^{2n-1}\rfloor$ is divisible by $2^n$ where $n$ is a positive integer. We have \begin{align*}\lfloor(1+\sqrt{3})^{2n-1}\rfloor &= (\sqrt{3}+1)^{2n-1}-(\sqrt{3}-1)^{2n-1}\\&=\dfrac{(4+2\sqrt{3})^n}{1+\sqrt{3}}+\dfrac{(4-2\sqrt{3})^n}{1-\sqrt{3}}\\&=2^n\left(\dfrac{(2+\sqrt{3})^n}{1+\sqrt{3}}+\dfrac{(2-\sqrt{3})^n}{1-\sqrt{3}}\right).\end{align*} Thus we must show that the second term is an integer. How can we continue?,['number-theory']
2422144,Deriving the equation of motion of a pendulum from energy conservation,"The kinetic energy of a simple pendulum is:
$$K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2$$ The potential energy of the pendulum is:
$$U=mgL(1-cos\theta)$$ The total energy of the pendulum is therefore:
$$E=K+U=K=\frac{1}{2}mL^2(\frac{d\theta}{dt})^2+mgL(1-cos\theta)$$ The total energy of the system is constant, therefore:
$$\frac{dE}{dt}=0$$ I am given to understand that taking the derivative of the total energy with respect to $t$ should allow me to rearrage for the equation of motion of a pendulum,
$$\frac{d^2\theta}{dt^2}+\frac{g}{L}sin\theta=0$$ but I don't see how.  Taking the derivative of the third equation returns $0$, which isn't very useful.  Am I missing something?","['ordinary-differential-equations', 'calculus']"
2422164,A continuous map whose graph is an embedded submanifold is smooth?,Let $M$ and $N$ be smooth manifolds and $f: M \to N$ a continuous map. Suppose the graph of $f$ is an embedded submanifold of $M \times N$. Is it true that $f$ is smooth?,"['smooth-manifolds', 'differential-geometry', 'differential-topology']"
2422165,Finding a closed rectifiable curve with prescribed winding numbers,This is an exercise from the conway book. Here n(γ;a) is the winding number of γ around a. I have no idea how to find such a curve. To me this exercise seems to require a curve with arbitary winding numbers... Could anyone help me how to find such a curve?,"['complex-analysis', 'winding-number']"
2422236,"How to see that if a Mobius transformation has a fixed point, then its conjugated to the following","I've managed to show that a non-trivial Mobius transformation has 1 or 2 fixed points. Now i was asked to prove the following: Say that $M, N$ mobius transformations are conjugated if there exists a mobius transformation $H$ such that $M \circ H = H \circ N$. Now 1) Show that if $M$ is a mobius transformation with only one fixed point, then $M$ is conjugated to a mobius transformation $N$ of the form $N(z) = az + b$ 2) Show that if $M$ is a mobius transformation with exactly 2 fixed points, then $M$ is conjugated to a mobius transformation $N$ of the form $N(z) = az$ I have no idea how to prove this. Can anyone tell me, or at least give me a hint. Thank you.",['complex-analysis']
2422325,Proof verification: $T\in\mathcal{L}(\mathcal{P}(\bf{R}))$ being injective and $\deg Tp\leq\deg p$ implies $T$ is surjective and $\deg Tp=\deg p$,"Notations : $\mathcal P(\mathbf R)$ denotes the polynomials with real coefficients, while $\mathcal{L}(\mathcal P(\mathbf R))$ denotes the linear transformations within $\mathcal P(\mathbf R)$ with respect to the field of real numbers. Also, $\mathcal P_m(\mathbf R)$ denotes the polynomials with real coefficient and degree less than or equal to $m$. Are the proofs to the following propositions Correct ? Theorem. Given that $T\in\mathcal{L}(\mathcal{P}(\mathbf{R}))$ is injective and that $\deg Tp\leq\deg p$ for all $p\in\mathcal{P}(\mathbf{R})$ such that $p$ is a non-zero polynomial then $a)$ $T$ is surjective Proof. Let $p$ be arbitrary non-zero polynomial. We may assume without loss of generality that $\deg p = m$ where $m\in\{0,1,2,3,...\}$ Let us now examine the function $T|_{\mathcal{P}_m\mathbf{(R)}}$. The proposition $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ together with $T\in\mathcal{L}(\mathcal{P}\mathbf{(R)})$ implies that $T|_{\mathcal{P}_m\mathbf{(R)}}\in\mathcal{L}(\mathcal{P}_m\mathbf{(R)})$, moreover the injectivity of $T$ implies the the injectivity  of $T|_{\mathcal{P}_m\mathbf{(R)}}$ and since injectivity and surjectivity are equivalent for linear operators defined on a finite-dimensional vector space it follows that $T|_{\mathcal{P}_m\mathbf{(R)}}$ is surjective, consequently $\exists q\in \mathcal{P(\mathbf{R})}(Tq=p)$. $\blacksquare$ $b)$ $\deg Tp=\deg p$ for all non zero polynomial $p\in\mathcal{P}(\mathbf{R})$ Proof. We prove the above propostion by recourse to Mathematical-Induction . Basis-Step: Let $p$ be an arbitrary polynomial with $\deg p=0$ ($p$ is non-zero) we know that $\forall p\in\mathcal{P}(\mathbf{R})(\deg Tp\leq\deg p)$ thus in particular $\deg Tp\leq \deg p$. Now assume for the purpose of contradiction that $\deg Tp<\deg p$ which implies that $\deg Tp = -\infty$ but since $T$ is injective and therefore $\operatorname{null}T=\{0\}$ it must be that $p=0$ a contradiction consequently $\deg Tp=\deg p=0$. Inductive Step: Assume that $k$ is arbitrary and that $\deg Tp=\deg p$ where $\deg p\in\{0,1,2,3,...,k\}$. Consider now an arbitrary polynomial $q$ such that $\deg q=k+1$ assume further that $Tq=r$ and that $\deg r<\deg q$ therefore $\deg r\leq k$ since $T$ is surjective it follows that for some $s\in\mathcal{P(\mathbf{R})}$, $Ts=r$ and by inductive hypothesis $\deg s = \deg r \leq k$ but $Ts=r=Tq$ and $s\neq q$ contradicting the fact that $T$ is injective therefore it must be that $\deg q=\deg r$. $\blacksquare$","['linear-algebra', 'proof-verification', 'linear-transformations']"
2422347,"Assume that $\alpha, \beta, \gamma \in [0,\pi/2]$, and $\sin\alpha+\sin\gamma=\sin\beta$, $\cos\beta+\cos\gamma=\cos\alpha$. Find $\alpha-\beta$.","Assume that $\{\alpha, \beta, \gamma\} \subset \left[0,\frac{\pi}{2}\right]$ , $\sin\alpha+\sin\gamma=\sin\beta$ and $\cos\beta+\cos\gamma=\cos\alpha$ . Try to find a value of $\alpha-\beta$ . Actually I have gotten that $\alpha+2\gamma+\beta=\pi$ and $\sin2\alpha+\sin2\beta=\sin(\alpha+\beta)$ , $\cos2\alpha+\cos2\beta=\cos(\alpha+\beta)$ But I can't get further more.",['trigonometry']
2422377,Geometric interpretation of Christoffel symbol symmetry in polar and cylindrical coordinates,"As outlined in this blog post , we can give a geometric interpretation to the Christoffel symbols (of the second kind) as follows: If you take the vector $\partial_i$ and infinitesimally translate this in the direction of $\partial_j$, it will change by $\Gamma_{ij}^k\partial_k$. For example, consider polar coordinates in the plane $(r,\phi)$: If we take the vector $\partial_r$ and translate this outwards in the $r$-direction, the vector is unchanged since it still has the same direction and magnitude. Thus $\Gamma_{rr}^r=\Gamma_{rr}^\phi=0$. If we translate the vector $\partial_r$ in the $\theta$-direction, the change in direction is given by $\frac{1}{r}\partial_{\theta}$, so $\Gamma_{r\theta}^r=0$ and $\Gamma_{r\theta}^\theta=\frac{1}{r}$. If we calculate these symbols, we find that they are symmetric in the lower two indices. The same goes for the Christoffel symbols for cylindrical and polar coordinates. Why is this? It doesn't seem 'obvious' to me that the change in $\partial_r$ from translation in the $\partial_\theta$ direction should be the same as the change of $\partial_\theta$ in the $\partial_r$ direction. I know that this is a feature of the Levi-Civita connection, and you can see this from its definition in terms of the metric. However, is there a geometric way of looking at this?","['riemannian-geometry', 'differential-geometry', 'curvature']"
2422378,"Does there exist any space with $S^n$ as a covering space, that admits an embedding in $R^{n+1}$?","As determined here , there exist spaces covered by $S^n$ that can be immersed in $R^{n+1}$. However, the existence of an embedding is a stronger condition. I suspect it to be impossible. Is there some sort of ""trick""/easy proof? What I've calculated so far: For any space $M$ covered by the sphere, we have $H_{0}\cong H_{n}\cong\mathbb{Z}$. All other homology groups are  are torsion groups $T_m$, with $k*g=0,\forall g\in T_{m}$, where $k$ is the degree of the map from $S^n$ to $M$. If $M$ is embeddable, then there exists $N$ such that $\partial N=M$. With use of the Alexander duality theorem, we can determine the relationships between the homology groups of $M$ and those of $N$: $T_{k}(M)\cong T_{k}(N)\times T_{n-k-1}(N)$, where $T_k$ represents the torsion-subgroup of $H_k$ . Also, $H_k(N)=T_K(N)$ for $k>0$. Additional : If $M$ admits an embedding, then it must admit an immersion. $M$ admits an immersion if and only if $M\times (0,1)$ is parallelizable.","['algebraic-topology', 'characteristic-classes', 'differential-topology', 'geometry']"
2422387,A tricky problem on permutations,"Assume that $x_1, x_2, ......, x_{10}$ is a permutation of $1, 2, ......, 10$. For $1\leq m < n \leq 10$, we have $x_m+m\leq x_n+n$. Try to find the number of those permutations. We can find that the inequality is equivalent to: $$x_n-1\leq x_{n+1}$$ but I can't get further more. Anyone has any idea?","['permutations', 'combinatorics']"
2422434,Why am I not correct when calculating the number of ways of selecting a pair when a five-card hand is drawn?,"Discrete Maths noob here. I'm trying to calculate amount of possible choices when choosing exactly one pair out of a deck of cards. I'm thinking that you could first choose any of the 52 cards, and that your next choice must one out of three cards (because you chose a value in your first choice), after that you can choose any out of the 48 remaining cards (subtracting the four varieties of the first card chosen), and then you can choose 1 out of 44, and lastly one out of 40. $$
{{52}\choose{1}}\cdot{{3}\choose{1}}\cdot{{48}\choose{1}}\cdot{{44}\choose{1}}\cdot{{40}\choose{1}}
$$ But this results in a really wrong answer. Any suggestions on how to think?","['combinatorics', 'discrete-mathematics']"
2422441,"Why is the dot product of two vectors $\mathbf{x},\mathbf{y}$ the same as $x^T y$?","I've always thought that a $1\times 1$ matrix is not the same as a scalar. However, in $\textbf{many}$ times throughout my first year in undergrad, I see people interchange $x\cdot y$ with $x^T y$ (a scalar in the former, and a $1\times 1$ matrix in the latter). Is this just sloppy/lazy notation? Surely there will be instances where using one or the other ""breaks"" the working of a question? e.g. Let $\mathbf{x,y}$ be vectors in $\mathbb{R}^3$, and let $X$ be a $2\times 2$ matrix. Then $(x\cdot y)X$ is defined, yet $x^T y X$ is not defined. Is there a situation where treating them as equivalent is beneficial?","['matrices', 'inner-products']"
2422494,"$n$ children throw respectively one ball at $m$ buckets at the same time, how many buckets can we expect to be empty?","Assume that $n$ children throw respectively one ball at $m$ buckets at the same time. The children throw so well that everyone hits a bucket. Determine the expected amount of empty buckets. First, we need a random variable $X$ that counts the number of the empty buckets. We define $$X := X_1 + X_2 + \ ... \ X_m,$$ with $X_k = 1$ if the $k$-th bucket is empty (and $0$ otherwise). The probability for one arbitrary bucket being empty can be calculated the following way: Since there are $m$ buckets, there is a chance of ${m-1 \over m}$ that one arbitrary child does not hit one specific bucket. Since there are $n$ children throwing the balls, there is a chance of $({m-1 \over m})^n$ that one specific bucket remains empty after every ball has been thrown. $X_k$ is obviously Bernoulli-distributed, so $E(X_k) = ({m-1 \over m})^n$ Using the linearity of the expected value, I receive $$E(X) = \sum_{k=1}^m \bigg({m-1 \over m}\bigg)^n = m\bigg({m-1 \over m}\bigg)^n$$ Using some examples, this seems to make sense. Say, we have $2$ children throwing the balls at $100$ buckets, we would receive $E(X) = 98,01$, which means that a little bit more than $98$ buckets should be empty, which makes sense considering the fact that both children could hit the same bucket, although it is quite unlikely. Is this solution correct? Furthermore, is there another, maybe more ""combinatorial-like""-way to go about this? This question was asked in our Discrete class, and although we did some probability theory there, I feel like there might be another approach to this problem.","['probability-theory', 'discrete-mathematics']"
2422499,How to define the exponential function without calculus? [duplicate],"This question already has answers here : An ""elementary"" approach to complex exponents? (3 answers) Closed 6 years ago . For fun, I would like to define the complex exponential function from these two properties: $\exp(0) = 1$ $\exp(z + w) = \exp(z) \exp(w)$ From here, I would like to find a way to compute values of $\exp(z)$, or at least to compute $\exp(1)$. So far, I found only two ways: Noting that $\exp'(z) = \exp(z)$ and solving the differential equation, which leads to $\int \frac{\exp'(z)}{\exp(z)} dz = \log(\exp(z)) + C = z$. Noting that $\exp'(z) = \exp(z)$, computing its Taylor series and checking that what I get is an entire function. The first approach is simply wrong because it involves logarithms, which I have not defined yet. The second approach looks much better. I haven't tried, but I guess I can find a way to manipulate the Taylor series to obtain the limit definition of $e$ and conclude that $\exp(1) = e$, which is my aim. However, I'm struggling to find another way that does not involve differentiation or limits in general. I would be happy to find a way to say $\exp(1) = e$ without calculus. I think that the irrational nature of $e$ forces me to use limits -- am I right?","['complex-analysis', 'exponential-function']"
2422545,"$f(z)$ is holomorphic, prove that $g(z) = \overline{f(\overline{z})}$ is holomorphic.","$f(z)$ is holomorphic, prove that $g(z) = \overline{f(\overline{z})}$
  is holomorphic. My attempt With $f(z) = f(x+iy) = u(x,y) + iv(x,y)$ we get $$g(z) =\overline{f(\overline{z})} = \overline{f(x+i(-y))} = u(x,-y) - iv(x,-y).$$ Since $f$ is holomorphic, it's true that
$$u'_x(x,y) = v'_y(x,y),$$
$$u'_y(x,y) = -v'_y(x,y).$$
For $g$ to be holomorphic the following must be true
$$u'_x(x,-y) = -v'_y(x,-y)$$
$$u'_y(x,-y) = v'_x(x,-y)$$ Using the Cauchy-Riemann equations for $f$ I get that
$$v'_y(x,-y) = -v'_y(x,-y),$$
$$-v'_x(x,-y) = v'_x(x,-y),$$
must be true for $g$ to be holomorphic, but this can only true for $v'_x = v'_y = 0$ no? How can I make my solution work? Is there a better (simple) way?","['complex-analysis', 'holomorphic-functions']"
2422567,Is $x^j+x^k+2$ irreducible whenever $j+k$ is odd?,"Let $j,k$ be positive integers with $j>k$ and consider the polynomial $$f(x)=x^j+x^k+2$$ I want to prove the conjecture : $f(x)$ is irreducible in $\mathbb Q[x]$, whenever $j+k$ is odd. This is true for $j\le 300$ as I checked with PARI/GP. If $f$ has real roots, they obviously must be negative and the absolute value of any root must be less than $2$ for $j>2$. Moreover, $-1$ cannot be a root because of $f(-1)=2$, so $f(x)$ never can have a linear factor. Can we use the bound of the absolute values of the roots and that the constant coefficient is prime to show that $f(x)$ must be irreducible in $\mathbb Q[x]$","['irreducible-polynomials', 'roots', 'calculus']"
2422576,I'm not sure I'm correct: $\|\sum_{n=1}^\infty x_n\| \leq \sum_{n=1}^\infty \|x_n\|$,"I was solving this Functional Analysis problem, but I'm not sure I'm correct on this one, the problem is: Let $\big(E,\|\bullet\|\big)$ be a normed vector space, and $(x_n)_n$ a sequence in $E$ such that $\sum_{n=1}^\infty x_n$ converges. Show
  that 
  $$\left\|\sum_{n=1}^\infty x_n\right\| \leq \sum_{n=1}^\infty\|x_n\|$$ My solution: We know that the following inequality holds for any $m\in\mathbb{N}$
  (just using the triangular inequality): $$\left\|\sum_{n=1}^m x_n\right\| \leq \sum_{n=1}^m \|x_n\|$$ Since the left side converges, the inequality still holds in the limit
  $m\rightarrow\infty$, hence we have the result. My doubts: The only thing I am not totally sure is my last statement, because I can't know if the right side converges, so how can I compare the two things? But then I thought I can use the following argument: The sequence $\left(\sum_{n=1}^m \|x_n\|\right)_m$ is clearly a monotone (non-decreasing) real sequence, so if it is bounded it converges and my result is correct, the other option is that the sequence is unbounded, in which case $$\sum_{n=1}^\infty \|x_n\|=\infty$$ And then it makes sense to write: $$\left\|\sum_{n=1}^\infty x_n\right\| < \infty$$ Can someone tell me if my reasoning is correct? Or if another way would be better to solve this problem?","['functional-analysis', 'normed-spaces', 'inequality', 'sequences-and-series']"
2422598,Prove that the graph has at least 200 vertices,"In the company, each employee has at least 50 acquaintances. It turned out that there are two employees who are familiar only through 9 handshakes (i.e. the shortest way of communication consists of 8 intermediate people). Prove that at least 200 employees work in this company. My attempts of solution are straightforward. Consider a graph in which the vertices are people, and the edge denotes the familiarity of people. Each vertex has 50 edges and there are 2 vertices whose shortest distance is 9. We enumerate the path. Note that the vertices connected to the first vertex can not coincide with the vertices connected to a 5 or 6 vertex. Similarly, vertices connected to 5 or 6 vertices can not intersect with vertices connected to 10 vertex. Otherwise, the graph would have a path shorter than 9. Thus there are at least 150 vertices in the graph. However, I do not know how to prove about 200 vertices. Thank you for any help or ideas!","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2422610,Prove $\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B$ using only minors,"Let $A,B$ be two $(m,n)$ matrices. Prove that $$\def\rk{\operatorname{rank}}\rk{(A+B)}\leq \rk A+\rk B$$
using only the definition of $\rk$ involving non-zero minors. I expressed every minor $\Delta_r$ of $A+B$ as a linear combination of minors of $A$ and $B$, each with order not less than $\lceil \frac{r}{2} \rceil$, but this approach didn't get me to the result. I am a beginner in linear algebra and that is the only definition of $\rk$ I have learnt, so I would highly appreciate any solution using it. EDIT: I think I managed to find a solution.
Let $A=(a_{ij}), B=(b_{ij})$ with $i\leq m$ and $j\leq n$. Also, $\rk A=r$ and $\rk B=s$. Then $A+B=(a_{ij}+b_{ij})$ and let $\Delta_{r+s+1}$ be a minor of $A+B$ of order $(r+s+1)$. Let $k=r+s+1$.
$$\Delta_{k}=\det \begin{pmatrix}
a_{i_1j_1}+b_{i_1j_1} & a_{i_1j_2}+b_{i_1j_2} &... &a_{i_1j_{k}}b_{i_1j_{k}} \\
...&...&...&...\\
a_{i_{k}j_1}+b_{i_{k}j_1}& a_{i_kj_2}+b_{i_kj_2}& ...& a_{i_kj_k}+b_{i_kj_k}\end{pmatrix}$$
Using the properties of determinants, we can develop the one above as a linear combination of determinants with the form
$$\det \begin{pmatrix}
a_{i_1p_1} & a_{i_1p_2}&... &a_{i_1p_{x}} & b_{i_1q_1} &b_{i_1q_2} &... &b_{i_1q_y} \\
a_{i_2p_1} & a_{i_2p_2}&... &a_{i_2p_{x}} & b_{i_2q_1} &b_{i_2q_2} &... &b_{i_2q_y} \\
...&...&...&...&...&...&...&...\\
a_{i_kp_1}& a_{i_kp_2}& ...& a_{i_kp_x} & b_{i_kq_1} & b_{i_kq_2} 
 & ... &b_{i_kq_y}\end{pmatrix}$$
where $x+y=k=r+s+1$ and $ \{ p_1,p_2,...p_x,q_1,q_2,...q_y \} = \{j_1,j_2,...j_k \}$ If $x \leq r$ then $y \geq s+1$ and we can write the last determinant as a linear combination of $(k-x)$order determinants consisting only of elements of  $B$. Since $k-x=y$, these will be minors of $B$ with order $y \geq s+1$ and since $\rk B=s$ it means that all these minors are $0$. The same holds true if $x\geq r+1$, when we write the determinant as a linear combination of minors of $A$ with order $x$, thus all equal to $0$. In conclusion, all the determinants with the form as above are $0$ and so $\Delta_k=0$, which means that $\rk(A+B) \leq r+s=\rk A+\rk B$","['matrices', 'matrix-rank', 'linear-algebra']"
2422612,"What is the difference between a kernel, and kernel (Gram) matrix?","Given a kernel , can we represent it as a Gram matrix ? For example, a linear kernel can be presented (in Python/MATLAB code) in a Gram matrix as follows: K = X*X.T . If this is true, how to represent other non-trivial kernels in their Gram matrix form, e.g., check the following link, page 5 , equation 17 showing Jensen-Shannon kernel : K(p,q) = exp(-JS(p||q)) https://pdfs.semanticscholar.org/3e43/4ca7cbd1869f41e338658f7ab4f954782ad8.pdf","['matrices', 'positive-definite', 'machine-learning', 'positive-semidefinite', 'python']"
2422620,Bounding Plane and Projective Occlusion of Cone and Ellipsoid Intersection,"We position a view cone in euclidean 3-space at origin and orient it towards $z$. Through this cone we observe an ellipsoid of arbitrary size, position and orientation which may or may not be intersecting with our cone volume. Now we wish to find out Is this ellipsoid intersecting our view cone volume at all? (i.e. can we see it?) If yes, what are the distances of the two planes orthogonal to $z$ that bound 
not the entire ellipsoid, but only the intersection? (giving us a one-dimensional range along the cone ray that covers the visible part of the ellipsoid) Does the visible part of the ellipsoid projected to the far plane cover 100% of the circular intersection of the view cone and the far plane? (i.e. Does it fully occlude any space behind it from the view cone's perspective) If yes, what is the nearest distance of a plane at which 100% of that circle is covered? (giving us a depth horizon after which any information further away can be discarded because it is guaranteed to be invisible) For these questions, I'm not only interested in solutions, but also approaches and general guidance, even if it's just about the 2-space analog case. Are there direct analytical solutions or at least iterative methods that converge quickly, precise or approximate? Would it be easier to use a 4-sided view pyramid rather than a cone? My background is in linear algebra and implicit surface distance estimation. Projective geometry is very new to me so any pointers are appreciated.","['euclidean-geometry', 'linear-algebra', 'projective-geometry']"
2422622,Is This Bayes' Theorem Solution Correct?,"Q: Considering 2 slot machines. One machine has a 10% chance of paying out, the other has a 20% chance.
One machine is red, the other green. You don't know which pays more.
Initially, you're 50% sure that the red machine pays more. You try it once, and it doesn't pay out. After this first attempt, how should you update the estimate that the red machine has a higher chance of paying out? So, I did this, but I'm not sure if it's right. Any clarification appreciated! $P(Red\ Pays\ More|No\ Pay\ Out) = P(No\ Pay\ Out|Red\ Pays\ More)P(Red\ Pays\ More)/P(No\ Pay\ Out)$ I assumed the $P(No\ Pay\ Out|Red\ Pays\ More) = 0.8$ (because there is a 0.8 chance of not being paid if the red machine is the one that pays more). $P(Red\ Pays\ More) = 0.5$ $P(No\ Pay\ Out) = 0.85$","['bayes-theorem', 'statistics', 'probability']"
2422626,Generating functions for the solutions to $3x_1 + x_2 = n$,"I need to find the amount of solutions with non negative integers to the equation $x_1 + x_2 + x_3 = n$
with $x_2 = 2x_1$. I transformed it to $y_1 + y_2 = n$ when $y_1=3x_1, y_2 = x_3$ (because $x_1 + x_2 + x_3 = n$ here is the same as $x_1 + 2x_1 + x_3 = n$, which is $3x_1 + x_3 = n$). Using generating functions $$[x^n] = \frac {1}{(1-x^3)(1-x)}$$ which is by expanding $1-x^3$:$$[x^n] = \frac {1}{(1-x)^2(1+x+x^2)}$$
The problem here is that that the expansion of $(1+x+x^2)$ involves complex numbers so I'm not sure how to proceed from here.","['generating-functions', 'combinatorics', 'binomial-theorem']"
2422633,Is there an easy way to (mentally) sum i.i.d. discrete uniform random variables?,"I was recently trying to do some mental mathematics and found myself stumped when trying to handle what ought to be a simple problem. My issues essentially boiled down to the question that I've asked, although for the sake of making the numbers easy I suppose that we can stick with the discrete uniform distribution with P(X=x)=1/6, i.e. the distribution that most often be used in problems related to six-sided fair dice. So, with all of this is mind, is there any ""easy"" way to even mentally do something as simple as find the distribution of the sum of two i.i.d discrete uniform random variables, each with P(X=x)=1/6? What if I was trying do something more advanced like finding the probability that the sum of 6 fair six-sided dice is greater than or equal to 18?",['statistics']
2422658,What is the structure group of the tangent bundle?,"I was wondering if anyone could tell me (by example) what is ""the structure group of the tangent bundle""? Next I want to know what is the meaning of the following sentence? Since the structure group of the tangent bundle of $M$ reduces to $U(n)$ and the determinant of every
  element of $U(n)$ is positive then $M$ is orientable. Thanks in advance","['manifolds', 'fiber-bundles', 'smooth-manifolds', 'differential-geometry']"
2422664,"Solving $\Box u=0$ in an open set with $u(p)$ and $\mathrm{grad}\, u(p)$ prescribed","Consider a $C^k$, $k\ge 2$, Lorentzian manifold $(M,g)$ and let $\Box$ be the usual wave operator $\nabla^a\nabla_a$. Given $p\in M$, $s\in\Bbb R,$ and $v\in T_pM$, can we find a neighborhood $U$ of $p$ and $u\in C^k(U)$ such that $\Box u=0$, $u(p)=s$ and $\mathrm{grad}\, u(p)=v$?","['riemannian-geometry', 'differential-geometry', 'general-relativity', 'partial-differential-equations']"
2422666,"How to prove this statement with ""first"" principle of Mathematical induction and not strong Mathematical induction?","Define a sequence $s_0,s_1,s_2,...$ as follows :
  $$s_0=0, s_1=4, s_k=6s_{k-1} - 5s_{k-2} \; \forall \; \text{integers} \; k\ge 2.$$ Prove by Principle of strong mathematical induction that $$s_n=5^n-1 \; \forall \; n\ge 0.$$ It's very easy to prove this by strong mathematical induction. However I am interested in proving this by ""first"" principle of mathematical induction only. Any ideas?","['exponential-function', 'algebra-precalculus', 'induction', 'sequences-and-series', 'discrete-mathematics']"
2422672,Understanding a proof about Riemannian metrics in three dimensions always being diagonalizable,"I've recently been working through Deturck's and Yang's Existence of elastic deformations with prescribed principal strains . First and formost, I'm interested in it's proof that Riemannian metrics can in three dimensions always be diagonalized, that is, we can always find an atlas of coordinate functions so that the metric components become $g_{i j} = 0$ for $i \neq j$ when represented with regards to this metric, but I have some trouble fully understanding it. Now the general idea of the proof is to choose some orthonormal frames $\{\overline{e_1}, \overline{e_2}, \overline{e_3}\}$ of vector fields on $M$, a corresponding dual basis $\{\overline{\omega}^1, \overline{\omega}^2, \overline{\omega}^3\}$ of one-forms (which he calls the reference frame), and solving them for a set of coordinate functions $\{x^1, x^2, x^3\}$ in which the metric becomes diagonal, with the dual coframe $\{\omega^1, \omega^2, \omega^3\}$. Now a large part of the proof comes down to some tedious but simple calculations involving the Frobenius theorem, the structure equations and some other fundamental constructs, but it's the final step of the proof that I cannot wrap my head around so far and that I'm hoping someone can explain to me. I hope it suffices to only reproduce the final part of the proof aswell as an outline of the preliminary steps of the proof here; if anyone shall require more of the proof, feel free to let me know. Now let, as mentioned above, $\omega^i, i = 1, 2, 3$ be the orthonormal coframe to the desired coordinate functions $(x^1, x^2, x^3)$ to which our metric becomes diagonal of which we want to show the existence. The first major step in the proof is that they show it's an equivalent condition for the existence of such a coframe $\omega^i$ with the property that the corresponding frame diagonalizes the metric is that $$\omega^1 \wedge \omega^2 \wedge \omega_2^1 = 0, \omega^1 \wedge \omega^3 \wedge \omega_3^1 = 0, \omega^2 \wedge \omega^3 \wedge \omega_3^2 = 0 \tag{1} $$ where $\omega_i^j$ is the connection form. Given the existence of such a coframe, one would be able to represent it with respect to the reference frame $\omega^j$ via $\omega^i = \sum_{j=1}^n b_j^i \overline{\omega}^j$ for some coefficients $b_i^j$ where $(b_i^j) = b \in C^\infty(M, SO(3))$. In other words, it's sufficient to find such a matrix-valued function $b$ with this property. Using some more calculations, they then rewrite the equation $(1)$ without dependency on the desired coframe $\omega^i$ but instead with the matrix components $b_i^j$; namely, they show that the existence of $\omega^i$ that satisfy equation $(1)$ is equivalent to the existence of a $b \in C^\infty(M, SO(3))$ that satisfy: $$0 = \sum_{p, q, j, k} b_p^i b_q^l \overline{\omega}^p \wedge \overline{\omega}^q \wedge \left( \frac 12 \left( b_k^l \overline{e}_k (b_j^i) - b_k^i \overline{e}_k (b_j^l) \right) \overline{\omega}^j + b_k^l b_j^i \overline{\omega}_k^j \right) \tag{2}$$ I'm sorry for this lengthy lead-up; now my actual question starts here as this is where I can't follow anymore. I'll just quote the rest of the proof step-by-step: Proposition 4.8. The linearization of [$(2)$] is diagonal hyperbolic. This is one of my main issues so far: Deturck and Yang go on to proof this statement, but they don't lose a word about how the existence from such functions $b_i^j$ can be seen from this fact. Why does the fact that the linearization of $(2)$ is diagonal hyperbolic guarantee the existence of a solution? What exactly are our $b_i^j$'s, why do they exist because of it? Proof. It suffices to linearize [(2)] around the frame where $b_j^i(x) \equiv \delta_j^i$, since we can choose the reference frame $\{\overline{\omega}^i\}$ to be equal to the frame $\{\omega^i\}$ around which we are linearizing. Now... what exactly is happening here? Why can we choose the desired frame $\overline{\omega}^i$ to be (locally, I assume?) equal to the reference frame? One of them is (supposed to be) diagonal whereas the other one is not, so why can they be equal here? Let $\beta_j^i = (d b)_j^i$ be the variation in $b - \beta_j^i$ is a skew-symmetric matrix-valued function. My next problem here is: which variance exactly are they talking about here? How is this variance defined? I'm sorry but I'm not familiar with any definition of variance in this context and couldn't find a definition that makes sense so far. Do they mean something like the (exterior) derivative of $b$ since they write $d b$? Or is it something else entirely? I otherwise only know a variance from a stochastic context and I highly doubt that's what they mean here. The linearization of [(2)] is thus: $$\frac 12 \left( \overline{e}_i \left(\beta_j^i\right) - \overline{e}_i \left(\beta_j^l\right) \right) \overline{\omega}^i \wedge \overline{\omega}^l \wedge \overline{\omega}^j + \text{ lower order terms in } \beta = 0$$ for $(j, j, l ) = (1, 2, 3), (2, 3, 1),$ and $(3, 1, 2)$ I guess I could understand why this is the linearization if I knew what exactly the variance here is. We write out the three equations for the linearization: $$\frac 12 \left( \overline{e}_1 \left( \beta_3^2 \right) - \overline{e}_2 \left(\beta_3^1 \right) \right) = \text{lower order terms in } \beta = 0 $$ $$\frac 12 \left( \overline{e}_2 \left( \beta_1^3 \right) - \overline{e}_3 \left(\beta_1^2 \right) \right) = \text{lower order terms in } \beta = 0 $$
  $$\frac 12 \left( \overline{e}_3 \left( \beta_2^1 \right) - \overline{e}_1 \left(\beta_2^3 \right) \right) = \text{lower order terms in } \beta = 0 $$ By alternately adding two of the equations together and subtracting the other, we obtain a system of the form: $$\overline{e}_1(\beta_3^2) = \text{lower order terms in } \beta = 0
$$ $$\overline{e}_2(\beta_1^3) = \text{lower order terms in } \beta = 0$$ $$\overline{e}_3(\beta_2^1) = \text{lower order terms in } \beta = 0$$ This is obviously diagonal form. q.e.d. This final part I can follow again: given the linearization he derived, I think I can see that the linearization here is diagonal. But, as mentioned above, how does the fact that this linearization is diagonal prove that a function $b \in C^\infty(M, SO(3))$ exists so that $(3)$ is satisfied? What's the connection here that I'm missing? I realize that this is a lengthy question, and I hope I phrased it in a manner that makes it clear what I can and what I cannot understand, and what answer(s) I seek here. To resume, my main issue is understanding how the linearization of system $(2)$ being diagonal is sufficient for the existence of a matrix function $b$ that satisfies $(2)$, why the desired coframe and the reference coframe can be chosen equal locally, and what exactly this variance ""$\beta_j^i = (d b)_j^i$"" is and how it plays into the linearization of the system $(2)$. Any help would be greatly appreciated. If anyone desires more details about the paper or the preliminary steps of the proof, I'm happy to provide them.","['diagonalization', 'riemannian-geometry', 'differential-geometry', 'linearization']"
2422688,Is there a closed form for $\sum _{k=1}^n \frac{1}{k}H_{k-1}^2$?,"Introduction This question appeared in the study ( Sum of powers of Harmonic Numbers ) of the sums of fourth powers of harmonic numbers $H_n = \sum _{k=1}^n \frac{1}{k}$ for $n=1,2,3,...$ and $H_0 = 0$. Let the sum in question be $$h_{0}(n) = \sum _{k=1}^n \frac{1}{k}H_{k-1}^2$$ Writing $H_{k-1} = H_{k} - 1/k$ we have $$h_{0}(n) = h_{1}(n) - 2 h_{2}(n) + h_{3}(n)$$ Hence the question is equivalent to finding closed forms for $$h_{1}(n)=\sum _{k=1}^n \frac{1}{k}H_{k}{}^2 \tag{1}$$ $$h_{2}(n) =\sum _{k=1}^n \frac{1}{k^{2}}H_{k} \tag{2}$$ $$h_{3}(n) =\sum _{k=1}^n \frac{1}{k^{3}}$$ Now $h_{3}$ is directly identified as a generalised harmonic number $$h_{3} = H_n^{(3)}$$ Some Calculations $h_{1}$ and $h_{2}$ are not independent as we shall prove next $$h_{1}(n) - h_{2}(n) = \frac{1}{3} (H_n^3 - H_n^{(3)})\tag{3}$$ Let us first recall Abel's partial summation $$\sum _{k=1}^n a_{k} b_{k}=A_{n} b_{k} + \sum _{k=1}^{n-1} A_{k} (b_{k}-b_{k+1})$$ where $$A_{k}=\sum _{m=1}^k a_{m}$$ In order to caculate $$h_{1}(n)=\sum _{k=1}^n \frac{H_k^2}{k}$$ we let $a(k)=\frac{1}{k},b(k)=H_k^2$ then $A_{k} = H_{k}$, and proceed with partial summation and (admittedly a little aimless) substitutions (borrowing the pretty layout from robjohn): $$
\begin{align}
\sum_{k=1}^n\frac{H_{k}^2}{k}
&=H_n^3+\sum _{k=1}^{n-1} H_k \left(H_{k}^2-H_{k+1}^2\right)\tag{3a}\\
&=H_n^3+\sum _{k=1}^{n-1} H_k (H_k^2-\left(H_k+\frac{1}{k+1}\right)^2)\tag{3b}\\
&=H_{n}^3-\sum _{k=1}^{n-1} \frac{H_k \left(2 H_k\right)}{k+1}-\sum _{k=1}^{n-1} \frac{H_k}{(k+1)^2}\tag{3c}\\
&=H_n^3-2 \sum _{k=1}^{n-1} \frac{\left(H_{k+1}-\frac{1}{k+1}\right)^2}{k+1}-\sum _{k=1}^{n-1} \frac{H_{k+1}-\frac{1}{k+1}}{(k+1)^2}\tag{3d}\\
&=H_n^3-2 \sum _{k=1}^{n-1} \frac{H_{k+1}^2}{k+1}+4 \sum _{k=1}^{n-1} \frac{H_{k+1}}{(k+1)^2}-\sum _{k=1}^{n-1} \frac{H_{k+1}}{(k+1)^2}-2 \sum _{k=1}^{n-1} \frac{1}{(k+1)^3}+\sum _{k=1}^{n-1} \frac{1}{(k+1)^3}\tag{3e}\\
&=H_n^3-2 \sum _{k=1}^{n-1} \frac{H_{k+1}^2}{k+1}+3 \sum _{k=1}^{n-1} \frac{H_{k+1}}{(k+1)^2}-\sum _{k=1}^{n-1} \frac{1}{(k+1)^3}\tag{3f}\\
&=H_n^3-2 \sum _{m=2}^{n} \frac{H_{m}^2}{m}+3 \sum _{m=2}^{n} \frac{H_{m}}{m^2}-\sum _{m=2}^{n} \frac{1}{m^3}\tag{3g}\\
&=H_n^3-2 \sum _{k=1}^n \frac{H_k^2}{k}+3 \sum _{k=1}^n \frac{H_k}{k^2}-\sum _{k=1}^n \frac{1}{k^3}\tag{3h}\\[9pt]
&3\sum_{k=1}^n\frac{H_{k}^2}{k}-3 \sum _{k=1}^n \frac{H_k}{k^2}= H_n^3-H_n^{(2)}\tag{3i}
\end{align}
$$ Explanation: $\text{(3a)}$: partial summation $\text{(3b)}$: use $H_{k+1}=H_k +\frac{1}{k+1}$ $\text{(3c)}$: expand bracket $\text{(3d)}$: make $H_k$ compatible with the denominator using $H_{k}=H_{k+1} -\frac{1}{k+1}$ $\text{(3e)}$: expand terms $\text{(3f)}$: collect terms $\text{(3g)}$: substitute  $k+1 \to m$ $\text{(3h)}$: start with $m=1$, as all first elements of the sums cancel, and finally let $m\to k$ $\text{(3i)}$: move the sums to the lhs. After division by $3$ the proof of (3) is completed. I found (3) initially by just playing around with substitutions like in this derivation and stopped when I had found a nice formula. We can also distribute the factors of the summand into the components of the partial summation differently, like for instance $a(k)=\frac{H_k}{k},b(k)=H_k$ then $A_{k} = \sum_{m=1}^k\frac{H_{m}}{m} = \frac{1}{2} \left( H_k^2+H_k^{(2)}\right)$ This leads to another interesting formula $$\sum _{k=1}^n \frac{H_k^2}{k}+\sum _{k=1}^n \frac{H_k^{(2)}}{k}=H_n H_n^{(2)}+\frac{1}{3}H_n^3+\frac{2}{3} H_n^{(3)}\tag{4}$$ Numerical series The first several terms of the sequences are $$h_{0}=\left\{0,\frac{1}{2},\frac{5}{4},\frac{301}{144},\frac{71}{24},\frac{82669}{21600},\frac{101191}{21600},\frac{23391199}{4233600},\frac{1074637}{169344},\frac{113452879}{15876000}\right\}$$ $$h_{1}=\left\{1,\frac{17}{8},\frac{701}{216},\frac{7483}{1728},\frac{1160603}{216000},\frac{1376693}{216000},\frac{543360959}{74088000},\frac{4894157017}{592704000},\frac{146372578939}{16003008000}\right\}$$ $$h_{2}=\left\{1,\frac{11}{8},\frac{341}{216},\frac{2953}{1728},\frac{388853}{216000},\frac{403553}{216000},\frac{142339079}{74088000},\frac{1163882707}{592704000},\frac{31983746689}{16003008000}\right\}$$ and, for completemess, $$h_{3}=\left\{1,\frac{9}{8},\frac{251}{216},\frac{2035}{1728},\frac{256103}{216000},\frac{28567}{24000},\frac{9822481}{8232000},\frac{78708473}{65856000},\frac{19148110939}{16003008000}\right\}$$ The online-encyclopedia of integer sequences ( https://oeis.org/ ) lists for numerator and denominator of the sequences the following $$h_{3} = A007408/A007409$$
$$h_{2} = A195505/A195506$$ For $h_{1}$ only the denomintor is listed which, suprisingly, is the same as for $h_{2}$. Notice also that the first 5 terms of the denominators of $h_{2}$ and $h_{3}$ are identical. For $h_{0}$ neither numerator nor denominator are listed.","['harmonic-numbers', 'sequences-and-series']"
2422698,Find $\displaystyle{\lim_{n \to \infty}} \sqrt[n]{n!}$ without Stirling's approximation. [duplicate],"This question already has answers here : Find $\lim_{n \to \infty} \sqrt[n]{n!}$. (6 answers) Closed 6 years ago . Okay I've seen many posts as this one, but in every post it uses Stirling(I can't use it because I've never learnt it), or weird equalities with logs (I also never learnt this) or other methods I'm not allowed to use in my exam. I have to prove that $\lim n!^{(1/n)}$ = $\infty$, only by using D'Alembert, $n^{th}$ root test, Stolz, squeeze theorem or by playing with inequalities etc What I thought is that I could prove that $\lim \frac{1}{n!^{(1/n)}} = 0$ (again only using those basic theorems) and that would mean that $\lim n!^{(1/n)} = \infty$ but I'm not sure if that's correct. Please don't mark as duplicate! I need an answer and I haven't found any post with an answer I can use!!","['real-analysis', 'limits']"
2422702,Under what conditions is $u^x + v^y$ factorable for odd $x$ and $y$?,"In what follows, suppose that $u, v \in \mathbb{N}$.  Furthermore, assume that $u \neq v$. Under what conditions is
$$u^x + v^y$$
factorable for odd $x$ and $y$? (I do know that it is factorable when $x=y$.  I would of course be interested in other conditions.)","['algebra-precalculus', 'number-theory', 'factoring', 'elementary-number-theory']"
2422723,An operator is semi-simple iff it is diagonalizable,"I was reading on direct sums when I got into this Wikipedia page A linear operator T on a finite-dimensional vector space is semi-simple if every T-invariant subspace has a complementary T-invariant subspace. An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an algebraically closed field is semi-simple if and only if it is diagonalizable I thought about how we could prove it, and I am following what is says in the page: This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant hyperplane, which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis. I understand that we can write $T=\bigoplus_i T_{\lambda_i}$ where $T_{\lambda_i}=T|_{U_{\lambda_i}}$ and ${U_{\lambda_i}}$ is the eigenspace spanned by one eigenvector, but why does that imply that $T$ is diagonalizable?","['eigenvalues-eigenvectors', 'diagonalization', 'linear-algebra']"
2422765,Sum of reciprocals of product of consecutive integers,"We know, for instance, that $$\sum_{r=1}^n r^\overline{3}=\sum_{r=1}^nr(r+1)(r+2)=\frac {n(n+1)(n+2)(n+3)}4=\frac {n^\overline{4}}4$$
and, in general, 
$$\sum_{r=1}^n r^\overline{m}=\frac {n^\overline{m+1}}{m+1}\tag{1}$$
which is analogous to 
$$\int_0^n x^m dx=\frac {n^{m+1}}{m+1}\tag{1A}$$ but is there is correspondingly neat form (and analogy) for sum of reciprocals, e.g.
  $\displaystyle\sum_{r=1}^n \frac 1{r(r+1)(r+2)}$
  and in general $$\sum_{r=1}^n \frac 1{r^\overline{m}}\tag{2}$$
  ? Or, alternatively, can it be shown that $(2)$ is an extension of $(1)$? Also, can it be shown easily that 
  $$\sum_{r=1}^\infty\frac 1{r^\overline{m}}=\frac 1{(m-1)(m-1)!}=\frac 1{m!-(m-1)!}$$
  ? NB: Here the notation for the rising factorial is used, i.e. $x^\overline{m}=\overbrace{x(x+1)(x+2)\cdots (x+m-1)}^{m\text{ terms}}$. Addendum See this for a further generalisation.","['telescopic-series', 'summation', 'sequences-and-series']"
2422770,Mean and variance of a Brownian motion process,"Say $X(t),t\geq 0,$ denotes a Brownian motion process with drift parameter $\mu=3$ and variance parameter $\sigma^2 = 9$. If $X(0)=10$ I want to find $E[X(2)]$ $Var[X(2)]$ $P(X(2)>20)$ My reasoning is the following: Since the Brownian motion has a Gaussian probability density with mean $t\mu$ and variance $t\sigma^2$, I would say that $E[X(2)] = 2\times 3 = 6$ $Var[X(2)] = 2\times 9 = 18$ $P(X(2)>20) = \int_{20}^\infty{\frac{1}{6\sqrt{2\pi}}e^{-(x-6)^2/36}dx}$ Is my reasoning correct?","['stochastic-processes', 'probability-theory', 'probability', 'brownian-motion']"
2422791,Prove that for any $x\geq0$ and $y \ge 0$ we have: $|\sqrt{x} − \sqrt{y}| \le \sqrt{|x − y|}.$,"Prove that for any $x\geq0$ and $y \ge 0$ we have:   $$|\sqrt{x} − \sqrt{y}|   \le \sqrt{|x − y|}.$$ $$|\sqrt(x) - \sqrt(y)|^2 \le \sqrt{|x-y|}^2$$
$$(\sqrt{x}-\sqrt{y})^2 \le |x-y|$$
$$x - 2\sqrt{xy} + y \le |x-y|$$ How do I take the absolute value separately to prove it?","['radicals', 'inequality', 'algebra-precalculus', 'proof-writing', 'absolute-value']"
2422793,A group of order $n^2$ with $n+1$ subgroups of order $n$ with trivial intersection is abelian,"The following problem is from my exercise sheet in group theory. I managed to do the first two items, but i am stuck with the third. Hints and comments on the solutions presented below will be the most apreciated. Problem: Let G be a group of order $n^2$ with $n+1$ subgroups of order $n$, such that the intersection of any two of them is $\{ e \}$. Show that: a) If $H$ and $K$ are two subgroups of order $n$, then $HK = G$. b) If $H$ is a subgroup of order $n$, then $H$ is normal in G. c) G is abelian. Solution: a) Since $H$ and $K$ are finite subgroups of order $n$ and $H \cap K = \{e\}$, we have that
$$
|HK| = \frac{|H||K|}{|H \cap K} = \frac{n.n}{1}= n^2
$$
Then, since $HK \subset G$ and have the same number of elements, then $HK = G$. b) Let $K \leq G$ such that $|K| = n, \ K \not= H$. By the previous item, we know that  $KH=G$. Since the left cosets form a partition of the set, we have that 
$$
G = \bigcup_{k \in K}kH.
$$
Now, suppose that for some fixed $k \in K, kH \not= Hk$. Since both sets have the same number of elements, then 
$$
Hk \setminus kH \not= \emptyset \  \therefore \  \exists \ k' \in K \setminus
\{k\},\  \exists \ h,h' \in H; \ hk = k'h'
$$ 
Then
$$
k' = hk(h')^{-1} \in (eH)(kH) = kH
$$
what is a contradiction. Therefore, $kH = Hk \ \forall \ k \in K$ and then follows that $H \vartriangleleft G$. c) ? Thanks in advance.","['finite-groups', 'abelian-groups', 'group-theory']"
2422800,Single approach to solving differential equations,"I am an engineer who uses mathematics for applications. I have learnt how to solve differential equations, both ordinary and partial. My impression has been that solving differential equations is all about knowing a bag of diverse tricks: separation of variables, reduction in order, power series method, etc. I would like to know if there is a single approach that would work for differential equations. I don't mind if the approach is tedious or if it involves successive approximations. All I wish for is that the procedure of solving differential equations be mechanical in nature, and applicable to widest possible variety of differential equations. I first thought that writing unknown function as Taylor series and successively finding the unknown coefficients is a very general, although tedious (which is alright with me), approach to solving differential equations. However I later learnt that it works only if the expansion is carried about a regular point, otherwise it gives nonsensical answer. Recently I have begun studying one-parameter group theoretic method for solving differential equations, and the author of a book promises it is a very general method. I wished to ask your opinion regarding this and whether there are any other general approaches which could be learnt with minimum prerequisites. Thanks in advance for any advice.","['ordinary-differential-equations', 'online-resources', 'partial-differential-equations']"
2422807,Triangle inequality for Minkowski functional,"Let $X$ be a vector space over $\mathbb K = \mathbb C$ or $\mathbb K =\mathbb R$ and $A \subseteq X$ absorbing. I want to prove that if $A$ is convex, the Minkowski functional
$$\mu_A:X\to \mathbb R_0^+, \quad \mu_A(x) := \inf\{t \in \mathbb R^+:t^{-1}x\in A\}$$
satisfies the triangle inequality, i.e.
$$\forall x,y\in X: \mu_A(x+y) \leq \mu_A(x)+\mu_A(y).$$ My attempt:
I already proved the result, that if a $A$ convex, then
$$\forall x\in X \,\forall s>\mu_A(x):s^{-1}x \in A. \quad (1)$$
Now first assume, that $\mu_A(x) = \mu_A(y) =: r$. From $(1)$, we know that for every $\epsilon > 0$ we have $$(r+ \varepsilon)^{-1}x \in A, (r+ \varepsilon)^{-1}y \in A.$$
Since $A$ is convex, $$\frac{1}{2}(r+ \varepsilon)^{-1}x + \frac{1}{2}(r+ \varepsilon)^{-1}y = (r+ \varepsilon)^{-1}(\frac{1}{2}x+\frac{1}{2}y) \in A.$$
By definition that gives us $$\frac{1}{2}\mu_A(x+y)=\mu_A\left(\frac{1}{2}x + \frac{1}{2}y\right) \leq r+\epsilon = \mu_A(x) + \mu_B(x) + \epsilon,$$
proving this special case.
Now I tried to proceed similarly when $\mu_A(x) \neq \mu_A(y)$, but I didn't quit get where I wanted. I think manipulating the convex combination in the right way might be the correct thing. Any hints?","['functional-analysis', 'topological-vector-spaces', 'analysis', 'vector-spaces']"
2422816,"Show that $ \frac{{d}}{dx}\sin(x) = \cos(x)$, using summation forms.",It is well-known that: $ \frac{{d}}{dx}\sin(x) = \cos(x)$. [statement (1)] Given the definitions: $$\sin(x) = \sum_{n=1}^{\infty} \frac{(-1)^{n-1}}{(2n-1)!} x^{2n-1}$$ And: $$\cos(x) = \sum_{n=0}^{\infty} \frac{(-1)^{n}}{(2n)!} x^{2n}$$ Can you show that statement (1) is valid?,"['summation', 'trigonometry']"
2422827,If $\dfrac{d\theta}{dt} = \omega$ then show that $\dfrac{d^2 \theta}{dt^2} = \omega \dfrac{d\omega}{d\theta}$.,"Let $$ \frac{d\theta}{dt} = \omega$$ show that $\frac{d^2\theta}{dt^2}$ can be expressed as $$\omega\frac{d\omega}{d\theta}$$ My approach is as follows: $$\frac{d^2\theta}{dt^2} = \frac{d}{dt}\biggl(\frac{d\theta}{dt}\biggl)$$ if we substitute $dt=\frac{d\theta}{\omega}$ then $$\frac{d^2\theta}{dt^2} = \frac{d}{\frac{d\theta}{\omega}}\biggl(\frac{d\theta}{\frac{d\theta}{\omega}}\biggl) = \frac{d(\omega)}{d\theta}\biggl({\frac{d\theta}{d\theta}\omega}\biggl) = \omega\frac{d\omega}{d\theta}$$ However, I have a feeling that this approach is mathematically incorrect. Hence I was wondering what other approaches are there, with mathematical rigor . Moreover, when changing variables of derivatives like this, be it first or second derivatives, are there any general rules that apply?","['derivatives', 'change-of-variable']"
2422856,When do $\sum_{n=1}^{\infty} \dfrac1{n^{1+a\sin(bn)}} $ and $\sum_{n=1}^{\infty} \dfrac1{n\ln^{1+a\sin(bn)}(n)} $ converge or diverge?,"It is well known that
$\sum_{n=1}^{\infty} \dfrac1{n^{1+c}}
$
and
$\sum_{n=1}^{\infty} \dfrac1{n\ln^{1+c}(n)}
$
converge for
$c > 0$
and
diverge for
$c \le 0$. This got me to wondering 
what would happen if
$c$ varied.
The obvious choice is
$c = a\sin(bn)$
where
$b$ is not a
multiple of
$\pi$. So that's my question: When conditions on
$a$ and $b$
make
$\sum_{n=1}^{\infty} \dfrac1{n^{1+a\sin(bn)}}
$
and
$\sum_{n=1}^{\infty} \dfrac1{n\ln^{1+a\sin(bn)}(n)}
$
converge or diverge? I don't have a clue.","['divergent-series', 'sequences-and-series']"
2422887,All $n$ such that $n+2$ is a cube and $n-2$ is a square. [duplicate],This question already has answers here : Find all integer solutions to $x^2+4=y^3$. [duplicate] (1 answer) Integers that satisfy $a^3= b^2 + 4$ (4 answers) Closed 6 years ago . I'm trying to find all integers $n$ such that $n+2$ is a cube and $n-2$ is a square. I already know that only 6 and 123 are solutions but not know how to show it. I tried to use the proof of 26 being the only one such that $n+1$ is a cube and $n-1$ is a square but I'm having troubles doing it as in $\mathbb{Z}[i]$ I don't know how to conclude that if $y^3=x^2+4=(x+2i)(x-2i)$ then both $x+2i$ and $x-2i$ are cubes (It is at least true?). Thank you very much.,"['number-theory', 'diophantine-equations', 'algebraic-number-theory']"
2422893,Importance of $|\mu(A)|<\infty$ in complex measure?,"In the book ' An Introductory Course in Functional Analysis ' by Bowers and Kalton, they give the definition of positive measure at page $209:$ Let $(X,\mathcal{A})$ be a measurable space, A set function $\mu:\mathcal{A} \rightarrow [0,\infty]$ is said to be countably additive if 
  $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$
  whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets. 
  A countably additive set function $\mu:\mathcal{A} \to [0,\infty]$ such that $\mu({\emptyset})=0$ is called a positive measure . The authos give definition of complex measure at page $213:$ Let $(X,\mathcal{A})$ be a measurable space, A countably additive set function $\mu:\mathcal{A} \rightarrow \mathbb{C}$ is called a complex measure . When we say $\mu$ is countably additive, we mean $$\mu\left( \bigcup_{j=1}^\infty A_j \right) = \sum_{j=1}^\infty \mu(A_j)$$
  whenever $(A_j)_{j=1}^\infty$ is a sequence of pairwise disjoint measurable sets in $\mathcal{A},$ where the series is absolutely convergent. At the same page, the authors mentioned the following: There is a significant difference between positive measures and complex measures. In the definition of complex measure, we require that a complex measure $\mu$ be finite; that is, $|\mu(A)|<\infty$ for all $a\in\mathcal{A}.$ This was not a requirement for a positive measure. Question: Why do we need to assume that $|\mu(A)| < \infty$ for complex measure while we do not need to assume it in positive measure? I strongly believe that purpose of inequality is not to ensure the series converges absolutely only, but it has other purpose.","['functional-analysis', 'real-analysis', 'measure-theory', 'definition']"
2422894,Prove that $\int \frac{x^m dx}{\ln x}=\ln(\ln x)+\frac{(m+1)\ln x}{1}+\frac{(m+1)^2 \ln ^2 x}{1 \times 2^2}+\cdots $,Prove that $$\int \frac{x^m dx}{\ln x}=\ln(\ln x)+\frac{(m+1)\ln x}{1}+\frac{(m+1)^2 \ln ^2 x}{1 \times 2^2}+\frac{(m+1)^3 \ln^3 x}{1 \times 2 \times 3^2}+\cdots \infty $$ My Try: I started with $$I=\int \frac{x^m}{\ln x}=\int \frac{x^{m+1} dx}{x \ln x}$$ Now by using parts taking $u=x^{m+1}$ and $v=\frac{1}{x \ln x}$ we get $$I=x^{m+1} \times \ln(\ln x)-(m+1)\int x^m \ln (\ln x)dx$$ can i have any clue to proceed,"['algebra-precalculus', 'indefinite-integrals', 'integration']"
2422899,Help visualising immersion of $\mathbb{R}P_3$ in $\mathbb{R^4}$,"Theoreticaly, there exist immersions of the real projective space $\mathbb{R}P_3$ in $\mathbb{R^4}$ . If an equation for one is given, eg $f:\mathbb{R}P_3 \rightarrow \mathbb{R^4}$ you can visualize it as the cross-sections of a hyperplane moving along the orthogonal dimension. You set the coordinates as $(x,y,z,t)$ , where $t$ is time, and imagine  the set of $(x,y,z)$ such that $(x,y,z,t)$ is in the image of $f$ at the given $t$ . By applying this procedure for the embedding of the real projective plane $\mathbb{R}P_2$ (as was asked here ), I've sketched the following example: For projective space, I'm having trouble getting either a visualization or an equation (for an immersion). I know if I could ""glue"" a $3$ -ball to the real projective plane then it would give $\mathbb{R}P_3$ . That would mean starting with a point that grows into a sphere instead of loop, but I can't apply the ""twisting"" without getting cone-like singularities. So that's not a good strategy. I know there's other ways to obtain $\mathbb{R}P_3$ , for example gluing two solid tori, but it's hard translating the correct ""gluing"" to immersion. Please provide an explicit function for immersion of $\mathbb{R}P_3$ in $\mathbb{R^4}$ , or guidance on how to correctly pick the cross-sections in $\mathbb{R}^3$ along time .","['algebraic-topology', 'general-topology', 'projective-space', 'geometry']"
2422906,Is the series convergent $x + x^{1+\frac{1}{2}}+x^{1+\frac{1}{2}+ \frac{1}{3}}+\cdots$,$$x + x^{1+\frac{1}{2}}+x^{1+\frac{1}{2}+ \frac{1}{3}}+\cdots$$ I know that $u_n = \lim_{n \to \infty}   x^ {\sum_{i=1}^n(1/i)}$ I used root test and ratio test but it isn't working.,"['sequences-and-series', 'convergence-divergence']"
2422954,Understanding application of trig identity $\cos^2\theta = \frac{1+\cos2\theta}{2}$ in integration,"I'm in Calc II and am (still) rusty on my trig after not having taken math for over a decade.  I'm trying to understand the application of the trig identity below. \begin{align}
\int^{\pi}_0 \cos^4 (2t) dt &= \int^{\pi}_0 (\cos^2 (2t))^2 dt\newline
\color{red}{\text{Using the property } \cos^2 \theta = \frac{1 + \cos 2\theta}{2}}\newline
&= \int^{\pi}_0 \left(\frac{1 + \color{blue}{\cos(4t)}}{2}\right)^2 dt
\end{align} The highlighted portion is the part I'm struggling with.  Because it's the $\cos^2(2t)$, rather than $\cos^2(t)$, is the idea that we let $\theta = 2t$, and so $2\theta = 4t$?  Would this apply to any input into the trig function?  For example, if it was $\cos^2(\frac t2 + 1)$, could I let $\theta = \frac t2 + 1$ and $2\theta = t + 2$, so that: $$\cos^2(\frac t2 + 1) = \frac{1 +\cos(t + 2)}{2}     ?$$ Confirmation that I'm understanding this correctly would be helpful.","['integration', 'trigonometry', 'calculus']"
2422955,Dimension of $U+V$,"Let $U$ and $V$ be the null spaces of $ A=\begin{bmatrix} 1&1&0&0\\0& 0&1&1 \end{bmatrix} $ and $ B=\begin{bmatrix} 1&2&3&2\\0&1&2&1 \end{bmatrix} $. Then what will be the dimension of $U+V.$ I calculated the null space of $U$ and $V$ as follows: 
\begin{align*}
U=\{x\in \mathbb{R}^4: Ax=0 \}\implies U=\text{span}\{(1,-1,0,0), \ (0,0,1,-1) \}\\
V=\{x\in \mathbb{R}^4: Bx=0 \}\implies V=\text{span}\{(1,-2,1,0), \ (0,-1,0,1) \}\\
\end{align*} 
For calculating the dimension of $U+V$, we have to see what is the dimension of $U\cap V$. For this I consider the matrix 
$$ \begin{bmatrix} 1&-1&0&0\\0&0&1&-1\\1&-2&1&0\\0&-1&0&1 \end{bmatrix}\sim 
 \begin{bmatrix} 1&-1&0&0\\0&-1&1&0\\0&0&1&-1\\0&0&0&0 \end{bmatrix} ,$$ which tells that the dimension of $U\cap V=1.$ So, $\dim(U+V)=3$. Now I want to ask is there any shorter method to do this? At least for the dimension of $U\cap V$.",['linear-algebra']
2423039,$\frac{1}{1^2}+\frac{1}{1^2+2^2}+\frac{1}{1^2+2^2+3^2}+...\to ?\;\;$ (Click here.),"$$\frac{1}{1^2}+\frac{1}{1^2+2^2}+\frac{1}{1^2+2^2+3^2}+\dots\to~?$$
I tried like below 
$$\frac{1}{1^2}+\frac{1}{1^2+2^2}+\frac{1}{1^2+2^2+3^2}+\dots=\\\sum_{n=1}^{\infty}\frac{1}{1^2+2^2+3^2+\dots+n^2}=\\\sum_{n=1}^{\infty}\frac{1}{\frac{n(n+1)(2n+1)}{6}}=\\\sum_{n=1}^{\infty}\frac{6}{n(n+1)(2n+1)}=\\
$$
Then I can use the fraction ,but $$\frac{1}{n(n+1)(2n+1)}=\frac{1}{n}+\frac{1}{n+1}+\frac{-4}{2n+1}$$ This is ugly to turn into telescopic series .
Can you help me to find :series converge to ? Thanks in advance .","['fractions', 'sequences-and-series', 'calculus']"
