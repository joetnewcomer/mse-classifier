question_id,title,body,tags
4650416,Is it possible to uniformly subdivide a sphere into arbitrarily small cells?,"I am not a geometer, so I might be misusing some terms. So let me try to be more explicit regarding what I mean. ""Subdivide a sphere into cells"" means to partition the set of all points on a sphere into finitely many non-overlapping (except at the boundary), contiguous regions (I'll call them cells). Note, that this doesn't exclude regions with arbitrarily complex boundaries (even infinite ones). ""Uniformly"" means that all cells in the subdivision should be the same shape and size. The shape doesn't need to be in any way regular or symmetrical, but you are not allowed to flip/mirror the shape, only translate and rotate it. ""Arbitrarily small"" means that for any given $\varepsilon > 0$ , you should be able to generate such a subdivision, so that each cell fits into a ball of radius $\varepsilon$ . Some examples: You can split a sphere into 8 equal ""triangle"" cells with angles 90-90-90 by halving it 3 times. But further subdividing those ""triangles"" is tricky (for example, you can't use edgewise subdivision to further refine them, because you will end up with ""triangles"" with different angles). You can split a sphere into an arbitrary amount of equal ""orange slices"", but they won't be ""arbitrarily small"" because they only get ""thinner"", not ""shorter"". Spherical polyhedra or spherical tilings satisfy the ""arbitrarily small"" requirement, but there are only finitely many uniform polyhedra, so this seems like a dead end. On the other hand, uniform spherical tilings assume that the cells themselves are also polyhedral, that they are vertex-transitive and so on. So it's not clear if relaxing these assumption allows for more tilings. My intuition is telling me, that the answer to the question in the title is ""No"" (such a subdivision is not possible), but I wasn't able to come up with a proof. Thoughts?","['spheres', 'tiling', 'geometry', 'spherical-geometry']"
4650476,"If $A\subset \Bbb Z$ with $|A| = n$, then there exists $t\in \Bbb Z\setminus p\Bbb Z$ such that $\phi(at/p) \le p^{-1/n}$ for all $a\in A$.","Let $A \subset \Bbb Z$ with $|A| = n$ , and $p$ be a prime. Consider $\phi:\Bbb R \to [0,\frac12]$ given by $\phi(x) = \min\{x-[x], 1 + [x] - x\}$ , where $[\cdot]$ is the floor function. There exists $t \in \Bbb Z\setminus p\Bbb Z$ such that $\phi(at/p) \le p^{-1/n}$ for all $a\in A$ . $\phi(x)$ is just the distance of $x$ from the nearest integer, $[x]$ or $[x] + 1$ . The problem gives pigeonhole principle vibes, but I'm yet to identify the pigeons and the holes. Regardless, here's my attempt. I'll consider $n=1$ first. Let $A = \{a\}$ and $p$ be a prime. If $p \mid a$ , then $a/p \in \Bbb Z$ , and any choice of $t\in \Bbb Z\setminus p\Bbb Z$ works. Assume $p \not\mid a$ . Then, we can express $a = pk + r$ for $1 \le r \le p -1$ and $k\in \Bbb Z$ . Consequently, $$\phi\left(\frac{at}{p} \right) = \phi\left(kt + \frac{rt}{p} \right) = \phi\left(\frac{rt}{p} \right)$$ We want $t\in \Bbb Z$ , $t\notin p\Bbb Z$ such that $\phi(rt/p) \le 1/p$ . Now, $r/p$ is in $S_p := \{1/p, 2/p, \ldots, 1 - 1/p\}$ and the fractional part of $rt/p$ for $t\in \Bbb Z\setminus p\Bbb Z$ also lies in $S_p$ . It shall suffice to show that for some $t\in \Bbb Z\setminus p\Bbb Z$ , we have $\{rt/p\} = 1/p$ (or $1 - 1/p$ .) This amounts to solving the congruence $rt \equiv 1 \bmod p$ for $t\in \Bbb Z\setminus p\Bbb Z$ . Since $r \ne 0$ , and $\Bbb Z/p\Bbb Z$ is a field ( $p$ is prime), every non-zero element (in particular, $r$ ) is invertible. So we are done. Note that $(\Bbb Z/p\Bbb Z)\setminus\{0\} \subset \Bbb Z\setminus p\Bbb Z$ when viewed under the inclusion $\Bbb Z/p\Bbb Z \hookrightarrow \Bbb Z$ . I haven't been able to generalize this to $n\in \Bbb N$ yet, and I'd appreciate any hints or ideas to help me progress. To use the pigeonhole principle, I suspect one should begin working with the cube $[0,1]^n$ and divide it into $p$ parts. Notation: $\{x\}$ denotes the fractional part of $x \in \Bbb R$ .","['pigeonhole-principle', 'abstract-algebra', 'combinatorics', 'discrete-mathematics']"
4650492,Chi-squared convergence to the Gaussian distribution,"I'm taking a Statistics course this semester and the professor mentioned that the chi-squared distribution $\chi_n^2$ satisfies that $$\sqrt{2\chi_n^2}-\sqrt{2n-1}$$ converges to a Gaussian distribution. I have been looking for a proof of this result, but haven't found one. We have defined the chi-squared distribution via its density function: $$
f(x)=\left\{\begin{array}{rl}
0 & \text{if }x\leq 0 \\
\frac{x^{n/2-1}e^{x/2}}{2^{n/2}\Gamma(n/2)} & \text{if }x>0
\end{array}
\right.
$$ and also shown that, if n is a positive integer, it follows the same distribution as the sum of $n$ independent squared gaussian variables.
I am familiar with the concept of generating functions and the central limit theorem. Is there a reference for a proof of this result? Preferrably, one that uses the definitions and results stated above. If not, could I be provided with one?","['statistics', 'normal-distribution', 'chi-squared', 'convergence-divergence', 'probability']"
4650523,What is the relation between Fubini-Study metric and the standard metric on hyperspheres?,"The relevant Wikipedia page mentions that "" the Fubini-Study metric is the metric induced on the quotient $\mathbb{CP}^n=S^{2n+1}/S^1$ , where $S^{2n+1}$ carries the so-called ""round metric"" endowed upon it by restriction of the standard Euclidean metric to the unit hypersphere "".
Furthermore, slightly above, they mention that the standard Hermitian metric on $\mathbb{C}^{n+1}$ , given by $ds^2=d\mathbf Z\otimes d\mathbf Z$ , is invariant under the diagonal action of $S^1$ , and therefore to derive the metric on $\mathbb{CP}^n$ we need only figure out the metric on $S^{2n+1}$ . I'm a bit confused by these statements.
It would seem like we're saying that to derive the metric on $\mathbb{CP}^n$ all we need to do is use the standard metric on $S^{2n+1}\subset \mathbb{R}^{2n+2}$ , and this metric will automatically be invariant under the suitable action of $S^1$ , and thus define a metric on the projective space. But if I were to follow this reasoning, I'd just take a complex ray $\psi\in\mathbb{CP}^n$ , represent it as some real normalised vector $f(\psi)\in S^{2n+1}\subset\mathbb{R}^{2n+2}$ , and then define the metric between two variations $\delta_i\psi$ as the standard Euclidean inner product between tangent vectors of $S^{2n+1}$ .
And because we're dealing with an inner product in a (subset of a) Euclidean space, shouldn't the metric just be $g(\partial_i,\partial_j)=\delta_{ij}$ on all points of the sphere?
Granted, I know that here $\partial_i$ would be tangent vectors attached to a specific point, and there's no global way to define these with a single chart.
Still, it would mean that the Fubini-Study metric is ""locally trivial"", which doesn't seem to be the case. I can see that part of the apparent complexity in the expressions for the Fubini-Study metric (as discussed e.g. here or here ) is probably due to expressing the metric in stereographic coordinates. But I can't quite figure out whether these expressions are also compatible with saying that the metric is indeed locally trivial (or if this is even true), and whether the metric is really just the standard metric on a hypersphere expressed in stereographic coordinates.","['information-geometry', 'projective-space', 'riemannian-geometry', 'differential-geometry']"
4650526,"How can the conditional expectation be the $L^2$ projection, when the $L^2$ projection depends on the choice of the norm?","one can define the conditional expectation of a random variable $X$ on $\mathbb{R}^n$ in an axiomatic way, without relying on any norm by being the $\mathcal{G}$ -measurable random variable $Y$ which fulfills $$\mathbb{E}[Y 1_A] = \mathbb{E}[X 1_A]$$ for any $A \in \mathcal{G}$ . However, one can also define it as the $L^2(\mathbb{R}^n, \mathcal{F}, \mathbb{P})$ projection onto all $\mathcal{G}$ -measurable random variables, where $\mathbb{P}$ is the distribution of $X$ . But this projection is defined by minimizing $$\mathbb{E}[\|X - Y\|^2]$$ and therefore depends on the choice of norm $\|\cdot\|$ on $\mathbb{R}^n$ . How can that be? Or to phrase it more generally, if I have the conditional expectation on some topological space, where I do not have such a natural norm as in $\mathbb{R}^n$ , with respect to which norm will it be the $L^2$ projection?","['projection', 'conditional-probability', 'conditional-expectation', 'probability-theory', 'probability']"
4650561,Investigating a property related to the biconnected property,"Let $(X, \tau) $ be a topological space and $A, B\subset X$ connected. We know that $A\cap B\neq \emptyset$ implies $A\cup B$ is connected. We also know that $A\cup B$ connected doesn't imply $A\cap B\neq \emptyset$ For an example $(\Bbb{R}, \tau_{\textrm{cofinite}})$ with $A=\Bbb{Q}$ and $B=\Bbb{R}\setminus \Bbb{Q}$ Now consider the following situation: A topological space $(X, \tau)$ have the property $\text{SG}$ if $\forall A, B\subset X$ (with $A,B$ more than $1$ point) connected be such that $A\cup B$ connected implies $A\cap B\neq\emptyset$ The property $\text{SG}$ is vacuously satisfied by any totally disconnected space. Is the classification of the $\text{SG}$ property already done? If yes then list some interesting property of the space satisfying $\text{SG}$ property.","['general-topology', 'soft-question', 'connectedness']"
4650574,How to compute this Conditional Expectation involving a Gaussian vector?,"Given a real number $a\ne 0$ and two independent $\mathbb R^d$ valued random variables $X$ and $Y$ with $Y\sim\mathcal N(\mu_Y,\Sigma_Y)$ ( $X$ 's dsitribution is unknown), I am trying to find a measurable function $f:\mathbb R^d \to \mathbb R^d$ which solves $$\min_{f \text{ measurable}} \mathbb E_{X,Y}\left[|f(X + aY) - Y|^2\right] \tag1$$ Well, if I denote $Z:=X + aY$ it is well known that the solution of $(1)$ is given by $f^* : z \mapsto \mathbb E[Y\mid Z = z]$ . Therefore my problem boils down to finding the expression of $f^*$ , which I have tried doing in the following way : Given that $X + aY = z $ , we have $\mathbb E[X\mid Z = z] + a\mathbb E[Y\mid Z = z] = z$ , i.e. $$f^*(z) = \frac{z-\mathbb E[X\mid Z = z]}{a} $$ But I don't know how to compute $\mathbb E[X\mid Z = z]$ . I have seen this and this post on stats.SE which seem to give a solution in case where both $X$ and $Y$ are Gaussian, but in my case $X$ is actually not Gaussian (it can be taken to be a Gaussian mixture if that helps). Is it possible to find a closed-form for $f^*$ in terms of the moments of $X$ and $Y$ ?","['conditional-expectation', 'probability-theory', 'normal-distribution']"
4650590,Let $g(y) := \lim\limits_{n\to \infty}x_{n}(y)$. Find $\int_{0}^{3}g(y)dy$.,"Let $f(x)$ be the function on $\mathbb{R}$ defined by $f(x)\!:=\sin(\pi x/2)$ . For $y$ in $\mathbb{R}$ , consider the sequence $\{x_{n}(y)\}_{n\geqslant0}$ defined by $$
x_{0}(y) := y\;\;\text{ and }\;\;x_{n+1}(y)=f(x_{n}(y))\;\text{ for all }\;n\geqslant1\\\text{and let }\,g(y):=\lim\limits_{n\to\infty}x_{n}(y)\,.$$ Find $\displaystyle\int_{0}^{3}g(y)\mathrm dy\,.$ My Attempt: Between $0$ and $3$ , $f$ has only two fixed points $0,1$ , so $g$ takes only value $0,1$ in $[0,3]$ . But how compute exactly $g$ in $[0,3]\,?$","['metric-spaces', 'fixed-point-theorems', 'real-analysis']"
4650647,How to tell if a polynomial is square,"Let $ q \in \mathbb{Z}[x_1,\dots,x_n] $ be a polynomial. Is there a good way to determine if $$
q=p^2
$$ for some polynomial $ p $ ? The first thing that comes to mind is that if $ q=p^2 $ then $$
\frac{\partial q}{\partial x_i}=2p \frac{\partial p}{\partial x_i}
$$ So $ p $ should be a factor of $ GCD(\frac{\partial q}{\partial x_1}, \dots \frac{\partial q}{\partial x_n}) $ . I don't know if this helps but in the example I'm working with $ q $ is a homogeneous degree $ 4 $ polynomial in $ 6 $ variables with integer coefficients (all the integers are pretty small, $ \leq 22 $ ).","['algebraic-geometry', 'factoring', 'polynomials']"
4650684,Find the exact value of $\int_{0}^{2}x[\frac{1}{x}]dx$.,"Find the exact value of $\int_{0}^{2}x\left[\frac{1}{x}\right]dx$ . Let $[x]$ denote $\lceil{x-\frac{1}{2}}\rceil$ . Using Desmos , I got $2.46736022133$ and WolframAlpha does not give me a solution. My intuition tells me that it might be possible to find an exact value using Trapezoidal Reimann Sums but I am not really sure how to go about doing it. After my attempt, I got stuck but I was at a point where I could plug it into WolframAlpha and it gave me $\frac{\pi^2}{4}$ . Why did it come out so nicely? My attempt: Where $A_n$ denotes the area of the $nth$ trapezoid from the right: $$A=\frac{h}{2}(a+b)$$ $$A_n=\frac{\frac{2}{2n-1}-\frac{2}{2n+1}}{2}(\frac{2n}{2n-1}+\frac{2n}{2n+1})$$ $$A_n=\frac{\frac{4n+2}{4n^{2}-1}-\frac{4n-2}{4n^{2}-1}}{2}\left(\frac{4n^{2}+2n}{4n^{2}-1}+\frac{4n^{2}-2n}{4n^{2}-1}\right)$$ $$A_n=\frac{2}{4n^{2}-1}\left(\frac{8n^{2}}{4n^{2}-1}\right)$$ $$A_n=\frac{16n^{2}}{\left(4n^{2}-1\right)^{2}}$$ Then: $$\int_{0}^{2}x\left[\frac{1}{x}\right]dx=\sum_{n=1}^{\infty}A_n=\sum_{n=1}^{\infty}\frac{16n^{2}}{\left(4n^{2}-1\right)^{2}}$$ I do not know how to solve this infinite summation so I plugged it into WolframAlpha and it gave me $\frac{\pi^2}{4}$ . How did it get to this conclusion? Is there a more efficient way to solve this?","['definite-integrals', 'real-analysis']"
4650753,Continuity of a multivariable function: doubts on how to reason,"I have some doubts concerning the continuity of a function in many variables. I will present a specific example, which I worked out (hopefully in a good way), and I will then ask you some questions because I'm really struggling with the ""deep"" theoretical sense of these procedures. Say I have the function $$f(x, y, z) = \begin{cases} (x + 2y + 3z)\ln(x^2 + 2y^2 + 3z^2) & (x, y, z) \neq (0, 0, 0) \\\\\\ C & (x, y, z) = (0, 0, 0)\end{cases}$$ It's asked if/when the function is continuous at the origin. First way I thought I could restrict to a particular path, like $x = y$ and $x = z$ , reducing to a one dimensional problem: $$f(x) = \begin{cases} 6x\ln(6x^2) & x \neq 0 \\\\ C & x = 0 \end{cases}$$ Here I can clearly study the limits when $x\to 0^+$ and $x\to 0^-$ and conclude that the function is continuous at the origin iff $C = 0$ . Alternatively, I can take a sequence $x_n = 1/n$ and operate that way. *Question 1: how can I be sure about this? I mean, how can I be sure that choosing some other path I wouldn't obtain something strange? This is a more general question actually, meant in general when applying the method of restriction to some path. Second way Our professor taught us the ""distance method"", that is: a function $f(x)$ is continuous at $x_0$ iff there exist a function $h(d)$ , a function of a distance $d$ (distance meant as in the Euclidean way, I guess) such that $$|f(x) - f(x_0)| \leq h(d)$$ with $h(d) \to 0$ for $d \to 0$ . This opened the abysmal world of majorizations and estimations, and I actually really need clarification from someone who got this well, in order to not sink in this oblivion. For example, using this, here is what I did. Please, read until the end (question included). $$|f(x, y, z) - \underbrace{f(0, 0, 0)}_{\text{supposed zero}}| \leq |x + 2y + 3z||\ln(x^2 + 2y^2 + 3z^2)|$$ Now I did: $|x + 2y + 3z| \leq |3x + 3y + 3z|$ and the same in the logaritm $$\leq  |3x + 3y + 3z||\ln(3x^2 + 3y^2 + 3z^2)|$$ At this point I did: $|3x + 3y + 3z| = 3(|x+y+z|) \leq 3(|x| + |y| + |z|)$ and also after that: $3(|x| + |y| + |z|) \leq 3(x^2 + y^2 + z^2)$ This allowed me to call a distance function $h(d^2) = x^2 + y^2 + z^2$ where $d^2$ would be the Euclidean distance squared ( $d^2 = x^2+y^2+z^2)$ , so in the end: $$\leq 3h(d)|\ln(3h(d))| \to 0 \qquad \text{as}\ d\to 0$$ So apparently I have managed to find such a function that, when shrunk, compress the function into it's value at the origin, which is zero. Hence again $C$ must be zero. And here I am stuck: How to really majorize? I mean things like $x < x^2$ are true when $|x| > 1$ where as the counterpart $x > x^2$ are true when $|x| < 1$ . Since I'm analyzing limits to zero, should I beware of such conditions from the beginning , or can I just think about finding a big ball $B$ , hence majorizing as if $(x, y, z) >> 0$ (large enought) and in the end shrinking letting $(x, y, z) \to (0, 0, 0)$ ? Isn't it always possible to compute such a majorization, even if a function is not continuous? How can I be sure with this method? Sorry for this poem, but THANK YOU SO MUCH to whoever will reply to this.","['real-analysis', 'continuity', 'multivariable-calculus', 'solution-verification', 'majorization']"
4650779,"In topology, what is meant by ""a disjoint union of sets""?","The following quoted material is from The Geometry of Celestial Mechanics by Hansjörg Geiges. Remark 1.4: The transformation from polar to Cartesian coordinates
is described by the smooth map \begin{align*}
p:\mathbb{R}^{+}\times\mathbb{R} & \to\mathbb{R}^{2}\backslash\left\{ \mathbf{0}\right\} \\
\left(r,\theta\right) & \mapsto\left(r\cos\theta,r\sin\theta\right).
\end{align*} The Jacobian determinant of this map is \begin{align*}
\det J_{p,\left(r,\theta\right)} & =\begin{vmatrix}\cos\theta & -r\sin\theta\\
\sin\theta & r\cos\theta
\end{vmatrix}=r\ne0,
\end{align*} so $p$ is a local diffeomorphism by the inverse function theorem.
Moreover, $p$ is covering map , which means the following. For
any point in $\mathbb{R}^{2}\backslash\left\{ \mathbf{0}\right\} $ one can find an open, path-connected neighborhood $U$ whose preimage $p^{-1}\left(U\right)$ is a non-empty, disjoint union of sets $U_{\lambda},\lambda\in\Lambda,$ such
that $p\vert_{U_{\lambda}}:U_{\lambda}\to U$ is a homeomorphism for
each $\lambda$ in the relevant index set $\Lambda.$ (You are asked
to verify this property in Exercise 1.3) Exercise 1.3: Verify the map $p$ in Remark 1.4 is a covering map
in the sense described there. As an index set one can take $\Lambda=\mathbb{Z}.$ How does one have to chose the neighborhood $U$ of a given point
in $\mathbb{R}^{2}\backslash\left\{ \mathbf{0}\right\} ?$ I have multiple questions about this, but I will only pose one in this post. What is meant by ""a non-empty, disjoint union of sets""? The non-empty part is easy.  I take disjoint to mean that no point in $p^{-1}\left(U\right)$ is in more than one $U_\lambda.$ The path-connected property of $U$ says that there is a continuous path connecting every pair of points in $U.$ If $\mathbf{p}_1$ lies in $p\left(U_1\right)$ and $\mathbf{p}_2$ lies in $p\left(U_2\right)$ ,  apparently the preimage of a path connecting $\mathbf{p}_1$ and $\mathbf{p}_2$ is a path in $p^{-1}\left(U\right)$ connecting a point of $U_1$ and a point of $U_2,$ even though they are disjoint. Intuitively, that seems contrary to the assertion that $U_1$ and $U_2$ are disjoint.","['continuity', 'elementary-set-theory', 'general-topology', 'differential-topology', 'transformation']"
4650794,$Au_{xx} + Bu = 0 $ then $u = 0$,"Let $A,B > 0$ and $0 < a < b < \infty$ . Consider $u \in C^{1}([0,b])$ , $u = 0$ in $[0,a)$ and $$
Au_{xx} + Bu = 0 \ \  \text{in} \ \ (a,b)
$$ with $u(a) = u(b) = 0$ . Then $u = 0$ in $[a,b)$ . My ideia: How $u \in C^{1}([a,b])$ and $u= 0$ in $[0,a)$ , then $u(0) = u (a) = u_{x}(a) = 0$ . By system, we have $$
u(x) = c_{1}\cos\bigg(\sqrt{\frac{A}{B}}x\bigg) + c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg), \ \ \text{in} \ \ (a,b)
$$ But, $u(0) = u (a) = u_{x}(a) = 0$ , then $$
u(x) =  c_{2}\sin\bigg(\sqrt{\frac{A}{B}}x\bigg)
$$ with $$
c_{2}\cos\bigg(\sqrt{\frac{A}{B}}a\bigg) = c_{2}\sin\bigg(\sqrt{\frac{A}{B}}a\bigg) = 0
$$ Soon, $\frac{A}{B}a = \frac{\pi}{4} + \pi n$ or $c_{2} = 0$ . And now? I can't continue.",['ordinary-differential-equations']
4650813,Leading order asymptotic behaviour of the integral $\int^1_0 \cos(xt^3)\tan(t)dt$,"I'm trying to get the leading order asymptotic behaviour of the integral: $$\int^1_0 \cos(xt^3)\tan(t)dt$$ I'm trying to use the Generalised Fourier Integrals and the Stationary Phase Method, but I can't understand how to start this. THIS IS WHAT I HAVE TRIED: We want to get: $$\Re \int^1_0e^{ixt^3}\tan(t)dt$$ $\phi(t)=t^3$ has a stationary point at $t=0$ so the main contribution is around that point and due to the small angle approximation $\tan(t) \approx t$ . $$\Re\int^1_0e^{ixt^3}tdt$$ I tried some substitutions but I can't find anything useful after that.","['integration', 'laplace-method', 'asymptotics']"
4650848,"Prove that if $2n$ points are colored red or blue, then we can always connect the red points to the blue points with non-intersecting line segments.","Take any set of 2n points in the plane with no three collinear, and then arbitrarily
color each point red or blue. Prove that it is always possible to pair up the red
points with the blue points by drawing line segments connecting them so that
no two of the line segments intersect. This problem is provided in Richard A. Brauldi's book on Introductory Combinatorics. I believe it is implied that the number of red points is equal to the number of blue points. I tried to reason as follows: it seems like it's always possible to divide the plane into two regions, such that each region contains an equal number of red and blue points. Further, subdivide those regions into subregions with equal numbers of red and blue points; and continue the process until all the regions contain exactly one red and one blue point. Then we can simply connect the two points of every region with a line; and since all pairs lie in distinct regions, it seems obvious that no lines will intersect. However, I don't know how exactly to prove this. That is, how do I gaurantee that it is in fact possible to divide any region of red and blue points into subregions of equal numbers of red and blue points? Or how would the problem be approached by a more professional. Thank you in advance.","['euclidean-geometry', 'combinatorics', 'geometry', 'combinatorial-proofs']"
4650857,"How deeply nested can absolute values be, without being denestable?","Construct a sequence of function spaces $F_k$ as follows: $F_0$ is the space of linear forms $\mathbb R^n\to\mathbb R$ . $F_{k+1}$ is the span of functions in $F_k$ and their absolute values. Here are some examples: $$F_0(\mathbb R^2)\ni\quad2x-y$$ $$F_1(\mathbb R^2)\ni\quad2|x|-y+|x+y|$$ $$F_2(\mathbb R^2)\ni\quad x-\big||2x|-y\big|$$ $$F_3(\mathbb R^2)\ni\quad\Big|x-\big||2x|-y\big|\Big|+|x+y|$$ $$\big||x|+y\big|\quad\not\in F_1(\mathbb R^2)$$ $$F_2(\mathbb R^2)\ni\quad\big||x|+|y|\big|=|x|+|y|\quad\in F_1(\mathbb R^2)$$ $$F_2(\mathbb R^2)\ni\quad\big||x|-|y|\big|=|x+y|+|x-y|-|x|-|y|\quad\in F_1(\mathbb R^2)$$ $$\max(x,y,z)\quad\not\in F_1(\mathbb R^3)$$ $$\max(x,y,z)\quad\in F_2(\mathbb R^3)$$ $$\max(x,y,z,w)\quad\in F_2(\mathbb R^4)$$ It's clear that $F_0\subseteq F_1\subseteq F_2\subseteq\cdots\subseteq F_k\subseteq F_{k+1}\subseteq\cdots\subseteq G$ , where $G$ is the space of continuous piecewise-linear functions with polyhedral cone ""pieces"". Is there some $k\in\mathbb N$ such that $F_{k+1}=F_k$ ? Of course the answer depends on $n$ . For $n\leq2$ at least, it is just $k=n$ , as a result of $G=F_n$ . A function $f\in G(\mathbb R^0)$ is trivial: $f()=0$ . A function $f\in G(\mathbb R^1)$ is $f(x)=\tfrac12f(1)(|x|+x)+\tfrac12f(-1)(|x|-x)$ . Hence $f\in F_1(\mathbb R^1)$ . A function $f\in G(\mathbb R^2)$ is sharp (i.e. non-differentiable) along some rays meeting at $(0,0)$ , and is smooth on the sectors between those rays. If all sectors are half-planes or larger, then $f\in F_1(\mathbb R^2)$ . If any sector is less than a half-plane, then there's some function $g\in F_2(\mathbb R^2)$ with three sharp rays that agrees with $f$ around the two sharp rays bounding that sector, so that $f-g$ has one less sharp ray than $f$ ; the original two rays are cancelled and a third ray is added. (The prototypical functions for this are $g(x,y)=\max(|x|,y)=\tfrac12(|x|+y+\big||x|-y\big|)$ and $g(x,y)=\min(|x|,y)=\tfrac12(|x|+y-\big||x|-y\big|)$ ; apply linear transformations to these.) Inducting on the number of sharp rays, we find $f=\sum g\in F_2(\mathbb R^2)$ . We also have $|x|=\max(x,-x)$ , and $-\max(x,y)=\min(-x,-y)$ , so anything in $F_k$ can be expressed in terms of $\max$ and $\min$ . Furthermore, addition distributes over $\max$ and $\min$ , and $\max$ and $\min$ distribute over each other: $$x+\max(y,z)=\max(x+y,x+z)$$ $$\max(x,\min(y,z))=\min(\max(x,y),\max(x,z))$$ It follows that anything in $F_k$ can be expressed in the form $$\min(\max(f_{11},f_{12},\cdots),\max(f_{21},f_{22},\cdots),\max(f_{31},f_{32},\cdots),\cdots)$$ for some linear functions $f_{ij}\in F_0$ . If we can find some universal bound $k$ on the nesting depth of $\max$ , i.e. if $\max\in F_k(\mathbb R^n)$ for all $n$ , then the same bound applies to $\min$ , and any function has a nesting depth at most $2k$ .","['polyhedra', 'absolute-value', 'maxima-minima', 'functions', 'functional-analysis']"
4650859,Brownian motion (Wiener process) as a random function,"In many articles concerned with functional data analysis, it is considered a regressor $X$ which is a random variable valued in some infinite dimensional set $F$ equipped with (semi/pseudo) metric $d$ . The assumption that $F$ is infinite-dimensional and equipped with (semi/pseudo) metric $d$ makes sense if I think that it is some class of functions (e.g., $C[0,1]$ or $L^2[0,1]$ ). Question If $X:=W=\{W_t, t\in [0,1]\}$ is the standard Wiener process on $[0,1]$ , what the set $F$ should exactly be? Comments Following Bosq (2000) p.15-16 , let $W=\{W_t, t\in T\}$ be a family of random variables defined on $(\Omega,\mathcal A,P)$ with values in a measurable space $(E,\mathcal B)$ . In order to interpret $W$ as a random function, consider the space $E^T$ of mappings from $T$ to $E$ equipped with the $\sigma$ -algebra $\mathcal S=\sigma(\pi_t,t\in T)$ where $\pi_t:E^T\to E$ is defined by $\pi_t(x)=x_t $ for $x\in E^T$ . Because $W_t^{-1}(B)=W_t^{-1}(\pi_t^{-1}(B))$ for all $B\in\mathcal B, t\in T,$ he inferred that $W$ is $\mathcal A - \mathcal S$ measurable. In this sense, $W$ can be seem as a random function.
Back to my case, it seems that the set $E^T$ of mappings is directly related to the set $F$ thaI mentioned in the begining. (Is $F=L^2[0,1]$ ?) I'm not much experienced in this field. Can someone give me directions?","['stochastic-processes', 'measure-theory', 'functional-analysis', 'statistics']"
4650886,"How do you solve equations where $x$ is in both the base and exponent (eg, $x^x=4x^2$) with an exact solution?","The equation in question is $x^x=4x^2$ , which can be rearranged to $x^{x-2}=4$ . Using other methods, I know the two real solutions are approximately $3.193826160$ and $0.4166376811$ , but I would quite like to know how to calculate the exact forms, since using limits and spamming the $=$ button on a calculator until the number stops changing isn't particularly efficient, and may not even work for other similar equations. I know that the Lambert $W$ function is likely involved, but I don't know how to rearrange equations into the necessary form to use it and answers to other questions on this site haven't been able to help that lack of understanding, unfortunately.","['algebra-precalculus', 'roots']"
4650900,Is it possible to explicitly determine max and min in this case?,"On the set $$A = \{(x, y, z) \in \mathbb{R}^3; xy + xz + yz \leq 1\}$$ does the following function admit global/local max/min? $$f(x, y, z) = xyz$$ attempts So, firt of all I proved we cannot appeal to Weierstrass Theorem, for the set $A$ is closed but not bounded. It's closed because set of the form $[a, +\infty)$ (in one dimension) are closed.
It's not bounded because, say, restricting on $x = y$ and $x = z$ , I obtain the restricted condition $- z^2 \leq 1$ , hence $z$ can be large enough such that no ball $B_r(0)$ can contain $A$ . (I proved it more rigorously; I omit this part because it's not important for the question). Now, for some reason I understood that $xy + xz + yz \leq 1$ represents a so called elliptic hyperboloid (when we take the $=$ sign, thanks to GeoGebra). I really do not know how to reduce that equation into the canonical form for an elliptic hyperboloid which shall be $$\frac{x^2}{a^2} + \frac{y^2}{b^2} - \frac{z^2}{c^2} = - 1$$ (some help about would be appreciated, even just to understand the method, but again: not really fundamental now). Since Weierstrass does not hold, there is no certainty about the existence of global max/min over $A$ . Question: is there a way to find those max/min, or to completely exclude their existence (or to prove it)? I thought I could study the behaviour of $f$ in the internal points of $A$ (with the gradient of $f$ ) and the points on the boundary. Though the gradient returns $$\nabla f = (0, 0, 0) \rightarrow \begin{cases} yz = 0 \\ xz = 0 \\ xy = 0 \end{cases}$$ Which are satisfied for points of the form $(0, 0, z)$ or $(0, y, 0)$ or $(x, 0, 0)$ . In all those points, the function returns zero as a value. I am stuck however on the boundary $\partial A$ , because I am not able to write down the equation for $\partial A$ .
If I could, I would study the restriction of $f(x, y, z)$ on the boundary, and maybe manage a bit the thing. So is there a more efficient way? How to write down $\partial A$ ?","['optimization', 'multivariable-calculus', 'real-analysis']"
4650939,Show that the area of two triangles formed by lines in parallelogram are equal,"Consider the parallelogram $ABCD$ . Let points $E$ and $F$ lie on $AB$ and $AD$ , respectively. The line $EF$ meets the extension of $DC$ in the point $H$ and the extension of $BC$ in the point $G$ . Show that the $|\Delta AGH| = |\Delta EFC|$ (see figure below). My first approach was to label the heights of the respective triangles $h_1$ ( $\Delta AGH$ ) and $h_2$ ( $\Delta EFC$ ). If the areas have to be equal, we have: $$
\begin{equation}
   \frac{|\Delta AGH|}{|\Delta EFC|} = 1 \iff \frac{h_1(|HF| + |EF| + |EG|)}{h_2|EF|} = 1 \iff \frac{h_1}{h_2}(\frac{|HF| + |EG|}{|EF|} + 1) = 1
\end{equation}
$$ But now, I realized that (because of A-A-A for similar triangles), $\Delta HFD\sim\Delta AFE\sim\Delta EGB$ , which means that we can rewrite the yet to be proven equation equivalently as: $$
\begin{equation}
\frac{h_1}{h_2}(\frac{DF}{AF} +\frac{BE}{AE} + 1) = 1
\end{equation}$$ But from here, I couldn't really find any way forward... any help would be deeply appreciated.","['euclidean-geometry', 'area', 'geometry', 'plane-geometry']"
4651012,Show that $\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right)$ exists,"This question comes from Rational Points on Elliptic Curves (Silverman & Tate) exercise 3.3(a). $3.3$ . Let $C$ be a rational cubic curve given by the usual Weierstrass equation.
Prove that for any rational point $P \in C(\mathbb{Q})$ , the limit $$\hat{h}(P)=\lim _{n \rightarrow \infty} \frac{1}{4^n} h\left(2^n P\right)$$ exists. The quantity $\hat{h}(P)$ is called the canonical height of $P$ . Note that the height of $x$ is $H(x)=H\left(\frac{m}{n}\right)=\max \{|m|,|n|\}$ and $h(P)=\log H(P)$ It gave a hint to suggest showing the sequence is Cauchy, so here is my attempt: We wish to show that, given $\varepsilon>0$ there exists $N$ such that if $m, n > N$ then $$\left|4^{-n}h(2^nP) - 4^{-m}h(2^mP)\right|<\varepsilon.$$ Write $P = x/y$ so we have $$\left|4^{-n}h(2^n\frac x y) - 4^{-m}h(2^m\frac x y)\right|.$$ Now, I'm not sure how to proceed but my best idea is that because $e^{\varepsilon}$ can be made arbitrarily small just like $\varepsilon$ itself, we can raise everything to the power of $e$ and get rid of small $h$ . Is this the right direction?","['cauchy-sequences', 'elliptic-curves', 'number-theory', 'limits', 'rational-numbers']"
4651034,"How can I find the box dimension of $\{ \frac{1}{n^m}\mid n=1,2,3,\ldots \}$ for a fixed $m$?","I recently learned of the concept of box dimension (also called the Minkowski-Bouligand dimension ) as a way of measuring the dimension of a set (with use in describing the non-integer dimensions of some fractals) - it's sometimes compared to the Hausdorff dimension and in some cases can be simpler to calculate. The box-dimension of a set $S$ contained in $d$ -dimensional Euclidean space considers the occupancy of a grid of equal size boxes of lateral dimension $\epsilon$ . In particular, calling the number of boxes containing at least one element of $S$ $N(\epsilon)$ , we have $$B(S) = \lim_{\epsilon \to0} \frac{\log(N(\epsilon))}{\log(\frac{1}{\epsilon})}$$ For example, the box dimension of the set $S={1, 1/2, 1/3,..}$ on the real line (i.e. $\frac{1}{n}$ for integers $n>0$ ) is $1/2$ , as noted in this question . What is the box dimension of $\{ \frac{1}{n^m}\mid n=1,2,3,\ldots \}$ ? Here are my thoughts. I believe that in general, $\{ \frac{1}{n^m}\mid n=1,2,3,\ldots \}$ has box dimension $\frac{1}{m+1}$ for positive $m$ . For my question asking about the infinite union of all of these sets, please see this question .","['measure-theory', 'dimension-theory-analysis', 'fractals']"
4651047,How do I integrate $\int_0^1\arctan(x)\log(\frac{1-x}{1+x})\mathrm{d}x$?,"I've recently come across an interesting integral, which is of the form: $$\int_0^1\arctan(x)\log\left(\frac{1-x}{1+x}\right)\mathrm{d}x$$ To start, I expanded the arctangent into its series expansion, then utilized the Weierstraß substitution in order to remove the fractional term from the logarithm: $$t = \frac{1-x}{1+x}$$ Finally, I'm left with this integral: $$2 \sum_{k \geq 0} \frac{(-1)^k}{2k+1} \int_0^1 \frac{(1-t)^{2k+1}}{(1+t)^{2k+3}} \log(t)\mathrm{d}t$$ Which looks an awful lot like the beta function, namely: $$B(x, y) = (1-a)^y \int_0^1 \frac{(1-t)^{x-1} t^{y-1}}{(1-at)^{x+y}} \mathrm{d}t, \quad a \leq 1$$ For the following values, the integrals are nearly identical: $$a=-1,$$ $$x=2k+2,$$ $$y=1$$ However, this is the bit where I fail to make progress. I see that the integrals are clearly just off by that logarithm, but I cannot find a relation between them in order to progress with this integral. I've tried differentiating with respect to the parameter $y$ in order to bring in that logarithm, but that obviously doesn't do much - as the parameter also lies in the denominator and causes unwanted trouble. I've also tried constructing integrals which are similar to this one, but only have the parameter $y$ in the numerator; however, I haven't been able to make much progress doing that either. These integrals end up looking nothing like the beta function.","['integration', 'calculus', 'definite-integrals', 'logarithms']"
4651050,What does it mean for a 1-form to be orthogonal to a 2-form?,"In Baez & Munian's book Gauge Fields, Knots, and Gravity , when introducing the Hodge star operator, they say At any point $p$ in a 3-dimensional Riemannian manifold $M$ , the Hodge star operator maps a 1-form $\nu$ , which we draw as a little arrow, into a 2-form $\omega \wedge \mu$ that corresponds to an area element that is orthogonal to $\nu$ . Conversely, it maps $\omega \wedge \mu$ to $\nu$ . In general, in $n$ dimensions the Hodge star operator maps $p$ -forms to $(n-p)$ -forms in a very similar way, taking each ' $p$ -dimensional area element' to an orthogonal ' $(n-p)$ -dimensional area element'. I am unclear on what they mean by orthogonality here, I assume they mean the inner product $\langle \nu, \omega \wedge \mu \rangle = 0$ . Earlier they defined what it means to take the inner product of two differential forms of the same degree, but in the above passage they are different degrees. What does it mean for a 1-form to be orthogonal to a 2-form?","['hodge-theory', 'differential-forms', 'riemannian-geometry', 'differential-geometry']"
4651052,"Is there a sense in which $(\mathbb{R},+)$ is ""free""?","Given a set $S$ , the free group on $S$ consists of finite strings of elements in $S$ . They can be visualized as paths on the integer grid $\mathbb{Z}^S$ starting at the origin, with the group operation given by path concatenation. There is no set for which $\mathbb{R}$ is the corresponding free group- however it is tempting to view $\mathbb{R}$ as a free group on a single ""infinitesimal generator"". More generally given a set $S$ we could consider the group where elements are pairs $(T, f)$ with $T \in \mathbb{R}$ and $f: [0,T] \to \mathbb{R}^S$ a continuous function and $f(0)=0$ . (The value of $T$ mirrors the length of a string in the free group. ) the product of $(T_1,f_1)$ with $(T_2,f_2)$ is a map $[0,T_1+T_2]\to\mathbb{R}^S$ given by $f_1(t)$ for $t \in [0,T_1)$ and $f_1(T_1)+f_2(t-T_1)$ for $t\in[T_1, T_1 + T_2]$ the identity element is $(0,0\mapsto 0)$ , In a commutative version of this definition, as in the free commutative group, two paths would be equal if they have the same start and end point. Does this construction have a name? I see there is infinitesimal generators in the context of Lie groups, but that does not seem to be related... Edit: the paths should also be equivalent up to reparameterization.","['reference-request', 'combinatorial-group-theory', 'free-groups', 'group-theory', 'soft-question']"
4651053,Counting paths to winning distribution,"In a tournament, there are $2n$ teams, $n \in \mathbb{N}$ . Each round, the teams pair up. This means that after each round, $n$ teams win and $n$ teams lose. If we are given a scorecard containing each team’s number of wins after $r$ rounds, we would like to determine all possible ways that this win outcome could have occurred. In essence, we would like to find all possible ways of representing a vector $(w_0, w_1, … w_{2n - 1}), w_i \in \mathbb{N_0}$ as a sum of vectors of the form $(r_0, r_1, … r_{2n-1}), r \in \{0, 1\}$ where $\sum_{i=0}^{2n-1} r_i = n$ . How might I go about achieving this? We tried representing each component of the vector as a node on a graph having a certain value. We were also interested in barycentric coordinates or other coordinate systems. I also tried more of a pure linear algebra approach and represented it as a system of linear diophantine equations, yet we still haven't been able to solve this one.","['graph-theory', 'vector-spaces', 'combinatorics', 'linear-algebra', 'probability']"
4651182,Structural description for the order of $\mathrm{GL}_n(\mathbb F_q)$,"This question is inspired by the cute answer to Order of general- and special linear groups over finite fields . The formula $$
|\mathrm{GL}_n(\mathbb F_q)|=q^{\frac{n(n-1)}2}(q-1)(q^2-1)\cdots(q^n-1).
$$ is obtained in that answer by simply but cleverly counting the number of  linearly independent $n$ -tuples of vectors in $\mathbb F_q^n$ . On the other hand, we know a lot about the structure of the group $\mathrm{GL}_n(\mathbb F_q)$ : we can choose (in many ways) a maximal torus ; let us take the one consisting of all invertible diagonal matrices, which is a subgroup of order $(q-1)^n$ . We can next locate the unipotent radical of the corresponding Borel subgroup which in our case is the subgroup of all upper triangular matrices with $1$ s along the main diagonal, thus has order $q^{\frac{n(n-1)}2}$ . This accounts for $q^{\frac{n(n-1)}2}(q-1)^n$ elements. How to account for the remaining factors $\frac{q^2-1}{q-1}$ , $\frac{q^3-1}{q-1}$ , ..., $\frac{q^n-1}{q-1}$ ? Do they also correspond to some subgroups that can be named, or maybe some explicitly describable conjugacy classes?","['finite-fields', 'combinatorics', 'algebraic-groups']"
4651193,"How do I interpret the intersection of a variety with a ""non-closed hyperplane?""","I am trying to understand Vakil's statement and proof of Bertini's theorem, which has been updated since many of the questions related to it were posted on this website (for what it's worth, I'm not entirely sure how to interpret the original statement either). I guess this question leads into the more general question of ""how do incidence varieties work?"" For context, this is the statement of Bertini's theorem in Vakil (Theorem 13.4.2, December 31, 2022 Draft): Suppose $X$ is a smooth subvariety of $\mathbb{P}_k^n$ of (pure) dimension $d$ . Then there is a nonempty (= dense) open subset of dual projective space ${\mathbb{P}_k^n}^\vee$ such that for every point $p = [H] \in U$ , $H$ doesn't contain any component of $X$ , and the scheme $H \cap X$ is smooth over $\kappa(p)$ of (pure) dimension $d - 1$ . (1) What is $H \cap X$ for a point $p = [H] \in {\mathbb{P}_k^n}^\vee$ , and why is it a scheme over $\kappa(p)$ (in particular, how do we make sense of this if $k$ is not algebraically closed and $p$ is not a closed point)? How do we make rigorous (in a scheme-theoretic sense) the notion of "" $H$ containing a component of $X$ ? The best I could do in interpreting this is the follows: Let $I \subseteq X \times {\mathbb{P}_k^n}^\vee$ be the ""incidence variety"" (introduced in a paragraph above the statement of the theorem) cut out by the equation $a_0x_0 + \cdots + a_nx_n = 0$ , where $x_0, \ldots, x_n$ are homogeneous coordinates for $\mathbb{P}_k^n$ and $a_0, \ldots, a_n$ are homogeneous coordinates for ${\mathbb{P}_k^n}^\vee$ . Let $p = [H] \in {\mathbb{P}_k^n}^\vee$ , and take the fiber of $p$ in $I$ , then project it down to $X$ . But this is merely a set-theoretic description and fails to give it a scheme structure; furthermore, even if I used the scheme-theoretic closure, it fails to interpret it as a scheme over $\kappa(p)$ . (2) How do incidence varieties ""work?"" They are typically specified informally as some collection of pairs of points in $\mathbb{P}_k^n \times {\mathbb{P}_k^n}^\vee$ , like $\{(p, H): \text{some property to do with $p$ and $H$}\}$ , but how do we interpret this rigorously in the context of scheme theory, where the points of a product are not generally the product of the points? An answer that does not involve the language of representable functors is preferable since I am unfamiliar with it, but welcome if unavoidable. The incidence variety that Vakil describes is essentially the relation $\{(p, H): \text{$H$ contains $p$}\} \subseteq \mathbb{P}_k^n \times {\mathbb{P}_k^n}^\vee$ , which he describes as the variety cut out by the equation $a_0x_0 + \cdots + a_nx_n = 0$ . This definition makes intuitive sense for $k$ -valued points, so the generalization feels like a natural extension of that basic intuition. But then, what exactly is a hyperplane $H$ associated with a non-closed point $p \in {\mathbb{P}_k^n}^\vee$ ? Can we interpret it as a subset of $\mathbb{P}_k^n$ ? What if $k$ wasn't a field, and we were working over $\mathbb{Z}$ instead? Could I reasonably interpret the point cut out by the prime $q \in \mathbb{Z}$ in ${\mathbb{P}_\mathbb{Z}^3}^\vee$ , say, as a ""hyperplane in $\mathbb{P}_\mathbb{Z}^e$ ?"" (I guess this also relates back to question 1.) All in all, I am very confused. I never understood this chapter of Vakil and ended up skipping it on a first read-through, but I am rapidly approaching the later chapters and am disturbed by the fact that I'm still not entirely sure what is meant by a ""general hyperplane,"" which I think is a big problem.","['algebraic-geometry', 'projective-schemes', 'schemes']"
4651227,A problem from Applied Asymptotic Analysis by Peter D. Miller (Exercise 1.8),"transcribed exercise Exercise 1.8. Suppose that $\mu$ is a continuous parameter and that for each $\mu \in[0,1]$ , we have $f(z, \mu)=O(g(z, \mu))$ as $z \rightarrow z_0$ from $D$ . The above proof suggests that it might be true that if the integrals exist in the Riemann sense for all $z$ close enough to $z_0$ , then $$
\int_0^1 f(z, \mu) d \mu=O\left(\int_0^1|g(z, \mu)| d \mu\right) \quad \text { as } z \rightarrow z_0 \text { from } D,
$$ since the integrals can be approximated by Riemann sums. Under what additional hypotheses on $f(z, \mu)$ and $g(z, \mu)$ can the proof be adapted to the continuous case? Can you find a counterexample? I am reading Applied Asymptotic Analysis in the series Graduate Studies in Mathematics Volume 75. I'm confused by ex 1.8. I don't think this proposition is true but I can't find a counterexample. However, I find that when $f(z,\mu),g(z,\mu)$ is continuous in $D\times[0,1]$ and $g(z,\mu)\neq0$ ,the statement is true. Any attempt to find a counterexample will be helpful! Here's what the notation means: Definition 1.2 (Big-oh near $\left.z_0\right)$ . Let $f(z)$ and $g(z)$ be two complex-valued functions defined in some set $D$ of the complex plane whose closure contains a point $z_0$ (that is, $z_0$ is a limit point of $D$ ). Then we write $$
f(z)=O(g(z)) \quad \text { as } z \rightarrow z_0 \text { from } D
$$ if there is a number $\delta>0$ such that $$
f(z)=O(g(z)), \quad z \in D \text { with } 0<\left|z-z_0\right|<\delta,
$$ Miller, Peter D. , Applied asymptotic analysis, Graduate Studies in Mathematics 75. Providence, RI: American Mathematical Society (AMS) (ISBN 0-8218-4078-9/hbk). xv, 467 p. (2006). ZBL1101.41031 .","['examples-counterexamples', 'asymptotics', 'analysis', 'real-analysis', 'riemann-integration']"
4651239,Spaces in which every sequence of distinct points converges,"This question is an offshoot of the following recent question: Does there exist a metric on $\mathbb{R}$ such that every sequence converges? By just adding the word 'distinct' the questions seems to change. An  example of a metric space in which every sequence of distinct points converges is $X=\{0,\frac 1 2, \frac  1 3,...\}$ with the usual metric from $\mathbb R$ .  (This space has infinitely many points). I am wondering if there is any Haudorff topological space with uncountably many points in which every sequence of distinct points converges.
I would also be interested in seeing more interesting examples of metric spces with this property.","['general-topology', 'metric-spaces']"
4651255,Conjectured relation between hyperbolic sums,Does $$ 2\sum_{k=1}^{\infty} \frac{(-1)^{k-1} k}{e^{2\pi k}-1} +\sqrt{2} \sum_{k=1}^{\infty} \frac{(-1)^{k-1}k}{e^{2\pi k}+1} \stackrel{?}{=} \frac{1}{4\pi} - \frac{2-\sqrt{2}}{8} $$ The two appear to agree numerically. Can someone confirm or link a reference? Thanks,"['hyperbolic-functions', 'sequences-and-series']"
4651256,Finding functions that satisfy a condition,"Let $(a_n)_n$ be a positive numbers sequence such that $$ \lim_{n\to\infty} a_n =0 $$ Find the functions $f :R \to R$ that admit primitives such that $$2f(x)=f(x+a_n)+f(x-a_n)$$ for every x $\in R$ and n $\in N$ I've been struggling for a while with this problem. I've noticed that the linear functions satisfy the conditions but I don't know how I might prove that there aren't other functions. My instinct was to first prove that $f$ is monotonous, since  the existence of primitives ensures that the function has Darboux, but I didn't have any success.","['limits', 'indefinite-integrals', 'real-analysis']"
4651299,"Proving $\sum_{cyc}\frac{\sqrt{y + z − x}}{\sqrt{y} + \sqrt{z} − \sqrt{x}} \leq 3$ for $x$, $y$, $z$ the sides of a triangle [duplicate]","This question already has an answer here : $\sum_{cyclic}\frac{\sqrt{b+c-a}}{\sqrt{b}+\sqrt{c}-\sqrt{a}}\leq 3$ (1 answer) Closed last year . Let $x,y$ and $z$ be the length of a triangle. Prove that $$\frac{\sqrt{y + z − x}}{\sqrt{y} + \sqrt{z} − \sqrt{x}} 
+ \frac{\sqrt{z + x − y}}{\sqrt{z} + \sqrt{x} − \sqrt{y}} 
+ \frac{\sqrt{x + y − z}}{\sqrt{x} + \sqrt{y} − \sqrt{z}} \leq 3$$ I tried to prove that one of the expressions $\leq1$ $x\leq y\leq z$ $∵x-y\leq 0, x-z\leq 0$ $∴(x-y)(x-z) \geq 0$ $x^2-(y+z)x+yz \geq 0$ $x^2+yz \geq x(y+z)$ $x^2+yz \geq xy+xz$ $x^2+yz+2x\sqrt{yz} \geq xy+xz+2x\sqrt{yz}$ $(x+\sqrt{yz})^2 \geq (\sqrt{xy}+\sqrt{xz})^2$ $x+\sqrt{yz} \geq \sqrt{xy}+\sqrt{xz}$ $x \geq \sqrt{xy}+\sqrt{xz}-\sqrt{yz}$ $-2x \leq -2(\sqrt{xy}+\sqrt{xz}-\sqrt{yz})$ $-x \leq -2(\sqrt{xy}+\sqrt{xz}-\sqrt{yz})+x$ $∵y+z > 0$ $y+z-x \leq x+y+z-2(\sqrt{xy}+\sqrt{xz}-\sqrt{yz})$ $y+z-x \leq (\sqrt{y}+\sqrt{z}-\sqrt{x})^2$ $∴ \sqrt{y+z-x} \leq \sqrt{y}+\sqrt{z}-\sqrt{x}$ $\frac{\sqrt{y+z-x}}{\sqrt{y}+\sqrt{z}-\sqrt{x}} \leq 1$ Similarly, the other two expressions can also be proven the same way. Therefore $$\frac{\sqrt{𝑦 + 𝑧 − 𝑥}}{\sqrt{𝑦} + \sqrt{𝑧} − \sqrt{𝑥}} + 
\frac{\sqrt{𝑧 + 𝑥 − 𝑦}}{\sqrt{𝑧} + \sqrt{𝑥} − \sqrt{𝑦}} +
\frac{\sqrt{𝑥 + 𝑦 − 𝑧}}{\sqrt{𝑥} + \sqrt{𝑦} − \sqrt{𝑧}} \leq 3$$ Is my proof correct？ $$\frac{\sqrt{𝑧 + 𝑥 − 𝑦}}{\sqrt{𝑧} + \sqrt{𝑥} − \sqrt{𝑦}} \geq 1$$ and $$\frac{\sqrt{𝑥 + 𝑦 − 𝑧}}{\sqrt{𝑥} + \sqrt{𝑦} − \sqrt{𝑧}} \leq 1$$ My proof is incomplete because the 2nd term does not sastify my proof. May I know what is the next steps?","['algebra-precalculus', 'inequality']"
4651327,Using Cayley-Hamilton theorem to find the minimal polynomial,"I need to find the minimal polynomial of $\beta = i + \sqrt[3]{2}$ over $\mathbb{Q}$ using Cayley-Hamilton theorem. So far I've found that $\{1, \sqrt[3]{2}, \sqrt[3]{4}, i, \sqrt[3]{2}i, \sqrt[3]{4}i\}$ is a basis of $\mathbb{Q}[\sqrt[3]{2},i]$ as a vector space over $\mathbb{Q}$ . Also, the matrix representing the multiplication by $\beta$ using this basis is $$
          A = 
          \begin{pmatrix}
            0& 0& 2&-1& 0& 0\\
            1& 0& 0& 0&-1& 0\\
            0& 1& 0& 0& 0&-1\\
            1& 0& 0& 0& 0& 2\\
            0& 1& 0& 1& 0& 0\\
            0& 0& 1& 0& 1& 0\\
          \end{pmatrix}
$$ where the element $$\lambda_{1} + \lambda_{2}\sqrt[3]{2} + \lambda_{3}\sqrt[3]{4} + \lambda_{4}i + \lambda_{5}\sqrt[3]{2}i + \lambda_{6}\sqrt[3]{4}i$$ in $\mathbb{Q}[\sqrt[3]{2},i]$ is represented by the vector $$
          \begin{pmatrix}
            \lambda_{1}\\
            \lambda_{2}\\
            \lambda_{3}\\
            \lambda_{4}\\
            \lambda_{5}\\
            \lambda_{6}\\
          \end{pmatrix}.
$$ I could't figure out how to use the characteristic polynomial of $A$ ( $p(x) = x^{6} + 3x^{4} - 4x^{3} + 3x^{2}$ ) to find a polynomial $f(x)$ in $\mathbb{Q}[x]$ such that $f(\beta) = 0$ .","['matrices', 'galois-theory', 'field-theory', 'abstract-algebra', 'extension-field']"
4651331,Computing $\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)$,"I am trying to compute the following limit: $$\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)$$ I have tried by rewriting it as $\lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right)$ and applying De l'Hopital's Rule but the expression quickly becomes unmanageable: $$\lim_{x\to 0}\left(\frac{3x+(x^3-3)\tan(x)}{3x^4 \tan(x)}\right)\overset{H}{=}\lim_{x\to 0}\frac{3+(x^2 - 3) \sec^2(x) + 2 x \tan(x)}{3 x^3 (4 \tan(x) + x \sec^2(x))}$$ so I then tried by using the Maclaurin expression for $\tan(x)$ : \begin{align*}
\lim_{x\to 0}\left(\frac{\cot(x)}{x^3}-\frac{1}{x^4}+\frac{1}{3x^2}\right)&=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\tan(x)}{x^2\tan(x)}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{x-\left(x+\frac{x^3}{3}+\frac{2}{15}x^5\right)}{x\left(\frac{x^3}{3}+\frac{2}{15}x^5\right)}+\frac{1}{3}\right)\\
&=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=+\infty\cdot 0
\end{align*} and I got an indeterminate form. I am currently out of ideas so I would appreciate some help in figuring this out, thanks. EDIT: It just occurred to me that \begin{align*}
\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}+\frac{1}{3}\right)=\lim_{x\to 0}\frac{1}{x^2}\left(\frac{-\frac{1}{3}-\frac{2}{15}x^2+\frac{1}{3}+\frac{1}{9}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)\\ \lim_{x\to 0} \frac{1}{x^2}\left(\frac{-\frac{1}{45}x^2+\frac{2}{45}x^4}{1+\frac{x^2}{3}+\frac{2}{15}x^4}\right)=\lim_{x\to 0}\frac{-\frac{1}{45}+\frac{2}{45}x^2}{1+\frac{x^2}{3}+\frac{2}{15}x^4}=-\frac{1}{45}.
\end{align*}","['limits', 'calculus']"
4651354,Exercise III.2.1(b) Hartshorne: Isn't the restriction of a constant sheaf on a connected space to a connected subspace again constant?,"Consider Exercise III.2.1(b) in Hartshorne: Let $X=\mathbb{A}_k^n$ for $n\geq 2$ and an infinite field $k$ , and let $Y\subseteq X$ be the union of $n+1$ hyperplanes in general position. Let $U=X\setminus Y$ , and denote by $\underline{\mathbb{Z}}$ the constant sheaf on $X$ , and also $\underline{\mathbb{Z}}_U:=j_{!}(j^{-1}\underline{\mathbb{Z}})$ as well as $\underline{\mathbb{Z}}_Y:=i_{*}(i^{-1}\underline{\mathbb{Z}})$ , where $j:U\to X$ and $i:Y\to X$ are the inclusion maps. Then show that $H^n(X,\underline{\mathbb{Z}}_U)\neq 0$ . Now I'm confused: consider the sheaf $i^{-1}\underline{\mathbb{Z}}$ on $Y$ . By definition, this is the sheaf associated to the presheaf $i^{-1,pre}\underline{\mathbb{Z}}$ defined by $$
i^{-1,pre}\underline{\mathbb{Z}}(W)=\lim_{\substack{V\subseteq_{\text{open}} X \\ W\subseteq V}}\underline{\mathbb{Z}}(V).
$$ But now as $X$ is irreducible, we have $\underline{\mathbb{Z}}(V)=\mathbb{Z}$ for all non-empty $V$ , and the restriction maps are just the identity, so $i^{-1,pre}\underline{\mathbb{Z}}(W)=\mathbb{Z}$ for all non-empty $W$ . But now as $Y$ is connected for $n\geq 2$ , this is in fact already a sheaf, namely the constant sheaf associated to $\mathbb{Z}$ on $Y$ . But then this is flasque, so the higher cohomology groups vanish, which gives me $H^n(X,\underline{\mathbb{Z}}_U)=0$ . Where am I going wrong?","['algebraic-geometry', 'homology-cohomology', 'sheaf-cohomology']"
4651356,Can the sample mean converge faster than $1/\sqrt{n}$?,"Consider a IID sequence $X_1,...,X_n$ of random variables with $\mathbb{E}[X_i] = 0$ and $\mathbb{E}[X_i^2] = 1$ . The central limit theorem implies that the sample mean converges in $L_1$ to $0$ at a rate $O(1/\sqrt{n})$ . I am interested in an anti-concentration result showing that this is the fastest possible rate over all $X_i$ s. So far, using the Paley-Zygmund inequality, I have derived such a result assuming the $X_i$ 's have sufficiently small $4^{th}$ moments; specifically, for some universal constant $c_0$ , $$\mathbb{E} \left[ \left| \frac{1}{n} \sum_{i = 1}^n X_i \right| \right] \geq \frac{c_0}{\sqrt{n}} \frac{n}{\mathbb{E}[X_i^4] + n} \in \Omega \left( \frac{1}{\sqrt{n}} \right),$$ which implies my desired result as long as $\mathbb{E}[X_i^4] \in O(n)$ . Previously, using Berry-Esseen, I was also able to get a similar result assuming $(\mathbb{E}[X_i^3])^{4/3} \in O(\sqrt{n})$ . However, it seems counterintuitive to me that having a large $3^{rd}$ or $4^{th}$ moment would speed up the rate of convergence, and I'm wondering if these assumption can be relaxed. Actually, in my application, it would be ok to assume $\mathbb{E}[X_i^p] \in O(n^{p/2})$ for any $p \geq 1$ , although it would be simpler if I can avoid making any assumptions on $X_i$ . Question: Without any further assumptions on the $X_i$ s, do there exist universal constants $c_0, n_0 > 0$ such that, for all integers $n > n_0$ , $$\mathbb{E} \left[ \left| \frac{1}{n} \sum_{i = 1}^n X_i \right| \right]
  \geq \frac{c_0}{\sqrt{n}}.$$ Alternatively, is there a counterexample, i.e., a sequence of IID sequences $\{\{X_{n,i}\}_{i = 1}^n\}_{n = 1}^\infty$ such that $$\lim_{n \to \infty} \mathbb{E} \left[ \left| \frac{1}{\sqrt{n}} \sum_{i = 1}^n X_{n,i} \right| \right] \to 0.$$","['statistics', 'central-limit-theorem', 'probability']"
4651406,Which cyclic groups are automorphism groups?,"It is easy to prove, e.g. here that any group $G$ with cyclic automoprhism group must be Abelian. Cyclic groups of order $\phi(p^n) = (p-1)p^{n-1}$ for $p \ne 2$ are obviously automorphism groups, as they are the automorphism groups of $C_{p^n}$ . FTFAG proves these are the only possibilities for finitely generated Abelian groups. That link also proves that cyclic groups of odd order (other than $C_1$ ) cannot be automorphism groups. However, this obviously leaves the question open for many even orders, the smallest being $14$ . Other than these cases, I cannot prove anything more about possible orders of cyclic automorphism groups, or find any further theorems. This MathOverflow answer talks about non locally cyclic groups with cyclic automorphism groups, and suggests $C_2$ , $C_4$ and $C_6$ may be the only possibilities. But all of these are of the form $C_{(p-1)p^m}$ . I can't find anything talking about this question for general groups. So I am wondering whether there are any groups with cyclic automorphism group not of order $(p-1)p^{m}$ . So I would like to know any counterexample to this, or any proof that rules out any even order of cyclic groups.","['automorphism-group', 'infinite-groups', 'cyclic-groups', 'group-theory', 'abelian-groups']"
4651425,Calculate the inscribed circle between one line and two circles,"I am currently working on finding a calculation that will help me determine the values of $C_x$ , $C_y$ and $C_r$ as shown in the image below. The goal is for the unknown red circle to touch the line and be tangent to the two green circles. This website explains the situation as pointed out by Intelligenti pauca For example, the given values are:

C1r = 2
C1x = 2
C1y = 8
C2r = 2.5
C2x = 11
C2y = 9
P1x = 5
P1y = 0
P2x = 12
P2y = 3

The outcome should be somewhere around:
Cr = 3.671
Cx = 6.596
Cy = 4.678 The worked out result in C# code (based on the accepted answer ) public static bool GetCircleFromLCC(out double C_x, out double C_y, out double C_radius)
    {
        // -- Input values --
        double C1_radius = 2;
        double C1_x = 2;
        double C1_y = 8;
        double C2_radius = 2.5;
        double C2_x = 11;
        double C2_y = 9;
        double P1_x = 5;
        double P1_y = 0;
        double P2_x = 12;
        double P2_y = 3;

        // -- Calculations --
        double u_x = P2_x - P1_x;
        double u_y = P2_y - P1_y;
        double norm = Math.Sqrt(u_x * u_x + u_y * u_y);
        u_x /= norm;
        u_y /= norm;
        double u1_x = -(u_x * P1_x + u_y * P1_y);
        double u1_y = -(-u_y * P1_x + u_x * P1_y);
        double u2_x = u_x * C1_x + u_y * C1_y;
        double u2_y = -u_y * C1_x + u_x * C1_y;
        double c1p_x = u2_x + u1_x;
        double c1p_y = u2_y + u1_y;
        u2_x = u_x * C2_x + u_y * C2_y;
        u2_y = -u_y * C2_x + u_x * C2_y;
        double c2p_x = u2_x + u1_x;
        double c2p_y = u2_y + u1_y;
        double a = 2.0 * (c1p_x - c2p_x);
        double b = 2.0 * (c1p_y - c2p_y + C1_radius - C2_radius);
        double c = Math.Pow(C1_radius, 2) - Math.Pow(C2_radius, 2) + Math.Pow(c2p_x, 2) - Math.Pow(c1p_x, 2) + Math.Pow(c2p_y, 2) - Math.Pow(c1p_y, 2);
        double a0 = Math.Pow(c1p_x, 2) + Math.Pow(c1p_y, 2) - Math.Pow(C1_radius, 2) + 2.0 * c / b * (c1p_y + C1_radius);
        double a1 = -2.0 * c1p_x + 2.0 * a / b * (C1_radius + c1p_y);
        double xdisc = Math.Pow(a1, 2) - 4.0 * a0;

        // Find quadratic roots
        if (xdisc > 0.00000001)
        {
            double root1 = (-a1 - Math.Sqrt(xdisc)) / 2;
            C_radius = 1 / b * (-c - a * root1);
            if (Math.Sign(C_radius) == Math.Sign(c1p_y) && Math.Sign(C_radius) == Math.Sign(c2p_y))
            {
                // Valid root, calculate the corresponding x,y coordinates
                u2_x = u_x * root1 + -u_y * C_radius;
                u2_y = u_y * root1 + u_x * C_radius;
                C_x = u2_x + P1_x;
                C_y = u2_y + P1_y;

                Console.WriteLine($""Solution 1   C_x: {C_x:0.0##}   y: {C_y:0.0##}   radius: {C_radius:0.0##}"");
                // Solution 1   x: 6.596   y: 4.678   radius: 3.671
            }
            double root2 = (-a1 + Math.Sqrt(xdisc)) / 2;
            C_radius = 1 / b * (-c - a * root2);
            if (Math.Sign(C_radius) == Math.Sign(c1p_y) && Math.Sign(C_radius) == Math.Sign(c2p_y))
            {
                // Valid root, calculate the corresponding x,y coordinates
                u2_x = u_x * root2 + -u_y * C_radius;
                u2_y = u_y * root2 + u_x * C_radius;
                C_x = u2_x + P1_x;
                C_y = u2_y + P1_y;

                Console.WriteLine($""Solution 2   C_x: {C_x:0.0##}   y: {C_y:0.0##}   radius: {C_radius:0.0##}"");
                // Solution 2   x: -48.359   y: 336.121   radius: 329.963
                return true;
            }
        }

        C_x = 0;
        C_y = 0;
        C_radius = 0;
        return false;

    }",['geometry']
4651427,Find maximum order of a group G [duplicate],"This question already has answers here : Finding an example of a group with some properties (2 answers) Closed last year . Let $G$ be a non-abelian group with $e$ being the identity element. We know that there exists $y$ of order $2$ in $G$ so that $x^2=y$ for all $x \in G \setminus \{e,y\}$ . Find the maximum order that $G$ can have. I have no clue where to start, I know that every element has order $1,2$ or $4$ so the group must have order a power of $2$ , but I don't have ideas how to find a way to ""limit"" the number of elements. I found out that the quaternions are a group of order $8$ that verify these requirements, so the minimum is greater or equal to 8.",['group-theory']
4651435,Valid proof for integral of $1/(x^2+a^2)$,"I'm trying to prove some integral table formulae and had a concern over my proof of the following formula: $$\int\frac{1}{x^2+a^2}\;dx=\frac{1}{a}\arctan\left(\frac{x}{a}\right)+C$$ Claim: $$\frac{1}{x^2+a^2}=\frac{1}{a^2}\sum_{k=1}^\infty(-1)^{k-1}\left(\frac{x}{a}\right)^{2k}\hspace{5mm}\forall\;x\in(-a,a)$$ Proof: $$\frac{1}{x^2+a^2}=\frac{1}{a^2}-\frac{x^2}{a^4}+\frac{x^4}{a^6}-\frac{x^6}{a^8}+\dots$$ $$\implies 1=(x^2+a^2)\left(\frac{1}{a^2}-\frac{x^2}{a^4}+\frac{x^4}{a^6}-\frac{x^6}{a^8}+\dots\right)$$ $$\implies 1=\left(\frac{x^2}{a^2}-\frac{x^4}{a^4}+\frac{x^6}{a^6}-\frac{x^8}{a^8}+\dots\right)+\left(1-\frac{x^2}{a^2}+\frac{x^4}{a^4}-\frac{x^6}{a^6}+\dots\right)$$ $$\implies 1=1\hspace{5mm}\forall\;x\in(-a,a)$$ My proof then uses the following: $$\int\frac{1}{x^2+a^2}\;dx=\frac{1}{a^2}\int\sum_{k=1}^\infty(-1)^{k-1}\left(\frac{x}{a}\right)^{2k}\;dx=\frac{1}{a}\arctan\left(\frac{x}{a}\right)+C$$ My concern is that this isn't a valid proof since the radius of convergence of arctangent's Taylor Series is finite. I'm 7 years removed from taking calculus so I'm admittedly forgetful of the fine details on this. Could someone explain if this is a valid approach or, if not, why?","['integration', 'calculus']"
4651493,"On a certain ""anticurl"" operator","I recently found myself curious what explicit formula I would get if I traced through the de Rham cohomology proof that if $\mathbf{F}$ is a vector field defined on all of $\mathbb{R}^3$ which has divergence 0, then $\mathbf{F}$ is the curl of some other vector field.  So, tracing through the proof, what I did starting with such a divergence-free vector field was: Convert $\mathbf{F}$ to a 2-form. Pull back along the contraction homotopy $H : \mathbb{R}^3 \times [0, 1] \to \mathbb{R}^3$ , $(x, y, z, t) \mapsto (tx, ty, tz)$ . Apply the integral operator $\mathfrak{I}$ which intuitively sends $dt \wedge \omega \mapsto \int_{t=0}^{t=1} \omega \, dt$ and for other $\omega$ without a $dt$ , only with $dx,dy,dz$ , $\omega \mapsto 0$ . Convert the resulting 1-form back to a vector field. After working through this, the formula I eventually got could be summarized as: $$(\operatorname{anticurl} \mathbf{F})(\mathbf{x}) := \left[ \int_0^1 t \mathbf{F}(t \mathbf{x})\,dt \right] \times \mathbf{x}.$$ This has the expected property that $\operatorname{div} \mathbf{F} = 0 \implies \mathbf{F} = \operatorname{curl} (\operatorname{anticurl} \mathbf{F})$ .  (And trying it out on for example $\mathbf{F}(x, y, z) = (y^a z^b, 0, 0)$ , I get $(\operatorname{anticurl} \mathbf{F})(x, y, z) = \frac{1}{a+b+2} (0, -y^a z^{b+1}, y^{a+1} z^b)$ which does have the expected curl.) It is also interesting that this $\operatorname{anticurl}$ is $\mathbb{R}$ -linear and in addition it respects rotations about axes through the origin. So now, what I was curious about was: this explicit partial inverse of curl seems like something that very likely would have shown up before.  If there's a standard name for it, that would be interesting to know, or otherwise it would be good to see an example usage of this formula in a textbook or other reference more on the introductory than research level.","['multivariable-calculus', 'curl', 'de-rham-cohomology', 'reference-request']"
4651524,numerical approaches to functional equations,"I'm interested in finding numerical approaches to solving functional equations such as $$f(xy) = f(x)+f(y),$$ where the equations had no derivatives or integrals, and contains arguments involving $x$ and $y$ . Reason I ask is that when searching the web/literature, I can find information on some methods for functional differential/integral equations (e.g. Laplace transform and method of steps for time-variant problems, etc.).  But I have not been able to find numerical methods (if there are any) for the type of functional equations like shown above. My idea for general approach (which has been around a long time): Search for smooth solutions by repeatedly differentiating functional equation wrt $x$ or $y$ , solving resulting system to eliminate one variable, obtaining ODE, solving ODE numerically given value of function at single point.  This method can be used to solve d'Alembert's equation, for example.  Of course, requiring differentiability greatly limits possible numerical solutions that could be searched for. I'm looking for areas of research and curious what information is out there on this topic.  Any guidance is appreciated. Intuitively, ideal method would involve being able to evaluate $f$ at points which can be iteratively generated, do not form a cycle, and are closely spaced in desired interval on the real line.","['functional-equations', 'abstract-algebra', 'recurrence-relations', 'ordinary-differential-equations']"
4651548,"Prove that if $f''(x)≤f(x)$ , then $ f'(x)<\sqrt{2}f(x)$ for every $x\in\mathbb{R}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I need help with this problem,
""Let $f\in\mathcal{C}^{2}(\mathbb{R})$ such that $f(x)$ , $f'(x)$ and $f''(x)$ are all strictly positive for every $x\in\mathbb{R}$ . Show that if $f''(x)\leq f(x)$ for every $x\in\mathbb{R}$ , then $f'(x)<\sqrt{2}f(x)$ for every $x\in\mathbb{R}$ "" I apologize if it seems like I'm asking you to do my work, but I honestly don't know where to begin with this...","['derivatives', 'inequality', 'linear-algebra', 'real-analysis']"
4651560,Proving that $0 \leq k \sin\left(\frac{2\pi}{n}\right) - \sin\left(\frac{2\pi k}{n}\right)$,"I need help proving that $$
0 \leq k \sin\left(\frac{2\pi}{n}\right) - \sin\left(\frac{2\pi k}{n}\right)
$$ For $n > 2$ and positive $k$ . I've tried all sorts of identities, and nothing have worked. Any help would be greatly appreciated. The things I have tried are the following: Using a identity for multiples angle formula: $$
k\sin \left(\frac{2 \pi}{n}\right)  >\sin\left(k\cdot\frac{2 \pi }{n}\right)=2^{k -1}\prod_{i=0}^{k-1}\sin \left(\frac{\pi i}{k}+\frac{2 \pi }{n}\right)
$$ $$
k \geq 2^{k -1}\prod_{i=1}^{k-1}\sin \left(\frac{\pi i}{k}+\frac{2 \pi }{n}\right)
$$ Which I could not make sense of. I also tried rewriting the inequality $$
k\sin \left(\frac{2 \pi}{n}\right)  +\sin \left(\frac{2 \pi (n -k)}{n}\right)>0
$$ Which got me here: $$\begin{split}
&=(k-1)\sin \left(\frac{2 \pi}{n}\right)+\sin \left(\frac{2 \pi}{n}\right)+\sin \left(\frac{2 \pi (n -k)}{n}\right)\\
&=(k-1)\sin \left(\frac{2 \pi}{n}\right)+2\sin \left(\frac12\left[\frac{2 \pi}{n}+\frac{2 \pi (n -k)}{n}\right]\right)\sin \left(\frac12\left[\frac{2 \pi}{n}-\frac{2 \pi (n -k)}{n}\right]\right)
\end{split}$$ But I could not make that work either.","['trigonometry', 'inequality']"
4651633,"For any finite group $G$, is it always true that $ b_0\cdot b_1 \cdot\cdot\cdot b_{n}=b_0?$","For a finite group $G=\{a_0, a_1, ..., a_n\}$ , if we define $b_i=a_j$ , does there always exist an arrangement for the index $i$ and $j$ , such that $b_0\cdot b_1 \cdot\cdot\cdot b_n=b_0$ ? My attempt: This holds for all Abelian groups since we can switch the order for the multiplication. For example, since the group is closed, we have $$a_0\cdot a_1 \cdot\cdot\cdot a_n=a_k$$ $$\Rightarrow a_{k}\cdot a_0\cdot a_1 \cdot\cdot\cdot a_{k-1} \cdot a_{k+1}\cdot\cdot\cdot  a_n=a_k$$ So we can define $b_0=a_k, b_1=a_0, b_2=a_1 ... b_n=a_n$ , then we have $$ b_0\cdot b_1 \cdot\cdot\cdot b_{n}=b_0$$ But what about non-Abelian groups?","['group-theory', 'abstract-algebra', 'finite-groups']"
4651667,Conditional average number of tasks in M/M/$\infty$ queue,"I'm trying to solve the following problem: Consider an M/M/ $\infty$ queue in which the time intervals between task arrivals are i.i.d. exponential with parameter $\lambda$ and task durations are also i.i.d. exponential with parameter $\mu$ . Show that, given that $n$ tasks are being processed at time $0$ , the expected number of tasks being processed at time $t$ is $ne^{-\mu t} + \frac{\lambda}{\mu}(1 - e^{\mu t}).$ So far, I've found that, when the queue is in equilibrium, the average number of tasks being processed follows a Poisson distribution with parameter $\frac{\lambda}{\mu}$ , but I can't make sense of how to arrive at this expression. How does one solve this?","['markov-chains', 'stochastic-processes', 'queueing-theory', 'probability-theory', 'probability']"
4651673,"Calculate $\mathbb{E}[N(X)]$, where $N(·)$ is the cdf of the standard normal distribution, and $X$ is a standard normal random variable.","I'm stuck with this problem: Calculate $\mathbb{E}[N(X)]$ , where N(·) is the cdf of the standard normal distribution, and X is a standard normal random variable. Here's where I'm stuck: We know that: $$\mathbb{E}[g(X)]=\int_{-\infty }^{\infty}g(x)f_{X}(x)dx$$ And we are given that: $$ g(x) = N(x) = \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x}e^{-\frac{u^2}{2}}du $$ $$ f_{X}(x)=\frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} $$ So, the solution should be to develop this: $$\begin{split}
\mathbb{E}[N(X)] &= \int_{-\infty }^{\infty} \left( \frac{1}{\sqrt{2 \pi}} \int_{-\infty}^{x}e^{-\frac{u^2}{2}}du \right) \left( \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}} \right) dx\\
&= \frac{1}{2 \pi} \int_{-\infty }^{\infty} \left( \int_{-\infty}^{x}e^{-\frac{u^2}{2}}du \right) e^{-\frac{x^2}{2}} dx\\
&= \frac{1}{2 \pi} \int_{-\infty }^{\infty} \int_{-\infty}^{x} e^{-\frac{u^2+x^2}{2}}dudx
\end{split}$$ My questions are: Can we combine the exponents moving the term with x inside, the last step above? If yes, this looks like a good opportunity to change variables to polar. If that's the case, how to change the limits of the integrals from $x$ and $u$ to $r$ and $\theta$ ? Any help would be extremely appreciated! Thanks!","['statistics', 'definite-integrals', 'polar-coordinates', 'probability', 'random-variables']"
4651686,Class groups of function fields of Riemann surfaces,"Let $X$ be a compact Riemann surface, and $f: X \to \mathbb{P}^1(\mathbb{C})$ be a holomorphic branched cover. This induces a finite extension of their fields of meromorphic functions $f^*: \mathbb{C}(z) \to \mathcal{M}(X)$ . $\mathbb{C}(z)$ is the field of fractions of $\mathbb{C}[z]$ , which is a principal ideal domain, and therefore a Dedekind domain. The integral closure $\mathcal{O}$ of $\mathbb{C}[z]$ in $\mathcal{M}(X)$ is also a Dedekind domain. I believe one can show that $\mathcal{O} = \{g \in \mathcal{M}(X): g^{-1}(\infty) \subseteq f^{-1}(\infty)\}$ , so it consists of the functions which are finite everywhere the cover $f$ is finite. What can we say about the class group of $\mathcal{O}$ ? Is it related to the Picard group of $X$ ? As I understand, there is a distinction between the two, in that the prime ideals of $\mathcal{O}$ should correspond to points in $f^{-1}(\mathbb{P}^1 - \infty)$ . So the ideal structure of $\mathcal{O}$ does not ""see"" the fiber of $f$ over $\infty$ , whereas the Picard group accounts for arbitrary divisors on $X$ . Different choices of $f$ will gives us different integral closures of $\mathbb{C}[z]$ in $\mathcal{M}(X)$ . Are there any properties which are invariant under choice of cover? See Forster, Lectures on Riemann Surfaces, chapter 8 for the algebraic properties of branched covers of Riemann surfaces.","['algebraic-number-theory', 'riemann-surfaces', 'complex-geometry', 'field-theory', 'algebraic-geometry']"
4651741,Linear Algebra - Linear transformation question,"Let $
b \in \mathbb{R}^4, \space A\in M_{4\times4} (\mathbb{R}). 
$ Suppose $$
\begin{bmatrix}  0  \\ 1 \\ 1 \\ 1\end{bmatrix}, \begin{bmatrix}  1  \\ 0 \\ 1 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 0 \\ 1\end{bmatrix},\begin{bmatrix}  1  \\ 1 \\ 1 \\ 0\end{bmatrix}, \begin{bmatrix}  1  \\ 1 \\ 1 \\ 1\end{bmatrix}$$ are all solutions of the equation $Ax=b$ . Prove that $$b = \begin{bmatrix}  0  \\ 0 \\ 0 \\ 0\end{bmatrix}.$$ I had an intuiton that the given vectors are linear-dependent since there are five of them and the dim of the vector space is four. The professor solved that question after the exam using linear transformation which seemed much easier and clean solution however he didn't explain the intuition behind. I  would like to get an explanation regarding that. Thanks","['abstract-algebra', 'linear-algebra', 'linear-transformations']"
4651772,Butterfly Theorem,"Let $ABC$ an isosceles acute triangle and $D$ be the middle of the base $AB$ . Let's consider $E \in AB$ and $O$ , the circumcenter of the triangle $ACE$ . Prove that the perpendicular in $D$ on $OD$ , the perpendicular from $E$ to $BC$ , and the parallel through $B$ at $AC$ are concurrent. I tried to apply Butterfly Theorem from here .",['geometry']
4651774,Describing the homotopy explicitly.,"Here is the question I am trying to solve: Let $X$ be a based space, and let $PX = \{ \beta: I \to X | \beta(0) = *\}.$ Show that $p_1: PX \to X$ by $p_1(\beta) = \beta(1)$ is a based fibration. I am confused about what should be the exact homotopy that I should write, here is a trial: We have the following commutative diagram: and here is a trial to the homotopy I should write: $H'(y,s)(t) = \begin{cases}
f(y) \left( \frac{2t}{2-s} \right) & 0 \leq t \leq 1- \frac{s}{2},\\
H(y, 2(t-1)+s) & 1- \frac{s}{2} \leq t \leq 1.
 \end{cases}$ Is this a correct homotopy or not? Can it be written and thought of in a more organized and systematic way. Edit for Anne: You draw a square like below and then you draw your homotopy as a line joining s(down) with s(up) then you determine the coordinates of the points on both the s axes and then you calculate the equation of the line joining the points on the x-axes which help in determining the point at which your homotopy changes its definition. EDIT 2 for Anne : Is this what you meant Anne?","['knot-theory', 'general-topology', 'homotopy-theory', 'algebraic-topology']"
4651776,How to get the joint pdf of $X$ and $Y$ and check if $X$ and $Y$ are independent? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed last year . Improve this question Suppose $X \sim Binomial(1, 0.4)$ and $Y \sim Binomial(2, 0.4)$ , respectively. Assume $P(X = 1, Y = 2) = 0$ and X and Y are uncorrelated. What is the joint probability of $X$ and $Y$ ? Also, how to find $P(X\le Y)$ ? [That they are not independent. Because $P(X=1)P(Y=2)=0.4\times 0.4^2\neq 0$ .] I am confused about the question. Because it seems that the information is not enough to get the joint pdf of $X$ and $Y$ . We know that $$
f_X(x)=0.4^x0.6^{1-x}, x=0, 1
$$ and $$
f_Y(y)=\frac{y(y-1)}{2}0.4^y0.6^{2-y}, y=0,1,2
$$ But how to get the joint pdf of $X$ and $Y$ and check if $X$ and $Y$ are independent?","['statistics', 'probability']"
4651800,"Solution of $\int_0^{\infty}p\,dp \int_0^{\pi} d \theta\,\frac{\cos n \theta}{(p^2+k^2-2pk\cos\theta)^{1/2}}$","How can I solve this integral \begin{equation} \int_0^{\infty}p dp \int_0^{\pi} d \theta \frac{\cos n \theta}{(p^2+k^2-2pk\cos\theta)^{1/2}} \end{equation} where $k>0$ and $n=2,3,4,\cdots$ . What I have done: First of all, I have used Laplace expansion in spherical coordinates: \begin{equation}
 \frac{1}{|\mathbf{x}-\mathbf{x}'|} =\sum_{\ell =0}^{\infty} \frac{r_<^{\ell}}{r_{>}^{\ell+1}}P_{\ell}(\cos\gamma), \quad \text{where} \ \cos\gamma = \cos\theta \cos\theta' + \sin\theta \sin\theta' \cos(\phi-\phi')
\end{equation} So we can assume that $\mathbf{k} = (k,0,0)$ and $\mathbf{p} = (p,\theta,0)$ in spherical coordinates. And we get: \begin{equation*}
        \frac{1}{(p^2 + k^2 - 2pk \cos\theta)^{1/2}} = \sum_{\ell = 0}^{\infty} \frac{r_<^{\ell}}{r_{>}^{\ell + 1}} P_{\ell}(\cos\theta)
\end{equation*} Then the integral becomes: \begin{align}
        &\int_0^{\infty} p\ \mathrm{d}{p} \int_0^{\pi} \ \mathrm{d}{\theta} \frac{\cos n \theta}{(p^2 + k^2 - 2pk \cos\theta)^{1/2}} \\[.2cm]
        ={}& \sum_{\ell=0}^{\infty} \left( \int_0^{k}\frac{p^{\ell}}{k^{\ell+1}}\cdot  p  \ \mathrm{d}{p} + \int_k^{\infty}\frac{k^{\ell}}{p^{\ell + 1}}\cdot p\ \mathrm{d}p\right)\left(\int_0^{\pi}\cos(n\theta) P_{\ell}(\cos\theta)\ \mathrm{d}{\theta}\right)\\[.2cm]
={}&k\cdot\sum_{\ell=0}^{\infty}\frac{2\ell+1}{(\ell+2)(\ell-1)}\left(\int_0^{\pi}\cos(n\theta) P_{\ell}(\cos\theta)\ \mathrm{d}{\theta}\right)
\end{align} I have used the Fourier expansion of Legendre polynomials which can be derived from the generating function. You can see it from my
answer , \begin{equation}
P_{n}(\cos\theta) = \sum_{k=0}^{n} \frac{(2k-1)!!}{(2k)!!} \frac{(2n-2k-1)!!}{(2n-2k)!!} \cos[(n-2k)\theta]
\end{equation} We set the coefficient as $a_{nk}$ : \begin{equation}
a_{nk} = \frac{(2k-1)!!}{(2k)!!} \frac{(2n-2k-1)!!}{(2n-2k)!!}
\end{equation} Thus, the integral over angles can be transformed into \begin{align}
        \int_0^{\pi}\cos(n\theta) P_{\ell}(\cos\theta)\ \mathrm{d}\theta&= \sum_{m=0}^{\ell}a_{\ell m} \int_0^{\pi} \cos (n\theta) \cos[(\ell -2m)\theta] \ \mathrm{d}{\theta}\\
            &\overset{(1)}{=}\sum_{m=0}^{\ell} a_{\ell m} \left(\frac{\pi}{2}\delta_{\ell-2m,n}+\frac{\pi}{2}\delta_{2m-\ell,n}\right)\\
            &\overset{(2)}{=}\pi\sum_{m=0}^{\ell}a_{\ell m}\delta_{\ell-2m,n}
\end{align} Finally, the result of this integral is: \begin{align}
        &\int_0^{\infty} p\ \mathrm{d}{p} \int_0^{\pi} \ \mathrm{d}{\theta} \frac{\cos n \theta}{(p^2 + k^2 - 2pk \cos\theta)^{1/2}} \\[.2cm]
        ={}&\sum_{\ell=2}^{\infty} \frac{2\ell+1}{(\ell+2)(\ell-1)}k \sum_{m=0}^{\ell} \pi a_{\ell m}\delta_{\ell-2m,n}\\[.2cm]
        \overset{(2)}{=}{}&\sum_{\ell =n,\  2|(\ell-n)}^{\infty}\frac{2\ell+1}{(\ell+2)(\ell-1)}\pi k a_{\ell,\frac{\ell-n}{2}},\quad j\mapsto \frac{\ell-n}{2}\\[.2cm]
        ={}&\pi k \sum_{j=0}^{\infty} \frac{2(2j+n)+1}{(2j+n+2)(2j+n-1)} a_{2j+n,j}\\[.2cm]
        ={}&\boxed{ \pi k \sum_{j=0}^{\infty} \frac{2(2j+n)+1}{(2j+n+2)(2j+n-1)} \frac{(2j-1)!!}{(2j)!!}\frac{(2j+2n-1)!!}{(2j+2n)!!}}
\end{align} where, $\displaystyle{\int_0^{\pi} \cos mx \cos nx \ \mathrm{d}{x} = \frac{\pi}{2}\delta_{mn},\quad (m,n\in\mathbb{N}^{+})}$ $\displaystyle{
\ell-2m  = -\ell, -\ell+2,-\ell+4, \cdots, \ell-2,\ell}$ . My Question: I have try to calculate this series via Mathematica , I define \begin{align}
f(n) &=  \sum_{j=0}^{\infty} \frac{2(2j+n)+1}{(2j+n+2)(2j+n-1)} \frac{(2j-1)!!}{(2j)!!}\frac{(2j+2n-1)!!}{(2j+2n)!!}\\[.2cm]
&=\sum_{j=0}^{\infty}\frac{1}{2j+n+2}\frac{(2j-1)!!}{(2j)!!}\frac{(2j+2n-1)!!}{(2j+2n)!!} + \sum_{j=0}^{\infty}\frac{1}{2j+n-1}\frac{(2j-1)!!}{(2j)!!}\frac{(2j+2n-1)!!}{(2j+2n)!!}
\end{align} I have found that this series has a very simple form: \begin{align}
f(2)= \frac{2}{3},\quad f(3) = \frac{3}{8},\quad f(4) = \frac{4}{15},\quad f(5)=\frac{5}{24},\quad f(6)=\frac{6}{35},\quad \cdots
\end{align} It seems to imply that the original integral result was: \begin{equation}
\int_0^{\infty} p\ \mathrm{d}{p} \int_0^{\pi} \ \mathrm{d}{\theta} \frac{\cos n \theta}{(p^2 + k^2 - 2pk \cos\theta)^{1/2}} \overset{?}{=} \boxed{\color{red}{  \frac{n}{n^2-1}\pi k} }
\end{equation} The computer tells this result should be correct, but I can't get this result by simplifying the series. Is there any way you can calculate this integral result, or can you simplify my series?","['integration', 'calculus', 'special-functions', 'sequences-and-series']"
4651814,"A theta function identity involving $\vartheta_2(q^3),\vartheta_3(q^3)$","How can we verify the theta function identity? $$
\left ( \vartheta_2(q)^2+3\vartheta_2(q^3) ^2\right )\left ( \vartheta_3(q)^2+3\vartheta_3(q^3)^2 \right )=4\vartheta_2(q)^2\vartheta_3(q)^2.
$$ Where two theta functions are defined by $$
\vartheta_2(q)
=\sum_{n\in\mathbb{Z}}q^{\left ( n+\frac{1}{2}  \right )^2},
\vartheta_3(q)
=\sum_{n\in\mathbb{Z}}q^{n^2}$$ where $q=e^{-\pi\frac{K^\prime}{K} }$ . So we have $$
\vartheta_2(q)=\sqrt{\frac{2kK}{\pi} },
\vartheta_3(q)=\sqrt{\frac{2K}{\pi} }.
$$ Where $K^\prime(k)=K(k^\prime),k^\prime{}^2+k^2=1$ , $K(k)$ is the complete elliptic integral of the first kind, and $k$ is an elliptic modulus.","['complex-analysis', 'calculus', 'theta-functions', 'sequences-and-series', 'elliptic-integrals']"
4651846,Geometric interpretation of the Hessian,"Assume we have a smooth function $f:\mathbb{R}^2 \to \mathbb{R}$ . We may then form the differential of $f$ , denoted by $Df$ , given by the row vector $$
Df=\Big[\frac{\partial f}{\partial x_1},\frac{\partial f}{\partial x_2}\Big]
$$ This quantity is a $1$ -form, i.e. for every $p \in \mathbb{R}^2$ , $Df(p):\mathbb{R}^2 \to \mathbb{R}$ is a linear map. The action of this object is easy to visualize: It is simply the linearization at a point $p$ of the given function, ie. the $Df(p)$ takes in a vector $v \in \mathbb{R}^2$ , and spits out the directional derivative of $f$ in the direction of $v$ . We may then also form the Hessian, $$
Hf = \begin{bmatrix}\frac{\partial f}{\partial^2 x_1} & \frac{\partial f}{\partial x_1\partial x_2} \\ \frac{\partial f}{\partial x_1x_2} & \frac{\partial f}{\partial^2x_2}\end{bmatrix}
$$ The action of this matrix is more difficult for me to understand. According to my understanding, this object should be interpreted as a $2$ -form, i.e., for each point $p \in \mathbb{R}^2$ it eats two vectors $v_1,v_2 \in \mathbb{R}^2$ and spits out a number. However, I am wondering what the geometric interpretation is of these two vectors - in the case of the differential of $f$ , it was clear that the vector it ate was to be interpreted as the direction of the directional derivative. What is the geometric intuition behind the two vectors that the Hessian takes in as argument?","['tensors', 'multivariable-calculus', 'hessian-matrix', 'geometric-interpretation', 'quadratic-forms']"
4651853,Edges of each colour for Ramsey Graphs,"Consider a 2-coloring of the edges of a complete graph $K_{n}$ . Assume it doesn't exhibit a monochromatic subgraph of $m$ vertices thus demonstrating that the Ramsey number $R(m,m)$ is greater than $n$ . In the case that $R(m,m)$ is in fact $n+1$ (call such a colouring a ""maximal"" Ramsey graph) can we say that the number of edges of each colour in the graph are equal? I have in mind the graph on 5 vertices with all edges round the perimeter red and all internal edges blue, but I wonder if such a symmetry continues for $R(4,4)$ or even unknown Ramsey numbers like $R(5,5)$ .","['graph-theory', 'combinatorics', 'ramsey-theory']"
4651856,Can you get an infinite number of derivatives from $\frac{d}{dx}\sin(x)$? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed last year . Improve this question I know that the first derivative of sine of $x$ is cosine of $x$ , but I'm really facing a problem trying to understand the other derivatives of the sine function. The following $2$ statements are both true: $$\frac{d}{dx}\sin(x) = \cos(x)$$ $$\frac{d}{dx}\cos(x) = -\sin(x)$$ And then, according to the constant multiple rule, we can see that the following is also true: $$\frac{d}{dx}-\sin(x) = -1 \cdot \frac{d}{dx}\sin(x) = -1 \cdot \cos(x) = -\cos(x)$$ Hence: $$\frac{d}{dx}-\cos(x) = -1\cdot\frac{d}{dx}\cos(x) = -1\cdot(-\sin(x)) = \sin(x) \ \dots$$ Can we keep repeating this process to infinity? And what does it really mean that the fourth derivative of $\sin(x)$ is $\sin(x)$ ? I've seen a visualisation that explains why the derivative of $\sin(x)$ is $\cos(x)$ , so I know why the first derivative of $\sin(x)$ is $\cos(x)$ , at least intuitively, but I can't really get the idea of the rest of the derivatives of $\sin(x)$ , or why they are repeating, and can we really keep repeating this with no problems?","['calculus', 'solution-verification', 'derivatives', 'trigonometry']"
4651888,Prove that the area of an inscribed hexagon is twice the area of a triangle,"Let $\Delta ABC$ be an acute triangle and denote the circumscribed circle of $\Delta ABC$ $\Gamma$ with midpoint $O$ . Let $A_1, B_1, C_1$ be the points on $\Gamma$ where the lines $AO, BO, CO$ intersect $\Gamma$ . Show that the area of the hexagon $AC_1BA_1CB_1$ is twice the area of $\Delta ABC$ . My first approach was to realize that the hexagon $AC_1BA_1CB_1$ contains $\Delta ABC$ , which means that we have to prove that $|\Delta ABC_1| + |\Delta BA_1C| + |\Delta CB_1A| = |\Delta ABC|$ . This could equivalently be written as: $
\begin{equation}
\frac{|\Delta ABC_1| + |\Delta BA_1C| + |\Delta CB_1A|}{|\Delta ABC|} = 1.
\end{equation}
$ But since the three ""outer"" triangles each share a side with $\Delta ABC$ , the ratio between the areas of $\Delta ABC$ and the respective ""outer"" triangle"" will be $\frac{h_i}{H_i}$ , where $h_i$ is the height of the outer triangle and $H_i$ is the height of $\Delta ABC$ (where both heights  are perpendicular to the shared side). This means that we can write the above equation as: $
\begin{equation}
\frac{h_1}{H_1} + \frac{h_2}{H_2} + \frac{h_3}{H_3} = 1
\end{equation}
$ Now, label the points where the lines $AO, BO, CO$ intersect the opposite side of $\Delta ABC$ $P, R, Q$ respectively ( $P$ on $AB$ , $R$ on $AC$ , $Q$ on $AB$ ). Since we have angles in the same segment of $\Gamma$ , we get the following similar triangles: $\Delta ABC_1\sim\Delta PBC, \Delta CQA_1\sim\Delta ABQ, \Delta ARB_1\sim\Delta RBC$ , which means that we can write: $
\frac{h_1}{H_1} = \frac{|AP|}{|BP|} \\
\frac{h_2}{H_2} = \frac{|CQ|}{|BQ|} \\
\frac{h_3}{H_3} = \frac{|AR|}{|CR|}
$ Which means that we have to prove that the sum of these (above) ratios equals $1$ . But from here, I can't seem to make much progress...","['triangles', 'area', 'circles', 'geometry']"
4652002,"Projection $\operatorname{Spec}(K[x,y]) \rightarrow \operatorname{Spec}(K[x])$ not closed","Let $K$ be a field. I am trying to show that the map $\operatorname{Spec}(K[x,y]) \rightarrow \operatorname{Spec}(K[x])$ induced by the inclusion $K[x] \hookrightarrow K[x,y]$ is not closed (topologically). My ""geometric"" idea was to show that the image of $V(xy-1) \subseteq \operatorname{Spec}(K[x,y])$ is exactly $\operatorname{Spec}(K[x]) \setminus \lbrace 0 \rbrace$ which is not closed. This comes from the intuition that this map ""is the projection $\mathbb{A}_K^2 \rightarrow \mathbb{A}_K$ "", but I do not know how to show this rigurously.","['algebraic-geometry', 'polynomials']"
4652018,Do we need intervals to define the Lebesgue measure?,"The Lebesgue measure is conventionally defined as $$\mu(X) = \inf\{\sum_{n \in \mathbb{N}}(b_n-a_n) | X \subseteq \bigcup_{n \in \mathbb{N}}(a_n,b_n) \}$$ Which can be thought of intuitively as covering the set with translated and stretched copies of the set $(0,1)$ . I am wondering whether we could equally well define it with any other measurable set, and get the same measure, just by dividing by the Lebesgue measure of that basic set. I.e. for some set $B$ $$\mu'(X) = \mu(B) \inf\{\sum_{n \in \mathbb{N}}s_n | X \subseteq \bigcup_{n \in \mathbb{N}}(r_n+s_nB) \}$$ Where $r_n$ are aritrary elements of $\mathbb{R}$ and $s_n$ are arbitrary elements of $\mathbb{R}_{>0}$ . The thing that got me thinking about this was trying to figure out a proof of $$\mu(AX) = |\det A|\mu(X)$$ Where it seems like it would be very helpful to be able to define the measure in terms of sets of the form $AP$ where $P$ is a product of open intervals. I do not know if this is the usual proof. But thinking about this, it seemed like a) this would work and b) this isn't anything particular to sets of the form $AP$ . Other than $0$ and $\infty$ measure sets, it seems like any set could work. Edit: meagre sets also won't work. Moreover, it seems clear that a sets like a disconnected union of a positive measure meagre set and another set wouldn't be able to cover $(0,1)$ without significant overlap, so it seems some kind of 'nowhere meagre' condition will be required. My best guess is that this condition should be that the intersection of any (non-empty) open set with $B$ is non-meagre. E.g. after imposing this restriction, even unbounded sets have to have almost all of their substance in a bounded interval, so the unboundedness shouldn't cause a problem. Highly disconnected sets like fat Cantor sets seem trickier to deal with, but there isn't any obvious proof they couldn't work. @fedja pointed out that this construction won't work for any meagre sets, so fat cantor sets won't work. It is clear that this at least produces a valid outer measure, and Caratheory's criterion turns it into a measure on some $\sigma$ alebra. This measure would also have to be translation invariant, so as long as the Caratheodory criterion produces the same $\sigma$ algebra, it I think the measure has to be the same. I haven't proven this, but I think if stretched and shifted copies of $B$ can cover $(0,1)^n$ so that the set of 'overlap' has Lebesgue measure $0$ , that would be enough to show that $(0,1)$ is in the $\sigma$ algebra generated. And it seems like equality would follow from that. But not being meagre anywhere doesn't seem like a strong enough condition to guarantee the former, and proving either of these things rigorously seems above my ability level. Although the $n$ -dimensional case motivated this, I would be happy with proofs for the $1$ dimensional case. I would ideally like a criterion for which sets this construction works for, though this may be rather optimistic. Failing that, I would like any results about what classes of sets the construction does or doesn't work for, and particularly classes of sets that this does work for.","['measure-theory', 'lebesgue-measure', 'outer-measure', 'real-analysis']"
4652020,Missing step in this proof of $\operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B$,"I know how to prove this inequality using bases for the row space, however, my professor presented a proof of \begin{align*}
  \operatorname*{rank}(A+B) \leq \operatorname*{rank}A + \operatorname*{rank}B
\end{align*} using rank-nullity theorem. He fell sick so he just sent the notes, which I don't understand, can someone please help me. \begin{align*}
  n - \operatorname*{null}(A+B) &\leq 2n - \operatorname*{null}A -\operatorname*{null}B \\
  \operatorname*{null}(A+B) &\geq -n + \operatorname*{null}A + \operatorname*{null}B \\
  \operatorname*{null}A + \operatorname*{null}B &\leq n + \operatorname*{null}(A+B)
\end{align*} Then it suffices to prove this inequality. \begin{align*}
  N(A) \cap N(B) \subseteq N(A+B)
\end{align*} I understand why this is true, but I don't understand the next line \begin{align*}
  \operatorname*{null}A + \operatorname*{null}B \setminus (N(A) \cap N(B)) \leq n 
\end{align*} Not only does this not make sense from the sense of 'where does $n$ come from', it also, if I understand correctly, makes no sense from the mathematical standpoint. He probably meant $\dim(N(A) \cap N(B))$ Hence, \begin{align*}
  \operatorname*{null}A + \operatorname*{null}B \leq n + \dim(N(A) \cap N(B)) \leq n + {\rm null}(A+B)
\end{align*} This last statement makes sense from the fact above that $N(A) \cap N(B) \subseteq N(A+B)$ If anyone knows how to prove this using rank-nullity theorem or understands what happened here, please let me know.","['matrices', 'proof-explanation', 'matrix-rank', 'linear-algebra']"
4652027,Check a problem solution $au + (b + i\lambda)u_{xx} = 0$ with boundary conditions.,"Let $a, b >0 $ and $\lambda \in \mathbb{R}$ such that $\lambda \neq 0$ . Consider $u \in C^{1}([\alpha, \beta])$ such that $u(\alpha) = u_{x}(\alpha) = u_{x}(\beta)=0$ and the problem $$
au + (b + i\lambda)u_{xx} = 0, \ \  \text{in} \ \ (\alpha, \beta)
$$ with $0 < \alpha < \beta < \infty$ . I would like to know if the above problem has a solution. I know that the problem $au + (b + \lambda)u_{xx} = 0$ with the same conditions has a solution and is null. I think in:
Let $V = (u,u_{x})^{T}$ , then system can be written as the following $$
V_{x} = BV, \ \ \text{with} \ \ V(\alpha) = 0
$$ with $$ B = \left[
\begin{array}{cccc}
0 & 1 \\
-\frac{a}{ (b+i\lambda)}& 0 \\
\end{array}
\right] $$ The solution of the differential Equation is given by $$
V(x) = e^{B(x - \alpha)}V(\alpha)
$$ Thus, the fact that $V(\alpha) = 0$ , we get $V = 0$ in $(\alpha, \beta)$ I didn't use the fact that $u_{x}(\beta) = 0$ . Am I right?",['ordinary-differential-equations']
4652037,Can you leave atoms without being used in the method of analytic tableaux?,"I have to prove that using the method of analytic tableaux:
{P, Q ∧ R} ⊢ P ∧ Q
For that, I did such: I hope it's understandable, I'm using my textbook syntax.
What is making me confused in my proof is that $TR$ is never used, so it seems to me there's something lacking in this proof. Is it correct though? I know that each branch in a tableau has to be closed in order to prove an inference, but can I leave atoms hanging like that?","['propositional-calculus', 'solution-verification', 'logic', 'discrete-mathematics']"
4652051,Series expansion of $\left(\frac{\sin x}{x}\right)^a$ at $x=0$,"I need a series expansion at $x=0$ (with nth term please) for $$f(x)= \left(\frac{\sin x}{x}\right)^a$$ where $a>0$ is a real number. We know that $$\sin x=x-\frac{x^3}{3!}+\frac{x^5}{5!}-...$$ So we have $$\frac{\sin x}{x}=1-\frac{x^2}{3!}+\frac{x^4}{5!}-...$$ Now if we take $y= -\frac{x^2}{3!}+\frac{x^4}{5!}-...$ then $$\left(\frac{\sin x}{x}\right)^a=(1+y)^a$$ Then by Binomial theorem we have $$\left(\frac{\sin x}{x}\right)^a=1+a y+\frac{a(a-1)}{2!}y^2+ +\frac{a(a-1)(a-2)}{3!}y^3+...   $$ Now I am stuck to calculate $y^2,y^3,...$ Another approach $$f(x)= \left(\frac{\sin x}{x}\right)^a$$ So on differentiation $$f'(x)=a  \left(\frac{\sin x}{x}\right)^{a-1} \left(\frac{x\cos x-\sin x}{x^2}\right) $$ So we have taking the limit of above function as $x\to 0$ $$f'(0)=0$$ This is again where I am stuck. Any help would be surely appreciated. Thank you.","['real-analysis', 'calculus', 'taylor-expansion', 'trigonometry', 'derivatives']"
4652054,A technical lemma on linear combinations.,"Lemma Let $\{x_1,\dots, x_n\}$ be a linearly independent set of vectors in a normed space $X$ . Then there is a number $c>0$ such that for every choice of scalars $\alpha_1,\dots,\alpha_n$ we have $$\lVert \alpha_1x_1+\cdots+\alpha_nx_n\rVert \ge c\left(\lvert\alpha_1\rvert+\dots+\lvert \alpha_n\rvert\right)\quad (c>0).\tag1$$ Comment: We all know that this is the notorious Lemma 2.4-1 of the book Introductory to Functional Analysis by Kreyszig. Before formalizing the question I searched the forum, but no answer fully satisfied my doubts, the answers I found were alternative proofs or not very exhaustive explanations. Please, I would like you to give me as detailed explanations as possible to the questions, evenif the are trivial, that I am going to ask you. I rewrite the proof and I hope that some of you can clarify my doubts also for those who will come after me. Proof We write $s=\lvert\alpha_1\rvert+\cdots+\lvert\alpha_n \rvert$ If $s=0$ , all $\alpha_j$ are zero, so $(1)$ holds for any $c$ . Let $s>0$ . Then $(1)$ is equivalent to the inequality which we obtain from $(1)$ by dividing by $s$ and writing $\beta_j=\alpha_j/s$ , that is, $$\tag2 \lVert\beta_1x_1+\cdots+\beta_n x_n\rVert\ge c\qquad \left( \sum_{j=1}^n \lvert\beta_j\rvert=1\right).$$ Hence it suffices to prove the existence of a $c>0$ such that $(2)$ holds for every $n-tuple$ of scalars $\beta_1,\dots, \beta_n$ with $\sum\lvert \beta_j \rvert=1$ . Suppose that this is false. Then there exists a sequence $\{y_m\}$ of vectors $$y_m=\beta_1^{(m)}x_1+\cdots+\beta_n^{(m)}x_n\qquad \left(\sum_{j=1}^n\lvert \beta_j^{(m)}\rvert=1\right)$$ such that $$\lVert y_m \rVert\to 0\quad\text{as}\; m\to \infty$$ Question 1 Why this fact deny the hypothesis of the existence of $c$ ? Now we reason as follows. Sice $\sum\lvert \beta_j^{(m)}\rvert=1$ , we have $\lvert\beta_j^{(m)}\rvert\le 1$ . Hence for each fixed $j$ the sequence $$\left(\beta_j^{(m)}\right)=\left(\beta_j^{(1)},\beta_j^{(2)},\dots\right)$$ is bounded. Consequently, by the Bolzano-Weierstrass theorem, $\left(\beta_1^{(m)}\right)$ has a converget subsequence. Let $\beta_1$ denote the limit of that subsequence, and let $\left(y_{1,m}\right)$ denote the corresponding subsequence, and let $\left(y_{1,m}\right)$ denote the corresponding subsequence of $\left( y_m\right)$ . By the same argument, $\left(y_{1,m}\right)$ has a subsequence $\left(y_{2,m}\right)$ for which the corresponding subsequence of scalars $\beta_2^{(m)}$ converges; let $\beta_2$ denote the limit. Continuing in this way, after $n$ steps we obtain a subsequence $(y_{n,m})=(y_{n,1},y_{n,2},\dots)$ of $(y_m)$ whose terms are of the form $$y_{n,m}=\sum_{j=1}^n\gamma^{(m)}x_j\qquad\left(\sum_{j=1}^n\lvert\gamma_j^{(m)}\rvert=1\right)$$ with scalars $\gamma_j^{(m)}$ satisfyng $\gamma_j^{(m)}\to\beta_j$ as $m\to \infty$ . Hence, as $m\to\infty,$ $$y_{n,m}\to y=\sum_{j=1}^n\beta_jx_j$$ where $\sum_\lvert\beta_j\rvert=1$ , so that not all $\beta_j$ can be zero. Question 2. I didn't understand the highlighted part above, could someone please explain the details to me?","['proof-explanation', 'functional-analysis']"
4652178,Geometric interpretation of integral $-\int\frac{1}{\sqrt{a+2bx-hx^{2}}}dx=\frac{1}{\sqrt{h}}\arccos\frac{b-hx}{\sqrt{b^{2}+ah}}$,"The following formula is given as ""the familiar arc-cosine form"" by Joos, in his Theoretical Physics.  The German language original has $e$ in place of $h$ . $$-\int\frac{1}{\sqrt{a+2bx-hx^{2}}}dx=\frac{1}{\sqrt{h}}\arccos\left(\frac{b-hx}{\sqrt{b^{2}+ah}}\right)$$ The identity is used in the derivation of Kepler's laws.  I have a couple of ways of deriving it.  One of which is included as a screen-scrape. Often such a ""cookbook"" formula is the product of a ""standard recipe"" in which the constants $a,b,h$ or $a,b,e$ have a meaning.  Since this identity involves trigonometry, I am inclined to believe there is some geometric interpretation that would make the expression seem less mysterious.  In particular, I am interested in a method that shows how it related to an ellipse, or a general second degree curve. Is there a compelling geometric interpretation of the above integral formula? All of the following is simply context, to show how the identity us used in treating the Kepler problem.  The derivations are based on what is given by Joos.  My derivation used a unit mass for most of the calculation.  That is, for example, angular momentum becomes $\frac{\mathfrak{L}}{m}:=\vec{\mathcal{L}}.$ One method of producing the formula in question: Read the right-hand side of equation in the lower left box from bottom to top. This is a result of an answer to a question about the same formula asked over six years ago. Deriving the (un)familiar arc-cosine integral identity The setup of the Kepler problem: We assert the central force law of areas (Kepler's second law) as given. $\dagger_1$ Conservation of energy. $\dagger_2$ Central force law of areas. Putting the Kepler problem in the advertised form: Integrating to obtain the expression for an elliptical orbit (Kepler's first law): Obtaining the relationship of time to the semi-major axis (Kepler's third law): Discussion of Quanto's answer(see my ""answer"" I can't get the image to display when added here) The filled region (blue $\cup$ gray) is $$2\int_{-p}^{s}\sqrt{1-\frac{x^{2}}{p^{2}}}dx.$$ The gray triangle is $$s\sqrt{1-\frac{s^{2}}{p^{2}}}.$$ Figuring out how to illustrate the second part may take a bit more time.","['integration', 'visualization', 'inverse-function', 'celestial-mechanics', 'quadratics']"
4652268,"Dice, conditional probability - where am I wrong?","There is a problem which I solved wrong. I know the correct solution from the book, but I cannot find the gap in my reasoning. The problem is: three fair dice are rolled at the same time. What is the probability of getting at least one ""1"" upon condition that at least one die shows a ""6""? The solution in the book is quite complicated, with a result of 30/91. My idea - before checking the book solution - was: let's just throw out the die that rolled a ""6"", now we have two dice and are looking for the probability of getting at least one ""1"" from them. Now this is fairly easy, it's 1-(5/6)^2 = 11/36. WRONG. Yes I know this is wrong. I went as far as write a program that simulated 10,000,000 rollings of three dice, and the relevant percentage worked out very close to 30/91, the result in the book, not my result. But where did I go wrong?","['conditional-probability', 'dice', 'probability']"
4652340,Minimizing $\frac{\|p\|_2^2}{\|p\|_\infty}$ for probability vector $p$,"Suppose $p$ is a discrete probability distribution over $d$ outcomes. I need to minimize the following $$J=\frac{\|p\|_2^2}{\|p\|_\infty}$$ For $d=4$ , Mathematica suggests the minimum is $\frac{2}{3}$ achieved at $\left(\frac{1}{2},\frac{1}{6},\frac{1}{6},\frac{1}{6}\right)$ . Is there a formula for general $d$ ? Notebook Motivation This provides the shape of quadratic $H$ for which a single step of gradient descent with $1/\|H\|$ step size makes the least progress. Hence it gives a bound on progress of gradient descent in general.","['optimization', 'linear-algebra', 'probability', 'upper-lower-bounds']"
4652346,Get solution to $y'=\frac{x-y-3}{x+y}$ that is not limited in domain?,"I need to solve the diffeq $$y'=\frac{x-y-3}{x+y}$$ I proceed as follows. Let $$x(t)=t+\frac32,\qquad \frac{\text{d}x}{\text{d}t} = 1,\qquad \text{d}x=\text{d}t$$ $$y(v(t))=v(t)-\frac32,\qquad \frac{\text{d}y}{\text{d}v} = 1,\qquad \text{d}y=\text{d}v$$ I make the substitution to get $$v' = \frac{t-v}{t+v}$$ Now, let's set $v(t) = t \cdot u(t)$ . This yields $$tu'+u = \frac{1-u}{1+u}$$ when we perform the substitution. The equation is now separable so we integrate to get $$u' = \frac{-u^2 - 2u + 1}{t(u+1)}\Longleftrightarrow \frac{u+1}{-u^2 -2u+1} \cdot u' = \frac{1}{t}$$ $$\implies -\frac12\ln\left(\left|-u^2-2u+1\right|\right) = \ln(|t|) + C$$ I now solve for $u$ , so we have $$\begin{align}
    -\frac12\ln\left(\left|-u^2-2u+1\right|\right) &= \ln(|t|) + C\\\\
    \ln\left(\left|-u^2-2u+1\right|\right) &= -2\ln(|t|) + C\\\\
    \left|-u^2-2u+1\right| &= \exp\left(\ln(|t|^{-2}) + C\right)\\\\
    u^2+2u-1+2 &= 2\pm  C|t|^{-2}\\\\
    (u+1)^2 &= 2\pm C|t|^{-2}\\\\
    u &= -1+\sqrt{2\pm C|t|^{-2}}
\end{align}$$ $$\implies v = -t+t\sqrt{2\pm Ct^{-2}}$$ $$\implies y+\frac32 = -\left(x-\frac32\right)+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}}$$ $$\implies y= -x+\left(x-\frac32\right)\sqrt{2\pm C\left(x-\frac32\right)^{-2}}$$ When I plot this in desmos, this is the graph I get Obviously, this is missing a chunk of the domain as the slope field of the diffeq produces solutions which extend the endpoints here out. Wolfram alpha however, gives the solutions $$y=-x\pm\sqrt{C+x^2+2\left(\frac{x^2}2-3x\right)}$$ As seen here, these solutions successfully capture all of the domain as we vary $C$ . What did I mess up in my solution to arrive at a $y(x)$ that is missing a chunk of the domain? (and how do i get wolfram alpha's solution?) Help is appreciated! :)","['calculus', 'ordinary-differential-equations']"
4652350,"What axioms of set theory actually permit me to recursively define things, with complicated recursion properties","Say we want to prove that every Noetherian space is quasi-compact, and we proceed in the following way: Let $X$ be Noetherian, and let $\underline{U}=\{U_\alpha\}_{\alpha\in\mathcal{A}}$ be an open cover admitting no finite-subcover (for the sake of contradiction). We can recursively approach this as follows: Take $A_1 = \{U\}$ for some $U\in\underline{U}$ . Given $A_n\subseteq \underline{U}$ for which $A_1\subseteq A_2\subseteq\dots \subseteq A_n$ take an element $x\in X\backslash (\cup A_n)$ , i.e. a point in $X$ not covered by $A_n$ (which exists since otherwise there is a finite subcover of $X$ contained in $\underline{U}$ ). Then pick $L\in\underline{U}$ for which $x\in L$ which is possible since $\underline{U}$ covers $X$ (which may use the axiom of specification? namely to define $\underline{U}_x\subseteq \underline{U}$ consisting of the opens in $\underline{U}$ containing $x$ ) then (AXIOM I DON'T KNOW) tells us that the recursively defined $\{A_n\}_{n\in\Bbb N}$ satisfy the property that $\cup A_1\subseteq \cup A_2\subseteq \cup A_3\subseteq \dots$ is a non-terminating increasing sequence of open subsets of $X$ , contradicting the Noetherian condiction. Hence $X$ is quasi-compact. Q. What is the (AXIOM I DON'T KNOW)? If you tell me it is the axiom of choice, tell me how you are recursively defining (presumably a modification of $\underline{U}_x$ ) recursively (without replicating my question lol)? Note : I don't care about the proof of the actual statement , I just want to understand how one can recursively define things, conditionally, in potentially complicated ways, from previously built things in the recursion. No idea what to tag this with, honestly","['elementary-set-theory', 'logic', 'set-theory']"
4652368,Why does a circle appear when we square a polynomial whose inflection points are all on the $x$-axis?,"I challenged myself to find a general formula for an $n$ -degree polynomial with $n-2$ inflection points, all on the $x$ -axis. Here is what I came up with (explanation is at the end). $$\text{Even }n:f_n(x)=1+\sum_{k=1}^{n/2}\left(x^{2k}\prod_{i=1}^k \frac{(2i-2)(2i-3)-n(n-1)}{2i(2i-1)}\right)$$ $$\text{Odd }n: g_n(x)=x+\sum_{k=1}^{(n-1)/2}\left(x^{2k+1}\prod_{i=1}^k \frac{(2i-1)(2i-2)-n(n-1)}{2i(2i+1)}\right)$$ For example, here is the graph of $y=f_8(x)=\frac{1}{5}(429x^8-924x^6+630x^4-140x^2+5)$ . Then I discovered that these polynomials have another interesting property. On a whim, for even $n$ , I drew the graph of $y=f_n(x)|f_n(x)|$ , which is like squaring the function but preserves positive and negative. As $n\to\infty$ , the turning points approach a circle . For odd $n$ , the turning points on the graph of $y=n^2 g_n(x)|g_n(x)|$ (note the $n^2$ ) approach a circle. (Another nice feature is that the turning points appear to be uniformly spaced around the circle.) My question is: Prove that, as $n\to\infty$ , the turning points on $y=f_n(x)|f_n(x)|$ and $y=n^2 g_n(x)|g_n(x)|$ approach a circle. (I think we will see the same phenomenon with any $n$ -degree polynomial whose $n-2$ inflection points are all on the $x$ -axis, where $n$ is large. That is, if we take any such polynomial, and multiply it by its modulus, and apply a certain vertical stretch, then the turning points will be approximately on a circle, uniformly spaced.) Here is how I derived $f_n(x)$ . Assume $f_n(x)=\sum\limits_{k=0}^n a_k x^k$ with leftmost and rightmost roots at $x=\pm1$ , and $a_0=1, a_1=0$ . To ensure that the inflection points are all on the $x$ -axis, let $f_n(x)=(x-1)(x+1)\frac{{f_n}''(x)}{n(n-1)}$ . Equate coefficients, then the above expression for $f_n(x)$ follows. The same method can be used to derive $g_n(x)$ , except we assume $a_0=0$ and $a_1=1$ . Possibly related: question about an $(n+1)$ -degree polynomial that is tangent to a circle at $n$ points. EDIT My derivation of $f_n(x)$ and $g_n(x)$ is incomplete, because I have not shown that they each have $n-2$ inflection points. I do not know how to show this.","['calculus', 'derivatives', 'circles', 'real-analysis']"
4652397,"Weak convergence in $W^{1,2}$ implies strong convergence under extra condition?","Suppose $U \subset \mathbb{R}^d$ is a smooth bounded domain. Let $(u_n)_{n\geq 1}$ be a sequence in the Sobolev space $W^{1,2}(U)$ such that it weakly converges to zero: $u_n \rightharpoonup 0$ as $n \to \infty$ . Also assume that for any bounded sequence of test functions $$(\phi_n)_{n\geq 1} \subset W^{1,2}_0(U)$$ we have the convergence $$\int_U \nabla u_n(x) \cdot \nabla \phi_n(x) dx \to 0$$ Can I conclude that $\nabla u_n \to 0$ in $L^2(U)$ ? Clearly I cannot take that test function $\phi_n = u_n$ because of the zero boundary condition.","['sobolev-spaces', 'functional-analysis', 'weak-convergence']"
4652457,Finding $\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2}$. [duplicate],"This question already has answers here : Limit of exponential and logarithm function (4 answers) Closed last year . I tried using using the logarithm, \begin{align}
L &=\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2},\\
\ln L
&= \ln \lim_{x \to \infty} (1 - \frac{e}{x})^{x^2}\\
&= \lim_{x \to \infty} \ln (1 - \frac{e}{x})^{x^2} \\
&= \lim_{x \to \infty} x^2 \ln (1 - \frac{e}{x})\\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1 - \frac{e}{x})\\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\
&= \lim_{x \to \infty} x^2 \cdot \lim_{x \to \infty} \ln (1) \\
&= \lim_{x \to \infty} x^2 \cdot 0 .\\
&= \infty \cdot 0
\end{align} but get an indeterminate expression at the end. Then, after some research, I found out about Euler's number as a limit , that is, $$
e = \lim_{n \to 0} (1 + n)^{\frac{1}{n}} = \lim_{n \to \infty} (1 + \frac{1}{n})^n
$$ and tried using both identities to see if one would work. However, after a substantial amount of working, I realised as the given limit had a power of $x$ , there would always be some ""residue"" left: Using the left-most identity: $$
\text{Let $n = -\frac{e}{x}$, hence $x = -\frac{e}{n}$ and $x^2 = \frac{e^2}{n^2}$. So, } \\
\lim_{x \to \infty} (1 - \frac{e}{x})^{x^2} = \lim_{x \to \infty} (1 + n)^{\frac{e^2}.{n^2}}\\
\text{Since $n = -\frac{e}{x}$, as $x \to \infty$, $n \to 0$. So, }\\
\lim_{x \to \infty} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}}.\\
\lim_{n \to 0} (1 + n)^{\frac{e^2}{n^2}} = \lim_{n \to 0} (1 + n)^{\frac{1}{n}\frac{e^2}{n}}\\
= e^{\frac{e^2}{n}}
$$ If you notice, there's a ""residue"" variable $n$ left in the denominator of the exponent. Using the other identity yields the same result: there's a residue variable left in the result. I have exhausted every technique I know and tried researching for similar questions, but none that use Euler's identities have the variable in the exponent raised to any power. They all have a multiple of $x$ , e.g. $10x$ , or some linear expression $x + c$ , e.g., $7x + 8$ , allowing the end result to be some constant. Any sort of help or pointing to some direction would be greatly appreciated.","['limits', 'calculus']"
4652485,"Minimum ""egg"" curve circumscribing a given rectangle","Construct a circle centred at $O$ , with diameter $AB$ and call its radius $r$ . Construct the perpendicular bisector of $AB$ , intersecting the circle at point $C$ . Draw rays $AC$ and $BC$ which extend some way beyond $C$ . Draw an arc with radius length $AB$ , hence twice the radius of the circle $(2r)$ , centred on $B$ and starting at $A$ and ending at the ray $BC$ . Draw a similar arc, centred on $A$ and starting at $B$ and ending at the ray $AC$ . Draw an arc centred on $C$ and completing the egg curve by connecting the arcs of radius $2r$ . This is the basic egg curve I am using. Now draw a rectangle inside the egg so that its corners touch the egg at $PQRS$ (labelling starts at upper right corner). The sides $RS$ and $PQ$ intersect with $AB$ at $M$ and $N$ respectively. The angle $MOR$ is $\theta$ and $PAN$ is $\lambda$ . $PB$ is now a chord of the circle radius $2r$ centred at $A$ , and $RQ$ is a chord of the circle radius $r$ centred at $O$ . NOTE: In the diagram, $X$ should be the length of $AN$ not $MB$ , but still equivalent. My question is, for a rectangle of width $55$ mm and height $68$ mm, what is the minimum radius $r$ of the original circle such that the egg circumscribes the rectangle. I have written some equations based on the trigonometry of triangles $ANP$ and $OMR$ $$X = 2r \cos λ$$ $$Y = 2r \sin λ$$ $$x = r \cos θ$$ $$y = r \sin θ$$ by definition $$Y + y = 68$$ $$X - x = r$$ $$2x = 55$$ chords $PB$ (radius $2r$ ) and $RQ$ (radius $r$ ) $$55 = 2r \sin(90-θ)$$ $$68 = 4r \sin(λ/2)$$ applying the cosine rule to triangles $PAQ$ and $OAQ$ , giving $2 $ expressions for $AQ^2$ , equated and simplified $$2r^2 + 4624 - 272 \sin λ = 2r^2\cos θ$$ I have tried in vain to solve this with no joy.","['optimization', 'trigonometry', 'systems-of-equations', 'geometry']"
4652524,Why are $p$-adic numbers ubiquitous in modern number theory?,"I'm currently at a stage where I think I'm quite comfortable with the appearance of local non-archimedean fields in the maths I encounter, having seen a fair bit of technology built upon their structure and applications to connected areas, yet I somehow still feel like I have an unsatisfying understanding of why their introduction is absolutely crucial to study algebraic number theory and arithmetic geometry, especially since all these applications are hugely advanced compared to the point at which $p$ -adic numbers are usually introduced in a student's career I think. If I want to motivate their definition, I sort of naturally go through the following implications in my head: to study a problem over the ring of integers $\mathbb{Z}$ (or more generally, the ring of integers of a number field) the strategy is to localise the problem at a prime $p$ , as to focus around it and worry about the problem ' one point at a time ', so-to-speak; we (literally, now) localise at the prime $p$ , just as one would do with the ring of regular functions on a variety, and replace $\mathbb{Z}$ with the ring $\mathbb{Z}_{(p)} := \{ \frac{x}{y} \in \mathbb{Q} \mid p\nmid y \}$ ; we can then complete at the maximal ideal $(p) \subseteq \mathbb{Z}_{(p)}$ to obtain the $p$ -adic integers $\mathbb{Z}_p$ , with the benefit that now there's access to approximation techniques such as Hensel's lemma, and the newly obtained ring has pretty much all the same algebraic properties as $\mathbb{Z}_{(p)}$ because of its identical valuation theory. It's this last step which still confuses me... even though I can appreciate the utility of approximation techniques, it still feels extremely arbitrary why it turns out to be so inevitable with all the theory that builds upon this measly little step. Somehow, every time I've seen the study of formal neighbourhoods in algebraic geometry it always feels like it's a tool to tackle a problem, and not the main object of study, whereas in my head the $p$ -adic numbers have turned out to be the main character in many areas of mathematics, which sort-of goes against this intuition of mine I find. Since this intuition really doesn't come from any of my teachers and I'm not quite sure it makes much sense, I wanted to ask if it's an apt way to think about the use of non-archimedean fields in mathematics; I'd also be very interested in learning about their role in the history of algebraic number theory since I'm not quite sure I can properly place their use and introduction on a timeline in a coherent way with all of the theory I have in mind. I apologise if my question is very hand-wavy, and I'd be super grateful for any sort of insight :) Thank you very much for your time!!","['algebraic-number-theory', 'p-adic-number-theory', 'motivation', 'algebraic-geometry', 'big-picture']"
4652634,How many triangles can be formed so that the area of $\triangle ABC$ is 9 times $\triangle DEF$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I am currently working on an Olympiad math problem, and I am struggling to find a solution. I would greatly appreciate your help in solving this problem. I was unable to solve the problem because I don't know the exact rules for solving the problem. Problem Source : Bangladesh Math Olympiad A small hint will be enough for me to proceed. In a triangle, $\triangle ABC$ , $\angle B=90^\circ$ , and every side has a positive integer-valued length. $△DEF$ is inside of $\triangle ABC$ in such a way that $AB\parallel DE$ , $BC\parallel EF$ , $AC\parallel DF$ , and
the distance between the parallel sides of the triangles are always $2$ . How many triangles can be formed so that the area of $\triangle ABC$ is $9$ times $\triangle DEF$ ?","['contest-math', 'trigonometry', 'geometry', 'triangle-inequality']"
4652658,How to show that $\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1$,"I am trying to show the stability of the Crank-Nicholson schema for the heat equation in 2 spatial dimensions, using the Von Neumann's stability analysis. After doing the algebra I was left with showing that $\mid{ \frac{1+\lambda(\cos q_{m}\Delta y+ \cos k_{m}\Delta x -2)}{1-\lambda(\cos q_{m}\Delta y +\cos k_{m}\Delta x -2)}\mid} \leq 1$ . Note that $\lambda>0$ . My reasoning. Consider the function $f(x, y)= \cos k_{m}\Delta x+\sin q_{m}\Delta y$ , its critical points are $(0,0)$ and $(\pi, \pi)$ . Note that $(0, 0)$ minimizes the denominator. Evaluating the numerator at $(0,0)$ the expression reduces to $1\leq 1$ and evaluating at $(\pi, \pi)$ it simplifies to $\mid 1- 4\lambda\mid \leq 1$ . I was wondering if you could provide some feedback on this. Is there a more rigorous way to go about it? Thanks in advance!","['multivariable-calculus', 'numerical-methods', 'partial-differential-equations']"
4652668,Showing a level curve is not $C^1$.,"Define $F(x,y) := x^3 - y^2$ and $C$ is the curve $F(x,y) = 0$ . Now when you graph this, we can see its clearly not $C^1$ , as there is a cusp at the origin (meaning not differentiable at origin). We can define $x$ globally as a function of $y$ , but according to Implicit Function Theorem, $F_x(0,0) = 0$ , so we can't locally define $x$ as a function of $y$ . The question is essentially asking if this is a contradiction of the Implicit Function Theorem and I believe it's not since $x^3 - y^2 = 0$ is not $C^1$ , so the assumptions of IFT are not met. The partials are $(3x^2, -2y)$ which exist and are continuous everywhere, which imply that it is $C^1$ , but this is clearly not the case. How do I go about showing the curve $x^3-y^2=0$ is not $C^1$ ?","['calculus', 'implicit-function-theorem', 'differential-geometry']"
4652710,Rudin's RCA Theorem $4.18$.,"There is the definition which we need for the proof: There is the theorem which we need for the $4.18$ : There is $4.18$ : Let { $u_\alpha : \alpha$ $\in$ $A$ } be an orthonormal set in $H$ . Each of the following four conditions on { $u_\alpha$ } implies the other three: $(1)$ ${u_\alpha}$ is a maximal orthonormal set in $H$ . $(2)$ The set $P$ of all finite linear combinations of members of { $u_\alpha$ } is dense in $H$ . $(3)$ The equality $\sum_{\alpha \in A}$ $|\hat x(\alpha)|^2$ $=$ $||x||^2$ holds for every $x$ $\in$ $H$ . $(4)$ The equality $\sum_{\alpha \in A}$ $\hat x(\alpha)$ $\overline {\hat y(\alpha)}$ $=$ $(x,y)$ holds for all $x$ . $\in$ $H$ and $y$ $\in$ $H$ . We shall prove that $(1)$ $\to$ $(2)$ $\to$ $(3)$ $\to$ $(4)$ $\to$ $(1)$ . I understand that $(1)$ $\to$ $(2)$ . In the book it's written that if $(2)$ holds, so does $(3)$ by Theorem $4.17$ . I don't understand how does $(2)$ imply $(3)$ by Theorem $4.17$ ? Also there's written that the implication $(3)$ $\to$ $(4)$ follows from the easily proved Hilbert space identity $4(x,y)$ $=$ $||x+y||^2$ $-$ $||x-y||^2 $ $+$ $i||x+iy||^2$ $-$ $i||x-iy||^2$ . How does it follow from this identity? Any help would be appreciated.","['combinations', 'orthonormal', 'analysis', 'hilbert-spaces', 'dense-subspaces']"
4652741,Prove that probability of winning stays the same,"Players $A$ and $B$ play tennis. When $A$ draws he wins the draw with probability $p_1$ , when $B$ draws he wins the draw with probability $p_2$ . Player $A$ draws first and the game is won by a player who reaches $n$ successful draws first. There are two ways to determine who draws next: Switch after every draw (i.e. $ABABAB...$ ) Switch at the moment the current drawer loses his draw (i.e. if $A$ has 5 successful draws and then fails, then B has 3 successful draws then fails we'll get $AAAAABBBA...$ ) Prove that probability of player $A$ winning is the same no matter which rule of switching drawing players is adopted. My approach : I got that for the first scheme the probability of $A$ winning is $P(A) = \sum_{N=n}^{\infty} [p_1 \cdot \binom{N-1}{n-1} p_{1}^{n-1} q_{1}^{N-n} \sum_{k=0}^{n-1} \binom{N-1}{k} p_{2}^k q_{2}^{N-k-1} ]$ , where $N$ is all possible draws at which $A$ won and $k$ runs through amount of successful draws for $B$ . I am struggling to come up with formula for $P(A)$ for the second scheme.","['discrete-mathematics', 'combinatorics', 'probability-theory', 'probability']"
4652758,Geometric interpretation of the isomorphism $\mathcal{N}_{Y/X} \cong \mathcal{O}_X(Y) \vert_Y$,"Let $X$ be a smooth variety / manifold over $\mathbb{C}$ of dimension $n$ and suppose that $Y \subset X$ is a
smooth $n-1$ -dimensional subvariety. The normal bundle $\mathcal{N}_{Y/X}$ comes from exact sequence $$  0 \to \mathcal{T}_Y \to  \mathcal{T}_X \vert_{Y} \to  \mathcal{N}_{Y/X} \to 0 $$ and it is known that as sheaf $\mathcal{N}_{Y/X}$ is isomorphic to the restriction of the invertible sheaf $\mathcal{O}_X(Y)$ to $Y$ . That's more or less a consequence of identification of the conormal bundle $\mathcal{N}_{Y/X}^*$ with the locally
free sheaf $\mathcal{I}_{Y/X}/\mathcal{I}_{Y/X}^2 = \mathcal{I}_{Y/X} \otimes \mathcal{O}_Y$ where $\mathcal{I}_{Y/X}$ is the ideal sheaf of $Y$ . For a proof, see e.g. 1.4.2 in the book 3264 and All That by Eisenbud & Harris. Even though the proof is formally clear I'm missing the clear geometric picture of the isomorphism $\mathcal{N}_{Y/X} \cong \mathcal{O}_X(Y) \vert_Y$ and how to think about it as map of vecor bundles. Does this isomorphism have a concrete geometric interpreteation or does it exist only on abstract level? Is it possible to write down an explicit isomorphism $\mathcal{N}_{Y/X} \to \mathcal{O}_X(Y) \vert_Y$ in terms of classical bundle map if we use the correspondence priciple and treat locally free sheaves as vector bundles? The latter means that we can associate to the free sheaf $\mathcal{O}_X^n $ the trivial bundle $X \times \mathbb{C}^n$ and to the local sections $\mathcal{O}_X^n(U) $ to sections $U \to X \times \mathbb{C}^n$ which correspond to polynomial maps $U \to  \mathbb{C}^n$ . Here a guess: we can find a global section $s: X \to \mathcal{O}_X(Y)$ such that at every local trivialisation $U \subset X$ with $\mathcal{O}_X(Y) \vert _U= U \times \mathbb{C}$ the restricted section $s \vert_U $ equals $u \mapsto (u, \tilde{s}(u))$ with $\tilde{s}(u)=0$ iff $u \in U \cap Y$ . The tangent bundle $\mathcal{T}_X $ restricted to $U$ can be identified with $ U \times (\bigoplus_{i=1}^n \mathbb{C} \cdot \frac{\partial}{\partial t_i})$ . Then it looks reasonable to try to define it locally at $U \cap Y$ as the map $$\mathcal{T}_X \vert _{U \cap Y} \to (U \cap Y) \times \mathbb{C}, \ \
\frac{\partial}{\partial t_i} \mapsto ds(\frac{\partial}{\partial t_i})$$ where $ds$ is differential of $s: U \to \mathbb{C}$ and $\frac{\partial}{\partial t_i} \in \mathcal{T}_X \vert _{U \cap Y} = (U \cap Y) \times (\bigoplus_{i=1}^n \mathbb{C} \cdot \frac{\partial}{\partial t_i})$ .
By the choice of $s$ the kernel of this map is exactly $ \mathcal{T}_Y \vert _U= \mathcal{T}_{Y \cap U}$ . Does this local map glue to global map $\mathcal{T}_X \vert _{Y} \to \mathcal{O}_X(Y) \vert _Y$ ? Also, is this the most natural way to ""capture the geometric picture"" behind the isomorphism $\mathcal{N}_{Y/X} \cong \mathcal{O}_X(Y) \vert_Y$ or is there a more ""natural"" way to think about it?","['vector-bundles', 'algebraic-geometry', 'line-bundles', 'differential-geometry']"
4652794,Alexandrov topology in terms of category theory,"An Alexandrov topology is a topology in which an arbitrary intersection of open sets is open.  The wikipedia article has some characterizations in terms of category theory: Finite inclusion map : The inclusion maps $f_i:X_i\to X$ of the finite subspaces of $X$ form a final sink . Finite generation : $X$ is finitely generated i.e. it is in the final hull of the finite spaces. What does final sink and final hull mean?  Can you suggest any references about this topic? (And does the second bullet mean the final hull of its final subspaces, or the final hull of finite spaces in general, whatever final hull is supposed to mean?)","['general-topology', 'category-theory']"
4652814,How to use $\uparrow$ to define an explicit bijective mapping $f:\varepsilon_{1}\rightarrow\mathbb{N}$?,"The map $f:\varepsilon_{1}\rightarrow\mathbb{N}$ which I am trying to define has to send $\varepsilon_{0}$ to some natural number. Since $\varepsilon_{0}=\omega\uparrow^{2}\omega$ , a potential candidate is $f(\varepsilon_{0})=P_{f(f(\omega))}=P_{f(3)}=P_{8}=19$ (from the calculations for small ordinals below). More generally, $f((\omega\uparrow^{n}\alpha)\cdot k) = P_{f^{n}(\alpha)}^{k}$ , where $n,k$ are finite, $f^{n}$ is simply $f$ composed with itself $n$ times, and $P_{i}$ denotes the $i$ -th prime. Some small ordinals will be mapped as follows: $f(0)=1$ , $f(1)=f(\omega\uparrow^{1}0)=P_{f(0)}=P_{1}=2$ , and more generally, $f(m)=f((\omega\uparrow^{1}0)\cdot m)=2^{m}$ , $m\ge 1$ . Finally, $f(\omega) = f(\omega\uparrow^{1}1)=P_{f(1)}=P_{2}=3$ . Am I going in the right direction? UPDATE: Something bothers me: $f$ may not well defined because the $\uparrow$ does not produce a unique notation. For example, $\omega=\omega\uparrow^{1} 1 = \omega\uparrow^{2} 1$ . Is there a way to avoid this pitfall? Perhaps, when an ordinal $\alpha$ can be denoted by several expressions, we take the smallest of the resulting numbers after applying $f$ to each of the expressions denoting $\alpha$ ? For example, $f(\omega)=f(\omega\uparrow^{1}1)$ . Is this a correct argument to get out of the problem? UPDATE: I am not sure I am using $\uparrow$ correctly: for example $\omega\uparrow^{1}\omega = \omega^{\omega}$ . Then $\omega\uparrow^{2}\omega = \underbrace{\omega\uparrow^{1}(\omega\uparrow^{1}(\omega\uparrow^{1}\omega\cdots}_{\omega}=\underbrace{\omega^{\omega^{\omega^{...}}}}_{\omega}=\varepsilon_{0}$ . But, what is $\omega\uparrow^{3}\omega$ ? $\omega\uparrow^{3}\omega=\underbrace{\omega\uparrow^{2}(\omega\uparrow^{2}(\omega\uparrow^{2}\omega\cdots}_{\omega}=\underbrace{\omega\uparrow^{1}(\omega\uparrow^{1}(\omega\uparrow^{1}\omega\cdots}_{\omega\uparrow^{2}\omega}\stackrel{?}{=}\underbrace{\omega^{\omega^{\omega^{...}}}}_{\varepsilon_{0}}=\;?$ This seems bigger than $\varepsilon_{1}$ ...In fact I am beginning to think it depends on how $\uparrow$ is defined, and there may be several ways to do so.","['elementary-set-theory', 'hyperoperation', 'ordinals', 'tetration']"
4652817,My contour integrations seem to contradict the residue theorem.,"I have been studying complex integration and I have been calculating some simple contour integrals. By integrating the function $f(z) = \frac{1}{z}$ around the unit circle centered on the origin I get $\int_{C} = 2\pi i$ as expected by the residue theorem. I then repeated using a different countour, a circle of radius 1 centered on $z=3$ , $g(t) = 3 + e^{it}$ where $0 < t < 2\pi$ where I get an integral of zero, again as predicted by the residue theorem. (The singularity at $z=0$ is not inside this contour integral). I then tried to integrate around the circle with center $z=0$ and radius 4. I was expecting/hoping to get the value $2\pi i$ since the function $f(z) = \frac{1}{z}$ has a single singularity included inside it's contour. Although this is probably the most simple application of the residue theorem that anyone applies when first studying the topic, however, I get an integral of zero. Surely the residue theorem would dictate a value of $2\pi i$ around this countour as it includes the one and only singularity at $z = 0$ ? I expect that I am misunderstanding some subtlety of the logarithm function. I have included my working below: $$f(z) = \frac{1}{z}$$ $$g(t) = 3 + 4e^{it} \quad 0 < t < 2\pi$$ $$\int_{g}f(g(t))g'(t)dt = \int_{0}^{2\pi}\dfrac{4ie^{it}}{3+4e^{it}}dt = \ln(3+4e^{2\pi i}) - \ln(3 + 4e^{0}) = \ln(7) - \ln(7) = 0$$","['complex-analysis', 'complex-integration', 'residue-calculus', 'logarithms']"
4652851,How does the integral $\int y''\cdot y'\text{ d}x$ work in the context of differential equations?,"In this question, the accepted answer starts off solving the diffeq $(y'/y)'=-ay$ by substituting $u = \log y$ to get $u'' = -a e^{u}$ and then multiplying both sides with $u'$ and integrating both sides to get $$\frac{(u')^{2}}{2} = -a e^{u} + \frac{c_{1}}{2}$$ I am baffled at how to get $$\int u'' \cdot u' \text{ d}x=\frac{(u')^{2}}{2}$$ through the context of differentials. Typically, I can understand integrating say $u'(x)$ through differentials, where we have $$\int \frac{du}{dx}\text{ d}x = \int \text{ d}u = u+C$$ How do I get the same thing for integrating $u'' u'$ ? Obviously, I can see this through the chain rule where $$\frac{d}{dx}\left[\frac12 y'^2\right] = \frac12\cdot 2y'\cdot y'' = y'' y'$$ but I cannot reason how how the integral would work $$\int \frac{d^2u}{dx^2}\cdot \frac{du}{dx}\text{ d}x = \int \frac{d^2u}{dx^2} \text{ d}u?????$$ Help is appreciated! :)","['integration', 'calculus', 'derivatives', 'ordinary-differential-equations']"
4652894,The Probability of Getting a Repeated Digit in a Random 4-Digit PIN Code,"I just wanted to check if my thinking is correct regarding a probability problem. The problem is as follows: You are randomly selecting a 4-digit PIN code. What is the probability that at least 2 of the digits are the same? My approach was to use the complement event, which means finding the probability that all 4 digits are unique. The number of possible combinations where all 4 digits are unique is: 10 * 9 * 8 * 7 = 5040 And since there are 10,000 possible combinations in total (10 options for each digit), the probability of getting a PIN code with all unique digits is: 5040/10000 = 0.504 Therefore, the probability of getting a PIN code with at least 2 of the digits being the same is: 1 - 0.504 = 0.496 or approximately 49.6% Could someone please confirm if my thinking and calculation are correct? Thank you in advance!","['discrete-mathematics', 'statistics', 'combinatorics', 'probability']"
4652914,Random walks on regular trees,"An infinite $d$ -regular tree is a graph where every vertex has degree $d$ and
there are no cycles. Suppose we define a standard random walk $X_n$ on such a tree. Is the claim that $X_n$ is transient for $d \ge 3$ true? If so, how can I prove that? I do know for a fact that random walks are transient on $\mathbb{Z}^d$ with $d \ge 3$ . Can we use this somehow to prove the claim?","['trees', 'random-walk', 'graph-theory', 'markov-chains', 'probability']"
4652935,"How do I translate ""is not sufficient"" into symbolic logic?","Take the proposition ""it is not sufficient for the monkey to dance in order for me to get an A on the test"" m = the monkey dances a = I get an A on the test It makes sense why I can translate the statement ""it IS sufficient for the monkey to dance in order for me to get an A on the test"" to m → a , because if m and a are both true the proposition is true, and if m is false I can understand why the proposition is vacuously true. Where my understanding falls apart is when you negate the statement: m a m → a ¬(m → a) T T T F T F F T F T T F F F T F It makes sense why ¬(m → a) is equivalent to ""it is not sufficient for the monkey to dance in order for me to get an A on the test"", because it is a negation of m → a . But it doesn't make sense that m and a both being true makes ""it is not sufficient for the monkey to dance in order for me to get an A on the test"" false. If the monkey dances, and I get an A on the test, wouldn't that make this proposition vacuously true? This proposition claims that the monkey dancing isn't sufficient, but just because both happen, it doesn't contradict the proposition. Is it just ""vacuously false""?","['propositional-calculus', 'logic', 'discrete-mathematics', 'logic-translation']"
4653029,On the limit defined by $A + B(A + B(A + B (A + B(\cdots))))$,"Suppose $A$ and $B$ are some constant ( $A,B\in\mathbb{R}$ ) Is there a simple expression for $x$ , where $x$ is: $$
x=A+B[A+B[A+B[\cdots]]]]
$$ ""..."" indicates the pattern repeats forever. In other words, it is recursive: $$
x=A+Bx
$$ It is very tempting to just do: $$
x=\frac{A}{1-B}
$$ However, is this correct? I don't think is that simple. For context, I come from an engineering background and my math isn't as rigorous. Please enlighten me. Does this wiki resembles this kind of series? Edit 1: I don't think the expression for $x$ is simple, because: $$
x\neq \frac{A}{1-B}
$$","['limits', 'telescopic-series', 'recurrence-relations', 'sequences-and-series']"
4653042,"Proof of Variation of Parameters for Linear, 2nd Order ODE","For the Linear, 2nd Order ODE $$\frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{,}$$ I've been asked to show that the equation's particular solution can be written $$y_p(x) = y_2(x)\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - y_1(x)\int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds\text{.}$$ It was given to start with $y_p(x) = y_1(x)v(x)$ and develop a 1st order ODE for $v'(x)$ . I started out by computing the 1st and 2nd order derivatives of $y_p(x)$ and substituting that into $\frac{d^2y}{dx^2} + P(x)\frac{dy}{dx} + Q(x)y = F(x)\text{.}$ This yields $$y_1\frac{d^2v}{dx^2} + 2\frac{dy_1}{dx}\frac{dv}{dx} + \frac{d^2y_1}{dx^2}v + P(x)y_1\frac{dv}{dx} + P(x)\frac{dy_1}{dx}v + Q(x)y_1v = F(x)\text{.}$$ I then multiplied both sides by $y_1$ to get $$y_1^2\frac{d^2v}{dx} + 2\frac{dy_1}{dx}\frac{dv}{dx}y_1 +vy_1\left[\frac{d^2y_1}{dx} + P(x)\frac{dy_1}{dx} + Q(x)y_1\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.}$$ Since $y_1$ is a solution of the homogeneous equation $\left(Ly=0\right)$ , the expression inside the brackets vanishes, and what's left can be written as $$\frac{d}{dx}\left[y_1^2\frac{dv}{dx}\right] + P(x)y_1^2\frac{dv}{dx} = y_1F(x)\text{.}$$ Now I multiplied both sides by the integrating factor $e^{\int^{x}P(s)ds}$ , leading to $$\frac{d}{dx}\left[y_1^2\frac{dv}{dx}e^{\int^{x}P(s)ds}\right] = y_1F(x)e^{\int^{x}P(s)ds}\text{.}$$ Notice that $e^{\int^{x}P(s)ds} = \frac{1}{W(y_1(x),y_2(x))}\text{,}$ so the equation becomes $$\frac{d}{dx}\left[\frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx}\right] = \frac{y_1F(x)}{W(y_1(x),y_2(x))}$$ Integrating both sides: $$\frac{y_1^2}{W(y_1(x),y_2(x))}\frac{dv}{dx} = \int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds$$ or $$\frac{dv}{dx} = \frac{W(y_1(x),y_2(x))}{y_1^2}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds = \frac{d}{dx}\left[\frac{y_2}{y_1}\right]\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds$$ This is beginning to look very similar to $y_p(x)$ , but after integrating, I would need the equation to be $$v = \frac{y_2(x)}{y_1(x)}\int^{x}\frac{y_1(s)F(s)}{W(y_1(s),y_2(s))}ds - \int^{x}\frac{y_2(s)F(s)}{W(y_1(s),y_2(s))}ds$$ How do I make this leap? What am I missing?",['ordinary-differential-equations']
4653048,"Find all real functions that satisfy: $f(x+y) = (f(x))^2 + (f(y))^2$, $x$, $y$ $\in$ $\mathbb{R}$","I am trying to find all real functions that satisfy the property: $f(x+y) = (f(x))^2 + (f(y))^2$ , $x$ , $y$ real numbers. I tried to substitute $x$ and $y$ with $0$ but end up with nothing, then I tried substituting $y$ with $x$ and I get $f(2x) = 2(f(x))^2$ , then I tried to prove that $f(0) = 0$ and I think I am missing an important step. How can I proceed from here? If it isn't the wrong already.","['real-numbers', 'functional-equations', 'functions']"
4653098,Generalisation of Baker–Campbell–Hausdorff (Matrix exponential simplification),"Let there be some matrix exponential function for matrices of the form $$M=\sum_{i=1}^p c_i A_i$$ where $M,A_i\in\mathbb{R}^{n\times n}$ are given, real square matrices and $c_i \in \mathbb{R}$ . In other words, we are interested in the matrix exponential of the following form: $$
f(M)=e^M=e^{\sum c_i A_i}.
$$ The matrix exponent is thus expressible in some finite real basis for the subspace we are interested in, as $M \in \mbox{Span}(A_i)$ . Does an approximation (or even identity) exist for the case in which this basis of matrices $A_i$ and their commutators is known? Concretely, I currently have a basis of less than 10 matrices (i.e., $p<10$ ), if this makes the problem easier. This problem arises as part of an optimization problem in which the $A_i$ are fixed (some chosen basis) and the $c_i$ are variable.","['matrices', 'matrix-rank', 'matrix-calculus', 'matrix-exponential']"
4653130,"Convergence of the positive operator $ \big( A+ (\lambda)^n B\big)^{\frac{1}{n}},$ where $0< \lambda <1.$","Let $A, B$ be two positive operators in $M_{n}(\mathbb{C}).$ We know that $\lim_{n\to \infty}A^{\frac{1}{n}}$ converges and $\lim_{n\to \infty}A^{\frac{1}{n}}= P_{\rm ran(A)},$ where $P_{\rm ran(A)}$ denote the projection into range of $A.$ I am interested to know the limit of the sequence $$\lim_{n\to \infty} \big( A+ (\lambda)^n B\big)^{\frac{1}{n}},$$ where $0< \lambda <1.$ Some simple case, we can conclude that the above limit exists. However, I do not know in full generality.
Any suggestions and input will be highly appreciated.","['matrices', 'monotone-functions', 'soft-question', 'functional-analysis']"
4653170,Can some please help me in this step? Differentiation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question ​""I can't understand how they differentiated from step $(1)$ to step $(2)$ "" So wave function is: $$ \varphi_v(x)=\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2}$$ To finally calculate value of most probable displacement \begin{align}
\frac{d\varphi}{dx} &= 0 \\
\frac{d}{dx}\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2y)e^{-y^2/2} &= 0 \\
\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}(2)\frac{d}{dx}(y)e^{-y^2/2} &= 0 \tag{1}\\
\left(\frac{1}{2\alpha(\pi)^{1/2}}\right)^{1/2}e^{-y^2/2}(1-y^2) &=0 \tag{2}\\
\end{align} So: \begin{align}
(1-y^2) &=0 \\
y^2 &= 1 \\
\left(\frac {x}{\alpha}\right) &= 1\\
x &= \pm \alpha
\end{align} My question Edited Full question","['calculus', 'derivatives']"
4653269,Can we differentiate a function w.r.t itself?,"A question came to my mind which is: is it possible to differentiate a function with respect to itself? I tried to think of how it will work and came to this. I begin with \begin{gather} f(x)=f(x). \end{gather} It's kind of silly but bear with me. Then I've applied the definition \begin{gather}\lim_{f(x) \to f(a)} \frac{f(x)-f(a)}{f(x)-f(a)}=\lim_{f(x) \to f(a)} \frac{f(x)-f(a)}{f(x)-f(a)}\end{gather} the L.H.S will be equal to $1$ . Then, \begin{gather}1=\lim_{f(x) \to f(a)} \frac{f(x)-f(a)}{f(x)-f(a)}.\end{gather} The trick I've made is: as ${f(x) \to f(a)} \equiv x\to a$ . Then I rewrote the expression \begin{gather} \lim_{x \to a} \frac{f(x)-f(a)}{f(x)-(a)} =\lim_{f(x) \to f(a)} \frac{f(x)-f(a)}{f(x)-f(a)},\end{gather} I multiplied by $x-a$ up and down, \begin{gather} \lim_{x \to a} \frac{f(x)-f(a)}{x-a}.\frac{x-a}{f(x)-f(a)},\end{gather} and I end up with this \begin{gather} 1=f'(a) \frac{dx}{df(x)}.\end{gather} My question is: is it meaningful to have $\frac{dx}{df(x)}$ ? The independent variable is $x$ , so how could we say that we measure the change of $x$ w.r.t. $f(x)$ ? My second question: is it possible to apply the chain rule from the beginning? because I know that we can apply chain rule for a function of some variable, like $f(x)$ for example. But could we apply the chain rule for an independent variable of this function, like $x$ is the independent variable of $f(x)$ . Could we say from the beginning $1=f'(x)\frac{dx}{d(f(x))}$ , and why?","['calculus', 'functions', 'derivatives', 'chain-rule']"
4653309,Why do I get two different answers while doing this simple limit math?,"Problem: $$\lim_{n\to\infty} \frac{1}{n^3}\cdot\frac{n(n+1)(2n+1)}{6}$$ Attempt 1: $$\lim_{n\to\infty} \frac{1}{n^3}\cdot\frac{n(n+1)(2n+1)}{6}$$ $$=\lim_{n\to\infty} \frac{(n+1)(2n+1)}{6n^2}$$ $$=\lim_{n\to\infty} \frac{n+1}{6n}\cdot\frac{2n+1}{n}$$ $$=\lim_{n\to\infty} \left(\frac{1}{6}+\frac{1}{6n}\right)\cdot\left(2+\frac{1}{n}\right)$$ $$=\left(\frac{1}{6}+0\right)\cdot\left(2+0\right)$$ $$=\frac{1}{6}\cdot2$$ $$=\frac{1}{3}$$ Attempt 2: $$\lim_{n\to\infty} \frac{1}{n^3}\cdot\frac{n(n+1)(2n+1)}{6}$$ $$=\lim_{n\to\infty} \frac{1}{n^3}\cdot\lim_{n\to\infty}\frac{n(n+1)(2n+1)}{6}$$ $$=0\cdot\lim_{n\to\infty}\frac{n(n+1)(2n+1)}{6}$$ $$=0$$ I know that attempt 1 is the correct attempt; however, I'm unsure as to why attempt 2 is not working. Why is attempt 2 not working?","['limits', 'solution-verification', 'limits-without-lhopital']"
4653311,Dot product between two vectors transformed by orthogonal matrices,"I am reading through the ""Matrix Transformations"" chapter of this book and more specifically on Orthogonal Matrices. I understand their properties and understand that multiplying vectors by them is an affine transformation, so the dot product between two vectors will be the same as between the same vectors multiplied by the same orthogonal matrix $\mathbf{M}$ . However this proof from the book confuses me to no end: And more specifically this: $$\mathbf{(Ma)\cdot (Mb)=(Ma)^T (Mb)}$$ What exactly is happening on the right side of the equation? Is the transpose of $\mathbf{Ma}$ somehow the reciprocal of the dot operation?","['matrices', 'orthogonal-matrices', 'transpose', 'linear-algebra']"
4653328,Proof of Chinese remainder theorem by isomorphism,"My note states that we can prove Chinese remainder theorem as the way shown: Let $m$ and $n$ be coprime natural numbers. Then $C_{mn}$ is isomorphic to $C_m \times C_n$ (where $C_m$ is cyclic group with order $m$ ) I understand how to prove they are isomorphic, but I don’t understand how that prove the Chinese remainder theorem. For example,let $m=5,n=7$ . The isomorphism shows that there is a bijective map between adding the result we get after applying $\pmod {35}$ and adding $(a,b)$ , where $a$ is the result after $\pmod 5$ and $b$ is the result after $\pmod 7$ . But how does it show that we could know what is $a\pmod {35}$ if we know $(a\pmod 7,a\pmod 5)$ for positive integer $a$ ? Edit : I know the chinese remainder theorem as If one knows the remainders of the Euclidean division of an integer n by several integers, then one can determine uniquely the remainder of the division of n by the product of these integers, under the condition that the divisors are pairwise coprime (no two divisors share a common factor other than 1)","['group-isomorphism', 'gcd-and-lcm', 'chinese-remainder-theorem', 'group-theory', 'isometry']"
4653345,Expression of odd and even functions,"Let $f,g$ be respectively odd and even analytic functions. Then, it is true that there exist $F,G$ real analytic such that $f(x)=xF(x^2)$ and $g(x)=G(x^2)$ . This easily follows by writing the functions in series. My question is: does it still holds if we assume $f$ and $g$ only $\mathcal{C}^1$ (with $F,G\in\mathcal{C}^1$ , too)? I suppose that this does not holds if $f$ and $g$ are only supposed to be continue. Thank you very much.","['functions', 'real-analysis']"
