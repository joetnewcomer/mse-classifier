question_id,title,body,tags
2074166,"Proving that the function $f(x,y)=\frac{x^2y}{x^2+y^2}$ has partial derivatives at $0$.","Let: $$f: (x,y) \mapsto \frac{x^2y}{x^2+y^2}$$ and  $\:f(0,0)=0$ It's partial derivatives are: $$\frac{\partial f}{\partial x} (x,y)=\frac{2xy^3}{(x^2+y^2)^2}$$ $$\frac{\partial f}{\partial y} (x,y)=\frac{x^2(x^2+y^2)}{(x^2+y^2)^2}$$ How can I prove that is has partial derivatives at $0$ and calculate them? Would showing that $f$ is continues at $0$ and stating that the partial derivatives at $0$ are also null suffice? Or should I try showing that the partial derivatives are equal at $0$?","['multivariable-calculus', 'partial-derivative']"
2074172,"Can conditionally convergent series be interpreted as a ""generalized Henstock-Kurzweil integral""?","One amazing thing about the Lebesgue integral is that is defined w.r.t. to a given measure and that there a lot of different measures making the Lebesgue integration a very general tool (consider Harmonic Analysis). The Henstock-Kurzweil integral doesn't seem anywhere nearly as general (at least not in this sense), so it looks less appealing. Although for real functions it's generally better because in that sense it is more general. On Wikipedia it says that the Henstock–Kurzweil integral can be thought of as a ""non-absolutely convergent version of Lebesgue integral"" (w.r.t. to Lebesgue measure I assume). Having just a tiny bit of hope for a more ""general Henstock-Kurzweil integral"" an obvious question arises: Is there a generalization of the Henstock-Kurzweil integral that you can take w.r.t. to different ""things"" (similar to measures) that also includes conditionally convergent series as ""integrals of sequences""? (one should compare this idea to Lebesgue integration w.r.t. counting measure)","['real-analysis', 'integration', 'gauge-integral', 'sequences-and-series']"
2074181,What is the importance of Bézout's identity?,"I am creating a function which generates a privateKey for RSA. The underlying algorithm for generating a privateKey is the Extended Euclidean algorithm. According to Wikipedia, the output of this algorithm is a ""Bézout's identity"". I never heard of Bézout's identity before and wanted to know what it's importance is and what is it used for, but I can't find a clear answer. Googling ""What is the importance of Bézout's identity?"" yields no relevant results. The closest thing I could find was a discussion on Wikipedia:Talk The point is that Bézout's identity is an important result which is
  used in many areas of mathematics. In particular it is one of the
  starting tools (with modular arithmetic) of Diophantine equation
  theory To someone who does not have an extensive mathematical background the above discussion is meaningless. Can someone describe the importance and use-cases for Bézout's identity in layman terms?","['number-theory', 'prime-numbers']"
2074189,Solve equation in determinant,"Let $ a,b,c,m,n,p\in \mathbb{R}^{*} $, $ a+m+n=p+b+c $. Solve the equation: $$ 
\begin{vmatrix}
x & a & b &c \\ 
a & x & b &c \\ 
m &n  & x &p \\ 
 m&  n&  p& x
\end{vmatrix}
=0
$$ I had used the Schur complement ($\det(M)=\det(A)\cdot (D-C\cdot A^{-1}\cdot B)$, for $ M= \begin{bmatrix}
A &B \\ 
C & D
\end{bmatrix}) $ but it didn't help me.","['matrices', 'linear-algebra', 'determinant']"
2074217,How to compute the gradient of the softmax function w.r.t. matrix?,"Let us consider the following functions \begin{equation}
y = \operatorname{softmax}(z)
\end{equation}
  \begin{equation}
z = h\cdot W + b
\end{equation} where $y, h, W$ and $b$ are $1 \times n$, $1 \times m$, $m \times n$ and $1 \times n$ matrices. Compute $\frac{\partial{y_i}}{\partial{W}}$. My efforts: \begin{equation}
\frac{\partial{y_i}}{\partial{W}} =  \frac{\partial{y_i}}{\partial{z}} \times \frac{\partial{z}}{\partial{W}}
\end{equation} Here $z$ is a vector and $W$ is a matrix so $\frac{\partial{z}}{\partial{W}}$ will be a 3D tensor. But $y_i$ is a scalar and $W$ is $m \times n$ matrix so $\frac{\partial{y_i}}{\partial{W}}$ should be of size $m \times n$. Please tell me where I am wrong?","['multivariable-calculus', 'neural-networks', 'matrix-calculus', 'derivatives']"
2074230,How to quickly solve $y=\int_{-\pi/4 }^{\pi/4 } \left[\cos x + \sqrt{1+x^2}\sin^3x\cos^3x\right]dx$?,"I'm currently trying to solve $$y=\int_{-\pi/4 }^{\pi/4 } \left[\cos x + \sqrt{1+x^2}\sin^3x\cos^3x\right]dx,$$ which is a GRE math subject test problem. I was able to get the answer ($\sqrt{2}$) by breaking up the integral $$\int_{-\pi/4 }^{\pi/4 } \cos x \ dx+\int_{-\pi/4 }^{\pi/4 } \sqrt{1+x^2}\sin^3x\cos^3x \ dx.$$ Then I used $$1+x^2=(z-x)^2$$ to get $$t=\frac{z^2-1}{2z}$$ as well as $$\sin x = \frac{2z}{1+z^2}, \ \ \cos x = \frac{1-z^2}{1+z^2}, \ \ dx = \frac{2 \ dz}{1+z^2}.$$ Note that $$z=\tan(x/2).$$ I then plugged these into $$\int_{-\pi/4 }^{\pi/4 }\sqrt{1+x^2}\sin^3x\cos^3x \ dx$$ to get something rather ugly
\begin{align*}
&\int_{-\tan{\pi/8}}^{\tan{\pi/8}}\sqrt{1+ \frac{z^2-1}{2z}^2 } \cdot\left( \frac{2z}{1+z^2}\right)^3\cdot\left(\frac{1-z^2}{1+z^2}\right)^3\cdot\left(\frac{2}{1+z^2}\right)dz \\
&= \int_{-\tan{\pi/8}}^{\tan{\pi/8}} \frac{16 z^3 (1 - z^2)^3 \sqrt{\frac{(z^2 - 1)^2}{4 z^2} + 1}}{(z^2 + 1)^7}dz.
\end{align*}
Since this is an odd function, the solution is $0$ and thus $$y=\int_{-\pi/4 }^{\pi/4 } \cos x \ dx + 0 =\sqrt{2}.$$ Even though I was able to get to the answer, I'd imagine on the math GRE subject test, I would not have had enough time to figure all this out. Are there any tricks for quickly solving definite integrals like this?","['integration', 'definite-integrals', 'calculus']"
2074259,"How can I show that a bijection between $[0,1)$ and $(0,1)$ cannot be continuous?","Suppose $f:[0,1)\to (0,1)$ is bijective. Prove that $f$ is not continuous. I know that $(0,1)$ and $[0,1)$ are not homeomorphic spaces because of the connected property. Can we then conclude that $f$ is not continuous because of that?","['general-topology', 'real-analysis']"
2074297,Why is covariance something to care about?,The reasoning behind why one might be interested in variance is quite intuitive to me but covariance is not. What information do I attain from covariance? Perhaps an example could help me. In my book covariance is just briefly mentioned and then ignored. Is there any geomtric way to motivate why it is defined the way it is that makes sense? I am primarily interested in the continuous case.,"['probability-theory', 'covariance']"
2074316,Calculating rotation axis from rotation matrix,"Suppose we are given a rotation matrix, i.e. an orthogonal $3 \times 3$ matrix $\mathbf{A} = (a_{ij})$ with $\det \mathbf{A} = 1$. Then the mapping $\mathbf{x} \mapsto \mathbf{A}\mathbf{x}$ will rotate points of $\mathbb{R}^3$ around some axis through the origin. I have some old notes (which I wrote decades ago) that say that the axis of rotation can be obtained in several ways: As an eigenvector of $\mathbf{A}$ corresponding to the eigenvalue $1$. As the vector $(a_{32} - a_{23}, \; a_{13} - a_{31}, \; a_{21} - a_{12})$ As any row of the matrix $\mathbf{A} + \mathbf{A}^T + [1 - \text{trace}(\mathbf{A})]\mathbf{I}$. As any row of the matrix $\text{adj}\,(\mathbf{A} - \mathbf{I})$. The rows of this matrix are all scalar multiples of one another, so it doesn't matter which row we use. As the cross product of the first two columns of the matrix $\mathbf{A} - \mathbf{I}$. This will actually be the third row of $\text{adj}\,(\mathbf{A} - \mathbf{I})$. I want to confirm all of these statements. Clearly #1 is obvious, and #2 is well-known, but I'm having trouble with the other three. It's possible that they're not even true. Can someone elucidate, please. In your answer, it's OK to assume that #1 and #2 are already known. Also, any thoughts on which calculations are most stable, numerically?","['matrices', 'rotations', 'linear-algebra', 'linear-transformations']"
2074319,Formal Construction of Polynomial Ring in Several Variables,"The formal construction of the polynomial ring in one variable is briefly the following: We take a ring $(R,+,\cdot)$ with $1_R$.
We define $R^{ \mathbb{N}}$ be the set of all the sequences $(a_0,a_1,a_2,a_3,...)$, $a_i \in R^{ \mathbb{N}},\forall i\in \mathbb{N}$ and we define the following operations: 
$$+:  R^{ \mathbb{N}} \times  R^{ \mathbb{N}} \longrightarrow  R^{ \mathbb{N}},\  ((a_0,a_1,a_2,...),(b_0,b_1,b_2,...))\mapsto (a_0,a_1,a_2,...)+(b_0,b_1,b_2,...):= (a_0+b_0,a_1+b_1,a_2+b_2,...)$$ and $$\cdot:  R^{ \mathbb{N}} \times  R^{ \mathbb{N}} \longrightarrow  R^{ \mathbb{N}},\ ((a_0,a_1,a_2,...),(b_0,b_1,b_2,...))\mapsto (a_0,a_1,a_2,...) \cdot (b_0,b_1,b_2,...):=(c_0,c_1,c_2,...)$$
with $c_n=a_0b_n+a_1b_{n-1}+...+a_{n-1}b_1+a_nb_0,\forall n\in \mathbb{N}=\{0,1,...\}$. Furthermore we define the equality $(a_0,a_1,a_2,...)=(b_0,b_1,b_2,...) \iff a_i=b_i, \forall i\in \mathbb{N}$. With these two binary operations we have that $(R^{ \mathbb{N}},+\cdot )$ is a ring with $1_{R^{ \mathbb{N}}}=(1_R,0_R,0_R,...)$. Now, polynomial is every element of the last ring of the form $(a_0,a_1,a_2,...,a_n,0_R,0_R,...)$. If $R[X]$ is the set of all the polynomials then $R[X]$ is a subring of $R^{ \mathbb{N}}$ and the mapping $f:R\longrightarrow R[X]$, $a\mapsto (a,0_R,0_R,...) $ is a monomorphism. So, we can say that $R$ is a subring of $R[X]$, and after this we have all the usual theorems. My question is: How we can do exactly the same construction in the polynomial ring in several variables with the same procedure? PS 1: I apologize for my English. If you don't understand something ask me please. PS 2: I know that a similar question already exists, but I think I have a different procedure. Thank you in advance.","['abstract-algebra', 'ring-theory', 'polynomials']"
2074330,"Prove that if $f:[0,1] \to \mathbb{R}$ is absolutely continuous, then $|f|^p$ is absolutely continuous for $p>1$","I am trying to show that if $f:[0,1]\to \mathbb{R}$ is absolutely continuous then $|f|^p$ is absolutely continuous for all $p>1$. Since the product of absolutely continuous functions is absolutely continuous, the result is obvious if $p$ is an integer. My first thought in trying to prove the result was to try to generalize the proof that the product of absolutely continuous functions is absolutely continuous. Following that strategy and using the fact that $f$ is bounded, it's not difficult to reduce the problem to proving the result for the case that $1<p<2$. Once I get here though I am not sure how to proceed. Any help is appreciated!","['real-analysis', 'absolute-continuity']"
2074332,"Proving the closed form for $\sum_{k_1=0}^{\infty}\cdots\sum_{k_n=0}^{\infty}\frac{1}{a^{k_1+\cdots+k_n}}$, where $k_1 \neq\cdots\neq k_n$ and $a>1$?","I recently encountered a problem that requires us to sum the series $$
\sum_{i=0}^{\infty} \sum_{j=0}^{\infty} \sum_{k=0}^{\infty} \frac{1}{3^i 3^j 3^k}
$$ given the condition that $i \neq j \neq k$. Upon generalizing the problem, I get this: $$
\sum_{k_1=0}^{\infty}\sum_{k_2=0}^{\infty}\cdots\sum_{k_n=0}^{\infty} \frac{1}{a^{k_1+\cdots+k_2}} = \frac{n! \times a^n}{\prod_{i=1}^{n} (a^i - 1)}
$$ for $a>1$ and $k_1 \neq \cdots \neq k_2$, i.e. all indices are distinct at all times. Now the closed form expression (on the right hand side) for the infinite series has been obtained purely by guessing. However, I've verified that the equation works, through a computer program. The only task now left to do is to prove the formula, which I'm unable to do.",['sequences-and-series']
2074361,How to determine the number of symmetric relations on a 7-element set that have exactly 4 ordered pairs?,"Let $A = \big \{1,2,3,4,5,6,7\big \}$. Find the number of symmetric relations on A, which contain exactly four ordered pairs  ? My approach : for a relation to be symmetric either pairs of type
  (i,i) should be present or both pairs (i,j) and (j,i) should be
  present. There are $n$ elements of type $(i,i)$ which can exist alone.
  and there are $\frac{n^2-n}{2}$ pairs of type $(i,j)$ and $(j,i)$, So,
  totally there are $n + \frac{n^2-n}{2} = \frac{n^2+n}{2}$ pairs,
  which evaluates to be $28$ for $n = 7$. So to form relations which
  contain only $4$ ordered pairs select $4$ from these in
  $\left(\begin{array}{c}28\\ 4\end{array}\right)$ ways. What am i doing wrong?","['relations', 'elementary-set-theory', 'proof-verification']"
2074368,"Why does the fact that ""$Tv$ is orthogonal to $v$ for all $v$ implies T is the zero operator"" break down for real inner product spaces?","Here's the awkwardly named theorem 7.14 (for which I can't think of a good name either) appearing in Axler's Linear Algebra Done Right, 3rd edition, p 210: The proof is algebraic, and I can't glean from it any intuition about why this theorem breaks down over $\mathbf{R}$, as the author claimed. What's so special about $\mathbf{C}$ that allows an inner product to be written in the above form, which seems impossible for the inner product over $\mathbf{R}$? I would also appreciate alternative (and more ""intuitive"", or perhaps  elementary) proofs for this result.","['complex-analysis', 'linear-algebra', 'proof-explanation', 'inner-products']"
2074412,Artinian rings that are not Artin algebras,"An Artin algebra $A$ is an algebra over a commutative Artinian ring $R$ which is finitely generated over $R$, e.g. finite dimensional algebra over a field. Clearly, any Artin algebra is left and right Artinian. What are examples of left and right Artinian rings that not Artin algebras?","['abstract-algebra', 'reference-request']"
2074418,Asymptotic normality is possible even when neither a mean nor a second moment exsit.,"Let $\{X_n\}$ be independent r.v. satisfying the Lindeberg conditon, set $s_n^2=\sum_{i=1}^n Var(X_i)$. Define $\{\xi_n\}$ to be independent r.v. and independence with $\{X_n\}$. The distribution of $\{\xi_n\}$ is
$$P(\xi_n=0)=1-\frac1{n^2},~P(|\xi_n|>x)=\frac{1}{n^2}x^{-1}.$$ Show that $$\sum_{i=1}^n\frac{X_i+\xi_i}{s_n}\overset{d}\to \mathcal N(0,1).$$ How can we do to prove the conclusion? Do we need the distribution of $\{\xi_n\}$, or it just states that the random variables have no finite first and second moment?","['probability-theory', 'central-limit-theorem']"
2074428,Special graphs in graph theory: generalized fan graphs,"I was going through some generalized graphs, where I came to know about generalized Petersen and generalized wheel graphs . Details about these two graphs are explained thoroughly on the web and got this link for generalized fan graphs . I have a little doubt about the generalized fan graphs. Although I know about the fans graphs. Are generalized fan graphs not simple graphs? Can anybody explain the generalized fan graphs. It will be of great help for my work.","['combinatorics', 'graph-theory', 'discrete-mathematics']"
2074430,Find all the prime factors of $1000027$,"Find all the prime factors of $1000027$ . I got all the factors by testing every number from $1$ to $103$ , but when I try to do it using algebra, I get stuck. My work: $$
1000027=(100+3)(100^2-3\cdot100+3^2).
$$ How do I simplify further? Source: Mathematics Magazine,
Vol. 23, No. 5, May - Jun., 1950,
Problems and Questions.","['prime-factorization', 'factoring', 'algebra-precalculus', 'prime-numbers', 'elementary-number-theory']"
2074441,"How to ""coordinate bash"" a geometry problem?","How do you perform ""coordinate bashing""? I read through this which gave some useful information, but I still don't know how to perform it. What I know about coordinate bashing: Coordinate bashing is assigning geometrical figures points on the coordinate plane. It involves using formulas such as the distance formula, slope formula, shoelace theorem, point to line distance, etc. Can someone provide an illustrated example or two?","['coordinate-systems', 'contest-math', 'soft-question', 'geometry']"
2074493,"Find a positive definite matrix with $a_{1,1}<0$","Find a matrix $A=[a_{i,j}]$ such that $A$ is positive definite and $a_{1,1}<0$. It is not easy. In my opinion, there is no such matrix if the matrix dimension is even.","['matrices', 'positive-definite', 'examples-counterexamples']"
2074513,"If $a+b+c = 6$ and $a$, $b$, $c$ are nonnegative then $a^2+b^2+c^2 \geq 12$ [duplicate]","This question already has answers here : Knowing that for any set of real numbers $x,y,z$, such that $x+y+z = 1$ the inequality $x^2+y^2+z^2 \ge \frac{1}{3}$ holds. (15 answers) Closed 5 years ago . Let $a,b,c$ be three positive real numbers such that $a+b+c = 6$. Prove that $a^2+b^2+c^2 \geq 12$. I tried using the AM-GM inequality to solve the same, however I wasn't able to make any considerable progress.","['inequality', 'cauchy-schwarz-inequality', 'algebra-precalculus', 'sum-of-squares-method', 'contest-math']"
2074542,"How to understand ""a.s. convergence does not come from a metric, or even from a topology""?","The sentence ""a.s. convergence does not come from a metric, or even from a topology"" comes from a remark in Durrett's probability theory textbook, after following two theorems. Theorem 2.3.2. $Xn \rightarrow X$ in probability if and only if for every subsequence $X_{n(m)}$
  there is a further subsequence $X_{n(m_k)}$ that converges almost surely to $X$. Theorem 2.3.3. Let $y_n$ be a sequence of elements of a topological space. If every
  subsequence $y_{n(m)}$ has a further subsequence $y_{n(m_k)}$ that converges to $y$ then $y_n \rightarrow y$. The remark says ""Since there is a sequence of random variables that converges in probability but not a.s., it follows from Theorem
2.3.3(above theorem) that a.s. convergence does not come from a metric, or even from a topology. I don't know how to understand this. Although I know metric is about distance and topology is about open sets, I didn't see the connection between these. And what are some implications/applications of this property?",['probability-theory']
2074549,"Functions of $W^{1,p}_{0}$ on the boundary","Well, I'm studying now Sobolev Spaces and I have a problem. Suppose an arbitrary open set $\Omega$, we will work there all the time.
The space $W^{1,p}_{0}$ is the completeness of $C^{\infty}_{0}$ in $W^{1,p}$ and friend told me that this space is the functions in $W^{1,p}$ but also vanish at the boundary of $\Omega$. My questions are: i) is this true? ii) if $\Omega$ is an open set, the function is not define at the boundary, does it make sense that it is defined at the boundary? iii) What happens if $\Omega$ is not bounded? Thank you very much.","['functional-analysis', 'sobolev-spaces']"
2074554,$f(A - f^{-1}(B)) = f(A)-B $ proof,"Let $f:E\to F$ be a function and $A\in P(E) $ and $B \in P(F)$.
I'm asked to prove : 
$$f(A-f^{-1}(B)) = f(A)-B \tag1$$ (Where for any set $X$ and $Y$ we have $X-Y := \{x \ | \ x\in X \ \land x\notin Y \}$) so $(1)$ is equivalent to $$f(A-f^{-1}(B)) \subset (f(A)-B) \land (f(A)-B)\subset f(A-f^{-1}(B))  $$
The problem is in proving $f(A-f^{-1}(B)) \subset (f(A)-B)$.
I tried to prove it in this way: let $y\in f(A-f^{-1}(B))$
$$\Rightarrow \exists x\in A-f^{-1}(B) : \ y=f(x) \\
\Rightarrow \exists x\in A: x\notin f^{-1}(B) \ \land \ y=f(x) \\
\Rightarrow \exists x \in A : y=f(x) \ \land \ y\notin f(f^{-1}(B))$$
I'm stuck here.
Is there a better way to prove $(1)$",['elementary-set-theory']
2074558,A subset of the real numbers which has only unilateral limit/accumulation points is countable.,"I have been thinking about this problem for a while and couldn't find a solution by myself and the one that I've heard about is a little difficult. If no one posts that solution, I will do it latter. Thanks in advance.","['real-analysis', 'elementary-set-theory', 'limits']"
2074601,Differential Geometry textbooks for someone interested in Algebraic Geometry,"Over the next two semesters I will be taking Algebraic Geometry courses in which we are supposed to cover almost all of Hartshorne. I have the adequate algebraic background; however, as some people contend, it is instructive to know a bit of differential geometry beforehand in order to create some intuition by relating new abstract concepts in algebraic geometry to their respective, ""easier to deal with"", analogues in differential geometry. As it happens, the furthest I have gone in differential geometry was just a multivariable calculus course and a course in curves and surfaces at the level of Shiffrin's notes . I have also studied Lie groups, and in doing so have avoided as much differential geometry as I could, but I know the inevitable - definition of manifolds, submanifolds, constant rank, immersions, submersions, tangent space, but I admit I wish I was more comfortable with those. Whenever I look into a Differential Geometry textbook - Lee, Spivak, Lang, Kobayashi (the ones so far), they are all either very long, or shorter but too advanced. And the language in differential geometry is, in my opinion, a mess. Wouldn't it be easier if we had something like a category theoretical approach as we have today in algebraic topology? Sorry for the rant in the last paragraph, back to my original point. I am looking for a textbook in Differential Geometry that is made for someone who is more algebraically intuitioned but still wants to understand the geometric picture. Perhaps it would be: A book in Differential Geometry with a view toward Algebraic Geometry, or A book in Algebraic Geometry directed at Differential Geometry, but not so advanced that a person with my background could follow, or The dreaded answer, there is none and the only way to learn Differential Geometry is by cramming the classics. Thank you.","['reference-request', 'book-recommendation', 'differential-geometry', 'algebraic-geometry']"
2074611,differentability from a functional inequality,"Let $f$ be a non-negative continuous function on $[0,\infty)$ vanishing at $0$. If $f\left(\frac{x}{n}\right)$ does not exceed $\frac{f(x)}{n}$ for any $n,x$ show that $f$ has a finite right-hand derivative at $0$.","['derivatives', 'continuity']"
2074622,Does the order matter for chain rule?,"Given $$\frac{\partial J}{\partial z_2}=\delta_1$$
$$z_2 = hW_2+b_2$$
Derive gradients of $J$ with respect to $h$ and $W_2$, where $J \in \mathbb{R}$, $z_2 \in \mathbb{R}^{D_x \times D_y}$, $\delta_1 \in \mathbb{R}^{D_x \times D_y}$,$W_2 \in \mathbb{R}^{H \times D_y}$, $h \in \mathbb{R}^{D_x \times H}$. Here's the correct solution:
 \begin{align*}
    &\frac{\partial J}{\partial h}=\frac{\partial J}{\partial z_2} \frac{\partial z_2}{\partial h}=\delta_1 W_2^T\\
    &\frac{\partial J}{\partial W_2}=\frac{\partial z_2}{\partial W_2} \frac{\partial J}{\partial z_2}=h^T \delta_1
\end{align*} The results are obtained by applying chain rule, however chaining in different orders. The change of orders reflect a compromise to meet the dimension requirements of $\frac{\partial J}{\partial W_2}$. It's very annoying that you have to examine the dimension every time. Is there any general rule that can be followed knowing which order to arrange in terms of applying chain rule without examining the dimension?","['matrices', 'calculus', 'derivatives']"
2074630,When to add and when to multiply with combination problems?,"Finding the number of ways a certain task can be done has always given rise to a few troubles for me. Issues seem to arise in questions where AND and OR rear they're heads. For example: Suppose a box contains 12 black marbles and 8 green marbles. How many ways can 4 black marbles and 3 green marbles be chosen? To me, I feel as though the correct answer would be $\binom{12}{4}\times \binom{8}{3}$ i.e. for every 4 black marbles selected there are 3 possible green marbles which could be selected. However, the solution for this particular problem says that there are $\binom{12}{4} +  \binom{8}{3}$ ways to draw the marbles. Some searching online, however, suggests my solution is correct, while other sources provide the latter approach as a solution. What is the correct way to approach this question and ones similar to it?","['combinations', 'combinatorics']"
2074633,A confusion on the definition of *almost everywhere*,"This is the definition from Terry Tao's Introduction to measure theory : A property $P(x)$ of a point $x \in \mathbb{R}^d$ is said to hold (Lebesgue) almost everywhere in $\mathbb{R}^d$ or for (Lebesgue) almost every point $x \in \mathbb{R}^d$, if the set of $x \in \mathbb{R}^d$ for which $P(x)$ fails has Lebesgue measure zero (i.e. $P$ is true outside of a null set). My question is that what if the set of $x \in \mathbb{R}^d$ for which $P(x)$ fails is not Lebesgue measurable? Then can we say whether $P(x)$ is true almost everywhere or not?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2074672,Show that the Area of image = Area of object $\cdot |\det(T)|$? Where $T$ is a linear transformation from $R^2 \rightarrow R^2$,"Prove that the area of an image in $2d$ cartesian coordinates is equal to the determinant of the linear transformation times the area of the initial shape. I've tried to formulate general expression for area given lots of points, but it feels like that's barking up the wrong tree. I've also proved it for transformations which are combinations of rotations and enlargements. If that is the case, the distance between each point in the shape will increase by a constant, which has to be the same no matter what the initial shape is. So we can take a unit square which is the easiest case, and it is trivial to show that the resulting area is $\det(T)$. But because these transformations stretch lengths by the same constant, it must stretch areas by the same constant - $\det(T)$. Similarly, is it possible to show that all linear transformations (i.e. shears and compressions) have some property that allows us to deduce that the area of the image must transform by some constant for any image?","['matrices', 'area', 'linear-transformations', 'determinant']"
2074693,Solve $\cos x-\sin(2x)=0$,Solve $\cos x-\sin(2x)=0$ I did: $$\cos x=\color{blue}{\sin(\pi /2-x)}$$ therefore: $$\color{blue}{\sin(\pi /2-x)}=\sin(2x)$$ Can I do that:?? now to solve only for $\pi/2-x=2x$ so $x=\pi/6+2\pi k$,"['algebra-precalculus', 'trigonometry']"
2074730,Let $n$ and $m$ be two natural numbers. Show that if there is an injection from $n$ into $m$ then $n\leq m$.,"Definition . A set is said to be finite if there is a bijection between this set and some natural numbers. Question . Let $n$ and $m$ be two natural numbers. Show that if there is an injection from $n$ into $m$ then $n\leq m$. Proof. By induction on $n$. If $n=0$, it is trival.  Assume for $n$ holds. Then, we will show that $n+1$ holds.  Now, let $f:n+1\rightarrow m$ be an injection. Then, we can say that there is an injection such that $f:n\rightarrow m$. Hence, $n=m$ or $n<m$. Assume $n=m$. So, $f:n+1\rightarrow n$ is an injection. Then, we must find a contruduction. My question is: how can I find?",['elementary-set-theory']
2074751,Looking for stationary points of the nonlinear recurrence $x = {\rm e}^{\jmath \alpha x}$,"A while ago I stumbled upon an interesting recurrence of the form $$x_{n+1} = {\rm e}^{\jmath \cdot \alpha \cdot x_n}.$$ I studied it for positive, real-valued $\alpha$ though one could look also at complex values. I noticed that for a range of $\alpha$, this recurrence seems to have one stationary point. Trivially, for $\alpha=0$ it is $x_\infty = 1$ but I am more interested to compute it for other values of $\alpha$. I couldn't find anything helpful analytically (there is a lot on linear recurrence relations but I didn't find anything helpful on nonlinear ones). Therefore, I continued to study it numerically. Here is what I found: For $0 \leq \alpha \lessapprox 1.96$ there seems to be a single stationary point. Here is a plot of the numerical values I found, displaying $\Re(x_\infty)$ and $\Im(x_\infty)$ for various values of $\alpha$. For each $\alpha$ the sequence seems to converge to the fixed point regardless of the initialization $x_0$, but I'm not sure of it. The problem when you check this numerically is that some very large values can occur. E.g., for $\alpha = \pi/2$ and $x_0 = 3-7j$ we have: \begin{align}
  x_1 & = \exp[7\pi/2]\cdot \exp[\jmath 3\pi/2] = \exp[7\pi/2]\cdot(-\jmath)  \approx - 59610\jmath \\
 x_2 & = \exp[-\jmath \pi/2\cdot \jmath \cdot \exp(\jmath 7\pi/2)] =\exp[\exp(7\pi/2)] \approx {\rm e}^{59610} \\
 x_3 & = \exp[\jmath \pi/2 \exp(\exp[7\pi/2])] = \exp[\jmath \varphi]
\end{align} where $\varphi$ (modulo $2\pi$) is not very easy to compute but $x_3$ is small, since $|x_3|=1$. From this point on it converges. If you try to plot this numerically, you will get fractal-looking shapes like these: Here, the blue color indicates the points where $x_n$ became to large to fit into my float data type (i.e., I got INF) which could mean either divergence or just numerical problems like the one I explained above. The other colors indicate convergence after $n$ iterations, defined as getting within a radius of 0.001 of the single stationary point found earlier. For $\alpha>1.962$, the sequence seems to form limit cycles of a discrete number of points but these are even harder to predict and seem somewhat chaotic. Now here are my questions: Is it possible to compute $\lim_{n\rightarrow \infty} x_n$ analytically? The recurrence seems to be a quite generic one, does it have a name under which I could search further? Is anything known on the convergence? Does it have a nontrivial region of convergence and is it possibly really fractal? Or are the fractal looking images merely an artifact of my limited data type?","['recurrence-relations', 'fractals', 'reference-request', 'complex-dynamics', 'sequences-and-series']"
2074793,Showing that $f(x) = x^3$ is injective?,"This is my attempt. Is it right? Is there a simpler way? Is there a way which relies on less background knowledge? A function is injective iff $f(x) = f(y) \implies x = y$. This is equivalent to $x \not = y \implies f(x) \not = f(y)$. We will prove this latter statement by contradiction: Suppose there are two numbers $x$, $x+a$ with $a >0$ and $x \not = x+a$ but $x^3 = (x+a)^3$. Expanding the RHS, subtracting $x^3$, and dividing both sides by $a>0$ yields $0 = 3x^2 + 3xa + a^2$. Taking this last equation as a quadratic in $x$, it has a real solution iff the discriminant is non-negative. But the discriminant is $9a^2 - 12a^2 = -3a^2$, so there are no real $x$ which satisfy $x^3 = (x+a)^3$.","['algebra-precalculus', 'functions', 'discrete-mathematics']"
2074812,Prove that the limit $\lim \limits_{n \to \infty}\left(1 + \frac{1}{\sqrt[3]{n^3 - n^2}}\right)^n$ doesn't exist or find it,"Given $\lim \limits_{n \to \infty}\left(1 + \frac{1}{\sqrt[3]{n^3 - n^2}}\right)^n$. My attempt: $\lim \limits_{n \to \infty}\left(1 + \frac{1}{\sqrt[3]{n^3 - n^2}}\right)^n = \lim \limits_{n \to \infty}\left(1 + \frac{1}{n\sqrt[3]{1 - 1/n}}\right)^n$. So as the  $n \to \infty \Rightarrow \sqrt[3]{1 - 1/n} \to 1$. $\lim \limits_{n \to \infty}\left(1 + \frac{1}{n\sqrt[3]{1 - 1/n}}\right)^n = \lim \limits_{n \to \infty}\left(1 + \frac{1}{n}\right)^n = e$. However, I'm not sure that my proof is strict enough, especially the moment, where I go from $\sqrt[3]{1 - 1/n}$ to $1$.","['exponential-function', 'sequences-and-series', 'limits']"
2074835,"Prove that if $f'(x) > g'(x)$, $f(a) \ge g(a)$ then $f(x) > g(x)$ in $(a,b]$","There's a similar question here: If $f(a) = g(a)$ and $f'(x) < g'(x)$ for all $x \in (a,b)$, then $f(b) < g(b)$ however in the original answer the conditions are a bit different as well as the solution uses Mean value theorem which we didn't learn. $f(x)$ and $g(x)$ are continuous in $[a,b]$ and differentiable in $(a,b)$. $\quad f(a) \ge g(a)$ and $f'(x) > g'(x)$ in $(a,b)$. Prove that $f(x) > g(x)$ in $(a, b]$. Let $E(x) = f(x) - g(x)$. If $E(a) = E(b)$ then by Rolle theorem there must be a $c$ such that $E'(c) = 0$. But $E'(x) = f'(x) - g'(x) > 0$ for $a<x<b$ therefore $E'(x) \neq 0$. Therefore $E(a) \neq E(b)$. Because $E(x)$ is continuous in $[a,b]$ there must be an absolute minimum and maximum for $E(x)$ in $[a,b]$. According to max/min theorem possible max/min can be on the edges of an interval, where the derivative is $0$ or where derivative doesn't exist. Because $E(x)>0$ and differentiable in $(a,b)$ the max/min has to be on the edges. $E'(x) > 0 \Rightarrow$ $E(x)$ is ascending for $a<x<b$ therefore $E(a)$ = min, $E(b)$ = max. Because $f(a) \ge g(a)$ then $E(a) \ge 0$. Because $E(a)$ is the absolute min from there $E(b) >E(x)>E(a)$ for $a<x<b$. Of course there can't be any $x_1, x_2$ such that $E(x_1)=E(x_2)$ because then again by Rolle $E'(x)$
 will have a zero but this contradicts the given conditions. Therefore if $E(b) >E(x)>E(a)$ for $a<x<b$ then $f(x)>g(x)$ for $a<x<b$. Q.E.D. Please let me know if there were parts I could've skipped in the proof.","['derivatives', 'calculus', 'proof-verification']"
2074867,First Isomorphism Theorem for Monoids?,"Background The First Isomorphism Theorem states, If $G$ and $H$ are two groups and $\varphi:G\to H$ be a group homomorphism, then $\varphi(G)$ is isomorphic to $G/\ker \varphi$. I was wondering that whether we can generalize this theorem to weaker algebraic structures and I observed the following, The definition of group homomorphism can be easily generalized to what we may call monoid homomorphism as follows, $\color{crimson}{\text{Definition 1.}}$ If $(G,\bullet)$ and $(H,\circ)$ be two monoids then a map $\varphi:G\to H$ is said to be a monoid homomorphism if $$\varphi(x\bullet y)=\varphi(x)\circ\varphi(y)$$for all $x,y\in G$. The definition of Normal Subgroup of a group can be suitably generalized too, $\color{crimson}{\text{Definition 2.}}$ Let $G$ be a monoid and $H$ be a submonoid of $G$. We will say $H$ to be a normal submonoid of $G$ if $aH=Ha$ for all $a\in G$. The definition of kernel can be given in the following way if $H$ is a monoid, $\color{crimson}{\text{Definition 3.}}$ Let $H$ be a monoid and $\varphi: G\to H$ be a monoid homorphism. Let $e$ be the identity element of $G$. Then we can define, $$\ker \varphi:=\{x\in G\mid \varphi(x)=e\}$$ Questions From this discussion, we can obtain the following more general version of the First Isomorphism Theorem, $\color{blue}{\text{Proposition 1.}}$ Let $G$ and $H$ are monoids and $\varphi:G\to H$ be a monoid homomorphism. Then prove that $\varphi(G)$ is isomorphic to $G/\ker \varphi$. I wanted to the argument of Theorem 10.3 of this book . However, a crucial theorem used in proving Theorem 10.3 is Theorem 9.2 and to prove Theorem 9.2 we need to prove that for a monoid $M$ if $N$ be any submonoid of $M$ then, $aN=N$ iff $a\in N$. But this I can't prove. More specifically, I can't prove the following proposition, $\color{blue}{\text{Proposition 2.}}$ Let $G$ be a monoid and $H$ be a normal submonoid of $G$. Then the set $G/H:=\{aH\mid a\in H\}$ is a monoid under the operation $(aH)(bH)=abH$ where $a,b\in G$. So, my questions are, Are the above propositions true? If so, then can anyone give some hint as to how I should proceed to a proof of both of the propositions? Remark So far I have been able to prove the following result, Theorem. If $G$ be a monoid and $H$ be a submonoid of $G$ then $H=\displaystyle\bigcup_{a\in H} aH$. Proof Sketch. Let $a\in H$. Then $aH\subseteq H$ by closure of $H$. Since $a$ is arbitrary we have $\displaystyle\bigcup_{a\in H} aH\subseteq H$. To prove the converse observe that, $$a\in H\implies a\in aH\implies \displaystyle\bigcup_{a\in H} aH$$ and since the above statement holds for all $a\in H$, we are done. but don't know how this helps (if at all)","['abstract-algebra', 'monoid']"
2074881,How to Integrate this function $\int(1-x^2)^ndx$,The actual question was to find the area bounded by the curve $y=\int_{-1}^{1}(1-x^2)^ndx$ and coordinate axes. But I haven't came across these type of problems with power $n$.,"['integration', 'definite-integrals', 'area']"
2074904,Continued fractions: convergence of fraction expansion,"Using the notation $[a_0,a_1,a_2,...]$ for the continued fraction $$
 a_0 + \dfrac{1}{a_1 + \dfrac{1}{a_2 + \dfrac{1}{...}}}
$$ where $a_0 ∈ \mathbb Z$ and $a_i ∈ \mathbb N$ for all $i>0$ Question: Consider the continued fraction expansion
$$
[1,0,1,1,2,1,1,4,1,1,6,1,...,1,2n,1,...]
$$ and let $p_i$ and $q_i$ denote the numerators and denominators (respectively) of its convergents. Prove that $$
p_{3n} = p_{3n-1} + p_{3n-2},  q_{3n} = q_{3n-1}+q_{3n-2}
$$
$$
p_{3n+1} = 2np_{3n} + p_{3n−1},  q_{3n+1} = 2nq_{3n} +q_{3n−1}
$$
$$
p_{3n+2} = p_{3n+1} + p_{3n},  q_{3n+2} = q_{3n+1} +q_{3n}
$$ for $n=1,2,....$ As I understand it, the numerators are $[1,0,1,1,2,1,1,4,1,1,6,1,...,1,2n,1,...]$ while the denominator's are $[1,1,...]$. But I do not know what exactly is meant with ""of its convergence"". I am also uncertain how to proceed with proving the above mentioned questions, but I do believe it relates to the ""fundamental recurrence formulas"" as I read a bit about in the following wikipedia article: https://en.wikipedia.org/wiki/Generalized_continued_fraction Merry Christmas and happy new year.","['complex-analysis', 'continued-fractions']"
2074929,What is the integral of a vector-valued function?,what does it mathematically mean an integral of a vector ? for example in physics we say that the impulse $\vec{I}$ is the time integral of force $\vec{f}$ : $\vec{I} = \int_{t_{1}}^{t_{2}} \vec{f} dt$ what does this object $~~ \int_{t_{1}}^{t_{2}} \vec{f} dt$ clearly mean ? is it a vector ?  does it have the same direction as $\vec{I}$ ?,['real-analysis']
2074931,Show that :$\int_{0}^{1}(1-x^2)^ndx={(2n)!!\over (2n+1)!!}$,"How do I show that Let $n\ge0$ . Then, $$I:=\int_{0}^{1}(1-x^2)^ndx={(2n)!!\over (2n+1)!!}$$ Here, $m!!$ denotes the product of all positive integers $i \in \left\{1,2,\ldots,m\right\}$ that have the same parity as $m$ . My try:
Using Binomial theorem $$(1-x^2)=1-nx^2+{n(n+1)\over2!}x^4-{n(n+1)(n+2)\over 3!}x^6+\cdots$$ $$\int_{0}^{1}\left(1-nx^2+{n(n+1)\over2!}x^4-{n(n+1)(n+2)\over 3!}x^6+\cdots\right)dx$$ $$I=x-n{x^3\over 3}+{n(n+1)\over 2!}{x^5\over 5}-{n(n+1)(n+2)\over 3!}{x^7\over 7}+\cdots|_{0}^{1}$$ $$I=1-{n\over 1\cdot 3}+{n(n+1)\over 2!\cdot 5}-{n(n+1)(n+2)\over 3!\cdot  7}+\cdots$$ I need help can't see how it will simplify to ${(2n)!!\over (2n+1)!!}$","['integration', 'definite-integrals']"
2074935,How to evaluate $I=\int_0^{\frac{\pi}{2}}\sin^2x\ln(\sin^2(\tan x))dx$,"$$I=\int_0^{\frac{\pi}{2}}\sin^2x\ln(\sin^2(\tan x))dx \hspace{15mm}(1)$$
Now, using definite integral property of $\int_a^bf(x)dx=\int_a^bf(a+b-x)dx$ $$I=\int_0^{\frac{\pi}{2}}\cos^2x\ln(\sin^2(\cot x))dx\hspace{15mm}(2)$$
After $\tan x=t$ substitution in $(1)$ and $\cot x=m$ in $(2)$, to get: $$I=\int_0^{\frac{\pi}{2}}\frac{t^2}{(1+t^2)^2}\ln(\sin^2 t)dt=\int_0^{\frac{\pi}{2}}\frac{m^2}{(1+m^2)^2}\ln(\sin^2 m)dm$$
After variable change and addition, I get: $$2I=\int_0^{\frac{\pi}{2}}\frac{x^2}{(1+x^2)^2}\ln(\sin^4 x)dx\implies \frac{I}{2}=\int_0^{\frac{\pi}{2}}\frac{x^2}{(1+x^2)^2}\ln(\sin x)dx$$ How could I proceed? Any other solutions which happen to be more efficient/simple?","['integration', 'definite-integrals', 'calculus']"
2074948,Domain of the fractional power of a linear operator,"Let $H$ be a $\mathbb R$-Hilbert space. We say that $(\mathcal D(A),A)$ is a linear operator , if $\mathcal D(A)$ is a subspace of $H$ and $A:\mathcal D(A)\to H$ is linear. Assume $(e_n)_{n\in\mathbb N}\subseteq\mathcal D(A)$ is an orthonormal basis of $H$ with $$Ae_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\in\mathbb N}\subseteq(0,\infty)$ with $$\lambda_{n+1}\ge\lambda_n\;\;\;\text{for all }n\in\mathbb N\;.\tag 2$$ Now, let $\alpha\in\mathbb R$ and $$A^\alpha x:=\sum_{n\in\mathbb N}\lambda_n^\alpha\langle x,e_n\rangle_He_n\;\;\;\text{for }x\in\mathcal D(A^\alpha)\;.\tag 3$$ $A^\alpha$ is called a fractional power of $A$. How is $\mathcal D(A^\alpha)$ usually defined? We need to make sure that the series in $(3)$ exists for all $x\in\mathcal D(A^\alpha)$. Since it doesn't make sense to restrict oneself unnecessarily, we should choose $\mathcal D(A^\alpha)$ to be ""maximal"" with respect to this property. Since we know that a series $\sum_{n\in\mathbb N}x_n$ in $H$ exists if and only if $\sum_{n\in\mathbb N}\left\|x_n\right\|_H<\infty$, I would define $$\mathcal D(A^\alpha):=\left\{x\in H:\sum_{n\in\mathbb N}\lambda_n^\alpha\left|\langle x,e_n\rangle_H\right|<\infty\right\}\;.\tag 4$$ However, I've seen that many authors write $$\mathcal D(A)=\left\{x\in H:\sum_{n\in\mathbb N}\lambda_n^{\color{red}{2}}\left|\langle x,e_n\rangle_H\right|^{\color{red}{2}}<\infty\right\}\tag 5$$ (While this doesn't need to be the case, let's assume that $\mathcal D(A)$ is ""maximal"" too). Maybe there is a relation of $(5)$ and Parseval's identity . In any case, I don't understand why $\mathcal D(A)$ in $(5)$ is not defined as my suggested definition of $\mathcal D(A^1)$ in $(4)$. So, what am I missing and is there anything wrong with my definition of $\mathcal D(A^\alpha)$?","['functional-analysis', 'operator-theory']"
2074967,Differential equation in polar coordinates,I have the following system: $\frac{dx}{dt} = 3x + y - x(x^2+y^2)$ $\frac{dy}{dt} = -x +3y -y(x^2+y^2)$ Converting this to polar coordinates gives us: $\frac{dr}{dt} = r(3-r^2)$ $\frac{d\theta}{dt} = -1$ This gives us a solution $\theta(t) = -t + \theta_0$. What would the solution for $r(t)$ be though?,"['ordinary-differential-equations', 'polar-coordinates', 'analysis']"
2074984,Find the range of the function $f(x) = \sqrt{\tan^{-1}x+1}+\sqrt{1-\tan^{-1}x}$,"Problem : 
Find the range of the function $f(x) = \sqrt{\tan^{-1}x+1}+\sqrt{1-\tan^{-1}x}$ My approach : Let $\tan^{-1}x =t $ $y = \sqrt{t+1}+\sqrt{1-t}$ Squaring both sides we get : $y^2= t+1+1-t +2\sqrt{(t+1)(1-t)}$ $\Rightarrow y^2= 2+2\sqrt{(t+1)(1-t)}$ Now how to get the range of this function, please guide, will be of great help , thanks.","['algebra-precalculus', 'functions']"
2075006,Sequential square packings,"There are various studies for packing sequential squares of size $1$ to $n$ . We can try to find the smallest square they will pack into, as in tightly packed squares . We can find the smallest square they will fit into, A005842 . We can find an optimal stacking for them. Adam Ponting found an efficient packing for squares of size $1$ to $(2 n+1)^2$ , as seen in his article square packing . I expanded that for Ponting Square Packing . In the below, an offset is used on his method to pack squares of size 32 to 200. A value of 31 can be subtracted from all sizes for a packing of squares 1 to 169. So, what is a square packing?  For the purposes of this article, I'll define it as follows: Up to 4 squares can have 2 fully exposed edges. All other squares must have 2 or more edges fully covered and a third at least partially covered. No holes are allowed in the packing. Ponting mentions only finding packings for an odd square number of sequential squares.  I took a look at order-4 and sizes representable in a 4x4 matrix and found 8 solutions. The second one has squares with sizes {{11, 8, 15,6},{7,12,5,16},{9,4,13,2},{3,10,1,14}}. I haven't been able to find solutions with a 6x6 or higher even matrix. Can anyone find those, or other square packings of sequential squares?","['packing-problem', 'matrices', 'tiling', 'combinatorics', 'recreational-mathematics']"
2075080,How can I find the $n$ th derivatve of this function,"Let $f(x)=\sqrt{1+\sqrt{1-x}}\,\,\,\,\,\ \forall x\in[0,1].$ It is not difficult to find first few derivatives of this function as $$f'(x)=-\dfrac1{2^2}(1+\sqrt{1-x})^{-1/2}(1-x)^{-1/2}$$ $$f''(x)=-\frac1{2^4}(1+\sqrt{1-x})^{-3/2}(1-x)^{-1}-\frac1{2^3}(1+\sqrt{1-x})^{-1/2}(1-x)^{-3/2}$$ $$f'''(x)=-\frac3{2^6}(1+\sqrt{1-x})^{-5/2}(1-x)^{-3/2}-\frac3{2^5}(1+\sqrt{1-x})^{-3/2}(1-x)^{-2}-\frac3{2^4}(1+\sqrt{1-x})^{-1/2}(1-x)^{-5/2}$$ However it is difficult to observe a pattern between these derivatives. How can I find the $n$th derivative of $f$?","['derivatives', 'real-analysis', 'pattern-recognition', 'algebra-precalculus', 'induction']"
2075093,Probability of a certain event subject to a constraint (1D diffusion equation),"I am considering a one-dimensional process represented by a diffusion equation. $$\partial_tP=D \partial_x^2P$$ The probability distribution $P(x,t)$ obeys that $P(x,0)=\delta(x-l)$ for a given value of $l>0$. We consider this distribution in the interval $[0,d]$. I want to calculate the probability of getting to the point $x=d$ subject to the condition of not having been at 0 at any previous time. How can I calculate this probability? Without the condition I thought that I can express the condition of getting to $x=d$ as: $$P(d)=\int_0^\infty P(d,t) dt$$ but I don't know how to incorporate the supplementary condition about 0.","['statistics', 'brownian-motion', 'partial-differential-equations']"
2075108,Why can't I integrate trigonometric functions without making a substitution?,"For example, $\int\sin^3x$ is turned into $\int sinxsin^2x$ then a substitution for $\sin^2x$ is made. What I would have done was $\int(sinx)^3$ and integrate via recognition:  $\int (ax+b)^n dx$ =$\frac{(ax+b)^{n+1}}{a(n+1)} + C$ . However this will give a different answer from the correct one. Why is that? Why can't this reverse chain rule work for trigonometric functions?",['trigonometry']
2075123,Find the pdf of a bivariate transformation using change of variables,"Problem $X,Y\sim n(0,1)$. $X,Y$ are indpendent.  $U = X+Y$, $V=X-Y$. Find $f_{U,V}(u,v).$ Work I've made some progress, but there are a number of steps remaining and I'm not sure how to proceed. I know there's a formula for this sort of thing $$f_{U,V}(u,v) = f_{X,Y}(h1(u,v), h2(u,v))|J|$$ but I wanted to try working it out as explicitly as possible, using the definitions and integral change of variables in order to see what's going on under the hood. $$f_{U,V}(u,v)$$
$$= \frac{\partial}{\partial u \partial v} F_{U,V}(u,v)$$
by the second FTC.
$$= \frac{\partial}{\partial u \partial v} \int_{\{(u,v): U < u, V < v\}} f_{U,V}(u,v)dudv $$ by the definition of $F_{U,V}$.
So we'll change variables from U-V into X-Y coordinates. First we'll measure the area element
$$dudv = \frac{\partial(u,v)}{\partial(x,y)}dxdy=2dxdy$$ using the Jacobian determinant, so that we can rescale appropriately the sum of the various volume chunks. Second, we change the integrand. Do we plug in values of $u=x+y$ and $v=x-y$ in the integrand? This seems like it would make sense given that we want to replace $u$ and $v$ with $x$ and $y$, and this is the correct equivalence between the two pairs. Third, we change the region. 
$$R=\{(u,v):U < u, V < v\}$$
$$=\{(u,v):X+Y < u, X-Y < v\}$$
but I'm not sure what we're getting out of this. Fourth, we have to do something with the partial derivatives. Perhaps chain rule, or something cool with the FTC -- but I don't know how that would work if we've already changed out our $u$ and $v$ that we're supposed to be differentiating with respect to. A big part of learning how to do this is surely in getting used to the notation and the manipulations, so I'd most appreciate as explicit a solution as possible, showing how each manipulation works and is motivated.","['normal-distribution', 'multivariable-calculus', 'integration', 'probability', 'random-variables']"
2075145,Evaluating a double integral of a complicated rational function,"Define the function $Q:\mathbb{C}^{2}\rightarrow\mathbb{C}$ to be the binary quadratic form, $$Q{\left(z,w\right)}:=z^{2}+w^{2}.\tag{1a}$$ Also, define $P:\mathbb{C}^{4}\rightarrow\mathbb{C}$ to be the polynomial of degree $5$, in four variables, $$\begin{align}
P{\left(a,b,x,y\right)}
&:=a\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-4b^{2}\right]\\
&~~~~~+2\left[\left(a^{2}+1\right)\left(a^{2}+b^{2}+1\right)-2b^{2}\right]x\\
&~~~~~+a\left(a^{2}+b^{2}+1\right)x^{2}\\
&~~~~~+a\left(a^{2}+b^{2}+1\right)y^{2}.\tag{1b}\\
\end{align}$$ Note that $P{\left(a,b,x,y\right)}$ is obviously even in both $b$ and $y$. Then, define the function $\mathcal{I}:\mathbb{R}^{2}\rightarrow\mathbb{R}$ via the double integral $$\mathcal{I}{\left(a,b\right)}:=\int_{-\infty}^{\infty}\mathrm{d}x\int_{0}^{\infty}\mathrm{d}y\,\frac{2^{4}xy\,P{\left(a,b,x,y\right)}}{Q{\left(a+x,1-y\right)}\,Q{\left(a+x,1+y\right)}\,Q{\left(x,b-y\right)}\,Q{\left(x,b+y\right)}}.\tag{1c}$$ It's not hard to show then that $\mathcal{I}{\left(a,b\right)}$ is even in the second parameter $b$: $$\mathcal{I}{\left(a,-b\right)}=\mathcal{I}{\left(a,b\right)};~~~\small{\left(a,b\right)\in\mathbb{R}^{2}}.$$ Problem: Given the pair of real parameters $\left(a,b\right)\in\mathbb{R}\times\mathbb{R}_{\ge0}$, find a closed form expression for the double integral $\mathcal{I}{\left(a,b\right)}$ in terms of elementary functions. The obstacle in the way of solving this problem appears to be tedium more than anything else. Integrating the rational integrand in $(1c)$ over $x$ should in principle yield a piecewise rational function. Thus, subsequent integration over $y$ should lead to a function that is at the very least piecewise elementary, if not simpler. However, attempting to solve the problem by brute force with partial fraction decompositions quickly leads to large numbers of cumbersome expressions, rending the integral quite unmanageable without a program such as Mathematica. It is my hope that there is some cleverly efficient approach to this integral that I'm just not seeing at the moment. Any advice here would be welcome. Cheers!","['calculus', 'closed-form', 'multivariable-calculus', 'integration', 'definite-integrals']"
2075163,"Prove that $3\sum\limits_{k=1}^n \frac{k X_k}{n^{3/2}}$ converges in distribution, for i.i.d. $X_k$ uniform in $[-1,1]$","Let $(X_n)$ denote a sequence of independent random variables, uniformly distributed on $[-1,1]$. We want to prove that $S_n=\frac{3}{n^{3/2}}\sum\limits_{k=1}^n k X_k$ converges in distribution as $n$ tends to infinity. I was trying to compute $F_{S_n}(x)=P(S_n\le x)=P\left(\sum\limits_{k=1}^n k X_k\le \frac13n^{3/2}x\right)$ (1). Denote $Y_k=k X_k$, and it is easy to know that $Y_k$ is uniformly distributed on $[-k,k]$. Now, we want to find the distribution of $\sum\limits_{k=1}^n Y_k$, and then find the limit of (1). I computed $f_{Y_k+Y_{k+1}}(z)$, which is the convolution of $f_{Y_k}$ and $f_{Y_{k+1}}$, and it equals to $\frac{z+2k+1}{2k(2k+2)}$ when $-2k-1\le z<-1$, $\frac{1}{(2k+2)}$ when $-1\le z<1$, and $\frac{-z+2k+1}{2k(2k+2)}$ when $1\le z\le 2k+1$. Then, I thought this way can be tedious, and I am not sure if I can continue this way to solve the problem. I am wondering if there is any cleverer way to do this. In general, when I deal with problems about the convergence in distribution of some specific random variables, I usually try two perspectives: Prove a stronger convergence, such as convergence in probability or in $L^p$. Compute the exact expression of corresponding distribution function, and take the limit. However, I am feeling that I missed some tools to prove convergence in distribution. Is there any other theorem, lemma or method used frequently to prove convergence in distribution?","['weak-convergence', 'probability-theory', 'probability-distributions']"
2075172,"Compute $\int_{\gamma}{Log(z)\over z}dz$ for $\gamma(t)=e^{it}$, $t\in[0,2\pi]$","Compute $\int_{\gamma}{Log(z)\over z}dz$ for $\gamma(t)=e^{it}$, $0\le t\le (2\pi)$. (Why is it that using ""\le"" code suddenly creates ""2""?). Before you vote to close this question, know that its duplicate has a confirmed answer understood to the OP but unfamiliar to me in its method. And this is a way for me to, through this question, to better understand the nature of integrals and Logarithm in an adjustable and convenient format. The use of $\text{Log(z)}$ probably refer to the principal logarithm, but it is defined on $(-\pi,\pi]$. If I split the integral, what should be done with the second part? Another exercise looking at $e^{it},t\in[0,\pi]$ stated that $Log(e^{it})$ is simply $it$ in a well-defined manner, but here it is quite confusing. I don't understand why and how to change this contour to another. Can you please contribute some theory regarding that problem?",['complex-analysis']
2075187,Extended Pythagorean Theorem,"Extended Pythagorean Theorem 
We all well familiar with the basic Pythagorean statement – I will use a different description that will serve the discussion that follows –
Given two points in $2D$ space, $(a,0)$ and $(0,b)$ with distance $d$ between them, than the relation between $a, b,$ and $d$ follows $a^2+b^2=d^2$ In $3D$ this is extended to three points $(a,0,0)$, $(0,b,0)$ and $(0,0,c)$ with an area of $s$ for the triangle created by the three points - than the following equation holds: $^1/_4(a^2+b^2+c^2)=s^2$ I wonder if this is true for higher dimensions, as example $4D$ - given four points $(a,0,0,0)$, $(0,b,0,0)$, $(0,0,c,0)$, $(0,0,0,d)$ and the volume for the pyramid created by the four point to be $v$ than $^1/_9(a^2+b^2+c^2+d^2)=v^2$ ???","['pythagorean-triples', 'geometry']"
2075217,How to find all analytic functions such that $f(z)=-f''(z)$ [duplicate],"This question already has answers here : $f'' + f =0$: finding $f$ using power series (3 answers) Closed 7 years ago . I'm trying to find all analytic functions such that $f(z)=-f''(z)$ I keep on trying but the solution seems a bit elusive... I'm thinking of $\{a\cdot e^{iz},a\cdot e^{-iz}|a\in \mathbb{C}\}$ but cannot prove it... any advice? :)","['complex-analysis', 'complex-geometry']"
2075242,"If $(\mathcal D(A),A)$ is a linear operator, then $\mathcal D(A)\subseteq\mathcal D(A^{1/2})$","Let $(H,\langle\;\cdot\;,\;\cdot\;\rangle)$ be a $\mathbb R$-Hilbert space. We say that $(\mathcal D(A),A)$ is a linear operator , if $\mathcal D(A)$ is a subspace of $H$ and $A:\mathcal D(A)\to H$ is linear. Assume $(e_n)_{n\in\mathbb N}\subseteq\mathcal D(A)$ is an orthonormal basis of $H$ with $$Ae_n=\lambda_ne_n\;\;\;\text{for all }n\in\mathbb N\tag 1$$ for some $(\lambda_n)_{n\in\mathbb N}\subseteq(0,\infty)$ with $$\lambda_{n+1}\ge\lambda_n\;\;\;\text{for all }n\in\mathbb N\;.\tag 2$$ Let $\alpha\in\mathbb R$, $$\mathcal D(A^\alpha):=\left\{x\in H:\sum_{n\in\mathbb N}\lambda_n^{2\alpha}\left|\langle x,e_n\rangle_H\right|^2<\infty\right\}$$ and $$A^\alpha x:=\sum_{n\in\mathbb N}\lambda_n^\alpha\langle x,e_n\rangle_He_n\;\;\;\text{for }x\in\mathcal D(A^\alpha)\;.$$ How can we show that $\mathcal D(A^1)\subseteq\mathcal D(A^{1/2})$?$^1$ I would prove the claim in the following way: Let $x\in\mathcal D(A^1)$ and $$\lambda_{\text{sup}}:=\sup_{n\in\mathbb N}\lambda_n\;.$$ Case 1: $\lambda_{\text{sup}}<\infty$ and hence $$\sum_{n=1}^N\lambda_n\left|\langle x,e_n\rangle\right|^2\le\lambda_{\text{sup}}\sum_{n=1}^N\left|\langle x,e_n\rangle\right|^2\xrightarrow{N\to\infty}\lambda_{\text{sup}}\left\|x\right\|^2\tag 3$$ by Parseval's identity Case 2: $\lambda_{\text{sup}}=\infty$, hence $$\lambda_n\ge 1\;\;\;\text{for all }n\ge n_1\tag 4$$ for some $n_1\in\mathbb N$ and thereby $$\sum_{n=1}^N\lambda_n\left|\langle x,e_n\rangle\right|^2\le\sum_{n=1}^{n_1-1}\left|\langle x,e_n\rangle\right|^2+\sum_{n=n_1}^n\lambda_n^2\left|\langle x,e_n\rangle\right|^2\tag 5$$ where the second sum on the right-hand side of $(4)$ is convergent for $N\to\infty$ by definition of $\mathcal D(A^1)$ In both cases, we obtain $x\in\mathcal D(A^{1/2})$. However, for some reason I think that my argumentation is too complicated. Is there a simpler one? $^1$ Note that I've explicitly written $\mathcal D(A^1)$; not $\mathcal D(A)$. However, it should be clear that $A$ can be extended to $\mathcal D(A^1)$ if $\mathcal D(A)$ is a proper subset of $\mathcal D(A^1)$.","['functional-analysis', 'operator-theory', 'proof-verification']"
2075263,Connections and Riemannian metrics Application,"I'm getting ready for a Differential Geometry exam and after trying to carry out the exercises from last year's final exam, I have come up with several questions. In the first question of the exam, we are given two pairs of vector fields, $\{X_1,Y_1\}$ and $\{X_2,Y_2\}$, defined as
$$X_1=(1+y^2)\frac{\partial}{\partial x}, Y_1=\frac{\partial}{\partial y}$$
$$X_2=(1+x^2)\frac{\partial}{\partial x}, Y_2=\frac{\partial}{\partial y}.$$
For the first section of the question we have to show that there exist connections $\nabla_1$ and $\nabla_2$ in
 $\mathbb{R}^2$ such that the pairs of vector fields are respectively parallel.  My question is, do I have to compute all of the possible combinations, $\nabla_{1{X_1}}Y_1, \nabla_{1{X_1}}X_1, \nabla_{1{Y_1}}Y_1$, etc. impose that they are equal to zero,
 find the Christoffel symbols and therefore say that both connections will be uniquely defined? Or maybe there exists an easier, less calculistic way to show it? For the second section, we must show that there does not exist any Riemannian metric in $\mathbb{R}^2$ such that it has $\nabla_1$ as its Levi-Civita connexion. If the previous section is true,
 then all I have to do is calculate the torsion $T^{\nabla_1}(X_1,Y_1)$ using the previously calculated Christoffel connection and, if it is not zero, then there will not exist such Riemannian metric. Is there any
 other way to approach this problem? Also, for the third section I have to give an expression of all the Riemannian metrics in $\mathbb{R}^2$ such that $\nabla_2$ is its Levi-Civita connexion. As far as I can see, all I have to do is
 impose $\nabla g=0$, but I really don't know how to apply this for those two vector fields, $\{X_2,Y_2\}$. Then I have to calculate Riemann's curvature tensor but I haven't had any problems with that so far.","['connections', 'differential-geometry']"
2075280,Problem in understanding chain rule.,"The chain rule for total derivative Assume that $g : \mathbb R^n \longrightarrow \mathbb R^m$ is differentiable at $a \in \mathbb R^n$, with total derivative $Df(a)$ and let $b = g(a)$ and assume that $f : \mathbb R^m \longrightarrow \mathbb R^p$ is differentiable at $b \in \mathbb R^m$, with total derivative $Df(b)$. Then the composition function $h = f \circ g : \mathbb R^n \longrightarrow \mathbb R^p$ is differentiable at $a \in \mathbb R^n$, and the total derivative $Dh(a)$ is given by $Dh(a) = Df(b) \circ Dg(a)$, the composition of the linear functions $Df(b)$ and $Dg(a)$. But I can't relate this concept to the chain rule involving partial derivatives as a linear combination.Please help me. Thank you in advance.",['multivariable-calculus']
2075291,"Is the property ""being a derivative"" preserved under multiplication and composition?","Since differentiation is linear, we therefore have that if $f, g: I\to \mathbb{R}$ is a derivative (where $I\subset \mathbb{R}$ is an interval), then so does their linear combination. What if we consider their multiplication and composition? Due to the forms of the product rule of differentiation of product function and chain rule of differentiation of composition, I highly doubt their product or composition necessarily is still a derivative, but I cannot construct counterexamples.","['real-analysis', 'examples-counterexamples']"
2075303,Prove $\sum\limits_{k=1}^n a_k\sum\limits_{k=1}^n \frac1{a_k}\le\left(n+\frac12\right)^2$ then $\max a_k \le 4\min a_k$,"Prove that if
  \begin{align}
&0<a_1,a_2,\dots,a_n \in \mathbb R,
&\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)\le\left(n+\frac12\right)^2\\
\end{align} then
  $$\max_k \space a_k \le 4\times\min_k\space a_k$$ What I've tried was
$$\left(\sum_{k=1}^n a_k\right)\left(\sum_{k=1}^n \frac1{a_k}\right)=n+\sum_{i\ne j}\frac{a_i}{a_j}=n+\sum_{i< j}\left(\frac{a_i}{a_j}+\frac{a_j}{a_i}\right)$$
which didn't help at all. Thanks.","['algebra-precalculus', 'inequality', 'calculus']"
2075307,Why do arrows point towards the codomain in function diagrams?,"The definition of a function given in Kenneth Rosen's Discrete Math Book is Let A and B be nonempty sets. A function from A to B is an assingment of exactly one element of B to each element of A. [Emphasis mine] It seems the author wants us to think of the elements in the codomain being assigned to the elements in the domain, not the elements in the domain being assigned to the ones in the codomain. However, in this book and most others, functions are often illustrated with such diagrams: If it is more useful to think of the elemnts in the codomain as assigned to the elements in the domain, is there a reason why the arrows are not pointing in the other direction?","['algebra-precalculus', 'elementary-set-theory', 'functions', 'discrete-mathematics']"
2075313,How to show that the Dini derivatives of a continuous function is measurable?,"Suppose $F:[a,b]\to\mathbb{R}$ is continuous. Show that
  $$
D^+(F)(x)=\limsup_{h\to 0+}\frac{F(x+h)-F(x)}{h}
$$
  is measurable. This question is related to this one . But specifically I would like to follow the hint in Stein-Shakarchi's Real Analysis : the continuity of $F$ allows one to restrict to countably many $h$ in taking the $\limsup$. I don't quite understand the hint. I guess one might aim at getting
$$
D^+(F)(x)=\lim_{m\to\infty}\sup_{n\geq m}\biggr[F\bigr(x+\dfrac1n\bigr)-F(x)\biggr]\cdot n\tag{1}
$$
But I don't see how to use the continuity of $F$ to get (1).","['real-analysis', 'measure-theory']"
2075347,Show that:$\int_{0}^{1}(1-\sqrt[k]{x})^ndx={1\over {k+n\choose n}}$,"Show that $$I=\int_{0}^{1}(1-\sqrt[k]{x})^ndx={1\over {k+n\choose n}}$$ My try: $x=u^k$ then $dx=ku^{k-1}du$ $$I=k\int_{0}^{1}(1-u)^n u^{k-1}du$$ $v=1-u$ then $dv=-du$ $$I=k\int_{0}^{1}v^n(1-v)^{k-1}dv$$ Using the binomial theorem to expand and integrate term by term is tedious, can anyone show me another quick easy way please? Thank you.","['binomial-coefficients', 'integration', 'definite-integrals']"
2075374,Why is $\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $?,"Let $f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4}) $ And let $ f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} ) $ It appears that $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ Why is that so ? Notice $$\int_0^{2 \pi} \ln(\sin(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) + \frac{5}{4}) dx = Re \int_0^{2 \pi} \ln(\cos(x) - \frac{5}{4}) dx = 0 $$ $$ \int_0^{2 \pi} \ln (\sin(x) - \frac{5}{4}) dx = \int_0^{2 \pi} \ln (\cos(x) - \frac{5}{4}) dx = 2 \pi^2 i $$ That explains the finite values of $\sup $ and $ \inf $ .. well almost. It can be proven that both are finite. But that does not explain the value of their product. Update This is probably not helpful at all , but it can be shown ( not easy ) that there exist a unique pair of functions $g_-(x) , g_+(x) $ , both entire and with period $2 \pi $ such that $$ g_-(n) = f_-(n) , g_+(m) = f_+(m) $$ However i have no closed form for any of those ... As for the numerical test i got about $ln(u) (2 \pi)^{-1}$ correct digits , where $u = m + n$ and the ratio $m/n$ is close to $1$ . Assuming no round-off errors i ended Up with $1.2499999999(?) $ . That was enough to convince me. I often get accused of "" no context "" or "" no effort "" but i have NOO idea how to even start here. I considered telescoping but failed and assumed it is not related. Since I also have no closed form for the product I AM STUCK. I get upset when people assume this is homework.
It clearly is not imho ! What kind of teacher or book contains this ? ——- Example :
Taking $m = n = 8000 $ we get $$ max(f_-(1),f_-(2),...,f_-(8000)) = 1,308587092.. $$ $$ min(f_+(1),f_+(2),...,f_+(8000)) = 0,955226916.. $$ $$ 1.308587092.. X 0.955226916.. = 1.249997612208568.. $$ Supporting the claim. Im not sure if $sup f_+ = 7,93.. $ or the average of $f_+ $ ( $ 3,57..$ ) relate to the above $1,308.. $ and $0,955..$ or the truth of the claimed value $5/4$ . In principe we could write the values $1,308..$ and $0,955..$ as complicated integrals.
By using the continuum product functions $f_-(v),f_+(w)$ where $v,w$ are positive reals. This is by noticing $ \sum^t \sum_i a_i \exp(t \space i) = \sum_i a_i ( \exp((t+1)i) - 1)(\exp(i) - 1)^{-1} $ and noticing the functions $f_+,f_-$ are periodic with $2 \pi$ . Next with contour integration you can find min and max over that period $2 \pi$ for the continuum product functions. Then the product of those 2 integrals should give you $\frac{5}{4}$ . —- Maybe all of this is unnessarily complicated and some simple theorems from trigonometry or calculus could easily explain the conjectured value $\frac{5}{4}$ .. but I do not see it. —— ——
Update
This conjecture is part of a more general phenomenon. For example the second conjecture : Let $g(n) = \prod_{i=0}^n (\sin^2(n) + \frac{9}{16} ) $ $$ \sup g(n) \space \inf g(n) = \frac{9}{16} $$ It feels like this second conjecture could somehow follow from the first conjecture since $$-(\cos(n) + \frac{5}{4})(\cos(n) - \frac{5}{4}) = - \cos^2(n) + \frac{25}{16} = \sin^2(n) + \frac{9}{16} $$ And perhaps the first conjecture could also follow from this second one ? Since these are additional questions and I can only accept one answer , I started a new thread with these additional questions : Why is $\inf g \sup g = \frac{9}{16} $? ---EDIT--- I want to explain better how to get a closed form for these numbers.
I already mentioned that the periods of these functions are $2 \pi$ and how to use that. But those few lines deserve more attention. Basically this is what we do : We use the fourier series $$ f(x) = \ln(\sin(x) + 5/4) = \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k} $$ Now we use the inverse of the backward difference operator
( similar but distinct from the so-called ""indefinite sum"" which is defined as inverse of the forward difference operator ) In other words we solve for $F(x)$ such that $$F(x) - F(x-1) = f(x)$$ We call this the "" continuum sum "" (CS) and write/define : $$ CS(f(x),x) := F(x) $$ $$ CS(f(x),y) := F(y) $$ For clarity : $$ \sum_0^y f(x) = CS(f(x),y) - CS(f(x),-1) $$ $$ \sum_0^0 f(x) = CS(f(x),0) - CS(f(x),-1) = f(0) $$ This operator is linear so we make use of that : $$CS(2 \cos(k(x+\pi/2),x) = \csc(k/2) \sin(k(x+1/2 + \pi/2)) -1.$$ This implies that : $$ CS(f(x),x) = F(x) = - \sum_{k=1}^{\infty} \frac{\csc(k/2)\sin(k(x + 1/2 + \pi/2)) - 1}{2^k k} $$ and $$ G(x) = \frac{d F(x)}{dx} = - \sum_{k=1}^{\infty} \frac{\csc(k/2) \cos(k(x+1/2+\pi/2))}{2^k} $$ Now $\sup f_+ = 7.93.. $ is the supremum of $ \exp(F(x) - F(-1) $ and $\inf f_+ = 0.95..$ is the infimum of $ \exp(F(x) - F(-1) $ .
And both these values are achieved at $x$ such that $G(x) = 0$ . The analogue for $f_-$ works. So the numbers from the OP can ( more or less) be given by these infinite series and hence the whole conjectures can be stated by these infinite series. We also know for instance $$  \sum_{k=1}^{\infty} \frac{1}{k 2^k} = \ln(2) ,  \sum_{k=1}^{\infty} \frac{(-1)^k}{k 2^k} = - \ln(3/2) $$ So that is hopefull. Also the max and min of functions can be given by contour integrals but that might not make things easier ? Many trig identities and symmetry are probably related.
But I see no clear proof. So that is how we compute the values and it might just help. Also notice : $$ t(x) = \ln(\sin(x) - 5/4) =  \ln(-1) + \ln(\sin(-x) + 5/4) = \ln(-1) + \sum_{k=1}^{\infty} \frac{-2 \cos(k(x + \pi/2))}{2^k k} $$ So the other case is no mystery. And ofcourse the case $ \ln(\cos^2(x) - 9/16) $ is also simply related.
( trig addition identities can be used ) EDIT : I wanted to make as little conjectures as possible when I posted this. Just one question/conjecture per post is the usual rule.
But many related conjectures exist.
Many might turn out to be equivalent or have already been shown to be equivalent.
(such as the analogue cosine cases with $\frac{3}{4}$ ) I thought it would be best not to flood with related conjectures and post the most important one. Which I did.
But one comment of Richard can not be ignored. Richard Stanley wrote $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5} $$ He might not be the first to notice this, even if I exclude my mentor who came up with the main conjecture and related ones.
(I think it was also mentioned on chat and comments on MSE) Anyways it is also a nice conjecture. Is it equivalent ? No. Lets use shorthand notations : $x = \sup f_-(n), y = \inf f_+$ . Then combining both conjectures ( the original and the ""Richard"" variant) we get $$x y = \frac{5}{4} , x^2 - y^2 = (x-y)(x+y) = \frac{4}{5}$$ Now this implies we can compute the value of $x$ (or $y$ ). We get an interesting situation here. If 2 conjectures are true than so is the third : $$x y = \frac{5}{4}$$ $$ x^2 - y^2 = (x-y)(x+y) = \frac{4}{5} $$ $x$ is the positive real solution to $5 * 4^2 x^4 - 4^3 x^2 - 5^3 $ or $80 x^4 - 64 x^2 - 125$ . NOTICE the 5's and 4's all over again. (or closed form for $x = \frac{1}{2} \sqrt {\frac{8 + \sqrt {689}}{5}}$ ) ( $689 = 13*53$ if that matters to anyone) This was all known in the $90's$ by my mentor. I call it the Raes-Stanley conjecture. Update ! I had a talk with my mentor why he did not mention the $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{4}{5} $$ part of the Raes-Stanley conjecture. Although he noticed the apparant identity, he does not actually believe that last part. He said that the value converges fast for $n,m > 210000$ to $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5} $$ In fact increasing $n,m$ from $210000$ to $314314$ both gives $$ \sup f_-(n)^2 - \inf f_+(n)^2 = \frac{3.999789007..}{5} $$ so barely any noticeable change, while $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ seems already heuristically confirmed. Testing for $n,m$ smaller than $100000$ might give the wrong impression and might be the cause of the mistake. Numerical coincidence might then lead to wrong conclusions. Roundoff errors might create an effect in the computations but he does not believe the ""Stanley part "" $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ can be saved by those errors. If anyone can prove or argue or compute a higher value than $\frac{3.999789007..}{5}$ , please inform me. Finally the Full Raes conjecture is the slight generalization : Let $f_- (n) = \Pi_{i=0}^n ( \sin(i) - \frac{5}{4}) $ And let $ f_+(m) = \Pi_{i=0}^m ( \sin(i) + \frac{5}{4} ) $ It appears that $$\sup f_- (n) \inf f_+ (m) = \frac{5}{4} $$ and $$\inf |f_- (n)| \sup f_+ (m) = \frac{5}{4} $$ Why is that so ? $|*|$ means the absolute value here. Indeed $$\inf |f_- (n)| = 0.157559...$$ $$\sup f_- (n) =  1.308592...$$ $$\inf f_+ (n) = 0.955225...$$ $$\sup f_+ (n) =  7.933553...$$ $$0.955225... * 1.308592... = 1.25 = \frac{5}{4}$$ $$0.157559... * 7.933553... = 1.25 = \frac{5}{4}$$ Let $g_- (n) = \Pi_{i=0}^n ( \cos(i) - \frac{5}{4}) $ And let $ g_+(m) = \Pi_{i=0}^m ( \cos(i) + \frac{5}{4} ) $ It appears that $$\sup g_- (n) \inf g_+ (m) = \frac{3}{4} $$ and $$\inf |g_- (n)| \sup g_+ (m) = \frac{3}{4} $$","['limsup-and-liminf', 'calculus', 'products', 'geometry', 'fractions']"
2075380,Evaluating $\int_0^{1/2} \log(1-x) \log(1-2x) \ dx$.,"While messing around with Wolfram Alpha, I discovered that $$\int_0^{\frac{1}2} \log(1-x) \log(1-2x) \ dx = 1 - \frac{\pi^2}{24} - \frac{\log(2)}2.$$ I've tried all sorts of standard tricks, but I cannot seem to prove it. Can someone prove this beautiful identity?","['integration', 'definite-integrals', 'calculus']"
2075416,An eigenvalue optimization problem,"Given a positive definite matrix $A \in \mathbb{R}^{n \times n}$, I would like to compute
$$\max_{x \in \Delta_n} \lambda_{\rm min} \left( {\rm diag}(x) A \right)$$
where $\Delta_n$ is the simplex, i.e., $\Delta_n  = \{x \in \mathbb{R}^n ~|~ x_i \geq 0, \sum_i x_i = 1\}$ ${\rm diag}(x)$ is the standard operation which transforms the vector $x \in \mathbb{R}^n$ into a diagonal matrix by putting the elements of $x$ on the diagonal. $\lambda_{\rm min}(W)$ returns the smallest eigenvalue of the matrix $W$. In this case, since ${\rm diag}(x)A$ is conjugate to ${\rm
   diag(x)}^{1/2} A {\rm diag(x)}^{1/2}$ and $A$ is positive definite,
we have that ${\rm diag}(x)A$ has real eigenvalues, and it makes
sense to talk about the smallest eigenvalue. I have a vague suspicion there might be an explicit formula for the solution of this problem, though I'm not sure. If that was the case, that would be great. If not, is there a polynomial-time algorithm which computes the optimal solution? Can this maybe be reduced to a semi-definite program? Something to go on is that $\lambda_{\rm min}(W)$ is a concave function of the symmetric matrix $W$. On the other hand, I haven't been able to translate this into concavity or convexity of $\lambda_{\rm min}({\rm diag}(x)A)$ as a function of $x$; the issue is that ${\rm diag}(x)A$ is not symmetric, so I'm not sure how useful this is.","['nonlinear-optimization', 'optimization', 'matrices', 'algorithms', 'linear-algebra']"
2075434,Find coefficient of $x^n$ in $(1+x+2x^2+3x^3+.....+nx^n)^2$,Find coefficient of  $x^n$ in $(1+x+2x^2+3x^3+.....+nx^n)^2$ My attempt:Let $S=1+x+2x^2+3x^3+...+nx^n$ $xS=x+x^2+2x^3+3x^4+...+nx^{n+1}$ $(1-x)S=1+x+x^2+x^3+....+x^n-nx^{n+1}-x=\frac{1-x^{n+1}}{1-x}-nx^{n+1}-x$ $S=\frac{1}{(1-x)^2}-\frac{x}{1-x}=\frac{1-x+x^2}{(1-x)^2}$. (Ignoring terms which have powers of x greater than $x^n$) So one can say that coefficient of $x^n$ in $(1+x+2x^2+3x^3+.....+nx^n)^2$ =coefficient of $x^n$ in $(1-x+x^2)^2(1-x)^{-4}$ Is there a shorter way.,"['algebra-precalculus', 'binomial-theorem']"
2075449,Determine whether the $x_n>0$ converges if $x_n^2 - 2x_n - 15 +4^{-n}$ converges,"For some sequence $x_n > 0$, we know that sequence $x_n^2 - 2x_n - 15 +4^{-n}$ converges. Does this mean that $x_n$ also converges? If so, proof that it is convergent, otherwise provide a counterexample. First of all, let's define $y_n = x_n^2 - 2x_n - 15 +4^{-n}$, as $n \to \infty \Rightarrow y_n = x_n^2 - 2x_n - 15 $. Assume $\lim y_n = a$, then $a = x_n^2 - 2x_n - 15$. This equation gives two roots: $x_n = 1 + \sqrt{4a + 16}$ and $x_n = 1 - \sqrt{4a + 16}$. And we know that both of them should be $>0$, since $x_n > 0$. From $1 - \sqrt{4a + 16} > 0$, we get $-4 \leq a < \frac{-15}{4}$. So $y_n$ converges, when $x_n$ is either $1 - \sqrt{4a + 16}$ or $1 + \sqrt{4a + 16}$. So I can take $x_n = \{ 1 - \sqrt{4a + 16}, 1 + \sqrt{4a + 16}, 1 - \sqrt{4a + 16}, 1 + \sqrt{4a + 16}, \dotso \}$, so $y_n$ still converges, while $x_n$ is not. Also if $1 - \sqrt{4a + 16} = 1 + \sqrt{4a + 16}$, then $x_n$ also converges. Despite all my thoughts, I'm pretty sure that this solution have some mistakes, for example, I'm not sure that I can cancel out $4^{-n}$.","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2075464,Stuck with this limit of a sum: $\lim _{n \to \infty} \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right)$.,"Here's the limit: $$\lim _{n \to \infty}  \left(\frac{a^{n}-b^{n}}{a^{n}+b^{n}}\right)$$ The conditions are $b>0$ and $a>0$. I tried this with the case that $a>b$: $$\lim _{n \to \infty}  \left(\frac{1-\frac{b^{n}}{a^{n}}}{1+\frac{b^{n}}{a^{n}}}\right)$$ It gives me the result $1$. But, in the case of $b>a$, I don't find a solution. Thanks for your attention.","['summation', 'limits', 'calculus', 'analysis']"
2075487,Find $\lim\limits_{x\to\pi/4} \frac{1-\tan(x)^2}{\sqrt{2}*\cos(x)-1}$ without using L'Hôpital's rule.,"Find $$\lim_{x\to\pi/4} \frac{1-\tan(x)^2}{\sqrt{2}\times \cos(x)-1}$$ without using L'Hôpital's rule. I can solve it using L'Hôpital's rule, but is it possible to solve it without using L'Hôpital's rule?","['calculus', 'limits']"
2075490,Continuity and Right Derivative,"$f(x)$ is continuous on $\mathbb{R}$, $f_+'(x)$ exists and is continuous on $\mathbb{R}$. Prove that $f'(x) $ exists on $\mathbb{R}$.
It's OK that $|f_+'(x)-f_+'(y)|<\epsilon$ if x and y are enclosed in a small interval. For a fixed x, we have $|\frac{f(y)-f(x)}{y-x}-f_+'(x)|<\epsilon$ if $y\in (x,x+\delta)$ but $\delta$ depends greatly on $x$. I wonder if there are better methods.","['continuity', 'real-analysis']"
2075510,Graduate Functional Analysis Course : Background Required [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I'm an undergraduate with some measure and integration theory background. The undergrad analysis course I took at my institution covered the equivalent of the first two chapters of Folland's Real Analysis (ie introductory measure theory constructions, the convergence theorems for Lebesgue intergrals and Fubini-Tonelli) along with parts of chapters 5 and 6 on Elementary Functional Analysis and Lp Spaces. I was told that with some preparation I could probably take the second semester graduate introductory course on Functional Analysis that follows the first semester measure theory course. However, I am not certain what exactly I am to focus on in my preparation. Currently, I intend to do: 1) Chapter 3 of Folland which covers the Lebesgue Differentiation and the Radon-Nikodym Theorems since this appears to be the one major area which the intro grad course covered but I didn't. 2) Review Point Set Topology from say, Munkres (I'm mostly familiar with this up till Arzela Ascoli and Stone-Weierstrass). 3) Read an undergrad book like Kreyzig's. Is there anything else I should focus on? The Functional Analysis course will be using Functional Analysis, Sobolev Spaces and Partial Differential Equations by Brezis. We probably won't cover the entire book. Thanks in advance.","['functional-analysis', 'real-analysis', 'advice', 'analysis']"
2075511,Convergence of product of convergent series,"Given $a_n, b_n$ such that $\exists N   \forall n>N  a_n, b_n \geq 0$ I want to show that if $\sum_{n=0}^{\infty}a_n$ and $\sum_{n=0}^{\infty}b_n$ converge then $\sum_{n=0}^{\infty}a_n b_n$ also converges. I tried to show it in the following way, if $\sum_{n=0}^{\infty}b_n$ converges, then $\lim_{n\rightarrow\infty}b_n=0$, so there exists $n_0$ such that $b_{n_0}\geq b_n$ for all $n\gt n_0$, therefore $a_n b_{n_0}\geq a_n b_n$, so that $\sum_{n=0}^{\infty}a_n b_n\leq \sum_{n=0}^{n_0}a_n b_n+b_{n_0}\sum_{n=n_0+1}^\infty a_n$, and as my series is bounded by sum of finitely many terms and tail of convergent series multiplied by a constant it's also convergent. Is my proof correct, or are there any flaws in my reasoning?","['sequences-and-series', 'convergence-divergence']"
2075524,Strictly increasing sequences of positive integers,"Let $\alpha$ and $\beta$ be nonnegative integers. Suppose the number of strictly increasing sequences of integers $a_0,a_1,\dots,a_{2014}$ satisfying $0 \leq a_m \leq 3m$ is $2^\alpha (2\beta + 1)$. Find $\alpha$. We need $0 \leq a_0 \leq 0, 0 \leq a_1 \leq 3, 0 \leq a_2 \leq 6,\ldots,0 \leq a_{2014} \leq 6042$. We also need to make sure that the sequence is strictly increasing. How do we count the number of sequences with both conditions satisfied?",['combinatorics']
2075531,Binomial or Uniform Probability?,"What is the probability of rolling exactly two sixes in $7$ rolls of a die? I know this is a binomial probability. $P(X=2)=\binom{7}{2}(1/6)^2(5/6)^5$. By the definition of probability formula ""Probability formula is the ratio of number of favorable outcomes to the total number of possible outcomes."" Then why probability of rolling exactly two sixes in $7$ rolls of a die isn't $=2/42$? The denominator is $42$ because in one die there are $6$ faces, and in $7$ rolls of a die, there are $7\times 6=42$ faces.",['probability']
2075565,Gauss Digamma theorem proof,"How to prove The Gauss Digamma theorem Let $p/q$ be a rational number  with $0<p<q$ then $$\psi\left(\frac{p}{q}\right) =
 -\gamma-\log(2q)-\frac{\pi}{2}\cot\left(\frac{p}{q}\pi\right)+2\sum_{k=1}^{q/2-1}
 \cos\left(\frac{2\pi pk}{q}\right)\log\left[ \sin \left(\frac{\pi
 k}{q}\right) \right]$$ If some how it can make it simpler you can take the case $p=1$. I feel I should use the following formulas $$\sum_{r=1}^m \psi \left( \frac{r}{m}\right) = -m(\ln(m)+\gamma)$$ $$\sum_{r=1}^m \psi \left( \frac{r}{m}\right)  \mathrm{exp}{\frac{2\pi rk i}{m}}= m\log\left(1-\mathrm{exp} \frac{2k\pi i}{m}\right)$$ Maybe a Fourier series expansion is my best bet but it is not my best category. Note I am aware of How to prove Gauss's Digamma Theorem? but seems like that question didn't recieve that much attenstion also the supposed-to-be accepted answer provides a link that doesn't work anymore. I think it is better to have our sloution here for future refrences.","['special-functions', 'integration', 'sequences-and-series', 'digamma-function']"
2075577,Find the maximum power of $24$ in $(48!)^2$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Find the maximum power of $24$ in $(48!)^2$ ? How to approach for such questions ?","['number-theory', 'discrete-mathematics']"
2075580,proving $t^6-t^5+t^4-t^3+t^2-t+0.4>0$ for all real $t$,"proving $t^6-t^5+t^4-t^3+t^2-t+0.4>0$ for all real $t$ for $t\leq 1,$ left side expression is $>0$ for $t\geq 1,$ left side expression $t^5(t-1)+t^3(t-1)+t(t-1)+0.4$ is $>0$ i wan,t be able to prove for $0<t<1,$ could  some help me with this",['algebra-precalculus']
2075613,For what value of $a\ f(x) = e^x + ax^3$ has inflection points,I took the second derivative: $f^{''}(x)= e^x + 6ax$. So $f$ has inflection points $\iff$ there exists $x_0$ such that $e^{x_0} + 6ax_0 = 0$. I don't know how to go further and find such $a$ for which exists at least one positive root of that equation.,"['derivatives', 'real-analysis', 'calculus']"
2075631,"equivalence relation partitioning for the set $[0,1]$","I have a question regarding equivalence partitioning in the interval of $[0,1]$. I was reading the following: Define an equivalence relation on $[0,1]$ by: $x \sim{y}$ if and only if the difference $y-x$ is rational. This relation partitions the interval $[0,1]$ into a disjoint union of equivalence classes. Let H be a subset of $[0,1]$ consisting of precisely one element from each equivalence class. Now since $H$ contains an element of each equivalence class, we see that each point in $[0,1]$ is contained in the union $ \bigcup_{r\in [0,1] r rational} (H \bigoplus r)$ of shifts of $H$. where $H \bigoplus r$ is defined as  $H \bigoplus r = \{ a+r; a\in{H}, a+r \leq 1\} \cup \{a+r-1; a\in H, a+r>1\}$ My question is specifically: how each point in $[0,1]$ is contained in the union of $H \bigoplus r$ and also how the equivalence relation 
$x \sim y$ if and only if the difference $y-x$ is rational"" would partition the interval $[0,1]$. Thank you","['general-topology', 'equivalence-relations', 'set-partition', 'elementary-set-theory']"
2075635,Understanding Cartesian product in naive set theory,"I read this definition in Analysis by Zorich. I am confused with this. For example, let $X=\{1,2\},Y=\{a,b\}$. Then $\mathcal P(X)\bigcup\mathcal P(Y)=\{\emptyset,\{1\},\{2\},\{1,2\},\{a\},\{b\},\{a,b\}\}$. But $(1,a):=\{\{1\},\{1,a\}\}$ is not a subset of $\mathcal P(X)\bigcup\mathcal P(Y)$ since $\{1,a\}$ is not in $\mathcal P(X)\bigcup\mathcal P(Y)$. How should I understand this? Any help will be appreciated. It seems to be a typo. Please check the answer below:)",['elementary-set-theory']
2075639,How do I solve $\frac{\mathrm dy}{\mathrm dx} = e^ {9y-x}$?,"$\dfrac{\mathrm dy}{\mathrm dx} = e^ {9y-x}$ I substituted $v = 9y-x$ (homogeneous). And I tried separable integration, but why do I get the wrong answers? We can separate it, can't we?","['ordinary-differential-equations', 'calculus']"
2075646,"Given a quadrilateral, prove it is a square.","Consider this game. You have a perfect ruler and a perfect protractor. You are also able
  to observe perfectly if two lines are parallel. Is it possible to prove that a given quadrilateral is a square, using
  exactly 3 steps? For example, a proof using 4 steps would be this. Observe that opposite sides are parallel (two steps). The angle between the diagonals is 90 degrees. The angle between any two adjacent sides is 90 degrees. My gut feeling is that it is not possible to do this in 3 steps and that the minimum would be 4. But how would one prove this?","['euclidean-geometry', 'alternative-proof', 'geometry']"
2075647,"Number of zeroes of solution to $y''(x)+e^{x^2}y(x)=0$ in $[0,3π]$","The question is to investigate the number of zeroes of $y''(x)+e^{x^2}y(x)=0$ in $[0,3π]$. Solving this ODE would not be an easy task as one has to use the power series solution and then investigating the zeroes of the solution will require more analysis. I thought it to compare this ODE with the standard $y''(x)+y(x)=0$ whose solution has three or four zeroes in the interval $[0,3π]$. Since the coefficient of $y(x)$ is $e^{x^2}\ge 1$ for $x\in [0,3π]$ so the solution of given ODE must have atleast three zeroes in $[0,3π]$. However, what  I thought was in the lights of Sturm-Comparison theorem so I am not sure. Am I correct to interpret this?",['ordinary-differential-equations']
2075657,Prove the existence of answer:,"Let $K$ be closed and convex. also $F:K \subseteq \mathbb R^n \to \mathbb R^n $ be a continuous function.If foe every $x,y \in K$ we have $(x-y)^T(F(x)-F(y))\ge \alpha ||x-y||^2  ;\alpha\gt0 $ we want to prove that there exist a unique $x^*$ such that $F(x^*)^T(x-x^*)\ge 0$. It is obvious proof of uniqueness of solutions.Assuming there are two answers we have $(x^*-x')^T(F(x')-F(x^*))\ge\alpha ||x-y||^2$ Which is a contradiction. But how to prove that there exist answer?","['functional-analysis', 'complex-analysis', 'real-analysis', 'analysis']"
2075661,Residue of $e^{1/z}$?,"I have a function that diverges like $e^{1/z}$ in the origin, and I'm integrating the function a closed contour in the complex plane. The point $z=0$ lies inside the contour, so if I integrate by residues the singularity at $z=0$ (a pole?) must be taken into account. How would you do that?","['residue-calculus', 'complex-integration', 'complex-analysis', 'analysis', 'exponentiation']"
2075664,In how many $4$-digit numbers the sum of two right digits is equal to the sum of two left digits,In how many $4$-digit numbers the sum of two right digits is equal to the sum of two left digits. My attempt :We should find number of two pairs that can be digits of this number for choosing the place of digits we have $*8$ (We should notice we cant have $0$ in the beginning.But the biggest problem is finding such pairs.How can I find them?,['combinatorics']
2075681,How to solve $1+\frac12-\frac13+\frac14-\frac15-\frac16+\frac18+\ldots+\left(\frac n7\right)\frac1n+\ldots$?,"$$1+\frac12-\frac13+\frac14-\frac15-\frac16+\frac18+\ldots+\left(\frac n7\right)\frac1n+\ldots$$
where $\left(\frac n7\right)$ is Legendre symbol.
I think its about algebraic number theory, but I can't find relative problem on book. If the signs were all +, the series would diverge as a harmonic sum. If the signs would alternate, it is well known that the sum of the series would be $\ln 2$. Here the signs follow a periodically repeating pattern of length seven:
$$++-+--0,$$
where the $0$ at end indicates that terms $1/n$ with $n$ divisible by seven are missing altogether.","['number-theory', 'algebraic-number-theory', 'sequences-and-series', 'legendre-symbol']"
2075694,Forming seven letter words by using the letters of the word $\text{SUCCESS}$,"If you have patience please read through the whole post. I have made clear what was my line of thought for each step that I did. Problem Statement:- How many words of seven letters formed by the letters of the word $\text{SUCCESS}$ so that $\text{(i)}$ the two $\text{C's}$ are together but not the two $\text{S's}$. $\text{(ii)}$ neither the two $\text{C's}$ nor the two $\text{S's}$ are together. My Attempt at a solution:- $\text{(i) Part - 1}$ $\text{1}^\text{st}$ Approach:- Since we have to arrange the letters in such a way that the $\text{C's}$ occur together but the $\text{S's}$ don't, so we first combine the two $\text{C's}$ together into one entity, reducing the total letters(or entities) to $6$. Since the $\text{S's}$ shouldn't occur consecutively(that's what I infer from the ""the two \text{S's} should not be together"" part in the question"") we first arrange the remaining letters(or entities) which are 
$\{\text{U,CC,E}\}$. $$\_\;\text{U}\;\_\;\text{CC}\;\_\;\text{E}\;\_$$ After arranging these letters we see that there are $4$ gaps in which the $3 \text{ S's}$ can be arranged so that none of the $\text{S's}$ are together, also the letters that were arranged before the $\text{S's}$ can be arranged among themselves in $3!$ ways. So, the total number of ways in which we can arrange the given letters so that the two $\text{C's}$ occur together but no two $\text{S's}$ occur together$=\displaystyle\binom{4}{3}\cdot3!=24$ But the textbook gives the answer as $12$. Now, just to check whether my first attempt for the $\text{1}^\text{st}$ part of the question was correct or not I decided to approach it differently. $\text{2}^\text{nd}$ Approach:- Number of ways in which the two $\text{C's}$ occur together$=\dfrac{6!}{3!}$ Since no two $\text{S's}$ must occur together, hence The number of ways in which no two $\text{C's}$ and no two $\text{S's}$ occur together$=\text{Total number of permutations $-$ Number of ways in which two $\text{C's}$ and two $\text{S's}$ occur together}$ Now, lets find the number of ways in which two $\text{S's}$ and two $\text{C's}$ occur together. The number of ways will be that in which we group two $\text{C's}$ and two $\text{S's}$ into, say, one super $\Bbb{C}$ and one super $\Bbb{S}$. So, we have to arrange $\{\text{S,U,$\Bbb{C}$,E,$\Bbb{S}$}\}$, which can be done in $5!$ ways. After starting out on working on this approach I found that there is a little confusion that I have that whether three $\text{S's}$ could occur together or not. If they can, then we have to exclude the permutation in which three $\text{S's}$ and two $\text{C's}$ occur together simultaneously from the permutation of two $\text{C's}$ and two $\text{S's}$ (so that they maybe counted in the total count) which is given by $4!$. So, the number of ways in which no two $\text{C's}$ and no two $\text{S's}$ occur together becomes $$\dfrac{6!}{3!}-(5!-4!)=24$$. But, if the three $\text{S's}$ cannot occur together then, we have the permutation of the letters under the assumed condition as $$\dfrac{6!}{3!}-5!=0$$ This approach turned out to be more confusing considering how I got $0$ ways when the three $\text{S's}$ cannot occur together which I think is not correct, because a simple counter example is $\text{SUSCCES}$. $\text{(ii) Part - 2}$ $\text{1}^\text{st}$ Approach:- In this Approach I used the number of ways in which the two $\text{C's}$ occur together but not the two $\text{S's}$, so this can also act as the verification for the answer of the $1^\text{st}$ part. First, we will find the number of ways in which the two $\text{S's}$ don't occur together, which can be found by the gap method as follows
$$\_\;\text{U}\;\_\;\text{C}\;\_\;\text{C}\;\_\;\text{E}\;\_$$ Since we have $5$ gaps, so the $3\text{ S's}$ can be placed in $\binom{5}{3}$ ways followed by the arrangement of the prearranged letters in $\dfrac{4!}{2!}$. So, we get the number of ways in which the two $\text{S's}$ don't occur together as $$\binom{5}{3}\cdot\dfrac{4!}{2!}=120$$ Now, from this we exclude the number of ways in which the two $\text{C's}$ occur together with the two $\text{S's}$ not occurring together simultaneously, which is given by:- if we consider the first approach of the first part of the question, and the first assumption of the second approach, as correct then$=\displaystyle\binom{4}{3}\cdot3!=24$ If we consider the second assumption of the second approach of the first part as correct, then $0$ (well lets leave this one, we already know there is some blunder in there that I have done) So, the total number of ways in which neither the two $\text{C's}$ nor the two $\text{S's}$ are together=$$\binom{5}{3}\cdot\dfrac{4!}{2!} - \displaystyle\binom{4}{3}\cdot3!=120-24=\boxed{96}$$ Viola, the answer of the second part is correct, so the book gives the wrong answer for the first part. Now, considering the book gave the wrong answer and the first assumption of the second approach in the first part was correct(yeah, this looks like a maze to me now to indicate all this XD), I thought of applying some PIE for my second method. $\text{2}^\text{nd}$ Approach:- Since this approach is inspired by the first assumption in 2nd approach for the first part, so I assumed that what the problem implies by the statement ""the two $\text{S's}$ are together"" is that only two of the $\text{S's}$ should occur together at a time. So, consider the following sets
$$A\rightarrow \text{All the permutations that include two C's together}\\
B\rightarrow \text{All the permutations that  include only two S's together}$$ So, $$|A|=\dfrac{6!}{3!}=120\\
|B|=\dfrac{6!}{2!}(\rightarrow\text{two S's occur together})-\dfrac{5!}{2!}(\rightarrow\text{three S's occur together})=300\\
|A\cap B|=5!(\rightarrow\text{two C's and two S's occur together simultaneously})-4!(\rightarrow\text{two C's and three S's ocurr together simultaneously})=96$$ Now, as per PIE, we have 
$$|A\cup B|=|A|+|B|-|A\cap B|=120+300-96=420-96$$ Now, $\left(A\cup B\right)^c=\left(A^c\cap B^c\right)$, and $\left(A^c\cap B^c\right)\implies \text{two C's don't occur together and two S's don't occur together}$
$$\|(A\cup B)^c|=|(A^c\cap B^c)|=\dfrac{7!}{3!\times2!}-|A\cup B|=420-(420-96)=\boxed{96}$$ Conclusion: What bugs me. What's wrong with the second assumption of the second approach in the
answer to the first part of the question that I am getting $0$ as the
answer given that I have already shown a counter example that tells
that there are infact more than $0$ ways. I find the language of the question quite confusing. The ""the two
$\text{S's}$"" part confuses me the most. How does it mean that only
two of the $\text{S's}$ should occur together and not three of them.
In my opinion, the mention of the THE in "" THE two
$\text{S's}$"" tells that only those two $\text{S's}$ should come
together that were initially together in the word $\text{SUCCESS}$
were the $\text{S's}$ considered to be different, but as the S's are
indistinguishable so that makes the statement ""only two of the three
$\text{S's}$ should occur together"". Lastly, I found the same question as the second part of the question
and one of the
solutions though
looked very lucrative, I can't seem to understand how it was
formulated, if you think that it can be understood by a student
having knowledge of highschool mathematics and what I consider to be
a little knowledge of engineering mathematics then, can you enlighten
me as to how it was formulated.","['permutations', 'combinatorics', 'proof-verification']"
2075745,Why is the solution to $x-\sqrt 4=0$ not $x=\pm 2$?,"If the equation is $x-\sqrt 4=0$, then $x=2$. If the equation is $x^2-4=0$, then $x=\pm 2$. Why is it not $x=\pm 2$ in the first equation?","['algebra-precalculus', 'radicals']"
2075757,A torsion-free divisible module over a commutative integral domain is injective,"Show that a torsion-free divisible module over a commutative integral domain is injective. ( $M$ is torsion-free if $rx=0$ , $0\not=x\in M \implies r=0$ .) I tried to use Baer's criterion for injectivity to prove that $M$ is injective but I don't see how $R$ being a commutative integral domain gives me additional information. The first two conditions, namely $M$ being torsion-free and divisible tell me about the injectivity of the maps $\varphi_x(r)=rx$ and $\varphi_r(x) = rx$ for $r\in R-\{0\}$ and also the surjectivity of $\varphi_r$ but I don't know how to proceed from this point.","['injective-module', 'abstract-algebra', 'modules']"
2075773,"If a determinant of a matrix is 0, is the graph formed from it acyclic?","I read an answer on the Mathematica subforum that explained that a graph's adjacency matrix can be triangular (except the diagonal) if and only if the graph is a DAG. It is easy to see that the determinant of said matrix will always be 0. Does this mean that every binary matrix (n x n) with a determinant of 0 will be an adjacency matrix to a DAG? If not, then can you provide a counterexample?","['matrices', 'graph-theory', 'adjacency-matrix', 'determinant']"
2075804,"Let $f$ be a function on $\mathbb{R}$, such that it has a limit. Show that it is bounded.","Suppose a continuous function $f : \mathbb{R} \mapsto \mathbb{R} $ such that it has a limit $l$ at $±\infty$. I had a question that asked me to show that $f$ is bounded on an interval $[-M;M] \subset \mathbb{R}$. To do that I used the fact that $[-M;M]$ is closed and bounded, thus is a compact, and as $f$ is continues, it shows that it is bounded on that interval. Now it is asked to show that $f$ is bounded on $\mathbb{R}$.
I can easily prove the case when $l \not=±\infty$ by using the definition of a limit and the fact that it is bounded on $[-M;M]$. We have:
$\forall \epsilon > 0 \: \exists \delta>0 \:  |x-x_0|<\delta \implies |f(x)-l|<\epsilon  $ As it's true for all epsilon, it is also true for $\epsilon = 1$.
Thus we have:
$\exists \delta>0 \:  |x-x_0|<\delta \implies |f(x)-l|<1$ Thus we have $(l-1)<f(x)<(1+l)$
Thus $\forall x>x_0$, $f$ is bounded. Now, as I have shown that $f$ is bounded on all intervals of type $[-M;M]$, we can simply put $x_0=M$, and so $f$ is bounded on the whole $\mathbb{R}$. I hope this proof is correct. Now what I struggle to understand is how come $f$ can be bounded if $l=\pm\infty$ (there is no condition on $l$, so I suppose it can also denote infinity).
Any tips and criticism will be highly valuable.","['real-analysis', 'limits', 'proof-verification', 'epsilon-delta', 'general-topology']"
2075828,Show that:$\sum\limits_{n=1}^{\infty}{n\over (4n^2-1)(16n^2-1)}={1\over 12}(1-\ln{2})$,"Show that $$\sum_{n=1}^{\infty}{n\over (4n^2-1)(16n^2-1)}={1\over 12}(1-\ln{2})$$ My try: We split into partial decomposition $$n={A\over 2n-1}+{B\over 2n+1}+{C\over 4n-1}+{D\over 4n+1}$$ Setting $n={1\over 2}$, ${-1\over2}$ we have $A={1\over3}$ and $B={-1\over 3}$ Finding C and D is a bit tedious I wonder what is the closed form for $$\sum_{n=1}^{\infty}{1\over an+b}=F(a,b)?$$ This way is not a good approach. Can anyone help me with a better approach to tackle this problem? Thank you.",['sequences-and-series']
2075832,Optional sampling theorem with possibly infinite stopping times,"I have a probability space $(\Omega, \mathcal{F}, P)$ and assume that $(M_n, \mathcal{F}_n)$ is a martingale on this space satisfying $M_n\geq0$ for all $n\geq1$. I also have two stopping times $\sigma_1$ and $\sigma_2$, with $\sigma_1 \leq \sigma_2$. These stopping times have positive probability of being infinite. The optional stopping theorem cannot be applied directly, but I would like to like to show that
$$
 E(M_{\sigma_2} \mid \mathcal{F}_{\sigma_1})
 =
 M_{\sigma_1 } \qquad \text{a.s.}
$$
What I know is that 
$(M_{\sigma_2 \wedge n}, \mathcal{F}_{\sigma_2 \wedge n})$ is also a martingale and that $(M_{\sigma_2 \wedge n})$ is uniformly integrable. I have applied to the optional sampling theorem to the martingale
$(M_{\sigma_2 \wedge n}, \mathcal{F}_{\sigma_2 \wedge n})$
to conclude that
$$
E(M_{\sigma_2 \wedge n} \mid \mathcal{F}_{\sigma_1 \wedge n})
=
M_{\sigma_1 \wedge n} \qquad \text{a.s.}
$$
I feel I am close to the result. How can I take the above expression ""to the limit""? My problem is that I do not know how to tackle the minimum operator in the filtration. What I also know is that $M_{\sigma_2}$ closes the martingale 
$(M_{\sigma_2 \wedge n}, \mathcal{F}_{\sigma_2 \wedge n})$, i.e. 
$E(M_{\sigma_2} \mid \mathcal{F}_n) = M_{\sigma_2 \wedge n}$ because $(M_{\sigma_2 \wedge n})$ is uniformly integrable... but I am not sure how to proceed. This question is part of a bigger assignment I am doing, but I think there are enough assumptions here to conclude the desired result.","['probability-theory', 'martingales', 'stopping-times']"
2075854,Trouble understanding Hadamard's proof that there are no zeros of the zeta function with real part 1,"So I was reading Hadamard's 1896 paper, here (fr) , which is the most celebrated proof of the fact that there are no zeros of the zeta function with real part 1 (and therefore of the prime number theorem).  My French is a little shoddy - that may be the problem, but I don't think so.  The first four articles are dedicated to proving the lack of zeros, the next six to extending the proof to related functions, and the remainder to using it to demonstrate the PNT and some corollaries.  It's article 3 in particular I'm confused by - it seems his argument goes like this: Suppose that $1 + ti$, $t$ real, is a zero of the zeta function. For some $\alpha < \frac{\pi}{2}$, consider the primes for which $t\log p$ is within $\alpha$ of an odd multiple of $\pi$; call them $q$. By some fairly simple algebra (this part I did follow), if $\rho = \limsup_{s\to1}\frac{\sum_q q^s}{\sum_p p^s} < 1$ then $\Re[\log\zeta(1+\epsilon+it)] \ge -(\rho + (1 - \rho)\cos\alpha)\log\zeta(1+\epsilon)$, $\epsilon$ being as always a sufficiently small positive real. Then comes my problem: Hadamard writes ""...ce qui serait en contradiction avec l'hypothèse $\zeta(1+ti) = 0$, ainsi qu'il a été remarqué au numéro précédent,"" which I understand to mean ""...which would be in contradiction to the hypothesis $\zeta(1+ti) = 0$, as was noted in the previous article (paragraph? 'numéro')."" (Article 4 then goes on to show that the limit being 1 would mean a pole at $1+2ti$, and that part was fine.) Article 2 does mention that if there were a zero there, the approach parallel to the real axis would approach similarly to $\log(s-1)$, and that makes sense, but it seems all that's established here is a constant multiple, and that's allowed, isn't it?  What am I missing? To be clear, I'm not asking for a proof of this fact; I know an easier proof, but due to the place it holds in history, I'm looking to understand this one.","['number-theory', 'riemann-zeta', 'proof-explanation']"
2075862,A knotty problem in trigonometric equations,"Solve the equation $$\tan^4 x + \tan^4 y  + 2\cot^2x \cot^2 y = 3 + \sin^2 (x+y) $$ for the values of x and y . Partial Solution :
One can easily guess that $ x = \pi/4 $  and $y =  \pi/4$ satisfy the given equation. So one solution is $x = n \pi \pm \pi/4 , n \in \mathbb{z} $ and $ y = n \pi \pm \pi/4 , n \in \mathbb{z} $. But how do we prove that this is the only solution?",['trigonometry']
2075878,Prove that these two lines are perpendicular.,"Consider a parallelogram $WXYZ$, with points $A$ and $B$ on sides $WX$ and $XY$ respectively, so that $\angle WAZ = \angle YBZ$. Let the midpoint of $WY$ be $M$. Prove that $OM$, where $O$ is the centre of the circle $AXB$, is perpendicular to $WY$. EDIT: In response to Mick's solution. I think you need to explain why the equal angles means the two lines are parallel. I think your solution breaks down when you say that KMN is a straight line without proof. Here is a picture where your first two paragraphs are correct, but doesn't solve the problem because the original angles are not the same.",['geometry']
2075891,Properties of mutual singular measures,"Let $\mu$ be a positive measure and $\nu_1, \nu_2$ arbitrary measures, all defined on the same measurable space $(S,\Sigma)$. We say that two arbitrary measure $\mu, \nu$ are mutually singular (notation $\nu \perp \mu$) if there exist disjoint sets $E$ and $F$ such that $\nu(A)=\nu(A \cap E)$ and $\mu(A) = \mu(A \cap F)$ for all $A \in \Sigma$. We say that $\nu$ is absolutely continuous w.r.t. $\mu$ (notation $\nu \ll \mu$), if $\nu(E) = 0$ for every $E \in \Sigma$ with $\mu(E)=0$. Now, I want to prove that If $\nu_1 \perp \mu$ and $\nu_2 \perp \mu$, then $\nu_1 + \nu_2 \perp \mu$. If $\nu_1 \ll \mu$ and $\mu_2 \perp \mu$, then $\nu_1 \perp \nu_2$. To start with 1.: $\exists E,F, G,H \in \Sigma$ such that
\begin{align}
\nu_1(A) = \nu_1(A \cap E)\  \text{ and }\  \mu(A) = \mu(A \cap F)\ \text{ for all $A \in \Sigma$}.\\
\nu_2(B) = \nu_2(B \cap G)\  \text{ and }\  \mu(B) = \mu(B \cap H)\ \text{ for all $B \in \Sigma$}.\\
\end{align}
So, for which sets $C,I,J \in \Sigma$ do I have to show that $(\nu_1 + \nu_2)(C) = (\nu_1 + \nu_2)(C \cap I)$ and $\mu(C) = \mu(C \cap J)\ \text{ for all $C \in \Sigma$}$? Secondly, I do not have any suggestions for 2. Do you have any suggestions?","['real-analysis', 'measure-theory', 'singular-measures']"
