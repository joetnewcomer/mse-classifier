question_id,title,body,tags
3877130,Proving Positivity of Extension of Linear Functional on $C(X)$,"Question I am currently working through Royden's proof that any continuous linear operator $L$ on $C(X)$ can be written as the difference of two positive linear functionals on $C(X)$ , where $X$ is a compact Hausdorff space equipped with the maximum norm. The proof starts by showing that $L_+ : C(X) \to \mathbb{R}$ with $$ L_+(f) := \sup_{0 \leq \psi \leq f}L(\psi)$$ is a non-negative functional when restricted to the non-negative elements of $C(X)$ . To extend this definition to arbitrary $f \in C(X)$ , Royden starts by choosing $M \geq 0$ such that $f + M \geq 0$ (which can be done since $f$ is bounded). He then shows that $L_+(f+M) - L_+(M)$ is independent of the choice of $M$ , and accordingly defines $L_+(f)$ to be this value. He then claims that under this definition, ""clearly $L_+(f):C(X) \to \mathbb{R}$ is positive."" I am having a hard time showing this last fact, and would appreciate any insight into how to show this. My work so far My best approach has been to split $f = f^+ - f^-$ . Then \begin{align}
L_+(f + M) - L_+(M) &= L_+(f^+ - f^- + M) - L_+(M) \\
&= L_+(f^+) - L_+(M) + L_+(M-f^-).
\end{align} The second step follows from The linearity of $L_+$ when restricted to non-negative functions on $C(X)$ $f^- \leq M$ since $f + M \geq 0$ $f^+$ and $f^-$ are in $C(X)$ and non-negative. Now the third term on the RHS is non-negative by construction of $L_+$ thus far. However, I can't quite demonstrate whether the difference of the first and second terms is also non-negative. Please let me know if I can provide any additional information. Thanks!",['functional-analysis']
3877143,*Weak Lusin* vs *Strong Lusin*,"In a book that I’m reading, the author presents two versions of Lusin’s theorem: Let $X$ be a topological space with a regular measure $\mu$ , and $Y$ to be second countable (i.e., admits a countable family $(B_i)_i$ Of open sets such that for any open set $B \subset Y$ , we can represent $B$ as an union of $B_i$ ’s). Weak Lusin - Under the above assumptions, if $f: X \to Y$ is measurable, then, for every $\epsilon>0$ there exists a compact set $K\subset X$ s.t $\mu(K^c) <\epsilon$ and $f \mid_K
$ is continuous. Strong Lusin - Under the same assumptions, if $f:X\to \mathbb R^d$ measurable, then for every $\epsilon>0$ there exists a compact set $K\subset X$ and a continuous function $g:X\to \mathbb R^d$ s.t $\mu(K^c) <\epsilon$ and $f=g$ on $K$ . The thing is, these two seem very similar to me. I can’t quite comprehend how the existence of a function $g$ coinciding with a function $f$ on $K$ , differs from having a continuous function $f$ only on $K$ . Is the different mainly due to the fact that exists a function $g$ continuous?
Another different seems to be the change in the space $Y$ . The strong version is restricted to $\mathbb R^d$ . Can anyone clarify the difference in this two versions? Perhaps present a counterexample.","['intuition', 'measure-theory', 'examples-counterexamples']"
3877154,adjoint matrix of an operator,"considering a $2\times 2$ matrix $\bf S$ , \begin{equation}
   {\bf S} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  \kappa\nabla .\\ 1/\rho \nabla . &  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} I want to get the adjoint of $\bf S$ , $$\bf S^{*}$$ for that I am using cofactor matrix and then I transpose this matrix (METHOD 1 to get adjoint matrix). \begin{equation}
   {\bf S}_{\text{cof}} =  \begin{bmatrix} a_{11} &  a_{12} \\ a_{21} &  a_{22}  
  \end{bmatrix} 
  \end{equation} as $$a_{ij}=(-1)^{i+j}\det\bf S_{ij}$$ \begin{eqnarray}
a_{11}&=&\frac{\partial}{\partial{t}}\\
a_{12}&=&-1/\rho \nabla .\\
a_{21}&=&-\kappa\nabla . \\
a_{22}&=&\frac{\partial}{\partial{t}}
\end{eqnarray} In matrix notation we have, \begin{equation}
   {\bf S}_{\text{cof}} =  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . &  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} Transposing the matrix $$\bf S_{\text{cof}}$$ we get: \begin{equation}
 {\bf S}^{*}=  \begin{bmatrix} \frac{\partial}{\partial{t}} &  -\kappa\nabla .   \\ -1/\rho \nabla .&  \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} This is the way I am doing but I saw some other papers doing something different, the procedure other people are doing is transposing $\bf S$ and change the signal of every first derivative (METHOD 2 to get adjoint matrix), in this sense the adjoint of $\bf S$ is: \begin{equation}
   {\bf S}^{*} =  \begin{bmatrix} - \frac{\partial}{\partial{t}} &  -1/\rho \nabla \\ -\kappa\nabla . & - \frac{\partial}{\partial{t}}  
  \end{bmatrix} 
  \end{equation} Which method is right? Am I doing something wrong? If you have any documentation please share it. Thank you,
Rafael","['matrices', 'adjoint-operators', 'derivatives']"
3877211,Killing vectors in Minkowski Metric,"Firstly, I know this is a physics-related problem, and I have posted here , but the physics forum seems so much more empty then this one, so here it goes: I was in the process to find the Killing vectors for the Minkowski Metric and I stumbbled into a material that does a different procedure at the very end of the process, in comparisson to usual books and articles I've seen. The reasoning goes as follows: For example, suppose we have found the Killing vector $$K=x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x}$$ The way I would check this is a generator for the boost in the x direction is by acting these vectors onto t and x and check they give, respectively, x and t. The way this material I've found does is , they suddenly deffine: $$
\Lambda=\exp[\lambda(x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})]=\sum_{n=0}^{\infty}\frac{1}{n!}\lambda^n (x \frac{\partial}{\partial_t} + t\frac{\partial}{\partial_x})^n
$$ then he proceeds to find the explicit form of the boost: $$
\Lambda t = x\sinh \lambda +t\cosh \lambda
$$ $$
\Lambda x = t\sinh \lambda +x\cosh \lambda
$$ I understand the steps in this process. What I don't get is where did the motivation for the exponentiation come. What does that mean? It seems to me to have something to do with the application of the diffeomorfisms, but I'm not sure. Also, would this be a more correct way to proceed then what I've done?I really would appreciate any comments on this, as well as reccomended material.","['special-relativity', 'general-relativity', 'tangent-spaces', 'differential-geometry']"
3877247,Does $x=x$ represent a valid algebraic equation? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have an equation $x=x$ . Is that a valid algebraic equation? Can $x=x$ be simplified to $1=1$ ? Can it be simplified down to True ? Can $x=x$ , $1=1$ , or True be graphed on a regular $(x,y)$ plot graph? Could it be graphed at all? If it is graph-able what would it look like?",['algebra-precalculus']
3877295,Can you prove why the recurrence $3P_{n} - 2P_{n + 1} = P_{n - 1}$ holds for some (a subset of the) prime numbers?,"Can you prove why this recurrence relation holds for some (a subset of the) prime numbers? And does it go on infinitely? (I think it does) $$3P_{n} - 2P_{n + 1} = P_{n - 1}$$ First few values for which it applies: $5,
7,
13,
20,
25,
26,
28,
45,
49,
60,
64,
78,
88,
89,
93,
95,
113,
136,
144,
148,
152,
159,
182,
212,
225,
229,
230,
236,
243,
247,
249,
262,
265,
277,
286,
288,
291,
302,
315,
323$ E.G:   For $n = 5, P_{n} = 11, P_{n + 1} = 13, P_{n - 1} = 7:$ $3(11) - 2(13) = 7$ $3P_{5} - 2P_{6} = P_{4}$ ...&c for the others","['number-theory', 'recurrence-relations', 'linear-algebra', 'pattern-recognition', 'prime-numbers']"
3877300,Independent vs Overall Percentage of Turns in Game where Next Player is Probabilistically Determined.,"I'm writing a program in which there are n ""players"". On each player's turn, the next player is randomly determined with a weight. My goal is to have each player have it be their turn a certain percentage of the time (e.g. Player 1 gets 0.20 of all turns). If there were three players, [1, 2, 3] and I would like each of them to have it be their turn [0.20, 0.50, 0.30] of the total number of turns respectively. When I run this simulation several thousand times and count up the number of turns each player takes, however, I get a distribution of [0.258, 0.403, 0.339] of the turns for each player. I am picking the next player by repeatedly sampling the list of players until I get one that is not the current player. I think this is being complicated by the fact that the next player must not be the player whose turn it currently is, but I am unsure why this. How can I sample the next player and achieve some specific proportion of turns in the long run?","['programming', 'statistics', 'probability-distributions', 'sampling', 'probability']"
3877331,"Given $\|x\| \leq 1$ in an infinite dimensional Hilbert space, show there exists a orthonormal sequence that converges weakly to $x$","Let $H$ be an infinite dimensional Hilbert space. Given $x\in H$ with $\| x\| \leq1$ , show there exists an orthonormal sequence $(x_n)$ such that $(x_n)$ converges weakly to $x$ . Below are my ideas and thoughts so far: I thought about using the orthonormal basis to construct such sequence. But since we don't know if $H$ is countable, we can't assume there exists an orthonormal basis. Also note that using Bessel's inequality, if we have an orthonormal sequence we have $\sum_{n} |\langle x,x_n\rangle|^2 \leq \| x\|^2=1$ . So $\lim _{n \rightarrow\infty} \langle x,x_n\rangle^2 =0$ . Hence $\lim _{n \rightarrow\infty} \langle x,x_n\rangle =0$ , which tells us $x_n$ converges weakly to zero. But I'm not sure if this helps us with the question... Any hints or ideas will be appreciated! Thank you","['hilbert-spaces', 'functional-analysis', 'weak-convergence']"
3877351,Does the cyclic group $C_n$ have the fewest automorphisms among groups of order $n$?,"The number of automorphisms of the group $C_n$ is $|\text{Aut}(C_n)|= \varphi(n)$ , the number of generators of the group. Using GAP, I have checked that this is minimal among groups of order $n$ for every $n<128$ , and it feels very plausible that this always holds. However, I haven't managed to show this is the case; the fact that a group can have elements of high order fixed by every automorphism seems like an obstacle to several approaches. A proof, or references to a theorem implying this, would be welcome.","['automorphism-group', 'group-theory', 'group-isomorphism']"
3877352,"Let $A,B,C$ are the subsets of $E$. Prove that: If $A\cup C\subset A\cup B$ and $A\cap C\subset A\cap B$ then $C\subset B$.","Let $A,B,C$ are the subsets of $E$ . Prove that: If $A\cup C\subset A\cup B$ and $A\cap C\subset A\cap B$ then $C\subset B$ . My attempt: For $A\cup C\subset A\cup B$ then with all $x\in C\subset A\cup B$ $(1)$ . For $A\cap C\subset A\cap B$ , we have $x\in C\Rightarrow x\in B$ $(2)$ . From $(1),(2)$ we have $C\subset B$ . I think my solution is not good, please correct me if I am wrong. Thank you so much!","['elementary-set-theory', 'algebra-precalculus', 'discrete-mathematics']"
3877395,"Given $a,b,x>0$, $x<y$, prove $(a^x+b^x)^{1/x} > (a^y+b^y)^{1/y}$ [duplicate]","This question already has answers here : If $n\geq m$ then $(x^m+y^m)^{1/m} \ge (x^n+y^n)^{1/n}$ (2 answers) Closed 3 years ago . I'm thinking about proving $f(x) = (a^x+b^x)^{1/x}$ has negative derivative for all positive $x$ . $$f'(x) = \left(b^x+a^x\right)^\frac{1}{x}\left(\frac{b^x\ln\left(b\right)+a^x\ln\left(a\right)}{\left(b^x+a^x\right)x}-\frac{\ln\left(b^x+a^x\right)}{x^2}\right)$$ To prove this is negative, I need $$x(a^x\ln(a)+b^x\ln(b)) < (a^x+b^x)\ln(a^x+b^x)$$ which is equivalent to $${a^x}^{a^x}{b^x}^{b^x} < (a^x+b^x)^{a^x+b^x}$$ This looks like a special case of $$A^AB^B < (A^A+B^B)^{A^A+B^B}$$ for any $A,B>0$ . I'm convinced this is true, but I don't know how to prove it either.","['logarithms', 'multivariable-calculus', 'inequality', 'exponential-function', 'karamata-inequality']"
3877400,Every submanifold of $\mathbb {R}^n $ is locally a graph,"I'm trying to do this Pollack exercise. I managed to do item (a), but I'm stuck on item (b) I don't know what function to set to $g_i$ . I thought about using that as $X$ is submanifold for all $p \in X$ exists chart $(V, \psi)=(V, r_1,..,r_N)$ such that $X \cap V = \{q \in V | r^{k+1}(q)=...=r^N(q)=0 \}$ , but i don't know how to proceed","['smooth-manifolds', 'calculus', 'manifolds', 'differential-topology', 'differential-geometry']"
3877422,Prove that $f$ is one-to-one on $B$ and $f^{-1}:Y\to B$ is Borel.,"Let $f: X\to Y$ be a continuous and surjective map between compact metric spaces. Prove that there is a Borel set $B\subset X$ such that $f(B)=Y$ , $f$ is one-to-one on $B$ and $f^{-1}:Y\to B$ is Borel. In other words, we can select from each of the sets $f^{-1}$ exactly one point in a way that the resulting inverse function is Borel. My attempt: For each $y \in Y$ we can pick one element $x_y \in f^{-1}(y)$ then have a collection $B'=\{x_y: y \in Y\}.$ Now $\{x_y\}$ is Borel. But we don't know $B'$ is Borel or not! Can we make it Borel?
Even if we couldn't then we can take a finite or countable subcollection B or B' and that will be Borel and $f$ is 1-1 on $B$ but we are lacking $f(B)=Y$ . Please help me from here.","['measure-theory', 'lebesgue-measure', 'borel-measures', 'real-analysis', 'borel-sets']"
3877488,A big list - Applications of the structure theorem of finitely generated modules over PIDs,"I'm now a TA on an undergraduate course ""Algebra II"" and the main topics of the course are ""rings and modules"" and ""fields and the Galois theory"". We shall cover the fundamental result, namely the structure theorem of finitely generated module over PIDs . Two typical applications of the theorem are The structure theorem of finitely generated abelian groups (as $\mathbb{Z}$ -modules); The canonical forms (including Smith canonical forms and Jordan canonical forms) in linear algebra (by viewing the finite dimensional $k$ -vector space $V$ as a $k[\lambda]$ -module). In this post, I'm hoping to collect some interesting applications of the structure theorem besides the two examples above, since the two examples can be seen in almost all textbooks, and these may be boring for students. For example, this is an interesting application: Solutions of homogeneous linear differential equations are a special case of structure theorem for f.g. modules over a PID . Although this is somehow a special case of the ""canonical form"" example, this is rather surprising and can be regarded as a good example for me. And moreover, although I'm actually collecting examples for the course, any applications beyond the scope of the course are still great ! For example, as @KCd hinted in the comment, the theorem can be seen in basic algebraic number theory all the time, such as the Dirichlet unit theorem. Thank you all in advance! And thanks @KCd for his helpful comments!","['big-list', 'examples-counterexamples', 'modules', 'abstract-algebra', 'commutative-algebra']"
3877553,Infinite Intersection of Nested Connected Sets that are Disconnected,"I am working through a book in real analysis and am having trouble with a problem. I know that the following statement is false: If we have connected sets $S_1 \supset S_2\supset S_3\supset\cdots$ , then $S=\cap\;S_n$ (the infinite intersection of all $S_n$ ) is connected. However, I am not being able to find a counterexample. What is one that I could understand fairly easily? Additionally, the next part of this asks whether this is true if the sets are also compact. I think that the statement is then true but am having a lot of trouble proving this as well.","['elementary-set-theory', 'connectedness', 'general-topology', 'compactness']"
3877592,Is there a useful asymptotic expansion of $(1 + z^{\sqrt{2} - 1} + z^{\sqrt{2}})^{-1}$ at $z = 0$?,"I believe that an asymptotic expansion in terms of powers of $z$ can not exist because we could use the geometric series to find something of the form $$
\frac 1{1 + z^{\sqrt 2 - 1} +z^{\sqrt 2 }} = \sum_{n \ge 0} (-1)^n\sum_{i = 0}^n \binom n i  z^{\sqrt 2  n -  i}
$$ so Dirichlet's Approx Thm would furnish the existence of a sequence of powers converging to $0$ . Using powers of logs does not work either since they would have to be indexed from infinity to 0. What functions should one use to asymptotically analyze series whose naive expansions in terms of powers yield powers converging to $0$ (e.g. $(1+z^a+z^b)^{-1}$ where $a/b\notin \Bbb Q$ )?","['asymptotics', 'taylor-expansion', 'analysis']"
3877601,Prove that $18!+1$ is divisible by $19$ and $23$ [duplicate],"This question already has answers here : how to prove $437\,$ divides $18!+1$? (NBHM 2012) (1 answer) $h!\:\!(p-1-h)! \equiv (-1)^{h+1}\!\pmod{p}$ [Wilson Reflection Formula] (2 answers) Closed 3 years ago . Prove that $18!+1$ is divisible by $19$ and $23$ For $19$ we can just use Wilson's Theorem but I wasn't able to think of how to prove for 23. One way is to just multiply $(-5)(-6)(-7)(-8)(-9)(-10)(-11)(11)(10)(9)…(2)(1)$ and then find its remainder with $23$ , and then add $1$ to it. But surely there's a better way right?","['elementary-number-theory', 'divisibility', 'discrete-mathematics', 'factorial']"
3877663,What can we get from the square integrability of the derivative?,"Let $f$ be a continuously differentiable function on $[1, +\infty]$ . Question: Is it true that $\int_1^{+\infty} (f')^2dx < +\infty$ implies $\int_1^{+\infty} (\frac f x)^2 dx< +\infty$ ? This is the integral version of the question in this post: If a positive series converge in square sum, will its average series converge in square sum? . I am interested in this question because it seems rather simple and concrete. I think there should be some techniques to deal with such kind of problems. However, I am not able to prove or give a counterexample of the claim. What I have got: (1) The claim holds when $f$ is a power function. (2) The claim fails if 'square integrability' of $f'$ and $\frac f x$ is replaced by 'absolute integrability', since $f$ can be chosen to be a constant $1$ .","['integration', 'convergence-divergence', 'functional-analysis', 'real-analysis']"
3877687,Deeper Algebraic Structure to Random Vectors?,"Given a probability space $\Omega,$ the space of square-integrable measurable functions $\Omega \to \mathbb{R}^n$ (""random vectors"") can be made a vector space over $\mathbb{R}$ in a natural way. Call this space $V.$ In probability theory, we proceed to define several operators on this space, like the expectation operator $E : V \to \mathbb{R}^n$ given by $(X_1,X_2...,X_n) \mapsto (E(X_1),E(X_2)...,E(X_n))$ . However, going just a bit deeper into the theory, we start to see some properties of $E$ nicer than linearity over $\mathbb{R}$ would alone suggest. For example, for any $k \times n$ matrix $A$ , we find that $E(AX) = AE(X).$ Similar occurrences occur with the bilinear covariance operator $\mathrm{Cov} : V \to \mathbb{R}^{n \times n}$ . For example, for any $k \times n$ matrices $A$ and $B,$ we find $\mathrm{Cov}(AX,BY) = A\mathrm{Cov}(X,Y)B^T,$ where $B^T$ denotes the transpose of $B.$ On one level, one can just view this as matrix algebra (and this may be all there is to it). But I've always been inclined to look for deeper algebraic structure than just matrix algebra when I see matrices, so I'm wondering if there's a deeper algebraic reason to this. For example, we could have viewed $V$ as a module over $n \times n$ matrices, but this approach doesn't seem to explain the transposes and the generalization to $k \times n$ matrices with $k \neq n.$ So, I'm wondering if there's some algebraic structure to $V$ in which the ""matrix linearity"" of the form seen in $E$ and $\mathrm{Cov}$ become natural (and hence easy to remember!).","['statistics', 'linear-algebra', 'probability-theory', 'modules']"
3877705,How to solve $\cos x-\sin 3x=\cos 2x$?,"My attempt: $$\begin{align} (\cos x- \cos 2x) - \sin 3x &= 0 \\
2\sin \frac{3x}{2}\sin \frac{x}{2} - 2 \sin \frac{3x}{2}\cos \frac{3x}{2}&=0\\
\sin\frac{3x} {2} \left(\sin\frac{x} {2} - \cos\frac{3x} {2}\right)&=0 \end{align}$$ Now either $\sin\frac{3x} {2} = 0$ or $\left(\sin\frac{x} {2} - \cos\frac{3x} {2}\right)=0$ . Solving for the former, $$\frac{3x}{2}=n\pi\rightarrow x = \frac{2n\pi}{3}$$ How can I solve $\left(\sin\frac{x} {2} - \cos\frac{3x} {2}\right)=0$ ? I know the elementary identities involving trigonometric ratios, but not complex numbers or calculus.",['trigonometry']
3877715,"If $T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi)$ and $T_0(\varphi)=1$, is there a function $f$ with $T_t(\varphi)=e^{-tf(\varphi)}$?","Let $E$ be a $\mathbb R$ -Banach space and $T_t:E'\to\mathbb C$ for $t\ge0$ with $$T_{s+t}(\varphi)=T_s(\varphi)T_t(\varphi)\;\;\;\text{for all }\varphi\in E'\text{ and }s,t\ge0\tag1$$ and $$T_0(\varphi)=1\;\;\;\text{for all }\varphi\in E'\tag2.$$ Are we able to conclude that $$T_t(\varphi)=e^{-tf(\varphi)}\;\;\;\text{for all }\varphi\in E'\text{ and }t\ge0\tag3$$ for some $f:E'\to\mathbb C$ with $f(0)=0$ ? Moreover, assuming $[0,\infty)\ni t\mapsto T_t(\varphi)$ is continuous for all $\varphi\in E'$ , are we able to conclude that $f$ is continuous? I guess the pure existence of $f$ is somehow an easy consequence of the functional equation solved by the exponential function. If it simplifies the matter, I'm also interested in the case where $E$ is a $\mathbb R$ -Hilbert space and/or separable.","['functional-equations', 'banach-spaces', 'hilbert-spaces', 'functional-analysis', 'exponential-function']"
3877734,Burns and Gidea's differential geometry/topology: $\Bbb S^2$ is diffeomorphic to $\Bbb R$!!,"Exercise 1.15.2 of Burns and Gidea's differential geometry/topology states that: Exercise 1.15.2: Consider a bijection between the real line $\Bbb R$ and the sphere $\Bbb S^2$ (such a bijection exists since these are sets with same cardinality). Show
that the composition of the local parametrizations of $\Bbb S^2$ from above with this bijection defines a smooth structure on $\Bbb R$ . Show that $\Bbb R$ endowed
with this smooth structure is diffeomorphic to the sphere $\Bbb S^2$ . With this
smooth structure, the real line is a sphere! The point of this exercise
is to stress that a manifold is not just a set that can be endowed with
some structure, but the set together with that structure. If so what is the role of invariance of dimensions? It seems that this exercise is a serious mistake by authors!! In the page 67, exercise 1.15.5 claims that Exercise 1.15.5: Provide the unit cube $Q\subset \Bbb R^{n+1}$ with a smooth structure. The point of this exercise it to illustrate that a
smooth manifold may not look smooth ! Of course this smooth structure
is not compatible with the smooth structure of $\Bbb R^{n+1}$ . Is the claimed statement correct? I have no idea about $n>2$ but in $n=1,2$ I think it is wrong by unique differential structure in dim $<4$ !","['differential-topology', 'smooth-manifolds', 'differential-geometry']"
3877823,There exists a unique path linking every two vertices in a tree $T$,"I've come up with what feels like a really convoluted proof for a fairly simple theorem. There are a few points I'd like to improve upon: I dislike using the physical language of ""following"" a path -- it feels more like an appeal to intuition than something that belongs in a formal proof. Can you suggest an alternate way of framing this? I'm not entirely convinced by my own proof -- in part (I) , for example, how do we know for sure that ""following"" (ugh, I did it again! :)) $P_1 \cup P_2$ will lead to a vertex in $P_1 \cap P_1 \triangle P_2$ ? How do I know that ""following"" $P_1 \cap P_1 \triangle P_2$ will lead to $P_2 \cap P_1 \triangle P_2$ ? Is this proof salveageable, or are there any fatal assumptions made along the way? Can you suggest a simpler proof? To clarify notation: By a graph I mean a pair $(V, E)$ with $V$ a set of elements called vertices, and $E = \{ \{v_1, v_2\} : v_1, v_2 \in V\}$ . I take a path to be a nonempty graph with $E = \{ \{ v_0, v_1\}, \{ v_1, v_2 \}, ..., \{v_{k-1}, v_k\}\}$ where the $v_i$ are distinct. The set theoretic operations I define as being applied componentwise to the elements of $G$ -- so $G_1 \cap G_2 = (V_{G_1} \cap V_{G_2}, E_{G_1} \cap E_{G_2})$ . I take this notation mostly from Diestel (maybe except for the abuse of the notation for set theoretic operations). Theorem There exists a unique path linking every two vertices in a tree $T$ Proof Existence follows from the definition of a tree (a connected acyclic graph). We show uniqueness as follows: let $P_1$ and $P_2$ be paths linking vertices $x_0, x_k \in T$ with $P_1 \neq P_2$ . Take the symmetric difference $P_1 \triangle P_2$ . Note that $P_1 \triangle P_2$ must be nonempty, since $P_1 \neq P_2$ . Further, $P_1 \cap (P_1 \triangle P_2) \neq \emptyset$ and $P_2 \cap (P_1 \triangle P_2) \neq \emptyset$ (otherwise we would have, for example, $P_1 \subset P_2$ , which is impossible since by hypothesis both paths link $x_0$ and $x_k$ ). If $P_1 \cap P_1 \triangle P_2 = P_1$ and $P_2 \cap P_1 \triangle P_2 = P_2$ (if one of these is true, both are true), then we have a cycle with $P_1 \cup P_2$ . Otherwise, follow $P_1 \cup P_2$ until we arrive at a vertex of $P_1 \triangle P_2$ . (I) Follow $P_1 \cup P_2$ until we arrive at a vertex $v$ in $P_1 \triangle P_2$ . This vertex is adjacent to vertices in both $P_1 \cap P_1 \triangle P_2$ and $P_2 \cap P_1 \triangle P_2$ . Then we can follow $P_1 \cap P_1 \triangle P_2$ until we reach a vertex in $P_2 \cap P_1 \triangle P_2$ , and follow $P_2 \cap P_1 \triangle P_2$ back to $v$ . Then a cycle exists, contradicting our hypothesis that $P_1 \neq P_2$ . Then $P_1 = P_2$ , and for every pair of points $x_0, x_k$ in a tree there exists a unique path.","['trees', 'graph-theory', 'alternative-proof', 'solution-verification', 'discrete-mathematics']"
3877862,Compute exact integrals with quaternions,"It's common knowledge that complex analysis is helpful in computing a bunch of exact real integrals. Is there any occurence of quaternions/quaternion formalism helping in the same way? If not, what could be some plausible reasons? Motivation To address comments asking for motivation, I will try to outline some correspondences in quaternion analysis to complex analysis, especially those that in complex analysis are useful for evaluating definite real integrals. Describing quaternions as $q= t+ix+jy+kz$ , A. Sudberry's 1977 Quaternionic Analysis notes that if the Cauchy-Riemann analogue of $$\frac{\partial f}{\partial t}+i\frac{\partial f}{\partial x}+j\frac{\partial f}{\partial y}+k\frac{\partial f}{\partial z}=0$$ holds, then an analogue of Cauchy's theorem holds for $C$ a smooth, closed 3-manifold in $\mathbb{H}$ : $$\int_{C} f(q) \; Dq=0, \\ \text{for }Dq = (dx \,dy\, dz-i\,dt\, dy \, dz - j\,dt\, dx \, dz - k\,dt \, dx \, dy).$$ He also notes that for such regular functions, an analogue of Cauchy's integral formula holds: $$f(q_0) = \frac{1}{2 \pi^2} \int_{\partial D}\frac{1}{|q-q_0|^2} (q-q_0)^{-1} Dq \,f(q) $$ for $D$ a domain in which $f$ is regular. These results for integrals at least partially mirror those in complex analysis. In complex analysis, Cauchy's theorem and integral formula are very useful for deriving the results of definite real integrals. The higher dimensionality of the quaternionic integrals maybe suggests that real integrals of functions of three variables would be useful to consider, but I do not know. Can these results for quaternionic integrals of regular functions be used to evaluate definite real integrals?","['integration', 'contour-integration', 'improper-integrals', 'quaternions']"
3877882,The relative Frobenius morphism $F_{X/S}$ is an isomorphism iff $X/S$ is étale?,"The statement comes from wiki , see the picture below. I know that if $X$ is étale over $S$ then $F_{X/S}$ is an isomorphism and a proof can be found here . But is the reverse true? I believe that I have found a counter example. Let $R$ be a domain of characteristic $p$ and $F$ be its fraction field. Denote the Frobenius map $F_R:R\to R'$ which is injective for a domain. Denote $S=R-0$ . Then $$F\otimes_{R} R'=(S^{-1}R)\otimes_R R' =(F_R (S))^{-1}R'=(S^p)^{-1}R'$$ But $(S^p)^{-1}R'=S^{-1}R'=F'$ since $\frac{1}{t}=\frac{t^{p-1}}{t^p}$ . So $F\otimes_R R'=F'$ . Explicitly the relative Frobenius map $F_{F/R}:F\otimes_R R'\to F',\frac{u}{v}\otimes t\mapsto \frac{u^p t}{v^p}$ has an inverse $\phi:F'\to F\otimes_R R',\frac{x}{y}\mapsto \frac{1}{y}\otimes y^{p-1}x$ .
Now we just need to show there exists a characteristic $p$ domain $R$ s.t. $R\hookrightarrow F$ is not étale. Let $K$ be a field of characteristic $p$ , and we consider the localization $R=K[x]\hookrightarrow K(x)=F$ which is not finitely generated (if so then $K(x)$ is finitely generated over $K$ , contradiction), hence not étale.","['etale-cohomology', 'algebraic-geometry', 'schemes']"
3877891,Is the set of trees over countably infinite vertices countable or uncountable?,"Suppose we consider two graphs to be the equivalent if they are isomorphic. The idea is that if we relabel the vertices of a graph, it is still the same graph. Using this definition of “being the same graph”, can you conclude that the set of trees over countably infinite vertices is countable? I know that for any isomorphism $f$ and any vertex $v$ , $v$ and $f(v)$ have the same degree.","['graph-theory', 'cardinals', 'discrete-mathematics']"
3877940,Has this family of Diophantine equations ever been studied?,"This might be a trivial question, so, if it is, I will delete it as soon as I get an answer. I try to come up with conjectures once in a while. After all, the sleep of reason produces monsters (Francisco Goya, 1799). Recently, I came up with one (perhaps someone already did), and it is this: Will the family of equations $$a^n=k_1^{n-1}+k_2^{n-2}+\ldots+k_{n-2}^2,$$ always have at least one solution for each $n > 0$ and $k_i>0$ ? Just so I'm even more clear, when $n = 8$ , we have the following: $$a^8=k_1^7+k_2^6+k_3^5+k_4^4+k_5^3+k_6^2.$$ I wrote a computer program and the cases $n = 3,4,5,6,7,8,9,10,11,12,13,14,15$ have at least one solution. I left out the cases $n = 1,2$ because one can obtain solutions by hand. So, has this ever been studied? I don't even know what to call this family of equations, so I can't just perform a Google search.","['number-theory', 'perfect-powers', 'elementary-number-theory', 'diophantine-equations']"
3877946,Is the space of absolutely continuous measures dense in the space of signed measures?,Let's say we have $\mathbb{R}^n$ or a compactification of it and denote with $\overline{\mathcal{M}(\mathbb{R}^n)}$ the space of measures that are absolutely continuous with respect to the Lebesgue measure on $\mathbb{R}^n$ . Is it true that $\overline{\mathcal{M}(\mathbb{R}^n)}$ is dense in the space of signed measures $\mathcal{M}(\mathbb{R}^n)$ ?,"['measure-theory', 'lebesgue-measure', 'functional-analysis']"
3877965,is there any one one correspondence between an empty set and set of natural number $\Bbb N$,"Actually, I wants to know that how an empty set is finite. If it's finite then it must have one to one correspondence to the segment of natural number.","['elementary-set-theory', 'natural-numbers']"
3878008,Find the Limit.?,"What is the limit of $Y_{n} := \frac{1}{3}Y_{n-1} + \frac{2}{3}Y_{n-2} $ for $n > 2$ and $Y_{1}<Y_{2}$ . I have tried solving, but the limit tends to exist between $y_{1}$ and $y_{2}$ and hence I am not able to evaluate what would be the ratio between which they would lie?","['limits', 'sequences-and-series']"
3878025,How can I solve the functional equation $ f ( x + 1 ) + 1 = f \big( f ( x ) + 1 \big) $?,"Let $ \mathbb N = \{ 0 , 1 , 2 , \dots \} $ . Find all the functions $ f : \mathbb N \to \mathbb N $ such that $$ f ( x + 1 ) + 1 = f \big( f ( x ) + 1 \big) $$ for all $ x \in \mathbb N $ . I noticed, while looking for injectivity, that $$ \forall ( x , y , n ) \in \mathbb N ^ 3 : f ( x ) = f ( y ) \implies f ( x + n ) = f ( y + n ) \text . $$ Then I found out that $ f ( 0 ) \ne 0 $ , because if it's the case, by letting $ x = 1 $ we'll have $$ f ( 1 ) + 1 = f \big( f ( 0 ) + 1 \big) \implies 1 = 0 \text . $$ Is there any method to solve this?","['functional-equations', 'functions', 'natural-numbers']"
3878056,Solve the diffrential equation $\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0$,"A solution curve of the differential equation $\left( {{x^2} + xy + 4x + 2y + 4} \right)\frac{{dy}}{{dx}} - {y^2} = 0$ , $x>0$ passes through the point (1,3). Find the solution curve. I am not able to proceed as I am not able to convert the standard differentiable form",['ordinary-differential-equations']
3878079,Continuity of the derivative function. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose we have a function f(x) that is differentiable for all values of x. Is it necessary for the derivative function to be continuous for all x ?.","['continuity', 'calculus', 'derivatives']"
3878088,Has this energy a maximum in $B$?,"Suppose $\mathcal{H}$ is an Hilbert space and $\langle ., . \rangle$ his scalar product. Suppose $T:\mathcal{H} \to \mathcal{H}$ is linear, continuos, compact and self-adjoint.
If we define $J:\mathcal{H} \to \mathbb{R}$ as $J(x)=\langle Tx, x \rangle$ for all $x \in \mathcal{H}$ , can we conclude $J$ has a maximum in $B=\{x \in \mathcal{H}: \|x\| \leq 1\}$ ? I know that if $\mathcal{H}$ is separable I can use the fact that $B$ is sequentially compact and I can work with a maximizing sequence. But If I do not know the separability of $\mathcal{H}$ ? Does the same statement hold? I tried proving that $J_{|B}:B \to \mathbb{R}$ is weakly superior semi-continuos. In fact if $u_n \rightharpoonup u$ then $J(u_n) \to j(u)$ . Is it enough to conclude?","['hilbert-spaces', 'banach-spaces', 'spectral-theory', 'functional-analysis']"
3878092,Prove that the intersection of 2 generalised eigenspaces is the zero space,"I have searched for my above question and came across the ""Trivial intersection of generalised eigenspaces"" post on math stack exchange but I do not understand the proof using coprime polynomials. How do I proof such a statement (below) using just the definition of eigenvalues/generalised Eigenspaces? I have seen/proven that if $\lambda \neq \mu $ . then the intersection between $ E_\lambda(T) \cap K_mu(T) = \{ \mathbf{0} \} $ (where $E_\lambda(T) $ is the Eigenspaces corresponding the eigenvalue $\lambda$ . (not sure whether this information is required for the proof) Let $ T: V \rightarrow V$ be a linear operator where $V$ is a finite dimensional vector space over $ \mathbb{C} $ . I want to prove that $$ \text{If } \lambda \neq \mu, \text{then } K_\mu(T) \ \cap \ K_\lambda(T) = \{\bf{0}\}  $$ where $$ K_\lambda(T) = \{ \mathbf{v} \in V : (T-\lambda I_V)^m(\mathbf{v})=\mathbf{0}\} $$ Currently, the lecturer has only gone through the above definition of generalised Eigenspaces (he currently assumes that m need not be the same for different $\mathbf{v} \in K_\lambda(T)$ , he has not gone through the prove that m can be chosen to satisfy all $\mathbf{v}$ in the generalised eigenspace yet) Anyway, I tried to prove the above statement by contradiction but I got stuck: Let $ \lambda \neq \mu $ and assume $$ \exists_{non-zero \ vector \ \mathbf{v} \in V}\  \text{such that } v \in K_\mu(T)  \cap K_\lambda(T) $$ Then $$ (T-\mu I_V)^m(\mathbf{v}) = \mathbf{0} = (T-\lambda I_V)^n(\mathbf{v}) $$ $$ (T-\mu I_V)^m(\mathbf{v}) = (T-\lambda I_V)^n(\mathbf{v})$$ $$ (T-\mu I_V)^m(\mathbf{v}) - (T-\lambda I_V)^n(\mathbf{v}) =\mathbf{0} $$ And I'm not sure how to proceed. Thank you for your time!!","['generalized-eigenvector', 'linear-algebra']"
3878097,Surface integral over a cylinder problem,"I have a generic cylinder $x^2+y^2=a^2, 0≤z≤h$ labeled as $G$ . It has an outward-pointing unit normal vector $\vec{n}$ . As seen in the picture below: Now I am asked to solve this integral on $G$ : $$\iint\limits_{G}\vec{n}⋅\vec{n}ds$$ Note: I know that the most convenient way to solve that problem, is just to do the calculation of the dot product which equals to 1. From there I can easily proceed to the calculation of the surface area of $G$ which equals to $2\pi ah$ . And here is my problem. I did not pay attention that $\vec{n}⋅\vec{n}=1$ in our case. I went straight forward and noticed that $\vec{n}=∇f(x,y,z)=(2x,2y,0)$ . Then I used the known equation of surface integral: $$\iint\limits_{S}f(x,y,z)ds=\iint\limits_{D}f(x,y,g(x,y))\sqrt{(z_x)^2+(z_y)^2+1} dA$$ And tried to solve the following integral: $$\iint\limits_{S}(2x,2y,0)⋅(2x,2y,0)ds=4\iint\limits_{S}x^2+y^2ds=4\iint\limits_{D}x^2+y^2dA=4\int_{θ=0}^{2\pi}\int_{r=0}^{a}r^3drdθ$$ Here I got a wrong answer. The problem is that there is no reference to value of $z$ which limits the height of that cylinder. I think that I have missed something here, because my way of solving seems logical to me in general, but I am sure that there is a right way to deal with the height constraint here that I can't understand well.","['multivariable-calculus', 'multiple-integral']"
3878142,Countably many discontinuities of a CDF in n-dimensions,"It is well known that a cdf of a random variable in $\mathbb{R}$ has at most countably many discontinuities. How does this generalize to $n$ -dimensions from the $1$ -dimensional case? That is, for the $1$ -dimensional case we show the following: If $F_X$ is the cdf of a random variable $X$ in $\mathbb{R}$ , then we can let $B_n := \{x : F(x^{+}) - F(x^{-}) \geq 1/n\}$ . Further, since $F$ is increasing, $\lim_{x \to -\infty} F(x) = 0$ , and $\lim_{x \to \infty} F(x) = 1$ we can conclude that $|B_n| \leq n$ . Therefore, the set of (jump) discontinuities is $B = \bigcup_{n \geq 1} B_n$ , and is at most countable. For $n$ -dimensions we define the cdf as follows: $$F_X(x_1, \dots, x_n) := \mathbb{P}(X_1 \leq x_1, \dots, X_n \leq x_n),$$ where $x_1, \dots, x_n \in \mathbb{R}$ . I believe we could approach the $n$ -dimensional case in either/or ways: 1. If we assume that the cdf is equipped with the Lebesgue measure $\lambda$ , we could some how apply Lebesgue's Differentiation (Density) Theorem: That is, if $A \subset \mathbb{R}^n$ , then a.e. point of $A$ is either $1$ or $0$ , where the set of discontinuities $\mathcal{B} \subset A$ are those points of density not $0$ or $1$ (countable). 2. The set of discontinuities in $n$ -dimensions is $\mathcal{B} := \mathcal{B}_1 \times \dots \times \mathcal{B}_n$ . Presumably, each $\mathcal{B}_i$ are countable and have measure $0$ . Hence, if we show any $\mathcal{B}_i$ has measure $0$ the entire $\mathcal{B}$ does too. I'm not sure how to go about doing this formally though, and whether (1) or (2) is the more viable/correct approach?","['cumulative-distribution-functions', 'measure-theory', 'probability-theory']"
3878208,"$\langle S\alpha,\alpha\rangle = \langle T\alpha,\alpha\rangle \Longrightarrow S=T$ for unbounded operators","Like showed for example here , here or here , it is well-known that on a complex Hilbert space $H$ (or inner product space), basically by polarisation, for any bounded linear operator $T : H \to H$ , we have \begin{equation}
\forall v \in H : \quad \langle Tv,v\rangle = 0 \quad\Longrightarrow\quad T = 0.
\end{equation} Applying this equation to the difference, an easy consequence this is that if $S : H \to H$ is another bounded linear operator on $H$ , then \begin{equation}
\forall v \in H : \quad \langle Sv,v\rangle = \langle Tv,v\rangle \quad\Longrightarrow\quad S = T.
\end{equation} Does the result also hold true if the dimension of $H$ is not finite? for possibly unbounded operators or do we must assume that the operator is self-adjoint, normal? What happens if the operator is not self-adjoint nor even symmetric? For example consider the difference of two covariant derivatives, which is not symmetric in general as $T$ , and the Hilbert space of equivalence classes of $L^2$ -Borel $p$ -forms on a Riemannian manifold.","['functional-analysis', 'differential-forms', 'differential-geometry']"
3878231,Using ODE to model a discrete phenomenon,"SETTING In this paper the authors study the effects of tourism in a national park in Austria on a endangered species of bird. They model the species size (number of breeding couples) with a continuous function of time $S = S(t)$ satisfying the following logistic ODE (with time dependent carrying capacity) $$
\dot{S}(t) = \gamma S(t)\left(1 - \frac{S(t)}{\beta H(t)}\right), \qquad S(0) = S_0 > 0
$$ where $\gamma, \beta > 0$ are constants and $H = H(t)$ , which represent the habitat extent, satisfies the following logistic ODE $$
\dot{H} = \alpha H(t) \left( 1 - \frac{H(t)}{K} \right), \qquad H(0) = H_0 >0,
$$ with $K >0$ constant. QUESTION : How reasonable is to model the dynamic of the size of a population with a continuous function $S$ ? For a very large population, I wouldn't see any problem, but in the specific case analyzed in the paper, $S_0 = 12$ and they argue in the appendix A.2 that the partridge carrying capacity is of 16 breeding couples. Therefore the actual discrete function that the authors want to model has a very limited range and I wonder, how legitimate it is to use an ODE. A more realistic model would be to use a discrete state dynamic system to model the population dynamic (actually a stochastic process would probably be even more suitable) and I wonder how legitimate is to approximate such a system with an ODE. Any suggestion/idea is very welcome! Even heuristic arguments. PS : I am definitely more familiar with ODE than with discrete state systems.","['mathematical-modeling', 'ordinary-differential-equations', 'biology', 'discrete-mathematics', 'dynamical-systems']"
3878235,cyclic rational inequalities $\frac{1}{a^2+3}+\frac{1}{b^2+3}+\frac{1}{c^2+3}\leq\frac{27}{28}$ when $a+b+c=1$,"I've been practicing for high school olympiads and I see a lot of problems set up like this: let $a,b,c>0$ and $a+b+c=1$ . Show that $$\frac{1}{a^2+3}+\frac{1}{b^2+3}+\frac{1}{c^2+3}\leq\frac{27}{28}$$ Any problem that involves cyclic inequalities like these always stump me. I know I'm supposed to use Cauchy-Schwarz or AM-GM at some point, but I can never get to a place where this might be useful. My first instinct is to get common denominators and hope stuff simplifies, but I can never get farther than that. For example, in this problem I did the following: $$\frac{1}{a^2+3}+\frac{1}{b^2+3}+\frac{1}{c^2+3}$$ $$=\frac{(a^2+3)(b^2+3)+(b^2+3)(c^2+3)+(c^2+3)(a^2+3)}{(a^2+3)(b^2+3)(c^2+3)}$$ $$=\frac{a^2b^2+b^2c^2+c^2a^2+6(a^2+b^2+c^2)+27}{(a^2+3)(b^2+3)(c^2+3)}$$ but this is where I get stuck. I've tried using Cauchy-Schwarz on parts of this fraction to simplify it, but I can never get anything to work. How could you prove this inequality, and what are the important things to look out for in problems of this nature","['contest-math', 'inequality', 'tangent-line-method', 'algebra-precalculus', 'rational-functions']"
3878236,prove that $S^1$ is smooth submanifold of $\mathbb{R}^2$ using the definition with diffeomorphism,"I'm tring to prove that the unit circle $$S^1=\{(x_1,x_2)\in\mathbb{R}^2\text{ such that }x_1^2+x_2^2=1\}$$ is an embedded submanifold of $\mathbb{R}^2$ using the following Characterization: A nonempty subset $M \subset\mathbb{R}^n$ is an m-manifold iff: For every $p\in M$ , there are two open sets $O,W\subset\mathbb{R}^n$ with $0_n\in O$ and $p ∈ M ∩ W$ ,
and a smooth diffeomorphism $ϕ: O → W$ , such that $ϕ(0_n) = p$ and $$ϕ(O ∩ (\mathbb{R}^m × {0_{n−m}})) = M ∩ W$$ . My attempt: lets fix $a\in S^1$ , there exists a unique $\theta_0\in[0,2\pi)$ such that $a=(cos(\theta_0),sin(\theta_0))$ and let $\phi$ be the diffeomorphism \begin{array}{cccc}
\phi : & (0,\infty)\times(\theta_0-\pi,\theta_0+\pi) & \longrightarrow & \mathbb{R}^2\backslash D_{\theta_0+\pi}\\
~~ & (r,\theta) & \mapsto &(rcos(\theta),rsin(\theta))  
\end{array} where $D_{\theta_0+\pi}$ is the half line at the origin with polar angle $\theta_0$ From here i dont know how to proceed. Should i modify this map or use as it is? and who are the two open sets of the charactrization?","['submanifold', 'differential-geometry']"
3878244,Statistical Odds calculation for a video game occurrence,"In a video game I'm playing, there is a random number generator that awards one trophy from a pool of possible trophies.  The pool of possible trophies is approximately 60 different items, and I was supposed to be awarded 5 random ones.  I received 3 of one type, and 2 of another. I'm trying to estimate the odds of this occurrence, versus the odds of receiving 5 different ones. For the ""3 of the same type"" the odds would be 1 out of 3600, and the odds of ""2 of the same type"" is 1 out 60.  Or not? Does the order of events impact the odds?  If so, the order was:
#1, #1, #2, #1, #2.  Would this imply the odds at each stage were NA, 1:50, 1:49, 1:48, 1:48 ? Apologies in advance if my terminology is incorrect.  I know ""odds"" and ""probability"" are not interchangeable, but it's been a LOOOONG time since my college statistics class. TIA.",['statistics']
3878271,Combinatorics - Choosing sticks,"Seven sticks with lengths 2, 3, 5, 7, 11, 13 and 17 inches are placed in a box. Three of the sticks are randomly selected. What is the probability that a triangle can be formed by joining the endpoints of the sticks? Express your answer as a common fraction. My work: Using the triangle inequality, I counted $9$ ways to choose sticks: $(3,5,7) ; (3, 11, 13) ; (5,7,11) ; (5,11,13); (5,13,17); (7,11,13); (7,11,17); (7,13,17) ;(11,13,17) $ . My Question: I'm not sure how to count the total number of ways to choose sticks. Is choosing $(3,5,7)$ the same as choosing $(5,3,7)$ , for example, or do those count as distinct ways to draw?",['combinatorics']
3878353,"Is $(M_n^5)_{n \in \mathbb{N}_0}$ a martingale, given that both $(M_n)_{n \in \mathbb{N}_0}$ and $(M_n^2 )_{n \in \mathbb{N}_0}$are martingales?",Let $(M_n)_{n \in \mathbb{N}_0}$ be a stochastic process with $M_0 = 10$ such that both $(M_n)_{n \in \mathbb{N}_0}$ and $(M_n^2 )_{n \in \mathbb{N}_0}$ are martingales. Then $(M_n^5)_{n \in \mathbb{N}_0}$ is also a martingale. Is the above statement true or false? I was suggested by someone to disprove this statement by finding a counterexample. I was trying so hard but still cannot come out with a martingale which is itself and the square of itself martingale. It seems easy once we can come out with a suitable martingale.,"['stochastic-processes', 'probability-theory', 'martingales']"
3878356,Random graph properties: understanding role of expectation,"It is often in the random graph theory proofs that we are looking at the expectation. But why? Why is it not the probability that we studying. To clarify my question, look at the following example. Assume we are working in the $G(n,p)$ model. What is the probability that we have an induced cycle with t edges in $G(n,p)$ ? My approach would be. Fix $t$ vertices. The probability of having induced cycle on these $t$ vertices is $p^t(1-p)^{\binom{n}{2}-t}$ . Now consider all possible $\binom{n}{t}$ subsets of $t$ vertices. Probability of having an induced cycle in a graph is equal to probability that at least one of these $t$ -subsets of vertices has an induced cycle, which is the sum of the probabilities over all $t$ -sets to have an induced cycle which is: $$\text{Probability of having an induced cycle in G(n,p)}= \binom{n}{t} p^t(1-p)^{\binom{n}{2}-t}$$ Suppose, I choose $N=N(p)$ such that this quantity is $<1$ . Then MY QUESTION IS: can I conclude that there exists a graph on $N$ vertices having no induced cycle, because the probability above is <1? Why do people even consider expectation? I know that it is possible to define indicator random variables for each $t$ -set and then calculate the expected number of induced cycles. Provided that this expected number is $<1$ , we can say that there will be a graph on $N$ vertices with no induced cycle. TL;DR Why just considering a probability alone would not be enough? Why do we even need expectation? Many thanks!","['random-graphs', 'statistics', 'graph-theory', 'expected-value', 'probability']"
3878377,Find $\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx$ in polar coordinates.,"I need to find: $$\int_0^2 \int_0^{\sqrt{3}x} f(\sqrt{x^2+y^2})dydx$$ in polar coordinates. Since $x=r\cos(\theta)$ and $y=r\sin(\theta)$ , I got: $y=\sqrt{3}x \iff r\sin(\theta)=\sqrt{3}r\cos(\theta)\iff \tan(\theta)=\sqrt{3}\iff \theta=\arctan(\sqrt{3})$ From this I conclude that $0 \leq \theta \leq \arctan(\sqrt{3})$ . My problem is I'm not sure where the radius is, I thought about from $0$ to $2\sqrt{3}$ (from graphing $y=\sqrt{3}$ ) but I think it is wrong. Is there something I'm missing in order to get the $r$ interval?","['multivariable-calculus', 'multiple-integral']"
3878405,Is indefinite integration suspect?,"In this post , Qiaochu Yuan remarks that 'it is convenient but misleading to write $$
\int f(x) \, dx=g(x)
$$ [where the derivative of $g$ is $f$ ]'. This sentiment seems to be shared by many contributors here, and I don't understand why. To me, both definite and indefinite integration are both valid operations you can perform on a function, and there is nothing suspect about indefinite integration. I know about the fundamental theorem of calculus, which (as far as I understand) explains the link between indefinite and definite integration. If by integration we mean computing the area under the graph, the fundamental theorem of calculus shows us that integration is the opposite of differentiation, since $$
\frac{d}{dx} \int_{a}^{x} f(t) \, dt = f(x)
$$ This shows that every continuous function has an antiderivative. Since a clear link between integration and antidifferentiation has been established, we give the antiderivative the convenient label 'indefinite integral'. (This also explains why the definite and indefinite integration notations are so similar.) This label is fine, so long as we remember that integration is defined as finding the area under the graph, while antidifferentiation is defined as finding the inverse of the derivative. Another result of the fundamental theorem of calculus is that $$
\int_{a}^{x}f(t) \, dt=\int f(x) \, dx
$$ So obviously every indefinite integral can be rewritten in terms of definite integrals, but I don't understand the motivation behind this. If $F$ is an antiderivative of $f$ , then why is it more correct to write $$
\int_{a}^{x} f(t) \, dt = F(x) \, ,
$$ compared to $$
\int f(x) \, dx = F(x) \, ?
$$","['integration', 'calculus', 'real-analysis']"
3878452,Continous Piecewise Function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I recently found this question: find $c$ and $d$ such that $$f(x) = \begin{cases} 
      cx+4d & x<2\\
      x^{2}+4 & 2\leq x\leq 3 \\
      dx^{2}+\frac{2x}{c}+1 & x>3
   \end{cases}$$ is continuous everywhere. How should I solve this?","['continuity', 'functions', 'piecewise-continuity']"
3878609,Symbol for membership of one and only,"Is there a symbol that states $a$ is only a member of $A$ as in the image? I can use ( $a \in A - B$ ) or ( $a \in A$ and $ a \notin B $ ). But, I need the shortest possible mathematical description for this if available. Since I use longer names for sets such as UniqueGeometryA and UniqueGeometryB , the mathematical phrases that I use become longer.","['elementary-set-theory', 'notation']"
3878783,Convergence of integral using Cauchy's theorem,"I've always thought that integrals of the form $\int_0^{x_0}  \frac{dx}{x^d}$ for $d\geq1$ don't converge. However, during a calculation, an integral of this kind (with $d=2$ ) appeared and I really needed to evaluate it in some way. Here's what I thought: given that, for $d=2$ , the integral is even, we can write it like (also adding a convergence factor) $$ \int_0^{x_0} \frac{dx}{x^2} = \frac{1}{2} \lim_{\delta \rightarrow 0} \int_{-x_0}^{x_0}\frac{dx}{(x-i\delta)^2}$$ Now, let $x\in \mathbb{C}$ and use Cauchy's theorem for the pole located at $x=i\delta$ $$ \int_{-x_0}^{x_0}\frac{dx}{(x-i\delta)^2} = 2\pi i Res\big[(x-i\delta)^{-2},x=i\delta\big] - \int_{\Gamma} \frac{dx}{(x-i\delta)^2} $$ where the contour is $\Gamma = \{x_0e^{i\theta}, \theta \in [0,\pi] \} $ In fact, the residue evaluates to zero. For the integral that is missing, let me perform the change of variables $x=x_0e^{i\theta}$ and $dx= ix_0e^{i\theta} d\theta$ , such that $$\int_{\Gamma} \frac{dx}{(x-i\delta)^2} = ix_0 \int_0^\pi d\theta \  \frac{e^{i\theta}}{x_0^2e^{2i\theta} - 2i\delta x_0e^{i\theta} - \delta^2} = ix_0 \int_0^\pi d\theta \  \frac{1}{x_0^2e^{i\theta} - 2i\delta x_0- \delta^2e^{-i\theta}}$$ A primitive for the last integrand is $$\frac{i}{e^{i\theta}x_0^2-i\delta x_0}$$ which implies $$\int_{\Gamma} \frac{dx}{(x-i\delta)^2}  = \frac{2x_0}{x_0^2+\delta^2}$$ Therefore, plugging all these results together seems to imply that $$ \int_0^{x_0} \frac{dx}{x^2}  = - \frac{1}{x_0}$$ Can anyone please tell me what did I do wrong? Because I'm pretty sure this should diverge, but I can't find the wrong step here.","['integration', 'complex-analysis', 'calculus', 'convergence-divergence']"
3878837,A combinatorics puzzle related to Isserlis' theorem,"The puzzle is as follows: $$
\newcommand{\empty}{\phantom{1}}
\newcommand{\boxes}[2]{\boxed{#1}\hspace{-0.04cm}\boxed{#2}}
$$ At iteration (1) you start out with a pair of boxes, one empty and one with a 1: $$
\boxes{1}{\empty}
$$ At iteration $i$ do the following: Fill each empty box, creating a new term for each box you fill Then, add new terms corresponding to all of the terms from iteration $i - 1$ , but with a $\boxes{i}{\empty}$ box tacked onto the end. Examples : Iteration (2) would look like: $$
\boxes{1}{2} + \boxes{1}{\empty}\:\boxes{2}{\empty}
$$ where we've added one term for the one empty box, and then tacked on $\boxes{2}{\empty}$ to the term from iteration (1). Iteration (3) would look like: $$
\boxes{1}{3} \boxes{2}{\empty} + \boxes{1}{\empty} \boxes{2}{3} + \boxes{1}{2} \boxes{3}{\empty} + \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty}
$$ Here the first two terms come from filling the two empty boxes, and the last two terms come from tacking on a $\boxes{3}{\empty}$ to the end of the previous terms. Iteration (4) would look like: \begin{multline}
\boxes{1}{3} \boxes{2}{4} 
+ \boxes{1}{4} \boxes{2}{3} 
+ \boxes{1}{2} \boxes{3}{4} 
+ \boxes{1}{4} \boxes{2}{\empty} \boxes{3}{\empty} 
+ \boxes{1}{\empty} \boxes{2}{4} \boxes{3}{\empty} \\
+ \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{4} 
+\boxes{1}{3} \boxes{2}{\empty} \boxes{4}{\empty}
+ \boxes{1}{\empty} \boxes{2}{3} \boxes{4}{\empty}
+ \boxes{1}{2} \boxes{3}{\empty} \boxes{4}{\empty}
+ \boxes{1}{\empty} \boxes{2}{\empty} \boxes{3}{\empty} \boxes{4}{\empty}
\end{multline} Once you have done $n$ iterations get rid of any terms which have an empty box. Here, iterations (1) and (3) would just give nothing, while (2) would give $\boxes{1}{2}$ and (4) would give: $$
\boxes{1}{3} \: \boxes{2}{4} 
+ \boxes{1}{4} \: \boxes{2}{3} 
+ \boxes{1}{2} \: \boxes{3}{4}
$$ How do you describe what you're left with at the end ? I think this should end up being identical to Isserlis' Theorem , but I'm hoping to come up with a slick combinatorial proof given this formulation. I haven't been successful so far, and all of the proofs that I have found use results from statistics that I am not familiar with.","['statistics', 'puzzle', 'means', 'combinatorics', 'gaussian']"
3878839,Is it possible to define the standard function notation $f(x)=y$ in terms of an arbitrary relation?,"$%PREAMBLE
\newcommand{\FITCH}[1]{\begin{array}{rlr}#1\end{array}}
\newcommand{\FC}[1]{\begin{array}{r}#1\end{array}} %FirstColumn
\newcommand{\SC}[1]{\begin{array}{c|l}#1\end{array}} %SecondColumn
\newcommand{\TC}[1]{\begin{array}{r}#1\end{array}} %ThirdColumn
\newcommand{\SUBPROOF}{\\[-0.27em]} %adjusts line spacing slightly
$ In elementary set theory (as far as I know) $f$ is a function if, and only if $f$ is a relation: $\forall x[x \in f \rightarrow \exists y \exists z[x = \langle y,z \rangle]]$ , and $f$ is many-one: $\forall x \forall y \forall z[\langle x,y \rangle \in f \land \langle x,z \rangle \in f \rightarrow y=z]$ Now, in maths, it is very unusual to see the notation $\langle x,y \rangle \in f$ being used, with the notation $f(x)=y$ being preferred. However, this type of notation doesn't work for any relation. Specifically, it only works if the $y$ is unique for any given $x$ . As an example, taking an relation $A=\{ \langle 1,2 \rangle, \langle 1,1 \rangle \}$ , using the notation $A(1) = 2$ and $A(1) = 1$ , we can conclude that $1=2$ , which is an absurd. This notation only works if the relation is a function. So we can define the notation as: $$f(x)=y \leftrightarrow f \text{ is a function } \land \langle x,y \rangle \in f$$ Which is valid. However it would be very interesting if we could define a notation $f(x)$ for any relation, and then prove that it's unique only for functions, and then be able to use it accordingly. A definition like: $$f(x) \leftrightarrow \langle x,y \rangle \in f$$ is not really valid, since y is free but only appears on one side of the definition. We could define it like: $$f(x)_y \leftrightarrow \langle x,y \rangle \in f$$ Which seems valid. However, it doesn't tell us how to ""remove"" it to use the unique value $f(x)$ , or even that $f(x)$ is a variable. We could try: $$f(x)=y \leftrightarrow \langle x,y \rangle \in f$$ However, that leads us right back to $1=2$ . So my question is, is it possible for us to define this type of notation for an arbitrary relation, and then prove that $f(x)=y \leftrightarrow \langle x,y \rangle \in f$ if, and only if, $f$ is a function?
Or is it necessary to embed the function status of $f$ in the definition? Edit: My question states: [...] So we can define the notation as: $$f(x)=y \leftrightarrow f \text{ is a function } \land \langle x,y \rangle \in f$$ Which is valid. It is not . After making a question to discuss the dubiousness of this (type of) definition, it is now clear that its validity would imply that every set is a function, which quoting Z.A.Q. 's answer : Needless to say, this latter statement is very much not a theorem of any reasonable set theory. The Q&A goes in more details for why this is the case. As such, the above definition is not consistent with set-theory and should not be used . Despite that, Noah's answer is still perfectly valid.","['elementary-set-theory', 'definition', 'first-order-logic']"
3879032,How do you evaluate the limit of this sequence involving n-th roots?,"This is the limit: \begin{equation}
\lim_{n\to+\infty}\sqrt[n]{n}\cdot\sqrt[n+1]{n+1}...\sqrt[2n]{2n}
\end{equation} I tried to rearrange the terms to apply the geometric mean theorem but my attempt was not successful. Any way to solve this will be fine.","['real-analysis', 'calculus', 'products', 'limits', 'sequences-and-series']"
3879043,Probability of finding the fly inside a range,"A fly is traversing the non-negative x-axis. It starts at $x_0=k$ . At the $i^{th}$ step (starting from the zeroth step), it uniformly randomly jumps to a point in the range $[0,x_i]$ . Probability that the fly is in the range $[a,b]$ after $n$ jumps is denoted by $P_n(a,b)$ . Find $$\lim_{y \to 0}\frac{P_n(1,1+y)}{y}$$ Any help will be appreciated. EDIT:
Sorry for the late response. Here is my brief try. If we consider the condition that after $n$ jumps, the fly is at a position $> a$ , but exclude the condition that it should be $< b$ . Let $p=a/k$ \begin{align}
\begin{split}
P_n(a)&=\int\limits_{x1=a}^k ~\int\limits_{x_2=a}^{x_!}....\int\limits_{x_n=a}^{x_{n-1}}\frac{1}{k}\frac{1}{x_1}...\frac{1}{x_{n-1}}\ dx_1 \ dx_2 ... \ dx_n\\
&=1-p+p\sum_{i=1}^{n-1}\frac{(\log p)^i}{i!}
\end{split}
\end{align}","['probability-distributions', 'probability-theory', 'probability']"
3879128,Finding $\lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx$,"I have to calculate the following limits, using a theorem but I don't really know what theorem to use (it is for the subject of measurement and integration, for the unit ""Measurable functions, integration and its properties""). $\space$ $$\lim_{n \to \infty} \int_{2}^{\infty} \frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)} dx$$ Do I have to use the dominated convergence theorem of Lebesgue? I have first of all calculated $\lim\limits_{n \to \infty}\frac{n\sin\left(\frac{x-2}{n}\right)}{(x-2)+(1+(x-2)^2)}$ and I've obtained $\frac{1}{2x-3}$ Now, I want to calculate $\int_{2}^{\infty} \frac{1}{2x-3}\,dx$ but $\ln(\infty)$ doesn't exist... so what am I doing wrong?","['integration', 'limits', 'measure-theory']"
3879137,11 birds in 3 same cages,"How many ways can we put 11 birds in 3 identical cages if we put at least 3 birds in every cage? Is this correct: We can arrange birds so that we have 3,3,5 or 3,4,4 birds in cages. first case: We select 5 birds for first cage and then 3 for second and we put last 3 in last cage. Clearly we must divide thi by 2 since if we put say birds $1,2,3$ and $4,5,6$ in third it is the same if we put $4,5,6$ second and $1,2,3$ in third cage. So we have $${1\over 2}{11\choose 5}{6\choose 3}$$ ways to do this. second case: We get $${1\over 2}{11\choose 3}{8\choose 4}$$ Now we add this and we are done?","['contest-math', 'combinations', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
3879144,"Let $X \subseteq \mathbb{R}$ s.t $X \cap (y+X) \neq \emptyset \ \forall y \in \mathbb{R}$, prove $X$ is not countable","Given $X \subseteq \mathbb{R}, y \in \mathbb{R}$ , define $y+X \triangleq \left \{ y+x : x\in X \right \}$ Let $X \subseteq \mathbb{R}$ s.t $X \cap (y+X) \neq \emptyset \  \forall y \in \mathbb{R}$ . Prove $X$ is not countable. My first idea was trying to use something similar to Cantor's diagonalization but I'm pretty sure it doesn't work here, so I'm kind of clueless. A hint on how I should approach this, rather than a solution, would be best.",['discrete-mathematics']
3879167,Showing $\sqrt{\frac{1-\cos20^\circ}{1+\cos40^\circ}} =\frac{\cos80^\circ}{\cos20^\circ}$,"Here is a problem: Here is (the short version of) the question: I did this problem using the Cosine Rule on $\triangle PQU$ and $\triangle PTU$ . This gave me an answer of $$\sqrt{\frac{1-\cos20^\circ}{1+\cos40^\circ}} \tag{1}$$ The given answer is, however: $$\frac{\cos80^\circ}{\cos20^\circ} \tag{2}$$ Both give the same numerical answer when put into a calculator. I was wondering whether it is possible to convert/manipulate $(1)$ into $(2)$ . Or, is using the Cosine Rule just not acceptable here, to get the ""correct"" answer? Many thanks. If you don't understand how I got my answer or how they got theirs, I can attach both pieces of working. Hopefully it is clear though! Thanks again.","['trigonometry', 'problem-solving', 'geometry']"
3879201,Show that the characteristic function of a finite signed measure on a normed vector space is uniformly continuous,"Let $E$ be a normed $\mathbb R$ -vector space, $\mu$ be a finite signed measure on $(E,\mathcal B(E))$ and $$\hat\mu:E'\to\mathbb C\;,\;\;\;\varphi\mapsto\int\mu({\rm d}x)e^{{\rm i}\varphi}$$ denote the characteristic function of $\mu$ . Replying to a previous formulation of this question, Kavi Rama Murthy has shown that if $E$ is complete and separable and $\mu$ is nonnegative, then $\hat\mu$ is uniformly continuous. It is easy to see that his proof still works in the general case as long as we are assuming that $\mu$ is tight $^1$ , i.e. $$\forall\varepsilon>0:\exists K\subseteq E\text{ compact}:|\mu|(K^c)<\varepsilon\tag1.$$ Taking a closer look at the proof, I've observed the following: Let $\langle\;\cdot\;,\;\cdot\;\rangle$ denote the duality pairing between $E$ and $E'$ and $$p_x(\varphi):=|\langle x,\varphi\rangle|\;\;\;\text{for }\varphi\in E'$$ for $x\in E$ . By definition, the weak* topology $\sigma(E',E)$ on $E'$ is the topology generated by the seminorm family $(p_x)_{x\in E}$ . Now, if $K\subseteq E$ is compact, $$p_K(\varphi):=\sup_{x\in K}p_x(\varphi)\;\;\;\text{for }\varphi\in E'$$ should be a seminorm on $E'$ as well. And if I'm not missing something, the topology generated by $(p_K:K\subseteq E\text{ is compact})$ is precisely the topology $\sigma_c(E',E)$ of compact convergence on $E'$ . What Kavi Rama Murthy has shown is that, since $\mu$ is tight, for all $\varepsilon>0$ , there is a compact $K\subseteq E$ and a $\delta>0$ with $$|\hat\mu(\varphi_1)-\hat\mu(\varphi_2)|<\varepsilon\;\;\;\text{for all }\varphi_1,\varphi_2\in E'\text{ with }p_K(\varphi_1-\varphi_2)<\delta\tag2.$$ Question : Are we able to conclude that $\hat\mu$ is $\sigma_c(E',E)$ -continuous? EDIT : In order to conclude that $\hat\mu$ is (uniformly) $\sigma_c(E',E)$ -continuous, we need to that $(2)$ holds for $K$ replaced by an arbitrary compact $\tilde K\subseteq E$ . Given $\varepsilon>0$ , we can show $(2)$ by choosing the compact subset $K\subseteq E$ such that $$|\mu|(K^c)<\varepsilon\tag3.$$ We may then write \begin{equation}\begin{split}\left|\hat\mu(\varphi_1)-\hat\mu(\varphi_2)\right|&\le\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:\varepsilon}\\&\;\;\;\;\;\;\;\;\;\;\;\;+\int_{K\cap\tilde K^c}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|\\&\;\;\;\;\;\;\;\;\;\;\;\;+\underbrace{\int_{K\cap\tilde K}\left|e^{{\rm i}\varphi_1}-e^{{\rm i}\varphi_2}\right|{\rm d}\left|\mu\right|}_{<\:2\varepsilon}\end{split}\tag4\end{equation} for all $\varphi_1,\varphi_2\in E'$ with $p_{\tilde K}(\varphi_1-\varphi_2)<\delta$ , where $$\delta:=\frac\varepsilon{\left\|\mu\right\|},$$ but I have no idea how we can control the second integral. EDIT 2 A ""proof"" of this claim can be (found in Linde's Probability in Banach Spaces ), but I have no idea why this proof is correct, since he is concluding the continuity immediately from $(2)$ (for a single $K$ ): Maybe we need to assume that $\mu$ is even Radon, i.e. that for all $B\in\mathcal (E)$ , there is a compact $C\subseteq E$ with $C\subseteq B$ and $|\mu|(B\setminus C)<\varepsilon$ . The author is actually imposing this assumption, but he obviously doesn't make use of it in his proof (he would need to consider an arbitrary compact $\tilde K\subseteq E$ , as I did above). $^1$ On a complete separable metric space, every finite signed measure is tight.","['measure-theory', 'characteristic-functions', 'functional-analysis', 'weak-topology', 'probability-theory']"
3879262,Solve equation including $f(x+c) + f(x)$ for $f(x)$.,"How to solve the following equation for $F(x)$ ? $$ (1-\alpha)\beta \cdot F(x+c) - \alpha \cdot F(x) = \alpha + (1-\alpha)\beta - {\alpha \cdot \gamma \over x + \phi}$$ where $\alpha, \beta, \gamma, \phi$ are all constant and $\in (0,1)$ ; $F(x)$ needs to be a $cdf$ . Or more general, is there a formal way to simplify and solve an equation for $f(x)$ of the following form: $$ f(x+c) + f(x) = g(x)$$ where $g(x)$ is known? Thank you in advance!","['functional-equations', 'delay-differential-equations', 'ordinary-differential-equations']"
3879287,"Need help regarding intuition of rows in a coordinate/basis matrix, where the columns are vectors.","Given a matrix M such that its columns are the vectors of a new basis with respect to another basis B. To find the coordinates of v in the other basis, we can simply take $M[v]_M = [v]_B$ . Let me give an example of M $$\begin{bmatrix}1&2\\ 4&3\end{bmatrix}$$ I believe they are linearly independent(i just pulled out some random number off my head and tested), but the numbers aren't that important. What i am confused about is we know that the columns of M form a set of basis vectors but when doing $M[v]_m$ matrix multiplication, we iterate within each $row_i$ of M for each value in the corresponding row of the output vector instead. Now, i learn that, in my school's materials convention, we represent linear functionals as row vectors instead, since column vectors are for things like coordinate vectors and this makes sense to me at least here, but above, i am using a basis matrix's rows like linear functionals? So yeah, is it just ""it is how it is because matrix multiplication rules"", or is there some special property or something about rows in matrices.","['matrices', 'linear-algebra', 'linear-transformations']"
3879364,"Does a function $f$ that satisfies $f(2x+y)=f(x+1)+f(y)+4xy$ for all $x,y$ exist? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Does a function $f$ that satisfies $f(2x+y)=f(x+1)+f(y)+4xy$ for all $x,y$ exist? I was able to figure out some specific values of such a function by plugging in certain numbers, for instance using $x=0,y=1$ I found that $f(1)=0$ .  Similarly, I found the values of $f(0)$ by using $x=1,y=0$ and $f(2)$ using $x=-1,y=2$ .  But I can't find the entire function...","['functional-equations', 'functions']"
3879439,"On $\mathbb{R}^\omega$ are these metrics equivalent $\sum_{n=1}^\infty \frac{1}{2^n}\min(1,|a_n-b_n|)$ vs with $\frac{|a_n-b_n|}{n^2(1+|a_n-b_n|)}$","How can I show that on $\mathbb{R}^\omega$ these two metrics are equivalent $\sum_{n=1}^\infty \frac{1}{2^n}\min(1,|a_n-b_n|)$ and $\sum_{n=1}^\infty \frac{1}{n^2}\frac{|a_n-b_n|}{1+|a_n-b_n|}$ ? Actually I'm not even sure whether they're equivalent but my intuition is that since as $a_n$ and $b_n$ get closer $\min(1,|a_n-b_n|)$ and $\frac{|a_n-b_n|}{1+|a_n-b_n|}$ will be nearly the same, and I assume that the $\frac{1}{2^n}$ and $1/n^2$ terms wont make a difference. However I don't know how to proceed with the proof. I'd love any kind of hint!","['sequences-and-series', 'functional-analysis', 'real-analysis']"
3879471,I tried to prove $\sum_{n=0}^∞ \frac{n^2}{2^n} = 6$ but I feel that my proof is long. Can anyone provide an alternate proof?,Some parts of the text might not be clear so please ask about them in the comments. Sorry for the uploaded image as it was taking a long time for me to write the whole proof.,['sequences-and-series']
3879508,Pattern of divisibility within $2^n+1\equiv0\pmod{n}$,"In recent days, I have been studying:the properties of $m^n+h\equiv0\pmod n$ where $m,n\in\mathbb{N}$ and $h\in\mathbb{Z}$ , and I am trying to calculate all solutions for $2^n+1\equiv0\pmod n$ for all $n\leq10^{15}$ . Obviously, if one were to calculate the solutions by checking the numbers one by one, it would have taken an extremely long amount of time to do so. Therefore, I have tried to reduce the numbers needed to check by finding the divisibility patterns within the solutions. From Proposition 3 of the paper ""Primitive solutions of $n|2^n+1$ "", if $n$ is not a power of $3$ and $n=3^kq$ where $k\in\mathbb{Z}$ , $k\geq0$ , $k|2^k+1$ and $q\neq1$ (In other words, $3^k|n$ and $n\neq3^k$ ), then $q$ is also divisible by one of the primes $p$ such that $p|2^{3^k}+1$ . For example, if $81|n$ and $n\neq81$ , then one of { $243,1539,13203,7064091,10970073,22032887841$ } divides $n$ . However, I have also noticed that the same argument seems to be true for some other divisors. An example is that if $171|n$ and $n\neq171$ , then it seems that one of { $513,3249,97641,29884473,27439598619,3279345384337785847895810090688081$ } divides $n$ . Is it true for all divisors, and how can I prove it? References: Toby Bailey and Christopher J. Smyth (2008). Primitive solutions of $n|2^n+1$ , https://www.maths.ed.ac.uk/~chris/papers/n_divides_2to_nplus1.pdf",['number-theory']
3879532,Show that a function $f : \mathbb R → \mathbb R$ is continuous if and only if its graph $[f]$ is path-connected,"The graph of a function $f : X → Y$ refers to the set $[f] :=$ { $(x, f(x)) : x ∈ X$ } $⊆ X × Y$ . Show that a function $f : \mathbb R → \mathbb R$ is continuous if and only if its graph $[f]$ is path-connected. My attempt: Suppose { $x_n$ } is a sequence in $\mathbb R$ converging to some $x$ . We will show that there is a subsequence so that $f(x_{n_k})→f(x)$ . This is sufficient; for if $f$ were not continuous at $x$ , there would exist an $ϵ$ such that for any $n$ we could find an $x_n$ with $|x_n-x|<1/n$ but $|f(x_n)−f(x)|>ϵ$ . Then we would have $x_n→x$ , but no subsequence of { $f(x_n)$ } could converge to $x$ , contradicting our claim. Since the graph of $f$ is path connected, there is a continuous path $c(t)=(u(t),v(t))$ with $v(t)=f(u(t))$ for each $t$ , and $c(0)=(a,f(a))$ , $γ(1)=(b,f(b))$ . By the intermediate value theorem, for each $n$ there is a $t_n∈[0,1]$ with $u(t_n)=x_n$ . Since $[0,1]$ is compact, we can find a subsequence { $t_{n_k}$ } converging to some $t$ . Then by continuity of $u$ , since $u(t_{n_k})=x_{n_k}→x$ , we have $u(t)=x$ . Now $v$ is also continuous, so $f(x_{n_k})=f(u(t_{n_k}))=v(t_{n_k})→v(t)=f(u(t))=f(x)$ .
We have thus constructed the desired subsequence. How to show the other conclusion","['connectedness', 'path-connected', 'sequences-and-series', 'elementary-set-theory', 'general-topology']"
3879636,Find all polynomial functions,"Find all polynomials $P(x)=a_0+a_1x+... a_nx^n; a_i \in Z$ such that $\forall x$ : $P(x)=(x-P(0))(x-P(1))...(x-P(n-1))$ I substituted some values like $0,1$ to get an idea of the polynomial coefficients and tried to see some pattern but it doesn't seem to be helping much. Also is it possible that calculus might play a role here since we have been given a polynomial function and not just a functional equation in general?","['contest-math', 'functional-equations', 'calculus', 'polynomials']"
3879661,Can a power series uniformly converge on open disc?,"Does there exist a power series of radius of convergence $R$ that uniformly converges on the open disc of radius $R$ ? Intuitively, I do not think this is the case since there would be a singularity at some point on $|z|=R$ , and so when we get near there, the series tends towards infinity and there is no way for the series to converge uniformly. But I have heard from others that the answer is that such a power series does in fact exist.","['complex-analysis', 'power-series', 'uniform-convergence', 'sequences-and-series']"
3879739,Solving a differential equation using an integrating factor,"I'm trying the solve the following equation: $ \left\{\begin{matrix}
(x^B+y^B)(xdy-ydx)=(1+x)x^9dx \\  y(-1)=A \end{matrix}\right. $ for $A=1$ and $A=0$ . $B\in2\mathbb{N}_0+1$ My solution is the following, but I got stuck: $(x^B+y^B)(xdy-ydx)=(1+x)*x^9dx$ $x^B*xdy-y*x^Bdx+y^B*xdy-y*y^Bdx-(1+x)*x^9dx=0$ $x^B*xdy+y^B*xdy-y*x^Bdx-y*y^Bdx-(1+x)*x^9dx=0$ $xdy(x^B+y^B)+[-y(x^B+y^B)-(1+x)*x^9]dx=0$ where: $Q(x,y)=x(x^B+y^B)dy$ $P(x,y)=[-y(x^B+y^B)-(1+x)*x^9]$ Then I made partial derivation: $\frac{∂Q}{∂x}:(x^B+y^B)x(Bx^{B-1})$ $\frac{∂P}{∂y}: -(x^B+y^B)-y(By^{B-1})$ And then subtract: $\frac{∂P}{∂y}-\frac{∂Q}{∂x}=-(x^B+y^B)-y(By^{B-1})-(x^B+y^B)-x(Bx^{B-1})$ But probably I did something wrong and
I'm stuck and not sure where made I mistake.. Can you help me please?",['ordinary-differential-equations']
3879750,Are derivatives of geometric progressions all irreducible?,"Consider the polynomials $P_n(x)=1+2x+3x^2+\dots+nx^{n-1}$. Problem A5 in 2014 Putnam competition was to prove that these polynomials are pairwise relatively prime. In the solution sheet there is the following remark: It seems likely that the individual polynomials $P_k(x)$ are all irreducible, but this appears difficult to prove. My question is exactly about this: is it known if all these polynomials are irreducible? Or is it an open problem? Thanks in advance.","['irreducible-polynomials', 'polynomials', 'open-problem', 'reference-request']"
3879761,How to solve this recursion which is not homogenous,"I have the following recursion $$a_n = \frac{1}{4}a_{n-1}+\frac{1}{4}(\frac{2}{3})^{n-1}$$ I've tried first to solve the homogeneous equation (shifting by one) $$(E - \frac{1}{4})a_n = 0$$ where $Ea_n = a_{n+1}$ is the shift operator. The only solution to this equation is $E=\frac{1}{4}$ . Now I thought that for a non-homogeneous equation, where the term $d(n)$ does not depend on the underlying recursion has the form $d(n) = k\mu^n$ and $\mu$ is not a root of the homogeneous equation, the solution is given by $$a_n = \frac{k\mu^n}{\Phi(\mu)}$$ where $\Phi$ is the characteristic equation of the homogeneous one. In my case $d(n) = \frac{1}{4}\frac{2}{3}^{n}$ , so $k=\frac{1}{4}$ and $\mu = \frac{2}{3}$ . Thus the solution should be given by $$a_n = \frac{\frac{1}{4}\frac{2}{3}^n}{\frac{2}{3}-\frac{1}{4}}=\frac{\frac{1}{4}\frac{2}{3}^n}{\frac{5}{12}}=\frac{3}{5}\frac{2}{3}^n$$ However, the solution should be $$\frac{3}{5}\frac{2}{3}^n-\frac{3}{5}\frac{1}{4}^n$$ . What did I wrong? Note: the question arises from another problem, see here","['summation', 'recursion', 'recurrence-relations', 'discrete-mathematics', 'sequences-and-series']"
3879837,Variance of a bootstrap estimator,"Suppose we have a sample $X_1,X_2,...,X_n \sim F$ , where the distribution $F$ is unknown.  Let $T_n = g(X_1,X_2,...,X_n) = \bar{X}^2$ , $\mu = \mathbb{E}[X_1]$ , and define the following: $$\alpha_k = \int \left | x - \mu \right | ^k dF(x) \ \ \ \  \text{and} \ \ \ \ \hat{\alpha}_k = \frac{1}{n}\sum_{i=1}^{n}\left | X_i - \bar{X_n} \right |^k.$$ We can see that $\hat{\alpha}_k$ is the plug-in estimator for $\alpha_k$ . Now suppose we take $B$ bootstrap samples $X_1^*, X_2^* , ..., X_n^*$ and compute $T_n^*$ .  I want to show that the variance of $T_n^*$ , that is, the variance of our bootstrap estimate, is $$v_{\text{bootstrap}}(T_n^*) = \frac{4\bar{X_n}^2 \hat{\alpha}_2}{n} + \frac{4\bar{X_n} \hat{\alpha}_3}{n^2}+\frac{\hat{\alpha}_4}{n^3}.$$ In general, the variance of a bootstrap estimator $S_n^*$ with $B$ bootstrap samples is $$v_{\text{bootstrap}} = \frac{1}{B}\sum_{b=1}^{B}\left ( S_{n,b}^* - \frac{1}{B}\sum_{r=1}^{n}S_{n,r}^* \right )^2,$$ where $S_{n,b}^*$ is the statistic computed from the $b^{\text{th}}$ bootstrap sample.  If I use this definition and apply it to the original problem, I obtain something like \begin{equation}
\begin{split}
v_{\text{bootstrap}} &= 
\frac{1}{B}\sum_{b=1}^{B}\left ( \bar{X}_{n,b}^{*2} - \frac{1}{B}\sum_{r=1}^{n}\bar{X}_{n,r}^{*2} \right )^2 \\ &=
\frac{1}{B}\sum_{b=1}^{B}\left [ \bar{X}_{n,b}^{*4} - 2\bar{X}_{n,b}^{*2}\frac{1}{B}\sum_{r=1}^{B}\bar{X}_{n,r}^{*2} + \frac{1}{B^2}\sum_{r=1}^{B}\bar{X}_{n,r}^{*2}\right  ] .\\
\end{split}
\end{equation} From here, I don't see anything I can do to get a nicer form.  So although this should simplify to $v_{\text{bootstrap}}(T_n^*),$ I am thinking this is probably not the best approach.  Another approach that I thought of was conditioning.  We have $$\mathrm{Var}[\bar{X}^{*2}] = \mathbb{E}[\mathrm{Var}[\bar{X}^{*2} | X_1, X_2, ... , X_n]] +\mathrm{Var}[\mathbb{E}[\bar{X}^{*2} | X_1, X_2, ... , X_n]].$$ This seems more computable.  The bootstrap distribution is as follows. \begin{array}{|c|c|c|c|}
\hline
x & \mathbb{P} (X^* = x )\\ \hline
X_1 & 1/n \\ \hline
X_2 & 1/n \\ \hline
\vdots & \vdots  \\ \hline
X_n & 1/n \\ \hline
\end{array} From this, the expected value and variance of $\bar{X}^*$ come easily, but the squared term in $\bar{X}^{*2}$ is what is tripping me up. Does anyone have any ideas or possible solutions?  Thanks.","['bootstrap-sampling', 'statistics', 'variance', 'estimation']"
3880000,Strong law of large numbers under equivalent measures,"Suppose $\{X_n\}$ is a sequence of square-integrable i.i.d. random variables under the measure $\mathbb{P}$ . Under the strong law of large numbers we have that \begin{align*}
\mathbb{P}\left(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^nX_j=\mathbb{E}_{\mathbb{P}}[X_1]\right)=1.
\end{align*} If $\mathbb{Q}$ is an equivalent measure to $\mathbb{P}$ with $\frac{d\mathbb{Q}}{d\mathbb{P}}\in L^2(\mathbb{P})$ then each $X_i$ is still integrable under $\mathbb{Q}$ and the strong law of large numbers applied to $\mathbb{Q}$ will give \begin{align*}
\mathbb{Q}\left(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^nX_j=\mathbb{E}_{\mathbb{Q}}[X_1]\right)=1.
\end{align*} As the measures are equivalent, this also means that \begin{align*}
\mathbb{P}\left(\lim_{n\to\infty}\frac{1}{n}\sum_{j=1}^nX_j=\mathbb{E}_{\mathbb{Q}}[X_1]\right)=1
\end{align*} which implies that $\mathbb{E}_{\mathbb{P}}[X_1]=\mathbb{E}_{\mathbb{Q}}[X_1]$ . However this is generally not the case. Can anyone find where the argument above breaks down?","['law-of-large-numbers', 'probability-theory', 'probability']"
3880003,"Evaluating $\int_{-4} ^4\int _0 ^{\sqrt{16-x^2}} \int _0 ^{16-x^2-y^2} \sqrt{x^2 + y^2}\,dz\,dy\,dx$","Question : Evaluate the given triple integral with cylindrical coordinates: $$\int_{-4} ^4\int _0 ^{\sqrt{16-x^2}} \int _0 ^{16-x^2-y^2} \sqrt{x^2 + y^2}\,dz\,dy\,dx$$ My solution (attempt): Upon converting the triple integral into cylindrical coordinates, I got: $$\int_{0} ^{\pi/2}\int _0 ^{4} \int _0 ^{16-r^2\cos^2\theta-r^2\sin^2\theta} r^2\,dz\,dr\,d\theta$$ After solving for the solution, I got $1024\pi/15$ as the answer. Apparently, this is incorrect. Could someone please show me where integral conversion is wrong? Thanks in advance!","['cylindrical-coordinates', 'multivariable-calculus', 'multiple-integral', 'bounds-of-integration']"
3880063,$f$ have finitely many critical points in $\Omega$,"Assume that $\Omega$ is an bounded open set in $R^m, f\in C^2(\overline{\Omega},R^m)$ . If $f$ does not have any critical point in $\partial \Omega$ , and all the critical points of $f$ in $\Omega$ are non-degenerate, prove that $f$ has finitely many critical points in $\Omega$ . Note that $a$ is a critical point of $f$ if $\nabla f(a)=0$ , and a critical point $a$ is said to be non-degenerate if $\det H_f(a) \neq0$ . Here $H_f$ denotes the Hessian matrix of $f$ . A few observations: $\overline{\Omega}$ and $\partial \Omega$ are both compact. Hence $f$ obtains its extreme value in $\overline{\Omega}$ , and the restriction of $f$ i.e. $f|_{\partial\Omega}$ also obtains its extreme value. And, take maximum value for example, $\max_{x\in \partial\Omega} f(x) \leq \max_{x\in \overline{\Omega}} f(x)$ . Need help.","['multivariable-calculus', 'derivatives', 'stationary-point']"
3880070,Variations on Gauss' integral trick,"This question is inspired by these two: Non-trivial values of error function erf(x)? Where is the mass of a hypercube? Upon reading these two, I realized there might be a geometric way to compute the error function, by a variation on Gauss' famous integral trick for evaluating $\int_{-\infty}^{\infty} e^{-x^2} dx$ .  Here's the idea: For convenience, define $E(t) = \int_0^t e^{-y^2} dy$ .  This is a rescaled $\text{erf}$ , which is helpful to eliminate some obnoxious constants in what follows.  We have $$E(t)^n = \left(\int_0^t e^{-y^2} dy\right)^n = \left(t\int_0^1 e^{-t^2 y^2} dy\right)^n = t^n \int_{[0,1]^n} e^{-t^2 ||\vec{y}||^2} d^n y$$ where the last expression is an integral over the n-dimensional unit hypercube.  As with the usual Gauss trick, we will change variables to spherical coordinates, so the last integral becomes $$t^n \int_0^{\sqrt{n}} e^{-t^2 r^2} A_n(r) dr$$ where $A_n(r)$ is the measure of a sphere of radius $r$ intersected with the unit hypercube, i.e. $A_n(r) := \left| S^{n-1} \cap [0,1]^n\right|$ .  So we have this interesting expression for the (rescaled) error function, $$\boxed{E(t) = t\left(\int_0^{\sqrt{n}} e^{-t^2 r^2} A_n(r) dr\right)^{1/n}}\tag{1}$$ Now here's where the second link above comes in: The radial mass distribution of an n-cube is approximately Gaussian, due to a miracle of the central limit theorem.  In particular, $$A_n \xrightarrow{d} \sqrt{\frac{2\sqrt{\mu}}{\pi \sigma^2}} \exp\left( -\frac{\left(r-\sqrt{n\mu}\right)^2}{\sigma^2/2\sqrt{\mu}} \right)\tag{2}$$ where $\mu=1/3$ and $\sigma = 4/45$ (these are the mean and variance of $X^2$ , where $X$ is uniform on $[0,1]$ , as explained in the link), and where $\xrightarrow{d}$ denotes weak convergence.  If you naively just plug in this expression in place of $A_n$ above, you can evaluate the resulting integral in terms of $E(x)$ and elementary functions.  In the limit $n\rightarrow \infty$ the right hand side of (1) would then simplify down to something like $t\exp\left(\frac{-\mu t^2}{1+t^2\sigma^2/2\mu}\right)$ , which is of course nonsense. The problem is of course that weak convergence is not good enough to evaluate this integral.  (By the Berry-Esseen theorem, we expect the deviation of $A_n$ from a Gaussian to be $\mathcal{O}(n^{-1/2})$ uniformly across its domain, whereas the integral coming from (2) vanishes exponentially with $n$ , so it gets swamped by the error terms as $n\rightarrow \infty$ .)  So the next thing I tried was an Edgeworth expansion .  This allows us to add corrections to the central limit theorem, and also to control the error in the convergence.  Unfortunately, as with the naive version above, the error terms in an Edgeworth expansion are always polynomial, whereas the integrals vanish exponentially. So this is also insufficient. So after realizing this, I decided to back up and try something different.  Both the central limit theorem and Edgeworth expansions ultimately come from manipulating the characteristic function (Fourier transform of the distribution function), so perhaps analyzing (1) in Fourier space is the way to go?  Note that (1) has the form of an inner product between $e^{-t^2 r^2}$ and the radial mass distribution of a cube $A_n(r)$ .  So by Parseval, we can evaluate it as an inner product of their Fourier transforms. Some observations in this direction: Let $V_n(r) := \left| B_r \cap [0,1]^n\right|$ be the volume of the intersection of a ball $B_r$ of radius $r$ centered at the origin and the unit hypercube.  Then $V_n(r)$ is the (cumulative) distribution function corresponding to the density $A_n(r)$ , and $A_n(r) = \frac{d}{dr} V_n(r)$ . $V_n(r)$ has the probabilistic interpretation $\mathbb{P}\left(\sum_{i=1}^n X_i^2 \leq r^2\right)$ , where $X_i$ are i.i.d. uniform on $[0,1]$ . $X_i^2$ as above has density $\rho(x) = \frac{1}{2\sqrt{x}}$ on the unit interval and $\rho(x) = 0$ elsewhere. $V_n(\sqrt{s}) = \mathbb{P}\left(\sum_{i=1}^n X_i^2 \leq s\right)$ , which is the distribution function of $\sum_{i=1}^n X_i^2$ .  The density for this random variable is $\rho^{*n}$ , the convolution of $\rho$ with itself $n$ times.  Hence $\rho^{*n}(s) = \frac{d}{ds} V_n(\sqrt{s}) = A_n(\sqrt{s}) \frac{1}{2\sqrt{s}}$ . Putting this all together, the original integral of (1) can be written $$
\begin{align*}
\int_0^{\sqrt{n}} e^{-t^2 r^2} A_n(r) dr & = \int_0^{n^{1/4}} e^{-t^2 s} A_n(\sqrt{s}) \frac{1}{2\sqrt{s}} ds \\
& = \int_0^{n^{1/4}} e^{-t^2 s} \rho^{*n}(s) ds \\
& = \langle e^{-t^2 s} , \rho^{*n}(s)\rangle_{L^2} \\
& = \langle \mathcal{F}[e^{-t^2 s}\Theta(s)] , \mathcal{F}[\rho(s)]^n\rangle_{L^2} \\
\end{align*}
$$ where $\Theta(s)$ is a Heaviside step function. The Fourier transform of $e^{-t^2 s}\Theta(s)$ is $\int_0^{\infty} e^{-t^2 s - 2\pi i k s} ds = \frac{-i}{2\pi}\frac{1}{k-it^2/2\pi},$ and the Fourier transform of $\rho(s)$ is $$\int_0^1 e^{-2\pi i k s} \frac{1}{2\sqrt{s}} ds = \int_0^1 e^{-2\pi i k y^2}dy = \frac{E(\sqrt{2\pi ik})}{\sqrt{2\pi ik}}$$ by analytic continuation, if you like.  So by Parseval we have $$
\int_0^{\sqrt{n}} e^{-t^2 r^2} A_n(r) dr = \int_{-\infty}^{\infty} \frac{i}{2\pi} \frac{1}{k-\frac{-it^2}{2\pi}} \left(\frac{E(\sqrt{2\pi ik})}{\sqrt{2\pi ik}}\right)^n
$$ Closing the contour in the lower half plane, we have a pole at $k=-it^2/2\pi$ , and so we can evaluate the integral and find $$
E(t) = t\left( -2\pi i \frac{i}{2\pi} \left(\frac{E(t)}{t}\right)^n \right)^{1/n} = E(t)
$$ So it's trivial. So in the end, it seems I just found a very complicated way of proving that $\text{erf}(t)=\text{erf}(t)$ .  This is kind of disappointing, given that we seemed to have used several non-trivial facts in the process.  My questions are: I want to throw this idea to the wind and see if anyone else can find a better use for it. Is there any way to patch up the above analysis so as to learn something non-trivial about e.g. the error function or the mass distribution of a hypercube? If not, is there an intuitive way of seeing why the above analysis is bound to give a trivial answer? (Also, in case it's not clear, what I might have hoped for in the above analysis was e.g. a non-trivial relationship between $E(t)$ and $E$ evaluated at some other value.  So a functional equation or something.)","['central-limit-theorem', 'fourier-analysis', 'geometry', 'error-function', 'probability-theory']"
3880138,De Rham Cohomology of Smooth $G$-manifolds,"Let $M$ be a smooth manifold and let $G$ be a compact, connected Lie Group that acts on $M$ by smoothly. Now suppose that the action of $G$ on $M$ is transitive. For some $m$ in $M$ , let $K$ be the stabiliser of $m$ . It should be clear that $K$ is a closed subgroup of $G$ , so we can associate $G/K$ with $M$ . Let $\mathfrak{g}$ denote Lie Algebra of $G$ and $\mathfrak{r}$ the Lie Algebra of $K$ . Because of compactness of $G$ , it is known that there exists some $Ad(G)$ -invariant and $ad(LG)$ invariant inner-product, say $\langle \ ,\  \rangle$ on $\mathfrak{g}$ , so as a $K$ - module, $\mathfrak{g} = \mathfrak{r} \oplus \mathfrak n $ where $\mathfrak{r}$ and $\mathfrak{n}$ orthogonal with respect to $\langle \ ,\  \rangle$ , and we regard $\mathfrak{n}$ as tangent space $T_m M$ . My question is towards calculating the cohomology of $M$ using exterior power of $\mathfrak{n}$ . Let $\omega$ be a $G$ - invariant differential $p$ - form on $M$ . So evaluating $\omega$ at point $m$ gives us an alternating, multi-linear function $\omega_m: \mathfrak{n}^p \rightarrow \mathbb{R}$ . It is mentioned that because $\omega$ is $G$ - invariant hence $\omega_m$ is $Ad(K)$ invariant, which I presume, means that for any $g \in K$ one has for left invariant vector fields $X_1,\dots, X_p$ : $$\omega_m(Ad(g^{-1})X_1(e),\dots, Ad(g^{-1})X_p(e)) = \omega_m(X_1(e),\dots, X_p(e))$$ I am having some trouble with the computation to convince myself to that this is indeed the case. What I have gotten so far is if we let $L_g$ denote left multiplication by $g$ and $R_g$ denotte multiplication of $g$ and $L{_g} _*$ to denote the pushforward maps, etc. then \begin{align*}
\omega_m(Ad(g^{-1})X_1(e),\dots, Ad(g^{-1})X_p(e)) &= \omega_m(L_{g^-1}{_*}R_g{_*}X_1(e),\dots,L_{g^-1}{_*}R_g{_*}X_p(e))\\
&=\omega_{g^{-1}m} (R_g{_*}X_1(e),\dots, R_g{_*}X_p(e))
\end{align*} And I am unable to proceed. Same for the converse case as well i.e. obtaining a $G$ -invariant differential $p$ form from a $Ad(K)$ invariant alternating, multi-linear function $\omega_m: \mathfrak{n}^p \rightarrow \mathbb{R}$ . Do let me know if I have any misconception/misunderstanding. Thanks!","['lie-algebras', 'algebraic-topology', 'lie-groups', 'differential-forms', 'differential-geometry']"
3880173,Prove that. $G/Z(G)\cong S_3$,"$M=\left(\begin{array}{ll}0 & i \\ i & 0\end{array}\right), N=\left(\begin{array}{ll}\omega & 0 \\ 0 & \omega^{2}\end{array}\right)$ and $G=<M, N>$ be the group generated by M,N
Here $\omega$ is cube root of unity Prove that. $G/Z(G)\cong S_3$ What i tried I look for relations that this generated group has I find. $M^4= I $ identity matrix , $ N^3=I$ , and. $ MN\neq NM $ $\implies  G$ is non -abelian $ \implies Z(G)= \{I\} $ Work reduced to show $G\cong S_3$ But this is not possible as
G has element  ,M of order 4
But $S_3$ has only elements of order 1,2,3 Is  i am doing it correctly ?? If   i right please  tell me the group to which G isomorphic","['group-theory', 'abstract-algebra', 'finite-groups']"
3880175,"Trigonometric functions -- sin - cos - tan - Differences, actual applications,","Beforehand, I want to point out that I'm studying some basic geometric function and I'm pretty behind with this topic, hence may write something not too clever or my question might be silly; I ask this because I don't know. I did some research regarding this topic however did not find anything, as I'm the only one asking such a question. I tried to read Trigonometric functions in Wiki but the issue with Wikipedia is that you need to know the topic, hence the language and expression, otherwise is really not understandable (explains something you don't know with something you don't know). Problem I'm came this 3 basic functions, sin, cos and tan, I do understand how to calculate them, which are pretty straight forward: $$\sin(x) = \frac{\text{opposite}}{\text{hypotenuse}}$$ $$\cos(x) = \frac{\text{adjacent}}{\text{hypotenuse}}$$ $$\tan(x) = \frac{\text{opposite}}{\text{adjacent}}$$ However what really bugs me if what is the actual difference of the 3 of them?
Obviously, there are 3 different results, hence 3 different 'sizes' (maybe is a ratio?) of the target inside angle. My expectations where that the result is the same one, and the 3 functions serve to get the vertices angle degree depending on the given value, but in fact, are completely different. Questions What is the actual difference between each of them? Why would someone want to calculate the sin rather than cos or tan, or cos rather than sin etc...? Can someone give some application or actual usage in a real problem of each one of them, and why you would've chosen one or the other? Any other insight is highly appreciated. Thanks","['trigonometry', 'geometry']"
3880196,Find $\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right )$ without using the chain rule. [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $$\frac{\mathrm{d} }{\mathrm{d} x}x\sin \left ( \sqrt{3x^{2}+5} \right )$$ I can't for the life of me differentiate this function while only using Trig Identities, Basic differentiation rules, and Limits (no L'Hopital, either.)","['calculus', 'derivatives']"
3880232,How analytic continuation allows for proof of these 2 theorems in theory of Partitions,"Consider these 2 theorems in textbook apsotol introduction to analytic number theory. 1st is generating functions for partitions I have self studied text and need help in verifying the argument of use of analytic continuation in  last image( last line of proof) : Is it due to reason that we can differentiate the formula derived infinitely many times for all complex numbers in disk |x|<1? Similarly, in case of proof of Euler Pentagonal Theorem here : See 2nd line of First image : Author says about analytic continuation. My understanding is that is it due to the fact that infinitely times differentiable in |x|<1 . Am i right or wrong. Do I need to add something else. I ask my questions here because there is no one to whom I can ask as it is not taught in my university. Kindly shed some light on this.","['integer-partitions', 'analytic-continuation', 'number-theory', 'complex-analysis', 'analytic-number-theory']"
3880233,Prove that $\sinh{2u}+2\sinh{4u}+3\sinh{6u}+...+n\sinh{2nu}=\frac{n\sinh{(2n+2)u-(n+1)\sinh{2nu}}}{4\sinh^2{u}}$,"Prove that $$\sinh{2u}+2\sinh{4u}+3\sinh{6u}+...+n\sinh{2nu}=\frac{n\sinh{(2n+2)u-(n+1)\sinh{2nu}}}{4\sinh^2{u}}$$ My attempt at a solution: Let $$S=\sum_{r=1}^{n}\cosh{2ru}$$ then $$\frac{dS}{du}=\sum_{r=1}^{n}2r\sinh{2ru}\Rightarrow\sum_{r=1}^{n}{r\sinh{2ru}}=\frac{1}{2}\frac{dS}{du}$$ To evaluate $S$ , I used $\cosh{2ru}=\frac{1}{2}{(e^{2ru}+e^{-2ru})}$ , from which $$S=\frac{1}{2}\left\lbrace\sum_{r=1}^{n}e^{2ru}+\sum_{r=1}^n{e^{-2ru}}\right\rbrace
=\frac{1}{2}\left\lbrace\frac{e^{2u}((e^{2u})^n-1)}{e^{2u}-1}+\frac{e^{-2u}(1-(e^{-2u})^n)}{1-e^{-2u}}\right\rbrace,$$ using the formula for the sum of the first $n$ terms of a geometric progression. After some algebra and cleaning up, I managed to obtain $$S=\frac{\sinh(2n+1)u}{2\sinh{u}}-\frac{1}{2}$$ and so $$\frac{dS}{du}=\frac{1}{2}\left[\frac{(\sinh{u})(2n+1)\cosh{(2n+1)u}-(\sinh{(2n+1)u})\cosh{u}}{\sinh^2{u}}\right]$$ but I struggle to spot the relevant hyperbolic identities (if needed) in order to proceed to the given result. Just curious, but is there an alternative method to reach the desired result?","['hyperbolic-functions', 'calculus', 'summation', 'sequences-and-series']"
3880289,Question regarding a series that contains logarithms,"$$\sum_{n=1}^\infty\bigl(3\log(n^2+1)-2\log(n^3+1)\bigr)$$ I tried the limit comparison test with $\sum_{n=1}^\infty\frac1{n^2}$ and then applied L'Hopital's Rule. What I ended up was a limit which was a real number and as the series of $\frac1{n^2}$ converges, my original series converges as well. However, I was wondering if there is another way of solving the problem, one that involves fewer computations.","['logarithms', 'sequences-and-series']"
3880297,Proving that every positive integer is of the form $x^2+y^2-5z^2$,"Prove that every positive integer can be written as $$x^2+y^2-5z^2$$ with $x$ , $y$ and $z$ are non-zero integers. I made the following observations if a number is congruent to 0,1,2 mod 4 than it can easily be expressed in this by taking z to be zero , as for the case when z is non zero I am not sure. if the number is congruent to 3 mod 4 than (x,y )have to even and z has to be odd all other cases dont work,  the opposite is true if the number is congruent 2 mod 4 to generalize for all types of integers mod 4 the parity of the numbers(x,y,z) that will satisfy are given below(I will denote even as 0 and odd as 1) and (x,y) can obviously be interchanged, therefore -if 0 mod 4 then (0,0,0) and (0,1,1)
-if 1 mod 4 then (0,1,0) and (1,1,1)
-if 2 mod 4 then (1,1,0)
-if 3 mod 4 then (0,0,1) so I tried to write a number congruent to 3 mod 4 as follows $$x^2+y^2-5z^2$$ = $$(2a)^2+(2b)^2-5(2c+1)^2$$ $$4(a^2+b^2-5c^2-5c-1)^2-1$$","['number-theory', 'diophantine-equations']"
3880306,Find the cosets of $D_{2n}$ in $S_n$,"Find the cosets of $D_{2n}$ in $S_n$ I tried to take examples and see, I did it in the case when $n=4$ , but for the case $n=6$ , the computation is became huge. Is there a simple way in which I can see the problem?","['dihedral-groups', 'quotient-group', 'abstract-algebra', 'symmetric-groups', 'group-theory']"
3880315,Quadrilateral with given angles,"We are looking for angles x and y. I have found the values of the following angles:
BEA = 74,
BDA = 64,
ACD = 68,
ECD = 112, plus the relationship $x+y = 68$ . All other angles equations, from triangles or the sum of angles in the quadrilateral (360) end up in the same equation! I have found through Geogebra that $x=18$ and $y=50$ but I can't figure out a second relationship to determine them geometrically! Does anyone have any ideas? Thank you!",['geometry']
3880327,2 identities of Shanks from Apostol's Book,"I am trying exercises of Ch-14 partitions from Tom Apostol Introduction to ANT and unable to Solve (a) part of Question 5. 5. If $x\ne 1$ let $Q_0(x)=1$ and for $n\ge 1$ define $$ Q_n(x) = \prod_{r=1}^n \frac{1-x^{2r}}{1-x^{2r-1}}. $$ (a) Derive the following finite identities of Shanks: $$ \sum_{m=1}^{2n} x^{m(m-1)/2} = \sum_{s=0}^{n-1} \frac
   {Q_n(x)}{Q_s(x)} x^{s(2n+1)}, $$ $$ \sum_{m=1}^{2n+1} x^{m(m-1)/2} = \sum_{s=0}^{n} \frac
   {Q_n(x)}{Q_s(x)} x^{s(2n+1)}. $$ (b) Use Shank's identities to deduce Gauss' triangular-number
theorem: $$ \sum_{m=1}^\infty x^{m(m-1)/2} = \prod_{n=1}^\infty
   \frac{1-x^{2n}}{1-x^{2n-1}} \text{ for } |x|<1. $$ I have question in proof of (a) as I am unable to derive any of the identity. I tried by evaluating $Q_n(x) / Q_s(x) $ and if I multiply it by $ x^{s(2n+1) }$ I don't see anything to get LHS in (I). So, kindly shed some light on how to prove it by proving any of the identity. I shall be really thankful for your help. Original image: https://i.sstatic.net/kUJbT.jpg","['analytic-number-theory', 'number-theory', 'integer-partitions', 'problem-solving']"
3880341,"Among any $9$ real numbers, two satisfy $0 < \frac{x-y}{1+xy} < \sqrt{2} - 1$ [duplicate]","This question already has answers here : For any given set of 13 distinct real numbers, prove we can always find two numbers $x$ and $y$ that $0<\frac{x-y}{1+xy}\leq 2-\sqrt{3}$. (2 answers) Closed 3 years ago . Prove that among any nine given real numbers there are two numbers $a, b$ with the property: $$0 < \dfrac{x-y}{1+xy} < \sqrt{2} - 1$$ The fraction looks like $\tan(a-b)$ , but I don't see if it can help. I don't have any other idea, I'm stuck. I also tried by reducing to absurd, but I didn't reach far. Could someone give me a clue?","['pigeonhole-principle', 'inequality', 'combinatorics']"
3880359,Limit of $\min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\}$ equals $e$,"Given $a_n = \min \{a\in \mathbb{N} : \sum_{i=1}^{a}\frac1{i}\geq n\}$ $\forall n \in \mathbb{N}$ , I want to prove that: $$\lim_{n\to\infty}\frac{a_{n+1}}{a_n}=e$$ For that purpose, I am trying to prove that $\log a_{n+1}-\log a_n \to 1$ . From the definition of $a_n$ , we have that $1+\frac12+\ldots+\frac1{a_n}\geq n$ , and we have: $$n\leq 1+\frac12+\ldots+\frac1{a_n}\leq \int_1^{a_n+1}\frac1{x}dx = \log(a_n+1)$$ In the same way, for $a_{n+1}$ we have $n+1\leq \log(a_{n+1}+1)$ . Combining both inequalities we have: $$\log (a_{n+1}+1)-\log (a_n+1) \geq 1$$ However, I don't know how to get the other inequality. Can somebody help me? Many thanks in advance!","['eulers-number-e', 'limits', 'logarithms', 'real-analysis']"
3880366,minimum value of $\frac{xy}{z}+\frac{yz}{x}+\frac{xz}{y}$ if $x^2+y^2+z^2=1$,"If $x^2+y^2+z^2=1$ what is the minimum value of $\frac{xy}{z}+\frac{yz}{x}+\frac{xz}{y}$ for $x,y,z \gt 0$ ? I would like to know if the minimum could be found using simpler ways.(like $AM \ge GM \ge HM$ ). knowing $xy+yz+xz \geq \frac{-1}{2}$ for real $x,y,z$ might help.","['maxima-minima', 'algebra-precalculus']"
3880367,Meaning of Janossy densities,"I'm studying the theory of finite point processes on ""An Introduction to the Theory of Point Processes: Volume I: Elementary Theory and Methods, Second Edition"" by Daley and Vere-Jones.
I have some difficulties in understanding an elementary concept.
I will use a different notation with respect the book. Background From my simple point of view, a finite point process $\mathsf{X}$ is a finite set of points $X_1$ , $\dots$ , $X_\eta$ in $\mathbb{R}^n$ , where the cardinality $\eta$ of the set $\mathsf{X}$ is random and the points $X_1$ , $\dots$ , $X_\eta$ contained in the set $\mathsf{X}$ are random as well. \begin{equation}\mathsf{X}=\{X_1,\dots, X_\eta\}\end{equation} so, this means that a finite point process is characterized by a discrete density $p_N(\eta)$ on $\mathbb{N}$ , which describes the cardinality of $\mathsf{X}$ , and a family of joint PDFs on $\mathbb{R}^{n\times\eta}$ of the form \begin{equation}\mathcal{S}=\{p_{X_1,\dots,X_\eta}(x_1,\dots,x_\eta)\}_{\eta\in \mathbb{N}^+}\end{equation} which describes how the points in $\mathsf{X}$ are distributed in $\mathbb{R}^n$ . Since $\mathsf{X}$ is a set, it is invariant with respect any permutations of the points $X_1$ , $\dots$ , $X_\eta$ , for example the notations $\{X_2,X_3,X_1\}$ and $\{X_3,X_2,X_1\}$ represents the same finite point process $\{X_1,X_2,X_3\}$ . This means that the joint PDFs in $\mathcal{S}$ need to be symmetric , i.e. permutation invariant, i.e. \begin{equation}p_{X_1,\dots, X_2}(x_{\sigma(1)},\dots, x_{\sigma(\eta)})=p_{X_1,\dots,X_\eta}(x_1,\dots,x_\eta)\end{equation} for every $\eta!$ possible permutations $\sigma(i)$ of the indexes $i$ . Now, define the Janossy density of order $\eta$ as \begin{equation}p_\mathsf{X}(\{x_1,\dots,x_\eta\})\triangleq \begin{cases}
 \eta! \, p_{X_1,\dots,X_\eta}(x_1,\dots, x_\eta)\,p_N(\eta) & \text{ if } \eta>0 \\
p_N(0) & \text{ if } \eta=0
\end{cases}\end{equation} where the Janossy of order zero is denoted as $p_\mathsf{X}(\varnothing)$ . My problem At page 125 the book says that the quantity \begin{equation}p_\mathsf{X}(\{x_1,\dots,x_\eta\})\text{ d}x_1\cdots\text{d}x_\eta\end{equation} is the probability of the event \begin{equation}\begin{aligned}
&\text{there are exactly } \eta \text{ points in the process,} \\ 
&\text{one in each of the } \eta \text{ distinct infinitesimal regions } (x_i, x_i+\text{d}x_i)
\end{aligned}\tag{E1}\end{equation} I don't get it. This is my reasoning: the quantity \begin{equation}p_{X_1,\dots,X_\eta}(x_1,\dots, x_\eta)\text{ d}x_1\cdots\text{d}x_\eta\end{equation} is the probability of the event \begin{equation}\begin{aligned}&X_1\in (x_1,x_1+\text{d}x_1)\text{ and } \\&X_2\in (x_2,x_2+\text{d}x_2) \text{ and }\\
&\vdots\\
&\text{ and }X_\eta\in (x_\eta,x_\eta+\text{d}x_\eta)\end{aligned}\end{equation} the quantity \begin{equation}\eta!\,p_{X_1,\dots,X_\eta}(x_1,\dots, x_\eta)\text{ d}x_1\cdots\text{d}x_\eta\end{equation} is the probability of the event \begin{equation}\tag{E2}\bigcup_{\sigma}\begin{cases}&X_1\in (x_{\sigma(1)},x_{\sigma(1)}+\text{d}x_{\sigma(1)})\text{ and } \\
&X_2\in (x_{\sigma(2)},x_{\sigma(2)}+\text{d}x_{\sigma(2)}) \text{ and }\\
&\vdots\\
&\text{ and }X_\eta\in (x_{\sigma(\eta)},x_{\sigma(\eta)}+\text{d}x_{\sigma(\eta)})\end{cases}\end{equation} where the union is performed over all the $\eta!$ possible permutations $\sigma(i)$ . For me the events $(\text{E1})$ and $(\text{E2})$ are the same. I cannot see the difference.","['point-processes', 'real-analysis', 'combinatorics', 'probability-theory', 'random-variables']"
3880412,"Classifying the representations of $G=\langle(123), (456), (23)(56) \rangle \subset S_6$.","This is exercise 3 of chapter 2 of the book ""Representations of finite and lie groups"" By B. Thomas. My questions are in bold. I am essentially confused by two steps but I think I understand the entire question apart from that. Let $G$ be a subgroup of order $18$ in the symmetric group $S_6$ given by $$ G = \langle(123), (456), (23)(56)\rangle.$$ Show that $G$ has a normal subgroup of order $9$ and four normal subgroups
of order $3$ . By considering quotients show that G has two representations of degree one and four inequivalent irreducible representations of degree $2$ , none of which is injective/faithful. My approach is as follows. Observe $|G|=18=2 \cdot 3^2$ .  Denote $n_3$ for the number of Sylow $3$ -subgroups. We see that by Sylow's theorem $n_3 \equiv 1 \bmod 3$ , so candidates are $n_3=1, 4, 7$ etc. However we also have that $n_3|2$ , so $n_3=1.$ This means there is one sylow subgroup of order $9$ , and another consequence of the Sylow theorems is that this group is then normal. We can construct such a group by considering the two generators of order $3$ : $$ H= \langle(123), (456) \rangle  \cong C_3 \times C_3.$$ Now this group has index $|G:H|= \frac{|G|}{|H|}=18/9=2$ and therefore it must indeed be normal in $G$ . We can also use the generators (and one of its inverses) of order $3$ to construct $4$ cyclic subgroups of order $3$ . $$ N_1=\langle (1 2 3)\rangle=\{e, (123), (132)\} \cong C_3$$ $$ N_2=\langle (4 5 6)\rangle=\{e, (456), (465)\}  \cong C_3$$ $$ N_3=\langle (1 2 3)(4 5 6)\rangle=\{e, (1 2 3)(4 5 6), (1 3 2)(4 6 5)\} \cong C_3$$ $$ N_4=\langle (1 2 3)(4 6 5)\rangle=\{e, (1 2 3)(4 6 5), (1 3 2)(4 5 6 )\}  \cong C_3$$ We have thus found four subgroups of order $3$ , but we still have to prove they are normal in $G$ . (How do I know these exhaust all the subgroups of $G$ one can have of order $3$ ? I do not know how to formulate this/the exact reason). I also thought earlier I knew why these were normal, but I was mistaken. We can now use the fact that we may lift representations of quotients to the original group using the canonical quotient homomorphism. $|G/H|=2$ and it is an abelian group (specifcally $C_2$ ) therefore by a corollary/consequence of Weddderburn's theorem we must have $2$ irreducible complex representations of $G/H$ , which are both $1$ dimensional (I prefer to use Dummit and Foote corollary 11(1), page 861 for this). We lift these to $G$ . Now we can consider the quotients $$G/N_1, G/N_2, G/N_3, G/N_4 $$ According to the hints and solutions section in the back, these each give a $2$ -dimensionial representation, why? Once we accept this we can also lift these $4$ representations to $G$ to end up with $4$ (distinct/inequivalent) representations of $G$ . The dimensions of the representations of $G$ we have so far are thus $1, 1, 2, 2, 2, 2$ . Then observe we must also have by theorem $10.(4)$ of page 861 of Dummit and Foote that: $$ \sum_{i=1}^r n_i ^2 = |G|$$ However if we compare this to what we already have, we find: $$ 2 \cdot 1^2 + 4 \cdot 2^2 =18 =|G|$$ So we indeed find that these are all representations by exhaustion. A lift of a quotient $G/N$ to $G$ always has $N$ contained in its kernel. We therefore know that lifted representations never have a trivial kernel and therefore they are not faithful (since they are not injective homomorphisms).","['symmetric-groups', 'group-theory', 'abstract-algebra', 'representation-theory']"
3880485,Differential equation with integrating factor,Hey I am supposed to solve the following differential equation: $(1-x^{2}y)dx+x^{2}(y-x)dy=0$ I found integrating factor: $\varphi (x)=-\frac{2}{x}$ So I multiply my original equation and I got: $\left ( \frac{1}{x^{2}} -y\right )dx+\left ( y-x \right )dy=0$ But then I try to integrate it and I got stuck. The answer should be: $y^{2}-2xy-\frac{2}{x}=C$ Can somebody help me? Thanks,['ordinary-differential-equations']
3880605,$v$-ideal (or divisorial ideal) which is not invertible,"An ideal $I$ of an integral domain $D$ is said to be invertible if there exists a fractional ideal $J$ such that $IJ=D$ . In this case, $J$ is unique, and can be denoted by $I^{-1}$ . It can also be proved that $I^{-1}=\{x\in K\ |\ xI\subseteq D\}$ where $K$ is the field of fraction of $D$ . Denote $I_v=(I^{-1})^{-1}$ . An ideal $I$ is called a $v$ -ideal or a divisorial ideal if $I_v=I$ . Of course, an invertible ideal is a $v$ -ideal, so a $v$ -ideal can be seen as a generalization of an invertible ideal. A well-known example of a non-invertible ideal is $\langle x,2\rangle$ as an ideal of the integral domain $\mathbb Z[x]$ . It can also be proved that it is not a $v$ -ideal either. My question is: is there any basic example of a $v$ -ideal which is not invertible? Thanks in advance!","['examples-counterexamples', 'ring-theory', 'abstract-algebra', 'ideals', 'commutative-algebra']"
3880664,A geometry problem about colinearity,"$\textbf{Problem:}$ Let $ABC$ be a triangle with circumcircle $\omega$ . Point $D$ lies on the arc $BC$ not containing $A$ of $\omega$ and is different than $B,C$ and the midpoint of arc $BC$ . Tangent of $\omega$ on $D$ intersects lines $BC$ , $CA$ , $AB$ at $A'$ , $B'$ , $C'$ , respectively. Lines $BB'$ and $CC'$ intersect at $E$ . Line $AA'$ intersects again the circle $\omega$ at $F$ . Prove that points $D,E,F$ are collinear. I tried to use menelaus theorem on bunch of triangles and in few ways I restated the problem to apply it.But all those attempts failed.I also tried to chase cross ratios but that didn't work out either. Any help or solution will be appreciated. Thanks @oldboy for the diagram.","['contest-math', 'euclidean-geometry', 'projective-geometry', 'geometry', 'triangles']"
3880788,A variation of a 'leaky bucket' problem,"I'm looking at a 'leaky bucket' problem with a few differences, the system is set up as follows: the tank has a practically unlimited capacity water is added with a known flow rate periodically the drain valve is adjusted so that all of the water is drained over say 10 seconds we are interested to know how much water is contained at time $t$ To give a concrete example, lets assume the water is added with a rate of 10L per second, every 4 seconds the drain is adjusted. When the drain is adjusted it is adjusted to $\frac{1}{10}$ of current volume of water. Initially I started with the following diff. eq. $dV=I-\frac{1}{10}V$ where $V$ stands for volume and $I$ is the incoming flow rate in L per sec. The issue is that in this equation the drain is constant with respect to volume, whereas in the actual problem the drain is adjusted periodically and in between those events the water is draining with a set flow rate. I have a feeling it involves using a $\sin$ function to model the period, but I'm not certain as to how to go about it.",['ordinary-differential-equations']
3880832,Evaluate $\int\sin^m(x)\cdot \cos^n(x) \;dx$. Can I solve this way?,"Evaluate $\int\sin^m(x)\cdot \cos^n(x)dx.$ I am doing some pre-reading for lecture and I have seen that my lecturer has used a long method. I have used a different method. $=\int\sin^m(x)\cdot \cos^n(x) \;dx \\=\int\sin^m(x)\cdot \cos^{2k+1}(x) \;dx\\=\int\sin^m(x)\cdot (1-\sin^2x)^k \cdot \cos(x) \;dx\\=\int(\sin^m(x)-\sin^{2m}(x))\cos(x) \;dx$ Let $u=\sin(x)$ Then, $du=\cos(x)\;dx$ $\int(u^m-u^{2m}\,du)=\frac{\sin^{m+1}(x)}{m+1}-\frac{\sin^{2m+1}(x)}{2m+1}+c.$ Could you check if this method is correct?","['integration', 'trigonometry', 'solution-verification', 'trigonometric-integrals']"
3880863,"If $f_n$ is uniformly integrable, then $\lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0$ for all $n$","Show that if $\{f_n\}_{n\geq 1}$ is a uniformly integrable family of functions, then $\lim_{t\to\infty}\int|f_n|I_{|f_n|>t} = 0$ for all $n$ . I feel like this should fall right out of the definition of uniform integrability, but it's not coming to me for some reason. The definition of uniform integrability I'm working with is; $\{f_n\}_{n\geq1}$ is uniformly integrable if for all $\epsilon>0$ there is a $\delta>0$ such that if $\mu(A)<\delta$ then $\int_A|f_n|<\epsilon$ , for all $f_n$ . It's not clear to me why I should be able to get $\mu\left(\{x:|f_n(x)|>t\} \right)<\delta$ for every $n$ . Any thoughts are appreciated. Thanks in advance.","['integration', 'uniform-integrability', 'measure-theory']"
3880918,Calculating average number of tries for an event whose probability increases with the number of unsuccesful tries,"I have a question that I didn't even know how to word on google.
I have a system where an event has a 2% chance of happening each roll for the first 50 rolls, and, starting from the 51st, its probability increases by 2% each unsuccessful try, so in the 51st try it has a 4% chance, in the 52nd a 6% chance and so on until the 99th in which it has 100% chance, however the chance resets to 2% if the event is rolled.
I have calculated the chance of rolling the event at least once within x number of pulls, for example for 52 pulls it is $1-( (.98^{50})(.96)(.94) )=.6714$ ,please do correct me if I am wrong. However I also wanted to know on average how many attempts it would take for the event to occur: I tried reasoning that for an event with a 2% chance it would take on average 50 tries, but that would mean that the increase in probability from the 51st have no weight on this average, and that seemed weird.
In addition, in some instances even if you roll the event there is a p chance that the roll still fails; p is determined before starting the 1st roll and doesn't change throughout the attempts. As you can probably imagine I'm not experienced at all in the field (or mathematics in general for the matter) so if it is possible to explain reasonings and calculations in a simple manner I would appreciate. Thank you in advance.","['expected-value', 'statistics', 'probability']"
3880939,Why trace is natural than (preferred to) determinant for smooth map $f:M\to N$?,"For a continuous map $f:(M,g)\to (N,h)$ , between Riemannian manifolds $(M,g)$ and $(N,h)$ we can pullback the $h$ by $f$ . Most of experts take trace from this new tensor and work with it. i.e. $\mathrm{tr_g}(f^*h)$ which I think is equals to $|df|^2$ . I think there is a simple reason from Linear Algebra that perhaps I missed it that Question: why they use trace (e.g. see this post ) and not determinant or any other operator? One primary reason is that it is similar to $\mathrm{tr }A^tB$ that is an inner product over $n\times n$ matrices. In the case of energy density of harmonic maps, $e(f):=\frac{1}{2}|df|^2$ is very natural operator because it is similar to (up to a constant $m$ ) the kinetic energy formula $E=\frac{1}{2}mv^2$ in physics. But these are not sufficient to not consider the determinant (or any other operator) case. I want to know Is the following expression meaningful and can it reveals nice properties of the space as well as trace case? or that is same as trace case? $$K(f):=\int_M\det_g(f^*h)dvol_g.$$ Update: It is also helpful remember that the trace is $\sum_i\lambda_i$ and determinant is $\prod_i\lambda_i$ .","['riemannian-geometry', 'linear-algebra', 'pullback', 'differential-topology', 'differential-geometry']"
