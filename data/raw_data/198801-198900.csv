question_id,title,body,tags
3855407,Salt in a container - ODE problem,"We work with a water container that can hold up to 1000 liters. The water container initially contains 500 liters of water, in which 20 kg of salt has been dissolved. There is agitation in the container, so the salt concentration is the same everywhere. Clean water flows into the container at a speed of 4L / min. At the same time, saltwater flows out of the container at a speed of 2L / min. This causes problems as the container is only at 1000L. After a while, an alarm sounds due to a filled container. (a) At what time does the alarm sound? The differential equation that describes this problem is given by $$y'=-\frac{2}{500+2t}\cdot y$$ It is stated that the salt content is 10kg when the alarm rings, i.e. $y(250)=10$ . Just as the alarm sounds, the inflow changes so that salt water now flows with 35g of salt per. liters in at the rate of 1L / min. There is still 2L flowing out per. minute. (b) How much salt is in the container when there is again 500L in the container? My answer: Well (a) was quite easy, I obtained that the alarm will sound after 250 minutes since $500+2\cdot 250=1000$ and then the alarm will ring. But the last part, i.e. (b) is not easy for me. I believe that I have to make an ODE from the information I just got, but here I tried many times. Notice that this problem came from a book where there was more question, I just picked the relevant part. (b) is also stated as ""voluntary"" in the book. Notice this is not a homework problem.",['ordinary-differential-equations']
3855415,Limit of regularized inverse matrix,"consider a (quadratic, but otherwise arbitrary) matrix $A$ . I am interested in the long-time behaviour of $$ r(t) = (I + t\cdot A)^{-1} r_0,$$ i.e. what is $\lim_{t\to\infty} r(t)$ ? An interesting arrangement (by inserting $(I+t\cdot A - t\cdot A)$ ) is the following: $$r(t) = r_0 - A(t^{-1}I + A)^{-1}r_0$$ The second part looks fairly similar to the limit definition of the Moore-Penrose pseudoinverse: Given a matrix $B$ , the M-P pseudoinverse is given by $B^+ = \lim_{t\to\infty} B^T(\frac1 t I + BB^T)$ . But as you can see, this doesn't exactly match this form (the problem being that A is not symmetric so it cannot be interpreted as a term of the form $BB^T$ ). My hypothesis is something along the lines of ""the components of $r$ in the kernel of $A$ stay constant, while the components in the orthogonal complement converge to 0"".","['matrices', 'pseudoinverse', 'linear-algebra', 'inverse']"
3855519,Why does the graph of $y^2=1-\frac{4x^{10^{12}}}{\pi^2}$ look so much like a square?,"I want to know why the equation $y^2=1-\dfrac{4x^{10^{12}}}{\pi^2}$ gives an approximate square. (See the figure below.) Background I was just playing around with functions and I wanted to see if $y=\left|\sin\bigg(\dfrac{\pi x}{2}\bigg)\right|$ (radians) would give a semicircle for the interval $[0,2]$ as the distance of $(1,0)$ is the same from $(0,0)$ , $(2,0)$ and $(1,1)$ , all of which will lie on the curve. The equation of a unit semicircle with its centre at $(1,0)$ is $y=\sqrt{2x-x^2}$ . I know that the curves of both the equations don't resemble each other much but I still thought of approximating the sine function using this because I thought that it could still be combined with another approximation to make a better approximation. Anyway, I did it and for $\phi=x~\mathrm{radians}$ , the value of $\sin\phi$ can to be approximately $\dfrac2\pi\sqrt{\pi x-x^2}$ . It looked like a semi-ellipse and so I verified it to find that it was a semi-ellipse. I thought of using this to derive the equation for an ellipse with it's centre at the origin and the value of $a$ and $b$ being $\dfrac\pi2$ and $1$ respectively. The equation came out to be : $y^2 = 1 - \dfrac{4x^2}{\pi^2}$ Finally, I thought of playing with this equation and changed the exponent of $x$ . I observed that as I increased the power, keeping it even, the figure got closer and closer to a square. $y^2=1-\dfrac{4x^{10^{12}}}{\pi^2}$ gave a good approximation of a square. For the exponent of $x$ being some power of $10$ greater than $10^{12}$ , a part of the curve began to disappear. I want to know why this equation gives an approximate square. Note : I would like to inform you that I have no experience with conic sections. Thanks!","['analytic-geometry', 'geometry']"
3855562,Is this knotted graph knotted?,"Above, I've drawn a knotted 3-valent graph. I suspect that it's not isotopic to the ""unknotted"" version below, but I'm not sure. Is it? I know about fundamental groups of knot complements, but it seems like that's probably more work than necessary. The easiest way would be to show that it's the companion of some simple knot, but I haven't been able to get that to work. Is this knotted (by which I mean, is that not-equals sign in the drawing correct), and if so, is there a good way to prove that?","['graph-theory', 'knot-theory', 'general-topology', 'knot-invariants']"
3855565,Does $\sum_{n=1}^\infty \frac{1}{\operatorname{Ta}(n)}$ converge?,"I made this question up for fun, sorry if it's quite silly. The $n$ -th Taxicab number , typically denoted $\operatorname{Ta}(n)$ , is defined as the smallest integer that can be expressed as a sum of two positive integer cubes in $n$ distinct ways. The second Taxicab number, which is also the most famous one, is $1729$ . It can be represented as $10^3+9^3$ or $12^3+1^3$ . The Taxicab numbers grow really rapidly, for example $$\operatorname{Ta}(6) = 24153319581254312065344$$ This got me thinking about the following series and whether anything can be said about it's convergence $$\sum_{n=1}^\infty \frac{1}{\operatorname{Ta}(n)}$$ How would you prove that this converges? Could you use the fact that $\zeta(3)$ converges to prove the convergence? The normal Taxicab numbers grow so rapidly that the convergence of the series above is quite certain. However, if we change the definition by Taxicab number slightly by introducing the notation $\operatorname{Tas}(n)$ as being numbers which are sums of two cubes in two or more ways, the numbers grow much less rapidly with $$\operatorname{Tas}(6) = 39312$$ What can then be said about the converge of $$\sum_{n=1}^\infty \frac{1}{\operatorname{Tas}(n)}$$","['number-theory', 'sequences-and-series']"
3855567,About a chain rule for Wronskians,"The Wronskian of $(n-1)$ times differentiable functions $f_1, \ldots, f_n$ is defined as the determinant $$
W(f_1, \ldots, f_n)(x) = \begin{vmatrix}
f_1(x) & f_2(x) & \cdots & f_n(x) \\
f_1'(x) & f_2'(x) & \cdots & f_n'(x) \\
\vdots & \vdots & \ddots & \vdots \\
f_1^{(n-1)}(x) & f_2^{(n-1)}(x) & \cdots & f_n^{(n-1)}(x)
\end{vmatrix}
$$ and used e.g. in the context of linear differential equations. While working on Wronskian of functions $\sin(nx), n=1,2,...,k$. I “discovered” the following chain rule for Wronskians: Let $I, J \subset \Bbb R$ be intervals, $g:I \to J$ and $f_1, \ldots, f_n: J \to \Bbb R$ be $(n-1)$ times differentiable functions. Then $$
 W(f_1 \circ g, \ldots, f_n \circ g)(x) = W(f_1, \ldots f_n)(g(x)) \cdot (g'(x))^{n(n-1)/2} \, .
$$ It may be surprising (it was to me!) that only the first derivative of $g$ occurs on the right hand side. That is a consequence of Faà di Bruno's formula for the derivatives of a composite function. This is surely a known identity, but I haven't found a reference so far. I searched for “Wronskian” in connection with ”chain rule”, “Faà di Bruno's formula“, or “Bell polynomials” and checked the Wikipedia and Wolfram Mathworld pages about those topics. So what I am asking for is a reference for that formula. Or perhaps it is a direct consequence of some other well-known identity for Wronskians? For the sake of completeness I'll provide a sketch of my proof of the above identity. Faà di Bruno's formula states that $$
 \frac{d^k}{dx^k}f_l(g(x)) = \sum_{j=1}^k f_l^{(j)}(g(x)) B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x))
$$ where $B_{k,j}$ are the Bell polynomials . This can be written as a matrix product $$
 \Bigl( (f_l \circ g)^{(k)}(x)\Bigr)_{k, l} = B(x) \cdot \Bigl( f_l^{(j)}(g(x))\Bigr)_{j, l}
$$ where $B(x)$ is the triangular matrix $$
\begin{pmatrix}
 1 & 0 & 0 & \cdots & 0 \\
 0 & b_{1, 1}(x) & 0 &\cdots & 0 \\
 0 & b_{2, 1}(x) & b_{2, 2}(x)& \cdots & 0 \\
 \vdots & \vdots & \vdots & \ddots & \vdots \\
 0 & b_{n-1, 1}(x) & b_{n-1, 2}(x) & \cdots & b_{n-1, n-1}(x)
\end{pmatrix}
$$ with $$
 b_{k, j}(x) = B_{k, j}(g'(x), g''(x), \ldots, g^{(k-j+1)}(x)) \, .
$$ It follows that $$
W(f_1 \circ g, \ldots, f_n \circ g)(x) = \det(B(x)) \cdot W(f_1, \ldots ,f_n)(g(x)) \, .
$$ The diagonal entries of $B(x)$ are $B_{k,k}(g'(x)) = (g'(x))^k $ , so that $$
\det(B(x)) = \prod_{k=1}^{n-1} (g'(x))^k = (g'(x))^{n(n-1)/2}
$$ and that gives exactly the desired result. (It looks easy once you have a proof, but it took me a while to figure this out :)","['wronskian', 'reference-request', 'chain-rule', 'real-analysis']"
3855590,Can two elliptic isometries generate a free group?,"Standard applications of the ping-pong lemma can be used to show that two hyperbolic isometries or two parabolic isometries of $\mathbb{H}^2$ generate a free group. (Assuming they have disjoint fixed points and after passing to high enough powers, of course.) I'm wondering if it's ever possible for two elliptic isometries to generate a free group (rank $2$ ). Clearly they would have to be irrational rotations about different fixed points. The action of each rotation on the boundary of $\mathbb{H}^2$ has dense orbits, so the standard ping-pong argument doesn't go through. If they don't generate a free group, is there a (preferably geometric) way to see where a relation would come from?","['group-theory', 'free-groups', 'hyperbolic-geometry']"
3855610,How many matrices in which product of numbers in each row and column is 1?,"We have a matrix $S$ . Every $S_{ij}$ is equal to $1$ or $-1$ . How many matrices we can have so that product of all entries in every row and column is $1$ ? This means number of $-1$ must be even in all rows and columns. And size of $S$ is $m\times n$ . My idea was that we can divide a matrix of $(m-1)\times (n-1)$ , then count all the possible ways this matrix can have $1$ and $-1$ which is $2^{(m-1)(n-1)}$ .
Then add other part of matrix and set those values in a way that number of $-1$ become even. But this way doesn't work. Sorry if I didn't explain my way very clearly, it's because English is not my native language.","['matrices', 'number-theory', 'combinations']"
3855721,"For which positive integers $x$, $y$ satisfy the following equation: $x^2 + y^2 = 2020$?","This problem is driving me absolutely crazy. I managed to determine the max value of $x$ and $y$ : $x^2 + y^2 = 2020$ $=>x^2 = 2020 - y^2$ It's obvious that square cannot be smaller than 0, and we're looking only for positive integers, therefore: $=> 2020-y^2 > 0$ And we get $y\in \{1, 2, 3 ...44\}$ . Hence $\sqrt{2020}  \approx\ 44,94$ , and $45^2=2025$ , we are only limited to integers between $1$ and $44$ . The same thing goes for variable $x$ . The question in sto solve for every pair of integers in range from 1 to 44 that satisfy this equation. I haven't found out any easier method of doing this, so i checked every single number, and got the solutions: $(x, y) =  \{ (42, 16) ; (24, 38) \} $ . Have you got any ideas how to approach problem like this one? Thanks in advance.","['exponentiation', 'algebra-precalculus', 'inequality']"
3855797,Number of $5$-Sylows of a simple group of order $660$.,"Let $G$ be a simple group of order $660$ . I am trying to find $n_5$ - the number of Sylow $5$ -subgroups of $G$ . I have easily proved that $n_5 \in \{1, 6, 11, 66\}$ using Sylow Theorems. Besides, I have concluded that if cannot be $1$ or $6$ , but I don't know how to exclude the other option (so the result will follow).","['simple-groups', 'group-theory', 'sylow-theory', 'finite-groups']"
3855915,Proving the probability of the sum of 500 dice being an even number,"Prove that the sum of rolling 500 dice being an even number is 1/2. The hint is to define A as the event that ""the sum of the first 499 dice is even"" and define B as the event that ""the 500th dice is even"". My Attempt: Define X as the event ""the sum of 500 dice is even"" Then: $$P(X) = P(B|A) + P(B^c|A^c)$$ $$=\frac{P(B \cap A)}{P(A)} +\frac{P(B^c \cap A^c)}{P(A^c)}$$ $$=\frac{P(B)P(A)}{P(A)} +\frac{P(B^c)P(A^c)}{P(A^c)}$$ $$P(B) + P(B^c) = \frac{1}{2}+\frac{1}{2}=1$$ Where the 2nd to 3rd step is because events B and A are independent of each other. Clearly this is wrong since $P(X)$ should be $\frac{1}{2}$ and not $1$ . I'm assuming there is a mistake in my first step. My thinking is that if the sum of the 499 dice is even, then the 500th has to also be even for the sum of 500 dice to be even, and if the sum of 499 dice is odd, then the 500th dice has to be odd. Any help is greatly appreciated :)","['conditional-probability', 'dice', 'probability-theory', 'probability']"
3855974,Finding the smooth inverse of a function,"Problem: The mapping $\phi: S^2 \longrightarrow S^2 $ by $$\phi(x,y,z)=(x\cos z+y\sin z,x\sin z-y \cos z,z)$$ is a diffeomorphism.  Where $S^2$ is a unit sphere in $\mathbb{R}^3$ . I've already shown that $\phi$ is smooth and bijective. The only thing I can't find is $\phi^{-1}$ that is smooth. Any help would be much appreciated!","['multivariable-calculus', 'smooth-functions', 'differential-geometry']"
3856010,Closed/Exact Forms and Complex Functions,"If $f : D \to \Bbb{C}$ is a function, where $D$ is simply connected, what does it mean to say that $f(z)dz$ is either closed or exact? I know what it means for $Pdx + Qdy$ to be exact or closed when everything is real, but what happens when you move over to $\Bbb{C}$ ?","['complex-analysis', 'differential-forms']"
3856113,compact subset of $\mathbb{C}$ is spectrum. (Banach space / hilbert space),"I'm studying that what set can be the spectrum of operator. If $\mathcal{H}$ is an infinite dimension Hilbert space and $K$ is a non-empty compact subset of $\mathbb{C}$ , show that there is an $A$ in $\mathcal{B}(\mathcal{H})$ such that $\sigma(A) = K$ . Show that any compact set in $\mathbb{C}$ is the spectrum of an operator. This is related question about this question. But it gave an answer when $\mathcal{H}$ is separable, not the infinite dimension case. But I believe that my question can be reduced to link. How to do that. In addition, what about this question? If $K$ is a non-empty compact subset of $\mathbb{C}$ , does there exist an operator $A$ in $\mathcal{B}(C[0,1])$ such that $\sigma(A) = K$ . I saw the result that in Banach space, this property does not hold in general, but I think in this case, we can find this one, but I cannot find this example with above example. can you help me?","['banach-spaces', 'functional-analysis', 'banach-algebras']"
3856117,Is the set of all Idempotent matrix in $M_n(\mathbb{F})$ linearly independent?,Is the set $I:=\{\text{set of all Idempotent matrix in}\ M_n(\mathbb{F})\}$ linearly independent? My thought: I think the answer is no if $\mathbb{F}$ is infinite. If $\mathbb{F}$ is infinite then the class of all idempotent matrix is infinite. Any subset of $I$ with more than $n^2$ elements is not linearly independent [as dim( $M_n(\mathbb{F}))=n^2$ ]. As $\mathbb{F}$ is infinite hence $I$ is infinite. But what about if $\mathbb{F}$ is finite?,"['matrices', 'linear-algebra', 'linear-transformations', 'idempotents']"
3856174,"Prove that if x,y,z are positive integers such that $x^3+y^3+z^3=3(x+y+z+xyz)$ then they must be consecutive numbers","I was thikning going for contradiction, assuming they're not consecutive but I found it very hard to find a statement that contradicts that condition. Maybe stating that there exist some $m, n \in \Bbb{Z}$ such that $|mn|\ne 1$ and $y=x+m$ and $z=x-n$ ? Another way would be assuming at least 2 of them are non-consecutive?
I would like to see general ideas on how to proceed.","['elementary-number-theory', 'algebra-precalculus']"
3856306,Do these triangles with parallel hypotenuses?,"This originated as a question that I encountered while woodworking, but nerdy me had to try and see if there was a solution.  Say I have two right triangles, and each of them have the same angles, but one is slightly larger than the other (with the right angle for both at the origin).  Arranging the triangles on top of one another in this way means that the hypotenuses are parallel and separated by a certain amount.  If I pick the most acute angle and name it theta, I know and can express three specific pieces of information (in addition to the obvious piece that one angle is 90 degrees for each triangle): The length of the opposite side of the larger triangle is 0.5. The length of the adjacent side of the smaller triangle is 1.5. The separation between the two hypotenuses is 0.125. The question is, can I solve the remaining pieces of information to fully define these triangles?  I know that you generally need three pieces of info to solve for any given triangle, and I don't have that for each individual triangle, I only have two pieces of information per triangle.  But I do have a fixed and known relationship between the two the separation of the parallel hypotenuses) which I feel like I ought to be able to use in some way as my third piece of information.  For the life of me though, I can't figure out how to do it.  Any ideas?  Am I trying to solve the unsolvable?  Is there a way to do this?  Thanks in advance for the help!","['trigonometry', 'triangles']"
3856310,Joint distribution of Brownian motion and its running maximum when time is different,"Let $B_t$ be a standard Brownian motion and $B_t^*=\max_{s\leq t}B_s$ . The joint distribution of $(B_t,B_t^*)$ is well known and its density function is given by $$
f(x,y)=\dfrac{2(2y-x)}{t}\cdot\dfrac{1}{\sqrt{2\pi t}}\exp\left(-\dfrac{(2y-x)^2}{2t} \right)
$$ if $x\leq y$ and $y\geq 0$ . My question is that is there a formula for the joint distribution of $(B_s,B_t^*)$ when $s\leq t$ ?","['stochastic-processes', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3856330,Elementary proof of 2.2 in Isaacs's Character Theory,"The exercise 2.2 of Isaacs' Character Theory is stated as follows: (a) Let $\chi$ be a character of $G$ . Show that $\chi(g)$ is afforded
by  a representation $\mathfrak{X}$ of $G$ such that all entries of $\mathfrak{X}(g)$ for all $g\in G$ lie in some field $F\subseteq\mathbb{C}$ with $|F:Q|<\infty.$ (b) Let $\varepsilon=e^{\frac{2\pi i}{n}}$ , where $n=|G|$ and $\chi$ be
a character of $G$ . Let $\sigma$ be an automorphism of the field $\mathbb{Q}[\varepsilon]$ and define $\chi^\sigma$ by $\chi^\sigma(g)=\chi(g)^\sigma$ . Show that $\chi^\sigma$ is a
character of $G$ . I know that if I can prove (a), then (b) will follow easily. Moreover, I know (a) is true according to the ""Brauer's splitting field theorem"" which asserts $\mathbb{Q}[\varepsilon]$ is a splitting field of $G$ . But this problem is in the second chapter of the book, the first chapter of the book is on Wedderburn's theory and the second chapter is on some basic facts of characters. So how can I show (a) in an elementary way?","['group-theory', 'abstract-algebra', 'representation-theory']"
3856441,"If $0\leq f_{n}\leq g_{n}$, $f_{n}\longrightarrow f$, $g_{n}\longrightarrow g$, $\int_{0}^{1}g{n}\longrightarrow \int_{0}^{1}g$, then so is $f_{n}$.","I am working on an exercise stating that: If $0\leq f_{n}\leq g_{n}$ are defined on $[0,1]$ , $f_{n}\longrightarrow f$ , $g_{n}\longrightarrow g$ and $\int_{0}^{1}g_{n}(x)dx\longrightarrow \int_{0}^{1}g(x)dx$ , then show that $\int_{0}^{1}f_{n}(x)dx\longrightarrow \int_{0}^{1}f(x)dx$ is also true. As a first thought, I wanted to use Dominated convergence theorem, but it turned out that DCT cannot be applied here since the exercise does not really say if $\int_{0}^{1}|g(x)|dx<\infty$ . The exercise also has a hint that DCT cannot be used, so I guess it does not assume the integrability of $g$ implicitly. I then tried to bound the integral, but what I obtained was only the following. Since $0\leq f_{n}\leq g_{n}$ , we must have $\int_{0}^{1}f_{n}(x)dx\leq \int_{0}^{1}g_{n}(x)dx$ , then it follows from Fatou that $$\int_{0}^{1}f(x)dx\leq \liminf_{n\rightarrow\infty}\int_{0}^{1}f_{n}(x)dx\leq \limsup_{n\rightarrow\infty}\int_{0}^{1}f_{n}(x)dx\leq \limsup_{n\rightarrow\infty}\int_{0}^{1}g_{n}(x)dx=\int_{0}^{1}g(x)dx,$$ which is not really useful.. It seems that I did not really use $f_{n}\longrightarrow f$ and $g_{n}\longrightarrow g$ , but I have no idea about how to use them. Any idea?","['measure-theory', 'lebesgue-measure', 'sequence-of-function', 'lebesgue-integral', 'measurable-functions']"
3856458,"Change of order of quantifiers ""every$\text{ }t$"" and ""almost surely"": what difference does it make?","Given a certain probability space $(\Omega,\mathcal{A},\mathbb{P})$ and a random variable $X:t\mapsto X(t)$ defined on it, which is the difference between the following statements: $$\color{blue}{\text{every }t}\text{ is }\color{red}{\text{almost surely}}\text{ a nondifferentiability point for }X(t)\tag{1}$$ $$\color{red}{\text{almost surely }} \color{blue}{\text{every }t}\text{ is a nondifferentiability point for }X(t)\tag{2}$$ ? I would rewrite $(1)$ as: $$\tag{1.int}\forall t\text{, }\mathbb{P}(t\text{ is a nondifferentiability point for}X(t))=1$$ and $(2)$ as: $$\tag{2.int}\mathbb{P}(t\text{ is a nondifferentiability point for}X(t), \forall t)=1$$ First, I don't know whether $(1.\text{int})$ and $(2.\text{int})$ are correct ""rewritings"" of $(1)$ and $(2)$ (resp.). In general, whichever the difference between $(1)$ and $(2)$ , which is the gist of such a difference from a mathematical standpoint? I cannot grasp it. Could you please give an example of a random variable for which $(1)$ holds true, but $(2)$ does NOT hold true? (or viceversa)","['quantifiers', 'almost-everywhere', 'intuition', 'probability-theory', 'random-variables']"
3856479,Problem 6 chapter 3 from Evans PDE 2nd edition,"I am working on the following problem I have solved (a) but I'm struggling with (b). This is what I've done so far: First I modified the given equation: $$u_t +div(u\mathbb{b})=u_t+Du\cdot\mathbb{b}+u\,div(\mathbb{b})=0.$$ Then I tried to solve it with the method of characteristics and got the following ODEs $$\dot{\mathbb{x}}=\mathbb{b},\quad \dot z=-div (\mathbb{b})\, z,\quad \dot t=1$$ with initial conditions $$\mathbb{x}(0)=a,\quad z(0)=g(a),\quad t(0)=0.$$ From these I then solve $z$ and get $$z=g(a)e^{-div(\mathbb{b})}.$$ I don't know how to proceed (or if I've made any mistakes) and I have no idea how to use the hint or part (a). Any help with this one? EDIT: I think my solution $z$ is incorrect, I didn't think about the fact that $\mathbb{b}$ depends on $\mathbb{x}$ . I haven't yet figured out the correct one.","['multivariable-calculus', 'partial-differential-equations']"
3856563,Derivative of Renyi entropy,"Let $\log$ denote the logarithm with base $2$ . It is claimed that for $0\leq p_i\leq 1$ , $\sum_i p_i = 1$ and for any $0\leq \alpha \leq \infty, \alpha\neq 1$ $$\frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\sum_i\frac{p_i^\alpha}{\sum_jp_j^\alpha}\log\frac{p_i^{\alpha-1}}{\sum_k p_k^\alpha}$$ I am not able to show this result. Taking the derivative with respect to $\alpha$ , using the product rule and noting that $\frac{d}{dx}a^x = \ln(a) a^x$ , I get $$\frac{d}{d\alpha}\left(\frac{1}{1-\alpha}\log\sum_i p_i^\alpha\right) = \frac{1}{(1-\alpha)^2}\log\sum_i p_i^\alpha + \frac{1}{1-\alpha}\frac{1}{\sum_j p_j^\alpha}\sum_ip_i^\alpha \ln(p_i)$$ How does one proceed to get the desired result?","['calculus', 'derivatives', 'entropy']"
3856582,"If $\partial A$ is bounded, then $A$ is bounded","Let $A \in \mathbb{R}^2$ , $A \neq \emptyset$ . If $\partial A$ is bounded, then $A$ is bounded, is it true?","['elementary-set-theory', 'multivariable-calculus', 'general-topology']"
3856735,Why does 'The King Property' of integration work?,"Today I learned about something known as the king's property which really helps in solving integrals and I wanted to know why does this property work.
I dont know if this terminology is used elsewhere ,
so what im talking about is this property $$ \int_a^b f(x)dx = \int_a^b f(a+b-x)dx $$","['integration', 'definite-integrals']"
3856792,Proving $\frac 12+\sum\limits_{n=0}^\infty\frac{(-1)^n}{(5+6n)(7+6n)}=\frac{\pi}6$ without Leibniz,"Show that $$\frac 12+\sum_{n=0}^\infty\frac{(-1)^n}{(5+6n)(7+6n)}=\frac{\pi}6$$ My proof uses the Leibniz series, $$1-\frac 13+\frac 15-\frac 17+\frac 19-\frac 1{11}+\cdots = \frac{\pi}4$$ \begin{align*}&\qquad 1-\frac 13+\frac 15-\frac 17+\frac 19-\frac 1{11}+\cdots \\ &= 1+\bigg(\frac 15-\frac 17\bigg)-\bigg(\frac 1{11}-\frac 1{13}\bigg)+\cdots -\bigg(\frac 13-\frac 19+\frac 1{15}-\frac 1{21}+\cdots\bigg) \\ &= 1+2\sum_{n=0}^\infty\frac {(-1)^n}{(5+6n)(7+6n)}-\frac{\pi}{12}\end{align*} $$\therefore \frac 12+\sum_{n=0}^\infty\frac {(-1)^n}{(5+6n)(7+6n)}=\frac{\pi}6$$ My question is, how would one solve the problem without the Leibniz series? If one didn't know of the series, I'm not asking for a way to solve the problem through deriving the series first, I'm asking for a way to solve the problem without using the series at all. I am curious because using the series the proof is incredibly short, clever and elegant. Thus, to me, it begs the question on how to solve the problem without the series, whilst still keeping clever and elegant (concision aside). I have no idea, hence why I have posted a question here. I assume some creativity is perhaps involved, vaguely speaking I suppose. Any thoughts? Thank you in advance.","['summation', 'number-theory', 'pi', 'sequences-and-series', 'convergence-divergence']"
3856812,Problems involving endomorphism of groups,"I was at a class, when the teacher used the following statement to prove some facts: $G$ is a group and $f \in End(G)$ such that $f(g)$ is conjugate to $g$ for all $g \in G$ then $f \in Aut(G)$ . Actually, in the end he proved something more about the function that showed that $f$ was in $Aut(G)$ (he
proved that $f = id_G$ ). But, anyways, I was curious if this was true, or if he made a mistake, I tried something:
Firstly, we know that $f$ may be injective, because if $g \in ker(f)$ then $1 =f(g)$ is conjugate to $g$ then $g=1$ , so $ker(f) = 1$ . Then I proved something else: Firstly if we consider $f: G \rightarrow Imf$ , we may have that $f$ is an isomorphism. It's true, since $f$ in injective as we proved, and surjective by the restriction of counter-domain. And actually, if we consider $f|_{Im(f^n)} : Im(f^n) \rightarrow Im(f^{n+1})$ would be an isomorphism by the same argument. Then I tried to obtain some information about the orbits. I did this by proving the following statement about $f$ : $\textbf{Statement}$ : Let $g \in G$ , and $O(g)$ the orbit or conjugacy class of $g$ . If $O(g)$ is finite
then $g \in Im(f)$ and exists some $i>0$ such that $f^i(g) = g$ $\textbf{Proof}$ : Since $O(g)$ is finite   and $f^n(g) \in O(g)$ for all $n$ then we must have $i,j$ , $i<j$ such that $f^i(g) = f^j(g)$ . Then, we have $f^i(g) = f^i(f^{j-i}(g)) \Rightarrow 
g = f^{j-i}(g)$ because $f^i$ will be injective as is compose of injective functions. Since $i<j$ we must have $i \leq j-1$ then $j-i-1 = j-1-i \geq 0$ , so $f(f^{j-i-1}(g)) 
= g$ then $g \in Im(f)$ So if $f$ is not surjective then we must have an infinity conjugacy class. I also proved a corollary of the previous statement that is: if $g \not \in Im(f)$ then there isn't $a_1, ... ,a_k \in G$ all with finite conjugacy class such that $ g= a_1 \cdot ... \cdot a_k$ .
It's true, otherwise it'd exist $i_1,... i_k$ such that $f^{i_l}(a_l) = a_l$ . Then we would have $f^{i_1 \cdot ... \cdot i_k}(g) = f^{i_1 \cdot ... \cdot i_k}(a_1 ... a_k) = 
f^{i_1 \cdot ... \cdot i_k}(a_1) \cdot ... \cdot  f^{i_1 \cdot ... \cdot i_k}(a_k) = a_1 \cdot ...
\cdot a_k = g$ and this would imply that $g \in Im(f)$ . Then corollary of corollary: if we multiply g in the left or in the right by elements whose inverse has finite conjugacy class then it may not have finite conjugacy class: for example if $a^{-1}$ has finite conjugacy class then if $ag$ had finite conjugacy class then, we would have a contradiction, since $g = a^{-1}(ag)$ I also tried something involving the left-inverse but I couldn't achieve any further. I'd like to know if this is true or has some counter-examples.","['group-homomorphism', 'group-theory', 'automorphism-group']"
3856867,Does there exist a Laplace series for the Laplace transform?,I am trying to get a better understanding of what the Laplace transform is. When I learnt about the Fourier transform I found it useful to learn about the Fourier series first. Does there exist some kind of Laplace series which is to the Laplace transform what the Fourier series is to the Fourier transform?,"['fourier-transform', 'fourier-series', 'laplace-transform', 'ordinary-differential-equations']"
3856996,Limit within a limit,Hey can someone please show me a step by step procedure to show that the below is possible. I can't seem to figure out how to reduce a limit within a limit to a single limit and the replacing of the variable 'a' and 'b' with 'c' (Someone mentioned iterated limits but I'm not sure how to exactly show that) $$\lim_{a\to 0}\dfrac{\lim\limits_{b\to 0}\dfrac{{f(x+a+b)-2f(x+b)+f(x)}}{b}}{a}=\lim_{c\to 0}\frac{{{f(x+2c)-2f(x+c)+f(x)}}}{c^2}$$ This idea is used when proving the Grunwald-Letkinov derivative ( https://en.wikipedia.org/wiki/GrünwaldLetnikov_derivative ) and they mentioned that this step is done by assuming that both 'a' and 'b' converge synchronously and can be justified by MVT. If possible could you explain that? and how would I use MVT to justify this.,"['real-analysis', 'calculus', 'limits', 'algebra-precalculus', 'derivatives']"
3857030,Is $N$ a subgroup of $H$?,"Let $N$ be a finite, normal subgroup of $G$ , and $H$ be a subgroup of $G$ such that $[G:H]$ is finite. We were tasked to show that if $[G:H]$ and $|N|$ are relatively prime, then $N\leq H$ . Initially, I thought of just using the Lagrange Theorem for each group-subgroup relationships that I have above, but realized that $|G|$ and $|H|$ might be groups of infinite order. I am having trouble and I don't know if what I thought initially is correct. Just give me hints on how to start and I'll do the rest. I still want to prove it on my own. Thank you very much.","['group-theory', 'normal-subgroups', 'finite-groups']"
3857104,"Understanding the proof and meaning of the Butterfly lemma (Zassenhaus) (Lang's Algebra, pp. 20--21)","I would like to type out my understanding of the Butterfly (Zassenhaus) Lemma, using the notation from pp. 20--21 of Lang's Algebra . I have trouble understanding Lang's proof so this is a hybrid of my own work and other sources. Butterfly lemma: Proof: First off, it is immediate that $U \cap V, U \cap v$ , and $u \cap V$ are subgroups of the unnamed group within which we are working. It is also clear that the latter two are subgroups of the former, since they are subsets of $U \cap V$ and groups themselves. We want to show that the latter two are in fact normal subgroups of $U \cap V$ . To see that $u \cap V$ is normal in $U \cap V$ , consider $x \in u \cap V$ and $z \in U \cap V$ . Because $x \in u$ and $z \in U$ , by normality we see that $zx z^{-1} \in u$ . Because $x \in V$ and $z \in V$ , by closure we see that $zxz^{-1} \in V$ . This shows that $zxz^{-1} \in u \cap V$ which proves normality of $u \cap V$ in $U \cap V$ . An entirely symmetrical argument works to show that $U \cap v$ is normal in $U \cap V$ . Since the product of normal subgroups is still normal, we are able to form the quotient group $\frac{U \cap V}{(u \cap V)(U \cap v)}$ , which we will return to later. Next we want to show that $u(U \cap v), u(U \cap V), (u \cap V)v$ , and $(U \cap V)v$ are groups. Because $U \cap v$ is a subgroup of the normalizer of $u$ , i.e. $U$ itself, it follows that $u(U \cap v)$ is a group. The same argument works to show that $u(U \cap V)$ is a group, and a symmetrical argument works to show that the latter two are groups. (I think these product groups could be written in either order, in terms of the set multiplication, but I haven't checked this. I'm just following Lang's notation here.) Now we want to show that $u(U \cap v)$ is normal in $u(U \cap V)$ and $(u \cap V)v$ is normal in $(U \cap V)v$ . (It is clear that the first and third are subgroups of the second and fourth respectively, since they are clearly subsets and are groups themselves.) Consider the function $f \colon u(U \cap V) \to \frac{U \cap V}{(u \cap V)(U \cap v)}$ given by $ab \mapsto b(U \cap v)(u \cap V)$ , where $a \in u$ and $b \in U \cap v$ . This is a well-defined function because if we take $a' \in u$ and $b' \in U \cap V$ such that $a'b' = ab$ , then $a^{-1}a' = bb'^{-1}$ so by the left-hand side, $a^{-1}a' \in u$ , and by the right-hand side, $bb'^{-1} \in U \cap V$ , so $a^{-1}a' = bb'^{-1}$ must be in $u \cap (U \cap V) = u \cap V \subseteq (u \cap V)( U \cap v)$ . This means that the inverse of $f(a'b')$ is the inverse of $f(ab)$ , which implies that $f(a'b') = f(ab)$ . Furthermore $f$ is a homomorphism, which can be seen as follows. Consider $a, \alpha \in u$ and $b, \beta \in U \cap V$ . We want to show that $f(ab \alpha \beta) = f(ab) f(\alpha \beta)$ . By normality $b \alpha b^{-1} = \alpha' \in u$ , so $\alpha = b^{-1} \alpha' b$ , which means that $ab \alpha \beta = ab b^{-1} \alpha' b \beta = a \alpha' b \beta$ . Therefore $f(ab \alpha \beta) = f(a \alpha' b \beta) = b \beta (u \cap V)(U \cap v) = f(ab) f(\alpha \beta)$ . The homomorphism $f$ is surjective, because for any $x \in U \cap V$ , $f(ex) = x(u \cap V)(U \cap v)$ . As for the kernel of $f$ , we are looking for $ab \in u(U \cap V)$ such that $f(ab) = (u \cap V)(U \cap v)$ . Clearly any $a \in u$ will suffice, and we need $b \in U \cap V$ by definition, but also $b \in (u \cap V)(U \cap v)$ , and the intersection of those two is just $(u \cap V)(U \cap v)$ . We can therefore write $b = xy$ , where $x$ is an element of $u$ and $y$ is an element of $U \cap v$ , which gives us $ab = axy = (ax)y \in u(U \cap v)$ . This shows that $\ker(f) \subseteq u(U \cap v)$ . On the other hand, if $cd \in u(U \cap v)$ , then because $U \cap v \subset (u \cap V)(U \cap v)$ , we see that $f(cd) = (u \cap V)(U \cap v)$ which shows that $u(U \cap v) \subseteq \ker(f)$ . Thus $\ker(f) = u(U \cap v)$ , so $u(U \cap v)$ is normal in $u(U \cap V)$ . By one of the isomorphism theorems, this establishes an isomorphism $\frac{u(U \cap V)}{u(U \cap v)} \cong \frac{U \cap V}{(u \cap V)(U \cap v)}$ . By a symmetrical argument we conclude that $\frac{u(U \cap V)}{u(U \cap v)} \cong \frac{(U \cap V)v}{(u \cap V)v}$ , as desired. Comments: Corrections or comments regarding my proof are appreciated. If anyone can explain Lang's proof in a way that I understand, I'd appreciate that too, because I don't get it. Lastly, if anyone can give me a decent intuition or main takeaway of this lemma, that would be great, because I'm not going to remember the details of this proof. Lang's proof:","['group-theory', 'abstract-algebra', 'solution-verification', 'intuition']"
3857241,Why can we always lift representations of the Lie algebra $\mathfrak{su}(N)$ to representations of the Lie group ${\rm SU}(N)$?,"The Lie group ${\rm SU}(N)$ is connected and compact, therefore the exponential map is surjective. In other words, if $g\in {\rm SU}(N)$ there is $X\in {\frak su}(N)$ such that $g = \exp X$ . Physicists often exploit this to turn the problem of finding unitary representations of ${\rm SU}(N)$ in terms of anti-hermitian representations of $\mathfrak{su}(N)$ . In that case if ${\bar D}:\mathfrak{su}(N)\to {\operatorname{End}}(V)$ is one anti-hermitian representation of $\mathfrak{su}(N)$ they define $D : {\rm SU}(N)\to \operatorname{GL}(V)$ by $$D(\exp X)=\exp {\bar D}(X)\tag{1}.$$ Now, I have a problem with this. Since the exponential map is continuous, and in this case it is surjective, if it were injective it would give a homeomorphism between ${\frak su}(N)$ and ${\rm SU}(N)$ . This cannot happen since ${\frak su}(N)$ is non-compact. Therefore the exponential map cannot be injective. But this makes (1) ambiguous. The reason is that given $g\in {\rm SU}(N)$ there is not just one $X\in \mathfrak{su}(N)$ with $\exp X =g$ , but there may be more. Say there are $X_1,\dots, X_n \in \exp^{-1}(g)$ , then it is not clear which one we should pick to use (1), unless of course it were the case that ${\bar D}(X_i) = {\bar D}(X_j)$ for all $X_i,X_j \in \exp^{-1}(g)$ for all $g\in {\rm SU}(N)$ , which I can't see why would be true for general ${\bar D}$ . In that case why is it ok to use (1) to define one ${\rm SU}(N)$ representation in terms of one ${\frak su}(N)$ representation? What happens with this injectivity issue I have described?","['lie-algebras', 'representation-theory', 'group-theory', 'lie-groups', 'mathematical-physics']"
3857248,"Prove $ \int_{\mathbb{R}^d} \frac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2 $","My question is: Prove that there exists some constant $c_d$ such that for any $\xi \in \mathbb{R}^d$ : $$\displaystyle \int_{\mathbb{R}^d} \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}dy = c_d |\xi|^2 $$ where $\langle \xi, x \rangle = \displaystyle \sum_{j=1}^{d} \xi_j y_j$ . I have $$ \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}} = \dfrac{|2\cos\langle \xi, y \rangle - 2|^2 }{|y|^{d+2}} = \dfrac{16|\sin^2\dfrac{\langle \xi, y \rangle}{2} |^2 }{|y|^{d+2}} \leq \dfrac{16}{|y|^{d+2}}$$ integrable since $d+2 > d$ so the mapping $y \mapsto \dfrac{|e^{i\langle \xi, y \rangle} + e^{- i\langle \xi, y \rangle} - 2|^2 }{|y|^{d+2}}$ is integrable in $\mathbb{R}^d$ . But I don't know how to find prove the equality above. Are there any ideas for this problem?
Thank you so much.","['integration', 'fourier-analysis', 'fourier-transform', 'multivariable-calculus', 'multiple-integral']"
3857275,Cauchys integral formula with an exponential,"The question is to evaluate the integral : $$
\oint\limits_{|z-1|=2}\frac{\mathrm{d}z}{z^2(z^2-4)e^z}
$$ by using Cauchy's integral formula. I have this so far: We have a circle with radius 2 centered at 1, and we have 3 singularities, $z=2, z=-2, z=0$ . However, $z=-2$ is not included within our circle, so we don't need to worry about it. Now we can rewrite what we have as $$
\oint\limits_{|z-1|=2}\frac{dz}{z^2(z^2-4)e^z} = \oint\limits_{|z|=\epsilon} \frac{\frac{e^{-z}}{z^2-4}}{z^2} \mathrm{d}z+ 
\oint\limits_{|z-2|=\epsilon}\frac{\frac{e^{-z}}{z^2(z+2)}}{z-2} \mathrm{d}z.
$$ My solution ends up as $\frac{i\pi}{2} $ + $\frac{i\pi}{8e^2}$ .
I think this is correct, but when I do my calculations I end up with the wrong solution (the solution is in the back of the book). Can anyone guide me in the right direction, or tell me if what I'm doing is wrong? The answer given in the back of the book is given as $\frac{-i\pi}{2} +\frac{i\pi}{4e^2}.$","['complex-analysis', 'cauchy-integral-formula']"
3857321,Whether my understanding of the Central Limit Theorem is a correct way to look at the idea? + a few small questions,"For the last while I've been attempting to truly understand what is being said by the Central Limit Theorem.  I get the general idea, but there are still one or two details that are troubling. To make sure my thinking is correct, I'm going to explain my interpretation of the CLT. At the end I'll ask my questions. The version of the CLT I'm working with comes from Mathematical Statistics and Data Analysis 3rd ed by Rice . Before stating the theorem the following definition is given Let $X_{1}, X_{2}, \dots$ be a sequence of random variables with cumulative distribution functions $F_{1}, F_{2}, \dots$ and let $X$ be a random variable with distribution function $F$ . We say that $X_{n}$ converges in distribution to $X$ if $$\lim_{n \to \infty} F_{n}(x) = F(x)$$ at every point at which $F$ is continuous. With that here is the form of the theorem I'm using: Central Limit Thm : Let $X_{1}, X_{2}, \dots$ be a sequence of independent random variables having mean $\mu$ and variance $\sigma^{2}$ and the common distribution function $F$ and moment generating function $M$ defined in a neighbourhood of zero. Let $$S_{n} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}$$ Then $$\lim_{n \to \infty} P \bigg(\frac{S_{n} - \mu}{\sigma/ \sqrt{n}} \leq x \bigg) = \Phi(x)\ -\infty < x < \infty$$ where $\Phi(x)$ is the standard normal distribution. For my purposes the existence of the moment generation function is not important because I'm just attempting to understand the theorem and not prove things here. I should also mention I'm finishing up Spivak's Calculus so some of the ideas in my explanation come from that. My Thought Process of What's Happening: We are given a sequence of random variables $X_{1}, X_{2}, \dots$ , we are not sure if this sequence of random variables converges or not, one thing we are interested in is if their partial sums eventually converge to a term. The partial sums were defined by: $$S_{n} = \frac{1}{n}\sum_{i = 1}^{n}X_{i}$$ These partial sums, $S_{n}$ can be seen as a sequence in themselves and they are also random variables as they are a function of the $X_{i}$ . The Law of Large Numbers shows that this sequence converges to $\mu$ . That is: $$\lim_{n \to \infty}S_{n} = \lim_{n \to \infty}\bigg(\sum_{i = 1}^{n}X_{i}\bigg) = \mu$$ From this we ask how do the values of our sequence of random variables, $S_{n}$ ( question 1 ) fluctuate around the value $\mu$ ? To determine this we standardize our random variables, $S_{n}$ : $$Z_{n} = \frac{S_{n} - \mu}{\sigma/\sqrt{n}}$$ This again is a sequence of random variables, which are a function of the random variables, $S_{n}$ . We know that $\lim_{n \to \infty}S_{n} \to \mu$ . By calculus since $Z_{n}$ is continuous this means $$ Z_{1}(S_{1}), Z_{2}(S_{2}),Z_{3}(S_{3}), \dots  \to Z(\mu)\ \textbf{question 2}$$ The CLT is discussing probability, in other words it expresses the probability of a realization of random variable occurring. This is where the cumulative distribution functions come into play.  So given a sequence of distribution functions $F_{1}, F_{2}, \dots$ and a distribution function $F$ what I'm envisioning occurring based on the convergence of the $Z_{n}$ is: $$\begin{array}
F_{1}(Z_{1}) & F_{1}(Z_{2}) & \dots & \to & F_{1}(Z) \\
F_{2}(Z_{1}) & F_{2}(Z_{2}) & \dots & \to & F_{2}(Z) \\
\vdots & \vdots & \dots & \to & \vdots \\
F(Z_{1}) & F(Z_{2}) & \dots & \to & F(Z) 
\end{array}$$ The reason for this is because the $F_{i}$ are continuous functions and by calculus properties, if $Z_{n} \to Z$ then $F(Z_{n}) \to F(Z)$ . Now the value that $F(Z)$ at the end of the convergence will take on is $\Phi(x)$ . Thus: $$\lim_{n \to \infty} P \bigg(\frac{S_{n} - \mu}{\sigma/ \sqrt{n}} \leq x \bigg) = \lim_{n \to \infty} F_{n}(Z) =  \Phi(x)$$ My questions (Marked as question 1 in the write up): I wrote $S_{n}$ , but are we looking at the behaviour of $X_{i}$ or the behaviour of $S_{n}$ around $\mu$ ? This has to do with $\lim_{n \to \infty}Z_{n}$ . Since we know by the Law of Large Numbers that $\lim_{n \to \infty}S_{n} = \mu$ , wouldn't that mean that $\lim_{n \to \infty}Z_{n} = 0$ all the time? The CLT revolves around the behaviour of the random variables around $\mu$ . To this end I can ""see"" the idea of since my $X_{i}$ are random variables, in turn the $S_{n}$ should also be random values, but again since we are taking things to the limit we end back up at the constant value of $\mu$ . What am I missing in my understanding of this process? Did I explain the ideas relatively correct? EDIT: Based off of the discussion I've been having with @apprentice, the following is another way to frame my issues: Based off a simple example of what the CLT is doing. Say we have a population, we take a sample from the population, take its average and plot it on a graph. We repeat this process over many times and by the end of it, based on the CLT the distribution of those mean values will be normally distributed. So now I'm trying to take that idea and translate it to the technical jargon used to define the CLT rigourously.","['statistics', 'probability-limit-theorems', 'central-limit-theorem', 'solution-verification', 'probability-theory']"
3857355,Behaviour of $u_{n}=u_{\lfloor n/2\rfloor}+u_{\lfloor n/3\rfloor}+u_{\lfloor n/6\rfloor}$,"I was looking at the following sequence: $$\begin{cases}
u_0=1\\
\forall n \in \mathbb{N^*}, \quad u_{n}=u_{\lfloor n/2\rfloor}+u_{\lfloor n/3\rfloor}+u_{\lfloor n/6\rfloor}
\end{cases}$$ and wanted to show that $$\forall n \in \mathbb{N}, \quad u_n\leq 3(n+1)$$ I know a way to do this is to see that $u_{n+1}\leq u_n+3$ but I can't seem to easily prove that fact (case by case analysis should work, I guess). I also wrote a Python script to check the first $1000000$ terms and found that the best bounding constant isn't $3$ , but actually $C=\frac{169}{73}$ . I read somewhere that you could find an explicit expression of $C$ in terms of $u_0$ , but I can't figure out how. What am I missing? Any help would be appreciated.","['recursion', 'recurrence-relations', 'sequences-and-series']"
3857377,Distinguishing non-isomorphic groups with a group-theoretic property,"I am teaching a first-semester course in abstract algebra, and we are discussing group isomorphisms. In order to prove that two group are not isomorphic, I encourage the students to look for a group-theoretic property satisfied by one group but not by the other. I did not give a precise meaning to the phrase ""group-theoretic property"", but some examples of the sort of properties I have in mind are $$
\forall g,h\in G:\exists n,m\in\mathbb{Z}:(n,m)\neq (0,0)\wedge g^n=h^m,\\
\forall H\leq G:\exists g,h\in G:H=\langle g,h\rangle,\\
\forall g,h\in G:\exists i\in G: \langle g,h\rangle = \langle i\rangle
$$ One of my students asked if, give two non-isomorphic groups, there is always a group-theoretic property satisfied by one group but not the other. In a sense, ""being isomorphic to that group over there"" is a group-theoretic property. But this is not really what I have in mind. To pin down the class of properties I have in mind, let's say we allow expressions involving quantification over $G$ , subgroups of $G$ , and $\mathbb{Z}$ , group multiplication, inversion, and subgroups generated by a finite list of elements the symbol $1_G$ (the group identity element), addition, subtraction, multiplication, exponentiation (provided the exponent is non-negative), and inequalities of integers , the integer symbols $0$ and $1$ , raising a group element to an integer power, and equality, elementhood, and logical connectives. I do not know much about model theory or logic, but my understanding is that this is not the first-order theory of groups. In particular, this MSE question indicates that there exist a torsion and a non-torsion group which are elementarily equivalent (meaning they cannot be distinguished by a first-order statement in the language of groups), but these groups can be distinguished by a property of the above form. I have also heard that free groups of different rank are elementarily equivalent, but these can also be distinguished by a property of the above form. My questions are: (1) Is there a name for the theory I am considering? Or something closely (or distantly) related? (2) Are there examples of non-isomorphic groups that cannot be distinguished by a property of the above form? Are there examples where the groups involved could be understood by an average first-semester algebra student?","['group-isomorphism', 'model-theory', 'logic', 'reference-request', 'group-theory']"
3857403,A matrix related to the möbius function,"Consider the matrix $A_n$ defined for positive integers $n$ by setting the $(i,j)$ th entry to $1$ if $j$ divides $i$ , and $0$ otherwise, for $1\leq i,j\leq n$ . For example, $$A_6=\begin{bmatrix}1&0&0&0&0&0\\1&1&0&0&0&0\\1&0&1&0&0&0\\1&1&0&1&0&0\\1&0&0&0&1&0\\1&1&1&0&0&1\end{bmatrix}.$$ This matrix has the interesting property that $\det A_n=1$ for all $n$ (since it is a lower triangular matrix), and that its inverse can be described explicitly as having $(i,j)$ th entry equal to $\mu(j/i)$ if $i\mid j$ , and $0$ otherwise, where $\mu$ is the möbius function . This fact is easily seen to be equivalent to möbius inversion. Q. Does this matrix have a name? This seems like a basic enough matrix that its properties could (should?) be well-studied, but I don't know what key words to search to find out more, if it has a special name at all.","['number-theory', 'mobius-function', 'linear-algebra']"
3857418,reduce a differential equation $y'+\dfrac{x}{y}=0$,"I want to reduce a differential equation. $y'+\dfrac{x}{y}=0$ I reduced this. But my answer don't much with Wolfram alpha . Please tell me what is wrong. $y'=-\dfrac{x}{y}=-\dfrac{1}{\left( \dfrac{y}{x}\right) }$ When I put, $u=\dfrac{y}{x}$ , $\dfrac{dy}{dx}=-\dfrac{1}{u}$ differentiate both sides with respect to x $\dfrac{du}{dx}=-\dfrac{1+u^{2}}{ux}$ $\dfrac{u}{1+u^{2}}\dfrac{du}{dx}=-\dfrac{1}{x}$ Integrate both sides with respect to x $\dfrac{1}{2}\log \left| u^{2}+1\right| =-\log \left| x\right| +C'$ Organize the formula $u^{2}+1=\dfrac{C}{x}$ $\begin{aligned}\dfrac{y^{2}}{x}+1=\dfrac{C}{x}\\ y=\pm \sqrt{Cx-x^{2}}\end{aligned}$",['ordinary-differential-equations']
3857507,Possible proof of Leibniz Theorem,"As a matter of fact, I had been reading integration for a while, and I came with a possible proof of Leibniz rule, which I am little unsure about. Leibniz's Rule says :' If a $f(t)$ is a continuous function on intervals [a,b] and $α(x)$ and $β(x)$ are differnetiable functions of x whose values lie in [a,b] then; $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx}$$ Now proof: (Note: I am denoting $\frac{dg(x)}{dx}$ as $g'(t)$ ) Let indefinite integral of $f(t)$ be $g(t)+c$ $$\int f(t)dt=g(t)+c \rightarrow f(t)=g'(t)$$ Now putting $$t=\alpha(x) \rightarrow f(\alpha(x))=g'(\alpha(x))...(1)$$ $$t=\beta(x)\rightarrow f(\beta(x))=g'(\beta(x))...(2)$$ Now let's put limits on our indefinite integral $$\int_{\beta(x)}^{\alpha(x)}f(t)dt=[g(t)+c]_{\beta(x)}^{\alpha(x)}=g(\alpha(x))-g(\beta(x))$$ Differentiating both sides $w.r.t$ $x$ $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dt=g'(\alpha(x))\frac{d(\alpha(x)}{dx}-=g'(\beta(x))\frac{d(\beta(x)}{dx}$$ Hence we get finally $$\frac{d}{dx}\int_{\beta(x)}^{\alpha(x)}f(t)dx=f(\alpha(x))\frac{d\alpha(x)}{dx}-f(β(x))\frac{dβ(x)}{dx}$$ {From equations $(1)$ and $(2)$ I replaced $g'(\alpha(x))$ with $f(\alpha(x))$ and similarly for 2nd term} Is this proof correct $?$ Please help and thanks in advance $!$ Any suggestions are appreciated.","['definite-integrals', 'derivatives']"
3857530,Isomorphic Symmetric Groups,"Let S(A) be the set of all bijective functions from A ---> A.
Similarly we define S(B).
Then both S(A) and S(B) are groups under function composition. If S(A) is isomorphic to S(B) , then is it true that |A| = |B| ? For finite sets this follows immediately.
I ve been stuck to proving ( or disapproving) that this result holds. My strategy was constructing an injective function from A to B, and then by following symmetry such a function will also exist from B to A as well. Then using F. Bernstein's Theorem, we could conclude that both sets are of equal cardinality.
However, I was not able to do so.
Any help would be appreciated.","['symmetric-groups', 'group-theory', 'set-theory']"
3857551,Maximum capacity of box without using Calculus?,"Lets use a square cardboard paper (side $s$ ) to make an open box. Remove four small squares from each corner and fold to make a rectangular box of height $x$ . To compute maximum possible volume, we can use AM-GM : $$ \sqrt[3]{(s-2x)(s-2x)\color{blue}{4}x} \le \dfrac{(s-2x)+(s-2x)+\color{blue}{4}x}{3} $$ $$ \Rightarrow V \le \dfrac{2}{27}s^3$$ Note : This matches the result obtained from calculus. Next take a rectangular paper ( $L \times B$ ) where $L > B$ . But AM-GM won't apply directly this time, since $L-2x > B-2x$ . This can be easily done by calculus. But I want to know if a valid solution using inequalities or other pre-Calculus methods exists? I tried : Let $(L-2x)=\lambda (B-2x)$ for maximum capacity. Then AM-GM, $$ \sqrt[3]{(L-2x)\lambda(B-2x)(2+2\lambda)x} \le \dfrac{(L-2x)+\lambda(B-2x)+(2+2\lambda)x}{3} $$ $$ \Rightarrow 2\lambda(1+\lambda)V \le \frac{1}{27}(L+\lambda B)^3$$ where $$ (L-2x)=\lambda(B-2x)=(2+2\lambda)x $$ gives a quadratic in $\lambda$ : $$\dfrac{\lambda (\lambda+2)}{2\lambda+1} = \dfrac{L}{B} = r$$ One obtains $$ V \le  \dfrac{(r+\lambda)^3}{\lambda (\lambda+1)} \dfrac{B^3}{54} $$ Is this correct? If it's incorrect, can this solution be improved? Or, does a different solution using pre-Calculus methods exist? Thank you for your time!","['inequality', 'volume', 'maxima-minima', 'calculus', 'algebra-precalculus']"
3857552,Are all isomorphic simply transitive subgroups of $S_n$ conjugate?,"Let $S_n$ be the symmetric group on $n$ letters. Suppose $G,G'\le S_n$ are transitive subgroups of order $n$ such that $G\cong G'$ . Must they be conjugate in $S_n$ ? My guess is that the answer is either yes, or if not then there should be exactly two conjugacy classes of simply transitive subgroups (represented by the left regular and the right regular representations), but I don't have a proof.",['group-theory']
3857564,On compactifications of Lindelöf spaces,"Consider a compact Hausdorff space $X$ and let $A$ be a $G_\delta$ subset of $X$ . Then the space $X\setminus A$ must be Lindelöf. Keeping this in mind, I wonder if the converse is true, that is, if any Lindelöf Hausdorff space X admits a compactification $Y$ such that the remainder $Y\setminus X$ is a $G_\delta$ subset of $Y$ . I'm familiar with the Stone-Cech compactification, but I can't see whether the remainder $\beta X\setminus X$ is a $G_\delta$ subset of $\beta X$ .","['general-topology', 'compactification', 'lindelof-spaces']"
3857632,"Find $ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|} $","$$ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|) + |y|(e^x - 1)} {|x| + |y|} $$ I tried in this way $ \lim_{(x,y)\to(0,0)} \frac{\sin( |x| + |y|)}{|x| + |y|} + \frac{|y|(e^x - 1)} {|x| + |y|} $ the first term, when $(x,y)\to(0,0)$ , is $1$ .
When $x\to 0 $ we have that $(e^x - 1) \to x$ . Now the limit to solve is: $\lim_{(x,y)\to(0,0)} 1 + \frac{|y|x} {|x| + |y|} = \lim_{(x,y)\to(0,0)} f(x,y) $ $ f(x,y) ≤ | \frac{|y|x} {|x| + |y|} |$ = $ \frac{|y||x|} {|x| + |y|} $ = $ \rho \frac{|\sin(\theta)||\cos(\theta)|}{|\sin(\theta)| + |\cos(\theta)|} $ ≤ $ \rho \frac{1}{2m} $ where $\frac{1}{2}$ is the maximum of the function in the numerator and m is the minimum of the function in the denominator and it is a positive number $ \rho \frac{1}{2m}  \to 0 $ when $\rho \to 0^+$ So the initial limit is 1 Is it ok?","['limits', 'multivariable-calculus', 'polar-coordinates']"
3857640,"Find $F$-related vector fields on $M\times N$, where $F(x)=(x, f(x))$","I am reading Lee's book on Differential geometry. In Chapter 4, Lee has this exercise. Let $M$ , $N$ be smooth manifolds, and let $f:M\to N$ be a smooth map.
Define $F : M\to M \times N$ by $F(x) = (x, f(x) )$ . Show that for every smooth vector field $V$ of $M$ , there is a smooth vector field on $W$ on $M \times N$ that is $F$ -related to $V$ . I can understand that we must have $W_{(x,f(x))}=V_x\oplus Df(x)V_p$ for all $x\in M.$ The set $\{(x,f(x)):x\in M\}$ is a closed set in $M\times N.$ Therefore, if we can show that for all $(p,f(p))\in M\times N$ there is a neighbourhood $U_p$ and a smooth vector field on $U_p$ extending $V_x\oplus Df(x)V_x$ we are done by partition of unity. But I cannot show that. Can someone help me out?","['vector-fields', 'smooth-manifolds', 'differential-geometry']"
3857644,Example of i.d. random variables s.t. $\frac{X_n}{n} \not\rightarrow 0 ~~~a.s.$,"I am looking for a counterexample to the following statement: Let $(X_n)_{n \in \mathbb{N}}$ be a sequence of identically distributed random variables, then $$ \frac{X_n}{n} \rightarrow 0 ~~~a.s.$$ The statement is true when the $(X_n)_{n \in \mathbb{N}}$ are i.i.d. and all $X_n \in L^1$ .
Also, the statement is true for convergence in probability since $$\forall \varepsilon > 0: \mathbb{P}\left(\left\vert \frac{X_n}{n} \right\vert > \varepsilon\right) = \mathbb{P}(\vert X_n \vert > \varepsilon n) \rightarrow 0$$ My first try was to modify the usual example of a sequence converging in probability but not almost surely (i.e., typewriter sequence). However, that forces the random variables to have different distributions. Any tips or pointers would be greatly appreciated.","['convergence-divergence', 'probability-distributions', 'probability-theory', 'examples-counterexamples']"
3857689,"Show that $f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2$ is coercive","I am trying to show that the function $$f(x_1,x_2)=2x_1+(x_2-x_1^2)^2+(1-x_1)^2$$ is coercive on $\mathbb{R}^2$ . To show the function is coercive, we require $\|(x_1,x_2)\|\rightarrow+\infty\implies f(x_1,x_2)\rightarrow +\infty.$ We proceed by using polar coordinates. This gives \begin{align}
f(r,\theta)&=2r\cos\theta+(r\sin\theta-r^2\cos^2\theta)^2+(1-r\cos\theta)^2 \\
&=r^2+1+r^4\cos^4\theta-2r^3\sin\theta\cos^2\theta\\
&\geq r^2+1-2r^3\sin\theta\cos^2\theta. \\
\end{align} However, I am unsure on how to proceed. I require $r\rightarrow +\infty\implies f(r,\theta)\rightarrow +\infty$ , but I am unable to show how $r^2+1-2r^3\sin\theta\cos^2\theta\rightarrow +\infty$ .","['coercive', 'multivariable-calculus']"
3857698,Is there a generalized upper bound for $\sum_{i = 1}^n | X \cap D_i |$ for $n > 2$?,"Let $D_1, ..., D_n$ be arbitrary $n$ sets where $D_i \cap D_j \neq \emptyset$ . In the simplified case where $n = 2$ , we have that $$
\begin{split}
| X \cap D_1 | + | X \cap D_2 | = &| X \cap (D_1 \setminus D_2) | + |  X \cap (D_1 \cap D_2) | \\
&+ | X \cap (D_2 \setminus D_1) | + |  X \cap (D_1 \cap D_2) | \\
= & |X| + |  X \cap (D_1 \cap D_2) | \\
\leq & |X| + | D_1 \cap D_2 |.
\end{split}
$$ My question is that, can we generalize the above upper bound to something like $$
\sum_{i = 1}^n | X \cap D_i | \leq |X| + c,
$$ where $c$ is dependent on $(D_1, D_2, ..., D_n)$ ? It is self-evident that, if $D_1, ..., D_n$ is a disjoint partition of a universe, then we have $\sum_{i = 1}^n | X \cap D_i | = |X|$ . However, it seems difficult for me to bound $c$ when $D_1, ..., D_n$ are not disjoint. It would be appreciated if you could give me any hint.","['elementary-set-theory', 'inclusion-exclusion', 'combinatorics']"
3857773,Approximation of the sum of a series $S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2}$ as $t\to +\infty$,"The function S(t) has the following infinite series form: \begin{align}
S(t) &=\frac 2\pi \int_0^{\pi/2} dx \sin(tx){\sum_{m\ odd}^{\infty} \alpha_m \cos[m(\frac \pi2-x)]}\\
&=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \\
\end{align} Where $m=1,3,5,...$ (odd numbers), $\alpha_m$ is given by this messy expression: $$\alpha_m=-\frac{1}{2m}\left[(\frac ab+1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab-1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{mU_b} + \frac{1}{2m}\left[(\frac ab-1)J_{\frac{m-1}{2}}(\frac{m\epsilon}{2})+(\frac ab+1)J_{\frac{m+1}{2}}(\frac{m\epsilon}{2})\right]e^{-mU_b}$$ where $a$ and $b$ are some positive constants $(a>b>0)$ , $J$ is the Bessel function of first kind, $\epsilon=\displaystyle \frac{a^2-b^2}{a^2+b^2}$ , $U_b=\displaystyle\frac 12[\tanh(2\mu_b)-2\mu_b]$ and $\mu_b=\tanh^{-1}(\displaystyle\frac ba)$ . I don't think this explicit form of $\alpha_m$ is going to help much in this problem, however, it has a very neat asymptotic behavior as: $$\alpha_m \sim \frac{p}{m^{3/2}} \qquad as\quad m \to +\infty$$ where $p$ is a positive constant. My question is: How can I get the asymptotic approximation of $S(t)$ as $t \to +\infty$ ? My observation and attempts: If $t \to m'$ , $m'$ is some large odd integer, we have: $$S(t) \to \frac{p}{2m'^{3/2}} \sin(\frac{\pi m'}{2})$$ \begin{align}
S(t) &=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2}\\
&=-\frac{2}{\pi t^3} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{1-(\frac{m}{t})^2}
\end{align} Since $\displaystyle\sum_{m\ odd}^{\infty} m^2 \alpha_m $ diverges due to the asymptotic behavior of $\alpha_m$ , if we look at the sum $\displaystyle\sum_{m\ odd}^{\infty}\frac{m^2 \alpha_m}{1-\frac{m^2}{t^2}}$ , when $t \to +\infty$ the major contribution should come from $\alpha_m$ of large $m$ . This, in some sense, justifies (not rigorously though) the move for replacing $\alpha_m$ by its asymptotic form $\displaystyle\frac{p}{m^{3/2}}$ in the sum. Thus, $$S(t)=-\frac{2}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^2\alpha_m}{t^2-m^2} \sim -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2} \quad as \quad t \to +\infty$$ Now, the goal is to find the asymptotics to $$I \equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2} $$ as $t \to +\infty$ . My attempts to solve this problem were all to approximate the summation with an integral by writing summation in the form of Reimann sum as $t \to +\infty$ . For instance, my first try is as following: \begin{align}
I &\equiv -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\
&=-\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \frac 2t\\
&\sim -\frac{p}{\pi t^{\frac32}} \cos(\frac{\pi t}{2}) \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx
\quad as \quad t \to +\infty
\end{align} Since the integral $\int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx$ diverges due to the pole at $x=1$ , but the principle value exists and $V.P. \int_0^{+\infty}\frac{x^{1/2}}{1-x^2}dx=-\frac \pi 2$ , it follows that $$I \sim \frac{p}{2 t^{\frac32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty$$ However, approximating the well-behaved summation by a divergent integral seems a bit problematic, because in this way I did not include the effect of $\cos(\pi t/2)$ term which resolves the singularity problem and also it is inconsistent with my observation $1$ . Therefore my second attempt is to include $\cos(\pi t/2)$ in my summation. Using triogeometric property $\cos(\displaystyle\frac{\pi t}{2}) = \cos[\displaystyle\frac{\pi}{2}(t-m)+\displaystyle\frac \pi2 m]=-\sin[\displaystyle\frac{\pi t}{2}(1-\displaystyle\frac mt)]\sin(\displaystyle\frac{\pi m}{2})$ , I thus deduce: \begin{align}
I &= -\frac{2p}{\pi t} \cos(\frac{\pi t}{2}) \sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\\
&=-\frac{2p}{\pi t}\sum_{m\ odd}^{\infty}\frac{m^{1/2}}{t^2-m^2}\cos(\frac{\pi t}{2})\\
&=\frac{p}{\pi t^{\frac32}}  \sum_{m\ odd}^{\infty}\frac{(\frac mt)^{1/2}}{1-(\frac mt)^2} \sin(\frac{\pi m}{2})sin[\frac{\pi t}{2}(1-\frac mt)]\frac 2t\\
&\sim \frac{p}{\pi t^{\frac32}} \int_0^{\infty} \frac{x^{1/2}}{1-x^2}\sin(\frac{\pi tx}{2})\sin[\frac{\pi t}{2}(1-x)]dx \quad as \quad t \to +\infty
\end{align} The above integral can be solved using methods suggested in the comments and answers of this question . Hence, $$I \sim \frac{p}{4t^{\frac 32}} \sin(\frac{\pi t}{2}) + \frac{p}{4t^{\frac 32}} \cos(\frac{\pi t}{2}) \quad as \quad t \to +\infty$$ However, this result is still not consistent with my observation $1$ (I am short by a factor of $2$ ), also I think there is a problem in the step transforming summation into integral due to the term $\sin(\displaystyle\frac{\pi m}{2})$ . At this point, I don't know what should I do to proceed! Any tips, comments, and suggestions are very much welcomed and also I am willing to acknowledge anyone who provides any sorts of help to this problem in my work. Thank you all in advance! Edit I have figured out how to deal with this problem now, the key is indeed the Euler-Maclaurin formula. I am going to post the answer in case anyone is curious about it.","['integration', 'approximation', 'asymptotics', 'sequences-and-series', 'riemann-sum']"
3857788,Generating function of Trinomial Coefficients,Let $f(n)=\sum_{k=0}^{\lfloor\frac{n}{2}\rfloor}{n\choose k}{n-k\choose k}$ . Show that $\sum_{n=0}^{\infty}f(n)x^n=\frac{1}{\sqrt{(1-3x)(1+x)}}$ . I managed to find the following recurrence relation for the trinomial coefficients in Wolfram: $(n+2)a_{n+2}=(2n+3)a_{n+1}+3(n+1)a_n$ . Now using $\sum_{n=0}^{\infty}(n+2)a_{n+2}x^n=\sum_{n=0}^{\infty}(2n+3)a_{n+1}x^n+\sum_{n=0}^{\infty}3(n+1)a_nx^n \implies \frac{dG}{G}=\frac{dx(x+3x^2)}{1-2x-3x^2}$ I can easily solve it except: Question: How do I prove the relation $(n+2)a_{n+2}=(2n+3)a_{n+1}+3(n+1)a_n$ ?,"['power-series', 'combinatorics', 'recurrence-relations', 'generating-functions']"
3857798,A differentiable function on Euclidean Space compatible with scalar multiplication is a linear map,"Here is how the question stated: Problem $f: \mathbb{R}^n \rightarrow \mathbb{R}$ is differentiable and $$f(\lambda x)=\lambda f(x), \forall \lambda\in \mathbb{R}, x\in \mathbb{R}^n.$$ Prove that $f$ is a linear map. My thoughts The equation $f(\lambda x)=\lambda f(x)$ immediately gives the compatibility of scalar, leaving compatibility of addtion to be verified. I try to derive the addtion from $f(\lambda x)=\lambda f(x)$ . Apart from compatibility of scalar, $f$ is a homogeneous function. Suppose that $x=(x_1,x_2,\cdots,x_n)$ , then I get $f(\lambda x_1,\cdots,\lambda x_n)=\lambda f(x_1,x_2,\cdots,x_n)$ . Differentiating by $\lambda$ , I get $$
f_1x_1+f_2x_2+\cdots +f_nx_n=f\left( x_1,x_2,\cdots ,x_n \right) 
$$ where $f_i$ is the partial derivative of $f$ about the $i^{\text{th}}$ variable of its domain. What I need now is $$
f\left( x+y \right) =f\left( x \right) +f\left( y \right) ,\forall x,y\in \mathbb{R}^n
$$ Similarly, we suppose $y=(y_1,y_2, \cdots ,y_n)$ , then we need $$
f\left( x+y \right) =\left( x_1+y_1 \right) f_1\left( x_1+y_1 \right) +\left( x_2+y_2 \right) f_2\left( x_2+y_2 \right) +\left( x_n+y_n \right) f_n\left( x_n+y_n \right) 
$$ equals to $$
f\left( x \right) +f\left( y \right) =x_1f_1\left( x_1 \right) +x_2f_2\left( x_2 \right) +\cdots +x_nf_n\left( x_n \right) +y_1f_1\left( y_1 \right) +y_2f_2\left( y_2 \right) +y_nf_n\left( y_n \right) .
$$ Because $f_i$ , as derivative, is linear, we can break the brackets and cancel $x_if_i(x_i)$ and $x_if_i(y_i)$ . However, terms of form $x_if_i(y_i)$ and $y_if_i(x_i)$ cannot be cancelled, which puzzles me. It is possible that my thoughts were totally off the track! Any help or idea would be welcome!","['multivariable-calculus', 'linear-transformations']"
3857852,Notation For Collection of Mutually Exclusive Subsets,I have a set $K$ that is formed by mutually exclusive subsets $k_1..k_n$ . Can I express it using following notation? $$ \biggr\rvert^{h=n}_{h=1} k_h \subset K\ $$,"['elementary-set-theory', 'notation']"
3857866,Multiplicativity of the Euler characteristic in an equivariant setting,"Let $E$ and $B$ be two complex varieties with finite group $G$ -actions. Suppose there is a $G$ -equivariant morphism $f\colon E\to B$ such that $f$ is a locally trivial fiber bundle with fiber $F$ in the complex topology. Denote by $[e(X)]:=\sum_{i}(-1)^{i}[H_{c}^{i}(X,\mathbb{C})]$ the alternating sum of the compactly supported singular cohomology groups as virtual $G$ -representations. Assume that $B$ is irreducible. Is there a $G$ -action on the cohomology groups of $F$ such that $[e(E)]=[e(B)][e(F)]$ as virtual $G$ -representations $?$ For example, when $E\to B$ is a trivial bundle, although there may not be a well-defined action of $G$ on $F$ , I think there is a well-defined action of $G$ on the cohomology groups of $F$ .","['algebraic-geometry', 'algebraic-topology']"
3857923,Finding 4 Different Sufficient Statistics,"If $X_1, ..., X_n$ is a random sample taken from a geometric population of the form $$
P(X = x;p) = p(1-p)^x,
$$ for $x = 0, 1, 2, ...,$ and $0 < p < 1$ , find four different sufficient statistics for $p$ . Attempt : I have found the joint pmf of the random sample to be $$
P(\mathbf{X} = \mathbf{x}; p) = p^n(1-p)^{\sum x_i}, ~~ x_i = 0, 1, 2, ..., ~~\text{and}~~ 0 < p < 1. 
$$ The only sufficient statistic I can think of is $T(\mathbf{X}) = \sum X_i$ , the sample total. Writing the joint pmf in exponential form, I have found $$
P(\mathbf{X} = \mathbf{x}; p) = \operatorname{exp}\Bigl[n\text{ln}(p) + \sum x_i \text{ln}(1-p)\Bigr]
$$ I don't see how it helps, though.","['statistical-inference', 'statistics']"
3857980,Definition of pencil of hypersurfaces.,"On Voisin's book Hodge theory and complex algebraic geometry II, she defined a pencil of hypersurfaces on a variety $X$ to be a projective line $\mathbb P^1 \subset P(H^0(X, L)) =: |L|$ , where $L$ is a holomorphic line bundle on $X$ . But how is that different from the definition of a general pencil? Do we require the line bundle $L$ here from a diviosr $D$ ? (My definition of a pencil is: A pencil is a linear system of dimension $1$ , i.e. a one-dimensional linear subspace of some $|D|$ , where $|D|\subset Div(X)$ is the set of all effective divisors linearly equivalent to $D$ and $|D|\cong \mathcal{P}(L(D))\cong \mathcal{P}(H^0(X,\mathcal{O}([D]))$ .) Moreover, how is Voisin's definition related to the hypersurfaces? Does that implicitly imply the divisors in $\mathbb P^1\subset |L|$ to be hypersurfaces in $X$ ? I wonder if Voisin's definition of the pencil of hypersurfaces is just the same as my definition of a pencil. But on Voisin's book, she has both divisor and hypersurface, I wonder if she abused the names here?","['complex-geometry', 'algebraic-geometry']"
3857981,Estimate product of Hölder functions,"Let $U \subseteq \mathbb{R}^n$ be the unit ball, let $k \in \mathbb{Z}_{\ge 0}$ , and let $0 < \alpha < 1$ . Let $f, g \in C^{k, \alpha}(U)$ be Hölder functions, i.e. $f$ and $g$ are of class $C^k$ , $$
|f|_{k, \alpha} = \sum_{|I| \le k} \sup_{x \in U} |D^If(x)| + \sum_{|I| = k} \sup_{x \neq y \in U}\frac{|D^If(x) - D^If(y)|}{|x - y|^\alpha} < \infty
$$ and $|g|_{k, \alpha} < \infty$ . Question. Is there a way to estimate $|fg|_{k,\alpha}$ in terms of some Hölder norms of $f$ and $g$ ? For instance, is it true that there exists $C > 0$ such that $$
|fg|_{k, \alpha} \le C(|f|_{k, \alpha} |g|_k + |f|_k |g|_{k, \alpha})
$$ or something of that sort? I read in a book that the above inequality is true when $k = 0$ and I'm wondering what is the right generalization.","['holder-spaces', 'functional-analysis', 'real-analysis']"
3858012,Is the product $AB$ invertible if $A$ is invertible and $B$ is non-invertible?,"$A$ is an invertible matrix and $B$ is a non-invertible matrix. Can $AB$ be invertible?
I have the following idea: Sup. $AB$ is invertible, then: $B=IB=(A^{-1}A)B=A^-1(AB)$ , then apply inverse both sides: $B^{-1}=(AB)^{-1}(A^{-1})^{-1}=(AB)^{-1}A$ , but $B$ is non-invertible (hip). This leads to a contradiction, as we supposed $AB$ is invertible. Therefore $AB$ is non-invertible. I'm not sure if the step where I apply ""inverse both sides"" is right. Otherwise I'm not sure how to prove this. Note 1: I CAN'T use $(AB)^{-1}=B^{-1}A^{-1}$ since the hypothesis for that theorem is $A, B$ invertible matrices and this is not the case. Note 2: I CAN'T use determinants yet.","['matrices', 'abstract-algebra', 'linear-algebra', 'inverse']"
3858048,Motivation behind certain integrating factor,"In this article about exact differential equations, for the fourth example, they have the differential equation: $$ (5xy^2 - 2y) dx  + (3xy^2 -x) dy = 0$$ And they say they integrating factor for this is $$ \mu(x,y) = x^a y^b$$ Now, I don't get how they arrived that this should be correct integrating factors for the problem. Of course, this does integrating factor does satisfy the requirement that the second-order partials commute but I can't really understand how I'd come up with this if it was not already told that this is the integrating factor. If not, for what kinds of differential equations would this integrating factor work? Similar post to this I have seen this post already and it is not the same as mine because the question was not really regarding the integrating factor which I mentioned but rather alternative methods.",['ordinary-differential-equations']
3858081,When is the continuous image of a measurable subset of a Polish space measurable?,"In page two of three of this note: http://math.iisc.ac.in/~manju/MartBM/RaoSrivastava_borelisomorphism.pdf It is said in the proof of Proposition 2, and in the section (ii) that ' $f$ is clearly bi-measurable.' It is clear to me that $f$ is continuous and injective, and that $Z$ is closed. But why is it clear that for any measurable subset of $Z$ , say $M$ , that $f(M)$ is a measurable subset of $X$ ? I can see that it is enough to show that for any open subset of $\prod_{n \geq 1} Z_n$ , $O$ , that $f(O \cap Z)$ is measurable, in an attempt to try to do so I noted that since $Z$ is closed, it is $G_{\sigma}$ (and Polish), so $O \cap Z$ is $G_{\sigma}$ , and $f(O \cap Z) = (f_0 \circ \pi_0)(O \cap Z)$ , and so it is enough to show (using that $f_0$ is bi-measurable) that $\pi_0(O \cap Z)$ is itself measurable, or in particular that the image of a $G_{\sigma}$ subset of $\prod_{n \geq 1} Z_n$ by $\pi_0$ is measurable. But I have been unable to show this.","['measure-theory', 'descriptive-set-theory', 'polish-spaces', 'real-analysis', 'general-topology']"
3858129,Some doubts in Brownian motion quadratic variation proof,"I quote Schilling, Partzsch (2012) Theorem Let $(B_t)_{t\ge0}$ be a one-dimensional Brownian motion and $(\Pi_n)_{n\ge 1}$ be any sequence of finite partitions of $[0,t]$ satisfying $\lim\limits_{n\to\infty}|\Pi_n|=0$ . Then the mean-square limit exists: $$\text{var}_2(B;t)=L^2(\mathbb{P})-\lim\limits_{n\to\infty}S_2^{\Pi_n}(B;t)=t\tag{1}$$ where $S_2^{\Pi}(B;t)=\sum_{t_{j-1}, t_j\in\Pi}|B(t_j)-B(t_{j-1})|^2$ and $\text{var}_2$ is the quadratic variation of a Brownian motion. In the proof of the above theorem, firs it is given that $\Pi=\{t_0=0<t_1<\ldots<t_n\le t\}$ is some partition of $[0,t]$ . Then, at a certain point it is shown that: $$\begin{align}\mathbb{E}\bigg[(S_2^{\Pi}(B;t)-t)^2\bigg]&=\sum_{j=1}^{n}\mathbb{E}\bigg[\left(B(t_j-t_{j-1})^2-(t_j-t_{j-1})\right)^2\bigg]\\&\color{red}{=}\sum_{j=1}^{n}(t_j-t_{j-1})^2\mathbb{E}\bigg[(B(1)^2-1)^2\bigg]\\&\color{red}{\le}2|\Pi|\sum_{j=1}^{n}(t_{j}-t_{j-1})=2|\Pi|t\underbrace{\rightarrow}_{\color{red}{|\Pi|\to 0}}0\end{align}$$ I cannot really understand the three parts in $\color{red}{\text{ red }}$ above. Why $\sum_{j=1}^{n}\mathbb{E}\bigg[\left(B(t_j-t_{j-1})^2-(t_j-t_{j-1})\right)^2\bigg]\color{red}{=}\sum_{j=1}^{n}(t_j-t_{j-1})^2\mathbb{E}\bigg[(B(1)^2-1)^2\bigg]$ ?; Why $\sum_{j=1}^{n}(t_j-t_{j-1})^2\mathbb{E}\bigg[(B(1)^2-1)^2\bigg]\color{red}{\le}2|\Pi|\sum_{j=1}^{n}(t_{j}-t_{j-1})$ ?; What does it mean to ""take limit as $|\Pi|\to0$ ""? Isn't $\Pi$ just a partition of $[0,t]$ ? What does it mean to ""make it go to $0$ ""? Does it mean that partition mesh becomes smaller and smaller?","['quadratic-variation', 'stochastic-processes', 'limits', 'brownian-motion', 'probability-theory']"
3858163,How to show ergodicity on this probability measure.,"I am looking at a way of describing an infinite checkerboard where in each tile a random constant matrix of size $d \times d$ is given . Step 1 : introduction Let $z$ a random vector with uniform distribution in $[-\frac{1}{2},\frac{1}{2}]^d$ and $(a_k)_{k \in \mathbb{Z}^d}$ a family of independent, identically distributed random matrices in $\Omega_0:=\mathbb{R}^{d^2}$ (i.e in more usual probabilistic terms, if we note $(\bar{\Omega},A,\mu)$ a probability space, it is a family $(X_k)_{k \in \mathbb{Z}^d}$ of independent, identically distributed random variables from $\bar{\Omega}$ to $\mathbb{R}^{d^2}$ ). Our goal is to define a random variable $a \in \Omega:=\{ a : \mathbb{R}^d \rightarrow \mathbb{R}^{d^2}, \quad \text{a is measurable} \}$ such that $a(x)$ is worth a random matrix on each tile of the checkerboard. Step 2 : description of one tile We introduced the following space : $$(\Omega_0,\mathcal{F}_0,\mathbb{P}_0)$$ where $\mathcal{F}_0$ is the Borel $\sigma$ -algebra on $\Omega_0$ , and $\mathbb{P}_0$ describes the distribution on a single tile i.e for any geometry paver $[\alpha_1,\beta_1] \times...\times [\alpha_{d^2},\beta_{d²}] \subset \mathbb{R}^{d^2}$ : $$\mathbb{P}_0([\alpha_1,\beta_1] \times...\times [\alpha_{d^2},\beta_{d²}]) \text{ gives the probability chance that $a_k$ } \in [\alpha_1,\beta_1] \times...\times [\alpha_{d^2},\beta_{d²}].$$ Step 3 : description of the checkerboard We then introduce the following product probability space : $$(\Omega',\mathcal{F}',,\mathbb{P}')=(\Omega_0^{\mathbb{Z^d}} \times \Box,   \ \mathcal{F}_0^{\otimes\mathbb{Z}^d} \times \mathcal{B}(\Box), \  \mathbb{P}_0^{\otimes\mathbb{Z}^d} \otimes \lambda)$$ where $\lambda$ is the Lebesgue measure on $\Box=[-\frac{1}{2},\frac{1}{2}]^d$ and $\mathcal{B}(\Box)$ is a the Borel tribe on $\Box \subset \mathbb{R}^d$ . We also introduce the following map : $$\pi : \Omega' \rightarrow \Omega, \quad \pi((a_k)_{k \in \mathbb{Z}^d},z):= \sum_{k \in \mathbb{Z}^d} \mathbb{1}_{k+z+\Box}(\cdot) a_k$$ with $\Omega$ define in the introduction. Finaly, we can equip $\Omega$ with a canonic tribe $\mathcal{F}$ and a probability $\mathbb{P}$ defines as the push-forward of $\mathbb{P}'$ under $\pi$ i.e : $$\mathbb{P}(B)= \mathbb{P}'(\pi^{-1}(B)), \quad \forall B \in \mathcal{F}.$$ Step 4 : interpretation The function $f : x \mapsto \sum_{k \in \mathbb{Z}^d} \mathbb{1}_{k+z+\Box}(x) a_k$ describe the checkerboard where : $f$ is worth the constant matrix $a_k$ on the tile number $k$ the vector $z$ describe the center of the checkerboard. If $z=0$ , then the first tile is $[-\frac{1}{2},\frac{1}{2}]$ , centered in $0$ . We have define a probability $\mathbb{P}$ that describes the chance that $a : x \mapsto \sum_{k \in \mathbb{Z}^d} \mathbb{1}_{k+z+\Box}(x) a_k \in B$ , for any set $B$ of $\mathcal{F}$ , so we have define a law for the random variable $a$ presented in the introduction. Step 5 : Question time As I am quite new to probability, I'm not sure to completely understand the above construction. First of all, do you have any remarks or thoughts that could help understand this probability model ? I would like to show that the measure $\mathbb{P}$ is ergodic and stationary . I have succeed to prove the stationarity i.e : for all $z \in \mathbb{R}^d$ and random real variables $f$ in $L^1(\Omega,\mathbb{P})$ , we have : $$\mathbb{E}[f \circ \tau_z]=\mathbb{E}[f]$$ where $\mathbb{E}$ is the expectation on $(\Omega,\mathcal{F},\mathbb{P})$ and $\tau_z$ is the shift operator defined by $\tau_z(a)=a(\cdot +z)$ from $\Omega$ to $\Omega$ . However, I am strugling to prove the ergodicity, defined by : for any measurable set $E \subset \Omega$ such as $\tau_zE=E$ for all $z \in \mathbb{R}^d$ , then $\mathbb{P}(E)=0 \text{ or } 1$ . I know that there exist another caracterization of ergodicity using Birkhoff's ergodic theorem but I would rather not use it for now. Update 1 I changed my mind about using Birkoff's theorem, so now it would be enough to prove that for any random variables $f \in L^1(\Omega,\mathbb{P})$ we have : $$\underset{R \rightarrow +\infty}{\lim} \frac{1}{|R\Box|} \int_{R \Box} f(\tau_z a) \mathrm{d}z = \mathbb{E}[f], \quad \mathbb{P}-a.e \ \ a \in \Omega$$ and it will give me the ergodicity I desire. Update 2 I've proposed a solution using Kolmogoroff's law inspired from sand piles problem but I'm unsure if it is correct or not. If anyone wants to give me his opinion on this solution I'll be happy to hear it.","['stationary-processes', 'ergodic-theory', 'stochastic-processes', 'random-matrices', 'probability-theory']"
3858212,Cross-entropy loss and stationary points,"I am trying to find the stationary points of the cross-entropy function for binary classification : $$
L(w) = -y \cdot \log(\sigma(wx)) - (1-y) \cdot \log (1-\sigma(wx))
$$ with $$
\sigma(wx) = \frac{1}{1+e^{-wx}}
$$ If I develop and simplify I get: $$
L(w) = \log(1+e^{-wx}) + wx(1-y)
$$ To find the stationary points in the close form I want to differentiate and solve for 0 $$
\frac{dL}{dw} = 0
$$ $$
\frac{dL}{dw} = \frac{-xe^{-wx}}{1+e^{-wx}} + x(1-y) = 0
$$ $$
\frac{xe^{-wx}}{1+e^{-wx}} = x(1-y)
$$ $$
\frac{x}{1+e^{wx}} = x(1-y)
$$ Here I start to face some problems: First, I am not sure that $x \neq 0$ Second, $y \in (0;1)$ , so obviously I am going to face some problems What am I doing wrong and how can I solve this problem? Thank you","['entropy', 'calculus', 'logistic-regression', 'derivatives', 'stationary-point']"
3858217,"symmetric,transitive but not equivalence relation.","How to count the number of binary relations over a set of size 'n' such that they are symmetric, transitive, but not an equivalence relation (i.e. they are not reflexive). Any help is appreciated.
Thank you.","['equivalence-relations', 'discrete-mathematics']"
3858223,STEP 2013 P1 Statistics and Probability Question,"The aim to the following: From a deck of 52 cards (numbered from 1,2,3,...,52) I pick 7 cards, each card having the same probability of being picked. What is the probability that only two of the selected cards add up to 53? My approach: Say we have the cards $A,B,C,D,E,F \text{ and } G$ . The probability that a randomly selected card, say $A$ , from these $7$ has its pair in the set is $\frac{6}{51}$ . Say this pair is $B$ . Now we need to figure out the number of other possible pairs and the probability that a given number doesn't have its pair. The probability that a number doesn't have its pair (from the $5$ numbers left) is $1-\frac{5}{51}=\frac{46}{51}$ . There are $\binom{5}{2}=10$ many different possible pairs. Thus the probability that exactly one pair add up to $53$ is $$p=\frac{6}{51}(\frac{46}{51})^{10}.$$ According to the markscheme this is wrong. Could somebody let me know why is that?","['statistics', 'solution-verification', 'combinatorics', 'probability']"
3858242,A problem on Left Hilbert algebra.,"I got stuck with the following problem while going through Section 10.1 from the book 'Lectures on von Neumann Algebras' by Strătilă and Zsidó. Let $\mathfrak{A}$ be a complex algebra with involution, which is also endowed with a scalar product $\langle\cdot | \cdot\rangle$ . We denote by $\xi\mapsto\xi^{\text{#}}$ the involution in $\mathfrak{A}$ and by $\mathscr{H}$ the Hilbert space obtained by the completion of $\mathfrak{A}$ . We denote by $\mathfrak{A}^2$ the vector space generated by the elements of the form $\xi\eta,\,\xi,\,\eta\in\mathfrak{A}$ . One says that $\mathfrak{A}$ is a left Hilbert algebra if $\mathfrak{A}\ni\eta\mapsto\xi\eta\in\mathfrak{A}$ is continuous, for any $\xi\in\mathfrak{A}$ . $\langle\xi\eta_1|\eta_2\rangle=\langle\eta_1|\xi^{\text{#}}\eta_2\rangle$ for any $\xi,\,\eta_1,\,\eta_2\in\mathfrak{A}$ . $\mathfrak{A}^2$ is dense in $\mathfrak{A}$ . $\mathscr{H}\supseteq\mathfrak{A}\ni\xi\mapsto\xi^{\text{#}}\in\mathscr{H}$ is a preclosed antilinear operator. In accordance with $1$ , for any $\xi\in\mathfrak{A}$ , one define $L_{\xi}\in\mathscr{B}(\mathscr{H})$ by the formula $L_{\xi}(\eta)=\xi\eta,\,\eta\in\mathfrak{A}$ . Problem: Prove that $I\in\overline{\{L_{\xi}:\xi\in\mathfrak{A}\}}^{so}$ , where $I$ is the identity map on $\mathscr{H}$ defined by $I(\eta)=\eta,\,\eta\in\mathfrak{A}$ . The authors say that it follows from property $3$ , but I am not getting how to argue that. Thanks in advance for any help.","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
3858316,"Is a Kähler manifold with fiber $F$ satisying $H^i(F,\mathcal O)=0$ projective?","Let $X$ be a Kähler manifold, $B$ a projective manifold, there is a smooth fibration $\pi:X\rightarrow B$ , such that all the fibers $F$ of $\pi$ satisfy $H^i(F,\mathcal O)=0,i>0$ , then is $X$ a projective manifold? I know there are some special cases: for example, if all the fibers are projective spaces, Kodaira had proved that the total space must be a projective manifold, but if we relax the condition to $H^i(F,\mathcal O)=0,i>0$ , is it still true? This question has already been asked in the comment of a question in MO and Jason Starr commented it is true, but he did not give a proof, and I can't work the proof out by myself, so I ask again here, I need someone to provide more details, thanks!","['complex-geometry', 'algebraic-geometry', 'kahler-manifolds', 'differential-geometry']"
3858320,Chain rule for matrix derivatives,"I have a function $f: \mathbb{R}^{n \times n} \to \mathbb{R}$ , where $\mathbb{R}^{n \times n}$ denotes the set of $n \times n $ real matrices. I have a closed form expression for $$
g(A) := \frac{\partial}{\partial A} f(A).
$$ The goal is to calculate $\frac{\partial}{\partial B} f(C'BC)$ where $B$ and $C$ are $k \times k$ and $k \times n$ matrices, respectively (so that $C'BC$ is a $n \times n $ matrix as it should be). I am thinking it must equal $ C'g(C'BC)C $ but I want to make sure and get some reference for this chain rule. Thanks a lot for your help. [EDIT] It seems clear that my conjecture is wrong. Any help to get me on the correct path would be greatly appreciated.","['matrices', 'calculus', 'matrix-calculus', 'derivatives', 'chain-rule']"
3858362,Solve $\frac{x^3-4x^2-4x+16}{\sqrt{x^2-5x+4}}=0$,Solve $$\dfrac{x^3-4x^2-4x+16}{\sqrt{x^2-5x+4}}=0.$$ We have $D_x:\begin{cases}x^2-5x+4\ge0\\x^2-5x+4\ne0\end{cases}\iff x^2-5x+4>0\iff x\in(-\infty;1)\cup(4;+\infty).$ Now I am trying to solve the equation $x^3-4x^2-4x+16=0.$ I have not studied how to solve cubic equations. Thank you in advance!,"['cubics', 'continuity', 'algebra-precalculus', 'factoring']"
3858389,(geometry) height of water in tilted canister?,"Recently there is a problem online where a rectangular canister with some water in it is tilted and participants are asked to draw what the surface of the water looks like. see the link below. https://gyazo.com/e909323dc3a832b81da5bce5038d54f0 I understand that the water will be parallel with the earth. what I am more interested in is finding out the final height of the water in terms of the height of the canister (y), length of the base of the canister(b), tilt of the canister(theta), and initial level of the water (L). I tried working through this and here is what I got https://gyazo.com/8d3ae66df57be70915a7ea9100cc1256 however when I try some of the extreme values things dont end up working out. Can someone help spot some mistakes that I made and give me a better final answer.","['physics', 'geometry']"
3858398,Understand how to evaluate $\lim _{x\to 2}\frac{\sqrt{6-x}-2}{\sqrt{3-x}-1}$,"We are given this limit to evaluate: $$\lim _{x\to 2}\frac{\sqrt{6-x}-2}{\sqrt{3-x}-1}$$ In this example, if we try substitution it will lead to an indeterminate form $\frac{0}{0}$ . So, in order to evaluate this limit, we can multiply this expression by the conjugate of the denominator. $$\lim _{x\to 2}\left(\dfrac{\sqrt{6-x}-2}{\sqrt{3-x}-1} \cdot \dfrac{\sqrt{3-x}+1}{\sqrt{3-x}+1}   \right) = \lim _{x\to 2}\left(\dfrac{(\sqrt{6-x}-2)(\sqrt{3-x}+1)}{2-x}\right) $$ But it still gives the indeterminate form $\frac{0}{0}$ . But multiplying the expression by the conjugate of the demoninator and numerator we get $$\lim _{x\to 2}\left(\dfrac{\sqrt{6-x}-2}{\sqrt{3-x}-1} \cdot \dfrac{\sqrt{3-x}+1}{\sqrt{3-x}+1} \cdot \dfrac{\sqrt{6-x}+2}{\sqrt{6-x}+2}\right) $$ $$\lim _{x\to 2}\left(\dfrac{6-x-4}{3-x-1} \cdot \dfrac{\sqrt{3-x}+1}{1} \cdot \dfrac{1}{\sqrt{6-x}+2}\right)$$ $$\lim _{x\to 2}\left(\dfrac{6-x-4}{3-x-1} \cdot \dfrac{\sqrt{3-x}+1}{\sqrt{6-x}+2}\right)$$ $$\lim _{x\to 2}\left(\dfrac{2-x}{2-x} \cdot \dfrac{\sqrt{3-x}+1}{\sqrt{6-x}+2}\right)$$ $$\lim _{x\to 2}\left(\dfrac{\sqrt{3-x}+1}{\sqrt{6-x}+2}\right)$$ Now we can evaluate the limit: $$\lim _{x\to 2}\left(\dfrac{\sqrt{3-2}+1}{\sqrt{6-2}+2}\right) = \dfrac{1}{2}$$ Taking this example, I would like to understand why rationalization was used. What did it change in the expression so the evaluation was possible? Especially, why multiplying by the numerator's and denominator's conjugate? I am still new to limits and Calculus, so anything concerning concepts I'm missing is appreciated. I still couldn't understand how a limit supposedly tending to $\frac{0}{0}$ went to be $\frac{1}{2}$ , I really want to understand it. Thanks in advance for you answer.","['rationalising-denominator', 'limits', 'calculus', 'limits-without-lhopital']"
3858426,series sum of sigmoid functions,"This is apparently true (from a paper on Restricted Boltzmann Machines): $${\sum}_{i=1}^{\infty}1/(1+e^{i-(x+0.5)})\approx \ln(1+e^x)$$ (according to the author an ""extremely close"" approximation)  If you just do an integral, you get: $${\int}^{\infty}_1dy/(1+e^{y-(x+0.5)})=(y-\ln(1+e^ye^{-(x+0.5)})){\vert}^{\infty}_1=-\ln(e^{-y}+e^{-(x+0.5)}){\vert}^{\infty}_1=\ln(1+e^{x-0.5})$$ I also tried the expansion (with a resulting double sum): $$1/(1+x)={\sum}_{j=0}^{\infty}(-1)^jx^j$$ I'm missing a trick.
Anyone have any clue how to derive this?  Thank you much.","['summation', 'sequences-and-series']"
3858517,Counting binary strings of length $n$ that contain no two adjacent blocks of 1s of the same length?,"Is it possible to count exactly the number of binary strings of length $n$ that contain no two adjacent blocks of 1s of the same length? More precisely, if we represent the string as $0^{x_1}1^{y_1}0^{x_2}1^{y_2}\cdots 0^{x_{k-1}}1^{y_{k-1}}0^{x_k}$ where all $x_i,y_i \geq 1$ (except perhaps $x_1$ and $x_k$ which might be zero if the string starts or ends with a block of 1's), we should count a string as valid if $y_i\neq y_{i+1}$ for every $1\leq i \leq k-2$ . Positive examples : 1101011 (block sizes are 2-1-2), 00011001011 (block sizes are 2-1-2), 1001100011101 (block sizes are 1-2-3-1) Negative examples : 1100011 (block sizes are 2-2 ), 0001010011 (block sizes are 1-1 -2), 1101011011 (block sizes are 2-1- 2-2 ) The sequence for the first $16$ integers $n$ is:  2, 4, 7, 13, 24, 45, 83, 154, 285, 528, 979, 1815, 3364, 6235, 11555, 21414. For $n=3$ , only the string 101 is invalid, whereas for $n=4$ , the invalid strings are 1010, 0101 and 1001.","['combinatorics-on-words', 'binary', 'combinatorics']"
3858575,"Geometry problem, finding missing angles","One of my students showed me a problem that she says is similar to what they would do in high school in her home country (which i am attaching here. . The goal of the problem is to find the measure of $\angle DEC$ using the given angle measures provided. I've tried working on this to see what other angle measures I could deduce, and I'm including that here This is where I'm stuck. I've tried: Labeled one unknown angle as x and determined all other unknown angles in terms of $x$ , but it's fully consistent and nothing seems to simplify to indicate what $x$ is. Drawn in lines parallel to the sides through various points, and use what I know about parallel lines cut by transversals, but it doesn't seem to get me any closer to the target. I suspect I might need to draw in some additional line or extend the diagram in some way but I can't figure out what. Any help would be greatly appreciated. Thanks!","['triangles', 'geometry']"
3858671,Blow up $\operatorname{Pic}^3(C)$ along $C$.,"I want to solve this problem from  E. Arbarello, M. Cornalba, P. A. Griffiths, J. Harris (auth.) - ""Geometry of Algebraic Curves Volume I"" I can show that the fibers of $\{(K_C-P) \in \operatorname{Pic}^3(C) |  P\in C\}$ are $\mathbb{P}^1$ using R-R theorem. For other points $u$ is isomorphism. But I can't understand why this map should be blow up. Probably there are some criteria (e.g. Castelnuovo's Contractibility Theorem but for higher dimension). Than we can do something similar to the first answer here . Thanks","['algebraic-curves', 'algebraic-geometry', 'blowup']"
3858748,"Looking for an algebraic geometry quote about mathematics, possibly used as a preface","I have a quote in mind that I cannot find a reference for. The quote said something about how mathematics can become unmoored from the original questions and start to become navel gazing. I'm definitely making it sound more harsh than it actually is, but this is the rough guideline. I think it was in the context of saying that the way algebraic geometry is studied you'd never think that it had something to do with finding zeros of polynomials even though that is extremely useful. Not trying to start a fight over if that is correct, just trying to find it. Thanks in advance!","['algebraic-geometry', 'math-history', 'reference-request']"
3858754,$ X = A \cup B $ where $ A $ and $ B $ are closed and $ A \cap B $ is locally connected. Show that $A$ and $B$ are locally connected.,"Let $(Y, \tau)$ a locally connected topological space. suppose $ Y = A \cup B $ where $ A $ and $ B $ are closed and $ A \cap B $ is locally connected. Show that $A$ and $B$ are locally connected. let's see that A is locally connected. Let $x \in A$ and $U \subset A$ open, then $U = A \cap W$ , where $ W $ is an open of $ X $ . We have two cases if $x \in A \setminus B$ and if $x \in A\cap B$ . If $x \in A \setminus B$ . Since $ A \setminus B \subset A $ is open in $X$ , let's take the component $C$ of $x$ in $ (A \setminus B) \cap W$ , $C$ is connected and is also open for $ X $ is locally connected and $C \subset A \cap W=U$ . If $x \in A \cap B$ , I don't know how to prove this case, I would appreciate any help.","['locally-connected', 'general-topology', 'connectedness']"
3858759,"Let f(x) = $x^2+ax+b,a,b \in R$. If $f(1)+f(2)+f(3)=0$, then the nature of the roots of the equation $f(x) =0$ is .....","Let $f(x) = x^2+ax+b$ for $a,b \in \mathbb{R}$ . If $f(1)+f(2)+f(3)=0$ , then the nature of the roots of the equation $f(x) =0$ is (A) real (B) imaginary (C) real and distinct (D) equal roots My attempts: \begin{align}
  f(1) &= 1+a+b \\
  f(2) &= 4+2a+b \\
  f(3) &= 9+3a+b \\
  f(1)+f(2)+f(3)
  &= 1+a+b+4+2a+b+9+3a+b \\
  0 &= 14+6a+3b
\end{align} now how can we take it further about the nature of the roots , whether the roots of $f(x)=0$ is imaginary or real , please help, thanks...","['algebra-precalculus', 'quadratics', 'polynomials']"
3858850,Left subtraction and left division with remainder for ordinals is unique,"From Wikipedia : Furthermore, one can define left subtraction for ordinals $\beta \leq \alpha$ : there is a unique $\gamma$ such that $\alpha = \beta + \gamma$ . I understand both the definitions of ordinal addition as stated on Wikipedia. However, is there any simple argument to see that the quoted statement is true? The first definition using disjoint union of well-ordered sets is more preferable though, rather than the definition by induction. I roughly think that as $\beta$ is a subset of $\alpha$ , taking its union with the well-ordered set $\gamma  = \alpha \setminus \beta$ works, but I'm not sure about uniqueness. Is it just because set complements are unique? More generally, it seems the following holds: Left division with remainder : for all $\alpha$ and $\beta$ , if $\beta > 0$ , then there are unique $\gamma$ and $\delta$ such that $\alpha = \beta·\gamma + \delta$ and $\delta < \beta$ . Is there a simple approach to prove this too?","['elementary-set-theory', 'ordinals']"
3858875,"Let $~v(x, y)~$ be the solution of $~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~$ on $~ℝ^2~$ , then which of the following is/are true?","Problem: Let the function $~v(x, y)~$ be the solution of $~\frac{𝜕^2𝑣}{𝜕𝑥^2}+\frac{𝜕^2𝑣}{𝜕𝑦^2}=0~$ on $~ℝ^2~$ and $~v = x~$ on the unit circle. Then at the origin $(a)~~ v~ $ attains maximum and minimum on the boundary of the circle. $(b)~~ v~ $ does not attain maximum and minimum on the boundary of the circle. $(c)~~ v~ $ is equal to zero. $(d)~~ v~ $ tends to infinity. My approach: From the Maximum-Minimum Principle $($ if $u$ is a harmonic function on a bounded domain $Ω$ in $ℝ^n$ , then $u$ attains its maximum and minimum value on the boundary of $Ω)$ , it is clear that option $\bf (a)$ is true and hence option $\bf (b)$ is not. Now it is given that $~v = x~$ on the unit circle, so form here how to verify the last two options ?","['multivariable-calculus', 'partial-differential-equations', 'real-analysis']"
3858936,Constructing an open non Jordan measurable set,"I am trying to construct an example of a bounded, open, non Jordan measurable set. I enumerate the rationals in $(0,1)^2$ by $\{q_n\}_{n=1}^\infty$ and define $$B_n=\Big(q_n^{(1)}-\frac{1}{2^{n+1}}, q_n^{(1)}+ \frac{1}{2^{n+1}} \Big) \times \Big(q_n^{(2)}-\frac{1}{2^{n+1}}, q_n^{(2)}+ \frac{1}{2^{n+1}} \Big)\cap (0,1)^2,$$ where $q_n=\big( q_n^{(1)},q_n^{(2)} \big)$ . I then define $B=\cup_{n=1}^\infty B_n$ , and try to show that $B$ is not Jordan measurable. I denote by $J^*$ the Jordan outer measure and by $J_*$ the Jordan inner measure, and am trying to show that $J^*(B)>J_*(B)$ . I think I've shown that $J^*(B)=1$ since $\overline{B}=[0,1]^2$ , but I'm having problems showing that $J_*(B)\leq \frac{1}{3}$ . If I define $B_N= \cup_{n=1}^N B_n$ , I can show that $J^*(B_N) \leq \frac{1}{3} \cdot \big( 1-\frac{1}{4^N} \big)$ , but since I only have finite additivity and not $\sigma$ -additivity, I am struggling to show that $J_*(B)\leq \frac{1}{3}$ . At some point I thought to myself that this was enough, but I ca't figure out how to show that any elementary subset $L\subset B$ , a finite union of rectangles, $m(L)\leq \frac{1}{3}$ . I think that this should be a relatively simple argument that I'm not seeing, and would appreciate any useful suggestions.","['measure-theory', 'proof-writing', 'real-analysis']"
3858971,"What to study after Miranda's ""Algebraic curves and Riemann surfaces""?","I am supervising a small reading group on Riemann surfaces. We are following Rick Miranda's book ""Algebraic curves and Riemann surfaces"". We will probably be done at the end of the year, and we would like to continue the seminar. What would be the next best thing to study ? The students are undergrad, so they know topology, algebra, complex analysis and multivariable calculus. We will also, roughly, be familiar with most of the book. (One of the students really wants to study sheaf theory, so something with some sheaf theory would be nice.) They don't know algebraic geometry (other than what is in Miranda). I have some ideas of course, in particular ""chapter on algebraic surfaces"" by Miles Reid, and ""Hodge theory and complex geometry I"" by Claire Voisin. But that might be too hard just after Miranda, so I am interested in other propositions. If possible, avoid suggestions like reading Hartshorne (it's a lot of heavy machinery, and for example most of the applications of chapter 4 can be obtained by elementary methods over $\mathbb C$ , like in Miranda's book.)","['algebraic-geometry', 'sheaf-theory', 'reference-request']"
3859050,Calculus of Variations (Pattern Recognition and Machine Learning),"According to Paul Sinclair this answer is incorrect. Can anyone explain how to use the calculus of variations to show that given $$E[L]=\int \int (y(\textbf{x})-t)^2 p(\textbf{x},t) d\textbf{x} dt$$ we have $$ \frac{\delta E[L]}{\delta y(\textbf{x})} = 2\int(y(\textbf{x})-t)p(\textbf{x},t)dt$$","['partial-derivative', 'multivariable-calculus', 'euler-lagrange-equation', 'calculus-of-variations']"
3859081,Total variation of almost all Brownian motion paths is infinite. Some doubts along the proof,"I quote Schilling, Partzsch (2012) . Let $(B_t)_{t\ge0}$ be a one-dimensional Brownian motion and $(\Pi_n)_{n\ge 1}$ be any sequence of finite partitions of $[0,t]$ satisfying $\lim\limits_{n\to\infty}|\Pi_n|=0$ . Define $$S_2^{\Pi}(B;t)=\sum_{t_{j-1}, t_j\in\Pi}|B(t_j)-B(t_{j-1})|^2$$ and $$\text{VAR}_p(B;t)=\sup\{S_p^{\Pi}(B;t): \Pi\text{ finite partition of }[0,t])\}$$ as the p-variation of a Brownian motion. Statement Almost all Brownian paths are of infinite total variation. In fact we have $\text{VAR}_p(B;t)=\infty$ a.s. for all $p<2$ . $\color{red}{(1.)}$ Proof Let $p=2-\delta$ for some $\delta>0$ . Let $\Pi_n$ be any sequence of partitions of $[0,t]$ with $|\Pi_n|\to0$ . Then \begin{align}\sum_{t_{j-1}\text{, }t_j\in\Pi_n}\left(B(t_j)-B(t_{j-1})\right)^2&=\sum_{t_{j-1}\text{, } t_j\in\Pi_n}\left(B(t_j)-B(t_{j-1})\right)^{2-\delta}\left(B(t_j)-B(t_{j-1})\right)^{\delta}\\&\le\max_{t_{j-1},\text{ }t_j\in\Pi_n}\left|B(t_j)-B(t_{j-1}\right|)^{\delta}\sum_{t_{j-1},\text{ }t_j\in\Pi_n}\left|B(t_j)-B(t_{j-1})\right|^{2-\delta}\\&\le\max_{t_{j-1},\text{ }t_j\in\Pi_n}\left|B(t_j)-B(t_{j-1}\right|)^{\delta}\text{ VAR}_{2-\delta}(B; t)\end{align} The left-hand side converges, at least for a subsequence, almost surely to $t$ . $\color{red}{(2.)}$ On the other hand, $\lim_{\Pi_n\to0}\max_{t_{j-1}, t_j\in\Pi_n}\left|B(t_j)-B(t_{j-1})\right|^{\delta}=0$ , since Brownian paths are (uniformly) continuous on $[0,t]$ . $\color{red}{(3.)}$ This shows that $\text{VAR}_{2-\delta}(B;t)=\infty$ almost surely. $\color{red}{(4.)}$ Questions : $\color{red}{(1.)}$ I know that, by definition, a function $f$ is said to be of finite total variation if $\text{ VAR}_1(f; t)< \infty$ . So, why here are we trying to show that ""almost all Brownian paths are of infinite total variation"", by considering $\text{ VAR}_p(B; t)$ with a generic $p<2$ and not straight with $p=1$ ?; $\color{red}{(2.)}$ I suspect that the property $B(t)\sim\mathcal{N}\left(0,\sqrt{t}\right)$ is somehow involved in the fact that $$\lim\limits_{|\Pi_n|\to0}\sum_{t_{j-1}\text{, }t_j\in\Pi_n}\left(B(t_j)-B(t_{j-1})\right)^2=t\text{ a.s.}\tag{1}$$ but I cannot see how one can explicitly show the almost sure convergence in $(1)$ immediately above (at least for some subsequence); $\color{red}{(3.)}$ Doesn't this contradict point $\color{red}{(2.)}$ ? That is, point $\color{red}{(3.)}$ seems to be saying that $\lim\limits_{|\Pi_n|\to0}\max_{t_{j-1}\text{, }t_{j}\in\Pi}\left|B(t_j)-B(t_{j-1})\right|^{\delta}=0$ , while point $\color{red}{(2.)}$ states that $\lim\limits_{|\Pi_n|\to0}\sum_{t_{j-1}\text{, }t_j\in\Pi_n}\left(B(t_j)-B(t_{j-1})\right)^2=t\text{ a.s.}$ ; $\color{red}{(4.)}$ Does that follow since for $\left|\Pi_n\right|\to0$ , according to all proof passages, one would have $$t\le\left(0\cdot\text{ VAR}_{2-\delta}(B;t)\right)\iff \text{ VAR}_{2-\delta}(B;t)\ge\displaystyle{\frac{t}{0}}=+\infty\iff\text{ VAR}_{2-\delta}(B;t)=+\infty\tag{2}$$ ? Finally, is the result stated in terms of ""almost surely"" since in general a Brownian motion is such that $t\mapsto B_t(\omega)$ is continuous for at least almost all $\omega$ ?","['uniform-continuity', 'proof-explanation', 'probability-theory', 'brownian-motion', 'total-variation']"
3859082,Whats the difference between two set being not nowhere dense and being dense?,"I am really confused with the definitions regarding not nowhere dense and dense. I know that for a set $A$ to be dense, I need to show that for any open interval $I \subseteq \mathbb{R}$ , $I \cap A \neq \emptyset$ In general talking , for something to be dense, if I were to pick any point in $\mathbb{R}$ , that point is either  in the dense set or close to that set (the neighborhood of that point has points from that set). Now I was working on a proof , to show $\mathbb{Q}$ is dense in $\mathbb{R}$ . But then I encounter a question asking to prove Rationals are not nowhere dense,immediately I say this statement is equivalent to rationals are dense in R (same proof as $\mathbb{Q}$ is dense in $\mathbb{R}$ ) but i guess i am wrong because my teacher didn't approve my proof( ironically)
I used the theorem "" between any two reals there is a rational "" to prove. We haven't learned about dense sets yet (I am doing some esearch, reading some definitions in our textbook and trying to solve the questiona given) but I didn't find much about nowhere dense and not nowhere dense definitions that I could understand clearly and start my proof.  Kindly if anyone could make it clear for me ? A Simple proof could also help","['general-topology', 'real-analysis']"
3859187,A group of 40 people consists of 20 women and 20 men. How many ways are there to form 12 pairs from the group?,"An answer I've been given is: $\frac{1}{12!}[(40C2)(38C2)(36C2)(34C2)(32C2)(30C2)(28C2)(26C2)(24C2)(22C2)]$ Which I don't fully understand. Why the answer stops at 22 and doesn't go all the way to 18 doesn't make sense. I can make sense of the $\frac{1}{12!}$ multiplication but I'm not 100% sure why it's necessary.  I've stumbled upon other answers that mention labeled vs unlabeled, could someone provide an example of a question where the $\frac{1}{12!}$ isn't necessary?","['combinations', 'combinatorics', 'discrete-mathematics']"
3859220,"Proving the differentiablity of a function at the point (0,0)","Let $\mathbb{R}^n$ be endowed with a norm $|| \ ||$ and let $f:\mathbb{R}^n\to\mathbb{R}$ be a function satisfying $|f(x)|\leq||x||^2$ for any $x\in\mathbb{R}^n$ . Show that $f$ is differentiable at the point $0\in\mathbb{R}^n$ . Honestly I have problem with the definition of the derivative of a function of many variables. The definition that I have is the following: The map $f$ is said to be differentible at the point $p\in U\subset\mathbb{R}^n$ if exist a linear map $L:\mathbb{R}^n\to\mathbb{R}^n$ such that $$
\underset{h\to0}{\textrm{Lim}}\dfrac{||{f(a+h)=f(a)-L.h}||}{||h||}=0
$$ From the hypothesis I can see that $f(0)=0$ and so we reduce our limit to $$
\underset{h\to0}{\textrm{Lim}}\dfrac{||{f(h)-L.h}||}{||h||}=0
$$ I can see also that if I choose $L$ to be the zero map, the last limit is true. Now my problem is if I can choose the map $L$ to be zero at the point $0\in\mathbb{R}^n$ or if there is something I'm not seeing in the definition that allows me to prove that $f$ is differentiable at $0\in\mathbb{R}^n$","['multivariable-calculus', 'real-analysis']"
3859237,Can this ``almost injective'' function exist?,"Let $\pi: X\to Y$ be a surjective function between the compact, metric and connected spaces $X$ , $Y$ , and $Y_0 = \{y\in Y: \#\pi^{-1}(y)>1\}$ . Suppose that: $Y_0$ is dense in $Y$ , $Y\setminus Y_0$ is a dense $G_\delta$ in $Y$ , and $\#\pi^{-1}(y) \leq N$ for all $y\in Y$ and an universal constant $N$ . My question is: can a function $\pi$ like this exist? Maybe I need more hypothesis on the topology of $X$ and $Y$ . Observe that the connectedness is crucial: sturmian codings of irrational rotations of the circle are surjective functions $\pi:K\to S^1$ from the Cantor set (a totally disconnected space) and the circle such that $\#\pi^{-1}(y) = 2$ for $y$ in a countable dense subset $Y_0 \subseteq S^1$ and $\#\pi^{-1}(y) = 1$ for $y \in S^1\setminus Y_0$ . Condition (3) is also needed: if $f\colon[0,1]\to[0,1]$ is the Thomae's function , $X = \{(x,y)\in[0,1]^2 : 0\leq y\leq f(x)\}$ is the subgraph of $f$ , $\pi: X\to[0,1]$ is the projection onto the first coordinate, and $Y_0 := \{y\in Y: \#\pi^{-1}(y)>1\} = [0,1]\cap\mathbb{Q}$ , then $X$ is connected and (1),(2) hold, but $\pi(y)$ is an uncountable set when $y \in Y_0$ . After building these examples I am more convinced than at the beginning that $\pi$ must be injective. I would appreciate any comment.",['general-topology']
3859243,Characterization of weak convergence with lower semicontinuity,"Let $\mu_n, \mu$ be probability measures on a Polish space $X$ . We say that $\mu_n$ converges weakly to $\mu$ if $$
\int f d\mu_n \to \int f d\mu \:\:\;\;\;\; \forall f \in C_b(X)
$$ Show that it is equivalent to ask $$
\int g d\mu \leq \liminf \int g d\mu_n \:\:\;\;\;\; \mbox{for all g lower semicontinuous bounded from below}
$$ This should be standard but unfortunately I could not find any reference","['measure-theory', 'probability-theory', 'weak-convergence']"
3859330,"What exactly is the reasoning for why $\frac{d}{dx}u(x, Ce^x) = \frac{\partial{u}}{\partial{x}} + Ce^x \frac{\partial{u}}{\partial{y}}$?","I am currently studying the textbook Partial Differential Equations – An introduction , second edition, by Walter A. Strauss. The section The Variable Coefficient Equation of chapter 1 says the following: The equation $$u_x + y u_y = 0 \label{4}\tag{4}$$ is linear and homogeneous but has a variable coefficient ( $y$ ). We shall illustrate for equation \eqref{4} how to use the geometric method somewhat like Example 1. The PDE \eqref{4}  itself asserts that the directional derivative in the direction of the vector $(1, y)$ is zero . The curves in the $xy$ plane with $(1, y)$ as tangent vectors have slopes $y$ (see Figure 3). Their equations are $$\dfrac{dy}{dx} = \dfrac{y}{1} \label{5}\tag{5}$$ This ODE has the solutions $$y = Ce^x \label{6}\tag{6}$$ These curves are called the characteristic curves of the PDE \eqref{4} . As $C$ is changed, the curves fill out the $xy$ plane perfectly without intersecting. On each of the curves $u(x, y)$ is a constant because $$\dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}} = u_x + yu_y = 0.$$ What exactly is the reasoning for why $\dfrac{d}{dx}u(x, Ce^x) = \dfrac{\partial{u}}{\partial{x}} + Ce^x \dfrac{\partial{u}}{\partial{y}}$ ? This seems to be an application of the chain rule, but I don't understand the reasoning behind why the chain rule is appropriate for this case, or for how it is applied.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'chain-rule']"
3859339,Why my derivatives of the equvalent trigonometric functions are different?,"I need to differentiate $$F=2\cos(x)-\sin(2x)$$ $$ F'=−2\sin(x)−2\cos(2x)=0$$ And Extrema should be $$\sin(x)=-\frac{1}{2},\sin(x)=1$$ But when I do it like that: $$F=2\cos(x)-\sin(2x)
=2\cos(x)-2\sin(x)\cos(x)$$ $$F'=-2\sin(x)-2(\cos^2(x)-\sin^2(x))=0$$ $$-2\sin(x)-2\cos^2(x)+2\sin^2(x)=0$$ $$2(-\sin(x)-\cos^2(x)+\sin^2(x))=0$$ $$2(-\sin(x)-1)=0$$ $$\sin(x)=-1$$","['calculus', 'trigonometry']"
3859344,Find $a\in\Bbb Z$ such that $a^3\equiv 3 \pmod{11}$ without Fermat or Euler.,"Find all $a$ integers such that $a^3\equiv 3 \pmod{11}$ I have this problem and I can't use Fermat or Euler theorems because we haven't seen them in class. I also have a solution that I don't understand. I would appreciate if someone explain to me the image solution and/or give me a different approach. PD: On the image, 'o sea' is Spanish for 'that means'.","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3859378,Does $\int_0^x \tan\left(\frac\pi4e^{-t}\right) dt $ have a horizontal asymptote?,"Let $$f(x) = \int_0^x  \tan\left(\frac\pi4e^{-t}\right) dt.$$ Does $f(x)$ have a horizontal asymptote? If so, what value does it tend to? Also, what are the necessary and sufficient conditions on which a function has horizontal asymptotes for a function that cannot be defined by elementary function, i.e., like above?","['limits', 'calculus', 'asymptotics']"
3859426,"Show that $\partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b)$ for all $a,b \in X$","My textbook Analysis II by Amann defines derivative as follows: Let $E=(E,\|\cdot\|)$ and $F=(F,\|\cdot\|)$ are Banach spaces over the field $\mathbb{K}$ ; $X$ is an open subset of $E$ . A function $f: X \rightarrow F$ is differentiable at $x_{0} \in X$ if there is an $A_{x_{0}} \in \mathcal{L}(E, F)$ such that $$
\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-A_{x_{0}}\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
$$ Then we write $\partial f(x_0)$ for $A_{x_0}$ . Could you please confirm if my below understanding is correct? Assume that $f,g:X \to F$ are differentiable at $x_0 \in X$ . Then there are $\partial f(x_0), \partial g(x_0) \in \mathcal{L}(E, F)$ such that $$
\lim _{x \rightarrow x_{0}} \frac{f(x)-f\left(x_{0}\right)-\partial f(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0
$$ and $$
\lim _{x \rightarrow x_{0}} \frac{g(x)-g\left(x_{0}\right)-\partial g(x_0)\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
$$ It follows that $$
\lim _{x \rightarrow x_{0}} \frac{(f+g)(x)-(f+g)\left(x_{0}\right)-(\partial f(x_0) + \partial g(x_0))\left(x-x_{0}\right)}{\left\|x-x_{0}\right\|}=0.
$$ Because the sum of $2$ continuous linear maps is again continuous linear. Then $\partial (f+g)(x_0) = \partial f(x_0) + \partial g(x_0)$ . As such, $$\partial (f+g)(x_0)(a+b) = \partial f(x_0)(a) + \partial g(x_0)(a) + \partial f(x_0)(b) + \partial g(x_0)(b)$$ for all $a,b \in X$ .","['frechet-derivative', 'banach-spaces', 'derivatives', 'functional-analysis']"
3859443,Limit to infinity rule for fractions?,"I am reading a book and it says to solve limits to infinity with a fraction such as: $$\frac{5X^2 + 8X - 3}{3X^2 + 2}$$ We divide the numerator and denominator by the highest power of X in the DENOMINATOR so in this case it is $X^2$ .  I get this helps simplify the equation, but what is to prevent someone from dividing by a higher power like $X^3$ ?  All components would evaluate to 0. Is there another rule for limits that I am not aware of? Thanks!","['limits', 'self-learning']"
3859476,Group with fewer than $p^2$ Sylow $p$-subgroups,"UPDATE : This question has been asked and answered on MathOverflow. Let $G$ be a finite group with fewer than $p^2$ Sylow $p$ -subgroups, and let $p^n$ be the power of $p$ dividing $\lvert G\rvert$ . I can show that if $P$ and $Q$ are any two distinct Sylow $p$ -subgroups of $G$ then $\lvert P\cap Q\rvert=p^{n-1}$ . I was wondering if this intersection is necessarily the same across all Sylow $p$ -subgroups of $G$ . Is the intersection $P\cap Q$ the same for any two distinct Sylow $p$ -subgroups $P$ and $Q$ ? We might as well assume that $G$ has more than one Sylow $p$ -subgroup, in which case here are two equivalent formulations: Does the intersection of all Sylow $p$ -subgroups of $G$ necessarily have order $p^{n-1}$ ? Must there exist a normal subgroup of $G$ of order $p^{n-1}$ ? I'm looking for a proof or counterexample of this conjecture. I know that the conjecture holds in the case where $G$ has $p+1$ Sylow $p$ -subgroups (see Group with $p+1$ Sylow $p$-subgroups ).","['group-theory', 'sylow-theory', 'finite-groups']"
3859507,Mathematical notation of a sorted array/list/set,"I have an array of numbers $A = [3, 5.01, 0.008, 899, 0.23,...]$ , which I need to sort, i.e. $A_{sorted} = \text{sort}(A) = [0.008, 0.23, 3, 5.01, 899,\dots]$ . What is the notation to indicate both $A$ and $A_{sorted}$ , and the "" $\text{sort}$ "" operation? Also, is there any compact notation for sorted arrays? (...by chance, any book/reference about notations for sorted arrays/lists/sets?...) - Thanks a lot!","['elementary-set-theory', 'reference-request']"
3859576,True or False: A rectangle is square if and only if its main diagonals are equal.,"My textbook says this is true, but I disagree. First of all, it should be written that the diagonals are congruent , not equal , yes? Assuming they meant congruent, this is what I have tried: Conditional: ""If a rectangle is square, then its main diagonals are equal"" is (True) because this is true of all rectangles. Conversely: ""If a rectangle's diagonals are equal, then it is a square"" is (False) because there exists a rectangle that is not a square that has equal diagonals. Am I right on this?","['logic', 'geometry', 'discrete-mathematics']"
3859667,Checking whether the minimal sufficient statistic is complete or not for negative exponential distribution,"Let $X_1, X_2..., X_n$ follows iid negative exponential distribution with pdf $$f(x) = \frac{1}{\theta^2} \: e^{-\frac{(x-\theta)}{\theta^2}} \: \: I_{(x>\theta)} $$ I have to show whether the minimal sufficient statistic for this pdf is complete or not?
I have found that the minimal sufficient statistic is $T=\left( X_{(1)}, \sum_{i=1}^{n} (X_i - X_{(1)}) \right)$ . If this minimal sufficient statistic is not complete then there exists a function $h(T)$ of the minimal sufficient statistic such that $E_\theta [h(T)] =0$ for all $\theta>0$ where $h(T)$ is not identically zero. Is this minimal sufficient complete or not? How can I find the function $h(T)$ of the minimal sufficient statistic? Note that, $X_{(1)} $ is the first order statistic i.e., $min\{X_1,..X_n\}$ . I have calculated the pdf of $X_{(1)}$ . Let $Y= X_{(1)}$ then the pdf of $Y$ is given by, $$ f(y) = \frac{n}{\theta^2} \: e^{-\frac{n(y-\theta)}{\theta^2}} \: \: I_{(y>\theta)} $$ I have also calculated $$E(X)= \theta^2 + \theta $$ and $$E(Y) = \frac{\theta^2}{n} + \theta$$ Now, please help me to find out $h(T)$ for which $E_\theta[h(T)] = 0$ for all $\theta>0$ if the minimal sufficient statistic is not complete.","['statistical-inference', 'statistics', 'order-statistics']"
3859677,Holomorphic line bundles with trivial Chern class are flat,"Let $X$ be a complex, projective algebraic variety and let's work in the differential-complex setting. Let $L$ be a non-trivial hermitian holomorphic line bundle and assume that $c_1(L)=0$ . Can we always find a connection such that the associated curvature form $\Theta(L)$ is $0$ ? In other words, in this setting do we have the following implication? trivial first Chern class implies flat line bundle","['differential-geometry', 'complex-geometry', 'algebraic-geometry', 'characteristic-classes', 'line-bundles']"
