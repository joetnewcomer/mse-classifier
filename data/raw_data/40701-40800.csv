question_id,title,body,tags
400651,Proof of $\lim_{s\to 1^+}\frac{\sum_p p^{-s}}{\ln(s-1)}=-1$,"I am looking for a proof of $$\lim_{s\to 1^+}\frac{\sum_p p^{-s}}{\ln(s-1)}=-1.$$
If the proof is too long, a direct reference is fine. Here the sum $\sum_p$ denotes the sum over all prime numbers. EDIT: Perhaps this is one of the examples Mathematica does not help. I put in $s=1.0001$ and calculate Sum[(Prime[n])^(-1.0001),{n,1,10000}]/Log(0.0001) I get $-0.294043$. I even use smaller $s$, and larger upper bound for $n$, but the answer is far from $-1$. If you can get it to close to $-1$, please let me know how you do it.",['number-theory']
400655,Why do algebraic varieties contain curves passing through two given points,"Let $X$ be an algebraic variety over the complex numbers. My definition of an algebraic variety is a finite type separated $\mathbf C$-scheme. Someone told me that such varieties have the following property. For all $x,y\in X$, there exists a smooth quasi-projective connected curve $C$ and a morphism $\gamma:C\to X$ such that the image of $\gamma$ contains $x$ and $y$. (Note that I allow the image of $C$ to be singular.) I know that this is true for projective varieties. The argument is classic. But why is this true in general? Maybe I'm wrong, and this fails for quasi-projective varieties. But can anyone then give me a counterexample?","['algebraic-geometry', 'algebraic-curves', 'complex-geometry']"
400664,Eigenvalues of a tridiagonal trigonometric matrix,"Let $D$ be the diagonal matrix w/alternating in sign diagonal entries: 
$$D_{kk}=(-1)^{k+1}\tan(\frac{k\pi}{2n+1}),$$
where $k=1,2,\dots n\in N$, and let $B$ be the $n$ by $n$ square $(0,1)$-matrix
$$B=
\begin{pmatrix}
 0     & 1 & 0 & \ldots & 0 \\
 1     & 0 & 1 & \ldots & 0 \\
 0     & 1 & \ddots & \ddots & \vdots \\
 \vdots    & \vdots & \ddots & 0 & 1 \\
 0     & 0  & \ldots & 1 & 1 \\
\end{pmatrix}.
$$
(a) Prove, that the eigenvalues of the product matrix $(-1)^{n+1}DB$ are
$$
 2\sin\left(\frac{k\pi}{2n+1}\right), \,\, k=1,2,\dots,n.
$$ The result follows from a continued fraction identity w/a lengthy proof and an exercise from the open Wiki book ""On 2D Inverse Problems"" ( http://en.wikibooks.org/wiki/On_2D_Inverse_Problems ), but a direct shorter proof w/some geometric intuition would be very useful. (b) The matrix $D$ is a discrete version of the operator $\frac{d^2}{dx^2}+2+\delta$. Is there differential/continuous/limiting equation version of the result in (a)?","['continued-fractions', 'matrices', 'trigonometry', 'linear-algebra', 'numerical-methods']"
400677,Covariance of order statistics (uniform case),"Let $X_1, \ldots, X_n$ be uniformly distributed on $[0,1]$ and $X_{(1)}, ..., X_{(n)}$ the corresponding order statistic. I want to calculate $Cov(X_{(j)}, X_{(k)})$ for $j, k \in \{1, \ldots, n\}$. The problem is of course to calculate $\mathbb{E}[X_{(j)}X_{(k)}]$. The joint density of $X_{(j)}$ and $X_{(k)}$ is given by $$f_{X_{(j)}, X_{(k)}}=\binom{n}{k}\binom{k}{j-1}x^{j-1}(y-x)^{k-1-j}(1-y)^{n-k}$$ where $0\leq x\leq y\leq 1$. (I used the general formula here .) Sadly, I see no other way to calculate $\mathbb{E}[X_{(j)}X_{(k)}]$ than by $$\mathbb{E}[X_{(j)}X_{(k)}]=\binom{n}{k}\binom{k}{j-1}\int_0^1\int_0^yxyx^{j-1}(y-x)^{k-1-j}(1-y)^{n-k}\,dx\,dy.$$ But this integral is too much for me. I tried integration by parts, but got lost along the way. Is there a trick to do it? Did I even get the limits of integration right? Apart from that, I wonder if there's a smart approach to solve the whole problem more elegantly.","['statistics', 'order-statistics']"
400705,Show that an Ehresmann connection on a principal G bundle is equivalent to a Lie Algebra Valued one form.,"Let $E$ be a smooth principal $G$-bundle on M. The vertical bundle $V$ is defined as $V=\ker(d\pi:TE\to \pi^*TM)$. An Ehresmann connection on $E$ is a smooth subbundle $H$ of $TE$ (also called the horizontal bundle), such that $TE=H\oplus V$. Now let $H$ be invariant with respect to the $G$ action on $E$, so $H_{eg}=d(R_g)_e(H_e)$ for all $e\in E$ and all $f \in G$, where $d(R_g)_e$ is the differential of the right action of $g$ at $e$. The Ehresmann connection, $H$ should be equal to a 1-form $\omega$ on $E$ with values in the Lie algebra $\mathfrak{g}$ of $G$. Can anyone provide me any insight as to why this is true and how one constructs this 1-form from the Ehresmann connection? I have poor knowledge of differential geometry.","['principal-bundles', 'lie-algebras', 'differential-geometry']"
400708,"How to prove $4\times{_2F_1}(-1/4,3/4;7/4;(2-\sqrt3)/4)-{_2F_1}(3/4,3/4;7/4;(2-\sqrt3)/4)\stackrel?=\frac{3\sqrt[4]{2+\sqrt3}}{\sqrt2}$","I have the following conjecture, which is supported by numerical calculations up to at least $10^5$ decimal digits:
$$4\times{_2F_1}\left(-\frac{1}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)-{_2F_1}\left(\frac{3}{4},\frac{3}{4};\frac{7}{4};\frac{2-\sqrt{3}}{4}\right)\,\stackrel?=\,\frac{3\sqrt[4]{2+\sqrt{3}}}{\sqrt{2}},$$
where $_2F_1$ denotes the hypergeometric function . Can you suggest any ideas how to prove it? The conjectural closed form was obtained using WolframAlpha query ToRadicals[RootApproximant[2.94844576626425580599908814238570067699233]]","['closed-form', 'special-functions', 'calculus', 'conjectures', 'hypergeometric-function']"
400749,Are absolute extrema only in continuous functions?,"The Extreme Value Theorem says that if $f(x)$ is continuous on the interval $[a,b]$ then there are two numbers, $a≤c$ and $d≤b$, so that $f(c)$ is an absolute maximum for the function and $f(d)$ is an absolute minimum for the function. So, if we have a continuous function on $[a,b]$ we're guaranteed to have both absolute maximum and absolute minimum, but functions that aren't continuous can still have either an absolute min or max? For example, the function $f(x)=\frac{1}{x^2}$ on $[-1,1]$ isn't continuous at $x=0$ since the function is approaching infinity, so this function doesn't have an absolute maximum. 
Another example: suppose a graph is on a closed interval and there is a jump discontinuity at a point $x=c$, and this point is the absolute minimum. The extreme value theorem requires continuity in order for absolute extrema to exist, so why can there be extrema where the function isn't continuous?","['optimization', 'calculus', 'functions', 'continuity', 'real-analysis']"
400770,Is there an open mapping theorem for affine morphisms?,"Let $A$ and $B$ be rings. If $\varphi : A \longrightarrow B$ is such that $^a\varphi : Spec(B) \longrightarrow Spec(A)$ is bijective, then in what conditions $^a\varphi$ is a homeomorphism? Or, more general, in what conditions $^a\varphi$ is open if one assume only surjectiveness? If I pick the constructible topology on the spectrum (applying the functor $(\bullet)^{Bool}$ that makes the space Boolean), then it will be a homeomorphism. So I was thinking about creating some way of returning back any information from the Boolean space to the original space when I restrict $A$ and $B$ to be some kind of rings. Thanks in advance.","['general-topology', 'commutative-algebra', 'algebraic-geometry']"
400784,How do you determine the points of inflection for $f(x) = \frac{e^x}{1+e^x}$?,"$$f(x) =\dfrac{e^x}{1+e^x}$$
I know we can find points of inflection using the second derivative test. The second derivative for the function above is $$f''(x) = \dfrac{e^x(1-e^x)}{(e^x+1)^3}$$ I have found one critical point for the second derivative which is $0$. I then determined that the function is concave up from $(-\infty,0)$ and concave down from $(0,\infty)$. I am now asked to find the points of inflection. How would I determine the exact points from where the function switches from concave up to concave down?","['calculus', 'derivatives']"
400788,Upper and Lower Triangular Matrices,"Given the matrix A=$ \left( \begin{array}{ccc}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8\\
1 & -1 & 2 & 3 \\
2 & 1 & 1 &2\end{array} \right) $, write it in the $L_{4 \times 4}U_{4 \times 4}$, where L is the lower triangular matrix and U is the upper triangular matrix. To be honest, I don't even understand what the question is asking of me, however I do know what upper and lower triangular matrices are. Thanks in advance for your help.","['matrices', 'linear-algebra']"
400805,Is there a conditional version of the asymptotic equipartition property?,"Let $X_i$ be independent random variables with $\operatorname{Pr}(X_i = x) = p_x$, and let $F_n$ be the empirical frequency distribution of $X_1, \ldots, X_n$: that is, $(F_n)_x$ For any frequency distribution $f$, write $K(f \| p)$ for the relative entropy
$$ \sum_x f_x \log \frac{f_x}{p_x}.$$
As $n$ grows, $K(F_n \| p)$ converges in probability to $K(p \| p)$, which is zero. This is basically the asymptotic equipartition property. Now, condition on $F_n$ lying in some open set $U$ in the space of frequency distributions. Conditionally, I suspect that $K(F_n \| p)$ should converge in probability to $\inf_{f \in U} K(f \| p)$. Is this true? (Maybe with extra assumptions?) If so, how do you prove it?","['probability-theory', 'information-theory']"
400806,Differential Equations Reference Request,"Currently I'm taking the Differential Equations course at college, however the problem is the book used. I'll try to make my point clear, but sorry if this question is silly or anything like that: the textbook used (William Boyce's book) seems to assume that the reader doesn't familiarity with abstract math, so it lacks that structure of presenting motivations, then definitions, then theorems and corolaries as we see in books like Spivak's Calculus or Apostol's Calculus. I've already seem a question like this on Math Overflow, however unfortunatelly some people felt offended somehow and said: ""are you saying that Boyce is easy? It doesn't matter if you know how to prove things, you must learn to compute"", and my point is not that: Apostol and Spivak teaches how to compute also , however since their books are aimed to mathematicians they take care to build everything very fine and their main preocupation is indeed in the theoretical aspects. I really don't like the approach of: well, in some strange way we found that this equation works, so memorize it, know how to compute things and everything is fine. I really want to understand what's going on, and until now I didn't find this possible with Boyce's book (certainly there are people that find this possible, but I'm used to books like Spivak's and Apostol's, so I don't really do well with books like Boyce's). I've already seem Arnold's book on Differential Equations, but the prerequisites for reading it are bigger: he uses diffeomorphisms many times and although I'm currently also studying differential geometry, I don't feel yet comfortable on reading a book like this one. Can someone recommend a book that covers ordinary differential equations, systems of differential equations, partial differential equations, and so on, but that can be read without much prerequisite, and that still has the structure of a book of mathematics? And when I say ""structure of book of mathematics"" is being like Spivak's and Apostol's books: not mixing up definitions, theorems and examples inside histories of how the theory developed. Since I'm a student of Mathematical Physics, of course motivations and examples from Physics are welcome, but not all mixed up in the text. In truth I don't really believe that a book like the one I described exists (if a book on this topic is good in my point of view, I believe it'll have a lot of prerequisites). Anyway I hope that I don't get misunderstood in what's my doubt, and really sorry if this question is silly in someway.","['ordinary-differential-equations', 'reference-request']"
400813,Representing sums of matrix algebras as group rings,"Let $A = M_{n_1}(\mathbb R) \oplus M_{n_2}(\mathbb R) \oplus ... \oplus M_{n_m}(\mathbb R)$ be a direct sum of real matrix algebras. Under what conditions does there exist a group ring $\mathbb R[G]$ which is isomorphic to $A$? I know every group ring is isomorphic to some such $A$ by Wedderburn's theorem, and I want to determine the extent to which the converse holds. I know that $A$ must have a $\mathbb R$-summand, corresponding to the linear span of $\sum_{g\in G} g \in \mathbb R[G]$. Is this condition sufficient as well? If not, is there a nice criterion, at least for small numbers of summands?","['noncommutative-algebra', 'representation-theory', 'group-theory']"
400840,Distribution of Digit Products,"A digit product $P(n)$ of a natural number $n$ is given by the product of its decimal digits. For example: $$P(1234) = 24,\;\;\; P(24) = 8,\;\;\; P(8) = 8$$
$$1\times2\times3\times4 = 24, \;\;\; 2\times4 = 8$$ Clearly $P(x) \leq x$ as a number with digits ""abcd..."" gives a product less than $a\times10\times10\times10...$. So $$P(x) < x\;\;\; \text{or}\;\;\; P(x) < 10$$ Letting $P^n(x)$ represent applying $P$ $n$ times, I have a question about $$P^\infty(x) = \lim_{n\rightarrow\infty}P^n(x)$$ The larger the number the more likely it is that it will contain a zero. So, for a random $x$ we expect that almost always $P^\infty(x)=0$. But we can get rid of all these from the number line: $$S = \{x\in\mathbb{N}_0|P^\infty(x)\neq0\}$$ What is the distribution of $P^\infty(s)$ for $s\in S$? How likely are the remaining digits? Is one infinitely more likely than the others? Here's a graph of $10$ to $1,000,000$:","['products', 'probability', 'number-theory']"
400895,property of function,"Let $f _n (x)=x ^n$  . If I want to get $f_{n+1}'(x)$ , firstly I find $f _{n+1} (x)=x^ {n+1}$   and next differentiate $f _{n+1} (x)=x^ {n+1}$  , I obtain $f _ {n+1}' (x)=(n+1)x^ n $ . But in other ways, firstly differentiate $f _n (x)$ , obtain $f  _n '(x)=nx^ {n−1} $ , and substitute $n+1$  for $n$  in $f  _n '(x)=nx^ {n−1}$  , I get $f_  {n+1}' (x)=(n+1)x ^n$   too. I think second ways is not definition of $f ′ _{n+1} (x)$ . Is there any function which satisfying first ways is not equal to second ways?",['derivatives']
400914,Where to start when learning math (again)? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question I have a few questions I hope you can help me answer. First, I'll introduce myself. I'm a finance undergraduate student in Australia, but I'm originally from Norway. Throughout school I always loved math, but I ended up studying finance. The last year or so I have started to realise that I should have done Computer Science or Engineering in stead, as I would like to see myself in a quant role when I finish studying. Even so, I've decided to finish this finance degree. The last month or so I've started programming in C++, and refreshing up on my maths knowledge (Khan Academy, algebra, precalculus). My problem is I don't know where to go from here, and what order I should be learning the different branches of math. I've picked up a book on linear algebra today; ""Linear algebra and its applications"" by Gilbert Strang. Do you think this is an alright place to start? Or do I start with basic calculus? And where can I go next? Differential equations? I hope some of you can help me in the right direction. Thanks.","['linear-algebra', 'calculus', 'ordinary-differential-equations']"
400954,"Show that If k is odd, then $\Bbb{Q}_{4k}$ is isomorphic to $\Bbb{Z}_k \rtimes_{\alpha} \Bbb{Z}_4$ for some $\alpha$","Show that if k is odd, then $\Bbb{Q}_{4k}$ is isomorphic to $\Bbb{Z}_k \rtimes_{\alpha} \Bbb{Z}_4$ for some $\alpha: \Bbb{Z}_4 \rightarrow Aut(\Bbb{Z}_k)$ . Calculate $\alpha$ explicitly. We know that $\Bbb{Q}_{4k} = \{ b^k_{2n}, b^k_{2n}a | 0 \leq k < 2n \}$ , and that everything outside of the cyclic group $\langle b \rangle$ is of order 4. What confuses me is that we only have one cyclic group in $\Bbb{Q}$ , right? However, in order to write it of the form $\Bbb{Z}_k \rtimes_{\alpha} \Bbb{Z}_4$ , we need to find a normal cyclic group of order k in $\Bbb{Q}_{4k}$ , and I can't really see that. For example, in the group $\Bbb{Q}_{12} = \{e, b, b^2, b^3, b^4, b^5, a, ab, ab^2, ab^3, ab^4, ab^5\}$ , we only have $\langle b \rangle$ as the cyclic group. I cannot see any normal cyclic group of order 3 here. For the second part of the problem, suppose we accept the fact that $\Bbb{Q}_{4k} = \Bbb{Z}_k \rtimes_{\alpha} \Bbb{Z}_4$ for some $\alpha: \Bbb{Z}_4 \rightarrow Aut(\Bbb{Z}_k)$ . There is a theorem in the textbook that states: Corollary Let $\bar{m}$ have exponent k in $\Bbb{Z}^×_n$ , and let $α : \Bbb{Z}_k → \Bbb{Z}^×_n$ be the homomorphism that takes the generator to $\bar{m}$ . Then writing $\Bbb{Z}_n = \langle b \rangle$ , $\Bbb{Z}_k = \langle a \rangle$ , and identifying $\Bbb{Z}^×_n$ with $Aut(\Bbb{Z}_n)$ , we obtain $$\Bbb{Z}_n \rtimes_{α} \Bbb{Z}_k = \{b^ia^j | 0 ≤ i < n, 0 ≤ j < k\},$$ where b has order n, a has order k, and the multiplication is given by $$b^ia^jb^{i'}a^{j'} = b^{i+m^ji'}a^{j+j'}.$$ Moreover, the nk elements $\{b^ia^j | 0 \leq i < n, 0 \leq j < k\}.$ Since we know that $aba^{-1}=b^{-1}$ in quaternionics, we need to find an $\bar{m}$ such that $b^0aba^{-1}= b^{0+m}a^{-1+1} = b^{-1}$ . So $\bar{m} = \bar{-1}$ , and $\alpha$ must be send the generator to $\bar{-1}$ . I guess this is what the question is asking for when it tells us the calculate $\alpha$ ""explicitly"". Is that correct? However, I cannot say that until I prove that $\Bbb{Q}_{4k} \cong \Bbb{Z}_k \rtimes_{\alpha} \Bbb{Z}_{4}$ , so I was wondering if anybody could help me with that. Thank you in advance","['group-theory', 'abstract-algebra']"
400979,Having trouble understanding upper and lower limits.,"I am seriously having trouble understanding the meaning of upper and lower limits. Can someone give me easy-to-follow examples and explanations for the following? Def: Let $\{s_n\}$ be a sequence of real numbers. Let $E$ be the set of numbers $x$ such that $s_{n_k} \to x$ for some subsequence $\{s_{n_k}\}$ . This set $E$ contains all subsequential limits plus possibly the numbers $+\infty$ and $-\infty$ . Putting $s^* =\sup(E)$ and $s_* = \inf(E)$ , we call them the upper and the lower limits. We also use the notation $$\lim_{n \to \infty} \sup (s_n) = s^*$$ $$\lim_{n \to \infty} \inf (s_n) = s_*$$ This is my understanding. a), $\{s_n\}$ is a sequence, and its subsequence (there are $\infty$ many patterns) $\{s_{n_k}\}$ has many different limits. That's why it's possible to have a set $E$ which may contain more than one limit. b), The limit of $\{s_n\}$ is not necessarily the limit of all subsequence of $\{s_n\}$ . c), The ""largest"" number $x$ in $E$ is the ""upper limit."" The analogue of the lower limit would be the ""smallest"". Now this is what I'm getting confused. What does $\sup (s_n)$ mean? Why do we have to take the limit as $n \to \infty$ to get the supremum? Isn't $\sup (s_n)$ already the supremum of $E$ ? Another thing, can't there be a subsequence of $\{s_n\}$ that has a limit point greater than that of $\{s_n\}$ ? I may be asking some weird questions, but that's just because this idea still doesn't click with me. The reason I am asking this question in the first place is, because I thought I understood it but I couldn't uderstand the following. Consider the series $$ \frac{1}{2}+\frac{1}{3}+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{2^3}+\frac{1}{3^3} + \cdots $$ I was working on the ratio test and I miserably failed to understand the text. My claim is that $$a_n = \frac{3^n+2^n}{6^n}$$ so the ratio test would give me $$\lim_{n \to \infty} \frac{a_{n+1}}{a_n} = \lim_{n \to \infty} \frac{1}{6} \left({\frac{3+2(\frac{2}{3})^n}{1+(\frac{2}{3})^n}}\right)$$ But the book tells me that $$\lim_{n \to \infty} \inf {\frac{a_{n+1}}{a_n}} = \lim_{n \to \infty} (\frac{2}{3})^n$$ $$\lim_{n \to \infty} \sup{\frac{a_{n+1}}{a_n}} = \lim_{n \to \infty} (\frac{3}{2})^n$$ $$\lim_{n \to \infty} \inf{\sqrt[n]{a_n}} = \lim_{n \to \infty} \sqrt[2n]{\frac{1}{3^n}}$$ $$\lim_{n \to \infty} \sup {\sqrt[n]{a_n}} = \lim_{n \to \infty} \sqrt[2n]{\frac{1}{2^n}}$$ I've spent 4 hours trying to understand this but I have absolutely no idea how and why these numbers popped up. Especially the ""2n th"" roots for the root test. I am asking a lot, but it's really driving me mad and I need help :P","['sequences-and-series', 'limsup-and-liminf', 'calculus', 'limits']"
400980,Why is Cauchy's integral formula always written with the function as the subject?,"Scouring textbooks, lecture notes, Wikipedia, etc., I notice that the standard presentation of Cauchy's integral formula is $$f(w)=\frac1{2\pi i}\int_L\frac{f(z)}{z-w}\,\mathrm dz\tag1$$ rather than $$\int_L\frac{f(z)}{z-w}\,\mathrm dz=2\pi i f(w).\tag2$$ I'm new to complex analysis, and have so far directly utilised only form $(2)$ (that is, that equation as an integral formula ҂ ), which, to boot, is a little more compact and reads more intuitively than form $(1).$ Or is the canonical form $(1)$ actually more natural? Please shed light on its intuitive interpretation that I must be missing. ҂ A formula is typically a rule for expressing a subject in terms of some other variable(s), so the circle-area formula being canonically presented as $$r=\sqrt{\frac A\pi}$$ would similarly cause a double take: is this a quirky misnomer, is $r$ more fundamental than $A$ when discussing circles, does this square-root form lend itself better to typical applications of the equation, etc.?","['soft-question', 'integration', 'complex-analysis']"
400981,Non-ergodic measure,"Is there an easy way to see that if $\mu$ and $\nu$ are $T$-invariant measures on the same space $X$, and $\mu \neq \nu$, then $\frac{1}{2}(\mu+\nu)$ is NOT ergodic? I know that ergodic measures are the extreme points of the convex set of invariant measures, but the proof of this fact requires the Radon-Nikodym Theorem. I'm just wondering if the above case has an elementary proof. So far, I don't see it.","['probability-theory', 'measure-theory', 'ergodic-theory', 'dynamical-systems']"
401000,Need help to prove,"I got the result below during my research. $$1=\frac{1}{1+a_1}+\frac{a_1}{(1+a_1)(1+a_2)}+\frac{a_1a_2}{(1+a_1)(1+a_2)(1+a_3)}+\frac{a_1a_2a_3}{(1+a_1)(1+a_2)(1+a_3)(1+a_4)}+... \tag 1$$ $$1=\frac{1}{1+a_1}+\sum\limits_{k=1}^ \infty\frac{\prod\limits_{j=1}^{k}a_j}{\prod\limits_{j=1}^{k+1} (1+a_j)}  $$ $$1+a_1=1+\frac{a_1}{1+a_2}+\frac{a_1a_2}{(1+a_2)(1+a_3)}+\frac{a_1a_2a_3}{(1+a_2)(1+a_3)(1+a_4)}+...$$ $$a_1=\frac{a_1}{1+a_2}+\frac{a_1a_2}{(1+a_2)(1+a_3)}+\frac{a_1a_2a_3}{(1+a_2)(1+a_3)(1+a_4)}+...$$ We can get the same relation but without $a_1$
$$1=\frac{1}{1+a_2}+\frac{a_2}{(1+a_2)(1+a_3)}+\frac{a_2a_3}{(1+a_2)(1+a_3)(1+a_4)}+...$$ Examples: Example-1: 
$a_n=c$ $$1=\frac{1}{1+c}+\frac{c}{(1+c)^2}+\frac{c^2}{(1+c)^3}+\frac{c^3}{(1+c)^4}+...$$ $$1+c=1+\frac{c}{1+c}+\frac{c^2}{(1+c)^2}+\frac{c^3}{(1+c)^3}+...$$ We know that if $\frac{c}{(1+c)}<1$ then $$1+\frac{c}{1+c}+\frac{c^2}{(1+c)^2}+\frac{c^3}{(1+c)^3}+...=\frac{1}{1-\frac{c}{(1+c)}}=1+c$$ Example-2: 
$a_n=x^{2^{n-1}}$ $$1=\frac{1}{1+x}+\frac{x}{(1+x)(1+x^2)}+ \frac{x^3}{(1+x)(1+x^2)(1+x^4)}+ \frac{x^7}{(1+x)(1+x^2)(1+x^4)(1+x^8)}+...$$ $$1=\frac{1-x}{1-x^2}+\frac{x(1-x)}{1-x^4}+\frac{x^3(1-x)}{1-x^8} +\frac{x^7(1-x)}{1-x^{16}} +...$$ $$1=\frac{1-x}{1-x^2}+\frac{x(1-x)}{1-x^4}+\frac{x^3(1-x)}{1-x^8} +\frac{x^7(1-x)}{1-x^{16}} +...$$ $$\frac{x}{1-x}=\frac{x}{1-x^2}+\frac{x^2}{1-x^4}+\frac{x^4}{1-x^8} +\frac{x^8}{1-x^{16}} +...$$ If we put $x---->x^2$ $$\frac{x^2}{1-x^2}=\frac{x^2}{1-x^4}+\frac{x^4}{1-x^8} +\frac{x^8}{1-x^{16}} +....$$ $$\frac{x}{1-x}=\frac{x}{1-x^2}+\frac{x^2}{1-x^2}=\frac{x(1+x)}{1-x^2}$$ Example-3: 
$a_n=n$ $$1=\frac{1}{2}+\frac{1}{2.3}+\frac{1.2}{2.3.4}+\frac{1.2.3}{2.3.4.5}+... $$ $$1=\frac{1}{2}+\frac{1}{2.3}+\frac{1}{3.4}+\frac{1}{4.5}+... $$ $$\frac{1}{2}=\frac{1}{2}-\frac{1}{3}+\frac{1}{3}-\frac{1}{4}+\frac{1}{4}-\frac{1}{5}+... $$ I wonder which general conditions are required for $a_n$ that Equation $ 1 $ is true. Thanks for answers and comments.",['sequences-and-series']
401044,Degree of transitive constituents is odd implies $|G|$ is odd,"I want to prove that: the order of a permutation group $G \le S^\Omega$ is odd if and only if the degrees of all transitive constituents of $G$ and the degrees of all transitive constituents of each $G_\alpha (\alpha \in \Omega)$ are odd.  This is Exercise 3.13 in Wielandt's text. (Let me recall that if $\Delta \subseteq \Omega$ is a fixed block of $G$, then $G$ restricted to $\Delta$, denoted $G^\Delta$, is called a constituent.  It is transitive iff $\Delta$ is a minimal fixed block (i.e. an orbit).  So the assertion asks to show that $|G|$ is odd iff each orbit of $G$ and each orbit of each $G_\alpha$ has odd length.) The hint given is to use these three facts: (i) $|G_\alpha|~|\alpha^G|=|G|$. (ii) $|G:G_{\alpha \beta}| = |\alpha^G| |\beta^{G_\alpha}| = |\beta^G| |\alpha^{G_\beta}|$. (iii) If prime $p$ divides $|G|$, then $G$ contains an element whose cycle decomposition contains a $p$-cycle. The implication is proved using (ii): If $|\alpha^G|$ or $|\beta^{G_\alpha}|$ is even for any $\alpha,\beta$, then by (ii) $|G|$ is even.  How do I prove the converse?","['permutations', 'finite-groups', 'group-theory', 'abstract-algebra']"
401055,"How to calculate R-square from adjusted r-square, n, and p?",Let $\bar{R}^2$ denote the adjusted coefficient of determination. I have $\bar{R}^2 = 0.9199$ with 15 cases. Now I am trying to find $R^2$ given the results below. I found the formula for $R^2$ but did not understand it. How do you calculate $R^2$ from $\bar{R}^2$? $\bar{R}^2 = 1-\dfrac{(n-1)(1- R^2)}{n-p-1}$,['statistics']
401112,Polynomial differential equation,"I came across this problem in an old olympiad paper (Putnam?) Find all polynomials $p(x)$ with real coefficients satisfying the differential equation $7\dfrac{d }{dx } [xp(x)]=3p(x)+4p(x+1)$   $\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -\infty<x<\infty$ I didn't find any ""official"" solution on internet and it would be interesting to see your approaches on this one. My approach was the following , i don't know if i'm right. 
Consider the leading coefficient $a_n$ on both sides. Then we have $7na_n=7a_n$.
Suppose $a_n\neq 0$ then we get $n=1$. I plugged in the general solution $p(x)=a_0+a_1x$ in the differential equation and solved for $a_1$ wich turned out to be $0$ , contradicition. 
The other option is that $a_n=0$ but then $a_{n-1 }$ would be the leading coefficient of the polynomial wich would be again $0$ (by the same reasoning). Hence by induction the only non zero leading coefficient must be $a_0$. Could you tell me if this is ok? Thanks in advance.","['ordinary-differential-equations', 'contest-math', 'polynomials']"
401122,explaining the derivative of $x^x$,"You set the following exercise to your calculus class: Q1. Differentiate $y(x) = x^x$. A student submits the following solution: Let $g(a)=a^x$ and $f(x)=x$.  Then $y(x) = g(f(x))$, so by the chain rule, $y'(x) = f'(x) g'(f(x)) = 1 \cdot x \cdot (x^{x-1}) = x^x$. How would you explain to the student why their solution is incorrect? To be clear, I know why this is wrong but am interested in good ways to explain it to undergraduate or high school students. In this question someone has problems differentiating $x^x$, but they didn't take the approach of my hypothetical student.","['education', 'calculus', 'fake-proofs']"
401162,Where to find the proof for the isomorphism between the (2n-1)_th homotopy group of Aut(V) to Z?,"I am learning Bott-Seeley's comment article on the Callias' index theorem. They quoted a theorem, that is, $\pi_{2n-1}(Aut(V_{C}^{n}))\simeq Z$ or $\pi_{2n-1}(GL_n(C))\simeq Z$. Can you tell me the key to the proof for this theorem or direct me to some reference books? Thanks a lot!","['algebraic-geometry', 'algebraic-topology']"
401182,Can the orbits of a semigroup touch without one being included in the other?,"Question . Let $(S(t))_{t \ge 0}$ be a continuous semigroup of linear operators on some Banach space $X$. Might there exist $f, g\in X$ and $0<t_0<t_1$ such that 
  \begin{equation}S(t_0)f=S(t_1)g\end{equation}
  but \begin{equation} S(t_0-\varepsilon_0)f\ne S(t_1-\varepsilon_1)g \end{equation}
  for all $0<\varepsilon_0\le t_0$ and $0<\varepsilon_1\le t_1$ (in particular, $f\ne g$)? Pictorially, I am wondering if the following configuration is possible: Of course we know that, when the evolution is given by a group , this is not possible: orbits either coincide or are disjoint. This is the case of autonomous ODE systems or of the Schrödinger equation. But here we have a semi group, such as the heat one, which only goes forward in time, not backwards. So the only obvious thing that we can say is that, as soon as they touch, orbits merge into one. But in principle I don't see why they should coincide in the past . Added : After some searches, I have found that for the special case of the heat equation, the answer is negative . This is commonly referred to as backward uniqueness property . Here is a simplified version, which takes into account classical solutions on bounded domains: Theorem (Taken from Evans's book on PDE, 2nd ed., pag.64) Let $U\subset \mathbb{R}^n$ be an open and bounded domain. Suppose $u, \bar{u}$ are classical solutions of 
\begin{equation}
\begin{cases} u_t=\Delta u & \text{in }U\times(0, T) \\  u=0 &\text{on }\partial U \times [0, T] \end{cases}
\end{equation}
If at time $T$ we have 
\begin{equation}
u(x, T)=\bar{u}(x, T),\quad \forall x\in U,
\end{equation} 
then $u\equiv \bar{u}$ on the whole parabolic cylinder $U\times (0, T]$. The property holds in much more general functional settings, as I read here (look for the keyword backward uniqueness ). All of this leaves the general question open. Is the backward uniqueness property true for all continuous linear semigroups? I guess that the answer should be negative, otherwise this would not be regarded as a special feature of the heat equation. However, I cannot find an explicit example.","['functional-analysis', 'partial-differential-equations']"
401190,What does a probability of $1$ mean?,"From a textbook on probability on the Law of Large Numbers: Theorem 3-19 (Law of Large Numbers): Let $X_1,X_2, \ldots , X_n$ be mutually independent random variables (discrete or continuous), each having finite mean  and variance. Then
  if $S_n = X_1 + X_2 +\dots+ X_n$, $$ \lim_{n \to\infty} P\left(\left|\frac{S_n}{n} - \mu\right| \geq \varepsilon\right) = 0 $$ Since $S_n$ is the arithmetic mean of $X_1,X_2, \ldots , X_n$, this theorem states that the probability of the arithmetic mean $\frac{S_n}{n}$ differing from its expected value $\mu$ by more than $\varepsilon$ approaches zero as $n \to \infty$. A stronger result, which we might expect to be true, is that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $  but this is actually false. However, we can prove that $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $
with probability one. The only difference between the last sentence and the one before that is the phrase 'with probability one'. What does probability one mean here ? The usual definition is that the event occurs with 100% certainty. If that is the case, why is the original assertion $ \lim_{n \to\infty} \frac{S_n}{n} = \mu $ false ?","['law-of-large-numbers', 'probability']"
401200,zeros of exponential polynomials,"Let $\exp[n;z]$ denote the $n$th Taylor polynomial for the exponential function. In the 1920's Szegő initiated the study of the asymptotic properties of the zeros (rescaled by dividing by $n$) of this family of polynomials and one consequence of his results is that they can approach arbitrarily closely to the imaginary
axis.  This prompts the following question: Is it possible for $\exp[n;z]$ to have a root which lies precisely on the imaginary axis?","['complex-analysis', 'polynomials']"
401216,$\mathbb{Z}[i]$ is a Dedekind domain,"I know that $\mathbb{Z}[i]$ is a PID, and that every PID is a Dedekind. But I want to show that $\mathbb{Z}[i]$ is a Dedekind, without using PID. One strategy coul be to show that $\frac{\mathbb{Z}[i]}{\mathfrak{P}}$ is a field for every nonzero prime ideal $\mathfrak{P}$. Could someone suggest me how to prove this?
Furthermore, what else am I supposed to prove to ensure that $\mathbb{Z}[i]$ is a Dedekind?",['number-theory']
401227,Runge-Kutta method and step doubling,"I am studying Runge-Kutta and step size control and came up with a few doubts. Because they are related with this integration method, I will divide it in two parts. First, allow me to introduce the problem. $1^{st}$ part - questions about Runge-Kutta Method Consider a $2^{nd}$ order Runge-Kutta with general form: $k_{1}=hf(x_{n},y_{n})$ $k_{2}=hf(x_{n}+\frac{1}{2}h,y_{n}+\frac{1}{2}k_{1})$ $y_{n+1}=y_{n}+k_{2}+O(h^{3})$ where $f(x_{n},y_{n}) = y'(x_{n})$ Now, if we are to consider $4^{th}$ order Runge-Kutta we would get $k_{1}=hf(x_{n},y_{n})$ $k_{2}=hf(x_{n}+\frac{1}{2}h,y_{n}+\frac{1}{2}k_{1})$ $k_{3}=hf(x_{n}+\frac{1}{2}h,y_{n}+\frac{1}{2}k_{2})$ $k_{4}=hf(x_{n}+h,y_{n}+k_{3})$ $y_{n+1}=y_{n}+\frac{1}{6}k_{1}+\frac{1}{3}k_{2}+\frac{1}{3}k_{3}+\frac{1}{6}k_{4}+O(h^{5})$ This leads me to the following questions: I understand why the 2nd order method has the $y_{n+1}$ indicated above. Yet, shouldn't the $4^{th}$ order version of the method be expressed only as $y_{n+1}=y_{n}+k_{4}+O(h^{5})$ given that $k_{4}$ implicitly has the values of $k_{1}$ to $k_{3}$ ? Why does the $4^{th}$ order version have those fractional coefficients? $2^{nd}$ part - questions about step-size control Consider the exact solution for an advance from $x$ to $x+2h$ by $y(x+2h)$ and the two approximate solutions by $y_{1}$ (one step 2h) and $y_{2}$ (two steps each of size $h$ ). Considering the $4^{th}$ order method we have: $y(x+2h)=y_{1}+(2h)^{5}\phi+O(h^{6})+...$ $y(x+2h)=y_{1}+2(h)^{5}\phi+O(h^{6})+...$ The difference between these estimates permits estimating the truncation error: $\Delta=y_{2}-y_{1}$ Which we then use to improve the numerical estimate of the true solution: $y(x+2h)=y_{2}+\frac{\Delta}{15}+O(h^{6})$ This brings me to the following questions: In the two expressions of y(x+2h) at the top, does $\phi$ include the terms $k_{1}...k_{4}$ in the first part of this post? Why is the final expression $5^{th}$ order, if the original problem was $4^{th}$ order? Where does the coefficient $\frac{1}{15}$ come from? Thank you for all the insight!","['runge-kutta-methods', 'ordinary-differential-equations', 'numerical-methods']"
401236,Trace of a differential operator,"Given the differential operator:
$$A=\exp(-\beta H)$$
where $$H=\frac{1}{2}\left( -\frac{d^2}{dx^2}+x^2 \right)$$
and $\beta\gt 0$
How can I get the trace of this operator?
Thanks in advance.","['matrices', 'operator-theory']"
401248,Jacobson Radical and Finite Dimensional Algebra,"In general, it is usually not the case that for a ring $R$, the Jacobson radical of $R$ has to be equal to the intersection of the maximal ideals of $R$. However, what I do like to know is, if we are given a finite-dimensional algebra $A$ over the field $F$, is it true that the Jacobson radical of $A$ is precisely the intersection of the maximal ideals of $A$?","['ring-theory', 'abstract-algebra']"
401250,Measurable function of a transformation still measurable,"I'm looking for a maybe simpler or more elemental proof of the following statement: Let $f:\mathbb{R^n}\to \mathbb{R}$ be (Lebesgue-)measurable.
Then $F:\mathbb{R^{2n}}\to \mathbb{R}$ such that $F(x,y)=f(x-y)$ is (Lebesgue-)measurable. The proof I know works in the following way:
First show that $f$ is measurable on $\mathbb{R^{2n}}$.
Then $T(x,y)=(x-y)$ is Lipschitz and hence $f\circ T$ is measurable (by a theorem ). Can one somehow argue without the Lipschitz continuity in the last step or with a different ansatz? Several books I've seen introduce the convolution using that $f(x-y)$ is measurable but proof that Lipschitz $T$ maps measurable sets onto measurable sets later in the text.","['measure-theory', 'functional-analysis']"
401252,Existence of variation,"Let $I[w] =\int_U L(Dw,w,x) dx$. Let $1<q<\infty$, and there exist constants $\alpha>0$,$\beta\ge0$ such that $$L(p,z,x)\ge \alpha |p|^q - \beta$$ This implies that if $I[w]$ exists,
$$I[w] \ge \alpha \|Dw\|_{L^q}^q -\beta |U|$$ Now Evans says that for $w\in W^{1,q}(U)$ that $I[w]$ is defined but possibly infinite. What is the reason for this? Is it true always that functions which are bounded below are integrable (where integral can be $\infty$)?","['sobolev-spaces', 'measure-theory', 'calculus-of-variations']"
401263,How many types of surface singularities multiplicity two exist?,"All varieties are over $\mathbb{C}$. Let $S$ be a reduced algebraic surface in $\mathbb{P}^3$ with a singular point $p$ of multiplicity two. The question is local so we reduce to $S \subset \mathbb{A}^3$ and $p=0$. Then $S$ is given as the zero set of a polynomial $f = f_2 + f_3 \ldots$ where $f_i$ is homogeneous of degree $i$ and $f_2$ is nonzero (Hartshorne, exercise I.5.3). The question in the title is vague, so let me split it into parts: If the singularity $p$ is isolated, is it necessarily an ordinary quadratic one? (i.e. is it resolved by a single blowup operation, resulting in a rational -2 curve?) If not, what are examples of other possibilities? Is it true that $p$ is isolated if and only if its two tangent directions (the linear factors of $f_2$, see again Hartshorne exercise I.5.3) are different? These two questions are based on intuition and not much more, so i hope they are not totally ridiculous. When answering, any elaboration on the concept of multiplicity and tangent directions is highly appreciated! Thanks! Edit: After learning about Du Val singularities, i realize the first question is pretty stupid.. But then again, the question could be replaced by ""Are the Du Val singularities the only surface singularities of multiplicity two?""","['surfaces', 'algebraic-geometry', 'singularity-theory', 'complex-geometry']"
401264,Harmonic functions in $\mathbb{R}^d$,"I want to establish the equivalence of the 3 standard definitions, and that harmonic functions are $C^\infty$.  The 3 definitions are: Mean value property and continuous. $C^2$ and $0$ Laplacian. Mean value property on arbitrarily small balls and continuous. The only help I think I need here is proof of the existence and uniqueness of solution for the Dirichlet problem on the ball. (That would help me to get 3 implies 2, along with the maximal property for harmonic functions of type 3, which should be easy to prove.)  2 implies 1 by using arguments to take a derivative under the integral, and then using gauss' divergence theorem.  1 implies 3 obviously.  I also need help seeing that harmonic functions must be $C^\infty$.  I am not used to methods that don't involve complex analysis, as must be used here. I know that there is a theory of plurisubharmonic functions.  As a bonus question, do those tend to be useful outside of complex analysis, and are they ever discussed for odd dimension?  For example, I have never seen them discussed in harmonic analysis, nor do they seem to be useful in relation to Brownian motions, which is why I am learning the d-dimensional version of harmonic function theory now. Edit: Come to think of it, I'd also like some help proving the open mapping property and maximal properties for harmonic functions.  Please only assume definition 3 here, because I will use it and a connectedness argument to establish 3 implies 2.  The precise statement of definition 3 is as in Greene and Krantz but for d dimensions: $f$ is ""harmonic (3)"" if $f$ is continuous and $\forall x \in U$ the domain of $f$ there exists $\epsilon>0$ such that all balls of radius $\epsilon$ or less centered at $x$ are contained in U, and $f$ satisfies the MVP for that ball/spherical shell.","['harmonic-functions', 'partial-differential-equations', 'analysis']"
401266,Tensor product of a number field $K$ and the $p$-adic integers,"In the paper A database of local fields , J. Jones and D. Roberts  introduced an isomorphism $K \otimes \mathbb{Q}_p \cong \prod\limits_{i=1}^g K_{p,i}$ , where $K$ is some finite dimensional extension of $\mathbb{Q}$ and $\{K_{p,i} \}$ is some collection of finite dimensional extensions of $\mathbb{Q}_p$ . The isomorphism is not mentioned again, and it is not clear why this is even an isomorphism of modules (over what ring?), algebras (over what ring?), abelian groups etc. $K \otimes \mathbb{Q}_p$ is called an ""associated $p$ -adic algebra,"" perhaps suggesting that this tensor product may be interpreted not as a module over $\mathbb{Q}$ but as an algebra over $\mathbb{Q}_p$ .  But this doesn't make sense, since $K$ doesn't contain $\mathbb{Q}_p$ as a subfield. I wanted to ask whether anyone can shed some light on this isomorphism-what it is an isomorphism of , how the isomorphism is given, how the fields $K_{p,i}$ are chosen etc., as well as the significance of these notions.  From the introduction in the paper it only reads ""for investigating some problems about number fields, it suffices to know just basic invariants of the $K_{p,i}$ , such as ramification index and residual degree."" Any explanation of the above, or link to a reference in which the details are given, would be greatly appreciated.","['modules', 'p-adic-number-theory', 'algebraic-number-theory', 'number-theory']"
401274,Proof of the equality of the difference of two sets iff sets are equal (direct vs. indirect),"I have a problem with the following (really) basic result: $$A\backslash B=B\backslash A \Longleftrightarrow A=B$$ More specifically, I am able to prove it only by contradiction (in particular in the necessary condition - for the sufficient condition I get a contradiction following the steps of a direct proof). So my problem is, can we actually prove it without using contradiction? Moreover (and this is probably a very dumb question, so I apologize), is there a general way to know if a given result that we know is true can be proven only by contradiction or there exist a direct proof of it? Looking forward to any feedback!","['alternative-proof', 'elementary-set-theory']"
401308,How to compute $\int_0^\infty \frac{\sin t}{t^{s+1}} dt $?,"How to compute $\displaystyle\int_0^\infty \frac{\sin t}{t^{s+1}}\;\text dt$ ? Here, the real part of the complex number $s$ is negative and greater than $-1$.","['trigonometry', 'integration', 'complex-analysis']"
401313,"Prove that $A_1,B_1,C_1$ is collinear","Let $ABC$ be a triangle inscribled inside circle $(O)$ . M is a point inside the triangle $ABC$ ($M \notin BC,CA,AB$) $AM,BM,CM$ meets $(O)$ again at $A',B',C'$ respectively. Midperpendicular of $MA', MB', MC'$ meet $BC,CA,AB$ at $A_1,B_1,C_1$ respectively. 1.Prove that $A_1,B_1,C_1$ is collinear 2.$M'$ is symmetric to M through $A_1B_1C_1$. Prove that $M'\in (O)$ Please give me some hint Thanks",['geometry']
401323,Accumulation points of accumulation points of accumulation points,"Let $A'$ denote the set of accumulation points of $A$.
Find a subset $A$ of $\Bbb R^2$ such that $A, A', A'', A'''$ are all distinct. I can find a set $A$ such that $A$ and $A'$ are distinct, but not one where $A,A',A'',A'''$ are all distinct.",['general-topology']
401329,"Law of Large Numbers, a confusion","According to Law of Large Numbers, if I throw a coin 1000 times approximately 500 will be head and 500 tail. Suppose that I throw the coin 700 times and I got 700 heads. Can I say that in the next 300 throws the probability of getting tails will be higher than probability of getting heads? Edit: To make an analogy, imagine that you have a box that contains 500 black and 500 white balls. If 700 times (700 exaggerated) you choose black, than it is more probable that for 701 you get white. However if you say me that instead of one box you have 1000 similar box and every time you choose from one box that means the probability of choosing black or white will never change. With independent events you mean this? Edit2: Imagine that there are billions of people that throwing coins 1000 times. For each person there is an empty box. When he throw the coin, if it is tail he puts a black ball in his box, when the coin is head he puts white ball. So at the end of the experiment there are billions of boxes that each box contains approximately 500 black 500 white ball. So they give me the opportunity to choose one box. The box that I choose represents the one possible coin throwing that I would make. I am asking what is the difference between throwing coins and choosing one box in billions of boxes? If there is no difference, than the first statement holds. For example I pick 400 black balls from my box than it is more probably to choose white ball from remaining 600 hundreds.","['law-of-large-numbers', 'probability']"
401335,A set $E$ is closed if and only if $E = E^-$,"Let $E$ be a subset of a metric space $(S,d)$. I want to show that the set $E$ is closed if and only if $E = E^-$ where $E^-$ is the closure of a set $E$. First I assumed $E = E^-$. Then since I know $E^-$ is the intersection of all closed sets containing $E$, $E^-$ must be closed because the intersection of any closed sets is a closed set. So it follows that $E$ is closed. Now if $E$ is closed, then $S \setminus E$ is open. That also means $E^- \setminus E$ is open since $E \subseteq E^-$ by definition. Suppose $E \subsetneq E^-$. Let $x$ be on the boundary of $E^-$. Then there does not exist an $r>0$ such that $\{s \in S \ | \ d(s, x) < r \} \subseteq E^- \setminus E$. That means $E^- \setminus E$ is not open which gives a contradiction. So it must be the case that $E^- = E$. Could someone give me feedback on my proof? And is there a better/shorter proof?","['general-topology', 'real-analysis']"
401389,Understanding Bayes' Theorem,"I worked through some examples of Bayes' Theorem and now was reading the proof. Bayes' Theorem states the following: Suppose that the sample space S is partitioned into disjoint subsets $B_1, B_2,...,B_n$ .  That is, $S = B_1 \cup B_2 \cup \cdots \cup B_n$ , $\Pr(B_i) > 0$ $\forall i=1,2,...,n$ and $B_i \cap B_j = \varnothing$ $\forall i\ne j$ .  Then for an event A, $\Pr(B_j \mid A)=\cfrac{B_j \cap A}{\Pr(A)}=\cfrac{\Pr(B_j) \cdot \Pr(A \mid B_j)}{\sum\limits_{i=1}^{n}\Pr(B_i) \cdot \Pr(A \mid B_i)}\tag{1}$ The numerator is just from definition of conditional probability in multiplicative form. For the denominator, I read the following: $A= A \cap S= A \cap (B_1 \cup B_2 \cup \cdots \cup B_n)=(A \cap B_1) \cup (A\cap B_2) \cup \cdots \cup(A \cap B_n)\tag{2}$ Now this is what I don't understand: The sets $A \cup B_i$ are disjoint because the sets $B_1, B_2, ..., B_n$ form a partition. $\tag{$\clubsuit$}$ I don't see how that is inferred or why that is the case.  What does B forming a partition have anything to do with  it being disjoint with A.  Can someone please explain this conceptually or via an example? I worked one example where you had 3 coolers and in each cooler you had either root beer or soda.  So the first node would be which cooler you would choose and the second nodes would be whether you choose root beer or soda.  But I don't see why these would be disjoint.  If anything, I would say they weren't disjoint because each cooler contains both types of drinks. Thank you in advance! :)",['probability']
401414,How to solve this simultaneous equation of $3$ variables.,"I've stuck in this equation system. No clue how to start ? $$\begin{eqnarray}
x+y+z &=&a+b+c\tag{1} \\
ax+by+cz &=&a^{2}+b^{2}+c^{2}\tag{2} \\
ax^{2}+by^{2}+cz^{2} &=&a^{3}+b^{3}+c^{3}\tag{3}
\end{eqnarray}$$ Find the value of $x,y,z$ is in the form of $a,b$ and $c$.
I want to know steps of solution.","['algebra-precalculus', 'systems-of-equations']"
401422,Can this sum ever converge?,"If I have a strictly increasing sequence of positive integers, $n_1<n_2<\cdots$, can the following sum converge? $$ \sum_{i=1}^\infty \frac{1}{n_i} (n_{i+1}-n_{i}) $$ I suspect (and would like to prove) that it always diverges. Haven't made much progress so far, though. On a related note, is there any characterization of which subsequences of $1/n$ have a convergent sum?",['sequences-and-series']
401471,Proof that $dx/|x|$ is a Haar measure on non-zero reals?,"Most importantly, what is the meaning of this notation $\lambda = dx/|x|$?  How do I compute say $\lambda(0,1)$ for example?","['measure-theory', 'harmonic-analysis']"
401476,Prove: symmetric positive matrix multiplied by skew symmetric matrix equals 0,My teacher gave me this task as preparation for the exam but I'm stuck and not sure if it's true anymore.,"['matrices', 'linear-algebra']"
401480,What type of input does trigonometric functions take in,"I see in my Book that 45 deg is equivalent of π/4 . Ι also do the conversion if I simply convert degrees into radians like this 45* π/180 = π/4 radians and back again π/4 * 180/π = 45 deg .
So I think that I grasp the Idea finally that π/4 is another way of saying 45 degrees. Now if I use my calculator and I say sin(45) witch I assume that 45 is degrees I get the number 0.7071067812. If I do the calculation sqrt(2)/2 I will get the same number hence sin(45)=sqrt(2)/2 . But when I input sin(π/4) I get the number 0.0137073546. So I say to my self that probably that function does not get that type of input. But then again I see in my book that sin(π/4) = sqrt(2)/2 . So this forces me to ask the question, ""What type of input do trigonometric functions take in?"" Is the calculator not working properly ? Excuse me if this looks simple to you but I am so bad in math and the calculator is the only validator I have at the moment and this confuses me allot.","['convention', 'functions', 'trigonometry', 'calculator', 'number-systems']"
401486,"$V=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\}$ is isomorphic to $\text{span}\{AB-BA:A,B\in V\}$","Background: Let
$$V:=\{A\in M_{3\times 3}(\mathbb{R}):\text{trace}(A)=0\}$$
be the vector space of $3\times 3$ real matrices with vanishing trace, and let $[\cdot,\cdot]:V\times V\to V$ be defined by
$$[A,B]=AB-BA$$
Note that this is well defined since $\text{trace}(AB-BA)=\text{trace}(AB)-\text{trace}(AB)=0$ for any matrices $A,B$. Finally, let
$$\hat{V}:=\text{span}\{[A,B]:A,B\in V\}$$ Question: I have to give an isomorphism
$$\varphi:(V,[\cdot,\cdot])\to(\hat{V},[\cdot,\cdot]),$$
i.e. a bijective map such that $\varphi([A,B])=[\varphi(A),\varphi(B)]$, $\forall A,B\in V$. I am aware that this is related to Lie algebra; namely $V=\mathfrak{sl}(3,\mathbb{R})$. However, I did not study Lie algebra yet. I am suppose to be able to prove it without referring to any well-known theorem of Lie algebra (unless I prove it first). I used a very computational argument. I am wondering if My argument is correct. There is a more fundamental way to prove it, i.e. without having to tediously compute all possible $[X_i,X_j]$ from a given basis. Atempt: Let $$
X_1 = \left(
\begin{array}{ccc}
 1 & 0 & 0 \\
 0 & -1 & 0 \\
 0 & 0 & 0
\end{array}
\right),\quad
X_2 = \left(
\begin{array}{ccc}
 1 & 0 & 0 \\
 0 & 0 & 0 \\
 0 & 0 & -1
\end{array}
\right),\quad
X_3 = \left(
\begin{array}{ccc}
 0 & 1 & 0 \\
 0 & 0 & 0 \\
 0 & 0 & 0
\end{array}
\right),$$
$$\quad
X_4 = \left(
\begin{array}{ccc}
 0 & 0 & 1 \\
 0 & 0 & 0 \\
 0 & 0 & 0
\end{array}
\right),\quad
X_5 = \left(
\begin{array}{ccc}
 0 & 0 & 0 \\
 0 & 0 & 1 \\
 0 & 0 & 0
\end{array}
\right),\quad
X_6 = \left(
\begin{array}{ccc}
 0 & 0 & 0 \\
 1 & 0 & 0 \\
 0 & 0 & 0
\end{array}
\right),$$
$$\quad
X_7 = \left(
\begin{array}{ccc}
 0 & 0 & 0 \\
 0 & 0 & 0 \\
 1 & 0 & 0
\end{array}
\right),\quad
X_8 = \left(
\begin{array}{ccc}
 0 & 0 & 0 \\
 0 & 0 & 0 \\
 0 & 1 & 0
\end{array}
\right)
$$
Then, $\{X_1,\ldots,X_8\}$ forms a basis for $V$, and we find
$$[X_1,X_2]=0,\quad[X_1,X_3]=2X_3,\quad[X_1,X_4]=X_4,\quad[X_1,X_5]=-X_5$$
$$[X_1,X_6]=-2X_6,\quad[X_1,X_7]=-X_7,\quad[X_1,X_8]=X_8,\quad[X_2,X_3]=X_3$$
$$[X_2,X_4]=2X_4,\quad[X_2,X_5]=X_5,\quad[X_2,X_6]=-X_6,\quad[X_2,X_7]=-2X_7$$
$$[X_2,X_8]=-X_8,\quad[X_3,X_4]=0,\quad[X_3,X_5]=X_4,\quad[X_3,X_6]=X_1$$
$$[X_3,X_7]=-X_8,\quad[X_3,X_8]=0,\quad[X_4,X_5]=0,\quad[X_4,X_6]=-X_5$$
$$[X_4,X_7]=X_2,\quad[X_4,X_8]=X_3,\quad[X_5,X_6]=0,\quad[X_5,X_7]=X_6$$
$$[X_5,X_8]=X_2-X_1,\quad[X_6,X_7]=0,\quad[X_6,X_8]=-X_7,\quad[X_7,X_8]=0$$
Thus, we have
$$
\begin{array}{cccc}
X_1=[X_3,X_6],& X_2=[X_4,X_7],& X_3=[X_2,X_3],& X_4=[X_1,X_4] \\
X_5=[X_2,X_5],& X_6=[X_5,X_7],& X_7=[X_7,X_1],& X_8=[X_7,X_3]
\end{array}
$$
which shows that $V$ is a subspace of $\hat{V}$, and since $\hat{V}$ is clearly a subspace of $V$, then $V$ and $\hat{V}$ are in fact the same vector space. Thus, we can take $\varphi$ to be simply the inclusion map. Indeed, $\varphi$ is then clearly bijective and we have
$$\varphi([A,B])=[A,B]=[\varphi(A),\varphi(B)]$$","['linear-algebra', 'lie-algebras', 'abstract-algebra']"
401495,Find what values of $n$ give $\varphi(n) = 10$,"For what values of $n$ do we get $\varphi(n) = 10$? Here, $\varphi$ is the Euler Totient Function. Now, just by looking at it, I can see that this happens when $n = 11$. Also, my friend told me that it happens when $n = 22$, but both of these are lucky guesses, or educated guesses. I haven't actually worked it out, and I don't know if there are any more. How would I go about answering this question?",['number-theory']
401497,Prove that a given meromorphic function is rational,"I'm doing some exercises in complex analysis, and I've reached one I simply can't figure out on my own, which is why I'm hoping for some help. The exercise: We assume that $h:\Bbb C\to \Bbb C \cup \{\infty\}$ is meromorphic with finitely many poles $z_1,...z_n$ and assume that there exist $k\gt0$, $N\in\Bbb N$ and $R\gt0$ such that $|h(z)|\le k|z|^N$ for $|z|\gt R$. Prove that h is a rational function. What I've been thinking so far: The definition of a rational function is, that you have to be able to write it on the form $f(z)=p(z)/q(z)$, where $p,q\in\Bbb C[z], q\neq 0$. So I guess I have to show that my function $h$ can be written this way too? I have, however not yet been able to figure out a way to do this which makes sense. Earlier we've done an exercise, in which we've proven that if we let f be an entire function and we assume that $|f(z)|\le A+B|z|^n$ for $z\in\Bbb C$, where $A,B\ge0$ and $n\in\Bbb N$, then f is a polynomial of degree $\le n$. I've been thinking this might be useful, although a meromorphic function isn't entire. Thank you for your time.",['complex-analysis']
401506,Quaternion group associativity,"Consider the eight objects $\pm 1, \pm i, \pm j, \pm k$ with multiplication rules: $ij=k,jk=i,ki=j,ji=-k,kj=-i,ik=-j,i^2=j^2=k^2=-1$, where the minus signs behave as expected and $1$ and $-1$ multiply as expected. Show that these objects form a group containing exactly one involution. Well, it is easy to determine closure from the definition. $1$ is clearly the identity, and the inverses can also be determined $(i,-i),(j,-j),(k,-k)$, $-1$ with itself, and $1$ with itself. The only involution is $-1$. Is there an easy way to check associativity of this group? There are too many possible combinations $(ab)c=a(bc)$ to check directly. ($8^3$ possible combinations)","['quaternions', 'group-theory', 'abstract-algebra']"
401522,"Prove that $\exists a,g,h\in G\colon h=aga^{-1}, g\neq h ,gh=hg$ in a finite non-abelian group $G$.","Let $G$ be a finite and non-abelian group.  How do I prove the following statement? $$\exists a,g,h\in G \colon\quad h=aga^{-1},\ g\neq h ,\ gh=hg.$$ Thanks in advance.","['contest-math', 'finite-groups', 'group-theory', 'abstract-algebra']"
401528,Relating the Künneth Formula to the Leray-Hirsch Theorem,"I am reading through Bott & Tu's Differential Forms in Algebraic Topology , which very early on discusses the Künneth formula and the Leray-Hirsch theorem for smooth principal bundles. The proof of Künneth is given explicitly, while Leray-Hirsch is said to follow by the same kind of argument but by adding the appropriate twists (no pun intended!). It seems to me that the proofs Bott & Tu offer are independent of one another. Thus, in the presence of the Künneth formula, one might boldly restate Leray-Hirsch as If a principal bundle contains global cohomology classes which freely generate the cohomology of the fibres, the bundle ""looks"" trivial in cohomology. But this makes me wonder if I am putting Descartes before the horse (bad pun now intended); that is, perhaps Künneth is really just a simple corollary of Leray-Hirsch. Indeed, this would be the case if someone could prove something along the lines of the following: Conjecture: Any smooth trivial principal bundle $\pi: G \times B \to B$ admits global cohomology classes on $G \times B$ which freely generate $H^*(G)$. So here is where I'm not certain how to proceed. Triviality of the bundle guarantees we can actually define a projection onto the fibres: $\rho: G \times B \to G$, which I could then use to pullback cohomology classes from $G$ into the total space. Are these global?","['homological-algebra', 'algebraic-topology', 'differential-geometry']"
401537,Series for $\pi$ which correspond to apollonian gaskets or hyperbolic tilings of the unit disk,"Consider the two partitions of the unit disk in $\mathbb{R}^{2}$ , the first an Apollonian gasket and the second is the $\{7,3\}$ hyperbolic tiling: Since the unit disk has radius $1$ , both of these have area $\pi$ , and for each partition, there is a series representation of $\pi$ , so: Given an Apollonian gasket in the disk of unit radius, is there an straightforward way of getting the sequence of areas of disks which comprise it? Given a regular hyperbolic tiling, is there a straightfoward way of obtaining the Euclidean area of the (distorted) polygons which comprise it?","['geometry', 'hyperbolic-geometry', 'area', 'pi', 'circles']"
401539,Probability on product spaces,"I am having some trouble, more of an argument with someone else, about a simple question regarding product spaces. Let $X_1,X_2,\dots,X_n$ a set of independent and identically distributed random variables from a population $P\in\mathcal{P}$, where $\mathcal{P}$ is a family of probability measures (non explicitly parameterized). The generic random variable $X$ is a measurable function from a probability space $\left(\Omega,\mathcal{F},P'\right)$ to $(\mathbb{R},\mathcal{B}(\mathbb{R}))$. In order to build the random vector containing the elements of the sample, $\tilde{X}=[X_i]_{n\times 1}$, and be able to calculate probability measures of events like $\{X_1<X_2\}$ or $\{X_1=X_2\}$, I decided to build a product space $(\Omega^n=\Omega\times\Omega\dots\times\Omega,\sigma(\mathcal{F}^ n)=\sigma(\mathcal{F}\times\mathcal{F} \dots,\times\mathcal{F}),P=P'\times P'\times\dots\times P')$, and let the vector function $\tilde{X}$ go from this product space to $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$. Therefore, the events $\{X_1=X_2\}=\{(w_1,w_2,\dots,w_n)\in\Omega^n:X_1(\omega_1)=X_2(\omega_2)\}$
and $\{X_1<X_2\}=\{(w_1,w_2,\dots,w_n)\in\Omega^n:X_1(\omega_1)<X_2(\omega_2)\}$ have a clear meaning. However, the person I am having the argument with argues that $\tilde{X}$ goes from the original probability space $\left(\Omega,\mathcal{F},P'\right)$ to $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n))$, and If I keep doing it the way of product spaces it is a lot harder to calculate the probabilities I want. If my friend's argument is true, I am having trouble finding the meaning of, and calculating the probabilities of the events above. What am I missing? Best regards, JM","['probability-theory', 'probability']"
401561,every integers from 1 to 121 can be written as 5 powers of 3,"We have a two-pan balance and have 5 integer weights with which it is possible to weight
exactly all the weights integers from 1 to 121 Kg.The weights can be placed all on a plate but you can also put some in a dish and others with the goods to be weighted. It's asked to find the 5 weights that give us this possibility. It also asks you to prove that the group of five weights is the only one that solves the problem. Easily i found the 5 weights: 1 , 3 , 9 , 27 , 81 but i can' demonstrate that this group of five weights is the only one that solves the problem. Can you help me ? Thanks in advance !",['number-theory']
401571,Algebraic expression in its most simplified form,I am trying to simplify the algebraic expression: $$\bigg(x-\dfrac{4}{(x-3)}\bigg)\div \bigg(x+\dfrac{2+6x}{(x-3)}\bigg)$$ I am having trouble though. My current thoughts are: $$=\bigg(\dfrac{x}{1}-\dfrac{4}{(x-3)}\bigg)\div \bigg(\dfrac{x}{1}+\dfrac{2+6x}{(x-3)}\bigg)$$ $$=\bigg(\dfrac{x(x-3)}{1(x-3)}-\dfrac{4}{(x-3)}\bigg)\div \bigg(\dfrac{x(x-3)}{1(x-3)}+\dfrac{2+6x}{(x-3)}\bigg)$$ $$=\bigg(\dfrac{x(x-3)+(-4)}{(x-3)}\bigg)\div \bigg(\dfrac{x(x-3)+2+6x}{(x-3)}\bigg)$$ $$=\dfrac{x(x-3)+(-4)}{(x-3)}\times \dfrac{(x-3)}{x(x-3)+2+6x}$$ $$=\dfrac{x(x-3)+(-4)(x-3)}{(x-3)x(x-3)+2+6x} $$ $$\boxed{=\dfrac{-4(x-3)}{2(1+3x)} }$$ Which does not appear is not the answer. Am I close? Where exactly did I go wrong? I have tried this question multiple times. Edit: Figured it out! $\dfrac{x(x-3) - 4}{x(x - 3) + 2(1 + 3x)}\implies\dfrac{x^2-3x-4}{x^2+3x+2}\implies \dfrac{(x-4)(x+1)}{(x+2)(x+1)}$ $(x+1)$'s cancel leaving us with: $\boxed{\dfrac{x-4}{x+2}}$,"['factoring', 'rational-functions', 'algebra-precalculus']"
401592,Normal subgroup if conjugate subgroup is subset,"I find this explanation in Isaacs' Algebra : Lemma. Let $H\subseteq G$ be a subgroup. Then $H$ is a normal subgroup if $H^g\subseteq H$ for all $g\in G$ . The reader should be warned that this lemma does not state that $H^g=H$ whenever $H^g\subseteq H$ . Since the inner automorphism induced by the element $g$ is a bijection, it is certainly true that $|H^g|=|H|$ , and if $H$ is finite, this equality of orders together with the containment $H^g\subseteq H$ certainly does imply that $H^g=H$ . For infinite subgroups, however, this does not follow and is not generally true. How does the lemma not state that? If $H^g\subseteq H$ , then the lemma says that $H$ is a normal subgroup, which by definition means $H^g=H$ . What am I missing?","['group-theory', 'definition']"
401605,Proof that a linear transformation is continuous,"I got started recently on proofs about continuity and so on. So to start working with this on $n$-spaces I've selected to prove that every linear function $f: \mathbb{R}^n \to \mathbb{R}^m$ is continuous at every $a \in \mathbb{R}^n$. Since I'm just getting started with this kind of proof I just want to know if my proof is okay or if there's any inconsistency. My proof is as follows: Since $f$ is linear, we know that there's some $k\in \mathbb{R}$ such that $|f(x)|\leq k|x|$ for every $x\in \mathbb{R}^n$, in that case let $a\in \mathbb{R}^n$ and let $\varepsilon >0$. Consider $\delta = \varepsilon /k$ and suppose $|x-a|<\delta$, in that case we have: $$|f(x)-f(a)|=|f(x-a)|\leq k |x-a|<k \frac{\varepsilon}{k}=\varepsilon$$ And since $|x-a|<\delta$ implies $|f(x)-f(a)|<\varepsilon$ we have that $f$ is continuous at $a \in \mathbb{R}^n$. Since $a$ was arbitrary, $f$ is continous in $\mathbb{R}^n$. Is this proof fine? Or there was something I've missed on the way?","['continuity', 'proof-verification', 'real-analysis']"
401606,What is the difference between an implicit ordinary differential equation and a differential algebraic equation?,"I'm rather confused on this particular point. What is the difference between an implicit ordinary differential equation of the form: x' = f(x',x,t); and a differential algebraic equation of the form: f(x',x,t) = 0; I've read some documentation on them online, and while some places mention that implicit ODEs are a special class of DAEs, they're not very clear on the point. Also, is there any specific consideration that needs to be made in regards to numerical solutions of such problems?",['ordinary-differential-equations']
401616,find the number of injections/surjections,"I've been banging my head on this problem for some time now, and could really use help. Bear in mind I'm not very good at this sort of thing and am struggling to get by in class. Problem: Given $ \alpha : A \to B       \;\;\;\;\;   |A|=p \;\;\;\;|B|=q $ Find: Number of Injections: $A \to B$ Number of Surjections: $ A \to B$ Prove that ∃ a bijection $A \to B$ $\Rightarrow$ p = q find number of bijections: $A \to B$ I apologize for dropping such a big problem on you all, but I just can't get anywhere with this. Help? EDIT:
These answers are really helpful and I believe I've got down the number of Injections, but  we haven't gone through permutations or combinatorics in class so my understanding of it is really hamstrung. I'm unclear on the Stirling theorem as discussed here: http://www.ma.utexas.edu/users/kbi/COURSES/TERM/11S/325K/L17.pdf , specifically the part where inclusion-exclusion comes into play. I'm not sure how to apply that, since right now I'm letting p = 4 and q = 3, so I don't know where inclusion-exclusion would be used (if at all!) since I don't know how elements would be missing.","['elementary-set-theory', 'functions']"
401623,"In a matrix ring, no zero divisors may have an inverse","In a general ring with 1, a right (left) zero divisor cannot have a right (left) inverse. In a matrix ring over a field, a stronger condition is satisfied: a (right or left) zero divisor cannot have a (right or left) inverse, with the proof I know invoking the rank plus nullity theorem (which makes sense, because infinite dimensional linear transformations can have right-inverses and be right-zero divisors). Is this more or less a peculiarity of matrices, or am I missing a more general property that they (and perhaps other spaces) possess?","['ring-theory', 'linear-algebra']"
401630,How to evaluate $\lim_{x\to 0} (1+2x)^{1/x}$,"Good night guys! I'm having some trouble with this: $$\lim_{x\to 0} (1+2x)^{1/x}$$ I know that $\lim_{x\to\infty} (1 + 1/x)^x = e$
but I don't know if i should take $h=1/(2x)$ or $h=1/x$ Can someone please help me? Thanks!",['limits']
401637,Calculate the limit of two interrelated sequences?,"I'm given two sequences: $$a_{n+1}=\frac{1+a_n+a_nb_n}{b_n},b_{n+1}=\frac{1+b_n+a_nb_n}{a_n}$$ as well as an initial condition $a_1=1$, $b_1=2$, and am told to find: $\displaystyle \lim_{n\to\infty}{a_n}$. Given that I'm not even sure how to approach this problem, I tried anyway. I substituted $b_{n-1}$ for $b_n$ to begin the search for a pattern. This eventually reduced to: $$a_{n+1}=\frac{a_{n-1}(a_n+1)+a_n(1+b_{n-1}+a_{n-1}b_{n-1})}{1+b_{n-1}+a_{n-1}b_{n-1}}$$ Seeing no pattern, I did the same once more: $$a_{n+1}=\frac{a_{n-2}a_{n-1}(a_n+1)+a_n\left(a_{n-2}+(a_{n-1}+1)(1+b_{n-2}+a_{n-2}b_{n-2})\right)}{a_{n-2}+(a_{n-1}+1)(1+b_{n-2}+a_{n-2}b_{n-2})}$$ While this equation is atrocious, it actually reveals somewhat of a pattern. I can sort of see one emerging - though I'm unsure how I would actually express that. My goal here is generally to find a closed form for the $a_n$ equation, then take the limit of it. How should I approach this problem? I'm totally lost as is. Any pointers would be very much appreciated! Edit: While there is a way to prove that $\displaystyle\lim_{n\to\infty}{a_n}=5$ using $\displaystyle f(x)=\frac{1}{x-1}$, I'm still looking for a way to find the absolute form of the limit, $\displaystyle\frac{1+2a+ab}{b-a}$.","['sequences-and-series', 'limits']"
401642,Where does the function $f(x) = \frac{2x}{x - 7}$ have an increasing slope?,"Where does the function $f(x) = \frac{2x}{x - 7}$ have an increasing slope? $a. x \le 0, x > 7$ $b. x<7$ $c. x > 7$ $d. x \in \Bbb R, x \neq 7$ This question is from a test of mine in a pre-calculus course (so no calculus allowed in answering the question).I have no idea as to how to solve this problem. I can tell where the function is negative and where it is positive, but that's about it, and I'm fairly sure that's no use here. Any ideas?",['algebra-precalculus']
401643,"What is the actual geometric meaning of trigonometric operations such as adding cos,sine,tan","$$\sin(\pi/4)+\cos(\pi/4)=\frac{\sqrt{2}}{2}+\frac{\sqrt{2}}{2}= \frac{2\sqrt{2}}{2}=\sqrt{2}$$ Thinking of trig components (cosine, sine) that I used to produce the result using the mechanics of algebra, makes me wonder what is the geometric representation of 
  $$\sin(π/4)+\cos(π/4)$$ The sine function corresponds to the shadow projection on $y$-axis (opposite) and the cosine function to the shadow projection on $x$-axis (adjacent). At the previous operations I actually added those lines shadowed on the Cartesian axes. In other words, I added those sides of the triangle that form a $45-45-90$. What is the actual geometric meaning of trigonometric operations such as adding cosine, sine, tangent, etc., or subtracting them? Am I just adding those sides and lines in order to get one new line with length $$\sqrt{2}$$ Is that all?",['trigonometry']
401657,How do I show that the binary representation on the real line is uncountable?,How do I show that the binary representation on the real line is uncountable? Thanks.,['elementary-set-theory']
401672,"If $2^x=0$, find $x$.","If $2^x=0$, find $x$. Solution : I know range of $2^x$ function is $(0,\infty)$. So $2^x=0$ is not possible for any real value of $x$ Hence, equation is wrong. We can't find value of $x$. Am I right? Please help me. Can $x$ be in $[-\infty,\infty]$? i.e is $2^x=0$ possible for $x=-\infty$?",['algebra-precalculus']
401679,"Given a triangle with points in $\mathbb{R}^3$, find the coordinates of a point perpendicular to a side","Consider the triangle ABC in $\mathbb{R}^3$ formed by the point $A(3,2,1)$, $B(4,4,2)$, $C(6,1,0)$. Find the coordinates of the point $D$ on $BC$ such that $AD$ is perpendicular to $BC$. I believe this uses projections, but I can't seem to get started. I tried the projection of $AC$ onto $BD$ and $AB$ onto $BC$, but to no avail. Any help is loved! Thanks.",['matrices']
401696,Exponent p-value generated in Excel,"Excel gave me a p-value of 1.44909E-09 Notice is does not say .09 but 09 This is confusing me, I am trying to analyze my data but am stuck at this point. If it were E-9 it could be 1.44909/1000000000 ? The rest of the data I have looks like this (blood pressure data): The mean SBP for control is 116.525 and experimental 119.125. The mean DBP for control is 67.575 and experimental 80.05. The standard deviation SBP for control is 15.076 and experimental is 20.613. The standard deviation DBP for control is 8.003 and experimental 11.856. The p-value for the control data-set is 39%. The p-value for the experimental data-set is ???? (also, since both data sets were close in range, is the data not significant?)","['statistics', 'real-analysis', 'exponentiation', 'data-analysis', 'statistical-mechanics']"
401697,$A$ be a $2\times 2$ real matrix with trace $2$ and determinant $-3$,"$A$ be a $2\times 2$ real matrix  with trace $2$ and determinant $-3$, consider the linear map $T:M_2(\mathbb{R})\to M_2(\mathbb{R}):=B\to AB$ Then which of the following are true? $T$ is diagonalizable $T$ is invertible $2$ is an eigen value of $T$ $T(B)=B$ for some $0\ne B\in M_2(\mathbb{R})$ if $ A=\begin{pmatrix}a_{11}&a_{12}\\a_{21}&a_{22}\end{pmatrix}$ Then I have calculated that matrix of $T$ will be \begin{pmatrix}a_{11}&a_{12}&0&0\\a_{21}&a_{22}&0&0\\0&0&a_{11}&a_{12}\\0&0&a_{21}&a_{22}\end{pmatrix} so $T$ is invertible I can say as $\det T=(\det A)^2=9$, could any one help to find out others as true/false?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors', 'diagonalization']"
401702,Infinite series and its upper and lower limit.,"I am learning analysis on my own and I am puzzled with the following question. Consider the series $$\frac{1}{2}+\frac{1}{3}+\frac{1}{2^2}+\frac{1}{3^2}+\frac{1}{2^3}+\frac{1}{3^3}+\cdots$$ Indicate whether this series converges or diverges. The following is what I was able to get. First I noticed that $$a_n = \frac{1}{2^n}+\frac{1}{3^n}$$ would be a simple and nice form of this sequence, so according to the ratio test, $$\lim_{n \to \infty} \frac{1}{6} \left( \frac{3+2(\frac{2}{3})^n}{1+(\frac{2}{3})^n}\right) = \frac{1}{2}$$ therefore the series must converge. However, I later understood the concept of upper an lower limits a little, so taking $$ a_n =
\begin{cases}
\frac{1}{2^n},  & \text{if $n$ is odd} \\
\frac{1}{3^{n-1}}, & \text{if $n$ is even}  \\
\end{cases}$$ I figured out the following $$\lim_{n \to \infty} \frac{1}{2}\left(\frac{2}{3}\right)^n = 0$$ when $n$ is odd and $$\lim_{n \to \infty} \frac{1}{2}\left(\frac{3}{2}\right)^n = +\infty$$ when $n$ is even. Through this experience I realized and really understood how a subsequence of a sequence may have multiple limits, and the supremum and the infimum of those limits give us the upper and lower limits, respectively. But the book does something different than I did, and I would like to have the following questions answered. 1),$\quad$ From my understanding, the ratio test tells us that $\Sigma a_n$ diverges when the upper limit is greater than 1. Clearly one of the limits goes to $+\infty$, so why cannot we conclude that the series diverge ? 2),$\quad$  I learned that by choice of the subsequence, the limit may or mayn't be different. In the case that it is different, how do we know what is the supremum and infimum? It would be clear that if they were $\pm \infty$ but in my case, why is it guaranteed that there is no other subsequence that has a negative limit ? Is there an algebraic way to guarantee them ? Or do we always have to conceptually understand all of the subsequential limits. 3),$\quad$  The book says the ratio test is inconclusive, which turns me back to question 1), but it used the ratio test to determine that the series converges. According to my even-odd wise definition of $a_n$, the ""larger limit"" (I am purposefully not saying supremum because I don't know how it's guaranteed that it is the supremum) using the ratio test can be found as $$\lim_{n \to \infty} \sqrt[n]{a_n} = \lim_{n \to \infty} \sqrt[n]{\frac{1}{2^n}} = {1 \over 2}$$ But the book says $$\lim_{n \to \infty} \sqrt[n]{a_n} = \lim_{n \to \infty} \sqrt[2n]{\frac{1}{2^n}} = {1 \over \sqrt{2}}.$$ Where did the $2n$ come from ? Can someone help me out ? Edit:Thanks for helping me out everyone! I really appreciate it.","['limits', 'sequences-and-series', 'analysis']"
401715,Solubility From Row Echelon Form,"Here is the question I am attempting to solve Determine which values of $k$, if any, will give: a) A unique solution, b) No solution, c) Infinitely many solutions to the system of equations. $$\begin{align} x+y+kz& =2 \\ \\ 3x+4y+2z& =k \\ \\ 2x+3y-z& =1\end{align}$$ I transformed this into row-echelon form. I got. \begin{pmatrix}
1 & 1 & k &2 \\ 
0 & 1 & 2-3k &k-6 \\ 
0 & 0 & -3+k & 3-k
\end{pmatrix} The answers say for a unique solution $k \neq 3$ a) So my reasoning is that if $k$ is not equal to $3$.Then we obviously have a unique solution, because if $k=3$, we would have everything in row $3$ equal to $0$. Hence we need to introduce parameters to solve the equation thus making us have infinitely many solutions. b) No solutions occurs if we have in the last row $(0,0,0|\alpha)$, where $alpha \neq 0$. We can't have any values of k that can attain this result. c) See a). Is this the right reasoning?","['matrices', 'linear-algebra']"
401721,How do I factor this?,"How do I factor $p^2+8pq+16q^2-9r^2$? I know how to group the first two terms, but I dont know what to do with the other half. Can someone help me with this problem?","['factoring', 'algebra-precalculus']"
401728,Why does the z-ultrafilter Stone-Čech compactification construction have the universal property?,"For the notions of $z$-filter, prime $z$-filters and $z$-ultrafilters see A Prime $\mathcal P$-filter is contained in a unique $\mathcal P$-ultrafilter? Let $X$ be a Tychonoff space. Let $BX$ be the set of $z$-ultrafilters of $X$. For each zero-set $Z$, let $Z^*=\{p\in BX: Z\in p \}$, then the sets $Z^*$ form a base of closed sets for a topology on $BX$. Now consider $K$ a compact Hausdorff space and $f:X\rightarrow K$ a continuous function. If $p\in BX$, it is easy to see that $\mathcal{F}_p=\{Z\subseteq K: Z$ is a zero-set and $f^{-1}[Z]\in p\}$ is a prime $z$-ultrafilter, then using that $K$ is $T_3$ one can show that $F_p$ has a unique cluster point $q$, as prime $z$-filters are contained in unique $z$-ultrafilters in $T_3$ spaces, then define $\bar{f}(p)=q$. I want to show $\bar{f}$ is continuous. To do this, it is enough to prove that for any zero-set $B$ of $K$, $\bar{f}^{-1}[B]$ is closed in $BX$, since $K$ is $T_3$. I have shown that if $p\notin \bar{f}^{-1}[B]$, then $p\notin (f^{-1}[B])^*$, so I only need to show  ${(f^{-1}[B])^*}^c\cap \bar{f}^{-1}[B]=\emptyset$, that is, $\bar{f}^{-1}[B]\subseteq (f^{-1}[B])^*$, since then ${(f^{-1}[B])^*}^c$ would be an open set such that $p\in {(f^{-1}[B])^*}^c$ disjoint with $\bar{f}^{-1}[B]$. The problem is that I do not know how to prove that $\bar{f}^{-1}[B]\subseteq (f^{-1}[B])^*$. So, is this true?, and, is there an easier way to prove that $BX$ has the universal property with respect to compact Hausdorff spaces? Thanks","['general-topology', 'filters', 'compactness']"
401733,What does the discriminant of an algebraic number field mean intuitively?,"If $E/F$ is a finite extension of fields and $\alpha_1,\ldots, \alpha_n$ is a basis
of $E/F$, the discriminant of $\{\alpha_1,\ldots, \alpha_n\}$ is $$\det(\operatorname{Tr}_{E/F}(\alpha_i\alpha_j))$$ I want to know if there is any other interpetration of this quantity? What information is actually conveyed by this quantity?","['algebraic-number-theory', 'number-theory']"
401734,The value of a limit of a power series: $\lim\limits_{x\rightarrow +\infty} \sum_{k=1}^\infty (-1)^k \left(\frac{x}{k} \right)^k$,What is the answer to the following limit of a power series? $$\lim_{x\rightarrow +\infty} \sum_{k=1}^\infty (-1)^k \left(\frac{x}{k} \right)^k$$,"['power-series', 'sequences-and-series', 'limits']"
401744,Geometric Meaning of the Partial Derivative,"Could anyone explain the geometric meaning behind the partial derivative? I know that for a ""normal"" derivative, its geometric meaning is the slope of the tangent of a curve. From what I understand about the partial derivative, it is the slope of the tangent of a cross section of a function with two or more variables. If anyone could expand upon the definition and clarify, I would be grateful. Thank you.","['multivariable-calculus', 'partial-derivative']"
401750,"If $f\colon G\to H$ is a surjective homomorphism, then $|C_G(g)| \geq |C_H(f(g))|$","Let $G$ be finite, $f\colon G\to H$  be a surjective homomorphism (hence $H$ is finite) and $g \in G$. Prove the order of center of $g$ in $G$ is greater than or equal to the order of the center of $f(g)$ in $H$, i.e. $$|C_G(g)| \ge |C_H(f(g))|.$$ Attempts at solution. If $f$ is an isomorphism this is clear. So if $\ker f=K$ we can consider $G \to G/K \xrightarrow{f'} H$, where $f'$ is the induced map. So we can reduce this problem to the case where $f$ is a projection map. Also if $f(x)f(g) = f(g)f(x)$ then $f(x^{-1}g^{-1}xg) = e$. So if $h = f(y)$, $h \in C_H(f(g))$ iff $[h,g] \in K$. The center of $g$ is the set of fixed points of the conjugation map (by $g$). And the center of $h$ is the set of fixed points under conjugation by $h$. So maybe this can be done by considering how the orbits collapse mod a normal subgroup?","['finite-groups', 'group-theory', 'abstract-algebra']"
401754,Set of homomorphisms form a group?,"Given vector spaces $V, W$ over field $F$, the set of all linear maps $V \to W$ forms a vector space over $F$ under pointwise addition. Is there an analogue for groups?  Can the set of all homomorphisms from groups $G \to K$ be given a group structure?","['group-theory', 'abstract-algebra']"
401762,sum of the eigenvalues = trace($A$)?,"Is it true that for a square matrix $A$, all of whose eigenvalues exist in the base field, sum of the eigenvalues = trace($A$)? The result holds in all the matrices I've studied.",['linear-algebra']
401783,Cauchy filters in metric spaces,"Some terminology: Let $(X,d)$ be a metric space. A filter $\mathcal F \subseteq \mathcal P (X)$ is Cauchy if $\forall \epsilon >0 \exists x\in X: B_\epsilon(x)\in \mathcal F$. A filter $\mathcal F \subseteq \mathcal P (X)$ is round if $\forall F\in \mathcal F \exists \epsilon>0 \forall x\in X:B_\epsilon(x)\in \mathcal F \implies B_\epsilon(x)\subseteq F$. A Cauchy filter $\mathcal F$ is said to be minimal if no filter that is properly contained in it is Cauchy. I found an article that remarks, as a matter of fact, that round Cauchy filters and minimal Cauchy filters coincide. Showing that a round Cauchy filter must be minimal is straightforward. But I can't immediately see how to prove the other implication. So, how does one show that a minimal Cauchy fiter must be round? Also, any references for the standard constructions of metric theory convergence by using filters (I know it's not strictly required, as sequences suffice, but still, I'm curious) are welcome. Thanks.","['general-topology', 'filters', 'metric-spaces']"
401791,An alternate proof of Egorov's Theorem,"I came up with a proof of Egorov's Theorem which I can't find in my books or on the net, which makes me think it's wrong, which means I have a misunderstanding somewhere which needs to be hammered out. Egorov's Theorem : Let $(X,M,\mu)$ be a finite measure space and $f_n$ a sequence of measurable functions on $X$ that converges pointwise a.e. on $X$ to a function $f$ that is finite a.e. on $X$.  Then for each $\epsilon > 0$, there is a measurable subset $X_{\epsilon}$ of $X$ for which $f_n\rightarrow f$ uniformly on $X_{\epsilon}$ and $\mu(X\backslash X_{\epsilon})<\epsilon$ Proof: Let $\epsilon > 0$. So first I let $E_0$ be the set of points at which $f_n$ does not converge to $f$, union the set of points at which $f$ is infinite.  Then $\mu(E_0)=0$.  Now define $$A_N = \{x\in X\backslash E_0 : |f_N(x)-f(x)|<\epsilon\},$$ and set $E_k = \bigcap_{N=k}^{\infty}A_N$.  Then since the $A_N$ are measurable and $E_k$ is the countable intersection of measurable sets, then the $E_k$ are measurable.  Furthermore the $E_k$ are ascending, since as $k$ grows one is intersecting fewer and fewer of the $A_N$. Thus since $\mu(E_0)=0$ (to justify the first equality) and by the continuity of measure (to justify the third), we have: $$\mu(X)=\mu(X\backslash E_0)=\mu(\bigcup_{k=1}^{\infty}E_k)=\lim_{k\rightarrow\infty}\mu(E_k).$$ Where the second equality holds since for every $x\in X- E_0$ there exists some $K$ such that for all $n > K$, $|f_n(x)-f(x)| < \epsilon$ which is what it means for $x\in E_K$; the reverse inclusion being trivial. Therefore since $\lim_{k\rightarrow\infty}\mu(E_k)=\mu(X)$, we can find a $K$ such that $\mu(X\backslash E_K)=\mu(X)-\mu(E_K)<\epsilon$, with $f_n\rightarrow f$ uniformly on $E_K$. Is this correct?","['measure-theory', 'real-analysis']"
401845,Why the $\nabla f(x)$ in the direction orthogonal to $f(x)$?,Why the $\nabla f$ in the direction orthogonal to $f$? I can't understand it intuitively. Is it because of the definition of gradient?,"['intuition', 'calculus', 'derivatives', 'real-analysis']"
401849,trigonometric identity related to $ \sum_{n=1}^{\infty}\frac{\sin(n)\sin(n^{2})}{\sqrt{n}} $,"As homework I was given the following series to check for convergence: $ \displaystyle \sum_{n=1}^{\infty}\dfrac{\sin(n)\sin(n^{2})}{\sqrt{n}}  $ and the tip was ""use the appropriate identity"". I'm trying to use Dirichlet's test and show that it's the product of a null monotonic sequence and a bounded series, but I can't figure out which trig. identity is needed. Can anyone point me towards the right direction? Many thanks.","['sequences-and-series', 'calculus']"
401863,How to prove a generalized integral identity,"$$
\int_{0}^{\infty }\frac{t}{(e^{2\pi t}-1)(1+t^{2})}dt=-\frac{1}{4}+\frac{\gamma}{2}
$$
where $\gamma$ = Euler Gamma
$$
\int_{0}^{\infty }\frac{t}{( e^{2\pi t}-1)(1+t^{2}) ^{2}}dt=\frac{\pi^2}{24} -\frac{3}{8}
$$
$$
\int_{0}^{\infty }\frac{t}{(e^{2\pi t}-1)(
1+t^{2})^{3}}dt=\frac{\pi^2}{96} +\frac{\zeta(3)}{8} -\frac{7}{32} 
$$","['improper-integrals', 'closed-form', 'special-functions', 'integration', 'definite-integrals']"
401868,Quasicompact over affine scheme,"Let $X$ be a scheme and $f : X \rightarrow \mathrm{Spec}\, A$ a quasicompact morphism. Are there any easy conditions on $A$ under which we can say that $X$ is quasicompact? Quasicompact morphism means only that there is an affine cover $\cup_{i \in I} \mathrm{Spec}\, A_i$ where $f^{-1}(\mathrm{Spec}\, A_i)$ is quasicompact. It doesn't seem to be enough.",['algebraic-geometry']
401881,Topology on Integers such that set of all Primes is open,"In my topology homework we are asked to describe a topology on the Integers such that: set of all Primes is open. for each $x\in\mathbb Z$, the set $\{x\}$ is not open. $\forall x,y \in\mathbb Z$ distinct, there is an open $U\ni x$ and an open $V\ni y$ such that $U\cap V=\emptyset$ i was looking at Furstenberg's topology as in this proof:
For $m, b \in\mathbb Z$ with $m > 0$ define $N(m,b):=\{mx + b : x ∈ Z\}$, an arithmetic progression streching towards infinity in both directions. A set $U$ is open if either: $U = \emptyset$; or For each $b\in U$ there is an $m>0$ such that $N(m,b)\subseteq U$. but as I understand the set of Primes is not open in this topology. Now I'm not sure what should I do: is there a way to modify this topology to make set of Primes open or should I think of something completely different. Any hints are appreciated!
thanks!","['general-topology', 'prime-numbers']"
401912,Find the eigenvalues and eigenvectors of an integral operator,"I need to find the eigenvalues e eigenvectors of this integral. $$\int_{0}^{1} K(x,y)\phi (y)dy,$$ where $K(x,y)=x(1-y),\; 0 \le x\le y \le 1$ and $K(x,y)=y(1-x),$ $0\le y\le x \le 1$ I really need an explanation here so I can solve the rest of the exercises that I have here.","['linear-algebra', 'calculus', 'functional-analysis', 'integral-equations']"
401933,"when is the region bounded by a Jordan curve ""skinny""?","How can I formalize and prove the following intuition?: Picture a very skinny rectangle, one with base length 1 and sides length $\epsilon$.  Or imagine a very flattened ellipse.  The interiors of these objects are ""skinny"" (or we might say $\epsilon$-skinny)  in the sense that each point in the interior is very close (or within $\epsilon$) to a point on the boundary. This notion seems easy to formalize. Now think of the sides of the rectangle, or the boundary of the ellipse, as the image of a Jordan curve $f : S^1 \rightarrow \mathbb{R}^2$ . It's intuitively ""obvious"" (though not at all obvious) that what makes the bounded region skinny (in the sense of the above paragraph) is the fact that: For every $\theta \in S^1$ there exists $\theta ' \in S^1$ such that $f (\theta )$ is close to $f (\theta ' )$ but $\theta$ is not very close to $\theta '$. I have been thinking about the right way to formalize these ideas and I'm a bit stuck.  I'd like to formulate them in the $C^0$ setting, that is to say without assuming tangent vectors or any calculus-related things.   Do you have any thoughts?  Thanks! Also: This is motivated by the completely well-defined question here: why is an annulus close to it's boundary when it's boundary curves are close?","['general-topology', 'plane-curves', 'geometric-topology', 'low-dimensional-topology']"
401937,"How is ""n+n/2+n/4....1"" equal to ""2n-1"" using the formula for geometric series?","I never knew not having good knowledge of basic maths will be so crippling!! So please help me out this time. I'll be working on my maths from today on. I was discussing about complexity of an algorithm on StackOverflow and I was told that the series $n+n/2+n/4 + \dots + 1$ evaluates to $2n-1$ and I was linked to the following formula on Wikipedia: Even after trying hard, I regret to say I still don't get it how using this formula I can conclude that my series evaluates to $2n-1$ . Please help me out as I am sure it will take only a few seconds for you. https://stackoverflow.com/questions/16748454/complexity-for-nested-loops-diving-by-2?noredirect=1#comment24124682_16748454","['asymptotics', 'sequences-and-series', 'algorithms']"
401940,Is it true that bounded metric can never be induced by norm.,"Let $(X, d)$ be a metric space where, $d$ is metric on $X$. We know that metric space $X$ is called bounded if there exists some number $r$, such that $d(x,y) ≤ r$ for all $x$and $y$ in $X$. I want to know is it true that a bounded metric doesn't satisfy homogeneity condition of norm. And thus could we conclude that bounded metric $d$ defined on metric space $X$ can never be induced by any norm. Thanks","['general-topology', 'normed-spaces', 'metric-spaces']"
401969,Self-intersection number of a complex curve in complex projective space,"I'm currently trying to get a grip on actually calculating some differential-geometric definitions. I'm looking at the following map from $\mathbb{CP}^{1}$ to $\mathbb{CP}^2$ : $f([z_0,z_1])=[z_0^3,z_0 z_1^2,z_1^3]$ What I don't understand is how one would go about calculating the self-intersection number of this. It's not an immersion, so do I need to take an immersion in the same homology class first? And how do I actually calculate it - by perturbing the map a little and then counting transversal intersections with sign (this turned out to be very messy with my choices), or by actually finding the Poincaré dual (how would one go about that?) and integrating it? In case anyone is wondering, this problem arose while trying to understand the adjunction inequality for J-holomorphic curves. Thanks already for any help you can give!","['algebraic-geometry', 'symplectic-geometry', 'differential-geometry']"
401970,"Equation $f(x,y) f(y,z) = f(x,z)$","How to solve the functional equation  $f(x,y) f(y,z) = f(x,z)$?","['functions', 'functional-equations']"
401975,why is an annulus close to it's boundary when it's boundary curves are close?,"This is the motivating question for the rather vague question here: when is the region bounded by a Jordan curve ""skinny""? Suppose we are given two Jordan curves in the plane, one inside the other and each contained in an epsilon neighborhood of the other. How can we conclude that the annular region between the two curves is itself contained in the two epsilon neighborhoods of the original curves? Note: by epsilon neighborhood of a curve I mean the set of all points which lie within a distance epsilon from some point in the image of the curve. Follow up: Since the statement above is false (see Hagen's answer) we might ask whether it is true if we assume point-wise closeness of chosen parametrizations of the curves.  This is answered in the affirmative by user7...","['general-topology', 'plane-curves']"
402005,What is rigorous notation for functions?,"I have seen many ways to denote a function: $f(x)=x^2, y=x^2, f: x\mapsto x^2$ and so on. What is exact notation for functions? Please include lethal doses of rigor, set theory, and of course notational exactness. Note: I am very familiar with functions in general. I just know that a lot of mathematical literature abuses notation when it comes to functions.","['notation', 'functions']"
402008,Inductively prove that this sequence of integrals is bounded.,"EDIT: I have an attempted solution to this in a post below, it is very long, but still incomplete. EDIT:Alright, I've pretty much almost finished my solution, but my biggest problem is the 2nd integral, rightmost inequality. I cannot find a way to evaluate it and compare it to the n=1 case for the upper integral, which should give me what I need. EDIT: still stuck! EDIT: 10 days later, I still need help with this.. Can someone please help me with this? I'm not even sure where to go from here. Here is the problem: Prove that $$\frac{1}{2}<\int_{0}^{\frac{1}{2}}\frac{dx}{\sqrt{1-x^{2n}}}\le0.52359,$$ for any integer $n\ge1$, And that $$\frac{1}{2}<\int_{0}^{1}\frac{dx}{\sqrt{4-x+x^{3}}}\le0.52359$$ Now I can do this for the $n=1$ case, as obviously this simply integrates to $\arcsin{\frac{1}{2}}$ giving $\frac{\pi}{6}$, but I'm completely stumped on any doing any further for this question. All help very appreciated, thank you for your time.","['induction', 'inequality', 'integration', 'integral-inequality']"
