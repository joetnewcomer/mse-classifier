question_id,title,body,tags
2739130,Probability that $m$ balls will fall into first box,"Suppose we have $n$ balls that are randomly distributed into $N$ boxes. Find the probability that $m$ balls will fall into the first
box. Assume that all $N^m$ arrangements are equally likely. Attempt: First, we notice that for the first box, we have $n$ choices, and for the second box we also have $n$ choices, and so on. Thus, we have $n^N$ ways to place the balls into the boxes. Pick $m$ balls out of the total $n$ balls, that gives ${n \choose m}$ . In how many ways can these $m$ balls go into the first box? Well, in just ${1 \choose 1 }= 1 $ ways. Thus, $$ P = \frac{ {n \choose m } }{n^N } $$ Is this correct?","['balls-in-bins', 'probability']"
2739173,Description of the image of a function,"Let $f:A\to B$ be a map, then the map induced by $f$ is defined as $f^{\to}:\mathrm{P}(A)\to \mathrm{P}(B),\ X\mapsto f(X).$ Here $\mathrm{P}(A)$ denotes the power set of $A.$ We know that $f^{\to}$ satisfys the following propeties: Property: $\bf 1) $ $f^{\to}(X)=\emptyset\Longleftrightarrow\ X=\emptyset;$ $\bf 2)$ $f^{\to}(X\cup Y)=f^{\to}(X)\cup f^{\to}(Y);$ $\bf 3)$ $f^{\to}(X\cap Y)\subset f^{\to}(X)\cap f^{\to}(Y).$ One may wonder if the properties above could ensure that a function $F:\mathrm{P}(A)\to \mathrm{P}(B)$ is induced by some map $f:A\to B.$ It's very possibly that there could be some counterexamples. Question: $\bf 1)$ Given a function $F:\mathrm{P}(A)\to \mathrm{P}(B)$ satisfying the properties above, can we always find some map $f:A\to B$ such that $F=f^{\to}?$ $\bf 2)$ If not, can we strengthen the requirements of $F$ to guarantee that $F$ is induced by some map $f:A\to B?$ EDIT: Now I've found proper conditions to ensure that $F$ is induced by a map $f:A\to B:$ Conditions: $\bf 1)$ $F(\emptyset)=\emptyset;$ $\bf 2)$ If $(X_i)_{i\in I}$ is a family of sets in $\mathrm{P}(A),$ then $F\bigg(\displaystyle\bigcup_{i\in I}X_i\bigg)=\bigcup_{i\in I} f(X_i);$ $\bf 3)$ If $X\in \mathrm{P}(A)$ is a singleton, then $F(X)\in \mathrm{P}(B)$ is also a singelton. We have the following theroem: Theorem 1: If map $F:\mathrm{P}(A)\to \mathrm{P}(B)$ satisfies all three conditions above, then there exists an $\pmb{ unique}$ map $f:A\to B,$ such that $F=f^{\to}.$","['elementary-set-theory', 'functions']"
2739227,The square of the solution of the equation...,"The square of the solution of the equation $x\sqrt{7} + \sqrt{8 - 3\sqrt{7}} - \sqrt{8 + 3\sqrt{7}} = 0$ is equal to: ... $x\sqrt{7} + \sqrt{8 - 3\sqrt{7}} - \sqrt{8 + 3\sqrt{7}} = 0$ $\implies x\sqrt{7} = \sqrt{8 + 3\sqrt{7}} - \sqrt{8 - 3\sqrt{7}}$ $\implies 7x^2 = \left(8 + 3\sqrt{7}\right) + \left(8 - 3\sqrt{7}\right) - \underline{2\sqrt{\left(8+3\sqrt{7}\right)\cdot\left(8-3\sqrt{7}\right)}}$ $\implies 7x^2 = 16 - 2\cdot\sqrt{64-63} = 14$ $\implies x^2 = 2$ I know how to do all the steps up to the part where the underlined section comes into play, can someone please explain where does this come from. Thanks!",['algebra-precalculus']
2739242,Sinusoidal regression,"I am trying to do my homework on finding parameters with some data and I am kind of stuck. The problem is to find parameters $(T_m, T_0, t_0, \omega) \in \mathbb R \times \mathbb R \times \mathbb R_{+} \times \mathbb R_{+}$ such that $$T_i = T_m + T_0 \sin (\omega (t_i-t_0)) + \epsilon_i$$ for all the data point $(t_i, T_i)$. $\epsilon_i$ is a small noise. I tried to minimize the quadratic error but since the quadratic error function is not convex, the minimization is not guaranteed to be global. Can someone help me find a method to solve the problem?","['numerical-methods', 'statistics', 'trigonometry']"
2739249,Evaluate $\lim_{x\to 0} \frac {1-(\cos 2x)^3(\cos 5x)^5(\cos 7x)^7(\sec 4x)^9(\sec 6x) ^{11}}{x^2}$,"Evaluate $$\lim_{x\to 0} \frac {1-(\cos 2x)^3(\cos 5x)^5(\cos 7x)^7(\sec 4x)^9(\sec 6x) ^{11}}{x^2}$$ Now I can apply L'Hospital rule twice but believe it or not it would seriously be a very tedious task to do. I also tried writing $\cos 2x=\frac {e^{i2x}+e^{-i2x}}{2}$ and so on but couldn't continue due to large powers. Moreover I don't see any standard limits popping out.  The only sequence I could notice was in the powers of the trigonometric functions which follow the series $3,5,7,9,11$ Any hints would be appreciated","['calculus', 'limits']"
2739291,"Prove for every natural number $n$, with $n ≥ 7$, that $\frac{(2n−18)}{(n^2−8n+ 8)} < 1$. [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Using induction, Prove for  every  natural  number $n$, with $n ≥ 7$, that $\frac{(2n−18)}{(n^2−8n+ 8)} < 1$. I cannot get the original equation to match the k+1 equation, and im not sure what I am messing up. I would post my work but I would rather see it done out completely from scratch. Any help appreciated!","['induction', 'discrete-mathematics']"
2739345,Is it true: A real block diagonal matrix is diagonalizable if and only if each block is diagonalizable.,"We know this theorem and I know the proof for that. Theorem. Define $A \in M_{n}(\mathbb{C})$ such that
  $$ A =  \begin{bmatrix} A_{1} & & 0  \\ &  \ddots &      
\\ 0   & & A_{k} \end{bmatrix} $$ 
  where $A_{k} \in M_{n_{i}}$ are block matrices.
  Then, $A$ is diagonalizable if and only if each of $A_{k}$ is
  diagonalizable. I am curious to know whether this theorem holds for real matrices or not?","['matrices', 'matrix-decomposition', 'matrix-calculus', 'linear-algebra']"
2739369,Ant crawling between opposite corners on a cubical box,"Consider a cubical box with $1\,\mathrm{m}$ side-length which has one corner placed at $(0,0,0)$ and the opposite corner placed at $(1,1,1)$ . What is the shortest distance that an ant crawling from the point $(0,0,0)$ to the point $(1,1,1)$ must travel? $$\text{(a)}\;\sqrt{6}\qquad \text{(b)}\;\sqrt{5}\qquad \text{(c)}\;2\sqrt{3}\qquad \text{(d)}\;1+\sqrt{3} $$ If we think mathematically, then the ant could first crawl along the height of the box to travel a distance of $1\,\mathrm{m}$ , and then move  diagonally along the upper side/face of the box traveling $\sqrt{2}\,\mathrm{m}$ more. So the total distance it travels is $1+\sqrt{2}\,\mathrm{m}$ , but this answer doesn't match any of the options. I was confused  where to post the problem: either this site or Puzzling.SE . I found a similar problem there , but since this puzzle has some mathematical content I decided to post it here.","['puzzle', 'solid-geometry', 'geometry']"
2739382,I'm so confused with Null hypothesis and Alternative hypothesis in this solution,"So the question says ""A company that claims the average time a customer waits on hold is less than $5$ minutes. A sample of $35$ customers have an average wait time of $4.78$ minutes . Assume the population standard deviation for the wait time is $1.6$ minutes. Test the company's claim"" So to me the null hypothesis is the status quo therefore $H_0\lt 5$ whereas the alternative hypothesis $H_1$ should be $H_1\ge5$ However the official answer to this solution is completely opposite it's stated $H_0\ge5$ , $H_1\lt5$ What am I missing here?","['statistics', 'probability']"
2739383,"Find the limit of a series of fractions starting with $\frac{1}{2}, \, \frac{1/2}{3/4}, \, \frac{\frac{1}{2}/\frac{3}{4}}{\frac{5}{6}/\frac{7}{8}}$","Problem Let $a_{0}(n) = \frac{2n-1}{2n}$ and $a_{k+1}(n) = \frac{a_{k}(n)}{a_{k}(n+2^k)}$ for $k \geq 0.$ The first several terms in the series $a_k(1)$ for $k \geq 0$ are: $$\frac{1}{2}, \, \frac{1/2}{3/4}, \, \frac{\frac{1}{2}/\frac{3}{4}}{\frac{5}{6}/\frac{7}{8}}, \, \frac{\frac{1/2}{3/4}/\frac{5/6}{7/8}}{\frac{9/10}{11/12}/\frac{13/14}{15/16}}, \, \ldots$$ What limit do the values of these fractions approach? My idea I have calculated the series using recursion in C programming, and it turns out that for $k \geq 8$, the first several digits of $a_k(1)$ are $ 0.7071067811 \ldots,$ so I guess that the limit exists and would be $\frac{1}{\sqrt{2}}$.","['sequences-and-series', 'calculus', 'limits']"
2739400,Equivalence of tangential and normal stably almost complex structure,"Let $M$ be a smooth manifold. $M$ is said to be tangential stably almost complex if $TM \oplus \underline{\mathbb{R}}^k$ can be given a structure of a complex vector bundle, for some $k$. $M$ is said to be normal stably almost complex if $\nu \oplus \underline{\mathbb{R}}^k$ can be given a structure of a complex vector bundle, where $\nu$ is the normal bundle of some embedding of $M$ into $\mathbb{R}^N$, for some $k$. It is stated in a few places that tangential stably almost complex structures and normal stably almost complex structures are equivalent, see for example: http://www.map.mpim-bonn.mpg.de/Complex_bordism#Stably_complex_structures . However I cannot figure out why. In the above link, it is stated that this follows from $TM \oplus \nu = \underline{\mathbb{R}}^N$. So suppose I have a tangential stably almost complex structure, then $(TM \oplus \underline{\mathbb{R}}^k) \oplus \nu = \underline{\mathbb{R}}^{N+k}$, the first summand on the left has an almost complex structure and I can see that the right hand side can be given an almost complex structure as well. But then how does one proceed next? It seems to me that one needs some method to extend an almost complex structure on $\mathbb{R}^m$ (the first summand) to $\mathbb{R}^{N+k}$ on each fibre, so that we can restrict it back to $\nu$. But since the space of almost complex structures $O(2n)/U(n)$ has nontrivial topology, whether this can be done seems to depend on the topology of $M$. So I guess I might be on the wrong path here. Any help is appreciated!","['almost-complex', 'differential-geometry', 'differential-topology']"
2739426,A magic rectangle with the greatest size - original question,"I filled in a $3\times k$ rectangle with non negativ integers, such that the sum of the three numbers in each column is the same number $n$, and in each row all the numbers are different. Find the maximum value of $k$. If you try for $n=0,1,2,3,4,5,6$, you get $1,1,2,3,3,4,4$. It is a strange sequence, but I got an extra homework. So I need help! I am very thankful for every solution!",['discrete-mathematics']
2739470,Find probability that $x^2+y^2 \leq N^2$ where x and y are random positive integers less than N,"Find $\lim_{N \to \infty} P(x^2+y^2 \leq N^2) $ where $x$ and $y$ are random positive integers less than N. I want to calculate it using the law of total probability: $\lim_{N \to \infty} P(x^2+y^2 \leq N^2) =  \lim_{N \to \infty} \sum_{i=1}^N\frac1N P(y^2 \leq N^2-i^2) =\lim_{N \to \infty}  \sum_{i=1}^N  \frac{\sqrt{N^2-i^2}}{N^2}$. Here I am stuck, because I do not understand how to calculate this sum.","['probability-theory', 'probability']"
2739493,S.N. Bernstein Law of Large Numbers,"while reading a paper named ""Network Embedding as Matrix Factorization: UnifyingDeepWalk, LINE, PTE, and node2vec"" ( http://keg.cs.tsinghua.edu.cn/jietang/publications/WSDM18-Qiu-et-al-NetMF-network-embedding.pdf ), the authors give in the Appendix a lemma which is stated as follows: (S.N. Bernstein Law of Large Numbers) Let $Y_1,Y_2,\ldots$ be a sequence of random variables with finite expectation $E(Y_j)<\infty$ (which, according to my understanding, is invariant for different $Y_i$), and uniformly bounded variance $Var(Y_j)<K<\infty$, $j\geq 1$, and covariances are s.t. 
$$ Cov(Y_i, Y_j)\rightarrow 0, |i-j|\rightarrow\infty $$
Then the law of large number holds. However, I failed to find a proof, or a similar statement of Bernstein's Law of Large Numbers after I tried to google it. The authors cited for this lemma Problems in Probabilities by Albert N. Shiryaev, which I believe is an exercise book. The interesting fact is that the statement does not assume that random variables are independent. Would anyone tell me where I can find the source of the theorem and the proof? 
Thanks! I hope this question is appropriate; if not, please tell me and I'll remove it.","['law-of-large-numbers', 'probability-theory', 'probability']"
2739500,kernel of $1$-form defines a distribution,"Consider two vector fields $F_1,F_2$ of $\mathbb{R}^3$ and define the distribution $$D(q) =\operatorname{span}\left\{F_1(q),F_2(q)\right\}=\left\{a(q)F_1(q)+b(q)F_2(q),\; a,b \in \mathcal{C}^\infty\right\}$$ Suppose $q=(x,y,z)$ such that 
$$
\det(F_1(q),F_2(q),[F_1,F_2](q))=0.
$$ Why is there $\alpha$ such that $D$ is isomorphic to $\ker \; \alpha$ and
$\alpha = y\, dx +dz$ ?","['differential-forms', 'vector-fields', 'differential-geometry', 'determinant']"
2739517,Ergodicity implies finite first moment in discrete time Markov chains?,"Suppose that $X(n)=(X_1(n),X_2(n))\in\mathbb{N}^2$ is a discrete time homogeneous Markov chain with uniformly bounded jumps (see below).
Assume that the chain is ergodic and let $\pi$ be its invariant probability measure. If $Y$ is random variable distributed as $\pi$, is it true that $\mathbb{E}[Y]<\infty$? In other words, I'm interested in understanding whether ergodicity and uniformly bounded jumps ensure a finite first moment. Uniformly bounded jumps . I mean that there exists some finite $c>0$ such that the transition $x\mapsto x+(\Delta_1,\Delta_2)$ can occur only if $-c\le \Delta_1,\Delta_2<c$, for all $x$.","['markov-chains', 'probability-theory', 'markov-process', 'probability']"
2739592,"Do there exist pairs of distinct real numbers whose arithmetic, geometric and harmonic means are all integers?","I self-realized an interesting property today that all numbers $(a,b)$ belonging to the infinite set $$\{(a,b): a=(2l+1)^2, b=(2k+1)^2;\ l,k \in N;\ l,k\geq1\}$$ have their AM and GM both integers. Now I wonder if there exist distinct real numbers $(a,b)$ such that their arithmetic mean, geometric mean and harmonic mean (AM, GM, HM) all three are integers. Also, I wonder if a stronger result for $(a,b)$ both being integers exists. I tried proving it, but I did not find it easy. For the AM, it is easy to assume a real $a$ and an AM $m_1$ such that the second real $b$ equals $2m_1-a$. For the GM, we get a condition that $m_2=\sqrt{(2m_1-a)a}$. If $m_2$ is an integer, then… what? I am not sure exactly how we can restrict the possible values of $a$ and $m_1$ in this manner.","['algebra-precalculus', 'means']"
2739597,Find $\frac{\tan 54-\tan 2}{\frac{\sin 2}{\cos 6}+\frac{\sin 6}{\cos 18}+\frac{\sin 18}{\cos 54}}$,Find $$S=\frac{\tan 54-\tan 2}{\frac{\sin 2}{\cos 6}+\frac{\sin 6}{\cos 18}+\frac{\sin 18}{\cos 54}}$$ All the angles are in degrees. My Try: I tried converting everything to $\sin$ and $\cos$  we get $$S=\frac{\sin 52}{\cos 2 \cos 54} \times \frac{\cos 6 \cos 18 \cos 54}{\sin 2 \cos 18 \cos 54+\sin 6 \cos 6 \cos 54+\sin 18 \cos 18 \cos 6}$$ any clue here?,"['algebra-precalculus', 'trigonometry', 'geometry']"
2739609,Proof similarity of complex power series,"Let be the functions $$
f(x)=\sqrt{2}\frac{\cos(\frac{\pi}{4}-\frac{1}{2}\arctan(x))}{(1+x^2)^{1/4}} \qquad g(x)=\frac{1}{\sqrt{1-x}}
$$ Obviously, they are holomorphic functions in a neighborhood of $0$. The series of $g(z)$ is well known and using Mathematica we have $$
f(x)= 1+\frac{x}{2}- \frac{3x^2}{8}-\frac{5x^3}{16}+\frac{35x^4}{128}+\frac{63x^5}{256}-\frac{231x^6}{1024}-\frac{429x^7}{2048}+\dotsc
$$
$$
g(x)= 1+\frac{x}{2}+ \frac{3x^2}{8}+\frac{5x^3}{16}+\frac{35x^4}{128}+\frac{63x^5}{256}+\frac{231x^6}{1024}+\frac{429x^7}{2048}+\dotsc
$$ Exercise . Proof the absolute value of the coefficients of the two series are the same. Attemp . With trigonometric manipulation I get
$$
f(x)=\frac{1+x+\sqrt{1+x^2}}{\sqrt{2}(1+x^2)^{3/4}\sqrt{1+\frac{1}{\sqrt{1+x^2}}}}
$$
My idea is use the Cauchy Integral Theorem to calculate the coefficients 
$$
a_n = \frac{1}{2\pi i}\int_C \frac{f(z)}{z^{n+1}}dz
$$
and with changes of variables get something that seems to
$$
\frac{1}{2\pi i}\int_C \frac{g(z)}{z^{n+1}}dz
$$ 
But if I work in $\mathbb{C}$ I need to define $\log(1+z^2)$ and work with it in changes of variables doesn't look so friendly. Can you give me a hint to prove it?","['complex-analysis', 'complex-numbers', 'power-series']"
2739650,Relations between distance function and gradient on a Riemannian manifold,"An exercise on my textbook, is to show that on a Riemannian manifold $M$, the distance $d(x,y)$ is equal to $\sup \{f(x)-f(y):f \in C^{\infty}(M)$, with $\|\nabla f\| \leq 1\}$. Using $\langle\nabla f,X\rangle=X(f)$, it's easy to show that the left hand side is larger than the right one (i.e. $d(x,y) \geq sup\{...\}$). But I cannot found a simple solution to fix the other side. I thought it could be done by approximationg $f(y)=d(x,y)$, which is a Lip-$1$ function, by a family of smooth function, but it may be too hard for a undergraduate textbook exercise. Or maybe we should consider find a $f$, whose gradient along a curve is just the curve's tangent vector field? Could you please give me some hint? Great thx!","['riemannian-geometry', 'differential-geometry']"
2739667,Show that $\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert $,"Please give me a hint for the following question. The Problem: (a) Let $D\in M_{nn}(\mathbb{R})$ be a diagonal matrix and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ be the linear operator associated with $D,$ $Tx=Dx$ for all $x\in\mathbb{R}^{n}.$ Show that 
  $$\Vert T\Vert=\max\limits_{1\leq i\leq n}\vert d_{i}\vert $$
  where $d_{1}, d_{2}, \cdots, d_{n}$ are the entries on the diagonal of $D$. 
   For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $ For this part of the question I have proved that  $\Vert T\Vert\leq\max\limits_{1\leq i\leq n}\vert d_{i}\vert $. (b) Suppose $A\in M_{nn}(\mathbb{R}^{n})$ is symetric (i.e.,$A^{T}=A$) and $T:\mathbb{R}^{n}\to\mathbb{R}^{n}$ is the linear operator associated with $A$. Show that 
  $$\Vert T\Vert=\max\{ \vert \lambda\vert; \lambda\; is\; the\; eigenvalue\; of\; A\}. $$","['real-analysis', 'operator-theory', 'functional-analysis', 'linear-transformations', 'linear-algebra']"
2739670,What is $\lim_{x\to1^{-}} (1-x)\left(\sum_{i=0}^{\infty} x^{i^2}\right)^{2}$?,"This is the problem I was presented with:
$$\text{Given } f(x) = \sum_{i=0}^{\infty} x^{i^2} ,$$
$$\text{find } \lim_{x\to1^{-}} (1-x)f(x)^2 .$$
At first, I tried figuring out a generating function for $f(x)$, squaring it, and multiplying it by $(1-x)$. I then learned that $f(x)$ does not have a traditional generating function connected to it. I then tried dividing $f(x)^2$ by $(1+x+x^2+x^3+x^4+x^5+\cdots)$, the expanded form of $\frac{1}{1-x}$. I couldn't figure out how to make that work out either. Is there some way I should simplify $f(x)$ to make this an easier limit to solve?","['power-series', 'sequences-and-series', 'calculus', 'limits']"
2739704,Integral of $\sin(x)\exp(-a/\sin x)$,"I'm looking for a way of evaluating $$\int_0^\pi\sin x \exp(-a/\sin x)dx$$ to get a second order Bickley function $K_2(a)$, which is basically the same  integral, but $\cos x$ instead of $\sin x$ and the limits change from $0$ to $\pi/2$, which is understandable. I'm a bit lost what kind of variable substitution could I do. Any suggestions? 
I have tried $-a/\sin x = u$, but that doesnt seem to give reasonable results. 
Thanks in advance!","['bessel-functions', 'integration', 'ordinary-differential-equations']"
2739747,"$\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy$?","Let $D:=[a,+\infty)\times[c,+\infty) $,where $a,c \in \mathbb{R};$  and let $f:D \mapsto \mathbb{R} $ be a nonnegative continuous function. Prove that if the existence of either of the iterated integrals $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy$ and  $\int_{c}^{+\infty}dy\int_{a}^{+\infty}f(x,y)dx$ ,then the improper double integral $\iint_{D}f(x,y)dxdy$ converges to the value of the iterated integral in question. WLOG,suppose $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I(\in\mathbb{R}).$
Given any $b,d\in\mathbb{R}$  with $b>a,d>c$,by Fubini's theorem,$$\iint_{R}f(x,y)dxdy=\int_{a}^{b}dx\int_{c}^{d}f(x,y)dy,R=[a,b]\times[c,d].$$ Let $\varphi(x,\beta ):=\int_{c}^{\beta}f(x,y)dy,$ then for each fixed $\beta_{0}(>c)$ and $\alpha^{""}>\alpha^{'}>a,$ we obtian $$\int_{a}^{\alpha^{""}}\varphi(x,\beta_{0} )dx\geq \int_{a}^{\alpha^{'}}\varphi(x,\beta_{0} )dx.$$
Since $\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=I,$
$\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ with respect to $\alpha$ has upper unbound on $[a,+\infty).$ So $\lim_{\alpha \rightarrow +\infty}\int_{a}^{\alpha}\varphi(x,\beta_{0} )dx$ exists and is a real number . Next we have $$\iint_{[a,+\infty)\times[c,\beta_{0} ]}f(x,y)dxdy=\int_{a}^{+\infty}\varphi(x,\beta_{0} )dx.$$ At this time, If we  can prove  $$\lim_{\beta \rightarrow +\infty}\int_{a}^{+\infty}\varphi(x,\beta_{} )dx=\int_{a}^{+\infty}\left (  \lim_{\beta \rightarrow +\infty}\varphi(x,\beta_{} )\right )dx,\quad (*)$$then $$\int_{a}^{+\infty}dx\int_{c}^{+\infty}f(x,y)dy=\iint_{D}f(x,y)dxdy=I$$ But until now,I have no idea to prove $(*)$ is true.I need some help to deal 
with it ,or better solutions in other ways .","['real-analysis', 'riemann-integration', 'multivariable-calculus', 'integration', 'improper-integrals']"
2739787,Consistency of an unusual estimator for a binomial parameter,"I have a random sample $X_1, \ldots, X_n$ from a $\operatorname{Bi}(N,p)$ population. How do I show that the estimator $$\hat{p} = \frac{\bar{X}}{\max(X_1,\ldots,X_n)}$$
is consistent? The normal way I would approach such a question is to take the expectation and variance of the estimator and use that $\operatorname{MSE}(\hat{p})=\operatorname{Var}(\hat{p}) + \operatorname{Bias}(\hat{p})^2$, but for a ratio of random variables this would be incredibly messy. Is there an easier alternative?","['statistics', 'estimation', 'parameter-estimation']"
2739808,Point of intersections of perpendiculars from a vertex of a triangle to the angle bisectors are collinear,"Prove that the feet of the four perpendiculars dropped from a vertex of a triangle upon the four angle bisectors of the other two angles are collinear. Drawing a diagram, it is clear that $AFBE$ and $AGCD$ are rectangles. Also $DE \parallel BC \parallel FG$. However I can't prove that $DE$ and $FG$ are the same line. Any hints?","['euclidean-geometry', 'triangles', 'geometry']"
2739810,Prescriptive version of counting hyperplane arrangements,"In Hyperplane arrangement theory, Zaslavsky's Theorem necessarily bounds 
the number of bounded and unbounded regions in the complement of a real hyperplane arrangement. While this counting theorem is great, is there some thing more prescriptive which tells us which elements of the intersection lattice of the halfspaces resulting from the original arrangement are empty specifically? Given some affine arrangement $A$ and the exact normal and offset of the planes in $A$, and a corresponding halfspace intersection lattice $L$, if I know some subset of $L$ is non-empty then is there an algorithm or some algebraic formulation, which prescribes another subset of $L$ which is empty? As an example, if I have some affine arrangement in $\mathbb{R}^1$ of two hyperplanes (points in this case)  $A,B$, then I know that either $A_l \cap B_r = \emptyset$ or $A_r \cap B_l = \emptyset$ where $A_r$ denotes the halfspace to the right of $A$ and $A_l$ notes that to the left.","['algebraic-geometry', 'combinatorial-geometry', 'geometry', 'algebraic-topology', 'algebraic-combinatorics']"
2739820,Singular Value Decomposition in terms of Change of Basis?,"I do see how SVD can be understood in terms of rotating. But it is hard for me to understand SVD in terms of change of basis. So, let me start from Diagonalization to clarify my question. When $B=\{v_1, \dots, v_n\}$ where $v_i$ are eigenvectors, $A=PDP^T$. Then $PDP^Tx$ means that $x$ becomes $[x]_B$ (that is, $P^Tx$) and scaling operates on $[x]_B$ (that is, D) and then the result is transformed into the original coordinate (that is, P). On the other hand, understanding SVD in terms of change of basis is really hard for me. Let me say that $A=U\Sigma V^T$. Then, given $U\Sigma V^Tx$, $V^T$ transforms x into the coordinate system consisting of column vectors of V. Scaling operates in that coordinate system ($\Sigma$). But after that, how can one transform the vector under coordinate system governed by V into the original coordinate system? $U$ is different from $V$! Thank you!",['linear-algebra']
2739835,Strange Limit in Proof of the Fresnel Integral,"I was playing around with the Fresnel integrals and I've come up with a proof for the fact $$\int_0^\infty \cos x^2 dx= \int_0^\infty \sin x^2 dx= \sqrt{\frac{\pi}{8}}$$ The proof goes as follows Use the fact that $$\int_{-\infty}^{\infty} e^{-u^2} du = \sqrt{\pi} $$
Letting $ u = \sqrt{-i} \cdot x, du = \sqrt{-i}*dx $
$$\sqrt{-i} \cdot \int_{-\infty}^{\infty} e^{ix^2} du = \sqrt{\pi}$$
$$\int_{-\infty}^{\infty} e^{ix^2} dx = \int_{-\infty}^{\infty} \cos{x^2} + i \cdot \int_{-\infty}^{\infty} \sin{x^2}dx = \sqrt{\frac{-\pi}{i}}=(1+i)\sqrt{\frac{\pi}{2}}$$ Equating real and imaginary parts (we can do this because we can assume that the two integrals have real values), we get $$\int_{-\infty}^\infty \cos x^2 dx= \int_{-\infty}^\infty \sin x^2 dx= \sqrt{\frac{\pi}{2}}$$ Then, using the fact that $\cos{x^2}$ and $\sin{x^2}$ are even functions, we get the fact above. My question stems from the substitution made. After the substitution is made, the bounds of the integral change from $\pm\infty$ to $\pm(1+i)\infty$ Is this integral valid? Is it possible to have a complex number in the bounds of the integral?","['improper-integrals', 'integration', 'calculus', 'proof-verification']"
2739861,Uniqueness of Solution to a Boundary Value Problem,"Question Let $f:\mathbb R_+ \to \mathbb R_+$ be a function twice continuously differentiable (with derivative $f'$ and second derivative $f''$), and $a$, and $b$ be parameters in $\mathbb R_+$. Consider the system \begin{align}
\dfrac{dx(t)}{dt}=&\; f\left(x(t)\right)-y(t), \\[2ex]
\dfrac{dy(t)}{dt}=&\; ay(t)\left(f'(x(t))-b\right),
\end{align} with $x(t)\ge 0$ and $y(t)\ge 0$ for all $t$, and  boundary conditions \begin{equation}
x(0)= x_0, \qquad\text{and}\qquad\lim_{t\to\infty}e^{-bt}x(t)y(t)^{-a}=0.
\end{equation} By choosing $f$ appropriately it is possible to show that this system, without the initial condition, can have multiple stationary points. I would like to show, that even if that is the case, the following conjecture it true: Conjecture $\;$ Given an $x_0$, there is a unique solution to the system that converges to one of the stationary points. Why I think the conjecture is true If $f'(0)>0$ and $f''(x)<0$ for all $x\in\mathbb R_+$, then there exists a unique stationary point and there is a simple proof of uniqueness of the solution given an arbitrary $x_0$ that involves drawing a phase diagram in the space $(x,y)$ and showing that there is a unique saddle path that the solution must be at all times (otherwise the second boundary condition would be violated) and that converges to the stationary point. The reason why I think the conjecture is true is because, given an $f$ in $\mathcal C^2$, a similar phase diagram can be drawn. Here is a sketch of an example of a phase diagram with multiple stationary points: Notice that the arrows flip between regions in a way that, given an $x_0$, there is only one saddle path (the lines in blue) that the solution could follow. My guess is that a proof for the conjecture would involve showing this is always the case. Background If $x$ denotes the capital stock, $y$ the consumption level, $f$ is the production function, $a$ the inverse of the intertemporal elasticity of substitution, and $b$ is the discount rate, then this system is describes the equilibrium allocation of a simple economic growth model.","['boundary-value-problem', 'ordinary-differential-equations']"
2739886,$S$ is closed under pairwise unions $⇒$ $S$ is closed under arbitrary unions?,"Let $X$ be a set and $S$ be a collection of subsets of $X$, such that given any $U,V\in S$, $U\cup V\in S$. Intuitively it seems like this should imply that arbitrary unions are also in $S$. That is, given index set $I$ and $\{U_i\}_{i\in I}\subseteq S$, $\bigcup_{i\in I}U_i\in S$. Is this the case?","['general-topology', 'elementary-set-theory']"
2739906,Integrate $\pi (r^2-x^2)$,We are told to integrate $\pi (r^2-x^2)$ from $-r$ to $r$ $$V=\int_{-r}^{r}\pi(r^2-x^2)dx=2\pi\int_0^r(r^2-x^2)dx=2\pi\left[r^2x-\frac{x^3}{3}\right]_0^r=2\pi\left(r^3-\frac{r^3}{3}\right)=\frac{4}{3}\pi r^3$$ why does the integral of $r^2$ equal $r^2 x$?,"['multivariable-calculus', 'definite-integrals', 'calculus']"
2739915,Radius of inscribed sphere of n-simplex.,"I want to calculate the radius of  inscribed sphere of $n$-simplex, where the side length of $n$-simplex is 1. For example, when $n=2$, the 2-simplex is  equilateral triangle with side length is 1. And the radius of inscribed sphere is $\frac{1}{2\sqrt 5}$. But when $n\ge 4$, I don't know how to calculate it.","['combinatorial-geometry', 'geometry']"
2739932,Symmetric Grothendieck inequality,"Grothendieck's inequality states that for all $n \times n$ matrices $(a_{ij})$ such that $$\max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij}\, x_i\, y_j\right| \leq 1,$$ there exists a universal constant $K$ , such that for $u_i, v_j$ in any Hilbert space, $$
\max_{x \in \{\pm 1\}^n,\, y \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \langle u_i , v_j \rangle \right| \leq K.
$$ I would like to prove the symmetric statement.  For all symmetric matrices $(a_{ij})$ such that $$
\max_{x \in \{\pm 1\}^n} \left|\sum_{ij} a_{ij} \,x_i\, x_j\right| \leq 1, 
$$ there exists a universal constant such that $$
\max_{x \in \{\pm 1\}^n} \left|\sum_{i,j} a_{ij} \langle u_i , v_j \rangle \right| \leq 2K
$$ for $u_i, v_j$ in any Hilbert space.  This should be a consequence of the original inequality.  I tried to use the polarization identity $$
\langle Ax, y\rangle = \langle Au, u\rangle - \langle Av, v \rangle
$$ where $u = (x+y)/2$ and $ v = (x-y)/2$ .  However, as $x$ and $y$ vary over $\pm 1$ vectors, $u$ and $v$ can be vectors in $\{\pm 1, 0\}$ .","['matrices', 'inequality', 'optimization']"
2739966,"A ""cracked glass"" Riemann Sum - Double integrals","For a 1-dimensional domain, you can partition a segment $(a,b)$ into uneven intervals, choose sample points in the uneven intervals, and form an integral in the limit that the uneven intervals go to $0$. For a 2-dimensional domain, it would be easiest to partition such a domain into squares. However, to be as crazy as possible, could you break the domain down into a union of circles, parallelograms, triangles, and trapezoids? From this crazy partition, would a double integral be attainable and would it be equal to the double integral obtained by ""normal"" partitions? Now lets focus on a single fragment in this ""cracked glass"" domain and a function $f(x,y)$ above this domain. Let the fragment be a circular fragment. Does the volume above this single circular fragment $$(f(\text{sample point}))\; (\text{area of circle fragment})$$ in the limit that the fragment goes to zero, equal the same value if the fragment was a parallelogram that went to zero [probably ""yes"" as both numbers by themselves are $0$. Maybe 1 tends to zero faster than the other, I don't know. But if we consider two different ""cracked glass"" patterns in which the only difference is one fragment is a circle while the other is a parallelogram, does the total Riemann sum remain the same]?",['multivariable-calculus']
2740027,A counterexample of reducible Markov chains about Birkhoff's ergodic theorem,"In R.Durrett's boos Probability Theory and Examples(5th). There is an example 6.2.4(CH6.2 P301), ""Let $X_{n}$ be an $\textbf{irreducible}$ Markov chain on a countable state space that has a stationary distribution $\pi$..., and applying the ergodic theorem, 
  $$ \frac{1}{n} \sum_{m=0}^{n-1} f(X_{m}) \longrightarrow \sum_{x}f(x)\pi(x).""$$ I want to know if there exists a counterexample removing the condition ""irreducible"", in other words, how about the reducible Markov chain? In this counterexample, the Birkhoff's ergodic theorem does not hold.","['stochastic-processes', 'probability-theory', 'markov-chains', 'probability', 'ergodic-theory']"
2740058,Help Looking for a Function with Particular Properties,"I'm sorry to bother everyone, but I've been searching for functions that satisfy several properties, and so far, I've yet to be able to think of any! Specifically, the properties needed are: $f(0)=0$ and $f(1)=1$ And on the interval $[0,1]$: $f(x)$ is differentiable (though this could be relaxed a bit, the smoother, the better) $f(x) \geq x$ $f(x)-x$ Is maximized as close to $x=1$ as possible. (Again, only between [0,1]. So, for example, for $f(x)=\sqrt{x}$, the argmax of $f(x)-x$ would be at $x=.25$ While I can find several with this value being $\leq .5$, I'm having a lot of trouble finding nice examples where this falls above $.25$!) Where $f(x)$ really only needs to be defined over the unit interval. I'm sorry if these properties aren't clear- just let me know and I'll try to explain better/be more specific if desired! Thank you so much for your responses!!! EDIT 1: Wow, you guys are astoundingly quick!! I'm so, so sorry, but I realized I (incredibly stupidly) left out two crucial properties: $f(x)$ is weakly increasing on $[0,1]$ $0 \leq f(x) \leq 1$ (Though this can be done artificially by rescaling, so I don't think this should be an issue) Again, I'm so sorry for forgetting to enter them into the original problem (I was trying to figure out the numbered list format, deleted the old copy, and forgot to add it back!!)","['recreational-mathematics', 'functions']"
2740072,Is this proof of the Law of Total Probability correct?,"I start with the Law: $$\operatorname{P}(X_1 \le x_1) = \operatorname{E} \bigl( \operatorname{P}(X_1 \le x_1 \vert X_2) \bigr)$$ And I express the expectation as: $$\begin{align}
\operatorname{E} \bigl( \operatorname{P}(X_1 \le x_1 \vert X_2) \bigr) = & \int_{-\infty}^{\infty} \operatorname{P}(X_1 \le x_1 \vert X_2 = x_2)\;f_2(x_2)\;dx_2 \\
& = \int_{-\infty}^{\infty} \Biggl(\int_{t \le x_1} \frac{f_{X_1,X_2}(t,x_2)}{f_{X_2}(x_2)} \; dt \Biggr)\;f_{X_2}(x_2)\;dx_2 \\
& = \int_{-\infty}^{\infty} \Biggl(\int_{-\infty}^{x_1} \frac{f_{X_1,X_2}(t,x_2)}{f_{X_2}(x_2)} \; dt\Biggr)\;f_{X_2}(x_2)\;dx_2 \\
& = \int_{-\infty}^{\infty} \int_{-\infty}^{x_1} \frac{f_{X_1,X_2}(t,x_2)}{f_{X_2}(x_2)}\;f_{X_2}(x_2) \; dt \; dx_2 \\
& = \int_{-\infty}^{\infty} \int_{-\infty}^{x_1} f_{X_1,X_2}(t,x_2) \; dt \; dx_2 \\
& = \int_{-\infty}^{x_1} \int_{-\infty}^{\infty} f_{X_1,X_2}(t,x_2) \; dx_2 \; dt \\
& = \int_{-\infty}^{x_1} f_{1}(t) \; dt \\
& = \operatorname{F}_{X_1}(x_1) = \text{P}(X_1 \le x_1)
\end{align}$$ I am especially not sure about changing the integrals around, as in a previous course I was told that the integral with the variable in the bounds had to be integrated first.","['probability-theory', 'probability-distributions', 'multivariable-calculus', 'integration', 'probability']"
2740099,Constant random variable,How do I plot the cumulative distribution function and probability mass function of the constant random variable $X(\omega)=2$ for all $\omega$?,"['probability-distributions', 'statistics', 'constants', 'probability', 'random-variables']"
2740112,Different p-adic topologies on $\mathbb{Q} $ are homeomorphic,"Endow $\mathbb{Q}$ (the rationals) with the p-adic (resp. q-adic) topology, where p,q are primes. Are these topological spaces homeomorphic? I know the norms are not equivalent, therefore do not induce the same topology.
What I am asking is if the two distinct topologies are homeomorphic to one another. The reason I am asking is that I know that $\mathbb{Q}_p$ and $\mathbb{Q}_q$ are homeomorphic to each other.","['general-topology', 'p-adic-number-theory']"
2740124,Which data define an $SU(3)$ structure?,"Let $(z_1,\overline{z}_1,\dots, z_n,\overline{z}_n)$ be the coordinates on $\mathbb{R}^{2n}$ given by the identification $\mathbb{R}^{2n} \simeq \mathbb{C}^n$.
Define
$$
g =\left| dz_1 \right|^2+ \dots + \left| dz_n \right|^2 \\
\omega = \frac{i}{2} (dz_1 \wedge d \overline{z}_1+ \dots + dz_n \wedge d \overline{z}_n) \\
\gamma=dz_1 \wedge \dots \wedge dz_m
$$
The subgroup of $GL(2n)$ that stabilizes $g$ and $\omega$ is $U(n)$ (""2-out-of-3-property"").
The subgroup of $GL(2n)$ that additionally stabilizes $\gamma$ is $SU(n)$. In one source an $SU(3)$ structure on a manifold $M$ is given by specifying a $2$-form $\omega$ and a $3$-form $\gamma$ so that in any point $p \in M$ there exists an element in the frame bundle $L \in GL(M)_p$ that pulls back $\omega$ and $\gamma$ to the forms above. Question:
  A priori, this should not be an $SU(3)$-structure, because the stabilizer of the forms $\omega$ and $\gamma$ from above is bigger than $SU(3)$.
  Is this correct? Is it maybe assumed that the manifold is a Riemannian manifold, and that $L \in GL(M)_p$ also pulls back the Riemannian metric to the $g$ defined above?
I can see that this would then be an $SU(3)$-structure.
Or is this a specialty of (complex) dimension 3, that here the stabilizer of $\omega$ and $\gamma$ is already equal to $SU(3)$?","['differential-geometry', 'linear-algebra']"
2740195,I want to verify my proofs.,"1).Prove: $\lim_{n \to \infty} \frac{3n^2-4}{n^2-4} = 3$. 
A proof: there exists an $\epsilon>0$. We define $N=\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil$ (notice $N>2$). So: 
$$
\begin{aligned}
|\frac{3n^2-4}{n^2-4}-3| < \epsilon \Leftrightarrow |\frac{3n^2-4-3n^2+12}{n^2-4}|<\epsilon \Leftrightarrow |\frac{8}{n^2-4}| < \epsilon  
\Leftrightarrow  \frac{8}{n^2-4}<\epsilon \Leftrightarrow\\ 8<n^2\epsilon-4\epsilon \Leftrightarrow \frac{8+4\epsilon}{\epsilon}<n^2 \Leftrightarrow  \sqrt{\frac{8}{\epsilon}+4}<n. 
\end{aligned}
$$ 
This statement is true for all $n>N$,
because: $$\lceil{\sqrt{\frac{8}{\epsilon}+4}}\rceil \geq \sqrt{\frac{8}{\epsilon}+4}.$$
 thus, the limit is 3. $\Box$ 2).$a_nb_n=1$. Prove or contradict: If $\lim_{n\to\infty}|a_n|=1$ so $\lim_{n\to\infty}|b_n|=1$.
This statement is true.
A proof: We choose $\epsilon = \frac{1}{2}$, so:
$$
-\epsilon < a_nb_n-1 < \epsilon\Rightarrow -\epsilon+1<a_nb_n<\epsilon+1 \Rightarrow -\frac{1}{2}+1<a_nb_n<\frac{1}{2}+1 
\Rightarrow \\ \frac{1}{2}<a_nb_n<1\frac{1}{2} 
$$
Also:
$$
||a_n|-1|<\epsilon \Rightarrow \frac{1}{2} <|a_n| <1\frac{1}{2} 
$$
It's obvious that $a_n\neq 0$ for almost all n, otherwise $\lim_{n\to\infty}a_n=0$.
If $a_n>0$ for almost all n, also $b_n>0$ for almost all n. we can divide and get:
$$
1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{a_n}<1 \Rightarrow 1<b_n<1
$$
If $a_n<0$ for almost all n, also $b_n<0$ for almost all n. we can divide and get:
$$
1<\frac{a_nb_n}{|a_n|}<1 \Rightarrow1<\frac{a_nb_n}{-a_n}<1 \Rightarrow 1<-b_n<1
$$
And from the sandwich theorem we get $\lim_{n\to\infty}|b_n| = 1$. $\Box$","['proof-verification', 'sequences-and-series', 'calculus', 'limits']"
2740218,"What is $x$, if $\cot ^{-1} \left(3x+\frac{2}{x}\right)+\cot ^{-1} \left(6x+\frac{2}{x}\right)+\cot ^{-1} \left(10x+\frac{2}{x}\right)+\cdots = 1$?","Let $$S_{n}=\cot ^{-1} \left(3x+\frac{2}{x}\right)+\cot ^{-1} \left(6x+\frac{2}{x}\right)+\cot ^{-1} \left(10x+\frac{2}{x}\right)+\cdots \quad\text{($n$ terms)}$$ where $x>0$ . If $\lim _{n \to \infty} S_{n}=1$ , then  find the value of $x$ . Can the given series be converted to telescopic series? I converted in into $\tan^{-1}$ but in every term $x$ is in numerator? Could someone please given some hint?","['telescopic-series', 'trigonometry', 'sequences-and-series', 'calculus']"
2740287,Finding an invertible matrix P and some matrix C,"Find an invertible matrix $P$ and a matrix $C$ of the form $C=\begin{pmatrix}a & -b\\b & a\end{pmatrix}$ such that the given matrix $A$ has the form $A = PCP^{-1}$ $A=\begin{pmatrix}5 & -2\\1 & 3\end{pmatrix}$ The first thing i tried to do was to find the eigenvectors of matrix $A$ and i got these vectors (which i glued together to get matrix $P$ and  $P^{-1}$) $P=\begin{pmatrix}1+ i& 1-i\\1 & 1\end{pmatrix}$ $P^{-1}=\begin{pmatrix}\frac{1}{2i} & \frac{-1+i}{2i}\\-\frac{1}{2i} & \frac{1+i}{2i}\end{pmatrix}$ Im not sure how to find the matrix $C$, i thought at first i could plug in the eigenvalues in the $C$ matrix, but i don't think that is what they problem i asking me to do. Any help will be appreciated","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra', 'inverse']"
2740300,"For rotation matrix $A$, find $B = A^4- A^3 + A^2 - A$","Find the value of $B = A^4- A^3 + A^2 - A$ where $A$ is the matrix given below $$ A=
    \left [ \begin{matrix}
    \cos\alpha & \sin \alpha  \\
    -\sin\alpha & \cos\alpha 
    \end{matrix} \right ]
$$ It's actually quite simple when you look at it. Find $A$, $A^2$, $A^3$, $A^4$ by doing some matrix multiplication and then add and subtract following the question. I did all of the tedious work mentioned above. But the matrix $B$ I obtain is filled with garbage. I assuming either we must use some trigno mumbo jumbo identities, or there's something beautiful that I missed from the question. Edit: Could the sum of a geometric series help?","['matrices', 'trigonometry', 'trigonometric-series']"
2740316,Unexpected use of topology in proofs,"One day I was reading an article on the infinitude of prime numbers in the Proof Wiki . The article introduced a proof that used only topology to prove the infinitude of primes, and I found it very interesting and satisfying. I'm wondering, if there are similar proofs that use topology where it's not obvious that it can be applied. I'm sure that seeing such proofs could also strengthen my intuition with topology. So my question is: ""Which theorems, not directly linked with topology, have interesting proofs that use topology?"". Thanks in advance!","['general-topology', 'examples-counterexamples', 'big-list']"
2740434,Finding a tight lower bound for $\left(\frac{1+x}{(1+x/2)^2}\right)^n$,"I am trying to find a tight lower bound for $\left(\frac{1+x}{(1+x/2)^2}\right)^n$ as a function of $x$ and $n$ and for large $n$, 
where $x$ changes with $n$  such that $\lim_{n\to\infty}x=0$. I am not sure wether my approach to solve this is right or not, but this is what I did:
\begin{align*}\left(\frac{1+x}{(1+x/2)^2}\right)^n&=e^{n(\ln({1+x})-2\ln{(1+x/2)})}\\
&=e^{n(x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+\cdots-2(\frac{x}{2}-\frac{x^2}{8}+\frac{x^3}{24}-\cdots))}\\
&=e^{n(-\frac{x^2}{4}+\frac{x^3}{4}-\frac{15x^4}{64}+\cdots)}\\
&\geq  e^{n(-\frac{x^2}{4})}\\
&=1-(n\frac{x^2}{4})+(n\frac{x^2}{4})^2-\cdots \\ 
&\geq 1-(n\frac{x^2}{4}) 
\end{align*}
We know $\lim_{n\to\infty}x=0$, but we don't know whether $\lim_{n\to\infty}nx^2=0$ . Hence, the last inequality is not necessarily correct, 
because the sum of the terms after $1-(n\frac{x^2}{4}) $ may not be greater than zero.","['exponentiation', 'asymptotics', 'sequences-and-series', 'limits']"
2740514,"Two group structures on Hom(C, G)","Let $\mathbf{C}$ be a pointed category (i.e. a category with a zero object), and let $C$ and $G$ be respectively a cogroup and a group object in $\mathbf{C}$. In this situation, the set $Hom_{\mathbf{C}}(C, G)$ gets two different group structures, one because $C$ is a cogroup and the other because $G$ is a group. In problem 1.60 of Strom's Modern Classical Homotopy Theory is asked to show that these two structures coincide. How can I prove this? I have written down a couple of big diagrams (as hinted in the book), but I can't seem to find the answer.","['category-theory', 'group-theory']"
2740523,Intuition behind Lagrange multiplier,"I noticed that all attempts of showcasing the intuition behind Lagrange's multipliers basically resort to the following example (taken from Wikipedia): The reason why such examples make sense is that the level curves of the f function are either only decreasing (d1 < d2 < d3) or only increasing (d1 > d2 > d3) concentrically, so it's obvious that the most centric level curve touching the constraint curve is the minimum/maximum that we are looking for. But in real life examples, I imagine we can have a function f whose level curves might look like this (they don't decrease/increase in an orderly fashion): In the above example that I thought of, the maximization of f (subject to the g constraint) would not be the blue point (where the constraint curve is tangent to a level curve of f), but the two green points. I haven't seen this kind of level arrangement in any tutorial/lecture/course and it just seems to me that every demonstration conveniently picks the most favorable scenario for presenting the intuition. I must be wrong somewhere but I can't figure out where.","['multivariable-calculus', 'optimization', 'lagrange-multiplier']"
2740527,existence of smooth points on varieties,"$P_1, P_2, \ldots, P_k$ are algebraically independent irreducible homogeneous polynomials in $n$ variables over the field of complex numbers. Then, is it true that there is a point $b = (b_1, b_2, \ldots, b_n)$ such that $\forall i \in [k], P_i(b) = 0$ and the rank of the Jacobian of $P_1, P_2, \ldots, P_k$ at $b$ is equal to $k$ ? If this is false, a counterexample would also be very helpful.","['jacobian', 'polynomials', 'algebraic-geometry', 'systems-of-equations']"
2740570,Real Analysis. Little hint for a question.,"A question from my Analysis list that I could not have any idea. Any help would be great. I don't want a complete solution, just a little hint , because I need to do at least one part alone. Let $U = \lbrace x \in \mathbb{R}^{m}\,|\, |x_{i}| < 1, i =1,...,m\rbrace$ and $f: U \longrightarrow \mathbb{R}$ a function differentiable, with $\displaystyle \Bigg| \frac{\partial f}{\partial x_{i}}(x)\Bigg| \leq 3$ for all $x \in U$. Then $f(U)$ is an interval of length $\leq 3m$.","['derivatives', 'real-analysis']"
2740597,Compute $\prod_{n=1}^{\infty} (1+x^{2n})$ for $|x|\lt 1$,Compute for $|x|\lt 1$:$$\prod_{n=1}^{\infty} (1+x^{2n})$$ I'm having trouble computing this product. My work ends up contradicting and saying $x$ has to equal $1$. Anyone know how to do this?,"['products', 'limits', 'calculus', 'analysis']"
2740685,"Give an example of sets $X,Y$, subset $A,B \subseteq X$ and a function $f:X→Y$ such that $f[A \cap B ]$ $\neq$ $f[A] \cap [B]$","Give an example of sets $X,Y$, subsets $A,B \subseteq X$ and a function $f:X→Y$ such that $f[A \cap B ] \neq f[A] \cap f[B]$ I'm not sure if I'm right for the first part, I have $X= \{1,2,3\}$  and $Y= \{1,2,3,4,5\}$. For the second part of the question, I'm not sure how to find a function.","['elementary-set-theory', 'examples-counterexamples', 'functions']"
2740695,Prove that $\sum_{a\in A} φ(a)=0$ where $A$ is finite Abelian group.,"I am attempting to solve the following problem: Let $A$ be a finite abelian group, and let $φ:A\to \mathbb{C^\times}$ be a homomorphism that is not the trivial homomorphism. Prove that $\sum_{a\in A} φ(a)=0$. I know by the structure theorem that $A$ is a direct product of cyclic groups. I have proven it for the special case that A is cyclic, but I need help to prove the general case. Proof of special case: Suppose $A$ is cyclic. Say $A=(\mathbb{Z}/n\mathbb{Z}, +)$. Since $φ$ is not trivial, $n\geq2$. Now, we have $φ(a+b)=φ(a)φ(b)$ and $φ(0)=1$. We also have $φ(a)=φ(1)^a$. It follows that $φ(1)\neq 1$, since otherwise $φ$ would be trivial. Now,  $0=φ(1)^n-1=[φ(1)^0+φ(1)^1+\cdots+φ(1)^{n-1}][φ(1)-1]$, and therefore $\sum_{a\in A} φ(a)=φ(1)^0+φ(1)^1+\cdots+φ(1)^{n-1}=0,$ as needed. QED.","['complex-numbers', 'abstract-algebra', 'group-homomorphism', 'abelian-groups', 'group-theory']"
2740808,Simpler proof that $\det A\neq 0$ if $A$ is invertible?,"One excercise asked me to ""Prove that the determinant of an inversible matrix can't be 0"" . I couldn't remember the proof the teacher gave and I didn't want to ""cheat"" because I'm practising for an exam, so after some thinking I came up with this. I'd like to know if someone has a simpler and/or shorter proof than mine, and if someone can find any mistakes in my proof. Oh, and if someone can suggest a way to shorten it or maybe use more symbols and less natural language. I lost the sheet of the proof the teacher provided, and Googling/searching books I found nothing (only proof that $\det{(A ^{-1})} = 1/\det A$). Maybe I didn't word it properly because English is not my native language. Proof First, proof that the determinant of an elementary matrix $[E]$ can't be 0: An elementary matrix $[E]$ is defined as a matrix obtained by applying a single row operation to the identity matrix. $|I|=1$, so: $|E|=1$ if it comes from adding a row to another one $|E|=-1$ if it comes from swapping two rows $|E|=k/k\in\Bbb R \land k≠0$ if it comes from multiplying a row by $k$ $|E|≠0$ Now, let's say we have a square matrix $[A]_{n\times n}$ that is invertible. To obtain the inverse of $[A]$ we have to apply row operations to the augmented matrix $[A|I]$ until we get $[I|A^{-1}]$. Let's say $E_1*E_2*...E_n$ are the elementary matrixes we premultiply our initial augmented matrix by to get the identity and the inverse. Then we get: $$E_1*E_2*...E_n * A = I$$ and $$E_1*E_2*...E_n * I = A^{-1}$$ Let's take the first one, and apply determinant to both sides of the equality $$|E_1*E_2*...E_n * A| = |I|$$
$$|E_1*E_2*...E_n|*|A| = 1$$ Now let's suppose $|A|$ could be $0$. We would get: $$|E_1*E_2*...E_n|*0 = 1$$
$$0 = 1$$ Which is absurd, and thus the determinant of an invertible matrix can't be 0 .","['matrices', 'proof-writing', 'linear-algebra', 'proof-verification']"
2740855,Probability two withdrawn balls are the same color,"Suppose we have $n$ white and $m$ black balls in a urn. First, randomly withdraw two balls, what is the probability (Call it $P_1$ ) that they are the same color? Now, suppose a ball is randomly withdrawn and then replaced before second one is drawn, what is the probability (Call it $P_2$ ) that withdrawn balls are same color? Finally prove that $P_2 > P_1$ . try For the first situation sample space size is ${m + n \choose 2 }$ . Now, in how many ways can we withdraw balls the same color? If both are white, then can do this in ${n \choose 2}$ ways and if both are black can do in ${m \choose 2}$ . Thus $$ P_1 = \frac{ {m \choose 2 } + {n \choose 2} }{ {m+n \choose 2 } } $$ Now, for second situation, two cases. If the first ball drawn is white, then the probability this happens is ${n \choose 1 } / {m+n \choose 1 } = \frac{n}{m+n} $ . For the seecond ball we want it to be white so this can be done in ${n-1 \choose 1 } / {m+n-1 \choose 1 } = \frac{n-1}{m+n-1} $ so for this case we have $\frac{n(n-1) }{(m+n)(m+n-1)}$ . Similarly if the first ball drawn is black we obtain probability $ \frac{m(m-1) }{(m+n)(m+n-1)}$ .Thus, $$ P_2 = \frac{ m(m-1)  + n(n-1) }{(m+n)(m+n-1) } $$ But, Im stuck in trying to prove $P_2 > P_1$ . Is my approach correct?",['probability']
2740998,Poincare-Wirtinger inequality for $H^{1}(D)$.,"I have a question about Poincare-Wirtinger inequality for $H^{1}(D)$. Let $D$ is an open subset of $\mathbb{R}^d$. We define $H^{1}(D)$ by
\begin{equation*}
H^{1}(D)=\{f \in L^{2}(D,m): \frac{\partial f}{\partial x_i} \in L^{2}(D,m),\ 1\le i \le d\},
\end{equation*}
where $\partial f/\partial x_i$ is the distributional derivative of $f$ and $m$ is the Lebesgue measure on $D$. $H^{1}(D)$ becomes a Hilbert space with the usual Sobolev norm $\|\cdot\|_{H^{1}(D)}$. In the following, we write $\nabla f$ for $(\partial f/\partial x_1,\ldots \partial f/\partial x_d)$. We also write $\|f\|$ for $(\int_{D}f^2\,dx)^{1/2}$ [Theorem 1] Let $D$ be a connected open subset of $\mathbb{R}^d$ with finite volume. We assume the canonical embedding $H^{1}(D) \subset L^{2}(D,m)$ is compact. Then, there exists a positive constant $C>0$ such that for any $f \in H^{1}(D)$
  \begin{equation*}
\|f-f_{D}\|\le C \|\nabla f\|,
\end{equation*}
  where $f_{D}:=m(D)^{-1}\int_{D}f\,dm$ [Proof of Theorem 1] Define $X \subset H^{1}(D)$ by $\{f \in H^{1}(D) : f_{D}=0 \}$. Then, $X$ is a closed subspace of $H^{1}(D)$. It suffices to prove there exists a $C>0$ such that 
\begin{equation*}
(*)\quad \|f\|\le C \|\nabla f\| \ \text{ for any }f \in X
\end{equation*}
We prove $(*)$ by contradiction. We assume there exist a $\{f_n\} \subset X$ such that $\|f_n\|>n \|\nabla f_n\|$. We may assume $\|f_n\|_{H^{1}(D)}=1$.
As $f_n$ is a bounded sequence in $X$, there exists a subsequence $f_{n_k}$ of $f_n$ and a $f \in X$ such that $f_{n_k} \to f$ weakly in $X$. $f_{n_k}$ is also denoted by $f_n$. By assumption, $f_n \to f$ in $L^{2}(D,m)$. Since 
$$1=\|f_n\|_{H^{1}(D)}\text{ and }\|f_n\|>n \|\nabla f_n\|$$
holds, $\|f_n\|\to \underline{1=\|f\|}$ and $\|\nabla f_n\| \to 0$. Since $f_n \to f$ in $L^{2}(D,dx)$ and $\|\nabla f_n\| \to 0$,
\begin{equation*}
\int_{D}(\nabla f_n,\nabla g)+\int_{D}f_n g\,dx\to \int_{D}fg\,dx.
\end{equation*}
for any $g \in X$. On the other hand, since $f_n$ convenes to $f$ weakly in $X$,
\begin{equation*}
\int_{D}(\nabla f_n,\nabla g)dx+\int_{D}f_n g\,dx\to \int_{D} (\nabla f,\nabla g)\,dx+\int_{D}fg\,dx.
\end{equation*}
Hence $\int_{D}(\nabla f,\nabla g)dx=0$ for any $g \in X$, which implies $\nabla f=0$ and $\Delta f=0$ in weak sense. By Wyle's Lemma $f$ is  smooth on $D$. Hence, $\nabla  f=0$ in classical sense. Since $D$ is connected, $f$ is a some constant. Since $ f \in X$, we have $f=0$, which contradicts to $\underline{1=\|f\|}$. My question and remark From the above proof, we do not know concrete estimates of C. If you know an another proof, please let me know.","['real-analysis', 'analysis', 'proof-verification']"
2741001,Understanding Galerkin method of weighted residuals,"I have a puzzlement regarding the Galerkin method of weighted residuals. The following is taken from the book A Finite Element Primer for Beginners , from chapter 1.1. If I have a one dimensional differential equation $A(u)=f$, and an approximate solution $U^N = \sum_{i=1}^N a_i \phi_i(x) $, and the residual $r^N = A(u^N)-f$. The Galerkin method is to enforce that each of the individual approximation functions $\phi_i$ will be orthogonal to the residual $r^N$. So in mathematical formulation is reads:
$$ \int_0^L r^N (x) a_i \phi_i(x) dx = a_i \int_0^L r^N (x)  \phi_i(x) dx =0    \Rightarrow \int_0^L r^N (x)  \phi_i(x) dx =0 \, .$$
Then, in the above equation we have to solve $N$ equations for $N$ unknowns, to find the $a_i$. But if $a_i$ are canceled here, how do I solve for them?","['finite-element-method', 'weighted-least-squares', 'galerkin-methods', 'numerical-methods', 'ordinary-differential-equations']"
2741007,Arithmetic sequence where every term is prime?,"I conjecture that there does not exist an arithmetic sequence where every number is prime. Put another way, the set
$$S_{a,b} = \{a + \delta b \ \vert \delta \in \mathbb{N}\}$$
Where $a, b \in \mathbb{N}$ contains some composite number. I have no idea how to approach such a question. Googling let me to theorems about densities of primes in arithmetic sequences (Green-Tao and the like), but nothing that answers this elementary question.","['number-theory', 'prime-numbers', 'elementary-number-theory']"
2741022,Is it always true that $Log(z^{-1})=-Log(z)$,"Is it always true that $Log(z^{-1})=-Log(z)$? Can we just write $z=re^{i\theta}$ and then $$-Log(z)=-(\ln r+i\theta)=-\ln r-i\theta=\ln (r^{-1})+i(-\theta)=Log(r^{-1}e^{-i\theta})=Log(z^{-1})$$ it seems suspicious to me. Note: For $z\in\mathbb{C}\setminus (-\infty,0]:Log(z)=ln|z|+iArg(z)$ Clarification: I know that it is not always true that $Log(z^{2})=2Log(z)$. So, writing $z=re^{i\theta}$ and $$2Log(z)=2(\ln r+i\theta)=2\ln r+2i\theta=\ln (r^{2})+i(2\theta)=Log(r^{2}e^{2i\theta})=Log(z^{2})$$ is false. And to me it looks kind of the same (maybe false) ""technique"".","['logarithms', 'complex-analysis']"
2741039,Is it possible to obtain a constant jacobian on change of coordinates?,"Let M to be a compact orientable $m$-dimensional manifold. I am interested in the following comment given by a user of forum:  suppose that ""you have an atlas $\{(U_{\alpha},\mathbf{x}_{\alpha})\}_{\alpha \in I}$ such that for all $\alpha , \beta \in I$ with $U_{\alpha} \cap U_{\beta} \neq \varnothing$ the jacobian matrix of $\mathbf{x}_{\beta} \circ \mathbf{x}_{\alpha}^{-1}$ is constant (i.e., the determinant of change of coordinates matrix is constant)."" (See 1 ). We know that the affin manifolds satisfy this property. We would like to know if given a compact orientable $m$-dimensional manifold there exists an atlas of manifold that  satisfies this property. (It is look false, but the question is more general, and I do not know how to solve it). Att","['differential-geometry', 'analysis']"
2741070,Discontinuous linear operator,"Let $X = C ^ \infty ([0,1] , \mathbb R )$ and let $\|\cdot\|$ be any norm
  of $X$. Define the operator $T:X\to X$ by $T(f) = \frac{df}{dx}$. Show that $T$ is not a continuous linear operator from $( X , \|\cdot\| )$
  into $(X , \|\cdot\| )$, Hint : Use the function $f_a (x) = \exp(ax) , a \in \mathbb R$. My answer is : $T$ is discountinuous if there is $f_a (x)  , a \in \mathbb R$, $X = C ^ \infty ([0,1] , \mathbb R )$
such that $\|Tf_a\| \to \infty$ as $a \to \infty$. $\|Tf_a\|= \|T(e^{ax}) \| = \|a e^{ax} \| \to \infty$ as $a \to \infty$. Thus $T$ is not continuous. Is my answer true?","['functional-analysis', 'continuity', 'operator-theory', 'proof-verification']"
2741081,Calculating number of functions with a power set,"How many functions are there with domain $A = \mathcal{P}(\{1, 2, 3, 4\})$ and codomain $\{0, 1\}$? If it was not a power set, I know that it would be $16$ derived from $2^4$.
With the power set, I can make $15$ unique alternatives from $$\{1\},\{2\},\{3\},\{4\},\{1,2\},\{1,3\},\{1,4\}, \{2,3\},\{2,4\},\{3,4\},\{1,2,3\},\{1,2,4\},\{1,3,4\},\{2,3,4\},\{1,2,3,4\}.$$ Would I be correct in assuming that the number of functions would be $2^{15}$. How many one-to-one functions are there with domain A and codomain $\{0, 1\}$? Since there are $4$ elements in $A$, can I assume that the answer is $4!$ ?","['combinatorics', 'characteristic-functions', 'discrete-mathematics']"
2741102,Measure of complement of union of nowhere dense set with positive measure,"The original question is: Let $A$ be Lebesgue measurable in $\mathbb{R}$ with positive measure. Show that it is not true that there must exist a sequence $\{x_n\}^\infty_{n=1}$ such that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ in $\mathbb{R}$ is measure zero. To disprove the statement, I want to take $A$ to be the fat rational set and argue that for any sequence $\{x_n\}$ in $\mathbb{R}$, the complement of the set stated above must have positive measure. Let $\{q_n\}$ be an enumeration of $\mathbb{Q}$, fix $\epsilon\in(0,1)$ and let
\begin{equation}
I_n=(q_n-\frac{\epsilon}{2^{n+1}},q_n+\frac{\epsilon}{2^{n+1}}).
\end{equation}
Let $G=\cup_nI_n$, and $A=[0,1]\setminus G$, then clearly any rationals cannot be in $A$ and it follows that $A$ is nowhere dense. It is easy to show that $A$ is Lebesgue measurable and has measure
\begin{equation}
m(A)= 1-\sum^\infty_{n=1}\frac{\epsilon}{2^n}=1-\epsilon\in(0,1),
\end{equation}
so $A$ satisfies the hypothesis of the question. Since $A$ is nowhere dense so $\bigcup^\infty_{n=1}(A+\{x_n\})$ is nowhere dense also for any sequence $\{x_n\}$ in $\mathbb{R}$. However, I found it difficult to prove that the complement of $\bigcup^\infty_{n=1}(A+\{x_n\})$ must have positive measure for arbitrary sequence $\{x_n\}$","['real-analysis', 'examples-counterexamples', 'lebesgue-measure', 'baire-category', 'measure-theory']"
2741131,"$\frac{dx}{dt}, \frac{dx}{d\frac{dx}{dt}}, \frac{dx}{d\frac{dx}{d\frac{dx}{dt}}}$ ad infinitum to $\sqrt{2x}$","Firstly, for a finite such tower, say $n$ deep, assuming there are no divisions by $0$ to get in the way $$\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}} = \frac{\mathrm{d}x}{\mathrm{d}t}\Big(\frac d{\mathrm{d}t}\big(\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}}\big)\Big)^{-1} = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}\Big(\frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac d{\mathrm{d}t}(...)}\Big)}\Big)}$$ The bottom of this tower is $$ \frac{\mathrm{d}x}{d \frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{\mathrm{d}x}{\mathrm{d}t}\frac{\mathrm{d}t}{d\frac{\mathrm{d}x}{\mathrm{d}t}} = \frac{x'}{x''}$$ and we can, at least in principle, work back up to the top to have the whole tower expressed as upto $n^{th}$ derivatives. I did write out what the next layers would look like but I couldn't see any nice pattern. Ok so now my question is whether it would be in any sense meaningful to consider the infinite tower, in which case the following analysis happens: Writing $F = \frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d\frac{\mathrm{d}x}{d ...}}}$ , we have $$F = \frac{\frac{\mathrm{d}x}{\mathrm{d}t}}{\frac{dF}{\mathrm{d}t}} \implies F = \sqrt{2x + c}$$","['derivatives', 'real-analysis']"
2741134,Does a self-adjoint operator with discrete spectrum have compact resolvent?,"I know that if one has a densely-defined operator $T$ with compact resolvent $(T-\lambda)^{-1}$ (for some $\lambda\in\mathbb{C}$) then its spectrum $\sigma(T)$ comprises a countable set of isolated eigen-values of finite multiplicity, accumulating at no finite point. What I don't know (and can't seem to find anywhere) is whether there is some sort of converse implication in the case of $T$ being a self-adjoint operator on a Hilbert space. Can anybody offer an answer to the title question? If positively, please may I have a reference?","['functional-analysis', 'spectral-theory', 'operator-theory', 'hilbert-spaces']"
2741146,"Is there a formal concept for ""locality of a function""?","Say we have a function that maps a string of size $n$ of some finite alphabet to another such string of size $n$. Or alternatively, a function that maps an $n$ dimensional real vector to another one. I am looking for a term/concept that captures the notion of how ""local"" the transformation is. For example, if such a function maps a string of 5000 digits to the same string, except multiplying the 43'th digit by 2 or by 10 billion, then it is extremely local, since if you change one digit in the input, this will only change the same digit in the output. But if we have a cryptographic hash function, then changing one digit even slightly, will completely change all the output digits, and all of them in different ways. So such a function is highly ""non-local"" Is there a formal concept of this notion of ""locality"" of a transformation?",['functions']
2741162,Congruence involving Bernoulli numbers,"I need to show that $$2B_{2m}\equiv1\pmod{4}$$ for $m\ge2.$ This should be something easy using (at most) Claussen and von Staudt's theorem, but I haven't been successful yet. Writing $B_{2m}=U_{2m}/V_{2m},(U_{2m},V_{2m})=1,V_{2m}>0$ I know $$V_{2m}\equiv2U_{2m}\pmod4,$$
but since $2\mid V_{2m},$ this is not very useful. I also wanted to try Hensel's lifting lemma for $f(x)=2x-1$ (because $2B_{2m}\equiv1\pmod2$ holds), but $f'(x)\equiv0\pmod2,$ so this is not the right way as well. Any other tips? Remark. I needed this result for finishing the following problem. Let $q$ be a prime number such that $2q+1$ is composite. Show that the numerator of the Bernoulli number $B_{2q}$ is divisible by a prime of the form $4n+3$. The congruence mentioned above is a final step in my solution.","['number-theory', 'congruences', 'bernoulli-numbers', 'prime-numbers']"
2741180,How to find 3D rectangle intersection with segment?,"I need to find the intersection of a segment with 3D rectangle. I have four corners of a 3D rectangle and start, end point of a segment. I want to find the intersection point on 3D rectangle. see the below image: $p_1,p_2,p_3,p_4$ are four corner of the rectangle. $a,b$ are start and end point of a segment. I need to find the intersection (red point in the image) segment e-f and c-d are not intersecting with the rectangle. in my case all segments are 90 degree upwards (parallel to Z axis). all points are 3D points $(x,y,z)$ I already searched lot in google, all solutions for plane and line($\infty$) not for a finite 3D rectangle and segment. Can anyone help me to solve this?","['solid-geometry', '3d', 'rectangles', 'geometry']"
2741211,Euler-Poisson-Darboux equation - PDE Evans,"suppose $n \geq 2,\;m \geq 2$ and $u \in C^{m}(\mathbb{R}^n \times[0,+\infty))$ solves the following IVP : \begin{cases}
u_{tt} - \Delta u = 0 \; & \text{in} \; \mathbb{R}^n \times(0,+\infty) \\
u = g, \; u_t = h \; & \text{on} \;  \mathbb{R}^n \times\{t = 0\}
\end{cases} let $x \in \mathbb{R}^n, \; t > 0, \; r > 0 $. Define \begin{align}
U(x;r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}u(y,t)dS(y) \\
G(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}g(y)dS(y) \\
H(x;r)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{\partial B(x,r)}h(y)dS(y) \\
\end{align} $$\alpha(n) = \frac{\pi^{\frac{n}{2}}}{\Gamma(\frac{n}{2}+ 1)} = \text{volume of the unit ball in } \mathbb{R}^n $$ (Euler-Poisson-Darboux equation) fix $x$ in $\mathbb{R}^n$ and let $u$ satisfies the IVP above. then : \begin{cases}
U_{tt} - U_{rr} - \frac{n-1}{r}U_r= 0 \; & \text{in} \; \mathbb{R}_{+}\times(0,+\infty) \\
U = G, \; U_t = H \; & \text{on} \;  \mathbb{R}_{+}\times\{t = 0\}
\end{cases} the author states that : \begin{align}
U_r(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ B(x,r)}\Delta u(y,t)dy \\
U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  +(\frac1n - 1)(\frac{1}{\alpha(n)r^n})\int_{ B(x,r)}\Delta u(y,t)dy \\
\end{align} I could derive $U_r$ on my own but I'm having trouble trying to compute $U_{rr}$ if I differentiate $U_r$ with respect to $r$ I get : \begin{align}
U_{rr}(x;,r,t)  & = \frac{1}{n\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy + \frac{r}{n} [\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} \\
\end{align} this means we must have : \begin{align}
\frac{r}{n}[\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy]'_{r} & =  \frac{1}{n\alpha(n)r^{n-1}}\int_{ \partial B(x,r)}\Delta u(y,t)dS(y)  -\frac{1}{\alpha(n)r^n}\int_{ B(x,r)}\Delta u(y,t)dy    \\
\end{align} how do you prove this ?","['analysis', 'partial-differential-equations']"
2741222,Hyperbolic substitution in $\int\frac{dx}{\sqrt{x^2-a^2}}$ for $x\le -a$,When trying to solve $\int\frac{dx}{\sqrt{x^2-a^2}}$ we can substitue $x$ in two ways: METHOD 1: $x=a\sec(\theta)$ and $dx=a\sec(\theta)\tan(\theta) d\theta$ and $\theta=\sec^{-1}(\frac{x}{a})$. METHOD 2: $x=a\cosh(u)$ and $dx=a\sinh(u)du$ and $u=\cosh^{-1}(\frac{x}{a})$ My calculus-book states that we can use a similar method when we assume $x\le -a$. I can see how that would work with method 1 because $\sec^{-1}(\frac{x}{a})$ is indeed defined for $x\le -a$. QUESTION: But how would that work in method 2? $\cosh^{-1}(\frac{x}{a})$ is only defined for $\frac{x}{a}\ge1$ and if $x\le-a$ then $\frac{x}{a}$ is negative. So in this case $\cosh^{-1}(\frac{x}{a})$ is not defined!!! What is happening here? Thanks!,"['substitution', 'hyperbolic-functions', 'integration', 'calculus']"
2741229,"If derivative of $f$ is continuous, then $f$ is continuous.","I have searched a lot, but i haven't found any proof about that statement. I have checked the proof of If $f$ is differentiable, then $f$ is continuous but it's not the same argument I think. Also, I want to know what's your opinion about the statement If derivative of $f$ is not continuous, then $f$ is not continuous","['derivatives', 'continuity', 'calculus']"
2741235,Show That A Particle In A Bounded Force Field Can Reach Any Point In Fixed Time Span,"I tried to prove that for a smooth bounded force field $F$ and $x\in{\bf R}^n$ there exists some $v\in{\bf R}^n$ such that a particle starting in $0$ with mass $1$ and velocity $v$ , obeying Newton's second law, will reach $x$ after exactly $t$ time units for some $t>0$ .  By rescaling impulse and force field, we can w.l.o.g. assume $t=1$ . Define $r:=\Vert x\Vert+1$ and let $\phi_G(v,.)$ denote the flow of such a particle starting with velocity $v$ in a force field $G$ .  Given that $F$ is bounded, there is some constant $c$ with $$\Vert\phi_{\alpha F}(v,1)-v\Vert<c\tag{1}\label{1}$$ for all $\alpha\in[0,1],v\in{\bf R}^n$ .  I believe, we can show $B_r\subseteq\phi_F(B_{r+c},1)$ . I'd like to do something like this: First, the degree of the map $\phi_F(.,1)$ on $\partial B_{r+c}$ is $1$ . This should follow from $(1)$ which implies that $(u,\alpha)\mapsto\phi_{\alpha F}(u,1)$ is a homotopy between the identity on $\partial B_{r+c}$ and $\phi_F(.,1)$ which never maps to $0$ .  Now infer (with some result from algebraic topology I reckon) that $\phi_F(B_{r+c},1)$ contains the connected component $C$ of ${\bf R}^n\setminus\phi_F(\partial B_{r+c},1)$ which contains $0$ .  Using $(1)$ again we have $B_r\subseteq C$ .","['dynamical-systems', 'physics', 'calculus', 'algebraic-topology', 'differential-geometry']"
2741267,Prove that Sequence $\phi _n(x)=n\phi(nx)$ is delta sequence,"Let $\phi : \Bbb R \to [0, \infty)$ be such that $\int_{-\infty}^{\infty} \phi (x) dx=1 $. Define $\phi _n(x)=n\phi(nx)$. Show that $\{\phi _n(x)\}_{n\in \Bbb N}$ is a delta sequence. Attempt: To show this I need to show $$\lim_{n\to\infty}(\phi_n,\psi)=(\delta,\psi)=\psi(0), \quad \forall \psi\in \mathcal{D}(\mathbb{R}).$$ $(\phi_n,\psi)=\int_{-\infty}^{\infty}n\phi(nx)\psi(x)dx.$ After Substitution $y=nx$, $\int_{-\infty}^{\infty}n\phi(nx)\psi(x)dx=\int_{-\infty}^{\infty}\phi(y)\psi\left(\frac{y}{{n}}\right)dy.$
After that I dont know, what have to do.","['functional-analysis', 'improper-integrals', 'integration']"
2741292,The Plucker relations are sufficient,"Consider the Grassmannian of codimension-$d$ subspaces of a given vector space $E$ (over an arbitrary field), which I will define as
$$ \operatorname{Gr}^d(E) = \{\text{linear surjections } \sigma: E \to F \mid \text{$F$ is any $d$-dimensional space} \}/\sim$$
where I identify two surjections $(\sigma_1: E \to F_1) \sim (\sigma_2: E \to F_2)$ if there is an isomorphism $m: F_1 \to F_2$ such that $m \sigma_1 = \sigma_2$. Then an isomorphism class of $\sigma$ is determined by the $\ker \sigma$. We also have projective space $\mathbb{P}(E) = \operatorname{Gr}^1(E)$, so that $\operatorname{Sym}^\bullet(E)$ gives homogeneous coordinates on $\mathbb{P}(E)$. The Plucker embedding is the map $\operatorname{Gr}^d(E) \to \mathbb{P}(\bigwedge^d E)$, taking a map $(\sigma: E \to F)$ to its $d$th exterior power $\bigwedge^d \sigma: \bigwedge^d E \to \bigwedge^d F$. Then $\bigwedge^d \sigma$ is a surjection from $\bigwedge^d E$ to a one-dimensional space, and so lives in $\mathbb{P}(\bigwedge^d E)$. So I am thinking of $\bigwedge^d \sigma$ as a point in the embedding of the Grassmannian, and elements of $\bigwedge^d(E)$ give me the linear coordinate functions. For example, if I write out the matrix of $\sigma: E \to F$ in some basis $(e_1, \ldots, e_m)$ for $E$ and any basis for $F$, then $(\bigwedge^d \sigma)(e_1 \wedge \cdots \wedge e_d)$ is proportional to the determinant of minor where we take the first $d$ columns of the matrix for $\sigma$. Of course, the question arises that, given some $(s: \bigwedge^d E \to L) \in \mathbb{P}(\bigwedge^d E)$, is $s$ in the image of the Plucker embedding? (i.e., is $s$ a $d$th wedge power?). The answer to this is given by the Plucker relations, which I will state as follows. Define a linear map $\omega^d_E$ on pure tensors by
$$ \begin{aligned}\omega^d_E: \bigwedge^{d+1} E \otimes \bigwedge^{d-1} E &\to \bigwedge^d E \otimes \bigwedge^d E\\
v_1 \wedge \cdots \wedge v_{d+1} \otimes u_1 \wedge \cdots \wedge u_{d-1} &\mapsto \sum_{i = 0}^{d+1} v_1 \wedge \cdots \wedge \hat{v_i} \wedge \cdots \wedge v_{d+1} \otimes v_i \wedge u_1 \wedge \cdots \wedge u_{d-1} \end{aligned}$$
Then the point $(s: \bigwedge^d E \to L)$ ""satisfies the Plucker relations"" if the linear map $(s \otimes s) \circ \omega^d_E$ is zero. From this standpoint, it is lovely to see the necessary condition: if $(\sigma: E \to F) \in \operatorname{Gr}^d(E)$, then $\wedge^d \sigma$ must satisfy the Plucker relations, since the wedge power essentially ""pulls through"" the $\omega$:
$$ (\wedge^d \sigma \otimes \wedge^d \sigma) \circ \omega^d_E = \omega_F^d \circ (\wedge^{d+1} \sigma \otimes \wedge^{d-1} \sigma)$$
and of course $\wedge^{d+1} \sigma = 0$ because $F$ is $d$-dimensional. However, I am having trouble with the opposite direction. How can I see that if $(s: \wedge^d E \to L) \in \mathbb{P}(\bigwedge^d E)$ satisfies the Plucker relations, then it is (proportional to) the $d$th power of some map $\sigma: E \to F$? I would really like a constructive proof: produce some $\sigma: E \to F$ from $s$, and show that as long as $s$ satisfies the Plucker relations, that $\wedge^d \sigma \sim s$. I can prove similar results for decomposable tensors in $\wedge^d E$, but for some reason with how things are set up, ""dualising"" that proof is eluding me. There is a proof similar to the style I am thinking of on page 175 of Martin Brandenburg's Thesis , unfortunately I cannot follow the proof starting from the ""two short exact sequences"". Which leads me to another question: Are there any good references for the Plucker embedding or Plucker relations thought of in this way?","['grassmannian', 'algebraic-geometry', 'exterior-algebra']"
2741405,powerball draw question,"This seems intuitive but would like a second opinion. Powerball with 69 numbers. Need to choose 5 correct (order does not matter). Prediction A: 60, 61, 64, 65, 68
Prediction B: 25, 35, 48, 59, 64 When I look at a list of 253 actual powerball drawings, the lowest number (slot 1) is never over 51. I know that 51 or higher for slot 1 is possible but is it correct to say that the likelihood of Prediction A is more of an edge case (due to a high number, 60 in this case, in slot 1)? Thus, Prediction A is less likely to appear than Prediction B? Slot 1 does not mean the first ball drawn. Rather, it is the lowest numbered ball after 5 balls are drawn. Thanks.","['lotteries', 'statistics', 'probability']"
2741414,"Proof Review for Triangle Inequality for $d((x_1, x_2), (y_1, y_2)) = max( |y_1 - x_1|, |y_2 - x_2|)$","Let $X = \mathbb{R}^2$ and define a function $d : \mathbb{R}^2 \times \mathbb{R}^2 \to \mathbb{R}$ by $d((x_1, x_2), (y_1, y_2)) = \max( |y_1
 - x_1|, |y_2 - x_2|)$, where $\max(a, b)$ is the maximum of $a$ and $b$. I can't tell if the following proof is correct and would appreciate it if people could please take the time to review it. At each step, I have done my best to explain my reasoning. If there is anything about my understanding that is incorrect and/or lacking, I would be grateful for explanation. The Proof Let $x = (x_1, x_2), y = (y_1, y_2), z = (z_1, z_2)$. We want to show that $d(x, z) \le d(x, y) + d(y, z)$; in other words, that the function $d$ satisfies the triangle inequality. $d(x, z) = \max(|z_1 - x_1|, |z_2 - x_2|)$ (By the definition of $d$.) We will begin by looking at $|z_1 - x_1|$ and $|z_2 - x_2|$ separately. The case for $|z_1 - x_1|$ follows. $|z_1 - x_1| \leq |z_1 - y_1| + |y_1 - x_1|$ (By the triangle inequality for $\mathbb{R}$.) $\leq \underbrace{\max(|z_1 - y_1|,|z_2 - y_2|)}_\text{This is $d(y, z)$, which is $\ge \ |z_1 - y_1|$} \ + \underbrace{\max(|y_1 - x_1|,|y_2 - x_2|)}_\text{This is $d(x, y)$, which is $\ge \ |y_1 - x_1|$}$ The case for $|z_2 - x_2|$ follows. $|z_2 - x_2| \leq |z_2 - y_2| + |y_2 - x_2|$ (By the triangle inequality for $\mathbb{R}$.) $\leq \underbrace{\max(|z_1 - y_1|, |z_2 - y_2|)}_\text{This is $d(y, z)$, which is $\ge \ |z_2 - y_2|$} \ + \underbrace{\max(|y_1 - x_1|, |y_2 - x_2|)}_\text{This is $d(x, y)$, which is $\ge \ |y_2 - x_2|$}$ Now we bring it all together to state the conclusion: $\therefore d(x, z) = \max(|z_1 - x_1|, |z_2 - x_2|)$ $\leq \underbrace{\max(|y_1 - x_1|, |y_2 - x_2|)}_\text{This is $d(x, y)$} \ + \underbrace{\max(|z_1 - y_1|,|z_2 - y_2|)}_\text{This is $d(y, z)$} = d(x,y) + d(y, z)$ I've spent a tremendous amount of time on trying to understand this problem, so I would greatly appreciate reviews. Since the kind reviewers are indicating that the proof is correct, I want to write a little note-to-self (and future viewers) that explains what I was misunderstanding. What Was My Misunderstanding? What I wasn't seeing was that we needed to show that each term in $d(x, z) = \max(|z_1 - x_1|, |z_2 - x_2|)$ -- $|z_1 - x_1|$ and $|z_2 - x_2|$ -- must be $\le d(x, y) + d(y, z)$. Why? Because if we show that $|z_1 - x_1| \le d(x, y) + d(y, z)$ and $|z_2 - x_2| \le d(x, y) + d(y, z)$, then we have that $d(x, z) = \max(|z_1 - x_1| \le d(x, y) + d(y, z), |z_2 - x_2| \le d(x, y) + d(y, z))$. By the properties of the $\max()$ function, this implies that $d(x, z)$ can at most be $\le d(x, y) + d(y, z)$! So essentially what we've done here is found an upper bound for each of the arguments of the $\max()$ function, $|z_1 - x_1|$ and $|z_2 - x_2|$.","['general-topology', 'metric-spaces', 'proof-verification']"
2741434,"Show that if $\eta \sim N(0, \sigma^2)$ then $\mathbb{E} [\eta^4] = 3 \sigma^4$?","For the normally distributed random variable $\eta \sim N(0, \sigma^2)$ I am trying to show that
$$
\mathbb{E} [\eta^4] = 3 \sigma^4
$$ Can anyone help me to understand how this might be done?","['stochastic-processes', 'statistics']"
2741495,Pull-back of algebraic cycle and moving lemma,"Suppose $f:X \rightarrow Y$ is morphism between non-singular projective varieties, and $Z$ is a codimension-$c$ cycle of $Y$. From the chapter on Intersection Theory of Stack Project, for rational equivalence relation, the pull back of the cycle class $[Z]$ represented by $Z$ is
\begin{equation}
f^*([Z])=(pr_X)_*([\Gamma_f] \cdot (pr_Y)^*[Z])
\end{equation}
When the scheme theoretic inverse image $f^{-1}(Z)$ is also of codimension $c$ and $Z$ is Cohen-Macaulay at the images of the generic points of $f^{-1}(Z)$, then the pull-back $f^*[Z]$ is just the cycle associated to the scheme $[f^{-1}(Z)]$. Question : for an arbitrary cycle $Z$, does there exist a cycle $Z'$ which is rationally equivalent to $Z$ such that the codimension of $f^{-1}(Z')$ is $c$ and
\begin{equation}
f^*([Z])=[f^{-1}(Z')]
\end{equation}","['intersection-theory', 'algebraic-geometry']"
2741528,Derivative of a random process using a Karhunen-Loève-expansion/basis expansion,"Suppose a random function $X(t)$ can be written with the help of the Karhunen-Loève expansion (or some other basis expansion) by 
$$X(t) = \sum_{k=1}^\infty \xi_k \psi_k(t),$$
where  $\psi_1(t), \psi_2(t),\dots$ are orthonormal eigenfunctions corresponding to the non-negative eigenvalues $\lambda_1,\lambda_2,\dots$ of the covariance operator $\Gamma$ and the random variables $\xi_k = \int_{a}^bX(s)\psi_k(s)ds$  are the uncorrelated scores with $E(\xi_k)=0$, $E(\xi_k \xi_l)=0$  for $k\neq l$ and $E(\xi_k^2)=\lambda_k$. I am interested in the derivative $X'(t)$ of $X(t)$. Suppose the eigenfunctions $\psi_k(t)$ are differentiable, then, a natural approach would be to use the basis expansion above to get $$X'(t) = \sum_{k=1}^\infty \xi_k \psi_k'(t).$$ This approach doesn't work always. For example, in the case where $X(t)$ is a Wiener process, $\psi_k(t)=\sqrt{2}\sin((k-1/2)\pi t)$ and $\lambda_k \sim 1/k^2$. However, while the derivative of $\psi_k$ exists and is obviously bounded, it is well known that the Wiener process is a.s. not differentiable. Why can't I see this fact from the derivative of the expansion? Which property am I missing? (What irritates me additionally is the fact that the variance of the right hand side, $E((\sum_{k=1}^\infty \xi_k \psi_k'(t))^2) = \sum_{k=1}^\infty \lambda_k \psi_k'(t)^2 \leq const \sum_{k=1}^\infty 1/k^2$, is bounded?!) Can someone enlighten me why this natural approach doesn't work in general and/or give me some conditions when it works?","['derivatives', 'brownian-motion', 'stochastic-processes']"
2741530,An approach to evaluating a Cauchy principal value that yields unexpected extra imaginary term,"$\newcommand{\PV}{\operatorname{PV}}$ I have been working on evaluating the first negative moment of a random variable with a piecewise density function by means of the Cauchy principal value, i.e. \begin{equation}
\tag{1}
PV\!E(X^{-1})=\PV\int_{-\infty}^\infty\frac{f_X(x)}{x}\,\mathrm dx = \lim_{\epsilon\to0}\left(\int_{-\infty}^{-\epsilon}+\int_\epsilon^\infty \right)\frac{f_X(x)}{x}\,\mathrm dx,
\end{equation} where $f_X$ is a probability density function. I was unable to get any traction using the definition in $(1)$ but was able to evaluate the moments of $X$ greater than one which yielded \begin{equation}
\begin{aligned}
E(X^{\epsilon-1})
&=\int_{-\infty}^0 x^{\epsilon-1}f_X(x)\,\mathrm dx%
+\int_0^\infty x^{\epsilon-1}f_X(x)\,\mathrm dx,\\
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
&=\int_0^\infty x^{\epsilon-1}f_X(x)\,\mathrm{d}x%
-e^{\mathrm i\pi\epsilon}\int_{-\infty}^0 |x|^{\epsilon-1}f_X(x)\,\mathrm dx,\\
&\propto\Gamma(\epsilon)\left(h_1(\epsilon)-e^{i\pi\epsilon}h_2(\epsilon)\right):=\Gamma(\epsilon)D(\epsilon).
\end{aligned}
\end{equation} I then tried taking the limit of the moments as $\epsilon\to0$ . As $\epsilon\to0$ , $D(\epsilon)\to0$ while the gamma term blows up. However, by writing the limit as \begin{equation}
\lim_{\epsilon\to0}E(X^{\epsilon-1})\propto\Gamma(\epsilon+1)D(\epsilon)/\epsilon,
\end{equation} we see that it is $0/0$ indeterminant.  Using L'Hopitals rule and evaluating the limit I got something of the form \begin{equation}
\lim_{\epsilon\to0}E(X^{\epsilon-1})= L-f_X(0)\pi\mathrm i.
\end{equation} Numerical evaluation revealed that $L=PV\!E(X^{-1})$ ; thus, what I have found is \begin{equation}
\tag{2}
\lim_{\epsilon\to0}E(X^{\epsilon-1})= PV\!E(X^{-1})-f_X(0)\pi\mathrm i.
\end{equation} I was able to come up with a proof of the result in $(2)$ by means of contour integration but was wondering if there other ways of getting to this result. So with that said, what is going on here? Why does adding the extra power to $t$ result in a residual imaginary term?","['residue-calculus', 'complex-integration', 'cauchy-principal-value', 'complex-analysis', 'contour-integration']"
2741549,The convergence in Lebesgue differentiation theorem,"Let $f$ be a function on $\mathbb R$ such that $f$ is locally integrable. It is well known that from the Lebesgue differentiation theorem we have
$$
\frac{1}{h}\int_t^{t+h} u(s)\,ds \to u(t)
$$
almost everywhere if $h \to 0$. My question is, can we prove that the convergence is in $L^1_{loc}$ ?",['real-analysis']
2741632,Evaluate $\lim_{n\to \infty} \left( \sum_{r=0}^n \frac {2^r}{5^{2^r}+1}\right) $,"Evaluate $$\lim_{n\to \infty} \left( \sum_{r=0}^n \frac {2^r}{5^{2^r}+1}\right) $$ I tried to create some infinite GP within the summation,  some algebraic manipulations like adding the first and last terms of the summation to find any series popping out of it and also tried writing it in the exponential form like $5^{2^r}=e^{2^r\ln 5}$ and also tried to do some power series thing. I also tried to find any method using integrals and Riemann sums but couldn't do so. Any hints would be greatly appreciated.","['limits', 'calculus', 'integration', 'summation', 'sequences-and-series']"
2741728,Can there be a trigonometric function reaching any finite number of points in $\mathbb{R} ^2$,"Today in math class we hade a discussion about linear regression, which is all about finding the best (though not perfect) linear equation that passes through a countable set of points, and people then started wondering about having an arbitary polynomial $p(x)$ reaching every point ""perfectly"", i.e. $p(x_k) = y_k$ for any list $((x_0,y_0), \cdots (x_n,y_n))$. This usually requires a polynomial of degree $n-1$, and is rather hard to find by hand. But I remember reading this post a while ago, and started to figure, whether an equation of the form $$y= \alpha \sin(ax) + \beta \sin(bx + \xi), \alpha, \beta,a,b \in \mathbb{R} , \xi \in \mathbb{R} \backslash \mathbb{Q}$$ (i.e. $\xi$ being irrational) would be able to pass through any countable list of points in $\mathbb{R}^2$, my motivation is the following Since the function is nonperiodic we can scale it down such that it
will be able to pass through points close to one another Since the individual $\sin$-functions are periodic, we have an extra $2 \pi n$ when we take the $\arcsin$, which will give us a countable amount of equations which we can use to solve using the conditions in any given list. My motivation may be a tad vague, but I hope you'll atleast be able to get the idea of what I'm after, a trigonometric function that passes through any point of our choise. Thanks in advance","['linear-regression', 'trigonometry', 'linear-algebra']"
2741786,Question about positive variations of function,"Given a function $f:\mathbb{R} \to \mathbb{C}$, Folland in his book ""Real Analysis"" defines the total variation of $f$ at $x \in \mathbb{R}$ as
$$
T_f(x) = \sup \left\{\sum_{1}^n |f(x_j)-f(x_{j-1})|: -\infty < x_0 <x_1 < \dots x_n = x , n \in \mathbb{N} \right \}
$$
and the space $BV(R)$ as the set of those functions with finite $T_f(x)$ for $x \in \mathbb{R}$. Then, for $f \in BV(R)$, he defines the positive variation of $f$ denoted by  $v(f)^+$
$$
v(f)^+ := \frac{1}{2} (T_f+f)(x) = \sup \left\{\sum_{1}^n [f(x_j)-f(x_{j-1})]^+: -\infty < x_0 <x_1 < \dots x_n = x , n \in \mathbb{N} \right \} + f(-\infty)$$ but I do not understand where the term $f(-\infty)$ comes from. What I was able to get is
$$ 
v(f)^+  =  \sup \left\{\sum_{1}^n [f(x_j)-f(x_{j-1})]^+ + \frac{1}{2}f(x_0): -\infty < x_0 <x_1 < \dots x_n = x , n \in \mathbb{N} \right \}
$$ What am I missing?","['real-analysis', 'measure-theory']"
2741821,Meaning of $\int_{T_xM} f(z) \ dz$,"Question So I'm from an engineering and recently learned about integrating on riemannian manifolds. However, I have been faced with the notation $$\int _{T_xM} f(z) \ dz,$$ where $M$ is a Riemannian manifold and $T_x$ is the tangent space at some point $x$. Note that I have learned that $T_xM$ is the space of all tangent vectors at $x$. Can anyone say what this integration notation means? I cannot find any references online. I have been taught that if $M$ is a Riemannian manifold then $$\int_M f=\int_{\mathbb{R}^n}f \sqrt{\mathrm{det}(A^tA)} \ dx_1\ldots,dx_n$$ where $A$ is the jacobian (I think).",['differential-geometry']
2741832,"Sequence of open intervals, decreasing radii going to zero, centered at the rationals might still give all of $\mathbb{R}$?","When one first learns measure theory, it is a small novelty to find out that
$$\bigcup_{n=0}^\infty B_{\epsilon/2^n}(r_n)$$
is not all of $\mathbb{R}$, where $\{r_n\}$ is an enumeration of the rationals and $\epsilon$ is an arbitrary positive number (notice this fact is equally impressive if $\epsilon$ is small or large). Of course, by measure arguments, the set above has measure at most $\epsilon$ and can't be all of $\mathbb{R}$. However, there doesn't seem to be another canonical line of reasoning that explains why the union above is not all of $\mathbb{R}$. That makes me wonder, what if we remove that ability to use this argument? Is there a pair of sequences of positive real numbers $\{c_n\}$ and $\{d_n\}$ both tending to $0$ such that 
  $$\sum_{n=0}^\infty c_n=\infty=\sum_{n=0}^\infty d_n$$
  where we can demonstrate
  $$\bigcup_{n=0}^\infty B_{c_n}(r_n)=\mathbb{R}\quad\text{and}\quad\bigcup_{n=0}^\infty B_{d_n}(r_n)\neq\mathbb{R}$$
  with a fixed enumeration of the rationals $\{r_n\}$? An existential proof of both questions would be sufficient for me. But an explicit $\{c_n\}$ and $\{d_n\}$ would be interesting to see. I feel like the $\{c_n\}$ construction might be fairly easy in comparison to $\{d_n\}$, and using dependent choice, I even think I have an argument off the top of my head: just let $\{c_n\}$ be fairly constant until you swallow up $[-N,N]$ and then let it decrease. Continue ad infinitum. But what about $\{d_n\}$?","['general-topology', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
2741887,Probability Poisson Distribution 90 and 110 calls in a 100 second period,"How can i get the probability using poisson distribution of this question: Telephone calls arrive at an exchange at an average rate of one every second. Find the probabilities of the following events, explaining briefly your assumptions. a) No calls arriving in a given five-second period. b) Between four and six calls arriving in the five-second period. c) Between $90$ and $110$ calls arriving in a 100-second period. (Give answer as a decimal.) I got a) and b) but i dont know the way to do c). I know I can use poisson distribution on $$P(X=90) + P(X=91) + \cdots + P(X=110)$$ but I feel like there is an easier way to do this. Thanks","['poisson-process', 'probability', 'poisson-distribution']"
2741962,computer program-software for galois,"I need a reference for a good algebra program-software, especially for Galois theory. What I have found so far is PARI which calculates the galois group over $\mathbb Q$ of a polynomial up to degree 8, but what I am missing which I need, is to be able to calculate, say for $a=\sqrt{2}+\sqrt{3}+\sqrt{5}$, the irreducible polynomial $irr_{\mathbb{Q}, a}$(x), or even maybe over different fields. Maybe Pari can calculate that as well but personally I couldn't find how and the manual was not very illuminating. Other programs I have heard of for algebra are CoCoa and Macaulay but it 's really time consuming searching what are the capabilities of each one of them, so I decided to post this as a question, in case anyone could suggest such a program which he found most convenient. So overall 2 questions: 1)In general, which program do you find most convenient for abstract algebra? 2)Specifically for Galois? (need not be different from the above)","['abstract-algebra', 'galois-theory', 'computational-algebra', 'math-software']"
2742128,Show that a matrix is invertible with norm less than one,"If $\|G\| < 1$, then show that $I-G$ is invertible. This can be proved by contradiction: If $I-G$ is singular, then 1 is an eigenvalue of $I-I+G$. So if the matrix norm is induced the 2-norm, $\| G\|$ is at least 1 since the largest singular value of a matrix is not less than its eigenvalue in absolute value. I have two questions:
1) Why does 1 have to be an eigenvalue of $I-A$ if A is singular? and why is the largest singular value of a matrix not less than its eigenvalue in absolute value? 2) I am learning iterative methods in class and i don't believe we have come acrossed eigenvalues yet; is there any other ways to prove this? Thanks!","['eigenvalues-eigenvectors', 'euclidean-algorithm', 'matrices', 'normed-spaces', 'numerical-methods']"
2742164,How to compute sheaf cohomology of a classifying space?,"David Wigner showed that the group cohomology invented by Calvin Moore can be related to sheaf cohomology when the coefficient is discrete by constructing a locally constant sheaf on the classifying space of the group. In the case where the group action is trivial, the computation is simple since we reduce the sheaf cohomology to a simpler topological cohomology. However, when the group action is non-trivial, I am not sure how to perform computations. Perhaps, we can reduce the problem to Čech cohomology in some way. Maybe, there are some other techniques. Is there any reference from which I can learn how to do explicit computations?","['algebraic-geometry', 'sheaf-cohomology', 'classifying-spaces', 'group-extensions', 'algebraic-topology']"
2742188,Evaluating a limit where the ratio test fails $\lim_{n\to\infty} \frac{1}{2^{2n}}\frac{(2n)!}{(n!)^2}\sqrt{n}$. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question $\lim_{n\to\infty} \frac{1}{2^{2n}}\frac{(2n)!}{(n!)^2}\sqrt{n}$. This is what happens when I try the ratio test, $\lim_{n\to\infty} |\frac{a_{n+1}}{a_n}|=
\lim_{n\to\infty} \frac{1}{4}\frac{(2n+2)(2n+1)}{(n+1)^2}\frac{\sqrt{n+1}}{\sqrt{n}}$ $\lim_{n\to\infty} \frac{4n^3+5n+2}{4n^2+8n+4}\frac{\sqrt{n+1}}{\sqrt{n}}=1$ There are so many factorials in this limit that I don't have the slightest guess of how to proceed with this unfortunate failure.","['sequences-and-series', 'limits']"
2742212,Explicit series for the minimum point of the Gamma function?,"Is there any explicit series, product, integral, continued fraction or other kind of expression for the point at which $\Gamma(x)$ has a minimum in $(0,1)$? The decimal value can be found here http://oeis.org/A030169 , but I haven't found much information there, or in other sources. This is the point $a \in (0,1)$ such that: $$\psi(a)=0$$ Using the integral representation of digamma function: $$\psi(1+s)=-\gamma+\int_0^1 \frac{1-t^s}{1-t}dt$$ Introducing $b=a-1$, we can deduce that: $$b \sum_{k=0}^\infty (-1)^k \zeta(k+2) b^k=\gamma$$ Inverting this power series, we can get a power series for $b$ in terms of Euler-Mascheroni constant, however, it's a long and tedious process and doesn't give us an explicit formula. There's a very old related question , however I'm asking for a more particular result.","['digamma-function', 'sequences-and-series', 'gamma-function', 'euler-mascheroni-constant']"
2742217,Determinant of an $n \times n$ matrix [duplicate],"This question already has answers here : Circular determinant problem (2 answers) Solve $n$th order determinant (3 answers) Closed 6 years ago . In my linear algebra class, my professor gave us this determinant for practice: $$det\pmatrix
    {1  & 2 & 3 & \dots & n \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ Where the $i$-th row or column is the set $$\{1,2,\dots,n\}$$ shifted $i-1$ times to the left. He recommended we add rows $2$ through $n$ to row $1$. And then factor out the constant. This yields: $$\frac{n(n+1)}{2}det\pmatrix
    {1  & 1 & 1 & \dots & 1 \\ 2  & 3 & 4 & \dots & 1 \\ \vdots & \vdots & \vdots & \dots & \vdots \\ n   & 1 & 2 & \dots & n-1}$$ However, I cannot for the life of me find a useful way to simplify this further. I have tried adding the first column to each subsequent column, subtracting the first row from each subsequent row, but nothing seems to simplify the matrix. Perhaps, there is an inductive solution, but I haven't been able to find one simply from the $2\times2$ and $3\times3$ cases. Any help would be appreciated.","['matrices', 'determinant']"
2742251,"""Totally disconnected"" analogue of a classical result from functional analysis","Consider the following result: Let $X$ be a compact Hausdorff space.  Then $x \mapsto \mathfrak m_x = \{ f \in \mathscr C(X) : f(x) = 0 \}$ defines a bijection $X \rightarrow \operatorname{m-spec} \mathscr C(X)$, where $\mathscr C(X)$ is the ring of continuous functions $X \rightarrow \mathbb C$. See for example my previous answer here .  There is an interesting analogue of these statement which I have seen mentioned in a number-theoretic context: Let $X$ be a compact totally disconnected Hausdorff space.  Then $x \mapsto \mathfrak p_x = \{ f \in \mathscr C^{\infty}(X) : f(x) = 0 \}$ defines a bijection of sets $X \rightarrow \operatorname{spec} \mathscr C^{\infty}(X)$, where $\mathscr C^{\infty}(X)$ is the ring of locally constant functions $X \rightarrow \mathbb Q$. Is this result true?  It seems strange that $\mathscr C^{\infty}(X)$ should have no nonmaximal prime ideals.","['functional-analysis', 'compactness', 'commutative-algebra']"
2742261,Extrema of an infinite product of sinc functions,"Consider the function
$$f(x)=\prod_{n=0}^\infty\operatorname{sinc}\left(\frac{\pi \, x}{2^n}\right),\tag1$$
where $\operatorname{sinc}(z)$ denotes the sinc function . It arises as a Fourier transform of the Rvachev $\operatorname{up}(x)$ function, which is basically a shifted version of the Fabius function (see, for example $^{[1]}$ $\!^{[2]}$ $\!^{[3]}$ ). Curiously, if we take a finite partial product from $(1)$ with at least 2 terms, its Fourier transform will be a continuous piecewise-polynomial function with finite support (and with continuous derivatives of progressively higher orders as we include more terms). We restrict our attention only to $x\ge0$. The function $f(x)$ has zeros at positive integers, and oscillates with a quickly decaying amplitude. Its signs on the intervals between consecutive zeros follow the same pattern as the Thue–Morse sequence . It appears that $f(x)$ has exactly one extremum on each interval between consecutive zeros (minimum or maximum, depending on its sign on that interval) — but I have not been able to find a complete rigorous proof of it. Can you propose one? Update: I extracted the second part of my original question into a separate one and edited it significantly. The following is just an interesting observation: Let us denote the value of the extremum on the interval $n<x<n+1$ as $\epsilon_n$. The absolute values of the extrema $|\epsilon_n|$ generally tend to decrease as $n$ increases, but they do not decrease strictly monotonically and, in fact, show quite irregular behavior, sometimes increasing sporadically. Here is how their graph looks on log scale:","['maxima-minima', 'infinite-product', 'sequences-and-series', 'calculus']"
2742264,A single digit written $(p-1)-times$ is divisible by $p$.,"Problem: ANY single digit written $(p-1)$ times is divisible by $p$ , with $p$ a prime $>5$ . For exemple: $222222$ is divisible by $7$ . How prove it? (if is really true)",['number-theory']
2742267,Cramer-Rao Lower Bound when the support for $x$ depends on $\theta$?,"Are there any grounds for the following claim? For distributions for which the support in $x$ depends on $\theta$, the
  CRLB $=0$. For a quick example (Casella-Berger, Example 7.3.13 ): Let $X_1,\dots,X_n$ be iid with pdf $f(x\mid\theta) = 1/\theta, 0 < x <\theta$. There, CB used the typical formula for Fisher information $I(\theta)$ to get CRLB $= \theta^2/n$ and subsequently showed that for the unbiased estimator $\hat{\theta} = \frac{n+1}{n}X_{(n)}$, $\mathrm{Var}\hat{\theta} = \frac{1}{n(n+2)}\theta^2$ which is uniformly smaller than the CRLB and thus Cramer-Rao Inequality is violated. However, my lecturer disagrees with this approach as CB ignored the indicator function $\mathbf{1}_{(0<x<\theta)}$ when taking the derivative of the log-likelihood function. His approach was to note that because the likelihood function is not continuous in $\theta$, it is not differentiable in $\theta$. In situations like these, we should define $I(\theta) = +\infty$ and thus CRLB $= 0$, in which case the Cramer-Rao Inequality was not violated. I Google'd a bit but did not find any references to validate this alternative approach although it does make some sense (although I can't seem to grasp the significance of having CRLB $= 0$). Has anyone come across something like this before?",['statistics']
2742275,Automorphism groups of $\mathbb{Z}^n $ and $\mathbb{Q}^n $ .,I was thinking about structures of  $Aut( \mathbb{Z}^n)$ and $Aut( \mathbb{Q}^n)$ . I know the group $Aut(\mathbb{Z})$ and $Aut(\mathbb{Q})$  but i dont know where i need to start for a general result. It will be good to look at an article about it but i couldn't find one. Any article suggestion or hint for starting would be great.,"['abstract-algebra', 'group-theory']"
2742321,Use Van Kampen Theorem to find the fundamental group of a circle $S^1$ joining a sphere $S^2$,"this is a topology question: Compute the fundamental group of a Christmas ball, obtained by joining a copy of the circle $S^1$ to a copy of the sphere $S^2$. My thoughts: Intuitively, the fundamental group should be Z, and a path may jump through the loop several times or not. One open set is the interior of a filled torus with the circle lying on the surface. Another set could be the whole $R^3$ with the closed disk removed. Then the first set contracts to a circle, and the second set contracts to a sphere. And I'm struggling to write out the exact fundamental group from here. Am I on the right track? Please point out the right direction if not, and help me with computing the fundamental group. Updated: using the van kampen theorem First to clarify, the ""join"" here means it is the union of the two copies, having a single point in common. We just learned van kampen theorem, and I'm thinking let X be the Christmas Ball, and let U =  $S^1$ and V = $S^2$ which X = U $\cup$ V. However, this won't satisfy that both U and V are open sets, will it? Could someone please give a formal proof on how to use van kampen theorem to solve this problem? Any help is greatly appreciated!","['general-topology', 'fundamental-groups']"
