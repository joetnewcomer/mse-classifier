question_id,title,body,tags
112592,Notation for some integrals,"I've seen some problems where the OP writes integrals in this form $$\int {dt} f\left( t \right)$$ or for double integrals $$\int {dx} \int {dtf\left( {t,x} \right)} $$ Do they represent another kind of integrals, or is it just notation?","['notation', 'multivariable-calculus', 'integration']"
112595,When does $G/Z(G)$ actually have trivial center?,"For a while I thought that, 'intuitively,' $G/Z(G)$ where $G$ is nonabelian with nontrivial center would have trivial center - that is, it would be 'totally nonabelian.'  I realize now that this is not necessarily true, as in cases where $G/Z(G)$ has order $p$ or $p^2$ where $p$ is prime. I also realize that what fed this idea and made it feel intuitive was the use of phrases about ""modding out"" or ""quotienting out"" normal subgroups.  To me, this phrasing makes it sound like we're sort of 'removing' that subgroup.  And indeed we are, in some cases.  $\mathbb{Z}_6$ has elements of orders 1, 2, 3, and 6.  Taking $\mathbb{Z}_6 / \{0,3\}$ (that is, 'quotienting out' a subgroup of order 2) we obtain $\mathbb{Z}_3$, and have effectively 'removed' the elements of order 2, since $\mathbb{Z}_3$ has only elements of orders 1 and 3. So my question is: are there specific conditions under which $G/Z(G)$ (as described above) has trivial center? More generally, can we say anything about when taking the quotient by a normal subgroup actually 'removes' the elements that characterize that subgroup?",['group-theory']
112605,Prove that for every $0<\alpha<\beta<\pi/2$: $\tan\beta/\tan\alpha>\beta/\alpha$,"Prove that for every $0<\alpha<\beta<(\pi/2)$: $$\displaystyle\frac{\tan\beta}{\tan\alpha}\gt\frac{\beta}{\alpha}$$ I tried setting a function $f(x) = \tan(x)$ and using the Mean Value theorem to prove this, but that didn't work since $f(0)$ is undefined. Does this involve some sort of trig identity?","['inequality', 'trigonometry', 'calculus']"
112619,Isometric Embedding of a separable Banach Space into $\ell^{\infty}$,The problem is: Let $X$ be a separable Banach space then there is an isometric embedding from $X$ to $\ell^{\infty}$. My efforts : I showed that there is an isometry from $X^*$ (topological dual) to $\ell^\infty$ in the following way: Let $(e_{i})_{i=1}^{\infty}$ be a dense sequence in $B_{X}$ then define $\Phi:X^*\rightarrow\ell^\infty$ by $\Phi(f)=(f(e_{i}))_{i=1}^\infty$. It is clear that $\Phi$ is an isometry. An initial idea and a secondary question Is there any canonical isometry from $X$ to $X^*$ since $X$ is separable (or not)?,"['functional-analysis', 'banach-spaces', 'analysis']"
112638,Flabby sheaves and exact sequences of sheaves - Question about proof,"I was going through this proof from Rotman's 'Introduction to homological algebra' (Pages 381-382) and I just can't seem to make sense of it, am not super well-versed in this so I don't know if it's too short and they're assuming that I know things I don't know or it's just simply going over my head. Here is the fragment verbatim: Let $\mathcal{F}$ be a sheaf over a space $X$. (i) If $0 \rightarrow \mathcal{F}' \xrightarrow{\iota} \mathcal{F} \xrightarrow{\varphi} \mathcal{F}'' \rightarrow 0$ is an exact sequence of sheaves with $\mathcal{F}'$ flabby, then $0 \rightarrow \Gamma(\mathcal{F}') \rightarrow \Gamma(\mathcal{F}) \rightarrow \Gamma(\mathcal{F}'') \rightarrow 0$ is an exact sequence of abelian groups. PROOF. It suffices to prove that $\varphi_X:\Gamma(\mathcal{F}) \rightarrow \Gamma(\mathcal{F}'')$, given by $\varphi_X:s \mapsto \varphi s$, is epic. Let $s'' \in \mathcal{F}''(X) = \Gamma(\mathcal{F}'')$. Define $\mathcal{X} = \{ (U,s):U \subseteq X$ is open,  $s\in \mathcal{F}(U),\varphi s=s''\mid U \} $. Partially order $\mathcal{X}$ by $(U,s) \preceq (U_1,s_1)$ if $U \subseteq U_1$ and $s_1 \mid U = s$. It is routine to see that chains in $\mathcal{X}$ have upper bounds, and so Zorn's Lemma provides a maximal element $(U_0,s_0)$. If $U_0=X$, then $s_0$ is a global section and $\varphi_X$ is epic. Otherwise, choose $x \in X$ with $x \notin U_0 $. Since $\varphi:\mathcal{F} \rightarrow \mathcal{F}''$ is an epic sheaf map, it is epic on stalks, and so there are an open $V \subseteq X$ with $V \ni x$ and a section $t \in \mathcal{F}(V)$ with $\varphi t = s'' \mid V$. Now $s - t \in \mathcal{F}'(U \cap V)$ (we regard $\iota: \mathcal{F}' \rightarrow \mathcal{F}$ as the inclusion), so that $\mathcal{F}'$ flabby provides $r \in \mathcal{F}'(X)$ extending $s - t$. Hence, $s = t + r \mid (U \cap V)$ in $\mathcal{F}(U \cap V)$. Therefore, these sections may be glued: there is $\tilde{s} \in \mathcal{F}(U \cup V)$ with $\tilde{s} \mid U = s$ and $\tilde{s} \mid V = t + r \mid (U \cap V)$. But $\varphi(\tilde{s}) = s''$, and this contradicts the maximality of $(U_0,s_0)$. MY QUESTIONS ARE: 1 - Why does it say that 'it is ENOUGH to prove that $\varphi_X$ is epic'? 2 - Why does it say 'If $U_0 = X$, then $s_0$ is a global section and $\varphi_X$ is epic'? Why is $\varphi_X$ epic if $s_0$ is global? 3 - I got stuck there so the rest of the proof I'm basically in La La land too","['general-topology', 'homological-algebra', 'sheaf-theory']"
112647,What is $\mathbb{N}^{<\mathbb{N}}$?,What is the definition of this symbol $\mathbb{N}^{<\mathbb{N}}$? How is it related to the infinite product $\mathbb{N}^{\mathbb{N}}$?,"['elementary-set-theory', 'definition']"
112660,How to prove $\varphi(n)=-\log \mathbb{E}[\exp(-nX)]$ is subadditive,"Let be $X$ a positive random variable. I would like to prove that the function $\varphi:\mathbb{N}\to [0,+\infty]$ defined by 
$\varphi(n)=-\log \mathbb{E}[\exp(-nX)]$
satisfies $\varphi(m+n)\leq \varphi(n)+\varphi(m)$. I tried Hölder and Jensen inequalities but I got only a weak result 
that $\varphi(n)\leq n\varphi(1)$.","['probability-theory', 'generating-functions']"
112662,Gaussian curvature of an ellipsoid proportional to fourth power of the distance of the tangent plane from the center?,"Is it true that the Gaussian curvature of an ellipsoid is proportional to fourth power of the distance of the tangent plane from the center? I can verify that it holds at the places where the major axes intersect the surface. ( Mathworld has an equation for the Gaussian curvature, which simplifies at those points.) But verifying that it holds elsewhere seems like it would get ugly. The motivation for this question is that Lord Kelvin proved that the charge density on a conducting ellipsoid is proportional to the distance of the tangent plane from the center, while McAllister (I W McAllister 1990 J. Phys. D: Appl. Phys. 23 359 doi:10.1088/0022-3727/23/3/016) finds that under certain assumptions, the charge density on a conducting surface is proportional to the fourth root of the absolute value of the Gaussian curvature. However, I think the assumptions of McAllister's result fail for the ellipsoid (actually I only have access to the abstract, so I'm not sure), so it would be nontrivial to learn that this held for the ellipsoid. (The proportionality is definitely not universal. For a pair of conducting spheres that are far apart and connected by a wire, the exponent is not 1/4. For a deep concavity, all of this definitely fails -- you get a a Faraday cage, which excludes the electric field almost completely.)",['differential-geometry']
112663,Are operator and mapping the same concept?,"I was wondering what differences and relations are between a mapping
and an operator generally? For topological vector spaces or functional analysis, it seems like an operator and a
mapping are the same concept, doesn't it? What differences are between an operator and an operation? Is an operation
a mapping from $X^n$ to $X$ for some set $X$ and some $n \in \mathbb{N}$? Thanks and regards!","['elementary-set-theory', 'topological-vector-spaces', 'functional-analysis']"
112671,Unique continued fraction,"If $x$ is a uniformly random number in $[0,1]$, what distribution should the $n$-th term in its continued fraction expansion follow? What is the expected vale of $a_n$ in $[a_0;a_1,a_2,\dots]$? Here is the expansion for $\pi$ . What does it say about a number if there is some regularity in the sequence?
Why does $e$ have regularity, but $\pi$ apparently does not?","['continued-fractions', 'probability-theory', 'real-analysis', 'pi', 'probability-distributions']"
112674,Definition of an affine subspace,"I am reading this introduction to Mechanics and the definition it gives (just after Proposition 1.1.2) for an affine subspace puzzles me. I cite: A subset $B$ of a $\mathbb{R}$-affine space $A$ modelled on $V$ is an affine subspace if there is a subspace $U$ of $V$ with the property that $y−x \in U$ for every $x,y \in B$ It later says that this definition is equivalent to to the usual one, namely that of closeness under sum with elements of a $U$, but it seems to me that there is a problem with the first definition. Just imagine the usual $\mathbb{R}^2$ plane as an affine space modeled on $\mathbb{R}^2$. According to this definition the subset $\{(0,0);(0,1)\}$ is an affine subspace, while this is not so according to the usual definition of an affine subspace. Is there an error in the book?","['affine-geometry', 'linear-algebra']"
112677,Proving that an integral domain has at most two elements that satisfy the equation $x^2 = 1$.,"I like to be thorough, but if you feel confident you can skip the first paragraph. Review: A ring is a set $R$ endowed with two operations of + and $\cdot$ such that $(G,+)$ is an additive abelian group, multiplication is associative, $R$ contains the multiplicative identity (denoted with 1), and the distributive law holds.  If multiplication is also commutative, we say $R$ is a commutative ring. A ring that has no zero divisors (non-zero elements whose product is zero) is called an integral domain, or just a domain. We want to show that for a domain, the equation $x^2 = 1$ has at most 2 solutions in $R$ (one of which is the trivial solution 1). Here's what I did: For simplicity let $1,a,b$ and $c$ be distinct non-zero elements in $R$.  Assume $a^2 = 1$.  We want to show that letting $b^2 = 1$ as well will lead to a contradiction.  So suppose $b^2 = 1$, then it follows that $a^2b^2 = (ab)^2 = 1$, so $ab$ is a solution as well, but is it a new solution?  If $ab = 1$, then $abb = 1b \Rightarrow a = b$ which is a contradiction.  If $ab = a$, then $aab = aa \Rightarrow b = 1$ which is also a contradiction.  Similarly, $ab = b$ won't work either.  So it must be that $ab = c$.  So by ""admitting"" $b$ as a solution, we're forced to admit $c$ as well. So far we have $a^2 = b^2 = c^2 = 1$ and $ab = c$.  We can procede as before as say that $(abc)^2 = 1$, so $abc$ is a solution, but once again we should check if it is a new solution.  From $ab = c$, we get $a = cb$ and $b = ac$, so $abc = (cb)(ac)(ab) = (abc)^2 = 1$.  So $abc$ is not a new solution; it's just one. At this point I'm stuck.  I've shown that it is in fact possible to have a ring with 4 distinct elements, namely $1,a,b$ and $c$ such that each satisfies the equation $x^2 = 1$ and $abc = 1$.  What am I missing?",['abstract-algebra']
112679,Why is the winding number homotopy invariant?,"It many sources it's stated that the winding number is invariant under homotopy, but I've yet to actually see why. Suppose you have the formal definition of the winding number. So for a continuous loop $\gamma\colon[\alpha,\beta]\to\mathbb{C}\setminus\{a\}$ which doesn't pass through a point $a$, one has the function $\theta(t)=\text{arg}(\gamma(t)-a)\in\mathbb{R}/2\pi\mathbb{Z}$. By the lifting lemma, there exists a continuous $\tilde{\theta}\colon[\alpha,\beta]\to\mathbb{R}$, such that $[\tilde{\theta}(t)]=\theta(t)$, and the winding number of $\gamma$ around $a$ is then defined as $$n(\gamma,a)=\frac{\tilde{\theta}(\beta)-\tilde{\theta}(\alpha)}{2\pi}.$$ Is there a straightforward proof that the winding number is invariant under homotopy with this definition for continuous loops which do not pass through $a$? Thanks.","['general-topology', 'algebraic-topology']"
112687,Integrating $\int \sin^n{x} \ dx$,"I am working on trying to solve this problem: Prove: $\int \sin^n{x} \ dx = -\frac{1}{n} \cos{x} \cdot \sin^{n - 1}{x} + \frac{n - 1}{n} \int \sin^{n - 2}{x} \ dx$ Here are the steps that I follow in the example that I am reading: $u = \sin^{n - 1}{x}$ $du = (n - 1) \cdot \sin^{n - 2}{x} \cdot \cos{x} \ dx$ $v = -\cos{x}$ $dv = \sin{x} \ dx$ $\int \sin^n{x} \ dx =  \sin^{n - 1}{x} \cdot \sin{x} \ dx$ $\int \sin^n{x} \ dx = \underbrace{\sin^{n - 1}{x}}_{u} \cdot \underbrace{-\cos{x}}_{v} - \int \underbrace{-\cos{x}}_{v} \cdot \underbrace{(n - 1) \cdot \sin^{n - 2}{x} \cdot \cos{x} \ dx}_{du}$ $\int \sin^n{x} \ dx = -\cos{x} \cdot \sin^{n - 1}{x} + (n - 1)\int \sin^{n - 2}{x} \cdot \cos^{2}{x} \ dx$ $\int \sin^n{x} \ dx = -\cos{x} \cdot \sin^{n - 1}{x} + (n - 1)\int \sin^{n - 2}{x} \cdot \left(1 - \sin^{2}{x}\right) \ dx$ Here is where I get lost. How did we go from $\int \sin^{n - 2}{x} \cdot \left(1 - \sin^{2}{x}\right) \ dx$ to $\int \sin^{n - 2}{x} \ dx - (n - 1) \int \sin^{n}{x} \ dx$? Even more specifically, where did $\sin^{n}{x}$ come from? $\int \sin^n{x} \ dx = -\cos{x} \cdot \sin^{n - 1}{x} + (n - 1)\int \sin^{n - 2}{x} \ dx - (n - 1) \int \sin^{n}{x} \ dx$ I get this part. $n\int \sin^n{x} \ dx = -\cos{x} \cdot \sin^{n - 1}{x} + (n - 1)\int \sin^{n - 2}{x} \ dx$ $\int \sin^n{x} \ dx = -\frac{1}{n} \cos{x} \cdot x \ \sin^{n - 1}{x} + \frac{n - 1}{n} \int \sin^{n - 2}{x} \ dx$ Could someone please explain what I am missing? Thank you for your time.","['trigonometry', 'calculus', 'integration']"
112698,Finding Limit $\lim_{x \to \infty} (2^x + 3^x + 5^x + 7 ^x + 11 ^x +13^x)^{\frac{1}{x}}$,Finding Limit $$\lim_{x \to \infty} (2^x + 3^x + 5^x + 7 ^x + 11 ^x +13^x)^{\frac{1}{x}}$$ So I let $$y = (2^x + 3^x + 5^x + 7 ^x + 11 ^x +13^x)^{\frac{1}{x}}$$ $\ln$ both sides: $$\ln{y} = \frac{1}{x} \ln {(2^x + 3^x + 5^x + 7 ^x + 11 ^x +13^x)}$$ Now what?,['limits']
112708,Examples of functions whose arc-length from the origin is given by their derivative,"I'm looking for functions $y:\mathbb{R}\rightarrow\mathbb{R}$ such that $$\int_{0}^{a} \sqrt{1+\left(\frac{dy}{dx}\right)^{2}} dx = \frac{dy}{dx}\Bigg|_{a}$$ (this kind of feels like a calculus-of-variations type problem, but I don't have any experience with the calculus of variations)","['plane-curves', 'ordinary-differential-equations', 'examples-counterexamples', 'arc-length']"
112724,Need help proving blockwise property of matrix multiplication.,"I want to prove the following: If $X$ and $Y$ are $n \times n$ matrices, and $$ X = \left[\begin{matrix} A
& B\\ C & D \end{matrix}\right],  Y = \left[\begin{matrix} E & F\\ G & H \end{matrix}\right] $$ where A, B, C, D, E, F, G, and H are $n/2 \times n/2$ submatrices, then
  the product $XY$ can be expressed in terms of these blocks: $$ XY = Z = \left[\begin{matrix} AE + BG & AF + BH\\ CE + DG & CF + DH \end{matrix}\right] $$ My initial thought was to use the matrix multiplication definition: $$Z_{ij} = \sum_{k=1}^n X_{ik} Y_{kj}$$ and show that each $Z_{ij}$ equals the element in $Z$ by going case by case. Case1 would be something like: $1 \leq i \leq(n/2), 1 \leq j \leq (n/2)$. So in this case, $X_{ij} = A_{ij}$ and $Y_{ij} = E_{ij}$ I am stuck here and not sure if I am on the right track. Please advise me on how to proceed from here (or suggest an alternative method).","['matrices', 'linear-algebra', 'block-matrices']"
112726,Tossing a fair coin until two consecutive tosses are the same,"A fair coin is tossed repeatedly and independently until two consecutive heads
or two consecutive tails appear. What is the PMF of the number of tosses ?","['probability-distributions', 'probability']"
112728,How do I exactly project a vector onto a subspace?,"I am trying to understand how - exactly - I go about projecting a vector onto a subspace. Now, I know enough about linear algebra to know about projections, dot products, spans, etc etc, so I am not sure if I am reading too much into this, or if this is something that I have missed. For a class I am taking, the proff is saying that we take a vector, and 'simply project it onto a subspace', (where that subspace is formed from a set of orthogonal basis vectors). Now, I know that a subspace is really, at the end of the day, just a set of vectors. (That satisfy properties here ). I get that part - that its this set of vectors. So, how do I ""project a vector on this subspace""? Am I projecting my one vector, (lets call it a[n]) onto ALL the vectors in this subspace? (What if there is an infinite number of them?) For further context, the proff was saying that lets say we found a set of basis vectors for a signal, (lets call them b[n] and c[n]) then we would project a[n] onto its signal subspace . We project a[n] onto the signal-subspace formed by b[n] and c[n]. Well, how is this done exactly?.. Thanks in advance, let me know if I can clarify anything! P.S. I appreciate your help, and I would really like for the clarification to this problem to be somewhat 'concrete' - for example, something that I can show for myself over MATLAB. Analogues using 2-D or 3-D space so that I can visualize what is going on would be very much appreciated as well. Thanks again.","['matrices', 'linear-algebra', 'signal-processing']"
112739,reflexive transitive closure or transitive closure,"This a problem on the definition of reflexive transitive closure in Elements of the Theory of Computation (H.R.Lewis). Definition 1.6.1: Let $R \subseteq A^2$ be a directed graph defined on a set $A$. The reflexive transitive closure of $R$ is the relation $$R^* = \{ (a,b) : a, b \in A\text{ and there is a path from }a\text{ to }b\text{ in }R\}\;.$$ Also, an example is given as $$R = \{(a_1,a_2), (a_1,a_3), (a_1,a_4), (a_2,a_3), (a_3,a_4)\}$$ and its reflexive transitive closure $$R^* = \{(a_1,a_1), (a_1,a_2), (a_1,a_3), (a_1,a_4), (a_2,a_2), (a_2,a_3), (a_2,a_4), (a_3,a_3), (a_3,a_4), (a_4,a_4) \}\;.$$ My doubt is whether the example goes with the definition and whether the definition is correct itself. By the definition, if $(a, b) \in R^*$ then there is a path from $a$ to $b$ in $R$. However, I can not find a path from $a_1$ to $a_1$ in $R$ but $(a_1,a_1) \in R^*$ as in the example. I think what the definition wants to say is the transitive closure of $R$. Edit: here's how the author defines path A path in a binary relation $R$ is a sequence $(a_1, \ldots, a_n)$ for some $n \geq 1$ such that $(a_i, a_{i+1}) \in R$ for $i = 1, \ldots, n-1$; this path is said to be from $a_1$ to $a_n$. The length of a path $(a_1, \ldots, a_n)$ is $n$. Although this doesn't seem to clear things up, I find the definition of path in directed graph in Discrete Mathematics and its Applications (Kenneth H.Rosen) A path from $a$ to $b$ in the directed graph $G$ is a sequence of edges $(x_0,x_1), (x_1,x_2), (x_2,x_3), \ldots, (x_{n-1},x_n)$ in $G$, where $n$ is a nonnegative integer, and $x_0=a$ and $x_n=b$, that is, a sequence of edges where the terminal vertex of an edge is the same as the initial vertex in the next edge in the path. This path is denoted by $x_0, x_1, x_2, \ldots, x_{n-1}, x_n$ and has length $n$. We view the empty set of edges as a path from $a$ to $a$. Thus, I was wrong about the length of $(a,a)$. It is of length 1 not 0. Moreover, the path denoted by just $x_0$ has length $0$. Since there is no edge of this path we view it from $a$ to $a$. It follows that $(a,a) \in R^*$ no matter whether $(a,a) \in R$ which satisfies the definition of reflexive transitive closure .","['discrete-mathematics', 'elementary-set-theory']"
112748,Does the ratio $\mathrm{Var}(x)/E(x)$ have any statistical meaning?,I am wondering how to compare the volatility of two sets of samples. Can I consider the ratio $\mathrm{Var}(x)/E(x)$ as a normalized variance?,['statistics']
112752,Prove: $\binom{n}{0}F_0+\binom{n}{1}F_1+\binom{n}{2}F_2+\cdots+\binom{n}{n}F_n=F_{2n}$,"Prove: $\binom{n}{0}F_0+\binom{n}{1}F_1+\binom{n}{2}F_2+\cdots+\binom{n}{n}F_n=F_{2n}$; 
I was stuck with this question for a while... Help me please!!! Thanks!!!","['elementary-number-theory', 'fibonacci-numbers', 'summation', 'binomial-coefficients', 'combinatorics']"
112767,"In algebraic geometry, how do you explicitly find the strict transform?","Let $X = Z(xy - zw)$ in $\mathbb{A}^4$ and $Y = Z(x, z)$. If $\pi: B \rightarrow \mathbb{A}^4$ is the blow up of $Y$, then how can I find the strict transform of $X$ and the exceptional divisor? I honestly have no idea how to find the strict transform in this case. I'm only fairly comfortable with blowing up single points. This is what I have so far, gathered from random lecture notes. It only deals with the exceptional divisor and is rather messy: Let $E = \pi^{-1}(Y)$ and $J = I(Y) = (x, z)$. By Theorem 14.7,
$B(J) = U_1 \cup U_2$ where $U_1, U_2$ are affine and $\mathcal{O}_{B(J)}(U_1) = k[\mathbb{A}^{4}][z/x] = k[x, y, z/x]$, $\mathcal{O}_{B(J)}(U_2) = k[\mathbb{A}^{4}][x/z] = k[y, z, x/z].$ We have $x = 0$ is a local equation of $E$ in $U_1$, $z = 0$ is a local equation of $E$ in $U_2$, and $k[E \cap U_1] = k[x, y, z/x]/(x)k[x, y, z/x] = k[y, z/x]$, $k[E \cap U_2] = k[y, z, x/z]/(x)k[y, z, x/z] = k[y, x/z].$ (I do not recognize what this means about $E$, but $E \not =            
\mathbb{A}^{2}$. Random guess: $E$ is the subset of $\mathbb{P}^{2}$ with coordinates
$x, y, z$ described by $(x : xy : z)$ for some $(x, y, z) \in U_1$ and
$(x : yz : z)$ for some $(x, y, z) \in U_2$. Using $U_1 \cap E =           
Z(J\mathcal{O}_{B(J)}(U_1)) = Z(x)$ and $U_2 \cap E = Z(J\mathcal{O}_{B(J)}(U_2)) =       
Z(z)$, we get $E = \{(x : 0 : z) \mid x \in \pi_1(U_1), z \in                 
\pi_3(U_2)\}$ . . . )",['algebraic-geometry']
112768,The Implicit Function Theorem for complex polynomials,"I'm looking for a reference that proves implicit function theorem for polynomials in two variables over the complex numbers via the real version. Such a theorem is needed, for example, in the theory of algebraic curves, in order to construct charts to prove they form a complex manifold. Also apparently a higher dimension version is useful for dealing with complete intersection curves in $\mathbb{P}^n$. I would also appreciate any reference on that. I've seen proofs of the implicit function theorem for real spaces, for example in Spivak's Calculus on Manifolds, but I've never been able to find a proof of the complex version.","['manifolds', 'reference-request', 'complex-analysis']"
112786,Convergence in law and uniformly integrability,"I'm looking for an elementary way of showing the following. If $(X_n)$ and $X$ are random variables such that $X_n \to X$ in distribution and such that $\{X_n\mid n\geq 1\}$ are uniformly integrable, then $E[X_n]\to E[X]$. I've seen another topic on this, but the solution given there is using Skorokhod's theorem stating that convergence in distribution is equivalent to almost-sure convergence of copies of the random variables in some abstract probability space. I would like to do without that if possible. Thanks in advance!","['probability-theory', 'probability']"
112790,Why are maximum likelihood estimators used?,"Is there a motivating reason for using maximum likelihood estimators? As for as I can tell, there is no reason why they should be unbiased estimators (Can their expectation even be calculated in a general setting, given that they are defined by a global maximum?). So then why are they used?","['statistics', 'maximum-likelihood']"
112804,What is the significance of theoretical linear algebra in machine learning/computer vision research?,"I am a computer science research student working in application of Machine Learning to solve Computer Vision problems. Since, lot of linear algebra(eigenvalues, SVD etc.) comes up when reading Machine Learning/Vision literature, I decided to take a linear algebra course this semester. Much to my surprise, the course didn't look at all like Gilbert Strang's Applied Linear algebra(on OCW) I had started taking earlier. The course textbook is Linear Algebra by Hoffman and Kunze . We started with concepts of Abstract algebra like groups, fields, rings, isomorphism, quotient groups etc. And then moved on to study ""theoretical"" linear algebra over finite fields, where we cover proofs for important theorms/lemmas in the following topics: Vector spaces, linear span, linear independence, existence of basis.
  Linear transformations. Solutions of linear equations, row reduced
  echelon form, complete echelon form,rank. Minimal polynomial of a
  linear transformation. Jordan canonical form. Determinants.
  Characteristic polynomial, eigenvalues and eigenvectors. Inner product
  space. Gram Schmidt orthogonalization. Unitary and Hermitian
  transformations. Diagonalization of Hermitian transformations. I wanted to understand if there is any significance/application of understanding these proofs in machine learning/computer vision research or should I be better off focusing on the applied Linear Algebra?","['linear-algebra', 'machine-learning']"
112813,Simultaneous Differential Equations,"How can we solve the simultaneous equations: $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot x}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{x\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ $$\frac{d}{dt}\left[\frac{1}{\sqrt{1-x^2-y^2}}\frac{\dot y}{\sqrt{\dot x^2+\dot y^2}}\right]=\frac{y\sqrt{\dot x^2+\dot y^2}}{(1-x^2-y^2)^\frac{3}{2}}$$ I am hoping that the solution is $y=x$, fingers-crossed.","['ordinary-differential-equations', 'integration']"
112829,Rotations on standard probability space,"Let $(I,B,\mu)$ be the standard probability space with $I=[0,1]$ , $B=$ Borel sets, and $\mu$ is Lebesgue measure. Now, for each $n\in N$, let $F_n$ be the rotation $F_n(x)=x+1/n \pmod 1$.
Is it correct that for every Borel set $E$ , $ \mu (F_nE\;\triangle\; E)\to0$ as $n\to \infty$? thanks",['measure-theory']
112833,Convexity of $\theta(q)$,"Define Jacobi's (fourth) theta function with argument zero and nome $q$: $$\theta(q) = 1+2\sum_{n=1}^\infty (-1)^n q^{n^2}$$ plot of the function via Wolfram|Alpha plot of the function via Sage I am looking for a simple/standard/illuminating proof of the fact that $\theta(q)$ is convex for $q\in[0,1]$. The proof I found goes like this: We have $$\theta'(q) = 2\sum_{n=1}^\infty (-1)^n n^2 q^{n^2-1}$$ and one can show that for some $q_0\in(0,1)$, $n^2q^{n^2-1} - (n+1)^2q^{(n+1)^2-1}$ is increasing in $[0,q_0]$ for any $n\ge 2$. This gives convexity of $\theta(q)$ in $[0,q_0]$. For the remaining values of $q$, one uses the representation of $\theta$ as a sum over Gaussian kernels: $$\theta(e^{-\pi^2t/2}) = 2 \sqrt{\frac{2}{\pi t}}\sum_{n=1}^\infty \exp\left(-\frac{(2n-1)^2}{2t}\right)$$ With this representation, one can show that the second derivative (wrt $q$) of each summand is positive for $q \ge q_1$, with $q_1 < q_0$. This yields convexity of theta. I don't like this proof, because it requires calculating $q_1$ and $q_0$ explicitly and it is not very illuminating. I tried playing around with the representation of $\theta(q)$ as the infinite product $$\theta(q) = \prod_{n=1}^\infty (1-q^{2n-1})^2(1-q^{2n}),$$ but didn't manage to find anything, except that the partial products $$\prod_{n=1}^N (1-q^{2n-1})^2(1-q^{2n})$$ all seem to be convex in $[0,1]$, which would prove the statement. All suggestions are very welcome!","['special-functions', 'elliptic-functions', 'theta-functions', 'analysis', 'complex-analysis']"
112835,"$y'''-y=x^{2}$ has solution -- `""multiplicity""`?","The page 667 of the book (sorry not in English) claims $y'''-y=x^{2}$ to have the solution $$y(x)=C_{1}e^{x}+e^{-x/2}\left(C_{2} \cos \left( \frac{\sqrt{3}x}{2} \right)+C_{3} \sin\left(\frac{\sqrt{3} x}{2}\right)\right) -x^{2}.$$ The book mentions that with the $m$ -multiple real root solution is $x^{k}e^{rx}$ and with $m$ -multiple conjugate pair $\alpha\pm i\beta$ the solution is $x^{k}e^{\alpha x}\cos(\beta x), x^{k} e^{\alpha x} \sin(\beta x)$ where $k=0...m-1$. Let's check how to use it in this example. We have 3th order DY so $m=3$. But what kind of roots does this have $r^{3}-1=0$?","['ordinary-differential-equations', 'terminology']"
112842,mathematical difference between column vectors and row vectors,"I'm writing a mathematical library; and I have an idea where I want to automatically turn column matrices and row matrices to vectors, with all of the mathematical properties of a vector. Answer I'm looking for: Someone with good mathematical reasoning explaining why: column matrices, column vectors, row matrices, row vectors should not be treated as the same thing. (The library will ofcourse understand operations like [[1,2],[3,4]] * [1,2], where [1,2] is a vector) or: some kind of showcase or example where it is impossible for a library that can't differentiate between row vectors and column vectors to know which one of several possible answers are correct. or: some kind of evidence that it is in fact possible to do this. please note: inner vector multiplication will be easily integrated by using a special function for that function rather than the * sign.","['linear-algebra', 'vector-analysis']"
112844,$C^*$-algebra which is also a Hilbert space?,"Does there exist a nontrivial (i.e. other than $\mathbb{C}$) example of a $C^*$-algebra which is also a Hilbert space (in the same norm, of course)? For $\mathbb{C}^n$ with $n > 1$ the answer is no by uniqueness of norm in $C^*$-algebras, since $\mathbb{C}^n$ is a $C^*$-algebra in the $\ell^\infty$ norm, which is not given by an inner product.  What about more generally?","['c-star-algebras', 'operator-algebras', 'functional-analysis']"
112853,Partitioned Multivariate Gaussian,"My question is on partitioned multivariate Gaussians which are shown as $$
f(x;\mu,\Sigma) = 
\frac{ 1}{(2\pi)^{(p+q)/2} \det(\Sigma)^{1/2}} 
\exp 
\bigg\{ 
-\frac{ 1}{2}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]^T
\left[\begin{array}{rr}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]^{-1}
\left[\begin{array}{r}
x_1 - \mu_1\\
x_2 - \mu_2
\end{array}\right]
\bigg\}
$$ and $$
\Sigma = 
\left[\begin{array}{rr}
\Sigma_{11} & \Sigma_{12}\\
\Sigma_{21} & \Sigma_{22}
\end{array}\right]
$$ In order to derive marginal and conditional densities for pieces of this Gaussian, in book Introduction to Graphical Models chapter 12, Jordan shows These are the final steps of this algebra, right after Schur's complement is used which was used to obtain the pieces for the inverse. However in the last step, the transition from 2nd to 3rd equation, I am not able to follow the derivation, for example in the last equation $-\Sigma_{22}^{-1}\Sigma_{21}$ seems to have disappeared, but it is there in the second equation. Does anyone know how? Is it because transpose of $\Sigma_{12}$ is $\Sigma_{21}$, he had to pull out transpose operator to a larger group of symbols, therefore had to use $(\Sigma_{21}^T)^T$, and inside the paranthesis it is $\Sigma_{12}$.. maybe? Thanks,","['statistics', 'linear-algebra']"
112859,Basis for $\mathbb R$ over $\mathbb Q$,Give me some examples of basis for $\mathbb R$ (as vector space over field $\mathbb F=\mathbb Q$ ). Thanks.,"['linear-transformations', 'linear-algebra']"
112865,Finding derivative of $\sqrt[3]{x}$ using only limits,"I need to finding derivative of $\sqrt[3]{x}$ using only limits So following tip from yahoo answers : I multiplied top and bottom by conjugate of numerator $$\lim_{h \to 0} \frac{\sqrt[3]{(x+h)} - \sqrt[3]{x}}{h} \cdot \frac{\sqrt[3]{(x+h)^2} + \sqrt[3]{x^2}}{\sqrt[3]{(x+h)^2} + \sqrt[3]{x^2}}$$ $$= \lim_{h \to 0} \frac{x+h-x}{h(\sqrt[3]{(x+h)^2} + \sqrt[3]{x^2})}$$ $$= \lim_{h \to 0} \frac{1}{\sqrt[3]{(x+h)^2} + \sqrt[3]{x^2}}$$ $$= \frac{1}{\sqrt[3]{x^2} + \sqrt[3]{x^2}}$$ $$= \frac{1}{2 \sqrt[3]{x^2}}$$ But I think it should be $\frac{1}{3 \sqrt[3]{x^2}}$ (3 instead of 2 in denominator?) UPDATE I found that I am using the wrong conjugate in step 1. But this (wrong) conjugate gives the same result when I multiply the numerator by it. So whats wrong with it? (I know its wrong, but why?)",['limits']
112876,Geometric explanation of the product metric,"Can someone describe to me the geometric intuition behind using a mapping $$ ((x_1,y_1),(x_2,y_2)) \mapsto \frac{d_1(x_1,y_1)}{1+d_1(x_1,y_1)} + \frac{1}{2} \frac{d_2(x_2,y_2)}{1+d_2(x_2,y_2)} $$
to define a metric on the product of the metric spaces $(X,d_1),(Y,d_2)$ ? Of course I can check the axioms, but that doesn't give me any insight; so why was it defined like this and not differently (especially, why the $\frac{1}{2}$)?
Why not use $ ((x_1,y_1),(x_2,y_2))$ $ \mapsto d_1(x_1,y_1) $ $+ d_2(x_2,y_2) $ ?","['geometry', 'metric-spaces', 'intuition']"
112881,A question about divisibility.,"What I've observed :
Pick any $3$ random positive integers, say $a$, $b$, $c$ which are not of the form $0\pmod{3}$ then one and only one of $a+b$, $b+c$, $c+a$, $a+b+c$  is always a multiple of $3$. What I've generalized :
Let $a_1$$;$ $a_2$; $...;$ $a_k$ be $k$ positive integers with $a_r$ $\not=$ $0\pmod{k}$ $\forall$ $1$ $\le$ $r$ $\le$ $k$. Then there exist $m$ and $n$ with $1$ $\le$ $m$ $\le$ $n$ $\le$ $k$ such that $\sum_{i=m}^n a_i$ is divisible by $k$. My question : Whether or not such a generalization is true. Note : The condition $a_r$ $\not=$ $0\pmod{k}$ $\forall$ $1$ $\le$ $r$ $\le$ $k$ is given to avoid the trivial solution, it being $m$ $=$ $n$ $=$ $r$.","['elementary-number-theory', 'algebra-precalculus']"
112888,Existence of Smooth Functions which Satisfy a Condition Regarding $C^\infty$-Functions in $\mathbb{R}^n$,"I recently ran into the following exercise: If $R(x)$ is a $C^\infty$-function near the origin in $\mathbb{R}^n$, satisfying $R(0)=0$ and $DR(0)=0$, show that there exist smooth functions $r_{jk}(x)$ such that$$R(x)=\sum r_{jk}(x)x_jx_k.$$ I know that the fundamental theorem of calculus applied to $\varphi(t)=F(x+ty)$ yields$$F(x+y)=F(x)+\int_0^1DF(x+ty)y\text{ }dt,$$provided that $F$ is $C^1$. Using this result, we can write $R(x)=\Phi(x)x$, $\Phi(x)=\int_0^1DR(tx)\text{ }dt$, since $R(0)=0$. Then $\Phi(0)=DR(0)=0$, so we can apply it again to obtain $\Phi(x)=\Psi(x)x$. I get stuck here, however. Do you guys have any ideas on how to begin to prove that such smooth functions exist? Thanks in advance.",['ordinary-differential-equations']
112893,Partitioning a set of integers into 4 subsets with equal subset sums,"Given $n (n \leq 20)$ positive integers and each integer is $\leq 10,000$. Can they be partitioned into $4$ subsets such that sum of the subsets are pairwise equal to each other. I am interested in an algorithmic solution.","['discrete-mathematics', 'computer-science', 'algorithms', 'combinatorics']"
112897,The space of smooth sections of a vector bundle.,"Let $M$ be a compact, finite-dimensional manifold and $\pi : \mathrm{B} \rightarrow M$ a vector bundle over $M$ whose typical fiber is $\mathbb{R}^n$. Denote by $\mathcal{C}^{\infty}(M, \mathrm{B})$ the vector space of all smooth sections of $\mathrm{B}$. If we choose Riemannian metrics on (the fibers of) $\mathrm{B}$ and on $M$, and then introduce a metric connection $D$ on $B$, we may define a family of seminorms on $\mathcal{C}^{\infty}(M, \mathrm{B})$ by
$$\| s \|_n = \sum_{i = 0}^n~\sup_{x \in M} |D^js(x)|,$$
where $|D^0s(x)|$ is just $|s(x)|$, and for $j \geq 1$,
$$|D^js(x)| = \sup |(D_{v_1} \circ \dots \circ D_{v_j}s)(x)|,$$
the supremum being taken over all $(v_1, \ldots, v_j) \in (T_xM)^j$ with $|v_k| = 1$, for $k = 1, \ldots, j$. It's not hard to prove that if $\{s_n\}_{n \in \mathbb{N}}$ is a Cauchy sequence in $\mathcal{C}^{\infty}(M, \mathrm{B})$ with respect to this family of seminorms, then it converges to a continuous section $s : M \rightarrow \mathrm{B}$. I'd like to prove that $s$ is actually smooth. QUESTION : What would be the best ($\sim$ least messy, shortest) way to prove this? Thanks.","['vector-bundles', 'differential-geometry']"
112898,Smoothness over a field and regularity,"Hartshorne, Algebraic Geometry In example III.10.0.3, Hartshorne remarks that with k algebraically closed, X smooth of dimension n over Spec k is equivalent to X regular of dimension n.  He references II.8.8. However II.8.8 requires that when one looks at a local ring B, the relative sheaf of differentials for this local ring must be a free B-module.  This is certainly true if X is irreducible.  But what if X is not irreducible? So my overall question: is the statement from III.10.0.3 true for X not irreducible?","['algebraic-geometry', 'schemes']"
112905,Curve - non singular curve and its genus,"Help me please with this problem:$ X \subset \mathbb{P}^{2}$
defined as $x^{3}y+y^{3}z+z^{3}x=0$ 1.Prove X - non singular curve and find its genus.
2.Prove X - maximal curve over $F_{8}$ field, and find all points of X. Thanks a lot!","['arithmetic-geometry', 'geometry', 'algebraic-geometry', 'abstract-algebra']"
112915,Special privilege enjoyed by Elliptic Curves with Complex Multiplication,"I think after reading the title one may understand the intention of me, this question is concerned about the Elliptic curves having a Complex Multiplication. I have been reading many theorems, ( celebrated papers of Zagier and Kolyvagin ). In majority, much of the proofs I came across, considers Elliptic Curves with Complex Multiplication. And many results have been discovered in that direction. I know what is meant by Complex Multiplication and endomorphism rings. But if some one asks for the reason behind such privilege enjoyed by the elliptic curves with CM ( Complex Multiplication ), what are the precise things one can tell ? To put in other way, how come the proofs are discovered about Elliptic curves with CM are discovered so easily and why not for the other case ? ( Is there some bird's eye view ? ) Thank you.","['number-theory', 'algebraic-geometry', 'elliptic-curves', 'algebraic-number-theory', 'reference-request']"
112918,Integral $\int\csc^3{x} \ dx$,"I found these step which explain how to integrate $\csc^3{x} \ dx$. I understand everything, except the step I highlighted below. How did we go from:
$$\int\frac{\csc^2 x - \csc x \cot x}{\csc x - \cot x}\,dx%$$
to 
$$\int \frac{d(-\cot x + \csc x)}{-\cot x + \csc x} \quad?$$ Thank you for your time!
$$
\int \csc^3 x\,dx = \int\csc^2x \csc x\,dx$$
To integrate by parts, let $dv = \csc^2x$ and $u=\csc x$. Then $v=-\cot x$ and $du = -\cot x \csc x \,dx$.
Integrating by parts, we have:
$$\begin{align*}
\int\csc^2 x \csc x \,dx &= -\cot x \csc x - \int(-\cot x)(-\cot x\csc x\,dx)\\
&= -\cot x \csc x - \int \cot^2 x \csc x\,dx\\
&= -\cot x\csc x - \int(\csc^2x - 1)\csc x\,dx &\text{(since }\cot^2 x = \csc^2-1\text{)}\\
&= -\cot x \csc x - \int(\csc^3 x - \csc x)\,dx\\
&= -\cot x\csc x - \int\csc^3 x\,dx + \int \csc x\,dx
\end{align*}$$
From
$$\int \csc^3 x\,dx = -\cot x\csc x - \int\csc^3 x\,dx + \int \csc x\,dx$$
we obtain
$$\begin{align*}
\int\csc^3x\,dx + \int\csc^3 x\,dx &= -\cot x \csc x + \int\csc x\,dx\\
2\int\csc^3 x\,dx &= -\cot x\csc x + \int\csc x\,dx\\
\int\csc^3x\,dx &= -\frac{1}{2}\cot x\csc x + \frac{1}{2}\int\csc x\,dx\\
&=-\frac{1}{2}\cot x\csc x + \frac{1}{2}\int\frac{\csc x(\csc x - \cot x)}{\csc x - \cot x}\,dx\\
&= -\frac{1}{2}\cot x \csc x + \frac{1}{2}\int\frac{\csc^2 x - \csc x\cot x}{\csc x - \cot x}\,dx\\
&= -\frac{1}{2}\cot x \csc x + \frac{1}{2}\int\frac{d(-\cot x+\csc x)}{-\cot x +\csc x}\\
&= -\frac{1}{2}\cot x\csc x + \frac{1}{2}\ln|\csc x - \cot x|+ C
\end{align*}$$","['trigonometry', 'calculus', 'integration']"
112926,"Global sections of $\mathcal{O}(-1)$ and $\mathcal{O}(1)$, understanding structure sheaves and twisting.","In chapter 2 section 7 (pg 151) of Hartshorne's algebraic geometry there is an example given that talks about automorphisms of $\mathbb{P}_k^n$ . In that example Hartshorne states that $\mathcal{O}(-1)$ has no global sections. However, we know that $\mathcal{O}(1)$ is generated by global sections. This is stated at the first of the section that if $\mathbb{P}_k^n = \operatorname{Proj} k[x_0,...,x_n]$ then the $x_0,...,x_n$ give rise to global sections $x_0,...,x_n\in\Gamma(\mathbb{P}_k^n,\mathcal{O}(1))$ . I guess I don't understand this twisted structure sheaf very well, or to be honest I don't think I understand structure sheaves in general as well as I would like. The twisting part seems simple at first- you shift the grading of Proj over and then take the structure sheaf.  Admittedly I don't feel comfortable using the structure sheaf other than using the basic facts about it that Hartshorne gives when it is introduced. If anyone could give some insight as to what's going on here or how I might be able to understand this better it would be much appreciated.
Thanks.",['algebraic-geometry']
112938,When is $\sin x$ an algebraic number and when is it non-algebraic?,"Show that if $x$ is rational, then $\sin x$ is algebraic number when $x$ is in degrees and $\sin x$ is non algebraic when $x$ is in radians. Details: so we have $\sin(p/q)$ is algebraic when $p/q$ is in degrees, that is what my book says. of course $\sin (30^{\circ})$, $\sin 45^{\circ}$, $\sin 90^{\circ}$, and halves of them is algebraic. but I'm not so sure about $\sin(1^{\circ})$. Also is this is an existence proof or is there actually a way to show the full radical solution. One way to get this started is change degrees to radians. x deg = pi/180 * x radian. 
So if x = p/q, then sin (p/q deg) = sin ( pi/180 * p/q rad). Therefore without loss of generality the question is show sin (pi*m/n rad) is algebraic. and then show sin (m/n rad) is non-algebraic.","['trigonometry', 'calculus', 'number-theory']"
112942,Solve equations for distance,"Let's say that I have a camera positioned in front of a wall. The camera is looking straight at the closest point on the wall. The camera can see 2 horizontal lines on the wall (extending to infinity). Taking into account the camera's field of view and the y position (up and down) of the lines in the camera's viewport, I can compute the angle between the 2 lines ($a$). I also know the distance of both of the lines to the floor ($i$ and $j$). Using this information, I need to compute the distance of the camera to the base of the wall ($d$). Here is a side-view diagram of this problem: Here are the 2 equations (with 2 unknowns) that I have come up with: $$\sin(a+b)d-\sin(b)d=j$$
$$\sin(b)d=i$$ So, the question is how to solve these 2 equations for $b$ and $d$. I feel like I'm missing something really simple. I'd really appreciate it if someone could point me in the right direction.","['geometry', 'trigonometry']"
112958,The error term in Taylor series and convolution.,"I've been wondering a lot why is the remainder of the Taylor expansion of a function, $R_n(x)$, expressed (in one of the many forms) as something very similar to aconvolution. Precisely: $$R_n(x) = \int_a^x \frac{(x-t)^n}{n!}f^{(n+1)}(t)dt$$ and if we're dealing with the Mc Laurin series, then it is a convolution: $$R_n(x) = \int_0^x \frac{(x-t)^n}{n!}f^{(n+1)}(t)dt$$ I studied this particular error formula because I find it very elegant, and it isn't hard to manipulate. I know the proof of this formula is simply made by induction on $n$, we start of by the linear approximation: $$f(x)=f(a)+f'(a)(x-a)+R_1(x)$$, so that $$R_1(x) = f(x)-f(a) - f'(a) (x-a)$$ $${R_1}(x) = \int\limits_a^x {f'\left( t \right)dt}  - \int\limits_a^x {f'(a)dt} $$ $${R_1}(x) = \int\limits_a^x {f'\left( t \right) - f'\left( a \right)dt} $$ So now we integrate by parts with $$f'\left( t \right) - f'\left( a \right) = u$$ $$t - x = v$$ to get $${R_1}(x) = \int\limits_a^x {\left( {x - t} \right)f''\left( t \right)dt} $$ We can similarily do this with $R_2(x)$, since $${R_2}(x) = {R_1}(x) - f''\left( a \right)\frac{{{{\left( {x - a} \right)}^2}}}{{2!}}$$ $${R_2}(x) = \int\limits_a^x {\left( {x - t} \right)f''\left( t \right)dt}  - \int\limits_a^x {\left( {x - t} \right)f''\left( a \right)dt} $$ $${R_2}(x) = \int\limits_a^x {\left( {x - t} \right)\left( {f''\left( t \right) - f''\left( a \right)} \right)dt} $$ So again integrating by parts gives $${R_2}(x) = \int\limits_a^x {\frac{{{{\left( {x - t} \right)}^2}}}{{2!}}f'''\left( t \right)dt} $$ Q1 : Can this be proved in an alternative way, noticing that the error is a 
convolution between $\dfrac{{{x^n}}}{{n!}}$ and $f^{(n+1)}(x)$? Q2 : How can this be interpreted in the scope of convolution ""theory"" and similar ideas?","['convolution', 'integration', 'taylor-expansion']"
112985,"""Every linear mapping on a finite dimensional space is continuous""","From Wiki Every linear function on a finite-dimensional space is continuous. I was wondering what the domain and codomain of such linear function are? Are they any two topological vector spaces (not necessarily the same), as along as the domain is finite-dimensional? Can the codomain be a different normed space (and may not be finite-dimensional)? I asked this because I saw elsewhere the same statement except the domain is a finite-dimensional normed space, and am also not sure if the codomain can be a different normed space (and may not be finite-dimensional). Thanks and regards!","['linear-transformations', 'normed-spaces', 'continuity', 'topological-vector-spaces', 'functional-analysis']"
112989,expected number of coin flips given a condition,"I have a fair coin, and I flip it until the following condition is met: #heads - #tails = N  OR  #tails - #heads = N where $N \geqslant 2$. What is the expected number of times I flip the coin?",['probability']
113004,Some questions about set closure / Kuratowski closure.,"Here is the page that is confusing me: Page 25 of General Topology (Willard) Definition 3.5 If $X$ is a topological space, and $E\subset X$, then the closure of $E$ in $X$ is the set 
  $$\overline{E} = \mathrm{Cl}(E) = \bigcap \{K\subset X\mid K\text{ is closed and }E\subset K\}.$$ [...] Lemma 3.6 If $A\subset B$, then $\overline{A}\subset \overline{B}$. [...] Theorem 3.7 The operation $A\mapsto \overline{A}$ in a topological space has the following properties: K-a) $E\subset \overline{E}$ K-b) $\overline{(\overline{E})}=\overline{E}$ K-c) $\overline{A\cup B} =\overline{A}\cup\overline{B}$ K-d) $\overline{\varnothing} = \varnothing$ K-e) $E$ is closed in $X$ iff $\overline{E}=E$. Moreover, given a set $X$ and a mapping $A\mapsto \overline{A}$ of $\mathscr{P}(X)$ into $\mathscr{P}(X)$ satisfying K-a through K-d, if we define closed sets using K-e, the result is a topology on $X$ whose closure operator is just the operation $A\mapsto\overline{A}$ we began with. Everything was fine until this page, and suddenly I'm utterly confused. Definition 3.5 defines what a closure is, and lemma 3.6 follows directly from this definition and is, it seems to me, almost self-evident... and then Theorem 3.7 happens and I've no idea what's going on. In the second-to-last paragraph, Willard implies that, in the collection F of all sets $\bar{A}=A$, $A \subset B \implies \bar{A} \subset \bar{B}$ does not directly follow from lemma 3.6 . We proceed now to the second part of the theorem. Let $X$ be any set and $A\to \overline{A}$ a mapping of $\mathscr{P}(X)$ into $\mathscr{P}(X)$ satisfying K-a through K-d. Let $\mathscr{F}$ be the collection of all sets $A$ such that $A=\overline{A}$. The assertion is that $\mathscr{F}$ satisfies F-a through F_c of Theorem 3.4. First note that if $A\subset B$, then by K-c, $\overline{B}=\overline{A}\cup\overline{B-A}$, so that $\overline{A}\subset \overline{B}$ (why couldn't we just refer to Lemma 3.5?) Wouldn't it follow even more clearly so? Isn't this valid: $ \bar{A} (= A) \subset \bar{B} (= B) \implies Lemma \; 3.6$ In fact, I'm having trouble understanding the significance of this Kuratowski closure operation altogether... don't K-a to K-e all follow from * Definition 3.5 ? I think I might need a more extensive article on this topic, because I don't fully understand the significance of the material on this page.",['general-topology']
113010,Do gonal morphisms have non-trivial automorphisms,"Let $X$ be a compact connected Riemann surface. Let $\pi:X\to \mathbf{P}^1$ be a gonal morphism, i.e., a morphism of minimal degree. Can $\pi$ have non-trivial automorphisms? (An automorphism of $\pi$ is an automorphism of $\sigma:X\to X$ of $X$ such that $\pi\circ \sigma = \pi$.) Is this true if $\pi$ is hyperelliptic, i.e., if $\deg \pi = 2$ and $g\neq 1$?","['riemann-surfaces', 'algebraic-geometry', 'algebraic-curves']"
113012,How to find $f'(\frac{\pi}{2})$ knowing $f(x) = \int_0^{g(x)}(1+t^3)^{-\frac12} \mathrm{d}t$?,Argh I had a picture for this but don't have enough reputation for anything on here. I also don't understand how to insert the math notation in here. If $f(x) = \int_0^{g(x)}(1+t^3)^{-\frac12} \mathrm{d}t$ where $g(x) = \int_0^{\cos x}(1+\sin (t^2))\mathrm{d}t$ I have to find $f'(\frac{\pi}{2}).$ I know it has something to do with substitution and I've tried integrating by parts and things like that too  but its not working out. I think there is something about the way that the questions been asked thats not helping anyway that is the first question. There'll be more in the future I'm sure.,"['calculus', 'integration', 'derivatives']"
113019,Proof of Vitali's Convergence Theorem,"This is an exercise from Rudin's Real and Complex Analysis . Prove the following convergence theorem of Vitali: Let $\mu(X)\lt \infty$ and suppose a sequence of functions, $\{f_n\}$ is uniformly integrable, $f_n(x)\to f(x)$ a.e. as $n\to \infty$, and $|f(x)|\lt \infty$ a.e., then $f\in L^1(\mu)$ and $$ \lim_{n\to\infty} \int_X |f_n-f|~d\mu = 0.$$ Attempt: Since $f_n$ is uniformly integrable, $\exists~\delta \gt 0$ such that whenever $\mu(E)\lt \delta$, we have $$\int_E |f_n|~d\mu \lt \frac{\varepsilon}{3} \quad \forall~n.$$ Since $\mu(X)\lt \infty$, Egoroff says that we can find a set $E$ such that $f_n \to f$ uniformly on $E^c$ and $\mu(E)\lt \delta$. So $\exists$ an $N$ such that for $n\gt N$ $$\int_{E^c} |f_n-f|~d\mu\lt \frac{\varepsilon}{3}.$$ So,
$$\begin{align*}
\int_X |f_n-f|~d\mu & = \int_{E^c} |f_n-f|~d\mu +\int_E |f_n-f|~d\mu\\
& \leq \int_{E^c} |f_n-f|~d\mu + \int_E |f|~d\mu + \int_E |f_n|~d\mu\\
& \lt \frac{\varepsilon}{3} + \frac{\varepsilon}{3} + \frac{\varepsilon}{3}\\
& =\varepsilon. 


\end{align*}$$ Now to show that $f\in L^1(\mu)$, I have to show that $\int_X |f|\lt \infty.$  Somehow I feel I have to use Egoroff again but I'm kind of lost. I'd be grateful if someone could look over what I've done above and see if it's okay and perhaps provide a little help with showing the $f\in L^1(\mu)$. Thanks.","['measure-theory', 'real-analysis']"
113025,"If $A \cup \{a\} \sim A$,  $P(A) \cup \{b\} \sim P(A)$?","Can someone help me with this homework? Thank you. Let $a \notin A$ and $b \notin P(A)$. If $A \cup \{a\} \sim A$, prove that $P(A) \cup \{b\} \sim P(A)$. (dont use the Axiom of choice).",['elementary-set-theory']
113027,What is the Jacobian?,"What is the Jacobian of the function $f(u+iv)={u+iv-a\over u+iv-b}$? I think the Jacobian should be something of the form  $\left(\begin{matrix}
  {\partial f_1\over\partial u} & {\partial f_1\over\partial v}  \\
  {\partial f_2\over\partial u} & {\partial f_2\over\partial v}
 \end{matrix}\right)$ but I don't know what $f_1,f_2$ are in this case. Thank you.","['complex-numbers', 'linear-algebra', 'functions']"
113036,"$f(x)$ is positive, continuous, monotone and integrable in (0,1]. Is $\lim_{x \rightarrow 0} xf(x) = 0$?","I'm having trouble with this question from an example test. We have a positive function $f(x)$ that's monotone, continuous and integrable in $(0,1]$. Is $\lim_{x \rightarrow 0} xf(x) = 0$? Progress The only problematic case seems to be when $f(x)$ is unbounded and monotonic decreasing. For that case, I found out that $xf(x)=\int_{0}^{x} f(x)dt$ and that $0\leq xf(x)\leq \int_{0}^{x} f(t)dt$. From here I'm not sure how to go on. Thanks!","['calculus', 'integration', 'limits']"
113048,Sum of bijective functions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can anyone please help me with this? Let $f,g_{1}, g_{2},\ldots,g_{k} \in \mathbb{Q}^{\mathbb{N}}$. $f$ is a sum of $g_{1},g_{2},\ldots,g_{k}$ if for every natural number $n$ $$f(n) = g_{1}(n) +g_{2}(n) + \cdots + g_{k}(n)$$ a) Prove that for every $f \in \mathbb{Q}^{\mathbb{N}}$,   $\exists$ three bijections $g_{1}, g_{2} , g_{3} \in \mathbb{Q}^{\mathbb{N}}$, such that $f$ is a sum of $g_{1}, g_{2},g_{3}$. b) Give example for $f \in \mathbb{Q}^{\mathbb{N}}$, that is not a sum of two bijections $g_{1}, g_{2} \in \mathbb{Q}^{\mathbb{N}}$. Thanks in advance.",['elementary-set-theory']
113059,What is a explanation in math to High School Students that why prove/disprove riemann hypothesis is important?,What is a good mathematical explanation for High School Students that proving/disproving the Riemann hypothesis is important? I asked this because I wonder and want a comprehensive example why this question is worth a million (Is there even an application to elementary mathematics?). Thanks in advance.,['number-theory']
113069,Is there a non-abelian group of order 49?,Is there any non-abelian group of order $n=49$? I assume there should be at least one but I cannot find an example.,"['finite-groups', 'abstract-algebra']"
113076,Point estimation vs hypothesis testing and interval estimation,Can someone explain what are point estimators good for? Hypothesis tests and interval estimations give a fuzzy answer in terms of something sort of like a probability of the value being in some interval. In applied statistics why would you look at a single number or point estimator rather than these intervals?,['statistics']
113081,Understanding an example of a Distribution,"Whilst reading the article about restrictions of distributions (generalized functions) on Wikipedia ( here ) I had trouble understanding the example of a distribution defined on the subset 
$V = (0,2) \subset \mathbb{R}$ that admits no extension to the space of Distributions on $U = \mathbb{R}$. The example I am referring to is the distribution \begin{equation}
S(x) = \sum_{n = 1}^\infty n \delta \left(x - \frac{1}{n}\right)
\end{equation} Now, my question is how does this example act on test functions ? If I take a smooth function $\psi$ whith supp $\psi \subset V$,  how do I apply $S$ to it ? I understand that $S$ is a modification of the Dirac delta distribution \begin{equation}
\langle \delta , \psi \rangle = \psi (0)
\end{equation} Below is my guess : \begin{equation}
\langle S , \psi \rangle = \sum^{\infty}_{n = 1} n \psi (\frac{1}{n})
\end{equation} Is that correct ? If yes, I am still not sure how this is well defined, given only the restriction that $\psi$ is zero outside $(0,2)$. Tanks a lot for your help!","['operator-theory', 'distribution-theory', 'functional-analysis']"
113100,A non weakly convergent sequence in $L^p(\mathbb{N})$,Can anyone provide an example of a sequence $(x_{k}) \in L^p(\mathbb{N})$ with $1<p<\infty$ such that $x_{k}(n)\rightarrow 0$ as $k\rightarrow \infty$ such $x_{k}$ doesn't converges weakly to zero. The sequence has to be unbounded because if the sequence is bounded with this the sequence is weakly convergent.,"['measure-theory', 'functional-analysis', 'real-analysis']"
113106,Number of possibilities to cross a hexagonal lattice.,"An ant walks along the line segments in the hexagonal lattice shown, from start to finish.  The ant must go in the direction shown if there is an arrow, and never goes on the same line segment twice.  How many different paths can the ant take? I am not looking for an answer to this, just a hint on how to get started. I have an adjacency matrix setup, and know that I can use A^r to find possibilities for r number of lengths, but that includes retaking the same paths. How do I get over that bump? Note: Original image here","['probability-theory', 'graph-theory', 'discrete-mathematics', 'combinatorics']"
113121,lim sup inequality $\limsup ( a_n b_n ) \leq \limsup a_n \limsup b_n $,"I´m not sure how to start with this proof, how can I do it?
$$
\limsup ( a_n b_n ) \leqslant \limsup a_n \limsup b_n 
$$
I also have to prove, if $ \lim a_n $ exists then:
$$
\limsup ( a_n b_n  ) = \limsup a_n \limsup b_n 
$$
Help please, it´s not a homework I want to learn.","['inequality', 'real-analysis', 'limsup-and-liminf']"
113126,"Meaning of ""a mapping preserves structures/properties""","Sometimes I see something like ""a mapping preserves the structures
of its domain and of its codomain"". From Wiki about morphisms in category theory: a morphism is an abstraction derived from structure-preserving
  mappings between two mathematical structures. The notion of morphism
  recurs in much of contemporary mathematics. In set theory, morphisms
  are functions; in linear algebra, linear transformations; in group
  theory, group homomorphisms; in topology, continuous functions, and so
  on. I was wondering why the structure-preserving mappings between two
topological/measurable spaces are defined by the ""inverse"" of the
mapping, while the structure-preserving mappings between two
groups/vector spaces are not? Why are the structure-preserving mappings between two topological
spaces chosen to be continuous mappings instead of open mappings? I also see that ""a mapping preserves some property of subsets,
points or whatever"". Such as Continuous linear mappings between topological vector spaces preserve
  boundedness. According to Brian's reply to my earlier question, this quote should
be understood as ""under a continuous linear mapping, the image of
any bounded domain subset is also a bounded codomain subset"", not as
""under a continuous linear mapping, the inverse image of any bounded
codomain subset is also a bounded domain subset"". I wonder why? It seems at first to me like how continuous mappings
preserve topologies, but it is actually in the same way as how group
homomorphisms preserve group structures. Thanks and regards!","['general-topology', 'measure-theory', 'abstract-algebra', 'category-theory', 'topological-vector-spaces']"
113132,Name for Cayley graph of a semigroups?,I did Google search and can't find a good answer. I thought I should ask experts here. Cayley graph is defined for groups. My question is: Is there a special name for the Cayley graph of semigroups?,"['geometric-group-theory', 'cayley-graphs', 'abstract-algebra', 'semigroups', 'group-theory']"
113147,Proving every positive natural number has unique predecessor,"I am independently working through Tao's Analysis I , and one of the exercises is to prove that every positive natural number has a unique predecessor. The actual lemma is (where $n++$ denotes the successor of $n$): Let $a$ be a positive [natural] number. Then there exists exactly one natural number $b$ such that $b++=a$. The hint given is to use induction, and the book uses the Peano axioms with $0\in\mathbb{N}$. Here is my attempt at a proof, but I am not sure if I applied induction properly (I didn't seem to need the inductive hypothesis in proving the $k+1$ case), and I am even more unsure if uniqueness is proven: The statement to prove is $(\forall a\not=0)(\exists!b)(b++=a)$, which is the same as $\forall a\exists!b(a\not=0\rightarrow (b++=a))$. Induction is used on $a$. The basis step is to prove for $a=0$, which is $\exists!b(0\not=0\rightarrow (b++=0))$. Since the antecedent of the conditional is false, the statement is vacuously true. [But is $b$ unique...?] For the inductive step, it is assumed that for an arbitrary $k$, $\exists!b(k\not=0\rightarrow (b++=k))$. Then $\exists!b((k++\not=0)\rightarrow (b++=k++))$ is to be derived. The second of these (the one being derived) can be rewritten as $(k++\not=0)\rightarrow (c++=k++)$, for some $c$. $k++\not=0$ is true even without assuming it, since $0$ does not have a predecessor. But $c++=k++$ implies $c=k$ (because the successor function is injective), which shows that a [unique?] $c$ can be chosen such that it is equal to $k$. This closes the induction and thus every positive natural number has a unique successor. My questions are: (1) Is this proof correct? (2) If the answer to the first question is 'no', is the general idea for the proof correct? (3) If the answers to both of the preceding questions are 'no', could someone point me in the right direction for a correct proof? Any help would be appreciated. Thanks.",['real-analysis']
113150,Entire map taking disc to annulus,"Does there exist an entire function on $\mathbb{C}$ mapping an open disc to an annulus? The reason I ask this is because I want to to answer this question: Suppose $f$ is entire, and suppose there exists an open disc $U$ and $\delta > 0$ such that for all $z\in U$, $|f(z)|>\delta > 0$. Does there exist a branch cut for $\log$ such that $\log f(z)$ is analytic on all of $U$? (Note that $f$ is not necessarily one-to-one). A problem arises if $U$ is mapped to an annulus. EDIT: I guess the answer to the first question should be no, since if $f$ is entire and nonconstant, it is an open map. Therefore, all interior points of $U$ remain interior for $f(U)$. Thus $\partial f(U) = f(\partial U)$. On the other hand, since $\partial U$ is connected, $f(\partial U)$ cannot be the boundary of an annulus (which consists of two connected components). What do you think? PS. For a set $S$, by $\partial S$ I of course mean the boundary of $S$.",['complex-analysis']
113157,Homotopy invariance of winding number in complex analysis.,"In topology, the winding number is homotopy invariant under the definition $n(\gamma,a)=\frac{\tilde{\theta}(\beta)-\tilde{\theta}(\alpha)}{2\pi}.$ I assume the must be true in the framework of complex analysis. Suppose you take as definition for the winding number $n(C,a)$ of a curve $C$ through $a$ to be 
$$
n(C,a)=\frac{1}{2\pi i}\int_C\frac{dz}{z-a}.
$$ Is is still true that $n(C,a)$ is hopotopy invariant under smooth curves $C$ not going through $a$? Thank you.",['complex-analysis']
113158,Relationship between two distinct notions for divisors on curves,"I've seen divisors on curves before, a few years ago in a course in algebraic geometry. Now I've come across them again, but they're somewhat more generalized. I was hoping someone could explain the similarities/differences between the two notions. The first notion (from Algebraic Curves by Fulton) says for any irreducible projective curve $C$, with nonsingular model $X$, a divisor on $X$ is a formal sum
$$ D = \sum_{P\in X} n_P P$$
where each $n_P\in \mathbb{Z}$ and all but finitely many are 0. The second notion (from Algebraic Geometry by Milne) says for any normal, irreducible variety $V$, a divisor on $V$ can be written uniquely as a finite (formal) sum
$$ D = \sum n_i Z_i,$$
where the $Z_i$ are the irreducible subvarieties of $V$ of codimension 1 (prime divisors). In the generalized case, when $V$ is a curve (i.e., dimension 1) then the $Z_i$ are 0 dimensional, so just points on the curve, and everything seems OK. But then we move on to the degree of a divisor. In Fulton, the degree of a divisor $\sum_{P\in X} n_P P$ is simply the sum of the coefficients, $\sum_{P\in X} n_P$. However, via Example 12.4 in Milne, the definition of a divisor is as follows: Let $C$ be a curve. If $D = \sum n_i P_i$, then the intersection number
$$ (D) = \sum n_i[k(P_i):k].$$
By definition, this is the degree of $D$. Now, we don't have any references for my class (as it's all over the place and would require about 10 minimum..), but this is how we defined the degree of a divisor for a curve in class (using the extension of $k(P_i)$ over $k$) and I'm confused. Is this suggesting that, by Fulton's definition, we have $[k(P_i):k] = 1$ for every point? Because if so, it seems silly to even bother putting that in. Now given Milne is actually writing the intersection number, I felt he could just leaving implicit that the $[k(P_i):k]$ are 1. However in class, we simply said a divisor is blah and its degree is blahh using the degree of the extensions, so there must be something non-trivial going on here! Now the only thing I haven't entirely accounted for above, is the fact that in Fulton, a curve over a field $k$ is a set of points in $k^n$, whereas Milne is using algebraic varieties, which I'm not terribly well versed with. However, Milne makes note of the fact that there is a one-to-one correspondence between maximal ideals of $k[V]$ and one point sets of $V$, so I would expect, for each $P_i$ in Fulton's definition, there is a corresponding maximal ideal, so that a divisor should be identical, except replacing each $P_i$ with the corresponding maximal ideal. Somewhat more troubling, I thought it was obvious at first, and didn't give it a second thought, but in retrospect, what is $[k(P_i):k]$? If $P_i$ is a subvariety of codimension 1, for a curve defined over $k$, then how can this not have to be 1? If nothing else, I'd also appreciate any additional references for these generalized divisors on curves (though I do like Milne's notes!). Thanks!","['algebraic-geometry', 'reference-request', 'algebraic-curves']"
113162,Why is $\sqrt{-2} \sqrt{-3} \neq \sqrt{6}$?,Why is $\sqrt{-2} \cdot \sqrt{-3} \neq \sqrt{6}$? Are there other examples where regular arithmetic goes wrong for complex numbers?,['complex-analysis']
113163,What is the antipode in a Combinatorial Hopf Algebra (or graded bialgebra)?,"In several papers I've seen on Combinatorial Hopf Algebras, the algebra and coalgebra structures are described, but no antipode is defined. CHAs generally have a natural grading, and are of finite dimension in each degree, and I have seen one comment which suggested that this leads to an obvious or unique antipode. If so, please can someone spell it out. (Are there any good introductory papers on CHAs, as the ones I've found so far all go straight in at the deep end.)","['quantum-groups', 'abstract-algebra', 'combinatorics']"
113165,A simple-looking diophantine equation,"Consider the diophantine equation $Q(x,y,z)=0$, where $x$, $y$ and $z$
are nonnegative integer unknowns and $$
Q(x,y,z)=x^3 + (-2y + 2)x^2 + ((z - 6)y + (2z + 1))x + ((2z - 4)y + 3z)
$$ Since the degree of $Q$ in $y$ and $z$ is $1$, the equation seems tractable. If
$t\geq 0$, then $(x,y,z)=(t,t,t)$ is a solution. Are there any others ?","['polynomials', 'diophantine-equations', 'number-theory']"
113167,Derivations of Polynomial Algebra,"Let $A=\mathbb{F}_p[x,y]$, the commutative polynomial algebra on two variables over the finite field $\mathbb{F}_p$. Define a derivation on an algebra as a map which satisfies the Leibniz rule, ie if $d$ is a derivation then $$d(ab)=ad(b)+bd(a)$$ Let $Der(A)$ be the $\mathbb{F}_p$-module (algebra?) of derivations of $A$. I'm interested in  knowing what $Der(A)$ is. First of all, I'm not sure exactly how complicated the algebraic structure it carries is. I know that it's at least a $\mathbb{F}_p$-module. But is it an algebra? I suspect no, but I can't nail down a reason why. Is it a module over a bigger ring? $\mathbb{F}_p[x,y]$ seems like it might be a good choice- but again, I'm not very sure of this. Finally, what's an explicit description of elements of $Der(A)$? Is it just linear combinations of $\frac{d}{dx}$ and $\frac{d}{dy}$? I apologize if this is a bit overly broad, but I'm encountering derivations over finite fields for the first time, and I'd like to really understand what's going on.","['finite-fields', 'abstract-algebra']"
113177,Characterization of linearity in terms of metric,"At least in Euclidean geometry and the upper half plane model of hyperbolic geometry, the statements '$y$ lies on the line segment determined by $x$ and $z$ ' and '$d(x,y)+d(y,z)=d(x,z) $' are equivalent. I wonder whether this characterization holds in other geometries and whether it has a name to it. Do geometries with this relation have some special properties? I also think it is somehow related to the notion of uniform convexity in Banach spaces, but that is a different problem since we don't ask for vector space sturcture here. Any insight or reference would be helpful!
Thanks!","['hyperbolic-geometry', 'geometry', 'functional-analysis', 'euclidean-geometry']"
113198,"Banach Spaces - How can $B,B',B'', B''', B'''',B''''',\ldots$ behave?","(ZFC) Let $ \big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $ be a Banach space. Define $ \mathbf{B} \; = \;\big\langle B,+,\cdot, \:\: \|\cdot\| \:\: \big\rangle $. Define $\: \mathbf{B}_0 = \mathbf{B} \:$. For all non-negative integers $n$, define $\mathbf{B}_{n+1}$ to be the Banach space that is the continuous dual of $\mathbf{B}_n$. Define the relation $\:\sim\:$ on $\:\{0,1,2,3,4,5,\ldots\}\:$ by $m\sim n \:$ if and only if $\: \mathbf{B}_m$ is isometrically isomorphic to $\mathbf{B}_n$. $\sim\:$ is obviously an equivalence relation. What can the quotient of $\:\{0,1,2,3,4,5,\ldots\}\:$ by $\:\sim\:$ be? The only thing I know about this is that $\:\{\{0,1,2,3,4,5,\ldots\}\}\:$ and $\:\{\{0,2,4,6,8,\ldots\},\{1,3,5,7,9,\ldots\}\}\:$ are both possible.","['dual-spaces', 'normed-spaces', 'functional-analysis', 'banach-spaces']"
113202,Number of cycles of all even permutations of $[n]$ and number of cycles of all odd permutations differ by $(-1)^n (n-2)!$,"I'm trying to solve task 44 of the first chapter of Stanleys Enumerative Combinatorics (found here ). Show that the total number of cycles of all even permutations of $[n]$ and the
  total number of cycles of all odd permutations of $[n]$ differ by $(−1)^n (n − 2)!$. Use generating functions. I might be completely off the track here, but the way I thought of this problem is the following. A permutation can be written as a product of disjoint cycles, and a permutation is odd iff there is an odd number of even-length cycles. The set of cycles partition $[n]$ into disjoint orbits, and thus the number of cycles of all odd permutations of $[n]$ should be equal to the number of partitions of $[n]$ into even parts, with an odd number of such parts (and similarly for the even permutations). Is this right or have I misunderstood something? Also, I'm not quite sure how to proceed from here and how to set up the generating functions. I did find this question , which might be of some use if my interpretation of this problem is correct. However, I did not clearly see how one could end up with $(-1)^n (n-2)!$ from that. Any help would be greatly appreciated :)","['generating-functions', 'combinatorics']"
113214,Deriving Cauchy's inequality with Fubini's theorem,"The exercise is the following: Let $f,g: X \to \mathbb K$ be two measurable functions such that $|f|^2, |g|^2 \in \mathcal L^1$. Making use of Fubini's theorem and by considering the function $$(x,y) \mapsto |f(x)g(x) f(y) g(y)|$$
  derive Cauchy's inequality:
  $$\left(\int_X |fg|\, d\mu \right)^2 \le \left(\int_X |f|^2 \, d\mu \right)\left(\int_X |g|^2\, d\mu\right)$$ I can prove that $\int_{X\times X} |f(x)g(x)f(y)g(y)| \, d(\mu\otimes \mu)= \left(\int_X |fg|\, d\mu\right)^2$ by an application of Fubini with $A = \{f\ne 0\}\cup \{g\ne 0\}$, which is $\sigma$-finite, since it can be written as a union $$A = \bigcup_{n = 1}^\infty (\{|f|^2>1/n\}\cup\{|g|^2\ge 1/n\})$$ where all sets on the RHS have to have finite measure, since $f,g \in L^2$. So $$
\begin{align*}
\int_{X\times X} |f(x)g(x)f(y)g(y)| \, d(\mu\otimes \mu) &= \int_{A\times A} |f(x)g(x)f(y)g(y)| \, d(\mu\otimes \mu) \\
&= \left(\int_A |fg|\, d\mu\right)^2 \\
&= \left(\int_X |fg|\, d\mu\right)^2
\end{align*}
$$ But I don't really see how to prove $$\int_{X\times X} |f(x)g(x)f(y)g(y)| \, d(\mu\otimes \mu)\le \left(\int_X |f|^2 \, d\mu\right)\left( \int_X |g|^2\, d\mu\right)$$
without making use of Young's inequality (or AM-GM). I think one should be able to see this last inequality directly somehow. Why I don't want to use AM-GM: The derivation in this exercise should probably be an alternative to the usual one, where one integrates $$\frac{|fg|}{\Vert f\Vert_2 \Vert g\Vert_2} \le \frac12 \left(\frac{|f|^2}{\Vert f\Vert_2^2}+\frac{|g|^2}{\Vert g \Vert_2^2}\right)$$ Some help would be very much appreciated, thanks! =)","['measure-theory', 'real-analysis']"
113215,Some basic questions on Markov chains (Durrett),"If you have a state space $S$, usually I think of a Markov chain $X_n$ on it as $X_n$ takes values in $S$ and satisfies the obvious Markov property and so on. In Durrett's book, he says one should instead consider $S^{\mathbb{N}}$ with $X_n(\omega) = \omega_n$. So in this new state space, am I meant to consider $\omega \in S^{\mathbb{N}}$ as consisting of all the states the original Markov chain can be in? Is it a realisation of the process as it's run through time? I don't understand this. He also defines a shift operator $\theta_n$ which acts as $\theta_n \omega (m) = \omega (m+n)$. So $X_j \circ \theta_n(\omega) = X_j(\theta_n(\omega)) = \omega_{j+n}$, which I am a bit unsure about.","['probability-theory', 'markov-chains']"
113226,determine the max flow function $f^*$ in a network given the maximum flow value,"Suppose I have a Network N ( i.e. just a Digraph D(A,V) with A=Arcs, V=Vertices; combined with a capacity function $c:V x V \to \mathbb{N}\cup\{0\}$ and two vertices s:=source, t:=sink singled out) I call $f:V x V \to \mathbb{N}\cup\{0\}$ a flow if it does not exceed capacity for any pair (u,v) and the net flow at any vertex is zero expect at the source and sink where a net flow is allowed. Now suppose I have a way of knowing the value of the maximum flow (this value just being the maximum flow $$\sum_{(s,v)\in A} f(source,v)$$ out of the source in any legal flow f. Similarly this value will equal the max total flow into the sink in any flow) I am wondering whether there is a clever way to determine the actual flow function say $f^*$ given that I know what the maximum flow value is ? If I had a way of knowing what the value of such a maximum flow is for any network N at no extra ""cost"" would this give me a more efficient way ? So far I have only used Ford Fulkerson to determine $f^*$","['graph-theory', 'discrete-mathematics', 'algorithms']"
113237,Derivative of a multivariable function,"Let us define a function $f$ from $M(n,\mathbb{R})$ to $M(n,\mathbb{R})$ by treating $M(n,\mathbb{R})\approx\mathbb{R}^{n^2}$, by $$f(X)=e^X+X$$ where $$e^X=1+X/{1!}+X^2/{2!}+\dots$$ I want to find the (Frechet) derivative of $f$. We know, if derivative exists at $X$, then $$f(X+H)-f(X)=f'(X)H+r(H)$$ where $r(H)/\|H\|\to \bf{0}$ as $H\to \bf{0}$. So I went on to find the difference, but couldn't figure out the linear part ($f'(X)H$) and the remainder part ($r(H)$). Any help is appreciated.",['multivariable-calculus']
113240,Pointwise convergence of sequences of holomorphic functions to holomorphic functions,"Let $(f_{n})_{n \in \mathbb{N}}$ be a sequence of holomorphic functions on the open unit disc $D$ in $\mathbb{C}$ , and suppose that this sequence converges pointwise to a function $f$ .
By Osgood's theorem one can conclude then that there is an open and dense subset $V$ of the disc, so that the function $f$ is holomorphic there and that the convergence of the sequence is locally uniform on $V$ .
If we further suppose that the limit function $f$ is also holomorphic on the entire disc $D$ , is it then possible to conclude that the sequence $(f_{n})_{n \in \mathbb{N}}$ converges locally uniformly to $f$ on $D$ ?",['complex-analysis']
113245,Euler Four-Square Identity variants?,"Is it well-known that there are an infinite number of Euler-type 4-square identities? Proof : $\begin{align}
&{(x_1^2+ x_2^2+ x_3^2+ x_4^2) (y_1^2+ y_2^2+ y_3^2+ y_4^2)\,=\,z_1^2+ z_2^2+ z_3^2+ z_4^2}\\
&\text{where,}\\
&{z_1 \,=\, x_1y_1+a_1y_2+ a_2y_3+ a_3y_4}\\
&{z_2 \,=\, x_2y_1+b_1y_2+ b_2y_3+ b_3y_4}\\
&{z_3 \,=\, x_3y_1+c_1y_2+ c_2y_3+ c_3y_4}\\
&{z_4 \,=\, x_4y_1+x_3y_2-x_2y_3+x_1y_4}\\
&\text{and,}\\
&{a_1 \,=\, -k(x_1x_3-nx_2)\,+\,x_2,\;\; b_1 \,=\, -k(x_2x_3+nx_1)\,-\,x_1,\;\; c_1 \,=\, k(x_1^2+x_2^2)\,-\,x_4 }\\
&{a_2 \,=\, k(x_1x_2+nx_3)\,+\,x_3,\;\;\;\; b_2 \,=\, -k(x_1^2+x_3^2)\,+\,x_4,\;\;\;\;\;\;\;\; c_2 \,=\, k(x_2x_3-nx_1)\,-\,x_1 }\\
&{a_3 \,=\, k(x_2^2+x_3^2)\,-\,x_4,\;\;\;\;\;\;\;\;\; b_3 \,=\, -k(x_1x_2-nx_3)\,+\,x_3,\;\;  c_3 \,=\, -k(x_1x_3+nx_2)\,-\,x_2}\\
&{k = \frac{2(x_4-n)}{x_1^2+x_2^2+x_3^2+n^2}}\\
\end{align}$ for arbitrary n .  Of course, for $n = x_4$, then $k = 0$, all the $z_i$ become bilinear, hence the Euler 4-square identity is a special case of this family.  In fact, there is an even larger 8-square family. Is it also already known there is an infinite number of Degen-type 8-square identities? More details here .",['algebra-precalculus']
113250,The area of the superellipse,"I'm watching this video , where D. Knuth explains the connection of $\pi$ and factorials, and other matters (it is very interesting). Almost at the end of the talk he says the area of the superellipse $$x^{\frac{1}{\alpha}}+y^{\frac{1}{\alpha}}=1$$ is given by $$A(\alpha) = \frac{2 \alpha \cdot\Gamma{(\alpha)}^2}{\Gamma{(2 \alpha)}}$$ whic would be $$A(\alpha) = 2 \alpha B(\alpha,\alpha) = 2 \alpha\int_0^1(1-u)^{\alpha-1}u^{\alpha-1}du  $$ I was trying to check this so I put $$A\left( \alpha  \right) = \int\limits_0^1 {{{\left( {1 - {x^{1/\alpha }}} \right)}^\alpha }dx} $$ Now let $x = {u^\alpha }$ $$A\left( \alpha  \right) = \alpha \int\limits_0^1 {{{\left( {1 - u} \right)}^\alpha }{u^{\alpha  - 1}}du} $$ What's going on? The $2$ in Knuth's formula probably comes from the fact he considers the full figure and not only a fourth, as I am, but I don't know what I'm doing wrong here. If you want to check, it is at $1:21:00$ aproximately. PS: Just as a curiosity, does Knuth have a stutter or is it he is just thinking about too many things in too little time? So it was just OK: $$A\left( \alpha  \right) = \alpha \int\limits_0^1 {{{\left( {1 - u} \right)}^\alpha }{u^{\alpha  - 1}}du}  = \frac{{\alpha \Gamma \left( {\alpha  + 1} \right)\Gamma \left( \alpha  \right)}}{{\Gamma \left( {2\alpha  + 1} \right)}} = \frac{{\Gamma {{\left( {\alpha  + 1} \right)}^2}}}{{\Gamma \left( {2\alpha  + 1} \right)}}$$","['special-functions', 'calculus', 'integration', 'definite-integrals', 'plane-curves']"
113266,Expected value of a minimum?,"One of the problems of my Statistics assignment requires me to calculate the expected value of a minimum. Here's the situation: The following density function is given: $f(x) = \frac{\theta}{x^2}$ where $x\ge\theta$ and $\theta>0$ I have to calculate: E[min{$X_i$}] My initial guess was that the smallest possible value of $X$ is $\theta$ sense $x\ge\theta$ so the expected value of the minimum would be $\theta$, but then again, in an acquired sample, you can't be 100% certain that the smallest possible value of $X$ will be one of the observations. So what then?",['statistics']
113267,Proof of $\sum_{0 \le k \le a} {a \choose k} {b \choose k} = {a+b \choose a}$ [duplicate],This question already has answers here : How to prove Vandermonde's Identity: $\sum_{k=0}^{n}\binom{R}{k}\binom{M}{n-k}=\binom{R+M}{n}$? (7 answers) Closed 10 years ago . $$\sum_{0 \le k \le a}{a \choose k}{b \choose k} = {a+b \choose a}$$ Is there any way to prove it directly? Using that $\displaystyle{a \choose k}=\frac{a!}{k!(a-k)!}$?,"['summation', 'binomial-coefficients', 'combinatorics']"
113288,"Etymology of ""topological sorting""","This may be a dumb question, but what's ""topological"" about topological sorting in graph theory? I thought topology was related to geometry and deformations.","['general-topology', 'graph-theory', 'computer-science', 'terminology']"
113295,pdf of a quotient of uniform random variables,"Suppose $x_1, x_2$ are IDD random variables uniformly distributed on the interval $(0,1)$.  What is the pdf of the quotient $x_2 / x_1$?","['probability-distributions', 'probability']"
113298,"If $A,B$ symmetric positive semidefinite, show tr$(AB) \geq 0$","Supposing $V$ is a finite dimensional vector space (over $\mathbb{R}$) of dimension $n$, and $A,B$ are symmetric positive definite linear mappings from $V$ to $V$, how can I show that in any orthonormal basis $\mathrm{tr}(AB) \geq 0$? I noticed that since they are symmetric we have that 
$$\mathrm{tr}(AB) = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ji} = \sum_{i=1}^n\sum_{j=1}^nA_{ij}B_{ij}$$ which is the sum of the elements of the element-wise product of $A,B$. I don't know if this is helpful.",['linear-algebra']
113317,"Must a measure on $2^{\mathbb{N}}$ be atomless to be a measure on $[0,1]$?","This question comes from section 4.4, page 17, of this paper . Let $\mu$ be a Borel measure on Cantor space, $2^\mathbb{N}$. The authors say that If the measure is atomless, via the binary expansion of reals we can view it also as a Borel measure on $[0,1]$. Is it necessary that $\mu$ be atomless?",['measure-theory']
113331,determine if 2 line segments are intersecting,currently I write a program where finding out whether 2 line segments intersect is an essential part of the algorithm. Could anyone tell me if there's a way to determine if two segments are intersecting (i.e. whether the intersection point of 2 lines lies on each line between the points of each segment) without computing the exact coordinates of the intersection point. (computing it would cause unnecessary overhead on runtime) P.S. Sorry for my bad English and many thanks in advance!,['geometry']
113338,Explanation of the following notation,"I am having a hard time understanding the meaning of the union operation in this equation. $$C(A)=\bigcup_{x \in A}C(x)$$ For context, here is the sentence: The candidate set for $x$ is $S \cap C(x)$. The candidate region for a set of points $A$ is $C(A)=\bigcup_{x \in A}C(x)$, with the candidate set $S \cap C(A)$. [1] Is this an indexed union? Also, I haven't seen a union operation except for when it appears between two sets. Any advice would be greatly appreciated. Thanks. [1] Clarkson, 1988. ""A randomized algorithm for  Closest-Point Queries"".","['computational-geometry', 'elementary-set-theory']"
113340,Elliptic curves with finitely many rational points,"A conjecture by Goldfeld says that half of all elliptic curves have rank zero (i.e. their Mordell-Weil group has finite order.) Are there any known infinite families of elliptic curves (over $\mathbb{Q}$) with only finitely many rational points? For example, in Silverman/Tate, there is an computation which shows that $y^2=x^3+x$ has exactly one rational point and $y^2=x^3+4x$ has exactly three rational points (not counting the point at infinity). I'm wondering if there are any known parameterizations giving an infinite number of such curves.","['elliptic-curves', 'reference-request', 'number-theory']"
113352,"Disprove uniform convergence of $\sum_{n=1}^{\infty} \frac{x}{(1+x)^n}$ in $[0,\infty)$","How would I show that $\sum_{n=1}^{\infty} \frac{x}{(1+x)^n}$ does not uniformly converge in $[0,\infty)$? I don't know how to approach this problem. Thank you.","['convergence-divergence', 'sequences-and-series']"
113353,"""The two notions of boundedness coincide for locally convex spaces""","From Wiki The boundedness condition for linear operators on normed spaces can be
  restated. An operator is bounded if it takes every bounded set to a
  bounded set, and here is meant the more general condition of
  boundedness for sets in a topological vector space (TVS): a set is
  bounded if and only if it is absorbed by every neighborhood of 0. Note
  that the two notions of boundedness coincide for locally convex
  spaces. I was wondering what ""the two notions of boundedness"" are referred to? In other words, their definitions? Are they for operators between TVSes, or for subsets of TVS? Thanks and regards!","['normed-spaces', 'topological-vector-spaces', 'functional-analysis']"
113370,How to show that the sum of $L^p$ spaces is Banach.,"Let $p<q$ be positive integers (with the allowance that $q$ may be $\infty$). How can we show that the sum of $L^p$ and $L^q$ is a Banach space under the norm $\|f\|=\inf\{\|g\|_p+\|h\|_q: g+h=f\}$? Let $\{f_n\}$ be a sequence in $L^p+L^q$, such that $$\sum_{n=1}^\infty \|f_n\|<\infty.$$ We would like to conclude that this implies that $\sum_{n=1}^\infty f_n$ converges to something in $L^p+L^q$, at which point it follows that $L^p+L^q$ is complete by a basic theorem. We could easily do this if for each $f_i$ it were possible to express $f_i=g_i+h_i$, where $g_i\in L^p, h_i\in L^q$ and $\|f_i\|=\|g_i\|_p+\|h_i\|_q$. Though we know Cauchy sequences in $L^p$ converge, it is not clear that for a given $f$, all sequences (or some sequence) $\{g_n+h_n\}$ such that the $(\|g_n\|_p+\|h_n\|_q)\to \|f_n\|$ have the property that $\{g_n\},\{h_n\}$ converge in their respective spaces. It seems possible to imagine the sum converging without the summands converging in their respective spaces. Any help would be much appreciated.","['banach-spaces', 'real-analysis']"
113373,Characterization of primary ideals in a principal ideal domain,"On the commutative algebra wiki, a table of properties lists that ""for a PID, the primary ideals coincide with the powers of prime ideals."" I played around with it, couldn't produce a proof, and have been searching around for a proof, since I'm sure this is a standard fact. I couldn't find a reference online. Can someone please provide a proof, or reference where I can read such a proof?","['abstract-algebra', 'principal-ideal-domains', 'reference-request', 'commutative-algebra', 'ideals']"
113380,Creating a reflexive Banach space from a weakly compact set.,"I have a set $K$ which is weakly compact in a Banach space $E$.  Also, its span is norm-dense in $E$.  ($E$ is weakly compactly generated.  But this doesn't enter the question anywhere.) I need to construct a reflexive Banach space, so I need its unit ball to be compact.  Since $K$ is compact, I thought I might try to use the space $\operatorname{span}(K)$.  I want its unit ball to still be weakly compact, is this the case? That is, if $K$ is weakly compact in $E$, then is $B_{Y}$ weakly compact in $Y$, where $Y = \operatorname{span}(K)$?",['functional-analysis']
