question_id,title,body,tags
4278427,What does this $\mathbb 1$ symbol in statistical learning theory mean?,"Searching for something I do not know the name of is hard...
Context is Statistical Learning Theory. Probably something basic, I do not remember from my boring statistics university lectures.
Without knowing how the symbol is called, I can only provide an image:","['notation', 'statistics']"
4278438,If $f$ is atomic then int $(f (U)) \neq \emptyset$ (interior). True or False?,"Let $f:X \to Y$ be a continuous function between continua. If $f$ is atomic then int $(f (U)) \neq  \emptyset$ (interior). I don't know if this conjecture is true.
Before presenting my attempt, I will recall some definitions. A map $f : X \to Y$ between continua is said to be atomic if it is surjective and for every subcontinuum $K$ of $X$ such that $f (K)$ is nondegenerate, we have that $K = f ^{−1}(f (K))$ . A map $f : X \to Y$ between continua is atomic if and only if $f ^{−1}(y)$ is a terminal subcontinuum in $X$ for each $y \in Y$ . My attempt: I first demonstrated the following. Lemma 1: Let $f:X \to Y$ be a continuous function between continua. If $U$ is an open of $X$ and $f^{-1} (y) \subseteq U$ for some $y \in Y$ , then $y \in $ int $f(U)$ . Now if i will start my attempt Let $U$ be an open of $ X $ . Let $ L $ be a nondegenerate continuum of $ X $ such that $L \subseteq U$ . If $|f(L)|>1$ . Let $y \in f(L)$ , then $f^{-1}(y) \subseteq f^{-1}(f(L))$ . Since $f$ is atomic then $L =f^{-1}(f(L))$ , thus $f^{-1}(y) \subseteq f^{-1}(f(L))=L \subseteq U$ .  Therefore, by Lemma 1, $y \in $ int $f(U)$ . If $|f(L)|=1$ . This case I have not been able to prove it. Any ideas or a counterexample?
Thank you so much.","['continuum-theory', 'analysis', 'separation-axioms', 'functions', 'general-topology']"
4278449,Variance and differential entropy,"Let $f:\mathbb R^d\to  [0,\infty)$ be a probability density function: $$\int_{\mathbb R^d} f(x)\,d x =1\,. $$ Suppose $f$ has finite second moment: $$\int_{\mathbb R^d} |x|^2\,f(x)\,d x <\infty\,. $$ I have read that $f$ has then finite entropy: $$\int_{\mathbb R^d} |\log f(x)|\,f(x)\,d x <\infty $$ but I am not sure how to prove this statement. Any help is appreciated. Edit. I can prove the thesis adding the hypothesis $f(x)\leq c_0\,e^{c_1|x|^2}$ for every $x$ . Indeed splitting the integral in three terms: $$ \int_{\{f\geq1\}} \log f(x)\,f(x) \,d x \,\leq\, \log c_0 + c_1\int_{\mathbb R^d}|x|^2\,f(x) \,d x \,,$$ $$ \int_{\{f\leq e^{-|x|}\}} -\log f(x)\,f(x) \,d x \,\leq\, \int_{\mathbb R^d} e^{-\frac{|x|}{2}} \,d x \quad(\textrm{since } -f\,\log f\leq \sqrt{f} \textrm{ for }0\leq f\leq1)\,,$$ $$ \int_{\{e^{-|x|}<f<1\}} -\log f(x)\,f(x) \,d x \,\leq\, \int_{\mathbb R^d} |x|\,f(x)\,d x \;.$$ Is it possible to remove the hypothesis of $f$ being controlled by $e^{|x|^2}$ ?","['entropy', 'real-analysis', 'lp-spaces', 'functional-analysis', 'probability-theory']"
4278451,Show that there exists a $c : f(c) = g(c)$,"I'll try to present a solution for this problem, and I hope I can receive feedback on what went wrong, if something went wrong of course. Let $f, g : [a, b] \to \Bbb R$ be continuous functions and $\int_{a}^{b} f(x) dx = \int_{a}^{b} g(x) dx$ . Show that there exists $c \in [a, b]$ such that $f(c) = g(c)$ . Solution Let's define $h(x) = \int_{a}^{x}f(x)dx-\int_{a}^{x}g(x)dx$ $h(x)$ is continous, since $f(x)$ and $g(x)$ is continous. I hope this argument is correct. We see $h(a) = h(b) = 0$ . Applying Rolle's Theorem, we get that $\exists \xi \in (a,b) : h'(\xi) = 0$ In other terms, $f(\xi) = g(\xi)$ $\square$ Thanks!",['real-analysis']
4278459,Disintegration of pushforward of $(Y\circ (\operatorname{id}\times X))_\#\mathsf P$,"Motivation. See my answer here . Throughout, let $(\Omega, \mathcal A, \mathsf P)$ be a fixed probability space. Let $X$ be a real random variable and assume that we have a family of real random variables $(Y_x)_{x\in\mathbb R}$ that is stochastically independent of $X$ . We will define $Y(\omega, x)=Y_x(\omega)$ and assume that $Y$ is $\mathcal A\otimes\mathcal B(\mathbb R)-\mathcal B(\mathbb R)$ -measurable. I want to prove that we have the disintegration $$(Y\circ (\operatorname{id}\times X))_\#\mathsf P(\mathrm dy)=(\pi_2)_\#\big(X_\#\mathsf P(\mathrm dx)\otimes Y(\cdot, x)_\#\mathsf P(\mathrm dy)\big).$$ Here: $\operatorname{id\times X}$ denotes the map from $\Omega$ to $\Omega\times\mathbb R$ mapping $\omega$ to $(\omega, X(\omega))$ ; $\#$ denotes push-forward (for measures). In other words: If we have any measure $\mu:\mathcal A\to[0,\infty]$ and any measurable function $f:\Omega\to\mathbf Z$ for any measurable space $\mathbf Z =(\mathfrak Z, \mathcal B)$ , then $f_\#\mu$ is the measure on $\mathbf Z$ defined by $f_\#\mu(B)=\mu(f^{-1}(B))$ for all $B\in\mathcal B$ . $\pi_2:\mathbb R^2\to\mathbb R$ denotes the projection onto the second coordinate, i.e. $\pi_2(x,y)=y$ . Furthermore, the product between probability measure and stochastic kernel on the right-hand-side is defined as in Korollar 14.23 in the book by Achim Klenke on Wahrscheinlichkeitstheorie (2013). This means that, by Definition, $$\nu\overset{\text{Def.}}=X_\#\mathsf P(\mathrm dx)\otimes Y(\cdot, x)_\#\mathsf P(\mathrm dy)$$ is the unique measure satisfying the regularity from the before-mentioned Korollar such that $$\nu(A\times B)=\int_A Y(\cdot, x)_\#\mathsf P(B)\,X_\#\mathsf P(\mathrm dx)$$ for all Borel-measurable $A,B\subset\mathbb R$ . Therefore, $$(\pi_2)_\#\nu(A\times B) = \nu(\mathbb R\times B) = \int_{\mathbb R} Y(\cdot, x)_\#\mathsf P(B)\,X_\#\mathsf P(\mathrm dx).$$ So in other words we want, for every measurable $B,\subset\mathbb R$ , that the following equality holds (both sides possibly being equal to $\infty$ ): \begin{equation}\tag{*}\label{*}(Y\circ (\operatorname{id}\times X))_\#\mathsf P(B)=\int_{\mathbb R} Y(\cdot, x)_\#\mathsf P(B)\,X_\#\mathsf P(\mathrm dx).\end{equation} My attempt. I will show this only if $X$ is a simple random variable, i.e. if its image $\operatorname{im}(X)$ is finite. Then I hope that one can go to general $X$ using some kind of approximation argument. If $\operatorname{im}(X)$ is finite, then, by Definition, $$(Y\circ (\operatorname{id}\times X))_\#\mathsf P(B)=\mathsf P(\{\omega\in\Omega:Y(\omega, X(\omega))\in B\})=\mathsf P\left(\bigcup_{x\in\operatorname{im}(X)} \{\omega\in\Omega : Y(\omega, x)\in B\text{ and }X(\omega)=x\}\right) = \sum_{x\in\operatorname{im}(X)}\mathsf P\left(\{\omega\in\Omega : Y(\omega, x)\in B\text{ and }X(\omega)=x\}\right). $$ By the stochastical independence assumed above, we have $$\sum_{x\in\operatorname{im}(X)}\mathsf P\left(\{\omega\in\Omega : Y(\omega, x)\in B\text{ and }X(\omega)=x\}\right)=\sum_{x\in\operatorname{im}(X)}\mathsf P(Y(\cdot, x)\in B)\mathsf P(X=x).$$ But the last expression equals $$\int_{\mathbb R} \mathsf P(Y(\cdot, x)\in B) \,X_\#\mathsf P(\mathrm dx)$$ which is equal to $$\int_{\mathbb R} Y(\cdot, x)_\#\mathsf P(B)\, X_\#\mathsf P(\mathrm dx).$$ So the proof is done. Is my approach saveable and is there an easier proof?","['pushforward', 'measure-theory', 'independence', 'probability-theory']"
4278517,Sum of angles under which a fixed line segment is seen from points situated on another line segment,"I have a question, like a picture attached below. I can find each angle by sin( or cosine) rule, but I think there is an easy way...a clue ...a concept which made it easy. can someone help me?
I do appreciate any hint. for example to find $A$ I use $$BC=\sqrt 2, AC=6 , AB=\sqrt {26}\\\cos(A)=\frac{c^2+b^2-a^2}{2bc}=\frac{36+26-2}{2*6*\sqrt{26}}$$ then find $A=11.3099$ and do like this for all the angles. But it is not the satisfying method. ( the gray squares are equal)
Thanks in advanced.","['geometry', 'calculus', 'triangles', 'trigonometry', 'algebra-precalculus']"
4278597,Wave equation: predicting geometric dispersion with group theory,"Context The wave equation $$
\partial_{tt}\psi=v^2\nabla^2 \psi
$$ describes waves that travel with frequency-independent speed $v$ , ie. the waves are dispersionless. The character of solutions is different in odd vs even number of spatial dimensions, $n$ . A point source in odd- $n$ creates a disturbance that propagates on the light cone and vanishes elsewhere: if the point source is a flash of light, an observer sees darkness, then a flash, then darkness. When $n$ is even, a disturbed media never returns to rest: the observer sees darkness, then brightness that lingers for all $t$ . This phenomena is known as geometric dispersion. Question Is it possible to show that geometric dispersion is predicted by the wave equation, using group theory? For a point source at the origin, we would be searching for spherically symmetric solutions, and the rotation group $SO(n)$ has a different structure depending on whether $n$ is odd or even. In particular, I am interested in doing this without actually solving the wave equation. Unfortunately, I don't know enough group theory to know if this is even possible. What I know I can 'show' geometric dispersion by solving the wave equation with an initial condition, or computing the Green's function for the wave equation and noting that it is either supported only on the light cone (odd $n$ ), or everywhere within the light cone (even $n$ ). I know some group theory 'for physicists'. Related This unanswered question is similar. I think my question is more specific: I'm asking about a way to predict (rather than explain) geometric dispersion using group theory. Update: (thanks to comments of Alp Uzman and GiuseppeNegro)
It appears to be possible using group theoretic machinery, described in the book Nonabelian harmonic analysis by Howe and Tan. The relevant section is 4.3.1. So an equivalent question becomes: can someone explain the result from Howe and Tan in a more accessible way? The book is beyond my level of group theory at the moment.","['dispersive-pde', 'wave-equation', 'partial-differential-equations', 'group-theory', 'mathematical-physics']"
4278640,Ore's theorem & Contraposition,"This is a question about contraposition i.e. if P implies Q then it is logically equivalent to not Q implies not P (~Q implies ~P) What is wrong with the following taken from Graph Theory? Let G be a (finite and simple) graph with n ≥ 3 vertices. We denote by deg v the degree of a vertex v in G, i.e. the number of incident edges in G to v. Then, Ore's theorem states that if deg v + deg w ≥ n for every pair of distinct non-adjacent vertices v and w of G then G is Hamiltonian. If P = deg v + deg w ≥ n for every pair of distinct non-adjacent vertices v and w of G and Q = G is Hamiltonian. Is it a valid statement to say, using ~Q implies ~P, that Not Hamiltonian implies that deg v + deg w < n for every pair of distinct non-adjacent vertices v and w of G This argument suggests a way of working out if a graph is not Hamiltonian! What is wrong with the argument?",['discrete-mathematics']
4278644,"find a nice smooth function $\lim_{x\to\infty} (f(x^2)-f(x))=1$, $\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$ and $\lim_{x\to-\infty} (f(x)-f(x^2))=1$?","can you find a nice real and smooth function where $\lim_{x\to\infty} (f(x^2)-f(x))=1$ , $\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$ and $\lim_{x\to-\infty} (f(x)-f(x^2))=1$ ? I've figured a function that follows the first limit. $$\lim_{x\to\infty} (f(x^2)-f(x))=1$$ I first tried logarithms and they didn't work. but it seemed close. $$\ln(x^2)-\ln(x)=\ln(x)$$ So next I tried logarithms of logarithms. and got $\ln(2)$ so I divided my function by $\ln(2)$ . $$\frac{\ln(\ln(x^2))}{\ln(2)}-\frac{\ln(\ln(x))}{\ln(2)}=1$$ because for all x this is true $\frac{\ln(\ln(x))}{\ln(2)}$ is a solution if we ignore the other two limits, the vertical asymptote at 1 and not being defined for all real numbers. $$\lim_{x\to\infty} (f(x)f(\frac{1}{x}))=1$$ The second limit I also found a solution for which is $f(x)=x^n$ where n is any real number because $x^n\times\frac{1}{x}^n=1$ $$\lim_{x\to-\infty} (f(x)-f(x^2))=1$$ for the last one I made sure to use the first on and reordered the terms to make it work and got $-\frac{\ln(\ln(-x))}{\ln(2)}$ . $$-\frac{\ln(\ln(-x))}{\ln(2)}+\frac{\ln(\ln(-x^2))}{\ln(2)}=1$$ even though I could find an example of a function that followed on of the limits, none of them followed all three limits and two of them weren't even defined for all real numbers. So can you find a function that follows all three of these limits, whose domain and range are the real numbers, and are smooth meaning to me continuous and differentiable everywhere.","['limits', 'calculus', 'continuity', 'real-analysis']"
4278670,"Let $X_1, \ldots , X_n$ be independent random variables having a common density with mean $\mu$ and variance $\sigma^2$, where","Let $X_1, \ldots , X_n$ be independent random variables having a common density with mean $\mu$ and variance $\sigma^2$ , where $\bar{X}=\frac1n\sum^n_{k=1}X_k$ . Calculate $\operatorname{Cov}(\bar{X}, X_k-\bar{X})$ Attempt $$\operatorname{Cov}(\bar{X}, X_k-\bar{X})=\operatorname{Cov}(\bar{X}, X_k)-\operatorname{Cov}(\bar{X},\bar{X})=\operatorname{Cov}(\bar{X}, X_k)-\operatorname{Var}(\bar{X})$$ However, $$\operatorname{Cov}(\bar{X}, X_k)=E(\bar{X}X_k)-E(\bar{X})E(X_k)$$ According to me, you have $$E(X_k)=\mu, E(\bar{X})=\mu, \operatorname{Var}(\bar{X})=\frac{\sigma^2}{n}$$ So, how do I calculate $E(\bar{X}X_k)$ ?, should I use the fact that they are independent variables? Edit $$E[\bar{X}X_k] = E\left[\frac{1}{n} \sum_{i=1}^n X_i X_k\right]
= \frac{1}{n} \sum_{i=1}^n E[X_i X_k] =\frac{\sigma^2}{n}+\mu^2?$$","['covariance', 'probability-theory', 'probability']"
4278717,Harmonic Numbers' Numerators Divisible by a Prime $p$,"For a prime $p$ , I am trying to determine the set of all $n$ for which the numerator of $H_n$ is divisible by $p$ , with $H_n$ being the $n$ 'th harmonic number. After going through a lot of literature, this turns out to be a difficult thing to do. The most promising paper regarding my question seems to be this paper by David W. Boyd . (PDFs easily accessible over the internet). Because the problem is difficult, I am trying to focus on primes $p<550$ as suggested by the paper. Using similiar notation to the paper, let $J_p$ be the set of those $n$ for which the numerator of $H_n$ is divisible by $p$ . They give in the paper: $J_3= \{2,7,22\}$ $J_5= \{4,20,24\}$ And I ran a computer program for quite some time to find: $J_7 = \{6,42,48,295,299,337,341,2096,2390,14675,16731,16735,102728...\}$ But I don't know if that's a complete list. EDIT: this turns out to be a complete list I believe this is the core of their technique / algorithm: Similiarly define, for a given $p$ : $G_m=\{p^{m-1}\leq n < p^m : p \mid H_n\} \quad  G_0=\{0\}$ Because it is known that $p \mid H_{p-1}$ , it must be that $p-1 \in G_1$ But I don't understand the next formulas that follow in the paper: $H_n=ap + O(p^2) , \quad H_{pn+k} = a + H_k + O(p) , \quad a + H_k = O(p)$ Does it mean that given the other parameters in the RHS of the equation, I can get the LHS in $O(p)$ time? Also, what is the variable $a$ exactly? It's not mentioned in the paper beforehand. How do I find it? Looking at the values in the given $J_p$ , it seems the solutions come in ""subsets"", so that solutions in subset $s_{i+1} \in S_{i+1}$ are formed from solutions $s \in S_i$ such that every $ps \leq s_{i+1} < ps + p$ for some $s$ . For example, looking at $J_7$ , then $42$ can generate solutions in the range $[7\cdot 42 , 7 \cdot 42 + 7]=[294,301]$ , similiarly $48$ generates solutions in the range $[7\cdot 48 , 7 \cdot 48 + 7]=[336,343]$ . And they both generate $2$ solutions each. This means I would only need to check divisibility of numerators in a small range (assuming $p$ is small), resulting in $O(mp)$ algorithm, with $m$ defined as above. Is that observation correct? I think it is similiar to how they define $G_m$ . But How can I quickly obtain $H_n$ mod $p$ for some $n$ using previous values? Only a few values need to be checked, but as $m$ grows, the numerators become extremely large, about $p^m$ . Another paper I found gives a more detailed example of the technique: p-Integral harmonic sums Answers regarding any of my questions will be great. The best answer I can hope for is an explanation of their technique, what they do exactly and how they do it. A sketch for their technique revolving around $J_7$ as an example would be great, going through each of the $m$ 's before $G_m$ becomes empty.","['number-theory', 'linear-algebra', 'discrete-mathematics', 'algorithms', 'computer-science']"
4278742,n-player survivor game,"Came up with a following game recently and after trying to understand the general strategy for hours I have to admit I failed at finding one, so I thought asking here would be a good idea. As a good friend of mine restated: Define the ""n-player survivor game"" as follows: label the players 1, .., n. First, in ascending order by index, each player publicly announces who they plan to eliminate (possibly themselves). Then, in descending order by index, each player eliminates the player they previously announced. If a player is already eliminated, they can no longer eliminate the player they previously announced. Each player is rational, with the following goals [in order]: (1) survive (don't get eliminated); (2) leave as few survivors as possible. If some player k can make multiple decisions with the same outcomes w.r.t. goals (1) and (2), they choose a decision uniformly at random among those decisions. If you try for n=3 and 4, it quickly becomes clear that players will follow a mixed strategy. Is there a closed form P_n(k) for the probability that player k survives in the n-player survivor game? (Players are not allowed to agree on strategies beforehand. The only information they can share is their announcement of who to eliminate.) We found a strategy for n=3 and made a tree of choices for n=4, but there's no clear sign of it being trivial for any given n. I can answer questions about the game, so you can ask in comments. I'll be up for a while. The game runs exactly once, so there may not be a clear winner, just surviving and eliminated players. I'm noting ""player k chooses player j"" as k!j to simplify the notation.","['probability', 'combinatorial-game-theory']"
4278750,Can Euclid´s propostion in Book 1 proposition 1 be saved by using a special value for the radius?,"We know that Euclid in Book 1 proposition 1 made an error in that he uses the intersection of two circles which cannot be shown to exist, following his axioms. If we imagine a closed spherical surface for example, we get intuitively the idea that at least one circle with a small enough radius can be found to fulfill his proposition for any geometry where the axioms would hold. Is that correct?","['euclidean-geometry', 'geometry']"
4278784,Center of the symplectic group,"I am trying to figure out what the center of $Sp(n)$ is. I know that $Sp(n) = U(n) \cap Sp(n,\mathbb{C})$ , where $Sp(n,\mathbb{C})$ are all $2n \times 2n$ complex matrices $A$ such that $A^TJA = J$ , where $$
J
=
\begin{pmatrix}
0 & -I_n\\
I_n & 0
\end{pmatrix}.
$$ I was able to find the center of most of the other classical matrix groups using the Spectral theorem and permutation matrices, but for this one I'm having a hard time. Maybe after I use the spectral theorem on an element of the center, I could decompose the unitary matrix that diagonalizes it by an element in $Sp(n)$ and some other matrix?","['lie-algebras', 'linear-algebra', 'group-theory', 'lie-groups', 'matrix-decomposition']"
4278822,How to prove $|z_1+z_2|<|1+\bar{z_1}z_2|$ if $|z_1|<1$ an $|z_2|<1$ [duplicate],"This question already has answers here : Show that $\left|\frac{\alpha - \beta}{1-\bar{\alpha}\beta}\right| < 1$ when $|\alpha|,|\beta| < 1$ (4 answers) Closed 2 years ago . I would like to prove that $|z_1+z_2|<|1+\bar{z_1}z_2|$ if $|z_1|<1$ an $|z_2|<1$ . I tried to multiply the numerator and denominator by $|1+z_1\bar{z_2}|$ or consider the square, but I couldn't finish anything. Someone has a hint as I can solve this problem?","['complex-analysis', 'analysis']"
4278832,Asymptotic integration of $\int_0^\infty\frac{x^{-\frac{1}{2}+a}J_{-\frac{1}{2}+a}(x\alpha)}{e^x-1}{\rm d}x$ when $\alpha \gg 1$,What is the asymptotic integration of $$\int_0^\infty\frac{x^{-\frac{1}{2}+a}J_{-\frac{1}{2}+a}(x\alpha)}{e^x-1}{\rm d}x$$ when $\alpha\rightarrow\infty$ . How to compute that using standard identities? Please give a general expression and then consider a special case where $a=\frac{5}{2}$ .,"['integration', 'definite-integrals', 'special-functions', 'asymptotics']"
4278872,$\left(S_{n}\right)_{n \geq 0}$ be a simple symmetric random walk. Prove $ P_{0}(S_{m}=x \mid S_{n}=y)=P_{0}(S_{n-m}=y-x \mid S_{n}=y) $,"I am stuck on the question below and I am not sure if I am heading in the right direction. Will be grateful for any direction. Let $\left(S_{n}\right)_{n \geq 0}$ be a simple symmetric random walk.
(a) Suppose that $m, n, x, y$ are integers such that $n>m>0$ , and $n, y$ have the same parity. Prove that $$
P_{0}\left(S_{m}=x \mid S_{n}=y\right)=P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right)
$$ (b) Hence compute $E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right)$ and $E_{0}\left(S_{m} \mid S_{2 m}=2 x\right)$ . My attempt I am stuck on how to fine $P_{0}\left(S_{m}=x \mid S_{n}=y\right)$ but this is what I did: Le $\varepsilon_{1}, \ldots, \varepsilon_{n}$ be elements of $\{-1,+1\}$ such that $\varepsilon_{1}+\cdots+\varepsilon_{n}=x$ . Then I can write $P_{0}\left(S_{m}=x \mid S_{n}=y\right)$ as: $$P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x \mid S_{n}=y\right)$$ $$=\frac{P_{0}\left(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x\right)}{P_{0}\left(S_{n}=y\right)}$$ From the notes of Markov-chain and random walks by Takis , using Lemma 16 p.83, I know that $$P_{0}\left(S_{n}=y\right)=\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right) 2^{-n}$$ And since each $\varepsilon_{i}$ should be uniformly distributed then $$P_{0}(\varepsilon_{1} +, \ldots, + \varepsilon_{m}=x ) = \frac{1}{2^m}$$ Therefore: $$P_{0}\left(S_{m}=x \mid S_{n}=y\right) = \frac{2^{-m}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}}$$ Similarly: $$P_{0}\left(S_{n-m}=y-x \mid S_{n}=y\right) =\frac{P_{0}\left(\varepsilon_{m} +, \ldots, + \varepsilon_{n}=y-x\right)}{P_{0}\left(S_{n}=y\right)} = \frac{2^{-(n-m)}}{\left(\begin{array}{c}n \\ (n+y) / 2\end{array}\right)2^{-n}}$$ I am not sure if what I am doing is correct so any insight would help me a lot. For the second part where I need to find $E_{0}\left(S_{m}+S_{n-m} \mid S_{n}=y\right)$ , I believe I can split this into $$E_{0}\left(S_{m} \mid S_{n}=y\right) + E_{0}\left(S_{n-m} \mid S_{n}=y\right)$$ Then I just take the expectation of the binomial distributed part and uniformly distributed part, am I right? Thank you","['uniform-distribution', 'random-walk', 'binomial-distribution', 'markov-chains', 'probability']"
4278912,Non-standard calculus of variations solution leads to contradiction,"I've been given the calculus of variations problem of extremizing $$I=\int_a^bf(x,y,y')dx=\int_a^by\sqrt{1+y'^2}dx$$ where of course $y$ is a function of $x$ . I came up with what I thought was a pretty sharp solution method that got around nonlinearities in the application of the Euler-Lagrange equation, but it leads to an apparent contradiction: Since there is no direct $x$ dependence in the integrand, the Hamiltonian/first-integral   must be constant for the extremizing $y$ , i.e. $$f-y'\frac{\partial f}{\partial y'}=c_1\Leftrightarrow y\sqrt{1+y'^2}-\frac{yy'^2}{\sqrt{1+y'^2}}=c_1\Leftrightarrow y+yy'^2-yy'^2=c_1\sqrt{1+y'^2}$$ $$\Leftrightarrow y^2=c_1^2(1+y'^2)\Leftrightarrow 1+y'^2= \frac{y^2}{c_1^2}.\tag{1}$$ Extremal $y$ must also satisfy the normal Euler-Lagrange equations $$\frac{\partial f}{\partial y}=\frac{d}{dx}\frac{\partial f}{\partial y'}\Leftrightarrow \sqrt{1+y'^2}=\frac{d}{dx}\left[ \frac{yy'}{\sqrt{1+y'^2}} \right]$$ $$\Leftrightarrow \sqrt{1+y'^2}=\frac{y'^2}{\sqrt{1+y'^2}}+yy''\left(\frac{1}{\sqrt{1+y'^2}}- \frac{y'^2}{(1+y'^2)^{3/2}} \right)$$ $$\Leftrightarrow (1+y'^2)^2=y'^2+y'^4+yy''+yy'^2y''-yy'^2y''$$ $$\Leftrightarrow y'^4+2y'^2+1=y'^2+y'^4+yy''$$ $$\Leftrightarrow y'^2-yy''+1=0.\tag{2}$$ Substituting the first-integral expression for $1+y'^2$ , $$\frac{y^2}{c_1^2}=yy''\Leftrightarrow y''-\frac{y}{c_1^2}=0.\tag{3}$$ However, the solution to this linear ODE is not a solution to the Euler-Lagrange ODE. The only way I can see that the argument is unjustified is if $c_1=0$ and therefore $y=0$ , but boundary conditions were given in the problem that this solution cannot satisfy. I've even written out proofs of the E-L equations and Hamiltonian conservation that don't appear to make any unjustified assumptions. Any ideas?","['classical-mechanics', 'ordinary-differential-equations', 'calculus-of-variations']"
4278979,A question about the maximal domain of a function,"So I have got the following equation: $$h(x) = \sqrt{\frac{1}{x+1}+1}$$ I need to find the maximal domain of the function. I have tried doing it algebraically: As this is a square root, $\frac{1}{x+1}+1$ , must be greater than $0$ and there is an asymptote at $x = -1$ . $\frac{1}{x+1}+1 > 0$ $\frac{1}{x+1} > -1$ $ 1 >-x-1$ $1+x>-1$ $\to x>-2$ , provided that $x \not= -1$ But this is incorrect $:($ The answers show a different maximal domain. Moreover, I do not know how they got their answer. I need help. Thanks!!!","['algebra-precalculus', 'functions', 'inequality']"
4278990,find all functions satisfying $f(x+y)-f(x-y)=f(x)f(y)$ for $f: \Bbb R \to \Bbb R$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I need some help to find all of the functions which satisfy the equation: $$f(x+y)-f(x-y)=f(x)f(y)$$ Actually I have no idea how to start to solve this problem.",['functions']
4279020,Does weak convergence imply pointwise convergence? [duplicate],"This question already has an answer here : Does weak convergence in $L^2$ implies convergence almost everywhere along subsequence? (1 answer) Closed 2 years ago . Let $f_n,f:\mathbb R^d\to[0,\infty)$ with integral $1$ over $\mathbb R^d$ .
Suppose that $$ \int_{\mathbb R^d}\phi(x)\,f_n(x)\,d x\,\to\,\int_{\mathbb R^d}\phi(x)\,f(x)\,d x $$ as $n\to\infty$ for all $\phi:\mathbb R^d\to\mathbb R$ continuous and bounded. Can I say that there exists a strictly increasing sequence $(m_n)_{n\in\mathbb N}$ such that $$f_{m_n}(x)\to f(x) \textrm{ for a.e. }x\in\mathbb R^d$$ as $n\to\infty$ ? I know this would be the case for convergence in total variation, since it is equivalent to convergence of densities in $L^1$ and so there exists a subsequence which is convergent almost everywhere. Is weak convergence sufficient?","['weak-convergence', 'real-analysis', 'functional-analysis', 'convergence-divergence', 'probability-theory']"
4279038,Are those sections $E_x$ still measurable a.e. when $E$ has measure $\infty$?,"I have a question about the  proposition below
(Proposition 5 on p. 419 of Real analysis (by Halsey Royden and Patrick Fitzpatrick) ): Proposition. Let $(X,\mathscr{A},\mu)$ and $(Y,\mathscr{B},\nu)$ be two measure spaces ( not necessarily $\sigma$ -finite ).
Assume the measure $\nu$ is complete. Let $E \subset  X \times Y$ be measurable with respect to
the product measure $\mu \times \nu$ and $(\mu \times \nu) (E) < \infty$ . Then for almost all $x \in X$ , the $x$ -section of $E$ , $E_x$ ,
is a $\nu$ -measurable
subset of $Y$ , the function $x \mapsto \nu (E_x)$ for $x \in X$ is a $\mu$ -measurable function, and $$\left(\mu\times \nu\right)(E)=\int_X \nu \left(E_x\right) \, \text{d}\mu (x). $$ My question is: Does the proposition hold if $\left(\mu\times \nu\right)(E)=\infty$ ? (Maybe the question seems unnecessary, since it does not arise in the proof of Fubini's theorem; but I think it is a very natural question.) I know if the spaces are $\sigma$ -finite, then it is clearly true. But for a non- $\sigma$ -finite space, it seems non-trivial for me. I tried to find a counterexample; the most common space which is not $\sigma$ -finite is an uncountable set endowed with its counting measure (however,  in this case, any subset is measurable, so it does not work). Could anybody give me some ideas? Would be appreciated. EDIT. (The definition of the product measure $\mu\times \nu$ ). Let $\mathcal{S}:= \{A\times B: A\in \mathscr{A},B\in \mathscr{B}\}$ be a semiring (the sets in $\mathcal{S}$ are called ""measurable rectangles""). We define a nonnegative set function $\mu\times \nu:\mathcal{S}\to [0,+\infty]$ by $(\mu\times \nu)(A\times B) = \mu(A)\cdot \nu (B)$ , for $A\times B\in \mathcal{S}$ . We can verify that the set function $\mu\times \nu$ defined on $\mathcal{S}$ satisfies $(\mu\times\nu)(\varnothing)=0$ and countable additivity. Carathéodory's Extension Theorem says that we can define an outer measure $\left(\mu\times \nu\right)^*:2^{X\times Y}\to [0,+\infty]$ (which is called the Carathéodory outer measure induced by $\mu\times \nu$ ); and a subset $T\subset X\times Y$ is called $\left(\mu\times \nu\right)^*$ -measurable  iff $\left(\mu\times \nu\right)^*(E)= \left(\mu\times \nu\right)^*(E\cap T)+\left(\mu\times \nu\right)^*(E\setminus T)$ for all $E\subset X\times Y$ ; moreover, the collection of all the $\left(\mu\times \nu\right)^*$ -measurable sets forms a $\sigma$ -algebra $\mathfrak{M}_{\left(\mu\times \nu\right)^*}$ on $X\times Y$ which contains $\mathcal{S}$ (and thus contains $\sigma(\mathcal{S})$ ), and the restriction of $\left(\mu\times\nu\right)^*$ to $\mathfrak{M}_{\left(\mu\times \nu\right)^*}$ becomes a measure (we still denote it by $\mu\times \nu$ ); and $\left(X\times Y, \mathfrak{M}_{\left(\mu\times \nu\right)^*},\mu\times \nu\right)$ is a complete measure space. In the proposition above, "" $E\subset X\times Y$ be measurable with respect to the product measure $\mu\times \nu$ "" means $E$ is $\left(\mu\times \nu\right)^*$ -measurable, i.e., $E\in\mathfrak{M}_{\left(\mu\times\nu\right)^*}$ (I do NOT mean that $E$ is in the product $\sigma$ -algebra $\mathscr{A}\otimes \mathscr{B}:= \sigma\left(\mathcal{S}\right)$ ).","['measure-theory', 'lebesgue-integral', 'real-analysis']"
4279046,An uncommon continued fraction of $\frac{\pi}{2}$,"I'm currently stuck with the following infinite continued fraction: $$\frac{\pi}{2}=1+\dfrac{1}{1+\dfrac{1\cdot2}{1+\dfrac{2\cdot3}{1+\dfrac{3\cdot 4}{1+\cdots}}}}$$ There is an obscure clue on this: as one can derive the familiar Lord Brouncker’s fraction below $$
\frac{4}{\pi}=1+\dfrac{1^{2}}{2+\dfrac{3^{2}}{2+\dfrac{5^{2}}{2+\dfrac{7^{2}}{2+\cdots}}}}
$$ from the Wallis' Formula : $$
\dfrac{2}{\pi}=\frac{1 \cdot 3}{2 \cdot 2} \cdot \frac{3 \cdot 5}{4 \cdot 4} \cdot \frac{5 \cdot 7}{6 \cdot 6} \cdot \frac{7 \cdot 9}{8 \cdot 8} \cdots
$$ the first fraction can be proved in the same manner. However, I'm not getting any close to it using the Wallis' Formula. Really appreciated if anyone could point me the right direction or explain further how to systematically derive those continued fractions from any given convergent cumulative product.","['continued-fractions', 'pi', 'sequences-and-series']"
4279082,Measure is Sum of Inner Measure and Outer Measure,"In E is measurable, then measure of E is the sum of the inner measure of a subset of E and the outer measure of the complement of the subset in E it is asked to prove $|E|=|A|_i+|E-A|_e$ where $A \subset E$ is any subset of $E$ , does not necessarily measurable. I am curious about the answer of this problem since I also haven't managed to solve it yet. Thank you.","['measure-theory', 'lebesgue-measure']"
4279104,Find a rigid motion,"I am looking at the following curve: $$c(t)=(t + \sqrt3\sin t, 2\cos t, \sqrt3t-\sin t)$$ My goal is to find a rigid motion $L$ of $\Bbb R^3$ and a helix of the form: $$\gamma (t)=(a\cos(t),a\sin(t),bt)$$ such that $L \circ c = \gamma$ . Now I found the curvature $\kappa=1/4$ and the torsion $\tau=-1/4$ of $c$ . From there I could find $a=2$ and $b=-2$ . But know I got stuck to find a rigid motion... How can I proceed? Many thanks for some help!","['curves', 'differential-geometry']"
4279124,Compute the following integral: $\int_D \ln(x^2+y^2)dxdy$,"Compute the following integral: $\int_D \ln(x^2+y^2)dxdy$ where $D$ is the disc $x^2+y^2 \le1$ in $R$ into polar coordinates. What I have tried: $r^2\le1 \implies r\le1$ we also have $0 \le\theta\le2\pi$ This produces $$\int_0^{2\pi} d\theta\int_0^1\ln(r)\cdot r dr \implies 2\pi\int \ln(r)\cdot r dr$$ Taking the integral by parts to tackle the $dr$ integral we have $du = \frac{1}{r}$ and $v = \frac{r^2}{2}$ Which gives $$2\pi \left[\frac{\ln(r)r^2}{2}\Biggr|_0^1-\frac{1}{2}\int_0^1 rdr\right]\implies-\frac{2\pi}{4}$$ However, the right answer should be $-\pi$ any idea where I went wrong? Also, the actual question asks me to let "" $D_\delta$ be the annulus $\delta^2\le x^2+y^2\le1$ as I'm supposed to show that this integral has a finite limit which is $-\pi$ although I'm unsure on how to proceed with this and would really appreciate the communities support.","['multivariable-calculus', 'multiple-integral']"
4279179,Symmetric difference inclusion proof,"Show $$\left(A_{1} * A_{2}\right) \vartriangle\left(B_{1} * B_{2}\right) \subset\left(A_{1} \vartriangle B_{1}\right) \cup\left(A_{2} \vartriangle B_{2}\right)$$ where $*$ can be $\cup, \cap, \setminus.$ I proved this inclusion for $\cup, \cap$ but I have no clue how to prove it for $\setminus$ . I know that the symmetric difference $A \vartriangle B$ of two sets $A, B \subset M$ is defined as follows: $$
A \triangle B:= (A \backslash B) \cup (B \backslash A) = (A \cup B) \backslash (A \cap B) 
$$ but I cannot apply my first steps of the other two proofs. Nothing seems to work. Has anyone a hint or an idea how to start?",['elementary-set-theory']
4279210,"If two curves only touch, do they technically ""intersect""?","I was playing sprouts with a few friends the other day, and one of them tried to be clever by ""squeezing"" their line next to another player's line to effectively prevent any passthrough play: Now, I'm pretty terrible with the trackpad in paint, so bare with my low quality art here. The focus is the upper right area where the red and blue lines look like they're intersecting (just imagine they aren't as it's a side effect of my terrible paint skills). The idea the red player had was to shrink the playable region between their line and the blue player's line as much as possible, effectively closing the area off. However, this annoyed both myself (green) and the red player. Since none of us could truly determine if the rules permitted this movement, we all agreed at the time that since the rules prevent intersection, the space between the red and blue lines was just really small. As such, pass through could still occur with the understanding that the line passing through would ""shrink"" into the space and expand back to normal size on the other side, effectively simulating a pass through without intersection. Unfortunately though, my curiosity is getting the better of me and now I'm wondering if it's wrong to assume this since two objects must overlap to be considered an intersection, by definition: (of two or more things) pass or lie across each other. Or not? If the outermost edges of two objects only touch , is it technically an intersection? Edit : For close votes related to this being off-topic due to not being about mathematics; my question isn't focused on the game of sprouts, but rather something I encountered during play that I was looking for clarification on. That something is terminology related to intersection and is mathematics related.","['combinatorial-game-theory', 'geometry', 'terminology']"
4279216,"Is the space $\mathcal C([0,1])$ endowed with the sup norm homeomorphic to $\mathcal C([0,1])$ endowed with the integral norm?","Consider the space $\mathcal C([0,1])$ of all continuous functions from $[0,1]$ into $\Bbb R$ . A norm which is natural to use in this space is the sup norm: $\|f\|_\infty=\sup|f|$ ; another one is the integral norm: $\|f\|_1=\int_0^1|f|$ . Are $\bigl(\mathcal C([0,1]),\|\cdot\|_\infty\bigr)$ and $\bigl(\mathcal C([0,1]),\|\cdot\|_1\bigr)$ homeomorphic? My guess is that they are not, but I am unable to prove it. It is clear that these metrics are not equivalent. And, of course, since $\bigl(\mathcal C([0,1]),\|\cdot\|_\infty\bigr)$ is a complete metric space, whereas $\bigl(\mathcal C([0,1]),\|\cdot\|_1\bigr)$ isn't, there is no bijection $f\colon\bigl(\mathcal C([0,1]),\|\cdot\|_\infty\bigr)\longrightarrow\bigl(\mathcal C([0,1]),\|\cdot\|_1\bigr)$ such that both $f$ and its inverse are uniformly continuous, but this doesn't prove the impossibility of the existence of a homeomorphism.","['general-topology', 'metric-spaces']"
4279251,"What does $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ look like and ""repeat"" extrema?","I have to find the absolute min and max of $f(x, y) = x^2 - x^2y$ on D. So first my question is what does this closed, bounded set, $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ look like? I have drawn what I think it looks like, below If my drawing is correct, To find global extrema, I am finding the max/min on the boundary and the critical points to do some comparisons. I have no problem finding the critical points, but a bit confused about the boundaries. Can I continue this problem by investigating each of the boundary/line? For example, setting $x = 1, 1 ≤ y ≤ 2$ (right) , $x= 0, 1 ≤ y ≤ 2$ (left) $y = 2, 0 ≤ x ≤ 1$ (top) $y =1, 0 ≤ x ≤ 1$ (bottom) And find the max/min on each boundary? Do i have to do this? I have done some previous questions, and realised that some of the extrema occur more than once. How can I avoid these ""repeats"" extrema? Do we always get ""repeats""? How do you tell? EDIT: Also can you have critical points in general  but have NO critical points satisfying $D = \{(x, y) : 0 ≤ x ≤ 1$ and $1 ≤ y ≤ 2\}$ ?
Then the max/min cannot occur here right?","['maxima-minima', 'multivariable-calculus']"
4279291,Cross-sections of a cone inscribed in a cylinder,"The Problem: Consider a right circular cone inscribed within a right circular cylinder. In any cross-section of the cylinder which passes through its axis, 50% of the area of the resultant rectangle lies within the triangular cross-section of the cone, as in this image: I have seen and been convinced by many proofs for the fact that such a cone occupies precisely 1/3 the volume of the cylinder. However, this conflicts with my (deeply amateur) intuition around this cross-section. Clearly, all the infinitely-many cross sections that could be taken revolving around the axis would be identical, with 50% of their area within the cone, this leads to the feeling that the cone occupies half the volume of the cylinder. The Question: Why is this a bad approach to this problem, and how is my intuition leading me astray? I don't need to be convinced that the cone occupies 1/3 of the volume of the cylinder, I just want to understand where and why my approach falls down. Some caveats/thoughts: I am (surprise surprise) not a mathematician, although I am an enthusiast! I'm not great with equations and formulae, so a literary answer would be appreciated (if at all possible). I am most interested in set and number theories, and these have doubtless guided my intuition on this problem. I understand calculus' approach to this problem using disc integration. I sense this misunderstanding relates to the idea that my suggested cross-sections are somehow ""overlapping"" closer to the axis, but they are two-dimensional, so how could that be? I can see that every cross section would contain all the points along the cylinder's axis, but that accounts for almost no elements of the set of points contained within each cross-section of the cone. Is this a mistake of trying to intuit three-dimensional volume using only two-dimensional cross sections? Is this just infinity stuff being unhelpful again? Help me, Stack Exchange!","['volume', 'problem-solving', 'geometry', 'intuition']"
4279304,"If $x:[0,\infty)\to E$ is càdlàg and $\tau_0:=0$, show that $\inf\left\{t>\tau_{n-1}:\Delta x(t)\in B\right\}\xrightarrow{n\to\infty}\infty$","Let $E$ be a normed $\mathbb R$ -vector space and $x:[0,\infty)\to E$ be càdlàg with $x(0)=0$ . Moreover, let $x(0-):=x(0)$ , $$x(t-):=\lim_{s\to t-}x(s)\;\;\;\text{for }t>0$$ and $$\Delta x(t):=x(t)-x(t-)\;\;\;\text{for }t\ge0.$$ Given $B\subseteq E$ bounded below, let $\tau_0:=0$ and $$\tau_n:=\inf\underbrace{\left\{t>\tau_{n-1}:\Delta x(t)\in B\right\}}_{=:\:I_n}$$ for $n\in\mathbb N$ . How can we show that $\tau_1>0$ and $\tau_n\xrightarrow{n\to\infty}\infty$ ? I know that $\left|\left\{t\in[a,b]:\Delta x(t)\ne0\right\}\right|\in\mathbb N_0$ for all $a,b\in\mathbb R$ and hence $\left\{t\ge0:\Delta x(t)\ne0\right\}$ is countable. Does the claim somehow follow from this fact? For example, if $\tau_1=0$ , then we would find a nonincreasing $(t_n)_{n\in\mathbb N}\subseteq I_1$ with $t_n\xrightarrow{n\to\infty}0$ . Is this a contradiction? I know how we can prove a similar statement for any right-continuous $y:[0,\infty)\to E$ . Assuming that $B$ is closed and $I:=\{t\ge0:y(t)\in B\}$ is nonempty, we can easily show that $\tau:=\inf I\in I$ . Moreover, if $y(0)\not\in B$ , we can show that $\tau>0$ .","['levy-processes', 'real-analysis', 'continuity', 'stopping-times', 'probability-theory']"
4279338,Function that wraps unit circle twice around itself,"I'm looking for an example of a function $f: S^1 \to S^1$ (from unit circle to unit circle), that is continuous, open and surjective but not injective. I have intuitively thought of example, that would be a function that ""wraps"" unit circle twice around itself. More precisely this is what I had in mind. We can uniquely write points on unit circle as $(\cos t, \sin t)$ , where $t \in [0, 2 \pi)$ . And the function would be $f: S^1 \to S^1$ with $(\cos t, \sin t) \to (\cos 2t, \sin 2t)$ .  Now, I'm not even sure if this would be a good thing to look at, and if it was, I don't know how to show that $f$ is continuous and open. Keep in mind that this is for course of real analysis and I'm not allowed to use facts and parametrizations from complex analysis.
Any comments or suggestions are very welcomed.","['general-topology', 'analysis', 'real-analysis']"
4279341,If a set is closed under unions and intersections is it closed under complements?,"I'm thinking of sigma algebras here, which are (nonempty) sets closed under countable unions, countable intersections, and complements. But you only need 2 of these conditions to guarantee the third: If a (nonempty) set is closed under countable unions and complements, then it is closed under countable intersections (countable De Morgan). If a (nonempty) set is closed under countable intersections and complements, then it is closed under countable unions (countable De Morgan). Now I ask: If a (nonempty) set $X$ is closed under countable unions and countable intersections, then is it closed under complements? (Does this change if $X$ is finite, countable, or uncountable?) (I say ""set"" because in ZFC everything is a set, but people often call these ""families"" or ""collections"", ie. sets of sets.) Bonus questions: If a set is closed under finite unions, then it is closed under countable unions? If a set is closed under finite intersections, then it is closed under countable intersections?","['general-topology', 'probability-theory', 'measure-theory']"
4279374,"Spivak Question 14, Chapter 1, clarifying a statement.","Part (a) proves $\left| a \right| = \left| -a \right|$ The actual proof in Spivak's workbook is pretty straight forward. Let's accept this as done. Part (b) proves $-b \le a \le b \iff \left| a \right| \le b$ . There are many questions that prove this many ways in Stack exchange, including of course questions about Spivak's approach. Again, for the sake of argument, let's accept this proved. Now, Spivak says it follows that $-\left| a \right| \le a \le  \left| a \right|$ A questioner asked for an explanation, and the most upvoted answer was let $b = |a|$ and you are done. Could someone explain the obvious here? Yes, indeed, if you substitute $|a|$ for $b$ it provides the asked for proof, but the notion that one can simply replace a variable with an absolute value is, I believe what the previous questioner and I are asking about. And, almost inevitably, it means that there is something we, or I in this case, am missing in my understanding of absolute values. If this question is a little confusing, it probably means I am missing something obvious, hence the question.","['calculus', 'algebra-precalculus', 'absolute-value']"
4279425,How can we prove that a positive-frequency wave and its time-derivative cannot both have compact support in space?,"Consider a complex-valued function of the form $$
\newcommand{\bfx}{\mathbf{x}}
\newcommand{\bfk}{\mathbf{k}}
f(t,\bfx)=\int d^Nk\ g(\bfk)\exp\big(-i\omega(\bfk)t-i\bfk\cdot\bfx\big)
\tag{1}
$$ where boldface denotes a list of $N$ real variables, the dot-product is defined as usual, and $$
\omega(\bfk)\equiv \sqrt{\strut{}1+\bfk\cdot\bfk}.
\tag{2}
$$ (I'm calling this a ""wave,"" but notice the constant term under the square root.) Suppose that $f(0,\bfx)$ is nonzero at least for some $\bfx$ . Can we choose $g(\bfk)$ so that $f(t,\bfx)$ and $df(t,\bfx)/dt$ both have compact support in $\bfx$ at $t=0$ ? The answer must be no, because otherwise I could use the Paley-Wiener theorem to construct a contradiction to the Reeh-Schlieder theorem . But that's a very indirect argument that uses relativistic quantum field theory, which surely isn't necessary for the simple question I'm asking here! How can we prove more directly that no such $g(\bfk)$ exists?","['smooth-functions', 'functional-analysis', 'asymptotics', 'partial-differential-equations']"
4279458,Prove that a certain function defined by cases from $\mathbb{Z}\times\mathbb{Z}\rightarrow\mathbb{N}$ is a bijection,"Question : Prove that the $g:\mathbb{Z}\times\mathbb{Z}\rightarrow\mathbb{N}$ defined as : $
    \begin{equation*}
g(x,y)=\begin{cases}
          2x+(2x+2y-1)(x+y-1) \quad &\text{ if } \, x \geq1 , y\geq1 \\
          2x+(x-y)(2x-2y-1) \quad &\text{ if } \, x \geq1 , y\leq0 \\
          1-2x+(y-x)(2y-2x-1) \quad &\text{ if } \, x \leq 0, y\geq1 \\
          1-2x-(1-2x-2y)(x+y) \quad &\text{ if } \, x \leq 0, y\leq0 \\
     \end{cases}
\end{equation*}
$ Is a bijection. My thinking: To show that it is injective I think we have to consider the sixteen cases and show that in every case $x_1 = x_2 , y_1 = y_2$ if $g(x_1,y_1) = g(x_2,y_2)$ but the problem that I am not even able to prove a single case and for proving it a surjective I have no idea how to even start. Second approach I thought of was to find the inverse and then using the fact if the inverse exist then it is a bijection but both the approaches seen to very difficult for that monster looking definition of $g$ .If someone has some good solution or even prooving even proof for single case in injectivity and one case in surjectivity will be great help. Edit 1: If we prove that the function $k:\mathbb{N}\times\mathbb{N}\rightarrow\mathbb{N}$ defined as : $
k(x,y) = x + \frac{(x+y-1)(x+y-2)}{2}
$ Is a bijection then above can prooved as it is a composition of $k$ and $h:\mathbb{Z}\rightarrow\mathbb{N}$ defined as : $
\begin{equation*}
h(x)=\begin{cases}
          2x \quad &\text{if} \, x \geq1 \\
          -2x+1 \quad &\text{if} \, x \leq0 \\
     \end{cases}
\end{equation*}
$ I hope to get a nice solution. Edit 2: I have proved that it is a injection considering all 16 cases the only part left is for to prove it a surjection, I still dont know how will I find expressions for all $k\in\mathbb{N}$ there exist $m,n\in\mathbb{N}$ such that $g(m,n) = k$","['functions', 'discrete-mathematics']"
4279470,Calculation of area of a cyclic quadrilateral,"I found this problem in a Geometry group on Facebook. I know such problems may have errors but the OP claims it is correct. We are given that $ABCD$ is a square.
Also $EG = GH = 4$ and $\angle EGH = 90^\circ$ .
Also $GF = 10$ .
We want to calculate the area of $DEGH$ . I tried to split it into two triangles, EGH and EDH. Area of $EGH = \frac {1}{2}*4*4 = 8$ . Also $EH^2 = 4^2+4^2 = 32$ . Area of $EDH = \frac {1}{2}*DE*DH$ and $DE^2+DH^2 = 32$ . Rectangle DEGH is cyclic, so angles EDG and GDH are $45^\circ$ . Also, if we draw the altitudes of triangles GDH and GDE, these are equal (because a square is formed) and then the area of DEGH is equal to the area of this square. However, I don't see how to calculate this altitude (or the side of the square). I also see some similar right triangles with hypotenuses 4 and (10+4) but then I am missing the side of the triangle ABCD. Any clues?? Thank you!","['euclidean-geometry', 'geometry']"
4279480,How to solve this ODE: $y^{(y(x))}(x)=f(x)$?,"$$\large{\text{Introduction:}}$$ This question will be partly inspired from: Evaluation of $$y’=x^y,y’=y^x$$ but what if we made the order of an differential equation equal to the function? Imagine that we had the following linear ordinary differential equation using nth derivative notation . In other words, the $y(x)\,th$ derivative of $y(x)$ is $f(x)$ . I would have made $f(x)\to f(x,y)$ , but that is too hard to solve. Note that $f(x)$ is any continuous infinitely differentiable function: $$\frac{d^{y(x)}}{dx^{y(x)}}y(x)=\text D^{y(x)}_x y(x)=f(x)\implies y^{(y)}=f(x)\implies y^{(y)}=f$$ $$\large{\text{Specific Values:}}$$ Here are some examples of points: $$y^{(y(0))}(0)=f(0), y^{\left(y \left(\frac12\right)\right)} \left(\frac12\right)=f\left(\frac12\right) $$ we could also imagine an inverse relation for $y(x)$ called $y^{-1}(x)$ : $$0=f\left(y^{-1}(0)\right), y^{\left(\frac12\right)} \left(\frac12\right) = f\left(y^{-1}\left(\frac12\right) \right),y’(1)=f\left(y^{-1}(1)\right)$$ $$\large{\text{Problem Statement:}}$$ this presents the problem that if $y(x)\not\in\Bbb Z$ , then we need a fractional derivative , but definitions for such derivatives are confusing, so we can use the inverse operation of the fractional derivative which is the fractional integral using the corresponding notation in the bolded link: $$y^{(y)}=f(x)\implies \text I_x^{y(x)}\text D_x^{y(x)}y(x)=\text I_x^{y(x)}f(x)\implies y+c_1=\text I^y_x f(x)$$ $$\large{\text{Integral Method:}}$$ Now let’s use Cauchy’s Formula for Repeated Integration as the definition for the fractional integration as seen in the bolded link: $$\,_a\text I^n_x g(x)=\frac 1{Γ(n)}\int_a^xg(t)(x-t)^{n-1}dt\implies y+c_1=\,_a \text I^{y(x)}_x f(x)=\frac{1}{Γ(y(x))}\int_a^x f(t)(x-t)^{y(x)-1} dt\implies \boxed{y!+c_0 Γ(y)=\int_{c_1}^x f(t)(x-t)^{y-1} dt}$$ $$\large{\text{Special Case Solution Using Integral method:}}$$ Let’s now solve the $f(x)=1$ case using the conjectured formula: $$y^{(y(x))}(x)=1\implies (y(x))!+c_0 Γ(y(x))=\int_{c_1}^x (x-t)^{y(x)-1} dt\implies y!+c_0 Γ(y)=\frac{(x-c_1)^y}y\implies \boxed{y!(y+c_0)-(x-c_1)^y=0}\implies x=\sqrt[y]{y!(y+c_0)}+c_1$$ Here is a complete interactive graph for this conjectured solution. Another problem is if the fractional derivative has the same definition using $2$ different operator definitions, like this one . $$\large{\text{Special Case Using Induction:}}$$ Another way for $f(x)=1$ is the following setting all constants of integration to be $0$ via quick induction: $$y^{(y(x))}(x)=1\mathop\implies^{n=y(x)} y^{(n)}(x)=1\implies y^{(n-1)}=x,y^{(n-2)}=\frac {x^2}{1\cdot2},y^{(n-3)}=\frac{x^3}{3\cdot2\cdot1}\implies y^{(n-k)}=\frac{x^k}{k!}$$ Now let’s set $n=k$ : $$y^{(n-k)}(x)=y^{(n-n)}(x)=y^{(0)}(x)=y(x)=\frac{x^{y(x)}}{(y(x))!}$$ Let’s try to solve for $x$ : $$yx^{-y}=y!$$ but we cannot use the W-Lambert function here. We can also write the conjectured solution as: $$y^{(y)}=1\mathop\implies^? y\ln(x)-\ln(y)-\ln(y!)=0$$ Using an Inversion theorem will be cumbersome. $$\large{\text{Conclusion:}}$$ Is this a correct way to solve $y^{(y)}=f(x)$ and if not, then how? Please correct me and give me feedback! $$\large{\text{Addendum:}}$$ Also see Solving $y^{(x)}(x)=ax+b$ in closed form for a twin question where a general solution was found. A particular solution for $f(x)=1$ using round $[x]$ is: $$y^{(k)}(x)=1\implies y=\frac{x^k}{k!}\implies y^{(x)}(x)=1\implies y=\frac{x^{[x]}}{[x]!}$$ Following the same logic: $$y^{(y)}(x)=1\mathop\implies^?y=\frac{x^{[y]}}{[y]!}\iff x=(y[y]!)^\frac1{[y]}$$ graph here","['cauchy-integral-formula', 'solution-verification', 'fractional-calculus', 'sequences-and-series', 'fractional-differential-equations']"
4279495,Connection between the many different definitions of the Axiom of Choice,"I'm currently trying to write a paper on the Axiom of Choice. With my research I have found one very simple definition of the Axiom of Choice : ""Let X be a non-empty set of non-empty sets.  There exists a choice function for X."" This seems intuitive to me and I feel like I can understand this from the classic shoes and socks example. But I am also seeing the Axiom of Choice defined as: ""The Cartesian Product of a nonempty family of nonempty sets is nonempty."" This seems less easy to understand at first glance, but reading further I understand how this could make sense. The issue is, I'm struggling to connect these two definitions. In my head I see them as two separate statements, each making sense individually. Is there an easy example to understand the Cartesian Product Definition, like can I relate it back to the sock and shoe example?","['elementary-set-theory', 'axioms', 'axiom-of-choice', 'set-theory']"
4279594,$1-\frac{1}{5\cdot 3^2}-\frac{1}{7\cdot 3^3}+\frac{1}{11\cdot 3^5}+\frac{1}{13\cdot 3^6}--++\cdots.$,"I want to evaluate the series $$1-\frac{1}{5\cdot 3^2}-\frac{1}{7\cdot 3^3}+\frac{1}{11\cdot 3^5}+\frac{1}{13\cdot 3^6}--++\cdots.$$ I can rewrite this as $$1+\sum_{n\geq 1} (-3)^{-3n}\left(\frac{3}{6n-1}+\frac{1}{6n+1}\right)$$ The answer should be $\frac{\ln7}{2}$ . Although I can't see explicitly, since I get a log function, maybe it can be solved by differentiating some test function $f(x)$ and then substituting an appropriate number.",['sequences-and-series']
4279608,Approximate the expected number of jobs in a year.,"A person is employed for one day at a time. When he is out of work, he visits the job
agency in the morning to see if there is work for that day. There is a job for her with
probability $\frac{1}{2}$ . If there is no work, he comes back the next day. When he has a job,
he will be called back to the same job for the next day with probability $\frac{2}{3}$ . When he
is not called back, he goes to the job agency again the next morning to look for a new
job. Approximate the average number of jobs the person works in a year. My approach : Let us denote $p_n$ as the probability that he has a job on day $n$ . By the law of total probability, $P[$ Job on Day $n] = P[$ Job on Day $n \ \cap$ same job as Day $n-1$$] + P[$ Job on Day $n \ \cap$ different or no job as Day $n-1$$]$ . This implies the recursion : $p_n = \frac{2}{3} p_{n-1} + \frac{1}{2}(1-p_{n-1})$ . Solving this recursion and I guess introducing the total number of jobs as a sum of indicators will give my expected number of jobs in a year. But I feel a bit weird about this recursion and I think it is not correct. Can anyone have a different approach to this?","['probability-distributions', 'expected-value', 'law-of-large-numbers', 'probability-theory', 'probability']"
4279620,Perspective geometry proof: projection of an ellipse is always an ellipse,"The purpose of this question is to show that the image of an ellipse in a plane (not necessarily one parallel to the image plane), is also an ellipse assuming perspective projection. In general, a conic is described by the following equation : $$
Ax^2+Bxy+Cy^2+Dx+Ey+F=0
$$ Then the perspective projection matrix $P\in\mathbb{R}^{3\times 3}$ allows the transformation of points from the ellipse plane to the points in the image plane using homogeneous coordinates as follow: $$
\begin{pmatrix}
x&y&1
\end{pmatrix}
\begin{pmatrix}
A&\frac{B}{2}&\frac{D}{2}\\
\frac{B}{2}&C&\frac{E}{2}\\
\frac{D}{2}&\frac{E}{2}&F\\
\end{pmatrix}
\begin{pmatrix}
x\\y\\1
\end{pmatrix}=0$$ My question is how can we deduce from here that the ellipse gets mapped to an ellipse assuming perspective projection? The projection is done as follow: Given an eyepoint $\mathbf{E}$ , we want to perspectively project the ellipse from the plane $(R_{w})$ where $w$ stands for ""world"" as in ""world coordinates frame"" onto another plane $(R_{IM})$ . $$
(R_{w})\mapsto (R_{IM})
$$ That plane has unit length normal $\mathbf{n}$ and origin point $\mathbf{e}$","['projective-geometry', 'conic-sections', 'geometry']"
4279639,What are the elements of this ring?,"Where $\Bbb F_5$ is the finite field of $5$ elements. I'm not sure what this notation is getting at: $$ \mathbb F_5 \left(\begin{bmatrix}1&2\\2&4\end{bmatrix}\right)$$ I assumed it was just $$ \left\{\begin{bmatrix}1&2\\2&4\end{bmatrix},\begin{bmatrix}2&4\\4&3\end{bmatrix},\begin{bmatrix}3&1\\1&2\end{bmatrix},\begin{bmatrix}4&3\\3&1\end{bmatrix},\begin{bmatrix}0&0\\0&0\end{bmatrix}\right\}$$ These matrices are nilpotent so multiplying them only generates the $0$ matrix","['finite-fields', 'matrices', 'notation', 'ring-theory', 'abstract-algebra']"
4279663,"$O$ is intersection of diagonals of the square $ABCD$. If $M$ and $N$ are midpoints of $OB$ and $CD$ respectively ,then $\angle ANM=?$","$O$ is intersection of diagonals of the square $ABCD$ .  If $M$ and $N$ are midpoints of the segments $OB$ and $CD$ respectively, find the value of $\angle ANM$ . Here is my approach: Assuming the length of the square is $a$ . We have $\tan(\angle AND)=2$ and I draw a perpendicular segment from $M$ to $NC$ and calling the intersection point $H$ then $\tan (\angle MNH)=\dfrac{\frac34a}{\frac a4}=3$ ( $MH$ can be found by Thales Theorem in $\triangle BDC$ )
Hence $$\angle ANM=180^{\circ}-(\tan^{-1}2+\tan^{-1}3)=45^{\circ}$$ I'm looking for other approaches to solve this problem if it is possible. Intuitively, If I drag the point $N$ to $D$ and $M$ to $O$ (the angle is clearly $45^{\circ}$ here) then by moving $N$ from $D$ to $C$ and $M$ from $O$ to $B$ with constant speed, I think the angle remain $45^{\circ}$ . But I don't know how to prove it.","['euclidean-geometry', 'geometry']"
4279707,"Find the numbers of ordered arrays $(x_1,...,x_{100})$ such that 2017 divides both their sum and their sum of squares","Find the numbers of ordered array $(x_1,\dots,x_{100})$ that satisfies the following conditions: $1)$ $2017\mid x_1+\dots+x_{100}$ $2)$ $2017\mid x_1^2+x_2^2+\dots+x_{100}^2 $ $3)$ $x_1,\dots,x_{100}\in\{1,2,\dots,2017\}$ Here's all i did : Let $\omega = e^{2 \pi i / 2017}$ . Note that : $$\sum_{0\le a,b\le (2017-1)}  \omega^{a(x_1+x_2+..+x_{100})+b(x_1^2+x_2^2+...+x_{100}^2)} = 2017^2$$ if $ (x_1;x_2;...x_{100})$ satisfies the conditions and : $$\sum_{0\le a,b\le (2017-1)}  \omega^{a(x_1+x_2+..+x_{100})+b(x_1^2+x_2^2+...+x_{100}^2)} = 0$$ if $ (x_1;x_2;...x_{100})$ doesn't satisfy the conditions. Symbol: $X$ is the set of all tuples $(x_1, x_2,,...,x_{100})$ , $Y$ is the set of all tuples $(x_1, x_2,,...,x_{100})$ satisfying the condition. $$\Rightarrow |Y| = \frac{1}{2017^2}  \sum_{0\le a,b\le (2017-1)} \sum_{(x_1;x_2;...;x_{100})\in X} \omega^{a(x_1+x_2+..+x_{100})+b(x_1^2+x_2^2+...+x_{100}^2)} $$ $$\Rightarrow |Y| = \frac{1}{2017^2}  \sum_{0\le a,b\le (2017-1)} \left(\sum_{1 \le x \le 2017} \omega^{ax^2+bx} \right)^{100}$$ Let $$G(a,b) = \sum_{1 \le x \le 2017} \omega^{ax^2+bx} $$ Case $1 : a=0 ;b=0 \Rightarrow G(0,0) = 2017$ Case $2: a=0 ;b >0 \Rightarrow G(0,b) =\sum_{1 \le x \le 2017} \omega^{bx} =0$ Case $3$$:a>0 ;b =0 \Rightarrow G(a,0) =\sum_{1 \le x \le 2017} \omega^{ax^2} \Rightarrow |G(a,0)|^2 = (-1)^{\frac{p-1}{2} }.2017 \Rightarrow |G(a,0)|^{100} = 2017^{50} $ Case $4 : a>0 ;b >0 $ In this case, I'm very stuck, I don't know how to solve it! I look forward to getting help from everyone. Thanks very much!","['prime-numbers', 'number-theory', 'elementary-number-theory', 'combinatorics', 'complex-numbers']"
4279731,Is $\sum_{a=0}^m\sum_{b=0}^n\cos(abx)$ always positive?,"Fix integers $m,n\geq0$ . Do we have the inequality $\displaystyle\sum_{a=0}^m\sum_{b=0}^n\cos(abx)>0$ for all $x\in\mathbb{R}$ ? We can also write this function as \begin{align*}
\sum_{a=0}^m\sum_{b=0}^n\cos(abx)&=m+n+1+\sum_{a=1}^m\sum_{b=1}^n\cos(abx)\\
&=m+n+1+\sum_{a=1}^m\frac{1}{2}\left(\frac{\sin((n+1/2)ax)}{\sin(ax/2)}-1\right)\\
&=\frac{m}{2}+n+1+\frac{1}{2}\sum_{a=1}^mD_n(ax),
\end{align*} where $$D_n(x)=\frac{\sin((n+1/2)x)}{\sin(x/2)}$$ is the Dirichlet kernel (up to a factor of $2\pi$ , depending on your convention). Using this formula, it is easy to check the conjecture for small values of $m$ and $n$ ( desmos link ).","['trigonometric-series', 'inequality', 'real-analysis']"
4279739,Solving infinite ladder of differential equations using generating functions.,"I am interested in solving the following infinite ladder of coupled differential equations. For any integer $k \geq 0$ , we have a real-valued function of a single real variable, $p_k (t)$ , which satisfies $$\dot{p}_k(t) = (k+1)p_{k+1}(t) - kp_k(t)$$ Here, $t \geq 0$ (""time""), and the dot denotes a derivative. The choice of notation $p_k$ is intentional, as these form a set of probabilities. That is, $$\forall t\geq 0, \quad  p_k(t) \geq 0 \, \,\text{and}\,\, \sum_{k=0}^\infty p_k(t)=1$$ (One can show that the differential equations conserve this sum.) To solve this problem, I attempted to introduce a generating function of the form $$g(z, t) \equiv \sum_{k=0}^\infty z^k p_k(t).$$ This function has the property that $g(0, t) = 0$ and $g(1,t) = 1$ . Moreover, by differentiating the equation with respect to $t$ , I found that it satisfies the following first-order, linear partial differential equation. $$ \partial_t g(z,t) + (z-1)\partial_z g(z,t) = 0$$ This seems promising to me, as I seem to have a well defined boundary value problem. Namely, letting $t \in [0,\infty)$ and $z \in [0,1]$ , I set the values of $g$ at the boundaries $z = 0, 1$ , and with the corresponding initial condition $g(z,0)$ . This seems like a well-posed problem. However, I'm having trouble finding the solution. I believe the general solution to the differential equation is $$ g(z,t) = f(e^{-t}(1-z)) $$ where $f$ is any differentiable function of a single variable. But when I try to satisfy the boundary conditions, I hit a snag. The $z = 1$ condition implies $f(0) = 1$ , but the $z = 0$ condition implies $f(e^{-t}) = 0$ . I'm pretty sure this breaks the camel's back: it seems to be saying $f = 0$ for all values! Am I missing something? Are there modifications to this process that can lead me to a solution? Thanks in advance!","['generating-functions', 'problem-solving', 'ordinary-differential-equations', 'partial-differential-equations']"
4279744,How to simplify $\sqrt{\tan^2 x + \cot^2x }$?,"How to simplify : $$\sqrt{\tan ^2 x + \cot  ^2x }$$ the option are : (i) $ \tan  x \cdot \sin  x$ (ii) $\sin  x \cdot \cos  x $ (iii) $ \sec  x \cdot \csc x  $ (iv) $ \frac{1}{\tan  x - \cot   x}$ (v) $ \csc^2 x - \sec ^2 x$ My approach : Since $\tan  x = \frac{\sin  x}{\cos  x}$ and $\cot  x = \frac{\cos  x}{\sin  x}$ , $$
\begin{align}
\sqrt{\tan ^2 x + \cot  ^2x } &= \sqrt{\frac{\sin ^2 x}{\cos ^2 x} + \frac{\cos ^2 x}{\sin ^2 x}} \\
&= \sqrt{\frac{\sin ^4 x + \cos ^4 x}{\sin ^2 x \cdot \cos ^2 x} }\\
&= \sqrt{\frac{(\sin ^2 x + \cos ^2 x)^2 - 2 \sin  x \cdot \cos  x}{\sin ^2 x \cdot \cos ^2 x} }\\
&= \sqrt{\frac{1 - 2\sin  x \cdot \cos  x}{\sin ^2 x \cdot \cos ^2 x} }\\
&= \sqrt{\sec ^2 x \cdot \csc^2 x - 2 \sec  x \cdot \csc x}\\
&= \sqrt{\sec  x \cdot \csc x ( \sec  x \cdot \csc x - 2)}\\
\end{align}
$$ from this point, I don't have any idea how should I approach this problem to get another form of this equation available on the option. Another approach I have in  mind is from changing $\cot   x = \frac{1}{\tan  x}$ $$
\begin{align}
\sqrt{\tan ^2 x + \cot  ^2x } &= \sqrt{\tan ^2 x + \frac{1}{\tan ^2 x}} \\
&= \sqrt{\frac{\tan ^4 x + 1}{\tan ^2 x} }\\
\end{align}
$$ From this point, I don't have any idea. What am I missing or what approach should you suggest to change the form to the option available on the option?",['trigonometry']
4279781,Must the radius of an open ball be real? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Let $\mathbb{Q}$ be the metric space with $d(p,q) = |p-q|$ . Is the radius of an open ball/a neighborhood of a point $p$ real, or in this case is it rational only? Thank you!","['analysis', 'real-analysis']"
4279821,Is the set of all invertible linear operators dense in the set of all linear operators?,"Exercise 2 after $\S$ 91 from Paul R. Halmos's ""Finite-Dimensional Vector Spaces"" (second edition) invites to prove or disprove the following assertion. For every (bounded) linear transformation $A$ (on an inner product space) there exists a sequence $(A_n)$ of invertible linear transformations such that $A_n \rightarrow A$ . The inner product space, say $\mathcal V$ , of the assertion is not specified to be over the complex (or real) field, and is not said to be finite-dimensional or complete either. Also, for reference, $\S 91$ (from the book) identifies "" $A_n \rightarrow A$ "" in inner product spaces by $\Vert A_n - A \Vert \rightarrow 0$ as $n \rightarrow \infty$ . Further, $\S \ 87$ has the following definition for the norm $\Vert \cdot \Vert$ of a linear operator: $\Vert A \Vert = \inf \ \big\{K: \Vert Ax \Vert \leq K \Vert x \Vert \text{ for all vectors } x\big\}.$ I am able to see why the assertion holds if $\mathcal V$ is finite-dimensional; see my ""constructive"" proof below. I am unable to imagine what happens in the general case however, and would appreciate a proof or a counterexample in infinite dimensional spaces. Would also appreciate an advice if my proof is found to be inaccurate! Thanks. Proof (in finite dimensions): Let $\mathcal V$ be a $k$ -dimensional inner product space ( $0 \leq k < \infty$ ), and let $A$ be any linear operator on $\mathcal V$ . If $\mathcal N(A)$ is the null-space of $A$ , then $\mathcal N^\perp(A) \oplus \mathcal N(A) = \mathcal V$ . Also, if the rank of $A$ is $m \ (\leq k)$ , and if $\mathcal R(A)$ is the range of $A$ , then $m = $ dim $\mathcal R(A)=$ dim $\mathcal N^\perp(A) = k-$ dim $\mathcal N = k-$ dim $\mathcal R^\perp(A)$ . It is also true that $\mathcal R^\perp(A) \oplus \mathcal R(A) = \mathcal V$ . We now begin constructing a sequence $(A_n)$ of invertible linear operators such that $A_n \rightarrow A$ . Let $B$ be the restriction of $A$ to the $m$ -dimensional $\mathcal N^\perp(A)$ . It is clear that $B$ maps $m$ -dimensional $\mathcal N^\perp(A)$ onto $m$ -dimensional $\mathcal R(A)$ , and is invertible. Next, write $C_n x_i = \frac{1}{n}y_i$ for $n = 1, 2, \cdots$ and for $i = 1, \cdots, k-m$ , where $\{x_1, \cdots, x_{k-m}\}$ and $\{y_1, \cdots, y_{k-m}\}$ are any bases in the $(k-m)$ -dimensional subspaces $\mathcal N(A)$ and $\mathcal R^\perp (A)$ respectively. (It is clear that $C_n$ is a linear map with rank $k-m$ , and is invertible.) Next, if, for every $z = (z_1+z_2)$ in $\mathcal V$ , we write $A_n z = B z_1 + C_n z_2$ for $n = 1, 2, \cdots$ , whenever $z_1$ and $z_2$ are in $\mathcal N^\perp(A)$ and $\mathcal N(A)$ respectively, then we find that $A_n$ is a linear mapping of $\mathcal V$ onto $\mathcal V$ , and is invertible. Finally, because $A_n z - Az = (Az + C_nz_2) - Az = C_n z_2$ , which implies that $(A_n - A)z \rightarrow 0$ , it follows that $A_n \rightarrow A$ .","['inner-products', 'operator-theory', 'vector-spaces', 'functional-analysis', 'linear-transformations']"
4279919,Minimizing the sum of cosines of non-obtuse angles formed by $n\geq4$ concurrent lines in $3$D space,"Suppose I have two lines in $3$ D space passing through the origin. The smallest angle formed between them would be between $0$ and $\pi/2$ . Minimizing the cosine of this angle we'll get $\cos {(\pi/2)}=0$ . For $3$ lines there will be in total $3$ angles between them. Let's again suppose these angles are between $0$ and $\pi/2$ . The minimum of the sum of the cosines of these angles will be $0$ ; when each vector is $pi/2$ away from the other two. But for $4$ lines and beyond, every angle cannot be made $\pi/2$ in $3$ D space. I need to find the minimum of the sum of cosines for $n$ number of lines. For example, let's take the case of $4$ lines. There are $6$ angles formed between them. I tried to minimize the sum of cosines numerically and got the value as $1$ . This is the case when $3$ lines are perpendicular, and the fourth vector is along  one of those $3$ . For $5$ lines, the minimum is $2$ ; lines double up in two of the axis and one vector perpendicular to them. For $6$ lines, the minimum is $3$ ; two lines along each axis. From numerical results, it seems that the minimum will be when the lines are along the $3$ D axes, but I don't yet have a proof for this. Can proof be made through induction? Kindly help out in any way possible. EDIT (27/10/21) Take the case of $4$ lines. We can imagine them as vectors pointing from the origin to the surface of a unit sphere, say $v_1,v_2,v_3,v_4$ . Moreover, since we are only concerned with the smallest angle between each and every pair, we can take all the vectors to lie in the same octant of the sphere.
Now, we want to minimize the sum $$\cos(\theta_{12})+\cos(\theta_{13})+\cos(\theta_{14})+\cos(\theta_{23})+\cos(\theta_{24})+\cos(\theta_{34})$$ $$=(\cos(\theta_{12})+\cos(\theta_{13})+\cos(\theta_{14}))+\cos(\theta_{23})+\cos(\theta_{24})+\cos(\theta_{34})$$ where the terms in the bracket is equal to $$v_1\cdot(v_2+v_3+v_4)$$ Since $v_2,v_3,v_4$ lies in an octant, their sum would also lie in that octant. The previous dot product is minimized when $v_1$ is along one of the axes. We can repeat the same argument for every vector and see that for minimizing the sum of cosines, every vector should be along the axes. I believe the above argument holds for any $n$ . Once we know the lines are along the axes, I guess it's easy enough to derive a form for the minimal value. Is there anything wrong with my argument? How to formalize this proof if correct?","['optimization', 'vectors', 'vector-spaces', 'geometry']"
4279953,Prove that the equation $p(p(x)) = q(q(x))$ has no real solutions. [duplicate],"This question already has an answer here : Prove the following polynomial equation has no answers (1 answer) Closed 2 years ago . $p(x)$ and $q(x)$ are polynomials which satisfy the identity $p(q(x)) = q(p(x))$ for all real $x$ . If the equation $p(x) = q(x)$ has no real solutions, prove that the equation $p(p(x)) = q(q(x))$ has no real solutions. What I Tried :- I just assumed $p(x) = a_nx^n + \dots + a_1x + a_0$ and $q(x) = b_nx^n + \dots + b_1x + b_0$ . It was also given that $p(x) = q(x)$ has no real solutions, so in the context of real numbers, I can claim $p(x) \neq q(x)$ for all $x$ . But I am not sure how that is going to help, and I have not used this information yet. After this, we have $p(b_nx^n + \dots + b_1x + b_0) = q(a_nx^n + \dots + a_1x + a_0)$ . But now, I am stuck. Expanding more is going to make it complicated. I am also thinking of showing this by contradiction somehow, that I am assuming first there exists a real root to $p(p(x)) = q(q(x))$ , but I am not sure how to do it. Can anyone help me? Thank You.","['irreducible-polynomials', 'algebra-precalculus', 'polynomials']"
4279981,Why is $\lim\limits_{x \to 0^+}(1+ x)^\frac1x$ is $e$ and not $\infty$?,"I am a grade 12th student. I am having doubts about limits as follows:- If $a>1$ , then $\lim\limits_{x \to \infty}a^x = \infty$ . The above is because a number greater than $1$ when multiplied with itself, increases. If we do it a large number of times, the number will get close to $\infty$ . Therefore its limiting value becomes $\infty$ . Applying same logic $\lim\limits_{x \to 0^+}(1+ x)^\frac1x = \infty$ . But $\lim\limits_{x \to 0^+}(1+ x)^\frac1x = e$ . Can anyone suggest to me where I am wrong? Edit I had asked my mentor about this. He said the following. In case of $\lim\limits_{x \to \infty}a^x$ , base and exponent are independent of each other whereas in case of $\lim\limits_{x \to 0^+}(1+ x)^\frac1x$ , the base and exponent are dependent on each other. Because of this dependence, as soon as we create the base, i.e. $(1+x)$ , $x$ becomes fixed and as a result the exponent becomes fixed and can't approach infinity. However, for $a^x$ , $x$ can become as small as we want and therefore tends to infinity. Can anyone please explain me why are we trying to fix the base in case of $\lim\limits_{x \to 0^+}(1+ x)^\frac1x$ ? Is there any such rule that we have to first create a base and then apply the limits? Am I having a wrong interpretation of limits? Can anyone please interpret his statements in a simplified language. Any help is appreciated. Thank you.","['limits', 'calculus']"
4280002,Euler-Cauchy equation boundary conditions problem,"ODE: $$xy''+2y'+ax=0$$ BCs: $$y(0)=\alpha$$ $$y'(L)=0$$ ODE solution: $$y(x)=-\frac{ax^2}{6}+\frac{c_1}{x}+c_2$$ $$\lim_{x\to0}y(x)=\infty\Rightarrow c_1=0$$ $$y'(x)=-\frac{ax}{3}$$ With $c_2$ dropping out, it cannot be determined from the second BC? And yet we can determine $c_2$ from the 1st BC: $$y(0)=c_2=\alpha$$ So why does the second BC seem redundant?","['boundary-value-problem', 'ordinary-differential-equations']"
4280026,"Finding all the surjective functions $ f: \mathbb{R} \to \mathbb{R}$ which satisfy $f(x+f(x)+xy)=2f(x)+xf(y) \ \forall x, y \in \mathbb{R}$","Find the function(s) $ f: \mathbb{R} \to \mathbb{R}$ which satisfies these two conditions: $f(x+f(x)+xy)=2f(x)+xf(y) \ \forall x, y \in \mathbb{R}$ function $f$ is surjective ( $\forall z \in \mathbb R, \ \exists x \in \mathbb R, \ f(x)=z$ ). My attempt: \begin{align}
&\text{let }P(x, y): \ f(x+f(x)+xy)=2f(x)+xf(y) \\
&P(0, y): \ f(f(0))=2f(0). \\
&P(x, 0): \ f(x+f(x))=2f(x)+xf(0). \\
\ \\
&\text{let } f(a)=f(b). \ \\
&P(a, -1): f(f(a))=2f(a)+af(-1). \\
&P(b, -1): f(f(b))=2f(b)+bf(-1). \\
& \Rightarrow a=b \text{ if } f(-1) \neq 0. \\
\ \\
&\text{Edit: }\\
& \text{if } f(-1)=0: \\
&P(x, -1): \ f(f(x))=2f(x). \\
& \text{Since the function } f \text{ is surjective, } f(x)=2x \text{ for } \forall x \in \mathbb{R}. \\
& x=-1; \ f(-1)=-2, \text{ which is contradiction.} \\
\ \\
&\therefore f(a)=f(b) \Rightarrow a=b.
\end{align} Well, I expect this function to be $f(x)=x. $ Can you show the full process of finding the function(s)?","['functional-equations', 'functions']"
4280142,Solving these coupled differential equations.,"I have a set of coupled differential equations of the form $$ \omega y_1 \frac{\partial q_1}{\partial y_1}+\omega y_2 \frac{\partial q_1}{\partial y_2}+\omega y_3 \frac{\partial q_1}{\partial y_3}+g\frac{\partial q_1}{\partial y_1}+t q_{3}e^{-g/\omega (y_{3}-y_1)}-t q_{2}e^{-g/\omega (y_{2}-y_3)}=(E+g^2/\omega)q_1$$ $$ \omega y_1 \frac{\partial q_2}{\partial y_1}+\omega y_2 \frac{\partial q_2}{\partial y_2}+\omega y_3 \frac{\partial q_2}{\partial y_3}+g\frac{\partial q_2}{\partial y_2}+t q_{1}e^{-g/\omega (y_{1}-y_2)}-t q_{3}e^{-g/\omega (y_{3}-y_1)}=(E+g^2/\omega)q_2$$ $$ \omega y_1 \frac{\partial q_3}{\partial y_1}+\omega y_2 \frac{\partial q_3}{\partial y_2}+\omega y_3 \frac{\partial q_3}{\partial y_3}+g\frac{\partial q_3}{\partial y_3}+t q_{2}e^{-g/\omega (y_{2}-y_3)}-t q_{1}e^{-g/\omega (y_{1}-y_2)}=(E+g^2/\omega)q_3$$ How do I solve these multivariate coupled differential equations? When I add all of these equations together, I obtain $$\omega y_1 \frac{\partial}{\partial y_1}(q_1+q_2+q_3)+\omega y_2 \frac{\partial}{\partial y_2}(q_1+q_2+q_3)+\omega y_3 \frac{\partial}{\partial y_3}(q_1+q_2+q_3)+g \nabla\cdot q=(E+g^2/\omega)(q_1+q_2+q_3) $$ I am very confused about how to handle the $\nabla\cdot q=\frac{\partial q_1}{\partial y_1}+\frac{\partial q_2}{\partial y_2}+\frac{\partial q_3}{\partial y_3}$ term.","['multivariable-calculus', 'partial-differential-equations']"
4280175,Getting the wrong value from $\lim_\limits{x\to-\infty}x-\sqrt{x^2+7x}$,"Consider the following limit $$
\lim_{x\to-\infty}x-\sqrt{x^2+7x}
$$ Going through some algebra leads to $$\begin{align}
\lim_{x\to-\infty}x-\sqrt{x^2+7x}&=\lim_{x\to-\infty}\frac{(x-\sqrt{x^2+7x})(x+\sqrt{x^2+7x})}{(x+\sqrt{x^2+7x})}\\
&=\lim_{x\to-\infty}\frac{-7x}{x+\sqrt{x^2+7x}}\\
&=\lim_{x\to-\infty}\frac{-7}{1+\sqrt{1+7/x}}=-\frac72
\end{align}$$ Using WolframAlpha 's step-by-step solution, however, gives this limit to be $-\infty$ . Here's what it's doing $$\begin{align}
\lim_{x\to-\infty}x-\sqrt{x^2+7x}&=\lim_{x\to-\infty}x-\lim_{x\to-\infty}\sqrt{x^2+7x}
\end{align}$$ where, by the power rule, $$\begin{align}
\lim_{x\to-\infty}\sqrt{x^2+7x}&=\sqrt{\lim_{x\to-\infty}(x^2+7x)}\\
&=\sqrt{\lim_{x\to-\infty}x^2}\\
&=\sqrt{\left(\lim_{x\to-\infty}x \right)^2}=\infty
\end{align}$$ and so $$
\lim_{x\to-\infty}x-\lim_{x\to-\infty}\sqrt{x^2+7x}=-\infty-\infty=-\infty
$$ What is wrong in this solution? I wondered if it was the power rule failing for undefined values of the square root, but not sure what to argue.","['limits', 'algebra-precalculus', 'wolfram-alpha', 'limits-without-lhopital']"
4280195,Methods to Analytically Solve a Nonlinear PDE,"I'm looking for suggestions to solve: $$ u_t = u_{xxxx}-3u(u_x)^2-\frac{3}{2}u^2u_{xx}+\frac{1}{2}u_{xx}+F $$ where $F$ is currently an unknown function of unknown type (might be linear, exponential, etc.). Ignoring that ambiguity (pretend it vanishes), I am unsure of any method to solve such a PDE. I attempted separation of variables but that didn't help because of the nonlinearities. I then tried seeking a steady-state solution however that didn't lead to anything fruitful. After that, I tried using the $1$ D and $2$ D Fourier Transforms in space and space and time respectively but again that didn't really help. In this case, I got terms that involved self-convolutions of either $u$ or $u_x$ and this made the equation even harder to solve. One last thought I had was to maybe use the Cole-Hopf Transform: i.e. introduce some new variable $w=\phi(u)$ . In doing this, I started calculating partials of $w$ that would appear in my PDE and the expressions I got seemed even more complicated. This kind of leads me to believe that this won't be a fruitful avenue either, however, I have never really tried using this method so I am wondering if I'm maybe doing something wrong. Here I found: $$ w_t = \phi'(u)u_t $$ $$ w_x = \phi'(u)u_x $$ $$ w_{xx} = \phi''(u)u_x^2+\phi'(u)u_{xx} $$ $$ w_{xxx} = \phi'''(u)u_x^3 + 3\phi''(u)u_xu_{xx}+\phi'(u)u_{xxx} $$ $$ w_{xxxx} = \phi^{(4)}(u)u_x^4 + 6\phi'''(u)u_x^2u_{xx}+3\phi''(u)u_{xx}^2+4\phi''(u)u_xu_{xxx}+\phi'(u)u_{xxxx} $$ Now I just don't really see how to use these equations to generate a simplified version of my original PDE or even how to definitively say that this won't help me in reducing the PDE. Any help or advice would be appreciated.","['multivariable-calculus', 'nonlinear-dynamics', 'transformation', 'partial-differential-equations']"
4280212,A product over the characters of a finite abelian group,"I'm comming up with the following problem in my algebraic number theory course: Problem : Let $G$ be an abelian group (the operation is denoted as multiplication) of order $fg$ . Let $a \in G$ be such that the order of $a$ is $f$ . Prove that $$
\prod_{\chi \in \widehat{G}}(1-\chi(a)T) = (1-T^f)^g,
$$ where $\widehat{G}$ is the group consists of all multiplicative characters $\chi: G \rightarrow \mathbb{C}^{\times}$ . We know that $G$ is canonically isomorphic to $\widehat{G}$ . Question : How to prove this? Attempts : I'm trying to expand both sides and compare the coefficient. The right hand side is direct by binomial theorem: $(1-T^f)^g = \sum_{k=0}^{g}\binom{g}{k}(-T)^{kf}.$ On the left hand side, the coefficient of $(-T)^n$ is $$
C_n := \sum_{1 \leq i_1 < i_2 < \cdots < i_n \leq fg} \chi_{i_1}(a) \chi_{i_2}(a) \cdots \chi_{i_n}(a).
$$ Comparing the coefficients, I'm trying to prove: Claim : $C_n = \binom{g}{m}$ when $n=fm$ for some $m \in \mathbb{Z}_{\geq 0}$ , and $C_n = 0$ when $f \nmid n$ . A special case : when $n=1$ and $a \neq 1_G$ , $C_1 = 0$ by the orthogonality of characters: $\sum_{\chi \in \widehat{G}} \chi(g) = 0$ if $g \neq 1_G$ . So to prove the claim, I also tried to imitate the proof of this orthogonality relation. Proof : Let $g \in G-\{1_G\}$ , then consider the group $G^{\prime}$ generated by $g$ . Then $|G/G^{\prime}| < n$ . Consider $H = \{\chi \in \widehat{G}: \chi(g)=1\}$ , then for any $\chi \in H$ , $\ker \chi \supset G^{\prime}$ , hence $\chi$ induces $\widetilde{\chi}: G/G^{\prime} \rightarrow \mathbb{C}^{\times}$ . Moreover, different characters in $H$ induces different characters on $G/G^{\prime}$ . Hence $$
|H| \leq |(G/G^{\prime})^{\wedge}| = |G/G^{\prime}| < n = |G| = |\widehat{G}|.
$$ Hence $H \subsetneq \widehat{G}$ and therefore, there exists $\psi \in \widehat{G}$ such that $\psi(g) \neq 1$ . Therefore $$
\sum_{\chi \in \widehat{G}} \chi(g) = \sum_{\chi \in \widehat{G}} \psi \chi(g) = \sum_{\chi \in \widehat{G}} \psi(g) \chi(g) = \psi(g) \sum_{\chi \in \widehat{G}} \chi(g).
$$ As $\psi(g) \neq 1$ , the only chance is that $\sum_{\chi \in \widehat{G}} \chi(g)=0$ . Inspired by this, I'm trying to consider the order $f$ subgroup generated by $a$ in $G$ . But I got stuck here and not knowing how to carry on. Further question : since the notations here, especially $f$ and $g$ here is also used in the decomposition of primes in number fields (where $f$ is the inertia degree and $g$ is the number of distinct prime ideals in the decompostion of $\mathfrak{p} \subset \mathcal{O}_K$ in $\mathcal{O}_L$ .) So just a wild guess, does this have any background on some more deeper results or useful tricks, or some relations to decomposition of primes? Where the result maybe used in number theory? Sorry for such a long post and thank you all for commenting and answering! :)","['algebraic-number-theory', 'group-theory', 'abelian-groups', 'characters']"
4280219,"Given the final points of all teams in a sports league, counting the number of possibilities that would lead to it","Does this problem have a name: ""Given the final points table of a sports league having $n$ teams, enumerate the possible results leading up to it, or at least provide the number of possible results"". So suppose there were 3 teams X, Y, Z which played each other twice (once at home and once away). A win meant 2 points, a draw/tie meant 1 and a loss meant 0 points. At the end the points table was as follows: X had 3 points Y had 3 points Z had 6 points Given this info what are all the possible results? One possibility is that X defeated Y at home but drew away. Z defeated X both times, & Y and Z drew both their games. In this way, one can enumerate all the possibilities. How many possibilities will be there in total? I wish to know whether this problem can be solved easily, or not (for $n$ teams where winning yields $p$ points, drawing $q$ points and losing $r$ points). In the latter case, is it equivalent to some other problem that has a name?","['combinatorics', 'combinatorial-proofs']"
4280295,Combined real + p-adic numbers?,"So most p-adic notes inevitably beat us with a club with Ostrowski's theorem on absolute values. However what if we forsake absolute values, and instead use a digit system where the value $v(x)$ of a number $x$ equals a tuple $(a,b)$ . Here $a$ is the smallest negative exponent of some prime $p$ in the representation of $x$ , while $b$ is the smallest positive exponent. For instance for $p=2$ , $v(9/4)$ = $v(1/4 + 2)$ = $(2,1)$ . However we allow infinite expansions too: We define a sequence as 'left-convergent' if for $\lim v_2(x_n - x_{n-1})$ the first tuple value approaches infinity, and likewise for 'right-convergent'. In other words, high positive or negative powers of a prime both converge to different zeros. And therefore we can write unconditionally convergent expressions of the form $\sum_{-\infty}^{\infty} p^n a_n$ where $a_n$ is between $0$ and $p-1$ . So we basically obtain real (base $p$ ) plus p-adic 'combined' numbers. The number zero can e.g. be represented as either: $\cdots 0.0 \cdots$ or $\cdots (p-1)(p-1)(p-1).(p-1)(p-1)(p-1) \cdots$ Since the latter is divisible by all factors of $p-1$ , for $p>2$ we aren't dealing with a field (at least by the usual definition) as there are zero divisors. Though I don't know what happens with $p=2$ ? Something like $\frac{1}{x}$ for $(x,p)=1$ will have multiple solutions. With the 'new' solutions corresponding to linear combinations of the real and p-adic solution. E.g. $\frac{1}{3}$ for $p=2$ has representations: $0.010101 \cdots$ (real) $\cdots 0101011;$ (2-adic) $\cdots 010101.1010101 \cdots$ (sum of both divided by two) This sort of feels like 'field extension' since we are adding new solutions to an equation. In these numbers, series like $\displaystyle \sum_{n=0} p^{(-1)^n n}$ converge to finite values. Also power series converge absurdly: $\sum_{n=-\infty}^{\infty} x^n = 0$ for all $x$ with a power of $p$ in the numerator or denominator. How about products? (responding to Julian Rosen). In general we can use Cauchy products if at least one of the numbers has a finite representation. However multiplying an infinite p-adic and infinite real number together causes convergence issues.
If we denote $\frac{1}{x}_p$ and $\frac{1}{x}$ for the p-adic and real representatives of a fraction. Then $\frac{1}{x}_p \cdot \frac{1}{x} = \frac{1}{x}_p \cdot \frac{x}{x^2} = \frac{1}{x^2} = \frac{x}{x^2}_p \cdot \frac{1}{x} =  \frac{1}{x^2}_p$ . Hence by implication there can't be a 'single' representative for this product. My questions are: Does this approach make sense/does it have a name? Does it have any benefits or unique advantages? E.g. tying p-adic results to real ones? For instance 'hypothetically' if we can demonstrate that the 'combined' expression (real + p-adic) is irrational and the real solution is rational, then the remaining term must be irrational.","['p-adic-number-theory', 'abstract-algebra', 'absolute-value', 'sequences-and-series']"
4280330,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable.,Let $X$ be an infinite dimensional Banach space. Prove that every Hamel basis of $X$ is uncountable. Can anyone help how can I solve the above problem?,"['banach-spaces', 'normed-spaces', 'functional-analysis', 'hamel-basis', 'baire-category']"
4280368,Looking for a $\sigma$-algebra generated by the set of sets containing $1$,"Let's consider the following set $$\mathcal{S} = \{S \subset \mathbb{R}: 1 \in S \}.$$ I would like to find $\sigma(\mathcal{S})$ . I think that $\sigma(\mathcal{S}) = \mathcal{P(\mathbb{R})}$ , where $\mathcal{P}$ denotes the power set. I have even proved it. The first inclusion, that is $\sigma(\mathcal{S}) \subset \mathcal{P(\mathbb{R})}$ , is trivial. The second one is a bit more tricky. Let's grab $P \in \mathcal{P(\mathbb{R})}$ . There are two options: $1 \in P$ , then $A \in \mathcal{S}$ and that implies $A \in \sigma(\mathcal{S})$ , $1 \notin P$ , then $1 \in P^C$ thus $P^C \in \sigma(\mathcal{S})$ which implies that $P \in \sigma(\mathcal{S})$ . These options implies that $\mathcal{P(\mathbb{R})} \subset \sigma(\mathcal{S})$ . Is my reasoning correct?",['measure-theory']
4280398,Theorems that deals only with critical metrics of Einstein-Hilbert like functional,"It is common that researchers study the critical metrics of Einstein-Hilbert like functional $${\cal R^2}=\int_M|{\rm Ric}_g|^2dV_g,\quad \text{or}\quad {\cal r^2}=\int_M{\rm scal}_g^2dV_g$$ and deduce some results. For example Michael Anderson in his paper Anderson, Michael T. , Extrema of curvature functionals on the space of metrics on 3-manifolds. II , Calc. Var. Partial Differ. Equ. 12, No. 1, 1-58 (2001). ZBL1018.53020 . has proved the following: Theorem 0.1. Let $(N, g)$ be a complete $\cal R^2$ critical metric with non-negative
scalar curvature. Then $(N,g)$ is flat. I haven't read the full paper. My question is that how this theorem can be useful while it deals only with critical metrics? I mean how can one use this and similar results to deduce nice results that are true without critical metrics assumption? I know that Einstein metrics are critical points of Einstein-Hilbert functional $r$ so I think one can replace critical metric with Einstein manifolds in similar theorems and the result will remain true. Is this the only possible application of these set of theorems that deals with critical metrics?","['differential-topology', 'riemannian-geometry', 'differential-geometry']"
4280500,An almost sure version of Doob-Dynkin lemma with null sets,"Let $(\Omega,\mathcal{F})$ be a measurable space, $X:(\Omega,\mathcal{F})\to (\Omega_2,\mathcal{F}_2)$ be a measurable function,
and $f:(\Omega,\mathcal{F})\to ( \mathbb{R},\mathcal{B}(\mathbb{R}))$ be a function.
Then the Doob-Dynkin lemma says that $f$ is $\sigma(X)$ measurable if and only if there exists a measurable $g:(\Omega_2,\mathcal{F}_2)\to ( \mathbb{R},\mathcal{B}(\mathbb{R}))$ such that $f=g\circ X$ . My question is that whether there is an almost sure version for the lemma? More precisely, let $\mathbb{P}$ be a probability measure on $(\Omega,\mathcal{F})$ , $\mathcal{N}$ be the $\sigma$ -algebra on $\Omega$ generated by $\mathbb{P}$ -null set, and $f$ is $\sigma(X)\vee \mathcal{N}$ measurable, where $\sigma(X)\vee \mathcal{N}$ denotes the $\sigma$ -algebra generated by $\sigma(X)$ and $\mathcal{N}$ . Then can we ensure there exists a measurable $g:(\Omega_2,\mathcal{F}_2)\to ( \mathbb{R},\mathcal{B}(\mathbb{R}))$ such that $f=g\circ X$ $\mathbb{P}$ -a.s.?","['reference-request', 'measure-theory', 'probability-theory', 'real-analysis']"
4280517,Simplest smooth ($C^{\infty}$) approximation to Dirac's $\delta$ with bounded support.,"I'm looking for a function $f(x)$ that approximates Dirac's $\delta$ distribution that has the following properties: $f\in C^\infty(\mathbf R)$ , it has finite derivatives of all orders. $f$ has a simple definition (in increasing complexity: polynomial $\rightarrow$ rational $\rightarrow$ exponentials $\rightarrow$ trigonometric/hyperbollic ). Ideally it has a simple expression, but if necessary it can also be defined piecewise. $f$ has bounded support, its only nonzero values occur within a closed interval centered at the origin. $f$ approximates the $\delta$ distribution in the sense that it has total integral of $1$ , it is a symmetric function, and it has a parameter $\lambda$ that can be adjusted such that in the limit of either $\lambda\to0$ or $\lambda\to\infty$ you have $f(0)\to\infty$ and $f(x\neq0)\to0$ . Does such a function exist? If so, what's the simplest one you know of? I need this function for a program I'm writing, (3) and (4) are very necessary for me. If the restrictions are too harsh it's OK as long as $f$ is at least continuously differentiable or even if $f$ doesn't satisfy (2) in the sense that it's defined in terms of more complicated transcendental functions I could work around it. Example of functions that approximate the $\delta$ distribution but don't satisfy all requirements are: $\frac{1}{2\lambda}[1-\tanh^2(x/\lambda)]$ is $C^\infty$ , simple, but doesn't have bounded support. The function $\frac{1}{\lambda}\zeta(x/\lambda)$ in this other MSE post has compact-, and therefore bounded-, support but it doesn't have a simple expression as it is defined in terms of a transcendental function. The piecewise defined function $f(x)=\{0, ~\text{if}~ |x|>\frac{\pi\lambda}{2}; \quad\frac{2}{\pi\lambda}\cos^2(x/\lambda), ~\text{if}~ |x|\leq\frac{\pi\lambda}{2}$ . It does have bounded support but its expression is not so simple because it's defined piecewise and it also violates (1), since it's second derivative is discontinuous. Thanks.","['functions', 'dirac-delta', 'approximation', 'real-analysis']"
4280545,"If $\sqrt{a}\sqrt{b}=\sqrt{ab}$ only holds for positive real $a$ & $b$, then why can we say $\sqrt{-a}=\sqrt{-1\cdot a}=\sqrt{-1}\sqrt{a}=i\sqrt{a}$?",I am a little bit bummed that I have this question as I'm sure it has been asked before (I couldn't find the answer) but... If $\sqrt{a}\sqrt{b} = \sqrt{ab}$ is only true for positive reals $a$ and $b$ . Then what allows us to say the following? $$\sqrt{-a} = \sqrt{-1\cdot a} = \sqrt{-1}\sqrt{a} = i\sqrt{a}$$ I don't know what allows the second equal sign. Is this just convention?,"['complex-analysis', 'algebra-precalculus', 'radical-equations', 'radicals']"
4280547,Does the integral of an operator be compact implies that the semigroup is compact?,"Let $M$ be a compact metric space and $$\mathcal C^0(M) :=\{f:M\to \mathbb R;\ f\ \text{is continuous}\}.$$ Consider the semigroup of  contractive bounded linear positive operators $$\{P^t : (\mathcal C^0(M),\|\cdot \|_{\infty}) \to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\}_{t\geq 0}, $$ i.e. $$\|P^t\|\leq 1,\ P^0 = \mathrm{Id}\ \ \text{and} \ P^{s+t} = P^t \circ P^s,\ \forall \ s,t\geq 0,$$ and $$ f\in \mathcal C^0(M),\ f\geq 0 \Rightarrow \ P^t f \geq 0,\ \forall\ t\geq 0.$$ Assume that there exist $K,\alpha >0$ , such that $$ \|P^t\|\leq Ke^{-\alpha t}.$$ and \begin{align}
G: (\mathcal C^0(M,\|\cdot\|_{\infty}) &\to \left(\mathcal C^0(M),\|\cdot\|_{\infty}\right)\\
f&\mapsto \left(x\mapsto \int_0^\infty \left(P^t f\right)(x)\ \mathrm{d}t\right)
\end{align} is a compact operator. Question: Do $G$ being a compact and $P^t$ presenting exponential decay imply that there exist a $t_0>0$ such that $P^{t_0}$ is a compact operator? In my head, it is harder to $G$ be compact than to $P^t$ be compact, but I was not able to prove it. Can anyone help me?","['linear-algebra', 'functional-analysis', 'analysis', 'compact-operators']"
4280553,What do we know about the intersection of co-countable and Euclidean topologies?,"The Euclidean topology on $\Bbb R$ is well-understood. It is the one generated by the open intervals (or even just the open intervals with rational end-points). To some extent, we also understand the co-countable topology, which is generated by the sets whose complement is countable. Easily we can see that the Euclidean topology is neither a subset nor a superset of the cocountable topology: $(0,1)$ is open in the Euclidean topology, but its complement is uncountable. $\Bbb Q$ is countable, but it is not closed in the Euclidean topology. So we can consider $\tau$ to be the intersection of these two topologies. Namely, $A\subseteq\Bbb R$ is a member of $\tau$ (i.e., open) if and only if it is empty or it is both co-countable and a countable union of intervals. So, for example, neither $(0,1)$ nor the irrational numbers are open in this topology, as remarked above. On the other hand, consider $A=\left\{\frac1{2^n}\mathrel{}\middle|\mathrel{} n\in\Bbb N\right\}\cup\{0\}$ , then $\Bbb R\setminus A$ is open. Questions. Is there a nice way to describe $\tau$ ? Does it have a name? Are there any nice (non-trivial) properties of this topology?",['general-topology']
4280618,Write the PDE $xu_{xx}+u_{yy}=x^2$ in canonical form,"I am having trouble getting the transformation right. Let me show my work: We know $b^2-4ac=-4x$ , so if $x>0$ the equation is eliptic. Let's make our substitution: $\epsilon= -bx+2ay=2xy $ $n= \sqrt{4ac-b^2}x=\sqrt{4x}x=2x^{\frac{3}{2}}$ Notice that $\epsilon_{x}=2y, \epsilon_{y}=2x, n_{x}=3x^{\frac{1}{2}}, n_{y}=0$ .
Thus, we have: $u_{x}=u_{\epsilon}\epsilon_{x}+u_{n}n_{x}=2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n}$ $u_{xx}= D_{\epsilon}[2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] +D_{n}[ 2yu_{\epsilon}+3x^{\frac{1}{2}}u_{n} ] = (2yu_{\epsilon\epsilon}+3x^{\frac{1}{2}}u_{n\epsilon})(2y) + (2yu_{\epsilon n}+3x^{\frac{1}{2}}u_{nn})(3x^\frac{1}{2})=4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn}$ $u_{y}=u_{\epsilon}\epsilon_{y}+u_{n}n_{y}=2xu_{\epsilon}$ $u_{yy}= D_{\epsilon}(2x u_{\epsilon})(2x)=4x^2u_{\epsilon\epsilon}$ When I substitute all of these expresions into our PDE I get: $xu_{xx}+u_{yy}=x^2 = x(4y^2u_{\epsilon\epsilon}+6x^{\frac{1}{2}} yu_{\epsilon n} + 9x u_{nn})+ 
4x^2u_{\epsilon\epsilon}=x^2$ Unfortunately, this is not a proper canonical form. I appreciate it if you can tell me what I'm doing wrong, I need to complete this exercise as quickly as possible, thank you for help!","['multivariable-calculus', 'partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
4280653,"Strictly monotonic function on an ordered set implies injectivity, when is the converse true?","I'm looking for the ""most general"" case in which the following statement is true: Let $\mathcal{F}_1$ and $\mathcal{F}_2$ be ordered sets and $f\colon \mathcal{F}_1\to\mathcal{F}_2$ an injective function, then $f$ is strictly monotonic. I'm well aware that ""the most general"" isn't well defined. I'm wondering what are some (general) conditions that could be added so that the statement is true. A couple of cases in which it's not true: An injective real valued function of a real variable that is not monotonic on any interval. $$\begin{align}\mathbb{R}&\to \mathbb{R}\\
x&\mapsto\begin{cases}x, &x \text{ is rational}\\
-x, & x \text{ is irrational}\end{cases}\end{align}$$ An injective continuous function that is not monotonic. $$\begin{align}\mathbb{Q}&\to\mathbb{Q}\cup\sqrt{2}+\mathbb{Q}\\
x&\mapsto \begin{cases}x,& x< \frac{\sqrt{2}}{2},\\-x+\sqrt{2}, & x>\frac{\sqrt{2}}{2}.\end{cases}\end{align}$$ where $\mathbb{Q}$ are the rational numbers. Edit: This question was about a function on an ordered field but, as suggested by Henry Davii on his answer, there is no need for $\mathcal{F}$ to be fields, they could just be sets. I've changed all the mentions of the word ""field"" on this question to ""set"".","['general-topology', 'monotone-functions', 'analysis', 'real-analysis']"
4280729,Relationship between injectivity of induced map of fundamental groups and restriction of covering space to open submanifold,"Let $X$ be a connected manifold, $U\subset X$ be an open submanifold of $X$ such that $\pi_1 U\to \pi_1 X$ is injective. How can I prove the universal covering $\widetilde{U}\to U$ is the restriction of the universal covering $\widetilde{X}\to X$ ? I guess it will need some knowledge about the Deck transformation group and covering space. But I have no idea how to give a proof. Could you please give me some help with the details? Thank you very much!","['general-topology', 'algebraic-topology', 'differential-geometry']"
4280785,Can the poles of a complex function $f(z)$ be defined as the locations where $\lvert f(z) \rvert = \infty$?,"According to Wikipedia , A zero of a meromorphic function $f$ is a complex number $z$ such that $f(z) = 0$ . A pole of $f$ is a zero of $1/f$ . Is there a reason why a pole cannot be defined as the location where $\lvert f(z) \rvert = \infty$ ? I was imagining this to be the case based on this plot of the magnitude of the complex gamma function:",['complex-analysis']
4280801,$\sqrt{6}+\sqrt{3}$ is not rational proof,"I want to prove $\sqrt{6}+\sqrt{3}$ is not rational; here is my attempt: Assume for the sake of contradiction that $\sqrt{6}+\sqrt{3}$ is rational. Then $(\sqrt{6}+\sqrt{3})^2$ must also be rational. Since $$(\sqrt{6}+\sqrt{3})^2=9+2\sqrt{6}\sqrt{3}=9+2\sqrt{2}\sqrt{3}\sqrt{3}=9+6\sqrt2,$$ we see $9+6\sqrt2$ must be rational. But, since $\sqrt{2}$ is not rational we have a contradiction and hence $\sqrt{6}+\sqrt{3}$ is not rational. Is it correct?","['algebra-precalculus', 'solution-verification']"
4280820,Combinatorial argument for a homework question: $\binom nr \binom r3=\binom n3\binom{n-3}{r-3}$ [duplicate],"This question already has answers here : Prove by a combinatorial argument that ${n \choose r}{r \choose s}={n \choose s} {n-s \choose r-s} $ (3 answers) Combinatorial Proof of ${{n}\choose{k}} {{k}\choose{j}} = {{n}\choose{j}} {{n-j}\choose{k-j}}$ [closed] (2 answers) Combinatorial proof of $\binom{k}{i}\binom{n}{k}=\binom{n}{i}\binom{n-i}{k-i}$ [duplicate] (2 answers) Closed 2 years ago . $${n\choose{r}}{r\choose3}={n\choose{3}}{{n-3}\choose{r-3}}$$ where $n\ge r\ge3$ I was able to show this algebraically using the combinations formula but I'm unsure how to approach it using the combinatorial argument. So far I've gotten this: There are $n\choose{r}$ ways to choose subsets of $r$ elements from a set of $n$ elements There are $r\choose3$ ways to choose subsets of 3 elements from a set of $r$ elements Therefore, by the product rule, there are $n\choose{r}$$r\choose3$ distinct ways to choose $r$ elements from a set of $n$ elements then to choose 3 elements from  a set of $r$ elements. At this point, I fail to see the connection between the two sides of the equation.","['combinatorial-proofs', 'solution-verification', 'combinatorics', 'binomial-coefficients', 'elementary-set-theory']"
4280872,How do I show that a map $f$ is a homeomorphism iff all component maps $f_i$ are homeomorphisms?,"I have the following problem: Let $I$ be a nonempty index set and let $(M_i,T_{M_i})$ and $(N_i,T_{N_i})$ be topological spaces. Moreover let $f_i:M_i\rightarrow N_i$ be maps. Finally endow $M=\prod_{i\in I} M_i$ and $N=\prod_{i\in I} N_i$ with the product topology. Show that the map $f=\prod_{i\in I} f_i: M\rightarrow N$ is a homeomorphism iff each $f_i$ is a homeomorphism. Proof We have just shown that $f$ is continuous iff each $f_i$ is continuous. From the lecture we know that a map $f$ between top. spaces is a homeomorphism iff $f$ is continuous, bijective and open. $\Rightarrow$ Let us assume that $f$ is a homeomorphism. We need to show that all $f_i$ are continuous, bijective and open. Since $f$ is continuous, we can immediately deduce that all $f_i$ are continuous. Let us denote $p_i:N\rightarrow N_i$ and $q_i:M\rightarrow M_i$ the projection maps. Since $M,N$ are endowed with the product topology we know that $p_i,q_i$ are both open. Thus we get immediately that $p_i\circ f$ is open. But since $p_i\circ f=f_i\circ q_i$ and $q_i$ is open, we can also deduce that $f_i$ has to be open. Now we only need to show that $f_i$ are bijective. We know that the projection map is surjective. Does it is correct till this point or do I wrote nonsense? (I would do the other inclusion similarly using the fact that $f_i$ are continuous, bijective and open) But for the bijectivity I have some problems. Could one gave me a hint? Thank you",['general-topology']
4280894,Calculate $\lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx}$,"Given $$f(x)=1-x^2+x^3 \qquad x\in[0,1]$$ calculate $$
\lim_{n\rightarrow\infty}\frac{\int_{0}^{1}f^n(x)\ln(x+2)dx}{\int_{0}^{1}f^n(x)dx}
$$ where $f^n(x)=\underbrace{f(x)·f(x)·\dots\text{·}f(x)}_{n\ \text{times}}$ . This is a question from CMC(Mathematics competition of Chinese)in $2017$ . The solution provides an idea: given $s∈(0,\frac{1}{2}),$ prove: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\$$ The final result is $\ln2.$ My approach For this: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0\\$$ I want to do piecewise calculation: $$\int_{s}^{1-s}f^n(x)dx+\int_{1-s}^{1}f^n(x)dx.$$ For this: $$\lim_{n\rightarrow\infty}\frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\$$ Here is the proof: when $\ \ n≥\frac{1}{s^2}$ , $$\frac{\int_{1-s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=\frac{\int_{0}^{s}(1-x^2(1-x))^ndx}{\int_{0}^{s}(1-x(1-x)^2)^ndx}\\\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{s}(1-x^2)^ndx}\leq\frac{\int_{0}^{s}(1-\frac{x}{4})^ndx}{\int_{0}^{1/\sqrt{n}}(1-\frac{x}{\sqrt{n}})^ndx}\\=\frac{\frac{4}{n+1}(1-(1-\frac{s}{4})^{n+1})}{\frac{\sqrt{n}}{n+1}(1-(1-\frac{1}{n})^{n+1})}\sim\frac{4}{\sqrt{n}(1-\frac{1}{e})}\rightarrow0.\\$$ For this: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.\\$$ Here is the proof: given $t,0<t<s<\frac{1}{2},$ then $$f(t)>f(s)>f(1-s).$$ Define $m_t=\min_{x\in[0,t]}f(x),M_s=\max_{x\in[s,1-s]}f(x),$ so $$m_t=f(t)>f(1-s)=M_s.$$ $$\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}\leq\frac{\int_{s}^{1-s}f^n(x)dx}{\int_{0}^{t}f^n(x)dx}$$ $$\leq\frac{(1-2s)M_s ^n}{tm_t ^n}=\frac{1-2s}{t}(\frac{M_s}{m_t})^n\rightarrow0.\\$$ In conclusion,we can get: $$\lim_{n\rightarrow\infty}\frac{\int_{s}^{1}f^n(x)dx}{\int_{0}^{s}f^n(x)dx}=0.$$","['contest-math', 'limits', 'calculus']"
4280913,Third derivative of $r(t)$ in Serret-Frenet TNB frame,"I obtained the formula by differentiating it as follows where $s$ denotes the arc length parameter. $T=\frac{dr}{ds}=\frac{dr}{dt}\frac{dt}{ds}  \Leftrightarrow r'=s'T \\ r'' = s''T +s'T'\\\frac{dT}{dt}=\frac{dT}{ds}\frac{ds}{dt}=s'\kappa N \\ \therefore r''= s''T+(s')^2\kappa N \\ r'''=s'''T+s''T'+2s's''\kappa N+(s')^2\kappa 'N+(s')^2\kappa \frac{dN}{dt} \\ \frac{dN}{dt}=\frac{dN}{ds}\frac{ds}{dt} =(-\kappa T+\tau B)s'\\r'''=(s'''-(s')^3\kappa ^2)T+(3s’s''\kappa+(s')^2\kappa')N+(s')^3\kappa\tau B
$ However, the coefficient of $N$ of $r'''$ is slightly different from deriving the formula of the torsion of a curve 's second answer (answered by Chappers). It's too late to comment on that thread, so please forgive me for posting a question along with my solution. Is there anything wrong with my solution?","['curves', 'calculus', 'differential-geometry']"
4280933,Probability N points in range,"I have looked for an answer to this problem but I didn't find any.
Apologies if my language is not at the level of this exchange. In my project, I deal with cells (imagine points of different colors) with coordinates x and y. These cells have different types and they are distributed on a surface.
I am measuring three quantities. Let's assume we have a square surface of side $L$ .  We have $N$ points of type $A$ randomly distributes on this surface, and also $K$ points of type $B$ also randomly distributed on this surface.  We can assume these points to be circular, with radius $r_c$ << L and $r_c$ < D (definition of D follows). I define the function $S(A, D)$ that gives the number of points ""B"" at a distance lower than ""D"" from a point A (so, it's really like looking at how many points fall in the circle centered in point of type A with radius ""D""). The first measure is: $$ M_{sum} =  \sum_{i=1}^N S(A_i, D) $$ The second measure is very similar, it's the average: $$ M_{avr} =  \frac{1}{N} \sum_{i=1}^N S(A_i, D) $$ For the third measure, I need another function, called $V$ . $$
  V(A, D) =
\begin{cases}
1,  & \text{if $S(A,D)>0$} \\
0, & \text{if $S(A,D)=0$}
\end{cases}
$$ The measure is: $$ M_{neig}=\frac{1}{N} \sum_{i=1}^N V(A_i,D)$$ The problem is the following.  I need to see if there are any significant differences in my data from a random distribution.
So, is there a way to estimate the expected values for these quantities?","['expected-value', 'statistics']"
4280953,"Prove $\{f(x_0):f\in A\}$ is closed interval for $A=\{f\in X^*:\|f\|\leq\beta,|f(x_i)|\leq\alpha_i\ \forall i\in I\}$","Let $X$ be a real normed space, let $\beta>0$ , and for some set $I$ , $\{x_i\}_{i\in I}\subset X,\{\alpha_i\}_{i\in I}\subset[0,\infty)$ . Then set $$A=\{f\in X^*:\|f\|\leq\beta,|f(x_i)|\leq\alpha_i\ \forall i\in I\},$$ and set $B=\{f(x_0):f\in A\}$ . I want to prove that $B$ is a closed bounded interval of the real line. Using that each $f\in A$ is norm-bounded by $\beta$ , it is easy to argue that $B$ is bounded. It is easy to show that $A$ is convex, and from there it follows that $B$ is convex. Hence, being a subset of the real line, it follows that $B$ is a bounded interval. The part I'm having troubles with is showing that $B$ is closed. It seems that $A$ is closed, and therefore weakly closed, being convex. Furthermore, we observe that $B=\phi(A)$ , where $\phi:X^{*}\to\mathbb R:f\mapsto f(x_0)$ is clearly continuous, so $\phi\in X^{**}$ . Hence $B$ is the continuous image of a weakly closed set. I don't know if this brings us any further. Other approaches I tried didn't seem helpful. Any help on the ` closed' part is much appreciated.","['weak-topology', 'functional-analysis', 'real-analysis']"
4280968,How do you derive that the inradius in a right triangle is $r=\frac{a+b-c}2$?,"If we have a right triangle then the inradius is equal to $$r=\frac{a+b-c}2,$$ where $c$ is the hypothenuse and $a$ and $b$ are the legs. This formula is mentioned in various places and it can be useful both in geometric problems and in problems on Pythagorean triples. 1 Question: How can one derive this formula? 1 It is stated on Wikipedia (in the current revision without a reference). Some posts on this site where this equation (or something closely related) is mentioned: If the radius of inscribed circle in a right triangle is $3 cm$ and the non-hypotenuse side is $14cm$, calculate triangle's area. , Prove the inequality $R+r > \sqrt{2S}$ , In a Right Angled Triangle. , How do I find the radius of the circle which touches three sides of a right angled triangle? , Is there a way to see this geometrically? , Range of inradius of a right Triangle . I will mention that I would be able to derive this myself in some way. (And some of the posts linked above in fact include something which basically leads to a proof.)
Still, I think that it is to have somewhere this nice fact as a reference. And I wasn't able to find on this site a question specifically about this problem.
I can post an answer myself - but I wanted to give others an opportunity to make a post first.","['euclidean-geometry', 'circles', 'geometry', 'triangles', 'plane-geometry']"
4280997,Fastest way to find $\cos{6\theta}$ or higher in terms of powers of $\cos$,I know that there is an answer here Is there a quicker way to write $\cos (n\theta)$ in terms of $\cos \theta$? but it uses the Chebychev polynomials which aren't present in my curriculum (so I can't use this method in exams). I was just wondering is there a fast way that I could obtain an expression for $\cos{6\theta}$ or higher that only involves complex numbers and trig identities? Faster than expanding $(z+\frac{1}{z})^6$ and then having to convert all the $\sin$ terms to cos which involves just a tedious amount of expansions.,"['trigonometry', 'complex-numbers']"
4281018,Determining if process $Z$ is a martingale.,"I have to determine whether a process $Z$ defined by $Z_0 = 0$ and $Z_t = W_t^5 - 10 \int_0^t W_u^3 du$ is a martingale. Where $W_t$ is a standard Brownian motion. I thought the best way to start was to determine $dZ_t$ and see what we get. So, I first used Ito's lemma to compute the first term. After some work I got $5W_t^4dW_t + 10 W_t^3dt$ . Then I wanted to compute $10 \int_0^t W_u^3 du$ , however I didn't know how to compute it. Maybe I can use Ito's lemma again, but I don't know how to do it. Could someone help me?","['martingales', 'derivatives', 'stochastic-differential-equations']"
4281061,Switch being red knowing that it worked? (conditional probability),"A spaceship has switches inside its cabinet control. 30% of
them are red, 70% are blue. You've learned
that the probability of a switch not to work is 0.12 if it is red ,
and 0.2 if it is blue . A switch is randomly pressed. What is the probability of it being a red switch knowing that it worked? I'm really stuck in what to think here. Probability is not really intuitive to me. This is what I've tried, being $R = ""Red""$ , $W = ""Worked""$ : $P(R | W) = \frac{P(R \cap W)}{P(W)}$ , now I have to find each the numerator and denominator, but I can't get there with the info that I have.","['conditional-probability', 'statistics', 'bayes-theorem', 'probability']"
4281075,How to find the Closed Form Expression of the function $f$ that outputs the number of ones in the binary expression of a given natural number? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am interested in finding the closed form expression of the number of ones in the binary expression of the given natural number. I have no idea whether it is possible to do so. Any help is appreciated.","['number-theory', 'binary', 'functions', 'closed-form']"
4281140,Visually explaining this probability union rule,"I'm trying to visually wrap my head around the following equivalence of the union probability rule: $$
P(A)+P(B)-P(A)P(B)=P(A)+(1-P(A))P(B)
$$ I understand that $(1-P(A))=P(A')$ and the whole equivalence makes sense to me algabraically. However, I have sketched these Venn diagrams to try and visualise the equivalence, and what I am struggling to understand is that if I was to combine the Venn diagrams for $P(A)$ and $P(A')P(B)$ then I would have $P(A)+P(B)$ , not $P(A)+P(B)-P(A)P(B)$ . Where am I tripping up in my reasoning?","['inclusion-exclusion', 'discrete-mathematics', 'probability']"
4281145,Is it possible to find the eigenfunction of the following differential operator?,Consider a differential operator of the form $\sum_{j=-\infty}^\infty(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}}-g z_j-g\partial_{z_j})$ Here g is a constant. The operator consisting up the first two terms $\sum_j(z_{j+1}\partial_{z_j}+z_j\partial_{z_{j+1}})$ has an eigenfunction of the form $\sum_m z_m e^{ikm}$ wth eigenvalue $2\cos k$ and the operator of the form $g(z_j+\partial_{z_j})$ has an eigenfunction of the form $e^{-z_j^2/2+Ez_j}$ with Eigenvalue $Eg$ . Is there a way to find an eigenfunction of the entire thing? Is there any ansatz that can help me proceed?,"['operator-theory', 'eigenvalues-eigenvectors', 'ordinary-differential-equations', 'partial-differential-equations']"
4281167,Schwarz-Pick Lemma from upper half plane,"Let $f: UHP \to D_1(0)$ map the upper half of the complex plane to the unit disk. It is known that $|f(z)|\leq 1$ for real $z$ and also at infinity. Furthermore, consider that $f$ is meromorphic in the UHP. Then, by the Maximus Modulus Principle one concludes that $|f(z)|\leq 1$ for all the domain. I am trying to show that $$g(z) = \frac{f(z)-f(\omega)}{1-f(z)\bar{f(\omega)}}*\left(\frac{z-\omega}{z-\bar{\omega}}\right)^{-1}$$ is also bounded as $|g(z)|\leq 1$ , in the UHP and for any $\omega$ (parameter) also in the UHP. I know that $g(z)$ is meromorphic in the UHP, and if I managed to show that it is bounded in the boundaries, I would use the MMP again to generalize for the whole UHP. Both when $z$ is real or infinity, I was able to show that the second term is $\leq 1$ . However, the term with $f(z)$ seems a bit harder to analyse. How can I proceed?",['complex-analysis']
4281173,The general solution of $\tan(x) = \tan(2x-π/2)$,"The general solution given by the teacher is $$x = \fracπ2 + kπ.$$ But to me, this solution is wrong because $\tan(π/2)$ has an indeterminate value (infinite). How do I know that one infinity is equal to the other? It has no sense. Therefore, I would put that it has no solution for the real values. Am I correct?","['algebra-precalculus', 'trigonometry']"
4281184,Determining the truth value of $∃x∀y (y=x^2+2x+1)$,"The domain for $x$ and $y$ are all integers. $∃x∀y (y=x^2+2x+1)$ can be interpreted as: There is an $x$ for all $y$ such that it satisfies $y=x^2+2x+1$ . There is a single $x$ value which results in all the domain values of $y$ - all the integers. This can't be true, as the relationship between $x$ and $y$ is given; a single $x$ value cannot result in every elements in the range of integers. Thus, the truth value of the statement is false. Yet, a different justification for the false value was stated on the answer sheet. Referring to it, the statement's truth value is false because for every $x$ , there is always an integer $y=x^2+2x+2>x^2+2x+1$ . Can anyone explain to me regarding its meaning? Thanks.","['quantifiers', 'logic', 'discrete-mathematics']"
4281204,Show that the matrix is a generator matrix of a MDS code,"Let $a_1,\dots,a_n$ be pairwise distinct elements of $\mathbb{F}_q$ and $k ≤ n$ . I have to show that the matrix $\begin{bmatrix}1&\dots&1&0\\a_1&\dots&a_n&0\\a_1^2&\dots&a_n^2&0\\\vdots&&\vdots&0\\a_1^{k-1}&\dots&a_n^{k-1}&1\end{bmatrix}$ is a generator matrix of an $[n+1,k,n−k+2]$ MDS code. if q is a power of 2, then the matrix $\begin{bmatrix}1&\dots&1&0&0\\a_1&\dots&a_q&1&0\\a_1^2&\dots&a_q^2&0&1\end{bmatrix}$ is a generator matrix of an $[q + 2, 3, q]$ MDS code. I don't even know where to start, the things we covered in the lecture are not so many, but what I thought could be useful, but don't know how to apply are: Theorem: Let $C$ be an $[n,k]$ code with generator matrix $G$ and parity check matrix $H$ . The following are equivalent: $C$ is MDS All $(n-k)\times(n-k)$ full size minors of $H$ are invertible All $k\times k$ full size minors og G are invertible Maybe I could use this theorem, part (3), but I'm not sure how to show that all the minors og G are invertible. Should I maybe, from definition of MDS, show that the distance is indeed $d(C)=n-k+2$ , and then conclude that the code is MDS? But aren't the elements arbitrary? Should I maybe look for a parity check matrix $H$ and do something? Please any hint is appreciated.","['finite-fields', 'matrices', 'abstract-algebra', 'discrete-mathematics', 'coding-theory']"
4281248,asymptotic for the complex exponential integral Ei(s),"Let $\operatorname{Ei}(s)$ denote the complex exponential integral function, which (if I am correct) can be defined for $s \in \mathbb{C}\backslash(-\infty,0]$ by $$\operatorname{Ei}(s) = \gamma+\log s -\operatorname{Ein}(-s)= \gamma+\log s+\sum_{n = 1}^\infty \frac{s^n}{n!n},$$ where $\operatorname{Ein}(s)$ is the entire function $\sum_{n = 1}^\infty \frac{(-1)^{n+1}s^n}{n!n}$ . Is it true that $$\operatorname{Ei}(s)\sim \frac{e^s}{s} \ (s \to \infty)$$ on $\{s \in \mathbb{C}: |\operatorname{Arg} s|\leq \frac{\pi}{2}-\epsilon\}$ for every $\epsilon > 0$ ?  More generally, for any $\epsilon > 0$ , can one give an asymptotic expansion of $\operatorname{Ei}(s)$ on that set?  Given computations in Mathematica, it seems unlikely that such an asymptotic relation can be extended to the case where $\epsilon = 0$ .  (Mathematica's command for the function is $\tt{ExpIntegralEi[s]}$ .) EDIT: I don't know why, but information on the web about the complex function $\operatorname{Ei}(s)$ is very scarce.  But it's an important function used a lot in analytic number theory, and in particular in the Riemann--von Mangoldt explicit formula for $\pi_0(x)$ , since one has $\operatorname{li}(s) = \operatorname{Ei}(\log s)$ .  Please, somebody, help!","['complex-analysis', 'special-functions']"
4281311,Maximizing an angle based on certain constraints,"$A (0,a)$ and $B(0,b)\; (a,b>0)\;$ are the vertices of $\triangle ABC$ where $C(x,0)$ is variable. Find the value of $x$ when angle $ACB$ is maximum. Now geometry's never really been my strong point, so I decided to go with a bit of calculus.
First, I used the sine rule: $$\mathrm{sinC=\frac{b-a}{2R}} $$ where R is the radius of the circumcircle. I note that for angle C to be maximum, sinC should be maximum. As such, R must be minimum.
Next, I used the relation $$\mathrm{R=\frac{(b-a)\cdot\sqrt{x^2+b^2}\cdot\sqrt{x^2+a^2}}{2\Delta}} $$ where $\mathrm{\Delta \text{ is the area of }ABC=\frac{(b-a)x}{2}}$ . A bit of comparatively lengthy differentiation gives me the value of $x$ as $\sqrt{ab}$ . When I go through the solutions, it's simply been stated: For angle ACB to be maximum, the circle passing through A,B will touch the X-axis at C. Beyond this, it's been solved using the very simple $\mathrm{OC^2=OA\cdot OB}$ , where O is the origin. So the above statement seems to be the difference between a lengthy differentiation and a one line solution. It's getting a little difficult for me to see why the above statement should be intuitive. Could someone shed a bit more light on it for me, and possibly provide an intuitive proof?","['euclidean-geometry', 'angle', 'circles', 'geometry', 'trigonometry']"
4281349,Explaining why the graph of $\cos(\sin x-\sin y)=\frac12$ looks the way it does,"I was playing around in Desmos, when I came across this graph : $$\cos(\sin x-\sin y)=\frac12.$$ My goal was to come up with reasons for why an equation leads to a graph, but I was stuck on this one. I could not come up with a reasonable explanation for how this equation leads to weird rounded shapes across the plane. If someone could explain I would be grateful.","['trigonometry', 'graphing-functions']"
4281379,Light bulbs in a rectangular grid,"Suppose we have a grid of $5 \times 5$ light bulbs with a switch to each bulb. If we press a switch it toggles all the lights in its row and column. Given any initial configuration, is it possible to make all the lights on or all the lights off by a particular sequence of switches? I do not remember whether I have read about this problem somewhere or it just came up in my mind by some similar problems. Any hints/suggestions/links are highly appreciated.","['puzzle', 'finite-fields', 'matrices', 'linear-algebra', 'discrete-mathematics']"
4281400,Book on advanced discrete probability?,"Searching for materials on discrete probability I find ""first course in probability"" type stuff and some seemingly rather specific things. Is there a principled, step-by-step introduction to advanced discrete probabilistic topics (e.g. in a book)? Are there any advanced general theorems in this area at all?","['discrete-mathematics', 'book-recommendation', 'probability', 'reference-request']"
4281417,Sum of tensor products is a bounded operator,"I came across this in a book: Let $B$ be a Banach space with Schauder basis $\{\omega_j\}$ , let $B'$ be its dual with $\{\nu_i\}$ being a corresponding biorthogonal system, i.e. $\langle \omega_j,\nu_i\rangle=\delta_{ji}$ . Then it is well-known that the sequence of operators $\{F_n\}$ defined as $$F_n=\sum\limits_{j=1}^n\omega_j \otimes \nu_j$$ is bounded. I am not seeing this. So bounded means there exist some $M>0$ such that for all $n\in\mathbb{N}$ and some $v\in B$ $$\frac{\|F_n v\|}{\|v\|}=\frac{\|\sum\limits^{n}_{j=1}(\omega_j\otimes\nu_j)(v)\|}{\|v\|}\leq M$$ Why is that bounded when I choose an infinite vector and then taking the limit $\lim\limits_{n\to\infty}F_n$ ?","['tensor-products', 'operator-theory', 'functional-analysis']"
4281469,Finding $\int_0^{\frac{\pi}{2}} \frac{x}{\sin x} dx $,"Is there a way to show $$\int_0^{\frac{\pi}{2}} \frac{x}{\sin x} dx = 2C$$ where $C=\sum_{n=0}^{\infty} \frac{(-1)^n}{(2n+1)^2} $ is Catalan’s constant, preferably without using complex analysis? The following is an attempt to expand it as as a series: \begin{align*}
\int_0^{\frac{\pi}{2}} \frac{x}{\sin x} dx
&= \int_0^{\frac{\pi}{2}} \frac{x}{1-\cos^2 x}\sin x\  dx \\
&= \sum_{n=0}^{\infty} \int_0^{\frac{\pi}{2}} x\sin x \ \cos^{2n}x \ dx \\
&= \sum_{n=0}^{\infty}\frac{1}{2n+1} \int_0^{\frac{\pi}{2}} \cos^{2n+1}x \ dx \\
&= \sum_{n=0}^{\infty} \frac{4^n}{\binom{2n}{n}(2n+1)^2}
\end{align*} which is close but not quite there.","['integration', 'calculus', 'catalans-constant', 'definite-integrals']"
4281473,Proving linear independence of the functions $\sin (nx)$ and $\sin (mx)$ without using Wronskian,"I was asked to show the linear independence of the functions $\sin (nx)$ and $\sin (mx)$ where $m,n\in \mathbb N$ , $m\neq n$ . It can of course be done by computing the Wronskian, but I just wanted to avoid it and go old school. Here's a small sketch of my solution- Let $f(x)=a\sin(mx)+b\sin(nx)=0\;\;\forall x$ We have three cases, namely both $m,n$ are even; exactly one of them is even; or both of them are odd. Case I: Both are even Let $m=2^{k_m}\times o_m$ and $n=2^{k_n}\times o_n$ where $o_m$ and $o_n$ are odd. WLOG, assume $k_m>k_n$ . Put $x=\frac \pi{2^{k_m}}$ and note that $$f\left(\frac \pi{2^{k_m}}\right)=a\sin(\mathtt{odd}\times \pi)+b\sin\left(\mathtt{odd}\times \frac \pi{2^l}\right)$$ for some $l\in \mathbb N$ . So, $$f\left(\frac \pi{2^{k_m}}\right)=0+cb$$ for some $c\in [-1,1]\backslash\{0\}$ , which completes the proof of $a=b=0$ . Case II: Exactly one of them is even We will use the same idea as before. Let the highest power of $2$ in the even number be $k$ and use that to arrive at the form $$a\sin(\mathtt{odd}\times \pi)+b\sin\left(\mathtt{odd}\times \frac \pi{2^h}\right)$$ and complete the proof. Case III: Both are odd Evaluate $f$ at $x_1=\frac \pi 2$ and $x_2=\frac {3\pi}2$ to get the equations $$a+b=0\\ a-b=0$$ solving which gives us $a=b=0$ . Is this proof okay? Especially, is the last case okay? If not, how can we tackle that case? Also, is there a smarter choice of the $x$ 's to arrive at the same conclusion? One solution that I got is that just checking the zeroes of the two functions are enough to arrive at the conclusion of dependence. While that's a very nice observation, please note that my question is whether my proof (especially the third case about which I'm most worried) is okay, and whether there are smarter choice of $x$ 's that can establish the same job.","['trigonometry', 'solution-verification', 'linear-algebra']"
