question_id,title,body,tags
3939493,Evaluating $\left.\int_0^{\pi/2}\sqrt{1+\frac1{\sqrt{1+\tan^nx}}}\text dx \middle/\int_0^{\pi/2}\sqrt{1-\frac1{\sqrt{1+\tan^nx}}}\text dx\right.$,"Evaluate the integral ratio $$\dfrac{I_1}{I_2}=\dfrac{\displaystyle \int_{0}^{\frac{\pi}{2}} \sqrt{1+\frac{1}{\sqrt{1+\tan^nx}}} \mathrm{d}x}{\displaystyle \int_{0}^{\frac{\pi}{2}} \sqrt{1-\frac{1}{\sqrt{1+\tan^nx}}} \mathrm{d}x}$$ Using Wolframalpha, I found out that the ratio of the above two integrals is $\sqrt{2}+1$ , that is, independent of $n$ . I couldn't think of a way to solve it. I don't think any simple substitutions would work here. Any useful hints would be appreciated.","['integration', 'definite-integrals', 'ratio', 'calculus', 'trigonometric-integrals']"
3939611,Infinite fraction series,"The infinite fraction $$f(n)=n+\frac{n}{n+\frac{n}{n+...}}$$ can be simplified to $$f(n)=\frac{n+\sqrt{n^2-4n}}{2}$$ However, I wanted to know if the fraction $$1+\frac{2}{3+\frac{4}{5+...}}$$ can be simplified further to get an answer using algebra or is the answer a transcendental number? On solving this in Desmos (till 25), I got 1.54149408254. This is an image of the Desmos page.","['fractions', 'continued-fractions', 'sequences-and-series']"
3939618,how to determine the domain of this function: $\sqrt{(\sin(x))^2 + \sin x}$?,"let $x$ be in this interval: $[0, 2\pi]$ . the exercise's request is to find the domain of this function: $$\sqrt{(\sin(x))^2 + \sin x}$$ Here's my attempt to solve it: I found the domain of the square root: $$(sin (x))^2 + \sin x \geq 0$$ In this case, I've found that $$\sin x \ge 0 $$ and that: $$\sin x \ge -1 $$ in the trigonometric circle, I have found that the set of solution is only the 4th quadrant, since the sin of -1 is $3/2\pi$ (270 degree). Is that correct?","['algebra-precalculus', 'functions', 'trigonometry']"
3939762,Four-letter word contains no two consecutive equal letters.,"This is taken from the book on Combinatorics, by Daniel Marcus. Request vetting: A16: Find the probability that a four-letter word that uses letters from $A,B,C,D,E$ contains (a) no repeated letters; (b) no two consecutive equal letters. (a)
Total (Inclusion) ways to form four-letter word from five letters: $5^4$ . Ways to have repetitions of $A$ : Two $A$ : Choose two positions for $A$ in $\binom{4}2= 6$ ways. Rest two positions can be filled in : $4\times 3= 12$ ways. So, by product rule: $72$ words. Also, the same for other $4$ letters, leading to $72\times 5= 360$ words. Three $A$ : Choose three positions for $A$ in $\binom{4}3= 4$ ways. Rest one position can be filled in : $4$ ways. So, by product rule: $16$ words. Also, the same for other $4$ letters, leading to $16\times 5= 80$ words. Four $A$ : Choose all four positions for $A$ in $\binom{4}4= 1$ ways. Also, the same for other $4$ letters, leading to $5$ words. Sum of above three cases: $360+80+5= 445.$ Left are: $5^4 - 445= 180.$ $p= \frac{180}{625}$ (b) no two consecutive same letters' case. Approach#1: To get, need subtract from $5^4$ the chances of having two consecutive same letters. This is divided into cases: Case 1: Position #1,2 are the same, and position #4 can match too : $5\times 1\times 4\times 5= 100$ cases. Case 2: position #2,3 are the same: $5\times 4\times 1 \times 4= 80$ cases. Case 3: Position #3,4 are the same, and position #1 can match too: $5\times 4\times 1\times 5=100$ So, get : $625- 280= 345$ cases. Approach#2: Alternatively, can 'directly' calculate by having $5$ choices for the first position, then $4$ choices, then again need eliminate one choice (of the second element) to have $4$ choices for the third position, and same for the fourth position. This leads to: $5\times 4^3= 320$ choices possible. The second approach seems correct, but not clear why first approach is faulty. Seems there are too many factors to consider in the first approach & hence error-prone (to miss some case(s) as here). So, second approach seems better.","['discrete-mathematics', 'combinatorics-on-words', 'combinatorics', 'probability']"
3939813,How to find number of injections from a finite set A to finite set B where |A| < |B|?,I need to find number of injections from a finite set A to finite set B where |A| < |B|. How could i do that? I don't understand what does |A| and |B| mean,"['order-theory', 'relations', 'discrete-mathematics']"
3939864,2 values expected value problem,"A bank account has 1000 dollars and get's a 5% interest daily. Every day the account has $0.405$ probability to lose $20$ dollars. What's the mean of the amount of money in the acount after 2 days? I thought about calculating $((1000-20*0.405)*1.05-20*0.405)*1.05$ But I think it might be wrong because the probabiltity changes depending on the previous day.
I mean that the probability to lose in both days is $0.405^2$ , one day lose and the other no lose would be $0.405*(1-0.405)$ to my understaing, so I would need to take it into acount somehow but don't know how. thank you.",['statistics']
3939888,Connection between quadratic characters and quadratic twists of elliptic curves,"So apparently there is some kind of a connection between quadratic characters and twists of elliptic curves that I can't understand. I am well aware of quadratic twisting of an elliptic curve $E$ , we get a curve $E^d$ , and the curves are isomorphic over $\mathbb{Q}(\sqrt{d})$ . I also know about quadratic characters associated to fields $\mathbb{Q}(\sqrt{d})$ , given by $$\chi=\chi_{\mathbb{Q}(\sqrt{d})}:\mathbb{Z}_{>0} \rightarrow \mathbb{C},$$ $$ \chi(n)=\left( \frac{disc(\mathbb{Q}(\sqrt{d}))}{n} \right). $$ In literature I often see twists of elliptic curves $E^d$ being denoted by $E^{\chi}$ . Why is that so? what is the connection? Am I even using the right character here?","['algebraic-curves', 'number-theory', 'algebraic-number-theory', 'elliptic-curves']"
3939911,Maximum value of $4|\cos x|-3|\sin x|$ [duplicate],"This question already has answers here : Find range of $f(x)=3|\sin x|-4|\cos x|$ (3 answers) Closed 3 years ago . How will I find the maximum of $4|\cos x|-3|\sin x|$ The absolute value confuses me, The maximum value would be 5 if it were not there.","['inequality', 'absolute-value', 'calculus', 'trigonometry', 'algebra-precalculus']"
3939918,Find a Noether normalization,"Let $I \subseteq k[X_{1}, X_{2}, X_{3}, X_{4}]$ be the ideal generated by the maximal minors of the $2 \times 3$ matrix $$\begin{pmatrix}
X_1 & X_2 & X_3\\
X_2 & X_3 & X_4
\end{pmatrix}.$$ Find a Noether normalization $k[Y_1, Y_2, Y_3, Y_4] \subseteq k[X_1, X_2, X_3, X_4]$ such that $I \cap k[Y_1, Y_2, Y_3, Y_4] = (Y_1, \ldots, Y_r)$ for a suitable $r$ . What I've done: I never encountered what a maximal minor is, so after some searching I suppose it means the determinant(s) of the maximal submatrices, i.e. in this case all $2 \times 2$ submatrices which then are (by deleting a column): $\begin{pmatrix}
X_1 & X_2\\
X_2 & X_3
\end{pmatrix}, \begin{pmatrix}
X_1 & X_3\\
X_2 & X_4
\end{pmatrix}, \begin{pmatrix}
X_2 & X_3\\
X_3 & X_4
\end{pmatrix}.$ Then, taking determinants we get $I = (X_1 X_3 - X_{2}^{2}, X_1 X_4 - X_3 X_2, X_2 X_4 - X_{3}^{2})$ . Is this correct? The next step would be to use the constructive proof of Noether's Normalization Lemma . However, I can't seem to understand the entire procedure of that proof and how to apply it to this problem. Perhaps if someone can illustrate this process, then I will better understand it after seeing it done.","['algebraic-geometry', 'commutative-algebra']"
3939983,A concave function on a compact and convex set is upper semi continuous,"Let $X$ be a compact and convex set and $f:X \to \mathbb{R}$ be a concave function. Therefore, $f$ is continuous on interior of $X$ . I want to know whether $f$ is upper semi continuous on $X$ . If not, under which (minimum) assumption is?","['semicontinuous-functions', 'real-analysis', 'calculus', 'functional-analysis', 'convex-analysis']"
3940010,Finding the derivative- is my answer correct? Is the method correct?,"Question - find the derivative of $y$ with respect to $z$ $$y=  2^{z^3} $$ So what I’d do is
Differentiate boths sides of the equation so $$\frac{dy}{dz}= \frac{d(2^{z^3 })}{dz}$$ Then I applied the chain rule by assuming, $$ u=z^3$$ which resulted in $$ 2^{z^3}\ln2\frac{dz^3}{dz}$$ Using the power rule I finally got the answer to be $$ 3z^2\cdot2^{z^3}\ln2$$ Is my answer correct?","['calculus', 'solution-verification', 'derivatives', 'chain-rule']"
3940059,"Maple calls absolute convergent sum divergent, but why?","Determine whether the sum is divergent, absolute convergent or conditionally convergent. First, I tried to solve this by hand. I tried to find a convergent majorant series for the series in question. $$\bigg|\cos(n^2+1) \frac{2+n^2}{1+n!} \bigg| \leq\frac{n^2}{n!}$$ It turns out that $\sum_{n=1}^{\infty}\frac{n^2}{n!}=2e$ , according to Maple that is. This result shows that the sum in the question is absolute convergent. I also tried to use Maple to double check my answer, and it gave me this. When Maple gives this sort of output, it usually means that the sum is not convergent. But didn't I just show that it was, or have I made a mistake? Can a ""Maple man"" tell me why Maple gives this result, and perhaps how I can avoid it?","['convergence-divergence', 'maple', 'summation', 'sequences-and-series']"
3940078,"Calculating $P\{\max(-x_{(1)},x_{(n)})\leq x \}$ where $X_{i}$ are i.i.d. from uniform distribution","Let $X_{i}$ be i.i.d. from uniform distribution $[-\theta,2\theta]$ . I am interested in if the following is true: $P\{\max(-x_{(1)},x_{(n)})\leq x \}= P\{-x_{(1)}\leq x , x_{(n)}\leq x \}\\ = P\{-x\leq x_{(1)} , x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x, ... ,-x\leq x_{(n)}\leq x \}\\= P\{-x\leq x_{1} \leq x \} ... P\{-x\leq x_{n} \leq x \}\\=(\int_{-x}^{x}\frac{1}{3\theta})^{n} $","['statistics', 'order-statistics', 'probability']"
3940125,"$TS^2$ is not trivial , seeing explicitely what might fail when we try and construct a trivialization","I just did an exercise, that was around the fact that if we have an orientable $1$ -dimensional vector bundle over a paracompact space then it's trivial. It's a well know fact that $TS^2$ is not trivial do to the hairy ball theorem, and this is a $2$ -dimensional vector bundle so the latter result does not apply but I was wondering what failed if I tried to construct a trivialization for this, that is , we have local trivializations of $TS^2$ and then we can glue them together using partitions of unity . What would fail for the fact that we do not have that this gives an isomorphism of vector bundles to $S^2\times \mathbb{R}^2$ .  I don't know if anyone has tried to do this or knows a reference where I could look something like this up but any discussion regarding what may fail at the fiber levels, since we are using partitions of unity and I guess we will need to use formulas for the transition functions trough the tangent vector represented in different bases. Any comment is appreciated. Thanks in advance.","['differential-topology', 'vector-bundles', 'differential-geometry']"
3940142,notation for the set n choose k,"Lately I've found myself writing things like $ijk \in {n \choose 3}$ , by which I mean that $i,j,k$ are coming from the sets enumerated by ${n \choose 3 }$ . It's a slight abuse of notation, since ${ n \choose 3 }$ is usually understood to be a number and not the set it is counting. Is there a standard notation for this set? i.e. the set of cardinality $k$ subsets of $\{1, \ldots, n\}$ . I'd be happy to learn that the answer is no and that my abuse of notation is common or clear enough.","['notation', 'combinatorics']"
3940183,How many distinctively different necklaces are possible if you use 0 to 3 beads out of 5 different beads given? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 3 years ago . Improve this question Today in my IB Math II HL/AP Calculus BC class we were going over IB Math I topics and did the given problem. So you would calculate the number of necklaces for a 0, 1, 2, and 3 bead necklace. I used $5C0+5C1+5C2(2!)(1/2)+5C3(3!/3)(1/2)=26 to find my answer. In a two bead necklace it would be multiplied by 2 because of the two different combinations, but then is cancelled out because a necklace is circular. However, my teacher said that a two bead necklace cannot exist, and therefore did not divide by 2, saying that a necklace A-B and B-A are two different necklaces. He said that two points cannot make a circular shape and must be treated as linear. Maybe he is right?? Therefore he gave the answer 36 because he did not divide the 5C2 term by 2. Thank you in advance, I hope I can find an answer to this problem.","['permutations', 'statistics', 'combinations', 'necklace-and-bracelets', 'probability']"
3940237,Collatz problem on integers modulo $N$,"Let $\langle n \rangle_N$ be a notation for an integer $n$ modulo $N$ .
Now consider the function \begin{align}
	f(n) = \begin{cases}
		(3n+1)/2 \text{,} & \text{if } n \equiv 1 \pmod{2} \text{,} \\
		n/2 \text{,}      & \text{if } n \equiv 0 \pmod{2} \text{,}
	\end{cases}
\end{align} and the sequences \begin{align}
	a_{i} = \left\langle f(a_{i-1}) \right\rangle_N
\end{align} for all $0 < a_0 < N$ . I deal with the question of whether, for given modulus $N > 2$ and for each starting value $0 < a_0 < N$ , there is an element $a_i = 1$ with $i \ge 0$ . I figured out that the answer is yes (i.e. the Collatz conjecture holds in the set of integers modulo $N$ ) in about half of the cases.
Moreover, I have found that necessary conditions for the existence of $a_i = 1$ are \begin{align}
	N &\neq -1 \pmod{3} \text{, and} \\
	N &\neq \phantom{+}0 \pmod{19} \text{.}
\end{align} My questions are: Has this problem been studied before? (Collatz problem on integers modulo $N$ ) Would anyone be able to sketch a proof of the necessary conditions? (Why 19?)","['modular-arithmetic', 'collatz-conjecture', 'number-theory', 'finite-groups', 'sequences-and-series']"
3940360,Proving $a_n = \sum_{k=1}^{n}\frac1{k}\sin(\frac{k\pi}{n+1})$ is increasing,"Is the sequence $a_n = \sum_{k=1}^{n}\frac1{k}\sin(\frac{k\pi}{n+1})$ increasing? Note that it converges to the integral $\int_0^\pi\frac{\sin x}{x} \,dx$ . I was plotting this sequence on Desmos and apparently it is monotone. But I'm having an hard time proving this. It seems that it's difficult to manipulate the sum: I don't think I can employ functions here and study some derivative; I used the sine sum-to-product formula while trying to show that $a_n < a_{n+1}$ , but it makes things more cumbersome. What could be a way to start? EDIT: for the sake of my problem, it suffices to show that the integral is un upper bound though. I'm not really interested whether it is increasing or not. As suggested by a user, this question was asked to complete the proof given in the answer to another question, which I posted here Bound the absolute value of the partial sums of $\sum \frac{\sin(nx)}{n}$ . Basically, what I'm trying to show is that the sequence of the maxima of the peaks of the partial sums of the Fourier series for the sawtooth wave is increasing to the integral given above (related to the Gibbs's phenomenon).","['sequences-and-series', 'real-analysis']"
3940437,Functorial Algebraic Geometry - Schemes as Gluing Construction,"Coming from a category theory background and having no experience with smooth manifolds the definition of a scheme as certain functors $\mathsf{CRing} \longrightarrow \mathsf{Set}$ feels much more natural to me than the definition as locally ringed space . Yet, I'm not really sold on the current way of constructing schemes of functors. Let me explain. Given a representing functor $\mathsf{CRing(A,-)}$ (which I will call affine scheme ), the usual Zariski-Topology on $A$ lets us define the notion of an open subfunctor $X \subseteq \mathsf{CRing}(A,-)$ , which I find quite natural. What confuses me are the following steps: Having the notion of open subfunctor of an affine scheme, we can define openness for arbitrary subfunctors of arbitrary functors $\mathsf{CRing} \longrightarrow \mathsf{Set}$ . We then restrict our attention to those functors, which satisfy some sort of gluing condition, that is to say we consider sheaves on the Zariski-site. The final step to obtain schemes is to restrict further to those sheaves, which admit a cover by affine schemes. I don't really get the following behaviour: We start with some sort of topology on subfunctors of affine schemes, which we generalize to arbitrary functors. So to me it feels like sheaves on the Zariski-site should already come equipped with the property of having a cover by affines, but they obviously do not . Why is that? I don't have much experience with sites yet, but I think I will be able to figure out sometime, where the definition of a sheaf on a site breaks regarding providing a cover. The next question is of more importance to me, because I could not figure out where (or if?) the following alternative approach to schemes fails. Why don't we (or can we) define the category of schemes as follows? The category of schemes $\mathsf{Sch}$ is the smallest full subcategory of $[\![\mathsf{CRing},\mathsf{Set}]\!]$ containing the affine schemes and being closed under forming colimits of gluing dates of open subfunctors of affine schemes (gluing dates being defined as in the usual topological setting). Thank you very much for your time and considerations!","['algebraic-groups', 'algebraic-geometry', 'category-theory']"
3940457,Find the linear transformation of a matrix knowing 4 linear transformations,"I'm stuck with an exercise where they give me 4 linear transformations $T:M_{2\times2}\to\Bbb R$ for 4 matrices 2x2 and then they ask me for the linear transformation of a fifth matrix. $$ T
  \begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
=3,
T\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
=-1,
T
  \begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
=0,
T
  \begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}
=0,
T
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
=?$$ and I have worked with this situation for $T:\Bbb R^n \to\Bbb R^m$ but never with matrices so I have the doubt that how is supposed to find 4 constants that will multiply de 4 matrices I know to compose the fifth matrix to finally can find the linear transformation of this last one. (I have tried adding up the 4 matrices as follows) $$ 
  \begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
+
\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
+
  \begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
+
  \begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}=
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
$$ \begin{cases}
2=a  &  \\
1=b & \\
2=c\\
1=d\\
\end{cases} and then use the coefficients $$ 
  2\begin{pmatrix}
    1 &0\\
    0 & 0\\
  \end{pmatrix}
+
1\begin{pmatrix}
    0 &1\\
    1 & 0\\
  \end{pmatrix}
+
  2\begin{pmatrix}
    1 &0\\
   1 & 0\\
  \end{pmatrix}
+
  1\begin{pmatrix}
    0 &0\\
    0 & 1\\
  \end{pmatrix}=
  \begin{pmatrix}
    a &b\\
    c & d\\
  \end{pmatrix}
$$ and I just want to know if I continue in this way or I have committed any mistakes, thanks for your time <3","['matrices', 'linear-algebra', 'linear-transformations']"
3940471,"Given iid random variables, expected number of times that a random variable is greater than all previous variables.","I'm given $X_1, X_2, \dots$ iid with continuous cdfs, and let $A_1 = \Omega, A_n=\{\omega:X_n
(\omega) > X_i(\omega) \textrm{ for } i < n\}$ . I'd like to show that $\lim_{n\rightarrow \infty}{\frac{1}{\log(n)}\sum_{j=1}^{n}{1_{A_j}}} = 1$ almost surely. This is a pretty interesting result, but I'm pretty unsure of how I might go about showing this.","['probability-theory', 'random-variables']"
3940488,"If set cardinality and power sets are not functions, then what are they?","Lets say you have a (finite) set $S$ and want to know its cardinality $|S|$ . Then this seems like a function $$|\cdot|: \mathbb{S} \to \mathbb{N}$$ which certainly cannot exist, given that there is no such thing as "" $\mathbb{S}$ "", which would be the set of all sets. The same goes for the power set $\mathcal{P}(S)$ , which would look like $$\mathcal{P} : \mathbb{S} \to \mathbb{S}$$ which also certainly makes no sense. However, these still seem like functions: they take in an input (a set) and spit out an output. So since these are not functions, what are they?",['elementary-set-theory']
3940531,Generalizing the Borsuk problem: How much can we shrink a planar set of diameter 1 by cutting it into $k$ pieces?,"Borsuk's problem asks whether a bounded set in $\mathbb{R}^n$ can be split into $n+1$ sets of strictly smaller diameter. While true when $n=1,2,3$ , it fails in dimension $64$ and higher; I believe all other $n$ are open as of this writing. However, it turns out that at least in the $n=2$ case we can be more precise than ""strictly smaller diameter""; if the original set has diameter 1, we can ensure that each piece has diameter at most $\frac{\sqrt{3}}{2}\approx 0.866$ , a bound attained by the circle of diameter $1$ . To see that this holds, we note that the regular hexagon of width $1$ is a solution to Lebesgue's universal covering problem , and can be split into three sets of diameter $\frac{\sqrt{3}}2$ as well: I am interested in putting bounds on such dissections with more than $3$ pieces: what is the minimum diameter one can ensure when cutting a planar set of unit diameter into $k$ pieces? Using the same approach as above (finding specific sets with a lower bound, and dissecting a universal cover for sets of diameter 1), I have some bounds for higher $k$ as well, though only for $k=3,4,7$ are they exact: (Extending this table beyond $k=7$ would be difficult, as working out optimal dissections for the circle would become much more complicated.) Edit: By taking spokes at $72^\circ$ angles on a regular hexagon (with one spoke meeting the hexagon at the midpoint of a side), I think I can get a slightly better upper bound of around $0.6434$ for the case $k=5$ . Optimizing spoke placement further (so that the distances between spoke endpoints are equal) gets me around $0.6223$ . In the limit, I think the diameter of each piece is asymptotic to $\sqrt{\frac{2\pi}{3\sqrt{3}k}}\approx \frac{1.1}{\sqrt{k}}$ by tiling with regular hexagons. Certainly one can do no better than $1/\sqrt{k}$ when dividing the circle, using the isodiametric inequality (if the pieces were any smaller, they would have too little area). Using a trivial dissection of the square, one also has an upper bound of $\frac{\sqrt{2}}{\lceil\sqrt{k}\rceil}$ . Some questions I have about this problem: Has this question been investigated before in the literature? If so, what is known? Are there any $k$ for which the circle does not present the worst-case scenario for dissection? Can the $k=5,6$ upper bounds be substantially improved? I think using Pal's slightly smaller solution to the universal covering problem would allow for a few adjustments when $k=6$ , but haven't worked out the details.","['dissection', 'combinatorial-geometry', 'geometry', 'reference-request']"
3940582,"According to the ""DI Method"" (Tabular Method) - isn't every integral a sum of the derivatives?","Many years ago I encountered this really amazing solving technique called the ""DI Method for integration"" which is also called the tabular method (If I am not mistaken). It lets us choose two functions $g(x)$ and $h(x)$ and construct this table, for one which we take the derivative over and over (until we hit $0$ or until we hit the same integral we started with) and the second function we integrate (Which will be much easier as it is just one part of the whole function we want to compute the integral of) And then take the product of the diagonals, with an alternating sign. It lets us solve integrals such as $$ \int xe^x  ~ dx,  ~~\int \sin (x) e^x ~ dx$$ Pretty quickly (This is just a variation of the Integration By Parts condensed in a fine table) So I thought about this method, and wondered what would happend if we have this amorphous integral: $$ \int f(x) ~ dx$$ And choose $g(x) = f(x)$ and $h(x) = 1$ . This would give of this table: $$\begin{array}{|c|c|c|}\hline 
\text{Sign}& D & I \\ \hline 
\color{red}{+} & \color{red}{f(x)} & 1  \\ \hline 
\color{green}{-} & \color{green}{f'(x)} & \color{red}{x} \\ \hline 
\color{purple}{+} & \color{purple}{f''(x)} & \color{green}{\frac{x^2}{2}} \\ \hline 
\color{blue}{-} & \color{blue}{f^{(3)}(x)} & \color{purple}{\frac{x^3}{6}} \\ \hline 
\vdots & \vdots & \color{blue}{\vdots} \\ \hline 
\end{array}$$ So we basically have a nice pattern, we can describe the integral as an infinite sum of derivatives as so: $$ \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot (-1)^k \cdot \frac{x^{k+1}}{(k+1)!}$$ Where $f^{(k)}(x)$ is the $k$ -th derivative ( $f^{(0)}(x) = f(x)$ ). This seems like a Taylor Series of a function, but comparing for example this specific infinite sum for $\frac{e^x}{x}$ we get a totally different sum then its Taylor expansion. I am not going to lie, the Taylor expansion seems to give a better approximation, but they are not the same as far as I checked: $$ \int \frac{e^x}{x} ~ dx = \text{Ei}(x) \approx  e^x \left ( \frac{1}{x} + \frac{1}{x^2} + \frac{2}{x^3} + \frac{6}{x^4} + \mathcal{O} \left ( \frac{1}{x^5} \right ) \right ) ~~~ \text{Taylor} $$ Our sum, is something else, starting like: $$ e^x \left ( 1 - \frac{x-1}{2} + \frac{x^2 -2x+2}{6} + \dots \right ) ~~~ \text{Our sum}$$ Which is much different. In addition, why can't we break our sum into two different sums, one for the derivatives and one for the rest, which would give us that every integral of a function is the infinite sum of the derivatives multiplied by this sum, which is evaluated to $1-e^{-x}$ : $$ \int f(x) ~ dx = \sum_{k=0}^{\infty} f^{(k)}(x) \cdot \sum_{k=0}^{\infty} (-1)^{k} \frac{x^{k+1}}{(k+1)!} = (1-e^{-x}) \cdot \sum_{k=0}^{\infty} f^{(k)}(x)$$ Why is it that different? Have I made a mistake somewhere in this
progress? Or my way of thinking is completely off? I would like to
hear what you think about it. Thank you!","['integration', 'summation', 'calculus', 'taylor-expansion', 'derivatives']"
3940590,Orthogonal idempotents in $M_{4\times 4}(\mathbb{C})$.,"If $e_1,\cdots,e_n \in M_{4\times 4}(\mathbb{C})$ are $n$ distinct, nonzero, $4\times 4$ matrices with complex entries that satisfy $e_ie_j = e_je_i = 0$ and $e_i^2 = e_i$ for all $1\leq i \leq n$ , then I want to show that $n \leq 4$ . My initial instinct was to look at the Jordan Canonical forms of the matrices $e_i$ , and note that the minimal polynomial of $e_i$ must divide $x^2-x$ , and hence must be $x,(x-1)$ or $x(x-1)$ . The minimal polynomial can't be $x$ (since the $e_i$ 's were assumed nonzero) and it can't be $x-1$ , since then $e_i$ is the identity matrix which is only orthogonal to the zero matrix.
Thus, then minimal polynomial of $e_i$ is $x(x-1)$ , and the invariant factor decomposition of $e_i$ must be one of \begin{align*}
&(x,x,x(x-1)) &(x-1,x-1,x(x-1))& &(x(x-1),x(x-1)).
\end{align*} However, at this point I realized that this method does not seem to be on the right track, since  there is no reason why the $e_i$ 's shouldn't be similar, so knowing their JCF doesn't seem to help me reason why the total number of such matrices should be less than $4$ . Any thoughts or hints would be greatly appreciated.","['matrices', 'linear-algebra', 'idempotents']"
3940619,Why is the multiplicative group of a field an algebraic group?,"Reading from here: https://en.wikipedia.org/wiki/Algebraic_torus In particular, the paragraph under the heading ""Multiplicative group of a field."" So, in my mind, a multiplicative group of a field is denoted $F^\times$ and is just the group $(F \backslash \{0\}, \times)$ where $\times$ is the multiplicative operation in $F$ . So, this multiplicative group is an algebraic group? That means it is also an affine variety: a set of solutions of a system of polynomial equations. What is this system of polynomial equations, is it $\{f-x : f \in F^\times \}$ ? Also, the next sentence says that the multiplicate group of $F$ is such that for any field extension $E \backslash F$ the $E$ -points are isomorphic to the group $E^\times$ . Are $E$ -points those points that are in $E \backslash F$ ? What does it mean that the $E$ -points are isomorphic to $E^\times$ ? I'm realizing I asked a few different questions here... If anyone could answer anything it would be greatly appreciated!! Thanks, I don't know what I'd do without this website!",['algebraic-geometry']
3940666,Categorifying $1^2+2^2+3^2+\cdots+24^2=70^2$,"Does $1^2+2^2+3^2+\cdots+24^2=70^2$ or a simple equivalent have an interesting categorification? Lots of combinatorical identities do. For instance, the Vandermonde convolution identity and the binomial theorem can both be reinterpreted as isomorphisms of $S_m\times S_n$ -sets. Obviously this identity involves particular numbers, so conceivably if it did have a categorification it would be some kind of ""exceptional"" situation that doesn't work for arbitrarily-sized sets. And also maybe instead of using a full symmetric group we can use a ""big enough"" group. (For instance, I guess there's a Mathieu group that acts very transitively on $24$ points, perhaps that can be related to $(2n)(2n+1)(2n+2)=24\cdot70^2$ for $n=24$ .) I'll use the (soft-question) tag since this seems pretty subjective.","['exceptional-isomorphisms', 'categorification', 'finite-groups', 'combinatorics', 'soft-question']"
3940717,Justifying $\int_0^\infty e^{-ax^2}\ \mathrm{d}x$ for complex values,"I have searched and failed to find a rigorous proof showing that $$\int_{0}^\infty e^{-ax^2}\ \mathrm{d}x = \frac{\sqrt{\pi}}{2\sqrt{a}}$$ is true for $\Re(a)=0$ and $\Im(a)\neq0$ . For example, why does $$
\int_{-\infty}^\infty e^{-ix^2}\ \mathrm{d}x = \sqrt{\frac{\pi}{i}}
$$ and does one need to invoke contour integration to show this? Or is the following technique valid: Suppose $\Re (b)>0$ and $a$ is complex; then $$
\int_{-\infty}^\infty e^{-ax^2}\ \mathrm{d}x \stackrel{?}{=} \lim_{b\to0}\int_0^\infty e^{-(a+ b)x^2}\ \mathrm{d}x = \lim_{b\to0}\frac{\sqrt{\pi}}{2\sqrt{a+b}}.
$$ I have an idea for contour integration but is there any way to justify this with real analysis? If not I would very much appreciate a hint for the complex analytic approach. Thanks!","['complex-analysis', 'gaussian-integral']"
3940729,Determine the minimum of $\frac{\int_0^1{x^2\left( f'\left( x \right) \right) ^2 dx}}{\int_0^1{x^2\left( f\left( x \right) \right) ^2dx}}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question For all non-zero continuously differentiable function $f:[0,1]\to\mathbb R,f(1)=0$ , determine the minimum of $$\dfrac{\displaystyle \int_0^1{x^2\left( f'\left( x \right) \right) ^2\mathrm dx}}{\displaystyle \int_0^1{x^2\left( f\left( x \right) \right) ^2\mathrm dx}}$$ I totally have no idea how to start. Can anyone help?","['analysis', 'real-analysis', 'maxima-minima', 'calculus', 'functional-analysis']"
3940749,Compute $\int_0^{2 \pi} \frac{ \cos(2nx) }{ a^2 + \cos^2 x} dx$,Compute the integral $$I_n=\int_0^{2 \pi} \frac{ \cos(2nx) }{ a^2 + \cos^2 x} dx$$ where $n$ is any integer. Using trig identities to evaluate special cases I found answers for $n=0$ and $n=1$ . For $n=0$ $$I_0=\frac{2\pi}{ a \sqrt{a^2 +1}}$$ For $n=1$ $$I_1=2\pi\frac{ 2a( \sqrt{a^2 +1} - a ) -1 }{ a \sqrt{a^2 +1}}$$ How do I find a generic answer for any integer n?,"['integration', 'definite-integrals']"
3940808,Explicit bijection between the set of binary sequences and $\mathbb{R}$?,"I know there is a standard proof (Cantor’s diagonalization argument) to show that the set of infinite binary sequences, call it $\Omega := \{0,1\}^\mathbb{N}$ , is uncountable. However, I would like to describe an explicit bijection between $\mathbb{R}$ and $\Omega$ to show that they are equipotent. Is there a direct way to do this? Or is my best bet to find a bijection between $\Omega$ and $(0,1)$ (or something) and compose it with a bijection from $(0,1)$ to $\mathbb{R}$ ? On that note, is there a ""standard"" bijection between $\Omega$ and $(0,1)$ ? Many thanks.","['elementary-set-theory', 'real-analysis']"
3940818,How to find the eigenvalues of this matrix in a simple way?,"Can anyone help how to find the eigenvalues of the following matrix in a simple way? I expand the characteristic polynomial being, $$
    \lambda(\lambda-3)(\lambda - 2k) = 0
$$ and get the answer but intuition is that there must be a simple way to find it. $$
    \begin{bmatrix} 1 + k & 1 & 1 - k \\ 1 & 1 & 1 \\ 1 - k & 1 & 1 + k \\ \end{bmatrix}
$$","['linear-algebra', 'eigenvalues-eigenvectors']"
3940929,solutions to quadratic matrix equation $XFX - F^{-1}X^{-1}F = 0$,"What is the solution for the matrix $X$ in the following quadratic matrix equation? $$ XFX - F^{-1}X^{-1}F = 0 $$ where $F$ is a $N \times N$ discrete Fourier transform (DFT) matrix, so it's unitary ( $F^{-1} = F^\dagger$ ). Based on similar quadratic matrix equations it seems that $X$ would be expressed in terms of fractional powers of $F$ . I have access to the eigenvectors of $F$ and hence all fractional powers of $F$ thanks to the discrete fractional Fourier transform (DFRT) in Candan et al. (2000). Found an almost trivial solution $X = F^{-1/3}$ by guessing and checking. Substituting $X = F^{-1/3}$ into the equation results in $F^{1/3} - F^{1/3} = 0$ which is true hence $X = F^{-1/3}$ is a valid solution. Are there other nontrivial solutions? $F^{-1/3 + 4n}$ for integer $n$ is another obvious one due to the periodicity of the fractional Fourier transform.","['matrices', 'matrix-equations', 'fourier-analysis']"
3940938,"What is Lie Theory/ a Lie Group, simply?","I'm studying physics, and I continually come across mentions of ""Lie Theory"" and ""Lie Groups"" as they relate to such topics as particle physics and String Theory, as well as vague mentions of ""symmetry"". I've attempted to read some texts on the topic and, while I feel I could probably sift through it in due time, it is very terse and esoteric. I've had group theory, calculus up to university calc II, and a teensy bit of analysis. Until I'm able to study this proper, what is Lie Theory/ a Lie Group, put simply? What is with these vague mentions of ""symmetry""? What can be understood in terms of what I know now?","['lie-algebras', 'abstract-algebra', 'intuition', 'group-theory', 'lie-groups']"
3941037,How to use stars and bars to count how many terms there are in a polynomial expansion?,"So here's what I understand from this. The number of terms in the expansion (before combining like terms) of any polynomial is given by ${k^{n}}$ where ${k}$ is the number of variables and ${n}$ is the degree of the polynomial. For example, ${(x + y + z)^4}$ is given by ${3^{4}=81}$ . To find the number of terms after combining, I can use the binomial theorem. This gives me ${n + 1}$ terms for an ${n}$ th-degree binomial. For example, ${(x+y)^{12}}$ has 13 terms. But the thing is, this doesn't work with trinomials and higher. My teacher instructed me to use the stars and bars method to find terms for such problems. I understand how to plug values in but don't quite understand how it works. For example, the number of terms in the trinomial ${(x+y+z)^{7}}$ is given by ${n + k - 1 \choose k - 1 }={9 \choose 2} = {36}$ . How exactly does this work? It seems relatively simple if you use the whole ""distribute ${n}$ pieces of candies among ${k}$ number of children"" type of problems, but I don't quite get it for this specific case.","['combinatorics', 'discrete-mathematics']"
3941124,Almost sure convergence of sum of independent Bernoullis,"Let $X_n$ be independent random variables where $X_n \sim Bernoulli(\frac{1}{n})$ . I'd like to show that $Y_n = \frac{1}{\log(n)}\sum_n{X_n}$ converges to $1$ almost surely. My idea is to first show that $Y_n$ converges to $1$ in probability, and then to use Kolmogorov's convergence criterion to show that the convergence is almost sure as well. However, I'm running in problems even in showing the convergence in probability. I'm trying to use the bound $\log(n) + \frac{1}{n} \leq \sum_{j=1}^{n}{\frac{1}{j}} \leq \log(n) + 1$ in tandem with Markov's inequality, but the absolute value in the Markov inequality is killing me. $P(|Y_n - 1| \geq \epsilon) \leq \frac{\mathbb E[|Y_n - 1|]}{\epsilon} \leq \frac{\mathbb E[Y_n] + 1}{\epsilon}$ , but of course this bound is not good enough. If I didn't have to worry about the absolute values, I would be able to use the upper bound from above, but I'm not sure how to proceed at this point. Does this look like a logical approach? I think that the second half of the proof might be a bit easier, since Kolmogorov's criterion just needs me to check that the sums of the means and variances converge...","['probability-theory', 'probability', 'random-variables']"
3941139,Which groups are the unit group of some integral domain?,"Similar to this question, I wonder if there is some classification of all the groups which are the unit groups of some integral domain. Since the question about rings is hard, I assume this is hard too, however perhaps the limitation to integral domains makes the question easier somehow (but if it does I don't see it).","['ring-theory', 'group-theory', 'abstract-algebra']"
3941274,"Approximation of $f(x, y)$ by $\sum_{n=1}^N c_n \chi_{A_n}(x) \chi_{B_n}(y)$","I believe the following statement is true, but I cannot prove it.
I would be grateful for your advice. Let $(X, \mathcal{A}, \mu)$ and $(Y, \mathcal{B}, \nu)$ be $\sigma$ -finite measure spaces, and let $(X \times Y, \mathcal{A} \times \mathcal{B}, \mu \times \nu)$ be product measure space. Define \begin{equation}
D := \left \{\sum_{n=1}^N c_n \chi_{A_n \times B_n} \mid N \in \mathbb{Z}_+ , c_n \in \mathbb{C}, A_n \in \mathcal{A} , B_n \in \mathcal{B}\right \}.
\end{equation} Then, $D$ is dense in $L^p(X \times Y; \mathbb{C})$ for all $p \in [1, \infty)$ .","['measure-theory', 'real-analysis', 'functions', 'lp-spaces', 'functional-analysis']"
3941363,A vectorial version of the div/curl lemma with divergence free potential.,"Introduction Let $Q \subset \mathbb{R}^3$ a bounded connected regular domain. Lets denote $A$ and $B$ the following spaces : $A$ the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ , noted $G$ , such as there exists $u \in (H^1_0(Q))^3, \ div \ u =0$ in $Q$ such as $G=\nabla u$ (i.e $G$ derives from a divergence free potential). $B$ the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ , noted $F$ , that verifies $$ \int_Q F:\nabla \varphi  \ \mathrm{d} x=0, \quad \forall \varphi \in (H^1_0(Q))^3, \quad div \ \varphi =0 \text{ in } Q$$ One can verify easily that the following orthogonal decomposition for the space of $3 \times 3$ function matrix with coefficient valued in $L^2(Q)$ : $$\mathcal{M}_3(L^2(Q))= A \  \oplus^\perp B$$ with the scalar product $<F|G>=\int_Q F:G \ \mathrm{d}x$ with the symbol $"":""$ being the usual scalar product for matrices : $C:D=\sum_{1 \leq i,j \leq 3} c_{ij} d_{ij}$ . ( $A$ is indeed closed in $\mathcal{M}_3(L^2(Q))$ using Poincaré inequality...) My question Let's take $F_n$ and $G_n$ two sequences in $A$ , respectivly $B$ . We assume that - $F_n$ weakly converges toward $G_0$ in $\mathcal{M}_3(L^2(Q))$ (i.e $\int_Q G_n :u \underset{n \rightarrow +\infty}{\longrightarrow} \int_Q G_0:u$ for any $u$ in $\mathcal{M}_3(L^2(Q))$ ) - $G_n$ weakly converges toward $G_0$ in $\mathcal{M}_3(L^2(Q))$ Now, for a fixed function $\varphi \in C^\infty_0(Q))$ , I am looking at the following quantity : $$\int_Q \varphi F_n: G_n \ \mathrm{d} x$$ and I would like to show that it converges toward $$\int_Q \varphi F_0: G_0\ \mathrm{d} x .$$ This look look exactly like a div/curl formula with a twist on the hypothesis through the divergence free potential. I have tried to prove it the same way using : $$\int_Q  \varphi F_n : \nabla u_n \ \mathrm{d} x = \int_Q F_n : \nabla (u_n \varphi) \ \mathrm{d} x - \int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x$$ where $G_n=\nabla u_n$ . The last term converges thanks to Rellich injection that gives me that $u_n$ converges strongly in $(L^2(Q))^3$ , and therefore I have a scalar product between a weak converging term and a strong converging term : $$\int_Q F_n : (u_n \nabla \varphi^t) \ \mathrm{d} x \underset{n \rightarrow + \infty}{\longrightarrow}  \int_Q F_0 : (u_0 \nabla \varphi^t) \ \mathrm{d} x.$$ However, for the first integral, I can't conclude anything since $div(u \varphi)$ is not $0$ , and so I can't use that $F_n$ belongs to $B$ ... Any ideas are welcomed. I was thinking of maybe using the Leray Projector $P$ that projects a function of $L^2(Q)$ on the space of divergence free $L^2(Q)$ functions., but then I need to be able to control the following quantity : $$\int_Q F_n : \nabla (P (u_n \varphi) - u_n \varphi) \ \mathrm{d} x$$","['divergence-operator', 'functional-analysis', 'partial-differential-equations']"
3941369,Show that $\| y\| =1$ and $\| y-x_j\|\geq \| x_j\|$,"I have an idea to solve this exercise but I am confused. The exercise says: Let $X$ be an infinite dimensional normed vector space over $\mathbb{K}$ , where $\mathbb{K}=\mathbb{R}$ or $\mathbb{C}$ Let $x_1, x_2, \dots ,x_n\in X$ . Show that there exists $y\in X$ such that $\| y\| =1$ and $\| y-x_j\|\geq \| x_j\|$ for all $j=1,2, \dots ,n$ .
We know that if $0≠x\in X$ , then there exists $f\in X^*$ such that $\|f\|=1$ and $f(x)=\mid \mid x\mid \mid$ . I was thinking we can show that $f(y)=1$ for some $y\in X$ then we are done. Or else I am thinking that we can say $\|f(y)\|= \| \| y\| \|=\|y\|=\|f\|\|y\|\Leftrightarrow \|y\|=\frac{\|f(y)\|}{\|f\|}$ , and probably try to show that $\frac{\|f(y)\|}{\|f\|}=1$ , but I don't know how I would do that. Thank you","['vector-spaces', 'functional-analysis']"
3941421,Cardinal of measures,"Let $\mathscr{X}=([0,1],\mathcal{B},\mu)$ be a measure space, and $\mathcal{B}$ is the Borel $\sigma$ -algebra with the subspace topology of the euclidean real line. We will look at $A:=\{f\mid f:\mathcal{P}([0,1])\rightarrow [0,\infty]\}$ , $B:=\{f\in A \mid f$ is $\sigma$ -additive on the Borel $\sigma$ -algebra $\}$ , $C:=\{f\in A \mid f(\emptyset)=0\}$ , and $D:=\{f\in A \mid \mathscr{Y}=([0,1],\mathcal{B},f)$ is a measure space $\}$ . Of course $C\cap B=D$ , and $A, B, C, D \neq \emptyset$ because Lebesgue measure $\in A, B, C, D$ . In particular, $A\cap B\cap C\cap D\neq\emptyset$ . Of course $\mid C\cap B \mid \leqslant \aleph^{(2^\aleph)}$ because $C\cap B\subset A$ . My question: $\mid C\cap B \mid=\mid D \mid=?$ Thanks for helping.","['elementary-set-theory', 'measure-theory']"
3941422,Do you know this finitely presented group on two generators?,"I computed using Sage the fundamental group of some topological space and got the infinite group $$\langle a, b\mid aba^{-1}ba\rangle.$$ By the change of variables $x=b^{-1}$ and $y=a$ , it can also be written as $$\langle x, y \mid xy=y^2x^{-1}\rangle.$$ Do you know if this group has a name ? Do you know if there exists a table of finitely presented groups of small ""complexity"" like this one ?","['group-presentation', 'infinite-groups', 'combinatorial-group-theory', 'finitely-generated', 'group-theory']"
3941441,A problem in limits to show that $\lim_{n\to\infty}$ $p_n$/$q_n$ = $π/4$,"Define two sequences of real numbers $p_n$ and $q_n$ with $n = −1, 0, 1, 2, \ldots$ as follows: $$p_n = 2p_{n−1} + (2n−1)^2p_{n−2}$$ and $$q_n = 2q_{n−1} + (2n−1)^2q_{n−2}$$ for every $n \geq 1$ , and starting with $p_{−1}= 0, q_{−1}= 1, p_0 = q_0 = 1$ .
Show that $$\lim_{n \to \infty}\frac{p_n}{q_n} = \frac{\pi}{4}.$$ I did:
Suppose $a_n$ is a sequence that satisfies $a_n = 2a_{n−1} + (2n − 1)^2a_{n−2}$ for all $n\geqslant1$ . Subtracting $(2n + 1)a_{n−1}$ from both sides we derive a simple recursion for a new
sequence $b_n = a_n − (2n + 1)a_{n−1}$ , namely $b_n = −(2n − 1)b_{n−1}$ . What to do after this... Is it correct?
If not kindly provide a detailed answer in the answer section.. As it helps a new beginner like me to understand who has just started to solve advanced problems.","['recursion', 'calculus', 'combinatorics', 'limits', 'algebra-precalculus']"
3941443,Some confusion about the proof for Darboux's Theorem,"I'm having some confusion in the proof of Darboux's Theorem. It appears similar questions have been asked before, but I'm still confused by the replies, so I thought I would ask my own. Here is my proof.
Let $g(x) = f(x) - \gamma x$ Assume $f'(a) < f'(b)$ w.l.o.g. We know $f'(a) < \gamma < f'(b)$ by hypothesis. So, $f'(a) - \gamma = g'(a) < 0$ and $f'(b) - \gamma = g'(b) > 0$ Since $g'(a) < 0$ and $g'(b) > 0$ , (opposite signs) we know $\exists c$ such that $g'(c) = 0$ That step right there is my confusion. I am basically using the IVT to claim there is a value in between. However, to use the IVT, the function has to be continuous. That is not an assumption in the problem, only that $f$ is continuous. I've found this question asked a couple of times, but the common reply seems to be that the derivative need not be continuous to have the intermediate value property because of Darboux's Theorem. But I am trying to prove Darboux's Theorem! So while I believe that fact, I can't use the theorem within its proof. I cannot seem to justify that step in the event that $g'$ is discontinuous. I have been told there is another version of the proof combining the MVT and IVT. However, I've found it online in a few places, and I'm having a hard time following it. So I am trying to figure out how to do it this way since I don't understand the other way. Can someone explain to me why I can use the IVT without the derivative being continnuous?","['continuity', 'proof-explanation', 'derivatives', 'real-analysis']"
3941589,Difficult implications of a detail in an(other) approach to the simultaneous Pell equations $24a^2+1=t^2$ and $48a^2+1=u^2$,"In reviewing & improving my attempt to solve the question on simultaneous Pell-equations (see here in MSE )
I came across a detail which has surely wider implications and which I cannot encompass (correct term?) so far. The original question is about: How many solutions (and which) have the simultaneous Pell equations $$ 24a^2+1=t^2 \\ 48a^2+1=u^2 \tag 1
$$ My ansatz, first part: I approached this problem the following way. First look at the difference equation: $$ 24a^2=u^2-t^2 \tag 2 $$ and use the method of parametrization of the pythagorean triples for $b^2 = c^2-a^2$ [which comes out to be $(a,b,c)=(n^2-m^2,2nm, n^2+m^2)$ (wikipedia)] such that I arrive at $$ u=2m^2+3n^2 \\ t=2m^2-3n^2 \\ a= nm \tag 3
$$ Expansion shows that it is indeed a solution for (2) $$ 24m^2n^2 \overset{?}= (2m^2+3n^2)^2-(2m^2-3n^2)^2 = 12m^2n^2- (-12m^2n^2) = 24m^2n^2 \tag 4
$$ But while this shows solutions for the difference -eq (2) it is easy to see, that this parametric solution for $(a,b,c)$ cannot at the same time be a solution for the first equation $$   24 a^2+1 = t^2 \tag {5a}
$$ which leads to $$ 24(n^2m^2)  =(2m^2-3n^2)^2 -1 \tag {5b}$$ This has no solution which can be seen when taken $\pmod 5$ . $\qquad \qquad \qquad $ update2: this is corrected from previous version after helpful comment of user servaes, see revision history So my conclusion on my scribble notes was first: no solutions, simply :-)) ... But actually there are known solutions for the simultaneous system, namely, $(a,t,u)=(0,\pm1,\pm1)$ and $(\pm1,\pm5,\pm7)$ and thus it looks, as if my approach is useless here and should be discarded. My ansatz, second part: In a second look on it, after some coffee,  I see that this ansatz can be revived, if we allow non-integer values for $(m,n)$ . For instance $(m,n)=(\sqrt 3,\frac1{\sqrt 3})$ gives $(a,t,u)=(\frac {\sqrt 3}{\sqrt 3},2\cdot 3-\frac33, 2\cdot 3+ \frac33 )=(1,5,7)$ . Analoguously $(m,n)=(\frac1{\sqrt 2},\sqrt 2,)$ gives $(a,t,u)=(\frac {\sqrt 2}{\sqrt 2},\frac22-3\cdot2, \frac22+ 3 \cdot2)=(1,-5,7)$ . In this example, the values $\sqrt3$ and $\sqrt 2$ are quite obvious when eq's (3) are considered. But to allow irrational numbers in that well known and such commonly used parametrization of the pythagorean triples shall likely have many more implications- but which I do not see here, even if we uset the restriction that the irrational values have to be selected in a way that $(a,t,u)$ are still all integer. This is surely of importance, if I want to use the full logic of my small approach for other, but comparable, cases like $Aa^2+1=t^2 ; Ba^2+1=u^2$ . Q1: can the introduction of that irrational values be understood as meaningful generalization of the Pythagorean-triple parametrization? Q2: under the assumption of quadratic irrational values for $(n,m)$ - can we still prove, that the set of solutions is that set of the two which are already known?","['number-theory', 'pell-type-equations', 'diophantine-equations']"
3941660,Confused: using Taylor series to find derivative,"TL;DR: read bolded parts Lets say I have f(x) = sin(x^2) and I want the f''''''(0) (6th derivative). Using taylor series, this is really simple. We plug in x^2 into the taylor polynomial of sin(x), and get this: Then the 6th derivative is 1/3! * 6! = 120. I am confused because taylor series seems really unrelated; there should be an equally easy way to do this just with derivatives and chain rule (no detour to taylor series) . But when I bash it out, I don't get a simple solution. (120 on the last line, typo) Why does taylor series come up in finding derivatives?","['calculus', 'derivatives', 'taylor-expansion']"
3941675,Using trigonometric substitution to integrate $\int\frac{x^3dx}{\sqrt{25-x^2}}$,"I tried solving this calculus problem so many times but I never got an answer that at least looked similar to the choices in the image. Can someone help?
I don't understand where the Sine came from. This is the answer I get $$\frac13(25-x^2)^{3/2} - 25\sqrt{25-x^2} + C $$ Edits are appreciated","['calculus', 'trigonometry', 'trigonometric-integrals']"
3941707,Calculate the determinant of the following $n \times n$ symmetric matrix: [duplicate],"This question already has answers here : Determinant of an $n\times n$ Toeplitz matrix (2 answers) Closed 3 years ago . Let $n$ be a positive integer and $A = (a_{ij})_{n\times n}$ , where $a_{ij} = |i-j|$ , for $i = 1, 2, \dots, n$ and $j = 1, 2, \dots, n$ . Calculate $\det A$ . I noticed that $a_{ii} = 0$ and $a_{ij} = a_{ji}$ , so A is a symmetric matrix. Also, I saw that, if we make the notation $A_n$ for the A with n elements, $A_n$ is constructed from $A_{n-1}$ with $n-1, n-2, \dots, 0$ as elements for the last line and last column. I tried to use Laplace expansion but with no result. This is how $A_n$ looks like: $A_n=\begin{bmatrix}
0&1&2& .&.&. &n-1\\
1&0&1&2& .&.&n-2 \\
2&1&0&1&.&.&. \\
.&.&.&.&.&.&. \\
.&.&.&.&.&.&2 \\
.&.&.&.&.&.&1 \\
n-1&n-2&.&.&2&1&0
\end{bmatrix}$ I calculated for a few small numers: $\det A_1 = 0$ , $\det A_2 = -1$ , $\det A_3 = 4$ , $\det A_4 = -12$ , $\det A_5 = 32$ , but I didn't figure out a rule such that I could find the determinant through induction. Can you help me on this one?","['matrices', 'determinant', 'linear-algebra', 'symmetric-matrices']"
3941710,Two Intuition Questions: Conservative VF and Dot Product,"All, In doing multivariable calculus, I have encountered line integrals. I have no problem calculating these, but I do have a question about the intuition behind two ideas (one of which, dot product, is early on in MV calculus). First, the role of gradients in a conservative vector field. I do understand how to determine if a vector field is conservative: it is the gradient of a potential function. And I can do the math from there. But what I don't understand is the intuition behind this. What does the gradient have to do with the fact that I don't have to calculate a curve in the conservative vector field? I'm having a hard time understanding the WHY even though I get the HOW. Second, on dot products. These are easy to do, and I understand that you need to dot product the force and the dr before taking the integral. But, again, why? The dot product, I guess, shows the relationship between two vectors - if it is zero, the vectors are orthogonal; and the dot product increases the closer the vectors get to parallel. But what is the intuition behind why the dot product can be used to determine work, i.e., w = F * d. Again, I can solve this, but I don't understand the intuition. Thanks.","['integration', 'multivariable-calculus', 'intuition']"
3941715,"extrapolate 3-vector field in $(0,1)^3$ from boundary vector fields","I'm working with the metric $ds^2=\frac{dxdy}{xy}$ and this applies to the vector field $\vec V.$ Also working with a semi-Riemannian manifold. I have a vector field $\vec V=\langle x\log x, -y\log y\rangle$ on the $x-y$ plane for $x,y\in (0,1).$ Assume $\vec V$ to be the projection onto $(x,y,0)$ . I've found that $f(x,y)=\big(\frac{x^2}{2}\log x - \frac{x^2}{4} - \frac{y^2}{2}\log y + \frac{y^2}{4} + C\big).$ So $\textbf{Grad}\big(f(x,y)\big)=\textbf{Grad}\big(\frac{x^2}{2}\log x - \frac{x^2}{4} - \frac{y^2}{2}\log y + \frac{y^2}{4} + C\big)=\big\langle x\log x,-y \log y \big\rangle.$ Now here is $\vec V$ sitting in the $x-y$ plane, with another ""copy"" sitting on the plane sliced by $z=1.$ Here, two more ""copies"" are added. I left out the last two copies because it would make the drawing too confusing. How can one reconstruct a (possibly unique) vector field in $(0,1)^3$ based on the boundary vector fields? Just to make it clear the 3-vector field in $(0,1)^3$ projected onto the faces of the cube should yield $\vec V$ up to congruence. My thought is to find the surfaces whose gradients equal the copies of $\vec V$ on the boundary. But epistemologically, this doesn't get me any closer to finding the vector field in 3-space. I think I need a new idea to solve the problem. Does anyone have any idea how to do this or is it not possible from the given information? My constraints are: $1)$ All the boundary vector fields must be Killing (they preserve the metric(s)) $3)$ The integral curves of each boundary vector fields must be analytic. $4)$ The boundary vector fields and the interior vector field must be bounded. Here is the shape that I think should resemble the interior 3-vector field:","['vector-fields', 'semi-riemannian-geometry', 'geometry', 'vector-analysis', 'differential-geometry']"
3941730,Turn a maximal preflow into a maximal flow in $O(mn)$,"I have to prove that you can turn a maximal preflow (so a preflow, such that the excess of our sink $t$ is maximal) into a maximal flow in $O(mn)$ time. My first idea was, to take an active node $v$ (which means a node which has positve excess) and randomly decrease the costs of the incoming edges of $v$ , until the excess equals $0$ . We do that, until there are no active nodes left. However, I am not sure if that already creates a maximal flow. This procedure would be in $O(mn)$ , since we iterate over a maximum of $n$ nodes (where $n := |V(G)|$ ), and in the worst case, every cost of an edge has to be reduced. The problem is, that I don't quite see, why the resulting flow should be maximal. It would be maximal, if there were no more paths from the source $s$ to the sink $t$ , but why is this the case here? Maybe, the costs of the edges have to be reduced in a specific way instead of randomly? Can someone give me some advice? Thanks in advance! Edit: I read something about the Push-Relabel algorithm which basically functions very similar. A better idea would be, to first turn our preflow to an acyclic preflow. This way, we can get a topological order of all nodes, lets say $\{v_1, ... v_n\}$ , with $v_1 := t$ and $v_n := s$ . Now, we go from $v_2$ to $v_{n-1}$ in the topological order, and check, whether the excess is positive. If it is positive, then we just decrease the incoming flow by reducing the flow on the incoming edges (randomly), until the excess is $0$ . This way, only the excess of the next nodes can be affected, so by iterating over all nodes $v_2$ to $v_{n-1}$ , the end result will be a flow $f'$ . This flow also has the same value as of our preflow, since the incoming edges to the node $t$ didn't change. Since $f$ was a maximal preflow, it follows that $f'$ must be a maximal flow as well. This sounds like a promising idea, sadly, I do not know how to turn our preflow acyclic. I mean, finding a cycle is possible, but how do I know, which edges can be deleted? It has to be possible in $O(nm)$ , since the other steps are in $O(nm)$ as well.","['graph-theory', 'network-flow', 'discrete-mathematics']"
3941748,Expressing hyperelliptic integrals through series,"Suppose I want to evaluate the following integral, but non-numerically : $$\int_1^2\frac1{\sqrt{x^5-x+1}}\,dx$$ This is, of course, a hyperelliptic integral, which cannot in general be expressed in terms of elementary (or even elliptic) functions. Furthermore, it is quite well-known that the denominator polynomial here is not even solvable in radicals, but just assume we know the roots. (It is far easier to numerically find polynomial roots than to numerically integrate.) Byrd and Friedman have this to say about the matter: For the evaluation of hyperelliptic integrals, one must usually resort to direct numerical integration or to the use of complicated series expansions. This second part has me intrigued. How can I express the integral above using series, hypergeometric or otherwise? Is there a general procedure to derive such a series representation? Note that if there were just two terms in the polynomial, the integrand would be expressible as a binomial series; the integral itself would then be expressible using the $_2F_1$ function. But there are three terms here, so $_2F_1$ cannot be immediately applied. Taylor and Padé expansions are not quite desirable either, since they require much work at extended precision and will not converge over the entire domain if truncated.","['integration', 'definite-integrals', 'differential-geometry', 'calculus', 'hypergeometric-function']"
3941754,"How to show that $\max(a,b)^2+\max(a,b)+a-b$ is bijective?","How can I show that $f:\mathbb{N}^2 \to \mathbb{N}$ given by $$f(a,b)=\max(a,b)^2+\max(a,b)+a-b$$ is bijective?
How could I show it to be injective and surjective? Or does it suffice to give a explicit inverse function? If so, what is the process of inversing this function?",['functions']
3941773,Stopping the Coronavirus puzzle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question A square region $2020 \times 2020 \text{ km}^2$ divided into $2020^2$ cells. Some cells are contaminated by covid-19 . Every week the virus spread to those cell which have at least $2$ side in common with contaminated cells. Find the maximum number of contaminated cells such that no matter where they are located the covid-19 pandemic will not spread to the entire region. My school friend gave me this problem( better to say a puzzle) may be during the lockdown period(July-August) but I forgot it and yesterday he asked me if I have been able to solve the problem or not? And then the answer was obviously not, although I put a sufficient effort behind the problem that time and after the meet yesterday and also today I gave a lots of time but unable to figure it out. Thanks for your attention!","['recreational-mathematics', 'puzzle', 'combinatorics']"
3941803,How do we know that the Sine function has no Non-Real Roots?,"In this question and answers ( How was Euler able to create an infinite product for sinc by using its roots? ) we use the fact that the real roots of $f(x)=\sin x$ occur when $x$ is an integer multiple of $\pi$ to obtain an infinite product for $\sin x$ in terms of of its factors. My question is, how do we know that $f(x)=\sin x$ has no non-real roots? Thanks in advance.","['proof-explanation', 'roots', 'factoring', 'trigonometry', 'complex-numbers']"
3941804,Calculate the following limit:,"Calculate $\;\lim\limits_{x\to\infty} \left[x^2\left(1+\dfrac1x\right)^x-ex^3\ln\left(1+\dfrac1x\right)\right]$ . It is a $\frac00$ case of indetermination if we rewrite as $\lim_{x\to\infty} \frac{((1+\frac1x)^x-e\ln(1+\frac1x)^x)}{\frac{1}{x^2}}$ , since $\lim_{x\to\infty} \frac{1}{x^2} = 0$ , $\lim_{x\to\infty} (1+\frac1x)^x = e$ and $\lim_{x\to\infty} \ln(1+\frac1x)^x = 1$ . I think that it is the type that has a solution without l'Hospital's rule, but it's quite difficult to find, so l'Hospital still remains the best try to me. I tried using it with different rewrites, but it seems that it needs to be used multiple times, and the expression gets harder and harder to calculate, so I assume that some other limit must be applied first to make the expression nicer. Also, I futilely tried to use the following known limits by changing $x$ into $y = \frac1x$ if needed (and adding and substracting $ex^2$ in the main parenthesis and trying to use the last 2 limits), but maybe it can help you: $\lim_{x\to0} \frac{a^x-1}{x} = \ln a$ , $\lim_{x\to0} \frac{\ln(1+x)}{x} = 1$ , $\lim_{x\to0} \frac{(1+x)^r-1}{x} = r$ , $\lim_{x\to0} \frac{(1+x)^\frac1x-e}{x} = -\frac{e}{2}$ , $\lim_{x\to\infty} (x-x^2ln(1+\frac1x)) = \frac12$ . Can you help me with this problem?",['limits']
3941829,Evaluating $\sum_{k=0}^{2020} \cos( \frac{2πk}{2021}$),"I am trying to come up with a solution to $$\sum_{k=0}^{2020}\cos\left(\frac{2\pi k}{2021}\right) $$ so far I have proceeded as to acknowledge the cosine is just the real part of the Euler's formula form: $$ \sum_{k=0}^{2020}\cos\left(\frac{2\pi k}{2021}\right)=\textrm{Re}\left(\sum_{k=0}^{2020}e^{2\pi ik/2021}\right) 
 $$ and so, rephrasing since it is basically a geometrical series (for all k>1): $$\sum_{k=0}^{2020}e^{2\pi ik/2021}=\frac{1-e^{2021(2\pi i)/2021}}{1-e^{2\pi ik/2021}}
 $$ now, simplifying $$ {e^{2021(2\pi i)/2021}} = {e^{2\pi i}}  $$ and using, that $$e^{2\pi i}=\cos\left(2\pi\right)+i\sin\left(2\pi\right)=1 $$ the result will be $$\sum_{k=0}^{2020}e^{2\pi ik/2021}=\frac{1-1}{1-e^{2\pi ik/2021}} = \frac{0}{1-e^{2\pi ik/2021}}= 0  $$ Did I happen to make a mistake anywhere, or should this be correct? I have seen other people solve this problem with the formula for sine and cosine sums in arithmetic progression, however, I am not allowed to use that formula since it was never introduced in our course. Any help would be very appreciated :)","['calculus', 'trigonometry', 'summation', 'real-analysis']"
3941894,Probability Q on rolling two 8-sided dice,"Let's say I roll two 8-sided dice. I win if the sums '7' and '11' show up before we see the sum '9' TWICE. What is my probability of winning? So this is my answer and please correct me if I am wrong: Prob of getting either 7 or 9 —> 6/64 + 6/64 = 12/64 —> 18.75% Prob of getting 9 twice —> 8/64 x 8/64 = 1/64 —> 1.6% But now to find the probability of winning, would it be 18.75 - 1.6 = 17.15% ? Am I right or wrong? Thanks! EDIT From the answers provided below, we have three different results: 84%
49%
60% Which one would be the correct answer?","['dice', 'probability']"
3941929,Existence of $R>0$ such that ${\int_{|z|=R}{f'(z)\over f(z)}dz}=0$,"I'm trying to solve the following problem: Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a meromorphic function such that $f({1\over z})$ is analytic at $z=0$ and $\displaystyle{\lim_{z\rightarrow\infty}f(z)}=0$ . Show that there exists $R>0$ such that $\displaystyle{\int_{|z|=R}{f'(z)\over f(z)}dz}=0$ . If I find a $R>0$ such that $f(z)$ has the same finite number of zeros and poles in $B(0,R)$ , then by the Argument Theorem I would have that integral is zero, but how to know that such $R$ exists? I would appreciate any hint.",['complex-analysis']
3942008,Differentiablity of a function $f(z)$ in the complex vs reals,"I am quite confused about the differentiability of a function in the complex plane vs the real numbers.
Consider the function $f(z) = |z|^2$ for $z \in \mathbb{C}$ . Using the definition of a derivative, and taking the limit as $\Delta z \rightarrow 0$ , we can see that it is only differentiable at $z = 0$ . Elsewhere, the limits are not unique, so do not exist. However, if we consider a similar function $F(x) = |x|^2$ for $x \in \mathbb{R}$ , then $F'(x) = 2x$ and exists everywhere in $\mathbb{R}$ . We know that $\mathbb{C}$ is a field extension of $\mathbb{R}$ , so $\mathbb{R} \subset \mathbb{C}$ . The domain of $f(x)$ is then a subset of the domain of $f(z)$ . Why isn't $f(z)$ instead differentiable for all $z = x + i0$ ? Likewise for a similar function, $g(z) = \operatorname{Re}(z)$ . $g(z)$ is nowhere differentiable in $\mathbb{C}$ , yet $G(x) = x$ is differentiable everywhere in $\mathbb{R}$ . Why is this the case? Is it due to the limit being used in the definition of a derivative? That the limit requires that $|f(z) - f(z_0)| < \epsilon$ , which means that the limit of $f(z)$ must approach $f(z_0)$ , as $z \rightarrow z_0$ , independent of the direction for the limit to exist. If we were only working with the reals, the limit as $x \rightarrow x_0^+$ must equal that of $x \rightarrow x_0^-$ for the limit to exist, but in the complex plane, the limit must be equal as $z \rightarrow z_0$ from all sides of $z_0$ , which is why the derivative for a function in $\mathbb{R}$ can exist, but not a similar one in $\mathbb{C}$ ? Does this mean that given two similar functions $f(z)$ and $F(x)$ acting on both $\mathbb{C}$ and $\mathbb{R}$ , respectively. Let $f(z=x+iy) = u(x,y) + iv(x,y)$ , $u(x,y) = F(x)$ , and $z_0 = x_0 + iy_0$ , then $$f(z) \text{ differentiable at point } z_0 \in \mathbb{C} \text{ and } u'(z) \text{ exists at } z_0 \implies F(x) \text{ differentiable at } 
x_0 \in \mathbb{R}?$$","['complex-analysis', 'derivatives', 'real-analysis']"
3944475,Counterexample regarding basic properties of limits,"Let $f, g$ be two functions defined on a deleted neighbourhood of $a ∈ R$ . If $g(x) \neq 0$ for every x on the deleted neighbourhood and $$\lim_{x\to a}{f(x)\over g(x)} = 1$$ then $$\lim_{x\to a}{(f(x) - g(x))} = 0$$ I'm asked to prove or disprove this claim, along with others. My instinct in these type of questions is to start figuring out if there are any piecewise function that contradict the statements but I almost feel like I'm searching in the dark with no clear strategy.. is there an agreed upon process regarding searching for counter examples?","['limits', 'proof-writing', 'examples-counterexamples']"
3944546,Toss 100 fair coins and take away the tails; toss the remaining coins and take away the tails. Continue until no coins remain. [duplicate],"This question already has answers here : Expectation of the maximum of i.i.d. geometric random variables (3 answers) Closed 3 years ago . 100 participants have a fair coin each, on a given round, the not already discarded participants flip their coins, those who flip a tail are discarded from the game, the remaining ones continue to play until nobody is left (everyone has been discarded). What would be the average number of trials (where each trial consists of a tossing and removing the tails) one would expect from doing this experiment? Does conditional expectation work for something like this? I know that each individual coin follows a Geometric distribution, but I am trying to figure out the sum of them to determine the average number of trials for a game like this. My Logic/Thought Process: I started out trying to think of the probability that a particular coin makes it to round $r$ which is $\frac{1}{2^m}$ . I then realized that each coin outcome can be modeled by a Geometric random variables with $p = 0.5$ . I am just now unsure how to take the leap from this single case to a case with 100 coins. I presume it has to do with summing the geometric random variables, but I am not sure.","['expected-value', 'statistics', 'combinatorics', 'probability']"
3944574,An identity for $\binom{-1/2}{n}$,"I would like to show the formula $$\forall n \in \mathbb{N},\quad\sum_{p=0}^n \binom{n}{p} \frac{(-1)^p}{p!} \prod_{k=0}^{p-1} \left(n+\frac{1}{2} + k\right) =  \binom{-1/2}{n} \, , $$ which is true according to Mathematica. I tried to use the identity $$\binom{-1/2}{n} = \frac{1}{n!} \sum_{k=0}^n s_{n,k}\left(-\frac{1}{2}\right)^k \, ,$$ where $s_{n,k}$ denotes the Stirling numbers of first kind, but without any success. We end up with the following identity to prove $$ \frac{1}{n!} \sum_{p=k}^n \binom{n}{p}\frac{s_{p,k}}{p!} \left(n+\frac{1}{2}\right)^k = s_{n,k} \, , $$ which is not a piece of cake for me neither. Among all the formulas I could find in Concrete Mathematics book from Ronald Graham, Donald Knuth, and Oren Patashnik, none comes in a handy. I confess I don't have a large culture of this field. I will be thankfull for any hint.","['summation', 'combinatorics', 'stirling-numbers']"
3944597,Let $f(x)=x^6-2x^3-8$ and $g(x)=x^2+2x+4$. Let $a_1$ through $a_6$ be its roots. Find the value of $\prod_{n=1}^{6} (g(a_n))$,"Question: Let $f(x)=x^6-2x^3-8$ and $g(x)=x^2+2x+4$ . Let $a_1$ through $a_6$ be its roots. Find the value of $\displaystyle \prod_{n=1}^{6} (g(a_n))$ My process: I first thought of setting $z=a_i^2+2a_i+4$ and then putting the value back into $f(z)$ and then from there finding the product using Vieta's. But the process turned incredibly long and tedious and I found it to be impossible to do by hand. So I just went to to the brute force method, expanding the whole expression out. Writing all the terms would be really tedious so I would like to abbreviate it. Let $P= a_1 \cdot a_2 \cdots \cdot a_6$ and let $e_n$ denote the sum of roots taken $n$ at a time. Here is the expression I got: $$\prod_{i=1}^{6} (a_i^2+2i+4)=P^2+2e_5P+2^2e_4P+2^3e_3P+2^4e_2P+2^5e_1P+2^6P+2^5*4*e_5+2^4*4^2*e^4+2^3*4^3*e_3+2^2*4^4*e_2+2*4^5*e_1+4_6+4(\sum_{cyc}(a_1a_2a_3a_4a_5)^2))^+4^2(\sum_{cyc}(a_1a_2a_3a_4)^2)+4^3\sum_{cyc}(a_1a_2a_3)^2+4^4\sum_{cyc}(a_1a_2)^2+4^5\sum_{cyc}(a_1)^2$$ Which is a monster in its own right. However, I was extremely interested in the patterns that kept popping up. Like the $2e_5P+2^2e_4P+2^3e_3P+2^4e_2P+2^5e_1P+2^6P$ where the power of 2 and the index of $e$ keeps increasing and decreasing with each other. Therefore I wondered if there could be any general formula for any $f(x)$ and $g(x)$ . And that is my question, is there any way to find the answer efficiently without all this unnecessary calculation; and also can this process be generalised to any polynomials $f(x)$ and $g(x)$ . And if it cannot be generalised, then is there any sort of algorithm or methodical way that one can take while solving this type of problem?","['algebra-precalculus', 'functions', 'roots', 'polynomials']"
3944620,Maximum value of $\sin(A/2)+\sin(B/2)+\sin(C/2)$?,"So I came across a question in my textbook: In triangle ABC, if $A$ , $B$ , $C$ represent angles, then find the maximum value of $\sin(A/2)+\sin(B/2)+\sin(C/2)$ ? So I have already tried and best and put my blood,sweat and tears into this question..But I'm not able to go solve further! So here's my approach:
By Using $\sin(C)+\sin(D)$ and $A+B+C= \pi$ ; $2\sin(\frac{A+B}{4})\cos(\frac{A-B}{4})+\cos(\frac{A+B}{2})$ Now,
Using $\cos(2A)$ formula i.e, $1-2\sin^2(A)
$ $2\sin(\frac{A+B}{4})\cos(\frac{A-B}{4})+1-2\sin^2(\frac{A+B}{4})$ So I got quadratic in variable $\sin(\frac{A+B}{4})$ $-2\sin^2(\frac{A+B}{4})+2\sin(\frac{A+B}{4})\cos(\frac{A-B}{4})+1$ But I dunno what to do After that Can I solve this question using this method or I have to use a different approach! BTW, the answer is 3/2 Edit:I have just finished my high school and preparing for entrance exam IIT-JEE,So please don't use hard terms to solve this question. This solution is sent by my teacher, atleast make me understand this one [https://i.sstatic.net/51pCB.png]","['algebra-precalculus', 'trigonometry']"
3944659,Is this a characteristic function? Why (not)?,"I have recieved a home assignment and the question is if $(1-|t|)e^{-|t|}$ is a characteristic function. If it is a yes, then what distribution does it correspond to? I was wondering if that is a combination of convex characteristic functions, the Polya theorem sadly does not work. I would appreciate any ideas.","['characteristic-functions', 'probability-theory', 'probability']"
3944665,Number of possible states on a $6\times 6$ grid,"You have a 6x6 square grid. You have 18 black and 18 white pieces. How many ways can you fill the board (one piece per square) such that there are 3 black and 3 white pieces per column, and such that there are 3 black and 3 white pieces per row? I've tried to do it purely combinatorally , just looking row by row, but it gets confusing very fast. Is there perhaps some way to do it utilizing symmetries, like how a solution translated horizontally or vertically is a solution, or how the 90 degree rotation around the center of a solution is a solution?","['abstract-algebra', 'combinatorics', 'problem-solving']"
3944667,Locally Lipschitz with respect to a variable and uniformly respect the other,"Let: $\mathbf{f}:D\subseteq \mathbb{R}^{n+1} \to  \mathbb{R}^{n} \ \  \ D \ \text{open}$ $\  \ \ \ \ \ (t,\mathbf{y}) \mapsto \mathbf{f}(t,\mathbf{y})$ Where $t\in \mathbb{R}$ and $\mathbf{y} \in \mathbb{R}^{n}$ . $\mathbf{f}$ is locally Lipschitz in $D$ with respect to $\mathbf{y}$ and uniformly respect $t$ if and only if: $$\forall (t,\mathbf{y})\in D   \ \exists B_r(t,\mathbf{y}) : \exists L\in\mathbb{R}_+ : \ \forall (t,\mathbf{z}) \in B_r(t,\mathbf{y}) \\ ||\mathbf{f}(t,\mathbf{y})-\mathbf{f}(t,\mathbf{y})||\leq L||\mathbf{y}-\mathbf{z}||$$ Where $L$ doesn't depend upon $t$ . The following proof is left as exercise to the reader Let $\mathbf{f}(t,\mathbf{y})=(f_1(t,\mathbf{y}),...,f_n(t,\mathbf{y}))$ and $\mathbf{y}=(y_1,...,y_n)$ then: $$\mathbf{f}\text{ and }\frac{\partial f_j}{\partial y_s } \ \text{continous in } D \ \  \forall j,s\in\{1,...,n\} \implies \mathbf{f} \text{  is locally Lipschitz in }D\text{ with respect to }\mathbf{y}\text{ and uniformly respect }t $$ I'm trying to prove it. Here is my attempt: Lemma Since mean value theorem is not true for vectorial functions, my idea was to use this weaker inequality: $\mathbf{g}:A\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ \mathbf{g}\in C^1(A)$ $\text{The segment } [\mathbf{y},\mathbf{z}]\subset A$ $\implies ||\mathbf{g}(\mathbf{y})-\mathbf{g}(\mathbf{z})||\leq \sqrt{n}\alpha||\mathbf{y}-\mathbf{z}||$ Where $$\alpha=\max \left\{\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_1(\mathbf{x})||),\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_2(\mathbf{x})||),...,\max_{[\mathbf{y},\mathbf{z}]} (||\nabla g_n(\mathbf{x})||)\right\}$$ The existences of the maximum of the $||\nabla g_j||$ is granted by Weierstrass Theorem, since $[\mathbf{y},\mathbf{z}]$ is compact. Attempt of proof I can define a family of functions $\mathbf{g}_t: \mathbf{y} \mapsto \mathbf{f}(t,\mathbf{y})$ (I'll indicate $g_{t,j}$ its $j$ -th component). One can easily verify the following: $$ \nabla f_j(t_0,\mathbf{y}_0)=(D_t f_j(t_0,\mathbf{y}_0),\nabla g_{t_0,j}(\mathbf{y}_0)) \ \ \forall(t_0,\mathbf{y}_0)\in D $$ In fact: $$D_{y_s} f_j(t_0,\mathbf{y}_0)=\lim_{h \to 0} \frac{f_j(t_0,\mathbf{y}_0+h\mathbf{e}_s)-f_j(t_0,\mathbf{y}_0)}{h}=\lim_{h \to 0} \frac{g_{t_0,j}(\mathbf{y}_0+h\mathbf{e}_s)-g_{t_0,j}(\mathbf{y}_0)}{h}=D_{y_s} g_{t_0,j}(\mathbf{y}_0)$$ This means that the following implication holds: $$D_{y_s} f_j(t,\mathbf{y})\leq N \ \ \ \forall(t,\mathbf{y})\in K\subset D \implies ||\nabla g_{t,j}(\mathbf{y})||\leq M  \ \ \ \forall(t,\mathbf{y})\in K $$ In particular the LHS proposition is true for every $j$ (and this obviously makes true also RHS for every $j$ ) by Weierstrass theorem if I choose $K$ to be a closed ball of generic center $(\overline{t},\mathbf{\overline{y}})$ (this ball  exists for every choice of $(\overline{t},\mathbf{\overline{y}})$ , since $D$ is open). After this premises, since $D_{y_s} \mathbf{g}_t:t \mapsto D_{y_s} \mathbf{f}(t,\mathbf{y})  $ and thanks to our hyphothesis it's easy to see that: $\mathbf{g}_t:A_t\subseteq \mathbb{R}^n \to \mathbb{R}^n \ \ \ A_t=\{\mathbf{y}:(t,\mathbf{y})\in D \} $ $\mathbf{g}_t \in C^1(A_t)$ Moreover since a ball is connected $\forall (t,\mathbf{z})\in K \ \ $ the segment $[ (\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z})]\subseteq K $ and this implies that $[\mathbf{\overline{\mathbf{y}}},\mathbf{z}]\subseteq A_t$ (this can be easily proved showing that the pythagorean distance of $(\overline{t},\overline{\mathbf{y}})$ from $(t,q\overline{\mathbf{y}}+(1-q)\mathbf{z})$ with $q\in [0,1]$ is $\leq$ than the distance $d((\overline{t},\overline{\mathbf{y}}),(t,\mathbf{z}))$ ) , so by our lemma: $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}\alpha_t||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ If we prove that $\alpha_t$ is bounded we are done because we substitute it with its upper bound, so that the inequality doesn't depend upon $t$ .Notice that: $$\alpha_t=\max \left\{\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,1}(\mathbf{y})||),\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,2}(\mathbf{y})||),...,\max_{[\mathbf{\overline{y}},\mathbf{z}]} (||\nabla g_{t,n}(\mathbf{y})||)\right\}$$ Since we know that for every $j \ \ $ , $||\nabla g_{t,j}(\mathbf{y})||\leq M \ \ \forall (t,\mathbf{y})\in  K$ ,we have just to prove that $\forall \mathbf{y} \in [\mathbf{\overline{y}},\mathbf{z}], (t,\mathbf{y})\in K$ , this can be done particularly easily showing that $ d((t,\mathbf{y}),(\overline{t},\mathbf{\overline{y}}))\leq d((t,\mathbf{z}),(\overline{t},\mathbf{\overline{y}}))$ (it's straight forward after you use the fact that $\mathbf{y}$ is convex combination of $\mathbf{\overline{y}}$ and $\mathbf{z}$ ). In conclusion: $$\alpha_t \leq M$$ And this implies that: $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{g}_t(\mathbf{\overline{y}})-\mathbf{g}_t(\mathbf{z})||\leq \sqrt{n}M||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ By the definition of $\mathbf{g}_t$ and defining $L=M \sqrt{n}$ : $$\forall (t,\mathbf{z})\in K \ \ ||\mathbf{f}(t,\mathbf{\overline{y}})-\mathbf{f}(t,\mathbf{z})||\leq L||\mathbf{\mathbf{\overline{y}}}-\mathbf{z}||$$ Since $(\overline{t},\overline{\mathbf{y}})$ was a generic point of $D$ , this concludes the proof. Is it correct?","['partial-derivative', 'multivariable-calculus', 'solution-verification', 'lipschitz-functions']"
3944729,Group topology induced by group action,"Let $G$ be a group, $X$ a topological space and $G\times X \longrightarrow X$ a group action so that $\begin{align*}
X\longrightarrow X \\
x \mapsto g\cdot x
\end{align*}$ is a homeomorphism $\forall g \in G$ . Is there a coarsest topology on $G$ so that: $G$ is a topological group The group action is continuous ? I have showed that the topology induced by the maps $\begin{align*} G \longrightarrow X \\
g\mapsto g\cdot x\end{align*}$ and $\begin{align*} G \longrightarrow X \\
g\mapsto g^{-1}\cdot x\end{align*}$ $\forall x\in X$ makes inversion, left action and right action continuous on $G$ .","['general-topology', 'group-theory']"
3944751,"Quotient, Tensor Quasicoherent Sheaves on Affine Open Subsets","Quotient and tensor sheaves are defined by sheafification which is somehow implicit. Is there any easy way to handle these constructions? Could we ignore the sheafification when we are working in affine open subsets? For example, let $X$ be a scheme and $\mathcal F$ and $\mathcal G$ be quasicoherent sheaves. Let $U\subseteq X$ be an affine open. Is it true that $(\mathcal F/\mathcal G)(U) = \mathcal F(U)/\mathcal G(U)$ and $\mathcal F\otimes_{\mathcal O_X}\mathcal G(U) = \mathcal F(U)\otimes_{\mathcal O_X(U)}\mathcal G(U)$ ? How about other constructions involving sheafification? (for example, image, cokernel, ann, nil and so on) Thank you in advance.","['algebraic-geometry', 'schemes']"
3944773,Showing that degree of polynomial reduced modulo $p$ divides degree of splitting field,"Let $f$ be a polynomial of degree $n$ with integer coefficients and $p$ a prime for which $f$ , considered modulo $p$ , is a degree- $k$ irreducible polynomial over $\mathbb{F}_p$ . Show that $k$ divides the degree of the splitting field of $f$ over $\mathbb{Q}$ . (this is Miklos Schweitzer 2020, Problem 10) My miscellaneous thoughts: We know that for a polynomial of degree $n$ , the degree of its splitting field is at most $n!$ from induction on the Ring Tower Theorem. Additionally, since $f(x)$ is irreducible over $\mathbb{F}_p$ , then the field $\mathbb{F}_p[x]/(f(x))$ has a root of $f(x)$ . Now suppose $k = 2$ . Then $f(x) = a_n x^n + a_{n-1} x^{n-1} + \cdots + a_1 x + a_0 \in \mathbb{Z}[x]$ reduces to $f(x) \equiv b_2 x^2 + b_1x + b_0 \in \mathbb{F}_p [x]$ where $b_i \equiv a_i \pmod{p}$ where applicable. I'm really not sure if these things are particularly useful or how to combine them. This is really hard!","['field-theory', 'galois-theory', 'abstract-algebra', 'contest-math']"
3944808,Invariant subspaces in Lie groups and algebras representations,"I am trying to prove the following statement: Let $G$ be a connected Lie group with Lie algebra $\mathfrak g$ and with a representation $\Phi: G \to \text{GL}(V)$ , with $V$ finite-dimensional. Then, $\Phi$ induces a unique representation $\varphi: \mathfrak g \to \text{End}(V)$ of the Lie algebra and a subspace of $V$ is $G$ -invariant if and only if it is $\mathfrak g$ -invariant."" Could you give some references or any proof? I know there are several entries in this forum about it, but I don't see one which helped me.","['lie-algebras', 'representation-theory', 'invariant-subspace', 'lie-groups', 'differential-geometry']"
3944900,Characterization of line bundles in $D^b(Coh(X))$,"For a complex algebraic variety $X$ (reduced and of finite type), consider $D^b(Coh(X))$ , the bounded derived category of coherent sheaves on $X$ . Question: (1) Is it true that $F\in D^b(Coh(X))$ is a line bundle (in degree $0$ ) if and only if $RHom(F, \mathbb{C}_p)\cong \mathbb{C}$ for every closed point $p$ , where $\mathbb{C}_p$ is the skyscraper sheaf at $p$ ? If this is not true in general, what conditions on $X$ can ensure the statement (e.g. $X$ is smooth)? (2) What happens when the ground field is not $\mathbb{C}$ ?",['algebraic-geometry']
3944953,$f(0)=0$ and $f'(x)=f(x)^2$ [duplicate],"This question already has answers here : $f'(x) = [f(x)]^{2}$. Prove $f(x) = 0 $ (5 answers) Closed 3 years ago . At first sight, this exercise seems to be already seen many times in this website but I could not find a Analysis-1 level proof of the following :
Let $f: \mathbb{R} \to \mathbb{R}$ be a differentiable function such that $f(0)=0$ and $\forall x \in \mathbb{R}$ , we have $f'(x)=[f(x)]^2$ . Show that $f(x)=0, \forall x \in \mathbb{R}$ . My attempt is : Since $f'(x) \geq 0$ $\forall x$ , we have that $\forall x \in \mathbb R^+ f(x) \ge 0$ . By contradiction, suppose that $\exists a > 0$ such that $f(a)>0$ . Since $f'=f^2\geq0$ , we get that $f(x)>0$ for all $x \in (a, +\infty)$ . Let $g: (a,+\infty) \to \mathbb{R}$ be given by $g(x)=\frac{1}{f(x)}$ . As $g'(x)=-1$ we have $g(x)=-x+c$ , $c \in \mathbb{R}$ . So, $f(x)=\frac1{-x+c}, \forall x \in (a,+\infty)$ . From $f(a)>0$ we get that $c>a$ . But then $f$ isn't continuous at $x=c$ , a contradiction. I think I showed that the function is constantly equal to $0$ on $(0,+\infty)$ , but how do I show it on $(-\infty,0)$ ? Should I just do the same proof with the g function on $(-\infty,a)$ with $a < 0$ ?","['continuity', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
3944986,"If $n>1$, are there $f_1,\dots,f_k\in\mathbb{Z}[x]$ of degree $n$ such that, for any prime $p$, some $f_i$ is irreducible and of degree $n$, mod $p$?","In the comments section of this question , the original poster raised an interesting variation on the problem, which I will state as follows: Let $n>1\in\mathbb{N}$ . Do there exist $f_1,\dots,f_k\in\mathbb{Z}[x]$ of degree $n$ such that, for any $p$ prime, there exists $i(p)\leqslant k$ with $f_{i(p)}$ irreducible and of degree $n$ , mod $p$ ? I am nearly certain that, for any $n\in\mathbb{N}$ , the answer to this question is no, and I suspect there is probably a solution to this using Dedekind's theorem or some similar tool. However, off the top of my head I am not able to see how such an argument might go in the general case. Can anyone give a proof? To get the ball rolling, I have written up an elementary proof for the case $n=2$ below. Other proofs for specific values of $n$ are very welcome as well. Theorem: Let $f_1,\dots,f_k\in\mathbb{Z}[x]$ be polynomials of degree at most $2$ . Then there exists a prime $p$ such that each $f_i$ is either reducible or of degree $\leqslant 1$ , mod $p$ . Proof: Suppose that $f_i=a_ix^2+b_ix+c_i$ for each $i$ . By the quadratic formula, if $a_i$ does not vanish mod $p$ , then $f_i$ will have a root mod $p$ ( $p>2$ ) if and only if $b_i^2-4a_ic_i$ is a quadratic residue mod $p$ . Thus let $\delta_i=b_i^2-4a_ic_i$ for each $i$ . It will suffice to find a prime $p$ such that $\delta_i$ is a quadratic residue mod $p$ , for every $i$ . Let $p_1,\dots,p_l$ be all of the odd prime divisors of each $\delta_i$ , and consider the system of congruences \begin{align}
p\equiv 1&\mod 4 \\
p\equiv 1&\mod p_1 \\
p\equiv 1&\mod p_2 \\
&\vdots \\
p\equiv 1&\mod p_l.
\end{align} The collection $\{4,p_1,\dots,p_l\}$ is pairwise coprime by construction, so Sun Zi's theorem tells us that the above system of congruences is equivalent to $p\equiv m\text{ }(\operatorname{mod} 4p_1\dots p_l)$ for some $m\in\mathbb{N}$ . Now, $m$ is certainly coprime with $4p_1\dots p_l$ , since $m$ is congruent to $1$ mod $4$ and $1$ mod $p_i$ , for each $i$ . Thus, by Dirichlet's theorem , there exists some $\lambda\in\mathbb{N}$ such that $m+\lambda\cdot 4p_1\dots p_l$ is prime; let $p=m+\lambda\cdot 4p_1\dots p_l$ . By construction, $p$ is congruent to $1$ mod $4$ and $1$ mod $p_i$ , for each $i$ . In particular, $p$ is a quadratic residue mod $2$ and mod each $p_i$ , so, by quadratic reciprocity , $2$ and each $p_i$ are all quadratic residues mod $p$ . Similarly, since $p$ is equivalent to $1$ mod $4$ , we have that $-1$ is also a quadratic residue mod $p$ . By construction, every $\delta_i$ is plus-or-minus a product of powers of $\{2,p_1,\dots,p_l\}$ , and so – since products of quadratic residues are quadratic residues – every $\delta_i$ is a quadratic residue mod $p$ . This means that each $\overline f_i$ is either linear or reducible, mod $p$ , as desired.","['algebraic-number-theory', 'irreducible-polynomials', 'number-theory', 'elementary-number-theory', 'galois-theory']"
3945084,Approximation of $\arcsin(\sqrt{1-x^2})$ by $2\arcsin(\sqrt{\frac{1-x}2})$,"I stumbled upon an use of $2\arcsin(\sqrt{\frac{1-x}2})$ to approximate $\arcsin(\sqrt{1-x^2})$ . The latter is equal to $\frac\pi 2-\arcsin(x)$ when $0\le x\le 1$ and is useful if you have a good approximation of $\arcsin$ around $0$ and want to use it for values near $1$ . On the flip side the starting approximation needs to cover $[-\frac1{\sqrt2},\frac1{\sqrt2}]$ in order to use the previous fact to approximate $\arcsin$ on the entire domain. However if we use the fact that $\arcsin(\sqrt{1-x^2})\approx2\arcsin(\sqrt{\frac{1-x}2})$ , our starting approximation only needs to cover $[-0.5, 0.5]$ . Empirically this approximation is extremely good, but I'm wondering how would one come up with this kind of approximation and are there any insights to explain why the two are so close?","['numerical-calculus', 'trigonometry', 'numerical-methods']"
3945157,Solution verification: Derivative of the infinite power tower $y(x) = x^{x^⋰}$,"I was doing the problem $(x^{x^{⋰}})'$ and I would like someone to verify my solution: \begin{align*}
&\left(y=x^{x^{^{⋰}}}\right)'\\
\implies & \;\left(y=x^{y}\right)'\\
\implies & \;\left(y^{\frac{1}{y}}=x\right)'\\
\implies & \;\left(\frac{\ln\left(y\right)}{y}=\ln\left(x\right)\right)'\\
\implies & \;\frac{y'-y'\ln\left(y\right)}{y^{2}}=\frac{1}{x}\\
\implies & \; y'-y'\ln\left(y\right)=\frac{y^{2}}{x}\\
\implies & \; y'=\frac{y^{2}}{\left(1-\ln\left(y\right)\right)x}\\
\implies & \; \left(x^{x^{⋰}}\right)'=\frac{\left(x^{x^{⋰}}\right)^{2}}{\left(1-\ln\left(x^{x^{⋰}}\right)\right)x} 
\end{align*}","['power-towers', 'calculus', 'solution-verification', 'derivatives']"
3945354,Complex polynomials converging in the compact-uniform topology,"Suppose that $p_n$ are polynomials with degree $n$ , with $p_n(0)=1$ and which converges uniformly on compact sets of $\mathbb{D}$ towards an analytic function $f$ .
Now, suppose that the $p_n$ have no roots in a disk of radius strictly greater than 1, say - for simplicity - $D(0,2)$ . Is it true that one can extend the uc convergence to all compacts of the disk $D(0,2)$ ?  Or in any intermediary disk $D(0, r)$ for some $1<r<2$ ? What would be simple assumptions on the $p_n$ to ensure such a behaviour (on the coefficients, the roots, on $f$ ...) ? Answer . As answered above by Sangchul, the answer to the first question is no. He gave a counterexample. Edit: a possible other formulation . By taking the inverses $z \to p_n(z)^{-1}$ , whose radii of convergence are greater than $2$ ,  and using the Hurwitz theorem, the question becomes: we have a sequence of rational functions with no poles in $D(0, 2)$ which takes the value $1$ at zero, and which converges uniformly on compact sets of $D(0, 1)$ towards an analytic function $g=f^{-1}$ . The radius of convergence $R$ of $g$ is greater than or equal to $1$ . Is it possible that $R=1$ ? If not, $R>1$ ; then, does the uniform convergence of $1/p_n$ towards $g$ holds (uniformly on compact sets) for some disk $D(0, s)$ with $1<s<R$ ?","['complex-analysis', 'polynomials']"
3945358,"Finding $\int \frac {e^x \sqrt{e^x - 1}}{e^x+3}\,dx$ by substituting $t^2=e^x-1$","Let's consider the following integral: $$\int \frac {e^x \sqrt{e^x - 1}}{e^x+3}\,dx$$ And let's take the substitution $t^2=e^x-1$ . Then, we get $2tdt=e^xdx$ and so: $$\int \frac {e^x \sqrt{e^x - 1}}{e^x+3}\,dx = 2\int \frac {t \sqrt{t^2}}{t^2+4}\,dt$$ So, can we take $\sqrt{t^2}=t$ there? It seems to me that we could do that only if we say at the beginning, when we took the substitution, that $t$ is non-negative. However, I've never seen that anybody has  written anything like that in any solution of antiderivative problems. So, if we want to be formal, should we write that we take non-negative $t$ ?","['integration', 'calculus', 'substitution']"
3945377,SVD of “almost” block diagonal matrix,"It is possible to show that SVD of block diagonal matrix is equivalent to independent SVDs of each block. I am wondering if there is something interesting to say on the case where the matrix is composed of 2 blocks of size $N$ , with $k$ rows and $k$ columns of size $2N+k$ (overlapping in a $k\times k$ corner) added on. I am particularly interested in the influence of the “shared” rows on the reconstruction of the entire matrix when using truncated SVD. An example: \begin{pmatrix}
  \begin{matrix}
  X_1
  \end{matrix}
  & 0 & \vdots \\
\
  0& X_2 & \vec{u} \\ 
\cdots & \vec{v} &\vdots
\end{pmatrix} Here, $X_1$ and $X_2$ are the two diagonal blocks; each block is $N \times N$ . I concatenate to them a single row $\vec{v}$ with $2N+1$ entries and a single column $\vec{u}$ with $2N+1$ entries, and aim to understand how the SVD of the complete matrix relates to the SVD of the block diagonal sub-matrix. Thanks","['matrix-decomposition', 'svd', 'matrices', 'linear-algebra', 'block-matrices']"
3945393,Prove that $c^2=a^2+b^2-2ab\cos\left(C-\frac{KS}{3}\right)$ holds on a smooth surface.,"Problem: Given a infinitely small geodesic triangle $\triangle ABC$ on a smooth surface, denote the corresponding edges as $a,b,c.$ Prove: the area of $\triangle ABC$ (denote as $S$ ) and the Gauss curvature on $C$ ( denote as $K$ ) satisfies $$c^2=a^2+b^2-2ab\cos\left(C-\frac{KS}{3}\right).$$ Since the triangle is infinitesimal, therefore it is naturally to use the first fundamental form and the Gauss-Bonnet theorem. However, I got stuck at how to handle the $\frac{KS}{3}$ in the equation. Since the coefficient is $\frac{1}{3}$ , if using Gauss-Bonnet theorem, we would include new variables $\angle A,\angle B$ . Could anyone help me out? Thanks a lot in advance!",['differential-geometry']
3945477,Limit of $(1+z)(1+z/2)\cdots(1+z/n)$,"Let $z$ be a complex number such that $\operatorname{Re}(z)<0$ and $$z_n = (1+z)\Bigl(1+\frac{z}{2}\Bigr) \cdots \Bigl( 1+\frac{z}{n} \Bigr).$$ Prove that $ \lim_{n \to \infty} z_n = 0$ . I noticed that: If $| z_n |^2 \to 0$ then $z_n \to 0$ . \begin{align*}
\lvert z_n \rvert^2
&= z_n \overline{z_n} \\
&= (1+z)(1+\overline{z}) \cdot \Bigl(1+ \frac{z}{n}\Bigr) \Bigl(1+ \frac{\overline{z}}{n}\Bigr) \\
&= (1 + 2 \operatorname{Re}(z) + \lvert z \rvert^2) \cdots \biggl(1 + \frac{2\operatorname{Re}(z)}{n} + \frac{\lvert z \rvert^2}{n^2}\biggr).
\end{align*} For a fixed $z$ there will be such $ n \in \mathbb{N}$ such that for every $m >n$ we have $\frac{2\left|\operatorname{Re}(z)\right|}{m} > \frac{\left|z\right|^2}{m^2}$ .
Which means $$ 1 + \frac{2\operatorname{Re}(z)}{m} + \frac{\lvert z \rvert^2}{m^2} < 1.$$ So from one point we will be multiplying by a factor always smaller than $1$ (but closer and closer to $1$ ). Is that a proper approach to the problem? I understand it is not enough to conclude that the limit is $0$ .",['complex-analysis']
3945490,The coefficient and asymptotic in generating function,"Let $L_n$ be the set of all the paths from $(0,0)$ to $(n,0)$ such that every step is $u=(1,1)$ , $d=(1,-1)$ and $r=(2,0)$ .
Notice that the path could go under the $x$ axis. a. Write a generating function of the number of paths in $L_n$ . b. Analyze the asymptotic of $|L_n|$ for a large n. My trial: in part a, I wrote: Every path = (empty path) or (r every path) or (u Schröder path d every path) or (d Schröder path u every path). Now, denote our g.f. by $A(x)$ and Schröder g.f. by $S(x)$ , and denote $x$ to count half of the length, get: $A(x)=1+xA(x)+xS(x)A(x)+xS(x)A(x)$ . Thus, $$A(x)=\frac{1}{1-x-2xS(x)}$$ Then if we substitute the formula of $S(x)=\frac{1-x-\sqrt {(1-x)^2-4x}}{2x}$ : $$A(x)=\frac{1}{\sqrt {(1-x)^2-4x}}$$ In part b, to analyze the asymptotic of $|L_n|$ , I guess I should find the coeifficient of $[x^n]$ in $A(x)$ where $A(x)=\sum_{n\geq 0} |L_n| x^{2n}$ .
However if we solve denominator of $A(x) = 0$ we get \begin{align*}
(1-x)^2-4x=0
\quad\Rightarrow\quad
x^2-6x+1=0
\end{align*} so, $x=3 \pm 2\sqrt2$ . So the g.f. will be analytic if we delete the intervals $(3+\sqrt2, \infty)$ and $(-\infty,3-\sqrt2)$ .
Write: $A(x)=\frac{1} {\sqrt{(x- r_1)(x-r_2)}}=\frac{1}{\sqrt{x-r_1}} \frac{1}{\sqrt{x-r_2}}= \frac{1}{\sqrt{1-x/r_1}} \frac{1}{\sqrt{1-x/r_2}}$ where $r_1=3+\sqrt{8}$ and $r_2=3-\sqrt{8}$ .
Then, $A(x)=(\sum_{n\geq 0}  (1/r_1)^n x^n \sum_{n\geq 0} (1/r_2)^n x^n)^{1/2}= \sum_{n\geq 0} \sum_{j=0}^{n} (1/r_1)^j (1/r_3)^{n-j} x^n$ ,
here I used Caushy formula. So, $[x^n] A(x)= \sum_{j=0}^{n} (1/r_1)^j (1/r_2)^{n-j}$ $L_n = \sum_{j=0}^{n} (r_2/r_1)^j (1/r_2)^n$ .
But this was not much useful! Edited :
Another approach, which does not use the Legendre polynomial: *I found in oeis the sequence of this problem is A001850 and it shows that the coefficient of the series is $\sum_{k=0}^{n} {n\choose k}{n+k \choose k}$ .In addition,  wikipedia page : https://www.google.com/url?sa=t&source=web&rct=j&url=https://en.m.wikipedia.org/wiki/Delannoy_number , mentioned the coefficient and the asymptotic of the above  g.f : $\frac{1}{\sqrt{1-6x+x^2}}$ which is the g.f of the  Delannoy number, but without explanation/proof.
How they find the coefficient of this generating function, and how they analayzed its asymptotic then? Many thanks!","['legendre-polynomials', 'analyticity', 'asymptotics', 'combinatorics', 'generating-functions']"
3945544,Why is everything geometrical modeled on $\Bbb R$?,"The reals naturally arise when discussing limiting processes of rational numbers like trying to compute roots or $\pi$ . Similarly I have read that axioms of Euclidean geometry (I mean those not explicitly relying on $\Bbb R$ a priori) naturally lead to the real numbers. So there is no doubt in their motivation or in their usefulness. Nevertheless I am wondering why they are so prominent in every geometric kind of situation. To be more explicit let me draw the following analogy: In algebra it is common to consider some property of the ring of integers and turn this into an abstract definition (think integral domain , factorial domain , principal ideal domain , euclidean domain etc.). I find it particularly interesting that in these settings properties we know and love in $\Bbb Z$ continue to hold, because the corresponding abstract definition perfectly encapsulates the essence of the properties (eg. prime=irreducible in factorial domains). On the analytic / geometric side however the most abstract things derived from the reals I know of are topological spaces and sigma-algebras. Yet as soon as we try to work with them we immediately come back to the reals by forcing connections to $\Bbb R$ ( metric spaces have a distance function evaluating in $\Bbb R$ , similarly normed vector spaces have norms evaluating in $\Bbb R$ , measures evaluate in $\Bbb R$ , path-connectivity and homotopy are based on the unit interval $[0,1]\subseteq \Bbb R$ etc. etc.). Compared to the algebraic side mentioned before, this feels rather unaxiomatic to me. Why don’t we assume that evaluations take place in totally ordered groups or something along these lines? Why do we pick the unit interval in $\Bbb R$ and not some totally ordered lattice? Is it just convenience, are the more general / abstract definitions unnecessary by some sort of Lefschetz-principle or is this purely coincidental? TLDR Everything geometrical (metric spaces, path-connectedness etc.) is inherently based on $\Bbb R$ . Why is that and why don’t we see more axiomatic versions. As always thank you very much for your time, patience and considerations.","['real-numbers', 'general-topology', 'geometry', 'meta-math']"
3945550,On approximating the distribution of the distance between two random points on an $n$-sphere,"Let $\tau\in(0,2)$ a given threshold value. What is the probability that the Euclidean distance $D$ between two points $\mathbf{x}$ and $\mathbf{y}$ selected uniformly at random on the $n$ -sphere $\mathcal{S}_n=\{\mathbf{v}\in\mathbb{R}^n:\|\mathbf{v}\|_2=1\}$ , is greater than or equal to $\tau$ ? It is well known that the probability density function of the Euclidean distance $d$ between two points on the $n$ -sphere $\mathcal{S}_n=\{\mathbf{v}\in\mathbb{R}^n:\|\mathbf{v}\|_2=1\}$ is $$f(d)=\frac{\Gamma(n/2) d^{n-2}}{\sqrt{\pi}\,\Gamma((n-1)/2)}\left(1-\left(\frac{d}{2}\right)^2\right)^{\frac{n-3}{2}}~.$$ It is also well known that, as $n$ increases, $f(d)$ approaches the normal distribution $\mathcal{N}\left(\sqrt{2}, \frac{1}{2n}\right)$ . However, even with this knowledge, it is not clear to me how to obtain an analytical expression of $\Pr(d\ge\tau)$ for a finite number of dimensions $n\gg 1$ , because it seems hard to evaluate the corresponding integral. Hence, I want to find an approximation of $\Pr(d\ge\tau)$ . More precisely I want to obtain a meaningful upper bound of $\Pr(d\ge\tau)$ .","['probability-distributions', 'geometry', 'probability']"
3945592,using Liouville-Ostrogradsky to solve differential equation,"I have a differential equation: $$y''+p_1\left(x\right)y'+p_0\left(x\right)y=0$$ for $x>0$ and it is given that $p_0,p_1$ are continuous for any $x\in \mathbb{R}$ . I am questioned if $y_1(x)=x$ and $y_2(x)=\ln(x)$ can be the solutions of this equation. Answer:
They are linear independent, therefore using Liouville-Ostrogradsky: $$W(y_1,y_2)(x)=ce^{-\int \:p_1\left(x\right)dx}$$ we get: $$W(y_1,y_2)(x)=\left|\begin{pmatrix}x&\ln\left(x\right)\\ 1&\frac{1}{x}\end{pmatrix}\right|=\ln\left(x\right)-1$$ so, we can choose c=1 for comfort and get: $$\ln\left(x\right)-1=e^{-\int p_1\left(x\right)dx}$$ therefore: $$\int p_1\left(x\right)dx=\frac{1}{\ln\left(\ln\left(x\right)-1\right)}$$ Therefore, after deriving both sides: $$p_1\left(x\right)=\frac{1}{x\cdot \left(\ln\left(x\right)-1\right)\cdot \ln^2\left(\ln\left(x\right)-1\right)}$$ Now the thing that bothers me, that is was said about $p_1$ that it is continuous for $x\in \mathbb{R}, x>0$ , and here the denominator is defined only for any $x>e$ , so I don't know if the authors of the question meant that it should be continuous for any $x>e$ and then we can continue and find $p_0$ or it contradicts the given statement and therefore those two functions cannot be a solution for the equation. Am I right that those are the two options of answers for this diff. equation ?
If it is about $x>e$ , then we can choose $$y=x+\ln(x)$$ , calculate $y'$ and $y''$ , put it all together in the first formula: $y''+p_1\left(x\right)y'+p_0\left(x\right)y=0$ and extract $p_0$ . Otherwise if the question means that $p_1(x)$ has to be continuous on $x>0$ , then $p_1(x)$ doesn't satisfies that because it is defined only for $x>e$ , is this correct ?","['ordinary-differential-equations', 'continuity', 'functions', 'wronskian', 'derivatives']"
3945630,Minimizing RSS for model with missing observations. Dummy variable vs Dropping observations,"Suppose that the relationship: $Y_i = β_1 + β_2*X_i + u_i$ is being fitted and that the value of X is missing for some observations. One way of handling the missing values problem is to drop those observations. Another is to set X = 0 for the missing observations and include a dummy variable D defined to be equal to 1 if X is missing, 0 otherwise. Demonstrate that the two methods must yield the same estimates of β_1 and β_2. Write down an expression for RSS using the second approach, decompose it into the RSS for observations with X present and RSS for observations with X missing, and determine how the resulting expression is related to RSS when the missing value observations are dropped. My attempt: Suppose we have n total observations and the first k of them are not missing, and all the rest are missing. So we have model 1 where missing x_i will be 0 and model 2 where d_i is 1 if x_i is missing, and 0 otherwise: We can see that in RSS_1 and RSS_2 second terms are the same. And the textbook, from which I took this task says that β_1 and β_2 which are minimizing RSS for two models will be the same. Namely, this will be β_1 and β_2 which could be obtained by minimizing the first term of RSS_2, but I have no idea why. Is this because the first term of RSS_1 does not depend on x_i?","['statistics', 'least-squares']"
3945671,How do you integrate over the directional derivative?,"Question: How do you integrate the directional derivative of a function over a rectangle? Let's say $K$ is a rectangle in $\mathbb{R}^2$ , and let's say that $\beta$ is a 2D vector that specifies a direction. Lastly, say $u(x,y)$ is a multivariable scalar function. How do I solve the following \begin{equation}
\int _K \beta \,\,\cdot \nabla u
\end{equation} where $\beta \,\,\cdot\nabla$ denotes the directional derivative in the direction of $\beta$ . I know that in 1D integrating over the derivative of a function gives back the original function, but I'm not sure how this works in 2D. Thanks!",['multivariable-calculus']
3945685,Computing the derivative of a matrix-valued function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Suppose $\mathbf{A} :\mathbb{R}^m\to\mathbb{R}^{m\times m}$ such that $\mathbf{A}(\theta)$ is a symmetric matrix. Let $\mathbf{A}(\theta)$ have the eigen-decomposition $\mathbf{A}(\theta) = \mathbf{U}(\theta)~\mathrm{diag}(\lambda_1(\theta),\ldots,\lambda_m(\theta))~\mathbf{U}(\theta)^\top$ . If I have a function $f:\mathbb{R}\to\mathbb{R}$ and I define \begin{align}
f(\mathbf{A}(\theta)) = \mathbf{U}(\theta) ~\mathrm{diag}(f(\lambda_1(\theta)),\ldots,f(\lambda_m(\theta)))~\mathbf{U}(\theta)^\top
\end{align} How does one compute $\frac{\partial}{\partial\theta_i} f(\mathbf{A}(\theta))$ ?","['matrix-calculus', 'derivatives', 'eigenvalues-eigenvectors']"
3945734,Is this multivariable calculus?,"The following is a question from Calculus made easy by Silvanus P. Thompson The volume of a right circular cylinder of radius $r$ and height $h$ is given by the formula $V=πr^2h$ . Find the rate of variation of volume with the radius when $r = 5.5 \text{in}$ . and $h = 20 \text{in}$ . If $r = h$ , find the dimensions of the cylinder so that a change of 1 in. in radius causes a change of 400 cubic inches in the volume. I understand the solution to the problem apart from the second part when $r=h$ . I got the first solution for that when I took h as a constant but there is second answer for the question in which $h$ varies with $r$ shown below: $$\frac {dV}{dr}=3πr^2$$ $$=400$$ and then we solve for $r$ which is equal to $h$ . My question is isn't this multivariable calculus and is this method correct?","['multivariable-calculus', 'calculus']"
3945740,Matrices and power series,"For any square matrix $A$ , we can define $\sin A$ using the formal power series as follows: \begin{equation}
\sin A=\sum_{n=0}^{\infty}\frac{(-1)^n}{(2n+1)!}A^{2n+1}
\end{equation} Prove or disprove: there exists a $2\times2$ real matrix $A$ such that \begin{equation}
\sin A=\begin{bmatrix}1 & 2020 \\0& 1\end{bmatrix}
\end{equation}","['matrices', 'trigonometry', 'power-series']"
3945763,A question about functional in normed space and Hahn Banach theory,"Let $X\neq {0}$ a normed space, $x_n\in X$ a sequence in $X,x\in X$ . Assume for every $x^*\in X^*$ : $x^*(x_n)\to x^*x$ . Show that: $\|x\|\leq \liminf_{n\to \infty} \|x_n\|$ . I tried to use Hahn Banach's corollary that says:
If $X$ is a normed space and $0\neq x\in X$ , there is a functional $x^*\in X$ such that $\|x^*\|=1$ and $x^*x=\|x\|$ . We can notice that $x$ that satisfies $x^*(x_n)\to x^*x$ is unique. Because $X\neq 0$ then the $x$ satisfying this is $\neq 0$ .
Then, by the corollary mentioned above and the given information we get: $$\|x\|=x^*x=\lim_{n\to \infty} x^*x_n.$$ Now, how can I use the fact that $\|x^*\|=1$ ?","['hahn-banach-theorem', 'functional-analysis', 'real-analysis']"
3945802,Defining manifold coordinates without an embedding,"This is something that's been bugging me for a while now, I understand that one of the most important facts about differential geometry is that it doesn't require embeddings into higher dimensional spaces. My problem is how we are then able to meaningfully define coordinates without making reference to an external coordinate system in which our manifold is embedded. For example, when we define a coordinate chart on the circle, we can use the projection onto the axes as: $$(x,y)\mapsto x\quad (x,y)\mapsto y \tag{1}.$$ This basically takes a circle centred at the origin and provides a four separate coordinate charts which cover the circle. This is fine, but it seems to require us to specify the $(x,y)$ coordinates in the first place. I've seen a similar construction where we begin with the polar angle $\theta$ and use this to define the coordinates in a similar way. But this again seems to require us to have the circle originally embedded in $\Bbb R^2$ ? How can we provide coordinates for a simple manifold like the circle without referencing an embedding?","['manifolds', 'circles', 'coordinate-systems', 'differential-geometry']"
3945840,Why does the empty set not get a relation in a cartesian product?,"As far as I understand, when $A=\{1,2\}$ and $B=\{1,2\}$ , then $A \times B =\{(1,1),(1,2),(2,1),(2,2)\}$ .
But $\emptyset \in A$ and $\emptyset \in B$ . Are any of these valid? If not, why not? a. $(\emptyset, 1) \in A \times B$ b. $(1, \emptyset) \in A \times B$ c. $(\emptyset, \emptyset) \in A \times B$ d. $\emptyset \in A \times B$","['elementary-set-theory', 'direct-product', 'relations']"
3945841,What change of variables is this?,"I have come across this integral: $$\int \frac{f(x+tv)-f(x)}{t}g(x)dx = \int -\frac{g(x)-g(x-tv)}{t}f(x)dx$$ which the author claims is justified by a change of variables, but I cannot see what they did. Would anyone be able to elaborate?","['integration', 'real-analysis']"
3945869,Please help me understand the solution of this question! [duplicate],"This question already has answers here : How many multiples of 3 are between 10 and 100? (SAT math question) (3 answers) how long does the care take to reach the last pole algebra word problem (2 answers) Closed 3 years ago . Question : Six bells commence tooling together and toll at intervals 2,4,6,8,10,12 minutes.In 30 hours,how many times do they toll together? I found the LCM of 2,4,6,8,10,12 minutes in order to find the minimum minutes after they will tool together. LCM (2,4,6,8,10,12)=120 It means they will toll together after 2 hours that is 120 mins.So in in 30 hours they should too together 15 times ( 30/2). But the answer provided in the textbook is 16 times.Can you please explain how they got 16?",['discrete-mathematics']
3945897,Why it is important to write a function as sum of even and odd functions?,For the function $f(x)$ we can write it as sum of even and odd functions: $$f(x)=\underbrace{\frac{f(x)+f(-x)}{2}}_{\text{Even}}+\underbrace{\frac{f(x)-f(-x)}{2}}_{\text{Odd}}$$ My question is why it is important for us to write a function as sum of these two even and odd functions? Is there any application of that?,"['even-and-odd-functions', 'calculus']"
3945899,Number of possible bit sequences of length m with at least n consecutive 1's in them,"I have seen similar questions to this but they each seem to be special cases of this general question. Answering this would be beneficial to my research, but I am not a combinatorics expert, and this seemingly simple question eludes me. Is there a simple formula to calculate this? Everything I have seen online has been centered around things like ""either 2 consecutive 1's or 0's"" or ""contains no .."". If it helps, I know that for $m = 8$ bits and say the sequence is denoted $S(m,n)$ $$  
S(m = 8, n = 1) = 255 \\  
S(8,2) = 201  \\
S(8,3) = 107  \\
S(8,4) = 48   \\
S(8,5) = 20   \\
S(8,6) = 8   \\
S(8,7) = 3   \\
S(8,8) = 1 
$$ Interestingly I'm finding that $S(8,4)=S(9,5)=S(10,6)=S(11,7)=48$ I haven't tested $S(12,8)$ because I don't want my computer to melt but I'm seeing a pattern... However this does not seem to work for $m<8$ .","['combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
3945939,Does $A^{-1} = A_0^{-1} + O_p(n^{-1})$ where $A = A_0 + O_p(n^{-1})$?,"Let $A$ be a random matrix that converges in probability to a constant matrix $A_0$ at the rate $O_p(n^{-1})$ : $$
A = A_0 + O_p(n^{-1}).
$$ I want to know if $A^{-1} = A_0^{-1} + O_p(n^{-1})$ ? Let $I$ be the identity matrix. Here is my attempt at proving it, which I'm not sure is valid mainly because I don't know if we are allowed to define a series expansion in the form of $O_p$ variables. $$
\begin{align}
A^{-1} &= (A_0 + O_p(n^{-1}))^{-1} \\
&= (A_0(I + A_0^{-1}O_p(n^{-1}))^{-1} \\
&= (A_0(I + O_p(n^{-1}))^{-1} \\
&= (I + O_p(n^{-1}))^{-1}A_0^{-1}.
\end{align}
$$ Now we perform a Neumann series expansion which gives $$
\begin{align}
(I + O_p(n^{-1}))^{-1}
&= \sum_{k=0}^\infty (-1)^{k} (O_p(n^{-1}))^{k} \\
&= I - O_p(n^{-1}) \\
&= I + O_p(n^{-1}).
\end{align}
$$ I have assumed here that $(O_p(n^{-1}))^0$ , is this true? Then we substitute this into $(I + O_p(n^{-1}))^{-1}A_0^{-1}$ to get $$
\begin{align}
A^{-1} &= (I + O_p(n^{-1}))A_0^{-1} \\
&= A_0^{-1} + O_p(n^{-1})A_0^{-1} \\
&= A_0^{-1} + O_p(n^{-1}).
\end{align}
$$ In the deterministic case, if we wanted to invert the matrix $(I+K)$ we need to have $||K|| < 1$ to ensure convergence. So does this mean that in the probabilistic case, to invert the matrix $(I + O_p(n^{-1}))$ we need to have $||O_p(n^{-1})|| < 1$ ? So the norm of an $O_p(n^{-1})$ term is required to be less than one..does this phrase even make sense? It seems an $O_p(n^{-1})$ term will be increasingly likely to have a value less than $1$ as $n \to $ \infty$, since the probability is becoming more concentrated around zero, but I don't think it can ever be fully guaranteed to be below one? So does $A^{-1} = A_0^{-1} + O_p(n^{-1})$ really hold and if it does is my proof valid? If not, what is a valid proof?","['asymptotics', 'solution-verification', 'convergence-divergence', 'probability-theory', 'probability']"
3945967,Why compactness of control space implies Value Function $>-\infty$,"I went through the Sec I.4 of the book Controlled Markov Process and Viscosity Solutions by Fleming and Soner . In the model, $x(s)$ for $s\in[t,t_{1}]$ follows the ODE, \begin{eqnarray}
x^{\prime}(s)&=&f(s,x(s),u(s)) \\
x(t)&=&x
\end{eqnarray} where $f(s,x,u)$ is a continuous function on the set $[t_{0},t_{1}]\times\mathbb{R}^{m}\times U$ , where $U\subseteq\mathbb{R}^{m}$ is an open set called the control space . $f$ also follows the property, \begin{eqnarray}
\forall s\in[t,t_{1}], x,y\in\mathbb{R}^{n},|u|\leq\rho,~~|f(s,x,u)-f(s,y,u)|&\leq & K_{\rho}|x-y|
\end{eqnarray} for some constant $K_{\rho}>0$ . The control $u(\cdot)$ must be bounded and belong to the set, $\mathcal{U}^{0}(t)=L^{\infty}([t,t_{1}]; U)$ , so that the IVP satisfied by $x(s)$ has a unique solution.  The problem is to minimize the cost function, \begin{eqnarray}
J(x,t;u)&=&\int_{t}^{t_{1}}L(s,x(s),u(s))ds+\psi(x(t_{1}))
\end{eqnarray} over the set of all admissible controls $\mathcal{U}^{0}(t)$ . This minimum is defined as the value function, \begin{eqnarray}
V(t,x)&=&\inf_{u\in\mathcal{U}^{0}(t)}J(x,t;u).
\end{eqnarray} The book states that if the control space is a compact set, $V(t,x)>-\infty$ . Is it because, the $J(x,t;u)$ is somehow continuous wrt $u$ in the space $\mathcal{U}^{0}(t)$ ? But still, where is the compactness of $U$ getting used. Thanks in advance.","['ordinary-differential-equations', 'optimal-control', 'analysis', 'supremum-and-infimum', 'compactness']"
