question_id,title,body,tags
2658072,Is there a simple characterization of the sets that admit a uniform probability distribution?,"The following subsets of $\mathbb{R}$ (not an exhaustive list) admit a uniform probability distribution: Finite sets Intervals (open or closed) with both endpoints finite Finite unions of the above Is there a simpler way to characterize the set of subsets $S \subset \mathbb{R}$ that admit a uniform probability distribution? One naive guess is that $S$ is bounded, but this requirement is neither necessary nor sufficient: e.g. the set $$\left \{ \frac{1}{n} : n \in \mathbb{Z}^+ \right \}$$
is bounded but does not admit a uniform probability distribution, while the set
$$\bigcup_{n = 0}^\infty \left[ n - 2^{-n}, n + 2^{-n} \right]$$
is not bounded but does admit a uniform distribution. My guess is that the answer is no, because by combining together sets of types 1 and 2 in the list above, we are combining ""apples and oranges"", as discrete and continuous probability distributions are qualitatively different. If we want to extend the support of a discrete random variable taking on a finite number of possible values to the real line, then we shouldn't think of its probability distribution as a probability mass function, but instead as a finite sum of Dirac delta functions, which are a very different beast from continuous probability density functions. Is this correct?","['uniform-distribution', 'probability-theory', 'probability-distributions']"
2658170,Is $\|e^{zT-zT^*}\|$ bounded for all $z\in \mathbb{C}$?,Let $E$ be an infinite-dimensional complex Hilbert space and $T\in \mathcal{L}(E)$. Is $\|e^{zT-zT^*}\|$ bounded for all $z\in \mathbb{C}$?,"['functional-analysis', 'complex-analysis']"
2658176,Structure theorem for modules implies Smith Normal Form,"I am working on a homework problem for an undergrad abstract algebra course and I have been stuck on something for a while. I am not looking for a full proof, but any help would be much appreciated. Apologies if this is something simple! The problem: Show that for any matrix $B \in M_{n,m}(R)$, the orbit $GL_n(R) \cdot B \cdot GL_m(R)$ contains a matrix $A$ in Smith normal form, and moreover the integer $k$ and the ideals $(A_{11}),\ldots,(A_{kk})$ are uniquely determined. Edit: $R$ is a PID, and $k$ is the highest integer $i$ such that $A_{ii} \neq 0$. I have the structure theorem at my disposal, and I have shown that $GL_n(R) \cdot B \cdot GL_m(R)$ corresponds to the set of matrices for the linear transformation $f: R^m \to R^n$ given by $f(m) = Bm$ for all possible bases of $R^m$ and $R^n$. It is suggested that I start with the $\ker(B) = \{0\}$ case and modify from there; I have figured out how to prove the full theorem assuming this case. Does anyone have any suggestions for going about this case? Again, I am not necessarily looking for a full proof, just something to help me get unstuck. Thank you for your time!","['abstract-algebra', 'ring-theory', 'modules', 'principal-ideal-domains']"
2658207,Expected Value of Drawing Tickets,"A collection of tickets comes in 4 colors: Red , Blue , White , and Green . There are twice as many reds as blues, equal number of blues
  and whites, and three times as many greens as whites. Choose 5 tickets
  at random with replacement . Let $X$ be the number of different colors
  that appear. Find E(X). $E(X) = \sum_{all \,x}xP(X=x)$ where $X \in \{1, 2, 3, 4\}$ $= 1P(X=1) + 2P(X=2)+3P(X=3)+4P(X=4)$ Each $P(X=x)$ calculation is fairly complicated as I need to account every combination such as for $P(X=4) = \frac{5!}{1!1!1!2!}(2(\frac{1}{7})^3(\frac{2}{7})(\frac{3}{7}) + (\frac{1}{7})^2(\frac{2}{7})^2(\frac{3}{7}) + (\frac{1}{7})^2(\frac{2}{7})(\frac{3}{7})^2)$ However, the answer for E(X) is just $2(1-(\frac{6}{7})^5)+1-(\frac{5}{7})^5+1-(\frac{4}{7})^5$ How did they come up with that? Is it just a simplified version of what I'm doing?","['statistics', 'probability']"
2658208,Why iterated limits are different from simultaneous limits?,"There is sequence $a_{m,n}=\frac{m}{m+n}$ we calculate the following limits $$\lim_{n\rightarrow\infty}\lim_{m\rightarrow\infty}a_{m,n} \qquad \lim_{m\rightarrow\infty}\lim_{n\rightarrow\infty}a_{m,n}$$ I find both of these limits to be $1,0$ respectively. But the limit of $$\lim_{m,n\rightarrow\infty}a_{m,n}=0.5$$ should be $0.5$ because we have a denominator that will be twice the numerator for very large but comparable values of $m,n$. What is the notion of limits in this situation? Edit: Can the simultaneous limit be written like this Since $m,n\rightarrow \infty \implies m \approx n\implies \lim_{m,n\rightarrow \infty}a_{m,n}=\lim_{n\rightarrow \infty}\frac{n}{n+n}=0.5$","['real-analysis', 'limits']"
2658210,Brauer group of global fields,"Is the Brauer group $\text{Br}(K)$ of a global field $K$ an $\ell$-divisible group for some prime $\ell$? If so, what $\ell$? Is $\text{Br}(K)[n]$ finite, for $n$ integer? I know from class field theory that it fits into an exact sequence $$0\to \text{Br}(K)\to\bigoplus_v\text{Br}(K_v)\xrightarrow{\sum_v \text{inv}_v} \mathbf{Q}/\mathbf{Z}\to 0$$ with $v$ running over all places of $K$, and $K_v$ the completion of $K$ at $v$. but I can't conclude. Thanks very much.","['brauer-group', 'class-field-theory', 'algebraic-geometry', 'algebraic-number-theory', 'arithmetic-geometry']"
2658239,Evaluate $\lim_{n\rightarrow \infty} \frac{n^{n+1}}{(1^{1\over n}+2^{1\over n}+\dots+n^{1\over n})^n}$,"Evaluate $$\lim_{n\rightarrow \infty} \frac{n^{n+1}}{(1^{1\over n}+2^{1\over n}+\dots+n^{1\over n})^n}.
$$ I know it's easy to find the answer  $e$ï¼Œ but I wanna collect some good ways.","['real-analysis', 'integration', 'calculus', 'limits']"
2658241,Proof of polar coordinates theorem in Evans' PDE Book,"Evans used this theorem '4' to derive the fundamental solutions of Poisson's equation.
in the appendix it says that this is a special case of a more general theorem called 'Coarea Formula'. I found proofs of that one but they involve some advanced level Geometry that I don't have good command of. Is there a way to prove this theorem '4'just by using multivariable calculus? And I think I know the intuition behind this, it's like a generalized disk method, right? In dimensions over than 3 it's quite hard to imagine.","['multivariable-calculus', 'multiple-integral', 'integration', 'proof-explanation']"
2658368,How is Category Theory used to study differential equations?,"I know that one can use Category Theory to formulate polynomial equations by modeling solutions as limits. For example, the sphere is the equalizer of the functions
\begin{equation}
  s,t:\mathbb{R}^3\rightarrow\mathbb{R},\qquad s(x,y,z):=x^2+y^2+z^2,~t(x,y,z)=1.
  \label{equalizer}
\end{equation}
One could then find out more about the solution set by mapping the equalizer diagram into other categories. More generally, solution sets of polynomial equations (and more generally, algebraic varieties ) are a central study object of algebraic geometry. As differential equations are central to all areas of physics, I assume that there have been made a lot of attempts to generalise these ideas to solution sets of these. However, I do not yet have a lot of knowledge about algebraic geometry, topos theory or synthetic differential geometry. Thus I would be grateful if someone could explain roughly where and how Category Theory is used to study differential equations. Can Category Theory really help to solve differential equations (for example by mapping diagrams of equations to other categories, similarly to how problems of topology are often solved by mapping topological spaces to algebraic ones in algebraic topology) or can it ""only"" provide schemes for generalising differential equations to other spaces/categories? I am particularly interested in names of areas I have to look into if I want to understand this better. Also literature recommendation would be very welcome. EDIT : I found a book by Vinogradov called Cohomological Analysis of Partial Differential Equations and Secondary Calculus where ""the main result [...] is Secondary Calculus on diffieties"". However, the material is very deep and thus I am still not completely able to say whether these ""new geometrical objects which are analogs of algebraic varieties"" can be used to help solving PDEs or if they serve to structure the theory of PDEs or result in other applications I am not aware of. Thus further information would be very appreciated!","['synthetic-differential-geometry', 'algebraic-geometry', 'reference-request', 'category-theory', 'ordinary-differential-equations']"
2658382,On what groups is the convolution of probabilities jointly weak*-continuous?,"Let $G$ be a locally-compact topological group. Let $C_0 (G)$ be the Banach algebra of the continuous functions that vanish at infinity and let $M(G)$ be its dual, which can be identified with the space of all regular Borel measures of finite total variation. Endow $M(G)$ with the weak* topology. Consider the convolution $M(G) \times M(G) \ni (\mu, \nu) \mapsto \mu * \nu \in M(G)$. I can prove that it is separately continuous, and if $G$ is compact I can prove that it is jointly continuous. Is it possible to change the above framework (for instance, the space of functions considered or the topology on it) in order to obtain joint continuity for more general groups?","['topological-groups', 'convolution', 'continuity', 'weak-convergence', 'measure-theory']"
2658455,Simultaneous rotations,"I've been puzzled by this question for a while now (something I stumbled upon myself): Say I have two 2-spheres $S^2$. For the $k$-th sphere ($k \in \{ 0,1 \}$), I define two axes:
$$
a_k = \hat{z} \\
b_k = \sin\left(\frac{\pi(k+1/2)}{2}\right) \hat{y} +  \cos\left(\frac{\pi(k+1/2)}{2}\right)\hat{z}
$$
Denote an $SO(3)$ rotation around $a_k$ by angle $x$ (with some convention about handedness) by $R_{a_k}(x)$, and similarly for $b_k$. For each sphere, I rotate a point $v_i^{(k)} = (0,0,1)$ to a new point $v_f$ according to
$$
v_f^{(k)}(x_1,x_2,x_3,x_4) = R_{a_k}(x_4) R_{b_k}(x_3) R_{a_k}(x_2) R_{b_k}(x_1) v_i^{(k)}
$$
where the angles $(x_1,x_2,x_3,x_4) \in [0,2\pi)^4$ are the same for both sphere (hence the title simultaneous rotations, just that the rotations are around  different fixed axes for each sphere) Now, after playing around with this a bit, I feel like under this map, there always exists a (non-unique) set of angles such that I can get to any point on $(S^2)^2$..  that is, the map $( v_j^{(1)}, v_j^{(2)})(x_1,x_2,x_3,x_4)$ is surjective. How do I go about showing that? Intuitively by drawing some pictures, it seems true but I struggle to wrap my head around showing it properly.. Thanks for reading!","['lie-groups', 'rotations', 'functions', 'geometry']"
2658469,Relationship between Affine Dependence and Linear Dependence in Oriented Matroids?,"I'm reading ""Lectures on Polytopes"" by Gunter Ziegler. The author first introduces the components of oriented matroids in affine case, then making a transition to linear case, with the condition ""1z = 0"" embedded into the matrix V. In the subsequent discussion, the author seems to use only the linear notions, such as in this one: However, this is just my guess. I'm not sure if it is the case. Is it correct to assume that the matrix V in Definition 6.5. has only 1's in its first row? If not, please explain why the condition ""1z = 0"" is no longer necessary. All the theory the author's built up up to this point have this condition.","['discrete-geometry', 'combinatorial-geometry', 'discrete-mathematics', 'combinatorics', 'matroids']"
2658473,Explicit relation between dual and adjoint of a linear map,"Let $V$ and $W$ be finite-dimensional vector spaces over some arbitrary field $K$, and let $V^\ast$ and $W^\ast$ be their respective dual spaces. Let $f:V \rightarrow W$ be a linear map. a) Define the dual of $f$ as the map $f^\ast : W^\ast \rightarrow V^\ast$, $e \mapsto e \circ f$. b) Suppose we define two fixed non-degenerate bilinear forms $\langle \cdot,\cdot \rangle_V : V^2 \rightarrow K$, $\langle \cdot,\cdot \rangle_W : W^2 \rightarrow K$. Define the adjoint of $f$ as the map $\bar{f}: W \rightarrow V$ satisfying
$\langle v,\bar{f}(w) \rangle_V = \langle f(v),w \rangle_W$. Since the bilinear forms are non-degenerate, the linear maps $\phi_V : V \rightarrow V^\ast$ and $\phi_W : W \rightarrow W^\ast$ given by
$$ \phi_V(v)(v_0) = \langle v,v_0 \rangle_V, \qquad \phi_W(w)(w_0) = \langle w,w_0 \rangle_W $$
are isomorphisms. Given this, how can we express $\bar{f}$ explicitly in terms of $f^\ast$ (and if required, $\phi_V$ and $\phi_W$)? Alternatively, how can we prove that the map $\bar{f}$ is guaranteed to exist? EDIT: As pointed out by @levap I have fixed the definition of $\phi_W$.","['linear-algebra', 'linear-transformations']"
2658476,Simple proof for a simple case of Hardy's inequality,"I am trying to prove Hardy's inequality in the case of 2, so: $$\int_0^\infty \frac{1}{x^2} \big(\int_0^x f\big)^2 dx\leq 2\int_0^\infty f^2$$ Where f is continuous over $\mathbb{R}_+$ and such that the integral of $f^2$ converges. The way I'm trying to do it is via an integration by parts, so what I have so far is that: $$\int_0^y \frac{1}{x^2} \big(\int_0^x f\big)^2 dx = -y\big(\frac{1}{y} \int_0^y f\big)^2 +2\int_0^y f(x)\frac{1}{x}\int_0^xf dx$$ I am however unable to prove that any of these three terms is finite, or to obtain a term ressembling the integral of $f^2$ as necessary.",['integration']
2658489,Second Chern class classifies rank $r$ vector bundle over $S^4$?,"On page 16 of Atiyah's Geometry of Yang-Mills Fields ( http://plouffe.fr/simon/math/Atiyah%20M.%20Geometry%20of%20Yang-Mills%20Fields%20(Pisa,%201979)(100s).pdf ) it stated that ""...the topological theory of fiber bundles over spaces like $S^4$ which are not contractible tells us that in this case they are precisely classified by the [same] integer $k$ (= second Chern class)"". So I'm just wondering if this is saying that a $SU(r)$-bundles $E \rightarrow S^4$ are classified by $c_2(E)$? In particular, if $E,E'$ are two $SU(r)$-bundles over $S^4$ and $c_2(E) = c_2(E')$ then $E\cong E'$ (isomorphic as vector bundle)? How do I see that this result is true?
What is a generalization of this result? Would it remain true for any simple Lie group $G$ instead of $SU(r)$? In particular, how do I know when do Chern classes classifies vector bundles over a topological space?","['algebraic-topology', 'mathematical-physics', 'differential-geometry', 'algebraic-geometry']"
2658494,What's the relation between the trace and the range of a projector?,"Knowing that, if $P$ is a projector it'll be:
$$P=P^2, ~~~P=P^\dagger$$
I was wandering if we can identify a relation between the dimention and the trace of a projector matrix.","['functional-analysis', 'linear-algebra', 'hilbert-spaces', 'operator-theory']"
2658495,Is there any number that I can't find? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I have a question that I think is quite weird and I can't find an answer. Is there any real number that I can't find using only $+,-,\times,\div,$ limits and radicals? For example, using some series, I can find $\pi$ or $e$. But is there any number that I can't find using only those? Sorry if I made any mistakes, I don't know how to ask that question or which tags to put.",['real-analysis']
2658510,Trying to understand a simple function,"I'm trying to make sense of a simple equation from Bingzhe Wu, Haodong Duan, Zhichao Liu, Guangyu Sun's paper SRPGAN: Perceptual Generative Adversarial Network for Single Image Super Resolution What I don't understand is $\rho(x)=\sqrt{x^2 + \epsilon^2}$. How am I supposed to find $\epsilon$ (the error term I guess?) if $p$ only takes $x$ as input? Thanks","['machine-learning', 'statistics']"
2658533,Consequence of convergence of indicator function to zero in probability.,"Let $ (\Omega, \mathcal{A}, P) $ be a probability space. Suppose that $ \{ X_n := 1_{A_n} \}_{n \in \mathbb{N}} $ is a sequence of simple random variables and $ X \equiv 0$. It is claimed that $ X_{n} \rightarrow_{P} X $ in probability is equivalent to the statement $ P(A_n) \rightarrow 0. $ Sadly I can't seem to grasp how this is so. I have the following argument: By definition $ X_n \rightarrow_{P} X $ implies for every $ \epsilon > 0 $, $$ \lim_{n \rightarrow \infty}P[ \omega \in \Omega: |X_{n}-X|\geq \epsilon ] = \lim_{n \rightarrow \infty}P[w \in \Omega: 1_{A_n}\geq \epsilon]=0.$$ 
Then for every $ n \in \mathbb{N} $ and every $ \omega \in [1_{A_n}\geq \epsilon] $ we have that $\omega$ satisfies 
$$1_{A_n}(\omega) \geq \epsilon $$
for every $\epsilon \geq 0$. In particular, if we take $ \epsilon > 1 $ then for no $\omega $ can the above inequality be correct, thus $[1_{A_n}\geq \epsilon ] \neq A_n $ since this would require that $1_{A_{n}}=1$. What is going on here? Much appreciate for any help!","['real-analysis', 'analysis', 'probability-theory']"
2658550,A question about probability (conflicting solutions),"There is a square of side $n$ units. Join the diagonals. Now the square is divided into 4 regions of equal area. Each of them is coloured differently. Given 2 points that can lie within any of the four regions, what is the probability that two points are in the same coloured region? The problem: I and my friend get two different solutions using two different lines of reasoning. It has to do with the order of the points being placed: Solution 1 (mine) : The order in which the points are placed does not matter. The number of ways 2 identical objects can be placed in 4 distinct regions is 10 (stars and bars). Out of those, there are 4 cases which satisfy the condition. Hence the probability is $2/5$. Solution 2 (my friend's) : The order of the points being placed does matter. That means there are 16 ways to place the points (4 orientations Ã— 4 regions). Out of those, 4 cases satisfy the condition. Hence the probability is $1/4$. Now of course, one line of reasoning must be wrong here. Which one is wrong? An explanation is appreciated.",['probability']
2658564,When is $HK$ a subgroup?,"Let $H$ and $K$ be subgroups of a group $G$. My question is: when is 
$$
HK = \{hk: h\in H, k\in K\}
$$
a subgroup of $G$? If $G$ is abelian, then this is clearly the case. I believe I have shown that if $H$ and $K$ are both normal subgroups of $G$, then $HK$ is a subgroup. Are there any more general results where this is the case? I do see things like this giving a list of equivalent statements, but I was wondering what statements imply that $HK$ is a subgroup.","['abstract-algebra', 'normal-subgroups', 'group-theory']"
2658609,"How to show the Cauchy-Schwarz inequality $\langle x,y\rangle\leq \|y\|\|x\|_*$ for computing the Fenchel conjugate of a norm","I want to show that the Fenchel conjugate of a norm is the indicator function on the unit ball of the dual norm. The Fenchel conjugate is defined for a function $f$ as, $$f^*(y) = \sup\limits_{x} \langle y, x\rangle -f(x),$$ the dual norm $\|\cdot\|_*$ of a norm $\|\cdot\|$ is defined as, $$\|z\|_* = \sup\limits_{\|u\|\leq 1}\langle z, u\rangle,$$ and the indicator function of a set $C$, denoted $i_C$, is defined as, $$i_C(x) = \begin{cases} 0 & x\in C,\\ +\infty & x\not\in C\end{cases}.$$ Then the problem is to show that $\|x\|^* = i_{\|x\|_*\leq 1}(x)=\begin{cases} 0 & \|x\|_*\leq 1\\ +\infty & \|x\|_*>1\end{cases}.$ First we consider $x$ with $\|x\|_*>1$. Then, by definition of the dual norm, we have,
$$\sup\limits_{\|y\|\leq 1}\langle y,x\rangle>1.$$ So, $\exists y:\|y\|\leq 1$ such that $\langle y,x\rangle >1$, i.e. $\langle y,x\rangle - \|y\| > 0$. If we choose $z=ty$ then we have, $$\langle z, x\rangle - \|z\| = t(\langle y,x\rangle -\|y\|)$$ and now letting $t\to\infty$ shows that $\langle z, x\rangle - \|z\|$ is unbounded, i.e. $\|x\|^* = \infty$ for $\|x\|_*>1$. Now, I am stuck on the remaining case. For this, we consider $x$ with $\|x\|_*\leq 1$. In the book I am reading, the next claim in the proof is, $$\langle x,y\rangle\leq \|y\|\|x\|_*,\quad\forall y$$ how do they arrive at this claim?","['functional-analysis', 'normed-spaces', 'convex-optimization', 'convex-analysis']"
2658614,Proving $\sum_{k=0}^{2m}(-1)^k{\binom{2m}{k}}^3=(-1)^m\binom{2m}{m}\binom{3m}{m}$ (Dixon's identity),"I found the following formula in a book without any proof: $$\sum_{k=0}^{2m}(-1)^k{\binom{2m}{k}}^3=(-1)^m\binom{2m}{m}\binom{3m}{m}.$$ I don't know how to prove this at all. Could you show me how to prove this? Or If you have any helpful information, please teach me. I need your help. Update : I crossposted to MO .","['combinatorics', 'summation', 'binomial-coefficients']"
2658618,Prove that $||(\sum^k_{i=1}|f_i|)^{\frac{1}{2}}||_p \leq (\sum^k_i ||f_i||^2_p)^{\frac{1}{2}}$,"In one popular Chinese Real Analysis textbook, I met a problem stated as the title: Prove the following inequality
\begin{equation}
\left|\left|\left(\sum^k_{i=1}|f_i|\right)^{\frac{1}{2}}\right|\right|_p \leq \left(\sum^k_i \left|\left|f_i\right|\right|^2_p\right)^{\frac{1}{2}},
\end{equation}
given the condition: $2\leq p < \infty, f_i \in L^p(E) (i=1,2,...,k)$. Here is the snapshot . My attempts are as follows: It suffices to consider $k=2$. Without loss of generality, assume all $f_i>0$. Put the $\frac{1}{2}$ on LHS out of the norm, and cancel it with the $\frac{1}{2}$ on RHS. Put the $2$ on RHS into the p-norm. It becomes: \begin{equation}
||\sum^k_{i=1}f_i ||_{\frac{p}{2}} \leq \sum^k_i \left|\left| f_i^2 \right|\right|_{\frac{p}{2}},
\end{equation} The condition $2\leq p < \infty$ reminds me the Clarkson inequalities , and in fact they appeared slightly before this problem. Yet the book didn't provide any hint. Then I'm stucked. Can anyone help me? Thank you for the comments! Though the book has rare typos, the original inequality may have square $2$ outside the LHS p-norm? That is, \begin{equation}
\left|\left|\left(\sum^k_{i=1}|f_i|\right)^{\frac{1}{2}}\right|\right|_p^2 \leq \left(\sum^k_i \left|\left|f_i\right|\right|^2_p\right)^{\frac{1}{2}},
\end{equation} such that the simplified one is \begin{equation}
||\sum^k_{i=1}f_i ||_{\frac{p}{2}}^2 \leq \sum^k_i \left|\left| f_i^2 \right|\right|_{\frac{p}{2}},
\end{equation} The reason is that, similar to Cauchy, Minkowski, Holder inequalities, both sides now have the same ""order""/""power"". No matther which case is true, this kind of inequality is elegant. Hope we can achieve the right one. @Calvin Khor I think your comment is correct. In this way, the question is to show: \begin{equation}
\left|\left|\left(\sum^k_{i=1}|f_i|^2\right)^{\frac{1}{2}}\right|\right|_p \leq \left(\sum^k_i \left|\left|f_i\right|\right|^2_p\right)^{\frac{1}{2}},
\end{equation} Then the proof is obvious!","['functional-analysis', 'real-analysis', 'measure-theory']"
2658643,Contraction of oriented matroid as related to polytope?,"I'm reading the following description of the contraction of oriented matroid, and its connection to polytopes: I have yet to find a numerical example to verify 6.13., but first I just want to check if my understanding is correct. If I was to compute V/v as described, is it correct that the resulting matrix will be the coordinates of the vertices of P (except x) being ""projected"" into a 2-dim line (i.e. the vertex figure P/x) in the 3-dim space? Also, in this example, P is a 2-dim polytope, and V is its vector configuration in R^3. My understanding is that V describes P after being ""lifted up"" one dimension. So if I was to remove that extra dimension from V/v (i.e. by deleting the corresponding row), would I recover the original vertex figure in 2-d?","['discrete-geometry', 'combinatorial-geometry', 'discrete-mathematics', 'matroids', 'polytopes']"
2658680,"Given $T:E^*\to E^*,$ does there exist $S:E\to E$ such that $\langle Te^*,e \rangle = \langle e^*,Se\rangle?$","Let $E$ be a Banach space and $T:E\to E$ be a bounded linear operator.
Denote $E^*$ a dual space of $E,$ that is, the space of all bounded linear functional $e^*:E\to\mathbb{R}.$ It is well known that if $T:E\to E$ is a bounded linear operator, then there exists a bounded linear operator $T^*:E^*\to E^*$ such that for all $e^*\in E^*$ and $e\in E,$
$$\langle Te^*,e \rangle = \langle e^*,Se\rangle.$$
In most literature, $T^*$ is called adjoint operator of $T.$ I would like to know its converse, that is, Question: Given a bounded linear operator $T:E^*\to E^*,$ does there exist bounded linear operator $S:E\to E$ such that for all $e\in E$ and $e^*\in E^*,$
  $$\langle Te^*,e \rangle = \langle e^*,Se\rangle?$$ If $T:E^*\to E^*$ is assumed to be weak$^*$-to-weak$^*$ continuous, then the question has positive answer. What happens if we remove weak$^*$-to-weak$^*$ continuity?
Is the answer still positive?","['functional-analysis', 'real-analysis', 'banach-spaces', 'adjoint-operators']"
2658704,Tangent vectors on manifolds,"I have an elementary understanding of differential geometry, and I know that the concept of a vector on a manifold can be defined in several different ways. Perhaps the easiest to understand, is in terms of equivalence classes of curves $\gamma:M\rightarrow\mathbb{R}^{n}$ on a manifold $M$. In this case, vectors are defined at each point $p\in M$, as tangent vectors to curves at that point, ""living"" in the tangent space $T_{p}M$ to that point. A tangent vector at a point $p\in M$ is then an equivalence class of curves, mutually tangent, at that point. Another way is to construct the notion of a vector using derivations. My question is (and apologies if it's a silly one), can there exist vectors $v$ in a given tangent space $T_{p}M$ that are not tangent to a curve passing through $p$ (essentially, are there cases where $v^{i}\neq\frac{\mathrm{d}x^{i}}{\mathrm{d}t}$)?","['tangent-bundle', 'smooth-manifolds', 'differential-geometry']"
2658770,Suggest some good note on harmonic measure,Can someone please suggest some good book or lecture note on harmonic measure? Thanks in advance. Now I am reading The logarithmic integral I by Paul Koosis.,"['reference-request', 'complex-analysis', 'measure-theory', 'harmonic-analysis']"
2658814,About the definition of Ehresmann connection,"Jeffrey Lee in his book ""Manifolds and differential geometry"" defines the notion of Ehresmann connection as Definition 12.12. A (linear Ehresmann) connection on a vector bundle $\pi: E \to M$ is a smooth distribution $\mathcal{H}$ on the total space $E$ such that $\mathcal{H}$ is complementary to the vertical bundle: $TE = \mathcal{H} \oplus \mathcal{V} \ E$ $\mathcal{H}$ is homogenous: $T_y \mu_r (\mathcal{H}_y) = \mathcal{H}_{ry}$, where $\mu_r: E \to E $ is the multiplication map. He also gives it as an exercise to prove that there is a bijection between such connections and covariant derivatives (which he calls Koszul connections). However M. Postnikov in his book ""Semester IV. Differential Geometry"" gives a definition using local-coordinate representation of $\mathcal{H}$, such that the last condition above is replaced by: the condition that in the trivialization $E|_U \cong U \times V$, with $V$ - vector space, and $x_1,...,x_n$ - coordinates on the base,  $a_1,...,a_m$ - coordinates on the fiber, the forms $\theta_i$ defining $\mathcal{H}=Ann(\{\theta_i\})$, have the form $$\theta_i=da^i + \Gamma_{kj}^{i}(x) a^j dx^k \ \ (*)$$or in other words that if $$\theta_i=da^i + e_k^i(x,a)dx^k \ \ (**)$$ then functions $e_k^i(x,a)$ must be linear in the coordinates on the fiber. Now, it is easy to see that Postnikov's definition satisfies all the conditions of Lee's definition. However if one tries to write coordinate representation of Lee's definition, one can get to the form $(**)$ with functions $e_k^i(x,a)$ being only homogenous in the coordinates on the fiber. M. Postnikov then uses his second condition in the explicit construction showing the equivalence of his notion of connection to the usual covariant derivate 
 - basically, $\Gamma_{kj}^{i}(x) dx^k$ give connection forms $\omega_j^i$ on the base. He also later emphasizes, that, contrary to the case of principle bundles, one cannot make an easy coordinate free definition in the case of vector bundles. From that I suspect that it might be actually impossible to retrieve a covariant derivative out of Lee's definition - there seems to be no way of separating coordinates on the base from coordinates on the fiber in $e_k^i(x,a)dx^k$ to get connection forms on the base. So can you please clarify what is the right take on the notion of Ehresmann connection? Is there a clean coordinate-free way of defining it similar to Lee's approach, contrary to what M. Postnikov is suggesting? Remark. I am familiar with an approach of passing to the frame bundle and defining there a connection using an equivariant fundamental form. I am curious if it is possible to work only on the vector bundle itself.","['connections', 'vector-bundles', 'differential-geometry']"
2658824,"""Beats"" via trig identity or something?","Planning on talking about resonance in DE. The solution to the IVP $$y''+y=\cos(t),\quad y(0)=y'(0)=0$$is $$y=\frac12 t\sin(t).$$Resonance, great. Now what if the forcing function has almost the resonant frequency? If $\alpha^2\ne1$ the solution to $$y''+y=\cos(\alpha t),\quad y(0)=y'(0)=0$$is $$y=\frac1{1-\alpha^2}(\cos(\alpha t)-\cos(t)).$$It's pleasant to note that this is $\sim\frac12 t\sin(t)$ as $t\to0$, and that for every $t$ it tends to $\frac12 t\sin(t)$ as $\alpha\to1$. For a fixed $\alpha$ close to $1$ it's clear if you think about it that $\cos(\alpha t)-\cos(t)$ exhibits ""beats"": there are intervals where it's close to $2\cos(\alpha t)$ and intervals where it's close to $0$. Alas when I say ""you"" here it's also clear that I mean you , not my DE students. Hence the question: Is there a clever way to  write $\cos(\alpha t)-\cos(t)$ so that the beats are evident? Like it's clear that $\cos(\epsilon t)\cos(t)$ shows beats - something analogous to that? Of course $$e^{i\alpha t}-e^{it}=e^{it}(e^{i(\alpha-1)t}-1),$$but I'd rather avoid complex numbers here and sadly the real part of a product is not the product of the real parts. I could use that, or just the sum formula for the cosine, to get a sum of two products, each of which is the sort of thing I want. But writing it as just the product of something with frequency $1$ and something with low frequency would be so much nicer. This must be well studied by people who study such things...","['almost-periodic-functions', 'ordinary-differential-equations']"
2658864,Minimal conditions for being in $L^2$ across probability measures,Let's suppose that we have a random variable $Y$ and two probability measures $\mathbb{P}^0 \sim \mathbb{P}^1$ with Radon-Nikodym derivative $Z = \frac{d\mathbb{P}^1}{d\mathbb{P}^0}$. Let us also assume that $\mathbb{E}^{\mathbb{P}^0} Y^2 <\infty$. Is there a condition to guarantee that $\mathbb{E}^{\mathbb{P}^1} Y^2 <\infty$ that is weaker than $\mathbb{P}^0 \left( Z<C \right)=1$ for some $0<C<\infty$?,"['probability-theory', 'probability']"
2658877,Is there an extension of the reverse triangle inequality to $n$ variables?,"The triangle inequality for real numbers states $|x+y| \leq |x| + |y|$ . This is extended easily on induction to the corresponding result for $n$ variables $$|x_1 + x_2 + ... + x_n| \leq |x_1| + |x_2| + ... + |x_n|, \quad \text{i.e. } \quad \left| \sum_{i=1}^n x_i \right| \leq \sum_{i=1}^n |x_i|.$$ The two variable triangle inequality (and indeed the $n$ variable version) is 'as tight as possible' since equality can be achieved; this happens when either of the variables is zero or when both have the same sign. The reverse triangle inequality goes in the opposite direction; it states $|x-y| \geq ||x| - |y||$ . This inequality is also as tight as possible; again we have equality when either of the variables is zero or they both have the same sign. To make the reverse triangle inequality look more like the normal triangle inequality, we can put $y \mapsto -y$ to give $|x+y| \geq ||x| - |y||$ . This inequality remains as tight as possible, except now we have equality when either of the variables is zero or they have opposite signs. Hence in the two variable case we can say $$||x| - |y|| \leq |x+y| \leq |x| + |y|.$$ It isn't obvious to me whether the reverse triangle inequality can be extended to 3 variables; trying to extend it in the same way one extends the normal traingle inequality doesn't seem to work, and I couldn't find anything about it online. My intuition tells we that there probably isn't an extension because subtraction is not commutative. If it is the case that there is no such generalisation of the reverse triangle inequality, does there exist a different formula for a lower bound on $|x+y+z|$ , or is the best we can do $0 \leq |x+y+z|$ ?","['algebra-precalculus', 'inequality', 'absolute-value']"
2658919,Regular monomorphism in Top category,"I want to prove that regular monomorphism in Top are exactly the embedding. We say that a injective continuous function $f: X \to Y$ between topological spaces is an embedding iff $\tau_X = \{f^{-1}(V): V \in \tau_{\,Y}\}$. We say the a monomorphism $f: X \to Y$ in a category is regular is it arises as the equalizer of to morphism $g,h: Y \to Z$. Now, given an embedding $f: X \to Y$, I consider $Z = \{0,1\}$ with the indescrete topology and the continuous functions $g= 1$ (the constant function) and $h(y)= 1$ iff $y \in f(X)$ (the characteristic function of the set $f(X)$). Clearly, $gf=hf$. How to prove the universal property of equalizer? If for some $f':X' \to Y$ I have $gf'=hf'$, then $f'(x')\in f(X)$, for all $x'\in X'$. Thus $f'(x')=f(x)$ for some unique (by injectivity of $f$) $x \in X$. So that $f^{-1}\circ f'$ make the triangle commutes. Is it a continuous mapping? I now $f'$ is continuous by hp. What about $f^{-1}: f(X) \to X$ Conversely, it suffices to prove that the equalizer of two parallel continuous functions $g,h: Y \to Z$ is just the equalizer set $\{y \in Y:g(y)=h(y)\}$ endowed with the usual subspace topology. Also in this case, I can I prove the universal property? Thanks","['category-theory', 'general-topology']"
2658939,"What am I missing about the definition presented here for an image f[a] in ""Set Theory for the Working Mathematician"" by Krzysztof Ciesielski?","In Set Theory for the Working Mathematician by Krzysztof Ciesielski, he gives the definition of an image as follows. For f : X â†’ Y , A âŠ‚ X, we define f[A] = {f(x): x âˆˆ X} = {y âˆˆ Y : âˆƒx âˆˆ X (y = f(x))} Isn't this just the whole range of the function? My intuition tells me it should be x âˆˆ A, but I don't want to assume a typo. If it is the whole range, why introduce the subset A?",['elementary-set-theory']
2658944,Complex exponentials and the math behind Mercator's projection,"I came across this post by David Bau , in which he reproduces the most widespread Mercator projection as the plot of the complex function $$f(z)=\exp \mathrm i z.$$ The result is familiar: preserving the shape of the different landmasses, at the expense of enlarging the relative area of North America and Europe with respect to Equatorial and Southern Hemispherical regions. This is equivalent to the fanning out of the top of the plot of the function on an applet provided by the same author: Two questions: Is the idea that if for instance, Barcelona being located at $41.3851Â°$ N, $2.1734Â°$ E, the coordinates in the $\mathbb R^2$ complex coordinates would be $41.3851 + 2.1734 \; \mathrm i,$ and after the transformation it would end up being plotted at $f(41.3851 + 2.1734\,\mathrm i)=\exp \left(\mathrm i (41.3851 + 2.1734\,\mathrm i) \right)=-0.09734 + 0.05893\,\mathrm i$ $(-0.09734, 0.05893)$? Is this an approximation or the exact reproduction of the actual cylindrical projection (Mercator) ? Some leg work... in R. Database is here : > head(city)
           city   city_ascii     lat     lng    pop     country iso2 iso3 province
1 Qal eh-ye Now    Qal eh-ye 34.9830 63.1333   2997 Afghanistan   AF  AFG  Badghis
2   Chaghcharan  Chaghcharan 34.5167 65.2500  15000 Afghanistan   AF  AFG     Ghor
3   Lashkar Gah  Lashkar Gah 31.5830 64.3600 201546 Afghanistan   AF  AFG  Hilmand
4        Zaranj       Zaranj 31.1120 61.8870  49851 Afghanistan   AF  AFG   Nimroz
5    Tarin Kowt   Tarin Kowt 32.6333 65.8667  10000 Afghanistan   AF  AFG  Uruzgan
6  Zareh Sharan Zareh Sharan 32.8500 68.4167  13737 Afghanistan   AF  AFG  Paktika ...

> head(coord)
[1] 34.9830+63.1333i 
    34.5167+65.2500i 
    31.5830+64.3600i 
    31.1120+61.8870i 
    32.6333+65.8667i 
    32.8500+68.4167i ... Just plotting the raw latitude and longitude of a bunch of cities in the complex coordinates: coord = complex(real = city$lat, imaginary = city$lng)
plot(Re(coord) ~ Im(coord), pch=20, col=rgb(0,0,0.5,.3)) ... Pretty much the Mercator map... Now transforming the dataset by simply logging the values $\log z$ renders an Azimuthal map with the South America oddly in the center: exi = log(1i * coord)
plot(Re(exi) ~ Im(exi), pch=20, col=rgb(0,0,0.5,.3)) Perhaps the raw data in the link provided started off as logs of the latitude and longitude:","['stereographic-projections', 'cartography', 'differential-geometry', 'differential-topology']"
2659001,Subdifferential of $a^\text{T}x+\alpha\sqrt{x^\text{T}Bx}$ at $x=0$,"I would like to compute the subdifferential of the function $$ f(x)=a^\text{T}x+\alpha\sqrt{x^\text{T}Bx} $$
where $\alpha>0$ and $B$ is symmetric positive definite. Attempted Solution (I am brand new to subdifferentiability) Since subderivatives, like normal derivatives, are linear, we can compute term-by-term. Since the first term is differentiable, we're really interested in computing the subdifferential of $$ \sqrt{x^\text{T}Bx} $$ at $x=0$. This has been asked in the question here , and I gather that the subdifferential should be $$ \{z:\sqrt{z^\text{T}{B}z}\leqslant\lambda_\text{min}\} $$ where $\lambda_\text{min}$ is the smallest eigenvalue of $B$. All together, the subdifferential is $$ \{a+\alpha z:\sqrt{z^\text{T}Bz}\leqslant\lambda_\text{min}\} $$ Is this correct? If so, can anyone elucidate why the subdifferential of $\sqrt{z^\text{T}Bz}$ is what it is? The linked question confused me.","['derivatives', 'convex-analysis']"
2659008,Finding value of product of Cosines,"Finding $$\left(\frac{1}{2}+\cos \frac{\pi}{20}\right)\left(\frac{1}{2}+\cos \frac{3\pi}{20}\right)\left(\frac{1}{2}+\cos \frac{9\pi}{20}\right)\left(\frac{1}{2}+\cos \frac{27\pi}{20}\right)$$ My Try: $$\left(\frac{1}{2}+\cos \frac{\pi}{20}\right)\left(\frac{1}{2}+\cos \frac{3\pi}{20}\right)\left(\frac{1}{2}+\sin \frac{\pi}{20}\right)\left(\frac{1}{2}-\sin\frac{3\pi}{20}\right)$$ So we have $$\left(\frac{1}{2}+\cos \frac{\pi}{20}\right)\left(\frac{1}{2}+\sin\frac{\pi}{20}\right)\left(\frac{1}{2}+\cos \frac{3\pi}{20}\right)\left(\frac{1}{2}-\sin\frac{3\pi}{20}\right)$$ Could some help me to solve it, Thanks in Advanced","['algebra-precalculus', 'products', 'trigonometry']"
2659054,is there a way to prove the Mean-value formulas using complex analysis?,"THEOREM : $U$ is an open subset of $\mathbb{R^n}$ and suppose $u \in C^2(U) $ is harmonic within $U$, then :
  $$u(x)= \def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{B(x,r)}u\,dy = \def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)}u\,dS$$
  for each ball $B(x,r) \subset U$ there's a nice proof in Evan's PDE book which only involves real analysis. since harmonic functions can be implicitly identified as the real or imagniary part of holomorphic functions, how does one prove or atleast reformulate this theorem using complex analysis notions ? any comment, references will be greatly appreciated.","['complex-analysis', 'harmonic-functions', 'partial-differential-equations']"
2659082,Block Matrix Inversion in Wikipedia,"Wikipedia provides two formulas for block-matrix inversion: $$ {\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&-\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\\-(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}&(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\end{bmatrix}},$$ and $${\begin{bmatrix}\mathbf {A} &\mathbf {B} \\\mathbf {C} &\mathbf {D} \end{bmatrix}}^{-1}={\begin{bmatrix}(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&-(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\\-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}&\quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{bmatrix}}.$$ Is it true then that all of the following equalities are true? \begin{align}\mathbf {A} ^{-1}+\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\\
-\mathbf {A} ^{-1}\mathbf {B} (\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; -(\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1} \\
-(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}\mathbf {CA} ^{-1}=&\;-\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1} \\
(\mathbf {D} -\mathbf {CA} ^{-1}\mathbf {B} )^{-1}=&\; \quad \mathbf {D} ^{-1}+\mathbf {D} ^{-1}\mathbf {C} (\mathbf {A} -\mathbf {BD} ^{-1}\mathbf {C} )^{-1}\mathbf {BD} ^{-1}\end{align}","['matrices', 'matrix-equations', 'linear-algebra', 'inverse']"
2659142,Why is $e^x$ the only function that is its own derivative? [duplicate],"This question already has answers here : Are the any non-trivial functions where $f(x)=f'(x)$ not of the form $Ae^x$ (6 answers) Closed 6 years ago . I've heard that $f(x) = Ae^x$ is only function (both elementary and non-elementary) that satisfies the property $f(x)=\frac{df(x)}{dx}$. Is this true (and if it's true, is there a definitive way to prove it)?","['derivatives', 'calculus']"
2659154,Show that Hill's Equation $u'' + a(t)u=0$ if $a(t)<0$ for all $t$ then $u\to\infty$ as $t\to\infty$,"(a) Consider the Hill's equation
  $$u'' + a(t)u = 0,$$
  where $a(t+T) = a(t)$ for all $t$. Show that, if $a(t)<0$ for all $t$, then the solution satisfying the initial condition 
  $$u(0)=u'(0)=1$$
  is unbounded as $t\to\infty$. Suppose $a(t) < 0$ for all $t$. Then, consider the expression,
\begin{align} u'(t) = 1 - \int_0^t a(s)u(s)ds, \tag{1}\end{align}
so that
$$u''(t) = -a(t)u(t)$$
Now we'll show $u'(t) \geq 1$ for all $t \geq 0$. By contradiction, suppose there exists some $t_0 > 0$ such that $u'(t_0) \leq \frac{1}{2}$, then $u(s) = 1 + \frac{s}{2}$ for all $0 \leq s \leq t_0$, then since $a(s) < 0$ for all $s$, and $u(s) \geq 1$ on the interval $(0,s)$, which is a contradiction since $u(t_0)\leq\frac{1}{2}$. Then, the integral $\int_0^t a(s)u(s) \leq 0$ so that $u'(t) \geq 1$ for all $t\geq 0$ and, hence, 
\begin{align*}
\lim_{t\to\infty} u(t) &= \lim_{t\to\infty} \int u'(t) dt \\
&= \lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \\
&\to \infty
\end{align*}
since $a(s)<0$ for all $s$ and $u(s)>0$ for all $s\in(0,t)$ we know that 
$$-\int_0^t a(s)u(s) \geq 0$$
Then,  $$\lim_{t\to\infty} \int \left( 1 - \int_0^t a(s)u(s)ds \right) \geq \lim_{t\to\infty} \int_0^t dt \to \infty$$
so $u$ is unbounded as $t\to\infty$. (b) Next suppose that $a(t)>0$ for all $t$ and 
  $$\int_0^T a(t)dt < \dfrac{4}{T}$$
  It may be shown that all solutions are bounded as $t\to\infty$. Use this result and that of part (a) to estimate the stable and unstable zones in the $\delta-\epsilon$ plane for the Mathieu equation and Missner's equation. I am not sure how to proceed with this part. Also, I do not feel too confident in my proof of part (a) the proof by contradiction felt a little wonky since I never actually reached a contradiction. Any advise would be appreciated.","['real-analysis', 'ordinary-differential-equations']"
2659170,Is the sum of a Darboux function and a continuous function Darboux?,"A Darboux function is a function that has the intermediate value property.  That is a function $f$ such that $$ \forall a,b \in \mathbb{R} : f[a,b] \supseteq [f(a),f(b)] \cup[f(b),f(a)] $$ We define the sum of two functions as such $$ (f+g)(x) = f(x)+g(x)$$ Now the question is: If $f$ is a Darboux function and $g$ is a continuous function, must $f+g$ be a Darboux function as well?","['real-analysis', 'riemann-sum']"
2659192,Generalization of convexity,"This is not a homework, this is just something that came to my mind recently. Assume $f$ is a sufficiently nice function.
We know that
$$\frac{df}{dx} \geq 0 \iff f(x_2) \geq f(x_1) \text{ for } x_2 \geq x_1$$
$$\frac{d^2f}{dx^2} \geq 0 \iff \frac{f(x_1) + f(x_2)}{2} \geq f(\frac{x_1 + x_2}{2})$$ Is there any nice way to generalize this for for higher derivatives? A generalization of nonnegativity of higher order derivatives being equivalent to some short nice condition not involving any derivatives at all? If not generalization, is there at least a nice extension to $\frac{d^3f}{dx^3}$?","['real-analysis', 'inequality', 'calculus']"
2659236,A remarkable(?) condition on sequences of natural numbers,"There is a remarkable condition on increasing sequences of natural numbers $(a_n)_n$:
$$\bigg\lfloor\frac{a_n^2}{a_{n+1}}\bigg\rfloor=2a_n-a_{n+1}\tag 1$$
that - when $n$ is big enough - seems to hold for all increasing sequences $a_n\lesssim p_n$, where $p_n$ is the $n$-th prime. Whether it holds for 
$a_n=p_n$ depends on a weaker form of Oppermann's conjecture:
$$(p_{n+1}-p_n)^2<p_{n+1}\tag 2$$
Trivially $\;p_n^2=p_{n+1}(2p_n-p_{n+1})+(p_{n+1}-p_n)^2$, why $\;(2)\;$ implies $\;(1)\;$ for $a_n=p_n$. In fact, I can't prove much of this and my claims rely on computations. I can't even prove that 
$$\bigg\lfloor\frac{n^2}{n+1}\bigg\rfloor=n-1,\;n>0$$
Except from proofs or counterexamples of certain examples I would like to know if there are something published on this subject. Looks like I was totally wrong about that every smaller sequences did fulfill the conditions $(1)$.","['conjectures', 'reference-request', 'prime-gaps', 'prime-numbers', 'sequences-and-series']"
2659241,Any large group is SQ-universal.,"This result is part of the preliminary section of ""Largeness and SQ-universality of Cyclically Presented Groups,"" by Gerald Williams. The Details: Definition 1: A group is large if it has a finite index subgroup that maps onto the free group $F_2$ of rank $2$ . Definition 2: A group $G$ is SQ-universal if every countable group can be embedded in a quotient group of $G$ . The Question: Prove that any given large group is SQ-universal. Thoughts: Let $G$ be large. Then there exists a subgroup $H$ of $G$ such that $[G: H]$ is finite and there exists epimorphism $\theta: H\to F_2$ . Thus, in a sense, $F_2$ can be seen as a subgroup of $G$ . This would be sufficient (almost) provided that the following hold. The group $F_2$ is SQ-universal. If $A$ is a subgroup of a group $B$ and $A$ is SQ-universal, then $B$ is SQ-universal. The former isn't very clear to me but the latter seems obvious. Please help :)","['abstract-algebra', 'group-theory', 'free-groups', 'quotient-group']"
2659246,"Can we assume that every possible ""process"" has an underlying probability distribution?","To make the question clearer, I will write the context in which I formulated it myself. I was studying the twelve coin puzzle , in which you are given 12 coins and by using a balance certain number of times you need to decide something about the coins.
The details actually don't matter.
The problem I wanted to solve was not the puzzle itself but giving a bound on the maximum number of coins $n$ you can have if you want to solve the puzzle using the balance $k$ times. This is a classical problem, which can be approached using techniques from combinatorics, for example.
For my approach, however, I used Information Theory to get the bound.
My argument started like this: Let $X$ be the random variable representing the possible states of the coins. What we want is to be able to determine $X$ out from $k$ uses of a balance, for any distribution of $X$ .... What remains of the proof, just like the details of this problem itself, are not relevant.
What matters is the question I made to myself at this point: Is it OK to assume some kind of distribution from the states of the coins? This looks weird, because even though the proof gives the same bound you can get with combinatorial methods, these do not assume anything about the underlying distribution of the coins. On the other hand, I'm assuming that the coins must have some kind of distribution! Even if we don't know it, it must be there. I am aware of other contexts where you assume data distributions must exist.
In Machine Learning for example, you make predictions based on the assumption that the underlying data has some distribution, which although unknown, is there! (So statistical arguments can be applied). The question I will do my best to abstract my question in a concise way out from my examples above. Is it natural to regard any outcome from a process as a random variable with an underlying distribution? Are there some studies in this direction? Is this beyond the scope of Mathematics/Statistics itself (and more related to how we interpret results in these fields in ""real life"")?","['information-theory', 'statistics', 'soft-question', 'random']"
2659249,Zolotarev number and commuting matrices,"Recently in a post ( link ) upper bounds on the singular values $\sigma_j(X)$ of a matrix $X$ have been considered. To restate the central observation, it says that if $AXâˆ’XB=F$ for $A$ and $B$ normal matrices, then we have that
$$Ïƒ_{1+Î½k}(X)â‰¤Z_k(Ïƒ(A),Ïƒ(B)) \sigma_1, \;\;\;\;\;\; Î½=rank(F),$$
where $Ïƒ(A)$ and $Ïƒ(B)$ are the spectra of $A$ and $B$, respectively, and $Z_k(E,F)$ the Zolotarev number. This nice result is from here . In the reference, as far as I can see the examples are throughly for disjoint $E$ and $F$. Are there any result when $E$ and $F$ are not disjoint?","['eigenvalues-eigenvectors', 'matrices', 'approximation-theory', 'rational-functions', 'singular-values']"
2659254,Why a n-th order linear homogeneous ODE has exactly n linearly independent solutions?,After some research I cannot find an answer that satisfies me for the question above. I know about the Cauchy criterion but I don't think that it proves the fact that there are exactly n linearly independent solutions to an n-th order linear homogeneous ODE. Is it a dimension problem ? (like maybe we could construct an isomorphism from the ODE to Rn prove that ?),"['ordinary-differential-equations', 'linear-algebra']"
2659264,For what $\alpha$ does $\iint_D\frac{1}{(x+y)^{\alpha}}\ dxdy$ converge?,"For what values of $\alpha$ does $$\iint_D\frac{1}{(x+y)^{\alpha}}\
 dxdy$$ converge? $D=\{0\leq y \leq 1-x, \quad 0 \leq \ x \leq 1\}.$ The double integral can be written as $$\int_{0}^{1}\left(\int_{0}^{1-x} \frac{1}{(x+y)^{\alpha}} \ dy\right)dx.$$ How does one find a primitive to the inner integral?",['multivariable-calculus']
2659288,Proof that ratio of vertices to edges in an infinite (square) grid is 1:2?,"I'm making the statement that the ratio of vertices to edges in an infinite square grid is $1:2$. I need this fact for deducing further theorems specific for my problem, however I can't find any theory on infinite grids. I would prefer to cite some literature on that (maybe also for hexagonal and triangular grids). If no formal literature exists that covers that issue I would want to write the proof. My thinking so far is each vertex has $4$ edges connected to it and each edge $2$ vertexes, which makes for a $2$ to $4$ ratio $-> 1:2$. However I feel this is not formal enough and the proof should involve infinity of the grid. Please post possible literature or a formal proof if you think mine isn't sufficient.","['infinity', 'geometry']"
2659319,Reduction of order where $y_1=(\beta \tan^2x+1)$ with $\beta \in R$,"I need to apply the reduction of order into differential equation $$\cos^2xy''-6y=0$$where the first solution is of the form $$y_1=(\beta \tan^2x+1)$$ with $\beta \in R$. I started solving equation with reduction of order, however I don't know how should I use the knowledge of ""$\beta \in R$."" $$y_1=(\beta \tan^2x+1)$$
$$y_2=u\cdot y_1=u\cdot(\beta \tan^2x+1)$$
$$y_2'=(u\cdot y_1)'= \dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x)$$ $$y_2''= (\dot u(\beta \tan^2x+1)+u(2\beta \cdot \tan x\cdot \sec^2x))'= \ddot u (\beta \tan^2x+1)+ \dot u(4\beta \tan x \cdot \sec^2x) +u(2\beta \sec^4x+4\beta \tan^2x \sec^2x) $$ After inserting $y_2, y_2',y_2''$ into   $\cos^2xy''-6y=0$ I got the form where any of the element of the equation want to reduce. Therefore, I would like to ask should I assume do with $\beta$, can I assume that $\beta$ equals for instance 1?","['reduction-of-order-ode', 'ordinary-differential-equations']"
2659353,Uniform convergence of alternating series,"If the sequence of functions $f_{n}: X \longrightarrow \mathbb{R}$  is such that $f_{1} \geq ... \geq f_{k} \geq ...$ and $f_{n} \longrightarrow 0$ uniformly in $X$. Prove that $\displaystyle \sum(-1)^{n}f_{n}$ uniformly converges in $X$ Since $f_{n}$ is uniformly convergent, the convergence of $f$ not dependes of $x$. Thus, is the proof reduced to the Leibniz test?","['real-analysis', 'uniform-convergence']"
2659385,How many degrees of freedom do orthogonal skew-symmetric matrices have?,"$n$ by $n$ real orthogonal matrices have $n (n-1)/2$ degrees of freedom . So do the skew-symmetric matrices. But what about matrices that are both skew-symmetric and orthogonal? Is the number of such matrices finite for any given $n$ ? If not, how many degrees of freedom do they have? We know that such matrices exist only if $n$ is even, in which case they are equal to $$\bigoplus_{i=1}^{n/2}\begin{bmatrix} 0 & 1\\
-1 & 0\end{bmatrix}$$ up to an orthogonal change of basis . However, the number of their degrees of freedom is still unclear to me.","['matrices', 'orthogonal-matrices', 'linear-algebra']"
2659391,Why does the commutative property of addition not hold for conditionally convergent series?,"I learned about the Riemann rearrangement theorem recently and I'm trying to develop an intuition as to why commutativity breaks down for conditionally convergent series. I understand the technique used in the theorem, but it just seems really odd that commutativity breaks down to me. It doesn't happen for other properties -- associativity holds for convergent series and commutativity holds for absolutely convergent series. What makes this particular property for this particular kind of convergent series ""special"" in this way? I'm aware of this question: why does commutativity of addition fail for infinite sums? but the answers haven't been helpful for me. JiK's answer is ""why would it?"", and goes on to talk about why you can't apply rules to infinite series, but this seems erroneous because associativity holds for convergent series and commutativity holds for absolutely convergent series. Fly by Night just explains conditionally vs absolutely convergent, josh314 says it applies to finite sums only which isn't true, and Denis just explains the theorem again, and Barry Cipra seems to have a similar kind of argument as JiK's and problematic for similar reasons. Is there a good way to understand why this is happening intuitively? or is this the wrong way to think about it and I should just accept that it's happening even though it's unintuitive? It's hard for me to just let it go without an intution because it seems like math starts to ""break"" here.. the theorem is sound but this property no longer holds in this case, which is really strange to me Does anyone know of any resources that go into depth on this kind of question?","['conditional-convergence', 'real-analysis', 'sequences-and-series', 'convergence-divergence']"
2659397,"Matrix exponential, containing a thermal state","Define an infinite matrix $$ M = 
\begin{bmatrix}
0 & -1 & 0 & 0 & \cdots \\
1 & 0 & -2 & 0 & \cdots \\
0 & 2 & 0 & -3 & \cdots \\
0 & 0 & 3 & 0 & \cdots \\
\vdots & \vdots & \vdots & \vdots & \ddots \\
\end{bmatrix}$$ Numerically, I've found that the first column of $\exp(M)$ is given by $\alpha(1,e^{-\lambda},e^{-2\lambda},e^{-3\lambda},\dots)^T$, where $\lambda \approx 0.27$ and $\alpha = \sqrt{1-e^{-2\lambda}} \approx 0.65$. Question: How to prove analytically that the first column of $\exp(M)$ has the stated form? [Context: This question comes from a quantum mechanical model of two harmonic oscillators coupled by a Hamiltonian of the form $\hat{H} \propto \hat{a}_1^\dagger \hat{a}_2^\dagger - \hat{a}_1 \hat{a}_2$, where $\hat{a}_i$ is the lowering operator for oscillator $i$.  The matrix $M$ is precisely this Hamiltonian on the subspace spanned by $\{\left|nn\right>\}$.  I've tried using a BCH approach, but was discouraged by the fact that the commutators are not very cooperative.  The exponential coefficients in the first column indicates (roughly) a thermal state.] Edit: I cross-posted this on mathoverflow , and got a beautiful solution by Jochen Glueck.  The exact value of $\lambda$ is $-\ln(\tanh(1))$.","['matrices', 'matrix-exponential', 'linear-algebra', 'infinite-matrices']"
2659414,How many positive integer solutions satisfy the condition $y_1+y_2+y_3+y_4 < 100$,In preparation for an upcoming test I have come across the following problem and I am looking for some help with it just in case a question of its kind comes up on a evaluation. Thanks! How many positive integer solutions satisfy the condition:$$y_1+y_2+y_3+y_4 < 100$$,"['combinatorics', 'discrete-mathematics']"
2659454,Finite index subgroups of surface groups,"It is not hard to see (say from the perspective of covering spaces) that there are a finite number of subgroups of a fixed finite index $n$ in a finitely generated free group $F_n$.  Given a closed oriented surface $F$, are there a finite number of subgroups of $\pi_1(F)$ of a given index?  If so is there a formula for this number?","['algebraic-topology', 'geometric-topology', 'group-theory']"
2659474,Inside Roots of Determinant of Polynomial Matrix,"Let ${\bf A}(x)$ be an $(n-1) \times n$ polynomial matrix and ${\bf b}(x)$ be a $1 \times n$ polynomial vector. Suppose that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) \end{bmatrix}=0 $$ does not have a root inside the unit circle. What condition must a $1 \times n$ constant vector ${\bf c}$ satisfy in order for $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix}=0 $$
to have a root inside the unit circle? I guess it is useful to use the fact that $$ \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x) - {\bf c} \end{bmatrix} = \det\begin{bmatrix} {\bf A}(x) \\ {\bf b}(x)  \end{bmatrix}-\det\begin{bmatrix} {\bf A}(x) \\  {\bf c} \end{bmatrix},$$ which means, in particular, that ${\bf c}={\bf b}(x_0)$ for some $x_0$ inside the unit circle is a sufficient condition.","['matrices', 'linear-algebra', 'determinant']"
2659475,Bijective Proof Involving the Motzkin Numbers,"In answering a question involving tree counting, I ran across the Motzkin numbers, which I had never encountered before.  The problem involved counting the number of rooted, ordered trees with $n$ nodes, subject to the condition that no nodes has degree greater than 3.  Basically, I solved the problem by finding a recurrence, computing some terms, and sticking the result in OEIS.  This turned up the Motzkin numbers, except that I had $T_n=M_{n-1}.$  (Now that I think of it, the correspondence would have been perfect if I'd changed to counting trees with $n$ edges.)  Except for the difference in index, the two sequences had the same initial values, and satisfied the same recurrence, so they are identical. Anyway, I see on Wikipedia that one way to define $M_n$ is the Motzkin number for $n$ gives the number of routes on the upper right quadrant of a grid from coordinate $(0, 0)$ to coordinate $(n, 0)$ in $n$ steps if one is allowed to move only to the right (up, down or straight) at each step but forbidden from dipping below the $y = 0$ axis. I have been trying to find a bijective proof that the number of such paths equals the number of admissible trees with $n$ edges, but I'm not having any luck.  I can see that going up corresponds to a node with two children, going straight to an node with one child, and going down to a node with no child, but I can't figure out how to make the correspondence in detail. I've drawn the $9$ trees with $4$ edges, and I've been comparing them to the paths pictured on Wikipedia, and I can match up some of them, but not all.  For example, there are only two trees that have two nodes of degree $3$, and there are only two paths shown that go up twice, so the pairs must correspond.  However, the two trees are symmetric, in that they correspond if left and right are interchanged, and I see no such symmetry in the path. I've also thought about traversing the tree by depth-first search say, and somehow encoding the edges as Up, Straight, or Down, but I haven't been able to make that work out. I've tried to find a proof on the Web, but everything I run across seems to be dealing with much more advanced problems. Please give me a proof, or a hint, or a reference.",['combinatorics']
2659520,"$f(x, \theta)= \frac{\theta}{x^2}$ with $x\geq\theta$ and $\theta>0$, find the MLE","Let $X$ be a random variable with density $$f(x, \theta)= \frac{\theta}{x^2}$$ with $x\geq\theta$ and $\theta>0$. a) Show if $S=\min\{x_1,\cdots, x_n\}$ is a sufficient statistics and if it is minimal. b) Find the Maximum Likelihood Estimator of $\theta$ and tell if it is unbiased. c) Find the distribution of $S$ and tell if there is an unbiased estimator of the form $cS$ for some $c$. attempt: There are several problems. $S$ doens't look like a sufficient statistics cause $L(\theta|x_1, \cdots, x_n)= \frac{\theta^n}{(x_1\cdots x_n)^2}$ doesn't seem to be known if we know the minimum. Morover there is no maximum for that function so I can't find MLE. Thanks!","['statistics', 'probability', 'statistical-inference', 'probability-distributions']"
2659526,"Proving that if two integers have opposite parity, then their product is even","If two integers have opposite parity, then their product is even. Proof Method: Direct Proof If two integers have opposite parity, then one is even and the other is odd. Suppose: $a$ is an even integer and $b$ is an odd integer, then by definition of even and odd integers $$a = 2m, \quad b = 2n+1,$$ while $m$ and $n$ are integers. $$ ab = 2m(2n+1)= 4mn+2m = 2(2mn+m)
$$
Let $c = 2mn+m$ be an integer, then $ab=2c$ is even Therefore, the product of two opposite parity integers is even Thank You!","['proof-verification', 'discrete-mathematics']"
2659565,Find the coefficient of $x^{18}$ in $(x + x^2 + x^3 + x^4 + x^5)(x^2 + x^3 + x^4 +\cdots)^5 $,"Find the coefficient of $x^{18}$ in $$(x + x^2 + x^3 + x^4 + x^5)(x^2 + x^3 + x^4 +\cdots)^5 $$ This is the first time coming across a generating function question and am not quite sure how to solve this and am looking for some help, thanks!","['generating-functions', 'combinatorics', 'discrete-mathematics']"
2659651,Variance of the Sum of $3$ Independent Trials,"Note: I was able to solve this question while I was typing it up, but I'll just leave this here in case someone else needs it. Feel free to improve on it in the answers. Suppose $3$ marksmen shoot at a target. The $i$th marksman fire $n_i$ times, hitting the target each time with probability $p_i$, independently of his other shots and the shots of the other marksmen. Let $X$ be the total number of shots the target is hit. Find $Var(X)$. Let $X_i$ be the number of times the $i$th marksman hits the target. $Var(X)$ $= Var(X_1 + X_2 + X_3)$ $= Var(X_1) + Var(X_2) + Var(X_3)$ since the shots of one marksman is independent from the others. $= \sum_{i=1}^{3} Var(X_i)$ Let $I_i$ be the indicator that the $i$th shot hit. $Var(X_i)$ $= Var(I_1 + I_2 + \ldots +I_{n_i})$ $= Var(I_1) + Var(I_2) + \ldots + Var(I_{n_i})$ since each shots of one marksman is independent of his other shots. $= \sum_{j=1}^{n_i}Var(I_j)$ Now we have: $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}Var(I_j)$ $Var(I_j)$ $=E(I_j^2) - (E(I_j))^2$ $= (1^2 \cdot p_i+0^2 \cdot (1- p_i)) - p_i^2$ considering each shot has the same probability ($p_i$). = $p_i - p^2_i$ Thus, $Var(X)= \sum_{i=1}^{3} \sum_{j=1}^{n_i}(p_i - p^2_i) = \sum_{i=1}^{3}n_i(p_i - p^2_i) = \sum_{i=1}^{3}n_ip_i(1 - p_i)=\sum_{i=1}^{3}n_ip_iq_i$ where $q_i = 1 - p_i$","['statistics', 'variance']"
2659699,set-theoretic difference of multisets,"What is the result of $A \backslash B$ , if $A$ and $B$ are multisets? For instance, if $A = \{1,1,3\}$ and $B = \{1,2\}$, would the result of $A \backslash B$ be $\{1,3\}$ or $\{3\}$?",['elementary-set-theory']
2659730,Writing a vector field on SO(3) in terms of local coordinates,"I'm looking at control systems on SO(3) of the form $\dot{g} = gf(g) + gh(g)u$, where $gf(g)$ is the drift vector field and $gh(g)$ is the control vector field. I'm interested in expressing these fields in terms of some local coordinates, i.e. $$gh(g) = \sum_{i}X^i\frac{\partial}{\partial x^i}$$ How would I go about computing the $X^i$s? As a specific example, suppose $h = L_x$ (an element of the standard basis of $\mathfrak{so}(3)$), and we use geodesic polar coordinates with the exponential map as the chart function. What would the local coordinate representation of $gh$ look like?","['control-theory', 'riemannian-geometry', 'differential-geometry']"
2659763,What is $e^{\int{\frac{1}{x}dx}}$?,"When doing integrating factors in order to solve differential equations, we often have something in a form similar to $$e^{\int{\frac{1}{x}dx}}$$ I know that $\int{\frac{1}{x}dx} = \ln{|x|}$, but for some reason, I often times see people simplifying $e^{\int{\frac{1}{x}dx}}$ down to just $x$. Is $e^{\int{\frac{1}{x}dx}}$ equal to $|x|$ or $x$?","['indefinite-integrals', 'integrating-factor', 'integration', 'ordinary-differential-equations']"
2659822,Intuitive proof of the sin and cos addition formulas? [duplicate],"This question already has answers here : How can I understand and prove the ""sum and difference formulas"" in trigonometry? (17 answers) Closed 6 years ago . I refer to these formulas: $\sin(xÂ±y)$ and $\cos(xÂ±y)$ Is there an obvious or intuitive proof to derive their simpler identities in terms of $\sin(x), \sin(y), \cos(x), \cos(y)$? I am tagging this with complex analysis because I'm open to such an explanation if it's simple enough to understand.","['algebra-precalculus', 'complex-analysis', 'trigonometry']"
2659873,How to find the set of $c$ for which the Julia set of $x^2+c$ completely lies in $\mathbb{R}$?,"How to find the set of $c$ for which the Julia set of $x^2+c$ completely lies in $\mathbb{R}$? I know that $c=-2$ must satisfies this because $J(x^2-2)=[-2,2]\in \mathbb{R}$. However, for other $c$, it's quite hard to analyze. Also, Is the part of a $J$ in $\mathbb{R}$ always a fractal except for $c=0$ or $c=-2$?","['complex-analysis', 'complex-dynamics', 'fractals']"
2659887,"When either $\frac{\partial M}{\partial y}$, or $ \frac{\partial N}{\partial x}$ do not exists, how to check whether given ODE is exact or not","For a given ODE, such as $$M(x,y) dx + N(x,y) dy = 0$$ if $\frac{\partial M}{\partial y} \not = \frac{\partial N}{\partial 
 x}$, how can we determine whether the given ODE is exact or not ?
  Moreover, how can we find $F(x,y)$ ? To be clear, (I guess) we can always integrate $M$ wrt $x$, and then take the derivative of the resulting function wrt $y$, and look for a possible $g(y)$, where $g(y)$ is the intagration ""constant"" coming from the integration of $M$ wrt $x$, but my question is that is there any other methods ? Moreover, to check whether there exists a $g(y)$ satisfying the above conditions leads an integration, which might not be possible or easy all the time, so in such cases, what can we do ? Edit: I'm not talking about non-exact ODEs, I'm basically asking what to do when either $M$ or $N$ is not belong to $C^1$, as I have explain in the comments of the answer of @dezdichado.",['ordinary-differential-equations']
2659905,Proof of a measure as finitely additive but not countable additive,"I am (self) studying probability theory and measure using the book from Ash, R. et al. [ 1 ]. I am trying to solve one of the basic problems (Section 1.2, problem 3) but with no avail...here is the link to the problem: Let $\Omega$ be a countable infinite set, and let $\mathcal{F}$ be the field consisting of all finite subsets of $\Omega$ and their complements. If $A$ is finite, set $\mu(A) = 0$, and if $A^{c}$ is finite, set $\mu(A) = 1$. (a) show that $\mu$ is finitely additive but not countable additive. (b) show that $\Omega$ is the limit of an increasing sequence of sets $A_{n} \in \mathcal{F}$, with $\mu(A_{n}) = 0$ for all $n$, but $\mu(\Omega)=1$ I would like to get some help as where to begin with...",['measure-theory']
2659973,Find the maximum value of given expression,"$$ x+2y+3z = 15 $$ Find the maximum value of
$$ 6(1+x)yz + x(2y+3z) $$ I substituted $2y+3z=15-x$
And tried to use AM GM to find maximum value of $xyz$ but they don't occur at same value. I know that this is not the right way to do it but I don't have any idea on how to do these type of questions. How do I proceed? Edit Do note that $x,y,z$ are positive real numbers.","['a.m.-g.m.-inequality', 'optimization', 'substitution', 'multivariable-calculus', 'maxima-minima']"
2659991,Integers divisibility,"Let $x_1, x_2, \cdots, x_n$ be integers such that $$5040 \mid x_1+x_2+\cdots+x_n,\\
5040 \mid x_1^3+x_2^3+\cdots+x_n^3,\\
5040 \mid x_1^5+x_2^5+\cdots+x_n^5,
$$ Then prove that 
$$5040 \mid x_1^7+x_2^7+\cdots+x_n^7.$$ I don't even know where to start... All I know is that $5040$ is a highly composite number with $60$ divisors and that it has all consecutive primes $2$, $3$, $5$ and $7$ in its factorization.",['number-theory']
2659999,Indeterminate form: $0^0$,"It is known that $\lim_{x\to 0^+}x^x=1$,
$\lim_{x\to 0^+}0^x=0$ and $\lim_{x\to 0}x^0=1$. So sometimes $0^0$ is left undefined, sometimes defined as $1$. A question then come to my mind: Given $(s_n)_{n\in\mathbb N},(t_n)_{n\in\mathbb N}$ are sequences$,\forall n\in\mathbb N,(s_n)\gt0\land(t_n)\gt0$, $\lim_n s_n=\lim_n t_n=0$. Given $a\in[0,1]$,When will $(s_n^{t_n})\to a$ ? Even giving more example may help. I have found some example, 1)$\forall n\in\mathbb N, s_n=t_n=\frac{1}{n},(s_n^{t_n})\to 1$ 2)$\forall a\in(0,1), \forall n\in\mathbb N, s_n=a^n\land t_n=\frac{1}{n},(s_n^{t_n})\to a$ 3)$\forall n\in\mathbb N, s_n=\frac{1}{n^n}\land t_n=\frac{1}{n},(s_n^{t_n})\to 0$ I think it sounds very 'easy' for $(s_n^{t_n})\to 1$, but are there similarity between the examples? Or Can I even ask for: If $\lim_{x\to 0^+}f(x)=\lim_{x\to 0^+}g(x)=0, a\in\mathbb R$, when will $\lim_{x\to 0^+}f(x)^{g(x)}=a$? Thanks.","['real-analysis', 'examples-counterexamples', 'sequences-and-series', 'limits']"
2660031,Cobordisms and compactness,"Two compact $n$-manifolds $M_0, M_1$ are said to be cobordant if there is an $(n+1)$-dimensional compact manifold $M$ such that $\partial M = M_0 \sqcup M_1$. What is the necessity of compactness here? From a naive perspective (i.e. mine) it seems as though we can just remove compactness from the definition, and just use two arbitrary $n$-manifolds $M_0, M_1$ instead.","['cobordism', 'differential-geometry']"
2660067,Meaning of complex valued equilibria of ODE's,"In the study of differential equations, we often have to determine the equilibrium points of an ordinary differential equation (ODE). Let us assume we have the following differential equation $$\dot{x} = x(x^2+1).$$ For an equilibrium point, we have to determine all the values $x_\text{eq}$ such that $\dot{x}_\text{eq}=0.$ The algebraic solutions for the given example are $$x_{\text{eq};1} = 0$$
$$x_{\text{eq};2,3} = \pm i,$$ in which $i$ is the imaginary unit. From standard stability theory, we know that the first equilibrium is unstable. The other two equilibria are not investigated because they are not real. From a practical point of view, this makes sense. But in the spirit of a quote from Einstein 'God does not play dice with the universe' I am not fully satisfied with the practical point of view. I am interested in the theoretical meaning of these complex valued
  equilibria. I do not (want to) believe that there is absolutely no real relevance of these equilibria to the behaviour of the real system . I
  am looking forward to inspiring and enlightening answers from our
  clever MathStackExchange users :).","['complex-analysis', 'stability-in-odes', 'ordinary-differential-equations', 'stability-theory']"
2660097,Integrating a surface in a cylinder,"Consider 
$$\Phi: \mathbb{R}^2\to   \mathbb{R}^3,~ \Phi(u,v)=\frac{1}{2}\begin{pmatrix}u+v\\u-v\\2uv\end{pmatrix}$$
I want to find the volume of the area $\Phi(\mathbb{R}^2)$ inside the cylinder $C:x^2+y^2<4$. First I found the Jacobi-matrix of $\Phi$
$$J(u,v)_\Phi=\frac{1}{2}\begin{pmatrix}1&1\\ 1&-1\\ 2v&2u\end{pmatrix},~J(u,v)_\Phi^T=\begin{pmatrix}\frac{1}{2}&\frac{1}{2}& 1v\\ \frac{1}{2}&-\frac{1}{2}&1u\end{pmatrix}$$ So the Metric-Tensor and its determinant are
$$\mathscr{G}(\Phi)=J(u,v)_\Phi^T\cdot J(u,v)_\Phi=\begin{pmatrix}\frac{1+2v^2}{2}&vu\\ vu&\frac{1+2u^2}{2}\end{pmatrix},\quad \mathscr{g}(\Phi)=\frac{2v^2+2u^2+1}{4}$$ But Im not sure how to find the   volume;","['real-analysis', 'differential-geometry']"
2660103,"How to calculate standard deviation in case of known x values, assuming a normal distribution?","I have learned that 17% of the vehicles within a certain population drive less than 7. 500 km per year. On average, a given vehicle in that population will drive 13.300 km per year. Assuming a normal distribution, how (mathematically, using a graphic calculator, or using Excel) can I calculate the standard deviation for the population at hand 1 ?","['statistics', 'standard-deviation', 'normal-distribution']"
2660140,Integral $\int \sqrt{1+x^2}dx$,"I was trying to do this integral
$$\int \sqrt{1+x^2}dx$$
I saw this question and its' use of hyperbolic functions. I did it with binomial differential method since the given integral is in a form of $\int x^m(a+bx^n)^p\,dx$ and I spent a lot of time on it so I would like to see if it can be done this way and where did I go wrong.
$$\int(1+x^2)^\frac{1}{2}dx$$
$m=0$, $n=2$, $p=\frac{1}{2}$ Because $\frac{m+1}{n}+p \in \mathbb Z$ I used substitution $x^{-2}+1=t^2$. From there I got: $$-\frac{dx}{x^3}=t\,dt$$
$$x=\frac{1}{\sqrt {t^2-1}}$$
$$t=\frac{\sqrt{1+x^2}}{x}$$ I expanded the original with $x^4$: $$\int \frac{x^4\sqrt{1+x^2}dx}{x^4}=\int \left(\frac{1}{\sqrt{t^2-1}}\right)^4t(-tdt)=\int\frac{-t^2dt}{(t^2-1)^2}$$ Now I used partial integration: $u=t$, $du=dt$, $dv=\frac{-tdt}{(t^2-1)^2}$, $v=\frac{1}{2(t^2-1)}$ Then $$\begin{align}
\int\frac{-t^2dt}{(t^2-1)^2}
&=\frac{t}{2(t^2-1)}-\frac{1}{2}\int \frac{dt}{t^2-1}= \\
&=\frac{t}{2(t^2-1)}-\frac{1}{2}\frac{1}{2}\ln\frac{t-1}{t+1}= \\
&=\frac{\frac{\sqrt{1+x^2}}{x}}{2\left(\left(\frac{\sqrt{1+x^2}}{x}\right)^2-1\right)}-\frac{1}{4}\ln\frac{\frac{\sqrt{1+x^2}}{x}-1}{\frac{\sqrt{1+x^2}}{x}+1}= \\
&=\frac{x\sqrt{1+x^2}}{2}-\frac{1}{4}\ln\frac{\sqrt{1+x^2}-x}{\sqrt{1+x^2}+x}=\\
&=\frac{x\sqrt{1+x^2}}{2}-\frac{1}{4}\ln\frac{\sqrt{1+x^2}-x}{\sqrt{1+x^2}+x}\cdot\frac{\sqrt{1+x^2}-x}{\sqrt{1+x^2}-x}=\\
&=\frac{x\sqrt{1+x^2}}{2}-\frac{1}{4}\ln(\sqrt{1+x^2}-x)^2=\\
&=\frac{x\sqrt{1+x^2}}{2}-\frac{1}{2}\ln(\sqrt{1+x^2}-x)+C
\end{align}$$ The solution is $$\frac{x\sqrt{1+x^2}}{2}+\frac{1}{2}\ln(x+\sqrt{1+x^2})+C$$ 
My solution looks very similar, so where did I go wrong?","['indefinite-integrals', 'integration', 'calculus']"
2660175,Is this notation correct? $\sqrt {-25}=Â±5i$,"I know that, It is correct $$\sqrt {-25}=5i$$ I know that, It is wrong $$\sqrt {-25}=-5i$$ Now, Is this notation correct? $$\sqrt {-25}=Â±5i?$$","['algebra-precalculus', 'notation', 'complex-numbers']"
2660176,If there is a point $p \in M$ such that $f(p) = g(p)$ and $df_p = dg_p$ then $f = g$.,"Let $M,N$ Riemannian manifold conected and $f,g:M \rightarrow N$ two isometries. If there is a point $p \in M$ such that $f(p) = g(p)$ and $df_p = dg_p$ then $f = g$. Comments: I'm considering the set $A = \{q \in M ; f(q) = g(q) \ \text{and} \ df_q = dg_q\}$. The set is not empty and closed, but I can not show it is open.","['riemannian-geometry', 'geometry']"
2660180,Is there difference between finitely presented groups and finitely generated groups?,A group is said to be finitely generated if it can be generated by a finite set of generators. Question : Is there difference between finitely presented groups and finitely generated groups?,"['group-theory', 'definition']"
2660202,Does an ordinal number $\beta$ such that $\alpha<\beta$ and $\beta<\alpha +1$ exist for some another ordinal number $\alpha$?,"My question is simple: is there a pair of ordinals $\alpha,\beta$ such that $\alpha<\beta$ and $\beta<\alpha+1$? I'm quite sure the answer is no, but I'm a bit afraid of limit ordinals. For me that result follows the fact that given $\alpha$ an ordinal number, $\alpha +1$ is the least element of the set $$\{\beta:\alpha<\beta \} .$$ However, I haven't found a nice explanation for what a limit ordinal is. I only know what Wiki says: limit ordinal is an ordinal $\omega$ such that there isn't any other ordinal $\alpha$ satisfying $\alpha+1=\omega$. Hence I'm not totally sure. Because larg ordinals may be very strangers. What do you think? Thanks. What do you think?","['elementary-set-theory', 'ordinals']"
2660211,"First order non linear differential equation , non separable","First time I run into an equation of this ""form"", for non linear equations I know separation of variables and substitution. It doesn't seem to be homogenous of degree $0$ so the substitution $v=\frac{x}{t}$ is out of the question?
Applying $\ln$ to both sides didn't do much either $$x'(t)=e^{t+x(t)}-1,\quad x(0)=1$$ Wolfram gives the solution $$x(t)=-\ln(c_1-t)-t$$",['ordinary-differential-equations']
2660219,About proof of fundamental lemma of calculus of variation,"I have found several proofs of fundamental lemma of calculus of variation for $C^1$ functions, but could someone help me to show this lemma with the following hypothesis?: Let be $g:[x,y]\rightarrow{\mathbb{R}}$ a continuous function such that $\int_x^yg(t)h(t)=0$  $\forall{h\in{C^\infty_c(x,y)}}$, show that $g=0$. Thanks.","['functional-analysis', 'real-analysis', 'analysis']"
2660226,Is any infinitesimal extension of an affine scheme affine?,"If $X$ is an affine scheme with coherent sheaf $\mathcal{F}$, $Y$ any scheme with $$0 \rightarrow \mathcal{F} \rightarrow \mathcal{O}_Y \rightarrow \mathcal{O}_X \rightarrow 0$$ exact such that $\mathcal{F}^2=0 \subset \mathcal{O}_Y$. Is $Y$ necessarily affine too? In more details, if $\mathcal{F} \subset \mathcal{O}_Y$ is a sheaf of ideals that squares to zero, such that the schemes $X$ with structure sheaf $\mathcal{O}_X$ is isomorphic to the scheme $Y'=(Y,\mathcal{O}_Y/\mathcal{F})$, is then $Y$ with its own structure sheaf affine?","['deformation-theory', 'algebraic-geometry']"
2660247,Relation between the roots of a function,"I have this question from my exam where it's asked to find the sum: $$S=\sum_{k=1}^n \frac{1}{(1-r_k)^2}$$
where $r_k$ are the roots of
$$f(x)=x^n-2x+2\quad,n\ge3$$
I recalled this relation
$$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$
and quickly realised that
$$\frac{d}{dx}\frac{1}{x}=-\frac{1}{x^2}$$
and using derivative would easily produce my answer. Indeed
$$\frac{d}{dx}\left( \frac{f'(x)}{f(x)} \right)=\frac{f''(x)f(x)-(f'(x))^2}{f(x)^2}=-\sum_{k=1}^n \frac{1}{(x-r_k)^2}$$
Evaluating at $x=1$ gives
$$S=\frac{(n-2)^2-n(n-1)}{1^2}=4-3n$$
However after the exam I realised that I have no ideea why
$$\frac{f'(x)}{f(x)}=\sum_{k=1}^n \frac{1}{x-r_k}$$
and I just memorized it, is there a nice way to show it? 
And of course maybe a more elegant solution to this exam question?","['roots', 'polynomials', 'calculus', 'functions']"
2660280,Smallest circumscribed polygon around regular polygons,"Given a regular $n$-gon $Q$, there are many polygons $P$ that entirely contain $Q$, and such that all $n$ vertices of $Q$ lie on edges of $P$. These circumscribing polygons $P$ have different numbers of edges. What is the smallest number of edges possible for a circumscribing polygon? In the following pictures, I present the solutions for $n=4,5,6$ for which the smallest circumscribed polygon are triangles, and for $n=7$ for which the smallest circumscribed polygon is a quadrilateral. Is the solution of this problem known for general $n$? Thank you for your help!","['combinatorics', 'polygons', 'geometry']"
2660281,Show $\lim_{n\to \infty}{|A_{n}|+|A_{n-2}| \above 1.5pt |A_{n-1}|}=2$,"Question: Let $A(n)$ be a finite square $n \times n$ matrix with entries
  $a_{ij}=1$ if $i+j$ is a  prime; otherwise equals to $0$. I write $|A(n)|$ to count the number of $1$'s in $A(n)$. Is it true, possibly trivially, and how can we show that $$\lim_{n\to \infty}{|A_{n}|+|A_{n-2}| \above 1.5pt |A_{n-1}|}=2$$ The sequence of $A(n)$'s can be found here in Sloan. The graph below illustrates the limit. I believe I have shown the question is true, conditionally using a parity argument along with Bertrand's Postulate. Solution: It is easy to verify that $A(n)$ has exactly $2n-1$ skew
  diagonal . Two small lemmas. $\underline{\text{lemma 1}}:$  Observe $a_{ii}=1$ if and only if
  $i=1$. Consequently $a_{11}$ is the only entry on the main diagonal of
  $A(n)$ equal to $1$. Note $A(n)$ is symmetric since addition is commutative. So the count of $1$'s below the main diagonal equals the counts of $1$'s above the main diagonal and the two counts combined form an even number. If we remember to add the sole $1$ for the entry $a_{11}$ then there are an odd number of entries in $A(n)$. In particular $|A(n)|$ is always odd. $\underline{\text{lemma 2}}:$ As mentioned earlier every matrix $A(n)$
  has $2n-1$ skew diagonals. Let $S_n$ be the $n$-th skew diagonal of $A(n)$. A skew analogue of Bertrand's
  Postulate shows
  that at least one of the skews between $S_n$ and $S_{2n-2}$ has all of its entries equal to $1$. Consequently for $n>2$ we have that there is always a  skew diagonal with entries equal to $1$ below the main skew diagonal. In particular $\{A(n)\}_{n=1}^\infty$ is a monotonically increasing sequence of odd integers. Lemma's 1 and 2 control the growth of $A(n)$ as $n \to \infty.$ By
  parity check alone as $A(n)$ grows to $A(n+1)$ the count of new $1$'s
  must be an nonzero even number. Because of this we have that
  $|A_n|=|A_{n-1}|+2j$ for some $j\in\mathbb{N}$, $j\neq0$. So we have the
  following from direct substitution \begin{align} {|A_{n}|+|A_{n-2}| \above 1.5pt|A_{n-1}|}&={\left(|A_{n-1}|+2j\right)+\left(|A_{n-1}|-2k\right)\above 1.5pt |A_{n-1}|}\\
 &=2\cdot{|A_{n-1}|+j-k\above 1.5pt |A_{n-1}|}\\
 &=2\cdot\left(1+{j-k\above 1.5pt |A_{n-1}|}\right)\\
\end{align} Taking limits we have
  \begin{align}
\lim_{n\to \infty}2\cdot\left(1+{j-k\above 1.5pt |A_{n-1}|}\right)&=2+\lim_{n\to \infty}\left({j-k\above 1.5pt |A_{n-1}|}\right)\\
&=2+0\\
&=2
\end{align} This completes the proof.","['algebra-precalculus', 'conjectures', 'sequences-and-series', 'elementary-number-theory']"
2660327,Disk on a tile floor [duplicate],"This question already has an answer here : Hidden variables in probability (1 answer) Closed 6 years ago . A disk 2 inches in diameter is thrown at random on a tiled floor, where each tile
is a square with sides 4 inches in length. Let C be the event that the disk will land
entirely on one tile. In order to assign a value to P(C), consider the center of the disk.
In what region must the center lie to ensure that the disk lies entirely on one tile?
If you draw a picture, it should be clear that the center must lie within a square
having sides of length 2 and with its center coincident with the center of a tile.
Since the area of this square is 4 and the area of a tile is 16, it makes sense to let
$P(C) = \frac{4}{16}.$ I don't really understand the statement for the last sentence ""Since the area of this square is 4 and the area of a tile is 16, it makes sense to let
$P(C) = \frac{4}{16}.$""","['probability-theory', 'probability']"
2660353,Regularity of a function from asymptotics of its Fourier transform?,"I know that if $f(t) \in L^1(\mathbb{R}) \cap C^k(\mathbb{R})$ then we must have $\hat{f}(s)=o(s^{-k})$ for the Fourier transform. Is there some sort of converse to this statement? Let $f \in L^1(\mathbb{R})$ be such that  $\hat{f}=o(s^{-k})$, What can we conclude about the regularity of $f$? I'm willing to replace $o(s^{-k})$ with $o(s^{-k-\epsilon})$ for some small $\epsilon > 0$ if that makes a big difference.","['functional-analysis', 'asymptotics', 'fourier-analysis', 'fourier-transform']"
2660361,How do I interpret Euler's formula? [duplicate],"This question already has answers here : How does $e^{i x}$ produce rotation around the imaginary unit circle? (8 answers) Closed 6 years ago . I don't understand the formula at all: $$e^{ix} = \cos(x) + i \sin(x)$$ I've tried reading all sorts of webpages and answers on the subject but it's just not clicking with me. I don't understand how we can define things when these are already known quantities. We've already got an exact definition for $e$ as $\lim_{n \to 0} (1+\frac{1}{n})^n$, and we've got $i = \sqrt{-1}$, and we've got $\cos$ and $\sin$ as the $x$ and $y$ coordinates of where right triangles meet the unit circle. So I don't understand how we get from those figures to Euler's formula. Then again I also don't understand why complex numbers are of the form $a + bi$ so that might have something to do with it. I have a hard time separating what identities are ""emergent"" from the previous mathematics and which pieces are ""by definition"" and why they are defined that way. I don't understand why we start talking about ""rotations"" as if it's obvious that's what's happening. I don't even know why $e$ is involved in any of this to begin with. Is $e^{ix}$ simply being written in the slightly rearranged complex form $a + ib$ where $a = \cos(x)$ and $b = \sin(x)$? Is this to be interpreted as $\cos(x)$ being the $x$-coordinate and somehow $i \sin(x)$ is the $y$ coordinate?","['exponential-function', 'trigonometry', 'complex-numbers', 'definition']"
2660364,The Rolle's theorem for continuous function with one-sided derivative,"Let $f:[a,b] \rightarrow \mathbb R$ be a continuous function with $f(a)=f(b)$ such that there exists in each point $x\in (a,b)$ the right derivative $f'_{+}(x)$. I want to show that there exist points $c_1,c_2 \in (a,b)$ such that $f'_{+}(c_1) \leq 0$ and $f'_{+}(c_2) \geq 0$.
(It is stated
 in Remark to the Rolle's theorem in Course of mathematical analysis by L. Schwarz, chap.3, $\S2$.) This is obvious if $f$ is a constant function. Let us assume that $f$ has a positive value. Then
the point $c_1$ we find exactly as in the proof of the classic Rolle's theorem: let $f$ takes the maximum value in $c_1,$ then $f_{+}'(c_1)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \leq 0$. If $\min f>0$, let $f$ takes the minimum value in $c_2$. Then    $f_{+}'(c_2)=\lim_{x\rightarrow c_1^+} \frac{f(x)-f(c_1)}{x-c_1} \geq 0$. My question is: How to find $c_2$ in general?","['derivatives', 'real-analysis', 'analysis']"
2660379,Nearly Bessel's Equation,"In my research I have come across a linear, second-order ordinary differential equation that looks much like Bessel's equation except for one small difference:
\begin{equation}
z^2\frac{\partial^2}{\partial z^2}R_m(z) + z\frac{\partial}{\partial z}R_m(z) + \left(z^2+Wz-m^2\right)R_m(z) = 0,
\end{equation}
where $m$ and $W$ are constants.  If $W\rightarrow0$, then Bessel's equation is recovered.  I believe this equation may be solved as a series, but its similarity to Bessel's equation makes me wonder if there is another way (maybe through some clever substitution) that makes use of well known functions.  I don't especially want to re-invent the wheel and I have tried to look up this equation, but I have not been able to find anything.  Does anyone know if this equation has previously established solutions?","['special-functions', 'ordinary-differential-equations']"
2660389,$T(M^\perp)\subseteq M^\perp \Longleftrightarrow T^*(M)\subseteq M?$,"Let $E$ be an infinite-dimensional complex Hilbert space and $\mathcal{L}(E)$ be the algebra of all bounded linear operators from $E$ to $E$. Let $M$ be a subspace of $E$ and $T\in \mathcal{L}(E)$. It is true that
  $$T(M^\perp)\subseteq M^\perp \Longleftrightarrow T^*(M)\subseteq M?$$","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2660473,Why are complex numbers defined as $a+bi$?,"I understand that a complex number $n = a + bi$ is defined as having real $a,b$ with $i = \sqrt{-1}$. However what I don't understand is the why. Why was it defined this way? How do we know this will be a useful way to define them? Was this emergent from previous mathematics or a defined ""workaround"" to address certain problems? Like what led to someone going, ""Hmm, we should define a new type of number and represent it as $a + bi$ where $i = \sqrt{-1}$!"" and so on. I have been trying to understand the basics of complex analysis but I haven't yet understood why numbers are represented this way before I can wrap my head around why all these other interesting areas of mathematics work themselves out. Like I could easily envision myself defining a specific number in some way only to find later that it was a bad definition or an incomplete or inaccurate way to describe something. What makes $a + bi$ correct and why did it come about?","['number-theory', 'complex-analysis', 'complex-numbers', 'definition']"
2660500,Continuous on paths implies continuous if space is locally path-connected?,"A map $\varphi:X\to Y$ between topological spaces is called continuous on paths if  for every continuous path $\gamma :[0,1]\to X$ the function $\varphi \circ \gamma :[0,1]\to Y$ is also continuous. Every continuous map is also continuous on paths. I am looking to prove the following claim: If $X$ is locally path-connected, then a map $\varphi:X\to Y$ which is continuous on paths is also continuous. I have found a proof of this which also assumes that $X$ is first countable (See: Continuity on paths implies continuity on space? ).
Is there a counter-example when $X$ is not first countable?","['algebraic-topology', 'general-topology']"
2660519,Dense set equivalent definitions,"I am an undergraduate student who is studying real analysis from Rudin's POMA and I am trying to prove that these two definitions that I have for dense sets are equivalent: 1) Given a metric space $X$ and $E \subset X$; $E$ is dense in $X$ iff every point of $X$ is a limit point of $E$ or $E = X$ or both of these are true. 2) Given a metric space $X$ and $E \subset X$; $E$ is dense in $X$ iff the intersection of $E$ and every non-empty open set of $X$ is non-empty. In an attempt to prove the equivalence I have encountered an example which I can't get my head around it.
Given the set $X$ such that X consists of all points $s_n$ , where $s_n = \sum_{k=0}^n (1/2)^k$ for all $n \ge 0$, and $2$ as well. Now define the metric for such a set to be the same as that of $\mathbb{R}$. Then $X$ is a metric space. Now according to definition (1) the only dense set in $X$ is $X$ itself, but according to (2) the set $V = X - \{2\}$ is a dense set in $X$ besides $X$ as well. However we should not have such a problem. So could you please point out what I am doing wrong. Thank you.","['general-topology', 'real-analysis']"
2660523,Why are complex numbers treated as coordinates?,"When I see examples of complex numbers on the complex plane (imaginary $y$-axis, real $x$-axis), we have some number $z = a + bi$ and almost every diagram, tutorial, says this maps to the point $a$ units over on the real line and then $b$ units up/down on the imaginary line which is labeled $..., -2i, -1i, 0, 1i, 2i, ...$. Why is this so? It's treated like some obvious statement. But I don't see why it should be the case. It's like $z = a + bi$ is really being treated as a coordinate pair $(a, b)$.","['coordinate-systems', 'complex-analysis', 'complex-numbers', 'definition']"
2660529,D. Joyce's decomposition of complex tensors,"This question/reference request is related to the books Compact manifolds with special holonomy , Riemannian holonomy groups and calibrated geometry and the paper Manifolds with many complex structures all by D. Joyce. In all of this there is the fallowing notation. Let $(M,J)$ be a complex manifold of real dimension $2m$. Fix some (real) coordinate chart $(U,x_1,...,x_{2m})$. Suppose we have a tensor $t=t^{a...}_{b...}$ i.e. $t=\sum_{a,...,b,...}\partial_{x_1} \otimes ...\otimes dx_b\otimes ...$ . Now he defines (tensors?) $$t^{\alpha ...}_{...}=\frac{1}{2}(t^{a ...}_{...}-i J^a_ct^{c ...}_{...}), \:\:t^{\overline{\alpha} ...}_{...}=\frac{1}{2}(t^{a ...}_{...}+i J^a_ct^{c ...}_{...})$$ $$t^{ ...}_{\beta ...}=\frac{1}{2}(t^{ ...}_{b...}-i J^c_bt^{ ...}_{c...}), \:\:t^{ ...}_{\overline{\beta} ...}=\frac{1}{2}(t^{ ...}_{b...}+i J^c_bt^{ ...}_{c...}).$$ The first question is whether one knows a reference for precise elaboration on that notation since in mentioned place I only found hardly this formulas. The second and most important question is how this correspond to the usual notation in which $Greek$ and $\overline{Greek}$ letters correspond to holomorphic and anti-holomorphic decomposition. Let me elaborate on that second question. For a fixed tensor t of type $(p,q)$ we have its decomposition into $2^{p+q}$ tensors being sections of $${T^{1,0}}^{P'}M \otimes {T^{1,0}}^{P''} M \otimes {{T^{1,0}}^*}^{Q'}M \otimes {{T^{1,0}}^*}^{Q''}M$$ where $P' \cup P'' = [p]$, $Q' \cup Q'' = [q]$ and $P',P'',Q',Q''$ actually indicate order in which $i$ and $-i$ eigenspaces are tensored. It seems to me (I would as well appreciate an easy reason for that) that for a fixed combination of bars and no-bars, $t^{\alpha \overline{\beta}...}_{\gamma \delta...}$ are coordinates for the tensor being a member of decomposition of $t$ corresponding to this ""type"" but still in coordinates $(U,x_1,...,x_{2m})$. Suppose now in addition that we have started with a holomorphic coordinates i.e. $x_j+ix_{m+j}$ are holomorphic coordinates. How are then this  $t^{\alpha ...}_{...}$ related to the components of $t$ in holomorphic coordinate i.e. with respect to $\partial_{z_j}, \partial_{\overline{z_j}}, dz_j, d\overline{z_j}$. For example for Hermitina metric $g$ is $g_{\alpha \overline{\beta}}$ in usual sense, i.e. $g=g_{\alpha \overline{\beta}}(dz_\alpha \otimes d\overline{z_\beta} + d\overline{z_\beta} \otimes dz_\alpha)$, the same as in the one defined in the beginning? Note that in formulas from the begging, Greek letters can be choose to be bigger than $m$ so it's probably not that this components are the same on the other hand latter in the book in case of holomorphic coordinates Joyce is using them as they were the same.","['complex-geometry', 'reference-request', 'differential-geometry']"
2660572,Inverse of the $n$-by-$n$ matrix $(a_{jk})$ where $a_{jk} = \binom{j-1}{k-1}$,"I have an interesting problem which can be solved by Induction and Gaussian Elimination, but due to the nice structure of the matrix, I think there can be many more approaches. Here's the problem : Let $$ A = (a_{jk})_{n \times n} $$ where $\displaystyle a_{jk} = \dbinom{j-1}{k-1}$ (by convention, this is $0$ for $k>j$ ). Prove that $$ A^{-1} = ((-1)^{j+k} a_{jk})_{n \times n} $$ The small alphabets along with subscripts denote the element of the $j^{\text{th}}$ row and $k^{\text{th}}$ column of the matrix. I think this problem might have deep links to theorems of linear algebra due to the nice structure of inverse. I'm specifically seeking an answer that makes such a connection. Thanks in advance.","['matrices', 'binomial-coefficients', 'linear-algebra']"
