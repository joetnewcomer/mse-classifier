question_id,title,body,tags
3923907,What happens when you multiply a cycle index notation?,I recently was introduced to this lovely combinatorial object https://en.wikipedia.org/wiki/Cycle_index#Applications . Now I'm curious if there are any operations we can do with it? Like if we square it or multiply it with another does the outcome have any relationship to the original two? Can we take derivatives? Can we take sums? Or is this just purely an organizational object with no capicity to be manipulated algebraically?,"['group-theory', 'combinatorics']"
3923924,Examples of functions with a removable discontinuity,"I recently came across the following example of a function which is discontinuous at a point: The cost of a sushi buffet is calculated by weighing your plate, where you are then charged £10 per pound. As part of a promotion, if you manage to fill your plate so that the weight is precisely 1lbs, you get your meal for free. This is a nice way to motivate a function like $$f(x) = \begin{cases}10\,x\hfil&\text{$x\neq 1$}\\\hfil0\hfil&\text{$x=1$}\end{cases}$$ which are nice for illustrating the idea of a limit to students. Are there any other real-world examples of functions like this, for the sake of explaining limits? Ideally one where you would actually want the limiting value, and not the assigned value, to motivate the concept further. (Apart from the obvious difference quotient.)","['calculus', 'algebra-precalculus', 'education']"
3923949,"In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair?","So the question is: In 1000 tosses of a coin, 560 heads appear. Is it reasonable to assume the coin is fair? Let $Y = \sum_{i=1}^{1000} X_i = $ number of heads. I chose $H_0: p = 0.5$ and $H_1: p > 0.5$ .
The $p$ -value is: $$P(Y\geq 560) = \sum_{y=560}^{1000}\binom{1000}{y}(0.5)^y(0.5)^{1000-y}$$ $$= 0.00008252494$$ Then I try to find $\alpha$ using $\frac{Y}{n} = \overline{X}$ as the estimator for $p$ : $$P(Y \geq 560) = P(\overline{X} \geq 0.56)$$ $$= P(\frac{\sqrt{n}(\overline{X} - p_0)}{p(1-p)} \geq \frac{\sqrt{1000}(0.56 - 0.5)}{0.5*0.5}$$ $$= P(Z\geq 7.59)$$ That Z-score seems incredibly high and I don't think they're even included in any Z-tables. Am I on the right track here or have I missed something?","['statistics', 'hypothesis-testing']"
3923953,Prove arithmetic formula for the number of bits in $j=2^k-1$,"If $k$ is an integer with $0\le k<155$ , and $j=2^k-1$ , then it holds: $$k\,=\,5\left(\left\lfloor\frac j{31((j\bmod31)+1)}\right\rfloor\bmod 31\right)-\left\lfloor\frac{12}{(j\bmod31)+3}\right\rfloor+4$$ Why does this equation hold for such a large interval of $k$ ? How can we prove that? This is a shortened version of more complex formula # posted by Hallvard B. Furuseth in a comp.c post on 2003-11-03, which reportedly works to $k$ of at least $3\cdot10^{10}$ . Update (not part of the bountied question): per this source , the same author gave that formula working for $0\le k<2040$ : $$k\,=\,8\left(\left\lfloor\frac j{255((j\bmod255)+1)}\right\rfloor\bmod 255\right)-\left\lfloor\frac{86}{(j\bmod255)+12}\right\rfloor+7$$ Try it online with Python or with Wolfram Mathematica ! The code computes the upper limits for both formulas. # Full formula, for reference (not in the above code or part of the bountied question): $$\begin{align}k\,=\,&30\left(\left\lfloor\frac j{((j\bmod(2^{30}-1))+1)\,(2^{30}-1)}\right\rfloor\bmod (2^{30}-1)\right)+\\&5\left(\left\lfloor\frac{j\bmod(2^{30}-1)}{31((j\bmod31)+1)}\right\rfloor\bmod 31\right)-\left\lfloor\frac{12}{(j\bmod31)+3}\right\rfloor+4\end{align}$$","['algebra-precalculus', 'modular-arithmetic']"
3923960,Showing that $\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0$,"If $\alpha ,\beta\in\mathbb{R}^+$ , show that $$\displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}=0$$ My attempt: $\displaystyle\lim_{x\to+\infty}\frac{(\log x)^\alpha}{x^\beta}= \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha$ . Since the function $y=z^\alpha$ is continuous, then $$ \displaystyle\lim_{x\to+\infty}\left[\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha=\left[\displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\beta /\alpha}}\right]^\alpha$$ Let $\eta=\beta/\alpha$ . Applying L'Hôpital's rule $\displaystyle\lim_{x\to+\infty}\frac{(\log x)}{x^{\eta}}=\displaystyle\lim_{x\to+\infty}\frac{\frac{1}{x}}{\eta x^{\eta -1}}=\frac{1}{\eta}\displaystyle\lim_{x\to+\infty}\frac{1}{x^{\eta}}=\frac{1}{\eta}*0=0$ Is my solution correct?","['real-analysis', 'calculus', 'solution-verification', 'limits', 'derivatives']"
3924010,Oscillatory integral with absolute value,"Suppose $f:[0,1]\to[0,+\infty)$ is a continuous function. Are there general conditions under which the following limit exists? $$\lim_{n\rightarrow \infty} \int_0^1 |\cos(nf(x))|dx$$ Obviously if $f$ is a constant that is not an integer multiple of $\pi$ the limit does not exist. Does the limit exist if $f'(x)\neq 0$ for all $x \in [0,1]$ ? If so, what if we drop differentiability and only require $f$ not be locally constant? When $f$ is not differentiable, can one still show $$\liminf_{n \rightarrow \infty} \int_0^1 |\cos(nf(x))|dx >0$$ under suitable conditions on $f$ ?","['absolute-value', 'oscillatory-integral', 'real-analysis']"
3924014,A problem connecting Hilbert spaces and normed linear spaces,"The problem is the following: Let $X$ be a normed linear space and $Y$ be a Hilbert space. Let $ A : X \rightarrow Y$ be a linear operator. For $y \in Y \, $ , let $ \, S_y = \{ x \in X : \| Ax − y \| \le \|Au − y \|, \, \forall \, u \in X \} $ Show that $S_y$ is nonempty if and only if $ \, y \in R(A) + R(A)^{\bot}$ . In addition, if $X$ is Hilbert and $N(A)$ is a closed subspace, show that for every $y \in R(A) + R(A)^{\bot}$ ,
there is a unique $x_y \in S_y$ such that $ \| x_y \| = inf \{ \| x \|: x \in S_y \}$ . My attempt: I guess that if I prove that $S_y$ is a convex subset of $Y$ , and as $Y$ is Hilbert, I can say that $ \exists ! \, x_y \in S_y $ , as required in the second part.
For the first part. I'm not quite sure how to go about it.","['hilbert-spaces', 'functional-analysis', 'analysis']"
3924124,Specific value of $\zeta(3/2)$?,Is anything known about the value of $$\zeta(3/2)=\sum_{n\geq 1}\frac{1}{n^{3/2}}?$$ It is a classical result that $\displaystyle \zeta(2)= \frac{\pi^2}{6}$ and $\zeta(3)$ has been shown to be irrational by Roger Apéry in 1979. Do we even know if $$\zeta(3/2)=\sum_{n\geq 1}\frac{1}{n^{3/2}}$$ is an irrational number or not? Is it  true that $$\zeta(3/2)=\sum_{n\geq 1}\frac{1}{n^{3/2}}=\frac{a}{b}\sum_{n\geq 1}\frac{(-1)^{k-1}}{n^{3/2}\binom{2n}{n}}?$$ Not sure if Apéry's proof can be adapted here since I haven't read it.,"['riemann-zeta', 'number-theory', 'zeta-functions', 'sequences-and-series']"
3924284,"The properties of convex function on the closed unit interval $[0,1]$.","Consider a continuous and convex function $F(x):[0,1]\longrightarrow\mathbb{R}$ . I am wondering if $F(x)$ is continuously differentiable in $[0,1]$ $F(x)$ is of bounded variation in $[0,1]$ $F(x)$ is absolute continuous in $[0,1]$ . The second one is correct, due to this post Proving a convex function is of bounded variation . However, the remaining two became mysterious to me. Royden's chapter 6 answers them if we have an open interval. Corollary 17: Let $\varphi$ be a convex function on $(a,b)$ . Then $\varphi$ is Lipschitz, and therefore absolutely continuous on each closed, bounded subinterval $[c,d]$ and $(a,b)$ Theorem 18: Let $\varphi$ be a convex function on $(a,b)$ . Then $\varphi$ is differentiable except at a countable number of points. By the Theorem 18, it is hard to believe that $F(x)$ will become differentiable in $[0,1]$ . But I cannot find a counterexample. That is, a convex function that is continuous on $[0,1]$ but is not differentiable. The Corollary 17 gives us pretty nice result, but seems like it does not apply to the closed interval. Is it possible to say that if we have $F(x)$ on $[0,1]$ is convex, then it will be convex on $(-\epsilon, 1+\epsilon)$ ? and then we can use Corollary 17 to conclude that it is absolutely continuous on $[0,1]\subset (-\epsilon, 1+\epsilon)$ . Thank you!","['absolute-continuity', 'convex-analysis', 'analysis', 'real-analysis']"
3924289,Basis for $\mathbb{R}^\mathbb{N}$ implies axiom of choice?,"Let $\mathbb{R}^\mathbb{N}$ denote the vector space over $\mathbb{R}$ of sequences of real numbers, with multiplication and addition defined by component. It's well-known that though the subspace $\mathbb{R}^\infty$ of sequences with only a finite number of nonzero terms has a basis $\mathbf{e}_1 = (1, 0, 0, 0, \ldots), \mathbf{e}_2 = (0, 1, 0, 0, \ldots)$ , this is not a basis of $\mathbb{R}^\mathbb{N}$ (expressing the constant sequence $(1, 1, 1, \ldots)$ would require an infinite sum $\mathbf{e}_1 + \mathbf{e}_2 + \mathbf{e}_3 + \cdots$ , and infinite sums in generic vector spaces are undefined). It's also been proved that the statement that all vector spaces have a basis is equivalent to the axiom of choice. I'm interested, though, in the specific space $\mathbb{R}^\mathbb{N}$ . Has it been proved that a basis for this set requires the axiom of choice and cannot be described explicitly? This isn't a homework question or anything; I'm just curious.","['vector-spaces', 'linear-algebra', 'hamel-basis', 'axiom-of-choice', 'set-theory']"
3924307,Why is it correct to assume $f = e^{\lambda x}$ in differential equation? [duplicate],"This question already has answers here : Why do we chose exponential function as a trial solution for second order linear differential equation with constant coefficient? (4 answers) Closed 3 years ago . I was solving the following differential equation: $$f'' + f = 0$$ And, as I learned in class, I assumed that $f = e^{\lambda x}$ and then found the roots of the polynomial and found the solution. But then the following came to my mind: Why is it correct to assume that $f = e^{\lambda x}$ ? How do we know that by doing this we are indeed finding all the solutions of the equation and we aren't only finding the solutions that take that form? Because doing this looks like a guessing game, we try to guess the solution and then find it by correcting and fixing the values of $\lambda$ .",['ordinary-differential-equations']
3924311,Derivative of Inverse of sum of matrices,"Given is the function $f : \mathbb{R}^p \to \mathbb{R}$ with $$
f(x) = q(x)^{\top} G^{-1} q(x)
$$ where $G = A + x_1 B_1 + \ldots + x_p B_p$ . The matrices $A, B_1, \ldots, B_p \in \mathbb{R}^{n \times n}$ are all symmetric positive definit. $q: \mathbb{R}^n \to \mathbb{R}^n$ and the Jacobian $\nabla q$ is known. Is it possible to derive a closed form for $\nabla f$ ? For me, the hard part is $G^{-1}$ . Any hints or suggestions are really appreciated!","['matrix-calculus', 'derivatives', 'inverse']"
3924330,Different forms of Tangent Angle Addition Identity,"I am trying to derive the tangent angle addition identity: $$\tan(x+y) = \frac{\tan x + \tan y}{1-\tan x\tan y}$$ by the argument shown here: While I can arrive at the correct identity algebraically and using trig ratios in a geometric diagram proof, I'm confused about why I cannot substitute 1 for the numerator, i.e. $$\tan(x+y) = \frac{1}{1-\tan x\tan y}$$ as I am using a unit square and $1 = \tan x+\tan y$ . I tried graphing these to compare and evaluating values but they are definitely not equal. Is there a general rule for making substitutions in diagram proofs like this one? If I cannot use $1$ in the numerator to represent $\tan x+\tan y$ , then why am I allowed to use $1$ in the denominator?","['trigonometry', 'solution-verification', 'geometry']"
3924358,What kind of mathematical rule was broken in here?,"I just wanted to write the iterated version of: $$ a^2 - b^2 = (a+b)(a-b)$$ As: $$ n^x - 1 = (n^{ \sqrt{x}} +1) \color{red}{(n^{ \sqrt{x}} -1)} = \\ 
(n^{ \sqrt{x}} +1)(n^{ \sqrt{\sqrt{x}}} +1) \color{red}{(n^{ \sqrt{\sqrt{x}}} -1)} = \\
(n^{ \sqrt{x}} +1)(n^{ \sqrt{\sqrt{x}}} +1)(n^{ \sqrt{\sqrt{\sqrt{x}}}} +1) \color{red}{(n^{ \sqrt{\sqrt{\sqrt{x}}}} - 1)} = \dots
 $$ And so: $$n^x - 1 = (n^{ \sqrt{x}} +1)(n^{ \sqrt{\sqrt{x}}} +1)(n^{ \sqrt{\sqrt{\sqrt{x}}}} +1) ... \Rightarrow \\n^x - 1 = \prod_{k=1}^{\infty} (n^{x^{\left (\frac{1}{2k}\right)}} + 1)$$ But when plugging $n=1$ we get: $$ 1^x - 1 \equiv 0 = \prod_{k=1}^{\infty} (1^{x^{\left (\frac{1}{2k}\right)}}  + 1) \equiv \prod_{k=1}^{\infty} (1+1) = \prod_{k=1}^{\infty} 2 = \infty$$ Where did I go wrong? I think that I ""omitted"" the $n^{x^{(\frac{1}{2k})}} - 1$ part, but isn't it valid when we talk about infinite series? I would appreciate if you could clear things up. Thank you!","['infinite-product', 'calculus', 'sequences-and-series']"
3924360,Induction proof: $2\times6\times10\times ... \times (4n-2) = (2n)!/n!$,"What I'm missing to solve this problem is the algebra steps to prove the inductive step.  The base case is clear: $2 = 2!/1!$ .  Suppose it's for true for $k \ge 1$ .  Must show it's true for $k + 1$ .  Using the hypothesis, we can write $$2\times6\times10\times14\times18\times ... \times (4k-2) = \frac{(2k)!}{k!}.$$ Now I take the obvious step: I multiply $4(k+1) - 2$ on both sides of the equation and I get \begin{align*}
2\times6\times10\times14\times18\times ... \times (4k-2) [4(k+1) - 2] = \frac{(2k)!}{k!} 4(k+1) - 2).
\end{align*} Now my job must be putting the RHS into the form $(2(k+1))!/(k + 1)!$ .  But I'm not able to do it.  I look at a special case such as $k = 3$ .  I get $14 \times 6!/3!$ and that equals $8!/4!$ , but it doesn't seem obvious how to put it in the general case.  I need some algebra help.","['induction', 'discrete-mathematics']"
3924383,Let $\mathbb{Z} \subset \mathbb{R}$ with the discrete topology prove that $X$ is connected if and only if $f: X \rightarrow \mathbb{Z}$ is constant,"Let $\mathbb{Z} \subset \mathbb{R}$ with the discrete  topology prove that $X$ is connected if and only if for every continuous function $f: X \rightarrow \mathbb{Z}$ is constant function. Attempt Suppose that $X$ is connected and consider an arbitrary continuous function $f: X \rightarrow \mathbb{Z}$ since $X$ is connected  then $f(X)\subset \mathbb{Z}$ is connected, and $f(X)$ is an open set since we be considering the discrete topology, that is $f(X) \subset \mathbb{Z}$ is open in the subespace topology
if for any $x\in X$ $f(x)=\mathbb{Z} \cap \mathbb{R}$ for all $x\in X$ .This is $f$ is a constant function. Conversely assume that for every continuous function $f:X \rightarrow \mathbb{Z}$ is constant we should prove that $X$ is connected. Assume that $X$ is not conected, then Exists $U,V\subset X$ such that $U,V \neq \emptyset$ and $U \cup V=X$ ,but $U \cap V = \emptyset$ then consider the function $h_1:U \rightarrow \mathbb{Z}$ given by $h_1(x)=1$ which is continuous function.
And consider $h_2:V \rightarrow \mathbb{Z}$ given by $h_2(x)=0$ which is continuous function.
Notice that $U\cap V= \emptyset$ implies that $h_1$ and $h_2$ are equal in their points of intersections.
Consider $h:U \cup V=X \rightarrow \mathbb{Z}$ given by $$h(x)=\begin{cases}
h_1(x) if\, x\in U&\\
h_2(x)if\, x\in V
\end{cases}$$ Which is non constant continuous function then not every continuous function $f:X \rightarrow \mathbb{Z}$ is constant which is a contradiction, then $X$ is connected.
Is my proof right?","['general-topology', 'solution-verification', 'connectedness']"
3924439,"Optimality of Khintchine's inequality, convergence in distribution and convergence of moments","Let $b_1,..b_n$ be real numbers and $\varepsilon_1,...,\varepsilon_n$ be independant Rademacher random variables. The Khintchine's inequality states that $$\mathrm{E}\left [ \left (  \sum_{i=1}^{n} b_i\varepsilon_i \right )^{2p}\right ]\leqslant \frac{\left ( 2p \right )!}{2^pp!}\left ( \sum_{i=1}^{n}b_i^2 \right )^p$$ for every integer $p \geqslant 1$ . I'm trying to prove that the constant $\frac{\left ( 2p \right )!}{2^pp!}$ is optimal, in the sense that it is impossible to obtain an inequality that holds for every Rademacher sum with a strictly smaller constant that does not depend on the dimension $n$ . Since $\frac{\left ( 2p \right )!}{2^pp!}$ is the $2p$ -th moment of a standard normal variable, my idea was to approximate a well chosen Rademacher sum with a standard normal variable to obtain the optimality. Let $b_1=...=b_n=1$ . The central limit theorem ensures that $Z_n=\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\varepsilon_i$ converges in distribution towards a random variable $X$ of distribution $\mathcal{N}(0,1)$ . If that implied that $$\lim_{n\rightarrow\infty}\mathrm{E}[Z_n^{2p}] = \mathrm{E}[X^{2p}]$$ then we would have $$\lim_{n\rightarrow\infty}\frac{1}{n^p}\mathrm{E}\left [  \left ( \sum_{i=1}^{n}\varepsilon_i \right )^{2p}\right ] = \frac{\left ( 2p \right )!}{2^pp!}$$ which proves the optimality. So my question really is : is it true that $\lim_{n\rightarrow\infty}\mathrm{E}[Z_n^{2p}] = \mathrm{E}[X^{2p}]$ ? I don't think the dominated convergence theorem works here since $Z_n$ is not bounded. The interpretation of convergence in distribution in terms of pointwise convergence of the characteristic functions yields $\forall t \in \mathbb{R}, \lim_{n\rightarrow\infty} \cos(\frac{t}{\sqrt{n}})^n=e^{-\frac{t^2}{2}}$ . Could that be of any use ?","['inequality', 'concentration-of-measure', 'probability-theory', 'probability']"
3924495,is 0 a perfect square,"Based on my research, I found that there are many arguments about this statement, the main factor is the true definition of perfect square. Some said they are the squares of the whole numbers, but some said that any number that can be written as a positive integer to the power of two and some said that an integer that can be expressed as the product of two equal integers.","['algebra-precalculus', 'definition', 'square-numbers']"
3924501,For a hyper cube of dimension N what number of its vertices can be covered by intersection with a hyperplane,"If you look at intersecting a binary cube (the set contained by [0,1]^n) with a plane in $\mathbb{R}^3$ . Then the plane can potentially intersect with the corners of the cube. Depending on the choice of plane you might intersect with 0,1,2,3 or 4 corners, but it’s impossible to intersect with any other number of corners. If we shift our attention to $\mathbb{R}^4$ it gets a little trickier, all the $\mathbb{R}^3$ numbers must be valid but we also know that hyperplanes such as $x_0+x_1+x_2+x_3=2$ let you intersect with 6 corners. This leads to our problem: Given a dimension $n$ , what does the list of possible intersection counts of a hyperplane with vertices of the Boolean cube look like? Even for $n=4$ at best we can show the list includes (but as of right now I haven’t exhausted) 0,1,2,3,4,6,8. Powers of 2 and Pascal’s triangle numbers are all present but there definitely are more exotic coverings out there especially for $n=5$ onwards","['euclidean-geometry', 'geometry', 'combinatorial-geometry', 'linear-algebra', 'combinatorics']"
3924521,When can the level of the test be exactly $\alpha ?$ in non randomized test. And how to use CLT to find the critical value.,"Let $X_{1}, \ldots, X_{n}$ be a sample from the Bernoulli distribution with parameter $p$ Consider testing $H_{0}: p=p_{0}$ versus $H_{1}: p=p_{1}$ where $p_{0}<p_{1}$ are known numbers. (a) Using the Neyman-Pearson lemma, find the most powerful test (Non-Randomized) among tests with level at most $\alpha$ .   When can the level of the test be exactly $\alpha ?$ (b) Use the CLT to find a critical value such that the level of the test is approximately (asymptotically for large $n$ ) $\alpha$ . To use Neyman Peason lemma I calculate $r$ as : $$r= \frac{f_1}{f_0} = \frac{p_1^{\Sigma x_i}(1-p_1)^{n-\Sigma x_i}}{p_0^{\Sigma x_i}(1-p_0)^{n-\Sigma x_i}}$$ For $p_1>p_0$ we have non randomized test is $\phi(x) =$ Reject $H_0$ for $\Sigma x_i \geq k$ . For given at most level $\alpha$ : $$\alpha \geq E_{p_0} \phi(x) = P_{p_0}\{\Sigma x_i \geq k\} = \Sigma_{r=k+1}^n (n_{C_r}) p_{0}^r (1-p_{0})^{n-r}$$ (a) I don't know how to proceed after this and what to say about When can the level of the test be exactly $\alpha ?$ (b) How to use CLT to find the critical value which satisfies the given condition. Please help me with this problem. Thankyou.","['statistical-inference', 'statistics', 'central-limit-theorem', 'hypothesis-testing']"
3924534,Are topological spaces (and open maps) comonadic over sets (and functions)?,"In the neighborhood formulation of topological spaces, the data of a topological space is a pair $(X,N)$ of a set $X$ and a function $N : X \to F X$ , where $FX$ denotes the set of filters of subsets of $X$ . We think of $N(x)$ as the filter of neighborhoods of the point $x \in X$ . We can extend to an endofunctor $F: \mathbf{Set} \to  \mathbf{Set}$ by setting, for any function $f: X \to Y$ , a function $Ff: FX \to FY$ , given by $$Ff(\phi) : = \{A \subseteq Y ~|~ f^{-1}( A) \in \phi\}~~~~.$$ A topological space is then a coalgebra for this endofunctor, subject to some further axioms. Furthermore, in the neighborhood formulation, a function $f: X \to Y$ is continuous between topological spaces $(X, N_X)$ and $(Y, N_Y)$ if, for any $x \in X$ and $A \in N_Y(f(x))$ , $f^{-1}(A) \in N_X(x)$ . In our terms, this is simply $N_Y \circ f \leq Ff \circ N_X$ . A continuous function $f: (X, N_X) \to (X, N_Y)$ is open if, for any $x \in X$ and $A \in N_X(x)$ , there exists $B\in N_Y(f (x))$ such that $B \subseteq f (A)$ . After some reflection, this turns out to be equivalent to $Ff \circ N_X \leq N_Y \circ f $ . Thus a function is continuous and open if $N_Y \circ f = Ff \circ N_X$ , i.e. if it is a coalgebra homomorphism. It would be nice if this could be refined so as to recover the category of topological spaces and open maps as a category of coalgebras for a comonad. It looks like $F$ is not actually a comonad as defined, so we would need another version of $F$ to get off the ground. Does anyone have ideas, or am I barking up the wrong tree?","['filters', 'general-topology', 'monads', 'category-theory']"
3924650,Standard deviation (sample) / average leads to constant when all values but one are zero,"I was doing some database work for a client, and I came across an oddity that I hoped someone could explain. Take the sequence 0,0,0...,0,n (where n is any non-zero number), calculate the standard deviation (sample), and divide that by the average, and you always get the square root of the total number of samples. e.g.(for 5 samples) 0,0,0,0,100 = 2.236067977
0,0,0,0,-0.21 = -2.236067977 (for 12 samples) 0,0,0,0,0,0,0,0,0,0,0,22 = 3.464101615
0,0,0,0,0,0,0,0,0,0,0,-0.1 = -3.464101615 Is there an obvious explanation for this?","['constants', 'statistics']"
3924762,Fundamental theorem of finite Abelian group,"In Gallian's Contemporary Abstract Algebra, he mentioned about this Greedy Algorithm for an Abelian group. It's basically a procedure of expressing a finite Abelian group as an internal direct product which is a result from fundamental theorem of finite Abelian group. But the validity of the algorithm is not clear for me. The algorithm is given as follows: Greedy Algorithm for an Abelian Group of Order $p^n$ ( $p$ is prime) Compute the orders of the elements of the group $G$ . Select an element $a_1$ of maximum order and define $G_1=\langle a_1\rangle$ . If $|G_1|=|G|$ , stop. Otherwise, replace $i$ by $i+1$ . Select an element $a_i$ of maximum order $p^k$ such that $p^k\leq|G|/|G_{i-1}|$ and none of $a_i$ , $a_i^p$ , $a_i^{p^2}$ ,..., $a_i^{p^{k-1}}$ is in $G_{i-1}$ and define $G_i=G_{i-1}\times\langle a_i\rangle$ . Return to Step 3. Internal direct product requires that none of the elements of $\langle a_i\rangle$ except identity should be in $G_{i-1}$ so why the condition in step 4 is weaker?","['direct-product', 'finite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
3924770,Show that controllability and observability are not affected by replacing A with (A+αI ),"I have been asked to show that controllability and observability are not affected by replacing $A$ with $(A+αI)$ . And also to show that this is not necessarily true for stabilizability. I have thought about different approaches: Popov-Belevitch-Hautus test If the system is controllable must be true that $$Rank [(A - \lambda I) B] = n$$ If we replace $A$ with $(A+αI)$ then $$Rank [((A +αI) - \lambda I) B] $$ $$Rank [(A  - (\lambda -α)I) B] $$ $$Rank [(A  - gI) B]  $$ where $g = (\lambda -α) \in C$ $(A  - gI)$ has already $rank = n$ for all $g \in C$ except for those which are eigenvalues for $A$ . For those values, we need to prove that the concatenation of $B$ will guarantee the rank to be n. The issue is that since B is the same we can't guarantee it. Controllability Matrix Given an LTI if and only if the control matrix $C$ has full column rank, then the system is controllable. $$ C = [B ,AB, A²B ... A^{n-1} B]$$ but then altering the diagonal of $A$ can affect the rank of C, or if not, how come? Controllability Gramian I tried to plug $(A+αI)$ in the integral, but doing that then I don't know how to prove that the matrix $W$ is still nonsingular for any t > 0.","['matrices', 'linear-algebra', 'linear-control', 'control-theory']"
3924782,Random i.i.d variable series,"I want to prove that if { $X_n$ } $_{n\in N}$ is a sequence of independent and identically distributed random variables, non-degenerated in 0, it means, none of them are almost sure  equal to 0, then $P(\sum_{i=1}^n X_{i}$ converges $) = 0$ Some advice would be very helpful. Thanks.","['statistics', 'probability-limit-theorems', 'probability-theory', 'probability']"
3924887,Does equality of sets follow not only from what they contain but also from what they are contained by?,The Axiom of extensionality states that two sets are equal if they contain the same elements: $\forall A \forall B [\forall x (x \in A \Leftrightarrow x \in B) \Rightarrow A = B]$ Can it be replaced by the statement that two sets are equal if they are contained by the same elements? $\forall A \forall B [\forall x (A \in x \Leftrightarrow B \in x) \Rightarrow A = B]$,"['elementary-set-theory', 'foundations', 'set-theory']"
3924888,How to show a null-set is a $\sigma$-algebra if and only if $\mu(\Omega)=0$?,"I am having a bit of trouble with my assigment on Measures and Integrals. First I have been giving the measure space $(\Omega, \mathcal{A}, \mu)$ and the $\mu$ -null set (or empty-set): $\mathcal{N}_\mu=\{N\subseteq \Omega| \quad \exists A \in \mathcal{A}: N \subseteq A \text{ and } \mu(A)=0\}$ . I have proved the following properties: $\emptyset \in \mathcal{N}_\mu$ and If $N_n\in \mathcal{N}_\mu, \quad n=1,2,... ,$ then $\bigcup_{n=1}^\infty N_n \in \mathcal{N}_\mu$ . The last thing I need to show is that: $\mathcal{N}_\mu$ is a $\sigma$ -algebra if and only if $\mu(\Omega)=0$ . Maybe some of you guys can give me a hint on what to do, to get started on the proof? Thank you.",['measure-theory']
3924946,Self-orthogonal Latin squares,"A Latin square $A$ is called self-orthogonal if $A$ and $A^{T}$ are orthogonal Latin squares. Use the elements of $\;\mathbb{Z}_v$ as the names of the rows and columns of your Latin square. Let $\boldsymbol{A}=(a_{ij})$ such that $a_{ij}=2i-j\in \mathbb{Z}_v$ . Prove that this forms a self-orthogonal Latin square if the $\gcd(v,6)=1$ . I have just tried to see that this works with small orders like $v=5$ : $$\boldsymbol{A}=\begin{pmatrix}
0 & 4 & 3 & 2 & 1\\
2 & 1 & 0 & 4 & 3\\
4 & 3 & 2 & 1 & 0\\
1 & 0 & 4 & 3 & 2\\
3 & 2 & 1 & 0 & 4\\
\end{pmatrix}\implies\boldsymbol{A}^{T}=\begin{pmatrix}
0 & 2 & 4 & 1 & 3\\
4 & 1 & 3 & 0 & 2\\
3 & 0 & 2 & 4 & 1\\
2 & 4 & 1 & 3 & 0\\
1 & 3 & 0 & 2 & 4\\
\end{pmatrix}$$ So it seems to work and I could probably sketch a general construction, but I don't know how the assumption $\gcd(v,6)=1$ makes this all work. Can someone explain?","['matrices', 'combinatorial-designs', 'combinatorics', 'latin-square']"
3924976,A generalization of SLLN: Convergence of combination of iid variables,"Problem: Let $(\varepsilon_i)_{i\geq 1}$ be a sequence of iid random variables . Let $(x_i)_{i\geq 1}$ be a sequence of real numbers.
Assume that: $\mathbb E[|\varepsilon_1|]<\infty$ and $\mathbb E[\varepsilon_1]=0$ . $\frac{1}{n} \sum_{i=1}^n (x_i)^2$ is bouned for all $n$ . Prove that: $\lim_{n\rightarrow \infty} \frac{1}{n} \sum_{i=1}^n x_i \varepsilon_i=0$ with probability $1$ . Consequence: If we choose $x_i=1$ for all $i$ . Then (loosely speaking) above problem recovers the strong law of large number . My question: My teacher suggests that I can prove the problem using Kronecker's lemma . But, I have no idea how to apply it. Could anyone give me a hint. Thank you in advance! Edit: Based on the useful comments below, the first condition is redundant. I mean: If the expectation is defined as Lebesgue Integral, then we have: $\mathbb E[\varepsilon_1]=0$ implies $\mathbb E[|\varepsilon|]<\infty$ . Therefore, the first condition should be rewritten: "" $\mathbb E[\varepsilon_1]=0$ in the sense of Lebesgue Integral"" . It is important to note that, the problem does not hold for Cauchy distribution. Indeed, if $\varepsilon_1$ has Cauchy distribution, then $\mathbb E[\varepsilon]$ is undefined in the sense of Lebesgue Integral (see mean of Cauchy ).","['law-of-large-numbers', 'probability-limit-theorems', 'probability']"
3924980,Compact operators in $\bigoplus_{i \in I} H_i$,"Let $\{H_i: i \in I\}$ be a collection of Hilbert spaces. We can form the Hilbert space direct sum $$H:= \bigoplus_{i \in I} H_i$$ Question : Is there a ""nice"" dense subset of $B_0(H)$ (compact operators on the direct sum $H$ )? For example, something like the operators $$\left\{\bigoplus_{i \in I}  T_i \ \Bigg| \ T_i \in F(H_i), \text{all but  finitely many $T_i$ are zero}\right\}$$ where $F(H_i)$ are the finite-rank operators on $H_i$ .","['hilbert-spaces', 'compact-operators', 'functional-analysis']"
3924990,Stuck at solving differential equation by using integrating factor,"This is the text: Solve the following differential eqution: $$(2x^3+3x^2y+y^2-y^3)~dx+(2y^3+3x^2y+x^2-x^3)~dy=0$$ This particular differential equation has to be solved by finding an integrating factor. I first tried making this equation into a Darboux differential eqaution, and didn't find any luck. These are the integrating factors that I tried: $u(x,y)=u(x)$ , $u(x,y)=x^my^n$ , $u(x,y)=u(y)$ . I don't have any idea what else to try. You don't have to answer it fully, just give me a hint or something. Thank you in advance.",['ordinary-differential-equations']
3925067,"Is there a group $G$ for which $\mathrm{Aut}(G) \simeq (\mathbb{R},+)$?","I know the classic theorem that $(\mathbb{Q},+)$ cannot be expressed as an automorphism group, i.e. there is no group $G$ such that $\mathrm{Aut}(G)\simeq (\mathbb{Q},+)$ . Theorem A. If $L$ is a locally cyclic group with no element of order $2$ , then $L$ cannot be expressed as an automorphism group. But how about $(\mathbb{R},+)$ ? I think the answer might be no , and the proof proceeds by showing that if $\mathrm{Aut}(G) \simeq \mathbb{R}$ , then some subgroup or quotient $H$ of $G$ will satisfy $\mathrm{Aut}(G) \simeq \mathbb{Q}$ , which contradicts Theorem A. So how can we construct this $H$ ? Assuming the axiom of choice, we can say $\mathbb{R} = \mathbb{Q}\oplus B$ for some additive subgroup $B$ of $\mathbb{R}$ (just by picking a $\mathbb{Q}$ -basis for $\mathbb{R}$ ). Now I'm tempted to do some Galois-type thing, where you use a ""fixed"" subgroup $$H=\mathrm{Fix}(B)=\{g\in G : b(g) = g \text{ for all } b\in B \},$$ and then try to say something about $\mathrm{Aut}(H)$ or $\mathrm{Aut}(G/H)$ (where, for the latter, maybe we take the normal closure of $H$ ). But I can't complete this line of reasoning. It seems key that $\mathrm{Aut}(G)$ splits as a direct sum --- that seems special. Am I just totally off-base here? Is there some obvious group $G$ whose automorphism group is $\mathbb{R}$ ? Related questions If I can prove that $\mathbb{R}$ is not an automorphism group, then the same would hold for the isomorphic group $\mathbb{R}_{>0}$ of positive reals under multiplication. But how about $\mathbb{R}^\times \simeq \mathbb{R}_{>0}\times C_2\simeq \mathbb{R}\times C_2$ --- can this be achieved as an automorphism group? If $\mathrm{Aut}(G)$ is a direct product of abelian groups, what can be said about $G$ ? A related observation is that if $\mathrm{Aut}(G)$ is abelian, then $G$ is nilpotent of rank $\leq 2$ . So there is a sense in which $G$ is ""almost"" abelian. I know $\mathbb{Q}^\times$ is the automorphism group of $(\mathbb{Q},+)$ , and that $\mathbb{R}^\times$ constitutes the continuous automorphisms of $(\mathbb{R},+)$ ... Then there's multiplicative/additive groups of other rings/fields ... I can't even resolve this question in the case of $\mathrm{Aut}(G) \simeq \mathbb{Z}\times \mathbb{Z}$ . For this one I can show that $G'$ would necessarily by cyclic, but that's about it. I'm very curious about which types of groups can be achieved as automorphism groups. A quick observation If $\mathrm{Aut}(G) \simeq \mathbb{R}$ , then $G$ must be infinitely generated (or else its automorphism group is countable) and nilpotent of rank $\leq 2$ (this follows for any group whose automorphism group is abelian).","['automorphism-group', 'group-theory']"
3925103,Nice definition of Grothendieck topology?,"In Scholze's masterclass on condensed mathematics, there was a discussion whether there is a ""good definition of a site"". It seems, that the definition of Grothendieck topology using sieves is the most general. If one works with Grothendieck pretopologies one has to worry about existence of certain pullbacks. So I am not interested in pretopologies, instead I want to conceptualize the sieve-definition. This is what I have: The usual definition of Grothendieck topology based on sieves (see e.g. https://en.wikipedia.org/wiki/Grothendieck_topology , or Alternative formulation of Grothendieck topology ) can be rewritten as follows: Def. A Grothendieck topology on a small category $\mathcal C$ is a functor $J : \mathcal C^{\mathrm{op}} \to \mathrm{Set}$ with the following three properties: $J$ is a subfunctor of $\mathrm{Sv}$ . Here $\mathrm{Sv}(X)$ is the set of sieves on $X$ and for any morphism $f : Y \to X$ the map $f^* : \mathrm{Sv}(X) \to \mathrm{Sv}(Y)$ is given by pullback of sieves (see https://en.wikipedia.org/wiki/Grothendieck_topology ) $J_{\mathrm{ind}}$ is a subfunctor of $J$ , where $J_{\mathrm{ind}}(X) = \{\mathrm{Hom}_{\mathcal C}(-,X)\}$ . For all $X \in \mathcal C$ and all $S \in J(X)$ , we have $$ J(X) = \{T \in \mathrm{Sv}(X) \mid \forall (f : Y \to X) \in S : f^*T \in J(Y)\}$$ 1 and 2 just say, that $J$ is somewhere between the indiscrete and the discrete Grothendieck topology. I am completely happy with that. The way I read 3 is: Being a $J$ -covering sieve is a $J$ -local property. This looks like a ""circular"" sheaf condition. If $X \in \mathcal C$ and $T$ is a sieve on $X$ , then we define a presheaf on $\mathcal C \downarrow X$ by $$ \mathcal F_{T}(f : Y \to X) = \begin{cases} \{*\}, &f^*T \in J(Y), \\ \emptyset, &\text{else.} \end{cases} $$ This is a $J$ -sheaf if and only if for all $Y$ and all covering sieves $S \in J(Y)$ , the map $$ \mathcal F_T(Y) \to \prod_{(f:V \to Y)\in S} \mathcal F_T(V) $$ is bijective. This for all $X$ and $T$ is equivalent to 3. So we can take: For all $X \in \mathcal C$ and all $T \in \mathrm{Sv}(X)$ , the presheaf $\mathcal F_T$ on $\mathcal C \downarrow X$ is a $J$ -sheaf. Grothendieck topologies are often advertised as the ""right generalization"" of topological spaces for the definition of sheaves. Since 1 and 2 don't say much about $J$ , all interesting topological features hide inside of 3. So it might be interesting to ask: Is there a nice(r) way to express 3? (or the entire definition?) Creative answers are highly appreciated. PS: I don't know the details of the discussion in the chat in the masterclass, because I watched it on Youtube ( https://www.youtube.com/watch?v=OT65JC3gKPY , at 3:00). Comments about the content of this discussion are also appreciated.","['topos-theory', 'algebraic-geometry', 'category-theory']"
3925135,Sangaku: to prove one of the intangents is parallel to $BC$,"Given an acute triangle $\triangle ABC$ whose incircle is $I(r)$ . Let $O(R)$ be the circle through $B$ and $C$ and which touches $I(r)$ interiorly. Show that the circle $P(p)$ which is tangent to $AB$ , $AC$ and $O(R)$ (externaly) is such that one intagent line from $P(p)$ and $I(r)$ is parallel to side $BC$ . I have seen a couple solutions for this online but I never really understood them and they all seem wrong to me. For example, this is the problem 2 in here . The solution they give doesn't ring a bell. When they say that $\angle ACB - \angle ADE = \angle AED - \angle ABC$ (which is correct) and then they claim that $\angle ADE < \angle ACB$ and $\angle AED > \angle ABC$ imply that $\angle ADE=\angle ABC$ it just sounds wrong. They could just use the congruence of $ADE$ and $AFG$ but instead they use this confuse argument. And things get worse after the co-axial system. They give a huge jump and conclude that $BCD'E'$ is cyclic. This is best solution I have found for this problem but I just can't agree with it.","['sangaku', 'euclidean-geometry', 'circles', 'geometry']"
3925148,inequality $\frac{2(x + y)^2}{2x^2 + y^2} \leq 3$,"I'm looking for a ""nice"" way to show that the following inequality holds, i.e. without differentiating and determining the maximum: $$
\frac{2(x + y)^2}{2x^2+ y^2} \leq 3 \quad \forall x,y \in \mathbb{R}
$$ It's rather easy to show $$
\frac{4xy}{x^2 + y^2} \le 2
$$ and obviously $$
\frac{2x^2 + 2y^2}{2x^2+y^2} \le 2
$$ but combining these two does not give me a sufficient low bound.","['multivariable-calculus', 'calculus', 'algebra-precalculus', 'inequality']"
3925172,Why is it that the congruence relations usually correspond to some type of subobject?,"From the perspective of universal algebra, quotient structures of algebraic structures are built using congruence relations . If $A$ is an algebraic structure (a set with a bunch of operations on the set) und $R$ congruence relation on a set, then the quotient $A/R$ is well-defined and it will be an algebraic structure of the same type. Now, as it turns out, in particular algebraic categories, these congruence relations on $A$ correspond exactly to some type of subobject of $A$ . For instance, the congruence relations on a ring correspond precisely to the ideals of that ring; the congruence relations on a group correspond precisely to the normal subgroups of that group; the congruence relations on a module correspond precisely to the submodules of that module. Why is it that the congruence relations usually correspond to some type of subobject? Is this a general phenomenon that can be generalized to all algebraic structures (as studied in this generality by universal algebra)?","['universal-algebra', 'category-theory', 'abstract-algebra', 'ideals', 'soft-question']"
3925189,Alternative proof for multiplicativity of resultant?,"The formula $R(fg,h)=R(f,h)R(g,h)$ follows easily if you express each resultant in terms of the roots of the polynomials involved. It can also be obtained by relating $R(f,h)$ to the determinant of multiplication by $f$ on the space $K[x]/(h)$ . But is there a  way to prove the above  formula by extending the three Sylvester matrices involved by cleverly chosen blocks  so that the determinants don't change, the three matrices become square matrices of equal size, and one matrix is the product of the other two?","['matrices', 'determinant', 'polynomials', 'resultant']"
3925232,${\rm Aut}(S_4)$ are all inner automorphisms.,"I have been able to follow the hints given in Hungerford and I have proved the fact that if $\sigma \in{\rm Aut}(S_4)$ and $\sigma$ fixes all the sylow-3 subgroups of $S_4$ then it is an identity map,  however how am I going to show that if $\phi:S_4\to{\rm Aut}(S_4)$ then the map is surjective? Can someone explain with some examples as to why this will hold true (not any proof but examples as to why any other form of automorphsim except inner automorphism will not be an automorphism)","['automorphism-group', 'group-theory', 'abstract-algebra', 'sylow-theory']"
3925259,What is the meaning of this sequence of random variables?,"Let consider a sequence of independent identically distributed random variables $(X_i)_{i\ge 1}$ which takes value in $\{1,...d\}$ with $p_k=\mathbb{P}(X_1=k)$ , for $k=1,...,d.$ Let $N \sim Poisson(\lambda)$ which is independent from the $(X_i)_{i\ge 1}$ . Then we define for $k=1,...,d$ : $N_k = \sum \limits_{i=1}^{N} \mathbb{1}_{\{X_i=k\}}$ . I have to find the probability distribution of the vector $(N_1,...,N_d)$ . But the fact is that I do not know how to interpret the random variable $N_k$ . Indeed the last term of the sum is $\mathbb{1}_{\{X_N = k\}}$ contains the index $N$ which is a random variable. For instance if we consider $k=1$ then $N_1 =\mathbb{1}_{\{X_1 = 1\}}+...+\mathbb{1}_{\{X_N = 1\}}$ . Thanks in advance !","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
3925310,Is there always an $O(n)$ number dividing only $O(1)$ among a set of $n$ numbers $\le n^2$?,"Let $n\in\mathbb{N}$ , $S\subseteq\mathbb{N}_{\le n^2}$ , $|S|=n$ . Is there always an $m=O(n)$ dividing only $O(1)$ numbers from $S$ ? Since the question wasn't clear to some people, here's a version with no weird big- $O$ : are there $k,c\in\mathbb{N}$ such that for every $n\in\mathbb{N}$ and every set $S$ of $n$ natural numbers all of which are $\le n^2$ , there exists an $m\in\mathbb{N}$ such that $m≤kn$ and the number of multiples of $m$ that are in $S$ is $\le c$ ? I'm also interested in the $c=0$ case. If it helps anyone I've done some empirical testing for the $c=0$ case. The first column is $n$ , the second is the maximum of smallest possible $m$ over all suitable $S$ , and the third is the ratio $m/n$ . 2 5 2.5 
3 7 2.33333 
4 9 2.25 
5 13 2.6 
6 17 2.83333 
7 19 2.71429 
8 23 2.875 It's a bit late (sorry) but there will be no better time than now to explain the motivation. It is to obtain a tight asymptotic bound on a function related to the separating words problem in automata theory. Here you can find the definition and my $\Omega(n^{1/2})$ lower bound. A positive answer would imply that this bound is tight by improving the analysis in Lemma 3 of Robson's ""Separating strings with small automata"" (if $c>0$ then just join the automata). If you're interested in tight-bounding this function you can also assume that the distance between any two numbers in $S$ is at least $n$ , but I thought that "" $|S|=n$ "" was neater. The most significant thing here for people trying to solve I think is that, well, I don't really follow the paper after section 2 but primes seem to only come up in similar contexts so it might be that a positive answer would easily imply an improvement on Robson's bound on separating words (by taking off the $log^{3/5}n$ , which maybe ""only comes from"" this number-theoretic question), a problem that was open in the years 1989-2020 (finally solved by Chase) and famous during some of them. The best upper bound for $m$ that I know is $O(n \log n)$ , from the aforementioned Lemma 3. Consider prime numbers greater than $n$ , each number in $S$ can be divisible by at most two of them. Therefore $m$ can be some prime number between $n$ and the prime number $2n$ primes after $n$ , the latter of which is $O(n \log n)$ by the prime number theorem. I don't know where this should go but I think I've proven that the general case implies the $c=0$ case. Assume that $c=c_g,k=k_g$ works, then we'll prove that $c=0,k=c_g k_g$ works. Take some $n_0,S_0$ then take the appropriate $m$ for $c=c_g,k=k_g,n=c_g n_0,S=\{xi \mid x\in S_0, 1\le i\le c_g\}$ . This $m$ is $\le k_g c_g n_0$ and does not divide any number in $S_0$ .","['number-theory', 'elementary-number-theory']"
3925318,"Can we assign a measure on sequences $\epsilon_n\in\{-1,1\}$ such that $\sum_{n\ge 1}a_n \epsilon_n $ converges? If so, what is the image?","Let $(a_n)_n$ be a sequence of real numbers, let $(\epsilon_n)_n$ be a sequence of i.i.d random variables with $P(\epsilon_n=1)=P(\epsilon_n=-1)=1/2$ , and let $m$ denote Lebesgue measure. Consider the spaces $$
\mathcal{A} = \{(a_n)_n: a_n\in\mathbb{R}\};\qquad \mathcal{E} = \{(\epsilon_n)_n: \epsilon_n = \pm 1\}
$$ $\mathcal{E}$ can be considered a ""random choice of sign"" in the probabilistic context, for example. Lastly, define a map $$
r:\mathcal{E}\to[0,1], (\epsilon_n)_n\mapsto \sum_{n\ge1}\frac{1+\epsilon_n}{2^{n+1}}
$$ My question: is the following map $$
f: \mathcal{A} \to \left[0,1\right],\,
(a_n)_n \mapsto m\left(\left\{x\in[0,1]:\sum_{n\ge 1}{r^{-1}(x)} a_n \text{ converges}\right\}\right)
$$ well-defined? If so, is it surjective; if not, what is its image? For example, if $(a_n)_n=1/n$ , by Kolmogorov's Three-Series theorem, $\sum_{n\ge 1} \epsilon_n/n$ converges almost surely, so $f((1/n)_n)=1$ ; likewise, $f((1/\sqrt{n})_n)=0$ . Many sequences (in particular, those that whose series are absolutely convergent) map to $1$ and many sequences map to $0$ (such as sequences with $\lim_{n\to\infty} a_n\ne 0$ ), but I struggled to find a sequence $(b_n)_n$ with $0<f((b_n)_n)<1$ . I've read a bit about random harmonic series and the Three-Series theorem but haven't made it much past that.","['measure-theory', 'functions', 'sequences-and-series', 'convergence-divergence', 'probability-theory']"
3925348,Compute the series $\sum_{n=1}^{+\infty} \frac{1}{n^3\sin(n\pi\sqrt{2})}.$,"I need to compute $$\sum_{n=1}^{+\infty} \frac{1}{n^3\sin(n\pi\sqrt{2})}.$$ This an exercice of ""Amar and Matheron, complex analysis"". I proved the convergence and now to compute the sum, I follow the hint of the book which is : Consider integrals of the form $$\int_{\gamma}\frac{dz}{z^3[\sin(\pi z)\sin(\sqrt{2}-1)\pi z]}$$ for a well-chosen $\gamma.$ I know this a residue theorem application but it seems a bit hard to have the good idea. I also tried with a summation factor. Any help will be greatly appreciated.","['complex-analysis', 'contour-integration', 'residue-calculus', 'sequences-and-series']"
3925366,Haar measure on Lie Group is unique,My question is Why is the Haar measure on a Lie Group unique upto scalar multiple? I know how to show it for $\mathbb R^n$ because there I have countable many open balls that form a base and the measure of the unit ball around 0 gives the constant scalar. How to show for general Lie Group?,"['measure-theory', 'haar-measure', 'lie-groups']"
3925385,Can we show $\frac{\nu\left(B_\varepsilon(x)\right)}{\mu\left(B_\varepsilon(x)\right)}\xrightarrow{\varepsilon\to0+}\frac{{\rm d}\nu}{{\rm d}\mu}(x)$?,"Let $(E,\mathcal E,\mu)$ be a finite measure space and $\nu$ be a finite signed measure on $(E,\mathcal E)$ with $\nu\ll\mu$ . By the Radon-Nikodým theorem, $$\nu=f\mu\tag1$$ for some $f\in L^1(\mu)$ . If $(E,d)$ is a metric space and $\mathcal E=\mathcal B(E)$ , are we able to show that $$g_\varepsilon(x):=\left.\begin{cases}\displaystyle\frac{\nu\left(B_\varepsilon(x)\right)}{\mu\left(B_\varepsilon(x)\right)}&\text{, if }\mu\left(B_\varepsilon(x)\right)>0\\0&\text{, otherwise}\end{cases}\right\}\;\;\;\text{for }x\in E$$ is convergent in a suitable mode of convergence (e.g. in $\mu$ -almost everywhere or in $L^1(\mu)$ as $\varepsilon\to0+$ and the limit $g$ is a version of $f$ ? If someone isn't aware of a rigorous proof, but can give an intuitive explanation of the basic idea, I'd appreciate that as well.","['measure-theory', 'lp-spaces', 'functional-analysis', 'probability-theory', 'radon-nikodym']"
3925391,Has there been any exploration on Cubic Power Series?,"I was interested in finding some identities/special values involving the function $$\gamma(z) = \sum_{i=0}^{\infty} z^{i^3} = 1 + z + z^8 + z^{27} + ... $$ which can be thought of as a ""cubic generalization of the famous $$\theta(z) = \sum_{i=0}^{\infty} z^{i^2} = 1 + z + z^4 + z^9 + ... $$ Which can be cooked up using jacobi theta functions. Unfortunately the term ""cubic theta function"" doesn't lead to any insight on this series since the ""cubic"" is reserved for a type of identity as opposed to the form of the series. Surely these have been looked at before does anyone have any links/intel about them?","['functional-equations', 'reference-request', 'complex-analysis', 'lacunary-series', 'theta-functions']"
3925433,Number of ways to represent any N as sum of odd numbers? [duplicate],"This question already has an answer here : Representing a nonnegative integer as the ordered sum of odd numbers (1 answer) Closed 3 years ago . I was solving some basic Math Coding Problem and found that For any number $N$ , the number of ways to express $N$ as sum of Odd Numbers is $Fib[N]$ where $Fib$ is Fibonnaci , I don't have a valid proof for this and didnot understand that how this can be solved using recurrences Can someone provide with it ? If you are not getting it Suppose for N=4 number of ways to write it as sum of Odd Numbers is 3 which is Fibonnaci at $3$ $4=> 1+1+1+1$ $4=> 1+3$ $4=> 3+1$ NOTE-> the composition is ordered $( 1+3)$ and $(3+1)$ are different .
UPD -> I do not claim that I observed it myself but in the problem solution I found it , I asked to just find some valid proof / reason to it","['integer-partitions', 'fibonacci-numbers', 'combinatorics', 'recurrence-relations']"
3925444,"Prove $f:[a,b) \longrightarrow (a,b)$ cannot be bijective and continuous (verification)","I would just like to make sure my proof for this is correct. If I made any wrong assumptions, please do let me know so I can correct them or even try a different approach. Thank you! Let $f(a)=c$ , by the bijectivity of $f$ . Now, consider the case where we remove $c$ from $(a,b)$ and what we are left with is a disconnected set which is the union of two connected sets, namely $(a,c) \cup (c,b)$ . We now impose a restriction on $f$ defined by $$g: (a,b) \longrightarrow (a,c) \cup (c,b), \ g(x) = f(x)$$ This restriction is continuous (by Theorem proved in class). The image of $g$ is the union of two connected components. Since $(a,b)$ is connected, its image must also be connected, which means it must lie entirely within one of the two components. But then this would imply that the original function, $f$ , is a proper subset of $(a,b)$ which contradicts the bijectivity of it. As such, it cannot be bijective and continuous. The intervals are regarded with the subset topology induced by the standard topology of $\mathbb R$","['continuity', 'general-topology', 'solution-verification']"
3925504,Can a given symmetric matrix be written as a linear combination of the identity and a rank-$1$ matrix?,"Given an invertible symmetric matrix $M$ , is there a way to determine whether it can be written as $$M = a v v^T + b \mathrm{I}$$ for some scalars $a,b \neq 0$ and a vector $v$ ? I'm given matrix $M$ and want to find $a, b$ and $v$ .","['matrix-rank', 'matrices', 'linear-algebra', 'symmetric-matrices', 'matrix-decomposition']"
3925514,An example of $p$-divisible group from the book of Demazure.,"I have some questions when I read the book ""Lectures on $p$ -divisible groups"". Precisely, my questions are in (page 76) chapter IV, section 3: The $F$ -spaces $E^{\lambda}, \lambda\geq 0$ . the problems in summary contain computations of kernel of maps of group schemes, invariants related with $p$ -divisible groups: Dieudonné modules, Hopf algebras, height, dimension... I put the questions as follows (should be classical ones as it is just quickly mentioned in the book) with some of my attempts (please correct the mistakes I possibly made): Question 1. In page 76, $\overline{M}^{\lambda}:=\mathbb{Z}_p[F]/(F^{r-s}-V^s)$ is defined, which I am already confused with: 1.1 Maybe $\overline{M}^{\lambda}:=\mathbb{Z}_p[F, V]/(F^{r-s}-V^s, FV=p, VF=p)$ ? But this module have elements of the form, say $V^n$ with $2\leq n\leq s-1$ , which not seems not expected in the original expression. 1.2 Or maybe for some reasons we can just put $V:=pF^{-1}$ and somehow it makes sense in $\mathbb{Z}_p[F]$ ? 1.3 Remark that $\mathbb{Q}_p\otimes \overline{M}^{\lambda}$ makes ""more sense"" for me. I can look at $\mathbb{Q}_p\otimes M^{\lambda}=E^{\lambda}=\mathbb{Q}_p[T]/(T^r-p^s)$ : as $p^{-1}$ exists, hence $T^{-1}$ exists. Hence change $T$ by $F$ we can rewrite the module as \begin{align*}
\mathbb{Q}_p[F]/(F^r-p^s)&=\mathbb{Q}_p[F]/(F^{-s}(F^r-p^s))\\
&=\mathbb{Q}_p[F]/(F^{r-s}-(pF^{-1})^s)\\
&=\mathbb{Q}_p[F]/(F^{r-s}-V^s)
\end{align*} where $V:=pF^{-1}$ .
This computation some how ""convince"" me with the statement in the book that "" $\overline{M}^{\lambda}$ is a lattice in $E^{\lambda}$ "", but still question 1 have to be answered. Question 2 (main question). In the same paragraph as question 1, it is claimed that $\overline{M}^{\lambda}$ is a Dieudonné module, precisely it is the associated Dieudonné module of the following constructed $p$ -divisible group: Let $G^{\lambda}$ be the $p$ -divisible group over $\mathbb{F}_p$ defined by the exact sequence $$ 0\to G^{\lambda} \to W(p) \xrightarrow{F^r-V^s} W(p)  $$ where $W(p)=\varinjlim(\mathbf{Ker}p^n: W_{\mathbb{F}_p}\to W_{\mathbb{F}_p})$ . In other words, the statement is: $M(G^{\lambda})=\overline{M}^{\lambda}$ (the Dieudonné module) and $E(G^{\lambda})=E^{\lambda}$ (the $F$ -space). 2.1 How to look at $W(p)=\varinjlim(\mathbf{Ker}p^n: W_{\mathbb{F}_p}\to W_{\mathbb{F}_p})$ ? Is it a $p$ -divisible group? (seems not: as I computed below, it looks not like a direct limit of finite group schemes, at least.) 2.1.1 How should I describe the group scheme $\mathbf{Ker}p^n: W_{\mathbb{F}_p}\to W_{\mathbb{F}_p}$ ? Can I describe in the following way: For $W_{\mathbb{F}_p}$ , as group scheme I may write $W_{\mathbb{F}_p}=\prod_{n\geq 0}\mathbb{A}^1_{\mathbb{F}_p}=\mathbf{Spec}  \mathbb{F}_p[X_0, X_1, \cdots]$ . Remark that we are working in characteristic $p$ case and hence we have $p^n: W_{\mathbb{F}_p}(R)\to W_{\mathbb{F}_p}(R)$ for some $\mathbb{F}_p$ -algebra $R$ is of the form $(x_0, x_1, \cdots)\mapsto (0, \cdots, 0, x_0^{p^n}, x_1^{p^n}, \cdots)$ where $x_0^{p^n}$ is in the $n+1$ -th position. (From this, by mimic the situation in algebraic geometry, the morphism on the coordinate ring should be $f: \mathbb{F}_p[X_0, X_1, \cdots]\to \mathbb{F}_p[Y_0, Y_1, \cdots]: f(X_0)=\cdots =f(X_{n-1})=0, f(X_n)=Y_0^{p^n}$ , $f(X_{n+1})=Y_1^{p^n}\cdots$ ) hence the kernel of $p^n$ described by should be $\require{AMScd}$ \begin{CD}
\mathbf{Ker}p^n @>{}>> W_{\mathbb{F}_p}\\
@VVV @VVp^nV\\
\mathbf{Spec}\mathbb{F}_p @>{}>> W_{\mathbb{F}_p}
\end{CD} which corresponds to $\require{AMScd}$ \begin{CD}
 \mathbb{F}_p[X_0, X_1, \cdots]\otimes \frac{\mathbb{F}_p[X_0, X_1, \cdots]}{I}  @<{}<<  \mathbb{F}_p[X_0, X_1, \cdots] \\
@AAA @AAp^nA\\
\mathbb{F}_p @<{}<< \mathbb{F}_p[X_0, X_1, \cdots]
\end{CD} with $I=(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)$ . Hence the kernel of $p^n$ is like $W_n(p)=\mathbf{Spec}\frac{\mathbb{F}_p[X_0, X_1, \cdots]}{(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)} $ and the limit is finally again $\mathbf{Spec}\mathbb{F}_p[X_0, X_1, \cdots]$ or maybe $\mathbf{Spec}\mathbb{F}_p[\![X_0, X_1, \cdots]\!]$ or else? 2.1.1.1 As this is not finite group, what is $\varinjlim \mathbf{Spec}\frac{\mathbb{F}_p[X_0, X_1, \cdots]}{(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)}$ ? (Is it $\mathbf{Spec} \varprojlim\frac{\mathbb{F}_p[X_0, X_1, \cdots]}{(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)}$ ? This may not be as in the formal group case, where the Hopf algebra is the inverse limite of the Hopf algebra of the finite group schemes. What is $\varprojlim\frac{\mathbb{F}_p[X_0, X_1, \cdots]}{(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)}$ ? Can I relate $\mathbf{Spec} (1+\mathbb{F}_p[\![t]\!])$ with $W_{\mathbb{F}_p}$ using Artin-Hasse exponential?) 2.2
How could I compute the Dieudonné module of $G^{\lambda}$ ? 2.2.1
How could I compute the Dieudonné module of $W(p)$ ? Since I may try to compute it by the exact sequence $$ M(W(p))\xrightarrow{V^r-F^s} M(W(p))\to M(G^{\lambda})\to 0$$ 2.2.2 How to calculate $M_n(W(p))$ ? As $M_n(W(p))\simeq M(W_n(p))$ , hence how to compute $M(W_n(p))$ ? (Having $W_n(p)=\mathbf{Spec}\frac{\mathbb{F}_p[X_0, X_1, \cdots]}{(X_0^{p^n}, \cdots, X_m^{p^n}, \cdots)}$ , it is not clear how can I have the claimed Dieudonné module.) 2.3 Finally, with the correct Dieudonné module we can see the correct height (which is the rank of the Dieudonné module) and correct dimension (which is the dimension of as formal group). How to see the dimension is $s$ as claimed? Thanks in advance for any remarks on any of these questions; the problems are in summary: the computations of kernel of maps of group schemes, invariants related with $p$ -divisible groups --- some nontrivial examples (even hard to find), Dieudonné modules, Hopf algebras... Any general remarks or good references are also welcome.","['number-theory', 'algebraic-geometry', 'galois-representations', 'arithmetic-geometry']"
3925523,Is this a correct way to show that $\sum_{n \geq 0} \frac{n^3}{n!}=5e$,"Is this a correct way to show that $\sum_{n \geq 0} \frac{n^3}{n!}=5e$ ? $$S_3 = \sum_{n \geq 0} \frac{n^3}{n!}=\sum_{n \geq 1} \frac{n^2}{(n-1)!} \implies$$ $$S'_3=S_3-e=\sum_{n \geq 1} \frac{n^2-1^2}{(n-1)!}=\sum_{n \geq 2} \frac{n+1}{(n-2)!} \implies$$ $$S'_3-3e=\sum_{n \geq 2} \frac{(n+1)-3}{(n-2)!}=\sum_{n \geq 3} \frac{1}{(n-3)!}=e\implies$$ $$S'_3-3e=e\iff S_3=5e$$ Exploring $ \sum_{n=0}^\infty \frac{n^p}{n!} = B_pe$, particularly $p = 2$. : One of the answers shows how $\sum_{n \geq 0} \frac{n^2}{n!}=2e$ and asserts that $\sum_{n \geq 0} \frac{n^3}{n!}=5e$ can be showed in the same manner of reasoning. Is my ""proof"" correct?",['sequences-and-series']
3925616,"Find the limit of $a_1 = 1 $ , $ a_{n+1} = \frac{\sqrt{1+a_n^2}-1}{a_n}$ , $n \in \mathbb{N}$","$a_1 = 1 $ , $ a_{n+1} = \frac{\sqrt{1+a_n^2}-1}{a_n}$ , $n \in \mathbb{N}$ It is easy to prove that the limit exists: the boundary of that expression is $0$ and it is monotonically decreasing. The problem is to actually find the limit (it is $0$ ) because if I take arithmetic of limits: $ \lim_{x \to +\infty} a_{n+1} =  \frac{\sqrt{1+ \lim_{n \to +\infty} a_n^2+1}}{\lim_{n \to +\infty} a_n}$ $g = \frac{\sqrt{1+g^2+1}}{g}  \implies g^2 = {\sqrt{1+g^2}-1} \implies g^2 + 1 = {\sqrt{1+g^2}} \implies 0 = 0$","['limits', 'sequences-and-series', 'analysis', 'real-analysis']"
3925618,Very Difficult Enumeration Problem,"Question : Suppose I have $k$ numbers $0,..,k-1$ in circular arrangement, or equivalently the classes $\mod k$ . Find the number of ways the numbers can be assigned to the sequence $x_1,x_2,...,x_n$ with $2n<k$ such that: $x_i\ne 0,\forall i\in\{1,...,n\}$ $x_i\ne\pm x_j, \forall i,j\in\{1,...,n\}$ $x_i+x_j\ne 1,\forall i,j\in\{1,...,\ell\}$ where $\ell\leq n$ edit: every operation is in $\mathbb{Z}_k$ above, you can assume $k$ to be odd I can do this without the last condition as I can clearly see it is equal to $(k-1)(k-3)...(k-2n+1)$ as each time I simply get to choose from the remaining elements of $\mathbb{Z}_k\backslash\{\pm a_1,...,\pm a_{i-1}\}$ . Maybe this can take the form of placing the $x_i$ in $k$ boxes or something? edit: I think I have an idea consider the numbers of $\mathbb{Z}_k$ ordered like so: $1,-1,2,-2,...,\frac{k-1}{2},-\frac{k-1}{2}$ as boxes for $x_1,..,x_{\ell}$ the conditions above are equivalent to placing every $x_i$ element in a box such that no two adjacent boxes both contain an element. This is equivalent to partitioning the elements of $1,-1,2,-2,...,-\frac{k-1}{2}$ with the $x_i$ such that no $x_i,x_j$ have only one element in between. This is just the number of solutions to $a_1+a_2+...+a_{\ell+1}=k-1$ with $a_1\geq 1, a_i\geq 2, i\in\{2,...,\ell\}, a_{\ell+1}\geq 0$ which is $\ell!\binom{k-2\ell}{\ell}=\frac{(k-2\ell)!}{(k-3\ell!)}$ The rest of the $x_{\ell+1},...,x_n$ can be chosen from $k-2\ell-1$ as the example I mentioned giving $(k-2\ell)...(k-3\ell+1)(k-2\ell-1)(k-2\ell-3)...(k-2n+1)$","['combinatorics', 'discrete-mathematics']"
3925623,Kakutani theorem,"Let $(X_n)_n$ be a sequence of nonnegative independent random variables, such that for all $n \in \mathbb{N},E[X_n]=1.$ Prove that $Y_n=\prod_{k=1}^nX_k$ converges a.s. Is it possible to prove this without using martingale convergence theorem? (Since we don't know the limit, one to do it is to prove that for every $\epsilon>0, P(\sup_{k}|Y_{n+k}-Y_n|>\epsilon)\to_{n}0$ to conclude with the completeness of $\mathbb{R}$ , another way is if there exist a sequence of nonnegative real numbers $(\epsilon_n)_n$ such that $\sum_{n}\epsilon_n,\sum_nP(|Y_{n+1}-Y_n|>\epsilon_n)$ converge).","['measure-theory', 'probability-limit-theorems', 'probability-theory']"
3925631,From monoids to groups,"I was looking at the case when you go from the monoid of natural numbers to the group of integers by means of a suitable equivalence relation. The key here was to find inverses for each natural and I was wondering how it could be generalized. I mean: Given any monoid, when can I find inverses for your elements? In other words, can I always go from a monoid to a group?
If not, when is it? When do not? If you could tell me where I can find information about this (a book or paper) it would be very helpful. Thank you.","['monoid', 'group-theory', 'abstract-algebra', 'semigroups']"
3925831,Being the first to pick 1 of 2 cards out of a deck of 52 that will win you 1 million dollars.,"Contestants in a game show are asked to from a line.
One by one, each of them will be given one card (face up) from a deck of 52 until someone gets either an Ace of Spades or an Ace of Clubs.
The first person who gets one of those cards will receive a sum of 1 million dollars. Before the game begins, you have the chance of choosing where in the line to position yourself in.
How will you find out where to which position will be the one most likely to be the first to receive one of the 2 black Ace cards. My first thought was to do a mock version of this and count how many cards were laid out before the first black Ace appeared. I repeated this for 25 times while recording the results for each trial to hopefully get an estimate to the average number of cards distributed before one of the black Aces appeared.",['probability']
3925839,"$\min(a,b+c)\leq \min(a,b)+ \min(a,c)$ for $a,b,c\geq 0$","Given $a,b,c\geq 0$ , then this following is true $$\min(a,b+c)\leq \min(a,b)+ \min(a,c)$$ I tried to prove it by using some cases, i.e. Case 1: if $\,\,a\leq b \leq c\,\,$ and $\,\,a\leq c \leq b,\,\,$ it's clear that \begin{align*}
\min(a,b+c)&\leq \min(a,b)+\min(a,c)\\
a &\leq a+a=2a
\end{align*} Case 2: if $\,\,b\leq a\leq c\,\,$ and $\,\,c\leq a \leq b,\,\,$ we have \begin{align*}
\min(a,b+c)&\leq \min(a,b)+\min(a,c)\\
a &\leq b+a
\end{align*} and \begin{align*}
\min(a,b+c)&\leq \min(a,b)+\min(a,c)\\
a &\leq a+c
\end{align*} Case 3: $\,\,c\leq b\leq a\,\,$ and $\,\,b\leq c \leq a,\,\,$ I am not sure about this case, because although $a$ is the biggest from $b$ and $c$ , it can satisfies $a\leq b+c$ or $b+c\leq a$ . Is the third case must be done with two cases again? i.e for $a\leq b+c$ and $b+c\leq a$ ? or there is another way to prove $\min(a,b+c)\leq \min(a,b)+ \min(a,c)$ for $a,b,c\geq 0$ ? Any help will be appreciated. Thanks.","['algebra-precalculus', 'inequality']"
3925846,How do we show this variety is normal?,"I just saw my question is closed. I think I need to provide some details. Suppose $X$ is a variety over a field $k$ and $f: X\to S=\mathbb{A}_{k}^{1}=\operatorname{Spec}(k[t])$ is a flat morphism. Assume the restriction of the morphism $f$ over $\operatorname{Spec}k[t,1/t]$ is smooth and the fiber over $t=0$ is an integral scheme. I want to show that $X$ is normal. After thinking for a long time, I think we may use Serre’s $R_{1}+S_{2}$ to prove it. For $R_{1}$ , it is clear, since the fiber over $0$ is integral, the singular point in this fiber cloud just be an proper closed subset of the fiber. Since other fibers are smooth, all singular locus could just be a codimension 2 subset. So it is regular codimension. Notice we do not use flat to prove $R_{1}$ , so I think we need to use flat to prove $S_{2}$ . But I do not how to do it.","['algebraic-geometry', 'schemes']"
3925859,Is there (or can there be) a general algorithm to solve Rubik's cubes of any dimension?,"I love solving Rubik's cube (the usual 3D one). But, a lecture by Matt Parker at the Royal Institute ( YouTube Link ) led me to an app that can simulate a four dimensional rubik's cube. But unfortunately it was so complex, that I soon got bored as I failed to solve it. The website to the 4D cube : http://superliminal.com/cube/cube.htm Following which today, I also found an app that can simulate a 5D cube!! The website to the 5D cube: http://www.gravitation3d.com/magiccube5d/ So, my two questions are : Is there a general (non brute-force) algorithm that can be used to solve a well-scrambled cube of any dimension (even though it may not be very efficient, but yet is not a simple search over all the available space of move sequences) ? [Point modified after reading @RavenclawPrefect's answer :D ] Mathematically, what is common in all these cubes, and ""hypercubes""? NOTE: By dimensions I mean physical dimension and not the number of rows of pieces on a face of the cube. So a four-dimension cube is a cube in x,y,z, $\delta$ dimensions where $\hat x,\hat y,\hat z,\hat \delta$ are mutually orthogonal unit vectors in the respective dimensions. For example, here is how the aforementioned 4D cube moves: https://miro.medium.com/max/2552/1*ga32DoV_Hc6e8t6PC1hFHw.gif Addendum: List of resources that may help finding an answer to this question: Solving Rubik's cube and other permutation puzzles","['puzzle', 'recreational-mathematics', 'algorithms', 'group-theory', 'rubiks-cube']"
3925919,"Closed-form formula for $E_{x}[\max(u^\top x,0)\max(v^\top x ,0)]$ where $u,v$ are fixed vectors in $\mathbb R^d$ and $x$ is uniform on the sphere","Let $x$ be uniformly distributed on the unit-sphere in $\mathbb R^d$ and let $u,v$ be fixed nonparallel vectors in $\mathbb R^d$ Question. Is there a closed-form formula for $f(u,v) := \mathbb E_{x}[\max(u^\top x,0)\max(v^\top x ,0)]$ ? Due to rotationational symmetry of the distribution of $x$ , my guess is that $f(u,v)$ admits a simple expression in terms of the angle between $u$ and $v$ .","['statistics', 'geometric-probability', 'probability']"
3925920,"How to prove that $|x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1})$ when $x,y \geq 0$ and $p \geq 1$? [duplicate]","This question already has an answer here : Prove that $|x^p - y^p| \le p|x-y|(x^{p-1} + y^{p-1})$ provided that $1 \le p \lt \infty$ and $x, y \ge 0$ (1 answer) Closed 3 years ago . For $x \geq 0$ , $y \geq 0$ , prove that $$
|x^p-y^p| \leq p|x-y|(x^{p-1}+y^{p-1}).
$$ I thought it would be simple but I messed everything up. Here are my attempts. Fixing $x > y \geq 0$ , I thought about the function $$
h(p)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1})
$$ Then $h(1)=0$ and I wanted to prove that $h(p) \leq 0$ when $p \geq 1$ by discussing its derivative but the derivative is messy offering no way out. Also I thought about after fixing $y \geq 0$ , consider the function $$
\varphi(x)=x^p-y^p-p(x-y)(x^{p-1}+y^{p-1})
$$ where $\varphi(y)=0$ and I want to prove that $\varphi(x) \leq 0$ when $x \geq y$ yet again it's a messy way. I also thought about restricting $p$ to $\mathbb{N}$ or $\mathbb{Q}$ using decomposition like $$
(x^p-y^p)=(x-y)(x^{p-1}+x^{p-1}y+x^{p-2}y^2+\cdots+y^{p-1})
$$ but the number of terms does not match. But I do believe this should be a easy question with some special background. I must have missed something critical. Any hint/solution appreciated!","['multivariable-calculus', 'calculus', 'inequality']"
3925924,"If there exsits a constant M such that $ \forall x\in [a,b], \forall n\geq 1,|S'_n(x)|\leq M, $ then $\sum_{n=1}^\infty a_n(x)$ converges uniformly.","Let series of functions $\sum_{n=1}^\infty a_n(x)$ converges on $[a, b]$ , its partial sum are $S_n(x)$ . If there exsits a constant M such that $$
\forall x\in [a,b], \forall n\geq 1,|S'_n(x)|\leq M,
$$ then $\sum_{n=1}^\infty a_n(x)$ converges uniformly. I tried to prove the problem by Cauchy criterion. $$
S_{n+p}(x)-S_n(x)=S_{n+p}'(\xi_1)(x-a)+S_{n+p}(a)-S_{n}'(\xi_2)(x-a)-S_{n}(a)
$$ But it seems we are unable to to control $|S'_{n+p}-S'_n|$ , so I  amstuck here. Is my way wrong, if so, please suggest a correct way. Appreciate any help!","['sequences-and-series', 'uniform-convergence', 'analysis', 'real-analysis']"
3925948,General knowledge regarding double convergence of sequences (and series).,"While proving a subspace of $\textit{L}^p(\mathbb{R}^n)$ is dense I stumbled upon a family of functions that was double indexed; this doesnt fall in our usual definition of sequence, it is merely a set of functions $\{f_{i,j}\}_{i,j\in \mathbb{N}}$ . The study of a collection of this type wasn't necessary for my objective, but i found a few papers online regarding double convergence and how it is defined. As a first thing i would like to know if anybody knows about this part of analysis, becouse although it sound interesting, it's probably not all that popular. To give some context to people who don't know what this is, ill give a first definition of convergence: Given a measure space $(X,\textit{A},\mu)$ and given $E\in \textit{A}$ with $|E|<+\infty$ ( $\star$ is this limitation necessary? $\star$ ), then we say that $\{f_{n,m}\}$ converges in the Pringsheim’s sense if for every $\epsilon > 0$ there exists $N∈\mathbb{N}$ such that $|f_{m,n}(x_0) −f|< \epsilon$ whenever $j$ , $k ≥N$ . And all the other kind of convergences can be defined as well (a.e., in measure,...) I now ask, what properties differenciate them from the normal sequences and convergence? By just thinking of examples i am pretty sure a major difference is that if a double sequence converges, it still can have subsequences converging elswhere, for example: $$f_{n,m}(x)=\frac{1}{n}+\frac{1}{m} \Longrightarrow \lim_{m\to +\infty}f_{n,m}=\frac{1}{n} \mbox{ and } \lim_{n\to +\infty}f_{n,m}=\frac{1}{m}$$ so when we fix one of the two between $n$ and $m$ we obtain a normal converging sequence: $$\forall \epsilon > 0\mbox{, } \exists N_1\in \mathbb{N} \mbox{ such that } \forall n\ge N_1 \Rightarrow |f_{n,m}-\frac{1}{m}|\le \frac{\epsilon}{2}$$ $$\forall \epsilon > 0\mbox{, } \exists N_2\in \mathbb{N} \mbox{ such that } \forall m\ge N_2 \Rightarrow \frac{1}{m}\le \frac{\epsilon}{2}$$ so that $$\forall \epsilon > 0\mbox{, } \exists N:=\max\{N_1,N_2\}\in \mathbb{N} \mbox{ such that } \forall n,m\ge N \Rightarrow |f_{n,m}-0|=|f_{n,m}-\frac{1}{m}+\frac{1}{m}-0| \le$$ $$\le|f_{n,m}-\frac{1}{m}|+\frac{1}{m}<\epsilon$$ We have obtained the ""double convergence of the double sequence"", but if i get the subsequence $\{f_{1,m}\}_{m\in \mathbb{N}}$ , this converges in the normal definition to $1$ ; actually you could find infinite sunsequences, $\forall n\in \mathbb{N}$ , $f_{n,m}\longrightarrow \frac{1}{n}$ , for $m\rightarrow +\infty$ . As you might have seen, i am just looking for insight on what to think of this topic, maybe also knowing
what interesting results one can reach. Also I'm sorry if my english isn't that good.","['double-sequence', 'convergence-divergence', 'analysis', 'real-analysis']"
3925958,Show that $4x^2-yz$ is a perfect square,"Here is my problem. $A=xy+yz+zx$ , where $x,y,z\in\mathbb{Z}$ . It is known that if we add $1$ to $x$ , and subtract $2$ from both $y$ and $z$ , the value $A$ won't change. Prove that $-A$ is a square of whole number. My attempt: \begin{align}
A=xy+yz+zx
\end{align} and \begin{align}
A=(x+1)(y-2)+(y-2)(z-2)+(z-2)(x+1)
\end{align} comparing them we get \begin{align}
4x+y+z=0.
\end{align} Now we insert last equation to first equation: \begin{align}
-A&=-xy-yz-zx\\
&=-x(y+z)-yz\\
&=4x^2-yz
\end{align} From here I don't know how to show that $4x^2-yz$ is a perfect square.","['number-theory', 'square-numbers']"
3926007,Least squares regression of sine wave,"Let's say that a sine-like function of a fixed frequency and zero-mean can only vary in amplitude and offset. That is, for variables $a \in R^+$ and $z \in [0..2\pi]$ $$ f(t) = a(\sin(kt + z))$$ (where $k$ is a constant proportional to the fixed frequency) For some set of $n$ samples $(t_1,y_1),(t_2,y_2)...(t_n, y_n)$ we want to fit $a$ and $z$ such that they minimize: $$ L(a,z) = \sum(f(t_i)-y_i)^2 $$ If f was a linear function we could just use ordinary least squares regression. Is there some similar technique for the above function?  What approach would you use to minimize $L$ ?","['regression', 'statistics', 'linear-algebra', 'analysis']"
3926044,Test and probability / Bayes,"There is a virus test with has 95% reliability. We randomly choose one person from a country with 10,000,000 people, in which it is estimated that the number of cases of this virus is 20,000.
The selected person is tested positive. What is the probability that this person is actually positive? This is clearly an example of Bayes theorem use, but I am getting an odd result: $\mathrm{P}(A \mid Β) = \frac {\mathrm{P}(B \mid A).\mathrm{P}(A)}{\mathrm{P}(B)}$ $\mathrm{P}(A) = \frac {20,000}{10,000,000} = 0.002$ Since the test has 95% reliability, in the entire country (with population 10,000,000), if all were tested, there would be 95,000 people who are sick test positive and 95,000 of the healthy people test negative, therefore a total of 190,000. So: $\mathrm{P}(B) = \frac {190,000}{10,000,000} = 0.019$ (unconditional probability). Finally, since the sensitivity of the test is 95%, $\mathrm{P}(B \mid A) = 0.95$ . So from Bayes theorem we get $\mathrm{P}(A \mid Β) = \frac {0.95*0.002}{0.019} = 0.1$ , that is, 10%. But this is a bit counter-intuitive. Is it correct? Thank you","['bayes-theorem', 'probability']"
3926123,Prove that the image of $F$ is a closed set in $\mathbb R^n$,"Being $F : \mathbb R^n\to \mathbb R^n$ a continuous function such that for every $x,y\in \mathbb R^n$ , $$\|x − y\| \leq\|F(x) − F(y)\|\,.$$ I'd like to prove that the image of $F$ is a closed set in $\mathbb R^n$ . So far I have tried to use that if $x_n$ is a sequence in $\mathbb R^n$ such that $F(x_n)$ is a Cauchy sequence, $x_n$ must be Cauchy as well, in addition to the fact that any closed set has to contain all the limits of its sequences, but with this approach I haven't been able of find a proof. Any other ideas on how to solve this proof?","['multivariable-calculus', 'general-topology', 'cauchy-sequences', 'real-analysis']"
3926197,Permutations with global limited repetition,"What is the number of permutations of $m$ objects in $n$ spaces such that there are exactly $l$ repetitions in the permutation? for example,
if $m = 5, n = 7, l = 4$ then $$a_1, a_1, a_1, a_3, a_3, a_4,a_4$$ is a legal sequence (4 repetitions, $a_1$ twice, $a_3$ once and $a_4$ once) but $$a_1, a_2, a_3, a_1, a_3, a_5, a_3$$ is not (only 3 repetitions).
and neither is $$a_1,a_1,a_1,a_1,a_1,a_1,a_1$$ legal. The main difference of this problem from other posts such as Calculating number of permutations given N repeats allowed is that my number of repeats is global, i.e I require that the number of repetitions of all objects together is exactly l.",['combinatorics']
3926214,Operators commuting with tensor product representations of SU(2),"I am currently investigating $SU(2)$ symmetric qubit systems. In the course of this work I proved the following theorem: Let $S_n$ denote the permutation group of $n$ elements. For $\sigma\in S_n$ define the tensor product permutation operator $P_\sigma\in \operatorname{End}((\mathbb{C^2})^{\otimes n})$ as $$P_\sigma: v_1 \otimes \cdots \otimes v_n \mapsto v_{\sigma(1)}\otimes \cdots \otimes v_{\sigma(n)}$$ Let $T \in \operatorname{End}((\mathbb{C^2})^{\otimes n})$ be a linear operator. Then $$\left(\exists \{c_\sigma\}_{\sigma \in S_n} \subset \mathbb{C}: T=\sum_{\sigma\in S_n} c_\sigma P_\sigma\right)\iff \forall U \in SU(2): [T, U^{\otimes n}]=0$$ In words: $T$ commutes with the tensor product of the fundamental representation of $SU(2)$ if and only if $T$ is a linear combination of permutation operators. The implication “ $\implies$ ” is trivially true. However I also proved the other direction. Now my question is whether this theorem is already known? If yes, I would be grateful if someone could point me to some references.","['representation-theory', 'young-tableaux', 'tensor-products', 'group-theory', 'symmetry']"
3926229,How many ways are there to choose 4 letters out of the following 8 letters without duplicates?,"There are eight letters: $A BCD EF AG$ How many ways can I choose four letters without duplicates? (1) Eliminating the duplicate letter $A$ , no of ways is ${7 \choose 4} = 35$ (2) No of ways to choose $4$ out of $8$ letters is ${8 \choose 4} = 70$ $\quad $ No of duplicates is ${(8-2) \choose 2} = 15$ $\quad $ No of ways is $70 - 15 = 55$ Both seem right, I don't know why and which one of them is wrong? Thank you.","['combinations', 'combinatorics', 'discrete-mathematics']"
3926261,Antisymmetric relation from a real life,"Would you know some examples of antisymmetric relation from a real life? That is, relation $$(x,y)\in R\quad 
 \text{and}\quad (y,x)\in R \rightarrow  x=y.$$ Thanks for your help.","['relations', 'discrete-mathematics']"
3926389,"How to prove that the points $E, F, G$ are collinear","Given $\triangle ABC$ and $\odot(ABC) $ , $AG$ and $AD$ are the angle bisectors of $\angle A$ . $CD$ and $AB$ intersects at $E$ . $DB$ and $AC$ intersects at $F$ . Prove that $E, F, G$ are collinear. I found that $DC=DB$ . By Menelaus theorem I need to prove $$\frac{BG} {CG} \cdot \frac{CF} {AF} \cdot \frac{EA} {EB} =1$$ and do I need to use the angle bisector theorem also?","['contest-math', 'euclidean-geometry', 'geometry', 'plane-geometry']"
3926393,Show that rank($A^{n+1}$) = rank($A^n$) [duplicate],"This question already has answers here : Given a square matrix A of order n, prove $\operatorname{rank}(A^n) = \operatorname{rank}(A^{n+1})$ (3 answers) Closed 3 years ago . Suppose $A$ is a $n \times n$ matrix i.e. $A \in \mathbb{C}^{n \times n}$ , prove that rank( $A^{n+1}$ ) = rank( $A^n$ ). In other words, I need to prove that their range spaces or null spaces are equal. If it helps, $A$ is a singular matrix. Note that, I don't want to use Jordan blocks to prove this. Is it possible to prove this without using Jordan form? I can use Schur's triangularization theorem. Also, it's not known if A is diagonalizable.","['matrices', 'matrix-rank', 'linear-algebra']"
3926404,"Solve in rational numbers, the equation, $x\lfloor x\rfloor\{x\}=58$","Solve in rational numbers, the equation, $$x\lfloor x\rfloor\{x\}=58$$ where $\lfloor x\rfloor$ and $\{x\}$ are the greatest integer less than or equal to $x$ and the fractional part of $x$ respectively. I tried to make an equation like: $$(a+b)ab=58$$ where $a=\lfloor x\rfloor$ and $b=\{x\}$ and giving the restrictions that $a$ is an integer and $b$ is a rational number such that $0≤b<1$ . Then I made a quadratic equation over $b$ and used the famous quadratic formula. But everything became complex (not the mathematical 'complex') and so please help me out. Thanks in advance!",['functions']
3926449,How do I solve a first order differential equation of the following form,If I had a differential equation $x'(t) + \frac{x(t)}{t} = e^{t^2}$ And I have a question saying solve the following first ODE for $x(t)$ how would I go about doing this?,"['calculus', 'ordinary-differential-equations']"
3926456,Can you prove that $f(x) = 0$ for every linear functional $f$ implies $x=0$ in a normed space without using Hahn-Banach?,"In the context of another problem, I have to use the fact that if $f(x) = 0$ for all $f$ , then $x=0$ . I constructed my own proof of this using the Hahn-Banach theorem, which goes like this: Proof : Assume that $x \neq 0$ . Construct a functional $g$ in the following way: let $g(x) = 1$ and on $\text{span}(x)$ let $g$ be linear (this is well-defined on $\text{span}(x)$ ). By Hahn-Banach, $g$ can be continuously extended to the whole space, which contradicts the assumption that every functional is zero in $x$ . This is probably overkill for such a simple-looking statement, but I have no idea how to prove it in a more elementary way. Edit :
I actually found a way to circumvent using Hahn-Banach (but not the axiom of choice). It goes like this: Proof : Assume that $x \neq 0$ . Let $(e_i)_{i \in I}$ be a basis of our vector space (where $I$ is some index set). Suppose that $x = \sum_{i \in I} x_i e_i$ . Since $x \neq 0$ , there exists an $x_i$ which is not zero, let's call its index $j$ . Construct the following linear functional $g$ : On $e_j$ , let $g$ be 1, on every other basis vector, let $g$ be zero. In that case $$g(x) = g(\sum_{i \in I} x_i e_i) = \sum_{i \in I} x_i g(e_i) = x_j g(e_j) = x_j \neq 0 $$ which is a contradiction.","['vector-spaces', 'functional-analysis']"
3926527,Why are vector fields defined to be sections of the disjoint union of the tangent spaces? Isn't that overly complicated?,"It is usual to define the tangent bundle $TM$ as the disjoint union of all tangent space and then to define vector fields as sections of \begin{align}
TM=\bigsqcup_{p\in M}T_pM=\bigcup_{p\in M}T_pM\times\{p\}&\to M\\
(v,p)&\mapsto p
\end{align} (I am reading John Lee's book, where tangent vectors are introduced as derivations, such that $T_pM\cap T_qM\neq\emptyset$ .) Why don't we simply define $TM=\bigcup_{p\in M}T_pM$ and say that a vector field is a map $\omega\colon M\to TM$ with $\omega_p\in T_pM$ for all $p\in M$ . Isn't this the essential property of a vector field : That is assigns to all $p\in M$ an element of $T_pM$ ? An analogous question arises for Differential forms. An attempt to clarify the question: I have to prepare a talk about the Maxwell equations in terms of differential forms and I have an audience who does neither know what a manifold is, nor what a tensor ist. I have 30 minutes for a crash course in this stuff. This means that I have to leave out as much information as possible, but I want everything I say to be correct. So one question I asked myself is: Do I miss out on something if I don't explain what disjoint unions and sections are and simply give them my definition above? Is there a good reason to define the tangent bundle as the disjoint union of tangent spaces instead of the ordinary union (besides wanting to introduce the notion of sections)? But this are questions I already asked myself before having to give this talk.","['tangent-spaces', 'manifolds', 'tangent-bundle', 'differential-forms', 'differential-geometry']"
3926560,Local systems and Monodromy representation functor,"Let sheaf $\mathcal{F}$ on $X$ be a local system, ie. for every $x$ in $X$ , there is an an open set $U$ containing $x$ such that $\left.\mathcal{F}\right|_U$ is isomorphic to a constant sheaf $\underline{V}_X$ associated with some finite-dimensional vector space $V$ over field $\mathbb{K}$ . (We assume that $X$ is a nice topological space.) I know that the category of local systems - which I denote by $\text{Loc}(X,\text{Vect}_{\mathbb{K}}^{\text{fin}})$ - is a full subcategory of the category of all sheaves of vector spaces. I am trying to understand the monodromy representation functor. What is the stalk $\mathcal{F}_x$ of sheaf $\mathcal{F}$ at any point $x$ in $X$ ? I know that the stalk is defined to be the direct limit of sections $\mathcal{F}(U)$ over open sets $U$ containing $x$ , but I don’t really understand how to determine such an abstract object. How can I show that the rule $\mathcal{F}\mapsto\mathcal{F}_x$ for all $x$ in $X$ extends to the functor $$\text{mon}:\text{Loc}(X,\text{Vect}_{\mathbb{K}}^{\text{fin}})\longrightarrow\text{Fun}(\Pi_1(X), \text{Vect}_{\mathbb{K}}^{\text{fin}})$$ from the category of locally constant sheafs to the category of functors between the fundamental groupoid $\Pi_1(X)$ and finite-dimensional vector spaces $\text{Vect}_{\mathbb{K}}^{\text{fin}} $ ? I know that it will have to be defined on objects and on morphisms. I have read that the $\text{mon}$ -functor induces an equivalence of categories. I know that this is the case as soon as it is fully faithful and essentially surjective, but how can I prove this? Moreover, I was wondering which type of object on the left hand side corresponds to constant sheaves, but I don’t really get a feeling for this monodromy representation... I am happy for your help!","['functors', 'local-systems', 'category-theory', 'algebraic-geometry', 'sheaf-theory']"
3926580,find limit of $\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$ with squeeze theorem [duplicate],"This question already has answers here : Evaluating Limit Question $\lim\limits_{n\to \infty}\ \frac{1+\sqrt[2]{2}+\sqrt[3]{3}+\cdots+\sqrt[n]{n}}{n}=1$? (4 answers) Closed 3 years ago . I'm trying to prove with squeeze theorem that the limit of the following series equals 1: $$\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$$ For the left side of the inequality I did: $$\frac{1+\sqrt{1}+\sqrt[3]{1}+...+\sqrt[n]{1}}{n} <  \frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n}$$ For the right side, at first I did the following: $$\frac{1+\sqrt{2}+\sqrt[3]{3}+...+\sqrt[n]{n}}{n} <  \frac{n\sqrt[n]{n}}{n}$$ But then I realized it wasn't true and that the direction of this inequality is the opposite. Do you have any idea which series with limit 1 is bigger from the original series? Thanks!","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
3926587,Minimum value of $\left(\frac{a-b}{b-c}\right)^{4}+\left(\frac{b-c}{c-a}\right)^4+\left(\frac{c-a}{a-b}\right)^{4}$,"If $a,b,c \in R$ find the minimum value of $$E=\left(\frac{a-b}{b-c}\right)^{4}+\left(\frac{b-c}{c-a}\right)^4+\left(\frac{c-a}{a-b}\right)^{4}$$ My attempt:
Let $x=a-b,y=b-c,z=c-a$ So $x+y+z=0$ We need to minimize $$E=\left(\frac{x}{y}\right)^{4}+\left(\frac{y}{z}\right)^{4}+\left(\frac{z}{x}\right)^{4}$$ $\implies$ $$E=\left(\frac{x}{y}\right)^{4}+\left(\frac{y}{x+y}\right)^{4}+\left(\frac{x+y}{x}\right)^4$$ Letting $\frac{x}{y}=t$ we get: $$E=t^4+\frac{1}{(t+1)^4}+\frac{(t+1)^4}{t^4}$$ Now i tried using derivatives, but very tedious.","['algebra-precalculus', 'a.m.-g.m.-inequality', 'inequality']"
3926602,Probability density functions and cumulative distribution functions: open or closed intervals?,"In Statistics, the probability density function, $f_X(x)$ and the cumulative distribution function, $F_X(x)$ , of a real-valued random variable $X$ are said to have the following meanings: $$f_X(x)=\Pr[X=x] \space\space;\space\space F_X(x)=\Pr[X\leq x]$$ And they are related by $$f_{X}(x)=\frac{d}{d x} F_{X}(x)  \space\space;\space\space  F_X(x)=\int_{-\infty}^{x} f_{X}(u) du$$ However,  I have found in various sources different criteria for including or not including the values of the endpoints of the interval. When do we consider a closed interval and an open interval? Which of the following options would be the correct one? $F_X(x)$ equals $\Pr[X < x]$ or $\Pr[X\leq x]$ ? $F_X(b)-F_X(a)$ equals $\Pr[a<X< b]$ , $\Pr[a\leq X< b]$ , $\Pr[a< X \leq b]$ or $\Pr[a \leq X \leq b]$ ? $f(x)dx$ equals $\Pr\big[X\in(x,x+dx)\big]$ , $\Pr\big[X\in (x,x+dx]\big]$ , $\Pr\big[X\in[x,x+dx)\big]$ or $\Pr\big[X\in[x,x+dx]\big]$ ?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
3926610,$\mathbb{R}^{\mathbb{N}}$ Hilbert space with uncountable orthonormal basis?,"Let's say we have a Hilbert space of sequences $u_n\in\mathbb{R}^{\mathbb{N}}$ , equipped with some inner product $(\cdot,\cdot)$ . Let's say there is a Hermitian operator $M$ acting on this space, with a continuous spectrum of eigenvalues $\lambda\in\mathbb{R}$ . Because $M$ is Hermitian, its eigenvectors are automatically orthogonal to each other $(a^{\lambda},a^{\lambda'})=0$ for $\lambda\neq\lambda'$ . The eigenvectors are assumed to be in the Hilbert space (i.e. have finite norm). Owing to the existence of this Hermitian operator $M$ , this Hilbert space has an uncountable orthonormal basis. I don't see any obstacles for such a scenario to occur, but it seems counter-intuitive for a sequence $u_n$ to have finite norm while also being orthogonal to an uncountable number of sequences. Does this scenario make sense? Or is there something about $M$ and the Hilbert space which makes it impossible? Edit : The reason I ask this question is that I seem to have come across a scenario in which this does in fact occur. The Hilbert space in question has the unusual inner product $$(u,v)=\sum_{n=0}^{\infty}\frac{(2n+1)!!}{(2n)!!}u_nv_n.$$ And the Hermitian operator is $$M_{nm}=(4n+3)\delta_{nm}+2n\delta_{n-1m}+(2n+3)\delta_{n+1m}.$$ You can check that it is Hermitian with respect to this inner product by noting that $\frac{(2n+1)!!}{(2n)!!}M_{nm}$ is symmetric. This operator can be written in the form $M=A+A^{\dagger}+3$ , where $$A_{nm}=2n\delta_{nm}+(2n+3)\delta_{n+1m}.$$ These operators obey the commutator $$[A,A^{\dagger}]=2M.$$ This means in particular that $$e^{cA}Me^{-cA}=e^{2c}M$$ Which implies that if $v$ is an eigenvector of $M$ with eigenvalue $\lambda$ , then $e^{-cA}v$ is an eigenvector with eigenvalue $e^{2c}\lambda$ , for any constant $c$ . Thus providing a continuous spectrum of eigenvalues. To find an eigenvector $v$ , we may solve the recursive relation associated with the equation $Mv=\lambda v$ , and find that there are normalizeable solutions for real $\lambda<0$ , with asymptotic behavior $$v_n\sim(-1)^n\frac{1}{\sqrt{n}}e^{-\sqrt{2n|\lambda|}},\quad n\to\infty$$ Hermiticity of $M$ Since this has been questioned a couple of times I will show that $M$ is Hermitian with respect to this inner product. As I mentioned it is equivalent to showing that $\frac{(2n+1)!!}{(2n)!!}M_{nm}$ is symmetric. $$\frac{(2n+1)!!}{(2n)!!}M_{nm}=\frac{(2n+1)!!}{(2n)!!}(4n+3)\delta_{nm}+\frac{(2n+1)!!}{(2n-2)!!}\delta_{n-1m}+\frac{(2n+3)!!}{(2n)!!}\delta_{n+1m}$$ This is symmetric since $$\frac{(2n+1)!!}{(2n-2)!!}\delta_{n-1m}=\frac{(2m+3)!!}{(2m)!!}\delta_{m+1n}.$$ Reviewing the definition of a Hermitian operator, it presumes it is a bounded operator, which we have found that this operator is not (assuming the rest of my calculations are correct). Perhaps the statement about orthogonality of eigenvectors is not true when the ""Hermitian"" operator is unbounded?","['hilbert-spaces', 'sequences-and-series']"
3926651,"Show that for $a \ne 0, b \in \Bbb{R}$ that $\int_{\Bbb{R}}f(ax+b)d\lambda(x) = \frac{1}{|a|}\int_{\Bbb{R}}f(x)d\lambda(x)$","Show that for $a \ne 0, b \in \Bbb R$ that $\int_{\Bbb R}f(ax+b)d\lambda(x) = \frac1{|a|}\int_{\Bbb R}f(x)d\lambda(x)$ where $\lambda$ is the Lebesgue measure. (Note that both sides are well defined and equal or neither are well-defined). I have this question for an assignment, but I don't really know where to start. Do I approximate $f$ by simple functions and try to show the assertion for simple functions? I don't need a full answer, but if someone could point me in the right direction or help me get started, that would be great :)","['invariance', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
3926658,$L^p$ integrability condition,"I am trying to prove an equivalent condition for functions belonging in $L^p$ . It's convergence of $\sum_{n\in\mathbb{Z}}2^{np}m(|f|>2^n)$ . the one direction is an easy application of Chebyshev's inequality. However I am having trouble with the reverse direction. that is whenever the series converges, then $\|f\|_p<+\infty$ .","['measure-theory', 'functional-analysis']"
3926886,Conditional expectation stable by passing into the limit in $L^{1}$?,"Does the conditional expectation converges when the sigma-algebre converges ?
e.g: $$Y \in L^{1}, (X_{n})_{n \ge 1} \xrightarrow[n \to \infty]{L^{1}}  X_{\infty}
 \Rightarrow \mathbb{E}[Y | X_{n}] \xrightarrow[n \to \infty]{L^{1}} E[Y | X_{\infty}]$$ ? My intuition is to say Yes: but I don't know how to properly prove it.","['conditional-expectation', 'convergence-divergence', 'probability-theory']"
3926901,PNT can't be applied Legendre's Conjecture as it can be to Bertrand's Postulate?,"1. Background Bertrand's Postulate says that there is at least one prime between an integer $n$ and its double $2n$ . $$n < p < 2n$$ Although not a proof, the Prime Number Theorem (PNT), that the number of primes up to $x$ denoted $\pi(x)$ is asymptotically equivalent to $x/\ln(x)$ , gives us a clue that this is true. \begin{align}
\frac{\pi(2n)}{\pi(n)} &\sim \frac{2n}{\ln(2n)} \cdot \frac{\ln(n)}{n} \\
& = \frac{2 \ln(n)}{\ln(2)+\ln(n)} \\
& \to 2 \text{ as } n \to \infty
\end{align} Which says that for large enough $n$ , on average we expect there to be $n/\ln(n)$ primes between $n$ and $2n$ . This logic is also presented by the Wikipedia article on Bertrand's postulate , read on 28th Nov 2020. 2. Question That same wikipedia article also suggests we can't apply similar logic to Legendre's Conjecture, which states there is at least one prime between two consecutive squares: $$n^2 < p < (n+1)^2$$ Here is the analogous argument, an elaboration of the more concise description in that wikipedia article. \begin{align}
\frac{\pi((n+1)^2)}{\pi(n^2)} &\sim \frac{(n+1)^2}{\ln(n+1)^2} \cdot \frac{\ln(n^2)}{n^2} \\
& = \frac{(n^2 + 2n + 1) \cdot 2\ln(n)}{2\ln(n+1) \cdot n^2} \\
& = \frac{(1+\frac{2}{n} + \frac{1}{n^2}) \cdot \ln(n)}{\ln(n+1)} \\
& \to 1 \text{ as } n \to \infty
\end{align} How should I interpret this result? Is the correct interpretation that this application of the PNT doesn't reveal any information about the number of primes between $n^2$ and $(n+1)^2$ because $\pi(n^2)$ grows too similarly to $\pi((n+1)^2)$ , where ""grows too similarly"" means asymptotic equivalence? 3. Alternative Conjecture? We can state a related conjecture: $$n < p < n^2$$ A similar application of the PNT: \begin{align}
\frac{\pi(n^2)}{\pi(n)} &\sim \frac{n^2}{\ln(n^2)} \cdot \frac{\ln(n)}{n} \\
& = \frac{n \cdot ln(n)}{2\ln(n)} \\
& = \frac{n}{2} \\
& \to \infty \text{ as } n \to \infty
\end{align} I'm not an expert, but this suggests that for large $n$ , we expect on average more than one prime between $n$ and $n^2$ . What makes the $n^2$ and $(n+1)^2$ case less amenable to this kind of analysis?","['analytic-number-theory', 'number-theory', 'prime-numbers']"
3926973,Logic and implication negation,"I am new to predicate logic and there is something I can't seem to understand. Let's have an implication:
If I have a sister, I have a sibling. Now, it's negation should be:
I have a sister and I do not have a sibling. But I can't understand why the following is not a valid negation:
If I have a sister, I do not have a sibling. It should be a negation because the first part of the implication happened and the second did not.","['predicate-logic', 'logic', 'discrete-mathematics']"
3927015,$a_n = u^n+v^n+(-1)^n$,"Anyone has solutions for this problem? Let $u, v$ are distinct roots of equation: $$x^2-tx+1=0 (t \in \mathbb{N}, t>2)$$ and sequence $$(a_n): a_n = u^n+v^n+(-1)^n$$ Prove that: $$a_{3^m}\mid a_{3^mn} \forall m,n \in \mathbb{N} ,  gcd(n,3)=1$$ Notes $S_n=\frac{a_{3^mn}}{a_{3^m}}\\$ $a=u^{3^m}$ then $\frac{1}{a}=v^{3^m}$ +If $n$ is odd $$S_n = \frac{a^{2n}-a^n+1}{a^{n-1}(a^2-a+1)}$$ Because $gcd(n,6)=1$ , polynomial $a^{2n}-a^n+1$ is divided by $a^2-a+1$ , then $\frac{a^{2n}-a^n+1}{a^2-a+1}$ is a symmetric polynomial with degree $2(n-1)$ So $S_n=\sum\limits_{i=0}^{n-1}s_i(a^i+\frac{1}{a^i})$ +If $n$ is even, then $n=2^tn'$ $S_n = S_{n'}.a_{\frac{n}{2}}a_{\frac{n}{4}}...a_{\frac{n}{2^t}}$","['divisibility', 'number-theory', 'gcd-and-lcm', 'polynomials', 'sequences-and-series']"
3927029,"Show that $\lim_{n\to\infty}\int_0^1|f_n(x)-1|\,dx = 1$ if $\lVert f_n\rVert_{1} = 2$ and $\lim_{n\to\infty}f_n(x) = 1$","Suppose that $\{f_n\}_{n=1}^\infty$ , $f_{n}:[0,1]\to\mathbb{R}$ , is a sequence of measurable functions such that for every $x\in [0,1]$ $$\lim_{n\to\infty}f_n(x)=1$$ and for every $n$ : $$\int_{0}^{1}|f_{n}(x)|\,dx = 2.$$ Prove that $$\lim_{n\to\infty}\int_{0}^{1}|f_{n}(x) - 1|\,dx = 1$$ What I tried/have: Such a sequence is clearly possible since we can take $f_n = 1+n\chi_{(0,\frac{1} {n})}$ . The proposition is also trivial if $f_n\geq 1$ . I also managed to get, using the reverse triangle inequality that $$\liminf_n \int_0^1|f_n(x) - 1|\,dx\geq \liminf_n\int_0^1|f_n(x)|\,dx - 1 = 1$$ but I'm quite stuck on proving that the $\limsup$ is less than or equal to 1. Since it is not possible to find a dominating function (eg. consider the sequence I stated before) and there is no clear way to construct a monotone sequence, I feel like I need to use Fatou's Lemma somehow. Could someone please give me some tips on how I might show that the limit superior is at most $1$ ? Or maybe, if there is an easier approach, how I would directly compute the limit?","['lebesgue-integral', 'measure-theory', 'convergence-divergence']"
3927053,"Retract of $F_2$ onto $[F_2,F_2]$","I'm trying to show that the commutator subgroup $[F_2,F_2]$ is not a retract of $F_2$ . I was trying to do the proof by contradiction so I assume there is a retract $f: F_2 \to [F_2,F_2]$ , then I know this is surjective and then I tried to apply abelianization to get the map $f^{ab}: \mathbb{Z}^2 \to {1}$ but this doesn't give me any contradictions since this map is surjective. Then I was thinking to apply the universal property of quotient groups somehow because I know $F_2/ [F_2,F_2] = \mathbb{Z}^2$ and that is a surjective map but I can't find anything that gives me a contradiction. I'm not sure where to go from here.","['group-theory', 'free-groups']"
3927072,Proof verification and understanding needed,"Prove :if A is infinite and B finite and B is a finite subset of A then A\B is infinite by using Exercise 1. Exercise 1
Let A,B be disjoint finite sets. and A≈m. and B≈n,then. A ∪ B ≈ m + n. Conclude that the union of two finite sets is finite. Note : problem comes from A book of set
Theory by Pinter Attempted proof
(Caveat Lector: let the reader beware...
My knowledge of infinite set is shaky
I can use induction and mapping) I proved exercise 1. (Complete rewrite) Write A=(A\B) $\cup$ B (1) Using $A \cup B $ from exercise 1 we get
A\B=( $A\cup B)\cap B^{c}$ (2) Now suppose that A has a denumerable subset B and A is finite; that is, A ≈ n, B ⊆ A, and B ≈ ω. So B $\subset$ (A\B) $\cup$ B. A\B can’t be finite since A is infinite
If a $\in$ A\B then a $\in B^{c}$ then $B^c$ is infinite which is contradiction since
B is finite Hence A/B is infinite Help","['elementary-set-theory', 'solution-verification']"
3927079,Sum of Indicators: $\sum_{n=1}^{\infty} \mathbb{1}\{X_1^2/\alpha^2 \geq n\} = \left(1 + \left\lfloor\frac{X_1^2}{\alpha^2}\right\rfloor\right)$,"I was working on proving the following proposition: Suppose $(X_n)$ is some iid sequence of random variables such that $\mathbb{E}(X_1)^2 < \infty$ . Then for every $\alpha > 0$ we have $$\mathbb{P}\left(\limsup_{n \to \infty}|X_n| \geq \alpha\sqrt{n}\right) = 0.$$ Proof. We wan't to invoke the Borel-Cantelli Lemma (B.C.), so since $X_n$ are iid we have $$\sum_{n=1}^{\infty}\mathbb{P}(|X_n| \geq \alpha\sqrt{n}) = \sum_{n=1}^{\infty}\mathbb{P}(X_n^2 \geq \alpha^2 n) = \sum_{n=1}^{\infty}\mathbb{P}\left(\frac{X_1^2}{\alpha^2} \geq n\right).$$ Using the Monotone Convergence Theorem, we can represent the right-most term as $$ \sum_{n=1}^{\infty}\mathbb{P}\left(\frac{X_1^2}{\alpha^2} \geq n\right) =  \sum_{n=1}^{\infty}\mathbb{E}\left[\mathbb{1}\left\{\frac{X_1^2}{\alpha^2} \geq n\right\}\right] = \mathbb{E}\sum_{n=1}^{\infty} \mathbb{1}\left\{\frac{X_1^2}{\alpha^2} \geq n\right\}.$$ Therefore, $$\mathbb{E}\sum_{n=1}^{\infty} \mathbb{1}\left\{\frac{X_1^2}{\alpha^2} \geq n\right\} = \mathbb{E}\left(1 + \left\lfloor\frac{X_1^2}{\alpha^2}\right\rfloor\right) \leq 1 + \frac{\mathbb{E}X_1^2}{\alpha^2} < \infty.  \tag{1}$$ So, we can invoke B.C. and are done. The bit about the indicator equality involving the floor function is given as a hint without going into detail. Question. How do we get the equality involving the floor in (1)? Specifically, I don't see how $$\sum_{n=1}^{\infty} \mathbb{1}\left\{\frac{X_1^2}{\alpha^2} \geq n\right\} = 1 + \left\lfloor\frac{X_1^2}{\alpha^2}\right\rfloor.$$ Could someone expand on this derivation please?","['borel-cantelli-lemmas', 'probability-theory', 'random-variables']"
3927094,"$\{f_n\}$ be a sequence of continuous functions converging pointwise to $0$, show $\lim \int_0^1 f_n dx = 0$","This problem comes from Rudin's Real and Complex Analysis and is Exercise 2.10. Let $\{f_n\}$ be a sequence of continuous functions such that $0\leq f_n\leq 1$ and $f_n(x)\to0$ for all $x\in [0,1]$ . We need to show that $$\lim_{n\to\infty}\int_0^1 f_n(x)dx = 0$$ In particular I am trying to show this without using Measure Theory or Lebesgue Integration. What I have tried so far is to consider the sequence of functions $\{g_n\}$ defined by $$g_n(x)= \min\{f_1(x),\ldots f_n(x)\}$$ Then $\{g_n\}$ is a continuous, decreasing sequence of functions which converge pointwise to $0$ . Since the sequence of continuous functions decreases to a continuous function, then we may apply Dini's Theorem to know that $g_n \to 0$ uniformly. Hence $$ \lim_{n\to 0}\int_0^1 g_n(x)dx = \int_0^1\lim_{n\to0}g_n(x)dx = 0$$ The only thing left to show is that $$ \lim_{n\to 0}\int_0^1 f_n(x)dx= \lim_{n\to 0}\int_0^1 g_n(x)dx $$ But this is where I am stuck. Again I am avoiding measure theory proofs.","['uniform-convergence', 'real-analysis']"
3927098,How to find if particle is moving in clockwise direction or not?,"I've been given a vector equation in the form of r(t), how do i find if the particle is going in clockwise direction or in anticlockwise direction?","['vectors', 'ordinary-differential-equations']"
3927103,Does anyone know of a much faster way to solve this polynomial question?,"A monic cubic $p(x) $ is divided by $(x^2 +x +1)$ and the remainder is $(2x+3)$ . When $p(x)$ is divided by $x(x+3)$ , the remainder is $5(x+1)$ . Find $p(x)$ So the way I solved it was through using long division - as you can imagine very lengthy, and not very efficient in exam situations. I then equated the remainders with the given remainders. However, in an exam situation, would there be an even faster way? I am open to suggestions.","['algebra-precalculus', 'polynomials']"
3927209,Law of Large Numbers - Elementary proof,"I am self-learning probability theory from William Feller's Introduction to probability theory and it's applications . I am having some difficulty in understanding an elementary proof regarding the the law of large numbers. I reproduce the proof in the book interspersed with questions. Any help or inputs would be extremely insightful. On page 141, the author proves, that there is a nice upper bound on the probabilities of the right and left tails of a binomial distribution: If $r \ge np$ , the probability of atleast $r$ successes in $n$ Bernoulli trials has an upper bound given by the inequality, $$P(S_n \ge r) \le b(r;n,p) \cdot \frac{1}{1-\frac{(n-r)p}{(r+1)q}} \tag{1}$$ If $s \le np$ , the probability of atmost $s$ successes in $n$ Bernoulli trials has an upper bound given by the inequality, $$P(S_n \le s) \le b(s;n,p) \cdot \frac{1}{1-\frac{sq}{(n+1-s)p}} \tag{2}$$ I am able to derive the above inequalities and the results are clear to me. I also know that $b(k;n,p)$ is monotonically increasing for $k < (n+1)p$ , then monotonically decreases for $k > (n+1)p$ . The Law of Large Numbers. Our intuitive notion of probability is based on the following assumption. If $S_n$ is the number of successes in $n$ Bernoulli trials, then $S_n/n$ is the average number of successes and should be near $p$ . We can now prove this mathematically. Consider, for example, the probability that $S_n/n$ exceeds $p + \epsilon$ . This probability is the same as $P\{S_n > n(p + \epsilon)\}$ and equals the left side of $(1)$ , when $r$ is the smallest integer exceeding $n(p+\epsilon)$ . Then, inequality $(1)$ implies: \begin{align*}
P(S_n > n(p+\epsilon)) < b(r;n,p) \cdot \frac{n(p+\epsilon) + q}{n\epsilon + q} \tag{3}
\end{align*} How to derive the expression on the right? For example, I tried using the fact that $r > n(p + \epsilon)$ as follows: \begin{align*}
\frac{(n-r)p}{(r+1)q} < \frac{(n - n(p + \epsilon))p}{(n(p+\epsilon)+1)q}
\end{align*} But, it seems to get me nowhere! With increasing $n$ , the fraction on the right remains bounded, whereas $b(r;n,p) \to 0$ since $b(r;n,p) < b(k;n,p)$ for each $k$ such that $(n+1)p \le k < r$ , and there are about $n\epsilon$ such terms $b(k;n,p)$ . Okay, I lost him, when he mentions there are $n\epsilon$ such terms $b(k;n,p)$ . Why should there be $n\epsilon$ such terms? It follows that as $n$ increases, $P\{S_n > n(p + \epsilon)\} \to 0$ . Using inequality $(2)$ , we see in the same way that $P\{S_n < n(p-\epsilon)\} \to 0$ and we have thus \begin{align*}
P\left\{ \left\lvert\frac{S_n}{n} -p \right\rvert < \epsilon \right\} \to 1 \tag{4}
\end{align*} In words: As $n$ increases, the probability that the average number of successes deviates from $p$ by more than any pre-assigned $\epsilon$ tends to zero.","['proof-explanation', 'probability-limit-theorems', 'probability-theory']"
3927227,"Help in understanding this, convergence in measure.","Let $(X,A,\mu)$ be a measurable space and let $ g,f_1,f_2,\ldots \in L^1(\mu)$ , $g\geq 0$ and $f:X\to \mathbb C$ be measurable.
Suppose that $f_n$ converges to $f$ in measure ( $\mu$ ) and that $|f_n|\leq g$ for all $n \in \mathbb N$ . Prove that : $$\lim_{n\to \infty} \int_{X} f_n \,d\mu = \int_X f \, d\mu.$$ I'm trying to show this with using the dominated convergence theorem of
Lebesgue since we have almost all the conditions.
But we have to show that $f_n$ converges to $f$ almost everywhere.
From the given information we can conclude that $f_n$ is a Cauchy sequence, so by another theorem there is a sub sequence $f_{n_j}$ that converges to $f$ almost everywhere.
I don't know how to proceed. Though I know that there is a theory in calculus which states: $a_n$ converges to $a$ iff  every subsequent of $a_n$ has a subsequent that converges to $a$ .
But I don't know how to use it. Can you help in this, and explain how it can be showed.","['measure-theory', 'lp-spaces', 'convergence-divergence', 'functional-analysis']"
3927234,Genus of $y^3=(x^2+1)^2(x^3-1)$,"I'm trying to proving that the genus of the normalization of $C:y^3=(x^2+1)^2(x^3-1)$ is 4 by using the Riemann-Hurwitz formula on the projection map $\pi:C\rightarrow\mathbb{P}^1$ which sends $[x:y:z]\mapsto[x:y]$ . The map has degree 3 so we have $g(C)-1=\deg\pi(g(\mathbb{P}^1)-1)+\frac{b}{2}$ .
Now, $\pi$ ramifies on $[1:\pm i:0],[1:-1:0],[1:\omega:0],[1:\bar{\omega}:0]$ ( $\omega^3=-1$ ) with ramification index 2. Something must be wrong: the formula gives $g(C)=2+3(-1)+\frac{b}{2}=\frac{b}{2}-1$ but $[0:1]$ ramifies. You can see this considering the homogeneous polynomial $Z^4Y^3=(X^2+Z^2)^2(X^3-Z^3)$ and the preimage of $[0:1]$ given by $Z^4Y^3=(Z^2)^2(Z^3)$ which gives $Z^4(Y^3+Z^3)=0$ from which I get $[0:1:0],[0:{\omega}:1],[0:\bar{\omega}:1]$ . They have positive ramification index so something must be wrong. Please help!","['algebraic-curves', 'algebraic-geometry']"
3927253,Convergence in probability implies convergence in $L^2 $,"Let $(X_n)_n$ a sequence of gaussian r.v. such that for any $m$ and $ n$ $(X_n − X_m)$ is gaussian.
If $X_n \overset{P}{\longrightarrow X}$ then $X_n \overset{L^2}{\longrightarrow}X$ (more precisely in $ L^p ,\ p\in [1, +∞)$ ). I tried to do this: convergence in probability implies convergence in law , and convergence in law of gaussians implies (Paul-Levy) that $ X \sim N(m,\sigma^2) $ with $m=\lim\limits_{n \to \infty} m_n $ and $\sigma^2=\lim\limits_{n \to \infty} \sigma^2_n$ . Convergence in probability also implies that the limit is a.s. unique. Because $L^2$ is complete it is enough to prove that the sequence is Cauchy. Using the hypothesis : $\int \lvert X_n-X_m \rvert^2 dP= \mathbb{E}[(X_n-X_m)^2]= (m_n-m_m)^2+(\sigma_n^2+\sigma_m^2)$ And i can find, fixed $\epsilon>0 , \hspace{0,3cm} n^* \in \mathbb{N} \hspace{0,3cm} $ such that $\hspace{0,1cm} \forall m,n \geq n^*\hspace{0,4cm}  (m_n-m_m)^2+(\sigma_n^2+\sigma_m^2)<\epsilon \hspace{0,2cm}$ because of the limits above. I would like to know where is the mistake and how i could fix it. I think i'm mistaken because the absolute moment equals the moment only if p is even, moreover it seems like i'm using convergence in probability only for the a.s. uniqueness of the limit. Any help would be appreciated.","['convergence-divergence', 'probability-theory', 'probability', 'normal-distribution']"
3927255,coincidence of $L^2$ and probability-closure of a Gaussian space,"A vectorial subspace $\mathcal{H} \subset L^2(\Omega,\mathcal{F},\mathcal{P})$ is called Gaussian space if every element is a gaussian distributed random variable. Already proven : Taken $X_1,\ldots,X_n \in \mathcal H \Rightarrow (X_1,\cdots,X_n)$ is gaussian. Prove that the closure of a Gaussian space is a Gaussian space. I'd like to proof the following : If $\mathcal{H}$ is a Gaussian space the closure of $\mathcal{H}$ with respect to the convergence in probability and $L^2$ are the same. My proof goes and interrupts as follows : We already know that if $X_n \overset{L^2}{\longmapsto} X$ then $X_n \overset{P}{\longmapsto} X$ which implies thanks to the previous points that $\overline{\mathcal{H}}^{L^2} \subset \overline{\mathcal{H}}^{P}$ . Conversely suppose that $X_n \overset{P}{\longmapsto} X$ , using that $E[\lvert X \rvert ^p] = \int_0^{+\infty} P(X > t)pt^{p-1} dt$ We have that $$\lvert \lvert {X_n - X}\rvert \rvert_{2}^{2} = E[\lvert {X_n - X}\rvert^2] = 2 \int_0^{+\infty} t P(\lvert X_n - X \rvert > t)dt$$ Noting that the integrand goes pointly to $0$ I'd like to pass the limit under the integral sign. From here I don't how to find inequalities to bound $t P(\lvert X_n - X \rvert > t)$ and applying dominated convergence theorem (or other limit under integral sign-theorems), to use the fact that $X_n \overset{P}{\longmapsto} X$ . I could also use a strengthening of dominated convergence theorem which says : $X_n \overset{P}{\longmapsto} X$ and $Y \in L^p (p \in [1,+\infty)) : \lvert X_n \rvert \leq Y a.e \Rightarrow X_n \overset{L^p}{\longmapsto} X$ and $X \in L^P$ but the problem remains how to bound $X_n$ for me. I'm noting that the hypothesis of $X_n,X$ being gaussian, i.e. $X_n, X \in \mathcal{H}$ hasn't been used yet. Any help or hint would be appreciated, thanks.","['measure-theory', 'probability-distributions', 'expected-value', 'lp-spaces', 'probability-theory']"
3927273,Asking for reference for challenging problems for Functional Analysis 1st course,"I am a masters 1st year student and my functional analysis course ended 1 week ago. The instructor followed  Irwin Krieszig's Book "" Introductory Functional Analysis with Applications"" for teaching and although he didn't gave any assignments or exercise problems, I solved problems from exercises given in Krieszig's Book. I was able to understand the concepts given in the book clearly and then do exercises of Krieszig without much difficulty. But I noticed that the problems of the book are  much easier for a masters student but as I was busy in some extra - curricular activities also , so I didn't tried other problems or asked for reference books or materials here. But Now I think that I must solve good number of Challenging problems in Functional Analysis as I will apply for phd in pure mathematics soon and I got my December break time to it. So, Can you please tell any book in functional analysis course whose exercises are challenging as compared to Krieszig's  / any assignments which any professor of a university gave which are available on web? Also, I am adding my course syllabus in the following images so that you come to know exactly which topics should be  available( I absolutely no problem If more topics are covered). All these topics were covered which are given in the images and none except them were covered. Edit: I am good in measure theory , so if any functional analysis book you are recommending assumes knowledge of measure theory  then I am fine with it. Looking for your help.","['book-recommendation', 'functional-analysis', 'analysis', 'reference-request']"
