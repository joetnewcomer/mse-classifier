question_id,title,body,tags
3049601,Closed form expression or asymptotic expansion for (periodic) generalized harmonic numbers?,"In contrast with the series $\sum_{k=1}^n k$ and $\sum_{k=1}^n1$ , there does not (as far as I know) exist a pure closed form expression (or a nice asymptotic expansion other than the Euler-Maclaurin expansion?) for the generalized harmonic numbers (and in fact the defining series for the Riemann zeta function) $$H_{s}(n)=\sum_{k=1}^n \frac{1}{k^s}\tag{1}$$ with $s\in\mathbb C$ (in particular $\Re(s)=\sigma\in(0,1)$ ). My question is however whether there might exist a closed form expression (or maybe an asymptotic expansion other than the Euler-Maclaurin expansion?) of which I call 'periodic generalized harmonic numbers' $P_{\sigma,t}(n)$ and $Q_{\sigma,t}(n)$ (however an expression for one of them would suffice) in which $$P_{\sigma,t}(n)=\sum_{k=1}^n \frac{\cos(t\ln(k))}{k^\sigma}\tag{2a}$$ and $$Q_{\sigma,t}(n)=\sum_{k=1}^n \frac{\sin(t\ln(k))}{k^\sigma}\tag{2b}$$ with $t\in\mathbb R$ and (in particular) $\sigma\in(0,1)$ . The reason why I'm not immediately interested in the Euler-Maclaurin expansion of the three concerning series is that all these expansions involve a complicated expression (involving Bernoulli numbers and derivatives of the concerning terms) which is in fact used to define the Riemann zeta function in some way. 
I'm in particular looking for a method to derive information from the Riemann zeta function by using another type of asymptotic expansion or even the closed form of the defining series (1) or preferably the (maybe more nicely behaving) 'splitted versions' (2a) and (2b). Is e.g. Fourier analysis an obvious direction to think of? See for my motivation for this matter also in 4.1.2 of http://fse.studenttheses.ub.rug.nl/19062/1/bMATH_2019_vanderReijdenIS.pdf .","['asymptotics', 'riemann-hypothesis', 'closed-form', 'sequences-and-series', 'trigonometry']"
3049668,Deforming antiholomorphic involutions,"Let $(M,J)$ be a compact smooth almost complex manifold. We can ""deform"" $J$ as follows: if $A$ is a smooth section of the endomorphism bundle $\mathrm{End}(TM)\to M$ satisfying $
AJ=-JA,
$ it follows that $
Je^A=e^{-A}J,
$ where $e^A$ is the matrix exponential of $A$ . It follows immediately that $$
J':=Je^A
$$ is another almost complex structure on $M$ . Suppose $\phi:M\to M$ is a diffeomorphism which is an anti- $J$ -holomorphic involution, i.e. $$
J\circ d\phi=-d\phi \circ J \qquad \text{and} \qquad \phi\circ \phi =\mathrm{id}.
$$ Question. Can we deform $\phi$ into an anti- $J'$ -holomorphic involution? I think this will be true if there exist a vector field $\eta\in \Gamma(TM)$ such that the diffeomorphism $\phi':M\to M$ defined by $$
\phi'(x)=(\phi\circ \mathrm{exp}\circ \eta)(x)
$$ is an anti- $J'$ -holomorphic involution (here $\exp$ is defined with respect to some Riemannian metric on $M$ ). How can I show the existence/nonexistence of $\eta$ ?","['complex-geometry', 'symplectic-geometry', 'riemannian-geometry', 'differential-geometry']"
3049669,"The Metric Tensor, A Body of Mass m and Minkowski Space","By saying the body has mass m, we mean that the metric approaches that of Minkowski space for large r and that $$g_{00} \sim 1-2m/r$$ This was under a section in General Relativity by Woodhouse titled The Fiedl of a Static Spherical Body I have no idea how the sentence implies the equation (so obviously)? I cannot see why we are only considering the 00 component and the right hand side I have no idea.","['tensors', 'general-relativity', 'mathematical-physics', 'differential-geometry']"
3049691,Is the sum of such two banach spaces also a banach space?,"Let $L^2(\mu)$ and $L^2(\nu)$ with respect to two different positive measures, then they are two Banach spaces. I'm considering whether the space $$L^2(\mu)+L^2(\nu)$$ is still a Banach space? e.g. $\mu$ be Lebesgue measure, $d\nu=ln(1+|x|)d\mu$ , my idea is that since both $L^2(\mu)$ and $L^2(\nu)$ are continuous embedded to the measurable functions space $\mathcal M$ , it's done.","['measure-theory', 'functional-analysis', 'real-analysis']"
3049718,Proving set identities: empty set case.,"I am currently refreshing my knowledge in naive set theory, and would like to prove that for all sets $A,B,C$ we have $$A\cap(B \cup C) = (A \cap B) \cup (A \cap C).$$ I understand that this can be done by proving both $$A\cap(B \cup C) \subset (A \cap B) \cup (A \cap C) \ \text{and} \ (A \cap B) \cup (A \cap C) \subset A\cap(B \cup C)  $$ hold true. We can do this by letting $x$ be an arbitrary element of $A\cap(B \cup C)$ and showing that it is an element of $(A \cap B) \cup (A \cap C)$ , and vice versa. But what about when $A \cap (B \cup C)$ is the empty set? Then I would think we can't let $x$ be an arbitrary element of $A \cap (B \cup C)$ since there are none. However, I am aware that $A \cap (B \cup C) \subset (A \cap B) \cup (A \cap C)$ is trivially true in this case. In the discrete mathematics course I took at my university, I did not see such cases be brought to attention. Should they be mentioned in proofs of such identities? Why / why not? If so, some suggestions as to how to incorporate them into proofs would be helpful :-).","['elementary-set-theory', 'proof-writing']"
3049769,"For which values of $\ a, b $ the matrix is diagonalizable","$$\ A = \begin{bmatrix} a & 0 & 0 \\ b & 0 & 0 \\ 1 & 2 & 1 \end{bmatrix} $$ for which values of a , b the matrix is diagonizable? in each case that $\ A $ is diagonizable, write similar diagonal matrix. My attempt: I split the solution into six cases. The characteristic polynomial of such matrix will be $\ p(x) = (a - \lambda)(1- \lambda) \lambda $ and therefore the eigen values should be $\ a , 1, 0 $ . 1) If $\ a = b $ and $\ a \not = 0,1  \Rightarrow $ 3 eigen values: $\lambda = 0,1,a $ $\ \lambda =  a, \ \ \ |I_a - A| = \begin{vmatrix} 0 & 0 & 0 \\ -a & a & 0 \\ -1 & -2 & a-1 \end{vmatrix}  \Rightarrow$ matrix rank is $\ 2 $ and therefore one eigen vector. $\ \lambda = 1 , \ \ \ |I - A | = \begin{vmatrix} -a & 0 & 0 \\ -a & 1 & 0 \\ -1 & - 2 & 0\end{vmatrix} \Rightarrow $ matrix rank is 2 again and therefore only one eigen vector $\ \lambda = 0, |0-A| = \begin{vmatrix} -a & 0 & 0 \\ -a & 0  & 0 \\ -1 & -2 & - 1  \end{vmatrix} \Rightarrow  $ matrix rank is 2 again and therefore only one eigen vector Therefore if $\ a = b $ and $\ a \not = 0 ,1 $ then the matrix is diagonizable and similar to the matrix $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & a \end{bmatrix} $ 2) $\ a = b $ and $\ a = 0 $ then $\ |0-A| = \begin{vmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow $ Matrix rank is one and therefore eigen value $\ 0$ has two eigen vectors and the eigen value $\ 1 $ must have at least one eigen vector because geometric multiplicity is always equal or greater then algebraic multiplicity. Therefore the matrix is diagonizable and similar to $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} $ 3) If $\ a= b $ and $ a= 1 $ then $\ |I - A | = \begin{vmatrix} 0 & 0 & 0 \\ -1 & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow $ algebraic multiplicity is $\ 2 $ while geometric is only one and therefore the matrix is not diagnosable. 4) $\ a \not = b $ and $\ a \not = 0,1 $ then $\ A  $ has three different eigenvalues and each one must have one eigenvector and because geometric multiplicity must be at least one and cannot exceed algebraic multiplicity and there the matrix is diagonizable and similar to $\ \begin{bmatrix} a & 0 & 0 \\ 0 & 0  & 0 \\ 0 & 0 & 1 \end{bmatrix} $ 5) $ a \not = b $ and $\ a = 0 $ then $\ 0 $ is eigen value with algebraic multiplicity of $\ 2 $ and so $\ |0 - A| = \begin{vmatrix} 0 &0 & 0 \\ b & 0 & 0 \\ -1 & -2 & -1 \end{vmatrix} \Rightarrow$ matrix rank is two and therefore algebraic multiplicity > geometric multiplicity and the matrix cannot be diagonalised. 6) $\ a \not = b $ and $\ a = 1 $ then $\ |I-A| = \begin{vmatrix} 0 & 0 & 0 \\ b & 1 & 0 \\ -1 & -2 & 0 \end{vmatrix} \Rightarrow $ matrix will be diagonizable only if $\ b = 1/2 $ and then geometric multiplicity of eigen value $\ 1 $ will be $\ 2 $ and then it will be similar to $\ \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix} $ Thank you, for those of you who made it so far. It is a question I had on a test and I got for this answer 2/14 points. After talking to friends who also studied the subject they told me that even though the answer may be unnecessarily long, they couldn't find any flaws.","['proof-verification', 'linear-algebra']"
3049781,What is the inverse of simply composited elementary functions?,"$A$ be an elementary function, algebraic over $\mathbb{C}$ , $f_1$ and $f_2$ are bijective elementary functions with elementary inverses, $F\colon z\mapsto A(f_1(f_2(z)))$ be a bijective elementary function. What is the inverse $F^{-1}$ of $F$ ? See Wikipedia: Elementary function , consider the definition of the elementary functions of Liouville and Ritt. Ritt wrote in Ritt, J. F.: Elementary functions and their inverses. Trans. Amer. Math. Soc. 27 (1925) (1) 68-90 : ""That every $F(z)$ of this type has an elementary inverse is obvious."" Is this really obvious? If all mentioned functions are bijective, $F^{-1}=f_2^{-1}\circ f_1^{-1}\circ A^{-1}$ . But only injectivity of $f_2$ and surjectivity of $A$ follow from the bijectivity of $F$ . Therefore $A$ may be non-injective. Bijectivity of $A$ can be defined by restriction and corestriction. But are this new function and its inverse (a partial inverse of the original $A$ ) still elementary functions? They may have different domain and/or codomain in comparison to the original elementary functions. It depends on the answer to the question "" Are all restrictions of an elementary function also elementary functions? "". And what if $A$ is not bijective: can its bijective restriction to the codomain of $f_1\circ f_2$ represented by a $single$ bijective elementary function?","['algebra-precalculus', 'closed-form', 'analysis']"
3049804,Cauchy sequences and convergent sequences [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am very confused with a little problem. What is the difference between a Cauchy sequence and a convergent sequence?",['functional-analysis']
3049825,Find $\det A$ and $\operatorname{Tr} A$ if $\det(A-\sqrt[n]{3}I_n)=0$,"$A \in M_{n}(\mathbb{Q})$ and I have to find $\det A$ and $\operatorname{Tr} A$ if $\det(A+\sqrt[n]{3}I_n)=0$ . I observed that $\sqrt[n]{3}$ is an eigenvalue of $A$ ,but I don't know how to continue. EDIT : My bad,the matrix has rational entries.","['matrices', 'linear-algebra']"
3049843,Radius of convergence of $\sum\limits_{n=1}^{\infty} \frac{n+2}{2n^2+2} x^n$,"I want to determine the convergence of the following series in dependency of $x$ : $\sum\limits_{n=1}^{\infty} \frac{n+2}{2n^2+2} x^n=\frac{3}{4}x+\frac{2}{5}x^2+\frac{1}{4}x^3+\frac{3}{17}x^4+ ... $ How can I solve this? EDIT:
@Winther said, I should try the ratio test: $q := \lim_{n \to \infty} \left| \frac{a_{n+1}}{a_n}\right| $ So we get $q = \lim_{n \to \infty} \left| \frac{\frac{n+3}{2(n+1)^2+2}}{\frac{n+2}{2n^2+2}}\right| =  \lim_{n \to \infty} \left| \frac{n+3}{2(n+1)^2+2}\frac{2n^2+2}{n+2} \right| = \lim_{n \to \infty} \left| \frac{2n^3+2n+6n^2+6}{2(n+1)^2n+4(n+1)^2+2n+4}\right| = \lim_{n \to \infty} \left| \frac{n^3+3n^2+n+3}{n^3+4n^2+6n+4}\right| $ With the tip from @Alex Vong to divide by $n^3$ the ratio test becomes: $ q = \lim_{n \to \infty} \left| \frac{1+3/n+1/n^2+3/n^3}{1+4/n+6/n^2+4/n^3}\right|= 1$ So now there is no clear statement if the series is convergent ( $q<1$ convergent, $q>1 $ divergent). Should I try another test (e. g. the root test)? EDIT 2: Corrected the first coefficients.","['convergence-divergence', 'sequences-and-series']"
3049876,Race of the wealthy gamblers: How do I get this closed form?,"UPDATE: If you can prove the following identity: $$\sum\limits_{t=0}^p \left(\frac{{2t+1 \choose t}}{2^{2t+1}}\right)^2 \frac{1}{t+2} = -1 + \frac{(2+p)}{2^{4p+4}}{2p+3 \choose p+1}^2$$ Then this is good enough to solve this question and get my gratitude as well as the 50 point bounty. I got this identity from Mathematica. See my answer below for details. My question relates to an infinite summation and it's very elegant closed form. The expression is the solution to a nice problem which I'll get into as well. Here is the summation: Let: $$a_t = \left({2t+1 \choose t} - {2t+1 \choose t-1}\right)\frac{1}{2^{2+2t}}$$ and, $$b_t = \left({2t+2 \choose t} - {2t+2 \choose t-1}\right)\frac{1}{2^{3+2t}}$$ For the very first terms of these sequences, ${n \choose -1} = 0$ for all $n$ . And the summation: $$S = \sum_{t=0}^{\infty} \left(1-\sum_{l=0}^{t-1}b_l\right) a_t = 1- \sum_{t=0}^{\infty} \left(\sum_{l=0}^{t-1}b_l\right) a_t =  7-\frac{20}{\pi} \tag{1}$$ I know the expression above is correct (verified with a python program), but have no idea how to prove this and would like to at least see how I might approach it. Now, why do I care about this summation? It is the solution to the following problem: Imagine two wealthy gamblers start tossing their own separate fair coins, winning 1\$ on heads and losing 1\$ on tails. Both start at 0\$ and have infinite bank balances. The first one wants to get to 2\$ and the second one wants to get to 3\$. What is the probability that the first one will reach his goal before the second one? One way to solve this is to consider the probability that a gambler reaches exactly $k$ dollars for the first time on toss $n$ . If he has $t$ tails, then he needs $t+k$ heads. So, $n=2t+k$ (note if k=2\$, he can only reach the target on even tosses and if k=3\$, he can only reach it on odd tosses). This probability turns out to be: $$\left({k+2t-1 \choose t} - {k+2t-1 \choose t-1}\right) \frac{1}{2^{k+t}} \frac{1}{2^t}$$ Now, let $A_n$ be the probability that the 2\$ targeting gambler wins on toss $n$ and $A$ be the probability that he wins. Then we have $A = \bigcup\limits_{n=1}^{\infty}A_n$ and so, $P(A) = \sum\limits_{n=0}^\infty P(A_n)$ . Now, for the 2\$ targeting gambler to win on the $n$ th toss, two things should happen: He should reach his target on the $n$ th toss for some even $n$ . His competitor, the 3\$ gambler should not reach his target on any toss upto $n-1$ (since he can only reach his target on odd tosses). Putting all of this together, you can see that the probability that the 2\$ gambler wins is given by equation (1) above. I have put together some python code that approximates $S$ by going upto a large number of tosses. A Reddit user pointed out the closed form for which he used a slightly different but related approach and Mathematica. Now, how do I prove that the summation above has the closed form mentioned $(7-\frac{20}{\pi})$ ? EDIT: Here is a short python snippet that demonstrates the summation in equation (1) above. a_t = np.array([(comb(2*t+1,t)-comb(2*t+1,t-1))/2**(2*t+2) for t in range(500)])
b_t = np.array([(comb(2*t+2,t)-comb(2*t+2,t-1))/2**(2*t+3) for t in range(500)])
b_sum = 1-np.concatenate(([0],np.cumsum(b_t)))
s = sum(a_t*b_sum[:500])
print(7-20/np.pi-s) Also, here is the Mathematica snippet that shows the result (thanks to @SteveKass for helping with that):","['summation', 'markov-chains', 'probability', 'sequences-and-series']"
3049887,"When is One Polynomial ""Similar"" to Another","Let $f$ and $g$ be functions. We will say they are similar (in somewhat of an extension to what it means for linear maps to be similar) if there exists a bijection $p$ such that $$f=p^{-1}\circ g\circ p.$$ Let this relation be denoted as $\sim$ . Under what conditions can polynomials be similar? Specifically, I am trying to find out for which polynomials $f$ can we say that $f\sim T_n$ for some $n$ where $T_n$ is the $n$ -th Chebyshev polynomial. Note that when $f$ , $g$ , and $p$ are all restricted to linear functions from $\mathbb{R}^n\to\mathbb{R}^n$ this aligns with the normal definition of similar matrices. Could a similar equivalence be drawn for polynomial functions? Possibly one could consider matrices over the field $\mathbb{R}[x]$ , and apply a similar theory to get partial results for the above problem.","['functional-analysis', 'real-analysis']"
3049891,"Finding $\iiint x^2\,{\rm d}x{\rm d}y{\rm d}z$ over the volume bounded by $\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2 }= 1$","Finding the value of $$\iiint x^2\,\mathrm dx\mathrm dy\mathrm dz$$ over the volume bounded by the ellipsoid $$\frac{x^2}{a^2} + \frac{y^2}{b^2} + \frac{z^2}{c^2}= 1.$$ I am taking limits as follow: $$-a \leq x \leq a$$ $$-\frac{b}{a} (a^2-x^2)^{1/2} \leq y \leq \frac{b}{a} (a^2-x^2)^{1/2}$$ $$-\frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2} \leq z \leq \frac{c}{ab} ((ab)^2 - (bx)^2 - (ay)^2)^{1/2}$$ And solving the integral as $\mathrm dz$ first then $\mathrm dy$ and then $\mathrm dx$ . But it is just becoming harder and harder with each step. I do not have a tutor. So, I highly need help of you masters. Please help me.","['multivariable-calculus', 'definite-integrals']"
3049905,Why does the criterion for convergence of a power series not imply every series with bounded terms converges?,"I am reading Complex Made Simple by David C. Ullrich. There is a result from which I am deducing bogus conclusions, so I must be misunderstanding it somehow: Lemma 1.0. Suppose $(c_n)_{n = 0}^{\infty}$ is a sequence of complex numbers, and define $R \in [0, \infty]$ by $$R = \sup \{r \ge 0: \text{the sequence } (c_nr^n) \text{ is bounded}\}.$$ Then the power series $\sum_{n=0}^{\infty}c_n(z-z_0)^n$ converges absolutely and uniformly on every compact subset of the disk $D(z_0, R)$ and diverges at every point $z$ with $|z-z_0|>R$ . My bogus conclusion: Let $c_n$ be a sequence of complex numbers and suppose that $c_n r^n$ is bounded. Then $\sum_{n=0}^{\infty} c_n r^n$ converges. My reasoning: Let $c_n$ be any sequence of complex numbers. The series $\sum_{n=0}^{\infty}c_n(z-z_0)^n$ converges absolutely whenever $|z - z_0|<R$ , so $\sum_{n=0}^{\infty}c_nr^n$ converges whenever $r < R$ , so $\sum_{n=0}^{\infty}c_nr^n$ converges whenever $c_n r^n$ is bounded.","['complex-analysis', 'convergence-divergence', 'power-series']"
3049932,References/Proof of the conjectured identity for the Stirling permutation number $\left\{{n\atop n-k}\right\}$,"While working with a combinatorics problem, I conjectured that $$ \left\{{n \atop n-k }\right\}=\sum_{p=0}^{k-1}\bigg\langle\!\!\bigg\langle{k\atop k-1-p}\bigg\rangle\!\!\bigg\rangle \binom{n+p}{2k}, $$ where $\left\{ {n \atop k} \right\}$ is the Stirling permutation numbers and $\big\langle\!\big\langle{n \atop k}\big\rangle\!\big\rangle $ denotes the Eulerian numbers of the second kind . All of my motivation comes from the fact that this is known to hold for $k = 1, 2, 3$ . (See this , for instance.) I have little background on this topic, and I was unable to find this one from DLMF . I numerically checked that this identity holds for $n = 1, \cdots, 10$ using CAS and OEIS A008517 . Although I hardly believe that this type of identity is not known, I could not find any proof or reference to this one. So any additional information will be appreciated!","['eulerian-numbers', 'combinatorics', 'stirling-numbers', 'reference-request']"
3049941,An example of a valuation function,"Let $F$ be the set of all functions from $\mathbb{N}$ into $\mathbb{N}$ . For $f,g\in F$ define $\text{val}(f,g)=\begin{cases}\min\{n\in\Bbb N~:~f(n)\neq g(n)\}&\text{if}~f\neq g\\\infty&\text{if}~f=g\end{cases}$ where $\infty$ is a new symbol. Question. For $n\in\mathbb{N}$ let $s_n\in F$ be the constant function that takes the value $n$ for all variables. For $n,m\in\mathbb{N}$ find $val(s_n,s_m)$ . My answer. Assume $s_n\neq s_m$ . Note that $s_n(x)=n$ for all $x$ and $s_m(x)=m$ for all $x$ . So, what I say about $\min\{n\in\Bbb N~:~f(n)\neq g(n)\}$ ? Can you help?",['elementary-set-theory']
3049949,Methods to solve $\int_{0}^{\infty} \frac{\cos\left(kx^n\right)}{x^n + a}\:dx$,"Spurred on by this question, I decided to investigate for different functions on the numerator. Here, I went from $\exp(..)$ to $\sin(..) / \cos(..)$ . I initially thought I could modify the result from $\exp(..)$ but got stuck . So I decided on another approach which here is a combination of Feynman's Trick, Laplace Transforms and coupled ODE Systems. I would love for a qualified eye to have a look over to see if what I've done is correct and/or another method (Not using Complex Analysis) to solve. Here I've included more of my algebra to aid in those who wish to go over. Consider the following two definite integrals \begin{align}
    I_{n,a,k} &= \int_{0}^{\infty} \frac{\sin\left(kx^n\right)}{x^n + a}\:dx \\
    J_{n,a,k} &= \int_{0}^{\infty} \frac{\cos\left(kx^n\right)}{x^n + a}:dx
\end{align} For $a,k \in \mathbb{R}^+$ and $n \in \mathbb{R}^{+}, n > 1$ . Here we define: \begin{align}
    I_{n,a,k}(t) &= \int_{0}^{\infty} \frac{\sin\left(tkx^n\right)}{x^n + a}:dx \\
    J_{n,a,k}(t) &= \int_{0}^{\infty} \frac{\cos\left(tkx^n\right)}{x^n + a}:dx
\end{align} Here we observe that: \begin{align}
    I_{n,a,k}(1) &= I_{n,a,k} & I_{n,a,k}(0) &= 0   \\
    J_{n,a,k}(1) &= J_{n,a,k} & J_{n,a,k}(0) &= a^{\frac{1}{n} - 1} \frac{\Gamma\left(1 -\frac{1}{n}\right)\Gamma\left(\frac{1}{n} \right)}{n} = \theta_{a,n}
\end{align} Here we will address each integral individually. For $I_{n,a,k}$ we take the derivative with respect to ' $t$ ': \begin{align}
    I_{n,a,k}'(t) &= \int_{0}^{\infty} \frac{kx^n\cos\left(tkx^n\right)}{x^n + a}\:dx = k\left[\int_{0}^{\infty} \cos\left(tkx^n\right)\:dx - a\int_{0}^{\infty} \frac{\cos\left(tkx^n\right)}{x^n + a}\:dx \right] \\
    \frac{1}{k}I_{n,a,k}'(t) &= \frac{1}{k^{\frac{1}{n}}t^{\frac{1}{n}}}\int_{0}^{\infty} \cos\left(u^n\right)\:du - aJ_{n,a,k}(t)
\end{align} Thus, \begin{equation}
    \frac{1}{k}I_{n,a,k}'(t) + aJ_{n,a,k}(t) = \frac{1}{k^{\frac{1}{n}}t^{\frac{1}{n}}}\int_{0}^{\infty} \cos\left(u^n\right)\:du
\end{equation} From Section X, we arrive at: \begin{equation}
    \frac{1}{k}I_{n,a,k}'(t) + aJ_{n,a,k}(t) =  \frac{\Gamma\left(\frac{1}{n}\right)\cos\left(\frac{\pi}{2n} \right)}{nk^{\frac{1}{n}}t^{\frac{1}{n}}}
\end{equation} Applying the same method to $J_{n,a,k}\left(t\right)$ we arrive at: \begin{equation}
    -\frac{1}{k}J_{n,a,k}'(t) + aI_{n,a,k}(t) = \ \frac{\Gamma\left(\frac{1}{n}\right)\sin\left(\frac{\pi}{2n} \right)}{nk^{\frac{1}{n}}t^{\frac{1}{n}}}
\end{equation} And thus, we arrive at the couple ordinary differential equation system: \begin{align}
    \frac{1}{k}I_{n,a,k}'(t) + aJ_{n,a,k}(t) &=  \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)t^{-\frac{1}{n}}\\
    aI_{n,a,k}(t) -\frac{1}{k}J_{n,a,k}'(t)  &=  \Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)t^{-\frac{1}{n}}
\end{align} Where $\Psi_{k,n} = \frac{\Gamma\left(\frac{1}{n}\right)}{n}k^{-\frac{1}{n}}$ . Although there are many approaches to solving this system, here I will employ Laplace Transforms: \begin{align}
    \frac{1}{k}\mathscr{L}\left[I_{n,a,k}'(t)\right] + a\mathscr{L}\left[J_{n,a,k}(t)\right] &=  \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\mathscr{L}\left[t^{-\frac{1}{n}}\right]\\
    a\mathscr{L}\left[I_{n,a,k}(t)\right] -\frac{1}{k}\mathscr{L}\left[J_{n,a,k}'(t)\right]  &=  \Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\mathscr{L}\left[t^{-\frac{1}{n}}\right]
\end{align} Which becomes: \begin{align}
    \frac{s}{k}\bar{I}_{n,a,k}(s)  + a\bar{J}_{n,a,k}(s) &=  \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\kappa(s)\\
    a\bar{I}_{n,a,k}(s) -\frac{s}{k}\bar{J}_{n,a,k}(s)   &=  \Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\kappa(s) + \frac{1}{k}\theta_{a,n}
\end{align} Where \begin{equation}
    \kappa(s) = \mathscr{L}\left[t^{-\frac{1}{n}}\right] = \Gamma\left(1 - \frac{1}{n}\right)s^{1 - \frac{1}{n}}
\end{equation} Solving for $\bar{J}_{n,a,k}(s)$ we find: \begin{align}
    \bar{J}_{n,a,k}(s) &= \frac{1}{s^2 + a^2k^2}\left[ak^2 \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\kappa(s) -k\Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)s\kappa(s) - s\theta_{a,n}\right] \\
    &=ak^2 \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\frac{1}{s^2 + a^2k^2}\kappa(s)-k\Psi_{k,n}\frac{s}{s^2 + a^2k^2}\kappa(s)\\
    &\qquad- \theta_{a,n}\frac{s}{s^2 + a^2k^2}
\end{align} Taking the Inverse Laplace Transform, we arrive at: \begin{align}
    &J_{n,a,k}(t) = \mathscr{L}^{-1}\left[ \bar{J}_{n,a,k}(s) \right] = ak^2 \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\mathscr{L}^{-1}\left[\frac{1}{s^2 + a^2k^2}\kappa(s)\right]\\
    &\qquad-ak\Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\mathscr{L}^{-1}\left[\frac{s}{s^2 + a^2k^2}\kappa(s)\right]- \theta_{a,n}\mathscr{L}^{-1}\left[\frac{s}{s^2 + a^2k^2}\right] \\
    &= ak^2 \Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\int_{0}^{t} \frac{1}{ak}\sin\left(ak\left(t - \tau\right)\right) \tau^{-\frac{1}{n}}\:d
tau\\
    &\qquad-ak\Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\int_{0}^{t} \cos\left(ak\left(t - \tau\right)\right) \tau^{-\frac{1}{n}}\:d
tau -\theta_{a,n}\cos\left(akt \right) \\
    &= k\Psi_{k,n}\cos\left(\frac{\pi}{2n} \right)\int_{0}^{t} \left[\sin\left(akt\right)\cos\left(ak\tau\right)  - \sin\left(ak\tau\right)\cos\left(akt\right) \right]\tau^{-\frac{1}{n}}\:d\tau\\
    &\qquad-k\Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\int_{0}^{t} \left[\cos\left(akt\right)\cos\left(ak\tau\right)  +\sin\left(ak\tau\right)\sin\left(akt\right) \right] \tau^{-\frac{1}{n}}\:d\tau \\
    &\qquad-\theta_{a,n}\cos\left(akt \right) \\
    &= k\Psi_{k,n}\cos\left(\frac{\pi}{2n} \right) \left[\sin\left(akt\right)\int_{0}^{t} \frac{\cos\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau - \cos\left(akt\right)\int_{0}^{t} \frac{\sin\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau \right]  \\
    &\qquad-k\Psi_{k,n}\sin\left(\frac{\pi}{2n} \right)\left[\cos\left(akt\right)\int_{0}^{t} \frac{\cos\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau+ \sin\left(akt\right)\int_{0}^{t} \frac{\sin\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau  \right] \\
     &\qquad-\theta_{a,n}\cos\left(akt \right) \\
     &= k \Psi_{k,n}\left[\sin\left(akt + \frac{\pi}{2n}\right) \int_{0}^{t} \frac{\cos\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau-\cos\left(akt + \frac{\pi}{2n}\right) \int_{0}^{t} \frac{\sin\left(ak\tau\right) }{\tau^{\frac{1}{n}}}\:d\tau \right] \\
     &\qquad-\theta_{a,n}\cos\left(akt \right) \\
     &= k \Psi_{k,n}\left[\sin\left(akt + \frac{\pi}{2n}\right) k^{\frac{1}{n} - 1} a^{\frac{1}{n} - 1}\int_{0}^{akt} \frac{\cos\left(u\right) }{u^{\frac{1}{n}}}\:du-\cos\left(akt + \frac{\pi}{2n}\right)k^{\frac{1}{n} - 1} a^{\frac{1}{n} - 1} \int_{0}^{t} \frac{\sin\left(u\right) }{u^{\frac{1}{n}}}\:du \right] \\
     &\qquad-\theta_{a,n}\cos\left(akt \right) \\
          &= k k^{\frac{1}{n} - 1} a^{\frac{1}{n} - 1} \Psi_{k,n}\left[\sin\left(akt + \frac{\pi}{2n}\right) \int_{0}^{akt} \frac{\cos\left(u\right) }{u^{\frac{1}{n}}}\:du-\cos\left(akt + \frac{\pi}{2n}\right)\int_{0}^{akt} \frac{\sin\left(u\right) }{u^{\frac{1}{n}}}\:du \right] \\
     &\qquad-\theta_{a,n}\cos\left(akt \right)
\end{align} Hence, \begin{align}
    J_{n,a,k}(t) &= \int_{0}^{\infty} \frac{\cos\left(tkx^n\right)}{x^n + a}\:dx \\
          &=a^{\frac{1}{n} - 1}\frac{\Gamma\left(\frac{1}{n} \right)}{n} \left[\sin\left(akt + \frac{\pi}{2n}\right) \int_{0}^{akt} \frac{\cos\left(u\right) }{u^{\frac{1}{n}}}\:du-\cos\left(akt + \frac{\pi}{2n}\right)\int_{0}^{akt} \frac{\sin\left(u\right) }{u^{\frac{1}{n}}}\:du \right] -\theta_{a,n}\cos\left(akt \right) 
\end{align} And finally, \begin{align}
    J_{n,a,k} &= J_{n,a,k}(1) = \int_{0}^{\infty} \frac{\cos\left(kx^n\right)}{x^n + a}\:dx \\
          &=a^{\frac{1}{n} - 1}\frac{\Gamma\left(\frac{1}{n} \right)}{n} \left[\sin\left(ak + \frac{\pi}{2n}\right) \int_{0}^{ak} \frac{\cos\left(u\right) }{u^{\frac{1}{n}}}\:du-\cos\left(ak + \frac{\pi}{2n}\right)\int_{0}^{ak} \frac{\sin\left(u\right) }{u^{\frac{1}{n}}}\:du \right] \\
          &\qquad-\cos\left(ak \right) a^{\frac{1}{n} - 1} \frac{\Gamma\left(1 -\frac{1}{n}\right)\Gamma\left(\frac{1}{n} \right)}{n} 
\end{align}","['integration', 'definite-integrals', 'ordinary-differential-equations', 'laplace-transform', 'solution-verification']"
3049953,Finding elements such that none add to a perfect square,"Bob asks us to find an inﬁnite set $S$ of positive integers such that the sum of any ﬁnite number of distinct elements of S is not a perfect square. Can Bob's request be fulfilled? I can find some finite sets, and the sequence A133662 (OEIS) seems to work but I don't know if that sequence is infinite or not. Maybe if we picked lots of elements with a common property?","['number-theory', 'square-numbers', 'sumset', 'sequences-and-series']"
3050013,Does every set of positive measure contain an uncountable null set?,"If $E$ is Lebesgue measurable and $m(E)>0$ , does there exist an uncountable $C\subset E$ with $m(C)=0$ ? This seems intuitively clear but I cannot prove it. Since $E$ has positive measure it contains a nonmeasurable set $V$ , and every measurable subset of $V$ is null, but I could not show $V$ contains uncountable measurable subsets. I also tried using the fact that for $\alpha\in(0,1)$ there is an interval $I$ s.t $m(E\cap I)\ge \alpha m(I)$ , attempting to construct a Cantor set inside $I$ with an uncountable intersection with $E$ but I was unsuccessful. Any hints? Is this even true?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
3050063,What is the structure preserved by strong equivalence of metrics?,"Let $d_1$ and $d_2$ be two metrics on the same set $X$ .  Then $d_1$ and $d_2$ are (topologically) equivalent if the identity maps $i:(M,d_1)\rightarrow(M,d_2)$ and $i^{-1}:(M,d_2)\rightarrow(M,d_1)$ are continuous. $d_1$ and $d_2$ are uniformly equivalent if $i$ and $i^{-1}$ are uniformly continuous.  And $d_1$ and $d_2$ are strongly equivalent if there exist constants $\alpha,\beta>0$ such that $\alpha d_1(x,y)\leq d_2(x,y)\leq\beta d_1(x,y)$ for all $x,y\in X$ . All three of these are equivalence relations, so we can take equivalence classes under each one.  If we take equivalence classes of metrics under equivalence, we can identify each equivalence class with a topology on $X$ .  If we take equivalence classes of metrics under uniform equivalence, we can identify each equivalence class with a uniformity on $X$ .  But my question is, if we take equivalence classes of metrics under strong equivalence, what kind of structure in $X$ can we identify each equivalence class with? Note that I'm looking for a structure that makes no reference to metrics, just as you can define topological spaces and uniform spaces with no reference to metrics.","['metric-spaces', 'category-theory', 'uniform-spaces', 'quotient-spaces', 'general-topology']"
3050093,Variance of $XY(1-Y)$ in terms of the means and variances $X$ and $Y$,"Consider two independent random variables X and Y. X has some distribution with mean $\mu_X$ and variance $\sigma^2_X$ . Y has some distribution with mean $\mu_Y$ and variance $\sigma^2_Y$ . I want to take the variance of the quantity $XY(1-Y)$ . Is the correct answer: $$\sigma^2_{XY(1-Y)} = \sigma^2_X[Y(1-Y)]^2+X^2[\sigma^2_Y(1-\sigma^2_Y)]?$$ I just did some kind of weird product rule but with variances and I'm not sure that's at all legit. The reason I want to know the variance of this quantity is that it is a measure of the real noise in an experimental system I am working with. I am an experimental physicist and my math skills are weak at best. You might recognize the form $XY(1-Y)$ from the binomial distribution: $Np(1-p)$ . My experiment does not follow a binomial distribution exactly, it follows a Poisson binomial distribution but with varying $N$ and $p$ for each shot of the experiment. So I need to know the variance of the quantity $XY(1-Y)$ when I know the variances and expectation values of $X$ and $Y$ . I'm trying to work this particular detail out to answer this question: Repeated binomial processes with different probabilities and number of trials .","['variance', 'probability-theory']"
3050117,Peetre's Inequality - not strict?,"(Peetre's inequality)
      Let $x,y \in \Bbb R^n$ and $s \in \Bbb R$ . Then $$ \frac{(1+|x|^2)^s}{(1+|y|^2)^s} \le 2^{|s|} (1+|x-y|^2)^{|s|}.$$ Proof: By switching roles of $x,y$ we may suppose $s \ge 0$ , and taking $s$ th root may assume $s=1$ . Then the argument i found online: \begin{align*}
(1+|x|^2) &=  1 + |x-y|^2 + |y|^2 +2(x-y)y  \\ 
 & \le 1 + |x-y|^2 + |y|^2 + (|x-y|^2+|y|^2) \\ 
& \le 2(1+|y|^2 + |x-y|^2 +|y|^2|x-y|^2 ) \\ 
& = 2(1+|y|^2)(1+|x-y|^2).
\end{align*} What I don't understand is that on the third inequality, isn't this clearly a strict inequality when $s\not= 0$ ? (at least by 1)","['inequality', 'functional-analysis', 'ordinary-differential-equations', 'real-analysis']"
3050153,Common Doubt: What did I do wrong here?-Number of ways to arrange green and blue bottles........,"Number of ways in which $7$ green bottles and $8$ blue bottles can be arranged in a row if exactly $1$ pair of green bottles is side by side, is ______ Note-Assume all bottles to be alike except for the color. Attempt _|_|_|_|_|_|_|_|_ $8$ Blue bottles firstly arranged in $1$ ways only since all are identical. Now selected $2$ green bottles in $1$ way only again as it is identical. Now the $6$ elements ( $1$ pair bottles and $5$ bottles green in color) can be placed into $^9C_6$ which is equal to $84$ ways. But the answer in answer key is $504$ ways. Where is the fault in my process? I tried to think about the flaw but couldn't encounter on it.","['permutations', 'self-learning', 'combinations', 'proof-verification', 'combinatorics']"
3050165,Is the orthogonal polar factor the unique submersion satisfying an orthogonality relation?,"$\newcommand{\psym}{\text{Psym}_n}$ $\newcommand{\sym}{\text{sym}}$ $\newcommand{\Sym}{\operatorname{Sym}}$ $\newcommand{\Skew}{\operatorname{Skew}}$ $\renewcommand{\skew}{\operatorname{skew}}$ $\newcommand{\GLp}{\operatorname{GL}_n^+}$ $\newcommand{\SO}{\operatorname{SO}_n}$ The orthogonal polar factor map $O:\GLp \to \SO$ , defined by requiring $A=
O(A)P$ for some symmetric positive-definite $P$ , is a smooth submersion satisfying $A \perp T_{O(A)}\SO$ . Question: Let $F:\GLp \to \SO$ be a smooth submersion satisfying $A \perp T_{F(A)}\SO$ . Does $F(A)=Q \cdot O(A)$ or $F(A)= O(A)  \cdot Q$ for some $Q \in \SO$ ? Edit: An equivalent reformulation of the question: $A \perp T_{F(A)}\SO=F(A)\skew \iff A \in (F(A)\skew)^\perp=F(A)(\skew)^\perp=F(A)\sym$ . Thus, if we define $S(A)=F(A)^{-1}A$ , $S:\GLp \to \sym$ is smooth. So, a submersion $F:\GLp \to \SO$ satisfies the orthogonality requirement if and only if there exist a smooth map $S:\GLp \to \sym$ , satisfying $A=F(A)S(A)$ . Using the polar decomposition, we have $O(A)P(A)=A=F(A)S(A)$ , so $S(A)=Q(A)P(A)$ where $Q(A)=F(A)^{-1}O(A)$ . We want to prove that $Q:\GLp \to \SO$ is constant. I will now prove that for matrices $A$ having distinct singular values, $Q(A)$ can obtain a finite number of values. (The set of admissible values depends on $A$ ). I am quite sure this fact can be used to force $Q$ to be constant or at least something very constrained, but I am not sure how. Here is the proof: Since $S(A)=Q(A)P(A) \in \sym$ , we have $PQ^T=(QP)^T=S^T=S=QP$ . By orthogonally diagonalizing $P$ , we can write $P=V\Sigma V^T$ , so we now have $$ V\Sigma V^T Q^T=QV\Sigma V^T \Rightarrow \Sigma V^T Q^TV=V^TQV\Sigma. $$ Setting $\tilde Q=V^TQV$ we thus have $ \Sigma \tilde Q^T= \tilde Q \Sigma$ where $\tilde Q \in \SO$ . Since we assumed that the singular values of $A$ are distinct (i.e. the diagonal entries of $\Sigma$ are distinct), an explicit calculation now shows that $ \tilde Q$ must be diagonal. Since it is also orthogonal, we must have $\tilde Q_{ii}=\pm 1$ for all $i$ . So, $\tilde Q$ can assume a finite set of values; (which implies the same thing for $ Q$ ). Comment: I am not sure for which $Q \in \SO$ $F(A)=Q\cdot O(A)$ satisfies the requirement. A necessary condition is $Q^2=Id$ ; I don't know if it's sufficient. Indeed, let $Q \in \SO$ , and set $F(A)=Q\cdot O(A)$ . Then $ A \perp T_{F(A)}\SO=T_{Q\cdot O(A)}\SO=QT_{O(A)}\SO,$ so for $A=Id$ we have $ Id \perp Q\skew \Rightarrow Q^T \perp \skew \Rightarrow Q^T \in \sym \Rightarrow Q^2=Id$ .","['differential-geometry', 'riemannian-geometry', 'orthogonal-matrices', 'lie-groups', 'matrix-decomposition']"
3050178,Prove $x^3+3y^3+9z^3-9xyz=1$ has infinity integers solutions!,A question from Alibaba Global Mathematics Competition (number theory) Prove $\displaystyle x^{3} +3y^{3} +9z^{3} -9xyz=1$ has infinitely many integer solutions. The hint for the question is to transform the left side to a complex polynomial. I found that: Set $\lambda =e^{j2\pi /3}$ then the equation can be transformed to: $$ \left( x+3^{1/3} y+3^{2/3} z\right)\left( x+3^{1/3} \lambda y+3^{2/3} \lambda ^{2} z\right)\left( x+3^{1/3} \lambda ^{2} y+3^{2/3} \lambda z\right)=1$$ but I don't know how to continue.,"['contest-math', 'number-theory']"
3050208,"I calculated $\sin 75^\circ$ as $\frac{1}{2\sqrt{2}}+\frac{\sqrt{3}}{2\sqrt{2}}$, but the answer is $\frac{\sqrt{2}+\sqrt{6}}{4}$. What went wrong?","I calculated the exact value of $\sin 75^\circ$ as follows: $$\begin{align}
\sin 75^\circ &= \sin(30^\circ + 45^\circ)  \\
&=\sin 30^\circ \cos 45^\circ + \cos 30^\circ \sin 45^\circ \\  
&=\frac12\cdot\frac{1}{\sqrt{2}} + \frac{\sqrt{3}}{2}\cdot\frac{1}{\sqrt{2}} \\  
&= \frac{1}{2\sqrt{2}} + \frac{\sqrt{3}}{2\sqrt{2}}
\end{align}$$ The actual answer is $$\frac{\sqrt{2} + \sqrt{6}}{4}$$ My main confusion is how the textbook answer is completely different from mine, even though if I compute $\sin 30^\circ \cos45^\circ + \cos 30^\circ \sin 45^\circ$ , it will be approximately the same value of $\sin 75^\circ$ . I think I'm having difficulty adding and subtracting the radicals. So, if someone can demonstrate to me how they got that answer, it will be helpful. Thanks.","['trigonometry', 'algebra-precalculus', 'geometry']"
3050228,"If $\ker f\subset \ker g$ where $f,g $ are non-zero linear functionals then show that $f=cg$ for some $c\in F$.","Let $V$ be a vector space with $\dim V=n$ . If $\ker f\subset \ker g$ where $f,g $ are non-zero linear functionals then show that $f=cg$ for some $c\in F$ . Now let $\mathcal B=\{v_1,v_2,\ldots ,v_n\}$ be a basis of $V$ , since $f,g$ are non-zero linear functionals then $\exists v_i\in \mathcal B $ such that $g(v_i)\neq 0\implies f(v_i)\neq 0$ Take $i=1$ without any loss of generality so take $g(v_1)\neq 0,f(v_1)\neq 0$ . Now take $c=\dfrac{f(v_1)}{g(v_1)}$ Then we need to show that $(f-cg)(v_i)=0\forall i$ Now $(f-cg)(v_1)=0$ How to show that $(f-cg)(v_i)=0\forall i\ge 2$ Can someone please help? Note::Another Question Why do we need the dimension of the vector space to be finite?","['linear-algebra', 'vector-spaces', 'linear-transformations']"
3050230,Show $I(V(f))=(f)$ when $f$ is irreducible.,"This is a question from Perrin's text, and it goes like this: Let $k$ be algebraically closed. Let $F\in k[x,y]$ be an irreducible polynomial. Assume that $V(F)$ is infinite. Prove that $I(V(F))=(F)$ . Here, $V(f)$ is the set of zeroes of the polynomial, and $I(V(F))$ is the ideal $(V(F))$ . Proof:
Since $k$ is a field, $k[x,y]$ is a unique factorization domain. So if $F$ is an irreducible polynomial in $k[x,y]$ that's the same as being a prime element. So $F$ generates a prime ideal. Next, we have that $\textbf{rad}(F)=(F)$ , since $(F)$ is a prime ideal. So by the Nullstellensatz, $(F)=I(V(F))$ . That is my proof for the problem, but nowhere in my proof did I use the hypothesis that $V(F)$ was infinite. So I was wondering if my proof is valid, or if I made some wrong assumption along the way. Also this proof would work for $k[x_1,...,x_n]$ , and the problem only asks for $k[x,y]$ , so I'm extra dubious about its correctness.",['algebraic-geometry']
3050244,Total rotation of a circle (or other closed curve) when 'rolled' along a curve in $\mathbb{R}^2$,"As to compute how much a circle rotates when 'rolled' along a curve in $\mathbb{R}^2$ , the most intuitive way to me to find the number of rotations is: $S/C+W/(2\pi)$ $S$ is the arc-length of the curve $C$ is the circumference of the circle $W$ is the total curvature of the curve However, this seems to agree with: $T/C$ $T$ is the arc-length of path of the center of the circle Can anyone intuitively explain why the latter works as well? Also I'm wondering whether $T/C$ still works if the circle is replaced by some closed curve (whereby $C$ is the arc-length of the closed curve and $T$ is the arc-length of path of the mass-center of the closed curve). Edit : After writing down the integrals, I think a sensible generalization might (rather than the mass-center) have more to do with the center of the osculating circle of the close curve at its current intersection with the curve it's being rolled on. $\ $ Edit: In other words: Let $\ c:[a,b]\to\mathbb{R}^2\ $ be some smooth curve along which a circle with radius $\text{abs}(r)$ is being rolled. Let $\ \text{center}:[a,b]\to\mathbb{R}^2\ $ be the center of the circle given by: $$\text{center}(t)=c(t)+r\frac{\{c_2'(t),-c_1'(t)\}}{||c'(t)||_2}$$ That is the sign of $r$ determines on which side of the curve the circle is being rolled. Expressed with integrals, the formulas for the total rotation are $S/C+W/(2\pi)={\large\int_a^b}\dfrac{||c'(t)||_2}{2r\pi}dt+ 
              {\huge\int_{\large a}^{\large b}}\dfrac{\det{\left(
\begin{array}{cc}
 c_1'(t) & c_2'(t) \\
 c_1''(t) & c_2''(t) \\
\end{array}
\right)}}{||c'(t)||_2^2\cdot (2\pi)}dt$ $T/C = {\large\int_a^b}\dfrac{||\text{center}'(t)||_2}{2\,\text{abs}(r)\pi}\cdot\text{sign}\left(\dfrac{1}{r}+\dfrac{\det{\left(
\begin{array}{cc}
 c_1'(t) & c_2'(t) \\
 c_1''(t) & c_2''(t) \\
\end{array}
\right)}}{||c'(t)||_2^3}\right)dt$ both of which are influenced by the signs of $r$ and the curvature determinant.","['curvature', 'differential-geometry']"
3050265,Which matrices can be realized as second derivatives of orthogonal paths?,"$\newcommand{\skew}{\operatorname{skew}}$ $\newcommand{\sym}{\operatorname{sym}}$ $\newcommand{\SO}{\operatorname{SO}_n}$ I am interested to know which real matrices $A \in M_n$ can be realized as second derivatives of paths in $\text{SO}_n$ starting at the identity. That is, for which matrices $A$ , there exist a smooth path $\alpha:(-\epsilon,\epsilon) \to \text{SO}_n$ , such that $\alpha(0)=Id$ and $\ddot \alpha(0)=A$ . We denote the space of realizable matrices by $D$ . Question: I prove below that $ (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew $ . Does $D=(\skew)^2+\skew$ always hold? Comment: Note that $(\skew)^2+\skew \subsetneq M_n$ , at least for odd $n$ : In that case every skew-symmetric matrix is singular, so $(\skew)^2 \subseteq \sym $ consists only of singular matrices, hence does not contain all symmetric matrices. Edit: I proved below that equality holds in dimension $n=2$ . Proof of $ (\skew)^2 \subseteq D \subseteq (\skew)^2+\skew $ : Every square of skew-symmetric matrix can be realized: For skew $B$ , take $\alpha(t)=e^{tB}$ . Then, $\dot \alpha(t)=Be^{tB}$ , $\ddot \alpha(t)=B^2e^{tB}$ . The space of realizable matrices is contained in $(\skew)^2+\skew$ : Indeed, since $\dot \alpha(t) \in T_{\alpha(t)}\SO=\alpha(t)\skew$ , we have $\dot\alpha(t)=\alpha(t)B(t)$ for some $B(t) \in \skew$ , so $$\ddot \alpha(t)=\dot \alpha(t) B(t)+\alpha(t) \dot B(t)$$ hence $\ddot \alpha(0)=\dot \alpha(0) B(0)+ \dot B(0)= B(0)^2+\dot B(0) \in (\skew)^2 +\skew,$ where the last equality followed from $\dot \alpha(0)=B(0)$ (put $t=0$ in $\dot\alpha(t)=\alpha(t)B(t)$ ). Edit 2: When trying to show the converse direction, I hit a wall: we need to show that there exist solutions $\dot\alpha(t)=\alpha(t)B(t)$ , where $\alpha(t) \in \SO,B(t) \in \skew$ , with arbitrary $B(0),\dot B(0) \in \skew$ . A naive attempt would be to define $\alpha(t)=e^{\int_0^t B(s)ds}$ for $B(s)=B(0)+s\dot B(0)$ . However, it is not true in general that $\alpha'(t)=\alpha(t)B(t)$ ; this happens if $B(t)$ , $\int_0^t B(s)ds$ commute , which happens if and only if $B(0),\dot B(0)$ commute. Proof $D = (\skew)^2+\skew$ for $n=2$ : $\alpha(t)$ can always be written as $\alpha(t)=\begin{pmatrix} c(\phi(t)) & s(\phi(t)) \\\ -s(\phi(t)) & c(\phi(t)) \end{pmatrix}$ , where $c(x)=\cos x,s(x)=\sin x$ , and $\phi(t)$ is some parametrization satisfying $\phi(0)=0$ . Differentiating $\alpha(t)$ twice, we get $$ \ddot \alpha(t)=-(\phi'(t))^2\alpha(t)+\phi''(t)\begin{pmatrix} -s(\phi(t)) & c(\phi(t)) \\\ -c(\phi(t)) & -s(\phi(t)) \end{pmatrix},$$ so $$ \ddot \alpha(0)=-(\phi'(0))^2Id+\phi''(0)\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}.$$ Since we can choose $\phi'(0),\phi''(0)$ as we wish, we conclude that $$ D=\mathbb{R}_{\le 0}Id+\mathbb{R}\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}=\mathbb{R}_{\le 0}Id+\skew.$$ Since $\skew=\text{span} \{ \begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}\}$ , and $\begin{pmatrix} 0 & 1 \\\ -1 & 0 \end{pmatrix}^2=-Id$ , we have $\skew^2=\mathbb{R}_{\le 0}Id$ , so indeed $D=(\skew)^2+\skew$ .","['orthogonal-matrices', 'differential-topology', 'lie-groups', 'differential-geometry']"
3050275,A Taylor theorem for Hölder continuous function?,"Let $ C^{m,s}_{b} $ be the space of bounded function $ u: \mathbb{R} \rightarrow \mathbb{R} $ which satisfies \begin{alignat*}{2}
\bigg| u^{(m)}(x) - u^{(m)}(y) \bigg| \leq C | x - y |^{s}.
\end{alignat*} I'm looking for a Taylor - type theorem which could bound the remainder of the following polynomial $$ u(x) = \sum_{k \geq N} \frac{(x-y)^{k}}{k!} u^{(k)}(y) + R_{N}(x) $$ by $$| R_{N}(x) | \leq |x-y|^{m+s}.$$ In the simple case that $ N=1 $ I know that $$ u(x) = u(y) + u'(y)(x-y) + \int_{0}^{1} \Big( u'\big(y + t(x-y) \big) - u'(y) \Big)dt (x-y) .$$ But I have yet been successful in finding the right expression for the arbitrary case. Could anyone please shed some light on the problem? Thanks in advance!","['holder-spaces', 'analysis', 'real-analysis']"
3050303,Evaluating the Cauchy product of $\sum_{n=0}^{\infty}\frac{(-1)^{n+1}}{n+1}$ and $\sum_{n=0}^{\infty}\frac{1}{3^n} $,"Using the Mertens' theorem for Cauchy products we know that the Cauchy product of series $$
\sum_{n=0}^{\infty}\frac{(-1)^{n+1}}{n+1}\qquad\text{and}\qquad\sum_{n=0}^{\infty}\frac{1}{3^n}
$$ does converge. But how can we try to find the value of this sum? What I only know is that we can write that this sum equals to $$
S=\sum_{n=0}^{\infty}c_n\qquad\text{where}\qquad c_n=\sum_{k=0}^{n}\frac{1}{3^{n-k}}\frac{(-1)^{k+1}}{k+1}
$$ but I'm not able to find any way to calculate the sum. Any hints?","['summation', 'sequences-and-series']"
3050353,"$A$ is an invertible $n\times n$ matrix, where $n$ is an even number. Given that $A^3+A=0$, calculate $\det(A^4)$. Is there too much information?","$A$ is an invertible matrix with $n$ columns and $n$ rows, where $n$ is an even number. We are given that $A^3+A=0$ and we need to calculate $\det(A^4)$ . Here is my solution: $$A^3+A=0 \implies A^{-1}(A^3+A)=0 \implies A^2=-I \implies A^4=I \implies \det(A^4)=1.$$ But I did not use the fact that n is even. Am I wrong, or this is not needed? If I'm wrong, please don't tell me the solution yet. Just tell me where I'm wrong. Thanks!",['linear-algebra']
3050358,Functions satisfying $f:\mathbb{N}\rightarrow\ \mathbb{N}$ and $f(f(n))+f(n+1)=n+2$,"Find all functions $f$ such that $f:\mathbb{N}\rightarrow\  \mathbb{N}$ and $f(f(n))+f(n+1)=n+2$ Let us plug in $n=1$ $f(f(1))+f(2)=3$ Since the function is from $\mathbb{N}$ to $\mathbb{N}$, $f(2)$ can only take the values $1,2$. Now we divide the problem into cases. Case-1: $f(f(1))=2,f(2)=1$ We can assume that $f(1)=c$ for the time being. Then plugging in $n=3$ and using $f(2)=1$ gives $$f(3)=4-c$$ and again since the range of the function is positive integers,then $4-c$ has to be positive and hence $c$ belongs to  {$1,2,3$}. Now, $$f(1)=c$$$$\implies f(f(1))=f(c)$$$$\implies 2=f(c)$$ by the assumption that $f(f(1))=2$ . Now,since $c$ can only take the values $1,2,3$,we start treating cases. If $c=1$,$$f(c)=2$$$$\implies f(1)=2$$ but we know from the deinition of $c$ that $f(1)=c=1$,a contradiction.If $c=2$,then $2=f(c)=f(2)$ but $f(2)=1$ by assumption. Finally,if $c=3$ $$2=f(c)=f(3)$$ but $$f(3)=4-c=4-3=1$$ which is once again a contradiction. Therefore there are no such functions in this case. Case-2: $f(f(1))=1,f(2)=2$ Again assuming $f(1)=c$ and using $f(n)\le n$ along with plugging $n=c-1$ will give us that $f(1)=1$ and then it is easy to prove that such a function exists by recursion. I can only give a ""sort of recursive"" way to define the function. Here it goes $$f:\mathbb{N}\rightarrow \mathbb{N}$$$$f(1)=1$$$$f(n)=n+1-f(f(n-1))$$ But this case is harder to deal with.Some help will be appreciated.","['contest-math', 'functional-equations', 'algebra-precalculus']"
3050362,Inverting the Laplacian,"I've had a hard time looking for literature on this, so here's my question: We take a look at the Laplacian $-\Delta$ as an unbounded operator on $\mathrm{L}^2(\mathbb{R}^3)$ . We know that $-\Delta$ is unitary equivalent to the multiplication operator with $|p|^2$ in Fourier space, so $-\Delta=\mathcal{F}^{-1} |p|^2 \mathcal{F}$ . So one could now use functional calculus and define an inverse Laplacian by setting $(-\Delta)^{-1}=\mathcal{F}^{-1} (1/|p|^2 )\mathcal{F}$ , which will also be unbounded of course. It is also known from theory of PDEs that one can invert the Laplacian on Schwartz functions using the Green's function $1/4\pi |x|$ , which is derived as a distributional Fourier transform of $1/|p|^2$ . So define $G$ on $\mathrm{L}^2(\mathbb{R}^3)$ by convolution with $1/4\pi |x|$ : $$(G\phi)(x)=\int \frac{\phi(y)}{4\pi |x-y|} dy$$ $G$ is also unbounded and coincides at least for the Schwartz functions with the above defined inverse Laplacian, $G\phi=(-\Delta)^{-1}\phi$ for all $\phi \in\mathcal{S}(\mathbb{R}^3)$ . Now my question is: Are both operators the same? Does $G=(-\Delta)^{-1}$ hold, i. e. is $D(G)=D((-\Delta)^{-1})$ and $G\phi=(-\Delta)^{-1}\phi$ for all $\phi \in D(G)=D((-\Delta)^{-1})$ ? My guess is that $\mathcal{S}(\mathbb{R}^3)$ is a core of $(-\Delta)^{-1}$ , and as $(-\Delta)^{-1}|_{\mathcal{S}(\mathbb{R}^3)}=G|_{\mathcal{S}(\mathbb{R}^3)}$ equality should follow by closing the restrictions. Any comments, hints on how to proceed or references are welcome! Thank you.","['operator-theory', 'functional-analysis', 'functional-calculus', 'partial-differential-equations']"
3050407,"If $(X^x_t)$ is the stochastic flow generated by a SDE and $(X_t)$ is the strong solution with $X_0=ξ$, is $X_t=X^ξ_t$ for all $t$ a.s.?","Let $(\Omega,\mathcal A,\operatorname P)$ be a complete probability space $(\mathcal F_t)_{t\ge0}$ be a complete and right-continuous filtration on $(\Omega,\mathcal A,\operatorname P)$ $\xi$ be an $\mathcal F_0$ -measurable square-integrable random variable on $(\Omega,\mathcal A,\operatorname P)$ $W$ be an $\mathcal F$ -Brownian motion on $(\Omega,\mathcal A,\operatorname P)$ $b,\sigma:[0,\infty)\times\mathbb R\to\mathbb R^d$ be Borel measurable with $$|b(t,x)|^2+|\sigma(t,x)|^2\le C_1(1+|x|^2)\;\;\;\text{for all }t\ge0\text{ and }x\in\mathbb R\tag1$$ for some $C_1\ge0$ and $$|b(t,x)-b(t,y)|^2+|\sigma(t,x)-\sigma(t,y)|^2\le C_2|x-y|^2\;\;\;\text{for all }t\ge0\text{ and }x,y\in\mathbb R\tag2$$ for some $C_2\ge0$ We know that there is a unique (up to indistinguishability) continuous $\mathcal F$ -adapted process $(X_t)_{t\ge0}$ with $$X_t=\xi+\int_0^tb(s,X_s)\:{\rm d}s+\int_0^t\sigma(s,X_s)\:{\rm d}W_s\;\;\;\text{for all }t\ge0\text{ almost surely}\tag3.$$ We say that $X$ is the pathwise unique strong solution of $${\rm d}X_t=b(t,X_t){\rm d}t+\sigma(t,X_t){\rm d}W_t\tag4$$ with initial condition $X_0=\xi$ . We observe that, if $Y$ is the pathwise unique strong solution of $(4)$ with initial condition $Y_0=\eta$ (for some $\mathcal F_0$ -measurable square-integrable random variable $\eta$ on $(\Omega,\mathcal A,\operatorname P)$ ), then $$\operatorname E\left[\sup_{s\in[0,\:t]}\left|X_s-Y_s\right|^2\right]\le\Lambda(t)\operatorname E\left[\left|\xi-\eta\right|^2\right]\;\;\;\text{for all }t\ge0\tag5$$ for some continuous nondecreasing $\Lambda:[0,\infty)\to[0,\infty)$ (which only depends on $C_2$ ). Thus, $$X_t=Y_t\;\;\;\text{for all }t\ge0\text{ almost surely on }\left\{\xi=\eta\right\}.\tag6$$ Now, let $(X^x_t)_{t\ge0}$ denote pathwise unique strong solution of $(4)$ with initial condition $X^x_0=x\in\mathbb R^d$ . We're able to assume that $$\Omega\times[0,t]\times\mathbb R\ni(\omega,s,x)\mapsto X_s^x(\omega)\tag7$$ is $\mathcal F_t\otimes\mathcal B([0,t])\times\mathcal B(\mathbb R)$ -measurable for all $t\ge0$ and $$(t,x)\mapsto X_t^x(\omega)\tag8$$ is (jointly) continuous for all $\omega\in\Omega$ . We easily obtain that $\left(X^\xi_t\right)_{t\ge0}$ is $\mathcal F$ -progressive. I want to conclude that $$X_t=X^\xi_t\;\;\;\text{for all }t\ge0\text{ almost surely .}\tag9$$ From $(6)$ we see that the claim is true as long as $|\xi(\Omega)|\le|\mathbb N|$ . In general, there is a $(\xi_n)_{n\in\mathbb N}$ with $\xi_n$ being an $\mathcal F_0$ -measurable random variable on $(\Omega,\mathcal A,\operatorname P)$ with $|\xi_n(\Omega)|\in\mathbb N$ for all $n\in\mathbb N$ , $$|\xi_n|\le|\xi|\;\;\;\text{for all }n\in\mathbb N\tag{10}$$ and $$|\xi_n-\xi|\xrightarrow{n\to\infty}0\tag{11}.$$ From $(10)$ , $(11)$ and the square-integrability of $\xi$ , we obtain $$\left\|\xi_n-\xi\right\|_{L^2(\operatorname P)}\xrightarrow{n\to\infty}0\tag{12}$$ and hence $$\operatorname E\left[\sup_{s\in[0,\:t]}\left|X^{\xi_n}_s-X_s\right|^2\right]\xrightarrow{n\to\infty}0\tag{13}\;\;\;\text{for all }t\ge0$$ from $(5)$ . On the other hand, by continuity of $(8)$ , we should have $$\sup_{s\in[0,\:t]}|X^{\xi_n}_s-X^\xi_s|\xrightarrow{n\to\infty}0\;\;\;\text{for all }t\ge0\tag{14}$$ and hence obtain $(9)$ by uniqueness (up to equality almost surely) of the limit in probability.","['stochastic-analysis', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory', 'stochastic-calculus']"
3050497,An exercise on the calculation of a function of operator,"The operator is given by $$A=\begin{pmatrix}
1 & 0 & 0\\
1 & 1 & 0\\
0 & 0 & 4
\end{pmatrix}$$ I have to write down the operator $$B=\tan(\frac{\pi} {4}A)$$ I calculate $$\mathcal{R} (z) =\frac{1}{z\mathbb{1}-A}=\begin{pmatrix} 
\frac{1}{z-1} & 0 & 0\\
\frac{1}{(z-1)^2} & \frac{1}{z-1} & 0\\
0 & 0 & \frac{1}{z-4}\end{pmatrix} $$ Now the B operator is given by: $$B=\begin{pmatrix}
Res_{z=1}\frac{\tan(\frac{\pi}{4}z)}{z-1} & 0 & 0\\
Res_{z=1}\frac{\tan(\frac{\pi}{4}z)}{(z-1)^2} & Res_{z=1}\frac{\tan(\frac{\pi}{4}z)}{z-1} & 0\\
0 & 0 & Res_{z=4}\frac{\tan(\frac{\pi}{4}z)}{z-4}
\end{pmatrix} $$ For me the result should be $$ B=\begin{pmatrix}
1 & 0 & 0\\
\frac{\pi}{2} & 1 & 0\\
0 & 0 & 0\end{pmatrix}$$ But the exercise gives as  solution: $$ B=\begin{pmatrix}
1 & 0 & 0\\
\frac{\pi}{4} & 1 & 0\\
0 & 0 & 1\end{pmatrix}$$ Where is the error?
Thank you and sorry for bad English","['matrix-calculus', 'operator-theory', 'functions', 'linear-algebra']"
3050498,"Solve the system $x^2(y+z)=1$ ,$y^2(z+x)=8$ and $z^2(x+y)=13$","Solve the system of equations in real numbers \begin{cases} x^2(y+z)=1 \\  y^2(z+x)=8 \\z^2(x+y)=13 \end{cases} My try: Equations can be written as: \begin{cases}\frac{1}{x}=xyz\left(\frac{1}{y}+\frac{1}{z}\right)\\
\frac{8}{y}=xyz\left(\frac{1}{x}+\frac{1}{z}\right)\\
\frac{13}{z}=xyz\left(\frac{1}{y}+\frac{1}{x}\right)\end{cases} Let $p=xyz.$ Then we have: \begin{cases}\frac{1}{x}-\frac{p}{y}-\frac{p}{z}=0\\
\frac{p}{x}-\frac{8}{y}+\frac{p}{z}=0\\
\frac{p}{x}+\frac{p}{y}-\frac{13}{z}=0\end{cases} Then we get $$\frac{p+1}{x}=\frac{p+8}{y}=\frac{p+13}{z}$$ Any clue here?","['algebra-precalculus', 'systems-of-equations', 'symmetric-polynomials']"
3050510,Evaluate $\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$,$\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$ What are the good/ clever methods to evaluate this limit? I tried taking $\tan^{-1} (x+5) = \theta$ to avoid inverse functions but its not helpful and makes it even more complicated. I also tried $\tan^{-1}a - \tan^{-1}b$ formula for the terms attached to x but that does not help to get rid of other terms multiplied by $1$ and $5$ . Edit: (Please address this in your answer) Can't we directly do this: $\lim_{x\to \infty} (x+5)\tan^{-1}(x+5)- (x+1)\tan^{-1}(x+1)$ $= (x+5)\dfrac{\pi}{2} - (x+1)\dfrac{\pi}{2}$ $ = \dfrac {5\pi - \pi}{2} = 2\pi$ I don't see anything wrong with it and it gives the right answer. Is this method correct? Can it be used in other questions too?,['limits']
3050619,Switching order of integration on unbounded domain,"Let's say ${f\left( {x,y} \right)}$ is a continuous function and assume that: $\int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dx} $ converges for all $y  $ 2) $\int\limits_{ - \infty }^\infty  {\left| {f\left( {x,y} \right)} \right|dy} $ converges for all $x $ Is the following statement correct? $$\int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dxdy} }  = \int\limits_{ - \infty }^\infty  {\int\limits_{ - \infty }^\infty  {f\left( {x,y} \right)dydx} } $$ This is supposed to be some version of Fubini's or Tonelli's theorem but I wasn't able to find the exact version for this case.",['multivariable-calculus']
3050628,How do I solve $\lim \limits_{x \to \frac{π}{3}} \frac{2 \sin x - \sqrt{3}}{\cos \frac{3x}{2}}$,"Alright, I know, there are easier ways to solve this, like L'hopitals Rule etc. But I'm not solving it for the answer, just doing it for the fun so I tried using substitution method. Put $t= x- \dfrac{π}{3}$ $\lim \limits_{x \to \frac{π}{3}} \dfrac{2 \sin x - \sqrt{3}}{\cos \frac{3x}{2}}$ $= \lim\limits_{t \to 0} \dfrac{2 \sin \left(t+\frac{π}{3} \right) - \sqrt{3}}{\cos \left( \frac{3t}{2} + \frac{π}{2}\right)}$ $= \lim\limits_{t \to 0} \dfrac{\sqrt{3}- \sin t - \sqrt{3} \cos t}{ \sin \frac{3t}{2}}$ Where do I go from here, I'm not able to eliminate the $t$ fully from the Nr and Dr, any help? Or any other alternative way that uses only the fact that $\lim\limits_{x \to 0} \dfrac{ \sin x}{x} = 1$ ? Thanks :)","['limits', 'limits-without-lhopital']"
3050655,Proving a quartic polynomial irreducible by Eisenstein (or other ways),"Please help me to prove that these polynomials are irreducible over $\mathbb{Q}$ : 1) $x^4+5x^3+7x^2-6$ 2) $x^4-4x^3+8x^2-8x+9$ Unfortunately , it takes infinite amount of time to prove that given polynomials are irreducible according to Eisenstein's criterion. So could you recommend and show other ways, that requires less time , because such exercises will come in final exam where I should solve a problems in a short time.","['irreducible-polynomials', 'ring-theory', 'abstract-algebra', 'factoring', 'polynomials']"
3050671,"$f$ integrable on $A \times B$, show subset of $A$ is negligible","$A \subset \mathbb R^m$ and $B \subset \mathbb R^n$ are boxes, and $f$ is integrable in the riemann sense on $(A\times B)$ . We define $$A_1 =\left\{a \in A \left|\exists \int_{B}f(a,b)db\right.\right\} \subset A.$$ Show that $A \setminus A_1$ has zero volume.","['integration', 'measure-theory']"
3050682,Volume of an Irregular Octahedron from edge lengths?,"Does anyone know how to calculate the volume of an irregular octahedron from the lengths of the edges? The octahedron has triangular faces, but the only information are the edge lengths. Alternatively, how might I calculate the length of a line drawn ( the yellow dashed line) between two opposing vertices? If this known it can be split into 4 irregular tetrahedra. In the Image the different edge lengths are coour coded, i.e all green edges are length x, blue edges are length y...","['quadrilateral', 'polyhedra', 'geometry', 'volume']"
3050694,Curvature of projection function onto smooth curve,"Suppose we have a smooth curve $C$ lying in $\mathbb{R}^2$ , and let us consider the orthogonal projection function $P_C(x)$ onto the curve, described by $$P_C(x) = \arg\min_{y \in C} \Vert x - y \Vert$$ where $\Vert \cdot \Vert$ is a norm, it can be $\Vert \cdot \Vert_2^2$ , or $\Vert \cdot \Vert_1$ . My question is: is there a general relationship between the second derivative of $P_C(x)$ and the curvature of the curve $C$ ? For example, relationship between the norm, whether it is ""positive definite"", etc. If no, under what restrictions on the curve $C$ and/or location of $x$ can we say something about their relationships? Does there exist work that discusses this problem or some problems related to it? To visualize the problem somewhat, we consider the picture below: Denoting the blue curve as $C_1$ and black curve as $C_2$ , $C_1$ clearly has greater curvature than $C_2$ , but what about $\Vert D^2P_{C_1}(x) \Vert$ vs. $\Vert D^2P_{C_2}(x) \Vert$ ?","['convex-optimization', 'reference-request', 'optimization', 'convex-analysis', 'differential-geometry']"
3050696,Proving that $\int_0^1 \frac{\arctan x}{x}\ln\left(\frac{1+x^2}{(1-x)^2}\right)dx=\frac{\pi^3}{16}$,"The following integral was proposed by Cornel Ioan Valean and appeared as Problem $12054$ in the American Mathematical Monthly earlier this year. Prove $$\int_0^1 \frac{\arctan x}{x}\ln\left(\frac{1+x^2}{(1-x)^2}\right)dx=\frac{\pi^3}{16}$$ I had small tries for it, such as writting: $$I=\int_0^1 \frac{\arctan x}{x}\ln\left(\frac{1+x^2}{(1-x)^2}\right)dx\overset{ x\to \tan \frac{x}{2}}=-\frac12 {\int_0^\frac{\pi}{2}\frac{x\ln(1-\sin x)}{\sin x} dx}$$ And with Feynman's trick we obtain: $$J(t)=\int_0^\frac{\pi}{2} \frac{x\ln(1-t\sin x)}{\sin x}dx\Rightarrow J'(t)=\int_0^\frac{\pi}{2} \frac{x}{1-t\sin x}dx$$ But I don't see a way to obtain a closed from for the above integral. Also from here we have the following relation: $$\int_0^1 \frac{\arctan x \ln(1+x^2)}{x} dx =\frac23 \int_0^1 \frac{\arctan x \ln(1+x)}{x}dx$$ Thus we can rewrite the integral as: $$I=\frac23 \int_0^1 \frac{\arctan x \ln(1+x)}{x}dx -2\int_0^1 \frac{\arctan x \ln(1-x)}{x}dx$$ Another option might be to rewrite: $$\ln\left(\frac{1+x^2}{(1-x)^2}\right)= \ln\left(\frac{1+x}{1-x}\right)+\ln\left(\frac{1+x^2}{1-x^2}\right)$$ $$\Rightarrow I= \int_0^1 \frac{\arctan x}{x}\ln\left(\frac{1+x}{1-x}\right)dx+\int_0^1 \frac{\arctan x}{x}\ln\left(\frac{1+x^2}{1-x^2}\right)dx$$ And now to use the power expansion of the log functions to obtain: $$\small I=\sum_{n=0}^\infty \frac{2}{2n+1}\int_0^1 \frac{\arctan x}{x} \, \left(x^{2n+1}+x^{4n+2}\right)dx=\sum_{n=0}^\infty \frac{2}{2n+1}\int_0^1\int_0^1 \frac{\left(x^{2n+1}+x^{4n+2}\right)}{1+y^2x^2}dydx$$ This seems like an awesome integral and I would like to learn more so I am searching for more approaches.
Would any of you who also already solved it and submitted the answer to the AMM or know how to solve this integral kindly share the solution here? Edit: In the meantime I found a nice solution by Roberto Tauraso here and another impressive approach due to Yaghoub Sharifi here .","['integration', 'definite-integrals', 'logarithms', 'harmonic-numbers', 'closed-form']"
3050705,How To Solve a Trigonometric Differential Equation,"Salutations, I have been trying to approach an ODE with trigonometric functions that I found interesting: $$y'+x\sin(2y)=xe^{-x^2}\cos^2(y)$$ I tried to find a result with wolfram web page (free version) and I got this one: $$y=\arctan\left(\frac{1}{2}e^{-x^2}(c+x^2)\right)$$ I have tried to approach this exercise by substitution of variables, also separable variables and I have not had luck by power series, 
and I do not know if methods like those of Ricatti and Bernoulli are appropriate for this case. This is just for academic curiosity and I would like to understand better this kind of ODEs. So, I require any guidance or starting steps or explanations about how to approach this kind of exercises. Thanks for your attention.","['proof-explanation', 'trigonometry', 'ordinary-differential-equations']"
3050739,Non cyclic group of order $p^3$ satisfies $G \simeq H \rtimes_{\theta}K$,"Let $G$ be a non-cyclic group of order $p^3$ for an odd prime $p$ . Prove that $G \simeq H \rtimes_{\theta}K$ , where $H$ is a normal subgroup of $G$ of order $p^2$ , $K$ is a subgroup of order $p$ , and $\theta : K \to Aut(H)$ is a homomorphism. I managed to prove that there exists a normal subgroup $H$ of order $p^2$ . Then I took some $g \in G-H$ . If $g$ is of order $p$ , I am done since $G \simeq H \rtimes \langle g \rangle $ . But what if all $g \in G - H$ are of order $p^2 $ ?","['semidirect-product', 'group-theory', 'abstract-algebra', 'finite-groups']"
3050743,"If for some $n\in\mathbb{N}$, $T^{n+1}\left( X\right) =T^{n}\left( X\right)$ do we have $T^{n}\left(X\right) $ closed?","Let $X$ be a Banach space, and let $T$ be a bounded operator on $X$ such
that for some $n\in 
%TCIMACRO{\U{2115} }%
%BeginExpansion
\mathbb{N}
%EndExpansion
$ , $T^{n+1}\left( X\right) =T^{n}\left( X\right) $ . Do we have $T^{n}\left( X\right) $ closed ?","['banach-spaces', 'operator-theory', 'linear-algebra', 'functional-analysis']"
3050758,How to move forward in Mathematics Studies with poor foundation,"So I discovered halfway through my first year university math term that my foundation of mathematics is very poor. I would memorize stuff rather than understand it and still get 95+ in high school so that's what I did. I quickly realized that doesn't work at all in university. It's embarrassing but I didn't really even know what a derivative really meant and I'm in University Math! Anyways, I passed my math courses barely with 60-70, and did way better on the finals than I did on my midterms which is a sign that I'm heading in the right direction (I went back and looked at a whole bunch of khan academy high school math concepts) but it was hard to juggle doing that and keeping up with current stuff in class. So now the term is over and I want to prepare for next term. My question is, should I go back and review basically all the high school stuff I memorized (and forgot), or should I just start studying the new material by reading the textbook ahead and actually understanding the concepts through practice this time? (The 2 math classes I had this previous term was Calculus 1 and Algebra/Proofs, and my next 2 classes are Linear Algebra and Calculus 2) Thanks!","['calculus', 'education']"
3050775,"Do ""$K/k$ twisted"" representations exist?","Given $k$ -representations $V,W$ of a group $G$ , where $k$ is a field, $K/k$ a field extension, if we have $V\otimes_k K\cong W\otimes_k K$ as $K$ -representations, do we have that $V\cong W$ ? Being more specific, what about in the case of $V,W$ irreps, $G$ a finite group, with $K/k$ finite and galois? In characteristic $0$ , with character theory, the question can be rephrased as: if the characters of $V$ and $W$ agree, then are $V$ and $W$ isomorphic over their field of definition? Any reference for these questions would be much appreciated.","['extension-field', 'characters', 'abstract-algebra', 'representation-theory']"
3050783,"How do you show that, for any integer, there is a triangle with side rational lengths and that integer area?","In Cohen's book, Cohen's Number Theory Volume 1 , the first exercise is to show that, for any integer, there is a triangle with side rational lengths such that the triangle has that integer as an area. For example, What are the side rational lengths for an area 2 triangle? Given Heron's formula for a triangle of area $2$ , $$\sqrt{\frac{(a+b+c)(a+b-c)(a-b+c)(-a+b+c)}{16}}=2$$ How do we find the sides for a side rational triangle $(a,b,c)$ that satisfies this equation? Another example, (9,10,17)/6 has area 1, and so on for each integer. Looking for the method for solving the exercise, not necessarily a compendium of known triples with integer areas.","['number-theory', 'triangles', 'geometry', 'diophantine-equations']"
3050784,Transformations on function curve,"If we have a function $f$ such that $f:R\to R^+$ where $f(x) = 3^{-x}$ , it's graph will be like that And we have another function $g$ such that $g: R\to R^+$ where $g(x) = f(x+1) = 3^{-x+1} = 3^{1-x}$ And because $g(x) = f(x+1)$ so the curve of $g$ is an image of the curve of $f$ by translation 1 unit to the left But it turns out that the translation is 1 unit to the right and not to the left The graph of $f(x+1)$ is supposed to be the same as that of $f(x)$ by translation towards the negative part of x-axis.Why isn't that the case here?","['algebra-precalculus', 'functions', 'exponential-function', 'graphing-functions']"
3050844,Evaluate $\int_{0}^{\frac{\pi}{2}} x\ln(\tan(x))dx $ [duplicate],"This question already has answers here : How to evaluate $\int_{0}^{\pi }\theta \ln\tan\frac{\theta }{2} \, \mathrm{d}\theta$ (6 answers) Closed 5 years ago . How do I evaluate: $$\int_{0}^{\frac{\pi}{2}} x\ln(\tan(x))dx $$ I know it equals $\frac{7\zeta(3)}{8}$ , but I'm not sure how to get there. I've been trying to convert it into a sum somehow, but I'm not getting anywhere. Equivalent integrals are: $$\frac{1}{2}\int_{-\infty}^\infty x\operatorname{sech}(x)\arctan(e^x)dx$$ $$\int_0^\infty \frac{\arctan(x)\ln(x)}{1+x^2} dx$$ I've tried differentiating under the integral but to no avail.","['integration', 'calculus']"
3050869,Quantifying how crowded points are in $d$ dimensional cube,"Let $x_1, \cdots, x_n$ be distinct points in the $d$ dimensional cube $[-1,1]^d$ . What is a lower bound on the quantity: $$ \sum_{1 \le j < k \le n} \frac{1}{||x_k-x_j||}$$ where $|| \cdot ||$ is the Euclidean norm. An asymptotic answer for large $n$ is also fine and bounds on any quantity that looks similar, such as replacing the norm with norm squared, is also welcomed. My motivation is a problem that appeared in the book The Cauchy-Schwarz Master Class which proved the following: If $-1 \le x_1 < x_2 < \cdots < x_n \le 1$ then $$  \sum_{1 \le j < k \le n} \frac{1}{x_k-x_j}  \ge \frac{n^2 \log n}8.$$","['inequality', 'geometry', 'upper-lower-bounds']"
3050894,Can $f^{(\infty)}(a)=0$ for almost all $a$?,"My question is: Does there exist an infinitely differentiable function $f$ such that $$\lim_{n\to\infty} f^{(n)}(a)=0\qquad{\text{for almost all } a\in[0,\infty)}$$ ? ( $f$ cannot be a constant function or a polynomial.) If we restrict us to $C^{\omega}$ (i.e. assuming $f$ is holomorphic), it is likely that the answer is no, since $$f^{(n)}(a)=\frac{n!}{2\pi i}\oint_{\gamma}\frac{f(z)}{(z-a)^{n+1}}dz$$ the integrand only decays exponentially in $n$ while there is a $n!$ factor there. However, I am not quite sure if the above argument is correct. For smooth functions, I have no ideas. Any help will be appreciated. Thanks in advance.","['complex-analysis', 'derivatives', 'real-analysis']"
3050901,"If $f(x+a)-f(x)$ is differentiable for each $a$, then $f$ is differentiable","$ f \in C(\mathbf{R})$ , if for each real $a$ , $f(x+a)-f(x)$ is differentiable, then $f$ is differentiable. It seems hard to convert difference to the original function","['derivatives', 'real-analysis']"
3050947,"Probability of choosing a biased coin $C$ which has probability $3/15$ of getting heads, assuming we got head on the first toss","Full question: there are 3 biased coins $A$ , $B$ , and $C$ each with probability $5/15, 3/15, 1/15$ of getting heads respectively. Also, they have probability $1/4$ for $A$ , $1/4$ for $B$ , and $1/2$ for $C$ of getting picked. If a coin was picked and tossed and the result was heads, what is the probability that the coin was coin $C$ ? My approach: Since the coin picked was $C$ and the result was head we merely multiply  the probability of both those things happening concerning $C$ : $$\frac1{15} \cdot \frac12 = \frac1{30}$$","['conditional-probability', 'discrete-mathematics', 'probability']"
3050956,Showing that Sobolev norms on manifolds are equivalent,"Let me first define a ""Sobolev space on manifold"". Let $M$ be a closed $n$ -dimensional manifold, $E \rightarrow M$ a complex vector bundle. Let us pick: A finite cover of $M$ by sets $U_i$ . charts $h_i:U_i \cong \Bbb R^n$ . Trivilizations $\phi_i$ of $E|_{U_i}$ $\mu_i$ particion of unity of subordinate to $\{U_i\}$ . Define the Sobolev norm of a section $u \in \Gamma(M,E)$ by $$ ||u||_k^2 := \sum_i ||(\mu_i \circ h_i^{-1}) (\phi_i \circ u \circ h_i^{-1} ) ||_k^2$$ this is well defined, the RHS being a fintie sum of Sobolev $k$ -norm of compactly supported functions on $\Bbb R^n$ . So I want to show The equivalence class of $|| \cdot ||_k$ is independent of the choices made. What I know: Result 1: Let $a \in C^\infty_c$ . Then $f \mapsto af$ extends to a bounded operator $M_a:W^s \rightarrow W^s$ for each $s \in \Bbb Z$ . $$||au||_s \le C(a)||u||_s$$ Result 2: Let $\phi:U' \rightarrow V'$ be a diffeomoprhism of open subsets of $\Bbb R^n$ with $U \subseteq U'$ and $V= \phi(U) \subseteq V'$ be relatively compact. Then $u \mapsto u\circ \phi$ extends to a bounded map  for all $s \in \Bbb Z$ . $$W^s (V) \rightarrow W^s(U) $$ Thoughts so far: Edit: I believe we start by proving 4. Let us see first vary the partition of unity, with $\tau:= \{ \tau_i \}$ .  So that $$ \tau _j  = \sum_i \tau_j \mu_i $$ \begin{align*}
 || (\tau_j \circ h_j^{-1}) (\phi_j \circ u \circ h_j^{-1}) ||_k^2 & \le \sum _i || (\tau_j \circ h_j^{-1}) (\mu_i \circ h_j^{-1})  (\phi_j \circ u \circ h_j^{-1}) ||_k^2  \\ 
 & \le C(\tau) ||u||_{k}^2
\end{align*} constant $C(\tau)$ dependent on partition. This uses Result 1 .  Then if we take a another cover $\{V_j\}$ . From independence of 4, we may choose a partition wrt $U_i \cap V_j$ . Using Result 2 , we take care of 2. Now I am stuck at addressing 3. This post should be pretty much self contained, but for those who might find it helpful in consulting original source, I am concered with Lemmea 3.6.2, pg 47.","['real-analysis', 'sobolev-spaces', 'functional-analysis', 'differential-topology', 'differential-geometry']"
3050962,How to prove the existence and uniqueness of the solution of a second order linear ODE?,"Considering homogeneous, linear, second order differential equation $$y'' + p(x)y' + q(x)y = r(x), a<x<b$$ $p, q, r$ are given. We already know $y(a) = A, y(b) = B$ and $q(x) < o$ . How to prove that this equation has unique solution in $[a,b]$ if it has a solution?","['analysis', 'ordinary-differential-equations']"
3050984,Does it make sense to learn mathematical concepts as you encounter them rather than in a fixed progression? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 5 years ago . Improve this question I understand the fundamentals of algrebra, but have very limited knowledge of geometry and trigonometry. I wish to learn calculus at this point. Is it reasonable to begin learning calculus, and learn these other concepts as I encounter them rather than learning the prerequisites in the standard fixed progression (i.e. Algebra I -> Algebra 2 -> Geometry -> Trigonometry, etc)? How difficult will it be to learn these concepts without the prescribed linear progression?","['calculus', 'soft-question', 'algebra-precalculus', 'ordinary-differential-equations']"
3050996,Limit of $S_n$ as $n \to \infty$,"Let $$S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx$$ for $n >0$ . Then as $n \to \infty$ , the sequence $(S_n)_{n>0}$ tends to $0$ $1/2$ $1$ $+\infty$ $$S_n = \int_{0}^{1} \frac{nx^{n-1}}{1+x}dx$$ put $x= \tan^{2}t$ $dx = 2 \tan t \sec^{2} t dt$ so, $$S_n = \int_{0}^{\pi/4} \frac{n \tan^{2n-2}t}{1+\tan^{2} t} 2 \tan t \sec^{2}t dt$$ $\int_{0}^{\pi/4} 2n \tan^{2n-1}t dt$ $2n\int_{0}^{\pi/4} \tan^{2n-1}t dt$ I don't know how to proceed further to find the limit.
Is there any other method to find the limit?",['limits']
3051030,Zeroes of some degree of two elliptic functions,"Let $\tau \in {\mathbb C}$ with $\mathrm{Im} \tau > 0$ , $a,b \in {\mathbb Q}$ not both integers (it's not clear to me whether assuming only $a,b \in {\mathbb R}$ will make a difference to the question or not), $\Lambda \subset {\mathbb C}$ the lattice generated by $1$ and $\tau$ and $\eta_1$ , $\eta_2$ the quasi-periods of the Weierstrass $\zeta$ function $-\int \wp$ corresponding to $\Lambda$ ( see e.g. https://en.wikipedia.org/wiki/Weierstrass_functions ), i.e. the values of the Weierstrass eta function at $1$ and $\tau$ . While doing some work in algebraic geometry, I've come across the following degree $2$ elliptic function \begin{equation*}
\Upsilon(z) = \frac{1}{z(z-a-b\tau)} + \sum_{\omega \in \Lambda \backslash \{0\}} \left[ \frac{1}{(z-\omega)(z-\omega-a-b\tau)} - \frac{1}{\omega^2} \right] - \frac{a \eta_1 + b \eta_2}{a+ b \tau}
\end{equation*} Yes, this is quite similar to $\wp$ . Since I know nothing about complex analysis, I'd appreaciate any help ""identifying"" this function -- Does it have a name, can it be expressed in particularly simple way in terms of other functions. Has it appeared anywhere else? It's extremely possible that I'm missing something simple. What I actually need to know about this function is related to its zeroes. Clearly, it has poles at $0$ and $a+b\tau$ , so we know the sum of the zeroes. Can its zeroes be computed? Is it easier than for $\wp$ ? http://people.mpim-bonn.mpg.de/zagier/files/doi/10.1007/BF01453974/fulltext.pdf (I'm under the impression that $\sum_{n \geq 0} \frac{1}{(an+b)^2}$ is harder than the other $\sum_{n \geq 0} \frac{1}{(an+b)(an+c)}$ , so maybe that's not so unlikely.) P.S. If anyone is curious, the algebraic geometry calculation I was doing was taking place on a geometrically ruled surface over an elliptic curve $E$ , specifically, the projectivization of the rank two bundle ${\mathcal E}$ which fits in a nonsplit s.e.s. $0 \to {\mathcal O}_E \to {\mathcal E} \to {\mathcal O}_E \to 0$ .","['complex-analysis', 'elliptic-functions', 'algebraic-geometry']"
3051049,Extract the Square Root of $ a^2-\left(\frac{\left(3a \sqrt{a}\right)}{2}\right)-\left(\frac{\left(3 \sqrt{a}\right)}{2}\right)+\frac{41a}{16}+1 $,Extract the Square Root of $$ a^2-\left(\frac{\left(3a \sqrt{a}\right)}{2}\right)-\left(\frac{\left(3 \sqrt{a}\right)}{2}\right)+\frac{41a}{16}+1 $$ I found a method to do this in a book but couldn't understand how to do it. Can anyone explain it?,"['algebra-precalculus', 'polynomials']"
3051071,Continuous vs. Discrete State-Space Model,"Time-invariant continuous model: $\dot{x}(t) = Ax(t)+Bu(t)$ $y(t) = Cx(t)+Du(t)$ Time-invariant discrete model: $x_{k+1} = Ax_{k}+Bu_{k}$ $y_{k} = Cx_{k}+Du_{k}$ Why does the continuous model result in a rate of change $\dot{x}$ , while the discrete model results in a new state $x_{k+1}$ ?","['control-theory', 'ordinary-differential-equations']"
3051093,Finding the coefficient of $x^2$ in $\tiny{\left(\left(\left(\left(x-2\right)^2-2\right)^2-2\right)^2-\cdots-2\right)^2}$,"Find the coefficient of $x^2$ in the expansion of $$\underbrace{\left(\left(\left(\left(x-2\right)^2 -2\right)^2 -2\right)^2 -\cdots  -2\right)^2}_{k\;\text{times}}$$ I tried to equate it to a polynomial of the form $$\underbrace{P_k x^3}_{\text{Terms of power}\geq3} + \underbrace{B_kx^2}_{\text{Terms of power=2}} + \underbrace{A_k x} _{\text{Terms of power=1}} +C$$ So we can write $$\underbrace{\left(\left(\left(\left(x-2\right)^2 -2\right)^2 -2\right)^2 -\cdots  -2\right)^2}_{k\;\text{times}} = P_k x^3 + B_kx^2 + A_k x +C$$ We can find $C$ easily if we simply set $x=0$ in the original equation and we get, $$\underbrace{\left(\left(\left(\left(0-2\right)^2 -2\right)^2 -2\right)^2 -\cdots -2\right)^2}_{k\;\text{times}} = \underbrace{\left(\ldots \left(\left(4-2\right)^2 -2\right)^2-\cdots-\cdots 2\right)^2}_{k-1 \;\text{times}}$$ Simplifying till the end we get, $$C=4$$ So we get, $$\underbrace{\left(\left(\left(\left(x-2\right)^2 -2\right)^2 -2\right)^2 -\ldots  -2\right)^2}_{k\;\text{times}} = P_k x^3 + B_kx^2 + A_k x +4$$ Not sure where to go from here. EDIT: I think I have found a solution to develop the recursion, please tell me if it is right. $$P_k x^3 +B_kx^2 + A_k x +C=\underbrace{\left(\left(\left(\left(x-2\right)^2 -2\right)^2 -2\right)^2 -\cdots  -2\right)^2}_{k\;\text{times}}$$ Now, we can also write it as $$P_k x^3 +B_kx^2 + A_k x +C=\left[\underbrace{\left(\ldots \left(\left(x-2\right)^2-2\right)^2 \ldots -2\right)^2}_{k-1\;\text{times}} -2\right]^2$$ Which is $$P_k x^3 +B_kx^2 + A_k x +C=\left[\left(P_{k-1}x^3 + B_{k-1}x^2 + A_{k-1} x+ 4\right)-2\right]^2$$ $$P_k x^3 +B_kx^2 + A_k x +C=\left(P_{k-1}x^3 + B_{k-1}x^2 + A_{k-1} x+ 2\right)^2$$ So, we get, $$P_k x^3 +B_kx^2 + A_k x +C=\left(P^2_{k-1}x^6+ 2P_{k-1}B{k-1}x^5 + \left(2P_{k-1}A_{k-1}B^2_{k-1}\right)x^4 + \left(4P_{k-1} + 2B{k-1}A{k-1}\right)x^3 + + \left(4B_{k-1} + A^2_{k-1}\right)x^2 + 4A_{k-1}x + 4\right)$$ Thus, we get, $$A_k = 4A_{k-1}$$ And, $$B_k = A^2_{k-1} + 4B_{k-1}$$ Since $\left(x-2\right)^2 = x^2 - 4x + 4$ we have $A_1 = -4$ and similarly $A_2 = -4\cdot4=-4^2$ and in general $$A_k=-4^k$$ Now, we can use the relation we have for $B_k = A^2_{k-1} + 4B_{k-1}$ Writing this as $$B_k = A^2 _{k-1} + 4B_{k-1} = Ak^2 _{k-1} + 4\left(A^2_{k-2} + 4B_{k-2}\right)$$ $$B_k = A^2_{k-1} + 4A^2_{k-2} + 4^2\left(A_{k-3}^2 + 4B_{k-3}\right)$$ So, $$B_k = A^2_{k-1} + 4A^2_{k-2} + 4^2A^2_{k-3} + \ldots 4^{k-2}A_1^2 + 4^{k-1}B_1$$ Then we can substitute, $B_1 = 1, A_1 = 4, A_2 = -4^2, A_3 = -4^3, \ldots A_{k-1}= -4^{k-1}$ We get, $$B_k = 4^{2k-2} + 4\cdot4^{2k-4} + 4^2\cdot4^{2k-6} + \ldots + 4^{k-2}\cdot4^2 + 4^{k-1}\cdot 1$$ $$B_k = 4^{2k-2} + 4^{2k-3} + 4^{2k-4} + \ldots 4^{k+1} + 4^k + 4^{k-1}$$ $$B_k = 4^{k-1}\left(1 + 4+4^2+4^3 + \ldots  4^{k-2} + 4^{k-1}\right)$$ $$B_k = 4^{k-1} \cdot \frac{4^k - 1}{4-1} = \frac{4^{2k-1} - 4^{k-1}}{3}$$ This is how I have solved it but I am wondering if there is a nicer solution.","['algebra-precalculus', 'polynomials']"
3051109,How to show that $\sum_{n=1}^{\infty}\frac{\phi^{2n}}{n^2{2n \choose n}}=\frac{9}{50}\pi^2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Given: $$\sum_{n=1}^{\infty}\frac{\phi^{2n}}{n^2{2n \choose n}}=\frac{9}{50}\pi^2$$ Where $\phi=\frac{\sqrt{5}+1}{2}$ How can I we show that the above sum is correct?
I have checked numerically, it seem correct, but i don't how to proves it","['golden-ratio', 'sequences-and-series']"
3051148,"Solving $y^2 = 4x^3 - p$, with prime $p \equiv 7 (\text{mod } 8)$","I'm trying to find integer solutions to equations of the form $$y^2 = 4x^3 - p \tag{1}$$ where $p$ is a prime and $p \equiv 7 (\text{mod } 8)$ . 1) Is there a simple way to check if solutions do not exist for a given $p$ ? 2) Is there a computationally efficient way to find at least one solution? or maybe for a subset of $p$ by assuming some additional property? Eventually I'd like to solve for some large $p$ ( > 1000 bits). I do not know if this can be done efficiently, but I'm starting with smaller $p$ to try to understand properties of the equation better. For reasonable sized values, I can use magma to test out some values of $p$ . I do this by noting that if there is an integer solution to $Y^2 = X^3 - 16p$ with $X$ a multiple of 4, then I have solved the original equation. This has helped me see that sometimes there are no solutions, but I haven't figured out if there is a simple way to determine when this occurs.","['number-theory', 'mordell-curves', 'elliptic-curves', 'diophantine-equations']"
3051220,Chern class cohomology coefficients complex/real/integral?,"I am reading Chern classes from Kobayashi and Nomizu. Given a vector bundle $\pi:E\rightarrow M$ with fibre $\mathbb{C}^r$ and Group $GL(r,\mathbb{C})$ they associate for each $k\leq r$ a cohomology class of $M$ and call it the $k$ -th Chern class. It looks like cohomology is $H^*(M,\mathbb{C})$ and not $H^*(M,\mathbb{R})$ . Can some one clarify what is happening here? Given $\pi:E\rightarrow M$ , let $p:P\rightarrow M$ be the associated $GL(r,\mathbb{C})$ bundle.  We have $$\text{det}\left(\lambda I-\frac{1}{2\pi\sqrt{-1}}X\right)=\sum_{k=0}^rf_k(X)\lambda^{r-k}$$ for $X\in \mathfrak{gl}(r,\mathbb{C})$ . 
Here $f_k:\mathfrak{gl}(r,\mathbb{C})\rightarrow \mathbb{C}$ are $GL(r,\mathbb{C})$ invariant degree $k$ homogeneous poly. on $\mathfrak{gl}(r,\mathbb{C})$ . These $f_k$ can be seen as $GL(r,\mathbb{C})$ invariant symmetric multilinear map $$\underbrace{\mathfrak{gl}(r,\mathbb{C})\times\cdots\times\mathfrak{gl}(r,\mathbb{C})}_{k-\text{times}}\rightarrow \mathbb{C}$$ giving an element of $I_{\mathbb{C}}(G)$ . After fixing a connection on the principal bundle $P(M,G)$ , there is a complex valued version of Weil homomorphism $I_{\mathbb{C}}(G)\rightarrow H^*(M,\mathbb{C})$ . These $f_k\in I^k_{\mathbb{C}}(G)$ gives an element $c_k$ in $H^{2k}(M,\mathbb{C})$ . But, they write $c_k\in H^{2k}(M,\mathbb{R})$ . What am I missing here? Does it mean $c_k\in H^{2k}(M,\mathbb{C})$ is image of some element in $H^{2k}(M,\mathbb{R})$ under some  map $H^{2k}(M,\mathbb{R})\rightarrow H^{2k}(M,\mathbb{C})$ ? One which  induce from $\mathbb{R}\rightarrow \mathbb{C}$ defined as $a\mapsto a+i 0$ ? EDIT : The book Calculus to cohomology by Ib Madsen and Jxrgen Tornehave says in remark $18.12$ (page $189$ ) that Definition $18.3$ (of Chern class) gives cohomology classes in $H^*(M,\mathbb{C})$ , but actually all classes lies in Real cohomology . This follows from (some result before). There was no clear explanation (for me) for that comment. EDIT : User Jessica L (last seen 7 years ago) said Chern classes can be defined by topological means (see Milnor's book on characteristic classes), which yields elements $c_k(V) \in H^{2k}(M;\mathbb{Z})$ .  The normalization in the Chern-Weil theory is chosen so that the associated elements of de Rham cohomology groups $H^{2k}(M;\mathbb{R})$ agree with the integral elements, and thus integrate to give integers. I think this answer and my question are related. So, any reference (which contains more details) for this  are also welcome. EDIT : Kobayashi and Nomizu (Foundations of Differential geometry) in page $59$ says the following. Let $P(M,G)$ be a principal fibre bundle over a paracompact manifold $M$ with group $G$ which is a connected Lie group. It is known that $G$ is diffeomorphic with a direct product of any of its maximal compact subgroups $H$ and a Euclidean space (Iwasawa). By the same reasoning as above the structure group cann be reduced to $H$ . See that $GL(r,\mathbb{C})$ is a connected Lie group, $U(r,\mathbb{C})$ is a maximal compact group and this says that $GL_r(\Bbb C) \cong U(r) \times \Bbb R^{r^2}$ . Using above result, we see that structure group $GL(r,\mathbb{C})$ for $P(M,G)$ can be reduced to $U(r,\mathbb{C})$ . Edit : As structure group $GL(n,\mathbb{C})$ of $P\rightarrow M$ can be reduced to $U(n)$ , we get a principal $U(n)$ bundle $Q\rightarrow M$ . Now, Lie algebra of $U(n)$ is $\mathfrak{u}(n)$ of skew Hermitian matrices. For $X\in \mathfrak{u}(n)$ , I believe (I checked it for some examples) that $\frac{1}{2\pi \sqrt{-1}}X$ has characteristic polynomial with real coefficients. So, 
we have $$\text{det}\left(\lambda I-\frac{1}{2\pi\sqrt{-1}}X\right)=\sum_{k=0}^rf_k(X)\lambda^{r-k}$$ for $X\in \mathfrak{u}(r,\mathbb{C})$ . 
Here $f_k:\mathfrak{u}(r,\mathbb{C})\rightarrow \mathbb{R}$ . I now consider Chern Weil Homomorphism for $Q\rightarrow M$ and these $f_k$ give real cohomology classes $H^{2k}(M,\mathbb{R})$ . Thus we get deRham cohomology classes with real coefficients and not just complex coefficients.","['de-rham-cohomology', 'characteristic-classes', 'differential-geometry']"
3051228,Show that $\sum\limits_{n\ge1}\frac1{n^2}=\sum\limits_{n\ge1}\frac3{n^2\binom{2n}n}$ without actually evaluating both series,"$$\sum\limits_{n\ge1}\frac1{n^2}=\sum\limits_{n\ge1}\frac3{n^2\binom{2n}n}\tag1$$ Note that $(1)$ holds since the LHS is given by $\zeta(2)$ whereas the RHS by $6\arcsin^2 1$ which both equal $\dfrac{\pi^2}6$ as it is well-known. However, I am interested in proving $(1)$ without actually evaluating both series. I am aware of an elegant approach contributed by Markus Scheuer as an answer to Different methods to compute Basel problem . Although this answers my question partially I am looking for different attempts. For example within Jack D'Aurizio 's notes there is a way proposed exploiting creative telescoping $($ see page $5$ f. $)$ , which I do not understand completely (yet).
Hence I have come across a proof of a similar equality concerning $\zeta(3)$ on AoPS given by pprime I am
confident that there are in fact other possible methods. I would like to see attempts of proving $(1)$ besides the ones mentioned which do not rely on actually showing that they both equal $\dfrac{\pi^2}6$ . Preferably these should be in the spirit of Markus Scheuer 's or Jack D'Aurizio 's approaches rather than the one similar by pprime . Thanks in advance! EDIT I I have found another interesting approach, again by Jack D'Aurizio , which can be found here utilizing harmonic sums and creative telescoping in combination. EDIT II As pointed out by Zacky on page $31$ of Jack 's notes another method can be found which makes it three possibilities provided by Jack alone. Quite impressive!","['alternative-proof', 'sequences-and-series']"
3051288,Maximising function of $n$ variables,"I am considering the following function $$f(x_1,\dots,x_n,y)=-\alpha \left(y-k_1\right)^2-\beta \sum_{i=1}^n\left(k_2-x_i\right)^2-\gamma \sum_{i=1}^n\left(y-x_i\right)^2 - \frac{\delta}{y-d} \sum_{i=1}^n (x_i-d)\, ,$$ where $(x_1,\dots,x_n,y)\in[d,1]^{n+1}$ , $d>0$ and $\alpha$ , $\beta$ , $\gamma$ , $\delta$ , $k_1$ , $k_2>0$ . Moreover, $x_i\leq y$ $\forall$ $i=1,\dots,$ $n$ . I'm trying to calculate the maximum of this function on that domain. Befor using ""brute force"" approach (i.e. by calculating derivatives, Hessian and so on), I wonder whether it's possible to obtain the absolute maximum in a more simple way. For example, I notice that $f\leq0$ ...","['optimization', 'multivariable-calculus', 'maxima-minima']"
3051302,The proof of positive semi-definite for a kernel,"How to prove the following kernel $K$ over $\mathbb R \times \mathbb R$ is positive semi-definite: $$K(x_i, x_j) = e^{-\lambda[\sin(x_i - x_j)]^2},$$ where $\lambda > 0$ . It looks like the gaussian kernel $e^{-\lambda\|x_i - x_j\|^2}$ . How can we link $\sin$ function to some kinds of norm? Or equivalently, how to prove the matrix $A$ defined by $$A_{ij} = e^{-\lambda[\sin(x_i - x_j)]^2}$$ is positive semi-definite for any $\lambda > 0$ and $x_1, \cdots, x_n > 0$ ?","['matrices', 'positive-semidefinite']"
3051333,Egyptian Fraction when numerator is greater than denominator,"I am doing an assignment about Egyptian fractions and I am a bit confused about what to do when the given fraction's numerator is greater than denominator. My initial idea was to subtract the fraction by 1, 1/2, 1/3 etc and when the numerator becomes less than denominator I would apply the proper algorithm. And I saw that this way does not work out. Do you have any suggestions? Thank you. EDIT: I cannot subtract by 1 because it will not be an Egyptian fraction.","['number-theory', 'egyptian-fractions', 'elementary-number-theory']"
3051334,Limiting a sequence of moment generating functions,"I was trying to solve the following problem: Let $\{X_n\}_{n=1}^{\infty}$ be a sequence of independent random variables with the probability mass function $P\{X_n = \pm1 \} = \frac{1}{2}$ , $n \in \mathbb{N}$ . Let $Z_n=\sum_{j=1}^{n}{X_j/2^j}$ . Show that $Z_n \xrightarrow{L} Z$ , where $Z \sim U[-1, 1]$ . (From An Introduction to Probability and Statistics , V.K. Rohatgi & A. K. Md. Saleh, (c) 2015, Problems 7.5, Page 320) Here $\xrightarrow{L}$ means convergence in law (or in distribution), and $U[-1, 1]$ is the uniform distribution on the interval $[-1, 1]$ . My approach was the following: We need to show that $$\lim_{n\rightarrow\infty} M_{Z_n}(t) = M_{Z}(t) = \frac{e^{1 \times t} - e^{-1 \times t}}{t \times (1 - (-1))} = \frac{e^t - e^{-t}}{2t}.$$ Since $$M_{Z_n}(t) = E_{Z_n}\left(e^{tZ_n}\right) = E\left(e^{t\sum_{j = 1}^{n}{\frac{X_j}{2^j}}}\right) = E\left(\prod_{j=1}^{n}{e^{t\frac{X_j}{2^j}}} \right) = \prod_{j=1}^{n}{E_{X_j}\left(e^{t\frac{X_j}{2^j}} \right)},\\
E_{X_j}\left( e^{t \frac{X_j}{2^j}}  \right) = e^{t \times \frac{-1}{2^j}} \times \frac{1}{2} + e^{t \times \frac{1}{2^j}} \times \frac{1}{2} = \frac{1}{2} \left( e^{\frac{t}{2^j}} + e^{\frac{-t}{2^j}} \right),$$ then $$M_{Z_n}(t) = \prod_{j = 1}^{n}{\frac{1}{2} \left( e^{\frac{t}{2^j}} + e^{\frac{-t}{2^j}} \right)}.$$ I cannot see how this sequence of functions converges to the required moment generating function of $U[-1,1]$ . I had many attempts, for instance using the power series representation of $e^x$ and limiting approximations, but failed in them all. After that I started thinking that perhaps I am missing knowledge of some theorems. Any idea how to proceed?","['probability-limit-theorems', 'probability-theory']"
3051385,Group cohomology of product with swapping (twisting) factors,"Let $M$ be a $G$ -module, where $G = \Bbb Z / 2 \Bbb Z$ . Define a $G$ -module structure on $A = M \oplus M$ by $g \cdot (a,b) = (g \cdot b, g \cdot a)$ .
What is the group cohomology $H^*(G, A)$ is terms of $H^*(G,M)$ ? Thoughts: I know that $H^n(G, -)$ commutes with products, but here the action on $A$ is not the diagonal action, so I'm not sure what to do here. Can we at least compute $H^1(G, A)$ , provided that $H^1(G, M)=0$ ?","['group-theory', 'group-cohomology']"
3051402,"If $a$, $b$ are the roots of $x^2-2x+3$.Then the equation whose roots are $a^3-3a^2+5a-2$ and $b^3-b^2+b+5$ is:","If $a$ , $b$ are the roots of $x^2-2x+3$ .Then the equation whose roots are $a^3-3a^2+5a-2$ and $b^3-b^2+b+5$ is: I have not been able to find a better method than to calculate $a$ and $b$ then substitute them into the roots for the new polynomial. I believe this question can't be transformed in a similar manner as mentioned in this question as the new roots are asymmetrical. Does a better method than the lackluster substitution, exist? The answer is: $x^2-3x+2$",['linear-algebra']
3051444,How to Simplify Yule Walker Expression for AR Coefficients?,"We have an AR(2) process (w/ intercept omitted): $$
y _ { t } = a _ { 1 } y _ { t - 1 } + a _ { 2 } y _ { t - 2 } + \varepsilon _ { t }
$$ We multiply the second order equation by $ y_{t-s} \text{ for } s = 0, s = 1, s = 2 \ldots $ $$
\begin{aligned} E y _ { t } y _ { t } & = a _ { 1 } E y _ { t - 1 } y _ { t } + a _ { 2 } E y _ { t - 2 } y _ { t } + E \varepsilon _ { t } y _ { t } \\ E y _ { t } y _ { t - 1 } & = a _ { 1 } E y _ { t - 1 } y _ { t - 1 } + a _ { 2 } E y _ { t - 2 } y _ { t - 1 } + E \varepsilon _ { t } y _ { t - 1 } \\ E y _ { t } y _ { t - 2 } & = a _ { 1 } E y _ { t - 1 } y _ { t - 2 } + a _ { 2 } E y _ { t - 2 } y _ { t - 2 } + E \varepsilon _ { t } y _ { t - 2 } \\ \cdots & \\ & \cdots \\ E y _ { t } y _ { t - s } & = a _ { 1 } E y _ { t - 1 } y _ { t - s } + a _ { 2 } E y _ { t - 2 } y _ { t - s } + E \varepsilon _ { t } y _ { t - s } \end{aligned}
$$ By definition, the autocovariances of a stationary series are such that $E y _ { t } y _ { t - s } =$ $E y _ { t - s } y _ { t } = E y _ { t - k } y _ { t - k - s } = \gamma _ { s } .$ We also know that $E \varepsilon _ { t } y _ { t } = \sigma ^ { 2 }$ and $E \varepsilon _ { t } y _ { t - s } = 0 .$ Hence, we
can use the equations in $( 2.24 )$ to form $$
\gamma _ { 0 } = a _ { 1 } \gamma _ { 1 } + a _ { 2 } \gamma _ { 2 } + \sigma ^ { 2 }
$$ $$
\begin{aligned} \gamma _ { 1 } & = a _ { 1 } \gamma _ { 0 } + a _ { 2 } \gamma _ { 1 } \\ \gamma _ { s } & = a _ { 1 } \gamma _ { s - 1 } + a _ { 2 } \gamma _ { s - 2 } \\ \text { Dividing by } \gamma _ { 0 } \text { yields } \\ \rho _ { 1 } & = a _ { 1 } \rho _ { 0 } + a _ { 2 } \rho _ { 1 } \\ \rho _ { s } & = a _ { 1 } \rho _ { s - 1 } + a _ { 2 } \rho _ { s - 2 } \end{aligned}
$$ Question: How did $ \gamma_1 / \gamma_0 $ turn into the $\rho_1$ function? The simple algebra of this division isn't making sense. What happens to the Variance $ \sigma^2 $ that's in $ \gamma_0 $ but somehow eliminated in the $ \gamma_1$ function?","['stochastic-processes', 'systems-of-equations', 'functions', 'sequences-and-series']"
3051454,"Proof verification. $\{x_n\}$ is a sequence such that $|x_{n+1} - x_n| \le C\alpha^n$ for $\alpha\in (0, 1), n\in\Bbb N$. Prove $x_n$ converges.","Let $\{x_n\}, n\in \Bbb N$ denote a sequence such that: $$
\begin{cases}
|x_{n+1} - x_n| \le C\alpha^n \\
0 < \alpha < 1
\end{cases}
$$ Prove $\{x_n\}$ converges. Given the fact $|x_{n+1} - x_n| \le C\alpha^n$ consider the following inequalities: $$
|x_{n+1} - x_n| \le C\alpha^n \\
|x_{n+2} - x_{n+1}| \le C\alpha^{n+1} \\
|x_{n+3} - x_{n+2}| \le C\alpha^{n+2} \\
\dots \\
|x_{n+p+1} - x_{n+p}| \le C\alpha^{n+p} \\
$$ Consider the sum of the inequalities: $$
|x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + |x_{n+3} - x_{n+2}| + \cdots + |x_{n+p+1} - x_{n+p}| \\ 
\le \sum_{k=0}^p C\alpha^{n+k} = C \sum_{k=0}^p \alpha^{n+k} \tag1
$$ By geometric sum: $$
C \sum_{k=0}^p \alpha^{n+k} = C \cdot \frac{\alpha^n(1-\alpha^{p + 1})}{1-\alpha} \le C \cdot \frac{\alpha^n}{1-\alpha}
$$ Lets fix some $\epsilon > 0$ , and $N\in \Bbb N$ such that: $$
C\cdot \frac{\alpha^{N}}{1-\alpha} < \epsilon
$$ Rewrite $\alpha$ as: $$
\alpha = \frac{1}{1+r},\ r \in \Bbb R_{>0}
$$ Thus: $$
C\cdot \frac{1}{(1-\alpha)(1+r)^N} < \epsilon \\
(1+r)^N > {C\over (1-\alpha)\epsilon} \\
N > \log_{1+r} {C\over (1-\alpha)\epsilon}
$$ Returning to $(1)$ we have by triangular inequality: $$
|x_{n}- x_{n+1} + x_{n+1} - x_{n+2} + \cdots + x_{n+p} - x_{n+p+1}| \\ \le |x_{n+1} - x_{n}| + |x_{n+2} - x_{n+1}| + \cdots + |x_{n+p+1} - x_{n+p}|
$$ Since the values are telescoping we obtain: $$
|x_{n} - x_{n+p+1}| < C \cdot \frac{\alpha^n}{1-\alpha} < \epsilon
$$ Now if we choose: $$
n > N > \log_{1+r} {C\over (1-\alpha)\epsilon}
$$ we obtain a regular definition of the Cauchy criteria, which means $\{x_n\}$ is a convergent sequence. Could you please verify the reasoning above? Thank you!","['cauchy-sequences', 'proof-verification', 'calculus', 'sequences-and-series', 'limits']"
3051480,Why the average of a set of value has the least square error?,"Now we have the equation $$\sum_{i}(x_i-\hat x_i)^2,$$ where $x_i$ is the observed value of a data sample $S$ . Here is the question: Why does this expression get its minimum value when $\hat x_i$ is the average of the data sample $S$ ? I tried to take the derivatives of that equation and make it to zero, but it seems there's something wrong, because $\hat x_i$ is kind of multi-variable. 
Can anyone help me out? Thanks a lot!","['statistics', 'least-squares']"
3051521,If $A + A^t = 2I$ then $\det(A) \geq 1$,"Let $A$ be a $n \times n$ real matrix such that $$ A + A^t = 2I, $$ where $I$ is the $n \times n $ identity matrix. Prove that $\det(A) \geq 1$ . It is obvious that tr $(A) = n$ . Furthermore, we also have that $$A - 2I = -A^t, $$ so we get that $$\det(A-2I) = (-1)^n \cdot \det(A).$$ Now, we let $\lambda_1, \lambda_2, \cdots, \lambda_n \in \mathbb{C}$ be the  eigenvalues of $A$ , so we get that $$(\lambda_1 - 2)(\lambda_2 - 2) \cdots (\lambda_n - 2) = (-1)^n\lambda_1\lambda_2 \cdots \lambda_n, $$ but I couldn't derive anything about the product $\lambda_1\lambda_2 \cdots \lambda_n = \det(A)$ (the only known thing is $\lambda_1 + \cdots + \lambda_n = n$ ). Also, I tried the same approach for $2 \times 2$ and $3 \times 3 $ matrices, but it didn't lead to anything (especially since the eigenvalues can be complex numbers).","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
3051523,Moduli interpretation of the integral anticanonical tower,"This question is related to my reading of On torsion in the cohomology of locally symmetric varieties by Scholze. In chapter $3$ , using the theory of canonical subgroup, he produces Frobenius maps defined over $\text{Spf(}\mathbb{Z}_p^{\text{cycl}})$ between the integral models of strict neighborhoods of the ordinary locus for the modular curve. Then, he computes the projective limit along Frobenius of these maps, and he gets a formal scheme over $\text{Spf}(\mathbb{Z}_p^{\text{cycl}})$ , whose generic fiber is perfectoid and is called the anticanonical tower of modular curves. Later on in his exposition, he writes that the $(C,\mathcal{O}_C)$ -points of this perfectoid space (where $C$ is the completion of an algebraic closure of $\mathbb{Q}_p$ , and $\mathcal{O}_C$ is its ring of integers) parametrize elliptic curves over $C$ with a trivialization of their Tate module. First, I do not see why this construction provides a unique elliptic curve! First, I would like to say that the construction provides a projective system of elliptic curves over $C$ , where every elliptic curve has $p^n$ -torsion trivialized (for $n$ becoming bigger along the tower), where the maps defining the projective system are quotient by the canonical subgroup. But why is such a kind of system the same as a unique elliptic curve with Tate module trivialized? Second question, does a similar description hold for the integral anticanonical tower? Is it true that an $R$ point of the anticanonical tower, where $R$ is a complete and flat (maybe normal) $\mathbb{Z}_p^{\text{cycl}}$ -algebra, gives a family of elliptic curves over $R$ with a trivialization (at least a generic trivialization) of its Tate module seen as a sheaf? Thank you for any kind of suggestion!","['number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
3051551,"What is the region of convergence of $x_n=\left(\frac{x_{n-1}}{n}\right)^2-a$, where $a$ is a constant?","The following recurrence relation came up in some research I was working on: $$x_n=\left(\frac{x_{n-1}}{n}\right)^2-a$$ Or equivalently the map: $$z\mapsto\frac{z^2}{n^2}-a$$ Where $n$ is the iteration number. Specifically, I'm interested in the size of the convergence region across the real line. Some stuff I know about this map: For $a = 1$ , it's easy, the ""size on the real line"" is $[-3,3]$ . I do have an infinite radical expansion for the size of the convergence region on the real line (see Solving the infinite radical $\sqrt{6+\sqrt{6+2\sqrt{6+3\sqrt{6+...}}}}$ ): $$\sqrt{a+2\sqrt{a+3\sqrt{a+...}}}$$ That's why it's easy for $a=1$ -- it's just the Ramanujan radical, and equals 3. It's also easy for $a=0$ -- it's $\exp\left(-\mathrm{PolyLog}^{(1,0)}(0,1/2)\right)$ as per Wolfram Alpha. Has anyone seen this map before? Here's the region of convergence on the complex plane, plotted numerically (for $a=6$ ):","['convergence-divergence', 'recurrence-relations', 'sequences-and-series']"
3051629,Geometric Hahn Banach implies Analytic Hahn Banach.,"I want to prove that the geometric Hahn Banach theorem implies the analytic one. Edit: 
To avoid confusion I will state the vesion of H.B theorems im familiar with: Analytic H.B: Let $X$ be a linear space(over $\Bbb R$ ) and $Y\subset X$ a subspace. Let $p:X \to \Bbb R$ be a sub-additive function. Suppose $f:Y\to \Bbb R$ is a linear map s.t $f(y) \le p(y)$ for all $y\in Y$ then there exists an extention $g:X\to \Bbb R$ of $f$ s.t. $g(x)\le p(x) $ for all $x\in X$ . Geometric H.B: Let $X$ be a linear space. $K \subset X$ convex s.t. each point in $K$ is an internal point. Let $D$ be a plane disjoint from $K$ then there exists hyperplane that contains $D$ and disjoint from $K$ . In my problem $X$ is NOT a normed space so there are no open sets. Given $X$ a linear space and $Y\subset X$ a subspace $p:X\to \Bbb R$ sub-additive, and $f:Y\to \Bbb R$ linear s.t $f(y)\le p(y)$ for $y\in Y$ we need to extend $f$ to $g:X\to \Bbb R $ and $g(x)\le p(x)$ fo r all $x\in X$ So, we look at $X \times \Bbb R $ and define $K = \{(x,t) : t>p(x)\}$ . The fact that $K$ is convex is easy. How can I show directly that every point in $K$ is internal? (can't say that $K$ is open). Now after showing that, we can look at $Graph(f)$ and observe that $Graph(f)\cap K = \emptyset$ . So by the geometric H.B theorem we have a hyperplane (which is a maximal subspace in this case because $(0,0)\in Graph(f)$ ) M containing $Graph(f) $ and disjoint from $K$ . Now my problem is to show that each $x\in X$ has a unique $t\in \Bbb R$ s.t. $(x,t) \in M$ .  (I need it in order to extend $f$ ). I know that if we take $v_0\notin M$ then each $v \in X \times \Bbb R$ has a unique representation as $v = \alpha v_0 + m$ for $m \in M$ , this is because $M$ is a maximal subspace. not sure if that helps. Thanks for helping!",['functional-analysis']
3051666,What is a nonlinear way the time on a clock can run?,"A clock runs irregularly but after 24 hours it has neither gained nor lost overall.
Find a way the clock can run irregularly such that there is no continuous 576-minute period across which the clock shows that 576 minutes have passed. The clock time should not discontinuously jump. I know that 24 hours = 1440 minutes and $576=\frac{2}{5}1440$ The likely idea is to divide the 24 hours/1440 minutes in a number of intervals and have the clock run faster in odd intervals (say running one hour in 30 minutes), then slower in even intervals (say running one hour in 90 minutes) so that overall it still runs 1440 minutes. I cant figure out which function would work for 576 minutes though.","['recreational-mathematics', 'functions']"
3051676,Recurrence $a_{n}=a_{\lfloor 2n/3\rfloor}+a_{\lfloor n/3\rfloor}$,"I am considering the sequence $$a_n=a_{\lfloor 2n/3\rfloor}+a_{\lfloor n/3\rfloor}$$ with $a_0=1$ , and I would like to calculate the limit $$\lim_{n\to\infty} \frac{a_n}{n}$$ I have seen this famous question and its answer, but since the recurrence in this question has only two terms on the RHS instead of three, I was wondering if there is a more elementary solution that does not use specialized knowledge like renewal theory. I have not made much progress; all I have managed to prove so far is that the sequence contains runs of arbitrarily long length, and this is probably not relevant to the desired limit.","['limits', 'asymptotics', 'recurrence-relations', 'sequences-and-series']"
3051694,Isn't a semialgebra an algebra?,"I was solving this exercise in a book, ""A First Look at Rigorous Probability Theory"", by Jeffrey Rosenthal. Exercise 2.7.3. Suppose $\mathcal{F}$ is a 
collection of subsets of $\Omega$ , such that $\Omega \in \mathcal{F}$ .
b) Assume $\mathcal{F}$ is a semialgebra. 
Prove that $\mathcal{F}$ is an algebra. However, it seems to me this exercise is wrong. 
A counterexample suffices to show it.
Suppose a set $\mathcal{J}$ of all intervals in $[0,1]$ . Then, it is easy
to show that $\mathcal{J}$ is a semi-algebra. Suppose two different
intervals $A = [0,\frac{1}{3}]$ and $B = [\frac{2}{3},1]$ are present in $\mathcal{J}$ . $A \cup B$ does not belong to $\mathcal{J}$ because it is not an interval. $\mathcal{J}$ is not an algebra because
it is not closed under finite unions. Am I missing something?","['measure-theory', 'examples-counterexamples']"
3051700,"Given a transition table do digraph, determine if it is DFA or NFA and build grammar","For the next transition table: $$\begin{array}{|c|c|c|c|}\hline&0&1&2\\\hline a&a&b&d\\\hline b&a&b&c\\\hline c&c&d&a\\\hline d&c&c&a\\\hline\end{array}\\a\text{ is initial state}\\\{a,d\}\text{ are final states}$$ Make the digraph of the finite automaton and indicate if it is DFA (deterministic) or NDFA (non-deterministic). Build a regular grammar that generates the language recognized by the automaton and indicate it. The digraph I made is: where $\mathrm{start}=a$ , $\,s0=b$ , $\,s1=c$ , $\,s2=d$ and the states marked with a tick are final states. The finite state machine (which is an automaton) $A=(\{a,b,c,d\},\{0,1,2\},\delta,a,\{a,d\})$ , where $\delta:\{a,b,c,d\}\times\{0,1,2\}\to\{a,b,c,d\}$ is deterministic because each state has at most one change of state for each letter of the alphabet and there are no state changes for the null word. A regular $\require{cancel}\cancel{\text{grammar}}\text{ expression}$ could be: $$\require{cancel}\xcancel{\begin{align*}L(A)&=0^*\\&\vee2^*\\&\vee(220^*2^*)^*\\&\vee(11^*0)^*\\&\vee(11^*20^*\vee2)^*\\&\vee(11^*20^*\vee(12))^*\\&\vee(11^*20^*\vee1)^*.\end{align*}}$$ \begin{align*}a&=0^*\vee1b\vee2d\vee\lambda\\b&=1^*\vee0a\vee2c\\c&=1d\vee2a\vee0^*\\d&=1c\vee0c\vee2a\vee\lambda\end{align*} and from here I do not know how to build the regular expression and then build the regular grammar (I have seen this and this links but I do not understand them since I could not even get the regular expression) because I have never seen an automaton with $2$ final states!! Also, the statement what does it mean by indicating the regular grammar? Everything is correct? Thanks! External link: Automaton created by automatonsimulator.com . You can test it introducing words here .","['automata', 'discrete-mathematics', 'regular-language']"
3051716,Solving a Second Order Order linear ODE when one solution is known,"The following problem is from the book ""Introduction to Ordinary Differential Equations"" by
Shepley L. Ross. Problem: Given that $y = x$ is a solution of $$ (x^2 -2x + 2)y'' - x^2y' + xy = 0$$ find a linearly independent solution by reducing the order. Write the general solution. Answer: Let $f(x)$ represent the solution we have. \begin{align*}
f(x) &= x \\
y &= f(x) v = xv \\
y' &= x v' + v \\
y'' &= x v'' + v' + v' = xv'' + 2v' \\
\end{align*} \begin{align*}
(x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\
(x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\
(x^3 -2x^2 + 2x)xv'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\
(x^4 -2x^3 + 2x^2)v'' + (-x^3 + 2x^2 -4x + 4)v'  &= 0 \\
\end{align*} Now we let $w = \frac{dv}{dx}$ and this gives us a separable differential equation. \begin{align*}
(x^4 -2x^3 + 2x^2)w' + (-x^3 + 2x^2 -4x + 4)w  &= 0 \\
(x^4 -2x^3 + 2x^2)w' &= (x^3 - 2x^2 + 4x - 4)w  \\
(x^4 -2x^3 + 2x^2) \,\, dw &= (x^3 - 2x^2 + 4x - 4)w \,\, dx  \\
\frac{dw}{w} &= \frac{ (x^3 - 2x^2 + 4x - 4) \, dx }{ x^4 -2x^3 + 2x^2 } \\
\end{align*} Now we perform the following integration using an online integral calculator: $$ \int \frac{ x^3 - 2x^2 + 4x - 4 }{x^4 -2x^3 + 2x^2} \,\, dx  =
	\frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1 $$ \begin{align*}
\ln{|w|} &= \frac{\ln{| x^2 - 2x + 2 |}}{2}  + \arctan{ \frac{ 2x - 2 }{2 } } + \frac{2}{x} + C_1 \\
\end{align*} At this point, I am confident that my attempt to solve the problem is wrong.
The book's answer is: $$ y = (x-2)e^{x} $$ I would expect the book's answer to have at least one constant if not two in the answer since there were no initial conditions given. Thanks, Bob Here is my second attempt to solve the problem. I think I go wrong in the last step but I am not sure where. Answer: Let $f(x)$ represent the solution we have. \begin{align*}
f(x) &= x \\
y &= f(x) v = xv \\
y' &= x v' + v \\
y'' &= x v'' + v' + v' = xv'' + 2v' \\
\end{align*} \begin{align*}
(x^2 -2x + 2)( xv'' + 2v') - x^2(x v' + v ) + x^2 v  &= 0 \\
(x^2 -2x + 2)( xv'' + 2v') - x^3 v'  &= 0 \\
(x^3-2x^2+2x)v'' + ( -x^3 + 2x^2 -4x +4)v' &= 0 \\ 
\end{align*} Now we let $w = \frac{dv}{dx}$ and this gives us a separable differential equation. \begin{align*}
(x^3-2x^2+2x)w' + ( -x^3 + 2x^2 -4x +4)w  &= 0 \\
\frac{dw}{w} &= \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} \, dx \\
\end{align*} Now we need to integrate the right hand side. We perform long division on the right
hand side and get: \begin{align*}
 \frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 + \frac{2x - 4}{ x^3-2x^2+2x } \\
\end{align*} Now we use the technique of partial fractions: \begin{align*}
\frac{4x - 6}{ x^3-2x^2+2x } &= \frac{2x - 4}{ x(x^2-2x+2  ) } = \frac{A}{x} + \frac{Bx + C}{x^2-2x+2} \\
2x - 4 &= A(x^2-2x + 2) + (Bx+C)x \\
\text{ We set  $x = 0$ and find } A &= -2 \\
2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + Bx^2 + Cx \\
0 &= -2 + B \\
B &= 2 \\
2x - 4 &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x - 4 + 2x^2 + Cx \\
2x &= -2(x^2-2x + 2) + (Bx+C)x  = -2x^2 + 4x + 2x^2 + Cx \\
C + 4 &= 2 \\
C &= -2 \\
\frac{x^3 - 2x^2 + 4x - 4}{x^3-2x^2+2x} &= 1 - \frac{2}{x} + \frac{2x - 2}{x^2-2x+2} 
\end{align*} We need to integrate the following: $$ \frac{2x - 2}{x^2-2x+2} $$ This can be done with the substitution $u = x^2  - 2x + 2$ which gives us $du = (2x - 2) dx$ . \begin{align*}
\ln{|w|} &= x - 2 \ln{|x|} + \ln{|x^2-2x+2|} + C_1 \\
\ln{|w|} &= \ln{e^x} - 2 \ln{|x|} + \ln{|x^2-2x+2|} + \ln{ e^{C_1} } \\
w &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\
\frac{dv}{dx} &= C_2 \left( \frac{e^x(x^2-2x+2)}{x^2} \right) \\
v &= \int \,\, C_2 \frac{e^x(x^2-2x+2)}{x^2} dx \\
\end{align*} At this point, I am confident that my attempt to solve the problem is wrong.
The book's answer is: $$ y = (x-2)e^{x} $$",['ordinary-differential-equations']
3051737,Show that $X\subset X\cup\left\{ X\right\}$,"Show that If $X$ is an any set, then $X\subset X\cup\left\{ X\right\}$ Proof. Let $t\in X$ . We must show $t\in X\cup\left\{ X\right\}$ , that is we need to show either $t\in X$ or $t\in\left\{ X\right\}$ , so we know that $t\in X$ , hence we are done. Can you check my proof?","['elementary-set-theory', 'proof-verification']"
3051738,Complete+bounded homeomorphic to incomplete+unbounded,"I'm aware that completeness and (total) boundedness are not preserved under homeomorphism, with $(0,1) \cong \mathbb R$ being a counterexample to both simultaneously. I'm curious if there exists a ""double counterexample"" in another way: Are there homeomorphic metric spaces $M$ and $N$ such that $M$ is both complete and bounded,
but $N$ is neither complete nor bounded? Of course in that case, $M$ could not be totally bounded, else it would be compact, which is certainly a topological property!","['general-topology', 'metric-spaces']"
3051754,$\sum\limits_{m\geq1}\sum\limits_{n\geq1}\frac{(-1)^n}{n^3}\sin\left(\frac{n}{m^2}\right)=\frac{\pi^6}{11340}-\frac{\pi^4}{72}$ Numerical evidence,"I am looking for numerical evidence that $$\sum_{m\geq1}\sum_{n\geq1}\frac{(-1)^n}{n^3}\sin\left(\frac{n}{m^2}\right)=\frac{\pi^6}{11340}-\frac{\pi^4}{72}$$ I have proven it, but I just want to be extra sure. Desmos only gives me accuracy to the third decimal place, but I know that some of you (@Claude Leibovici) are able to give me extremely high decimal accuracy. Proof: We know that for $|t|\leq\pi$ , $$t^2=\frac{\pi^2}3+4\sum_{n\geq1}\frac{(-1)^n}{n^2}\cos nt$$ Solving for the sum then integrating both sides from $0$ to $x$ , $$\sum_{n\geq1}\frac{(-1)^n}{n^3}\sin(nx)=\frac{x^3}{12}-\frac{\pi^2x}{12}$$ Then plugging in $x=\frac1{m^2}$ for integer $m\geq1$ , $$\sum_{n\geq1}\frac{(-1)^n}{n^3}\sin\bigg(\frac{n}{m^2}\bigg)=\frac{1}{12m^6}-\frac{\pi^2}{12m^2}$$ then applying $\sum_{m\geq1}$ on both sides, $$\sum_{m\geq1}\sum_{n\geq1}\frac{(-1)^n}{n^3}\sin\bigg(\frac{n}{m^2}\bigg)=\frac1{12}\zeta(6)-\frac{\pi^2}{12}\zeta(2)$$ We simplify to reach our conclusion $$\sum_{m\geq1}\sum_{n\geq1}\frac{(-1)^n}{n^3}\sin\bigg(\frac{n}{m^2}\bigg)=\frac{\pi^6}{11340}-\frac{\pi^4}{72}$$","['fourier-series', 'proof-verification', 'decimal-expansion', 'sequences-and-series']"
3051755,Finding the value of $\cos(\frac{180^o}7)$ -- where am I wrong?,"Let $\alpha=\frac{180^o}7$ . Consider that $$\sin(3\alpha)=\sin(180^o-3\alpha)=\sin(4\alpha)$$ and $$\cos(3\alpha)=-\cos(180^o-3\alpha)=-\cos(4\alpha)\text{.}$$ From $\sin(3\alpha)=\sin(4\alpha)$ , we get $$\sin(\alpha)\cos(2\alpha)+\sin(2\alpha)\cos(\alpha)=\sin(2\alpha)\cos(2\alpha)+\sin(2\alpha)\cos(2\alpha)$$ $$\cos(2\alpha)(\sin(\alpha)-\sin(2\alpha))=\sin(2\alpha)(\cos(2\alpha)-\cos(\alpha))$$ $$\cos(2\alpha)\sin(\alpha)(1-2\cos(\alpha))=2\sin(\alpha)\cos(\alpha)(2\cos^2(\alpha)-\cos(\alpha)-1)$$ $$\cos(2\alpha)(1-2\cos(\alpha))=2\cos(\alpha)(2\cos^2(\alpha)-\cos(\alpha)-1)$$ $$\cos(2\alpha)(1-2\cos(\alpha))=-2\cos^2(\alpha)(1-2\cos(\alpha))-2\cos(\alpha)$$ $$(\cos(2\alpha)+2\cos^2(\alpha))(1-2\cos(\alpha))=-2\cos(\alpha)$$ $$(\cos(2\alpha)+2\cos^2(\alpha))(2\cos(\alpha)-1)=2\cos(\alpha)$$ $$(4\cos^2(\alpha)-1)(2\cos(\alpha)-1)=2\cos(\alpha)$$ $$8\cos^3(\alpha)-4\cos^2(\alpha)-4\cos(\alpha)+1=0$$ From $\cos(3\alpha)=-\cos(4\alpha)$ we get $$\cos(2\alpha)\cos(\alpha)-\sin(2\alpha)\sin(\alpha)=\sin(2\alpha)\sin(2\alpha)-\cos(2\alpha)\cos(2\alpha)$$ $$\cos(2\alpha)(\cos(2\alpha)+\cos(\alpha))=\sin(2\alpha)(\sin(2\alpha)+\sin(\alpha))$$ $$(2\cos^2(\alpha)-1)(2\cos^2(\alpha)+\cos(\alpha)-1)=\sin^2(2\alpha)+\sin(2\alpha)\sin(\alpha)$$ $$(2\cos^2(\alpha)-1)(2\cos^2(\alpha)+\cos(\alpha)-1)=1-\cos^2(2\alpha)+2\sin^2(\alpha)\cos(\alpha)$$ $$(2\cos^2(\alpha)-1)(2\cos^2(\alpha)+\cos(\alpha)-1)=1-(2\cos(\alpha)-1)^2+2(1-\cos^2(\alpha))\cos(\alpha)$$ $$4\cos^4(\alpha)+2\cos^3(\alpha)-4\cos^2(\alpha)-\cos(\alpha)+1=1-(4\cos^2(\alpha)-4\cos(\alpha)+1)+2\cos(\alpha)-2\cos^3(\alpha)$$ $$4\cos^4(\alpha)+2\cos^3(\alpha)-4\cos^2(\alpha)-\cos(\alpha)=-4\cos^2(\alpha)+4\cos(\alpha)-1+2\cos(\alpha)-2\cos^3(\alpha)$$ $$4\cos^4(\alpha)+4\cos^3(\alpha)-7\cos(\alpha)+1=0$$ Now, let $c=\cos(\alpha)$ . From the previous equations: $$8c^3-4c^2-4c+1=0\ ...(1)$$ $$4c^4+4c^3-7c+1=0\ ...(2)$$ Subtract the equations: $$4c^4-4c^3+4c^2-3c=0$$ Since $c$ cannot be $0$ ( $c=0$ doesn't fulfill (1)), we get $$4c^3-4c^2+4c-3=0$$ $$8c^3-8c^2+8c-6=0\ ...(3)$$ Subtract (3) from (1): $$4c^2-12c+7=0$$ $$4c^2-12c+9=2$$ $$(2c-3)^2=2$$ $$2c-3=\pm\sqrt2$$ $$2c=3\pm\sqrt2$$ $$c=\frac{3\pm\sqrt2}2$$ $$\cos(\frac{180^o}7)=\frac{3\pm\sqrt2}2$$ Since $\cos(\frac{180^o}7)\le1$ , we get $$\cos(\frac{180^o}7)=\frac{3-\sqrt2}2$$ But I substituted it to (1), and it doesn't fulfill the equation. What did I do wrong?",['trigonometry']
3051775,Is a set bounded in every metric for a uniformity bounded in the uniformity?,"This is a follow-up to my question here .  A subset $A$ of a uniform space is said to be bounded if for each entourage $V$ , $A$ is a subset of $V^n[F]$ for some natural number $n$ and some finite set $F$ .  A subset of a metric space is said to be bounded if it is contained in some open ball.  Now this answer shows that if $U$ is the uniformity induced by a metric $d$ , then a set bounded with respect to $U$ is also bounded with respect to $d$ , but the converse need not be true. But I’m interested in whether something weaker is true.  Suppose that $(X,U)$ is a metrizable uniform space, and $A$ is a subset of $X$ which is bounded with respect to every metric which induces $U$ .  Then is $A$ bounded with respect to $U$ ? To put it another way, is the collection of bounded sets with respect to a metrizable uniformity equal to the intersection of the collections of bounded sets with respect to each of the metrics for the uniformity?","['uniform-spaces', 'general-topology', 'metric-spaces', 'examples-counterexamples']"
3051780,"Existence of topological space which has no ""square-root"" but whose ""cube"" has a ""square-root""",Does there exist a topological space $X$ such that $X \ncong Y\times Y$ for every topological space $Y$ but $$X\times X \times X \cong Z\times Z$$ for some topological space $Z$ ? Here $\cong$ means homeomorphic.,"['homotopy-theory', 'general-topology', 'homology-cohomology', 'algebraic-topology']"
3051827,"n-th order polynomial with all roots where all coefficients are 1 or -1, highest order of n?","I have polynomial $f(x） = \sum_{i=0}^n a_i x^i $ , where $a_i = \pm 1$ . All roots of $f(x)$ are real. What's the highest order of $n$ ? Note that the roots are real but can be irrational. Even if there are duplicate roots, that's fine","['number-theory', 'polynomials']"
3051836,Polynomials have smallest degree,"I am interested in the following problem. Problem 1. Find polynomial P (x) has integer coefficients, there is no rational roots, with the smallest degree such that for each positive integer $m$ there exists a positive integer a such that $$m \mid P (a). $$ In the case of $\deg P = 2$ , use quadratic residue, we can see that it cannot happen. In the case of $\deg P = 4$ , I have the answer (use quadratic residue). Therefore, I need an answer for the case of $\deg P = 3$ , ie the following problem. Problem 2. Let the polynomial $ P (x) = ax ^ 3 + bx ^ 2 + cx + d$ , $a,\,b,\,c,\,d\in\mathbb Z$ and $a \ne 0 $ , $ P (x) $ is irreducible on $ \mathbb Z [x] $ . Prove that there exists a positive integer $ m $ such that $$m\nmid P(n),\quad\forall\,n\in\mathbb N^*.$$","['number-theory', 'polynomials', 'elementary-number-theory']"
3051936,Understanding the homotopy operator for de Rham Cohomology,"This is in John Lee's Smooth Manifold 2nd Edition, pg 444 For any smooth manifold $M$ , there exists a linear map $$ h:\Omega^p(M \times I ) \rightarrow \Omega^{p-1}(M)$$ such that $$ h(dw)+d(hw) = i_1^*w - i^*_0w$$ The proof begins goes as follows. This is the same one here . Let $s$ be the standard coordinate on $\Bbb R$ and let $S$ be the vector field on $M \times \Bbb R$ given by $S_{(q,s)} = (0, \partial/\partial s|_s)$ . Let $w$ be a smooth $p$ -form on $M \times I$ . Define $hw \in \Omega^{p-1}(M)$ by $$ hw = \int_0^1 i^*_t (S \lrcorner w) \, dt. $$ Specifically, given $q \in M$ , this means, $$ (hw)_q = \int_0^1 i^*_t\Big( (S \lrcorner w )_{(q,t)} \Big) \, dt $$ The notation $S \lrcorner w$ is interior multiplication. Otherwise denoted by $\iota_Sw$ . My question is, what does this integral even mean ? At each point $q \in M$ , the integrand is a function of $t$ with values in the vector space $\wedge^{p-1} (T^*q M)$ . How do we integrate over this? My thoughts on the way to see this: We work locally, in nhood $U_q$ . For $r \in U_q$ our integrand is given by $$ \sum a_I(r,t) dx^I$$ summing over indices $I$ increasing $p-1$ length subset of $0$ to $n$ . Our new $p-1$ form is given by $$ \sum \int a_I(r,t) \, dt \, d x^I $$ There is a problem however, that we have to show this is independent of the choice of coordinate.","['homotopy-theory', 'de-rham-cohomology', 'algebraic-topology', 'differential-geometry']"
3051943,"Balls with duplicate colors grouped into groups of 5, probability of a group with $\ge 2$ colors","This arose out of an online game, and both exact answers and approximations would be greatly appreciated. $280$ balls are randomly put into $56$ bins such that all bins contain exactly $5$ balls. Among all $280$ balls, $2$ of them are colored red, $2$ of them green, $2$ of them blue (the rest can be considered uncolored, or any other color). I want to know the probability that, among the $56$ bins of balls, there exists $\ge 1$ bin which satisfies the following condition: the bin contains balls of at least two colors among the colors red, green, and blue. For example, if we use ""O"" to denote a ball that is not colored as red, green, or blue. $\{R, B, O, O,  O\}$ and $\{R,R,G,O,O\}$ are bins that satisfy the condition, while $\{R,R,O,O,O\}$ and $\{B,O,O,O,O\}$ do not. My Attempt I think an exact answer can be arrived (using multinomials), but I could not proceed beyond writing out the denominator. I decided to do a Poisson approximation, where $n = 56$ and $p$ is the probability that, when we randomly sample $5$ balls out of $280$ , the $5$ balls satisfy the condition. I calculated $p$ as follows: $$
1 - \frac {
\binom{274}{5} + \binom{6}{1} \binom{274}{4} + 3 \binom{2}{2} \binom{274}{3}
} {\binom{280}{5}}
$$ And from then on I used $\lambda = n p$ and used $1 - e^{-\lambda}$ as the final probability that there exists $\ge 1$ bin which satisfies the condition. Is my $p$ correct, or did I count it wrong? Can I use Poisson approximation here? I know that Poisson can be used for weakly dependent events, but I am not sure this qualifies. Edit: The Randomization Process I realized that I probably should have stated the randomization process. The original game was randomized by randomly sampling 5 out of 280 into the first bin, 5 out of the remaining 275 into the second, 5 out of the remaining 270 into the third, and so on.","['combinatorics', 'probability']"
3051954,Probability that a graph is bipartite,"Given the empty graph on $n$ vertices, we add $m$ of the $\binom{n}{2}$ possible edges, uniformly at random. What is the probability that the resulting graph is bipartite (equivalently, contains no odd cycles) ? Alternative formulation: In the random graph model $G(n,m)$ , how many of the $\binom{\binom{n}{2}}{m}$ graphs are bipartite ?","['graph-theory', 'random-graphs', 'probability']"
3051972,Woodbury Matrix Inversion,"I am trying to invert a matrix using Woodbury identity. The inversion using Cholesky decomposition has the following pseudo-code: For $t=1,2,...$ $(1)\;\; \text{Read}\;x_t\in\mathbb{R}^n$ $(2)\;\;D_{t-1}=diag(|\theta_t^1|,...,|\theta_t^n|)$ $(3)\;\;A_t=A_t+x_tx_t'$ $(4)\;\;A_{t}^{-1}=\sqrt{D_{{t-1}}}\left(a\mathbf{I}+\sqrt{D_{{t-1}}}A_t\sqrt{D_{{t-1}}}\right)^{-1}\sqrt{D_{{t-1}}}$ $(5)\;\;\text{Read}\;y_t\in\mathbb{R}$ $(6)\;\;b=b+y_tx_t$ $(7)\;\;\theta_t=A_{t}^{-1}b$ End For The well known application of Sherman-Morrison on Recursive Least Squares is as follows: $$A_t^{-1}=(aI+x_tx_t')^{-1}=A_{t-1}^{-1} - \frac{(A_{t-1}^{-1}x_t)(A_{t-1}^{-1}x_t)'}{1+x_t'A_{t-1}^{-1}x_t}$$ where $A_0^{-1}=\frac{1}{a}I$ and we can set $A_t = A_{t-1}+\sum_{t=1}^Tx_tx_t'$ , which will lead to time complexity of $O(n^2)$ . The above technique is mentioned here . The two implementation in $\texttt{R}$ are as follows: X <-matrix(runif(1000),20,10)
Y<-rnorm(20)
a<- 0.1

Cholsky<-function(X,Y,a){
  X <- as.matrix(X)
  Y <- as.matrix(Y)
  T <- nrow(X)
  N <- ncol(X)
  aI<- diag(a,N)
  bt<- matrix(0,ncol=1,nrow=N)
  for (t in 1:T){
    xt<-X[t,]
    At <- aI + (xt %*% t(xt))
    InvA<-chol2inv(chol(At))
    bt <- bt + (Y[t] * xt)
    theta<- InvA %*% bt
  }
  return(theta)
}
Cholsky(X,Y,a)


Morrison<-function(X,Y,a){
  X <- as.matrix(X)
  Y <- as.matrix(Y)
  T <- nrow(X)
  N <- ncol(X)
  At<-diag(1/a,N)
  bt<- matrix(0,ncol=1,nrow=N)
  for (t in 1:T){
    xt<-X[t,]
    At <- At + (xt %*% t(xt))
    InvA <- At - ((t(xt%*%At)%*%(as.matrix(xt%*%At)))
                    /as.numeric(xt%*%At%*%xt+1))
    bt <- bt + (Y[t] * xt)
    theta<- InvA %*% bt
  }
  return(theta)
}
Morrison(X,Y,a) They don't give the same result. So, perhaps I should not expect the implementations to be equivalent. I was wondering if I could invert the following (for the above case) more efficiently: $$A_t^{-1}=\left(D_{t-1}^{\frac{1}{2}}A_tD_{t-1}^{\frac{1}{2}}+aI\right)^{-1}$$ where $A_t=A_{t-1}+x_tx_t'$ and $A_0=\mathbf{0}$ . Essentially, I want to invert: $$M_t=\left(a\mathbf{I}+\sqrt{D_{{t-1}}}x_tx_t'\sqrt{D_{{t-1}}}\right)$$ I say: $$M_{t}^{-1}=M_{t-1}^{-1}-\frac{(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)(M_{t-1}D_{t-1}^{\frac{1}{2}}x_t)'}{1+x_t'D_{t-1}^{\frac{1}{2}}M_{t-1}D_{t-1}^{\frac{1}{2}}x_t}$$ where $D_0=diag(\mathbf{1}$ ). So, $M^{-1}_0=\frac{1}{a}I$ RLS identity given above $(aI+u_tv_t)^{-1}$ uses $u=A_{t-1}^{-1}x_t,v_t=u_t'$ ,
  I am using $u=M_{t-1}^{-1}D_{t-1}^{\frac{1}{2}}x_t,v_t=u_t'$ One may write the implementations in $\texttt{R}$ as follows: X <-matrix(runif(1000),20,10)
Y<-rnorm(20)
a<- 0.1

#Cholesky implementation
X <- as.matrix(X)
Y <- as.matrix(Y)
T <- nrow(X)
N <- ncol(X)
bt<- matrix(0,ncol=1,nrow=N)
At<- diag(0,N)
I<- diag(a,N);Mt<-diag(1/a,N)
theta0<- rep(1,N)
for (t in 1:2){
  xt<-X[t,]
  Dt <- diag(sqrt(abs(as.numeric(theta0))))
  At <- At + (xt %*% t(xt))
  Mt <-  I + (Dt%*%At%*%Dt)
  InvA <- chol2inv(chol( Mt )) 
  AAt<- Dt %*%InvA%*% Dt
  bt <- bt + (Y[t] * xt)
  theta0 <- AAt %*% bt
  print(theta0)
} Above is the correct implementation of the pseudo code. If I swap the following lines. I don't get the same answer. Mt <-  Mt + (Dt%*%At%*%Dt)
InvA<- Mt - ((t(xt%*%Dt%*%Mt)%*%(as.matrix(xt%*%Dt%*%Mt)))
                   /as.numeric(xt%*%Dt%*%Mt%*%t(as.matrix(xt%*%Dt))+1)) Why is that?","['matrices', 'numerical-linear-algebra', 'linear-algebra', 'inverse']"
