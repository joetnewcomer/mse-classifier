question_id,title,body,tags
4846805,Equivalent condition for the following monotone-like condition for multivariate function,"Let $f:\mathbb R^n\times \mathbb R^n\rightarrow \mathbb R$ be a continuously twice differentiable function. I am considering the following condition (1) on $f$ : For any $x,x'\in\mathbb R^n$ , If it is either $f(x,x')>f(x,x)$ or $f(x',x)>f(x,x)$ , we should have $f(x',x')\geq\max\{f(x,x'),f(x',x)\}$ . Equivalently, $$\tag{1}\max\{f(x,x'),f(x',x)\}>f(x,x)\Rightarrow f(x',x')\geq\max\{f(x,x'),f(x',x)\},~\forall x,x'\in\mathbb R^n.$$ My question is : Is there an equivalent condition to (1)? One possibility is that the monotonicity of partial derivatives is related to some of the inequalities in (1). More specifically, let the following inequalities be true for all $x,x'\in\mathbb R^n$ such that $x\neq x'$ : $$(x'-x)(D_xf(x,x')-D_xf(x,x))>0~\textrm{and}$$ $$(x'-x)(D_yf(x',x)-D_yf(x,x))>0~\textrm.$$ According to my previous question ( link ) this monotonicity implies the following: $$f(x,x')>f(x,x)\Rightarrow f(x',x')>f(x',x).$$ We have the relationship between $f(x',x')-f(x',x)$ and $f(x,x')-f(x,x)$ . But what is missing to answer (1) is the relationship between $f(x',x')-f(x,x')$ and $f(x,x')-f(x,x)$ , that I cannot find from my previous question. Any condition that reveals the relationship between $f(x',x')-f(x,x')$ and $f(x,x')-f(x,x)$ ? Any comment and answer will be highly appreciated.","['multivariable-calculus', 'functions', 'monotone-functions']"
4846870,Does $\lim_{x \to 1}\left(\frac{x}{[x]}\right)$ exist?,"Does the limit $\lim_{x \to 1}\left(\frac{x}{\lfloor x\rfloor}\right)$ exist? Where $\lfloor x\rfloor$ is the Greatest Integer Function or the Floor function. My teacher defined that a limit exists when the Left-hand Limit = Right-Hand Limit and If any one is undefined but the other is finite then also the limit exists. Like $\lim_{x \to 0}\sqrt{x}$ does exist although the LHL does not lie in the domain. In this question the domain of the function $\frac{x}{\lfloor x\rfloor}$ is $\mathbb R\setminus[0,1)$ where $\mathbb R$ represents Real Numbers. So, The Left-Hand Limit is not defined and Right Hand Limit is $1$ , which is finite so according to my teacher Limit does exist. But when I tried this question in WolframAlpha, it is saying that the LHL is $-\infty$ and RHL is $1$ , so limit does not exist. So, a more general question is: do we have to consider the domain of a function when we calculate a limit?","['limits', 'calculus', 'limits-without-lhopital']"
4846882,Can the definition of almost periodic functions be simplified?,"A bounded continuous function $f : \mathbb{R} \to \mathbb{C}$ is almost periodic if for every $\epsilon>0$ , there exists some $L>0$ , such that every interval of $\mathbb{R}$ with length $\ge L$ contains some real number $T$ such that $\Vert f(\cdot+T)-f \Vert_\infty \le \epsilon$ . An equivalent definition is the relative compactness of the set of all functions $f(\cdot+T)$ with $T$ varying in $\mathbb{R}$ in the space of all continuous bounded functions endowed with $\Vert \cdot \Vert_\infty$ . It is necessary to have $$\liminf_{T \to \infty} \Vert f(\cdot+T)-f \Vert_\infty = 0.$$ Is it also sufficient? My thought: the triangle inequality and the invariance of the norm $\Vert \cdot \Vert_\infty$ under translations show that the function $T \mapsto \Vert f(\cdot+T)-f \Vert_\infty$ is sub-additive on $\mathbb{R}_+$ .","['almost-periodic-functions', 'functional-analysis']"
4847075,"Example of Borel measure on R which is not Borel regular, but have finite value on all compact sets?","The answer below this question: Example of a Borel measure, which is not Borel-regular provides an example of Borel-irregular measure. Here, I am asking a harder question: Can we find a Borel measure $\mu$ (on $\mathbb R$ or on any other space) that is NOT Borel regular, but for every compact set $K,$ $\mu(K)$ is finite? Easy examples like the counting measure are not going to work here.","['borel-measures', 'measure-theory', 'functional-analysis', 'real-analysis']"
4847106,How to use axiom of extensionality,Assume $x = y$ and we want to proof $\{x\} = \{y\}$ . I read that this follows from the axiom of extensionality. The axiom states: $\forall x \forall y (\forall z( z \in x \iff z \in y) \implies x = y)$ . Obviously $z \in \{x\} \iff z = x \iff z = y \iff z \in \{y\}$ so $\{x\} = \{y\}$ . Am I doing this correctly or am I assuming things I can't directly assume? For example do I need to proof that $z = x \iff z = y$ with axioms or is this just a consequence of substitution.,"['elementary-set-theory', 'first-order-logic', 'set-theory']"
4847148,Behaviour of the solution of a second order ODE,"I am currently studying the following second order ODE \begin{cases}
\ddot y(x)\left(\ln(x) - 2\ln(y(x))\right) - 2\frac{(\dot y(x))^2}{y(x)} = 0 &\text{in }[0,T]\\
y(0) = 0\\
\dot y(T) = c
\end{cases} for $c$ a positive constant and $T > 0$ and arbitrarily small constant. Clearly, solving this ODE explicitly is not doable. But actually for my purpose I don't need a whole solution but rather to understand the behaviour of $\dot y(x)$ near $0$ . I would like to know if $\lim_{x \to 0} \dot y(x) = 0 $ or not. Being not really familiar with the tools used in ODE theory, I was wondering if one of you guide me on this topic. I solved the ODE numerically using Maple and the software gave me the following plots where I fixed $T = 1/2$ and the initial conditions set at $c = 1, 1/2$ and $1/3$ corresponding to $1, 2$ and $3$ respectively. It seems that the slope at the origin is always strictly positive, whatever initial condition I set (if I decrease $c$ or $T$ , the behaviour remains the same). However, I have absolutely no idea how to do that. Any help?","['ordinary-differential-equations', 'graphing-functions', 'real-analysis', 'calculus', 'derivatives']"
4847202,Show that $F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t$ is holomorphic in $\mathbb{C}$,"I am trying to show that $$F(z)=\int_{-\infty}^{\infty} e^{-t^2+4 z t} d t$$ is a holomorphic function in $\mathbb{C}$ . The idea is to use the following theorem for parametric integrals: Suppose $I$ is an interval, possibly unbounded, $A \subset \mathbb{C}$ is an open subset and suppose $\phi(t,z)$ is defined on $(I\backslash P) \times A$ , where $P=\{t_1,\dots,t_N\}$ . Suppose that for each $t \in (I\backslash P)$ , $\phi(t,z) \in H(A)$ and $\phi, D_z \phi \in C((I\backslash P) \times A)$ . Suppose further that for each $K \subset A$ a compact subset, there exists $F_K$ such that $$|\phi(t,z)| \leq F_K(t), \quad \forall t \in I \backslash P, z \in K,$$ where $$\int_{I} F_K(t) dt < \infty.$$ Then if $f(z)= \int_{I} \phi(t,z) dt$ then $f \in H(A)$ and $f^\prime (z)= \int_{I} D_z \phi(t,z) dt$ . ATTEMPT Let $\phi(t,z)=e^{-t^2+4zt}$ . Obviously $\phi(t,\cdot) \in H(\mathbb{C})$ for $t$ fixed, and $\phi, D_z \phi$ are continuous in $\mathbb{R} \times \mathbb{C}$ . Let $K \subset \mathbb{C}$ be a compact subset. We know that $$|\phi(t,z)|=e^{-t^2+4 Re(z)t}.$$ Now we can suppose that $K \subset \{z: \alpha < Re(z) < \beta\}$ . The problem in trying to bound $|\phi(t,z)|$ is that $t$ can be both negative and positive, and so can $\alpha$ and $\beta$ , depending on where the compact is located in the complex plane. For example: If $K$ is in the half-plane where $Re(z) >0$ , then $\alpha,\beta >0$ , and if $t <0$ , then $4Re(z)t <0$ , and thus $$|\phi(t,z)| \leq e^{-t^2},$$ and it is well known that $\int_{0}^{\infty} e^{-t^2} dt< \infty$ . So we want to define $F_K$ as $$F_K=\left\{ \begin{array}{cl}
e^{-t^2} & : \ t < 0 \\
\text{something} & : \ t \geq 0
\end{array} \right.$$ The problem is that, for $t \geq 0$ I can't seem to find a good bound to get a convergent integral, and by working like this it seems that I will have to do many cases by exhaustion. Is there a simpler way? I know of the way using Morera and Fubini, but that is not the objective here.","['integration', 'complex-analysis', 'convergence-divergence', 'parametric']"
4847211,To what extent will Radon-Nikodym's theorem hold if we do not admit axiom of choice?,"In the proof of the Radon-Nikodym theorem, the function $f = \frac{\mathrm{d} \nu}{\mathrm{d} \mu}$ is constructed as the supremum of measurable functions satisfying $\int_A f \mathrm{d} \mu \leq \int_A \mathrm{d}\nu$ , and the existence of this supremum is by Zorn's lemma, an equivalent statement of the axiom of choice (AC), which is not admitted by many mathematicians. So, does Radon-Nikodym's theorem still hold if we don't admit AC? And if it doesn't, then can we get a similar statement like Radon-Nikodym's theorem (probably weaker)?","['axiom-of-choice', 'measure-theory', 'probability-theory', 'real-analysis']"
4847223,Find $\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1}$.,"I'm having trouble finding $\lim\limits_{x \to 0} \frac{ \sqrt{1+x} - 1} { \sqrt[3]{1+x} - 1}$ . Here's my attempt: $$
\lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1} 
= \lim_{x \to 0} \frac{\sqrt{1+x} - 1}{\sqrt[3]{1+x} - 1} 
\cdot \frac{\sqrt{1+x} + 1}{\sqrt{1+x} + 1} 
= \lim_{x \to 0} \frac{x}{(\sqrt[3]{1+x} - 1) 
\cdot (\sqrt{1+x} + 1)}
$$ I'm having trouble getting rid of the given expression in the denominator. My professor mentioned multiplying with $$
\frac{\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1}
{\sqrt[3]{(1+x)^2} + \sqrt[3]{1+x} + 1}
$$ because of the factorization of $a^n-b^n$ (???) but even when I do that I don't get the correct result, which is $\frac{3}{2}$ . Any alternative ways to solve this would be appreciated, if someone could explain the professors logic it would also be of great help. Thanks.","['limits-without-lhopital', 'real-analysis', 'calculus', 'limits', 'radicals']"
4847239,Is the following discrete variant of Miranda's theorem correct?,"The Intermediate Value Theorem says that, if $f$ is a continuous function on $[0,1]$ , and $f(0)<0<f(1)$ , then there is an $x\in[0,1]$ such that $f(x)=0$ . The following theorem can be seen as a discrete version of it. Suppose we partition the interval $[0,1]$ into any number of intervals. If $f(0)<0<f(1)$ , then there is an interval $[x,y]\subset [0,1]$ on which $f$ switches signs, that is, $f(x)\leq 0\leq f(y)$ . Note that this theorem holds even when $f$ is not continuous. Miranda's theorem is a multi-dimensional extension of the Intermediate Value Theorem. Let $f:[0,1]^d\to \mathbb{R}^d$ be a continuous function such that $f_i(x)<0$ when $x_i=0$ and $f_i(x)>0$ when $x_i=1$ , for all $i\in[d]$ . Then there is a point $x\in[0,1]^d$ for which $f_i(x)=0$ for all $i\in[d]$ . I am looking for a discrete version of Miranda's theorem. Suppose we partition the box $[0,1]^d$ into a grid of arbitrary size, and suppose $f_i(x)<0$ when $x_i=0$ and $f_i(x)>0$ when $x_i=1$ , for all $i\in[d]$ . Does there exist a grid cell in which every function $f_i$ switch signs? That is: for all $i\in[d]$ , there exists a corner in which $f_i\leq 0$ and a corner in which $f_i\geq 0$ (not necessarily the same corner for all $i$ )? Is there a similar theorem that can be seen as a discrete version of Miranda's theorem?","['general-topology', 'discrete-mathematics']"
4847258,"Find all natural numbers $x,y,z$ such that $x^7+y^7=7^z$","Find all natural numbers $x,y,z \in \mathbb{N}$ such that $x^7+y^7=7^z$ . Firstly, I note that given that the right hand side only has factors of $7$ , it must be odd and so $x^7+y^7$ must be odd i.e $x^7$ and $y^7$ have opposite parity, which then means $x$ and $y$ have opposite parity. Also, I have got the following: $$
x^7+y^7=7^z \implies x^7+y^7\equiv0\pmod{7}
$$ By Fermat's Little Theorem, we have $x^7\equiv x\pmod{7}$ and similarly for $y$ , $y^7\equiv y\pmod{7}$ , so then $x^7+y^7\equiv0\pmod{7}\implies x+y\equiv0\pmod{7}$ . I have also factorised $x^7+y^7$ : $$
x^7+y^7=(x+y)(x^6-x^5y+x^4y^2-x^3y^3+x^2y^4-xy^5+y^6)=7^z
$$ So not only does $7$ divide $x+y$ , we also have that $7\mid(x^6-x^5y+x^4y^2-x^3y^3+x^2y^4-xy^5+y^6)$ but at this point I feel like I am just not making any real progress. Any help is appreciated.","['exponential-diophantine-equations', 'number-theory', 'diophantine-equations']"
4847262,Prove an inequality using Jensen's inequality,"Let $Q\in M_1(M_1(\Sigma)),$ whereby $M_1(\Sigma)$ denotes the space of probability measures on $\Sigma.$ We define a new measure $\mu_Q (\Gamma) = \int_{M_1(\Sigma)} \nu(\Gamma)Q(d\nu), \Gamma \in B_{\Sigma},$ the Borel sigma algebra on $\Sigma, \nu \in M_1(\Sigma).$ Let $V: \Sigma \rightarrow [0, \infty]$ be bounded and measurable function. I need to prove the following inequality $$\int_{M_1(\Sigma)} \exp[\int_{\Sigma} Vd\nu] Q(d\nu)\ \leq \int_{\Sigma} \exp[V] d\mu_Q.$$ Given that the exponential is convex, I use the Jensen's inequality to get for the inner integral, $$\exp[\int_{\Sigma} V d\nu] \leq \int_{\Sigma} \exp[V] d\nu.$$ Inserting this into the integral expression we get, $$\int_{M_1(\Sigma)} \exp[\int_{\Sigma} Vd\nu] Q(d\nu)\ \leq\int_{M_1(\Sigma)}\int_{\Sigma}\exp[V] d\nu Q(d\nu).$$ I do not know how to go from here. I guess one may write $d\mu_Q = \nu Q(d\nu),$ given the integral definition of $\mu_Q.$ But inserting $Q(d\nu) = \frac{1}{\nu} d\mu_Q,$ is not providing the result. One should very probably use Fubini, but even this did not convince me how to correctly provide the final result. Can somebody help ? Thanks.","['measure-theory', 'probability-distributions', 'jensen-inequality', 'probability-theory']"
4847270,Proof check that a given metric on $\mathbb{S}^1\times\mathbb{R}$ is not time-orientable,"I want to prove that there exist a non time-orientable Lorentzian metric on the manifold $M=\mathbb{S}^1\times\mathbb{R}.$ I have an idea on how to do this but since it's the first time I approach this kind of exercises I would like to have a check of my reasoning... Let $\theta$ be the coordinate on $\mathbb{S}^1$ and $t$ be the coordinate on $\mathbb{R}:$ my intuition suggests to consider the metric given by the matrix $$\begin{pmatrix}\cos\theta & \sin\theta\\ \sin\theta & -\cos\theta\end{pmatrix}$$ on the tangent space $T_{(\theta,t)}M$ for each $\theta$ and $t:$ this metric is Lorentzian, since for each choice of the coordinates this matrix has signature $(1,-1).$ I know that a metric is time-orientable if and only if there exists a vector field $X$ on $M$ such that $X_p$ is timelike for each $p\in M:$ to conclude I then have to prove that such a $X$ cannot exist. Working in coordinates such an $X=(X_1,X_2)$ should verify the relation $$(X_1,X_2)\begin{pmatrix}\cos\theta & \sin\theta\\ \sin\theta & -\cos\theta\end{pmatrix}\begin{pmatrix}X_1\\X_2\end{pmatrix}<0$$ for each $\theta$ and $t$ (the metric is $t-$ indipendent but $X_1,X_2$ are functions of $\theta$ and $t$ ): a simple computation then gives $$(X_1^2-X_2^2)\cos\theta+2X_1X_2\sin\theta<0$$ The function on the LHS is continuous: since for $\theta=0$ and $\theta=\pi$ we respectively get $$X_1^2-X_2^2$$ $$X_2^2-X_1^2$$ we find that this value cannot alway be negative, hence a timelike vector field $X$ cannot exists. Is this reasoning correct or are there some mistakes? In this case how can I correct them?","['semi-riemannian-geometry', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4847271,What's the probability of stumbling onto a Hamiltonian path?,"Suppose we have a graph $G$ , which for sake of convenience we'll require to be vertex-transitive. Then there's a natural notion of a random path: start at any vertex $v_1$ (and this is where the vertex-transitivity is used), and from a vertex $v_i$ choose $v_{i+1}$ with equal probability from all the neighbors of $v_i$ that haven't been visited yet. If $v_i$ has no unchosen neighbors, the path terminates. If $G$ has any Hamiltonian paths, then clearly there is a non-zero probability that a path chosen this way will be Hamiltonian; the question is, what is the probability that it is? There are a couple of trivial cases: if we have a cycle graph $C_n$ or a complete graph $K_n$ then the probability is clearly $1$ . (For $C_n$ once we've made our first choice we have no other choices to make, and for $K_n$ as long as there are unused vertices there's a path from that vertex.) But already for a graph as simple as $D_{2n}$ (two copies of $C_n$ with edges between corresponding nodes) it's not clear to me what the probability is, or even what the asymptotics are. Has this probability been studied, either for specific graphs $G$ (the Petersen graph immediately springs to mind) or for families of graphs like $D_{2n}$ or $\mathbb{Z}_2^n$ ? Is there any known characterization of the graphs where a Hamiltonian path will always be found this way? Does anyone have references to similar problems? Many thanks in advance!","['graph-theory', 'combinatorics', 'probability', 'hamiltonian-path']"
4847272,Why does Python's calculation of $0^0$ include the digits of pi?,"When you run the following Python code, which is equivalent to $\lim_{x \to 0^+} x^x$ , x = 0.0000000001; print(x ** x) The output will correctly approach 1. (The output is 0.9999999976974149 ). But when you change: x = -0.0000000001 Which would be equivalent to $\lim_{x \to 0^-} x^x$ , the following output is produced: (1.000000002302585-3.141592660823578e-10j) This is also correct but why do I see an approximation of $\pi$ ? Surely this can't be a coincidence. Is it the side product of an algorithm that's being used? Or is there a purely mathematical reason for it? Edit: I only tested it in python. I don't know if similar outputs arise in other programming languages.","['limits', 'computer-science']"
4847281,Prove the sign and zeroes of $Ax^2 + 2Bxy + Cy^2$ (without using the second derivative test),"Let $$P(x,y) = Ax^2 + 2Bxy + Cy^2 \\A \neq 0 \quad x,y \in \mathbb R.$$ Without using the second derivative test, prove that If $AC - B^2 > 0$ , then (i) $P$ has no zeroes outside the origin and (ii) $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ for all $x,y$ If $AC - B^2 = 0$ , then (i) $P$ has zeroes outside the origin and (ii) outside these zeroes, $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ If $AC - B^2 < 0$ , then (i) $P$ has zeroes outside the origin (ii) there exist $x,y$ such that $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ and (iii) there also exist $x,y$ such that $\operatorname{sgn}(P) = -\operatorname{sgn}(A)$ Please note: The second derivative test of calculus is not available, as this question is intended as a lemma for proving the second derivative test This post is related but distinct My proof is below: I request verification, critique, or alternate proofs. Let $$f(x,y) = (x -\frac B A y)^2 \\g(y) = \left(\frac y A \right)^2.$$ Then $$P(x,y) = A \cdot [f(x,y) + (AC-B^2)g(y)]\\ \operatorname{sgn}(P(x,y)) = \operatorname{sgn}(A) \cdot \operatorname{sgn}[f(x,y) + (AC-B^2)g(y)].$$ Observe that the range of both $f$ and $g$ is $[0, \infty)$ . Case 1: If $AC - B^2 > 0$ , then $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ unless both $f$ and $g$ are zero.  But $g(y) = 0$ implies $y = 0$ , and $f(x,0) = 0$ implies $x = 0$ , so this can only happen at the origin. Case 2: If $AC - B^2 = 0$ , then $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ unless $f$ is zero, which happens for infinite values of $x, y$ . Case 3: If $AC - B^2 < 0$ , then for $|x| \gg |y|$ , the first term dominates, and $\operatorname{sgn}(P) = \operatorname{sgn}(A)$ , and for $|y| \gg |x|$ , the second term dominates, and $\operatorname{sgn}(P) = -\operatorname{sgn}(A)$ .  Furthermore, for any fixed $y$ , then varying $x$ causes $f$ to take on its entire range, so there exist infinite values of $x,y$ which make $P$ zero.","['discriminant', 'multivariable-calculus', 'calculus', 'polynomials', 'optimization']"
4847295,Bounds for recursive binomial process,"I have a sequence of binomial random variables with the following law: $$
Z_1 = \text{Bin}(n,p) \text{ and } Z_k = \text{Bin} (Z_{k-1}, p)
$$ I would like to bound the quantity $\frac{Z_k}{Z_1}$ . By the law of total expectation, we can find that $E[Z_k] = E[E[Z_k|Z_{k-1}]] = \dots = p^{n-1}E[Z_1]$ , intuitively we can see that $Z_k$ is roughly on the order of $Zp^{n-1}$ I would like to derive a bound that either confirms, or rejects, my intuition, i.e. a bound of the form $$P(Z_k \leq Z_1p^{n-1}) \geq b_k$$ where $b_k$ is some (currently) unknown term. I believe that a bound based off of Azuma's inequality leads to non-informative bound.","['stochastic-processes', 'probability-theory']"
4847301,Various definitions of cartesian products that result in different solutions,"My question is about the cartesian product. There are several definitions out there that do not seem to describe the same object, a cartesian product. The most general definition according to wikipedia is this one: $$
\prod_{i \in I} A_i 
:= \biggl\{ f: I \to \bigcup_{i \in I} A_i 
\;\bigg|\; \forall i \in I: f(i) \in A_i \biggr\}. 
$$ Let's apply it to $\{1,2\} \times \{1,2\} = \{1,2\}^2$ and let us use $I = \{a,b\}$ as the index set. Applying the definition gives us $$
\bigl\{ \{(a,1),(b,1)\}, \{(a,1),(b,2)\}, 
\{(a,2),(b,1)\}, \{(a,2),(b,2)\} \bigr\}.
$$ Now let us look at another definition which is more popular: $$
\prod_{i=1}^n A_i 
= \bigl\{ (a_1, \dots, a_n) \;\big|\; 
a_i \in A_i \text{ for } i = 1, \dots, n\}.
$$ In this case our example gives us $$
\bigl\{(1,1), (1,2), (2,1), (2,2) \bigr\}.
$$ So for $\{1,2\}^2$ we get two different sets, depending on the definition we use. Why is that no problem? So what is the cartesian product of $\{1,2\}^2$ ? Maybe this is a general misunderstanding of me on how definitions work?","['elementary-set-theory', 'set-theory']"
4847344,When and how do we dehomogenize a homogeneous function?,"When and how do we dehomogenize a homogeneous function? To solve Prove the sign and zeroes of $Ax^2 + 2Bxy + Cy^2$ (without using the second derivative test) , ""user"" set $$t = \frac x y$$ and wrote $$Ax^2 + 2Bxy + Cy^2 =y^2\left(At^2+2Bt+C\right)$$ to solve the problem. Where did that come from? Ted Shifrin wrote When you study a homogeneous function, it is natural to dehomogenize by evaluating $f(x,y)$ at either $(1,y/x)$ or $(x/y,1)$ I'd like to learn more about this technique, and am unable to find a reference here or via Google.  What does it mean to dehomogenize a function? When and how do we do it? Update Although I'm still looking for a reference, I worked out a ""derivation"" in this case, to which I ask for feedback and verification: Goal: Characterize the sign of $$P(x,y) = Ax^2 + 2Bxy + Cy^2$$ on $\mathbb R^2 - \{(0,0)\}$ . Approach: Before attempting any algebra, understand $P$ qualitatively .  Two things are clear: (i) When $|x|$ is large, $A$ dominates, and when $|y|$ is large, $C$ dominates; it's only when $|x| \approx |y|$ that things are messy (ii) The absolute magnitude of $x$ and $y$ are irrelevant; it's only their ratio that matters. This suggests that we divide by $x^2$ , which will preserve the sign (which is what we care about) and perhaps replace absolute magnitude with ratio.  (We can handle $x = 0$ later.) $$P(x,y) =  {x^2} (A + B \frac y x + C \frac {y^2} {x^2}) \\
\operatorname{sgn}[P(x,y)] =  \operatorname{sgn}(A + B \frac y x + C \frac {y^2} {x^2})$$ which is a polynomial in $\frac y x$ , whose range of signs is obvious from its discriminant. Furthermore, we can assume WLOG that $x \neq 0$ , since if $x = 0$ , then $y \neq 0$ , and $\operatorname{sgn} P$ is the same by symmetry.","['multivariable-calculus', 'calculus', 'homogeneous-equation', 'real-analysis']"
4847356,Derivatives of morphisms of linear algebraic groups,"I am currently trying to learn about linear algebraic groups and their lie algebra structure. However, I am struggling to explicitly calculate the derivatives of morphisms between algebraic groups, as soon as they are not explicitly given by polynomial functions. (e.g. det: $GL_n \to G_m$ is clear for me as you can just differentiate and get the trace). However, if you take e.g. quotient maps, I don't know how to find derivatives in a kind of ""algorithmic"" way. Take for example Exercise 4.4.11(3) in Springer's book on Linear Algebraic groups. There, he gets the map $\varphi: SL_2 \to PSL_2$ by firstly taking the inclusion $SL_2 \to GL_2$ and then the quotient map $GL_2 \to GL_2/Z(GL_2)$ . How would one go about calculating the derivative $d\varphi$ to show that it is an isomorphism in this case (for char(k) not 2)?","['group-schemes', 'algebraic-geometry', 'algebraic-groups']"
4847358,Question about function of two variables.,"The function is: $$
f(x,y)=\begin{cases}
xy^3 \sin \left(\frac{1}{{x^4+y^4}}\right), & \text{if } (x,y) \neq (0,0) \\
0, & \text{if } (x,y) = (0,0)
\end{cases}.$$ I have to check if it is of class $C^1$ . I've found that $d_xf(0,0) = 0$ and $d_{y} f(0,0) = 0$ I know how to prove by definition it's differentiable at the origin and obviously it's differentiable elsewhere.
But I don't know how to prove that partial derivatives are/are not continuous.
For example i get $d_xf(x,y) = y^3\sin\left(\frac{1}{x^4+y^4}\right) - \frac{(4x^4y^3)\cos\left(\frac{1}{x^4+y^4}\right)}{(x^4+y^4)^2}$ Any kind of help would be appreciated.","['partial-derivative', 'continuity', 'multivariable-calculus']"
4847369,Alternative to Rudin's method to prove Q does not have the least upper bound property,"I came across a proof that that shows that if x is the supremum of the set of rationals such that $x > 0$ and $x^2 < 2$ , then $x > 1$ and $x^2 = 2$ . This is a similar problem to the beginning of Rudin's famous textbook. The part of this proof that I cannot see how to do is to derive two definitions of h. There are two definitions of h used, one used to prove by contradiction that $x^2$ is not less than 2, and the other to prove by contradiction that $x^2$ is not greater than 2. I cannot see how to derive the equations used for h. There are other ways to define h, but I am trying to specifically derive these definitions so I can follow the rest of the text. Here is how it is written: Suppose $x \in Q$ and $x$ = supremum of E = $\{q \in Q: q>0$ and $q^2 < 2\}$ Then, $x \ge 1$ and $x^2 = 2$ Pf: Showing $x \ge 1$ : Since $1 \in$ E and $x = sup$ E , $x \ge 1$ Showing $x^2\ge 2$ (by contradiction): Assume $x^2 < 2$ . Define $h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\}$ Then, if $x^2 < 2$ then $h > 0$ We now prove that $x + h \in E$ . Indeed, $(x + h)^2 = x^2 + 2xh + h^2 < x^2 + h(2x + 1)$ as $h < 1$ . Hence $(x + h)^2 \le x^2 + (2 - x^2)\cdot \dfrac{2x + 1}{2(2x+1)}$ $= x^2 + \dfrac{2-x^2}{2} < 2 + \dfrac{2-2}{2} = 2$ Therefore, $x + h \in E$ and $x + h > x$ so x is not an upper bound for E. Therefore, x is not sup(E), which is a contradiction. Hence, $x^2 \ge 2$ . Showing $x^2\le 2$ (by contradiction): We now prove that $x^2 \le 2$ . Suppose $x^2 > 2$ . Let $h = \dfrac{x^2-2}{2x}$ . Then $h>0$ and $x-h > 0$ .  Showing that x - h is an upper bound for E: We have $(x - h)^2 = x^2 - 2xh + h^2$ $= x^2 - (x^2 - 2) + h^2$ $= 2 + h^2$ $> 2$ . Thus for all $q \in E$ , $q < x-h < x$ so $x$ is not the supremum of E, which contradicts assumption. Therefore, since as per above $x^2 \ge 2$ and $x^2 \le 2$ , we conclude that $x^2 = 2$ . Question about the above proof I follow all of the above and can write the proof in an alternative way with h defined as $1/n$ , $n \in N$ , but I cannot seem to derive these formulas for h: $h = min\{\frac{1}{2}, \frac{2-x^2}{2(2x+1)}\}$ and $h = \dfrac{x^2-2}{2x}$ I am probably making some kind of algebraic error or not seeing an inequality that induces the definitions. Can someone help me derive these definitions/formulas for h in a step by step manner? Many thanks.","['analysis', 'real-analysis']"
4847389,Problems in $p$-adic numbers accessible to an early undergraduate,"I am going for a research internship this summer in $p$ -adic numbers. I am currently taking a number theory course, and have the essentials of a first year mathematics student, like multivariable calculus, linear algebra, probability, and a proofs course. I was curious to know if there are any problems in $p$ -adic numbers that are accessible to an undergraduate like me. Note that I am very motivated, and am willing to self-study abstract algebra and analysis as required for such problems (I have already begun with Fraleigh). Thank you!","['number-theory', 'p-adic-number-theory', 'algebraic-number-theory', 'research']"
4847405,Confusion about spectral measure of operator in tracial von Neumann algebra,"The setting is that $(M, \tau)$ is a tracial von Neumann algebra ( $\tau$ is a faithful, normal tracial state) that is a subset of the bounded operators on a Hilbert space $H$ . Let $x \in M$ be a self-adjoint operator. Let $\mu_x$ be the spectral measure of $x$ (with respect to $\tau$ ). I am a bit confused about where the wrong step in the following ""proof"" that if $x$ is not invertible (i.e. $0 \in \sigma(x)$ ) then $\mu_x(\{0\}) > 0$ . If this is true, then by considering $x - \lambda$ for $\lambda \in \sigma(x)$ , it seems we would get that $\sigma(x)$ could only contain countably many points, which is false in general. Suppose that $x$ does not have a bounded inverse in $M$ , or equivalently that $x$ is not bijective. For a self adjoint $x \in M$ , it is a fact that $\chi_{\mathbb{R} \setminus \{0\}}(x)$ is equal to the projection onto the closure of the image of $x$ (or, equivalently the orthogonal complement to the kernel of $x$ ). Hence, for $x$ to not be invertible, $\chi_{\mathbb{R} \setminus \{0 \}}(x) \neq 1$ . As $\chi_{\mathbb{R} \setminus \{0\}}(x) + \chi_{\{0\}}(x) = 1$ , this means that $\chi_{ \{0\}}(x)$ is a non-zero projection. From faithfulness of $\tau$ , then $\tau(\chi_{ \{0\}}(x)) > 0$ and hence $\mu_x(\{0\}) > 0$ .","['von-neumann-algebras', 'functional-analysis', 'operator-algebras']"
4847406,"Calculate $I = \int_{-1}^1 \log(1-x) \, \log(1+x) \, dx$","Question $$I = \int_{-1}^1 \log(1-x) \, \log(1+x) \, dx$$ My try $$
I = \int_{-1}^1 \log(1-x) \, \log(1+x) \, dx \\
= \int_{-1}^1 \log(1-x) \, \left(\log2 + \sum_{n=1}^{+\infty} \frac{(-1)^{n-1}}{2^n \, n} \, (x-1)^n \right) \, dx \\
= \log2 \int_{-1}^1 \log(1-x) \, dx + \int_{-1}^1 \log(1-x) \, \sum_{n=1}^{+\infty} \frac{(-1)^{n-1}}{2^n \, n} \, (x-1)^n \, dx \\
= \log2 \, (2\log2 - 2) + \sum_{n=1}^{+\infty} \left( \frac{(-1)^{n-1}}{2^n \, n} \int_{-1}^1 (x-1)^n \, \log(1-x) \, dx \right) \\
\overset{\begin{subarray}{c} t=1-x \\ dx=-dt \end{subarray}}{=}\, 2\log2 \, (\log2 - 1) + \sum_{n=1}^{+\infty} \left( \frac{(-1)^{n-1}}{2^n \, n} \int_{0}^2 (-t)^n \, \log{t} \, dt \right) \\
= 2\log2 \, (\log2 - 1) + \sum_{n=1}^{+\infty} \left( \frac{(-1)^{n-1}}{2^n \, n} \int_{0}^2 (-1)^n t^n \log{t} \, dt \right)
$$","['integration', 'calculus', 'definite-integrals', 'real-analysis']"
4847473,"Covering number of the sphere (Vershynin): $\mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n$","My reference is the text surrounding Lemma $5.2$ in Introduction to the non-asymptotic analysis of random matrices by Roman Vershynin. Definition. Let $(X, d)$ be a metric space and let $\varepsilon>0$ . A subset $\mathcal{N}_{\varepsilon}$ of $X$ is called an $\varepsilon$ -net of $X$ if every point $x \in X$ can be approximated to within $\varepsilon$ by some point $y \in \mathcal{N}_{\varepsilon}$ , i.e. so that $d(x, y) \leq \varepsilon$ . The minimal cardinality of an $\varepsilon$ -net of $X$ , if finite, is denoted $\mathcal{N}(X, \varepsilon)$ and is called the covering number of $X$ (at scale $\varepsilon)$ . Equivalently, $\mathcal{N}(X, \varepsilon)$ is the minimal number of balls with radii $\varepsilon$ and with centers in $X$ needed to cover $X$ . Suppose we have an $\varepsilon$ -net that covers $X$ in the optimal way , i.e., the number of balls used is $\mathcal{N}(X, \varepsilon)$ . Are these balls always disjoint? I don't think so. The following is the text of the main lemma, concerning covering numbers of the sphere. Lemma $5.2$ (Covering numbers of the sphere). The unit Euclidean sphere $S^{n-1}$ equipped with the Euclidean metric satisfies for every $\varepsilon>0$ that $$
\mathcal{N}\left(S^{n-1}, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n
$$ Proof. This is a simple volume argument. Let us fix $\varepsilon>0$ and choose $\mathcal{N}_{\varepsilon}$ to be a maximal $\varepsilon$ -separated subset of $S^{n-1}$ . In other words, $\mathcal{N}_{\varepsilon}$ is such that $d(x, y) \geq \varepsilon$ for all $x, y \in \mathcal{N}_{\varepsilon}$ , $x \neq y$ , and no subset of $S^{n-1}$ containing $\mathcal{N}_{\varepsilon}$ has this property. One can in fact construct $\mathcal{N}_{\varepsilon}$ inductively by first selecting an arbitrary point on the sphere, and at each next step selecting a point that is at distance at least ${\varepsilon}$ from those already selected. By compactness, this algorithm will terminate after finitely many steps and it will yield a set $\mathcal{N}_{\varepsilon}$ as we required. How does compactness imply the algorithm's termination in finitely many steps? The maximality property implies that $\mathcal{N}_{\varepsilon}$ is an $\varepsilon$ -net of $S^{n-1}$ . Indeed, otherwise there would exist $x \in S^{n-1}$ that is at least $\varepsilon$ -far from all points in $\mathcal{N}_{\varepsilon}$ . So $\mathcal{N}_{\varepsilon} \cup\{x\}$ would still be an $\varepsilon$ -separated set, contradicting the maximality property. Moreover, the separation property implies via the triangle inequality that the balls of radii $\varepsilon / 2$ centered at the points in $\mathcal{N}_{\varepsilon}$ are disjoint. On the other hand, all such balls lie in $(1+\varepsilon / 2) B_2^n$ where $B_2^n$ denotes the unit Euclidean ball centered at the origin. Comparing the volume gives $\operatorname{vol}\left(\frac{\varepsilon}{2} B_2^n\right) \cdot\left|\mathcal{N}_{\varepsilon}\right| \leq \operatorname{vol}\left(\left(1+\frac{\varepsilon}{2}\right) B_2^n\right)$ . Since $\operatorname{vol}\left(r B_2^n\right)=r^n \operatorname{vol}\left(B_2^n\right)$ for all $r \geq 0$ , we conclude that $\left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n$ as required. This is probably straightforward, but I want to make sure I have the details correct. The conclusion follows from $\mathcal{N}(X, \varepsilon) \le \left|\mathcal{N}_{\varepsilon}\right| \leq\left(1+\frac{\varepsilon}{2}\right)^n /\left(\frac{\varepsilon}{2}\right)^n=\left(1+\frac{2}{\varepsilon}\right)^n$ , right? I ask because $\mathcal{N}(X, \varepsilon) < \left|\mathcal{N}_{\varepsilon}\right|$ is a possibility. Wouldn't the same proof work for $B^n_2$ also? I don't see why anything would change. I believe $\mathcal{N}\left(B^n_2, \varepsilon\right) \leq\left(1+\frac{2}{\varepsilon}\right)^n$ is also correct. Thanks for your thoughts and help!","['euclidean-geometry', 'spheres', 'geometry', 'analysis', 'compactness']"
4847479,What is the value of $x$ when $\frac{1}{4\sqrt{x}}=\frac{-1}{5}$?,"Q) What is the value of $x$ when $\frac{1}{4\sqrt{x}}=\frac{-1}{5}$ ? Ans) First of all let me tell that I know how to find the value of $x$ in the above equation. $\frac{1}{4\sqrt{x}}=\frac{-1}{5}$ $\implies \sqrt{x}=-\frac{5}{4}$ $\implies x=(-\frac{5}{4})^{2}$ $\implies x=\frac{25}{16}$ . My doubt : I can't understand that how is it possible that $\sqrt{\frac{25}{16}}=\frac{-5}{4}$ because we know that $\sqrt{x^{2}}=|x|$ . Therefore, $\sqrt{\frac{25}{16}}=|\frac{5}{4}|=\frac{5}{4}$ . Please help me out with this equation. For e.g.- when we solve equations like: Find the value of $x$ when $x^{2}=16$ . Generally for finding the value of $x$ we take square root on both sides. $x^{2}=16$ $\implies \sqrt{x^{2}}=\sqrt{16}$ $\implies |x|=4$ Therefore the values of $x$ are $+4$ and $-4$ . So, we generally don't face any problem to solve these types of equations.",['algebra-precalculus']
4847499,"Can a disk of area $1$ be completely covered by six disks of areas $\frac12,\frac13,\frac14,\frac15,\frac16,\frac17$?","Can a disk of area $1$ be completely covered by six disks of areas $\frac12,\frac13,\frac14,\frac15,\frac16,\frac17$ ? The disks may overlap. I made a desmos graph where you can move the disks around. So far this is my best effort (the equations can be seen when you open the Desmos graph): Approximately 99.96% of the big disk is covered. There are small gaps near the perimeter. No matter how I arrange the disks, there always seems to be gaps. It seems impossible to cover the big disk, but I don't know how to prove it. (In the Desmos graph the disks all have their areas multipled by $\pi$ but that doesn't matter.) Related fact: A square of area $1$ can be covered by six disks of areas $\frac12,\frac13,\frac14,\frac15,\frac16,\frac17$ , but just barely . This question was inspired by the question ""What is the largest disk that will be completely covered by randomly placed disks of areas $1,\frac12,\frac13,\dots$ with probability $1$ ?"".","['circles', 'geometry']"
4847554,Differential Equation Absolute Value,I must solve $\frac{dy}{dt} = y(1-y)$ . I work through this using partial fractions and obtain | $\frac{y}{1-y}|$ $=$ $Ae^t$ . I am confused what to do with the absolute values. Would I have two separate general solutions being $\frac{y}{1-y}$ $=$ $Ae^t$ and $\frac{-y}{1-y}$ $=$ $Ae^t$ ?,"['multivariable-calculus', 'derivatives', 'ordinary-differential-equations', 'real-analysis']"
4847575,How to take derivative of an arithmetic function?,"Arithmetic functions are defined from natural numbers to complex numbers. Therefore, they are not continuous in the analytic sense and consequently cannot be differentiated analytically. However, we do know that some arithmetic functions are asymptotic to analytic functions. That is, when we connect the points of an arithmetic function on the coordinate plane, it looks as though we could take a derivative, as if it were a curve. Is there a definition in the literature for such a derivative? For example, it is accepted that $\frac{d\pi(n)}{dn} \sim \frac{1}{\log n}$ which is a consequence of the Prime Number Theorem. But how can this derivative $\frac{d\pi(n)}{dn}$ be formally expressed? $\pi(n)$ function looks like below: Here you can see it can not be differentiable. But as you scale down the graph, you can see it will look like a curve.","['arithmetic-functions', 'number-theory', 'derivatives', 'real-analysis']"
4847582,Propositional tautologies whose analogues for sets are false,"Are there tautologies of propositional logic whose analogues for sets are false? I believe I have found such a tautology. For example, $((p \rightarrow q) \vee (q \rightarrow p))$ is a tautology, but the analogue for sets $((A \subseteq B) \vee (B \subseteq A))$ is not always true, for there are sets which are not subsets of each other. But maybe I am just misunderstanding something. My real question is, can someone clarify my misunderstanding, if indeed I am misunderstanding something?","['elementary-set-theory', 'boolean-algebra', 'propositional-calculus', 'logic']"
4847661,"Value of $\int_0^{2\pi}\left(\frac{\sin 2\theta}{\sin \theta} \right)^n \sin((m+1) \theta) \sin \theta d \theta$ on $(m, n) \in \mathbb{Z}_{\geq 0}^2$","Define $$f(m,n) = \int_0^{2 \pi} \left( \frac{\sin 2 \theta}{\sin \theta} \right)^n \sin((m+1) \theta) \sin \theta d \theta.$$ I want to find a nice formula for the value of $f(m,n)$ in terms of $m$ , $n$ for tuples $(m, n) \in \mathbb{Z}_{\geq 0}^2$ . I'm struggling to manipulate the integrand in a way to evaluate the integral, indeed, integral-calculator.com fails to find an antiderivative (although that is trying to solve the more general problem $(m, n) \in \mathbb{R}^2$ ) Here are the values of $f(m,n)$ for small $m$ , $n$ : \begin{array} {rr|rrrr} && & &m & &\\  & & 0 & 1 & 2 & 3 & 4 \\ \hline & 0 & 1 & 0 & 0 & 0 & 0 \\ &  1 & 0 & 1 & 0 & 0 & 0 \\n & 2 & 1 & 0 & 1 & 0 & 0 \\ & 3 & 0 & 2 & 0 & 1 & 0 \\ & 4 & 2 & 0 & 3 & 0 & 1 \\ \end{array} Note I know for a fact that $f(m,n) \in \mathbb{Z}_{\geq 0}$ for $(m, n) \in \mathbb{Z}_{\geq 0}^2$ , as can be seen in the table. (This integral arises as a certain inner product of two representations of a certain group.) But I'm struggling to see much more than that. It appears $f(m,m)=1$ for all $m$ and $f(m,n)=0$ for $m>n$ as well as when $m+n$ is odd. But other than that, I'm really not sure. What could I do to determine $f(m,n)$ for more values?","['integration', 'calculus', 'definite-integrals']"
4847666,On the elliptic curve $X^3 + 6\cdot 163^2 X - 7\cdot 163^3 = Y^2$ and others,"I. Ellipse Given the general equation, $$a^3 + b^3 + c^3 = (c + m)^3$$ Let, \begin{align}
c &= (n + 1)(a - m) + n b\\
a &= p + q + m + 4 m n + 3 m n^2\\ 
b &= p - q + 2 m n + 3 m n^2
\end{align} and after removing a trivial factor, it becomes the simple ellipse centered at the origin, $$p^2 + 3 q^2 = d$$ where $d = 3m^2 n(n+1) (3 n^2 + 3n + 1).$ For example, the ellipse $p^2+3q^2-7=0$ . Choose some desired $d$ with integer $(p,q)$ . Thus we need to solve for $m$ , $$m = d\,\sqrt{\frac1{3d n(n+1) (3 n^2 + 3n + 1)}}$$ and find rational $n$ such that $m$ is also rational. Assume prime $d=1\,(\text{mod}\,6)$ , or Euler's Theorem , say $d=p^2+3q^2=7$ so $(p,q) = (2,1)$ . Then one solution is $n = 1/3,\, m = 3/2$ . Substituting $(p,q,m,n)$ into the expressions for $(a,b,c)$ , we find, $$\left(\frac{42}{6}\right)^3 + \left(\frac{15}{6}\right)^3 + \left(\frac{49}{6}\right)^3=\left(\frac{49}{6}+\frac{3}{2}\right)^3$$ II. Elliptic curve Here's the difficult part: Characterize which prime $d=p^2+3q^2$ such that the quartic in $n$ to be made a square is solvable, $$3d n(n+1) (3 n^2 + 3n + 1) = w^2$$ with $w\neq 0$ . After some correspondence with S. Tomita, we find this is birationally equivalent to a cubic in $X$ to be made a square, $$X^3 + 6d^2 X - 7d^3 = Y^2$$ or an elliptic curve . The primes of form $d=1\,(\text{mod}\,6) = p^2+3q^2$ are given by A002476 . From that, I tested 150 primes with $d<2024$ using the online Magma calculator and found a LOT had rank $r > 0$ including $d = 163$ . There were only 26 with rank $r=0$ , namely, $$73, 97, 193, 241, 337, 409, 457, 601, 673, 769, 937, 1009, 1033, 1129, 1153, \ 1201, 1249, 1297, 1321, 1489, 1609, 1753, 1777, 1801, 1873, 2017$$ so cannot be used for a non-trivial solution to $a^3 + b^3 + c^3 = (c + m)^3$ . It can be observed they are a subset of A107008 , the primes of form $u^2+72v^2.$ Update : Based on the Magma code in the answer below, the observation is apparently true for $d<40000$ . III. Question Conjecture 1: Given the elliptic curve $X^3 + 6d^2 X - 7d^3 = Y^2$ with prime $d = p^2+3q^2$ , then a necessary (but not sufficient) condition that it has rank $r=0$ is it is a subset of $d = u^2+72v^2$ . (Only a subset since $\color{red}{313} =u^2+72v^2$ has rank $r = 2$ .) Conjecture 2: For those special $d = u^2+72v^2$ with rank $r=2$ , then a necessary (but not sufficient) condition is it is a subset of $d = x^2+144y^2$ . (Only a subset since $193=x^2+144y^2$ has rank $r = 0$ .) For $d < 10009$ , there are only 22, $$\color{red}{313}, 433, 577, 1657, 1993, 2137, 3529, 3697, 4057, 4129, 4297, 4513, 4801, 6529, 6553, 7057, 8089, 8209, 8641, 9241, 9337, 9601$$ all of which have form $x^2+144y^2.$ If conjectures are not true, then what are the first counter-examples? Note : Can someone test $d=1471$ ? Magma says it has rank $r=1$ , but seems unable to give a generator.","['elliptic-curves', 'number-theory', 'conic-sections', 'prime-numbers', 'quadratic-forms']"
4847670,Help finding second directional derivative,"I'm studying multivariable-calculus and I'm trying to solve this question: Let $GL(n,\Bbb R)$ be the group of $n×n$ invertible matrices of real numbers now let $A=\begin{pmatrix}
0 & 1 \\
2 & 3 
\end{pmatrix}$ $B=\begin{pmatrix}
1 & 2 \\
3 & 0 
\end{pmatrix}$ $C= \begin{pmatrix}
1 & 2 \\
0 & 3 
\end{pmatrix}$ $I= \begin{pmatrix}
1 & 0 \\
0 & 1 
\end{pmatrix}$ Define $F:GL(2,\Bbb R) \to GL(2,\Bbb R)$ given by $F(X)=X^{-1}AX$ let $f:GL(2,\Bbb R) \to \Bbb R\ $ given by $\ f(X)=[F(X)]_{11}\ $ when $[F(X)]_{11}$ is the component in the first row and first column of the matrix $F(X)$ I need to find the second directional deriative $D_{B}D_{C}f(I)$ what I did: first I calculated $f(X)$ if $X= \begin{pmatrix}x & y \\z & w \end{pmatrix}\ $ then $f(X) = \frac{-2xy+z(w-3y)}{(xw-yz)}\ $ so now I tried to find an expression for the function $D_{C}f\ $ but no matter which of the 2 definition of the directional deriative the I know I have tried both of them got me stuck because I got to a point where I needed to do alot of complex calculation and I don't think that is the point of the question. The methods I know 1.with the limit definition: $\ \lim_{t\to 0}\frac{f(X+tC)-f(X)}{t}$ 2 . with the gradient : $D_{C} f = \nabla f \cdot{C}$ My question is am I right that there is an easier way to solve this question? and if so it means that my understanding of this topic is not so good, so can you refer me to some notes\videos\books that can help me get a better understanding? also if you can give me a hint on how to solve this question that will be great too","['partial-derivative', 'multivariable-calculus', 'reference-request']"
4847674,How to prove $\int_0^{\frac{\pi}{2}}{x\arctan(\sin{x})}\mathrm{d}x=-2\sum_{n=1}^{\infty}\frac{(-1)^{n}a^{2n-1}}{(2n-1)^{3}}$,"a in all descriptions is $\sqrt{2}-1$ I recently tried to change $\frac{1}{\sin{x}}$ to $x$ in a simple Feynman integral problem: $\int_0^{\frac{\pi}{2}}\frac{\arctan(\sin{x})}{\sin{x}}\mathrm{d}x$ , but I found that such a change would make the problem too complicated to be solved by Feynman integrals. I tried the calculator, and the result is an infinite series: $-2\sum_{n=1}^{\infty}\frac{(-1)^{n}a^{2n-1}}{(2n-1)^{3}}$ In other words,I found that $$\int_0^{\frac{\pi}{2}}{x\arctan(\sin{x})}\mathrm{d}x=-2\sum_{n=1}^{\infty}\frac{(-1)^{n}a^{2n-1}}{(2n-1)^{3}}$$ but I can't prove this equation. I tried to expand the integral, but unfortunately I couldn't prove it this way. I have also tried to use the residue theorem, but it is clear that this does not conform to several forms of using the residue theorem. Can someone please prove this equation? Thank you!","['integration', 'sequences-and-series']"
4847707,"$f$ is differentiable at any quadratic irrational $a\in (0,\infty)\setminus\mathbb{Q}$. (""How to learn mathematics"" edited by Kunihiko Kodaira.)","I am reading ""How to learn mathematics"" (in Japanese) edited by Kunihiko Kodaira. Kodaira wrote as follows (I used ChatGPT3.5 for this English translation.): I attended Professor Teiji Takagi's lecture on introductory analysis during my first year of university. One of the practice problems was about the continuity of a function $f(x)$ defined as follows: for the interval $x > 0$ , if $x$ is irrational, then $f(x) = 0$ , and if $x = p/q$ is a rational number (where $p/q$ is in reduced form and $q > 0$ ), then $f(x) = 1/q$ . As is well known, $f(x)$ is discontinuous at $x = a$ if $a$ is rational and continuous at $x = a$ if $a$ is irrational. I remember realizing that by slightly modifying the definition of $f(x)$ to $f(x) = 0$ for irrational $x$ and $f(x) = 1/q^3$ for $x = p/q$ (where $p/q$ is a rational number in reduced form with $q > 0$ ), $f(x)$ is discontinuous at $x = a$ if $a$ is rational, continuous at $x = a$ if $a$ is irrational, and further, differentiable at $x = a$ if $a$ is a quadratic irrational. Since then, I began to consider alternative proofs for theorems and to explore variations of problems. Let $f:(0,\infty)\to\mathbb{R}$ be the function such that $f(x) = 0$ for $x\in (0,\infty)\setminus\mathbb{Q}$ and $f(x) =\frac{1}{q^3}$ for $x\in (0,\infty)\cap\mathbb{Q}$ , where $x=\frac{p}{q}$ and $\frac{p}{q}$ is in reduced form. It is obvious that $f$ is not continuous at $a\in (0,\infty)\cap\mathbb{Q}$ . Let $a\in (0,\infty)\setminus\mathbb{Q}$ . Let $\varepsilon$ be an arbitrary positive real number. Let $N$ be a positive integer such that $\frac{1}{N^3}<\varepsilon$ . Let $d_i$ be the distance between $a$ and the rational number whose denominator is $i$ and which is closest to $a$ for each positive integer $i$ . Let $d:=\min\{d_1,\dots,d_N\}$ . Let $\delta$ be a positive real number such that $\delta<d$ . If $|x-a|<\delta$ , then $|f(x)-f(a)|=|f(x)|<\frac{1}{N^3}<\varepsilon$ . So, $f$ is continuous at $a\in (0,\infty)\setminus\mathbb{Q}$ . But I cannot prove that $f$ is differentiable at any quadratic irrational $a\in (0,\infty)\setminus\mathbb{Q}$ . Please tell me a proof of this fact.","['continuity', 'calculus', 'derivatives']"
4847713,Are there Examples of a function with elementary antiderivative that we know its antiderivative is too big too be written down?,"EDIT: I asked this question on MO here . I recently learned that there are many very large numbers that have been defined, such as $TREE(3)$ and many others that we cannot write down. What made me interested is the idea that there is a function that take some finite and small number to an absolute beast of a number. So I wonder if there is some function with an elementary antiderivative that we know its antiderivative is too large to be written down, but we can write down the function itself. What I mean by writing down the antiderivative:  Is to write it without any shorthand notation like $\sum_n f(x_n)c_n$ Like how we can't layout the digits of $TREE(3)$ if we wanted to But the antiderivative is finite. What I mean to be too large is the humanity can't write it down without any shorthand notation because the antiderivative has a lot of terms (say for example $10^{10000}$ term) and composition of functions. What I mean by    writing down the function is: It is possible to write down its distributive form without any shorthand notation like $(1+x)^4$ i will count this as shorthand and its distributive form is $1+4x+6x^2+4x^3+x^4$ so functions like $x^{TREE(3)},\  {TREE(3)}  $ doesn't count.","['integration', 'examples-counterexamples', 'analysis', 'indefinite-integrals', 'soft-question']"
4847720,How to calculate $\int_{-\infty}^\infty\frac{e^x}{1+e^{3x}}dx$ using residues [duplicate],"This question already has answers here : Integrating $\int_0^{\infty} \frac{dx}{1+x^3}$ using residues. (3 answers) Closed 5 months ago . The community reviewed whether to reopen this question 5 months ago and left it closed: Original close reason(s) were not resolved My original problem was this $$\int_0^\infty\frac1{1+x^3}dx$$ I wanted to solve it using the Residue theorem, but the only integrals I could integrate using it are across the entire real line. So I made the substitution $u=\ln x$ to get $$I=\int_{-\infty}^\infty\frac{e^x}{1+e^{3x}}dx=\operatorname{Re}\left(2\pi i\sum_{k=\frac{\pi i}3+2n\pi i}\operatorname{Res}_{z=k}\left(\frac{e^z}{1+e^{3z}}\right)\right)$$ By integration along the semicircular contour, and the integral along the arc is well-known to go to zero. These poles are of first order, so $$\operatorname{Res}_{z=k}\left(\frac{e^z}{1+e^{3z}}\right)=\lim_{z\rightarrow k}\frac{e^z(z-k)}{1+e^{3z}}=\frac1{3e^{2k}}$$ So the answer should be $$I=\frac{2\pi}3\operatorname{Re}\left(\sum_{k=0}^\infty e^{-2(\pi i/3+2k\pi i)}\right)$$ I know that the real answer is $I=\frac{2\pi}{3\sqrt 3}$ and so the infinite series must evaluate to $\frac1{\sqrt 3}$ for my answer to be right. However, according to Wolfram Alpha, the infinite series diverges. So I am not sure what exactly I have done wrong. I was thinking that maybe this is because I only need to choose $\frac{\pi i}3$ and $\pi i$ as poles, but I have no other idea. Edit: It is very clear that my question is not a duplicate because I am wondering why my answer diverges while the duplicate only asks for how to solve it. I added tags to make it clear.","['complex-analysis', 'residue-calculus', 'solution-verification']"
4847725,Solving the comparison equation $y^{\prime \prime \prime}-x y^{\prime}-\mu y=0$ when $\mu=\frac{3}{4}$.,"I need to solve the comparison equation from Airy Functions and Applications to Physics by Vallee and Soares $$y^{\prime \prime \prime}-x y^{\prime}-\mu y=0$$ when $\mu=\frac{3}{4}$ . When $\mu=\frac{1}{2}$ this equation has general solution $$
y_{1 / 2}=a \mathrm{Ai}^2\!\left(2^{-2 / 3} x\right)+b \mathrm{Bi}^2\!\left(2^{-2 / 3} x\right)+c \mathrm{Ai}\!\left(2^{-2 / 3} x\right) \mathrm{Bi}\!\left(2^{-2 / 3} x\right)
$$ When $\mu=1$ $$y_1=a \mathrm{Ai}(x)+b \mathrm{Bi}(x)+c\mathrm{Hi}(x).$$ In general, The above image does give me the solution when $\mu =\frac{3}{4}$ , but it is in terms of contour integrals that I am not sure how to evaluate as I don't think the parameterizations of those curves are nice. How can I solve the equation more explicitly in this case of $\mu=\frac{3}{4}$ ? Here is a link to the reference Airy Functions and Applications to Physics page 108 in the first edition. The page prior has the comparison equation stated. EDIT: I am looking for an answer more explicit than the one terms of hypergeometric functions given by Mathematica, similar to the case of $\mu = \frac{1}{2},1$ . Wolfram spits out $$
 y(x)=c_1\frac{\sqrt[3]{-1} x_1 F_2\left(\frac{7}{12} ; \frac{2}{3}, \frac{4}{3} ; \frac{x^3}{9}\right)}{3^{2 / 3}}+ c_2
{}_1 F_2\left(\frac{1}{4} ; \frac{1}{3}, \frac{2}{3} ; \frac{x^3}{9}\right)+c_3\frac{(-1)^{2 / 3} x^2{ }_1 F_2\left(\frac{11}{12} ; \frac{4}{3}, \frac{5}{3} ; \frac{x^3}{9}\right)}{\sqrt[3]{3}}
$$ but I require something more explicit stated not in terms of hypergeometric functions, but maybe something involving $\operatorname{Ai}, \operatorname{Bi}$ . Thank you","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4847728,The character table of $G/Z(G)$ and $G/N$ given knowledge of the character table of $G$,"Suppose for a group $G$ , of which we completely understand its conjugacy class structure and character table, is there a simple algorithm to deduce the character table of $G/Z(G)$ ? Or more generally, $G/N$ for $N \trianglelefteq G$ ?","['group-theory', 'abstract-algebra', 'representation-theory', 'characters']"
4847735,"The rate at which the expectation of the square of the empirical median of i.i.d. $[-1,1]$-valued uniform random variables goes to zero","Suppose that $X_1,X_2,\dots$ is an i.i.d. sequence of $[-1,1]$ -valued uniform random variables.
Let $\bar{X}_t$ be the empircal mean of the sample $X_1,\dots,X_t$ .
Then $\mathbb{E}\big[|\bar{X}_t|^2\big] = \operatorname{Var}[\bar{X}_t] = \frac{1}{3t}$ . Hence for the empirical mean we have that $\mathbb{E}\big[|\bar{X}_t|^2\big]$ goes to zero as fast as $t^{-1}$ , for $t \to \infty$ . I'm wondering at which rate the expected value of the square of the empirical median of these random variables goes to zero as the sample grows. Specifically, for each $t \in \mathbb{N}$ (if it helps, assume that in what follows $t$ is odd), let $M_t$ be the empirical median of the sample $X_1,\dots,X_t$ . How fast $\mathbb{E}\big[|M_t|^2\big]$ goes to zero as $t \to \infty$ ? We can interpret formally the question as finding $\alpha>0$ for which, if $0<\beta < \alpha $ then $\mathbb{E}\big[|M_t|^2\big] \cdot t^{\beta} \to 0, t\to\infty$ , while if $\gamma > \alpha$ then $\mathbb{E}\big[|M_t|^2\big] \cdot t^{\gamma} \to \infty, t\to\infty$ . Given that for the empirical median we don't have any simple formula as we do for the empirical mean (where we can leverage the variance to obtain an easy computation), this problem seems way less trivial. Is there a smart way to deduce the rate of convergence?","['probability-distributions', 'median', 'probability']"
4847760,An orthonormal sequence in an inner product space converges weakly to $0$.,"It is a well-known theorem that ""An orthonormal sequence in a Hilbert space converges weakly to $0$ "". The proof uses the Bessel's inequality and the Rise representation theorem for which completeness of the space is a necessary condition. However, I somehow proved the same theorem for any inner product space. If my proof were correct, then textbooks would use that stronger result instead of the current one. Theorem: Let $E$ be an inner product space, $\{ x_n\}$ an orthonormal sequence in an inner product space, and . Then $\{ f(x_n)\}$ converges to $0$ for any $f\in E'$ . Proof: Let $f\in E'$ . Assume to the contrary that $\{f(x_n)\}$ does not go to $0$ . Then there is some $\epsilon >0$ and a subsequence $\{ x_{k_n}\}$ such that $|f(x_{k_n})|\geq \epsilon $ . But then infinitely many of $f(x_{k_n})$ 's are positive or they are negative. So without loss of generality, assume that $f(x_{k_n})$ 's are all positive. Then $n\cdot \epsilon \leq f(x_{k_1})+...+f(x_{k_n})=| f(x_{k_1})+...+f(x_{k_n})|=|f(x_{k_1}+...+x_{k_n})|\leq ||f||\cdot ||x_{k_1}+...+x_{k_n}||=||f||\cdot \sqrt{n}$ where the last equality follows from the Pythagorean theorem. But we then obtain $||f||\geq \epsilon \cdot \sqrt{n}$ , which contradicts with boundedness of $f$ . So $f(x_n)\rightarrow 0$ . $\square$ I couldn't see the issue here.","['hilbert-spaces', 'inner-products', 'functional-analysis']"
4847771,Integrating from where I left off and where to go next?,"$$
I=\int_0^\infty \frac{x \cdot e^{-2x}}{\sqrt{e^x-1}} \, dx
$$ Let $(u=\sqrt{e^x-1})$ which gives $(\ln(u^2+1)=x) $ and $(\frac{2u \, du}{u^2+1}= dx)$ . The new bounds of integration are still 0 and infinity. Now we have $$
\int_{0}^{\infty} \frac{\ln(u^{2}+1)}{\left(e^{2ln(u^{2}+1)}\right) \cdot u} \cdot \frac{2u}{u^{2}+1} \, du
$$ Bounds: From 0 to infinity. After simplification using log rules, we have $(\frac{\ln(u^2+1)}{(u^2+1)^3})$ and the $(u)$ cancels, leaving us with $
\int_0^\infty \frac{\ln(u^2+1)}{(u^2+1)^3} \, du \quad \text{or} \quad \int_0^\infty 2 \ln(u^2+1) \cdot (u^2+1)^{-3} \, du
$ Then using $(u=\tan(w))$ , $(du=\sec(w)^2)$ , and $
ln(u^2+1) \rightarrow \ln(\tan(w)^2+1) \rightarrow \ln(1/\sec(w)^2) \rightarrow -2 \ln(\cos(w))
$ we are left with $
\int_0^{\frac{\pi}{2}} -4 \ln(\cos(w)) \cdot \cos(w)^4 \, dw
$ Would using complex analysis be appropriate to solve? If so, how would I evaluate the closed form of this integral that I left off on? If not, how do I consider the integral from where I left off?
Thanks","['complex-integration', 'improper-integrals', 'calculus', 'definite-integrals']"
4847775,Simple normal crossings divisor and blow up,"I have to prove some things about simple normal crossings divisors and the blow up. The definition of a simple normal crossings divisor we use is a finite union $V = \cup_i V_i$ of irreducible quasi-projective curves on a nonsingular quasi-projective surface $S$ is a simple normal crossings divisor if each $V_i$ is nonsingular, no three different different $V_i$ intersect and if two curves $V_i \neq V_j$ intersect, this intersection is a single point and the two tangent lines of $V_i$ and $V_j$ in this point are distinct. Let $\pi: \text{Bl}_0(\mathbb{A}^2) \rightarrow \mathbb{A}^2$ be the blow up at the origin. I have to prove the following. If $Z=\mathbb{V}(x_0x_1(x_0+x_1)) \subseteq \mathbb{A}^2$ , then $\pi^{-1}(Z) \subseteq \text{Bl}_0(\mathbb{A}^2)$ is a simple normal crossings divisor. If $Z=\mathbb{V}(x_0^2-x_1^3) \subseteq \mathbb{A}^2$ , then $\pi^{-1}(Z) \subseteq \text{Bl}_0(\mathbb{A}^2)$ is not a simple normal crossings divisor. Repeatedly blow up $\mathbb{A}^2$ and $Z = \mathbb{V}(x_0^2-x_1^3)$ until the preimage of $Z$ is a simple normal crossings divisor. For the first item I observed that $\pi^{-1}(Z) = \mathbb{V}(x_0) \cup \mathbb{V}(x_1) \cup \mathbb{V}(x_0+x_1)$ . Set $V_1 = \mathbb{V}(x_0)$ , $V_2 = \mathbb{V}(x_1)$ and $V_3 = \mathbb{V}(x_0+x_1)$ . Then I computed the intersection with the charts using an example in our notes \begin{align*}
        V_1 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_0}^1) & = \emptyset\\
        V_1 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_1}^1) & \cong \{(x_0,x_1,v)| x_0=vx_1,v=0\}\\
        V_2 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_0}^1) & \cong \{(x_0,x_1,u)| x_1=ux_0,u=0\}\\
        V_2 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_1}^1) & = \emptyset\\
        V_3 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_0}^1) & \cong \{(x_0,x_1,u)| x_1=ux_0,1+u=0\}\\
        V_3 \cap (\mathbb{A}^2 \times \mathbb{A}_{x_1}^1) &  \cong \{(x_0,x_1,v)| x_0=vx_1,1+v=0\}.
    \end{align*} Then it is clear that this a is a simple normal crossings divisor, since these intersections have no intersection and are nonsingular and irreducible. Now I tried to do the same for the second item with $V=\pi^{-1}(Z)$ and found the following intersection with the charts \begin{align*}
        V \cap (\mathbb{A}^2 \times \mathbb{A}_{x_0}^1) & \cong \{(x_0,x_1,u)| x_1=ux_0,1-u^3x_0=0\}\\
        V \cap (\mathbb{A}^2 \times \mathbb{A}_{x_1}^1) & \cong \{(x_0,x_1,v)| x_0=vx_1,v^2-x_1=0\}.
    \end{align*} I think these intersections are nonsingular and irreducible, so in order for this to not be a simple normal crossings divisor we need to find a contradiction with another property. I cannot prove this however. I was thinking that there might be a problem in the computations in the first item, since I do not understand how the blow up behaves around the origin $(0,0)$ . Any help is welcome, involving these computations or just a general explanation about simple normal crossings divisor or blow up.","['algebraic-geometry', 'blowup']"
4847780,Proving two averages are asymptotically equivalent,"Suppose $f(n)\sim g(n)$ as $n\to\infty$ . Is it necessarily true that \begin{equation}\frac{1}{n}\sum_{k=1}^n|f(k+1)-f(k)|\sim\frac{1}{n}\sum_{k=1}^n|g(k+1)-g(k)|\end{equation} as $n\to\infty$ ? Intuitively this should be true; one would expect the average values of these differences to be asymptotically the same as the analogue for $g$ , but I cannot figure out how to make this more rigorous. My idea was that, if the above is true, then it is also true that $\sum_{k=1}^n|f(k+1)-f(k)|\sim\sum_{k=1}^n|g(k+1)-g(k)|$ by multiplying through by $n$ . But this doesn't really yield anything.","['summation', 'asymptotics', 'analysis', 'real-analysis', 'functional-analysis']"
4847786,Flat Maurer-Cartan connection iff flat Berry connection,"I am studying two connections on induced representation spaces $\text{Ind}_{H}^{G} \Gamma$ , where $H \subseteq G$ are groups, and $\Gamma$ is an irrep of $H$ . The first is the canonical or $H$ -connection, which is the projection of $G$ 's Maurer-Cartan form into $H$ 's Lie algebra. A paper by Milnor classifies all such flat connections in terms of monodromy groups $\pi_1(G/H)/\ker \rho$ , induced by $\pi_1$ -irreps $\rho$ . These irreps determine the monodromy after a non-contractible closed path in the coset space. The second is the Berry connection (see here and here for mathematican description), $\langle a,\mu |\partial a,\nu\rangle$ , for ""position states"" $|a,\mu\rangle$ of a particle on the induced-rep space, where $a \in G/H$ , and $\mu,\nu$ index the irrep space of $\Gamma$ . Here a position state moves in the coset space $G/H$ , and obtains a holonomy (in physics: Berry phase or matrix) after a closed path that acts on the $\mu$ factor. The possible monodromies (physics: topological Berry phases or matrices) for non-contractible closed paths are classified by the monodromy group $H/\ker \Gamma$ . The two connections are related but not generally equal [ Eq. (4.2) here or here] . For all examples I'm aware of, the connections are zero iff the two monodromy groups match . In other words, I see that both connections are flat for a given inducing $H$ -irrep $\Gamma$ if there exists a $\pi_1$ -irrep $\rho$ such that $$
\frac{H}{\ker \Gamma} = \frac{\pi_1(G/H)}{\ker \rho}~.
$$ I was hoping someone could either prove this or provide a counterexample. Here are some examples: Both connections are flat and the monodromy is trivial for the sphere, $\text{Ind}_{U(1)}^{SU(2)} 0$ , where $0$ is the trivial irrep of $U(1)$ . The above equation holds since $H/\ker\Gamma = U(1)/U(1)$ , $\pi_1$ is trivial, and its only (trivial) irrep can be picked to be $\rho$ . Both connections are not flat and the monodromy is trivial for the sphere with a monopole of nonzero strength $\lambda$ , $\text{Ind}_{U(1)}^{SU(2)} \lambda$ , where $\lambda$ is a non-trivial irrep of $U(1)$ . Conjecture holds since $H/\ker \Gamma = U(1)/\mathbb{Z}_{|\lambda|} = U(1)$ , $\pi_1$ is the same (i.e., trivial), and so it's impossible to find a $\rho$ that can fill in the above equation. Both connections are flat and the monodromy is nontrivial for the Poincare homology sphere with some inducing $A_5$ -irrep $\Gamma$ , $\text{Ind}^{SO(3)}_{A_5} \Gamma$ . The above equation holds since $\pi_1 = 2A_5$ , and so one can pick the $\pi_1$ -irrep corresponding to $\Gamma$ under the double cover to be $\rho$ .","['fiber-bundles', 'connections', 'vector-bundles', 'mathematical-physics', 'differential-geometry']"
4847806,Proving a limit using the dominated converge theorem.,"Let $\Omega \subset \mathbb{R}^n$ be an open set and consider the usual Lebesgue space $L^p(\Omega)$ . Adicionally, consider the space $$ U(\Omega) =  \left\{ f \in L^p(\Omega) \, \colon \, \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \,  \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0 \right\} .$$ My goal is to show that $L^p(\Omega) = U(\Omega)$ , using the Dominated Convergence Theorem. My attempt. The inclusion $U(\Omega) \subset L^p(\Omega)$ is direct just by definition of $U(\Omega)$ . On the other hand, let $f \in L^p(\Omega)$ be arbitrary. In order to show that $f \in U(\Omega)$ , it suffices to guarantee that $$ \lim_{r \to 0} \, \sup_{x \in \mathbb{R}^n} \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = 0. $$ For every $x \in \mathbb{R}^n, r > 0$ we have that $$ \int_{B(x,r) \cap \Omega} |f(y)|^p \, dy = \int_{\mathbb R^n}|f(y)|^p \chi_{B(x,r) \cap \Omega}(y) \, dy. $$ Now, there are two questions that arise: The first question is how to deal with the supremum. I believe that before applying the DCT I should find a way to get rid of the supremum but I've been unable to find one. The second question is related to applying the DCT with limits that tend to $0$ .There are some posts on MSE about this (e.g. [ $1$ ] , [ $2$ ] and [ $3$ ] ). The main reasoning presented in such posts is to use the sequential formulation of the limit to retrieve conclusions. I believe that I understand this reasoning but I have a more informal question: in pratice (say in articles or books, for example), what is the most common way of dealing with the DCT when limits tend to zero? Do the authors explicitly describe the use of the sequential formulation of a limit or do they just present a dominating function and move the limit (tending to $0$ ) inside of the integral ? Thanks for any help in advance.","['lebesgue-integral', 'real-analysis', 'functional-analysis', 'limits', 'supremum-and-infimum']"
4847807,How to do derivative of expectation (characteristic function)?,"I type a lot of stuff here, which may looks like daunting. But most of them are just background of my question and are not relevant on my question. Basically, my question is how to get derivative of expectation (here it is characteristic function)? You can skip the background and directly solve my question at last. Suppose $X$ is integrable. Then the following statements are equivalent: for all $u,v \in \mathbb{R}^p, u \perp v$ , we have $E(u^TX|v^T X)=0$ ; $X$ has a spherical distribution; Proof: $1 \Rightarrow 2$ . Step 1. It suffices to show that, for any $t_1, t_2 \in \mathbb{R}^p$ such that $||t_1||=||t_2||$ , we have $\phi_X(t_1)=\phi_X(t_2)$ . (Background: it uses a lemma that is a random vector $X$ has a spherical distribution if and only if its characteristic function is a function of $t^Tt$ ; that is, $   \phi_X(t)=g_0 (t^Tt)$ for some function $g_0$ . Here $\phi_X(t)=E(e^{it^T X})$ , where $i=\sqrt{-1}, t \in \mathbb{R}^p$ ) Step 2. Let $\mathcal{S} = \{t: ||t|| =||t_1|| \}$ . By another lemma (see below), there is a differential function $c: (0, 2\pi) \to \mathcal{S}$ that passes through $t_1$ and $t_2$ . It then suffices to show that $\phi_X$ is constant on $\mathcal{C}$ . Lemma: Let $\mathcal{S}=\{ x:||x||=r\}$ , where $r>0$ , and let $t_1,t_2\in \mathcal{S}$ . Then there is a differentiable mapping $c:(0, 2\pi) \mapsto \mathcal{S}$ that passes through $t_1$ and $t_2$ ; that is, there exist $\alpha_1,\alpha_2 \in (0, 2\pi)$ such that $c(\alpha_1)=t_1$ and $c(\alpha_2)=t_2$ . Proof: Let $\mathcal{H}$ be a hyperplane that passes through $t_1,t_2,0$ in $\mathbb R^p$ . Then $\mathcal{C}=\mathcal{H} \cap \mathcal{S}$ is a circle centered at the origin with radius $r$ . Pick an arbitrary point $t_3$ in $\mathcal{C}$ that is not $t_1$ and $t_2$ , and let $t_4$ be the point on $\mathcal{C}$ obtained by rotating $t_3$ counterclockwise 90 degree along $\mathcal{C}$ . Then any point $t$ in $\mathcal{C}$ that is not $t_3$ can be written as $t=\cos(\alpha)t_3+\sin(\alpha)t_4$ for some $\alpha \in (0, 2\pi)$ . By construction, $\{  c(\alpha): \alpha \in (0, 2\pi) \}$ is a differentiable curve that passes through $t_1$ and $t_2$ . Step 3. A sufficient condition for this is $$\partial \phi_X[c(\alpha)]/\partial \alpha=0$$ for all $\alpha \in (0, 2\pi)$ . The left hand side is $$\partial \phi_X[c(\alpha)]/\partial \alpha=\partial E(e^{ic(\alpha)^T X})/\partial 
 \alpha\stackrel{?}{=}  E[(e^{ic(\alpha)^T X} iX^T\dot{c}(\alpha))]  \stackrel{?}{=}   E\{ e^{ic(\alpha)^T X} iE[X^T \dot{c}(\alpha)|c(\alpha)^T X]   \}$$ Question: how to get the last two equality signs? I don't know how to get derivative with expectation.","['calculus', 'statistics', 'linear-algebra', 'probability']"
4847826,Derivative of a function with respect to a function,"This is Definition 10 from Semi-Riemannian Geometry With Applications to Relativity by Barrett O'Neill, page 7: On both sides of Definition 10, he is differentiating with respect to functions , not independent variables. What does that mean? He defines both x i and u i to be functions, not coordinates.
He does so on the first two pages of the book: Here's a picture of what the functions do:","['manifolds', 'semi-riemannian-geometry', 'derivatives', 'differential-geometry']"
4847827,Simplifying $(A^c\setminus B^c)^c\cap (A\cup B)$,"How can we simplify the following set-theoretical expression? $$(A^c\setminus B^c)^c\cap (A\cup B)$$ Where $A, B\neq \emptyset$ and $A, B\subset E$ . Let $x\in (A^c\setminus B^c)^c\cap (A\cup B)$ , so $x\in (A^c\setminus B^c)^c$ and $x\in (A\cup B)$ . $$(A^c\setminus B^c)^c = (A^c\cap B)^c$$ Therefore $x\notin A^c, B$ , and so $x\in A, B^c$ . Thus, $$(A^c\setminus B^c)^c\cap (A\cup B) = A\cap B^c$$",['elementary-set-theory']
4847891,About the domain of the derivative,"Is the domain of the derivative $f'(x)$ of a function $f(x)$ conditioned by the function $f(x)$ itself? I mean, take for example $f(x) = \ln(x)$ whose domain is $x > 0$ .
We have $f'(x) = \frac{1}{x}$ , whose natural domain is $\mathbb{R}\backslash\{0\}$ . Yet I don't ""care"" about the branch $x < 0$ , for $f(x)$ is not defined over here. So my question is: is $f'(x)$ a new function, or when dealing with its domain I have to take into account the informaton about the domain of $f(x)$ ?","['calculus', 'derivatives', 'analysis']"
4847918,What makes a space 'locally Euclidean' Euclidean?,"I come from a Physics background so I realise if my knowledge is lacking in this. I'm following an introduction to manifolds and to start, I found this definition of a 'locally Euclidean' topological space from Wikipedia: A topological space $X$ is called locally Euclidean if there is a non-negative integer $n$ such that every point in $X$ has a neighborhood which is homeomorphic to real n-space $\mathbb{R}^n$ What defines whether a space is 'Euclidean'? To me, I would associate a Euclidean space as a space where we can define a length element $ds^2$ as the sum of the squares of cartesian coordinates eg. $ds^2 = dx^2 + dy^2 + dz^2+...$ . This doesn't seem to fit in this context because no where in the definition of a manifold include a notion of distance. Also, I don't see how having a homeomorphism relates to this notion of distance. So why does the existence of a homeomorphism have anything to do with being Euclidean?","['euclidean-geometry', 'general-topology', 'manifolds']"
4847953,Regular schemes (related to quadratic rings),"Question. Let $X=\mathrm{Spec}\mathbb{Z}[x]/(x^2-p)$ .
Show that $X$ is a regular scheme (i.e., all the local rings $\mathcal{O}_{X,x}$ are regular local rings) if and only if $p=2$ , or $p\equiv 3\pmod{4}.$ Thoughts. It seems to have something to do with the quadratic reciprocity law.
Since $X$ is locally noetherian, it suffices to check that $\mathcal{O}_{X,x}$ is regular for all closed points $x$ of $X$ , which corresponds to maximal ideals of $\mathbb{Z}[x]/(x^2-p)$ .
So we should determine them.
If $\mathfrak{p}$ is prime, then its intersection with $\mathbb{Z}$ is either $p, 0,$ or a prime $\ell\neq p$ . if $\mathfrak{p}\cap\mathbb{Z}=(0)$ , then $\mathfrak{p}$ is of the form $(f(x))$ where $f$ is divisible by $x^2-p$ . But $x^2-p$ is irreducible, so $f(x)=(x^2-p)$ , and $\mathfrak{p}$ is not maximal -- for example, $(x^2-p)\subset (x,p)$ . if $\mathfrak{p}\cap \mathbb{Z}=(p)$ , then $\mathfrak{p}=(x,p)=(x)$ . if $\mathfrak{p} \cap \mathbb{Z} = (\ell)$ for $\ell\neq p$ , then $\mathfrak{p}$ can be viewed as a prime ideal in $\mathbb{F}_\ell[x]/(x^2-p)$ . If $p$ is not a quadratic residue mod $\ell$ , then $x^2-p$ is irreducible over $\mathbb{F}_\ell$ , and $\mathfrak{p}=(\ell, x^2-p)$ . If $p$ is a quadratic residue mod $\ell$ , then $\mathfrak{p}=(\ell,x+g(x))$ or $\mathfrak{p}=(\ell,x-g(x))$ , where $g(x)\in \mathbb{Z}[x]/(x^2-p)$ is chosen such that its reduction mod $\ell$ (inside $\mathbb{F}_\ell[x]/(x^2-p)$ ) is a square root of $p$ . Having classified all closed points of $X$ , I am not sure how to show regularity at all these points under the assumption that $p=2$ or $p\equiv 3\pmod{4}$ . Could someone give me a proof or hint? Thanks in advance! Since $X$ is clearly irreducible of dimension $1$ , we know $\dim \mathcal{O}_{X,x}=1$ . So, we just need to show that $\dim_{k(x)}\mathfrak{m}_x/\mathfrak{m}_x^2=1$ for each closed point $x$ as classified above.
For example, when $\mathfrak{p}=(x)$ , we have $k(\mathfrak{p})=\mathbb{F}_p$ and $\mathfrak{m}_\mathfrak{p}/\mathfrak{m}_\mathfrak{p}^2$ is one-dimensional over the residue field with basis $x$ . So, the point $(x)$ is always regular.","['number-theory', 'algebraic-geometry', 'arithmetic-geometry']"
4847982,Definite integral $\int_{0}^{\pi/2}\frac{\mathrm dx}{1+\sin^4x}$ and series of double factorial $\sum_{n=0}^{\infty}(-1)^n\frac{(4n-1)!!}{(4n)!!}$.,"When I evaluate the following definite integral, $$I=\int_{0}^{\pi/2}\frac{\mathrm dx}{1+\sin^4x},\tag 1$$ I have two methods: the first one: let $u=\cot x$ , then $$I=\int_{0}^{\infty}\frac{u^2+1}{u^4+2u^2+2}\mathrm du=\frac{\sqrt{1+\sqrt{2}}}{4}\pi,$$ this is a integral about rational function who always has an antiderivative (it is not easy to get the antiderivative). The second method is: $$I=\int_{0}^{\pi/2}\sum_{n=0}^{\infty}(-1)^n\sin^{4n}x\ \mathrm dx
=\sum_{n=0}^{\infty}(-1)^n\int_{0}^{\pi/2}\sin^{4n}x\ \mathrm dx
=\frac{\pi}{2}\sum_{n=0}^{\infty}(-1)^n\frac{(4n-1)!!}{(4n)!!}.$$ The series $\sum_{n=0}^{\infty}(-1)^n\frac{(4n-1)!!}{(4n)!!}$ is convergent,
but I have no method to evaluate the sum of this series directly. Combining above two methods, we can get $$\sum_{n=0}^{\infty}(-1)^n\frac{(4n-1)!!}{(4n)!!}=\frac{\sqrt{1+\sqrt{2}}}{2}.\tag 2$$ What I concern most is how to evaluate series $(2)$ directly and evaluate the general integral $$I_n=\int_{0}^{\pi/2}\frac{\mathrm dx}{1+\sin^nx},\quad n=1,2,3,4,\cdots.$$ Does $I_n$ have closed form? For $n=1,2,4$ , we have $$I_1=1,\quad I_2=\frac{\pi}{2\sqrt2},\quad I_3=?,\quad I_4=\frac{\sqrt{1+\sqrt{2}}}{4}\pi.$$","['definite-integrals', 'sequences-and-series']"
4847987,What is $\int_{-\infty}^{\infty}\left(\frac{1}{\tan^{-1}\left(x^{2}\right)+1}-\frac{2}{\pi+2}\right)dx$?,"I was messing around in Desmos and noticed that $\frac{1}{\tan^{-1}\left(x^{2}\right)+1}$ had an asymptote at $y=\frac{2}{\pi+2}$ . I wondered what $\int_{-\infty}^{\infty}\left(\frac{1}{\tan^{-1}\left(x^{2}\right)+1}-\frac{2}{\pi+2}\right)dx$ would be (assuming it converges) and Desmos gave me $1.14450701513$ . However, I want an exact answer (in a closed form preferably) so I also tried to solve it analytically: $$\int_{-\infty}^{\infty}\left(\frac{1}{\tan^{-1}\left(x^{2}\right)+1}-\frac{2}{\pi+2}\right)dx$$ $$2\int_{0}^{\frac{\pi}{2}}\left(\frac{1}{u+1}-\frac{2}{\pi+2}\right)\frac{x^4+1}{2x}du$$ $$\frac{1}{\pi+2}\int_{0}^{\frac{\pi}{2}}\frac{\left(\pi-2u\right)\sec^{2}u}{\left(u+1\right)\sqrt{\tan u}}du$$ After integration by parts with help from @KStarGamer, I got: $$2\int_{0}^{\frac{\pi}{2}}\frac{\sqrt{\tan u}}{\left(u+1\right)^{2}}du$$ Wolfram Alpha says I can represent the integrand as: $$x^\frac{1}{2}-2x^\frac{3}{2}+O(x^\frac{5}{2})$$ But I have not yet learned series expansions or big O notation so I may have made a mistake and I don't know where to go from here.","['integration', 'improper-integrals', 'definite-integrals']"
4848010,Query on methods to solve the equation $\sin(\theta) = -\cos(\theta)$,"in the interval of $0< \theta <360$ solve for $\theta$ , the equation $(1)$ , I would just say $\tan \theta =-1$ and solve that. $$\sin\theta=-\cos\theta\tag1$$ but what if i said $\sin θ +\cos θ =0$ then $\cos θ (\tan θ +1)=0$ . I'd have solutions for $\cos θ =0$ as well as $\tan θ =-1$ , why is doing this method wrong since arithmetically going through the workings out it seems fine , but the solutions for $\cosθ=0$ are wrong? I'm trying to get a verdict on if the 2nd  method is still right or wrong ? i have a feeling its wrong since the solutions dont work, but the  process of me getting to $\cos(θ)⋅[\tanθ+1]=0$ seemed sound that I wouldnt realise it was wrong ? How do I stop myself from carrying out this sort of working out ,is there a way for me to intuitively prove its wrong even before the equation ends up in the form of $\cos(θ)⋅[\tanθ+1]=0$ because if i carried out a similar working out for this question "" Solve for theta , $\sinθ=3\sinθ\cosθ$ "" and solving it similarly like so I'd end up with $\sinθ(1-3\cosθ)=0$ ,and it would give the right solutions.",['trigonometry']
4848057,"Probability of 3 Aces, Last Picked is an Ace","""Aaron picks an integer $k \in [1,52]$ . Then, he draws the first $k$ cards from a standard, shuffled 52-card deck. Aaron wins a prize if the last card he draws is an ace and if there exists exactly one ace in the remaining cards. What $k$ should Aaron pick?"" I am struggling to understand how to get to the probability that Aaron wins with this approach: We need to have 2 aces in the first $k-1$ cards, ${k-1 \choose 2}$ , then the $k^{th}$ card needs to be an ace. Then we need to count the ways of positioning the last remaining ace, which is ${52-k \choose 1}$ . Finally, there are $4!$ ways of ordering the 4 aces. This aims to count all combinations of the 52 cards in the deck that lead to a win. I would then simply divide this by $52!$ the number of possible combinations. But the solution I found takes a different calculation, $\frac{{k-1 \choose 2}{52-k \choose 1}}{{52 \choose 4}}$ . Could you please help me understand this? Then I understand the optimisation step to get to the optimal $k$ , no issue there. Is there a way to approach this problem ignoring cards values, and thinking of it in terms of 52 balls, 48 black and 4 white? Thank you!","['puzzle', 'problem-solving', 'probability']"
4848068,Finding the value of a combined area in a square with center $O$,"I hope this message finds you well. I'm reaching out to ask for your help in solving a challenging geometry problem that I encountered in a recent exam. Despite my best efforts, I haven't been able to find a solution. I'm eager to gain insights that will improve my understanding of this geometric challenge. Problem Description Let $\mathbf{\large{O}}$ be the center of the square $ \mathbf{\large{ABCD}} $ . Find the value of $\mathbf{\large{\frac{[BOEG]+[\Delta DEF]+[\Delta CHF]}{[ABCD]}}}$ in the form $\mathbf{\large{\frac{a}{b}}}$ , where $\mathbf{\large{a}}$ & $\mathbf{\large{b}}$ are natural numbers and coprime. Determine the value of $\mathbf{\large{a + b}}$ . Approach As I predicted the area of $\Delta DEF$ = $[GEFH]$ but I can't figure out a way of proving that if we can prove it somehow then maybe we can go ahead easily. I believe that your expertise in geometry could provide valuable guidance in tackling this problem. Your insights and perspective would be greatly appreciated, and I am confident that your assistance will lead to a breakthrough in my understanding of this complex geometric concept. Thank you in advance for taking the time to consider my request.","['analytic-geometry', 'area', 'geometry']"
4848070,Concentration inequality for a biased random walk on $\mathbb{Z}$,"Consider a biased random walk $(X_s)$ on $\mathbb{Z}$ with the probability of moving to the right equal to $p>1/2$ and the probability of moving to the left equal to $q=1-p$ . Here $X_t$ is the position of the walk at time $t$ . I am wanting to know if a concentration inequality of the form: $$\mathbb{P}(|\max_{s\leq t} X_s - X_t|>f(t))<g(t)\,,$$ where $g(t)\to 0$ as $t\to \infty$ , such that the above inequality holds? The goal for me is to be able to say $|\max_{s\leq t} X_s - X_t|\leq f(t)$ with high probability (say as $t\to \infty$ , this probability goes to one.). If it does, I am wondering what $f(t), g(t)$ could be? My attempt: I know that the distribution of $\min_{s\leq t} X_s$ is geometric from here but I do not know the distribution of $\max_{s\leq t} X_s$ . I could try to apply Markov's inequality and say: \begin{align*}
\mathbb{P}(|\max_{s\leq t} X_s - X_t|>f(t)) &\leq \frac{\mathbb{E}|\max_{s\leq t} X_s - X_t|}{f(t)} \\
&\leq \frac{\mathbb{E}|\max_{s\leq t} X_s|+\mathbb{E}|X_t|}{f(t)} \\
&= \frac{h(t) + c_2t}{f(t)} \,,
\end{align*} where $h(t)=\mathbb{E}|\max_{s\leq t} X_s|, c_2t=\mathbb{E}|X_t|$ . So if I know the distribution of $\max X_s$ , then I can choose $f(t)$ , such that $$\frac{h(t) + c_2t}{f(t)} \to 0\quad\text{as}\quad t\to \infty\,.$$ But I expect better $f(t),g(t)$ are already known using concentration inequalities more powerful than the Markov's inequality since the biased random walk on $\mathbb{Z}$ is a well-studied problem. Any thoughts? Thanks.","['random-walk', 'probability-theory', 'markov-chains']"
4848074,What is a numerical method to solve IVP at irregular singular point?,"I have tried to search for numerical method to solve IVP with irregular singular point but didn't find any. What is the proper method I can use to solve such equations? $$y''+\frac{y'}{x^2}+y=x^3+6x+3$$ With initial conditions $$y(0)=0, y'(0)=0$$ Obviously, collocation method (which I know) won't work. I'm not interested in this particular problem, it's only an example.","['numerical-methods', 'singularity', 'ordinary-differential-equations']"
4848138,Prove that lines connecting intersection points on a conic curve is still tangent to another conic curve,"While working on conic curve problems, I have observed certain similarities. Let $\Gamma$ be a conic curve in the plane, and let $A$ be a moving point on $\Gamma$ . Fix two points $B$ and $C$ in the plane. The lines $AB$ and $AC$ intersect $\Gamma$ at points $D$ and $E$ respectively. My goal is to demonstrate that $DE$ is always a tangent to another fixed conic curve. I attempted to perform a projective transformation such that $BC$ is projected onto an infinite line. Subsequently, an affine transformation is applied so that $\Gamma$ becomes either a circle, a parabola, or a hyperbola. If the outcome of the affine transformation results in a circle, proving the theorem is straightforward, given the fixed angles formed by the lines $AB$ and $AC$ . However, proving the remaining cases through algebraic manipulation proves to be challenging. If $BC$ does not intersect with $\Gamma$ , the entire process is simplified. In this case, we can first project $\Gamma$ onto a circle and then project $BC$ onto an infinite line, making the entire process more manageable. Please let me know if a geometric background is required for the problem. Any form of assistance would be appreciated! UPD : An example image of the description above are as follows, where $DE$ defined above is marked as "" $EG$ "":","['analytic-geometry', 'geometry']"
4848154,For a sequence of random variables $\{X_{n}\}_{n=1}^{+\infty}$ that satisfies the following conditions:,"For a sequence of random variables $\{X_{n}\}_{n=1}^{+\infty}$ that satisfies the following conditions: $X_{n}$ converges in probability to a positive number, i.e., $\exists\ a>0,$ such that $\forall\ \delta>0,$ there is $\mathbb{P}(|X_n-a|>\delta)\rightarrow 0$ . For each $X_{n}$ , we have an almost sure lower bound, i.e., $X_n>b>-\infty\ \ a.s.$ . Can we then conclude that $\sum_{n=1}^{+\infty}X_n=+\infty\ \ a.s.$ ?",['probability-theory']
4848201,Logarithm in two variables,"By definition, the logarithm log of base $b$ is given by $$\log_b(b^x)=x,$$ $$\log_b(1)=0.$$ In a similar fashion, I am trying to define a function that is always decreasing as its input is getting smaller. The problem is that my example is supposed to work with two variables. What I want is $f_b$ such that $$f_b(\frac{n}{b^k}) > f_b(\frac{n}{b^{k+1}})$$ for every $b > 1$ , $n$ , and natural $k$ . Also, it must hold for every allowed value that $$\sum_{k=0}^{\infty}f_b(\frac{n}{b^k}) = \infty.$$ I tried defining it as $$f_b(\frac{n}{b^k})=\frac{1}{n+k},$$ but picking $k=1$ , $n=b$ gives $\frac{1}{b+1}$ , and $n=1$ , $k=0$ gives $1$ , thus $b=0$ . Could you please let me know what a well-defined function for any $b$ could be? Is it even possible?","['systems-of-equations', 'calculus', 'functions']"
4848214,Probability that n people will get one right and one left shoes from n pairs of shoes.,"Suppose we have a closet with n pairs of shoes. n people will randomly pick 2 shoes from the closet. What is the probability that all of them will have 1 right shoe and 1 left shoe? My approach to this question was like this:
We have $C(2n,2)\times C(2n-2,2) \times...\times C(2,2)$ ways for n people to choose all of the shoes.
Now, I can calculate the number of ways that none of the n people will have 1 left and 1 right. Meaning, I can calculate the ways that all n people will either have 2 right or 2 left. Then I will take the complement and divide by the number of total ways that n people can choose 2n shoes randomly Another approach is to count the number of ways that all of them will have both right and left however I don't know how to approach it. I thought that the first person has n ways to choose say right pair, and n ways to choose left. The second has n-1 * n-1 and so on. We get n!*n! but I am not sure if this approach is correct. Also, Can I solve it using probability and not use counting? For instance say I have n/2n for the first person to choose a right pair, then for the left 1/n. How do I continue?",['probability']
4848219,A beautiful mathematical induction problem: $11 \mid 1+2^{2^n}+3^{2^n}+4^{2^n}+5^{2^n}$,"In a book on mathematical induction, I found the following problem: Let $p$ be a prime number of the form $4k+3$ . Prove that for all $n \in \mathbb{N}$ , $\displaystyle\sum\limits_{i=1}^{2 k+1} i^{2^{n}}$ is divisible by $p$ . So I applied this in the particular case where $11=4\cdot2+3$ Prove that for all $n\in\mathbb{N},$ $11 \mid 1+2^{2^n}+3^{2^n}+4^{2^n}+5^{2^n}.$ So, I wrote $P(k+1)$ as follows: $1+(2^{2^n})^2+(3^{2^n})^2+(4^{2^n})^2+(5^ {2^n})^2$ Now, I'm stuck, I can't apply my inductive hypothesis in such a way that proves $P(k+1)$ . What do you think of this discussion?","['elementary-number-theory', 'induction', 'discrete-mathematics']"
4848239,Question on trigonometric notation (specifically wrt parentheses) [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 months ago . Improve this question $\sin x = \sin(x)$ $\sin xy = \sin(x*y)$ $\sin xy = y\sin(x)$ $\sin x*y = \sin(x*y)$ $\sin x*y = y\sin(x)$ #1 is a given, since the whole field discusses ""trigonometric functions"" If #2 were the conventional understanding of the notation, which seems to be the case from a cursory look at various textbooks, then #3 would be at odds, from a semantic perspective, with #2, in which case it would be odd for anyone to assume that #3 should be the correct way to understand the given notation. Am I correct in assuming that #2, not #3, is the norm and nobody really serious about the topic in question assumes #3? If #2 were a variant of #4, then there would be no confusion as to the meaning of the notation; however I've yet to see any expression that would utilize the * as in #4 to express sin(xy). Am I correct to assume that #4 is not part of conventional notation? I apologize if this seems like a strange question.","['notation', 'trigonometry']"
4848260,An Euclidean geometry problem,"Consider two sets of three unit, coplanar vectors $(\mathbf{T}_1,\mathbf{N}_1,\mathbf{S}_1)$ and $(\mathbf{T}_2,\mathbf{N}_2,\mathbf{S}_2)$ . Such that $\mathbf{T}_1\cdot\mathbf{N}_1=\mathbf{T}_2\cdot\mathbf{N}_2=0$ , and $\mathbf{S}_1$ makes an angle $0<\sigma<\pi$ with $\mathbf{N}_1$ , and similarly $\mathbf{S}_2$ makes an angle $0<\rho<\pi$ with $\mathbf{N}_2$ . We align $\mathbf{S}_1$ and $\mathbf{S}_2$ such that $\mathbf{S}_1=-\mathbf{S}_2$ , and impose some $\gamma\neq 0$ or $\pi$ such that $\mathbf{T}_1\cdot\mathbf{T}_2=\cos\gamma$ . The question is, what is $\mathbf{N}_1\cdot\left(\mathbf{T}_1\times\mathbf{T}_2 \right)$ in terms of $\sigma$ , $\rho$ and $\gamma$ ? I approached the problem by writing \begin{align}
0=\mathbf{S}_1+\mathbf{S}_2=\sin\sigma\: \mathbf{T}_1+\cos\sigma\: \mathbf{N}_1+\sin\rho\: \mathbf{T}_2+\cos\rho\: \mathbf{N}_2
\end{align} Then crossing with, say, $\mathbf{T}_2$ and dotting with, say, $\mathbf{N}_1$ we get an equation involving $\mathbf{N}_1\cdot\left(\mathbf{T}_1\times\mathbf{T}_2 \right)$ and $\mathbf{N}_1\cdot\left(\mathbf{N}_2\times\mathbf{T}_2 \right)$ . Crossing with, say, $\mathbf{T}_2$ and dotting with, say, $\mathbf{N}_2$ we get another equation and so on, until we have a homogeneous system of 6 equations involving the 6 mixed products. On top of that we can write $\mathbf{T}_1$ in terms of the basis $(\mathbf{T}_2,\mathbf{N}_2,\mathbf{T}_2\times\mathbf{N}_2)$ and get another equation from which we can extract a non-trivial solution. The answer is \begin{align}
\left(\mathbf{N}_1\cdot\left(\mathbf{T}_1\times\mathbf{T}_2 \right)\right)^2=1-\sec^2\sigma\left(\cos^2\gamma+\sin^2\rho+2\cos\gamma\sin\rho\sin\sigma \right)
\end{align} I'm looking for a more direct, geometrical way to the answer.","['euclidean-geometry', 'vectors', 'geometry']"
4848261,limit of $\frac{x^2}{\ln{|x|}}$,"For unknown reasons I have more problems solving this limit than expected. I want to determine the limit: $\lim_{x \to 0} \sqrt[3]{\dfrac{x^2}{\ln(|x|)}}$ . Using graphic plotter, the limit exists and has to be 0. Well, being rough $\frac{0}{\infty} = 0$ . But that is not very mathematical. Also, L'Hospital does not work for this. I tried using $t(x) = \frac{1}{x}$ , which means $\lim_{t \to \infty} \sqrt[3]{\dfrac{1}{t^2\ln(|\frac{1}{t}|)}}$ , maybe in order to use L'Hospital here. But nope.... Can someone help me? Really appreciate your help.",['limits']
4848266,Bin packing : item to be packed in a certain bin depend on previously packed items to that bin.,"I am working on an engineering problem. I need to implement an algorithm that looks like a certain variant of bin packing. Specifically,  in this variant of the bin packing, the size of a certain item i to be packed in bin j is a function of already packed items in the bin j. Does that variant of the bin packing exist/studied in the literature already? its name? Otherwise do you think that this problem better fits another well known algorithm studied in the literature. Thank you.","['np-complete', 'combinatorics', 'integer-programming', 'discrete-mathematics', 'algorithms']"
4848281,Proving $ \sum_{k=0}^{n-1}\cos^2\left(\frac{2k\pi}{n}\right) = \frac{n}{2}$ with Euler's formula,"I am working on proving some trigonometric identities and I am currently working on the following: For all $n \in \mathbb{N} ,  n \ge 2$ show that $$ \sum_{k=0}^{n-1}\cos^2\left(\frac{2k\pi}{n}\right) = \frac{n}{2}$$ I know that usually the term ""for all n"" might hint a proof with induction, however I have done it a little differently this time and wanted to see how far I can get with some Algebra, like so: $$ \sum_{k=0}^{n-1}\cos^2\left(\frac{2k\pi}{n}\right) =  \sum_{k=0}^{n-1}\frac{\left(e^{\frac{i2k\pi}{n}} + e^{\frac{-i2k\pi}{n}}\right)^{2}}{2^2} $$ $$= \sum_{k=0}^{n-1}\frac{e^{\frac{i4k\pi}{n}} + 2 + e^{\frac{-i4k\pi}{n}}}{4} = \frac{1}{4}\sum_{k=0}^{n-1}\left(e^{\frac{(i4k\pi)}{n}} + 2 + e^{\frac{(-i4k\pi)}{n}}\right) $$ which further gives me $$\frac{1}{4}\left(\sum_{k=0}^{n-1}e^{\frac{i4k\pi}{n}} + \sum_{k=0}^{n-1}2 + \sum_{k=0}^{n-1}e^{\frac{-i4k\pi}{n}}\right) =$$ $$ \frac{1}{4} \left(\frac{1-(x)^{n}}{1-x} +2(n-1) + \frac{1-(y)^{n}}{1-y}\right) $$ for $x$ being $$ x = e^{\frac{i 4 \pi}{n}}$$ and $y$ being $$y = e^{\frac{-i4\pi}{n}}$$ (I apologize, somehow it won't let me format the e-forms into the fractions) and since the fractions both equal zero, I am left with $$\frac{2(n-1)}{4} = \frac{n-1}{2} $$ and now I am confused as to what I have missed to do or where I made my error which prevented me from getting to n/2 .I would be so so grateful if someone could help me double checking my proof and point me to my oversight; as well I always love to see different approaches on how to reach the result and further suggestions are always appreciated. Thank you a lot !! Edit: Error was found. Thanks a lot! Corrected version: $$ \frac{1}{4} \left(\frac{1-(x)^{n}}{1-x} + (2+2(n-1)) + \frac{1-(y)^{n}}{1-y}\right) $$ for $x$ being $$ x = e^{\frac{i 4 \pi}{n}}$$ and $y$ being $$y = e^{\frac{-i4\pi}{n}}$$ which then gives the desired result of $$\frac{2n}{4} = \frac{n}{2} $$","['calculus', 'trigonometry', 'complex-numbers', 'sequences-and-series']"
4848295,Let $k$ be an odd integer. Let $A$ be a matrix such that $A^k = A + I$. Prove that $\det(A) > 0$.,"Problem Let $k$ be an odd integer. Let $A$ be a matrix such that $A^{k} = A+ I$ . Prove that $\det(A) > 0$ . My attempt: We know that $\det(A) = \lambda_1\dots\lambda_n$ where $\lambda_1,\dots,\lambda_n$ are the complex eigenvalues of $A$ . We know that $\lambda_1,\dots,\lambda_n$ are roots of $X^{2k+1} - X -1$ . I can see graphically that the only real root of $X^{2k+1} -X -1$ is positive by plotting some graphs, and since the product of two conjugate complex numbers is positive, I think it has something to do with it. The issue is that if $\lambda$ is an eigenvalue of $A$ , I'm not sure that $\bar{\lambda}$ is an eigenvalue as well since $A$ possibly has complex coefficients (it's not specified in the statement).","['matrices', 'determinant', 'linear-algebra']"
4848315,Why is this conic through a triangle similar to this other one through one of its cevian triangles?,"Update 09/02/24: I have accepted dan_fulea's excellent efforts as an answer so that it doesn't appear as 'unanswered' because this post is no longer the well-defined question I first asked. I'm still interested in other responses too though, especially if people can point to existing literature that may be relevant. Let $\triangle ABC$ be a triangle and $K$ some other generic point (*). Let $\triangle DEF$ be the cevian triangle of $\triangle ABC$ with respect to $K$ . That is, $D$ is where $AK$ meets $BC$ , $E$ is where $BK$ meets $CA$ , and $F$ is where $CK$ meets $AB$ . If $U,V,W$ are the respective midpoints of $EF, FD, DE$ then it can be proved (using areal coordinates for example) that the three lines $AU,BV,CW$ are concurrent, at $T$ say. Let $\mathcal{C}_1$ be the conic through $A,B,C,K$ and $T$ , and $\mathcal{C}_2$ be the conic through $D,E,F,K$ and $T$ . It appears that $\mathcal{C}_1$ and $\mathcal{C}_2$ are directly similar i.e. have the same eccentricity and the same ""orientation"". My questions are whether or not this is known and how to prove explain it? (Update 27/01/24: They in fact do not always have the same eccentricity when they are hyperbolas. See below for details which contains a sketch proof that was found with the help of a computer. I'm still not able to explain the relationship between the two conics so I am updating the question rather than posting an answer.) I was initially interested in drawing these conics because it would follow that, if $P_1$ , $P_2$ are two points on $\mathcal{C}_1$ and $Q_1, Q_2$ are two points on $\mathcal{C}_2$ such that $P_1KQ_1$ are collinear and $P_2TQ_2$ are collinear then $P_1P_2$ and $Q_1Q_2$ are parallel. I can write down the equations of the conics in areal coordinates (with respect to $\triangle ABC$ ) in terms of the coordinates of $K$ , and this allows me to calculate their centres and show using a fairly complicated identity that $\mathcal{C}_1$ is an ellipse/hyperbola if and only if $\mathcal{C}_2$ is an ellipse/hyperbola (making use of formulae from Chapter 2 of The Algebra of Geometry by C. J. Bradley) but I'm not sure if this is the best approach because the algebra seems to be getting too complicated to do much more. (*) Specifically, $K$ is not on $AB,BC$ or $CA$ , nor on the medians of $\triangle ABC$ , nor on the line through $A$ parallel $BC$ , the line though $B$ parallel to $CA$ or the line through $C$ parallel to $AB$ . Update 27/01/24: Having not been able to think of anything else I resorted to getting computer help with the algebra. I am adding some comments here in case of interest. Suppose $K$ has areal coordinates $(l,m,n)$ with respect to $\triangle ABC$ . So $A,B,C$ are represented by $(1,0,0), (0,1,0),(0,0,1)$ and $D,E,F$ by $(0,m,n),(l,0,n),(l,m,0)$ . Note these are not normalised so coordinates don't sum to 1. Now $U$ is given by $(l/(n+l)+l/(m+l),m/(l+m),n/(n+l))$ and $V,W$ symmetrically. It can be verified that the point $T$ with coordinates $(l(m+n),m(n+l),n(l+m))$ lies on $AU,BV$ and $CW$ . (The reference above by C. J. Bradley contains an introduction to areal coordinates and how they are used to prove this sort of thing.) If we let $$ \Phi_{1}^{\text{areal}} = \begin{pmatrix}
	0 & n(l^2 - m^2) & m(n^2 - l^2) \\
	n(l^2 - m^2) & 0 & l(m^2 - n^2) \\
	m(n^2 - l^2) & l(m^2 - n^2) & 0
\end{pmatrix}$$ $$\Phi_{2}^{\text{areal}} = \begin{pmatrix}
	2mn(m-n) & n^2(l - m) & m^2(n - l) \\
	n^2(l - m) & 2nl(n-l) & l^2(m - n) \\
	m^2(n - l) &l^2(m - n) & 2lm(l-m)
\end{pmatrix} $$ then it can be verified that the conics $\mathcal{C}_1, \mathcal{C}_2$ are given by the following equations in areal coordinates $(X, Y, Z)$ $$\begin{pmatrix}
	X \\
	Y \\
	Z
\end{pmatrix}^{\text{tr}} \Phi_{i}^{\text{areal}} \begin{pmatrix}
	X \\
	Y \\
	Z
\end{pmatrix} = 0$$ since it can be checked that they contain $A,B,C,K,T$ and $D,E,F,K,T$ respectively. We have $$\det \Phi_1^{\text{areal}} = 2lmn(n^2 - m^2)(l^2 - n^2)(m^2 - l^2)$$ $$\det \Phi_2^{\text{areal}} = 4l^2m^2n^2(n - m)(l - n)(m - l)$$ so $$
\det \Phi_{1}^{\text{areal}} = \frac{1}{2}\left(1 + \frac{l}{m} \right)\left(1 + \frac{m}{n} \right)\left(1 + \frac{n}{l} \right) \det \Phi_{2}^{\text{areal}}
$$ At this point I found it convenient to translate into Cartesian coordinates. If we write $$ T = \begin{pmatrix}
	p_1 & q_1 & r_1 \\
	p_2 & q_2 & r_2 \\
	1 & 1 & 1
\end{pmatrix}$$ where in Cartesian coordinates now $A = (p_1, p_2), B = (q_1, q_2), C = (r_1, r_2)$ then a point with areal coordinates $(X, Y, X)$ with $X+Y+Z=1$ has Cartesian coordinates $(x, y)$ given by $$\begin{pmatrix}
	x \\
	y \\
	1
\end{pmatrix} = T \begin{pmatrix}
	X \\
	Y \\
	Z
\end{pmatrix}$$ (We could simplify by applying a suitable affine transformation to translate $C$ to the origin, rotate $B$ to the $y$ -axis and rescale so that $A = (p, q), B = (0, 1) $ and $C = (0, 0)$ for some $p, q$ .) Define for $i = 1$ and $2$ $$\Phi_i^{\text{cart}} = \begin{pmatrix}
	a_i & h_i & g_i \\
	h_i & b_i & f_i \\
	g_i & f_i & c_i
\end{pmatrix} = (T^{-1})^{\text{tr}} \Phi_i^{\text{areal}} T^{-1}$$ so that $\mathcal{C}_i$ is given by the equation $$\begin{pmatrix}
	x \\
	y \\
	1
\end{pmatrix}^{\text{tr}}\Phi_i^{\text{cart}}\begin{pmatrix}
	x \\
	y \\
	1
\end{pmatrix} = 0.$$ I cannot explain why but it turns out that $a_1 = a_2, b_1=b_2, h_1=h_2.$ So in particular the discriminants of the two conics $\Delta_i = h_i^2 - a_i b_i$ are the same and both equal to: $$\frac{1}{(\det T)^2}(l + m)(l + n)(m + n)\left(l^2(m+n) + m^2(n+l) + n^2(l+m) - 6lmn\right).$$ Hence $\mathcal{C}_1$ is an ellipse/hyperbola if and only if $\mathcal{C}_2$ is an ellipse/hyperbola. I haven't looked much at edge and degenerate cases but it can be seen that $\Delta_i = 0$ contains three lines and a cubic where the zero locus of the cubic looks to result in a pair of parabolas. This also shows that our conics have the same ""orientation"" since the gradients of the major/minor axis of an ellipse and asymptotes of a hyperbola only depend on $a_i, b_i, h_i$ . (See here and here for formulas.) Now the eccentricity depends only on $a_i, b_i, h_i$ and the sign of $\det \Phi^{\text{cart}}_i$ which is the same as the sign of $\Phi_i^{\text{areal}}$ . (See here for a formula.) By the formula above, they will have the same eccentricity if $$ \left(1 + \frac{l}{m} \right)\left(1 + \frac{m}{n} \right)\left(1 + \frac{n}{l} \right) > 0.$$ This partitions the plane by lines into regions with the same/different eccentricity which I didn't initially appreciate. Basically, when they are hyperbola, sometimes they are ""on opposite sides of the asymptotes"" and sometimes the same. The parallel line property depicted above seems to hold in all non-degenerate cases so I guess this may come down to relations involving $a, b, h$ only. It would be nice have a little more insight into what is going on. For example, I still think it is interesting to ask if it can be shown more directly that $A,B,C,K,T$ are con-cyclic if and only if $D,E,F,K,T$ are con-cyclic without resorting to this much algebra. I also feel like it remains to be explained why $a_1 = a_2, b_1=b_2, h_1=h_2$ but that it might be related to the parallel line property? Update 09/02/24: Adding a few extra remarks partially in response to the dan_fulea's excellent efforts because they didn't fit as a comment. (i) I hadn't thought to draw the conic through $UVWKT$ so it's cool that this is also related as it is to $\mathcal{C}_1$ and $\mathcal{C}_2$ . Let's call this $\mathcal{C}_3(UVWKT)$ . I think the fourth circle you've defined as being the circle through $\text{circumcentre}(\triangle ABC)$ , $G(\triangle DEF), K, T,$ where $G(\triangle)$ is the centroid of $\triangle$ , is perhaps best described as the conic $\mathcal{C}_4(\text{centre}(\mathcal{C}_1)G(\triangle ABC)G(\triangle DEF)KT)$ . i.e. noticing it also passes through $G(\triangle ABC)$ we have 5 points to define it naturally in the general setting when $ABCKT$ are not necessarily concyclic. Your property that $O(C1),O(C2),O(C3),O(C4),G(\triangle DEF)$ are collinear and the distances between them are in the stated ratios seems to still hold. I suppose it might be possible to write down the equations of $\mathcal{C}_3, \mathcal{C}_4$ in areal coordinates as has been done for $\mathcal{C}_1, \mathcal{C}_2$ to prove the general connection for these conics too. It is nice that the line through the centres of the these 4 conics is fairly easily described as the line through the centre of $\mathcal{C}_1$ (generally not the circumcentre of $ABC$ ) and the centroid of $DEF$ . (ii) I think your observation that if $\mathcal{C}_1$ is a circle then $DEF$ and $ABC$ are similar is also really nice. Notice that when they are circles, as well as your relations (*), from the equation for $\mathcal{C}_1$ we also have $a^2:b^2:c^2 = l(m^2-n^2):m(n^2-l^2):n(l^2-m^2)$ that can be used. (ii) It is easy to see from the areal coordinate equation of C1 that the isotomic conjugate of $K$ (call it $K_t$ with areal coordinates $(1/l,1/m,1/n)$ ) also lies on $\mathcal{C}_1$ . It is known that this conic $\mathcal{C}_1$ through $ABKK_t$ is a parabola if and only if $K$ (equivalently $K_t$ ) lies on the cubic curve mentioned in the original post. This cubic curve is known as the Tucker Nodal Cubic . (iii) The point $T$ is the complement of the isotomic conjugate of $K$ . I don't know if it has a specific name. (iv) There are corresponding 'isotomic analogues' of $\mathcal{C}_2$ , $\mathcal{C}_3$ , $\mathcal{C}_4$ which are quite fun to draw in which $K$ is replaced by the isotomic conjugate of $K$ . These are likewise related to $\mathcal{C}_1$ but are even more closely related to each other because these 'isotomic counterparts' also have the 'same size'. (v) The line through $K$ and $G(\triangle ABC)$ is tangent to $\mathcal{C}_2$ . The line through the isotomic conjugate of $K$ and $G(\triangle ABC)$ is tangent to $\mathcal{C}_3$ . (vi) There's so much symmetry in the equations, as well as the isotomic conjugate, I'm now wondering what happens if we permute the areal coordinates of $K$ and draw $\mathcal{C}_1$ , $\mathcal{C}_2$ , $\mathcal{C}_3$ , $\mathcal{C}_4$ again...?","['euclidean-geometry', 'homothety', 'conic-sections', 'geometry']"
4848320,Hamiltonian path for a $N\times N$ grid. Focus on all the possible ending points,"On a $3\times3$ grid, draw a path that starts in the center of a square in the corner of the grid,
and repeatedly moves from the center of one square to the center of an adjacent square,
without visiting any squares twice, and such that every square in the grid is included.
For example, the following is a path that meets these conditions: (a) What are the possible ending positions for a path drawn according to these rules? What
if you started from an edge square or the center square of the grid? Explain. (b) Repeat the above problem for a $4\times4$ grid. (c) Can you describe what happens in general for an n×n grid where n is a positive integer? My work: For the $3\times3$ grid, I tested all possible combinations and I found that the when starting from a corner square: All other corner squares and the center square was reached.
When I started from the center square, I reached all corner squares. Question:
For a $4\times4$ grid, and a $N\times N$ grid, what are the possible ending points? Here is a diagram for reference. This is a valid path for a $3\times3$ grid.","['graph-theory', 'combinatorics', 'hamiltonian-path']"
4848464,Why is it possible to cancel a function in $\frac{\partial f}{\partial x}$?,"I am a freshman engineering student. In my university, in the first semester we studied differentiation and continuity, infinite series and conic sections in mathematics and some thermodynamics in physics. In second term we study integration and Linear algebra in mathematics and Schrödinger equations in physics. I don’t know if this is standard or normal for other countries but all my friends in other universities have to take Schrödinger  in second semester. This seems like we are trying to run before learning how to walk. $\require{cancel}$ I want to ask how we can do this $\frac{ \partial w(x,t)}{ \partial x}= \frac{2i\pi w(x,t) P}{h} $ then $\frac{ \partial \cancel{ w(x,t)}}{ \partial x}= \frac{2i\pi \cancel{ w(x,t)} P}{h} $ in other words $\frac{ \partial}{ \partial x}= \frac{2i\pi  P}{h} $ I want to ask, how is it possible to cancel a function this way and what does $\frac{\partial }{\partial x}$ even mean without a function?
When I tried to ask my professor, he told me something about linear operators and I didn’t understand a word from him. When I googled this, it was something related to linear algebra.
Is it possible to explain this to someone who didn’t study multi variable calculus or linear algebra or ODE or PDE? Another question is: Is it normal to take Schrödinger equations in the second semester of the first year without proper mathematics?","['partial-derivative', 'calculus', 'derivatives', 'partial-differential-equations']"
4848480,2 tables of 6 people: What's a schedule such that all pairs share a table for an equal amount of time?,"The problem There are 2 tables seating 6 people each. With 12 people, how many arrangements (with all 12 people seated) are necessary so that every pair shares a table for the same number of arrangements? Example Here's an example with 4 arrangements: Arrangement 1: 1,2,3,4,5,6 and 7,8,9,10,11,12 Arrangement 2: 1,2,3,4,11,12 and 7,8,9,10,5,6 Arrangement 3: 1,2,9,10,5,6 and 7,8,3,4,11,12 Arrangement 4: 7,8,3,4,5,6 and 1,2,9,10,11,12 1 and 2 share a table 4 times. 1 and 3 share a table 2 times. 1 and 4 share a table 2 times. ... 11 and 12 share a table 4 times. This is an invalid answer, because not all pairs share a table the same number of times. My reasoning so far In a single arrangement, a given person shares a table with 5 people out of 11 (other people). So, that person must sit with every other person 5 out of 11 times. So the answer must be a multiple of 11. There are $\frac{\binom{12}{6}}{2}= 462$ unique arrangements, so that's an upper bound. (Division by two because the two tables are interchangeable). (I'm also curious about the same problem, but with 2 tables of $n$ people)","['combinatorial-designs', 'recreational-mathematics', 'combinatorics']"
4848516,Implicit differentiation gives different answer,"I'm trying to implicitly differentiate the following equation: $\frac{x+f(x)}{c+f(x)}=4$ where $c$ is a constant. Here's what I get: $$
\frac{\mathrm d}{dx}\left(\frac{x+f(x)}{c+f(x)}\right)=\frac{\mathrm d}{\mathrm dx}(4)
$$ $$
\frac{(1+f'(x))(c+f(x))-(x+f(x))(f'(x))}{(c+f(x))^2}=0
$$ $$
\frac{c+f(x)+cf'(x)+f(x)f'(x)-xf'(x)-f(x)f'(x)}{(c+f(x))^2}=0
$$ $$
c+f(x)+cf'(x)-xf'(x)=0
$$ $$
c+f(x)=xf'(x)-cf'(x)
$$ $$
f'(x)=\frac{c+f(x)}{x-c}
$$ However, when I isolate $f(x)$ first... $$
\frac{x+f(x)}{c+f(x)}=4
$$ $$
x+f(x)=4c+4f(x)
$$ $$
x-4c=3f(x)
$$ $$
\frac{x}{3}-\frac{4}{3}c=f(x)
$$ ... I get a different answer: $$
f'(x)=\frac{\mathrm d}{\mathrm dx}\left(\frac{x}{3}-\frac{4}{3}c\right)
$$ $$
f'(x)=\frac{1}{3}
$$ Hopefully someone can shed some light on where I'm going wrong. Thanks!","['calculus', 'implicit-differentiation', 'derivatives']"
4848608,The Final Fantasy VIII leveling up problem,"1. Introduction In the video game Final Fantasy VIII, you control six characters. Each character starts the game with the following amount of experience points: Character Initial EXP $C_1$ $6000$ $C_2$ $7000$ $C_3$ $7000$ $C_4$ $7000$ $C_5$ $10000$ $C_6$ $12000$ The goal is to engage in battles, win them and receive experience, until each character reaches $99000$ points. 2. Rules The following rules must be observed: Rule 1: Only three characters may engage in a battle at the same time. These are called active party members. The inactive party members do not receive experience points. The player can switch the group of active party members anytime between battles. $C_1$ is the group leader and must always be active . Rule 2: Only two of the three active party members may receive experience points after winning a battle. The player can choose which characters receive the points. The leader $C_1$ may or may not receive experience, as the player decides. Rule 3: One of the three active party members receives a bonus experience after winning a battle. The player decides which character receives the bonus points. 3. Mechanics The amount of experience received after each battle is given by the formula: $EXP=240\cdot(5\cdot (\dfrac{100-X}{X})+4)$ The variable $X$ stands for the average active party level. Considering $C_x$ , $C_y$ and $C_z$ are the active party members, then: $X=\dfrac{(C_x+C_y+C_z)}{3000}+1$ The amount of bonus experience received after each battle is given by the formula: $BONUS=30\cdot(5\cdot (\dfrac{100-Y}{Y})+4)$ The variable $Y$ stands for the character level that earns the bonus points. Considering $C_x$ the character to receive the bonus, then: $Y=\dfrac{C_x}{1000}+1$ 4. Example Since the amount of experience points the party receives after each battle lowers, as the characters earn more points, the general idea is to keep one or two low leveled characters in the active party, in order to boost the experience in each battle. Another general idea is that, since the group leader $C_1$ may not be switched out, it is best to level him up last, so that he drags down the average party level as long as possible. Quick example: Battles EXP Bonus Member 1 Member 2 Member 3 $0$ $0$ $0$ $C_1: 6000$ $C_2: 7000$ $C_3: 7000$ $1$ $16902$ $1845$ to $C_2$ $C_1: 6000$ $C_2: 25747$ $C_3: 23902$ $2$ $6075$ $595$ to $C_2$ $C_1: 6000$ $C_2: 32417$ $C_3: 29977$ $3$ $4977$ $424$ to $C_2$ $C_1: 6000$ $C_2: 37818$ $C_3: 34954$ Again, the player can change the active party members anytime, except for the group leader $C_1$ . Which is the minimum amount of battles required to complete the challenge?","['optimization', 'calculus']"
4848630,Calculate the distance between $AC$ and $BD$ in the regular tetrahedron $ABCD$.,"Question Consider the regular tetrahedron $ABCD$ . The points $M$ and $P$ are the means of the segments $BC$ and $AM$ , respectively. On the edge $BD$ take the point $K$ so that $BK = 3\times KD$ Knowing that the distance between the lines $PK$ and $AC$ is $\sqrt{3}$ cm, calculate the distance between $AC$ and $BD$ . My idea **Drawing ** I know that most of these types of problems are solved using the volumes expressed in two ways. So we can easily show that $AC \perp BD$ , which makes the angle between these two lines $90^{\circ}$ We can use Chasles Formula (I explained it in the comments) and we get that the volume of the regular tetrahedron is $= \frac{AC\times BD\times dis(AC, BD)\times\sin{90^{\circ}}}{6}$ . We can say that because we have a regular tetrahedron $AC=BD$ and we also know that $\sin{90^{\circ}}=1$ If we can express the volume again using the given distance we can find the distance between $AC$ and $BD$ . I don't know where to start! I hope one of you can help me! Thank you!","['angle', 'geometry', 'volume']"
4848640,Is this proof for divergence of harmonic series rigorous? Can it be made so?,"Bartle & Sherbert, edition 4, page-97 gives the following proof: Assume that the series converges to some $S$ : $$\implies S= 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots$$ Then they proceed to derive an inequality by converting the terms of the type $\frac{1}{2n-1}+\frac{1}{2n}$ into $\frac{1}{2n}+\frac{1}{2n}=\frac{1}{n}$ $$1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+ \dots > \left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) +\dots=1+\frac{1}{2}+\frac{1}{3} \dots=S$$ They have shown $S>S$ from the assumption, hence, there's a contradiction. But I do not like their insistence upon using the "" $\dots$ "", as I feel like they're hiding something behind that. If we do a similar analysis a bit more rigorously, i.e, working solely with sequences of partial sums up to a defined, fixed $n$ , (instead of those dots), we have: $$S_{2n}=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4} \dots +\frac{1}{2n-1}+\frac{1}{2n}$$ $$S_{2n} >\left(\frac{1}{2}+\frac{1}{2}\right)+\left(\frac{1}{4}+\frac{1}{4}\right) \dots +\left(\frac{1}{2n}+\frac{1}{2n}\right)=1+\frac{1}{2}+\frac{1}{3} \dots \frac{1}{n}=S_n \implies S_{2n} > S_{n}$$ This is hardly news, since the 1-harmonic is monotone increasing. Now assuming that the sequence $S_n$ converges, and passing onto the limit in the above equation yields: $$S \geq S$$ which is fine ?","['calculus', 'sequences-and-series', 'real-analysis']"
4848655,How would understanding 'change of basis' explain where I went wrong?,"(Sorry in advance if I don't get all the terminology right, and for the lengthy question!) Intro I've got two $2\times 2$ matrices called $\textbf{M}$ and $\textbf{N}$ , which represent the linear transformations M and N respectively. $$\textbf{M} =\begin{bmatrix}a & b\\c & d\end{bmatrix}; \quad \textbf{N} =\begin{bmatrix}e & f\\g & h\end{bmatrix}$$ I've got two methods (A and B) to try and find $\textbf{P}=\textbf{MN}$ but the results they give me are inconsistent! Can anyone explain why?? (A) The Long-Winded Way I know that the columns of a $2\times 2$ matrix are the images of $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ respectively when I apply the linear transformation represented by that matrix to a coordinate axes, S, with unit vectors $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . And these images are described as column vectors 'as seen by' S - i.e. the column vectors are in 'the language of' $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . (I'll call this my 'theorem'.) Let S' be the coordinate axes with unit vectors $\hat{\textbf{i}'}$ and $\hat{\textbf{j}'}$ and S'' be the coordinate axes with unit vectors $\hat{\textbf{i}''}$ and $\hat{\textbf{j}''}$ . I'll also say that S' is the image of S under the transformation N and S'' is the image of S' under the transformation M. (I'm applying N before M!) So using my theorem I know that: $$\hat{\textbf{i}'} = e\hat{\textbf{i}} + g\hat{\textbf{j}}$$ $$\hat{\textbf{j}'} = f\hat{\textbf{i}} + h\hat{\textbf{j}}$$ because the unit vectors of S' are the images of those of S, under the transformation that $\textbf{N}$ represents. Similarly, I can reason that: $$\hat{\textbf{i}''} = a\hat{\textbf{i}'} + c\hat{\textbf{j}'}$$ $$\hat{\textbf{j}''} = b\hat{\textbf{i}'} + d\hat{\textbf{j}'}$$ because the unit vectors of S'' are the images of those of S' under the transformation M, and these images are described as column vectors (in terms of the unit vectors of S') in the columns of $\textbf{M}$ . So, by substituting the top 2 equations into the bottom 2: $$\hat{\textbf{i}''} = a(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + c(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (ae+cf)\hat{\textbf{i}} + (ag+ch)\hat{\textbf{j}}$$ $$\hat{\textbf{j}''} = b(e\hat{\textbf{i}} + g\hat{\textbf{j}}) + d(f\hat{\textbf{i}} + h\hat{\textbf{j}}) = (be+df)\hat{\textbf{i}} + (bg+dh)\hat{\textbf{j}}$$ Now that I have expressed the unit vectors of S'' in 'the language of' S, I should be able to construct a matrix (call it $\textbf{P}$ ) which describes the single transformation from S to S''. It's LHS and RHS columns will be the unit vectors $\hat{\textbf{i}''}$ and $\hat{\textbf{j}''}$ expressed as column vectors in terms of $\hat{\textbf{i}}$ and $\hat{\textbf{j}}$ . Therefore, $$\textbf{P} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix}$$ But I also know that the matrix $\textbf{MN}$ describes the transformation N, followed by M. So if applied to S, it should transfrom it to S', then S'' - i.e. it would have the exact same effect as if we just applied the transfromation represented by $\textbf{P}$ to S! So $\textbf{MN} = \textbf{P}$ because they describe the same transformation, and therefore $$\textbf{MN} = \begin{bmatrix}(ae+cf) & (be+df)\\(ag+ch) & (bg+dh)\end{bmatrix}$$ (B) The Normal Matrix Multiplier Algorithm For this method I just use the normal method where you go left to right along the rth row of $\textbf{M}$ and down the qth column of $\textbf{N}$ , multiplying corresponding elements and adding all the products together, to get the element in the rth row and qth column of $\textbf{MN}$ . $$\textbf{MN} = \begin{bmatrix}a & b\\c & d\end{bmatrix}\times \begin{bmatrix}e & f\\g & h\end{bmatrix} = \begin{bmatrix}(ae+bg) & (af+bh)\\(ce+dg) & (cf+dh)\end{bmatrix}$$ Conclusion So why are the two methods giving me different results? Can anyone point out a mistake I made. I asked a friend studying maths at uni and he said it's got something to do with a 'change of basis' but I didn't really understand. (I fear that this might be a big concept so, if it is to do with this and it's too much to explain here, could anyone point me to good videos/websites/pdfs/online resources that explain it well?) Thank you!","['matrices', 'change-of-basis', 'linear-transformations']"
4848659,Perpendicular distance from stright line to point on arc,"I have a line with a circular arc cutting through it at 2 points, A and B. Given that I know the length of line CD (which is the distance between line AB and the highest point on the arc - perpendicular to AB) and I know the length of line AB, is it possible to calculate the distance between any point on line AB and its corresponding perpendicular point on the arc?
for example, if point E is half way between points A and C, how do I calculate the length of EF?","['pi', 'geometry']"
4848669,Is a function a set or a rule?,"My textbook says that a function is a set, and that it is a kind of relation, which is also a set. Now: $$f(x) = x+5$$ is called a function, but the above expression is not a set. This is also true for other functions, like trigonometric functions such as $\sin x$ , etc. I have heard arguments that a function is a rule , and it is expressed as a set. But when we say a function is a kind of relation, this directly implies that it is a set. So, what is a function?","['elementary-set-theory', 'functions', 'relations']"
4848687,Determine the entries $x$ and $y$ in a matrix so that its only eigenvalue is $1$.,"I am doing some self-study in preparation for an exam, and in this problem I am given the following matrix in $R^{3×3}$ : $\begin{pmatrix}
1&0&1\\
0&1&-1\\
0&x&y\\
\end{pmatrix}$ I need to determine the set of x and y values so that the only eigenvalue of this matrix is one. At first glance, we have $x = 0$ and $y = 1$ , since that would make this into a unipotent matrix and I know that the only eigenvalue of a unipotent matrix equals 1. Now consider the characteristic polynomial of the matrix, which I have determined to be $p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y)$ . We see that $a_2 = (y + 2)$ and $a_0 = (x+y)$ , and that $a_2$ is the sum of eigenvalues and that $a_0$ is the product of eigenvalues, both of which must be one. Solving for x and y gives us x = 2 and y = -1. For the third method, consider the companion matrix of the characteristic polynomial. Transforming $p(\lambda) := -\lambda^3 + (y + 2)\lambda^2 - (x + 2x + 1)\lambda + (x+y)$ to the monic polynomial $p(\lambda) := \lambda^3 - (y + 2)\lambda^2 + (x + 2x + 1)\lambda - (x+y)$ and then determining the companion matrix, we get: C(p) := $\begin{pmatrix}
0&0&x+y\\
1&0&-(x+2x+1)\\
0&1&y+2\\
\end{pmatrix}$ If the sum of all columns equals 1, then one of the eigenvalues of this matrix equals 1. Summing the third column gives us a value of 1 for all x and y. Therefore, knowing that the eigenvalues of C(p) are the roots of $p(\lambda)$ , we see that 1 is an eigenvalue of the original matrix for all x and y. To see that it is the only real eigenvalue, I factor $(\lambda - 1)$ out of the original monic polynomial and solve for the remaining quadratic coefficients. I get: $p(\lambda) := (\lambda - 1)(\lambda^2 - (y + 1)\lambda + (x+y))$ Solving for the quadratic coefficients gives us: $\lambda_{2,3} = \frac{(y+1) ± \sqrt{(y^2 - 2y -4x + 1)}}{2}$ So if we find x and y in the real numbers so that the term under the radical is less than zero, we have found x and y where 1 is the only real eigenvalue. From this point I am stuck.","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'characteristic-polynomial']"
4848745,Sum of measures is less than measure of sum,"I am asked to solve the following exercise: let $A,B$ be Lebesgue measurable subsets of the real line. Prove that $$\lambda(A) + \lambda(B) \leq \lambda(A+B).$$ I am aware of a related question, but that only pertains to finding a counter example so that equality does not hold. A standard example being $A = \mathbb{Z}$ and $B= [0,1],$ with $A+B = \mathbb{R}$ . What I’ve tried: I’ve found that it suffices to assume $A$ and $B$ have finite, nonzero measure, since these cases are trivial. I’ve also found that we may assume that both sets contain $0$ , which is probably useful. (The statement for general $A$ , $B$ then follows by translation invariance.) Then I got stuck. Any hints or solutions? Thanks in advance.","['measure-theory', 'lebesgue-measure']"
4848747,What is the length of AR in the given triangle ABC?,"I need your help with a question on triangles. It is from the entrance exam of an institute in India for students in 10th grade. In the given triangle ABC, if DP, MQ AR and ES are perpendiculars to BC such that DP = 6 cm, MQ = 4 cm and ES = 8 cm Then find AR. The image of the figure👆 The image of the original question with options 👇 MY ATTEMPT: I tried to use Pythagoras theorem and tried substituting Pythagorean triplets to find the base. Then I also tried similarity of triangles △DPC & △MQC and △ESB &△MQB to find the base BC. I even tried to join AM and then extend it to intersect BC at F so as to use the Menelaus theorem and Ceva theorem. However, that didn't work for me too. Any help would mean a lot to me.","['euclidean-geometry', 'triangles', 'geometry']"
4848778,Can affine transformation make two intersective lines become parallel?,"The book Geometry of Conics , by Akopyan and Zaslavsky, states a problem as follows: Probelem 9 A line intersects a hyperbola at points $P$ and $Q$ and
its asymptotes at points $X$ and $Y$ . Prove that the segments $PX$ and $QY$ are equal. Besides, it gives a proof as follows: Apply an affine transformation such that the line becomes parallel to one of the axes of the hyperbola. Then the desired equality would follow from symmetry. But since the segments in question lie on a line, their images under the affine transformation will also be equal. It's well-known that the affine transformation preserves the parallelism, in another word, under such transformation, parallel lines are also parallel, and intersective lines are still intersective. How can an affine transformation make the line and the axis (not always parallel) become parallel?","['conic-sections', 'geometry']"
4848804,Power of a laser beam,"I have the following function $$u(x,y,z)=\frac{Ak_0w_0^2}{2iz+k_0w_0^2}\exp\left(-k_0\frac{x^2+y^2}{2iz+k_0w_0^2}\right),$$ where $A,k_0,w_0$ are constants. I want to compute the following quantity $$P=\int\int_{\mathbb{R}^2}|u(x,y,z)|^2dxdy,$$ showing that it does not depend on $z$ . I dont really know how to deal with $|u(x,y,z)|$ . I tried to separate $$\frac{1}{2iz+k_0w_0^2},$$ into real and imaginary part but i don't end up with something manageable. Any help will be very appreciated! EDITED: Asuming $z>0$ , from @K.dfaoite answer $$G(z)=\iint_{\mathbb R^2}|u(x,y;z)|^2\mathrm dx\mathrm dy \\ =\left|\frac{Ak_0{\omega_0}^2}{2\mathrm iz+k_0{\omega_0}^2}\right|^2\iint_{\mathbb R^2}\exp\left[-2 \mathfrak R\left(\frac{k_0}{2\mathrm iz+k_0{\omega_0}^2}\right)~(x^2+y^2)\right]\mathrm dx\mathrm dy.$$ Since $$\frac{1}{2iz+k_0w_0^2}=\frac{k_0w_0^2-2iz}{(k_0w_0^2)^2-4z^2}.$$ we have that $$-2 \mathfrak R\left(\frac{k_0}{2\mathrm iz+k_0{\omega_0}^2}\right)=-\frac{2k_0^2w_0^2}{k_0^2w_0^4-4z^2},$$ so $$\iint_{\mathbb R^2}\exp\left[-2 \mathfrak R\left(\frac{k_0}{2\mathrm iz+k_0{\omega_0}^2}\right)~(x^2+y^2)\right]\mathrm dx\mathrm dy=\pi\frac{k_0^2w_0^4-4z^2}{2k_0^2w_0^2}.$$ On the other hand $$\left|\frac{Ak_0{\omega_0}^2}{2\mathrm iz+k_0{\omega_0}^2}\right|^2=A^2k_0^4w_0^4\left|\frac{1}{2iz+k_0w_0^2}\right|^2=\frac{A^2k_0^2w_0^4}{(k_0^2w_0^4-4z^2)^2}(k_0^2w_0^4+4z^2),$$ using both thins we clearly see that the power $P$ does depend on $z$ , $$P=\frac{\pi A^2w_0^2}{2}\frac{k_0^2w_0^4+4z^2}{k_0^2w_0^4-4z^2},$$ which is not the result that should be... I don't see any mistake. Any help will be very appreciated!!","['complex-analysis', 'multivariable-calculus', 'definite-integrals']"
4848808,"Show that $X_{t}:=\alpha X_{t-1}+\epsilon_{t}$ is strictly stationary for $|\alpha|<1$ and $\epsilon_{t}$ i.i.d$~\sim N(0,\sigma^{2})$.","The title can be shortened to ""prove that $AR(1)$ processes are strictly stationary when $|\alpha|<1$ "". This has been discussed many times on MSE and Cross Validated, but I found no mathematical proof of why it is strictly stationary. For $t\in\mathbb{Z}$ , consider the process recursively defined as $X_{t}:=\alpha X_{t-1}+\epsilon_{t}$ where $\epsilon_{t}$ s are i.i.d $\sim N(0,\sigma^{2})$ . I want to show that the process $\{X_{t}:t\in\mathbb{Z}\}$ is strictly stationary when $|\alpha|<1$ . I have a not-bad attempt, but I got stuck in the end. First, by the recursive relation, it is easy to see that $$X_{t}=\alpha^{n}X_{t-n}+\sum_{k=0}^{n-1}\alpha^{k}\epsilon_{t-k}.$$ Define $Y_{n}:=\sum_{k=0}^{n-1}\alpha^{k}\epsilon_{t-k}$ . We recall that any linear combination of independent Gaussian random variables is Gaussian. In particular, since $\epsilon_{t}$ is i.i.d $N(0,\sigma^{2})$ , it follows that $Y_{n}\sim N(0,\sigma^{2}\sum_{k=0}^{n-1}\alpha^{2k})$ . We set $\sigma_{n}^{2}:=\sigma^{2}\sum_{k=0}^{n-1}\alpha^{2k}$ . Note that since $|\alpha|<1$ , when $n\rightarrow\infty$ , the variance converges $$\sigma_{n}^{2}\longrightarrow\sigma^{2}\sum_{k=0}^{\infty}\alpha^{2k}=\dfrac{\sigma^{2}}{1-\alpha^{2}}.$$ Consider the characteristic function $\varphi_{n}(t)$ of $Y_{n}$ . It is of the following form $$\varphi_{n}(t):=\mathbb{E}(e^{itY_{n}})=e^{it\mu_{Y_{n}}-\frac{1}{2}\sigma^{2}_{Y_{n}}t^{2}}=e^{-\frac{1}{2}\sigma^{2}_{n}t^{2}}\longrightarrow e^{-\frac{1}{2}\frac{\sigma^{2}}{1-\alpha^{2}}t^{2}}=:\varphi(t).$$ We note that $\varphi(t)$ is the characteristic function of $Y\sim N(0,\frac{\sigma^{2}}{1-\alpha^{2}})$ and $\varphi(t)$ is continuous at $t=0$ . Hence, in view of the Lévy's continuity theorem , it follows that $Y_{n}\longrightarrow Y\ \text{in distribution}$ when $n\rightarrow\infty$ . Now, since $X_{t}=\alpha^{n}X_{t-n}+Y_{n}$ and since $|\alpha|<1$ , we have $$X_{t}=\alpha^{n}X_{t-n}+Y_{n}\longrightarrow Y,\ \text{in distribution, as}\ n\rightarrow\infty.$$ This is where I got stuck. From here, we can see that, since the argument works for every $t\in\mathbb{Z}$ , it follows that $X_{t}=_{d}Y$ for all $t$ . So, every random variable in the process is Gaussian with the same parameters. This does not imply strict stationarity directly because it does not specify the joint distribution. However, since $X_{t}$ s are Gaussian, weak stationarity and strict stationarity are equivalent. I know that $\mathbb{E}X_{t}=\mathbb{E}Y=0$ and $\mathbb{E}X_{t}^{2}=\mathbb{E}Y^{2}=\frac{\sigma^{2}}{1-\alpha}.$ However, I still don't know how to compute $\text{Cov}(X_{t},X_{s})$ . Don't I need the joint distribution condition of $(X_{t},X_{s})$ for this? An extension from this is the following question: How can I show that $X_{t}$ is not strictly stationary when $\alpha=1$ (let us assume real space). I know that when $\alpha=1$ , the above argument does not work. And the variance blows up. But that has nothing to say about the strict stationarity. What about $|\alpha|>1$ ?","['statistics', 'stationary-processes', 'stochastic-processes', 'time-series', 'probability-theory']"
4848809,When is it possible to transform limits using substitution?,"In calculus, if I have to calculate $\lim_{x \to x_0}f(x)$ , we can usually do substitutions of the from $x=\phi(y)$ and then our limit would be $\lim_{y \to \phi(x_0)}f(\phi(y))$ . However, we cannot just choose some arbitrary $\phi$ . As a counterexample, take $\lim_{x \to 2}x$ . You cannot take $x=\phi(y)=0$ , so my intuition says that $\phi$ needs to be bijective in a small enough neighbourhood of $x_0$ . Also, I would assume that $\phi$ needs to be continuous at $x_0$ . The substitution $\phi(x)=\text{floor}(x)$ would also fail, firstly because of non-injectivity, but also because of non-continuity. Do we need any more conditions on $\phi$ ? For context, the reason why I am asking this is because in my university coursed, our profesor said that if we want to calculate $\lim_{(x, y) \to (0, 0)}f(x, y)$ , we can just change to polar coordinates and calculate and it would be easier. I am currently studying engineering, so I would be looking for some theoretical, pure maths proof on the restrictions of the substitution $(x, y)=\phi(u, v)$ . To rephrase my question. Given two topological spaces $(T, \tau_T)$ , $(S, \tau_S)$ and $(U, \tau_U)$ , $D \subseteq T$ , a function $f\colon D \to S$ and $x_0$ an accumulation point of $D$ . Now let $V$ be a neighbourhood of $x_0$ and $\phi\colon U \to D \cap V$ . What would be the minimum conditions that $\phi$ needs to obey such that $\lim_{x \to x_0}f(x)=\lim_{y \to \phi(x_0)}f(\phi(y))$ ? Is the bijectivity and continuity of $\phi$ equivalent to the equality of limits in the general case? My intuition says yes, however from working a lot in abstract algebra these types of substitutions required $\phi$ to be an isomorphism, in our case shouldn't $\phi$ be a homeomorphism? I can't think of a counterexample where if $\phi$ is continuous and bijective would fail if $\phi^-1$ is not continuous? And one more thing, the polar coordinates substitution ""fails"", if we work on the direct product of the extended real number line, at points that have infinity on one component, since I don't think polar coordinates are defined just like how the angle isn't defined for the origin.","['real-analysis', 'multivariable-calculus', 'calculus', 'limits', 'general-topology']"
4848823,Why does $\tan10^\circ+\tan20^\circ+\tan50^\circ$ equal $\tan60^\circ$?,"My calculator says that $\tan10^\circ+\tan20^\circ+\tan50^\circ=\tan60^\circ$ , and this is confirmed to 15 decimal places by a more precise online calculator. So it looks plausible. However, I have not managed to prove the result, despite its apparent simplicity. A similar result was proved by MSE poster achille hui, using the formula $$n \cot n\theta = \sum_{k\,=0}^{n-1}\cot\!\left(\!\theta+\frac{k\pi}{n}\!\right)
\;\;(n=1,2,...).$$ In terms of cotangents, the desired result is $\cot40^\circ+\cot70^\circ+\cot80^\circ=\cot30^\circ\;(=3\cot60^\circ)$ . However, I can't see how to fit these angles to the formula with $n=3$ . Is there another formula of this sort that does the trick? A more general question arises: What other identities of the form $\tan\alpha+\tan\beta+\tan\gamma=\tan\delta\,$ hold, where the angles are all simple rational multiples of $\pi$ ?",['trigonometry']
4848850,Basic properties of Green function and resolvent,"I am trying to understand better the properties of the resolvent and Green function of a bounded self-adjoint operator $H$ at $z=E+i\eta$ when $\eta \to 0^+$ . The resolvent is the operator $R(H;z)=(H-zI)^{-1}$ , which is defined when $z$ is is not in the spectrum. If I am not mistaken, the Green function is defined as $G:\mathcal{H}\times \mathcal{H\times \mathbb{C}}$ by $G(\psi,\phi; z):= \langle  R(H,z)\phi,\psi \rangle$ , and is defined when $z$ is not in the spectrum. My question is what happens when $E$ is in the spectrum of $H$ to $\Vert R(H;E+i\eta)\Vert$ or $G(\psi, \phi;E+i\eta)$ when $\eta\to 0^+?$ It seems that when $E$ is an approximate eigenvalue, that $\Vert R(H;E+i\eta)\Vert\to \infty$ by Weyl's criterion for the spectrum. On the other hand, I don't see what should happen to the Green function when $\eta \to  0^+$ . Should it tend to $\infty$ ? And is there an importance to when the Green function vanishes? I am motivated by the case where $H= \Delta+D$ on $\ell^2(\mathbb{Z})$ , with $\Delta$ being the discrete Laplacian and $D$ being a diagonal operator with finitely many diagonal values.   I am trying to consider the Green function using the Random walk expansion for the Green function given in chapter 6 of Random operators by Aizenman and Warzel for such operators.","['greens-function', 'spectral-theory', 'functional-analysis']"
4848863,Find the lengths of the principal axes of the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ (Arnold 85),"Find the lengths of the principal axes of the ellipsoid $$\sum_{i \leq j} x_ix_j = 1.$$ -- Arnold, Trivium 85 My solution is below.  I request verification, feedback, or alternate approaches (especially ways to simplify). Solution: In $\mathbb R^n$ , the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ has a single axis of length $\sqrt {\frac 8 {n+1}}$ and all other axes length $2\sqrt 2$ . Proof: Recall that if $D$ is a diagonal matrix, then $$\mathbf x^\top D \mathbf x = 1$$ is an ellipsoid in standard position, with axis $i$ of length $\frac 2 {\sqrt {D_{ii}}}$ . Simple multiplication shows that the ellipsoid $\sum_{i \leq j} x_ix_j = 1$ has equation $$\mathbf x^\top S \mathbf x = 1$$ where $$S_{ij} = \begin{cases}1 &\text{ if } i = j\\ \frac 1 2 &\text{ otherwise}.\end{cases}$$ Since $S$ is symmetric, the spectral theorem shows that $S$ has $n$ orthogonal eigenvectors with real eigenvalues, and that $S$ decomposes into $S = DQ$ , with $Q$ orthogonal and $D$ diagonal.  The diagonal entries of $D$ are the eigenvalues of $S$ . Since $Q$ is orthogonal, it preserves lengths.  Consequently, if the eigenvalues of $S$ are $\lambda_i$ , then the ellipsoid's axes will have length $\frac 2 {\sqrt {\lambda_i}}$ . Inspection shows that the vector $\mathbf \ell$ with all components $1$ is an eigenvector of $S$ with eigenvalue $\frac {n+1} 2$ .  Inspection likewise shows that for $1 \leq i < n$ , the vectors $\mathbf m_i$ with components $$m_{i_j} = \begin{cases}
1 &\text{ if } j = i \\
1 - \sqrt n - n &\text{ if } j = n\\
1 + \sqrt n&\text{ otherwise}\end{cases}$$ are other eigenvectors, each with eigenvalue $\frac 1 2$ , which completes the proof. Remark: The components of $\mathbf m_i$ were determined by solving for $$a + (n-2)b + c = 0 \text{ since } \mathbf m_i \cdot \mathbf \ell = 0 \\
2ab + (n-3)b^2 + c^2 = 0 \text{ since } \mathbf m_i \cdot \mathbf m_j = 0 \text { when } i \neq j.$$ Is there a simpler way to determine them?  The fact that the rows of $S$ are rotations of each other suggests some type of symmetry of its eigenvalues, and we know their sum from $\operatorname{trace} S = n$ , but I could not develop this further.","['spectral-theory', 'linear-algebra', 'geometry', 'quadratic-forms']"
4848906,"Any ""tricks"" for computing matrix commutators?","I happen to need to find the commutator of various 2x2 and 3x3 matrices relatively often. It is particularily tedious, but even after much practice, I am not finding it getting significantly faster. Are you aware of any ""tricks"" or shortcuts I can take when computing matrix commutators (or recognising that two matrices commute by looking at them)? EDIT thanks to comment, most of the matrices I deal with are hermititan ( $H_{ij} = H_{ji}^*$ ) Certainly two diagonal matrices will always commute, but I am not aware of many other properties.","['matrices', 'linear-algebra', 'quantum-mechanics']"
4848916,Solve the second-order ordinary differential equation $4y''+\frac1{y^3}=0$,"$$4y''+\frac1{y^3}=0$$ I managed to turn this into a first order equation by multiplying by $y'$ , then integrating by $x$ : $4y''y'+y'\frac1{y^3}=0\\2y'+\frac{-1}{2y^2}=E$ but when I tried to simplify, I encountered an unfamiliar expression: $y'={2Ey^2+1\over4y^2}\\{dy\over dx}{4y^2\over Ey^2+1}=1\\\int{4y^2\over Ey^2+1}dy=\int1dx$ How can I integrate this expression by y? Is there anything I could've done differently to solve the equation?",['ordinary-differential-equations']
4848917,Graph of a continuous function is locally Euclidean,"I am working through Lee's ""Intro to Topological Manifolds,"" and in the third chapter the subspace topology is introduced. After showing that subspaces of Hausdorff and second-countable spaces are Hausdorff and second-countable, we realize that since $\mathbb{R}^n$ satisfies both of these conditions, a subspace of $\mathbb{R}^n$ will be a manifold if it is also locally Euclidean; hence, to know if a subspace of $\mathbb{R}^n$ is a manifold, we need only check that it is locally Euclidean. (A manifold is a Hausdorff, second-countable, locally Euclidean topological space.) A first example is that of a continuous map $f:U\subset \mathbb{R}^n \rightarrow \mathbb{R}^k$ , where $U$ open; the claim is that the graph $$ \Gamma\left(f\right) = \left\{ \left(x,f\left(x\right)\right) \, \middle| \, x \in U \right\} \subset \mathbb{R}^{n+k} $$ is an $n$ -manifold. As mentioned above, it's surely Hausdorff and second-countable. Naturally the homeomorphism between $\Gamma\left(f\right)$ and $U$ that we seek is going to be $$ \Phi \left(x \right) = \left(x,f\left(x\right)\right) $$ and it remains to show that this is, indeed, a homeomorphism. It is certainly a bijection. We can see that $\Phi ^{-1}\left(x,f\left(x\right)\right) = x $ so that $$ \Phi^{-1} = \pi_1|_{\Gamma\left(f\right)}$$ which is the restriction of a continuous map, and is thus continuous. I am struggling to show that $\Phi$ itself is continuous. Note that the author has not yet introduced product topologies , and I am meant to work only with subspaces. The claim is that ""It is continuous because $f$ is,"" and I am trying to prove this. My proof seems much messier than that concise statement suggests. Am I missing something simple here? My proof Let $V\subset \Gamma\left(f\right)$ be open, so $V = W\cap \Gamma\left(f\right)$ with $W\subset \mathbb{R}^{n+k}$ by definition of the subspace topology on $\Gamma\left(f\right)$ . We want to show that $\Phi^{-1}\left(V\right)$ is open in $U$ . Let $x \in \Phi^{-1}\left(V\right)$ . Then $\Phi\left(x\right) \in V = W \cap \Gamma\left(f\right)$ , so $\Phi\left(x\right) \in W$ and there exists some ball neighborhood of $\Phi(x) = \left(x,f\left(x\right)\right)$ contained in $W$ , namely $B_1 = B_{\epsilon}\left(\left(x, f\left(x\right)\right)\right) \subset W$ . (I will now show that there is a neighborhood of $x$ contained in $\Phi^{-1}\left(V\right)$ that maps within $B_1$ , proving the openness of the former and thus continuity of $\Phi$ . This will utilize continuity of $f$ and properties of the Euclidean metric.) Since $f:\mathbb{R}^n \rightarrow \mathbb{R}^k$ is continuous, there is a $\delta' >0$ such that $$d\left(x', x\right) < \delta' \implies d\left(f\left(x'\right), f\left(x\right)\right) < \epsilon / \sqrt{2}, $$ where the $d$ indicate the metrics on $\mathbb{R}^n$ and $\mathbb{R}^k$ respectively. Then let $\delta = \min \left\{\delta', \epsilon/\sqrt{2}\right\}$ . Then if $   d\left(x', x\right) < \delta$ , we have $$ d\left(\left(x', f\left(x'\right)\right), \left(x, f\left(x\right)\right) \right)^2 = d\left(x', x\right)^2 + d\left(f\left(x'\right), f\left(x\right)\right)^2 < \left(\frac{\epsilon}{\sqrt{2}}\right)^2 = \epsilon^2 $$ which says that $$ d\left(\Phi\left(x'\right), \Phi\left(x\right)\right) < \epsilon $$ and so we have shown that $$x' \in B_{\delta}\left(x\right) \implies \Phi\left(x'\right) \in B_1. $$ Since $\Phi\left(x'\right) \in \Gamma\left(f\right)$ obviously, we have further that $$x' \in B_{\delta}\left(x\right) \implies \Phi\left(x'\right) \in B_1 \cap \Gamma\left(f\right) \subset W\cap \Gamma\left(f\right) = V. $$ Hence, $B_{\delta}\left(x\right)$ is an open neighborhood of $x$ in $\Phi^{-1}\left(V\right)$ that maps inside of $V$ , so $\Phi^{-1}\left(V\right)$ is open and $\Phi$ is continuous. $\blacksquare$","['manifolds', 'general-topology']"
4848930,Solve nonlinear system of equations and show it has infinite solutions,"So we have this system of nonlinear equations \begin{align*}
\sin(x+u) - e^y + 1 = 0\\
x^2 + y + e^u = 1
\end{align*} and we want to show that it has infinitely many solutions $(x,y,u)$ . I tried starting by finding the Jacobian matrix (so I can use the theorem of implicit functions) $$
J_{(x,y,u)} = \left[\begin{array}{ccc}
   \cos(x+u) & -e^y & \cos(x+u) \\
   2x & 1 & e^u
    \end{array}\right]
$$ but obviously this is not a square matrix so the determinant is not defined. I was thinking we could add another equation, I was told "" $H(x,y,u) = u$ (or $x$ or $y$ )"" could work but how is that ""allowed""?","['inverse-function-theorem', 'jacobian', 'multivariable-calculus', 'calculus', 'implicit-function-theorem']"
4848932,Tight lower bound on the number of triangles in a graph,"Let $G$ be a graph with $n$ vertices and $m$ edges. Show that the minimum number of triangles is at least \begin{equation*}
    \frac{4m}{3n}\left(m-\frac{n^2}{4}\right).
\end{equation*} Prove that the bound is tight when $m=n^2/3$ . I managed to prove the first part. I am having trouble showing why a graph with $n^2/3$ edges has $n^3/27$ triangles (using the bound). I thought about constructing complete bipartite graphs and then adding the remaining edges until I hit $n^2/3$ . However, I am getting more triangles than I'm supposed to. My proof of the first part. Given an edge $(u,v)\in E(G)$ , there are at least $d_u + d_v - 2 - (n-2) = d_u + d_v - n$ triangles including the edge $(u,v)$ . Hence, the minimum number of triangles is given by \begin{equation*}
    \sum_{(u,v)\in E}\frac{d_u+d_v-n}{3} = \frac{1}{3}\left(\sum_{u\in V}d_u^2 - mn\right).
\end{equation*} Applyin Cauchy-Schwarz to the vectors $(d_1,\dots, d_n)$ and $(1,\dots,1)$ we obtain \begin{equation*}
    n\sum_{u\in V}d_u\geq \left(\sum_{u\in V}d_u\right)^2 = 4m^2 .
\end{equation*} Therefore, the number of triangles in $G$ is greater than or equal to \begin{equation*}
    \frac{1}{3}\left(\frac{4m^2}{n}-mn\right) = \frac{4m}{3n}\left(m-\frac{n^2}{4}\right).
\end{equation*}","['graph-theory', 'combinatorics']"
4848935,Weak convergence and fidi convergence on $\mathbb R^{\mathbb N}$,"I have a sequence $(\mu_n)_{n\in \mathbb N}$ of probability measures on the measurable space induced by the Borel $\sigma$ -field over $\mathbb R^{\mathbb N}$ equipped with its canonical topology (product topology). I know that, in general, $(\mu_n)_{n\in \mathbb N}$ converges to a tight measure $\mu$ iff $(\mu_n)_{n\in \mathbb N}$ is tight and the finite-dimensional marginal distributions converge to those of $\mu$ . My question is: Do I really need tightness of $(\mu_n)_{n\in \mathbb N}$ when working on $\mathbb R^{\mathbb N}$ ? I saw a proof (Example 2.6 in Convergence of Probability Measures, 2nd Edition by Billingsley) that shows tightness of the sequence is not required. But I lack the intuition why it should be possible to say something about the joint distribution of an infinite dimensional object from only finitely many dimensions without any compactness assumption like tightness.","['stochastic-processes', 'probability-theory']"
4848940,Variance recursion formula in Galton-Watson process,"Consider a Galton-Watson process with expected offspring $\mathbb{E}[\xi]=\mu<\infty$ and variance $\text{Var}(\xi)=\sigma^2<\infty$ where the offspring in generation $t\in\mathbb{N}$ is given by $Z_t$ . Suppose I introduce a new random variable $W_t$ given by $W_t=\mu^{-t}Z_t$ . Noting this setup, in the proof of Proposition $1.4$ on page $7$ of this source they say: ""a straightforward calculation yields $$\text{Var}(W_t)=\frac{\sigma^2}{\mu^{t+1}}+\text{Var}(W_{t-1})""$$ but I am unable to see what straightforward calculation this follows from. Any help would be appreciated.","['statistics', 'variance', 'stochastic-processes', 'probability', 'random-variables']"
4849015,Is a locally compact weak Hausdorff space Hausdorff?,"Definitions. Let $X$ be a topological space.  We say $X$ is locally compact if every member of $X$ has neighborhood basis of compact sets. weak Hausdorff if for every continuous map $f\colon Z\to X$ , where $Z$ is compact Hausdorff, $f(Z)$ is closed in $X$ . Question. ( $Q$ ) Must a locally compact weak Hausdorff space be Hausdorff? Additional background. This question has an equivalent formulation in terms of compactly generated spaces, or "" $k$ -spaces"".  We need a few more definitions. A subset $A\subseteq X$ is $k_2$ -closed (resp. $k_2$ -open ), if $f^{-1}(A)$ is closed (resp. open) for every continuous map $f\colon Z\to X$ from a compact Hausdorff space $Z$ . (Often the above is simply called "" $k$ -closed"", but there are several competing definitions for that term, one of which we are about to introduce here, so we borrow the terminology from this answer for precision.) A subset $A\subseteq X$ is $k_3$ -closed (resp. $k_3$ -open ) if its intersection $A\cap K$ with every compact Hausdorff subset $K\subseteq X$ is closed (resp. open) in $K$ . For $n=2$ or $3$ , $X$ is $CG_n$ (""compactly generated"") if subsets in $X$ are closed if and only if they are $k_n$ -closed. $k_n$ -Hausdorff if the diagonal in $X\times X$ is $k_n$ -closed. The last definition really isn't especially interesting for $n=3$ in light of the answers here. Also there is a definition for $k_1$ as well; it won't be relevant for us, but see the preceding links. Equivalent and related questions. As discussed in the first link above, under either $CG_n$ assumption, the weak Hausdorff condition is equivalent to the otherwise stronger condition $KC$ (Compact subsets are closed).  However, the $KC$ condition in turn immediately upgrades to $T_2$ in the presence of local compactness, as we obtain a basis of closed neighborhoods at each point, which combines with $T_1$ to yield $T_2$ .  (We get $T_1$ from the weak Hausdorff condition, as every singleton is the image of a compact Hausdorff space, hence closed.) Conversely, any locally compact Hausdorff space is easily seen to be $CG_n$ for either $n$ . As a result, the following question, for either $n=2$ or $3$ , is equivalent to ( $Q$ ). ( $Q_n$ ) Must a locally compact weak Hausdorff space be $CG_n$ ? Finally, we note that in general, weak Hausdorff implies $k_2$ -Hausdorff, and under either $CG_n$ assumption, the two are equivalent. Therefore, we could also ask ( $Q'$ ) Must a locally compact $k_2$ -Hausdorff space be Hausdorff? ( $Q_n'$ ) Must a locally compact $k_2$ -Hausdorff space be $CG_n$ ? An affirmative answer to either of these equivalent (to each other) questions implies an affirmative answer to the other questions. In general, a big part of what motivated this question was my desire to find more examples of spaces that are $k_2$ -Hausdorff or even weak-Hausdorff, yet fail to be $KC$ .  As explained above, in the locally compact case any non-Hausdorff example will also fail $KC$ , leading me to this line of inquiry. Addendum. Regarding ( $Q_n$ ), I had difficulty finding any example at all of a locally compact space that failed to be a $CG_2$ -space. $\pi$ -base turned up nothing , though this answer did give a nice non- $T_1$ example.
(The original paper referenced there explains how to modify the example to be $T_1$ , but after such modification it will still fail to have any reasonable separation, as it will not even have unique limits for convergent sequences ( $US$ ), so will not answer our questions here.) For locally compact spaces that fail to be $CG_3$ , $\pi$ -base only has non- $T_1$ examples.","['separation-axioms', 'general-topology', 'examples-counterexamples', 'compactness']"
