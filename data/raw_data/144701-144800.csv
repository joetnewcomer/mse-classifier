question_id,title,body,tags
2359789,Complete Matching and Maximum Matching,"This is the problem I have been struggling with for a while (from Discrete Mathematics and its Applications (Rosen) seventh edition): Suppose that a new company has five employees: Zamora, Agraharam, Smith, Chou, and Macintyre. Each employee will assume one of six responsibilities: planning, publicity, sales, marketing, development, and industry relations. Each employee is capable of doing one or more of these jobs: Zamora could do planning, sales, marketing, or industry relations; Agraharam could do planning or development; Smith could do publicity, sales, or industry relations; Chou could do planning, sales, or industry relations; and Macintyre could do planning, publicity, sales, or industry relations. a) Model the capabilities of these employees using a bipartite graph. b) Find an assignment such that each employee is assigned one responsibility. c) Is the matching you found in part (b) a complete matching? Is it a maximum matching? I have been struggling with part C for a while.  Would the matching I found in part B be considered a complete matching and/or a maximum matching? Here's what I have so far (ignore part C): I have found many different definitions but none that adequately answer my question.  Thanks for you help!","['graph-theory', 'bipartite-graphs', 'matching-theory', 'discrete-mathematics']"
2359827,Universal Covering of $S^1\vee S^2$,"I'm trying to get my head around covering maps and would just like some help with this (probably) rather simple problem. My attempt was to make $\mathbb{R}^2$ a universal covering of $S^1\vee S^2$. I want to find a universal cover of f $S^1\vee S^2$. My idea was to proceed as follows: Consider $S^1$ as the unit interval quotiented by the equivalence relation $0\sim 1$. Consider the unit sphere $S^2$ as the unit square quotiented by following relation $(0,y)\sim (1,y)$ & $(x,0)\sim (y,0) \forall x,y$ & $(x,1)\sim (y,1) \forall x,y$. I.e as the unit cylinder with top and bottom sides considered as single points. Now my idea was to join these two objects together and somehow embed them in $\mathbb{R}^2$ to get a universal covering. I'm not sure how to proceed. I would like to know, (a) is there a way of doing this, (b) is there a better way of doing it, and (c) how we use this to find the fundamental group of $S^1\vee S^2$?","['algebraic-topology', 'general-topology', 'covering-spaces']"
2359844,There are finitely many critical points for a non-degenerate action.,"Let $(M,\omega)$ be a compact symplectic manifold, and $J$ be a compatible almost complex structure. Given a $1$-periodic Hamiltonian $H: M \times \mathbb{R} \to \mathbb{R}$, we define the action functional $A_H: \Omega_0(M) \to \mathbb{R}$ (here $\Omega_0(M)$ is the space of contractible loops) by
$$A_H(x)=\int_D u^*\omega+\int_0^1H(x(t),t)dt,$$
where $u$ is a capping disk for $x$ (we put an asphericity condition so that this is well-defined). One can prove that the critical points correspond to solutions of $\dot{x}(t)=X_t(x(t))$ (where $X_t$ is the Hamiltonian vector field), and that the flow of $-\nabla A_h$ is given by solutions of the Floer equation (disregarding the fact that the flow may not be globally defined etc). A non-degenerate critical point is a path $x$ which is a critical point and such that the linearized flow has no eigenvalue $1$.
The book by Audin, Damian claims that if all critical points are non-degenerate, then there are finitely many, but I don't understand why that is true. The question then is: Why nondegeneracy in this case implies finitely many critical points?","['symplectic-geometry', 'differential-geometry', 'differential-topology']"
2359845,Multivariable Taylor series convergence,"Let us have a Taylor series expansion for multivariable function $$
f(x_1,x_2,\ldots,x_N)=\sum_{n=0}^{\infty}{\sum_{j_1+j_2+\ldots +j_N=n}{\dfrac{1}{j_1!j_2!\ldots j_N!}\dfrac{\partial^{n}f}{\partial x_1^{j_1}\partial x_2^{j_2}\ldots \partial x_N^{j_N}}\Bigg|_{x=0}x_1^{j_1}x_2^{j_2}\ldots x_N^{j_N}}},
$$ where in the second sum summation goes over all possible combinations of non-negative integer solutions of the equation $j_1+j_2+\ldots + j_N=n$ . In particular I’m interested in the expansion of the function $$
f(x_1,x_2,\ldots,x_N)=\exp\left\{\dfrac{1}{2}D_{st}{x_s}{x_t}\right\},
$$ where $D_{st}x_sx_t$ means sum $\sum_{s=1}^{N}{\sum_{t=1}^{N}{D_{st}x_sx_t}}$ , because that is a moment-generating function for multivariable normal distribution with a covariance matrix $D$ . I need to prove that in the case of such a function it’s Taylor expansion converges to this function for every possible vector $(x_1,x_2,\ldots,x_N)\in \mathbb{R}^{N}$ . I know that for a function of one variable its Taylor series $$
\sum_{i=0}^{n}{\dfrac{f^{(i)}(0)}{i!}x^{i}}+R_{n}(x)
$$ converges to it if and only if its remainder term $R_n(x)$ has a limit $$
\lim_{n\rightarrow \infty}{R_n(x)}=0,
$$ and for any given interval $x\in (-R,R)$ this is true if, for example, $$
\left|f^{(n)}(\xi)\right|\leq M
$$ for any $n$ and $\xi\in(-R,R)$ . I’m interested if there some way to prove that for multivariable function in question its series converges to it on all $\mathbb{R}^N$ so that its series unambiguously defines it. This question has stemmed out of that The theorem inverse to Isserlis' theorem .","['multivariable-calculus', 'taylor-expansion', 'power-series']"
2359861,Global sections on quasi coherent sheaves on affine scheme,"This is a lemma from Hartshorne's Algebraic Geometry. Let $X=\text{Spec}(A)$ be an affine scheme $f\in A, D(f)\subseteq X$ .
Let $\mathcal{F}$ be a quasi coherent sheaf on $X$ . If $s\in \Gamma(X,\mathcal{F})$ is such that $s|_{D(f)}=0$ then for some $n>0$ , $f^ns=0$ . If $t\in \Gamma(D(f),\mathcal{F})$ then for some $n>0$ $f^nt$ extends to a global section of $\mathcal{F}$ over $X$ . I tried to see what this means in case of simple quasicoherent sheaves. For $X=\text{Spec}(A)$ , structure sheaf $\mathcal{O}_X$ is a quasicoherent sheaf of $\mathcal{O}_X$ modules.  In this case, $\mathcal{F}(X)\rightarrow \mathcal{F}(D(f))$ is $\mathcal{O}_X(X)\rightarrow \mathcal{O}_X(D(f))$ i.e., $A\rightarrow A_f$ . So, $s|_{D(f)}=0$ means that $\frac{s}{1}=0\in A_f$ i.e., $f^ns=0$ for some $n\in \mathbb{N}$ . For $X=\text{Spec}(A)$ and for an $A$ module $M$ , $\widetilde{M}$ is a quasicoherent sheaf of $\mathcal{O}_X$ modules. In this case, $\mathcal{F}(X)\rightarrow \mathcal{F}(D(f))$ is $\widetilde{M}(X)\rightarrow \widetilde{M}(D(f))$ i.e., $M\rightarrow M_f$ . So, $s|_{D(f)}=0$ means that $\frac{s}{1}=0\in M_f$ i.e., $f^ns=0$ for some $n\in \mathbb{N}$ . For $X=\text{Spec}(A)$ , structure sheaf $\mathcal{O}_X$ is a quasicoherent sheaf of $\mathcal{O}_X$ modules.  In this case, $\mathcal{F}(X)\rightarrow \mathcal{F}(D(f))$ is $\mathcal{O}_X(X)\rightarrow \mathcal{O}_X(D(f))$ i.e., $A\rightarrow A_f$ . Let $t\in \Gamma(D(f),\mathcal{F})=A_f$ so, $t=a/f^n$ for some $n\in \mathbb{N}$ i.e., $f^nt=a\in A_f$ . So, $a\in \Gamma(X,\mathcal{F})$ is such that its restiction to $\Gamma(D(f),\mathcal{F})$ is $a/1=f^nt$ . For $X=\text{Spec}(A)$ and for an $A$ module $M$ , $\widetilde{M}$ is a quasicoherent sheaf of $\mathcal{O}_X$ modules. In this case, $\mathcal{F}(X)\rightarrow \mathcal{F}(D(f))$ is $\widetilde{M}(X)\rightarrow \widetilde{M}(D(f))$ i.e., $M\rightarrow M_f$ . Let $t\in \Gamma(D(f),\mathcal{F})=M_f$ so, $t=a/f^n$ for some $n\in \mathbb{N}$ i.e., $f^nt=a\in M_f$ . So, $a\in \Gamma(X,\mathcal{F})$ is such that its restiction to $\Gamma(D(f),\mathcal{F})$ is $a/1=f^nt$ . Now comes the general case $\mathcal{F}$ a quasi coherent sheaf of $\mathcal{O}_X$ modules. We can find an open cover for $X=\text{Spec}(A)$ of the form $\{D(g_i)\}_{i=1}^r$ and $A_{g_i}$ modules $M_i$ such that $\mathcal{F}|_{D(g_i)}=\widetilde{M_i}$ . To prove that $f^ns=0$ , the strategy is to prove that $f^ns|_{D(g_i)}=0$ for
all $1\leq i\leq r$ . Then by gluing we see that $f^ns=0$ . Let $s_i=s|_{D(g_i)}$ . We have $\mathcal{F}(D(g_i))\rightarrow \mathcal{F}(D(f)\cap D(g_i))=\mathcal{F}(D(fg_i))$ . As $\mathcal{F}|_{D(g_i)}=\widetilde{M_i}$ we have $\widetilde{M_i}(D(g_i))\rightarrow \widetilde{M_i}(D(fg_i))$ i.e., $M_i\rightarrow (M_i)_{fg_i}$ and $s_i\mapsto \frac{s_i}{1}=0\in (M_i)_{fg_i}$ i.e., $g_i^{n_i}f^{n_i}s_i=0$ . Let $n=\max\{n_i:1\leq i\leq r\}$ . We then have $g_i^nf^ns_i=0$ . I am stuck here. Any hints are welcome. In other words what we want to prove is the following : Given $f\in A$ , the restirction map $\Gamma(X,\mathcal{F})\rightarrow \Gamma(D(f),\mathcal{F})$ is close to being injective upto localization. We have $s\in \Gamma(X,\mathcal{F})$ such that $s|_{D(f)}=0$ then $f^ns=0$ for some $n$ . If we consider image of $f^ns$ in $\Gamma(X,\mathcal{F})_f$ it is just $s$ . So, $\Gamma(X,\mathcal{F})_f\rightarrow \Gamma(D(f),\mathcal{F})$ is injective. close to being surjective upto localization. We have $t\in \Gamma(D(f),\mathcal{F})$ then there exists an element $s\in \Gamma(X,\mathcal{F})$ whose restriction is $f^nt$ for some $n$ . It is not as straightforward as the  injective case  but i am sure this means $\Gamma(X,\mathcal{F})_f\rightarrow \Gamma(D(f),\mathcal{F})$ is surjective. So, we are trying to prove that $\Gamma(X,\mathcal{F})_f\cong\Gamma(D(f),\mathcal{F})$ . This is just my interpretation and it could mean something different than that of the original question. Is my interpretation close to the question given? This seems to be reasonable to expect. We have similar result called qcqs lemma which says If $X$ is a quasi compact quasi separated scheme and $s\in \Gamma(X,\mathcal{O}_X)$ then the natural map $\Gamma(X,\mathcal{O}_X)_s\rightarrow \Gamma(X_s,\mathcal{O}_X)$ is an isomorphism.","['quasicoherent-sheaves', 'affine-schemes', 'algebraic-geometry']"
2359871,Limit Definition for Half-Derivative,"The derivative of a function $f$ is defined as
$$\lim_{h\to 0} \frac{f(x+h)-f(x)}{h}$$
Let
$$d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$
and, in fact, let all $d_n$ be defined by
$$\lim_{h\to 0} d_n(f,h)=f^{(n)}(x)$$
In order to obtain $d_2$, we can plug $d_1$ into itself to get
$$\frac{\frac{f(x+2h)-f(x+h)}{h}-\frac{f(x+h)-f(x)}{h}}{h}$$
$$\frac{f(x+2h)-f(x+h)-f(x+h)-f(x)}{h^2}$$
$$\frac{f(x+2h)-2f(x+h)-f(x)}{h^2}$$
and, in general, we can use induction to prove that, for natural $n$,
$$d_n(f,h)=\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$
However, I am interested in finding $d_\frac{1}{2}$. It should satisfy
$$d_\frac{1}{2}(d_\frac{1}{2}(f,h),h)=d_1(f,h)=\frac{f(x+h)-f(x)}{h}$$
So that when it is composed with itself as shown, $d_1$ is the result. It can possibly be obtained by figuring out how to extend the expression
$$\frac{1}{h^n}\sum_{k=0}^n (-1)^k \binom{n}{k}f(x+(n-k)h)$$
to non-integer $n$. Does anybody have any ideas about how to do this?","['derivatives', 'fractional-calculus', 'limits']"
2359879,Characteristic polynomial and minimal polynomial of a matrix,"let $A$ be a $n \times n$ matrix : $$A=\begin{bmatrix}1 & 2 &... & n\\n+1 & n+2 & ... & 2n\\&...\\&&...\\n^2-n+1 & n^2-n+2 & .... & n^2\end{bmatrix}$$ How can I find the characteristic polynomial  and minimal polynomial of A? I know that the row space and column space of A is $$\bigl\langle(1,1,...,1), (1,2,...,n)\bigr\rangle$$ but could not proceed.","['matrices', 'linear-algebra', 'determinant']"
2359924,"A question on a subspace of $C[0,1]$, that is closed in $L^{2}$","Let $S$ be a subspace of $C[0,1]$, that is closed  as a subspace of $L^{2}[0,1]$. a. Show that $S$ is closed  in $(C[0,1], ||.||_{\infty})$. b. Show that there  is a constant $M$ such that for all $f\in S$, we have $||f||_{\infty} < M||f||_{2}$. c. Show that for each $y\in [0,1]$, there is a function $k_{y}$ in $L^{2}$, such that for all $f\in S$ we have $f(y)=\int k_{y}(x)f(x) dx$. My attempt: I have solved the first two questions. The first question follows from the fact that $||f||_{2}\leq ||f||_{\infty}$. Then part (b) is just the bounded inverse theorem. I am stuck at (c). I thought to define a map say $\phi: L^{2} \to \mathbb{R}$, given by $\phi(f)=f(y)$ , for a fixed $y\in [0,1]$. If $\phi$ is continuous then by Riesz's representation theorem there exists a $k_{y} \in L^{2}$ such that $$\phi(f)= \int k_{y}(x)f(x)dx = f(y)$$, and we are done. The second inequality can help in showing the continuity but only for $f\in S$. Is this map at all bounded? How can this be solved. Thanks in advance!!","['functional-analysis', 'real-analysis', 'lebesgue-measure', 'measure-theory']"
2359959,"If $f:\mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ with $a \in \mathbb{R}^n$, and rank$(Df_a) = m$, prove or disprove the following:","If $f:\mathbb{R}^n \to \mathbb{R}^m$ is $C^1$ with $a \in \mathbb{R}^n$, and  rank$(Df_a) = m$, then there exists $\epsilon > 0$, such that $B_\epsilon (f(a)) \subseteq f(\mathbb{R}^n)$. My first instinct is to use Taylor's Theorem and try $f(a+h)-f(a) = Df_a (h) + r(h)$ and see if I can turn $f(a+h) - f(a)$ into some ball of radius epsilon using the fact that $Df_a$ is onto but I really have no clue how to proceed from here.","['multivariable-calculus', 'real-analysis']"
2359979,"If a commutator has an eigenvalue $0$, do the two operators share an eigenvector?","Let $A,B$ be two diagonalizable linear operators, such that their commutator $\left[A,B\right] = AB-BA$ has an eigenvalue $\lambda = 0$. Does this mean $A$ and $B$ share an eigenvector? Of course, the vice-versa is correct. If they do share an eigenvector, the commutator necessarily has an eigenvalue of $0$. I'm not sure though if this is a sufficient condition, or only a necessary one.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2359992,How to resolve the sign issue in a SVD problem?,"Question: When performing a simple Singular Value Decomposition , how can I know that my sign choice for the eigenvectors of the left- and right-singular matrices will result in the correct matrix without just guessing and checking? If it makes things easier, feel free to restrict your answers to just real-valued or real-valued, square matrices. Context Consider the matrix $$A=\begin{pmatrix}2&-4\\4&4\end{pmatrix}$$ which has the left-singular matrix $$AA^T=\begin{pmatrix}20&-8\\-8&32\end{pmatrix}$$ and the right-singular matrix $$A^TA=\begin{pmatrix}20&8\\8&32\end{pmatrix}$$
The eigenvalues for both matrices are $36$ and $16$ (meaning the singular values of $A$ are $6$ and $4$, respectively). The normalized left-singular eigenvectors are $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\-2\end{pmatrix}\ \ \ \textbf{u}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\1\end{pmatrix}$$ and the normalized right-singular eigenvectors are $$\textbf{v}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}1\\2\end{pmatrix}\ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}-2\\1\end{pmatrix}$$ With these in hand, we can construct the SVD which should look like this: $$A=U\Sigma V^T=\frac{1}{5}\begin{pmatrix}1&2\\-2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\-2&1\end{pmatrix}$$ However, if you actually perform the matrix multiplication, the result is $$U\Sigma V^T=\begin{pmatrix}-2&4\\-4&-4\end{pmatrix}= -A \neq A$$ Since the normalized eigenvectors are unique only up to a sign, one resolution to this problem is to choose $$\textbf{u}_{36}=\frac{1}{\sqrt{5}}\begin{pmatrix}-1\\2\end{pmatrix} \ \ \ \ \textbf{v}_{16}=\frac{1}{\sqrt{5}}\begin{pmatrix}2\\-1\end{pmatrix}$$ which produces the correct SVD $$U\Sigma V^T=\frac{1}{5}\begin{pmatrix}-1&2\\2&1\end{pmatrix}\begin{pmatrix}6&0\\0&4\end{pmatrix}\begin{pmatrix}1&2\\2&-1\end{pmatrix}=\begin{pmatrix}2&-4\\4&4\end{pmatrix}=A$$ This begs the question: How was I supposed to know that I had chosen the wrong sign convention for my eigenvectors without checking it by hand? I have a suspicion that the correct sign convention corresponds to the sum of the components of the eigenvectors being positive (and if they sum to zero then the topmost component should be made positive), but this seems like a pretty arbitrary condition despite it holding for several examples that I have checked.","['eigenvalues-eigenvectors', 'svd', 'linear-algebra']"
2360002,Difference between the volume/covolume of a lattice,"I am trying to learn some basic knowledge on lattices for studying Minkowski's theorems and Dirichlet's unit theorem. My problem is, I could not build the basics of  lattice theory and there are some blurred parts. I will give some definitons and then ask my questions . Let $L \subset \mathbb{R}^n$ be a lattice. It is a set $\{ \sum \alpha_i x_i | \alpha_i \in \mathbb{Z}\}$ where $\{x_1,\dots,x_n\}$ is a linearly independent set in $\mathbb{R}^n$. So we can either say that $L$ is the set above or we can define a matrix $$B=[ (x_1)_{n\times 1}   \dots (x_n)_{n \times 1}]_{n \times n}$$- generator matrix of $L$- 
and say $L = \{Bv : v \in \mathbb{Z}^{n \times 1} \}$. I saw that $vol(L)$ is defined as Lebesgue measure $\mu(P)$ where $P=\{ \sum a_i x_i | 0\le a_i <1 \}$, the parallelpipped of $L$ with respect to base $\{x_1,\dots,x_n\}$.
Now, i)Is $vol(L) = \det(B)$, or, $\det(B)=vol(\mathbb{R}^n / L)$?  If the latter is correct, what is $\mathbb{R}^n / L$, exactly? ii)What is the difference between covolume$(L)$ and volume$(L)$?","['algebraic-geometry', 'algebraic-number-theory', 'lattices-in-lie-groups', 'measure-theory', 'integer-lattices']"
2360044,Blow-up and polygon,"If $P \subset \Bbb R^2$ is a lattice polygon (i.e vertices with integer coordinates), we know that blowing-up the surface $X_P$ at a $T$-invariant point is the same as cutting a corner of $P$ and taking the corresponding toric variety. I understand this using the fan construction. This is equivalent to take the closure of the embedding given by the monomial inside $P$ cutted at one corner but I have no intuition why this works. For example, the convex hull of $(0,0), (6,0)$ and $(0,6)$ gives the usual Veronese embedding so we get $X_P \cong \Bbb CP^2$. Now if I delete the monomials $x^ky^j$ for $k \geq 4$, the corresponding embedding should gives the blow-up of the projective plane $\Bbb CP^2$. Why ?","['polygons', 'algebraic-geometry', 'blowup']"
2360046,Positive part of functions from Sobolev space involving time,"Assume that $u \in W^{1,2}(0,T; W_0^{1,2}(\Omega))$, where $\Omega$ is a bounded domain in $\mathbb{R}^n$ and $T <+\infty$. Let $A \subset \Omega \times (0, T)$ be a Lipschitz domain such that $u(\cdot,t_0) \leq 0$ on $\partial (A \cap \{t=t_0\})$ for a.a. $t_0 \in (0, T)$ in the sense of traces of functions from $W^{1,2}(\Omega)$. Is it true that $\max(u, 0)|_{A} \in W^{1,2}(0,T;W_0^{1,2}(\Omega))$? (Here under $\max(u, 0)|_{A}$ I mean a function $w$ such that $w = \max(u,0)$ on $A$ and $w=0$ on $(\Omega \times (0, T)) \setminus A$). P.S. Analogous property is, of course, valid for functions from $W_0^{1,2}(\Omega)$. [Added] The question is also actual for the space $W^{1,2}(0,T;L^2(\Omega)) \cap L^2(0,T;W_0^{1,2}(\Omega))$.","['functional-analysis', 'bochner-spaces', 'sobolev-spaces', 'partial-differential-equations']"
2360058,Formal definition of direct sum of operators.,"What is the formal definition of direct sum of operators and functions? Is it always some thing like the following? $A: U \to U, B: V \to V$ be continuous and let $T:U\oplus V\to U\oplus V$ $$T(x) = (A \oplus B)(u \oplus v)= A(u) \oplus B(v)$$. $X$ is a topological vector space and $U,V$ are subspaces. Can we use direct sum to describe the following relations? 1) $A: U \to U, B: V \to U$ be continuous and define: $$A\oplus B =T:U\oplus V\to U, u\oplus v\mapsto T(u\oplus v)$$ For example, $T(u\oplus v)=A(u) + B(v)$ 2) $A: U \to U, B: U \to V$ be continuous and define $A\oplus B =T:U\to U\oplus V, u\mapsto T(u\oplus B(u))$ Edited Source: [1] http://cc.bingj.com/cache.aspx?q=direct+sum+of+hilbert+spaces+operators&d=4885758912236128&mkt=en-US&setlang=en-US&w=kmBEdXxmF9OtyUjV8AanzoUQsyc0N2mZ [2] Bound of direct sum of operators [3] Considering operators on the direct sum of Hilbert spaces as operator valued matrices","['notation', 'reference-request', 'direct-sum', 'operator-theory', 'functional-analysis']"
2360081,Geometric Intuition of Definition of Intersection Multiplicity (of two algebraic curves),"Hartshorne defines the intersection multiplicity of a point in the intersection of two plane algebraic curves $f(x, y) = 0$, $g(x, y) = 0$ in $\mathbb{A}^{2}$ to be the length of the $\mathcal{O}_{P}$-module $\mathcal{O}_{P}/(f, g)$. I have seen in other definitions that $\dim_{k}\mathcal{O}_{P}/(f, g)$ is used instead of the length, which I guess is the same in most cases. Question : Does the above definition coincide with our geometric intuition of intersection multiplicity? I have seen very few related questions to this, all of which have unsatisfactory answers in regards to my question. I have done some examples, such as $f(x, y) = y - x^{2}, g(x, y) = y - 2x - 1$ (the tangent line at $(1, 1)$). Then our desired ring is $\mathcal{O}_{(1, 1)}/(f, g) \cong k[x,y]_{(x-1, y-1)}/(y - x^{2}, y - 2x + 1) = k[x]_{(x-1)}/(x-1)^{2}$, which has $1, x$ as basis over $k$ and so has dimension $2$. So it works! But I have still no geometric insight from doing such an example.","['intersection-theory', 'algebraic-geometry']"
2360102,"In the derivation of the integral representation of the beta function, why is the upper limit 1 after changing variables?","In the derivation of the beta function the following proof is given: $\begin{align}
 \Gamma(x)\Gamma(y) &= \int_{u=0}^\infty\ e^{-u} u^{x-1}\,du \cdot\int_{v=0}^\infty\ e^{-v} v^{y-1}\,dv \\[6pt]
 &=\int_{v=0}^\infty\int_{u=0}^\infty\ e^{-u-v} u^{x-1}v^{y-1}\,du \,dv.
\end{align}$ Changing variables by u=zt and v=z(1-t) shows that this is $\begin{align}
\Gamma(x)\Gamma(y) &= \int_{z=0}^\infty\int_{t=0}^\color{red}{1} e^{-z} (zt)^{x-1}(z(1-t))^{y-1}\big|J(z,t)\big|\,dt \,dz \tag{1}\\[6pt]
 &= \int_{z=0}^\infty\int_{t=0}^\color{red}{1} e^{-z} (zt)^{x-1}(z(1-t))^{y-1}z\,dt \,dz \\[6pt]
 &= \int_{z=0}^\infty e^{-z}z^{x+y-1} \,dz\cdot\int_{t=0}^\color{red}{1}t^{x-1}(1-t)^{y-1}\,dt\\
 &=\Gamma(x+y)\,\beta(x,y),
\end{align}$ How was the upper limit of integration calculated after the change of variables.   I thought I understood the Fubini-Tonnelli theorem but I don't understand how the value of 1 was calculated. Why is there no $\frac{1}{z}$ dependence in the upper limit of (1)? In a single dimension its easy to get the limits 0,1 using u=1/x-1 but in the two dimensional plane I'm confused by this substitution.","['special-functions', 'integration', 'functions', 'beta-function']"
2360105,Determining whether a function is injective,"Question: Let f : $\mathbb{R}$ $\to$ $\mathbb{R}$ via $ f(x) = \frac{x}{1+x^2}$. Is $f$ injective? My attempt: I am unable to find a counter example to prove that it is not injective. Suppose $ f(a) = f(b)$ for some $a,b \in \mathbb{R}$. $ \frac{a}{1+a^2}=\frac{b}{1+b^2}$ $a\ +\ ab^2\ =\ b\ +ba^2$ $ ab^2-b\ \ =ba^2-a$ $ b\left(ab-1\right)=a\left(ab-1\right)$ $ \left(b-a\right)\left(ab-1\right)=0$ Would this imply that $ a = b$? In questions where I cannot easily figure out a counter example to prove a function is not injective, what should I do? Should I try to prove that it is injective and then reach a contradiction?","['elementary-set-theory', 'functions']"
2360111,Calculate the area of a triangle with four circles inside,"The four shown circles have the same radius and each one is tangent to one side or two sides of the triangle. Each circle is tangent to the segment which is inside the triangle ABC. Besides, the central lower circle is tangent to its neighbor circles. If AC = 12 cm, what is the value of the area of the triangle ABC? I tried to assign angle variables to the triangle to compute the sides of the triangle, and so to use Heron's formula to calculate the area, but after all, it appears that some data is missing. I haven't found any book or article which treats these kind of problems. Many thanks in advance",['geometry']
2360112,Interplay between axiomatic hyperbolic geometry and the hyperboloid model.,"If I can prove a statement about axiomatic hyperbolic geometry, in the language of the hyperboloid model, to be true, must it be true in the original axiomatic hyperbolic geometry? I believe that the axioms for hyperbolic geometry are consistent (due to Beltrami). Does it follow that we can then engage in this interplay? This article ( https://en.wikipedia.org/wiki/Model_theory#Using_the_compactness_and_completeness_theorems ) seems to suggest that we can do so. If possible, could you point me to some textbooks which deal with some of these questions (preferably with examples in a geometric setting)? Thank you!","['logic', 'geometry']"
2360117,A partial differential equation with trig functions,"Let $\xi:\mathbb{R}^3\rightarrow \mathbb{R}$ be a function $\xi(x,y,z)$. I have the following PDE:
$$
\cos(\xi)\partial_y\xi = \sin(\xi)\partial_x\xi
$$
which is equivalent to $\tan(\xi)=\xi_y/\xi_x$. Clearly, $\xi_x = \xi_y = 0$ (where $\partial_i\xi=\xi_i$) is a solution, which would imply $\xi(x,y,z) = \eta(z)$ is only a function of $z$. But are there any other solutions? If not, how to prove it? My thoughts so far: obviously, 
\begin{align}
\xi_x &= \cos(\xi)\\
\xi_y &= \sin(\xi)
\end{align} 
together describe a solution.
We can solve these separately to get:
$$
\xi = 2\arctan(\exp(y+f_1(x,z))\;\;\;\&\;\;\; 
\xi=2\arctan(\tanh([1/2][x+f_2(y,z)]))
$$
respectively. 
This implies that $$
\exp(y+f_1(x,z)) = \tanh([x+f_2(y,z)]/2) $$
Not sure if this line of thinking is useful though.","['trigonometry', 'partial-derivative', 'ordinary-differential-equations', 'partial-differential-equations']"
2360128,Hartshorne exercise II.3.22 (c); Dimension of Fibers,Let $f:X \to Y$ be a dominant morphism of integral schemes of finite type over a field. Show there is an open dense subset $U  \subseteq X$ s.t. $\dim {U _y} = \dim X - \dim Y$ for all $y$ in the image of $U$. Been working on this for awhile now and can't figure it out.,"['dimension-theory-analysis', 'algebraic-geometry']"
2360164,A right angle at the focus of a hyperbola,"$P$ is a point on a hyperbola. The tangent at $P$ cuts a directrix at point $Q$. Prove that $PQ$ subtends a right angle to the focus $F$ corresponding to the directrix. I have tried to use the general equation of the hyperbola and gradient method to show, but too many unknowns and I can't continue. I tried to show $m_1 m_2 = -1$, but I stuck halfway. Note (From @Blue). This property holds for all conics, except circles, which have no directrix. For ellipses and hyperbolas, the property holds for either focus-directrix pair. A proof incorporating this level of generality would be nice to see. We can restate the property in a way that includes the circle as a limiting case: $P$ is a point on a conic with focus $F$. The line perpendicular to $\overline{PF}$ at $F$ meets the tangent at $P$ in a point on the directrix corresponding to $F$; if $P$ is a vertex, then the perpendicular, tangent, and directrix are parallel, meeting at a point ""at infinity"". In the case of a circle, the perpendicular is parallel to the tangent (so that they ""meet"" in a point on a ""directrix at infinity"").","['analytic-geometry', 'conic-sections', 'circles', 'geometry']"
2360181,Why doesn't the definition of dependence require that one can expresses each vector in terms of the others?,"I was reviewing my foundations on linear algebra and realized that I am confused about independence and dependence. I understand that by definition independence means: A set of vectors $\{x_1,\ldots,x_k\}$ is independent if the only linear combination that gives the zero vector is the zero vector itself. i.e. if $[x_1, \ldots, x_k]c = Xc = 0$ iff $c=0$ I understand what the definition says but it sort of goes against my intuition of what the definition of dependence should be (and hence its negation independence ). In my head intuitively dependence means that the a set of vectors depends on each other. In other word one should always be able to express one vector as a linear combination of the others. Something like: $$ \forall x_i \in \{x_1,\ldots,x_k\}, \exists c \neq 0 : \sum_{j \neq i} c_j x_j = x_i$$ however with my definition above (which is wrong and is not what the standard definition is, I know but I am trying to come to terms why its wrong) implies that a set of independent vectors with the zero vector tacked on is not dependent (i.e. independent) which is the opposite of what is should be. i.e. tacking the zero vector and the set remains independent (this should be wrong cuz [0,...,0,1] is not the zero vector and only the zero vector should give 0). Consider for a simple example $ \{ x_1,x_2,0 \}$ where $x_1,x_1$ only give zero with the zero vector (standard definition of independence). With my definition of things its obvious that these vectors are independent. In reality they should be dependent because [0,0,1] is now in the nullspace but things are only independent if only the zero vector is in the nullspace. With my definition the vectors are independent because there is no way to express any of them in terms of each other. For example: $a x_1 + b x_2 = 0$ $c x_1 + d 0 = x_2$ $e x_2 + f 0 = x_1$ non of the above can be made true with non zero (non-trivial) linear combinations. Thus, the vectors are not dependent so they are independent. I know its sort of an ""edge case"" condition for the definition but it sort of flipped my world to find out that I've been thinking about such a fundamental concept like independence and dependence wrongly in linear algebra and I'm trying to come to terms with it. Why is my intuition incorrect? Why was the standard definition of independence as $Xc = 0 \iff c=0$ the accepted definition of independence? Whats wrong with my definition? Are they essentially the same definition except for this weird edge case? last footnote is about what the word dependence means with respect to the number and vector zero. I think what my last confusion boils down to is why $0x = \mathbf{0}$ is considered as $\mathbf{0}$ depending on $x$. I guess in my head saying that we don't need any of $x$ to express $\mathbf{0}$ seems to mean that $\mathbf{0}$ doesn't need $x$ (or any other vector). But the convention according to everything pointed out by everyone in these set of answers points out to the opposite. I don't understand why. Is it that just having an equation linking terms means dependence even if we specify with a zero that we don't actually need the term?","['intuition', 'linear-algebra', 'linear-transformations']"
2360202,"If $\sin(\alpha)+\cos(\alpha)=a,$ denote $|\sin(\alpha)-\cos(\alpha)|$ in terms of $a$","I attempted to solve it using the following identities: $1.\ a^2-b^2.\ 2.\ (a\pm b)^2.\ 3. a^3\pm b^3.\ 4.\ (a\pm b)^3.$ Since none of my efforts led me to the correct answer (which is $\sqrt{2-a^2}$), I found it better not to write my lengthy work towards solving it. I mostly got the expression $2\sin(\alpha)\cos(\alpha).$ So, any helpful hints, comments or answers are welcome!","['trigonometry', 'absolute-value']"
2360205,Almost equivalent definitions of the Riemann–Stieltjes integral,"Below, I will present two definitions of the Riemann–Stieltjes integral, the second of which is more general. My question concerns the relationship between these two definitions. Definition 1 : Let $f,g:[a,b] \to \mathbb{R}$. For a partition $P=\{x_0, x_1,x_2 \cdots x_{n-1},x_n\}$ of $[a,b]$, consider the sum $$S(P,f,g)  \stackrel{\rm def}{=} \sum_{i=0}^{n-1} f(c_i) \left[g(x_{i+1}) - g(x_{i})\right]$$ where we have ""sample points"" $c_i \in [x_i, x_{i+1}]$. $f$ is then said to be Riemann–Stieltjes integrable with respect to $g$ if there is a real number $L$ with the following property: for all $\epsilon>0$ there is a $\delta>0$ such that for any partition $P$ with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$ and any sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we have $$\left|S(P,f,g) -
 L\right| < \epsilon$$ Definition 2 : We modify the above definition so that it is like this instead: for all $\epsilon>0$ there is partition $P_{\epsilon}$ such that any refinement $P' \supset P_{\epsilon}$ satisfies $$\left|S(P',f,g) -
 L\right| < \epsilon$$ independent of the sequence of points $\{c_i\}_{{0\leq i \leq n-1}, c_i \in [x_i, x_{i+1}]}$ we choose. Remark : The first definition implies the second. Simply let $P_{\epsilon}$ be any partition with $\text{sup}_{{0\leq i \leq n-1}}(x_{i+1} - x_i) < \delta$. However, interestingly, the second definition does not imply the first. Take $$g(x) =
\begin{cases}
0  & x \in [0, \frac 12) \\
1, & x \in [\frac 12, 1]
\end{cases}$$ $$f(x) =
\begin{cases}
0  & x \in [0, \frac 12] \\
1, & x \in (\frac 12, 1]
\end{cases}$$ as a counterexample. For this example, the integral exists and is equal to $0$ in the sense of the second definition by ensuring our chosen partition $P{_\epsilon}$ is such that $\frac 12 \in P_{\epsilon}$. This ensures $g(x_{i+1}) - g(x_i) = 0$ except in the interval $[x_k, \frac 12]$; however, this interval does not affect the sum since $f \equiv 0$ in $[x_k, \frac 12]$. Conversely, for the first definition we needn't have $\frac 12 \in P$. $\frac 12$ may be in the interior of some subinterval $[x_i, x_{i+1}]$ (ie., $x_i < \frac 12 < x_{i+1}$). This would mean that $g(x_{i+1}) - g(x_i) = 1$, and depending on the ""sample point"" $c_i$ we choose in this subinterval, the sum may be $1$ or $0$. This can happen regardless of how fine the partition is, and hence the integral does not exist. Problem: Are there any regularity conditions we can impose on $g$ to ensure the equivalence of the above definitions? Strict monotonicity is a natural example. If that doesn't work, consider stronger conditions (e.g., $g$ is homeomorphism onto its image, or a $C^{1}$ diffeomorphism).","['real-analysis', 'integration', 'stieltjes-integral', 'riemann-integration']"
2360232,Find the number of ways to rearrange the word AARDVARK if ...,"a)No more than two As can appear together? My Answer: I tried the complement. The total number of rearrangements minus if we glued the A's together and rearranged it that way. 
$$\binom 83\binom52\binom31\binom21\binom11-\binom62\binom41\binom31\binom21\binom11$$ b) each R must be preceded by an A My Answer:I glued the R's with the A's (R-A), but that left me with one A left. Then I counted how many ways you could arrange each letter. The $\binom62$ is for the R-A's. $$\binom62\binom41\binom31\binom21\binom11$$ Not sure if these are correct or if there is another way to go about the problems. Please help me understand what I should be doing. Note: Stirling numbers should not be used since my class it has yet to be discussed.","['combinations', 'combinatorics', 'discrete-mathematics']"
2360398,Well-ordered set and countable set [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Question 1: Let $P$ is a well-ordered set of real numbers with the usual order. Is $P$ always countable? I think it is true, but I can't prove it. Question 2: Let $P$ is a well-ordered set. Is $P$ always countable? Thanks in advance.","['general-topology', 'elementary-set-theory']"
2360435,Qcqs lemma in Ravi Vakil's notes,"Qcqs lemma in Ravi Vakil's notes says that : If $X$ is a quasi compact quasi separated scheme and $s\in \Gamma(X,\mathcal{O}_X)$ then the natural map $\Gamma(X,\mathcal{O}_X)_s\rightarrow \Gamma(X_s,\mathcal{O}_X)$ is an isomorphism. I was trying to prove this on my own as this looks related to my previous question . This is more or less a trivial statement in case of affine schemes.  When $X=\text{Spec}(A)$, $s\in \Gamma(X,\mathcal{O}_X)=\mathcal{O}_X(X)$.
We have, $X_s=\{\mathfrak{p}\in X:s_{\mathfrak{p}}\notin \mathfrak{m}_\mathfrak{p}\subseteq \mathcal{O}_{\mathfrak{p}}\}$.  Identifying $\mathcal{O}_X(X)$ with $A$ and $\mathcal{O}_{\mathfrak{p}}$ with $A_{\mathfrak{p}}$ we have $s_{\mathfrak{p}}=\frac{s}{1}\in A_{\mathfrak{p}}$.
So, $X_s=\{\mathfrak{p}\in X:\frac{s}{1}\notin \mathfrak{p}A_{\mathfrak{p}}\}=\{\mathfrak{p}\in X:s\notin \mathfrak{p}\}=D(s)$.
So, $\Gamma(X_s,\mathcal{O}_X)=\Gamma(D(s),\mathcal{O}_X)=A_s$. We thus have isomorphism $\Gamma(X,\mathcal{O}_X)_s\rightarrow \Gamma(X_s,\mathcal{O}_X)$. Let $X$ be an arbitrary scheme. We have $\Gamma(X,\mathcal{O}_X)\rightarrow \Gamma(X_s,\mathcal{O}_X)$, just the restriction map. In case of restiction map $\Gamma(X,\mathcal{O}_X)\rightarrow \Gamma(X_s,\mathcal{O}_X)$, we have $s\mapsto s|_{X_s}$. A section $t\in \mathcal{O}(U)$ is invertible iff $t_x\in \mathcal{O}_x$ is invertible for every $x\in U$. By definition, $s_x\in \mathcal{O}_x$ is invertible for every $x\in X_s$ so, $s|_{X_s}$ is invertible. So, $s^n|_{X_s}$ is invertible in $\Gamma(X_s,\mathcal{O}_X)$. Image of every element of $S=\{1,s^n:n\in \mathbb{N}\}$ is invertible in $\Gamma(X_s,\mathcal{O}_X)$. Let $A$ be a ring, $S$ be a multiplicatively closed subset of $A$ then, we have the notation of localization of $A$  with respect to $S$ which comes with map $\pi:A\rightarrow A_S$ with $a\mapsto \frac{a}{1}$ and with universal property that given any ring $B$ and a ring homomorphism $f:A\rightarrow B$    such that $f(s)$ is a unit in $B$ for every $s\in S$ then there exists a unique map $g:A_S\rightarrow B$ such that $f=g\circ \pi$. So, we do have a map $\Gamma(X,\mathcal{O}_X)_s\rightarrow \Gamma(X_s,\mathcal{O}_X)$ coming from the  map $\Gamma(X,\mathcal{O}_X)\rightarrow \Gamma(X_s,\mathcal{O}_X)$. I am not able to proceed further. Ravi Vakil has considered some exact sequences but I think it can be done just from the map from restriction. Any hints are welcome.","['schemes', 'affine-schemes', 'algebraic-geometry']"
2360475,"Evaluating $-\int_0^1\frac{1-x}{(1-x+x^2)\log x}\,dx$","I was trying do variations of an integral representation for $\log\frac{\pi}{2}$ due to Jonathan Sondow, when I am wondering about if it is possible to evaluate $$\int_0^1-\frac{1-x}{(1-x+x^2)\log x}\,dx,\tag{1}$$ Wolfram Alpha online calculator provide me a closed-form with code int -(1-x)/((1-x+x^2)log(x)) dx, from x=0 to x=1 Question. Please provide me hints to know how evaluate previous this definite integral as
  $$\int_0^1-\frac{1-x}{(1-x+x^2)\log x}\,dx=\log \left(\frac{\Gamma(1/6)}{\Gamma(2/3)}\right)-\frac{\log \pi}{2}$$
  as said Wolfram Alpha. Many thanks.","['integration', 'definite-integrals']"
2360485,"A space whose powers ""generate"" all spaces via quotients","I've been asked to prove (or disprove) that there exists a topological space $X$ with the following property: For every space $T$ there is a set $I$ and a homeomorphism $T\cong A$ where $A$ is a quotient of a subspace of $X^I = \prod_{i\in I} X$ (product topology) Of course, this smells like a universal property, and a rather strong one, so I'm inclined to think there is no such $X$. But how can one prove that such a space does not exist?",['general-topology']
2360494,How to find solutions to equation $f'(x)+f(x)-Kf(-x)=0$,"I want to find solutions to the ordinary differential equation $$f'(x)+f(x)-Kf(-x)=0$$ when $K$ is a nonzero real constant. When $K=1,$ I can find the solution $f(x)=C(x-\tfrac{1}{2}),$ but for more general $K$ it is more challenging. I am able to find power series solutions simply by taking $$f(x)=\sum\limits_{n=0}^{\infty}a_nx^n$$ but I would really like to have something more insightful here, such as a closed-form solution in terms of whatever functions they may be, or even just some properties that we can say about solutions to this differential equation. Alternatively, if anyone can give me suggestions on how to proceed with finding out these things for myself, I would welcome any help. The main issue comes from that $f(-x)$ term anyway. Thanks.","['ordinary-differential-equations', 'closed-form', 'functional-equations']"
2360526,Prove by counterexample that a bounded sequence in a metric space need not have a convergent subsequence,"I am trying to prove that a bounded sequence in a metric space need not have a convergent subsequence. My counterexample is: Consider the metric space:  ($(0,1]$,standard norm on $\Bbb R$ restrict to $(0,1]$) and the sequence $1,\frac{1}{2},\frac{1}{3},...$ which is bounded. As $0\notin (0,1]$,the sequence is not convergent. I thought about proving that if the sequence has a convergent subsequence, it must converge to $0$, then as $0\notin (0,1]$, we have a contradiction. Here is my attempt: Suppose, in order to get a contradiction, that a subsequence of the sequence $x_n=\frac{1}{n}$, call it $(x_{n_j})$, is a convergent subsequence. Suppose it converges to $a\in (0,1]$, then from the definition of convergence: $(\exists a\in (0,1])(\forall \epsilon>0)(\exists N\in \Bbb N)(j\ge N\implies |x_{n_j}-a|<\epsilon)$ To obtain the contradiction, we prove that for this $a$: $(\exists \epsilon>0)(\forall N\in \Bbb N)(\exists j\ge N\land |x_{n_j}-a|\ge\epsilon)$ Proof: We know that $\frac{1}{n}$ can be less then any $a>0$ if we take $n$ large, so we have a term $x_{n_l}$ of $(x_{n_j})$ such that $x_{n_l}<a$. Set $\epsilon=a-x_{n_l}$, then for all $N\in \Bbb N$, take $j\ge l$, then we have $|a-x_{n_j}|>\epsilon$ since the sequence $\frac{1}{n}$ is decreasing. Could some please check if the argument above is correct? Thanks in advance!","['real-analysis', 'metric-spaces', 'sequences-and-series', 'analysis']"
2360579,k-schemes determined by k-rational points,"Are $k$-schemes determined by their $k$-rational points? 
More precisely, if $X$ and $Y$ are $k$-schemes, such that $X(k)=Y(k)$. Is then automatically $X$ isomorphic to $Y$?","['schemes', 'algebraic-geometry']"
2360592,How do I calculate the 4th power of the Dirichlet integral with Fourier transforms?,"I'm pretty new to Fourier Transforms and I stumbled upon this exercise, which asks me to calculate this integral
$$\int_{-\infty}^{+\infty}\left(\frac{\sin(ax)}x\right)^4\,dx$$
The exercise suggests to use fourier transforms, but I really don't get how! I recognized that the term inside the parentheses is a Dirichlet integral, so I know its transform, which is $$\frac{1}2\sqrt\frac{\pi}2 \left(\text{sgn}(a-\lambda) +\text{sgn}(a+\lambda\right))$$ but I can't just multiply everything by $e^{-ia\lambda}$ as if were nobody's business! And then inside that integral I have the 4th power the transform, how do I deal with it? Thanks a lot for your patience","['integration', 'fourier-transform']"
2360603,How to find the intersection of two hyperplanes in $n$ dimensions?,"I want to find out the intersection of two surfaces that exist in $n$-dimensions. These surfaces are defined by a system of $2$ linear equations. $$A_1x+B_1y+C_1z+\cdots = D_1$$ $$A_2x+B_2y+C_2z+\cdots = D_2$$ But first, how would these surfaces look like? In $3$-D space, they would be planes, but in dimensions higher than $3$ are these not planes? Can we generalize a way to find the intersection of such linear expressions in some way? Also can this problem be solved using some tools, preferably Matlab?","['linear-algebra', 'surfaces', 'systems-of-equations']"
2360624,Anti-isomorphisms,"Definition : Let $G,H$ be groups and $f: G \to H$ a function such that $f(gh) =f(h)f(g)$ for any $g,h \in G$. Then we call $f$ an antihomomorphism. (Note the swapped order of $f(h)$ and $f(g)$.) I was deriving some properties of antihomomorphisms and I found that there were a lot of similarities with the usual homomorphisms: For example, for any antihomomorphism $f: G \to H$, we have: $f(e_G) = e_H$ $f(g^{-1}) = f(g)^{-1}$ If we define $ker f$ in the usual way (i.e. $ker f = \{g \in G|f(g) = e_H\}$), we have: $f$ injective $\iff ker f = \{e_G\}$ $ker f \unlhd G$ If we denote the existence of a bijective antihomomorphism between $2$ groups with $\asymp$, we have: $G/ker f \asymp Im(f)$ Also interesting: $G \asymp H$ and $H \asymp F \Rightarrow G \cong F$ I know that the existence of an isomorphism between $2$ groups means that both groups have exactly the same structure. So my question is: From a group theoretic point of view, what is the use of bijective
  antihomomorphisms (= anti-isomorphisms) between 2 groups. Can we give
  it an interpretation like we have for regular isomorphisms?","['abstract-algebra', 'group-theory', 'group-isomorphism', 'group-homomorphism']"
2360629,Could we compute integral of $z_i\bar{z_j}/\sum |z_k|^2$ on $\mathbb{C}P^n$ with the Fubini-Study metric?,"Let $\mathbb{C}P^n$ be the complex projective space with homogeneous coordinate $Z=[z_0,\ldots,z_n]$. Let $\omega$ be the Fubini-Study metric $\omega=\frac{i}{2\pi}\partial\bar{\partial}\log||Z||^2$. From this stackexchange answer we know that the volume of $\mathbb{C}P^n$ with the Fubini-Study metric is $\frac{\pi^n}{n!}$, i.e.
$$
\int_{\mathbb{C}P^n}1\cdot \omega^n=\frac{\pi^n}{n!}.
$$
Then by symmetry, it is easy to see that 
$$
\int_{\mathbb{C}P^n}\frac{z_i\bar{z_i}}{||Z||^2}\cdot \omega^n=\frac{\pi^n}{(n+1)!}, \forall~ 0\leq i\leq n.
$$
Now for different $i$ and $j$, could we compute
$$
\int_{\mathbb{C}P^n}\frac{z_i\bar{z_j}}{||Z||^2}\cdot \omega^n?
$$
More generally, for any $k\geq 0$ and any two tuples $(i_0,\ldots i_k)$ and $(j_0,\ldots j_k)$, define a function 
$$
f(Z)=\frac{z_{i_0}\ldots z_{i_k}\bar{z_{j_0}}\ldots\bar{z_{j_k}}}{||Z||^{2k}}.
$$
Could we get a formula for the integral
$$
\int_{\mathbb{C}P^n}f\cdot \omega^n?
$$ Edit: According to the answer of @AmitaiYuval, we only need to consider the case where
  $$
f(Z)=\frac{|z_{i_0}|^2\ldots |z_{i_k}|^2}{||Z||^{2k}}.
$$ Edit: For $\mathbb{C}P^1$, a iterating computation shows that for 
  $$
f(Z)=\frac{|z_0|^{2k}|z_1|^{2(k-l)}}{||Z||^{2k}}
$$
  we have
  $$
\int_{\mathbb{C}P^1}f\cdot \omega=\frac{{k\choose l}\pi}{k+1}.
$$
  But we are still looking for the answer for higher dimensions.","['complex-geometry', 'riemannian-geometry', 'differential-geometry', 'algebraic-geometry']"
2360631,Weak star closure and Eberlein-Smulian theorem,"I'm trying to understand the implication $(iii)\rightarrow(i)$ on Albiac and Kalton's proof of the Eberlein-Smulian Theorem. Specifically, why $x^{\ast\ast}\in \overline{A_0}^{w^\ast}$. I thought of showing that $$\overline{A_0}^{w^\ast} = \{y^{\ast\ast}\in X^{\ast\ast} : y^{\ast\ast}(x^\ast) \geq 1\}$$ but while the inclusion $\subset$ is immediate, to show the other one, given $y^{\ast\ast}$ such that $y^{\ast\ast}(x^\ast) \geq 1$, I tried to construct a net in $A_0$ converging to $y^{\ast\ast}$. Not only could I not make such construction but also it seems that it would be the same as constructing such a net converging to $x^{\ast\ast}$ and showing directly that $x^{\ast\ast}\in \overline{A_0}^{w^\ast}$. Instead of constructing a net, I also tried to find a convenient neighborhood basis for the weak* topology to work with and show that for every weak* neighborhood $V$ of $x^{\ast\ast}$, $V\cap A_0\neq\emptyset$, but all to no avail. I'd appreciate some suggestions on how to proceed.","['functional-analysis', 'banach-spaces', 'proof-explanation']"
2360649,Matrix similarity equivalent to same characteristic polynomial and same geometric multiplicity,"I'm wondering whether the similarity of two square matrices is equivalent to them having the same characteristic polynomial and the same geometric multiplicity for each eigenvalue. It's obvious in case they are both diagonalizable but is it true when they are not? I can't seem to figure it out, any help would be nice.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2360657,Continuity on Box Topology,"I was reading about box topology and product topology from Munkres's Topology. Their is an example given below: $$\mathbb{R}^{\omega}=\prod_{n\in \mathbb{Z}_+}X_n$$ where $X_n=\mathbb{R}$ for each $n$ . $$f: \mathbb{R}\rightarrow \mathbb{R}^{\omega}$$ $$f(t)=(t,t,\dots), t\in \mathbb{R}$$ the $n$ -th coordinate function of $f$ is the function $f_n(t)=t$ . It is clear by the counter example given that if $\mathbb{R}^{\omega}$ is box topology then $f$ is not continuous. My question: $$\mathbb{R}^{\omega}=\prod_{n\in \mathbb{Z}_+}X_n$$ where $X_n=\mathbb{R}$ for each $n$ . $f: \mathbb{R}\rightarrow \mathbb{R}^{\omega}$ . The $n$ -th coordinate function of $f$ is the function $f_n:\mathbb{R}\rightarrow \mathbb{R}$ is a non-constant onto continuous function, $\mathbb{R}^{\omega}$ is box topology. Then can $f$ ever be continuous? What if we omit onto?","['continuity', 'general-topology', 'box-topology']"
2360690,When to use the multiplication rule in probability versus when to use a tree?,"So, as I have understood it, if you have two experiments and you want to know the probability of a set of two outcomes happening concurrently, then you multiply the chance of the first outcome by the chance of the second outcome and voila, you have your probability. However, I am confused as to when this doesn't work. For example, I just did a problem: James lives in San Francisco and works in Mountain View. In the
  morning, he has 3 transportation options (bus, cab, or train) to work,
  and in the evening he has the same 3 choices for his trip home. What is the probability that he uses the same of mode of transportation twice? My first inclination was 1/9th but apparently I am wrong. I was told to use a tree to count the favorable outcomes. I did so, and see that the answer is 1/3, but for the life of me I can't see the difference between this question and the first type I mentioned. I am obviously missing some finer points or nuance in the question which should clue me in. What is it?",['probability']
2360693,Number of permutations with $n-2$ fixed-points and one $2$-cycle same as number of permutations with $n-6$ fixed-points and three $2$-cycles.,I have a question to an exercises which i cannot solve: Determine all $n\geq 6$ for which the following statement is correct : There are as many permutations with $n-2$ fixed-points and one $2$-cycle as permutations with $n-6$ fixed-points and three $2$-cycles. I'm trying to find a bijection between the two sets of those permutation but i can't find a function. Any help is highly appreciated. Thanks! Edit: Made a mistake in the problem of the exercise. The bold text is edited.,"['permutations', 'combinatorics', 'discrete-mathematics']"
2360723,equivalence of Hilbert spaces,"the question might be stupid, but I am confused. Let us consider the following Hilbert space $l_{2}^{W}$, the space of infinite sequences with a scalar product:
$$
<X,Y> = \sum_{i=1}^{\infty}x_{i}y_{i}w_{i},
$$
for some vector $W= (w_{1},w_{2}, \dots)$ with $0 < w_{i} < \infty$ and $\limsup w_{i} < \infty$. I attempt to show that all spaces $l_{2}^{W}$ are all equivalent, i.e. norms generated by the scalar product are equivalent:
for any $W_{1}$ and $W_{2}$ there exist $0 < C_{1} < C_{2}$ such that $C_{1} ||X||_{W_{2}} \leq ||X||_{W_{1}} \leq C_{2} ||X||_{W_{2}} $. Should one impose more constraints on the vector of weights then?","['real-analysis', 'hilbert-spaces', 'functional-analysis', 'measure-theory', 'analysis']"
2360725,Finding the general solution for trigonometric equation,"Solve for general solutions $\tan(x/3) = 1$ When I solve this equation my answer comes to be $x = 3\pi/4 \pm 2n\pi,  15\pi/4 \pm 2n\pi$ where $n$ is an integer However when I graph the equation $y = \tan(x/3) - 1$ values for $x$ such as $11\pi/4$ do not equal zero. What would $x$ be equal to then?",['trigonometry']
2360747,Local ring after adjoining a root,"Let $\mathbb{Z}_p$ be the ring of $p$-adic numbers, $\Phi _p(x)$ the cyclotomic polynomial and $\zeta$ be a $p$-th root of unity. So $\zeta \not \in \mathbb{Z}_p$ and we get 
$$ \mathbb{Z}_p[X]/\langle\Phi_p(x)\rangle \cong \mathbb{Z}_p[\zeta] = \mathbb{Z} \oplus \zeta \mathbb{Z} \oplus \dots \oplus  \zeta^{p-2} \mathbb{Z} $$
a free $\mathbb{Z}_p$-module of rank $p-1$. Let $\pi := \zeta-1$ and I want to know how to see that $\mathbb{Z}_p[\zeta]$ is a local ring with maximal ideal $\pi \mathbb{Z}_p[\zeta]$. My idea is to use the obviously fact that $\Phi _p(\pi +1) =0$ and the extension formula $ \Phi_p(x + 1) = p + \binom{p}{2}x + \binom{p}{3}x^2 + \dots + \binom{p}{p - 1} x^{p - 2} + x^{p - 1} $. From this I can conclude that we have following inclusions:
$p \mathbb{Z}_p[\zeta] \subset \pi \mathbb{Z}_p[\zeta]$
and $\pi^{p-1} \mathbb{Z}_p[\zeta] \subset p \mathbb{Z}_p[\zeta]$ It's obviosly that it would be enough to show that $\mathbb{Z}_p[\zeta] /  \pi\mathbb{Z}_p[\zeta]  \cong \mathbb{F}_p$ and that every maximal ideal $M \subset \mathbb{Z}_p[\zeta]$ contains $\pi$ but unfortunately I haven't a idea how to realize this two steps using the information above...","['abstract-algebra', 'ring-theory', 'local-rings', 'commutative-algebra']"
2360757,Derivative of trace,"I had never even heard of matrix calculus before yesterday but need it for a simple calculation and am stumped. If I can solve the following problem, I think I'll be fine for my larger calculuation. Let $F: \text{GL}(2,\mathbb{C}) \to \mathbb{R}$ via 
$$
g \mapsto \text{tr}((gAg^{-1})^*gAg^{-1})
$$
For some fixed matrix $A$. So this is like an inner product, but conjugated. My question is, what is the derivative of this? I want to take the derivative at the identity, so I should get a map:
$$
dF\vert_{id}: T_0\text{GL}(2,\mathbb{C}) \cong \text{End}(2,\mathbb{C}) \to T_o\mathbb{R} \cong \mathbb{R}.
$$
In particular, given some generic matrix $M$ (some endomorphism), What is $dF\vert_0 (M)$?","['multivariable-calculus', 'matrix-equations', 'matrix-calculus', 'linear-algebra']"
2360790,"Calculating surface area of $x^2 + y^2 + z^2 = 4$, $z \geq 1$","I'm asked to calculate the surface area of 
$$D : x^2 + y^2 + z^2 = 4 , \quad z \geq 1.$$ My attempt Let 
$$x=2\sin{\theta}\cos{\phi}$$
$$y=2\sin{\theta}\sin{\phi}$$
$$z=2\cos{\theta}$$
where 
$$ 0 \leq \theta \leq \frac{\pi}{3}$$ 
$$0\leq\phi\leq2\pi.$$ I realize the normal vector to the surface is $(x,y,z)$ which has length $\sqrt{x^2 + y^2 + z^2} = 2$. Surface area is calculated by integrating over the area D, with the length of the normal vector as the integrand. But since I changed the surface D to the surface given by the spherical coordinate system (call this surface E), I need to add a factor to compensate (Jacobi Determinant). But since I've got a variable substitution with 3 functions and 2 variables, this won't be a square matrix so I won't be able to take the determinant of it. What am I doing wrong?","['multivariable-calculus', 'surface-integrals', 'jacobian']"
2360792,Determine if these functions are injective,Determine if the following functions are injective. $$f(x) = \frac{x}{1+x^2}$$ $$g(x) = \frac{x^2}{1+x^2}$$ My answer: $f(x) = f(y)$ $$\implies \frac{x}{1+x^2} = \frac{y}{1+y^2}$$ $$\implies x+xy^2 =y+yx^2$$ $$\implies x=y$$ Hence $f(x)$ is injective $g(x) = g(y)$ $$\implies \frac{x^2}{1+x^2}=\frac{y^2}{1+y^2}$$ $$\implies x^2+x^2y^2=y^2+y^2x^2$$ $$\implies x^2=y^2$$ $$\implies \pm x=\pm y$$ So $g(x)$ is not injective,"['algebra-precalculus', 'functions']"
2360813,Find minimum of $a+b+c+\frac1a+\frac1b+\frac1c$ given that: $a+b+c\le \frac32$,"Find minimum of $a+b+c+\frac1a+\frac1b+\frac1c$ given that: $a+b+c\le \frac32$ ($a,b,c$ are positive real numbers). There is a solution, which relies on guessing the minimum case happening at $a=b=c=\frac12$ and then applying AM-GM inequality,but what if one CANNOT guess that?!","['inequality', 'a.m.-g.m.-inequality', 'algebra-precalculus', 'maxima-minima', 'sum-of-squares-method']"
2360831,What numbers are integrally represented by this quartic?,"This is from the polynomial example $x^4 + 3 x^2 + 7x + 4$ discussed in the framework of Chebotarev Density in Lenstra-Stevenhagen , especially pages 10,11,12 in this pdf. The article appeared in 1996 in The Mathematical Intelligencer. I guess I should repeat that they point out that $x^4 + 3 x^2 + 7x + 4$ is reducible mod every prime. This has thrown me off, I had this idea that reducibility would mean that the homogeneous polynomial below, the norm form, would integrally represent low powers of that prime, especially $p$ or $p^2$ or $p^3.$ Something very much like that happened with these earlier questions, what numbers are integrally represented by this quartic polynomial (norm form) https://mathoverflow.net/questions/127160/numbers-integrally-represented-by-a-ternary-cubic-form/127295#127295 primes represented integrally by a homogeneous cubic form I thought I had something reasonable which is to take the companion matrix
$$
A =
\left(
\begin{array}{rrrr}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
 -4 & -7 & -3 & 0
\end{array}
\right)
$$
and create
$$ f(x,y,z,t) = \det \left(  x  I + y A + z  A^2 + t A^3 \right)  $$
$$  f(x,y,z,t) =  x^4 + (-6z - 21t)x^3 + (3y^2 + (21z - 2t)y + (17z^2 + 21tz + 138t^2))x^2 + (-7y^3 + (-16z + 42t)y^2 + (-21z^2 - 99tz - 203t^2)y + (25z^3 + 28tz^2 + 139t^2z - 91t^3))x +  (4y^4 - 24ty^3 + (12z^2 + 84tz + 68t^2)y^2 + (-28z^3 - 64tz^2 - 84t^2z + 100t^3)y + (16z^4 + 48t^2z^2 - 112t^3z + 64t^4)) $$
Now, this is automatically completely multiplicative. If there is a quadruple of integers that cause $f$ to take the value $m,$ another quadruple that gives $n,$ we can explicitly find (using Cayley-Hamilton) a quadruple by which $f$ represents $mn.$ I had a fairly simple picture of what numbers would pop up, but nothing seems to work properly with this one. For one thing, it seems we cannot get quadratic non-residues $\pmod 7,$ which already strikes me as cheating. Here are the first 150 represented primes: jagy@phobeusjunior:~$ ./Lenstra_Chebotarev 
       1      29      53      71     137     149     163     193     263     317
     337     421     499     541     547     557     599     613     617     631
     641     673     701     709     739     743     751     809     823    1031
    1061    1087    1103    1163    1229    1283    1327    1367    1373    1409
    1423    1453    1471    1499    1579    1583    1619    1621    1733    1789
    1933    1997    2017    2039    2069    2081    2087    2111    2137    2153
    2179    2207    2221    2237    2251    2293    2297    2333    2347    2377
    2389    2447    2473    2503    2521    2531    2543    2683    2689    2713
    2797    2909    2969    3011    3187    3221    3259    3271    3301    3319
    3371    3467    3511    3593    3623    3677    3691    3803    3833    3847
    3889    3907    3917    4013    4019    4127    4139    4201    4211    4243
    4271    4327    4337    4349    4391    4421    4463    4621    4649    4691
    4729    4733    4817    4831    4937    4943    4957    4993    4999    5051
    5153    5209    5231    5413    5419    5443    5483    5503    5527    5531
    5573    5623    5653    5693    5779    5783    5791    5839    5849    5861 Hmmmm. It seems that when $p$ is not represented, we can represent $2 p^2$ 18 =  2 3^2     mod  seven  4     mod  nineteen  18
50 =  2 5^2     mod  seven  1     mod  nineteen  12
98 =  2 7^2     mod  seven  0     mod  nineteen  3
242 =  2 11^2     mod  seven  4     mod  nineteen  14
338 =  2 13^2     mod  seven  2     mod  nineteen  15
578 =  2 17^2     mod  seven  4     mod  nineteen  8
722 =  2 19^2     mod  seven  1     mod  nineteen  0
2738 =  2 37^2     mod  seven  1     mod  nineteen  2
3362 =  2 41^2     mod  seven  2     mod  nineteen  18
4418 =  2 47^2     mod  seven  1     mod  nineteen  10
6962 =  2 59^2     mod  seven  4     mod  nineteen  8
7442 =  2 61^2     mod  seven  1     mod  nineteen  13
10658 =  2 73^2     mod  seven  4     mod  nineteen  18
15842 =  2 89^2     mod  seven  1     mod  nineteen  15
18818 =  2 97^2     mod  seven  2     mod  nineteen  8
20402 =  2 101^2     mod  seven  4     mod  nineteen  15
22898 =  2 107^2     mod  seven  1     mod  nineteen  3
25538 =  2 113^2     mod  seven  2     mod  nineteen  2
34322 =  2 131^2     mod  seven  1     mod  nineteen  8
38642 =  2 139^2     mod  seven  2     mod  nineteen  15
45602 =  2 151^2     mod  seven  4     mod  nineteen  2
49298 =  2 157^2     mod  seven  4     mod  nineteen  12
55778 =  2 167^2     mod  seven  2     mod  nineteen  13
59858 =  2 173^2     mod  seven  1     mod  nineteen  8
64082 =  2 179^2     mod  seven  4     mod  nineteen  14
65522 =  2 181^2     mod  seven  2     mod  nineteen  10
72962 =  2 191^2     mod  seven  1     mod  nineteen  2
77618 =  2 197^2     mod  seven  2     mod  nineteen  3
99458 =  2 223^2     mod  seven  2     mod  nineteen  12 On that note, when $p,q$ are primes that are not represented, we can represent $pq$ as long as $pq \equiv 0,1,2,4 \pmod 7.$ Here are the first 100 numbers represented with $\gcd(x,y,z,t) = 1.$ jagy@phobeusjunior:~$ ./Lenstra_Chebotarev  | sort -n  | head -100
1 =   1      mod  seven  1     mod  nineteen  1
4 =  2^2     mod  seven  4     mod  nineteen  4
15 =  3 5     mod  seven  1     mod  nineteen  15
16 =  2^4     mod  seven  2     mod  nineteen  16
18 =  2 3^2     mod  seven  4     mod  nineteen  18
21 =  3 7     mod  seven  0     mod  nineteen  2
29 =  29     mod  seven  1     mod  nineteen  10
46 =  2 23     mod  seven  4     mod  nineteen  8
50 =  2 5^2     mod  seven  1     mod  nineteen  12
53 =  53     mod  seven  4     mod  nineteen  15
60 =  2^2 3 5     mod  seven  4     mod  nineteen  3
64 =  2^6     mod  seven  1     mod  nineteen  7
65 =  5 13     mod  seven  2     mod  nineteen  8
70 =  2 5 7     mod  seven  0     mod  nineteen  13
71 =  71     mod  seven  1     mod  nineteen  14
72 =  2^3 3^2     mod  seven  2     mod  nineteen  15
78 =  2 3 13     mod  seven  1     mod  nineteen  2
81 =  3^4     mod  seven  4     mod  nineteen  5
84 =  2^2 3 7     mod  seven  0     mod  nineteen  8
85 =  5 17     mod  seven  1     mod  nineteen  9
86 =  2 43     mod  seven  2     mod  nineteen  10
91 =  7 13     mod  seven  0     mod  nineteen  15
95 =  5 19     mod  seven  4     mod  nineteen  0
98 =  2 7^2     mod  seven  0     mod  nineteen  3
102 =  2 3 17     mod  seven  4     mod  nineteen  7
114 =  2 3 19     mod  seven  2     mod  nineteen  0
116 =  2^2 29     mod  seven  4     mod  nineteen  2
119 =  7 17     mod  seven  0     mod  nineteen  5
133 =  7 19     mod  seven  0     mod  nineteen  0
134 =  2 67     mod  seven  1     mod  nineteen  1
137 =  137     mod  seven  4     mod  nineteen  4
149 =  149     mod  seven  2     mod  nineteen  16
158 =  2 79     mod  seven  4     mod  nineteen  6
163 =  163     mod  seven  2     mod  nineteen  11
177 =  3 59     mod  seven  2     mod  nineteen  6
183 =  3 61     mod  seven  1     mod  nineteen  12
184 =  2^3 23     mod  seven  2     mod  nineteen  13
193 =  193     mod  seven  4     mod  nineteen  3
200 =  2^3 5^2     mod  seven  4     mod  nineteen  10
205 =  5 41     mod  seven  2     mod  nineteen  15
207 =  3^2 23     mod  seven  4     mod  nineteen  17
212 =  2^2 53     mod  seven  2     mod  nineteen  3
218 =  2 109     mod  seven  1     mod  nineteen  9
219 =  3 73     mod  seven  2     mod  nineteen  10
225 =  3^2 5^2     mod  seven  1     mod  nineteen  16
235 =  5 47     mod  seven  4     mod  nineteen  7
240 =  2^4 3 5     mod  seven  2     mod  nineteen  12
242 =  2 11^2     mod  seven  4     mod  nineteen  14
246 =  2 3 41     mod  seven  1     mod  nineteen  18
254 =  2 127     mod  seven  2     mod  nineteen  7
256 =  2^8     mod  seven  4     mod  nineteen  9
260 =  2^2 5 13     mod  seven  1     mod  nineteen  13
263 =  263     mod  seven  4     mod  nineteen  16
267 =  3 89     mod  seven  1     mod  nineteen  1
270 =  2 3^3 5     mod  seven  4     mod  nineteen  4
280 =  2^3 5 7     mod  seven  0     mod  nineteen  14
282 =  2 3 47     mod  seven  2     mod  nineteen  16
284 =  2^2 71     mod  seven  4     mod  nineteen  18
287 =  7 41     mod  seven  0     mod  nineteen  2
288 =  2^5 3^2     mod  seven  1     mod  nineteen  3
303 =  3 101     mod  seven  2     mod  nineteen  18
312 =  2^3 3 13     mod  seven  4     mod  nineteen  8
315 =  3^2 5 7     mod  seven  0     mod  nineteen  11
317 =  317     mod  seven  2     mod  nineteen  13
324 =  2^2 3^4     mod  seven  2     mod  nineteen  1
329 =  7 47     mod  seven  0     mod  nineteen  6
336 =  2^4 3 7     mod  seven  0     mod  nineteen  13
337 =  337     mod  seven  1     mod  nineteen  14
338 =  2 13^2     mod  seven  2     mod  nineteen  15
340 =  2^2 5 17     mod  seven  4     mod  nineteen  17
344 =  2^3 43     mod  seven  1     mod  nineteen  2
351 =  3^3 13     mod  seven  1     mod  nineteen  9
364 =  2^2 7 13     mod  seven  0     mod  nineteen  3
378 =  2 3^3 7     mod  seven  0     mod  nineteen  17
380 =  2^2 5 19     mod  seven  2     mod  nineteen  0
387 =  3^2 43     mod  seven  2     mod  nineteen  7
392 =  2^3 7^2     mod  seven  0     mod  nineteen  12
393 =  3 131     mod  seven  1     mod  nineteen  13
408 =  2^3 3 17     mod  seven  2     mod  nineteen  9
417 =  3 139     mod  seven  4     mod  nineteen  18
421 =  421     mod  seven  1     mod  nineteen  3
422 =  2 211     mod  seven  2     mod  nineteen  4
435 =  3 5 29     mod  seven  1     mod  nineteen  17
441 =  3^2 7^2     mod  seven  0     mod  nineteen  4
442 =  2 13 17     mod  seven  1     mod  nineteen  5
456 =  2^3 3 19     mod  seven  1     mod  nineteen  0
459 =  3^3 17     mod  seven  4     mod  nineteen  3
464 =  2^4 29     mod  seven  2     mod  nineteen  8
466 =  2 233     mod  seven  4     mod  nineteen  10
476 =  2^2 7 17     mod  seven  0     mod  nineteen  1
485 =  5 97     mod  seven  2     mod  nineteen  10
494 =  2 13 19     mod  seven  4     mod  nineteen  0
499 =  499     mod  seven  2     mod  nineteen  5
501 =  3 167     mod  seven  4     mod  nineteen  7
513 =  3^3 19     mod  seven  2     mod  nineteen  0
519 =  3 173     mod  seven  1     mod  nineteen  6
522 =  2 3^2 29     mod  seven  4     mod  nineteen  9
529 =  23^2     mod  seven  4     mod  nineteen  16
532 =  2^2 7 19     mod  seven  0     mod  nineteen  0
536 =  2^3 67     mod  seven  4     mod  nineteen  4
jagy@phobeusjunior:~$","['number-theory', 'norm-forms', 'algebraic-number-theory']"
2360963,Radius of convergence is the point at which the function ceases to be analytic: false for $k \neq \mathbb{C}$?,"Theorem : Let $U$ be an open set in $\mathbb{C}$, $f$ an analytic function on $U$, and $z_0 \in U$.  Then $f$ has a power series expansion centered at $z_0$.  If $r$ is a positive real number, and $U$ contains the disc of radius $r$ centered at $z_0$, then the radius of convergence of that power series is at least $r$. Any analytic continuation of $f$ to a larger open set $W$ containing $U$ is unique, so the same result holds with $U$ replaced by $W$. In other words, the boundary of the disc of convergence of a local power series expansion of an analytic function is the point at which the given function ceases to be analytic. The definition of an analytic function has a natural generalization to any topological field $k$ which is complete with respect to some absolute value (the main examples are $k = \mathbb{R}$ or a finite extension of $\mathbb{Q}_p$).  For $U$ an open set of $k^n$, an analytic function $f: U \rightarrow k$ is one which has a local power series expansion about every point of $U$.  This is defined in Serre, Lie Groups and Lie Algebras . When $k = \mathbb{R}$, the theorem is false.  Let $U = k$, and define $f: k \rightarrow k$ by $f(x) = \frac{1}{1+x^2}$.  Interpreting $f$ as the restriction to $\mathbb{R}$ of a complex analytic function, we see that $f$ is analytic, and about $z_0 = 0$ has the power series expansion $$1 - x^2 + x^4 - \cdots$$ which has radius of convergence $1$ (which follows from the Theorem and the fact that $\frac{1}{1+z^2}$ is meromorphic on $\mathbb{C}$ with singularities at $i, -i$. What about when $k$ is a finite extension of $\mathbb{Q}_p$?  Is the theorem still true?","['p-adic-number-theory', 'manifolds', 'number-theory', 'complex-analysis', 'analyticity']"
2361011,"Why is the power function considered an algebraic function, but the exponential function is NOT algebraic?","I am curious to know why if a mathematical expression contains an exponential function that expression may NOT be considered an algebraic expression, but if it contains a power function (if the variable is the base of a power expression) then that expression as a whole can be considered an algebraic expression.","['exponential-function', 'linear-algebra', 'exponentiation']"
2361040,Influence of small constant term on roots of polynomial,"Let $p(x)=a_n x^n + a_{n-1} x^{n-1} + ... + a_1 x + a_0,\enspace a_i\in\mathbb{C}$ some polynomial. Suppose that $|a_0|$ is very small (compared to the other coefficients' magnitude). Is there any way the (complex) roots of $p(x)$ could be heavily affected by setting $a_0 = 0$ ? I think the answer could be no because the polynomial is continuous in the constant term and thus small changes in the constant term will affect the function only slightly. But does this hold true for the location of the roots? Why am I asking this? I try to incorporate a not selfwritten custom root finder into my Matlab-program but unfortunately in some rare cases one of its loops doesn't converge if the input vector's constant coefficient is of the magnitude $\approx 10^{-18}$ and then the algorithm crashes. However it does converge if I set the constant term to zero but I worry whether I could get wrong results. EDIT 1: Based on the answer by @Fixed Point, I could find a successor ( NAClab ) of the Matlab root finder that I was using and it doesn't crash anymore. I then went on to quickly investigate the polynomials Fixed Point proposed. Here are the results: Figure1: Up to degree 17 the root finder keeps the results on the real axis.
From degree 18 onwards the calculated roots gain an imaginary part which grows linearly with the degree of the polynomial. The ratio between the constant coefficient $a_0$ and the highest order coefficient $a_n$ is in the order of $10^{-16}$ when the roots begin to diverge from the real axis and grows with approximately one order of magnitude per increase of polynomial degree. Figure2: Here the constant coefficient is set to zero, one can see that the roots that are close to each other get perturbed quite significantly. The Matlab code to reproduce the results can be downloaded here . EDIT 2: To address Fixed Point's questions: Since Brent's algorithm is guaranteed to converge (it could be slow but it will converge), I am curious as to why you were having the problem that you said you were. When developing the MIMO extension for ANP (animated nyquist diagram, a leisure project for educational purposes) I came to realize that the program would have to deal with high order polynomials even for small MIMO systems. I then noticed that Matlab's 'roots' would produce very inaccurate results when there were roots with high multiplicity present - even in trivial, obvious cases like $(x+1)^4=0$ (try roots(poly([-1,-1,-1,-1])) ). Even if my program should only be used for entertainment, that wasn't good enough. After finding Multroot (by Zeng) and unit-testing it with quite some success using randomized MIMO systems I found that besides some trivial to solve crashes it had a more severe flaw that had to do with a small constant term. How/why was your application crashing? One such polynomial can be defined in Matlab as follows (it's the one that finally lead me to this SE question): p = hex2num(['bfae7873980ada44';'bfd79794c0074ef6';'bfe9e4c737c98680';'bfe5502ed16afae0';'bf81513e302abba0';'3fc59ae0b4d97164';'bc80000000000000'])'; Use it as input to Multroot: multroot(p) Most likely it will end with an error saying that an output argument hasn't been assigned. (Beware that the algorithm uses randomized initial vectors and thus succeeds with a small chance) Was this MATLAB's fzero which was crashing? As explained I didn't use 'fzero' and unfortunately it can't help me here, as it says in the documentation that it needs a change of sign to detect a zero - which isn't the case for all roots of a general polynomial.","['polynomials', 'roots', 'calculus', 'continuity', 'numerical-methods']"
2361046,Set cardinality exercise mistake in a textbook?,"During studying set cardinality, I came across on this exercise: $$2{}^{\aleph_0}2 = {}^{\aleph_0}2 + {}^{\aleph_0}$$ I understand that ${}^{\aleph_0}2$ is just $2^{\aleph_0}$ written in this way, at least previous text implies it. But I am really confused about what $^{\aleph_0}$ is supposed to be as there is no explanation for it whatsoever. There is a solution to the excercise, but it uses the same notation and therefore I do not understand it, even if it's quite short. I think it's some typographical mistake, but having only brief background in cardinality, I can't figure out what the equation means. I bet it's either meant to be $2^{\aleph_0}$ or simply ${\aleph_0}$. Could you please find an explanation of this typo or post a meaningful version of the equation? P.S. The textbook is in Slovak, so I won't post the solution here. Though I will translate it if it's necessary.","['cardinals', 'notation', 'elementary-set-theory']"
2361071,Existence of a positive semidefinite matrix that satisfies a set of equality constraints,"Given vectors $a_1, b_2, a_2, b_2 \in \mathcal{R}^{n\times 1}$, I am interested in finding a positive semi-definite matrix $M \in \mathcal{R}^{n\times n}$, $M \succeq 0$, such that $M\cdot a_1 = b_1$, $M\cdot a_2 = b_2$. Here $n \gg 2$, say $n = 1000 $. $a_1, a_2$ are not parallel and are non-zero. To write it in equations, I want to solve the following semidefinite program \begin{equation*}
\begin{aligned}
& \underset{M}{\text{minimize}}
& & 0 \\
& \text{subject to}
& & M\cdot a_1 = b_1 \\
&& & M\cdot a_2 = b_2 \\
&&& M \succeq 0.
\end{aligned}
\end{equation*} Depending on the value of $a_1, b_2, a_2, b_2$, sometimes a numerical solver will report this program is infeasible (no such $M$ exists). I have experimented with multiple solvers with identical result. I can further impose that $a_1^T\cdot b_1>0, a_2^T\cdot b_2>0$, but the result is the same. An observation: If $a_1, a_2$ are orthogonal, it appears the problem is always feasible. My intuition is that the number of free variables in $M$ is $(n(n+1)/2 -n)$, because a symmetric matrix has $n(n+1)/2$ free variables, and positive semidefiniteness requires all principal minors to be positive, adding $n$ constraints. It appears this intuition is not correct. What is the requirement of $a_1, b_2, a_2, b_2$ for $M$ to exist?","['semidefinite-programming', 'convex-optimization', 'numerical-optimization', 'positive-semidefinite', 'linear-algebra']"
2361080,Is $ f : \mathbb{Q} \to \mathbb{Q}$ via $ \ f(x) = x^{5} -3$ surjective?,Question: Is $ f : \mathbb{Q} \to \mathbb{Q}$ via  $ \ f(x) = x^{5} -3$ surjective? My attempt: No. Consider $ y = 2$. There doesn't exist $ x \in \mathbb{Q} \ such \ that \ y = f(x)$. Am I correct? Is this the right approach to prove a function is not surjective?,"['elementary-set-theory', 'functions', 'elementary-number-theory']"
2361098,Balanced ternary: combinatorial sum,"I was working on the following problem: In balanced ternary, how many numbers with $2n$ digits can be expressed using the same number of $+1$ and $-1$ digits, where $n$ is an integer? After some combinatorial shenanigans, I ended up with this sum as my answer: $$\sum_{k=0}^{n-1} \binom{2n-1}{2k}\binom{2n-2k-1}{n-k}$$ But I cannot figure out how to evaluate it. It does not seem to telescope, and WA would not give me a formula. Does anybody know how to evaluate this sum?","['combinatorics', 'summation', 'binomial-coefficients']"
2361099,Does the tangent line really touch a single point?,"The principle of the theory of the tangent to a curve is that it is the limit of the secant when the two intersection points, one of them approaches the other, I mean in Calculus I. I intend to prove that for every function $f$ differentiable on an interval $I$ and a number $a$ from that interval, the equation : $f’(a)(x-a)+f(a)=f(x)$ accepts $a$ as a double solution. Can I? Or prove that their intersection is a double point.","['derivatives', 'graphing-functions', 'calculus', 'geometry']"
2361126,Strong convexity and the Legendre transform,"Suppose that I have a strongly convex function $f(\mathbf{x}): \mathbb{R}^m \rightarrow \mathbb{R}$. Is the Legendre transform of this function also strongly convex? As far as I can tell, strict convexity of $f$ implies strict convexity of the Legendre transform of $f$ (as shown in https://proofwiki.org/wiki/Convexity_of_Function_implies_Convexity_of_its_Legendre_Transform ). However, I am unsure as to whether the same argument holds for strongly convex $f$. EDIT: The following link appears to support this claim for the case of $m =1$. https://books.google.ca/books?id=HjznBwAAQBAJ&pg=PA86&lpg=PA86&dq=linear+programming+vanderbei+strongly+convex+legendre+transform&source=bl&ots=mBLOVV7_a0&sig=xMI0kBz7bHdN4kkEnvQQ87LGkhk&hl=en&sa=X&ved=0ahUKEwiayNCfupXVAhVFeSYKHQI_Dl8Q6AEIJjAA#v=onepage&q=linear%20programming%20vanderbei%20strongly%20convex%20legendre%20transform&f=false","['transformation', 'convex-analysis', 'analysis', 'inner-products']"
2361140,how to compute $\int_{0}^{\infty} \frac{dx}{x^{2017}+1}$,"How do I simplify this following integral
$$\int_{0}^{\infty} \frac{dx}{x^{2017}+1}$$ I think the answer is
 $\frac{1}{2017}\pi \csc\left(\frac{\pi}{2017}\right)$ which is really close to one, what is the best method to solve this ?  I was trying compute it with residue integral but I noticed this integral does have a massive poles. Any ideas?","['integration', 'calculus']"
2361178,How to obtain the square root of the transition matrix of Markov,"Suppose 
$$Q = \{q_{ij}\}$$
with $\sum\limits_j q_{ij} = 1,\quad |q_{ij}|\leq1$
is the transition matrix of a Markov chain. Then how to find the square root of $Q,$ namely
$$R^2 = Q.$$
One way I used is using the Taylor expansion of $\sqrt{x},$ since every entry of $Q$ is smaller than $1,$ then $Q^n$ should be convergent. But is there any better way to computer the square root of a matrix?","['matrices', 'markov-chains', 'linear-algebra']"
2361180,Modeling population growth with variable rate in a differential equation,If you consider the classic differential equation used in textbooks to model animal population growth and forget adding in the carrying capacity terms: $dP/dt$ = rP It would seem that the above DE would benefit from some variable rate r instead of some fixed rate r in order to achieve a more accurate model. I tried googling around for this and I'm guessing I simply wasn't using the best keywords. I'm wondering if someone could give an example of a DE where a variable rate r is used and how one usually solves such an equation. Thanks,['ordinary-differential-equations']
2361251,When is Pinsker’s inequality tight?,"Let $P(x)$ and $Q(x)$ be two distributions of $\mathbf{x}$. Pinsker’s inequality says $$D(P(x)\|Q(x)) \geq \frac{1}{2 \ln 2} \|P(x) - Q(x)\|_1^2,$$ where $\|\cdot\|_1$ is the $\mathcal{L}^1$ norm, and $D(P(x)\|Q(x))$ is the KL-divergence $$D(P(x)\|Q(x)) = \int_{\mathcal{X}} P(x)\log_2 \frac{P(x)}{Q(x)} dx.$$ Given that $\int x^2 P(x) dx \neq \int x^2 Q(x) dx$ (Second moment constraint). My question is when Pinsker’s inequality is tight. Thanks! PS: My previous question is without the second moment contraint. In that case, $P = Q$ (a.e.) can make Pinsker's inequality tight.","['probability-theory', 'statistics', 'hypothesis-testing']"
2361261,Inequalities (AMGM & Cauchy Schwarz),"Let $ x, y, z \geq 0 $ such that $x+y+z=1$. Find the maximum value of
  $$x (x+y)^2 (y+z)^3 (z+x)^4.$$ Hi recently I've been stuck on this problem for quite some time, and I would appreciate some help/hints on how to approach this inequality. Also, would be good if the approaches were based on AM-GM or Cauchy Schwarz. Thanks!","['inequality', 'a.m.-g.m.-inequality', 'optimization', 'multivariable-calculus', 'maxima-minima']"
2361346,"If $E$ is non-empty and bounded above, then $\sup E \in \overline E$","If $E \subset \mathbb{R}$ is non-empty and bounded above, then $\sup E \in \overline E$ (the closure of $E$) This is a theorem in Rudin's real analysis (theorem 2.28,p35) but I would like to know under what circumstances it holds. The proof provided is the following: Write $y := \sup E$, if $y \in E$, then $y \in \overline E$. If $y \notin E$, then for every $h >0$, there exists $x \in E$ such that $y-h < x < y$. Thus $y$ is a limit point of $E$ and $y \in \overline E \quad \triangle$ Now, I understand that this theorem is true for the Euclidean distance function (i.e. $d(x,y):= |y - x|)$, but does the theorem hold for all other metrics we can define on $\mathbb{R}$ as well? My guess would be no, since I was unable to prove this so I think Rudin proved this theorem thinking about the Euclidean metric.","['general-topology', 'real-analysis', 'supremum-and-infimum']"
2361375,How badly-behaved are the derivatives of non-analytic smooth functions?,"Suppose $f:\mathbb{R} \to \mathbb{R}$ is a smooth function such that $f^{(n)}(0) = 0$ for all $n \in \mathbb{N}_{\geq 0}$ and that $f$ is not analytic. In particular, we assume that $f$ is not identically $0$ in any neighbourhood of $x=0$. Does it follow that for all $\epsilon>0$ $$\lim_{n \to +\infty} \sup\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = +\infty$$ and $$\lim_{n \to +\infty} \inf\{f^{(n)}(x) : x \in (-\epsilon, \epsilon)\} = -\infty?$$ Motivation: the standard examples we all know and love [e.g., $\exp (-\frac{1}{|x|}), \exp\left(-\frac{1}{x^2}\right)$] have derivatives which exhibit extreme oscillatory behaviour near the origin when $n$ gets large. For these functions, this makes sense intuitively. To ensure the function ""smoothly"" and ""flatly"" reaches $x=0$, the first derivative needs to rapidly become small in magnitude, which is only possible if the second derivative temporarily becomes large in magnitude, but then it's necessary for the second derivative to rapidly become small again (since $f^{(2)}(0) = 0$), which means that the third derivative has to do some work, and you can see a pattern developing.","['derivatives', 'real-analysis', 'analyticity']"
2361377,Extrinsic curvature of plane curve,"I asked this question first on physics forum, but there is no response. I post it here and hope someone can help me out. I know two kinds formulas to calculate extrinsic curvature. But I found they do not match. One is from ""Calculus: An Intuitive and Physical Approach""$K=\frac{d\phi}{ds}$ where $\Delta \phi$is the change in direction and $Δs$ is the change in length. For parametric form curve $(x(t),y(t))$ the extrinsic curvature is given by
$$ K=\frac{x'y''−x''y'}{(x'^2+y'^2)^{3/2}} $$
where $′≡\frac{d}{dt}$ The extrinsic curvature formula in general relativity from ""Eric Possion, A Relativist's Toolkit"" is given by $K=∇_{\alpha}n^{\alpha}$. For a plane curve $(x(t),y(t))$ in flat space, the outgoing unit normal vector is $(n^x,n^y)=(\frac{y′}{\sqrt{x'^2+y'^2}},\frac{-x′}{\sqrt{x'^2+y'^2}})$, and the extrinsic curvature is ( my interpretation of Possion's formula )
$$K=\frac{\partial}{\partial x}n^x+\frac{\partial}{\partial y}n^y=\frac{1}{x'}\frac{\partial}{\partial t}n^x+\frac{1}{y'}\frac{\partial}{\partial t}n^y=2\frac{x'y''−x''y'}{(x'^2+y'^2)^{3/2}}$$ Is there anything wrong here? Thanks in advance!","['general-relativity', 'differential-geometry', 'calculus']"
2361389,"Evaluate $\frac{\Gamma(n/2)}{\Gamma(n/2+h)},$ $n,h \in \mathbb{N}$","Suppose $n,h \in \mathbb{N}$. How can I evaluate $$\frac{\Gamma(n/2)}{\Gamma(n/2+h)},$$
where $\Gamma$ is the Euler gamma function. All I know is that $$\Gamma(n+1) = n!$$","['real-analysis', 'integration', 'gamma-function', 'calculus']"
2361393,Is the first fundamental form the outer product of $\nabla s$ for a surface $s$?,"Take two $2$-dimensional vectors $\boldsymbol a:=(a_1, a_2)$ and $\boldsymbol b := (b_1, b_2)$. The outer product of $\boldsymbol a$ and $\boldsymbol b$ is $$
\begin{bmatrix}
    a_1b_1 & a_1b_2\\
    a_2b_1 & a_2b_2 \\
\end{bmatrix}.
$$ Now take a surface $S\subseteq\textbf{R}^3$ parametrized by $\boldsymbol s(u,v)$. The first fundamental form of $\boldsymbol s(u,v)$ is $$
\begin{bmatrix}
    \boldsymbol s_u \cdotp \boldsymbol s_u & \boldsymbol s_u \cdotp \boldsymbol s_v\\
    \boldsymbol s_v \cdotp \boldsymbol s_u & \boldsymbol s_v \cdotp \boldsymbol s_v \\
\end{bmatrix},
$$ which looks suspiciously like the outer product of $\nabla\boldsymbol s := (\boldsymbol s_u, \boldsymbol s_v)$ with itself.
(The dot $\cdotp$ stands for dot product of vectors.) Does this make sense? Does it generalize to other settings, like higher dimensions, or surfaces not in parametric form?","['multivariable-calculus', 'differential-geometry', 'linear-algebra']"
2361398,How to prove that a length is equal to the inradius of a triangle,$D$ is the midpoint of the side $BC$ of the triangle $ABC$. The line joining $D$ and the incentre $I$ of the triangle intersects altitude $AA'$ at the point $P$. Prove that the length of $AP$ is equal to the radius of the incircle of the triangle..,"['trigonometry', 'euclidean-geometry', 'triangles', 'geometry', 'fractions']"
2361415,Show that $y = \frac{2x}{x^2 +1}$ lies between $-1$ and $1$ inclusive.,"Prove, using an algebraic method,that $y=\frac{2x}{x^2 +1}$ lies between $-1$ and $1$ inclusive. Hence, determine the minimum and maximum points $y=\frac{2x}{x^2 +1}$ . What I tried: Firstly, I thought of using partial fractions but since $x^2 +1 =(x-i)(x+i)$, I don't think it is possible to show using partial fractions. Secondly, decided to use differentiation $y=\frac{2x}{x^2 +1}$ $\frac {dy}{dx} = \frac {-2(x+1)(x-1)}{(x^2 +1)^2 }$ For stationary points: $\frac {dy}{dx} = 0$ $\frac {-2(x+1)(x-1)}{(x^2 +1)^2 } = 0$ $x=-1$ or $x=1$ When $x=-1,y=-1$ When $x=1,y=1$ Therefore, this implies that $y=\frac{2x}{x^2 +1}$ lies between $-1$ and $1$ inclusive. ^I wonder if this is the correct method or did I leave out something? The third way was using discriminant Assume that $y=\frac{2x}{x^2 +1}$ intersects with $y=-1$ and $y=1$ For $\frac{2x}{x^2 +1} = 1$, $x^2 -2x+1 = 0$ Discriminant = $ (-2)^2  -4(1)(1) = 0 $ For $\frac{2x}{x^2 +1} = -1$, $x^2 +2x+1 = 0$ Discriminant = $ (2)^2  -4(1)(1) = 0 $ So, since $y=\frac{2x}{x^2 +1}$ touches $y=-1$ and $y=1$, $y=\frac{2x}{x^2 +1}$ lies between $-1$ and $1$ inclusive. Is the methods listed correct?Is there any other ways to do it?","['derivatives', 'inequality', 'calculus', 'algebra-precalculus', 'absolute-value']"
2361447,"let $a_1,a_2,. . . ,a_n \in \mathbb{R^+} $ then prove that :","Let $a_1,a_2,. . . ,a_n \in \mathbb{R^+} $. Prove that :
  $$\frac{ a_{1} }{ a_{2} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}}
+\frac{ a_{2} }{ a_{1} ^{2}+ a_{3} ^{2}+\cdots+ a_{n} ^{2}}
+\cdots+\frac{ a_{n} }{ a_{1} ^{2}+ a_{2} ^{2}+\cdots+ a_{n-1} ^{2}}
 \geq \frac{ 4}{ a_{1}+ a_{2} + a_{3}+\cdots+ a_{n}}$$ I do not know where to start. please help me .","['inequality', 'cauchy-schwarz-inequality', 'convex-analysis', 'calculus', 'multivariable-calculus']"
2361474,Proof of Lemma 8.5.14 in Terence Tao Analysis I,"Lemma 8.5.14. Let X be a partially ordered set with ordering relation $\leq$, and let $x_0$ be an element of $X$. Then there is a well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element, and which has no strict upper bound. Proof. The intuition behind this lemma is that one is trying to perform the following algorithm: we initalize $Y:=\{x_0\}$. If $Y$ has no strict upper bound, then we are done; otherwise, we choose a strict upper bound and add it to $Y$ . Then we look again to see if $Y$ has a strict upper bound or not. If not, we are done; otherwise we choose another strict upper bound and add it to $Y$ . We continue this algorithm “infinitely often” until we exhaust all the strict upper bounds; the axiom of choice comes in because infinitely many choices are involved. This is however not a rigorous proof because it is quite difficult to precisely pin down what it means to perform an algorithm “infinitely often”. Instead, what we will do is that we will isolate a collection of “partially completed” sets $Y$, which we shall call good sets, and then take the union of all these good sets to obtain a “completed” object $Y_{\infty}$ which will indeed have no strict upper bound. We now begin the rigorous proof. Suppose for sake of contradiction that every well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element has at least one strict upper bound. Using the axiom of choice (in the form of Proposition 8.4.7), we can thus assign a strict upper bound $s(Y)\in X $ to each well-ordered subset $Y$ of $X$ which has $x_0$ as its minimal element. Let us define a special class of subsets $Y$ of $X$. We say that a subset $Y$ of $X$ is good iff it is well-ordered, contains $x_0$ as its minimal element, and obeys the property that $x=s\left(\{y\in Y:y<x\}\right)$ for all $x \in Y\backslash \{x_0\}$. Note that if $x \in Y\backslash \{x_0\}$ then the set $\{y \in Y :y<x\}$ is a subset of $X$ which is well-ordered and contains $x_0$ as its minimal element. Let $\Omega:=\{Y \subseteq X: Y\, \text{is good}\}$ be the collection of all good subsets of $X$. This collection is not empty, since the subset $\{x_0\}$ of $X$ is clearly good (why?). We make the following important observation: if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. In particular, given any two good sets $Y$ and $Y^\prime$, at least one of $Y^{\prime}\backslash Y$ and $Y \backslash Y^{\prime}$ must be empty (since they are both strict upper bounds of each other). In other words, $\Omega$ is totally ordered by set inclusion: given any two good sets $Y$ and $Y^\prime$, either $Y \subseteq Y^\prime$ or $Y^\prime \subseteq Y$. Can anyone help me to understand ""if $Y$ and $Y^\prime$ are two good subsets of $X$, then every element of $Y^{\prime}\backslash Y$ is a strict upper bound for $Y$ , and every element of $Y\backslash Y^{\prime}$ is a strict upper bound for $Y^{\prime}$. """,['real-analysis']
2361477,Confidence interval interpretation difficulty,"I have seen a lot of questions in this forum related to what my question is, but I didn't find any convincing answer. So I would to like to put this question: When we are dealing with 95% confidence interval we mean that if we repeat process of collecting samples of same size and calculate 95% intervals for those samples then 95% of those intervals will contain the true population parameter. Let the infinite number of intervals be represented by 100 for simplicity. Then 95 of these intervals will contain true population parameter. Suppose we got an interval at the starting of the above process (L,U). 
Then if I ask what is the probability that this interval (L,U) contains the true population parameter then shouldn't it be 95/100 = 0.95? (Because this interval (L,U) can be anyone of 100 and it would contain true population parameter of its one of those 95). But this interpretation of confidence interval is considered incorrect. Can someone explain me why is this so?","['probability-theory', 'probability', 'confidence-interval']"
2361479,Connection matrix for a vector bundle,"Let $E \rightarrow M$ be a vector bundle. 
In the book Differential Analysis on Complex Manifolds by R.O. Wells a connection on E is defined to be a map a linear map
$D: \Omega(E) \rightarrow \Omega^1(E)$ such that $D(\phi s) = d\phi\otimes s + \phi Ds$. Now the book goes to choose a local basis of sections $f = (e_1,\ldots ,e_n)$(a frame) over U and shows how one can define a connection matrix with respect to a frame. It then shows if one changes the frame with a mapping $g : U \rightarrow GL(n)$ how the connection matrix changes. For example if A is the connection matrix one finds $A(fg)=g^{−1}dg+g^{-1}A(f)g$ and defining the curvature as $F(f)=A(f) \wedge A(f)+dA(f)$ one finds $F(fg)=g^{-1}F(f)g$. I noticed that if I considerd only transformaions $g: U \rightarrow G$  where $G$ is a matrix group and if $A(f)$ initially lies in the Lie Algebra of G then after a transformation $A(fg)=g^{−1}dg+g^{-1}A(f)g$ would also be in the Lie Algebra since the first term is the maurer cartan form and the second one is the adjoint representation. My question is if there is some sort of name or notion for only considering connections on a vector bundle such that the connection matrix lies in the Lie-algebra of a subgroup of $GL(n)$  ?  Does this have to do with structure groups ?",['differential-geometry']
2361499,"$x^7+y^7+z^7$ is divisible by $7^3$, then $x+y+z$ is divisible by $7^2$","Let $x, y, z$ be positive integers, and $7 \nmid xyz$. If $7^3|x^7+y^7+z^7$, show that $7^2|x+y+z$. by Fermat's little theorem, $x^7 \equiv x \pmod7$, then $x^7+y^7+z^7\equiv x+y+z \equiv 0 $ (mod 7) so we have $7 | (x+y+z)$.
what should I do next?","['number-theory', 'polynomials', 'integers', 'divisibility']"
2361515,Are small balls in a metric space quasi-symmetric to a compact ball connected?,"I'm trying to solve a problem but I'm stuck. Maybe someone can help me. I will denote every metric on every metric space by $d$ and I will use closed balls, denoting them with the letter $B$, so for $x\in X$ and $r>0$
$$B(x,r)\colon=\lbrace y\in X\:\colon \: d(x,y)\leq r\rbrace.$$ To the problem:
let $X$ be a metric space and let $V$ be a compact ball inside a Carnot group $G$, endowed with the usual CC metric. Assume that there exists a quasi-symmetric homeomorphism $f\colon V\longrightarrow X$. Question: Are small enough balls in $X$ connected? That is, does there exists a constant $K>0$ such that all balls of radius $\leq K$ in $X$ are connected? Intuitively I think that the answer should be yes: $V$ is a closed ball inside a Carnot group and therefore is compact, connected, simply connected i.e. very nice. $X$ not only is homeomorphic to $V$ but it's also quasi-symmetrically equivalent to it. It cannot be that ugly, right? All the ""bad cases"" I can think of do not have compact boundary. I tried using that quasi-symmetric maps on uniformly perfect spaces are $\alpha$-Hölder continuous for some $\alpha\in (0,1]$: $\textbf{Lemma:}$ Suppose that $X$ is a bounded and uniformly perfect metric space. Let $f\colon X\longrightarrow Y$ be a quasisymmetric homeomorphism. Then there are constants $A,B\geq 1$ and $\alpha\in (0,1]$ such that 
$$ \frac{1}{A}d(x,y)^{1/\alpha}\leq d(f(x), f(y))\leq Bd(x,y)^\alpha.$$ Since connected spaces are uniformly perfect we can apply this to our situation to obtain that for every closed ball $B(x,r)$ there is a connected set $U$ and a constant $D$ such that
$$ B(x,\frac{1}{D}r^{1/\alpha^2}) \subset U\subset B(x,r).$$ 
I'm not sure this is the right approach as I can't make any progress from here. I'm aware that since the metric space is compact, it would be enough to show that the closure of each open ball is the closed ball with the same radius, in order to say that balls are connected. But I couldn't show this and I don't even expect every ball to be connected.. Does anyone have other ideas that I can try? Thank you very much!","['metric-spaces', 'differential-geometry', 'metric-geometry']"
2361530,Folding a corner on a paper to the opposite side,"If we bend the $\text{A4}$ paper by connecting two opposite corners, what length is the $\color{red}{\text{bend}}$? I drew the two blue lines on the last image, which are sides of a rhombus. By the help of that rhombus, I managed to calculate the bend as the length of the shorter diagonal: $$\frac ab\sqrt{a^2+b^2}$$ Taking the length of paper sides as $1$ and $\sqrt2$ , the bend is then $\frac{\sqrt3}{\sqrt2}=\frac{\sqrt6}{2}$. But what if we bend the point not to the opposite corner, but to the
  some point $T$ on the oppoiste side of the rectangle with sides $a,b$ where that point is $c$ units away from the bottom left corner? When $c=0$, we get $B=\frac ab\sqrt{a^2+b^2}$ as the above case. When $c=b$, we get $B=b$, as you are just folding the paper then. But how do we solve for the $B$ in general over some $a,b,c$? We can also extend the line of the bottom side and put our point $T$ outside of the paper, as long as the bottom left point is in the same place; and consider these cases too.","['recreational-mathematics', 'euclidean-geometry', 'geometry']"
2361540,Full rank condition when stacking vector valued function,"Let $f: \mathbb{R}^1 \mapsto \mathbb{R}^n$ be a smooth vector-valued function. Consider 
the $n \times n$ matrix $A(x)$ obtained from a vector $x \in \mathbb{R}^n$ by appropriately stacking 
$[f(x_1),\dots, f(x_n)]$. The question is: what are easily verifiable sufficient conditions which guarantee
that the matrix $A$ is generically of full rank, that is, $A(x)$ is full rank except possibly for points $x$ in sets of measure zero? To fix ideas, obviously $f$ cannot be linear, otherwise there is linear dependence in $A$.","['matrix-rank', 'linear-algebra', 'vectors', 'vector-analysis']"
2361618,How to calculate per unit cost?,"I need help understanding this math problem from my TEAS Mometrix Practice exam. It gives an explanation but I still don't understand. The question is: Mandy can buy $4$ containers of yogurt and $3$ boxes of crackers for $\$9.55$. She can buy $2$ containers of yogurt and $2$ boxes of crackers for $\$5.90$. How much does one box of crackers cost? The answer is $ \$2.25$. I understand that I have to set up the problem like this:
\begin{align}
4x+3y &= 9.55 \\
2x+2y &= 5.90
\end{align} but the explanation says that I have to multiply the bottom equation by $-2$; therefore, it becomes $-4x+ -4y= -11.80$ and then  I have to add the equations 
\begin{align}
4x+3y   &= 9.55\\
-4x+-4y &= -11.80
\end{align} by top to bottom getting  either $\pm 2.25$ Can you explain why this is the answer? And also why does one have multiply by -2?",['algebra-precalculus']
2361652,What am I doing wrong in solving this differential equation?,"let $g$ be a function of $t$ $$e^{3t}g'(t)+3e^{3t}g(t)=2e^{6t}$$ Now, Divide each side by $e^{3t}$ and then multiply by the integrating factor $\mu(t)$ $$ \mu g'(t)+3\mu g(t) = 2 e^{3t} \mu$$ Now our aim is to make $\frac{d}{dt}(\mu g) = LHS$ of the equation, to satisfy this we end up with : $$\mu'(t)= 3 \mu$$ Solving for $\mu$ using separation $$\mu = ce^{3t}$$ for this case we can say $c=1$. Then The equation is now looks like this : $$\frac{d}{dt}( \mu g) = 2e^{6t} \\ e^{3t}g = \frac{1}{3}e^{6t}+c \\ g= \frac{e^{3t}}{3}+ce^{-3t}$$ But the answer sheet says that $$g= 2te^{3t}+ce^{3t}$$ Is my answer wrong? Or can they both be true? If they can both be true why is this case? How do you think book has solved this question?",['ordinary-differential-equations']
2361657,Give example of two non-isomorphic groups that have same as sets.,Does there exists two non-isomorphic groups with the following properties: 1.Two groups are same as sets. 2.Two groups have the same identity element. 3.Each element has same inverse in each of the groups. I have done an exercise on isomorphism chapter of J.Gallian's book where all the criteria are fulfilled but they are isomorphic.But is there a non-isomorphic example?,"['abstract-algebra', 'group-theory']"
2361692,Uses of the internal logic of a topos in algebraic geometry?,"I see why topoi arise naturally in AG, but I'm having a hard time finding examples, if any, of the application of their more logical side to geometric questions. I'm looking for examples of proofs in AG using the internal logic of topoi, or an explanation of why there aren't (m)any.","['topos-theory', 'algebraic-geometry']"
2361748,An amazing property of the Catenary,"I discovered that if we want an arc of catenary  in the interval $[a,b]$ we solve $$\int_a^b \sqrt{\cosh '(x)^2+1} \, dx=\int_a^b \cosh x \, dx$$ which means that the ""result"" of the length is equal to the result of the area in the same interval, though in different units. So I asked myself if there are any other curve with the same property. I set $$y=\sqrt{y'^2+1}\to y^2=y'^2+1; y(0)=1$$ then $$y'=\sqrt{y^2-1}\to dx=\frac{dy}{\sqrt{y^2-1}}\to x=\cosh^{-1} \,y$$ hence, arbitrary constant is zero, $y=\cosh x$ But I am not sure how to deal with the other solution $y'=-\sqrt{y^2-1}$ even if Mathematica gives the same result $y=\cosh x$ I'd like someone checking  this proof, you know: I am not a pro, I am just an (almost) retired high school teacher :) Update 9/1/2020. Now I am officially retired :)","['hyperbolic-functions', 'area', 'ordinary-differential-equations', 'arc-length']"
2361794,Definitions of a proper embedding,"I have seen the word ""proper"" refer to two seemingly different notions in various contexts. A continuous map is proper if preimages of compact sets are compact. A continuous map between manifolds $f:M \to N$ is proper if it respects boundaries, i.e. $f^{-1}(\partial N) = \partial M$. Is this just an unfortunate instance of inconsistent notation or do these notions coincide in some contexts?","['manifolds', 'general-topology']"
2361811,Why isn't this relation transitive?,"Here's a question from my e-text: By definition, a relation $R$ on $A$ is transitive if whenever $xRy$ and $yRz$, then $xRz$. That is, the logic is sort of like a ""chain"" or a hypothetical syllogism, if you will. So in the diagram above, you have $xQy$ and $yQx$, and you also have $xQx$, so why isn't this transitive? I don't quite understand their logic.","['relations', 'elementary-set-theory', 'discrete-mathematics']"
2361813,Strong induction $n=2^a\cdot b$ [duplicate],"This question already has answers here : Proof by Strong Induction: $n = 2^a b,\, b\,$ odd, every natural a product of an odd and a power of 2 (3 answers) Closed 6 years ago . Given $n\in\mathbb N$, there exists a non-negative integer $a$ and an odd integer $b$ such that $n=2^a\cdot b$ Base Case:
$n=1 =2^0\cdot1$ I.H: Assume its true for $n=1,2,\ldots,k$ How would i prove this?","['induction', 'proof-writing', 'discrete-mathematics']"
2361816,Why dense subsets are convenient to prove theorems,"Could you please explain the following concept (preferably by examples) about dense subsets: If you want to prove that every point in $A$ has a certain property that is preserved under limits, then it suffices to prove that every point in a dense subset $B$ of $A$ has that property. What are the examples of properties that are preserved under limits? Why is it sufficient to prove such properties for a dense subset $B\subseteq A\subseteq closure(B)$?",['real-analysis']
2361824,Sequence $a_n = a_{n - 1} + a_{\lfloor n/2 \rfloor}$,"Given a sequence with $a_1 = 1$. And $a_n = a_{n  - 1} + a_{\lfloor n/2 \rfloor}$ for $n \geq 2$, prove that $a_n$ is not divisible by 4 for any $n \in \mathbb{N}$. Don't have any conjectures.","['number-theory', 'sequences-and-series']"
2361836,What is a difference space?,"I've read this sentence on Wikipedia : The modern approach defines the three-dimensional Euclidean space more
  algebraically, via vector spaces and quadratic forms, namely, as an
  affine space whose difference space is a three-dimensional inner
  product space. But it doesn't define what a difference space , so I wonder, what is it?","['euclidean-geometry', 'geometry']"
2361853,$\int (x^2+1)/(x^4+1)\ dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I have Divided the numerator and Denominator by $x^2$
to get $\dfrac{1+x^{-2}}{x^2+x^{-2}}$ then changed it into $(1+(x^{-2}))/[(x-x^{-1})^2 +2]$ then took $x-(1/x)$ as $u$ and Differentiated it with respect to $x$ to get $dx=du/(1+x^{-2})$  Finally I got this expression: $$
\int\frac{x^2+1}{x^4+1} \, dx = \int (u^2+2)^{-1} \, du
$$ After this I need help!","['integration', 'polynomials', 'integration-by-parts', 'calculus']"
2361855,Why I can not solve this differential equation?,"I am asked to use the variation of parameters method to find the particular solution : $$t^2y'' - t(t+2)y' + (t+2)y = 6t^3 \\ t>0 \\ y_1(t) =t \\ y_2(t) = te^t$$ Where $y_i$ are the solutions of the homogenous one. I am using method of variation of parameters so I have the following formulas in my mind : $$u_1' = \frac{-y_2g}{W(y_1,y_2)} \ u_2' = \frac{y_1 g}{W(y_1,y_2)}$$ where $W(y_1,y_2)$ is the wronskian. Then clearly $u_1 = -6t + c_1 \ u_2 = -6e^{-t} + c_2$  So the general solution is : $$y= (-6t+c_1)(t) + (-6te^{-t}+c_2)(te^t) \\ = tc_1+te^{t}c_2-6t^2-6t$$ So I can take $-6t^2-6t$ as the particular solution. However answer sheet says that the answer is $-6t^2$ I kept checking my calculation mistakes on and on and on but couldn't find any? What am I doing wrong?",['ordinary-differential-equations']
2361875,Evaluating $\psi^{(1/2)}(x)$ of the extended polygamma function,"The polygamma function is generally given by $$\psi^{(n)}(x)=\frac{d^{n+1}}{dx^{n+1}}\ln(\Gamma(x)),~n\in\mathbb N_{\ge0}$$ where $\Gamma$ is the gamma function .  This can be extended to negative integers by letting $$\psi^{(n-1)}(x)=\int_a^x\psi^{(n)}(x)$$ Unfortunately, there is no fixed $a$ such that the above holds true for any $n$, but it does capture the general idea. Using fractional calculus , one can extend the polygamma function to complex values.  Here, I could have $$\psi^{(z)}_u(x)=\frac1{\Gamma(1-\{a\}-bi)}\frac{d^{\lceil a\rceil+2}}{dx^{\lceil a\rceil+2}}\int_u^x\frac{\ln(\Gamma(t))}{(x-t)^{\{a\}+bi}}~\mathrm dt$$ where $z=a+bi$, $a>-1$, $\lceil a\rceil$ is the ceiling function, and $\{a\}$ is the fractional part of $a$.  $u$ could be any constant such that the integral converges.  For $z\in\mathbb R^+$, the above expression reduces down to the normal polygamma function regardless of $u$ (at least for $x>0$). For $a<-1$, $$\psi^{(z)}_u(x)=\frac1{\Gamma(z)}\int_u^x(x-t)^{z-1}\ln(\Gamma(t))~\mathrm dt$$ Particularly of interest, I'm trying to find the closed form of $\psi^{(1/2)}_u(x)$. $$\psi^{(1/2)}_u(x)=\frac1{\sqrt\pi}\frac{d^3}{dx^3}\int_u^x\frac{\ln(\Gamma(t))}{\sqrt{x-t}}~\mathrm dt$$ By applying Leibniz rule , this reduces down to $$\frac{d}{dx}\int_u^x\frac{\ln(\Gamma(t))}{\sqrt{x-t}}~\mathrm dt=\lim_{y\to x}\left[\frac{\ln(\Gamma(y))}{\sqrt{x-y}}-\frac12\int_u^y\frac{\ln(\Gamma(t))}{(x-t)^{3/2}}~\mathrm dt\right]$$ However, I can't figure out how to simplify this, if possible. Trying to attack the original integral, then differentiating the result, I'm tempted to use this series expansion , but integrating term by term of that expression is quite nasty and the resulting series looks quite doubtful from anywhere near a closed form. Also unsure if this is any help, but if we appropriately define $\sqrt z=|z|^{1/2}e^{i\theta/2}$, where $\theta\in[0,2\pi)$, one could attempt a keyhole contour of radius $u$ centered at $x$ to deduce that (hopefully this is correct) $$\int_u^x\frac{\ln(\Gamma(t))}{\sqrt{x-t}}~\mathrm dt=\frac12\int_\pi^{3\pi}\frac{\Gamma(x+ue^{i\theta})}{\sqrt{-ue^{i\theta}}}~d\theta$$ But again, doesn't seem much helpful","['polygamma', 'fractional-calculus', 'integration', 'gamma-function']"
2361889,Graphically Organizing the Interrelationships of Basic Algebraic Structures,"I have never taken a formal course in Abstract Algebra (yet), but I am interested in learning more about the subject, as I know it is extremely important in Modern Mathematics and a powerful tool beyond (like in, say, Physics). However, in my limited exposure to the subject, I have found the taxonomy of the various Algebraic Structures very difficult to follow. For example, in basic Group Theory, there are groups, but also semigroups, monoids, semilattices, quasigroups, loops, and Abelian groups. I have no difficulty in understanding the definitions of each, but understanding (or maybe more accurately, remembering ) the interrelationships between them is confusing and difficult. I suppose this is largely because many of the names of these structures (like many names in math, unfortunately) are very non-descriptive. I have seen some authors create a flowchart-like graphic to help illustrate these relationships (e.g. a monoid is a semigroup with identity, or equivalently a group without inverses, etc.), and while helpful, they are always rather limited in scope and always deal with a linear progression of the structure hierarchy and do not include structures that ""branch out"" so to speak from the main hierarchy (e.g. quasigroups and semilattices which are special cases of magmas and semigroups respectively, but have no direct inclusive relationship to, say, monoids) So my question is: Are there any robust graphics illustrating the interrelationships between the various algebraic structures? Personally, I'm mainly interested in such graphics for Groups and Rings (especially Rings), but I leave this question open to answers for more advanced structures too (like modules).","['abstract-algebra', 'visualization', 'ring-theory', 'group-theory', 'definition']"
2361933,Shortest way to answer probability / game question?,"Your game piece is on the starting space, and the finish is 10 more spaces away. During each of your turns, you roll a fair six-sided die, and move that many spaces forward. However, if the result would put you past the finish, you do not move. What is the probability that you will reach the finish in 3 or fewer turns? What is the shortest way to get this answer? I know the answer is 2/9, but curious what is the most efficient method of answering this? EDIT: The question (and the given answer / method to obtain in a long way can be found at this link: Check out this problem I found on Brilliant! https://brilliant.org/practice/conditional-probability-casework-calculations/?problem=discrete-mathematics-problem-109432&chapter=conditional-probability-2 I would have simply copied and pasted the answer but couldn't get the graphics to display properly and it will be better displayed in the associated link.","['statistics', 'probability', 'calculus']"
2361937,"Methodologies to Evaluate $\lim_{L\to \infty}\int_0^\infty \frac{\sin(Lx)}{x}\cos(x^3/3)\,dx$","In This Answer , I wrote ""It is straightforward to show that $\displaystyle \lim_{L\to \infty}\int_0^\infty \frac{\sin(Lx)}{x}\,\cos(x^3/3)\,dx=\frac\pi2$."" For completeness, I've included the ""straightforward approach"" that I had in mind in that which now follows. First, for any $\nu>0$ we can write $$\begin{align}
\int_0^\infty \frac{\sin(Lx)}{x}\,\cos(x^3/3)\,dx-\frac\pi2&=\int_0^{\nu} \frac{\sin(Lx)}{x}\,\left(\cos(x^3/3)-1\right)\,dx\\\\
&+\int_{\nu}^\infty \frac{\sin(Lx)}{x}\,(\cos(x^3/3)-1)\,dx\tag1\\\\
&=\int_0^{\nu L} \frac{\sin(x)}{x}\,\left(\cos\left(\frac{x^3}{3L^3}\right)-1\right)\,dx\\\\
&+\int_{\nu L}^\infty \frac{\sin(x)}{x}\,\left(\cos\left(\frac{x^3}{3L^3}\right)-1\right)\,dx 
\end{align}$$ The second integral on the right-hand side of $(1)$ converges as an improper Riemann integral, which can be shown be integration by parts with $u=\frac{\sin(Lx)}{x^3}$ and $v=\sin(x^3/3)$.  And integration by parts with $u=\frac{\cos(x^3/3)}{x}$ and $v=\frac{\cos(Lx)}{L}$ facilitates showing that the limit as $L\to \infty$ is $0$. Second, given $\epsilon>0$, there exists a $\delta>0$ such that $|\cos(x^3/3L^3)-1|<\epsilon$ whenever $|x|<\delta$.  We take $\nu <\min(\delta, (3\pi)^{1/3})$.  Since $\cos(x^3/3L^3)-1$ is decreasing for $x\in [0,\nu L]$, the second mean value theorem guarantees that there exists a number $\xi \in (0,\nu L)$ such that $$\begin{align}
\left|\int_0^{\nu L} \frac{\sin(x)}{x}\,\left(\cos\left(\frac{x^3}{3L^3}\right)-1\right)\,dx\right|&=\left|\left(\cos\left(\frac{\nu^3}{3}\right)-1\right)\right|\,\left|\int_\xi^{\nu L}\frac{\sin(x)}{x}\,dx\right|\\\\
&<\epsilon \pi/2
\end{align}$$ whence we see that $\lim_{L\to \infty}\int_0^{\nu L} \frac{\sin(x)}{x}\,\left(\cos\left(\frac{x^3}{3L^3}\right)-1\right)\,dx=0$.  And we are done! While the given approach is a standard one, I am interested in seeing ""better, stronger, faster"" approaches.  For example, direct use of the Dominated Convergence theorem to $\int_0^\infty \frac{\sin(x)}{x}\cos(x^3/3L^3)\,dx$ doesn't seem to apply here.  The Riemann-Lebesgue Lemma fails since $\frac{\cos(x^3/3)}{x}$ is not $L^1$.  The function $\cos(x^3/3)$ does not have compact support on $\mathbb{R}$.  And integration by parts schemes haven't appeared to be illuminating. So, is there a ""better, stronger, faster"" approach?","['improper-integrals', 'integration', 'definite-integrals', 'limits']"
2361950,Prove that there exists no choice of parameters such that this expression is an integer,"I wish to prove that the expression
$$ \frac{3^{0}2^{q_0}+3^{1}2^{q_1}+...+3^{k-2}2^{q_{k-2}}+3^{k-1}}{2^{q}-3^k}$$
Can be no positive integer other than 1 for any choice of positive integers $k$ and $q> q_0>q_1>...>q_{k-2}>0$ (note this incidentally imposes $q\ge k$). For the simple cases of $k=1$ and $k=2$, the result is easy to show. At $k=1$, the numerator is simply $1$, so it is trivial that we cannot obtain an integer other than 1. At $k=2$, the numerator is $2^{q_0}+3$, which is prime for $0<q_0<5$, so we need only consider $q \ge 6$; the inequality $2^q-9 > 2^{q-1}+3 \ge 2^{q_0}+3$, valid for $q>4$, then gives the result. I'd like to generalize this to all positive integer $k$, but I don't see a good way to do this inductively. One can relate a numerator at a given $k$, call it $n_k$, to the numerator at $k-1$ with the choice of $q_0$ through $q_{k-3}$ each less by $q_{k-2}$, call it $n_{k-1}$, as $n_k=2^{q_{k-2}}n_{k-1}+3^{k-1}$, but I don't see a good way to use this for an inductive argument. Edit: Note that, if we assume the expression is an integer, call it p, for some choice of parameters as above, then we also have the result $$p=\frac{2^q+3^k}{2^q+3^k}p = \frac{3^{0}2^{q_0+q}+...+3^{k-1}2^q+3^k*2^{q_0}+...+3^{2k-2}2^{q_{k-2}}+3^{2k-1}}{2^{2q}-3^{2k}}$$ Which is an instance of the original expression with $k \rightarrow 2k$ and a new set of $q$'s which satisfy the constraints. We've therefore shown that if there exists a choice of parameters such that the expression is a positive integer other than 1 at some value of $k$, then there also exists a choice of parameters yielding the same integer at $2k$ (incidentally, it's also fairly straightforward to see that one can use the factorization of $2^{qn}-3^{kn}$ to get a similar result for $k \rightarrow nk$ for any positive integer $n$). For this reason, the desired proof is equivalent to showing there exists no working choice of $q$'s for $k$ sufficiently large. That is, it is only necessary to show that $\exists K \in \Bbb{N}$ s.t. there is no working choice of $q$'s $\forall k>K$.","['number-theory', 'abstract-algebra']"
2361965,Tensor product and the pure elements,"From my understanding, one important thing about tensor product is that not all elements are pure elements, that is in $M\otimes N$ not all elements are in the form of $m\otimes n$ ,they are linear combinations of pure elements. My professor mentioned that while doing a problem thinking the tensor product only contains pure elements is a very common mistake. But I don't quite see the importance of this, if we can check some property $P$ for all pure elements, then shouldn't it hold for $M\otimes N$? For example, if $m\otimes n = 0$ for each $m\in M, n\in N$, then $M\otimes N = 0$. Can you give me an example where checking all pure elements is not good enough for $M\otimes N$.","['abstract-algebra', 'linear-algebra', 'tensor-products']"
2361990,Differentiating a function of functions,"If I have the function $ h \equiv h(f(x,t),g(x,t))$, where $x$ and $t$ are independent variables and $f$ and $g$ are functions of $x$ and $t$. Then is, $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\mathrm{d}f }{\mathrm{d} t},g \right) + h\left(f , \frac{\mathrm{d}g }{\mathrm{d} t}\right)$$ or $$\frac{\mathrm{d}h }{\mathrm{d} t} = h\left(\frac{\partial{d}f }{\partial{d} t},g \right) + h\left(f , \frac{\partial{d}g }{\partial{d} t}\right)\ ?$$ And can you tell me why? I think the first one might be correct.",['multivariable-calculus']
