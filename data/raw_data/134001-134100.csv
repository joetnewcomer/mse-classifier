question_id,title,body,tags
2107320,Find the third derivative of $xe^{(x^3)}$ and show points where the derivative is $0$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 7 years ago . Improve this question Find the third derivative of $f(x)=xe^{(x^3)}$. Along with the graph of the third derivative show the points where the third derivative is $0$. This is a problem that  I am supposed to solve in Wolfram Mathematica. I have begun with finding the derivative using D[[f[x],{x,3}] but I am having trouble finding the points and showing them on the graph.
In the presentations from my school it says that I am supposed to use the function Last[Last[FindRoot[f'[x], {x,0}]]], but I don't understand why I should use it because it's not really showing anything on my graph. By the way, {x,0} in the FindRoot[] function is supposed to be find the solutions for the equation in the area around 0 or ..? I'm guessing that since the graph of the derivative looks like this: The whole code that I have written for this problem is this:
GRAFIK[x_] = x*E^(x^3) r1[x_] = D[x*E^(x^3), {x, 3}] Simplify[3 (6 E^x^3 x + 9 E^x^3 x^4) + 
  x (6 E^x^3 + 54 E^x^3 x^3 + 27 E^x^3 x^6)] a = Plot[r1[x], {x, -1.5, 1}] rese1 = Last[Last[FindRoot[r1[x] == 0, {x, 0}]]] rese2 = Last[Last[FindRoot[r1[x] == 0, {x, -1.5}]]] rese3 = Last[Last[FindRoot[r1[x] == 0, {x, -0.5}]]] b = ListPlot[{rese1, GRAFIK[rese1]}, PlotStyle -> PointSize[0.02]]; c = ListPlot[{rese2, GRAFIK[rese2]}, PlotStyle -> PointSize[0.02]]; d = ListPlot[{rese3, GRAFIK[rese3]}, PlotStyle -> PointSize[0.02]]; Show[{a, b}, {a, c}, {a, d}]","['derivatives', 'mathematica']"
2107330,Mean Squared Error Maximum uniform distribution,"Let $X_1,\ldots,X_n$ be Uniform Distributed Random variables on $[0,\theta]$ and let T be $\max\{X_1,\ldots,X_n\}$ an estimator for $\theta$. I derived that the $F_T(x)=(\frac{x}{\theta})^n$ for $0<x<\theta$. Then I calculated the expected value of $T$, $$\operatorname{E}[T]=\int_0^\theta x\cdot\frac{nx^{n-1}}{\theta^n} \, dx=\frac n {n+1} \theta$$ and that $$
\operatorname{Var}(T)=\int_0^\theta x^2 \frac{nx^{n-1}}{\theta^n} \, dx - \left(\frac n {n+1}\theta \right)^2 = \frac n {n+2}\theta^2 - \left(\frac n {n+1} \theta\right)^2=\frac{n\theta^2}{(n+2)(n+1)^2}.
$$ When I now determine the Mean Squared Error of $T$, I find: $$\operatorname{MSE}(T)=\operatorname{Var}(T)-(\operatorname{E}[T]-\theta)^2 = \frac{n\theta^2}{(n+2)(n+1)^2} - \left(\frac n {n+1}\theta-\theta\right)^2 = \frac{-2\theta^2}{(n+1)^2(n+2)}$$ However, as $\theta>0$, the Mean Squared Error of $T$ is negative so I was wondering whether anything was wrong with this calculation. Could anyone help me please?",['statistics']
2107331,The probability that there will be 3 cloves with four leaves?,"Suppose there is $1$% of cloves with four leaves. We pick $100$ cloves. Let $X$ denote the event a clove has four leaves . What's the probability of having 4 cloves? I am wondering which method should I use? 1st possibility: The first such clove has a probability of being picked of $\frac{1}{100}$, the second $\frac{3}{99}$ the third $\frac{2}{98}$ and the last one $\frac{1}{97}$. By multiplying these, I could get the final probability of getting $4$ such cloves. Is this correct? 2nd possibility: My second thought would have been to use the Binomial distribution, although I am unsure I can use it, as we don't repeat the experience several times, we just pick $100$ cloves and see whether it has $4$ four-leave cloves. Any help will be appreciated.",['probability']
2107367,Find solution in integers of $x^3+x-y^2=1$ [duplicate],"This question already has an answer here : Cubic diophantine equation (1 answer) Closed 7 years ago . Find integers $x$ and $y$ such that
  $$x^3+x-y^2=1.$$ My try: $$x^3+x-y^2=1 \implies x^3+x-1=y^2.$$ Now, when $x^3+x-1$ is a perfect square?","['number-theory', 'elliptic-curves', 'diophantine-equations']"
2107373,How is $\lim\limits_{n \to \infty} \left( \frac{\ln(n+1)}{\ln(n)}\left(1 + \frac{1}{n}\right) \right)^n = e$?,"Tried this question here How to calculate $\lim\limits_{n \to \infty} \left( \frac{\ln(n+1)^{n+1}}{\ln n^n} \right)^n$? and was curious about the result. The answer according to Wolfram Alpha is $e$, so I wanted to try it. $\lim\limits_{n \to \infty} \left( \frac{\ln((n+1)^{n+1})}{\ln (n^n)} \right)^n$ $\lim\limits_{n \to \infty} \left( \frac{(n+1)\ln(n+1)}{n\ln (n)} \right)^n$ $\lim\limits_{n \to \infty} \left( \frac{\ln(n+1)}{\ln(n)}\left(1 + \frac{1}{n}\right) \right)^n$ This is similar to the typical definition $\lim\limits_{n \to \infty} \left(1 + \frac{1}{n}\right)^n = e$ but it has the extra log factors. How come these two happen to be equivalent? Is it valid to apply L'Hospital's Rule to the logs even though they're inside the $()^n$? Or can it be applied to just part of the function and not the other half? What's the correct way to handle this extra log multiplier? For instance: $\lim\limits_{n \to \infty}\frac{\ln(n+1)}{\ln(n)} = \lim\limits_{n \to \infty}\frac{\frac{d}{dn}\ln(n+1)}{\frac{d}{dn}\ln(n)} = \lim\limits_{n \to \infty}\frac{n}{1+n} = \lim\limits_{n \to \infty}\frac{1}{1/n+1}  = 1$ but I don't think we can necessarily analyze this ""separately"" from the main result; I think they must be taken together somehow.  I also considered squeeze theorem but couldn't think of another function approaching $e$ from the other side.","['number-theory', 'calculus', 'limits']"
2107382,Why is $9 \times 11{\dots}12 = 100{\dots}08$?,"While I was working on Luhn algorithm implementation, I discovered something unusual. $$ 9 \times 2 = 18 $$
$$ 9 \times 12 = 108 $$
$$ 9 \times 112 = 1008 $$
$$ 9 \times 1112 = 10008 $$ Hope you can observe the pattern here. What to prove this?
What is it's significance?","['decimal-expansion', 'sequences-and-series']"
2107409,recurrence relation of vertices on an hexagon,"I have the following question. 
Let there be an hexagon $ABCDEF$. We'll define a legal route of $n$ steps across the hexagon, as wakling one step (either clockwise or counterclockwise) at a time.
So a route of $2$ legal steps would be for example walking from $A \to B \to C$ or from $A \to F \to A$. 
We will define $a(n)$ as the number of legal routes of $n$ steps that start and end at vertex $A$.
I need to find a recurrence relation for $a(n)$.
I know that the initial terms are $a(0)=1$, $a(2)=2$ and $a(4)=6$.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2107460,Partitions of $n$ with exactly 3 parts,"I'm learning about generating functions, and came across this question: find the generating function for the number of partitions of a number $n$, into exactly 3 parts. I just solved a problem where the condition was that each part is no greater than 3, but I'm kind of stuck on this one. Any ideas? Thanks!","['generating-functions', 'combinatorics']"
2107569,Polar decomposition of composition of two $2 \times 2$ matrices,"In one of Ruelle`s papers ""Rotation Numbers for Flows and Diffeomorphisms"" Ruelle has the following calculation which I do not understand completely. Assume you have two invertible $2 \times 2$ matrices $A$ and $B$ with polar decompositions $A = U(\theta(A))|A|$ and $B = U(\theta(B))|B|$ where $U(\theta)$ is the planar rotation matrix by $\theta$ and $|B|=\sqrt(BB^T)$ etc. Then he says that
$$
|\theta(AB)-\theta(A)-\theta(B)| \leq \pi
$$ I don`t quite understand how he gets this result without a constant depending on norms of A and B. One can start by saying
$$
AB = U(\theta(AB))|AB| = U(\theta(A))|A| U(\theta(B))|B|
$$
$$
= U(\theta(A)+\theta(B))U(-\theta(B))|A|U(\theta(B))|B|
$$
so that
$$
U(\theta(AB)-\theta(A)-\theta(B)) = U(-\theta(B))|A|U(\theta(B))|B||AB|^{-1}.
$$
Somewhere in the paper he gives as a hint $|\theta(PQ)| \leq \pi$ if $P$ and $Q$ are positive but I cant see how to use it.","['dynamical-systems', 'linear-algebra']"
2107580,Prove that $\underline{\lim} \{a_n\}+ \underline{\lim} \{b_n\} \leq \underline{\lim} (a_n + b_n)$,"In my real analysis class, my instructor proved that $\underline{\lim}  \{a_n\}+ \underline{\lim} \{b_n\} \leq     \underline{\lim} (a_n + b_n)$. (Note that $\underline{\lim}$ is the limit inferior of a sequence.) but a lot of details were left out and I want to make sure that I have a correct proof and that my logic is correct. Note that $\underline{lim} \{a_n\} = lim_{n \rightarrow \infty} \inf\{a_k|k \geq n\}$ and in my class we define $t_n =\inf\{a_k|k \geq n\}$. My proof: Suppose that $\underline{\lim}  \{a_n\} = a$ and $\underline{\lim}  \{b_n\} = b$. First consider $\underline{\lim}  \{a_n\} = a$. This means that $ \displaystyle \lim_{n \rightarrow \infty} t_n = a$. So for all $\epsilon > 0$ there exists $N_1 \in \mathbb{N}$ such that when $n \geq N_1$ we have that $ | t_n - a | < \frac{ \epsilon } { 2}$. It follows that $a - \frac{\epsilon}{2} < t_n < a_n$. By a similar argument, there exists $N_2$ such that for all $n \geq N_2$, we have  $b - \frac{\epsilon}{2} < b_n$. So if we choose $N = \max \{N_1, N_2 \}$, for all $n \geq N$ we have $a - \frac{\epsilon}{2} < a_n$ and $b - \frac{\epsilon}{2} < b_n$. Combining these inequalities gives $a_n + b_n > a+b-\epsilon$ Then since $a+b - \epsilon$ is a lower bound for the set $\{ a_n + b_n | n \geq N \}$, we must have $\inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$ and $ \displaystyle \lim_{N \rightarrow \infty} \inf\{a_n + b_n | n \geq N \} \geq a+b- \epsilon$      (because the ineq. was true for all $n \geq N$). Then since $\epsilon$ was arbitrary, $ \underline{ lim} (a_n + b_n) \geq a+b$","['limsup-and-liminf', 'real-analysis', 'inequality', 'proof-verification']"
2107591,Conditional expectation of $X$ given $|X|$,"Let $X$ be in integrable, with density $f$ with respect to the Lebesgue measure. 
  Compute the conditional expectation : $ \operatorname{E} \left[ X\, \Big|\, |X| \,\right] $ My ansatz was : $ \operatorname{E} \left[ X\, \Big|\, |X| \,\right] = \operatorname{E} \left[ \operatorname{sgn}(X)\cdot |X| \, \Big|\, |X| \,\right] =  |X| \operatorname{E} \left[ \operatorname{sgn}(X) \, \Big|\, |X| \,\right] $ At this point I am stuck since one cannot assume in general that $\operatorname{sgn}(X)$ is independent of $|X|$ nor measurable.","['probability-theory', 'conditional-expectation']"
2107607,Surface integral via divergence theorem and Stokes' theorem,"I want to compute $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS$ where $\Sigma:=\{(x,y,z)\in\mathbb{R}^3:\sqrt{x^2+y^2}=z, 2+\frac{x}{4}\leq z\leq 4\}$, $F=(y,z,x)$ and $\omega (-3,0,3)=\frac{\sqrt{2}}{2}(1,0,1)$. 1)  Applying the divergence theorem I got $\iint_{\Sigma}(\nabla\times F)\cdot \omega dS=\iiint_{V(\Sigma)}\nabla\cdot (\nabla\times F)dV-\iint_{z=4\cap cone}(\nabla\times F)\cdot \omega dS-\iint_{z=2+x/2\cap cone}(\nabla\times F)\cdot \omega dS=0-0-(-16\pi )=16\pi$ 2) To check the result I've also applied Stokes's theorem taking as C the circumference $\sqrt{x^2+y^2}=4$ and I get $\oint_C F\cdot dr=\int_{0}^{2\pi}(4\sin (t),4,4\cos (t))\cdot (-4\sin (t), 4\cos (t),0)dt- \iint_{z=2+x/2\cap cone} (\nabla\times F)\cdot \omega dS =16\pi -0=16\pi$ So, is what I did correct?","['multivariable-calculus', 'stokes-theorem', 'surface-integrals', 'divergence-operator']"
2107663,"NAND, XOR and AND","This is the question I am trying to prove. Assume that p NAND q is logically equivalent to ¬(p ∧ q). Then, (a) prove
that {NAND} is functionally complete, i.e., any propositional formula is equivalent to
one whose only connective is NAND. Now, (b) prove that any propositional formula is
equivalent to one whose only connectives are XOR and AND, along with the constant
TRUE. Prove these using a series of logical equivalences. For part a, I have (A NAND B) NAND (A NAND B) ㄱ[(A NAND B) ⋀ (A NAND B)] LOGICALLY EQUIVALENT ㄱ[ ㄱ(A ⋀ B) ⋀  ㄱ(A ⋀ B)]       LOGICALLY EQUIVALENT (A ⋀ B) V  (A ⋀ B)         DE MORGAN (A ⋀ B)                    IDEMPOTENT using Table 6 and using Table 7 and 8 I believe that is all I have to do for part a but I am confused on how to do part b.  I think I have to make another propositional formula using just XOR and AND but what am I trying to prove it to, just (A OR B)?","['propositional-calculus', 'logic', 'discrete-mathematics']"
2107666,Proving that $\int_{-1}^0 \frac{e^x+e^{1/x}-1}{x}dx=\gamma$,"This came up while I was looking at the asymptotic behavior of $f(x)=\int_0^x \frac{e^t-1}{t}dt=\sum_{h=1}^\infty\frac{x^h}{h\cdot h!}$, which is a nice and entire function, as $x\to -\infty$, namely that I think $f(x)\sim -\ln(-x)-\gamma$ as $x\to -\infty$. I've reduced that to the problem of proving $$\int_{-1}^0 \frac{e^x+e^{1/x}-1}{x}dx=\gamma$$
where $\gamma\approx 0.577$ is the Euler-Mascheroni constant.","['asymptotics', 'integration', 'definite-integrals']"
2107669,Tiles Combinatorics,"I have the following question: Let there be a road of length $n$.
there are 3 types of tiles: of lengths 1,2,3.
We'll define  $a(n)$ as the number of roads of $n$ tiles where a tile of length 1 and a tile of length 2 can not be adjacent. I need to find a recurrence relation for $a(n)$. I've manually calculated $a(n)$ for $0\leq n \leq6$ and I got $a(0)=1, a(1)=1, a(2)=2, a(3)=2, a(4)=4, a(5)=6, a(6)=9$, but I can't find the general relation. Thanks for the help.","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2107684,Norm of the product of a self-adjoint operator and one of its spectral projections,"Let $A$ be a bounded self-adjoint operator on a Hilbert space $H$ such that $A$ has a cyclic vector. That is, there exist $x \in H$ such that the linear subspace spanned by $\{ x, Ax, A^2x,...\}$ is dense in $H$. Also let $E$ denote the spectral measure corresponding to $A$, i.e. $E : \Sigma \rightarrow B(H)$ with $E(\Omega)=E_{\Omega}=\chi_{\Omega \cap \sigma(A)} (A)$ given by the functional calculus for $A$. Here $\Sigma $ is the Borel-$\sigma$-Algebra over $\mathbb{R}$ and $\chi_M$ denotes the characteristic function of a set $M$. $\sigma(A)$ is the spectrum of $A$. Question : Assume that the interval $[0,1]$ is contained in the spectrum $\sigma(A)$ of A. What can we say about the norm $||E_\Omega A||$ if $\Omega$ is, for example, a subinterval of $[0,1]$, say $\Omega=[\tfrac{1}{4}, \tfrac{1}{2})$? What if $\Omega=[\tfrac{1}{4}, \tfrac{1}{3}) \cup ([\tfrac{1}{3}, \tfrac{1}{2}) \cap \mathbb{Q})$? One thing we know is that $||E_\Omega A|| = ||A E_\Omega||$. So we only have to compute the norm on the range of $E_\Omega$. But I don't really see how the cyclic vector comes into play. It tells us that $H$ is separable. I don't know how that could be helpful.","['functional-analysis', 'spectral-theory', 'operator-theory']"
2107722,Derivation practical problem rate of change pumping water from cylinder to reservoir,"I'm stuck at this problem and the solution doesn't make sense to me, I would like to understand what's wrong with my reasoning. There is a cylindrical tank with height 15m and radius 10m, fluid is being pumped into a biddon, which is composed of a rectangle and a prism (see picture). We know that the liquid level is decreasing with 2cm/sec in te cylindrical tank, and want the rate the level increases in the biddon at a level of 6m My way of solving is first calculate $V^\prime$. $V=hr^2\pi$ so
$V^\prime=\pi r^2h^\prime\, \to\, V^\prime=10000(2\pi)=20000\pi$ Then for the body I split up in two parts, 
First  $V=1000(1500h)$ Second $V=(800\cdot1500h)/2$ (I did $\frac{30}{10}=\frac{x}{6}\,\to\,x=18$ and $b=18-10$) Added them together $V=15\cdot10^5h+6\cdot10^5h \,\to\, 21\cdot10^5h$ Then  $\frac{V^\prime}{21\cdot10^5}=h^\prime$ so $h^\prime=0.01496\,$cm/sec However the result should be 0.019 cm/s, and it involves the height of the cylinder, but I don't see how that matters, or why my way is wrong. I would really appreciate your help :)","['derivatives', 'calculus']"
2107726,Smoothness of continuous bilinear maps,"$E_1,E_2,F$ are Banach spaces. There is one thing I don't understand if the following proof: I don't understand how he can say ""This proves the first assertion."" I'm assuming that $E_1 \times E_2$ is provided the sum norm $\|(h_1,h_2)\| = \|h_1\| + \|h_2\|$, so if we call $\lambda : E_1 \times E_2 \to \,\, ; \,\, (h_1,h_2) \mapsto \omega(x_1,h_2) + \omega(h_1,x_2)$, what he shows is that $\frac{\omega((x_1,x_2) + (h_1,h_2)) - \omega(x_1,x_2) - \lambda(h_1,h_2)}{\| (h_1,h_2) \|}
\\
= \frac{ \omega(x_1+h_1,x_2+h_2) - \omega(x_1,x_2) - (\omega(x_1,h_2) + \omega(h_1,x_2))}{ \|(h_1,h_2) \| }
\\
= \frac{ \omega(h_1,h_2) }{ \| (h_1,h_2) \|} = \frac{ \omega(h_1,h_2)}{ \|h_1\| + \|h_2\|}$ due to bilinearity. I know that $\| \omega(h_1,h_2) \| \leqslant \| \omega \| \|h_1\| \|h_2 \|$, where $\| \omega \| = sup_{\|h_1\|=1,\|h_2\|=1} \omega(h_1,h_2)$ but I don't think that is enough to prove that $\omega(h_1,h_2)/(\|h_1\|+\|h_2\|) \to 0$ as $(h_1,h_2) \to 0$, since I don't think $\frac{\|h_1\| \|h_2\| }{\|h_1\| + \|h_2\|}$ tends to zero as $(h_1,h_2) \to 0$. What am I missing?","['derivatives', 'linear-algebra', 'calculus']"
2107729,Bijection from the set of odd natural numbers to integers [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How to construct  a bijection $f: A\to \Bbb{Z}$ where $A$ is the set of odd natural numbers?","['real-analysis', 'functions']"
2107764,General Method to prove $s$ is a supremum of the given interval,"Is it possible to work out a ""general"" recipe to show an element, say $s$, is the supremum (or infimum) of a given set? I've noticed that in my lecture notes and in the book I'm reading, I can never find a general recipe, but surely there must be guidelines. There are three cases (I think): Show $b$ is the supremum of $A$ when $A =[a,b]$ or $A=(a,b]$ Show $b$ is the supremum of $A$ when $A=[a,b)$ or $A = (a,b)$ Show $b$ is the supremum of $A$ when $A$ is a set described by a given property, for example $A = \{n\in \mathbb{N}\,\,\,\,\, | \,\,\,\,\,\,(-1)^nn^2\}$ (I've made this up). For example I show there is a general recipe for case 1: Case 1 Since we are looking for the supremum, it doesn't really matter whether it is $A =[a,b]$ or $A=(a,b]$ , so I'll just use $A=(a,b]$. By definition of the set, we know that $b \geq x$, $\forall x\in A$. Now suppose there exists another element $c$ such that $c\geq x$, $\forall x \in A$, then since $b\in A$ we have that $b \leq c$ and hence $b = \sup(A)$. Case 3 Well here the general recipe is very general, meaning that it depends a lot on the description of the set. However one can generalise saying that first of all, if possible, we should try to rewrite the expression giving us the elements of the set. After this, we should start from remembering that $n\in\mathbb{N}$ means that $n \geq 1$ or $n>0$. Similarly, we can use domains and properties of several functions, such as $-1\leq \cos(x) \leq 1$. We use these inequalities to try to bound the expression describing the elements of $A$. Once we have found that, we are basically either in case $1$ or in case $2$. Sometimes, we might have to use induction, but that is rare. Should we use approximation property for suprema? Case 2 Is there a general recipe? I always make these exercises quite...luckily, although I can feel there is some kind of underlying structure. Most of the times I used Archimedean properties, if not all the times. MY QUESTION How would we define the general recipe for case $2$? Can someone tell me? And also, are my recipes for case $1$ and case $3$ correct?","['self-learning', 'real-analysis', 'supremum-and-infimum', 'elementary-set-theory']"
2107783,How to solve this differential equation of second order (non-linear)?,How to solve the system using the Runge-Kutta Felberg method (differential equation of second order (non-linear))?,"['differential', 'ordinary-differential-equations', 'nonlinear-system']"
2107816,existence of a limit of a function $f:\mathbb{R}^2 \to \mathbb{R}$,"Can anyone calculate the value of K. It seems 0 is the value as I have seen it in many questions but not sure. If anyone can arrive at this value, then please.",['multivariable-calculus']
2107822,query on maximum minimum of multi-variable function,"Q.How do we find out the sum of absolute maximum and absolute minimum values of $f(x,y)=(x+y)^2-(x+y)+1$ on a unit square $\{(x,y):0<x<1,0<y<1\}$? 
Using the method of $rt-s^2$ its value is zero. So now how do I search for the Max and main value?",['multivariable-calculus']
2107828,Derivative of the inverse of a radical function,"The problem I'm attempting to tackle says to take the derivative of the inverse, or ($f^{-1})'(a)$ of $\sqrt{x^3+x^2+x+22}$ when $a = 5$. From what I understand, I'm supposed to use the fact that ($f^{-1})'(a)$=$\frac{1}{f'(f^{-1}(a))}$. I was having a really hard time figuring out what $f^{-1}$ was, however, so I decided to punch it into Wolfram Alpha and see what it came up with for the inverse. I don't have high enough reputation to post an image, but here's the link for what I typed in. http://www.wolframalpha.com/input/?i=inverse+of+sqrt(x%5E3%2Bx%5E2%2Bx%2B22) Am I doing something wrong here, or missing some important step? Did I do something wrong with Wolfram Alpha? Or is this genuinely the answer that I needed for $f^{-1}(x)$ and I'm just supposed to plug 5 into that and keep solving from there? I'm only in the first chapter of Calculus II, and that giant equation seems a bit crazy for being in one of the first assignments of the semester. Thanks for any clarification you can offer.","['derivatives', 'inverse-function', 'calculus', 'functions']"
2107837,"$|x-f(x)|<5777$, Show that $f$ is surjective [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $f:\mathbb{R}^2\to\mathbb{R}^2$ be a continuous map such that $\forall \ x : |x-f(x)|<5777$. Show that $f$ is surjective.","['algebraic-topology', 'general-topology']"
2107881,Left ideals of matrix ring over a field,"The claim is made here that for $k$ a field The left ideals of $M_n(k)$ are all of the form  $$\{A \in M_n(k) \mid
 \operatorname{ker}A > S \}, \rlap{ \qquad \text{for some subspace
 $S$.}}$$ I was trying to think through that claim. I understand why all the left ideals of $M_n(k)$ are of the form
$$\{A \in M_n(k) \mid \operatorname{Rowspace}(A) < S \}, \rlap{ \qquad \text{for some subspace $S$.}}$$ In the presence of an inner product, we have that $\operatorname{ker}A^T = \operatorname{Range}(A)^{\bot}$, and therefore we get the characterization we want. But what about if there isn't an inner product? Does the same hold for $M_n(\Delta)$, where $\Delta$ is a division ring?","['matrices', 'noncommutative-algebra', 'linear-algebra', 'ideals']"
2107887,Does the operator $A^*A$ have a name?,"The self-adjoint operator $A^*A$ (for a generic linear operator $A$) is used in functional analysis, linear algebra, statistics, physics, and probably many other fields. I am curious if there is a standard way to refer to this operator? There's the Gramian of course, but that seems specifically to refer to a collection of finite-dimensional vectors. ""Gram operator"" seems natural to me, but Googling suggests that it's not widely used.","['functional-analysis', 'linear-algebra', 'terminology', 'operator-theory']"
2107888,Is there a link between the two definitions of homogeneity for differential equations (for a first order equation and diff equations of higher order)?,"According to Wikipedia, there are two definitions for homogeneity of a differential equation. The first is for first order differential equations: A first-order ordinary differential equation in the form: ${\displaystyle M(x,y)\,dx+N(x,y)\,dy=0}$ is a homogeneous type if both functions $M(x, y)$ and $N(x, y)$ are homogeneous functions of the same degree $n$.[1] That is, multiplying each variable by a parameter   ${\displaystyle \lambda }$, we find ${\displaystyle M(\lambda x,\lambda y)=\lambda ^{n}M(x,y)\,}$,     and     ${\displaystyle N(\lambda x,\lambda y)=\lambda ^{n}N(x,y)\,.} $ Thus,
  ${\displaystyle {\frac {M(\lambda x,\lambda y)}{N(\lambda x,\lambda y)}}={\frac {M(x,y)}{N(x,y)}}\,.}$ Which to me suggests something about how is we were to rescale each of the axes, the differential equation will remain the same. Then there is the definition for higher-order equations: A linear differential equation is called homogeneous if the following condition is satisfied: If   ${\displaystyle \phi (x)}$  is a solution, so is   ${\displaystyle c\phi (x)}$, where ${\displaystyle c}$ is an arbitrary (non-zero) constant. Note that in order for this condition to hold, each term in a linear differential equation of the dependent variable y must contain y or any derivative of y. A linear differential equation that fails this condition is called inhomogeneous.
  A linear differential equation can be represented as a linear operator acting on $y(x)$ where $x$ is usually the independent variable and y is the dependent variable. Therefore, the general form of a linear homogeneous differential equation is ${\displaystyle L(y)=0\,}$ ... It should be noted that the existence of a constant term is a sufficient condition for an equation to be inhomogeneous, as in the above example. So I get that this definition also has something to do with scaling, although it seems to be only scaling the y. Is there a connection between these two definitions? Are they referring to the same but just with different order equations?","['ordinary-differential-equations', 'transformation', 'linear-transformations']"
2107903,Understanding the derivatives in backpropagation algorithm,"I'm having trouble understanding the derivatives in the backpropagation algorithm. I'll use the example presented here . If you're unfamiliar with the algorithms I'm talking about - it's Okay, my question is only about derivatives. So I have the following functions: $$ x_1 = W_1x_0$$
$$ x_2 = f_1(x_1)$$
$$E = \frac{1}{2} || x_2 - y||^2$$ where $x_0$ is a vector of size 4x1, $W_2$ is a matrix of size 5x4, and $f_1$ is some nonlinear function (for example, the logistic function). $y$ is a vector with the same dimension as $x_2$. Now, I need to take the derivative of E w.r.t. $W_1$. I'll use the chain rule: $$ \frac{\partial E}{\partial W_1} = \frac{\partial E}{\partial x_2} \frac{\partial x_2}{\partial x_1} \frac{\partial x_1}{\partial W_2}$$ I can understand the first derivative: the derivative of a scalar (that comes from the function E) w.r.t. a vector is a vector. I'm not sure about the next part. The derivative of $x_2$ w.r.t. $x_1$ is the derivative of a vector w.r.t. a vector. Isn't that supposed to be a matrix, somehow? And the part I least understand is the last: The derivative of $x_1$ w.r.t. $W_1$. Isn't it impossible to take the derivative of a vector w.r.t. a matrix?","['derivatives', 'partial-derivative', 'neural-networks', 'calculus']"
2107931,Why axiom 3) of topology is redundant?,"I'm learning topology and I'm reading Dugundji's General Topology book. He gives  the definition of axioms of a topology $\tau$ on $X$: 1) & 2) (a topology is closed under arbitrary union and finite intersection ) 3) $\emptyset$ and $X$ belong to topology $\tau$. then he says: ""*Observe that since the union (resp. intersection) of an empty family of sets in X is $\emptyset$ (resp. X) axiom 3 is actually redundant. *"" My question are, why he consider a empty family of sets of $X$ in the observation? whitout axiom 3, how do we know  that in fact the empty set belongs to the topology $\tau$? I really don't understand his observation. Any help would be appreciated.",['general-topology']
2107935,Diffeomorphism invariance of the Ricci tensor,"It may be a very simple question, but I would be happy to have a quick answer on how we can show that the Ricci tensor is invariant under a diffeomorphism? To be precise, if $ \phi : M\to M$ is a diffeomorphism, I want to show $$ \text{Ric} (\phi^* g) = \phi^* \text{Ric} (g).$$","['riemannian-geometry', 'differential-geometry']"
2107950,Lagrange's theorem and divisibility consequences.,"There are some simple, but sometimes intriguing, divisibility statements
that are straightforward consequences of Lagrange's theorem. For instance: $p$ divides $a^{p-1}-1$ (Fermat's little thm) $n!$ divides $(p^n-1)(p^n-p)\cdots(p^{n}-p^{n-1}).$ The latter one can be derived  from the fact that $S_n  \hookrightarrow GL_{n}(\mathbb{F}_p)$. I've noticed  that  simple   examples like those  can be very compelling  for students (begginers). Question : Are there more interesting divisibility statements that are immediate conseguences of Lagranges' thm? That is, coming from the simple fact a group $H$ is a subgroup of a finite group $G$?","['number-theory', 'group-theory', 'finite-groups']"
2108066,Conceptual Problem: How to know when something has an asymptote and something has a point that does not exist,Question Let's say that we have the following $f(x)=\frac{x+1}{x^2-1}$ The next thing that we would do is to factor the denominator to get the following: $f(x)=\frac{x+1}{(x+1)(x-1)}$ then we next divide both the numerator and the denominator by (x+1) to get: $\frac{1}{x-1}=f(x)$ But then If we look at the equation we would presume that there is an asymptote which is x=1. However if we plug in x=-1 why is that undefined and not an asymptote? To clarify what I ask for is how to know when it is an asymptote and when it is just a point that is undefined algebraically. (sure we can use a graph but that is tedious),"['algebra-precalculus', 'asymptotics', 'graphing-functions']"
2108103,How to compute $e^A$?,"Let $\omega>0$. Then how to compute matrix $e^A$, where $$A=
  \begin{bmatrix}
    0 & \omega  \\
    -\omega & 0
  \end{bmatrix}.
$$ I am excited to see matrix as powers. I don't know how it can be power but any way it seems good stuff to learn. How  to compute it?","['matrices', 'algebra-precalculus', 'calculus']"
2108104,"What is the difference between [0, 1] and [0, 1)?","It's been I couple of years since I learned intervals and I've forgotten enough of it for it to confuse me on a daily basis. I have 2 questions: What is the difference between [0, 1) and [0, 1]. Is the first continuous (from 0 to 0.99...9) and the other discrete? Can discrete values be represented on a real number line? In 9 minus 6 (image here) for example, it seems like I'm counting the spaces between the vertical lines. If I'm doing right, then the discrete values become continuous. Am I understanding this incorrectly?","['algebra-precalculus', 'combinatorics', 'discrete-mathematics']"
2108116,Calculating the inverse Fourier transform of $ \frac{2i \sin(\pi \omega)}{\omega^{2}-1}$ using the definition,"I applied the FT to a piecewise function defined as: $$f(t) =
\begin{cases}
\sin(t),  & \text{-π≤t≤π} \\
0, & \text{otherwise}
\end{cases} $$ and got $F[f(t)]= \frac{2i\sin(\pi\omega)}{\omega^2-1} $ I thought it'd be a interesting to attempt to applying the inverse FT to try to get back to the piecewise function. However I am an engineer and have limited knowledge of complex variables. When applying the definition of the inverse Fourier transform I get: $$f(t)= \frac{1}{2\pi} \int_{-\infty}^{\infty} \frac{2i\sin(\pi\omega)}{\omega^2-1} e^{i\omega t} dt. $$ Is anyone able to help me do this? or give me a hint of where to start?
I have no idea how to choose the contour and which direction I should be closing it.","['fourier-analysis', 'complex-integration', 'complex-analysis', 'contour-integration', 'fourier-transform']"
2108169,Prove $k+1>k $ where $k$ is an integer,"I'm being asked to prove that if $k∈ \Bbb Z$, $k+1>k$. Judging from our instructions, it appears (I am unsure) as though I cannot use the law of induction to solve this.  A hint gives that the proof depends on $1∈\Bbb N$. I was thinking of approaching this by using if $k∈\Bbb N$ then $x+k∈\Bbb N$.  Problem is, we were not given $k∈\Bbb N.$  Is this solvable without induction? We are given associativity, commutivity, and the identity elements for addition and multiplication, the additive inverse, the properties of $=$, and basic set theory and logic. Order for the integers is given by Let $m,n,p∈\Bbb Z$.  If $m<n$ and $n<p$, $m<p$. Also assumed, and possibly relevant is if $x∈\Bbb Z$, then $x∈\Bbb N$ or $-x∈ \Bbb N$ or $x=0$.","['integers', 'elementary-set-theory', 'elementary-number-theory']"
2108174,Proving that the set of sequences $\{x_k\}$ such that $x_k \ge 0$ has an empty interior.,"Show that in the normed vector space $\mathcal l_1$ (vector space of real convergent sequences under the norm $||r_k||_1= \sum |r_k|$), the set $P=\{\{x_k\} \mid x_k \ge 0 \; \forall k \in \Bbb N\}$ has an empty interior. My proof so far goes like this:
Assume that $\exists \{x_k\} \in \mathcal l_1 \text { such that } \{x_k\} \in int(P)$, we have then that $\{x_k\} \in P$ and that $\exists B(\{x_k\},\epsilon) \subset P$ (by the definition of interior i'm working with), so I know I have to find a sequence $\{y_k\} \in B(\{x_k\},\epsilon)$ with negative terms (a sequence ""close enough"" to $\{x_n\}$  that is not in $P$, leading to a contradiction). But I can't seem to intuitively find a sequence with those propierties, maybe I'm just taking the long road and it's simplier than that, am I missing something here?","['real-analysis', 'sequences-and-series', 'vector-spaces']"
2108195,Does $(x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x}$ hold for $x\geq 1$?,"While calculating some integrals I happened to face the following estimate: $$\int_m^{m+1}\int_n^{n+1}\frac{dy dx}{x+y}\geq\frac{1}{m+n+1}.$$ After some tedious calculations, I figured that this estimate follows from the inequality $$ (x+1)\log{(x+1)}-2x\log{x}+(x-1)\log{(x-1)}\geq\frac{1}{x}\quad\text{for }x>1.$$ (If we interpret $0\log0$ as $\lim_{\epsilon\rightarrow 0} \epsilon\log{\epsilon}=0$, then the inequality also holds for $x=1$.) But how do we prove this inequality? What I have tried : Let $f(x)=x\log{x}-(x-1)\log{(x-1)}$ for $x>1$. Then the RHS equals $f(x+1)-f(x)$ so by the Mean Value Theorem there exist some $\xi$ between $x$ and $x+1$ such that $$f(x+1)-f(x)=f'(\xi)=\log{\left(1+\frac{1}{\xi-1}\right)},$$ and it suffices to show that $\log{(1+\frac{1}{x})}\geq\frac{1}{x}$ ... which is unfortunately not valid ! I think some clever use of the MVT can solve this problem, but I don't see how I should proceed. Please enlighten me.","['derivatives', 'real-analysis', 'inequality', 'calculus', 'fractions']"
2108199,Arc Length of an Ellipse using integration,"I was thinking about what the arc length of an ellipse is, but throughout my calculations I got stuck. Here is how I approached the problem:$$$$We have an ellipse in the form:
$$\frac{x^2}{a^2}+\frac{y^2}{b^2}=1\Rightarrow y=\pm \frac{b}{a}\sqrt{a^2-x^2}$$By applying the formula of the arc length of a function, we get:$$L=4\int_0^a\sqrt{1+\frac{b^2x^2}{a^2(a^2-x^2)}}dx=4\int_0^a\sqrt{\frac{a^4+(b^2-a^2)x^2}{a^2(a^2-x^2)}}dx$$Now I made a little subsitution recalling trigonometry: $$x=a\sin(u)\\dx=a\cos(u)du$$So the Integral now can be expressed as:$$L=4\int_0^{\frac{\pi}{2}}a\cos(u)\sqrt{\frac{a^4+(b^2-a^2)a^2\sin^2(u)}{a^2(a^2-a^2\sin^2(u))}}du=\\4\int_0^{\frac{\pi}{2}}a\cos(u)\sqrt{\frac{a^4+(b^2-a^2)a^2\sin^2(u)}{a^2(a^2\cos^2(u)+a^2\sin^2(u)-a^2\sin^2(u))}}du=\\4\int_0^{\frac{\pi}{2}}\sqrt{a^2+(b^2-a^2)\sin^2(u)}du$$So we have:$$L=4a\int_0^{\frac{\pi}{2}}\sqrt{1+\frac{(b^2-a^2)}{a^2}\sin^2(u)}du$$
Letting $m=\frac{(b^2-a^2)}{a^2}$ we finally get:$$L=4a\int_0^{\frac{\pi}{2}}\sqrt{1+m\sin^2(u)}du$$
However, at this point I do not know any way on how to integrate this function because the $m$ is 'in the way'. Does anyone have any hints?","['arc-length', 'trigonometry', 'calculus', 'integration', 'conic-sections']"
2108203,How can we know which function is greater than the other without drawing them?,"Two functions In other words, what is a a conventional way to know if the function $f(x)$ is greater than $g(x)$, taking into account that sometimes they exchange the highest position after intersecting. Thus a more specific question could be: How to know on each range, which function is greater than the other?","['calculus', 'functions', 'integration', 'area', 'graphing-functions']"
2108245,How did Euler prove this identity?,"While studying Fourier analysis last semester, I saw an interesting identity: $$\sum_{n=1}^{\infty}\frac{1}{n^2-\alpha^2}=\frac{1}{2\alpha^2}-\frac{\pi}{2\alpha\tan\pi\alpha}$$
whenever $\alpha \in \mathbb{C}\setminus \mathbb{Z}$, which I learned two proofs using Fourier series and residue calculus. More explicitly, we can deduce the theorem using Fourier series of $f(\theta)=e^{i(\pi - \theta)\alpha}$ on $[0,2\pi]$ or contour integral of the function $g(z)=\frac{\pi}{(z^2-\alpha^2)\tan\pi z}$ along large circles. But these techniques, as long as I know, wasn't fully developed at Euler's time. So what was Euler's method to prove this identity? Is there any proof at elementary level?",['sequences-and-series']
2108288,"For a vector bundle $\xi:E\to M$, why is Trace a parallel section of the bundle $End(\xi)^*$?","For any $L\in$ $End(\xi)$, $Tr(L)$ has a well defined value independent of the choice of basis, so we have a globally defined function $Tr:End(\xi)\to\mathbb{R}$. This function is tensorial, and thus represents a section of $End(\xi)^*$. My book (Walschap) claims that this section is parallel, but I don't understand his argument. Suppose we have a basis $\beta$ of parallel sections of $\xi$ and a parallel section $X$ of $End(\xi)$, all taken along a curve $c$ in $M$. Then according to my book $\beta^{-1}(t)\circ X(t)\circ \beta(t)$ is constant in the space of $n\times n$ matrices. Why? First of all I assume $\beta(t)$ here is supposed to be a matrix whose columns are the same as the chosen basis, in a particular coordinate chart. Then I see that $X(t)\circ\beta(t)$ is parallel. I'm not sure where to go from here.","['vector-bundles', 'connections', 'smooth-manifolds', 'differential-geometry']"
2108341,Prove that $\prod\limits_{k=1}^{[n/2]} (3+2\cos\frac{2kπ}{n}) =F_n$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question $F_1=1; F_2=2; F_{n+1}=F_{n}+F_{n-1}$ (Fibonacci) Prove that $\prod\limits_{k=1}^{[n/2]} \left(3+2\cos\dfrac{2kπ}{n}\right) =F_n$ please help me :(","['fibonacci-numbers', 'polynomials', 'trigonometry', 'chebyshev-polynomials', 'products']"
2108357,"fundamental group of manifold, Lee's text topological manifold","I am reading Lee' text ""Introduction to Topological Manifold"", I have a question about his proof of theorem 7.21.  I include his proof below for reference. My question is about the statement underlined in red.  I know that $U$ and $U'$ are connected since they are coordinate balls but how do we know that their intersection cannot be uncountable?  I couldn't think of a proof to show that they are countable.  Any help would be great.  Thank you.","['manifolds', 'differential-geometry', 'proof-explanation']"
2108362,"If the localization of a ring at each prime ideal is Noetherian, does this imply that ring is Noetherian? [duplicate]","This question already has answers here : A non-noetherian ring with all localizations noetherian (2 answers) Closed 7 years ago . If the localization $R_p$ of a ring $R$ at each prime ideal $p$ in A is Noetherian, does this imply that $A$ is Noetherian? What we call such rings which is not Noetherian but localization at each prime ideal is Noetherian ? Can somebody provide me any counterexample of (1) and also a good reference?","['localization', 'noetherian', 'algebraic-geometry', 'commutative-algebra']"
2108374,How to find the area of these sets?,"Let $T_1=[P_1,P_2,P_3]$ be a triangle. Choose a point $P_4$ on $T_1$ such that the area of triangle $T_2=[P_2,P_3,P_4]$ is half the area of $T_1$.  Repeat this process in the natural way. It is not too difficult to see the resulting sequence of triangles converges to a point. One can ask: What the area of the set of possible convergents as compared to the area of $T_1$? A quick hack when $P_1=(1,1)$, $P_2=(2,0)$ and $P_3=(0,0)$ yields the following graph of 30,000 random convergents: A reasonable estimate of the area of the triangle containing these convergents would be $2/9$.  Experimentally, at least, this type of simulation quickly finds an answer to the first question. Now suppose $P_{k+3}$ (and each following point) is chosen randomly either half way between $P_k$ and $P_{k+1}$ or one-third of the way from $P_k$ to $P_{k+2}$. Here is a graph of a few of these convergents: What is the area of the set of possible convergents in this case? Finally, suppose we choose the next point to lie half way on the left side three times as frequently as half way on the right side.  Here is the resulting rather weird graph of 100,000 or so such convergents: It seems to my untrained eyes as though asking questions about area no longer makes much sense, so I won't.","['sequences-and-series', 'fractals', 'geometry']"
2108384,Prove this $ \int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45 $,"I have trouble with this seemingly simple problem
$$
K=\int_0^\infty\frac{\coth^2x-1}{(\coth x-x)^2+\frac{\pi^2}{4}}dx=\frac45.\tag{A}
$$
Here is the Wolfram Alpha computation Proof . I tried to find residue at the pole $x=\frac{\pi i}{2}$ and get $\frac{9}{5\pi i}$ ( link to WA). Therefore residue theorem tell me that $K=\frac{18}{5}\neq\frac45$. I'm stuck. How to prove (A)?","['calculus', 'complex-analysis', 'integration', 'definite-integrals', 'contour-integration']"
2108394,Etale Cohomology of a Field,"How should I concretely think of the etale cohomology of $\text{Spec }k$, where $k$ a field? Is it possible to get some concrete examples? This is apparently connected (a reformulation in another language) to Galois theory, and we have concrete examples for that, so it would be helpful to me if I could make some analogies. I am new to etale cohomology, but I will outline what I know below. Please tell me if there are any mistakes in my understanding. So we would like to put a sheaf (of abelian groups) on $\text{Spec }k$. But this is etale cohomology, and instead of open sets, we have instead etale $k$-algebras, which are products of finitely many separable field extensions of $k$. So instead of inclusions of open sets, we have etale morphisms $k\rightarrow k_{i}$ where $k_{i}$ is an etale $k$-algebra which lead to the map of 1-point (since the spectrum of a field is composed of only the generic point) topological spaces $\text{Spec }k_{i}\rightarrow \text{Spec }k$ and restriction maps $\mathcal{F(k)}\rightarrow \mathcal{F(k_{i})}$. Intuitively, what are the abelian groups $\mathcal{F}(k)$ and $\mathcal{F}(k_{i})$? Under what law of composition? Do they have anything to do with the generators of the field extension, or the basis we use when we express the field extension as a vector space? Any concrete examples? The sheaf $\mathcal{F}(k_{sep})$, where $k_{sep}$ is a separable closure of $k$, is then defined as the direct limit (the stalk) $\mathcal{F}(k_{sep})$=$\varinjlim \mathcal{F}(k_{i})$ Why must we define $\mathcal{F}(k_{sep})$ as a direct limit? How do the $k_{i}$ form a directed system? What are the morphisms I must use to define the direct limit? Finally, we have $\mathcal{F}(k)$=$\mathcal{F}(k_{i})^{\text{Gal }(k_{i}/k)}$ i.e. ($\mathcal{F}(k)$ is the subgroup of $\mathcal{F}(k_{i})$ that is fixed under the action of $\text{Gal }(k_{i}/k))$. How is this so? It seems I would need the answers to the previous questions first before I can get this one. I am guessing this will be generalized to become a reformulation of the main theorem of Galois theory.","['etale-cohomology', 'algebraic-geometry']"
2108415,function with finite measure support in $L^p(\mathbb{R})$,"Let $p > q$ be fixed numbers in $[1,\infty]$. Give a proof or a counterexample to each of the statements below. (a) If $f \in L^p(\mathbb{R})$ has finite measure support, then $f \in L^q(\mathbb{R})$. (b) If $f \in L^q(\mathbb{R})$ has finite measure support, then $f \in L^p(\mathbb{R})$. (c) If $f \in L^p(\mathbb{R})$ is bounded, then $f \in L^q(\mathbb{R}$). (d) If $f \in L^q(\mathbb{R})$ is bounded, then $f \in L^p(\mathbb{R})$. For (a) I think it is true, since $f \in L^p(\mathbb{R})$ then $|f|^p$ is finite almost everywhere so is f. So let
$$K_o=\lbrace x \in \mathbb{R} \; ; \; f(x)=0\rbrace \qquad K=\lbrace x \in \mathbb{R} \; ; \; f(x)\neq 0 \; \& \; \infty \rbrace \qquad K_{\infty}=\lbrace x \in \mathbb{R} \; ; \; f(x)=\infty\rbrace$$
Then $\mu(K_{\infty})=0$
$$\int|f|^q =\int_{K_0}|f|^q+\int_K |f|^q +\int_{K_{\infty}} |f|^q=\int_K |f|^q=M^q \mu(K)<\infty$$ For (b) my guess is false but I could not find any counterexample. Also no idea about (c) and (d)","['lp-spaces', 'measure-theory']"
2108420,How to show that $\int_{0}^{\pi}(1+2x)\cdot{\sin^3(x)\over 1+\cos^2(x)}\mathrm dx=(\pi+1)(\pi-2)?$,How do we show that? $$\int_{0}^{\pi}(1+2x)\cdot{\sin^3(x)\over 1+\cos^2(x)}\mathrm dx=(\pi+1)(\pi-2)\tag1$$ $(1)$ it a bit difficult to start with $$\int_{0}^{\pi}(1+2x)\cdot{\sin(x)[1-\sin^2(x)]\over 1+\cos^2(x)}\mathrm dx\tag2$$ Setting $u=\cos(x)$ $du=-\sin(x)dx$ $$\int_{-1}^{1}(1+2x)\cdot{(u^2)\over 1+u^2}\mathrm du\tag3$$ $$\int_{-1}^{1}(1+2\arccos(u))\cdot{(u^2)\over 1+u^2}\mathrm du\tag4$$ $du=\sec^2(v)dv$ $$\int_{-\pi/4}^{\pi/4}(1+2\arccos(\tan(v)))\tan^2(v)\mathrm dv\tag5$$ $$\int_{-\pi/4}^{\pi/4}\tan^2(v)+2\tan^2(v)\arccos(\tan(v))\mathrm dv=I_1+I_2\tag6$$ $$I_1=\int_{-\pi/4}^{\pi/4}\tan^2(v)\mathrm dv=2-{\pi\over2}\tag7$$ As for $I_2$ I am sure how to do it.,"['integration', 'definite-integrals', 'calculus']"
2108457,"True or False : There exists a continuous map $f:[0,1] \to SL_2(\Bbb R)$ which is surjective. (NBHM 2017 exam India)","My approach : Suppose there exists such a continuous surjective map. Then since $[0,1]$ is compact, $SL_2(\Bbb R)$ is also compact. So essentially what I have to prove is that $SL_n(\Bbb R)$ is compact i.e. closed and bounded. I know that determinant map is continuous. As $SL_2(\Bbb R)=\det^{-1}\{1\}$ and $\{1\}$ is a closed set, hence $SL_2(\Bbb R)$ is a closed set. I don't know how to go about boundedness of $SL_2(\Bbb R)$. Nevertheless, if it turns out to be bounded set, then it would mean that such $f$ exists(correct me if I'm wrong here). and if it doesn't, then such a $f$ doesn't exist. Also how to construct such an $f$ if it does exist?","['real-analysis', 'continuity', 'compactness', 'general-topology', 'group-theory']"
2108539,"Injectivity of $f:\mathbb R_{>0} \times \mathbb R \to \mathbb R^2, (x,y) \mapsto (xy, x^2 - y^2)$","I want to show that the function $f:\mathbb R_{>0} \times \mathbb R \to \mathbb R^2, (x,y) \mapsto (xy, x^2 - y^2)$ is injective. I showed this by calculating an inverse $f^{-1}:\mathrm{Im}(f) \to \mathbb R_{>0} \times \mathbb R$. But this involves a long calculation and nasty substitutions. So I wondered if there is a more elegant way of doing this. I would appreciate some hints :)","['real-analysis', 'functions']"
2108578,Explain why $\int_0^\infty\frac{\sin{4x}}{x}\prod\limits_{k=1}^n \cos\left(\frac{x}{k}\right) dx\approx\frac{\pi}{2}$,"Why do we have, for every $n\in\mathbb N$, $$\int_0^\infty\left(\prod_{k=1}^n \cos\left(\frac{x}{k}\right)\right)\frac{\sin{4x}}{x}dx\approx\frac{\pi}{2}\ ?$$","['sequences-and-series', 'calculus', 'integration', 'pi', 'trigonometric-integrals']"
2108585,Incircle problem,"Triangle ABC has incircle $ \beta$ which meets BC at D. A diameter of the incircle has endpoints E and D. A line joining A and E meets BC at F. Given that $DC \gt BC$. Prove that $BD =FC$ I couldn't find any synthetic geometry methods, so I resorted to coordinate geometry. I took the incircle as a circle with radius 1 and centre (0,1). $B \equiv (-x,0) , C \equiv (y,0)$
I found $A \equiv (\frac{x-y}{xy-1} , \frac{2xy}{xy-1} )$
Then on extending AE we get $ F \equiv (y-x,0)$ and hence it is proved that $\overline{BD} = \overline{FC}$. I hope anyone could provide me proof with Euclidean geometry, which is more intuitive and brainy than bashing. Note: Please help in putting a suitable title.","['euclidean-geometry', 'geometry']"
2108596,Formula for $\sum_{k=1}^n \frac{1}{k(k+1)(k+2)}$?,"I was trying to find a formula for the series $$\sum_{k=1}^n \frac{1}{k(k+1)(k+2)} =? $$ I tried to break this into partial fractions...To see if I could telescope this series..
The partial fraction went like this 
$$\frac{1}{k(k+1)(k+2)}=\frac{1}{2k}-\frac{1}{k+1}+\frac{1}{2k+4}$$ But the terms are so random that they hardly cancel.... I also tried partial sums but couldn't make much headway....Any help to solve this would be appreciated","['summation', 'sequences-and-series', 'closed-form']"
2108603,Distribute water over uneven ground,"We are in a (closed and connected) uneven area that lives in $X \subset\mathcal R$. Height of the ground is indicated by $f : X \to \mathcal R$, which is a twice continuously differentiable function. It rains a fixed measure of water $e$. The area is nonabsorbant, such that the water does stay on the ground. This water is particularly persistent, in that it does not go for local minima, but intelligently finds the global minima. This implies that there is a unique water stand over $X$. Denote the distribution of water over the space as $w : X \to \mathcal R$. I would like to compute $w(x)$, and the stand of the water (how high it has risen). Define as the effective height of the ground $$ \tilde f(x) = f(x) + w(x) $$ One of the requirements onto $w$ is that $$ w(x) + f(x) = h \, \forall x : w(x) > 0 \tag 1$$ That is, effective surface is flat if there is water on it (and there is a unique effective surface heigh $h$ for any area with water on it). Secondly, $$ \int_X w(x) dx = e \tag 2$$ Easier problem Assume that $f$ was non-decreasing. Then, there exists $\bar x$ such that $w(x) > 0$ for $x < \bar x$ and $w(x) = 0$ for $x > \bar x$. We can find $\bar x$ using that $$ \int_0^{\bar x} w(x) dx = e = f(\bar x) - \int_0^{\bar x} f(x)dx$$ Then, we can simply compute $w(x) = f(\bar x) - f(x)$. I suppose there is some sort of transformation of $X$ into a new domain that is sorted by $f(x)$, and then I could apply this solution approach? Or how would I go on with solving this (original, not the easy) setup? Is there a better characterization/reformulation that allows me to find it quickly numerically?","['algebra-precalculus', 'ordinary-differential-equations']"
2108607,How to find sides of parallelogram given centroid and two vertices?,"I've been given a parallelogram with two of its vertices, $A$ and $B$, being equal to (-5, -8, 3) and (4, 7, -5) respectively, and a centroid $S$ at (-10, 4, 6). How do I go around finding remaining coordinates of points $D$ and $C$?",['linear-algebra']
2108630,proof: Hilbert Schmidt operator is compact,"Consider the Hilbert Schmidt operator $K: L^2(\Omega) \rightarrow L^2(\Omega)$ , $\Omega \subset \subset \mathbb R^N$ , with $k \in L^2(\Omega \times \Omega)$ and $f \in L^2(\Omega)$ , $$(Kf)(x) := \int_\Omega k(x,y)f(y)\, dy.$$ I want to show that the Hilbert Schmidt operator $K$ is a compact operator. Therefore I'm using this characterization. Let $X$ , $Y$ be normed linear spaces and $X$ reflexive. A continuous linear operator $T: X \rightarrow Y$ that maps weakly convergent sequences onto strongly convergent sequences is compact. (We already know that $K$ is well-defined as is proven here .) My question here is, isn't it obvious that $K$ is compact? We know that $K$ is linear and bounded, hence continuous. Every continuous map takes weakly convergent sequences to weakly convergent sequences. The norm itself is also continuous. Weak convergence together with convergence of the norms implies convergence. Thus $K$ is compact. Am I missing something here? Or better: What am I missing here? $\,$ I'm also adding the proof from the textbook for completeness: Proof. Let $(f_n)_{n \in \mathbb N} \subset L^2(\Omega)$ a weakly convergent sequence, then $(f_n)_{n \in \mathbb N}$ is bounded. That is, $\exists C > 0 $ such that $||f_n||_{L^2(\Omega)} \leq C$ , $\forall n \in \mathbb N$ . By Fubini's theorem we have for almost every $x\in \Omega$ that $$ || k(x,\cdot) ||_{L^2(\Omega)} = \int_\Omega |k(x,y)|^2 \, dy < \infty .$$ Thus for almost every $x \in \Omega$ we have $\begin{align}
\lim_{n \rightarrow \infty} (Kf_n)(x) & = \int_\Omega k(x,y)f_n(y) \, dy = \lim_{n \rightarrow \infty} \langle k(x,\cdot), f_n \rangle_{L^2(\Omega)} \\
& = \langle k(x,\cdot), f \rangle_{L^2(\Omega)} = \int_\Omega k(x,y)f(y) \, dy = (Kf)(x)
\end{align}$ By Cauchy-Schwarz's inequality we have $$ (Kf_n)(x) \leq ||f_n||_{L^2(\Omega)} \int_\Omega |k(x,y)|^2 \, dy \leq C \, \int_\Omega |k(x,y)|^2 \, dy $$ Hence by Lebesgue's dominant convergence theorem we have convergence of the norms $$ \lim_{n \rightarrow \infty} \int_\Omega |(Kf_n)(x)| \, dx = \int_\Omega |(Kf)(x)| \, dx ,$$ that is $|| Kf_n ||_{L^2(\Omega)} \rightarrow || Kf ||_{L^2(\Omega)}\, \, (n\rightarrow \infty)$ . Since weak convergence together with (strong or normal) convergence of the norms implies (strong) convergence, $K$ is compact.","['banach-spaces', 'normed-spaces', 'operator-theory', 'functional-analysis', 'weak-convergence']"
2108643,Does Euclid's Book I Proposition 24 prove something that Proposition 18 and 19 don't prove?,"The question title says it all. Book I, Proposition 24 states If two triangles have two sides equal to two sides respectively, but have one of the angles contained by the equal straight lines greater than the other, then they also have the base greater than the base. Book I, Proposition 18 states In any triangle, the angle opposite the greater side is greater. It seems that Proposition 24 proves exactly the same thing that is proved in Proposition 18. Does Proposition 24 prove something that Proposition 18 (and possibly Proposition 19) does not ? It seems that Proposition 24 could be proved in a single step by invoking Proposition 18.  What is the reason it could not be done that way ?... an example of why not, would be great.","['euclidean-geometry', 'geometry']"
2108697,Prove that $EX=E(E(X|Y))$,"Prove that $EX=E(E(X|Y))$ I know that I should prove it from definition of conditional distribution and conditional expected value, but I don't know how. I have also looked at theorem(""Total expectation"") which should be connected with the proof. If someone would tell me, which properties of expectation I should use to prove it, I would be really glad.","['probability-theory', 'conditional-expectation']"
2108709,What does one need measure zero for? [closed],Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 7 years ago . Improve this question I'm taking multivariable calculus course (third one actually) and we're introduced to measure zero. What does one need measure zero for? Measure zero of a set $A$ in $\mathbb{R}^n$ means that one can find a collection of sets whose volume can be made arbitrarily small and so that $A$ is contained in the sum over such collection. Why is this an important property?,['multivariable-calculus']
2108748,Proving or giving a counterexample for $f^{-1}(A) \subseteq f^{-1}(B) \Rightarrow A \subseteq B$,"Let $X,Y$ be sets and $f: X \to Y$. If $A,B \subseteq Y$, is it true
  that $$f^{-1}(A) \subseteq f^{-1}(B) \qquad \Rightarrow \qquad A
 \subseteq B$$ I know this is a stupid question, but at the moment I cannot come up with either a counterexample or a proof. I am sorry to ask this with that much reputation, but at the moment I do not really see if this is right or wrong. Edit. It is wrong. For example consider $f = x^2$. Then $$f^{-1}([0,1]) = [0,1] = f^{-1}([-1,1])$$ but clearly $$[-1,1] \nsubseteq [0,1]$$ Thanks for the answers until now. May this implication be fixed with either assuming $f$ to be injective or surjective?",['functions']
2108787,Find the limit of $x(\sqrt{x^2+1}- \sqrt[3]{x^3+1})$ as $x\to +\infty$.,Calculate $\lim_{x\to +\infty} x(\sqrt{x^2+1}- \sqrt[3]{x^3+1})$. First thing came to my mind is to simplify this to something easier. So multiply the numerator and the denominator by something like we used to do when two square roots involves. But I am trying to find that suitable term but it seems out of my reach. Can anybody help me to solve this ? Any hint or help would be nice . Thanks.,['limits']
2108810,Check Convergence of the series: $\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}}$,I have to check the convergence of this series: $$\sum_{n=1}^\infty {\frac{ \sqrt{n+1} - \sqrt{n}}{\sqrt{n}}}$$ Which is equal to $$\sum_{n=1}^\infty \frac{\sqrt{n+1}}{\sqrt{n}} - 1$$ What can I do here to check whether this series convergences or not? Thank you very much.,"['convergence-divergence', 'sequences-and-series', 'calculus', 'limits']"
2108815,von Neumann algebra generated by the Cuntz algebra,"Let $H$ and $K$ be two Hilbert spaces. Let us consider two pairs of isometries $(x_1,x_2)$ in $B(H)$ and $(y_1,y_2)$ in $B(K)$ satisfying in 
$$x_1x_1^*+x_2x_2^*=1_H~~~,~~~y_1y_1^*+y_2y_2^*=1_K$$ Cuntz proved that the C*-algebra generated by $x_1,x_2$ is the same as the C*-algebra generated by $y_1,y_2$. Q. Is the von Neumann algebra generated by $x_1,x_2$  the same as the von Neumann algebra generated by $y_1,y_2$?","['functional-analysis', 'c-star-algebras', 'operator-algebras', 'von-neumann-algebras']"
2108822,"If cosB=3/4, how do I find cos 2B and cos(B/2)?","I got $\cos 2B$;
$\cos 2B=2 \cos^2 B - 1
             =9/8 -1
              =1/8$ but when I tried cos1/2B, I got: COS1/2B=cos^2*1/4B-1, then I solved it, and didn't get the answer which was in the book, why?",['trigonometry']
2108828,Solving $\lim_{x\to1}(4^x-3^x)^{\frac{1}{1-x}}$ and others like it without L'hospital [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Ok so I have issues with this specific 'type' of limits. : $\\$ $$\lim_{x\to1}(4^x-3^x)^{\frac{1}{1-x}}$$ and $\\$ $$\lim_{x\to-1}\biggr(-4\cdot\arctan(x)\cdot{\frac{1}{\pi}}\biggr)^{\frac{1}{x+1}}$$
$\\$ It seems like they are quite similar, but I'm not sure what to do. I've tried taking $\ln$ of the limit to simplify it but have reached nothing. I would appreciate any hint whatsoever. However, I am not allowed to use L'hospital rule or integrals to solve these. Thanks in advance!","['limits-without-lhopital', 'limits']"
2108850,"Under a finite dimensional norm-induced metric space, is every bounded sequence has a converging subsequence?","What I know is that every bounded sequence in $\Bbb R^n$ has a converging sub sequence regardless of the chosen norm, since this is true under the Euclidean norm, and all norms on $\Bbb R^n$ are equivalent. My question is Is every bounded sequence has a converging sub sequence for any norm-induced metric space? If not, a counter-example is appreciated. Thanks!","['functional-analysis', 'real-analysis']"
2108864,Lyapunov stability,"i have a question regarding Lyapunov stability and basin of attraction. Let $${x}'=-x-y$$
  $${y}'=2x-y+y^3$$
Use $$V(x,y)=x^{2}+\frac{1}{2}y^{2}$$ to determine the stability of (0,0) and a basin of attraction. We have $$\dot{V}(x,y)=2x(-x-y)+y(2x-y+y^3)$$ or 
$$\dot{V}(x,y)=-2x^2-y^2+y^4$$.
Now V is positive definite and continuously differentiable V(0,0)=0 Now,$$\dot{V}(x,y)$$ is negative definite if $$-y^2+y^4<0$$ or $$-1<y<1$$ .
Now the textbook says that in order for $$x^{2}+\frac{1}{2}y^{2}=c$$ to stay in the strip $$ \kappa  = \left \{  \right.(x,y):x \in R,y \in (-1,1) \left.  \right \}$$ it must hold true that $$c\in (0,1/2)$$. This is where i stuck ,i don't understand the last step from which he concluded that $$c\in (0,1/2)$$.Can someone help me?","['stability-in-odes', 'lyapunov-functions', 'ordinary-differential-equations', 'basins-of-attraction']"
2108867,Find the result of $\sum_{n=1}^{\infty} \sin\left(\frac{1}{2^n}\right)$,"Applying ratio test , we can prove this series $\displaystyle \sum_{n=1}^{\infty} \sin\left(\frac{1}{2^n}\right)$ converges. How can we calculate or estimate the sum? Any help is appreciated, thank you.","['trigonometry', 'sequences-and-series', 'closed-form']"
2109066,replace the worst $20\%$ in normal distribution- exercise,"Here is an exercise, I tried to solve. It is a long since, I study statistics, so I would really appreciate if someone could check the answer/solution I give below. Exercise: Assume we are given a sample of $160$ cows. The mean value of the milk production is $ \mu= 100 \textrm{kg},$ and the standard deviation is $ \sigma= 30 \textrm{kg}.$ We wish to replace the $20\%$ of the cows with the minimum production of milk. Find the quantity of milk the these cows produce. Solution: We denote by $X$ the random variable of the amount of milk.
By the central limit theorem, we can assume that $ X \sim N(100, 30^2) ,$ and thus $$ \frac{ X - 100}{30} \sim N(0,1)$$ follows the standard normal distribution. Put $ c$ for the maximum quantity of milk, produced by cows that belong to the worst $20 \%.$ Then we have: $$ P( X \leq c ) =0.2= 1-0.8=1- \Phi(0.84)= \Phi(-0.84)$$ $$ \Longrightarrow P(Z \leq \frac{ c-100}{30})= \Phi(-0.84) ,$$ where $ \Phi$ is the probability density function of the standard normal distribution. Therefore we obtain $$ \frac{c-100}{30} = -0.84 \Longleftrightarrow c=74,8 .$$ In conclusion, the cows that we should replace, are these that produce milk less than $ 74.8 \textrm{kg}. \quad  \square$ Could someone check this attempt of solution ? Thank you in advance!","['statistics', 'proof-writing', 'proof-verification', 'statistical-inference']"
2109092,A new result of implicit function (Theorem M-R),"Let $f\in C^0(\mathbb R^2, \mathbb R)$, so that there exists an $M>0 \in \mathbb R$ with $f(x,y) \geq M$ for all $(x,y)\in\mathbb R^2$. Does it follow for any $g \in C^0(\mathbb R,\mathbb R)$ that there exsists a unique $ h \in C^0(\mathbb R,\mathbb R)$ so that
$$\int_0^{h(a)} f(a,x)\,\text{d}x=g(a)$$
for all $a\in\mathbb R$?","['functional-analysis', 'real-analysis', 'implicit-function-theorem', 'analysis']"
2109121,Showing the set-theoretic image is contained in the scheme theoretic image.,"$\newcommand{\Spec}{\operatorname{Spec}}$I'm looking at the proof of Corollary $8.3.5$ in Vakil's FOAG, which is to show that under certain conditions, the closure of the set-theoretic image of a scheme morphism $\pi:X\to Y$ is the scheme-theoretic image. The first step in this is to see, of course, that the set-theoretic image is indeed contained in the scheme-theoretic image. Vakil asks the reader to check this, so it must not be too complicated, but I'm having trouble proving it. The scheme-theoretic image is described as follows: for a closed subscheme $i:Z\hookrightarrow Y$, we say the image of $X$ lies in $Z$ if $\mathscr J_{Z/Y}\to\mathscr O_Y\to\pi_*\mathscr O_X$ is exact, where $\mathscr J_{Z/Y}$ is the sheaf of ideals induced by $Z$. We get the scheme-theoretic image by taking the intersection over all such $Z$. So all we need to show is that if we have such a $Z$, and $p\in X$, then $\pi(p)\in Z$. If we choose an affine open set $\Spec B$ containing $\pi(p)$, then by definition of $X$ lying in $Z$, we have $$\mathscr J_{Z/Y}(\Spec B)=\ker(B\to\mathscr O_X(\pi^{-1}\Spec B))$$ If we let $I(B)=\mathscr J_{Z/Y}(\Spec B)$, then we note that $Z=\cup \Spec B/I(B)$ as $\Spec B$ runs over all affine open subsets of $Y$, so we just need to show that if $\pi(p)$ corresponds to the prime ideal $\mathfrak q$ of $B$, then $\mathfrak q\supseteq I(B)$. I feel this should just be an unraveling of definitions, but I'm not seeing it. Can anybody help?","['schemes', 'algebraic-geometry']"
2109126,There is an $80\%$ chance of rain on each of the next $6$ days. What is the probability that it will rain on exactly $2$ of those days? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question There is an $80\%$ chance of rain on each of the next six days. What is the probability that it will rain on exactly two of those days? Could you show the workings as well as showing in the equation format. I don't quite understand the equation formatting yet... :(",['probability']
2109157,Are there real definite integrals which can only be evaluated by contour integration?,"Contour integration is a great tool for definite integrals, especially of the following type: $$\int_{-\infty}^\infty \frac{f(x)dx}{P(x)}$$ where $P(x)$ is a polynomial with no zeroes on the real axis (as far as I remember) and $f(x)$ is some appropriate regular function, which vanishes fast enough in the upper half-plane of $\mathbb{C}$. I'm not going to go into details here and refer everyone to a textbook definition. Contour integration seems like magic to be, and I'm sure to most of the regular people who use it either during their education or for various applications like physics. It's simple for me to see how it works in the complex plane, but not how it applies to the real functions. Why would some poles which are not on the real line affect the value of a real integral which still can be seen as the area under the curve defined by the integrand? I know that some integrals which can be solved by contour integration can be also solved in other ways, for example the most simple one: $$I=\int_{-\infty}^\infty \frac{dx}{1+x^2}=\pi$$ Using $(1+i)(1-i)=1+x^2$ makes the contour integration very simple, however it's also simple to show that $I=4\int_0^1 \frac{dx}{1+x^2}$ and connect the following integral to the definition of arcangent and Lebniz series. But not all of the integrals can be easily solved that way. What about: $$\int_{-\infty}^\infty \frac{\cos x dx}{1+x^2}=\frac{\pi}{e}$$ Can this integral be solved by real methods? Are there real integrals with closed forms, which can only be evaluated by contour integration? And if they exist, then how can it be explained? To be clear, I understand that we can numerically confirm the value to any precision by just using numerical methods for the real integral in question. Except for some highly oscillatory integrands (probably).","['integration', 'definite-integrals', 'contour-integration', 'calculus']"
2109183,Product of two convergent sequences is convergent,"I want to show that if I have $\lim_{n\to\infty}a_n = a$ and $\lim_{n\to\infty}b_n = b$ then $\lim_{n\to\infty}a_nb_n = ab$ I want to do it in a slightly different way than by book but I don't know if it is correct. $(a_n)$ converges to $a$ means $$\forall \epsilon >0, \exists n_1 \in\mathbb{N}: |a_n - a| < \frac{\epsilon}{2|b|} \,\,\,\ \forall n\in\mathbb{N}, n>n_1$$
$(b_n)$ converges to $b$ means $$\forall \epsilon >0, \exists n_2 \in\mathbb{N}: |b_n - b| < \frac{\epsilon}{2|a|} \,\,\,\ \forall n\in\mathbb{N}, n>n_2$$
Hence take $n_0 := \max\{n_1,n_2\}$ and we have $$|a_nb_n - ab| = |a_nb_n -a_nb+a_nb-ab| = |a_n(b_n-b)+b(a_n-a)| \leq |a_n|\frac{\epsilon}{2|a|}+|b|\frac{\epsilon}{2|b|}=\frac{\epsilon}{2} +\frac{\epsilon}{2} = \epsilon$$ so it's proven. Can I do that trick in the penultimate step, where I cancel out $|a_n|$ and $|a|$ ? I know that for large $n$ they will be the same, but I think this proof right now is not very rigorous!","['real-analysis', 'proof-writing', 'sequences-and-series', 'proof-verification']"
2109192,Simplifying fraction with factorials: $\frac{(3(n+1))!}{(3n)!}$,"I was trying to solve the limit: $$\lim_{n \to \infty} \sqrt[n]{\frac{(3n)!}{(5n)^{3n}}}$$ By using the root's criterion for limits (which is valid in this case, since $b_n$ increases monotonically): $$L= \lim_{n \to \infty} \frac{a_n}{b_n} = \lim_{n \to \infty} \frac{a_{n+1}-a_n}{b_{n+1}-b_n}$$ Now I realise using Sterling's formula would make everything easier, but my first approach was simplifying the factorial after applying the criterion I mentioned before. So, after a few failed attempts I looked it up on Mathematica and it said that $\frac{(3(n+1))!}{(3n)!}$ (which is one of the fractions you have to simplify) equals $3(n+1)(3n+1)(3n+2)$. Since I can't get there myself I want to know how you would do it. Just so you can correct me, my reasoning was: $$\frac{(3(n+1))!}{(3n)!} = \frac{3\cdot 2 \cdot 3 \cdot 3 \cdot 3 \cdot 4 \cdot (...) \cdot 3 \cdot (n+1)}{3 \cdot 1 \cdot 3 \cdot 2 \cdot 3 \cdot 3 \cdot (...) \cdot 3 \cdot n } = $$
$$= \frac{3^n(n+1)!}{3^{n}n!} = \frac{(n+1)!}{n!} = n+1$$ Which apparently isn't correct. I must have failed at something very silly. Thanks in advance!","['algebra-precalculus', 'factorial']"
2109196,"Show that three complex numbers $z_1, z_2, z_3$ are collinear iff $\operatorname{Im}(\overline{z_1}z_2+\overline{z_2}z_3+\overline{z_3}z_1) = 0$","I need to show that $\operatorname{Im}(\overline{z_1}z_2+\overline{z_2}z_3+\overline{z_3}z_1) = 0 \iff z_1,z_2,$ and $z_3$ are collinear. I know that $\operatorname{Im}(\overline{z_1}z_2+\overline{z_2}z_3+\overline{z_3}z_1) = 0$ implies that $\overline{z_1}z_2+\overline{z_2}z_3+\overline{z_3}z_1 \in \mathbb{R}$, but I am not sure how to argue in either direction. Please help. Thank you","['complex-numbers', 'geometry']"
2109204,Find an integer quartic root of a $3\times 3$ matrix,"Find a $3 \times 3 $ matrix $X$ with integer coefficients such that
  \begin{align*}
X^{4} &= 3
\begin{bmatrix}
2 &-1 &-1 \\
-1 &2 &-1 \\
-1 &-1 &2 
\end{bmatrix}.
\end{align*} My attempt. Let us consider the matrix 
\begin{align*}
A &= 3
\begin{bmatrix}
2 &-1 &-1 \\
-1 &2 &-1 \\
-1 &-1 &2 
\end{bmatrix} \\
&= \begin{bmatrix}
6 &-3 &-3 \\
-3 &6 &-3 \\
-3 &-3 &6 
\end{bmatrix}
\end{align*}
Calculate the roots of characteristic polynomial, i.e calculate the eigenspace $AZ=\lambda Z$, this is given for the equation system $A-\lambda I=0$, where $I$ is $3 \times 3$ identity matrix. 
\begin{align*}
\begin{vmatrix}
6-\lambda & -3 & -3 \\
-3 & 6-\lambda & -3 \\
-3 & -3 & 6-\lambda
\end{vmatrix} &= -\lambda \left( \lambda-9\right)^{2}
\end{align*}
Therefore, the polynomial function, the zero $\lambda=9$ has multiplicity $2$, and $\lambda=0$ has multiplicity $1$ and these special values are called the eigenvalues of the matrix $A$. We need to know the dimension of the eigenspace generated by this eigenvalue. Thus, solve the system $\left(A-3I\right)Z=0$ where $Z^{T}=\left(x,y,z \right)$ in order to find the eigenvectors. 
(1) For $\lambda =0$, then $\left(A-3I\right)Z=0Z$. Thus, $x=y=z=0$. Hence, $v_{1}= \left(1,1,1\right)^{T}$ is an eigenvector corresponding to $\lambda=0$. (2) For $\lambda=9$. Then, we choose $x=0$, $y=1$, then $z=-1$. Hence, $v_{2}= \left(0,1,-1\right)^{T}$. Also, choose $x=1$, $y=-1$, then $z=0$, hence, $v_{3}= \left(1,-1,0\right)^{T}$. Furthermore, $v_{2}$ and $v_{3}$ are eigenvector corresponding to $\lambda=9$. Thus, we have the matrix $S=\left[v_{1} \ v_{2} \ v_{3} \right]$. Then, 
\begin{align*}
S &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\end{align*}
and its inverse 
\begin{align*}
S^{-1} &= 
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix} 
\end{align*}
Thus, $A=SJS^{-1}$, where 
\begin{align*}
J &= 
\begin{bmatrix}
0 &0 &0 \\
0 &9 &0 \\
0 &0 &9 
\end{bmatrix} 
\end{align*}
where $J$ is the Jordan canonical form of $A$. Hence, $\displaystyle X=SJ^{1/4} S^{-1}$
\begin{align*}
X&=SJ^{1/4}S^{-1} \\
A &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\begin{bmatrix}
0 &0 &0 \\
0 &9^{1/4} &0 \\
0 &0 &9^{1/4} 
\end{bmatrix}
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix}\\
\end{align*}
Now, $9^{1/4}= \sqrt[]{3}, \ - \ \sqrt[]{3}, \ \sqrt[]{3} \ i$, and $\ - \ \sqrt[]{3} \ i$. All these four values can be utilized, for $9^{1/4}$ and accordingly values of $X$ can be changed. All combination can be calculated to find the values of $X$. \begin{align*}
X&=SJ^{1/4}S^{-1} \\
A &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\begin{bmatrix}
0 &0 &0 \\
0 &\sqrt[]{3} &0 \\
0 &0 &\sqrt[]{3} 
\end{bmatrix}
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix}\\
&=\begin{bmatrix}
2/\sqrt[]{3} & -1/\sqrt[]{3} & -1/\sqrt[]{3} \\
-1/\sqrt[]{3} & 2/\sqrt[]{3} & -1/\sqrt[]{3} \\
-1/\sqrt[]{3} & -1/\sqrt[]{3} & 2/\sqrt[]{3}
\end{bmatrix}
\end{align*}
\begin{align*}
X&=SJ^{1/4}S^{-1} \\
A &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\begin{bmatrix}
0 &0 &0 \\
0 &- \ \sqrt[]{3} &0 \\
0 &0 &- \ \sqrt[]{3} 
\end{bmatrix}
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix}\\
&=\begin{bmatrix}
-2/\sqrt[]{3} & 1/\sqrt[]{3} & 1/\sqrt[]{3} \\
1/\sqrt[]{3} & -2/\sqrt[]{3} & 1/\sqrt[]{3} \\
1/\sqrt[]{3} & 1/\sqrt[]{3} & -2/\sqrt[]{3}
\end{bmatrix}
\end{align*}
\begin{align*}
X&=SJ^{1/4}S^{-1} \\
A &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\begin{bmatrix}
0 &0 &0 \\
0 &\sqrt[]{3} \ i &0 \\
0 &0 & \sqrt[]{3} \ i
\end{bmatrix}
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix}\\
&=\begin{bmatrix}
2i/\sqrt[]{3} & -i/\sqrt[]{3} & -i/\sqrt[]{3} \\
-i/\sqrt[]{3} & 2i/\sqrt[]{3} &-i/\sqrt[]{3} \\
-i/\sqrt[]{3} & -i/\sqrt[]{3} & 2i/\sqrt[]{3}
\end{bmatrix}
\end{align*}
\begin{align*}
X&=SJ^{1/4}S^{-1} \\
A &= 
\begin{bmatrix}
1 &0 &1 \\
1 &1 &-1 \\
1 &-1 &0 
\end{bmatrix} 
\begin{bmatrix}
0 &0 &0 \\
0 &  - \ \sqrt[]{3} \ i &0 \\
0 &0 &  - \ \sqrt[]{3} \ i
\end{bmatrix}
\begin{bmatrix}
1/3 & 1/3 & 1/3 \\
1/3 & 1/3 & -2/3 \\
2/3 & -1/3 & -1/3 
\end{bmatrix}\\
&=\begin{bmatrix}
-2i/\sqrt[]{3} & i/\sqrt[]{3} & i/\sqrt[]{3} \\
i/\sqrt[]{3} & -2i/\sqrt[]{3} &i/\sqrt[]{3} \\
i/\sqrt[]{3} & i/\sqrt[]{3} & -2i/\sqrt[]{3}
\end{bmatrix}
\end{align*} However, you can see that non of $X$'s have integer coefficients. Any idea where I have messed up something! Any help would be appreciated!","['matrices', 'canonical-transformation', 'linear-algebra']"
2109245,"If $f$ is continuous and $f(f(x))+f(x)+x=0$ for all $x$ in $\mathbb R^2$, then $f$ is linear?","Let $f\in C^0(\mathbb R^2,\mathbb R^2)$, with $f(f(x))+f(x)+x=0$ for all $x$ in $\mathbb R^2$. Is $f$ linear ( : $\forall (x,y) \in (\mathbb R^2)^2,f(x+y)=f(x)+f(y)$ ) ? This problem can be translated to a problem of Geometry of the plane.","['real-analysis', 'linear-algebra', 'functional-equations', 'geometry']"
2109313,Does $\sum q^\sqrt n$ converge?,"Does $\sum q^\sqrt n$ converge? ($q>0$) It is clear that if $q\ge1$ series diverges, but what about $q\in(0; 1)$?",['sequences-and-series']
2109316,How do I change my result into the pascal identity? see details please.,"Use the defintion of binomial theorem to prove the identy.
$$\binom{n}{k} = \binom{n-1}{k}+\binom{n-1}{k-1}$$ The definition of the binomial theorem
$$\binom{n}{k} = \frac{\prod_{i=0}^{k-1}(n-i)}{k!}$$ $$\binom{n-1}{k} = \frac{\prod_{i=0}^{k-1}(n-1-i)}{k!}$$ $$\binom{n-1}{k-1} = \frac{\prod_{i=0}^{k-2}(n-1-i)}{k-1!}$$ $$\frac{\prod_{i=0}^{k-1}(n-1-i)}{k!}+\frac{\prod_{i=0}^{k-2}(n-1-i)}{k-1!}$$ Then I come to the result $$\frac{\prod_{i=0}^{k-2}(n-1-i)(n)}{k-2!(k-1)k}$$ $$\frac{\prod_{i=0}^{k-2}(n-1-i)}{k!} \cdot n$$ What can I do to make this result into $$\frac{\prod_{i=0}^{k-1}(n-i)}{k!}$$","['combinatorics', 'discrete-mathematics']"
2109364,"$\ker S\subset\ker T\Leftrightarrow\exists R, T=RS$","Let $V,W$ be two vector spaces over $F=\mathbb{R}$ or $\mathbb{C}$, such that $W$ is finite dimensional and $S,T\in L(V,W)$. Show that $\ker S\subset\ker T$ if, and only if, there exists a linear operator $R:W\to W$ such that $T=RS$. The converse was easy to verify, since if $Sv = 0$, then $Tv = RSv = R(0) = 0$. I tried to prove the other assertion by constructing such $R$ considering basis for $\operatorname{range} T$ and $\operatorname{range} S$ and send one to the other, but this is as far as I got. Could anyone give me a hint?","['linear-algebra', 'linear-transformations', 'vector-spaces']"
2109376,Derivative of a discrete summation,"Given an infinite list of numbers $\{x_i\}$ is it possible and sensible to compute the first and second derivative of $\sum_{n=1}^{\infty} x_i$? To give more context $x_i$ are real numbers, scores between -10 to 10 of a player in a game, convergence is unknown, and i would use first derivative to know when a player got to his best\worst ""peak"" in his career, interpertation of the second derivative in this context is not clear to me, I will think about it, suggestions are welcome...","['derivatives', 'discrete-mathematics']"
2109390,Proof of the radical expression of $\cos\dfrac {2\pi}{17}$,Question: How would you prove the equation$$\small\cos\dfrac {2\pi}{17}=\dfrac {-1+\sqrt{17}+\sqrt{34-2\sqrt{17}}+2\sqrt{17+3\sqrt{17}-\sqrt{34-2\sqrt{17}}-2\sqrt{34+2\sqrt{17}}}}{16}\tag1$$ I'm not too sure how to prove it and I'm not sure where to begin. I started with $\exp(2\pi i)=\cos(2\pi)+i\sin(2\pi)=1\implies \cos(2k\pi)+i\sin(2k\pi)=\exp(2k\pi)$. But I'm not sure what to do next.,['trigonometry']
2109409,Proof: $\lim\limits_{n\to\infty}\frac{\sqrt[n]{n!}}{n}=e^{-1}$,Prove that $\displaystyle\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}=e^{-1}$ . Here's my solution: Stirling's formula tells us $$\lim_{n\to\infty}\frac{n!}{n^ne^{-n}\sqrt{2\pi n}}=1$$ which implies $$\lim_{n\to\infty}\sqrt[n]{\frac{n!}{n^ne^{-n}\sqrt{2\pi n}}}=1$$ then simplifying the left side we have $$\lim_{n\to\infty}\sqrt[n]{\frac{n!}{n^n}}\lim_{n\to\infty}\sqrt[n]{e^{n}}\lim_{n\to\infty}\frac{1}{\sqrt[2n]{2\pi n}}=e\lim_{n\to\infty}\frac{\sqrt[n]{n!}}{n}=1$$ since $\lim_{n\to\infty}\sqrt[2n]{2\pi n}=1$ . Divide both sides by $e$ and we're done. Is this correct? This is a problem from Problems in Real Analysis by Radulescu and Andreescu. The book gives two other proofs. Thanks!,"['real-analysis', 'solution-verification', 'limits']"
2109419,problem on convergence of series $\sum_{n=1}^\infty\left(\frac1n-\tan^{-1} \frac1n\right)^a$,"Finding the set of all positive values of $a$ for which the series
$$
\sum_{n=1}^\infty\left(\frac1n-\tan^{-1} \frac1n\right)^a
$$ converges.
How will it depend on the the value of a that is its power of the term?
After expanding the arc tan term I get the form of summation of $[ -1/n^3(1/3+1/n^2+......]^a $. now how does it depend on a ?","['convergence-divergence', 'sequences-and-series', 'calculus']"
2109428,Derivative of arcsin,In my assignment I need to analyze the function $f(x)=\arcsin \frac{1-x^2}{1+x^2}$ And so I need to do the first derivative and my result is: $-\dfrac{4x}{\left(x^2+1\right)^2\sqrt{1-\frac{\left(1-x^2\right)^2}{\left(x^2+1\right)^2}}}$ But in the solution of this assignment it says $f'(x)=-\frac{2x}{|x|(1+x^2)}$ I don't understand how they get this. I checked my answer on online calculator and it is the same.,"['functional-analysis', 'functions', 'derivatives']"
2109446,"What is the largest number we can get using $n$ ones, addition, multiplication and brackets?","Let's say we have $n$ ones, i.e. $1,1,\dots,1$ $n$ times and are allowed to add them, multiply and insert brackets wherever we want. What is the largest number we can get for a particular $n$? Is there a closed form or at least an OEIS sequence? For $n=5$ it appears to be $(1+1)(1+1+1)=6$, for $n=6$ it appears to be $(1+1+1)(1+1+1)=9$, for $n=9$ I found $(1+1+1)(1+1+1)(1+1+1)=27$ to be the largest number. But I don't see a way to find a general formula. I guess it would make sense to start from the other end - for each number $N$ find a factorization with the least sum of factors or something like that.","['combinatorics', 'elementary-number-theory']"
2109457,The foot of the height in an orthocentric tetrahedron is the orthocenter of a face.,Consider a tetrahedron with a right-angle corner (like the corner of a cube) at $D$. Prove that the foot of the height from $D$ to the face ABC is the orthocenter of $ABC$. Is this well-known?,['geometry']
2109511,When does $f\in L^1$ vanish?,"I am asking about the sort-of converse to this question : under what additional conditions on $f:\mathbb{R}_+\rightarrow\mathbb{R}$ does the following hold?
$$
\int_{\mathbb{R}_+}f<\infty\implies \lim_{x\rightarrow\infty}f(x)=0
$$ Where the limit above is made in the topological sense: for every increasing diverging sequence $x_n$, $f(x_n)\rightarrow 0$. Surely, by the linked question, the set of such functions includes uniformly continuous ones, but can we expand the set and completely characterize it? Is it the set of BV functions? Absolutely continuous?","['lebesgue-integral', 'measure-theory']"
2109526,"Stirling Number of second kind (unsigned) and binomial coefficient, proof of equality?","I have to prove the following equality concerning Stirling numbers of second kind and the binomial coefficient. And it does not matter which technique I use for my proof. But I personally wanted to prove this by induction. The problem is that I just do not know how to start the induction??? If anybody would give me a little hint how I can start the base case that would be really great ! (greetings from germany) prove for $n,k \in \mathbb{N}_0$
\begin{equation*}
\begin{Bmatrix}
n\\  k
\end{Bmatrix}
=\frac{1}{k!}\sum\limits_{i \in [0,k]}(-1)^{k-i}\binom{k}{i}i^n
\end{equation*}","['binomial-coefficients', 'stirling-numbers', 'induction', 'proof-writing', 'discrete-mathematics']"
2109532,"Compute the following integral: $I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx$","Compute $$I = \int_1^\infty \log^2 \left(1-\frac 1 x\right) \, dx$$
I made the substitution: $$t=\frac 1 x$$ It follows:
$$I=\int_0^1 \frac{\log^2(1-t)}{t^2} \, dt$$
My next step would be to compute the derivative of the following integral with parameter $y$, w.r.t to $y$:
$$F(y)=\int_0^1 \frac{\log^2(y-t)}{t^2} \, dt$$
Or something like this. I think would be a nice solution to use this kind of approach. But I am getting stuck after computing the derivative.","['improper-integrals', 'real-analysis', 'integration', 'definite-integrals']"
2109537,"Understanding relation of $\sin,\cos,\tan$ with $e$","Let us start with the obvious. I know the formulae for angles. I know how to apply them. I also know the formulae involving $e$. But I don't understand what sine has to do with Euler's $e$. (Neither do I for cosine or tangent) If you were to build a course that relies on truly understanding those three functions and to a certain degree their implications, where would you start?","['trigonometry', 'pi']"
2109573,"If $E_P(X) \in $ span($E_{P_i}(X)$), $1 \leq i \leq n$, for all integrable $X$, then is $P$ a convex combination of the $P_i$?","Let $(\Omega, \mathfrak{F}, P)$ be a probability space and let $R = \{P_1,...,P_n \}$ be a finite set of probability measures on $(\Omega, \mathfrak{F})$, each of which is absolutely continuous with respect to $P$. I'm wondering if the following result holds. Suppose that for all random variables $X$ which are integrable with respect to $P$ and $P' \in R$ we have that $E_P(X)$ lies in the interval spanned by $E_{P_i}(X)$, $1 \leq i \leq n$. Then $P$ is a convex combination of the members of $R$. In this paper the result is shown for the case where $\Omega$ is finite. The proof relies on identifying random variables and probability measures with members of $\mathbb{R}^n$ and proceeds by appealing to some basic facts about convex sets. How might one approach this problem for general $(\Omega, \mathfrak{F})$? Addendum. Here is what I've come up with. I would appreciate feedback. We begin with a general measurable space $(\Omega, \mathfrak{F})$ and the space $V$ of all bounded random variables on $(\Omega, \mathfrak{F})$. The space $V$ is a (real) vector space with respect to pointwise addition and scalar multiplication, and if we equip $V$ with the $\sup$-norm, it becomes a Banach space (it is complete). We now identify probability measures $P$ on $(\Omega, \mathfrak{F})$ with continuous linear functionals in the dual space $V'$ via the embedding $P \mapsto \int (\cdot) dP$. Hence, for $X \in V$, we have $P(X) = E_P(X)$, where $E_P$ is expected value with respect to $P$. In order to consider sets of probability measures with certain topological properties, we endow $V'$ with its weak*-topology, which is the topology generated by the evaluation functionals $\lambda_X$ in the double dual $(V')'$, defined by $\lambda_X(\phi) = \phi(X)$, where $X \in V$ and $\phi \in V'$. That is, the weak*-topology is the weakest topology that makes the evaluation functionals continuous, and, moreover, a linear functional $\lambda$ on $V'$ is continuous if and only if it is an evaluation functional. Now, $V'$ is a locally convex topological vector space in the weak*-topology, and the set of probability measures on $(\Omega, \mathfrak{F})$ is weak*-compact in $V'$. It follows that any weak*-closed set of probability measures is weak*-compact. This much is just my attempt to summarize some facts that I learned by studying chapter 14 of Royden's Real Analysis , together with the thought that probability measures can be identified with continuous linear functionals in $V'$ (I also consulted Chapter 3 and Appendices D and E of Walley's Statistical Reasoning with Imprecise Probabilities ). If what I have said so far is acceptable, then I believe that I've answered my question here . Theorem Let $(\Omega, \mathfrak{F}, P)$ be an arbitrary probability space and $R$ an arbitrary set of probability measures on $(\Omega, \mathfrak{F})$ with $|R| \geq 2$. Then, the following two assertions are equivalent. (a) $E_P(X)$ lies strictly inside the interval spanned by $\{E_{P'}(X) \}_{P' \in R}$, i.e.,
$$\inf_{P' \in R} \{E_{P'}(X) \} < E_P(X) < \sup_{P' \in R} \{E_{P'}(X) \}.$$
(b) $P$ is in the interior of the convex weak*-closure of $R$, i.e. $P \in \text{int}(\text{co}R).$ Proof. We note that the convex weak*-closure $\text{co}R$ of $R$ is a convex, weak*-compact subset of $V'$ with non-empty interior, because $|R| \geq 2$. We begin by supposing that $P \notin \text{int}(\text{co}R)$. By a standard separating hyperplane result, there exists a continuous linear functional $\lambda$ on $V'$ and $\alpha \in \mathbb{R}$ such that $\lambda(P) \geq \alpha$ and $\lambda(P') \leq \alpha$ for all $P' \in \text{co}R$. But the continuous linear functionals on $V'$ consist of all and only the evaluation functionals. So for some $X \in V$, we have $\lambda = \lambda_X$, and hence $\lambda_X(P) \geq \alpha \geq \lambda_X(P')$ for all $P' \in \text{co}R$. But by the definition of the evaluation functional, this implies 
\begin{equation}\label{eqn: spanning violation}
E_P(X)=P(X) \geq \alpha \geq P'(X) = E_{P'}(X)
\end{equation}
for all $P' \in \text{co}R$. Hence, $E_P(X) \geq \sup_{P' \in \text{co}R}\{E_{P'}(X) \} \geq \sup_{P' \in R}\{E_{P'}(X) \}$, and (a) is violated. Conversely, suppose that $P \in \text{int}(\text{co}R)$. Then there exists a collection $\{P_i \}_{i=1}^n$, $n \geq 2$, of probability measures in $R$ such that $P = \sum_{i=1}^n \beta_i P_i$, with $\beta_i > 0$ and $\sum_{i=1}^n \beta_i = 1$ (I'm not sure I can justify this step; suggestions here are would be appreciated). For an arbitrary random variable $X \in V$, we have
    \begin{align*}
    E_P(X) &= \int X dP = \int X d(\sum_{i=1}^n \beta_i P_i) \\
    &= \sum_{i=1}^n \beta_i \int X dP_i = \sum_{i=1}^n \beta_i E_{P_i}(X).
    \end{align*}
We note that this calculation relies on the fact that the $P_i$ are finite measures. We now see that $E_P(X)$ is a strict convex combination of the $E_{P_i}(X)$, so (a) is satisfied.","['functional-analysis', 'probability-theory', 'measure-theory', 'convex-analysis']"
2109581,"Intuitively, why are there 4 classical Lie groups/algebras?","I would like to understand the big picture in mathematics.  Lie groups and Lie algebras seem to play a central role in bridging analysis and algebra.  I'm curious to understand, intuitively, why there are 4 classical Lie groups/algebras $A_{n},B_{n},C_{n},D_{n}$ and what are their various natures. $A_{n}$ special linear Lie algebra $\mathfrak {sl}_{n+1}$ $B_{n}$ odd-dimensional special orthogonal Lie algebra $\mathfrak {so}_{2n+1}$ $C_{n}$ symplectic Lie algebra $\mathfrak {sp}_{2n}$ $D_{n}$ even-dimensional special orthogonal Lie algebra $\mathfrak {so}_{2n}$ These writings raise my curiosity: Each of the four types $W, S, H, K$ of simple primitive Lie algebras $(L, L_{0})$ correspond to the four most important types of geometries of manifolds: all manifolds, oriented manifolds, symplectic and contact manifolds. Victor Kac , page 20. Whenever we pick a Dynkin diagram and a field we get a geometry : $A_{n}$ projective, $B_{n}$ $D_{n}$ conformal, $C_{n}$ symplectic. John Baez, in his blog. John Baez, in writing about the octonions, combines the orthogonal Lie algebras/groups so as to have three families. These arise naturally as symmetry groups of projective spaces over $\mathbb{R}$, $\mathbb{C}$, and $\mathbb{H}$, respectively. @JakobH and @arctic-tern discuss that at this post . I imagine that I could study the associated Weyl groups and the infinite families of polytopes (simplexes, cross-polytopes, hypercubes, demicubes) which they are symmetry groups for.  I write my thoughts about that here .  I imagine that I should study the bilinear and sesquilinear forms as described in this Wikipedia article on the classical groups . Concretely, I've been looking at the Dynkin diagrams and learning how they relate to the Cartan matrices and the orthogonal bases used to describe the root systems.  The Dynkin diagrams are, in each case, chains that differ only at one end.  The links in the chain show that two fundamental roots are not orthogonal but differ by 120 degrees. I think the associated Cartan matrix can be thought of as describing the looseness in the root system, that is, the number of times that a root $\alpha$ can be added to $-\beta$ and stay within the root system. The Cartan matrix needs to describe an independent basis and thus needs to be nondegenerate with determinant nonzero, and in fact, we can write the Cartan matrix as $D\cdot S$ where $S$ is positive-definite. We can thereby show that if the Dynkin diagram grows in certain ways, for example, if it forms a cycle, then the determinant will become zero, the system will collapse. The determinant of the $n\times n$ Cartan matrix for a chain grows by the recursion formula $d_{n+1}=2d_{n}-d_{n-1}$, which is to say, $d_{n}=(d_{n-1}+d_{n+1})/2$. In the case of $A_{n}$ the initial conditions are $d_{1}=2$ and $d_{2}=3$ so it grows further $4,5,6...$.  But in the case of $B_{n}$ and $C_{n}$ we have $d_{1}=2$ and $d_{2}=2$ and so the determinants stay constant $2,2,2...$ and likewise for $D_{n}$ we have $4,4,4...$  The point is that the one end of the chain constrains what is possible at the other end of the chain.  The constraint is given by the fact that the wrong link at the end will make the whole system collapse.  It seems like a system for propagating a signal or for describing entanglement. But what I'm trying to understand is how to explain the four possible options for the Dynkin diagrams and what they imply about the characteristics of the Lie algebras/groups.  I am trying to explore the relationship between the Cartan matrices and the fundamental roots $\left \{  e_{i}-e_{i+1} | 1\leq i < n\right \}$ with each diagram distinguished by an additional root as follows: $A_{n}: e_{n}-e_{n+1}$ $B_{n}: e_{n}$ $C_{n}: 2e_{n}$ $D_{n}: e_{n-1}+e_{n}$ Each of these new roots maintains an independent system of roots but the cases $B_{n}, C_{n}, D_{n}$ ""economize"" and thereby compromise the options for expanding the system. Finally, I'm noticing based on what I've read at Wikipedia about positive-definite matrices that the formula $z^{\mathrm{T}}Sz=0$, with $D\cdot S$ equalling the Cartan matrix as discussed above, seems to work like a formula in logic. For example, for $A_{2}$ we get $z_{1}^{2}+(z_{1}-z_{2})^{2}+z_{2}^{2}=0$ forcing $z_{1}=0,z_{1}=z_{2},z_{2}=0$, which means that the vector $z=0$. Whereas for a cycle we could get 
$(z_{1}-z_{2})^{2}+(z_{2}-z_{3})^{2}+(z_{3}-z_{1})^{2}=0$ allowing a degenerate, nonzero solution $z_{1}=z_{2}=z_{3}$. Setting $S = \begin{pmatrix}
k & \frac{-1}{k}\\ 
\frac{-1}{k} & 1
\end{pmatrix}$ to handle the cases $A_{2} (k=1), B_{2} (k=2), G_{2} (k=3)$, we get the equation $(1-\frac{k}{4})z_{2}^2+k(z_{1}-\frac{z_{2}}{2})^2=0$ which forces us to have solutions $z_{2}=0$ and $z_{1}=z_{2}/2$ and also shows that we we will have degenerate solutions for $k\geq 4$. I have yet to find an intuitive explanation why there are 4 classical Lie algebras/groups.  I would very much appreciate one.  Otherwise, I suspect an answer might be found by working from the most concrete vantage point rather than the most abstract one. Thank you for helping me.","['symmetry', 'classical-groups', 'geometry', 'lie-algebras', 'lie-groups']"
2109585,Solving trigonometric equations like $1-s\sin^2\theta=a\sin^6\theta+b\cos^6\theta$,"For what values of $s\in\mathbb{R}$ does the following identity hold for all $\theta\in\mathbb{R}$: $$1-s\cos^2\theta\sin^2\theta=a\sin^6\theta+b\cos^6\theta\tag{1}$$ for some $a,b\in\mathbb{R}$? In other words, for what values of $s$ can the expression $1-s\cos^2\theta\sin^2\theta$ be simplified to $a\sin^6\theta+b\cos^6\theta$ for some $a$ and $b$ not depending on $s$ or $\theta$? I know that $s=3$ holds, since we have the identity: $$1-3\cos^2\theta\sin^2\theta=\sin^6\theta+\cos^6\theta$$ but I would like to know (purely out of interest) if there are other values of $s$ for which it holds. I would also like to know for what if any values of s the following identity holds: $$1-s\cos^2\theta=a\sin^6\theta+b\cos^6\theta\tag{2}$$ for some $a,b\in\mathbb{R}$; and I would also like to know for what if any values of $s$ we have the following identity: $$1-s\sin^2\theta=a\sin^6\theta+b\cos^6\theta\tag{3}$$ I am not aware of any theory that would help me solve trigonometric equations like $(1)$, $(2)$ and $(3)$ except by trial and error, but I would love to know if there is a method. I assume that there are only particular values of $s$ for which equations like these can hold, but I can not see any way of finding them. The context of this question relates to a question I recently asked here, where I was attempting to evaluate integrals of the form $\int_0^\infty\left(a+\sin^2{\theta}\right)^{-\frac{1}{3}}\;d\theta$; there are complicated reasons as to which values of $a$ give nice values, but I noticed that the simple method I was using could possibly be extended for certain values of $a$ if I knew the solutions to equations $(1)$, $(2)$ and $(3)$ above. Thus I am wondering if anybody knows how to solve these equations",['trigonometry']
2109593,Cartesian products and power sets,"another discrete math question. I was watching this video about Cartesian products, power sets and cardinality when a thought occurred to me. If the set S were arbitrary, would its Cartesian product be a subset of the power set? In other words would this statement be true: $S \times S \subseteq \mathcal{P}(S)$ I've looked around on the site to find a similar question, but any questions I found involved the Cartesian product of two or more sets, not one. I think the statement is false because with an arbitrary set, we don't know what the elements are, so there's a chance the statement is false by default. Any thoughts?","['logic', 'elementary-set-theory', 'discrete-mathematics']"
2109595,An application of valuative criterion for properness,"I want to show: Let $R$ be a Dedekind domain with fraction field $K$. Let $X$ be a scheme and $X \to \operatorname{Spec}R$ a proper morphism. Show that the natural map $$X(\operatorname{Spec}R)\to X (\operatorname{Spec}K)$$ is a bijection. Here $X(Y):=\text{Hom}(Y,X)$. I know that if R is a valuation ring, then this is exactly the valuative criterion for properness. So I consider the localization of $R$ at every maximal ideal $m$, try to show that the data of $X(\operatorname{Spec}R)$ is equivalent to the data of $X(\operatorname{Spec}R_m)$ for all $m$ with some conditions. But it seems not so clear. Any hint or reference would be helpful.","['valuation-theory', 'algebraic-geometry', 'commutative-algebra']"
