question_id,title,body,tags
4215342,Correct measure in concentration inequalities or hypothesis testing,"In most discussions of concentration inequalities or calculations of rejection region in hypothesis testing, the measure used is left vague. For example, for independent random variables $X_1, \ldots, X_n$ satisfying $0 \le X_i \le 1$ , Hoeffding's inequality is usually stated (Wikipedia link) as $$\text{P}\left(\overline{X} - \mathbb{E} \overline{X} \ge t\right) \le e^{-2nt^2} \tag{1}$$ where $\overline{X} = \frac{1}{n}(X_1 + \cdots + X_n)$ and $t \ge 0$ . To make this concrete, suppose $(\Omega, \mathcal{F}, \mathbb{P})$ is the underlying probability space. Then do we treat $\overline{X}$ as a function on $\Omega$ or on $\Omega^n$ , because that determines if "" $P$ "" in $(1)$ is $\mathbb{P}$ or $\mathbb{P}^{\otimes n}$ (product measure on $(\Omega^n, \mathcal{F}^{\otimes n})$ . I think $\overline{X}$ should be a function on $\Omega$ looking at the proof of the inequality. But now if you consider the general McDiarmid's inequality which states (or check Ledoux's book The Concentration of Measure Phenomenon ): Let $(K, \mathcal{A}, \mu)$ be a probability space, let $L > 0$ be a constant, and let $f \colon K^n \to \mathbb{R}$ be a measurable function (for the product $\sigma-$ algebra $\mathcal{A}^{\otimes n}$ ) which is $L-$ Lipschitz for the normalized Hamming metric (i.e., $d(x,y) = \frac{1}{n}\left|\{i=1,\ldots,n : x_i \neq y_i\}\right|$ for $x,y \in \Omega^n$ ), then $$\mu^{\otimes n} \left\{ f - \int_{K^n} f \,\mathrm{d}\mu^{\otimes n}\ge t  \right\} \le e^{-2nt^2/L^2}$$ Let $K = [0,1]$ , let $X_1, \ldots, X_n$ be i.i.d random variables (mapping $\Omega$ to $K$ ) with distribution $\mu$ (i.e., $\mu = \mathbb{P} \circ X_i^{-1}$ ), and let $f(x_1, \ldots, x_n) = \frac{1}{n}(x_1 + \cdots + x_n)$ . Then clearly $f$ is $1-$ Lipschitz. Denote by $X$ the function $(X_1, \ldots, X_n) \colon \Omega^n \to K^n$ . Change of variables implies $$\int_{K^n} f \, \mathrm{d}\mu^{\otimes n} = \int_{\Omega^n} f(X) \, \mathrm{d} \mathbb{P}^{\otimes n} = \int_\Omega X_1 \, \mathrm{d}\mathbb{P} = \mathbb{E}(X_1)$$ and we can write $$
f(X) = \overline{X}
$$ Therefore, change of variables again and McDiarmid's inequality implies $$\mathbb{P}^{\otimes n} \left\{ \overline{X} - \mathbb{E}(X_1) \ge t \right\} \le e^{-2nt^2} \tag{2}$$ And now compare equations $(1)$ and $(2)$ . Why the discrepancy in measure?","['measure-theory', 'concentration-of-measure', 'statistics', 'hypothesis-testing', 'probability-theory']"
4215353,"Showing that $x^n-ax^{n-1}-bx-1$, for non-negative integers $n\geq3$, $a\geq1$, $b$, has no double roots","Let $f(x)=x^n-ax^{n-1}-bx-1$ be a polynomial, where $n, a, b$ are non-negative integers, with $n\geq 3$ and $a\geq 1$ . I would like to prove that this polynomial does not have any double root. (This is supported with a Mathematica routine for $(n,a,b)\in [3,10]\times [1,50]\times [0,50]$ ). In fact, I was able to show that the only possible double root must be a real negative number $x_0$ (actually a quadratic irrational number) which can happen only for the case $n$ odd. Of course, I tried to study the system $f(x)=f'(x)=0$ which lead to the quadratic equation $$
b(n-1)x^2+(ab-a(n-1)+n)x-a(n-1)=0.
$$ (Indeed from $f(x)=0$ and $f'(x)=0$ , we obtain $x^{n-1}(x-a)=bx+1$ and $x^{n-2}(nx-(n-1)a)=b$ . It is easy to see that $f(x)$ does not have rational roots - rational root theorem - so we can combine $$
x^{n-1}=\frac{bx+1}{x-a}\ \mbox{and}\ x^{n-2}=\frac{b}{nx-(n-1)a}
$$ to obtain the previous quadratic relation). Also it is possible to prove that such a double root must satisfy a relation like $$
ax^{n-1}+b(n-1)x+n=0.
$$ So, I tried to use some arithmetic behaviour of power of quadratic irrationals of the form $x_0=(r+s\sqrt{\xi})$ , where $r,s\in \mathbb{Q}$ and $\xi$ is a non-square positive integer. However, without success. Of course, it is possible to impose conditions on $a$ and $b$ for which we do not have double roots (as for example, $b>a+2$ forces $f(-1)>0$ and so we have exactly two negative roots, counted with multiplicity. Here we used Descartes' sign rule). Thus, I would like to ask you guys for some suggestion. Thanks!","['roots', 'calculus', 'polynomials', 'derivatives', 'quadratics']"
4215363,"Euler's method, Multiple choice does not match my answer.","This is the original question. Use Euler's method with h=0.2 to estimate y when x =1 if $y' = (y^2-1) /2 $ and y(0) = 0 A. 7.690 B. 12.730 C. 13.504 D. 90.676 My answer follows. n= 5, h= 0.2
a= x_0= 0, b=1, y0= 0 I'm using the formula $y_{(n+1)} = y_n + h * y'$ and generated the following table. Note this table shows the answer as $-0.47141$ because $y_{n+1}$ is on the previous line of the table below.
However, this doesn't match any of the given answer choices.
This is an employment test covering Advanced Placement Calculus BC, so this is all the context I have. Where is the mistake? Are there other variations called ""Euler's method"" which generate different answers? x h y y^2 y' next y 0 0.2 0.00000 0.00000 -0.50000 -0.10000 0.2 0.2 -0.10000 0.01000 -0.49500 -0.19900 0.4 0.2 -0.19900 0.03960 -0.48020 -0.29504 0.6 0.2 -0.29504 0.08705 -0.45648 -0.38634 0.8 0.2 -0.38634 0.14925 -0.42537 *  -0.47141 * 1 0.2 -0.47141 0.22223 -0.38889 -0.54919","['calculus', 'ordinary-differential-equations', 'eulers-method']"
4215365,Can I use the distributive law to pull this term out of a double summation?,"In Concrete Mathematics , Graham Knuth, and Patashnik use the notation $[P(x)]$ to denote $1$ if $P(x)$ is true and $0$ if $P(x)$ is false, which lets you write the sum $$\sum_{1\leq k \leq n} a_k$$ as $$\sum_k a_k[1\leq k \leq n]$$ which sums over all integers $k$ , but only ""counts"" those in the desired range. But I get confused once multiple indices are involved. is this reasoning valid? \begin{align}
\sum_{1\leq j\leq k \leq n} a_ja_k &= \sum_{j,k} a_ja_k[1\leq j \leq k \leq n] \\
&= \sum_{j,k} a_ja_k[1\leq j \leq n][j\leq k \leq n]\\
&= \sum_j\left(\sum_k a_ja_k [1\leq j \leq n][j\leq k \leq n]\right)\\
&= \sum_j \left(a_j[1\leq j\leq n]\sum_k a_k [j\leq k \leq n]\right)\\
&= \sum_{1\leq j \leq n} \left(a_j\sum_{j\leq k\leq n} a_k\right)
\end{align} I'm specifically wondering about the second-to-last equality. The book says you can use the distributive law to pull a term out of a sum as long as the term doesn't ""depend on the index of the sum"". In the second to last equality, I'm pulling out the term $a_j[1\leq j \leq n]$ from the inner sum. The index variable (I hope that's the right name for it) of the inner sum is $k$ and the term I'm pulling out doesnt contain $k$ . But the lower bound for the inner sum is $j$ and the term I'm pulling out does contain a $j$ . So is this legal?","['summation', 'combinatorics', 'discrete-mathematics']"
4215399,Probability that sum of digits of random numbers matches.,"Given two random 9-digit numbers (with the standard assumptions), what is the probability that the sum of their digits match? I saw this , and it seemed extremely relevant. Also, how does the answer change if we work in different number systems, or allow for ""digits"" which range from 35 to 54 (e.g., 38 is allowed to be a digit)? Edit for context : My motivation is strictly intellectual curiosity. My first attempt to solve this problem didn't get very far; I thought the answer might be solved by simple counting and elementary probability. The fact that we are working with sums, and not just the digits themselves though, seemed to complicate things for me. I linked a thread because the general topic of my question (about the sums of digits of random numbers) is directly addressed in that thread (but my exact question is not), showing that I did research and am trying to in good faith spur discussion. I think my question is interesting enough to warrant a thread despite me being clueless (for the most part) as to how to proceed.","['statistics', 'discrete-mathematics', 'probability']"
4215406,How to calculate $\mathbb{E}\left(\varPhi^{2}\left(\frac{W_{s}}{\sqrt{1-s}}\right)\right)$?,"How can we calculate $$\mathbb{E}\left(\varPhi^{2}\left(\frac{W_{s}}{\sqrt{1-s}}\right)\right)$$ where $W$ denotes a Wiener process, and $\varPhi$ is the CDF of the standard normal random variable... If $\varphi$ denotes the PDF of the standard normal variable, then $$\mathbb{E}\left(\varPhi^{2}\left(\frac{W_{s}}{\sqrt{1-s}}\right)\right)=\int_{\mathbb{R}}\varPhi^{2}\left(\frac{x}{\sqrt{1-s}}\right)\frac{1}{\sqrt{2\pi s}}e^{-\frac{x^{2}}{2s}}dx=\int_{\mathbb{R}}\varPhi^{2}\left(\frac{x}{\sqrt{1-s}}\right)\frac{1}{\sqrt{s}}\varphi\left(\frac{x}{\sqrt{s}}\right)dx=...?$$ I can't continue...","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
4215431,"Is it just a coincidence that the solution to $y=x$, $y=mx+b$ ($m<1$) is the sum of an infinite geometric series with first term $b$ and ratio $m$?","The solution for the system of linear equations of $y = x$ and $y = mx + b$ is $$x=y= \frac{b}{1-m}.$$ I noticed that this is also the sum of an infinite geometric series, where the first term is $b$ and common ratio is $m$ (granted $m$ is less than $1$ ): $$ \frac{b}{1-m} = b + bm + bm^2 + \cdots bm^n + \cdots$$ Is this all a big coincidence or is there some deeper meaning to this relationship?","['algebra-precalculus', 'geometric-series']"
4215441,"complex Lie groups, real Lie groups, and forgetful functors","In the answer to this question What does it mean to be a real Lie group , there is some nice discussion. My questions are: I understand that the complex Lie algebra $\mathfrak{sl}(2,\mathbf{C})$ as the complexification of both of the real Lie algebra $\mathfrak{su}(2)$ and $\mathfrak{sl}(2,\mathbf{R})$ . I understand that the complex Lie group $SL(2,\mathbf{C})$ as the complexification of both of the $SU(2)$ and $SL(2,\mathbf{R})$ . In this case, am I correct to see the two different real Lie groups (say $SU(2)$ and $SL(2,\mathbf{R})$ ) may have the same complexification to the same (isomorphic) complex Lie group (say $SL(2,\mathbf{C})$ )? Are the more than two (or three or more) different real Lie groups having the same complexification to the same (isomorphic) complex Lie group? Reversely, am I correct to say that there are two (or more) different ways to apply the different forgetful functors to go from a complex Lie group to different real Lie groups? About this: ""Do there exist two complex Lie groups which, after applying the forgetful functor, become isomorphic real Lie groups?"" What are some examples? About this: ""Does there exist a real Lie group which is not isomorphic, as a real Lie group, to some complex Lie group with the forgetful functor applied?"" It is easy to get a real Lie group which as a smooth manifold that has an odd real dimension, such as $SO(2n+1)$ and $SU(2n+1)$ . So I suppose this odd real dimension smooth manifold cannot be obtained from the complex Lie group (of a complex smooth manifold ) with the forgetful functor applied? Are there alternative possibilities than what I say? Many thanks (in advance) for answering!","['lie-algebras', 'smooth-manifolds', 'differential-topology', 'lie-groups', 'differential-geometry']"
4215443,Linearity of Variance and Expected value,"I know Expected value has the property of linearity $E(X+a)=E(X) + a$ , but it also seems to hold for $E(X^3+a)$ . But Variance also has the property $V(X+a)=V(X) + 0$ , but it does not hold for $V(X^3+a)=V(X^3) + 0$ . I discovered this from a question that asked to find the expected value adn variance for $Y = X^3 + 2$ given a distribution for $X$ . $$\begin{array}{|r|r|r|r|r|}\hline x&-1&0&1&2\\\hline p(x)&0.1&0.4&0.3&0.2\\\hline\end{array}$$ The table shows the distribution for $X$ , so I worked out $E(Y) = E(X^3 - 2) = 1.8 - 2 =-0.2$ . For variance I am thinking $V(Y) = V(X^3 - 2) = E(X^{3\times2}) - E(X^3)^2 - 0 =E(X^6)-0.2^2$ from which I get 11.16","['expected-value', 'statistics', 'variance']"
4215456,Growth rate of $(\log{n})^{3n}$ and $(n)^{\frac{n}{3}}$,"To find the growth rate of $(\log{n})^{3n}$ and $(n)^{\frac{n}{3}}$ , I took the log of both: $(\log{n})^{3n}$ : $$\log (\log{n})^{3n} = 3n \log{\log{n}} \in O(n \log n)$$ but the answer I have says it's $O(n)$ . $(n)^{\frac{n}{3}}$ : $$\log (n)^{\frac{n}{3}} =\frac{n}{3} \log{n} \in O(n \log n) $$ also the answer I have says it's $O(n)$ . What do you think please?","['elementary-number-theory', 'discrete-mathematics']"
4215531,Integrate $I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx$ [duplicate],"This question already has answers here : Evaluating $\int_0^\infty e^{-ax}\frac{\sin{(bx)}}{x}dx $ (2 answers) Show that $\int_0^\infty e^{-x}\cos x \text{dx}=\int_0^\infty e^{-x} \sin x \text{dx}$ (5 answers) Closed 2 years ago . If $a > 0$ then show $$\int_0^\infty \frac{e^{-ax}\sin bx}{x} \mathrm dx=\tan^{-1}\frac{b}{a},$$ hence also show that $$(i) \quad \int_0^\infty \frac{\sin bx}{x}\mathrm dx=\frac{\pi}{2} \text{ when } b>0,$$ $$(ii) \quad\int_0^\infty \frac{\sin bx}{x}\mathrm dx=-\frac{\pi}{2} \text{ when } b<0.$$ $$I=\int_0^\infty \frac{e^{-ax}\sin bx}{x}\mathrm dx .$$ $$\frac{dI}{db}=\int_0^\infty \frac{e^{-ax}}{x} \frac{\partial}{\partial b} (\sin bx)\mathrm dx 
 =\int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx 
 =\frac{a}{a^2+b^2}.$$ But, how is $$e \int_0^\infty \frac{e^{-ax}}{x}x\cos bx\mathrm dx = \frac{a}{a^2 +b^2}.$$ I was solving the problem by differentiating inside the integral sign. There's an $x$ in denominator. So, It's not my answer . Note : I am doing differentiation under integral sign. Question: Show that $\int_0^\infty e^{-ax}\cos bx\mathrm dx=\frac{a}{a^2 +b^2}$ (recent linked question didn't prove it. They were just telling to prove what I will get if I integrate by parts. And, another one prove my question is equal to 1/2. Thats why I am rejecting them. There's no possible answer which matches with mine.)","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'derivatives']"
4215566,Finding the number of bijections $f:\mathbb{N} \to \mathbb{N}$ such that $\sum_{i=1}^{\infty}\frac{f(n)}{n^2} < \epsilon$ [duplicate],"This question already has answers here : Infinite series, injective function and rearrangement inequality (3 answers) Closed 2 years ago . I started by taking the example $f(n) = n$ which is the identity bijection and $\sum{\frac{f(n)}{n^2}} = \sum{\frac{1}{n}} > \infty $ Suppose we change the values of the bijection at finitely many places(say , $i_1,i_2,\cdots i_k$ ) then we would still have the series $\sum_{i=i_{k+1}}^{\infty}{\frac{f(i)}{i^2}} = \sum_{i=i_k}^{\infty}\frac{1}{i_k}$ converging to $\infty$ Suppose we pick up a bijection in such a way so that $f(n) < n$ at infinitely many places then there are infinitely many places at which $f(n) >n$ then we can pick up a subsequence say $(n_k)$ such that $f(n_k) > n_k , \forall k \ge 1$ . Then $\sum_{i = n_k}^{\infty}\frac{f(n_k)}{n_k^2} > \sum_{i = n_k}^{\infty} \frac{1}{n_k} > \infty $ . So from here I tried to conclude that $\sum\frac{f(n)}{n^2}$ converges to $\infty$ . Hence from here I tired to conclude that there doesnot exist any bijection $f(n)$ such that the $\sum{\frac{f(n)}{n^2}}$ converges . How do I fix my reasoning?","['functions', 'solution-verification', 'natural-numbers', 'real-analysis']"
4215574,"Are all sets, ordered sets?","The definition of an ordered set according to W. Rudin in his book, Principles of Mathematical Analysis is: An ordered set is a set S in which an order is defined He also defined order in his book: Let S be a set. An order on S is a relation, denoted by <, with the following two properties: If x, y ∈ S then one and only one of x < y, x = y, x > y is true. If x, y, z ∈ S and x < y and y < z then x < z. I can't think of any set that doesn't have an order. Is there any set that is not ordered?
A counterexample would be very helpful.","['analysis', 'real-analysis']"
4215606,Hodge star without a metric,"The standard definition of the Hodge star is as follows: the Hodge dual of a differential $p$ -form defined on an $n$ dimensional manifold $M$ , $\alpha \in \Omega^p(M)$ , is the unique form $\star \alpha \in \Omega^{(n-p)}(M)$ that satisfies \begin{equation}
\beta \wedge \star \alpha = \langle \beta, \alpha \rangle \omega
\end{equation} for any differential $p$ -form $\beta$ . Of course, implicit in this definition, there is a choice of metric on $M$ , which induces an inner product on $p$ -forms, as well as a choice of volume form $\omega$ , although from what I gather this tends to be taken to be the volume form induced by the metric. The Hodge dual is then an isomorphism $\star \colon \Omega^p(M) \to \Omega^{n-p}(M)$ , which reflects the fact that these spaces have the same dimension. I noticed, however, that really the Hodge star is built out of two steps: the choice of a volume form gives you an isomorphism between $(n-p)$ -forms and $p$ -vector fields, and the choice of a metric gives an isomorphism between $p$ -vector fields and $p$ -forms. I worked through the details in the case $p=1$ . First, given a volume form $\omega$ , we have what one could call a primitive Hodge star, $\ast \colon \Gamma(TM) \to \Omega^{n-1}(M)$ which is just an interior product: $\ast X = \iota_X \omega$ . The other isomorphism is just the musical isomorphism $\sharp \colon \Omega^1(M) \to \Gamma(TM)$ , which depends on the choice of a metric of course. I claim that the composition of the two is the Hodge star, namely, $\star \alpha = \ast \sharp \alpha = \iota_{\sharp \alpha} \omega$ . And indeed, using the fact that the interior product is a derivation I can show this, since for any 1-form $\beta$ \begin{align}
\beta \wedge (\ast \sharp \alpha) &= \beta \wedge (\iota_{\sharp \alpha} \omega) \\
&=  (\iota_{\sharp \alpha} \beta) \wedge \omega - \iota_{\sharp \alpha}(\beta \wedge \omega) \\
& = \beta(\sharp \alpha) \omega \\
&= g^{-1}(\beta, \alpha) \omega
\end{align} where $\beta \wedge \omega$ vanishes because it is a top form and I use $g^{-1}$ for the induced inner product on 1-forms. It also works nicely in coordinates, since if I introduce the $(n-1)$ -forms $\omega_a = \iota_{\frac{\partial}{\partial x^a}} \omega$ , then \begin{equation}
\ast X = \iota_X \omega = X^a \iota_{\frac{\partial}{\partial x^a}} \omega = X^a \omega_a
\end{equation} My question is whether this actually generalizes to higher dimensions (I am thinking of an isomorphism $\Gamma(\Lambda^p TM) \to \Omega^{n-p}(M)$ determined by a choice of volume form) and whether it might be useful to think of the bit of Hodge star determined only by a volume form in contexts in which a metric might not be readily available.","['hodge-theory', 'duality-theorems', 'differential-forms', 'differential-geometry']"
4215621,An embedding of $U(n)\to \text{Spin}^c(2n)$,"Let $(V,\langle,\rangle)$ be an $2n$ -dimensional real inner product space and consider its Spin $^c$ group $\text{Spin}^c(V)\subset Cl(V)\otimes_{\Bbb R} \Bbb C$ . Suppose there is a compatible (orthogonal) almost complex structure $J:V\to V$ . We can then view $V$ as an $n$ -dimensional complex vector space with a hermitian inner product $h$ given by $h(u,v)=\langle u,v\rangle -i\langle Ju,v\rangle$ . In particular we can consider the unitary group $U(V)$ . Define a map $\rho:U(V)\to \text{Spin}^c(V)$ as follows: Given $A\in U(V)$ , we can choose a unitary basis $v_1,\dots,v_n$ for $V$ such that $Av_k=e^{i\theta_k}v_k$ . Put $$\rho(A)=e^{(i\sum_k \theta_k)/2}\prod_{k=1}^n \left(\cos \frac{\theta_k}{2}+\sin\frac{\theta_k}{2}\cdot v_kJv_k \right)$$ In p.393 of Lawson & Michelson's book Spin Geometry , it is asserted that this map is a well-defined continuous group homomorphism, but I can't see why. For well-definedness, we have to care about two choices: the choices of $\theta_1,\dots,\theta_n$ , and the choice of basis. Clearly $\rho(A)$ is independent of the choice of the $\theta_k$ 's. Also it was quite easy to show that $\rho(A)$ is independent of the order of $v_1,\dots,v_n$ , because $v_kJv_k v_{k+1}Jv_{k+1}=v_{k+1}Jv_{k+1} v_kJv_k$ . So now if $\{v_1,\dots,v_n\}$ and $\{w_1,\dots,w_n\}$ are two unitary bases with $Av_k=e^{i\theta_k}v_k$ and $Aw_k=e^{i\psi_k}w_k$ , then we may assume $\theta_k=\psi_k$ for all $k$ . If the $\theta_k$ 's are all distinct (i.e. $A$ has $n$ distinct eigenvalues), then it is easy to see that $\rho(A)$ is well-defined. But I can't handle the case where $A$ has an eigenvalue with eigenspace $\dim>1$ . Also I can't see why $\rho$ is a continuous group homomorphism. Any helps will be very appreciated. Edit: There is a double cover $p:\text{Spin}^c(V)\to SO(V)\times S^1$ . Consider the natural map $f:U(V)\to SO(V)\times S^1$ given by $f(A)=(A,\det A)$ . Assuming $\rho$ is well-defined, I have shown that $\rho$ is a lift of $f$ , i.e. $p\rho=f$ . On the other hand, by covering space theory, there is a unique lift $\tilde{f}:U(V)\to SO(V)\times S^1$ of $f$ such that $\tilde{f}(\text{id})=1$ . Thus, if $\rho$ is continuous, then we must have $\rho=\tilde{f}$ .","['spin-geometry', 'abstract-algebra', 'linear-algebra', 'group-theory', 'clifford-algebras']"
4215636,How many quadrilaterals can be made using 4 points on a 5x5 dot grid?,"This is from some general combinatorics work I have been doing.
To specify, the dot grid looks something like this (excuse the rudimentary approach): $$\begin{array}{ccccc}
\bullet & \bullet & \bullet & \bullet & \bullet\\
\bullet & \bullet & \bullet & \bullet & \bullet\\
\bullet & \bullet & \bullet & \bullet & \bullet\\
\bullet & \bullet & \bullet & \bullet & \bullet\\
\bullet & \bullet & \bullet & \bullet & \bullet
\end{array}$$ While I can get around finding the number of triangles and rectangles that can be made with 4 dots, what has stumped me is general quadrilaterals. Thanks.","['combinations', 'combinatorics']"
4215652,Why is $\frac{7 \cosh(\sqrt 6)}{13}$ near $\pi$?,"$\frac{7 \cosh(\sqrt 6)}{13} = 3.1415926822 ...$ $\pi = 3.1415926535 ... $ Why are these numbers so close to each other? Is this just a coincidence? P.S. About ten days ago, I saw this question on twitter, but no one has answered it.","['calculus', 'pi', 'approximation', 'hyperbolic-functions']"
4215740,Finding a number that equals another number that references it,"Apologies in advance if I have not formatted this problem correctly. Context : I need to find a way to calculate a number that will equal a service fee applied to a product, taking into account the service fee will also be applied to this number. The system is sort of a middle man between a client and supplier. Currently the the supplier takes the cost of the service charge. We need to be able to allow the client to take this charge. And to do that it it needs to be added as a line item on the invoice, the problem is the service charge will be applied to the line item also as it is calculated using the total transaction value. So in the case above a £2 line item could be added to offset the original service charge, but the 2% would also be applied leaving that (£0.04) unaccounted for. Example :
The cost of a product is £100, the an service fee would be (2%) £2. In this case the number couldn't be £2 because a 2% fee would also be applied to the £2 leaving £0.04. When I first looked at this problem I originally thought the value could be: ((Cost of Product) * 0.02) + ((Cost of Product) * 0.02) * 0.02) But this is wrong also as there is still a small amount remaining. Is there an easy way to calculate what the value should be?",['functions']
4215776,Compactness of the set of functions majorized by an increasing function,"Let $L$ be the set of all integrable real-valued functions defined over $[0,1)$ . The set $L$ is a topological vector space (TVS) with addition of functions $f+g$ and multiplication of a function with a real number defined in the usual way. Consider a nonnegative increasing function $f\in L$ and the subset $$S=\left\{g\in L: g \textrm{ is nonnegative and increasing,}\\ \int_{z}^1 g(x)\leq \int_z^1 f(x), \forall z\in[0,1), \\ \int_{0}^1 g(x)= \int_0^1 f(x)\right\}.$$ My question is: is $S$ a compact set? why? My idea: define a metric $M$ by defining the distance between two functions $g,h$ as $\int_0^1 |g(x)-h(x)|dx$ , and then showing that $L$ is compact ""with respect to"" this metric. A relevant question: The TVS $L$ is metrizable. Suppose we prove that $L$ is compact ""with respect to"" a metric $M$ . This TVS might not be compact ""with respect"" to some metric $N$ though. Then compactness is not a property of the TVS; it is rather a property of a pair (TVS, Metric). If this is right, then, how should one interpret theorems that simply mention compactness, but no metric. For example Bauer's maximum principle requires a convex and compact set, but would compactness  with respect to any metric be sufficient for this principle to hold?","['general-topology', 'topological-vector-spaces', 'vector-spaces', 'compactness']"
4215819,Is closure of intersection open in intersection of closures for open subsets?,"Let $X$ be a topological space, and $A, B \subset X$ be open in $X$ . Is $\overline{A \cap B}$ open in $\overline{A} \cap \overline{B}$ ? Background: It can be shown that $\overline{A \cap B} \subset \overline{A} \cap \overline{B}$ . In general, these sets are different, e.g. if $A = (0, 1)$ and $B = (1, 2)$ are open intervals in $\mathbb{R}$ , then $\overline{A \cap B} = \emptyset$ , while $\overline{A} \cap \overline{B} = \{1\}$ . For our question, $\emptyset$ is open in $\{1\}$ . Without the openness requirement for $A$ and $B$ , one can give the following counter-example. Let $A = \mathbb{Q} \cup [0, 1]$ , and $B = \mathbb{R} \setminus \mathbb{Q}$ . Then $\overline{A \cap B} = [0, 1]$ , and $\overline{A} \cap \overline{B} = \mathbb{R}$ . Suppose $\overline{A} \cap \overline{B}$ is connected, and $\overline{A \cap B} \neq \emptyset$ . Then $\overline{A \cap B}$ is open in $\overline{A} \cap \overline{B}$ if and only if $\overline{A \cap B} = \overline{A} \cap \overline{B}$ .",['general-topology']
4215824,"Define ""Almost Always Less Than""","Is there a formal definition of ""almost always less than"" or ""almost always greater than""? I think one could define it using probabilities but not sure how to go about it. If one could show the following, then I think you could say $X$ is almost always less than a value $x$ . Is there other ways of going about this? $$
P(X<x)=1
$$",['probability']
4215825,Using the law of large numbers to calculate an integral,"Let $f:[0,1]\to\mathbb{R}$ be continuous. Prove $$
\lim_{n\to\infty}\int_0^1\int_0^1\cdots\int_0^1f\left((x_1x_2\cdots x_n)^{1/n}\right)dx_1dx_2\cdots dx_n = f\left(\frac1e\right)
$$ My idea so far is to use uniform distributions to calculate this. Let $X_1,X_2,\dots$ be independent Uniform $(0,1)$ variables. My idea is to somehow use that this integral is equal to $$
\lim_{n\to\infty}\mathbb{E}\left((X_1X_2\cdots X_n)^{1/n}\right).
$$ I think I can somehow use the fact that $\frac1n\sum_{i=1}^n\log X_i\overset{a.s.}{\to}\mathbb{E}\log X_i=-1$ .
I think I have some of the ideas, but my question is how can I actually put together these ideas in a rigorous way prove this? Do I need the full power of the SLLN, or would I somehow be able to just use the WLLN? I also think maybe I will have to use the dominated convergence theorem, but I'm not sure. I'm struggling with self doubt on this question. Any help appreciated!",['probability']
4215835,ODE-separation of variables proof,"Let's assume an ODE of the type $$y'=g(x)h(y(x)),$$ where $g:I\to\mathbb{R}$ , $h:J\to\mathbb{R}$ are continuous functions and ${\xi\choose\eta}\in I\times J$ . In the case of $h(\eta)=0$ the solution is clear. So let's assume $h(\eta)\neq 0$ . Then, we know that by solving the equation: $$
\int_{\eta}^y\frac{1}{h(s)}ds=\int_{\xi}^xg(t)dt
$$ with respect to $y$ delivers a solution of the ODE. In our lecture the professor has proven this statement by constructing another function, namely $$H(u):=\int_{\eta}^u\frac{1}{h(s)}ds$$ and then showing its invertibility. After some additional steps he finally comes to the conclusion that $$y:=H^{-1}\left(\int_{\xi}^xg(t)dt\right)$$ is a (unique) solution of the ODE. In the literature this seems a standard way of proving the theorem. However, I was wondering why we cannot simply say that in the case of $h(\eta)\neq 0$ it directly follows from the implicit function theorem? EDIT: Some more details regarding the implicit function theorem: Let be $y(\xi)=\eta$ and $F:\mathbb{R}^2\to\mathbb{R}$ , where $$
F(x,y)=\int_{\eta}^y\frac{1}{h(s)}ds-\int_{\xi}^xg(t)dt.
$$ Now we look at $F(x,y)\overset{!}{=}0$ and see that $F(\xi,y(\xi))=0$ . All assumptions of the implicit function theorem are satisfied (I have skipped the details), so we know there exists a function $f$ such that $F(x,f(x))=0$ . Further, we know how the derivative of $f$ looks like, namely $f'(x)= (-1) \left(\left(\int_{\eta}^{f(x)}\frac{1}{h(s)}ds\right)'\right)^{-1}\left(-\int_{\xi}^xg(t)dt\right)'=f(x) h(f(x)).$ Hence, the implicit function $f$ is a solution of the ODE.
(I know this is a bit sloppy but I hope it explains the main idea)","['alternative-proof', 'proof-explanation', 'ordinary-differential-equations', 'real-analysis']"
4215851,Integrals with complex exponents,"My text book says that the solution to $$\int_{-1}^{-1/2} -e^{-i \omega t} dt + \int_{1/2}^{1} e^{-i \omega t} dt$$ is $$\frac{2}{\omega} ( \sin(\omega)  – \sin (\frac{\omega}{2}))$$ but I can not see how to arrive at that. What I get when solving the integrals is : $$\left[\frac{e^{-i \omega t}}{i \omega}\right]_{t=-1}^{-1/2} + \left[\frac{e^{-i \omega t}}{-i \omega}\right]_{t=1/2}^{1}$$ which continues into $$\frac{e^\frac{i \omega}{2}}{i \omega} 
– \frac{e^{i \omega}}{i \omega} 
+ \frac{e^{-i \omega}}{-i \omega} 
– \frac{e^\frac{-i \omega}{2}}{-i \omega}$$ Reversing the sign of the first two components in this last line would give the correct result using Euler’s identity, but I can not find a justification for that?","['integration', 'complex-analysis']"
4215862,Surjections/injections seen through quantifier adjoints,"Any map of sets $A \xrightarrow{f} B$ induces a functor of posets $Sub(B) \xrightarrow{f^{-1}} Sub(A)$ . It is known to have left and right adjoints $\exists_f$ and $\forall_f$ (defined as $\exists_f: X \mapsto f(X)$ and $\forall_f: X \mapsto \{y: f(x) = y \implies x \in X\}).$ I've noticed that for the first adjunction, $f$ is injective iff the unit is an isomorphism (i.e. $f^{-1}f(X)=X)$ , $f$ is surjective iff the counit is an isomorphism. Similar two statements hold for the second adjunction. It's easily seen from the definitions or from the well-known connection between (co)unit and (faith)fulness of adjunct functors. I feel that this has to be more than a random observation, but it does not find a place in my head. So I may try to ask this way: is there some generalization of those facts? For example, $\exists_f$ and $\forall_f$ exist in any locally cartesian closed category with pullbacks, in particular all topoi have such adjunctions. What if we try to define injections/surjections this way? Or is this just a simple manifestation of some well-known general fact I somehow fail to see?","['topos-theory', 'algebraic-geometry', 'category-theory']"
4215873,Triangle table: Moscow MO 1958 triangular table with numbers,"In the following triangular table $$0\,\, 1 \,\,2\,\, . . . . . . . . . . . . . . . 1957 \,\,1958$$ $$\,\,\,\,1 \,\,3 \,\,5\,\, . . . . . . . . . . . . \,\,\,\,3915$$ $$. . . . . . . . . . . .$$ each number (except for those in the upper row) is equal to the sum of the two nearest numbers in the row above. Prove that the lowest number is divisible by $1958$ . My progress:
Well, $1958$ surely isn't special. So let's prove for any $$a_1,a_2,a_3,\dots a_n$$ triangular table. So we have $$a_1~~a_2~~a_3~~a_4\dots \dots~~a_n $$ $$ \downarrow$$ $$(a_1+a_2)~~(a_2+a_3)~~(a_3+a_4)~~(a_4+a_5)\dots \dots ~~(a_{n-1}+a_n) $$ $$ \downarrow$$ $$(a_1+2a_2+a_3)~~(a_2+2a_3+a_4)\dots\dots ~~( a_{n-2}+2a_{n-1}+a_n)$$ $$ \downarrow$$ $$(a_1+3a_2+3a_3+a_4)~~(a_2+3a_3+3a_4+a_5)\dots \dots  $$ $$ \downarrow$$ $$(a_1+4a_2+6a_3+4a_4+a_5)~~ (a_2+4a_3+6a_4+4a_5+a_6)\dots $$ $$ \downarrow$$ $$(a_1+5a_2+10a_3+10a_4+5a_5+a_6)\dots  $$ $$ \vdots$$ Now, we only care about the first term of each row. In the last row of the table, there is only one term. Now, one can notice the pattern in the first terms, in general (proovable by induction) they are of the form $$(a_1+\binom{k}{1}a_2+\dots + \binom {k}{2}a_{k-1}+\binom{k}{k-1}a_k+a_{k+1}).$$ So the last row's term will be $$(a_1+\binom{n-1}{1}a_2+ \binom{n-1}{2}a_3+\dots + \binom{n-1}{n-2}a_{n-1}+a_n). $$ then we have $a_1=0,a_n=1958$ Here, we have $$\binom{1957}{1}1+ \binom{1957}{2}2+\dots + \binom{1957}{1956}1957+1958. $$ So we have to show that $$ 1958|\binom{1957}{1}1+ \binom{1957}{2}2+\dots + \binom{1957}{1956}1957$$ How to proceed ?","['contest-math', 'number-theory']"
4215877,A convergent sequence has either a maximum or a minimum or both.,"At odd times when I am not writing my Ph.d thesis I am solving some problems from Polya-Szego book which seems to me really interesting. Here is the one of the problems (Problem 106 from Volume 1) which I've solved and would be grateful if you can take a look. A convergent sequence has either a maximum or a minimum or both. Suppose $\{a_n\}$ be a convergent sequence but there is no $\min \limits_{n\in \mathbb{N}} a_n$ and there is no $\max \limits_{n\in \mathbb{N}}a_n$ . It is easy to show the following fact: If the sequence $\{a_n\}$ does not have a minimum then $\forall n\in \mathbb{N}$ $\exists k>n$ such that $a_k<a_n$ . In the same way you can show that if the sequence $\{a_n\}$ does not have a maximum then $\forall n\in \mathbb{N}$ $\exists m>n$ such that $a_m>a_n$ . Then using it we can construct two increasing subsequences $n_1<n_2<n_3<\dots$ and $m_1<m_2<m_3<\dots$ such that $n_1=m_1=1$ and $a_{n_1}>a_{n_2}>a_{n_3}>\dots $ and $a_{m_1}<a_{m_2}<a_{m_3}<\dots$ . Since $\{a_n\}$ is convergent then $a_{n_k}\to a$ and $a_{m_k}\to a$ as $k\to \infty$ .
Then $a=\inf \limits_{k\in \mathbb{N}}a_{n_k}$ and $a=\sup \limits_{k\in \mathbb{N}}a_{m_k}$ . Hence $a_{n_1}>a$ and $a_{m_1}<a$ but this is a contradiction because $n_1=m_1=1$ . Please let me know is everything valid in this short proof?","['sequences-and-series', 'real-analysis']"
4215904,Derive a homogenous linear recurrence from $x_n=2^n+F_n$,"Suppose that the sequence $x_n$ satisfies that $$x_n = 2^n + F_n$$ where $\{F_n \}$ denotes the Fibonacci sequence, I want to derive a homogenous linear recurrence for $x_n$ I can show that $$x_n-x_{n-1}-x_{n-2} = \frac{1}{4}2^n$$ By plug in the relationship of Fib Sequence, however the above is non-homogenous, can anyone help?","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
4215915,Line bundles on complete flag varieties independent of isogeny class,"Let $G$ be a semisimple connected linear algebraic group over an algebraically closed field $k$ . If $G^{sc}$ is the simply-connected cover of $G$ (i.e., the semisimple connected simply-connected linear algebraic group over $k$ with the same Dynkin type as $G$ ), then there is a central isogeny $\phi\colon G^{sc}\to G$ . In other words, $\phi$ is a surjective morphism of algebraic groups whose kernel is finite and central in $G^{sc}$ . Fix a maximal torus $T^{sc}$ sitting inside a Borel subgroup $B^{sc}$ of $G^{sc}$ . Thus, $T:=\phi(T^{sc})$ is a maximal torus sitting inside the Borel subgroup $B:=\phi(B^{sc})$ of $G$ . Let $\Psi=(\Sigma,\Lambda,\Sigma^\vee,\Lambda^\vee)$ be the root datum corresponding to $(G,T)$ , and let $\Psi^{\text{sc}}=(\Sigma^{\text{sc}},\Lambda^{\text{sc}},(\Sigma^{\text{sc}})^\vee,(\Lambda^{\text{sc}})^\vee)$ be the root datum corresponding to $(G^{sc},T^{sc})$ . Here, $\Lambda$ denotes the character lattice of $T$ , and $\Sigma$ denotes the root system of $T$ . The central isogeny $\phi$ induces a central isogeny of root data $\sigma:\Psi\to\Psi^{sc}$ . In other words, $\sigma$ is an injective group homomorphism $\Lambda\to\Lambda^{sc}$ with finite cokernel, such that $\sigma|_{\Sigma}:\Sigma\to\Sigma^{sc}$ is a bijection satisfying $\sigma^\vee((\sigma(\alpha))^\vee)=\alpha^\vee$ for all $\alpha\in\Sigma$ . Given any character $\lambda\in \Lambda$ , there is a line bundle $\mathcal{L}(\lambda)$ on $G/B$ whose total space is $G\times_B V_\lambda=G\times V_\lambda/((g,v)\sim (gb,b^{-1}\cdot v))$ , where $b\in B$ , $g\in G$ , and $v\in V_{\lambda}$ . Here, $V_\lambda$ is the one-dimensional irreducible representation of $T$ determined by $\lambda$ , and we view it as a representation of $B$ via the natural map $B\to T$ . Here are my questions: Is there an isomorphism of complete flag varieties $G/B\simeq G^{sc}/B^{sc}$ ? If not, is it possible to place some additional restrictions so that these complete flag varieties are isomorphic (e.g., by requiring that $k$ has characteristic $0$ , etc.)? If $G/B\simeq G^{sc}/B^{sc}$ as flag varieties, then we can view the line bundle $\mathcal{L}(\sigma(\lambda))$ on $G^{sc}/B^{sc}$ as a line bundle on $G/B$ . If this is the case, given a character $\lambda\in\Lambda$ , is there an isomorphism of line bundles $\mathcal{L}(\lambda)\simeq \mathcal{L}(\sigma(\lambda))$ on $G/B$ ? For both questions, could you please provide a complete proof or direct me to a reference that proves or disproves the fact? Thanks.","['algebraic-groups', 'representation-theory', 'algebraic-geometry', 'homogeneous-spaces', 'line-bundles']"
4215936,Counting number of vertices in a simplicial complex,"In some computation I have to do, I have to deal with the following situation: I have an arbitrary number of tetrahedra (=3 simplexes) and I am allowed to identify pairs of faces to each other. There is no restriction in how this is done, for example it is also allowed to identify two faces, which belong to the same tetrahedra. Each face is only allowed to be identified once, so I it not possible to have three faces identified to each other. Furthermore, it is also possible that after this procedure there are some faces, which are not identified to anything else. Now my question is the following: If i know the number of edges, faces and tetrahedra, as well as as the number of face identifications, is it possible to calculate the number of vertices? I naively guessed at the beginning the following formula $$4\text{ number of tetrahedra} - 3\text{ number of identifiactions}$$ in which i basically viewed every tetrahedron (which has 4 vertices) at the beginning to be disjoint and substract three vertices for each identification. However, this is obviously wrong, since when I glue 4 tetrahedra as in the following figure, the answer is clearly wrong: Here my calculation gives $4\cdot 4- 3\cdot 6=-2$ , which is quite far from the correct answer $5$ . Any idea how do it correctly?","['triangulation', 'combinatorics', 'discrete-mathematics', 'simplicial-complex']"
4215942,The eigenvectors of a general upper triangular matrix,"I am searching for the eigenvectors of an upper $n\times n$ triangular matrix $U$ with distinct and non-zero entries. What I know is that the diagonal entries are the eigenvalues of $U$ and to determine $v_{\lambda_{i}}$ I took the simplest case that is for $n=2$ , we can see that an upper triangular matrix has the first column of the canonical basis of $\mathbb{R}^{n}$ as an eigenvector to $\lambda_{1}$ of $U$ . Moreover, I believe $v_{\lambda_{2}}$ can be founded as a linear combination of $e_{2}$ and $e_{1}$ . Using the basic form of the eigenvalue problem, I am able to derive the second eigenvector of $U$ as follow : $$
U(e_{2}+\alpha e_{1})=u_{2,2}(e_{2}+\alpha e_{1})
$$ $$
u_{2,2}+u_{1,2}e_{1}+\gamma u_{1,1}e_{1}=u_{2,2}e_{2}+\gamma u_{2,2}e_{1}
$$ $$
u_{1,2}=\gamma(u_{2,2}-u_{1,1})
$$ assuming distinct eigenvalues $(u_{1,1}\neq u_{2,2})$ we get : $$
v_{\lambda_{2}}=e_{2}+\frac{u_{1,2}}{u_{2,2}-u_{1,1}}e_{1}
$$ For the $j-1$ eigenvector corresponding to $\lambda_{j-1}$ , what I can see is that it must have its first $j-1$ entries as non-zero so I expect $v_{\lambda_{j-1}}$ to be formed as a linear combination of the past $j-2$ eigenvectors such that : $$
v_{j-1}=e_{j-1}+\alpha_{1} v_{1}+\ldots+\alpha_{j-2}v_{j-2}
$$ I am not able to see what pattern the coeffecients $\{\alpha_{1},\alpha_{2},\ldots,\alpha_{j-1}\}$ have except for the simple case of $n=2$ therefore I hope someone can assist me in generalizing $U$ for its full size to determine the formula for the $j^{th}$ eigenvector","['linear-algebra', 'eigenvalues-eigenvectors']"
4215951,Expected value of $S_t$ where $dS_t=(\mu S_t+a)dt + (\sigma S_t +b)dW_t$,"I have the stochastic differential equation: $$dS_t=(\mu S_t+a)dt + (\sigma S_t +b)dW_t$$ where $W_t$ is a Wiener process with $S_0 > 0$ and $\mu, \sigma, a, b \in \mathbb{R}$ . I have found the solution of this equation: $$S_t= S_0\beta^{-1}_t+(a-\sigma b)\int_0^t \frac{ \beta_s} {\beta_t}ds + b\int_0^t \frac{ \beta_s} {\beta_t}dW_s$$ using an integrating factor of $\beta_t = \exp(-(\mu-\frac{1}{2}\sigma^2)t - \sigma W_t)$ and proved that it is a unique solution of this SDE. I want to now find the expected value of this solution, so $\mathbb{E}(S_t)  $ for all $t\geq 0$ , with $- \frac{1}{2} \sigma < \mu < 0$ . Could someone just point me in the right direction?","['stochastic-integrals', 'expected-value', 'stochastic-processes', 'stochastic-differential-equations', 'probability-theory']"
4215968,Hint on solving the second order differential equation $(x^2-3x)y''+xy'-(x+3)y=0$,"I want to solve this differential equation: $$(x^2-3x)y''+xy'-(x+3)y=0$$ The original question is as follows: $0<x<3$ is the largest interval in which the solution of $(x^2-3x)y''+xy'-(x+3)y=0$ , $y(1)=2$ , $y'(1)=1$ is certain to exist? So I tried by taking y=uv as complete solution. Then I tried to apply Normal form method and got $u=1/\sqrt{x-3}$ . So by putting value of P and  Q in normal form formula I got, $v''+[4x^2+x-36/4x(x-3)^2]v=0$ . How should I solve further? Is there a 'nicer' way to solve.","['derivatives', 'ordinary-differential-equations']"
4215996,"What is the derivative of $\sum_{i,j}^n \langle A^{ij}x_i,x_j \rangle$","Given different vectors $x_1,\dots,x_n$ and $n^2$ matrices $A^{11}\dots,A^{nn}$ with $A^{ij}=A^{ji}$ , I would like to calculate  the derivative with respect to $x_k$ of $$\sum_{i,j}^n \langle A^{ij}x_i,x_j \rangle$$ Am I right that this should be $\sum_{i\not= k}\langle A^{ik}x_i,x_k\rangle + 2\langle A^{kk}x_k,x_k\rangle$","['partial-derivative', 'multivariable-calculus', 'matrix-calculus']"
4216030,Properly express the integral to interchange the order of integration,"Calculate the integral, draw the region A and express it appropriately to interchange the order of integration $$\int_Af=\int_{-1}^1\int _{-2|x|}^{|x|}e^{x+y}\,dy\,dx$$ Let's first see what $A$ is like Note that in our drawing of A $ -1 \leq x \leq 1 $ y $ -2 \leq y \leq 1 $ We have already found the new outer limits, now we will find the inner ones.
On the other hand, let us note that we can express our integral as the sum of two integrals, taking the left part as $ B $ and the right part as $ C $ , so $$\int_Af=\int_Bf+\int_Cf= \int_{-1}^1\int _{2x}^{-x} e^{x+y} \,dy\,dx+\int_{-1}^1\int _{-2x}^x e^{x+y} \, dy \, dx$$ Let's start by changing the order of integration in $B$ , we know that $ -2 \leq y \leq 1 $ if $ y = -x $ then $ -y = x $ , on the other hand if $ y = 2x $ then $\frac{y}{2} = x$ . Then $$\int_Bf=\int_{-1}^1\int _{2x}^{-x} e^{x+y} \, dy \, dx = \int_{-2}^1\int_{\frac{y}{2}}^{-y}e^{x+y} \, dx \, dy$$ In the same way for the integral in $C,$ $ y = x $ and if $ y = -2x $ then $ - \frac {y} {2} = x $ then $$\int_Cf=\int_{-1}^1\int_{-2x}^x e^{x+y} \, dy \, dx = \int_{-2}^1 \int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy$$ then $$\int_Af = \int_{-2}^1\int_{\frac{y}{2}}^{-y} e^{x+y} \, dx \, dy + \int_{-2}^1\int_{-\frac{y}{2}}^y e^{x+y} \, dx \, dy$$ However, at the time of integral I do not get the desired result, I suppose that the way in which I change the integration limits is not adequate, can someone help me to change the integration limits appropriately?","['multivariable-calculus', 'multiple-integral']"
4216036,Limit of sequences equals $f'(0)$,"I'm working on a question that states: Consider a differentiable function $f:\mathbb{R}\to\mathbb{R}$ . Furthermore consider the sequences $(x_n)_n$ and $(y_n)_n$ which both have limit $0$ and $x_n < y_n$ for all $n\in\mathbb{N}$ . Is the following true? $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(0). $$ If yes give a proof, if no give a counter example and find the extra requirements for it to be true. I thought it was true and as a proof I did the following: Given that $f$ is differentiable and that $x_n < y_n$ , by the mean value theorem there exists a $c_n \in (x_n,y_n)$ such that $$ \frac{f(y_n)-f(x_n)}{y_n-x_n} = f'(c_n). $$ As $x_n$ and $y_n$ both converge to $0$ we have that $c_n$ will also converge to $0$ . Therefore, by taking the limit for $n$ to infinity we find that $$\lim_{n\to\infty} \frac{f(y_n)-f(x_n)}{y_n-x_n} = \lim_{n\to\infty}f'(c_n) = f'(0). $$ At this point I realised that I had used the continuity of $f'$ which was not given in the question. I am now wondering if this is a necessary condition for the problem or if it can be solved without using the continuity of $f'$ . If it is a necessary condition what is a suitable counter example where $f'$ isn't continuous? Any help is appreciated.","['limits', 'derivatives', 'continuity', 'real-analysis']"
4216043,Using first principle method to get derivative of $\sin(x°)$,"I saw the derivation of the derivative of $\sin x$ when $x$ was in radians but I don't know why we can't use the same derivation to get the derivative of $\sin(x°)$ . Here's my attempt: Note that $x$ and $h$ used below are in degrees. Let $f(x)=\sin(x)$ . So $$f'(x)= \lim_{h\rightarrow 0}\frac{\sin(x+h) - \sin x}{h}$$ Now, $$f'(x) = \lim_{h\rightarrow 0} \frac{2\cdot \cos(x+\frac{h}{2})\cdot \sin(\frac{h}{2})}{h}$$ $\Rightarrow$ $f'(x) = \cos(x)\cdot \lim_{h\rightarrow 0}\frac{\sin(\frac{h}{2})}{(\frac{h}{2})}$ Since $h$ is also in degree we get $f'(x)= \cos (x)$ where $x$ is in degree. Where am I wrong ?","['limits', 'derivatives']"
4216064,How to strengthen $ h \big( 2 h ( x ) \big) = h ( x ) + x $ to force $ h $ to be linear?,"Let $ h : \mathbb R \to \mathbb R $ be an injective function such that $$
h \big( 2 h ( x ) \big) = h ( x ) + x
$$ for all $ x \in \mathbb R $ , and $ h ( 0 ) = 0 $ . What would be an as mild as possible extra condition such that the only solution of the equation $ h ( x ) = - \frac x 2 $ is $ x = 0 $ ? Note that the function $ x \mapsto - \frac x 2 $ satisfies the conditions, so somehow we need to fully exclude this function. For example, both the extra condition $ h ( x ) \le x \ \forall x $ as well as the extra condition $ h ( x ) \ge x \ \forall x $ force $ h ( x ) = x \ \forall x $ , and thus $ 0 $ is the only solution to the equation in question. But what about other conditions as e.g. monotone or surjective?","['real-numbers', 'functional-equations', 'functions', 'real-analysis']"
4216074,upper bound on the integral of a product of non-negative functions [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question i have two functions $G_1(\theta,\phi)$ , $G_2(\theta,\phi)$ , $G_{1,2}: [0,\pi] \times [0,2\pi] \to \mathbb{R}^+_0$ i know that both functions satisfy that: \begin{equation*}
\int_0^\pi\int_0^{2\pi}G_{1,2}(\theta,\phi)\,d\phi\, d\theta = 4\pi
\end{equation*} and i'm trying to find an upper bound B, for the integral of the product of both functions over their domain: \begin{equation*}
\int_0^\pi\int_0^{2\pi}G_1(\theta,\phi)G_2(\theta,\phi) \, d\phi \, d\theta \leq B
\end{equation*}","['integration', 'multivariable-calculus', 'definite-integrals', 'upper-lower-bounds']"
4216100,Norm of Riemannian curvature tensor on an Einstein manifold under the RIcci flow,"I'm trying to prove that if $(M, g_0)$ is Einstein and $g(t) = (1-2\lambda t)g_0$ is a solution to its Ricci flow, then $\|\operatorname{Rm}(t)\|^2 = C R(t)^2$ , where $R$ is the scalar curvature and $C$ is a constant depending only on $g_0$ . This is a claim I've seen in a book but I'm not sure which norm they're using. Assuming it's the one induced by the metric, we have: $\|\operatorname{Rm}\|^2 = g^{ri}g^{sj}g^{pk}g^{q\ell}R_{rspq}R_{ijk\ell}$ , but how can we relate this to the Ricci tensor? I'd appreciate any help. This is the discussion in the book:","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4216112,"In triangle $ABC$, $AP$ is the angle bisector","In triangle $ABC$ , $AP$ is the angle bisector of $\measuredangle A$ . If $BP=16,CP=20$ and the center of the circumcircle of $\triangle ABP$ lies on $AC$ , find $AB$ and $AC$ . $AP$ is the angle bisector of $\measuredangle BAC$ which means that $\measuredangle BAP=\measuredangle CAP=\alpha$ . On the other hand, $OA=OP=R_{\triangle ABP}=R$ , so $\measuredangle PAO=\measuredangle APO=\alpha$ . This makes $OP\parallel AB$ . I am trying to figure out the best way to use that parallelity. The most straightforward think we can note is $\triangle OPC\sim\triangle ABC$ . I don't see if this can be helpful, though. Another true thing is $\dfrac{BP}{PC}=\dfrac{AB}{AC}=\dfrac{16}{20}=\dfrac{4}{5}$ .","['euclidean-geometry', 'triangles', 'circles', 'geometry']"
4216177,Does positive joint probability imply positivity of a conditional event?,"I have very little experience with probability so apologies if the title is confusing!! Let $\mu, \nu$ be probability measures on measure spaces $X,Y$ (if helpful we can assume $X = Y$ are compact subsets of $\mathbb{R}^d$ , but I don't want to place any assumptions involving absolute continuity of $\mu, \nu$ w.r.t. Lebesgue). Let $\Gamma(\mu, \nu)$ denote the set of all probability measures on $X \times Y$ with marginals $\mu, \nu$ , i.e. for all $A \subseteq X$ and $B \subseteq Y$ we have $\gamma(A \times Y) = \mu(A)$ and $\gamma(X \times B) = \nu(B)$ . Fix an arbitrary $\gamma \in \Gamma(\mu, \nu)$ and let $E \subseteq X \times Y$ such that $\gamma(E) > 0$ . For all $x \in X$ and $y \in Y$ let $E_x = \{y \in Y \mid (x,y) \in E\}$ and $E^y = \{x \mid (x,y) \in E\}$ . Then does $\gamma(E) > 0$ give us that there always exist $x \in X$ (alternatively, $y \in Y$ ) such that $\nu(E_x) > 0$ (alternatively, $\mu(E^y) > 0$ )? I can see this is true when $\gamma$ is the product measure (see, e.g., Folland 2.36) but I'm not seeing how to generalize the proof. By the disintegration theorem ( https://en.wikipedia.org/wiki/Disintegration_theorem ) we can get things like $$
\gamma(E) = \int_Y \gamma_y(E)\ \mathrm{d} \nu(y)
$$ and $$
\gamma(E) = \int_X \gamma_x(E)\ \mathrm{d} \mu(x),
$$ whence we know $\gamma_y(E) > 0$ for $\nu$ -a.e. $y$ and $\gamma_x(E) > 0$ for $\mu$ -a.e. $x$ . But it's not clear to me how to turn these into statements about $\mu(E^y)$ and $\nu(E_x)$ , respectively...any advice? Thank you!!","['conditional-probability', 'measure-theory', 'marginal-probability', 'probability']"
4216199,"Showing ${\sin mx\over\sin x}=(-4)^{(m-1)/2}\prod_{j=1}^{(m-1)/2}\left(\sin^2x-\sin^2{2\pi j\over m}\right)$ for odd $m>0$ (from Serre's ""Arithmetic"")","On pages 9 and 10 of Serre's A Course in Arithmetic , there is a lemma (towards a proof of the quadratic reciprocity law) stating that
for any positive odd integer $m$ , $${\sin mx\over\sin x}=(-4)^{(m-1)/2}\prod_{j=1}^{(m-1)/2}\left(\sin^2x-\sin^2{2\pi j\over m}\right)$$ Serre claims that this result is ""elementary"", saying that one should start by proving that $\sin(mx)/\sin(x)$ is a polynomial of degree $(m-1)/2$ in $\sin^2 x$ with $(m-1)/2$ roots given by $\sin^2(2\pi j/m)$ for $1\le j\le m$ , then compare coefficients of $e^{i(m-1)x}$ on both sides to get the factor of $(-4)^{(m-1)/2}$ . I was not able to follow this outline, as I'm not exactly sure which sine identity to start with. I suspect Serre would like me to expand $${\sin mx\over \sin x} = {e^{imx}-e^{-imx}\over e^{ix} - e^{-ix}},$$ since he mentions coefficients of $e^{i(m-1)x}$ . So I was thinking of performing induction over all odd $m$ (the case $m=1$ is easy), but have not been able to get the computations right. If anyone would be able to point me in the right direction, I'd very much appreciate it!","['elementary-number-theory', 'trigonometry']"
4216225,Unexpected use of linearity of expectation with indicator random variable in problems,"Can people suggest some problems (probability puzzle type) where the use of linearity of expectation together with indicator random variable is unexpected/hard to see but it makes problems much easier? I have encountered a lot of questions asking similar types of problems in the various domains so I think combined use of linearity of expectation and indicator random variable deserves its own. This and This are the question I was motivated from, but some of problem in my second mentioned problem are not puzzle related.","['big-list', 'expected-value', 'intuition', 'soft-question', 'probability']"
4216234,Can we always find smaller not-too-small compact (Hausdorff) subspaces?,"Say that a property $\mathscr{P}$ of topological spaces is incompressible iff there is some cardinal $\kappa$ such that, for all cardinals $\lambda$ , there is a topological space $\mathcal{X}$ of size $>\lambda$ which has property $\mathscr{P}$ but has no subspace $\mathcal{Y}$ with property $\mathscr{P}$ and $\kappa\le \vert \mathcal{Y}\vert<\vert\mathcal{X}\vert$ . For example, connectedness is incompressible via $\kappa=2$ : consider the Dedekind completion of a ""large"" dense linear order all of whose nondegenerate intervals have the same cardinality as the whole. On the other hand, hereditary properties like Hausdorffness are never incompressible since we can just look at any ""moderately sized"" subspace of a ""large"" appropriate space. Marginally less trivially, path connectedness is not incompressible since, unlike general connectedness, it is witnessed by ""small configurations"" (= paths). My question is: Is ""compact + Hausdorff"" incompressible? As an aside, note that $\mathsf{ZFC}$ proves that $[0,1]$ has no uncountable compact subspaces of size $<2^{\aleph_0}$ ; consequently, if for example $2^{\aleph_0}=\aleph_2$ then $[0,1]$ is a space of size $\aleph_2$ with no compact subspace of size $\aleph_1$ . So the set of sizes of compact spaces of a given space can be complicated.","['general-topology', 'logic', 'set-theory']"
4216243,Finding $\iint_S \nabla \times F\ dS$,"I have to solve the following problem which seems difficult: Find $$ \iint_S \nabla \times F\ dS $$ where $S$ is given by $$r(t,s)=\left( 9+(\cos t)(\sin s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \ 9+(\cos t)(\cos s)\left(2+\frac{\sin (5s)}{2}\right), \ \ \  9+\frac{\sin t}{3}\left(2+\frac{\sin (5s)}{2}\right) \right) $$ where $0\leq t\leq 2\pi$ , $\ \ $ $0\leq s\leq \pi$ , $\ \ \ $ and $F:=(z,0,y)$ . I'm not sure how to proceed, any help is appreciated. Should I use Gauss divergence? When I plotted $S$ in wolfram (not sure why is different from the answer below) https://www.wolframalpha.com/input/?i=%289%2B%28cos+t%29%28sin+s%29%282%2Bsin+%285s%29%2F%282%29%29%2C++9%2B%28cos+t%29%28cos+s%29%282%2Bsin+%285s%29%2F2%29%2C+9%2B%28%28sin+t%29%2F3%29%282%2Bsin+%285s%29%2F2%29+%29%2C+0%3C%3D+s%3C%3D+pi%2C+0%3C%3Dt%3C%3D+2pi",['multivariable-calculus']
4216245,Corollary of Hoeffding’s Inequality,"Question I am not from a statistics background I came across the following corollary of Hoeffding’s Inequality and couldn't find the derivation or proof for it so that could anyone please share some reference of proof or just prove it here? Hoeffding’s Inequality Let $Y_1, Y_2, \cdots, Y_n$ be i.i.d. observations such that $\mathbb{E}(Y_i) = \mu$ and $a \leq Y_i \leq b$ . Then for any $\epsilon > 0$ ; $$
\mathbb{P}( | \bar{Y_n} - \mu | \geq \epsilon) \leq 2 e^{-2 n \epsilon^2 / (b - a)^2}
$$ Corollary of Hoeffding’s Inequality Let $Y_1, Y_2, \cdots, Y_n$ are independent with $\mathbb{P}( a \leq Y_i \leq b ) = 1$ and common mean $\mu$ , then, with probability at least $1 - \delta$ , $$
| \bar{Y_n} - \mu | \leq \sqrt{ \frac{c}{2n} \log{(\frac{2}{\delta})}}
$$ References Theorem 6 and Corollary 7 in this pdf","['statistics', 'concentration-of-measure', 'probability-theory']"
4216324,a best critical region of two parameters in normal distribution,"This question is from Introduction to Mathematical Statistics written by Hogg et al In page 448, 1.6 Let $ X_1, X_2, \ldots, X_{10} $ is a random sample from a distribution that is $N(\theta_1, \theta_2)$ . Find a best test of the simple hypothesis $H_0 : \theta _1= \theta'_1 = 0, \theta_2=\theta'_2=1$ against the alternative simple hypothesis $H_1 : \theta _1= \theta''_1=1, \theta_2=\theta''_2=4 $ I briefly write a best critical $C=\lbrace(x_1,x_2, \ldots, x_{10})| 3\sum_{i=1}^{10}x_i^2 +2\sum_{i=1}^{10}x_i \ge k\rbrace$ But we have to consider a joint distribution of chi square and normal. Is that right? That looks so complicated so I feel unsure of a above expression. Help me please!",['statistics']
4216352,"Find the directional derivative of $f_{\vec u}(5,2)$ for the function $xy+y^3$.","Find the directional derivative of $f_{\vec u}(5,2)$ for the function $f(x,y)=xy+y^3$ with $\frac{(3i-4j)}{5}$ ? I got $-\frac{17}{5}$ (as the answer) from the derived formula below. $  \left(2\right)\left(\frac{3}{5}\right)+\left(5\right)+3\left(2\right)^2\left(-\frac{4}{5}\right)$ Incase the above is not readable here is the problem on symbolab. It is in the form $ai+bj$ where $a=y$ and $b=x+3y^2$ , as can be seen in the Symbolab, I already plugged in the $x$ and $y$ values which is how I got to my final answer of $-\frac{17}{5}$ . $a$ and $b$ were found by taking the partial derivative of $x$ and $y$ via Symbolab. Obviously my answer was wrong, I am not sure where I went wrong.","['calculus', 'vectors', 'derivatives', 'vector-spaces']"
4216355,How to solve this complicated trigonometric equation for the solution of this triangle.,"In $\triangle ABC$ , if $AB=AC$ and internal bisector of angle $B$ meets $AC$ at $D$ such that $BD+AD=BC=4$ , then what is $R$ (circumradius)? My Approach:- I first drew the diagram and considered $\angle ABD=\angle DBC=\theta$ and as $AB=AC$ , $\angle C=2\theta$ .  Therefore $\angle A=180-4\theta$ . Also as $AE$ is the angular bisector and $AB=AC$ , then $BE=EC=2.$ Now applying the sine theorem to $\triangle ADB$ and $ \triangle BDC$ gives $$\frac{BD}{\sin {(180-4\theta)}}=\frac{AD}{\sin \theta}$$ $$\frac{BC}{\sin(180-3\theta)}=\frac{BD}{\sin 2\theta}$$ Now we know that $BC=4$ and then solving both the equations by substituting in $BD+AD=4$ , we get $$\sin 2\theta .\sin4\theta+\sin2\theta.\sin\theta=\sin3\theta.\sin4\theta$$ $$\sin4\theta+\sin\theta=\frac{\sin3\theta.\sin4\theta}{\sin2\theta}$$ Now I have no clue on how to proceed further from here. Though I tried solving the whole equation into one variable ( $\sin\theta$ ), but it's getting very troublesome as power of $4$ occurs. Can anyone please help further or else if there is any alternative method to solving this problem more efficiently or quickly ? Thank You","['triangles', 'trigonometry', 'geometry']"
4216400,Some questions about the proof of backwards mathematical induction,"The backwards mathematical induction is Let $n$ be an natural number and $P(m)$ be a property, and if $P(m+1)$ is true, then $P(m)$ is true. The problem is given that $P(n)$ is true, prove $\forall m\in\mathbb{N}$ and $m\leq n$ , $P(m)$ is true. What make me puzzled is the hint in the book as I should make induction according to $n$ , but I think that the $n$ is fixed? What’s the meaning of changing $n$ ? You could find the problem on tao’s analysis.","['peano-axioms', 'discrete-mathematics']"
4216423,Nine person of equal strength playing in tournament,"Nine persons $P_i$ , i = 1, 2, ...., 9 of equal strength are playing a tournament such that they are first grouped in to three groups A, B, C each containing three persons at random and a winner from each group is selected and. a new group is formed and finally from this group the winner of the tournament is decided. Probability that $P_2$ and $P_4$ were in different groups given that $P_4 $ is the winner of the tournament is   equal to (A) $\frac{2}{3}$ (B) $\frac{1}{2}$ (C) $\frac{1}{4}$ (D) $\frac{3}{4}$ Let us treat it a case of 9 distinct balls (each player) and three sets of three identical boxes though three identical boxes are put in three distinct boxes Let us put ball representing $P_2$ & $P_4$ in different boxes and then make the number of combinati0ns ${}^7{C_3} \times {}^4{C_2} = 210$ Now the arrangement of three groups is $210\times3!=1260$ Now total number of cases without restriction = ${}^9{C_3} \times {}^6{C_3} = 1680$ Now the arrangement of three groups is $1680\times3!=10080$ Probability that $P_2$ and $P_4$ are in different group = $\frac{1260}{10080}=\frac{1}{8}$ I am not sure about my approach",['probability']
4216433,What is the mistake in this computation involving the absolute value of the expected value?,"It is well known that $|E[X]|\leq E[|X|]$ , where $X$ is a random variable taking values in $\mathbb{R}$ and $E[X]$ is its expected value. Now, suppose that $|E[X]|<a$ for some $a\in \mathbb{R}$ . This is equivalent to $-a<E[X]<a$ and multiplying by -1 we get $a>-E[X]=E[-X]>-a$ . If $X\geq 0$ then $|X|=X$ so that $E[|X|]=E[X]<a$ .
If $X < 0$ then $|X|=-X$ so that $E[|X|]=E[-X]<a$ . In conclusion, this would show that $|E[X]|<a$ implies $E[|X|]<a$ . But since this works for any $a\in \mathbb{R}$ then $|E[X]|< E[|X|]$ would be impossible, violating the well-known inequality. There must be some beginner's mistake in my elaboration and I can't seem to find it.","['expected-value', 'calculus', 'absolute-value', 'probability']"
4216603,"Solution Verification - find values of $a,b,c$ such that $\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}$ exists","Let $f$ be any twice differentiable function. Find values of $a,b,c$ such that $$\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}\tag{1}$$ exists for all $x$ . For $(1)$ to exist, we require $$\lim_{h\to 0} 
\left(a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)\right)=0\tag{$*$}$$ (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives $a+b+c=0$ . Hence applying L'Hôpital's rule on $(1)$ , we get $$\lim_{h\to 0} \frac{a\cdot f(x+h)+b\cdot f(x) + c\cdot f(x-h)}{h^2}=\lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}\tag{2}$$ For $(2)$ to exist, we require $$\lim_{h\to 0} \left(a\cdot f'(x+h) - c\cdot f'(x-h)\right)=0\tag{$**$}$$ (I've used this statement intuitively. I do not have a rigorous/proper proof for this statement.) Which gives $a-c=0$ . Hence applying L'Hôpital's rule on $(2)$ , we get $$\lim_{h\to 0} \frac{a\cdot f'(x+h) - c\cdot f'(x-h)}{2h}=\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2}  \tag{3}$$ $$\implies\lim_{h\to 0} \frac{a\cdot f''(x+h) + c\cdot f''(x-h)}{2} = \frac{a+ c}{2}\cdot f''(x) \tag{4}$$ My questions are: Is the above solution correct? Can you provide a proper proof for the two statements that I have marked with asterisks? i.e  if it is given that $\lim_{x\to 0}\phi_2(x)=0$ , then for $\lim_{x\to 0}\frac{\phi_1(x)}{\phi_2(x)}$ to exist is it necessary that $\lim_{x\to 0}\phi_1(x)=0$ Edit:-","['limits', 'derivatives']"
4216609,Order Preserving bijection is the identity,"Let $\preceq$ be a well-ordering on a set $X$ and let $f:X\to X$ be a bijective function. If $f$ is order-preserving, that is, $x\preceq y$ implies $f(x)\preceq f(y)$ , than I have to prove that $f$ is the identity function on $X$ . I attempted to assume the opposite, but that didn't lead to anything. My second, more promising idea, was as follows. Since $X$ is well-ordered, there exists a least element, call it $x_1$ . If I manage to prove that $f(x_1)=x_1$ , then I can simply consider $X-\left\{x_1\right\}$ and repeat this process to prove $f$ is the identity. I suppose I would need the Axiom of Choice to make this rigorous. Assuming the contrary, there exists some $x_2\in X$ , such that $f(x_1)=x_2$ , and $x_1\neq x_2$ . Obviously, $x_1\preceq x_2$ . Because $f$ is order-preserving, it follows that $f(x_1)\preceq f(x_2)$ , so $f(x_1)\preceq x_1$ . But $x_1$ is the least element, so this all means that $f(x_1)=x_1$ . Contradiction, therefore $f(x_1)=x_1$ . As I was writing this question, asking for guidance, I came up with the solution above. Is my approach correct? Is there some other interesting way to prove this?",['elementary-set-theory']
4216621,Can a parallel transported frame be obtained from the eigenvectors of a curvature tensor?,"Let $(M,g,\nabla)$ be a Riemannian manifold of dimension $n$ and let $\gamma:\mathbb{R}\to M$ be a geodesic, namely, $\nabla_{\dot{\gamma}}\dot{\gamma}=0$ . It is well know that, given a basis $\{e_{j}(p)\}_{j=1}^{n}$ on the tangent space $T_{p}M$ , where $p=\gamma(0)$ , one can build a parallel transported frame $\{E_{0}(\gamma(t)):=\xi(\gamma(t)),E_{1}(\gamma(t)),\ldots,E_{n}(\gamma(t))\}$ on any tangent space $T_{\gamma(t)}M$ along the geodesic. In particular, this means that, for any $l$ , $\nabla_{\xi}E_{l}=0$ and $E_{l}(\gamma(0))\equiv e_{l}(p)$ . Now, setting $\xi:=\dot{\gamma}$ , let us introduce the symmetric endomorphism: $$
Ricc_{\xi}(p):=R(\xi(p),J(p))\xi(p),\qquad \forall J(p)\in T_{p}M
$$ where $R$ is the Riemann curvature tensor, i.e, $R(X,Y)Z|_{p}=R^{i}_{jkl}(p)X^{j}(p)X^{k}(p)Z^{l}(p)e_{i}(p)$ with $\{e_{i}(p)\}_{i=1}^{n}$ is a orthonormal basis on the tangent space $T_{p}M$ . I know that $Ricc_{\xi}$ is symmetric because of the properties of the Riemann tensor, therefore, I can always diagonalized $Ricc_{\xi}$ on (in principle any tangent space) the initial point of the geodesic, that is, $p=\gamma(0)$ and I will get an eigensystem whose eigenvectors can be used as a basis on $T_{\gamma(0)}M$ . Now, supposing that I know the isomorphism (parallel transport map) $\Gamma_{t}:T_{p}M\to T_{\gamma(t)}M$ since I solved the geodesic equation, my question is: Can a parallel transported frame be obtained evolving the eigenvectors of the operator $Ricc_{\xi}$ through the map $\Gamma_{t}$ ?",['differential-geometry']
4216682,Diffeomorphism of an open set and almost all of $\mathbb{R}^n$,"I am aware of the statement that a open set in $\mathbb{R}^n$ , if it is star-like, is diffeomorphic to $\mathbb{R}^n$ , although this is apparently not so easy to prove. I am wondering if a weaker statement exists. Namely, if $U$ is an open set of $\mathbb{R}^n$ , does there always exist a diffeomorphism $\phi : U\to\mathbb{R}^n\setminus N$ where $N$ is a Lebesgue-negligible set? Edit: Here is a reference the claim that star-shaped open sets are diffeomorphic to $\mathbb{R}^n$ .","['measure-theory', 'diffeomorphism']"
4216714,Why does relative entropy decrease under pushforward?,"I am reading the paper at https://arxiv.org/abs/1006.3028 (J. Lehec, ""Representation formula for the entropy and functional inequalities""). The main concept here is the relative entropy of the probability measures $\mu$ and $\gamma$ , defined as $$H(\mu | \gamma)=\int \log\left( \frac{d\mu}{d\gamma}\right) d\mu, $$ or $+\infty$ if $\mu$ is not absolutely continuous with respect to $\gamma$ (that is, the density $\frac{d\mu}{d\gamma}$ does not exist). This is also known as the Kullback-Liebler divergence. Remark on sign conventions . This definition seems to be more common of information theory. With this definition, $H(\mu| \gamma)$ is a nonnegative convex function of $\mu$ . The common physicist's definition, on the other hand, has the opposite sign; it is thus a nonpositive concave function of $\mu$ . The first inequality in the second section reads $$\tag{1}
H(\mu\circ T^{-1} | \gamma\circ T^{-1})\le H(\mu | \gamma)$$ for all measurable maps $T$ . Main question . What is the fastest proof of (1)? Following the references in the paper I actually found a proof. In the book ""Large deviations and applications"" of Varadhan (reference [24], section 10) I see that the relative entropy can be characterized as $$
H(\mu|\gamma)=\inf\left\{ c\,:\, \int F\, d\mu \le c + \log \int  e^F\, d\gamma,\ \forall F \text{ bounded and measurable}\right\}.$$ Using this characterization, (1) follows. I wonder if there is a way to avoid the characterization, though. NOTE . The characterization is an immediate consequence of the convex duality described in this question , which is an application of the Jensen inequality. Secondary question . The word ""entropy"" makes me think of the second principle of thermodynamics, and it suggests some quantity that is monotonic in time. Now, the map $\mu\mapsto \mu\circ T^{-1}$ can be interpreted as a step in time for the discrete dynamical system $x\mapsto T(x)$ . Can (1) be seen as a version of the second principle of thermodynamics for such discrete systems?","['entropy', 'measure-theory', 'probability-theory']"
4216723,What is the Borel sigma-field of positive functions?,"I was reading the paper ""The Logistic Normal Distribution for Bayesian, Nonparametric, Predictive Densities"" published in 1988 in Journal of the American Statistical Association by Peter J. Lenk . At the beginning of the paper, the author mentions a sigma-algebra that I don't understand. His goal is to work with random pdf afterwards and he states that for a probability space $(\Omega, F, P)$ and a space $X$ : [A random pdf] $f$ is modelled by a stochastic process from $\Omega \times X$ to $\mathbb R^+$ , the positive reals such that the sample
paths integrate to $1$ . With this goal in mind, $\mathbb {R^{+}}^X$ is
the space of functions from $X$ to $\mathbb R^+$ with the Borel
sigma-field $B(X, \mathbb R^+)$ . I don't understand what the Borel sigma-field mentionned is supposed to be, as the topology considered was not specified. Is there a ""standard""/""canonical"" choice of topology that allows to define this sigma-algebra, or is it an imprecision in the paper ? Thank you very much in advance!","['borel-sets', 'stochastic-processes', 'measure-theory', 'random-functions']"
4216733,"almost sure convergency of $W_{n}=\frac{Z_{n}}{m^{n}}\to W$ implies $\frac{Z_{n}}{\mu^{n}}\to0$, where $\mu>m$.","I though to apply  Borel-Cantelli, however it does not seem to work. m and $\mu$ are constants, with $0<m<\mu$ , with random identically distributed variables $Z_{n}$ , which take values in $\mathbb{N}_{0}$ . What I got was (all of them are nonnegativ so I leave the abs away):\ $\sum^{\infty}_{n=0}\mathbb{P}(\frac{Z_{n}}{\mu^{n}}>\epsilon) =\sum^{\infty}_{n=0}\mathbb{P}(Z_{n}>\mu^{n}\epsilon)=\sum^{\infty}_{n=0}\mathbb{E}(\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}})=\mathbb{E}(\sum^{\infty}_{n=0}\mathbb{1}_{\{Z_{n}>\mu^{n}\epsilon\}})$ \
I though when I get expected value it would be easier to estimate because of linearity and at this point I tried to estimate the last part with $\sum^{\infty}_{n=0}\mathbb{P}(|\frac{Z_{n}}{m^{n}}-W|>\epsilon)$ turning it into expected value. However it doesn´t seem to help a lot. Well now because of the comment I notice, that is sufficient to use , that $\frac{m}{\mu}<1$ then because W is some finite value $\lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}=\lim_{n\to \infty}\frac{Z_{n}}{\mu^{n}}\frac{m^{n}}{m^{n}}=W\lim_{n\to \infty}(\frac{m}{\mu})^{n}=0$","['borel-cantelli-lemmas', 'stochastic-processes', 'probability']"
4216740,$C_0^\infty(\mathbb{R}^k)$ functions can be approximated via polynomials,"Let $f:\mathbb{R}^k\rightarrow \mathbb{R}$ be a vanishing at infinity function, also infinitely differentiable, i.e. $f\in C_0^\infty(\mathbb{R}^k,\mathbb{R})$ . Is it true that I can always approximate $f$ with some polynomial $p$ ? Can I use here directly the right version of the Stone-Weierstrass theorem considering the subalgebra of polynomials $P:=\lbrace p:\mathbb{R}^k\rightarrow \mathbb{R} \mid p \text{ polynomial} \rbrace$ ?","['weierstrass-approximation', 'functions', 'functional-analysis', 'real-analysis']"
4216766,"Let $Y \subset X$; let $X$ and $Y$ be connected. Show that if $A$ and $B$ form a separation of $X \setminus Y$, then $Y \cup A$ is connected.","I am trying  to solve Let $Y \subset X$ ; let $X$ and $Y$ be connected. Show that if $A$ and $B$ form a separation of $X \setminus Y$ , then $Y \cup A$ is connected. My ATTEMPT: We will show that $Y \cup A$ is connected. Let it is not, then there is a separation $(U, V)$ on $Y \cup A$ . Since $Y$ is a connected subspace of $Y \cup A$ , either $Y \subset U$ or $Y \subset V$ . Suppose $Y \subset U$ , then $V \subset A$ . Therefore, $X = (B \cup U)   \cup V $ . Since $(U,V)$ is a separation of $Y \cup A$ , then no limit points of $U$ is in $V$ and vice versa. Similarly, no limit points of $B$ is in $V$ , because $V \subset A$ and $(A,B)$ form a separation of $X-Y$ .  Therefore, $(B \cup U)^\prime \cap V^\prime = (B^\prime \cup U ^\prime) \cap V^\prime =  (B^\prime  \cap V^ \prime) \cup (U^\prime \cap V^\prime) = \emptyset \cup \emptyset = \emptyset$ , hence no limit points of $B \cup U$ is in $V$ and vice versa. Is my attempt correct?  But I can not proceed further? $\color{red}{\text{Actually, I want to prove $B \cup U$ closed in $X$ and $V$ is open in $X$.}}$ If I can prove it then $(B \cup U, V)$ is a separation on $X$ , hence a contradiction. Please help me.",['general-topology']
4216781,"$\{\omega: f_n(\omega)\in[a,b]\text{ for finitely many }n\}$ is measurable","The question: Given a sequence of measurable functions $(f_n):\Omega\rightarrow \mathbb{R}$ . Prove the following set $A$ is measurable: $$A=\{\omega:f_n(\omega)\in[a,b]\text{ for finitely many }n\}$$ $$B=\{\omega:f_n(\omega)\in[a,b]\text{ for infinitely many }n\}$$ I found another similar post but I don't think it was helpful. My thoughts: For $A$ , $f_n(\omega)\in[a,b]$ for finitely many $n$ means: $$\forall \,N < \infty\,\,\exists \,\,n\leq N \text{ such that } f_n(\omega)\in [a,b]$$ translate that to set language is: $$\bigcup_{N\in\mathbb{N}}\bigcap_{n\leq N} \{\omega:f_n(\omega) \in [a,b]\}$$ However, this does not seem right, cause if for some $n \leq N$ , set $\{\omega: f_n(\omega)\in[a,b]\}$ is empty, then the intersection over all $n \leq N$ would be empty, and that is not necessarily true. For $B$ , $f_n(\omega)\in[a,b]$ for infinitely many $n$ means: $$\exists \,N < \infty\,\,\forall \,\,n\geq N \text{ such that } f_n(\omega)\in [a,b]$$ in set it is: $$\bigcap_{N\in\mathbb{N}}\bigcup_{n\geq N} \{\omega:f_n(\omega) \in [a,b]\}$$ Are my thoughts correct? I apologize if I made any stupid mistakes. Since I don't have a solid background on sets or logic, any help would be very much appreciated!","['measure-theory', 'logic', 'real-analysis', 'sequences-and-series', 'elementary-set-theory']"
4216789,Why can't this be a simple solution to Baby Rudin 9.12.d?,"Fix two real numbers $a$ and $b$ , $0<a<b$ . Define $f: \mathbb{R}^2 \rightarrow \mathbb{R}^3$ by $$f_1(s,t) = (b+a \cos s) \cos t$$ $$f_2 (s,t) = (b+a \cos s ) \sin t$$ $$f_3(s,t) = a \sin s.$$ $f$ maps $\mathbb{R}$ onto the torus, $K = f(\mathbb{R})$ . Let $\lambda$ be an irrational. Define $g:\mathbb{R} \rightarrow K$ by $g(t) = f(t, \lambda t )$ . Prove that the range of $g$ is a dense subset of $K$ . I know that this can proven using the Kronecker approximation theorem, as shown here A mapping from $\mathbb{R}^1$ to a dense subset of the surface of torus in $\mathbb{R}^3$ . My question is can't we simply use continuous functions to prove this. Define $h: \mathbb{R} \rightarrow \mathbb{R}^2$ by $h(t) = (t, \lambda t)$ . $h$ is continuous. We also know that $f$ is continuous, so any restriction of $f$ is continuous. Then $g(t) = f(h(t))$ is continuous. Then, by problem 4.4 in Rudin, if $g$ is a continuous mapping of a metric space $X$ into a metric space $Y$ and $E$ is a dense subset of $X$ , then $g(E)$ is dense in $g(X)$ . Since $\mathbb{R}$ is dense in $\mathbb{R}$ , and $g$ is continuous, then $g(\mathbb{R})$ is dense in $K$ . Where is my mistake? Or is this valid? EDIT: The mistake is in the last sentence as pointed out by Salcio in the comments.","['continuity', 'analysis']"
4216790,Bijection between $\mathbb N^{\mathbb N}$ and $2^{\mathbb N}$,"Let us define $$\mathbb N^{\mathbb N}=\mathbb N\times\mathbb N\times\mathbb N\times\dots$$ I want an explicit bijection between $\mathbb N^{\mathbb N}$ and $2^{\mathbb N}=\mathcal P(\mathbb N)$ , i.e., the power set of $\mathbb N$ The fact that this bijection exists in not hard to see. It is very clear that $\mathbb {N^N}$ is the same as $[0,1)$ (with an extra decimal point) which is same as $\mathbb R$ (in terms of infinities of course). And we know that the infinities of $2^{\mathbb N}$ and $\mathbb R$ are same. Also, it's not hard to see that the map $\{a_1,a_2,\dots a_n\}\to (a_1,a_2,\dots a_n,0,0,\dots)$ and $\{a_1,a_2,\dots\}\to (a_1,a_2,\dots)$ gives an idea of a bijection, only that it's not really a bijection since the order matters in ordered pairs, but not in sets. Some other ideas are available here as well. But, what I want is a well constructed ( nice if possible) bijection between $\mathbb N^{\mathbb N}$ and $2^{\mathbb N}$ .","['elementary-set-theory', 'infinity', 'cardinals', 'set-theory']"
4216811,A trigonometric identity: $\frac1{\sin40 ^\circ}+\tan10^\circ=\sqrt{3}.$ [duplicate],"This question already has an answer here : Proving $\cot 20^\circ - \cot 40^\circ + \cot 80^\circ = \sqrt{3}$ (1 answer) Closed 2 years ago . My sister asked me such a trigonometric identity (her high school challenging problem): prove: $$\frac1{\sin40 ^\circ}+\tan10^\circ=\sqrt{3}.$$ I found that this is really true (surprising... with a calculator), but as an undergraduate equipped with calculus and linear algebra, I have no idea how to attack this problem. Thanks in advance for any help!",['trigonometry']
4216817,What is the logic behind the trick used to prove the summation series of Sine of Angles in Arithmetic Progression?,"why does multiplying by $2sin(b/2)$ cancel out most of the terms,what is the intuition behind it?how does multiplying by it creates a chain of terms that get cancelled? edit:1 I have attached an image about what I tried to simplify using $sin(d)$","['trigonometry', 'summation', 'sequences-and-series']"
4216820,"For all $x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$, who is $\xi'(x)\cdot h$?","If $f,g:\mathbb{R}^m\rightarrow\mathbb{R}^n$ are differentiable with $f\in C^1$ and $\xi:\mathbb{R}^m\rightarrow \mathbb{R}$ where $$\xi(x)=\langle g(x),\int_0^{|x|}f(tx)dt\rangle,$$ with $|x|=\sqrt{\langle x,x\rangle}$ , then for all $x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$ , who is $\xi'(x)\cdot h$ ? I don't know how to proceed with the second term. What I've done is: $\forall x\not =0 \in\mathbb{R}^m$ and $h\in\mathbb{R}^m$ , $$\xi'(x)\cdot h=\lim_{w\to 0}{\frac{\xi(x+wh)-\xi(x)}{w}}=$$ $$=\lim_{w\to 0}\left\langle\frac{g(x+wh)-g(x)}{w},\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right\rangle=$$ $$=\left\langle g'(x)\cdot h,\lim_{w\to0}\left(\frac{\int_{0}^{|x+wh|}f(tx+twh)dt-\int_{0}^{|x|}f(tx)dt}{w}\right)\right\rangle$$ And after that, how could I establish that $$\lim_{h\to 0}\frac{r_\xi(h)}{|h|}=\lim_{h\to 0}\frac{\xi(x+h)-\xi(x)-\xi'(x)\cdot h}{|h|}=0?$$","['integration', 'inner-products', 'derivatives', 'analysis']"
4216828,What is the largest possible first place score in olympic sport climbing?,"Consider a competition with $n$ competitors, and $k$ contests. Each competitor receives a combined score, which is the product of their placements in each of the contests (assume no ties in the contests). The competitor with the lowest combined score wins. For example, olympic sport climbing has $n=20$ competitors and $k=3$ for speed, boulder, and lead. Brooke Raboutou got 12th place in speed, 2nd place in boulder, and 8th place in lead for a combined score of 192. Janja Garnbret's combined score was $14\times 1\times 4=56$ , which was the first-place score. What is the largest possible score of the winner(s) of such a contest? By brute force (i.e., computing all possible outcomes), here is the answer for some values of $n$ and $k$ . n\k 1 2 3 4 5 1 1 1 1 1 1 2 1 2 2 4 4 3 1 3 6 9 18 4 1 4 8 24 48 5 1 5 15 40 - 6 1 6 24 - - 7 1 7 35 - -","['combinatorics', 'extremal-combinatorics']"
4216843,"The proof differentiability of multilinear function using ""product"" of function.","In Kolk's Multidimensional Real Analysis I: Differentiation He used the following Corollary 2.4.3 to prove that a multilinear(k-linear) map $T \in Lin^k(\mathbf{R}^n,\mathbf{R})$ is differentiable.(In the proof of proposition 2.7.6) The whole argument is a single sentence: The differentiability of T follows from the fact that $T(a_1,\cdots, a_k) \in \mathbf{R}$ are polynomials in the coordinates of vectors $a_1,\cdots, a_k \in \mathbf{R}^n$ . See corollary 2.4.3. I know the fact that $T(a_1,\cdots, a_k) \in \mathbf{R}$ are polynomials in the coordinates of vectors $a_1,\cdots, a_k \in \mathbf{R}^n$ . It's a basic property of multilinear map. But how does the conclusion relate to corollary 2.4.3? Note that he defines the product $\langle f_1, f_2 \rangle$ as the composition: $
\begin{equation}
\langle f_1, f_2 \rangle: x \mapsto (f_1(x),f_2(x)) \mapsto \langle f_1(x), f_2(x) \rangle : \mathbf{R}^n \rightarrow \mathbf{R}^p \times \mathbf{R}^p \rightarrow \mathbf{R}
\end{equation}
$","['multivariable-calculus', 'multilinear-algebra']"
4216858,existence of certain Second-order partial derivatives,"The function $f(x,y)=|y|$ is an example for the following conditions: $f$ is defined on the whole $\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial x\,\partial x}$ and $\dfrac{\partial^2 f}{\partial x\,\partial y}$ do exist at any point $(x_0,y_0)\in\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial y\,\partial x}$ and $\dfrac{\partial^2 f}{\partial y\,\partial y}$ don't exist at some point $(x_0,y_0)$ Now I would like to find an example for slightly changed conditions: $f$ is defined on the whole $\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial x\,\partial x}$ and $\dfrac{\partial^2 f}{\partial y\,\partial x}$ do exist at any point $(x_0,y_0)\in\mathbb{R}^2$ $\dfrac{\partial^2 f}{\partial y\,\partial y}$ and $\dfrac{\partial^2 f}{\partial x\,\partial y}$ don't exist at some point $(x_0,y_0)$ and I'm not able to do this. Is this possible?","['partial-derivative', 'multivariable-calculus', 'derivatives']"
4216868,Evaluate $\lim_{x\to(e^{-1})^+}\frac{e^{\frac{\ln(1+\ln x)}x}}{x-e^{-1}}$,"Evaluate $\lim\limits_{x\to(e^{-1})^+}\dfrac{e^{\dfrac{\ln(1+\ln x)}x}}{x-e^{-1}}$ It's a $\dfrac00$ form. Approach $1$ : L'Hopital $$\lim_{x\to(e^{-1})^+}\dfrac{e^{\dfrac{\ln(1+\ln x)}x}\cdot\dfrac{\dfrac1{1+\ln x}-\ln(1+\ln x)}{x^2}}1\\=\lim_{x\to(e^{-1})^+}e^{\dfrac{\ln(1+\ln x)}x}\cdot\dfrac{1-\ln(1+\ln x)^{1+\ln x}}{x^2(1+\ln x)}$$ Again this is indeterminate. So, I abandoned this approach. Approach $2$ : Series expansion. $$\ln(1+\ln x)=\ln x-\frac{(\ln x)^2}2+\cdots$$ This is to be divided by $x$ . Don't think I am going anywhere with this. Any hint please?","['contest-math', 'limits', 'calculus', 'logarithms']"
4216879,a nonstandard differential equation - product of a function and her consecutive derivatives equal 1,"For which $n\in\mathbb{N}$ there exists an open interval $I$ and a function $f\colon I\to\mathbb{R}$ , that is $n$ times differentiable and $f(x)\cdot f'(x)\cdot\ldots\cdot f^{(n)}(x)=1$ for all $x\in I$ ? Own problem. For $n=1$ it's easy to see that $I=(0,\infty)$ and $f(x)=2\sqrt{x}$ work fine. It seems to complicate a lot already for $n=2$ : https://www.wolframalpha.com/input/?i=f%28x%29*f%27%28x%29*f%27%27%28x%29%3D1 (I hardly understand this reply). What tools could crack a problem like this?","['derivatives', 'ordinary-differential-equations']"
4216912,Are there infinitely many Fermat prime pairs?,"Suppose $p$ and $q$ are prime numbers. Let’s call the pair $(p, q)$ a Fermat pair iff $| p - q | < 2\sqrt{2} (pq)^{\frac{1}{4}}$ . Such prime pairs possess a rather interesting property: they can neatly be expressed using only their product (even though prime decomposition is generally a computationally hard problem). If I recall correctly, this fact was first discovered by Pierre de Fermat (hence the name of the pairs). To be concrete, if $N = pq$ and $p > q$ , then $$p = \lfloor \sqrt{N} \rfloor + 1 + \sqrt{(\lfloor \sqrt{N} \rfloor + 1)^2 - N}$$ $$q = \lfloor \sqrt{N} \rfloor + 1 - \sqrt{(\lfloor \sqrt{N} \rfloor + 1)^2 - N}$$ Proof: Suppose $a = \frac{p + q}{2}$ and $b = \frac{p - q}{2}$ . Then $N = a^2 - b^2$ . Now, from $b < \sqrt{2} N^{\frac{1}{4}} < \sqrt{2a}$ we can derive that $(a-1)^2 \leq N < a^2$ , which yields us our formulas. My question is: Are there infinitely many Fermat pairs? If the answer is known, it should be positive. Why? Because any pair of twin primes is a Fermat pair. Therefore the negative answer to this question would have given negative answer to the Twin Prime Conjecture (and that problem is currently open). However, if it is known, I would like to see a proof that there are infinitely many Fermat pairs.","['number-theory', 'prime-factorization', 'elementary-number-theory', 'prime-numbers']"
4216977,Definition of Frobenius-Perron (transfer) operator,"I came across two definitions for the transfer operator. The two definitions seem to me to be very similar, but I couldn't deduce one of them from the other. The first definition is the classic one, which I have already found in several books. Let $(X,\mu)$ be a $\sigma$ -finite measure space. For $f\in L^{1}(X,\mu)$ , define a signed measure $\mu_{f}$ on $X$ by $$\mu_{f}(A)=\int_{T^{-1}(A)}f \ d\mu$$ for all measurable $A\subset X$ . By the Radon-Nikodym theorem, there exists a unique element $P_{T}f\in L^{1}(X,\mu)$ such that $$\mu_{f}(A)=\int_{A}P_{T}f \ d\mu$$ for all measurable $A\subset X$ . The operator $P_{T}\colon L^{1}(X,\mu)\to L^{1}(X,\mu)$ define by $$\int_A P_{T}f \ d\mu = \int_{T^{-1}(A)} f\ d\mu\qquad \forall f\in L^{1}(X,\mu)$$ is called the Perron-Frobenius or transfer operator . The second definition can be found, for example, in the book An introduction to infinite ergodic theory by  J. Aaronson. The transfer operator $\widehat{T}:L^1(X,\mu)\to L^1(X,\mu)$ is given by, $$\int_X g (\widehat{T}f)\ d\mu = \int_X (g\circ T)f\ d\mu \qquad \forall f\in L^1(X,\mu),\ g\in L^{\infty}(X,\mu)$$ this equality is deduced from the following equality, $$\widehat{T}f=\frac{d\nu_f \circ T^{-1}}{d\mu}\qquad \text{where } \nu_f (A)= \int_A f \ d\mu$$ So I would like to know if $P_{T}$ and $\widehat{T}$ are the same operator. It seems so, but I couldn't get to prove it.","['measure-theory', 'operator-theory', 'ergodic-theory', 'analysis', 'dynamical-systems']"
4216996,What is the distribution of the weighted sum of two multivariate normal random variables?,"Say we have two multivariate normal random variables $X$ and $Y$ with the same dimensionality, means $\mu_x$ and $\mu_y$ , and covariance matrices $\Sigma_x$ and $\Sigma_y$ . It is possible to show that the distribution of $X+Y$ is also multivariate random normal with mean $\mu_x + \mu_y$ and covariance $\Sigma_x+\Sigma_y$ , but what if the distributions are not equally weighted? Formally, what is the distribution of the sum of $p_xX+(1-p_x)Y$ where $p_x\in(0,1)$ is the proportion of the sum attributable to the distribution $X$ ?","['statistics', 'probability-distributions', 'normal-distribution', 'probability-theory', 'probability']"
4217003,Explicit description of $\mathbb{Z}\otimes_{\mathbb{N}}\mathbb{Z}$,"$\newcommand{\Q}{\mathbb{Q}}\newcommand{\N}{\mathbb{N}}\newcommand{\Z}{\mathbb{Z}}$ Recall the definition of the tensor product $\otimes_{\mathbb{N}}$ of commutative monoids (see also this note by Harold Simmons ). Question. Is there an explicit/""nice"" description of the tensor product $\Z\otimes_{\N}\Z$ , where $\Z=(\Z,\cdot,1)$ is the multiplicative monoid of integers? A partial description. Eric Wofsey mentioned a really nice way to partly describe $\Z\otimes_\N\Z$ here : since $\Z \setminus \{0\} \cong\N^{\oplus\N}\oplus\Z_2$ via the map sending a non-zero integer $k= \pm 2^{n_1}3^{n_2}5^{n_3}\cdots$ to $((n_1,n_2,n_3,\ldots),\mathrm{sgn}(k))$ , we may use distributivity of tensor products along direct sums to write \begin{align*}
(\Z\setminus\{0\})\otimes_{\N}(\Z\setminus\{0\}) &\cong (\N^{\oplus\N}\oplus\Z_2)\otimes_\N(\N^{\oplus\N}\oplus\Z_2)\\
&\cong (\N^{\oplus\N}\otimes_\N\N^{\oplus\N})\oplus(\N^{\oplus\N}\otimes_\N\Z_2)\oplus(\Z_2\otimes_\N\N^{\oplus\N})\oplus(\Z_2\otimes_\N\Z_2)\\
&\cong \N^{\oplus\N}\oplus\Z_2^{\oplus\N}\\
&\cong (\Z\setminus\{0\},\cdot)^{\oplus\N}.
\end{align*} One could then add back $0$ to each factor of $\Z\setminus\{0\}$ above, adding new elements and relations to $(\Z\setminus\{0\},\cdot)^{\oplus\N}$ . What is the result of this?","['monoid', 'semigroups', 'abstract-algebra', 'tensor-products']"
4217009,Why are zeros of functions so important?,"I realise this is a general question. I am self-teaching mathematics and I have observed that the zeros of real and complex functions are of much interest. Question: Why are the zeros of real or complex so important? I was going to say that the zeros completely determine a function because the function can be factorised into factors, with each factor corresponding to one of the zeros. For example, $f(x) = (x-1)(x-3)$ is completely determined by its zeros at $x=1$ and $x=3$ . However this doesn't work for functions like $g(x) = (x-1)(x-3) + 2$ . I appreciate your patience with a question that might be naive.","['complex-analysis', 'functions', 'real-analysis']"
4217038,"Profile decomposition of $f$ satisfying $\lim_{x\to\infty}[f(x+a)-f(x)]=0$, for any $a\geq 0$.","Let $f\in C[0,\infty)$ , and $\forall\ a\geq 0, \lim_{x\to\infty}[f(x+a)-f(x)]=0.$ Show that there exists continuous $g$ and continously differentiable $h$ , such that $f=g+h$ , $\lim_{x\to\infty}g(x)=0, \lim_{x\to\infty}h'(x)=0$ . Let $h(x)=\int_x^{x+1}f(t)dt$ , then $\lim_{x\to\infty}h'(x)=0$ simutanenously. But the property of $g$ maybe difficult to prove.","['continuity', 'calculus', 'derivatives']"
4217072,Combinatorics - Probability of given two cards next to each other (Total cards = 13),"There is a set of 13 cards, numbered from 1 to 13. What is the probability that when these cards are randomly laid out in a line, Card-1 and Card-2 will be next to each other? The given answer (from CK12.org) is $1/{13 \choose 2}=\frac{1}{78}$ which does not make sense to me. My answer is; Total permutations = $13!$ Permutations with Card-1 and Card-2 next to each other = Put card-1 and card-2 to a bigger single card, so there are 12 cards, therefore $12!$ permutations. However, within the bigger card, there are $2!$ permutations, so $2\cdot12!$ permutations where card-1 and card-2 are next to each other. Therefore the probability = $\frac{2\cdot12!}{13!}=\frac{2}{13}$ . Can anyone help me understand where I have gone wrong?","['permutations', 'combinatorics']"
4217089,Relationship between the covector $(df)_p$ and the differential $df_p$ of $f$ at $p$,"Let $M$ be a smooth manifold and let $f\in C^\infty(M)$ . The differential of $f$ at $p\in M$ is the linear map $df_p:T_p M\to T_{f(p)}\mathbb{R}$ defined by $$df_p(v)(g)=v(g\circ f),$$ where $v$ is a derivation in $T_p M$ and $g\in C^\infty(\mathbb{R})$ . On the other hand, given $f\in C^\infty(M)$ , we can obtain a smooth covector field $df$ on $M$ by defining $$(df)_p(v)=vf$$ for $p\in M$ and $v\in T_p M$ . Some people claim that $(df)_p$ and $df_p$ are actually the same thing, by identifying $T_{f(p)}\mathbb{R}$ with $\mathbb{R}$ . But I would like to cast a doubt on it. Let $g\in C^\infty(\mathbb{R})$ . If the claim is true, we should be able to see that $$D_{(df)_p(v)}\Big|_{f(p)}g:=\frac{d}{dt}\Big|_{t=0}g(f(p)+t(df)_p(v))=v(g\circ f),$$ in which I try to go through $\mathbb{R}\cong\mathbb{R}_{f(p)}\cong T_{f(p)}\mathbb{R}$ . But I ended up with $$\frac{d}{dt}\Big|_{t=0}g(f(p)+t(df)_p(v))=g'(f(p))(vf).$$ What seems to be the problem in between my calculation? Thank you.","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
4217094,Inverting the Hopf map from $\mathbb{CP}^1$ to $\mathbb{S}^2$,"Let $$
\alpha=e^{i\varepsilon}\cos\left(\frac{\theta}{2}\right)\qquad\beta=e^{i\kappa}\sin\left(\frac{\theta}{2}\right)
$$ be 2 generic complex numbers, we could find a Hopf map from $\mathbb{CP}^1$ to $\mathbb{S}^2$ : $$
\mathcal{H}\begin{bmatrix}\alpha\\\beta\end{bmatrix}
=
\begin{bmatrix}2\text{Re}(\bar\alpha\beta) \\ 2\text{Im}(\bar\alpha\beta)\\|\alpha|^2-|\beta|^2\end{bmatrix}
$$ I wonder is this a bijection? It looks like given a vector in $\mathbb{S}^2\subset\mathbb{R}^3$ , we could solve for $\alpha$ and $\beta$ . However, I'm not pretty sure how we could express this inverse map that goes from $\mathbb{S}^2$ to $\mathbb{CP}^1$ . Thanks!","['complex-analysis', 'hopf-fibration']"
4217099,Partial derivative of $x^TA^TAx$ with respect to $A$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm trying to evaluate $\nabla_{A} x^TA^TAx$ for vector $x \in \mathbb{R}^n$ , and matrix $A \in \mathbb{R}^{m \times n}$ . I evaluated it elementwise and was wondering if there is a clean, closed form formula for this?","['matrices', 'matrix-calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
4217177,How to draw six lines in 3-d space such that angle between any pair of them is the same.,"My boss claimed earlier today that its possible to draw 6 lines in 3-d space, all passing through the origin such that the angle between any pair of them is the same as that between any other pair. As an example, the 3-d coordinate system with $x$ , $y$ and $z$ axes is such a system with three lines where the angle between any pair is $\frac{\pi}{2}$ . I can't imagine what a picture like this (with the $6$ lines) would look like. Can anyone provide insight into this? He also said this was an application of linear algebra.","['solid-geometry', 'linear-algebra', 'geometry', '3d']"
4217208,Genus zero affine curves,"Let $k$ be a number field. I am attempting to classify all the affine genus zero smooth geometrically integral curves up to its (smooth) compactification (or projectivization, if I'm not mistaken). It is known that any smooth projective geometrically integral curve of genus zero with a $k$ -point is isomorphic to $\mathbb{P}^1_k$ . Question 1. If $X$ is a genus zero affine curve over $k$ such that $X(k) \neq \emptyset$ , is it true that the compactification of $X$ will be isomorphic to $\mathbb{P}^1_k$ ? Question 2. In the case where $X$ has no $k$ -points, would its compactification be a smooth projective conic in $\mathbb{P}^2_k$ ? Are they unique up to isomorphism? I'm asking these questions with reference to the classification of genus zero curves in the projective case found here: Curves of genus 0 .","['algebraic-curves', 'algebraic-geometry']"
4217227,Exchange of the one-sided derivative and the one-sided limit of a real variable,"Let $f$ be a real-valued continuous function on $[0, 1)$ . Suppose that $f$ is infinitely-differentiable on $(0 , 1)$ . Is there some sufficient condition for $f$ to be right-differentiable at $0$ with $$f_{+}'(0) = \lim_{x \to 0^{+}} f'(x),$$ where I use the notation $f_{+}'(0) := \lim_{\Delta x \to 0^+} \frac{f(\Delta x) - f(0)}{\Delta x}$ for the right derivative of $f$ at $0$ ?","['limits', 'derivatives', 'analysis']"
4217283,How large must $f(f(z))-z^2$ be in the unit disk?,"This is a follow-up question to $|f(f(z))-z^2|$ must be large somewhere in the disc $\mathbb{D}$? , where the following was proven: Let $f$ be holomorphic in the unit disk $\Bbb D$ with $f(0) = 0$ and $|f(z)| < 1$ for all $z \in \Bbb D$ . Then $$ \tag{*}\sup_{z \in \Bbb D} |f(f(z))-z^2| \geq \frac 14 \, .$$ The given proofs work by assuming the contrary, and applying Parseval's identity or the Schwarz-Pick theorem to $F(z) = 4(f(f(z))-z^2)$ . This leads to inequalities for the first Taylor coefficients of $f$ which can not be satisfied. Inspection of those proofs shows that the used inequalities are not sharp, which leads to the conjecture that the number $1/4$ in $(*)$ can be replaced by a larger number. Therefore I am curious what the best possible lower bound in $(*)$ would be. Let $$
 {\cal F} = \{ f: \Bbb D \to \Bbb D \text{ holomorphic}, \, f(0) = 0 \}
$$ be the family of functions to which the Schwarz lemma can be applied. For $f \in \cal F$ define $$
 S(f) = \sup_{z \in \Bbb D} |f(f(z))-z^2| \, ,
$$ and finally set $$
 C = \inf_{f \in \cal F} S(f) \, .
$$ We know that $C \ge 1/4$ , and the question is if better estimates or the exact value can be computed. What I can show is that $$ \tag{**}
 \boxed{\frac 12 \le C \le 1 \, .}
$$ The upper bound $C \le 1$ comes from choosing $f(z) = 0$ . For the lower bound one proceeds as in the linked question above, but with more careful estimates. Assume that $f \in \cal F$ , $f(z) = az + bz^2 + \cdots$ , and $$ \sup_{z \in \Bbb D} |f(f(z))-z^2| < 1/2 \, .$$ Then $|a|^2 + b \le 1$ and \begin{align}
1 &\ge 4 |a|^4 + 2 |a(a+1)b - 1| \\&\ge 4 |a|^4 + 2\left(1 - |a|(|a|+1)(1-|a|^2)\right) \\&= 6|a|^4+2|a|^3-2|a|^2-2|a|+2.\end{align} On the other hand, plotting the right-hand side as a function of $|a| \in [0, 1]$ shows that it is always strictly larger than one: It is also confirmed by WolframAlpha that the minimum of the right-hand side is $\approx 1.1185 > 1$ . This is a contradiction and completes the proof that the supremum is at least $1/2$ . My question: Are there better bounds for $C$ , or can the exact value be computed?","['complex-analysis', 'inequality']"
4217303,Solving a simple matrix multiplication/evaluation problem in a shorter/simpler way,"I've solved the following problem by calculating matrix A's entries as variables and then evaluating them using the system of linear equations. As I'm a bit new to matrices, I wanted to know whether there are shorter ways to solving this problem cause solving it from the way I did seems so long... If $$
	\begin{bmatrix} 
	2 & 1 \\
	\end{bmatrix} . A =\begin{bmatrix} 
	3 & 5 \\
	\end{bmatrix}
$$ and $$
	\begin{bmatrix} 
	3 & 4 \\
	\end{bmatrix} . A =\begin{bmatrix} 
	-1 & 2 \\
	\end{bmatrix}
$$ what's the value of $$
	\begin{bmatrix} 
	8 & 9 \\
	\end{bmatrix} . A
$$ $A$ is a matrix",['matrices']
4217309,False implies anything [duplicate],"This question already has answers here : In classical logic, why is $(p\Rightarrow q)$ True if $p$ is False and $q$ is True? (18 answers) Closed 2 years ago . I understand Implication , as follows: p = rain stopped.
q = i go out.

p->q =  if ""rain stopped"" then ""i go out"".

p                       q                       p->q
-----------------------------------------------------------------------------------------
F                       F                       T
(rain is not stopped)   (i don't go out)        (if rain is not stopped, I don't go out)
F                       T                       F
(rain is not stopped)   (i go out)              (if rain is not stopped, I go out)
T                       F                       F
(rain is stopped)       (i don't go out)        (if rain is stopped, I don't go out)
T                       T                       T
(rain is stopped)       (i go out)              (if rain is stopped, I go out) But, this video says that $F \rightarrow T = T$",['discrete-mathematics']
4217318,Prove the following by contradiction: for any $n \in \mathbb{Z}$ we have $9 \nmid (n^2 + 3)$,"I understand that I need to derive false from $\neg(9 \nmid (n^2 + 3))$ , or $9 \mid n^2 + 3$ . To do this, I attempted to say $n^2 + 3 = 9k$ for some $k \in \mathbb{Z}$ . Then I tried something like $3 = -n^2 = -3(k - 1)$ , but $-3 \mid 3$ . That doesn't contradict the statement. I think that I'm missing something very fundamental here. Any clarity would be appreciated.",['discrete-mathematics']
4217328,How to show $\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx$ converges / diverges,How to show $\int_{k}^{\infty}\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})dx$ converges for $k=1$ and diverges for $k=0$ ? I have that $\left|\frac{1}{\sqrt{x}}\sin({x+\frac{1}{x}})\right|\geq\frac{\left|\sin({x+\frac{1}{x}})\right|}{x+\frac{1}{x}}$ . But this doesn't get my anywhere useful. Is there a function that could bound and show convergence at $\infty$ ?,"['integration', 'calculus', 'improper-integrals', 'real-analysis']"
4217369,Solving the integral $\int_{0}^{1} d{v} \frac{e^{-y^2(1+v^2)}}{1+v^2}e^{\frac{2 t v^2}{v^2+1}}$,"For my research, I have to solve many integrals of the Owen's T function . As such, I am having struggles in calculating the integral $$
\int_{0^{-}}^{t}d{s} \  e^{(t+s)}  \ \operatorname{T}\left(\frac{x}{\sqrt{2t}}, \sqrt{\frac{s}{2t-s}}\right).
$$ I asked a similar question in [ 1 ] for the integral $$
\int_{0^{-}}^{t}d{s} \ \operatorname{T}\left(\frac{x}{\sqrt{2t}}, \sqrt{\frac{s}{2t-s}}\right),
$$ which was beautifully answered. Nevertheless, if one follows the same procedure the exponential complicates things. What I have done until know is following [ 1 ] which yields \begin{align}
f(x,t)=\int_{0^{-}}^{t}d{s} \ e^{(t+s)}  \ T\left(\frac{x}{\sqrt{2t}}, \sqrt{\frac{s}{2t-s}}\right)=t\int_{0}^{1} d{u} \  e^{t(1+u)}  \ T\left(\sqrt{2}y, \sqrt{\frac{u}{2-u}}\right)
\end{align} where we performed the change of variable $s=tu$ and took $y=\frac{x}{2\sqrt{t}}$ . Substituting the definition for Owen's T function and using Tonelli theorem gives \begin{align}
    f(x,t)&= \frac{1}{2\pi}t \int_{0}^{1}d{u}\int_{0}^{\sqrt{\frac{u}{2-u}}}d{v} \ e^{t(1+u)}  \ \frac{e^{-y^2(1+v^2)}}{1+v^2}\\
    &=\frac{1}{2\pi}t \int_{0}^{1}d{v} \ \int_{\frac{2v^2}{1+v^2}}^{1}d{u} \ e^{t(1+u)}  \ \frac{e^{-y^2(1+v^2)}}{1+v^2}\\
    &= \frac{1}{2\pi}t \int_{0}^{1} d{v} \  \frac{e^{-y^2(1+v^2)}}{1+v^2}\left(\frac{e^{2 t}}{t}-\frac{e^{\frac{2 t v^2}{v^2+1}+t}}{t}\right)\\
    &= \frac{1}{2\pi} e^{2 t}\int_{0}^{1} d{v} \  \frac{e^{-y^2(1+v^2)}}{1+v^2}-\frac{1}{2\pi}e^{t} \int_{0}^{1} d{v} \  \frac{e^{-y^2(1+v^2)}}{1+v^2}e^{\frac{2 t v^2}{v^2+1}}\\
    &= T\left(\tfrac{x}{\sqrt{2t}}, 1\right)e^{2 t}+\frac{1}{2\pi} e^{t} \color{blue}{\int_{0}^{1} d{v}  \frac{e^{-y^2(1+v^2)}}{1+v^2}e^{\frac{2 t v^2}{v^2+1}}}\\
\end{align} The Owen T term, i.e. the first term, can be written in terms of Error function. However, I am struggling in solving the integral in the second term. I have tried integrate by part, but this just yield a more complicated expression. Someone who knows a way forward? Also I checked above calculation numerically and it indeed holds up! In[270]:= ClearAll[""Global`*""];
int1[x_, t_]:=OwenT[x/Sqrt[2t],1]*Exp[2t]+1/(2\[Pi])*Exp[t]*NIntegrate[ Exp[-(x/(2Sqrt[t]))^2*(1+v^2)]/(1+v^2)*Exp[(2tv^2)/(1+v^2)], {v,0,1},WorkingPrecision->7] ;
int2[x_, t_]:=NIntegrate[Exp[(t+s)]*OwenT[x/Sqrt[2t],Sqrt[s/(2t-s)]], {s,0,t},WorkingPrecision->7] ;
{x,t}=RandomReal[{0,1},2,WorkingPrecision->50]; 
int1[x, t]
int2[x, t]

Out[274]= 0.01602865
Out[275]= 0.01603027","['integration', 'calculus', 'definite-integrals', 'error-function']"
4217394,Why is the second derivative of $f:\mathbb R^n \to \mathbb R$ a function $\mathbb R^n \times \mathbb R^n \to \mathbb R$?,"In my multivariable analysis class notes my teacher wrote the following: Let $D \subseteq \Bbb R^n$ , $p \in D$ and $f:D \to \mathbb R$ be a class $C^2$ function. The function: $$\mathbb R^n \times \mathbb R^n \to \mathbb R$$ $$((y_1,...,y_n),(z_1,...,z_n)) \mapsto \sum_{i,j = 1}^n y_i z_j \frac{\partial ^2 f}{\partial x_j\partial x_i}(p)$$ Is the second derivative of the function $f$ at the point $p$ , denoted ad $f''(p)$ . Why is the second derivative a function with domain $\mathbb R^n \times \mathbb R^n$ ? If $\mathcal L(A,B)$ denotes the set of all bounded linear operators from $A$ to $B$ , shouldn't the second derivative of $f$ at $p$ be a function $f'' : \mathbb R^n \to \mathcal{L}( \mathbb R^n,\mathcal{L}(\mathbb R^n, \mathbb R) )$ instead?","['multivariable-calculus', 'derivatives']"
4217422,Ratios on bounds of sum of divisors of $8n+5$ and $2n+1$,"I am trying to understand if it can be shown that $\frac{S(8n+5)}{S(2n+1)}<1$ is not possible, where $S(n)$ is the sum of divisors of $n$ . There seems to be no example that's available till $100$ million (that's no proof of course)
In general is there any known lower bound on $\frac{S(m n+p)}{S(k n +p')}$ where $m,k$ are integers and $p,p'$ are coprime and $k<m$ .","['number-theory', 'divisor-sum', 'modular-arithmetic']"
4217432,Calculate the measure of segment AB in triangle rectangle ABC,"For reference: Given the triangle $ABC$ , straight at $B$ . The perpendicular bisector of $AC$ intersects at $P$ with the angle bisector of the outer angle $B$ , then $AF \parallel BP$ ( $F\in BC$ ) is drawn.
If $FC$ = $a$ , calculate BP(x). (Answer: $\frac{a\sqrt2}{2})$ My progress: Point P is on the circumcircle of ABC because the angle $\measuredangle ABP = 135^o$ $ Where~ AC = 2R\\
\triangle CBP \rightarrow
PC^2 = BP^2 + BC^2 - \sqrt2BCBP\\
but~PC = PA = R\sqrt{2} \text{(since P is in the bisector AC)}\\
2R^2 = BP^2 +BC^2 - \sqrt{2}BCBP\\
0= BP^2 - \sqrt{2}BCBP + BC^2 - 2R^2\\
0= (BP - \frac1{ \sqrt{2}}BC)^2 + \frac{BC^2}2 - 2R^2\\
0= (BP - \frac1{ \sqrt{2}}BC)^2 + \frac{BC^2-4R^2}2\\
0= (BP - \frac1{ \sqrt{2}}BC)^2 - \frac{AB^2}2\\
$ I can't find the relationship between BC, AB and a...
If anyone finds another way to solve by geometry I would be grateful",['geometry']
4217484,Lax-Milgram and the existence of solution to parabolic equation,"I think it is standard and common to use Lax-Milgram theorem to prove the existence of solution to elliptic equation. However, can we use it to establish the existence of parabolic equation? I do not find some examples in standard PDE textbooks. Suppose I have a parabolic equation $$ \partial_t u - \partial_{x_j}(a_{ij} \partial_{x_i} u) + b_i \partial_{x_i} u + c u =f(x,t)$$ on $\Omega \times [0,T]$ . Then the weak formulation should be $$ \int_{\Omega} \partial_{t} u \varphi + a_{ij} \partial_{x_i} u \partial_{x_j} \varphi + b_i \partial_{x_i} u \varphi + c u \varphi-f\varphi=0,$$ for all $\varphi(x) \in H^1_0(\Omega)$ and a.e. $t\in[0,T]$ . But I do not know how can we define the bilinear mapping in this way. May I get some help?","['parabolic-pde', 'functional-analysis', 'partial-differential-equations']"
4217492,Fake proof of the Axiom of Specification,"I came up with a fake proof of the axiom of specification, but can't pinpoint exactly what's wrong with the proof. Let $P$ be some logical formula which evaluates sets to true or false, and let $X$ be some set. I claim that there exists a set $S=\{x\in X\mid P(x)\}$ . By choice, we can well-order $X$ . Thus, WLOG we can assume that $X$ is an ordinal. We now conduct transfinite induction to construct a function $\phi_X:X\to X+1$ . Base case: $\phi_0:0\to X+1$ is a function. Successor Case: Given $\phi_\alpha:\alpha\to X+1$ , let $\phi_{\alpha+1}$ be as follows: $$\phi_{\alpha+1}(x)=\begin{cases}\phi_\alpha(x)&x\in\alpha\\X&x=\alpha\wedge\lnot P(\alpha)\\\alpha&x=\alpha\wedge P(\alpha)\end{cases}$$ Inductive Case: Given $\phi_\alpha$ for all $\alpha<\gamma$ , let $\phi_\gamma(x)$ equal $\phi_\alpha(x)$ for any $x<\alpha<\gamma$ . Thus, we have a map $X\to X+1$ whose image is $S\cup\{X\}$ . Now, we extend this map to a binary relation $R$ on Set by sending any set not in $X$ to itself. Thus, by the axiom schema of replacement, $R[X]=S\cup\{X\}$ is a set, and thus, so is $S$ .","['elementary-set-theory', 'axiom-of-choice', 'fake-proofs']"
4217497,Prove/Disprove that you can't draw an X inside a box without lifting the pen,"I apologize if this is a repost, but I couldn't find the question in case it does actually exist here. I tried and it seems to me that we cannot draw a square with its diagonals without lifting the pen off the paper. How do you mathematically prove or disprove this claim?","['graph-theory', 'geometry', 'eulerian-path']"
4217498,"Is $\ \left\{\ \left(\frac{3}{2}\right)^n\mod 1:\quad n \in \mathbb{N}\ \right\}\ $ a dense subset of $\ [0,1]\ $?","Is $\ \left\{\ \left(\frac{3}{2}\right)^n\mod 1:\quad n \in \mathbb{N}\ \right\}\ $ a dense subset of $\ [0,1]\ $ ? I know how to prove that if $\ \gamma\ $ is irrational, then sets like $\ \left\{\ \gamma\ n\mod 1:\quad n \in \mathbb{N}\ \right\}\ $ is a dense subset of $\ [0,1].\ $ But my question above seems unrelated to this, I think. I've got a feeling the answer is yes, but I'm not sure how to go about proving it. This question arose when thinking about this question I asked a few days ago.","['elementary-number-theory', 'general-topology', 'proof-writing', 'real-analysis']"
4217540,Which sequences of $\pm 1$ can appear as the image of a Hilbert symbol,"In his book A Course in Arithmetic , Serre adresses the question of which sequences $\pm 1$ can appear as the image of a Hilbert symbol. By this I mean the following: Let $V$ denote the set of places of $\mathbb{Q}$ and let $(a,b)_v$ denote the Hilbert symbol of two numbers $a,b \in \mathbb{Q}^{\times}$ at a place $v \in V$ : Theorem. Let $(a_i)_{i \in I}$ be a finite family of elements in $\mathbb{Q}^{\times}$ and let $(\varepsilon_{i,v})_{i\in I, v \in V}$ be a family of numbers equal to $\pm 1$ . In order that there exists $x \in \mathbb{Q}^{\times}$ such that $(a_i,x)_v = \varepsilon_{i,v}$ for all $i \in I$ and all $v \in V$ , it is necessary and sufficient that the following conditions be satisfied: Almost all the $\varepsilon_{i,v}$ are equal to $1$ . For all $i \in I$ we have $\prod_{v \in V} \varepsilon_{i,v}=1$ (i.e. the sequence always satisfies Hilbert reciprocity). For all $v \in V$ there exists $x_v \in \mathbb{Q}_{\nu}^{\times}$ such that $(a_i,x_v)_v = \varepsilon_{i,v}$ for all $i \in I$ . The above is theorem 4 from section 2.2 of chapter 3 of ibid. I have two questions concerning this: Though I have searched quite extensively, I have not been able to find another resource where the question of which binary sequences can appear as the image of a Hilbert symbol is treated. Is anyone aware of any other place where a similar theorem is proved? We know that property (2.) above (Hilbert reciprocity) applies to any number field (in fact to any global field). Is there a theorem analogous to the above that applies to all number fields / global fields? Many thanks.","['algebraic-number-theory', 'number-theory', 'p-adic-number-theory', 'reference-request', 'abstract-algebra']"
