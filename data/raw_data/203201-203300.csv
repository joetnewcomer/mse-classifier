question_id,title,body,tags
4011110,Finding minimal sufficient statistics for this family coming from the given probability mass function,"Let $X_i\big|_{i = 1...n}$ be random sample from the PMF: $P(X_i = 0) = \frac{1-\theta}2;\;P(X_i = 1) = \frac12 ; P(X_i = 2) = \frac\theta2$ where $\theta\in(0,1)$ . Find the minimal sufficient statistics. Let $X = (X_1, \ldots, X_n) $ and $x = (x_1, \ldots, x_n)$ , then $P(X = x, \theta ) \\= \prod_{i=1}^n \big(\frac{1-\theta}2\big)^{I(x_i = 0)} \big(\frac{1}2\big)^{I(x_i = 1)} \big(\frac{\theta}2\big)^{I(x_i = 2)}\\ =  \prod_{i=1}^n \big(\frac{1}2\big)^{I(x_i = 0) +I(x_i = 1) + I(x_i = 2)} (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \\ = \prod_{i=1}^n \frac12 (1-\theta)^{I(x_i = 0)} \;\theta^{I(x_i = 2)} \text{ because  } I(x_i = 0) +I(x_i = 1) + I(x_i = 2) = 1\\ 
= \frac{1}{2^n}(1-\theta)^{\sum_{i = 1}^nI(x_i = 0)} \theta^{\sum_{i=1}^nI(x_i = 2)} $ Now, $$ \frac{P(X= x, \theta )}{P(X = y, \theta)} = (1-\theta)^{\sum_{i = 1}^nI(x_i = 0)- \sum_{i = 1}^nI(y_i = 0)} \;\theta^{\sum_{i=1}^nI(x_i = 2) - \sum_{i = 1}^nI(y_i = 2)}$$ is independent of $\theta$ iff $$\sum_{i = 1}^nI(x_i = 0)= \sum_{i = 1}^nI(y_i = 0)$$ and $$\sum_{i=1}^nI(x_i = 2) = \sum_{i = 1}^nI(y_i = 2)$$ Hence, $T(X) = (\sum_{i=1}^nI(X_i = 0), \sum_{i=1}^nI(X_i = 2))$ is minimal sufficient. Is my attempt correct?","['statistical-inference', 'statistics', 'sufficient-statistics']"
4011135,Jacobians meaning geometrically,"When tackling double and triple integrals, we always use the Jacobian matrix when changing variables, such as to polar coordinates. What actually is the Jacobian and what does it represent or mean geometrically? Also what does it represent geometrically in terms of infinitesimal areas $$ \delta x \delta y $$ .","['jacobian', 'multivariable-calculus', 'geometry']"
4011138,Green function derivation,"My professor derived the free-space dyadic Green function from the general Green function: $$\tag{1}
\stackrel{\leftrightarrow}{\boldsymbol{G}}=\left[\stackrel{\leftrightarrow}{\mathbb{I}}+\frac{1}{k^{2}} \nabla \nabla\right] G_{0}(R)$$ With the scalar Green function: $$\tag{2}
G_{0}(R)=\frac{e^{i k R}}{4 \pi R}, \quad R \equiv\left|\boldsymbol{r}-\boldsymbol{r}^{\prime}\right|$$ In the derivation, he states the following: $$\tag{3}
\nabla \hat{\mathbf{R}}=\nabla\left(\frac{\mathbf{R}}{R}\right)=\frac{\nabla \mathbf{R}}{R}+\mathbf{R} \nabla \frac{1}{R}$$ where $\boldsymbol{R}=\boldsymbol{r}-\boldsymbol{r}^{\prime}, \quad \widehat{\boldsymbol{R}}=\frac{\boldsymbol{R}}{R}$ . He says we can see from eq. (3) that $\nabla \mathbf{R}=\mathbb{I}$ , but I don't see that? Can someone explain how he gets to that conclusion?","['proof-explanation', 'calculus', 'derivatives', 'algebra-precalculus']"
4011157,Genus of an isolated curve on a surface,"Let $S$ be a smooth projective surface, and $C \subset S$ a smooth curve. Take the field to be $\mathbb{C}$ , so that the curve is a Riemann surface. Now suppose additionally that $C$ admits no deformations inside $S$ (so that the normal bundle of $C$ has no sections, $H^0(C,N_{C/S})=0$ , or these deformations are obstructed by $H^1(C,N_{C/S})$ ). My question is: are there bounds on the genus of the curve $C$ for a given surface $S$ ? In many simple cases, the genus of such a curve is zero. For example, this is true for any del Pezzo surface, and for any toric surface, such as the Hirzebruch surfaces. More generally, I might expect there to be a bound in terms of properties of the surface $S$ , such as the Chern numbers $\int_S c_1(T_S)^2$ and $\int_S c_2(T_S)$ . I would be very happy to be directed to a reference for this.","['complex-geometry', 'algebraic-geometry', 'algebraic-topology']"
4011170,Do you find this proof convincing?,"I must show that the function $f:\mathbb R^n \to \mathbb R$ defined as $ f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2 $ for fixed $\mathbf a_1,...,\mathbf a_k \in\mathbb R^n$ has a global minimum. Here's my reasoning: The function $f$ is continuous.
Pick $\delta \gt 0$ , by definition closed balls (neighborhoods) are compact; so define $\bar{B}_n(\mathbf 0,n\delta) =\{ \mathbf x\in\mathbb R^n :\Vert\mathbf x\Vert \le n\delta\}$ for $n \in \mathbb N$ , and define $f_n:\bar{B}_n(\mathbf 0,n\delta) \to \mathbb R$ as $f_n=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2$ . By the maximum/minimum value Theorem there is a $\mathbf y \in \bar{B}_n(\mathbf 0,n\delta) $ such that $f_n(\mathbf y)\le f_n(\mathbf x)$ for all $\mathbf x \in \bar{B}_n(\mathbf 0,n\delta)$ . Call $f_n(\mathbf y)= y_n$ . For all $k \in \mathbb N$ we have: $y_{n+1} \le y_n$ because $\bar{B}_k(\mathbf 0,k\delta) \subset \bar{B}_{k+1}(\mathbf 0,(k+1)\delta)$ , and $y_k \ge 0$ because $f_k(\mathbf x) \ge 0$ for all $\mathbf x$ s in the domain; so the sequence $\{y_n\}$ is bounded below and non increasing, hence it converges, as $n \to \infty$ , to a number $y$ . $y$ is the global minimum of $f(\mathbf x)=\sum_{j=1}^{k} \Vert \mathbf x -\mathbf{a}_j\Vert^2$ . I'm not so sure if I can claim that $y$ is indeed the global minimum of the function $f$ EDIT: I don't seem to be able to complete the proof with my method, also I'm thinking that if I prove there is a closed ball big enough (with all the $\mathbf a_i$ s in it) such that every element $\mathbf q$ outside the ball has its $f( \mathbf q)$ greater than the minimum inside then all the work above is superfluous. People in the comments and in the answers have provided a more efficient strategy: Let $f(\mathbf a_1)=\sum_{j=2}^{k} \Vert \mathbf a_1 - \mathbf a_j\Vert^2=M$ , and define the closed ball $\bar{B}(\mathbf a_1,\sqrt{M})= \{ \mathbf x \in \mathbb R^n: \Vert \mathbf x - \mathbf a_1 \Vert \le \sqrt{M} \}$ . By the minimum value theorem there is some $\mathbf y$ in $\bar{B}(\mathbf a_1,\sqrt{M})$ such that $f(\mathbf y) \le f(\mathbf x)$ for all $\mathbf x$ in $\bar{B}(\mathbf a_1,\sqrt{M})$ . Pick any element $\mathbf q \notin \bar{B}(\mathbf a_1,\sqrt{M})$ , which means that $0 \le \sqrt{M} \lt \Vert \mathbf q - \mathbf a_1 \Vert$ and thus $M \lt \Vert \mathbf q - \mathbf a_1 \Vert ^2$ . By definition we have $f(\mathbf y) \le f(\mathbf a_1) = M$ , but it's also true that $\Vert \mathbf q - \mathbf a_1 \Vert \le f(\mathbf q)$ . Therefore $f(\mathbf y) \lt f(\mathbf q)$ which completes the proof.","['multivariable-calculus', 'calculus', 'solution-verification', 'analysis']"
4011177,Eigenvalues of a matrix in relation to another matrix,"Let $A,B \in \mathcal{M}_n(\mathbb{K}) $ (where $\mathbb{K} \in \{\mathbb{R},\mathbb{C} \} $ ) invertible matrices. Prove that if $\lambda = 0 $ is an eigenvalue of matrix $C$ , where $C = BA - AB ~$ then $ \lambda = 1 $ is an eigenvalue of matrix $D~$ where $D = B^{-1}A^{-1}BA$ I don't know where to start this proof, since I can't recall any eigenvalue properties that relate sum or diference of matrices.","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4011230,On isomorphism of two quotient of polynomial rings,"$\mathbf{The \ Problem \ is}:$ Show that  the ring $R_1=\mathbb{C}[x,y,z]/(xy-z^2)$ is not isomorphic to $R_2 = \mathbb{C}[x,y,z]/(xy-z)$ . $\mathbf {My \ approach}:$ There was a hint stating to show that $R_2$ is a UFD and $x$ is irreducible in $R_1$ . Now, $x$ is not prime in $R_1$ as $x\mid xy=z^2$ but then primality of $x$ would mean $x\mid z$ , then $R_1$ will not be a UFD. I have only read up to free modules and Gauss' lemma, a proof involving those things only will be very helpful for me. A small hint is warmly appreciated.","['irreducible-polynomials', 'ring-theory', 'abstract-algebra', 'unique-factorization-domains']"
4011243,"Proving that for a CW pair $(X,A)$, $X \times I$ deformation retracts to $(X \times \left\{ 0 \right\}) \cup (A \times [0,1])$","I am trying to understand the proof of Allen Hatcher's Proposition 0.16 from ""Algebraic Topology"". If $X$ is a cell complex, he defines $(X,A)$ to be a CW-pair if $A \subseteq X$ is closed and is a union of cells i.e. images of open disks under extensions of their attachment maps via their boundaries. Let $D^n$ denote closed $n$ -disks and $I=[0,1]$ , as well as $X^n$ (resp. $A^n$ ) all the $n$ -cells in $X$ (resp. $A$ ). After proving that $D^n \times I$ deformation retracts to the base of the cylinder glued to its vertical boundary (that is $(D^n \times \left\{ 0 \right\}) \cup (\partial D^n \times I)$ ), which I get, he claims that $X^n \times I$ is obtained from $(X^n \times \left\{ 0 \right\}) \cup \left( (X^{n-1} \cup A^n) \times I \right)$ by attaching copies of $D^n \times I$ along $(D^n \times \left\{ 0 \right\}) \cup (\partial D^n \times I)$ . I don't understand why or how this is the case. By his definition, $X^n$ is obtained from $X^{n-1}$ by attaching the $D^n$ disks via maps $\varphi^n_\alpha: \partial D^n_\alpha \rightarrow X^{n-1}$ and taking $X^n := X^{n-1} \bigcup_{\varphi^n_\alpha} D^n_\alpha$ . I see that we can extend these maps to $\tilde\varphi^n_\alpha: \partial D^n_\alpha \times I \rightarrow X^{n-1} \times I$ and hence see $X^n \times I$ as the disjoint union of $X^{n-1} \times I$ with $D^n_\alpha \times I$ under the equivalences given by $\tilde\varphi^n_\alpha$ . But why is it that we can attach $D^n \times I$ precisely along $(D^n \times \left\{ 0 \right\}) \cup (\partial D^n \times I)$ ? (I also looked at the answer here but it doesn't help alleviate my confusion.)","['proof-explanation', 'general-topology', 'cw-complexes', 'algebraic-topology']"
4011259,Use induction to prove that pegs in a line can be interchanged,"I am looking into the following problem: Ten pegs of 2 colors are laid out in a line of 11 holes (5 black to
the left, a hole, 5 white to the right).   I want to interchange the
black and white pegs, but I am only allowed to move pegs into an
adjacent empty hole or to jump over one peg into an empty hole. Can I
make the interchange? The configuration is like this (the space is the empty hole): BBBBB WWWWW I think I proved it can be done by induction and I wanted to make sure my induction proof is sane. I start with $B=1$ and $W=1$ (denoting the numbers of black and white pegs). B W  
 BW  
WB   
W B So it is possible to make the swap for $B=1$ and $W=1$ . I also confirm that this is possible for $B=2$ and $W=2$ BB WW  //start  
B BWW  // move black to the right  
BWB W  // white jump's to the left  
BW BW  // move black to the right(inner was reversed)   
BWWB   // white jump's to the left  
BWW B  // move black to the right  
BW WB  // move white to the right (outer right reversed but move backwards)    
 WBWB  // jump black to the right   
W BWB  // move white to the left  
WB WB  // move black to the left (outer left reversed)  
WBW B  // move white to the left  
W WBB  // jump black to the right   
WW BB  // move white to the left Proof: Base case: For $B=1$ and $W=1$ we can interchange the pegs using only the valid moves. Inductive hypothesis: I can interchange $B = N$ and $W = N$ pegs using only valid moves Inductive step: We have $B = N + 1$ and $W = N + 1$ pegs. BBBBB...BBB WWW...WWWWW We can interchange the inner $B=N$ and $B=N$ by inductive assumption. Hence we get: BWWWW...WWW BBB...BBBBW I.e. we are left with a misplaced peg (the $nth + 1$ ) at each side. But we can move the $nth +1$ towards the center using valid moves as each one is just an application of the base case (subproblems of size $1$ ). I.e. BWWWW...WWW BBB...BBBBW  
B WWWW...WWWBBB...BBBBW  (all whites moved to the right) The B W which is at the outer left is just a subinstance of the original problem and can be interchanged W BWWW...WWWBBB...BBBBW Now all the whites can continue moving to the left and we do the same (symmetrical) process in the right side as well. In the last step we end up with WWWWW...WWB WBB...BBBBB Those (outer) misplaced pegs is a sub-instance of the original problem of size $1$ and can be interchanged. Hence we can do the interchange for $B = 5$ and $W = 5$ or in general any number of pegs $B=n$ and $W=n$","['puzzle', 'proof-writing', 'solution-verification', 'discrete-mathematics', 'induction']"
4011274,From which set does the number $\sqrt[3]{-1}$ belong to?,"I was trying to draw the function $f(x)=\sqrt[3]{x^2(6-x)}$ by hand (I'm in my first year of engineering; having Calculus I; this drawing is actually an exercise given for my class) and used WolframAlpha to see if I've got it right. Well, I was expecting to miss a few things on my first try, but I didn't even consider the possibility that I would start getting its domain wrong (I thought it was all the real numbers, however, the program said it was all real numbers equal to or below $6$ ). So, trying to understand what I'd missed, I've concluded that Wolfram must see $\sqrt[3]{-1}$ as only as a complex number when I swore it was at least real (since the equation $x^3 = -1$ has at least a real number that solves it, i.e. $-1$ ). I mean, if Wolfram is right (and I'm supposing it is), then $f(x=7)$ does not return the ""real number"" $\sqrt[3]{-49}$ (I don't even know if this number is real anymore, and I'm getting more and more confused whilst writing this text), and my professor has put a wrong graph as an answer for the exercise above. Look, although I wanted the ability to understand what I'm not understanding so that I could be, at least, didactic, I'll try to resume my confusion with this question (and with it, I'll try to understand on my own why Wolfram didn't consider numbers like $x=7$ for $f(x)$ domain): what in heavens is this $\sqrt[3]{-1}$ ? $\\$ PS: sorry if I've said anything wrong... I don't speak English fluently.","['real-numbers', 'calculus', 'functions', 'complex-numbers']"
4011275,"write five digits numbers using $0,0,0,1,1,2,2,2,2,3,3,3,4,4$.","It is given that we want to write five digits numbers using $0,0,0,1,1,2,2,2,2,3,3,3,4,4$ . How many ways are there to write it $?$ My thought In the first place, it seems cumbersome question a little bit , because i think that we should write them separately in terms of the leading term such that firstly i check over $1$ as leading term , then $2$ so on. However , this process is exhaustive and i hope to find  elegant approach for this question.","['combinatorics', 'discrete-mathematics']"
4011417,Different complex structures of a torus.,"Consider a complex $\mathbb C/\Lambda$ , once we choose a lattice $\Lambda$ , then the torus is uniquely determined. For example, if we choose different lattices: $\Lambda_1=\mathbb Z\oplus i\mathbb Z$ , $\Lambda_2=\mathbb Z\oplus 2i\mathbb Z$ , and let $T_1=\mathbb C/\Lambda_1$ , $T_2=\mathbb C/\Lambda_2$ , then do $T_1$ and $T_2$ have different complex structures? I know they have the same diffeomorphism sturcture, but why they have different complex structures? Does there exist an intuitive explanation why they have different complex structures?","['complex-analysis', 'complex-geometry', 'riemann-surfaces']"
4011451,"How to evaluate $\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\, dy$?","I need to solve the integral $$\int_{0}^{1}\int_{0}^{1} \sqrt{1 + 4(x^2 + y^2)}\,dx\,dy$$ I am using polar coordinates here to get : $$ \int_{0} ^{\pi/4}\int_{0}^{\sec \theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta +  \int_{\pi/4} ^{\pi/2}\int_{0}^{\operatorname{cosec}\theta} \sqrt{(1 + 4r^2)} r \,dr \,d\theta$$ After this integral becomes too complex to solve further . For eg : the first integral gives : $$\int_{0}^{\pi/4}\frac1{12}{((1 + 4\sec^2\theta)^{3/2} - 1)}\, d\theta$$ After this I am stuck how to proceed further, Please help. Thank You.","['integration', 'multivariable-calculus', 'multiple-integral']"
4011457,"How do I determine if $f(x)$ has a continuous derivative on $[a,b]$?","When finding the length of a curve using calculus, the first step is to determine whether $f(x)$ has a continuous derivative on the curve (i.e. on $[a,b]$ ). But how do we determine this? My understanding is that you need to find the second derivative, i.e. $f''(x)$ . If $f''(x)$ exists, this implies that $f'(x)$ is a continuous derivative along the curve of $f(x)$ . Is that accurate? I apologize if this is an obvious question. I just feel that my textbook/online resources give much more convoluted answers than this, when the solution is pretty simple. My concern is that I may be overlooking something.","['arc-length', 'calculus', 'derivatives']"
4011491,Divisors of n! including 1 which sum to n!,"I believe that given any positive integer $n$ , we can find a set of divisors of $n!$ , including $1$ , which sum to $n!$ I checked for the first 6 positive integers, and tried to prove it by induction on $n$ . However I was unable to formalise the proof, can anybody present a method of proving this? Or maybe without induction?","['number-theory', 'elementary-number-theory', 'prime-numbers']"
4011493,Prove that $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is integrable function.,"Let $ I\subseteq\mathbb{R}^{n} $ be a box.Let $ f_{i}:I\to[0,1] $ be integrable functions. Prove that $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is integrable function. My first intuition was to use Weierstrass M-test, but we never proved it for multivariable functions and I cant see why would it hold. The second intuition was to show that the set of discontinuities of $ \sum_{i=1}^{\infty}\frac{f_{i}}{2^{i}} $ is of measure zero, but im not sure how to show it. Any help would be appreaciated. Thanks in advance.",['multivariable-calculus']
4011524,Show there must be two numbers in the list whose difference is $12$.,"This question came up while tutoring: "" $55$ numbers are randomly selected from the first $100$ positive integers. Show there must be two numbers in the list whose difference is $12$ . "" It definitely requires the Pigeonhole Principle but we were unsure how to set it up. My idea was to list all of the possible pairs whose difference is $12$ : $$(1,13), (2,14), \ldots (87, 99), (88, 100)$$ of which there are $88$ pairs. I'm not sure what to do from here. I know similar questions have been asked here, like this one . Is there a way to apply those techniques to this problem?","['pigeonhole-principle', 'discrete-mathematics']"
4011539,How to prove differentiability of the following unknown function,"We have $f:\Bbb{R}^n\rightarrow \Bbb{R}^m$ s.t $f(x)=o(\|x-x_0\|)$ , and I have to prove $f$ is differentiable at $x_0$ . So by definition, I know that $f$ is differentiable if there exists a linear transformation $L_{x_0}$ s.t $\lim_{x\rightarrow x_0}\frac{f(x)-f(x_0)-L_{x_0}(x-x_0) }{\|x-x_0\|}=0$ . Also I know that $L_{x_0}(x-x_0)$ satisfies $L_{x_0}(x-x_0)\in O(\|x-x_0\|)$ . Any help would be appreciated.","['multivariable-calculus', 'derivatives', 'real-analysis']"
4011571,Distribution of sum of product of normal random variables.,"The distribution of a product $Z=XY$ of two normally distributed random variables is given by the product distribution https://mathworld.wolfram.com/NormalProductDistribution.html . What is the distribution of $Q=\sum_{i=1}^n Z_i$ , where $Z_i= X_i Y_i$ , and $X_i,Y_i\sim \mathcal N(0,\sigma_i)$ ? A nice answer was given in: Distribution of sum of product-normal distributions. by @wolfies, for the case where $\sigma_i=1$ (that is, all the $X_i,Y_i$ are identically distributed standard normal). The distribution can be expressed in terms of the modified Bessel function of the second kind. And as $n\to\infty$ , the distribution approaches a normal distribution. But I am interested in the case where the $\sigma_i$ are different. If no-closed form solution can be found, I am curious about when $p(Q)$ will approach a normal distribution. Is there an ""effective"" $n$ in terms of the $\sigma_i$ ? My first guess was the participation ratio $n_\text{eff} = (\sum_{i=1}^n \sigma_i^2)^2/\sum_{i=1}^n \sigma_i^4$ . Here is how I've approached it so far. The characteristic function of each $Z_i$ is given, I believe, by $$\varphi_{Z_i}(t)=\frac{1}{\sqrt{t^2\sigma_i^2 + 1}}$$ So by independence the characteristic function of $Q$ is given by $$ \varphi_Q(t) = \prod_{i=1}^n\varphi_{Z_i}(t) = \prod_{i=1}^n \frac{1}{\sqrt{t^2\sigma_i^2 + 1}}$$ And $p(Q)$ should be given by the inverse Fourier transform of $\varphi_Q(t)$ . I am stuck trying to perform the inverse Fourier transform, and any help would be greatly appreciated! edit: @Henry gave a very nice answer regarding the asymptotic behavior of $p(Q)$ as $n\to\infty$ . but I am still curious about the behavior of $p(Q)$ for $n$ small. Can $p(Q)$ be computed exactly? If not, how large must $n$ be before $p(Q)$ is approximately normal, as a function of the $\{\sigma_i\}$ ?","['statistics', 'central-limit-theorem', 'characteristic-functions', 'probability-distributions', 'normal-distribution']"
4011579,Proof by mathematical induction in sets,"How would I go about writing a proof for this question? Is there too little information given about set A itself? Prove by mathematical induction that |2^A| = 2^|A| for every finite set A. I'm new to the idea of sets but I do know that 2^A is the powerset of A and |2^A| is the number of elements in the powerset of A. E.g. A = {1,2,3}
2^A = { {},{1},{2},{3},{1,2},{1,3},{2,3},{1,2,3} }
|2^A| = 8 = 2^|A|","['induction', 'proof-writing', 'discrete-mathematics']"
4011693,Let $X$ and $Y$ be finite sets. Then the set $Y^X$ is finite and $\#(Y^X) = (\#Y)^{\#X}$.,"Let $X$ and $Y$ be finite sets. Then the set $Y^X$ is finite and $\#(Y^X) = (\#Y)^{\#X}$ . I can see how to do it combinatorially. Let $y \in Y$ . There are $\#(X)$ choices for such y to come from x, therefore we have $\#Y^{\#X}$ choices. I am not sure how to do it bijectively.","['elementary-set-theory', 'self-learning']"
4011707,Sum Identities and the uniqueness of Sin and Cos,"The below problem I am having a hard time solving. I suspect that there is some property that characterizes $\sin$ and $\cos$ and using that together with the Uniqueness Theorem (Identity Theorem) we can reach the conclusion. Let $S(z)$ , $C(z)$ be power series convergent for all complex numbers, satisfying the identities: $$S(z+w) = S(z)C(w) + S(w)C(z)$$ $$C(z+w) = C(z)C(w)-S(z)S(w)$$ $$C(0)=1, S(0)=0.$$ Prove that for some $m \in \mathbb{C}$ , $S(z) = \sin (mz), C(z) = \cos(mz)$ for all $z \in \mathbb{C}$ . One idea I had was to at least try to deduce the Pythagorean Identity. So I defined $F(z) = [C(z)]^2+[S(z)]^2$ . I was able to show $F(2z) = [F(z)]^2$ for each $z\in \mathbb{C}$ .","['complex-analysis', 'trigonometry', 'power-series', 'complex-numbers']"
4011716,The Kähler condition on a Riemann surface,"A Hermitian metric $h$ on a complex manifold $X$ is Kähler if the associated $2$ -form $\omega=\mathrm{Im} (h)$ is closed. This condition is trivial on compact Riemann surface, implying that every Hermitian metric on a compact Riemann surface is Kähler. An equivalent condition for a metric to be Kähler is that this metric ""osculates to order 2"" the standard metric, meaning that the components of the metric satisfy $$
h_{ij}(z) = \delta_{ij} + O(|z|^2).
$$ The proof of the fact that $d\omega=0$ implies this condition that most books do (for example, Griffiths-Harris or Huybrechts) is clear when $\dim X\geq 2$ . However, the proof given in those books does not work for a Riemann surface since the condition $d\omega=0$ is vacuous in that case. My question is how does one prove that, if $h$ is a Hermitian metric on a Riemann surface, that locally can be written as $$
h=h(z) dz\otimes d\bar{z},
$$ then $h(z)=1+O(|z|^2)$ .","['hodge-theory', 'riemann-surfaces', 'kahler-manifolds', 'complex-geometry', 'differential-geometry']"
4011742,What is the advantage of LU decomposition over storing the elimination matrix directly?,"The answers to this question indicate that one of the advantages of LU decomposition is that the algorithm do not redo the elimination steps. Why do we insist on storing the factorized form of $L$ and $U$ ?  Why not just save $E$ and $U$ , where \begin{align}
Ax = b
&\implies EAx = Eb && \text{(multiply both sides by $E$)} \\
&\implies Ux = Eb && \text{(since $EA = U$)}
\end{align} As you can see, we just need to store $U$ and the elimination matrix $E$ , so whenever we have a new matrix $b$ on the right-hand side, we can just multiply it out with $E$ , and use back-subsitution to solve for $x$ . What's advantage of storing $L$ and $U$ over storing $E$ and $U$ ?","['matrices', 'linear-algebra', 'matrix-decomposition']"
4011755,Questions about Definition of Vector Spaces,"I know that a vector space is over a field and has the operations of addition (among elements, which are called vectors) and scalar multiplication (in which a field element acts on a vector to yield a vector). Now, I want to reconcile this understanding with my previous intuition by asking some questions. Why can't a vector space have cross products/dot products defined inside it? What problems would be run into? In general, what kinds of ""products"" can we define in a vector space? Over what fields is $\mathbf{R}^n$ considered a vector space? Is it correct that vector space isomorphism only exists in the same field? Or can we claim that different vector spaces in different fields are isomorphic? What is the intuition used to extend vector spaces to infinite dimensions in functional analysis? Why do we only have to check for closure under addition and scalar multiplication to prove that a set is a vector space when a vector space has six (or seven, depending on how you count) axioms? I'm studying linear algebra for the second time in a proof-based context, so answers relevant to this level would be appreciated!","['abstract-algebra', 'linear-algebra', 'functional-analysis', 'vector-spaces']"
4011778,"Clockwise , Counter-clockwise convention","When we calculate the solid angle subtended by half a sphere we simply do it as follows: $\Omega=\int_{0}^{2\pi}\,d\phi \int_{0}^{\pi/2}\,sin \theta \,d \theta  =2\pi$ Here,: $\theta$ = co-latitude (clockwise taken positive) $\phi$ = azimuthal angle (anti-clockwise taken positve) Now this can also be calculated as: $\Omega=\int_{0}^{\pi}\,d\phi \int_{0}^{\pi}\,sin\theta \,d\theta  =2\pi$ Here, the azimuthal ring corresponding to a co-latitude just doesn't take a full $\,2\pi$ circle at once but gets added to another equal half from the lower portion in the $\pi/2$ to $\pi$ part of the co-latitude to give precisely the same thing. The first integral above calculates the solid angle subtended by the upper hemispherical cap surface area  while the second one calculates the angle subtended by the surface area of the $0$ to $\pi$ the right of the axis (speaking loosely). Now of course they should match due to the isotropy of the sphere and other obvious things. The problem I face is this:
What part of the convention (of choosing the co-latitude, as I have done) makes, an attempt to calculate the same thing like this: $$\Omega\int_{0}^{\pi}\,d\phi \int_{-\pi/2}^{\pi/2}\,sin\theta \,d\theta   $$ futile ? Since this i integral is obviously zero. Now this might sound like a stupid question and it probably is but it bugs me. It is obvious on one hand that since $\sin $ is an odd function about 0 so the area under its curve gets cancelled in the full-wave from $-\pi/2$ to $\pi/2$ thus giving zero. But on the other hand it seems that nothing of the clockwise-anticlockwise convention seems to stop me from writing the wrong integral and getting the wrong answer. Probably in the last integral the half azimuthal ring at a particular positive value of the co-latitude gets cancelled by the half azimuthal ring at the negative of that co-latitude value (this explanation doesn't seem ryt to me though). To put my rambling into context, This thing came to mind when I was trying to solve a problem where I had to calculate the electric flux through a part of a sphere. For that I needed to find the solid angle subtended by the shaded surface area.
Did it as follows: $\int_{-\pi/2}^{\pi/2}\,d\phi \int_{\pi/2-\alpha}^{\pi/2}\,sin\theta \,d\theta  $ which yields $\pi\sin\alpha$ This was listed as the correct option even when at $\alpha=\pi$ it becomes zero instead of $2$ In think this is again related to the fact that co-latitude changes sign to the left of the axis so probably the integral needs to to be split up or something. So to sum up, How would you put the limits for that integral so that it gives $2\pi$ ? (but remember I want the upper part to  become the surface that subtends) Do conventions and the clockwise
counter-clockwise messing up is whats confusing me or is it totally unrelated to all of the mumbo-jumbo I said?","['spheres', 'geometry']"
4011831,$n$-th Derivative $\frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|}$ via Recursive Product Rule,"Let $g(x) = x^{-\frac{1}{2}}$ and $f(x) = e^{-\sqrt{x} |\omega|}$ . I am trying to find an expression for the $M$ -th derivative of their product: \begin{align}
    \frac{d^M}{dx^M} \left[ f(x) g(x) \right] = \sum_{k=0}^{M} \binom{M}{k} f^{(M-k)}(x) g^{(k)}(x)
\end{align} where $M$ is an even integer and $\omega \in \mathbb{R}$ is a constant parameter. The derivative for $g(x)$ is easy: \begin{align}
    \frac{d^{k}}{d x^{k}} g(x) = \frac{(2k-1)!!}{2} x^{-\frac{2k+1}{2}}
\end{align} However, for $f(x)$ , it is not so easy, and this is where I get stuck. For the $M$ -th derivative of $f(x)$ , Mathematica gives me: M! DifferenceRoot[
   Function[{\[FormalY], \[FormalN]}, {(2 + 6 \[FormalN] + 
          4 \[FormalN]^2) \[FormalY][1 + \[FormalN]] + 
       4 (1 + \[FormalN]) (2 + \[FormalN]) x \[FormalY][
         2 + \[FormalN]] - \[FormalY][\[FormalN]] Abs[w]^2 == 
      0, \[FormalY][0] == 
      E^(-Sqrt[x] Abs[w]), \[FormalY][1] == -((
       E^(-Sqrt[x] Abs[w]) Abs[w])/(2 Sqrt[x]))}]][M] But I have no idea what any of that means, and their site is not very helpful... It seems that the solution will involve some kind of recursive application of the product rule, however, I can't seem to find a pattern after I do a handful of layers... Question: What is $\frac{d^{n}}{d x^{n}} f(x) = \frac{d^{n}}{d x^{n}} e^{-\sqrt{x} |\omega|}$ when $n$ is an even integer?","['calculus', 'partial-derivative', 'radicals', 'derivatives', 'exponential-function']"
4011835,Identity map between two Banach spaces always bounded,"Let $X$ be a Banach spaces, $\| \cdot \|_1$ and $\| \cdot \|_2$ be two different norms. Suppose that $\| x\|_1 \leq M\| x\|_2$ for all $x\in X$ . I've seen many places that say that the identity map $I: (X, \| \cdot \|_1) \rightarrow (X, \|\cdot \|_2)$ is always bounded, i.e. continuous. But I'm not sure why it's true. And where does it use the completeness? This is how I have attempted to prove it. Let $x\in X$ . Then $\|I(x) \|_1= \|x \| _2 \leq \| I\|_1 \| x\|_1 \leq M\| x\|_2$ . Not sure how to proceed. Thank you.","['banach-spaces', 'functional-analysis']"
4011946,Is there a way to prove that $A \setminus C = (A \setminus (B \cup C)) \cup ((A \cap B) \setminus C)$?,"Drawing out the Venn Diagrams I know this is true. $(A \setminus (B \cup C))$ is just all the values that are solely in A. $((A \cap B) \setminus C)$ is just all the values in both A and B but not in C. And taking the union of these two leads us to the values that are either only in A or in only A and B (but not C). I am unsure how to rigorously prove this, however. Would starting off by letting $x \in (A \setminus (B \cup C))$ be a good start? Venn Diagram","['elementary-set-theory', 'proof-writing']"
4011973,What is the Type 1 error for the following test?,"For testing the hypotheses H0: P = 0.8 versus H1: P ≠ 0.8 at 1% significance level, we
obtain a sample of n = 100 and p = 0.75. Options: A. 0.100 B. 0.140 C. 0.212  (Correct answer) D. 0.010 E. 0.020 So far I have, z= (P^-P)/std dev = -1.25 za/2 = 2.575 Don't reject null But I don't know how to get the answer","['statistics', 'probability-distributions', 'probability']"
4011975,Show that if $K \leq H \leq G$ and $K \triangleleft G$ then $K \triangleleft H$,Show that if $K \leq H \leq G$ and $K \triangleleft G$ then $K \triangleleft H$ My try: Let $a\in H$ then $a\in G $ because $H\leq G$ And how $K \triangleleft G$ then $aKa^{-1} \leq K \ \ \ \forall  \ a \in H$ (because $a$ is in $G$ too) then $K \triangleleft H$ is right?,"['group-theory', 'normal-subgroups']"
4011999,prove that $a_1 + \cdots +a_n \le n^2. $,"Let $n$ be a positive integer, and consider a sequence $a_1 , a_2 , \cdots , a_n $ of positive integers. Extend it periodically to an infinite sequence $a_1 , a_2 , \cdots $ by defining $a_{n+i} = a_i $ for all $i \ge 1$ . If $a_1 \le a_2 \le \cdots \le a_n \le a_1 +n $ and $a_{a_i } \le n+i-1 \quad\text{for}\quad i=1,2,\cdots, n, $ prove that $a_1 + \cdots +a_n \le n^2. $ Any help? I tried and noticed that for all $i \le a_1$ , we have $a_i \le n$ . Then for all $i \in \{2, 3, \cdots, a_1\},$ we have $a_{a_i} \le n+i-1$ .","['contest-math', 'inequality', 'combinatorics']"
4012010,"Using Mathematical Induction to Prove that $f^n (x) = 3^n e^{3x}$, where $f^n (x)$ is the $n$th derivative of $f$, and $n$ is a natural number.","Here is the question that I am not sure if I am doing right. Let $f(x) = e^{3x}$ and suppose $f^n (x)$ represents the $n$ th derivative of $f(x)$ with respect to $x$ . Use mathematical induction to prove $f^n (x) = 3^n e^{3x}$ , where $n$ is a natural number. And here is my attempt: First, we have to show that the base case is true. So for $n = 1$ , We know that $f'(x) = 3e^{3x}$ . We want to see if $f^1 (x)$ is the same. So \begin{align*}
f'(x) &= f^1 (x) \\
3e^{3x} &= 3^1 e^{3x} \\
3e^{3x} &= 3e^{3x}
\end{align*} Since LS $\equiv$ RS for $n = 1$ , the base case is true. For the inductive step, we want to assume that $f^n(x) = 3^n e^{3x}$ is true for $n = k$ , such that $f^k(x) = 3^k e^{3x}$ . Then we want to show that this is also true for $n = k + 1$ . \begin{align*}
f^{k+1}(x) &= 3^{k + 1}e^{3x} \\
&= (3)^k(3)^1e^{3x} \\
&= (3)^1(3^k e^{3x}) \\
&= 3f^k(x)
\end{align*} I don't know if this is correct or not, or if I am on the right track. Some help or some suggestions would be appreciated!","['calculus', 'proof-writing', 'derivatives']"
4012013,Unique number of combinations after $z$ steps of merging items by specific rules,"I encountered an issue where I am trying to estimate the number of unique combinations: There exists $N$ distinct molecules (or, for example, balls), where each molecule is labelled with some functional groups (or labels) from a set of functional groups $\{x\}$ (e.g. molecule 1 is labelled with $A,B$ , molecule 2 is labelled with $C$ , molecule 3 is labelled with $A,B,D$ ). At every step, you can combine two molecules by a reaction (or rule) in a set of reactions $\{y\}$ - i.e. molecules that have specific pairs of functional groups (e.g. $A+B$ , $A+C$ , $B+C$ , order does not matter) can merge to become a new molecule, in doing so their original functional groups remain except for those that were recombined (e.g. mol 1 + mol 2 -> new mol with only a label of $B$ , but not $A$ or $C$ ). For each distinct molecule, you can have unlimited supply. The question is how can one quickly estimate the total number of possible molecules after $z$ steps? From what I tried, I formulated it into a matrix $A$ of $N \times x$ , where each cell $(i, j)$ is $0$ or $1$ and represents whether a molecule $i \in \{N\}$ has a functional group $j \in \{x\}$ . I know how to calculate the number of combinations after 1 step - $\sum_{j_1, j_2 \in \{y\}} \left( \sum_{i_1} a_{i_1 j_1} \times \sum_{i_2} a_{i_2 j_2} \right)$ . But I am not so sure on how to do more than one step - N can be rather large, so ideally we would not make a new entry in the matrix to store the new molecules, but rather modify existing entries. Heuristic algorithms would be fine too, if they are fast and relatively accurate - $N$ will be around $10^4$ , there will be ~5 elements each in $\{x\}$ and $\{y\}$ . Thanks in advance!","['permutations', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'algorithms']"
4012016,Expected number of tosses to get a head from a coin using integration formulae?,"I recently started learning Expectation Probability , first of all Any Good resources to study it will be appreciated if any one can share What I have learnt so far the expected  value of some Unknown Variable say $x$ be $ E(x)$ which boils down this equation $ E(x) = \int_{-\infty}^\infty x*P(x) dx $ where P(x) is the Probability of some specific $x$ . So I wanted to find the Expected Number of tosses to Get a  Heads from an Unbiased Coin So I used this equation to solve it $E(Getting Heads) =>  \int_{0}^\infty\dfrac{x}{2^x}  dx$ , **IT IS A PRETTY STANDARD RESULT THAT E(GETTING  HEADS) = 2, But this integral is giving me another answer , which is $\dfrac{1}{\ln^2\left(2\right)}$ Can Anyone tell me where am I going wrong with understanding stuff.","['expected-value', 'probability']"
4012043,Derivation of $\frac{du}{dt}$ from The Chemical Basis of Morphogenesis by A. Turing,"I am currently reading The Chemical Basis of Morphogenesis by A. Turing. On page $56$ of his article, Turing uses the substitution \begin{align}
\xi&=b(u+v), \\
\eta&=(p-a')u+(p'-a')v,
\end{align} to transform \begin{align}
\frac{d\xi}{dt}&=a'\xi+b\eta+R_1(t),\\
\frac{d\eta}{dt}&=c\xi+d'\eta+R_2(t),
\end{align} into $$\frac{du}{dt}=pu+\frac{p'-a'}{(p'-p)b}R_1(t)-\frac{R_2(t)}{p'-p}+\xi\frac{d}{dt}\left(\frac{p'-a'}{(p'-p)b}\right)-\eta\frac{d}{dt}\left(\frac{1}{p'-p}\right).$$ Here, $p$ and $p'$ are the (real) roots of $(p-a')(p-d')=bc$ . I am seeking advice/hints on how I can derive this final expression.","['proof-writing', 'ordinary-differential-equations']"
4012089,"Brussel Sprout Random Variable, Mean, SD, and Probability","A local farmer's market sells 1 pound bags of Brussels Sprouts. The farmer believes that the mean number of Brussels Sprouts in each bag is 18 with a standard deviation of 3.4. You buy 2 pounds of Brussels Sprouts. a. You want to explore the range of sizes between brussels sprouts. You find the difference between the number of sprouts in your two bags. Define your random variable for this situation, and find its mean and standard deviation. I'm a bit confused with this question. Would the random variable be the # (number) of brussels sprouts in each bag? Wouldn't the mean be 18 and the standard deviation be 3.4? b. IF the number of Brussels Sprouts per bag is normally distributed, what is the probability that the difference in number of Brussels Sprouts between two bags is more than 4? I have no idea how to answer b. Would we make a normal probability distribution and use normalcdf?","['statistics', 'probability']"
4012094,Differentiate $\frac{e^{-2x}}{\sqrt x}$,"Differentiate, with respect to $x$ , $\frac{e^{-2x}}{\sqrt x}$ . I  a having difficulties with differentiation. The answer is $$-\frac{e^{-2x}(4x+1)}{2x\sqrt{x}}$$ But I am looking at my working out and can't seem to solve the question. This is what I did: $$\frac{d}{dx}\left(\frac{e^{-2x}}{\sqrt{x}}\right) = \frac{\sqrt{x}\times \frac{d}{dx}(e^{-2x})-e^{-2x}\times \frac{d}{dx}(\sqrt{x})}{(\sqrt{x})^2} = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}\frac12(\sqrt{x})^{-1/2}}{x}$$ $$ = \frac{\sqrt{x}(-2e^{2x})-e^{-2x}}{2x\sqrt{x}}$$ image of my work If someone could please help me solve this question or give advise please help me! Thank You!","['calculus', 'derivatives']"
4012112,"denumerable set: If A, B are denumerable sets, AXB is a denumberable set. Prove ZXN, ZXZ and QXQ are also denumerable sets.","So I prove $\mathbb{Z}\times \mathbb{N}\sim\mathbb{N}$ part: $f:\mathbb{Z}\times \mathbb{N} \to \mathbb{N}\times\mathbb{N}$ such that $$f:(z,n)\mapsto\begin{cases} (2z,n), & z>0 \\ (2(-z)+1,n), & z\leq 0\end{cases} $$ $f: \text{bijective}, 
\implies (z_1, n_1) \neq (z_2, n_2)
\implies f(z_1, n_1) \neq f(z_2, n_2)
\implies f: \text{injective}$ . $f(\mathbb{Z}\times \mathbb{N}) = \mathbb{N}\times \mathbb{N}
\implies \mathbb{Z}\times \mathbb{N}\sim \mathbb{N}\times \mathbb{N} \wedge \mathbb{N} \times \mathbb{N} \sim \mathbb{N}$ i am not sure how to prove that $\mathbb{Z}\times \mathbb{Z}$ and $\mathbb{Q}\times \mathbb{Q}$ are denumerable sets. Thank you so much for your help.",['elementary-set-theory']
4012116,Ancient Egyptian Method for getting an Area of Quadrilateral,"I am currently learning the history of calculus, and I am really curious about the Ancient Egyptian way to get an Area of Quadrilateral. So, I hope I could get some enlightenment. Here is the Egyptian Way: Consider Quadrilateral with 4 different lengths a,b,c,d. (a,c are facing each other, and b,d are facing each other.) (Each set of sides may not be parallel.) They calculated the area as follows, $$A=\frac{a+c}{2}\times \frac{b+d}{2}$$ However, we know this is wrong, but I want to show if this method is overestimated or underestimated, and why. P.S. I think they tried this way because getting an Area of rectangular is the easiest way, and they decided to make rectangular with lengths of averages of two opposite sides. And this new area somehow looks similar (close) to the original area.",['geometry']
4012120,"$u(x,t)\cdot(\nabla\rho(x,t))$ versus $(u(x,t)\cdot\nabla)\rho(x,t)).$","I am new in the Navier Stokes Equation In the Eulerian description of the fluid, if I have $g(t)=\rho(x(t),t)$ , then: $$g'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+\frac{\partial \rho}{\partial x}(x(t),t)x'(t)=\frac{\partial \rho}{\partial t}(x(t),t)+ u(x,t)\cdot(\nabla\rho(x,t)).$$ $u(x,t)=x'(t)$ is the velocity field of the flow of the fluid, $\rho:\Omega×[0,T] \rightarrow \mathbb{R}$ is the mass density, and $\nabla$ is the nabla operator. MY QUESTION IS how did $u(x,t)\cdot (\nabla\rho(x,t))$ become $(u(x,t)\cdot\nabla)\rho(x,t))?$ I searched on internet and they said that: Generally the convective derivative of the field u·∇y, the one that contains the covariant derivative of the field, can be interpreted both as involving the streamline tensor derivative of the field u·(∇y), or as involving the streamline directional derivative of the field (u·∇) y, leading to the same result. BUT I don't know why and how did this lead to the same result!!","['multivariable-calculus', 'partial-differential-equations', 'derivatives', 'chain-rule', 'fluid-dynamics']"
4012144,Uniform randomization of visits yields Poisson process - well-known?,"From various experiments, I seem to have rediscovered the following idea Proposition ( Uniform randomization of visits yields Poisson process ). Suppose that a process iteratively visits elements in a set where each visit selects an element at equal probability (uniform distribution across the set). Let $z$ be the time it would take to scan all of the elements once (scan the set). Then from the perspective of any given element, the visits arrive as a Poisson process with rate $\zeta = 1/z$ . I say rediscovered because I suspect that the above proposition is a consequence of well-known principles of Poisson processes.  However, I haven't found anything explicit.  Where might I find something? My sketch proof of the proposition : Choose a target element from the set. Let $T$ be the time to between visits to the target. It is sufficient to show that $T$ has an exponential distribution with rate parameter $\zeta$ as this occurs if and only if the target is receiving visits as a Poisson process with rate $\zeta$ . Partition the set into $n$ subsets that require equal durations to scan. By construction, the target can reside in only one subset. Moreover each subset takes duration $\tfrac{d}{n}$ to scan at so $m$ visits will take duration $m\tfrac{d}{n}$ . Now for any $t>0$ $$
  \mathbb{P}(T \leq t) = 1 - \mathbb{P}(T > t)
$$ Put $m = \tfrac{tn}{d}$ so that $m$ visits take duration $t$ . Consequently $T > t$ occurs if and only if there are $m$ visits that each miss the target hence $$\begin{align*}
  \mathbb{P}(T \leq t) &= 1 - \mathbb{P}(\text{$m$ visits that each miss the target})
\\
  &= 1 - \left( \frac{n-1}{n} \right)^m
\end{align*}$$ as each visit selects a subset at equal probability and there are $n-1$ subsets that do not contain the target. Therefore $$\begin{align*}
  \mathbb{P}(T \leq t) &= 1 - \left( \left(1- \frac{1}{n} \right)^n \right)^{\frac{m}{n}}
\\
&= 1 - \left( \left(1- \frac{1}{n} \right)^n \right)^{\zeta t}
\\
&\rightarrow 1 - \exp(-\zeta t) & \text{as $n \to \infty$}
\end{align*}$$ which is the cumulative distribution function for the exponential distribution with rate parameter $\zeta$ , as required.","['uniform-distribution', 'probability-theory', 'poisson-process', 'reference-request']"
4012158,Number of points chosen form a polygon to have no isosceles and equilateral triangles.,"Let $\Omega$ be a regular polygon with $n$ sides. Let's choose $\Gamma$ a set of vertices, for which any triangle with the vertices in $\Gamma$ is neither isosceles, nor equilateral. Find $\max |\Gamma|$ . (Bulgarian Team Selection Test) Attempt : I have tried to prove for small cases: $3$ , $4$ , $5$ , $6$ , $7$ etc., but no rule confirms, to be proven by induction . And, technically, induction is not possible in this case, because, polygon $\Omega \setminus A$ , where $A$ is a vertex, won't be regular .","['geometry', 'polygons', 'combinatorial-geometry', 'combinatorics', 'induction']"
4012183,Quadratic equation in trigonometric form $\sqrt 3 a\cos x + 2b\sin x = c$,"Let a,b,c be three non-zero real number such that the equation, $\sqrt 3 a\cos x + 2b\sin x = c$ , $x \in \left[ { - \frac{\pi }{2},\frac{\pi }{2}} \right]$ , has two distict roots $\alpha$ and $\beta $ where $\alpha +\beta=\frac{\pi}{6}$ . Then find the value of $\frac{b}{a}$ . My approach is as follow $\sqrt 3 a\cos x + 2b\sin x = c$ $3{a^2}{\cos ^2}x = {c^2} + 4{b^2}{\sin ^2}x - 4bc\sin x$ $ {\sin ^2}x\left( {4{b^2} + 3{a^2}} \right) - 4bc\sin x + {c^2} - 3{a^2} = 0$ Roots are real when $16{b^2}{c^2} + 4\left( {4{b^2} + 3{a^2}} \right)\left( {{c^2} - 3{a^2}} \right) > 0$ $\sin \alpha  + \sin \beta  = \frac{{4bc}}{{4{b^2} + 3{a^2}}};\sin \alpha \sin \beta  = \frac{{{c^2} - 3{a^2}}}{{4{b^2} + 3{a^2}}}$ , how I will proceed from here","['trigonometry', 'quadratics']"
4012219,Why is the direct product of two groups $G_1 \times G_2$ too small to be their coproduct?,"We’re looking at the following definition in my class. Definition. The coproduct $X_{1} \amalg X_{2}$ of $X_{1}$ and $X_{2}$ , together with the morphisms $i_{j}: X_{j} \rightarrow X_{1}\amalg X_{2}$ , is characterized by the following universal property: Given any object $Y$ with morphisms $f_{j}: X_{j} \rightarrow Y,$ there exists a unique $f: X_{1} \amalg X_{2} \rightarrow Y$ such that $f_{j}=f \circ i_{j}$ . In $G_1 \times G_2$ , there exist two subgroups isomorphic to $G_1$ and $G_2$ , namely the sets of elements $\{(g_1,e_2): g_1 \in G_1\}$ and $\{(e_1,g_2): g_2 \in G_2\}$ , respectively. These two subgroups have the property that their members commute with each other: $(g_1,e_2) \cdot (e_1,g_2) = (g_1,g_2) = (e_1,g_2) \cdot (g_1,e_2)$ . The reason given for the direct product not being the coproduct is that, in a group $Y$ with maps $f_1: G_1 \to Y$ and $f_2: G_2 \to Y$ , elements of $Y$ don’t commute with each other. I’m not yet able to derive this. My understanding of the argument is that, given the property of the two subgroups above, I ought to be able to derive $f_1(g_1) \cdot_Y f_2(g_2) = f_2(g_2) \cdot_Y f_1(g_1)$ , which would not be true for a general group $Y$ (unless $Y$ is abelian). Now if I also assume $G_1 \times G_2$ to be the coproduct, I’d have: $$f_1(g_1) \cdot_Y f_2(g_2) = (f \circ i_1)(g_1) \cdot_Y (f \circ i_2)(g_2) = f(g_1,e_2) \cdot_Y f(e_1,g_2)$$ It’s here that I don’t know how to keep going to get the relation above.","['group-theory', 'category-theory']"
4012359,Searching for all functions with a certain property,"Let $f:A\rightarrow\mathbb{R}$ be a function with the following property: For all $x_1,x_2\in A$ if we take the line segment connecting the points $(x_1,f(x_1))$ and $(x_2,f(x_2))$ , take its perpendicular bisector, and see where it intersects the graph of the function, in that point the function has a slope equal to the slope of the line segment. The linear function and the circle are obvious solutions to this problem, I'm searching for other solutions, if there are any. Edit: I've found out this has some connection to the mean value theorem, I don't know if that's helpful or not. Also I think the function can't have an inflection point, although I can't prove it.","['calculus', 'functions', 'derivatives']"
4012395,"Using the property ""onto"" in a proof","Definition (Minimal Open Set) : A nonempty open set $U$ of $X$ is said to be a minimal open set if and only if any open set which is contained in $U$ is $∅$ or $U$ . Definition (Minimal continuous map) : Let $X$ and $Y$ be the topological spaces. A map $f: X → Y$ is called minimal continuous if $f^{-1}(M)$ is an open set in $X$ for every minimal open set $M$ in $Y$ . Definition (T $_{min}$ Space) : A topological space $(X,\tau)$ is said to be T $_{min}$ space if every nonempty proper open subset of $X$ is minimal open set. Theorem: Let : $f:X\rightarrow Y$ be a minimal continuous, onto map and Y be a T $_{min}$ space.
Then $f$ is continuous. Proof: Let be $f$ a minimal continuous, onto map. Let $N$ be any nonempty proper open set in $Y$ . By
hypothesis, $Y$ is T $_{min}$ space. It follows that $N$ is a minimal open set in $Y$ . Since $f$ is minimal continuous, $f^{-1}(N)$ is an open in X. Therefore is a continuous. Now, where did we use that $f$ is onto? I think if $f$ were not be onto, the proof would still work.","['general-topology', 'functions']"
4012439,Method of Undetermined Coefficients when ODE does not have constant coefficients,"Even though the Method of Undetermined Coefficients is usually taught as a method for solving nonhomogeneous linear ODEs with constant coefficients, it seems to also work if the coefficients are not constant. For example, let $$
t^2y''-2y=t.
$$ The solution to the associated homogeneous equation, $t^2y''-2y=0$ , is spanned by the functions $t^2$ and $t^{-1}$ . Using the Method of Undetermined Coefficients, one can guess a particular solution of the form $y_p(t)=At+B$ , where $A$ and $B$ are constants. Substituting $y_p(t)$ into the differential equation and solving gives $A=-\frac{1}{2}$ and $B=0$ , so that the general solution to $t^2y''-2y=t$ is $$y(t)=C_1t^2+C_2t^{-1}-\frac{1}{2}t.$$ However, if we consider the equation $$t^2y''-2y=t^2$$ instead, then our initial guess for a particular solution, $y_p(t)=At^2+Bt+C$ ( $A$ , $B$ , $C$ constants) ""overlaps"" with part of our solution to $t^2y''-2y=0$ . This suggests that we should look for a particular solution of the form $y_p(t)=At^5+Bt^4+Ct^3$ instead. Substituting this into the ODE to try to solve for $A$ , $B$ , and $C$ doesn't give anything useful; in fact, through Variation of Parameters, we get a particular solution $y_p(t)=\frac{1}{3}\ln(t)t^2$ , which the Method of Undetermined Coefficients has no hope of finding. I'm not really sure what's going on here. Why does the Method of Undetermined Coefficients fail to give me the correct particular solution in the second case? Is there anything general that can be said here about applying this method in the nonconstant coefficient case?","['calculus', 'ordinary-differential-equations']"
4012488,Minimizing costs to make a cylinder,"A cylinder must have a volume $V=4000$ and its made from just one rectangle sheet, so its top and bottom must be cut from this sheet. I have to find the dimensions of this sheet such that the parts I lost (when I cutoff the top and the bottom) are the lowest. I thought about minimize the difference between the area of the original sheet and the parts I'll use to make my cylinder: (I imagine this is the best way to don't waste sheet - the original problem doesn't have any image). So the area of the original sheet is $$(h+2R)\cdot4R=4hR+8R^2$$ and the area I'll use to make the cylinder is $$4hR+2\cdot\pi R^2$$ so the difference between them is $$8R^2-2\pi R^2=0$$ and derivating it: $$16R-4\pi R$$ which the only root is $R=0$ . What is wrong?","['calculus', 'derivatives']"
4012503,Arrangement without any monotone section,"Here's the question:
""In how many ways can the sequence 45678 be arranged such that there aren't any three consecutive terms increasing or decreasing"" The first thing I thought of is to do complementary counting and then make cases for each number of consecutive terms that there could be (3, 4, 5). However there's a lot of overlap between the three cases and I'm not sure how to deal with that in an efficient way. If someone could solve it and then tell me how they did it that would be great.","['permutations', 'combinatorics', 'contest-math']"
4012504,What is the smallest number of questions needed?,"$x, y, z$ are $3$ random integers from $1$ to $10$ inclusive. Now, you have to guess these numbers, and you are allowed to ask questions. Each question is of the form, 'Is $a\le x\le b, c\le y\le d,$ and $e\le z\le f$ ?', where $a, b, c, d, e, f$ are arbitrary integers from 1 to 10 inclusive. The answer to each question is either YES if ALL three conditions are fulfilled, or NO otherwise. Assume the answers are always truthful. Find the smallest number of questions needed to guarantee the knowledge of $(x, y, z)$ . So far, I have a solution that uses only $12$ questions: Set A Is $1\le x \le 9, 1\le y \le 9, 1\le z \le 9$ ?
If yes, go to set B, otherwise, continue Is $10\le x \le 10, 1\le y \le 10, 1\le z \le 10$ ? (This is equivalent to asking if $x=10$ .) Is $y=10$ ? (Use the same form as above.) Is $z=10$ ? Move on to Set B. Set B If $x\le 9$ , is $1\le x \le 3?$ If yes, move on to set C. $4\le x \le 6?$ (If no, $7\le x \le 9$ .) Move on to set C. Replace $1, 2, 3$ with $4, 5, 6$ or $7, 8, 9$ respectively. Set C If $1\le x \le 3$ , is $x=1?$ $x=2$ ? (If no, $x=3$ .) Repeat sets B and C for all variables that are below $9$ . It is easy to verify that the worst case scenario, at $(9,10,9)$ , takes only $12$ questions. However, I cannot continue on from this point. Please advise.","['optimization', 'combinatorics']"
4012534,$\frac{2}{3} \cdot \frac{5}{6} \cdot \frac{8}{9} \cdot ... \cdot \frac{995}{996} \cdot \frac{998}{999}$ and $0.1$ which is bigger?,$\frac{2}{3} \cdot \frac{5}{6} \cdot \frac{8}{9} \cdot ... \cdot  \frac{995}{996}  \cdot  \frac{998}{999}$ and $0.1$ which is bigger? The expression seems a bit different from Mathematical induction problem: $\frac12\cdot \frac34\cdots\frac{2n-1}{2n}<\frac1{\sqrt{2n}}$,"['inequality', 'convergence-divergence', 'induction', 'sequences-and-series']"
4012577,interpolating function with inclined asymptote,"I would like to find an interpolating function that is a curve with an inclined asymptote. I already tried with the parabolic and with the homographic (y= (a x+b)/(c x+d)). Unfortunately they are both phisically inconsistent with the phenomenon that I am analysing. I need a function like the c) in the image attached, which is a curve that has an inclined asymptote. Can you help me ?","['interpolation', 'functions']"
4012598,Cohomology of a smoothly embedded space curve,"Let $\mathbb{P}^3 = P(\mathbb{C}^4)$ and $\gamma:C\rightarrow \mathbb{P}^3$ be a smoothly embedded algebraic space curve. Then its total Chern class is $c(C) = c(TC) = 1+a\in H^*(C)$ , where $a^2=0$ . Since $C$ is compact and orientable we have Poincare duality and hence a well defined 'shriek - map' $$\gamma^!: H^*(C)\overset{PD_C}{\longrightarrow} H_{2-*}(M) \overset{\gamma_*}{\longrightarrow} H_{2-*}(\mathbb{P}^3)\overset{PD_{\mathbb{P}^3}^{-1}}{\longrightarrow}H^{*+4}(\mathbb{P}^3).$$ Let $g$ be the genus of $C$ and let $\chi(C):=2-2g$ and $d$ be the degree of $C$ . Then $$\gamma^!(1) = d\cdot c_1(\mathcal{O}_{\mathbb{P}^3}(1))^2\quad\text{and }\quad \gamma^!(a) = \chi \cdot c_1(\mathcal{O}_{\mathbb{P}^3}(1))^3.$$ Does anyone know why these two relations hold and point me to some literature? My knowledge in algebraic geometry is very limited since I am more familiar with manifolds, homology/cohomolgy of topological spaces so Chern classes for me are always defined in that context. I know that there are Chern classes in algebraic geometry (i.e. for Chow rings) and that these notions are related, but my understanding is very superficial. Still it would help me a lot if I could understand the relations above. Thanks!","['algebraic-topology', 'algebraic-geometry', 'homology-cohomology', 'characteristic-classes', 'projective-space']"
4012634,Is there always a linear isometry from a normed vector space into its dual?,Let $\mathbb K=\mathbb R$ or $\mathbb K=\mathbb C$ and $E$ be a normed $\mathbb K$ -vector space. Can we show that there is a linear isometry from $E$ into $E'$ or is there a counterexample? I think this should be true: My idea is that we should be able to consider the linear isometry $\iota_1$ from $E$ into its completion $\tilde E$ and the linear isometry $\iota_2$ from $\tilde E$ into its dual $\tilde E'$ ...,"['functional-analysis', 'dual-spaces']"
4012651,$\lim\limits_{n\to \infty}\frac{3^n+6^n-1}{6^n-1}$,$\lim\limits_{n\to \infty}\dfrac{3^n+6^n-1}{6^n-1} = 1$ I proved the statement above but now the exercise says that I have to find $n_0$ so that $\left\vert x_n-1\right\vert < \frac{1}{50}$ with $(x_n)$ as the given seqeunce above. I wrote the sequence like this: $(x_n)=\frac{3^n}{6^n-1}+1$ and then I looked at $\left\vert \frac{3^n}{6^n-1}+1-1\right\vert < \frac{1}{50}$ which is equivalent to $\frac{\frac{1}{2^n}}{1-\frac{1}{6^n}} < \frac{1}{50}$ . But now I don't know how to go on... Can someone help me?,"['analysis', 'sequences-and-series']"
4012674,Find the locus of this point P,"Find the locus of all points inside $\triangle ABC$ such that $PA^{2}+PB^{2}=PC^{2}$ . At first, i tried finding a right angled triangle and then tried to go on applying Pythagorean Theorem and finding other trivia but that didn't seem to work. I think the locus is most likely to be some line segment rather than an arc of some circle as that seems to be quite unrelated seen from an Euclidean perspective. Applying Apollonius' Theorem and Stewart's Theorem on some triangles might be the key to this problem although i could not find such triangles. I don't know whether this can be solved by trigonometry or not but since this is a problem from a chapter on euclidean geometry, i am sure that there is some clever way to look at this problem.","['euclidean-geometry', 'triangles', 'geometry', 'locus']"
4012688,Combinatorics - Painting the Unit Squares of a 3x3 Square,"I need some help with the exercise I was assigned in math class. We were given a 3x3 square made of unit squares, and we wanted to paint all the squares. We had 3 sets of paint, red, blue, and green. Each paint was distinct from each other, but two unit squares with the same color were the same. Additionally, there's only enough paint of each color to color 3 unit squares, and all the paint has to be used. The goal of the exercise was to find the total amount of distinct possible ways such that no two unit squares that share an edge (that is to say they are next to each other vertically or horizontally, having same color across a diagonal is fine). I've started by trying to take a look at each possible arrangement of the bottom 3 unit squares, but I can't seem to find a pattern between it. Is there a better way than continuing by brute force? Any ideas on how I should proceed from here?","['algebra-precalculus', 'combinatorics']"
4012729,"If $f(z)$ is differentiable, show that $|f'(z)|^2=(\frac{\partial u}{\partial x})^2+(\frac{\partial v}{\partial x})^2$. [duplicate]","This question already has answers here : Why $f^{\prime}(z)=u_{x}+iv_{x}$? (2 answers) Closed 3 years ago . Let $f(z)$ be a complex valued function which can be represented by $$f(z)=u(x,y) + iv(x,y).$$ If $f(z)$ is differentiable, show that $$\left|f'(z)\right|^2=\left(\frac{\partial u}{\partial x}\right)^2+\left(\frac{\partial v}{\partial x}\right)^2.$$ I am a beginner in complex analysis, so bear with me. In my solution, it is given that $$f'(z)=u_x + iv_x=\frac{\partial u}{\partial x} + i \frac{\partial v}{\partial x}$$ and things follow from here.
My doubt is, how does the above came into picture. As far as i know, $f(z)$ is a complex valued function with real part as $u(x,y)$ and imaginary part being $v(x,y)$ which are both functions of $x$ and $y$ .
How does one calculate the derivative of $f(z)$ by taking partial derivative with respect to $x$ only? Any help would be appreciated from the community.",['complex-analysis']
4012880,Prove that $\{Y:Y \subseteq X\}$ is a set.,"I am a beginner and self-learning Real Analysis. Problem 3.4.6 from Tao's Analysis-I poses the following question. I'd like to someone to verify my proof, and help me write a technically correct proof. Let $X$ be a set. Then the set $\{Y : Y \text{ is a subset of } X\}$ is a set. (Hint given with the question: start with the set $\{0, 1\}^X$ and apply the replacement axiom, replacing each function $f$ with the object $f^{-1}(\{1\})$ .) My Attempt. Using the Power set Axiom, if $X$ and $Y$ are sets, then $Y^X$ consists of all functions for $X \to Y$ I assume for simplicity that set $X$ has two elements $\{x_1,x_2\}$ . By definition of the power set axiom, the set $Y^X$ consists of 4 functions as follows : $f_1 \text{ maps } x_1\to 0 \text{ and } x_2 \to 0$ $f_2 \text{ maps } x_1\to 0 \text{ and } x_2 \to 1$ $f_3 \text{ maps } x_1\to 1 \text{ and } x_2 \to 0$ $f_4 \text{ maps } x_1\to 1 \text{ and } x_2 \to 1$ I want to apply the replacement axiom, replacing each function $f_i$ with the object $f_i^{-1}(\{1\})$ . $\{ f_1^{-1}(\{1\}),f_2^{-1}(\{1\}),f_3^{-1}(\{1\}),f_4^{-1}(\{1\})\}$ $\implies$ $\{ \emptyset,\{x_2\},\{x_1\},\{x_1,x_2\} \} $ which consists of all subsets of $X$ . *** My Question*** I have simply replaced the functions $(f_1,f_2,f_3,f_4)$ with the object $f^{-1}(\{1\})$ . In order to use the replacement axiom, I must show that for each $x$ , the statement $P(x,y)$ is true for atmost one $y$ . What should be the proposition $P(x,y)$ ? $\{x_1,x_2\}$ consists of all the elements of $X$ , but from the definition of a function we know that bijective functions are invertible, but in the last function both $\{x_1,x_2\}$ map to $1$ , in that case, I don't think I can write $f_4^{-1}(\{1\})$ = $\{x_1,x_2\}$ .","['inverse-function', 'real-analysis', 'functions', 'solution-verification', 'elementary-set-theory']"
4012903,Confusion on pairwise disjoint and disjoint,"I've just started trying to learn some set theory and topology and I've come across the definition of disjoint sets quite a lot I've seen lots of Definitions such as A set (of sets) $\mathcal{A}$ is disjoint if $\bigcap \mathcal{A} = \emptyset$ . The set $\mathcal{A}$ is pairwise disjoint when $\forall x \in A: \forall y \in A: x \neq y \implies x \cap y = \varnothing$ I can't quite understand what the difference actually is as I saw on s.e that a pairwise disjoint set is related to a $k$ -wise disjoint. I also saw this definition An indexed collection $\{A_i\}_{i\in I}$ of subsets of $X$ is said to be pairwise disjoint if $A_i\cap A_j=\varnothing$ whenever $i\neq j$ . This confused me even further as an indexed collection can be a surjective function so when $i=j$ it could well be that $A_i$ = $A_j$ in that case how is their intersection empty?. What I'm really trying to understand is what is pairwise and k wise disjoint sets?
Thanks in advance","['elementary-set-theory', 'self-learning', 'general-topology']"
4012920,$\frac 21 \times \frac 43 \times \frac 65 \times \frac 87 \times \cdots \times \frac{9998}{9997} \times \frac {10000}{9999} > 115$,"Prove that $$x = \frac 21 \times \frac 43 \times \frac 65 \times \frac 87 \times \cdots \times \frac{9998}{9997} \times \frac {10000}{9999} > 115$$ saw some similar problems like show $\frac{1}{15}< \frac{1}{2}\times\frac{3}{4}\times\cdots\times\frac{99}{100}<\frac{1}{10}$ is true but didn't manage to get 115. I could get a weaker conclusion of $x>100$ though. \begin{align}
x^2 &= \left(\frac 21 \times \frac 21\right) \times \left(\frac 43 \times \frac 43\right) \times \cdots \times \left(\frac{10000}{9999} \times \frac {10000}{9999}\right) \\
&\ge \left(\frac 21 \times \frac 32\right) \times \left(\frac 43 \times \frac 54\right) \times \cdots \times \left(\frac{10000}{9999} \times \frac {10001}{10000}\right) \\
&= 10001
\end{align} so $x > 100$","['inequality', 'convergence-divergence', 'sequences-and-series']"
4012992,Questions about mass conservation in PDE,"Let us consider the following PDE: $$\partial_t{f}(t,x,u)+u\cdot\nabla_xf(t,x,u)+g(x)\cdot\nabla_uf(t,x,u)=0,$$ where $(t,x,u)\in[0,T]\times\mathbb{R}^d\times\mathbb{R}^d$ and $g:\mathbb{R}^d\to \mathbb{R}^d$ is a continuous function. If $f$ is a solution of this PDE in classical sense, the usual proof of the fact that $\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)dxdu$ reads as follows: $$\frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=\displaystyle\int_{\mathbb{R}^{2d}}\partial _tf(t,x,u)dxdu=-\displaystyle\int_{\mathbb{R}^{2d}}u\cdot\nabla_xf(t,x,u)dxdu-\displaystyle\int_{\mathbb{R}^{2d}}g(x)\cdot\nabla_uf(t,x,u)dxdu.$$ By using integration by parts and asuming an appropiate decay of $f$ , we deduce that $\frac{d}{dt}\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)dxdu=0.$ However, there are some details of this proof that I don't understand: Why can we iterate the time derivative in the first step? Is this property still true if we consider weak solutions?, that is if $f$ verifies: $$\displaystyle\int_{0}^T\displaystyle\int_{\mathbb{R}^{2d}}f(t,x,u)\left[\partial _t\psi(t,x,u)+u\cdot\nabla_x\psi(t,x,u)+g(x)\cdot\nabla_u\psi(t,x,u)\right]dxdu+\displaystyle\int_{\mathbb{R}^{2d}}f(0,x,u)\psi(0,x,u)dxdu=0,\\\forall\psi \in C^1([0,T]\times \mathbb{R}^{2d}) \text{ with } \operatorname{supp}\psi\subset [0,T[\times C \text{ and } C \text{ compact in }\mathbb{R}^{2d}$$ Any help about this questions will be welcome.","['lebesgue-integral', 'real-analysis', 'functional-analysis', 'partial-differential-equations', 'derivatives']"
4013027,"Measure, Integration & Real Analysis Sheldon Axler SEction 2B Exercise 12","the question can be found here, but is also repeated below. Suppose $f:\mathbb{R}\rightarrow\mathbb{R}$ is a function. (a) For $k\in\mathbb{Z}^{+}$ , let $$G_k = \{a\in\mathbb{R}: \exists\delta>0 \text{ s.t. } \lvert f(b) - f(c)\rvert < \frac{1}{k}, \forall b,c\in(a-\delta, a+\delta\}.$$ Prove that $G_k$ is an open subset of $\mathbb{R}$ for each $k\in\mathbb{Z}^{+}$ . (b) Prove the set of points where $f$ is continuous equals $\cap_{k=1}^{\infty}G_{k}$ . (c) Conclude that the set of points at which $f$ is continuous is a Borel set. So, I'm really just looking for a help at where to start. I think that  I have an idea of where to set for (b) since the definition of $G_k$ is essentially just the definition of continuity using $\varepsilon_k = \frac{1}{k}$ , but I'm fairly lost on what to do for the other sections. Thank you!","['measure-theory', 'real-analysis']"
4013038,Does a $\mathrm{C}^*$-algebra generated by projections contain support projections,"I think we have for a (normal?) state $\varphi$ on a von Neumann algebra $\mathcal{M}$ a projection $p_\varphi$ with some nice properties called its support. It arises as follows: Define the following null space: $$N_\varphi=\left\{g\in \mathcal{M}\,|\,\varphi(|g|^2)=0\right\}.$$ This set is a $\sigma$ -weakly closed left ideal. Therefore there exists a projection $q_\varphi$ such that $N_\varphi=\mathcal{M}q_\varphi$ . Some properties include the fact that $g\in N_\varphi$ if and only if $gq_\varphi=g$ . Also, for all $f\in \mathcal{M}$ we have $$\varphi(q_\varphi)=\varphi(fq_\varphi)=\nu(q_\varphi f)=0.$$ Also if we define the projection $p_\varphi:=1_{\mathcal{M}}-q_\varphi$ . We have $$\varphi(f)=\varphi(fp_\varphi)=\varphi(p_\varphi f)=\varphi(p_\varphi fp_\varphi),$$ and $\varphi(p_\varphi)=1$ . I understand that a $\mathrm{C}^*$ -algebra generated by projections is not necessarily a von Neumann algebra... but Question: Does a $\mathrm{C}^*$ -algebra generated by projections have support projections?","['von-neumann-algebras', 'c-star-algebras', 'operator-algebras', 'functional-analysis', 'dual-spaces']"
4013062,Calculating the Rolling Variance of a set of numbers,"I would like to subscribe to a WebSocket stream which will supply me with many numbers per second. From this data, I would like to calculate the variance of say the last 1000 numbers. How can I do this in a rolling fashion? That is, I would like some computation comparable to this one for the mean of the last 1000 numbers: $$\rm{mean}_{i+1} = \rm{mean}_{i} + \frac{1}{1000}\left(x_{i+1}-x_{i-999}\right)$$ Thanks in advance for any help.
Ben",['statistics']
4013133,Extreme random variable sequence independence,"During solution of probability problem I came across following:
Let $X_n$ - be a sequence of i.i.d (independent, identically distributed) random variables, uniformly distributed over $[0;1]$ . Then is it true that random variables $m_n$ , determined as $m_n=min\{X_1, ..., X_n\}$ are also independent or not? If not, do you have any ideas of proving that such 𝑚𝑛 approaches 0 almost surely then? I was trying to proof this via fact mentioned above, and do not have anymore ideas.. Do you have any ideas about this one? This is very crucial for my solution, would appreciate any help!","['independence', 'probability-distributions', 'probability', 'random-variables']"
4013134,Using theorem of logical equivalences to show $p \land (\sim q \lor p) \equiv p$,"I'm new to the whole discrete math thing, and I'm having trouble finding any laws to start breaking the statement $p \land (\sim q \lor p) \equiv p$ down into its equivalences laws. Can I have some help please. here's the chart I'm using heres an example of the format I need to answer in. statements       | reason ~[~p^(pvq)]vq=t  | given p^(pvq)vq=t      | distributive law (p^p)v(p^q)vq=t  | distributive law pv(p^q)vq        | Idempotent law pvq=t            | Absorption law","['first-order-logic', 'discrete-mathematics']"
4013148,What is the projective bundle over $(\Bbb P^1\times\Bbb P^1)/(\Bbb Z/2)$ which isn't $\Bbb P(\mathcal{E})$ for some vector bundle $\mathcal{E}$?,"I'm thinking about when a (Zariski-locally trivial) $\Bbb P^n$ -bundle comes from the projectivization of a vector bundle, and I've found Sasha's answer here on MO stating that $(\Bbb P^1\times\Bbb P^1)/(\Bbb Z/2)$ has a $\Bbb P^1$ -bundle which isn't the projectivization of a vector bundle. Unfortunately, it doesn't say what this bundle is . Does anyone know an explicit description of what the intended bundle is, and an easy way to see it's not the projectivization of a vector bundle?","['algebraic-geometry', 'projective-schemes']"
4013164,Is broken Sobolev space a Sobolev space?,"The definition of a broken Sobolev space is as follows.
Given infinite-dimensional (but mesh-dependent)
spaces on an open bounded domain $\Omega \in R^3$ with Lipschitz boundary. The mesh, denoted by $\Omega_h$ , is a disjoint partitioning of $\Omega$ into open elements $K$ such that the union of their closures
is the closure of $\Omega$ . The collection of element boundaries $\partial K$ for all $K \in \Omega_h$ , is denoted by $\partial \Omega_h$ . We assume that each element boundary $\partial K$ is Lipschitz. The shape of the elements is
otherwise arbitrary for now. The broken Sobolev space is defined as $$\hat{H}^{1}\left(\Omega_{h}\right)=\left\{u \in L^{2}(\Omega):\left.u\right|_{K} \in H^{1}(K), K \in \Omega_{h}\right\},$$ where $H^1$ is the standard Sobolev space. According to my understanding, if $\Omega_h$ can be partitioned into a finite number of subdomains, $K$ 's, and because the union of the intersections of $K$ 's is a set of measure zero, then $\forall u \in \hat{H}^1, \int_{\Omega_h} \sum_{|\alpha|\leq 1} (D^\alpha u)^2 dx = \sum_{K=1}^n \int_K  \sum_{|\alpha|\leq 1} (D^\alpha u)^2 dx$ . This seems mean that $\hat{H}^{1}$ is also a Sobolev space. If the partion of $\Omega_h$ consists of an infinite number of $K$ 's, then the equality of the integration above would not hold. In this case, $\hat{H}^{1}$ is not a Sobolev space. Am I right about the above two statements? Thanks a lot.","['finite-element-method', 'functional-analysis']"
4013222,Metric space with a constraint,"For $\mathbb{R}^n$ with $n \ge 2,$ define $$d_{p,q}:= \left( \sum\limits_{i =1}^n |x_i - y_i|^p \right)^q$$ for $p,q \in \mathbb{R}.$ Prove that $ d_{p,q}  \text{ is a metric on } \mathbb{R}^n \iff 0 < q, \ pq \le 1.$ I am used to proving or disproving a metric space by validating all the four axioms of finiteness, definiteness, symmetry and triangle inequality without any constraints on the metric. I came across this exercise and I thought of sharing it here to generate ideas on how best to tackle it. Clearly, if $0 < q, pq \le 1$ then $d_{p,q}(x,y)$ is a metric space since all the four axioms are easily met. How do we prove the other direction that is if $d_{p,q} (x,y)$ is a metric we show that $0 <q, pq\le q$ ? We need to fulfill the two directions of the $""\iff""$ . Benevolent contributors I rely on you for your support.","['metric-spaces', 'vector-spaces', 'analysis', 'real-analysis', 'functional-analysis']"
4013227,Eigenvectors of a normal operator are an orthogonal basis,"I am trying to prove the following. Let $T\colon\mathbb C^n\to\mathbb C^n$ be a normal operator. Then there is a basis for $\mathbb C^n$ consisting of orthogonal eigenvectors of $T$ . I am allowed to use the following fact which I've already established: $$T\boldsymbol x=\mu\boldsymbol x\iff T^*\boldsymbol x=\overline\mu\boldsymbol x.$$ I've managed to produce a proof using Schur's theorem (i.e., $T$ is unitarily similar to a diagonal matrix), but this feels like too much ""heavy machinery"" to tackle this proof. Is there another more direct way to go about it?","['matrices', 'linear-algebra', 'functional-analysis']"
4013244,Every Vertical Strip Has A Point With Positive Slope,"Consider a differentiable function $f$ on $[x_0, x_1]$ . Suppose $f(x_0) < f(x_1)$ . It seems intuitively obvious that every vertical strip between $f(x_0)$ and $f(x_1)$ contains a point with positive slope. i.e. a point $x$ with $f'(x) > 0$ and $a < f(x) < b$ Let the lower bound of this vertical strip be $a$ . Let the upper bound of this vertical strip be $b$ . This seems like it should be a very common question, I believe it can be proved formally using the intermediate + mean value theorem. Does anyone have any idea how to prove this?","['continuity', 'calculus', 'derivatives', 'real-analysis']"
4013247,Why is the Nth derivate isolated in many textbook definitions?,"My textbook keeps writing definitions in the above form, with the LHS of the equation being the highest order derivative, and the RHS being everything else. Why is this form stressed over a form like this ? Is there any mathematical reason, if it is just a convention then why? Notice that the second equation has a function of t on the RHS and everything else on the LHS.",['ordinary-differential-equations']
4013264,Find function f(z) for a matrix,"Let $z \in \mathbb{C}$ . Let $ f(z)= 
\begin{cases}
    1,& \text{if } |z|<\frac{1}{2}\\
    0, &             \text{if} |z-2| <\frac{1}{2} \\
0, & \text{if} |z-3| <\frac{1}{2}
\end{cases}$ Let \begin{equation*}
A = 
\begin{pmatrix}
4 & 1 & 0 & -4 & 0 & 0\\
0 & 4 & -1 & 0 & -4 & 2\\
0 & 0 & 6 & 0 & 0 & -6\\
2 & 0 & 0 & -2 & 1 & 0\\
0 & 2 & -1 & 0 & -2 & 2\\
0 & 0 & 3 & 0 & 0 & -3\\
\end{pmatrix}\end{equation*} Find $f(A)$ . Attempt: We know that $\mathbb{C}^6$ is a Hilbert space. and $B(\mathbb{C}^6) \simeq \mathbb{M_6}$ . So we can consider $A$ as a function in $B(\mathbb{C}^6)$ And we can write $Ae_j= \sum_{n=1}^6a_n e_n$ , where $(e_n)$ is the orthonormal basis for $\mathbb{C}^6$ . Also I know that the spectrum $\sigma(A)= \{\text{eigenvalues of} A \}$ From here I'm not sure how to proceed. I'm trying to solve this using Banach Algebra point of view. But any other suggestions will be much appreciated too! EDIT: My apologies, there was a typo in the original question. Now corrected Thank you!","['banach-algebras', 'matrix-exponential', 'matrices', 'linear-algebra', 'functional-analysis']"
4013282,Is the image of the intersection of preimage distributive and equal to the intersection of original set in the image?,"If $x\in\alpha^{-1}(A)\cap\alpha^{-1}(B)$ , then we apply $\alpha$ to both sides and get $\alpha(x)\in\alpha(\alpha^{-1}(A)\cap\alpha^{-1}(B))$ . Ok now this is the part where I am a bit unsure of. Can we just distribute $\alpha$ over the intersection and get $\alpha(x)\in A\cap B$ ? Because it is true that $f(A)\cap f(B)\subseteq f(A\cap B)$ . The intersection of two image is just a subset of the image of intersection so they are not equal, so that means $\alpha(\alpha^{-1}(A))\cap\alpha(\alpha^{-1}(B))\subseteq\alpha(\alpha^{-1}(A)\cap\alpha^{-1}(B))$ right? So if I distribute the $\alpha$ over the intersection, wouldn't I be getting a smaller set? Edit: Ok the above is not correct. It is the case that $f(A\cap B)\subseteq f(A)\cap f(B)$ if the function is not injective and equal if injective.","['elementary-set-theory', 'functions']"
4013287,"Prove that there exists $\gamma \in (a,b)$ such that $f \circ f(\gamma)=\gamma$","If $a,b,c \in R$ such that $a<b<c$ and $f:R \to R$ is a continuous function satisfying $f(a)=b,f(b)=c,f(c)=a$ , then there exists $\gamma \in (a,b)$ such that $(f \circ f)(\gamma)=\gamma$ My try : We have $f(a)=b, f(b)=c$ $\implies$ $f \circ f(a)=c$ . But $a=f(c)$ , so we get $f \circ f \circ f(c)=c$ Similarly $f \circ f \circ f(b)=b$ and $f \circ f \circ f(a)=a$ . Now since $a<b<c$ we have $f \circ f \circ f(a)<f \circ f \circ f(b)<f \circ f \circ f(c)$ So this means $f \circ f \circ f(x)$ is strictly increasing in $(a,b)\cup[b,c)$ and so it is One-One. Now since $f \circ f \circ f$ is One-One, that means $f \circ f$ is also One one which means $f$ is One-One. But how to show that there exists $\gamma \in (a,b)$ such that $f \circ f(\gamma)=\gamma$","['continuity', 'algebra-precalculus', 'functions', 'monotone-functions']"
4013299,Why is the Image sheaf a subsheaf?,"Given a morphism of sheaves $\psi:\mathcal{F} \rightarrow \mathcal{G}$ , we define the image sheaf Im $\psi$ to be the sheaf associated to the image presheaf $\text{Im}^{\text{pre}} \psi$ . By Hartshorne's construction, the sheaf $\mathcal{H}^+(U)$ associated to a presheaf $\mathcal{H}(U)$ is the set of functions $\{s:U \rightarrow \bigcup \mathcal{H}_p\}$ such that $\forall p \in U, s(p) \in \mathcal{H}_p$ and $\exists V \subset U$ containing p and $t \in \mathcal{H}$ such that $\forall q \in \mathcal{H}(V), s(q) = t_q$ . So my question is that why is the image subsheaf Im $\psi$ a subsheaf of $\mathcal{G}$ ? Clearly $\text{Im}^{\text{pre }} \psi (U)$ will always be a subgroup/subring of $\mathcal{G}(U)$ by definition, but I don't see why this sheafification of the image presheaf on an open $U$ , i.e. $\text{Im}^{\text{pre }} \psi (U)$ , needs to be a subgroup/subring of $\mathcal{G}(U)$ Edit: I don't know if Hartshorne mentions this but it is mentioned in Liu's Algebraic geometry and arithmetic curves in Lemma 2.16.","['algebraic-geometry', 'sheaf-theory']"
4013355,Does the short exact sequence $0\mapsto \mathbb{Z}^k\mapsto \Gamma\mapsto \mathbb{Z}^\ell\mapsto 0$ split?,I've seen in Nonsplit extension of $\mathbb{Z}$ by itself that every short exact sequence of the form $0\mapsto \mathbb{Z}\mapsto G\mapsto \mathbb{Z}\mapsto 0$ splits. I wonder if this is still valid if we change the exact sequence by $$0\mapsto \mathbb{Z}^k\mapsto \Gamma\mapsto \mathbb{Z}^\ell\mapsto0$$ where $\Gamma$ is a discrete group. Thanks in advance,"['group-theory', 'abstract-algebra', 'exact-sequence']"
4013415,How do I bypass the limit for the tan function on the calculator?,"So basically, my partner and I are creating problems for a project. She created one where we aren't sure what the correct answer is. Problem: $Find \ cos\frac{\theta}{2} \ if \ tan \ \theta = \frac{3}{4}; \pi < \theta<\frac{3\pi}{2}$ I'll show you what we did. First, we used the pythagorean formula to find $cos \ \theta$ (we need it later). The half-angle formula for cosine is $cos \frac {\theta}{2} = \pm \sqrt{\frac{1 + cos \ \theta}{2}}$ . These are our steps. $cos \frac {tan^{-1}\frac{3}{4}}{2} = \pm \sqrt{\frac{1 + cos \ (tan^{-1}\frac{3}{4})}{2}}$ $ = \pm \sqrt{\frac{1 + \frac{4}{5}}{2}}$ $ = \pm \sqrt{\frac{\frac{9}{5}}{2}}$ $ = \pm \sqrt{\frac{9}{10}}$ $ = \pm \frac{3\sqrt{10}}{10}$ To decide whether we have a negative sign or a positive, we have to look at the domain. $ \pi < \theta<\frac{3\pi}{2} \ changes \ to  \ \frac{\pi}{2} < \frac{\theta}{2} < \frac{3\pi}{4}$ We now know that $cos \frac {\theta}{2}$ is located in the second quadrant, which makes our final answer $cos \frac {tan^{-1}\frac{3}{4}}{2} = - \frac{3\sqrt{10}}{10}$ But then, I checked the calculator. The values are equal except for the negative sign My partner said that the calculator thinks tangent is in the first quadrant, and if it were, cosine would be positive. But since we are in the third quadrant, cosine is in the second quadrant. I know there are domain restrictions, but I don't entirely understand what she meant. Also, how do I get the calculator to display the same answer?",['trigonometry']
4013483,"Prove the identity $k \choose 2$ + $k \choose k-2$ + $k^2$ = $2k \choose 2$, where $k\geq2$ using a combinatorial proof.","A combinatorial proof for an identity proceeds as follows: State a counting question. Then, answer the question in two ways: One answer must correspond to the left-hand side (LHS) of the identity. The other answer must correspond to the right-hand side (RHS). Conclude that the LHS is equal to the RHS. Using this proof method, give a combinatorial proof of the identity: $k \choose 2$ + $k \choose k-2$ + $k^2$ = $2k \choose 2$ , where $k\geq2$ I understand you have to essentially split the $2k \choose 2$ . I could treat it as $m +n \choose 2$ where m and n are equivalent and equal to k. And I get where the $k \choose 2$ and $k \choose k-2$ come from but I have no idea how the $k^2$ comes into play. and why thats involved?","['combinations', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4013504,How can $\frac{1}{\ln(x)}$ be equal to $0$?,"High school calculus student checking in here – first time poster. I got asked a question by one of my friends:
If $\cot x = \frac{1}{\tan(x)}$ , how could $\cot(x) = 0$ ? For this to be possible, he reasoned, $\tan(x)$ would have to be equal to infinity – and division by infinity does not work. Rewriting $\cot(x)$ as $\frac{\cos(x)}{\sin(x)}$ makes the problem make sense, but doesn't explain why it doesn't make sense in the previous form. Furthermore, as I was looking for more examples of similar things, I found that (at least according to desmos) $\frac{1}{\ln(x)}$ is satisfied by the coordinate $(0, 0)$ . I cannot wrap my head around how this is possible, particularly since $\ln(x)$ is not even defined at $0$ . I wasn't able to find a good answer to this online, and we would really appreciate an elegant (i.e. understandable for high school students) explanation of why this is.","['trigonometry', 'graphing-functions', 'logarithms']"
4013530,"If $g\circ f$ is one-to-one, and $f$ and $g$ have the same domain/codomain, must $f$ and $g$ also be one-to-one?","If $g\circ f$ is one-to-one for the functions $f : A \to B$ and $g : A \to B$ , must both $g$ and $f$ also be one-to-one? Normally, when $f$ and $g$ have different domains/codomains, only $f$ must be one-to-one. However, if both $g$ and $f$ have the same domain/codomain, and their composition is one-to-one, must $g$ also be one-to-one? My intuition tells me yes, but I wasn't able to work out a formal proof. Any input for why g would/wouldn't have to be one-to-one would be much appreciated!","['elementary-set-theory', 'functions']"
4013531,"If $A^{n-1}$ is not diagonalizable but $A^n$ is, show $A^n$ is zero","Let $n$ be an integer greater than or equal to 2 and let $A$ be a $n^{th}$ order complex square matrix. Show that if $A^{n-1}$ is not diagonalizable and $A^n$ is diagonalizable, then $A^n=0$ . I have tried to think of a way to prove that but couldn't. I know that, If a matrix is diagonalizable over a field $F$ if and only if its minimal polynomial is a product of distinct linear factors over $F$ . Can we use this to prove it or what is the proper way to think about it?","['matrices', 'diagonalization', 'linear-algebra']"
4013573,Is the set $A$ of anti-symmetric relations over $\mathbb{N}$ countable? [duplicate],"This question already exists : Is the set of partial order relations over $ \mathbb{N}$ countable? [duplicate] Closed 3 years ago . I think this set is uncountable, but I don't really know how to prove this. Maybe somehow find an injection from $\mathcal{P}( \mathbb{N} \times  \mathbb{N}) \rightarrow A$ ?",['elementary-set-theory']
4013715,Inequality with $abc=1$,"Let $a;b;c$ be positive real numbers and $abc=1$ Find maximum value of: $P=\dfrac{1}{\sqrt{a^2+2b^2}}+\dfrac{1}{\sqrt{b^2+2c^2}}+\dfrac{1}{\sqrt{c^2+2a^2}}$ I tried to use Cauchy-Schwarz: $a^2+2b^2 \geq \dfrac{1}{3}(a+2b)^2$ Then: $P \leq \dfrac{\sqrt{3}}{a+2b}+\dfrac{\sqrt{3}}{b+2c}+\dfrac{\sqrt{3}}{c+2a}$ So, what should I do next? Thank you.","['contest-math', 'algebra-precalculus', 'cauchy-schwarz-inequality', 'inequality']"
4013736,Is the multiplication operator on $\ell^p$ compact?,"Let $p\in[1,\infty]$ , $a\in\ell^\infty$ and $$T:\ell^p\to\ell^p\;,\;\;\;x\mapsto ax.$$ Note that $\left\|T\right\|=\left\|a\right\|_{\ell^\infty}$ . It's easy to see that if $a\in\ell^1$ , then $T$ is nuclear and hence compact. Can we more generally conclude that $T$ is compact? Assume $p<\infty$ . The intuitive approach would be to define $$T_n:\ell^p\to\ell^p\;,\;\;\;x\mapsto(a_1x_1,\ldots,a_nx_n,0,\ldots)$$ for $n\in\mathbb N$ and to show that $\left\|T_n-T\right\|\to0$ . Now, $$\left\|(T_n-T)x\right\|^p=\sum_{i=n+1}^\infty|a_ix_i|^p\tag1$$ for all $x\in\ell^p$ and $n\in\mathbb N$ . The right-hand side of $(1)$ clearly converges to $0$ as $n\to\infty$ , but this only yields convergence in the strong operator topology. On the other hand, if $(a_n)_{n\in\mathbb N}$ would be nonincreasing and $a_n\to0$ (e.g. $a_n=\frac1n$ ), then $$\left\|(T_n-T)x\right\|^p\le|a_{n+1}|^p\left\|x\right\|_{\ell^p}^p\tag2$$ for all $x\in\ell^p$ and $n\in\mathbb N$ and we could immediately conclude $\left\|T_n-T\right\|\to0$ . EDIT : Maybe the general result can be obtained from the convergence in strong operator topology (which holds by $(1)$ ) by noting that $\ell^p$ has the approximation property; but I'm not sure whether this is true.","['operator-theory', 'functional-analysis']"
4013822,Coordinates of a point after rotating a line from one end [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I am working on a project in javascript and p5.js and am stuck at this point. I have to rotate a line from one of its ends with a certain angle in Radians/Degrees. Here's what I want to get- the image Sorry for my bad drawing. So I want a formula to get C(x,y) to use in my code. What should be the values of x and y in terms of t-ratios and x1,y1,x2,y2 Sorry,flip the positions of A and B.","['coordinate-systems', 'trigonometry', 'rotations']"
4013888,How do you prove Osborn's rule?,"Given a trigonometric identity written in terms of sine and cosine, it is possible to write down the corresponding hyperbolic identity using Osborn's rule: Replace $\cos$ with $\cosh$ Replace $\sin$ with $\sinh$ However, negate the product of two $\sinh$ terms. For example, $$
\cos^2x+\sin^2x \equiv 1 \implies \cosh^2x-\sinh^2x=1 \, .
$$ I tried proving this rule using the following two identities \begin{align}
\cos ix &\equiv \cosh x \\
\sin ix &\equiv i\sinh x \, .
\end{align} Given an arbitrary trigonometric identity of the form $$
f(\cos z,\sin z) \equiv g(\cos z, \sin z)
$$ then letting $z=ix$ we see that \begin{align}
f(\cos ix,\sin ix) &\equiv g(\cos ix, \sin ix) \\
f(\cosh x,i\sinh x) &\equiv g(\cosh x, i\sinh x) \\
\end{align} From here, it seems the rule makes sense because the product of two $i\sinh x$ terms gives us $\color{red}{-}\sinh x$ . But I'm not quite sure how to formalise this argument. Moreover, an identity such as $$
\sin 2z \equiv 2\sin z \cos z
$$ would correspond to $$
i\sinh x \equiv 2i\sinh x \cosh x \, .
$$ Here, we could divide through by $i$ and the expected hyperbolic identity pops out, but I'm unsure if this 'divide by $i$ ' trick always works, since there may be other terms that don't contain an $i$ in them. To give a slightly contrived example, $$
\sin x + \cos 2x \equiv \sin x + \cos^2 x - \sin^2x
$$ corresponds to $$
i\sinh x + \cosh 2x \equiv i\sinh x + \cosh^2 x + \sinh^2 x \, ,
$$ and here, dividing by $i$ does not help. So I think I've understood the crux of how to prove Osborn's rule, but there are a few loose ends that I need to sort out.","['trigonometry', 'solution-verification', 'hyperbolic-functions']"
4013906,Combinatorics on divisors,"Consider the set $D$ of all divisors of $10000$ . What is the number of subsets $H \subseteq D$ containing at least two elements such that for every $a, b \in H$ , $a \mid b$ or $b \mid a$ ?
I tried translating the problem into one about integer grid, which makes it ask for the number of ways to choose at least two points in the square with vertices $(0, 0)$ and $(4,4)$ ordered in such a way that they can be reached sequentially by only moving up and right. But I fail to see a way to calculate it besides setting up this recurrence: let $a_{m,n}$ be the number of subsets of the square between $(0,0)$ and $(m,n)$ that satisfy the above property. We can form such a subset in a number of ways: it could be a subset already counted in $a_{m-1,n}$ , or in $a_{m,n-1}$ , or one of those same subsets with the addition of $(m,n)$ , or be composed of $(m,n)$ and a point in the rectangle with vertices $(0,0$ ) and $(m, n)$ (there are $(m+1)(n+1)-1$ such points). If we simply added $a_{m-1,n}$ and $a_{m,n-1}$ we would be counting twice the subsets already counted in $a_{m-1, n-1}$ . The recurrence is thus: $\begin{cases} a_{0, 0} = 0 \\ a_{n,0} = a_{0,n} = 2a_{n-1, 0} + n \\ a_{m,n} = 2(a_{m-1,n} + a_{m,n-1} - a_{m-1,n-1}) + (m+1)(n+1)-1\end{cases}$ . And the answer we are looking for is $a_{4,4}$ . Is there a more elegant way to answer this however?","['contest-math', 'prime-factorization', 'combinatorics', 'integer-lattices']"
4013948,Are they known properties for the self-convolution application $H : f \mapsto f \star f$?,"I was wondering if the application $H : L^1(\mathbb{R}) \rightarrow L^1(\mathbb{R})$ defined by : $$H(f) := f \star f, \quad \forall f \in L^1(\mathbb{R})$$ had some known properties ? For example we know that , using Young inequality, that $||H(f)||_{L^1(\mathbb{R})} \leq ||f||^2_{L^1(\mathbb{R})}$ , but that's very common. Can we caracterize $H(L^1(\mathbb{R}))$ or $H^{-1}(\{0 \})$ ? Is the application $H$ surjective ? Are they counter-example to the surjectivity ? I'm looking for any properties or references since I'm just curious about it. I guess using Fourier transform will probably be a very useful tool. Apart from these first questions, I'm also wondering if there exists properties for $G : L^p(\mathbb{R}) \rightarrow L^r(\mathbb{R})$ defined by : $$G(f) = g \star f\quad \forall f \in L^p(\mathbb{R})$$ for a fixed $g$ in $L^p(\mathbb{R})$ with $1 \leq p,q,r \leq + \infty$ verifying the young equality : $$\frac{1}{p} + \frac{1}{q} = 1 + \frac{1}{r}.$$","['convolution', 'lp-spaces', 'functional-analysis', 'reference-request']"
4013964,Linear reward-inaction algorithm for two-armed bandit,"Suppose there are two slot-machines. When playing one of them, you win with probability $p$ and while playing the other you win with probability $q$ , where $0 < q < p <1$ . A gambler approaches them. He does not know the probabilities of victory for either of slot-machines. However, he has recently read ""Theories of Learning"" by Hilgard and Bower and has therefore decided to employ the following algorithm from that book: Suppose $p_n$ is the probability of choosing the first slot-machine on $n$ -th pull. Then $p_0 = \frac{1}{2}$ and for $n \geq 1$ $p_{n} = \alpha + (1 - \alpha) p_{n-1}$ if he pulled the lever of the first slot-machine and won, $p_{n} = (1-\alpha)p_{n-1}$ if he pulled the lever of the second slot-machine and won and $p_{n} = p_{n-1}$ otherwise. Here $\alpha \in [0;1]$ is some constant. What is the expected number of wins the gambler scores if the number of pulls he makes is geometrically distributed with parameter $\gamma$ ? What have I tried: The expected number of wins is $\sum_{n=0}^{\infty} \gamma^n E[q_n]$ , where $q_n$ is the probability of winning after $n$ -th pull. Because $q_n = p_np + (1 - p_n)q = q + p_n(p - q)$ . Thus the expected number of wins is $\frac{q}{1-\gamma} + (p-q)\sum_{n=0}^{\infty} \gamma^n E[p_n]$ . So, all we need is to find $E[p_n]$ . For $p_n$ we have the following recurrence: $$p_0 = \frac{1}{2}$$ $$p_n = \begin{cases}  \alpha + (1 - \alpha) p_{n-1} & \quad \text{ with probability } p_{n-1}p \\ (1-\alpha)p_{n-1}  & \quad \text{ with probability } (1 - p_{n-1})q \\ p_{n-1}  & \quad \text{ with probability } 1 - q - p_{n-1}(p - q) \end{cases}$$ However, because this recurrence is non-linear it can not be easily translated into recurrence for expectations. And here I am stuck.","['expected-value', 'stochastic-processes', 'recurrence-relations', 'probability']"
4013975,How to define linear equations in an introductory linear algebra class,"I believe there is an issue of clarification with respect to the definition of linear equations in many linear algebra texts. Here is a typical one ``A linear equation in the $n$ variables $x_1,x_2, ..., x_n$ is an equation that can be written in the form $a_1x_1+a_2x_2+ ... +a_nx_n=b$ where the coefficients $a_1,a_2,...,  a_n$ and the constant term $b$ are constants.'' No problem with the definition so far except that the text proceeds to state that $x^2-y^3=5$ is not linear because ""it contains powers"".  There is no further explanation. Of course a well-prepared student would say: $(x+y+5)^2-(x+y)^2=35$ appears to contain powers and is a linear equation, how do you know that $x^2-y^3=5$ cannot be simplified? I want to say the issue is with ""[it] can be written"" in the definition. That some operational explanation should be added. Here are my two questions: (a) Which introductory textbooks go deeper in the issue? (b) How do you approach the issue? (Again this is introductory linear algebra, students come in with just calculus I as prerequisite).",['linear-algebra']
4014031,Divisibility bound on sums of numbers,"Suppose $p$ and $q$ are two prime numbers. Let $N(p)$ be the set of natural numbers that are divisible only by primes $\leq p$ . Define $N(q)$ similarly. If $N(p,q)$ is the set of numbers that can be written as a sum of a number in $N(p)$ and a number in $N(q)$ , what is the upper bound on the primes that can divide numbers in $N(p,q)$ , if any?",['number-theory']
4014046,A function with a condition,"This question was asked for one of our internal tests (high school) $\mathit f(x):$ $ $$\Bbb R  \mapsto \Bbb R $ is a continuous function and has the property $\;\mathit f(f(x)) = 1-x \,\,$$\,\forall \,\, x \in [0,1] $ and $\mathit J= \int _0 ^1 f(x) dx$ , then find the value of $\mathit 1/J.$ Will someone provide an example of such a function. NB: I do not require a solution, please provide me only an example of such a function.","['calculus', 'functions', 'real-analysis']"
4014072,How to find $F^{(n+1)}(x)$ if $F(x)=\int_0^x(x-t)^nu(t)dt$?,"if $F(x)=\int_0^x(x-t)^nu(t)dt$ then find $F^{(n+1)}(x)$ from Leibniz rule $a(x)=0,b(x)=x,a'(x)=0,b'(x)=1$ and $G'(x)=n(x-t)^{n-1}\\$ so $F^{(1)}(x)=n\int_0^x(x-t)^{n-1}u(t)dt$ and $F^{(n)}(x)=n.(n-1).(n-2)...1\int_0^x(x-t)^{n-n}u(t)dt=n!\int_0^xu(t)dt$ . How can I continue from here? is it $F^{(n+1)}(x)=0$ because applying Leibniz rule one more time gives $G'(x)=0$ ?","['calculus', 'derivatives', 'leibniz-integral-rule']"
4014102,Show that $f$ is bijective.,"Show that $f$ , defined in $0<x<s, (s>0)$ as follows: $$f(x) = {2x-s \over x(s-x)}$$ is a bijective function of this interval in the reals. To show that it is injective, we assume $f (x) = f (y)$ and after manipulating we find: $$(y-x)(-s^2+s(y+x)-2xy)=0$$ then $x=y$ , right? And to show that is surjective just take $f(x)=y$ and manipulating we get $$x = {ys-2 \pm \sqrt {4+y^2s^2} \over 2y}$$ Is that right? The result looks kind of ugly, I was not very confident.","['calculus', 'functions', 'algebra-precalculus', 'real-analysis']"
4014105,Variance of a single random variable with two terms.,"A random variable X, representing the time until some event occurs, has the following pdf. $$(14/99)e^{-0.5x}+(85/99)e^{-0.25x}$$ .
Using integration, I get the expectation as 14.303 via $\int_0^\infty xf(x)dx$ , and a variance of 463.453 via $\int_0^\infty (x-\overline{x})^2f(x)dx$ . I don't understand why the variance is so high. I also computed the median, which was much higher than the mean. When I try and compute the variance using $Var(X)=E(X^2)-(E(X))^2$ I get a negative value.
2) Why is there a mismatch between using these 2 methods?",['statistics']
4014144,"How do I solve $\min_x \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx)$ for $\lVert x \rVert_2 = 1$.","Let $f(x) = \max(c_1^Tx, c_2^Tx, \dots, c_k^Tx)$ .
where $x, c_1, c_2, \dots, c_k \in \mathbb R^n$ .
What fast iterative methods are available for finding the (approximate) min of $f$ with the constraint $\lVert x \rVert_2 = 1$ ? Notes: $f$ is convex and and non negative, that is, $c_i$ positively span $\mathbb R^n$ For my use case $k \gg n$ and (rougly) $3 \le n \le 50$ and $100 \le k \le 10000$ .
I tried projected subgradients (projected to the sphere) but it can be slow to converge and improving the initial guess doesn't seem to accelerate the method. From the literature, this seems to be equivalent to Riemannian manifold subgradient methods which use the exponential map. I haven't tried bundle methods yet but I am investigating them now. They seem like they might be too slow. I've also tried approximating $f$ with a smooth maximum LogSumExp and doing gradient descent with projection. I also tried unconstrained gradient descent with a quadratic penalty . The approximation is too inaccurate and the evaluation of exp is too slow for my use case. I'm not interested in SQP because I suspect whatever method used to smooth (e.g. LogSumExp, hyperbolic) will be too costly/inaccurate to evaluate. Edit:
I believe this problem is equivalent to a linear programming problem with quadratic equality constraints. I haven't been able to to find much literature on this type of problem. Perhaps I can use a SDP method but I'm not sure.","['convex-optimization', 'nonlinear-optimization', 'real-analysis', 'non-convex-optimization', 'optimization']"
4014174,$S^3$ as union of 2 solid tori,"I am working through Hatcher's Algebraic Topology and don't get what he means by ""standard decomposition of $S^3$ into two solid tori"", $$S^3= S^1 \times D^2 \cup D^2 \times S^1$$ Can someone explain (or visualize) what the union looks like?
First, I thought it looked like a genus- $2$ -torus: but I guess that's not what he means, actually...
I have basic knowledge in algebraic topology and I am not familiar with Heegaard Splittings. To obtain an embedding of $X$ in $S^3-K$ as a deformation retract we will use the standard decomposition of $S^3$ into two solid tori $S^1\times D^2$ and $D^2\times S^1$ , the results of regarding $S^2$ as $\delta D^4=\delta(D^2\times D^2)=\delta D^2\times D^2\cup D^2\times\delta D^2$ . Geometrically, the first solid torus $S^1\times D^2$ can be identified with the compact region in $\Bbb R^3$ bounded by the standard torus $S^1\times S^1$ containing $K$ , and the second solid torus $D^2\times S^1$ is then the closure of the complement of the first solid torus, together with the compactification point at infinity. Notice that meridional circles in $S^1\times S^1$ bound disks in the first solid torus, while it is longitudinal circles that bound disks in the second solid torus. I have read many similar questions and answers, but none I found could help me out, so I started a new one.","['geometry', 'algebraic-topology']"
