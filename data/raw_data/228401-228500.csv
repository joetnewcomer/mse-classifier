question_id,title,body,tags
4726653,Invariance of trace of an operator,"I am following Reed & Simon's text on functional analysis. They first introduce the trace of a positive operator in the following theorem: Let $H$ be a separable Hilbert space, $\{\varphi_n\}_{n=1}^\infty$ an orthonormal basis. Then for any positive operator $A \in \mathscr{L}(H)$ we define $tr (A) = \sum_{n=1}^\infty (\varphi_n, A\varphi_n)$ . The number $tr A$ is called the trace of $A$ and is independent of the orthonormal basis chosen. They also give the following definition of a trace class operator: An operator $A \in \mathscr{L}(H)$ is called trace class if and only if $tr |A| < \infty$ . The family of all trace class operators is denoted by $\mathscr{T}_1$ . Afterwords, they define the trace on $\mathscr{T}_1$ instead of positive operators: The map $tr: \mathscr{T_1} \rightarrow \mathbb{C}$ given by $tr (A) = \sum_{n=1}^\infty (\varphi_n, A\varphi_n)$ where $\{\varphi_n\}$ is any orthonormal basis is called the trace. So far I understand all of the above. My confusion comes from the following remark in the book: We remark that it is not true that $\sum_{n=1}^\infty |(\varphi_n, A\varphi_n)| < \infty$ for some orthonormal basis implies $A \in \mathscr{T}_1$ . For $A$ to be in $\mathscr{T}_1$ the sum must be finite for all orthonormal bases. I see how why the first part of this remark holds, since not every convergent series is absolutely convergent. However, I thought the trace is invariant under a change of basis so why is proving finiteness for all orthonormal bases necessary? This is not included as part of the definition of trace class that I've included above. Does the invariance of the trace operator only hold for positive operators? I am having some trouble seeing the difference between the trace of a positive operator and the trace of an operator in $\mathscr{T}_1$ .","['operator-theory', 'trace', 'functional-analysis']"
4726699,Does $\frac{1}{1-e^{-\frac{1}{e^x}}} - e^x - \frac{1}{2} $ really explode with oscillatory behavior past $x = 15$?,"I was looking at the function $$ \frac{1}{1-e^{-\frac{1}{e^x}}}-e^x - \frac{1}{2}$$ I thought I had reason to believe this tends to 0  as $x$ tends to positive infinity because $$ \sum_{n=0}^{\infty} e^{nx} = \frac{1}{1-e^x}= -\frac{1}{x}+\frac{1}{2}-\frac{1}{12}x \ + \ ...$$ So: $$ \sum_{n=0}^{\infty} e^{-\frac{n}{x}} = \frac{1}{1-e^{-\frac{1}{x}}} =  x + \frac{1}{2}+\frac{1}{12x} \ + \  ...$$ So: $$ \sum_{n=0}^{\infty} e^{-\frac{n}{e^x}} = \frac{1}{1-e^{-\frac{1}{e^{x}}}} =  e^x + \frac{1}{2}+\frac{1}{12e^x} \ + \  ...$$ So one would expect that $$ \frac{1}{1-e^{-\frac{1}{e^{x}}}} - e^x - \frac{1}{2} $$ tends to 0 as $x$ goes to infinity as the coefficients are proportional to $\frac{B_n}{n!}$ Surprisingly, both Wolfram Alpha, and Desmos disagree, see images below: My gut feeling tells me i'm still correct here but the odds of tripping up TWO different graphing platforms seems unlikely so I wanted to get a reality check from someone: Here are links to the graphs: Wolfram and Desmos","['calculus', 'graphing-functions', 'taylor-expansion', 'mathematica']"
4726740,Does $f(x)=x$ and $f(f(x))=x$ have exactly the same set of solutions when $f(x)$ is monotone?,"Does $f(x)=x$ and $f(f(x))=x$ have exactly the same set of solutions when $f$ is a monotone function? My try: It is pretty obvious that every solution of $f(x)=x$ is also the solution of $f(f(x))=x$ , since $f$ is a function. But how about the converse. Case $I:$ When $f$ is Monotone Increasing Let $x_0$ be the solution of $f(f(x))=x$ and let us assume that $x_0$ is not the solution of $f(x)=x$ . Then WLOG, let us start with $f(x_0)<x_0$ . So we have $$f(x_0)<x_0 \Rightarrow f(f(x_0))<f(x_0)<x_0 \Rightarrow x_0<x_0$$ This is a contradiction. Thus every solution of $f(f(x))=x$ is also the solution of $f(x)=x$ . Case $II:$ But how about when $f$ is monotone decreasing? Any help?","['inverse-function', 'monotone-functions', 'analysis', 'functions', 'algebra-precalculus']"
4726742,How to differentiate an expression with a variable and parameters,I came across the expression of $\sqrt{x^2+a^2}$ with $a$ as a non zero real number parameter. I thought that I would simply treat $a^2$ as simply one integer when doing derivations but my instructor said that I was wrong. Can anyone please help explain why this is the case and inform me how to treat parameters in f(x) expressions? thanks a lot!,"['calculus', 'derivatives', 'parametric']"
4726786,Find every bijection of non-zero real numbers whose inverse is its reciprocal,"Find all functions $f $ : $\mathbb R^{×}$ $\to$ $\mathbb R^{×}$ that are one-to-one and
onto and such that $f^{−1}(x)= 1/ f(x)$ , Where $\mathbb R^{×}=\mathbb R-$ {0} My approach:= At first I have consider $f(x)=x$ and $f(x)=1/x$ then this does not satisfy $f^{−1}(x)= 1/ f(x)$ Now I am unable to construct such functions.",['analysis']
4726815,How to approach this tricky log problem,"Consider the following equation: $$(7 + 4\sqrt{3})^{t^2 - 5t + 5} + (7 - 4\sqrt{3})^{t^2 - 5t + 5} = 14$$ I have tried and exhausted all the methods I know of and resorted to brute force to solve this problem and got $t = 4$ , $t = 1$ .
However I was wondering if there was some sort of elegant solution to prove the answer algebraically.","['elementary-functions', 'calculus', 'logarithms']"
4726827,"Solving $\iint_{\mathscr{D}}\frac{(x-x')^2f(x',y')\,\mathrm{d}x'\mathrm{d}y'}{\left((x-x')^2+(y-y')^2\right)^{3/2}}=1$ for the unknown function $f$","While solving a fluid dynamical problem, the following integral equation arised: $$
\iint_\mathscr{D} \frac{(x-x')^2 f(x',y') \, \mathrm{d}x' \mathrm{d}y'}{\left( (x-x')^2 + (y-y')^2 \right)^{3/2}} = 1
$$ for the unknown function $f$ on the domain $\mathscr{D}$ defined as a square of unit length centered at the origin, i.e. $x,y \in [-1/2,1/2]$ What I tried is to expand $f$ in the form of multivariate Taylor series and evaluate the resulting integrals term by term.
This approach does not seem to be of help since identification with the right-hand side of the above equation is not possible. It would be great if someone here could provide useful hints that could help solve this integral equation. Thank you!","['integration', 'real-analysis', 'multivariable-calculus', 'calculus', 'greens-function']"
4726854,Is every finite simple group a quotient of a braid group?,"Question: Is every finite simple group a quotient of a braid group? Context: The braid group on two strands $ B_2 $ is isomorphic to $ \mathbb{Z} $ and so the infinite family of abelian finite simple groups (cyclic of prime order) are all quotients of $ B_2 $ . The braid group on three strands $ B_3 $ is a universal central extension of $ PSL(2, \mathbb{Z}) $ , $ B_3 \cong \mathbb{Z} : PSL(2,\mathbb{Z}) $ . So the infinite family of finite simple groups $ PSL(2,p)\cong PSp(2,p) $ are all quotients of $ B_3 $ . This pattern continues with higher $ B_n,n \geq 3 $ . There is a quotient of $ B_n $ which is a (finite index) subgroup of $ Sp(2m,\mathbb{Z}) $ for some $ m $ ( $ m $ is the rank of the homology of a certain space, details given in this paper https://link.springer.com/article/10.1007/BF02566275. ) And moreover if we take this subgroup mod $ p $ then we get all of $ Sp(2m,p) $ . Thus we have that $ Sp(2m,p) $ for all primes $ p $ are quotients of that $ B_n $ . Assuming that every integer is the $ m $ corresponding to some $ n $ then we have that every finite simple group $ PSp(2m,p) $ is a quotient of some Braid group $ B_n $ . Since $ PSp(2m,p) \cong PSL(2,p) $ then $ PSL(2,5) $ and $ PSL(2,7) $ are indeed quotients of $ B_3 $ and all cyclic groups are quotients of $ B_2 $ . The next group to check would be $ A_6 $ . So one way to answer this question would just be to show that $ A_6 $ is not a quotient of any braid group $ B_n $ .","['simple-groups', 'group-theory', 'quotient-group', 'braid-groups']"
4726891,Can anything be said about convexity of this function?,"I have the following optimization problem, to find the Excess Mass of for a given $t \ge 0$ $EM(t) = \sup_{u \ge 0}\  \mathbb{P}\Big[s(X) \ge u\Big] - t \cdot Leb\ (\{s \ge u\})$ where, $s:R^d \to R^+$ is a scoring function $X$ is a random variable from a distribution $f$ be the PDF of the above-said distribution $s \ge u$ is the level set of $s$ at level $u$ $Leb$ is the Lebesgue measure I was wondering if anything can be commented on the convexity of the function. i.e., positive, negative, or could be anything? Basically, my main objective is to improve the time complexity (keeping the $t$ fixed) using binary search over linear search if anything could be commented on that.
Even if generalizing is not possible, what could be the conditions for which the function is concave/convex? I have tried plotting it on a few datasets ( $X$ ) and scoring functions ( $s$ ) and always found the graphs concave upwards. So, in this case, the maximum lies at the end.
Although I need some theoretical backing to make a statement conclusively. Reference: On Anomaly Ranking and Excess-Mass Curves Edit: I plotted $EM_t(u)$ vs $u$ by keeping the $t$ fixed.","['multivariable-calculus', 'measure-theory', 'statistics', 'probability']"
4726903,"Proving the identity $ \sum_{k=0}^{\min\{p,q\}} \binom{n}{k} \binom{n-k}{p-k} \binom{n-p}{q-k} = \binom{n}{p} \binom{n}{q}$","While preparing for an exam, I am trying to prove the identity below. I searched for similar questions but only found Combinatorial proof of ${n\choose p}{n\choose q}=\sum_{k=0}^n {n \choose k}{n-k \choose p-k}{n-k \choose q-k}$ (Someone mentioned that this identity may be not true). Question Let $p,q ,n\in \mathbb{N}$ s.t $p,q \le n$ $$ \sum_{k=0}^{\min\{p,q\}} \binom{n}{k} \binom{n-k}{p-k} \binom{n-p}{q-k} = \binom{n}{p} \binom{n}{q}$$ My attempt First of all, I want to find a combinatorial proof since it's important for me to understand the ""combinatorial story"" behind it. I draw this in order to understand maybe what's the ""story"". But I'm stuck understanding what are the purple elements and the whole connection to the LHS (Note that the index k ""runs"" over vaules from $0$ to $\min{(p,q)}$ ). I would appreciate if someone here can try to continue my solution or suggest an alternative solution (if you do please explain each step the motivation behind it).","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4726939,Probability of forming a trapezium using $4$ of the $12$ equally spaced points on the circumference,"This is a question from the $2022$ APMOPS competition online paper $1$ last question : There are $12$ equally spaced point on the circumference. If $4$ of the $12$ points are to be chosen at random, what is the probability that a quadrilateral having the $4$ points chosen as vertices will be trapezium ( Trapezium is a quadrilateral with one pair of parallel sides)? My attempts : We choose $2$ points as a vertices. We have $\binom{12}{2}=66$ ways to choose the vertice. Let us label the vertices as vertices $1$ . In the remaining $10$ points, we need to choose $2$ of the points to form a vertex parallel to vertices $1$ . For every vertices $1$ , we have $5$ vertices formed by the remaining $10$ points that parallel to the vertices $1$ . So the total number of ways to form a trapezium is $66\times5\div2=165$ as we double counted the two vertices we choose. But we also double counted the total number of ways having $4$ points chosen as vertices will be a rectangle . We can still choose a vertices from $12$ equally spaced points on the circumference. We can form $\binom{12}{2}=66$ vertices by the points. In this $66$ vertices , $6$ of the vertices(the $6$ of the vertices are all the axis of symmetry of the regular dodecagon) can’t form a rectangle. For every of the remaining $60$ vertices , we have another $1$ vertices formed by the remaining $10$ points that parallel and equal to the previous vertices we choose. We counted four times the total number of ways ( two times for the two vertices we chose, and the remaining two times for the remaining two vertices was formed) having $4$ points chosen as vertices will be a rectangle . So we need to divide $4$ and we get the answer $15$ ( $60\div4=15$ ). So the final answer is $$\frac{\text{P(four points form a trapezium)}}{\text{P(four points randomly chosen from the 12 points)}}=\frac{165-15}{\binom{12}{4}}=\frac{150}{495}=\frac{10}{33}$$ Is my approach correct?","['contest-math', 'geometry', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4726968,What algebraic manipulations are needed for real analysis?,"What level of algebraic manipulations are needed for real analysis and higher math, and how should one attain it? I'm able to solve most proofs in typical Real Analysis textbooks or university exams.  Yet, despite that, when presented with a challenging, but elementary, algebraic manipulation (i.e. the kind seen on olympiads), I sometimes struggle. Take, for example, some of the problems on this AoPS Intermediate Algebra Test , such as: Find all solutions (real and complex) to $\sqrt{x − 5} + \sqrt{x + 15} = 10$ and to $\sqrt[3]{x^2 - 1} + \frac {20}{\sqrt[3]{x^2 - 1}} = 12$ Simplify $\sqrt[4]{161 − 72\sqrt 5}$ or this one from Gelfand's Algebra: How to factor $a^{3} + b^{3} + c^{3} - 3abc$ into a product of polynomials I find these and similar problems quite challenging. Should I go back and build up these elementary algebraic manipulation skills before proceeding further with higher math?  If so, how?  When and how did you built these skills? In the spirit of math.SE, I'll share my ""work"" on this question so far: It would seem yes, go back and build these foundations before proceeding , because you need the foundations before building. But: The fact that you're able to succeed at math well beyond that raises doubts if these really are foundations, or more contest style challenges. These algebraic manipulations, beyond the very basics, don't seem to be covered in any standard text, at any level, except for contest math .  High school algebra and precalculus texts do not teach the advanced manipulations needed to solve problems like the above.  And university level analysis, linear algebra, abstract algebra start after them. All of which suggests that this level of manipulation is primarily part of contest math , but not generally a foundation or component of ""standard"" math.  It's well established the difference: contest math revolves around ""tricks,"" usually to remove deliberate obfuscation; standard math revolves around underlying concepts and techniques which expose a unity and clarity Support for the above: When looking for resources on problems like the above, the results are almost entirely contest math sites (even the names, like ""Simon's Favorite Factoring Trick,"" are from contest math). Still: Eventually, students of ""standard"" math need to be able to use the techniques, eventually.  There have been problems, such as What is the locus of points in the plane $\{v : v \cdot (v-a) = 0\}$ for fixed $a$? or simplfying $xy−bx−ay−ab=0$ ,  where I get stuck on the manipulations.  Do they just pick them up somehow? Do they become obvious once you've learn enough e.g. analysis and algebra? Update: Another example of where this came up in analysis is How to solve a particular system of non-linear multivariate equations? To fill this gap, I invested some effort in going through books and resources on algebraic manipulations for contest math, and while I found it helpful, it certainly doesn't provide the satisfaction or insight as e.g. proving problems in analysis.  So I'm confused whether I should return to analysis or continue working these manipulations, and, if I do, when and how can I pick these manipulations up?  When and how have others done so?","['contest-math', 'self-learning', 'real-analysis', 'algebra-precalculus', 'soft-question']"
4727000,Finding the radius of convergence of this power series,"I need help finding the radius of convergence of the power series $$\sum_{n=1}^{\infty}\left(\sqrt{n+a}-\sqrt{n}\right) x^n,$$ for some $a > 0$ . I know the radius must be 1 but I can't formally show it by computing the limit of the quotients of the coefficients. Basically I want to show $$\lim\limits_{n\to \infty}\frac{\sqrt{n+a+1}-\sqrt{n+1}}{\sqrt{n+a}-\sqrt{n}} = 1.$$ I have tried expanding the quotient using the binomial formula, but I can't get it to work without using handwavy arguments. Thanks in advance!","['power-series', 'limits', 'calculus', 'convergence-divergence']"
4727006,Show that a function given by series is not continuous at 0,"How to show that the function $$
f(x)=\sum\limits_{n=1}^\infty e^{-n^2x}\sin(nx), \quad x \geq 0,
$$ is not continuous at $0$ ? The problem is that there is $n^2$ in the power of $e$ . If $n$ was there, I would be able to calculate the sum explicitly","['analysis', 'real-analysis', 'continuity', 'sequences-and-series', 'limits']"
4727066,Show that a l-homogenous polynomial is harmonic under certain conditions,"Let $a,b \in \mathbb{R}^d$ and $l \in \{2,3\}$ . Then $P:\mathbb{R}^d \to \mathbb{C}, P(x):=(<a,x>+i<b,x>)^l$ is a $l$ -homogenous polynomial. Show that $P$ is harmonic iff $||a||=||b||$ and $<a,b>=0$ . My attempt was to calculate the Laplace of $P(x)$ . So I wrote $P(x)=(\sum_{i=1}^d a_i x_i+i \sum_{i=1}^d b_i x_i)^l$ . I'm not sure if it's correct but I got: $$\Delta P(x)=l(l-1)(\sum_{i=1}^d a_i x_i+i b_i x_i)^{l-2}\sum_{i=1}^d (a_i +i  b_i)^2$$ Assuming $P$ is harmonic, then it's Laplace is $0$ and since $l$ and $l-1$ can't be $0$ by definition one or both of the other factors has to be $0$ . But those are complex numbers (without the exponents) and so for them to be $0$ their real and imaginary part have to be $0$ . So $\sum_{i=1}^d a_i x_i=\sum_{i=1}^d b_i x_i=\sum_{i=1}^d a_i=\sum_{i=1}^d b_i=0$ . I'm not sure what to conclude from here on or how to show the reverse direction. Also please check my work so far as I think I made some mistake or false assumption. Thanks! EDIT: I wrote the sums as one and corrected my mistake when I calculated the Laplace operator. Now the forwards direction seems to be done. But I still don't know how to prove the reverse direction.","['complex-analysis', 'polynomials']"
4727095,Convergence of recursive sequence similar to harmonic series,"Given $k>1$ and sequence $\{a_n\}$ satisfies $0<a_1<1$ , and $\forall n\ge1$ : $$a_{n+1}=a_n+\frac{a_n^k}{n^k} $$ Prove: $\{a_n\}$ is convergent. I think there is a limit to $\{a_n\}$ , but I cannot find a function to fit it and use induction, such as: $$a_n<C(1-n^{1-k})$$","['calculus', 'analysis', 'sequences-and-series']"
4727097,Proof of $\sum_{k=m}^{n} \binom{n}{k} \binom{k}{m} = \binom{n}{m}2^{n-m}$,"I'm trying to prove combinatorial identities to prepare for a test. Here is one of them: $$\forall 1\le m\le n:\sum_{k=m}^{n} \binom{n}{k} \binom{k}{m} = \binom{n}{m}2^{n-m}$$ This question has been answered here Stuck in prove by induction with combinations. . (algebraically). I'd like to solve this in a combinatorial approach. LHS: Possible ways to choose $k$ elements from $n$ elements, then from those $k$ elements I choose $m$ elements. RHS: Possible ways to choose $m$ elements from $n$ elements. For the left $n-m$ elements I choose one of two options. I understand that on both sides I get subsets of the $n$ elements that has to do something with 2 options, but I'm not sure what story combines both side of the equation. Any help or suggestions would be appreciated. Thank you!","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4727119,Extinction of non-dominant species in generalized competitive Lotka-Volterra systems,"I am studying the generalized $n$ -species competitive Lotka-Volterra system where populations of species $i$ are defined by the standard differential equation: $$ \dot x_i = f_i(\mathbf{x}) := x_i \left( 1 - \sum_j a_{ij}x_j \right) $$ where all $a_{ij} \geq 0$ and $a_{ii}=1$ . I know that any asymptotic behavior can be observed in general. However, I am wondering if there exist some constraints on the competition (or interaction) matrix $\mathbf{A}$ — with elements $a_{ij}$ — such that only a single species survives. More formally, do there exist characteristics of $\mathbf{A}$ such that there is only one fixed point $\mathbf{x}^\star_k$ wherein $x_k = 1$ and $\forall j\neq k, x_j=0$ ? I have already observed that if $\mathbf{A}$ is a lower triangular matrix with all elements $1$ we obtain that only the dominant species $x_1$ will survive. We can also always reorder/relabel any system such that the species with index $1$ is the dominant one. It will the the only one to survive as we have in the fixed point $\mathbf{x}^\star$ : $$
f_1(\mathbf{x}^\star) = x_1^\star(1-x_1^\star) = 0, \;\text{thus}\; x_1^\star = 1,
$$ as we are interested in systems with $x_i(0) > 0$ . This is just logistic growth of species $1$ .
For subsequent species $x_2$ we obtain $$
f_2(\mathbf{x}^\star) = x_2^\star(1-x_2^\star-x_1^\star) = 0, \;\text{thus}\; x_2^\star = 0,
$$ and the same holds for any other $i>2$ . Is this the only way that we can make only a single (dominant) species survive if all initial abundances are positive, $x_i>0$ ? Or are there perhaps more constraints we can place on the competition matrix $\mathbf{A}$ (or its elements $a_{ij}$ ) that ensure only a single species survives in the end?","['matrices', 'constraints', 'ordinary-differential-equations', 'population-dynamics']"
4727120,Finding a closed form for $\sum_{k=1}^\infty\sum_{n=k}^\infty\left(\frac{(-1)^k}{k^3\binom{n+k}{k}\binom{n}{k}}(\frac1{n^2}-\frac1{(n+1)^2})\right)$,"Consider the sum $$\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Using the above or otherwise I need a closed form for $$\sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right) $$ I tried as follows: Denote $$S_{N,k}:=\sum_{n=k}^{N} \frac{1}{\binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ On writing the formula for $n\choose k$ we get $$S_{N,k}:=\sum_{n=k}^{N} \frac{(n-k)!}{(n+k)!}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Now $$\frac{(n-k)!}{(n+k)!}=\frac{(n-k)(n-k-1)!}{(n+k)!} $$ Using partial fractions we have $$\frac{(n-k-1)!}{(n+k)!}=\frac{1}{(n-k)...(n+k)}=\frac{1}{(2k)!(n-k)}-\frac{1}{(2k-1)!1!(n-k+1)}+\frac{1}{(2k-2)!2!(n-k+2)}-... $$ So we have $$\frac{(n-k-1)!}{(n+k)!}=\frac{1}{(2k)!}\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m}}{n-k+m}$$ Plugging the above partial fraction decomposition in $S_{N,k}$ we get $$S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}\frac{(-1)^m \binom{2k}{m} (n-k)}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ $$S_{N,k}=\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=0}^{2k}(-1)^m \binom{2k}{m} \left(1-\frac{m}{n-k+m}\right)\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ Now since $$\sum_{m=0}^{2k}(-1)^m \binom{2k}{m}=0$$ So we get $$S_{N,k}=-\frac{1}{(2k)!}\sum_{n=k}^{N} \left(\sum_{m=1}^{2k} \frac{(-1)^m\ m \ \binom{2k}{m}}{n-k+m}\right)\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right) $$ After this I am unable to simplify further. Please help me to find the sum. Edit Using the above or otherwise I need a closed form for $$\sum_{k=1}^{\infty}\sum_{n=k}^{\infty}\left(\frac{(-1)^k}{k^3 \binom{n+k}{k}\binom{n}{k}}\left(\frac{1}{n^2}-\frac{1}{(n+1)^2}\right)\right) $$","['summation', 'number-theory', 'real-analysis', 'binomial-coefficients', 'sequences-and-series']"
4727143,Calculus with little $o$ notation,"Let $f(x)$ be a smooth function for all $x \geq 0$ and suppose that it satisfies $$f'(x) = o(x^\alpha)$$ as $x \to 0$ and for some $\alpha > 0$ . Can we conclude that $$
f(x) = c + o ( x^{1+\alpha}) ?
$$ I can prove this if $\alpha = 0$ almost trivially. In this case, we have $f'(x) = o(1)$ which implies that $f'(0) = 0$ . By definition of derivative, we have $$
f'(0) = \lim_{x \to 0} \frac{f(x)-f(0)}{x} = 0 
$$ But now, by definition of little- $o$ notation, $$
f(x) - f(0) = o(x) \implies f(x) = f(0) + o(x).
$$ How do I extend such an argument for all $\alpha$ ? In fact, I will also be happy if there is a proof for $\alpha \in {\mathbb N}$ . Here is my attempt at a proof for the case $\alpha = 1$ . I think if you agree that this proof is correct, then I can easily generalize this to all integer values of $\alpha$ . For $\alpha=1$ , we have $f'(x) = o(x)$ so $$
\lim_{x \to 0} \frac{f'(x)}{x} = 0 
$$ Since $f'(0) = 0$ , we can use L'Hôpital's rule which implies $f''(0) = 0$ . We now use the following definition the second derivative $$
f''(0) = \lim_{x \to 0} \frac{2[f(x) - f(0)-xf'(0)]}{x^2}  = 0 
$$ That this definition of $f''(0)$ is true follows from L'Hôpital's rule. Now using the definition of little- $o$ notation and using $f'(0) = 0$ again, we find $$
f(x) - f(0) = o(x^2) \implies f(x) = f(0) + o(x^2) . 
$$ Assuming I haven't made any serious error above, this type of argument clearly generalizes trivially for all $\alpha \in {\mathbb N}$ .","['asymptotics', 'ordinary-differential-equations']"
4727151,"Proving that a Quadrilateral is cyclic made by intersection of perpendiculars from $B,C,D$ from $\triangle ABC$ where $D$ is a pont on $BC$ .","(sorry, couldn't come up with a better title) The question: $D$ is a point in the base $BC$ of $\triangle ABC$ and through $B, D, C$ lines are drawn perpendicular to $AB,AD,AC$ respectively meeting one another in $E,F,G$ . Prove that $A,E,F,G$ are concyclic. the fig: In quadrilateral $AEBD$ , $\angle EBA = \angle ADE = 90^{\circ}$ . Thus quadrilateral $AEBF$ is cyclic. The center of the circle circumscribing this quadrilateral is the mid point of $AE$ . In quadrilateral $ADGC$ , $\angle ADG = \angle ACG = 90^{\circ}$ . Thus it is a cyclic quadrilateral. The center of the circle circumscribing this quadrilateral is the mid point of $AG$ . In quadrilateral $ABFC$ , $\angle ABF= \angle ACF = 90^{\circ}$ . Thus it is a quadrilateral. The center of the circle circumscribing this quadrilateral is the mid point of $AF$ . How to proceed from here?","['quadrilateral', 'geometry']"
4727198,Has anyone looked at the ODE $x_{ssss} - x_{ss} x = c$ before?,"In my research, I've come across the following inhomogenous nonlinear ODE ( $c \geq 0$ is an undetermined constant): $$x_{ssss} - x_{ss} x = c$$ It has boundary conditions $$x_s(0) = x_{sss}(0) = x(1) = x_{ss}(1) = x_{sss}(1) = 0$$ I'm trying to understand if anyone has studied this equation, or if there are any analytical techniques I can use to better understand it. Thank you so much!","['nonlinear-analysis', 'ordinary-differential-equations', 'reference-request']"
4727218,Is there a combinatorial interpretation of the arithmetic derivative?,"The arithmetic derivative is a derivation on $\mathbb{Z}$ that is $1$ for all prime numbers.  On positive integers other than 1, this always returns a positive integer.  My question is, is there a way to interpret the arithmetic derivative as counting something?","['number-theory', 'derivations', 'combinatorics']"
4727265,"Is a Family a Function, or the Range of a Function?","In Halmos'book on naive set theory, Halmos introduced the idea of a ""family of sets"". He explicitly defined a family to be a function from some indexing set to an indexed set. However, whenever he talks about a family, what he seems to always be refering to is actually the range of the family. I get so confused whenever he speaks of families in his text, whether he is speaking about the function or the range of the function is very ambiguous. Does ""family"" then formally mean the indexing function itself, or perhaps the range of the function? Thank you in advance.",['elementary-set-theory']
4727296,Is every Lindelöf space Menger?,A space is Lindelöf if every open cover admits a countable subcover. A space is Menger if given a countable sequence of open covers $\mathcal U_n$ for $n<\omega$ there exist finite subcollections $\mathcal F_n\subseteq\mathcal U_n$ such that $\bigcup\{\mathcal F_n:n<\omega\}$ is a cover. Is every Lindelöf space Menger? (The answer to this question is asserted in the comments of this MO question - I'm posting here as I don't believe it's been answered on the StackExchange network directly.),"['general-topology', 'lindelof-spaces', 'compactness']"
4727314,Simplest form for $\int_0^\infty \frac{\sin(ax)}{b+x}~dx$,"Regarding the integral, $$I=\int_0^\infty \frac{\sin(ax)}{b+x}~dx,~~~~a>0\land b>0$$ Let $t=ax$ $$I=\int_0^\infty \frac{\sin(t)}{ab+t}~dt$$ Let $\theta=ab+t$ $$\begin{align} I&=\int_{ab}^\infty \frac{\sin(\theta-ab)}{\theta}~d\theta\\
\\
&=\cos(ab)\int_{ab}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta\\
\\
&=\cos(ab)\int_{0}^\infty \frac{\sin(\theta)}{\theta}~d\theta-\cos(ab)\int_{0}^{ab} \frac{\sin(\theta)}{\theta}~d\theta-\sin(ab)\int_{ab}^\infty \frac{\cos(\theta)}{\theta}~d\theta \end{align}$$ Define the sine integral and cosine integral as: $$\text{Si}(z)=\int_0^z \frac{\sin(\theta)}{\theta}d\theta,~~~~~\text{Ci}(z)=-\int_z^\infty \frac{\cos(\theta)}{\theta}d\theta$$ We get: $$\boxed{\int_0^\infty \frac{\sin(ax)}{b+x}~dx=\frac\pi2\cos(ab)-\cos(ab)\text{Si}(ab)+\sin(ab)\text{Ci}(ab)~}$$ Question: Is this the simplest form we can get for this integral?","['integration', 'improper-integrals', 'definite-integrals', 'analysis', 'real-analysis']"
4727329,Question on Increasing function,"Let $ f(x) = \frac{x^5}{5} + \frac{x^4}{4} + x^3 + \frac{kx^2}{2} + x$ be a real valued function. Then the greatest value of $k^2$ for which $f(x)$ is an increasing function $∀$ $x \in \mathbb{R}$ My approach was that : $f'(x) = x^4+x^3+3x^2+kx+1$ $f''(x) = 4x^3+3x^2+6x+k$ $f'''(x) = 12x^2+6x+6 = 6(2x^2+x+1)$ ie, $f'''(x)$ has no real roots, and is always greater than zero So, 1. the cubic $f''(x)$ , will have only one root (ie. $f'(x)$ will have a minima or a maxima) $f'(x)$ will always be greater than zero as we want the function to be increasing Net, we can say that the graph of the $f'(x)$ will be upward opening, and will touch touch the $x-$ axis only at 1 point or always stay above it with one minima. Can you please carry forward this method, or give some other solution, if possible, please try to refrain from giving solution completely based on graph making websites?","['calculus', 'derivatives', 'monotone-functions']"
4727330,"Let $f:\mathbb{R} \rightarrow \left[-\frac{1}{2}, \frac{1}{2}\right]$ given by $f(x) = \frac{x}{x^2+1}$","Let $f:\mathbb{R} \rightarrow \left[-\frac{1}{2}, \frac{1}{2}\right]$ a function given by $f(x) = \frac{x}{x^2+1}$ . Determine if $f$ is injective, surjective or bijective. Graphically, we can see that the function is not injective. I have been trying to show that if $a,b\in \mathbb{R}, f(a)=f(b) \Rightarrow a=b$ , but I can't conclude that $f$ is not injective using simple algebra. For another hand, to see the surjectivity, I need to prove that for all $y \in \left[-\frac{1}{2}, \frac{1}{2}\right]$ exists $x \in \mathbb{R}$ s.t. $f(x)=y$ . But I have problems with the algebra again. Any help?","['calculus', 'algebra-precalculus', 'discrete-mathematics', 'real-analysis']"
4727331,Prove $ f $ is identically zero if $f(0)=0$ and $|f′(x)|\le|f(x)|$,"I'm working on Omar Hijab's Introduction to Calculus and Classical Analysis
and meet this question: Let $f : \mathbb{R} → \mathbb{R}$ be differentiable with $f(0) = 0$ and $|f'(x)|≤|f(x)|$ for
all $x \in \mathbb{R}$ . Show that $f = 0$ . I know this question has been answered before, but I want to solve it following the book's solution, the solution is: Let $g(x) = f(x)^2$ . Then $g'(x) ≤ 2g(x)$ , so $e^{−2x}g(x)$ has nonpositive
derivative and $e^{−2x}g(x) ≤ g(0) = 0$ . Thus $f$ is identically zero. I'm able to get $e^{−2x}g(x)$ has nonpositive derivative, that gives: $$\frac{e^{-2x}g(x)-g(0)}{x} \le 0$$ However I can only see it follows the solution when $ x \ge 0$ , if $x \le 0$ then it becomes: $$e^{−2x}g(x) \ge g(0)$$ What did I miss? Thank you for any help!","['derivatives', 'analysis', 'real-analysis']"
4727333,A tricky system of non-linear multivariate equations,"Given $$\begin{align*}
x^2 + y^2 &= z^2\\
ax + by &= cz\\
a^2 + b^2 &= c^2\\
\end{align*}$$ how can I solve for $y$ and $z$ in terms of $x$ ? My work so far: Note that once we solve for $y$ in terms of $x$ , then $z$ will immediately follow.  So let's start with $y$ : $$z^2 = \frac{(ax + by)^2}{c^2}\\
y^2 = \frac{a^2x^2 + 2axby + b^2y^2 }{c^2} - x^2\\
(1-\frac{b^2}{c^2})y^2 - \frac{2abx}{c^2}y-(\frac{a^2}{c^2}-1)x^2 = 0.$$ To avoid getting lost, define new constants $A,B,C$ , such that $$Ay^2 + Bxy +Cx^2 = 0\\
y = \frac{-Bx \pm \sqrt{B^2x^2 - 4ACx^2}}{2A}\\
y =\frac{-B \pm \sqrt{B^2 - 4AC}}{2A}x.$$ I believe that this can be simplified further, and that in fact $B^2 = 4AC$ , but haven't been able to show that yet. Is my work so far correct? Is there a simpler approach? How do I finish this? How in general are these problems best solved?  (The approach I've taken is messy brute force - is there a more direct, simpler, or elegant approach?) Update Inspired by dxiv's hint: $$(x^2 + y^2)(a^2 + b^2) = c^2z^2 = (ax + by)^2\\
a^2x^2 + b^2x^2 + a^2y^2 + b^2y^2 = a^2x^2 + b^2y^2 + 2axby\\
b^2x^2 + a^2y^2 = 2axby\\
a^2y^2 -2abxy + b^2x^2= 0\\
y = \frac{2abx \pm \sqrt{4a^2b^2x^2 - 4a^2b^2x^2}}{2a^2}\\
y = \frac b a x.
$$","['systems-of-equations', 'multivariable-calculus', 'polynomials', 'nonlinear-system', 'algebra-precalculus']"
4727343,How to prove $\mathbb Z^2 \ast \mathbb Z$ is quasi isometric to $\mathbb Z^2 \ast \mathbb Z^2$?,"How to prove $\mathbb Z^2 \ast \mathbb Z$ is quasi isometric to $\mathbb Z^2 \ast \mathbb Z^2$ ? Here $\ast$ is the free product of groups. I am thinking of proving they are commensurable. In other words, proving they share a finite index subgroup. $\mathbb Z^2 \ast \mathbb Z$ is the fundamental group of $T \vee S^1$ , and $\mathbb Z^2 \ast \mathbb Z^2$ is fundamental group of $T \vee T$ (here $\vee$ is wedge sum). So we can also turn the problem into finding compact cover of each space such that the two covers have the same fundamental group. My problem right now is the only compact cover of a torus is torus itself, so I'm not sure how to find such a compact cover?","['product-space', 'free-product', 'covering-spaces', 'general-topology', 'algebraic-topology']"
4727347,"Whether the relation $R = \{(1,1), (2,2), (3,3), (2,1), (1,3)\}$ is anti-symmetric or not?","From my own understanding, a relation is anti-symmetric if it has $(a, b)$ but does not have $(b, a)$ while $(a, a)$ and $(b, b)$ are allowed. But in my college textbook, the relation $R = \{(1,1), (2,2), (3,3), (2,1), (1,3)\}$ is given as only reflexive and neither transitive nor anti-symmetric. I'm confused now.","['relations', 'discrete-mathematics']"
4727390,"How to evaluate $\,\lim\limits_{x \to 1}\left(x^n-1\right)\ln^m(1-x)\,$ without L'Hopital? [duplicate]","This question already has answers here : What is the workings to solve $\lim\limits_{n\rightarrow \infty} \frac{n^t}{e^n}$? (5 answers) Closed 12 months ago . This question arose as part of the evaluation of $\int_{0}^{1} x^{n-1} \ln^{m}(1-x) \, \mathrm{d}x$ using integration by parts, where we would require to show that $$
L=\lim_{x \to 1} \left( x^n-1\right)\ln^m(1-x) \qquad n,m \in \mathbb{N}
$$ Checking several cases with Wolfram Alpha, the claim seemed to hold. My attempt: \begin{align}
L & \overset{\color{blue}{x = 1-e^{-t}}}{=}\lim_{t \to \infty} \left(\left(1-e^{-t} \right)^n-1 \right)\left(-t \right)^m\\
& = \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{t^m}{e^{kt}}\\
& \overset{\color{green}{\text{L'H}}}{=}  \sum_{k=1}^{n}\binom{n}{k}(-1)^{k+m}\lim_{t \to \infty}\frac{m!}{k^me^{kt}}\\
& =0
\end{align} I was wondering if the limit in the title could be shown by a method (which could be completely different than the one I tried) that doesn't use L'Hopitals rule. The reason is that I believe another proof with L'H would be similar-ish in spirit to mine, and I was more interested in other ways of tackling the original limit altogether. Another idea I tried to make work expanding the natural log part in a series, but since the limit is on the border of the convergence region I wasn't sure if it would work. Any ideas or suggestions are welcome. Thanks!","['limits-without-lhopital', 'real-analysis', 'alternative-proof', 'calculus', 'limits']"
4727409,On the definition of Hirsch length,"I found definition of the Hirsch length in two books for two classes of groups: J.A. Hillman defined it for elementary amenable groups in his book ""The Algebraic Characterization of Geometric 4-Manifolds"" as following: The Hirsch number of $h(G)$ of an elementary amenable group is a nonnegative integer or $\infty$ , defined as follows: if $G$ is in $X_1$ let $h(G)$ be the rank of an abelian subgroup of finite index in $G$ . If $h(G)$ has been defined for all $G$ in $X_{\alpha}$ and $H$ is in $l X_{\alpha}$ let $$h(H)=l.u.b.\{ h(F)|F\leq H,\; F\in X_{\alpha}\}.$$ Finally, if $G$ is in $X_{\alpha +1}$ , so has a normal subgroup $H$ in $lX_{\alpha}$ with $G/H$ in $X_1$ , let $h(G)=h(H)+h(G/H)$ . For definitions of $X_1$ and $X_{\alpha}$ , please see the book ""Algebraic Characterization of Geometric 4-Manifolds"" p. 3. See also Section 1.5 of Hillman's later monograph Four-manifolds, geometries and knots ( arXiv ). On the other hand J.S. Robinson  defined it for polycyclic groups in his book ""A course in the theory of groups"" as follows: In a polycyclic group $G$ the number of infinite factors in a cyclic series
is independent of the series and hence is an invariant of $G$ (known as the
Hirsch length). Now I have two questions: (1) Can we define the Hirsch length for all groups generally? (2)  What is relationship between elementary amenable groups and polycyclic groups? I mean is every polycyclic group an elementary amenable group or conversely?",['group-theory']
4727413,Variance of the Norm of a Gaussian Random Vector,"If $\mathbf{X} \sim \mathcal{N}_N(\mathbf{m}, \mathbf{C})$ is an $N$ -dimensional gaussian vector, where $\mathbf{m} \in \mathbb{R}^N$ and $\mathbf{C} \in \mathbb{R}^{N \times N}$ , what is the variance of $$ Y=\|\mathbf{X}\|^2 $$ where $\|\cdot\|$ denotes the $L_2$ -norm (Euclidean norm) ? As pointed out by this question , $Y$ has a generalised chi-squared distribution and its mean can be obtained easily. However, I want to know what is its variance. Can anybody please give some help?","['statistics', 'normal-distribution', 'probability']"
4727429,"Almost uniform convergence, equivalent definition","The definition of almost uniform convergence given by Bartle is this : $f_n$ is almost uniformly convergent if for each $\delta \gt 0 $ exist $E_{\delta}$ in $X$ with $\mu(E_{\delta}) \lt \delta$ such that $f_n$ converges uniformly to $f$ in $X$ \ $E_{\delta}$ Now, I am wondering if this definition is equivalent : $f_n$ is almost uniformly convergent iff $f_n$ converges uniformly to $f$ except for a set of measure zero. That is, the definition of almost everywhere convergence (but changing pointwise convergence for uniform convergence) Are these two definitions the same thing? Edit : Now, thinking a little more about this, what I am proposing is Egoroff Theorem, right? We need to add the hypothesis $\mu(X) \lt \infty$ , correct?",['measure-theory']
4727458,Why does Wolfram say this symmetric matrix has complex eigenvalues?,"According to Wolfram , the following matrix has complex eigenvalues. Symmetric matrices have real eigenvalues, so I’m not sure what I’m failing to understand. The matrix is the shape operator of the hypersurface $f(x,y,z) = (x,y,z,xyz)$ . I don’t see why this manifold would have complex-valued principal curvatures.","['matrices', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
4727505,Convert integral $\int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx}$ to elliptic integral,"I found this integral from a friend of mine $$I = \int_0^{\frac{\sqrt3}2} {\frac{1}{{{x^{3/4}}{{\left( {1 + x} \right)}^{3/4}}}}dx} $$ Which its closed-form is : $$\frac{{2\sqrt 2 {\pi ^{3/2}}}}{{3{\Gamma ^2}( {\frac{3}{4}} )}}$$ Look at this closed form I have a feeling that the given integral can be involved with the complete elliptic integral of the first kind $K( {\frac{1}{2}} )$ . But I don't know how to convert this integral to get that result. I tried to use sub: $$\eqalign{
  & x = \frac{{1 - t}}{{1 + t}} \Rightarrow t = \frac{{1 - x}}{{1 + x}} \Rightarrow dx =  - \frac{2}{{{{\left( {1 + t} \right)}^2}}}dt  \cr 
  &  \Rightarrow I = \frac{2}{{{2^{3/4}}}}\int\limits_{7 - 4\sqrt 3 }^1 {\frac{1}{{\sqrt {1 + t} {{\left( {1 - t} \right)}^{3/4}}}}} dt,{\text{ since }}7 - 4\sqrt 3  = {\left( {2 - \sqrt 3 } \right)^2}{\text{ then let}}:t = {u^2}  \cr 
  &  \Rightarrow I = \frac{4}{{{2^{3/4}}}}\int\limits_{2 - \sqrt 3 }^1 {\frac{u}{{\sqrt {1 + {u^2}} {{\left( {1 - {u^2}} \right)}^{3/4}}}}} du \cr} $$ So I get stuck here. May I ask for help? Or give me a hint about substitution. Thank you very much. Edit #1: After using generalized binomial theorem and changing order of summation and integration, with some manipulation with the last sum, I arrived at: $$I=2\ 2^{3/4} \sqrt[8]{3} \, _2F_1\left(\frac{1}{4},\frac{3}{4};\frac{5}{4};-\frac{\sqrt{3}}{2}\right)$$ May be, there is a transformation with $_2F_1$ can link this result with the elliptic integral. I am still trying to figure out.","['integration', 'definite-integrals', 'analysis', 'calculus', 'elliptic-integrals']"
4727526,"$\frac{X_1+...+X_{n-1}-\log n }{\sqrt{\log n} }\rightarrow N(0,1)$ where $X_n\sim \operatorname{Ber}(\log \frac{n+1}{n})$","Prove : $\frac{X_1+...+X_{n-1}-\log n }{\sqrt{\log n} }\rightarrow N(0,1)$ where $X_n\sim \operatorname{Ber}({\lambda}_n)$ and $\lambda _n = \log \frac{n+1}{n}$ I tried using a similar proof to CLT with characteristic function but didnt manage to get there. Here is my attempt: $$ S_n = X_1 +...+X_n$$ $$\varphi_{\frac{S_{n-1}-\log n }{\sqrt{\log n} }}(t)=\varphi_{S_{n-1}-\log n}(\frac{t}{\sqrt{\log n}})$$ $$\varphi_{X_i-\lambda _i}=(\lambda _ie^{it}+1-\lambda _i)e^{-it\lambda _i}$$ So I get a multiplacation of $\varphi_{X_i-\lambda _i}(t/\sqrt(\log n))$ and I don't see why it should go to $e^{\frac{-t^2}{2}}$ I might add that I don't know any representation of e as a form of an infinite changing multiplication, so I don't see how this way would lead me to the solution. Thank you,","['central-limit-theorem', 'probability-theory']"
4727529,How could I have known to use the pigeonhole principle to solve this problem?,"I was tyring to solve: Let $a_1,a_2,\dots ,a_{2n}$ and $b_1,b_2,\dots ,b_{2n}$ be two sequences of integers such that for every $1\leq i \leq 2n$ : $1\leq a_i , b_i \leq n$ . Prove there exists two nonempty sub-sets of indices $I,J\subseteq [2n]$ such that $\sum_{i\in I}a_i=\sum_{j\in J}b_j$ . This problem has been solved for example in: Prove there exists two sub-sets of indices $I,J\subseteq [2n]$ such that $\sum_{i\in I}a_i=\sum_{j\in J}b_j$ The pigeonhole principle - how to solve questions like that? Need hint about with pigeonhole principle problem I understand the solutions there, but it's not clear to me how I was supposed to get there on my own and that's what I want to know. Understanding a solution does not necessarily mean that I have understood how to deal with a similar question, that is my concern. I started writing on paper visual representation the series like $a_1,a_2,\dots ,a_{2n}$ and $b_1,b_2,\dots ,b_{2n}$ and circled some of the indexes and hoped that it would somehow help me understand what are the pigeons in this case. But it didn't help, so I would appreciate some help (like tell how you think about it). Thanks for reading and hope someone can help me:)","['pigeonhole-principle', 'combinatorics']"
4727538,Does a PDF define a random variable?,"I have a simple question about the meaning of random variables in probability. I understand how one can define a probability space $(\Omega, \Sigma, P)$ and then a (real) random variable $X : \Omega \to \mathbb{R}$ which, if continuous, defines a PDF $f(x)$ such that $$P(a \leq X \leq b) = \int_a^b f(x) dx .$$ This direction is fine, but in practice I think the reasoning is in the opposite direction and I don't see how. For instance say you have a Gaussian PDF $$f(x) = \frac{1}{\sqrt{2 \pi}} e^{-x^2/2} .$$ I think it is common to say that this defines a gaussian random variable , but what exactly is this random variable? What is the probability space and what is the function $X: \Omega \to \mathbb{R}$ ? One possible answer (see e.g. this post ) is that the PDF defines a probability space with $\Omega=\mathbb{R}$ and $P([a,b]) = \int_a^b f(x) dx$ , so that $f(x)$ is the PDF of the ""identity"" random variable $X : \mathbb{R} \to \mathbb{R}$ . But then how should I understand more complex situations like convergence in distribution $X_n \to X$ ? I think $X$ and the $X_n$ 's should be defined on the same probability space. So what is this space?","['probability-theory', 'random-variables']"
4727589,Can $\operatorname{Tr} f(M)$ for matrices $M$ be upper bounded by $\sum_{i=1}^n f(M_{ii})$?,"For a convex, analytic function $f: \mathbb{R} \to \mathbb{R}$ and a real-valued, symmetric matrix $M \in \mathbb{R}^{n \times n}$ we have the bound $$\operatorname{Tr} f(M) \geq \sum_{i = 1}^n f (M_{ii}).$$ Here $f(M)$ denotes the matrix function of $f$ applied to $M$ , not the element-wise application of $f$ to $M$ . Can the trace of $f(M)$ be similarly upper bounded by the trace of the element-wise application of $f$ to $M$ ? In particular I am interested in a bound for $f(x) = x^{-1}$ applied to positive-definite matrices.","['matrices', 'trace', 'convex-analysis']"
4727599,Visualizing the curl of curl,"In some differential equations from phyiscs, e.g. in elastodynamics, terms with the curl of the curl of a vector field appear. For example $(\lambda + 2 \mu) \nabla(\nabla\cdot \mathbf{x}) - \mu \nabla\times\nabla \times \mathbf{x} = \rho \ddot{\mathbf{x}}$ . Is there a way to visualize or have an intuitive/physical understanding of what is represented by the curl of a curl of a vector field.","['multivariable-calculus', 'curl']"
4727632,Centre of mass of a semi circle using average of the co-ordinates only.,"Prior to this question, I had asked another question about how to take average of the ""x, y"" co-ordinates of a continuous function which was difficult compared to a system of discrete points. And I was met with this answer :- $$mean = \frac{1}{b-a} \int_a^b f(x)dx$$ Say we have a semi circle $$y = \sqrt{1-x^2} $$ where the radius of the semi circle is 1. And a set, S' = {(x, y): where (x, y) are all the co-ordinates contained within and on the semi-circle} How can I find the average all the y co-ordinates in set S'. I already know that the average of X co-ordinates on a semi circle is ""0"" and for Y it is $$Y = \frac{2}{π} $$ (by using the mean formula given above). However, for the Avg. of X and Y in the set S' I did not know how to apply the given formula. So I thought that all the  Coordinates in the set S' can be represented by equation for semi-circle where the radius changes from 0 to 1. so, for Avg of all X in set S' was as such :- $$X.Avg = \frac{{ \frac{1}{ π } \int_0^π (1)cos(x)dx + \frac{1}{ π } \int_0^π (0.99)cos(x)dx + .. + \frac{1}{ π } \int_0^π (0)cos(x)dx}}{ π(0.5)}$$ I Hope I properly conveyed what I was thinking. Anyways, each integral would be a zero so, $$X.Avg = 0$$ I am a bit unsure whether the denominator would be the area of the semi-circle. for Y, $$Y.avg = \frac{{ \frac{1}{ π } \int_0^π (1)sin(x)dx + \frac{1}{ π } \int_0^π (0.99)sin(x)dx + .. + \frac{1}{ π } \int_0^π (0)sin(x)dx}}{ π(0.5)}$$ this comes out to be $$ Y.Avg = \frac{2}{π^2} $$ which I know is wrong, because my physics teacher had already derived it $$ Y.Avg = \frac{4}{3π} $$ Help.","['average', 'statistics', 'geometry']"
4727659,Finding the solutions of $xp^2-2py+4x=0$,"I am trying to find the general solution, $p$ and $c$ discriminant and hence the singular solutions (if any) of the following D.E. where $p=\mathrm dy/\mathrm dx$ . $$xp^2-2py+4x=0$$ The obvious thing to do is isolate $p$ using the quadratic formula. The $p$ -discriminant is $4y^2-16x^2$ so I suspect one or both of $y=\pm 2x$ could be the envelope of the general solution (i.e., singular soln. of our D.E.). $$p=\frac{y}{x}\pm \sqrt{\left(\frac yx\right)^2-4}$$ Substitute $y=tx$ to get: $$x\cdot\frac{\mathrm dt}{\mathrm dx}=\pm\sqrt{t^2-4}$$ Integrating: $$\ln|cx|=\pm \ln\left|t+\sqrt{t^2-4}\right|$$ where $c$ is the parameter of the general soln.
Now we have two cases: $$cx=t+\sqrt{t^2-4}\\ \implies cx^2=y+\sqrt{y^2-4x^2}$$ $$cx=\frac{1}{t+\sqrt{t^2-4}}=\frac{t-\sqrt{t^2-4}}4\\ \implies cx^2=y-\sqrt{y^2-4x^2}\\ \text{where } 4c\mapsto c$$ I am unable to decide which one to go with for finding the $c$ -discriminant. Is there any other way to solve this D.E. instead of isolating $p$ by quadratic formula? Update : I realized I had made a lot of silly mistakes. I've fixed them. Now it's clear that collectively, the two cases yield (take $c/2\mapsto c$ ): $$c=\frac{y\pm\sqrt{y^2-4x^2}}{2x^2}\\ =\frac{-(-y)\pm\sqrt{(-y)^2-4x^2\cdot 1}}{2x^2}$$ Using the quadratic formula (in reverse), the $c$ -discriminant turns out to be $y^2-4x^2$ . This confirms that $y=\pm 2x$ are indeed envelops and hence, singular solutions. $$\phi(x,y,c)=x^2c^2-yc+1=0$$ See this Desmos slideshow (graph of the envelope and the family $\phi$ ). Alternate approach: Isolate $y$ in the original D.E. $$y=\frac{px}{2}+\frac{2x}{p}$$ This seems a little off from the Clairut's form i.e., $y=px+f(p)$ . Substitute $u=x^2$ . Denote $\mathrm dy/\mathrm du$ by $m$ . So differentiating w.r.t. $y$ , we have $\frac{1}{m}=\frac{2x}p$ i.e., $p=2mx$ . $$y=um+\frac{1}{m}$$ This is in Clairut's form i.e., $y=um+f(m)$ so the general soln. is given by $y=uc+f(c)$ i.e., $$y=uc+\frac{1}{c}=cx^2+\frac{1}{c}$$ And the singular soln. can be determined by eliminating $m$ from the D.E. using $u+f'(m)=0$ i.e., $m=\pm1/\sqrt u$ . $$y=\pm 2\sqrt u=\pm 2x$$","['discriminant', 'ordinary-differential-equations']"
4727678,Brezis' exercise 6.15.2: $\|I + \lambda (T-\lambda I)^{-1}\| \le \frac{\|T\|}{|\lambda|- \|T\|}$,"I'm trying to solve an exercise in Brezis' Functional Analysis , i.e., Let $(E, |\cdot|)$ be a real Banach space. Let $T \in \mathcal L(E)$ , i.e., $T:E \to E$ is a bounded linear operator. Let $I:E \to E$ be the identity map. Let $\rho(T)$ be the resolvent set of $T$ . Let $\sigma(T)$ be the spectrum of $T$ , i.e., $\sigma(T) := \mathbb R \setminus \rho(T)$ . Let $\lambda \in \mathbb R$ such that $\|T\| < |\lambda|$ . Prove that $$
\|I + \lambda (T-\lambda I)^{-1}\| \le \frac{\|T\|}{|\lambda|- \|T\|}.
$$ Let $\lambda \in \rho(T)$ . Check that $(T-\lambda I)^{-1} T = T (T-\lambda I)^{-1}$ and prove that $$
d(\lambda, \sigma(T)) := \inf_{h \in \sigma(T)} |\lambda-h| \ge \frac{1}{\| (T-\lambda I)^{-1} \|}.
$$ There are possibly subtle mistakes that I could not recognize in below attempt. Could you please have a check on it? I'm also happy to see other approaches. We need a result from the same book, i.e., Exercise 6.14 Assume that $\|T\| < 1$ . Prove that $(I-T)$ is bijective and that $$
\|(I-T)^{-1}\| \le \frac{1}{1- \|T\|}.
$$ Let $S_n := I+T+\cdots+T^{n-1}$ . Prove that $$
\|S_n-(I-T)^{-1}\| \le \frac{\|T\|^n}{1- \|T\|}.
$$ Clearly, $\lambda \neq 0$ . Let $K:= \frac{T}{\lambda}$ . Then $\|K\| < 1$ . By exercise 6.14.2 , we get $$
\|I-(I-K)^{-1}\| \le \frac{\|K\|}{1- \|K\|}.
$$ The claim then follows. 2. Let $A := (T-\lambda I)^{-1} T$ and $B:=T (T-\lambda I)^{-1}$ . Then $(T-\lambda I) A= T$ . Because $(T-\lambda I)$ is bijective, it suffices to prove that $(T-\lambda I) B= T$ . First, we have $$
\begin{align*}
B - \lambda I(T-\lambda I)^{-1} &= T (T-\lambda I)^{-1} - \lambda I (T-\lambda I)^{-1} \\
&= (T-\lambda I) (T-\lambda I)^{-1}= I.
\end{align*}
$$ Then $B= I + \lambda I(T-\lambda I)^{-1}$ and thus $$
\begin{align*}
(T-\lambda I) B &= (T-\lambda I)  + (T-\lambda I) \lambda I(T-\lambda I)^{-1} \\
&= (T-\lambda I) + \lambda I= T.
\end{align*}
$$ By Proposition 6.7 (in the same book), $\sigma(T)$ is compact and $\sigma(T) \subset [-\|T\|, \|T\|]$ . Then $\rho(T)$ is open. Let $r:= \frac{1}{\| (T-\lambda I)^{-1} \|}$ . As in the proof of Proposition 6.7 , $B(\lambda, r) :=\{h \in \mathbb R : |h-\lambda| < r\} \subset \rho(T)$ . The claim then follows.","['banach-spaces', 'operator-theory', 'solution-verification', 'functional-analysis', 'spectral-theory']"
4727689,"Show that in any map $f:X \rightarrow Y$, the map $F:X \rightarrow X \times Y$, defined by $x\mapsto^{F} (x, f(x))$ is injective.","My attempt: Let $x_1, x_2 \in X$ , such that $x_1 \ne x_2$ . Then $F(x_1)=(x_1, f(x_1))$ and $F(x_2)=(x_2, f(x_2))$ . Since $x_1 \ne x_2$ , $F(x_1) \ne F(x_2)$ , irrespective of whether $f(x_1) = f(x_2)$ or not, so F is always injective.","['functions', 'solution-verification']"
4727702,Surprising approximation of exponential series?,"Consider the following expression $$
y_j= \sum_{k=0}^{L}  \frac{e^{-\sum_{i=-k}^k(k-|i|)x_{j+i}}-e^{-\sum_{i=-k}^k(k+1-|i|)x_{j+i}}}{\sum_{i=-k}^k x_{j+i}}\tag{1}
$$ for $1\leq j \leq L$ . Given smooth periodic data $\{y_j\}$ , I would like to find an analytical approximation to the periodic array $\{x_j\}$ . My thoughts: In general, this seems hard, but despite the relatively complex nature of (1), surprisingly some simulations give me hope that something could be said about approximating an inverse formula, in specific cases and for large $L$ . For example, when $y_j=y$ , for all $j$ , as shown here , we have $$
y\simeq \frac12\sqrt{\frac{\pi}{x}}\hspace{2mm}\Leftrightarrow\hspace{2mm} x\simeq\frac{\pi}{4}y^{-2}\tag{2}
$$ where, necessarily, $x=x_j$ for all $j$ . Interestingly enough, this simple approximation works reasonably well for non-constant and smooth $y_j\in[0,50]$ , as the following plots show Here, we first plotted an initial sequence $\{x_j\}$ of 400 points, given by $x_j=10^{-4}x e^{\sin x}$ , followed by its transformation via (1) and a second transformation on the obtained $y_j$ by (2), to recover an estimation for $x_j$ . Comparing the first and third plots suggests that a transformation simply given by (2), on data $\{y_j\}$ , could be enough to capture the unknown $\{x_j\}$ . Indeed, by overlapping the two sets, we have While we obtain a relatively similar function for $x_j$ , estimate (2) drastically fails when the scale of $y_j$ changes. For example, if we take $x_j$ corresponding to $5y_j$ , we get which produces not only a significantly worse estimate for $x_j$ , but also for $y_j$ , as seen in the second plot, compared to the previous one Nonetheless, I would anticipate that the scaling issue could be potentially fixed by amplifying the behavior of $x_j$ , perhaps through some transformation based on its second derivative (and/or a sigmoidal function of it?). I trust there could be a way to improve the estimate from equation (2) to be applied to large-scale $y_j$ , but I am struggling to understand the main mechanism through which that could be achieved. Any ideas? Edit 1 (some observations): Let $R(\{x_j\}) := y_j$ as defined in (1). A somewhat naive way of inverting (1) is to take $x_j=R(\{y_j\})$ or, more generally, $x_j=R(\{f(y_j)\})$ for some function $f$ . Interestingly, when $f(y)=y^2$ , $R(\{y_j^2\})$ becomes very similar to the approximation given by (2), as seen in the following plots Still far from the desired function but, despite being relatively surprising, I believe this is simply a consequence of the fact that, when $y_j$ is large, $R(\{y_j\})\simeq 1/y_j$ , but there might be room for further conclusions. Perhaps defining $f$ accordingly could give us a better inverse? Edit 2 (numerical approaches): While the analytical approach is the main question here, gradient descent and BFGS algorithms are being attempted here , which might hint at overall dynamics that could potentially motivate the theoretical exercise. Through gradient descent, a relatively fast estimate can be found here , for some sample data, but it yields non-negative values in $\{x_j\}$ . For example, gradient descent yields the following $\{x_j\}$ distribution convergence, where blue is the initial guess Ideally, I would like to guarantee $x_j>0, \forall j$ , but I would be more than happy if the transform seen here could be, at least locally, achieved, as the overall monotonicity seems preserved and it ""suggests"" some relatively simple rescaling mechanism. The uniqueness question follows: How many solutions, in $\mathbb{R}$ , does (1) have? Naturally, this is extra, but I thought it could be relevant to share.","['approximation', 'inverse-function', 'sequences-and-series', 'limits', 'exponential-function']"
4727704,Multiplicity of intersection of $y^2=x^3$ and $x^2=y^3$ at the origin,"Those curves intersect at the origin with multiplicity 4, if I did everything correctly. In fact, parametrizing by $t \mapsto (t^2,t^3)$ the first curve and plugging into the second, yields $t^4=t^9$ , i.e. $t^4(t^5-1)=0$ and one can see the multiplicity of $t=0$ being $4$ . However, if I try the algebraic geometry approach, by calculating locale Rings, I get a different result and I cannot figure out where I'm making a mistake. First, $$\mathbb{K}[x,y]/(y^2-x^3) \cong \mathbb{K}[t^2,t^3]$$ Then, $$\mathbb{K}[x,y]/(y^2-x^3,x^2-y^3) \cong$$ $$\mathbb{K}[t^2,t^3]/(t^4-t^9) = \mathbb{K}[t^2,t^3]/(t^4(1-t^5)) $$ Now localizing at the origin, $1-x^5$ becomes a unit and thus the final localized ring is: $$\mathbb{K}[x,y]_P/(y^2-x^3,x^2-y^3) \cong$$ $$\mathbb{K}[t^2,t^3]/(t^4) = \{a+bt^2+ct^3\}$$ But this is a vector space over $\mathbb{K}$ of dimension 3, instead of 4. Where is my error?","['localization', 'algebraic-geometry', 'intersection-theory', 'local-rings']"
4727707,How to evaluate the limit $\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \quad (a \in \mathbb{C})$,"I want to evaluate this limit expression. I have this problem when I explored a corollary of infinity q-binomial theorem. $
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x} \\
x \in {\mathbb{R}} \quad , \quad \left|x\right|<1 \quad , \quad a \in \mathbb{C}
$ When $a\in{\mathbb{R}^{+}}$ , it is clear that $
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{+\infty} \dfrac{1-{q}^{a+k}x}{1-{q}^{k}x}=
\lim\limits_{{q}\to {1}^{-}} \prod\limits_{k=0}^{a-1} \dfrac{1}{1-{q}^{k}x}=
\dfrac{1}{{(1-x)}^{a}}
$ Now I need to extend this conclusion to the broader situation that $a \in \mathbb{C}$ . Here the trouble is that I don't know how to do with it. So I ask for help here.","['infinite-product', 'limits', 'complex-numbers', 'analysis']"
4727749,"A question about solutions of O.D.E $(2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0,\ x\geq 0$","I solved firstly the O.D.E $$(*)\ \ \ \ (2x^2+2y^2+x)dx+(x^2+y^2+y)dy=0, x\geq 0, $$ by using the Integrating Factor $\rho (x,y)=\frac{1}{x^2+y^2}$ . Consequently, I proved that every solution $y$ of this equation is given by form: $$2x+\frac{1}{2}\ln(x^2+y^2)+y=c,\ x\geq 0 $$ with $c$ be a random parameter. $\bullet\ $ Question 1: Can I tell with certainty, that there exist a solution of $(*)$ such that $y'(x)\leq 0, \ \forall\ x\geq 0$ ? $\bullet\ $ Question 2: Can I tell with certainty, that for every solution $y$ of $(*)$ holds $\displaystyle\lim_{x\to+\infty}y(x)=-\infty$ ? About the 1st question, we can see that for every solution $y$ of $(*)$ we can rewrite $(*)$ as $y'=-\dfrac{2x^2+2y^2+x}{x^2+y^2+y},\ x\geq 0$ but i think it's not clearly that $y'(x)\leq 0, \ \forall\ x\geq 0$ About the 2nd question, I tried by contradiction by taking the limit as $x\to+\infty$ to the last equation. However, that didn't end up as I expected. Any help or suggestion it would be helpful, please. Thanks a lot.","['calculus', 'functions', 'ordinary-differential-equations']"
4727803,When is the zero set of a multivariate $p$-adic power series algebraic?,"Let $f = f(z_1, \dots, z_n)$ be a power series in $n$ variables with coefficients in the $p$ -adic integers $\mathbb{Z}_p$ . Let $g(z_1, \dots, z_n) = f(pz_1, \dots, pz_n)$ , so that $g$ converges on all of $\mathbb{Z}_p^n$ . Under what conditions is the zero set of $g$ algebraic, meaning that it is cut out by polynomial equations? So far, I know that if $n=1$ , then the answer is always by the Weierstrass preparation theorem. This result allows one to factor $g(z_1)$ as $g(z_1) = h(z_1)u(z_1)$ , where $h$ is a polynomial and $u$ is a nowhere vanishing power series with coefficients in $\mathbb{Z}_p$ . But when $n > 1$ , the Weierstrass preparation theorem in multiple variables gives only a factorization $g(z_1,\dots, z_n) = h(z_1,\dots,z_n)u(z_1,\dots,z_n)$ , where $h$ is a polynomial in $z_1$ with coefficients in the power series ring $\mathbb{Z}_p[[z_2,\dots,z_n]]$ . This is not immediately enough to deduce that the zero locus of $g$ is cut out by polynomials. Can anything further be said when $n>1$ ? What about when $n=2$ ?","['p-adic-number-theory', 'analyticity', 'rigid-analytic-spaces', 'algebraic-geometry', 'power-series']"
4727809,Geometric construction of a triangle given 2 lines and a point,"Lines $t$ and $s$ and point $P$ are given. Line $t$ is a perpendicular from the centroid of triangle $ABC$ to $BC$ ,  line $s$ is a bisector of angle $\angle ABC$ , and $P$ is the midpoint of $BC$ . Construct  triangle $ABC$ . I constructed a perpendicular from $P$ to $t$ and its intersection with the line s is point $B$ . Point $C$ is the centrosymmetric image of point $B$ with respect to $P$ , and point $A$ lies on the line $BP'$ , where $P'$ is the axisymmetric image of point $P$ with respect to $s$ .  How do I find point $A$ ?","['euclidean-geometry', 'triangles', 'geometry', 'geometric-construction']"
4727833,What kinds of selection principles hold for Fortissimo space?,"Note: This question is being posted primarily as a reference to include for https://topology.pi-base.org/ following the guidelines for references as spelled out at https://github.com/pi-base/data/blob/master/CONTRIBUTING.md . A Fortissimo space is defined as follows, as in https://en.wikipedia.org/wiki/Counterexamples_in_Topology : let $Y$ be an uncountable set and suppose $\infty \not\in Y$ . Then let $X = Y \cup \{ \infty \}$ have the topology $\{ U \subseteq X : U \subseteq Y \vee (\infty \in U \wedge X \setminus U \text{ is countable} \}.$ When the uncountable set $Y$ is the continuum, this is https://topology.pi-base.org/spaces/S000022 . A space is Rothberger https://en.wikipedia.org/wiki/Rothberger_space if it satisfies the selection principle $\mathsf S_1(\mathcal O, \mathcal O)$ : for every sequence $\langle \mathscr U_n : n \in \omega \rangle$ of open covers of $X$ , there is a selection $U_n \in \mathscr U_n$ for each $n \in \omega$ so that $\{ U_n : n \in \omega \}$ is a cover of $X$ . It is easy to see that any Fortissimo space is Rothberger since you take the initial selection to be any neighborhood of $\infty$ . Then you run through an enumeration of the countable complement of that neighborhood. There are games naturally corresponding to selection principles as discussed in the Wikipedia article on Rothberger spaces linked above. Indeed, the procedure described above is a winning strategy for the second player in the Rothberger game. One can also define stronger strategies like Markov strategies for the second player which only depend on the current play by the first player, and no other previous information. One can also investigate other cover types like $\omega$ -cover and $k$ -covers. $\omega$ -covers (resp. $k$ -covers) of a space $X$ are open covers so that every finite (resp. compact) subset of $X$ is contained in a member of the cover and $X$ itself is not a member of the cover. Since $X$ is anticompact ( https://topology.pi-base.org/properties/P000136 ), the $\omega$ -covers and $k$ -covers coincide. Can the discussion above be improved to Markov strategies and/or include $\omega$ -covers?","['general-topology', 'infinite-games']"
4727874,What’s the simplest way to prove that the polynomial $f(z)=z^4 + z^3 + z^2 +2z +3$ has no real zero?,"I have managed to write the polynomial mentioned in the title as a sum of squares as follows: $$ z^4 + z^3 + z^2 +2z +3 = \left( z^2 + \frac12 z - \frac18 \right)^2 + \left( z + \frac{17}{16} \right)^2 + \frac{475}{256} $$ Is this the simplest way to prove that it has no real zero, or is there another way that is simpler? Update. The first comment below, written by Piquito, seems interesting, but I don't understand why the inequality would be true for all real z. Can someone explain it?","['roots', 'factoring', 'polynomials', 'algebra-precalculus', 'quartics']"
4727900,Inverse Mellin Transform $(ix)^{-s}$,"Consider the inverse Mellin transform of the function $(ix)^{-s}$ for $x>0$ . This is: $$
\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty} (i x)^{-s} ds
$$ What does this evaluate to? I believe that if you simply had $x^{-s}$ then the result would be $\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty} x^{-s} ds = \delta(x-1)$ . However what happens in this case where there a factor of $i$ ?","['complex-analysis', 'complex-integration', 'mellin-transform']"
4727903,Is every second countable space k-Lindelöf?,"Every second countable space must be Lindelöf – given a cover, decompose each open set into basic open sets. This is a countable cover that refines the original cover, so by picking arbitrary open sets from the original cover containing each of the countably-many basic open sets in our refinement, we obtain a countable subcover. A space is $k$ -Lindelöf if every $k$ -cover (each compact is contained in a single open set) admits a countable $k$ -subcover. All separable metrizable spaces must be second countable and must be $k$ -Lindelöf (due to a theorem later deprecated in favor of this based on this post). There are several spaces that are $k$ -Lindelöf but not second countable. The pi-Base does not know any second countable space that's not $k$ -Lindelöf. Is every second countable space in fact $k$ -Lindelöf?","['general-topology', 'lindelof-spaces']"
4728007,Evaluate $\sqrt{x+\sqrt{{x^2}+\sqrt{{x^3}+\sqrt{{x^4}...}}}}$,"I recently became fascinated by infinite nested radicals, first drawn attention to me from a question in my textbook about the value of $\sqrt{1+\sqrt{{1}+\sqrt{{1}+\sqrt{{1}...}}}}$ which turned out to be $\phi$ when I worked it out, a rather beautiful result. I then tried to find a formula to evaluate the general case $$\sqrt{x+\sqrt{{x}+\sqrt{{x}+\sqrt{{x}...}}}}$$ which I succeeded in; it can be evaluated as $$\frac{1+\sqrt{1+4x}}{2}$$ Multiplying the nested radical which was equal to $\phi$ by $x$ produces the following nested radical: $$\sqrt{{x^2}+\sqrt{{x^4}+\sqrt{{x^8}+\sqrt{{x^{16}}...}}}}$$ so this is equal to $x\left(\frac{1+\sqrt5}{2}\right)$ . However, I have tried and failed to find the value of the following infinite square root: $$\sqrt{x+\sqrt{{x^2}+\sqrt{{x^3}+\sqrt{{x^4}...}}}}$$","['nested-radicals', 'convergence-divergence', 'radicals', 'sequences-and-series']"
4728025,"Log-tangent integral using exponential generating function $\int_0^{\pi/2}x\log^n(\tan x)\,dx$","$$I_n=\int_0^{\pi/2}x\log^n(\tan x)\ dx,\quad n\in\mathbb{Z}$$ I want to evaluate this log-tangent integral using exponential generating functions . My attempt is below. Define, $$G(t)=\sum_{n=0}^\infty\frac{I_n}{n!}t^n=\int_0^{\pi/2}x\sum_{n=0}^\infty\frac{(t\log(\tan x))^n}{n!}\ dx=\int_0^{\pi/2}x\tan^{t}(x)\ dx$$ rewriting $x$ as $\arcsin(\sin(x))$ and expanding by its Maclaurin series, $$\int_0^{\pi/2}x\tan^{t}(x)\ dx=\int_0^{\pi/2}\arcsin(\sin(x))\frac{\sin^t(x)}{\cos^{t}(x)}\ dx=\int_0^{\pi/2}\sum_{n=0}^\infty\binom{2n}{n}\frac{\sin^{2n+1}(x)}{4^n(2n+1)}\frac{\sin^t(x)}{\cos^{t}(x)}\ dx\\
=\sum_{n=0}^\infty\binom{2n}{n}\frac{4^{-n}}{(2n+1)}\int_0^{\pi/2}\sin^{2n+t+1}(x)\cos^{-t}(x)\ dx=\sum_{n=0}^\infty\binom{2n}{n}\frac{4^{-n}}{2(2n+1)}B\left(\frac{2n+t+2}{2},\frac{1-t}{2}\right)$$ where $B(m,n)$ is the Beta function, $$B(m,n)=\frac{\Gamma(m)\Gamma(n)}{\Gamma(m+n)}$$ hence, $$G(t)=\sum_{n=0}^\infty\binom{2n}{n}\frac{4^{-n}}{2(2n+1)}\frac{\Gamma\left(n+t/2+1\right)\Gamma(1/2-t/2)}{\Gamma(n+3/2)}.$$ Now I need to extract the coefficient of $t^n/n!$ as, $$I_n=[t^n/n!]G(t)$$ but I am having difficulties doing so. If I can get a closed form for $G(t)$ I may be able to take its logarithmic derivative and use the Cauchy product, but I don't have a closed form either. Any ideas? Thanks in advance.","['integration', 'summation', 'definite-integrals', 'special-functions', 'generating-functions']"
4728035,Use series method to obtain a general solution of the Cauchy-Euler Equation $x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$,"Use series method to obtain a general  solution of the Cauchy-Euler Equation $$x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$$ To be very honest, I didn't understand the language of the question for it says, ""series method"". I assumed that it intended to say, ""Find a power series solution of the Cauchy-Euler Equation $$x^2\frac{d^2y}{dx^2}-x\frac{dy}{dx}+\frac 34y=0.$$ "" But this is again ambiguous, for ut specifies, no point(s) around which I am supposed to find a power series solution. I assumed that, they intended to find tge power series solution about the regular singular point $x=0.$ As, $x=0$ was a regular singular point, I tried applying the method of Frobenius to find a power series solution about $0$ . I proceeded as: Let $y=\sum_{n=0}^{\infty}c_nx^{n+r}$ be a solution to the equation where, $r$ is a constant that nay be determined with the assumption that $c_0\neq 0.$ This means, $y'=\sum_{n=0}^{\infty}(n+r)c_nx^{n+r-1}$ and $y''=\sum_{n=0}^{\infty}(n+r)(n+r-1)c_nx^{n+r-2}.$ Substituting these in the original equation we find, $$\sum_{n=0}^{\infty}\big [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_nx^{n+r}=0.$$ Hence, we obtain the recurrence, relation $ [(n+r)(n+r-1)-(n+r)+\frac 34\big ]c_n=0$ for all $n\geq 0.$ If $n=0$ then, $(4r^2-8r+3)c_0=0$ and as, $c_0\neq 0$ so, $4r^2-8r+3=0$ is actually the indicial equation. We find, $r_1=\frac 32,r_2=\frac 12.$ Now, if $r=r_1$ then, from the recurrence relation we have(under the value of $r=r_1$ ) $$c_n(n+n^2)=0$$ for all $n\geq 1.$ But here is the problem, as this means $c_n=0,\forall n\geq 1.$ I dont know, where I went wrong. Any help will be greatly appreciated. I feel that the problem is not in work, really. The thing is, the particular integral of this ODE will simply contain just two terms, but then what is meant by the series method?",['ordinary-differential-equations']
4728041,"Knowing concentration, can we say one random variable is bounded by another?","If we have two random variables $X, Y$ , such that for any $\varepsilon \in (0, 1]$ we have $$\mathbb{P} \left( | X - Y | \geq \varepsilon \right) \leq C $$ for some constant $C > 0$ , can we say that in particular $|X|$ is bounded by $Z := |Y| + 1$ with probability at least $1-C$ ? This seems intuitive, but I can't see the formal argument here. I always find it difficult to manipulate these kinds of expressions.","['probability-theory', 'probability', 'random-variables']"
4728059,Finding the minimum of $f(x)=3x^2+4y^2+4xy-11x-6y$ where $x$ and $y$ are reals.,"Question: Find the minimum of $f(x, y)=3x^2+4y^2+4xy-11x-6y$ where $x$ and $y$ are reals and characterise the instances of equality. My approach: My first thought was to factorise this into a square. However, that is clearly not possible. AM-GM and Jenson's does not apply here  due to the $-11x-6y$ . Cauchy-Schwarz and other inequalities do not seem to apply either. According to dcode.fr , the minimum is $-\dfrac{41}{4}$ and equality is achieved when $x=2$ and $y=-\frac{1}{4}$ , but I do not know how to prove this. Any help will be appreciated!","['inequality', 'real-analysis', 'polynomials', 'algebra-precalculus', 'quadratics']"
4728113,"Closed form for Bessel type integral $\int_0^\infty J_0 (rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x$","Following this link , Zacky calculated the integral $$\int_0^\infty x{J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}}\, {\rm d}x=\frac{e^{-a\sqrt{r^2+z^2}}}{\sqrt{r^2+z^2}} \,.$$ It is a special case of the more general Bessel type integrals found in Watson . I'm wondering if also a closed form expression for $$\int_0^\infty {J}_0(rx) \,\frac{e^{-z\sqrt{x^2+a^2}}}{\sqrt{x^2+a^2}} \, {\rm d}x$$ can be found. I tried the method Zacky applied, but it doesn't seem to work, because the appearance of error-functions complicate matters. I also tried the approach from Watson, but only converted the improper integral into a definite integral. Equation 2f (Watson, p.416) gives, while omitting the factor $t^{\mu+1}$ , for $\nu=1/2$ and $\mu=0$ $$\sqrt{\frac{\pi}{2a}}\int_0^\infty J_0(bt) \, \frac{e^{-a\sqrt{t^2+z^2}}}{\sqrt{t^2+z^2}} \, {\rm d}t = \frac12 \int_0^\infty {\rm d}u \int_0^\infty {\rm d}t \, J_0(bt) \, u^{-3/2}\, e^{-\frac{a}{2}\left(u+\frac{t^2+z^2}{u}\right)} \\
=\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}u}{u} \, I_0\left(\frac{b^2u}{4a}\right) \, e^{-\frac{(2a^2+b^2)u}{4a}-\frac{z^2a}{2u}}
$$ where the $t$ -integral can be found here . Substituting $v=\frac{b^2u}{4a}$ then gives $$=\frac14 \sqrt{\frac{2\pi}{a}} \int_0^\infty \frac{{\rm d}v}{v} \, I_0(v) \, e^{-\frac{(2a^2+b^2)v}{b^2}-\frac{b^2z^2}{8v}} \, .$$ Using the representation $$I_0(v)=\frac1\pi \int_0^\pi {\rm d}\theta \, e^{-v\cos(\theta)}$$ finally yields $$=\frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}v}{v} \, e^{-\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}-\frac{b^2z^2}{8v}} \\
\stackrel{w=\frac{\left(a^2+b^2\cos^2(\theta/2)\right)2v}{b^2}}{=} \frac{1}{\sqrt{8\pi a}} \int_0^\pi {\rm d}\theta \int_0^\infty \frac{{\rm d}w}{w} \, e^{-w-\frac{\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)^2}{4w}} \\
= \frac{1}{\sqrt{2\pi a}} \int_0^\pi {\rm d}\theta \, K_0\left(z\sqrt{a^2+b^2\cos^2(\theta/2)}\right)$$ resorting to this integral representation of the Bessel-K-function .","['integration', 'improper-integrals', 'analysis', 'real-analysis']"
4728125,"Prove if $\frac{a - b}{a + b} + \frac{b - c}{b + c} + \frac{c - a}{c + a} = 0$, then one of the terms is equal to $0$.","The problem: Numbers $a$ , $b$ , and $c$ are such, that $\frac{a - b}{a + b} + \frac{b - c}{b + c} + \frac{c - a}{c + a} = 0$ . Prove that one of the terms is equal to $0$ . The textbook I'm using provides a hint, that unfortunately haven't brought me any closer to the solution yet: Prove that $\frac{a - b}{a + b} + \frac{b - c}{b + c} + \frac{c - a}{c + a} = -\frac{(a - b)(b - c)(c - a)}{(a + b)(b + c)(c + a)}$ Below are several outcomes that I was getting through the process of simplifying the initial equation, trying to achieve the result from the hint. After expanding the equation: $\frac{a}{{a + b}} - \frac{b}{{a + b}} + \frac{b}{{b + c}} - \frac{c}{{b + c}} + \frac{c}{{c + a}} - \frac{a}{{c + a}} \implies$ I'm resulting in this equation: $\implies \frac{a(b - c)(b + c) \space - \space b(c - a)(c + a) \space - \space c(a - b)(a + b)}{(a + b)(c + a)(b + c)}$ or in this one: $\implies \frac{ab(a + b) - bc(c - b) - ac(a + c)}{(a + b)(c + a)(b + c)}$ Another way I've tried is a direct simplification with a common denominator: $\frac{{(a - b)(b + c)(c + a) + (b - c)(a + b)(c + a) + (c - a)(a + b)(b + c)}}{{(a + b)(b + c)(c + a)}}$ Unfortunately, none of the approaches have led me to a conclusive proof. I would greatly appreciate any guidance, insights, or alternative approaches that could help me prove the statement. * This problem is originally taken from a school's mathematics olympiad competition in my country and it has been challenging me for already a while.","['contest-math', 'fractions', 'algebra-precalculus']"
4728165,Is the Rényi entropy a continuous function with respect to the parameter $\alpha$?,"The Rényi entropy of order $\alpha$ , where $\alpha > 0$ and $\alpha \neq 1$ , is defined as $$
\mathrm{H}_\alpha(X)=\frac{1}{1-\alpha} \log \left(\sum_{i=1}^n p_i^\alpha\right) 
$$ Here, $X$ is a discrete random variable with possible outcomes in the set $\mathcal{A}=\left\{x_1, x_2, \ldots, x_n\right\}$ and corresponding probabilities $p_i \doteq \operatorname{Pr}\left(X=x_i\right)$ . I would like to know if $\mathrm{H}_\alpha(X)$ , considered as a function of $\alpha$ , is a continuous function. I think that, if the probability distribution function $p_i$ is $\neq0$ for all $i$ , then $\mathrm{H}_\alpha(X)$ will be a continuous function of $\alpha$ . This is because the sum and logarithm operations in the entropy formula are continuous functions, and the composition of continuous functions remains continuous. Is it true also if $p_i=0$ for some $i$ ?","['renyi-entropy', 'entropy', 'probability', 'information-theory']"
4728169,Solving this problem algebraically: $81^{\sin^{2}(x)} + 81^{\cos^{2}(x)} = 30 $,"I saw this problem on an instagram feed $$81^{\sin^{2}(x)} + 81^{\cos^{2}(x)} = 30$$ solve for x And I saw that most people had solutions where you plug in x = 30 degrees or $\frac{\pi}{6}$ and the like. I get that if you assume that the two numbers added are integers, you can sort of back-calculate and say that $81^{\frac{1}{4}} = 3 $ and $81^{\frac{3}{4}} = 27$ so that gets you an answer that works. But I was trying to come up with a good algebraic way to solve it -- I thought of using logs, but I don't think you can really do that here. I did see on method that says let $u = 81^{\cos^{2}(x)}$ and you'd get, since $\cos^{2}(x) = 1 - \sin^{2}(x)$ $81^{1-\cos^2{x}} = \frac{81}{81^{\cos^2{x}}}$ So we can substitute in $\frac{81}{u} + u = 30$ $81 + u^2 = 30u$ $u^2 - 30u + 81 = 0$ Solve this by factoring $(u - 3)(u - 27) = 0$ so solutions are at 3 and 27 that means $ u = 81^{cos^{2}(x)} = 3 $ therefore $\cos(x) = \frac{\sqrt{3}}{2}$ And it all works. But now that I seem to have figured this out I'd be curious as to why it works; or rather, is there some other method that would make sense? Again I thought of logs or something, but I feel like this method was ""cheating"" somehow...",['algebra-precalculus']
4728205,limit point and isolated point example.,"I'm trying to understand limit point and isolated point conceptually with example. Do you think my approach is correct. Example for limit point: Let's consider an example in the real number line, which is a one-dimensional version of $\mathbb{R}$ . Let's consider the set $E = \{\frac{1}{n} : n \in \mathbb{N}\}$ . This set includes elements like 1, 1/2, 1/3, 1/4, and so on. You can imagine this set as points on the number line that get closer and closer to 0 as n increases. The point $x = 0$ is a limit point (or accumulation point or cluster point) of the set $E$ . Here's why: For every $r > 0$ , no matter how small, we can find a point in $E$ within $r$ of $0$ . For example, if we choose $r = 0.001$ , we can find an $n$ such that $1/n < 0.001$ . Here, if $n > 1000$ , then $1/n < 0.001$ . In other words, the ""ball"" $B_r(0)$ (which in this one-dimensional case is just the interval $(0-r, 0+r)$ ) contains points from $E$ . And we can do this for any $r$ we choose, no matter how small. So there are points in $E$ that are arbitrarily close to $0$ , which is the definition of a limit point. Therefore, $0$ is a limit point of $E$ . Example for Isolated point: Let's say we have the set $E = \{1, 2, 3, 1/n : n \in \mathbb{N}, n > 3\}$ . This set includes elements like 1, 2, 3, 1/4, 1/5, 1/6, and so on. Now, consider the point $x = 2$ . This point is an isolated point of the set $E$ . Here's why: There exists an $r > 0$ (say, $r = 0.5$ ) such that the ball $B_r(2)$ contains only the point $x = 2$ from the set $E$ . In other words, no other point in $E$ is within $r$ of 2. Remember that in this one-dimensional case, a ""ball"" $B_r(x)$ is just the interval $(x - r, x + r)$ . So $B_{0.5}(2) = (1.5, 2.5)$ , and no other points from $E$ fall within this interval. Therefore, $2$ is an isolated point of $E$ . Furthermore, it's also clear that $2$ is not a limit point of $E$ because there is an open interval around $2$ that contains no points of $E$ other than $2$ itself. Thus, according to the second definition, $2$ is indeed an isolated point of $E$ .","['general-topology', 'real-analysis']"
4728225,Triangle with two medians,"I need help with this proof: AD and BE are medians in triangle ABC, meeting at point F. Point G is the middle of segment BF. The continuation of the segment AG intersects the side BC at point H. Prove that BC=5BH. Here's what I could find: AD=DC BE=EC BG=GF=FE ED || AB However I don't see how these facts help me prove the claim.
Any help would be appreciated.","['triangles', 'geometry']"
4728237,How to calculate the limit $\lim_{x\rightarrow 0}k^2\operatorname{csch}^2(kx)-\frac{1}{4}\operatorname{csch}^2\left(\frac{x}{2}\right)$？,"I have just learnt the hyperbolic function and I meet a problem goes like follows: $$\lim_{x\rightarrow 0}k^2\operatorname{csch}^2(kx)-\frac{1}{4}\operatorname{csch}^2\left(\frac{x}{2}\right)$$ I have tried refomulating it as a form of fraction and applying the L'Hopital's Law but it seems that it just leads the problem to a much more messy land, because multiple differentiations is required. I wonder if there exists any easy way to solve it? Any comments will be appreciated.","['limits', 'hyperbolic-functions']"
4728238,Why Wolfram gives inconsistent result for: $\int_0^\infty \frac{e^{x-x^2}-e^{-x-x^2}}{x}~dx$,"Why Wolfram gives inconsistent result for : $$I=\int_0^\infty \frac{e^{x-x^2}-e^{-x-x^2}}{x}~dx$$ As shown in the picture below, the analytical integration gives $-i\pi$ , but numerical integration gives $1.93193$ . The result must be a real number, but why Wolfram gives a complex result for the analytical integration? What I suspect is Wolfram might treat this integral as Frullani integral, where $f(x)=e^{x-x^2}$ , and $f(-x)=e^{-x-x^2}$ . We get $$\int_0^\infty \frac{e^{x-x^2}-e^{-x-x^2}}{x}~dx=\int_0^\infty \frac{f(x)-f(-x)}{x}~dx=(f(\infty)-f(0))\ln\left(\frac1{-1}\right)=-i\pi$$ But this is WRONG!","['integration', 'wolfram-alpha', 'improper-integrals', 'analysis', 'complex-analysis']"
4728274,Solving the reduced sextic $z^6+z^2+az+b=0$ using the two-parameter Kampé de Fériet function?,"First, some background. I. One-Parameter forms The Bring-Jerrard quintic $x^5+x+\alpha=0$ has solution, $$x = -\alpha\sum_{k=0}^\infty(-1)^k\frac{(5k)!}{k!(4k+1)!}\;\alpha^{4k}$$ This series has a narrow radius of convergence, namely $|\alpha|<\left(\frac{4^4}{5^5}\right)^{1/4}\approx 0.53$ . But it can be extended via analytic continuation using the generalized hypergeometric function ${_pF_q},$ $$x = -\alpha\,{_4F_3}\Big(\frac15,\frac25,\frac35,\frac45;\,\frac24,\frac34,\frac54;\,-\frac{5^5}{4^4}\alpha^4\Big)$$ for more general $\alpha$ , thus solving the general quintic. II. Two-Parameter forms The quintic and sextic two-parameter equations, $$By^5+Ay^2+y+1 = 0$$ $$Bz^6+Az^2+z+1 = 0$$ can be solved as, $$y = -\sum_{j=0}^\infty \sum_{k=0}^\infty (-1)^k \frac{(2j+5k)!}{j!k!(j+4k+1)!}\;A^j B^k$$ $$z = -\sum_{j=0}^\infty \sum_{k=0}^\infty (+1)^k \frac{(2j+6k)!}{j!k! (j + 5 k + 1)!}\; A^j B^k$$ with the quintic root $y$ by Passare and Tsikh in this paper , and the sextic root $z$ by Robert Israel in this old MSE answer . The second equation is just the general sextic in disguise since the general sextic can be reduced to the form, $$z^6+z^2+\alpha z+\beta =0$$ Unfortunately, R. Israel's solution $z$ also has a narrow radius of convergence. So we are seeking an analytic continuation such that it will be valid for more general $(A,B)$ thus solving the general sextic. III. Questions If the analytic continuation for the quintic $x$ involves the one -parameter generalized hypergeometric function , does the analytic continuation for R. Israel's sextic $z$ involve the two -parameter Kampé de Fériet function ? Or is it some other two-parameter function, maybe like the Appell series , Humbert series , etc? The Kampé de Fériet function can solve the general sextic in its reduced form. In Mathematica syntax, what would be the input to solve, for example, $z^6+z^2+3z+2=0$ ?","['analytic-functions', 'special-functions', 'polynomials', 'sequences-and-series']"
4728313,A coin toss problem,"I am carrying out some self-study relating to mixed random variables and I ran into a problem that requires one to show that $$b^{m+1}\sum\limits_{k=0}^n\binom{m+k}{m}a^k + a^{n+1}\sum\limits_{k=0}^m\binom{n+k}{n} b^k = 1,$$ if $a,b> 0$ and $a+b=1.$ I have tested that the equality holds for different combinations of $n,m$ between 1 and 4. For this range of values, it appears $$b^{m+1}\sum\limits_{k=0}^n\binom{m+k}{m}a^k + a^{n+1}\sum\limits_{k=0}^m\binom{n+k}{n} b^k =(a+b)^{\min(n,m)} = 1.$$ I do not know how to provide a rigorous proof of this assertion. Can someone please set me on the right path?","['combinatorics', 'probability-theory', 'probability', 'real-analysis']"
4728343,"Constructing simply connected domains, starting from disks","Let $\mathcal{G} $ be the collection of nonempty, open, connected subsets of $\mathbb{C} $ . For each $\mathcal{S}\subset\mathcal{G} $ , we may ask whether $\mathcal{S} $ is 'closed' under the following two 'operations'. If $D_1, D_2\in\mathcal{S} $ and $D_1\cap D_2 $ is nonempty and connected, then $D_1\cup D_2\in\mathcal{S} $ . If $D_1, D_2, D_3,\ldots\in\mathcal{S} $ and $D_n\subset D_{n+1} $ for all $n=1,2, 3,\ldots $ , then $\bigcup_{n=1}^\infty D_n\in\mathcal{S} $ . In page 87 of Complex Analysis by Freitag and Busam, the authors remark that the following can be proven in a non-trvial way: If $\mathcal{S}\subset\mathcal{G} $ contains all (nonempty open) disks and is closed under the above two operations, then $\mathcal{S} $ contains all simply connected domains in $\mathbb{C} $ So, how do we prove this statement? Can we generalize this to higher dimensions?","['complex-analysis', 'fundamental-groups', 'algebraic-topology']"
4728347,Let $G$ be a finite simple non abelian group and $\{1\}\neq H\le G$ be such that $\vert C_{G}(H)\vert=\vert G:H \vert$. Then $H=G$.,"Let $G$ be a finite simple non abelian group and $\{1\}\neq H\leq G$ be such that $\vert C_{G}(H)\vert=\vert G:H \vert$ . Show that $H=G$ . I can find only $[G,G]=G$ and $Z(G)=\{1\}$ and if $H$ is a proper subgroup of $G$ , then $G \hookrightarrow S_{\vert G:H\vert}$ . Can't bring $C_G(H)$ into the picture. Can anyone please provide a hint to think about it?","['simple-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4728348,Bound a function with parameter involving logarithm,"In my master project I encounter the following function $$f_\varepsilon(x) = \ln\left(\frac{x^{1 + a} + \varepsilon^\beta}{\lambda(x)^2 + \varepsilon^2}\right)$$ for $a$ close to zero, $\beta \in (1, 2)$ and $$\lambda(x) = \frac{x}{|\ln(x)|^2}.$$ I aim to bound the following quantity $$g_\varepsilon(x) = \bigg|\frac{1}{f_{\varepsilon}(x)} - \frac{1}{f_{0}(x)}\bigg|$$ on a certain interval $[0, x_0]$ , $x_0 \ll 1$ , where $f_0$ is just $f_\varepsilon$ with the parameter $\epsilon = 0$ . Clearly, for $x \to 0$ , we get $$g_\varepsilon (0) = \frac{1}{(2 - \beta)|\ln \varepsilon|}$$ so I feel that we should get something like $$g_\varepsilon(x) \le \frac{C}{(2 - \beta)|\ln \varepsilon|}$$ for $C > 0$ independent of $\varepsilon$ . I tried to bound $|\ln(\varepsilon)|g_\varepsilon(x)$ by such a constant but I couldn't prove rigourously that it didn't depend on $\varepsilon$ . Any help would be greatly appreciated.","['logarithms', 'real-analysis', 'calculus', 'upper-lower-bounds', 'inequality']"
4728409,Proving that $I=\int_0^\infty \frac{n x^{n-1}}{(1+x)^{n+1}}dx<+\infty$,"I got this integral from a probability question in which I am interested to prove that it is $<\infty$ $$I=\int_0^\infty \frac{n x^{n-1}}{(1+x)^{n+1}}dx,~~~n \ge 1$$ I tried substitution $y =x+1$ , but it does not help, so I am thinking maybe a direct integration is not the way to go.  I tried bounding the integrand using $\frac{1}{(1+x)^{n+1}}<1 $ but I get $+\infty$ Any idea, how to do it?","['integration', 'improper-integrals', 'definite-integrals', 'real-analysis', 'beta-function']"
4728446,Ten parabolas are drawn in a plane.No three parabola are concurrent. Find the total number of disjoint regions of the plane.,"Ten parabolas are drawn in a plane. Any two parabola intersect in two real, and distinct, points. No three parabola are concurrent. Find the total number of disjoint regions of the plane. I counted the number of regions for 1,2 and 3 parabolas and got 2, 5 and 10 regions respectively. I tried forming a recurrence relation but could not come up with anything.",['combinatorics']
4728513,How do I calculate the probability of getting a 5 card straight in 7 card poker?,"So here's my progress on this problem. The first straight to examine is $\begin{pmatrix} A & 2 & 3 & 4 & 5 & n & m\end{pmatrix}$ . If we start with all one suit, there are $\binom{47}{2}$ ways to obtain this hand. Next, we iterate through the suit of one card, say, the Ace, and we have $\binom{46}{2}$ , then $\binom{45}{2}$ , then $\binom{44}{2}$ ways to obtain this hand without repeating previous hands. To iterate through all suits of all cards, take the sum \begin{equation}
\sum_{k,l,m,n,p=0}^{3}\binom{47-k-l-m-n-p}{2}
\end{equation} and subtract $4\binom{47}{2}$ to discount straight flushes. Next, to count the hands of $\begin{pmatrix} 2 & 3 & 4 & 5 & 6 & p & q\end{pmatrix}$ , $\begin{pmatrix} 3 & 4 & 5 & 6 & 7 & r & s\end{pmatrix}$ , etc., we start with $\binom{43}{2}$ to obtain the first hand, so as not to double count hands of the form $\begin{pmatrix} A & 2 & 3 & 4 & 5 & 6 & m\end{pmatrix}$ . Therefore we add 9 hands of the following form, subtracting the possible straight flushes. \begin{equation}
9 \left( \sum_{k,l,m,n,p=0}^{3}\binom{43-k-l-m-n-p}{2}\right) - 9\binom{46}{2}
\end{equation} These sums may be reduced to one dimension by observing that the weight of each possible sum, r, for $k+l+m+n+p=r$ is equivalent to the coefficient of each term $x^r$ in the expansion $(x^0+x^1+x^2+x^3)^5$ .
The problem is that these equations describe 6412688 possible straight hands while there should be 6180020 possible hands (according to Wikipedia). I can not for the life of me figure out what hands I am double counting here. Any help is appreciated.","['permutations', 'statistics', 'combinatorics', 'poker', 'probability']"
4728565,Largest natural number $n$ such that $\int_1^\infty\frac{\log^nx}{e^x}\mathrm dx<1$,"Find the largest natural number $n$ such that $$\int_1^\infty\frac{\ln^nx}{e^x}\mathrm dx<1$$ Upon searching on site and google, I found a similar integral: $$\int_0^\infty\frac{\ln x}{e^x}\mathrm dx=-\gamma$$ But the techniques involving solving such integrals is beyond what I've learnt till now. The elementary method I know is to think of some adaptation of squeeze theorem like to search a $f$ such that: $$f>\frac{\ln^nx}{e^x}$$ and $$\int_1^\infty f\mathrm dx = 1$$ But I can't imagine any such $f$ upon hit and trial. (P.S.: This question came in my class test of applications of derivatives. According to answer key, $n=7$ )","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'derivatives']"
4728579,Proving two matrices have the same eigenvalues when one is Hermitian matrix,"I have a $A$ Hermitian matrix that can be written as $A = B + iC$ where $i$ is an imaginary number. Also, $B$ and $C$ are $n$ dim real-value matrices. Now, there is a matrix, $$
  D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix}
$$ Here, I would like to prove that $A$ and $D$ have the same eigenvalues in this case. I have first attempted to prove that D is actually a symmetric matrix as below, $$
  \begin{align*}
     A = A^* = \bar{A}^T \\
     A = B + iC = (B - iC)^T, B = B^T, C = -C^T \\
     D = \begin{pmatrix} B & -C \\ C & B \end{pmatrix} = \begin{pmatrix} B^T & C^T \\ -C^T & B^T \end{pmatrix} = D^T
  \end{align*}
$$ However, I have no clue about writing the relation of eigenvalues with respect to matrices $A$ and $D$ . Can anyone give me some suggestions for the next step? Thank you for your time!","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4728638,"How to prove $\mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top$, where $\mathbf{Q}$ is any element-wise squared correlation matrix?","Let $(X_1,…,X_n)$ be a random vector with $0<\prod_{j=1}^n\text{Var}(X_j)<∞$ . Let $\mathbf{Q}=(\mathbf{q}_{1},…,\mathbf{q}_{n})=(ρ_{jk}^2)_{n×n}$ , where $ρ_{jk}$ is the Pearson correlation coefficient between $X_j$ and $X_k$ . How to prove or disprove $$\mathbf{1}^\top\mathbf{Q}^+\mathbf{Q}=\mathbf{1}^\top$$ where $\mathbf{Q}^+$ is the Moore-Penrose inverse of $\mathbf{Q}$ and $\mathbf{1}^\top$ is a row vector of ones? $$$$ It seems that when any row or column vectors in $\mathbf{Q}$ are linearly dependent, they must be equal. If it is true, let $\mathbf{H}_{n×n}=(\mathbf{h}_1,…,\mathbf{h}_r,\mathbf{0},…,\mathbf{0})^\top$ be the reduced row echelon form of $\mathbf{Q}$ , where $r=\text{rank}(\mathbf{Q})$ ; and there is $\mathbf{h}_{j}^\top\mathbf{h}_{k}=0$ for all $j\neq k$ . Then, let the column indices of the leading ones in each nonzero rows of $\mathbf{H}$ be $j_1,…,j_r$ , and let $$\mathbf{F}_{n×r}=(\mathbf{q}_{j_1},...,\mathbf{q}_{j_r}),\;
\mathbf{G}_{r×n}=(\mathbf{h}_1,...,\mathbf{h}_r)^\top$$ According to rank factorization from reduced row echelon forms , we have $\mathbf{Q}=\mathbf{FG}$ According to construction of Moore–Penrose inverse by rank decomposition , we have $$\mathbf{Q}^+=\mathbf{G}^\top(\mathbf{GG^\top})^{-1}(\mathbf{F^\top F})^{-1}\mathbf{F}^\top$$ Thus, \begin{equation}
\begin{split}
& \mathbf{1}^\top\mathbf{Q}^+\mathbf{Q} 
= \mathbf{1}^\top\mathbf{G}^\top(\mathbf{GG^\top})^{-1}\mathbf{G} \\
& = \mathbf{1}^\top
\begin{bmatrix}
  \mathbf{h}_1 & \cdots & \mathbf{h}_r 
\end{bmatrix}
\begin{bmatrix}
  \mathbf{h}_1^\top\mathbf{h}_1 & \cdots &
    \mathbf{h}_1^\top\mathbf{h}_r \\
  \vdots &  \ddots &  \vdots \\
  \mathbf{h}_r^\top\mathbf{h}_1 & \cdots & 
    \mathbf{h}_r^\top\mathbf{h}_r \\
\end{bmatrix}^{-1}
  \begin{bmatrix}
  \mathbf{h}_1^\top \\
  \vdots \\
\mathbf{h}_r^\top \\
\end{bmatrix} \\
& = \mathbf{1}^\top\sum_{i=0}^r (\mathbf{h}_i^\top\mathbf{h}_i)^{-1} \mathbf{h}_i \mathbf{h}_i^\top = \mathbf{1}^\top
\end{split}
\end{equation} where $\mathbf{h}_i^\top\mathbf{h}_i$ is the number of ones in $\mathbf{h}_i$ and $\mathbf{h}_i\mathbf{h}_i^\top$ is an $n×n$ block diagonal matrix with main-diagonal blocks of either ones or zeros. Therefore, the question may turn into how to prove or disprove that, when any rows in $\mathbf{Q}$ are linearly dependent, they must be the same.","['correlation', 'idempotents', 'matrices', 'pseudoinverse', 'linear-algebra']"
4728656,"Gradients of $(u, v) \mapsto \frac12 \left\| A - u v^T \right\|_{\text{F}}^2$ via the chain rule","Given the matrix $A \in {\Bbb R}^{n \times m}$ , let the scalar field $f : {\Bbb R}^n \times {\Bbb R}^m \to {\Bbb R}_0^+$ be defined by $$ f (u, v) : = \frac12 \left\| A - u v^T \right\|_{\text{F}}^2 $$ where $\| \cdot \|_{\text{F}}$ denotes the Frobenius norm. Find the gradients $\nabla_u f$ and $\nabla_v f$ . Solution: Let $R=A-uv^T$ . The gradients are $\nabla_u f = - R v$ and $\nabla_v f = - R^T u $ . I am struggling. I have read about the differentials method and I have tried to apply rules from The Matrix Cookbook , but I always have problems with transposed matrices in the result. Is there a systematic way to solve these problems without making use of summations? If you can solve the derivative it would be great, but it would also be awesome if you just point me out to somewhere where this is explained properly","['scalar-fields', 'multivariable-calculus', 'matrix-calculus', 'derivatives', 'chain-rule']"
4728693,Exterior square of rational function of one variable,"Let us consider vector space $$V = (\Lambda^2 (\mathbb C(t))^\times)\otimes_\mathbb Z\mathbb Q.$$ Here I consider the group $A=(\mathbb C(𝑡))^\times$ of non-zero rational functions under multiplication. It is an abelian group and so we can take exterior square. Denote by $W$ the subspace of $V$ generated by elements of the form $f\wedge g$ such that the divisors of the functions $f$ and $g$ are disjoint (as divisors on $\mathbb P^1$ ). Is it true that $V=W$ ? Here is an example of some element from $W$ : $$\left(\dfrac{t-a}{t-b}\right)\wedge (t-c)$$ (The points $a,b,c$ are mutually different). However, it is not clear whether $\xi_{a,b}:=(t-a)\wedge (t-b)$ lies in $W$ or not. (Both divisors of the function $(t-a), (t-b)$ contain $\infty\in\mathbb P^1$ .) On the other hand, it is easy to see that my question is equivalent to the fact that for any $a,b$ we have $\xi_{a,b}\in W$ .","['algebraic-geometry', 'abstract-algebra', 'rational-functions', 'exterior-algebra']"
4728704,"If $0<x<1$ and $x$ is rational, does $2^x$ have to be irrational? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 12 months ago . Improve this question If $0<x<1$ and $x$ is rational, does $2^x$ have to be irrational? Why? Also, if $2^x$ is rational, and $0<x<1$ and $x$ is rational, does $x$ have to be irrational? (i.e. contrapositive of the first statement) I believe a better (and easier) way would be to prove the latter statement and then the former one would be proven as well (due to contra-positivity), however I was unable to prove that anyway.","['rationality-testing', 'number-theory', 'elementary-number-theory', 'real-analysis', 'discrete-mathematics']"
4728721,What function satisfies $F'(x) = F(2x)$?,"The exponential generating function counting the number of graphs on $n$ labeled vertices satisfies (and is defined by) the equations
$$
F'(x) = F(2x) \; \; ; \; \; F(0) = 1
$$
Is there some closed form or other nice description of this function?
Does it have a name? Of course, the series itself does not converge for any nonzero $x$, but like the Lambert W function (counting trees) it has combinatorial meaning. And the Lambert W function has a nice description as the inverse of $x e^x$; maybe there is a similar description of $F$?","['generating-functions', 'delay-differential-equations', 'combinatorics', 'reference-request']"
4728732,Finding critical points and inflection points for $f(x)=\frac{2x+5}{3x+1}$,"Does this function $$f(x)=\frac{2x+5}{3x+1}$$ not have any critical points?
I have to find the intervals on which $f(x)$ is increasing or decreasing. The first derivative of $f(x)$ is $$f'(x)=-\frac{13}{(3x+1)^2}$$ which is undefined at $x=-1/3$ but that is not in the domain of $f(x)$ , so $x=-1/3$ cannot be a critical point,right? And for what value of x is $f'(x)=0$ ? I also have to find the inflection points of $f(x)$ to fnd the intervals on which $f(x)$ is concave up or down so I found the 2nd derivative of $f(x)$ , $$f''(x)=\frac{78}{(3x+1)^3}$$ but then I got stuck again.","['continuity', 'calculus', 'derivatives']"
4728749,CLT Problem of $N_t=\sup\{n\geq 1 | S_n \leq t\}$,"Let $(Y_k)$ be i.i.d denote $\mu =\mathbb{E}[Y_1],\ \sigma ^2 = Var[Y_1]$ and there exists $c>0$ s.t $P(Y_1\geq c)=1$ $$ S_n=Y_1+...+Y_n,\ \ \ N_t=\sup\{n\geq 1 | S_n \leq t\}$$ for $t>0$ a) prove $\frac{N_t}{t} \overset{t\rightarrow \infty}{\rightarrow} 1/\mu$ a.s b)prove weak convergence $$\frac{N_t-\frac{t}{\mu}}{\sigma \sqrt{t/\mu^3}}\rightarrow N(0,1)$$ This is a question from a test in probability. I tried denoting $m(x)=\mathbb{E}[N_t|Y_1]$ , something we saw in the past. But since I dont know the density function I didnt know how to calculate it and continue. I came to the conclusion (without proving) that $$ m(x)=\mathbb{E}[P(Y_1>x)+P(Y_1<x)(1+N(x-Y_1)]$$ How do I continue? Thank you,","['central-limit-theorem', 'probability-theory']"
4728766,"Prove $\int_{0}^{\pi} \sqrt[n]{\prod_{k=1}^{n}\csc^2(x-\alpha_k)}dx\geq4π$ for reals $\alpha_1,\alpha_2,...\alpha_n$","Prove for all positive integer $n$ and for all $\alpha_1,\alpha_2,...\alpha_n \in \mathbb{R}$ Prove that $\int_{0}^{\pi} \sqrt[n]{\prod_{k=1}^{n}\csc^2(x-\alpha_k)}dx\geq4π$ I have tried by applying GM-HM inequality
We know the inequility GM $\geq $ HM Appliying this on $\csc^2(x-\alpha_1), \csc^2(x-\alpha_2), \cdots \csc^2(x-\alpha_n)$ We will get $$\sqrt[n]{\prod_{k=1}^{n}\csc^2(x-\alpha_k)}\geq \frac{n}{\sum_{k=1}^{n}\sin^2 (x-\alpha_k)}$$ I am stuck after that. Please help me to proceed further","['integration', 'inequality', 'definite-integrals', 'integral-inequality', 'trigonometry']"
4728863,"Compactness, pseudocompactness, and realcompactness without Hausdorff","A space is compact provided every open cover admits a finite subcover. A space is pseduocompact provided every continuous image of the space into the Euclidean line $\mathbb R$ is bounded. A space is realcompact provided it embeds as a closed subspace of $\mathbb R^\kappa$ for some cardinal $\kappa$ . We can quickly see that every pseudocompact+realcompact space is compact: take the space $H\subseteq \mathbb R^\kappa$ (by realcompactness); its projection $H_\alpha\subseteq\mathbb R$ for each factor $\alpha<\kappa$ must be bounded (by psuedocompactness), and thus $\overline{H_\alpha}$ is compact by the Heine-Borel theorem. This makes $H$ a closed subset of the compact space $\prod_{\alpha<\kappa}\overline{H_\alpha}$ , and thus compact. It's immediate that every compact space is pseudocompact. It's also true that every compact Hausdorff space is realcompact. So we have a cute characterization for Hausdorff spaces: A Hausdorff space is compact if and only if it is both pseudocompact and realcompact. My question is this: is there a natural property $R$ that implies neither Hausdorff nor pseudocompact, such that an arbitrary space is compact if and only if it is both pseudocompact and $R$ ?","['general-topology', 'realcompact-spaces', 'compactness']"
4728910,On a problem similar to the Goat problem,"So I was trying to remember what the goat problem was when I thought of this: Imagine points $A$ , $B$ , and $C$ on a circle. $B$ and $C$ are reflections of each other on the diameter that $A$ is on. What would $\theta=\angle BAC$ be if shape $BAC$ is half of the circle? My idea was to let the center of the circle be $O$ . We then draw radii $OB$ , $OA$ , and $OC$ . Angle $BOC$ is $2\theta$ and angles $BOA$ and $COA$ are $\pi-\theta$ . Using area formulas for triangles $ABO$ and $ACO$ , we get $$r^2\sin\theta+\theta r^2=\frac{\pi}2r^2$$ Where $r$ is the length of the radius. Dividing by $r^2$ , we get stuck: $$\theta+\sin\theta=\frac{\pi}2$$ Are there other ways of reaching this equation, possibly even giving a closed form solution? I tried using Lagrange Inversion but the coefficients don't exist. Wolfram Alpha states that $\theta\approx48^\circ$ if this helps.","['trigonometry', 'circles', 'geometry']"
4728924,Adjoint to the continuity equation,"Consider the continuity equation on $[0,T] \times \mathbb{R}$ $$ \partial_{t}u + \partial_{x}(bu)=0$$ with initial data $u(0,x) =u_{0}(x)$ which we assume to be smooth for now (also assume $b$ is smooth). I have read in papers that the formal adjoint to the forward continuity equation is the backward transport equation, i.e. $$\partial_{t}v + b\partial_{x}v=0 $$ with final data $v(T,x) = v^{T}(x)$ . I would like to know if there is a precise way to show this? In other words how can we make sense of the backward transport equation as the adjoint of the forward continuity equation? Here is my attempt but I am not totally convinced by it. Consider $u,v \in C^{\infty}_{c}(\mathbb{R})$ for convenience and define the operator $L$ by $Lu = \partial_{t} u + \partial_{x}(bu)$ . Then we look for an operator $L^{*}$ (the adjoint) which satisfies $$ \langle Lu, v \rangle = \langle u, L^{*}v \rangle$$ where the duality bracket can be assumed to be $L^{2}([0,T]; L^{2}(\mathbb{R}))$ (i'm not actually sure why we can assume this). Then integrating by parts we see that \begin{aligned}\langle Lu, v \rangle &= \int_{t}\int_{x} v (\partial_{t}u + \partial_{x}(bu))~dxdt \\ &= - \int_{t}\int_{x} u(\partial_{t}v+b\partial_{x}v)~dxdt +  \int_{\mathbb{R}} u(x,T)v(x,T)-u(x,0)v(x,0)~dxdt. \end{aligned} So formally the forward continuity equation will be adjoint to the transport equation if the boundary term vanishes, for which we can simply require that $u(x,t):=v(x,T-t)$ , i.e. take $u$ to be the time-reversal of $v$ .  So the two equations will be adjoint if one is posed backwards in time and the other one is posed forwards in time. But this doesn't really make sense since I am defining $u$ in terms of $v$ and the duality equality should hold for any $u,v$ .","['operator-theory', 'partial-differential-equations', 'functional-analysis', 'real-analysis']"
4728957,A (short) proof for the paracompactness of CW complexes,"So, for a while now I've been looking for a short but concise proof of the fact that Every CW complex is paracompact. I finally found the following proof (shortest so far) of this theorem but couldn't understand a few things the author did here: The lemma being referred to (4th line from below) is as follows: Lemma. Let $K$ be compact, $C\subseteq K$ be closed, and $\mathcal U=\{U_\alpha\}$ be an open cover of $K$ . If $\{\psi_\alpha\}$ is a partition of unity subordinate to $\{C\cap U_\alpha\,:\,U_\alpha \in\mathcal U\}$ , then there exists a partition of unity $\{\Psi_\alpha\}$ subordinate to $\mathcal U$ s.t. $\Psi_\alpha\big|_C=\psi_\alpha$ for each $\alpha$ . Also, the carrier of $x$ , written $C(x)$ , is defined as follows: Definition. If $(X,\mathcal E)$ is a cellcomplex, the carrier of $A\subseteq X$ , $C(A)$ , is the intersection of all subcomplexes of $(X,\mathcal E)$ containing $A$ . I have understood all the individual logical steps in the proof. The part that confuses me is the following: The author proved in the end that $(X_{\Gamma_0},U_{\Gamma_0},p_0)$ cannot be maximal in $T$ . How does this imply that there is a partition of unity subordinate to $\mathcal U$ ? Maybe I'm just overlooking something very trivial here! The proof is taken from Lundell's The Topology of CW Complexes . P.S.: I'm sorry for the image but typing the entire proof would have been too tedious.","['proof-explanation', 'general-topology', 'paracompactness', 'cw-complexes']"
4728963,Symplectomorphism taking a Lagrangian to its isotopic copy,"Suppose $\iota_0:L\xrightarrow{}M$ is a compact Lagrangian submanifold in a symplectic manifold $(M,\omega)$ . Let $\iota_t:L\times I\xrightarrow{} M$ be an isotopy and denote $\iota_1(L)$ by $L’$ . I wonder if there is a symplectomorphism $\varphi:M\xrightarrow{}M$ that takes $L$ to $L’$ . Such a diffeomorphism exists by the Isotopy Extension Theorem, but I don’t see a way to turn that into a symplectomorphism. Any negative results are also appreciated.","['differential-topology', 'symplectic-geometry', 'reference-request', 'differential-geometry']"
4728979,"Prove that if $\lim_{x\to \infty} f'(x) = 0$, then $\lim_{x\to \infty} f(x+1)-f(x) = 0$","I'm working on this proof and I think I have a sketch but I'm not sure it's rigorous enough. Suppose $f:\Bbb R \to \Bbb R$ is differentiable and that $$\lim_{x\to \infty} f'(x) = 0$$ Prove that $$\lim_{x\to \infty} f(x+1)-f(x) = 0$$ Proof: We want to prove that for any $\epsilon > 0$ , there is an $x_0$ such that $|f(x+1)-f(x)|< \epsilon$ for all $x \ge x_0$ . By the Mean Value Theorem, there is some $c \in (x, x+1)$ such  that $f(x+1) - f(x) = f'(c)$ . By the convergence of $f'(x)$ , there is some $y_0$ such that $|f'(x)| < \epsilon$ for all $x \ge y_0$ . Thus $$x \ge y_0 \implies c>y_0 \implies |f'(c)| = |f(x+1) - f(x)| < \epsilon$$ and $x_0 = y_0$ . $\square$ Thanks!","['limits', 'proof-writing', 'solution-verification']"
4728991,Hilbert's 13th problem and the reduced septic $x^7+(x+p)(x+q)(x+r)=0$?,"I. Sextic The general sextic can be reduced to a two-parameter form, $$Ax^6+Bx^2+x+1=0$$ $$y^6+y^2+ay+b=0$$ To transform $P(x)\to P(y)$ and vice versa is easy. They have solutions, $$x = -\sum_{j=0}^\infty \sum_{k=0}^\infty (+1)^k \frac{(2j+6k)!}{j!k! (j + 5 k + 1)!}\; A^j B^k$$ and, $$y_k=\sum_{n=1}^\infty\frac{\left(\frac n6\right)!}{n!\,\Gamma\big(2-\frac{5n}6\big)}\,e^\frac{(2k+1)\pi i n}6(-p)^{1-\frac{5n}6}(-q)^\frac n6\,{_2 F_1}\Big(1-n,-\frac n6;2-\frac{5n}6;\frac pq\Big)$$ where $p,q=\frac{-a\pm\sqrt{a^2-4b}}2.$ The first is by Robert Israel, while the second is by Tyma Gaidash in this post . Both are limited by different radii of convergence. However, I was surprised by the second solution since the hypergeometric function ${_2F_1}$ is a one -variable function, whereas what should solve the general sextic is the Kampé de Fériet function which is two -variable. This has implications for deg- $7$ equations. II. Septic The general septic can be reduced to the 3-parameter, $$u^7+Au^3+Bu^2+Cu+1=0$$ By a minor change of variables, this can be expressed as, $$v^7+(v+p)(v+q)(v+r)=0$$ The Lauricella series is a three -variable function and can perhaps solve the reduced septic. But Hilbert's 13th Problem asks if the septic's root can be expressed as the composition of a finite number of two -variable functions. III. Question Which brings me to my question. Can we solve the reduced septic in the same manner as Tyma did for the reduced sextic? Would the one-variable ${_2F_1}$ still do, or do we need something more general?","['special-functions', 'closed-form', 'polynomials', 'sequences-and-series']"
4729003,Approximate Maclaurin series for $\sqrt x$,"One of my hobbies for the past while has been Taylor and Maclaurin series. I understand that since $ f(x)=\sqrt x $ isn't analytic at zero, we can't make a Maclaurin series expansion for it. However, after some playing around, I was able to construct a Maclaurin series which closely approximates square root of x. It isn't obvious to me why this function should exist (I tried searching for it), and I was hoping someone more knowledgeable could explain, or point me in the right direction at the very least. The function is: $$f(x)= \frac{1}{\sqrt \pi} \sum_{n=0}^{\infty}\frac{x^n (-1)^n}{n! (1-2n)}$$ Here's a plot for reference :) Approximate Maclaurin series for sqrt(x) Edits after some insight from Goncalo, I think I understand. I'll construct the approximation, but first some foreground The $\sqrt x \operatorname{erf}(\sqrt x) $ term seemed mysterious, and led me down a rabbit hole. I initally assumed that given an analytic function $ f(x) $ , $ f(\sqrt x)$ wouldn't have a Maclaurin series representation because $ \sqrt x $ isn't analytic at zero. However, as long as $ f(x) $ is an even function, we're in luck! Consider $$ f(x) = \cos(\sqrt x) $$ We could write this as $$ \cos(\sqrt x) = \sum_{n=0}^{\infty} \frac{x^\frac{n}{2} \operatorname{t}(n)}{n!} $$ Where $ \operatorname{t}(n) = \cos(\frac{\pi}{2}n)$ Now, since cosine is an even function, all the fractional power terms are cancelled (!!), meaning we can rearrange to $$ \cos(\sqrt x) =  \sum_{n=0}^{\infty} \frac{x^n \operatorname{t}(2n)}{n! \operatorname{P}(n,2n)} $$ Where $\operatorname{P}(k,x)$ is the falling factorial: $$ \operatorname{P}(k,x) = \prod_{m=0}^{k-1}x-m$$ Interestingly, this lets us find values for imaginary numbers, and smoothly extends the domain for $ \cos(\sqrt x) $ into the negative reals. We can use this trick on the integral of $ \operatorname{erf}(x) $ like so. First, set up this ugly but useful Maclaurin series, which represents the J $ _{th} $ integral of the Gaussian $ e^{-x^2} $ $$ \operatorname{t}(n,J) = \left\{n+\operatorname{mod}\left(J,2\right)=1:0,\frac{\cos\left(\frac{\pi}{2}\left(n-J\right)\right)P\left(\frac{n+1-J}{2},n+1-J\right)}{n+1-J}\right\} $$ Let $ \operatorname{even}(x) = \sum_{n=0}^{\infty}\frac{x^n t(n,2)}{n!} $ . This is the second integral of the Gaussian. Notice that if you normalize this function like so: $\frac{1}{\sqrt \pi}(2\operatorname{even}(x)-1)$ , it approximates $\operatorname{y}=x$ . But we'll leave it for now. Since $ \operatorname{even}(x) $ is an even function, we can use our trick to find $ \operatorname{even}(\sqrt x) $ We'll set up a new Maclaurin series based on our first one: $$ \operatorname{u}(n) = \frac{\operatorname{t}(2n, 2)}{P(n,2n)} $$ Now, we have $ \operatorname{even}(\sqrt x)=\sum_{n=0}^{\infty}\frac{x^n \operatorname{u}(n)}{n!}$ All that's left to do is normalize, and we get $$ \frac{1}{\sqrt \pi}(2\sum_{n=0}^\infty\frac{x^n \operatorname{u}(n)}{n!}-1) \approx \sqrt x $$ If anyone finds this interesting, here's a Desmos demo","['real-analysis', 'taylor-expansion', 'sequences-and-series', 'power-series', 'radicals']"
4729041,Every compact subset of real line is spectrum of self-adjoint operator,"It is known that if $H$ is Hilbert space and $T$ is self-adjoint operator on $H$ , then the spectrum is real and closed. But is every closed or compact subset of real numbers a spectrum of some self-adjoint operator on some $H?$ For interval I think we can find the construction but how if the set is a mess, for example, Cantor set?","['operator-theory', 'functional-analysis']"
4729061,How to solve $\sin\left(1+\frac{1+\sqrt{5}}{2}\right)<1/2$?,Problem : Find a geometric construction or a proof by hand to show : $$\sin\left(1+\frac{1+\sqrt{5}}{2}\right)<1/2$$ As attempt I introduce the inequality : $$\sin\left(1+\frac{1+\sqrt{5}}{2}\right)-1+\frac{1+\sqrt{5}}{2}-\frac{559}{500}<0$$ Then I introduce : $$f(x)=\sin\left(1+x\right)-1+x-\frac{559}{500}$$ Or : $$g(x)=\sin\left(x\right)+x-2-\frac{559}{500}$$ Then we can use power series around $x=610/233$ but it's tedious by hand . How to solve the problem ?,"['power-series', 'trigonometry', 'golden-ratio', 'inequality']"
4729065,Random walk on a k-dimensional grid,"Consider a $k$ -dimensional grid with integer points and let's begin our random walk at the origin $(0,0,\dots,0)$ . Every step you have to move on each cartesian axis in the following way: $x_1$ axis: one step forward with probability $\frac{1}{3}$ , one step backward with probability $\frac{1}{3}$ , remain where you were with probability $\frac{1}{3}$ . $ \vdots $ $x_k$ axis : one step forward with probability $\frac{1}{3}$ , one step backward with probability $\frac{1}{3}$ , remain where you were with probability $\frac{1}{3}$ . So we can define the random discrete vector $U_i=(X_{i1},\dots,X_{ik})$ that register the random actions taken on each axis at the $i$ -th step of our random walk. The marginal density function of the random variable $X_{ij}$ is: $$
f_{X_{ij}}(t) = \begin{cases} \frac{1}{3} \mbox{ if $t\in\{-1,0,1\}$} \\
0 \mbox{ otherwise}\end{cases}
$$ We would like to compute the following probability: $$
P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr)
$$ as the number of steps $N$ approaches infinity.
So we would like to know the probability after $N$ steps (where $N$ is a sufficiently large number) to be outside the $k$ -dimensional cube with size $2\sqrt{N}$ . $k=1$ case We begin with the 1-dimensional case (a line). We want to compute: $$
P\Bigl(\Bigl|\sum_{i=1}^{N}X_i\Bigr|>\sqrt{N}\Bigr) = P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) + P\Bigl(\sum_{i=1}^{N}X_i<-\sqrt{N}\Bigr) = 2\cdot P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr)
$$ Since the random variables $\{X_i\}_{i=1}^{N}$ are indipendents and equally distributed, and $\mathbb{E}(X_i) = 0$ , $Var(X_i) = \frac{2}{3}$ , then we can apply the central limit theorem, so that: $$
P\Bigl(\sum_{i=1}^{N}X_i>\sqrt{N}\Bigr) = P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}>\frac{\sqrt{N}}{\sqrt{N}\sqrt{\frac{2}{3}}}\Bigr) = 1- P\Bigl(\frac{\sum_{i=1}^{N}X_i}{\sqrt{N}\sqrt{\frac{2}{3}}}\leq\sqrt{\frac{3}{2}}\Bigr) \longrightarrow 1-\phi\Bigl(\sqrt{\frac{3}{2}}\Bigr) \approx 0.1112
$$ where $\phi(t) = \frac{1}{\sqrt{2\pi}}\int_{\infty}^{t}e^{-\frac{s^2}{2}}ds$ is the cumulative distribution function of a standard normal variable ( $Z\sim\mathcal{N}(0,1)$ ). So the probability requested is $2\cdot 0.1112 = 0.2224$ . $k$ -dimensional case Call $P\Bigl(\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr) = p = 0.2224$ which is the same for all axes ( $p$ is indipendent from the index $j$ ). $$
P\Biggl(\bigcup_{j=1}^{k}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \sum_{j=1}^{k}p - \sum_{j<m}^{k}p^2 + \dots + (-1)^{k+1}p^k = \sum_{h=1}^{k}(-1)^{h+1}\binom{k}{h}p^h = 1-(1-p)^{k}
$$ Where we have applied the fact that the random variables $\Bigl|\sum_{i=1}^{N}X_{i1}\Bigr|,\dots,\Bigl|\sum_{i=1}^{N}X_{ik}\Bigr|$ are indipendent, so for example in the second term: $$
P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\cap\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Bigr)P\Bigl(\Bigl\{\Bigl|\sum_{i=1}^{N}X_{im}\Bigr|>\sqrt{N}\Bigr\}\Bigr) = p^{2}.
$$ Limit case: $k\to\infty$ We can now easily compute the following probability: $$
P\Biggl(\bigcup_{j=1}^{\infty}\Bigl\{\Bigl|\sum_{i=1}^{N}X_{ij}\Bigr|>\sqrt{N}\Bigr\}\Biggr) = \lim_{k\to\infty} 1-(1-p)^{k} = 1
$$ since $p=0.2224\in(0,1) \implies (1-p)\in(0,1)$ . So the probability of escaping from a $k$ -dimensional grid of size $2\sqrt{N}$ as $N\to\infty$ and $k\to\infty$ approaches the value 1! My question: Do you think that this process is correct? I first applied the CLT and then used the indipendence of these random variables (since each step our ""point"" has to make a move across all the axes, and the movement made on the axis $x_i$ does not influence the movement made on the axis $x_j$ ). Table of values: $k=1 \implies P = 0.2224$ ; $k=2 \implies P = 0.3978$ ; $k=3 \implies P = 0.5327$ ; $\vdots$","['probability-limit-theorems', 'random-walk', 'probability']"
4729084,Closed form for $\sum_{n=1}^{\infty}\frac{(2\log\phi)^{2n+3}B_{2n}}{2n(2n+3)!}$,"I need a closed form for the sum $$\sum_{n=1}^{\infty}\frac{(2\log\phi)^{2n+3}B_{2n}}{2n(2n+3)!} $$ where $\phi=\frac{1+\sqrt{5}}{2}$ is the golden ratio and $B_n$ are the Bernoulli numbers. I tried denoting $$S:=\sum_{n=1}^{\infty}\frac{(2\log\phi)^{2n+3}B_{2n}}{2n(2n+3)!} $$ Using the integral representation of $B_{2n}$ we have $$B_{2n}=\frac{4(-1)^n n (2n-1)}{(2\pi)^{2n}}\int_{0}^{1}\frac{\log(1-t)\log^{2n-2}(t)}{t}\ dt $$ So we get by inserting this above representation of $B_{2n}$ in $S$ and interchanging the order of sum and integral (which needs to be justified) we get $$S=4\int_{0}^{1}\frac{\log(1-t)}{t\log^2 t}\left(\sum_{n=1}^{\infty}\frac{(-1)^n (2n-1)}{(2n+3)!} \left(\frac{\log\phi\log t}{\pi}\right)^{2n}\right)\ dt $$ Now we have ( see here ) $$\sum_{n=1}^{\infty}\frac{(-1)^n(2n-1)a^{2n}}{(2n+3)!}=\frac{a^3-6a\cos a-18 a+24\sin a}{6a^3} $$ So putting $a=\frac{\log\phi\log t}{\pi}$ we obtain $$S=4\int_{0}^{1}\frac{\log(1-t)}{t\log^2 t}\left(\frac{1}{6}-\frac{\pi^2\cos (\frac{\log\phi\log t}{\pi})}{\log^2\phi\log^2 t}-\frac{3\pi^2}{\log^2\phi\log^2 t}+\frac{\pi^3\sin (\frac{\log\phi\log t}{\pi})}{\log^3\phi\log^3 t}\right)\ dt $$ The above integral is convergent as I tried on Mathematica cloud with the code NIntegrate[Log[1-t]/(t Log[t]^2)*(1-6Cos[Log[GoldenRatio]*Log[t]/Pi]/(Log[GoldenRatio]*Log[t]/Pi)^2-18/(Log[GoldenRatio] Log[t]/Pi)^2+24 Sin[Log[GoldenRatio]*Log[t]/Pi]/(Log[GoldenRatio] Log[t]/Pi)^3), {t, 0, 1}] The answer I got is 0.00184137 I also know that $$\coth x=2\sum_{n=1}^{\infty}\frac{B_{2n}(2x)^{2n-1}}{(2n)!} \ \ \ \ \, 0<|x|<\pi$$ Now $\log\phi <1<\pi $ , but I am unable to use this above formula either by integrating or differentiating it. Any help would be highly appreciated.","['summation', 'bernoulli-numbers', 'generating-functions', 'trigonometry', 'sequences-and-series']"
