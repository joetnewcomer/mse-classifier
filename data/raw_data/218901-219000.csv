question_id,title,body,tags
4474096,Distribution of N balls in N buckets in descending order?,"Let's suppose I have $N$ buckets and $N$ balls, and I add a ball to a uniformly random bucket, N times. (So, for example, it's possible each bucket contains one ball.  It's also possible that one of the buckets has N balls, and the others have 0.) After this, lets suppose I rearrange the buckets in a descending order of how many balls they hold, and label (one of) the buckets with the most balls 1, the next 2 and so on until N is (one of) the buckets with the least balls. What is the expected value $E_N(i)$ of the number of balls that the ith bucket has? For $N=1$ its trivial: $$
E_1(1) = 1
$$ For $N=2$ , either they will fall 1-1 or they will fall 2-0, both with equal probability, so: $$
    E_2(1) = (2 + 1) / 2 = 1.5\\
    E_2(2) = (0 + 1) / 2 = 0.5
$$ Is there a general solution for $E_N(i)$ ?  and what about as N approaches infinity? Does the distribution/function have a name?","['statistics', 'combinatorics', 'balls-in-bins', 'probability']"
4474097,$\int_0^1 |F_X^{-1}(q) - F_Y^{-1}(q)| dq = \int_\mathbb R |F_X(x) - F_Y(x)| dx$,"Let X,Y be one-dimensional variables. I would like to prove that fact that $$\int_0^1 |F_X^{-1}(q) - F_Y^{-1}(q)| dq = \int_\mathbb R |F_X(x) - F_Y(x)| dx$$ where $F_X$ denotes cumulative distribution function of variable $X$ and $F_X^{-1}$ denotes quantile function. My work so far For sure this equality can be proven by only changing the variables in integrals. I tried to do $q = F_X(x)$ , then $dq = f(x) dx$ , where $f(x)$ is a probability density function. Therefore; $$\int_0^1 |F_X^{-1}(q) - F_Y^{-1}(q)| = \int_{\mathbb R}|(x - F_Y^{-1}(F_X(x)))|f(x)dx$$ which unforunetly is not exacly what I was looking for. Could you please give me a hand in proving this fact? EDIT I found out, that those integrals are equal to area between graphs $F_X$ and $F_Y$ , however I'm not sure how it can be proven.","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4474103,How can a cylinder be rotated at any angle $\theta$ about the origin using cartesian coordinates? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How can a cylinder be rotated at any angle $\theta$ at origin using cartesian coordinates? What would be the equation for a cylinder (first figure) rotated about the origin?","['calculus', 'trigonometry']"
4474143,What is the intuition behind distributional derivative and why distributional derivative is useful?,"Why we study distributional derivative? Let $\Omega\subset \Bbb{R}^n$ be any open set. $D(\Omega)=C_c^{\infty}(\Omega) $ : Linear space of test functions i.e smooth functions with compact support. $D'(\Omega) $ : Continuous dual of $D(\Omega) $ For $f\in D'(\Omega) $ we define distributional derivative of $f$ , $D^{\alpha}f$ or $\partial^{\alpha}f$ by $$\langle\partial^{\alpha}f,\varphi\rangle=(-1)^{|\alpha|}\langle f,\partial^{\alpha}\varphi\rangle$$ There are locally integrable function which is not differentiable in classical sense but the regular distribution generated by the locally integrable function possess distributional derivative. What is the intuition behind distributional derivative and why distributional derivative is useful? Can you explain some application where we need some sort of differentiation but classical differentiation is no longer useful?","['applications', 'distribution-theory', 'real-analysis', 'functional-analysis', 'derivatives']"
4474229,Solve sinusoidal function algebraically,"Can someone give me a hint on the following question
solve the sinusoidal function that have a y value that is 1 unit greater than its minimum during the first two cycles of function $Y=-1.2(\cos(-0.99x)-6)+4.45$ I have found out the minimum of this function which is $-3.95$ I'm not getting how this function has $2$ cycles ? If we already know the value of $Y$ (which should be $-2.95$ I think), then what are we going to solve?","['algebra-precalculus', 'functions', 'problem-solving', 'trigonometry']"
4474253,Special case of multi variable integration with natural log result?,"We know the famous equation $PV = nRT$ but in thermodynamics we typically deal with differentials $P\,dV$ and even $V\,dP$ at times. Given: $$
d(PV) = V\,dP + P\,dV
$$ integrating both sides $$
PV = \int \frac{nRT}{P}\,dP + \int \frac{nRT}{V}\,dV 
$$ Typically I'm assuming a decoupling constant can be used to decouple the factors particularly $P$ and $V$ . In fact physically the system is not defined for the first 2 variables which we can assume are independent. So the number of moles can't change and the temperature can also be assumed independent. After integration assuming $n$ and $T$ are independent: $$
PV = nRT \cdot \ln(PV)
$$ Is this a special result that should be memorized in multivariable calculus regarding a natural log of the same compound variable on the left?","['integration', 'multivariable-calculus', 'calculus', 'derivatives']"
4474257,Sequences with Tetrahedral Stacks of Oranges,"A grocer stacks oranges into a tetrahedral pyramid (i.e, a pyramid with a triangular base and
3 triangular sides). Let Cn be the total number of oranges where n is the height (number of levels)
in the pyramid. (C1 = 1, C2 = 4, C3 = 10, etc.). Give a closed-form formula for Cn and report the
value for C100. This is what I have tried so far but I am unsure how correct this is: Tn = (n * (n + 1) * (n + 2)) / 6 The proof uses the fact that the nth tetrahedral
the number is given by, Trin = (n * (n + 1)) / 2 It proceeds by induction. Base Case T1 = 1 = 1 * 2 * 3 / 6 Inductive Step Tn+1 = Tn + Trin+1 Tn+1 = [((n * (n + 1) * (n + 2)) / 6] + [((n + 1) * (n + 2)) / 2] Tn+1 = (n * (n + 1) * (n + 2)) / 6","['discrete-mathematics', 'sequences-and-series']"
4474322,Do the primes form the least (infinite) primitive set?,"I just learned about primitive sets from the recent Numberphile video , and I'm having some difficulty with my intuition for these sets. For reference, a set $S \subseteq \mathbb{N}_{\geq 2}$ is called primitive if no element of $S$ divides any other. I'm particularly concerned with infinite primitive sets. Any infinite subset $S$ of $\mathbb{N}$ can of course be uniquely ordered as a strictly increasing sequence $(a_n)_{n \in \mathbb{N}}$ , and infinite sequences can be compared using the lexicographic order . In this way, we can define a partial order on infinite subsets of $\mathbb{N}$ . In particular, we can consider this to be a partial order on the set of infinite primitive sets. My question is: is the set of primes a least element in this partial order? It seems to me that you can argue that this is true by induction. If you choose your sequence term by term, and you always pick the smallest term possible, then you'll always be picking the primes, since the elements that come before are always made up of your previous choices. However, if this is true, then it seems to me that you could just prove the Erdös primitive set conjecture by applying the series comparison test. This conjecture (now proven) states that if $S$ is a primitive set, then $\sum_{n \in S} 1/n\log(n) \leq \sum_{p \text{ prime}} 1/p\log(p)$ . If the set of primes is minimal as I asked above, then since $1/n\log(n)$ is decreasing, the above inequality should be true. But, this is far too simple to be missed as a proof for 30 years. So, if the answer to my first question is ""yes"", then why doesn't this work as a proof?","['number-theory', 'order-theory', 'solution-verification', 'sequences-and-series']"
4474324,Are equilateral polyhedra with triangular faces rigid?,"For the purposes of this question, a polyhedron has triangular facets. Convex polyhedra are rigid by Cauchy’s theorem. Steffen’s polyhedron is an example of a non-convex polyhedron that is flexible (i.e., non-rigid). However, it appears to have edges of different lengths. My question: are there equilateral flexible polyhedra or are all equilateral polyhedra rigid? Motivation: I have a bars-and-balls magnetic construction set and I would like to build a flexible polyhedron. But all the bars I have are equal in length.","['polyhedra', 'geometry']"
4474386,Power or polynomial function?,"According to the definition, $f(x) = a·x^n$ is a power function. If we shift it to $f(x) = a·(x - c)^n$ or, more general, to $f(x) = a·(x - c)^n + d$ , it becomes a polynomial function (not a power function anymore). Is this just a matter of mere formalism or nomenclature?",['functions']
4474533,Find the minimum value of a trigonometric function,"If the minimum value of $f\left(x\right)=\left(1+\frac{1}{\sin ^6\left(x\right)}\right)\left(1+\frac{1}{\cos ^6\left(x\right)}\right),\:x\:∈\:\left(0,\:\frac{\pi }{2}\right)$ is $m$ , find $\sqrt m$ . How do I differentiate this function without making the problem unnecessarily complicated? If there are any other methods to finding the minimum value I am open to those too.","['maxima-minima', 'calculus', 'derivatives', 'inequality']"
4474627,Count pairwise coprime triples such that the maximum number of the triple is not greater than N,"Problem Statement: Given N you are to count the number of pairwise coprime triples which satisfy $1≤a,b,c≤N$ . Example: For example N=3,
valid triples are (1,1,1),(1,1,2),(1,2,1),(2,1,1),(1,1,3),(1,3,1),(3,1,1),(1,2,3),(1,3,2),(2,1,3),(2,3,1),(3,2,1),(3,1,2) Hence answer for N=3 is 13. Source: own problem. Solution so far: I have found the oeis series A256390 but Reyna & Heyman's work seems too overwhelming for me to understand.If someone can explain that in simple terms with an example it would be great or if you have an alternate solution you are welcome too. I have found a solution. I will move the following to answer once I have better understanding. For now documenting my understandings here: I found a code ( modified )
This is a deterministic approach and had nothing to do with Reyna & Heyman's approximation.
For N=6 answer should be 64 . Now let's see his implementation. He's constructing graph with just 7 edges I:mu[1] * f[1] * f[1] * f[1]=216
I:mu[2] * f[2] * f[2] * f[2]=-27
I:mu[3] * f[3] * f[3] * f[3]=-8
I:mu[4] * f[4] * f[4] * f[4]=0
I:mu[5] * f[5] * f[5] * f[5]=-1
I:mu[6] * f[6] * f[6] * f[6]=1
3B:(mu[1] * f[2] + mu[2] * f[1]) * f[2] * f[2]=-27
3B:(mu[1] * f[3] + mu[3] * f[1]) * f[3] * f[3]=-16
3B:(mu[1] * f[5] + mu[5] * f[1]) * f[5] * f[5]=-5
3B:(mu[1] * f[6] + mu[6] * f[1]) * f[6] * f[6]=7
3B:(mu[2] * f[3] + mu[3] * f[2]) * f[6] * f[6]=-5
3B:(mu[2] * f[6] + mu[6] * f[2]) * f[6] * f[6]=2
3B:(mu[3] * f[6] + mu[6] * f[3]) * f[6] * f[6]=1
6A:mu[3] * mu[2] * mu[1] * 1 * 3 * mark[1]=6
6A:mu[6] * mu[2] * mu[1] * 1 * 3 * mark[1]=-3
6A:mu[6] * mu[3] * mu[1] * 1 * 2 * mark[1]=-2
6A:mu[6] * mu[3] * mu[2] * 1 * 1 * mark[2]=1 Ans=(216-27-8+0-1+1)+3*(-27-16-5+7-5+2+1)+6*(6-3-2+1)=64 gcd(a,b,c)=1 Case: I have labelled 3 sections of the program with I: , 3B: , 6A: tags.
For gcd(a,b,c)=1 problem I: tags are after which we stop.
We don't need 3B: or 6A: tags. We neither need to construct the graph nor traverse it. And it indeed clocks less than a sec for n=1000000. But for pairwise gcd=1 problem we need to exclude/include more scenarios.So lets proceed. gcd(a,b)>1 or gcd(b,c)>1 or gcd(c,a)>1 Case: For each of the items labelled 3B: he adds an edge from first mu node to first f node with weight equal to either of first f[i] value outside the first pathesis. in 3B:(mu[1] * f[2] + mu[2] * f[1]) * f[2] * f[2] (mu[1] * f[2] + mu[2] * f[1]) part is for selecting an item in a column which is not multiple of 2 .wile the other two columns have multiple of 2 . 3 in 3B is C(3 1) for permuting this column either at start , middle or end. Essentially for all pair of factors of i we draw edge from smaller factor to larger factor with weight f[i] .If the edge already exist we skip that.
For 6 all pair of factors are: 1,2  =>already edge exists with weight f[2] so will skip
1,3  =>already edge exists with weight f[3] so will skip
1,6  =>draw edge with weight f[6]
2,3  =>draw edge with weight f[6]
2,6  =>draw edge with weight f[6]
3,6  =>draw edge with weight f[6] Tiangle counting: Note Neither for I: labeled values nor for 3B: labeled values the graph is required. The graph is being constructed for 6A: values mark[c] denotes wheather it has been traversed earlier as first distance. if yes it has value equal to weight of edge used to traverse to it. Before traversing node a , for each edge incident on a we mark the source node in by weight and clear it once traversing a case is complete. So mark[c] to have a valid value node x has to be node a . Hence a triangle. w1 * w2 * mark[c] is equivalent to w1 * w2 * wc The overall Time complexity = O(n+E + E^1.5 ) = $O(n+nlogn+(nlogn)^{1.5})$ for n=1000000, its around = $O(n+10n+430n)$ = $4*10^8$ Since median of factors of a number is its square root we managed to get a sparse graph, so time complexity is order of $(nlogn)^{1.5}$ .
If graph was dense ie $E=n^2$ then triangle finding is $O(n^3)$ when we arrive at complexity similar to @mike-earnest 's answer. Conversely if you substitute $n^{0.5}$ for $n$ in @mike-earnest 's answer you get complexity similar to this.","['graph-theory', 'number-theory', 'mobius-function', 'combinatorics', 'mobius-inversion']"
4474695,"If $L=\lim_{m\to \infty}\sum_{p=1}^m \frac{p}{2p+m+m^2}$, then find floor(L).","If $L=\displaystyle\lim_{m\to \infty}\sum_{p=1}^m \frac{p}{2p+m+m^2}$ , then find floor(L). This is from a preparatory examination for the college entrance exams for high-school students. I first tried to convert it into a definite integral by the conventional way, but the $m^2$ term kept interfering. Now the only other thing that I could think of was the sandwich theorem or the squeeze theorem, for which also I could think about only the upper bound. This was $$L\leq \lim_{m\to \infty}\sum_{p=1}^m \frac{p}{2p+m}= \int_0^1\frac{xdx}{2x+1}=\frac{2-\ln 3}{4}$$ while this gives me the correct value of the floor function, but the solution claims that $L=\frac 12$ .","['limits', 'calculus', 'summation']"
4474754,Recurrence relations with non-constant coefficients - Book Recommendations,"I've recently realised that recurrence relations (difference equations) can be quite powerful, especially when you start using non-constant coefficients... Can anyone recommend any good books that go deep into recurrence relations from the beginning to more complicated order equations with non-constant coefficients. Ideally with questions and answers, and reference to finance/economics. Why do university courses often have a whole course on differential equations but not an equivalent for recurrence relations. Given that they are often taught in the early stage as kind of analogous to each other, e.g. the similarity of appearance and technique of solving second-order equations...it kind of feels like they would both go on to be equally important. My knowledge is limited so please forgive me if any of this seems trivial, thanks!","['algebra-precalculus', 'recurrence-relations', 'ordinary-differential-equations']"
4474762,CW Complexes are Locally Path Connected: Rotman's Proof,"I found Rotman's proof rather fiddly, and my question is why he doesn't do it in the following easier way (probably because I am missing something): He has already proved (actually, the proof is left to exercises, but these come before his proof of local path connectedness),  that if $X$ is a $CW$ complex then $1),\ $ there is an increasing sequence $\{X^n\}_{n\ge 0}$ of sets whose union is $X$ , ( $X$ has the weak topology induced by the $X^n), \ X^0$ is discrete and $2).\ $ for each integer $n$ there is a family of continuous functions $\{f_{\alpha}^{n-1}: S^{n-1}\to X^{n-1}\}_{\alpha\in I}$ such that $X^n=\left(\coprod_{\alpha} D^n\right)\coprod_f X^{n-1}$ where $f=\coprod_{\alpha} f_{\alpha}^{n-1}.$ In particular, $2). $ says that $X^n$ is obtained from a quotient of the disjoint union of $\coprod_{\alpha} D^n$ and $X^{n-1}.$ Now, $X^0$ is (trivially) locally path connected, and it's easy to show that the disjoint union of locally path connected spaces is locally path connected, so if we can show that the quotient of a locally path connect space is locally path connected, then an induction gives what I believe is an alternative, easier proof. Or am I missing something? But this is not hard: Suppose $p:X\to Y$ is a quotient map and $X$ is locally path connected. Take an open $V\subseteq Y$ and let $C$ be path component of $V.$ If $x\in p^{-1}(C)$ then $x\in p^{-1}(V)$ also so there is an open path connected $U\subseteq p^{-1}(V)$ containing $x$ (since $X$ is locally path connected). Now, if $u\in U$ there is a path $\varphi: [0,1]\to U$ such that $\varphi(0)=x$ and $\varphi(1)=u.$ It follows that $p\circ \varphi:[0,1]\to V$ is a path such that $p\circ \varphi(0)=p(x)$ and $p\circ \varphi(1)=p(u).$ But $p(x)\in C$ and this forces $p(u)\in C$ as well, so that in fact $\varphi([0,1])\subseteq p^{-1}(C).$ But then $u\in p^{-1}(C).$ Putting all this together, we see that $U\subseteq p^{-1}(C)$ , which means that $p^{-1}(C)$ is open in $X$ and therefore that $C$ is open in $Y$ (since $Y$ has the quotient topology). So $C$ is an open, path connected set contained in $V.$ We conclude that $Y$ is locally path connected.","['geometric-topology', 'general-topology', 'cw-complexes', 'algebraic-topology']"
4474837,Tangent to $x^2+y^2-6x-6y=-13$ and $x^2+y^2+2x+2y=-1$,"Considering the circles $\lambda: x^2+y^2-6x-6y=-13$ and $\theta: x^2+y^2+2x+2y=-1$ find the line simultaneously tangent to them. I found the implicit derivative of those two, $\lambda: y'=-\frac{x-3}{y-3}$ and $\theta: y'=-\frac{x+1}{y+1}$ but then I don't know how to proceed. I thought that setting $y'_{\lambda}=y'_{\theta}$ would solve, but it didn't. If anyone could help I'll appreciate.","['analytic-geometry', 'calculus', 'circles', 'tangent-line']"
4474894,Why do we assume the discriminant to be greater than or equal to zero while calculating the range of a function?,"Since I don't have enough reputation to comment, I am asking this question again. I cannot understand why we can assume that the quadratic has real roots and then say $D\ge0$ . The answer states that $b^2 - 4ac = (b + 2ax)^2$ and since this is a squared term it satisfies for all $x$ belonging to $\mathbb R$ . But that is the question we began with, how do we know that $x$ is real. Plugging a non-real value of $x$ does not satisfy the relation of $D$ .","['functions', 'quadratics', 'polynomials', 'discriminant']"
4474930,Orthonormal basis given by the integral of a vector field along a curve orthogonal to the tangent,"A friend and I are trying to understand the construction on section 3.2 of this paper by Bernatzki and Ye. Suppose $\gamma$ is a smooth simple closed curve in $\mathbb{R}^3$ . Then this paper claims there exist vector fields $v_i: \gamma \to \mathbb{R}^3$ , $i = 1,2,3$ , such that $v_i \perp \dot{\gamma}$ everywhere, and the three vectors $V_i = \int_{\gamma} v_i(s) ds$ , $s$ being the arc-length parameter, form an orthonormal basis. Furthermore, we can make the support of the $v_i$ arbitrarily small. Why is this true? We know that each $v_i$ can be written as a linear combination $f_iN + g_i B$ , with $(T,N,B)$ being the Frenet frame, $f_i$ and $g_i$ being functions, but the integrals $\int f_iN + g_i B$ don't seem to have particularly nice forms, even after applying the Frenet-Serret equations and integration by parts. I think almost any choice of $v_i$ should yield $V_i$ forming a basis of $\mathbb{R}^3$ , but there seems to be difficulty in proving this rigorously.","['curves', 'vector-bundles', 'vector-analysis', 'differential-topology', 'differential-geometry']"
4474931,Stability considerations for a special type of LTV systems,"I have been considering the stability of an (nonautonomous) LTV system $\dot{x} = A(t)x$ of the form $\begin{bmatrix}\dot{x}_{1}\\ \dot{x}_{2} \end{bmatrix} = \begin{bmatrix}-a(t) & -b(t)\\ 1 & 0 \end{bmatrix} \begin{bmatrix}x_{1}\\x_{2}\end{bmatrix}$ , where we know that $a(t), b(t) > 0$ and both $a(t), b(t)$ are both continuous, differentiable, and bounded. By simulating such a system, it clearly seems to be stable, but I do not quite see how to prove that this is true and which restrictions that apply. By computing the eigenvalues we get $\lambda_{1,2}(t) = \frac{-a(t) \pm \sqrt{a(t)^{2} - 4b(t)}}{2} < 0\; \forall\; t $ , with the restrictions we placed on $a(t), b(t)$ . However, as emphasized by, e.g., Khalil (Nonlinear Systems (2002), Section 4.6), the eigenvalues of $A$ cannot be used to assert stability of such a system. If we instead fix $a, b$ , then, by the same eigenvalues, the (autonomous) LTI system $\dot{x} = Ax$ is stable as long as $a, b > 0$ . I know that asserting the stability of general LTV systems is difficult, and from what I can see, most of the literature refer to numerical integration to find the transition matrix of the system. But is there really no other way to discuss the system shown above? The most ""obvious"" alternative that comes to mind is to find an appropriate Lyapunov function candidate, but I do not seem to be able to come up with a good candidate (?)","['linear-algebra', 'linear-control', 'ordinary-differential-equations', 'control-theory']"
4474942,Why is the multiplication map $C_c^\infty \times C^\infty \to C_c^\infty$ not continuous?,"Let $C_c^\infty(\mathbf{R}^d)$ be the family of all compactly supported smooth functions equipped with the test function topology. Let $C^\infty_{\text{loc}}(\mathbf{R}^d)$ be the class of all smooth functions, equipped with the topology of locally uniform convergence of the function and all it's derivatives. It is stated in Treves, Chapter 41 that the multiplication operator $$ C^\infty_c(\mathbf{R}^d) \times C^\infty_{\text{loc}}(\mathbf{R}^d) \to C_c^\infty(\mathbf{R}^d) $$ is not continuous. It is certainly separately continuous, and  sequentially continuous in each variable. But I can't find a resource that proves that the map is not continuous. Why is this the case?","['topological-vector-spaces', 'functional-analysis', 'distribution-theory']"
4474961,How can we know how many times a number is divisible by 2?,For example 36: 36/2=18 18/2=9 So for 36 it is divisible by 2 2 times. For 2ⁿ×(2k-1) it is divisible by 2 n times. But how do I know for 3k-2? Is there a formula to find how many times a number is divisible by 2?,"['divisibility', 'discrete-mathematics']"
4474964,"Hirsch, Smale, and Devaney's ""Differential Equations, Dynamical Systems, and an Introduction to Chaos"" - polar coordinates","To study the system $$x' = −y +\epsilon x(x^2 + y^2)\\
y' = x +\epsilon y(x^2 + y^2),$$ the authors perform a change of coordinates, getting $$r'=\epsilon r^3\\
\theta'=-1.$$ So, they state that Thus when $\epsilon>0$ , all solutions spiral away from the origin, whereas when $\epsilon<0$ , all solutions spiral toward the origin. I cannot see this because, trying to solve the firt ODE, I found $$r^2(t) = \dfrac{1}{2(c-\epsilon t)}.$$ If $\epsilon>0$ , the $r$ is not real for $t$ large. Thank you in advance!","['stability-in-odes', 'polar-coordinates', 'ordinary-differential-equations', 'dynamical-systems']"
4474965,Find $\mathbb{E}\left[X|Y=\frac{1}{4}\right]$.,"Let $X,Y$ be random variables with joint density given by $$f(x,y)=\begin{cases}
                    \frac{3}{8}\left ( x+y^{2} \right ) & \text{ if } 0<x<2,\text{ }0<y<1 \\
                    0 & \text{ otherwise.}
                \end{cases}$$ Find $\mathbb{E}\left[X|Y=\frac{1}{4}\right]$ . $\textbf{My attempt:}$ $$
\begin{align*}
\mathbb{E}\left[X|Y=\frac{1}{4}\right]
&=\int_{0}^{2}xf\left(x,\frac{1}{4}\right)dx\\
&=\frac{3}{8}\int_{0}^{2}x\left(x+\frac{1}{16}\right)dx\\
&=\frac{67}{64}.
\end{align*}$$ Why is this wrong?","['conditional-probability', 'conditional-expectation', 'solution-verification', 'probability-theory', 'probability']"
4474996,Questions regarding the notation used when referring to a set of ordered pairs,"First let's define a set of ""normal"" elements (i.e. not a set of ordered pairs): $A=\{x\in \mathbb{N}:x>10\}$ Let's call it the ""first"" definition. I can also define set A and its elements' properties in another way: $\forall x(x\in A \iff x\in \mathbb{N} \land x>10)$ I can call this the ""second"" definition, and I know that it is equivalent to the first one. Now let's try a set of ordered pairs. $B=\{(x,y)\in \mathbb{N}\times\mathbb{N}:x>y\}$ So far so good. But I am unsure about how to proceed with the second ""definition"" similiar to the first example. Here is what I have tried: $\forall x(x \in B \iff x\in \mathbb{N}\times\mathbb{N} \land x_1>x_2)$ , where $x_1$ and $x_2$ refer to the first and second element of the ordered pair, though it does feel ""informal"" to be refering to them in such a way. $\forall x \forall y((x,y)\in B \iff (x,y)\in \mathbb{N}\times\mathbb{N} \land x>y)$ is another thing I've tried. This one sounds more formal but less intuitive, as I haven't even defined a variable that refers to the element (ordered pair) of set B. Are my methods correct? Is there something I can improve, or another method I haven't thought of? Am I completely missing the point? Sorry if this is a dumb question, and thank you in advance!","['elementary-set-theory', 'notation']"
4475028,When is an image of a Hopf algebra a Hopf algebra?,"Suppose $R\subseteq S$ is a flat extension of rings and $A/R$ , $B/S$ are flat Hopf algebras. Let $\varphi:A\otimes_R S\to B$ be a surjective $S$ -Hopf algebra homomorphism. When is it the case that $\varphi(A)=\varphi(A\otimes 1)$ is a Hopf algebra over $R$ such that $\varphi(A)\otimes_R S=B$ as $S$ -Hopf algebras? (Rephrasing: suppose $Y\to X$ is a flat morphism of schemes, $G/Y$ and $H/X$ are flat affine group schemes, and we have a closed inclusion $G\to H\times_X Y$ of group schemes over $Y$ . When does this come from pulling back some inclusion $\mathcal{G}\to H$ ? My main motivation for considering this question is the case $R$ is a Dedekind domain, $S=\mathrm{Frac}(R)$ , and $H=\mathrm{GL}_n$ , to obtain integral models from faithful representations.) If we suppose $\varphi(A)/R$ is flat and $S$ is a localization of $R$ (so that $-\otimes_R-=-\otimes_S-$ ), then I believe this follows by using the following fact three times to construct the structure maps: in any Abelian category, a diagram of the form has a unique solution. Commutativity of the diagrams necessary to make this a Hopf algebra then comes from, say, using flatness assumptions to embed everything into the corresponding commutative diagrams associated to $B$ . However, without the assumption that $S/R$ is a localization, this fails: for example, it is not too hard to construct counterexamples for $S/R$ a quadratic étale cover. This is not too surprising: for example, if $L/K$ is a finite field extension and $G/K$ is an affine algebraic group, not every subgroup of $G\otimes_K L$ necessarily comes from a subgroup of $G$ . In situations like this, can we say anything? Note by ""ring"" I mean commutative ring and by ""Hopf algebra"" I mean commutative Hopf algebra (but not necessarily with commutative comultiplication).","['algebraic-groups', 'representation-theory', 'algebraic-geometry', 'hopf-algebras', 'commutative-algebra']"
4475057,Understanding counting using multinomial coefficients,"I'm studying Chapter 1 of Ross A First Course in Probability Theory (8th Edition) and I'm grappling with multinomial coefficients. All given examples come from this chapter. Specifically $${n \choose n_1 ... n_r}=\frac{n!}{n_1! ... n_r!}$$ gives the number of ways to choose groups of objects of sizes $n_1, ..., n_r$ where $\sum_{i=1}^{i=r} n_i = n$ . Then we have the following 3 examples: 10 officers are to be divided as follows. 5 on patrol, 2 at the station and 3 in reserve. In how many ways can this be done? The answer is of course $$\frac{10!}{5!2!3!}$$ 10 kids are to be divided into two teams A and B of size 5 each where each team will play in a separate division. In how many ways can this be done? Again, the answer is similar to what we'd expect $$\frac{10!}{5!5!}$$ 10 kids divide themselves up into two teams of 5 to play basketball at the playground. In how many ways can this be done. This is where my confusion begins . The example says the answer is $$(\frac{10!}{5!5!})/2!$$ because even though this looks like the previous problem, it is different since the order doesn't matter here. Firstly, it looks exactly the same. I do not see why order matters in EITHER of examples # $2$ and # $3$ . Secondly, example # $1$ looks exactly like the situation of example # $2$ except with $3$ groups instead of $2$ so, if order mattered in example # $2$ , then it should have mattered in example # $1$ , no?. So, my question : What am I missing here? Any feedback is much appreciated.","['combinatorics', 'probability', 'multinomial-coefficients']"
4475071,Can we say $f(x)=\frac{x}{\ln(x^2)}$ has a removable discontinuity at $x=0$?,I came across a function $$f(x)=\frac{x}{\ln(x^2)}$$ It is evident that $f(0)$ is undefined. But we have: $$\lim _{x \rightarrow 0^{+}} f(x)=0=\lim _{x \rightarrow 0^{-}} f(x)$$ So is $x=0$ a removable discontinuity?,"['limits', 'algebra-precalculus', 'functions', 'continuity']"
4475082,Three-of-a-kind Poker Hand Problem,"Problem: Three-of-a-kind poker hand: Three cards have one rank and the remaining two cards have
two other ranks. e.g. {2♥, 2♠, 2♣, 5♣, K♦} Calculate the probability of drawing this kind of poker hand. My confusion: When choosing the three ranks, the explanation used $13 \choose 1$ and $12 \choose 2$ . I used $13 \choose 3$ instead which ends up being wrong. I do not know why.","['combinations', 'combinatorics', 'poker']"
4475119,"N-queen examples - Discrete Mathematics, Kenneth Rosen (pg-33)","From the first paragraph and Q1 we see that the author chose i is a row and j column in p(i,j)
Then in Q2, we see that innermost loop is of k, which designates row of second coordinate in Q2. The description specifies that Q2 asserts that there is at most one queen in each row, but innermost loop makes conjunction of the column values from j+1 to n . How does that make sense?
I have a sense that Q2 asserts for columns, not rows. It's the opposite, assertion works for rows, at most 1 queen in a row. I'm trying to wrap my head around this, maybe write down what ""loop"" would create but not sure where to begin. Thank you. Edit:
[ Full example ] 3","['propositional-calculus', 'discrete-mathematics']"
4475136,How to prove $s > \frac{11}{2}t-3t\ln t$ in this problem,"The problem is: Given $f(x)=\frac{\ln x}{x}$ , line $l$ is the tangent of curve $y=f(x)$ at $(t,f(t))$ ,  and intersects the curve at another point $(s,f(s))$ where $s<t$ . (1) Find the range of $t$ ; (2) (i) Prove $\ln x\le 1 +\frac{1}{e}(x-e)-\frac{1}{2e^2}(x-e)^2+\frac{1}{3e^3}(x-e)^3$ ; $\quad$ (ii) Prove $s>\frac{11}{2}t-3t\ln t$ . Now I have solved the problem (1) and (2)(i), but I can't figure out the proof of (2)(ii). In problem (1) we can get $t\in(e^{3/2},+\infty)$ , and the intersection is the null point of the function $F(x)=\frac{\ln x}{x}-\frac{1-\ln t}{t^2}(x-t)-\frac{\ln t}{t}$ . Therefore, we have: $\color{red}{\frac{\ln s}{s}-\frac{1-\ln t}{t^2}(s-t)-\frac{\ln t}{t}=0}$ , where $s<t$ and $t>e^{3/2}$ . Then we intend to prove $s>\frac{11}{2}t-3t\ln t$ . Maybe the inequality in (2)(i) is helpful for enlarging and reducing the inequality, but I can't find how to use it properly.","['inequality', 'derivatives']"
4475165,Looking for an alternative and easier way to prove Sylvester's theorem,"My professor provided two enunciates of Sylvester's Theorem, the first is the following: Two symmetric n × n matrices B and C are congruent if and only if the diagonal representations for B and C have the same rank, index and signature . He didn't give a proof of this enunciate but he did give a proof of the following version of Sylvester's Theorem: Given a scalar product $\langle \cdot \rangle$ on the euclidean space V, there exists a basis { $v_1,...,v_n$ } such that the matrix of V with respect to this basis is \begin{pmatrix}
I_p & 0 & 0 \\
0 & -I_q & 0 \\
0 & 0 & 0 \\
\end{pmatrix} However, the proof is a bit ""overwhelming"" and with too many passages, making it quite hard (at least for me) to be able to do it on my own since I get often stuck, so I wondered if any of you knew a more comprehensible way to prove this second enunciate? I tried looking online but this enunciate is apparently very rare to find in books and whatnot...","['matrices', 'inner-products', 'linear-algebra', 'symmetric-matrices']"
4475182,"How are the functions determined for real-world applications (business, population models, etc.) of calculus?","The following problem has been taken from Paul's Online Notes: ""We need to enclose a rectangular field with a fence. We have 500 feet of fencing material and a building is on one side of the field and so won’t need any fencing. Determine the dimensions of the field that will enclose the largest area."" This problem is easy to model in the real-world because the area of the field will depend upon the length x and the width y . Therefore, the area of the field can be modeled as function of either x or y : $A(x) = xy$ Likewise, the perimeter can be modeled as a function with the given constraint: $500 = 2x + y$ You can then find the answer by relating the two functions to each other and taking the derivative to find the critical points. This type of problem makes sense to me because the functions that are required are easy to determine. However, take another problem from Paul's Online Notes, particularly from the section of business applications: An apartment complex has 250 apartments to rent. If they rent x apartments, then their monthly profit in USD is given by the function: $P(x)=-8x^2 +3200x-80000$ How many apartments should they rent to maximize their profits? I do understand the mathematical theory of finding the extrema via critical points, and I also understand the business reasons as to why the maximum profit cannot be equal to the maximum apartments available to rent (i.e. the answer isn't going to be ""all of them""). But if I were the owner of this apartment complex, how would I even determine the original function $P(x)$ in order to actually determine my maximum profit? For other applications such as population modeling, how is the function determined for population growth? I see problems in textbooks such as, ""The population of flies grows at a rate of $e^{2t}-19.23$ .
In how many years will the population..."" How are these functions determined in real-life? And how would they change if something were to happen to the business? For example, how would the function in the apartment complex problems change if they suddenly had 300 apartments available to rent?","['optimization', 'functions', 'exponential-function', 'applications']"
4475222,Prove that the statements (1) and (2) are true.,"Question: Let $u:\mathbb{R}\to \mathbb{R}$ be a twice continuously differentiable function such that $u(0)>0$ and $u'(0)>0$ . Suppose $u$ satisfies $$u''(x)=\frac{u(x)}{x^2+1}\quad \forall x\in \mathbb{R}.$$ Prove that the following statements are true: (1) The function $uu'$ is monotonically increasing on $[0,\infty )$ . (2) The function $u$ is monotonically increasing on $[0,\infty )$ . This was one of the MCQ type question in my exam which I did by exemplifying $u(x)=1+\tan ^{-1}x$ , luckily it made me discard the other options. I am looking for the proofs of above statements. I tried to solve it being a second order ordinary differential equation, couldn't do it with existing methods that I know. Any help? Thanks.","['functions', 'monotone-functions', 'ordinary-differential-equations', 'real-analysis']"
4475229,Group lasso with weighted parameters and L0 norm penalty,"I have explored the following hard problem for a long time. I need some help for the (possibly) final steps. Specifically, \begin{equation}\tag{1}
\min_{\mathbf{x}\in\mathbf{R}^n}\left\{ f(\mathbf{x}):= \frac{1}{2}\|\mathbf{x}-\mathbf{v}\|_2^2 + \lambda\|\mathbf{Ux}\|_2+\lambda_0\|\mathbf{x}\|_0\right\}.
\end{equation} where $\mathbf{v}\in\mathbf{R}^n$ is a fixed vector and $\mathbf{U}=\mathbf{diag}(\mathbf{u})$ with $u_i>0, \forall i=1,\ldots,n$ , and $\lambda, \lambda_0>0$ . Since we have no idea about how many nonzero elements the optimal $\mathbf{x}^*$ has, we define $\Omega^k=\{\mathbf{x}\mid \|\mathbf{x}\|_0=k, \mathbf{x}\in \mathbf{R}^n\}$ to represent the set of all $n$ -dimensional vectors with exact $k$ non-zero elements. We can show that $\mathbf{x}=\mathbf{0}$ if $\mathbf{U}^{-1}\mathbf{v}\le \lambda$ , which is a sufficient but not necessary condition due to the existence of $\ell_0$ penalty. Otherwise, we assume the optimal $\mathbf{x}^*\in\Omega^k$ , then the original problem can be reduced to \begin{equation}\tag{2}
\min_{\mathbf{x}_k\in\Omega^k}\left\{ f(\mathbf{x}_k):= \frac{1}{2}\|\mathbf{x}_k-\mathbf{v}\|_2^2 + \lambda\|\mathbf{U}_k\mathbf{x}_k\|_2+\lambda_0k\right\}
\end{equation} where the subscripts $k$ denote notations with $k$ nonzero entries.  The solution to (2) can be computed by the following fixed point operator (I've shown it has a unique fixed point. You may be very smart to propose a closed-form solution for this group lasso problem with weighted parameters within the group.) $$
\mathbf{x}_k=T(\mathbf{x}_k) = (\mathbf{I}+\frac{\lambda \mathbf{U}_k^2}{\|\mathbf{U}_k\mathbf{x}_k\|_2})^{-1}\mathbf{v}_k.
$$ For convenience, let $\mathbf{A}_k=(\mathbf{I}+\frac{\lambda \mathbf{U}_k^2}{\|\mathbf{U}_k\mathbf{x}_k\|_2})^{-1}$ , then $\mathbf{A}_k$ is a positive definite dianonal matrix with diagonal entries $A_{ki}\in(0,1),\forall i=1,\ldots,n$ . Plugging this into (2) yields \begin{align}\tag{3}
f(\mathbf{x}_k)=& \frac{1}{2}\|\mathbf{A}_k\mathbf{v}_k-\mathbf{v}\|_2^2 + \lambda\|\mathbf{U}_k\mathbf{A}_k\mathbf{v}_k\|_2+\lambda_0k\\
=&\frac{1}{2}\|\mathbf{A}_k\mathbf{v}_k\|_2^2-\mathbf{v}_k^T\mathbf{A}_k\mathbf{v}_k + \lambda\|\mathbf{U}_k\mathbf{A}_k\mathbf{v}_k\|_2 +\frac{1}{2}\|\mathbf{v}\|_2^2+\lambda_0k\\
=&-\mathbf{v}_k^T(\mathbf{A}_k-\frac{1}{2}\mathbf{A}^2_k)\mathbf{v}_k + \lambda\|\mathbf{U}_k\mathbf{A}_k\mathbf{v}_k\|_2 +\frac{1}{2}\|\mathbf{v}\|_2^2+\lambda_0k
\end{align} where $\mathbf{A}_k-\frac{1}{2}\mathbf{A}^2_k\succ\mathbf{0}$ since $A_{ki}\in(0,1),\forall i=1,\ldots,n$ . The intuition for doing this is from this paper ''Neural Network Compression via $\ell_0$ Sparse Group Lasso on the Mobile System'', in which the solution to the following similar but simpler problem is proposed. \begin{equation}\tag{4}
\min_{\mathbf{x}\in\mathbf{R}^n}\left\{ f(\mathbf{x}):= \frac{1}{2}\|\mathbf{x}-\mathbf{v}\|_2^2 + \lambda\|\mathbf{x}\|_2+\lambda_0\|\mathbf{x}\|_0\right\}.
\end{equation} The only difference is that there are no weights for the group lasso term in (4). This problem looks very hard, because we need to try the $2^n$ possibilities to get the minimum objective value. But actually we only need to check $n+1$ possibilities, which is great. The solution is motivated by the following simple derivations. Since we have known the closed-form solution for group lasso, i.e. $\mathbf{x}=(\|\mathbf{v}\|_2-\lambda,0)_+\frac{\mathbf{v}}{\|\mathbf{v}\|_2}=(1-\frac{\lambda}{\|\mathbf{v}\|_2},0)_+\mathbf{v}$ . Suppose $\mathbf{x}^*\in\Omega^k$ , \begin{align}
f(\mathbf{x}_k)=&\frac{1}{2}\|\mathbf{x}_k-\mathbf{v}\|_2^2 + \lambda\|\mathbf{x}_k\|_2+\lambda_0k\\
=&\frac{1}{2}\left\|(\|1-\frac{\lambda}{\|\mathbf{v}_k\|_2})\mathbf{v}_k-\mathbf{v}_k-\mathbf{v}_k^-\right\|_2^2 + \lambda\left\|(\|\mathbf{v}_k\|_2-\lambda)\frac{\mathbf{v}_k}{\|\mathbf{v}_k\|_2}\right\|_2+\lambda_0k\\
=&\frac{1}{2}\left\|-\frac{\lambda}{\|\mathbf{v}_k\|_2}\mathbf{v}_k-\mathbf{v}_k^-\right\|_2^2 + \lambda(\|\mathbf{v}_k\|_2-\lambda)+\lambda_0k\\
=&\frac{1}{2}\lambda^2+\frac{1}{2}\|\mathbf{v}_k^-\|_2^2 + \lambda\|\mathbf{v}_k\|_2-\lambda^2+\lambda_0k\\
=&-\frac{1}{2}\lambda^2 + \lambda\|\mathbf{v}_k\|_2-\frac{1}{2}\|\mathbf{v}_k\|_2^2+\frac{1}{2}\|\mathbf{v}_k\|_2^2+\frac{1}{2}\|\mathbf{v}_k^-\|_2^2+\lambda_0k\\
=&-\frac{1}{2}(\|\mathbf{v}_k\|_2-\lambda)^2+\frac{1}{2}\|\mathbf{v}\|_2^2+\lambda_0k
\end{align} where $\mathbf{v}_k^-=\mathbf{v}-\mathbf{v}_k$ . The above implies that the greater $\|\mathbf{v}_k\|_2$ is, the smaller the objective value is, since the other terms are fixed. Thus, we only need to sort $\mathbf{v}$ by the absolute values of its elements in descending order and go through each (ordered) $\mathbf{v}_k$ . Finally, we compare the minimum of them with $\frac{1}{2}\|\mathbf{v}\|_2^2$ (corresponding to the solution of $\mathbf{0}$ ). For the weighted version, I got stuck at (3). I appreciate any instruction or comments.","['optimization', 'statistics', 'sparsity', 'analysis']"
4475331,"Show that, if you place arbitrarily 5 points on a sphere S, there is a hemisphere of S that contains 4 of them.","I am going through a self teaching journey in mathematics. Right now I am reading Book of Proof, by Richard Hammack, and on the Chapter on Counting, I came across the following exercise: Given a sphere $S$ , a great circle of $S$ is the intersection of $S$ with a plane through its center. Every great circle divides $S$ into two parts. A hemisphere is the union of the great circle and one of these two parts. Show that if five points are placed arbitrarily on $S$ , then there is a hemisphere that contains four of them. $\bf{My\,answer}$ : Let $A=\{p_1,p_2,\ldots,p_5\}$ be a set of arbitrarily placed points on the sphere $S$ . Since 3 points are sufficient to define a plane in $\mathbb{R}^3$ , we can construct a plane $P$ by taking any two elements of $A$ and the center $c$ of $S$ . Without loss of generality, choose the points $p_1,p_2$ and $c$ . The intersection of the plane $P$ with the sphere $S$ is, by definition, a great circle of $S$ and it defines two hemispheres $H_1$ and $H_2$ . Since $p_1$ and $p_2$ are on a great circle of $S$ , both hemispheres $H_1$ and $H_2$ contain $p_1$ and $p_2$ . Now, let's look at the remaining points $p_3,p_4$ and $p_5$ . Notice that for $k\in\{3,4,5\}$ , it follows that $p_k\in H_1$ or $p_k\in H_2$ . Since we have three points to place in two different regions, by the Pigeonhole Principle, it follows that one of $H_1$ or $H_2$ contain at least two of $p_3,p_4,p_5$ . Without loss of generality, assume $H_1$ contains $p_3$ and $p_4$ . By construction, $H_1$ also contains $p_1$ and $p_2$ . Thus, there is a hemisphere of $S$ that contains four of the five arbitrarily placed points on $S$ . ————————————————————————————————————————————- I desperately need some feedback on my proofwriting skills. It feels like my argument is solid, but at the same time, my writing might be somewhat convoluted. (I might have defined something in non standard ways, of even wrong ways) It might be important to note that English is not my native language. Feel free to be very critical. I wanna be able to write proofs acceptably.","['pigeonhole-principle', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4475357,Are $L^p$ spaces hemicompact?,"A topological space $X$ is said to be hemicompact if one can a find a countable family of compact subsets such that every compact subset of $X$ is contained in some set of this family. Is it true that $L^p(\mathbb T^d), p \geq 1$ is hemicompact, where $\mathbb T^d$ is the $d$ -dimensional flat torus? I feel the answer is no but I have no real evidence either way. I am interested in understanding whether $C(X;Y)$ (with the compact-open topology) is metrisable with $X= L^p$ and $Y$ some Polish space. As I understand, a necessary and sufficient condition for this is that $X$ is hemicompact.","['general-topology', 'functional-analysis']"
4475375,Is there a simple abstract reason why a profinite group is an inverse limit of finite groups?,"Let $G$ be a profinite group (defined as a Hausdorff, compact, totally disconnected topological group). Suppose you know that as a profinite set, it's an inverse limit of finite sets. Is there an abstract category-theoretic way to deduce $G$ is an inverse limit of finite groups, or must the proof have more content? I know how to prove it using more content, but I've been advised (if I understood correctly) that there's a simple abstract reason. I can't work out what it is.","['limits-colimits', 'profinite-groups', 'topological-groups', 'category-theory', 'general-topology']"
4475396,Some questions on Radon-Nykodym derivative,"Suppose that Q and R are two probability distributions on $\Omega$ equipped with $\mathcal{F}$ sigma algebra. Moreover, suppose that $Q<<R$ . By Radon-Nykodym derivative we know that there exists a signed measure $f$ such that $$Q(A)=\int_{A}fdR, \forall A\in \mathcal{F}.$$ Since $Q$ is nonnegative, we can conclude that f is nonnegative $R$ a.e. My question is related to the well definedness of KL-divergence which is $$\mathbb{E}^{Q}[\log{\frac{dQ}{dR}}]$$ we know that $\frac{dQ}{dR}$ is nonnegative $R$ a.e. . But here we are taking an integral with respect to $Q$ . How can we show that $\frac{dQ}{dR}$ is also nonnegative $Q$ a.e. ? Furthermore, we know that $\frac{dQ}{dR}\in L^1({R})$ . How can we show that $\log{\frac{dQ}{dR}} \in L^1(Q)$ ?","['measure-theory', 'radon-nikodym', 'probability-theory', 'real-analysis']"
4475408,Efficient algorithms to determine whether vertices form a deadlock,"$\textbf{I. Problem Statements}$ Let $m, n \in \mathbb{N}^*$ and $G = (V, E)$ be a simple graph. First we define some notations: (1) $[m] := \{1, 2, \dots, m\}$ . (2) $e_i$ is the elementary vector with $i$ -th coordinate being $1$ and other coordinates being $0$ . (3)A vertex $v \in V$ is a $n$ -dimensional vector belonging to $(\{0\} \cup [m])^n$ whose $n-1$ coordinates are $0$ and one coordinate is non-zero. For example $[1, 0, 0]$ is a vertex while $[1, 1, 0]$ is not. There are $mn$ vertices. (4)A state $s \in (\{0\} \cup [m])^n$ is a set of vertices: $\{s_i \neq 0|[0, 0, \dots, \underbrace{s_i}_{i\text{-th}\,coordinate}, 0, 0, \dots, 0] \in V\}$ . There are $(m+1)^n$ states in total. The edges connect vertices in $V$ . We say a state $s$ is a deadlock iff one of the following two conditions: (1) $\exists x, y \in s$ such that $xy \in E$ . (Explicit law) (2) $\forall i \in [m]$ , $s + e_i$ is a deadlock. (Recursive law) I want to find an efficient algorithm which is polynomial w.r.t $|E|$ to determine whether a state $s$ is a deadlock, or prove this problem is $NP$ or $NP$ -hard. $\textbf{II. Background and Examples}$ I know this problem looks weird but it has a very practical background. Consider $n$ vehicles moving along $n$ distinct paths whose lengths are $m$ . Each vehicle is a vertex and a set of vehicles is a state. For each time we can ask one vehicle to move one step. However, due to geometric constraints, e.g., narrow roads, vertices might pairwise conflict, so there are deadlocks. To be convenient, I define the $\text{Next}(s)$ operator: It adds the non-zero coordinates of $s$ by $1$ without changing the $0$ coordinates, for example $\text{Next}([5, 2, 0]) = [6, 3, 0]$ . For the first example, $\{a, b, c\}$ forms a deadlock. $ab, bc, ca$ are not edges and $\{a, b, d, e\}, \{b, c, d, e\}, \{a, c, d, f\}$ do not form perfect matchings. However: (1)If we move $a$ to $d$ and get $\{d, b, c\}$ , ${d, b, g, e}$ forms a perfect matching $\{de, bg\}$ so neither of $d$ and $b$ could move (No.1 explicit law). (2)If we move $b$ to $e$ to get $\{a, e, c\}$ , $a$ could not move to $d$ because $de$ is an edge (No.1 explicit law). $c$ could not move to $f$ since $af$ is an edge (No.1 explicit law). $e$ could not move to $h$ as $hc$ is an edge. (3)We could not move $c$ to $f$ as $af$ is an edge. Then $\{a, b, c\}$ is a deadlock because of the No.2 recursive law. For the second example $s = \{a, b, c\}$ and $\text{Next}(s) = \{d, e, f\}$ . The induced bipartite graph $G[s \cup \text{Next}(s)]$ has a perfect matching so none of $a, b, c$ could move. Please note that three vehicles move one by one. If they can move simultaneously, they can move to $\{d, e, f\}$ together without an annoying deadlock. For the third example, $\{a, b\}$ forms a deadlock. $\{a, b\}$ can only move simultaneously to $\{a_1, b_1\}, \{a_2, b_2\}, \{a_3, b_3\}$ , then neither of them could move, as $\{a_4, b_3, a_5, b_4\}$ and $\{a_3, b_4, a_4, b_5\}$ have perfect matchings. If the edge $a_4b_4$ is removed, $\{a, b\}$ is not a deadlock. So a naive recursive algorithm may look many steps forward. $\textbf{III. My attempts}$ (1) Formulate these into formal math definitions. I do some case studies into several examples. (2) The perfect matching $s$ and $\text{Next}(s)$ is the root cause of deadlocks. Therefore, I think about a DFS algorithm similar to the Edmonds Blossom algorithm . However, the obstacle is that perfect matchings are only generators of the deadlock set, not the set itself. For example, in the third example, $\{a, b, a_1, b_1\}$ does not contain a perfect matching. (3)I consider the $m$ -radix bitmasking dynamic programming. They key idea is to reduce duplicate computations. If the algorithm has checked the state $\{a, b, c, d, e\}$ is not a deadlock, then the algorithm should not consider the edges connecting $\{b, c, d, e\}$ when checking $\{Next(a), b, c, d, e\}$ . But this idea also reduces to a super long boolean expression. Shall I consider the cliques in the complement graph $\bar{G}$ ? The bounty expired without answers. If you have a new idea, you can comment on this problem and I will start another bounty.","['graph-theory', 'matching-theory', 'combinatorics', 'algorithms', 'computer-science']"
4475411,Question about the proof about topological compactifications of $\mathbb{R}$,"I have a question regarding the proof about compactifications of $\mathbb{R}$ .
I am reading Van Douwen ´s paper C haracterizations of $\beta \mathbb{Q}$ and $\beta \mathbb{R}$ , where he defines: We call a compactification $\gamma X$ of a space $X$ topological if
every autohomeomorphism $h$ of $X$ has a continuous extension $\gamma h: \gamma X \rightarrow \gamma X$ . and then he proves that for the halfline $\mathbb{H}$ , $\alpha \mathbb{H}$ and $\beta \mathbb{H}$ are the only topological compactifications of $\mathbb{H}$ . He then states as an immediate consequence that $\alpha \mathbb{R}$ , $\beta \mathbb{R}$ and the two-point
compactification are the only three topological compactifications of $\mathbb{R}$ . My question is : Why is this immediatelly seen? I would like to show this a bit more precisely. I tried to say this: For any $x \in \mathbb{R}$ , the map $f: x \rightarrow - x$ induces an autohomeomorphism of $\beta \mathbb{R}$ which implies that $\beta[0,\infty)$ is identical with $\beta(-\infty,0]$ . But I am not sure how to conclude exactly the result. Will appreciate any help. P.S. $\alpha X$ denotes the Alexandroff one-point compactification and $\beta X$ denotes the Stone-Čech compactification. P.P.S. I am sorry I couldn´t find the paper link online.","['alexandroff-compactification', 'compactification', 'solution-verification', 'general-topology', 'compactness']"
4475416,Manifolds with isometry group $ SU_n $,"The sphere $ S^n $ equipped with a metric of constant positive curvature $ g $ has orientation preserving isometry group $$
Iso^+(S^n,g) \cong SO_{n+1}(\mathbb{R})
$$ Indeed every compact group is the isometry group of some manifold see https://mathoverflow.net/questions/87070/can-every-lie-group-be-realized-as-the-full-isometry-group-of-a-riemannian-manif?rq=1 However it is not clear to me how this works in particular examples. Is there a ""well-known"" family of Riemannian manifolds $ M^n $ whose group of orientation preserving isometries is $ SU_n $ ? I know that $ \mathbb{C}P^{n-1} $ with the Fubini Study metric has orientation preserving isometry group $ PU_n $ , when $ n $ is even, (see What is the compact Riemannian manifold $M$ such that $SU(n)/\mathbb{Z}_n$ is the isometry group of $M$? ) so that's close but not quite it.","['riemannian-geometry', 'differential-geometry']"
4475434,Every graph has a large min-degree subgraph,"If a graph has large minimum degree, i.e. everywhere, locally, many edges per vertex, it also has many edges per vertex globally: $\varepsilon(G)=$ $\frac{1}{2} d(G) \geqslant \frac{1}{2} \delta(G)$ . Conversely, of course, its average degree may be large even when its minimum degree is small. However, the vertices of large degree cannot be scattered completely among vertices of small degree: as the next proposition shows, every graph $G$ has a subgraph whose average degree is no less than the average degree of $G$ , and whose minimum degree is more than half its average degree:। Proposition 1.2.2. Every graph $G$ with at least one edge has a subgraph $H$ with $\delta(H)>\varepsilon(H) \geqslant \varepsilon(G)$ . Proof. To construct $H$ from $G$ , let us try to delete vertices of small degree one by one, until only vertices of large degree remain. Up to which degree $d(v)$ can we afford to delete a vertex $v$ , without lowering $\varepsilon$ ? Clearly, up to $d(v)=\varepsilon$ : then the number of vertices decreases by 1 and the number of edges by at most $\varepsilon$ , so the overall ratio $\varepsilon$ of edges to vertices will not decrease. Formally, we construct a sequence $G=G_{0} \supseteq G_{1} \supseteq \ldots$ of induced subgraphs of $G$ as follows. If $G_{i}$ has a vertex $v_{i}$ of degree $d\left(v_{i}\right) \leqslant \varepsilon\left(G_{i}\right)$ , we let $G_{i+1}:=G_{i}-v_{i}$ ; if not, we terminate our sequence and set $H:=G_{i}$ . By the choices of $v_{i}$ we have $\varepsilon\left(G_{i+1}\right) \geqslant \varepsilon\left(G_{i}\right)$ for all $i$ , and hence $\varepsilon(H) \geqslant \varepsilon(G)$ . What else can we say about the graph $H$ ? Since $\varepsilon\left(K^{1}\right)=0<\varepsilon(G)$ , none of the graphs in our sequence is trivial, so in particular $H \neq \emptyset$ . The fact that $H$ has no vertex suitable for deletion thus implies $\delta(H)>\varepsilon(H)$ , as claimed. Precisely, what does it mean by saying ""the vertices of large degree cannot be scattered completely among vertices of small degree""? If I were to take the subgraph containing the largest degree vertex and its neighbors, then this would certainly have a larger minimum degree than the average degree of the original graph. Wouldn't this be all that is needed to imply there is no way to ""drown out"" the large degree vertices? Because a high degree vertex surrounded by many many small degree vertex is still a high degree vertex in the graph...","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4475437,Confusion on a subtle point of the construction of the functor $\text{Spec}: \mathsf{Rings} \rightarrow \mathsf{L.R.S}$.,"In general, when constructing the locally ringed space $(\text{Spec}(A),\mathscr{O}_{\text{Spec}(A)})$ associated to a ring $A$ we consider the basis for the topology on $\text{Spec}(A)$ given by $\mathscr{B} =  {\{D(f)\}}_{f\in A}$ define a sheaf of rings on it, and extend it in the usual way to all of $\text{Spec}(A)$ . When describing this process it is often the case that one proves that for $f,g \in A$ such that $D(f) = D(g)$ one must have $A_{f} \cong A_{g}$ . Hence, for each open $D(f)$ we can make a choice of $\bar{f} \in A$ such that $D(\bar{f}) = D(f)$ and set $\mathscr{O}_{Spec(A)}(D(f)) = A_{\bar{f}}$ . Given a ring $A$ this process does yield a locally ringed space with the desired property. However this construction should also yield a functor $\text{Spec}: \mathsf{Rings} \rightarrow \mathsf{L.R.S}$ . What I am  confused about is the following: Given that the category of rings is not small does it really make sense to ""choose"" for each basic open subset of each ring a localization to define as the section of our sheaf? It seems to me that this process cannot work within ZFC and would require something like the addition of global choice or a universe axiom. Thinking about this problem I believe I have found a way around this by using the saturation of manipulatively closed sets to define the structure sheaf. Indeed, since $D(f) = D(g)$ one has that $S_{f} = \{1,f,f^2,...\}$ (resp $S_g$ ) such that the saturations $\bar{S_f} = \bar{S}_{g}$ are equal as sets, so the localizations are literally equal and not just isomorphic. Is all of this worry on my part necessary? Apologies if I have misunderstood something obvious.","['axiom-of-choice', 'algebraic-geometry', 'schemes']"
4475464,Dual algebra structure of the divided power coalgebra,"Background Let $K$ be a (unital, associative) commutative ring and consider the dual $K$ -module $K[x]^*$ of the polynomial algebra. Then there is a $K$ -algebra structure on $K[x]^*$ satisfying $$x_n \cdot x_m = \binom{n+m}{m} x_{n+m}$$ where $x_n$ denotes the dual element to $x^n$ . This is the dual algebra associated to the divided power coalgebra structure on $K[x]$ (the one that defines a bialgebra structure along with the usual algebra structure on $K[x]$ ). There is always a $K$ -algebra map $$\Phi : K[[t]] \rightarrow K[x]^*$$ that takes $t^n$ to $n! x_n$ . I have shown that $\Phi$ is an isomorphism if and only if $K$ is a $\mathbb{Q}$ -algebra. Question What are the precise conditions on $K$ such that there exists a $K$ -algebra isomorphism $K[x]^* \cong K[[t]]$ ? I would really appreciate a ""(co)homological"" argument as well, say by considering $\mathbb{CP}^\infty$ .","['coalgebras', 'algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
4475504,What is wrong with the equations I have set?,"I have been learning calculus on Khan Academy, and came to differential calculus on BC Calculus.
The problem says: ""Each month the balance B on Harper's loan increases by 22% and decreases by 250 dollars"".
We have to find the equation that describes the relationship best. No problem there.
It is logical that: $\frac{\partial B}{\partial t}= 0.22B - 250$ But, also I think that this is correct too: $B_{t} = 1.22B_{t-1} - 250$ however taking derivative of $B^t$ I don't think we get $\frac{\partial B}{\partial t}$ like above, but $1.22B^{'}$ , which are not the same. What am I missing?","['calculus', 'ordinary-differential-equations']"
4475527,Vakil 6.4.G: Regrading a ring generated in arbitrary degrees,"I'm working through Vakil's FOAG currently and am stuck with how to apply the hint in problem 6.4.G; the problem states Suppose $S_\bullet$ is generated over $S_0$ by $f_1, \dots, f_n$ . Find $d$ such that $S_{d\bullet}$ is finitely generated in ""new"" degree 1 (= ""old"" degree $d$ ). (This is surprisingly tricky, so here is a hint. Suppose there generators $x_1, \dots, x_n$ of degrees $d_1, \dots, d_n$ respectively. Show that any monomial $x_1^{a_1}\dots x_n^{a_n}$ of degree at least $nd_1\dots d_n$ has $a_i \geq (\prod_j d_j)\ /\, d_i$ for some $i$ . Show that the $(nd_1\dots d_n)$ th Veronese subring is generated by elements in “new” degree 1.) The first part of the hint is easy to show by picking $i$ such that $a_id_i = \max_{1\leq j \leq n} a_i d_i$ . Then for the monomial $x_1^{a_1}\dots x_n^{a_n}$ of degree $a_1d_1 + \dots + a_nd_n \geq nd_1 \dots d_n$ , we have $$
na_id_i \geq a_1d_1 + \dots + a_nd_n \geq nd_1 \dots d_n
$$ giving us the relation we need. However I get a bit stuck trying to apply this to the last sentence — how does this tell us that the $(nd_1\dots d_n)$ th Veronese subring $S_{nd_1\dots d_n \bullet}$ is generated in degree 1 using the fact that $S_\bullet$ is generated by $x_1, \dots, x_n$ (I'm not entirely sure why both $f_1, \dots, f_n$ and $x_1, \dots, x_n$ are used here).","['ring-theory', 'algebraic-geometry']"
4475537,Understanding the sum of iid exp(1),"If $X,Y$ $\sim$ $i.i.d.$ $exp(1)$ , $X+Y$ $\sim$ $Gamma(2,1)$ . I know how to arrive at this using sum of MGFs, but I'm wondering if there's an intuitive way to understand this.","['probability-distributions', 'probability-theory', 'probability']"
4475542,Finding slant length of any given point for a cone with an angled base,"I'm trying to write a program that prints out the flat pattern for a cone with an angled bottom when given its height, base diameter, and base pitch like in the diagram above. My approach has been to divide the base circumference into small increments and for each increment, find the slant length from the cones vertex. The lowest point I've found to be $\sqrt{H^2R^2}$ where $H$ =height and $R$ = base radius. For each increment, both $R$ and $H$ should change as the slants move up the pitch. Its been a while since I've done math like this and am having trouble coming up with a method to account for this! How can I find the new height and diameter given a distance travelled along the base circumference? or is there an easier way to determine the slant length for any given point on the cones base circumference? The program will be used to fabricate sheetmetal truncated cones for roof flashings.","['trigonometry', 'geometry', '3d']"
4475560,"Determining whether $A(x) = \int_0^x |\sin(1/u) |\,du$ is differentiable at $0$ [duplicate]","This question already has answers here : Is $\int_0^x\left|\sin\left(\frac{1}{t}\right)\right|\mathrm{d}t$ differentiable at $0$? (3 answers) Closed 2 years ago . Let $$A(x) = \int_0^x |\sin(1/u) |\,du = \lim\limits_{a\to 0^+} \int_a^x |\sin(1/u)|\,du$$ Determine, with proof, whether $A$ is differentiable at $0$ . Clearly $A(0) = 0$ . By definition, I need to show that $$\lim\limits_{x\to 0^+} \frac{A(x)}x = \lim\limits_{x\to 0^-} \dfrac{A(x)}x$$ I know $A$ is odd. Also, $$A(x) = \sum_{k=k_x}^\infty \int_{1/((k+1)\pi)}^{1/(k\pi)} |\sin(1/u)| du + \int_{1/(k_x\pi)}^x |\sin(1/u)| du,$$ where $k_x = \lceil 1/(x\pi)\rceil$ . Using the latter expression, I can show that $$\left|\dfrac{A(x)}x\right|\leq \frac{1}{|x|}\left|\sum_{k=k_x}^\infty \frac{1}{k\pi} - \frac{1}{(k+1)\pi} + x-\frac{1}{k_x \pi}\right| = 1,$$ but obviously this isn't enough to show the derivative exists.","['integration', 'real-analysis', 'calculus', 'trigonometry', 'derivatives']"
4475667,"Prove that $\kappa (A) = \sup\Big\{ \frac{||Ax||}{||Ay||},\ ||x|| = ||y||\Big\}$.","I am trying to prove this for my numerical analysis class. This is from chapter 4.4 of Kincaid and Cheney's book. So far I haven´t got any good idea. I have tried $$
\|A\| \|A^{-1}\| = \sup \|A\frac{u}{\|v\|}\|\cdot \|A^{-1}\frac{v}{\|u\|}\|,\quad \|u\|=\|v\|
$$ But I coudn't advance any further. What confuses me the most is that there is not $||A^{-1}||$ in the equation. As far as I know there is not relation between a matrix norm and its inverse norm besides $\|A^{-1}\| \geq (\|A\|)^{-1}$ which haven't been useful. Please help.","['condition-number', 'matrices', 'linear-algebra', 'matrix-norms', 'numerical-methods']"
4475672,Is Probability Mass Function essentially the same as a list of Probabilities?,"For the most common example of tossing a coin two times. There are four outcomes.
We know that range of X = Rx{0,1,2} Where X is the event of an outcome of heads at least once. Px(0) = 1/4 two tails and no heads Px(1) = 1/2 HT and TH Px(2) = 1/4 Two heads and no tails The limitations is that Px(k) > 0 and ΣPx = 1. Is there something more to PMF than being a list of probabilities? If no then why is there an entire equation making it such?","['permutations', 'statistics', 'probability']"
4475720,Enumerating an uncountable graph,"Proposition 1.4.1. The vertices of a connected graph $G$ can always be enumerated, say as $v_{1}, \ldots, v_{n}$ , so that $G_{i}:=G\left[v_{1}, \ldots, v_{i}\right]$ is connected for every $i$ . Proof. Pick any vertex as $v_{1}$ , and assume inductively that $v_{1}, \ldots, v_{i}$ have been chosen for some $i<|G|$ . Now pick a vertex $v \in G-G_{i}$ . As $G$ is connected, it contains a $v-v_{1}$ path $P$ . Choose as $v_{i+1}$ the last vertex of $P$ in $G-G_{i}$ ; then $v_{i+1}$ has a neighbour in $G_{i}$ . The connectedness of every $G_{i}$ follows by induction on $i$ . In my interpretation, this theorem is saying that we can grow out a connected graph from an original starting position one by one. What happens if there are uncountably many nodes? Would this enumeration still be possible?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
4475731,Getting two different answers on differentiating $\cos^{-1}(\frac{3x+4\sqrt{1-x^2}}{5})$,"Question given in my book asks to find $\frac{dy}{dx} $ from the following equation. $$y=\cos^{-1}\left(\frac{3x+4\sqrt{1-x^2}}{5}\right)$$ My Attempt: Starting with substitutions, Putting $\frac35=\cos\alpha\implies \frac45 = \sin\alpha$ . Putting $x = \cos\beta\implies \sqrt{1-x^2} = \sin\beta$ . $$\begin{align}&y=\cos^{-1}\Big(\frac{3x+4\sqrt{1-x^2}}{5}\Big)\\ \implies& y = \cos^{-1}\Big(\frac{3}{5}x + \frac{4}{5}\sqrt{1-x^2}\Big)\\\implies& y = \cos^{-1}(\cos\alpha\cos\beta + \sin\alpha \sin \beta)\\\implies& y  = \cos^{-1}[\cos(\alpha-\beta )] \tag{1}\\\implies& y = \alpha-\beta\\\implies& y = \cos^{-1}\left(\frac35\right) - \cos^{-1}(x)\\\implies&\color{blue}{\boxed{ \dfrac{dy}{dx}  =\frac{1}{\sqrt{1-x^2}}}}.\end{align}$$ But, if I consider $(1.)$ again, $$y = \cos^{-1}\left[\cos\color{red}{(\alpha-\beta)}\right]$$ This is also equals to, $$\cos^{-1}\left[\cos\color{red}{(\beta - \alpha)}\right].$$ differentiating this, will give the negative of the answer which I got earlier. My book shows that $\frac{-1}{\sqrt{1-x^2}}$ is correct. But why? I think the mistakes lie in the very first step of substitution i.e., $\frac35=\cos\alpha$ doesn't imply $\frac45 = \sin\alpha$ . It should be $\sin\alpha = \pm \frac45$ . Similary $x = \cos\beta$ doesn't imply $\sqrt{1-x^2} = \sin\beta$ instead $\sin\beta = \pm \sqrt{1-x^2}$ . But how can I make sure that in the equation $(1.)$ , $(\alpha - \beta)$ lies in the principal branch of the inverse cosine function?","['calculus', 'derivatives', 'trigonometry']"
4475785,Does there exist a function which converts exponentiation into addition?,"A useful property of the logarithm is that it can ""convert"" multiplication into addition, as in $\ln(a)+\ln(b)=\ln(ab) \text{ for all } a, b \in \mathbb{R}^+$ Does there exist a function $f$ , which holds a similar property for exponentiation? $f(a)+f(b)=f(a^b) \text{ for all } a, b \in \mathbb{R}^+$ If so, are there any closed-form expressions for such a function?","['functional-equations', 'functions', 'logarithms']"
4475787,Calculating win probability in a specific game of chance,"Consider a ""game"" (scare quotes because there's no strategy involved, just pure luck) with the following rules: Each of two players begins with some number of pieces (not necessarily the same number for both players), each labeled with a digit between 1 and 6. On each player's turn, they roll two dice. For each die, if a player holds a piece labeled with that die's number, that piece is eliminated. The winner is the first player to eliminate all their pieces. My question is how to determine which player is more likely to win given the number and distribution of their pieces. Two points are obvious: Having fewer pieces than your opponent makes you more likely to win; Having pieces that are more evenly distributed than your opponent makes you more likely to win. To illustrate the second point, if one player has [1,2,3,4,5,6] and the other has [1,1,1,1,1,1], the first will benefit from any die roll while the second will only benefit from a 1. How do these two factors interact? I did some coding and found that in practice, [1,2,3,4,5,6] seems to beat [1,1,1] more often than not, but lose to [1,1] or [1,1,2]; [1,2,2,3,3,3,4,4,4,4] and [6,6,6,6,6] are about evenly matched; etc. But is there some general formula which, given how many pieces each player has and how many distinct digits they represent, can tell you who has the higher win probability?","['statistics', 'probability']"
4475827,Proving a function is a Bijection from a set to its complement power set,"I want to prove in elegant way that the below function is a bijection: $ f:A \rightarrow \mathbb{P}(\mathbb{N})\backslash A \space; f(a) = \mathbb{N}\backslash a \space $ for each $ \space a \in A;A$ is the set of all finite subsets of the natural numbers and the codomain is the complement of $A$ in the power field of the natural numbers However, in the surjection attempt Im struggling to find the right domain to fit the co-domain. Maybe my initial assumption that the function is bijection is wrong? if so, why?","['functions', 'logic']"
4475831,Does $v^Tw>0$ imply that $\exists A>0$ such that $v=Aw$?,"Let $v,w \in \mathbb{R}^n$ satisfy $v^Tw>0$ . Does there exist a symmetric positive-definite matrix $g$ such that $v=gw$ ? The condition $v^Tw>0$ is necessary for the existence of such $g$ : $$
v^Tw=w^Tg^Tw=w^Tgw>0.
$$ I guess there should be an easy argument for the existence of such $g$ , but I don't see it immediatley. Motivation: I am trying to understand whenever two vectors can be gradients w.r.t different metrics.","['matrices', 'linear-algebra', 'symmetry', 'positive-definite', 'matrix-decomposition']"
4475840,Combinatorial problem using vector space,"Let me state the problem first. This is an exercise from discrete math. Let $k$ , $n$ be positive integers with $1 \leq k \leq n$ . Let $\mathcal{F} = \{A_1,A_2, \cdots, A_m\}$ be a set of subsets of $\{1,2,\cdots,n\}$ , such that $|A_i \cap A_j| = k$ for all $i \neq j$ . What I want to show is $m \leq n$ . The problem suggests the way: First, define $m$ vectors $v_1,v_2, \cdots, v_m \in \mathbb{R}^n$ , as $(\text{$j$th coordinate of $v_i$}) = 1 \text{ if } j \in A_i \text{ and } 0 \text{ else}$ . And show that $\{v_1,v_2, \cdots, v_m\}$ are linearly independent. If I was able to prove that, then the result is immediate since $\mathbb{R}^n$ has a basis of size $n$ , so $m \leq n$ from the fact that basis is maximal linearly independent subset. However, I have no idea how to tackle it. How can I use the property $|A_i \cap A_j| = k$ to prove their linear independence? This is definitely a combinatorial problem, but the problem suggests (a seemingly very effective) approach using vector space. Thank you for any form of help, hint, or solution.","['linear-algebra', 'vector-spaces', 'combinatorics']"
4475853,Is a complex algebraic set with a Zariski dense subset of algebraic points already defined over the algebraic numbers?,"Edit: I now crossposted this question on MO: https://mathoverflow.net/questions/428384/is-a-complex-algebraic-set-with-a-zariski-dense-subset-of-algebraic-points-alrea Let $X$ be a complex algebraic set, i.e. the (not necessarily irreducible) vanishing set of some polynomials in $\mathbb{C}[X_1,\ldots,X_n]$ . If $X$ contains a Zariski dense subset of points with coordinates in the algebraric numbers $\bar{\mathbb{Q}}$ , is it true that $X$ is defined over $\bar{\mathbb{Q}}$ , i.e. that $X$ can be defined as the vanishing set of some polynomials in $\bar{\mathbb{Q}}[X_1,\ldots,X_n]$ ? It seems to be the case that the converse of this stament holds, as discussed here . I read that the implication in my question follows from Lagrange interpolation, but it is not clear to me how Lagrange interpolation works in the present multivariable case and how it can be used to answer my question. For context: I try to understand a proof of Mann's theorem in this survey on applications of o-minimality (from mathematical logic) to diophantine geometry. We want to prove the following: Theorem. Let $Y\subseteq\mathbb{C}^n$ be an algebraic set and consider $\mathbb{G}=(\mathbb{C}^\times)^n$ , the $n$ th power of the multiplicative group of units in $\mathbb{C}$ . Then the set $Y\cap\mathbb{G}^\mathrm{tor}$ of points on $Y$ such that each coordinate is a root of unity is a finite union of cosets of subgroups of $\mathbb{G}$ . At some point on page 15, we want to use bounds from Galois theory and need to reduce to the case that $Y$ is defined over a number field. In the paragraph in paranthesis on p. 15, the following argument is sketched: From basic properties of the Zariski topology using that $\mathbb{G}$ is open, we can see that $Y\cap\mathbb{G}^\mathrm{tor}=\overline{Y\cap\mathbb{G}^\mathrm{tor}}\cap\mathbb{G}^\mathrm{tor}$ ,  and may thus assume that $Y=\overline{Y\cap\mathbb{G}^\mathrm{tor}}$ . Then $Y$ contains a dense set of points with algebraic coordinates, since the elements of $\mathbb{G}^\mathrm{tor}$ have algebraic coordinates. Now the author Scanlon claims that it follows from Lagrange interpolation that $Y$ is defined over $\bar{\mathbb{Q}}$ . The fact that $Y$ is defined over a number field then just follows from Noetherianess of $\bar{\mathbb{Q}}[X_1,\ldots,X_n]$ . A similar reduction also seems to work in the case of torsion points on abelian varieties defined over $\mathbb{C}$ . I would also be grateful for details or references regarding this case. Thanks for your help!","['affine-varieties', 'algebraic-number-theory', 'algebraic-geometry', 'abelian-varieties']"
4475855,Given a sequence with $a_1=1$ and $a_{n+1}=a_n-\frac13a_n^2$. Is there an easier way to get an upper bound of $1/a_{100}$?,"Suppose that a sequence $\{a_n\}_{n=1}^\infty$ satisfies $a_1=1$ and $$a_{n+1}=a_n-\frac13a_n^2,\qquad n=1,2,3,\cdots.$$ Which of the following is true? (A) $2<100a_{100}<\frac52$ (B) $\frac52<100a_{100}<3$ (C) $3<100a_{100}<\frac72$ (D) $\frac72<100a_{100}<4$ This problem comes from my sister's final exam (she is in high school). I have a solution, but my solution is unsatisfactory since it requires some rather complicated computation, which seems impossible to  finish during an exam. My solution. Note that the recurrence relation can be rewritten as $$\frac1{a_{n+1}}=\frac1{a_n}+\frac1{3-a_n},\qquad n=1,2,3,\cdots.$$ Therefore, $$\frac1{a_{100}}=1+\sum_{n=1}^{99}\frac1{3-a_n}.$$ It follows from the original recurrence relation that $a_n$ is decresing and thus $a_n\leq 1$ . Using $\frac1{3-a_n}\geq \frac13$ , we have $$\frac1{a_{100}}\geq 1+\frac{99}3=\frac{102}3,$$ which implies that $100a_{100}\leq \frac{300}{102}<3$ . Hence (C) and (D) are excluded. My gut told me that (B) should be the right answer, since $a_n$ will decreasing to $0$ (an exercise for those who are just learning the concept of limits,) in which case $\frac1{3-a_n}$ is almost $\frac13$ . Let's check the correctness of (B). After some calculations we have $$a_1=1,\qquad a_2=\frac23,\qquad a_3=\frac{14}{27},\qquad a_4=\frac{67\times 14}{27^2\times 3}=\frac{938}{2187}.$$ Since $a_n\leq a_4$ for $n\geq 4$ , we have $$\frac1{a_{100}}\leq 1+\frac12+\frac37+\frac1{3-\frac{14}{27}}+\frac1{3-\frac{938}{2187}}\times 96\approx 39.66<40,$$ where I turned to a caculator. Therefore, $100a_{100}>\frac52$ . Note: I tried to use $a_3$ rather than $a_4$ , which was easier to compute, but it turned out that using only $a_3$ was not enough to get the desired bound. My question. Is there any other methods to get the upper bound of $1/a_{100}$ ? I want a method involving not so many caclulations. At least they can be completed in an exam in which calculators are not allowed.","['alternative-proof', 'algebra-precalculus', 'recurrence-relations', 'sequences-and-series']"
4475896,stochastic process: how can probability space be the same?,"It is said that stochastic process is just a family of random variables $X(\omega)$ supplied with parameter $t$ , which means that for different values of $t$ we have different functions of $\omega$ , $X_t(\omega)$ , but all of them are defined on the same sample space $\Omega$ . But how is that possible? Let us say we have random walk with discrete time $$X_n = Z_1 + Z_2 + ... + Z_n$$ where each $Z_i$ is defined as a function of random experiment $\omega \in \Omega = \{H, T\}$ . Thus for $X_1 = Z_1$ we have event space $\Omega$ with just 2 elements. For $X_2 = Z_1+Z_2$ we have events which are pairs $(\omega_1, \omega_2)$ with 4 possible outcomes, thus $X_2$ is a function $X_2(\omega_1,\omega_2)$ , and so on. Thus for each $n$ we have different event space with the size which grows as $2^n$ . What am I doing wrong? Should we construct the sample space some other way? May be we should say from the beginning that sample space is just an infinite sequence of head-tail experiments $$\Omega = \{ (\omega_1, \omega_2, ...) : \omega_i \in \{H, T\}\}$$ and than say that $$X_n = X_n(\omega_1, \omega_2, ...)$$ ?","['elementary-set-theory', 'stochastic-processes', 'probability-theory']"
4475899,"If we know the graph of $f(x)$ and of $g(x)$, is there a way to graph their composition $f(g(x))$?","My question is that if we know the graph of $f(x)$ and of $g(x)$ ,
s there a way to graph $f(g(x))$ Example: $\sin (\ln (x))$ How do we reach this graph? How does this graph relate to its parent functions?","['functions', 'graphing-functions', 'function-and-relation-composition']"
4475927,Example of groups that are not quasi-isometric but have the same growth rate?,"I have started working on group growth earlier this year, mainly using Drutu and Kapovich's notes. This morning I found myself wondering if I could find an example of groups that are not quasi-isometric but have the same growth rate. Spontaneously, I thought about finite groups, groups of linear growth and free groups. All those cannot provide such an example. I firmly believe this to be possible but I have been unable to find one. Google searches have not helped me either so maybe some of you can.","['geometric-group-theory', 'group-theory']"
4475938,"Hypothesis test, finding p-value","A coin has a probability of getting tails of $\theta$ unknown. We would like to do the following hypothesis test over the value of $\theta$ : $$\begin{cases}H_0 : \theta=0.5 \\ H_1 : \theta > 0.5 \end{cases}$$ Suppose we flipped the coin 5 times and got 4 tails. Calculate the p-value. Let $X$ be a random variable such that it takes $1$ if we got tails, and $0$ if heads (a Bernoulli distribution with unknown parameter $\theta$ ) and $X_1, \dots, X_5 $ a simple random sample of $X$ . Then the p-value is going to be $P_{H_0}(T\geq t)$ (assuming $H_0$ is true), with $T=\overline{X}_n$ and $t=\frac{4}{5}=0.8$ . So, $P_{H_0}(T\geq t)=P_{H_0}(\overline{X}_n\geq0.8)=1-P_{H_0}(\overline{X}_n <0.8)$ By the central limit theorem, $\overline{X}_n \sim N\left(0.5,\frac{(0.5)(1-0.5)}{5} \right)=N(0.5, 0.05)$ (approximately). So, $p-value=1-\phi \left(\frac{0.8-0.5}{0.22} \right)=0.0869$ which is incorrect. Where's my mistake? The correct answer is 0.187","['statistics', 'probability-limit-theorems', 'central-limit-theorem', 'p-value', 'hypothesis-testing']"
4475968,Can we always let two uncommute hermitian matrices $A$ and $B$ commute by enlarge the dimension?,"For example, we have two hermitian matrices $$A=\left( \begin{matrix}
	1&		0\\
	0&		-1\\
\end{matrix} \right) ,B=\left( \begin{matrix}
	0&		1\\
	1&		0\\
\end{matrix} \right) .$$ And for this special case, I can find another two hermitian matrices by placing $A$ and $B$ as the principal submatrix commute as follows: $$\tilde{A}=\left( \begin{matrix}
	1&		0&		-1\\
	0&		-1&		1\\
	-1&		1&		0\\
\end{matrix} \right) ,\tilde{B}=\left( \begin{matrix}
	0&		1&		1\\
	1&		0&		1\\
	1&		1&		0\\
\end{matrix} \right) .$$ I wonder if there exists general theorems guarantee that we can always find two enlarged commuting hermitian matrix like this?","['matrices', 'hermitian-matrices', 'linear-algebra']"
4475980,"Is $L^*L$, where $L^*$ denotes the formal adjoint, positive semi-definite?","Let $E$ , $F$ be vector bundles with metric over a smooth (not necessarily compact) manifold $X$ .
Let $L:C^\infty(E) \rightarrow C^\infty(F)$ be a differential operator.
Let $L^*:C^\infty(F) \rightarrow C^\infty(F)$ be the formal adjoint of $L$ with respect to the $L^2$ -inner product on the $L^2$ -sections of $E$ and $F$ , i.e. $L^*$ satisfies: $$
\langle L f,g \rangle_{L^2(F)} =
\langle f, L^*g \rangle_{L^2(E)}
$$ for all $f \in C^\infty(E)$ , $f \in C^\infty(F)$ with compact support. Then $L^*$ is again a differential operator and can therefore be applied to all smooth sections of $F$ . Question:
Are all eigenvalues of $L^*L$ acting on smooth sections non-negative?
Here, smooth sections need not be in $L^2$ . $L^* L$ acting on $L^2_2(E)$ has only non-negative eigenvalues, because of $$
\langle L^* L f, f \rangle 
=
\langle Lf, Lf \rangle
=
|L f|^2 \geq 0.
$$ But according to what I know, it could be possible that there exists some section $f \in C^\infty(E)$ that is not in $L^2_2(E)$ , for example because its $L^2$ -integral is infinite, but satisfies $Lf=-f$ . Context about my application:
I am interested in the case where $P$ is a principal bundle over an asymptotically conical manifold, $A$ is an asymptotically flat connection on $P$ , and $L=\nabla_A$ on $\operatorname{Ad} P$ .
I am also interested only in sections $f \in C^\infty(E)$ which decay at infinity, but they may not decay fast enough to be in $L^2(E)$ .","['partial-differential-equations', 'adjoint-operators', 'differential-geometry']"
4475988,What is the motivation behind the definition of the matrix sum of a graph?,"I was studying graph theory using the textbook Combinatorics and Graph Theory by Harris, Hirst, and Mossinghoff. In section 1.2, the matrix sum is defined as follows: Given a graph $\mathscr{G}$ of order $n$ with adjacency matrix $A$ , and given a positive integer $k$ , define the matrix sum $S_k$ to be $$S_k = \mathbb{I} + A + A^2 + \ldots + A^k,$$ where $\mathbb{I}$ is the $n \times n$ identity matrix. As I learned, this sum helps us with better methods to find the eccentricity, radius, and diameter of an undirected graph: for example, if $k$ is the smallest positive integer such that row $j$ of $S_k$ contains no zeros, then $\mathrm{ecc}(v_j) = k$ . What I don't quite understand yet is why exactly it makes sense to define this sum this way. I understand that this questions is a bit imprecise, but could someone motivate the reason why this definition exists this way?","['graph-theory', 'linear-algebra', 'combinatorics']"
4476015,"In the system $\dot x =(|x|^2 -1)(\bar x -x)$, characterize the type of equilibrium point $\bar x$ is","Consider the following system of ODEs : $\dot x =(|x|^2 -1)(\bar x -x)$ with $x \in \mathbb R^n, n\in \mathbb N, n\ge 1$ and $\bar x \in \mathbb R^n$ ,  assigned. Which of the following statements is surely true? (a) $\bar x$ is a locally attractive equilibrium point if it is close enough to the origin (b) $\bar x$ is a stable equilibrium point if it is far enough away from the origin (c) $\bar x$ is the only point of equilibrium in the system (d) $\bar x$ < is a globally asymptotically stable equilibrium point if it is close enough to the origin The given correct answer was (a) (but seems to be wrong) Can someone help me out to prove it formaly and also what is the reasoning to rule out the other ones?
If it were just in n=1, I could just solve $(|x|^2 -1)(\bar x -x)=0 $ to find the equilibrium points and $(|x|^2 -1)(\bar x -x)>0 $ to find the intervals where the solution trajectory increases and $(|x|^2 -1)(\bar x -x)<0 $ the intervals where it decreases, but in n dimensions how do I deal with that ?** So so far I only have that the equilibrium points are $x=\bar x$ and all the points in the hypersphere $|x|=1$ Edit : Even in the one-dimensional case my analysis is not making any sense with the  answer that is supposed to be correct:
For n=1, the equilibrium points are $\bar x, 1,-1$ Now analyzing the sign of the right-hand side of the ODE:
Suppose $\bar x$ is close enough to the origin,as option (a) states, that is $ -1< \bar x < 1 $ , then $(x^2-1)(\bar x- x)<0 \iff x \in (-1,\bar x)\cup(1,+\infty)$ , so I get the following diagram for the trajectories over the real line: ---->>>-1<<<< $\bar x$ >>>>1<<<<---- , meaning that inside $\bar x$ is unstable. What am doing wrong? Edit 2 So with the help of MatthewH, it seems like (a) is certanly wrong at least in the n=1 case
I will continue the analysis at least in the n=1 case
Now option (b) seems to be the correct one because if $\bar x$ is far enough away from the origin, that is $\bar x>1$ or $\bar x<-1$ . To fix the idea I consider $\bar x >1$ : $(x^2-1)(\bar x- x)<0 \iff x \in (-1,1)\cup(\bar x,+\infty)$ so I get the following diagram for the trajectories over the real line: ---->>>-1<<<<1>>>> $\bar x$ <<<<---- , meaning that $\bar x$ is a sink, that  is asymptotically stable: attractive and stable. Therefore option b is the correct one To rule out the other ones: Option c is not correct because there are infinite equilibrium points Option d is not correct : $\bar x$ can't be globally asymptotically stable equilibrium point because when it is inside $(-1,1)$ , solution curves with initial value outside of that interval can't reach it Does this seems ok? Can someone provide a solution in n dimensions?","['stability-in-odes', 'multivariable-calculus', 'ordinary-differential-equations', 'dynamical-systems']"
4476034,Why can't I take the squareroot of this complex equation?,"Let $w\in \Bbb{C}$ . I want to solve $3=e^{2w}$ and find all solutions for $w$ , so I don't need to choose a branch of logarithm. My first intuition was to direclty apply the logarithm to this equation. Then $2w=\log(3)+2\pi i k$ . Hence $w=\frac{\log(3)}{2}+\pi i k$ . But if I apply the squareroot first then $\sqrt{3}=e^w$ . But then applying the logarithm we get $\log(\sqrt{3})+2\pi i k$ . Here I am confused because I take $2\pi i k$ and not $\pi i k$ . So I think I somehow miss some solutions. Where is my mistake?","['complex-analysis', 'exponential-function', 'analysis']"
4476051,Are propositions sets of possible worlds?,"In article ""Against Set Theory"" by Peter Simons (Appeared in Johannes Marek and Maria Reicher, eds., Experience and Analysis. Proceedings of the 2004 Wittgenstein Symposium. Vienna: öbv&hpt, 2005, 143–152.). He wrote: One effect of set theory in ontology has thus been to cripple the development of an adequate ontology of collective entities. This however is far from the worst of its effects. In general the employment of set theory, usually hand in hand with model-theoretic semantics, has been to persuade many philosophers that the rich panoply of entities the world throws at us can be reduced to individuals and sets of various sorts, for example sets as properties, sets of ordered tuples as relations, sets of possible worlds as propositions , and so on and so forth. (bold italics is mine) I can understand properties being interpreted as sets and vise-verse, also relations being interpreted as sets of ordered tuples and vise-verse, but what I don't know of is interpreting propositions as sets of possible worlds. Is that a common way of interpreting propositions? Can anybody clarify this point?","['elementary-set-theory', 'philosophy', 'propositional-calculus', 'logic']"
4476061,"Basis-free, field-independent definition of determinants?","Let $T$ be a linear operator on a finite-dimensional vector space $V$ over the field $K$ , with $\dim V=n$ . Is there a definition of the determinant of $T$ that (1) does not make reference to a particular basis of $V$ , and (2) does not require $K$ to be a particular field? As motivation, if $K=\mathbb C$ , I know of three ways to define $\det(T)$ , two of which refer to a choice of basis, and the other of which relies on $\mathbb C$ being algebraically closed: Choose an ordered basis $B$ of $T$ , and let $\mathcal M(T)$ denote the matrix of $T$ with respect to this basis. Then apply any of the formulas/algorithms for calculating a determinant to $\mathcal M(T).$ [This works for any field, but requires choosing a basis to express $\mathcal M(T)$ .] Choose an ordered basis $B$ of $T$ , and let $\det_n$ be an alternating multilinear map from $V^n\to K$ . Then the determinant of $T$ can be defined as $\det_n(TB)/\det_n(B)$ . [This works for any field, but requires choosing a basis to extract ""column vectors of the matrix of $T$ .""] Define $\det(T)$ as the product of eigenvalues of $T$ , repeated according to their algebraic multiplicity. [This makes no reference to a basis, but only works because $\mathbb C$ is algebraically closed.]","['matrices', 'determinant', 'linear-algebra', 'linear-transformations']"
4476063,calculating the Fibonacci sequence every k-step,"So given the Fibonacci sequence $$F_0 = 0 $$ $$F_1 = 1 $$ $$F_n = F_{n-1} + F_{n-2}, n \geq 2$$ I randomly come across some comment that show me you can calculate it jumping 2,4,8 or 16-step by starting with the appropriate term in the sequence, as follow: $$F_{2n}  = 3F_{2(n-1)} - F_{2(n-2)}, n \geq 2$$ $$F_{4n}  = 7F_{4(n-1)} - F_{4(n-2)}, n \geq 2$$ $$F_{8n}  = 47F_{8(n-1)} - F_{8(n-2)}, n \geq 2$$ $$F_{16n}  = (47^2-2)F_{16(n-1)} - F_{16(n-2)}, n \geq 2$$ I test it experimentally and it seem to hold, but I can't figure it out how to get those coefficients let alone get to that formula. Is that some nice thing with powers or two or it can be generalize to any number you desire? $$F_{kn} = M_kF_{k(n-1)} - F_{k(n-2)}$$ what is formula for this $M_k$ , if any?","['fibonacci-numbers', 'discrete-mathematics']"
4476137,"Is there a ""universal connection"" on the universal $G$-bundle?","Let $G$ a Lie group, $X$ a smooth manifold. Let $EG \rightarrow BG$ be the (topological) universal $G$ -bundle. We know for every (topological) principal $G$ -bundle $P \rightarrow X$ there is a (continuous) classifying map $f_P : X \rightarrow BG$ such that $P \approx f_P^*EG$ . I was wondering if there exists a (perhaps infinite-dimensional) smooth structure on $EG \rightarrow BG$ ,
and a principal $G$ -connection $\omega$ on $EG$ ,
such that every smooth principal $G$ -bundle on $X$ equipped with connection
is a pullback of $EG \rightarrow BG$ equipped with $\omega$ , along some smooth map $X\rightarrow BG$ ?","['classifying-spaces', 'principal-bundles', 'connections', 'algebraic-topology', 'differential-geometry']"
4476177,Random generators of the symmetric group $S_n$,"Suppose $x_1,x_2,...,x_m\in S_n$ are uniformly randomly chosen ( $m=O(1)$ , or at least $m\ll n!$ ) and that they generate the entirety of $S_n$ . (The probability that two random generators (i.e., $m=2$ ) generate $S_n$ is already about $3/4$ for large $n$ , [0] so heuristically you'd expect most combinations for even $n=3$ to work anyway.) Let $a_0=I$ begin at the identity permutation and $a_{i+1}=a_ix_k$ , where $k$ is chosen uniformly at random. For a fixed, uniformly randomly chosen $p\in S_n$ , what is the expected value of the first $i$ such that $a_i=p$ ? In other words, what is the expected length of a random walk ending on $p$ on the (directed) Cayley graph of $S_n$ generated by $\{x_i\}$ ? I'm looking for crude heuristics, not necessarily rigorous ones—I'm just struggling to visualize the problem. I suppose the difficulty in visualization relates to the unintuitive fact that permutations tend to have a rather small order and thus don't ""mix"" well. I can find some related results: for example, for two randomly chosen $g,h\in S_n$ which generate $S_n$ (or in fact $A_n$ ), along with their inverses $g^{-1},h^{-1}$ , the average shortest word to an arbitrary permutation is $O(n^2 (\log n)^c)$ for some absolute constant $c$ . [1] [2] contains some results and a conjecture on the worst-case diameter with $O(1)$ generators. But I can't seem to find estimates for a random walk. [0] https://math.stackexchange.com/a/453068/677124 [1] https://www.sciencedirect.com/science/article/pii/S0021869314004797 [2] https://pages.uoregon.edu/kantor/PAPERS/STOCdiameter.pdf","['permutations', 'group-theory', 'symmetric-groups', 'reference-request']"
4476197,Extremal of Compact Banach Space Embedding,"Let $(X, \|\cdot	\|_X)$ and $(Y,\|\cdot	\|_Y)$ be (nontrivial real) Banach spaces such that $X \subseteq Y$ . We say $X$ is compactly embedded into $Y$ if the following two conditions hold : There exists a constant $C>0$ such that $\|	x\|_Y \leq C\|x	\|_X$ for every $x \in X$ . If a sequence $\{x_n\}_{n \geq 1} \subseteq X$ is bounded with respect to the norm $\|\cdot	\|_X$ , then its closure in $Y$ is a compact subset of $Y$ . The best constant $C$ is given by: \begin{equation*}
C = \inf_{x \in X, x \neq 0} \frac{\|	x\|_Y}{\|	x\|_X}
\end{equation*} Suppose $X$ is compactly embedded into $Y$ . Does it follow that the above best constant is attained? That is, does there exist a nonzero element $x \in X$ such that $C= \frac{\|	x\|_Y}{\|	x\|_X}$ ? If not, under what additional assumption will such an extremal element exist? My attempt: For each positive integer $n$ , there exists a nonzero element $x_n \in X$ such that: \begin{equation*}
C \leq \frac{\|	x_n\|_Y}{\|	x_n\|_X} < C + \frac{1}{n}
\end{equation*} Put $y_n = \frac{x_n}{\|	x_n\|_X}$ . Then $y_n \in X$ , $\|	y_n\|_X=1$ , and: \begin{equation*}
C \leq \frac{\|	y_n\|_Y}{\|	y_n\|_X}=\|	y_n\|_Y < C + \frac{1}{n}
\end{equation*} By property $2$ , there is a $y \in Y$ and a subsequence $\{n_k\}_{k\geq 1}$ such that $y_{n_k} \to y$ as $k \to \infty$ with respect to the $\|	\cdot \|_Y$ norm. By continuity of norm and squeeze theorem, we have $\|	y \|_Y = C>0$ so that $y \neq 0$ . Now all I need to prove is that $y \in X$ . But I am stuck on this step. Clearly $\{y_{n_k}\}_{k \geq 0}$ is a Cauchy sequence in $\|	\cdot \|_Y$ but I have no idea about how to make it Cauchy in $X$ . On the other hand, I am unable to come up with counterexamples .","['optimization', 'banach-spaces', 'functional-analysis', 'real-analysis']"
4476202,Solve the trig system of equations,"This is a repost, I'm new here and I did a terrible job explaining the question last time.
I'm trying to make an IK system, the rig basicly works in 2d with the bones shown below. (Rig in Unreal) The map of the Bones is as such . ^ Original on paper a, b, c, x, & y are known I need to solve for $\alpha$ and $\theta$ The equations I have right now are: $ x = a\cos(\theta)-b\sin(\frac{\pi}{2}-\alpha+\theta)+c\cos(\theta)$ $y = -(a\sin(\theta)+b\cos(\frac{\pi}{2}-\alpha+\theta)+c\sin(\theta)) $ The equations can be simplified (thanks to Blue ) $d := a + c$ $\phi := \alpha-\theta$ $ x = d\cos\theta-b\cos\phi$ $ y = d\sin\theta+b\sin\phi$ $\alpha = \theta+\phi$ With this method you have to solve for $\phi$ and $\theta$ I have tried a lot of different ways to solve for $\phi$ but all of them end with me getting stuck and or it becomes unequal (making them useless). Edit: thanks for all the support, I didn't think anyone would answer, but I typed in the equations wrong, I forget the ""a""s at the beginning and the - on the y. I guess that's what waking up does. Sorry!","['trigonometry', 'systems-of-equations']"
4476203,History of Serre Duality and its topological/geometric analogue.,"I have been reading about Serre Duality and am wondering: What sort of computations would motivate coming up with such a duality theorem? On the Wikipedia page , it is claimed ""Serre duality is the analog for coherent sheaf cohomology of Poincaré duality in topology, with the canonical line bundle replacing the orientation sheaf."" I am searching for some clarification on what coherent sheaf cohomology of Poincare duality is and how Serre came up with importing such a result. So really, I am asking for three things here: Where can I go read about the historical development of Serre Duality and related theorems? What is coherent sheaf cohomology for Poincare duality? What sort of computations did people do to dream up of Serre Duality? For completeness, there is also a related question here which I missed.","['algebraic-geometry', 'math-history', 'reference-request']"
4476219,$\overline{ \{\Delta > 0\} } = \{\Delta\ge 0\}$ for $\Delta$ a certain determinant function,"Let $A=(a_{ij})$ be a real  matrix. Consider the $(n+1)\times (n+1)$ bordered matrix $\tilde A= (a'_{ij})$ where $\tilde a_{ij} = a_{ij}$ for $1\le i,j\le n$ , $a'_{ij} = 1- \delta_{ij}$ if $\max(i,j) = n+1$ . For instance, if $(a_{ij})$ is $3\times 3$ , then the bordered matrix $\tilde A$ is $$\tilde A=\left(\begin{matrix} a_{11}& a_{12} & a_{13}& 1 \\ a_{21}&a_{22}&a_{23}&1 \\a_{31}&a_{32}& a_{33}& 1\\1&1&1&0\end{matrix} \right)$$ If $A$ is symmetric, then so is $\tilde A$ . Consider the function from real symmetric matrices to real numbers $$\Delta\colon A\mapsto \det \tilde A$$ Note that $\Delta$ is a homogeneous polynomial of degree $n-1$ in the entries of $A$ . I would like to show that if $A$ is a real symmetric matrix -with $\Delta(A)=0$ , then for every $\epsilon > 0$ there exists a symmetric matrix $A'$ , $\|A'- A\|< \epsilon$ , and $\Delta (A')>0$ . Notes: This is related to my previous question . There, the problem was: $\det A= 0$ , find $A'$ close to $A$ , with $\det A'>0$ . @Martin R: showed us how to find a ray $A + \epsilon L$ , $\epsilon>0$ on which the function $\det$ is $>0$ . Moreover, if $A$ is symmetric, then $L$ can be chosen to be symmetric too. Why I am interested  in this problem:
Consider $A$ a symmetric matrix that is positive definite on the subspace $\sum x_i = 0$ . Then there exist $\lambda> 0$ such that the matrix $A + \lambda J$ is positive definite ( $J$ the matrix with all $1$ -- see this question ). Consider a principal minor of the matrix $A + \lambda J$ . It will be a polynomial of degree $1$ ( since $J$ is or rank $1$ ) in $\lambda$ . The leading term must be $\ge 0$ . Now the leading term is exactly $- \times$ one of the determinant in the problem. Moreover, this inequality is true for all $A'$ symmetric, close to $A$ ( since $A_{|V}$ positive definite then same is true for $A'$ close to $A$ ). Therefore ( based on our still unproved result), the inequalities must be strict. Thank you for your interest! Any feedback would be appreciated!","['real-analysis', 'matrices', 'linear-algebra', 'symmetric-matrices', 'quadratic-forms']"
4476229,Baby Rudin Theorem 5.5 (Chain rule): What's wrong with the obvious proof?,"In Theorem 5.5, Rudin proves the chain rule, but does so in a somewhat different fashion than expected. It seems we can prove the chain rule more easily. Theorem:
Suppose $f:[a,b]\to\mathbb{R}$ is continuous  on $[a,b]$ and $g:I\to\mathbb{R}$ where $I$ is an interval that contains the range of $f$ and $g$ is continuous at $f(x)$ . Then if we define $h(t)=g(f(t))$ , $t\in[a,b]$ , then $h'(x)=g'(f(x))f'(x)$ . Proof:
If we write down $g'(f(x))f'(x)$ , it looks like we're done if we can just express $g'$ in terms of the same limits as $f'$ , i.e., $\lim_{t\to x} g(f(t))=\lim_{y\to f(x)} g(y)$ . Showing this seems equivalent to Theorem 4.7, that $h$ is continuous at $x$ , but I'll write it for the sake of completeness: $\lim_{y\to f(x)}g(y)=g(f(x)),$ so $\forall \epsilon>0 \exists\delta>0$ st $\forall y\in I$ st $d(y,f(x))<\delta, d(g(y),g(f(x)))<\epsilon$ . Since $f$ is continuous at $x$ , $\exists \eta>0$ st $\forall t\in[a,b]$ st $d(t,x)<\eta,$ $d(f(t),f(x))<\delta$ ,which means $d(g(f(t)),g(f(x)))<\epsilon$ . So then $\lim_{t\to x} g(f(t)) = \lim_{y\to f(x)}g(y)$ . Then we have $g'(f(x))*f'(x) = \lim_{y\to f(x)}\frac{g(y)-g(f(x))}{y-f(x)}*\lim_{t\to x}\frac{f(t)-f(x)}{t-x}$ $=\lim_{t\to x}\frac{g(f(t))-g(f(x))}{f(t)-f(x)}*\lim_{t\to x}\frac{f(t)-f(x)}{t-x}$ $=\lim_{t\to x}\frac{g(f(t))-g(f(x))}{t-x} = h'(x)$ . Is there an issue to this approach I'm not seeing?","['calculus', 'derivatives', 'real-analysis']"
4476240,"Explaining symmetry in sequences defined by $\frac{a_{n+1}}{n+1}=\frac{a_n}{n}-\frac12$. (Eg, for $a_1=5$, we get $5,9,12,14,15,15,14,12,9,5$)","I've noticed recently the following fact: consider ratios $\frac{a_n}{n}$ with $n = 1,\dots,10$ . If we require that $$
\frac{a_{n+1}}{n+1} = \frac{a_n}{n} - \frac12
$$ that is, each successive ratio drops by $\frac12$ in value, then for $a_1 = 5$ we get $a_2 = 9$ , $a_3 = 12$ , $a_4 = 14$ , $a_5 = 15$ , $a_6 = 15$ , $a_7 = 14$ , $a_8 = 12$ , $a_9 = 9$ and $a_{10} = 5$ . That is, the numerators show a symmetric behvaior $a_{11 - n} = a_n$ . This looks pretty random to me, so I wonder whether there's something obvious I've missed why such symmetry may arise, and perhaps a more general case where it happens exists.","['algebra-precalculus', 'sequences-and-series']"
4476356,Diophantine equation $\frac{x^4 + y^4}{z^4 + t^4} = u^2$,"Find (many, a family of) non-trivial solutions of the diophantine equation $$\frac{x^4 + y^4}{z^4 + t^4}= u^2\ \ (*)$$ Notes: If the $\{x,y\}$ , $\{z,t\}$ are proportional then we have a trivial solution. So we are interested in non-trivial solutions The equation $\frac{x^4 + y^4}{z^4 + t^4}= 1$ is known as the generalized taxicab problem, see this question . The smallest known solutions are fairly large. Perhaps one could try to solve it for some concrete $u>1$ , like $u=2$ . Origin of the problem: I was looking for squares $a^2$ , $b^2$ , $c^2$ , such that $a^2 + b^2$ , $b^2 + c^2$ , $a^2 + b^2 + c^2$ are also square. Let's write $$\begin{aligned}a &= (x t)^2 - (y z)^2 \\
  b &= 2 x y z t\\
  c&= (x z)^2 - (y t)^2\end{aligned}$$ Now $a^2 + b^2 = ((x t)^2 + (y z)^2)^2$ , $b^2 + c^2 = ((x z)^2+ (y t)^2)^2$ , and $a^2 + b^2 + c^2 = x^4 t^4 + y^4 z^4 + x^4 z^4 + y^4 t^4= (x^4 + y^4)(z^4 + t^4)$ . Therefore, we need $(x^4 + y^4)(z^4 + t^4)$ to be a square. Note that we need non-proportional solutions, otherwise we get $a=0$ , or $b=0$ . The equation can be rewritten as $$(x^4 + y^4)(z^4 + t^4) = v^2\ \ (**)$$ Per Fermat, the equation $x^4 + y^4 = w^2$ has no trivial solutions. It would be nice to have a family of solutions. Possible approach to finding solutions to the equation in form $(**)$ . List pairs $x<y$ relatively prime and determine the square free part of $x^4 + y^4$ . Then look for concidences $s(x^4 + y^4) = s(z^4 + t^4)$ ( perhaps a simplistic approach). Any feedback would be appreciated! Thank you for your attention!","['number-theory', 'diophantine-equations']"
4476391,MCMC sampling of $\beta_0+\beta_1x_i$ separately,"I'm trying to sample $\beta_0$ and $\beta_1$ from a set of data modelled by a Bernoulli distribution. \begin{equation}
p(y)=θ^{y}(1−θ)^{1−y}
\end{equation} Where the relationship between $y$ and my primary $x_i$ is modelled by a logistic regression. \begin{equation}
\nu = log(\frac{\theta}{1-\theta}) = \beta_0 + \beta_1x_i \
\end{equation} And my prior is: \begin{equation}
\pi(\beta_0,\beta_1) \propto 1
\end{equation} I've found my posterior $\pi(\beta_0,\beta_1|y)$ : \begin{equation}
\pi(\beta_0,\beta_1|y) = f(y|\beta_0,\beta_1)\pi(\beta_0,\beta_1) \\
\pi(\beta_0,\beta_1|y) = (\frac{e^{\beta_0+\beta_1x_i}}{1+e^{\beta_0+\beta_1x_i}})^{\sum_{i=1}^ny_i}(\frac{1}{1+e^{\beta_0+\beta_1x_i}})^{n-\sum_{i=1}^ny_i}
\end{equation} So first I'm trying to sample $\beta_0$ and $\beta_1$ separately, so sampling from $\pi(\beta_0|y,\beta_1)$ then from $\pi(\beta_1|y,\beta_0)$ . But since $\pi(\beta_0,\beta_1) \propto 1$ , would those posteriors be the same thing and I'd be sampling from the same posterior twice? Or am I missing something? Would I just sub in $\nu$ instead of $\beta_0 + \beta_1x_i$ in the posterior written above and sample from that? Any help would be greatly appreciated.","['statistics', 'bayesian', 'sampling', 'bernoulli-distribution']"
4476450,What is an equivalent of Student's t-test for exponential distribution?,"I measured a feature on 2 populations. I want to make sure I have a meaningful difference between my populations. The student's t-test requires a normal distribution, but the feature I measured follows an exponential distribution: In the graph above you can see the distribution of the feature for both of my populations (one in orange, one in blue).
I found the likelihood-ratio test, but I don't understand how to apply it... I want an equivalent of the student's t-test but for exponential distributions. EDIT: I want to test if the mean is significantly different. CLT looks promising. EDIT 2: So following Alberto Sinigaglia advice, I tried to use CLT. Here is what I did: For each of my populations, I sampled 50 values at random and computed the mean of the sample. I did this 1000 times for each population. So I end up with 1000 means for each population. Here is the distribution of the means: It does not look like a normal distribution to me.","['statistics', 'probability-distributions', 'probability-theory', 'hypothesis-testing']"
4476499,Induced group action on cohomology of vector bundle,"Let $p:E\to X$ be a vector bundle on a smooth projective curve $X$ . Let $\Gamma$ be a finite subgroup of $Aut(X)$ which has a lifted action on $E$ . How do we get an induced action of $\Gamma$ on $H^i(X,E)$ ? I can see how the action could be defined for $i=0$ as these are just sections. What can we do for $i>0?$","['group-actions', 'vector-bundles', 'algebraic-geometry', 'sheaf-cohomology']"
4476566,Proof of Zariski lemma in Wikipedia,"That wikipedia page has a proof of Zariski's lemma, which states that if $A$ is a Jacobson ring (in particular when $A$ is a field) and $B$ is a finitely-generated algebra of $A$ , which is a field, then $B$ is a finite extension of $A$ (as a vector space). In the proof, the following statement is proved: (*) Let $B\supseteq A$ be integral domains such that $B$ is finitely generated as $A$ -algebra. Then there exists a nonzero $a$ in $A$ such that every ring homomorphism $\phi :A\to K$ , $K$ an algebraically closed field, with $\phi (a)\neq 0$ extends to $\widetilde {\phi }:B\to K$ . The proof of this statement begins with If $B$ contains an element that is transcendental over $A$ , then it contains a polynomial ring over $A$ to which $φ$ extends (without a requirement on $a$ ) and so we can assume $B$ is algebraic over $A$ (by Zorn's lemma, say). I don't understand why is it possible to assume that. Using Zorn's lemma we can find a ring $R$ such that $A\subseteq R\subseteq B$ , every element in $R\setminus A$ is trancendental over $A$ and $B$ is algebraic over $R$ . However, if we prove the existence of $a\in R$ which satisfies the statement, how can we know that there is an element in $A$ which satisfies it?","['zariski-topology', 'proof-explanation', 'algebraic-geometry']"
4476630,Expected value of $\bar x^2$ shows different values when calculating through different equations.,"Here are the basic notations: Population mean = $\mu$ Sample mean = $\bar x$ Population variance = $\sigma^2$ E = Estimator Assumption $E(x_i) = \mu$ $E(x_i)$ is any random variable within the population space. $$E(\bar x) = E(\frac{1}{n} \sum_{i=1}^n x_i)$$ $$E(\bar x) = \frac{1}{n} \sum_{i=1}^n E(x_i)$$ $$E(\bar x) = \frac{1}{n} \sum_{i=1}^n \mu$$ $$E(\bar x) = \frac{1}{n}. n. \mu$$ $$E(\bar x) = \mu$$ Now, $$Var(x_i) = E(x_i^2) - (E(x_i)^2)$$ $$\sigma^2 = E(x_i^2) - \mu^2$$ $$ E(x_i^2)= \mu^2 + \sigma^2 $$ $ E(x_i^2)= \mu^2 + \sigma^2 $ Let this be equation 1 Also, $$var(\bar x) = E(\bar x^2)-(E(\bar x))^2$$ $$E(\bar x^2) = \frac{\sigma^2}{n} + \mu^2$$ $E(\bar x^2) = \frac{\sigma^2}{n} + \mu^2$ Let this be equation 2 Also using the Estimator, $$E(\bar x^2) = E(\frac{1}{n} \sum_{i=1}^n x_i)^2$$ $$E(\bar x^2) = \frac{1}{n^2} \sum_{i=1}^n E(x_i)^2$$ From equation 1 we know that $ E(x_i^2)= \mu^2 + \sigma^2 $ Hence, $$E(\bar x^2) = \frac{1}{n^2} \sum_{i=1}^n (\sigma^2+\mu^2)$$ $$E(\bar x^2) = \frac{1}{n^2} \sum_{i=1}^n (\sigma^2+\mu^2)$$ $$E(\bar x^2) = \frac{1}{n^2} .n^2. (\sigma^2+\mu^2)$$ $$E(\bar x^2) = (\sigma^2+\mu^2)$$ How can $E(\bar x^2)$ have two values as shown above and equation 2 Where $E(\bar x^2) = \frac {\sigma^2}{n} + \mu^2$","['data-analysis', 'statistics', 'variance', 'standard-deviation']"
4476640,"Does a simple graph with $(\text{deg}(\alpha_1),\text{deg}(\alpha_2),...,\text{deg}(\alpha_9))=(6,6,6,6,5,3,3,3,3)$ exist.","Is there a (simple) graph $\Omega=(\{\alpha_1,\alpha_2,...,\alpha_9\},\Omega_K)$ with $$(\text{deg}(\alpha_1),\text{deg}(\alpha_2),...,\text{deg}(\alpha_9))=(6,6,6,6,5,3,3,3,3)$$ Some additional information: A graph is a pair $G = (V, E)$ , where $V$ is a set whose elements are called vertices, and $E$ is a set of paired vertices, whose elements are called edges. The edges of a graph define a symmetric relation on the vertices, called the adjacency relation. Specifically, two vertices $x$ and $y$ are adjacent if $\{x, y\}$ is an edge. A graph may be fully specified by its adjacency matrix $A$ , which is an $n \times n$ square matrix, with $A_{ij}$ specifying the number of connections from vertex $i$ to vertex $j$ . For a simple graph, $A_{ij}\in \{0,1\}$ , indicating disconnection or connection respectively, meanwhile $A_{ii}=0$ . As I am currently learning for my discrete maths exam, I am looking at old exam questions. Unfortunately I'm not really sure where to start here. Thanks in advance! Does a simple graph with $(\text{deg}(\alpha_1),\text{deg}(\alpha_2),...,\text{deg}(\alpha_9))=(6,6,6,6,5,3,3,3,3)$ exist.","['graph-theory', 'discrete-mathematics']"
4476688,Are sets in ZFC a primitive notion?,"I read everywhere (including here on math.stackexchange) that the notion of set in ZFC is primitive. To my (probably mis-)understanding, though, a primitive notion is a concept that is not defined in terms of previously-defined concepts. This is where my confusion begins. Aren't ZFC axioms there exactly to define what a set is (in first-order logic terms)? Correct me if I am wrong, but isn't something a set exactly if and only if satisfies those axioms? With those at hand, one can for example show that {1,2} is a set but the collection of all sets is not a set. To my very uneducated brain, this is what you do with any other definition in math: you say that something is something if and only if it satisfies certain axioms. Therefore I am wondering: how does a set differ from say, a measure of probability? For both of them, I have a bunch of axioms that formally define what they are. I do see that $\in$ and $=$ are primitive notions because they are indeed never defined but simply appear in the ZFC axioms. However, sets do. What am I grossly misunderstanding?","['elementary-set-theory', 'foundations', 'logic', 'set-theory']"
4476712,Show $z^5+6z^3-10$ has exactly two zeroes on the annulus $2<\vert z \vert<3$,So I need to show and find the zeros of $$z^5+6z^3-10$$ on $\{z \in \mathbb{C} : 2 < \vert z \vert < 3\}$ . Whats the quickest way of doing this? I was reading Gamelin-Greene and they split the function into BIG+little. Here is $6z^3$ my big and $z^5-10$ my little? OR By Rouche's Theorem can I let $f(z)=z^5+6z^3-10$ and let $h(z)=z^5-10$ ?,"['complex-analysis', 'roots']"
4476752,"simple pendulum equation, why it cannot be solved with laplace transform (the general solution)","Usually to solve the simple pendulum equation: $\qquad \ell {\ddot  \theta }+g\sin \theta =0\,$ Using the first term of Taylor series is used as approximation, but although $\sin \theta$ can be transformed to Laplace ""space"" and use it to find a general solution, I can't find it. Why there is no general solution in Laplace?","['laplace-transform', 'ordinary-differential-equations']"
4476754,Relation between roots of $f$ and roots of the partial sums of its power-series,"It is well known we can write any holomorphic function as a local power series, for example $$\exp(z) = \sum_{n=0}^\infty \frac{z^n}{n!}$$ Obviously each partial sums are polynomials of degree $n$ which have always $n$ roots. I now wonder what is the relation or if there is any relation between the roots of a holomorphic function and the roots of its partial sums. For exmample it is well known that the exponential function does not have any roots, but for the partial sums $$(p_1(z)=1,) p_2(z)=1+z, p_3(z)=1+z+\frac{z^2}{2}, p_4(z)=1+z+\frac{z^2}{2}+ \frac{z^3}{6},..$$ we have the zero sets $$\{-1\}, \{-1-i,-1+i\},\{\approx-1,5961,\approx -0.702\pm1.807i\},..$$ (exact forms of the last roots are very poor) For other holomorphic function that have roots, for example the Sine, we know $$\sin(z)=\sum_{n=0}^\infty(-1)^n \frac{z^{2n+1}}{(2n+1)!}$$ It has roots in $k\pi$ , $k\in\mathbb{Z}$ , and its partial sums $$ p_1(z)=z, p_2(z)=z-\frac{z^3}{6}, p_3(z)=z-\frac{z^3}{6}+ \frac{z^5}{120},..$$ have the zero sets $$\{0\},\{0,\pm\sqrt{6}\},\{0,\pm3.2369 \pm 0.6908 i\},..$$ Obviously $0$ is always gonna be a root, but whats is about the other roots? Do the roots of the partial sums converge towards the roots of the limit function (what sounds reasonable)? And if so, what happens in the case where the limit function itself has no roots (e.g. exponential function?) May the zero set be diverging? Is there any relation between the zeros of the partial sums (that always exists) and the zeros of the limit function? Edit: I made some mathematica code to plot the zeros for increasing partial sums and indeed it looks like in the case of the exponential function its zeros are diverging. Since I have no idea how (or if its even possible) to upload mathematica code here, here some images For the sine on the other hand it seems like that all the sine zeros are generated, but also there are many diverging roots. However I could only calculate up to $n=50$ because for $n$ too big mathematica really needs some time to actually calculate the roots.","['complex-analysis', 'power-series', 'roots', 'real-analysis']"
4476780,Not following the derivation of $\frac{dy}{dx}=-\frac{F_x}{F_y}$,"I've seen similar questions to mine asked on the forum, but I haven't seen answers that address the part I'm confused about. My calculus textbook (Thomas from Pearson) derives the following formula to ""take some of the algebra out of implicit differentiation"": Suppose the function $F(x,y)$ is differentiable and the equation $F(x,y)=0$ defines $y$ implicitly as a differentiable function of $x$ . Then at any point where $F_y\neq 0$ , we have $$\frac{dy}{dx}=-\frac{F_x}{F_y}$$ . (The formula itself is pretty intuitive to me, except for the negative sign.) I feel like I am misinterpreting the derivation given, as it seems to be using $F(x,y)$ to denote two different functions and treating them as if they are the same. The derivation goes like this: Suppose that (1) the function $F(x,y)$ is differentiable and that (2) the equation $F(x,y)=0$ defines $y$ implicitly as a differentiable function of $x$ . Since $w=F(x,y)=0$ , the derivative $\frac{dw}{dx}$ must be zero. As I understand this, they are defining a new function $w:\{(x,y):F(x,y)=0\}\rightarrow\{0\}$ , a level curve of the original $F(x,y)$ , which is zero everywhere on its domain, and we're to suppose that its domain defines $y$ implicitly in terms of $x$ . But then they continue: ... Computing the derivative [of the equation $w=F(x,y)=0$ ] from the chain rule, we find $$0=\frac{dw}{dx}=F_x\frac{dx}{dx}+F_y\frac{dy}{dx}=F_x+F_y\frac{dy}{dx}.$$ Therefore, we have $$\frac{dy}{dx}=-\frac{F_x}{F_y}.$$ This is where I get confused. In the example questions, it is clear that $F_x$ and $F_y$ denote the partial derivatives of the original function $F(x,y)$ of which $w$ is a level curve. But this use of the chain rule seems to assume that those are also the partials of w (which is a constant function, and should have zero derivatives, no?). I'm interpreting this as a special case of $$\frac{dw}{dt}=\frac{\partial w}{\partial x}\frac{dx}{dt}+\frac{\partial w}{\partial y}\frac{dy}{dt}$$ where $t=x$ , and where $\frac{\partial w}{\partial x}$ and $\frac{\partial w}{\partial y}$ are written as $F_x$ and $F_y$ . But I'm not seeing how the former and the latter partials are equivalent. Why can we assume both that $\frac{dw}{dx}=0$ and that $F_x=\frac{\partial w}{\partial x}$ , when $F_x$ is not zero in general? Or is that assumption not actually being made by using the chain rule this way? What am I missing or getting wrong here? I'd really appreciate if someone would set me on the right track so that I can get some intuition for why this theorem works.
Thanks!","['partial-derivative', 'multivariable-calculus', 'implicit-differentiation', 'chain-rule']"
4476844,"What will be the value of $ 2\cos^{2}\theta - 1 $ , if $ \cos^{4}\theta - \sin^{4}\theta = \frac{2}{3}$","The question was What will be the value of $ 2\cos^{2}\theta - 1 $ , if $ \cos^{4}\theta - \sin^{4}\theta = \frac{2}{3}$ Here is my working: $\cos^{4}\theta - \sin^{4}\theta = \frac{2}{3} $ $(\cos^{2}\theta)^{2}-(\sin^{2}\theta)^{2} = \frac{2}{3}$ $(\cos^{2}\theta-\sin^{2}\theta)\cdot(\cos^{2}\theta+\sin^{2}\theta) = \frac{2}{3}$ $\cos2\theta = \frac{2}{3}$ But I am stuck at this step so can anyone help me ?",['trigonometry']
4476860,Showing $\lim_{x\to\infty} x^n P(X >x)=0$,"Hello I am trying to self-teach myself probability by follow a lecture course and example sheets. However, I am completely stuck on the following problem. Let X be a real-valued random variable. Suppose that the moment-generating function $m(\theta) = E(e^{\theta X})$ is finite for some $θ > 0$ . Show that for $\forall n > 0$ , $\lim_{x\to\infty} x^n P(X > x) = 0$ . My attempt :  I am not even sure how I should use the moment-generating function in relation to the limit. The only step I can see so far is $P(X > x)  = 1 - F(x)$ . Any hints in the right direction would be greatly appreciated :) Update: Using Markov Inequality for the variable $e^{\theta X}$ which is non negative random I can say $P(e^{\theta X} \geq x) \leq \frac{E(e^{\theta X})}{x}$ which is a much better place to start. The answer is below for those who don't want spoilers.","['limits', 'probability']"
4476919,When does every group with order divisible by $n$ have a subgroup of order $n$?,"According to Sylow's theorem, every finite group with order divisible by $p^k$ for some prime $p$ has a subgroup of order $p^k$. Is this the best possible result in this direction? That is, if $n$ is not a power of a prime, does there always exist a group with order divisible by $n$ that does not have a subgroup of order $n$? EDIT: Just to clarify, I am aware that groups like this exist. The standard example seems to be $A_4$, which has order divisible by $6$ but no subgroup of order $6$. What I am looking for is a proof that a counterexample exists for any $n$ that is not a power of a prime.","['group-theory', 'abstract-algebra', 'finite-groups']"
4476940,Use combinatorial proof to show that $\sum\limits_{k=m}^{n}\binom{k}{m}=\binom{n+1}{m+1}$.,"I am going through a self-teaching journey in mathematics. Right now I am reading Book of Proof, by Richard Hammack, and in the Chapter on Counting, I came across the following exercise: Use combinatorial proof to show that $\sum\limits_{k=m}^{n}\binom{k}{m}=\binom{n+1}{m+1}$ . My proof was the following: Suppose $m\leq n$ and let $S=\{0,1,2,\ldots,m,\dots,n\}$ . Notice that $|S| = n+1$ and that the right hand side of the equation, by definition, is the number of subsets of order $m+1$ of $S$ . Now, we will count the numbers of subsets of order $m+1$ of $S$ in a different way: for every element $k$ of $S$ such that $m\leq k\leq n$ we count the number of subsets of order $m$ of $S$ such that, for every subset we construct, all elements are less than $k$ . By construction, there are always $k$ numbers less than $k$ in $S$ , therefore, there are $\binom{k}{m}$ such subsets. For each of these subsets, it's union with $\{k\}$ is a different subset of order $m+1$ of $S$ . Since $k$ could be any number in $[m,n]$ , we have that $\sum\limits_{k=m}^{n}\binom{k}{m}$ counts all subsets of $S$ with order $m+1$ . Since the right-hand and left-hand sides are solutions to the same counting problem, we conclude they are equal. $\blacksquare$ I desperately need some feedback on my proof-writing skills. It feels like my argument is solid, but at the same time, my writing might be somewhat convoluted. I don't know if it is clear enough, not straightforward enough, or if I explained too much. How detailed should I be? Feel free to be very critical. I wanna be able to write proofs acceptably.","['combinatorial-proofs', 'proof-writing', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
4476955,Computing surface area of a plane,"I am learning about surface integrals and flux integrals and I have done the following exercise for practice. Since I am still not entirely sure if I have understood correctly I would appreciate some feedback on my solution below (whether it is correct, if/what can be improved), thanks. Let $S$ be that portion of the plane $x+2y+2z=4$ lying in the first octant, oriented with outward normal pointing upward. Find (a) the area of $S$ ; (b) $\int_{S} (x-y+3z)\sigma$ ; (c) $\int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz$ where $\sigma=n_1dy\wedge dz+n_2dz\wedge dx+n_3dx\wedge dy$ is the area $2$ -form (and $\mathbf{n}=(n_1,n_2,n_3)$ is the outward-pointing unit normal to the surface $S$ .) What I have done: We parametrize the surface by $\mathbf{g}:\Omega\to\mathbb{R}^3,\ \mathbf{g}\begin{pmatrix}u\\v\end{pmatrix}=\begin{bmatrix} u\\v 
\\ \frac{4-u-2v}{2} \end{bmatrix}$ and note that $D\mathbf{g}=\begin{bmatrix}1 & 0\\0 & 1\\-\frac{1}{2} & -1\end{bmatrix}$ and that $\mathbf{n}=\frac{\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}}{\lVert\frac{\partial\mathbf{g}}{\partial r}\times\frac{\partial\mathbf{g}}{\partial\theta}\rVert}=\frac{\left(\frac{1}{2},1,1\right)}{\frac{3}{2}}=\begin{bmatrix}\frac{1}{3}\\\frac{2}{3}\\\frac{2}{3}\end{bmatrix}$ is a viable outward pointing unit normal vector so (a) $\text{area}(S)=\int_{S}\sigma=\int_{\Omega}\mathbf{g}^*\sigma=\int_{\Omega}\left(\frac{1}{2}\cdot \frac{1}{3}+\frac{2}{3}+\frac{2}{3}\right)dudv=\int_{\Omega}\frac{3}{2}dudv\overset{*}{=}\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=\frac{3}{2}\int_{u=0}^{u=4}\left(2-\frac{1}{2}u\right)du=6.$ ( $^*$ To be in the first octant it must be $u\geq 0,\ v\geq 0$ and $z=\frac{4-u-2v}{2}\geq 0\Leftrightarrow v\leq 2-\frac{1}{2}u$ .) (b) $\int_{S}(x-y+3z)\sigma=\int_{\Omega}\mathbf{g}^* \left( (x-y+3z)\sigma \right)=\int_{\Omega}\left( \left(u-v+\frac{3}{2}(4-u-2v)\right)\left(\frac{3}{2}\right) \right)dudv=\int_{u=0}^{u=4}\left(\int_{v=0}^{v=4}\left(-\frac{3}{4}u-6v+9\right)dv\right)du=\int_{u=0}^{u=4}\left(6-\frac{3}{8}u^2\right)du=16.$ (c) $\int_{S} zdx\wedge dy+ydz\wedge dx+xdy\wedge dz=\int_{\Omega}\mathbf{g}^*\left(zdx\wedge dy+ydz\wedge dx+xdy\wedge dz\right)=\int_{\Omega}\left( \left(\frac{4-u-2v}{2}\right)+v+u\cdot\frac{1}{2} \right)dudv=\int_{\Omega}2dudv=2\int_{u=0}^{u=4}\left(\int_{v=0}^{v=2-\frac{1}{2}u}dv\right)du=2\int_{u=0}^{u=2}\left(2-\frac{1}{2}u\right)du=8.$","['surface-integrals', 'multivariable-calculus', 'solution-verification', 'multiple-integral', 'differential-forms']"
4476967,Family of irreducible polynomials,"Consider the following family of polynomials $$P_n(X)=\sum_{i=0}^n(n+1-i)X^i,\,n\ge 1$$ Let’s write down the first few $$
\begin{align}
&P_1(X)=X+2\\
&P_2(X)=X^2+2X+3\\
&P_3(X)=X^3+2X^2+3X+4\\
&P_4(X)=X^4+2X^3+3X^2+4X+5
\end{align}
$$ My claim is that this family is a family of irreducible polynomials in $\Bbb{Z}[X]$ . I proved it for $n \le 5$ by the Eisenstein criterion after the change of variable $X=t+1$ For $n=6$ after the change of variable the polynomial writes as follows $$Q_6(t)=P_6(X-1)=t^6+8t^5+28t^4+56t^3+70t^2+56t+28$$ and the Eisenstein criterion (only workable $p=2$ ) doesn’t work anymore. By reducing $\bmod 7$ we prove the claim for $n=6$ . I tested with Mathematica and irreducibility is checked for $n\le 150$ . I noticed $$P_n(X)=XP_{n-1}(X)+n+1$$ But I am struggling to find a generic proof. I have given up on counterexample. Thanks for your help.","['irreducible-polynomials', 'polynomials']"
4476970,prove an integral inequality on the unit square,"Let $f$ be a continuous function on the unit square. Prove that $\int_0^1 (\int_0^1 f(x,y) dx)^2 dy + \int_0^1 (\int_0^1 f(x,y) dy)^2 dx \leq (\int_0^1 \int_0^1 f(x,y)dx dy)^2 + \int_0^1 \int_0^1 f(x,y)^2 dx dy.$ I think writing the integrals as Riemann sums and taking limits should yield the result. If one divides the unit square into $n^2$ equal squares and picks a point $(x_i, y_j)$ in each square and defines $a_{ij} = f(x_i, y_j)$ , then the corresponding inequality in terms of Riemann sums is $\frac{1}{n^3} \sum_i ((\sum_j a_{ij})^2 + (\sum_j a_{ji})^2) \leq \frac{1}{n^4} (\sum_{ij} a_{ij})^2 + \frac{1}{n^2}\sum_{ij} a_{ij}^2.$ How can I manipulate this inequality to get the result I want (e.g. maybe by writing it as $\sum_{ij} x_{ij}^2 \ge 0$ where the $x_{ij}$ 's are carefully selected)?","['integration', 'contest-math', 'real-analysis', 'calculus', 'inequality']"
4476985,Use combinatorial proof to show that $\sum\limits_{k=0}^{n}\binom{n}{k}\binom{k}{m}=\binom{n}{m}2^{n-m}$.,"I am going through a self-teaching journey in mathematics. Right now I am reading Book of Proof, by Richard Hammack, and in the Chapter on Counting, I came across the following exercise: Use combinatorial proof to show that $\sum\limits_{k=0}^{n}\binom{n}{k}\binom{k}{m}=\binom{n}{m}2^{n-m}$ . My proof was the following: Notice that the right-hand side of the equation is the number of ways we can make a string of length $n$ from the symbols $\{A,B,C\}$ with repetition allowed. First we choose the position of the $A$ 's. We can do that in $\binom{n}{m}$ ways, such that $m\leq n$ is the number of $A$ 's in our string. For the remaining $(n-m)$ positions, we have two options: they are either $B$ or $C$ , therefore there are $2^{n-m}$ ways of filling them up. Thus, as we claimed, $\binom{n}{m}2^{n-m}$ . Now we will count the number of ways we can make a string of length $n$ from the symbols $\{A,B,C\}$ with repetition allowed in a different way: first, we select the number of positions that are not $C$ , and fill the rest with $C$ 's. We can do this in $\binom{n}{k}$ ways, where $k$ is the number of positions that are not $C$ . From those $k$ positions, we choose the ones that are $A$ 's and fill them up, which we can do in $\binom{k}{m}$ ways, where $m$ is the number of $A$ 's in our string. Finally, fill every blank space with $B$ 's. Thus, the number of string with $(n-k)$ $C$ 's and $m$ $A$ 's is $\binom{n}{k}\binom{k}{m}$ . Since $k$ can be any number in $[0,n]$ , to count all the possible strings we have $\sum_{k=0}^{n}\binom{n}{k}\binom{k}{m}$ . Since the right-hand and the left-hand sides are solutions to the same counting problem, we conclude they are equal. $\blacksquare$ I desperately need some feedback on my proof-writing skills. It feels like my argument is solid, but at the same time, my writing might be somewhat convoluted. I don't know if it is clear enough, not straightforward enough, or if I explained too much. How detailed should I be? Feel free to be very critical. I wanna be able to write proofs acceptably.","['combinatorial-proofs', 'proof-writing', 'solution-verification', 'combinatorics', 'discrete-mathematics']"
