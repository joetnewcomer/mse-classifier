question_id,title,body,tags
2225936,How to solve first order non-linear ODE $y'=\frac{2x+y+4}{4x-2y}$,"I can solve first order non-linear ODE like below:
$$y'=\frac{2x+y+4}{4x+2y}$$
This is quite easy by substituting $2x+y$ with $u$. However, I have tried to solve the differential equation as title:
$$y'=\frac{2x+y+4}{4x-2y}$$
Note that the denominator has the negative sign $4x-2y%$. It seems that neither substituting nor setting $v=\frac{y}{x}$ is working. Please help me with the detailed answer. Thanks a lot.","['ordinary-differential-equations', 'nonlinear-system']"
2225972,"Is it true that $\emptyset\notin\{\emptyset,\{\emptyset\}\}$","I've been having a hard time understanding why $\emptyset\notin\{\emptyset,\{\emptyset\}\}$. Why isn't it an element in that set? Also why is $\{\emptyset\}\in\{\emptyset\}$ true? I know that $\{0\}\notin\{0\}$ so why is that one true and this false? Determine whether these statements are true or false. $\emptyset\in\{\emptyset\}$ $\emptyset\in\{\emptyset,\{\emptyset\}\}$ $\{\emptyset\}\in\{\emptyset\}$ $\{\emptyset\}\in\{\{\emptyset\}\}$ $\{\emptyset\}\subset\{\emptyset,\{\emptyset\}\}$ $\{\{\emptyset\}\}\subset\{\emptyset,\{\emptyset\}\}$ $\{\{\emptyset\}\}\subset\{\{\emptyset\},\{\emptyset\}\}$","['elementary-set-theory', 'discrete-mathematics']"
2226059,Partition of number vs dividing into blocks,"Let $P(n)$ be the number of partitions of integer $n$ and let $A(n,k)$ be the number of ways to put $n$ indistinguishable toys into $k$ distinguishable boxes but with the following restriction: the number of toys in $i$-th box must be divisible by $i$. For which $(n,k)$ we have $A(n,k) < P(n), \ A(n,k) = P(n), \ A(n,k) > P(n)$ ? My guess is $k < n, \ n \ge k$ and third inequality is never satisfied. Am i right? Why?
I suppose there is a bijection between $P$ and $A$. Let take random partition of $n$. Let's group up identical components and write them like $component \cdot number$. For those who appear only once we write $component \cdot 1$. It gives us the way of putting toys into boxes. For example, let's take:
$10 = 3 + 2 + 2 + 1 + 1 + 1$. We get $10 = 3 \cdot 1 + 2 \cdot 2 + 1 \cdot 3$. And it is exactly the way we can form it into boxes. We put one block of $3$ toys into box three (so $3$ toys), two blocks of $2$ toys into box number two (so $4$ toys) and three blocks of $1$ toys into box number one (so $3$ toys). On the other hand from the box setup we can get the original partition. For example we have $3$ toys in box one, and $3$ toys in box three. So we have 3 blocks of $1$ toy in box one and 1 block of $3$ toys in box three, so we get $3 \cdot 1 + 1 \cdot 3 = 3 + 1 + 1 + 1$. So having bijection we can say that if $k<n$ we won't have big enough box to get the partition of $n$ containing itself. If $n=k$ we are good, because we can get every partition and every box setup has its own partition as we have shown before. If $k>n$ nothing changes, because we we can't put anything else than $0$ in boxes number $n+1,...,k$ Is that OK?","['combinatorics', 'integer-partitions', 'discrete-mathematics']"
2226088,Open set between two curves,"Let $U := \{ (x,y) \in \mathbb{R}^2 : x^2 > y \} \;\cap\; \{ (x, y) \in \mathbb{R}^2 : x \geq 0 \}$. I want to prove (!) that $U$ is an open set. Therefore I made a sketch: Here $U$ is represented by the blue area.
The basic idea I had for the proof was to show that $U$ can be described as an infinite union of open spheres around all real points on the $x$-axis with $x > 0$. I've tried and  have come to the conclusion that there must be an easier way to prove this as it was part of an exam in my university. EDIT As stated in the comments, the sketch of $U$ above is wrong as e.g. $(0, -1) \in U$. It should rather look like this: And then it is obvious that $U$ cannot be open as there is no open surrounding of $(0, -1)$ which is a subset of $U$.",['general-topology']
2226089,"Prob. 2, Chap. 5 in Baby Rudin: How is the inverse function differentiable?","Here is Prob. 2, Chap. 5 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: Suppose $f^\prime(x) > 0$ in $(a, b)$. Prove that $f$ is strictly increasing in $(a, b)$, and let $g$ be its inverse function. Prove that $g$ is differentiable, and that 
  $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)} \qquad \qquad \qquad (a < x < b). $$ My effort: Let $x_1$ and $x_2$ be any two real numbers such that 
  $$ a < x_1 < x_2 < b.$$ 
  Then $f$ is differentiable on the interval $\left[ x_1, x_2 \right]$ and hence on the segment $\left( x_1, x_2 \right)$, and $f$ is of course continuous on the interval $\left[ x_1, x_2 \right]$. So, by the Mean-Value Theorem, we can find a point $x \in \left( x_1, x_2 \right)$, such that 
  $$ f\left( x_2 \right) - f\left( x_1 \right) = \left( x_2 - x_1 \right) f^\prime (x) > 0, $$
  and so 
  $$  f\left( x_1 \right) <  f\left( x_2 \right) \qquad \qquad \qquad ( a< x_1 < x_2 < b), $$
  and therefore $f$ is strictly increasing on $(a, b)$. As $f$ is strictly increasing on $(a, b)$, so it is injective, and therefore the inverse function $g$ exists; this $g$ is a mapping of $\mathrm{range} f$ into (rather onto) $(a, b)$, and is defined by 
  $$ g \left(  y \right) = x \ \mbox{ for all } \ y = f(x) \in \mathrm{range} f.$$ 
  Thus the mapping $h$ of $(a, b)$ into $(a, b)$, given by 
  $$ h(x) = g \left( f(x) \right) \qquad \qquad \qquad ( a< x < b),$$
  is the identity mapping. So 
  $$h^\prime(x) = 1 \qquad \qquad \qquad (a < x < b). \tag{1} $$
  But by Theorem 5.5 in Baby Rudin (i.e. the Chain Rule), we know that if $g^\prime$ exists at each point in the range of $f$, then we obtain 
  $$h^\prime(x) = g^\prime\left( f(x) \right) f^\prime(x) \qquad \qquad \qquad (a < x < b). \tag{2} $$
  From (1) and (2), we can conclude that 
  $$ g^\prime\left( f(x) \right) f^\prime(x) = 1 \qquad \qquad \qquad (a < x < b), $$
  and since $f^\prime(x) > 0$ for all $x \in (a, b)$, therefore 
  $$ g^\prime\left( f(x) \right) = \frac{1}{f^\prime(x)  } \qquad \qquad \qquad (a < x < b). $$ Is my reasoning so far correct? If so, then how to show that the function $g$ is indeed differentiable (at each point in the range of $f$)? Or, is there a problem in my reasoning above? P.S.: After reading the above answers, I have arrived at this solution to my original problem and would be grateful for the appraisal thereof of the Math SE community. As $f$ is differentiable at each point in $(a, b)$, so $f$ is continuous in $(a, b)$, and as $f^\prime(x) > 0$ for every $x \in (a, b)$, so $f$ is strictly increasing on $(a, b)$. Thus $f$ is a continuous bijective mapping of $(a, b)$ onto the range of $f$. We now show that the range of $f$ is also an open interval. Suppose $y_1$ and $y_2$ are any two points in the range of $f$ such that $y_1 < y_2$, and suppose that $y$ is any real number such that $y_1 < y < y_2$. As $y_1$ and $y_2$ are in the range of $f$, so there exist some points $x_1$ and $x_2$ in $(a, b)$ such that $f\left(x_1 \right) = y_1$ and $f \left( x_2 \right) = y_2$; moreover, as $y_1 < y_2$ and as $f$ is strictly increasing, so we must also have $x_1 < x_2$, for otherwise we would obtain $f\left( x_1 \right) \geq f\left( x_2 \right)$. Now as $f$ is continuous on the interval $\left[ x_1, x_2 \right]$ and as $$y_1 = f \left( x_1 \right) < y < f \left( x_2 \right) = y_2,$$ so by the intermediate-value theorem for continuous functions, there is some point $x \in \left( x_1, x_2 \right)$ such that $y = f(x)$. Thus we have shown that, for any points $y_1$ and $y_2$ in the range of $f$ such that $y_1 < y_2$, the segment $\left( y_1, y_2 \right)$ is also contained in the range of $f$. That is, the range of $f$ is an interval. We now show that the range of $f$ is an open interval. For any point $y_0$ in the range of $f$, we have a unique point $x_0 \in (a, b)$ such that $y_0 = f\left(x_0\right)$. But as $a < x_0 < b$, so we also have $$ a < \frac{a+x_0}{2} < x_0 < \frac{x_0+b}{2} < b,$$ and therefore $$ f \left( \frac{a+x_0}{2} \right) < f\left( x_0 \right) < f \left( \frac{x_0+b}{2} \right);$$ that is, for any point $y_0$ in the range of $f$, we have points $f\left(\frac{a+x_0}{2} \right)$ and $f \left( \frac{x_0+b}{2} \right)$ in the range of $f$ such that $$ f\left(\frac{a+x_0}{2} \right) < y_0 < f \left( \frac{x_0+b}{2} \right).$$ So the range of $f$ has no maximum element and no minimum element. Hence the range of $f$ is a possibly infinite (on either side) open interval, say $(c, d)$. Let the function $g \colon (c, d) \to (a, b)$ be the inverse of the function $f \colon (a, b) \to (c, d)$. We show that $g$ is continuous by showing that the inverse image under $g$ of every open set in $(a, b)$ is open in $(c, d)$. For this it suffices to show that the inverse image under $g$ of every open interval $(u, v) \subset (a, b)$ is open in $(c, d)$. But as $f$ and $g$ are bijective and are the inverses of each other, so $$g^{-1} \left[ (u, v) \right] = f \left[ (u, v) \right]. $$
  We now show that $$f \left[ (u, v) \right] = \left( f(u), f(v) \right).$$ 
  Suppose $y \in f \left[ (u, v) \right]$. Then $y = f(x)$ for a (unique) point $x \in (u, v)$. As $a < u < x < v < b$ and as $f$ is strictly increasing on $(a, b)$, so we must have $f(u) < f(x) < f(v)$, that is, $y \in \left( f(u), f(v) \right)$. Conversely, suppose that $y \in \left( f(u), f(v) \right)$. Then $f(u) < y < f(v)$, and $f$ is continuous on the closed interval $[u, v]$; so (by the intermediate-value theorem for continuous functions)  there is some (unique) point $x \in (u, v)$ such that $y = f(x)$, which implies that $y \in f\left[ (u, v) \right]$. Therefore, $f \left[ (u, v) \right] = \left( f(u), f(v) \right)$, which is open in $(c, d)$, as required. Thus the inverse function $g \colon (c, d) \to (a, b)$ is continuous whenever $f^\prime(x) > 0$ for all $x \in (a, b)$. Now let $y_0 \in (c, d)$. Then there is a unique point $x_0 \in (a, b)$ such that $y_0 = f\left( x_0 \right)$, which implies that $g \left( y_0 \right) = x_0$. Since $g$ is continuous at $y_0$, so $$ \lim_{y \to y_0} g(y) = g\left( y_0 \right); $$ 
  that is, $$\lim_{y \to y_0} x = x_0,  \ \mbox{ where } \ x = g(y); $$
  thus, as $y \to y_0$ in $(c, d)$, the point $x = g(y) \to x_0$ in $(a, b)$. Now as $f^\prime\left( x_0 \right) \neq 0$, so we find that 
  $$g^\prime \left( y_0 \right) = \lim_{y \to y_0 } \frac{ g(y) - g\left( y_0 \right) }{ y - y_0 } = \lim_{x \to x_0 }  \frac{ x - x_0 }{ f(x) - f \left( x_0 \right) } = \lim_{x \to x_0 } \frac{ 1 }{ \frac{ f(x) - f\left( x_0 \right) }{ x - x_0 } } = \frac{1}{ f^\prime \left( x_0 \right) },$$ 
  which shows that $g^\prime$ exists at each point $y_0 \in (c, d)$, and also that $$ g^\prime \left( y_0 \right) = \frac{1}{ f^\prime \left( x_0 \right) },$$ as required. Have I finally managed to get this solution correct?","['derivatives', 'real-analysis', 'inverse-function', 'calculus', 'analysis']"
2226138,Maximizing the probability of a urn problem,"How can 20 balls, 10 white and 10 black, be put
  into two urns so as to maximize the probability of
  drawing a white ball if an urn is selected at random
  and a ball is drawn at random from it? Intuitively i know the right answer: put 1 white, 0 blacks in one urn and 9 white and 10 blacks in the other, but i just want to arrive at it with more mathematical arguments. Here's my attempt: Define the events: W:= the ball taken is white B:=the ball taken is black $U_1$:=the ball taken is from urn 1 $U_2$:= the ball taken is from urn 2. We are interested in W event, which can be written as the following disjoint union: $W= WU_1\cup WU_2 $. Hence, $P(W)=P(W|U_1)P(U_1)+P(W|U_2)P(U_2)$. Supposing that we put $u$ balls in the first urn, and that from these $w$ are white, we have the following distribuition: $U_1:$  $u$ balls, $w$ white and $u-w$ black. $U_2:$ $20-u$ balls, $10-w$ white and $20-10+w$ black. Therefore, $P(W)=\frac{w}{2u}+\frac{10-w}{2(20-u)}$ I'm actually having trouble in order to maximize it. Can someone help?",['probability']
2226171,Example of compact non-Hausdorff space that fails to be locally compact,Can you give an example of a compact non-Hausdorff space with a point which has no local base formed by compact neighborhoods? Thank you in advance.,['general-topology']
2226195,Pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous,"A real-valued function $f:X \rightarrow \mathbb{R}$ is upper semicontinuous if for each $c \in \mathbb{R}$, its pre-image $f^{-1}(-\infty,c)$ is open in $X.$ In encyclopaedia , there is the following statement: Let $\mathcal{F}$ be an arbitrary family of upper semicontinuous functions on a given topological space $X$. Then the function $F(x) = \inf_{f \in \mathcal{F}} f(x)$ is upper semicontinuous. The article stated that the proof can be found in General Topology by Bourbaki, Chapter $5 - 10$. But I do not have access to that book. In Wikipedia , similar statement appears. Likewise, the pointwise infimum of an arbitrary collection of upper semicontinuous functions is upper semicontinuous. I manage to prove the statement holds for countably many functions, its proof is similar to proving infimum of measurable functions is measurable, but I have no idea on how to prove for arbitrary case.","['general-topology', 'real-analysis', 'supremum-and-infimum', 'semicontinuous-functions']"
2226196,Exponential equation,"""Let $a, b\in(1, +\infty)$ fixed. Solve the equation:$a^{a^t}=b^{\frac{1}{t^2}\cdot b^\frac{1}{t}}$. "" This problem is from G.M. 3/2017. I can't solve it. For $t\geqslant0$ i showed that there is a unique solution by writing the equation $$t\cdot a^t \cdot \ln{a} = \frac{1}{t} \cdot b^\frac{1}{t} \cdot\ \ln{b}$$
in wich the left member is increasing and the right one is decreasing, but I couldn't find the solution, and for negative $t$ I don't know what to do.","['real-numbers', 'functions', 'systems-of-equations']"
2226205,Nonlinear differential equation involving sine,"We know from the usual pendulum differential equation that it involves the sine function, and to solve it, usually we use the small angle approximation. But to fully solve it, the method used is to get the first integral, suppose we have $$\frac{d^2\theta}{dt^2} + B \sin(\theta) = 0$$ we can multiply both sides by $\frac{d\theta}{dt}$, so that $$\frac{d\theta}{dt}\frac{d^2\theta}{dt^2} + B \sin(\theta)\frac{d\theta}{dt} = 0$$ then the first integral is, $$\frac{d}{dt}\Bigg(\frac{1}{2} \Bigg(\frac{d\theta}{dt}\Bigg)^2 - B \cos(\theta) \Bigg) = 0 $$ Now, I have this nonlinear differential equation but it has a first order term in it instead of just the second order and the zeroth order, $$\frac{d^2\theta}{dt^2} + A \frac{d\theta}{dt} + B \sin(\theta) = 0$$ $A$ and $B$ are just constants. How do I go about solving this? Is there a technique or other way to solve this?","['ordinary-differential-equations', 'nonlinear-system']"
2226222,Find parametar λ if plane and line don't intersect,"How can I find λ, if I have line $3\lambda x-2=3y+1=\lambda z$ and plane $\lambda x-3y+2z-3=0$ and they don't intersect. The given solution says that λ=3, but I don't have any idea how can I come to that solution. Small hint would be helpful...",['geometry']
2226229,Rank $1$ bilinear form is a product of two linear functionals on a finite dimensional vector space.,"Let $\mathbb{f}$ be a  non-zero bilinear form on a finite dimensional vector sppace $V.$ Then have to show that  $\mathbb{f}$ can be expressed as a product of two linear functionals i.e.,  $\mathbb{f}(\alpha, \beta)=L_1(\alpha)L_2(\beta)$ for $L_i \in V^*$ iff  $\mathbb{f}$ has rank $1.$ I proved that if  $\mathbb{f}$ is product of two linear functional then its rank is $1$ using left operator and the right operator. I am looking for the proof of the other direction. Help me. Many thanks.","['matrices', 'bilinear-form', 'linear-algebra', 'multilinear-algebra']"
2226280,How can I prove that this sequence is monotonic?,"I have a sequence $(u_n)$ that is defined as: $u_0 = 2$, $u_{n+1} =\frac{u_n}{2} + \frac{1}{u_n}$ I have tried to prove that it is monotonic using induction but I wasn't able to succeed. How can I prove it easily ? Thank you",['real-analysis']
2226308,Solution of differential equation which is quadratic in $\frac{dy}{dx}$,"Consider the following differential equation
$$ x\frac{dy}{dx} + y = x^4(\frac{dy}{dx})^2$$ I used the quadratic formula and got $\frac{dy}{dx} = \frac{x \pm \sqrt {x^2 + 4yx^4}}{2x^4}$. Now how to proceed or is there any other method? Edit: I proceeded further this way $\frac{dy}{dx} = \frac{1 \pm \sqrt {1 +4yx^2}}{2x^3}$ Let $1 + 4yx^4 = t^2$ then $8yx + 4x^2\frac{dy}{dx} = 2t\frac{dt}{dx}$ multiplying throughout with $x$ I got $4yx^3 +2x^3\frac{dy}{dx} = tx\frac{dt}{dx}$ now putting the value of $2x^3\frac{dy}{dx}$ and $t^2$ in 1st equation I got
$tx\frac{dt}{dx} - t^2 = \pm t$
which gave 2 solutions $$t = 0$$ $$and$$ $$x\frac{dt}{dx} = t \pm 1$$ 
Using $t  = 0$ I got one solution $$1 + 4x^2y = 0$$ and the other by solving the differential equation $x\frac{dt}{dx} = t \pm 1$ as $$c^2x^2 + 4y^2x^2 = 4cyx^2 + 2c$$ Now the answer to this question which I have got is $$4x^2y + 1 = 0$$ $$and$$ $$xy - c^2x + c = 0$$ Why I couldn't get the second solution right?",['ordinary-differential-equations']
2226343,Help with a question on Taylors Theorem,"Let $g:R→R$ be a twice differentiable function satisfying $g(0)=1,g'(0)=0$ and $g''(x)−g(x)=0$ for all $x \in \mathbb{R}$ Fix $x \in R$. Show that there exists $M>0$ such that for all natural numbers $n$ and all $\theta$ from 0 to 1
  $|g^{(n)}(\theta x)|≤M$ Also, find the coefficients of the Taylor expansion of $g$ about $0$, and prove that this expansion converges to $g(x)$ for all $x \in \mathbb{R}$. I have proved that $g(x)$ has derivatives of all orders, not sure how to use this to proceed with the above question.","['real-analysis', 'analysis']"
2226382,Lines from the centers of squares on two sides of a triangle to the third side?,"I have been working on the following problem from Visual Complex Analysis . My question is not necesarily if the solution is right, but more of a meta question about the solution and complex numbers. I apologize in advance if the question is a little vauge. Construct two squares on the sides of an arbritrary triangle. Prove that the lines connecting the centers of the squares to $m$, the midpoint of the third side, are perpendicular and of equal length. I solved this question by following the solution of a very similar exercise from the same book. (Place?) the figure in the complex plane, and let the sides of the trianle be the complex numbers $2a, 2b, 2c$. Then $s$ is the complex number $a+ia$. Furthermore, $p$ is $2a + b + ib$, and $m$ is $2a + 2b + c$. $s-m = -a-2b-c + ia$ $p-m = -b-c+ib$ are the dotted lines from $s$ to $m$ and from $p$ to $m$, respectively. Using the fact that $2a+2b+2c = 0$, it is not difficult to show that $i(p-m) = s-m$, which concludes the proof. I am used to thinking of a complex number as a point in a plane, with coordinates. But this understanding seems to go completely out the window. complex numbers are no longer points in the complex plane; instead, they are arrows. If we place them all at the origin, the result would not resemble the geometry problem at all. But it seems that somehow we are allowed to move all these arrows in any way we want.","['complex-numbers', 'geometry']"
2226392,Solving many quadratic programs with the same objective matrix,"If one wants to solve many linear systems with the same matrix,
$$\mathbf A\mathbf x_1 = \mathbf b_1, \quad \mathbf A\mathbf x_2 = \mathbf b_2, \quad \ldots, \quad \mathbf A\mathbf x_k = \mathbf b_k,$$
it is preferable to first precompute a factorization of the matrix $\mathbf A$, so that each system with different right-hand sides can be solved efficiently. In my case, $\mathbf A$ is symmetric and positive definite, so the Cholesky factorization works well. Of course, solving $\mathbf A\mathbf x_i = \mathbf b_i$ is equivalent to solving the unconstrained quadratic minimization $\min \tfrac12\mathbf x_i^T\mathbf A\mathbf x_i - \mathbf b_i^T\mathbf x_i$. I am interested in solving many such problems, but with additional inequality constraints,
$$\begin{gather}
\min & \tfrac12\mathbf x_i^T\mathbf A\mathbf x - \mathbf b_i^T\mathbf x_i \\
\text{s.t. } & \mathbf C_i\mathbf x \succeq \mathbf d_i.
\end{gather}$$
The constraints and the linear term of the objective both vary between problems, but the quadratic term remains constant. Is there an analogous method for speeding up such problems by exploiting a factorization of $\mathbf A$? For equality constraints ($\mathbf C_i\mathbf x = \mathbf d_i$) we have tried the Uzawa method , but it does not support inequalities. More details: The matrix $\mathbf A$ is sparse, symmetric, and positive definite. Its sparse Cholesky factorization is available, but I can also precompute other factorizations if necessary. If a general solution is not possible, I am also interested in the special case where each constraint acts on only one degree of freedom, i.e. each row of $\mathbf C_i$ has only one nonzero entry.","['optimization', 'numerical-linear-algebra', 'linear-algebra', 'quadratic-programming']"
2226394,"If $a_n+b_n\sqrt{3}=(2+\sqrt{3})^n$, then what's $\lim_{n\rightarrow\infty}\frac{a_n}{b_n}$?","Let $a_n$ and $b_n$ be integers defined in the following way: $$a_n+b_n\sqrt{3}=(2+\sqrt{3})^n.$$ Compute $$\lim_{n\rightarrow\infty}\frac{a_n}{b_n}.$$ I tried expanding using binomial theorem:
$$ (2+\sqrt{3})^n=\binom{n}{0}2+\binom{n}{1}2^2\sqrt{3}+\binom{n}{2}2^3\cdot 3+\ldots+\binom{n}{n-1}2\sqrt{3}^{n-1}+\binom{n}{n}\sqrt{3}^{n}$$
and I think we have that: $$ a_n=\binom{n}{0}2+\binom{n}{2}2^3\cdot 3+\ldots+\binom{n}{n}\sqrt{3}^{n}$$ $$ b_n=\binom{n}{1}2^2\sqrt{3}+\ldots+\binom{n}{n-1}2\sqrt{3}^{n-1}$$ But, frankly, I do not know what to do next.","['binomial-theorem', 'limits']"
2226431,"Can six ""$1\times1\times1$ pyramids"" fit inside a $1\times1\times1$ cube? [duplicate]","This question already has answers here : Equidecomposability of a Cube into 6 Trirectangular Tetrahedra (4 answers) Closed 7 years ago . Consider the pyramid $\left \{ (x, y, z): x,y,z\geq 0, x+y+z\leq 1 \right \}$. This is a pyramid that has $3$ pairwise-orthogonal edges, all of length $1$ (just like a corner of a room): From now on I'll call it a "" unit pyramid "" (UP).
One can easily check that the volume of a UP is exactly $\frac{1}{6}$.
So that raises the question: Can we take six UPs and fit them all perfectly inside a unit cube? When I thought about it, my initial answer was an obvious yes. So I tried to imagine such arranging, but I couldn't. I was quite surprised since I usually have a very good spatial vision. Then I tried drawing a solution and failed. Since I'm not tech-savvy enough to use a 3D graphics software, I decided to take this problem into the real world. So here's a unit pyramid: It's composed of 3 edges of length 1 (which I set to be $5$ cm) and 3 edges of length $\sqrt2$ ($\approx 7$ cm). Now, there's only one reasonable way to attach another one to it, if our goal is a unit cube: (since now we have our square base). Now I added all the edges necessary to form a cube: And as your 3D mind can see, now we have four unit pyramids: It's pretty hard to see the region that has stayed uncovered by our 4 UPs, so here it is, emphasized: And what do you know - that's a regular tetrahedron !
A quick reality check: a regular tetrahedron with edges of length $\sqrt 2$ has a volume of $\sqrt 2 (\sqrt 2)^3/12=4/12=1/3$, which suits the fact that we entered 4 UPs inside. But now, I'm pretty certain that you can't fit another 2 of them inside. To be sure, I constructed them seperately (last picture, I promise!): On the left there's a UP, and on the right - our regular tetrahedron (would you believe that one is twice the volume of the other?)
Now you'll just have to trust me that it won't fit. Well, that convinced me that the mission of fitting 6 unit pyramids inside a unit cube is impossible, but of course I wouldn't call it a mathematical proof. Does anyone have a more mathematically-convincing argument? I'd be curious to hear your ideas and thoughts about this. Thank you for your time reading the question!","['solid-geometry', 'geometry']"
2226451,"If $f(x)=ax^2$ for $x\in\mathbb{Q}$ and $f$ is continuous, then is $f(x)=ax^2$ for $x\in\mathbb{R}$?","As in the title. Let's say that the function $f:\mathbb{R}\rightarrow\mathbb{R}$ is given by $f(x)=ax^2$ for $x\in\mathbb{Q}$. Then, if we also have that $f$ is continuous everywhere, is $f(x)=ax^2$ for $x\in\mathbb{R}$? It seems quite likely (mostly by intuition) to me, yet I'd gladly see any proof (hints...) of counterexample.","['continuity', 'rational-numbers', 'functions']"
2226528,Probability of eighth best reaching the semifinals,"$16$ players $P_1, P_2, ..... P_{16}$ take part in a tennis knockout tournament. The order of the matches is chosen in random. Lower suffix player is better than higher suffix, the better wins. What is the probability that the eighth best reaches the semifinals? Basically I think that we just have to keep selecting only those players who are below $P_8$. For the first round the probability of choosing a player below $P_8$ would be $\frac{8}{15}$. But then I find it difficult to arrange the players who got selected in the next round.",['probability']
2226563,How to prove that $\lim_{x\to 1^-} \left(\left(\sum_{n=1}^{\infty}x^n \right)\cdot \log\left(\frac{1}{x}\right) \right)= 1$ WITHOUT computing the sum,"That's it. I want to know a way to do it so i can use it to prove other results like this ones:
$$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^2} \right)\cdot \sqrt{\log\left(\frac{1}{x}\right)} \right)= \frac{\sqrt{\pi}}{2}$$
$$ \lim_{x \to 1^-} \left(\left(\sum_{n=1}^{\infty}x^{n^m} \right)\cdot \log\left(\frac{1}{x}\right)^{1/m} \right)= \frac{\Gamma \left(\frac{1}{m}\right)}{m}$$","['summation', 'gamma-function', 'limits']"
2226602,Prove $\lim_{n\to\infty}(1+1/n)^n=e$,"I would like to show that $\lim_{n\to\infty}(1+1/n)^n=e$, using the binomial theorem and the power series expansion of $e$. So to be clear: I do not want to use that $e^x:=\lim_{n\to\infty}(1+x/n)^n$, because that's not how I've learned the definition of $e^x$. Basically I would like to show the following:
$$
\lim_{n\to\infty}\sum_{k=0}^n\begin{pmatrix}n\\k\end{pmatrix}\left(\frac{1}{n}\right)^k=\sum_{k=0}^\infty\frac{1}{k!}
$$
I'm guessing this should be possible. Maybe some rewriting could work:
$$
\sum_{k=0}^n\begin{pmatrix}n\\k\end{pmatrix}\left(\frac{1}{n}\right)^k=\sum_{k=0}^n\frac{1}{k!}\cdot\frac{n!}{(n-k)!}\cdot\left(\frac{1}{n}\right)^k
$$ EDIT (I was shown a mistake in the comments) So I have to show the following:
$$
\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{k!}\frac{n!}{(n-k)!}\cdot\left(\frac{1}{n}\right)^k=\lim_{n\to\infty}\sum_{k=0}^n\frac{1}{k!}.
$$ This seems doable, given that I see the term $\begin{align}\frac{1}{k!}\end{align}$ at both sides. So what can I do with $\begin{align}\frac{n!}{(n-k)!}\cdot\left(\frac{1}{n}\right)^k\end{align}$?","['power-series', 'sequences-and-series', 'limits-without-lhopital', 'limits']"
2226618,Show that $G/H$ has the trivial topology if $H \subset G$ is dense,"I'm having trouble on this assignment question and was hoping someone can point me in the right direction. Let $G$ be a group equipped with a Hausdorff topology in which the group operation and inversion are continuous. Let $H$ be a subgroup. Let $g_1 \sim g_2$ if $g_1h = g_2$ for some $h \in H$ and denote by $G/H$ the set of equivalence classes. Let $G/H$ have the quotient topology. Prove that if $H$ is dense in $G$ , then $G/H$ has the trivial topology. My attempt: The goal is to show that if $A \subset G/H$ is nonempty and open, then $A = G/H$ . Let $\rho$ denote the standard quotient map from $G$ to $G/H$ that maps each $g \in G$ to the equivalance class containing $g$ . Suppose $A$ contains the element $C = \{g_1, g_2, ...\} \in G/H$ . ( $C$ is an equivalence class.) Then, $C \subset \rho^{-1}(A)$ . But $C = gH$ for some $g \in G$ , and $H$ is dense, so $C$ is also dense in $G$ . So, $G = \overline{C} \subset \overline{\rho^{-1}(A)}$ , and so $\overline{\rho^{-1}(A)} = G$ . I think we need to show that $\overline{\rho^{-1}(A)} = \rho^{-1}(\overline{A})$ . Because that would give us $\overline{A} = \rho(G) = G/H$ (which doesn't quite show the desired result $A = G/H$ , so maybe I'm going in the wrong direction). But we only have $\rho^{-1}(\overline{A}) \subset \overline{\rho^{-1}(A)}$ from the continuity of $\rho^{-1}$ . This is where I'm stuck. Any help would be greatly appreciated!",['general-topology']
2226769,Multiplicative Partitions $abcd=2^8\cdot 3^8$,"Suppose $a,b,c,d$ are positive integers and $abcd=2^8\cdot 3^8$. We consider permutations of an $(a,b,c,d)$ solution identical. For example, solutions $(1,1,1,2^8\cdot 3^8),(1,1,2^8\cdot 3^8,1), (1,2^8\cdot 3^8,1,1), (2^8\cdot 3^8,1,1,1)$ are considered equivalent. How many solutions $(a,b,c,d)$ are there?",['combinatorics']
2226771,Combinatorial reasoning in an expected value problem,I saw this problem: There is an urn with $a$ red and $a$ blue balls. We are drawing from it without replacement until we have drawn all the blue balls (we know there are $a$ of them). What is the expected value of the number of balls remaining in the urn? This can be solved by taking $a+ (a-1) {a \choose 1} +  (a-2) {a+1 \choose 2}+\dotsb+1 {2a-2 \choose a-1 } $ and applying the identity $\sum_{i=0}^{k} {n+i \choose i}= {n+k+1 \choose k} $ multiple times. It gives $\frac {2a \choose a+1} {2a \choose a}$ which simplifies to $\frac {a}{a+1}$. Is there a reasoning that produces this result directly?,"['combinatorics', 'expectation', 'probability']"
2226849,Show that $B \cong \mathbb Z /2\mathbb Z$ for a Boolean field B,"The title says it already. For a Boolean ring B which is also a field, I have to prove that the relation $B \cong \mathbb Z /2\mathbb Z$ holds. Any help? I have to prove this as a part of an exercise and in the previous part of the exercise, I have already shown that $x+x=0$ for any $x\in B$ and that $B$ is commutative, so my guess would be that I somehow have to use these facts in my proof.","['abstract-algebra', 'ring-theory']"
2226864,"Can one determine the joint distribution of $(X,Y)$ from the probability densities of $X$, $Y$, and $X+Y$?","Can one determine the joint distribution of $(X,Y)$ from the probability densities of $X$, $Y$, and $X+Y$? Here, $X$ and $Y$ are random variables from a sample space $(\Omega, \mathbb{P}) \to \mathbb{R}$. This is NOT a homework question.",['probability']
2226876,"Is the sequence $a_{n+1}=a_n-\frac{1}{a_n}$, $a_0=2$ bounded?","The sequence again for convenience
$$
a_{n+1}=a_n-\frac{1}{a_n},\;a_0=2
$$
My friend asked me this question and I do not know how to tackle it. It's clear it does not have a limit, but I am not sure whether it is unbounded; it seems to oscillate with a large amplitude when you simulate it numerically. I also can't seem to get anywhere with generating functions, but I also don't know how to use them for nonlinear recurrence relations.","['recurrence-relations', 'real-analysis', 'sequences-and-series']"
2226898,Is this question wrong? I am getting $\frac{\pi}{4}$,"Again going through the IIT advanced questions, I came across this one: Question: Suppose $f(x)=3x^3-13x^2+14x-2$ and $\alpha,\beta,\gamma$ are the roots of $f(x)=0$ such that $\alpha<\beta<\gamma $ then $$\tan^{-1}([\alpha])+\,\tan^{-1}([\beta-1])+\,\tan^{-1}([\gamma-1]) =\,??$$ Note: $[x]$ symbolises Greatest Integer less than or equal to $x$ Options: A: $\frac{3\pi}{4}$     B: ${\pi}$     C: $\frac{\pi}{2}$ D: Can't  be Decided I figured out $\alpha$ to be in $(0,1)$,
$\beta$ to be in $(1,2)$ and $\gamma$ to be in $(2,3)$. according to that, the answer should be coming $\frac{\pi}{4}$ but no option given. Some help please?","['polynomials', 'trigonometry', 'functions']"
2226912,Prove the matrix is not diagonalizable,"For any reals, $a, b$ show that $$D = \begin{bmatrix}
a & 1\\ 
0 &  b
\end{bmatrix}$$
  Is not diagonalizable. I got that the eigenvalues are $\lambda_1 = a, \lambda_2 = b$ However, for $a \ne b$, there are $2$ distinct eigenvalues so $D$ must be diagonalizable right? So what about $a= b$?","['matrices', 'linear-algebra']"
2226913,Elementary Geometry: Prove that ABCD is a tangential quadrilateral!,"The task is as follows: ABCD is a convex quadrilateral with points E, F, G and H on its sides AB, BC, CD and DA so that they divide the respective side at the ratio of the adjacent sides, so $$\frac {AE}{EB}=\frac {DA}{BC}, \frac {BF}{FC}=\frac {AB}{CD}, \frac {CG}{GD}=\frac {BC}{DA}, \frac {DH}{HA}=\frac {CD}{AB}.$$
  Show that ABCD is a tangential quadrilateral if and only if EFGH is a cyclic quadrilateral. I have already shown that if ABCD is a tangential quadrilateral, EFGH is a cyclic quadrilateral. But I have not succeeded in proving the reverse direction yet. I cannot figure out what to make out of these ratios. Thank you in advance!","['circles', 'quadrilateral', 'euclidean-geometry', 'geometry']"
2226934,Determining ring homomorphisms on Boolean ring of power set of some set,"Let $X$ be a set and $R=P(X)$, the collection of all subsets of $X$. For $A,B\in R$, define: $$A+B=(A\cup B)-(A\cap B),$$
$$AB=A\cap B,$$ where $-$ represents complement. With these operations, $R$ becomes a Boolean ring. Let $V\in R$ be non-empty. Concider two maps $f,g: R\rightarrow R$ defined as
$$f:A\mapsto A\cap V,$$
$$g:A\mapsto A\cup V.$$ I have to determine which, if any, of these maps is a ring homomorphism. In the definition of homomorphism I handle, it is required that, for a homomorphism $h$, $h(1)=1$. I have found that both functions are not homomorphisms, but I'm not quite sure of it as my proof is far from rigorous (and involves Venn-diagrams). Can someone please confirm/ disprove my beilef and maybe present a rigorous proof?","['abstract-algebra', 'ring-theory', 'elementary-set-theory']"
2226935,What would be a criteria to discover if a state is a tensor product or not?,"In Quantum Mechanics states of composite systems are described by tensor products. As is known, if we have $\mathcal{H}_1$ and $\mathcal{H}_2$ (which of course could be the same space) in the tensor product $\mathcal{H}_1\otimes \mathcal{H}_2$ we have the so called factorizable tensors which are writen as $|\psi_1\rangle\otimes |\psi_2\rangle$ with $|\psi_i\rangle \in \mathcal{H}_i$ but we also have more general elements which cannot be factored this way. Indeed if $|u_n(i)\rangle$ is a basis of $\mathcal{H}_i$ then $|u_n(1)\rangle \otimes |u_m(2)\rangle$ is a basis of $\mathcal{H}_1\otimes \mathcal{H}_2$ so that a general element is $$|\psi\rangle = \sum_{nm} c_{nm}|u_n(1)\rangle \otimes |u_m(2)\rangle.$$ My question is: if we have one $|\psi\rangle$ and we want to determine whether or not it is a factorizable tensor, what would be a useful criteria to determine this?","['quantum-mechanics', 'tensor-products', 'hilbert-spaces', 'functional-analysis', 'linear-algebra']"
2226956,"Trigonometric Integration ${\int_{0}^{2\pi}} \sin(2x)\cos(3x)\, dx$","$${\int_{0}^{2\pi}} \sin(2x)\cos(3x)\, dx$$ I think you need to use a double angle formula but I am not sure how I am supposed to split the $3x$.","['definite-integrals', 'integration', 'trigonometry', 'calculus']"
2226966,Four-point condition for tree distances: is there a detropicalization proof?,"Theorem 1. Let $G$ be a tree.
Let $x$ , $y$ , $z$ and $w$ be four vertices of $G$ .
For any two vertices $s$ and $t$ of $G$ , let $d \left(s, t\right)$ denote the minimum length of a path from $s$ to $t$ . Then, the two largest ones among the three numbers $d \left(x, y\right) + d \left(z, w\right)$ , $d \left(x, z\right) + d \left(y, w\right)$ and $d \left(x, w\right) + d \left(y, z\right)$ are equal. This is a known fact, sometimes called the ""four-point condition"" (or at least it is similar to the latter; IIRC the latter is about leaves in phylogenetic trees, where $d$ is no longer the minimum length of a path but the minimum cost in some metric, and I'm not fully sure if there are any clear-cut implications between these results). I am wondering if there is a proof of Theorem 1 along the following lines: For any two vertices $s$ and $t$ of $G$ , define some univariate polynomial $Q_{s, t} \in K\left[X\right]$ (for some field $K$ , possibly $\mathbb{F}_2$ ) having the property that $Q_{s, t}$ is monic with leading term $X^{d\left(s, t\right)}$ (or $X^{d\left(s, t\right) + c}$ for some constant $c$ ). Prove the identity $Q_{x, y} Q_{z, w} \pm Q_{x, z} Q_{y, w} \pm Q_{x, w} Q_{y, z} = 0$ for some choice of $\pm$ signs. Conclude that the largest one among the three numbers $d \left(x, y\right) + d \left(z, w\right)$ , $d \left(x, z\right) + d \left(y, w\right)$ and $d \left(x, w\right) + d \left(y, z\right)$ must be equal to another of these three numbers, since otherwise the leading term in the preceding identity could not cancel. Therefore, Theorem 1. This kind of proof seems to be foreshadowed by the well-known tinfoil (not sure whether it has ever been made rigorous) that ""the space of phyologenetic trees is the tropical Grassmannian"" and, in particular, Theorem 1 is a sort of ""tropical version"" of the Ptolemy theorem $XY \cdot ZW \pm XZ \cdot YW \pm XW \cdot YZ = 0$ for four points $X, Y, Z, W$ lying on a circle (or line). Of course, the latter theorem itself is merely a shadow of more general results about Grassmannians (the Plücker relations for $\operatorname{Gr}\left(4,2\right)$ ). I also have my personal interest in such a proof: I posed Theorem 1 on a graph theory midterm (specifically, exercise 6 on midterm #2 in Spring 2017 Math 5707 at the University of Minnesota; you can now find three different solutions on the course website ), and discovered that the proofs I knew were less slick than I had hoped for...","['graph-theory', 'trees', 'grassmannian', 'tropical-geometry', 'combinatorics']"
2226996,Linear independence of derivatives at a point implies trivial intersection of image,"Let $f$ and $g$ be differentiable paths from $\mathbb{R}$ to $\mathbb{R^{n}}$ such that $f(a)=g(b)=p$ for some pair $a,b \in \mathbb{R}$. If $f'(a)$ and $g'(b)$ are linear independent then $\exists k>0$ such that $f(B(a,k))\cap g(B(b,k))=${$p$}. Here $B(x,k)$ stands for the open ball with radidus $k$ and center at $x$. It's easy to see that the images of $(a,a+k)$ and $(b,b+k)$ shouldn't have a point in commom because the paths are ""growing"" to different directions so the inclination may guarantee what I need,still I don't know how to go further.","['derivatives', 'real-analysis']"
2227004,Why did no student correctly find a pair of $2\times 2$ matrices with the same determinant and trace that are not similar?,"I gave the following problem to students: Two $n\times n$ matrices $A$ and $B$ are similar if there exists a nonsingular matrix $P$ such that $A=P^{-1}BP$. Prove that if $A$ and $B$ are two similar $n\times n$ matrices, then they have the same determinant and the same trace. Give an example of two $2\times 2$ matrices $A$ and $B$ with same determinant, same trace but that are not similar. Most of the ~20 students got the first question right. However, almost none of them found a correct example to the second question. Most of them gave examples of matrices that have same determinant and same trace. But computations show that their examples are similar matrices. They didn't bother to check that though, so they just tried random matrices with same trace and same determinant, hoping it would be a correct example. Question : how to explain that none of the random trial gave non similar matrices? Any answer based on density or measure theory is fine. In particular, you can assume any reasonable distribution on the entries of the matrix. If it matters, the course is about matrices with real coefficients, but you can assume integer coefficients, since when choosing numbers at random , most people will choose integers.","['random-matrices', 'matrices', 'similar-matrices', 'probability', 'linear-algebra']"
2227021,Prove $\prod_1^\infty (1+p_n)$ converges,"Let $p_{2n-1} = \frac{-1}{\sqrt{n}}$, and $p_{2n} = \frac{1}{n}+\frac{1}{\sqrt{n}}$. Prove $\prod_1^\infty (1+p_n)$ converges. By numerical simulations, it appears to converge (to something around $0.759$). However, I'm not sure how to prove this. I know we can skip the first term since it's $0$. Then we can write it in the following form. \begin{align*}
\prod_1^\infty (1+p_n) &= \prod_1^\infty \left(1+\frac{1}{2n}+\frac{1}{\sqrt{2n}}\right)\left(1-\frac{1}{\sqrt{2n+1}}\right)  \\
&=  \left(1+\frac{1}{2}+\frac{1}{\sqrt{2}}\right)\left(1-\frac{1}{\sqrt{3}}\right)\left(1+\frac{1}{4}+\frac{1}{\sqrt{4}}\right)\left(1-\frac{1}{\sqrt{5}}\right)...
\end{align*}
Any thoughts?","['real-analysis', 'infinite-product', 'analysis']"
2227027,"If a function is undefined at a point, would its derivative be undefined at that point as well?","If $f(x)$ is defined everywhere except at $x=x_0$, would $f'(x_0)$ be undefined at $x=x_0$ as well? One example is: $$f(x)=\ln(x)\rightarrow f'(x)=\frac{1}{x}$$ In this particular case, both $f(x)$ and $f'(x)$ are undefined at $x=0$. I wonder if this always holds true. Thank you.","['derivatives', 'calculus']"
2227032,A Nasty Elliptic Integral,"I am trying to evaluate: $$\int_{-\infty}^{\infty}\frac{dx}{\left|1+\alpha x^{2}\right|},\textrm{ }\alpha\in\mathbb{C}\backslash(-\infty,0]$$ It's simple to see that it is convergent for such $\alpha$—but that's probably the only simple thing about it! I've been lost in trying to solve this one for several days, now. Performing a change of variables yields: $$\int_{0}^{\infty}\frac{\sqrt{q}dx}{\sqrt{x}\sqrt{x^{2}+px+q}}$$ where $p=\frac{\cos\theta}{r}$ and $q=\frac{1}{r^{2}}$, where $\alpha=re^{i\theta}$. Performing yet more changes of variables and integrating by parts yields (assuming I didn't screw up somewhere along the way): $$\frac{2}{r}+\frac{2}{r}\int_{\frac{p}{2}}^{\infty}\frac{x\sqrt{x-\frac{p}{2}}}{\left(x^{2}-\Delta^{2}\right)^{3/2}}dx$$ where $\Delta=\frac{i}{2r}\sqrt{4-\cos^{2}\theta}$. This version, Mathematica is able to compute, giving the formula: $$\int_{a}^{\infty}\frac{x\sqrt{x-a}}{\left(x^{2}-b^{2}\right)^{3/2}}dx=\frac{\textrm{sgn}\left(\textrm{arg}\left(-b^{-2}\right)\right)}{\left(a^{2}-b^{2}\right)^{1/4}}K\left(\frac{1}{2}-\frac{a}{2\sqrt{a^{2}-b^{2}}}\right)$$ where $K$ is the complete elliptic integral of the first kind. However, this formula is only valid for $a,b\in\mathbb{C}$ satisfying $\textrm{Im}\left(a\right)=0$, $\textrm{Re}\left(a\right)>0$, $\textrm{Re}\left(b^{2}\right)<0$, and satisfying either: “$\textrm{Re}\left(a\right)>\textrm{Re}\left(b\right)$ and $a>\textrm{Re}\left(b\right)$” OR “$b\notin\mathbb{R}$”. All of these conditions are satisfied for my integral, except for the $\textrm{Re}\left(a\right)>0$ condition, which makes no sense. $a=\frac{p}{2}=\frac{\textrm{Re}\left(\alpha\right)}{\left|\alpha\right|^{2}}$, and the initial integral is valid even for $\alpha$ with $a\leq0$, as long as $\textrm{Im}\left(\alpha\right)\neq0$. So: any ideas for how to evaluate: $$\int_{-\infty}^{\infty}\frac{dx}{\left|1+\alpha x^{2}\right|}?$$ Thanks!","['integration', 'elliptic-integrals']"
2227038,What is the average maximum number of pairs such that $x_i > y_i$ for uniformly distributed random variables $X$ and $Y$?,"Given $X \sim U(a, b)$ and $Y \sim U(c, d)$ I would like to find average maximum number of pairs such that for $pair_i = (x_i, y_i): x_i > y_i$ when I have $n$ samples for $X$ and $n$ samples for $Y$. Example: $X$ is uniformly distributed between $1$ and $10$ and $Y$ is uniformly distributed between $3$ and $11$. I draw $5$ samples for each random variable: $$
X = [5, 7, 9, 3, 7] \qquad Y = [6, 9, 4, 8, 5]
$$ I could arrange it into pairs: $$
pairs_a:(5, 6) \quad (7, 9) \quad (9,4) \quad (3,8) \quad (7,5)
$$ or another example: $$
pairs_b:(9, 8) \quad (7, 6) \quad (7,5) \quad (5,4) \quad (3,9)
$$ Here $pairs_a$ gives me 2 pairs where $x_i > y_i$ but $pairs_b$ gives me 4 such pairs. In this example, the maximum number of pairs fulfilling my condition is 4 (nothing will be greater than $9$ coming from Y).","['probability', 'probability-distributions']"
2227074,Prove by definition that every upper semi-continuous function can be expressed as infimum of a sequence of continuous functions.,"Suppose $X$ is a metrizable space. A real-valued function $f:X \rightarrow \mathbb{R}$ is upper semicontinous if for any real number $c$, its preimage $f^{-1}(-\infty,c)$ is open in $X$. In this post , we see that every lower semicontinuous can be expressed as thesupremum of increasing continuous functions, where the sequence of continuous functions is defined as $f_k(x) = \inf \{ f(y) + k d(x,y): y \in X \}$. I am curious as to how would one show that an upper semicontinuous function $f$ can be expressed as the infimum of non-increasing continuous functions that converge pointwise, by using the definition of upper semicontinuity above, which is $f^{-1}(-\infty,c).$","['general-topology', 'metric-spaces', 'supremum-and-infimum', 'semicontinuous-functions']"
2227081,"If $f(x)\cdot f(f(x)) = 1$ for all $x \in \mathbb{R},$ and if $f(10) = 9$ then find the value of $f(5)$","If $f\colon\mathbb{R}\rightarrow \mathbb{R}$ is a continuous function satisfying $f(x)\cdot f(f(x)) = 1$ for all $x \in \mathbb{R},$ and if $f(10) = 9$ then find the value of $f(5).$ Attempt: Put $x=10,$ we have $f(10)\cdot f(f(10)) = 1.$ So, $9\cdot f(9) = 1,$ then $f(9) = \frac{1}{9}.$ Could someone help me how calculate $f(5)$ ? Thanks.",['functions']
2227088,PDF of the sum of independent normal and uniform random variables,I have a continuous uniform random variable $P$ and a standard normal random variable $X$. The pdf of $P$ is: $\frac{1}{b-a}$ and the pdf for $X$ is: $\frac{1}{\sqrt{2\pi}}e^{-x^2/2}$ How to find the PDF of $Y=P+X$?,"['probability-theory', 'random-variables', 'probability-distributions']"
2227097,Find the limit $\lim\limits_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\frac{1}{3}\right)^n \left(\frac{2}{3}\right)^{k-n}$,"The problem is to find the following limit: $$\lim_{n \to +\infty}\sum\limits_{k=n}^{3n} \binom{k-1}{n-1} \left(\dfrac{1}{3}\right)^n \left(\dfrac{2}{3}\right)^{k-n}$$ I see that it looks similarly to the formula from the binomial theorem, but don't get how we can make use of it. Any ideas would be greatly appreciated.","['binomial-coefficients', 'limits', 'calculus', 'probability', 'contest-math']"
2227114,Infinite Sigma algebra contains infinite sequence of nonempty disjoint sets,"I realize this question has been answered but I wonder if this argument would work. Proof: If the infinite Sigma algebra X contains an infinite sequence of strictly nested nonempty sets, then we are done. Otherwise, take the set of all maximal strict nests in X, then the smallest element of these nests are disjoint from each other. If there are infinitely many then we are done. Otherwise, delete those small elements from each set in X and repeat the process above. Since the process can be repeated infinitely many times and the resulting smallest set in each nest form a disjoint set, the proof is finished.","['real-analysis', 'measure-theory', 'elementary-set-theory', 'proof-verification']"
2227163,Sample space for identical objects,"Sample space for a set of balls in an urn out of which say $p$ are of blue color (hence identical), $q$ are of red color is written as a multiset and not as a set, or rather when it is written as a set the identical objects are written as different objects in the following way:- $\text{Set representation} - \{b_1,b_2,\ldots,b_p,r_1, r_2,\ldots,r_q\}\\
\text{Multiset representation}-\{b,b,\ldots\text{p times}, r,r,\ldots\text{q times}\}$ And based on the above sample space if we are asked to find the probability of selecting a blue ball we get it as $\dfrac{p}{p+q}$, which was found using the classical definition of probability . According to the classical definition of probability, probability of an event $E$ as given as 
$$P(E)=\dfrac{n(E)}{n(S)}$$ Now why is that the ways of selecting $r$(here, $r=1$) things out of $n$ identical things is ${n}\choose{r}$ and not $1$ as is the case in combinatorics . Edit 1:- To further clarify my point consider these two contrasting cases below:- Case 1:- Consider an urn in which there are $30$ distinct balls out of which there are $10$ distinct blue balls and $20$ distinct white balls. Let's say the distinction b/w the balls of identical color is made by marking them with numbers which cant be felt by the one drawing the ball. Now in a random draw if we are asked the probability of picking a blue ball, then according to the classical definition of probability it will be $\dfrac{10}{30}=\dfrac{1}{3}$. We had arrived the above probability because we know that 
$$n(E:\text{drawing a blue ball})={{10}\choose{1}}=10\\
n(S:\text{drawing a ball from the earn})={{30}\choose{1}}=30$$ Just listing the sets $E$(event space) and $S$(sample space) for comparing these with the next case. $E=\{B_1,B_2,B_3,\ldots,B_{10}\}\\
S=\{B_1,B_2,B_3,\ldots,B_{10},W_1,W_2,W_3,\ldots, W_{20}\}$ Case 2:- The  setting for this case is the same as the previous case the only difference is that that the balls of same color are indistinguishable. So, if we are asked the probability of drawing a blue ball the probability would come out as $\dfrac{1}{2}$. My thinking behind this probability is that the sample space for this random draw would be $$S=\{W,B\}$$ as all the white balls are identical to each other and so are all the blue balls. Similarly, the event space would be $$E=\{B\}$$ as all the blue balls are identical. But by this logic the probability of drawing a blue ball would still be $\dfrac{1}{2}$, when the blue balls are present in a vast quantity as compared to the white balls which would be other than the expected result. So, my question again boils down to the same one as in the pre edit part of the post, i.e. Why do we use the multi-set representation for identical objects or while writing it in set notation why do we consider the identical objects as distinct objects?","['probability', 'discrete-mathematics']"
2227170,Calculate $f^\prime(0)$ of $f(x) = \prod_{n=0}^{100} (x-n)$.,"How can one calculate $f^\prime(0)$ of $f(x) = \prod_{n=0}^{100} (x-n)$ by hand? I tried to compute the derivative of $x$ and got $1$, and the derivative of $x(x-1)$ at $0$ is $-1$, and the derivative of $x(x-1)(x-2)$ at $0$ is $2$, and the derivative of $x(x-1)(x-2)(x-3)$ is $-6$. I don't see a pattern, and I'm not sure of an alternative route.","['derivatives', 'calculus']"
2227177,Proving subset sums with the pigeon hole principle,"I'm having trouble with three problems dealing with the pigeon hole principle. They are: Prove that any subset of size ten of the first 40 positive integers must have two different subsets
of size three with the same sum. What is the largest value of m for which it is true that any subset of size ten of the first m positive integers must have two different subsets of size three with the same sum? Show that any subset of size ten of the first 24 positive integers contains two pairs of values with the same sum. My attempts: For problem (1) I essentially tried to brute force it with a C++ program. I got some interesting results (if they turn out to be right). My program returned that there existed 9,880 possible sums for the first 40 positive integers (assuming size three) (e.g.) a+b+c and that there was only 112 unique sums. Therefore, by the pigeon hole principle there must be a sum which repeats. Problem (2) I honestly did not know how to deal with this problem. However, when I first read the question I did not think that m had an upper bound (i.e. it works for every m). But, now I'm not so sure and do not know where to start. Problem (3) For this problem I also tried a brute force method and got a total of 176513040 possible sums with 47 being unique. However, I don't think this is right. Nor do I think that my answer to problem 1 is right. Can anyone offer some advice? Thank you!","['pigeonhole-principle', 'discrete-mathematics']"
2227191,Uniform convergence and lengths,"Consider the following sequence of piece-wise smooth functions $f_n:[0,2]\rightarrow \mathbb{R}$ (below drawn first two functions by their graphs, and is contunued then in natural way.) The function $f:[0,2]\rightarrow \mathbb{R}$, $f(x)=0$ for all $x$ is the limit of this sequence. Q. 1 Note that the lengths of graph of each $f_n$ is $2\sqrt{2}$, but the length of limiting function is not $2\sqrt{2}$; why this happens? (Such problem appears as a puzzle shown below picture in some book, but I was looking it through a sequence of functions, it convergence etc.) Here sequence $f_n$ converges to $f$ uniformly; but still the length of their graphs do not converges to length of graph of limit of $f_n$. This raises following question: Q If $f_n:[a,b]\rightarrow \mathbb{R}$ is a sequence of piecewise smmoth functions, and converging uniformly to $f:[a,b]\rightarrow \mathbb{R}$, then under what more conditions on $f_n$, we can guarantee the convergence of lengths of $f_n$ to length of $f$?","['uniform-convergence', 'real-analysis', 'convergence-divergence', 'calculus']"
2227192,How do we prove this conjecture $\int_{0}^{\infty}e^{-\sqrt{x}}\ln{\left(1+{1\over \sqrt{x}}\right)}=2\gamma ?$,"I was observing this question by @Brightsun and conjecture $(1)$ $$\int_{0}^{\infty}e^{-\sqrt{x}}\ln{\left(1+{1\over \sqrt{x}}\right)}=2\gamma \tag1$$ An attempt $x=u^2$ then $(1)$ becomes $$2\int_{0}^{\infty}ue^{-u}\ln{\left(1+{1\over u}\right)}\tag2$$ $$2\sum_{n=0}^{\infty}{(-1)^n\over n!}\int_{0}^{\infty}u^{n+1}\ln{\left(1+{1\over u}\right)}\mathrm du\tag3$$ Changing $(2)$ by applying $e^x$, $(3)$ diverges. How would one prove $(1)?$","['gaussian-integral', 'calculus', 'euler-mascheroni-constant', 'integration', 'definite-integrals']"
2227196,"Real methods for the evaluating $\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt$","One can show by integrating the following function $$f(z) = z^{n-m-1}(1+z^2)^m$$ around the following contour By equating the circular part of $|z|=1$ and the line on the imaginary part. $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=2^{-m}\sin\left(\frac{n\pi -m\pi}{2} \right)\int^1_{0}t^{n-m-1}(1-t^2)^m\,dt \tag{1}$$ The left side looks close to the Beta representation $$2\int^{\pi/2}_0 \sin^{2n-1}(t) \cos^{2m-1}(t)\,dt = \frac{\Gamma(n)\Gamma(m)}{\Gamma(n+m)} \tag{2}$$ Questions Can we show (1) using elementary transformations ? What real methods can we use to prove the integral $$\int^{\pi/2}_{0}\cos(nt)\cos^m(t)\,dt=\frac{\pi \Gamma(m+1)}{2^{m+1}\Gamma\left(\frac{n+m+2}{2}\right)\Gamma\left(\frac{2-n+m}{2}\right)}\tag{3}$$","['special-functions', 'integration', 'definite-integrals', 'beta-function']"
2227199,Does the series $\sum_{n=2}^{\infty } \frac{1}{n(\ln(n))^{2}}$ converge or diverge?,"As said above, does this series converge or diverge? Is there a certain identity/theorem to prove this? $$\sum_{n=2}^{\infty }\frac{1}{n(\ln(n))^{2}}$$","['summation', 'sequences-and-series', 'calculus']"
2227244,Probability that $x^2-y^2$ is divisible by $k$,"Let two numbers $x$ and $y$ be selected from the set of first $n$ natural numbers with replacement(i.e. the two numbers can be same).The question is to find out the probability that $x^2-y^2$ is divisible by $k\in \mathbb{N}$ For $k=2$ Any number can be expressed as $2p,2p+1$.Now $x^2-y^2=(x-y)(x+y)$ If both numbers are of form $2p+1$ then (x-y) would be divisible by $2$ .if two numbers are of different forms then it will not be divisible by $2$.So the probability in this case is $a^2+(1-a)^2$ where $a$ is probability that number chosen is divisible by $2$ which is $\frac{\lfloor \frac{n}{2} \rfloor}{n}$.However this gets complicated with $k=3$ onwards because numbers in different forms may be divisible.In other words if there a generalisation or way to solve for some large $k$.Thanks.",['probability']
2227280,Are there an equal number of positive and negative numbers?,"For every positive number there exists a corresponding negative number. Would that imply that the number of positive numbers is ""equal"" to the number of negative numbers? (Are they incomparable because they both approach infinity?)",['elementary-set-theory']
2227310,Is this a convergent series? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Put $a_{0}= \pi/2$  and  $a_{n}=\sin(a_{n-1})$. Is  $\sum a_{n}$ a convergent  series?","['real-analysis', 'divergent-series', 'sequences-and-series']"
2227324,Prove that $f(x) \leq f(0)e^{-x}$,"function $f : [0,\infty) \to [0,\infty)$ satisfies next two conditions. a) $$\lim_{x \to \infty} f(x)=0$$
b) $f''(x)$ exists, is continuous and $$f''(x) \geq f(x)$$ for all $x$ in domain. Then, prove that $$f(x) \leq f(0)e^{-x}$$ for $\forall x \in [0,\infty)$. I don't know how to prove it...","['calculus', 'analysis']"
2227359,Definition of Essentially Self Adjoint Operators,"I have two definitions of an essentially self adjoint operator A symmetric operator with a self adjoint closure An operator with a unique self adjoint extension. I can easily show that (1) implies (2) but the converse escapes me. I can get as far (correctly I hope) that if $T$ has a unique self adjoint extension $S$ then $T$ is densely defined, closable, and symmetric and $ T \subset cl(T) \subset S = S^* \subset [cl(T)]^* = T^*$ I have a reference Reed & Simon, V.1 p.256 which further refers to section (V.2) X.1, but I didn't find the answer there.","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'adjoint-operators']"
2227375,"Composition of two continuous, surjective functions, which is a covering map","I'm stuck with the following problem (number 12.21.1 of Topologia-Marco Manetti, 2008 edition): Let $p:E \to F$ and $q:F \to X$ two continuous, surjective functions, where $E,F,X$ are connected and locally path connected topological spaces. Prove that, if $qp:E \to X$ is a covering map, then also $p$ and $q$ are covering maps. What I was thinking about is to take $x \in X,$ and an open set $V \subset X, x\in V$, which respects the covering condition, and the counterimage through $qp$ of $V.$
Then, since $(qp)^{-1}(V)=\bigcup_{i \in I}C_i,$ where $C_i$ are all connected components such that $qp|_{C_i}:C_i \to V$ are homeomorphisms, I obtained that $p|_{C_i}$ is injective. I made a quite similar reasoning taking the counterimage of $V$ through $q,$ but I couldn't reach very satisfying results. Someone also told me that it doesn't really seem to be true. Thanks to everybody.","['algebraic-topology', 'general-topology', 'covering-spaces']"
2227377,We throw 6-side dice 5 times. Find probability that we will have exactly 4 different values.,"I am not sure if I did it correctly. Of course
$|\Omega|=6^5$ Now $A=\{$event that we obtained exactly 4 different results on 5 throws$\}$ $$|A|= {{6}\choose{1}}{5\choose2}{5\choose1}{4\choose1}{3\choose1}$$ Which means that firstly we choose value which will occur twice. Next we choose on which throws it will occur. Afterwards we just choose 3 different values for remaining spots.","['probability-theory', 'probability']"
2227385,Find the range of the following differentiable function,Let $f: R\to R$ be a differentiable function defined by $$f\left(\frac{x+y}{3} \right)=\frac{2+f(x)+f(y)}{3}$$  for all real $x$ and $y$ and $f'(2)=2$. Then (i) what is the range of $f(|x|)$ ? (ii)what is the number of solutions of the equation $x^2+(f(|x|))^2=9$? Progress: (i) $f\left(\frac{x+y}{3} \right)=\frac{2+f(x)+f(y)}{3} \implies f'\left(\frac{x+y}{3} \right). \left(1+y'\right)=f'(x)+f'(y)$. I stuck here.How can I find the range? and how can I find the number of solutions? Edit: $f\left(\frac{x+y}{3} \right)=\frac{2+f(x)+f(y)}{3} \implies f'\left(\frac{x+y}{3} \right). \left(1+y'\right)=f'(x)+f'(y) y'$.,"['derivatives', 'functions']"
2227387,Types of convergence in probability confusion?,"Okay so I recently learned about three types of convergence in probability. Convergence in distribution, convergence in probability and almost sure convergence. I have the definitions memorised but I am struggling to understand what they mean in essence, why we need all these different types of convergence and a few technical details. Here are my concerns. 1) In two of these convergences we require that a certain limit is satisfied. Take for example convergence in probability it says $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0:\lim_{n \rightarrow \infty} P(|X_n-X|>\epsilon)=0$. Now in my mind intuitively this is saying that as we go further and further along the sequence of random variables ($n \rightarrow \infty$) the probability that $X_n$ differs from $X$ by any positive amount becomes arbitrarily small. But I don't understand what it means for $|X_n-X|$ to become small since these are random variables which are themselves functions. How exactly do we measure distance between these two functions? I have seen some metrics of spaces of functions but I don't think that is what is being used here. Would it just mean that $|X_n(w)-X(w)|$ gets small for any $w$ in the sample space? In which case why don't we specify the definition as $X_1,X_2,...\implies X$ in probability if for all $\epsilon>0$ and for all $w$ in the sample space :$\lim_{n \rightarrow \infty} P(|X_n(w)-X(w)|>\epsilon)=0$? 2) Almost sure convergence says $X_1,... \rightarrow X$ if we have $P(\lim_{n \rightarrow \infty} X_n=X)=1$ again what does this mean? Is it saying that $X_n(w)=X(w)$ in the limit as $n$ tends to infinity for all $w$ in the sample space. 3) Why do we need both almost sure and convergence in probability? I know that almost sure is stronger and thus I suppose I have answered my own question but I don't really see why. To me almost sure says that $X_n$ becomes the same as $X$ as we let $n$ become very large and convergence in probability to me seems to say $X_n$ can't be different from $X$ as $n$ becomes large this seems like two ways to say the same thing. I think that is all I have at the moment hopefully I have expressed my concerns in a way that makes it somewhat easy to see what I don't get. If anyone could help clarify that would be great. Thanks! By the way I have met these definitions in my lectures but nobody in our year has covered measure theory yet so I wonder if I am not truly meant to understand these definitions yet.","['probability-theory', 'probability']"
2227456,Sum of the serie $\sum_{n=1}^{\infty}\frac{1}{4n^3-n}$,"I am trying to calculate the sum of this infinite series
$$\sum_{n=1}^{\infty}\frac{1}{4n^3-n}.$$
I only know that
$$\frac{1}{4n^3-n}=-\frac{1}{n}+ \frac{1}{2n+1} +\frac{1}{2n-1}.$$
Can you help me, please? thanks.","['real-analysis', 'sequences-and-series']"
2227533,Does $G\times K\cong H\times K$ imply $G\cong H$?,Let $G\times K$ be a finite group. Suppoose $G\times K\cong H\times K$. Is this sufficient to imply $G\cong H$?,"['finite-groups', 'group-theory']"
2227542,Finding the eigenvalues and a basis for the eigenspaces of a $3\times3$ matrix.,"For the  matrix $A \in M_{3\times3}(\mathbb{R})$ below, I need to find the eigenvalues and a basis for the corresponding eigenspaces: $$\begin{bmatrix}\
1 & -3 & 3 \\           
3 & -5 & 3 \\
6 & -6 & 4 \\
\end{bmatrix}$$
I have tried to use the formula $\det(I\lambda - A) = 0$ but I ended up with equation $\lambda^3 - 12\lambda - 16 = 0$, of which I can't seem to find the solutions to.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2227574,Convergence of $\sum_{n=1}^{\infty} \frac{\sin(n)}{n}$,"I am trying to argue that
$$
\sum_{n=1}^{\infty} \frac{\sin(n)}{n}
$$
is divergent. It get that it must be divergent because $\sin(n)$ is bounded and there is an $n$ on the bottom. But I have to use one of the tests in Stewart's Calculus book and I can't figure it out. I can't use the Comparison Tests or the Integral Test because they require positive terms. I can't take absolute values, that would only show that it is not absolutely convergent (and so it might still be convergent). The Divergence Test also doesn't work. I see from this question: Evaluate $ \sum_{n=1}^{\infty} \frac{\sin \ n}{ n } $ using the fourier series that the series is actually convergent, but using some math that I don't know anything about. My questions are (1) Is this series really convergent? (2) Can this series be handled using the tests in Stewart's Calculus book?","['convergence-divergence', 'sequences-and-series', 'calculus']"
2227578,derivative of $\left(\frac{\ 1}{3x}\right)$ using limit definition.,I'm trying find derivative of $\left(\frac{\ 1}{3x}\right)$ using limit definition. I do this: $(\left(\frac{\ 1}{3x+h}\right) - \left(\frac{\ 1}{3x}\right))/h$ $(\left(\frac{\ 1}{3x+h}\right)(3x) - \left(\frac{\ 1}{3x}\right)(3x+h))/h$ $(\left(\frac{\ 3x}{3x(3x+h)}\right) - \left(\frac{\ 3x+h}{3x(3x+h)}\right))/h$ $\left(\frac{\ 3x - 3x - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)/h$ $\left(\frac{\  - h}{3x(3x+h)}\right)* \left(\frac{\   1}{h}\right)$ $\left(\frac{\  - 1}{3x(3x+h)}\right)$ Answer: $\left(\frac{\  - 1}{9x^2}\right)$ But in https://www.symbolab.com/solver/derivative-calculator the answer is $\left(\frac{\  - 1}{3x^2}\right)$ I think my calculations are correct. How can symbolad answer be different? Thanks!,['derivatives']
2227587,Is self-adjoint the same as hyper-maximal symmetric?,"I'm reading von Neumanns 1931 book on Quantum Mechanics. Here he presents spectral theory for unbounded operators and quite a bit of the notation and naming has changed in the mean time. Specifically for him a symmetric operator (for all $x,y\in D(A)$ $\langle Ax,y\rangle = \langle x,Ay\rangle$) is called hermitian and an symmetric operator is called maximal if it admits no proper symmetric extensions. He characterises maximality via the Cayley Transform, an operator $A$ is maximal iff the Cayley Transform $U(A)$ has image or domain the entire Hilbert space $H$. An operator is defined to be hyper-maximal if the Cayley Transform is unitary, ie has both image and domain all of $H$. He shows that the hyper-maximal symmetric operators are precisely those that admit a spectral decomposition. For example it is clear that self-adjoint implies maximal: If $A=A^*$ and $B$ is symmetric so that $B\lvert_{D(A)}=A$, if $y\in D(B)$ then for every $x\in D(A)$ you've got:
$$|\langle Ax,y\rangle| =| \langle x,By\rangle| ≤\|x\|\,\|By\|$$
it follows $y\in D(A^*)=D(A)$ and $A^*y = By$. My questions: What is the modern terminology for maximal? And Is hyper-maximal the same as self-adjoint?","['functional-analysis', 'spectral-theory']"
2227678,Showing that $f(x)=\frac x{x+1}$ is the unique function satisfying $f(x)+f\left(\frac1x\right)=1$ and $f(2x)=2f\big(f(x)\big)$,"We are given a function $ f : \mathbb Q ^ + \to \mathbb Q ^ + $ such that $$ f ( x ) + f \left( \frac 1 x \right) = 1 $$ and $$ f ( 2 x ) = 2 f \big( f ( x ) \big) \text . $$ Find, with proof, an explicit expression for $f(x)$ for all positive rational numbers $x$ . Every number I have evaluated is of the form $ f ( x ) = \frac x { x + 1 } $ and this clearly fits the functional equations, but I can't prove that it's the only solution. Can anyone help me? I have put down the start of my workings which led me to the conjecture of $ f ( x ) = \frac x { x + 1 } $ . Plugging in $ x = 1 $ clearly gives $ f ( 1 ) = \frac 1 2 $ and $ f ( 2 ) = 2 f \big( f ( 1 ) \big) = 2 f \left( \frac 1 2 \right) $ which we can plug back into the first equation to get that $ f ( 2 ) = \frac 2 3 $ . Working in this vein I have been able to show that $ f ( x ) = \frac x { x + 1 } $ for particular values of $ x $ , but not in general. The most difficult part appears to be proving it for the even integers. To prove $ x = 8 $ , we have $$ f ( 12 ) = 2 f \left( \frac 6 7 \right) = 4 f \left( \frac 3 { 10 } \right) = 4 - 4 f \left( \frac { 10 } 3 \right) \\
= 4 - 8 f \left( \frac 5 8 \right) = 8 f \left( \frac 8 5 \right) - 4 = 16 f \left( \frac 4 9 \right) - 4 \\
= 32 f \left( \frac 2 { 11 } \right) - 4 = 64 f \left( \frac 1 { 12 } \right) - 4 = 60 - 64 f ( 12 ) \text , $$ giving us $ f ( 12 ) = \frac { 12 } { 13 } $ . This will probably be the main area of difficulty in the proof.","['algebra-precalculus', 'contest-math', 'functional-equations']"
2227687,Binomial coefficient and fibonacci numbers,"I found the following problem in putnam and beyond:
prove that $$F_{2n} = \sum_{k=1}^{n} F_{k}{{n}\choose {k}}$$ The answer in the back of the book uses the closed form of $F_n$, but it seems to me their should be a solution using only the properties of the binomial cooeficient. I tried using $${{n} \choose {k}} = {{n-1} \choose {k}}+ {{n-1}\choose {k-1}}$$ but this seems to be a dead end, and I have no other ideas. Any help is appreciated!",['combinatorics']
2227719,Complexity/Operation count for the forward and backward substitution in the LU decomposition?,"If I have a linear system of equations $Ax=b$ where $A \in \mathbb{R} ^{n\times n}, x \in \mathbb{R} ^{n}, b \in \mathbb{R} ^{n} $ this system can be solved for $x$ via an LU decomposition: $$A = LU$$ where $U \in \mathbb{R} ^{n\times n}$ is upper triangular and $L \in \mathbb{R} ^{n\times n}$ is lower triangular. I understand a forward substitution is then required where one first solves: $$Ly=b$$ for $y$. And then we solve:
$$Ux=y$$ for $x$. I am currently trying to determine the operation count or the FLOPS for each of the forward substitution and backward substitution. I have seen that the correct value is approximately given by $\mathcal{O}(n^{2})$ flops but I am unsure how one can arrive at this value. I can see that for the backward substitution, for example, the system is represented as: $$\begin{bmatrix}
u_{11} & u_{12}  & \cdots   & u_{1n} \\ 
0 & u_{22} &\cdots  &u_{2n} \\ 
 \cdots&  \cdots & \ddots  &\vdots  \\ 
0 &  0 & \cdots & u_{nn}
\end{bmatrix} \begin{bmatrix}
x_{1}\\ 
x_{2}\\ 
\vdots \\ 
x_{n}
\end{bmatrix} = \begin{bmatrix}
y_{1}\\ 
y_{2}\\ 
\vdots \\ 
y_{n}
\end{bmatrix}$$ From which: $$x_{i} = \frac{1}{u_{ii}} \left ( y_{i} - \sum_{j=i+1}^{n}u_{ij}x_{j} \right ); i = n, ..., 1$$ From an equation like this, how can one identify the approximate operation count?","['lu-decomposition', 'computational-complexity', 'linear-algebra']"
2227742,"$\int_0^{\pi/2}sin^p\,\theta\;cos^q\,\theta\;d\theta = \frac{\Gamma{\frac{p+1}{2}}\Gamma{\frac{q+1}{2}}}{2\Gamma{\frac{p+q+2}{2}}},\; p,q > -1$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Show that: $$\int_0^{\pi/2}sin^p\,\theta\;cos^q\,\theta\;d\theta = \frac{\Gamma{\frac{p+1}{2}}\Gamma{\frac{q+1}{2}}}{2\Gamma{\frac{p+q+2}{2}}},\; p,q > -1$$ Here's the question.",['calculus']
2227789,Generators of $GL_2(\mathbb{Q}_p)$,"A well known fact is that the  group $GL_2(\mathbb{Q}_p)$ is generated by the following matrices:
$1) \text{ } 
   w=
  \left( {\begin{array}{cc}
   0 & 1 \\
   1 & 0 \\
  \end{array} } \right)
$ $2) \text{ } \mathbb{Q}_p^{\star}
 \left( {\begin{array}{cc}
   1 & 0 \\
   0 & 1 \\
  \end{array} } \right)
$ $3) \text{ } 
 \left( {\begin{array}{cc}
   \mathbb{Z}_p^{\star} & 0 \\
   0 & 1 \\
  \end{array} } \right)
$ $4) \text{ } 
 \left( {\begin{array}{cc}
   p & 0 \\
   0 & 1 \\
  \end{array} } \right)
$ $5) \text{ } 
 \left( {\begin{array}{cc}
   1 & p\mathbb{Z}_p \\
   0 & 1 \\
  \end{array} } \right)
$ I need a reference for the above fact. In particular, for $b \in \mathbb{Q}_p$, I am trying to write lower unipotent elements $
 \left( {\begin{array}{cc}
   1 & 0 \\
   b & 1 \\
  \end{array} } \right)
$ in terms of the above matrices. Any help is welcome. Thanks.","['p-adic-number-theory', 'abstract-algebra', 'matrix-decomposition', 'algebraic-number-theory', 'number-theory']"
2227807,Count how many number are there from $1$ to $N$ who have all $10$ digits in it at least once.,Count how many number are there from $1$ to $N$ who have all $10$ digits in it at least once. Can we have a generalized method to solve this problem?,['discrete-mathematics']
2227812,"If F is the Fermat point of triangle ABC, then what is the algebraic formula that gives minimum sum distance, i.e., FA+FB+FC=?","Fermat point (F) of a triangle is at the least distance from triangle vertices. If one of the vertex angle is greater than 120 degrees, then F is at that vertex, and minimum distance is equal to sum of the two short sides of triangle. Otherwise, F lies within triangle ABC and the sides AB/BC/CA subtend equal angle of 120 degrees at F. In this case, what is the algebraic formula for finding the minimum distance (in terms of triangle lengths a,b,c)?","['maxima-minima', 'euclidean-geometry', 'geometry']"
2227819,Approximating specific integrals via sums: Tight error predictions,"Dear math enthusiasts, I am facing a particular problem where I am looking at integrals of the form $$I = \int_{-\infty}^\infty p(t) {\rm e}^{-t^2} {\rm d}t,$$
where $p(t)$ are certain polynomials. These are either even or odd symmetric, the even symmetric ones being the more interesting case since for odd ones, the integral is zero (in fact, they are auto- and cross-products of Hermite polynomials but I think this detail is not relevant). For now, we can just consider the simplest example $p(t)=t^2$. The reason why I am looking at the integrals is that I actually have sums of the form $$S(t) = \sum_{n=-\infty}^\infty t_0 p(t-nt_0) {\rm e}^{-(t-nt_0)^2}$$ which I want to quantify. For $t_0$ small enough, these sums are very close to $I$ for any $t$. I need to quantify how close, i.e., I am interested in $$\max_t |S(t)-I|.$$ So, what I did was to interpret $S$ as a quadrature of the integral $I$ and use the residual formulas for quadratures. Since it is a linear quadrature, the standard results predict a residual of the order $|I-S|< {\rm const} \cdot t_0^2 \cdot \max|f''(t)|$, where $f(t) = p(t){\rm e}^{-t^2}$ (which gives $\max|f''(t)| = 2$ for $p(t)=t^2$).  In other words, the error should decay quadratically with $t_0$. All this is not surprising and well within what I expected. Until I tried it and realized that  empirically, the error decays much much faster with $t_0$ than this pessimistic bound predicts. Here is an example: In this example, I computed $S$ for $p(t)=t^2$, varying the grid spacing $t_0$. I plot $\max_t|I-S(t)|/I$ where the exact value $I$ is equal to $\sqrt{\pi}/2$. Obviously, $t_0=1$ is too coarse but as I make it finer, the error goes to zero very rapidly (Note that the plot is doubly logarithmic!). In fact, around $t=0.35$ it reaches the numerical accuracy of my double floating point but I would expect the exponentially decaying trend to continue. The predicted upper bound is shown in the dashed line (it is a line with slope 2 due to the double logarithmic plot). So here is my question: Can I make tighter predictions of $|S(t)-I|$ as $t_0\rightarrow 0$? I know it is easy to construct examples where the residual formula is basically tight, so it must have to do with the particular function I am integrating, especially the ${\rm e}^{-t^2}$ term. I keep getting reminded of Gaussian tails (erfc functions of some sort) but I cannot put my hands on how I could get there. Just before posting I stumbled upon Euler-Maclaurin, but it confuses me as well since what I read about it talks about finite sums (and derivatives being evaluated at the borders) while mine seems infinite (and everything becomes zero far enough from $t=0$). It looks like a standard result and I would not be surprised about a very simple answer that I was just not seeing. Any hint is appreciated, many thanks in advance! edit : Thanks to user14717 I got what I needed. For someone stumbling across a similar problem, here is what worked: Theorem 5.1 in [*] says the following: Let $a>0$ such that the function $w(t)$ to be integrated is analytic in the string $|{\rm Im}(t)|<a$ and decays to zero uniformly as $|t|\rightarrow \infty$. Then: $$|I-S| \leq 2\sqrt{\pi} \frac{M}{{\rm e}^{2\pi a/t_0}-1},$$ where $M$ is a constant satisfying $\int |w(t+ib)| {\rm d}t \leq M$ for all $b \in (-a,a)$. If we apply this for $w(t) = {\rm e}^{-t^2}$ (i.e., my $p(t)=1$), we obtain $M={\rm e}^{a^2}$ as the best $M$ for a given $a$. This gives the family of bounds $$|I-S| \leq 2\sqrt{\pi} \frac{{\rm e}^{a^2}}{{\rm e}^{2\pi a/t_0}-1},$$ which is valid for any $a>0$. To obtain the tightest bound, we need to minimize over $a$. This is not possible analytically, however, for $t_0<1$, the value $a=\frac{\pi}{t_0}$ is very close to the optimum. Inserting it gives the bound $$|I-S| \leq  \frac{2\sqrt{\pi}}{{\rm e}^{(\pi/t_0)^2}-{\rm e}^{-(\pi/t_0)^2}} \approx 2\sqrt{\pi} {\rm e}^{-(\pi/t_0)^2}.$$ And now, let us plot it: Adapting this to any $p(t)$ should be a breeze now. I could barely be happier! :) Thanks so much! [*] http://epubs.siam.org/doi/pdf/10.1137/130932132 edit2 : For an even symmetric polynomial $p(t)$ it is then very easy to show that $$|S-I|\leq \frac{2{\rm e}^{a^2} h(a)}{{\rm e}^{2\pi a/t_0}-1},$$ where $h(a)$ is a polynomial of same degree as $p(t)$. For $a=\pi/t_0$, this gives $$|S-I|\leq \frac{2{\rm e}^{\pi^2/t_0^2}}{{\rm e}^{2\pi^2/t_0^2}-1}h(\pi/t_0) \approx 2{\rm e}^{-\pi^2/t_0^2}h(\pi/t_0),$$ i.e., still exponential convergence, as expected. In fact, for a degree $2k$ polynomial $p(t) = \sum_n \alpha_n t^{2(k-n)}$, an explicit form of $h(t)$ is given by $$h(t) = \sqrt{\pi} \sum_n \sum_\ell |\alpha_n|\frac{k! (2\ell)!}{4^\ell (\ell!)^2 (k-\ell)!} b^{2(k-n-\ell)},$$
though it doesn't matter much since the relevant part is the exponential convergence.","['real-analysis', 'integration', 'sequences-and-series']"
2227826,Vanishing of curvature tensor implies path independence of parallel transport only to second order?,"It can be shown that the curvature tensor measures the path independence of parallel transport only to second order. This is somewhat understandable as we want a local (pointwise) measure of curvature, so it should be natural that $R$ essentially measures ""infinitesimal"" parallel transport. It is however accepted that the curvature tensor represents the total obstruction for the integrability of parallel transport. The usual argument that proves $""R=0""\Longrightarrow \ ""M$ is flat$""$ involves having an arbitrary frame $e_{(i)}$ at some point $p$ of the manifold and parallel transporting it to every other point. Since parallel transport is path independent, the result is unambigous and thus we have a parallel frame. Question: How is the second order path independence implied by the vanishing of the curvature tensor enough for finite parallel transport to be integrable? I have a feeling this might be trivial, but I am nontheless confused!","['connections', 'riemannian-geometry', 'differential-geometry', 'curvature']"
2227853,Find points of tangency for a given function.,"I have spent 3 hours or so trying to tackle this problem but the answers I get arent the ones written on the page , I do not want to assume that the asnwers on the page are incorrect so here I go , The Problem The graph : $y=\frac{5x}{4} + \frac{1}{x}$ (In the first quarter) , has a tangent in point A , and a tangent in point B (both tangent to the function graph) that are perpendicular to each other . (90 deagrees).
It is given that the rate of x of point A is bigger by times 3 than point B (Xa = $t$ while Xb = $\frac{1}{3}t$). Find the credentials (x , y) of point A , and point B if it is given that the X of point A , is a natural number (bigger than zero , and not a fraction). (Didnt finish 1. so didnt get to it) What I did : I wrote that the x of point a is t , then I found the derivative of 
$f{(t)}$ which is equal to the angle of tangence (or in other words $m$ of $t$) , derivative : $f'{(t)} = 1.25 + -\frac{1}{t^2}$ after that I found the derivative of point B $(\frac{1}{3}t , f{(\frac{1}{3}t)})$ Which was $f'(\frac{1}{3}t) =  \frac{5}{12} + 3 == 3.416$ I was taught that $m1 * m2 = -1$ . so I took the two m's and followed through : Ma = $1.25 + -\frac{1}{t^2}$ , Mb = $3.416$ . Then Ma * Mb == -1 (Right ?) $(1.25 + -\frac{1}{t^2}) * 3.416 =  -1$
Which led me to $5.27t^2 = 3.416$ led me to this --> $t = 0.7726$ The answers in the book show : A(2,3) and B(0.666 , 2.666) , I tried finding my mistake  , but it seems that this also leads me to this. what did I miss here? Would love some help and insight , The problem was translated , took me a while , hope you understand it (my English isn't perfect to say the least).","['derivatives', 'tangent-line', 'linear-algebra']"
2227870,"Gauss Bonnet theorem, degree of surface with genus","I'm reading Frankel's The Geometry of Physics and I have been trying to understand the proof of the Gauss Bonnet theorem but I'm stucked with one part of the proof that is left as an exercise. I have arrived until $$\frac{1}{4\pi}\int\int K dS = deg(n: M^{2} \rightarrow S^{2})$$ with $K$ the curvature and the right part the (Brouwer) degree of the Gauss normal map. What I don't know is how to prove that the degree of the map in case of a surface of genus $g$ ($g$ holes) would be $1 - g$, so: $$\frac{1}{4\pi}\int\int K dS = 1 - g$$ As an extra, could you suggest a reference to read a proof of the Gauss Bonnet for physicists? Thanks.","['algebraic-topology', 'mathematical-physics', 'differential-geometry']"
2227882,"If $R$ is not reflexive, then exist $R^*$ such that $R \subset R^*$ and $R^*$ is reflexive.","I tried to solve this, but I don't think it is right.
I suppose that $R$ is not reflexive and $R \not\subset R^*$ or $R^*$ is not reflexive. Case 1: $R$ is not reflexive and $R \not\subset R^*$ We have that $R$ is not reflexive then $(x,x) \not\in R$, then $(x,x) \in   R^c$. And $R \not\subset R^*$ then $R^c \subset R$, since $(x,x) \in R^c$ then $(x,x) \in R^*$. Thus $R^*$ is reflexive.","['relations', 'elementary-set-theory']"
2227883,Chern classes of a closed embedding,"Let $X$ be a $K3$ surface, $\rm{Pic}(X)=\mathbb{Z}\cdot H$ where $H^2=2$, $C\in |2H|$ be smooth, $L$ a degree $6$ line bundle on $C$ and $i:C\to X$ the inclusion. I wanted to compute the first and second Chern classes of the torsion sheaf $i_*L$ on $X$. I was once told that everything can be computed using Grothendieck-Riemann-Roch, especially when there is a closed embedding. Perfect, I've used it and I've found $c_1(i_*L)=2H$ and $c_2(i_*L)=2$. Anyway, it seems to me that I have overkilled the problem and that in fact I have lost all the geometry behind this computation. I am pretty sure there must exist a easier (= more geometric) solution to arrive to the same result without using G-R-R. I'd appreciate any comment about different approaches. Thank you very much! P.S.: I have studied G-R-R on Fulton and the technique he uses is the so called ""deformation of the normal bundle"". I think my lack of a geometric meaning of the computation above sits inside my non-understanding of this technique (that I still look as a magical technical detail..). I hope some of the proposed alternative computations can help me with this bigger gap.",['algebraic-geometry']
2227977,Let $f:X\dashrightarrow Y$ be a rational map from a variety to a projective variety. Can $f$ be extended to a morphism $f:X\to Y$?,"I use the following definition for a variety: Let $k$ be a fixed field (not necessarily algebraically closed), then a scheme $X\to\operatorname{Spec}k$ is called a variety if $X$ is integral, and the structure morphism $X\to\operatorname{Spec}k$ is separated and of finite type. We say that a variety $X\to\operatorname{Spec}k$ is projective if we can consider $X$ as a closed subscheme of $\mathbb{P}_k^n$ for some $n$, i.e. if we can factor $X\to\operatorname{Spec}k$ as $X\hookrightarrow\mathbb{P}_k^n\to\operatorname{Spec}k$ for some $n$. Now, my intuition about rational maps is that they fail to be ordinary maps when they have some sort of ""singularity"", and intuitively, this isn't the case when the target scheme is projective. This is my (incomplete) work so far: Let $U\subseteq X$ be some nonempty open and let $f:U\to Y$ be a morphism of varieties. Now, if we can extend $U\to Y\to\mathbb{P}^n$ to some morphism $g:X\to\mathbb{P}^n$, then since $g(U)\subseteq Y\subseteq\mathbb{P}^n$, then $$g(X) = g(\overline{U})\subseteq \overline{g(U)} = \overline{Y} = Y\subseteq\mathbb{P}^n$$ so that we can factor $g$ through $Y$, and therefore we may replace $Y$ with $\mathbb{P}^n$. Now, given a map $f:U\to\mathbb{P}^n$ with $U\subseteq X$, then it suffices to show that if $X$ is affine, then $f$ can be uniquely extended to $X$, because in that case, we can simply cover by affine opens and glue uniquely. Furthermore, we can assume that $U$ is a distinguished open $D(f)$. Therefore, we have reduced to the following theorem: Let $X = \operatorname{Spec}(k[x_1,\ldots,x_m]/I)$ be an affine variety over $k$, let $f\in k[x_1,\ldots,x_m]/I$ be some nonzero element, and let $f:X\vert_{D(f)}\to\mathbb{P}^n$ be a morphism of $k$-varieties, then $f$ can be extended uniquely to a morphism $g:X\to\mathbb{P}^n$ of $k$-varieties. However, I'm at a loss for how to prove this. Are there any examples that can help illustrate why this is true or false?",['algebraic-geometry']
2227987,4 member committees with more women than men,"From a group of 10 men and 5 women, 4 member committees are to be formed each of which must contain at least one woman. Then the probability for these committees to have more women than men is: $\frac{2}{23}$ $\frac{1}{11}$ $\frac{21}{220}$ $\frac{3}{11}$ Since each committee must have at least one woman, 4 women have already been placed into separate committees. There is one woman yet to be placed. For a committee to have more women than men, there can be 2 women and 0 men or 1 man. For a committee having only one woman, there must not be any more men in the committee. This is what I could reason so far. But I don't know how to proceed. Edit: I might have misinterpreted four-member committees (having 4 members) as 4 committees.","['combinations', 'combinatorics', 'discrete-mathematics']"
2228018,How to calculate the third Point if two points and all distances between the Points are available?,I have a triangle with 3 Points. The positions of two points are available. And also i know the distances between the three Points. How is it possible to calculate the position of the third point? Available Point A (the x and y coordinate) Point B (the x and y coordinate) AB (distance between Point A and Point B) BC (distance between Point B and Point C) AC (distance between Point A and Point C) Unknown Point C (the x and y coordinate),"['trigonometry', 'triangles', 'geometry']"
2228019,Calculating the rank of $\widetilde{k[t]/(t)}$ on $Spec(k[t])$,"Let $\mathscr F:=\widetilde{k[t]/(t)}$ be the coherent sheaf on $Spec(k[t])$. I want to calculate the rank of $\mathscr F$. In general, if $\mathscr F$ is a (quasi)coherent sheaf on an irreducible scheme $X$ we define the rank of $\mathscr F$ to be $$\dim_{\kappa(\eta)}\mathscr F_{\eta}/\mathfrak m_{\eta}\mathscr F_{\eta}$$ where $\eta$ is the generic point of $X$. In the case we are considering, $X:=Spec(k[t])$, the generic point is equal to $(0)$, hence if we let $S=k[t]\smallsetminus\{0\}$, we have that $\mathscr F_{\eta}=S^{-1}(k[t]/(t))=\{\frac{a}{f}\mid a\in k,f\in k[t]\smallsetminus 0\}$, with addition given by $$\frac{a}{f}+\frac bg=\frac{ag(0)+bf(0)}{fg}$$ and $k(t)$-module structure given by $$\left(\frac{f}{g}\right)\cdot\left(\frac{a}{h}\right)=\frac{a\,f(0)}{gh}.$$ Since the maximal ideal $\mathfrak m_{\eta}$ is zero here, we see that the rank is just equal to $\dim_{k(t)}\mathscr F_{\eta}$. I believe this is equal to $1$, since $\mathscr F_{\eta}$ seems to be generated over $k(t)$ by $\frac{1}{1}$. Now where my trouble lies: this must be wrong, because it is an exercise in Vakil that rank is additive on exact sequences of coherent sheaves, so in particular we could take the sequence $$0\to\widetilde{k[t]}\overset{\times\,t}{\to}\widetilde{k[t]}\to\widetilde{k[t]/(t)}\to0$$ and conclude that the rank of $\widetilde{k[t]/(t)}$ is zero. So I am wondering, what's wrong with my calculation?","['schemes', 'coherent-sheaves', 'sheaf-theory', 'algebraic-geometry']"
2228031,Exhaustive set of values of $a$ in quadratic equation,"If the equation $2^{2x}+a\cdot 2^{x+1}+a+1=0$ has roots of opposite sign, the exhaustive set of values of $a$ is Attempt: Put $2^x=t,$ then equation is $t^2+2at+(a+1) = 0$ given roots are of opposite sign and parabola $f(t) = t^2+2at+(a+1)$ is upward so $x=0\Rightarrow t=1$ must lie between these two roots could some help me to solve it, thanks",['algebra-precalculus']
2228080,How do you prove that an implicitly defined function cannot be made explicit?,"Consider for example the function: $$x^2 e^y + \log(x)y^2 = 0$$ I suspect that neither x nor y can be isolated in this function (that it, it cannot be written as either $x(y)$ or $y(x)$.  However, I can't really prove that other than to say ""it's difficult to do if not impossible"". Is there a method by which it can be proven that no finite number of algebraic operations will lead to such a simplification? (The above function was presented as an example, but I am looking for a general solution)",['algebra-precalculus']
2228083,$\lim_{n\to \infty} \sum_{i=0}^n \frac {r^i}{i!} = e^r$,"Prove that $$\lim_{n\to \infty} \sum_{i=0}^n \frac {r^i}{i!} = e^r$$
Using that $\lim_{n\to \infty} \sum_{i=0}^n \frac{1}{i!} = e$ and $\lim_{n \to \infty} (1 + \frac{1}{n})^n = e$ without differentiation, L'Hôpital and Taylor series. I have no idea how to do it, any help is appreciated!","['summation', 'sequences-and-series', 'calculus', 'limits']"
2228130,Limit of sum when solving random graph problem,"The main problem is to prove that $$
  \lim\limits_{n \to +\infty}\Bigg( \frac{1}{\log n} \cdot \sum\limits_{k = 3}^n \frac{n!}{(n - k)!\cdot k \cdot n^k}\Bigg) = \frac12
$$ It is easy to prove that this limit is not bigger than 1 but every attempt to have better result was in vain.","['random-graphs', 'calculus', 'limits']"
2228143,Prove that $9\int_{0}^{\infty}x^5e^{-x^3}\ln(1+x)dx=\Gamma\left({1\over 3}\right)-\Gamma\left({2\over 3}\right)+\Gamma\left({3\over 3}\right)?$,"My last observance of this question by @Brightsun Not so interesting integral but does have a neat closed form $$9\int_{0}^{\infty}x^5e^{-x^3}\ln(1+x)\mathrm dx=\Gamma\left({1\over 3}\right)-\Gamma\left({2\over 3}\right)+\Gamma\left({3\over 3}\right)\tag1$$ An attempt: $u=1+x$, then $(1)$ becomes $$\int_{1}^{\infty}(u-1)^5e^{-(u-1)^3}\ln u\mathrm du\tag2$$ With this function $e^{-x^3}$ in $(1)$, it makes very hard to appl integratin by parts. Recall $$\Gamma(t)=\int_{0}^{\infty}x^{t-1}e^{-x}\mathrm dx\tag3$$ Differentiate w.r.t t $$\Gamma^{'}(t)=\int_{0}^{\infty}x^{t-1}\color{red}{e^{-x}}\ln x\mathrm dx\tag4$$
 The problem is that the red part is not $e^{-x^3}$ Another attempt: Recall $${1\over k}\Gamma(t)=\int_{0}^{\infty}x^{kt-1}e^{-x^k}\mathrm dx\tag5$$ Differentiate $(5)$ w.r.t t $${1\over k^2}\Gamma^{'}(t)=\int_{0}^{\infty}x^{kt-1}e^{-x^k}\ln x\mathrm dx\tag6$$ Setting $k=3$ and $t=2$ we have $$9\int_{0}^{\infty}x^5e^{-x^3}\ln(x)=\Gamma^{'}(2)\tag7$$ Not sure what is $\Gamma^{'}(2)?$ How else can we tackle $(1)?$","['gaussian-integral', 'integration', 'definite-integrals', 'calculus']"
2228148,Unique solution of Cauchy problem,"Let , $a,b,c,d \in \Bbb R$ such that $c^2+d^2 \not =0$. Then the Cauchy problem $au_x+bu_y=e^{x+y}$ , $x,y\in \Bbb R$ with $u(x,y)=0 $ on $cx+dy=0$ has a unique solution if (A) $ac+bd \not=0$. (B) $ad-bc \not =0$. (C) $ac-bd\not=0$ (D) $ad+bc \not=0$ Using Lagranges equations we get , $bx-ay=C_1$ and $u-\frac{a}{a+b}e^{x+y}=C_2$. Then the solution becomes $\displaystyle u(x,y)=\frac{a}{a+b}e^{x+y}+\phi(bx-ay)$. Then how I can proceed further to find out the answer.","['cauchy-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
2228191,How do I replace variables in $\frac{d^2 \psi}{dx^2}$?,"I have $$\frac{d^2 \psi}{dx^2}=a^3 x \psi$$ but I need to change to $z$ by $$z=ax$$ and end with $$\frac{d^2 \psi}{dz^2}=z \psi$$ How do I change variables here? This is what I tried: $$dz=adx$$ $$\frac{d^2 \psi}{dz^2} = \frac{d}{dz}\left (\frac{d \psi}{dz}  \right )$$
$$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{dx}{dz}  \right )$$
$$=\frac{d}{dz}\left (\frac{d \psi}{dx} \frac{1}{a} \right )$$ ...but then what do I do with $\frac{d\psi}{dx}$??","['derivatives', 'change-of-variable']"
2228192,Example of a nontrivial finite endomorphisms of projective space $\mathbb{P}_k^n$,"I'm looking for an example of a nontrivial finite endomorphism of projective space $\mathbb{P}^n$. By nontrivial, I mean that the morphism $f:\mathbb{P}^n\to\mathbb{P}^n$ should not simply restrict to some very obvious affine morphism $f:V_+(x_i)\to V_+(x_i)$, though of course these examples are the easiest to come up with, making my task a bit more difficult. More specifically, I'm looking for some finite endomorphism on $\mathbb{P}_k^n$ the intuitively spreads out the affine opens to cover a larger part of $\mathbb{P}_k^n$ then before. Is there some easy example of this? As a slightly related question, if I replace ""finite"" with ""surjective"", what examples are there?",['algebraic-geometry']
2228226,What is the $m$th derivative of $\log\left(1+\sum\limits_{k=1}^N n_kx^k\right)$ at $x=0$?,"Let $n_k$ be integers. Is there a general formula for the Taylor expansion of
$\log(1+\sum_{k=1}^N n_kx^k)$ at $x=0$?
This boils down to find an expression for the $m$th derivative of  $\log(1+\sum_{k=1}^N n_kx^k)$ evaluated $x=0$:
$$
\frac{d^m}{dx^m}\log(1+\sum_{k=1}^N n_kx^k)\Biggr|_{x=0}?
$$ Expanding a (work-in-progress) example like $-\log(n_3x^3+n_2x^2+n_1x+1)$ gives:
$$
\begin{array}{cl}
 x&            (- n_1)\\ 
+ \frac{x^2}2& (+ n_1^2 - 2n_2 ) \\
+ \frac{x^3}3& (- n_1^3 + 3n_2 n_1   - 3n_3) \\
+ \frac{x^4}4& (+ n_1^4 - 4n_2 n_1^2 + 4n_3 n_1  + 2n_2^2 ) \\
+ \frac{x^5}5& (- n_1^5 + 5n_2 n_1^3 - 5n_3n_1^2 - 5n_2^2 n_1  + 5n_3n_2    ) \\
+ \frac{x^6}6& (+ n_1^6 - 6n_2 n_1^4 + 6n_3n_1^3 + 9n_2^2n_1^2 - 12n_3n_2n_1 - 2n_2^3 +3 n_3^2  ) \\
&\dots
\end{array}
$$ Some findings: For each $\frac{x^k}k$ the factor in brackets relates to partitions of $k$, which is not very surprising. Fixing $k$ then the sign is a term inside a bracket can be determined by the sum of powers of $n_j$: Even sums are negative (e.g. $-n_1^4$) and odd sums positive (e.g. $+5n_2^2n_1$), which reminds me on Möbius' function... From one line to the other you can see that multiplying a $n_1$ to the second term adds $1$ to the prefactor (and changes sign) This looks like a combinatorial thing. . .","['combinatorics', 'oeis', 'polynomials', 'sequences-and-series']"
2228257,Prove that $T(x^* + W^{\perp}) = y^*$ is an isometric isomorphism from $V^*/W^\perp$ to $W^*$,"Let $(V, \| \cdot \|)$ be a normed vector space and $W$ be a linear subspace. Prove that $T: V^*/W^{\perp} \to W^*, \ T(x^* + W^{\perp}) = y^*$ where $y^*(x) = x^*(x)$ for all $x \in W$, is an isometric isomorphism. $\perp$ denotes the annihilator, and $*$ the dual. There was a hint included that said ""First show that $W^{\perp}$ is a closed linear subspace of $V^*$. Prove that $T$ is a well-defined linear operator. To show that $T$ is an isometric isomorphism apply the Hahn-Banach theorem."" I'm stuck at the last part. Let $y^* \in W^*$ then from Hahn-Banach we have that $\exists \ x^* \in V^*$ s.t $x^* = y^*$ on $W$, and $\|x^*\|_{V^*} = \|y^* \|_{W^*}$. But how can I arrive at $\|T(x^* + W^{\perp})\|_{W^*} = \|x^*+ W^{\perp}\|_{V^*/W^{\perp}}$?","['functional-analysis', 'banach-spaces']"
2228300,"How many positive integers between 0 and 1,000,000 contain the digit 9?","In the example, it says that its easier to count the number of integers that do not contain the digit 9 what i did is, since its from $0$ to $1{,}000{,}000$ i can simply say its to $999{,}999$
and for 6 digits there are 8 choices of numbers, so $6^8$.
now subtract that from $1{,}000{,}000$ and we get $679{,}616$ as the numbers that contain the digit $9$, is that correct?",['combinatorics']
2228319,"closed ideals of $C(X,\mathcal{B})$","Question:- Let $X$ be a compact Hausdroff space and $\mathcal{B}$ be a simple C*-algebra. Prove that the closed ideals of $\mathcal{B}\otimes_{min} C(X) \cong^* C(X,\mathcal{B})$ are of the form $\mathcal{B}\otimes J$ where $J$ is a closed ideal of $C(X)$. Proof:- Let $I$ be a proper closed ideal of $C(X,\mathcal{B})$ and set 
$$E = \{x\in X : f(x) = 0 \text{ for all } f\in I\}.$$
Then $E$ is a closed subset of $X$ and $I\subseteq I(E) := \{f\in C(X,\mathcal{B}) : f(x) = 0 \text{ for all } x\in E\}$. Indeed, let $x$ be a limit point of $E$. Suppose there exist $f\in I$ such that $f(x)\neq 0$. Then, by continuity, there exists an open set $U$ containing $x$, such that $f(y)\neq 0$ for all $y\in U$. Since $U$ is neighborhood of $x$, there is $q\in E$ and $q\neq x$, for which $f(q)\neq 0$, which is a contradiction. Therefore, $x\in E$. claim : $I(E)\subseteq I$. Assume that $X\cap E = \emptyset$. Then, for every point $p\in X$, there exists a continuous function $f\in I$ such that $f(p)\neq 0$. Then, by continuity, there must exist an open set $U$ containing $p$ so that $f(q)\neq 0$ for all $q\in U$. Thus, we may assign to each point $p\in X$ a continuous function $f\in I$ and an open set $U$ of $X$ such that $f(q)\neq 0$ for all $q\in U$. Since this collection of open sets covers $X$, which is compact, there must exists a finite subcover which also covers $X$. Call this subcover $U_1,\cdots, U_n$ and the corresponding functions $f_1, \cdots, f_n$. Consider the function $g$ defined as 
$$g(x) = f_1(x) f^*_1(x)+\cdots+f_n(x) f^*_n(x).$$
Since $I$ is an ideal, $g\in I$. For every point $p\in X$, there exists an integer $i$ between $1$ and $n$ such that $f_i(p)\neq 0$. This implies that $g(p) \neq 0$. Also, $g$ is positive element of $\mathcal{B}$. how to proceed further...... if I prove that $g^{-1}$ exist, then $g.g^{-1}\in I$, which implies, $I = C(X,\mathcal{B})$, a contradiction (as $I$ is proper).","['functional-analysis', 'c-star-algebras', 'operator-algebras']"
2228328,$f(z)=\frac 1 {x^2+y^2}+i \frac 1 {x^2+y^2} $ is differentiable and holomorphic,"Is the function  $ f(z)=\dfrac 1 {x^2+y^2} + i \dfrac 1 {x^2+y^2} $ is differentiable and holomorphic somewhere? We have $z=x+iy$ and $f(z)= \dfrac{1+i}{|z|^2}$ . Now, $f(0)= \lim_{\delta z \rightarrow 0} \frac{f(0+\delta z) - f(0)}{\delta z}=$ undefined. So $f(z)$ is not differentiable at origin.  Also since since $ f(z)= |z|^2$ is not differentiable anywhere except origin , so $f(z)$ is not not differentiable and hence it is not holomorphic.   But I am not sure, please help me",['complex-analysis']
2228353,Solving $\int\frac{\frac{1}{x}}{1-\frac{1}{x}}\;dx$ in the simplest way,"I am trying to solve a problem in my book: $$\int\frac{\frac{1}{x}}{1-\frac{1}{x}}\;dx$$ I am trying to do this without using any techniques (eg. no u-sub, no parts...) just using what I know about basic integrals, by inspection. I tried to pull the denominator into the numerator to get some constant that I can pull in front of the integrand, but I have been unsuccessful. I am looking to see how I should simplify this and break it apart into smaller integrals that are easy to compute.
Thank You.","['integration', 'calculus']"
