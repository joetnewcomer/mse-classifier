question_id,title,body,tags
3770868,Prove that $\sum_{r=2}^{n} \left \lfloor n^{\frac{1}{r}} \right \rfloor = \sum_{r=2}^{n} \left \lfloor \log_{r}(n) \right \rfloor$.,"Prove that $$\sum_{r=2}^{n} \left \lfloor n^{\frac{1}{r}} \right \rfloor = \sum_{r=2}^{n} \left \lfloor \log_{r}(n) \right \rfloor\,.$$ I have tried to use substitutions of $n=p^k$ in order to try and simplify, but it doesn't seem to lead anywhere, I'm not entirely sure how I can get around the floor functions.","['ceiling-and-floor-functions', 'summation', 'logarithms', 'elementary-number-theory', 'combinatorics']"
3770874,"Given that $f(x)$ is a polynomial of degree $3$, and its remainders are $2x - 5$ and $-3x + 4$ when divided by $x^2 - 1$ and $x^2 - 4$ respectively.","So here is the Question :- Given that $f(x)$ is a polynomial of degree $3$ , and its remainders are $2x - 5$ and $-3x + 4$ when divided by $x^2 - 1$ and $x^2 - 4$ respectively. Find $f(-3)$ . What I tried :- Since it's given that $f(x)$ is a polynomial of degree $3$ , I can assume $f(x) = ax^3 + bx^2 + cx + d$ for some integers $a,b,c,d$ and $a\neq 0$ . Then we have :- $$ax^3 + bx^2 + cx + d = (x^2 - 1)y + (2x - 5)$$ $$ax^3 + bx^2 + cx + d = (x^2 - 4)z + (-3x + 4)$$ This gives that $(x^2 - 1)y + (2x - 5) = (x^2 - 4)z + (-3x + 4)$ . But I am not sure how to proceed further since we have $3$ variables to deal with , and I am stuck here. Any hints or explanations for this problem will be greatly appreciated !!","['cubics', 'number-theory', 'elementary-number-theory', 'polynomials', 'problem-solving']"
3770908,Finding perpendicular lines in $\mathbb R^4$,"Let $$g=\begin{pmatrix}2\\-5\\-3\\-3\end{pmatrix}+\mathbb R\begin{pmatrix}1\\2\\3\\4\end{pmatrix}$$ and $$h=\begin{pmatrix}1\\-3\\0\\-1\end{pmatrix}+\mathbb R\begin{pmatrix}2\\3\\4\\5\end{pmatrix}.$$ Find all lines that are perpendicular to both $g$ and $h$ . Find the smallest affine subspace in $\mathbb R^4$ that contains both $g$ and $h$ . As for 1: One can easily see that the two lines are skew. Now, if $v_g$ and $v_h$ are the direction vectors of the lines I am first interested in a base of $U^\perp$ where $U=\langle v_g,v_h\rangle$ . I got $$U^\perp=\left\langle\begin{pmatrix}2\\-3\\0\\1\end{pmatrix},\begin{pmatrix}-1\\1\\1\\-1\end{pmatrix}\right\rangle=:\langle v_1,v_2\rangle.$$ So now we should get two perpendecular lines $$l_1=p_1+\mathbb R v_1\quad\text{ and }\quad l_2=p_2+\mathbb R v_2$$ and need to find $p_1$ and $p_2$ . We can parametrize $g$ via $$
\vec{P}_{\lambda}=\left(\begin{array}{c}
2+\lambda\\
-5+2\lambda\\
-3+3\lambda\\
-3+4\lambda
\end{array}\right)
$$ and $h$ via $$
\vec{G}_{\mu}=\left(\begin{array}{c}
1+2\mu\\
-3+3\mu\\
4\mu\\
-1+5\mu
\end{array}\right).
$$ So the connection of $g$ and $h$ has the direction vector $$
v=\overrightarrow{P_{\lambda}G_{\mu}}=\left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right).
$$ The condition $v\perp g$ and $v\perp h$ yields $$
\left\langle \left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right),\left(\begin{array}{c}
1\\
2\\
3\\
4
\end{array}\right)\right\rangle =0=\left\langle \left(\begin{array}{c}
-1+2\mu-\lambda\\
2+3\mu-2\lambda\\
3+4\mu-3\lambda\\
2+5\mu-4\lambda
\end{array}\right),\left(\begin{array}{c}
2\\
3\\
4\\
5
\end{array}\right)\right\rangle 
$$ and thus, $$
20+40\mu-30\lambda=0\,\,\,\,\,\,\,\,\,\,\text{and}\,\,\,\,\,\,\,\,\,26+54\mu-40\lambda=0.
$$ The solution of this system of linear equations is given by $\mu=1$ and $\lambda=2.$ With that, we find \begin{align*}
l_{1} & =\vec{P}_{2}+\mathbb{R}\overrightarrow{P_{2}G_{1}}\\
 & =\left(\begin{array}{c}
4\\
-1\\
3\\
5
\end{array}\right)+\mathbb{R}\left(\begin{array}{c}
-1\\
1\\
1\\
-1
\end{array}\right).
\end{align*} Is this correct so far? But how do I get the second one? As for 2: For the smallest subspace that contains both $g$ and $h$ I would take $g+v$ where $v$ is the direction vector between $g$ and $h$ as mentioned above. Does this make sense?","['orthogonality', 'analytic-geometry', 'linear-algebra']"
3770941,Evaluate $\lim\limits_{x \to \infty} \sqrt[n]{(1+x^2)(2+x^2)...(n+x^2)}-x^2 $,"I'm trying to calculate: $$T = \lim\limits_{x \to \infty} \sqrt[n]{(1+x^2)(2+x^2)...(n+x^2)}-x^2$$ Here is my attempt. Put $x^2=\dfrac{1}{t}$ so when $x\to \infty, t \to 0$ and the limit become \begin{align*}
T &= \lim\limits_{t \to 0} \sqrt[n]{\left(1+\dfrac{1}{t}\right)\left(2+\dfrac{1}{t}\right)...\left(n+\dfrac{1}{t}\right)}-\dfrac{1}{t}\\
&=\lim\limits_{t \to 0} \sqrt[n]{\left(\dfrac{t+1}{t}\right)\left(\dfrac{2t+1}{t}\right)...\left(\dfrac{nt+1}{t}\right)}-\dfrac{1}{t} \\
&=\lim\limits_{t \to 0} \dfrac{\sqrt[n]{(t+1)(2t+1)...(nt+1)}-1}{t}
\end{align*} My idea is to use $\lim\limits_{x\to0}\dfrac{(ax+1)^{\beta}-1}{x} =a\beta .$ But after some steps (above), now I'm stuck.
Thanks for any helps.","['limits', 'calculus', 'real-analysis']"
3770956,Are there exotic balls?,"Suppose $M$ is a compact smooth manifold with boundary, which is homeomorphic to the compact ball $\mathbb{B}^d\subset \mathbb{R}^d$ . Must $M$ be diffeomorphic to $\mathbb{B}^d$ or are there exotic smooth structures? I suspect that the answer is well known or might follow from a simple argument that experts (which I am certainly not) have up their sleeves. A natural candidate in $d=4$ would be to take $M$ to be the compact unit-ball in an exotic $\mathbb{R}^4$ , but I don't know how to check whether $M$ is exotic (or even whether it has a smooth boundary to be honest).","['smooth-manifolds', 'geometric-topology', 'manifolds', 'general-topology', 'algebraic-topology']"
3770960,Hint to an Inequality,"I have the following problem in my book- Prove that for all non-negative real numbers $x, y, z$ - $$ 6(x+y-z)(x^2+y^2+z^2)+27xyz \le 10(x^2+y^2+z^2)^{3\over 2} $$ And the solution : $$\color{red}{10(x^2+y^2+z^2)^{3\over 2}-6(x+y-z)(x^2+y^2+z^2)}=(x^2+y^2+z^2)(10\sqrt{x^2+y^2+z^2}-6(x+y-z))$$ $$ =(x^2+y^2+z^2)\left(\color{red}{{10\over 3}\sqrt{(x^2+y^2+z^2)(2^2+2^2+1^2)}}-6(x+y-z)\right) $$ By C-S, $$ \ge(x^2+y^2+z^2)\left({10(2x+2y+z)\over 3}-6(x+y-z)\right)={(x^2+y^2+z^2)(2x+2y+28z)\over 3}$$ By AM-GM, $ x^2+y^2+z^2 \ge 9\sqrt[9]{x^8y^8z^2\over 4^8} $ and $ 2x+2y+28z\ge 9\sqrt[9]{4^8xyz^7} $ and multiplication accompanied by division with 3 ends the proof. The validity of the proof is undoubted, but I want to know that how the author comes up with such an idea ? What has the question reflected that hints the major steps to the proof in red? Note :- I want just the idea that leads to the proof, not a solution.","['proof-explanation', 'a.m.-g.m.-inequality', 'multivariable-calculus', 'cauchy-schwarz-inequality', 'inequality']"
3771008,Definition of geodesic not as critical point of length $L_\gamma$ [*] [duplicate],"This question already has answers here : geodesic computation: ""energy"" minimization versus arc length minimization (2 answers) Closed 3 years ago . Context of this question:
This question follows from a post Decomposition of a function and chain rule. and discusses on something different. Using calculation of variation we can find critical points of a function of a vaiable curve $\gamma$ with its end points fixed at $a,b$ and therefore defining geodesic on a manifold. (The rest of the paragraph is unnecessary reading for the question; it's mainly for the purpose of arranging my several posts on a topic.) Along the geodesic, exponential maps on a manifold project a tangent vector at a point $p$ (locally approximately linearly) to another point, as discussed here What is exponential map in differential geometry (a related but different concept of exponential maps of Lie group is discussed here Relations between two definitions of Lie algebra ). Geodesics have as such properties like the 'closed curve' { $\exp_p(v),\forall v$ of the same norm and belonging to $T_pM$ } is perpendicular to all the geodesics passing through $p$ , and is the shortest curve connecting $a,b$ (i.e. it's also a critical point for length). So we can say the 'closed curve' is very much resembles a circle, and a geodesic a radius or a straight line (We can perhaps even say that with geodesic and exponential maps we 'maps' projective geometry on to a manifold, similar to what we do when, with homeomorphism in definition of a manifold, we 'map' Euclidean space to a manifold). With the fact that geodesics is the shortest curve, we can define a metric (a measure of distance, NOT Riemannian metric which is an inner product and 2-tensor, as discussed here: Calculation of inner product for Riemannian metrics. ) on a manifold. The metric is homeomorphic to the original metric of the manifold, as discussed here Comparison of metrics on a manifold. . My question is as follows: A critical point of 'energy' (as Spivak calls it) $E(\gamma)=\int_a^b \langle \frac{d\gamma}{dt},\frac{d\gamma}{dt}\rangle dt$ --where $\frac{d\gamma}{dt}$ is tangent vector along $\gamma$ at the point of $\gamma(t)$ --is called geodesic. (I guess he uses the name 'energy' for in physics square of velocity is proportional to energy.) Why we define critical point for energy, instead of critical point for length $L(\gamma)=\int_a^b\sqrt{\langle \frac{d\gamma}{dt},\frac{d\gamma}{dt}\rangle} dt$ , to be geodesic?",['differential-geometry']
3771027,The quadratic equations $x^2+mx-n=0$ and $x^2-mx+n=0$ have integer roots. Prove that $n$ is divisible by $6$.,"QUESTION: Suppose that $m$ and $n$ are integers, such that both the quadratic equations $$x^2+mx-n=0$$ and $$x^2-mx+n=0$$ have integer roots. Prove that $n$ is divisible by $6$ . MY APPROACH: $\because$ the roots $\in\Bbb{Z}$ therefore, discriminant of the quadratic equations must be a perfect square.. $$\therefore m^2+4n=p^2$$ and $$m^2-4n=q^2$$ for some, $p,q≥0$ and $p,q\in\Bbb{Z}$ . Now subtracting these equations we get, $$8n=p^2-q^2$$ $$\implies p^2-q^2\equiv0\pmod{8}$$ Therefore, $p$ and $q$ cannot be of the form $(2×n)$ where $n$ is odd. But this does not seem to help much. So I going back one step, we can write, $$n=\frac{p^2-q^2}{8}$$ But here I am stuck.. I do not know how may I use $8$ with the property of squares to prove that $n$ must be divisible by $6$ .. Any help will be much appreciated... Thank you so much :)","['systems-of-equations', 'discriminant', 'number-theory', 'roots', 'quadratics']"
3771057,Is there a convention to interpret equalities of functions as series?,"I am wondering about whether there is a default or standard interpretation of statements such as $$\sum_{n=1}^\infty f_n(x) = f(x)$$ or equivalently $$\sum_{n=1}^\infty f_n = f$$ In some cases these statements can mean 'uniformly convergent to $f$ ' or just 'pointwise convergent to $f$ '. But sometimes I come across these equalities without the uniform or pointwise qualification, and thus in these situations I don't know whether as a default to interpret them as meaning pointwise or uniform convergence. For example, when I first learnt about power series, we had not yet met the notions of uniform convergence (or pointwise). We simply defined $f(x) = \sum_{n=1}^\infty a_nx^n$ . In hindsight, this equality really is equivalent to asserting the pointwise convergence of the series to $f$ over the radius of convergence. (Although it also turns out to be uniformly convergent within the radius) Another example comes from the second answer in this question: When can a sum and integral be interchanged? , from the user Jonas Teuwen. In particular, he states that $f = \sum_n f_n$ in his answer. How should these equalities be interpreted? Is there a default, e.g. just assume it means pointwise, or is it entirely context dependent? [Note: my current understanding is that when we deal with infinite series of functions, writing it as an equality is really a shorthand for some first order logic statement.  I.e. it is completely analogous to the fact that stating $\lim_{n \rightarrow \infty} a_n = l$ in the case of real sequences really means $\forall \epsilon >0 \exists N \forall n>N (|a_n - l|< \epsilon)$ . In this way I think of the equality symbol as just shorthand for a more verbose expression when it comes to series of functions, rather than meaning equality of mathematical objects so to say. In this sense, I don't know how to interpret the statements abut equality of series without any context.]","['sequences-and-series', 'real-analysis']"
3771085,Compute the value of $M=\frac{2015!}{2002!\cdot13!}\;\bmod\;2017$,"Recently, I have found this problem: Given that $2017$ is a prime number, compute the value of the following expression: $$M=\frac{2015!}{2002!\cdot13!}\;\bmod\;2017$$ I have tried two diffent ways: -In Python I have written a small program to compute the result. Here's the code: >>>def F(n):
   ...  if n==1:
   ...          return 1
   ...  else:
   ...          return n*F(n-1) And: >>>import sys
>>>sys.setrecurcionlimit(2020)
>>>M = F(2017)/(F(2002) * F(13)) % 2017 -In the second way, I have written all prime factors of $\{2015,2014,\cdots,2004,2003\}$ and then I have simplyfied the left side of the expression. I obtained: $$M\;\bmod\;2017\equiv5\cdot7\cdot17\cdot19\cdot31\cdot41\cdot53\cdot59\cdot61\cdot67\cdot167\cdot223\cdot251\cdot401\cdot503\cdot2003\cdot2011\;\bmod\;2017$$ where all numers are primes. But how can we go on?","['number-theory', 'modular-arithmetic', 'elementary-number-theory']"
3771088,Perelman's entropy functional computation problem.,"I was reading Hopper and Andrews's book on Ricci flow in Riemannian Geometry. I came across the following proposition on the monotonicity of Perelman's $\mathcal{W}$ -functional. Let $(g(t),f(t), \tau(t))$ evolve by \begin{align*}
\frac{\partial g}{\partial t} &= -2\text{Ric} \\
\frac{\partial f}{\partial t} &= -|\nabla f|^{2} + \Delta f - \text{Scal} + \frac{n}{2\tau} \\
\frac{d \tau}{d t} &= -1. 
\end{align*} Consider the function $$
w = (\tau(\text{Scal} + 2\Delta f - |\nabla f|^{2}) + f - n)u,
$$ where $u = (4\pi \tau)^{-n/2}e^{-f}$ . Also, consider the operator $$
\Box^{*} = -\frac{\partial}{\partial t} - \Delta + \text{Scal}. 
$$ We have to show that $$
\Box^{*}w = -2\tau \left | \text{Ric} + \text{Hess}{f} - \frac{g}{2\tau} \right|^{2} u. 
$$ I'm having trouble proving this. The book refers to Peter Topping's lecture notes on Ricci flow where the computation is done. In the first line of their proof, I found the following which I find problematic. They write $$
\Box^{*}w = \Box^{*}(u) \frac{w}{u} - \left(\frac{\partial}{\partial t} + \Delta \right) \left(\frac{w}{u}\right) - 2 \left \langle \nabla \left( \frac{w}{u} \right) , \nabla u \right \rangle. 
$$ I believe that the term comes from the following intermediate step, $$
\Box^{*}w = \Box^{*}(u) \left( \frac{w}{u} \right) + u \Box^{*}\left( \frac{w}{u} \right).
$$ The last term in the above expression troubles me. What should have been simply $$
 u \text{Scal} \frac{w}{u}
$$ is written as $$
-2\left \langle \nabla \frac{w}{u}, \nabla u \right \rangle. 
$$ I don't understand how are these equal. When I tried to solve the derivative myself I found exactly these terms which I couldn't wish away. I found the same problem as an exercise in Chow, Lu, and Ni's book on Hamilton's Ricci flow as well. I would be happy to provide more details if needed.","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
3771147,"Can I solve this Cauchy problem? If yes, how?","Background: I am still studying This question and I could solve my problems with it if the following question can be answered. My problem: Suppose $F:\mathbb{R}\times \mathbb{R} \times \mathbb{R} \to \mathbb{R}$ is $C^{\infty}$ . Suppose $F$ is strictly convex in the third variable but possibly $F_{pp}(t_0,u_0,\beta_0)=0$ for some $(t_0,u_0,\beta_0) \in \mathbb{R}\times \mathbb{R} \times \mathbb{R}$ . For example $F(x,z,p)=p^4-e^x\sin(z)$ as in the quoted question. Can I say that $\frac {\partial}{\partial x}F_p(x,u(x),u'(x))=F_z(x,u(x),u'(x))$ has local solution for every $t_0,u_0,\beta$ such that $u(t_0)=u_0$ and $u'(t_0)=\beta$ ? My attempt: If $F_{pp}$ was strictly positive than I would write $u''=F_{pp}^{-1}(F_z-F_{xp}-F_{zp})$ and I would solve the question with the classical theory about ordinary Cauchy's problems. But if for some value $F_{pp}=0$ ? For the case $F(x,z,p)=p^4-e^x\sin(z)$ we could observe that $F_{pppp}>0$ thus we could reproduce the reasonment as above. But there are strictly convex function such that does not exists $k \geq 0$ with $\frac{\partial ^k}{\partial p^k}F>0$ as $F(x,z,p)=e^{-\frac{1}{p^2}}$ .","['ordinary-differential-equations', 'calculus-of-variations', 'euler-lagrange-equation', 'functional-analysis', 'partial-differential-equations']"
3771149,In a metric space is a dense subset of a dense subspace dense in the space itself?,"In a metric space is a dense subset of a dense subspace dense in the space itself? I think that must be false, but I couldn't think of any examples of the contrary.","['general-topology', 'metric-spaces']"
3771155,Let $A_1 \cap A_2 \cap \dots \cap A_n \neq \varnothing $. Then $A_1 \cup A_2 \cup \dots \cup A_n \neq \varnothing$.,"Let $A_1,A_2,\dots, A_n$ be sets such that $A_1 \cap A_2 \cap \dots \cap A_n \neq \varnothing $ holds for all $n$ . Then $A_1 \cup A_2 \cup \dots \cup A_n \neq \varnothing$ . Is the following proof correct? Proof: If $A_1 \cap A_2 \cap \dots \cap A_n \neq \varnothing $ , then $A_1,A_2,\dots, A_n$ must all have at least one common element. Therefore the sets $A_1,A_2,\dots, A_n$ are all non-empty. Hence there exists one non-empty set among $A_1,A_2,\dots, A_n$ . Hence $A_1 \cup A_2 \cup \dots \cup A_n \neq \varnothing$ .","['elementary-set-theory', 'solution-verification']"
3771166,Find the complete solution set of the equation ${\sin ^{ -1}}( {\frac{{x + \sqrt {1 - {x^2}} }}{{\sqrt 2 }}} ) = \frac{\pi }{4} + {\sin ^{ - 1}}x$ is,"The complete solution set of the equation ${\sin ^{ - 1}}\left( {\frac{{x + \sqrt {1 - {x^2}} }}{{\sqrt 2 }}} \right) = \frac{\pi }{4} + {\sin ^{ - 1}}x$ is (A)[-1,0] (B)[0,1] (C) $[-1,\frac{1}{\sqrt{2}}]$ (D) $[\frac{1}{\sqrt{2}},1]$ My approach is as follow ${\sin ^{ - 1}}\left( {\frac{{x + \sqrt {1 - {x^2}} }}{{\sqrt 2 }}} \right) - {\sin ^{ - 1}}x = \frac{\pi }{4}$ ${\sin ^{ - 1}}\left( {\left( {\frac{{x + \sqrt {1 - {x^2}} }}{{\sqrt 2 }}} \right)\sqrt {1 - {x^2}}  - x\sqrt {1 - {{\left( {\frac{{x + \sqrt {1 - {x^2}} }}{{\sqrt 2 }}} \right)}^2}} } \right) = \frac{\pi }{4}$ ${\sin ^{ - 1}}\left( {\left( {\frac{{x\sqrt {1 - {x^2}}  + 1 - {x^2}}}{{\sqrt 2 }}} \right) - x\sqrt {1 - \left( {\frac{{{x^2} + 1 - {x^2} + 2x\sqrt {1 - {x^2}} }}{2}} \right)} } \right) = \frac{\pi }{4}$ ${\sin ^{ - 1}}\left( {\left( {\frac{{x\sqrt {1 - {x^2}}  + 1 - {x^2}}}{{\sqrt 2 }}} \right) - x\sqrt {\left( {\frac{{2 - 1 - 2x\sqrt {1 - {x^2}} }}{2}} \right)} } \right) = \frac{\pi }{4}$ $\left( {\left( {\frac{{x\sqrt {1 - {x^2}}  + 1 - {x^2}}}{{\sqrt 2 }}} \right) - x\sqrt {\left( {\frac{{1 - 2x\sqrt {1 - {x^2}} }}{2}} \right)} } \right) = \frac{1}{{\sqrt 2 }}$ Not able to proceed from here",['trigonometry']
3771188,"Prove that $ |A| = \lim_{t\rightarrow \infty}| A \cap (-t,t)|$ for all $A \subset \mathbb{R}$","Problem taken from the books sheldon Axler Measure , integration Real analysis Prove that $ |A| = \lim_{t\rightarrow \infty}| A \cap (-t,t)|$ for all $A \subset \mathbb{R}$ My attempt : $\lim_{t\rightarrow \infty}| A \cap (-t,t)|=|A \cap (-\infty,\infty)|=\min| A|$ Im newly learning measure theory",['measure-theory']
3771207,Can I understand $1$-form this way?,"It's said a function $f$ on $M$ is a $0$ -form; I think an example of one form is $df=\frac{\partial f}{\partial x_1}dx^1+\dots+\frac{\partial f}{\partial dx^n}dx^n$ , where $(x,U)$ is the local coordinate system, $dx_i$ is basis for tangent space $M_p$ of a manifold $M$ at p, $dx^i$ is dual basis for dual space ${M_p}^*$ . (There may be something that goes wrong here, because when I read the book I am often not sure if the author is refering to vector/basis or dual vector/basis.) Since $dx^i$ is first order tensor, so it's function of a vector, and therefore so is $df$ . When $df$ acts on one-cube or other 1-dim object, say a vector $v$ , we get $df(v)=df=\frac{\partial f}{\partial x_1}dx^1(v)+\dots+\frac{\partial f}{\partial dx^n}dx^n(v)$ which is a number (or  it is a number only when $f$ maps $M$ to $\mathbb{R}$ or $\mathbb{C}$ and so each partical derivative is a number?). And so $df$ is actually a one-tensor, and a measure function giving a 'length' of a vector. We can further simplify the example, for example, let $f$ be the indentity map of $M$ , then $df$ is also identity map of the tangent bundle $TM$ . Then $df(v)=v$ , which means $df$ is not 1-tensor or measure function. Seems to contradict my  thought above. (But if we let $f$ be a function mapping points on $M$ to numbers, then $df(v)$ gives a number. So does $0$ -form $f$ have to be such?) Then what's 1-form? Would anyone give an illuminating example? (Since $k$ -form is related to wedge product, the topic discussed here is related to my another post Is wedge product of n-dimension the length/area/volume,... of an oriented set of n vectors? , which contains a bit of discussion as well.) It is said $k$ -form is alternating covariant tensor field (alternating covariant vector field when k=1), which means it is a section of a subset $\Omega^k(TM)$ of k-fold covariant tensor field $\mathcal{T}^k(TM)$ . $\mathcal{T}^k(TM)$ is a collection of functions $T:TM\times\dots\times TM\rightarrow \mathbb{R}$ . So k-form should be at each point we have products of k 1-tensor, i.e. { $v_{1,p}*\otimes...\otimes v_{k,p}*, p\in M,$ and $v_{i,p}\in M_p$ , i.e. is a tangent vector at $p$ }. So perhaps we can roughly think that given k vector fields, changing every tangent vector to its dual vector, then we get a k-form. And it will act on k tangent vectors at one point, e.g. { $dx_{i,p}$ , $1\leq i\leq k$ }, a subset of the basis of $(x,U)$ where U is a neighborhood of $p$ . Now it seems safe to say $df$ as one tensor can't be identity map of the tangent bundle $TM$ . But still, how should we understand 1-form? (Edited to add:) So 1-form $\omega$ (if exact, = $d\eta$ , where $\eta:M\rightarrow \mathbb{R}^1$ ) is almost completely an analog of $df$ for $f:\mathbb{R}^n\rightarrow :\mathbb{R}^1$ . (It and its integration are more general than a measure function of length, volume, etc.) It's related to tensor because tensor is a linear function (to one dimensional space, whatever the order of tensor!) of several tangent vector (an analog of $dx$ ). So a 1-tensor is linear operator on vector(s), i.e. a $1\times n$ matrix (a transposed vector). It's therefore natural to think of k-tensor as a candidate of first order (linear approximation of) change of a function mapping vector(s) to $\mathbb{R}$ . Considering its variables, 1-form is function of point of $M$ (so is $df$ ), an analog to the f'(x) part (when n=1) or linear operator (matrix) $A$ part of $df$ , both parts being function of $x$ in n-dim Euclidean space domain. 1-form is also a (linear) function of tangent vector, WHEN the point variable is fixed, an analog to ' $\dot \ dx$ ' (dx as scalar or vector) part of $df$ , indicating $df$ being linear transformation (i.e. matrix or having a matrix representation) of or proportional to $dx$ ; however, this fact looks so trivial in the usual calculus that I ignore it. Combining this two facts, we can say 1-form is like a  (variable) matrix (probably named Jacobian matrix) ACTING on 1 vector, whose entries vary with the point variable. In other words, we can say it is, in the usual calculus, similar to case of differential forms on manifold (1-forms is the derivative of a function between two manifolds (one is $\mathbb{R}^1$ ), and so maps a tangent vector to a tangent vector $\in T\mathbb{R}^1$ ), more comprehensive to understand differential of $f$ as $df$ , rather than $f'(x)$ (when n=1) or as matrix $A$ alone (which easily causes us to think $dx$ is not part of but outside the derivative, as we do in writing $\int f'dx$ and 1-order part of function expansion $f'(x)(x'-x)$ , and find the two are the same linear approx of difference.). And we can show the relation better by writing $df$ as $df_x(dx)$ where x and dx are completely independent (except that dx is at x), $df_x$ (x fixed) being a linear transformation ( $t\rightarrow kt, t\rightarrow At$ ). Comparing $df_x(dx)$ with the notation in diff geom, $\omega_{*p}(v)$ or $\omega_*(v_p)$ , we see  the latter can really be understood as at first a function of p, and then when p is fixed, a function of (tangent) vector, i.e. of a linear approximation of displacement of point in $M$ ; or reversely we can understand $df$ directly as a function a $dx_x$ . Another change caused by such an switch from $df$ to 1-form perspective is that now we regard integral as (the limit of) sum of value of 1-form $\omega_{p_r}(v)$ or $df_{x_r}(dx)$ at several $p_r$ or $x_r$ , that is, the linear or 1-st order approximation of change in $\eta$ (the function whose derivative in manifold sense is $\omega$ , if $\omega$ is exact) or $f$ , instead of regarding integral as sum of area; since we can more conveniently obtain it given small displacement at $\omega_{p_r}(v)$ or $df_{x_r}(dx)$ . Note this perspective is different from the fundamental law of calculus as it still involves sum of infinite segmentation. We can also see $\eta$ and $f$ as scalar field on a manifold or Euclidean space. With the new integration perspective and scalar field perspective, now we can look into, for example, length integration $L_\gamma$ of a curve in $\mathbb{R}^3$ . We need to place the curve on a surface $M$ (to illustrate the vector basis' and dual basis' role; note that if we set the basis to be of 2-dim space, we have to do so). Then we can find the 1-form $\omega: \gamma'dt \mapsto \sqrt{\langle \gamma', \gamma'\rangle}dt$ , i.e. $d\gamma \mapsto \sqrt{\langle d\gamma, d\gamma\rangle},\ TM\rightarrow T\mathbb{R}$ , which is the derivative of $\eta:\gamma\mapsto {L_{\gamma(\tau)}}|_0^t,\ M\rightarrow \mathbb{R}$ . (The expression of $\eta$ as function of an element $\in M-\gamma$ remain undefined.) let $\gamma=\{(x, f(x)), f:\mathbb{R}^2\rightarrow\mathbb{R}\}$ , we can find the 1-form $df: ({x^1}'(t), {x^2}'(t))dt\mapsto \frac{df}{dt}dt$ , i.e. $dx\mapsto df(x), T\mathbb{R}^2\rightarrow T\mathbb{R}$ ,
where $$\frac{df}{dt}dt=\frac{\partial f}{\partial {x^1}}{x^1}'(t)dt+\frac{\partial f}{\partial {x^2}}{x^2}'(t)dt=\frac{df}{dt}dt=\frac{\partial f}{\partial {x^1}}d{x^1}+\frac{\partial f}{\partial {x^2}}d{x^2}.$$ We can find another one form $dg: dx\mapsto \sqrt{(df(x))^2+(dx)^2}, T\mathbb{R}^2\rightarrow T\mathbb{R}$ , (here the square is norm's square).
which is the derivative of $g: x\mapsto \int_0^t \sqrt{(df(x(\tau)))^2+(dx(\tau))^2}, \ \mathbb{R}^2\rightarrow\mathbb{R}$ ,
where $$\int_0^t \sqrt{(df(x(\tau)))^2+(dx(\tau))^2}=\int_0^t \sqrt{(\frac{\partial f}{\partial {x^1}}{x^1}'(\tau)d\tau+\frac{\partial f}{\partial {x^2}}{x^2}'(\tau)d\tau)^2+({x^1}'(\tau), {x^2}'(\tau))d\tau)^2}={L_{\gamma(\tau)}}|_0^t.$$ (The expression of $g$ as function of an element $\in \mathbb{R}^2-\{x|\ (x,f(x))\in \gamma\}$ remain undefined.)
So we have two 'measure' functions $\eta, \ g$ for curve length with different domain, whose derivatives are 1-forms. New question : It seems there is an error: though $dg$ is 'integrable', it seems not to be a linear function of the vector ( $dx^1, dx^2$ ), for the sum under $\sqrt{}$ has $\Delta=-4(f_{x_1}^2+f_{x_2}^2+1)\neq 0$ . This state of 'being not 1-form but still integrable' seems weird. Is that common?","['differential-forms', 'differential-geometry']"
3771211,Probability that new baby born is a boy in a nursery,"There are $2$ boys and unknown number of girls in a nursery. A new baby is just born inside the room. We pick randomly a baby from the room, it turns out that the baby is a boy. What is the probability that the new baby just born is a boy? We can solve this using Bayes' rule, as follows, $P(\text{new baby is boy} | \text{picked a boy})$ . We end up getting $\frac{3}{5}$ . Why is this answer intuitively not dependent on the number of girls in the nursery?",['probability']
3771256,The image of a morphism between two varieties,"By varieties I mean any of affine varieties, quasi-affine varieties, projective varieties or quasi-projective varieties over an algebraic closed field $k$ , where for affine varieties, I mean the classical definition of irreducible Zariski closed subset of $k^n$ as in the first Chapter of Hartshorne. Is it true that, if $f:X \rightarrow Y$ is a morphism between varieties, then $f(X)$ is a finite union of locally closed sets? I asked this because Chevalley's theorem suggests that, if $f:X \rightarrow Y$ is a morphism of finite type of noetherian schemes, then $f(X)$ is finite union of locally closed sets. Does this theorem imply that the above question is true? Also I notice that the main theorem of elimination theory seems to suggest that when $X$ is a projective variety, $f$ is necessarily a closed map and $f(X)$ is closed, is this true? However, in general, a morphism of varieties does not have to be an open or closed map, for example, consider $f:A^1 \rightarrow P^2$ , where $x$ is sent to $(x,1,0)$ , then $f(X)$ is neither open nor closed. Notice that in this example, $f(X)$ is locally closed.","['general-topology', 'algebraic-geometry', 'commutative-algebra']"
3771286,Is $X$ a Borel subset of $\beta X$?,"Consider a Tychonoff space $X$ and $\beta X$ its Stone- $\check{\rm C}$ ech compactification. I'm currently studying the existence of certain types of regular Borel measures on $X$ . Since it's much simpler to obtain regular Borel meaures for a compact, I'd like to obtain them for $\beta X$ , and then consider the restriction to $X$ . To do so, I'd like to know whether $X$ is a Borel subset of $\beta X$ .","['borel-sets', 'general-topology', 'borel-measures']"
3771292,Prove that the circumference of an ellipse is given by this infinite series,"Prove that the circumference of an ellipse is given by : $$2\pi a\left[1-\sum_{n=1}^{\infty}\left(\frac{\left(2n-1\right)!!}{\left(2n\right)!!}\right)^{2}\frac{e^{2n}}{2n-1}\right]$$ The parametric of an ellipse is : $$x=a\cos(\theta)$$ $$y=b\sin(\theta)$$ The Circumference of parametric curve can be computed via: $$\int_{\alpha}^{\beta}\ \sqrt{\left(\frac{dx}{dt}\right)^{2}+\left(\frac{dy}{dt}\right)^{2}}d\theta$$ Assuming the curve is not self-intersecting over the given interval. Using this we see that the circumference of an ellipse is: $$4\int_{0}^{\frac{\pi}{2}}\ \sqrt{a^{2}-\left(a^{2}-b^{2}\right)\cos^{2}\left(\theta\right)}d\theta$$ Assuming $a>b$ ,then $e=\frac{c}{a}=\frac{\sqrt{a^{2}-b^{2}}}{a}$ follows the integral is : $$4a\int_{0}^{\frac{\pi}{2}}\sqrt{1-e^{2}\cos^{2}\left(\theta\right)}=4a\int_{0}^{\frac{\pi}{2}}\sqrt{1-e^{2}\sin^{2}\left(\theta\right)}$$ It's well-known that : $$\sqrt{1-x}=-\sum_{n=0}^{\infty}\binom{2n}{n}\frac{x^{n}}{4^{n}\left(2n-1\right)}$$ Which is convergent for $\left|x\right|<1$ . Since $0<e^{2}\sin^{2}\left(\theta\right)<1$ ,hence the integral maybe written as: $$4a\left[\int_{0}^{\frac{\pi}{2}}d\theta+\int_{0}^{\frac{\pi}{2}}-\sum_{n=1}^{\infty}\binom{2n}{n}\frac{e^{2n}\sin^{2n}\left(\theta\right)}{4^{n}\left(2n-1\right)}d\theta\right]$$ Fubini/Tonelli theorems implies that the integral is indeed: $$4a\left[\frac{\pi}{2}-\sum_{n=1}^{\infty}\binom{2n}{n}\frac{e^{2n}}{4^{n}\left(2n-1\right)}\int_{0}^{\frac{\pi}{2}}\sin^{2n}\left(\theta\right)d\theta\right]$$ From : $$\int_{0}^{\frac{\pi}{2}}\sin^{2n}\left(\theta\right)d\theta=\frac{\pi}{2}\prod_{k=1}^{n}\frac{2k-1}{2k}\tag{$n \in \mathbb N^+$}$$ Then integral transforms to: $$2\pi a\left[1-\sum_{n=1}^{\infty}\binom{2n}{n}\frac{e^{2n}}{4^{n}\left(2n-1\right)}\prod_{k=1}^{n}\frac{2k-1}{2k}\right]$$ $$2\pi a\left[1-\sum_{n=1}^{\infty}\frac{\left(2n-1\right)!!}{\left(n!\right)^{2}4^{n}}\frac{e^{2n}}{2n-1}\right]$$ But how to finish ?","['infinite-product', 'binomial-coefficients', 'sequences-and-series']"
3771294,Wasserstein Distance `Lifts' the Underlying Metric.,"Let $\mathcal{X},\mathcal{Y}\subset \mathbb{R}^d$ and $\mu,\nu$ Borel probability measures on $\mathcal{X},\mathcal{Y}$ respectively. The Wasserstein-2 distance is defined as $$
W_2(\mu,\nu):=\inf_{\pi\in\Pi(\mu,\nu)}\Big(\int_{\mathcal{X} \times \mathcal{Y}}|x-y|^2d\pi(x,y)\Big)^{1/2}. $$ Where $\Pi(\mu,\nu)$ is the space of all joint distributions with marginals $\mu,\nu$ . Recall $W_2$ ( on the space of borel probability measures with finite second moments ) satisfies the properties of a metric. Furthermore a sequence of probability measures converges in this metric if and only if they converge weakly and their second moments converge. $\underline{Question :}$ I have heard people say that the Wasserstein ` lifts' the underlying metric. Are they referring to anything  specific, or is this just a fancy way to say what I wrote above?","['measure-theory', 'probability-theory', 'metric-spaces']"
3771327,Show that $\sum_{n=1}^{\infty} \frac{(-1)^{n}}{n^{2}}\cos nx=\frac{1}{12}(3x^{2}-\pi^2)$ for $-\pi \leq x \leq \pi $,"How do we show that for $-\pi \leq x \leq \pi $ , $$\sum_{n=1}^{\infty} \frac{(-1)^{n}}{n^{2}}\cos nx=\frac{1}{12}(3x^{2}-\pi^2)$$ I know that without $(-1)^{n}$ term, the series converges to $\frac{x^{2}}{4}-\frac{\pi x }{2}+\frac{\pi^{2}}{6}$ for $x$ in $[0,2\pi]$ .  But I'm not sure how to find the sum if there is a $(-1)^{n}$ term there. Thank you for your help!","['fourier-analysis', 'ordinary-differential-equations', 'partial-differential-equations', 'fourier-series', 'sequences-and-series']"
3771330,Pullback in the category of graphs,"Consider the category of (undirected) multigraphs (possibly with loops) and multigraph homomorphisms.
What are pullbacks in such a category? Is there an informal, colloquial and intuitive way to describe them? According to the definition of pullback , given the multigraphs $G_1 = (V_1, E_1, r_1)$ , $G_2 = (V_2, E_2, r_2)$ and $G$ and two multigraph morphisms $h_1 \colon G_1 \to G$ and $h_2 \colon G_2 \to G$ , the pullback of $h_1$ and $h_2$ exists and (I guess) should be a multigraph $G'$ whose vertices are couples $(v_1,v_2) \in V_1 \times V_2$ and whose edges are couples $(e_1, e_2) \in E_1 \times E_2$ such that their components are identified via $h_1$ and $h_2$ , i.e. $h_{1_V}(v_1) = h_{2_V}(v_2)$ and $h_{1_E}(e_1) = h_{2_E}(e_2)$ . But what does it mean intuitively? What does $G'$ look like? It seems to me that $G'$ sounds like the ""minimal"" multigraph ""compatible"" with $h_1$ and $h_2$ , but I am not sure this informal explanation makes sense. I guess I can find more information in the reference suggested in the accepted answer of this question , but I cannot access it. Context . An (undirected) multigraph (possibly with loops) is a triple $G = (V,E,r)$ where $V$ is the set of vertices, $E$ is the set of edges, and $r \colon E \to \{ \{v,w\} \mid v,w \in V\}$ associates every edge with its two endpoints (possibly they coincide). Given two multigraphs $G = (V, E, r)$ and $G' = (V', E', r')$ , a multigraph homomorphism $h \colon G \to G'$ is a couple $h = (h_V \colon V \to V', h_E \colon E \to E')$ of functions that ""preserve edges"", i.e. such that if $r(e) = \{v,w\}$ then $r'(h_E(e)) = \{h_V(v), h_V(w)\}$ .","['graph-theory', 'category-theory', 'morphism', 'discrete-mathematics', 'pullback']"
3771358,Are algebraic varieties strictly more general than (differentiable) manifolds?,"I have read that every non-singular algebraic variety is a smooth manifold. However, I was wondering if every smooth manifold can be expressed as a non-singular algebraic variety, or even just a general algebraic variety; such that algebraic varieties are a strict generalization of manifolds. If not, does not restricting algebraic varieties to be defined in terms of polynomial equations allow for a generalization? (Any references on the relation of varieties and manifolds, and any practical use of such a relation, would be appreciated, as I'm new to algebraic geometry. Thanks!)","['algebraic-geometry', 'smooth-manifolds', 'differential-geometry']"
3771388,How rare is it to get a $8$ in minesweeper? (Bruh reputation requirements),"I need help on this, ignore if it is already answered. Ok so today I was wondering, could you get an $8$ in minesweeper, and how rare it is? All I know is that it will be rare. Very rare indeed. I don't really know how to say it. It is so annoying to be honest (tbh) . I also do not know what you'd need to answer, so it is complicated, well, because of that. If you are answering, it might be hard. Oh, I probably don't know but here is a predicted formula of how rare it is First, this variable.
p8 = How rare it is to get 8 mines forming a hole like below. X = empty / O = mine
O O O
O X O
O O O
Rc = How rare it is to randomly click inside a patch that has minesweepers in all the directions you look, using the same
example as the square mines forming a hole in the middle (This means there is no middle mine therefore)
And then, the predicted formula below. It isn't advanced so you could make a better formula. It would please me.
p8 ÷ Rc = 8r
Forgot to mention. 8r = formula result So yeah. Not much to explain because I'm new to stack exchange. Anyways, the end of this, probably.",['probability']
3771407,Prove that $\sum_{n=1}^{\infty }\left ( \frac {\sin((2n-1)x)}{(2n-1)x)}\right )^k \frac{(-1)^{n-1}}{2n-1}=\frac π 4$ for $0\lt x\lt \frac \pi {2k} $,"Question :- Prove that $$
\sum_{ n =1}^{\infty }
\left\{\frac{\sin\left(\left[2n - 1\right]x\right)}
{\left(2n - 1\right)x}\right\}^{k}\
\frac{\left(-1\right)^{n - 1}}{2n - 1} = \frac{π}{4}
\qquad\mbox{for}\quad 0\lt x\lt \frac{\pi}{2k}
$$ While reading some papers , I came across this series.Unfortunately I do not have any link to the website since I took screenshot of It few months back . The Author claims that the above series is true for $0\lt x\lt \pi/\left(2k\right)$ . However he does not provide any mathematical proof instead he calculates the sum for different $x$ and $k$ like for $k = 100$ and $x = \pi/200$ the above sum up to $50$ terms is $$
0.78539 81633 97448 30961 55824
$$ which is very close to $\pi/4$ . I verified It myself for $k = 1$ . Actually the author is working on various variations of the
Gregory-Leibniz series and series of form $$
\frac{\sin\left(\mathrm{f}\left(x\right)\right)}
{\mathrm{g}\left(x\right)}
\quad\mbox{and}\quad 
\frac{\cos\left(\mathrm{f}\left(x\right)\right)}{\mathrm{g}\left(x\right)}
$$ ${\tt Mathematica}$ evaluates the series in terms of Lerch transcendent $\Phi$ function . I couldn't find any way to prove the  given series. Thank you for your help !!.","['integration', 'real-analysis', 'complex-analysis', 'calculus', 'sequences-and-series']"
3771487,Explicit description of $\mathcal{O}_{\Bbb{P}^1}(-1)$ as a line bundle,"I understand the construction of $\mathcal{O}_{\Bbb{P}^1}(-1)$ as a sheaf on $\Bbb{P}_\Bbb{C}^1$ , but I'm trying to understand how exactly does this define a line bundle and why people call this the ""tautological line bundle"". Following the suggestion of ""tautological"", my first idea was to define the map: \begin{align*}
\pi:\Bbb{A}^2&\to\Bbb{P}^1\\
(x_0,x_1)&\mapsto (x_0:x_1) 
\end{align*} whose fibers are clearly lines. Now, I can neither see how to define the trivialization maps nor how this relates to the sheaf $\mathcal{O}_{\Bbb{P}^1}(-1)$ , so probably I'm on the wrong path. I'm having a hard time trying to come up with different ideas, because I don't even know how to find a variety $X$ so that $\pi:X\to\Bbb{P}^1$ is the line bundle I'm looking for.","['algebraic-geometry', 'line-bundles', 'quasicoherent-sheaves']"
3771502,"How to solve for a specific gradient in an implicit relationship, when no points are known?","The problem Consider the following relation: $$x^2-3xy+y^2=7$$ I'm struggling with what is essentially the following task: Find all coordinates of all points where the gradient of the tangent of the curve is ${2\over3}$ . Using implicit differentiation, I arrive at the following derivative of $y$ wrt $x$ : $${\text dy\over\text dx}=\frac{3y-2x}{2y-3x}$$ This is somewhat of a mindbender for me, since it looks like the derivative itself depends on both $x$ and $y$ . How can this be? How does one work with this? Even more daunting is the task of solving $\frac{\text dy}{\text dx}=\frac{2}{3}$ . The equation cannot be solved as we have a single, two-variable equation: $$2(2y-3x)=3(3y-2x)$$ Sadder yet is that by my logic, there is an infinite number of solutions ( $xy$ pairs) that satisfy this. Unfortunately, graphing this relation gives the solution set $(x,0)$ where $x\inℝ$ . The correct answer is $(\sqrt7,0)\cup(-\sqrt7,0)$ . My questions How do I work with a derivative which depends on both $x$ and $y$ , and conceptually, how is that even possible? Why is it that graphing the solutions of $2(2y-3x)=3(3y-2x)$ did not work and gave erroneous solutions? How is the answer $(\sqrt7,0)\cup(-\sqrt7,0)$ obtained? Thank you very much :)","['calculus', 'implicit-differentiation', 'derivatives']"
3771562,"Prove or disprove the statement: For all real numbers $x,y$, $⌊xy⌋=⌊x⌋⌈y⌉$. What's wrong with my counter example?","Prove or disprove the statement: For all real numbers $x,y$ , $⌊xy⌋=⌊x⌋⌈y⌉$ . I used the counter example $x = 1.9$ , $y = 1.9$ . $⌊1.9 \cdot 1.9⌋ =  3$ $⌊1.9⌋⌈1.9⌉ = (1)(2) = 2$ But the professor said that this statement was actually true and I got 0 marks. Does anyone know what was wrong with my counter example?",['discrete-mathematics']
3771576,The minimizers of energy and length of a curve,"Assume S is a regular surface with a Riemannian metric $g$ . The energy of a curve $c: [0,a] \to S$ is defined as : $$ E(c) = \frac{1}{2} \int_0^a g_{c(t)}(\dot c(t),\dot c(t))\mathsf{dt} $$ This is quite similar to the definition of the length in terms of the defining functions i.e.: $$L(c) = \int_0^a \sqrt{g_{c(t)} (\dot c(t), \dot c(t) )}\mathsf{dt}$$ The Lemma 2.3 in Chapter 9 of Riemannian Geometry by Do Carmo is: About the formula $$aE(\gamma)=(L(\gamma))^2\leq (L(c))^2\leq aE(c),$$ the author has proved the inequality: $$(L(c))^2\leq aE(c)$$ using the Schwarz inequality. The question is: why $aE(\gamma)=(L(\gamma))^2$ ?","['geodesic', 'smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
3771643,"Prove $D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R})) : \text{deg}(D(p)) = \text{deg}(p) - 1$ is surjective","Suppose $D \in \mathcal{L}(\mathcal{P}(\mathbf{R}),\mathcal{P}(\mathbf{R}))$ is such that $\deg(D(p)) = \deg(p) - 1$ for every nonconstant polynomial $p \in \mathcal{P}(\mathbf{R})$ . Prove that $D$ is surjective. I have attempted an answer, however, I think it is incorrect: We can redefine this as a linear map between two finite dimentional vector spaces: $$
D \in \mathcal{L}(\mathcal{P}_m(\mathbf{R}),\mathcal{P}_{m-1}(\mathbf{R}))
$$ for $m > 0$ . Let $(1, x, x^2 \ldots, x^{m-1})$ be a basis for $\mathcal{P}_{m-1}$ . We can extend this to a basis of $\mathcal{P}_m$ because $\mathcal{P}_{m-1} \subset \mathcal{P}_m$ : $$(1, x, x^2 \ldots, x^{m-1}, x^m)\text{.}$$ Then define $D$ : \begin{align}
D(x^i) &= x^i, i = 0, \ldots, m - 1 \\
D(x^m) &= 0
\end{align} Clearly then, $\text{range}(D) = \mathcal{P}_{m-1}$ , as $(1, x, x^2 \ldots, x^{m-1})$ is a basis for $\text{range}(D)$ . Hence $D$ is surjective. The reason I think this answer is incorrect, is because I have chosen my own definition of $D$ , not proved it for an arbitrary $D$ . However, for similar questions, I often see the answers choose a specific mapping, and I struggle to know when that is acceptable and when it isn't.","['linear-algebra', 'linear-transformations']"
3771649,Is $\oint_{\left | z \right |=2} \frac{e^{\frac{1}{z}}}{z(z^{2}+1))}dz$ equal to zero?,"I based my analysis on the fact that the only residue that's outside the curve is the reside in $\infty$ that's equal to zero, so all the other resides inside the curves must add to zero too. Am I correct?","['complex-analysis', 'contour-integration', 'residue-calculus']"
3771659,"For i.i.d random variables $X$ and $Y$, is $E[X \mid \sigma(X+Y)] = \frac{X+Y}{2}$?","Let $X$ and $Y$ be i.i.d. random variables; we want to calculate the conditional expectation with respect to the $\sigma$ -algebra generated by $X+Y$ : $$E [X \mid \sigma(X+Y)]$$ Now, generally for random variables $X, Y \in L^1$ , if $$E[X1_A(X)1_B(Y)] = E[Y1_A(Y)1_B(X)]\ \quad (A, B \in \mathcal{B}(\mathbb{R}))$$ then $$E[X1_C(X+Y)] = E[Y1_C(X+Y)]\ \quad (C \in \mathcal{B}(\mathbb{R}))$$ So here is my solution so far: the above holds for i.i.d. random variables $X, Y$ , so $$E[X \mid \sigma(X+Y)] = E[Y \mid \sigma(X+Y)]$$ , and then we have $$E[X \mid \sigma(X + Y) ] = \frac{1}{2} E[X + Y \mid \sigma(X + Y) ] = \frac{X+Y}{2}$$ I feel like I am missing something here...","['conditional-expectation', 'measure-theory', 'probability-theory']"
3771660,"A step in the proof of Fubini theorem (Theorem 2.36, Folland)","This is a first case of the proof of the Fubini-Tonelli theorem, given in Folland's Real Analysis. I'm confused with the line underlined in blue at the end (namely, 'the preceding argument applies to' part): $\newcommand{\blueunderline}[1]{\color{blue}{\underline{\color{black}{\text{#1}}}}}$ 2.36 Theorem. Suppose $(X, \mathcal{M}, \mu)$ and $(Y, \mathcal{N}, \nu)$ are $\sigma$ -finite measure spaces. If $E \in \mathcal{M} \otimes \mathcal{N},$ then the functions $x \mapsto \nu\left(E_{x}\right)$ and $y \mapsto \mu\left(E^{y}\right)$ are measurable on $X$ and $Y,$ respectively, and $$
\mu \times \nu(E)=\int \nu\left(E_{x}\right) d \mu(x)=\int \mu\left(E^{y}\right) d \nu(y)
$$ Proof. First suppose that $\mu$ and $\nu$ are finite, and let $\mathcal{C}$ be the set of all $E \in$ $\mathcal{M} \otimes \mathcal{N}$ for which the conclusions of the theorem are true. If $E=A \times B$ , then $\nu\left(E_{x}\right)=\chi_{A}(x) \nu(B)$ and $\mu\left(E^{y}\right)=\mu(A) \chi_{B}(y),$ so clearly $E \in \mathcal{C} .$ By additivity
it follows that finite disjoint unions of rectangles are in $\mathcal{C},$ so by Lemma 2.35 it will suffice to show that $\mathcal{C}$ is a monotone class. If $\left\{E_{n}\right\}$ is an increasing sequence in $\mathcal{C}$ and $E=\bigcup_{1}^{\infty} E_{n},$ then the functions $f_{n}(y)=\mu\left(\left(E_{n}\right)^{y}\right)$ are measurable and increase pointwise to $f(y)=\mu\left(E^{y}\right) .$ Hence $f$ is measurable, and by the monotone convergence theorem, $$
\int \mu\left(E^{y}\right) d \nu(y)=\lim \int \mu\left(\left(E_{n}\right)^{y}\right) d \nu(y)=\lim \mu \times \nu\left(E_{n}\right)=\mu \times \nu(E).
$$ Likewise $\mu \times \nu(E)=\int \nu\left(E_{x}\right) d \mu(x),$ so $E \in \mathcal{C} .$ Similarly, if $\left\{E_{n}\right\}$ is a decreasing sequence in $\mathcal C$ and $\bigcap_{1}^{\infty} E_{n},$ the function $y \mapsto \mu\left(\left(E_{1}\right)^{y}\right)$ is in $L^{1}(\nu)$ because $\mu\left(\left(E_{1}\right)^{y}\right) \leq \mu(X)<\infty$ and $\nu(Y)<\infty,$ so the dominated convergence theorem can be applied to show that $E \in \mathcal{C}$ . Thus $\mathcal{C}$ is a monotone class, and the proof is complete for the case of finite measure spaces. Finally, if $\mu$ and $\nu$ are $\sigma$ -finite, we can write $X \times Y$ as the union of an increasing sequence $\left\{X_{j} \times Y_{j}\right\}$ of rectangles of finite measure. $\blueunderline{If $E \in \mathcal{M} \otimes \mathcal{N},$}$ $\blueunderline{the preceding argument}$ $\blueunderline{applies to}$ $\blueunderline{$E \cap\left(X_{j} \times Y_{j}\right)$}$ for each $j$ to give $$\mu \times \nu\left(E \cap\left(X_{j} \times Y_{j}\right)\right)=\int \chi_{X_{j}}(x) \nu\left(E_{x} \cap Y_{j}\right) d \mu(x)=\int \chi_{Y_{j}}(y) \mu\left(E^{y} \cap X_{j}\right) d \nu(y)$$ and a final application of the monotone convergence theorem then yields the desired result. $\blacksquare$ Transcribed from this screenshot This was my initial thinking: Assume that $\nu,\mu$ are $\sigma$ -finite. As $X,Y$ have exhausting sequences formed by elements of $\mathcal{M}, \mathcal{N}$ , we can write $X \times Y$ as the union of an increasing sequence $X_j \times Y_j$ of rectangles of finite measure when measured by $\mu \times \nu$ . Let $E \in \mathcal{M} \times \mathcal{N}$ . We know that the restriction to $X_j \times Y_j \in \mathcal{M} \times \mathcal{N}$ is still a sigma algebra; in other words, from $(X \times Y, \mathcal{M} \otimes \mathcal{N}, \mu \times \nu)$ is a measure space, we know that the restriction $(X_j \times Y_j, (\mathcal{M} \otimes \mathcal{N}) \cap (X_j \times Y_j), (\mu\times\nu)|_{(X_j \times Y_j)}(\cdot) = \mu \times \nu (\cdot \cap X_j \times Y_j))$ is a measure space. This space is clearly a finite measure space. We apply the preceding result and conclude. However, I realized there is no reason for an integral in a restricted measure space to equal in integral in a larger measure space. Furthermore, thinking in this way causes some issues when checking for the conditions to use the monotone convergence theorem. Can anyone help clarify how the 'preceding argument applies to $E \cap (X_i \times Y_i)$ ' to give the result? Thanks.","['measure-theory', 'solution-verification', 'probability-theory', 'real-analysis']"
3771671,Advantages of Each Coordinate System,"I am currently learning about the spherical coordinate system in class, but I do not know its advantages or even if it is advantageous in using this coordinate system over another. I am very comfortable in using the rectangular coordinate system and the cylindrical coordinate system (polar coordinate system but just in 3D), as the rectangular coordinate system is just the cartesian coordinate system with another dimension and the cylindrical coordinate system is just the polar coordinate system with an additional dimension. However, the concept of spherical coordinates come out of nowhere (that I know of) and I am unable to see its advantages. For example, if wanting to calculate an integral in the first octant, you can just restrict to $x>0$ , $y>0$ , and $z>0$ for the rectangular coordinate system. And for cylindrical coordinates, you can restrict $z>0$ , $0<\theta<\frac{\pi}{2}$ , and $r$ its corresponding boundary conditions. My question is are there ever cases when using spherical coordinates are more intuitive than using cylindrical or rectangular coordinates?","['cylindrical-coordinates', 'coordinate-systems', 'multivariable-calculus', 'spherical-coordinates']"
3771694,Uncountable sets - Why is the following proof false?,"Let $S$ be any subset of the natural numbers. Then the sum $$
\sum_{n \in S} \frac{1}{2^n}
$$ converges against a unique value for each subset $S$ .
This sum yields a computable number because it is possible to compute it digit by digit. Therefore, these sums map every subset of $\mathbb{N}$ to a unique computable number. This is a contradiction because the set of all subsets of $\mathbb{N}$ is uncountable but the computable numbers are countable. Where is the error here?","['elementary-set-theory', 'computational-mathematics']"
3771719,A question regarding matrices and minimal polynomials.,"I am trying to answer the following question: Let $A$ be a $3\times 3$ real matrix and $A^4=I$ and $A\neq \pm I$ ,then does it imply $A^2+I=O$ ? Attempt: Since $A$ is $3\times 3$ real matrix,it has at least one eigenvalue $c\in \mathbb R$ .Then $(x-c)|m(x)$ ,where $m$ is the minimal polynomial of $A$ .Now, $A^2+I=O$ implies $x^2+1$ annihilates $A$ and hence $m(x)| x^2+1$ but then $(x-c)$ must be a factor of $(x^2+1)$ and hence $c$ must be a root of $x^2+1$ which is not possible as $c\in \mathbb R$ . Is this solution alright?","['eigenvalues-eigenvectors', 'matrices', 'minimal-polynomials', 'solution-verification', 'linear-algebra']"
3771778,Simplify the radical $\sqrt{x-\sqrt{x+\sqrt{x-...}}}$,"I need help simplifying the radical $$y=\sqrt{x-\sqrt{x+\sqrt{x-...}}}$$ The above expression can be rewritten as $$y=\sqrt{x-\sqrt{x+y}}$$ Squaring on both sides, I get $$y^2=x-\sqrt{x+y}$$ Rearranging terms and squaring again yields $$x^2+y^4-2xy^2=x+y$$ At this point, deriving an expression for $y$ , completely independent of $x$ does not seem possible. This is the only approach to solving radicals which I'm aware of. Any hints to simplify this expression further/simplify it with a different approach will be appreciated. EDIT: Solving the above quartic expression for $y$ on Wolfram Alpha , I got 4 possible solutions","['nested-radicals', 'algebra-precalculus']"
3771781,If $p$ and $q$ are primes such that $q \mid {\frac{x^p-1}{x-1}}$ then prove that $q\equiv 1 \pmod{p}$ or $q\equiv 0 \pmod{p}$.,"QUESTION: If $p$ and $q$ are primes such that $$q \mid {\frac{x^p-1}{x-1}} ,  (x\in\Bbb{N}, x>1)$$ then prove that $q\equiv 1 \pmod{p}$ or $q\equiv 0 \pmod{p}$ . MY ANSWER: I came across this lemma, but couldn't prove the second part properly. Here's what I did - By Fermat's Little Theorem we know that $x^p\equiv{x}\pmod{p}$ . Therefore, $$\frac{x^7-1}{x-1}\equiv\frac{x-1}{x-1}=1\pmod{7}$$ Therefore, $q\equiv{1}\pmod{7}$ . Now, I cannot prove that $q\equiv{0}\pmod{7}$ . Not simultaneously ofcourse, I know that 😅.. Here's my try - We can write the above equation as $$x^6+x^5+x^4+x^3+x^2+x+1$$ But what after this? Even if I chose $q=7$ , it does not divide the above equation for all values of $x$ . Say $x=7$ , then the equation can be rewritten as $$7^6+7^5+7^4+7^3+7^2+7+1$$ and $$7\nmid{7^6+7^5+7^4+7^3+7^2+7+1}$$ So, how do I rigorously prove that $q\equiv{0}\pmod{p}$ ? Or, for which cases is this true? Any help will be much appreciated. Thank you :) EDIT: Terrible Mistake :P The first proof of $q\equiv{1}\pmod{7}$ is wrong. So, now I am left with a full question to be proved °_°","['number-theory', 'modular-arithmetic', 'elementary-number-theory', 'prime-numbers']"
3771816,"Solve the integral $\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x$","As an extension to my discussion in one of the answers to my previous question on simplifying the integrand, I'd like to evaluate the following integral: $$\int_1^3\!\sqrt{x-\sqrt{x+\sqrt{x-...}}}\,\mathrm{d}x$$ The above radical, when solved, yields 4 possible solutions: $$1) y=\frac{1}{2}(-\sqrt{4x-3}-1)\\2)y=\frac{1}{2}(\sqrt{4x-3}-1)\\3)y=\frac{1}{2}(1-\sqrt{4x+1})\\4)y=\frac{1}{2}(\sqrt{4x+1}+1)$$ Definitely, only one of these solutions has to be considered as an integrand. Since the limits of integration are positive(and square roots are involved), I suspect that the integrand must be positive as a whole. Thus, solutions $(1$ ) and $(3)$ are ruled out. However, I cannot decide which expression amongst $(2)$ and $(4)$ is legitimate. It was brought to my attention that this involves the notion of convergence, a concept I'm not yet completely familiar with(I have a naive understanding of convergence in infinite geometric series). Thus, I'd like to know: Which of the above 4 solutions to the radical is legitimate for solving this integral, and why?","['integration', 'calculus', 'real-analysis']"
3771913,Are powers of a natural number distributed widely in the residue system?,"I encountered the following while solving another problem and need some help. Let $p,d$ be relatively prime natural numbers. I think that for many of $(p,d)$ tuples, numbers in set $\mathbb{N}_d = \{0,1,2,\cdots,d-1\}$ which are residue of one of the $1, p, p^2, \cdots$ modulo $d$ are kind of widely, uniformly distributed.
More precisely, let's call $n\in\mathbb{N}_d$ which there exists natural number $u$ such that $n \equiv p^u\;(\text{mod} \;d)$ , a good number. Are good numbers 'widely' distributed? What I actually need for the original problem is that, there are only few $(p,d,c)$ such that for the smallest non trivial divisor of $p$ , let's say $t$ , some numbers called ' $c$ -good numbers', which are $n\in\mathbb{N}_d$ such that there exists $u$ with $n\equiv cp^u\;(\text{mod }d)$ , are all smaller than the rational number $d/t$ . This was what I mean by 'widely distributed'. Are there some known theorems or results that imply or mean the wide distribution of powers in the residue system, which may potentially be useful for the problem?","['number-theory', 'elementary-number-theory', 'exponentiation']"
3771943,$b^* a^* ab \leq \Vert a\Vert^2 b^* b$ in a $C^*$-algebra.,"Let $A$ be a $C^*$ -algebra and $a,b \in A$ . In a proof I'm reading the following is claimed: $b^* a^* ab \leq \Vert a\Vert^2 b^* b$ . I want to understand this: Here is my reasoning: we view $A \subseteq \tilde{A}$ with $\tilde{A}$ the unitisation of $A$ . Then we know that $a^* a \leq \Vert a^* a \Vert 1$ since this holds in every unital $C^*$ -algebra (by a Gelfand-representation argument). Then $$b^* a^*a b \leq b^* \Vert a^* a \Vert 1 b = \Vert a \Vert ^2 b^* b$$ Is the above correct? I find arguments with unitisations always a bit tricky.","['c-star-algebras', 'solution-verification', 'functional-analysis']"
3771956,How to evaluate $ \:\sum _{n=3}^{\infty \:}\frac{4n^2-1}{n!}\:\: $?,"I am trying to evaluate: $$ \:\sum _{n=3}^{\infty \:}\:\:\frac{4n^2-1}{n!}\:\: \quad (1)$$ My attempt: $$ \:\sum _{n=3}^{\infty \:}\:\:\frac{4n^2-1}{n!}\:\: \quad  = 4\sum _{n=0}^{\infty \:} \frac{(n+3)^2}{(n+3)!} + \sum _{n=0}^{\infty \:} \frac{1}{(n+3)!}$$ The last form is very similar to the expotential series, but I can't get it from here. Any ideas?","['convergence-divergence', 'sequences-and-series']"
3771968,"$f\left( x \right) = {x^3} + x$, then $\int\limits_1^2 {f\left( x \right)dx} + 2\int\limits_1^5 {{f^{ - 1}}\left( {2x} \right)dx} $","If $f\left( x \right) = {x^3} + x$ , then $$\int\limits_1^2 {f\left( x \right)dx}  + 2\int\limits_1^5 {{f^{ - 1}}\left( {2x} \right)dx} $$ is________. My approach is as follows: $$g = {f^{ - 1}} \Rightarrow g\left( x \right) = {f^{ - 1}}\left( x \right)$$ $$g\left( {2x} \right) = {f^{ - 1}}\left( {2x} \right)$$ $$2y = {8x^3} + 2x$$ $${f^{ - 1}}\left( {{x^3} + x} \right) = x$$ $$\int\limits_1^2 {f\left( x \right)dx}  + 2\int\limits_1^5 {{f^{ - 1}}\left( {2x} \right)dx} $$ I am not able to proceed further.","['integration', 'calculus', 'functions', 'definite-integrals']"
3771980,"Moving from $(0,0)$ to $(5,5)$ without right angles","In a Cartesian coordinate system, we can move from $(a,b)$ to $(a+1,b) , (a,b+1)$ and $(a+1,b+1)$ , but there must be no right angle occur if we draw lines during the move. In how many ways can we do that so that we start from $(0,0)$ and end at $(5,5)$ ? I found solution in AoPS but they just use brute-force method. Can anyone give some hints please. Thank you! Also, I think that this problem is AIME , but I don’t know the year.","['contest-math', 'combinatorics']"
3772016,Cutting a cuboid to fit in a hemisphere,"Today while making dinner consisting of instant noodles, I thought of the most ridiculous question I've ever asked this site. The Instant Noodle Problem Suppose you are a college student preparing one of those cuboid-shaped instant noodle packages. You plan on breaking the noodles such that each piece can be entirely submerged by boiling water (i.e; below the top of the bowl). The noodles have dimensions $2\times2\times\frac{1}{2}$ , thus having volume $$V_{n}=2$$ Meanwhile the bowl is a hemisphere of radius $1$ $$V_{b}=\frac{2\pi}{3}\approx2.094$$ Clearly, the bowl has just enough room to fit the noodles. Accordingly, What is the minimum planar-breaks required to fit the noodles in the bowl? I imagine this is a tough problem, perhaps playing out like the moving sofa problem . Hence I plan on rewarding a 250 point bounty to most efficient solution / tightest bounds or an answer with exemplary research.","['optimization', 'puzzle', 'geometry', 'volume']"
3772018,The advantage of Complex Differentiation and Inverse Function Theorem,"One interesting phenonmenon in complex analysis is the following, If $f:\mathbb C\to\mathbb C$ is complex differentiable at point $a$ ( $\equiv$ derivative is a spiral similarity), and a local homeomophism with inverse $g$ near $a$ , then $g$ is complex differentiable at point $b=f(a)$ . Its proof is as the page from Ahlfors' Complex Analysis , https://i.sstatic.net/UTdED.png Same argument applies to usual one variable differentiation and probably any normed field, since we are allowed to invert quotient before taking limits on their norm, hence showing an implcit linkage between analysis and algebra. However in multivariable calculus, one cannot invert quotient to prove similar theorem. By taking $f(x+h)=y+k, f(x)=y$ as in usual proof of inverse function theorem, one is required to show that for some real $\lambda>0$ , $|k|\ge \lambda |h|$ , so that
one can bound the usual quotient $\dfrac{|h-f'(x)^{-1}k|}{|k|}$ by $\dfrac{|k-f'(x)h|}{|h|}$ up to some multiplicative constant. This is far from inverting quotients. Is there any explicit explanation of this interplay between algebra and analysis? P.S. One interesting corollary found is that if a homeomorphism $f:U\to V$ , where both are subset of $\mathbb R^2$ , and has invertible differential at point $A$ , then its inverse is differentiable at point $f(A)$ . (by normalising function so that it is complex differentiable) P.S.2 Its generalisation (not verified): If $f:\mathbb C^n\to\mathbb C^n$ is local homeomorphism (from $U$ to $V$ ) and differentiable at $a\in U$ with invertible differential, then its local inverse is differentiable at $b$ . By this, if $f:\mathbb R^{2n}\to\mathbb R^{2n}$ satisfies similar condition, its inverse is differentiable at $b$ . P.S.3 Maybe an interesting question is whether one can define some algebraic structure on $\mathbb C^n$ like bicomplex number such that one can invert quotients for proof. (But it just need not be commutative, causing more problem.)","['complex-analysis', 'multivariable-calculus', 'abstract-algebra', 'derivatives', 'soft-question']"
3772024,Simplifying $\sum_{k=0}^{24}\binom{100}{4k}.\binom{100}{4k+2}$,"How to evaluate the following series: $$\sum_{k=0}^{24}\binom{100}{4k}\binom{100}{4k+2}$$ What I have tried : Considering expansion of $\displaystyle (1+x)^n=
\binom{n}0+ \binom n1 x + \binom n2 x^2+\cdots$ By this I can get easily the result : $$\sum_{k=0}^{100}\binom{100}{k}\binom{100}{100-k}=\binom{200}{100}$$ Which seems to be almost similar to $\displaystyle \sum_{k=0}^{24}\binom{100}{4k}\binom{100}{98-4k}$ . I think what I should do next is substitute $i$ and $-i$ and add equations. What I am not sure is which ones to add.
Any suggestion or other approaches? Much Appreciated.","['complex-analysis', 'combinations', 'binomial-coefficients', 'combinatorics']"
3772067,Expected number of coin tosses untill number of head exceeds number of tails?,"This problem is basically from MIT OCW probability section. A fair coin is flipped until the number of heads exceeds the number of tails. What is
the expected number of flips? I tried this question and landed with 25/12 as the expected number of flips required. I want to check if this is correct.","['expected-value', 'probability']"
3772073,Non-simplicity of Frobenius complements,"I'm reading a paper and it says the following theorem implies that the Frobenius complement of any finite Frobenius group is not a non-abelian simple group. ( Zassenhaus 1936 ) Let $G$ be a finite Frobenius group and $A$ be its Frobenius complement. Then the Sylow $p$ -subgroups of $A$ are cyclic for odd $p$ and are either cyclic or generalized quaternion for $p=2$ . I know that if $A$ has cyclic Sylow $2$ -subgroups or no Sylow $2$ -subgroups, then $G$ is solvable and hence not a non-abelian simple group. That follows from the fact that if all Sylow subgroups of a group are cyclic then this group is solvable. But I don't have any idea about how to deal with the case where the Sylow $2$ -subgroups are generalized quaternion. Maybe this question is easy and the reason why I'm stuck is that I'm not familiar with generalized quaternion groups. Any help is sincerely appreciated. Thank you!","['finite-groups', 'abstract-algebra', 'sylow-theory', 'group-theory', 'group-actions']"
3772102,What is the proof for $F(x)=\int_{a}^{x} e^{t^2}dt$ not being elementary?,"Simply as stated above: What is the proof, or how does one prove, $F(x)=\int_{a}^{b} e^{t^2}dt$ isn't elementary? All I know is that it can be proven , but I couldn't find a proof for it.","['integration', 'calculus']"
3772114,A question about the projection function,"Let $p:A\times B\to. A$ $p(x,y)=x$ The projection function $p$ is clearly subjective. I want to know if it is bijective? The projective function suspiciously looks like $1_A(x)=x$",['functions']
3772259,Linear algebra constructions on vector bundles,"There's a meta-theorem hanging around in the theory of vector bundles that ""Any canonical construction in linear algebra gives rise to a geometric version for holomorphic vector bundles."" (quote from Huybrecht's Complex Geometry ) In textbooks, this usually done hands on case-by-case.
I'm looking for a broader view. Which functors $\text{Vect}\to \text{Vect}$ extend to functors in the category of vector bundles over $X$ ? Here, an extension of a functor $F:\text{Vect}\to \text{Vect}$ is a functor $\bar F$ between vector bundles such that if $V$ is the typical fiber of $E$ then $F(V)$ is the typical fiber of $F(E)$ . A partial answer: a functor $\text{Vect}\to \text{Vect}$ is smooth if for every $V$ the induced map $F:GL(V)\to GL(FV)$ is smooth.
In this case, given cocycles $\psi_{ij}$ defining a bundle, the following cocycles defines an extension of $F$ : $$
U_i\cap U_j\xrightarrow{\psi_{ij}} GL(V)\xrightarrow{F} GL(FV)
$$ I believe the most common linear algebra constructions on bundles, such as tensors, sums and exterior powers, come from smooth functors. Are there non-smooth functors $\text{Vect}\to \text{Vect}$ that extend to vector bundles in the above sense? Can we characterize them? Shifting the categorical perspective, does regarding vector bundle as a locally free sheaves brings any insight here? Can these concepts be carried on to algebraic geometry?","['vector-bundles', 'linear-algebra', 'differential-geometry']"
3772260,Find the angle θ (all the circles are tangent),"In the following figure ABCD is a side square $\alpha$ , the points $P_0, P_1, P_2, P_3, Q_0, Q_1, Q_2, Q_3, X \ and \ Y$ are points of tangency, $BC \ and \ ZB$ are the diameters, respectively, of the blue and green semi-circles. Determine the angle $\theta$ Answer: $θ=67,5°$ There is a lot of homoteties, but I only could find that LK= $\frac{\sqrtα}{4}$ . I guess that $BP_1$ are diagonal of the square, but I don't know how to prove (or disprove) this. Can someone help me to solve this problem? Thanks for antetion. [Question image]","['euclidean-geometry', 'tangent-line', 'angle', 'circles', 'geometry']"
3772268,The Weil pairing for elliptic curves over the $\mathbb{C}$,"Let $E = \mathbb{C} / (\mathbb{Z} + \tau \mathbb{Z})$ be an elliptic curve over $\mathbb{C}$ .
Then how can I show that $e_n(1/N, \tau/N) = \exp(2 \pi i / n)$ ?
If we can show it, then the fundamental property of the Weil pairing shows the complete description of it. I know that this is the section 24 of Mumford's Abelian varieties.
But it's too hard for me. For the case of $\dim = 1$ , can I show it more easily?","['complex-geometry', 'algebraic-geometry', 'elliptic-curves']"
3772288,Convergence of recursively defined sequence,"I have the following problem: Suppose $a$ and $k$ are positive reals and $ a^2 > 2k $ .
Set $x_{0} = a$ and define $x_{n}=x_{n-1} + \frac{k}{x_{n-1}}$ for $n\geq1$ . Prove that $\lim_{n \to \infty}\frac{x_{n}}{\sqrt{n}}$ exists and determine its value. For reference this is a problem in Hardy and William's The Green Book of Mathematical Problems . I believe I have a proof of this and would appreciate if anyone could check for its correctness. I'd also appreciate if someone could give a simpler argument. Here is my proof: We show that $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}}=\sqrt{2k}$$ This will show the sequence converges to $\sqrt{2k}$ . From squaring the defining relation, we get $$x_{n}^2=x_{n-1}^2+2k+\frac{k^2}{x_{n-1}^2} > x_{n-1}^2 +2k$$ Applying this same estimate $n$ times gives us the lower bound $ x_{n}^2 > a^2+2nk$ , so that $x_{n} > \sqrt{2nk+a^2}>\sqrt{2k(n+1)}$ (*) We then get $$\liminf_{n \to \infty}\frac{x_{n}}{\sqrt{n}} \geq \liminf_{n \to \infty}\sqrt{2k\frac{n+1}{n}}=\sqrt{2k}$$ We'll now show $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k}$$ We first derive an upper bound on $ x_{n}-x_{n-1} $ . Applying the estimate (*), to $x_{n-1}$ gives us $$x_{n}= x_{n-1} + \frac{k}{x_{n-1}} < x_{n-1} + \frac{k}{\sqrt{2k(n-1)+a^2}} < x_{n-1} +\sqrt{\frac{k}{2n}}$$ where $a^2 > 2k$ was used. Thus, for $n\geq1$ , we get $$x_{n}-x_{n-1} < \sqrt{\frac{k}{2n}}$$ (**) Observe that by telescoping, we have $$x_{n}=a+\sum_{j=1}^{n}x_{j}-x_{j-1}$$ Applying the estimate (**) to each term in the summand gives $$\frac{x_{n}-a}{\sqrt{n}}=\frac{1}{\sqrt{n}}\sum_{j=1}^{n}x_{j}-x_{j-1}<\sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}}$$ Estimating this last term with an integral gives: $$\sqrt{\frac{k}{2n}}\sum_{j=1}^{n}\frac{1}{\sqrt{j}} < \sqrt{\frac{k}{2n}}(1+\int_{1}^{n}\frac{dx}{\sqrt{x}})=\sqrt{\frac{k}{2n}}(2\sqrt{n}-1)$$ Thus, we get $$\frac{x_{n}}{\sqrt{n}}<\frac{a}{\sqrt{n}}-\sqrt{\frac{k}{2n}}+\sqrt{2k}$$ Taking the limsup of both sides gives $$\limsup_{n \to \infty}\frac{x_{n}}{\sqrt{n}}\leq\sqrt{2k}$$ which completes the proof. Feedback and or corrections are much appreciated!","['calculus', 'sequences-and-series', 'real-analysis']"
3772324,Limit of $L^p$ norm is $L^\infty$ norm variation,"I'm familiar with the result that $$\lim_{p \to \infty} ||f||_p=||f||_\infty$$ when $f \in L^p([0,1])$ , but I've come a cross a variation of this fact that I'm having trouble showing. The assertion is that given $f \in L^\infty(\mathbb{R})$ $$\lim_{n \to \infty}\left(\int \frac{|f(x)|^n}{1+x^2} \, dx\right)^\frac{1}{n}=||f||_\infty$$ The function $\frac{1}{1+x^2}$ inside the integrand is what is tripping me up. I'm not sure how to deal with it in order to run the typical argument.","['integration', 'measure-theory', 'functional-analysis', 'real-analysis']"
3772356,Reference request: Pushforward in Cohomology,"Given a map $f:X\rightarrow Y$ between top. spaces, there are conditions such that $f$ defines a push-forward or Gysin-map / wrong-way-map in cohomology, that is a homomorphism $f_*: H^*(X)\rightarrow H^*(Y)$ . From what I have read so far, if $f:X\rightarrow Y$ is a fiber bundle of smooth manifolds, then one can define such a map via integration on fibers. More generally, if $f$ is proper, then one can use duality to find a pushforward. And in the algebraic context, proper maps between varieties define a pushforward of the respective Chow - rings, where one needs the notion of a degree of dominant morphisms. Currently I am interested in the algebraic topology side of things and would like to get enough background knowledge in order to compute pushforwards in simple situations. But the algebraic geometry situation is interesting for me as well. But I struggle to find a good introduction into the subject. Especially, I don't find any computational examples, which usually help me a lot. Can someone point me towards some literature?","['algebraic-geometry', 'homology-cohomology', 'algebraic-topology', 'reference-request']"
3772371,"Write the polynomial of degree $4$ with $x$ intercepts of $(\frac{1}{2},0), (6,0)$ and $(-2,0)$ and $y$ intercept of $(0,18)$.","Write the polynomial of degree $4$ with $x$ intercepts of $(\frac{1}{2},0), (6,0) $ and $ (-2,0)$ and $y$ intercept of $(0,18)$ .
The root ( $\frac{1}{2},0)$ has multiplicity $2$ . I am to write the factored form of the polynomial with the above information. I get: $f(x)=-6\big(x-\frac{1}{2}\big)^2(x+2)(x-6)$ Whereas the provided solution is: $f(x)=-\frac{3}{2}(2x-1)^2(x+2)(x-6)$ Here's my working: Write out in factored form: $f(x) = a\big(x-\frac{1}{2}\big)^2(x+2)(x-6)$ I know that $f(0)=18$ so: $$18 = a\big(-\frac{1}{2}\big)^2(2)(-6)$$ $$18 = a\big(\frac{1}{4}\big)(2)(-6)$$ $$18 = -3a$$ $$a = -6$$ Thus my answer: $f(x)=-6\big(x-\frac{1}{2}\big)^2(x+2)(x-6)$ Where did I go wrong and how can I arrive at: $f(x)=-\frac{3}{2}(2x-1)^2(x+2)(x-6)$ ?","['algebra-precalculus', 'roots', 'polynomials']"
3772376,Showing $P\left(\bigcap_{n=1}^{\infty} B_n\right)=1$ if $P(B_n)=1$ for every $n$,"Resnick - Probability path 2.11: Let $\{B_n, n\geq 1\}$ be events with $P(B_n)=1$ for every $n$ . Show that $$P\left(\bigcap_{n=1}^{\infty} B_n\right)=1$$ I was thinking to use a sequence such that it is equal to $\bigcap\limits_{n=1}^{\infty} B_n$ , for $n$ going to infinity, and then apply the continuity property of a probability measure. Although, since it is not mentioned about the limit of $B_n$ , neither if it is non increasing, I don't know how to proceed.
Any help would be appreciated. Thanks!","['probability-theory', 'probability']"
3772397,"How do I evaluate $\sum_{m,n\geq 1}\frac{1}{m^2n+n^2m+2mn}$ [duplicate]","This question already has answers here : Compute: $\sum_{k=1}^{\infty}\sum_{n=1}^{\infty} \frac{1}{k^2n+2nk+n^2k}$ (3 answers) Closed 3 years ago . I saw a problem here which state to evalute $$\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\frac{1}{m^2n+n^2m+2mn}$$ My attempt Let $$f(m,n)=\sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\frac{1}{m^2n+n^2m+2mn}$$ and interchanging $m,n$ as $n,m$ we have $$f'(n,m) = \sum_{m=1}^{\infty}\sum_{n=1}^{\infty}\frac{1}{mn^2+nm^2+2mn}$$ then I add $f(m,n)+f'(n,m)$ which gives the same which gives me the twice the original series. I failed to evaluate the series with the process I have applied.  How do I evaluate the series?  Any help will be appreciated.","['summation', 'sequences-and-series']"
3772440,Intuition on the special linear group.,"For any (unital and commutative) ring $R$ we can define the special linear group as the kernel of the determinant, that is $$
0 \to \operatorname{SL_n}(R) \to \operatorname{GL_n}(R) \xrightarrow{\det} R^\times \to 0
$$ is an exact sequence. Of course, when $R = \mathbb{R}$ or $\mathbb{C}$ , this means that we can think of this group as the linear automorphisms of $R^n$ preserving the canonical measure and orientation (by the change of variable formula and the fact that the derivative of a linear transformation is itself). Is there any way to describe $\operatorname{SL_n}(R)$ in this manner for a general ring? Perhaps we want to think of an action of $\operatorname{SL_n}(R)$ on the affine $n$ -space over $R$ (maybe even using the language of group schemes) and think on some invariant it preserves.
Even better would be to find an algebraic variety (if $R$ is a field) or a scheme which realizes $\operatorname{SL_n}(R)$ as its group of automorphisms.
Are there any results on this?","['group-theory', 'algebraic-geometry', 'intuition']"
3772455,"An intuitive explanation for the emergence of the $\pm$ symbol in the expansion of sin, cos or tan of $\frac{\alpha}{2}$ in terms of $\cos\alpha$","For some angle $\theta$ , $$\cos(2\theta) = 2\cos^2\theta - 1 \implies \cos(x) = \cos\Big(2\cdot\dfrac{x}{2}\Big) = 2\cos^2\Big(\dfrac{x}{2}\Big)-1$$ $$\implies \cos^2\Big(\dfrac{x}{2}\Big) = \dfrac{1+\cos(x)}{2}$$ $$\implies \cos\Big(\dfrac{x}{2}\Big) = \pm \sqrt{\dfrac{1+\cos(x)}{2}}$$ The expansions for $\sin\Big(\dfrac{x}{2}\Big)$ and $\tan\Big(\dfrac{x}{2}\Big)$ in terms of $\cos(x)$ are mentioned below. I have not derived them for that would make this question unnecessarily lengthier. $$\sin\Big(\dfrac{x}{2}\Big) = \pm \sqrt{\dfrac{1-\cos(x)}{2}}$$ $$\tan\Big(\dfrac{x}{2}\Big) = \pm \sqrt{\dfrac{1-\cos(x)}{1+\cos(x)}}$$ Now, I was looking for an intuitive explanation for the emergence of the $\pm$ symbol in these identities and also an explanation for the fact that the symbol does not appear in the expansion of $f(2\phi)$ in terms of $f(\phi)$ where $f$ is some trigonometric function. I thought of taking the identity $\sin\varphi = \pm \sqrt{1 - \cos^2\varphi}$ as reference. In this identity, the cause of emergence of the $\pm$ symbol seems to be the fact that the value of $\cos\varphi$ alone is not enough information to determine the value of $\sin\varphi$ . In other words, for a given value of $\cos\varphi$ , there are multiple possible values of $\sin\varphi$ (i.e. the value of $\sin\varphi$ is not unique). For example, if $\cos\varphi = \dfrac{1}{2}$ , then two possible values of $\varphi$ for $0 < \varphi \leq 2\pi$ are $\dfrac{\pi}{3}$ and $\dfrac{5\pi}{3}$ and hence, there are two possible values of $\sin\varphi$ , specifically, $\dfrac{\sqrt{3}}{2}$ and $\dfrac{-\sqrt{3}}{2}$ . Now, my main question here is that when we talk about one of the three identities that I mentioned above, sy $\cos\Big(\dfrac{x}{2}\Big) = \pm \sqrt{\dfrac{1+\cos(x)}{2}}$ , then what is it tha twe assume to be given to us? Is it just $\cos(x)$ that is given or are the values of all trigonometric functions at $x$ given? I know that the obvious answers looks like the former and you might be wondering how the latter would even imply that the value of $\cos\Big(\dfrac{x}{2}\Big)$ is not unique if all the trigonometric ratios of $x$ are given. Let me elaborate. Let's say that the values of all trigonometric functions at $\alpha$ for some angle $\alpha$ are given to us. Then, for $\alpha \in (0,2\pi]$ , there is one and only one value of $\alpha$ . Let us call that value $\lambda$ . But, when we look past the previous restriction that $\alpha \in (0,2\pi]$ and we look for values of $\alpha$ for $-\infty < \alpha < \infty$ i.e. all possible values of $\alpha$ , then there are infinite possible values of $\alpha$ and all of them are co-terminally related to $\lambda$ . Now, if we take a look at all the possible values of $\alpha$ and for all of them, we evaluate $\cos(2\alpha)$ , we get $\cos(2\lambda)$ in all the cases. This could explain why the $\pm$ symbol does not appear when we express $\cos(2\phi)$ in terms of $\cos\phi$ for some angle $\phi$ . But, I have observed (and mathematically proved) that for all possible values of $\alpha$ that are of the form $(2\pi)n + \lambda$ , where $n$ is an odd, $\cos\Big(\dfrac{\alpha}{2}\Big) = -\cos\Big(\dfrac{\lambda}{2}\Big)$ and where $n$ is even, $\cos\Big(\dfrac{\alpha}{2}\Big) = \cos\Big(\dfrac{\lambda}{2}\Big)$ . Let me provide an example for the sake of clarity. Let $\cos\gamma = \dfrac{\sqrt{3}}{2}$ and $\sin\gamma = \dfrac{1}{2}$ , then $\lambda = \dfrac{\pi}{6}$ and a co-terminal of $\lambda$ whose difference from $\lambda$ is an odd multiple of $2\pi$ is $\dfrac{13\pi}{6}$ . Now, for both these two values, the corresponding values of cosine of half of these angles are : $\dfrac{\sqrt{3}+1}{2\sqrt{2}}$ and $-\dfrac{\sqrt{3}+1}{2\sqrt{2}}$ respectively. So, we have established the fact that if we are provided with the values of all trigonometric functions at an angle, then there are infinitely many possible values of that angle and for all these values, the cosine of two times these angles is always a unique value but this is not the case with the cosine of half of these angles. Now, this result is valid for sine instead of cosine as ell. So, we can also say that the value of sine of half of all the possible values of $\alpha$ is not unique either. This is primarily due to the fact that $f(\pi + \delta) = -f(\delta)$ if $f(x)$ is either $\sin(x)$ or $\cos(x)$ for some angle $\delta$ . But, the period of the tangent function is $\pi$ , unlike the sine and cosine functions, whose period is $2\pi$ . So, if $\mu$ is some angle and we know the values of all trigonometric functions at $\mu$ , then for all the possible values of $\mu$ , the tangent of half of those values will always be unique. So, this caused my previous assumption that the values of all trigonometric functions at the given angle are known to us while solving the problem to completely break down. Conclusion : So, now I think that only the value of $\cos(\alpha)$ is known to us while solving the problem and that makes it way easier to deduce that the value of $f_x\Big(\dfrac{\alpha}{2}\Big)$ is not unique, where $f_x$ is either sine, cosine or tangent. So, I want to know if the way that I finally interpret the cause of emergence of the $\pm$ symbol and how I think that only the value of $\cos(\alpha)$ is known to us in context of these identities is correct. Please let me know if I have made some conceptual error in this post. It was long one, so it was not possible for me to go through it one more time and hence, I would appreciate your help in making this post error-free. Thanks! PS : I previously asked this question which is similar to this question but I only asked about $\cos\Big(\dfrac{\alpha}{2}\Big)$ in that question and I got a pretty satisfying answer from Keeley Hoek but I had not mentioned about sine and tangent in that question and this question is more focused on whether the assumption that I made in the previous question was indeed right or wrong since it doesn't seem to work with $\tan\Big(\dfrac{\alpha}{2}\Big)$ .","['trigonometry', 'intuition']"
3772489,"On an infinitely large chessboard, in how many paths of length $10$ can a knight take and end up in its original position?","The knight is moved exactly $10$ times. A knight has $8$ possible ways to move once.
So I believe there are $8^{10}= 2^{30} \sim 1$ billion permutations. How many in which the knight ends up on the same square?","['chessboard', 'combinatorics']"
3772525,"Well-definedness of ADM mass, energy, linear and angular momentum","In asymptotically flat initial data sets, one can define the ADM mass, energy, linear and angular momenta via spatial limits of integrals over 2-spheres. My questions are: Is asymptotical flatness enough to ensure that the ADM mass, energy and linear momentum converge? (I know that for the angular momentum, that is not true). Does convergence of one of them imply convergence of the others? And regarding the angular momentum, is being asymptotically Euclidean enough?
(Chrúsciel obtained some fall-off rates in his work in 1987 that ensure finiteness of the ADM angular momentum, but they are stronger than asymptotically Euclidean.) Thanks!","['semi-riemannian-geometry', 'smooth-manifolds', 'asymptotics', 'general-relativity', 'differential-geometry']"
3772534,Proof with euclidean geometry (tangents lines),"Tangents to a circumference of center O, drawn by an outer point C, touch the circle at points A and B. Let S be any point on the circle. The lines SA, SB and SC cut the diameter perpendicular to OS at points A ', B' and C ', respectively. Prove that C 'is the midpoint of A'B'. I saw a solution by Projective Geometry. I want to know if there is a solution by euclidean geometry. I think that is possible to do with Menelaus Theorem, but I don't know wich triangles I have to use. Thanks for attention.","['contest-math', 'euclidean-geometry', 'projective-geometry', 'tangent-line', 'geometry']"
3772546,"Is $n!\alpha \bmod 1$ dense in $[0,1]$?","We know that positive integer times a irrational number modulo $1$ generate a dense set in $[0,1]$ . According the answer of this post: Multiples of an irrational number forming a dense subset . I see no reason why the proof cannot be extended to $n!\alpha$ for $\alpha$ be an irrational number. We can just replace $i$ and $j$ with $i!$ and $j!$ and the argument still holds. Is that true?","['general-topology', 'sequences-and-series', 'factorial', 'real-analysis']"
3772550,Double integral of two periodic functions,"I'm quite stuck with the following practice question. Suppose $f,g: R \to R$ are continuous $2\pi$ -periodic functions. Let $h(s) =\int_{0}^{2\pi} f(s-t)g(t)  \ dt$ . Prove that $$\int_{0}^{2\pi} h(s)  \ ds = \left( \int_{0}^{2\pi} f(t)\ dt \right) \left( \int_{0}^{2\pi} g(t) dt \right)$$ My Attempt $\begin{equation}
\int_{0}^{2\pi} h(s)  \ ds  = \int_{0}^{2\pi}\int_{0}^{2\pi} f(s-t)g(t) \ dt \ ds  \\
= \int_{0}^{2\pi}g(t) [\int_{0}^{2\pi} f(s-t)\ ds] \ dt
\end{equation}$ Could someone point out the right direction to go from here?",['multivariable-calculus']
3772566,Can't solve quadratic function,"I'm trying to get a simple quadratic function of the form $y=ax^2+bx+c$ which goes through the following points: $$(0;0) \;(\frac d2;2)\; (d;0)$$ The solution I've calculated $3$ times on paper has always been $y=\frac8d- \frac{8x^2}{d^2}$ , but this can't be true, for an arbitrary $d$ , putting $x$ as $0$ should result in $0$ , but $\frac8{10}-8\times{\frac0{100}}$ is $\frac 8{10}$ , and not $0$ . Neither does input $d$ result in $0 \to y=\frac 8d$ Am I just approaching this problem incorrectly or am I daft?","['algebra-precalculus', 'quadratics']"
3772589,Is it true that $\mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X]$ for all $n \geq 2$?,"Let $X$ denote a random variable with a smooth distribution over $[0, 1]$ . Is it true that $$\mathbb{E}[X^n] > \mathbb{E}[X^{n-1}]\mathbb{E}[X]$$ for all integers $n = 2, 3, ...$ ? This seems to be a simple application of Jensen's inequality: however, the exact proof eludes me! My thoughts so far: In the case of $n = 2$ , we know that $$\mathbb{E}[X^2] > \mathbb{E}[X]\mathbb{E}[X] = \mathbb{E}[X]^2$$ by Jensen's inequality (since the function $f(x) = x^2$ is strictly convex on $[0, 1]$ ). Similarly, Jensen's inequality tells us that $$\mathbb{E}[X^n] > \mathbb{E}[X]^{n} = \mathbb{E}[X]^{n-1}\mathbb{E}[X]$$ but that is, unfortunately, not quite the inequality I am after. Finally, in the case where $X$ is uniformly distributed, one can calculate that $$ \mathbb{E}[X^n] = \frac{1}{n+1}$$ and similarly $$ \mathbb{E}[X^{n-1}] = \frac{1}{n}$$ $$ \mathbb{E}[X] = \frac{1}{2}$$ which allows one to verify that $$\mathbb{E}[X^n] = \frac{1}{n+1} > \mathbb{E}[X^{n-1}]\mathbb{E}[X] = \frac{1}{2n}$$ for all $n > 1$ . So this inequality holds in the uniform case, at least.","['jensen-inequality', 'probability-theory', 'probability']"
3772594,Directional derivative in the direction of a sum of two vectors.,"Q. Let $f: \mathbb{R}^{n} \rightarrow \mathbb{R}$ be a map. For each vector $\mathbf{v} \in \mathbb{R}^{n}$ , we define $$
D_{\mathbf{v}} f(\mathbf{a})=\lim _{t \rightarrow 0} \frac{f(\mathbf{a}+t \mathbf{v})-f(\mathbf{a})}{t}
$$ if the limit exists. $D_{\mathbf{v}} f(\mathbf{a})$ is the directional derivative of $f$ with respect to $v$ at $a$ .
Show that for vectors $\mathbf{v}, \mathbf{w} \in \mathbb{R}^{n},$ one has $$
D_{\mathbf{v}+\mathbf{w}} f(\mathbf{a})=D_{\mathbf{v}} f(\mathbf{a})+D_{\mathbf{w}} f(\mathbf{a})
$$ My attempt: $$
\begin{array}{l}\lim _{t \rightarrow 0} \frac{f(a+t(\mathbf v+\mathbf w))-f(a)}{t} \\ =\operatorname{lim}_{t \rightarrow 0}\frac{ f(a+t\mathbf v+t\mathbf w)-f(a)}{t}\\
=\operatorname{lim}_{t \rightarrow 0}\frac{ f(a+t\mathbf v+t\mathbf w)-f(a+t\mathbf v)}{t}+\frac{ f(a+t\mathbf v)-f(a)}{t}
\end{array}
$$ Now, I have to prove that $$\operatorname{lim}_{t \rightarrow 0}\frac{ f(a+t\mathbf v+t\mathbf w)-f(a+t\mathbf v)}{t}=D_{\mathbf{w}} f(\mathbf{a})
$$ But how to?",['real-analysis']
3772635,"Solving the system $\sqrt{x} + y = 7$, $x + \sqrt{y} = 11$","I want to solve the following nonlinear system of algebraic equations. Indeed, I am curious about a step by step solution for pedagogical purposes. I am wondering if you can come up with anything. I tried but to no avail. \begin{align*}
\sqrt{x} + y &= 7 \\
x + \sqrt{y} &= 11
\end{align*} The answer is $x=9,\,y=4$ . A geometrical investigation can give us better insights as depicted below. $\hspace{2cm}$","['nonlinear-system', 'calculus', 'systems-of-equations', 'algebra-precalculus']"
3772637,"Show that if $x \in \partial (A \cap B)$ and $x \not\in (A \cap \partial B)\cup (B \cap \partial A)$, then $x \in \partial A \cap \partial B$.","This question is for my exam prep. I want to solve the following example: Show that if $x \in \partial (A \cap B)$ and $x \not\in (A \cap \partial B)\cup (B \cap \partial A)$ , then $x \in \partial A \cap \partial B$ ,
where $A, B \subseteq \mathbb{R^n}$ .","['general-topology', 'real-analysis']"
3772646,Question about product of ideals in a $C^*$-algebra,"Consider the following fragments from Murphy's book ' $C^*$ -algebras and operator theory' I'm trying to understand why $B \cap I = BIB$ . Attempt: The inclusion $BIB \subseteq B\cap I$ is trivial since $B$ is hereditary and $I$ is an ideal. To show the other inclusion, it suffices to show that $(B\cap I)^+ \subseteq BIB$ since the positive elements of the $C^*$ -algebra $B \cap I$ (this is a $C^*$ -subalgebra because $B \cap I$ is a closed ideal of $B$ ) linearly span $B\cap I$ . Fix $a \in B \cap I $ . Then $a^{1/2} \in B \cap I$ . Let $(u_\lambda)$ be an approximate unit for $B$ . Then $$a = \lim_\lambda u_\lambda a = \lim_\lambda {u_\lambda} a^{1/2}a^{1/2} \in BIB$$ Is this correct?","['c-star-algebras', 'functional-analysis', 'ideals']"
3772654,Does $-6(x-\frac{1}{2})^2$ = $-\frac{3}{2}(2x-1)^2$?,"I'm confused about a question I posted this morning . I am trying to understand if $-6(x-\frac{1}{2})^2$ can be rewritten as $-\frac{3}{2}(2x-1)^2$ ? I tried multiplying out the expression $-6(x-\frac{1}{2})^2$ to a polynomial form $36x^2-36x+9$ but that didn't take me closer to understanding my goal. I noticed that I can remove the fraction inside $-6(x-\frac{1}{2})^2$ by doubling the contents: $-6(x-\frac{1}{2})^2$ <> $-6(2x-1)^2$ # used <> for does not equal I don't think I can simply half the factor -6 to get $-3(2x-1)^2$ As you can no doubt see, I am confused. Does $-6(x-\frac{1}{2})^2$ = $-\frac{3}{2}(2x-1)^2$ ? If it does could someone show me how to transform from $-6(x-\frac{1}{2})^2$ to $-\frac{3}{2}(2x-1)^2$ in granular baby steps?",['algebra-precalculus']
3772661,Show that $\mathcal{F}$ is an algebra but not a $\sigma$-algebra.,"Let $\Omega = \mathbb{R}$ and $\mathcal{F}$ be the collection of all finite unions of disjoint intervals of the form $(a,b]\cap\mathbb{R}$ , $-\infty\leq a < b\leq \infty$ . Show that $\mathcal{F}$ is an algebra but not a $\sigma$ -algebra. MY ATTEMPT Indeed, $\Omega\in\mathcal{F}$ : it suffices to take $a = -\infty$ and $b = \infty$ . If $A = (a,b]$ , then $A^{c} = (-\infty,a]\cup(b,\infty)\in\mathcal{F}$ because $(-\infty,a]\in\mathcal{F}$ , $(b,\infty)\in\mathcal{F}$ and the union is finite. Finally, if $A = (a,b]$ and $B = (c,d]$ , then one has that \begin{align*}
A\cap B = (a,b]\cap(c,d] =
\begin{cases}
\varnothing & \text{if}\,\,(a\geq d)\vee(b\leq c)\\\\
(c,b] & \text{if}\,\,a \leq c < b \leq d\\\\
(a,d] & \text{if}\,\,c \leq a < d\leq b
\end{cases}
\end{align*} which clearly belongs to $\mathcal{F}$ . But $\mathcal{F}$ is not a $\sigma$ -algebra. Indeed, it suffices to consider the sequence of sets: \begin{align*}
S_{n} = \left(a,b - \frac{1}{n}\right]\in\mathcal{F} \Rightarrow \bigcup_{n\in\mathbb{N}} S_{n} = (a,b)\not\in\mathcal{F}
\end{align*} Consequently, $\mathcal{F}$ is not a $\sigma$ -algebra. Is there any theoretical flaw in my reasoning? Any contribution is appreciated.","['measure-theory', 'proof-writing', 'solution-verification']"
3772687,Probability of exactly $2$ sixes in $3$ dice rolls where $2$ dice have $6$ on $2$ faces?,Three dice are rolled. One is fair and the other two have 6 on two faces. Find the probability of rolling exactly 2 sixes. My textbook gives an answer of $\frac{20}{147}$ but I get an answer of: $$\frac{1}{6}\frac{2}{6}\frac{4}{6}+\frac{1}{6}\frac{4}{6}\frac{2}{6}+\frac{5}{6}\frac{2}{6}\frac{2}{6}=\frac{8}{216}+\frac{8}{216}+\frac{20}{216}=\frac{36}{216}=\frac{1}{6}$$ I just want to know where I am going wrong or could the textbook be mistaken ?,"['dice', 'probability']"
3772724,Finding $\lim_{n\to\infty}\frac1{n^3}\sum_{k=1}^{n-1}\frac{\sin\frac{(2k-1)\pi}{2n}}{\cos^2\frac{(k-1)\pi}{2n}\cos^2\frac{k\pi}{2n}}$,"For all $n\ge 1$ , let $$
a_{n}=\sum_{k=1}^{n-1} \frac{\sin \left(\frac{(2 k-1) \pi}{2 n}\right)}{\cos ^{2}\left(\frac{(k-1) \pi}{2 n}\right) \cos ^{2}\left(\frac{k \pi}{2 n}\right)}
$$ Find $\displaystyle\lim_{n\to\infty}\frac{a_n}{n^3}$ . This is a problem from the 2019 Putnam competition . The official solutions use two different strategies; one reduces the expression as a telescoping sum and the other uses the asymptotic property of sine near zero. The two community-wiki answers below essentially elaborate on the official solutions. Remark. I am curious if one can write $\displaystyle\frac{a_n}{n^3}$ as a Riemann sum so that one can write the limit as an integral. Naively, the fraction looks very much like $\frac{1}{n}\sum_{k=1}^{n-1}\cdots$ , which is in the setting of Riemann sums. (This only serves as a comment, not a requirement for solving the problem.)","['limits', 'calculus', 'trigonometry', 'sequences-and-series']"
3772750,Hessian form of a real valued function on a submanifold of $\mathbb{R}^{n+m}$,"I recently came across a result in a text in the field of differential geometry and I'm wondering why it is true. Let $M\subset\mathbb{R}^{n+m}$ be an $n$ -dimensional submanifold and $w:M\longrightarrow\mathbb{R},\,w(x)=u(x)-\langle x,z\rangle$ , where $u:M\longrightarrow\mathbb{R}$ , $z\in\mathbb{R}^{n+m}$ and $\langle\cdot,\cdot\rangle$ denotes the canonical inner product on $\mathbb{R}^{n+m}$ . The author states that we have $D_{M}^{2}w(x)=D_{M}^{2}u(x)-\langle II_{x}(\cdot,\cdot),z\rangle$ , where $II_{x}$ is the second fundamental form at the point $x$ and $x$ being a critical point of $w$ . How is the exact computation to get this result? What I tried is to first compute the gradient w.r.t $M$ of $w$ : $\nabla^{M}w(x)=\nabla^{M}u(x)-z^{tan}$ , where $z^{tan}$ is the tangential component of $z$ . If this is correct (is it?), I don't know how to get the Hessian of $w$ at $x$ . Thanks in advance!","['hessian-matrix', 'submanifold', 'riemannian-geometry', 'differential-geometry']"
3772751,What is the equation for an oblique cylinder centered on the origin?,"I want to find the equations for a solid oblique cylinder centered on the origin. What's this equation? Work I've Done So Far Suppose the cylinder has radius $R$ and height $h$ . First, I found the equation for a regular solid cylinder centered on the origin. $$\begin{align}
x^2 + y^2 &\leq R^2 \\
-\frac{h}{2}\leq z &\leq \frac{h}{2}
\end{align} $$ So then I tried to make a cylinder slanted. And looking top down the cylinder moves So, then I reasoned that I could just substitute these new values in. My equation for a solid oblique cylinder would be $$\begin{align}
(x-h\cos(\alpha))^2 + y^2 &\leq R^2 \\
0\leq z &\leq h \sin(\alpha)
\end{align} $$ But this is not centered on the origin. Using my diagram above, the center has $y=0$ , and then the average of $(0,0)$ and $(2R + h\cos(\alpha),h\sin(\alpha)$ . Then the center is $$\mathbf{r}_{center}=\left(R + \frac{h}{2}\cos(\alpha),0,\frac{h}{2}\sin(\alpha)\right)$$ So the oblique cylinder centered on the COM is $$\begin{align}
\left(x-\left(R+\frac{h}{2}\cos(\alpha)\right)\right)^2 + y^2 &\leq R^2 \\
-\frac{h \sin(\alpha)}{2}\leq z &\leq \frac{h \sin(\alpha)}{2}
\end{align} $$ But this also seems wrong to me because I think the $x$ coordinate of the center of a thin cross section of the cylinder should vary with height and my formula has no $z$ . So another possibility might be $$\begin{align}
\left(x-\left(\frac{z}{2}\cos(\alpha)\right)\right)^2 + y^2 &\leq R^2 \\
-\frac{h \sin(\alpha)}{2}\leq z &\leq \frac{h \sin(\alpha)}{2}
\end{align} $$ This one seems reasonable because (1) the center of the circle would vary with height and (2) it reduces to the regular cylinder when $\alpha = 90^{\circ}$ .",['geometry']
3772804,Determine if the autonomous system is Hamiltonian,"Consider the autonomous system $\dot{x}=-y-\alpha ^2xy^2$ and $\dot{y}=x^3$ , where $\alpha$ is a real parameter. (a) For which values of $\alpha$ is this system Hamiltonian? For each case, find the Hamiltonian. (b) For each value of $\alpha$ , find all equilibrium solutions of the above system. Can the principle of linearized stability be used to determine their stability? (c) Show that for all $\alpha \in R$ the origin is a stable equilibrium. (Hint: Can you use Hamiltonian functions from (a)?). So the way to solve this is to use the fact that $\dot{x}=\partial H/\partial y$ and $\dot{y}=-\partial H/\partial x$ , where $H(x,y)$ is the Hamiltonian for this system. But then $H(x,y)=-\frac{x^4}{4}+V(y)$ and $H(x,y)=-\frac{y^2}{2}+V(x)$ but I don't know how to solve this further.  The only way to have Hamiltonian is if $\alpha =0$ , ie. $H(x,y)=-\frac{x^4}{4}-\frac{y^2}{2}$ . Then we will have system $\dot{x}=-y$ and $\dot{y}=x^3$ . Is this right? So only $\alpha$ that would work is if $\alpha =0$ and then only equilibrium will be the origin $(0,0)$ . Then the Jacobian would be $Df=\begin{pmatrix} 0 & -1 \\ 3x^2 & 0 \end{pmatrix}=\begin{pmatrix} 0 & -1 \\ 0 &0\end{pmatrix}$ . So then what is equilibrium solution of this system and can principle of linearized stability be used to determine its stability? How do we show for $\alpha \in R$ , the origin is a stable equilibrium? I am not sure if I got the Hamiltonian correct, since I am assuming only $\alpha =0$ will give a Hamiltonian. But the questions assume there are other $\alpha$ values that will work just as well.
Please help.",['ordinary-differential-equations']
3772819,How to solve and draw the graph of this function equation,"I see a graph of a function equation in the title page of this book , but the specific drawing method is not given in the book. I want to know how to solve this function equation and draw its image: $$f(x)+f(2x)+f(3x)=0$$",['functions']
3772843,How to Make Meaningful Conclusions?,"I recently appeared for an Interview for my college and I was asked the following question. The Interviewer said that this question was a Data Science question. He asked the same question to a friend of mine as well. The question- Suppose 7.5% of the population has a certain Bone Disease. During COVID pandemic you go to a hospital and see the records. 25% of the COVID Infected patients also had the Bone Disease. Can we say for sure if the Bone Disease is a symptom of COVID-19? My Reponse- I said No, and explained it as it's not necessary that COVID-19 is causing these symptoms, it could very well be possible that the 7.5% of the country's population which already had the disease is more susceptible to the virus due to lowered immunity. Hence making conclusions is not possible. Then the interviewer asked me How can we be sure if it is a symptom or not? I replied saying we can go to more Hospitals, collect more data and see if it correlates everywhere. The Interviewer then said If we have the same results everywhere will you conclude it's a symptom? I had no good answer but I replied that Just correlation of data is not sufficient, we also need to check if the people who have COVID-19 had the bone disease prior to getting infected or not. See if that percentage also correlates and stuff. Here he stopped questioning however I couldn't judge If I was right or wrong. I am in Grade-12 so I have no experience in Data Science as such. I do know a fair bit of statistics however I have never solved such questions. Can someone provide me insights on how to solve such questions and make meaningful conclusions? I have asked the same question on Data Science SE however i noticed the other questions there were quite different so I wasn't sure if this question is appropriate there. If there are any better SE suggestions do comment them.","['data-analysis', 'statistics', 'soft-question']"
3772876,"If there is a linear function $g$ which is at least as good of an approximation as any other linear $h$, then $f$ is differentiable at $x_0$.","This question is related to one in this question where the author asks what is the intuition behind saying that derivative is the best linear approximation. One of the answers by user ""Milo Brandt"" is that we have two theorems, one of which is: $f$ is differentiable at $x_0$ if and only if there is a linear function $g$ which is at least as good of an approximation as any other linear $h$ . I am struggling to prove one part of this theorem. First, I think that $g$ and $h$ are supposed to be affine and not linear in a sense that $g(x) = A + B(x-x_0)$ and $h(x) = C + D(x-x_0)$ where $A,C \in \mathbb{R}^m$ and $B,D : \mathbb{R}^n \to \mathbb{R}^m$ are linear functions. Assume that there is such function $g$ which is at least as good as an approximation as any other $h$ . By definition, this means that there exists $\delta > 0$ such that for all $x$ that have $|x - x_0 | < \delta$ we have $|f(x) - g(x)| \leq |h(x) - f(x)|$ . I would like to show that $f$ is differentiable at $x_0$ , in other words, that there exists a linear function $\lambda : \mathbb{R}^n \to \mathbb{R}^m$ such that: $$ \lim \limits_{x \to x_0} \frac{|f(x) - f(x_0) - \lambda(x-x_0)|}{|x-x_0|} = 0 $$ This translates to be able to find such $\lambda$ so that for each $\varepsilon > 0$ we can find $\delta > 0$ such that when $|x - x_0| < \delta$ , we have $|f(x) - f(x_0) - \lambda(x-x_0)| < \varepsilon |x-x_0|$ . I think intuitively, I would like to show that $\lambda = B$ is correct choice. From $g$ being as good as approximation as any $h$ , I have the following: there is $\delta > 0 $ so that for all $|x - x_0| < \delta$ I have $|f(x) - g(x)| \leq |C - f(x) + D(x-x_0)|$ . Now, I can use triangle inequality and also result that I have already proven which is that any linear function $D$ is bounded in the following way: $|D(x-x_0)| < M|x-x_0|$ . In this case, I can show that I can always find $\delta > 0$ such that for all $|x-x_0| <\delta$ I have $|f(x) - g(x) | \leq |C-f(x)+D(x-x_0)| \leq |C - f(x)| + |D(x-x_0)| < |C - f(x)| + M|x-x_0|$ . As $h$ is arbitrary, I could choose $M = \varepsilon$ as I also know that $M = \sqrt{mn}$ $ \mathrm{max}_{ij}|D_{ij}|$ . But then I would only get that $|f(x) - g(x)| < |C - f(x)| + \varepsilon |x-x_0|$ . How to get rid of the second term? Should I use continuity? Do I somehow use that $g$ is linear now? Any help would be appreciated - thanks!","['multivariable-calculus', 'derivatives', 'real-analysis']"
3772886,Differentiating powers of e where the power is an exponential function like $e^{a^x}$ when $a$ is a constant,"Using the fact that $e = \lim_{h \to \infty} (1+h)^{1/h}$ , you can answer $\frac{d}{dx} e^{p(x)}$ where $p(x)$ is a polynomial using the variable x. However, I have troubles finding the derivative of $e^{p(x)}$ where “something” is a exponential function. Is there a way/formula (or multiple formulae) finding the derivative of such functions? I’ll be glad if you include the proof of such ways.","['calculus', 'derivatives', 'exponential-function']"
3772890,The set of elements of order $7$ of $A_7$ is not a conjugacy class.,"I think the problem is simple but I just want to make sure I am doing it right. Elements of order $7$ are of the form $(abcdefg)$ . There are $6!$ such elements. But $6! \nmid 7!/2$ and thus if it was a conjugacy class, it would contradict orbit stabilizer theorem.","['permutations', 'abstract-algebra', 'solution-verification', 'group-theory', 'group-actions']"
3772923,Is the volume of a cube the greatest among rectangular-faced shapes of the same perimeter?,"My child's teacher raised a quesion in class for students who are interested to prove. The teacher says that the volume of a cube is the greatest among rectangular-faced shapes of the same perimeter and asks his students to prove this proposition. I considered the relationship between the length of the sides of a cube and the lengths of the sides of rectangular-faced shapes in different situation. But when the calculations came down to polynomials, I couldn't proceed due to the uncertainty of the variables in the polynomials. Can anyone please find a good way to prove the above proposition? Or is there already a proof? Thank you for your help!",['geometry']
3772942,Ultrafilter with finite set,"While I was working in some exercise about filters, a question came to my mind: let $X$ a set and $F\subseteq X$ a non empty finite set. How many ultrafilters $U$ there are such that $F\in U$ ? I think that there exist a unique ultrafilter that contains $F$ but I can't see why or how to prove but my intuition says that it is true. Am I wrong? My work: take $F$ a non empty and finite subset of $X$ . Suposse that there exist two different ultrafilters $U$ and $V$ such that $F\in U$ and $F\in V$ . Since $U\neq V$ , w.l.g., we can take $A\in U\setminus V$ . Then $A\notin V$ but $V$ is an ultrafilter and therefore $X\setminus A\in V$ . Moreover, $F\cap (X\setminus A)\neq\emptyset$ and $F\cap A\neq\emptyset$ . But then, from here, what can I do? If my intuition is wrong, then, is there a bound over the number of ultrafilters that contains a fixed finite set? Thanks.","['elementary-set-theory', 'boolean-algebra', 'filters']"
3772951,Proof with invertible matrices,"Let $A,B,C$ be matrix of the same size, and suppose A is invertible, Prove that $(A-B)C=BA^{-1}$ then $C(A-B)=A^{-1}B$ I tried to prove it as following. $(A-B)C=BA^{-1}$ implies $AC-BC=BA^{-1}$ so $ACA-BCA=B$ taking $A^{-1}$ $CA-A^{-1}BCA=A^{-1}B$ Any hint will be appreciated","['matrices', 'linear-algebra']"
3772955,Why we can not define the solution of the differential equation beyond a particular $t$? [duplicate],"This question already has an answer here : Why can't a union of two intervals be the maximum existence interval of a solution? (1 answer) Closed 3 years ago . I was studying Ordinary Differential Equations, and my book was trying to explain the maximal interval of solution of a differential equation. If we consider this differential equation $$
\frac{dx}{dt} = x^2 \\
\text{with initial condition}~x(0)= a,~~~a \gt 0
$$ Then, the solution to this equation is $$
x(t) = \frac{1}{ a^{-1} - t}$$ Now, if $t$ starts to increase from $0$ the denominator will decrease and consequently $x(t)$ will increase, when $t =a^{-1}$ the denominator is $0$ and $x(t) = + \infty$ . As we let $t$ to decrease from $0$ , our denominator will increase and finally when $t = -\infty$ , $x(t) = 0$ . So, the solution of the differential equation is defined on the interval $(-\infty, a^{-1})$ . But the problem comes when the book makes this statement but there is no way to define the solution that extends further into the future beyond $t=a^{-1}$ . Why the function is not defined for $t \gt a^{-1}$ ? There is a discontinuity at $t=a^{-1}$ but beyond that the function is nice , why the solution is not defined after $t = a^{-1}$ ? If compare $x(t)$ with some other simple functions like, for example, $f(x) = \frac{1}{2-x}$ , $f(x)$ is well defined after $x = 2$ , and here we have its graph: . Why the book says that beyond $t=a^{-1}$ the solution is not defined?","['functions', 'ordinary-differential-equations']"
3772987,Probability - constant probability throughout the day,"The probability of an electronics store selling at least one computer in an 8-hour working day is 0.8. Assuming a constant probability throughout the day, what’s the probability of the store selling at least one computer in any given 2-hour time window? Hi, i want to understand how to approch this kind of questions.
if any one can explain? and get the answer.","['statistics', 'probability']"
3773000,Maximal and prime ideal of $R:=\prod\limits_{n=1}^\infty \mathbb{Z}/m\mathbb{Z}$,"Let $m$ be an integer such that $m \ge 2$ . We define $R$ as the countable direct product
of the ring $\mathbb{Z}/m\mathbb{Z}$ $$R:=\prod_{n=1}^\infty \mathbb{Z}/m\mathbb{Z}$$ I am trying to prove that the dimension of $R$ is $0$ .
Which means I have to prove that all the prime ideal of $R$ is also a maximum ideal of $R$ ( $*$ ).
It is obviously that if $m$ is prime then it only have $1$ element in $\operatorname{Spec} R$ , then it's maximal.
But I don't know how to show ( $*$ ) when m is not a prime number. The ideals of $\mathbb{Z}/m\mathbb{Z}$ have the form $(d)$ , where $d$ divides $m$ . And the maxium ideal of $\mathbb{Z}/m\mathbb{Z}$ is look like $(p)$ where $p$ is a  prime dividing $m$ . Let $$R_i=\prod_{n=1}^i \mathbb{Z}/m\mathbb{Z}$$ The ideal of $R_2$ , looks like $(d_1)\times (d_2)$ , $(d_1)\times R_1$ or $R_1\times (d_2)$ , but the ideal of $R_2$ is a prime ideal iff $(p)\times R_1$ or $R_1 \times (p)$ and $p$ is prime (1) and is also a maximal ideal(2) (if (1)(2) are true I think I can prove it). But I am not sure if (1) and (2) are true. And what I could do when $i$ is $\infty$ ?","['algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3773010,Prove the inequality $\sum_{cyc}\frac{a^3}{b\sqrt{a^3+8}}\ge 1$,"Let $a,b,c>0$ and such $a+b+c=3$ ,show that $$\sum_{cyc}\dfrac{a^3}{b\sqrt{a^3+8}}\ge 1\tag{1}$$ I tried using Holder's inequality to solve it: $$\sum_{cyc}\dfrac{a^3}{b\sqrt{a^3+8}}\sum b\sum \sqrt{a^3+8}\ge (a+b+c)^3$$ But the following is not right $$\sum\sqrt{a^3+8}\le 9$$ so please help me prove $(1)$","['summation', 'inequality', 'tangent-line-method', 'sum-of-squares-method', 'algebra-precalculus']"
3773028,Reparametrization theorem,"The reparametrization theorem says the following: If $α:I\to\mathbb{R}^n$ is a regular curve in $\mathbb{R}^n$ , then there exists a reparametrization $\beta$ of $\alpha$ such that $β$ has unit speed. My question is this: If the curve is not regular, then is there no arc length parameterization?.
What I tried was to get the following example $t\mapsto (|t|t,t^2)$ for $t\in[-1,1]$ whose graph would approximate this: It is understood that when reaching point $(0,0)$ the particle that follows this route stops instantly and then he continues its journey, but if there were a parameterization $\beta$ by arc length, it means that when reaching that point it would continue with $||\beta'||=1$ , is there such a possibility? How would it be explained if it existed. Thanks.","['curves', 'arc-length', 'differential-geometry']"
3773051,How I get the nth derivative of the function $y = e^x x^2$,I'm totally confused. Please anyone help me. $y' = e^x x^2 + 2 e^xx$ $y''= e^x x^2 + 4 e^xx + 2e^x$ $y'''= e^x x^2 + 6 e^xx + 6 e^x$ next $y''''$ but I failed to get any pattern.,"['calculus', 'derivatives']"
3773060,Conditional entropy - example (clipping function),"Given a random variable X that is uniformly distributed on $[-b,b]$ and $Y=g(X)$ with a center clippper function $$g(x) = \begin{cases} 0, ~~~ x\in [-c,c] \\ x, ~~~ \text{else}\end{cases}$$ Now I want to compute the information dimension $d(X), d(Y)$ and the conditional information dimension $d(X|Y)$ and show that $d(X) = d(X|Y) + d(Y)$ in this case. The information dimension is defined as $$ d(X) = \lim_{m\rightarrow \infty} \frac{H(\hat{X}^{(m)})}{m}$$ with $$\hat{X}^{(m)} := \frac{\lfloor2^m X \rfloor}{2^m} $$ the quantization of X. For a discrete distribution, $d(X) = 0$ , and for a continuous one-dimensional distribution, $d(X) = 1$ . For a mixed distribution with discrete and continuous components of the form $P_X = d P_X^{(ac)} + (1-d) P_X^{(d)}$ , the information dimension is $d(X)=d$ . Now I know, that the random variable X has a continuous component $\Rightarrow d(X) = 1$ . The distribution $P_Y$ is a discrete-continuous mixture: $$ P_Y = \begin{cases} \frac{c}{b}, ~~~Y=0\\ \frac{1}{2b},~~~Y \in [-b,-c] \cap [c,b]\\ 0,~~~\text{else} \end{cases}$$ Therefore, $d(Y)=\frac{b-c}{b}$ . Now my question is the following: how do I compute the conditional information dimension? $$d(X|Y) = \lim_{m \rightarrow \infty} \frac{H(\hat{X}^{(m)}|Y)}{m} = \int_\mathcal{Y} d(X|Y=y)dP_Y(y) = \mathbb{E}_{Y\sim P_Y}(d(X|Y=Y))$$","['entropy', 'conditional-probability', 'information-theory', 'probability-theory', 'probability']"
3773065,For which $n \in N$ is the following matrix invertible?,"For which $n \in N$ is the following matrix invertible? $$\left[\begin{array}{[c c c]} 
10^{30}+5 & 10^{20}+4 & 10^{20}+6 \\
10^{4}+2 & 10^{8}+7 & 10^{10}+2n \\
10^{4}+8 & 10^{6}+4 & 10^{15}+9 \\
\end{array}\right]$$ My attempt :
For the matrix to be invertible, it must be non-singular. To compute the determinant, I split the large determinant into smaller determinants using the column addition property but it got too tedious to compute (and too lengthy to type out here :P) The answer : Replacing even numbers by zero and odd numbers by one, we have $$|A| = \left| \begin{array}{c c c} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{array}\right|$$ which is an odd number and hence $|A|$ can not be zero. Hence A is invertible for all $n \in N$ . I did not understand how all the odd numbers were simply replaced with 1 and the even numbers with 0. I would appreciate it if someone could provide a different answer or explain the given answer.","['matrices', 'determinant', 'linear-algebra']"
3773073,"On cardinality of varieties (Hartshorne I.4.8, following Hartshorne's hint)","In Hartshorne's Algebraic Geometry Exercise I.4.8, Exercise : Show that any variety of positive dimension over $k $ has the same cardinality as $ k $ . Hints : Do $\mathbb{A}^{n} $ and $\mathbb{P}^{n}$ first. Then for any $ X $ , use induction on the dimension $ n $ . Use (4.9) to make $ X $ birational to a hypersurface $ H \subset\mathbb{P}^{n+1 }$ . Use (Ex. 3.7) to show that the projection of $  H$ to $\mathbb{P}^{n}$ from a point not on $ H$ is finite-to-one and surjective. Although this has been asked in many posts in MSE, namely: Cardinality of variety and Cardinality of quasiaffine variety . My question is different. In these posts, it seems that the proof provided (although they are indeed wonderful and helpful) did not follow the hints by Hartshorne. I tried to follow the hints, but didn't get the promising results. So my question is: How to finish this exercise using the methods provided in the HINTS ? My attempts : (1) I had made some attempts and found that using Noether normalisation theorem, we may direct prove the desired result. Maybe I will post an answer using this in the posts mentioned above. Yet I still hope to know how to follow the hint. (2) I have managed to prove that the cardinality of $\mathbb{A}^{n} $ and $\mathbb{P}^{n}$ are the same as the cardinality of the ground algebraically closed field $k$ . Yet what's next?","['cardinals', 'algebraic-geometry']"
3773081,Defining the derivative of a vector field component,"I'm reading 'Core Principles of Special and General Relativity' by Luscombe, specifically the introductory section on problems with defining usual notion of differentiation for tensor fields. I'll quote the relevant part: The second way (to see whether the partial derivative of a tensor is a tensor) is to look at the definition of derivative, $$\frac{\partial T^i}{\partial x^j}=\lim_{dx^j\to 0}\frac{T^i(x+dx^j)-T^i(x)}{dx^j}$$ The numerator is not in general a vector! We're comparing (subtracting) vectors from different points, yet the transformation properties of tensors are defined at a point . Since the equation above is a notational mess, here's my attempt to interpret it: $$\bigg(\frac{\partial T^i}{\partial x^j}\bigg)_p=\partial_j(T^i\circ x^{-1})(x(p))=\lim_{h\to 0}\frac{(T^i\circ x^{-1})(x(p)+[0,\ldots,h,\ldots,0])-(T^i\circ x^{-1})(x(p))}{h}$$ where $[0,\ldots,h,\ldots,0]\in\mathbb{R}^n$ has $h$ as its $j$ -th coordinate. Is my above interpretation correct? If so, what's the issue with defining the derivative of a vector field component in this way?","['vector-fields', 'differential-geometry']"
