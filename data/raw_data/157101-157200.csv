question_id,title,body,tags
2677958,"number of ways to make a pile of $n$ poker chips using red, white, and blue chips and such that no two red chips are together","Find and solve a recurrence relation for the number of ways to make a pile of $n$ poker chips using red, white, and blue chips and such that no two red chips are together (consecutive). I believe I have found the recurrence relation correctly, however it is solving it where I am running into problems Part $1)$ Find a recurrence relation Consider the color of the top chip, if it is red, then he one below cannot be red and the remaining $n-2$ chips give $a_{n-2}$ different ways. If it is not red, then the remaining $n-1$ chips give $a_{n-1}$ different ways. Then I believe my recurrence relation is $$a_n = 2a_{n-1}+2a_{n-2}$$ with $a_1=3, a_2=8$ If you have two poker chips you have 8 ways to make a pile without
two consecutive red chips: $wb,bw,rw,wr,rb,br,ww, bb$. Or you
calculate all possible ways and substract the $rr$ combination: $a_2=3^2-1=8$ Similar for $a_1$: The are 3 ways without two consecutive red chips:
$r,b,w\Rightarrow a_1=3$ Part $2)$ Solve the recurrence relation Using the characteristic equation, $$x^n = 2x^{n-1}+2^{n-2}$$ Dividing by the smallest power gives, $$x^2-2x-2=0$$ Solving for $x$ gives, $$x = 1 \pm \sqrt3$$ Then using the general solution: $$a_n = A_1x_1^{n+1}+A_2x_2^{n+1}$$ Using conditions from above, $$ 3 = A_1(1+\sqrt3)^2+A_2(1-\sqrt3)^2$$
$$ 8 = A_1(1+\sqrt3)^3+A_2(1-\sqrt3)^3$$ Solving for $A_1, A_2$ I got $A_1 = b=\frac{3+\sqrt{3}}{12}, A_2 =  b=\frac{3-\sqrt{3}}{12}$ Therefore the final answer would be $$a_n = \frac{3+\sqrt{3}}{12}(1+\sqrt3)^{n+1} + \frac{3-\sqrt{3}}{12}(1-\sqrt3)^{n+1}$$ However an answer I found online to this question is the following, so I am not sure where I went wrong in solving it, looking for some help thanks! $$a_n = \frac{1}{4\sqrt3}\left[ (1+\sqrt3)^{n+2} - (1-\sqrt3)^{n+2}\right]$$","['combinatorics', 'recurrence-relations', 'discrete-mathematics']"
2677979,How do you solve $(D^2+1)y=4\cos{x}$?,"I'm stuck on this question: $$(D^2+1)y=4\cos{x}$$ where $D^2$ denotes the differential operator $\frac{d^2}{dx^2}$ As far as I know for trig functions, I'm supposed to assume $y=A\sin{x}+B\cos{x}$ and substitute to get $A$ and $B$. But however, for such, $$(D^2+1)y=0$$ for all values of $A$ and $B$. I don't know what other types of $y$ I should assume. I'm basically clueless here, so any hints would be great.",['ordinary-differential-equations']
2678017,Number of Non negative integral solutions of $x+2y+3z+4w=12$,Find Number of positive integral solutions of $x+2y+3z+4w=12$ I tried to split the problem in to cases : we have $2z+2y+4w$ is always  Even and $12$ is Even number. Hence $x+z$ should be even number which implies Case $1.$ $x$ is Even and $z$ is even Letting $x=2p$ and $z=2q$ we get $p+y+3q+2w=6$ Now Let $q=0$ we have $$p+y+2w=6$$ $$p+y=6-2w$$ Number of solutions by stars and bars is given by $$\sum_{w=0}^{3} 7-2w=16$$ Again letting $q=1$ we have to repeat same process. This method i feel is so lengthy. Any other way?,"['algebra-precalculus', 'combinatorics', 'systems-of-equations']"
2678044,Proving solutions of differential equations are periodic,"Given the system of differential equations: $$
\left\{
\begin{array}{c}
x'(t) = y(t) \\
y'(t) = -V'(x(t))
\end{array} 
\right. 
$$ Where $V(x) = x^2  + \epsilon x^4$, so $V'(x) = 2x + 4\epsilon x^3$. I want to prove that every solution of this equation that doesn't go through the point $(x,y) = (0,0)$ is periodic. Before this, I found out (as an exercise) that the fixed points of this system of equations are $(x,y) = (\sqrt{1/2\epsilon}i , 0)$ and $(x,y) = (-\sqrt{1/2\epsilon}i , 0)$. And that $H = \frac{y^2}{2} + V(x)$ is a constant function (so for every solution $(x(t), y(t))$, H remains a constant).
Perhaps this might be useful in proving that the solutions are periodic, but I couldn't figure out why, if that's the case. I've tried proving that the solutions are periodic, but didn't get far. Also what I thought was confusing, is that the $(x,y) = (\sqrt{1/2\epsilon}i , 0)$ is a fixed point, and a solution to the system of equations. Yet it doesn't go through $(0,0)$, but it's not periodic because it's constant. What am I missing? What do I need to prove that the solutions are periodic?",['ordinary-differential-equations']
2678083,Conditional Distribution from Gamma Distribution,"Consider a random sample of size $n$ froma  gamma distribution, $X_i\sim GAM(\theta, \kappa)$ , with $\kappa$ being the shape parameter and $\theta$ being the scale parameter and let $\bar X=\dfrac{1}{n}\sum X_i$ and $\tilde X=(\prod X_i)^{1/n}$ be the sample mean and geometric mean, respectively. Show that the conditional distribution of $\bar X |_{\tilde X = \tilde x}$ does not depend on $\kappa$ . I have absolutely no idea how to find this conditional distribution.  Even if there are some strategies to go about solving this problem without solving for the conditional distribution explicitly, I don't know what they are.  How would you show this?  Is there a general strategy for finding conditional distributions of statistics that I don't know about?  I'm so lost.","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
2678105,Understanding Linear Algebra Geometrically - Reference Request,"I know geometry and I know linear algebra but when I understand a linear algebraic concept geometrically, my head just explodes and things just become so much clearer and easier to understand...not to mention easier to remember or figure out its properties and explain them to others. Here are a few examples. Orthogonal matrices - If you think of an orthogonal matrix as a rotation then some of its properties are obvious. Orthogonal matrices are always invertible because rotations can simply be reversed. They always preserve the Euclidean norm because rotating a vector doesn't change its length. Orthogonal matrices forming a group is also easy to see because it is easy to see them satisfying the group axioms. Determinant - The determinant of a linear transformation can be understood as follows. Start with the (chosen) basis of your domain. It forms a parallelepiped. Call it $P$. It has a certain volume $V(P)$. Now apply your linear transformation $T$ to the chosen basis. A new parallelepiped $T(P)$ is formed and its volume in the range space (embedded in the codomain) is now $V(T(P))$. The determinant (in absolute value) is the ratio of the new volume to the old one. This intuitively explains, for example, why the determinant is zero for non-invertible transformations. The dimension of such a transformation will always be strictly less than the dimension of the domain/codomain so the volume of the transformed parallelepiped will always be zero. I always imagine a parallelepiped in $\mathbb{R}^3$ collapsing onto a plane. This also explains why the determinant of an orthogonal matrix is always $\pm1$ because rotating a parallelepiped won't change its volume. In addition, it kind of helps with the Jacobian determinant and why is the Jacobian ""necessary"" when transforming variables. Singular value decomposition - Every matrix having an SVD says the fantastical fact that any linear transformation can be considered a rotation, then a dilation (different directions by different factors), and then a rotation again. Projection matrices - Imagine an arbitrary vector's shadow onto a line or a plane. I imagine a vector collapsing onto its shadow and properties like $P^2=P$ are immediate for any projector operator $P$. Take this and run with it. My question is, can anyone point to some good reading material where a geometric interpretation of various linear algebra concepts is offered ? This could be anyone's class/teaching notes, published papers, something from recreational mathematics, or just a good book.","['matrices', 'reference-request', 'geometric-interpretation', 'geometry', 'linear-algebra']"
2678107,Moving limit into sup norm,"I need help understanding where I'm going wrong with this line of thought: Assume $f_n$ converges pointwise to $f$, so $\lim \limits_{n \rightarrow \infty}f_n(x) = f(x) \forall x \in X$, then since the suprumum norm is a norm therefore continuous we can move a limit inside, like this: $\lim \limits_{n \rightarrow \infty}\|f_n-f\|_\infty = \|\lim \limits_{n \rightarrow \infty}(f_n-f)\|_\infty = \|0\|_\infty = 0$ So all pointwise convergent sequences are also uniform convergent. This is clearly not the case so where am I going wrong?","['real-analysis', 'uniform-convergence', 'limits', 'normed-spaces', 'functional-analysis']"
2678128,Existence of distinct roots of a shifted complex polynomial,"Question: Let $f(x)\in \mathbb{C}[x]$. Does there always exist some $\alpha \in \mathbb{C}$ such that $g(x):=f(x)-\alpha$ has distinct roots? My intuition leads me to believe this is true. For a simple example, if $f(x)=x^n$ then we can set $\alpha=1$ so that the roots of $g(x)$ are the $n$th roots of unity. Proceeding via contradiction, if there were some $f(x)$ such that for all $\alpha\in \mathbb{C}$, $g(x)$ does not have distinct roots, what goes wrong? Taking this route, such a $g(x)$ would be inseparable, so for any $\alpha$ we see that $g(x)$ would always have a common root with its derivative. Can anyone find a contradiction? Is there a simple direct proof? Is the statement incorrect and can someone provide a counterexample?","['abstract-algebra', 'factoring', 'complex-analysis', 'geometry', 'analysis']"
2678176,Normal bundle of the diagonal,"The normal bundle of a smooth submanifold $X \subset Y \subset R^N$ is defined as $N(X,Y)=\{(x,v) : x \in X, v \in T_xY, v \perp T_xX\}$. I want to show that $TX \cong N(\Delta,X\times X)$, where $\Delta$ is the diagonal submanifold of $X \times X$. Intuitively to me, the points of $N(\Delta,X \times X)$, should be of the form $(x,v-v)$ but I have no idea how to show that.  I showed that the normal bundle is locally trivial, but I don't see if this can help in any way. Can someone help please?","['vector-bundles', 'differential-geometry', 'differential-topology']"
2678231,Alternating harmonic series convergence $S_n = 1-\frac12 + \frac13 - \cdots + (-1)^n\frac1n$,"Let's consider the alternating harmonic series 
$S_n = 1-\frac12 + \frac13 - \cdots + (-1)^n\frac1n$. By rearranging its terms, we get 
$S_n = (1-\frac12)-\frac14 + (\frac13-\frac16)-\frac18 + (\frac15-\frac1{10})-\cdots$. This equals to $S_n = \frac12-\frac14 + \frac16-\frac18 + \frac1{10}-\cdots$. By extracting $\frac12$ as common factor, we get: $S_n = \frac12(1-\frac12 + \frac13-\frac14 + \cdots)$.
So in essence, $S_n = \frac12 S_n$,
therefore $1=\frac12$. I have read the wikipedia article about Riemann series and roughly my understanding is that if the series converges, we can rearrange the terms and get any other number, or even to diverge.
What could be an acceptable explanation of the paradox? Obviously 1 does not equal $\frac12$! In which of the above steps lies the error?",['sequences-and-series']
2678236,Retraction onto a circle in a simplicial complex,"Let $X$ be a connected space homeomorphic to a finite simplicial complex. If there is an embedding $i: S^1 \hookrightarrow X$ which has a retract $r: X \rightarrow S^1$, then necessarily the first Betti number $b_1(X)$ is nonzero. Is the condition $b_1(X) \neq 0$ sufficient for a circle retract to exist?","['algebraic-topology', 'general-topology', 'simplicial-complex']"
2678265,Difference between membership and inclusion,"I've taken the definition of membership to be the following: Membership $A \in B: A$ is one of the members of $B$. However, I'm not sure where to make the distinction between membership and inclusion, and hence I can't wrap my head around the solutions to the following questions: ""Say whether the following are true or false"" h. $\{2\}\in\{x:x$ is a number between $1$ and $9\}$  (False) i. $\{2\}\subseteq\{x:x$ is a number between $1$ and $9\}$   (True) and similarly, n. $\emptyset\subseteq\{a,b,c\}$ (True) o. $\emptyset\in\{a,b,c\}$ (False) I am not sure why (h) and (o) are false but (i) and (n) are true, i.e. I don't see how the same element can be a subset but not a member of the same set. Is it possibly because membership is only valid between an element and a set rather than a set and a set, while inclusion is valid between a set and a set? I would appreciate any help in clarifying this, thank you.","['inclusion-exclusion', 'elementary-set-theory']"
2678273,Stochastic process $\{X_t\}_{t\geq 0} \space : \space X_t = A\sin(ωt + Θ)$ and mean values.,"Exercise : Consider two independent random variables $A,Θ$ and define the stochastic process $\{ X_t \}_{t \geq 0}$ with the formula : 
  $$X_t = A\sin(ωt + Θ)$$
  where $ω \in \mathbb R$. If the random variable $A$ follows an exponential distribution with $λ=1$ and $Θ$ a uniform distribution in $[0,2\pi]$, compute the  $E[X_t]$ and $E[X_t X_s]$, for all $s,t \geq 0$. Attempt - Discussion : Starting off determining the mean and variance of the random variables defined : $$E[A] = 1/λ = 1, \space V[A] =  1/λ^2 = 1$$ $$E[Θ] = \frac{1}{2}(0+2\pi)=\pi, \space V[Θ] = \frac{1}{12}(b-a)^2=\frac{1}{3}\pi^2$$ Now, in this case that this is a joint distribution example (considering that the expression for the stochastic process has two differently distributed random variables), how should I approach the computations of $E[X_t]$ and $E[X_tX_s]$ ? Sorry if this is a soft question but I'm a early beginner on Stochastic Processes.","['stochastic-processes', 'probability-theory', 'probability', 'probability-distributions']"
2678387,Differential equation for non-ordinary homogenous equation,"What is the solution to the differential equation:
$$y''+y^\alpha=0$$ where $\alpha\neq 1,0$? Tried finding solutions on websites with differential equation calculators but they gave nothing. So does it even exist?","['ordinary-differential-equations', 'calculus']"
2678409,"Limit as $(x,y) \to (0,0)$ of $\frac{x^2\sin(x)}{x^2 + y^2}$","I need to find
$$\lim_{(x.y) \to (0,0)}\frac{x^2 \sin(x)}{x^2 + y^2}$$ Wolfram says that this limit is undefined. However, I attempted to solve this and I got that the limit is $0$. Therefore, I'd be grateful if you could tell me where my reasoning went wrong. Since the limit is at the origin, I can apply polar coordinates: $$\lim_{r \to 0} \frac{r^2 \sin^2(\theta) \sin(r \sin(\theta))}{r^2(\sin^2(\theta)+\cos^2(\theta))} = \lim_{r \to 0}\sin^2(\theta) \sin(r \sin(\theta))$$
Now, we know that this expression $\sin(\theta) $ is bounded. Therefore $r \sin(\theta)$ approaches $0$ since $r$ approaches $0$. This entails that $\sin(r \sin(\theta)) $ approaches $0$ because $\sin(0) = 0$. Finally. $\sin^2(\theta) $ is bounded, and so the limit in questoin becomes $0$. Where is the error in my reasoning?","['multivariable-calculus', 'calculus', 'limits']"
2678410,How to understand the exponential operator geometrically?,"Consider the geometric interpretation of an orthogonal matrix, a projection matrix, a (Householder) reflector, or even just matrix-vector multiply in general. A matrix takes a vector from a vector space (after a basis has been fixed) and performs a scaling, rotation, reflection, shear, projection, or a combination of these. This can include affine transformations as well. An orthogonal matrix represents a rotation. A projection matrix represents the projection of a vector onto a subspace. A Householder reflector reflects a vector onto an axis where a coordinate can become zero. A Givens matrix is a rotation with the same effect. My question is, what is a geometric interpretation of the exponential operator ? For example, if $P$ is a projection matrix, then it represents the shadow of a vector onto a subspace. The property $P^2=P$ becomes obvious. But what is $e^P$? What does this represent geometrically? Similarly, thinking of an orthogonal matrix as a rotation, some of its properties become very obvious. It is always invertible because rotations can always be undone. It always preserves the Euclidean norm of a vector because a rotation cannot change the length of a vector, and so on. But what does the exponential of an orthogonal matrix represent? Addendum After looking at some comments and counter-comments posted below, let me clarify. After the initial setup (fields, building a vector space on top of it, selecting a basis, etc.) a matrix represents a linear transformation. If you imagine all possible linear transformations in $\mathbb{R}^3$ or $\mathbb{R}^2$, it turns out that they represent only a handful of geometric transformations such as dilation, reflection, rotation, shear, and so on. All linear transformations are some combination of these. In fact, the singular value decomposition's existence for any matrix tells us that every single linear transformation can be thought of as a rotation, dilation, followed by a rotation. For some matrices, the geometric transformation is easy to see and easy to explain. For example, an orthogonal matrix represents a rotation. Thinking about orthogonal matrices like this, it is ""obvious"" that it must always be invertible, it must always preserve the Euclidean norm, the determinant must be $\pm1$ because rotations will preserve volume, etc. A projection matrix collapses a vector onto a subspace and it becomes apparent that $P^2$ must always be equal to $P$ for any projection matrix $P$. My question is that for a given matrix $A$, what does $e^A$ represent? Does it represent something? Is there a general statement that can be made, like ""the exponential always maps a matrix to an orthogonal matrix"" or ""the exponential of a matrix is always a projection followed by a dilation"" or something? If not, then when can we say something and what can we say? For example, the exponential of any matrix is always invertible, but why? This is easy to see algebraically by what happens geometrically? Another example, the exponential of a dilation is another dilation where the new dilation factor is the exponential of the old dilation factor. Well, what if I take the exponential of a rotation? It must be a dilation/rotation/shear/reflection or some combination, but which is it in general? What about the exponential of a projection? Why and how does the exponential turn a projection into an invertible transformation even when the projection itself is not invertible? And why does the determinant of the exponential equal the exponential of the trace?","['geometric-interpretation', 'linear-transformations', 'geometry', 'matrix-exponential', 'linear-algebra']"
2678433,Cumulative distribution function for discrete distribution on $\mathbb{Q} \subseteq \mathbb{R}$.,"Consider a function $F : \mathbb{R} \rightarrow [0,1]$ with the following $3$ properties: $F$ is monotonically increasing, $F$ is right-continuous and $\lim_{x \rightarrow -\infty} F(x) = 0$ and $\lim_{x \rightarrow \infty} F(x) = 1.$ Such a function $F$ is called a cumulative distribution function. Every $F$ defines a probability distribution $P_F$ on $(\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ via 
$$ P_F((-\infty,t]) :=F(t) $$
On the other hand, every probability measure $P$ on $(\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ defines a cumulative distribution function $F_P(t)$ via $$ F_P(t) := P((-\infty,t]).$$
So we can see, that there is a bijection between all probability distributions on $(\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ and all cumulative distribution functions. Now let $(q_n)_{n \in \mathbb{N}}$ be an enumeration of all rational numbers. Then we can define a probability measure $P_{\mathbb{Q}}$ on $(\mathbb{R}, \mathcal{B}_{\mathbb{R}})$ by demanding $P_{\mathbb{Q}}(q_n) = \frac{1}{2^n}$ and $P_{\mathbb{A}} (x) = 0$ if $A \subseteq \mathbb{R} \setminus \mathbb{Q}$. My question is now: how can the corresponding cumulative distribution function $F_{P_{\mathbb{Q}}}$ be described? The question is puzzling to me; I cannot possibly imagine, that such a function can be (say) right-continuous. I am happy to hear any thoughts on this! Kind regards,
Joker","['probability-theory', 'probability', 'rational-numbers']"
2678456,Deriving the marginal of x from a uniform on the unit circle,"So this is a rather theoretical question as I kinda don't understand how to approach this from the perspective I want or how that approach is flawed (which I suspect might be the case). Let's consider the uniform distribution of the points $(x,y)$ on the unit circle. Note that it is degenerate as it has zero measure in the 2D space. We are interested in finding out what is the marginal of $x$ under this distribution. By changing to polar coordinates one can easily see that this corresponds to a uniform over the angle $\theta$. Since $x=\cos(\theta)$ by the change of variable formula we have that $$ p(x) = \frac{1}{2 \pi} \left|\frac{d x}{d \theta}\right|^{-1} = \frac{1}{2 \pi} \frac{1}{\sqrt{1 - x^2}}$$ This, of course, is wrong (a bit) since we have to take into account the fact that the cosine is not bijective so we have to split it to its to branches (where it is the same) and add them so the first factor actually is $\pi^{-1}$ and you can convince yourself this is the correct choice by the fact that $$\int_{-1}^{1} \sqrt{1-x^2} dx = \pi$$ Ok, however now I want to somehow derive that without using polar coordinates. Here is my approach and please anyone corrects me on this or help me finish it. Using the definition of the unit circle we have that the joint distribution is $$p(x,y) = \frac{1}{2 \pi} \delta (x^2 + y^2 - 1)$$
where $\delta$ is a dirac measure. Now trying to integrate that over $y$ gives me issues. Speicifically, since outside the dirac measure the function is constant it would seem that $p(x) = \frac{1}{2 \pi}$ which we know is wrong. So why is this wrong and how is the correct way to derive the result (I think I missing some knowledge of how to correctly do this integral).","['dirac-delta', 'statistics', 'calculus', 'density-function']"
2678480,Formula for a Stadium shape (2D Capsule)?,"This shape (Known as a stadium, it's basically a 2D capsule.) I need a formula to draw this shape on a graph, preferably in that orientation (on its side). What I've tried: I've tried adapting the formula for a capsule from 3D to 2D, but to no avail, I'm only able to produce a circle from it. So not really sure how to approach this problem as I've not tried mapping any shape like this to a function before. Context: Not sure how relevant this is to the question, but I'm experimenting with different camera speed-up and slow-down functions for an RTS game I'm making, and in order to tell the camera how to move, I'm trying different functions to map its speed (y axis) with time (x axis), as it is told to move or ceases being told to move. One such method I want to try is, if you divide the stadium shape into 6 sections, one section being the top left curve part, another being the bottom left curve part, another being the top middle part, etc, mapping the top left curve part followed by the top middle part as the speed time graph of the camera for acceleration, and the top middle part and the top right part for deceleration, which I could achieve by taking the magnitude of the formula to remove the bottom half. I could also just use the equation for a circle and isolate the top left, and once it reaches the peak just keep it at that value until the input is let go of, and then switch to the top right, but in searching for the stadium formula I found that it doesn't seem to be anywhere online, therefore this question would likely be useful to others searching for it.","['graphing-functions', 'geometry']"
2678486,"How to show that if $x, y, z$ are rational numbers satisfying $(x + y + z)^3 = 9(x^2y + y^2z +z^2x)$, then $x = y = z$","Let $x,y,z$ rationals Show that if $(x+y+z)^3=9(x^2y+y^2z+z^2x)$ then $x=y=z$ I tried this :
Let $x$ be the smallest variable Write $y=a+x$ and $z=b+x$ Prove $a=b=0$
 by factoring the equation as the sum of three squares.
any suggestions?","['algebra-precalculus', 'rational-numbers']"
2678528,"find the sum for each n, k","for each n, k  find the sum of $ \sum {|A_{i_1} \cup A_{i_2} \cup ... \cup A_{i_k} | } $ on all ordered sets of $ (A_{i_1}, A_{i_2}, ... ,A_{i_k}) $ where each $ A_{i_j} \subseteq \{1, ..., n\} $ I've found that the number of different terms in sigma is $ 2^{nk} $ and for $ k=1 $ got the answer of $ \sum_1^n i \binom{n}{i}$ which I'm not sure is correct I just want to know if there's an easier way to solve this or if there's a formula for each n,k",['combinatorics']
2678602,"EDİTED: Find the derivative of $f(x)=a^x$, using the definition of the derivative.","EDİTED: I want to ask MSE to confirm the correctness of the alternate solution and its mistake. My previous attempt was wrong,That's why I made a new initiative. Using the definition of the derivative I wanted to find the derivative of the function $f(x)=a^x$ . Here $a≠0,a\in \mathbb{R^{+}}$ and $a^x=e^{x \ln a}$ I wrote these: $(a^x)'=\lim_{\delta x\to 0}\frac{a^{x+\delta x}-a^x}{\delta x}=a^x×\lim_{\delta x\to 0}\frac {a^{\delta x}-1}{\delta x}$ Now,I must find $\lim_{\delta x\to 0}\frac {a^{\delta x}-1}{\delta x}$ I tried to do something: $$a^{\frac{\delta x}{a^{\delta x}-1}}=\left(1+\left(a^{\delta x}-1\right)\right)^{\frac{1}{a^{\delta x}-1}}$$ $$\lim_{\delta x \to 0} {a^{{\frac {\delta x}{a^{\delta x}-1}}}}=\lim_{\delta x \to 0} {{\left(1+\left(a^{\delta x}-1\right)\right)^{\frac{1}{a^{\delta x}-1}}}}$$ $$a^{\lim_{\delta x\to 0}{{{\frac {\delta x}{a^{\delta x}-1}}}}}=\lim_{\delta x \to 0} {{\left(1+\left(a^{\delta x}-1\right)\right)^{\frac{1}{a^{\delta x}-1}}}}$$ $$a^{\lim_{\delta x\to 0}{{{\frac {\delta x}{a^{\delta x}-1}}}}}=e$$ $$\lim_{\delta x\to 0}{{{\frac {\delta x}{a^{\delta x}-1}}}}=\log_a{e}$$ $$\frac{1}{\lim_{\delta x\to 0}{{{\frac {\delta x}{a^{\delta x}-1}}}}}=\frac{1}{\log_a{e}},\log_a{e}≠0$$ $$\lim_{\delta x\to 0} \frac {1}{ \frac{\delta x}{a^{\delta x}-1}}=\ln a$$ $$\lim_{\delta x\to 0}\frac {a^{\delta x}-1}{\delta x}=\ln a$$ I used: $\lim_{n\to 0}{n}=\lim_{\delta x\to 0}{(a^{\delta x}-1)}=a^0-1=0$ $\lim_{\delta x \to 0} {{\left(1+\left(a^{\delta x}-1\right)\right)^{\frac{1}{a^{\delta x}-1}}}}=\lim_{n\to 0}{(1+n)^{\frac 1n}}=e$ Finally, $$(a^x)'=\lim_{\delta x\to 0}\frac{a^{x+\delta x}-a^x}{\delta x}=a^x×\lim_{\delta x\to 0}\frac {a^{\delta x}-1}{\delta x}=a^x \ln a$$ Is this method/way/solution correct? Thank you!","['derivatives', 'limits', 'proof-verification', 'calculus', 'limits-without-lhopital']"
2678728,Conditional Expectation of Second Moment Poisson,"For $X_i$ iid $Poisson(\lambda)$, I'm struggling to evaluate $\mathbb{E}\big[\frac{1}{n}\sum_{i=1}^nX_i^2|\sum_{i=1}^nX_i\big]$. From this question , I know that $E(X_j|\sum_{i=1}^nX_i)=\frac{1}{n}\sum_{i=1}^nX_i$. I've also shown that $\mathbb{E}[X^2]=\lambda^2+\lambda$, but I'm not sure where to go now.","['self-learning', 'probability-theory', 'conditional-expectation', 'probability']"
2678737,Proving that the adjoint of an adjoint of an operator is the operator itself.,"I will copy the proof of the fact that if $A$ is in $\mathcal{B}(\mathcal{H})$ for a Hilbert space $\mathcal{H}$, then $A^{**} = A$. For (a) we first note that by the definition of the adjoint we have $\langle A^*x,y\rangle = \langle x,A^{**}y\rangle$ for all $x$ and $y$ in $\mathcal{H}$. Since
  also $\langle Ay,x\rangle = \langle y,A^*x\rangle$, taking conjugates we see that $\langle x,Ay\rangle = \langle A^*x,y\rangle$. Thus $\langle x,A^{**}y\rangle = \langle x,Ay\rangle$ for all $x$ and $y$, or equivalently $\langle x,A^{**}y-Ay\rangle = 0$ for all $x$ and $y$. This forces $A^{**}y = Ay$ for all $y\in\mathcal{H}$, giving (a). I understand more or less everything here except for the last part. I do not understand how $\langle x,A^{**}y-Ay\rangle = 0$ for all $x$ and $y$ ""forces"" that $A^{**}y=Ay$. Any help is appreciated.","['functional-analysis', 'adjoint-operators']"
2678757,Does the following limit exists?,"$$
\lim_{a\to 0}\left\{a\int_{-R}^{R}\int_{-R}^{R}
\frac{\mathrm{d}x\,\mathrm{d}y}
{\,\sqrt{\,\left[\left(x + a\right)^{2} + y^{2}\right]
\left[\left(x - a\right)^{2} + y^{2}\right]\,}\,}\right\}
\quad\mbox{where}\ R\ \mbox{is a}\ positive\ \mbox{number.}
$$
 The integral exists, since you can take small disks around singularities and make a change of variables. It seems that the limit should be $0$, as $\lim_{a \to 0}\left[a\log\left(a\right)\right] = 0$.","['limits', 'integration', 'calculus', 'analysis']"
2678762,Under $MA+\neg CH$ there exists a $Q$-set.,"A $Q$-set is an uncountable set of reals such that in the subspace topology all of its subsets are $F_\sigma$. Apparently, buried among more general statements and longer proofs somewhere in this paper it is proven that If Martin's Axiom holds then every set of reals of cardinality greater than $\omega$ and less than $2^\omega$ is a $Q$-set. All of the results in that paper seem much more general than I'll ever need and anyway I haven't been able to find any that seem (to me) relevant to the present problem. I'm wondering if anybody has a reference to where I can find this result or knows how to prove it. I actually only need the existence of a $Q$-set, not the stronger result stated here. Thank you.","['descriptive-set-theory', 'general-topology', 'set-theory']"
2678771,An inequality with Sobolev norm.,"Let $\Omega\subset\mathbb{R}^{n}$ be a smoothly bounded
domain. Is the following statement true? There exists a positive constant $C$ such that 
$$
\intop_{\Omega}\dfrac{\left|f\left(x\right)\right|^{2}}{\delta\left(x\right)}dx\leq C\left\Vert f\right\Vert^{2} _{W^{1,2}},
$$
for any $f\in C_{c}^{\infty}\left(\Omega\right)$. Here $\delta\left(x\right)$
denotes the distance from $x$ to the boundary of $\Omega$.","['functional-analysis', 'real-analysis', 'integral-inequality']"
2678774,Prove function is not differentiable even though all directional derivatives exist and it is continuous.,"In the following link , the function below is provided as an example of a function being continuous and all directional derivatives existing. Yet, it is not differentiable. How do I prove that this is not differentiable? $$
f(x,y)= 
\begin{cases}
    \frac{y^3}{x^2+y^2},& \text{if } (x,y) \neq (0,0)\\
    0,              & \text{otherwise}
\end{cases}
$$","['derivatives', 'continuity', 'calculus', 'limits']"
2678783,Can Ricci flow develop singularity if metric is bounded?,"Suppose I have a Ricci flow $(M, g(t))$ on $[0, T)$ can it develop a singularity if the metrics $g(t)$ are uniformly bounded? i.e $C^{-1}g(0)\leq g(t) \leq Cg(0)$ for all $t\in [0,T)$. In the Kahler case, I think if you can control the metric, you can control the curvature and then by Shi's theorem you can extend the flow past time $T$, but is there similar statement in the Riemannian case? If not, is there a counterexample?","['ricci-flow', 'riemannian-geometry', 'differential-geometry']"
2678805,How can we come up with the definition of natural logarithm?,"I learned calculus for 2 years, but still don't understand the definition of $\ln(x)$ $$\ln(x) = \int_1^x \frac{\mathrm d t}{t}$$ I can't make sense of this definition. How can people find it? Do you have any intuition?","['intuition', 'real-analysis', 'logarithms', 'calculus', 'definition']"
2678852,Number of steps the path-avoiding snail must take before a step size of $(2n - 1)/2^k$?,"Suppose the path-avoiding snail walks along the grid according to the following algorithm: At each step, the snail steps unit distance if doing so will not collide with its trail. If a step of unit distance will collide with the trail, the snail moves half way toward the trail. After each step, the snail turns right or left or remains straight. Let $a(n)$ be the minimal number of steps required before the snail can have a step size of $(2n-1)/2^k$ for some $k$ such that the fraction is reduced. Here are a few examples of minimal walks (solved by brute force): This shows that you can have a step size with numerator $3$ in $7$ steps, numerator $5$ in $9$ steps, numerator $7$ in $8$ steps, and numerator $9$ in 10 steps—therefore $a(2) = 7$, $a(3) = 9$, $a(4) = 8$ and $a(5) = 10$. Is there a general strategy of determining $a(n)$? Also, given a random walk (where the turns are chosen uniformly at random) what is the expected number of steps before the snail takes a step of $(2n-1)/2^k$ for some $k$? Known values (assuming the implementation is correct): a(2) = 7      a(9)  = 11    a(16) = 10    a(23) = 12    a(30) = 13
a(3) = 9      a(10) = 12    a(17) = 12    a(24) = 13    a(31) = 12
a(4) = 8      a(11) = 11    a(18) = 13    a(25) = 12    a(32) = 11
a(5) = 10     a(12) = 12    a(19) = 14    a(26) = 13    a(33) = 13
a(6) = 11     a(13) = 11    a(20) = 13    a(27) = 14    a(34) = 14
a(7) = 10     a(14) = 12    a(21) = 13    a(28) = 13    a(35) > 14
a(8) = 9      a(15) = 11    a(22) = 14    a(29) = 12 Here are some walks for $n=20$, $n=21$, and $n=22$: n = 20: (0,0)--(1,0)--(1,1)--(0,1)--(0,1/2)--(0,1/4)--(1/2,1/4)--(1/2,5/8)--(1/2,13/16)--(1/4,13/16)--(1/4,17/32)--(1/4,25/64)--(1/8,25/64)--(1/8,89/128)
n = 21: (0,0)--(1,0)--(1,1)--(0,1)--(0,1/2)--(0,1/4)--(1/2,1/4)--(1/2,1/8)--(3/4,1/8)--(3/4,9/16)--(3/4,25/32)--(3/4,57/64)--(3/8,57/64)--(3/8,73/128)
n = 22: (0,0)--(1,0)--(1,1)--(0,1)--(0,1/2)--(0,1/4)--(1/2,1/4)--(1/2,5/8)--(1/4,5/8)--(1/4,7/16)--(1/4,11/32)--(1/8,11/32)--(1/8,43/64)--(9/16,43/64)--(9/16,43/128)","['random-walk', 'combinatorics', 'extremal-combinatorics']"
2678876,Non-normal Bivariate distribution with normal margins,"I was working on a question and it stated the following: Now I am supposed to show that the bivariate distribution is non normal even though the marginal distributions are. I generated 1000 iid variates for X1 and X2 graphed them in the following: I know that the bivariate distribution is non-normal graphically but someone told me that we can show this analytically which I do not follow. Let $Z=X_1+X_2$, If $(X_1,X_2)$ was bivariate normal then $Z$ would also be normal, but $P(Z=0) \ge  P(|X_1| \le 1) \approx 0.68$ i.e. It has positive mass at zero, which cannot be the case with a continuous distribution. I believe the probabilities come from the way we defined $X_1$ and $X_2$ but I do not really see it. Why is $P(|X_1| \le 1) \approx 0.68$ And then why is $P(Z=0) \ge  P(|X_1| \le 1)$","['statistics', 'probability', 'bivariate-distributions', 'probability-distributions']"
2678895,"Function which creates the sequence 1, 2, 3, 1, 2, 3, ...","I was wondering how to map the set $\mathbb{Z}^+$ to the sequence $1, 2, 3, 1, 2, 3, \ldots$. I thought it would be easy, but I was only able to obtain an answer through trial and error. For a function $f \colon \mathbb{Z}^+ \rightarrow \mathbb{Z}$, we have that $f(x) = x \bmod 3$ gives the numbers $1, 2, 0, 1, 2, 0, \ldots$ $f(x) = (x \bmod 3) + 1$ gives the numbers $2, 3, 1, 2, 3, 1, \ldots$ After a bit of experimenting, I finally found that $f(x) = ((x + 2) \bmod 3) + 1$ gives the numbers $1, 2, 3, 1, 2, 3, \ldots$ More generally, if I want to map the set $\mathbb{Z}^+$ to the sequence $\{1, 2, \ldots, n, 1, 2, \ldots, n, \ldots\}$, I need to use the function $$f(x) = ((x + n - 1) \bmod n) + 1$$ I was only able to come to this result by trial and error. I was not able to find a solution to this relatively simple question online (although perhaps my search terms were off). How would one come to this result in a more systematic way?","['modular-arithmetic', 'sequences-and-series', 'functions', 'discrete-mathematics']"
2678999,Group of connected components of a Neron Model of an elliptic curve,"I'm struggling a little bit with the definition of the connected component group of neron models. Let $E/K$ be an elliptic curve and $K$ a p-adic local field with residue class field $\kappa$. Moreover, denote by $K^{unr}$ the maximal unramified extension of $K$. I would like to know the size of the group of connected components. What I have read so far is that I can view it as the quotient $E(K^{unr})/E^0(K^{unr})$ but I'm still not really sure about this. And is the quotient the same as the finite etale group scheme $A_\kappa/A^0_\kappa$ over $\kappa$ where $A$ is the Neron model of $E$ over the ring of integers of $K$? If someone could just assure me that this is right would be great, thanks!","['algebraic-topology', 'elliptic-curves', 'algebraic-number-theory', 'algebraic-geometry']"
2679000,"Weak convergence in $L^2[0,1]$","In $\ell^2$, a sequence converges weakly if it is bounded and each component converges. My question is: How can I think about weak convergence in $L^2[0,1]$?","['functional-analysis', 'weak-convergence', 'hilbert-spaces']"
2679115,Formula for determinant of sum of matrices,"Some time ago I came across this apparently quite obscure formula that expands the determinant of a sum of two matrices that I had put on my notes (assuming that I made no errors in my writing): $$\det(A+B)=\det(A)+\det(B)+\text{Tr}(\text{adj}(A)B)$$ Where $\text{adj}()$ denotes the adjugate of the matrix. I cannot seem to find any mention of this formula online. Does anyone know of the name (and maybe a proof) of it? Furthermore, is there any more info on it, like conditions that $A$ and $B$ must obey for it to hold?","['matrices', 'linear-algebra', 'determinant']"
2679118,"Assuming $\sum n a_n$ converges, does it follow that $\sum a_n$ converge? [duplicate]","This question already has answers here : Is there a sequence such that $\sum{a_n}$ diverges but $\sum{na_n}$converges? (2 answers) Closed 6 years ago . Let us assume that the series 
$$\sum n a_n$$
converges. Does it follow that the series
$$\sum a_n$$
converge ? In case the series were positive, it's trivial using the comparison test. What can we say in the general case ?","['sequences-and-series', 'convergence-divergence']"
2679142,Once Continuously Differentiable?,"I'm hoping someone can clarify this because I can't seem to find a solid answer. I know functions can be continuously differentiable, but I just read in a textbook ""this applies to once continuously differentiable..."" but they don't give an example and google seems unhelpful. Am I right in assuming that once continuously differentiable is equivalent to continuously differentiable? If not can you please provide an example that clearly demonstrates the difference.","['derivatives', 'real-analysis']"
2679153,On a condition for skew-symmetry,Let $A\in\mathbb{R}^{n\times n}$ be a generic lower triangular matrix and let $P\in\mathbb{R}^{n\times n}$ be a symmetric positive definite matrix. True or false. Does $AP + PA^\top=0$ imply $AP=0$?,"['matrices', 'matrix-decomposition', 'matrix-equations', 'linear-algebra']"
2679220,Cesaro-Stolz limit,"We have $x_{n+1}=x_n+\dfrac{\sqrt n}{x_n}$ with $x_1=1$ and we have to compute $\displaystyle\lim_{n\to\infty} \dfrac{x_n}{n^{3/4}}$. I already proved that $x_n$ is strictly increasing and tends to infinity and I know that we can use the Cesaro-Stolz Lemma to solve it but it hasn't been successful for me. I tried to apply it for $\displaystyle\lim_{n\to\infty} \dfrac{(x_n)^4}{n^3}$, but got stuck.","['recurrence-relations', 'real-analysis', 'sequences-and-series', 'limits']"
2679241,Proof for known values of the Hermite constant,"I understand that the values of the Hermite constant for $1 \leq n \leq 8$ and $n=24$ have been determined exactly. For example, Lagrange proved for $n=2$ the value of the Hermite constant is $\gamma_n = \sqrt{\frac{4}{3}}$, and this value is achieved with the unique extremal form $$
q(x,y) = x^2 + xy + y^2.
$$ However, I can't find proof in any literature for any values of the Hermite constant. Can anybody direct me towards some proof of any values of the Hermite constant? Edit: For context, let $f: \mathbb{R}^n \to \mathbb{R}$ be a quadratic form, i.e. for $\mathbf{x} = (x_1, \cdots, x_n) \in \mathbb{R}^n$ then $f(\mathbf{x}) = \sum_{ij} f_{ij} x_i x_j$. Then we define the Hermite variable in $n$ dimensions: $$
\gamma_n(f) = \frac{\inf_{\mathbf{x}}\{f(\mathbf{x}): \mathbf{x} \in \mathbb{Z}^n - \{\mathbf{0}\}\}}{disc(f)^{1/n}}.
$$ Then the Hermite constant in $n$ dimensions is the maximal value of this variable over all possible quadratic forms, i.e. $$
\gamma_n = \sup_{f}\{\gamma_n(f)\}.
$$","['number-theory', 'geometry-of-numbers', 'algebraic-number-theory']"
2679258,Product of sheaf of ideals,"Let $(X, O_X)$ be a ringed space, $I$ be a sheaf of ideals on $(X, O_X)$ and let $F$ be an $O_X$-module.
I have two questions. a) Is there an example where the presheaf $P : U \mapsto I(U) \cdot F(U) \leq F(U)$ is not a sheaf on $X$ ? b) Is it correct that we define the product $I \cdot F$ as being the image of the sheaf morphism 
$$I \otimes_{O_X} F \to O_X \otimes_{O_X} F \cong F,$$
and if so, do we necessarily have that $I \cdot F$ is isomorphic (as sheaf of $O_X$-module) to the sheafification of $P$ ? According to this question , this is true but the comment has no further explanation as to why this is true. For a), if I consider a collection of pairwise compatible sections $s_U \in P(U)$, we can write
$$s_U = \sum_{i=1}^{n_U} a_i^U b_i^U$$
with $a_i^U \in I(U), b_i^U \in F(U)$, but I don't see how the conditions $s_U\vert_{U \cap V} = s_V\vert_{U \cap V}$ could help to glue the various $s_U$ together. Thank you for your help.","['modules', 'sheaf-theory', 'algebraic-geometry', 'ideals']"
2679275,"Evaluate $\lim\limits_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}$","Evaluate $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}$$ $$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{x}=\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}y^2$$ now 
$$\lim_{(x,y)\to (0,0)}\frac{\sin(xy^2)}{xy^2}\cdot \lim_{(x,y)\to (0,0)} y^2=1\cdot 0=0$$ Is there a problem with this calculation regarding the where the function is defined? In general if we look at $(x,0)\to(0,0)$ or $(0,y)\to(0,0)$ are those iterative limits?",['multivariable-calculus']
2679304,Why does the subspace need to go through the origin?,"I understand that the main difference between a subspace and a hyperplane is that the subspace must go through the origin. Why does need to happen? In other words, why does a subspace always have to go through the origin? What restricts it from doing otherwise?",['linear-algebra']
2679385,Show that $\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{2n}{4n}-1}$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question For any real matrix $A$ of dimension $3$ and for any $n \geqslant 2$, show that$$\det(A-I_3)^2+ \det(A-2I_3)^2 +\cdots+\det(A-2nI_3)^2\geqslant\frac{\det A^2}{\binom{4n}{2n}-1}.$$ I know that $\det(A-xI_3)$ is equal to zero polynomial from Cayley-Hamilton, but I do not know what I should use to deal with the binomial and to solve it. Any help please? Thanks in advance.","['matrices', 'linear-algebra']"
2679387,Finding when a double integral is convergent,"Given is the integral 
$$\iint_{\mathbb{R}^2} \frac{1}{(1+x^2+y^2)^k}\,dx \, dy$$
the question asks for the values of $k$ for which the integral will converge, and in turn find the value which the integral converges to. Using $k=1$ shows that it diverges, but I'm not sure how I should go about finding the values for which it converges. Thanks in advance for any help.","['multivariable-calculus', 'improper-integrals', 'integration', 'calculus']"
2679393,"Set theory bracket notation, what is excluded $X=\{\emptyset,\{\emptyset\},\{\{\emptyset\}\}\}$ and $Y=X\setminus\{\{\emptyset\}\}$","If $X=\{\emptyset,\{\emptyset\},\{\{\emptyset\}\}\}$ and $Y=X\setminus\{\{\emptyset\}\}$ then what element is excluded from $X$? Is it $\{\{\emptyset\}\}$, or $\{\emptyset\}$? In a similar vein, if $Z=\{a, b, c\}$, does it make sense to say $Z\setminus a$? Thanks","['notation', 'elementary-set-theory']"
2679412,Clarification for the meaning of arc lenght and area forms,"Let $C$ be an oriented regular curve and $ds$ its arc lenght. If $\alpha:I\to C$ is a parametrization compatible with the orientation of $C$, then $$\alpha^*(ds)=\|\alpha'\|dt$$
  Let $S\subset\mathbb{R}^3$ be an oriented regular surface and $dA$ its area form. If $\psi:U\subset\mathbb{R}^2\to S$ is a parametrization compatible with the orientation of S then $$\psi^*(dA)=\|\psi_u×\psi_v\|du \wedge dv$$ I (more or less) understand forms on open subsets of $\mathbb{R}^n$ and their pullback and I just started with forms on manifolds. I don't think I understand what is the meaning of $ds$ and $dA$ in those statements. I thought $ds$ should be the 1-form of $C$ whose $p-$component ($p\in C$) is $(\alpha'(q))^*$, where $p=\alpha(q)$. So, the pullback of $ds$ by $\alpha$ should be a 1-form on $I$ whose $q-$component acting on a vector of the tangent space of $I$, which in this case is just some $x \in \mathbb{R}$, is $ds_p=(\alpha'(q))^*$ acting on $\alpha'(q)x$, which is simply $$(\alpha'(q))^*[\alpha'(q)x]=x(\alpha'(q))^*[\alpha'(q)]=x$$
So maybe I'm missunderstanding the meaning of $ds$ (and similarly with $dA$) How should I interpret those differential forms?","['multivariable-calculus', 'differential-forms', 'smooth-manifolds', 'differential-geometry']"
2679447,Real analytic non-holomorphic function from the $\mathbb{C}$ to $\mathbb{C}$,"This is going to be completely obvious, but I can't seem to get a satisfying answer on my own. Any help would be much appreciated. I thought I understood the definitions of complex vs real analytic functions, but an example I found has proved me wrong. What does it mean for a function to be a real analytic non-holomorphic function from  $\mathbb{C} \rightarrow \mathbb{C}$. It seems to imply having a series expansion but I don't understand how that doesn't make it holomorphic.",['complex-analysis']
2679451,"Showing that the Brownian bridge $X_{[0,1]}$ is simply Brownian motion $B_{[0,1]}$ conditioned on $B_1=0$","I'm currently trying to prove Exercise 1.19 in Liggett's Continuous Time Markov Processes , which asks me to prove the following. Let $(B_t)_{t\ge0}$ be a Brownian motion, and let $X_t=B_t-t B_1$ for $0\le t\le 1$. Then show that for $0<t_1<\cdots<t_n<1$, the distribution of $(B_{t_1},\ldots,B_{t_n})$ conditioned on $|B_1|\le\varepsilon$ converges to the distribution of $(X_{t_1},\ldots,X_{t_n})$ in law. Now, I'm relatively familiar with the characterization of Gaussian processes, and methods of proving convergence in law (by looking at the characteristic functions for example) but I have very few tools at my disposal for proving convergence in law of a conditional distribution. Furthermore, I have seen the problem stated before as showing that $B_{[0,1]}$ conditioned on $B_1=0$ is simply $X_{[0,1]}$ up to finite-dimensional distributions — but in what sense does it make sense to condition on $\{B_1=0\}$? Is this just notation for the law conditioned on $| B_1|\le\varepsilon$ as $\varepsilon\to\infty$ or is there a larger idea or theory here?","['stochastic-processes', 'probability-theory', 'brownian-motion']"
2679509,Extension of places to an algebraic closure,"Let $K$ be a number field and $v$ a place of $K$. One often extends $v$ to a place $\overline{v}$ of an algebraic closure $\overline{K}$ of $K$, which is equivalent to choosing an embedding $\overline{K} \hookrightarrow \overline{K_v}$, where $\overline{K_v}$ denotes an algebraic closure of the completion $K_v$. However, I haven't found an explanation of how this extension process is done concretely. Can you help me on this one?","['number-theory', 'algebraic-number-theory', 'p-adic-number-theory']"
2679516,Limit of Recursive Sequence $y_{n+1}=y_n+\frac{1}{ny_n}$ with $y_1=1$,"I came up with this sequence as I was playing around with another question on this site. And so I decided to ask the users if they can find a solution. Let $$y_1=1$$
$$y_{n+1}=y_n+\frac{1}{ny_n}.$$ Evaluate $$\lim\limits_{n\to \infty} y_n \leq \infty.$$","['recurrence-relations', 'sequences-and-series', 'limits']"
2679521,"If $AA^T$ is a diagonal matrix, what can be said about $A^TA$?","I am trying to answer this question and any method I can think of requires a knowledge of $A^TA$ given that $AA^T=D$, where $D$ is diagonal and $A$ is a square matrix. I could not find anything useful in MSE or elsewhere and I was unable to do any progress by myself.","['matrices', 'matrix-congruences', 'linear-algebra']"
2679545,$\frac{\partial^2 u}{\partial x^2}=\frac{\partial^2 u}{\partial t^2}\implies \frac{\partial^2 v}{\partial \xi\partial \eta}=0$,"I'm reading ""Fourier analysis, an introduction"" by Elias M. And Rami Shakarchi and in chapter one it said: The function $u(x,t)$ satisfied the equation $$\dfrac{\partial^2 u}{\partial x^2}=\dfrac{\partial^2 u}{\partial t^2}$$ Now let's set $2$ new variables: $\xi=x+t$ and $\eta=x-t$. We now define the function $v$ such that $v(\xi,\eta)=u(x,t)$. I understand this but in the book from this they conclude $2$ things that I'm failed to understand how: $$\dfrac{\partial^2 v}{\partial \xi\partial \eta}=0$$
And $$v(\xi,\eta)=F(\xi)+G(\eta)$$($F,G$ are twice differentiable functions) How from the change of variables they got to this? I would be happy with either a hint or an answer, thank you very much.","['multivariable-calculus', 'change-of-variable', 'partial-derivative']"
2679578,How can I solve this find the digits problem more rigorously?,"Adjoin to the digits $739$ three more digits so that the resulting number $739 \text{_ _ _}$ is divisible by $6, 7, 8$, and $9$. I can do some quick guess and check as well as some little tricks with $9$, $7$, $6$, and $8$ to arrive to an answer, but how could I show this with more rigor (like more concrete number theory)? Answers: $739368$ and $739872$","['number-theory', 'modular-arithmetic']"
2679616,"Proving if $X_n$ are uniformly integrable and $X_n \Rightarrow X$, then $EX_n \to EX$","Below is the proof from Billingsley's Convergence of Probability Measures. However, in the proof, I don't understand the final step. That is, we want to show that$$
\int_0^\alpha P[t<X_n<\alpha] \,\mathrm{d}t \to \int_0^\alpha P[t<X<\alpha] \,\mathrm{d}t
$$
using the bounded convergence theorem. However, to use the bounded convergence theorem, we need that $P[t<X_n<\alpha]$ converges pointwise to $P[t<X<\alpha]$. But the function used here, $\mathbb{1}_{(t,\alpha)}(x)$ is not a continuous function. So how do we show pointwise convergence here? And what does $P[X=\alpha]=0$ have to do here?","['weak-convergence', 'probability-theory']"
2679621,Absolute convergence of $\sum_{n=1}^{\infty}z_n^2$,"There are two complex series given ($z \in \mathbb{C}$. We know that: 
$$\sum_{n=1}^{\infty}z_n^2$$
converges absolutely. We are to show that 
$$\sum_{n=1}^{\infty} \frac{z_n}{n}$$
also converges absolutely. I tried to prove it with no success unfortunately. How should the proof look like?","['complex-analysis', 'absolute-convergence', 'sequences-and-series', 'convergence-divergence']"
2679630,"If $f''(x)$ exist in $(a, b)$ then there exist $\xi \in (a, b)$ so that $f''(\xi)(b-a)=f'(b)-f'(a)$","Let $f$ be differentiable on $[a, b]$ and $f'$ is differentiable on $(a, b)$. How can we prove that there exists $\xi \in (a, b)$, such that $f'(b)-f'(a)=f''(\xi)(b-a)$ ? We cannot use the mean value theorem because $f'(x)$ may have discontinuities at $a$ or $b$. I came up with this: If $f''(\xi_1)>\frac{f'(b)-f'(a)}{b-a}$ and $f''(\xi_2)<\frac{f'(b)-f'(a)}{b-a}$ for some $\xi_1, \xi_2$, then by Darboux's theorem there will be $\xi$, such that $f''(\xi)=\frac{f'(b)-f'(a)}{b-a}$. Therefore we can assume $f''(x)>\frac{f'(b)-f'(a)}{b-a}$ or $f''(x)<\frac{f'(b)-f'(a)}{b-a}$ for all $x$. Am I going the right direction?","['derivatives', 'real-analysis']"
2679666,How many valid mazes of some size are there?,"A square grid of size $n\times m$ is given, where each tile can either
  be a floor tile or a wall tile. One corner tile represents the entrance , and the opposite corner
  tile of that is called the exit . These two tiles cannot be a
  wall tile and are classified as a floor tile. A grid configuration is a valid maze if both properties hold: Entrance and exit tiles are connected. That is, one should be able to move from the entrance to the exit tile by only moving up, down,
  left or right at each step, walking over floor tiles. No floor tile is fully enclosed by wall tiles. That is, starting at any floor tile, one should be able to visit every other floor tile by
  moving only in those four directions, on floor tiles. Let there be only one unique entrance-exit configuration, so one can
  assume that the entrance tile is at the bottom left corner and that
  the exit tile is at the upper right corner How many valid mazes are there for a grid of size $n\times m$?
  ($n,m\ge2$) That is, how can we find $N(n,m)$ - number of valid mazes as descrbed above? Trivially, we have a simple bound; $$N(n,m)\le \sum_{k=0}^{nm-2-(n+m-3)}\binom{nm-2}{k}\lt2^{nm-2}$$ Since the grid has $(nm-2)$ tiles that can either be a wall or floor, and at least $(n+m-3)$ of those tiles must be floor tiles, or we would not have a connected path from entrance to exit. But I'm not sure how to count all of the invalid mazes that have unreachable floor tiles. I've tried observing cases split into sets each with cases with $k$ walls to see how many invalid cases I can count depending on $k$ and $m,n$, but wasn't able to find a general pattern, nor was sure about it as I kept noticing I was missing cases or counting some more than once as I was not sure how to do it properly.","['combinations', 'combinatorics', 'recreational-mathematics']"
2679695,"An integral of a positive function over a ""small"" set is ""small"".","Consider a measure space $(X,\mathfrak{A},\mu)$, $f$ an integrable function and $f \geq 0$. Show that for every $\epsilon > 0$ there exists a $\delta > 0$ such that for every $A \in \mathfrak{A}$ if $\mu(A)<\delta$ then $\int_A f d\mu < \epsilon$. I am given a hint which is to consider $f_n(x) = \min\{n, f(x)\}$. I do this and given that $f_n \leq f_{n+1}$ I can use the Beppo-Levi theorem and conclude that $\displaystyle \int_A f d\mu = \lim_{n \to \infty} \int_A f_n d\mu$. If $f$ is bounded I can further conclude that there exists M such that $\displaystyle\lim_{n \to \infty} \int_A f_n d\mu \leq \int M d\mu = M\mu(A)$ and take $\delta = \frac{\epsilon}{M}$. I don't really know what to do if $f$ is not bounded.","['integration', 'measure-theory']"
2679726,Giving sense to a sum over an uncountable set of elements which are equal to zero,"Let $(X_i)$ be a simple random walk in $\mathbb{Z}$ starting from the origin. Let $\Sigma$ be the set whose elements are $(Y_i)_{i \in \mathbb{N}}$, which are infinite random walk trajectories. Consider an event $\mathcal{A} \subset \Sigma$.
We have that, since we sum over the probability of disjoint events,
$$
P(A) = \sum\limits_{(Y_i)_{i \in \mathbb{N} }\in \mathcal{A}} P(X_i = Y_i \, \, \, \forall i \in \mathbb{N} ).
$$ Now the sum on the right hand side is uncountable and each probability on the right hand side equals zero. So does the expression make sense? How is it possible to give sense to it? Should I write it as an integral over all possible trajectories?","['random-walk', 'probability-theory', 'functional-analysis', 'measure-theory', 'sequences-and-series']"
2679747,Check of Laplace transform,"This afternoon I solved a ODE with a Laplace transform. Now I want to check if the solution whether it is correct, but I cannot get to the solution.
The ODE is:
$$\ddot{x} +4x = f(t), x(t=0)=3, \dot{x}(t=0)=-1$$
As a solution I found:$$ {x(t) =3\cos(2t)-\frac 12\sin(2t)+  \frac 12\int_0^t f(\tau) \, \sin(2(t-\tau))d\tau}$$ The problem is: How do I check the last part (under the integral?","['ordinary-differential-equations', 'laplace-transform']"
2679757,Why is the probability of picking an odd number from the set of natural numbers not $\frac{1}{2}$?,"Why is the probability of picking an odd number from the set of natural numbers not $\dfrac{1}{2}$? Could anyone explain it to me in simple terms? I am only curious about the reason why and that's the reason why I asked for a ""simple"" explanation. I remember my teacher mentioning that it is because the set is infinite. Is that right? can someone elaborate?",['probability']
2679760,Numerical evaluation of the period of a limit cycle,"How can I calculate all the periods of the limit cycle of the Ueda-Duffing equation with forcing: $\ddot{x} + k \dot{x} + x^3 = B \cos(t) $ for each set of parameters $(k, B)$ ? Edit: The equation exhibits sub-harmonic resonance for some sets of parameters (and chaotic behaviour too). 
E.g. for $k=0.08, B=0.2$ there are 5 coexisting attractors  of period $2n\pi  $ with $n=1, 2, 3$","['numerical-methods', 'ordinary-differential-equations', 'nonlinear-system']"
2679769,"$ya^n - by = 1$ has one solution, then $xa-bx = 1$ also has one solution","Let $(R, +, \cdot)$ be a ring and $a,b \in R, n \in \mathbb{N}^*$ such that $b^2 = b$ and the equation $ya^n - by = 1$ has one solution. Prove that the equation $xa - bx = 1$ also has one solution. Let $y$ be the solution of the equation $ya^n - by = 1$ . By multiplying the equation $ya^n - by = 1$ with $b$ we obtain: $$bya^n - by = b \iff bya^n - ya^n = b-1 \iff (b-1)ya^n = b-1.$$ If $(b-1)$ is inverible, then $ya^n = 1$ , so both $y$ and $a^n$ are invertible. But $ya^n - by = 1 \iff by=0 \implies b = 0$ . This means that the equation $xa - bx = 1$ has the solution $x = ya^{n-1}$ . I just don't know how to proceed if $(b-1)$ is not invertible.","['abstract-algebra', 'ring-theory']"
2679776,"Let $G$ be a group of order $2^tk$, then prove that elements of odd order form a subgroup","Actually, this question already has multiple answers on this website: using Burnside's theorem and one with induction and $p$-Sylow groups . I'm asking this question here, however, because the exercise below appears in my group theory syllabus in the third chapter, with only the following topics covered: definition of groups, many examples, subgroups, direct product, homomorphisms, generators, order, index. Thus I don't understand the two answers I found on this site and I'm looking for a more elementary approach using the topics included in the first three chapters of my syllabus. So I'm asked the following. Let $G$ be a finite group of order $2^tk$, $\ t,k\in\mathbb{Z}$, $k$ odd and suppose that $G$ has an element of order $2^t$. Prove that the elements of $G$ of odd order form a subgroup of order $k$ and index $2^t$ in $G$. Everything I tried so far led me nowhere and it does not contribute anything to show this here. I hope anyone can be give me a hint or (partial) proof to get me going!","['finite-groups', 'group-theory']"
2679783,Under what conditions a rational function has bounded derivative?,"Under what conditions a rational function has bounded derivative? This question arise to me when considering the following theorem: If $f \in C^1(I,\mathbb{R})$ where $I$ is an interval then: $f$ is globally lipschitz $\iff \exists L \ge 0.\forall
 t \in I.|f'(t)| \le L $ So taking rational function $f(x) = \frac{p(x)}{q(x)}$ we have $f'(x) = \frac{p'(x)q(x)-p(x)q'(x)}{q(x)^2}$. My view I think I should assume that $f:\mathbb{R} \to \mathbb{R}$ so that $\forall x \in \mathbb{R}.q'(x) \neq 0$ (however this doesn't seem to be necesary). And then perhaps a condition on the degree guarantees boundedness...","['real-analysis', 'calculus', 'rational-functions', 'lipschitz-functions', 'ordinary-differential-equations']"
2679814,"$p,q,r$ three projections; show that if $p+q+r = 0$, then $p =q =r =0$","Let $\mathbb F$ be a field of zero characteristic. Let $E$ be a $\mathbb F$-vector-space. Let $p,q,r$ be three projections of $E$. Prove that if $p+q+r = 0$, then $p =q =r =0$. The case of finite dimension is really easy using the trace (since the rank or a projection is equal to its trace). How to deal with the case of infinite dimension? Some thoughts: One can see that $p(E) \cap q(E)= \{0\}$ since $-2$ is not an eigenvalue for $r$. Likewise, $p(E) \cap r(E)= q(E) \cap r(E) =\{0\}$. Otherwise, $\ker p \cap \ker q \subset \ker r$ $\ker p \cap \ker r \subset \ker q$ and $\ker q \cap \ker r \subset \ker p$ ...","['linear-algebra', 'vector-spaces']"
2679894,What is the proof behind $\lim f(g(x)) = f(\lim g(x))$?,In general I see that we can evaluate $\lim f(g(x)) = f(\lim g(x))$ if $f$ is continuous. How do we know this? How do we prove this? I tried looking at it with the epsilon-delta definition of a limit but I didn't get very far since I don't know how you even begin to analyze $f(\lim g(x))$.,"['continuity', 'epsilon-delta', 'calculus', 'limits']"
2679900,Analytically finding the value of $n$ for which $n \prod\limits_{k=0}^{n-1}\frac{60-k}{60}$ is maximized,"I was solving a problem where I needed to find the value of $n\in [0,60] \cap \mathbb N$ for which $$f(n)=n \prod_{k=0}^{n-1}\frac{60-k}{60}$$ is maximized. Somewhat irresponsibly, I computed the value of the expression for $n=1,2,...,9$, saw that it decreases after $n=8$ and assumed that $n=8$ gives the maximum value. Not only does this assume that the value of $f$ ""increases then decreases,"" allowing only one candidate for a maximum, but it also doesn't shed any light on how I would solve the question or graph the function in the general case. Therefore, I have 3 questions: (1) How would I find the maximum (or maxima, if unsure of how many extrema there would be) analytically? (2) Is it trivial to show that $f$ ""increases then decreases""? (3)  How might I sketch $f$ (by hand) ? Can the answers for the above questions be generalized for the general case function $g(n)=n \prod_{k=0}^{n-1}\frac{\alpha-k}{\alpha}$ for $n\in [0,\alpha] \cap \mathbb N$?","['graphing-functions', 'products', 'discrete-mathematics']"
2679948,Where to find informations for projective planes over algebraic closed fields like $\bar{Q}$,"The title really says it all: where can I find some informations about projective planes over algebraically closed fields like $\mathbb{P}^2_{\bar{\mathbb{Q}}}$ ? Here $\bar{\mathbb{Q}}$ stands for the algebraic closure of the field of rational numbers. I mean, for $\mathbb{C}$ there are well known references, but here for this particular case above I can't find something useful. I'm interested in particular in the study of algebraic curves in these spaces.","['reference-request', 'projective-space', 'algebraic-number-theory', 'algebraic-geometry']"
2679963,"Derivative of $d(x,A)$","For $A \subseteq \mathbb{R}$, define $$d(x, A) = \inf_{y \in A}|x-y|.$$
Let $A \subseteq \mathbb{R},$ define $f : \mathbb{R} \rightarrow [0,\infty)$ by $$f(x) = d(x, A).$$ Then I can show that, for any $x, y$, $$|f(x) - f(y)| < |x - y|.$$ So $f$ is Lipschitz which yields that $f$ is of bounded varition. So $f'$ exists almost everywhere. I need to show that for $x$ such that $f'(x)$ exists, then $f'(x) \in \{-1, 0, 1\}.$ $\textbf{Attemp}$ I know that $f(x) = 0 \leftrightarrow d(x, A) = 0 \leftrightarrow x \in \ \mbox{cl} \ A$ where $\mbox{cl} \ A$ is the closure of the set $A$. Since $$ \frac{|f(x) - f(y)|}{|x - y|} \leq 1$$ for any $x,y \in \mathbb{R} ,$ then $|f'(x)| \leq 1.$ Specifically, $$f'(x) = \lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} .$$ If $x \in \ \mbox{cl} \ A,$ then $f(x) = 0.$ Then there exists a sequence $\{x_n\}$ such that $x_n \in A$ and $x_n \rightarrow x.$ So $f(x_n) = 0$ for all $n$. This yields that $f(x) - f(x_n) = 0$ for all $n$. Since $f'(x)$ exists, then $$\lim_{y \rightarrow x} \frac{f(x) - f(y)}{x-y} = \lim_{n \rightarrow \infty} \frac{f(x) - f(x_n)}{x - x_n} = 0.$$ Now $x \not\in \ \mbox{cl} \ A.$ I am not sure how to proceed in this case. Any help or suggestion ?","['derivatives', 'real-analysis', 'analysis']"
2680067,Spectral theorem for Fourier transform,"The Fourier transform restricts to an isometry $F$ of the subspace of real even functions in $L^2(\mathbb{R}^n)$. Also $F$ is self adjoint since $$\int F(u)v = \int u F(v)$$
for real even functions.
If this is correct, how does the spectral theorem reads when applied to $F$? Namely, what are the spectrum (I guess $-1$ and $1$) and the projection-valued measure? Can we describe eigenspaces explicitly if they exist?",['functional-analysis']
2680075,"Given a set of primes S, show that no prime in S divides the sum of staggered products of these primes.","Definition: Given a set of primes $S = \{p_1, p_2\, \cdots, p_n\}$ with $|S| \ge 2$ , form the sum of staggered products using the following formula: $$
\sum_{i} \prod_{j \ne i} p_j
$$ Example: If $S = \{2, 3, 5 \}$ , then $\sum_{i} \prod_{j \ne i} p_j = 3 \cdot 5 + 2 \cdot 5 + 2 \cdot 3 = 31$ Question: Is it true that no prime in $S$ divides the sum of staggered products ? i.e. Does $p_k | \sum_{i} \prod_{j \ne i} p_j$ for any $k \in \{1, \cdots, n \}$ ? Comments: I've done some numerical experiments and it seems to be true, but I'm not sure how to go about proving it. I've considered rewriting the sum of staggered products as $\sum_{i} \prod_{j} \frac{p_j}{p_i}$ , so I could maybe work in modulo $p_k$ and maybe show that $\sum_{i} \frac{1}{p_i} \not\equiv 0$ , but I'm not sure how to actually do this. Any suggestions would be helpful. [This isn't a homework question, but I'd still like it if full answers weren't provided :)]","['number-theory', 'prime-factorization', 'prime-numbers']"
2680085,I know $\mathbb{R}$ is the real number line. What really is $\mathbb{R}^n$? [closed],Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 years ago . Improve this question I know $\mathbb{R}$ is the real number line. What really is $\mathbb{R}^n$ ? EDIT (based on comments below): What actually is the result of a cartesian product? That is something I failed to grasp too. What do you get when you multiply the number line $\Bbb{R}$ with itself? I mean the final result? Thanks.,"['vectors', 'real-numbers', 'elementary-set-theory', 'definition']"
2680092,What is the value of $\left \lfloor \frac{1}{3} + \frac{i}{2} \right \rfloor $?,"How do you get the floor function of a complex number? Is it defined? In particular, for example, what is the value of $\left \lfloor \frac{1}{3} + \frac{i}{2} \right \rfloor $? I do not know the answer to this problem. This is merely out of curiosity, no special reason for posting.","['complex-numbers', 'algebra-precalculus', 'integers', 'definition', 'ceiling-and-floor-functions']"
2680109,Does Day and Night on a Klein bottle have a steady state?,"Place a $m \times n$ ($m,n \ge 3$) square grid on a Klein bottle. On each square, we select an arbitrary non-mirror symmetric marker, and arrange them on the Klein bottle in some way. This arrangement is called a ""position"". Now, we introduce the following operation on positions. For each square, we do the following: If the symbol on the square is the same orientation as 3, 6, 7, 8 symbols of its moore neighbors , do nothing to that symbol. Otherwise, replace it by its mirror reflection. This definition takes advantage of the fact that although the Klein bottle is not globally orientable, it is locally orientable (which is true of every space). What we have defined basically is Day and Night on a Klein bottle, where life is the mirror symmetry of death. My question is this: is there a still-life (meaning, a position for which the operation does nothing)?","['discrete-mathematics', 'cellular-automata', 'chaos-theory', 'klein-bottle', 'non-orientable-surfaces']"
2680131,Finding the roots of $(x^2+7x+6)^2 + 7(x^2+7x+6) + 6=x$,"I have been trying to solve this one problem from the Duke Math Meet, which does not provide a solution: Find all solutions of $(x^2+7x+6)^2 + 7(x^2+7x+6) + 6=x$ At first I tried to factorize the polynomial, but always had the right-hand side $x$ remain, which was inconvenient. I then tried using the fact that one can write the left-hand side as a composition of functions, and equated that with the inverse of the quadratic plugged into the function, but that was a very nasty equation with square roots, still ending up with a quartic. What other solution paths are viable for this problem, and is there a way to factor the quartic?",['algebra-precalculus']
2680166,$L^2$ convergence of holomorphic functions,"I am trying to solve a problem from this website that asks the following: Prove that, for a sequence of holomorphic functions on a compact set,
convergence in $L^2$ implies uniform convergence. I am struggling with how to relate the $L^2$ norm to the fact that the functions are holomorphic. There is also a related question, which asks if the space of square-integrable holomorphic functions on a bounded region $U\subseteq\mathbb C$ is a Banach space. This seems like it would follow from showing that a sequence of functions that is ""$L^2$-Cauchy"" is uniformly Cauchy, but I'm not sure.","['complex-analysis', 'lp-spaces']"
2680167,A solution of the Laplace equation that is compactly supported or vanishes at $\infty$ or near the boundary,"Consider the Laplace equation 
\begin{equation}\label{1}
\Delta u =0 
\end{equation}
 on a bounded smooth domain (or the whole space).
Does there exist a solution for the Laplace equation that is compactly supported 
or vanishes near the boundary (or at $\infty$) ?","['real-analysis', 'partial-differential-equations', 'functional-analysis', 'complex-analysis', 'analysis']"
2680173,second-order ODE particular solution,I am trying to solve for the ODE $$y'' + 2y' + y = e^{-x}$$ I tried the ansatz of the form $$ y = Ae^{-x} $$ But the coefficients cancel out leaving me with $ 0 = e^{-x}$. I'm guessing I have to be smarter with the ansatz? Maybe include a function like $y = Af(x)e^{-x}$ but that seems like it'll just result in another DE. Any hint or suggestions on better ways to solve this would be appreciated.,['ordinary-differential-equations']
2680206,If $L^*$ is the formal self-adjoint of and $n$th order differential operator $L$ show that the formal adjoint of $L^*$ is $L$.,If $L^*$ is the formal self-adjoint of and $n$th order differential operator $L$ show that the formal adjoint of $L^*$ is $L$. Proof By definition and hypothesis $L^* = L$. Therefore $\left(L^* \right)^* = \left(L\right)^* = L.$ Is it that easy or do I have to do some integration?,"['functional-analysis', 'ordinary-differential-equations']"
2680245,Clarification about Milne's Etale Cohomology text,"I am reading Miln'es Etale Cohomology (1980 Princeton University Press). In page 46 to give some exampeles of $E$-morphisms he writes $E=(et)$ of all etale morphisms of finite-type. Next page he writes that $E$-morphisms are open and any open immersion is an $E$-morphism. Thus if I understand correctly, given any open immersion $U\to X$ it has to be an $E$-morphism. In this case $E$ is etale of finite-type. Therefore any open immersions needs to be of finite-type. But that is false. Could you please tell me if my way of thinking is flawed?","['etale-cohomology', 'algebraic-geometry']"
2680258,What's the difference between MUTUALLY EXCLUSIVE and PAIRWISE DISJOINT?,"When I study Statistical Theory, I find that these two concepts confuse me a lot. By definition, if we say two events are PAIRWISE DISJOINT, that means the intersection of these two event is empty set.
If we say that two events are MUTUALLY EXCLUSIVE, that means if one of these two events happens, the other will not. But doesn't it means that these two events are PAIRWISE DISJOINT？ If we say two events are MUTUALLY EXCLUSIVE, then they are not INDEPENDENT. Can we say that two PAIRWISE DISJOINT events are not INDEPENDENT as well？ If these two concepts are different (actually my teacher told me they are), could you please give me an example that two events are MUTUALLY EXCLUSIVE but not PAIRWISE DISJOINT, or they are PAIRWISE DISJOINT but not MUTUALLY EXCLUSIVE. Thank you for your help.",['statistics']
2680297,Evaluate using contour integration $\int_0^{\pi}\frac{\sin(2\theta)}{5-3\cos(\theta)}d\theta$,"I had been given in my complex analysis examination the following problem. Evaluate the following integral by using contour integration: $$
\int_0^{\pi}\dfrac{\sin(2\theta)}{5-3\cos(\theta)}d\theta
$$ The answer to which is:
$$
\dfrac{2}{9}(log_e(1024)-6).
$$ I tried to get this answer trying different ways but I just could not find a way to do it. A thing to notice is that there is no $\pi$ term occurring in the final answer which means whatever the contour is which will give the answer easily is such that by integrating along that contour the value which we will get will be proportional to $\dfrac{1}{i\pi }$. This is because the $2\pi i$ term multiplying the residue should be canceled out as the actual answer does not have $\pi$ dependence. The $pi$ in the denominator gives a hint to me that this going to be complicated as in complex analysis at least in introductory courses nowhere $\pi$ comes in the denominator unless the problem is intentionally set up that way. Another thing to notice is that if we have to use a contour that covers the angle from $0$ to $\pi$. Usually, the limits given in such problems is $0$ to $2\pi$ which is easy to do just by replacing $sine$ and $cosine$ term as follows:
$$
 sin(\theta) \rightarrow \dfrac{1}{2i}(z-\dfrac{1}{z})
$$
$$ 
cos(\theta) \rightarrow \dfrac{1}{2}(z+\dfrac{1}{z})
$$
$$
d\theta \rightarrow \dfrac{dz}{iz}
$$
and then carrying out the contour integral along $|z|=1$ contour. The limits of the integral could be converted to $0$ to $2\pi$ by proper substitution and then the contour integral by above procedure could be carried out but that would not help as it will bring in fractional order poles (as $cos\dfrac{\theta}{2}$  term will appear) which residue theory can't deal with. So, what I need is a hint on how to do it.","['residue-calculus', 'complex-analysis', 'integration', 'definite-integrals', 'contour-integration']"
2680299,Infinite matrix which cannot be represented by bounded linear operator,"Let ${\cal V}$ be n Hilbert space over $\mathbb{R}$ or $\mathbb{C}$, with an orthonormal basis $(e_n)_{n=1}^{\infty}$. For every bounded linear operator $A: {\cal V} \to {\cal V}$, we can associate $A$ with an matrix $(a_{ij})_{i,j=1}^{\infty}$ where $a_{ij} = \langle Ae_j,e_i\rangle$. Is there a matrix $(b_{ij})_{i,j=1}^{\infty}$ such that: 1. $\sum_{i=1}^{\infty}|b_{ij}|^2 < \infty$ 2. $\sum_{j=1}^{\infty}|b_{ij}|^2 < \infty$ 3. $\sup_{i,j \ge 1} |b_{ij}| < \infty$ and there is no bounded linear operator $B : {\cal V} \to {\cal V}$ such that $b_{ij} = \langle Be_j,e_i\rangle$ I am really struggling with this problem, I will appreciate any help!","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'sequences-and-series', 'infinite-matrices']"
2680397,Solving following second order matrix differential equation,"Which methods (analytical or numerical) do you suggest to solve the following matrix diff. equation? And why? (Presumably analytical method) $$ \textbf{M}\ddot{V}+\textbf{C}\dot{V}+\textbf{K}V=\textbf{P},$$ where matrices $\ddot{V}$ , $\dot{V}$ and $V$ with respect to $t$ are as follows: $$ \mathbf{M} \begin{bmatrix} \ddot{V}(1,t) \\ \ddot{V}(2,t) \\ \vdots \\ \ddot{V}(n,t) \end{bmatrix} +\mathbf{C} \begin{bmatrix} \dot{V}(1,t) \\ \dot{V}(2,t) \\ \vdots \\ \dot{V}(n,t) \end{bmatrix} +\mathbf{K} \begin{bmatrix} V(1,t) \\ V(2,t) \\ \vdots \\ V(n,t) \end{bmatrix} = \mathbf{P}.$$ In here, the matrices $\textbf{M}$ , $\textbf{C}$ , $\textbf{K}$ and $\textbf{P}$ are as follows ( $\textbf{M}$ , $\textbf{C}$ and $\textbf{K}$ are $n \times n$ matrices, and $\textbf{V}$ , $\textbf{P}$ , $\textbf{F}$ are $n \times 1$ matrices) $$\mathbf{P} = \frac{P}{\rho A} \begin{bmatrix} \sin\left(\dfrac{\pi vt}{l}\right) \\ \sin\left(\dfrac{2\pi vt}{l}\right) \\ \vdots \\ \sin\left(\dfrac{n\pi vt}{l}\right) \end{bmatrix} +\mathbf{F}(t), \quad\text{where } \mathbf{F}(t) = \begin{bmatrix} f_1(t) \\ f_2(t) \\ \vdots \\ f_n(t) \end{bmatrix}.$$ In here, $l$ , $P$ , $A$ , $\rho$ , $\alpha$ , $v$ , $N$ , $E$ and $I$ are constants. It doesn't matter what are these constants or initial or boundary conditions for me. You can select appropriate values. The matrix $\textbf{F}(t)$ is ungiven. So solution $\textbf{V}$ will depend on $\textbf{F}$ .","['matrix-equations', 'numerical-methods', 'ordinary-differential-equations']"
2680422,Prove to me that this answer is correct,"I'm doubting myself and my teacher, so I need some external verification. So, the graph on the left is given to me, and I'm told to derive it. The answer key is shown on the right. What I do not understand is why $x = -5$, $-1$ and $3$ are not inclusive. I understand that the limits at $x = -5$, $-1$ and $3$ do not exist, obviously. However, they are still derivable at those points, correct? Because in the original graph, they are clearly marked inclusive. It even says the domain is $[-5, \infty)$, so that completely means $-5$ is inclusive for $f(x)$. (That infinity one should really be a right parenthesis.) So anyway, is my teacher wrong, or am I wrong? Can anyone actually prove to me in the most clear, concise manner possible that $f'(x)$ at the points $-5$, $-1$ and $3$ are non-inclusive? Thank you for your time and help in advance. It is much appreciated.","['derivatives', 'calculus']"
2680440,Can solutions that we get from a direction field of a Linear ODE cross each other,"Question: When we sketch the direction fields of a given Linear ODE, can they cross each other ? Reasoning: Let we have a linear ODE of the form
$$y'(x) = f(x,y(x))$$ This means that for a given $(x,y)$, $f(x,y)$ have a definite value, hence at that point, the derivative of any function that satisfies the linear ODE is that unique value $f(x,y)$. Therefore, if at some point we have more than one solution, the solutions cannot cross each (they can meet, but not cross) other because that would violate the uniqness of the derivative at that point as I have argued. Now, I have asked the same question to one of my professors by giving the above argument, but he said that ""we don't know the form of $f(x,y)$, so we cannot sure such a thing"", however, my argument directly eliminates his argument in my view, but I want to make sure that the above argument is a valid argument and my conclusion is correct.","['derivatives', 'ordinary-differential-equations']"
2680472,Linear program with $10$ variables,"$$\begin{array}{ll} \text{maximize} & \frac{a}{100} + \frac{4b}{100} + \frac{9c}{100} + \frac{16d}{100} + \frac{25e}{100} + \frac{36f}{100} + \frac{49g}{100} + \frac{64h}{100} + \frac{81i}{100} + \frac{100j}{100}\\ \text{subject to} & a+b+c+d+e+f+g+h+i+j = 100\end{array}$$ When I use the Lagrange multiplier and I take all my partial derivatives, I do not get any of the variables in terms of $\lambda$. For example, when I take the derivative with respect to $a$, I get $$\frac{1}{100} = \lambda$$ and with respect to $b$, I get $$\frac{4}{100} = \lambda$$ Is there any way I can find the values of $a$, $b$, $c$, $d$, $e$, $f$, $g$, $h$, $i$ and $j$ which maximises my function?","['optimization', 'linear-programming', 'multivariable-calculus', 'convex-optimization', 'lagrange-multiplier']"
2680477,Demonstration of relation between geodesics and FLRW metric,"I am reading a book of General Relativity and I am stuck on a demonstration. If I consider the FLRW metric as : $\text{d}\tau^2=\text{d}t^2-a(t)^2\bigg[\dfrac{\text{d}r^2}{1-kr^2}+r^2(\text{d}\theta^2+\text{sin}^2\theta\text{d}\phi^2)\bigg]$ with $g_{tt}=1$, $\quad g_{rr}=\dfrac{a(t)^2}{1-kr^2}$ and $\quad g_{\theta\theta}=\dfrac{g_{\phi\phi}}{\text{sin}^2\theta}=a(t)^2 r^2$ It is said in this book that, despite of the utility of comoving coordinates, the dependance of time in scale factor $a(t)$ can be better understood if we consider a set of coordinates called ""Free Fall coordinates"" and noted $(\tilde{x}^\mu, \mu=0,1,2,3)$ with a metric locally Lorentzian near to the origin $\tilde{x}^{\mu}=0$ : $g_{\mu\nu}=\eta_{\mu\nu}+\dfrac{1}{2}g_{\mu\nu,\alpha\beta}(0)\tilde{x}^{\alpha}\tilde{x}^{\beta}+\,.......\quad\text{(eq1)}$ with $\eta_{00}=-\eta_{11}=-\eta_{22}=-\eta_{33}=1\quad\quad$ and $\eta_{\mu\neq\nu}=0$ and $g_{\mu\nu,\alpha\beta}=\dfrac{\partial^2 g_{\mu\nu}}{\partial \tilde{x}^{\alpha}\partial \tilde{x}^{\beta}}$ Moreover, one takes the expression of classic geodesics : $\dfrac{\text{d}}{\text{d}\tau}\bigg(g_{\mu\nu}(x)\dfrac{\text{d}x^{\nu}}{\text{d}\tau}\bigg)-\dfrac{1}{2}\dfrac{\partial g_{\lambda\nu}}{\partial x^{\mu}}\dfrac{\text{d}x^{\lambda}}{\text{d}\tau}\dfrac{\text{d}x^{\nu}}{\text{d}\tau}=0\quad\quad\mu=0,1,2,3\quad \text{(eq2)}$ The author says that, by applying $\text{(eq1)}$ into the relation $\text{(eq2)}$, one gets, at first order, the following relation : $\dfrac{\text{d}^2 \tilde{x}^{\alpha}}{\text{d}\tau^2} = -\eta^{\alpha\gamma}\bigg[g_{\mu\gamma,\nu\beta}-\dfrac{1}{2}g_{\mu\nu,\gamma\beta}\bigg]\tilde{x}^{\beta}\dfrac{\text{d}\tilde{x}^{\mu}}{\text{d}\tau}\dfrac{\text{d}\tilde{x}^{\nu}}{\text{d}\tau}\quad\quad\text{(eq3)}$ I can't manage to obtain the $\text{eq(3)}$ from $\text{eq(1)}$ and $\text{eq(2)}$, if someone could help me for the details of the demonstration, this would be nice. For the moment, if I put the definition of $g_{\nu\mu}$ into $\text{eq(2)}$,  I get : $\dfrac{\text{d}}{\text{d}\tau}\bigg(g_{\mu\nu}(x)\dfrac{\text{d}x^{\nu}}{\text{d}\tau}\bigg)=\bigg(\dfrac{\text{d}g_{\mu\nu}}{\text{d}\tau}\bigg)\,\dfrac{\text{d}x^{\nu}}{\text{d}\tau}+g_{\mu\nu}\dfrac{\text{d}^2x^{\nu}}{\text{d}\tau^2}\quad\quad \text{eq(4)}$ If I separate the two terms on RHS on $\text{eq(4)}$ : $\bigg(\dfrac{\text{d}g_{\mu\nu}}{\text{d}\tau}\bigg)\,\dfrac{\text{d}x^{\nu}}{\text{d}\tau}=\dfrac{\text{d}}{\text{d}\tau}\bigg(\eta_{\mu\nu}+\dfrac{1}{2}g_{\mu\nu,\alpha\beta}(0)\tilde{x}^{\alpha}\tilde{x}^{\beta}\bigg)\dfrac{\text{d}x^{\nu}}{\text{d}\tau}=$ $\dfrac{1}{2}\,g_{\mu\nu,\alpha\beta}(0)\bigg[\dfrac{\text{d}\tilde{x}^{\alpha}}{\text{d}\tau}\,\tilde{x}^{\beta}+\tilde{x}^{\alpha}\,\dfrac{\text{d}\tilde{x}^{\beta}}{\text{d}\tau}\bigg]\dfrac{\text{d}x^{\nu}}{\text{d}\tau}=$ $g_{\mu\nu,\alpha\beta}(0)\bigg[\dfrac{\text{d}\tilde{x}^{\alpha}}{\text{d}\tau}\,\tilde{x}^{\beta}\bigg]\dfrac{\text{d}x^{\nu}}{\text{d}\tau}$ Concerning the second term on RHS of $\text{eq(4)}$, maybe I could write : $g_{\mu\nu}\dfrac{\text{d}^2x^{\nu}}{\text{d}\tau^2}=\eta_{\mu\nu}\dfrac{\text{d}^2x^{\nu}}{\text{d}\tau^2}$ by neglecting the term $\dfrac{1}{2}g_{\mu\nu,\alpha\beta}(0)\tilde{x}^{\alpha}\tilde{x}^{\beta}$ in the expression of $g_{\mu\nu}$. Another problem, How can I transform the second term on LHS of $\text{(eq2)}$ : $-\dfrac{1}{2}\dfrac{\partial g_{\lambda\nu}}{\partial x^{\mu}}\dfrac{\text{d}x^{\lambda}}{\text{d}\tau}\dfrac{\text{d}x^{\nu}}{\text{d}\tau}\quad\quad \text{eq(5)}$ ?? Indeed, it seems that we can deduce from this term the wanted term : $\dfrac{1}{2}\,\eta^{\alpha\gamma}\,g_{\mu\nu,\gamma\beta}\tilde{x}^{\beta}\dfrac{\text{d}\tilde{x}^{\mu}}{\text{d}\tau}\dfrac{\text{d}\tilde{x}^{\nu}}{\text{d}\tau}$ But $\tilde{x}$ coordinates (""Free Fall coordinates"") appear in this last expression instead of ""Comobile coordinates"" $x^{\mu}$ into $\text{eq(5)}$ , so I don't know how to get it ? Finally, to do the link between ""Fre Fall coordinates"" and ""Comoving coordinates"", can I write : $g_{\mu\nu}\text{d}x^{\mu}\text{d}x^{\nu}=\eta_{\alpha\beta}\text{d}\tilde{x}^{\alpha}\text{d}\tilde{x}^{\beta}$ ?? Any help is welcome UPDATE 1 : You can find below 2 image captures resuming the different equations (mostly important eq(4.4), eq(4.5); eq(4.6) and eq(4.87)) : UPDATE 2 : Thanks for your demo. Just a last question, how do you do the link between the definition of coordinates called ""Free Fall coordinates"" and noted $(\tilde{x}^\mu, \mu=0,1,2,3)$ : $g_{\mu\nu}=\eta_{\mu\nu}+\dfrac{1}{2}g_{\mu\nu,\alpha\beta}(0)\tilde{x}^{\alpha}\tilde{x}^{\beta}+\,.......\quad\text{(eq1)}$ and the comoving coordinates that you use into your following start point : $\frac{\partial g_{\lambda\nu}}{\partial x^\mu} = g_{\lambda\nu,\mu\beta} x^\beta$ I mean, you don't use $\tilde{x}$ notation in your demontsration, that makes me confused. Regards","['tensors', 'differential-geometry', 'general-relativity', 'geodesic']"
2680479,Stalk of tensor product sheaf is tensor product of stalks via adjunctions and abstract nonsense,"Let $(X, \mathcal{O}_{X})$ be a ringed-space with sheaves of modules $\mathcal{F}$ and $\mathcal{G}$. I would like to show that for any point $p \in X$,
$$
\mathcal{F}_{p} \otimes_{\mathcal{O}_{X, p}} \mathcal{G}_{p} \simeq (\mathcal{F} \otimes_{\mathcal{O}_{X}} \mathcal{G})_{p}
$$
using the fact that left adjoints preserve colimits. I have shown an adjunction between the sheaf tensor and the internal hom functor. The problem is, this seems to only give me that the functor,
$$
- \otimes_{\mathcal{O}_{X}} \mathcal{F}
$$
preserves colimits. Similarly I have the tensor-hom adjunction for modules over a ring, so I would ideally show this for the tensor product presheaf. But this still only gives me that 
$$
- \otimes_{\mathcal{O}_{X}(U)} \mathcal{F}(U)
$$
preserves colimits. But of course if I want to take a directed limit over open sets containing a point $p$, the ring $\mathcal{O}_{X}(U)$ will change as $U$ changes. Similarly I would also need the result for the bifunctor
$$
- \otimes_{\mathcal{O}_{X}} -
$$
to get the tensor product of both stalks. Is there any way to use this method to show that the tensor product sheaf commutes with taking stalks, or do I just need to do it manually?","['tensor-products', 'sheaf-theory', 'algebraic-geometry', 'adjoint-functors', 'limits-colimits']"
2680485,Zariski topology and surfaces,"Let $X,Y$ be complex algebraic surfaces. Is there a map $f : X \to Y$ which is not algebraic but induces an homeomorphism with respect to the Zariski topology ? Motivation : there are plenty of such maps for curves, since any bijection is automatically an homeomorphism. For surfaces, I suspect that homeomorphism should be given by an algebraic map but I'm not able to prove it.","['zariski-topology', 'algebraic-geometry']"
2680537,Is the low-rank approximation of a matrix unique?,"Let $A \in \mathbb{R}^{m \times n}$ be a matrix with $\text{rank}(A)=\min(m,n)$. The Eckart–Young–Mirsky theorem for the Frobenius norm states that the best approximation of rank $k<\min(m,n)$ for $A$, denoted $A_k$, is: $$\arg \min_{A_k} \left\Vert A - A_k \right\Vert_F^2 = \sum_{i=1}^k \sigma_i u_i v_i^T$$ where $$A= \overset{\max(m,n)}{\underset{i=1}{\sum}} \sigma_i u_i v_i^T$$ is the singular value decomposition of $A$ (The singular vectors $u_i$ and $v_i$ are normalized and their corresponding singular values $\sigma_i$ are sorted in descending order). Is this solution the unique global minimizer?","['optimization', 'matrices', 'svd', 'matrix-rank', 'linear-algebra']"
2680630,Find the volume of the region inside both the sphere $x^2+y^2+z^2=4$ and the cylinder $x^2+y^2=1$,Find the volume of the region inside both the sphere $x^2+y^2+z^2=4$ and the cylinder $x^2+y^2=1$ $$2\int_0^{2\pi}\int_0^{\frac{\pi} 6}\int_0^{2}1.r^2\sin \varphi dr d\varphi d\theta + \int_0^{2\pi}\int_{\frac{\pi} 6}^{\frac{5\pi} 6}\int_0^{\frac 1 {\sin \varphi}}1.r^2\sin \varphi dr d\varphi d\theta $$ This question is in the lecture notes but I didn't understand how the answer is obtained. Could someone help?,"['multivariable-calculus', 'spherical-coordinates', 'integration', 'volume']"
2680646,Show that ${\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n} + \frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}}$,"Let $n\in\mathbb{N}$ such that ${n}>{0}$ . Show that $${\frac{x_1^{n}+x_2^{n}+...+x_n^{n}}{x_1x_2...x_n}+\frac{\sqrt[n]{x_1x_2...x_n}}{x_1+x_2+...+x_n}}\ge{n+\frac{1}{n}}$$ when $x_k > 0, \forall k$ . I thought I can use AM GM inequality, but if I try to break it in two smaller pieces and apply AMGM I get a false affirmation for $\frac{\sqrt[n]{x_1x_2...x_n}}{\ x_1+\ x_2+...+x_n}$ . Any help please?","['real-analysis', 'inequality', 'calculus']"
2680716,Analytic Grothendieck Riemann Roch,"I was wandering if is there an analytic version of the Grothendieck-Riemann-Roch theorem. If so, could you please tell me the references?","['topological-k-theory', 'algebraic-geometry', 'reference-request', 'intersection-theory', 'differential-geometry']"
2680726,Looking for a closed form solution $P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) $,"$$P_{n+2}(x)=x^2 P'_{n+1}(x)+(n+1)xP_{n+1}(x)-(n+1)P'_n(x) \tag{1}$$ where $P_{0}(x)=x$ , $P_{1}(x)=x^2$ are the given initial conditions. I calculated few terms:
$$P_{2}(x)=3x^3-1$$
$$P_{3}(x)=3.5x^4-6x$$
$$P_{4}(x)=3.5.7x^5-51x^2$$
$$P_{5}(x)=3.5.7.9x^6-546x^3+24$$ We can estimate the $P_n(x)$ form from a few terms , $P_{n}(x)=a_{n}x^{n+1}+b_{n}x^{n-2}+c_{n}x^{n-5}+d_{n}x^{n-8}+\cdots.$
If we put these terms in Equation 1, We get;
$$a_{n+2}x^{n+3}+b_{n+2}x^{n}+c_{n+2}x^{n-3}+....=  
a_{n+1}(2n+3)x^{n+3}+[b_{n+1}2n-a_{n}(n+1)^2]x^{n}+[c_{n+1}(2n-3)-b_{n}(n+1)(n-2)]x^{n-3}+......
$$ We can write that
$$a_{n+2}=(2n+3)a_{n+1}$$
$$b_{n+2}=2nb_{n+1}-a_{n}(n+1)(n+1)$$
$$c_{n+2}=(2n-3)c_{n+1}-b_{n}(n+1)(n-2)$$ Next term can be gotten as 
$$d_{n+2}=(2n-6)d_{n+1}-c_{n}(n+1)(n-5)$$ $a_{n}$ can be expressed as for $n>0$ $a_{n}=1.3.5.7.....(2n-1)=\frac{(2n-1)!}{2^{n-1}(n-1)!}$ I am looking for a closed form $P_n(x)$ or a generating function. Please help me to find a similar Rodrigues formula of Legendre Polynomials for the defined $P_{n}(x)$ above (if it is possible)? Note: Legendre Polynomials, Rodrigues formula is $$\frac{1}{2^nn!}\frac{d^n}{dx^n}(x^2-1)^n$$ Edit: $P_n(x)$ satisfy the relation $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ $$\sum_{n=0}^\infty \frac{x^{2n}P_{n+1}(x)}{n!}=0 $$ My atempt to find generating function: $$G(x,t)=\sum_{n=0}^\infty P_{n}(x) t^n =P_0(x)+P_1(x)t+P_2(x)t^2+P_3(x)t^3+..... \tag {2}$$ $$xt\frac {\partial G(x,t)}{\partial t}=xP_1(x)t+2xP_2(x)t^2+3xP_3(x)t^3+.....\tag {3}$$ $$x^2\frac {\partial G(x,t)}{\partial x}=x^2P'_0(x)+x^2P'_1(x)t+x^2P'_2(x)t^2+x^3P'_3(x)t^3+.....\tag {4}$$ $$t\frac {\partial G(x,t)}{\partial x}+t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=P'_0(x)t+2P'_1(x)t^2+3P'_2(x)t^3+4P'_3(x)t^4+.....\tag {5}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2P'_0(x)+(x^2P'_1(x)+xP_1(x)-P'_0(x))t+(x^2P'_2(x)+2xP_2(x)-2P'_1(x))t^2+(x^2P'_3(x)+3xP_3(x)-3P'_2(x))t^3+....\tag {6}$$ $$xt\frac {\partial G(x,t)}{\partial t}+x^2\frac {\partial G(x,t)}{\partial x}-t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2+P_2(x)t+P_3(x)t^2+P_4(x)t^3+....\tag {7}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+P_2(x)t^2+P_3(x)t^3+P_4(x)t^4+....\tag {8}$$ $$xt^2\frac {\partial G(x,t)}{\partial t}+x^2t\frac {\partial G(x,t)}{\partial x}-t^2\frac {\partial G(x,t)}{\partial x}-t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x^2t+G(x,t)-x-x^2t\tag {9}$$ $$G(x,t)-xt^2\frac {\partial G(x,t)}{\partial t}+(t^2-x^2t)\frac {\partial G(x,t)}{\partial x}+t^3\frac {\partial^2 G(x,t)}{\partial x \partial t}=x\tag {10}$$ I could not get progress after here. Please help me how to get the generating function after this step. UPDATE (03/23/2018): Thanks a lot for all answers. All answers are very helpful to solve the problem. I would like to write other property of the $P_{n}(x)$. I believe that it will be helpful to find the closed form of $P_n(x)$. $$U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ $$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!} $$ We know that $$\frac{x^2-y}{1-xy}=\cfrac{\sum_{n=0}^\infty \frac{y^{n}P_{n+1}(x)}{n!}}{\sum_{n=0}^\infty \frac{y^{n}P'_{n}(x)}{n!}} $$ Thus We can rewrite it as $$\frac{(x+y)^2-xy}{1-xy(x+y)}=\frac{x^2+xy+y^2}{1-xy(x+y)}=\cfrac{\sum_{n=0}^\infty \frac{(xy)^{n}P_{n+1}(x+y)}{n!}}{\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}} $$ $$\frac {\partial U(x,y)}{\partial x}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+y \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!} $$
$$\frac {\partial U(x,y)}{\partial y}=\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}+x \frac{x^2+xy+y^2}{1-xy(x+y)}\sum_{n=0}^\infty \frac{(xy)^{n}P'_{n}(x+y)}{n!}  $$ $$\frac {\partial U(x,y)}{\partial x}/\frac {\partial U(x,y)}{\partial y}=\frac {1-xy(x+y)+y(x^2+xy+y^2)}{1-xy(x+y)+x(x^2+xy+y^2)}$$ $$(1+x^3)\frac {\partial U(x,y)}{\partial x}=(1+y^3)\frac {\partial U(x,y)}{\partial y}$$ $$\int_0^{U(x,y)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ If we derivative both side for x $$\frac{\partial U(x,y)}{\partial x} F(U(x,y))=\frac{1}{1+x^3}$$ $$\frac{1}{F(U(x,y))}=(1+x^3)\frac{\partial U(x,y)}{\partial x}$$
If we derivative both side for y
$$\int_0^{U(x,y)} F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ $$\frac{\partial U(x,y)}{\partial y} F(U(x,y))=\frac{1}{1+y^3}$$ $$\frac{1}{F(U(x,y))}=(1+y^3)\frac{\partial U(x,y)}{\partial y}$$ To find $F(z)$,We can put $y=0$ $$\int_0^{U(x,0)}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$U(x,0)=P_0(x)=x $$ $$\int_0^{x}  F(z)\mathrm dz=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$
$$F(z)=\frac{1}{1+z^3}$$
Thus the solution of $U(x,y)$ is 
$$\int_0^{U(x,y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}+\int_0^{y} \frac{\;\mathrm dz}{1+z^3}$$ Let's define $g(x)$ , $g(0)=0$;
$$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$y=\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x+y=\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}$$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$g(x+y)=U(g(x),g(y))$$ $$\int_0^{U(g(x),g(y))} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ $$x=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}$$ $$g^{-1}(x)=\int_0^{x} \frac{\;\mathrm dz}{1+z^3}$$ $$g'(x)=1+g^3(x)$$ So we can write that 
$$g(g^{-1}(x)+g^{-1}(y))=U(x,y)=\sum_{n=0}^\infty \frac{(xy)^{n}P_{n}(x+y)}{n!} $$ Addition formula of $g(x)$ can be written as: $$g(x+y)=\sum_{n=0}^\infty \frac{g^n(x)g^n(y)P_{n}(g(x)+g(y))}{n!} $$ $$\int_0^{g(x+y)} \frac{\;\mathrm dz}{1+z^3}=\int_0^{g(x)} \frac{\;\mathrm dz}{1+z^3}+\int_0^{g(y)} \frac{\;\mathrm dz}{1+z^3}$$ If a closed form of $P_n(x)$ can be found, We can write a closed form addition formula for $g(x)$ like $\tan(x)$","['derivatives', 'recurrence-relations', 'polynomials', 'partial-differential-equations', 'generating-functions']"
2680750,Construction of a (super/sub-)martingale which converges,"Good morning, I have the following problem. I have a stochastic process $(D_t)_{t\geq T}$ (the starting point $T$ is not relevant) adapted to a filtration $(\mathcal{F}_t)_{t\geq T}$ and such that $\mathbb{E}[D(t+1)|\mathcal{F}_t]=D(t)\left(1-\frac{1}{t^2}\right)+\frac{t+1}{t^2}$. I know that $\mathbb{E}[|D(t)|]$ is finite for all $t$. I need to find a sub/super-martingale defined in terms of $D(t)$ which allows me to conclude that $D(t)/\log(t)\rightarrow X$ almost surely. Do you have any idea? Thanks in advance.","['probability-theory', 'martingales']"
2680776,Proof that ring is commutative,"Proof that a ring $(A, + , \cdot)$ is commutative if 
$$x(y^2+y)=(y^2+y)x$$ I set $x$ to be $$(x-xy^2) $$and I get that $$xy^3=yxy^2.$$But I don't know how to get $xy=yx$.","['abstract-algebra', 'ring-theory']"
