question_id,title,body,tags
2722347,Finding a bounded function with unbounded derivative at zero,"I am working on a linear analysis problem where we have boiled down the problem to finding a continuous function $f:\mathbb{R} \to \mathbb{R}$ that is bounded, but has infinite derivative at zero. So far, we have conjured up the example $$f_n(x) = \frac{2}{\pi}\arctan(nx)$$
This sequence of functions will have infinite derivative at $0$ when $n\to \infty$, and is bounded by $1$. I believe this will work for the sake of our problem, but I would like to find a function that doesn't depend on $n$. I can picture what this should look like, but I can't come up with an example function. Any ideas? All appreciated.","['derivatives', 'examples-counterexamples', 'functions']"
2722362,What is the number of ordered pairs of unordered pairs of a set $S$ that share one element?,"Question : Let $S = \{1,2, \dots ,n \}$. I want to count how many ordered pairs of unordered pairs (sets of the form $ ( \{ i,j\}, \{k,\ell \} | i,j,k,\ell \in S  )$, where $( \{1,2 \}, \{2,3 \} ) \ne ( \{2,3 \}, \{1,2 \} ) $  ). I can find such that $|\{i,j\}\cap\{k,\ell\}| = 1$ My attempt : If I fix a pair $\{i,j \}$, then I can find $(n-1)(n-2)$ other pairs that have one element in common with it. First, I choose the $(n-1)$ remaining elements that are different from $i$, then I choose the $(n-2)$ remaining elements that are different from $j,i$. But now if I let the pair $\{i,j \}$ vary, I can't find a way to add all the pairs of pairs up without double counting. The solution is supposed to be $n (n-1) (n-2)$ if I have not mis-phrased the question.",['combinatorics']
2722366,Convergence in distribution implies convergence in density,"Let $\{F_n\}$ be a sequence of absolutely continuous (wrt Lebesgue measure $\mu$) distributions on $R^k$, and $\{f_n\}$ be a corresponding sequence of densities. Suppose $F_n \Rightarrow F$, where $\Rightarrow$ stands for weak convergence. Can we show that $f_n$ converges pointwise to $f$? Here $f$ is the corresponding density of $F$. I know, in general, this is not true if no further assumptions are imposed (see e.g. Sweeting, 1986). How about $F$ is the normal distribution with zero mean and some positive definite covariance matrix $\Sigma$?","['weak-convergence', 'real-analysis', 'statistics']"
2722381,Understanding Kantorovich's inequality,"I'm looking for a proof of the Kantorovich inequality, namely: $$    \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(K(A)+\frac{1}{K(A)}\right)$$
Where $ K(A)= \lVert  A\rVert_2 \lVert  A^{-1}\rVert_2 $ and $A $ is an Hermitian positive definite  matrix and $x$  a vector with the accurate size. Or alternatively $$   \langle  Ax,x\rangle \langle A^{-1}x,x\rangle \leq \frac{1}{4}\left(\left(\frac{\beta}{\alpha}\right)^{2}+\left(\frac{\alpha}{\beta}\right)^{2}\right)$$
where $0 < \alpha = \lambda_1 \leq \cdots \leq \lambda_n = \beta$ are the eigenvalues of $ A$. There are a lot of proofs on internet but this is the one that I found easier to understand . Nevertheless, I'm stuck figuring out something: They use $ f,g: [\alpha,\beta]\to \mathbb{R} $ two convex function with $ f$ positive and $f(t)\leq g(t) $ for every $t \in [\alpha, \beta] $. Then they claim that $ F=f(A)$ and $ G=g(A)$ are well defined and hermitian positive definite. I don't even understand why they mean by $ f(A)$. If any of you guys can suggest me alternative documentation (other proof) or help me with this one, I would be very grateful","['matrices', 'numerical-methods', 'inequality', 'proof-explanation']"
2722391,Can we construct differential geometry without multi-variable calculus?,"To define the basic constructions of differential geometry (for example, the partial derivative of a function over a manifold), the general tactic seems to be to convert discussions of objects over the manifold into discussions of objects living in $\mathbb{R}^i$ by using the charts. An example off the top of my head is something like so: Define things over the manifold directly, such as functions $f: M \rightarrow \mathbb{R}$ use the charts $(U\subset M, \phi: U \rightarrow \mathbb{R}^n)$ to locally convert parts of the manifold into $\mathbb{R}^n$ now use the chart to construct $f'= f \circ \phi^{-1}$  to convert discussions of $f$ over the manifold into discussions over $f': \mathbb{R}^n \rightarrow \mathbb{R}$. Now we have a ""calculus-able"" object $f'$ in our hands, so we proceed to use calculus to define things like partial derivatives of $f'$, and their relationship to $f$. However, to perform this (and other) constructions, we need to have the integral and differential structure of $\mathbb{R}^n$. Is there some way to ""escape"" this, and build these structures into the manifold?","['differential-geometry', 'multilinear-algebra']"
2722409,Proof that $\sum\limits_i \frac{(p-1)!}i$ is divisible by $p$,"Problem: Let $p$ be a prime. Consider $\sum\limits_{i = 1}^{p-1} \frac1i = \frac K{(p-1)!}$. Rearranging, we have
  $K = \sum\limits_{i = 1}^{p-1} \frac{(p-1)!}i$.
  Prove that $p \mid K$. Hint: consider the factorization $x^{p-1} - 1 \equiv (x-1) ...(x - (p-1)) \pmod{p}$. Attempt: I am finding applying the hint difficult. Sure, the RHS of the congruence relation contains $(p-1)!$, but I can't see how the factorization would help me prove the expression for $K$. Any help is appreciated.","['number-theory', 'congruences', 'elementary-number-theory']"
2722421,"An example of a homogeneous, non-symmetric space","A Riemannian manifold $M$ is said to be homogeneous if the group of isometries $Isom(M)$ acts transitively on $M$. A Riemannian manifold is said to be symmetric if it is connected, homogenuous, and in addition, there exists a point $p\in M$ and an involution $\phi\in Isom(M)$ such that $p$ is an isolated fixed point of $\phi$  (that is, there exists an open neighborhood $V$ of $p$ where $p$ is the only fixed point of $\phi$ among all elements of $V$) It thus follows that even though a homogeneous spaces are very nice and ""symmetric"" in the sense that all points look geometrically the same, they are nevertheless not considered to be a symmetric space by the definition above (at least a priori). The inevitable question is thus: Is there a relatively simple example of a connected and homogeneous space which is not a symmetric space?","['isometry', 'homogeneous-spaces', 'differential-geometry']"
2722429,What is the inflation map on second group cohomology in terms of extensions?,"Let $G$ be a group, and $A$ an abelian group on which $G$ acts. Let $N\le G$ be a normal subgroup, and let $A^N$ denote the $N$-invariants of $A$. Then we have for every $n\ge 1$, we have a sequence in cohomology:
$$H^n(G/N,A^N)\rightarrow H^n(G,A^N)\rightarrow H^n(G,A)$$
If $n=2$, this allows us to produce, very abstractly, an extension $F$ of $G$ by $A$ from a given extension $E$ of $A^N$ by $G/N$. Is there a nice description of how one might construct $F$ from $E$?","['group-cohomology', 'group-theory']"
2722459,Which totally complex number field's embeddings correspond to geometric rotations/reflections in an argand diagram?,"For which totally complex number fields $K$ with embeddings $\{ \sigma_1, \dots, \sigma_m\}$ do we have the equality: $$
|\sigma_1(x)| = |\sigma_2(x)| = \dots = |\sigma_m(x)|,
$$ for all $x \in K$ where $|\cdot|$ corresponds to the complex absolute value $|x| = (x\bar{x})^{1/2}$? In other words, which number fields have embeddings that do not affect the distance of a coordinate on an argand diagram?","['number-theory', 'algebraic-number-theory', 'complex-numbers']"
2722469,Counting bit strings of length $70$ with two restrictions,"There is a bit string of length $70$. 
At least one of the following restrictions must apply: i)The first $9$ bits cannot contain exactly $5$ 1s ii)The first $49$ bits cannot contain exactly $27$ 1s. How many combinations are there? My first instinct is to do the following:
$2^{70}$ and subtract some number of combinations. I am not sure how to go about this or if there is a more elegant way.
I am currently getting the following result, but I am not confident in it because I believe that there may be some double counting: $2^{70}-(\binom{9}{5}+\binom{49}{27})=$ $997,056,092,945,595,000,000$","['combinations', 'combinatorics', 'bit-strings', 'discrete-mathematics']"
2722565,"Find out if a point is inside triangle, only data available are distances","Finding out if a point is inside a triangle is quite easy when coordinates are given, but how do you find out when only data available are: triangle's edges sizes distance from the point to triangle vertices ?","['trigonometry', 'geometry']"
2722591,"Showing that a measurable function is almost everywhere 0, proof check.","Let $f$ measurable and almost everywhere finite on $[0,1]$. Let $\int_{E} f\,d\lambda = 0$ for any measurable $E\subset[0,1]$ with $\lambda(E) = \frac{1}{2}$. (Where $\lambda$ is the Lebesgue measure and the function is Lebesgue measurable). I would like to show that $f$ is almost everywhere $0$. That is the last part in a many part question but it would kill every other part. I believe my proof is correct. Put $A=f^{-1}((0,\infty])$, $B=f^{-1}([-\infty,0))$, $C=f^{-1}(\{0 \})$. Notice that $A\cup B \cup C = [0,1]$. Claim: We have that $\lambda(A) < \frac{1}{2}$ and $\lambda(B) < \frac{1}{2}$ (and so $\lambda(C) > 0$). Proof: Suppose that $\lambda(A) \geq \frac{1}{2}$. Put $A' = \{x\in[0,1]:\lambda(A\cap[0,x]) \geq \frac{1}{2} \}$. This set is nonempty as it contains $1$. Put $\alpha = \inf A'$. The continuity of the measure provides that $\lambda(A\cap[0,\alpha]) = \frac{1}{2}$. By hypothesis we then have
$$\int_{A\cap[0,\alpha]}f\,d\lambda = 0. $$
Since $f$ is nonnegative on the domain of integration, it must be that this $f$ is almost everywhere $0$ on it. This contradicts that $f$ is strictly positive on $A$. A similar proof gives the result for $B$. Since $\lambda(B) < \frac{1}{2}$ it must be that $\lambda(A\cup C) = \lambda(A)+\lambda(C) \geq \frac{1}{2}$. But then we may do the same trick. Claim: We have that $\lambda(A) = \lambda(B) = 0$ and so $\lambda(C) = 1$ (and $f$ is 0 almost everywhere) Proof:Put $A'' = \{x\in[0,1] : \lambda(A\cup(C\cap[0,x]))\geq\frac{1}{2} \}$. This set is also nonempty because it contains $1$. Put $\beta = \inf A''$. The continuity of the measure provides that $\lambda(A\cup(C\cap[0,\beta])) = \frac{1}{2}$. By hypothesis we have
$$\int_{A\cup(C\cap[0,\beta])}f\,d\lambda = 0 .$$
Since $f$ is nonnegative on the domain of integration, it must be that $f$ is almost everywhere $0$ on it. Since $f$ is strictly positive on $A$, this can only be so $\lambda(A) = 0$. A similar proof gives the result for $B$.","['lebesgue-measure', 'lebesgue-integral', 'measure-theory', 'proof-verification']"
2722607,Primitive roots of unity occuring as eigenvalues of a product,"I am currently trying to understand the proof of Benson's Lemma (1.9.1) in Generalized Quadrangles by Payne and Thas. Background We have two $k × k$ matrices $Q$ and $M$ . We want to determine some formula for $\operatorname{tr}(QM)$ . The following information has been proven thus far: The matrix $Q$ is a permutation matrix with order $n$ . Hence, the eigenvalues $\xi$ of $Q$ are all $n$ -th roots of unity. and The matrix $M$ is a real symmetric matrix. It has eigenvalues $\lambda_0, \lambda_1,$ and $\lambda_2$ with multiplicity $m_0, m_1,$ and $m_2$ respectively. We know that each $\lambda_j$ is an integer, and furthermore that $\lambda_0 = 0$ and $m_1 = 1$ . We also know that $QM = MQ$ , and that $\operatorname{tr}(QM)$ is an integer. Lastly, we know that there is some eigenvalue $\theta_1$ of $QM$ such that $\theta_1 = \lambda_1$ . Deductions Since both $Q$ and $M$ are normal matrices, they are both diagonalizable. Moreover, since they commute, they are simultaneously diagonalizable (by $S$ , say). Then $$ S(QM)S^{-1} = (SQS^{-1})(SMS^{-1}),$$ and so the eigenvalues $\theta$ of $QM$ have the form $\theta = \xi\lambda$ where $\xi$ is an eigenvalue of $Q$ and $\lambda$ is an eigenvalue of $M$ . Since the eigenvalue $\theta_1$ agrees with $\lambda_1$ (which has a multiplicity of $1$ as an eigenvalue of $M$ ), it follows that $\theta_1$ has a multiplicity of $1$ as an eigenvalue of $QM$ . Moreover, since $\lambda_0 = 0$ has a multiplicity of $m_0$ in $M$ , it follows that $0$ is an eigenvalue of $QM$ also with multiplicity of $m_0$ . Therefore, all other eigenvalues of $QM$ have the form $\xi\lambda_2$ . We know that there are exactly $m_2$ such eigenvalues. The Problem Payne and Thas claim the following fact: For each divisor $d$ of $n$ , let $U_d$ denote the sum of all primitive $d$ -th roots of unity. Then $U_d$ is an integer. For each divisor $d$ of $n$ , the primitive $d$ -th roots of unity all contribute the same number of times to eigenvalues $\theta$ of $QM$ with $|\theta| = \lambda_2$ . Let $a_d$ denote the multiplicity of $\xi_d\lambda_2$ as an eigenvalue of $QM$ , with $d \mid n$ and $\xi_d$ a primitive $d$ -th root of unity, then we have $$\operatorname{tr}(QM) = \lambda_1 + \sum_{d \mid n}(a_dU_d)\lambda_2.$$ This bolded statement is what I am having trouble understanding. If $\xi_1$ and $\xi_2$ are two different primitive $d$ -th roots of unity, why should the multiplicity of $\xi_1\lambda_2$ and $\xi_2\lambda_2$ be equal? If $\xi$ is a $d$ -th root of unity, and there exists an eigenvalue $\xi\lambda_2$ of $QM$ , why do all other $d$ -th roots of unity appear as well?","['matrices', 'eigenvalues-eigenvectors', 'finite-geometry', 'roots-of-unity']"
2722629,Is it true that $b^n-a^n < (b-a)nb^{n-1}$ when $0 < a< b$?,"A Real Analysis textbook says the identity
$$b^n-a^n = (b-a)(b^{n-1}+\cdots+a^{n-1})$$ yields the inequality
$$b^n-a^n < (b-a)nb^{n-1} \text{ when } 0 < a< b.$$
(Note that $n$ is a positive integer) No matter how I look at it, the inequality seems to be wrong. Take for instance, the inequality does not hold for $n=1$ when one tries mathematical induction. It does not hold for other values of $n$ too. I guess there is something I am missing here and I will appreciate help.",['real-analysis']
2722649,Can a function $f:X\to \mathbb{R}$ be re-written as a function $g:X\times Y\to \mathbb{R}$?,"In other words, if I have a single-variable function can I write it as a multi-variable function (without changing the mapping at all)? I feel like the answer should be yes, because I should just be able to define $g(x,y) \equiv f(x)\quad  \forall y \in Y$ If this is indeed true, then does that mean that the set  $\{g\vert g:X\times Y\to \mathbb{R}\}$ nests the set $\{f\vert f:X\to\mathbb{R}\}$?","['elementary-set-theory', 'notation', 'functions']"
2722666,The $\mathbf{F}$-metric induces the weak topology on the set of bounded varifolds,"Some preliminary definitions and notation: (1) Given a vector space $\mathbb{V}$, we denote by $G_k(\mathbb{V})$ the $k$-grassmannian of $\mathbb{V}$ , i.e. the set of all $k$-dimensional vector subspaces of $\mathbb{V}$; (2) Given a differential (or riemannian) manifold $M$, we denote by $G_k(M)$ the $k$-grasmannian bundle on $M$ , i.e. the bundle with fibers $G_k(T_pM)$; (3) A $k$-dimensional varifold on $M$ is any Radon measure $V$ on $G_k(M)$; (4) We denote by $\mathcal{V}_k(M)$ the set of all $k$-dimensional varifolds on $M$; (5) The mass of a varifold $V\in \mathcal{V}_k(M)$ is defined by $\|V\|=\int_{G_k(M)}dV$. One can define a topology - the weak topology - on $\mathcal{V}_k(M)$ as follows. Let $J:=\mathcal{C}_c(G_k(M))$ be the set of compactly supported continuous functions $f:G_k(M)\to \mathbb{R}$ and consider the family of functions $\{\varphi_f\,:\, f\in J\}$ where
$$\begin{matrix}
\varphi_f:&\mathcal{V}_k(M)&\to&\mathbb{R}\\
&V&\mapsto&\int_{G_k(M)}f\,dV.
\end{matrix}$$
The weak topology on $\mathcal{V}_k(M)$ is the least topology on $\mathcal{V}_k(M)$ for which every $\varphi_f$ is continuous or, in other words, is the topology on $\mathcal{V}_k(M)$ generated by the subbasis
$$\beta=\{\varphi_f^{-1}(\Omega)\,:\, f\in J,\,\Omega\subset_{op} \mathbb{R}\}.$$ One can ask if the weak topology on $\mathcal{V}_k(M)$ is metrizable. Well, define $\mathbf{F}:\mathcal{V}_k(M)\times \mathcal{V}_k(M)\to [0,+\infty]$ by
$$\mathbf{F}(V,W)=\sup E(V,W),$$
with 
$$E(V,W)=\left\{\int_{G_k(M)}f\,dV-\int_{G_k(M)}f\,dW\,:\, f\in J,\,|f|\leq 1,\,\mathrm{Lip}(f)\leq 1\right\}.$$ One verifies that $\mathbf{F}$ has the three axioms of a metric, although it can be $+\infty$. But given any $c\geq 0$, $\mathbf{F}$ is always finite on 
$$\mathcal{V}_{k,c}(M):=\{V\in \mathcal{V}_k(M)\,:\, \|V\|\leq c\}.$$ Indeed, for any $f\in J$, $|f|\leq 1$, $\mathrm{Lip}(f)\leq 1$,
$$\int_{G_k(M)}f\,dV-\int_{G_k(M)}f\,dW\leq \int_{G_k(M)}\,dV+\int_{G_k(M)}\,dW=\|V\|+\|W\|\leq 2c.$$ Therefore, $\mathbf{F}$ is a finite genuine metric on $\mathcal{V}_{k,c}(M)$. I want to prove that: ($\ast$) The weak topology and the $\mathbf{F}$-induced topology coincide on $V_{k,c}(M)$. This $\mathbf{F}$-metric appears in several papers in the internet (generally talking about minimal surfaces) but always with its definition and the claim ($\ast$) above, followed by some sort of ""it is easy to prove"" or ""one easily verifies"". Is it really that easy? I guess I could prove that $\tau_w\subset \tau_{\mathbf{F}}$ but, given an open $\mathbf{F}$-ball $B_\epsilon(V)$, how do I find a weak-basic open $U$ with $V\in U\subset B_\epsilon(V)$? (such open must be a finite intersection of elements of $\beta$. Which $f_i$'s and real opens $I_i$'s to choose?)","['geometric-measure-theory', 'general-topology', 'measure-theory', 'metrizability']"
2722673,Counting how many ways 30 green balls...,"How many ways are there to distribute 30 green balls to 4 persons if Alice and Eve together get no more than 20 and Lucky gets at least 7? The answer given to me was $2464 = C(26, 3) − 66 − 46-24$ but I got $C(26, 3) - 6$. Here is what I did: Using ""stars and bars"" (or whatever the actual name for it is), I found the total number of possible combinations. Since 7 balls must immediately go to Lucky, there are 23 balls left to distribute. There are 3 'bars', so the total number of possible combinations is $\binom{26}{3}$. Because this number accounts for the situations where Alice and Eve have $
\geq 21$ balls, I must subtract those out. (Here is where I think I messed something up). When Alice and Eve have 21: This leaves 2 balls left to distribute, so $\binom{3}{2}$ When Alice and Eve have 22: This leaves 1 ball left to distribute, so $\binom{2}{1}$ When Alice and Eve have 23: This leaves no balls left to distribute, so it equals 1. So $C(26, 3) - 6$. Where did I go wrong?","['combinatorics', 'discrete-mathematics']"
2722679,Cardinality of a coxeter group,"Let ${G}$ be a Coxeter group with the next presentation 
\begin{equation}
G = \left\langle s_1,s_2,\cdots,s_{n-1} : (s_is_{i+1})^3=1 , \ (s_is_j)^2=1 \ ,\ |i-j| > 1 \right\rangle
\end{equation}
Where $S=\{s_1,\cdots,s_{n-1}\}$ denote the set of generators of the group I need to prove that $G \cong S_n $. For that, I define a function $\phi : S \longrightarrow S_n $ by $\phi(s_i)=(i,i+1)$. it is clear that the images of this function satisfy the relations of the group G, so I can extend $\phi$ to a homomorphism $\widetilde\phi:G \longrightarrow S_n$. This homomorphism is surjective, hence $|G| \geq n!$, so if i can show that $|G|\leq n!$ then $\widetilde\phi$ is a isomorphism. My idea is: I denote by $S_k=\{s_1,\cdots,s_{k} \}$ and $G_k=\left\langle S_k : (s_is_{i+1})^3=1 , \ (s_is_j)^2=1 \ ,\ |i-j| > 1   \right\rangle$. I want to prove that $|G_k|\leq (k+1)!$ Some hint?","['abstract-algebra', 'group-theory', 'symmetric-groups', 'coxeter-groups']"
2722690,Inclusion between spin groups?,"I think this should have an answer, but I can't see what it is. It's inspired by the section labelled ""Spinors"" in Parker's and Taubes's paper, ""On Witten's Proof of the Positive Energy Theorem."" Here's the question: Take $V$ be a real four dimensional vector space with an inner product of signature $(3,1)$, e.g. Minkowski space. By choosing a timelike vector $e_0$ we get an inclusion of $SO(3)$ into the identity component of $SO(3,1)$ by identifying elements of $SO(3)$ with transformations that fix the the orthogonal complement of $e_0$. Does this induce an injection of $Spin(3)$ into $Spin(3,1)$ such that the inclusions commute with the covering maps?","['spin-geometry', 'differential-geometry', 'general-relativity', 'lie-groups']"
2722718,Hahn-Banach extensions from $E$ to $E^{**}$.,"I was thinking the following problem while reading some functional analysis notes. Is it possible to characterize the Hahn-Banach extensions (meaning, extensions with the same norm) of a
functional in a Banach space $E$ to the double continuous dual $E^{**}$ ? My intuition tells me that there aren't many. This is due to two facts: Theorem 1 (Goldstine) The unit ball $B_E$ of $E$ is $w^*$ -dense in the unit ball of $E^{**}$ , $B_{E^{**}}$ Theorem 2 There is a canonical (H-B) extension of a functional $\varphi \in E^*$ given by the natural map $E^* \to E^{***}$ . Even more so, this map splits. Are there examples of spaces and functionals that admit many H-B extensions to its double dual? Just for reference, the first example one would think is $c_0$ which has $c_0^{**}=\ell^\infty$ . But in this case, it is well known that the H-B extension is always unique. Edit : I'll extend the results I know a little in order to attain a possible answer. Theorem 3 (Phelps) Given a closed subspace $Y$ of a Banach space $X$ , every functional on $Y$ has a unique norm-preserving extension if and only if the distance from a functional $f \in X^*$ to $Y^\perp$ is attained uniquely, in the sense that there exists a unique $g \in Y^\perp$ such that $$ d(f,Y^\perp) = \|f-g\|$$ Form this is an easy exercise that the Hahn-Banach extension is always unique if and only if the dual space is strictly convex. Using theorem 3 and theorem 2, it is also possible to observe the following: The triple dual, $E^{***}$ splits $$ E^{***} = E^* \oplus E^\perp$$ Then every functional on $E$ admits unique H-B extension to $E^{**}$ if and only if the norm on $E^{***}$ satisfies $$\|f\|_{E^{***}} = \|f|_E\|_{E^*} + \| f - f_E \|_{E^\perp} $$ Is this always the case?
Thanks in advance.","['functional-analysis', 'banach-spaces', 'dual-spaces']"
2722766,Compare the heights of two midpoints in a hyperbolic triangle,"Let  $\Delta ABC$ be a triangle in Hyperbolic Geometry . Suppose that the midpoints of $AB$ and $AC$ are $E$ and $F$ respectively. Then can we compare the their distances to the base $BC$? In other words, can we compare the ""heights"" of these two midpoints? Are they equal? I guess they are related to length of $AB$ and $AC$, but I am not very clear about the precise relation.","['hyperbolic-functions', 'hyperbolic-geometry', 'trigonometry', 'triangles']"
2722792,Structure coefficients of a coframe,"From the textbook ""Introduction to General Relativity, Black Holes & Cosmology"" by Yvonne Choquet-Bruhat, p.10: In a moving coframe in a domain $U$, the differentials of 1-forms $\theta^i$ are given by: $$d\theta^i \equiv -\cfrac{1}{2} C^i_{jk} \theta^j \wedge \theta^k$$ Show that the structure coefficients of a coframe $\theta^i := a^i_j dx^j$ are given by: $$ C^i_{hk} \equiv A^j_k\partial_ha^i_j-A^j_h\partial_k a^i_j$$ Where $A$ is the inverse matrix of $a$. $\underline{\text{An attempt at the solution:}}$ $$
d\theta^i = da^i_j \wedge dx^j \\ = \frac{\partial a^i_j}{\partial x^k} dx^k \wedge dx^j \\ = -\frac{\partial a^i_j}{\partial x^k} dx^j \wedge dx^k \\ = - \frac{\partial a^i_j}{\partial x^k} (A^j_l \theta^l) \wedge (A^k_m \theta^m ) \\ = - [\frac{\partial a^i_j}{\partial x^k}A^j_lA^k_m]\theta^l\wedge\theta^m \\ \implies C^i_{lm} = 2 \frac{\partial a^i_j}{\partial x^k}A^j_lA^k_m
$$ Which is clearly incorrect as I only have a single term in my result; where is the error in my working?","['tensors', 'smooth-manifolds', 'differential-forms', 'differential-geometry', 'index-notation']"
2722798,Formalize idea of 'homeomorphism that preserves geodesics'?,"I am seeking to formalize the following idea : Take a sheet of paper and lay it flat on a table. Choose two points A and B on the paper, then draw a line segment from A to B. Fold the sheet of paper so that the crease lays across the line segment, then reopen it so that no part of the paper touches itself. Then [ it seems to me that ] that the shortest path from A to B while staying on the sheet of paper is marked out by the image of the line segment. What I have tried : Formally we could identify manifold $M$ as the original sheet of paper, manifold $N$ as the result of folding and then reopening the paper, and $f : M \rightarrow N$ as the actions of 'fold and then reopen'. I think that $f$ can be taken as a homeomorphism, but I hesitate to call $f$ a geodesic map as $N$ is not smooth. How can I describe what $f$ is? (I consulted the answers to earlier questions about turning 2D trajectories into 2D geodesics and length-minimizing curves being geodesics , and also Petrunin & Yashinski's very-approachable lectures on piecewise distance preserving maps. The formalisms there didn't quite seem to fit.) Petrunin, Anton; Yashinski, Allan , Lectures on piecewise distance preserving maps , Mathematics (2014)","['manifolds', 'metric-geometry', 'differential-geometry', 'geodesic']"
2722893,The Modulus of all the roots of a Polynomial are equal to $1$,"Suppose the real number $\lambda \in (0,1)$, and let $n$ be a positive integer. Prove that all roots of the polynomial $$f\left ( x \right )=\sum_{k=0}^{n}\binom{n}{k}\lambda^{k\left ( n-k \right )}x^{k}$$ have modulus equal to $1.$ The Putnam problem 2014 B4 is similar:  Show that for each positive integer $n,$ all the roots of the polynomial $\sum_{k=0}^n 2^{k(n-k)}x^k$ are real numbers.","['real-analysis', 'polynomials', 'roots', 'calculus']"
2722902,Proving the trig identity $\frac{1-\cos\theta}{\sin\theta} = \frac{\sin\theta}{1+\cos\theta}$ without cross-multiplying,I need to prove the following identity. $$\frac{1-\cos\theta}{\sin\theta} = \frac{\sin\theta}{1+\cos\theta}$$ I want to prove it by deduction rather than cross multiplying.,['trigonometry']
2722955,"Image of a basis forms a basis, if and only if matrix is invertible","Suppose $B_1=\{v_1,v_2,...,v_n\}$ is a basis of $\mathbb{R}^n$, and $M$ is an $n*n$ matrix. Prove that $B_2=\{Mv_1,Mv_2,...,Mv_n\}$ is also a basis of $\mathbb{R}^n$ if and only if $M$ is invertible. Following is what I have so far: Assume $B_2$ is basis of $\mathbb{R}^n$. Then, $B_2$ is a set of linearly independent vectors, and $B_2$ spans $\mathbb{R}^n$. Since $B_1$ is also a basis of $\mathbb{R}^n$, then any element(vector) of $B_2$ is a linear combination of elements(vectors) of $B_1$ and vice-versa. $Mv_1= a_{11}v_1+a_{21}v_2+...+a_{n1}v_n$ , where $a_{11},a_{21},...,a_{n1}\in \mathbb{R}$ Likewise, $Mv_2= a_{12}v_1+a_{22}v_2+...+a_{n2}v_n$ , where $a_{12},a_{22},...,a_{n2}\in \mathbb{R}$ $\begin{bmatrix}Mv_1&Mv_2&...&Mv_n\end{bmatrix}=\begin{bmatrix}v_1&v_2&...&v_n\end{bmatrix}\begin{bmatrix}a_{11}&a_{12}&...&a_{1n}\\a_{21}&a_{22}&...&a_{2n}\\ \vdots&\vdots&\vdots&\vdots\\a_{n1}&a_{n2}&...&a_{nn}\end{bmatrix}$ Not sure what to do next ...","['matrices', 'proof-writing', 'linear-algebra', 'hamel-basis']"
2722999,Metallic Riemannian manifolds,"I have recently heard about the existence of the so called metallic Riemannian manifolds . As far as I understand, those are manifold with a polynomial structure, compatible with the Riemannian metric, induced by a $(1,1)$ tensor $J$ satisfying a metallic equation
$$
J^2 = pJ + qI
$$
where $p$ and $q$ are positive integers and $I$ is the identity operator. Why are they studied? Is there any geometric application? I mean, does the existence of a metallic structure imply some interesting geometric or topological property of the manifold?","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry', 'differential-topology']"
2723035,Maximum Likelihood Estimate with Multiple Parameters,"I am not very familiar with multivariable calculus, but something tells me that I don't need to be in order to solve this problem; take a look: Suppose that $X_1,...,X_m$ and $Y_1,...,Y_n$ are independent exponential random variables with $X_i\sim EXP(\lambda)$ and $Y_j\sim EXP(\theta \lambda)$ . Find the $MLE$ of $\lambda$ and $\theta$ . Finding the MLE of $\lambda$ is simple; by ignoring the $Y_j$ altogether and just looking at the $X_i$ , it turns out to be $\sum x_i/m$ .  However, for $\theta$ , I am no longer sure since the distribution of $Y_j$ is also dependent on $\lambda$ .  I don't know if I need to go as far as finding the gradient or if I can somehow use my previous result, but either way, I honestly don't know how to do it. Any advice would be appreciated.","['statistical-inference', 'probability-distributions', 'maximum-likelihood', 'statistics', 'parameter-estimation']"
2723075,Countable elementary sub-structure of the automorphism group of the binary rooted tree,Let $G$ be the automorphism group of the binary rooted tree. The downward Löwenheim-Skolem theorem states that G has a countable elementary sub-structure. My question is whether such sub-structure is explicitly known?,['group-theory']
2723091,Why does the expected value of $\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right]$ equal $(\mu_4-\sigma^4)/4$?,"Question Let $X_i,X_j,X_k$ be IID random variables with finite moments. In particular we denote $E[X_i] := \mu$, $E[(X_i - \mu)^4] := \mu_4$ and $Var[X_i] := \sigma^2$. Why does the expected value of $$\left[{1\over2}(X_i-X_j)^2-\sigma^2\right] \left[{1\over2}(X_i-X_k)^2-\sigma^2\right]$$ equal $(\mu_4-\sigma^4)/4$? This question comes from this answer (point 2) to a question about the variance of the sample variance. My attempt The only route I see is to expand the product, I obtain $$ \frac{1}{4} E[X_i^4] + E[X_i^2]^2 - E[X_i^3]E[X_i] -\frac{5}{2} \sigma^2 E[X_i^2] +3\sigma^2E[X_i^2] + \sigma^4$$ which is not correct.","['statistics', 'probability', 'expected-value']"
2723139,Relation between simple critical points of Hamiltonian and gradient systems,"I'm doing some exercices to see how are related the hamiltonian and the gradient systems. I did an exercise, but I don't know if my approach is correct and I have a few questions about it. Let's state it: Let $H:\mathbb{R}^2 \to \mathbb{R}$ be a $C^2$ function. We consider the Hamiltonian: \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}\end{cases} and the gradient systems: \begin{cases}\dot{x}=\frac{\partial H(x,y)}{\partial x}\\ \dot{y}=\frac{\partial H(x,y)}{\partial y}\end{cases} a) Given an ODE on the plane, we'd say that a simple critical point is a critical point whose eigenvalues are different of $0$ . Prove that if we have a simple critical point for one of those systems, it's also a simple critical point for the other one. So here's my try: Let's suppose that we have a simple critical point for a hamiltonian system. We know that if $(x,y)$ is a simple critical point, we have \begin{cases}\dot{x}=-\frac{\partial H(x,y)}{\partial y}=0\\ \dot{y}=\frac{\partial H(x,y)}{\partial x}=0\end{cases} It's straightforward that: \begin{cases}\frac{\partial H(x,y)}{\partial x}=0\\ \frac{\partial H(x,y)}{\partial y}=0\end{cases} so it's a simple critical point of the gradient system too. It's correct? The condition of ""simple"" means that $(x,y)\neq (0,0)$ ? Thanks.",['ordinary-differential-equations']
2723169,"If $\sqrt{1-x^2}+\sqrt{1-y^2}=a(x-y)$, prove that $\frac{dy}{dx}=\sqrt{\frac{1-y^2}{1-x^2}}$","If $\sqrt{1-x^2}+\sqrt{1-y^2}=a(x-y)$, prove that $\frac{dy}{dx}=\sqrt{\frac{1-y^2}{1-x^2}}$ My Attempt $$
\frac{-2x}{2\sqrt{1-x^2}}-\frac{2y}{2\sqrt{1-y^2}}.\frac{dy}{dx}=a-a\frac{dy}{dx}\\
\implies \frac{dy}{dx}\bigg[a-\frac{y}{\sqrt{1-y^2}}\bigg]=a+\frac{x}{\sqrt{1-x^2}}\\
\frac{dy}{dx}=\frac{a\sqrt{1-x^2}+x}{\sqrt{1-x^2}}.\frac{\sqrt{1-y^2}}{a\sqrt{1-y^2}+x}=\sqrt{\frac{1-y^2}{1-x^2}}.\frac{a\sqrt{1-x^2}+x}{a\sqrt{1-y^2}-y}
$$ How do I poceed further and find the derivative ?","['derivatives', 'calculus']"
2723181,Distribution of Squared Euclidean Norm of Gaussian Vector,"If $\mathbf{X} \sim \mathcal{N}_N(\mathbf{m}, \mathbf{C})$ is an $N$-dimensional gaussian vector, where $\mathbf{m} \in \mathbb{R}^{N}$ and $\mathbf{C} \in \mathbb{R}^{N \times N}$, what is the distribution of
$$
Y = \| \mathbf{X} \|^2
$$
where $\| \cdot \|$ denotes the $L_2$-norm (Euclidean norm) ? It may be useful to know that the mean can be easily calculated via
$$
\mathbb{E}[ \| \mathbf{X} \|^2 ] = \mathbb{E}\left[\sum_{i=1}^N X_i^2 \right] = \sum_{i=1}^N \mathbb{E}[X_i^2] = \sum_{i=1}^N (\sigma^2_i + m_i^2) = \sum_{i=1}^N\sigma_i^2 + \sum_{i=1}^N m_i^2 = \mathrm{tr}(\mathbf{C}) + \| \mathbf{m} \|^2
$$
where $\mathrm{tr}(\cdot)$ denotes the trace of a matrix. EDIT: Related question: link .","['statistics', 'normal-distribution', 'probability-distributions']"
2723192,Implications of conditional independence between random variables,"Consider two probability spaces $(\mathcal{I}, \mathbb{P}, \mathcal{F})$, $(\mathcal{J}, \mathbb{P}, \mathcal{G})$. Take any $a\in \mathbb{R}$. In the notation below, $1\{\cdots\}$ is $1$ if the condition inside is satisfied and $0$ otherwise. Consider the following random variables: 1) $\forall i \in \mathcal{I}$, $e^i: \mathcal{J}\rightarrow \mathbb{R}$ 2) $Z: \mathcal{J}\rightarrow \mathcal{Z}\subseteq \mathbb{R}$, with $\mathcal{Z}$ finite 3) $W: \mathcal{I}\rightarrow [0,1]$, where $W(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big)$ 4) $Q: \mathcal{I}\rightarrow 
\{0,1\}$, where $Q(i)\equiv 1\{W(i)>0\}$ 5) $\forall z \in \mathcal{Z}$, $W_z: \mathcal{I}\rightarrow [0,1]$, where $W_z(i)\equiv \mathbb{P}\Big(\{j\in \mathcal{J} \text{ s.t. } e^i(j)\leq a\}\Big| \{j\in \mathcal{J} \text{ s.t. } Z(j)=z\}\Big)$ 6) $\forall z \in \mathcal{Z}$, $Q_z: \mathcal{I}\rightarrow \{0,1\}$, where $Q_z(i)\equiv 1\{W_z(i)>0\}$ Assume that for some $z\in \mathcal{Z}$
$$
E\Big(W\Big)= E\Big(W_z\Big)
$$
where $E$ denotes expectation. Does this imply
$$
E\Big(Q \Big)= E\Big(Q_z\Big) \text{ ?}
$$ I've done some simulations and it seems that the answer is no but I would like some help to formalise this. Also, are we using somewhere that $\mathcal{Z}$ is finite and that the two probability spaces have the same probability measure?","['conditional-expectation', 'probability', 'random-variables']"
2723223,What can we say about the minimal polynomial over a field $\mathbb{F}$,"Let $\mathbb{F}$ be a field. Suppose $A \in GL(m,\mathbb{F})$, i.e. $A$ is an $m \times m$ invertible matrix with coefficients in $\mathbb{F}$. Now let $A$ have order $n$. What can we say about the minimal polynomial of $A$?","['matrices', 'abstract-algebra', 'linear-algebra', 'minimal-polynomials']"
2723251,Matrix square roots of -I,"Since we can see matrices as generalizations of complex numbers, I asked myself if there is a way to classify those matrices which are the ""Basis"" for the complex part. That is, I would like to identify the set of $n\times n$ real valued matrices $M$ whose square $M^2$ is equal to $-I$, where $I$ is the $n\times n$-identity matrix. Is this set already classified? The matrix $J = \left( \begin{smallmatrix}0 & -1\\1 & 0 \end{smallmatrix} \right)$ satisfies $J^{2} = -I$ in the 2-dimensional case. EDIT: Thanks to your comments and answers I have the following observation (if its wrong, please tell me): Suppose that $M$ satisfies that $M^t=-M$, then we can observe for two vectors $x,y\in\mathbb{R}^{2n}$ that $y^tAx=(Ax)^{t}y=x^{t}A^ty=-x^tAy$ My interpretation is that if the angle between $x$ and $Ay$ is $\alpha$ then $\pi+\alpha$ is the angle between $Ax$ and $y$, since ${\displaystyle \cos \;x=-\cos(x+\pi )}$ (here we suppose that the length of the vectors are not relevant). If now $y=x$ and then we can see that $Ax$ is orthogonal to $x$. Is it true that $A^2$ will be a rotation of 180 degrees, right? Because of vector length we will have something like $A^2=-CI$, where $C$ is just a constant. Is that true?","['matrices', 'linear-algebra']"
2723294,How to determine the equation of the hyperplane that contains several points,"I have a question regarding the computation of a hyperplane equation (especially the orthogonal) given n points, where n>3. For lower dimensional cases, the computation is done as in : http://tutorial.math.lamar.edu/Classes/CalcIII/EqnsOfPlanes.aspx Here we simply use the cross product for determining the orthogonal. However, best of our knowledge the cross product computation via determinants is limited to dimension 7 (?). How to get the orthogonal to compute the hessian normal form in higher dimensions? Thank you in advance for any hints and
with best regards
Dan","['orthogonality', 'linear-algebra', 'vectors']"
2723323,Plane tangent to three circles,"I want to find the planes tangent to three given circles in 3D space. I'm not sure how many solutions there are, in general. My guess is that there are 8. This suggests that we might have to find the roots of some polynomial of degree 8, which would be bad news. A Google search for ""plane tangent to three circles"" yields exactly one result, which is this question . It was asked in 2011, and was not answered. Maybe the nasty notation scared people away, so let me suggest a nicer one: Let's call the three circles $C_1$, $C_2$, $C_3$, and suppose that $C_i$ is defined by a center point $P_i$, a radius $r_i$, and a unit vector $N_i$ normal to its plane. So, again, the question is: find the equations of the tangent planes in terms of the $P_i$, $r_i$, and $N_i$. The case where the three radii are equal is of some interest, if that's easier. Also, I'm interested only in the case where the circles are in ""general position"", which means (I think) that the number of solutions is finite but non-zero. So, please feel free to ignore special cases like the circles having a common tangent line, or being coplanar, or lying on a common cylinder or cone, etc.","['analytic-geometry', 'solid-geometry', 'geometry']"
2723327,Question about eigenvalues when a sequence of matrix converges,"Let ${A_n}$ be a sequence of $p \times p$ symmetric positive semi-definite matrix and $A_n$ converges to a matrix $A$, that is every elements of $A_n$ converges to corresponding element of $A$. Then can I say anything about the relationship between the eigenvaues of $A_n$ and $A$?
For example, If the norm of $A_n-A$ is small enough, can I make the eigenvalues of $A_n$ close enough to that of $A$?","['matrices', 'normed-spaces', 'eigenvalues-eigenvectors', 'convergence-divergence']"
2723333,Branching process and calculating the probability of extinction,"Suppose the male descendants of a man follow a branching process.We further suppose that each man has 3 children, and the number of male descendants follows a binomial distribution, Bin(3,0.5). (a) What is the probability that Guy A line of male descendants will become extinct by the third generation? How about becoming extinct exactly at the third generation? (b) Suppose Guy B has two sons and a daughter.What is the probability that Guy B line of male descendants will eventually become extinct? I have just started learning the branching process and not sure whether I did the question correctly.My working for the question is as follows, (a) I know that the generation function is of the form: $$P(s)=\frac18 + \frac38 s + \frac38 s^2 + \frac18 s^3 = \frac18(1+s)^3,$$ Let Un=P(Xn=0) where Xn denotes the number male descendants at time n.
$$U(1)=P(X1=0)=\frac18$$
$$U(2)=P(U(1))=\frac{729}{4096}$$
$$U(3)=P(U(2))\approx 0.204325$$ Let T=min{n $\ge$ 1 : Xn=0) So I think the question in part (a) is asking P(T$\le$3|X0=1) and U(3), $$P(T\le3|X0=1)=U(1)+U(2)+U(3)\approx 0.5073$$ (b)
If the initial generation has 1 male descendant, then the probability of it being extinction will be equals to solving P(u)=u where $0<u<1$. Solving it I get that the roots of u are 1,$-2+\sqrt 5$,$-2-\sqrt5$. Since $0<u<1$, then the probability of it being extinct given that guy B initial generation has 2 male descendants will be: $$(-2+\sqrt 5)^2=9-4\sqrt5$$","['stochastic-processes', 'probability']"
2723367,"Function $g(x)$, such that $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$ for all $f\in L^1(\mathbb{R})$","It's not too hard to show that for all $f\in L^1(\mathbb{R})$ and for $g(x) = x-\frac{1}{x}$ we have $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$:use the substitution $y = x - \frac{1}{x}$ and split the integral in two parts: $x<0$ and $x>0$. On one of them make the choice $x = \frac{y + \sqrt{y^2 + 4}}{2}$, on the other $x = \frac{y - \sqrt{y^2 + 4}}{2}$ and add the resulting integrals. The existence of even one function $g(x)$, such that the above works is astonishing. Are there other choices of $g$, such that $\int_{\mathbb{R}} f(g(x))dx = \int_{\mathbb{R}} f(x)dx$ for all $f\in L^1(\mathbb{R})$? Edit: $g(x) = x + a$ for any a would be a solution, but I was wondering if there are more complicated $g$ which work, or if $x - \frac{1}{x}$ is just a single example.","['real-analysis', 'integration', 'definite-integrals', 'calculus']"
2723369,Improper integral of the form $I=\int_{0}^{\infty} \frac{\sin(ax)}{x^2+b^2}dx$,"My teacher showed in class that starting with $\int_{-\infty}^{\infty} e^{inx}g(x)dx$ for an even function $g(x)$ we have 
$$\int_{0}^{\infty} \cos(nx)g(x)dx=\pi i\sum \operatorname{Res}\left( f(z)\right)$$ and for an odd function $g(x)$:
$$\int_{0}^{\infty} \sin(nx)g(x)dx=\pi \sum \operatorname{Res} \left(f(z)\right)$$ with $n>0$, $f(z)=e^{inz}g(z)$ and $\lim\limits_{z\to\infty}g(z)=0$ However, when he tried to do an example, by mistake he forgot to add an $x$ in the numerator and left it as 
$$\int_{0}^{\infty} \frac{\sin(4x)}{x^2+4}dx$$ and added the $x$ later. While $$\int_{0}^{\infty} \frac{x\sin(4x)}{x^2+4}dx$$ is not that hard to evaluate, many solutions are found here I tried to do the first one when I got home, but the methods that I learned until now were unsuccessful. Can I get some help on how to evaluate $I=\int_{0}^{\infty} \frac{\sin(4x)}{x^2+4}dx$ please?","['complex-analysis', 'improper-integrals', 'contour-integration']"
2723385,Understanding of Spin(n) and SO(n),"I want to make sure I understand the relation between spin and rotation (mainly between SU(2) and SO(3), but also in general). (I am a physics major, so I apologize if my statements are not very rigorous, but I want to make sure I understand the basic underlying concepts.) So SU(2) is the double cover of SO(3). Also Spin(3) is the double cover of SO(3). So, SU(2) and Spin(3) are isomorphic. Now I am a bit confused about the objects that these groups act on. If I think of SU(2), they act (in the fundamental representation) on 2 dimensional objects, which are called spinors. Now, if I understand it right, when labeling the representations of SU(2) by j (the value of the angular momentum), if j is half integer it is a spinorial representation, while if j is integer it is a vectorial representation. So for j=1, the object acted upon are 3 dimensional vectors, not spinors? But as we are in SU(2), which are complex matrices, the vectors are complex vectors? So the difference between a complex vector and a tensor is given by the representation to which they belong to? So a 3 dim object which changes under a 3x3 representation of SU(2) is a complex vector, while a 2 (or 4, 6 etc) dimensional object changing under a 2x2 (4x4, 6x6 ..) representation of SU(2) is a spinor? Or is it anything deeper that this? Now if we go to higher spin, lets say that a k dimensional object changes under the k-dim representation of Spin(n) and another k-dim object under k-dim representation of Spin(m). Do we decide whether they are spinors or not based on whether that representation is spinorial or vectorial? I.e. a k-dim object on its own can't be called a complex vector or a spinor, unless we know how it transforms? Please let me know if what I said is wrong, and how should I think about all these? Thank you!","['representation-theory', 'group-theory', 'spin-geometry']"
2723406,How many permutations do you need to force fixed points?,"For simplicity let $k$ be given fixed, and $n$ grows large. We are interested in a small set $M_n={g_1,...,g_m}$ of permutations in $S_n$, s.t for all $a\in S_n$, there is $g_i \in M_n$ with $ag_i$ having at least $k$ fixed points. The question is how large must $M_n$ be asymptotically depending on $k,n$. Another interesting question after we solve this one, is what happens when we force $M_n$ to be a subgroup of $S_n$. If ones assums $n \geq (k+1)!$ there is the following upper bound which will be proved below after the lower bound) Upper bound: $m=O((k+1)!nlog(n))$. Note for $k=1$ we also have the clear upper bound of $n$ taking what's genereated by a large circle. A (sad) lower bound of $m=\theta ((k+1)!)$ also follows in similiar spirit to the upper bound. Here is also a lower bound of $\lfloor(n/2)\rfloor$ for $k=1$ (and in particular for the rest of the $k$): We start with the complete bipartite graph with both sides having $n$ elements, each perfect matching corresponds to a matching which says who the left guy goes to in the right. Each $g_i$ kills a set of edges (i.e if in our permutation that edge exists, then multiplying by $g_i$ would lead to a fixed point for the left guy). Thus what I claim that if you remove a union of any $\lfloor(n/2)\rfloor-1$ perfect matchings off the complete bipartite graph with $n$ vertices on each side, there is still a perfect matching standing. Note that it is possible to use $\lfloor(n/2)\rfloor$ matchings to cover so that we won't be able to find a perfect matching, so this is tight. Now if we could only use $\lfloor(n/2)\rfloor-1$, then let's see the set that kills Halls condition. Say it is the set $A$ of size $a$. Then it sees a set $B$ of size at most $a-1$. Clearly $a\leq \lfloor(n/2)\rfloor-1$, otherwise $A$ sees everyone after removing the matchings. So we managed to cover the complete bipartite graph $AxB^c$ with $\lfloor(n/2)\rfloor-1$ matchings exhausting $A$. But $B^c$ has size at least $n-(a-1)\geq n- \lfloor(n/2)\rfloor-1+1=n-\lfloor(n/2)\rfloor > \lfloor(n/2)\rfloor-1$, and so $A$ sees all of $B^c$ after removing our matchings,a contradiction. Here is an upper bound as promised and the other lower bound: We want a set of $m$ permutations $g_{1},..,g_{m}$, s.t for all permutations $a$, there is $g_{i}$ with $ag_{i}$ having at least $k$ fixed points. Given a fixed $a$, if we pick a random $g$, what is the probability $ag$ has at least $k$ fixed points? Well since choosing $g$ at random is like $a^{-1}g$ at random, we're asking what is the probability a random $g$ has at least $k$ fixed points. It turns out easier to count the other thing, how many permutations have at most $k-1$ fixed points? Well in paricular how many have exactly $s$ fixed points? That's like fixing the $s$ fixed points, and then picking a derangment (no fixed point) on the rest; number of derangments on $n$ elements is (this one by inclusion exclusion) $n!(\sum_{0}^{n}\frac{(-1)^{i}}{i!})$, for large $n$ this is $\sim\frac{n!}{e}$. Thus the number of those with exactly $s$ fixed points is like $\binom{n}{s}\frac{(n-s)!}{e}$. So the number with at most $k-1$ fixed points is: $\sum_{s=0}^{k-1}\binom{n}{s}\frac{(n-s)!}{e}$. Thus our desired probability is $1-\frac{\sum_{s=0}^{k-1}\binom{n}{s}\frac{(n-s)!}{e}}{n!}=1-\frac{1}{e}(\sum_{s=0}^{k-1}\frac{\binom{n}{s}}{n\cdot(n-1)..(n-s+1)})=1-\frac{1}{e}(\sum_{s=0}^{k-1}\frac{\frac{n(n-1)..(n-s+1)}{s!}}{n(n-1)..(n-s+1)})=1-\frac{1}{e}(\sum_{s=0}^{k-1}\frac{1}{s!})=\frac{\sum_{s=k}^{\infty}\frac{1}{s!}}{e}$, this by Taylor approximation is at least (and up to a constant exactly) $\frac{1}{e(k+1)!}$. At last we can get to our problem. We need to union bound over $n!$ guys, so we want $(1-\frac{1}{e(k+1)!})^{m}<\frac{1}{n!}$. If $m=e(k+1)!$ we get roughly $\frac{1}{e}$, so $e(k+1)!(nlogn)$ is what we want and tight (to get the union bound) up to a const. So our answer is that $m=(k+1)!(nlog(n))$ suffices. To get a lower bound we try to flip our heads as usual. Assume we have just $m$, and we take a random element, the probability m catches us is $\frac{1}{e(k+1)!}$, so the probability at least one of them catches us by union bound is at most $\frac{m}{e(k+1)!}$, so $m\geq(k+1)!$ is a bound.","['permutations', 'combinatorics', 'discrete-mathematics']"
2723408,Volume of a high dimensional cone,"I would like to choose arbitrarily two vectors $a$ and $b$ $\in \mathbb{S}^{n-1}$ and I would like to calculate the probability they are at least some distance $\delta$ apart. This probability should be
$$\frac{|\mathbb{S}^{n-1}|-|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|}{|\mathbb{S}^{n-1}|}$$
So (I think), my problem consists of finding $|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}|$ ($|\cdot|$ being the surface measure in $\mathbb{R}^n$). I suppose $a$ can be taken as the unit vector $(1, 0, \cdots, 0)$. By symmetry, the points $\{ x \in \mathbb{S}^{n-1}: \|x-a\| = \delta \}$ should be a ""circle"" with center somewhere on the line between $(0, \cdots, 0)$ and $a$. 
I need to find that center and the radius. I search for a point on the ""circle"" with only the first two components being non-zero: The center should have coordinates $(1-x, 0, \cdots, 0)$ and the point on the circle has coordinates $(1-x, y, 0, \cdots, 0)$. $x$ and $y$ need to verify
$$x^2 + y^2 = \delta$$ and $$(1-x)^2 + y^2 = 1$$ This gives $x = \delta / 2$ and $y = (\sqrt{3}/4) \delta$. Going back to the surface we want to calculate:
$$|\{ x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta \}| = |\{x \in \mathbb{S}^{n-1}\}\cap \{B((1-\delta / 2, 0, \cdots, 0), r = (\sqrt{3}/4) \delta)\}$$
This should just be the measure of the spherical cap of the sphere centered at $(1-\delta / 2, 0, \cdots, 0)$. To estimate this surface area, I wanted to use that surface area is the push-forward measure of a Gaussian measure on $\mathbb{R}^n$ under $x \rightarrow x/\|x\|$ (times the measure of $\mathbb{S}^{n-1}$ since the latter is a probability measure). So to estimate the above measure, we can also calculate the Gaussian measure of a cone (rays going from $0$ to points in $\{x \in \mathbb{S}^{n-1}: \|x-a\| \leq \delta\})$. Parametrizing this cone, I get $(t, \text{""disc of radius"" }t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}) = (t, disc)$
$$(2\pi)^{-n/2}\int_{(t, disc)}e^{-\|x\|^2/2} = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\int_{(disc)}e^{(x_2^2 + x_3^2 + \cdots + x_n^2)/2}dxdt$$
The last integral is over a radial function so
$$ = (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}\int_0^{t*\frac{\sqrt{3}\delta}{4(1-\delta/2)} }e^{-s^2/2}dsdt$$
We can take $\delta$ as very small, so
$$= (2\pi)^{-n/2}\int_0^{\infty}e^{-t^2/2}\omega_{n-2}t*\frac{\sqrt{3}\delta}{4(1-\delta/2)}dt \leq (2\pi)^{-n/2}\omega_{n-2}\sqrt{3}\delta$$
EDIT: In the end, we have to multiply with $|\mathbb{S}^{n-1}|$ since the above is only the uniform probability on $\mathbb{S}^{n-1}$ Are these calculations correct? I am a bit confused about the term $(2\pi)^{-n/2}$, then $\delta$ could be quite large and we would still not occupy a large amount of the mass.","['gaussian-integral', 'spheres', 'probability', 'geometry', 'convex-cone']"
2723416,Different definitions of closed differential form,"I have recently read, out of curiosity, Tao's very motivating introduction to Differential forms and integration . At pages 5 and 8 he seems to define a closed $k-$form (on some manifold) to be a $k-$form whose integral along any closed $k-$submanifold (i.e., one with zero boundary) vanishes. I am used, instead, to the definition that a form is closed iff its differential is zero. I think that in general Tao's definition is strictly stronger than mine . Here are my thoughts. In the following, we only consider oriented (and, if necessary, compact) manifolds. I believe that the following conditions should all be equivalent for a $k-$form $\omega$: $d\omega=0$ (my definition of being closed) $\int_S d\omega = 0$ for all $(k+1)-$submanifolds $S$ $\int_{\partial S} \omega=0$ for all $(k+1)-$submanifolds $S$, i.e., $\omega$ vanishes on all exact submanifolds The equivalence of 2. and 3. is given by Stokes' theorem. 1. clearly implies 2. and I think that 2. implies 1. because the integration pairing is non-degenerate. In particular, if the above holds, I think that Tao's condition is equivalent to mine iff the $k-$th singular homology of the ambient manifold vanishes $(*)$. I thought maybe de Rham's theorem could solve the problem, but if I interpret it right, it only implies that $\omega$ is exact if and only if its integration along every closed $k-$submanifold gives $0$ $(!)$. Which is kind of dual to 1. $\Leftrightarrow$ 3. above. Now, it seems very strange to me that Tao should use a definition that is not equivalent to the usual one, so I think I must be missing something. Please help me to answer the following: A) Are my three conditions above equivalent and is $(!)$ correct? B) Is $(*)$ right? In particular, when considering singular homology of manifolds, can we restrict to smooth submanifolds (with boundary) instead of arbitrary continuous images of simplices? C) Did I get Tao's definition of closed form right? If so, is it really different from mine? Why?","['differential-forms', 'integration', 'differential-geometry']"
2723434,Finding the missing number in a sequence of factorials,"On one of our tests, the extra credit was to find which number you would take out from the set $\{1!,2!,3!,...(N-1)!,N!\}$ such that the product of the set is a perfect square. My answer was as follows: Assume $N$ is even. First note that  $(n!)=(n-1)!\cdot n$. Apply this to the odd numbers to get the product:
$$(2!)(2!)\cdot 3 \cdot(4!)(4!)\cdot 5\cdots ((N-2)!)((N-2)!)\cdot (N-1)\cdot N!$$ Let $ 2!4!6!...(N-2)!=E$. Then our equation is equal to: 
$$E^23\cdot 5\cdot 7\cdots (N-1)\cdot (N!)$$ Expand $N!$: $$E^2\cdot 3\cdot 5\cdot 7\cdots (N-1)\cdot 2\cdot 3\cdot 4\cdots N$$ Group the odd terms together: $$E^2\cdot 3\cdot 3\cdot 5\cdot  5\cdot 7 \cdot 7\cdots (N-1)\cdot 2\cdot 4\cdot 6\cdots N$$ Let $O=1\cdot3\cdot5...\cdot (N-1)$: $$E^2O^2 2\cdot 4\cdot 6\cdots N = E^2O^2\cdot (2\cdot 2)\cdot (2\cdot 3)\cdot (2\cdot 4)\cdots (2\cdot (N/2))$$ Group together the $2$'s: $$E^2O^22^{(N/2)}1\cdot2\cdot3...(N/2)=E^2O^22^{(N/2)}\cdot(N/2)!$$ So, if $N/2$ is even, it can be expressed as $2m$ for some $m$. So we have: $$E^2O^22^{2m}(N/2)!=(EO2^m)^2(N/2)!$$
Therefore, if $N$ is even, the number missing is $(N/2)!$ if $N/2$ is even. For example, for $N=4$, $2!$ is missing, $N=6$ is impossible ($3$ is odd), and for $N=100$, $50!$ is missing. I got partial credit - but my teacher said I missed a case, and that I should leave a sticky note on her desk with a number on it to show that I've fixed the proof. I think the flaw might be in assuming $N$ is even, but I'm not sure how to deal with the case of $N$ odd. And, I've found that my solution gives $4!$ for $N=8$, but $3!$ is also a solution. 
Past that, how can one number show that I've fixed the proof?","['factorial', 'sequences-and-series', 'elementary-number-theory']"
2723437,The closed points of a scheme and irreducibility,"Suppose $X$ is a scheme. We say a point $x$ is closed in $X$ if $\overline{\{x\}}=\{x\}$. Let $t(X)$ be the subspace of all closed points in $X$. We say X is irreducible if its topological space is irreducible. My question is if it is true that $X$ is irreducible if and only if $t(X)$ is irreducible. And in general if $X$ is only a topological space, $t(X)$ is subspace of all closed points in $X$, if it is true that $ X$ is irreducible if and only if $t(X)$ is irreducible I try to prove it is right. But I spend a lot of time and do not know how to prove it.","['schemes', 'general-topology', 'algebraic-geometry']"
2723472,Why do the characters not determine the group?,"Motivation/Context Looking at finite groups $G$ . Of course the character table is (up to permutation of rows and columns) determined by $G$ up to isomorphism. I thought about why the converse is not true (question 1)? Question 2 Given a complete set of characters of a finite group $G$ , but not the group table (or the generators). What is exactly the minimum amount of information that is missing, necessary to determine the group $G$ (i.e. the group table) up to isomorphism, uniquely? Own efforts I have been looking at the famous example of the quaternion group $Q$ and the dihedral group of order $8$ , $D_4$ . They have up to permutation of rows and elements the same character table. However, they disagree in the order. I understand that a large part of the information necessary to determine the group table up to isomorphism must be contained in the character table, but I fail to pinpoint what is exactly the missing information in the general case.","['representation-theory', 'group-theory']"
2723571,Closed form for fixed $m$ to $\int\frac{dx}{x(x+1)(x+2)(x+3)...(x+m)}$,"$I=\displaystyle\int\frac{dx}{x(x+1)(x+2)(x+3)...(x+m)}$ Attempt: $\dfrac{ A_0 }{ x   }+\dfrac{ A_1 }{ x  +1 }+\dfrac{ A_2 }{ x +  2 }...+\dfrac{ A_m }{ x +m }  =\dfrac{1}{x(x+1)(x+2)(x+3)...(x+m)}$ But things got very messy. I also thought that 1)applying integral by parts, or 2) taking terms one from left head, one from right hand and use some kind of a symmetry, or 3) using trigonometric identites etc. I cannot see the solution, any hint, help would be perfect. Thank you in advanced.","['real-analysis', 'calculus', 'integration', 'sequences-and-series', 'analysis']"
2723576,"Compute $\sum\limits_{n=1}^\infty \arctan\left(\frac{(-1)^{n+1}}{F_{n+1}(F_n+F_{n+2})}\right)$, where $F_n$ is $n$th Fibonacci number","Compute $\sum\limits_{n=1}^\infty \arctan\left(\frac{(-1)^{n+1}}{F_{n+1}(F_n+F_{n+2})}\right)$, where $F_n$ is $n$th Fibonacci number. Attempt: I thought telescopic series method and this identity. $$\arctan a+\arctan b=\arctan\left(\frac{a+b}{1-ab}\right)$$ But I am unable to find an analogy, a hint, or anything.","['fibonacci-numbers', 'sequences-and-series']"
2723577,Center of the group of invertible bounded linear operators with bounded inverse on Normed linear spaces,"Let $X$ be an Normed linear space. Let $Iso(X)$ be the set of all invertible bounded linear operators on $X$ with bounded inverse. If $T \in Iso(X)$ is such that $T \circ S=S \circ T, \forall S \in Iso (X)$, then is it true that $T=kI$ for some scalar $k$ ? If $X$ is Banach space, then the claim is true, which I can show as follows: Let $0\ne v \in X$. Pick $0\ne L_v \in X^*=\mathcal L(X,F)$ ($F$ is the underlying field) such that $||L_v||<\dfrac {1}{2||v||}$. Fix $a\in X$ with $L_v(a)\ne 0$. Define $S:X \to X$ as $S_v(x)=L_v(x)v,\forall x \in X$. Then $||S_v||\le ||L_v||||v||<1/2$. Then $I-S_v \in Iso(X)$ (this is where I need $X$ to be Banach). Let $T \in Iso(X)$ which commutes with every member of $Iso(X)$. Then we have in particular $T((I-S_v)(a))=(I-S_v)(T(a))$ i.e. $T(a-S_v(a))=T(a)-S_v(T(a))$ i.e. $T(a)-T(S_v(a))=T(a)-S_v(T(a))$ i.e. $T(L_v(a)v)=L_v(T(a))v$ i.e. $L_v(a)T(v)=L_v(T(a))v$ i.e. $T(v)=k_v v$ for some scalar $k_v \in F$. And now it can be easily shown that $k_v=k_w,\forall 0\ne v,w \in X$, hence $T$ is a scalar multiple of identity. I don't know what happens if $X$ is not Banach. Please help.","['functional-analysis', 'normed-spaces', 'linear-transformations']"
2723580,Relation between harmonic series $H(m)$ and polygamma function?,"I have the following formula: $$h(x)=\sum_{R=1}^m \frac{1}{R+1}-1$$ I have re-expressed this (correctly, I hope!) in terms of the harmonic number $H(m)=\sum_{R=1}^m \frac{1}{R}$: $$h(x)=H(m)+\frac{1}{m+1}-2$$ However, Mathematica insists on simplifying $h(x)$ to $$\gamma+\psi_0(m+2)-H(m)-2$$ where $\gamma$ is the Euler-Mascheroni constant and $\psi$ is the polygamma function, which various websites tell me is given by $\psi_0(m+2)=\frac{d^1}{d(m+2)^1} \ln \Gamma(m+2)$. Is there a proof for Mathematica's simplification? And have I summarised it correctly? I've never worked with gamma functions, so I'd be grateful for help.","['derivatives', 'harmonic-functions', 'polygamma', 'harmonic-numbers', 'sequences-and-series']"
2723629,Why is the maximum Rayleigh quotient equal to the maximum eigenvalue?,"(Note: I'm only interested in real-valued matrices here, so I'm using ""transpose"" and ""symmetric"" instead of the more general ""transjugate"" and ""Hermitian"" in the hope that it will simplify the proof. But the theorem apparently holds for complex-valued matrices as well.) The Rayleigh quotient $R(M,v)$ of a symmetric matrix $M$ and a vector $v$ is defined as $\frac{v^T M v}{v^T v}$, where $x^T$ is the matrix transpose of $x$. I've been told that the vector $v$ which gives the largest Rayleigh quotient is, in fact, the eigenvector corresponding to the largest eigenvalue of $M$. And furthermore, the value of the quotient in this case is equal to that eigenvalue. However, I've been unable to find a full proof of this fact, or an explanation of why it should work this way. Why is there this connection between the Rayleigh quotient and the eigenvalues? Anything from an intuitive explanation to a formal proof would be appreciated.","['matrices', 'transpose', 'eigenvalues-eigenvectors', 'symmetric-matrices']"
2723640,"An urn contains n red balls, n white, n black. What is the probability of not getting all colors?","I have this problem which I have been struggling with for a while now An urn contains n red balls, n white balls and n black balls. You
  draw k balls at random without replacement ($k\leqq n$). Find an
  expression for the probability that you do not get all colors. I tried to solve this in the following way:
I note that it should be logical to think P(not getting all colours in k draws) = P(getting exactly one color only in k draws OR get exactly two different colors only in k draws). Therefore, I choose the events $A_1=\{$get one color k times$\}$ and $A_2=\{$get two colors k times$\}$ Therefore, we seek $P(A_1 \cup A_2)$. Clearly, $A_1 = \frac{{n\choose k}}{3n\choose k}$ and  $A_2 = \frac{{2n\choose k}}{3n\choose k}$. And because the both events are disjoint, we simply get $P(A_1 \cup A_2) = P(A_1) + P(A_2) = \frac{1}{3n\choose k}\big({n\choose k}+{2n\choose k}\big)$. According to my textbook, however, the answer should be $\frac{3}{3n\choose k}\big({2n\choose k}-{n\choose k}\big)$. What am I doing wrong? Are the answers equivalent because of some mystical binomial-identity?","['binomial-coefficients', 'probability-theory', 'statistics', 'probability', 'combinatorics']"
2723684,Why are Alternating Forms Zero when Evaluated for a Set of Linear Dependent Vectors?,"I am reading a textbook on differential geometry and gauge theories and it it says off-hand that the alternating property of alternating multilinear forms implies that a form of this kind evaluated on a linearly dependent set of vectors is zero. Would someone mind clarifying why the alternating property implies this?  For example, let's say it's a 2-linear form which maps a pair of vectors in the same vector space to the reals, we switch the order of the pair in the Cartesian product and that changes the sign of the real which we map to. $\phi(v_2,v_1)=-\phi(v_1,v_2)$ If the two vectors are linearly dependent so there exists a weighted combination of them added together which adds to zero (or equivalent definition), why does that imply that evaluating the 2-form on that pair of vectors has to equal 0?  I have studied differential forms and other exterior algebra, so any explanation in terms of these is fine, I would just like to know the proof for this. EDIT: I did specify the multilinear case but to I will see if I can generalize to the trilinear case to answer the  comment below, and then you just keep adding on indefinitely for a general case (can someone correct if this is wrong). $\phi(v_1,v_2,v_3)=-\phi(v_3,v_2,v_1)$ If any of these vectors equals 0 the form is 0 by multilinearity.  From linear dependence, a vector in the set can be expressed in terms of the others, say $v_3 = av_2 + bv_2$ $\phi(v_1,v_2,av_1 + bv_2)=-\phi(av_1 + bv_2,v_2,v_1)$ $\phi$ is trilinear so $\phi(v_1,v_2,av_1 + bv_2) = a\phi(v_1,v_2,v_1) + b\phi(v_1,v_2,v_2)$ $-\phi(av_1 + bv_2, v_2, v_1)=-a\phi(v_1,v_2,v_1)-b\phi(v_1,v_2,v_2)$ $a\phi(v_1,v_2,v_1) + b\phi(v_1,v_2,v_2)=-a\phi(v_1,v_2,v_1)-b\phi(v_1,v_2,v_2)$ This implies that $\phi(v_1,v_2,v_3)=0$. For the $k$th case: $\phi(v_1,...,v_{k-1},v_k)=-\phi(v_k,v_{k-1},...,v_1)$ From linear dependence: $v_k=av_1+...+dv_{k-1}$ $\phi(v_1,...,v_{k-1},av_1 +...+dv_{k-1})=-\phi(av_1+...+dv_{k-1},...,v_1)$ $\phi$ is $k$-linear so $\phi(v_1,...,v_{k-1},av_1+...+dv_{k-1})=a\phi(v_1,...,v_{k-1},v_1)+...+d\phi(v_1,...,v_{k-1},v_{k-1})$ $-\phi(av_1...+dv_{k-1},v_{k-1},...,v_1)=-a\phi(v_1,v_{k-1},...,v_1)-...-d\phi(v_{k-1},v_{k-1},...,v_1)$ $a\phi(v_1,...,v_{k-1},v_1)+...+d\phi(v_1,...,v_{k-1},v_{k-1})=-a\phi(v_1,v_{k-1},...,v_1)-...-d\phi(v_{k-1},v_{k-1},...,v_1)$ Hence $\phi(v_1,...,v_{k-1},v_k)=0$.","['differential-forms', 'exterior-algebra', 'differential-geometry', 'multilinear-algebra']"
2723711,"A $5\times 5$ grid of single-digit numbers in $\mathbb N$, with one cell empty. What number should be in the cell?","16 .$$\begin{array}{|c|c|c|c|c|}
\hline
2 & 7 & 4 & 3 & 5
\\\hline
7 & 3 & 4 & 5 & 4
\\\hline
1 & 3 & 2 & 2 & 6
\\\hline
2 & 4 & 5 & 4 & \mathbf{?}
\\\hline
8 & 3 & 6 & 3 & 5
\\\hline
\end{array} $$ Is the answer 1, 2, 3, 4, or 5? Photograph of the problem source Really I don't understand. How does such a question relate to logic? For me, it's a game about numbers.Not Logic. It really annoys me to solve such a question. Anyway, I took 12 minutes for this question in exam. I came home. I could not even ""solve"" it at home ether. I think, such a question is nonsense. No science has anything to do with it. Please help me with the question and please explain me, what does it really mean to solve such a question?","['algebra-precalculus', 'puzzle']"
2723724,"Show that for $n!/2 - p$, $p$ is the least prime factor if $n > p$","Apparently this isn't clear, so here are some examples: $$3!/2 - 3$$
$$4!/2 - 3$$
$$5!/2 - 3$$ and 
$$5!/2 - 5$$
$$6!/2 - 5$$
... How would I go about proving this? I found it from a matrix I generated from looking at error functions.","['number-theory', 'prime-numbers']"
2723727,Differentiating the single-layer potential,"Suppose $f\in L^2[-1,1]$ and consider the single layer potential with moment $f$ on $[-1,1]$
$$ Kf(x,y) = -\frac{1}{2\pi}\int_{-1}^1 \ln|(x,y) - (\xi,0)|f(\xi)\, d\xi $$
Formally I shown that for $x\in[-1,1]$
$$ \frac{\partial Kf}{\partial y}\bigg|_{y=0} = \frac{f(x)}{2} $$
by differentiating under the integral sign. But I have trouble justifying interchanging the differentiation and integration. As far as I am aware, this is doable if $f\in C^0[-1,1]$ (or maybe $f\in C_c^0[-1,1]$), but I'm not entirely sure if I am allowed to simply abuse the fact that $C^0[-1,1]$ is dense in $L^2[-1,1]$. I am looking for a reference to this result if possible, since layer potential theory is well-studied.","['integral-equations', 'potential-theory', 'measure-theory', 'partial-differential-equations']"
2723767,"How to prove that the span of $\cos((n+1/2)x)$ and $\sin(nx)$ is dense in $L^2(-\pi,\pi)$?","Consider the following functions in $L^2(-\pi,\pi)$: $$ f_n(x)=\cos\big((n+1/2)x\big), g_k(x)=\sin(kx), n=0,1,\dots, k=1,2,\dots.$$ I am trying to prove $\text{span}(f_n \cup g_k)$ is dense in $L^2(-\pi,\pi)$. I already know the standard trigonometric polynomials are dense, that is $\cos(nx),\sin(nx) $ form a complete orthonormal system. Is there any easy way to reduce the problem to this well-known fact? One way to prove the density of the trigonometric polynomials is to use Stone-Weierstrass theorem, but here our set of ""modified"" trigonometric polynomials do not form an algebra, so we can't use the theorem directly. Any advice? (The reason I am interested in this specific set of generators is that they arise naturally as eigenfunctions of a differential operator).","['real-analysis', 'fourier-series', 'fourier-analysis', 'hilbert-spaces', 'functional-analysis']"
2723793,Geodesics of the Unit Sphere using Christoffel symbols,"I want to derive the geodesics of the unit sphere by using Christoffel symbols. The metric is the standard round metric: $ds^2 = d\theta^2 + \sin^2\theta d\phi^2$ ie. $g_{ij} = \begin{pmatrix} 1 & 0 \\ 0 & \sin^2\theta \\\end{pmatrix}$ And the Christoffel symbols: $\Gamma_{jk}^i := \frac12g^{il}(\partial_j g_{l k} + \partial_k g_{j l} - \partial_\beta g_{j k})$ I worked out to be: $\Gamma^\theta = \begin{pmatrix} 0 & 0 \\ 0 & -\sin\theta\cos\theta \\\end{pmatrix}$ , $\Gamma^\phi = \begin{pmatrix} 0 & \cot\theta \\ \cot\theta & 0 \\\end{pmatrix}$ And we are given that the geodesics parametrised by arclength $g_{ij}\frac{du^i}{d\tau}\frac{du^j}{d\tau} = \frac{d\theta}{d\tau}^2 + \sin^2\theta \frac{d\phi}{d\tau}^2 = 1$ satisfy the geodesic equation $\frac{d^2u^i}{d\tau^2} + \Gamma_{jk}^i\frac{du^j}{d\tau}\frac{du^k}{d\tau} = 0 = \frac{d^2\theta}{d\tau^2} - \sin\theta\cos\theta\frac{d\phi}{d\tau}^2 = \frac{d^2\phi}{d\tau^2} + 2\frac{\cos\theta}{\sin\theta}\frac{d\phi}{d\tau}\frac{d\theta}{d\tau}$ Now all of the simple Great Circles satisfy this equation fairly simply; for lines of longitude $(\tau,a)$ , and for the equator $(\frac\pi2,\tau)$ all terms vanish. I also verified that simple lines of latitude fail to satisfy the equations. So my question is this: do these differential equations characterise great circles? how can I verify them? $ \frac{d^2\theta}{d\tau^2} - \sin\theta\cos\theta\frac{d\phi}{d\tau}^2 = 0$ $ \frac{d^2\phi}{d\tau^2} + 2\frac{\cos\theta}{\sin\theta}\frac{d\phi}{d\tau}\frac{d\theta}{d\tau} = 0$","['differential-geometry', 'general-relativity']"
2723801,Cubic Spline Interpolation math,"I am writing a code snippet in Python to do an interpolation using cubic splines. I have first done the math, and then attempted to implement the pseudo code in Python. However, I think i might have messed up with the running index or a coefficient. Would someone please be kind enough to check my math ? The resulting curve is not smooth, does not fit the interior points and is all over the place. Let $(x_{0},y_{0}),(x_{1},y_{1}),\ldots,(x_{n},y_{n})$ be $n+1$ points in $\mathbb{R}^2$. A spline is a piece-wise polynomial function of the form - $$S(x)=\begin{cases}
        S_{0}(x),& \text{if } x_{0}\le{x}\lt x_{1}\\
        \vdots\\
        S_{i}(x),& \text{if } x_{i}\le{x}\lt x_{i+1}\\
        \vdots\\
        S_{n-1}(x),& \text{if } x_{n-1}\le{x}\lt x_{n}\\
\end{cases}$$ $S_{i}(x)$ is a cubic polynomial with $4$ four coefficients, $\forall{i}$. There are $n$ intervals and a total of $4n$ unknowns. So, we need $4n$ conditions. Suppose $S_{i}(x)$ has the form $S_{i}(x)=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}$. The first and second derivatives of the cubic polynomials are: $\begin{aligned}
S_{i}(x)&=A_{i}(x-x_{i})^3+B_{i}(x-x_{i})^2+C_{i}(x-x_{i})+D_{i}\\
S_{i}'(x)&=3A_{i}(x-x_{i})^2+2B_{i}(x-x_{i})+C_{i}\\
S_{i}''(x)&=6A_{i}(x-x_{i})+2B_{i}\\
\end{aligned}$ Also, $\begin{aligned}
S_{i}(x_{i})&=D_{i}\\
S_{i}'(x_{i})&=C_{i}\\
S_{i}''(x_{i})&=2B_{i}\\
\end{aligned}$ We define $h_{i}=x_{i}-x_{i-1}$. We have: $\begin{aligned}
S_{i-1}(x_{i})&=A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}\\
S_{i-1}'(x_{i})&=3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}\\
S_{i-1}''(x_{i})&=6A_{i-1}h_{i}+2B_{i-1}\\
\end{aligned}$ Four properties of cubic splines The spline should satisfy meet the below criteria - The function $S(x)$ will interpolate all data points. $S(x)$ must be continuous. And so in each interval, $S_{i}(x_{i})=y_{i}$ and $S_{i-1}(x_{i})=y_{i}$. The curve $S(x)$ should be smooth without jumps. $S'(x)$ must be continuous on the interval $[x_{i},x_{i+1}]$. Therefore, the slopes at each interior points must match. $S_{i}'(x_{i})=S_{i-1}'(x_{i})$. The curve $S(x)$ should be not have any abrupt changes in its bentness or convexity. $S''(x)$ will be continuous on the interval $[x_{i},x_{i+1}]$. $S_{i}''(x_{i})=S_{i-1}''(x_{i})$. A choice of one of the following two conditions at the end points $(x_{0},y_{0})$ and $(x_{n},y_{n})$ (a) The natural spline: $S'_{0}(x_{0})=0=S'_{n-1}(x_{n})$ (b) The clamped cubic spline : $S'_{0}(x_{0})=f'(x_{0})$ and $S'_{n-1}(x_{n})=f'(x_{n})$ where $f$ is presumably the function, we are trying to approximate. Let's determine the $4n$ conditions. Evaluation of the coefficients $A_i,B_{i},C_{i},D_{i}$ 1) The first condition yields $A_{i-1}h_{i}^3+B_{i-1}h_{i}^2+C_{i-1}h_{i}+D_{i-1}=D_{i}$ 2) The second condition yields $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$ 3) The third condition yields $6A_{i-1}h_{i}+2B_{i-1}=2B_{i}$ The above equations can be somwhat simplified, if we substitute $S_{i}''(x_{i})=2B_{i}=z_{i}$. Thus, we have $B_{i}=z_{i}/2$. 1) The last equation becomes : $6A_{i-1}h_{i}=2(z_{i}/2)-2(z_{i-1}/2)=z_{i}-z_{i-1}$. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ 2) The first equation becomes : $\begin{aligned}
C_{i-1}h_{i}&=y_{i}-y_{i-1}-A_{i-1}h_{i}^3-B_{i-1}h_{i}^2\\
C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-(A_{i-1}h_{i}^2+B_{i-1}h_{i})\\
C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-\left(\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^2+\frac{z_{i-1}}{2}h_{i}\right)\\
C_{i-1}&=\frac{y_{i}-y_{i-1}}{h_{i}}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)\\
\end{aligned}$ We define $b_{i}=\frac{y_{i}-y_{i-1}}{h_{i}}$. In all of the above the equations, the running index $i$ goes from $1$ to $n$. Thus, we now have our equations for determining the coefficients. $A_{i-1}=\frac{z_{i}-z_{i-1}}{6h_{i}}$ $B_{i-1}=\frac{z_{i-1}}{2}$ $C_{i-1}=b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)$ $D_{i}=y_{i}$ The system of equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ If we substitute these values in the second equation $3A_{i-1}h_{i}^2+2B_{i-1}h_{i}+C_{i-1}=C_{i}$, we should get a recurrence relation between $z_{i}$ - $\begin{aligned}
3\frac{z_{i}-z_{i-1}}{6h_{i}}h_{i}^{2}+2\frac{z_{i-1}}{2}h_{i}+b_{i}-h_{i}\left(\frac{z_{i}+2z_{i-1}}{6}\right)&=b_{i+1}-h_{i+1}\left(\frac{z_{i+1}+2z_{i}}{6}\right)\\
\left(\frac{2z_{i}+z_{i-1}}{6}\right)h_{i}+\left(\frac{z_{i+1}+2z_{i}}{6}\right)h_{i+1}&=b_{i+1}-b_{i}\\
h_{i+1}z_{i+1}+2z_{i}(h_{i}+h_{i+1})+z_{i-1}h_{i}&=6(b_{i+1}-b_{i})
\end{aligned}$ for $i=1,2,3,\ldots,n-1$ This system of linear equations in $z_{0},z_{1},z_{2},\ldots,z_{n-1}$ can be represented in the matrix form as - $\begin{bmatrix}
h_{1} & 2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\
0 & h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\
0 & 0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\
\vdots & & & & \ddots & \\
0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n}) & h_{n}
\end{bmatrix}
\begin{bmatrix}
z_{0}\\
z_{1}\\
z_{2}\\
\vdots\\
z_{n-1}
\end{bmatrix}
=\begin{bmatrix}
6(b_{2}-b_{1})\\
6(b_{3}-b_{2})\\
6(b_{4}-b_{3})\\
\vdots\\
6(b_{n}-b_{n-1})
\end{bmatrix}$ This matrix has $n-1$ rows and $n+1$ columns. So, we need two additional conditions. For natural splines, $z_{0}=0=z_{n}$. The first column and the last column in the above system of linear equations can be eliminated, resulting in, $\begin{bmatrix}
2(h_{1}+h_{2}) & h_{2} & 0 & \ldots & 0 & 0 \\
h_{2} & 2(h_{2}+h_{3}) & h_{3} & \ldots & 0 & 0 \\
0 & h_{3} & 2(h_{3}+h_{4}) & \ldots & \\
\vdots & & & \ddots & \\
0 & 0 & 0 &\ldots & h_{n-1} & 2(h_{n-1}+h_{n})
\end{bmatrix}
\begin{bmatrix}
z_{1}\\
z_{2}\\
\vdots\\
z_{n-1}
\end{bmatrix}
=\begin{bmatrix}
6(b_{2}-b_{1})\\
6(b_{3}-b_{2})\\
6(b_{4}-b_{3})\\
\vdots\\
6(b_{n}-b_{n-1})
\end{bmatrix}$","['calculus', 'interpolation']"
2723829,"what is a ""section"" exactly?","I have read 4 chapters of Hartshorne's Algebraic Geometry, when I go back to the beginning of scheme and the definition of a section, I am kind of confused why we call such a element ""section"". Let me quote the definition: If $\mathcal{F}$ is a presheaf, we say $\mathcal{F}(U)$ the sections of the presheaf $\mathcal{F}$ over the open set $U$. In the case of structure sheaf(on a variety or affine scheme), we say $\mathcal{O}(U)$ the set of (regular) functions. But how about other sheaves? What should I view these ""sections"" as?(functions? rational functions? maps?) When those sections can become rational functions? (In basic algebra, for a short exact sequence $0 \to A \to B \to C \to  0$, a ""section"" is a map $C \to B$ such that the composition $C \to B \to C$ is identity. In algebraic geometry, are these ""sections"" have similar property?) Any point of view is welcome! Thanks a lot!",['algebraic-geometry']
2723856,Differentiation using product rule,"I'm having trouble simplifying these questions, particularly when they involve square roots of $x$. Differentiate the following with respect to $x$ and simplify: $y=(x+2)x^\frac{3}{2}$ My attempt: Using product rule: $u=x^\frac{3}{2}, v=(x+2)$ therefore $\frac{du}{dx}=\frac{3}{2}x^\frac{1}{2}, \frac{dv}{dx}=1\\$ $\frac{dy}{dx}= \frac{3\sqrt{x}}{2}(x+2)+(\sqrt{x})^3$ Factorise:$\sqrt{x}[\frac{3}{2}(x+2)+(\sqrt{x})^2]\\\sqrt{x}[\frac{3}{2}x+3+x]$ The given answer is $\frac{\sqrt{x}}{2}(5x+6)$, which I can't achieve and I can't understand why the denominator of 2 is a common factor.",['derivatives']
2723903,How can I find all primitive pythagorean multiples given one even number including that number?,"I am trying to make an algorithm to find all pythagorean triples given a even number such as 4. Then the triple would be (3,4,5). Is there a way to do this?
I am using maple to do this but can also work with python. My code so far is 
PythagoreanTriplets := proc (x) local y, z, a, b, primitive; x; 2*ab; y = a^2-b^2; z = a^2-b^2; primitive := sqrt(x^2+y^2); for a in divisors(x) do for b in divisors(x) and not a do if gcd(a, b) = 1 then primitive := print(x, y, z) end if end do end do; return primitive end proc","['diophantine-equations', 'linear-diophantine-equations', 'number-theory', 'maple', 'discrete-mathematics']"
2723977,Algebraic closure of $\mathbb{Q}$ in $\mathbb{C}$. Alternative proof?,Let $\mathcal{A}$ be the algebraic closure of $\mathbb{Q}$ in $\mathbb{C}$. Prove that $[\mathcal{A}:\mathbb{Q}] = \infty$. I can show this using $[\mathbb{Q}(\sqrt[n]{2}):\mathbb{Q}] = n$ for all $n \in \mathbb{N}$ and $[\mathcal{A}:\mathbb{Q}] = [\mathcal{A}:\mathbb{Q}(\sqrt[n]{2})][\mathbb{Q}(\sqrt[n]{2}):\mathbb{Q}]$. Does anyone know an alternative proof? It's just curiosity.,"['alternative-proof', 'abstract-algebra', 'extension-field', 'field-theory']"
2723990,how to prove the set of constructible numbers is countable?,I know what is constructible numbers and I know to how to prove a set is countable by bijection but I don't know how to prove the set of constructible numbers is countable.,"['abstract-algebra', 'elementary-set-theory']"
2724043,finding a function satisfiying a certain equation,"I want to know that if there exists any function $f\in L^2[0,1]$ satisfying
$$f(x) = \int^x_0 f(y)\,dy$$
I do not know the value of $f(0)$ and if $f$ is differentiable.","['derivatives', 'real-analysis', 'lebesgue-integral']"
2724046,Show Lipschitz continuity of ODE solution with respect to initial condition,"I am working on the nonlinear pendulum differential equation given by $$
\theta'' = -g\sin \theta.
$$ Let $t \geq 0$ be fix, I define for $\theta_0 \in (-\frac\pi2, \frac\pi2)$ a mapping $S$ by $$
S(\theta_0) = \theta(t; \theta_0),
$$ where $\theta(t; \theta_0)$ is the solution of the initial value problem for the ODE defined above with initial conditions $\theta'(0) = 0$ and $ \theta(0) = \theta_0$. I believe the mapping $S$ is Lipschitz continuous, but is it true? If yes, how could I prove it?",['ordinary-differential-equations']
2724073,Expectation as an Operator vs. as a Functional,"I've been reading probability formalisms, and I see people referring to expectation often as an operator, and less often as a functional (in the context of a vector space of random variables). I can see how $\mathbb{E}: X \mapsto \mathbb{R}$ is a functional, in that it takes $X$, an element of a vector space over $\mathbb{R}$, and returns an element of that field. But I fail to see how it's an operator; in what sense is the expected value of a random variable itself a function? (Of course, there's the sense in which any constant function $X: \Omega \to \{r\}$ is a random variable, but that doesn't seem ""clean"" enough to motivate the separate treatment).","['functional-analysis', 'probability-theory', 'vector-spaces']"
2724101,How to prove that a subset is a partition?,"If $A$ and $B$ are two sets, prove that $\{ A \cap B, A\setminus B \}$ is a partition of $A$. I think this is a problem more of not knowing how to construct proofs than not comprehending the material (although, honestly, I'm finding myself to be quite weak in the Linear Algebra course I'm taking). As an aside to the question, I've acquired Velleman's How to Prove It, and I would not mind opinions as to whether that's a good start to learning how to answer questions such as this without looking at solutions. Moving on to the question. From the way the set is defined, if $A$ is disjoint from $B$, is their intersection not the empty set? Furthermore, I know that my proof needs to apply the definition of partition: The union of the elements of $P$ is equal to $X$. (The elements of $P$ are said to cover $X$.) The intersection of any two distinct elements of $P$ is empty. (We say the elements of $P$ are pairwise disjoint.) But I'm not sure how to do this: I feel like if $A$ is disjoint from $B$ and my feeling that the intersection is the empty set is true, then I am in fact dealing with $A$, and that the singleton $\{A\}$ is a partition. But that's about as far as I get. Any explanations that help me further my understanding of the material are extremely appreciated.","['real-analysis', 'set-partition', 'elementary-set-theory', 'linear-algebra', 'definition']"
2724125,Divergence-free vectorfield has volume-preserving flow,"Recently I read that divergence-free vectorfields give rise to volume-preserving flows, but I fail to prove this statement. Let $M$ be an oriented,finite dimensional, smooth manifold equipped with a volume form $\omega$. Furthermore let $X$ be a divergence-free vectorfield on $M$ with respect to $\omega$ with compact support. We know that the vectorfield $X$ gives rise to a global flow $\phi_t(x)$. Claim:  This flow preserves the volumeform, i.e. for any fixed time $t\in \mathbb{R}$ the smooth function $\phi_t:M\rightarrow M$ fulfils $\phi^{*}_t \omega=\omega$, where $f^{*}$ denotes the pullback of a form for a smooth function $f:M\rightarrow N$ between two smooth manifolds $M$ and $N$. Attempt: So far I tried to work in local coordiantes, hoping that a straightforward calculation yields the desired result. In local coordinates we can write $\omega_x=f(x) dx^1 \wedge \dots \wedge dx^n$ for a smooth function $f:U\subset M\rightarrow \mathbb{R}$. Then: $\phi^{*}_t \omega_x=f(\phi_t(x)) d\phi^1_t \wedge \dots \wedge d\phi^n_t$, where $\phi^i_t=x^i\circ \phi_t$ and $x^i$ are the coordinate functions. Further $d\phi^1_t=\partial_i \phi^1_t dx^i$ (using the summation convention) and so on: $\Rightarrow \phi^{*}_t \omega_x=f(\phi_t(x)) \partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^{i_1} \wedge \dots \wedge dx^{i_n}=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t dx^1\wedge \dots \wedge dx^n$ We want this to be equal to $\omega_x$ and so we need to show that $f(x)=f(\phi_t(x)) \epsilon^{i_1 i_2\dots i_n}\partial_{i_1}\phi^1_t \dots \partial_{i_n}\phi^n_t$. Since $X$ is divergence free, we have: $0=L_X \omega=d \iota_X \omega$, by Cartans magic formula and since $\omega$ is an $n$-form and therefore closed. Now to establish a connection between $X$ and $\phi_t$ we need to exploit the fact that $\phi_t$ is its flow: $\frac{d}{dt}\phi_t= X(\phi_t)$. In particular $\frac{d}{dt}\phi^i_t(x)=X(\phi_t(x))(x^i)$ for all $x\in M$. My problem now is that in order to establish a connection between the flow and the vectorfield, we need the time derivative of the flow, which does not occur in the calculations above so far. So how exactly can I exploit this connection? If it is of any help, we may assume that the manifold $M$ is compact and Riemannian and $\omega$ the Riemannian volume form. Thanks a lot in advance!","['vector-fields', 'differential-geometry', 'differential-topology']"
2724143,If $A$ is idempotent then $A$ is similar to a diagonal matrix with only $0$'s and $1$'s on the diagonal.,"I am trying to use Jordan normal form to show that if $A^2 = A$ then it is similar to a diagonal matrix with only $0$'s and $1$'s. I've proved that the eigenvalues of $A$ have to be either $0$ or $1$ so we know the diagonal elements of the JNF have to be $0$ or $1$. How do we know that none of the off-diagonals above are $1$? I don't full understand JNF - I've only learned about it in terms of elementary divisors and the minimal and characteristic polynomials, but a lot of resources online talk about it in terms of eigenspaces which is confusing.","['matrices', 'abstract-algebra', 'jordan-normal-form', 'linear-algebra']"
2724162,"How to compute the Hausdorff dimension of a ""semi"" self-similar shape?","The essence of self-similar fractals is that rescaling the original shape and gluing together a number of identical copies will produce the same overall shape. The quadratic type 2 curve is an example of such a fractal. You scale down the original shape by a factor of $1/4$ and piece together $8$ copies to get the same fractal: This results in a fractal with dimension $\log_{4}{8} = 1.5$: What I'm interested in, and what I mean by ""semi"" self-similar , is fractals where there are differently scaled copies of the shape building up the entire image. For example, if the vertical ""middle"" of the quadratic type 2 curve is treated as one iteration, rather than two, then the copy of the curve making up this piece is only scaled by $1/2$, while all of the other pieces are scaled by the original $1/4$. This ends up producing quite a different-looking fractal: Basic Structure: Limit Fractal: Another example would be a ""shark fin"" fractal, similar to the Koch snowflake, but where the ""middle third"" has a right triangle with height $1/3$, and hypotenuse $\sqrt{2}/3$: Basic Structure: Limit Fractal: Any ideas about how to calculate the Hausdorff dimensions of such shapes would be greatly appreciated!","['hausdorff-measure', 'measure-theory', 'fractals']"
2724184,"Explanation of ""without loss of generality"" in an application of Inverse Function Theorem.","Let $U$ be an open subset of $\mathbb R^{n+m}=\mathbb R^n\times \mathbb R^m$ and $g:U\to\mathbb R^m$ a $C^1$ function. Let $p=(x_0,y_0)\in U$ be a point such that
$$g'(p):\mathbb R^{n+m}\to \mathbb R^m\text{ is surjective}\tag{$*$}$$ The book I'm reading says (A) Without loss of generality, we can assume that the restriction $g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Question: Why there is no loss of generality in this assumption? (I'm interested in the explicit proof that the general case can be reduced to this one.) Context: The book proves the Inverse Function Theorem, in which the condition ""$g'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism"" is an assumption, and then proves the Lagrange Multiplier Method as an application. Because of (A), which appears in the proof of the Lagrange Multiplier Method, the Inverse Function Theorem implies the following: (B) There are a neighborhood $A\subset \mathbb R^n$ of $x_0$, a neighborhood $V\subset U$ of $(x_0,y_0)=p$ and a $C^1$ function $\xi:A\to\mathbb R^m$ such that
  $$(x,\xi(x))\in V\quad\text{and}\quad g(x,\xi(x))=g(p),\qquad \forall\ x\in A$$ My try: I know that, from $(*)$, there exists an $m$-dimenisonal subspace $X$ of $\mathbb R^{n+m}$ such that the restriction $g'(p)\big |_{X}$ is an isomorphism. So, my question is: how to rigorously pass from $X$ to $\{0\}\times \mathbb R^m$? Well, I know that there exists a bijective linear map $h:\{0\}\times \mathbb R^{m}\to X$. Let $H:\mathbb R^{n+m}\to\mathbb R^{n+m}$ be a bijective linear extension of $h$. Take $x_p=(x_p^1,x_p^2)\in \mathbb R^{n+m}$ such that $H(x_p)=p$. Define $\tilde{g}:H^{-1}(U)\to\mathbb R^{m}$ by $\tilde{g}(y)=g(H(y))$. Then, $\tilde{g}'(p)\big |_{\{0\}\times \mathbb R^m}$ is an isomorphism. Is it correct? If so: (C) There are a neighborhood $\tilde{A}\subset \mathbb R^n$ of $x_p^1$, a neighborhood $\tilde{V}\subset H^{-1}( U)$ of $(x_p^1,x_2^p)=x_p$ and a $C^1$ function $\tilde{\xi}:\tilde{A}\to\mathbb R^m$ such that
  $$(y,\tilde{\xi}(y))\in \tilde{V}\quad\text{and}\quad \tilde{g}(y,\tilde{\xi}(y))=\tilde{g}(x_p),\qquad \forall\ y\in \tilde{A}.$$ To finish my argumment, I have to obtain (B) from (C). Is it possible? I suspect that we should define $V=F(\tilde{V})$. But how to define $A$ and $\xi$ form $\tilde{A}$ and $\tilde{\xi}$?","['multivariable-calculus', 'proof-explanation', 'lagrange-multiplier', 'inverse-function-theorem', 'analysis']"
2724190,Calculate: $\lim_{x \to 0} \frac{f(3x)+f(5x)-2f(2x)}{x}$,"$f$ is a derivable function in $0$, and $f'(0)=a$ Calculate in terms of $a$: $\lim_{x \to 0} \frac{f(3x)+f(5x)-2f(2x)}{x}$","['derivatives', 'limits-without-lhopital']"
2724205,Density of $\{\sin(x^n)|n\in\mathbb{N}\}$ for $x>1$,"While reading other topics, e,g, Is $n \sin n$ dense on the real line? or Is $\{ \sin n^m \mid n \in \mathbb{N} \}$ dense in $[-1,1]$ for every natural number $m$? , the following problem appeared in my head: is $\{\sin(x^n)|n\in\mathbb{N}\}$ dense in $[-1,1]$ for all $x>1$? or a weaker problem: if $x>1$, then $\lim_{n\to\infty} \sin(x^n)$ does not exist? I proved the second one for $x=2$ and $x=3$ (with use of sine/cosine multiple angle formulas) and have some thoughts for $x\in\mathbb{N}$, but I have completely no idea how to deal e.g. with $x=e$.","['transcendental-numbers', 'limits', 'transcendence-theory', 'irrational-numbers', 'number-theory']"
2724219,Confusion over notion of compact manifold with or without boundary,"I am trying to understand how to do the following two questions concerning compact manifolds: Show that$\ M\ $is a compact manifold in $\mathbb{R}^{n},\ $then $\partial\ M\ $is also compact; if also $\ M\ $is $\ n\ $-dimensional, then $\partial\ M=\ $bdry$\ M\ $ Show that a compact manifold cannot be represented by a (single) parametric equation. My confusion is what it means for a manifold to be compact.  In some books having to do with advanced calculus or theory of manifolds, it states that the notion of compactness when describing manifolds is distinct from the topological notion of compactness.  I also come across questions where it asks to prove properties about compact manifolds without boundary.  This just makes it more confusing. It is like saying a closed interval, circle or sphere has no boundary points or an empty boundary, but yet it is closed and bounded.  On Wikipedia, compact manifold is discussed in the topic of closed manifolds.  Again, I am encountering topological notions associated with compactness.  But textbooks says otherwise. Can someone please help me with some clarifications over my confusions please.  Thank you in advance","['manifolds', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2724255,If $8R^2=a^2+b^2+c^2$ then prove that the triangle is right angled.,"If $8R^2=a^2+b^2+c^2$ then prove that the triangle is right angled. Where $a,b,c$ are the sides of triangle and $R$ is the circum radius 
My Attempt 
From sine law, 
$$\dfrac {a}{\sin A}=\dfrac {b}{\sin B}=\dfrac {c}{\sin C}=2R$$
So,
$$a=2R \sin A$$
$$b=2R \sin B$$
$$c=2R \sin C$$
Then,
$$8R^2=a^2+b^2+c^2$$
$$8R^2=4R^2 \sin^2 (A)+ 4R^2 \sin^2 (B) + 4R^2 \sin^2 C$$
$$8R^2=4R^2(\sin^2 (A)+\sin^2 (B) +\sin^2 (C)$$
$$2=\sin^2 (A)+\sin^2 (B)+\sin^2 (C)$$","['trigonometry', 'triangles']"
2724265,Is the spectral norm of the Jacobian of an $M$-Lipschitz function bounded by $M$?,"Well, the title pretty much says everything. I have a function $f: \mathbb{R}^n \mapsto \mathbb{R}^n$, which is $M-$Lipschitz with respect to the vector $L^2$ norm, i.e. $$||f(x)-f(y)||_2\leq M ||x-y||_2~\forall~x,y \in \mathbb{R}^n~.$$ 
Let $J_f: \mathbb{R}^n \mapsto \mathbb{R}^{n\times n}$ denote the Jacobian function of $f$, i.e. $$\left(J_f(x)\right)_{i,j} = \frac{\partial}{\partial x_j} f_i(x)~.$$ Assume that $J_f(x)$ is symmetric for every $x$. My question is, is the following inequality true (and why, if so)?
$$||J_f(x)||_2 \leq M~\forall~x \in \mathbb{R}^n~,$$ where the last norm $||\cdot||_2$ refers to the spectral norm, i.e. the largest absolute singular value.","['matrices', 'multivariable-calculus', 'spectral-norm', 'jacobian', 'lipschitz-functions']"
2724307,Why does the virtual fundamental class deserve its name?,"I'm reading about virtual fundamental classes, following Behrend and Fantechi's approach in their paper, The Intrinsic Normal Cone. I understand that these classes enjoy the following desirable properties They refine the top chern class (related to the chosen perfect obstruction theory); They agree with the usual fundamental class in the unobstructed case; They are used to define Gromov-Witten classes, in the sense that these definitions satisfy the GW-axioms. My question is: Other than the above properties, is there another reason that this is the ""correct"" substitute for the fundamental class? For example, does it satisfy some kind of Poincare Duality property? Any further insight would be much appreciated.",['algebraic-geometry']
2724438,Suppose that $A$ is finite and that $f:A \to B$ is surjective. Then $B$ is finite and $\vert{B}\vert \leq \vert{A}\vert$,"Please check my below proof. My proof is somewhere messy since I don't know how to organize and present ideas efficiently. I'm happy to receive any suggestion to have a shorter, more concise, and more elegant proof :) Theorem: Suppose that $A$ is finite and that $f:A \to B$ is surjective. Then $B$ is finite and $\vert{B}\vert \leq \vert{A}\vert$ , the equality holds $\iff f$ is bijective. Proof: $A$ is finite $\implies$ there exists a bijection $t:I_n \to A$ where $I_n$ is an initial segment of $\mathbb{N}$ . $\implies f\circ t:I_n \to B$ is a surjection. Let $g:B \to I_n$ s.t $g(b)=\min(f\circ t)^{-1} \{b\}$ . If $g(b)=g(c)$ , then $\min(f\circ t)^{-1} \{b\}=\min(f\circ t)^{-1} \{c\}$ . This implies there exists $m$ s.t $f\circ t (m)=b$ and $f\circ t (m)=c$ . Thus $b=c$ . So $g$ is injective. $\implies g:B \to g[B]$ is bijective. $g[B] \subseteq I_n \implies$ g[B] is finite. As a result, $B$ is finite. $g[B] \subseteq I_n \implies \vert{B}\vert \leq \vert{I_n}\vert=\vert{A}\vert \implies \vert{B}\vert \leq \vert{A}\vert$ . The equality holds $\iff g[B] = I_n \iff g:B \to I_n$ is bijective. Now we prove ( $g:B \to I_n$ is bijective) $\iff (f$ is bijective). It is easy to show that $(g:B \to I_n$ is bijective) $\Leftarrow  (f$ is bijective). So our task is to prove $(g:B \to I_n$ is bijective) $\implies  (f$ is bijective). Assume that $f(a_1)=f(a_2)=b$ . Then $\exists x_1,x_2 \in I_n$ s.t $f \circ t(x_1)=f \circ t(x_2)=b \implies x_1,x_2 \in \{m \in \mathbb{N} \mid f\circ t (m)=b\}$ . Assume $x_1 \neq x_2$ . Without loss for generality, we assume $x_1 < x_2$ . This implies $x_2 \neq \min(f\circ t)^{-1} \{b\} \implies \not \exists b \in B$ s.t $g(b)=x_2 \implies g$ is not surjective (CONTRADICTION). Thus $x_1=x_2$ or equivelently $a_1=a_2$ . To sum up, $f(a_1)=f(a_2)=b \implies a_1=a_2$ . As a result, $f$ is injective $\implies f$ is bijective.","['elementary-set-theory', 'proof-writing', 'functions', 'proof-verification']"
2724474,Diffeomorphism between a submanifold of $\mathbb{R}^4$ and $\mathbb{S}^2$,"Problem 5-1 from John Lee's Introduction to Smooth Manifolds ask us to show that the submanifold $\Phi^{-1}(0,1) \subset \mathbb{R}^4$ is diffeomorphic to $\mathbb{S}^2$, where $\Phi: \mathbb{R}^4 \to \mathbb{R}^2$ is given by
$\Phi(x,y,s,t) = (x^2 + y, x^2 + y^2 + s^2 + t^2 + y)$. For me, it seems natural to try to show the map $F: \Phi^{-1}(0,1) \to \mathbb{S}^2$ given by $F(x,y,s,t) = (y,s,t)$ is a diffeomorphism. However the condition $x^2 + y = 0$ implies that if $(x,y,s,t) \in \Phi^{-1}(0,1)$, then $y \leq 0$ and so the map $F$ cannot be surjective. Any idea is welcome!","['manifolds', 'smooth-manifolds', 'differential-geometry', 'differential-topology']"
2724475,Solve: $5\cos^2 (x) - 4\sin (x)\cos (x)+3\sin^2 (x)=2$,"Solve to find the general value of $x$:
$$5\cos^2 (x) - 4\sin (x)\cos (x)+3\sin^2 (x)=2$$ My Attempt: $$5(1-\sin^2 (x))-4\sin (x)\cos (x)+3\sin^2 (x)=2$$
$$5-5\sin^2 (x)-4\sin (x)\cos (x)+3\sin^2 (x)=2$$
$$2\sin^2 (x)+4\sin (x)\cos (x)=3$$",['trigonometry']
2724505,Tensor Product and Physics,"During lecture, my abstract algebra professor said that the exactness of the tensor product is ""absolutely essential"" to the existence of physical phenomena such as black holes and the big bang. Is it more or less directly related to the existence of such phenomena or is it a big stretch to make such a conclusion? If the former, what is the connection?","['category-theory', 'tensor-products', 'mathematical-physics', 'linear-algebra']"
2724594,Free throws in basketball game - Probability,"The random variable $X_i$ indicates whether in a basketball game the $i$-th free throw is hit ($X_i = 1$) or not ($X_i = 0$).
$10$ free throws are executed, so that $i = 1,\ldots 10$.
It holds that $P(X_i=0)=0,3$ and $P(X_i=1)=0,7$. 
X denotes the sum of these ten independent random variables: $X = \sum_{i=1}^{10}X_i$. (a) Determine $P (X = 8)$. What does this probability mean in terms of content? (b) What is the probability that the number of hits is at most five? (c) Which number of hits can we expect on average? $$$$ (a) Does it hold that $P(X=8)=0,3^2\cdot 0,7^8=0.0051883209$ ? That would mean that the probability that 8 of 10 free throws were successful is equal to $0,5\%$. Is this correct? (b) We want to calculate the probability $P(X\geq 5)$, which is equal to $P(X=1)+P(X=2)+P(X=3)+P(X=4)+P(X=5)=0,3^9\cdot 0,7^1+0,3^8\cdot 0,7^2+0,3^7\cdot 0,7^3+0,3^6\cdot 0,7^4+0,3^5\cdot 0,7^5=0.0007043841$, or not? (c) We are asked to calculate the expected value, aren't we? Is this equal to $$E(X)=\sum_{i=1}^{10}x_i\cdot P(X=x_i)\\ =1\cdot P(X=1)+2\cdot P(X=2)+3\cdot P(X=3)+\ldots 9\cdot P(X=9)+10\cdot P(X=10)$$ or not?","['probability-theory', 'binomial-distribution', 'probability']"
2724669,Convergence in distribution of r.v.,"I have the sequence of independent r.v. $(X_n)$ with distributions $$ \Pr\left(X_n = \frac1{\sqrt n}\right)=\Pr\left(X_n=-\frac1{\sqrt n}\right) = \frac12$$ 
I should find the distribution to which the following two processes converge to in distribution: $$Y_n = \sum_{i=1}^n X_i,\qquad Z_n = \sum_{i=n+1}^{2n} X_n.$$
From some experiments I got to the conclusion that this distribution must be a Gaussian. My idea was to use the Lindeberg principle and then the Portemanteau theorem. I have that $$E[X_n] = 0 \, \forall n, E[X_n^2]=\frac1n \quad\text{ and }\quad  E[|X_n|^3]=O(n^{-3/2})$$ Now I choose the sequnce of iid Gaussian r.v. $W_n \sim \mathcal{N}(0,1/n)$.
Using the principle I have that $$|E[g(X_1+\ldots+X_n)]-E[g(W_1+\ldots+W_n)] \leq \frac{C}{6}\sum_{i=1}^{n}(E(|X_i|^3)+E(|W_i|^3)) \text{ and } \\|E[g(X_{n+1}+\ldots+X_{2n})]-E[g(W_{n+1}+\ldots+W_{2n})] \leq \frac{C}{6}\sum_{i=n+1}^{2n}(E(|X_i|^3)+E(|W_i|^3))$$
Approximating $$\sum_{i=1}^{n}(E(|X_i|^3)+E(|W_i|^3)= \int_1^n(E(|X_i|^3)+E(|W_i|^3)$$ since both the third moments are $O(n^{-3/2})$ we get something $O(n^{-1/2}) \to 0$ as $n\to +\infty$. Update: So far I can say that the sequence of r.v. $$Z_n = X_{n+1}+\dots+X_{2n} \to W_{n+1}+\dots+W_{2n}, W_i \sim \mathcal{N}(0,1/i)$$ in distribution, thanks to the Linderberg Principle. Also, being Normal r.v. the $W_i$ their sum is itself a Normal r.v. $W$ with mean $0$ and $$Var(W)=\sum_{i=n+1}^{2n}(Var(W_i))\approx ln(2)$$
Can I conclude that $Z_n\to^d W\sim \mathcal{N}(0,ln(2))$? Can you help me understand why the sequence $(Y_n,n\geq 1)$ does not converge? Is it because the variance of the corresponding normal would be $\approx log(n)$ and not a constant?","['weak-convergence', 'probability-theory', 'normal-distribution', 'probability-distributions']"
2724692,Is the space of random continuous functions separable?,"Let $\Omega$ and $X$ be compact metric spaces with $P$ a probability measure on $\Omega$. Then we get the following definition of random continuous functions from ""Random Probability Measures on Polish Spaces, H. Crauel page 21"": let $f : X \times \Omega \to \mathbb{R}$ be a function such that: for all $x \in X$ the map $\omega \mapsto f(x, \omega)$ is measurable, for all $\omega \in \Omega$ the map $x \mapsto f(x, \omega)$ is continuous and bounded, $\omega \mapsto \sup\{|f (x, ω)| : x \in X\}$ is integrable with respect to P (it is
measurable by separability of X). If $f$ and $g$ are both functions satisfying (1)-(3) then identify f and g if $P(\{\omega : f(\cdot, \omega) \not= g(\cdot, \omega) \}$. Finally, a random continuous function is (the equivalence class of)
a function $f : X \times \Omega \to \mathbb{R}$ satisfying (1)–(3) above. The set of all random continuous functions is a linear space, denoted by $C_\Omega(X)$ and a norm can be defined by
$$
|f|_\infty = \int \sup_{x \in X}|f(x, \omega)|dP(\omega).
$$ My question : is $C_\Omega(X)$ separable?","['metric-spaces', 'measure-theory']"
2724724,Is the Scalar Product Definition in my book wrong?,"In Rainer Kress'es book ""linear integral equations"" (2nd edition) on page 9 it says Definition 1.19 Let X be a complex (or real) linear space. Then a function $(\cdot , \cdot): \rightarrow X \times X \rightarrow \mathbb{C}\; (or \mathbb{R})$ with the properties. (H1) $(\varphi, \varphi) \geq 0 $ (positivity) [...]. for all $\varphi \in X$ is called a Scalar product. Now, if the mapping goes from $X\times X$ to $\mathbb{C}$, we might have to compare imaginary numbers with the > relation, which is not possible to my knowledge.
Is this a mistake in the book, or did I miss something? Edit: pardon my formatting, I'm typing this on my phone Clarification Why do we map $X \times X \rightarrow \mathbb{C}$ in the first place if we implicitly assume it is real anyway. I find this confusing. Result My confusion came from, that I thought of a mapping to be a  Scalar product which turned out it was not.","['linear-algebra', 'inner-products']"
2724751,Composition of Riemann integrable and continuous function is integrable,"Prove that if $f : [a,b] \to [c,d]$ is Riemann integrable , and $g: [c,d] \to \mathbb{R}$ is continuous then $g\circ f$ is integrable. By Lebesgue we know because $f$ is integrable then $f$ must be discontinuous on at most a set of measure zero, so I need to show that $g\circ f$ is continuous except for at most a set of discontinuous points of measure zero. I need some hints on how to do that, please help.","['calculus', 'riemann-integration']"
2724753,"Discrete subgroups of $(\mathbb{R},+)$ [duplicate]","This question already has answers here : Subgroup of $\mathbb{R}$ either dense or has a least positive element? (4 answers) Closed 1 year ago . I  have a simple question on discrete subgroups. We say that a subset $C$ of $\mathbb{R}$ is discrete if every point of $C$ is isolated in the topology inherited from $\mathbb{R}$. Is it true that every discrete subgroup of $(\mathbb{R},+)$ is of the form $r\mathbb{Z}$ with $r \in \mathbb{R}$? And if so, how can i prove it? \medskip My attempt: I don't really know how to prove it, i mean, i cannot think of an example of a discrete subset of $\mathbb{R}$ which is not, somehow, related to $\mathbb{Z}$... Thanks in advance!","['general-topology', 'real-analysis', 'real-numbers', 'group-theory']"
2724791,$I \subseteq I_n$ and $I \neq \varnothing \implies I$ has the greatest element [Proof Verification],"Please check my below proof. Thank you for your help! Definition: $I_n=\{m \in \mathbb{N} \mid m \leq n\}$ Theorem: $I \subseteq I_n$ and $I \neq \varnothing \implies I$ has the greatest element. Proof: Let $M$ be $\{m \in \mathbb{N} \mid \forall i \in I, i \leq m\}$ . Then $n \in M \implies M \neq \varnothing$ . By well-ordering theorem, $M$ has the least element $m_0$ . Now we prove $m_0 \in I$ . There are two possible cases. $m_0=0$ $\implies I=\{0\} \implies I$ has only one element $0$ and $0$ is also the greatest element of $I$ . $m_0>0$ $\implies m_0=k+1$ . If $m_0 \not \in I$ , then $\forall i \in I, i < m_0$ . Thus $\forall i \in I, i \leq k \implies k \in M \implies k+1$ is not the least element of $M \implies m_0$ is not the least element of $M$ . (CONTRADICTION). As a result, $m_0 \in I$ . So we have that $I$ has the greatest element.","['proof-writing', 'elementary-set-theory', 'proof-verification']"
2724820,"How can I prove that the cyclotomic integers $\frac{\zeta_p^r - 1}{\zeta_p^s - 1}$, with $\ p\nmid rs$, are units?","I am reading a paper in which the cyclotomic integers 
$$\frac{\zeta_p^r - 1}{\zeta_p^s - 1},\ p\nmid rs$$
are claimed to be units, but I'm not sure how to show that this is the case. Taking the norm, we have $$N\left( \frac{\zeta_p^r - 1}{\zeta_p^s - 1} \right) = \prod_{\sigma \in \operatorname{Gal(\Bbb Q(\zeta_p)/\Bbb Q)}} \sigma\left( \frac{\zeta_p^r - 1}{\zeta_p^s - 1}\right) = \prod_{\sigma \in \operatorname{Gal(\Bbb Q(\zeta_p)/\Bbb Q)}}  \frac{\sigma^r(\zeta_p) - 1}{\sigma^s(\zeta_p) - 1}$$ This should simplify to be $1$ if the element is a unit, but I can't see how this simplifies in this way. Is there a simpler way to see that these are units explicitly? (The paper states that there is an obvious inverse, but multiplying the two together doesn't make it obvious that this is the inverse.)","['number-theory', 'abstract-algebra', 'algebraic-number-theory', 'cyclotomic-fields']"
2724875,An exercise in Banach Space Theory,"I am currently reading a book and  while i was reading I came across an exercise : Prove that a Banach Space $X$ has finite dimension if and only if every linear subspace of $X$ is closed. My solution goes like this : The direction ($\implies$) is immediate since if we have a subspace $F \subseteq X$ then $F$ must be also of finite dimension hence closed. For the opposite direction I assumed that $X$ has an infinite dimension , then we know that there exist a non trivial linear functional $F:X \to \mathbb{R}$ which is not bounded . Then I proved that $\ker F$ must be dense in $X$ and from hypothesis we have that $\ker F$ is also closed, hence $\ker F=X$ which means that $F$ is trivial and we have a contradiction. Is this proof correct ? I am asking because I didn't use the hypothesis that $X$ is Banach throughout the proof. Thanks in advance !","['functional-analysis', 'banach-spaces', 'proof-verification']"
2724890,Exercise 2.20 from “Mathematical Statistics - Jun Shao”,"Let ${ \left\{ X_{ i } \right\}  }_{ i=1 }^{ n } \sim E(a,\theta)$
  where $a \in {\rm I\!R}$, and $\theta > 0$. Show that the smallest
  order statistic, $X_{(1)}$, has the exponential distribution
  $E(a,\theta/n)$ and that $2\sum _{ i=1 }^{ n }{ (X_{ i }-X_{ (1) }) }
 /\theta \sim { \chi  }_{ 2n-2 }^{ 2 }$. I have shown that the smallest order statistic $X_{(1)} \sim E(a,\theta/n)$. But I'm struggling in the second part. I think I can use the Basu Theorem, but I believe that there are other simpler option. Any hint?","['statistics', 'order-statistics', 'statistical-inference']"
2724904,Applied - how to represent confidence (failure probability) in geometric progression of computer upgrades,"I think there is a correct way of representing confidence in the below table, but I was never strong at statics. This is based on a practical problem. In reality, the ""upgrade"" has gone through a number of QA tests, and in fact should not cause any breakages. In practice an upgrade sometimes causes failures. What I want to do is minimise the impact of such a failure. I have 1000 computers. I want to upgrade these computers with an identical upgrade. There is an unknown chance that this upgrade will break a computer. I want to do it efficiently (in as few rounds as possible). I want to define my confidence in each round of the likelihood that a computer will break. A long time ago I was told the ""best"" way to upgrade a large number of computers was a ""doubling"" method (geometric progression). My specific question is: A) How do I measure my confidence in the upgrade (IE> I want to know (probabilistically) how many computers will break). In the below table I think the confidence level is far too low.
OR
How to I report probability of a failure occurring (which is probably more important, but perhaps is just the inverse of the confidence. B) Is this actually (provably) efficient? 2(n)  Num of upgrades  upgrades   Confidence level
      per round        completed
1     2                2          0.2%
2     4                6          0.6%
4     8                14         1.4%
8     16               30         3.0%
16    32               62         6.2%
32    64               126        12.6%
64    128              254        25.4%
128   256              510        51.0%
256   512              1000       100.0% Further notes. The idea is that if a breakage happens, you fix the upgrade, and restart from the beginning. If I had a formula then I could re-calculate based on the reduced total population (as some would already be upgraded Not to over complicate, but what if I have, 1% VVIP and 10% VIP, I assume I just leave them until the end of the process to have lowest probability of failure. Thank you!","['statistics', 'probability', 'confidence-interval']"
2724956,Convergence of stopping time to that of standard Wiener process,"I ran into the following problem in one of my applied work and would appreciate if someone could kindly shed some lights. Settings : Let $\{X_n\}$ be a sequence of cadlag processes in the Skorohod space $D[0,1]$ (I could take the stronger assumption that $X_n$ lies in $C[0,1]$, the space of all continuous functions, if needed). We assume that $X_n$ converges in distribution to the Wiener process,
$$
X_n \stackrel{d}{\longrightarrow} W
$$
Now fix constants $0 \leq l \leq u \leq 1$ and $c > 0$. Define a sequence of stopping times $\tau_n$ by
$$
\tau_n = \inf\{ l \leq t \leq u : X_n(t) > c \} \wedge u
$$
where $a \wedge b = \min(a, b)$. In other words, $\tau_n$ is the first moment, if exists, when the process $X_n$ moves above a threshold $c$, and equal to $u$ otherwise. Question : Does $X_n(\tau_n)$ converge in distribution? If it converges in distribution, is it possible to derive the
    closed-form of the cumulative distribution function of the limit? My guess : I conjecture that $X_n(\tau_n) \stackrel{d}{\longrightarrow} W(\tau)$, where
$$
\tau = \inf \{ l \leq t \leq u: W(t) > c \} \wedge u
$$
To this end, I have attempted to show $\tau_n$ converges in probability to $\tau$. Were this established, $(X_n, \tau_n)$ would converge in distribution to $(W, \tau)$, and continuous mapping theorem would entail the desired convergence. I couldn't figure it out, however. Any suggestion will be greatly appreciated!","['stochastic-processes', 'probability-theory', 'probability']"
2724960,Probability of getting full house in poker,"I have this problem which I have solved, but using two different methods. I am quite new to combinatorics and want to know how to intuitively understand the difference between the following two methods. The problem consist of calculating the probability of getting a full house being dealt a 5-card poker hand. First of, I solve this by simply saying that $P($get full house$)=\frac{2 {13\choose 2} {4 \choose 3} {4 \choose 2}}{{52 \choose 5}}$. This is the right answer according to my text-book. However, at my first attempt at solving this I forgot the factor 2 in the numerator. My reasoning goes like this: All the possible ways of getting a full house consists of all the ways we can combine two different ranks (i.e. ${13 \choose 2}$) times all the possible ways of choosing 3 cards out of 4 suits, times all the possible ways of choosing 2 cards out of 4 suits. Now, all this seems logical to me. The thing that makes me doubt whether I truly understand what I'm doing is how the factor 2 comes in place. I'm thinking: Because we choose two different ranks without regards to order, we have to compensate for those combinations and therefore multiply with $2!$, because obviously it does matter if I (for example) choose the ranks (ace,knights) and in this sequence choose three ace and two knights. On the other hand, I can solve the problem using the method described here: https://math.stackexchange.com/a/808328/518320 Which as well seems intuitively clear, thinking the way the user describe the process in that thread. What is the difference in the two methods? Maybe this is obvious, but I'm new to combinatorics. And is my reasoning above accurate? Thanks!","['probability-theory', 'statistics', 'probability', 'combinatorics', 'discrete-mathematics']"
2724961,Distribution in sets and subsets,"Suppose we have a set $S$ with two elements, 
$$S=\{A,B\}$$
Now the subsets are $2^2$, I am going to make a new set and call it $S_1$, 
$$S_1=\{\{\},\{A\},\{B\},\{A,B\}\}$$
There are $2^4$ subsets for $S_1$ yet I am going to eliminate the empty set and re-define my $S_1$ as:
$$S_{1_{new}}=\{\{A\},\{B\},\{A,B\}\}$$
for $S_{1_{new}}$ there are 8 subsets. I shall repeat the procedure $n$ times in similar manner. Now I am wondering how many A and B I will have in the set $n$-th. Another question is that suppose my original set $S$ has N elements how will this be generalised?","['combinatorics', 'discrete-mathematics']"
2724989,Evaluation of a path integral,"I am stuck weeks in the following calculation from a paper: Path integral $I$ is written as $$ I = \int_{\mathbf{r}_0 = \mathbf{0}}^{\mathbf{r}_L=\mathbf{R}}\exp\left[-A\int_0^L\dot{\mathbf{r}}_t^2\,dt+\frac{i\eta}{2L^2}\int_0^L\int_0^L (\mathbf{r}_t-\mathbf{r}_s)^2\,ds\,dt\right]{\cal D}\mathbf{r}.$$ Since the contribution of the path $\mathbf{r}_t$ to $I$ becomes most dominant when $\mathbf{r}_t$ makes the exponent stationary, we have $\mathbf{r}_t$ as the solution of the following Euler-Lagrange equation subject to the boundary conditions $\mathbf{r}_0 = \mathbf{0}$ and $\mathbf{r}_L = \mathbf{R}$: $$\ddot{\mathbf{r}}_t + \alpha^2\mathbf{r}_t - \beta\int_0^L\mathbf{r}_s\,ds=0,$$
where $\alpha^2 = \dfrac{i\eta}{AL}$ and $\beta = \dfrac{i\eta}{AL^2}$. With this solution, $I$ is expressed as 
$$I = \exp(-A\mathbf{r}_L\dot{\mathbf{r}}_L).$$ I don't have any problem with the Euler-Lagrange equation but cannot obtain the last expression. Anyone help me out of this?","['functional-analysis', 'calculus-of-variations']"
