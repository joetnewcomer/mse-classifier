question_id,title,body,tags
3747968,"Family of Generalized Integrals ${I}(a,b,p)=\int_0^{ab} \left( \left\{\frac{x}{a}\right\}-p\right) \left( \left\{\frac{x}{b}\right\}-p\right) \; dx$","Background: I came across the following family of generalized Franel integrals, and found them quite interesting.  I don't think I've seen anything about these integrals before, at least not generalized, and I want to know if this is a known family of generalized integrals.  I am also interested behind the behavior of this family of integrals, specifically when $p=\frac{1}{2}$ .  I want to find out if there's a simple algebraic closed form expression for this family of integrals.  Wolfram does not do a good job factoring the integrands, but to be fair it didn't factor $I(a,b,p)$ as I have done in this post.  Also, how would you approach cases where $\gcd{(a,b,c,\dots)} \neq 1$ ? Consider the family of generalized integrals as the following: $${I}(a,b,p)=\int_0^{ab} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \; dx$$ $${I}(a,b,c,p)=\int_0^{abc} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right)\; dx$$ $${I}(a,b,c,d,p)=\int_0^{abcd} \left( \bigg\{\frac{x}{a}\bigg\}-p\right) \left( \bigg\{\frac{x}{b}\bigg\}-p\right) \left( \bigg\{\frac{x}{c}\bigg\}-p\right) \left( \bigg\{\frac{x}{d}\bigg\}-p\right)\; dx$$ $$\ldots$$ Where $a,b,c,\ldots \in \mathbb{N}$ , $p \in \mathbb{Q}^+$ , and $\gcd{(a,b,c,\ldots)}=1$ . Calculations: Express the integral as the following: $${I}(a,b,p)=\sum_{i=0}^{a-1} \sum_{k=0}^{b-1} \int_0^1 \left(\frac{t+i}{a}-p\right)\left(\frac{t+k}{b}-p\right) \; dt$$ Changing the order of the summations and integral and using some algebra: $${I}(a,b,p)=\int_0^1 \left(\frac{a-1}{2}+t-ap\right)\left(\frac{b-1}{2}+t-bp\right) \; dt$$ Expanding the integrand out and factoring yields: $${I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+\frac{at}{2}\left(1-2p\right)+\frac{bt}{2}\left(1-2p\right)+\frac{(a+b)}{4}\left(2p-1\right)+{\left(t-\frac{1}{2}\right)}^2 \; dt$$ $${I}(a,b,p)=\int_0^1 \frac{ab}{4}{\left(2p-1\right)}^2+{\left(t-\frac{1}{2}\right)}^2 \; dt$$ And so: $$\boxed{{I}(a,b,p)= \frac{ab}{4}{\left(1-2p\right)}^2+\frac{1}{12}}$$ Calculated similarly,I got the following: $$I(a,b,c,p)=\frac{abc{\left(1-2p\right)}^3}{8}+\frac{c}{24}\left(1-2p\right)$$ $$I(a,b,c,d,p)= \frac{abcd}{16}{\left(1-2p\right)}^4+\frac{{(1-2p)}^2}{48}\left(ab+cd\right)+\frac{1}{80} $$ However, as @Varun Vejalla and @OliverDiaz pointed out in the comments, these results are illogical, and there actually is no closed form for $I(a,b,c,d,p)$ . Further Observations: Interestingly enough, $p=\frac{1}{2}$ is a special case for this entire family generalized integrals.  Why is this?
Assuming the aforementioned conditions are met: $$I\left(a,b,\frac{1}{2}\right)=\frac{1}{12}$$ $$I\left(a,b,c,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^3 \; dt=0$$ $$I\left(a,b,c,d,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^4 \; dt=\frac{1}{80} $$ $$I\left(a,b,c,d,e,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^5 \; dt=0$$ And so it seems that the following statement is true: $$I\left(a_1,a_2,\ldots,a_n,\frac{1}{2}\right)=\int_0^1 {\left(t-\frac{1}{2}\right)}^n \; dt=\cases{        0       & $n \; \text{is odd}$ \cr
                        \frac{1}{2^n\left(n+1\right)}       & $n \; \text{is even}$ }$$ However, Wolfram Alpha computed $I\left(a,b,c,d,\frac{1}{2}\right)=0$ for valid $a,b,c,d$ values. Final Remarks: I wonder what other interesting observations can be made about this family of generalized integrals.  Specifically, are there other interesting special cases, and if so why are they so special?","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'solution-verification']"
3747985,How to solve Cauchy-Euler ODE $x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x$?,"The equation is $x^2 y''(x) - 2x y'(x) + 2 y(x) = x \sin x$ . Solving for the homogeneous solution is simple: the characteristic equation results in roots 1 and 2 so we have $y_c = Ax + Bx^2$ , which works. Since $y_p = u(x) y_1 + v(x) y_2$ by variation of parameters, the Wronskian is $x^2$ , and $u(x) = - \int \frac{x^2 (x \sin x)}{x^2}$ and $v(x) = \int \frac{x (x \sin x)}{x^2}$ we should get $y_p = -x \int x \sin x + x^2 \int \sin x$ . However, this solution doesn't work. Plugging the equation into Wolfram, the particular integral should be $y_p = -x \int \frac{\sin x}{x} + x^2 \int \frac{\cos x}{x} - x\sin x$ . I've tried to reverse engineer what the values of $u(x)$ and $v(x)$ but they don't make sense. What is missing here?",['ordinary-differential-equations']
3747996,"Proving that $((\mathbb{Z}/p\mathbb{Z})^{\times},\times)$ is a group under the condition that $p$ is a prime number.","I am currently studying abstract algebra through Evan Chen's Napkin. He says under example 1.1.9 that: Let $p$ be a prime. Consider the non-zero residues modulo $p$ , which we denote by $(\mathbb{Z}/p\mathbb{Z})^{\times}$ . Then $((\mathbb{Z}/p\mathbb{Z})^{\times},\times)$ is a group. It then asks under question 1.1.10: Why do we need the fact that $p$ is prime? which, to my understanding, is an exercise to the reader. Since the binary operation is multiplication, I am pretty sure we are talking about modular multiplication, which looks like: The above is modular multiplication modulo 10. What I am getting confused about however is the group $(\mathbb{Z}/p\mathbb{Z})^{\times}$ . In the book, it is defined as the non-zero residues modulo $p$ , i.e.: $$(\mathbb{Z}/p\mathbb{Z})^{\times}=(1,2,3,...,n-1)$$ However, on Wikipedia, the multiplicative group of integers modulo $n$ is the set of numbers from $(1,2,3,...,n-1)$ coprime to $n$ . Assuming it is the book's definition, my proof would be: If $n$ is a positive integer not prime, then there exist 2 numbers $a$ and $b$ such that $a\times{b}=n$ . However, since neither $n$ or 0 is found in the set $(1,2,3,...,n-1)$ , then closure of the binary operation, multiplication, is not achieved. Furthermore, since the identity is 1, then there doesn't exist an inverse for all numbers since there is at least one factor of n in $(1,2,3,...,n-1)$ , which we will denote $d$ , as $d\times{k}\mod(n)\neq{1}$ for any positive integer $k$ . $(\mathbb{Z}/n\mathbb{Z})^{\times},\times)$ is therefore not a group. Instead, choose a positive integer $p$ . Then closure is achieved since two numbers $a$ and $b$ can never multiply to $p$ as that would violate the statement that $p$ is prime. I am unable to continue the proof from here since I am unsure how to prove an inverse in $(1,2,3,...,p-1)$ exists for all integers in $(1,2,3,...,p-1)$ . It is obvious that the identity is 1 and there exists associativity since the operation is multiplication though for both cases. To summarise, when answering could you please: Clarify the concrete meaning of $(\mathbb{Z}/p\mathbb{Z})^{\times}$ by writing out the set. Check that the binary operation within this group is, in fact, modular multiplication. Check whether the first bit of my proof is valid and give hints to finishing the proof or provide an alternative proof (though my knowledge in abstract algebra is a bit limited considering I just started learning it). I apologise if I have a silly misconception or one that is quite trivial, though I have started looking at cyclic groups and the book has not yet introduced me to quotient groups. I thank anyone in advance for their help! Update following suggestion Yourong 'DZR' Zang To prove the statement suggested, we define the sets: $$[n]=\{n+pk_1:k_1\in\mathbb(Z)\}$$ $$[m]=\{m+pk_2:k_2\in\mathbb(Z)\}$$ $$[a]=\{nm+pk_3:k_3\in\mathbb(Z)\}$$ Obviously $[a]\equiv[n][m]\mod{p}$ since $(nm+pk_3)\mod{p}=nm$ and $((n+pk_1)(m+pk_2))\mod{p}=(nm+nk_2p+mk_1p+k_1k_2p^2)\mod{p})=nm$ . We now prove the sets are equal. Since $(n+pk_1)(m+pk_2)=(nm+nk_2p+mk_1p+k_1k_2p^2)=nm+p(nk_2+mk_1+k_1k_2p)$ , letting $k_3=nk_2+mk_1+k_1k_2p$ lets the result directly follow. Now I have proven this statement, I am having trouble identifying how this proves that there exists an inverse for all numbers in $(\mathbb{Z}/p\mathbb{Z})^{\times}$ since that would require the existence of a number $h\in(\mathbb{Z}/p\mathbb{Z})^{\times}$ such that $nh\equiv{1}\mod{p},\forall{n}\in(\mathbb{Z}/p\mathbb{Z})^{\times}$ .","['elementary-number-theory', 'group-theory', 'abstract-algebra', 'cyclic-groups']"
3748013,Proving a variant of the Kolmogorov SLLN,"Let $X_1, X_2, ...$ be a sequence of independent r.v.'s (not necessarily identical). Now for all $i$ , we have $E(X_i) = 0$ and $E|X_i|^{1 + \delta} \leq C$ for some $\delta > 0$ and $C < \infty$ . Prove $$
n^{-1}\sum_{i = 1}^nX_i \to 0 \ a.s.
$$ My angle of attack: If $\delta \geq 1$ :
there is nothing to prove, it is just the simpliest case of Kolmogorov SLLN. So the challenging part is when $\delta \in (0,1)$ . So Kolmogorov SLLN says that if $$
\sum_{i=1}^\infty\frac{Var(X_i)}{i^2} < \infty,
$$ Then we will have convergence. So my thinking is that if I can show $E|X_n|^2 \sim \mathcal{O}(n^p)$ with $p \in (0,1)$ , then I am done. This should obviously be connected with the condition that $E|X_i|^{1+\delta} \leq C$ .","['probability-limit-theorems', 'law-of-large-numbers', 'almost-everywhere', 'convergence-divergence', 'probability-theory']"
3748055,Evaluating $\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}}$,"Evaluate $$\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin({\frac{1}{x}})}{x-\sqrt{1+x^2}}$$ My attempt: $$\lim_{x\to+\infty} \frac{\sqrt{x}\cos{x}+2x^2\sin\left({\frac{1}{x}}\right)}{x-\sqrt{1+x^2}}=\lim_{x\to+\infty} \frac{x^2\sqrt{x}\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{x\left(1-\sqrt{1+\frac{1}{x^2}}\right)}$$ $$=\lim_{x\to+\infty} x\sqrt{x}\cdot \frac{\left(\frac{\cos{x}}{x^2}+2\frac{\sin{\frac{1}{x}}}{\sqrt{x}}\right)}{1-\sqrt{1+\frac{1}{x^2}}}$$ Both numerator and denominator tend to zero, while $x\sqrt{x} \to +\infty$ . Any help is appreciated.","['limits', 'calculus', 'limits-without-lhopital']"
3748082,Completing the proof using strong induction for $E = \bigcap_{n=1}^\infty E_n $,"I want to follow up my previous question . My original question was: Fix that $E$ is the set of real numbers $x \in [0,1]$ whose decimal expansion contains only the digits $4$ and $7$ . Let $S_n$ be the set consisting of all natural numbers not exceeding $10^n$ whose digits consists only of $4$ or $7$ . For example, \begin{equation*}
    \begin{split}
        S_1 &= \{4, 7\} \\
        S_2 &= \{44, 77, 47, 74\} \\
        S_3 &= \{444, 744, 474, 447, 774, 747, 477, 777\} \\
        \vdots
    \end{split}
\end{equation*} I want to prove that $E$ can be defined as: \begin{equation*}
    E = \bigcap_{n=1}^\infty E_n, \textrm{ where } E_n = \cup_{a \in S_n} \left[\frac{a}{10^n}, \frac{a+1}{10^{n}}\right]  
\end{equation*} For instance, \begin{equation*}
    \begin{split}
        E_1 &= [0.4, 0.5] \cup [0.7, 0.8] \\
        E_2 &= [0.44, 0.45] \cup [0.77, 0.78] \cup [0.47, 0.48] \cup [0.74, 0.75] \\
E_3 &= [0.444, 0.445] \cup [0.447, 0.448] \cup [0.474, 0.475] \cup [0.477, 0.478]\\
        &\cup [0.744, 0.745] \cup [0.747, 0.748] \cup [0.774, 0.775] \cup [0.777, 0.778]  \\
        &\vdots
    \end{split}
    \end{equation*} and I had no idea how I could prove $\bigcap_{n=1}^\infty E_n \subseteq E$ . My original question also got a wonderful response, but I eventually came up with an alternative proof. Here is that proof: Let $y \in \bigcap_{n=1}^\infty E_n$ . Then, $y \in E_n$ for each $n$ which implies that $y$ is in exactly one of the closed intervals $\left[\frac{a_n}{10^n}, \frac{a_n+1}{10^{n}}\right]$ . Define the decimal expansion of $y$ as $y=0.d_1d_2d_3\ldots\;$ . First we show that $d_1$ is either $4$ or $7$ . STTC that $d_1 \notin \{4, 7\}$ . If $d_1 \in \{0, 1, 2, 3\}$ , then $y\le0.4$ . If $y<0.4$ , then $y\notin E_1$ , which is not possible. If $y=0.4$ , then $y\notin E_2$ , which is also not possible. If $d_1 \in \{5, 6\}$ , then $0.5\le y \le 0.7$ . If $0.5< y < 0.7$ , then $y\notin E_1$ , which is not possible. If $y=0.5$ or $y=0.7$ , then $y \notin E_2$ , which is not possible. If $d_1 \in \{8, 9\}$ , then $0.8 \le y< 1$ . If $0.8 <y< 1$ , then $y\notin E_1$ , which is not possible. If $y =0.8$ , then $y\notin E_2$ , which is also not possible. Thus, $d_1 \in \{4, 7\}$ . Similarly, suppose to the contrary that $d_2 \notin \{4, 7\}$ . My idea is that I want to show that if $d_2 \notin \{4, 7\}$ , then that would force that either $y \notin E_2$ or $y \notin E_3$ , which would signal a definite pattern, which is all I want (no need for a formal induction). Thus: If $d_2 \in \{0, 1, 2, 3\}$ , then $0.400 \le y \le 0.740$ . If $0.400 \le y < 0.440$ , then $y \notin E_2$ which is not possible. If $0.440\le y <0.444$ , then $y \notin E_3$ . If $0.444 \le y < \dots$ , If $d_2 \in \{5, 6\}$ , then $0.450 \le y \le 0.770$ . If $y = 0.45$ , then $y \notin E_3$ . If $0.45 < y < 0.47 $ , then $y \notin E_2$ . If $0.47 \le y < 0.474$ , then $y \notin E_3$ . If $0.474 \le y \dots$ , If $d_2 \in \{8, 9\}$ , then $0.480 \le y \le 0.80$ . I haven't developed bullet 3. for $d_2$ because I couldn't even complete the argument in the first two bullets for $d_2$ . Can someone please suggest how the argument for $d_2$ can be completed? (Again, no need for formal induction. I just want to develop an argument for $d_2$ that is similar to $d_1$ .)","['alternative-proof', 'general-topology', 'induction', 'real-analysis']"
3748124,Why does $\frac{a}{b}<0$ imply $ab<0$?,"I'm not sure if this was asked before, but my question is: why does $\frac{a}{b}<0$ imply $ab<0$ ? How do you prove it both intuitively and rigorously(using math)? I think I understand it intuitively: it's becuase for $\frac{a}{b}$ to be negative, exactly one of $a$ or $b$ has to be negative. For $ab$ to be negative, exactly one of $a$ or $b$ has to be negative. That means that these two imply each other. But how would I prove this rigorously? If I multiply both sides of $\frac{a}{b}<0$ by $b$ , first of all, I don't know whether $b$ is positive or negative so I don't know which way the inequality sign is facing, and second, even if we did know that it flipped or didn't flip, we would only get $a<0$ or if the sign didn't flip $a>0$ . Do I split it into cases then(case 1: $b<0$ and case 2: $b>0$ )? It seems like it would work but there might be a more slicker way of proving it?","['fractions', 'algebra-precalculus', 'inequality']"
3748193,Find the value of $\sin^{-1}(\cos 2)-\cos^{-1}(\sin 2) +\tan^{-1}(\cot 4) -\cot^{-1}(\tan 4)+\sec^{-1}(\csc 6)-\csc^{-1}(\sec 6)$,"The given expression simplifies to $$\sin^{-1}(\sin 2)-\cos^{-1}(\cos 2)+\tan^{-1}(\tan 4)-\cot^{-1}(\cot 4)+\sec^{-1}(\sec 6)-\csc^{-1} (\csc 6)$$ $$=(\pi-2)-2+(4-\pi)-(2\pi-4)+(2\pi-6)-(2\pi-6)$$ $$=-2\pi+4$$ But the given answer is $5\pi-16$ . I rechecked all the the principle branches, and they all seem to be right. Where did use the wrong value?","['trigonometry', 'solution-verification', 'inverse-function']"
3748208,"Second derivatives, Hamilton and tangent bundle of tangent bundle TTM","I'm learning the Hamilton formalism of classical mechanics, where a second order differential equation is formalized as two first order differential equations on the cotangent bundle of the configuration manifold. I find the concept of tangent spaces and the notion of the derivative $f_*: TM \to TN$ as a function between tangent spaces very elegant, natural and intuitive. I still struggle, though, with an intuitive understanding of tangent spaces of tangent spaces. Let the $n$ dimensional configuration space $M$ be a smooth manifold, $\pmb{q} \in M$ , then $TM$ is the tangent bundle and $\pmb{v} \in TM$ a tangent vector. Even without local coordinates, every tangent vector can canonically be split into a point $q$ and a vector $\dot q \in T_qM$ . Therefore $\pmb v = (q, \dot q)$ . The intuitive notion of a tangent vector is the notion of a change of position or a velocity (thus the notation) starting at a point. Now lets look at the tangent space of the tangent space $TTM$ . Let $\pmb a \in TTM$ be tangent vector to $TM$ . The intuitive notion of $\pmb a$ is a change of velocity or acceleration. Just as we could do for $TM$ , we can split $\pmb a$ into a ""point"" $(q, \dot q)$ in $TM$ and a vector in $T_{(q, \dot q)}TM$ given by $(\dot{q}, \ddot q)$ , with $\dot{q}$ denoting a change of the fiber and $\ddot{q}$ denoting a change of the vector within the same fiber. Combining with the previous, $\pmb a \in TTM$ consists of $(q, \dot{q}, \dot{q}, \ddot{q})$ . What you might disregard as a double occupancy in notation, is a real problem for my understanding. It seems like the information about the position change is duplicated, not even necessarily consistently. Which roles do the vector component $\dot{q} \in T_{\pmb q}M$ and the fiber change component $\dot{q} \in TTM$ play generally in manifolds? Which role do they play in the Hamilton formalism (if any different)? How to construct a second derivative $f_{* *}: TTM \to TTN$ ? How do these components appear there? How, if at all, does this relate to curvature and torsion of curves? How, if at all, does the exterior derivative $dd=0$ or any other relevant derivative relate to this? The Hamilton equations of motion are $\dot{\pmb q} = \frac{\partial H}{\partial \pmb p}, \dot{\pmb p} = -\frac{\partial H}{\partial \pmb q}$ (with $H: T^*M\to\mathbb R$ and $(\pmb q, \pmb p) \in T^*M$ ). How does the notational double occupancy of $\dot{\pmb q}$ resolve here? In this question I concentrated on $TTM$ while the Hamilton formalism is defined on $TT^*M$ . Is there a fundamental difference between $TT^*M$ and $TTM$ that is relevant to the problem in question?","['co-tangent-space', 'hamilton-equations', 'differential-topology', 'tangent-bundle', 'differential-geometry']"
3748351,Is it possible to solve $\frac{dy}{dx}=y^x$?,"I was trying to solve the equation below just for curiosity, $\frac{dy}{dx}=y^x$ I found that i could only equate $y^x$ with $e^{-i^2x\ln(y)}=\cos(ix\ln(y))-i\sin(ix\ln(y))$ , so the original equation became, $\frac{dy}{dx}+i\sin(ix\ln(y))=\cos(ix\ln(y))$ After I arrived here I couldn't continue. Can someome solve this problem, please?","['calculus', 'ordinary-differential-equations']"
3748395,Function diverges but area under curve is finite?,"I have a curve given by $f(x)=\frac{x}{b^2}(1-\frac{x^2}{b^2})^{-1/2}$ for $x \in (0,b)$ . Upon integration, I get that the area under the curve on the interval $(t, b)$ is given by $F(t) =\sqrt{1-\frac{t^2}{b^2}}$ . The value of $f(x)$ is $0$ for $x=0$ but grows and diverges to positive infinity as $x \to b$ , but the area under the curve  on the interval $(0, b)$ gives a unit area. How can a diverging curve have a finite area? Any help? Ps: its not a homework question.","['integration', 'curves', 'functions', 'area']"
3748455,Difference of Hyperbolic foci and Spivak's Solution,"This is all related to Spivak's Calculus book 3rd Edition, Chapter 4, Appendix III Polar Coordinates, Exercise 5. Here is the exercise: Here is his solution: My problem is the highlighted part of his solution. From what I know, if $R_1$ is the distance form one focus of a hyperbola and $R_2$ is the distance from the other focus of the hyperbola, to a point on the hyperbola, then: $|R_1-R_2|=c$ , where $c$ is constant. When the point is on one of the two parts of the hyperbola $R_1>R_2$ and vice versa. However, he chooses $r>s$ if $a>0$ or $r<s$ if $a<0$ for no apparent reason. Since $a$ is constant, he is clearly making a choice. It is like he is constraining the point to only this one part . If this choice did not alter his desired result I would be fine with it. However if I have not made any mistakes, By his choice, indeed $r = Λ/(1+ε\cos(θ))$ ; By  choosing the opposite, $r = Λ/(1-ε\cos(θ))$ . After arriving at these results I was even more confused since it felt like for a point moving on each part of the hyperbola there was a different equation (in polar coordinates) describing it.
So finally my questions are, Did he, and if he did, why did he make this choice? If my results are correct, how do these two polar equations connect?","['calculus', 'geometry', 'polar-coordinates']"
3748463,Proving the cross product matrix tranformation identity with an alternative solution,"I'm solving a problem from the book, Mathematics for 3D Game Programming and Computer Graphics, Third Edition , by Eric Lengley. The problem goes: Let $N$ be the normal vector to a surface at a point $P$ , and let $S$ and $T$ be tangent vectors at the point $P$ such that $S \times T = N$ . Given an invertible 3 $\times$ 3 matrix $M$ , show that $(MS) \times (MT) = (\text{det}M(M^{-1})^{T}(S \times T)$ , supporting the fact that normals are correctly transformed by the inverse transpose of the matrix $M$ . The author provided a hint stating we can represent $(MS) \times (MT)$ as $$
(MS) \times (MT) =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
MT
$$ We then find a matrix $G$ such that $$
GU
=
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
$$ where $$
U =
\begin{bmatrix}
0 & -S_{z} & S_{y} \\ 
S_{z} & 0 & -S_{x} \\ 
-S_{y} & S_{x} & 0 
\end{bmatrix}
$$ and show that $G = (\text{det}M)(M^{-1})^{T}$ to solve the problem. I am aware that there is an alternative solution to this problem, but I would like to solve it through the hints provided. Unfortunately, I am only able to go as far doing: $$
G =
\begin{bmatrix}
0 & -(MS)_{z} & (MS)_{y} \\ 
(MS)_{z} & 0 & -(MS)_{x} \\ 
-(MS)_{y} & (MS)_{x} & 0 
\end{bmatrix}
M
U^{-1}
$$ At this point, I do not know how to proceed with showing that $G = (\text{det}M)(M^{-1})^{T}$ . How would you proceed? I'd like to ask for hints on solving the problem.","['matrices', 'transformation']"
3748494,Prove $EK'\parallel BX\Leftrightarrow EK'=2EK$ in a given triangle $EBC$ with $\angle E=90^\circ\;\ldots$,"In a given triangle $EBC$ $\ \angle E=90^\circ$ , $D\in BC$ such as $ED\perp BC$ , $M'$ is midpoint of $BE$ , line $CM'$ intersects the circumcircle of $\triangle EBC$ second time in $X$ . $A$ is the second intersection of the line $ED$ with the circumcircle of $\triangle EBC$ . $K$ is the intersection of $AX$ with the perpendicular bisector of $BX$ . Let $K'\in AX$ . Prove that $XK'=2XK\Leftrightarrow EK'\parallel BX$ . This is the missing intermediate step of my solution to this problem . My attempt: I see in the case of $EK'\parallel BX$ the triangles $BXM'$ and $M'NE$ are congruent ( $N=EK'\cap CX$ ) thus $M'$ is the midpoint of $XN$ and $\triangle XM'P\sim \triangle XNB$ but I'm failing too show that $B,\,K,\,N$ are collinear. In the case of $XK'=2XK$ $\ \triangle XKP\sim\triangle XK'B$ hence $BK'\parallel XN$ but I'm missing to show $BK'=XN$ . However, if it's more simple to solve the original problem and hence show the desired result (i.e. ""the hole in the solution is as large as the solution itself""), you are free to answer the original question instead. Thank you.","['contest-math', 'euclidean-geometry', 'geometry']"
3748497,Compute the improper integral $\int_A \frac{dx dy dz}{(1+x^2z^2)(1+y^2z^2)}$ over an infinite cuboid,"Let $A := \{ (x, y, z) \in \mathbb{R}^{3} : 0 < x < 1, 0 < y < 1, 0 < z \} $ . Show that the function $f: A \to \mathbb{R}$ defined by $$f(x, y, z) := \frac{1}{(1+x^2z^2)(1+y^2z^2)}$$ is integrable in $A$ and calculate the integral $\int_{A} f$ . My attempt: Showing the integral exists is easy using the Comparison test for improper integrals. Now, for computing the integral first I've used Fubini's theorem, so I got that $$\int_{A} f = \int_{0}^{\infty} \left( \frac{\arctan(z)}{z} \right)^{2} \: dz \,,$$ which is not an elementary integral. Then I thought about using that $f$ is an even function, and I got that $$\int_{A} f = \frac{1}{8} \int_{D} f\,,$$ where $D:= \{ (x, y, z): -1 < x <1, -1 < y <1 \}$ and tried to use a change of variables to cylindrical coordinates...what doesn't make things easier. I also thought about non-linear changes of variables such as $u := xz$ , $v := yz$ , $z = z$ ... But I am no longer able to solve the integral.","['integration', 'improper-integrals', 'real-analysis', 'multivariable-calculus', 'multiple-integral']"
3748500,"How can I find real, symmetric matrices $A,B$ such that $Ax =\lambda B x $ has no real eigenvalues and eigenvectors? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question $x \in \mathbb C^n$ is called eigenvector of the pair $(A,B)$ with eigenvalue $\lambda$ for two symmetric matrices $A,B \in \mathbb R^{n,n}$ if it holds $Ax = \lambda B x$ . What's the strategy behind finding two symmetric matrices $A,B \in \mathbb R^{n,n}$ such that the pair $(A,B)$ doesn't have any real eigenvalues or eigenvectors?","['matrices', 'symmetric-matrices', 'eigenvalues-eigenvectors']"
3748529,How to show that $(I+T/n)^{-1}$ converges strongly to the identity operator?,Let $T$ be an m-accretive operator on a Hilbert space. Then how can I show that $(I+T/n)^{-1}$ converges in the strong operator topology to the identity operator $I$ ? It seems trickier than I expected...Could anyone help me?,"['hilbert-spaces', 'functional-analysis']"
3748552,Identity for a sum of product of binomial coefficients,"For some fixed positive integers $r_1,\ldots,r_n$ , I would like to find a sum: $$
\sum_{i_1+\cdots+i_n=k}\binom{r_1+i_1}{r_1}\cdots\binom{r_n+i_n}{r_n}=\sum_{i_1+\cdots+i_n=k}\binom{r_1+i_1}{i_1}\cdots\binom{r_n+i_n}{i_n},
$$ where $k=0,\ldots,r_1+\cdots+r_n$ ( $i_j$ ranges from $0$ to $r_j$ , for $j=1,\ldots,n$ ). If reformulate the problem. Multiply $n$ finite sums: $$
\sum_{i_1=0}^{r_1}\binom{r_1+i_1}{r_1}\cdots\sum_{i_n=0}^{r_n}\binom{r_n+i_n}{r_n}
$$ collect and sum parts such that $i_1+\cdots+i_n=k$ . What is the result of every such sum. I have found similar question here , but I can not connect it to this problem. Also found a paper which uses probabilistic method to establish several generalisations of Vandermonde identity (which to my dilettante view is somewhat similar to my problem). Here is a small example just to be clear what I want to achieve. Let $n=3$ and $r_1=1$ , $r_2=2$ , $r_3=3$ . Now take $k=3$ , it takes six combinations of $(i_1,i_2,i_3)$ : $(1,1,1)$ , $(1,2,0)$ , $(1,0,2)$ , $(0,1,2)$ , $(0,2,1)$ , $(0,0,3)$ so that $i_1+i_2+i_3=k$ (note that $i_1, i_2, i_3$ can take values at most $1$ , $2$ and $3$ respectively). So the sum is: \begin{align*}
&&{2\choose1}{3\choose2}{4\choose3}+{2\choose1}{4\choose2}{3\choose3}+{2\choose1}{2\choose2}{5\choose3}+\\
&&{1\choose1}{3\choose2}{5\choose3}+{1\choose1}{4\choose2}{4\choose3}+{1\choose1}{2\choose2}{6\choose3}=\\
&&24+12+20+30+24+20=130.
\end{align*}","['binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
3748580,A weak cancellation property for monoids,"Suppose $M$ is a (commutative) monoid. Typically the cancellation property is defined as $a + c = b + c \Rightarrow a = b$ for all $a,b,c \in M$ . Recently I was working on a problem where I thought I needed cancellation, but it turned out that the weaker version $a + c = c \Rightarrow a = 0$ for all $a,c \in M$ would already be sufficient. My questions are: Is this actually a weaker property than cancellation?
It is implied by cancellation by choosing $b = 0$ , but despite trying some things out myself I am not yet 100% convinced that it is not just cancellation in disguise. If it is actually a weaker version of cancellation, is there some reading or other material on it anywhere or does it even have a name? Note: Commutativity is not really needed, but it was where I stumbled upon this so I just kept it for the sake of simplicity.","['monoid', 'abstract-algebra']"
3748637,Help with the last step in solving $\lim_{x\to0}\frac{(1+\sin x +\sin^2 x)^{1/x}-(1+\sin x)^{1/x}}x$,"I managed to solve part of this limit but can't get the final step right. Here's the limit: $$
\lim_{x\to0} {
	\frac
	{
		\left(
			1+\sin{x}+\sin^2{x}	
		\right)
		^{1/x}
		-
		\left(
			1+\sin{x}
		\right)
		^{1/x}
	}
	{
		x
	}
}
$$ Let's begin then. Using $f(x) = e^{\log{f(x)}} = \exp{\left[\log{f(x)}\right]}$ it gets to: $$
\lim_{x\to0} {
	\frac
	{
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}+\sin^2{x}\right)}
			}
			{
				x
			}
			\right]
		-
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}\right)}
			}
			{
				x
			}
			\right]
	}
	{
		x
	}
}
$$ Multiplying and dividing by the right terms: $$
\lim_{x\to0} {
	\frac
	{
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}+\sin^2{x}\right)}
			}
			{
				\sin{x}+\sin^2{x}
			}
			\frac
			{
				\sin{x}+\sin^2{x}
			}
			{
				x
			}
			\right]
		-
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}\right)}
			}
			{
				\sin{x}
			}
			\frac
			{
				\sin{x}
			}
			{
				x
			}
			\right]
	}
	{
		x
	}
}
$$ Grouping the $\sin{x}$ in the first exponential function: $$
\lim_{x\to0} {
	\frac
	{
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}+\sin^2{x}\right)}
			}
			{
				\sin{x}+\sin^2{x}
			}
			\frac
			{
				\sin{x}
			}
			{
				x
			}
			\left(1+\sin{x}\right)
			\right]
		-
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}\right)}
			}
			{
				\sin{x}
			}
			\frac
			{
				\sin{x}
			}
			{
				x
			}
			\right]
	}
	{
		x
	}
}
$$ Applying the know limits, specifically: $$\lim \limits_{x\to0}{\frac{\log{\left(1+\sin{x}+\sin^2{x}\right)}}{\sin{x}+\sin^2{x}}=1}$$ $$\lim \limits_{x\to0}{\frac{\log{\left(1+\sin{x}\right)}}{\sin{x}}=1}$$ $$\lim \limits_{x\to0}{\frac{\sin{x}}{x}=1}$$ $$\lim \limits_{x\to0}{\left(1+\sin{x}\right)}=1$$ We should end up with: $$
\lim_{x\to0} {
	\frac
	{
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}+\sin^2{x}\right)}
			}
			{
				\sin{x}+\sin^2{x}
			}
			\frac
			{
				\sin{x}
			}
			{
				x
			}
			\left(1+\sin{x}\right)
			\right]
		-
		\exp
			\left[
			\frac
			{
				\log{\left(1+\sin{x}\right)}
			}
			{
				\sin{x}
			}
			\frac
			{
				\sin{x}
			}
			{
				x
			}
			\right]
	}
	{
		x
	}
}
=
\frac
{
	\exp{\left[1(1)(1)\right]}
	-
	\exp{\left[1(1)\right]}
}
{
	0
}
	= \frac{e-e}{0}
	= \frac{0}{0}
$$ Undetermined form, yay. Any hint is really appreciated, and I'm really really sorry for all the spaghetti rendering, it's hard to look at.","['limits', 'limits-without-lhopital', 'real-analysis']"
3748652,Determine the set of values taken by a Bohr's function,"Let $f(s)$ be a general Dirichlet series $$f(s)=\sum_{n=1}^\infty a(n)e^{-s\lambda(n)}$$ where $\lambda(n)$ and $a(n)$ are given by It is easy to prove that the Bohr function associated with this Dirichlet series is $$F(z_1,z_2,z_3)=\cos(iz_1)-\frac{1}{2}i\sin(iz_2)(1+\cos(iz_1))+\frac{1-2e^{-z_3}}{2-e^{-z_3}}$$ Now I was told to find the set of values taken by $F$ at any $s$ with a real part $0$ : $$U_f(0)=\{F(Z):\Re(Z)=0\}$$ I tried to plot some values of the function using python and got the following result. It seems to me that this is a triangle with rounded corners. So I claim (maybe falsely) that for some $u\in U_f(0)$ we could always find $0\leqslant\theta_1,\theta_2,\theta_3$ such that $\theta_1+\theta_2+\theta_3=1$ and there exists a point $v=(-1)\theta_1+(1-i)\theta_2+(1+i)\theta_3$ (these three points are said to be significant in the hint) such that $$|u-v|\leqslant 1$$ Therefore I expanded $u$ and got $$F(iy_1,iy_2,iy_3)=\left(\cos y_1+1+\frac{\cos y_3+1}{4\cos y_3-5}\right)+i\left(\frac{\cos y_1+1}{2}\sin y_2-\frac{3\sin y_3}{4\cos y_3-5}\right)$$ However, I couldn't do much after this. I think it is possible to prove that this set is closed (might be wrong) and therefore $$U_f(0)=\overline{U_f(0)}=\overline{V_f(0)}$$ which is the set of values taken by $f(it)$ . Then $$\begin{align*}
f(it)
={}&\frac{-3\cos(t\log 3)}{4}+\cos(t)+\frac{1}{2}\\
&+i\left(\frac{\sin(t\log 2-t)}{4}+\frac{\sin(t\log 2+t)}{4}+\frac{3\sin(t\log 3)}{4}+\frac{\sin(t\log 2)}{2}\right)\\
&+\frac{-3^{1-it}}{4(2\cdot 3^{it}-1)}
\end{align*}$$ Even though plotting this function gives the same thing, I believe this is even worse than bashing on $U_f(0)$ . This problem is from Apostol's Modular Functions and Dirichlet Series in Number Theory . I tried to bash out the answer as you can see. But I believe there should be some better ideas and some smarter ways to tackle this problem. Any help will be appreciated; thanks in advance.","['analytic-number-theory', 'functions', 'graphing-functions', 'dirichlet-series']"
3748721,total differentiability of $\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)}$ if the denominator equals $0$,"Let $f:\mathbb{R}^3\to\mathbb{R}$ be given by $f(x,y,z)=\frac{x^3z^4}{(x^2+y^2)(y^2+z^2)}$ if the denominator is not equal to $0$ and otherwise by $f(x,y,z)=0$ . Find all the points where $f$ is differentiable. $f$ is surely differentiable when the denominator is not equal to zero so we only need to consider the case that $(x^2+y^2)(y^2+z^2)=0$ .
However, even if we can show that the partial derivatives of $f$ are not continuous at those points, that still doesn't imply that $f$ is not differentiable there. How do I continue from here?","['multivariable-calculus', 'derivatives', 'real-analysis']"
3748746,"Given positive real numbers $a$, $b$, $c$, $d$, $e$ with $\sum_{\text{cyc}}\,\frac{1}{4+a}=1$, prove that $\sum_{\text{cyc}}\,\frac{a}{4+a^2}\le1$.","Let $a, b, c, d, e$ be positive real numbers such that $$\dfrac{1}{4+a} + \dfrac{1}{4+b} +\dfrac{1}{4+c} +\dfrac{1}{4+d} +\dfrac{1}{4+e}  = 1.$$ Prove that $$\dfrac{a}{4+a^{2}} + \dfrac{b}{4+b^{2}} +\dfrac{c}{4+c^{2}} +\dfrac{d}{4+d^{2}} +\dfrac{e}{4+e^{2}}  \leq 1.$$ My question is how to prove this inequality by using AM-GM inequality? My solution (using the Chebyshev inequality). Since $\dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} =1,$ we have $$1 = \dfrac{1}{4+a}+\dfrac{1}{4+b}+\dfrac{1}{4+c}+\dfrac{1}{4+d}+\dfrac{1}{4+e} \geq \dfrac{a}{4+a^2}+\dfrac{b}{4+b^2}+\dfrac{c}{4+c^2}+\dfrac{d}{4+d^2}+\dfrac{e}{4+e^2}$$ $$\Leftrightarrow \dfrac{1-a}{(4+a)(4+a^2)}+\dfrac{1-b}{(4+b)(4+b^2)}+\dfrac{1-c}{(4+c)(4+c^2)}+\dfrac{1-d}{(4+d)(4+d^2)}+\dfrac{1-e}{(4+e)(4+e^2)} \geq 0.$$ Suppose that $a \geq b \geq c \geq d \geq e$ . Then, we get $$\dfrac{1-a}{4+a} \leq \dfrac{1-b}{4+b} \leq \dfrac{1-c}{4+c} \leq \dfrac{1-d}{4+d} \leq \dfrac{1-e}{4+e}.$$ and $$\dfrac{1}{4+a^2} \leq \dfrac{1}{4+b^2} \leq \dfrac{1}{4+c^2} \leq \dfrac{1}{4+d^2} \leq \dfrac{1}{4+e^2}.$$ Applying the Chebyshev inequality, one gets $$ \sum_{cyc}\dfrac{1-a}{(4+a)(4+a^2)} \geq \dfrac{1}{5} \sum_{cyc}\dfrac{1-a}{4+a}. \sum_{cyc}\dfrac{1}{4+a^2} = \dfrac{1}{5}\sum_{cyc}\dfrac{1}{4+a^2} \sum_{cyc} \left( \dfrac{5}{4+a}-1 \right)=0.$$","['tangent-line-method', 'a.m.-g.m.-inequality', 'analysis', 'real-analysis', 'inequality']"
3748759,Prove: $\lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x)$,"Let $f$ be a continuous function $[0,1] \rightarrow \mathbb{R}$ .
Prove: $\forall \ t> 0;\frac{1}{t}\ln\Big(\int_0^1 e^{-tf(x)}dx\big) \le -\min \ f(x)$ $\lim_{t \to \infty} \frac{1}{t}\ln\big(\int_0^1 e^{-tf(x)}dx\big) = -\min \ f(x)$ I solved 1) Let $L =\min \ f(x)$ so : $\frac{1}{t}\ln(\int_0^1 e^{-tf(x)}dx) \le \frac{1}{t}\ln(\int_0^1 e^{-tL}dx) = \frac{1}{t}\ln(e^{-tL}) = \frac{-tL}{t} = -L$ But I'm not sure how to bound it from below or how to use another method to show the equality","['integration', 'limits', 'calculus']"
3748796,Is the radius of convergence related to the ratio limit or half of the interval of convergence?,"I have a series $S$ with general terms $a_n=\frac{(-1)^n(x-1)^n}{(2n-1)2^n}$ , $n\ge 1$ : $$S = \sum_{n=1}^\infty \frac{(-1)^n(x-1)^n}{(2n-1)2^n}$$ Finding the ratio $\left|\frac{a_{n+1}}{a_n}\right|$ and then finding the limit of the ratio as $n\to\infty$ , I find the limit to be $1$ and the interval to be $-1 \lt x \lt 3$ . More declaratively, the interval is $\left|\frac{x−1}{2}\right| \lt 1$ which I've refined to what was said earlier. I've read conflicting sites that state the radius $R$ of convergence is $\frac{1}{N}$ , where $N$ is the limit as found earlier, but also that it's half the interval length. Here's my work: $$\begin{align}
\left|\frac{a_{n+1}}{a_n}\right| &= \left|\frac{\frac{(-1)^{n+1}(x-1)^{n+1}}{(2(n+1)-1)2^{n+1}}}{\frac{(-1)^{n}(x-1)^{n}}{(2n-1)2^{n}}}\right| \\
&= \left|\frac{(-1)^{n+1}(x-1)^{n+1}(2n-1)(2^n)}{(-1)^n(x-1)^n(2(n+1)-1)(2^{n+1})}\right| \\
&= \left|\frac{(-1)(x-1)(2n-1)}{(2n+2-1)(2)}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times \left|\frac{2n-1}{2n+2}\right|
\end{align}$$ Then, finding the limit $L$ : $$\begin{align}
L &= \lim_{n\to\infty} \left(\left|\frac{-(x-1)}{2}\right| \times \left|\frac{2n-1}{2n+1}\right|\right) \\
&= \left|\frac{-(x-1)}{2}\right| \times \lim_{n\to\infty} \left|\frac{2n-1}{2n+1}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times \lim_{n\to\infty} \left|\frac{\frac{2n}{n}-\frac{1}{n}}{\frac{2n}{n}+\frac{1}{n}}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times \lim_{n\to\infty} \left|\frac{2-\frac{1}{n}}{2+\frac{1}{n}}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times \left|\frac{\lim_{n\to\infty} \left(2-\frac{1}{n}\right)}{\lim_{n\to\infty} \left(2+\frac{1}{n}\right)}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times \left|\frac{2}{2}\right| \\
&= \left|\frac{-(x-1)}{2}\right| \times 1 \\
&= \left|\frac{-(x-1)}{2}\right|
\end{align}$$ Then I know my interval is $\left|\frac{-(x-1)}{2}\right| \lt 1$ : $$\left|\frac{-(x-1)}{2}\right| \lt 1 \\
-1 \lt \frac{x-1}{2} \lt 1 \\
-2 \lt x-1 \lt 2 \\
-1 \lt x \lt 3$$ If the limit found earlier is $1$ , the radius would be $R = \frac{1}{1} = 1$ , yet I've found the interval to be $(-1, 3)$ , which would imply $R = 2$ . Where have I made an error?","['power-series', 'convergence-divergence', 'sequences-and-series']"
3748842,Cardinality of the set of all the subsets of $X$ which have cardinality less than $|X|$,"Let $X$ be an infinite set of cardinality $|X|=\kappa$ , and let $\mathcal{P}_{< \kappa}(X)$ be the set of all subsets $S$ of $X$ such that $|S| < \kappa$ . Is it true that $|\mathcal{P}_{< \kappa}(X)| < 2^{\kappa}$ ? I do not know the anser to the question, and any idea is welcome. Thank you very very much in advance for your help. NB. I have an elementary knowledge of set theory. All that I know about this issue is what I found stated and proved in Jech, Set Theory, Third Millenium Edition, pp. 51- 52: \begin{equation}
| \mathcal{P}_{< \kappa}(X) | = \kappa^{< \kappa},
\end{equation} where $\kappa^{< \kappa}$ is defined as \begin{equation}
\kappa^{< \kappa}= \sup \{ \kappa^{\mu}: \mu \textrm{ is a cardinal and } \mu < \kappa \}.
\end{equation}","['elementary-set-theory', 'set-theory']"
3748890,Finding the Laurent Series of $f(z)=\displaystyle\frac{z^2 e^{1/z}}{z-1}$ for $0<|z|<1$,"I have to calculate the Laurent series at the origin of $f(z)=\displaystyle\frac{z^2 e^{1/z}}{z-1}$ for $0<|z|<1$ , my idea was to use the Cauchy product, but I don't know if it is correct. $$\frac{1}{z-1}=-\sum_{n=0}^\infty z^n\Rightarrow \frac{z^2}{z-1}=-\sum_{n=0}^\infty z^{n+2}$$ and also $e^{1/z}=\displaystyle\sum_{n=0}^\infty \frac{1}{n! z^n}$ , then we have $$
\begin{array}{ccl}
f(z)&=&\displaystyle\frac{z^2}{z-1}\cdot e^{1/z}\\
&=&\displaystyle\left(-\sum_{n=0}^\infty z^{n+2}\right)\cdot\left(\sum_{n=0}^\infty \frac{1}{n! z^n}\right)\\
&=&-\displaystyle\sum_{n=0}^\infty\left(\sum_{k=0}^nz^{k+2}\cdot \frac{1}{(n-k)!z^{n-k}}\right)\\
&=&-\displaystyle\sum_{n=0}^\infty\left(\sum_{k=0}^n\frac{z^{2k-n+2}}{(n-k)!}\right)
\end{array}
$$","['complex-analysis', 'power-series', 'laurent-series']"
3748894,"Finding derivative of $f(z) =\sqrt r[\cos\theta/2 +i \sin\theta/2]$, where $r>0$ and $0<\theta<2π$.","Show that $f(z) =\sqrt r [\cos\theta/2 +i \sin\theta/2]$ , where $r>0$ , $0<\theta<2π$ is differentiable. Find $f'(z)$ . I have tried to solve it by cauchy-rieman(CR equation) but got stuck in midway. Please help me to solve it. Thanks a lot","['complex-analysis', 'multivariable-calculus', 'complex-integration', 'derivatives', 'complex-numbers']"
3748936,Consider the sum $S=\sum\frac{1}{x^2}$ which is over all the positive real solutions of the equation $\frac{\tan{x}}{x}=n$,"Consider the sum $S(n)=\sum\dfrac{1}{x^2}$ where summation is performed over all the positive real solutions of the equation $\dfrac{\tan{x}}{x}=n$ . If it's given that $S(n)=1$ , $n\in\mathbb{Q}$ , find $n$ . My attempt: We're interested in the roots of $$\frac{\tan x}{x}=k$$ Then, performing Taylor expansion of $$\sin x = kx\cos x$$ $$x- \frac{(x^6)}{6}+ ......  = kx(1- \frac{x^2}{2}+........$$ I am stuck here. Any hints will be appreciated. Thanks","['roots', 'calculus', 'functions', 'sequences-and-series', 'trigonometry']"
3748948,Convergence of series with negative terms,"Among the series $\sum\limits_{n=1}^{\infty}\frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{n}, \sum\limits_{n=1}^{\infty} \frac{(-1)^{\lfloor\log n\rfloor}}{n}$ , and $\sum\limits_{n=1}^{\infty}\frac{(-1)^{\lfloor \sqrt{n}\rfloor}}{\sqrt{n}}$ ; which are convergent? The leibniz test fails here as the series are not alternating and the series are not absolutely convergent. I think we have to consider separately the sum of positive and negative terms and then check for their convergence.  Any hints? Thanks beforehand.","['calculus', 'sequences-and-series', 'real-analysis']"
3748956,Show that all terms of this sequence are integers.,"The sequence is given by the following formula: $$(n+3)a_{n+2}=(6n+9)a_{n+1}-na_n$$ with $a_0=1$ and $a_1=2$ . Show that all terms of this sequence are integers. I have an original solution for this problem (next, will be briefly outlining that). It first determines a generating function $\big(f(x)=\sum_{n=0}^\infty a_nx^n \big)$ and finds its explicit form in some neighborhood of $0$ . Namely, $$f(x)=\frac{1-x-\sqrt{x^2-6x+1}}{2x},  \ \ |x|<3- \sqrt 2$$ Then, says this function satisfies the quadratic equation $xt^2-(1-x)t+1=0$ . After some operations with it, they end up with obtaining another recurrence formula for $(a_n)$ as follows: $$a_{n+1}=a_n + \sum_{k=0}^n a_k a_{n-k}$$ Obviously, an easy induction will complete the proof now. However, I'd like to know if one could approach this by some other methods/thinking. The reason why I'm curious is that this solution gets quite a messy appearance while finding the form of $f(x)$ and I don't think this beautiful problem has no other rather attractive approach. By the way , the OS says proving that it's an increasing sequence is an easy job and skipping that part. Maybe it is really easy but I'm troubled to show... Any help is appreciated.","['recurrence-relations', 'sequences-and-series']"
3748967,Prove that $f(z)=\sum_{n=1}^\infty \exp(-n!z)$ has no analytic continuation,"Prove that $f(z)=\sum_{n=1}^\infty\exp(-n!z)$ has no analytic continuation to any open connected subset of $\mathbb{C}$ that strictly contains $\{z\in\mathbb{C}:\text{Re}z>0\}$ . My proof: If $U$ is an open connected subset of $\mathbb{C}$ that strictly contains $\{z\in\mathbb{C}:\text{Re}z>0\}$ , and $f$ can be extended analytically to $U$ , then there exists $b\in\mathbb{R}$ such that $bi\in U$ . Furthermore, we may assume that $b$ is rational, which means that $n!b$ is an even integer for all $n$ large enough. This means that if $a>0$ , then $$f(a+bi)=C+e^{-n!a}+e^{-(n+1)!a}+e^{-(n+2)!a}+...$$ where $C$ is a complex number. Therefore, we get $|f(a+bi)|\rightarrow \infty$ as $a\rightarrow 0+$ by the monotone convergence theorem. This is means that there isn't even a continuous extension. My questions: is my proof correct? Also I feel like my proof is too specific to this question, if they change the formula of $f$ to for example $\sum_{n=1}^\infty e^{(-n!+\sin n)z}$ my proof will not work. Are there any better, more general proofs available? Thanks!!",['complex-analysis']
3749030,Variance of a series of IID's vs a multiple of a random variable,"For any random variable, X, Var(aX) = a^2*Var(X), which is easy to demonstrate. Suppose you have a series of IID's, and want to find the variance.  So, in that case for example, Var(X+X+X+X+X) = Var(X) + Var(X) + Var(X) + Var(X) + Var(X) = 5Var(X) since there is no covariance involved.  But isn't Var(X+X+X+X+X) = Var(5X) = 25Var(X)? Follow-up, when doing the variance of a sum of dependent random variables would you add two times every possible pairwise covariance to the individual variances?",['probability']
3749037,$3 \times 3$ matrix with determinant a large power of $2$,"Here's a little curiosity I found.  The following $3 \times 3$ matrix has entries that are distinct primes $< 100$ and its determinant is $2^{19}$ . $$ \pmatrix{71 & 31 & 97\cr 61 & 67 & 23\cr 7 & 83 & 73}$$ Does anyone know of another such matrix whose determinant is nonzero and divisible by a larger power of $2$ ?  If not, what is the least $p$ such that there is a $3 \times 3$ matrix with entries distinct primes $\le p$ and determinant nonzero and divisible by a larger power of $2$ ?  What if we drop the requirement of the entries being primes (and allow distinct positive integers $\le p$ )?","['matrices', 'elementary-number-theory', 'determinant', 'linear-algebra']"
3749045,Evaluate $\int_0^{\frac{\pi}{2}} \frac{\sin^3{(2x)}}{\ln{\left(\csc{x}\right)}} \mathop{dx}$,Challenge problem by friend is $$\int_0^{\frac{\pi}{2}} \frac{\sin^3{(2x)}}{\ln{\left(\csc{x}\right)}} \mathop{dx}$$ I know you can write $\ln{\left(\csc{x}\right)}=-\ln{\sin{x}}$ and $\sin{(2x)}=2\sin{(x)}\cos{(x)}$ .  I tried rewriting the integral but then could not go further.  Even Wolfram Alpha ( https://www.wolframalpha.com/input/?i=integral+of+sin%5E3%282x%29%2F%28log%28csc%28x%29%29%29+dx+from+0+to+pi%2F2 ) could not get a closed form!?  Is it even possible.,"['integration', 'definite-integrals', 'real-analysis', 'calculus', 'indefinite-integrals']"
3749072,10 equivalent definitions of normal subgroup,"I've found various equivalent definitions of normal subgroup from this Wikipedia page . I've just finished proving their equivalence. Could you please verify if it is fine or contains logical mistakes? Let $N$ be a subgroup of $G$ . Then the following statements are equivalent. a. For all $g,h \in G$ : $gh \in N \iff hg \in N$ . b. For all $g \in G$ : $gNg^{-1} \subseteq N$ . c. For all $g \in G$ : $gNg^{-1} = N$ . d. The sets of left and right cosets of $N$ in $G$ coincide. e. For all $x,y,g,h \in G$ : $x \in gN$ and $y \in hN \implies xy \in (gh)N$ . f. For all $n \in N, g \in G$ : $n^{-1} g^{-1} n g \in N$ . g. For all $n \in N, g \in G$ : $g n g^{-1} \in N$ . h. $N = \bigcup_{n \in N} \operatorname{Cl}(n)$ where $\operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\}$ . i. For all $g \in G$ : $gN = Ng$ . j. There exists a group homomorphism whose domain is $G$ and kernel is $N$ . My attempt: To make it easier to follow, I put each part of the proof between two consecutive definitions. For all $g,h \in G$ : $gh \in N \iff hg \in N$ . Assume the contrary that there exist $n\in N$ and $g \in G$ such that $gng^{-1} \notin N$ . Then $g^{-1} (gn) = n\notin N$ by (a). This is a contradiction. For all $g \in G$ : $gNg^{-1} \subseteq N$ . Substituting $g$ for $g^{-1}$ in $gNg^{-1} \subseteq N$ , we get $g^{-1}  N g\subseteq N$ . Substituting $N$ for $g^{-1} N g$ in $gNg^{-1} \subseteq N$ , we get $N \subseteq g^{-1} N g$ . As a result, $g^{-1} N g = N$ . Substituting $g$ for $g^{-1}$ in $g^{-1} N g = N$ , we get the desired result. For all $g \in G$ : $gNg^{-1} = N$ . It follows from (c) that $gN= Ng$ . The result then follows. The sets of left and right cosets of $N$ in $G$ coincide. Because of (d) and the fact that $h \in hN \cap Nh$ , we have $hN=Nh$ . It follows from $x \in gN$ and $y \in hN$ that $x = gn_1$ and $y = hn_2$ for some $n_1,n_2 \in N$ . Then $xy = gn_1 hn_2$ . Because $Nh = hN$ , $n_1 h = h n_3$ for some $n_3 \in N$ . Then $xy = g h n_3 n_2 = (gh) (n_3 n_2) \in (gh) N$ . For all $x,y,g,h \in G$ : $x \in gN$ and $y \in hN \implies xy \in (gh)N$ . We have $n^{-1} g^{-1} n \in n^{-1} g^{-1} N$ and $g \in gN$ . Then by (e), we have $n^{-1} g^{-1} n g \in (n^{-1} g^{-1} g)N = n^{-1} N = N$ . For all $n \in N, g \in G$ : $n^{-1} g^{-1} n g \in N$ . Substituting $g$ for $g^{-1}$ in $n^{-1} g^{-1} n g \in N$ , we get $n^{-1} g n g^{-1}  \in N$ . Because $n^{-1} \in N$ , we have $g n g^{-1}  \in N$ . For all $n \in N, g \in G$ : $g n g^{-1} \in N$ . It follows from (g) that $\operatorname{Cl}(n) \subseteq N$ for all $n \in N$ . Then $\bigcup_{n \in N} \operatorname{Cl}(n) \subseteq N$ . On the other hand, $n \in \operatorname{Cl}(n)$ and thus $N \subseteq \bigcup_{n \in N} \operatorname{Cl}(n)$ . The result the follows. $N = \bigcup_{n \in N} \operatorname{Cl}(n)$ where $\operatorname{Cl}(n) := \{gng^{-1} \mid g \in G\}$ . It follows from (h) that $\operatorname{Cl}(n) \subseteq N$ for all $n \in N$ .  As a result, for all $g \in G, n \in N$ , we have $g n g^{-1} = n'$ for some $n' \in N$ . Hence $gn=n'g$ for some $n' \in N$ . Thus $gN \subseteq Ng$ . By symmetry, we also have $Ng \subseteq gN$ . The result then follows. For all $g \in G$ : $gN = Ng$ . Let $G/N := \{gN \mid g \in G\}$ . We define a binary operation $G/N \times G/N \to G/N$ by $(gN) (hN) \mapsto (gh)N$ . Let's prove that it's well-defined, i.e. $gN = aN$ and $hN = bN$ implies $(gh) N = (ab) N$ . We have $gN= aN$ implies $g=an_1$ for some $n_1 \in N$ . Similarly, $h=bn_2$ for some $n_2 \in N$ . Then $gh = an_1bn_2$ . It follows from (i) that $Nb=bN$ and thus $n_1b=bn_3$ for some $n_3 \in N$ . Hence $gh =abn_3n_2$ . Because $n_2,n_3 \in N$ , we have $n_3n_2 \in N$ and thus $(n_3n_2)N =N$ . As a result, $(gh)N = (abn_3n_2)N = (ab)(n_3n_2)N = (ab)N$ . It's then straightforward to verify that $G/N$ together with above operation is group. Now we define a map $\phi: G \to G/N, g \mapsto gN$ . It's easy to verify that $\phi$ is in fact a group homomorphism such that $\operatorname{ker} \phi = \{g \in G \mid gN = 1N =N\} =N$ . There exists a group homomorphism whose domain is $G$ and kernel is $N$ . Let $\phi: G \to K$ be such a group homomorphism that $\operatorname{ker} \phi = N$ . If $gh \in N$ then $\phi (gh) = \phi(g) \phi (h) = 1$ . This means $\phi(g) = (\phi(h))^{-1}$ . As a result, $\phi (hg) = \phi(h) \phi (g) = \phi(h) (\phi(h))^{-1} = 1$ and thus $hg \in N$ . By symmetry, we have $hg \in N \implies gh \in N$ . This completes the proof. For all $g,h \in G$ : $gh \in N \iff hg \in N$ .","['normal-subgroups', 'group-theory', 'abstract-algebra', 'solution-verification']"
3749077,Evaluating: $\lim_{t\to\infty}\frac1t\int_0^t\sin(\alpha x)\cos(\beta x)dx$,I tried evaluating the integral but maybe there's an easier way. Please help. Here is what I did: $\begin{aligned}\lim_{t\to\infty}\frac1t\int_0^t \sin(\alpha x)\cos(\beta x)dx&=\lim_{t\to\infty}\frac1t\int_0^t\frac12(\sin(\alpha x+\beta x)+\sin(\alpha x-\beta x))dx\\&=\lim_{t\to\infty}\frac1{2t}\left(\frac{\cos((\alpha-\beta)t)}{\alpha-\beta}-\frac{\cos((\alpha+\beta)t)}{\alpha+\beta}-2\right)\end{aligned}$,"['integration', 'limits', 'calculus', 'definite-integrals']"
3749082,Bar problem and heat conduction equation,"A thin bar, defined through $ x\in [0,l] $ has a temparature distribution $ \theta (x,t) $ and has at the point $ x=0 $ a temperature of $0$ .
At the other end there is a heat emission to another medium of temperature $0$ .
Here holds $$ \frac{ \partial \theta }{ \partial x} (l,t)+ \sigma \theta (l,t)= 0 $$ for all $ t \geq 0 $ At the timepoint $ t_o =0 $ the bar has a temperature destribution $ x \mapsto f(x) $ And it holds the heat conduction equation $$ \frac{ \partial \theta }{ \partial t} = a^2 \frac{ \partial^2 \theta }{ \partial x^2} $$ where $ \sigma \in \mathbb{ R}^+, a \in \mathbb{R} \backslash \{0\} $ are constants. Firstly..how can I show with the seperation method , so plugging in $ \theta (x,t)= u(x)v(t) $ I am confused, at which equation do I have to use the separation method? that it will become the Sturm-Liouville Eigenvalueproblem $$ u''+ \lambda u = 0  , u(0)=0$$ $$ \sigma u(l)+ u'(l)=0 $$ where $ \lambda \in \mathbb{R} $ is a constant. And how can I solve the problem and determine the Eigenvalues?
many thanks in advance!","['heat-equation', 'sturm-liouville', 'ordinary-differential-equations', 'partial-differential-equations']"
3749086,How to evaluate the partial derivative of a differential?,"Suppose $M$ is a finite-dimensional smooth manifold and $f\in C^\infty (M)$ . Let us now define function $F:TM\to \mathbb{R}$ by $(p,w)\mapsto \bigr(df(p)\bigr)(w)$ for $p\in M$ and $w\in T_pM$ . Now, I want to show that if $\gamma:I\to M$ is a smooth curve and $(x,U)$ is a local chart in $M$ such that $\gamma(I)\subset U$ ; then $$\frac{\partial F}{\partial x^i}(\gamma(t),\gamma'(t))=\frac{d}{dt}\Bigr(\frac{\partial F}{\partial v^i}\bigr(\gamma(t),\gamma'(t) \bigr) \Bigr),$$ for $i=1,\ldots,$ dim( $M$ ); where $\displaystyle v^i:=\frac{\partial }{\partial x^i}$ . I do know that this should follow easily from the chain rule, but I simply do not understand the partial derivative of $F$ , seems realy weird to me.","['differential-topology', 'derivatives', 'differential-geometry']"
3749117,"UMVUE for $P(X_1>t)$ for some fixed $t>\mu$ when $X_i \sim \operatorname{Exp}(\sigma, \mu)$","$X_1,\ldots,X_n$ is a sample from the density $$f_X(x)=\frac{1}{\sigma}e^{-\frac{x-\mu}{\sigma}}\quad,\,x>\mu$$ I know form a previous step that the UMVUE for $\sigma$ and $\mu$ is $\frac{1}{n-1}(\sum_iX_i-nX_{(1)})$ and $\frac{n}{n-1}X_{(1)}-\frac{1}{n-1}\bar{X}$ . Also, I know that $P(X_1>t) = e^{-\frac{(x-\mu)}{\sigma}}$ .
My question is if there is a way I can use this information to find the UMVUE for $P(X_1>t)$ ? I tried to use the Rao-Blackwell theorem, with $h(X_1) = 1 \text{ if } X_1>t$ 0 OW unbiased estimator for $e^{-\frac{(x-\mu)}{\sigma}}$ . However, the computations for the estimator became tedious and couldn't get to a final answer.","['statistical-inference', 'statistics', 'probability-distributions', 'parameter-estimation', 'probability']"
3749123,What is the jacobian factor?,"The excercise from the book I am solving problem 1.4 of the famous book Pattern recognition and machine learning of Bishop. The idea of the exercise is that in a simple function $f(x)$ the maximum is the same if we apply a transformation $x = g(y)$ . However, in the case of a probability density, this does not hold anymore. I have solved the exercise, but Bishop says this happens because of the Jacobian factor and I do not understand what this means. Could someone help me with this concept?","['machine-learning', 'jacobian', 'pattern-recognition', 'probability', 'density-function']"
3749146,"Let $f:\mathbb {R}^2 \to \mathbb {R}$ be a $C^1$-function with $D_2f(0,0)=1$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question What's true and what's false? Let $f:\mathbb {R}^2 \to \mathbb {R}$ be a $C^1$ -function with $D_2f(0,0)=1$ . Assume $g:\mathbb {R}\to \mathbb {R}$ fulfills $g(x)=0$ and $f(x,g(x))=0$ , then $g$ is in a neighbourhood of $0$ continuous differentiable. For every function $g:\mathbb {R}^m \to \mathbb {R}^n$ exists a function $f:\mathbb {R}^{m+n} \to \mathbb {R}^n$ such that $(x,g(x))$ exactly describes the solution $f(x,y)=0$ I'm pretty sure that 1. is true, that's just the implicit function theorem. But I don't really know about 2..","['analysis', 'real-analysis']"
3749152,Does $\int _0^{\pi }e^x\sin ^n x\:\mathrm{d}x$ have a closed form?,"Does a closed form for $\displaystyle\int _0^{\pi }e^x\sin ^n x\:\mathrm{d}x$ exist? I tried to evaluate this with values like $n=1,2$ with integration by parts and it seemed fine but when i tried with higher values such as $n=3,4,5$ it became more tedious and couldn't manage to evaluate. Could you please help me find the closed form of this expression please?.","['integration', 'definite-integrals']"
3749166,Rate of Growth of Permutations of Rubik's Cubes,"I'd like to know how fast the number of permutations grows on an $n\times n\times n$ Rubik's Cube as $n$ increases. I'm well aware of the $\frac{3^88!2^{12}12!}{12}$ calculation for the permutations of a $3\times3\times3$ , and I know this idea generalizes (with a little bit of work to deal with the denominator). But I'm struggling to come up with a general formula for the number of permutations in an $n\times n\times n$ Cube. I would hypothesize that the rate of growth is worse than super-exponential, but I'm not sure.","['permutations', 'group-theory', 'combinatorics', 'rubiks-cube']"
3749175,L'Hopital's rule conditions,"I have seen easy geometrical argument why L'Hopital's rule ( $\lim_{x \to a} \frac{f(x)}{g(x)} = \lim_{x \to a} \frac{f'(x)}{g'(x)}$ ) works (local linearization). But, I still don't understand this: why is rule defined just when limit is in form $\frac{0}{0}$ or $\pm \frac{\infty}{\infty}$ ? Why must be $f(a) = g(a) = 0$ ? why must be $g'(a) \neq 0$ ? Counterexample for 1: $\lim_{x \to 0} \frac{e^x}{x^2 + x + 1} = \frac{1}{1}$ but is also $\lim_{x \to 0} \frac{(e^x)'}{(x^2 + x + 1)'} = \lim_{x \to 0} \frac{e^x}{2x + 1} = \frac{1}{1}.$ So, L'Hopital's rule works here but $\frac{1}{1} \neq \frac{0}{0}$ ! Also, I read that there is another condition:
4. $\frac{f'(x)}{g'(x)}$ must exist does condition 3 implies this? can you give example when original limit exist but $\frac{f'}{g'}$ does not and how is this possible if functions $f, g$ are differentiable?","['limits', 'calculus', 'geometry', 'real-analysis']"
3749177,Fibonacci and tossing coins,"Consider the following scheme starting with a sequence $\sigma_0 = \langle 1,1,\dots,1\rangle$ of length $k$ , successively followed by sequences $\sigma_i$ of the same length but shifted by one to the right, where the first entry $\sigma_{i0}$ equals the sum of all values above, and $\sigma_{ij} = \sigma_{i0}$ . For $k = 5$ one has: 1  1  1  1  1                        
    1  1  1  1  1                     
       2  2  2  2  2                  
          4  4  4  4   4             
             8  8  8   8   8          
               15 15  15  15  15      
                  29  29  29  29  29  
                      56  56  56  56  56
                         108 108 108 108 108
                             208 208 208 208 208 Calculating the sum for each column one gets e.g. for $k = 5$ : 1  2  4  8 16 30 58 112 216 416 802 1546 2980 5744 ... It turns out that for $k = 3$ and $k = 4$ these sequences, namely 1 2 4 6 10 16 26 42 68 110 178 288 466 754 1220 1974 ... and 1 2 4 8 14 26 48 88 162 298 548 1008 1854 3410 6272 ... seem to be the numbers of ways to toss a coin $n$ times and not get a run of $k$ (see A128588 and A135491 ). Conjecture : This holds in general, i.e. for arbitrary $k$ . My question is two-fold: How to prove this conjecture? What do the schemes above have to do with tossing a coin and counting runs? Guess : When you try to calculate the numbers of ways to toss a coin $n$ times and not get a run of $k$ you may come up with those schemes. But how? Note that the sequence for $k=3$ ( A128588 ) happens to be double the Fibonacci numbers. The schemes arose when I tried to mimic epidemic spread in a SIR-like discrete model (see here ).","['combinatorics', 'sequences-and-series']"
3749188,"Is it allowed to take the total derivative of an infinitesimal, and is it equal to zero?","For instance, I start with this relation: $$
s^2=x^2+y^2
$$ Taking the total derivative on each side, I get: $$
2sds=2xdx+2ydy
$$ Can I take the total derivative a second like this: $$
d[sds]=d[xdx]+d[ydy]\\
dsds+sd[ds]=dxdx+xd[dx]+dydy+yd[dy]\\
(ds)^2=(dx)^2+(dy)^2
$$ where $d[d[s]]=0$ .","['calculus', 'implicit-differentiation', 'derivatives']"
3749200,Kernel of $(I-A)^2$ where $A$ has unique real eigenvalue $1$,"Assume I have a real square matrix $A$ with a simple eigenvalue $1$ and corresponding eigenvector $v$ . Then $v$ should span the kernel of the matrix $I-A$ , where $I$ is the identity matrix. Now I am wondering the following: does $v$ also span the kernel of $(I-A)^2$ ? Writing $(I-A)^2 x = 0 \iff (I-A)x = A(I-A)x$ this question should be equivalent to asking whether $\operatorname {span}(v) = \ker(I-A)$ can lie in the image of $I-A$ . Thanks in advance for any help on this matter.","['linear-algebra', 'linear-transformations']"
3749229,how to use the chain rule in a multivariable function?,"Problem: Let the function $f(x,y)=(x^2+y^2)\sin(x)$ where $x=r^2e^s$ and $y=rs$ Using the chain rule compute $\frac{\partial f}{\partial r}$ and $\frac{\partial f}{\partial s}$ and then compute $\frac{\partial^2 f}{\partial r^2}$ , $\frac{\partial^2 f}{\partial s^2}$ , $\frac{\partial^2 f}{\partial r \partial s}$ and $\frac{\partial^2 f}{\partial s \partial r}$ I do this: Using the chain rule $$\frac{\partial f}{\partial r}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial r}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial r}$$ $$\frac{\partial f}{\partial r}=[(x^2+y^2)\cos(x)+2x\sin(x)]2re^s+2y\sin(x)s$$ $$\frac{\partial f}{\partial s}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial s}+\frac{\partial f}{\partial y}\frac{\partial f}{\partial s}$$ $$\frac{\partial f}{\partial s}= [(x^2+y^2)\cos(x)+2x\sin(x)]e^sr^2+2y\sin(x)r$$ is this right? and I don't know how to compute the second-order partial derivatives, I need to use the chain rule?","['partial-derivative', 'multivariable-calculus', 'calculus', 'chain-rule']"
3749233,Proof that $A\mapsto A^3$ is one to one where $A$ is a symmetric matrix,"Let $ S_n$ denote the set of real symmetric matrices and $f: S_n\to S_n$ , $A\mapsto A^3$ . I want to prove that $f$ is a bijective function. Using spectral theorem  I manage to prove that $f$ is onto. Now for the one to one stuff I found some difficulties and I doubt that it's not true but I can't find a counterexample. This is my try: let $A,B\in S_n$ such that $S=A^3=B^3$ . Since $S$ is symmetric then by spectral theorem I prove that $A$ and $B$ have the same spectrum (the set of eigenvalues) and the same eigenspaces but I can't from this point prove that $A=B$ . Any suggestion or idea?","['functions', 'linear-algebra', 'symmetric-matrices']"
3749237,Closed-Form solution for nested integrals of this polynomial?,"I was wondering whether there is a closed-form solution for this (nested) integral: $$
\int_{-1}^{1}\int_{t_{0}}^{1}\int_{t_{1}}^{1}...\int_{t_{a-2}}^{1}\prod_{\begin{array}{c}
i<j\\
j=\{0,..,a-1\}\\
i=\{0,..,a-1\}
\end{array}}\left(t_{i}-t_{j}\right)^{4}dt_{a-1}dt_{a-2}...dt_{0}
$$ These are the results I get for $a=2$ and $a=3$ : $a=2$ : $$
\int _{-1}^1\int _{t_0}^1(t_0-t_1)^4 dt_1 dt_0 = \frac{32}{15}
$$ $a=3$ : $$
\int _{-1}^1\int _{t_0}^1\int _{t_1}^1(t_0-t_1)^4 (t_0-t_2)^4 (t_1-t_2)^4dt_2dt_1dt_0 = \frac{8192}{33075}
$$ Is there a known closed-form solution $\forall a$ ? Edit : As noted by  @Steven Stadnicki in the comments, the function inside the integrals can also be written as $$
\prod_{\begin{array}{c}
i\ne j\\
j=\{0,..,a-1\}\\
i=\{0,..,a-1\}
\end{array}}(t_{i}-t_{j})^{2}
$$ Thanks!","['integration', 'definite-integrals', 'recurrence-relations', 'calculus', 'closed-form']"
3749253,Irreducibility of $X\times_k\overline{k}$ (Hartshorne 3.15 (a)),"I'm working on Hartshorne's exercise II.3.15 (a), namely: Let $X$ be a scheme of finite type over a field $k$ (not necessarily algebraically closed). Show that the following three conditions are equivalent: (i) $X\times_k\overline{k}$ is irreducible. (ii) $X\times_kk_s$ is irreducible (where $k_s$ is the separable closure of $k$ ) (iii) $X\times_kK$ is irreducible for every field extension $K|k$ . I've found the following solution on the internet, which I'm having a hard time trying to understand (I've selected part (ii) $\Rightarrow$ (i), which seems to be the essential one). My questions are: Why can we restrict ourselves to purely inseparable extensions $K|k$ ? What about the others? Why can we immediatly reduce to the case $X=\text{Spec}(A)$ ? Why is it relevant to consider that $\text{Spec}(A)$ is homeomorphic to $\text{Spec}(A_{\text{red}})$ ? (I know the homeomorphism exists, I just can't see why it is important here) He says that "" $A\otimes_k K$ having a zero-divisor is equivalent to a system of equations with coefficients in $k$ having a solution over $K$ "". I have no idea what this means. An element in $A\otimes_k K$ is something of the form $\sum_ia_i\otimes b_i$ and the product of things like that looks just like a big mess. How do we find this system he is talking about? Thank you!","['affine-schemes', 'algebraic-geometry', 'schemes', 'sheaf-theory']"
3749310,Evaluate $\int \:\frac{3x^5+13x^4+32x^3+8x^2-40x-75}{x^2\left(x^2+3x+5\right)^2}\:dx$,I am supposed to evaluate $$\int \:\frac{3x^5+13x^4+32x^3+8x^2-40x-75}{x^2\left(x^2+3x+5\right)^2}\:dx$$ I started using partial fractions $$3x^5+13x^4+32x^3+8x^2-40x-75=x\left(x^2+3x+5^2\right)^2A+\left(x^2+3x+5^2\right)^2B+x\left(x^2+3x+5^2\right)\left(Cx+D\right)+x^2\left(Ex+F\right)$$ I managed to get to $$\:\int \:\left(\frac{2}{x}-\frac{3}{x^2}+\frac{x+1}{x^2+3x+5}+\frac{4x}{\left(x^2+3x+5\right)^2}\right)\:dx$$ Am i on the right track? is there an easier way to simplify the original integral?,"['integration', 'indefinite-integrals']"
3749315,brownian motion unbounded variation,"I have been doing a little bit of reading regarding random processes and probability theory recently for some research I have been doing, and I have come across the claim in many places that Brownian motion cannot be treated with Riemannian integration due to the fact that it is of unbounded variation. I have been trying to find a rigorous proof for that, but I have been having a difficult time. I intuitively have the idea that since Brownian motion is considered as a continuous random walk, then it is theoretically possible for it to exceed whatever bound may be placed on it. Is this the right way of thinking of it? And can anyone produce  more rigorous proof to show this? Thanks!","['brownian-motion', 'probability-theory']"
3749325,How to derive a (first-order) differential equation from a slope field,"A first order differential equation has a slope field shown in the following. Question: a) Suggest, with reasons, the simplest first order differential equation consistent with the slope field shown. b) Suggest a possible general solution for your differential equation. So what are the things that I should look for or things that I need to keep in mind while trying to derive a differential equation from a slope field? In the first glance, I think it resembles a cubic? But that is just from observation, how can I confirm this?","['calculus', 'ordinary-differential-equations']"
3749361,Proof of Rotation Rule for a $90^\circ$ Rotation About the Origin,"I tried to prove the following rules algebraically: $90$ degree rotation. $(x,y)\to(-y,x)$ : I got 2 results at the end. However, I can't come up with a rigorous way to eliminate one of them (maybe this can be done using some mathematical definitions?). Furthermore, I wonder if it's possible to incorporate the fact that the rotation is counterclockwise into the proof itself, thus the proof will only give 1 result?","['analytic-geometry', 'algebra-precalculus', 'geometry']"
3749374,"If $g\in{G}$ and G is a group, then the map $G\rightarrow{G}$ given by $x\mapsto{gx}$ is a bijection.","Right now, I am reading Evan Chen's Napkin to study Abstract Algebra and other various topics. In lemma 1.2.5, he states: Let $G$ be a group, and pick a $g\in{G}$ . Then the map $G\rightarrow{G}$ given by $x\mapsto{gx}$ is a
bijection. to which he asks the reader to prove this lemma. As I am not familiar with this notation, I think the map $G\rightarrow{G}$ given by $x\mapsto{gx}$ is the same as a function $f$ such that $f:G\to{G}$ and $f(x)=gx$ , though I am not sure (I used Is there any difference between mapping and function? and Different arrows in set theory: $\rightarrow$ and $\mapsto$ as points of reference). Given that my assumption is right, then surjection would be proven by: Let $y=f(x)$ . Then $y=gx$ and $x=\frac{y}{g}$ , demonstrating $\forall{y}\in\mathbb{R},\exists{x}\in\mathbb{R}$ such that $y=f(x)$ and proving surjectivity. Then for injectivity, suppose $f(x_1)=f(x_2)$ . Then $gx_1=gx_2$ and thus $x_1=x_2$ , proving injectivity. Thus, since $f$ is both injective and surjective, then $f$ is bijective. Similarly, this implies the map $G\rightarrow{G}$ given by $x\mapsto{gx}$ is bijective. My main question is whether the map $G\rightarrow{G}$ given by $x\mapsto{gx}$ is the same as a function $f$ such that $f:G\to{G}$ and $f(x)=gx$ . Could you also check my proof to make sure I haven't missed any considerations (such as whether I need to consider the binary operator of G). If my assumption in the main question is correct, this is just the same as proving the function $f(x)=gx$ is bijective (something very easy to do).","['functions', 'group-theory', 'abstract-algebra', 'solution-verification']"
3749427,"Showing $X_{(n)}$ is not complete for $\theta \in [1,\infty)$ when $X_i$'s are i.i.d $\text{Unif}(0,\theta)$","I am trying to show that the order statistic $X_{(n)}$ for a set of RV $\{X_i\}_{1}^{n}$ where $X_i\overset{iid}\sim \text{Unif}(0,\theta)$ is complete when $\theta \in (0,\infty)$ but not when $\theta \in [1,\infty)$ . By completeness $E[g(X_{(n)})]=0$ iff $g(X_{(n)})=0$ a.e. $X_{(n)}\sim n\theta^{-n}X_{(n)}^{n-1}$ then if $$E[g(X_{(n)})]=\int_{0}^{\theta}g(X_{(n)})n\theta^{-n}X_{(n)}^{n-1}dX_{(n)}=0$$ this implies $$g(\theta)=0, \forall \theta \in(0,\infty)$$ Since for any $X_{(n)}$ there exists a $\theta=X_{(n)}$ one can conclude $g(X_{(n)})=0$ when the parameter space is restricted to $\theta \in [1,\infty)$ then by the statement above one can conclude $$g(\theta)=0, \forall \theta \in [1,\infty)$$ not guaranteeing that $g(X_{(n)})=0$ for $X_{(n)}\in(0,1)$ I am having trouble using this to justify non completeness.","['statistical-inference', 'statistics', 'uniform-distribution']"
3749433,"Are ""exact functors"" between triangulated categories exact?","A morphism of triangulated categories is often called ""exact"" if it preserves the shift operator and takes distinguished triangles to distinguished triangles. Are exact functors between triangulated categories necessarily exact as functors? (Ie, do they preserve finite limits and colimits?)","['triangulated-categories', 'algebraic-geometry']"
3749447,Tighter bound for the total number of possible $m$-ary tree with $n$ nodes and maximum height $h$?,"I know that the total number of possible $m$ -ary tree with $n$ nodes is \begin{align}
C_n&=\frac{1}{(m-1)n+1}{mn \choose n},
\end{align} which is the Catalan number. I want to know if I can get a tighter (in the binomial coefficient) bound for the total number of possible $m$ -ary tree with $n$ nodes if I restrict the maximum height of the tree to $h$ -- i.e., trees can be of height $h$ or less. Note: I am aware of the bijection between $m$ -ary trees and $(m-1)$ -dyck paths. From Brian M. Scott's answer , there does not seem to be a closed form for the number of bounded dyck paths. I was wondering if I could get a upper bound on that number that is tighter (in the binomial coefficient term) than the Catalan number?","['trees', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'inequality']"
3749462,Differential of Gauss map,"Let $S$ we a regular surface, differential of Gauss map is $\mathrm{d} \mathrm{N}_{\mathrm{p}}: \mathrm{T}_{\mathrm{p}}(\mathrm{S}) \rightarrow \mathrm{T}_{\mathrm{p}}(\mathrm{S})$ . To evaluate the differential at some point, we can choose a curve $\alpha(t) = x(u(t),v(t))$ on $S$ as a image of plane curve $(u(t),v(t))$ under the chart map $\textbf{x}$ .
So we have : $$\begin{aligned}
d N_{p}\left(\alpha^{\prime}(0)\right) &=d N_{p}\left(\mathbf{x}_{u} u^{\prime}(0)+\mathbf{x}_{v} v^{\prime}(0)\right) \\
&=\left.\frac{d}{d t} N(u(t), v(t))\right|_{t=0} \\
&=N_{u} u^{\prime}(0)+N_{v} v^{\prime}(0)
\end{aligned}$$ The first equality and the third one is easy, what I don't understand is the second one, we know in general $df_p(v) = \frac{d}{dt}_{t=0}(f\alpha(t))$ so for here shouldn't it be $\left.\frac{d}{d t} N(x(u(t), v(t)))\right|_{t=0}$ . This is the result from Do Carmo's differential geometry textbook page 142-143. The second question is  the book says in particular $d
   N_{p}\left(\mathbf{x}_{u}\right)=N_{u} \text { and } d
   N_{p}\left(\mathbf{x}_{v}\right)=N_{v}$ . I don't know how to get it. My idea is to choose plane curve $(u(t),v(t))$ vary only along the u-axis so $N_u =dN_p(x_u)$ in this case.Is my interpretation right?","['multivariable-calculus', 'differential-geometry', 'surfaces', 'real-analysis']"
3749482,Intuition for why we can apply complex analysis to solving 2D cases in applied science problems,"In fluid dynamics and elasticity theory (and probably many other theories Im not familiar with) , when we consider a 2D ""flat"" case, we summon complex analysis for help. It usually starts with introducing some potentials, partial derivatives of which are equal to something we are interested in. Even though I've completed a course of complex analysis, to me all of this seems like magic. I don't have any intuition or know any intrinsic reasons for why this helps us solve ""flat"" problems. What should I read up on, or draw my attention to, in order to gain some intuition for why this sort of approach is used, why it is needed, and why it works?","['complex-analysis', 'applications']"
3749596,Show that $\mathbb R$ satisfies the second axiom of countability,"In my general topology exercise I have to prove the following: A topological space $(X,\tau)$ is said to satisfy the second axiom of countability if there exists a basis $B$ for $\tau$ , where $B$ consists of only a countable number of sets. Show that $\mathbb R$ satisfies the second axiom of countability And then they give the tip ""use exercise 3"", which is: Let $B$ be the collection of all open intervals $(a,b)$ in $\mathbb R$ , with $a < b$ and $a,b \in \mathbb Q$ . Prove that $B$ is a basis for the euclidean topology. I already proved exercise 3, and I think that the tip basically says to use the basis from exercise 3. So now I have to prove that $B$ has a countable number of sets. My approach: We know that the set $\mathbb Q$ is countable so, let $U_\alpha=\{(\alpha,n), \forall n \in \mathbb{Q}\}$ . Then $U_\alpha \sim \mathbb Q$ . So every $U_\alpha$ is countable. Now, the family of sets $\mathcal F= \{U_\alpha, \forall \alpha \in \mathbb Q \}$ is also countable because $\mathcal F \sim \mathbb Q$ , So $B = \bigcup \mathcal F$ is the union of a countable number of sets. My question is, is this proof right and is this the most correct way to prove it?","['elementary-set-theory', 'general-topology', 'solution-verification']"
3749619,Showing that $\sin^2x\cdot\sin^22x\cdot\sin^24x\cdot\sin^28x\cdots\sin^22^nx\leq\frac{3^n}{4^n}$,"Show that $$\sin^2x\cdot\sin^22x\cdot\sin^24x\cdot\sin^28x\cdots\sin^22^nx\leq\frac{3^n}{4^n}$$ I understand the result of an arithmetic sequence $(\sin1^\circ)(\sin3^\circ)(\sin5^\circ)…(\sin89^\circ)$ , how about the geometric sequence case?","['algebra-precalculus', 'geometric-inequalities', 'trigonometry', 'inequality']"
3749717,Verifying the period of $f(x)=\sin(x)+\cos(x/2)$,"It seems clear from the graph of $f(x)=\sin(x)+\cos(x/2)$ that the period $p$ of the function is equal to $4\pi$ . To verify that $4\pi$ is a period of $f(x)$ , note that \begin{align}
\sin(x + 4\pi) + \cos\left(\frac{x + 4\pi}{2}\right) & =\sin(x)\cos(4\pi)+\cos(x)\sin(4\pi)+\cos(x/2)\cos(4\pi/2)-\sin(x/2)\sin(4\pi/2) \\
& =\sin(x)+\cos(x/2)
\end{align} Thus $4\pi$ is indeed a period of $f$ .  My question is, how would one go about trying to prove that $4\pi$ is the smallest $p>0$ such that $f(x+p)=f(x)$ ?","['periodic-functions', 'calculus', 'trigonometry']"
3749740,Evaluate the limit $\lim\sqrt[n]{\frac{1}{n!}\sum(m^m)}$,"In some problem, I need to evaluate this limit: $$\lim_{n\rightarrow \infty}\sqrt[n]{\frac{1}{n!}\sum^n_{m=0}(m^m)}.$$ I know about Taylor series and that kind of stuff. I'm not sure where to start, maybe Stirling but after using it I still could not solve it. Any help will be appreciated. Using Stirling's equivalence, I get to: $$\lim_{n\rightarrow \infty}\frac{e}{n}\sqrt[n]{\frac{\sum^n_{m=0}(m^m)}{\sqrt{2\pi n}}}$$ I don't know if this is useful anyway.","['limits', 'sequences-and-series']"
3749747,"For an infinite sequence of functions $\Bbb{R}\to\Bbb{R}$, each function is a composition of a certain finite set of functions $\Bbb{R}\to\Bbb{R}$.","Given an infinite sequence of functions $\{g_1, g_2, \ldots, g_n, \ldots\}$ where $ g_n : \Bbb R \to \Bbb R$ prove there's a finite set of functions $ \{ f_1, f_2, \ldots, f_M \} $ such that any $ g_n $ can be represented as a composition of $ f_m $ 's. Honestly, not sure even how to approach this. The intuition is that if the infinite sequence of functions is not defined using finite set of functions and composition then the sequence definition would be infinite itself, but I don't know how to formalize that.","['sequence-of-function', 'functions', 'analysis', 'function-and-relation-composition']"
3749756,Is there a linear dynamical system on the sphere with a two-point attractor?,"Let $N=(1,0,0)$ denote the North Pole of $\mathbb S^2$ . Therefore, $-N$ denotes the South Pole. Question . Is there a skew-symmetric $M\in\mathbb R^{3\times 3}$ such that, letting $u(t,P)\in\mathbb S^2$ denote the solution to $$\frac{\mathrm{d}u}{\mathrm{d}t}= Mu,\qquad u(0)=P,$$ for an arbitrary $P\in\mathbb S^2$ , it holds that either $u(t,P)\to N$ or $u(t,P)\to -N$ as $t\to \infty$ ? The matrix $M$ must satisfy the skew-symmetry $M^T=-M$ in order for $u(t, P)$ to remain on $\mathbb S^2$ for all $t>0$ . In particular, the trace of $M$ must vanish. My intuition is that such a dynamical system cannot exist. Indeed, since $\operatorname*{tr}(M)=0$ , the dynamical system $u(t, \cdot)$ should be ""area-preserving"", in some sense. And this shouldn't be compatible with the requirement that, asymptotically, everything collapses to a couple of points, which is a set with zero area. However, if my intuition is wrong, I would be very happy to see an example.","['spheres', 'linear-algebra', 'ordinary-differential-equations', 'dynamical-systems']"
3749765,Which linear maps on a finite field are field multiplications?,"I am mainly interested in the fields $\mathrm{GF}(2^n)$ , but the question can be asked for any prime. We can write out each element $x\in\mathrm{GF}(2^n)$ in base $2$ and note that its additive group combined with multiplication by elements of $\mathrm{GF}(2)$ is isomorphic to the vector space $\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ . Let $v:\mathrm{GF}(2^n)\to\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ stand for this ""vectorisation"" operation. Linear maps on $\left(\mathbb{Z}/(2\mathbb{Z})\right)^n$ may be represented by $n\times n$ , $\{0,1\}$ -valued matrices. Since field multiplication is linear for any $x\in\mathrm{GF}(2^n)$ there is a matrix $M_x$ such that for all $y\in\mathrm{GF}(2^n)$ \begin{align}
    M_x v(y) = v(x\cdot y),
\end{align} There are, however $2^{n\times n}$ matrices and only $2^{n}$ field elements, so the question is what can we say about the structure of the set of matrices $\{M_x \mid x\in \mathrm{GF}(2^n)\}$ as a subset of the full set of matrices? Loosely speaking - if I give you a matrix then how can you tell if it represents a field element?","['finite-fields', 'abstract-algebra', 'linear-algebra', 'vector-spaces']"
3749783,Solve for $x$ in $\sin^{-1}(1-x)-2\sin^{-1}x =\frac{\pi}{2}$,"Let $x=\sin y$ $$\sin^{-1}(1-\sin y)-2\sin^{-1}\sin y=\frac{\pi}{2}$$ $$\sin^{-1}(1-\sin y)-2y=\frac{\pi}{2}$$ $$1-\sin y =\sin (\frac{\pi}{2}+2y)$$ $$1-\cos2y=\sin y$$ $$\sin y(2\sin y-1)=0$$ $$x=0,~ \frac 12$$ Clearly, $x=\frac 12$ isn’t correct, because it doesn’t satisfy the original expression Why was an extraneous root obtained in this solution? I want to know the reason behind it.","['trigonometry', 'inverse-function']"
3749795,Finding the limit of $\mathbb{E}[\theta^n]/\mathbb{E}[\theta^{n-1}]$,"Let $\theta$ denote a smoothly distributed random variable with support $[0, 1]$ . I am trying to evaluate $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]}$$ I suspect, but cannot show, that the limit equals $1$ . Does anyone know how to do this? My attempts so far: Since $\theta \in [0, 1]$ , it seems reasonably clear that both $\mathbb{E}[\theta^n] \rightarrow 0$ and $\mathbb{E}[\theta^{n-1}]  \rightarrow 0$ as $n \rightarrow \infty$ (we are raising numbers that are less than $1$ to ever higher powers). Thus, we can apply L'Hopital's rule to find that $$ \lim_{n \rightarrow \infty} \frac{\mathbb{E}[\theta^n]}{\mathbb{E}[\theta^{n-1}]} \equiv \lim_{n \rightarrow \infty} \frac{\int_0^1 \theta^nf(\theta)d\theta}{\int_0^1 \theta^{n-1}f(\theta)d\theta} =  \lim_{n \rightarrow \infty} \frac{\int_0^1 \ln(\theta)\theta^nf(\theta)d\theta}{\int_0^1 \ln(\theta)\theta^{n-1}f(\theta)d\theta}$$ I am a bit unclear, however, how to proceed from this point (or whether better approaches are available).","['expected-value', 'limits', 'probability']"
3749804,"If $a_{n+1}=2a_n −n^2+n$ Define a sequence $a_n$ that satisfy the recurrence relation as described above, with $a_1 = 3$","If $$a_{n+1}=2a_n −n^2+n$$ Define a sequence $a_n$ that satisfy the recurrence relation as described above, with $a_1 = 3$ Find the value of $$\dfrac{ |a_{20} - a_{15} | }{18133} $$ Attempt First  evaluate $a_{0}$ $$a_{1} = 2a_{0} \Rightarrow a_{0}= \frac{3}{2}$$ Then, use Z-transform: $$a_{n+1} - 2a_{n} + n^2 - n = 0$$ $$z(\mathbf{A}(z)-a_{0}) - 2\mathbf{A}(z) + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{(z-1)^2} = 0$$ $$\Rightarrow \mathbf{A}(z) = \dfrac{z(3z^3 -9z^2 + 9z - 7)}{2(z-2)(z-1)^3}$$ ​ $$\Rightarrow \mathbf{A}(z) = \dfrac{2z}{z-1} + \dfrac{z}{(z-1)^2} + \dfrac{z(z+1)}{(z-1)^3} - \dfrac{z}{2(z-2)}$$ The inverse of the Z-transform will be: $$\boxed{a_{n} = 2 + n + n^2 - 2^{n-1}}$$ ​ Now: $$a_{20} = 422-2^{19}$$ $$a_{15} = 242-2^{14}$$ Is it Correct?? Any other precise solution will be highly appreciated","['linear-algebra', 'z-transform', 'recurrence-relations']"
3749813,In how many ways $MISSISSISSISSIPI$ can be arranged that no 2 $S$'s > aren't aside,"In how many ways $MISSISSISSISSIPI$ can be arranged that no 2 $S$ 's
aren't aside What I understood is I need them in this shape 1-MSISISISISISISPS 2-SMSPISISISISISIS I thought of assuming $M,P=I$ then calculate them as a binary sequence of 1's and 0's
such as 101010101010, $S=1$ , $I=0$ so $C(16,8)$ then we multiply by $2!$ for M,P But I'm not sure about this approach","['combinatorics', 'discrete-mathematics']"
3749823,Properties of functions of mean zero,"Let $f,g: \mathbb{R} \longrightarrow \mathbb{R}$ be differentiable functions and $a<b$ such that $$\frac{1}{b-a}\int_{a}^{b}f(x)\;dx=0 \quad \text{and} \quad \frac{1}{b-a}\int_{a}^{b}g(x)\;dx=0 \tag{1}.$$ So, I think that I can conclude that $$\int_{a}^{b}f'(x)\;dx=0 \tag{2}$$ Moreover, I can conclude that $$\int_{a}^{b}f'(x)\;dx=0 \Rightarrow f(b)-f(a)=0? \tag{3}$$ And $$g(b)\cdot f'(b)-g(a)\cdot f'(a)=0? \tag{4}$$ I ask this because I would like to conclude that $$g(x)\cdot f'(x)\Bigg|_{a}^{b} -\int_{a}^{b}f'(x)g'(x)\;dx=-\int_{a}^{b}f'(x)g'(x)\;dx. \tag{5}$$ These statements are in general true?","['integration', 'definite-integrals', 'derivatives', 'real-analysis']"
3749855,"Showing that $a$, $b$, $c$, $d$ are in geometric progression iff $(a^2+b^2+c^2)(b^2+c^2+d^2)=(ab+bc+cd)^2$","If the real numbers $a$ , $b$ , $c$ , $d$ are in geometric progression, show that $$
\left(a^{2}+b^{2}+c^{2}\right)\left(b^{2}+c^{2}+d^{2}\right)=(a b+b c+c d)^{2}
$$ Prove that the converse also holds. The simplest way I could think of is making/assuming a general G.P Let $r$ be the common ratio and $a$ be the first term, then $$b=a r, c=a r^{2}, d=a r^{3}$$ after multiplying a lot of terms, many times, (skipping significant steps here so it is more readable) $\begin{aligned} \mathrm{LHS} &=\left(a^{2}+b^{2}+c^{2}\right)\left(b^{2}+c^{2}+d^{2}\right)=\left(a^{2}+a^{2} r^{2}+a^{2} r^{4}\right)\left(a^{2} r^{2}+a^{2} r^{4}+a^{2} r^{6}\right) \\ &=a^{2}\left(1+r^{2}+r^{4}\right) \cdot a^{2} r^{2}\left(1+r^{2}+r^{4}\right) \\ &=a^{4} r^{2}\left(1+r^{2}+r^{4}\right)^{2} \\ &=\left(a^{2} r+a^{2} r^{3}+a^{2} r^{5}\right)^{2} \\ &=\left(a \cdot a r+a r \cdot a r^{2}+a r^{2} \cdot a r^{3}\right)^{2} \\ &=(a b+b c+c d)^{2}=R H S \end{aligned}$ Once after expanding each term, and getting no where, I realised that I had already gotten the answer, all I had to do was take the terms inside the and then I had the RHS. But it all was so tedious and took multiple attempts. Though, this method guarantees that converse holds, Can it be done more elegantly? Edit: I am in highschool (and just a little more interested in math) So I don't have knowledge about the much spoken Cauchy's identity.","['algebra-precalculus', 'geometric-progressions', 'sequences-and-series']"
3749872,"Is $22/7$ the closest to $\pi$, among fractions of denominator at most $50$?","Is $22/7$ the closest to $\pi$ , among fractions of denominator at most $50$ ? I am currently studying continued fractions, while I know that for all denominators at most $Q_n$ , $\frac{P_n}{Q_n}$ is the closed approximation. But what about the denominators between $Q_n$ and $Q_{n+1}$ ?","['number-theory', 'continued-fractions', 'pi', 'diophantine-approximation']"
3749885,differential of normal vector,"For regular surface $S\subset \mathbb{R}^3$ ,we have differential of normal vector $dN_p:T_p(S) \to T_p(S)$ . What's the size of linear map $dN_p$ ?I was a bit confuse here. Since we know dimension of $T_p(S)$ is 2,so $dN_p$ should be a $2\times 2$ matrix,but in another view,each point in $T_p(S)$ is tangent of curve on $S$ ,so it's $v\in \mathbb{R}^3$ so $dN_p$ is a $3\times 3 $ matrix. Can someone explain a bit more clear?","['multivariable-calculus', 'differential-geometry', 'surfaces', 'real-analysis']"
3749900,Bijection Cancellation rule for cartesian product,"Suppose $A$ , $B$ and $C$ are sets, and that there is a bijection between $C \times A$ and $C \times B$ . Is there necessarily a bijection between $A$ and $B$ ? I know this should work for finite sets - you can use a size argument to demonstrate $A$ and $B$ have the same size, so there's a bijection between them. And I know that this works the other way around - if there's a bijection between $A$ and $B$ , then for any set $C$ there's a bijection between $C \times A$ and $C \times B$ . But is this true in general?",['elementary-set-theory']
3749911,Restricting a function in the disk algebra,"Let $A$ be the disk algebra, i.e.  continuous functions on the closed unit disk in $\Bbb{C}$ that are analytic on the interior of the disk. By the maximum-modulus theorem, we have an isometric morphism of algebras: $$\varphi: A \to C(S^1): f \mapsto f\vert_{S^1}$$ The book I read claims that $\varphi(A)$ is contained in the closed subalgebra $B$ of $C(S^1)$ generated by $1$ and $z$ . Why is this the case? My intuition is that $f \in A$ can be written as $f(z) = \sum_n a_n z^n$ on the interior of the disk? If this also holds for $|z| = 1$ what I want to prove becomes obvious but I'm not sure this holds. Also, do we have $B = \varphi(A)$ ? Or only the inclusion $\varphi(A) \subseteq B?$ Thanks for any help!","['complex-analysis', 'banach-algebras', 'functional-analysis']"
3749934,Find the value of $\sum_{r=0}^{\infty} \tan^{-1}(\frac{1}{1+r+r^2})$,"The given expression can be written as $$\tan^{-1}(\frac{r+1+(-r)}{1-(-r)(r+1)})$$ $$=\tan^{-1}(r+1)-\tan^{-1}(r)$$ Therefore $$\sum =\tan^{-1}(1)-\tan^{-1}(0)+\tan^{-1}(2)....$$ Since it goes on to infinity, all the terms except $-\tan^{-1}(0)$ get cancelled. So the answer should be $0$ or $-\pi$ . But the right answer is $\frac{\pi}{2}$ . What’s wrong with this solution? I know how to get $\frac{\pi}{2}$ , I figured out an alternate for it, but I want to know what went wrong here.","['trigonometry', 'inverse-function']"
3749951,What we can do if the inverse of our function can not be determined explicitly,"My question is a little bit general: what we can do if the inverse of our function can not be founded explicitly? For example, let consider the function $$f(x)=x \cos (x)\quad\quad \quad\mbox{ for }\;x\in [0, \frac12]$$ this function is clearly invertible on $[0, \frac12]$ , but we can not have an explicit formula of the inverse (you can try !). Let $y \in f([0, \frac12])$ ; Is there any approximation or something to do to have an expression of $y$ such that $$x=f^{-1}(y)$$","['trigonometry', 'functions', 'real-analysis']"
3749981,Find an angle between a triangle and a plane,"The hypotenuse $AB$ of triangle $ABC$ lies in plane $Q$ . Sides $AC$ and $BC $ , respectively, create angles $\alpha$ and $\beta$ towards the plane Q (meaning they are tilted towards the plane $Q$ with such angles). Find the angle between plane $Q$ and the plane of the triangle, given $\sin(\alpha) = \frac{1}{3} $ and $\sin(\beta)=\frac{\sqrt5}{6}$ . I'm really struggling with these kinds of problems and I can't seem to find any material in English that covers this topic. Only videos I found about planes use  normal vectors and equation of the plane, which is not necessary for this. The picture wasn't given but Here's my interpretation: let $CK$ be the perpendicular line from point $C$ to plane $Q$ . $CD$ is the height of triangle $ABC$ . What I'm struggling to understand is what will the dihedral angle be in this case? Well, I know that the angle between two planes is the angle between two perpendicular lines of such planes. One of which must be $CD$ , but what will the other line be?  Is it $KD$ ? How can I know for sure that $KD$ is a perpendicular line? Anyway, I don't think I'm understanding the problem clearly. If someone can provide a graphical solution, i'll be very thankful.","['euclidean-geometry', 'proof-writing', 'geometry', 'solid-geometry', 'triangles']"
3749995,Integer solutions of $2a+2b-ab\gt 0$,"Let $a\in\mathbb{N}_{\ge 3}$ and $b\in\mathbb{N}_{\ge 3}$ . What are the solutions of the Diophantine inequality $$2a+2b-ab\gt 0?$$ By guessing, I found 5 solutions: $$\text{1)}\, a=3,\, b=3$$ $$\text{2)}\, a=3,\, b=4$$ $$\text{3)}\, a=4,\, b=3$$ $$\text{4)}\, a=5,\, b=3$$ $$\text{5)}\, a=3,\, b=5.$$ Are these all the solutions? How could I find all the solutions rigorously?","['elementary-number-theory', 'inequality', 'discrete-mathematics', 'diophantine-equations']"
3750008,"Existence of a prime in $(\phi(n), n]$","The question is: for any $n\geq2$ , is there always a prime $p$ satifying $\varphi(n)<p\leq n$ ? Here $\varphi(n)$ is the Euler totient function. We know that there is always a prime between $n-O(n^\theta)$ and $n$ , where $\theta$ can be $0.525$ ( Wiki: Prime gap ). Under Riemann hypothesis, one can improve this bound to $O(\sqrt n\log^2n)$ . But on the other hand, there are infinite many $n$ such that $\phi(n)\geq n-C\sqrt n$ for some constant $C$ (just choose $n=p(p+k)$ where $p$ and $p+k$ are both prime; for some $k$ these $p$ are infinite). So these upper bound for prime gap don't help. So can we prove this propsition, or give a counterexample? (or give a evidence to explan why is this hard to prove, maybe?) (The propsition is equivalent to: if $\varphi(n)>\varphi(k)$ for all $1\leq k<n$ , then $n$ is prime)","['number-theory', 'analytic-number-theory', 'totient-function', 'prime-gaps', 'prime-numbers']"
3750026,"Counter example to: If $F$ is a conservative field in region $A$ and $B$, so $F$ is conservative in $A \cup B$","I am looking for a counter example and am explanation for why the following statement is false: If $F$ is a conservative field in region $A$ and $B$ , so $F$ is conservative in $A \cup B$ . And what about $A \cap B$ (is it true in that case)?","['multivariable-calculus', 'calculus']"
3750048,Does the inverse of a matrix with complex conjugate rows have complex conjugate columns?,Let $A \in \mathbb{C}^{2n \times 2n}$ be an invertible matrix which consists of $n$ rows and their $n$ complex conjugates (in any random order). Is it true that $A^{-1}$ will consist of $n$ columns and their $n$ complex conjugates?,"['matrices', 'linear-algebra', 'complex-numbers']"
3750061,How to sum $\sum_{n=0}^{\infty} \frac{(2n)!!}{(2n+1)!!} (2n+2)^{-2}$,"While solving the following interesting integral (*) in MSE: How to evaluate $\int _0^{\frac{\pi }{2}}x\ln \left(\sin \left(x\right)\right)\:dx$ I took another route and encountered the sum $$S=\sum_{n=0}^{\infty} \frac{(2n)!!}{(2n+1)!!} (2n+2)^{-2} ~~~~(1)$$ Using Walli's formula $S$ can also be expressed as $$S=\sum_{n=0}^{\infty} \int_{0}^{\pi/2}\frac{\cos^{2n+1} \theta}{(2n+2)^2}~~~~~~~~~~(2)$$ Mathematica gives both (1) and (2) in terms of the hypergeometric series (fumction) as $$S=\frac{1}{4} ~_4F_3~[\{1,1,1,1\},\{3/2,2,2\};1]~~~~(3)$$ Interestingly, the said integral (*) is doable otherwise as $$S=\frac{1}{16} [\pi^2 \ln 4-7 \zeta(3)]=0.329236..~~~~(4)$$ I have checked numerically that all of (1-4) are identical. The question is how to show by hand that $S$ in (1) and (2) equals (4).","['integration', 'riemann-zeta', 'definite-integrals', 'sequences-and-series']"
3750080,Can the Killing form induce an endomorphism in a Lie algebra?,"Let $\mathfrak{g}$ be a Lie algebra over $\mathbb{R}$ of the finite dimensional Lie group $G$ ; let $\langle \cdot , \cdot \rangle$ be a left-invariant Riemannian metric on $G$ . If $B:\mathfrak{g}\times \mathfrak{g}\to \mathbb{R}$ is the Cartan-Killing form $(X,Y)\mapsto \text{Tr}( \text{ ad}_X \circ \text{ad}_Y)$ . Is it true that there is a symmetric endomorphism $\phi$ on $\mathfrak{g}$ such that for every $X\in \mathfrak{g}$ we have $\langle X,X \rangle=B(\phi(X),X)$ ?.","['lie-algebras', 'lie-groups', 'differential-geometry']"
3750108,What is the RN derivative of infinite product measure?,"Suppose $\mu_k$ and $\nu_k$ , $k=1,2,...$ are sigma-finite measures on spaces $(S_k,\mathcal F_k)$ such that $\nu_k<<\mu_k$ for each $k$ . Let $f_k=\dfrac{d\nu_k}{d\mu_k}$ for each $k$ . Then is it true that $\nu:=\prod_{k=1}^\infty \nu_k<<\prod_{k=1}^\infty \mu_k:=\mu$ with $\dfrac{d\nu}{d\mu}(s_1,s_2,...)=\prod_{k=1}^\infty f_k(s_k)$ ? The result is true when you have a finite product.","['measure-theory', 'radon-nikodym']"
3750141,"Transient, Positive Recurrent, or Null Recurrent","\begin{align}
P_{1, 2^i} = 2^{-i} ~~&\textrm{and} ~~P_{i+1, i}= 1, ~i\in \mathbb Z^+\\P_{i, 1} = 1/(i+1) ~~&\textrm{and}~~ P_{i, i+1} = I/(I+1), ~i \in \mathbb Z^+.
\end{align} Given the following sets of transition probabilities, how do I determine whether the irreducible DTMC is transient, positive recurrent or null recurrent? So I understand that the state is positive recurrent iff its mean recurrent time < infinity, and null recurrent iff its mean recurrent time = infinity. Although how do you determine this given the transition probabilities?","['statistics', 'markov-chains', 'probability']"
3750157,Geometric Interpretation of Projective Cover,"The projective cover of a module $V$ is defined to be an epimorphism $p\colon P\rightarrow V$ where $P$ is a projective module and $\ker(p)$ is superfluous.
Now I wonder if there is any interpretation (or an analogue) of this notion in terms of algebraic geometry?","['algebraic-geometry', 'abstract-algebra', 'representation-theory']"
3750161,Dominated convergence theorem and Cauchy's integral formula,"Let $U\subseteq \mathbb{C}$ be open and $\bar B(a,r) \subseteq U$ . Let $\gamma(t) =a+ re^{it}$ with $t \in [0,1]$ be the boundary path of $B(a,r)$ . By Cauchy's integral formula $f(w) = \frac{1}{2 \pi i}\int_{\gamma} \frac{f(z)}{(z-w)} dz$ ,  where $w \in B(a,r)$ . I want to prove $\frac{d f(w)}{dw} = \frac{1}{2 \pi i} \int_{\gamma} \frac{f(z)}{(z-w)^2}dz$ . The usual argument is to interchange the order of differentiation and integration and this is justified by uniform convergence. Is it possible to justify this interchange with the DCT? My attempt: For DCT to apply, I need to check that $\frac{d}{dw}(\frac{f(\gamma(t))}{\gamma(t)-w}\gamma'(t)) = \frac{d}{dw}(\frac{f(re^{it})}{re^{it}-w}ire^{it}) = \frac{f(re^{it})}{(re^{it}-w)^2}ire^{it} $ is dominated by some function which is integrable over $[0,1]$ . Because $f$ is continous over a compact it is bounded by some $M$ and therefore $\frac{M}{(r+ |w-a|)^2}$ should be the desired dominating function. For the bounty: I am happy with the accepted answer. I would just like to know if my attempt is wrong and if it is necessary to consider the real and imaginary parts of $w$ . Many thanks!","['integration', 'measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'complex-analysis']"
3750186,Is there a good (non-calculational) reason for the formula $|v \times w|^2 + (v \cdot w)^2 = (|v||w|)^2$?,"If $v$ and $w$ are 3D vectors, we have the formula $$|v \times w|^2 + (v \cdot w)^2 = (|v||w|)^2.$$ This formula is used to give the magnitude formula $|v \times w| = |v| |w| \sin(\theta)$ . But the only explanation I have ever seen for it is calculational — write down both sides. It's very unsatisfying, in my view. Is there a better reason for this formula than just ""bash it out""? I would welcome both geometric answers or algebraic answers, but they shouldn't secretly use the above formula or the magnitude formula itself.","['cross-product', 'geometry', 'multivariable-calculus', 'vector-analysis', 'exterior-algebra']"
3750194,"For normal subgroups $A,B,C$ in $G$ : If $A\subseteq B$ , $A\cap C=B \cap C$ and $AC=BC$ then $A=B$","The origin of this is question 2.54 in J Rotman - The Theory of Groups. Some preliminary definitions are as follows :
Assume $N$ is a group then The group operation is written multiplicatively so that $1$ is taken to be the identity of $N$ and the inverse of $a\in N$ is $a^{-1}$ $H$ is a normal subgroup of $N$ iff for all $n\in N$ : $nHn^{-1}\subseteq H$ If $S$ and $T$ are non-empty subsets of $N$ then $ST = \{st\in N: s\in S \ \text{and} \ t\in T\}$ A seemingly relevant theorem goes as follows : If $S,T$ are normal subgroups of a group $G$ then $ST=TS$ $ST$ is a subgroup The question is then: Let $A,B$ and $C$ be normal subgroups of a group $G$ with $A\subseteq B$ . If $A\cap C = B \cap C$ and $AC=BC$ then $A=B$ My attempt: Since already $A\subseteq B$ it is enough to show that $B \subseteq A$ to conclude that $A=B$ . Let $b\in B$ : If $b\in C$ then $$b\in B\cap C = A\cap C$$ so $b\in A$ which would entail that $B\subseteq A$ . We therefore assume that $b \notin C$ . Here is where I get stuck For $c\in C$ we have $bc\in BC$ . By the ""relevent"" theorem above: $BC=AC=CA$ . Therefore there exists a $a\in A$ and $c'\in C$ such that $bc=c'a$ . Hence, since $BC=CA$ is a subgroup, $b=c'ac^{-1}\in CA$ . I would have liked to conclude that $c'ac^{-1}\in A$ by the fact that $A$ is normal, however I am not sure that $c=c'$ . Another observation is that in $b=c'ac^{-1}$ we can not have $a\in C$ since then $b\in C$ by closure. The fact that $1b\in BC=CA$ only changes the above to $b=c'a$ but then again is $c'a\in A$ How can I proceed? a hint would be much appreciated!","['elementary-set-theory', 'group-theory']"
3750240,"$f\in L^2[0,1]$ iff $f\in L^1[0,1]$ and there is nondecreasing $g$ with $|\int_a^b f(x)dx|^2 \leq (g(b)-g(a))(b-a)$ for $0\leq a\leq b\leq 1$","Let $f:[0,1]\to \Bbb C$ be measurable. I am trying to show that $f\in L^2$ iff $f\in L^1$ and there is a nondecreasing function $g:[0,1]\to \Bbb R$ such that $$ \left\lvert \int_a^b f(x)~dx \right\rvert^2 \leq (g(b)-g(a))(b-a)$$ for $0\leq a\leq b\leq 1$ . One implication is easy: we just let $g(x)=\int_0^x |f(t)|^2~dt$ and apply Holder's inequality. But I can't show the other implication. Any hints?","['integration', 'lp-spaces', 'lebesgue-integral', 'real-analysis']"
3750248,Splitting a continuous monotonically-increasing function $f(x)$ as $h(x)+h(x+\epsilon) = f(x)$,"Given a continuous monotonically-increasing function $f: [0,1]\to \mathbb{R}$ and a parameter $\epsilon>0$ , does there exist a continuous monotonically-increasing function $h$ such that, for all $x\in[0,1]$ : $$h(x)+h(x+\epsilon) = f(x)?$$ If $\epsilon=0$ then $h(x)=f(x)/2$ . But when $\epsilon>0$ , the function $f$ should be split
into two parts with a ""phase difference"" of $\epsilon$ . It seems easy, but I could not find the formula for this $h$ .","['continuity', 'monotone-functions', 'real-analysis']"
3750275,Proof that any number is equal to $1$,"Before I embark on this bizzare proof, I will quickly evaluate the following infinite square root; this will aid us in future calculations and working:
Consider $$x=\sqrt{2+\sqrt{{2}+\sqrt{{2}+\sqrt{{2}...}}}}$$ $$x^2-2=\sqrt{2+\sqrt{{2}+\sqrt{{2}+\sqrt{{2}...}}}}=x \implies x^2-x-2=0\implies x=2$$ as $x>0$ . Now for the proof:
I was attempting some different infinite expansions/square roots when trying to solve another question of mine ( Evaluate $\sqrt{x+\sqrt{{x^2}+\sqrt{{x^3}+\sqrt{{x^4}...}}}}$ ) and I came across this: $$x+\frac{1}{x}=\sqrt{(x+\frac{1}{x})^2}=\sqrt{2+x^2+\frac{1}{x^2}}=\sqrt{2+\sqrt{(x^2+\frac{1}{x^2}}})^2=\sqrt{2+\sqrt{2+x^4+\frac{1}{x^4}}}=\sqrt{2+\sqrt{2+\sqrt{(x^4+\frac{1}{x^4})^2}}}=\sqrt{2+\sqrt{2+\sqrt{2+x^8+\frac{1}{x^8}}}}=\sqrt{2+\sqrt{{2}+\sqrt{{2}+\sqrt{{2}...}}}}=2$$ if you keep on applying this and using the result found at the start of the question. So we have that for any real number $x$ that $$x+\frac{1}{x}=2\implies x^2-2x+1=0\implies (x-1)^2=0$$ so we finally have: $$x=1$$ Where have I gone wrong, for surely this cannot be correct?","['nested-radicals', 'convergence-divergence', 'fake-proofs', 'sequences-and-series']"
3750284,"Krull dimension of $\mathbb{C}[x,y,z,w]/(xw-yz)$","I have the following exercise: Consider the rings $A:=\mathbb{C}[x,y,w,z]/(xw-yz)$ and $B:=A/(\bar{x}, \bar{y})$ . (i) Calculate the Krull dimensions of $A$ and $B$ . (ii) Consider the prime ideal $P=(\bar{w}, \bar{z})\subset A$ and let $Q$ be its image in $B$ . Calculate the height of $P$ and the height of $Q$ . (i) Dimension of A I know that $\mathbb{C}[x,y,w,z]$ has Krull dimension $=4$ , because $\mathbb{C}$ is a field. So I expect dim $A<4$ (upper bound). The first problem is that $A$ is a integral domain...so can I be sure that dim $A<4$ and not dim $A\leq 4$ ?
I think no: if $A$ had been a integral domain, a chain in $\mathbb{C}[x,y,w,z]$ could always have extended to a chain in $A$ by addition of $(0)$ . However, if really dim $A<4$ , I would like to find a chain of prime ideals of $A$ with lenght 3. In this case a prime ideal of A has height $=3$ and by definition of Krull dimension, dim $A\geq3$ (lower bound) so I can conclude dim $A=3$ . (Though this is just a supposition, I'm not really sure that dim $A=3$ ). But I can't find the ideals that form the chain! Dimension of B Because of $(0)\subset(\bar{x})\subset(\bar{x,}\bar{y})$ , can I immediately conclude dim $B=2$ ? I'm not sure...Can dim $B$ not depend on dim $A$ ? (ii) Height of P $A$ is a integral domain, so we have $(0)\subset(\bar{w})\subset(\bar{w},\bar{z})=P$ and height of $P$ is 2, isn't it? Height of Q I don't know how to calculate its height. Perhaps because $B$ is obtained by quoting $A$ with $(\bar{x},\bar{y})$ , instead $P$ is generated by the two remaining $\bar{z},\bar{w}$ ,its image in $B$ has the same height. If you can help me, thank you so much.","['algebraic-geometry', 'abstract-algebra', 'krull-dimension']"
3750294,Simple proof that the Airy function decays faster than any polynomial,"My question concerns the Airy function $\mbox{Ai} (\cdot)$ , which is the solution of the Airy equation $$y'' = x y$$ that decays exponentially as $x \to \infty$ . I have read about how the asymptotic expression for $\mbox{Ai} (\cdot)$ is obtained. Comparing the Airy equation above with the much simpler $y'' = Ay$ (for constant $A>0$ ), and its decreasing solution - namely $y(x) = e^{-\sqrt{A}x}$ , I have an intuition for why Ai $(\cdot)$ should decay exponentially as $x \rightarrow \infty$ . So my question is: Without getting into the asymptotic analysis, is there a more direct way to say that Ai $(\cdot) \rightarrow 0$ as $x \rightarrow \infty$ faster than any polynomial? I am expecting that the role of $f(x) = x$ in the Airy equation is just that of a function which increases to $\infty$ as $x \rightarrow \infty$ . So a related question is: Can we make a similar conclusion for a decreasing solution of \begin{align*}
y''(x) = f(x) y(x)\text{ and }y(0)=1,
\end{align*} where $f>0$ is an increasing function such that $\lim_{x \to \infty} f(x) = \infty$ ? Thanks!","['analysis', 'ordinary-differential-equations', 'real-analysis']"
3750304,Proof of Stone's Theorem on unitary groups,"I dont understand a particular step in the proof of Stone's Theorem [ B.C. Hall, ""Quantum Theory for Mathematicians"",p.210-213]. Let me state the Theorem and explain where I got stuck. Stone's Theorem : Suppose $U(\cdot)$ is a strongly continuous one-parameter unitary group on a Hilbert space $H$ . Then the infinitesimal generator of $U(\cdot)$ is densely defined and self adjoint and $U(t)= e^{itA}$ for all $t\in\mathbb{R}$ . Proof: After proving that $A$ is densely defined one shows that $A$ is symmetric and essentially self adjoint. Then one defines $V(t):=e^{it\bar{A}}$ , where $\bar{A}$ is the closure of $A$ . Now it suffices to show that for an arbitrary $\psi\in Dom(A)$ the function $w(t):= U(t)\psi-V(t)\psi=0$ for all $t\in \mathbb{R}$ . Now comes the part where I have problems. In the book it is claimed that $$
\frac{d}{dt}w(t)=iAU(t)\psi-iAV(t)\psi
$$ Since $A$ is the infinitesimal generator of $U$ it is clear that $\frac{d}{dt}(U(t)\psi)=iAU(t)\psi$ . The infinitesimal generator of $V(t)$ however is $\bar{A}$ , so that $\frac{d}{dt}(V(t)\psi)=i\bar{A}V(t)\psi$ . The equation above would follow if we knew that $V(t)\psi\in Dom(A)$ . It is also true that $\frac{d}{dt}(V(t)\psi)=iV(t)\bar{A}\psi=iV(t)A\psi$ , so it would also suffice to show that $V(t)$ and $A$ commute. I do not see why either of these conditions should be true. Maybe there is something else that I am missing here.","['quantum-mechanics', 'proof-explanation', 'operator-theory', 'functional-analysis']"
