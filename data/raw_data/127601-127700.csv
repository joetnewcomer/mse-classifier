question_id,title,body,tags
1950216,Equation of a line using parameters $\theta$ and $\rho$ or $r$,"I am curious as to how the below two equations of a line are derived: $$x\sin(t)-y\cos(t)+p=0\tag{1}\label{1}$$ where $t$ is the angle the line makes from $+x$-axis and $p$ is the shortest distance from the line to the origin (length of vector from origin to line orthogonally). $t$ is given by $\arctan(y/x)$ and $p$ can be found by knowing a point $(x,y)$ on the line after finding $t$. Another equation of the line is: $$x\cos(t)+y\sin(t)=r\tag{2}\label{2}$$ It is basically the same idea. $r$ is the shortest distance from the line to the origin, which is again the length of a vector pointing orthogonally to the line. The Question: The two equations are different yet similar. I am not sure how they are derived. I have attempted to derive the equation \eqref{2} below, but I am stuck with regards to deriving equation \eqref{1} I tried to derive the second equation as follows (I need verification): The dot product of two vectors is $0$ if and only if they are perpendicular. Given a line containing two points pointed to by two vectors $\langle x,y\rangle$ and $\langle x_0,y_0\rangle$. The vector from $\langle x_0,y_0\rangle$ to $\langle x,y\rangle$ is the vector $\langle x-x_0,y-y_0\rangle$. We can use the unit circle to define a unit vector, $n$, (contained in the unit circle) normal to our line, $$\langle nx,ny\rangle=\langle\cos t,\sin t\rangle$$ If a point $(x,y)$ lies on our line, it must satisfy the equation $\langle\cos t,\sin t\rangle*\langle x-x_0,y-y_0\rangle=0$, where $*$ is the dot product. Multiplying out, you get: $$\langle\cos t,\sin t\rangle*\langle x-x_0,y-y_0\rangle=x\cos t-x_0\cos t+y\sin t-y_0\sin t=x\cos t+y\sin t-(x_0\cos t+y_0\sin t)$$ Let $r=x_0\cos t+y_0\sin t$. Then equation can be rewritten as: $$x\sin t+y\cos t=r$$ where $r$ is the shortest distance from origin to the line, i.e, the length of the vector pointing from origin to the line orthogonally. Is this correct?","['parametrization', 'trigonometry', 'linear-algebra']"
1950238,"How to show that if $f$ is rapidly decreasing, we have $|f(x-y)|\le C_N/(1+|x|)^N$ when $|y|\le |x|/2$.","Let $u(x,t)=(f*\mathcal{H}_t)(x)$ for $t>0$ where $f$ is a function in the Schwartz space and $\mathcal{H}_t$ is the heat kernel. Then we have the following estimate (from Stein and Shakarchi's Fourier Analysis): $$|u(x,t)|\le \int_{|y|\le|x|/2}|f(x-y)|\mathcal{H}_t(y)dy+\int_{|y|\ge|x|/2}|f(x-y)|\mathcal{H}_t(y)dy \\
\le \frac{C_N}{(1+|x|)^N}+\frac{C}{\sqrt{t}}e^{-cx^2/t}.$$ To get the first inequality, the text says ""Indeed,since $f$ is rapidly decreasing, we have $|f(x-y)|\le C_N/(1+|x|)^N$ when $|y|\le |x|/2$."" However, this is what I don't understand. I don't know how to get this specific form of inequality given $|y|\le |x|/2$. A related inequality given in the text is that if $g$ is rapidly decreasing then by considering the two cases $|x|\le 2|y|$ and $|x|\ge 2|y|$, we have $\sup_x |x|^l |g(x-y)|\le A_l (1+|y|)^l.$ I think this one can be shown by similar reasoning as the above one, but I really have no idea how to show this by considering the two cases. A function $f$ is rapidly decreasing if it is indefinitely differentiable and 
$$\sup_{x\in R} |x|^k |f^{(l)}(x)|<\infty \; \text{for every} \; k,l\ge 0.$$ I would greatly appreciate it if anyone could show this inequality.","['real-analysis', 'fourier-analysis', 'fourier-transform', 'convergence-divergence', 'analysis']"
1950308,How many natural solutions to $x_1+x_2+x_3+x_4=100$ with $x_1 \neq x_2 \neq x_3 \neq x_4$?,"I have invented a little exercise of combinatorics that I can't solve (without brute forcing): in how many ways can I choose 4 non-negative integers, the sum of which is 100 and they are all different? So: $x_1+x_2+x_3+x_4=100$ $x_1, x_2, x_3, x_4 \in \mathbb N$ $x_1 \neq x_2 \neq x_3 \neq x_4$ I have thought that the result have to be the number of total combination $\binom {100 + 4 - 1} {4 - 1}$ minus the ways I can solve $2x_1 + x_2 + x_3 = 100$. Anyway, I am not able to solve it either. The solution should be: 161664 or $\binom {100 + 4 - 1} {4 - 1} - 15187 = 176851 - 15187 = 161664$ Does anyone know how to calculate the combinations of this problem?","['combinations', 'combinatorics']"
1950310,"Determinant of matrix with $A_{ij} = \min (i, j)$","Given a $n\times n$ matrix whose $(i, j)$-th entry is the lower of $i,j$, eg.
$$\begin{pmatrix}1 & 1 & 1 & 1\\
1 & 2 & 2 & 2 \\
1 & 2 & 3 & 3\\
1 & 2 & 3 & 4 \end{pmatrix}.$$
The determinant of any such matrix is $1$. 
How do I prove this?
Tried induction but the assumption would only help me to compute the term for $A_{nn}^*$ mirror.","['matrices', 'linear-algebra', 'determinant']"
1950372,"Show $f\colon\{\,x+iy \mid x,y\in\mathbb{Q}\,\}\rightarrow\mathbb{Q}\times\mathbb{Q}$, $f(x+iy)=(x,y)$ is well defined","My original assignment was to prove that the set $\{\,x+iy \mid x,y\in\mathbb{Q}\,\}$ is countable. Since I now that $\mathbb{Q}\times\mathbb{Q}$ is countable, I immediately thought of the injection $f\colon \{\,x+iy \mid x,y\in\mathbb{Q}\,\}\rightarrow\mathbb{Q}\times\mathbb{Q}$ with $f(x+iy)=(x,y)$. My teacher then asked me, whether or not this function is well defined. I'd say it's pretty obvious that $f$ is well defined, since unique $x+iy$, gives unique pairs of $x$ and $y$, but I'm not quiet sure on how I can show it more strictly?",['functions']
1950449,How to compute $\lim_{x\to 0^+}\frac{\arctan x-x}{x^2}$ without Taylor's formula or L'Hôpital's rule?,"I have to find
$$\lim_{x \rightarrow 0^+} \frac{ \arctan(x)-x}{x^2}$$ without Taylor's formula or L'Hôpital's rule. How  to tackle it? Any idea is welcome.","['limits-without-lhopital', 'limits']"
1950456,Multiplication theorem for modified Bessel Function of the Second Kind,"The modified Bessel Function of the Second Kind $K_v(z)$ can be expressed by 
$$
K_v(z)=\frac{\pi\csc(\pi v)}{2}(I_{-v}(z)-I_v(z)),
$$
if $v$ is not an integer. $K_v(z)$ satisfies the following multiplication theorem. 
$$
\lambda^{v}K_v(\lambda z)=\sum_{l=0}^{\infty}{\frac{(-1)^l}{l!}(\frac{(\lambda^2-1)z}{2})^jK_{v-j}(z)}, ~~\text{if}~~ |\lambda^2-1|<1.
$$
My question is that whether the condition $|\lambda^2-1|<1$ here is necessary at least when $\lambda$ is real. I saw this formula  in the book 'Handbook of Mathematical Functions' edited by Abramowitz and Stegun. However, when I checked in Wikipedia, it seems that this condition is not necessary and the formula is valid for any $\lambda$. Can anyone show me which is correct?","['special-functions', 'complex-analysis', 'analysis']"
1950466,If $x_i \perp x_j$ then show that $x_j$ 's are linearly independent,"Let $x_1, x_2, ...,x_n \in \mathbb{H}$, $\mathbb{H}$ is the Hilbert space. $x_j \neq 0$ for every $j$.   If $x_i \perp x_j$ for $i \neq j$ then show that $x_j$ 's are linearly independent. I think I need to use the fact that $<\alpha x + \beta y , z> = \alpha<x,z> + \beta<y,z>$ but how?","['functional-analysis', 'hilbert-spaces']"
1950523,Estimating Parameter - What is the qualitative difference between MLE fitting and Least Squares CDF fitting?,"Given a parametric pdf $f(x;\lambda)$ and a set of data $\{ x_k \}_{k=1}^n$, here are two ways of formulating a problem of selecting an optimal parameter vector $\lambda^*$ to fit to the data. The first is maximum likelihood estimation (MLE): $$\lambda^* = \arg \max_\lambda \prod_{k=1}^n f(x_k;\lambda)$$ where this product is called the likelihood function. The second is least squares CDF fitting: $$\lambda^*=\arg \min_\lambda \| E(x)-F(x;\lambda) \|_{L^2(dx)}$$ where $F(x;\lambda)$ is the CDF corresponding to $f(x;\lambda)$ and $E(x)$ is the empirical CDF: $E(x)=\frac{1}{n} \sum_{k=1}^n 1_{x_k \leq x}$. (One could also consider more general $L^p$ CDF fitting, but let's not go there for now.) In the experiments I have done, these two methods give similar but still significantly different results. For example, in a bimodal normal mixture fit, one gave one of the standard deviations as about $12.6$ while the other gave it as about $11.6$. This isn't a huge difference but it is large enough to easily see it in a graph. What is the intuition for the difference in these two ""goodness of fit"" metrics? An example answer would be something along the lines of ""MLE cares more about data points in the tail of the distribution than least squares CDF fit"" (I make no claims on the validity of this statement). An answer discussing other metrics of fitting parametric distributions to data would also be of some use.","['least-squares', 'maximum-likelihood', 'statistics', 'numerical-methods', 'parameter-estimation']"
1950525,Piecewise continuity with countably infinite discontinuities,"If a function has countably infinite number of discontinuities, can it still be called piecewise continuous [say greatest integer function]? I read that a piecewise continuous has finitely many discontinuities. Thanks for the help in advance.","['continuity', 'functions']"
1950536,Let $X\ge 0$ be a random variable. Show that for any $M\gt 0$ that $M\sum_{k=1}^\infty P(X\gt kM)\le E(X)\le M\sum_{k=0}^\infty P(X\gt kM)$.,"Let $X\ge 0$ be a random variable. Show that for any $M\gt 0$ that
$$M\sum_{k=1}^\infty P(X\gt kM)\le E(X)\le M\sum_{k=0}^\infty P(X\gt kM).$$ Attempt We know, $E(X)=\int_0^\infty(1-F(x))dx $ and $P(X\gt kM)=1-F(kM)$, where $F$ is distribution function. So by using this I get,  $$M\sum_{k=1}^\infty 1-F(kM)\le \int_0^\infty(1-F(x))dx \le M\sum_{k=0}^\infty 1-F(kM).$$ I was trying to approach like this but can't proceed further. If I am on right track then how to proceed further and if not then how to solve this problem? Thanks",['probability-theory']
1950629,Find the explicit solution for the following differential equation.,"I have the following differential equation $$\frac{dx}{dt} = x^2-4$$ Separating the variables, I get $$\frac{dx}{x^2-4} = dt$$ Let us write it in partial form $$\frac{dx}{(x-2)(x+2)} = dt$$ $$\frac{dx}{4(x-2)} - \frac{dx}{4(x+2)} = dt $$ $$ \frac{dx}{(x-2)} - \frac{dx}{(x+2)} = 4dt $$ $$ \ln{|x-2|} - \ln{|x+2|} + C_1 = 4t + C_2 $$ Let $C_2 - C_1 = C$ $$ \ln{|\frac{x-2}{x+2}|} = 4t + C $$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t+C}$$ $$e^{\ln{|\frac{x-2}{x+2}|}} = e^{4t}e^C$$ Let $e^C = C$ Since it is a constant $$\frac{x-2}{x+2} = Ce^{4t}$$ Let $x(0) = x_0$ $$\frac{x_0-2}{x_0+2} = C$$ Substituting for C $$\frac{x-2}{x+2} = \frac{(x_0-2)e^{4t}}{x_0+2}$$ I am rather stuck in here. The solutions manual to this question gives: $$x(t) =\frac{2[x_0 + 2 + (x_0 - 2)]e^{4t}}{x_0 + 2 - (x_0 - 2)}$$ The solutions manual does not elaborate on how it came to the solution above. How do I approach the problem? Any hints? Source: Differential Equations and Boundary Value Problems: Computing and Modeling (5th Edition) 
by C. Henry Edwards (Author), David E. Penney (Author), David T. Calvis (Author) Question 5 Chapter 2.2 NOTE: $x(0)$ is not given at all so this is not a mistake. Hence, we simply do $x(0) = x_0$.",['ordinary-differential-equations']
1950646,When is a smooth homeomorphism a diffeomorphism?,"I would like to know when a smooth homeomorphism is actually a diffeomorphism. I know that if $f$ is a homeomorphism and an immersion (i.e. an embbeding), then $f$ is a diffeomorphism. However, I do not want to use the notions of immersion, submersion,... I read somewhere that in low dimensions this actually happens. Is it true?","['general-topology', 'differential-geometry', 'differential-topology']"
1950654,Finding the radius of convergence of $\sum\limits_{j=0}^{\infty}2^jz^{j^2}$ using Cauchy-Hadamard formula: what am I doing wrong?,"For a power series $\sum_{j=0}^{\infty}a_j(z-z_0)^j$ the Cauchy-Hadamard formula states that:$$R=\frac{1}{\operatorname{lim sup}\sqrt[n]{|a_n|}}$$
Where $\sqrt[n]{|a_n|}$ is a sequence formed from the set of coefficients $\{a_j\}$ and R is the radius of convergence (ROC). When I'm trying to apply this to find the ROC for $\sum_{j=0}^{\infty}2^jz^{j^2}$ i get the following:
$$\{a_j\}=1,2,4,... \Rightarrow \sqrt[n]{|a_n|} = 1^{\frac10},2^{\frac11},4^{\frac12}...=1,2,2...$$
From which i deduce that $R=\frac12$ but the book says the answer is $R=1$. What am I doing wrong? I'm also curious as to why $\sum_{j=0}^{\infty}2^jz^{j^2}$ can be called a power series. The book I'm using is Fundamentals of Complex Analysis by Saff & Snider, section 5.4 exercise 3(b).","['complex-analysis', 'sequences-and-series', 'power-series']"
1950700,"Rock, Paper, Scissors Probability For Multiple Players","Let the number of players for a game of rock, paper, scissors be $n$. In each round of the game, each player chooses one of rock, paper, or scissors. Then, if exactly two of the three options have been chosen, the players who chose the losing option among those two are eliminated. (For example, if everybody chooses either rock or paper, those choosing rock are eliminated.) Otherwise, everyone stays in the game. The game continues until all but one player has been eliminated. How would you work out the probability that a winner is determined after $k$ rounds? (Assuming each choice is equally as likely)","['recreational-mathematics', 'probability']"
1950706,Show that this measure theory statement is closed under countable unions,"We say that a set $ A \subset [0,1]$ is dyadic-($\epsilon,n$) if for every $\epsilon > 0$ there exist a set $B$ which is the union of dyadic interval $[p/2^n,(p+1)/2^n]$ at scale $2^{-n}$ such that $A$ and $B$ differ on a set of measure less than $\epsilon$. I want to prove the following statement: "" if $A$ is a Lebesgue measurable set, for every $\epsilon > 0$ there exist an integer $n$ such that $A$ is dyadic-($\epsilon,n$)"". I can prove this with a direct proof (regularity of measure and dyadic coverings), but I'm interested in a $\sigma$-albegra constructive proof. To conclude such proof I need to show that the claim (""for every $\epsilon > 0$ there exist an integer $n$ such that $A$ is dyadic-($\epsilon,n$)"") is closed under countable union. Hope now it makes sense. Thank you.","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1950712,"Find area of the triangle ABC, given the coordinates of vertices in plane","$A, B$ and $C$ are the points $(7,3), (-4,1)$ and $(-3,-2)$ respectively. Find the area of the triangle $ABC$. I've worked out the lengths of each side of the triangle which are $AB=5\sqrt5$, $BC=\sqrt10$ and $AC=5\sqrt5$. I know that the formula for the area of a triangle is $\frac12hb$ but when I checked the solutions the answer to the area of this triangle is $17\frac12$. I do not understand how this answer is achieved.","['analytic-geometry', 'area', 'triangles', 'geometry']"
1950720,Definition of Frenet frame for curves in $\mathbb{R}^n$.,"I have a question about the definition of Frenet frame here (pg 6) https://www.math.cuhk.edu.hk/~martinli/teaching/4030lectures.pdf . The definition above is summarized below: We are given a regular curve, $c$, in $\mathbb{R}^n$ parametrized by arc length, such that $c'(s),c''(s),\dots,c^{n-1}(s)$ are linearly independent for each $s$. A Frenet frame for $c$ is defined to be a positively oriented orthonormal basis $\{ e_1(s), \dots, e_n(s) \}$ such that $\text{Span}\{e_1(s),\dots,e_k(s)\} = \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} n$ and $\langle c^{k}(s) , e_k(s) \rangle > 0$. Condition 1. above does not look right to me because no assumptions about $c^{n}(s)$ are made. Should 1. be replaced by $\text{Span}\{e_1(s),\dots,e_k(s)\} =
    \text{Span}\{c'(s),\dots,c^{k}(s)\}$ for $k=  1,\dots, \color{red} {n
    - 1}?$ A similar assumption consistent with the document above is also made here http://www.cs.elte.hu/geometry/csikos/dif/dif2.pdf (pg 7. first definition on top of the page) so I wonder if I am missing something.",['differential-geometry']
1950788,"$f:\mathbb R^2 \to \{0,1\}$ be a function , does there exist an equilateral triangle in plane with vertices $x,y,z$ such that $f(x)=f(y)=f(z)$?","Let $f:\mathbb R^2 \to \{0,1\}$ be a function ; then is it true that there exists an equilateral triangle in plane with vertices $x,y,z \in \mathbb R^2$  such that $f(x)=f(y)=f(z)$ ?","['combinatorics', 'analysis']"
1950799,Is $Ei(2)-Ei(1)$ transcendental?,"Is the number $$\int_1^2 \frac{e^x} x \, dx=Ei(2)-Ei(1)$$
  transcendental ? The first few digits are $$3.059116539645953407912984195895401006500992980687334462880866822688\cdots$$ Working with lindep and algdep-command with PARI/GP I did not find an indicitation that the given number is algebraic. But can it be proven that it is transcendental ? I tried to apply the known powerful theorems (Lindemann-Weierstrass and Baker ), but without success. If the number cannot be verified, is at least the status of $Ei(1)$ and $Ei(2)$ known ? Here https://en.wikipedia.org/wiki/Exponential_integral the definition of $Ei(x)$","['number-theory', 'integration', 'transcendental-numbers']"
1950806,Permutations of the word ENGLAND such that permutations start and end with a vowel [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question How many words can be made by using the letters of the word ENGLAND such that words begin and end with a vowel? I couldn't understand how come the answer to this question is $120$.","['permutations', 'combinatorics']"
1950815,Contradicting Answer when calculating determinant in two different ways,"So I'm supposed to decide which $x \in \mathbb{R}$ gives a non-zero determinant: \begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix} It works well when when I re-write the matrix using Gaussian Elimination, only adding/subtracting rows to other rows. However, out of interest, I tried adding two rows to each other ""simultaneously"", because I suspected that it would bring me to the solution quicker. Basically, I tried subtracting row 3 with row 2, and row 2 with row 3 and replace them ""at the same time"". It's kind of difficult to explain, so I'll show you instead: $$\begin{vmatrix}1&x&x&2\\x&1&2&x\\x&2&1&x\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&-1&1&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=\begin{vmatrix}1&x&x&2\\0&0&0&0\\0&1&-1&0\\2&x&x&0\end{vmatrix}=0$$
Apparently this suggests that all values on $x$ produces a zero-valued determinant, which isn't true. I'm suspecting that there is something wrong in cross-adding/subtracting rows to other rows, but I can't quite figure out why. My wild guess is that there's some equivalence to multiplying a row by zero. In order to pin-point this, I tried to compare it to a set of equations: $$\begin{cases} x + y = 3 \\ 2x + y = 5\end{cases}$$ From this you obtain the matrix: $$\begin{pmatrix}1&1&3\\2&1&5\end{pmatrix}\sim \begin{pmatrix}1&0&2\\0&1&1\end{pmatrix}$$
Which gives the solution $\begin{pmatrix}x\\y\end{pmatrix}=\begin{pmatrix}2\\1\end{pmatrix}$ However, if I multiply row 1 with 2 and subtract it from row 2, and at the same time multiply row 2 with 1/2 and subtract that to row 1, I obtain:
$$\begin{pmatrix}0&1/2&1/2\\0&-1&-1\end{pmatrix}\sim\begin{pmatrix}0&1&1\\0&0&0\end{pmatrix}$$ So basically, I've lost information about $x$, which makes sense. However, what bothers me is that I get confused when this is applied generally, such as in the case with the determinant. My questions are therefore: In what way does this type of ""cross-addition/cross-subtraction"" of
matrices lose information? Maybe a vague question, but is there some
way of seeing where/how the information is lost? Can it be compared to multiplying by 0 somehow? Is there any general rule for how you cannot perform operations on rows in matrices to avoid losing information? I know that you can't multiply a whole row by 0, but are there other things to look out for? Basically, I'm just curious about this, since it hasn't been mention anywhere in my book or on any lecture. Maybe the answer is obvious and I'm just blind, but even if that's the case, I'd appreciate some guidelines! Thanks a head! EDIT: From experimenting a bit, I've started to realize that doing this cross-addition is similar to completely removing a row, which is similar, and even equivalent, to multiplying by 0. (That's why you always end up with at least one 0-row, or if you continue, a 0-matrix.) Maybe that was the connection I was looking for? It seems to make sense, but I may have missed something.","['matrices', 'gaussian-elimination', 'linear-algebra']"
1950822,Number of $3$s in the units place,"I'm stuck on this problem: (1) In the sequence $7,7^2,7^3,7^4,\ldots,7^{2014}$ how many terms have $3$ as the units digit? After some random stuff, I have found that the unit digits of $7$ go in the order $7,9,3,1$ And then back to $7$. But I don't know how to utilize this in the problem!",['algebra-precalculus']
1950831,How do I prove that $f(n) = 0$ for all $n$ where $n$ is a positive integer? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question In particular, $f$ is defined such that $f(mn)=f(m)+f(n)$, where $m$ and $n$ are positive integers. Also $f(n)\ge 0$ for all $n$, $f(10)=0$, and $f(n)=0$ if $n$ ends in a $3$. Thank you.",['functions']
1950859,Introduction to viscosity solutions theory,"Can you recommend an introduction to viscosity solutions theory ? More specifically, I'm looking for a modern treatment similar to Chapter 10 of Evans's Partial Differential Equations , but somewhat more detailed and comprehensive. (Of course, I'm aware of the User's Guide , but it is not quite what I'm looking for).","['partial-differential-equations', 'hamilton-jacobi-equation', 'functional-analysis', 'viscosity-solutions', 'analysis']"
1950866,"What is the meaning of ""expected value of an estimator""?","I am studying statistics and i am having trouble understanding some proofs because i don't quite understand what the concept of ""expected value of an estimator"" means and what is the difference with the value of the esimator itself. Say i got a sample and I take the variance v of that sample. That variance v is my estimator. What is the meaning of the expected value of the variance? Thanks a lot!","['descriptive-statistics', 'statistics']"
1950915,How to solve $x'^2-1-2xx''=0$,"Find the general solution of:
  $$(x')^2-1-2xx''=0$$ Is there a well-known technique for solving  this ODE?",['ordinary-differential-equations']
1950922,How to check f is surjective,"Let $f:\mathbb{R} \rightarrow \mathbb{R}$ be a continuous function s.t $|f(x)-f(y)|\ge \frac{1}{2}|x-y|$ for all $x,y$.
Then $f$ is injective can be seen easily as $x\neq y$ implies $f(x)\neq f(y)$. 
But how to check that $f$ is surjective or not ?","['continuity', 'functions']"
1950954,Rewriting Infinite Sum,"Are the following equal?
$$\sum_{k = 1}^{\infty}a_k = \sum_{k = 1}^{\infty}(a_{2k} + a_{2k - 1})$$
If I expand the summations they are the same series, so they should be equivalent in all respects (convergence, divergence, etc.), right? I just want to confirm that there is no problem with manipulating an infinite series like this.","['real-analysis', 'sequences-and-series', 'calculus']"
1950975,"Prove that if $B(x,r)\subset B(x',r')\Longleftrightarrow d(x,x')\leq r'-r$","For this proof, we're in $\mathbb{R}^n$ and $d$ is the Euclidean metric. So the claim can be written $(|x-y|<r\Rightarrow |x'-y|<r')\Longleftrightarrow |x-x'|\leq r'-r$. I think that I have ($\Leftarrow$): Assume $|x-x'|\leq r'-r$. Consider $y\in B(x,r)$. Notice that   $|x-x'|+|x-y|<|x-x'|+r\leq r'$. Since $|(x'-x)+(x-y)|\leq |x'-x|+|x-y|< r'$, we have $|x'-y|<r'$. Therefore, $y\in B(x',r')$ and $B(x,r)\subset B(x',r')$. I've gone through 8 sheets of paper and as many hours on the other direction, but have yet to come up with anything substantial. What I'm most confused about is how we can obtain the $-r$ term on the right side of the inequality.",['analysis']
1951013,Closest distance from point to surface is normal proof,"If I have a point and a surface, the vector for the closest distance between the point and the surface will be normal to the surface: Intuitively it makes sense. If we assume the contrary (that the closest distance vector is not normal to the surface) then we can minimize the distance by travelling in the direction that makes the distance vector normal. But how would I prove this formally? And does this theorem hold for any type of surface?",['multivariable-calculus']
1951015,Left Inverses of a Matrix,"I am confused about left inverses after examining two particular problems. From what I understand in order to find the left inverse of an $m\times n$ matrix I must first transpose it then augment it with the identity matrix that would result after multiplying $A$ by an unknown $X$. After this I should perform RREF as far as I can and whatever I'm left with (barring any inconsistent rows) will be the columns of $X$ (the inverse of $A$). At that point it should be true that $XA = I$, however I do not get that for the first problem. I checked to make sure the first matrix does not have a right inverse so I don't know why $AX$ produces the identity matrix when it was the left inverse that was being solved for. $A = \begin{bmatrix}1 & 2 \\ 2 & 5 \\ 3 & 7\end{bmatrix}$ $\begin{bmatrix}1 & 2 & 3 & | & 1 & 0\\ 2 & 5 & 7 & | & 0 & 1\\\end{bmatrix} => RREF => \begin{bmatrix}1 & 0 & 1 & | & 5 & | & -2\\0 & 1 & 1 & | & -2 & | & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}5-r & -2-t \\ -2-r & 1-t \\ r & t\end{bmatrix}$ (See correction) However $XA$ does not produce the identity matrix while $AX$ does. Is this because I transposed $A$ in order to augment it and find $X$? As far as I know, $X$ should be the left inverse, so $XA$ should work. What really confuses me is that for this second matrix, $B$, I attempt the exact same thing and find that $XB = I$ as expected. $B = \begin{bmatrix}2 & -1 \\ 4 & -1 \\ 2 & 2\end{bmatrix}$ $\begin{bmatrix}2 & 4 & 2 & | & 1 & 0\\ -1 & -1 & 2 & | & 0 & 1\end{bmatrix} => RREF => \begin{bmatrix}2 & 4 & 2 &| & 1 & 0 \\ 0 & 1 & 3 & | & 1/2 & 1\end{bmatrix}$ Therefore, $X = \begin{bmatrix}-1/2+5s & 1/2-3s & s \\ -2+5t & 1-3t & t\end{bmatrix}$ Could someone please explain what is happening? I strongly suspect it has to do with taking the transpose, but why then does it work for one and not the other? The problems are both ""cooked"" so I know there are definitely left inverses for both. **** Correction / Solved **** $X = \begin{bmatrix}5-r & -2-r & r\\ -2-t & 1-t & t\\\end{bmatrix}$","['matrices', 'transpose', 'linear-algebra', 'inverse']"
1951020,Change in connection forms under change of frame,"Let $\nabla$ be a linear connection in a vector bundle $\pi:E\to M$. Fix $X\in \mathfrak{X}(M)$ and a local frame $s_1,\ldots,s_k\in \Gamma(E_U)$. It seems to me that the plan is to use linear algebra ideas to study the operator $\nabla_X$ in $\Gamma(E)$, great. So one writes $$\nabla_Xs_j = \omega^i_{\hspace{.5ex}j}(X)s_i$$and proves that the $\omega^i_{\hspace{.5ex}j}$ are one-forms. Wonderful. Now comes my question. Take another local frame $\overline{s_1},\ldots,\overline{s_k}\in\Gamma(E_U)$. Write $\overline{s_j} = h^i_{\hspace{.5ex}j}s_i $ for some convenient smooth functions $h^i_{\hspace{.5ex}j}$. Repeating the above discussion for this new frame, and fiddling around, I find out that $$\overline{\omega^i_{\hspace{.5ex}j}} h^k_{\hspace{.5ex}i}= \omega^k_{\hspace{.5ex}i}h^i_{\hspace{.5ex}j} + {\rm d}h^k_{\hspace{.5ex}j}.$$
I expected the first term but not the ${\rm d}h^k_{\hspace{.5ex}j}$ (apart from the Leibniz rule, I mean). What is the interpretation for the extra term?","['differential-forms', 'connections', 'vector-bundles', 'differential-geometry']"
1951021,Why in calculus the angles are measured in radians? [duplicate],This question already has answers here : Why does the derivative of sine only work for radians? (17 answers) Why do we require radians in calculus? (11 answers) Closed 7 years ago . Why is the formula $\lim\limits_{h\rightarrow 0}\frac{\sin h}{h}=1$ not valid when $h$ is measured in degrees?,"['limits', 'angle', 'trigonometry', 'calculus', 'geometry']"
1951173,Symmetric difference = Ø,"This is my first post on this forum :) I've been trying for a while to solve this problem about symmetric difference of two sets: Given two sets $A$ and $B$, we call symmetric difference of $A$ and $B$ the following: $$A\mathrel{\triangle}B = (A\cup B)-(A\cap B)$$ Show that if $A\mathrel{\triangle} B = \emptyset$, then $(A\subset B) \vee (B\subset A)$. I don’t know where to start; thank you very much for the help :)",['elementary-set-theory']
1951213,Separation of variables on a second order ODE,"I am attempting to grasp the basics of separation of variables for a second order separable differential equation, and am failing to do so: Given the equation: $$ x=\frac{d^2y}{dx^2}$$ I know from calculus class intuitively that the solution is: $$ y=\frac{x^3}{6}$$ But if I am to use separation of variables, why don't I get: $$ \frac{y^2}{2}=\frac{x^3}{6} $$ Since I should have had to integrate the $y$ side twice as well.",['ordinary-differential-equations']
1951232,Understanding results of Gram-Schmidt orthogonalization,"I just started learning about Gram-Schmidt, and I understand what it does, but I'm having trouble showing why it works. For example, let $P_k$ for $k=1,...,p$ be a projection matrix for a matrix $A_{1:k}$ (so the first $k$ columns). Then let's say we have $u_1 = a_1$, and any $u_k = (I-P_{k-1})a_k$ for each $k$. How do I: 1) Show that the $u_k$ are orthogonal 2) Show that the span of $u_1,...,u_k$ is the same as the span of $a_1,...,a_k$ for each $k=1,...,p$. I feel a bit dumb because I realize that both these results are the the whole point of Gram-Schmidt. #2 I thought should be especially obvious but when I tried to prove it I got no where fast. I was trying to take advantage of the fact that $P$ is symmetric/idempotent, but maybe that's not the approach to take? Can anyone help? I'd really appreciate it.","['statistics', 'orthogonality', 'linear-algebra', 'vector-spaces']"
1951267,Why is $\{\emptyset\} \not = \emptyset$?,"Why is the following true? $$\{ \emptyset \}  \neq \emptyset $$ Is it just because the Empty set already refers to there being a set present and so it is just the same argument that $\{ \{3\} \} \neq \{ 3\} $? And is there a proof for the following? For any set$\, X$, $$X \cup \emptyset = X \quad \text{and} \quad X \cap \emptyset = \emptyset$$",['elementary-set-theory']
1951295,Conjugacy classes and characteristic polynomials,"So I was working in $GL(2,p)$ and I was wondering if two matrices are similar iff they have the same characteristic polynomial. One direction is obvious, but I was wondering if it was true that if two matrices have the same characteristic polynomial then they are conjugate?","['abstract-algebra', 'group-theory']"
1951314,Composition of 2 involutions,"How can we prove that any bijection on any set is  a composition of 2 involutions ? Since involutions are bijections mapping elements of a set to elements of the same set, I find it weird that this applies to any bijection. Thanks for your help !","['logic', 'functions', 'involutions']"
1951323,Why does $\lim_{h \to 0} f(x+h) = f(x)$ according to Lang's Short Calculus?,"On Chapter III ( Derivatives ), section 5 ( Sums, products, & quotients ) of Lang's Short Calculus , Lang writes the following excerpt: It's unclear to me why Lang asserts $$f(x+h) = f(x) + h \frac{f(x+h)-f(x)}{h} $$ From independent research, I know this is true if the function is continuous ( https://www.quora.com/Does-Lim-f-x+h-f-x-as-h-approaches-0 ). However, my question is how did Lang arrive at this assertion (aka why is the assertion true)? Thanks.","['derivatives', 'calculus', 'limits']"
1951349,Conditional probability question involving Bayes' theorem,"A jar has 300 buttons in it. 299 buttons have one side red, the other side blue. One of the buttons has both sides blue. I randomly take a button from the jar and toss it 5 times getting 5 blues in a row. What is the probability I chose the special blue button? I did: 
$$
\begin{align}
P(\text{chose blue button}|\text{5 blue tosses}) &= \frac{P(\text{choose blue button and 5 blue tosses})}{P(\text{5 blue tosses})}\\
&= \frac{\frac{1}{300}\cdot 1^5}{\frac{299}{300} \left(\frac{1}{2}\right)^5 + \frac{1}{300}1^5}
\end{align}
$$
Is this correct?","['bayes-theorem', 'probability']"
1951368,"If $f(x+1) = x^2 + 3x +5$, then find $f(x)$","A challenge problem from precalc class. I don't know what to do with the one from $f(x+1)$, it doesn't factor well, I could do completing the square but then how do I find $f(x)$? Just stuck on this.","['algebra-precalculus', 'functions']"
1951400,"What is ""wrong"" with $\sum_{n=0}^\infty \frac{(-2x)^n}{1+n}$ at $x = 0$","I was working with this power series: $$\sum_{n=0}^\infty \frac{(-2x)^n}{1+n}$$ Which converges for $|x| < \frac{1}{2}$ according to Cauchy-Hadamard theorem. No problem so far. Then I found its sum, that is: $$ \frac{\log(1+2x)}{2x} $$ And here I get confused, since previously I found that the series converges for x=0, but I can not evaluate its sum at x = 0. What am I missing here? Any help or comment is appreciated, and thanks for your time.","['power-series', 'sequences-and-series', 'calculus', 'limits']"
1951409,Properties of projection matrix for linear models,"1) Let's say we have the linear model $y=Xβ$. Suppose the model has full rank $X$ and projection matrix $P_X$, show that $P_XX = X$ and that $C(P_X) = C(X)$. 2) Suppose that $P_1$ and $P_2$ are projections for the model $Q_1$ nested within model $Q_2$. Show that $P_1$, $P_2 - P_1$, and $I-P_1$ are all projection matrices and are all mutually orthogonal. I've been having some real difficulties with these 2 problems. I think the first one might be simple (I have a guess for the column space equivalency) but I don't even know where to start for the second one-the mutual orthogonality is what gets me. I'd appreciate any help I can get.","['regression', 'matrices', 'statistics', 'linear-algebra', 'vector-spaces']"
1951437,Submodule maximal if and only if quotient module simple?,"Why is the case that a submodule $L$ of a module $L$ is maximal, among submodules of $M$ distinct from $M$, if and only if the quotient module $M/L$ is simple?","['abstract-algebra', 'modules', 'linear-algebra']"
1951490,"When indexing set is empty, how come the union of an indexed family of subsets of S is an empty set?","And how is intersection of an indexed family of subsets if the set itself? 
I am reading the Bert Mendelson's Introduction to Topology , and I can't figure how this works. When the indexing set is an empty set, doesn't that mean we can't find any subset of the form $A_\alpha$? Therefore, there is no element in the set $S$ such that $x \in A_\alpha$? I could accept the idea that $\cup_{\alpha \in I}A_\alpha = \emptyset$, but can't understand why $\cap_{\alpha \in I}A_\alpha = S$. To my understanding, I think $\cap_{\alpha \in I}A_\alpha = \emptyset$ as well.",['elementary-set-theory']
1951503,"Writing a function $ f: U \to \mathbb{R} $, where $ U $ is an open ball in $ \mathbb{R}^{n} $ centered at $ \mathbf{0} $, in terms of the dot product.","Let $ U $ be an open ball in $ \mathbb{R}^{n} $ centered at the origin $ \mathbf{0} $. Let $ f: U \to \mathbb{R} $ be a continuous function. Here is my question: If $ \displaystyle \lim_{\mathbf{x} \to \mathbf{0}} \frac{f(\mathbf{x})}{\| \mathbf{x} \|} = 0 $, then does there necessarily exist a continuous function $ \epsilon: U \to \mathbb{R}^{n} $ such that the following properties hold? $ f(\mathbf{x}) = \epsilon(\mathbf{x}) \bullet \mathbf{x} $ for all
  $ \mathbf{x} \in U $, where $ \bullet $ denotes the dot product on
  $ \mathbb{R}^{n} $. $
  \displaystyle \lim_{\mathbf{x} \to \mathbf{0}} \epsilon(\mathbf{x}) = \mathbf{0}
  $. I was able to prove the existence of $ \epsilon $ without the requirement that it be continuous on all of $ U $. The basic idea is to use the fact that the taxicab norm and the Euclidean norm on $ \mathbb{R}^{n} $ are equivalent. Thank you very much for your kind assistance!","['multivariable-calculus', 'real-analysis', 'limits']"
1951541,Finding the CDF given a PDF,"Let $\alpha$, $\beta$ be two fixed positive constants, and define the density (pdf) $$f(x) = \begin{cases} 
\frac{\alpha}{2}e^{\alpha x} & \text{if} x<0\\
\frac{\beta}{2}e^{-\beta x} & \text{if} x\geq 0
\end{cases}$$
  Find the cumulative distribution function (cdf) of $X$. I am blanking on how to do this again I haven't taken a probability theory course for about 5 years now I thought we just integrate for the two different condtions to get the cdf so essentially I would do $$\int_{-\infty}^{0}f(x) \ \ \ \text{for} \ \ x < 0$$ 
and $$\int_{0}^{\infty}f(x) \ \ \ \text{for} \ \ x \geq 0$$ If someone could just provide a detailed solution I would greatly appreciate it.","['probability-theory', 'probability-distributions']"
1951621,Arc Length with Polar Coordinates,"When trying to calculate the length of the curve $r=1+\cos\theta$ whose graph is the following: We need to evaluate $\int\limits_{0}^{\pi}\sqrt{r^2+(\frac{dr}{d\theta})^2}$, that is the following part starting at the point $(2,0)$ and then multiply by 2. Since $\int\limits_{0}^{2\pi}\sqrt{r^2+(\frac{dr}{d\theta})^2}=0$ I understand tat we cant do the whole thing at once since at the pole $(0,0)$ the curve has a sharp. But then trying to integrate $\int\limits_{\pi}^{3\pi}\sqrt{r^2+(\frac{dr}{d\theta})^2}=-8$ (in this interval the curve has no sharp) so I do not understand why do we get a negative value. Now im getting more confused since if we try to calculate the lenght of the curve $r=2-2\cos\theta$ then we do can integrate over the whole interval $\int\limits_{0}^{2\pi}\sqrt{r^2+(\frac{dr}{d\theta})^2}=16$ without having any problems","['multivariable-calculus', 'real-analysis', 'integration', 'polar-coordinates']"
1951645,Does there exist an uncountable number of isolated points?,"This question arose from my thinking about order-preserving isomorphisms on the Real numbers. These functions must be injections (“one-to-one”)
$$a\ne b \implies f(a)\ne f(b)$$
and preserve order
$$a\le b \implies f(a)\le f(b)$$ In other words, I knew that these functions must be strictly increasing, perhaps with a finite number of points with zero slope. For if there were any intervals $(a,b)$ on which the function had zero or decreasing slope, it would no longer be an injection or preserve order: two different inputs would have the same output. But then I realized that even an infinite, albeit countable, number of points would suffice, for example, a function that is increasing everywhere except at every integer; something like $f(x)=x+sin(x)$. This function’s graph looks like a smoothed-out staircase, and its derivative $f^{\prime}(x)=1+cos(x)$ is tangent to the $x$-axis at those points. So then taking a step further, I wondered if perhaps an uncountable number of points would suffice, provided that these points weren’t “together in a straight line”, or put more precisely, they didn’t make an interval. My first thought was an uncountable set of irrational numbers within an interval, say, $[1,2]$. This function would be increasing at all rational points, but have zero slope at all irrational points. (There are uncountably many irrational numbers within $[1,2]$.) If such a function exists, and is continuous and differentiable, its derivative would be something like the Dirichlet function:
$$
g^{\prime}(x) =
\begin{cases}
1 &: &x < 1\\
1 &: &x \in [1,2]\cap\mathbb{Q}\\
0 &: &x \in [1,2]-\mathbb{Q}\\
1 &: &x > 2\\
\end{cases}
$$ I'm not sure if that’s possible. If it isn’t, then the points in my uncountable set must all be isolated, that is, there must exist an $\epsilon$-neighborhood around each point that doesn’t contain any other points in the set. Is it possible to select uncountably many isolated points?","['general-topology', 'real-analysis']"
1951669,"Let $ A $, $ B $ and $ C $ be sets. If $ A \cup B = A \cup C $ and $ A \cap B = A \cap C $, then show that $ B = C $.","I’m stuck on this one problem in my textbook regarding proofs in set theory. I’ve done the following so far: Let $ x \in B $ . As $ B \subseteq A \cup B $ , we have $ x \in A \cup B $ , so $ x \in A \cup C $ because $ A \cup B = A \cup C $ . Hence, $ x \in (A \cup B) \cap (A \cup C) $ , which yields $ x \in A \cup (B \cap C) $ . Now, either $ x \in A $ or $ x \in B \cap C $ . If $ x \in B \cap C $ , then $ x \in C $ because $ B \cap C \subseteq C $ , so $ B \subseteq C $ . This is only half of the proof, though, as I have to prove that $ C \subseteq B $ as well; and I also haven’t even considered the case $ x \in A $ for the last step in what I have so far. I’ve drawn many different Venn diagrams, and although I see why the statement is true, I just can’t formalize it. Any pointers, help or guidance is very much appreciated. Thanks!","['elementary-set-theory', 'proof-verification']"
1951708,Why does $\sum_{n=0}^k \cos^{2k}\left(x + \frac{n \pi}{k+1}\right) = \frac{(k+1)\cdot(2k)!}{2^{2k} \cdot k!^2}$?,"In the paper ""A Parametric Texture Model based on Joint Statistics of Complex Wavelet Coefficients"", the authors use this equation for the angular part of the filter in polar coordinates: $$\sum_{n=0}^k \cos^{2k}\left(x + \frac{n \pi}{k+1}\right)$$ My friend and I have tested many values of $k > 1$, and in each case this summation is equal to 
$$\frac{(k+1)\cdot(2k)!}{2^{2k} \cdot k!^2}$$
The paper asserts this as well. We are interested in having an analytic explanation of this equality, if it really holds. How can we derive this algebraically? TL;DR Is this true, and if so, why?
  $$\sum_{n=0}^k \cos^{2k}\left(x + \frac{n \pi}{k+1}\right) = \frac{(k+1)\cdot(2k)!}{2^{2k} \cdot k!^2}$$","['signal-processing', 'physics', 'trigonometry', 'computer-science']"
1951733,Polynomial as sum of two polynomials with roots on the unit circle,"Can any polynomial $P\in \mathbb C[X]$ be written as $P=Q+R$ where $Q,R\in \mathbb C[X]$ have all their roots on the unit circle (that is to say with magnitude exactly $1$) ? I don't think it's even trivial with degree-1 polynomials... In this supposedly simple case, with $P(X)=\alpha X + \beta$,  this boils down to finding $\alpha_1$ and $\beta_1$ such that $|\alpha_1|=|\beta_1|$ and $|\alpha-\alpha_1|=|\beta-\beta_1|$. I can't prove that geometrically, let alone analytically... Furthermore I don't think anything can be said about the sum of two polynomials with known roots... Can someone give me some hints ?","['complex-analysis', 'polynomials']"
1951765,"Is there a ""nice"" function with zeros at lattice points in $\Bbb C$?","All functions will be considered on $\Bbb C$: The function $\sin(\pi z)$ has zeroes precisely at the lattice $\Lambda = \Bbb Z$ in the complex plane. Further, it is nice in the sense that:
$$\frac{\sin(\pi z)}{\pi z} = \prod_{n\in \Lambda\backslash\{0\}}\bigg(1 - \frac{z^2}{n^2}\bigg).$$
Now, let us take $\Lambda = \Bbb Z[i] = \{m+ni : m,n\in \Bbb Z\}$. Is there a similarly nice function $f(z)$ satisfying:
$$f(z) = z^k\prod_{n\in \Lambda\backslash\{0\}}\bigg(1 - \frac{z^4}{n^4}\bigg)$$
for some $k$? Note that I am raising $z$ to the power $4$ instead of $2$ now. Similarly, is there any nice function $g(z)$ for the lattice:
$$\Lambda = \Bbb Z[\omega] = \{m + n\omega : m,n \in \Bbb Z\}$$
where $\omega$ is a primitive cube root of $1$. I expect this $g$ to satisfy:
$$g(z) = z^k\prod_{n\in \Lambda\backslash\{0\}}\bigg(1 - \frac{z^6}{n^6}\bigg)$$
for some $k$? I would also like to know about the more general case of $\Lambda$ being the ring of integers in an arbitrary imaginary quadratic number field $K$. (Motivation: I would like to use this function to compute the zeta function of the number field $K$ at even integers mimicking Euler's Proof of $\zeta(2k)$.)","['zeta-functions', 'algebraic-number-theory', 'number-theory', 'complex-analysis', 'integer-lattices']"
1951795,Why is $\frac{p(1-p)}{1−(1−p)^2}=\frac{1-p}{2-p}$? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question I found this solution in an old exam: $$\frac{p(1-p)}{1−(1−p)^2}=\frac{1-p}{2-p}$$ Without any further explanation. Could someone explain to me how to do this transition?",['algebra-precalculus']
1951806,Show that the random variables $Y_1$ and $Y_2$ are independent,"Let $X_1,X_2$ be i.i.d with pdf
  $$f_X(x)=\begin{cases}
e^{-x} & \text{for } 0< x<\infty{}
\\0 & \text{elsewhere }
\end{cases}$$
  Show that the random variables $Y_1$ and $Y_2$ with $Y_1=X_1+X_2$ and $Y_2=\frac{X_1}{X_1+X_2}$ are independent. I know that for $Y_1$ and $Y_2$ to be independent. $P(Y_1\cap Y_2)=P(Y_1)P(Y_2)$.","['independence', 'probability-distributions', 'statistics', 'probability', 'random-variables']"
1951837,Find an example of series which converges only absolutely on $\mathbb Q$,"I am currently working on the completeness of metric spaces, so I studied the following theorem: If $E$ is a Banach space then any absolutely convergent series is convergent. Since $\mathbb Q$ is not complete, I wanted to illustrate this theorem with an absolutely convergent series on $\mathbb Q$ which wouldn't converges: $$\sum_{n=0}^\infty \vert r_n\vert\in\mathbb Q\quad\text{ and }\quad \sum_{n=0}^\infty r_n\in\mathbb R\setminus\mathbb Q \quad (\text{ with $r_n\in \mathbb Q$}).$$ Is was not able to find such series. Is there an example like it (as simple as possible ?).","['absolute-convergence', 'sequences-and-series']"
1951842,Prove standard deviation $=\sqrt{npq}$,"In class we learnt that for a binomial experiment, the standard deviation is equal to $\sqrt{npq}$. Is there a way to derive/prove this? I have a basic understanding of standard deviation and binomial but have no idea where to start in this proof.","['statistics', 'binomial-distribution', 'standard-deviation']"
1951868,How to show that axiomatic intersection multiplicity is invariant under affine transformations?,"Question: (Exercise 3.3.16, p.139, of Algebraic Geometry: A Problem-Solving Approach by Garrity et al) Show that for any polynomials $f$ and $g$ in $\mathbb{C}[x,y]$ and a point $p$ in $\mathbb{C}^2$ , for any affine change of coordinates $T$ we have $$I(p, V(f) \cap V(g)) = I(T(p), V(T^{-1} f) \cap V(T^{-1} g))$$ Hint: this problem is actually not that hard. Its solution involves little or no calculations. I fail to see at all how one can prove or disprove this using the axiomatic definition of intersection multiplicity at all, since the definition only refers to a single point, but this problem asks to compare calculations at different points. Is the idea simply supposed to be that for all of the axioms, an affine change of coordinates leaves their corresponding conditions invariant, i.e. $p$ lies on a common component of $V(f)$ and $V(g)$ if and only if $T(p)$ lies on a common component of $V(T^{-1}f)$ and $V(T^{-1}g)$ ? How can we show this invariance for each of the axioms using the definition of affine transformation and change of coordinates? (i.e. using formulas?) It says briefly (on p. 60) of Kirwan's Complex Algebraic Curves , that Since the conditions are independent of the choice of coordinates we may assume that $p = [0\ 0\ 1]$ . It also says that we can assume that $f$ and $g$ are irreducible by 3. and 4. (using the number of axioms I give below), that $I(p, V(f) \cap V(g))$ is finite by 1. , and that $I(p, V(f) \cap V(g)=k>0$ by 2. , and that by induction on $k$ we may assume that any intersection multiplicity strictly less than $k$ can be calculated using only the conditions 1.-6. (The idea being to show that the intersection multiplicity can always be calculated using only these axioms, and thus that these axioms must determine the intersection multiplicity completely.) Just like in Garrity et al, no careful proof is given of any of these claims. I get the ""idea"" behind all of them, none of them seem shocking and they all seem plausible, but I don't think that I know how to prove them rigorously if I needed to do so. Context: This might be easier using the definition of intersection multiplicity in terms of resultants -- this is not what I am asking about here. I am talking about the axiomatic definition (which turns out to be equivalent to that given in terms of resultants). I am copy-pasting the definition given in Garrity et al, Algebraic Geometry: A Problem Solving Approach p. 138, theorem 3.3.12, although similar definitions can be found on Wikipedia and in Kirwan's Complex Algebraic Curves , Theorem 3.18, p. 59: Given polynomials $f$ and $g$ in $\mathbb{C}[x,y]$ and a point $p$ in $\mathbb{C}^2$ , the intersection multiplicity is the uniquely defined number $I(p, V(f) \cap V(g))$ such that the following axioms are satisfied: 1. $I(p, V(f) \cap V(g)) \in \mathbb{Z}_{\ge 0}$ , unless $p$ lies on a common component of $V(f)$ and $V(g)$ , in which case $I(p, V(f) \cap V(g) = \infty$ . 2. $I(p, V(f) \cap V(g)) = 0$ if and only if $p \not\in V(f) \cap V(g)$ . 3. Two distinct lines meet with intersection multiplicity one at their common point of intersection. 4. $I(p, V(f) \cap V(g)) = I(p, V(g) \cap V(f))$ . 5. $I(p, V(f) \cap V(g)) = \sum r_i s_j I(p, V(f_i) \cap V(g_j))$ when $f= \prod f_i^{r_i}$ and $g = \prod g_j^{s_j}$ . 6. $I(p, V(f) \cap V(g)) = I(p, V(f) \cap V(g+af))$ for all $a \in \mathbb{C}[x,y]$ .","['algebraic-curves', 'algebraic-geometry']"
1951880,"The density of the fractional part of $x^n$ in $(0,1)$?","If $\{ \text{the fractional part of } x^n, n \in \mathbb{N} \}$ is dense in $(0,1)$, what is the range of $x$? Obviously, $[-1,1]$ is not the case, and all integers are not the cases either, what will happen if $x>1$?","['fractional-part', 'real-analysis', 'sequences-and-series', 'exponentiation']"
1951898,"Integrate $\frac{x^3-3xy^2}{(x^2+y^2)^2}$ over $1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4$ and $x,y\geq0.$","Integrate $\frac{x^3-3xy^2}{(x^2+y^2)^2}$ over $1\leq x^3-3xy^2\leq2,2\leq3x^2y-y^3\leq4$ and $x,y\geq0.$ Let $R$ be the region. I substitute $u=x^3-3xy^2,v=3x^2y-y^3$. $\det J(x,y)=9(x^2+y^2)^2.$ Also notice that $(x+yi)^3=u+vi$, so $(x^2+y^2)^3=u^2+v^2$. Then the rquired integral
$$=\int_R\frac{x^3-3xy^2}{9(x^2+y^2)^4}9(x^2+y^2)^2dxdy$$
$$=\int^4_2\int^2_1\frac u{9(u^2+v^2)^{4/3}}dudv$$
$$=\int^4_2\frac 1 6[\frac 1 {(1+v^2)^{1/3}}-\frac 1{(4+v^2)^{1/3}}]dv$$ This has no closed form solution. I wonder where I did wrong... Thank you.","['multivariable-calculus', 'calculus']"
1951946,What is the difference between a tensor product and an outer product?,"I have seen the tensor product written as $$ 
\left( \begin{array}{c}
a \\
b \\
 \end{array} \right) \otimes 
\left( \begin{array}{c}
c \\
d \\
 \end{array} \right) = \left( \begin{array}{c}
ac \\
ad \\
bc \\
bd \\
 \end{array} \right)$$
However I have also seen it written as 
$$ 
\left( \begin{array}{c}
a \\
b \\
 \end{array} \right) \otimes 
\left( \begin{array}{c}
c \\
d \\
 \end{array} \right) =  
\left( \begin{array}{c}
a \\
b \\
 \end{array} \right)  
\left( \begin{array}{c}
c & d \\ \end{array} \right) =   
\left( \begin{array}{c}
ac & ad\\
bc & bd \\\end{array} \right)
$$
Which I have seen in the context of outer products. 
Why are there two ways to do a tensor product?","['tensor-products', 'linear-algebra']"
1952056,Computing the integral $\int \exp(ix^2) dx$,"I'm trying to compute the following integral: $I_1 =  \int_{-\infty}^\infty \exp(iu^2) du$. This is what I did, but it is wrong and I don't know why: $$I_1^2 = \left (\int_{-\infty}^\infty \exp(iu^2) du \right)^2 = \int_{-\infty}^{\infty} \exp(iu^2) du \int_{-\infty}^{\infty} \exp(iv^2) dv = \iint_{\mathbb{R}^2} \exp{i(v^2+y^2)} dA$$ Using polar coordinates: $$I_1^2 = \lim_{c\to \infty} \int_0^{2\pi} d\varphi \int_0^{c} \exp(ir^2) r dr$$ But the last integral doesn't converge. What is wrong here?","['gaussian-integral', 'integration', 'calculus']"
1952070,Proof of a probability inequality claim,"I'm reading a paper which makes an assertion of this form with no proof, where $\mu_S$ is a sample mean random variable, $\mu_T$ is a true or population mean r.v, and $\varepsilon$ is a constant: For every real number $k \in (0,1)$ $$\Pr[|\mu_{S2} - \mu_{S1}| \geq \varepsilon] \leq \Pr[|\mu_{S2} - \mu_{T}| \geq k \varepsilon] + \Pr[|\mu_{T} - \mu_{S1}| \geq (1-k)\varepsilon]$$ This isn't really obvious to me. How do I prove this, or at least satisfy myself that this is true? For anyone interested in the source, this is from Bifet & Gavalda 2006. Appendix A Theorem 1.2.","['probability-theory', 'inequality']"
1952113,Show $\frac{1}{4}\leq \mu(A\cap B \cap C)$,"Let $\{X,\mathcal{A},\mu\}$ be a probability space. Let $A$ , $B$ , $C$ be sets in $\mathcal{A}$ , which satisfy $$\mu (A\cap B)\geq \frac{1}{2}, \quad \mu (A\cap C)\geq \frac{1}{2}, \quad \mu (B\cap C)\geq \frac{1}{2}.$$ Show that $\frac{1}{4}\leq \mu(A\cap B \cap C)$ . I've managed to show in a previous assignment that $$\mu(A\cap B \cap C)\leq \min\{\mu (A\cap B), \mu (A\cap C), \mu (B\cap C)\},$$ but now I'm stuck with the above inequality. I've tried to draw it, but still no luck. Any hints?",['measure-theory']
1952143,$f_n(x)=\sum_{k=0}^n \frac{x^{2k}}{(2k)!}$ converges uniformly on any compact subset of $\Bbb R$,"Consider a sequence of functions on $\Bbb R$ defined by 
$$f_n(x)=\sum_{k=0}^n \frac{x^{2k}}{(2k)!}, n \ge 0.$$
I'd like to show that $\{f_n(x)\}$ converges uniformly on any compact subset of $\Bbb R$. We have $\{f_n(x)\}$ is a monotonically increasing sequence which converges pointwise to a continuous function $f(x)=\frac{e^x+e^{-x}}2$ since 
$$\lim_{n \to \infty}f_n(x)=\sum_{k=0}^\infty \frac{x^{2k}}{(2k)!}=\frac{e^x+e^{-x}}2,$$ by Dini's theorem $\{f_n(x)\}$ converges uniformly. How does the proof look? Is there any better proof?","['uniform-convergence', 'sequences-and-series', 'functions']"
1952156,"$k[[X_1,\ldots,X_n]]$ is UFD for $k$ field","As the title says, I'm trying to prove that $k[[X_1,\ldots,X_n]]$ is UFD for $k$ field. In Lang's Algebra, there is a proof by induction on $n$. The base of induction is clear (we even have discrete valuation ring). Let $R_n = k[[X_1,\ldots,X_n]] \cong R_{n-1}[[X_n]]$ and assume that $R_{n-1}$ is UFD. Let $f\in R_n$, $f(X_1,\ldots,X_n)\neq 0$. Lang now handwaves that one could apply some change of variables to conclude that without loss of generality $f(0,\ldots,0,X_n)\neq 0$. Since $R_{n-1}$ is complete local ring with unique maximal ideal ${\frak m}=(X_1,\ldots,X_{n-1})$, this means that coefficients of $f$ are not all in $\frak m$. This is necessary to apply Weierstrass preparation theorem to conclude that $f = gu$ where $g\in R_{n-1}[X_n]$ and $u$ is invertible in $R_n$. Since $R_{n-1}[X_n]$ is UFD, $g$ can be factored into irreducible polynomials $g = g_1\ldots g_k$. Lang now claims that this gives existence of factorization in $R_{n-1}[[X_n]]$. Question. Why does factorization into irreducibles of $g$ in $R_{n-1}[X_n]$ induce factorization into irreducibles in $R_{n-1}[[X_n]]$? How do we know that $g_i$'s are irreducible in $R_{n-1}[[X_n]]$ (actually they could as well be invertible, which is definitely not a problem, but what in the case that they are not?) I'm probably missing something very obvious here. Bonus question. Is there a better way to prove the theorem?","['abstract-algebra', 'unique-factorization-domains', 'ring-theory', 'commutative-algebra', 'formal-power-series']"
1952177,Do we use compact for $\textbf{sets}$ or for $\textbf{spaces}$?,"Before stating my questions, let me first state my understandings. Note this post is talking on metric spaces case. For the definition of a set $E$ being open in a metric space $(X,d)$, we mean that for every $a\in E$, there exists a ball $B(a,r)=\{x\in X|d(x,a)<r\}$ such that $B(a,r)\subseteq E$. Notice that the ball in the definition is with respect to $(X,d)$ (the ball centered at point $a$ and consist of all point in $X$ such that $d(x,a)<r$), it is quite important. Since every subset of a metric space is always a metric subspace; and every metric subspace is open in itself, by trivial reason. If we consider $E\subseteq X$ to be a metric space, then $E$ is always open in $(E,d)$. For example, $[0,1]$ is not open in $(\mathbb{R},d)$; however, $[0,1]$ is open in $([0,1],d)$. So what metric space the set $E$ is with respect to play an important role when talking whether the set $E$ is open. Now go to the definition of compactness , and also sequentially compact .
It has two variations the definition would make: (For $\textbf{compactness}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every open covering $\mathcal{A}$ of $E$ contains a finite subcollection that also covers $E$, then $E$ is called a $\textbf{compact set}$. Let $(X,d)$ be a metric space. If every open covering $\mathcal{A}$ of $X$ contains a finite subcollection that also covers $X$, then $X$ is called a $\textbf{compact metric space}$. (For $\textbf{sequentially compactess}$) Let $(X,d)$ be a metric space, let $E\subseteq X$, if every sequence of points of $E$ has a convergent subsequence, then $E$ is called a $\textbf{sequentially compact set}$. Let $(X,d)$ be a metric space. If every sequence of points of $X$ has a convergent subsequence, then $X$ is called a $\textbf{sequentially compact metric space}$. I don't know which is the regular definition people use. And also, I'm not sure whether this two versions of definition differs in some tricky circumstance, just like the ""open set"" mentioned above.","['general-topology', 'metric-spaces', 'analysis', 'definition']"
1952192,"Linear, first order ODE, given general solution","Construct a linear first order ODE of $$xy'+a_0(x)y=g(x)$$
such that the general solution is $$y=x^3+\frac{c}{x^3}$$ If I substitute the general solution into the question, 
$$xy'+a_0(x)(x^3+\frac{c}{x^3})=g(x)$$
this form looks unusual to solve and a(x), g(x) are not given in any expression.  how can I to approach this?",['ordinary-differential-equations']
1952200,How to see equivalence of definitions of Lebesgue (outer) measure?,"By definition, the Lebesgue (outer) measure of a set $A \subset \mathbb{R}$ is given by$$m^*(A) = \inf_{\bigcup I_i \supset A} \sum_i m(I_i),$$where the infimum is taken over all countable collections of intervals $I_i$ such that $A \subset I_i$. How do I see that the following definition is equivalent:$$m^*(A) = \inf_{G \supset A} m(G),$$where the infimum is taken over all open sets $G \supset A$?","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1952204,The Chudnovsky pi formula $1/\pi$ revisited,"Define the constants, $$A=163\cdot1114806\\B=13591409\\C=640320$$ Given the binomial coefficient $\binom{n}{k}$ , then we have the pi formulas, $$\frac{1}{\pi} =\frac{12}{(C)^{3/2}}\sum^\infty_{k=0} \frac{(6k)!}{(3k)!\,k!^3} \frac{3Ak+ B}{(-C^3)^k}$$ and $$\frac{1}{\pi} =\frac{12}{(C+4)^{3/2}}\sum_{k=0}^\infty\tbinom{2k}{k}\sum_{j=0}^{k/3} (-1)^j\tbinom{k}{3j}\tbinom{2j}{j}\tbinom{3j}{j}\frac{Ak+B-\color{blue}{1448}/3}{(C+4)^k}$$ $$\frac{1}{\pi} =\frac{12}{(C-4)^{3/2}}\sum_{k=0}^\infty\tbinom{2k}{k}\sum_{j=0}^{k/3} (+1)^j\tbinom{k}{3j} \tbinom{2j}{j}\tbinom{3j}{j}\frac{Ak+B+\color{blue}{1448}/3}{(-C+4)^k}$$ and $$\frac{1}{\pi} =\frac{12}{(C+12)^{3/2}}\sum_{k=0}^\infty\tbinom{2k}{k}\sum_{j=0}^k(-3)^{k-3j}\tbinom{k}{3j} \tbinom{2j}{j}\tbinom{3j}{j}\frac{Ak+B-\color{blue}{1448}}{(-C-12)^k}$$ $$\frac{1}{\pi}=\frac{12}{(C-12)^{3/2}}\sum_{k=0}^\infty\tbinom{2k}{k}\sum_{j=0}^k\,(+3)^{k-3j}\,\tbinom{k}{3j}\tbinom{2j}{j}\tbinom{3j}{j}\,\frac{Ak+B+\color{blue}{1448} }{(-C+12)^k}$$ The first is the Chudnovsky formula, while the rest are also Ramanujan-Sato series (of level 9?). One can give the general form of the Chudnovsky using Eisenstein series. Q: But what yields the blue number $\beta$ ? These are $\beta=4, 24, 76, 1448$ for $d=19,43,67,163$ , respectively. ( Note: Typo corrected. ) P.S. A similar phenomenon happens for the Ramanujan pi formula which uses $d=58$ . I discuss this briefly in my blog Ramanujan Once A Day .","['sequences-and-series', 'modular-forms', 'pi']"
1952224,Proving Ramanujan's formula,"While reading this PDF , they introduced a theorem by Ramanujan: Theorem If $$\alpha^2+\alpha\beta+\beta^2=3\lambda\gamma^2$$ Then $$(\alpha+\lambda^2\gamma)^3+(\lambda\beta+\gamma)^3=(\lambda\alpha+\gamma)^3+(\beta+\lambda^2\gamma)^3$$ I was wondering if there is a simple way to prove this? You could just expand both sides and show that they are equivalent, but that doesn't give me insight to how Ramanujan came up with this formula in the first place. Note: $\alpha,\beta,\gamma,\lambda\in\mathbb{N}$","['algebra-precalculus', 'proof-explanation']"
1952317,Why integral domain is also called entire ring?,"I remember reading somewhere (most probably in Lang's Algebra ) that integral domain in also known by the name ""entire ring"". I was thinking that is it somehow connected with complex analysis, but unfortunately I could not figure out much. I know that if $ \Omega$ is a domain in $\mathbb C$ then $R=\{f: \Omega \to \mathbb C: f \text { is holomorphic}\}$ is an integral domain. Is it true that every integral domain can be obtained as ring of holomorphic function of some domain? Also what might be the possible reason for using the terminology 'entire ring' for integral domain?","['abstract-algebra', 'ring-theory', 'complex-analysis', 'integral-domain']"
1952329,Sum of a series indexed by ordinals,"If $\mu$ is an ordinal, how can we formalize that $$
  \sum_{\lambda<\mu}x_{\lambda}=z
$$ When $\mu=\omega$, this is just the usual infinite series, the partial sums converge to $z$.  What is the definition for higher ordinals? I don't think this is quite the same as an uncountable sum, it is a sum over an well-ordered index. We can define it as a limit points of finite sums, but I was wondering whether there is a ""better"" (possibly equivalent) definition that takes advantage of the extra structure (well-oredring of the index). I suspect this will make sense only if countably many elements are non-zero. As for the context, I guess the most general one could be that of topological vector spaces, so that we have the vector operations and limits.","['functional-analysis', 'real-analysis', 'logic']"
1952330,Prove or disprove: if $R$ and $S$ are two equivalence relations on a set $A$ then $R\cup S$ is also an equivalence relation.,"Does my disproof below suffice to disprove the statement below? Also, does anyone have a simpler way to disprove this? If $R$ and $S$ are two equivalence relations on a set $A$ then $R\cup S$ is also an equivalence relation. Disproof. Suppose $A=\{a,b,c\}$ and $R$ and $S$ are equivalence relations on $A$ . Let \begin{align*}
R&=\{(a,a),(b,b),(c,c),(a,b),(b,a)\} \\
S&=\{(a,a),(b,b),(c,c),(b,c),(c,b)\}
\end{align*} Then $$R\cup S=\{(a,a),(b,b),(c,c),(a,b),(b,a),(b,c),(c,b)\}$$ But then $a(R\cup S)b \land b(R\cup S)c \nRightarrow a(R\cup S)c$ , thus $R\cup S$ is not transitive and therefore not an equivalence relation.","['examples-counterexamples', 'discrete-mathematics', 'equivalence-relations', 'relations', 'solution-verification']"
1952351,Bezier offset self intersections,"An ""offset"", or parallel curve, is ""a curve whose points are at a fixed normal distance from a given curve"" . It might happen that the offset curve intersect itself, as shown here (the most inner green curve). This kind of ""loop"" is called ""local"" and appears when the curvature of given curve is greater than $1/d$ (i.e. the offset distance). I would like to compute the approximate cubic Bezier offset curve/curves to a given cubic Bezier curve ($B$) but the local loops are giving me troubles. I though I would split the original curve at parameters ($t$), where the curvature radius ($r$) falls below the given offset distance. The signed curvature radius can be calculated as: $$r = \frac{\|B'(t)\|^3}{B'(t) \times B''(t)}$$ The problem I have is I don't know how to find those parameter values. I thought I would somehow get some nice polynomial whose roots are the location along the original curve where the splits would occur. But I cannot figure out how to derive this polynomial from the curvature formula or whether it is even possible (there is a polynomial in denominator)? Could someone, please, give me a helping hand here? My question is: where to trim the original curve (into possibly more parts) so that the curvature radius would be at least the offset distance everywhere? Alternatively, there is a paper ""Error Bounded Variable Distance Offset Operator for Free Form Curves and Surfaces"" written by Elber and Cohen. In the chapter 4, it describes a way of trimming off the local loops by usage of tangents but unfortunately I don't really understand it. Perhaps someone could cast some light into this method instead?","['polynomials', 'spline', 'curvature', 'bezier-curve', 'geometry']"
1952354,"Show an $n\times n$ matrix $A$ is sim. to the companion matrix for $p_A(t)\iff \exists$ a vector $x$ such that $x,Ax,\ldots,A^{n-1}x$ is a basis","Show that an $n\times n$ matrix $A$ is similar to the companion matrix for $p_A(t)$ if and only if there exists a vector $x$ such that $$x, Ax, \ldots, A^{n-1}x$$ is a basis for $\mathbb C^n$. The companion matrix for $p(t) = t^n + a_{n-1}t^{n-1} + \cdots + a_1t + a_0$ is given by $$A_p = [e_2 \;|\; e_3 \;|\; \cdots \;|\; e_n \;|\; -{\bf a}]\;\;,\;\; {
\bf a} = (a_0, a_1, \ldots , a_{n-1})^T$$ My attempt: I tried to prove the backwards direction because I had a little bit of an idea on what to do... Because $x, Ax, \ldots, A^{n-1}x$ is a basis for $\mathbb C^n$, we know that $x, Ax, \ldots, A^{n-1}x$ spans $\mathbb C^n$ and is linearly independent with $n$ vectors. Hence, we may create an $n\times n$ matrix $S$ such that $$S = [x \;|\; Ax \;|\; \cdots \;|\; A^{n-1}x],$$ of which by Equivalent Conditions for a Nonsingular Matrix , we have that $S$ is invertible because its columns are linearly independent. So $S^{-1}$ exists. Questions: How can I show that $A = S^{-1}A_pS$? My method might be a lost cause unfortunately, so I apologize if this question is somewhat dumb. I could probably show it by multiplying out $S^{-1}AS$ and showing it equals $A$, but I'm not entirely sure what $S^{-1}$ looks like in terms of its columns I've defined. Thus, the more important question--What does the inverse of $S$ look like exactly given its columns? Is it even possible to say?","['matrices', 'similar-matrices', 'companion-matrices', 'linear-algebra', 'solution-verification']"
1952411,Linearity of Variance,"Is $Var(X + Y) = Var(X) + Var(Y)$ generally, for two random variables $X, Y$?
Is $Var(aX) = a^2Var(X)$ generally? Important:
In which cases, $Var(X + Y) \leq Var(X) + Var(Y)?$ $Var$ = Variance","['statistics', 'probability', 'variance']"
1952421,How to solve this conditional pdf?,"I have a joint distribution$$f_{XY}(x,y) = (2\pi\sigma^2)^{-1}\text {exp}\left(-{{x^2+y^2}\over{2\sigma^2}}\right)$$ I need to calculate $$f_{xy}(x,y|x^2+y^2<b^2)$$ I think I should use bayes theorem transfer it and calculate $$f_{xy}(x^2+y^2<b^2|x,y) \ \ \ \ *$$ what confuses me is if function * equates to $$f_{xy}(x^2+y^2<b^2)$$ If it's valid, then 
$$f_{xy}(x,y|x^2+y^2<b^2)={{f_{xy}(x^2+y^2<b^2|x,y) \cdot f_{xy}(x,y)}\over{f_{xy}(x^2+y^2<b^2)}} = \\ =f_{xy}(x,y)$$ So I think it should be wrong. Then how to explain the condition of function * please? Thank you very much. I have solution: I wonder if $$F_{xy}(x,y|x^2+y^2<b^2) = F_{x}(-\sqrt{b^2-y^2}<x<\sqrt{b^2-y^2} ) + F_{y}(-\sqrt{b^2-x^2} <y<\sqrt{b^2-y^2}) = \\F_{x}(\sqrt{b^2-y^2})-F_{x}(-\sqrt{b^2-y^2})+F_{y}(\sqrt{b^2-x^2})-F_{y}(-\sqrt{b^2-x^2})$$
is correct? it is easy to calculate $f_{x}$ and $f_{y}$, then I can know $F_{x}$ and $F_{y}$, then it is easy to calculate $F_{xy}(x,y|x^2+y^2<b^2)$ like above. Finally what I need to do is differentiate it. But I don't know if it's valid.","['statistics', 'probability', 'probability-distributions']"
1952435,What are function fields?,"Given a field $F$, I know that $F[X]$ is the ring of polynomials in $X$. I know that this is not a field. I have seen the notation for $F(x)$ with round brackets. Usually when we use round brackets we take the smallest field containing $F$ and $x$. For example, if $F(\alpha) = F[\alpha]$ for all algebraic $\alpha \in E$ ($E$ some extension of $F$). My question is what $F(x)$ is when $x$ is a variable. Is this just the quotient of polynomials? What is the definition? Is this what is called a function field.","['abstract-algebra', 'function-fields', 'polynomials', 'field-theory']"
1952440,A question on a proof of the existence of non-measurable sets,"This is an excerpt of Rosenthal's book: ""A First look at Rigorous Probability Theory"" and contains an important theorem in the development of measure theory. While I generally understand the proof well, there is a point that I find confusing, namely why does he take out $0$ from $H$. Could you please enlighten me as to the reason? Thank you. The r-shifts are defined as,","['probability-theory', 'equivalence-relations', 'measure-theory', 'discrete-mathematics']"
1952496,"Convergence in probabiliy, just not getting it","Let $\{X_n,n>1\}$ be a sequence of i.i.d random variables with the probabilty density function $$f(x) =e^{-(x-a)} \text{ for } x\geq a.$$ Set $Y_n = \min\{X_1,X_2,\ldots,X_n\}$. Show that $Y_n$ converges in probability to $a$ as $n \to \infty$ So there's the question! The only info we have is the definition of the convergence in probability.","['statistics', 'probability', 'convergence-divergence']"
1952498,Geometric intuition for directional derivatives,"What I'm trying to do in this post is to see that the intuition I've built is correct, and, if it's not, I would like someone to share its own intuition on why directional derivates are related with the gradient vector. My intuition: The formal definition of a directional derivative is: $$
\frac{\partial f}{\partial \vec{v}} =\nabla f(a,b) \cdot \vec{v}
$$ where $\vec{v}$ is the vector that indicates the direction where we need to compute the rates of change. By the definition of partial derivatives, when we compute $\frac{\partial f}{\partial x}$ , we're fixing a plane in $y$ direction, and just analysing what a tiny change in $x$ effects our output. Same happens in $\frac{\partial f}{\partial y}$ , we fix a plane in $x$ direction, and analyse what a tiny change in $y$ effects our output. Now, when we compute a directional derivate of $\vec{v}$ , what we're doing (in my head) is fixing a plane, $\beta$ , that has $\vec{v}$ as one of it's directional vectors and intersects the surface. Just like the picture below: Because $\beta$ has $\vec{v}$ as one of it's directional vectors, what we're essentially doing is checking what a tiny change in direction of $\vec{v}$ causes to our output (surface). But we already know what a tiny change in $x$ causes and what a tiny change in $y$ causes, respectively, $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ ... Assuming that everything I've said is correct, we can decompose $\vec{v}$ in some linear combination of the basis vector of our space, in this case, the standard basis: $$
\vec{v} = \left[\begin{matrix}
a\\
b\\
c\\
\end{matrix}\right] = a \cdot \left[\begin{matrix}
1\\
0\\
0\\
\end{matrix}\right] + b \cdot \left[\begin{matrix}
0\\
1\\
0\\
\end{matrix}\right] + c \cdot \left[\begin{matrix}
0\\
0\\
1\\
\end{matrix}\right]
$$ Ignoring the third vector $\left[\begin{matrix}
0\\
0\\
1\\
\end{matrix}\right]$ because it deals with our output, we can see $\left[\begin{matrix}
1\\
0\\
0\\
\end{matrix}\right]$ and $\left[\begin{matrix}
0\\
1\\
0\\
\end{matrix}\right]$ as some change in $x$ and $y$ direction, that are computed by their partial derivatives, and we're looking on what a tiny change in $\vec{v}$ direction causes to our output, hence: $$
\frac{\partial f}{\partial \vec{v}} = a \cdot \frac{\partial f}{\partial x} + b \cdot \frac{\partial f}{\partial y}\\
\frac{\partial f}{\partial \vec{v}} = \nabla f(a,b) \cdot \vec{v}
$$ Am I correct?","['multivariable-calculus', 'partial-derivative', 'derivatives']"
1952557,Continuous solution of ODE,"The question is to find a continuous solution to the ODE: $$(1+x^{2})y' +2xy = f(x)$$
with the intial condition $y(0)=0$,
and where $$f(x)= \begin{cases}
x & 0 \leq x<1 \\
-x & x \geq 1.
\end{cases}
$$
I split up this question into cases. For case 1, $f(x)=x$ and case 2 $f(x)=-x$. I then solved the ODE for each separate case. Case 1 I solved using integrating factors to get: $$y_1=\frac{x^2}{2(1+x^2)}.$$ Case 2 I used the same method to get: $$y_2=\frac{-x^2}{2(1+x^2)}.$$ For both the cases my c-value calculated from the initial condition was 0. However when I look at the graph of these two functions for the interval $0$ to infinity, they are discontinuous. I don't think I did the ODE wrong as I double checked. Whats up here? If I use the condition of $y_1(1)=y_2(1)$ Then I will get $y_1(1)=1/4$. However just looking at the equation of y2, it doesn't seem like its possible to get a positive number.",['ordinary-differential-equations']
1952598,"Proving algebraically $a^2+b^2\ge a^{\alpha}b^{2-\alpha}$ for $0\le\alpha\le2$ and $a,b\ge0$","My Analysis professor showed this inequality and elegantly proved it using polar coordinates, saying that it can't be done algebraically. Instead, here's how I think I have handled it: firstly we see it's true for $ab=0$; dividing by the RHS we get $$\left(\frac{a}{b}\right)^{2-\alpha}+\left(\frac{a}{b}\right)^{-\alpha}\ge1, $$ or equivalently, setting $t=a/b$, $$t^2+1\ge t^\alpha$$ which holds because the LHS is $\ge t^2\ge t^\alpha$ for $t\ge1$ and $\ge1\ge t^\alpha$ for $0\le t<1$. Am I missing something? Are there fancy algebraic (perhaps some linear algebra inequalities) ways to prove the inequality?","['alternative-proof', 'real-analysis', 'inequality', 'linear-algebra']"
1952632,Laurent series for $\cot (z)$,"I'm looking for clarification on how to compute a Laurent series for $\cot z$ I started by trying to find the $\frac{1}{\sin z}$. I've found multiple references that go from an Taylor expansion for $\sin z$ directly to an expression for $\frac{1}{\sin z}$ but I am unable to follow how they got there. This thread Calculate Laurent series for $1/ \sin(z)$ started to answer my question but I do not understand how to use the given formulas to ""iteratively compute"" the coefficients, and the example given has several coefficients in place and I'm not sure how they were obtained.","['laurent-series', 'complex-analysis', 'taylor-expansion', 'residue-calculus']"
1952659,what areas of pure math are related to control and systems engineer [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Questions about choosing a course, academic program, career path, etc. are off-topic. Such questions should be directed to those employed by the institution in question, or other qualified individuals who know your specific circumstances. Closed 7 years ago . Improve this question i am an undergraduate control and system engineer undergraduate i had previous interest in mathematics now i want to study pure mathematics in conjunction with control engineering but it would be better for me if the pure math courses are
related to control and systems so what are these areas that will be most related to my control engineering major also i wanted to note that my control and system major involves numerous communications courses.
note:
i have taken calculus differential equations courses up till now in my engineering major.","['first-order-logic', 'number-theory', 'game-theory', 'logic', 'elementary-set-theory']"
1952688,"If A and B are both invertible, then $[(A^{-1})B]^T$ is invertible.","If $A$ and $B$ are both invertible, then $[A^{-1}B]^T$ is invertible. Is this true or false? I have tried coming up with a proof but I don't really know where to start.","['matrices', 'transpose', 'linear-algebra', 'inverse']"
1952769,Drawing two cards from a deck of 16 (4 ranks and 4 suits),"I'm reviewing for an exam and have come across a problem marked incorrect on my homework. The problem reads, There are 16 cards in a deck. The cards have 4 ranks (Jack, Queen, King, and Ace) and 4 suits (Clubs, Diamonds, Hearts, and Spades). You are dealt two cards. What is the probability you get a Diamond card? I misread this question when I first asnwered it, and I'm unsure how to get the correct solution. The solution page says that the solution is $\frac{9}{20}$. What is the probability you get two cards of the same rank? I said that once the first card is drawn, you'll have three remaining cards with that same rank out of a total of 15 cards, so you have a $\frac{3}{15} = \frac{1}{5}$ chance. This answer was the same as the solution manual. Is my logic correct? What is the probability you don't get a Diamond card? This is just 1 - (the solution to part a) = $\frac{11}{20}$. What is the probability you don't get a Diamond card and you get two cards of the same rank? Let A be the event you don't get a Diamond card. Let B be the event you get two cards of the same rank. $P(A' \cup B) = P(B) - P(A \cap B) = \frac{1}{5} - \frac{1}{10} = \frac{1}{10}$ Is there an easier way to think about this? I believe I have the most misunderstanding on the first question. Thank you for any assistance.",['probability']
1952774,It is possible to apply a derivative to both sides of a given equation and maintain the equivalence of both sides? [duplicate],"This question already has answers here : Can we differentiate equations without changing the solutions? (3 answers) Closed 7 years ago . I have an equation with this shape, where $k,t_1,r \in \Bbb N$: $$2^{(x+1)^2}=k+t_1(x^2+r)$$ And I noticed that I can find $t_1$ in terms of $x$ as follows: $$ \frac{{\rm d} }{{\rm d}x} 2^{(x+1)^2} = \frac{{\rm d} }{{\rm d}x} \left( k+t_1(x^2+r) \right)$$ And then I would continue as follows: $$ 2(x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_12x$$ $$ (x+1)\cdot2^{(x+1)^2}\cdot\ln(2) = t_1x$$ So finally: $$ t_1=\frac{(x+1)\cdot2^{(x+1)^2}\cdot\ln(2)}{x}$$ Is it possible to use a derivative in such case? I think that if both functions $2^{(x+1)^2}$ and $t_1(x^2+r)$ are equivalent and the derivative exists it might be possible, but I am not very sure about the validity of that step. Thank you!","['derivatives', 'calculus']"
1952785,How to get solutions in elementary functions to the following non-linear ODE,$y\frac{\text{d}^2y}{\text{d}x^2}-2\left(\frac{\text{d}y}{\text{d}x}\right)^2+xy^3\frac{\text{d}y}{\text{d}x}=0$. I want to get solution in elementary functions in order to do asymptotic analysis. I tried the problem with Mathematica. It did not give any solutions.,['ordinary-differential-equations']
1952831,How to formally prove that we cannot find a polynomial in $\textbf Z[x]$ with degree $2$ with such a root?,I am trying to find the kernel of the map from $\textbf Z[x]$ to $\textbf C$. The map is evaluating at $\sqrt 2 + \sqrt 3$. A solution says that we cannot find polynomials of degree $2$ or $3$ that has such a root. So it skips the procedure of trying degree $2$ and $3$. And the final solution is the ideal in $\textbf Z[x]$ generated by $$x^4 − 10x^2 + 1 = (x − \sqrt 2 − \sqrt 3)(x − \sqrt 2 + \sqrt 3)(x + \sqrt 2 − \sqrt 3)(x + \sqrt 2 + \sqrt 3)$$ A solution says there are $\sqrt 6$'s in $(\sqrt 2 + \sqrt 3)^2$ and $(\sqrt 2 + \sqrt 3)^3$. So we cannot find polynomials with root $\sqrt 2 + \sqrt 3$ of degree $2$ or $3$. And that is what I am confused about: How does it imply the fact? Thanks in advance!,"['abstract-algebra', 'ring-theory', 'polynomials', 'proof-explanation']"
1952874,"Find all pairs $(a,b)\in \mathbb{R^2}$ such that $af_n^2+bf_{n+1}^2 $ is a member of the Fibonacci sequence for all $n\in\mathbb{N}$.","Question : Let$\{f_n\}$ be the Fibonacci sequence $\{1,1,2,3,...\}$. Find all pairs $(a,b)\in \mathbb{R^2}$ such that $af_n^2+bf_{n+1}^2 $ is a meber of the sequence for all $n\in\mathbb{N}$. I used  $f_n=\frac {1}{\sqrt 5}[\phi^n-(-1)^n\phi^{-n}]$ with $\phi=\frac {1+\sqrt 5}{2}$. To reduce the given expression but then was not getting any helpful ideas to  proceed. So any help/hint will be appreciated. Can we find such $a,b$ so that $af_n^k+bf_{n+1}^k$ for some $k>2$ in positive integers? Thank You.","['fibonacci-numbers', 'algebra-precalculus', 'number-theory', 'sequences-and-series', 'elementary-number-theory']"
1952892,The Perfect Sharing Algorithm (ABBABAAB...),"Less of a question and more of an exercise, it has to do with something I found while doing some programming and being unable to find things. Basically I wanted a formula for the perfect sharing algorithm as I call it,(ABBABAABBAABABBA...), I don't know the proper name of the sequence but it's used for truly fair sharing between 2 people. I couldn't find a formula so I figured this out after a while. Start with AB Every A turns into an AB and every B turns into a BA. AB -> ABBA -> ABBABAAB ... This allowed me to get a computer to achieve the algorithm in many ways, but it also raised some questions. Question 1: Can I repeat this forever and have it properly generate the algorithm? The algorithm is normally created by taking AB, then inverting each 2-state 'digit' and sticking it on the end (ABBA). You then take this entire sequence and repeat the process (ABBABAAB). This is an infinite sequence. Is what I'm doing going to generate the same sequence as the second method? Question 2. 2 people decide they want to share a task, so they use this algorithm. a) If they know how many turns have occurred but forget who's turn it is, can they generate an equation that tells them who's turn it is given the number of turns that have passed? b) The 2 people forget where they are in the sequence, but they know who's turn it is right now. How many previous turns will they need to remember in order to find their place again under the worse possible scenario?","['recurrence-relations', 'infinite-groups', 'sequences-and-series', 'fractals']"
1952894,On Borel-measurable discontinuous functions,"Let $g:\mathbb{R}\to[0,1]\subset\mathbb{R}$ be a Borel-measurable function such that $g$ is $1$-periodic (i.e. $g(x+1)=g(x)$ for all $x\in\mathbb{R}$) and for any $a,b\in\mathbb{R}$ with $a<b$ and any $c,d\in[0,1]$ with $c<d$ the set $\{x\in[a,b]: g(x)\in[c,d]\}$ has positive Lebesgue measure. Question 1: Does a function $g$ as described above even exist? Question 2: If the answer to Question 1 is `yes', then is it true that for almost every $\beta\in[0,1]$ the function $f_\beta:\mathbb{R}\to[-1,1]$ given by $f_\beta(x):=g(x+\beta)-g(x)$ is not continuous? Thanks for the help!","['continuity', 'measure-theory']"
1952900,Nehari Manifold properties: Show that Nehari manifold is bounded away from zero.,"Let $E$ be a real Banach space and $I\in\mathcal{C}^1(E;\mathbb{R})$ a functional. Define the Nehari manifold 
\begin{align}
\mathcal{N}=\{u\in E\backslash\{0\}:I'(u)u=0\},
\end{align}
where the Frechet derivative of $I$ at $u$, $I'(u)$, is an element of the dual space $E^*$, and we denote $I'(u)$ evaluated at $v\in E$ by $I'(u)v$. Suppose $u\neq 0 $ is a critical point of $I$, i.e., $I'(u)=0$. Then $u\in\mathcal{N}$. Set $S=S_1(0)=\{u\in E:||u||=1\}$ and assume $I(0)=0$. Suppose the following two conditions are true: For all $u\in E\backslash \{0\}$ there is a $t_u$ such that if $\phi_u(t)=I(tu)$, then $\phi'_u(t)>0$ for all $0<t<t_u$ and $\phi'_u(t)<0$ for all $t>t_u$, There exists a $\delta>0$, independent of $u$, such that $t_u\geq\delta$ for all $u\in E$. The Nehari manifold has some useful properties as indicated in the references provided here: Question about Nehari manifold . I am in particular interested in justifying that 1 and 2 implies that $\mathcal{N}$ is bounded away from $0$, i.e., for every $u\in\mathcal{N}$ there is a $\rho>0$, independent of $u$, such that $||u||\geq\rho$. I am not sure how to approach the problem.","['functional-analysis', 'calculus-of-variations', 'sobolev-spaces', 'partial-differential-equations']"
1952949,Almost sure convergence of $A_n \to \infty$ implies $P(A_n<N \text{ i.o.})=0$,"Given $A_n\rightarrow \infty$ almost surely (a.s). Show that $\forall\  N > 0$, $P\left\{A_n<N\  \text{infinitely often}\right\} = 0$. My thought: By the sake of contradiction, assume there exists an $\infty> N >0$ such that $P\left\{A_n<N\  \text{infinitely often}\right\} > 0$. This is equivalent to: $\lim_{n\rightarrow \infty} P\left\{\cup_{k\geq n} (A_k-N) < 0\right\} > 0$. But this means there exists a $k_1\geq n$ such that $A_{k_1} < N$ (1) On the other hand, since $A_n\rightarrow \infty$ a.s, for every  $\epsilon > 0$, $\lim_{n\rightarrow \infty} P\left\{\cup_{k\geq n} |A_k- \infty| \geq \epsilon\right\} = 0$. This means $|A_k-\infty|<\epsilon$ for every $k\geq n$. Since $k_1\geq n$, $\infty - A_{k_1} < |A_k- \infty| < \epsilon$ for any $\epsilon > 0$. Let $\epsilon = N$, we get $\infty < N+A_{k_1} < 2N$ (due to (1) ). This is a contradiction, since $\infty > N$. My question: I don't find my mathematical notation above legit, because I am doing subtraction with $\infty$. But I would like to know if my solution above is correct, so could someone please help verify? Also, I think there should be a shorter/slicker way to solve it, as my solution above is quite complicated and use more of real analysis. Anyone wants to give it a try? Any thoughts about my solution or this problem would be really appreciated anyway.","['real-analysis', 'limsup-and-liminf', 'probability-theory', 'borel-cantelli-lemmas', 'measure-theory']"
1953039,show that $X/(X+Y) $ has a cauchy distribution if $X $ and $X+Y $ are standard normal,"A random variable $X$ has a cauchy distribution with parameters $a$ and $b$ is the density of $X$ is $f(x\mid a,b)=\dfrac{1}{\pi b}\dfrac{1}{1+(\frac{x-a}{b})^2}$ where $-\infty <x< \infty $, $-\infty <a <\infty$, $b>0$ Suppose $X$ and $Y$ are independent standard normal random variables then show that $X/(X+Y)$ has a cauchy distribution. Since $X$ and $Y$ are independent standard normal random variables then the joint pdf for $(X,Y)$ is $$f_{X,Y}(x,y)=\frac{1}{2\pi}e^{-x^2/2}e^{-y^2/2}$$ I used the Jacobian method to try and find $f_{U,V}$ so I let $U=X+Y$ and $V=X/(X+Y)$ so $x=uv$ and $y=u-uv$ and then $J=u$. So $f_{U,V}(u,v)=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u-uv)^2/2}=\dfrac{u}{2\pi}e^{-(uv)^2/2}e^{-(u^2-2u^2v + u^2v^2)/2}=\dfrac{u}{2\pi}e^{-(u^2v^2)/2}e^{(-u^2/2)+(u^2v) - (u^2v^2/2)}= \dfrac{u}{2\pi}e^{-(u^2v^2)-(u^2/2)+(u^2v)} = \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} $ Therefore $$f_{U,V}(u,v)= \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))} ; -\infty<u,v<\infty$$ I now want to find the $V$ marginal density and that should have a cauchy distribution. $f_V(v)=2\int_0^{\infty} \dfrac{u}{2\pi}e^{-u^2(v^2-v+(1/2))}du=\int_0^{\infty} \dfrac{u}{\pi}e^{-u^2(v^2-v+(1/2))}du$. If I let $s=u^2$ then $\dfrac{ds}{2u}=du$ and then integral then becomes $ \int_0^{\infty} \dfrac{1}{2\pi}e^{-s(v^2-v+(1/2))}ds = \dfrac{1}{2\pi}\bigg( \dfrac{1}{v^2-v+(1/2)} \bigg)$. I think I might have made a mistake somewhere because I cant see how I can rearrange this to show that $f_V$ is a cauchy distribution.","['statistics', 'transformation', 'normal-distribution']"
1953048,$\frac{a_n}{b_n}=\sum_{k=1}^{n} \frac{1}{k}$ are in their lowest terms.,"Let $$\frac{a_n}{b_n}=\sum_{k=1}^{n} \frac{1}{k}$$ be in their lowest
  terms (i.e. $\gcd(a_n,b_n)=1$). Prove the following: there are infinite $n$'s such that $b_{n+1}<b_n$. there are infinite $n$'s such that $a_n$ is not a power of a prime. I have not idea how to approach this problem. Any hint will be appreciated.","['number-theory', 'contest-math', 'elementary-number-theory']"
