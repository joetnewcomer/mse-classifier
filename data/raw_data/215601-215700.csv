question_id,title,body,tags
4375994,Show that $\pi =3\arccos(\frac{5}{\sqrt{28}}) + 3\arctan(\frac{\sqrt{3}}{2})$,"Question: Show that, $$\pi =3\arccos(\frac{5}{\sqrt{28}}) +
 3\arctan(\frac{\sqrt{3}}{2}) ~~~~~~ (*)$$ My proof method for this question has received mixed responses. Some people say it's fine, others say that it is a verification, instead of a proof. Proof: $$\pi =3\arccos(\frac{5}{\sqrt{28}}) + 3\arctan(\frac{\sqrt{3}}{2})\iff \frac{\pi}{3} = \arccos(\frac{5}{\sqrt{28}}) + \arctan(\frac{\sqrt{3}}{2}) 
$$ $$\iff \frac{\pi}{3} = \arctan(\frac{\sqrt{3}}{5})+\arctan(\frac{\sqrt{3}}{2})$$ As $\arccos(\frac{5}{\sqrt{28}})=\arctan(\frac{\sqrt{3}}{5})$ The plan now is to apply the tangent function to both sides, and show that LHS=RHS using the tangent addition formula to expand it out. I.e. $$\tan(\frac{\pi}{3}) = \tan\bigg(\arctan(\frac{\sqrt{3}}{5})+\arctan(\frac{\sqrt{3}}{2}\bigg)$$ $$\iff \sqrt{3} = \frac{\frac{\sqrt{3}}{5}+\frac{\sqrt{3}}{2}}{1-\frac{\sqrt{3}}{5} \frac{\sqrt{3}}{2}}$$ and the RHS will reduce down to $\sqrt{3}$ . Hence LHS=RHS. Some things that I've noticed about this method of proof: It could be used to (incorrectly) prove that $$\frac{\pi}{3}+\pi = \arccos(\frac{5}{\sqrt{28}}) + \arctan(\frac{\sqrt{3}}{2})$$ So because this method of proof can be used to prove things true, that are obviously false, that means it can't be used? Instead of proving (*), wouldn't this method of proof actually prove that? $$\arccos(\frac{5}{\sqrt{28}})+\arctan(\frac{\sqrt{3}}{2})=\frac{\pi}{3} + \pi k$$ for some $k\in \mathbb{Z}$ which we must find. In this case being when $k=0$ .","['algebra-precalculus', 'solution-verification', 'trigonometry']"
4376060,Is it possible to measure the length of the Lie Bracket?,"If $X,Y$ are (local) vector fields on a Riemannian manifold $(M,g)$ . Is there any bound, formula, estimate,... for the length of the Lie bracket, i.e., $g([X,Y],[X,Y])$ ? For example, consider the $n$ -dimensional sphere $\mathbb{S}^n$ , and a local orthonormal frame $\{e_i\}$ , i.e., $g(e_i,e_j)=\delta_j^i $ . What can we say about $\|[e_i,e_j]\|^2$ ? Is it possible to find an orthonormal frame such that $\|[e_i,e_j]\|^2=1$ for every $i,j$ ?","['lie-derivative', 'riemannian-geometry', 'differential-geometry']"
4376070,How can the Chern-Simons form trivialize a non-trivial characteristic class?,"I have a very basic confusion about Chern-Simons forms: On Wikipedia and other sources, it is stated that the Chern-Simons 3-form $$Tr(A\wedge dA+\frac23 A\wedge [A\wedge A])$$ trivializes $Tr(F^2)$ , where $$F=dA+[A\wedge A]$$ is the curvature 2-form of the connection 1-form $A$ of a Lie-group principle bundle. Now, $Tr(F^2)$ is proportional to the second Chern character, $$\frac12 c_1^2-c_2$$ which is a characteristic class, i.e., a cohomology class in $H^4(BU(n), \mathbb{Q})$ . More specifically, we can imagine $Tr(F^2)$ as a specific representing cocycle of this class, which is pulled back via the classifying map of the principle bundle from the base manifold to $BU(n)$ (chosen some $U(n)$ representation of the Lie group $A$ takes values in). A locally computable trivialization of $Tr(F^2)$ should arise from a trivialization of the corresponding cocycle on $BU(n)$ , also by pullback via the classifying map. However, since $\frac12 c_1^2-c_2$ is a non-trivial characteristic class, $Tr(F^2)$ shouldn't have any locally computable trivialization. So where is my misconception here? Is it that even though $c_1$ and $c_2$ are non-trivial as $\mathbb{Z}$ -valued characteristic classes, the Chern character is trivial as a rational characteristic class? Or is it that if we consider those characteristic classes applied to principle bundles instead of vector bundles as usual, they become trivial? Or has it something to do with the choice of $U(n)$ representation of the concerned Lie group? Or am I misinterpreting the statement that the Chern-Simons form trivializes the Chern character?","['classifying-spaces', 'de-rham-cohomology', 'homology-cohomology', 'characteristic-classes', 'differential-geometry']"
4376158,Prove these two projections are equivalent,"Problem: Let $e_1, e_2, f_1$ and $f_2$ be projections of $M$ which is a Von Neumann algebra, such that $e_1e_2=f_1f_2=0$ . If $e_1+e_2=f_1+f_2$ , $e_1 \sim e_2$ and $f_1 \sim f_2$ , prove that $e_1 \sim f_1$ . My attempt: Suppose $e_1$ and $f_1$ are not equivalent then by the comparison lemma there is a central projection $p$ such that either $pe_1\prec pf_1$ or $pf_1\prec pe_1$ (In this notation, for example $pe_1\prec pf_1$ means $pe_1$ is equivalent to a subprojection of $pf_1$ but $pe_1$ and $pf_1$ are not equivalent). Let's assume $pe_1\prec pf_1$ . Then we have: $$pe_2 \sim pe_1\prec pf_1\sim pf_2.$$ Thus $pe_2\prec pf_2$ . Moreover, since $p$ is central and $e_1e_2=f_1f_2=0$ , we infer that $pe_1+pe_2\preceq pf_1+pf_2$ . I wonder if we are allowed to replace $\preceq$ with $\prec$ in the last relation. At this stage, I don't know how to arrive at a contradiction while we already have $pe_1+pe_2=pf_1+pf_2$ . Any help would be highly appreciated.","['von-neumann-algebras', 'c-star-algebras', 'operator-theory', 'banach-algebras', 'functional-analysis']"
4376162,Complement of a neighborhood of a smooth conic in $\Bbb CP^2$,"In p.4 of this pdf: https://www.intlpress.com/site/pub/files/_fulltext/journals/pamq/2008/0004/0002/PAMQ-2008-0004-0002-a001.pdf , there is the following paragraph and I have some questions about it. Let $C \subset \Bbb CP^2$ be a smooth conic and $C \subset N$ a regular neighborhood with boundary $L$ . Since the normal bundle of $C$ in $\Bbb CP^2$ has degree $4$ , we see that $L$ is a $\Bbb Z/4$ -quotient of $S^3$ . Set $M := \Bbb CP^2 \setminus \text{Int} N$ . $M$ is a rational homology ball with $Ï€_1(M) = \Bbb Z/2$ which bounds $L$ . I know that $C$ is homeomorphic to $S^2$ and that the normal bundle of $C$ in $\Bbb CP^2$ has degree 4, but why is $L$ a $\Bbb Z/4$ quotient of $S^3$ ? How do we know that $M$ is a rational homology ball with $\pi_1(M)=\Bbb Z/2$ ?","['complex-geometry', 'algebraic-geometry', 'homology-cohomology', 'low-dimensional-topology', 'algebraic-topology']"
4376205,"If $H$ and $K$ are non-isomorphic but of same order, can there be $\varphi_1, \varphi_2$ such that $N\rtimes_{\varphi_1}H\cong N\rtimes_{\varphi_2}K$?","Let $N$ , $H$ and $K$ be finite groups, such that $|H| = |K|$ and $H$ and $K$ being non-isomorphic. Can there exist $\varphi_1$ and $\varphi_2$ such that $ N \rtimes_{\varphi_1} H \cong N \rtimes_{\varphi_2} K $ ? Naming $ G_1 := N \rtimes_{\varphi_1} H$ and $ G_2 := N \rtimes_{\varphi_1} K$ , I know that $G_1$ and $G_2$ respectively contain copies of $H$ and $K$ . I think in some special cases it is possible that say if $K \leqslant G_1$ , then $G_1$ would have had a greater order because of containing both $H$ and $K$ . Also I see that whatever $\varphi_1$ and $\varphi_2$ could be, they cannot both be trivial homomorphisms since then the semidirect product is a direct product and it's clear that $ N \times H \not\cong N \times K$ . But I wonder if in general we can prove that no such pair $\varphi_1$ and $\varphi_2$ exists for any arbitrary choice of $N$ , $H$ and $K$ .","['semidirect-product', 'group-theory', 'group-isomorphism']"
4376263,Prove that a set is infinite if and only if it is equipotent to a proper subset.,"Prove that a set $X$ is infinite if and only if it is equipotent to a proper subset $H$ . Proof. Suppose $X$ is equipotent to a proper subset $H$ . Then there exists a bijection $f: H \to X$ . Toward a contradiction, suppose $X$ is finite. Since $X$ is finite, $H$ is finite. Moreover, we can write $$X = \{x_1, x_2, \dots, x_n\} \hspace{1cm} H = \{h_1, h_2, \dots, h_m\}$$ Since $f$ is injective, $h_j \neq h_k$ implies $f(h_j) \neq f(h_k)$ . But recall that $H$ is proper, so $\exists \, x_i \in  X \setminus H$ . But at this point, I get stuck. Intuitively, I understand that this can not be a bijection, but I'm having trouble showing this. I have already read Proof that a set is infinite if and only if it has an infinite proper subset and this post does not provide an actual proof answer or assumes that the set is countable.","['elementary-set-theory', 'set-theory']"
4376264,Proving that a formula defines an explicit bijection $\mathbb{N}^2 \to \mathbb{N}$,"I'm wondering if the following argument is sufficient to prove that the below map of sets defines a bijection. Define $f: \mathbb{N}^2 \to \mathbb{N}$ by $f(a,b) = 2^a \cdot (2b - 1)$ . By the fundamental theorem of arithmetic, any $n \in \mathbb{N}$ can be written uniquely, up to ordering of factors, as a product of primes. Namely, $n = \prod\limits_{i=1}^n p_i^{e_i}$ . If $p_j = 2$ for some $j$ , then we take $a = e_j$ ; otherwise, $a = 0$ . Then the product of the remaining primes is odd, and hence is equal to $2b - 1$ for some $b \in \mathbb{N}$ . Then $f(a,b) = n$ . Since this factorization into primes is unique, $f$ is injective. Is this sufficient?","['elementary-set-theory', 'solution-verification']"
4376275,Why does convergence in distribution imply tightness,"A common proof for this proposition is the following: Fix $\epsilon>0$ and $M_o>0$ . There exists an N such that $P(|X_n|\geq M_o) \leq \epsilon$ for all $n\geq N$ . For the rest of the $X_n$ where $n<N$ (finitely many) we find $M_n$ such that $$P(|X_n|\geq M_n) \leq \epsilon$$ We then take the maximum of all the $M_n$ $n=0,1,2,..N-1$ and we get the constant for tightness or bounded in probability.
My confusion is with proving the second statement of the proof. I have seen some cite Portmanteau lemma as justification and others mention points of continuity of the cdf.
Thank you.","['probability-theory', 'asymptotics']"
4376292,Asymptotic equivalence with the Notable Limit of Natural Logarithm,"I hope this is not too easy of a question Basically I've been fiddling with Asymptotic Equivalences to solve limits in a faster and less prone to error way, and I am facing the limit: $$
\lim_{x\to0}{\frac{e^x-\ln(e+x)}{x^3-2x}}
$$ The approach I wanted to use is that of substitution via Asymptotic Equivalence, aiming at substituting $e^x-1$ with $x$ and $\ln(e+x)$ . My idea was to add and subtract $1$ in the numerator in order to obtain the right situation to substitute $e^x-1$ with $x$ ; this decision probably isn't the best one to solve this limit in a definite manner since I'll remain with $\frac{1}{x^3-2x}$ that is of indefinite form but as I was trying out this path I started asking myself if it was possible to substitute $\ln{e+x}$ via Asymptotic Equivalences. I was thinking that maybe $$
\ln{\frac{1+f(x)}{f(x)}}=1\implies \ln{\frac{e+x}{x}}=e;
$$ am I completely wrong? If not can one of you point me to a resource (be it online or a book) in which I can delve deeper into Notable Limits and Asymptotic Equivalences?","['limits', 'calculus', 'asymptotics']"
4376338,Let $A$ be abelian of order $m^2$ with $|A[d]|=d^2$ for every $d\mid m$. Prove $A\cong \Bbb{Z}/m\Bbb{Z}\times\Bbb{Z}/m\Bbb{Z}$,"While studying elliptic curves I was trying the following problem related with group theory: Let $A$ be an abelian group of order $m^2$ with $|A[d]|=d^2$ for every $d\mid m$ . Prove that $A\cong \mathbb{Z}/m\mathbb{Z}\times\mathbb{Z}/m\mathbb{Z}$ $A[d]$ denote the set of points of $A$ with order dividing $d$ . If $m=p$ prime then either $A\cong \mathbb{Z}/p^2\mathbb{Z}$ or $A\cong \mathbb{Z}/p\mathbb{Z}\times\mathbb{Z}/p\mathbb{Z}$ , but $|A[p]|=p^2$ implies that there are $p^2$ elements of order $p$ and thus $A\cong \mathbb{Z}/p\mathbb{Z}\times\mathbb{Z}/p\mathbb{Z}$ , since in $\mathbb{Z}/p^2\mathbb{Z}$ there are $\varphi(p^2)=(p-1)p$ elements of order $p$ . In a similar way, using the theorem of classification of finite abelian groups we can prove it for $m=p_1p_2$ with $p_1,p_2$ distinct prime numbers. What about the general case? For any given $m$ we can solve it by checking cases, but if $m=p_1^{a_1}\dots p_n^{a_n}$ with $p_1,\dots,p_n$ there are many options that checking the cases one by one seems not a reasonable option. My question is if there are some clever argument that solves the general case withouth checking the cases by hand. Thanks for your help.","['abelian-groups', 'group-theory', 'torsion-groups', 'finite-groups']"
4376404,Limit of a series with interchanging signs,"Let $S_x=\displaystyle\sum_{n=1}^{\infty}(-1)^{n+1}(1-\frac{1}{x^n+1})$ . The problem is to find the following limit: $\displaystyle\lim_{x\to1^{-}}S_x$ . I've tried to tackle the problem in several methods so far. Proving  that this series is convergent is fairly easy. For evaluating the limit, I've tried bounding the sum and found out that answer is between $0,\frac12$ . Here is a sketch of the method: $$\forall x<1 : S_x=\displaystyle\sum_{n=1}^{\infty}(-1)^{n+1}(1-\frac{1}{x^n+1})=\displaystyle\sum_{n=1}^{\infty}\bigg(\frac{1}{x^{2n}+1}-\frac{1}{x^{2n-1}+1}\bigg)$$ $$=\displaystyle\sum_{n=1}^{\infty}\frac{x^{2n-1}-x^{2n}}{(x^{2n-1}+1)(x^{2n}+1)}=(1-x)\displaystyle\sum_{n=1}^{\infty}\frac{x^{2n-1}}{(x^{2n-1}+1)(x^{2n}+1)}$$ Hence, $S(x)$ is clearly positive. For the upper bound: $$\frac{x^{2n-1}}{(x^{2n-1}+1)(x^{2n}+1)}\le x^{2n-1}$$ $$\implies \displaystyle\sum_{n=1}^{\infty}\frac{x^{2n-1}}{(x^{2n-1}+1)(x^{2n}+1)}\le\displaystyle\sum_{n=1}^{\infty}x^{2n-1}=1+x\cdot\displaystyle\sum_{n=1}^{\infty}x^{2n}$$ $$=1+x\cdot(\frac{1}{1-x^2}-1)=(1-x)+\frac{x}{1-x^2}$$ $$\implies S_x\le(1-x)\displaystyle\sum_{n=1}^{\infty}\frac{x^{2n-1}}{(x^{2n-1}+1)(x^{2n}+1)}$$ $$\le(1-x)\cdot((1-x)+\frac{x}{1-x^2})=(1-x)^2+\frac{x}{1+x}$$ Which approaches $\frac12$ as $x$ gets closer to $1$ . But clearly, this is not enough to evaluate the desired limit. I'm guessing (by the numerical tests) that the answer is zero. Either I shall take another approach, or the bound shall be refined. I couldn't take the problem any further from here. Any help would be appreciated!","['power-series', 'limits', 'sequences-and-series']"
4376406,Why does the definition of limits of a function have strict inequality?,"Definition (As written in Michael Spivak's Calculus) The function $f$ approaches a limit $l$ near $a$ means: for every $\epsilon >0$ there is some $\delta > 0$ such that, for all $x$, if $0<|x-a|<\delta$, then $|f(x)-l|<\epsilon$. my question is: why can't it be: $$0<|x-a|\leq \delta,|f(x)-l|\leq \epsilon$$
After looking at limits of functions for a long time just to grasp it's meaning and using the definition quite a lot solving homework I realized I keep writing the same inequality without really understanding why. The only explanation given in Spivak's book for this part of the definition goes over it without explaining the inequality. I tried looking for an explanation myself but wasn't really able to find anything wrong with it. Is it also possible to write the definition like that or is there a problem with that? (first non-homework related question :p)","['calculus', 'definition']"
4376417,Property of an operator that is Fredholm and compact?,"I have been asked this question at my course on Functional Analysis, to tell something about an operator that is both compact and Fredholm. The answer needs to be related to the spaces between which our operator acts. What I know is that a Fredholm operator is already a bounded linear map between Banach Spaces (let's call them $X$ and $Y$ ). I also know that the spectrum of a compact operator is the same as the point spectrum, and that it is at most countable. I was hoping to somehow show using these facts that $Y$ must be finite dimensional. Maybe we could show that the image is finite dimensional, and hence show that $Y$ is also. But for now this is just speculation. Thanks in advance!","['compact-operators', 'functional-analysis']"
4376440,Is there an examble of a non additive base of natural numbers with ratio of two consecutive terms goes to 1?,"Here $\mathbb{N}=\{1,2,3,\dots\}$ . We say that a set $A\subset\mathbb{N}$ is an additive base of natural numbers if there is
a positive integer $h\in \mathbb{N}$ such that every natural number can be written as $a_1+\dots+a_h$ for some (not necessarily distinct) $a_i\in A\cup\{0\}$ . Some famous examples of such additive bases are the $k$ -powers (Waring's problem) and the
set of primes including $1$ (a theorem by Schnirelmann). All the examples I encountered so far had the same property. If $A\subset\mathbb{N}$ was
an additive base and $a_n$ was the $n$ -th term of the set then $$\lim_{n\to \infty}\frac{a_{n+1}}{a_n}= 1.$$ It wasn't very hard to prove that indeed this is true for every additive base.
Then I started to look the reverse direction. I tried to find a set $A=\{a_n\}_{n=1}^{\infty}$ (the terms are in ascending order) with $a_1=1\in A$ and $\lim_{n\to \infty}\frac{a_{n+1}}{a_n}= 1$ which is not an additive base but I couldn't. I would be glad if someone could enlightened me with such an example. Thanks in advance!","['number-theory', 'additive-combinatorics', 'combinatorics']"
4376446,How many different possible Wordle tweets can exist?,"Wordle is a viral web game where you have up to six attempts to guess a secret five-letter English word* using the letters A-Z. After each $5$ -letter guess (which must itself be a real English word*),
an emoji describes the secret word for each letter guessed: â¬œ (White) means the guessed letter is not in the secret word. ðŸŸ¨ (Yellow) means the guessed letter is in the secret word, but in a different position. ðŸŸ© (Green) means the guessed letter is in the secret word, in that position. For example, if the secret word is ENTER, here is a possible playthrough: PIZZA TROUT TROUT (pointless, but legal) SHEEP ENTER would result in the following, tweetable, output (with letters to help folks whose browsers don't parse the emoji as intended): â¬œâ¬œâ¬œâ¬œâ¬œ (WWWWW) ðŸŸ¨ðŸŸ¨â¬œâ¬œâ¬œ (YYWWW) ðŸŸ¨ðŸŸ¨â¬œâ¬œâ¬œ (YYWWW) â¬œâ¬œðŸŸ¨ðŸŸ©â¬œ (WWYGW) ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ© (GGGGG) Note that the last row may not be ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ©ðŸŸ© (GGGGG) if the correct answer is not found in six tries. Also note that the repeated T in TROUT only was colored once as it only appears once in the secret word. I know that the total possible combinations of â¬œðŸŸ¨ðŸŸ© (WYG) used in a string of five emoji would be equal to $3^5=243$ . But this is ignoring the English word restriction (is every combination of emoji even possible?). * A specific answer may depend on the dictionary used. The actual game of Wordle uses a restricted dictionary of words that can be guessed (including obscure ""Scrabble words""), and a further restricted dictionary of words that might be used as a solution (words more likely to be recognized as such by most players).","['recreational-mathematics', 'combinatorics']"
4376510,Confusion about curved exponential family in Casella & Berger,"In Definition 3.4.7 of Statistical Inference by Casella & Berger, 2nd ed., a curved exponential family is defined to be a family of the form $$
f(x \vert \boldsymbol\theta) = h(x)c(\boldsymbol\theta)\exp\left(\sum_{i=1}^{k}w_i(\boldsymbol\theta)t_i(x)\right)
$$ for which the dimension of the vector $\boldsymbol\theta$ is equal to $d<k$ . A normal $n(\theta, \theta)$ (i.e. mean and variance are both $\theta$ ) is used in problem 3.33 and in Example 3.4.9 as an example of a curved exponential family. But I'm having trouble seeing why this is a curved family. Since there is only one parameter, $d=1$ . Working out the form of the exponential family to which this belongs, I get $$
\frac{1}{\sqrt{2\pi\theta}}\exp\left(\frac{\theta}{2}\right)\exp(x)\exp\left(-\frac{x^2}{2\theta}\right)
$$ so that $h(x) = \exp(x)$ , $c(\theta) = \frac{\exp\left(\frac{\theta}{2}\right)}{\sqrt{2\pi\theta}}$ , and $w_1(\theta) = -1/2\theta$ , $t_1(x) = x^2$ . In that case, $k=1$ . So we have a situation where $d=k$ , which does not meet the condition for a curved exponential family according to the above definition. I'm confused about why the text considers it a curved family in Example 3.4.9 and Exercise 3.33(a). Can anyone clarify this for me?","['statistical-inference', 'probability-distributions', 'probability']"
4376513,Classification of coherent sheaves on $\Bbb P^1$ with a doubled point (nonseparated),"Let $X$ be the scheme obtained by gluing two copies of $\Bbb P^1_k$ along $D(x)$ (basically the projective version of the line with two origins). Is there a good classification of coherent sheaves on $X$ ? By analogy with the case of $\Bbb P^1$ it seems clear that a coherent sheaf splits as a direct sum of a locally free part and a torsion part, but I'm worried that the extremely convenient property that a locally free sheaf on $\Bbb P^1$ is a direct sum of $\mathcal{O}(d_i)$ for various $d_i$ won't hold (the proof I know is linear algebra on the transition matrix, which I can't figure out how to adapt to this case). Has this been written down somewhere before, or can anyone help me get to the right answer?","['coherent-sheaves', 'algebraic-geometry', 'schemes', 'reference-request']"
4376536,Boundedness of an operator composed with a sequence of pseudo inverses,"I'm reading a paper and the following fact is given without proof, and I was hoping one of you smart folks could shed some light on it or provide a counter example: Consider an infinite dimensional separable Hilbert space $\mathcal{H}$ , and let $A$ and $L$ denote two linear, compact operators. Suppose further that $L$ is symmetric and positive definite, so that the spectral theorem gives $$
L(\cdot) = \sum_{\ell=1}^\infty \lambda_\ell \langle \phi_\ell,\cdot\rangle \phi_\ell. 
$$ We define a pseudo-inverse of $L$ as $$
L^{-1}\pi_n(\cdot) = \sum_{\ell=1}^n \frac{ \langle \phi_\ell,\cdot\rangle}{\lambda_\ell} \phi_\ell.
$$ ( $\pi_n$ is the projection onto the span of $\phi_1,...,\phi_n$ ). The claim in the paper is that if it is assumed that $$
\sum_{\ell=1}^\infty \frac{ \|A(\phi_{\ell})\|^2}{\lambda_\ell} < \infty,
$$ then $$
\sup_{n\ge 1} \|AL^{-1}\pi_n\|_{op} < \infty. 
$$ $\|\cdot \|_{op}$ is the usual operator norm. I cannot see why this is true! The assumption seems to imply something about some sort of Trace norm of $AL^{-1}\pi_n$ , but if I try to work out what the Trace or Hilbert-Schmidt norms are of $AL^{-1}\pi_n$ , I get something like $$
\sum_{\ell=1}^n \frac{ \|A(\phi_{\ell})\|^2}{\lambda_\ell^2},
$$ and assuming $\sum_{\ell=1}^\infty \frac{ \|A(\phi_{\ell})\|^2}{\lambda_\ell^2} < \infty$ is evidently a much stronger condition. Am I missing something simple as to why the condition implies the operator norms are uniformly bounded?","['operator-theory', 'linear-algebra', 'functional-analysis', 'real-analysis']"
4376543,Proving the compound angle formula for: $\sin(a+b)$,"So I am currently trying to prove the compound angle formula for $\sin$ . Here is what I have attempted so far: From the picture about(not very well drawn(I know!)), I find that $\angle EAD = \angle GBF = \beta$ So... $\sin(\alpha + \beta) = \frac{BC}{BA} = \frac{CH+HB}{BA} = \frac{CH}{BA} + \frac{HB}{BA}$ $\sin(\alpha) = \frac{GB}{BA}$ $\sin(\beta) = \frac{ED}{EA} = \frac{GH}{GB} = \frac{HF}{GF}$ $\cos(\alpha) = \frac{GA}{BA}$ $\cos(\beta) = \frac{DA}{EA} = \frac{HB}{GB} = \frac{GH}{GF}$ Now, looking at the denominators for $\sin(\alpha + \beta)$ and for $\sin(\alpha)$ , I found a way to sneak in latter into the first equation by: $\frac{HB}{BA} = \frac{HB*GB}{BA*GB} = \frac{GB}{BA} * \frac{HB}{GB} = \sin(\alpha)\cos(\beta)$ I can already see the second half of the equation starting to come out. I am currently stuck on the first half of the equation, I have tried to manipulate to allow access for $\sin(\beta)$ or $\cos(\alpha)$ but I am not getting anywhere.","['triangles', 'algebra-precalculus', 'trigonometry']"
4376555,Does the category of topological spaces with an action of a topological monoid admit coequalizers?,"Let $\mathsf{Top}$ be the category of topological spaces with continuous maps and let $T$ be a monoid in $\mathsf{Top}$ . Let $f,g: M \to N$ be two parallel maps in $\mathsf{Top}_T$ , the category of topological spaces with a continuous action of $T$ .
I would like to know whether there exists the coequalizer of $(f,g)$ in $\mathsf{Top}_T$ . Let me sketch what I know. The forgetful functor $U : \mathsf{Top}_T \to \mathsf{Set}_T$ has a right adjoint  (endow $T$ -sets with the indiscrete topology), hence $U$ preserves colimits. In particular, if $\mathsf{Coeq}(f,g)$ exists in $\mathsf{Top}_T$ , then it has to be the quotient of $N$ by the smallest equivalence relation containing $\{ (f(x),g(x)) \mid x \in X \}$ (that is, the coequalizer in $\mathsf{Set}_T$ ) endowed with a suitable topology.
Denote it by $Q$ , with projection $q : N \to Q$ . If $T$ is core-compact, I know that $T \times - : \mathsf{Top} \to \mathsf{Top}$ preserves colimits (because it admits a right adjoint) and hence $(Q,q)$ with the quotient topology on $Q$ would be the coequalizer. If $T$ is arbitrary, I thought I may consider the finest topology on $Q$ which also makes the $T$ -action $T \times Q \to Q$ a continuous map (so, the finest in the quotient topology such that $T \times Q \to Q$ is continuous). However, I am stuck in proving that $(Q,q)$ with the latter topology on $Q$ is universal in $\mathsf{Top}_T$ . In particular, if $(C,c)$ is any test object in $\mathsf{Top}_T$ with $c \circ f = c \circ g$ , then I am not able to prove that the unique map $\tilde{c} : Q \to C$ such that $\tilde{c} \circ q = c$ , provided by the universal property of $(Q,q)$ in $\mathsf{Set}_T$ , is continuous. Does anybody have a suggestion? Or does there exist a counter-example which shows that, in general, such coequalizer does not exist?","['universal-property', 'general-topology', 'category-theory']"
4376585,"Symplectic version of ""Gram-Schmidt""","Let $w$ be a symplectic form on a vector space $V$ of dimension $2g$ . Suppose we already have a free family $(a_1, \dots, a_g)$ such that $w(a_i, a_j) = 0$ . I also have a family $(b_1, \dots, b_g)$ which verify that $(a_1, \dots, a_g, b_1, \dots, b_g)$ is a basis of $V$ . I've read somewhere that there is a symplectic version of Gram-Schmidt to make the basis $(a_1, \dots, a_g, b_1, \dots, b_g)$ a symplectic one, but concretely, how does it work ? Thank you !","['symplectic-linear-algebra', 'gram-schmidt', 'linear-algebra']"
4376586,Showing that this particular sequence of functions defined by recursion is point-wise bounded,"Let $(s_n)_n$ be a summable sequence. Consider the following sequence of functions $(z_n)_{n\geq 0}$ , $z_n:[0,+\infty)\rightarrow\mathbb{R}$ , defined recursively as \begin{align*}
z_0(x)&\equiv 0\\
z_1(x)&=x\\
z_{n+1}(x)&=\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)
\end{align*} It is sufficient for my purposes to show that such sequence is pointwise bounded. Since it is pointwise non-negative (by induction) with an upper bound I would conclude. Notice that I don't need a uniform bound (which in fact does not exist in my opinion), instead I need to show that \begin{equation*}
\forall \,x\geq 0\quad \exists\quad L=L(x)>0\quad\text{s.t.} \quad\lim_{n\to+\infty} z_n(x)<L.
\end{equation*} Obviously every map of the sequence is non-decreasing, as one can check by induction. First I tried some numerical calculation in order to explore its behavior (I still didn't know at the time if it was bounded or not) and it seems that it admits a pointwise limit to an unbounded function. Here the graph . In this graph on the x-axis we have the integer value $n$ of $z_n$ while on the y-axis are plotted the values of the $z_n(x)$ for several values of $x$ (in this case the values are $1,\dots,10$ ). As you can see each ""orbit"" seems to approach a limit value. The sequence $(s_n)_n$ is actually defined $s_n=2^{-\frac{2}{3}\beta(n+1)}$ for $\beta>0$ (in the graph $\beta=0.7$ ). I want to prove the result for this specific sequence (and for every value of $\beta>0$ ) but I'm confident that it holds for every summable sequence $s_n$ . Of course the value of the limit depends on $s_n$ , and in particular for small values of $\beta$ the convergence is slow, but I still observed it numerically (in this case the limit has a big value).
I tried different approaches, e.g. evaluating the difference \begin{align*}
|z_{n+1}(x)-z_{n}(x)|=\left|\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)-\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)\right|
\end{align*} but I can't do any progress in this direction. I also tried to take the logarithm and see if some tricks are possible \begin{equation*}
\log z_{n+1}=\frac{1}{2} \log z_n(x)+\log(\sqrt{z_{n-1}(x)}+s_n)
\end{equation*} but my attempts were unsuccessful. Also computed the ratio, which for $n$ sufficiently large, neglecting $s_n$ , we can approximate as \begin{equation*}
\frac{z_{n+1}(x)}{z_n(x)}=\frac{\sqrt{z_{n}(x)}\bigg(\sqrt{z_{n-1}(x)}+s_n\bigg)}{\sqrt{z_{n-1}(x)}\bigg(\sqrt{z_{n-2}(x)}+s_{n-1}\bigg)}\approx \sqrt\frac{z_{n}(x)}{z_{n-2}(x)}
\end{equation*} but nothing comes to my mind at this point.
I need to formalize this result in order to prove the existence of so called self-similar solutions for a dyadic model of turbulence, which is the topic of my master thesis. Thanks to anyone who can give any suggestions or ideas.","['sequence-of-function', 'analysis', 'real-analysis', 'sequences-and-series', 'limits']"
4376595,Let $A$ and $B$ be open balls in an ultrametric space. Prove that if $A$ and $B$ have a nonvoid intersection then one of them contains the other.,"Let's say I have $A=B_{r_{1}}(x)$ and $B=B_{r_{2}}(y)$ and the metric $D(a, c) \leq \max\big[D(a, b) \;,\; D(b, c)\big]$ (ultrametric ). I suppose what I want to prove is: $A \cap B\neq \emptyset \implies A \subset B$ . I am not sure where to even start, is it something to do with the triangle inequality? Thanks!","['elementary-set-theory', 'general-topology', 'ultrametric', 'metric-spaces']"
4376603,Tensor bundle as associated bundle,"Given a smooth manifold $M$ of dimension $d$ , consider the frame bundle $FM \overset{\pi_{FM}}\longrightarrow M$ . We can construct the tangent bundle $TM \overset{\pi_{TM}}\longrightarrow M$ as an associated $GL(d, \mathbf{R})$ -bundle by first taking the product $FM \times \mathbf{R}^d$ . Intuitively, this is a bundle over $M$ with typical fiber $GL(d, \mathbf{R}) \times \mathbf{R}^d$ . Next, we take a quotient by $\sim_{GL(d, \mathbf{R})}$ , where $\sim_{GL(d, \mathbf{R})}$ is given by $$\left([\mathbf{e}_i], v\right) \sim \left([\mathbf{e}_i] \blacktriangleleft g, g^{-1} \blacktriangleright v\right)$$ for all $g \in GL(d, \mathbf{R})$ , where $[\mathbf{e}_i]$ denotes frames and $v$ denotes coefficients. Letting the quotient space be denoted $(FM \times \mathbf{R}^d) / GL(d, \mathbf{R})$ , there exists a vector bundle isomorphism $(FM \times \mathbf{R}^d)/GL(d, \mathbf{R}) \longrightarrow TM$ given by $[\left[\mathbf{e}_i\right], v] \mapsto \sum_i v_i\mathbf{e}_i$ . Analogously, I would like to construct a general tensor bundle $T^p_qM$ as an associated $GL(d, \mathbf{R})$ -bundle. I suspect that this is done as follows: Letting $\rho: GL(d, \mathbf{R}) \longrightarrow GL(d, \mathbf{R})$ be a group representation, construct the associated vector bundle $(FM \times \mathbf{R}^c)/\sim_{\rho}$ , where $\sim_{\rho}$ is given by $$
\left([\mathbf{e}_i], t\right) \sim \left([\mathbf{e}_i] \blacktriangleleft g, \rho(g^{-1}) \blacktriangleright t\right)
$$ for all $g\in GL(d, \mathbf{R})$ , where $\mathbf{R}^c \cong T_pM^{\otimes p}\otimes T^*_pM^{
\otimes q}$ and $t$ denotes tensor coefficients. My understanding is that $\rho$ should encode the typical transformation law associated to a $(p, q)$ -tensor under a change of frame. (Q1) Is this the correct $\rho$ ? Additionally, I have seen the transformation law for a $(p,q)$ -tensor given by $$
\widetilde{T}^{i_1,\ldots,i_p}_{j_1,\ldots,j_q} = \sum_{\substack{1\leq k_1,\ldots, k_p \leq d}\\1\leq\ell_1,\ldots,\ell_q\leq d}T^{ k_1,\ldots, k_p }_{\ell_1,\ldots,\ell_q}g_{i_1k_1}\cdots g_{i_pk_p}g^{j_1\ell_1}\cdots g^{j_q\ell_q}.
$$ However, I'm having trouble seeing how such a transformation can be described using a representation $\rho$ . (Q2) I was under the assumption that the tensor components live in a multidimensional array, so how can this live in the matrix group $GL(d, \mathbf{R})$ ?","['principal-bundles', 'fiber-bundles', 'tensors', 'representation-theory', 'differential-geometry']"
4376607,"Why is my method not applicable, one-sided confidence interval","I have ""solved"" a question in mathematical statistics, but unfortunately received an incorrect answer and I would appreciate any help in understanding why my method did not work in this case. The question is as follows: In city A , 500 randomly selected people were tested for antibodies
to covid-19, which resulted in 110 giving positive results. In city B , a random sample of 500 people was also drawn, and here 91 had positive results.
Can it be said that it is statistically certain that city A has a
larger population share with antibodies than city B ?
(Justify your answer with an appropriate test. Use 5% significance
level.) I started by producing $P^*(A)=\frac{110}{500}=0.22$ and respectively $P^*(B)=\frac{91}{500}=0.182$ . Then I chose to continued with a one-sided hypothesis testing where I assumed that the data was binomially distributed according to $Bin(n=500,p)$ , with $$H_0:P=0.182 \text{ and }H_1:>0.182.$$ And finally brought out the one-sided interval according to: $$I_p=\lambda_\alpha \sqrt{\frac{P^*(A)(1-P^*(A))}{n}}\Rightarrow [0.189,\infty].$$ And concluded that $H_0$ should be rejected since 0.182 is not included in the interval. But according to the results, $H_0$ should not be rejected and they have also used a different method than I did. Why is this method not applicable in my case? Note : At first I thought that errors were because I only used Î± and not $\frac{\alpha}{2}$ in the calculation, however, even if I were to replace this, 0.182 is not in the interval.","['statistics', 'confidence-interval']"
4376674,"Truncation error, finite differences","Consider the following FDM problem: Find $u$ such that $$ -u^{\prime \prime}(x)+b(x) u^{\prime}(x)+c(x) u(x)=f(x) ~~\text { in }(0,1), $$ and conditions $u(0) = u(1) = 0$ , where $$ b(x)=x^{2}, \qquad c(x)=1+x, \qquad f(x)=-2+13 x^{2}+3 x^{3}-x^{4}-5 x^{5}. $$ I am trying to find an upper bound on the local truncation error using finite difference method (forward, backward, and centered). So far, I have the following: Approximate $u\left(x_{i}\right)$ respectively by $U_{i}, V_{i}$ , and $W_{i}$ , where $U_{i}$ is the solution of the finite-difference scheme: $$
\frac{1}{h^{2}}\left[-U_{i-1}+2 U_{i}-U_{i+1}\right]+\frac{b_{i}}{h}\left[U_{i}-U_{i-1}\right]+c_{i} U_{i}=f_{i},
 \tag3$$ $V_{i}$ is the solution of $$
\frac{1}{h^{2}}\left[-V_{i-1}+2 V_{i}-V_{i+1}\right]+\frac{b_{i}}{h}\left[V_{i+1}-V_{i}\right]+c_{i} V_{i}=f_{i},
 \tag4$$ and $W_{i}$ is the solution of $$
\frac{1}{h^{2}}\left[-W_{i-1}+2 W_{i}-W_{i+1}\right]+\frac{b_{i}}{2 h}\left[W_{i+1}-W_{i-1}\right]+c_{i} W_{i}=f_{i},
 \tag5$$ with $U_{0}=V_{0}=W_{0}=U_{N+1}=V_{N+1}=W_{N+1}=0$ . I know that I have to use Taylor's formula, but don't know how find the upper bound on the local truncation error for (3), (4), and (5), or even how to derive the truncation error. I am also trying to find the order. (3) has order O(h2), and the other two have order O(h) - is this correct? Can anyone help me here?","['ordinary-differential-equations', 'finite-difference-methods', 'numerical-methods', 'derivatives', 'finite-differences']"
4376686,When is $\sigma(E[X|\mathcal F]) \subset \sigma(E[Y|\mathcal F])$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Improve this question Let $X,Y$ be integrable random variables on the same probability space and $\mathcal F$ a sub-sigma algebra. Under what conditions on $X,Y$ do we have $\sigma(E[X|\mathcal F]) \subset \sigma(E[Y|\mathcal F])$ ? Is it $\sigma(X) \subset \sigma(Y)$ ?","['measure-theory', 'conditional-probability', 'conditional-expectation', 'real-analysis', 'probability-theory']"
4376703,Overthinking equality of real numbers,"This is a very basic question that's been bugging me for some time. I know ""="" means equality: that is, the expression on other side of the equation is the same exact number, so doing anything to one side requires the same thing to the other side. This makes sense when we were with concrete numbers: if you have $5 = 5/1$ and multiply by 8 on both sides, you maintain the equality. I get a bit confused (but also not really confused - this is definitely me overthinking since I've been ""isolating the variable"" since grade school) when we introduce a variable $x$ . Let's say we have $2x + 1 = 9$ . Why can I just subtract $2x$ from both sides? It's because $x$ represents an unknown number, and so properties of numbers work right? Or like $\frac{2}{x} = 17$ why can I just multiply by $x$ on both sides? It's because $x$ is just a number and so behaves like a number - it doesn't matter that we don't know the value, right? With two variabes, let's say we have $y + 3x = 17$ . How do I know the relationship between $x,y$ is ""preserved"" when I subtract $3x$ from both sides to get $y = 17-3x$ ? Or should I not think in terms of ""preserved relationships between x, y"" and instead ""maintaining equalities.""",['algebra-precalculus']
4376708,A martingale and mesurability problem,"Let $X$ be an integrable random variable and $\mathcal F_t$ a filtration. $E[X|\mathcal F_t]$ is an $\mathcal F$ -martingale by the tower property. Let $f$ be a bounded mesurable function. Set $Y_t=f(E[X|\mathcal F_t])$ . Let $0<s<t \in \mathbb R$ . The question: is $E[Y_t|\mathcal F_s]$ mesurable with regard to $\sigma(E[X |\mathcal F_s])$ ? When $f$ is the identity, this is immediate by the martingale property. If it helps, we can suppose that $\mathcal F_t$ is generated by a stochastic process $Z_t$ and that $E[X|\mathcal F_t] = E[X|Z_t]$ . EDIT: If it helps even more, we can assume that $X=Z_1$ , that $(Z_t)$ is Markov (with regard to its filtration $\mathcal F_t$ ), sample-continuous, and that $0<s<t \leq 1$ .","['measure-theory', 'conditional-expectation', 'real-analysis', 'martingales', 'probability-theory']"
4376709,Is there an elegant method to find the minimum value of $\frac{\sqrt{x^{4}+x^{2}+2 x+1}+\sqrt{x^{4}-2 x^{3}+5 x^{2}-4 x+1}}{x}$ for positive $x$?,"Find the minimum value of $\frac{\sqrt{x^{4}+x^{2}+2 x+1}+\sqrt{x^{4}-2 x^{3}+5 x^{2}-4 x+1}}{x}$ for $x\gt0$ . In some textbook, the problem is usually tackled by calculus. So I started to investigate the problem using triangle inequality only.  The answer is so interesting and simple that the minimum value of $S(x)$ is $\sqrt{10}$ when $x=\frac{1+\sqrt{13}}{6}$ . My Question: How many elegant methods are there to find the minimum point?","['maxima-minima', 'calculus', 'trigonometry', 'triangle-inequality']"
4376717,Vector bundles from short exact sequences,"In the following I consider complex manifolds and holomorphic vector bundles. I'm trying to understand the construction of non-trivial vector bundles $V$ via short exact sequences involving sums of line bundles, $$
0 \to V \to \bigoplus_i L_{a_i} \to \bigoplus_j L_{b_j} \to 0 \,,
$$ in arguably the simplest possible setting, of bundles over the complex projective line $\mathbb{CP}^1$ . It is known that any vector bundle on $\mathbb{CP}^1$ is isomorphic to a unique sum of line bundles. Hence given the two line bundle sums above, and assuming that the maps in the short exact sequence are as general as possible, one should be able to compute the line bundle sum to which $V$ is isomorphic. (Since a line bundle on $\mathbb{CP}^1$ is specified by a single integer, this is simply a map from two lists of integers of length $n_1$ and $n_2$ to one of length $n_1-n_2$ .) Not all choices of input line bundle sums will result in a vector bundle $V$ - for some choices $V$ will not have constant rank but instead be a sheaf - and this too should be simple to determine from the integer lists specifying the input line bundle sums. My question is whether there is any reference that discusses this problem or which would help me approach it. (There are certain obvious cases, but I would like to determine the complete result.)","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'exact-sequence']"
4376723,To find the number of ways to put $14$ identical balls into $4$ bins with the condition that no bin can hold more than $7$ balls.,"To find the number of ways to put $14$ identical balls into $4$ bins with the condition that no bin can hold more than $7$ balls. I have tried the following: The total no of ways to distribute $14$ identical balls into $4$ bins without any restriction is $$\binom{14+4-1}{4-1}= \binom{17}{3}.$$ Note that there can't be two bins with more than $7$ balls since we have only $14$ identical balls only. Now, we count the no. of ways so that one bin has more than $7$ balls. So, it has at least $8$ balls and the remaining $6$ can be distributed in $\binom{6+4-1}{4-1}= \binom{9}{3}$ ways. We can choose one bin out of $4$ in $4$ ways. Hence the reqd number of ways = $$\binom{17}{3} - 4 \times \binom{9}{3}.$$","['inclusion-exclusion', 'solution-verification', 'combinatorics', 'balls-in-bins', 'discrete-mathematics']"
4376726,Zero-seeking device on a uniform distribution,"This is a surprisingly difficult problem in a stochastic processes textbook that I'm reading. It's meant to be solved through a Markov chain. A zero-seeking device operates as follows: If it is in state $m$ at time $n$ , then at time $n+1$ , its position is uniformly distributed over the states $0,1,\ldots,m-1$ . Find the expected time until the device first hits zero starting from state $m$ . My first instinct for solving this problem was to use first-step analysis. Such an approach led me to a sequence of $m+1$ linear equations. Let $v_i = \mathbb{E}(X_{T}=0|X_0=m)$ . Then we have the following: $$v_m = \frac{1}{m}v_{m-1}+\frac{1}{m}v_{m-2}+\ldots+\frac{1}{m}v_{1}+1$$ $$v_{m-1}=\frac{1}{m-1}v_{m-2} + \frac{1}{m-1}v_{m-3}+\ldots+\frac{1}{m-1}v_1+1$$ $$\vdots$$ $$v_1=1$$ $$v_0=0$$ I've tried approaching this problem by implementing the differenced expectation, $\nabla v_i=v_i-v_{i-1}$ , but that didn't lead me anywhere. I also see that we can build up the individual $v_i$ s starting from $v_1$ , but I don't know how to evaluate $v_m$ for some arbitrary $m$ . How would you guys approach this problem? Edit: Ok I heard from my professor that the correct way to approach this problem was to recognize that $v_m = \frac{1}{m} + v_{m-1}$ . Now my question is this how I'm ever supposed to come to that realization. It doesn't seem obvious from the equations below.","['markov-chains', 'stochastic-processes', 'intuition', 'probability-theory', 'probability']"
4376756,Verifying the Bernstein condition for random matrices,"Theorem 6.2 of the following paper by J. Tropp states the matrix Bernstein inequality for the subexponential case. Given two i.i.d. sequences $X_1,\dots,X_n \sim N(0,\Sigma)$ and $Y_1,\dots,Y_n \sim N(0, \Gamma)$ , I am trying to bound the maximal eigenvalue of $$
\sum_{i=1}^n Z_i=\sum_{i=1}^n X_i Y_i^T.
$$ The summands are products of Gaussian vectors so they are subexponential and I should be able to use the Bernstein bound directly. I am a bit confused on verifying (and identifying the relevant parameters) in the condition: $$
E[Z_i^p] \preceq\frac{p!}{2} R^{p-2} A^2_k, \qquad p=2,3,4,\dots.
$$ Is there a simple way to show that this holds? An update a slightly more straight forward statement of the matrix Bernstein result is given in Wainwright's High Dimensional Statistics text (Theorem 6.17). It is stated as follows: Let $\{ Q_i\}$ be a sequence of independent, zero mean and symmetric
random matrices, and assume all matrices in the sequence satisfy the
Bernstein condition with parameter $b>0$ , that is, for $i=1,2,\dots,n$ \begin{align*}
         E[Q_i^p] \preceq \frac{1}{2}p! b^{p-2} \text{var}(Q_i),
\quad p=2,3,4,\dots,
     \end{align*} where $\text{var}(Q) := E[Q^2]-(E[Q])^2$ . Then for all $\varepsilon \ge 0$ , \begin{align*}
         P \left(\frac{1}{n} \| \sum_{i=1}^n Q_i\| \ge \varepsilon \right)
         \le 
         2 \text{rank} 
         \left ( \sum_{i=1}^n \text{var}(Q_i) \right )
         \exp \left \{-\frac{n\varepsilon^2}{2(\sigma^2 + b \varepsilon)} \right \},
     \end{align*} where \begin{align*}
         \sigma^2 := \frac{1}{n} \|\sum_{i=1}^n \text{var}(Q_i)\|.
     \end{align*} The result is easier to work with since it gives us the Bernstein condition should hold with the variance matrix. For my problem, we can show that $$
\text{var}(Z_i) = \Sigma \Gamma,
$$ and since $Z_i$ is a product of two gaussian vectors, we know it must be subexponential, so we can conclude that there exists some b>0 (that I do not know how to solve for) such that $$
E[Z_i^p] \preceq\frac{p!}{2} b^{p-2} \Sigma \Gamma.
$$ Further, assuming $\Sigma, \Gamma$ are full rank, we have that $$
\text{rank}(\sum_{i=1}^n \text{var}(Z_i)) \le nd
$$ where $d$ is the dimension of the $X_i$ 's. We can therefore show that for some $b>0$ and for all $\varepsilon>0$ \begin{align*}
        P \left(\frac{1}{n} \| \sum_{i=1}^n Z_i\| \ge \varepsilon \right)
        \le 
        2 nd
        \exp \left \{-\frac{n\varepsilon^2}{2(\|\Sigma\Gamma\| + b \varepsilon)} \right \},
    \end{align*} This pretty much answers most of my question, but if anyone has a way to identify the correct $b$ that would be very helpful. Edit 2 My previous approach is incorrect because I assumed that the $Z_i$ are symmetric which they of course are not. The correct way to use the Bernstein result is to define the block matrix: $$
Q_i = \begin{bmatrix}
O & X_iY_i^T\\
Y_iX_i^T & O\\
\end{bmatrix}
$$ Then note that $Q_i$ is mean zero and has variance $\text{diag}(\text{tr}(\Gamma)\Sigma, \text{tr}(\Sigma)\Gamma)$ . The odd moments of $Q$ with $p=2k+1$ take the form: $$
Q_i^{2k+1} = 
\begin{bmatrix}
O & \|X_i\|^{2k} \|Y_i\|^{2k} X_iY_i^T\\
\|X_i\|^{2k} \|Y_i\|^{2k} Y_iX_i^T & O\\
\end{bmatrix}
$$ and the even moments $p=2k$ take the form $$
Q_i^{2k} = \begin{bmatrix}
\|X_i\|^{2k-2} \|Y_i\|^{2k} X_iX_i^T & O\\
O &\|X_i\|^{2k} \|Y_i\|^{2k-2} Y_iY_i^T \\
\end{bmatrix}
$$ I am pretty stumped on how to argue that the expectation of either of these matrices is less than or equal to the right hand side of the bernstein condition with respect to the semi-definite ordering.","['matrices', 'statistics', 'concentration-of-measure', 'probability']"
4376785,Solving differential equation $ \frac{dy}{dx}=\frac{1}{3e^y}-1 $ results in strange result?,"I was trying to solve this differential equation but I think I may be making a mistake. $$ \frac{dy}{dx}=\frac{1}{3e^y}-1 $$ First I multiply both sides by $3e^y$ , then bring everything to one side and factorise $$ 3e^y\left(1+\frac{dy}{dx}\right)=0$$ Giving values for both y and dy/dx as $$ y=0, y=-x+c $$ This is not the right answer based on an online ODE solver, and I am sure I did something wrong. Is it possible to solve the equation this way? The expected result given $y(0)=0$ is $$y=\ln\left(\frac{1+2e^{-x}}{3}\right)$$",['ordinary-differential-equations']
4376792,"The Collatz Conjecture function should induce a collection of Grothendieck groups, one for each $n \in \Bbb{Z}$ or $\Bbb{N}$. Their properties?","This question is about the Collatz conjecture . Let $\Bbb{N}$ include $0$ . The Collatz conjecture function is given by: $$
f: \Bbb{N} \to \Bbb{N}, \\
f(n) = \begin{cases} 
\dfrac{n}{2}, \text{ if } n = 0 \pmod 2,\\
\dfrac{3n + 1}{2}, \text{ if } n = 1\pmod 2 \text{ and } n \gt 1\\
\end{cases}
$$ Now, break up the conjecture into an infinite number of cases, one for each $n \in \Bbb{N}$ as is what usually happens when mathematicians collectively attack problems.  The result is a proven special cases section on the conjecture's Wikipedia page. We can also extend $f$ to $\Bbb{Z}$ as is and get several all-negaive value loops. Anyway, let the domain and range space be $\Bbb{N}$ for now.  We know that $\Bbb{N}$ forms a commutative monoid under addition.  Define the equivalence relation: Fix $n \in \Bbb{N}$ .  Define the equivalence relation on $\Bbb{N}$ : $$
i\sim j \iff f^i(n) = f^j(n)
$$ Then $i\sim j, i' \sim j' \implies$ $i + i' \sim j + j'$ so that the equivalence relation respects $+$ on $\Bbb{N}$ . Then you can form a congruence monoid $N = \Bbb{N} / \sim$ .  For example for $n = 1$ this is $f^0(1) = 1$ , $f^1(1) = 2$ , $f^2(1) = 1, \dots$ so that the equivalence classes are $2 \Bbb{N}$ and $2\Bbb{N} + 1$ . Therefore we say that the Collatz conjecture induces the group $\Bbb{Z}/2 = G(M/\sim) = $ the Grothendieck group of $\Bbb{N}/\sim$ . That makes perfect sense!  There's a loop of ""length 2"" there namely $(1,2,1,2,1,\dots)$ where entries are iterate values $f^i(1), i \geq 0$ (so the entries are $0$ -based indexed). Therefore, I reckon that the Grothendieck groups $G(\Bbb{N}/\sim_{n})$ where $\sim_n$ means $n$ is the input to $f^i(n)$ in the definition of $\sim$ must have some property that flags whether or not $n$ gives an example to the conjecture, is there? I don't think it's finiteness.  For example $n$ terminating at $1$ gives a finite group as well as $n$ going off and then self-looping somewhere else would give a finite group. Further Attempt. The $f^i(n)$ sequence can either shoot off to infinity, self-loop somewhere (here at $i = 0$ or further at $i = x \gt 0$ ), or it can drop back down to a value $m \lt n$ .  In the first case we have that $G_n := G(\Bbb{N}/\sim_n)$ is trivial.  In the second case we have: $$
n, n_1, n_2, \dots, n_j, n_{j+1}, \dots, n_{k-1}, n_k = n_j, n_{j+1}, \dots
$$ for some $n_k \gt n_{j} \geq n$ .  And in the third case we have: $n, n_1, n_2, \dots, n_{k-1}, n_{k} = 2, 1, 2, 1, \dots$ for some $k \gt 0$ , since by induction on $n$ we've handled all cases $m \lt n$ and they go to the 1-2 loop. The Grothendieck group in the second case is that of the monoid: $$
\{[0], [1], [2], \dots, [j-1], [j],[j+1], \dots, [k-1] \}
$$ What is the Grothendieck group isomorphic to in this case?","['collatz-conjecture', 'abstract-algebra', 'group-theory', 'grothendieck-construction', 'open-problem']"
4376802,"no. of ways of arranging 5 As, 7 Bs and 4 Cs such that atleast 3 CA pairs occur in a word","Here's the problem statement In how many ways can we arrange $5$ $A$ s, $7$ $B$ s and $4$ $C$ s into a $16$ -letter word such that there are atleast three $CA$ pairs occurring in a word The solution mentioned proceeds to calculate the number of words containing exactly four $CA$ pairs and then the number of words containing exactly three $CA$ pairs (which it does by calculating number of ways of inserting $2$ $A$ s in $11$ out of $12$ gaps (blank group allowed) of a string containing $3$ $CA$ pairs, $7$ $B$ s and one $C$ . I agree with this solution, however I cannot figure out why my method fails. I think calculating number of words that contain $3$ $CA$ s pairs and their permutations should suffice, because their permutations do include the cases where another $CA$ pair is formed (so that accounts for $4$ $CA$ pairs formed) and when $C$ does not precedes any of the $A$ s, it accounts for the words containing $3$ $CA$ pairs. So, number of words should be $$\frac{13!}{7! \times 3! \times 2!} =102960$$ whereas, according to solution 1 $$\frac{12!}{7! \times 4!} + \frac{11!}{7! \times 3!} \times {12\choose10} = 3960 + 87120 = 91080$$ can someone point out my mistake",['combinatorics']
4376803,Lebesgue Outer Measure Null Set Proof,"I am trying to solve the following proof: Let $||$ be the Lebesgue outer measure. Show that if $|B|=0$ then $|A\cup B|=|A|$ . If I consider $B=\{x_1,x_2,....\}$ this is easy enough. Consider an arbitrary open covering of $A \subset \cup_{i=1}^{\infty} I_i$ . Let us consider the open covering for $B\subset\cup_{n=1}^{\infty}(x_n-\frac{\epsilon}{2^n},x_n+\frac{\epsilon}{2^n})$ . Then
Consider the open covering of $A\cup B\subset \cup_{n=1}^{\infty}\bigg\{ I_n\cup (x_n-\frac{\epsilon}{2^n},x_n+\frac{\epsilon}{2^n})\bigg\}$ . Then we get $$|A\cup B|\le \sum_{n=1}^{\infty}l(I_n)+\epsilon\sum_{n=0}^{\infty}\frac{1}{2^n}= \sum_{n=1}^{\infty}l(I_n)+2\epsilon.$$ Since the covering for A was arbitrary: $$|A\cup B|\le |A|+2\epsilon.$$ Letting $\epsilon \rightarrow 0$ $$|A\cup B|\le |A|.$$ The reverse inequality holds by monotonicity. I can't think of another set $B$ that would have measure zero that is not a countable set of singletons. If there is can anyone provide an example?","['measure-theory', 'lebesgue-measure', 'outer-measure']"
4376833,"Solving the system $k= a\cos\alpha+b\sin\beta$, $h=a\sin\alpha+b\cos\beta$","I've got problems with this system. I need to calculate $\theta_2$ and $\theta_4$ knowing all the other values $(k,~h,~a_2,~a_3)$ . Any idea on how I can solve this? \begin{equation}
    \begin{cases}
      k= a_{2}\times\cos(\theta_{2}) + a_{3}\times\sin(\theta_{4})\\
      h= a_{2}\times\sin(\theta_{2}) + a_{3}\times\cos(\theta_{4})\\
    \end{cases}\,.
\end{equation} As suggested, I'll link some of my tries in resolving this problem, all without success. First Try, using the relation $\sin^2(x) + \cos^2(x) = 1$ Second Try, elevating everything squared Third Try, using an extended version of sum-to-product formulas Clarification: this formulas comes from a kinematic problem, I'm trying to model this arm . I'm sorry if it appears confusing.","['algebra-precalculus', 'systems-of-equations', 'trigonometry']"
4376860,"Consider the topology on $\mathbb R$ generated by the basis $\{[a, b) : a,b \in \mathbb Q\}$. What is the closure of the set $(0,\sqrt 2)$?","Consider the topology on $\mathbb R$ generated by the basis $\left \{[a,b) : a,b \in \mathbb Q\right \}$ . Denote the topology by $\Gamma_l$ . What is the closure of the set $(0,\sqrt 2)$ in $\Gamma_l$ ? The guess candidates are $\left [0,\sqrt 2\right )$ , $\left (0,\sqrt 2\right ]$ and $\left [0,\sqrt 2\right ]$ . Out of these, I could eliminate $\left [0,\sqrt 2\right )$ since $$\left [0,\sqrt 2\right )=\bigcup_{n=1}^\infty \left [0,q_n\right )$$ where $\{q_n\}_{n=1}^\infty$ is an increasing sequence of rationals converging to $\sqrt 2$ , and $q_1>0$ . [ Note: As Theo pointed out, it does not eliminate the possiblity. Being open does not imply that it's not closed] But, I am unable to perform such manipulations on $\left (0,\sqrt 2\right ]$ or $\left [0,\sqrt 2\right ]$ . I tried to look at the complement of $\left (0,\sqrt 2\right ]$ which is $(-\infty,0]\cup\left (\sqrt 2,\infty\right )$ one of which is closed and the other is open. I would like to know whether there are any tricks to guess the closure of such sets in these weird topologies, and a proof for the current problem.","['elementary-set-theory', 'general-topology']"
4376871,Proving $\int_0^{1/2}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3)$,"By comparing some results, I found that $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\text{Li}_3\left(-\frac12\right)-\frac{13}{24}\zeta(3).\tag{1}$$ I tried to prove it starting with applying IBP: $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2\left(-\frac12\right)-\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx$$ then using the fact that $\ln(1-x)\ln(1+x)=\frac14\ln^2(1-x^2)-\frac14\ln^2\left(\frac{1-x}{1+x}\right)$ : $$\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2(1-x^2)}{x}dx}_{1-x^2\to x}-\frac14\underbrace{\int_0^{\frac12}\frac{\ln^2\left(\frac{1-x}{1+x}\right)}{x}dx}_{(1-x)/(1+x)\to x}$$ $$=\frac18\int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx-\frac14\int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx.$$ Using: \begin{gather}
\int\frac{\ln^2(x)}{1-x}dx=\sum_{n=1}^\infty\int x^{n-1}\ln^2(x)dx\\\
\overset{\text{IBP}}{=}\sum_{n=1}^\infty\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\
=-\ln^2(x)\ln(1-x)-2\ln(x)\operatorname{Li}_2(x)+2\operatorname{Li}_3(x),
\end{gather} and \begin{gather}
\int\frac{\ln^2(x)}{1+x}dx=\sum_{n=1}^\infty(-1)^{n-1}\int x^{n-1}\ln^2(x)dx\\\
\overset{\text{IBP}}{=}\sum_{n=1}^\infty (-1)^{n-1}\left(\ln^2(x)\frac{x^n}{n}-2\ln(x)\frac{x^n}{n^2}+2\frac{x^n}{n^3}\right)\\\
=\ln^2(x)\ln(1+x)+2\ln(x)\operatorname{Li}_2(-x)-2\operatorname{Li}_3(-x).
\end{gather} we have: $$\int_{\frac34}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)-2\ln(2)\ln^2(3/4)+2\ln(3/4)\text{Li}_2(3/4)-2\text{Li}_3(3/4),$$ $$\int_{\frac13}^1\frac{\ln^2(x)}{1-x}dx=2\zeta(3)+\ln^2(3)\ln(2/3)-2\ln(3)\text{Li}_2(1/3)-2\text{Li}_3(1/3),$$ $$\int_{\frac13}^1\frac{\ln^2(x)}{1+x}dx=\frac32\zeta(3)-\ln^2(3)\ln(4/3)+2\ln(3)\text{Li}_2(-1/3)+2\text{Li}_3(-1/3).$$ Combining the three integrals, we get $$\int_0^{\frac12}\frac{\ln(1-x)\ln(1+x)}{x}dx=\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))-\frac13\text{Li}_3(3/4)$$ $$+\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))+\frac14\ln(3/4)\text{Li}_2(3/4)$$ $$-\frac14\ln(2)\ln^2(3/4)+\frac14\ln(2)\ln^2(3)-\frac58\zeta(3)$$ and finally $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=\ln(2)\text{Li}_2(-1/2)-\frac12(\text{Li}_3(1/3)-\text{Li}_3(-1/3))+\frac13\text{Li}_3(3/4)$$ $$-\frac12\ln(3)(\text{Li}_2(1/3)-\text{Li}_2(-1/3))-\frac14\ln(3/4)\text{Li}_2(3/4)$$ $$+\frac14\ln(2)\ln^2(3/4)-\frac14\ln(2)\ln^2(3)+\frac58\zeta(3).$$ and I think by using the polylogarithm identities, we can simplify this result into (1). My question is how to prove (1) without going through all this mess if possible? Edit :  I also tried the Cauchy product $$\left(\sum_{n=1}^\infty a_n x^n\right)\left(\sum_{n=1}^\infty b_n x^n\right)=\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n a_k b_{n-k+1}\right)$$ of the integrand: $$\frac{\text{Li}_2(-x)}{1-x}=\left(\text{Li}_2(-x)\right)\left(\frac1{1-x}\right)=\left(\sum_{n=1}^\infty\frac{(-1)^n x^n}{n^2}\right)\left(\frac1x\sum_{n=1}^\infty x^{n}\right)$$ take $a_n=\frac{(-1)^n}{n^2}$ and $b_n=1=n^0$ $$=\frac1x\sum_{n=1}^\infty x^{n+1}\left(\sum_{k=1}^n\frac{(-1)^k(n-k+1)^0}{k^2}\right)=\sum_{n=1}^\infty x^{n}\left(\sum_{k=1}^n\frac{(-1)^k}{k^2}\right).$$ By using the definition of the $n$ th generalized skew harmonic number of order $2$ : $$\overline{H}_n^{(2)}=\sum_{k=1}^n\frac{(-1)^{k-1}}{k^2}$$ we have $$\frac{\text{Li}_2(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(2)}$$ or in general: $$\frac{\text{Li}_a(-x)}{1-x}=-\sum_{n=1}^\infty x^n \overline{H}_n^{(a)}.$$ Employing this series expansion, we have $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty  \overline{H}_n^{(2)}\int_0^{\frac12}x^n dx$$ $$=-\sum_{n=1}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}}$$ let the index start from zero since $\overline{H}_0^{(a)}=0$ $$=-\sum_{n=0}^\infty \frac{\overline{H}_n^{(2)}}{(n+1)2^{n+1}}$$ shift the index $$=-\sum_{n=1}^\infty \frac{\overline{H}_{n-1}^{(2)}}{n2^{n}}.$$ Write $\overline{H}_{n-1}^{(2)}=\overline{H}_{n}^{(2)}+\frac{(-1)^n}{n^2}$ , we get $$\int_0^{\frac12}\frac{\text{Li}_2(-x)}{1-x}dx=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\sum_{n=1}^\infty \frac{(-1)^n}{n^32^{n}}=-\sum_{n=1}^\infty \frac{\overline{H}_{n}^{(2)}}{n2^{n}}-\text{Li}_3\left(-\frac12\right).$$ Now we need to find this sum, which is, by comparing with (1), equal to $\frac{13}{24}\zeta(3).$","['integration', 'definite-integrals', 'alternative-proof', 'calculus', 'polylogarithm']"
4376882,Injectivity of pushout in the category of groups,"Let $G,H,K$ be groups with $i:K\rightarrow G,j:K\rightarrow H,f:G\rightarrow G *_K H$ and $g:H\rightarrow G *_K H$ such that $f\circ i=g\circ j$ . I want to prove that if $i$ and $j$ are injective then so is $f$ . I first thought about using the universal property, if we manage to find a group $L$ and $\varphi:G\rightarrow L,\psi:H \rightarrow L$ with $\varphi$ injective such that $\varphi\circ i=\psi\circ j$ then by the universal property there would exist $u:G *_K H\rightarrow L$ such that $\varphi=u\circ f$ , it would then follow that $f$ is injective. I tried with $L=G*H$ and $\varphi:G\hookrightarrow G*H$ but failed to find a morphism $\psi$ satisfying the commutativity condition. I also tried to use the construction of the pushout in the category of groups, but it led nowhere. Thanks for your help.","['group-theory', 'abstract-algebra', 'category-theory']"
4376895,Minimum tile moves needed for symmetry,"Suppose you have a $n$ x $n$ grid, and you place $k$ tiles on it at random, such that it is possible to rearrange the tiles into a pattern symmetrical about every major axis of the square grid (diagonals, horizontals and verticals). My question is, in general, what is the number $a$ , in terms of $n$ and $k$ , such that given any random tiling of the grid, it can be restored to symmetry in no more than $a$ moves. I should clarify that a single move consists of swapping an empty square with an edgewise adjacent tiled square. An example of a sequence on a $3$ x $3$ grid is shown below: In this example, $n = 3$ and $k = 4$ . I have verified by hand that $a = 4$ is the minimum needed for these values to ensure a grid can always be restored to symmetry. I've attempted to solve this problem by analysing every possible symmetry, for a specific value of $n$ and $k, $ and trying to use them to generate the initial starting configurations, such that after $a$ iterations of the generation, every possible starting configuration would be available in the generated set. However, this didn't seem to lead anywhere useful. The issue I'm facing is, for larger values of $n$ , the number of possible symmetries the square can have grows pretty fast, and the issue becomes choosing the right one such that the choice minimizes the moves needed to reach it. Is this problem feasible to solve in the general case, or does it depend on too many factors?","['combinatorics', 'discrete-mathematics', 'discrete-optimization', 'group-theory', 'symmetry']"
4376933,Adjoint system associated to Schauder basis,"Let $X$ be reflexive Banach space or if you need one could assume that $X=H$ where $H$ is a usual separable Hilbert space and $\{e_n\}_{n = 1}^\infty$ be a Schauder basis in it which means for any x $\in X$ you have a unique representation $x = \sum_{n = 1} ^{\infty} x_ne_n$ . By saying that system $\{f_n\}_{n=1}^{\infty}$ is adjoint to $\{e_n\}_{n = 1}^{\infty}$ I mean a biorthogonal system associated with $\{e_n\}_{n=1}^{\infty}$ i.e. $(f_i, e_j) = \delta_{ij}$ for $\forall$ i, j. My question is would adjoint system $\{f_n\}_{n=1}^{\infty}$ associated to arbitrary Schauder basis $\{e_n\}_{n=1}^{\infty}$ be Schauder basis in $X^*$ (or in $H$ respectively) itself or not? I'll just add that answer is positive if you'd ask the same question for complete, minimal system or Riesz basis. Thank you for any advices and remarks.","['banach-spaces', 'schauder-basis', 'functional-analysis']"
4376961,Solving functional equations using recursion,"If $f:\mathbb N \rightarrow \mathbb N$ such that $f(f(f(n)))+f(f(n))+n=3f(n) \; \forall \; n\in \mathbb N,$ Then Find $f$ Solution Provided in book: Replace $n$ with $f(n)$ successively in parent functional equation $k$ times. We get $\underbrace {fofof...of(n)}_\text{k+3 times }+\underbrace {fofof...of(n)}_\text{k+2 times }+\underbrace {fofof...of(n)}_\text{k times }=3 \underbrace {fofof...of(n)}_\text{k+1 times }$ $\cdots \cdots(1)$ Let $a_0=n$ for some fixed $n$ and $a_{k+1}=f(a_k) \; \forall \;k\geq 0$ My doubt: How did he assume $a_0=n$ and How it became $a_{k+1}=f(a_k)$ Please Help In this. I also checked for duplicate but i couldn't find it. Also Any other way to solve this kind of problem?","['functional-equations', 'algebra-precalculus', 'functions', 'recurrence-relations']"
4376980,Probability of dense subgraph in a random graph,"What is the probability that a random graph with $n$ vertices and degree sequence $\left(d_i\right)_{i=1..n}$ has a subgraph of $k$ vertices and density $\delta$ ? The random graph is typically obtained through the configuration model . The (simpler) case where the degree sequence is not prescribed but only the number $m$ of edges, leading to an ErdÅ‘sâ€“RÃ©nyi random graph , is also of interest. Context: I generate random graphs and then add dense subgraphs to them in order to test an algorithm for detecting them, but I need to know if it is reasonable to assume that the initial graph has no such subgraph.","['random-graphs', 'graph-theory', 'combinatorics', 'random-matrices', 'probability']"
4376987,Measurable group's quotient is a measurable group?,"A measurable group is a group equipped with a $\sigma$ -algebra making the multiplication and the inversion measurable. For any measurable space $X$ and an equivalence relation $\sim$ on $X$ , there is the unique $\sigma$ -algebra on $X^*=X/\sim$ making the obvious projection $q:X\to X^*$ satisfy: ""For all $A\subseteq X^*$ , $q^{-1}(A)$ is measurable if and only if $A$ is measurable."" Call this $\sigma$ -algebra the quotient $\sigma$ -algebra . Let $G$ be a measurable group and let $N$ be its normal subgroup. Assume that $N$ is at most countable. Give $G/N$ the quotient $\sigma$ -algebra. Is $G/N$ also a measurable group? My Attempt The analogous statement is true for topological groups. So I tried to mimic its proof. It goes as: Show that $q:G\to G/N$ is an open map; Show that, if $f:W\to Y$ and $g:X\to Z$ are open maps, then $f\times g$ is an open map; Conclude that $q\times q$ is open. The rest is easy. If 1 - 3 are changed into the analogous statements, then 1 is also true for measurable groups (provided that $N$ is at most countable). But I could not prove 2. Once I prove 2, I am done. For 2, let $\mathcal A$ be the algebra generated by the set $\{B\times C:\text{$B\subseteq W$ and $C\subseteq X$ are measurable}\}$ . I proved that if $A\in\mathcal A$ , then $(f\times g)(A)$ is measurable. But the $\sigma$ -algebra on $W\times X$ is the $\sigma$ -algebra generated by $\mathcal A$ . I am stuck here.","['group-theory', 'quotient-group', 'measure-theory', 'quotient-spaces']"
4377007,Group action and orbit question,"I have the following group action question that I would like some advice on how should i proceed. Let $X$ be the group $\mathbb{Z}/n\mathbb{Z}$ and $G$ be the group $(\mathbb{Z}/n\mathbb{Z})^*$ . Let $G$ act on $X$ via $$g\cdot x=gx$$ Given $m\in X$ , show that the orbit containing $m$ is the same as the orbit containing $d$ where $(m,n)=d$ . I have tried using the following to solve it to no avail. Bezout's identity: There exists $a,b\in \mathbb{Z}$ such that $am+bn=d$ Let $a,b>0$ be integers and $d = (a, b)$ . Suppose $d|N$ . Then the
linear congruence $$ax \equiv N (\text{mod }b)$$ $\space\space\space\space\space\space\space$ has $d$ mutually incongruent solutions modulo $b$ . Does anybody have any advice on how I should solve this?","['group-theory', 'group-actions']"
4377023,Forced damped oscillator - why am I getting infinite amplitude at resonance(incorrectly),I'm trying to find the amplitude of steady state response of the following differential equation: $$\ddot{x}+2p\dot x + {\omega_0}^2x=\cos(\omega t)$$ A particular solution is $$x_p=\Re{\dfrac{e^{i\omega t}}{\omega_0^2 - \omega^2 + i2p\omega}} $$ The amplitude at steady state is then $$A=\dfrac{1}{\sqrt{(\omega_0^2 - \omega^2)^2 + (2p\omega)^2}}$$ The denominator has minimum value when $\omega^2  =\omega_0^2 - 2p^2 $ : $$A=\dfrac{1}{2p\sqrt{\omega_0^2-p^2}}$$ This expression seems to suggest that the amplitude goes to infinity as $p$ approaches $\omega_0$ . But amplitude has to be finite (from other examples of LRC tank circuit etc). Pretty sure I'm wrong but not able to see where. Any help?,['ordinary-differential-equations']
4377066,Proof that eigenvalues of the Coulomb hamiltonian are not nonnegative,"Let's consider the Coulomb Hamiltonian $$
-\Delta - \frac{1}{|x|}$$ in $\mathbb{R}^3$ . It is known that eigenvalues of the Coulomb Hamiltonian are negative. I know it has negative eigenvalues, but I can't prove it hasn't nonnegative eigenvalues. That is, I want to prove the following. If smooth function $u$ satisfies the following $$\int_{\mathbb{R}^3
} |u(x)|^2 dx + \int_{\mathbb{R}^3
} |\nabla u(x)|^2 dx ï¼œ \infty$$ and $$-\Delta u - \frac{u}{|x|} = \lambda u$$ for some $\lambda \geq 0$ , then $u=0$ . Any advice would be appreciated.","['integration', 'operator-theory', 'derivatives', 'eigenvalues-eigenvectors']"
4377083,Direct Sum of Groups - are there 2 types of direct sums?,"This has been a point of confusion for quite some time now for me. I have come across two different definitions of the direct sum on groups, each with a different notation for the symbol. These two definitions are given here: https://en.wikipedia.org/wiki/Direct_sum_of_groups https://en.wikipedia.org/wiki/Direct_sum#Direct_sum_of_abelian_groups Clearly these two definitions cannot be the same, as one says the set of the direct sum group is the cartesian product of the two sets, wheras the other one says, it is the set generated by the two normal subgroups. So which one is correct, are both of these 'equivalent', and how do we know which definition of direct sum to take in a given context?","['group-theory', 'definition', 'direct-sum']"
4377188,What correlation exists between the assumptions $a)$ and $b)$?,"Let $F\in C^1(\mathbb{R}\setminus\{0\})$ and let $\varepsilon>0$ small. Consider the two assumptions: $$a)\quad F(s)\ge \frac{1}{s^2}\quad\mbox{ for $0<s<\varepsilon$};$$ $$b)\quad sF^{\prime}(s)\ge \frac{1}{s}\quad\mbox{ for $0<s<\varepsilon$}.$$ As an exercise, I need to understand which is stronger between them; I mean, if $a)\implies b)$ or $b)\implies a)$ . Or, in general, what kind of relation exists between them. I tried this: conditions $b)$ is equivalent to $F^{\prime}(s)\ge \frac{1}{s^2}$ for $0<s<\varepsilon$ . Thus $$\int_s^{\varepsilon} F^{\prime}(v) dv \ge\int_s^{\varepsilon}\frac{1}{v^2} dv\iff F(\varepsilon)-F(s)\ge \frac{1}{s} -\frac{1}{\varepsilon}\iff F(s)\le C-\frac{1}{s},$$ for a suitable constant $C>0$ .
Apparently, I do not see any relation beteween them. Could someone please help me understand what I am doing wrong? Or please give some hints? Thank you in advance!","['integration', 'real-analysis', 'calculus', 'functional-analysis', 'derivatives']"
4377205,"Let $G$ be a group, $H\unlhd G$, prove that the commutator subgroup $H'$ of $H$ is a normal subgroup in $G$.","Let $G$ be a group, $H$ , a normal subgroup of $G$ , prove that the commutator subgroup $H'$ of $H$ is a normal subgroup in $G$ . My ideas: I want to prove it like this: $gH'g^{-1} = H'$ $\forall g \in G$ but I don't know how to continue, because $g$ may not lie in $H$ and $ghg^{-1}h^{-1}$ it's not a commutator.","['derived-subgroup', 'normal-subgroups', 'group-theory', 'abstract-algebra']"
4377209,An algebraic approach to the analyticity of holomorphic functions,"Famously, complex analysis is much ""nicer"" (more rigid) than real analysis. The reason for this is that, as I like to put it, complex analysis is just one-dimensional algebraic geometry. This is formalized via GAGA, but the fundamental fact that makes this work is that holomorphic functions are always analytic. As a result, we have a fundamental sequence of inclusions $\mathbb{C}[x_1,\dotsc,x_n]_{(x_1,\dotsc,x_n)}\subset\mathbb{C}\{x_1,\dotsc,x_n\}\subset\mathbb{C}[[x_1,\dotsc,x_n]]$ which allows us to work with holomorphic functions algebraically. We find that holomorphic germs form a regular local ring of dimension $n$ , admit a division algorithm (Weierstrass preparation theorem), and so on. As for why holomorphic functions are analytic, however, the standard proof involves expanding the inside of a path integral. This isn't exactly surprisingâ€”after all, $\mathbb{C}$ is an analytic object, so the proof ought to use analysis. In terms of the intuition behind this, though, the best I've been able to find is ""holomorphic functions satisfy the Cauchy-Riemann equations"". My problem with this is that it's based on differential equations over $\mathbb{R}$ , rather than using the basic properties of $\mathbb{C}$ itself (minimal algebraically closed complete Archimedean field). In general, suppose we have a topological field $K$ , or a field with absolute value if you like. We can define a perfectly good theory of differentiability for functions on open subsets of affine space $K^n$ , and we get a theory of smooth functions, analytic functions, and manifolds. From a purely algebreo-geometric point of view, we can always work with varieties over $K$ , and we can do calculus on them algebraically. Moreover, if I'm not mistaken, we should be able to naturally make the set of $K$ -points of any smooth variety into a $K$ -manifold. This is typically done over $\mathbb{R}$ , $\mathbb{C}$ , the $p$ -adic numbers (and their algebraic extensions), and more generally fields with a complete absolute value. So my question is as follows: what conditions on $K$ will make it so that the smooth theory of $K$ -manifolds will be describable, at least approximately, via the algebraic one? What will cause the smooth functions to be analytic? And, in particular, what properties of $\mathbb{C}$ allow its theory of smooth functions to be described algebraically from this perspective?","['complex-analysis', 'complex-geometry', 'algebraic-geometry', 'commutative-algebra']"
4377213,"Let $f(x)=x^2-2$ with $x\in [-2,2]$. Show that the equation $f^{n}(x)=x$ has $2^{n}$ real roots. [Where $f^{n}(x)=f(f^{n-1}(x))$]","Let $f(x)=x^2-2$ with $x\in [-2,2]$ . Show that the equation $f^{n}(x)=x$ has $2^{n}$ real roots. [Where $f^{n}(x)=f(f^{n-1}(x))$ ] My solution: Let $x=2\cos(\theta)$ for $\theta\in [0,\pi]$ $\implies$ $f(x)=4\cos^2 (\theta)-2=2\cos(2\theta)$ $f(f(x))=(f(x))^2-2=(2\cos^22\theta)^2-2=2(2\cos^{2}2\theta-1)=2\cos(2^2 \theta)$ $\implies$$f(f(x))=2\cos(4\theta)$ Similarly $f(f(f(x)))=2\cos(2^3\theta)$ $\vdots$ $\underbrace {f\circ f\circ\cdots \circ f}_\text{n times}(x)=2\cos(2^n \theta)$ $\implies$ From Question i.e, $f^{n}(x)=x$ $\implies$ $2\cos(2^n\theta)=2\cos\theta$ $\implies$ $2^n\theta=2m\pi \pm \theta$ $\implies$ $\theta=\dfrac{2m\pi}{2^{n}-1}$ $\quad$ or $\quad$ $\theta=\dfrac{2m\pi}{2^{n}+1}$ $\forall \; \theta \in [0,\pi]$ Checking Result for $n=1$ $\theta=\dfrac{2m\pi}{1}\;$ or $\; \theta=\dfrac{2m\pi}{3}$ $\implies$ $\theta = \dfrac{2\pi}{3}, 0$ i.e., $2$ solution. Checking Result for $n=2$ $\theta=\dfrac{2m\pi}{3}\;$ or $\; \theta=\dfrac{2m\pi}{5}$ $\implies$ $0, \dfrac{2\pi}{3},\dfrac{2\pi}{5},\dfrac{4\pi}{5}$ i.e. $4$ solution. $\implies$ There are $2^{n}$ distinct root of $f^{n}(x)=x\;$ equation. Is my Solution Correct?","['algebra-precalculus', 'functions', 'solution-verification']"
4377222,Is the semicubical parabola differentiable?,"Let $\alpha(t)=(t^2,t^3)$ for $t \in \mathbb{R}$ .
The image of the curve looks like the semicubical parabola. I wanted to know if $\alpha$ is differentiable.
The component functions are both differentiable, with $\alpha'(t)=(2t,3t^2)$ But looking at its image, it should not be differentiable in $t=0$ . I am a little bit confused, because as far as I know,
if $f :[a,b] \rightarrow \mathbb{R}^n$ . Then f is differentiable in $x_0$ if every component function $f_1,...,f_n$ is differentiable in $x_0$ . I did some thinking/calculating and on one hand $\alpha$ should be differnetiable, but on the other hand the curve $\beta(t)=(x,x^{2/3})$ has the same image whilst not being differentiable in $0$ . Is $\alpha$ differentiable in $0$ (and why/why not)?
Are there ""representation functions"" of a image that are not differentiable and other representations that are? Edit: I meant, the graph of the function $\beta(t)=x^{2/3}$ is the same as the image of $\alpha$ .","['analysis', 'differential-geometry']"
4377229,Solving an ODE and determining limiting value,"Context: This is from Chapter 3 of Christodoulou and Klainerman's stability of Minkowski space. We have a family of metrics $m_u$ on $S^2$ for each $u \in (u_0, \infty)$ (for some fixed $u_0 < 0$ ). We also define, separately, $m_\infty$ to be the standard unit round metric on $S^2$ . The goal is to prove, in an appropriate sense, that $(S^2, m_u)$ converges to $(S^2, m_\infty)$ (under some additional assumptions, the most geometrically illuminating of which is that $K(u) \to 1$ as $u \to \infty$ , where $K(u)$ is the Gaussian curvature of $m_u$ . Let $(e_A)_{A = 1, 2}$ be an $m_\infty$ -ONB such that the matrix $m_u(e_A, e_B)$ is diagonal, with smallest eigenvalue $\lambda(u)$ and largest eigenvalue $\Lambda(u)$ . The goal is to show that $\lambda(u), \Lambda(u) \to 1$ as $u \to \infty$ . Notation: We let $r(u)$ be the ""radius"" of $(S^2, m_u)$ , defined by the formula $$
\text{Area}(S^2, m_u) = 4\pi r(u)^2.
$$ Define $\mu(u) = \sqrt{\det_{m_\infty} m_u(e_A, e_B)} = \sqrt{\lambda(u)\Lambda(u)}$ . Assumptions: 1) The following integral is finite, and uniformly bounded for all values of $u$ : $$
\int_u^\infty r(u')^{-1} \kappa(u')\, du'.
$$ Here $\kappa$ is some nonnegative scalar function. One can derive the following ODE for $\mu$ : $$
\partial_u \mu(u) = r^{-1}\kappa \mu.
$$ C-K then say that $$
\mu(u) = \exp(-\int_u^\infty r(u')^{-1}\kappa(u')\, du').
$$ My question: There should be a constant out front corresponding to the ""initial"" value $\lim_{u \to \infty}\mu(u)$ . Hence it appears they are saying it is 1. I don't see how this is possible, as this seems to be what we are trying to prove?","['partial-differential-equations', 'general-relativity', 'ordinary-differential-equations', 'differential-geometry']"
4377254,Why is the surface of a torus flat?,"Why is the surface of a torus is said to be flat? If you consider the geometry of the torus, its surface has locally positive (spherical), negative (hyperbolic) and flat curvature.","['curvature', 'differential-geometry']"
4377261,Deriving the tangential directional derivative of a scalar function on a parametric surface,"Given are a parametric surface $S(u,v) = \big( x(u,v), y(u,v), z(u,v) \big)$ and a scalar function $f(u,v)$ , both defined on the same domain $\Omega \subset \mathbb{R}^2$ . As such, we can associate every point on the surface $S(u,v)$ with a scalar value $f(u,v)$ . Throughout, it is assumed that the surface is regular and the function $f$ differentiable. â€‹ Out of curiosity, I'm interested in deriving an expression for the tangential directional derivative of the function $f$ as seen from the surface $S(u,v)$ , which I'll denote $\tilde{f}$ . That is, considering a point $p = S(u,v)$ and a nearby point $q = S(u+ha, v+hb)$ with $h$ variable and for some (domain) direction $w = \begin{pmatrix}a\\b\end{pmatrix}$ , what is the result of $$\lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\|q-p\|}.$$ Below I've written down my approach â€” the question is whether the result is correct (I do have some doubts about it, as explained below). As the numerator is the difference of two function values, it is equivalent to $f(u+ha, v+hb) - f(u,v)$ . Substituting the expressions for $p$ and $q$ , the denominator is $\| S(u+ha, v+hb) - S(u,v) \|$ . From the Taylor expansion , we know that $S(u+ha, v+hb) \approx S(u,v) + ha \frac{\partial S}{\partial u}(u,v) + hb \frac{\partial S}{\partial v}(u,v)$ for small $h$ , which in the limit becomes an equivalence. As such, in the limit the denominator represents the length of a tangent vector I'll denote $h \tilde{w}$ . Introducing the $3 \times 2$ Jacobian matrix $J = \big(\frac{\partial S}{\partial u}, \frac{\partial S}{\partial v}\big)$ , we can express this tangent vector as $h \tilde{w} = h J w$ . We can now re-write the above as $$\lim_{h \to 0} \frac{\tilde{f}(q) - \tilde{f}(p)}{\| q - p \|} = \lim_{h \to 0} \frac{f(u+ha, v+hb) - f(u,v)}{h} \frac{h}{\| q - p \|} = D_w f(u,v) \frac{1}{\| \tilde{w} \|} = \frac{\nabla f \cdot w}{\| \tilde{w} \|},$$ where $D_w f(u,v)$ is the directional derivative of $f(u,v)$ in the direction of $w$ . Next, with $\tilde {w} = J w$ , we have $w = J^+ \tilde{w}$ , with $J^+$ the (left) Moore-Penrose inverse . That is, $J^+ = (J^TJ)^{-1} J^T$ . Introducing the first fundamental form (i.e. the metric tensor ) $g = J^TJ$ , we can write $J^+ = g^{-1} J^T$ . Therefore, we have $$\frac{\nabla f \cdot w}{\| \tilde{w} \|} = \frac{ \left(\nabla f\right)^T g^{-1} J^T \tilde{w}}{\| \tilde{w} \|} = \left( J g^{-1} \nabla f \right) \cdot \frac{\tilde{w}}{\| \tilde{w} \|},$$ which follows from transposing (recall that $g$ is symmetric and therefore $g^{-1}$ is as well). On the one hand, this looks promising, as $J g^{-1} \nabla f$ matches what I've seen described as the tangential gradient (also referred to as the surface gradient ) in the literature. Therefore, like the ordinary directional derivative, the tangential directional derivative appears to be the dot product of a gradient and a direction vector. On the other hand, I'm a bit sceptical about this vector appearing normalised . Is this a result of using this specific approach, or did I make a mistake/wrong assumption somewhere along the way?","['derivatives', 'differential-geometry']"
4377283,Can I use Bayes Theorem to understand Medvedev vs Nadal win predictor probabilities at AO 2022?,"In recent days this picture constantly popped up in my LinkedIn feed. It shows two predictions for the outcome of the 2022 Australian Open Final between Medvedev and Nadal, made in two different points in time: one before the match and the second when the score was 2-0 Medvedev (remember: the first who gets to 3 wins). In the end Nadal won 2-3 against all odds, but it's not what I'm interested in. I am rather interested in understanding if it makes sense to apply some Bayesian notions to this scenario. Specifically, what I asked myself is: can I imagine that the model which output the second prediction was a Bayesian model and treat these two sets of predictions as the prior and the posterior? If I can, can I compute the likelihood that made the model output that posterior? And anyway, does it make any sense? I mean, is it something interesting/relevant? With this in mind, I defined: $P(H)$ = prior hypothesis: Medvedev's initial probability of winning = $64/100$ $P(\neg H)$ = Medvedev's prior propability of losing = $36/100$ $P(E|H)$ = (not sure how to phrase it, any help is appreciated...) = $\lambda$ $P(E|\neg H)$ = (not sure how to phrase it, any help is appreciated...) = $\mu$ $P(H|E)$ = Medvedev's updated probability of winning = $96/100$ Then I applied the formula as follows: $$
P(H|E) = \frac {P(H) P(E|H)}{P(E)} = \frac {P(H) P(E|H)} {P(H) P(E|H) + P(\neg H) P(E|\neg H)}
$$ $$
\frac{96}{100} = \frac {\frac{64}{100} \lambda} {\frac{64}{100} \lambda+ \frac{36}{100} \mu}
$$ Following some advice, I solved for $\lambda / \mu$ : $$
\frac{\lambda}{\mu} = \frac{27}{2} = 13.5
$$ Also, for simplicity, I found two values $\lambda_0$ and $\mu_0$ such that $\lambda_0 + \mu_0 = 1$ and I got: $$
\lambda_0 = 27/29 \approx 93\% \text{  ;  } \mu_0 = 2/29 \approx 7\%
$$ Given this I'd conclude that a Bayesian model that started with a 64-36% prior would have needed to compute a 93% likelihood in Medvedev's win to update it to 96%. I mean, it had to believe that Medvedev would win 93% of those 64 matches. Is my claim valid? Is this formulation correct? Is this a correct way to apply the Bayesian Theorem in a practical scenario? I would like to hear and learn from the community! :) Note You might have encountered this question, slightly differently on CrossValidated but I preferred to move it here since I discovered it is a bigger community and it is possibly even more relevant here.","['statistical-inference', 'statistics', 'bayesian', 'bayes-theorem', 'probability']"
4377349,Wolfram Alpha wrong Integral,I want to evaluate the following integral: $$\int_{-\infty}^0\frac{R}{\sqrt{x^2+R^2}^3}\mathrm{d}x$$ If I do it with the substitution $x=R\tan(\varphi)$ I get: $$\int_{-\infty}^0\frac{R}{\sqrt{x^2+R^2}^3}\mathrm{d}x=\frac{1}{R}$$ Which is the same like Wolfram Alpha. But if I use the antiderivative $F(x)=\frac{x}{R\sqrt{x^2+R^2}}$ I get: $$\int_{-\infty}^0\frac{R}{\sqrt{x^2+R^2}^3}\mathrm{d}x=\underbrace{F(0)}_{=0}-F(-\infty)=-\frac{1}{R\sqrt{1+\frac{R^2}{x^2}}}\bigg|_{x=-\infty}=-\frac{1}{R}$$ So where is the mistake?,"['integration', 'wolfram-alpha']"
4377358,Sum of matrix inverses,"I am trying to compute the following finite sum: $$\sum_{i=1}^n\left(A+\lambda_iB\right)^{-1}$$ In this sum, $A$ and $B$ are are positive-definite matrices (so they are inversible). The $\lambda_i$ are positive numbers so the sums $\left(A+\lambda_iB\right)$ are also positive-definite (and inversible). Is there a way to factor or rewrite this sum in a way where we can only compute $A^{-1}$ and $B^{-1}$ once instead of having to inverse so many matrices? I tried to use Woodbury's identity but I couldn't simplify the sum much. I could also rewrite the sum as: $$\sum_{i=1}^n B^{-1}\left(B^{-1}+\lambda_iA^{-1}\right)^{-1}A^{-1}$$ but it doesn't really simplify the problem. Edit: To simplify the problem we can instead compute the following sum: $$S(\lambda) = \sum_{i=1}^n\left(I+\lambda_iC\right)^{-1}$$ where $I$ is the identity matrix and $C$ is another positive-definite matrix.","['matrices', 'matrix-calculus', 'positive-definite', 'symmetric-matrices']"
4377373,Ã‰tale topology corresponds to complex topology?,"I always had the intuition that on a complex projective variety, the Ã©tale topology can be thought as the complex topology of the analytification, so every closed point has a ""small"" neighbourhood. However, I do not see how something like $B_\epsilon(0)$ for the projective line can be represented by an Ã©tale map $U \to \mathbb{P}^1$ as I do not think an epsilon ball can be given a scheme-structure. Could somebody explain to me how this can be constructed as an Ã©tale neighbourhood or if this is not possible, what kind of neighbourhoods, which do not come from open immersions, are added? Best, Matthias","['etale-cohomology', 'algebraic-geometry']"
4377389,Finding a constant such that the following integral inequality holds,"I am tasked to find a constant $c>0$ such that for all $C^1$ functions in $(0, 1)$ this variational problem is true: $$cu(0)^2 \leq \int^1_0 u'^2 + u^2 dt$$ My instinct told me to calculate the minimizer of that funcitonal, and I got that it must be of the form $c_1e^t + c_2e^{-t} = \bar{u}$ , where $\bar{u}$ is the minimizer. Then we can compute the minimum quite easily in the general case and get that $c_1(e^{2} - 1)$ is a  lower bound for the functional. But all this allows me to state is that: $$c_1(e^{2} - 1) \leq \int^1_0 u'^2 + u^2 dt$$ I am not sure ow to introduce the $u(0)$ term.","['integration', 'ordinary-differential-equations', 'calculus-of-variations', 'calculus', 'optimization']"
4377396,Possible range of correlation between two random variables,"I was given this question in a job interview and I wasn't sure how to answer it. Suppose you have two random variables, $X,Y$ . Let us say that $X=1$ with probability $50\%$ and $0$ otherwise, $Y=1$ with probability of $30\%$ and $0$ otherwise. The first question was what is the range of probabilities for the event $X=1 ,Y=1$ ? The answer to that would be from $0\%$ to $30\%$ , so this is not the focus of my question The second question, which I was unsure of, is the following - what is the range of the correlation between both variables? The standard formula for correlation seemed quite unhelpful, especially since the question wasn't phrased like that with 0 and 1 (it was framed in the sense that $X$ or $Y$ occurred or did not occur)","['statistics', 'probability']"
4377397,Self-centralizing subgroups of order $p^2$ in $p$-groups of maximal class,"Let $G$ be a finite $p$ -group. It's an old result of Suzuki that if $G$ possesses a self-centralizing subgroup of order $p^2$ then $G$ has maximal class, and in fact the converse is true. However, the proof of the converse appears not particularly easy, at least as far as I know. See Huppert, Section III.14, or Leedham-Green--McKay for definitions. I will refer to Huppert for specific results. The only proof I know is the following: if $x\in G$ lies outside all $2$ -step centralizers $K_i=C_G(\gamma_i(G)/\gamma_{i+2}(G))$ for $2\leq i\leq n-2$ then $C_G(x)$ has order $p^2$ by Huppert, III.14.13. This proof is easy, just an induction up the central series. Thus the problem is proving that not all maximal subgroups are $2$ -step centralizers. If $|G|$ is small, say around $|G|\leq p^{p+1}$ , then there are not enough $K_i$ for all maximal subgroups to be one of the $K_i$ . If $|G|$ is large though, one appears to have to use the fact that $G$ cannot be exceptional, i.e., all $K_i$ are in fact equal (Huppert, III.14.18), or at least $G/Z(G)$ is not exceptional and hence the $K_i$ are at most two maximal subgroups (Huppert, III.14.6). So my question is, if one is only trying to prove the existence of self-centralizing subgroups of $p$ -groups of maximal class, is there a faster way that doesn't involve using the lack of exceptionality? One naive approach is to use induction on $|G|$ . The result holds for $G/Z(G)$ , so you obtain a self-centralizing subgroup there. The preimage is a subgroup $B$ of order $p^3$ , containing $Z(G)$ . If this subgroup is non-abelian then any subgroup $A$ of order $p^2$ other than $Z_2(G)\leq B$ is self-centralizing. But if $B$ is abelian, this doesn't work. And I don't see why $B$ is not abelian. Equivalently, $B$ can be contained in the $2$ -step stabilizer $C_G(Z_2(G))$ , and I don't see how to force it not to be, or at least that there is a choice of it not being. It seems that I end up back at trying to make sure there are enough maximal subgroups that are not the $K_i$ to allow me to make a choice, but I cannot see how to do this without proving the much stronger results about exceptionality.","['nilpotent-groups', 'group-theory', 'finite-groups', 'p-groups']"
4377468,Efficiently approximating the integral of $\operatorname{erf}(x y) \exp(-x^2)$,"I need to efficiently approximate this integral which represents the Gaussian-weighted area of a right triangle whose three points are the origin plus two points that form a vertical ""edge"", one of which is the edge's nearest point to the origin at $y=0$ and the other, the corner, which is at the same value of $x$ . This integral has two inputs, $x$ which is the distance to the edge and $s$ which is the slope from the origin to the corner point. To help with avoiding high degree components the slope needs be no larger than 1 (if the slope is originally larger than 1 the inputs are swapped and the result used to subtract to $\operatorname{erf}(x)\operatorname{erf}(s)$ ). $$\int_0^x \frac{\operatorname{erf}(t s) \exp(-t^2)}{2\sqrt{\pi}}\, dt$$ I tried to approximate it in the $x = [-3 , 3], s = [-1 , 1]$ range with a simple 2D Chebyshev approximation, however to achieve my desired maximum absolute error of about $4\cdot10^{-5}$ I need about 31 coefficients in my 2D polynomial. That's a bit much, ideally I would be able to use fewer operations. I have tried to improve the situation by applying operations to the integral before fitting, such as the reciprocal, dividing by $x$ or $s$ or trying to somehow use the limits to gain some sort of advantage, like making a polynomial to interpolate between the limit at $s=0$ (which is $\frac{1-\exp(-x^2)}{2\pi}$ ) and the integral at $s=1$ (which is $\frac{{\operatorname{erf}(x)}^2}{8}$ ), or somehow use the limit at $x=\inf$ (which for my purposes is very close to what we have at $x=3$ ) which is $\frac{\arctan(s)}{2\pi}$ , but this didn't help much in achieving more precision with fewer polynomial coefficients. Perhaps there is something clever that I didn't think of or something completely different than a mere polynomial that could be more efficient, perhaps a way to break the problem down into 1D steps. Integral to approximate, shown with added contours. Yellow is positive, blue negative. Horizontal axis is $x = [-3,3]$ , vertical is $s = [-1,1]$ . I'm aware that I could well segment the approximation into a table of small piecewise 2D polynomials, however due to the need to avoid table lookups and dynamic branching I'd rather keep it all in one piece. This integral is used to closely approximate the ideal graphical result of a concave or convex polygon convolved with a 2D Gaussian kernel by calculating the sum of the Gaussian-weighted areas of triangular subdivisions of the polygon, my current implementation works nicely but needs more precision and hopefully can be made more efficient.","['integration', 'chebyshev-polynomials', 'approximation', 'polynomials', 'gaussian-integral']"
4377483,Is there an explicit construction of this bijection?,"As part of my answer to another question , I needed the following fact: if $S = \{1, \ldots, n\}$ and $k \leq n/2$ , then there is a bijection $f : {S \choose k} \to {S \choose k}$ such that $t \cap f(t) = \emptyset$ for all $t \in {S \choose k}$ . Here $n$ and $k$ are positive integers, and ${S \choose k}$ denotes the family of all size- $k$ subsets of $S$ . Here's the proof I found for that fact. Let $p = \left\lvert{S \choose k}\right\rvert = {n \choose k}$ , and write ${S \choose k} = \{t_1, \ldots, t_p\}$ . Construct a bipartite graph $G$ on partite sets $A = \{a_1, \ldots, a_p\}$ and $B = \{b_1, \ldots, b_p\}$ by drawing an edge $a_ib_j$ whenever $t_i \cap t_j = \emptyset$ . Observe that $G$ is an ${n-k \choose k}$ -regular bipartite graph, where ${n-k \choose k} > 0$ , and therefore has a perfect matching $M$ , by Hall's Theorem. Now for each $i \in \{1, \ldots, p\}$ we have $a_ib_j \in M$ for exactly one value of $j$ , and we get the desired bijection just by taking $f(t_i) = t_j$ for the corresponding value of $j$ . Unfortunately, the proof above doesn't give an explicit construction of the bijection $f$ , which makes it hard to naturally use this bijection in a combinatorial proof. When $n = 2k$ , the function $f(t) = S-t$ is an easy example of a bijection with this property. Is there a nice explicit construction of such a bijection for general $k$ ? Some partial thoughts: it's tempting to try to build on the $n=2k$ case by modifying the function $f(t) = S-t$ , say by taking the function $f$ to be ""take the $k$ least elements of $S-t$ "", but it seems that the natural approaches to modifying that function end up failing to be injective (hence also fail to be surjective). For example, the "" $k$ least elements of $S-t$ "" function fails at $n=5$ and $k=2$ because it yields $f(\{3,4\}) = f(\{3,5\}) = \{1,2\}$ . When $k=1$ this is just asking for a derangement of $\{1, \ldots, n\}$ , and a function like $f(\{i\}) = \{i+1 \mod n\}$ works, where $x \mod n$ is the residue of $x$ modulo $n$ . When $k=2$ and $n \geq 4$ , I believe the following function works, where $\{x,y\} + i \mod n$ is shorthand for $\{x+i \mod n, y+i \mod n\}$ : $f(\{i, j\}) = \begin{cases} \{i, j\} + 2 \mod n, & \text{if $i-j \equiv \pm 1 \pmod{n}$} \\
\{i, j\} + 1 \mod n, & \text{otherwise.}\end{cases}$ This suggests that in a general construction, maybe we can just assign an integer $r_t$ for each $t \in {S \choose k}$ and use a map of the form $t \mapsto t+r_t \bmod{n}$ , with the values of $r_t$ chosen cleverly to ensure bijectivity and disjointness. However, this approach is doomed to fail when $t$ is a difference set for $\mathbb{Z}_n$ . To use an example of such a set due to Jungnickel, Pott, and Smith, when $n = 11$ and $t = \{1,3,4,5,9\}$ , it is easy to check that $t + r_t \mod 11$ intersects $t$ regardless of the choice of $r_t$ . So this approach cannot work in general either. Relevant external literature I've found so far: The $n = 2k+1$ case appears to have been solved by Kierstead and Trotter (1988) , in a superficially-different but equivalent formulation. Kai Jin (2019) refers to the problem of finding an explicit $1$ -factorization of the related ""bipartite Kneser graphs"" (equivalent to the graph $G$ described in the proof above) as a ""challenging open problem"", but we are only looking for an explicit description of one matching in a bipartite Kneser graph, not an entire $1$ -factorization.","['combinatorics', 'combinatorial-proofs']"
4377484,Functional inverse of $z=1+w+\cdots+w^{n-1}$,"Migrated to MO . I am interested in the functional inverse of $$
z=1+w+\cdots+w^{n-1},\quad w\geq0,\ n>1.
$$ This function is strictly increasing on $w\geq0$ and thus admits an inverse. My attempt: By Lagrange's theorem we may write the inverse: $$
w(z)=a+\sum _{k=1}^\infty g_k\frac{(z-f(a))^k}{k!},
$$ with $f(w)=1+w+\cdots+w^{n-1}$ and $$
g_k=\lim_{w\to a}\partial_w^{k-1}\left({\frac {w-a}{f(w)-f(a)}}\right)^k.
$$ Expanding $f(w)$ around $w=1$ gives $f(w)=nF(1,1-n;2;1-w)$ , which is a hypergeometric function. Choosing $a=1$ we write $$
\frac{f(w)-f(1)}{w-1}=-n\frac{F(1,1-n,2,1-w)-1}{1-w}=\frac{n(n-1)}{2}F(1,2-n;3;1-w).
$$ It follows $$
w(z)=1+\sum _{k=1}^\infty a_k\frac{2^k}{n^k(n-1)^k}\frac{(z-n)^k}{k!},
$$ with $$
a_k=\lim_{w\to 1}\partial_w^{k-1}\left(F(1,2-n;3;1-w)\right)^{-k}.
$$ Evaluating the limit for $a_k$ is certainly non-trivial. One thought was to use the FaÃ  di Bruno formula but I am a little unclear on the details of this calculation and am curious if $a_k$ can be written in a ""nice"" form that does not involve Bell polynomials. Could someone please fill me in on the details of computing the $a_k$ 's? Also, for which values of $z$ does this series converge? I was able to write a quick one line code in Mathematica to compute the $a_k$ 's, which may lend itself to finding a pattern: a[k_] := Limit[D[Hypergeometric2F1[1, 2 - n, c, 1 - w]^-k, {w, k - 1}], w -> 1] /. c -> 3 $a_1$ , $a_2$ , and $a_3$ seem to factor nicely while $a_4$ does not. Edit: Using FaÃ  di Bruno's formula I was able to write down an explicit form for the $a_k$ 's giving a final solution of $$
\bbox[5px,border:2px solid #C0A000]{%
w(z)=1+\frac{2(z-n)}{n(n-1)}+\sum_{k=2}^\infty\sum_{\ell=1}^{k-1}(-k)^{(\ell)}B_{k-1,\ell}\left(\left\{(-1)^m\tfrac{m!(2-n)_m}{(3)_m}\right\}_{m=1}^{k-\ell}\right)\frac{\left(\frac{2(z-n)}{n(n-1)}\right)^k}{k!},%
}
$$ where $(s)^{(n)}=\Gamma(s+1)/\Gamma(s-n+1)$ is the falling factorial, $(s)_n=\Gamma(s+n)/\Gamma(s)$ is the Pochhammer symbol, and $B_{n,k}$ is the partial Bell polynomial. It is interesting to note that this result also works for the more general case $\{n\in\Bbb R:n>1\}$ . All that I am still curious about is the radius of convergence for this series which I believe is $|z-n|<n-1$ . Here is Mathematica code to compare the exact function $w(z)$ to approximation obtained by truncating its series expansion: a[k_, n_] := 
 Sum[FactorialPower[-k, l] BellY[k - 1, l, 
    Table[(-1)^m (m! Pochhammer[2 - n, m])/Pochhammer[3, m], {m, 1, 
      k - l}]], {l, 1, k - 1}]
g[z_, n_, K_] := 
 1 + (2 (z - n))/(n (n - 1)) + 
  Sum[a[k, n] ((2 (z - n))/(n (n - 1)))^k/k!, {k, 2, K}]
gAprx[z_, n_] := 
 Quiet[N[Solve[(1 - w^n)/(1 - w) - z == 0, w, PositiveReals][[1, 1, 
     2]]]]
P[n_] := Manipulate[
  Plot[{gAprx[z, n], g[z, n, m]}, {z, 1, 2 n - 1}], {m, 2, 20, 1}]
P[5]","['real-analysis', 'lagrange-inversion', 'taylor-expansion', 'sequences-and-series', 'hypergeometric-function']"
4377501,Proving that $0.999\ldots=1$ using the supremum axiom,"Use the supremum axiom to show that $0.999\ldots=1$ . Hints: Consider the set $C=\{0.9, 0.99, 0.999, \ldots\}$ . Discuss if $C$ is upper bound, and find the supremum(s) of $C$ . We know that every number with decimal expansion $x=0.a_1a_2a_3\ldots$ is such that $0\leq x \leq 1$ . What would happen if $0.999\ldots < 1$ ? My attempt: Clearly, $0.999\ldots=\operatorname{sup}{(C)}$ since $0.999\ldots=\sum_{i=1}^{\infty}9\cdot10^{-i}$ has a $n+1$ -th positive term while $\sum_{i=1}^{n}9\cdot10^{-i}, \forall n \in \mathbb{N}$ doesn't, so $c \leq 0.999\ldots, \forall c \in C$ . We know that $0.999\ldots \leq 1$ , so all we need to do is show that $1=\operatorname{sup}{(C)}$ . I thought about showing there is no $k\in\mathbb{R}$ such that $0.999\ldots < k < 1$ , but I got stuck here. My question: How do I prove this with just elementary set theory? I don't want an answer based on arithmetic or analysis, since that's not how this question was meant to be solved. I would also like to know if showing $1=\operatorname{sup}{(C)}$ is the right path and if so how to proceed. My research: First, I asked my colleagues, but nobody I talked to got this question in the exam right. I tried to contact the professor but he did not respond. Online, I've found this thread but it did not get anywhere. I've wondered about this question for months so help would be really appreciated.","['elementary-set-theory', 'inequality', 'supremum-and-infimum']"
4377588,How to construct discontinuous derivative with piecewise-polynomials?,"Problem : Construct differentiable function with piecewise-polynomials, but it has discontinuous derivative. My Attempt At first I tried to make similar one of : $$x\to x^2\sin\left(\frac{1}{x}\right)$$ Since $x^2$ is already polynomial, I tried to imitate $\sin$ part with $$f\quad \colon\quad
 x\to (-1)^n2^n\left(x-\frac{1}{2^{n+1}}\right)\left(x-\frac{1}{2^n}\right)\quad \left(\frac{1}{2^{n+1}} \le x \le \frac{1}{2^n}\right)$$ for each $n \in \mathbb{N}$ . Then $f$ is differentiable for $x\neq 0$ . And I have problems : How can I find $f$ is bounded or not? (to find $f$ is continuous or not at $x=0$ ) (I tried to bound it with $x^2$ but I dont think its valid because $x^2$ is convex but the $f$ switches convex and concave infinitely many times ) (If $f$ is continuous  at $x=0$ ) Can I construct discontinuous derivative with $f$ and something? If I can't for '2', Is there any piecewise-polynomial which has discontinuous derivative?","['calculus', 'derivatives', 'analysis']"
4377607,Proof of the Spectral Theorem for Normal Operators,"I am reading Invariant Subspaces by H. Radjavi and P. Rosenthal. I need help understanding the proof of the spectral theorem for normal operators, as stated below. Theorem . If $A$ is a normal operator on a separable space, then there exists a finite measure space $(X,\mu)$ and $h\in L^\infty(X,\mu)$ such that $A$ is unitarily equivalent to the operator $M_h$ on $L^2(X,\mu)$ . I will reproduce the proof here, and ask questions simultaneously. I understand the proof in the case where $A\in \mathcal B(\mathcal H)$ has a cyclic vector $f$ , i.e. $\bigvee_{m,n = 0}^\infty \{A^n(A^*)^m f\} = \mathcal H$ , where $\bigvee$ denotes the closed linear span . So, let us move to the general case directly. Proof. Since for any $f\in \mathcal H$ , $\bigvee_{m,n = 0}^\infty \{A^n(A^*)^m f\}$ reduces $A$ , Zorn's lemma implies that there exists a collection $\{M_n\}_{n=1}^\infty$ of pairwise-orthogonal subspaces of $\mathcal H$ such that $\mathcal H = \bigoplus_{n=1}^\infty M_n$ , each $M_n$ reduces $A$ , and $A\vert_{M_n}$ has a cyclic vector for each $n$ (the fact that the collection is countable follows from separability of $\mathcal H$ ). How exactly is Zorn's lemma being used here, to produce the required collection of subspaces? It would be helpful if someone could provide the details. By the cyclic case, for each $n$ , there is a finite measure $\mu_n$ on $X_n:= \sigma(A\vert_{M_n})$ such that $A\vert_{M_n}$ is unitarily equivalent to $M_z$ on $L^2(X_n,\mu_n)$ . We can assume $\mu_n(X_n) \le 2^{-n}$ . Let $h_n(z) = z$ for all $z\in X_n$ . Then, $A$ is unitarily equivalent to $\bigoplus_{n=1}^\infty M_{h_n}$ on the space $\bigoplus_{n=1}^\infty L^2(X_n,\mu_n)$ . We must show how to regard $\bigoplus_{n=1}^\infty L^2(X_n,\mu_n)$ as $L^2(X,\mu)$ in such a way that $\bigoplus_{n=1}^\infty M_{h_n}$ is unitarily equivalent to $M_h$ on $L^2(X,\mu)$ for some $h$ . Relabel the elements of the sets $\{X_n\}$ to make the sets pairwise disjoint, and then let $X = \bigcup_{n=1}^\infty X_n$ . Define the measurable subsets of $X$ as the subsets which are countable unions of measurable subsets of the $\{X_n\}$ , and if $S = \bigcup_{n=1}^\infty S_n$ is a measurable set with $S_n \subset X_n$ for every $n$ , define $\mu(S) := \sum_{n=1}^\infty \mu_n(S_n)$ . Then, $(X,\mu)$ is a finite measure space. If we define $h$ on $X$ by $h(x) = h_n(x)$ for all $x\in X_n$ , then $h\in L^\infty(X,\mu)$ (in fact $\|h\|_\infty = \|A\|$ ). How is $\|h\|_\infty = \|A\|$ ? I believe this has something to do with $\|T\| = r(T)$ (the spectral radius) for normal operators $T\in \mathcal B(\mathcal H)$ . It is easily verified that $M_h$ on $L^2(X,\mu)$ is unitarily equivalent to $\bigoplus_{n=1}^\infty M_{h_n}$ , and hence to $A$ . Could someone please help me with explicit details of the above verification? I'm confused primarily because $\bigoplus_{n=1}^\infty M_{h_n}$ is not an operator on $L^2(X,\mu)$ , but on $\bigoplus_{n=1}^\infty L^2(X_n,\mu_n)$ . I understand that we are identifying $L^2(X,\mu)$ and $\bigoplus_{n=1}^\infty L^2(X_n,\mu_n)$ using the recipe above; but I am not clear about how to proceed. Thank you very much! Please let me know if any of the notations is not clear.","['proof-explanation', 'operator-theory', 'hilbert-spaces', 'functional-analysis', 'spectral-theory']"
4377609,An identity for holomorphic functional calculus,"Let $A$ be a $C^*$ -algebra, $p\in A$ a projection, and $a\in A$ self-adjoint. Let $\Gamma$ be the circle of radius $1/2$ with center $1$ . It is claimed that $$\frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}a(z-p)^{-1}\,dz=pa(1-p)+(1-p)ap.$$ I'm looking for a hint to prove this. Comment: I understand that $$\frac{1}{2\pi i}\int_\Gamma(z-p)^{-1}\,dz=p,$$ hence $pa(1-p)=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)}{z-p}\,dz$ and $(1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{(1-p)a}{z-p}\,dz$ . So $$pa(1-p)+(1-p)ap=\frac{1}{2\pi i}\int_\Gamma\frac{a(1-p)(z-p)+(1-p)a(z-p)}{(z-p)^2}\,dz.$$ But I'm not sure if this is the way.","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4377613,Cross section of a sphere cut by cylinder,"This is not a calculus question regarding the area of cross section, but it is more of just a geometry question. This must have an easy answer, but I'm sort of confused now. Suppose you cut out a sphere(say, $x^2+y^2+z^2=16$ , a sphere centered at the origin of radius $4$ ) by a cylinder which has an axis parallel to the z-axis, but not centered at the origin(say, $(x-2)^2+y^2=1$ in the coordinate space). Then, think of the figure which has a boundary made by the cylinder and the sphere.(My English fails me,sorry.) Then, the figure could be explained as a cut of a cylinder which is not parallel to the $xy$ -plane, so it must be an ellipse, yet the cut is on the sphere, so it must be a circle. Where does this contradiction come from? Can anyone explain it to me? Thanks in advance.",['geometry']
4377646,Proving that an injective function has an inverse,"My textbook defines inverses as follows. f and g are inverses of eachother if and only if $f( g(x) )=x $ for all $x\in$ dom( $g$ ) $g( f(x) )=x $ for all $x\in$ dom( $f$ ) It then proceeds to state that $f$ has an inverse if and only if it is injective (I am aware $f$ technically needs to be surjective as well but my course doesn't bother with that, and I know that any injective function can be made surjective by restricting the codomain) , and I am trying to prove that. I have already proven that injectivity $\Leftarrow$ invertibility as follows. Let $f$ be an invertible function. Then there exists a function $g$ such that $f( g(x) )=x $ for all $x\in$ dom( $g$ ) $g( f(x) )=x $ for all $x\in$ dom( $f$ ) Let $a,b\in$ dom( $f$ ) such that $f(a)=f(b)$ Then $a=g(f(a))=g(f(b))=b$ I now need to prove that injectivity $\implies$ invertibility . Intuitively, it makes sense because if $f$ maps every element of the domain to a unique element in the codomain, then the mapping of every element of the codomain (restricted to image of $f$ ) to the domain must also be a function. I don't know how to mathematically prove this using the given definitions, and would appreciate any hint on how to go about it.","['proof-writing', 'functions']"
4377669,Why does the method of Lagrange Multipliers fail when $\nabla g =0$?,"I know that one of the preconditions to find the extrema of $f(x,y)$ subject to $g(x,y)=0$ using the method of Lagrange Multipliers is that $\nabla g \neq 0$ . I do understand that if $\nabla g$ does equal zero, we will have to equate ( $(0,0)$ with some other finite ordered pair representing the gradient of $f$ and we will end up missing an extremum. I am looking for a more intuitive reason for why this is a condition. For instance, why does it graphically mean that we will end up skipping an extremum? Why can't we say that, well, since we cannot equate $(0,0)$ with some $(a,b): a,b\neq0$ , such an extremum doesn't exist at all?","['optimization', 'multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
4377727,Spectral radius of the restriction to invariant subspace,"Let $(X,\|\cdot\|_{X})$ and $(Y,\|\cdot\|_{Y})$ be two complex Banach spaces such that $X\hookrightarrow Y$ and $X$ is dense in $Y$ . Let $T:Y\to Y$ be a bounded linear operator that leaves $X$ invariant, i.e. $T(X)\subset X$ . Furthermore, suppose that the restriction $T|_X$ is a bounded operator on $X$ . Is it possible to say something about the relation of the spectral radii $r_Y(T)$ and $r_X(T|_X)$ ? In particular, is it possible that $r_X(T|_X)>r_Y(T)$ ? Remark: If one restricts the operator to a subspace the point spectrum can only decrease but it's not clear whether this holds for the whole spectrum.","['spectral-radius', 'spectral-theory', 'functional-analysis']"
4377769,Locally free sheaf and locally constant system,"Suppose that $(M,\mathcal{O})$ is a $k$ -ringed space, where $k$ is a field. By $k$ -ringed I mean the structure sheaf is a sheaf of $k$ -algebras with local stalks and residue field $k$ . Suppose that $V$ is a sheaf of vector spaces over $k$ such that $V\otimes_k \mathcal{O}$ is locally free of finite rank. Does it follow that $V$ is a local system, i.e. locally constant? The assumption implies that the stalks $V$ are of locally constant dimension. I am interested in the case where $M$ is an analytic space/variety (so locally connected, hausdorff, etc. can be assumed).","['analytic-geometry', 'coherent-sheaves', 'algebraic-geometry', 'sheaf-theory', 'general-topology']"
4377799,If $\tan\theta +\sin\theta=m$ and $m^2 -n^2=4\sqrt{mn}$ so prove that $\tan\theta-\sin\theta=n$,"If $\tan\theta +\sin\theta=m$ and $m^2 -n^2=4\sqrt{mn}$ so prove that $\tan\theta-\sin\theta=n$ I found the similar question in Quora . There was slightly a mistake. $$(\tan\theta+\sin\theta)^2-n^2=4\sqrt{mn}$$ $$(\tan\theta-\sin\theta)^2+4\tan\theta\sin\theta-n^2=4\sqrt{mn}$$ $$(\tan\theta-\sin\theta)^2+4\sqrt{\tan^2\theta-\sin^2\theta}-n^2=4\sqrt{mn}$$ $$(\tan\theta-\sin\theta)^2+4\sqrt{m(\tan\theta-\sin\theta)}-n^2-4\sqrt{mn}=0$$ I just said that ""we have to take $4\sqrt{m(\tan\theta-\sin\theta)}=4\sqrt{mn}$ to satisfy $\tan\theta-\sin\theta=n$ but I don't like it, I just want direct derivation."" And in quora they just took $4\sqrt{m}$ common and wrote $$(\tan\theta-\sin\theta)^2+4\sqrt{m}(\sqrt{\tan\theta-\sin\theta-n})-n^2=0$$ which is totally wrong cause $\sqrt{\tan\theta-\sin\theta-n}\neq\sqrt{\tan\theta-\sin\theta}-\sqrt{n}$","['algebra-precalculus', 'trigonometry']"
4377810,Is there any intuitive way to check whether a function is continuous at a given point?,"In my exams, the questions on continuity of multivariable functions are framed like ""Discuss the continuity of $f(x,y)$ at $(a,b)$ ..."" or ""Is $f$ continuous at $(a,b)$ ...?"", and likewise. If I know beforehand that the given function is discontinuous at a given point $(a,b)$ , then I just need to find out two paths where the the value of $\lim_{(x,y)\to (a,b)} f(x,y)$ are different or not equal to $f(a,b)$ . On the other hand, if I know that the function is continuous at the given point, then I can use the $\varepsilon-\delta$ definition of continuity to prove continuity. But since the questions don't seem to be giving much away about the continuity of the function at the given point, I'm not sure which approach should I take first while trying to solve the question. Is there any quick and intuitive way to figure out whether a given multivariable function is continuous or not? At least in the cases where the given function is of the form $\frac{p(x,y)}{q(x,y)}$ , where $p$ and $q$ are polynomials in $x$ and $y$ ? (Exceptions are fine. Just a generic and practically useful trick would do.) For example, the function $$f(x,y) =\begin{cases} \frac{x^{4}-y^{4}}{x^{4}+y^{4}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases}$$ is discontinuous at $(0,0)$ , while the function $$g(x,y) =\begin{cases} \frac{x^{2}y^{2}}{x^{2}+y^{2}} & (x,y)\neq (0,0) \\  0 & (x,y)=(0,0) . \end{cases}$$ is continuous at $(0,0)$ . Is there any easy way to pick this just by looking at the functions? The same issue exists with finding a limit and proving the existence of the limit.","['limits', 'multivariable-calculus', 'continuity', 'epsilon-delta']"
4377865,The canonical measure on a Riemannian manifold,"It seems well-known that the volume form natually induces a canonical measure on a Riemannian manifold. For example, this page mentions this fact. I am wondering the construction of such a measure. For my background, I learned some real analysis and read Tu's An Introduction to Manifolds . In Tu's book, the integral over a smooth manifold is defined for smooth forms (actually, it is no harm to extend the definition for continuous forms) via the usual Riemann integral. For an $n$ -form $\omega$ on a coordinate chart $(U,\phi)$ , the integral is defined by $$ \int_U \omega = \int_{\phi(U)} (\phi^{-1})^* \omega $$ where the right hand side is the Riemann integral. For an $n$ -form on a smooth manifold, we use an open cover of coordinate open sets and a partition of unity subordinate to the cover to define the integral of $\omega$ . For an oriented $n$ -dimensional Riemannian manifold, the volume form can be written as $$vol = \sqrt{g} dx^1 \wedge \dots \wedge dx^n$$ Then, we can use a partition of unity to define the integral of a smooth (or continuous) function via Riemann integral. In particular, the integral of a function $f$ is defined by $$ \int_M f = \int_M fvol$$ where the RHS is the integral over an $n$ -form. Here is my idea about the construction of the canonical measure $\mu$ on the Borel $\sigma$ -algebra of a Riemannian manifold $M$ . For a measurable set $V$ contained in a chart $(U,\phi)$ , since a hemeomorphism maps a Borel set to a Borel set, we can define $\mu(V)$ by setting $$ \mu(V) = \int_V vol = \int_{\phi(V)} (\phi^{-1})^* vol $$ where the RHS is now intepreted as a Lebesgue integral. Then for any Borel set $V$ , we choose an open cover consisting of coordinate open sets and a partition of unity to define $\mu(V)$ . After checking $\mu(V)$ is independent of the cover and a partition of unity, a  positive measure $\mu$ is defined so that we can talk about sets of measure zero and $L^2$ functions. Is this the right way to construct the canonical measure on the Borel $\sigma$ -algebta of an oriented Riemannian manifold? If so, is this the most commonly used measure for geometric analysis (e.g. Jost's Riemannian Geometry and Geometric Analysis)?","['measure-theory', 'riemannian-geometry']"
4377867,Can three perpendicular bisectors determine a triangle?,"I would like to know if it is possible to determine the sides of a triangle given the lengths of the three perpendicular bisectors.
Assume that the three perpendicular bisectors intersect at $P$ and intersects $a, b, c$ respectively at $K, L, M$ , and let the lengths of $PK, PL, PM$ be $p_a, p_b, p_c$ which is given. Is it possible to determine the lengths of $a, b, c$ ? We know that $$\frac {a^2}{4}+p_a^2=\frac {b^2}{4}+p_b^2=\frac {c^2}{4}+p_c^2=r^2$$ Where $r$ is the radius of the circumscribed circle.
Additionally, we have $$\frac{1}{2}(ap_a+bp_b+cp_c)=\sqrt{s(s-a)(s-b)(s-c)}$$ And by substitution, we have $$a^2+4p_a^2=\frac{a^2(a^2+4p_a^2-4p_b^2)(a^2+4p_a^2-4p_c^2)}{a^2(a^2+4p_b^2-4p_c^2)}$$ What to do next?","['euclidean-geometry', 'triangles', 'geometry', 'plane-geometry']"
4377870,Graph where edges have group structure,"In the mathematical literature there are examples of graphs where the vertices form a group - the most famous example are probably the Cayley graphs . I'm curious about a somewhat dual situation. Are there examples in mathematics
of multigraphs (many possible edges between the same two vertices), where the set of edges between two vertices forms a group in an interesting way? I'd be particularly interested in the case of an abelian group, that could be interpreted as ""the sum of two edges is again an edge"".","['graph-theory', 'group-theory', 'abelian-groups', 'examples-counterexamples']"
4377910,Some Applications of the Spectral Theorem for Normal Operators,"I have used the spectral theorem for normal operators, to prove some basic results related to the spectra of operators. Please provide feedback for my proofs, and feel free to suggest any other ways (using the spectral theorem). The Spectral Theorem . If $T$ is a normal operator on a separable space, then there exists a finite measure space $(X,\mu)$ and $h\in L^\infty(X,\mu)$ such that $T$ is unitarily equivalent to the operator $M_h$ on $L^2(X,\mu)$ . Throughout, assume that $T$ is a normal operator. By the Spectral Theorem as stated above, there exists a unitary $U$ such that $U^*TU = M_h$ . We have $\sigma(T) = \sigma(UM_hU^*) = \sigma(M_h) = \operatorname{essran} h$ (the essential range of $h$ ). I have also shown that if $\operatorname{essran} h \subset A \subset\mathbb C$ , then $h(x) \in A$ a.e. on $X$ , and I will use this in the proofs below. $1$ . $T$ is Hermitian if and only if $\sigma(T) \subset \mathbb R$ . Suppose $T$ is Hermitian, i.e. $T=T^*$ . So, $M_h = M_h^* = M_{\overline{h}}$ . This implies $h = \overline{h}$ everywhere, so $h$ is real. As $\operatorname{essran} h \subset \overline{\operatorname{im} h} \subset \mathbb R$ , we have $\sigma(T) \subset \mathbb R$ . Conversely, suppose $\sigma(T) \subset \mathbb R$ , i.e. $\operatorname{essran} h \subset \mathbb R$ . So, $h$ is real a.e., i.e. $h = \overline{h}$ a.e. Let $f\in L^2(X,\mu)$ .
As $h = \overline{h}$ a.e., we have $hf = \overline{h}f$ a.e.; but elements of $L^2(X,\mu)$ are really equivalence classes of functions agreeing with each other a.e, so $M_h f = M_{\overline{h}} f$ . That is, $M_h = M_{\overline{h}} = M_h^*$ . As $T = UM_hU^*$ , we have $T=T^*$ . $2$ . $T$ is unitary if and only if $\sigma(T) \subset S^1$ , where $S^1 := \{z\in \mathbb C: |z| = 1\}$ . Suppose $T$ is unitary, i.e. $TT^* = T^*T = I$ . We get $TT^* = UM_hU^*UM_{\overline{h}}U^* = UM_hM_{\overline{h}} U^* = UM_{h\overline{h}}U^* = I$ . So, $M_{h\overline{h}} = I$ . Take arbitrary $f\in L^2(X,\mu)$ and see that $f = h\overline{h} f$ which means $|h|^2 = 1$ giving $|h| = 1$ . That is, $\operatorname{im} h \subset S^1$ . Since closure preserves inclusions, and $S^1$ is a closed subset of $\mathbb C$ , I argue that $\operatorname{im} f\subset S^1$ . As $\operatorname{essran} h \subset \overline{\operatorname{im} h}$ , we have $\operatorname{essran} h \subset S^1$ . Therefore, $\sigma(T)\subset S^1$ . Conversely, suppose $\sigma(T) \subset S^1$ , i.e. $\operatorname{essran} h \subset S^1$ . So, $|h| = 1$ a.e. on $X$ , i.e. $h\overline{h} = 1$ a.e. Using an argument similar to that in the previous proof, we conclude $M_{h\overline{h}} = M_h M_{\overline{h}} = I$ . It follows that $TT^* = T^*T = I$ , i.e. $T$ is unitary. $3$ . $T$ is positive if and only if its spectrum is on the non-negative real axis, i.e. $\sigma(T) \subset \mathbb R_{\ge 0}$ . Firstly, let us see that $T$ is positive if and only if $M_h$ is positive. We have $\langle Tx,x\rangle = \langle UM_hU^*x,x\rangle = \langle M_hU^*x, U^*x\rangle$ , and the claim follows from the observation that $U^* = U^{-1}$ is invertible. Now, suppose $T\ge 0$ , i.e. $M_h \ge 0$ . So, for every $f\in L^2(X,\mu)$ , $\langle M_hf,f\rangle \ge 0$ . Expanding the inner product, we have $\langle M_hf, f\rangle = \int_X hf\overline{f}\, d\mu = \int_X h|f|^2 \, d\mu \ge 0$ . Putting $f = \chi_{\{h < 0\}}$ , we get $\int_X h\chi_{\{h < 0\}}\, d\mu = \int_{\{h < 0\}} h\, d\mu \ge 0$ ; but $\int_X h\chi_{\{h < 0\}}\, d\mu = \int_{\{h < 0\}} h\, d\mu \le 0$ , giving $\int_{\{h < 0\}} h\, d\mu = 0$ . This forces $\mu(\{h < 0\}) = 0$ , i.e. $h\ge 0$ a.e. Define $h_1:= h\chi_{h\ge 0}$ . Clearly, $h = h_1$ a.e., so $\operatorname{ess ran} h = \operatorname{ess ran} h_1$ . As $\operatorname{ran} h_1 \subset [0,\infty)$ (giving $\overline{\operatorname{ran} h_1} \subset [0,\infty)$ ), we have $\operatorname{ess ran} h_1 \subset [0,\infty)$ . It follows that $\operatorname{ess ran} h \subset \mathbb R_{\ge 0}$ , as required. For the converse, assume $\sigma(T)\subset \mathbb R_{\ge 0}$ . Then, $\operatorname{essran} h \subset \mathbb R_{\ge 0}$ . This implies $h(x) \ge 0$ a.e. on $X$ . Therefore, for any $f\in L^2(X,\mu)$ , $\int_X h|f|^2\, d\mu = \int_X hf\overline{f} d\mu \ge 0$ , i.e. $\langle M_h f,f\rangle \ge 0$ . So, $M_h \ge 0$ which yields $T\ge 0$ . $4$ . $T$ is a projection if and only if $\sigma(T)\subset \{0,1\}$ . If $P\in \mathcal B(\mathcal H)$ is an idempotent operator, i.e. $P^2 = P$ , then the following are equivalent: (i) $P$ is an orthogonal projection onto a closed subspace (ii) $P$ is Hermitian (iii) $P$ is normal. Suppose $T$ is a projection; then $T^2 = T$ . So, $M_{h^2} = M_h$ which gives $h = h^2$ a.e., i.e. $h(x) \in \{0,1\}$ a.e. Define $h_1:= h\chi_{S}$ , where $S = h^{-1}(\{0,1\})$ . Clearly, $h_1 = h$ a.e. and $h_1(x) \ge 0$ for all $x\in X$ . Consequently, $\operatorname{ess ran} h = \operatorname{ess ran} h_1 \subset \overline{\operatorname{ran} h_1} \subset \{0,1\}$ . Thus, $\sigma(T) \subset \{0,1\}$ . Conversely, suppose $\sigma(T)\subset\{0,1\}$ , i.e. $\operatorname{ess ran} h \subset \{0,1\}$ . Then, $h(x) \in \{0,1\}$ a.e., i.e. $h^2 = h$ a.e. which means $M_h = M_{h^2}$ , and $T=T^2$ . Since $\sigma(T) \subset \mathbb R$ , $T$ is also Hermitian. In view of the aforementioned equivalence, $T$ is a projection. Thank you very much for reading! I would appreciate any feedback.","['measure-theory', 'operator-theory', 'solution-verification', 'functional-analysis', 'spectral-theory']"
4377934,Does every quasisimple finite group have a faithful complex irrep?,"Every simple finite group has a faithful complex irrep. Indeed any nontrivial irrep of a simple finite group is faithful. That leads me to ask: Does every quasisimple group have a faithful complex irrep? Note: I am aware that not all finite groups have a faithful complex irrep. For example, any noncyclic abelian group (e.g. $ C_2 \times C_2 $ ) has no faithful complex irreps.","['representation-theory', 'group-theory', 'simple-groups', 'finite-groups']"
4377944,Transition probability exponential in distance or difference,"Is there a situation, for example in physics or in dynamical systems,
where we have a Markov chain where the transition probability between two states satisfies a law such as $$
p(y|x) = C e^{-d(x,y)}
$$ where $d(x,y)$ is either a distance (as in a metric space), or a difference of some kind (for example, difference of energy density in physics)? The Boltzmann distribution has a similar law for the relative probabilities (the Boltzmann factor), but that's not quite the same as transition probabilities.","['statistical-mechanics', 'statistics', 'markov-chains', 'dynamical-systems']"
4377957,Warped Product Metric: assumption on the metric,"I am studying a book of differential geometry where the author says: Let $(M,g)$ be a Riemannian manifold of dimension $2$ with coordinates $(t,x)$ endowed with the following warped product metric: \begin{align*}
M=\mathbb{R} \times \mathbb{S}^1, \quad g(t,x)=dt^2+f^2(t) dx^2,
\end{align*} where $f: \mathbb{R} \rightarrow (0,\infty)$ is smooth, odd with $f^{\prime}(0)=1$ . Could you please explain to me these assumptions on $f$ ? For example, I speculate that $f$ must be smooth and odd so that one can extend it to all of $\mathbb{R}$ , right? But why do we need $f^{\prime}(0)=1$ ? Is this without loss of generality?","['surfaces', 'riemannian-geometry', 'differential-geometry']"
4377963,Why do all eighteen of these rotations have the same angle of rotation?,"In 3 dimensions, a rotation can be characterized by a roll (an $\alpha$ degree rotation around the $x$ axis), pitch (a $\beta$ degree rotation around the $y$ axis), and a yaw (a $\gamma$ degree rotation around the $z$ axis), say in that order. (Let's call roll, pitch, and yaw the different 'modes' of rotation.) Therefore we can express the total rotation $R =  R_\alpha * P_\beta * Y_\gamma$ where each is a rotation matrix that I don't want to write out here. We can bash out what this matrix product evaluates to (it's not that difficult), and we find that its trace is $tr(R) = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma}$ . Alternatively, we can characterize $R$ by an axis-of-rotation which it leaves fixed and the angle that everything else gets rotated by (around the axis-of-rotation). This is Euler's Rotation Theorem. Therefore, if we let $T$ be any rotation (there are many) that brings the axis-of-rotation to the x-y axis, and then $S_\theta$ a rotation around the x-y axis by the angle theta (ie. the matrix $\begin{pmatrix}\cos{\theta} & -\sin{\theta} & 0 \\ \sin{\theta} & \cos{\theta} & 0 \\ 0 & 0 & 1\end{pmatrix}$ ), we can write $R = T S_\theta T^{-1}$ . Taking the trace, we get $tr(R) = tr(T S_\theta T^{-1}) = tr(T^{-1} T S_\theta) = tr(S_\theta) = 1 + 2\cos{\theta}$ where we use the following trace property: Cyclic Permutation Property of Trace (or just the ""Trace Property""). For three square matrices $A,B,C$ , we have $tr(ABC) = tr(CAB) = tr(BCA) := x$ . (Thus we also have $tr(BAC) = tr(ACB) = tr(CBA) :=y$ ; but $x \ne y$ in general.) Note that $CAB, BCA$ are cyclic permutations of $ABC$ , hence the name of the property. Therefore, we have $1 + 2\cos\theta = \cos{\alpha} \cos{\beta} + \cos{\beta} \cos{\gamma} + \cos{\gamma}\cos{\alpha} + \sin{\alpha} \sin{\beta} \sin{\gamma}$ . It is interesting that this expression is symmetric in the variables $\alpha, \beta, \gamma$ . This means that if we exchange the angles of in any of the 3!=6 ways (while keeping the modes in the same order), we obtain rotations that still have the same angle of rotation (but possibly different axes). Call this the ""Angle Permutation Property"". The Trace Property above also implies the same with the 3 cyclic permutations of the modes of rotation - call that the ""Mode Cyclic Permutation Property"". Combining the 6 permutations of the angles ( $\alpha, \beta, \gamma$ ) with the 3 cyclic permutations of the modes, we have found a set of 18 related rotations all of which have the same angle of rotation (but possibly different axes). Is there a geometric argument for the truth of the Angle Permutation Property? of the Mode Cyclic Permutation Property (for which a geometric argument for the Trace Property suffices)? Are their axes related in a similar way as well? What is the geometric significance of this equivalence of these 18 rotations? By the way, assuming $\alpha, \beta, \gamma$ are all small, we obtain $\theta^2 \approx \alpha^2 + \beta^2 + \gamma^2 (+\alpha\beta\gamma)$ , which perhaps makes sense because the three rotations are in some vague sense ""orthogonal"" (which sense?) but is perhaps surprising given that the surface of the sphere has only two dimensions, not three!","['matrices', 'rotations']"
4378000,"Ramsey Theory on subsets of $\{1, 2, \dots, n\}$, $k$ colored.","Prove that there exists a $n$ natural for each $k > 0$ natural such that, by coloring all the subsets of $\{1, 2, \dots, n\}$ with $k$ colors, there exist two disjoint subsets among them, $X$ and $Y$ , such that $X, Y, X \cup Y$ are identically colored. I am aware of Schur's theorem, which has a relatively similar statement, but it is actually for numbers in a set with their sum, while this problem is for sets in a 'family' with their union. I'm thinking to create a bijective indicator, that transforms each of the elements of $S \in \mathcal{P(n)}$ into a natural number $\omega(S)$ , indicator function that also have the property that for distinct sets: $$\omega(A) + \omega(B) = \omega(A \cup B)$$ Actually, If I proved this, I would have solved the problem since Schur's theorem may be used. However, I couldn't find a proof for this fact on a general case. For example, a valid (satisfies the definition conditions, but does not form a correct Schur-type set) function for $n = 3$ would be: $\omega(\emptyset) = 0$ , $\omega(\{1\}) = 1$ , $\omega(\{2\}) = 3$ , $\omega(\{3\}) = 5$ , $\omega(\{1, 2\}) = 4$ , $\omega(\{1, 3\}) = 6$ , $\omega(\{2, 3\}) = 8$ and $\omega(\{1, 2, 3\}) = 9$ .","['coloring', 'combinatorics', 'ramsey-theory', 'elementary-set-theory', 'arithmetic-combinatorics']"
4378070,"Integral of Bessel function, power, and hyperbolic cosecant","I have the integral $$
I(a)=\int\limits_0^\infty dk \ \frac{k^3J_1(ak)}{\sinh(k )}
$$ Where $J_1$ is a Bessel function of the first kind . By plotting the integrand, it seems $I(a)$ exists as the integrand is finite and goes to zero for large $k$ . I wonder if $I(a)$ may be expressed in `common' special functions ? Mathematica does not evaluate it, and I have been unable to find it in Gradshtein. I have been able to find an approximation to $I(a)$ as $a \to 0$ by replacing $J_1$ with its asymptotic form for small argument. Unfortunately, in my application the interesting case is $a\to \infty$ . If $I(a)$ cannot be simplified, how can we develop an approximation to it for large $a$ ? Background: The integral arises in a Laplacian boundary value problem for the induced charge on a grounded plate.","['integration', 'definite-integrals', 'special-functions', 'asymptotics', 'bessel-functions']"
4378103,Confusion regarding infinite series involving complex number,"I've been asked to find $\Omega$ , such that : $$\Omega=\frac{\Omega_1}{\Omega_2}=\frac{1+e^{\frac{2i\pi}{3}}+e^{\frac{4i\pi}{3}}+e^{\frac{6i\pi}{3}}+e^{\frac{8i\pi}{3}}+.....}{i+\frac{i^2}{2}+\frac{i^3}{4}+\frac{i^4}{8}+.......}$$ I noticed that in the numerator, $e^{\frac{2i\pi}{3}}=(e^{2\pi i})^{1/3}=1^{1/3}=\omega$ which is the cube root of unity. Hence, $e^{\frac{4i\pi}{3}}=(e^{2\pi i})^{2/3}=1^{2/3}=\omega^2$ . Similarly, $e^{\frac{6i\pi}{3}}=(e^{2\pi i})^{3/3}=1^{3/3}=\omega^3=1$ , and the pattern repeats. Since, $1+\omega+\omega^2=0$ , The entire series $\Omega_1$ must also be equal to $0$ , and hence $\Omega=\frac{\Omega_1}{\Omega_2}=0$ However, this is what one of my teachers did in a note : I don't see how this second method is correct, and my method is wrong. I don't think that in the second method, we can add the successive terms in the gp series like that. Any help would be highly appreciated.","['solution-verification', 'roots-of-unity', 'sequences-and-series', 'geometric-series', 'complex-numbers']"
4378105,What is the mode of a continuous random variable?,"Consider a ""discrete"" random variable $X$ . A mode of $X$ is just a maximizer of $P(X = x)$ . This is obviously useful, and we can easily see that a mode is a ""most likely"" value for $X$ . If, instead, we have a ""continuous"" real-valued random variable $X$ with a PDF $f_{X}$ , I think we usually define a mode of $X$ to be a maximizer of $f_{X}$ . I have two questions: How can we interpret the mode of a continuous random variable? In other words, why is the mode of a continuous random variable useful to probability theory? Some websites say that it is ""the value most likely to lie within the same interval as the outcome"", but I can't make sense of that. Some people point to the obvious geometric visualization of the mode (as the peak of the PDF) but I don't think that justifies the usefulness of the mode at all. Is there a more general definition of mode, removing the assumptions above that $X$ is real-valued and has a PDF?","['soft-question', 'probability']"
4378107,Why does mutual information use KL divergence?,"Mutual information between a pair of random variables $X,Y$ having joint distribution $P_{(X,Y)}$ and marginal distributions $P_X,P_Y$ respectively is defined as $$I(X,Y)\equiv D_{\text{KL}}(P_{(X,Y)}\|P_X\otimes P_Y ),$$ where $D_{\text{KL}}$ is the KL divergence. Intuitively, this measures how much ""information"" is revealed about one random variable through observing the other by quantifying how far the joint distribution is from the product of marginals (this distance being zero when $X,Y$ are independent). Why not more flexibly allow for other notions of statistical distance ? i.e. Why not define $$\tilde I(X,Y,d)\equiv d(P_{(X,Y)},P_X\otimes P_Y )$$ for arbitrary distance $d$ ? There are distances that are at least as compelling as KL divergence, such as Jensen-Shannon divergence , which at least symmetrizes KL divergence, or
the Wasserstein metric , which is actually a metric and enjoys other attractive properties (as observed in the ML literature ). I understand mutual information as defined has connections with entropy, so perhaps this makes the definition tractable? What merits are there in using the KL divergence vs. other distances?","['statistics', 'entropy', 'information-theory']"
4378258,The determinant of a Cartan matrix is positive,"Let $R\subset V$ be an abstract root system in a finite-dimensional vector space $V$ . Let $\Pi \subset R$ be a base of $R$ . Define the Cartan matrix of $R$ as $C(R)=(\langle \alpha,\beta \check{}\rangle)_{\alpha,\beta \in \Pi}$ . My lecture notes claim: Since we may assume $V$ to be a Euclidean vector space, $\det(C(R)) > 0$ . Why is that? Why do we have to assume that $V$ is a Euclidean vector space? Since $\langle\alpha,\beta \check{}\rangle=2 \frac{(\alpha,\beta)}{(\beta,\beta)}$ (for an inner product $(-,-)$ on $V$ that is invariant under the Weyl group of $R$ ) the definition of the Cartan matrix is reminiscent to the Gram matrix of an inner product? Are the two related?","['determinant', 'lie-algebras', 'matrices', 'definition', 'abstract-algebra']"
4378270,Externalizing A Concrete Application of Double Negation Toposes,"I'm trying to come up with concrete problems which can be solved via topos theory, and I've found a good case study which has been really instructive. I've spent the past few weeks trying to understand how it interacts with double negation sheaves, but I can't quite get it straight in my head. Hopefully somebody here will be able to help ^_^. For completeness, recall the Weierstrass Approximation Theorem, which says $$\forall f : C \big ( [0,1], \ \mathbb{R} \big ) . \ \forall \epsilon : \mathbb{R}_{> 0} . \ \exists p : \mathbb{R}[t] . \ \forall t : [0,1] . \ | f(t) - p(t) | < \epsilon$$ Now say we have a continuous family of functions $f_x : [0,1] \to \mathbb{R}$ instead. so $f : X \times [0,1] \to \mathbb{R}$ for some topological space $X$ . It's reasonable to ask if the polynomials $p_x$ approximating $f_x$ vary continuously in $X$ . We might expect the Weierstrass Approximation Theorem to be constructive. After all, the argument by Bernstein Polynomials actually gives us a sequence in-hand of approximating polynomials. Then, inside the topos $\mathsf{Sh}(X)$ , the theorem is true, and externally we would be able to see that For all $f : X \times [0,1] \to \mathbb{R}$ continuous, for all $\epsilon > 0$ , there is an open cover $U_\alpha$ of $X$ and polynomials $p_\alpha$ with coefficients continuous on $U_\alpha$ so that $|f(x,t) - p_\alpha(x,t)| < \epsilon$ for every $t \in [0,1]$ . which gives us a local solution to our problem. Unfortunately, in the usual proof that the bernstein polynomials really do approximate $f$ we do the standard analysis trick of separating into ""good"" and ""bad"" parts, then we show the good parts are small, and there aren't many bad parts. This uses LEM when we assert that every summand is either good or bad. From here, it's reasonable to pass to the double negation sheaves $\mathsf{Sh}_{\lnot \lnot}(X)$ , where the argument goes through... But I can't figure out how to externalize the claim! I know that $\mathsf{Sh}_{\lnot \lnot}(X)$ is equivalent to sheaves on the locale of regular opens. But usually this is a pointfree locale, and I'm not sure how the (dedekind) real numbers there (which, I think, are continuous maps from the locale of regular opens to the locale $\mathbb{R}$ ) relates to the real numbers in $\mathsf{Sh}(X)$ (which are continuous maps $X \to \mathbb{R}$ ). I've heard that truth in the double negation sheaves is the same as ""truth on a dense open set"", so that our claim reads something like ""there is a dense open set $V$ and an open cover $U_\alpha$ of $V$ so that ...."" but I'm still not sure if that works. Any guidance on externalizing statements from the double negation sheaves would be fantastic! Ideally a concrete example like this, with some explanation as to how the translation goes. (Also, I'm aware of the truly constructive proof in Bridges Constructive Functional Analysis , Chapter $4.3$ , which gets us out of this hole. But this is a good case study in working with double negation sheaves, so I would still like to know how to externalize the claim in the more complicated way.)","['weierstrass-approximation', 'functional-analysis', 'sheaf-theory', 'constructive-mathematics', 'topos-theory']"
4378290,Standardized radius of validity of normal coordinates in differential geometry?,"Depending on the curvature of a certain manifold $M$ , intuitively  it is interesting to quantify how quickly normal coordinates defined at a point $p$ deteriorate away from $p$ . Is there a standardized measure to quantify the actual size of the neighbourhood of $p$ the normal coordinates are valid in? If so, how is it calculated?","['neighbourhood', 'differential-geometry']"
4378386,proof of a bounded sequence satisfying a certain property,"Determine with proof whether there exists a bounded sequence $\{a_n\}_{n\ge 1}$ of real numbers such that for all $k > l \ge 1$ we have $|a_k-a_l|\ge \frac{1}{k-l}$ . I think the statement is true. Consider the sequence defined by $a_0 = 0, a_1 = 2$ and for $0\leq k < 2^n, a_{2^n+k} = a_k+\frac{1}{2^{n-1}}.$ I tried proving the claim by induction on $k - l$ , with the base case being $k-l=1$ . To show this, perhaps it might be useful to assume for a contradiction that there are two consecutive terms whose absolute difference is less than $1$ ? Then maybe split the proof up into cases depending on whether $0\leq k,k+1 < 2^n$ or $k = 2^n-1$ for some $n\ge 1$ . Assuming the base case holds, however, let $1\leq k < l$ and find $n_1$ and $m_1$ so that $2^{n_1} \leq l < 2^{n_1+1}, 2^{m_1} \leq k < 2^{m_1+1}.$ Then $a_l = a_{l-2^{n_1}} + \frac{1}{2^{n_1-1}}$ . But the problem is it's not clear whether the difference between $l-2^{n_1}$ and $k-2^{m_1}$ is smaller than $k-l$ .","['calculus', 'sequences-and-series', 'induction', 'real-analysis']"
4378463,Proof that $|\cos(x+y)|\geq |\cos(x)|-|\sin(y)|$,"I am working on a problem, and one of the keys to the solution for it is to prove the following inequality: $$|\cos(x+y)|\geq |\cos(x)|-|\sin(y)|$$ for $x,y\in \mathbb{R}.$ I have verified that this is true numerically, and I tried various things to show this, but none seem to work. First, I tried to expand the LHS, $$|\cos(x+y)|=|\cos(x)\cos(y)-\sin(x)\sin(y)|\geq |\cos(x)\cos(y)|-|\sin(x)\sin(y)|,$$ but here I run into a problem, since $$|\cos(x)\cos(y)|-|\sin(x)\sin(y)|\not\geq |\cos(x)|-|\sin(y)|.$$ Next, I rearranged the terms in the initial inequality, so $$|\sin(y)|\geq |\cos(x+y)|-|\cos(x)|.$$ Then, I worked with the RHS, $$|\cos(x+y)|-|\cos(x)|\leq |\cos(x+y)-\cos(x)|$$ with the intention of using sum to product formulas to prove this. However, again, we run into the problem that $$|\cos(x+y)-\cos(x)|\not\leq |\sin(y)|$$ everywhere. I think it is possible to use the same ideas and prove by casework, but if possible, I would like something simpler than that. A friend suggested that the inequality looked similar to the triangle inequality with some Law of Sines involved in this way: $$\cos(x+y)=\sin\left(\frac{\pi}{2}-x-y\right)$$ $$\cos(x)=\sin\left(\frac{\pi}{2}+x\right)$$ and the angles add up to $\pi.$ Perhaps this provides some motivation for the solution, but I'm not able to see it yet. Any help appreciated.","['algebra-precalculus', 'trigonometry', 'inequality']"
4378506,How to derive the triple product rule with Nonstandard Analysis?,"$\frac{\partial x}{\partial y} \cdot \frac{\partial y}{\partial z} \cdot \frac{\partial z}{\partial x} = -1$ according to the triple product rule However, it would be 1, if derivatives behaved like fractions. And in Nonstandard Calculus derivatives do behave like fractions. (it treats derivatives as fractions of actual infinitesimal quantities, rather than d/dx applied to a function) So, how does Nonstandard Calculus get the correct answer here? (which it must since it is equivalent to standard calculus)","['multivariable-calculus', 'nonstandard-analysis', 'analysis']"
