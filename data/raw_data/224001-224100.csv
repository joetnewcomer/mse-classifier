question_id,title,body,tags
4609001,Definition about probability mass function by Rober V. Hogg,"The following definition of pmf is on page51 from Probability and statistical inference by Robert V. Hogg, etc. The pmf $f(x)$ of a discrete random variable X is a function that satisfies the following properties: (a) $f(x)\gt 0, x\in S;$ (b) $\sum_{x\in S}f(x)=1;$ (c) $P(X\in A)=\sum_{x\in A}f(x),$ where $A\subset S$ . My question is about part(c). X is a random variable, and A is a subset of sample space S, how can $X\in A$ . Moreover, how can ${x\in A}$ ? In my opinion, the LHS of part(c) just means that we want to compute the probability of a single event, and the RHS of part(c) just means that we sum up all the related possibilities that we need. For example, if X: numbers of the sum of a fair six-side dice. Then P(X=3)=P({(1,2),(2,1)})=1/36+1/36=2/36, where A is a subset of S. Am I right? And could someone explain more about part(c)? It's better if someone can give me an example.","['statistical-inference', 'statistics', 'probability-distributions', 'probability-theory', 'probability']"
4609011,"For a finite extension $K/\mathbb{Q}_2$, the extension $K(\sqrt{-1})/K$ is always totally ramified","Let $K$ be a finite extension of the $2$ -adic numbers $\mathbb{Q}_2$ , and suppose that $-1$ is not square in $K$ . Write $K(i)$ for the quadratic extension $K[X]/(X^2 + 1)$ , where $i^2 = -1$ . Is it true that $K(i)/K$ is always a totally ramified extension? If so, how do you prove it? My attempt My first idea was to find an Eisenstein polynomial defining the extension. The element $i$ has minimal polynomial $f(X) = X^2 + 1$ over $K$ , so I wanted to find $a \in K$ such that $f(X + a) = x^2 + 2aX + a^2 + 1$ is Eisenstein. This is equivalent to having $v_K(a^2 + 1) = 1$ . The existence of such an $a$ is equivalent to there being some unit $u \in \mathcal{O}_K^\times$ such that $a^2 = u\pi_K - 1$ , so we need $u\pi_K - 1$ to be square for some unit $u$ . I am unsure how to show that such a $u$ always exists. Another idea is to consider the unique unramified quadratic extension of $K$ , and show that it does not contain a square root of $-1$ . I know that an unramified extension is always obtained by adjoining an $m^\mathrm{th}$ root of unity for $(m,p) =1 $ , but I don't know how to show that $-1$ won't be square in that extension.","['local-field', 'number-theory', 'p-adic-number-theory', 'ramification']"
4609026,Evaluate $\int \frac{\sqrt{1+x^4}}{1-x^4}dx$,"How to evaluate $$\int \frac{\sqrt{1+x^4}}{1-x^4}dx.$$ If we substitute $x=\sqrt{\tan{\theta}}$ , it becomes $$\int \frac{1}{2\cos{\theta}\cos{2\theta}\sqrt{\tan{\theta}}}d\theta.$$ What can I do next ? Edit Are these steps correct ? $$=\int \frac{1}{2\cos{\theta}\cos{2\theta}\sqrt{\frac{\sin\theta}{\cos\theta}}}d\theta$$ $$=\int \frac{1}{2\cos{2\theta}\sqrt{\cos^2 \theta\frac{\sin\theta}{\cos\theta}}}d\theta$$ $$=\int \frac{1}{2\cos{2\theta}\sqrt{\cos^2 \theta\frac{\sin\theta}{\cos\theta}}}d\theta$$ $$=\int \frac{1}{\sqrt{2}\cos{2\theta}\sqrt{\sin{2 \theta}}}d\theta$$ Now If we substitute $t=\sin{2\theta}$ , it becomes $$=\int \frac{1}{\sqrt{2}(1-t^2)\sqrt{t}}dt$$ $$=\int \frac{\frac{1}{t^2}}{\sqrt{2}\frac{(1-t^2)}{t}\sqrt{\frac{t}{t^2}}}dt$$ $$=\int \frac{\frac{1}{t^2}}{\sqrt{2}(\frac{1}{t} -t)\sqrt{\frac{1}{t}}}dt$$ Now If we substitute $u^2=\frac{1}{t}$ , it becomes $$=\int \frac{-2u}{\sqrt{2}(u^2 -\frac{1}{u^2})u}du$$ $$=\int \frac{-u^2}{\sqrt{2}(u^4 -1^2)}du$$ $$=\int \frac{-u^2}{\sqrt{2}(u^2 +1)(u^2-1)}du$$ $$=-\frac{1}{2\sqrt{2}}\int \frac{u^2+1+u^2-1}{(u^2 +1)(u^2-1)}du$$ This can be solved easily now and we will get an elementary solution. But Wolfram alpha gives a solution in non elementary functions. Therefore I am confused. I guess the above solution is correct only for some restricted values of x.","['integration', 'indefinite-integrals']"
4609028,Derivation of entropy inequality for scalar conservation laws from viscous equation - discontinuous or infinite integrand,"I am self-studying Numerical Methods for Conservation Laws by LeVeque. I have a question about the derivation of the entropy inequality for the convex scalar conservation law $$
u_t+f(u)_x=0\tag{1}
$$ which allows us to select unique weak solutions. I am sure that my question has been answered somewhere on stack exchange, but I haven't been able to express the mathematical verbiage that would allow me to find it. Background Let $\eta(u)$ be our ""entropy"" function which satisfies a conservation law of the form $$
\eta(u)_t+\psi(u)_x=0
$$ where $\psi(u)$ is some ""entropy flux"" function. Assume that both $\eta(u)$ and $\psi(u)$ are convex. Multiplying (1) by $\eta'(u)$ , we have $$
\eta'(u)u_t+\eta'(u)f'(u)u_x=0
$$ from which we infer $\psi'(u)=\eta'(u)f'(u)$ . Since we care about weak solutions admitting initial data with discontinuities in $u$ , we consider the viscous equation for (1) as $\epsilon \to 0$ . $$
u_t+f(u)_x=\epsilon u_{xx}
$$ From which we can write $$
\eta(u)_t+\psi(u)_x=\epsilon\eta'(u)u_{xx}
$$ Manipulating the right-hand side, we have $$
\eta(u)_t+\psi(u)_x=\epsilon(\eta'(u)u_x)_x-\epsilon \eta''(u)u_x^2
$$ Now, consider a rectangle $[x_1,x_2] \times [t_1,t_2]$ where $u(x,t)$ has some discontinuity in the limiting solution as $\epsilon \to 0$ . $$
\int_{t_1}^{t_2}\int_{x_1}^{x_2} \eta(u)_t+\psi(u)_x dxdt = \epsilon \int_{t_1}^{t_2}\int_{x_1}^{x_2} (\eta'(u)u_x)_x dxdt -\epsilon \int_{t_1}^{t_2}\int_{x_1}^{x_2} \eta''(u)u_x^2 dxdt
$$ LeVeque writes As $\epsilon \to 0$ , the first term on the right hand side vanishes. (This is clearly true if $u$ is smooth at $x_1$ and $x_2$ , and can be shown more generally.) The other term, however, involves integrating $u_x^2$ over $[x_1,x_2] \times [t_1,t_2]$ . If the limiting weak solution is discontinuous along a curve in this rectangle, then this term will not vanish in the limit. The entropy condition for weak solutions $$
\eta(u)_t+\psi(u)_x\leq 0
$$ follows. Question Can you help me understand why $$
\lim_{\epsilon\to 0}\left[\epsilon \int_{t_1}^{t_2}\int_{x_1}^{x_2} (\eta'(u)u_x)_x dxdt\right]=0
$$ but $$
\lim_{\epsilon\to 0}\left[\epsilon \int_{t_1}^{t_2}\int_{x_1}^{x_2} \eta''(u)u_x^2 dxdt\right]\neq 0
$$ Related: Burgers' equation - Integrate discontinuity over rectangle Integral form of the conservation law $u_t + f(u)_x=0$","['integration', 'continuity', 'derivatives', 'partial-differential-equations']"
4609029,Criterion on a region to be a triangle in the three dimensional space.,"Given the real numbers $a_1,a_2,b_1,b_2,c_1,c_2\in [0,1]$ . Suppose we know that the set $$\begin{cases}x+y+z=1 \\a_1\le x \le a_2\\ b_1\le y \le b_2\\ c_1\le z \le c_2\\  \end{cases}$$ is a surface (not empty and not reduced to a point). Is there a criterion that enables us to know if the surface is a triangle? For example, if $a_1=b_1=c_1=0$ and $a_2=b_2=c_2=1$ , then the surface is a triangle (the standard 2-simplex). But, for example, the plot of $$\begin{cases}x+y+z=1 \\0\le x \le 0.2\\ 0.1\le y \le 0.6\\ 0.6\le z \le 0.7\\  \end{cases}$$ is not a triangle.","['simplex', 'geometry', '3d']"
4609037,Orthographic projection of a rectangle,"Rectangle $ABCD$ is projected using orthographic projection onto a plane that makes a known angle with the plane of the rectangle, as shown in the figure above.  Can the lengths of the sides of the rectangle be determined using the projected image? (Note: This is not specific to the parallelogram given in the figure.)","['projection', 'geometry']"
4609066,Question about the Poincaré first return function,"Let $M$ be a compact and connected two-dimensional differentiable manifold of class $C^2$ . Let $\varphi: \mathbb R \times M \to M$ be a flow of class $C^2$ in $M$ . Let $\sigma:[-1,1]\to M$ be an embedding of class $C^2$ such that $\Sigma:=\sigma((-1,1))$ is transversal to every orbit of $\varphi$ . Let $V\subset [-1,1]$ be the set of points whose image has its first return in $\Sigma$ .. We define the real function $f:V\to (-1,1)$ as follows $v\in (-1,1)$ is carried by $\sigma$ to $\sigma(v )\in\Sigma$ then the first return to $\Sigma$ from $\sigma(v)\in\Sigma$ is $\varphi(t_{\sigma(v)},\sigma(v))$ now it we return to $(-1,1)$ by $\sigma^{-1}$ . So that $$
f(v):=\sigma^{-1}(\varphi(t_{\sigma(v)},\sigma(v)))
$$ I'm trying to prove that $f'(v)\neq 0$ for all $v$ , but I can't see it. My ideas were to prove that $f$ is strictly monotone but I failed in the attempt. My second attempt was to get an explicit relationship between $f'(v)$ with $\sigma$ and $\varphi$ but I've been having a hard time getting anywhere. Maybe the answer is obvious but I need something concrete to justify it because I don't see it so clearly. Any suggestion will be of incredible help, please detail your answer a bit so I can understand it.","['differential-topology', 'ordinary-differential-equations', 'dynamical-systems']"
4609068,Find coordinates of centroid of an area,"I would like to find the centroid $(x_c^A,y_c^A)$ of the orange area $A$ , marked in the attached picture, i.e. of the area between $$
f(x)=\ln(x)\qquad\textrm{ and }\qquad g(x)=1, 0\leqslant x\leqslant e.
$$ To this end, I first determined that $A=e-1$ , because $B=\int_{1}^e\ln(x)\, dx=1$ . Hence $C=A+B=e$ . Now, the $x$ -coordinate $x_c^C$ of the centroid of $C$ , which is a rectangle satisfies $$
x_c^C=\frac{(x_c^A\cdot A)+(x_c^B\cdot B)}{C}=\frac{e}{2}.
$$ Moreover, $$
x_c^B=\frac{1}{B}\int_1^e x\ln x\, dx=\frac{1}{4}(e^2+1),
$$ so that $$
x_c^A=\frac{e^2-1}{4(e-1)}.
$$ Equivalently, I get $$
y_c^A=\frac{1}{e-1}.
$$ Would like to know if I am correct.","['multivariable-calculus', 'centroid']"
4609152,How would one go about integrating $\int x^2y'dx$ ?? - Implicit Integration??,"My friend and I are attempting to do some crazy integral stuff which he came up with but I am still having trouble getting my head around, but here's the general gist of what he is trying to get at: Propose we have a function: $f(x)={\sqrt{x}}$ . This of course is easy enough to integrate in of itself but what we're trying to figure out is if we make it so ${y^2}=x$ then integrate both sides of the function, can we still get the same answer. Here's how he computed it: $y^2=x$ $y^2y'=xy'$ $\int y^2y'dx=\int xy'dx$ The $\int y^2y'dx$ can be solved using u-substitution because recall that y is a function with respect to x rather than a constant. Additionally, the $\int xy' dx$ can be solved using integration by parts where one can arrive at $\int xy'dx=xy-\int y dx$ note how that $\int ydx = Y$ so now we have: $\frac{1}{3}y^3=xy-Y$ Now we solve for $Y$ : $Y=xy-\frac{1}{3}y^3$ $Y=\frac{2}{3}y^3$ This is the correct answer, but it's just in terms of y, substituting $y=\sqrt{x}$ into the equation, we arrive at: $Y=\frac{2}{3}x^{\frac{3}{2}}$ which is the correct answer if we'd just used the power rule of integrals. Now that was just an example to see if it worked the true problem arrises from when we take a new line, notably: $x^2+y^2=1$ and ""implicitly integrate"", note the quotations, just a made up name. If we do what we normally do: $x^2y'+y^2y'=y'$ $\int x^2y'dx + \int y^2y'dx = \int y'dx$ $\int x^2y'dx + \frac{1}{3}y^{3} = y$ Our problem occurs when we attempt to integrate that $x^2$ guy. After preforming 2 integrations by parts the integral ends up containing $Y$ which is just something that we don't want to happen. So if anyone knows what to do and we're just not seeing something painfully obvious please do tell. Addendum This is work we've done on $\int x^2y'dx$ and some other stuff as per the request of StinkingBishop: If $u=x$ , $du=dx$ and $dv=xy'$ , $v=\int xy'dx$ and then by subbing into the IBP formula we arrived at: $x(xy-Y)-\int (xy-Y)dx$ which is where problems arise because now we're integrating $Y$ . Additionally, we tried taking derivatives and manipulating the equation but the furtherest we got was: (Differentiation) $2x+2yy'=0$ then $y'=\frac{-x}{y}$ $x^2+y^2=1$ $\frac{x^2}{y}+y=\frac{1}{y}$ (Recall: $y'=\frac{-x}{y}$ ) $-xy'+y=\frac{1}{y}$ (Integrating) $-(xy-Y)+Y=\int \frac{dx}{y}$ and this is where we got stuck again, if anyone can help that would be great.","['integration', 'calculus']"
4609158,What is $\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right )$?,"I'm trying to rewrite $\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right )$ in some other way. I tried using Levi-Civita symbol and Kronecker delta, but I'm stuck. Here is what I did: $$\left ( \vec{\nabla} \times \vec{A} \right ) \cdot \left ( \vec{\nabla} \times \vec{A} \right ) = \left ( \vec{\nabla} \times \vec{A} \right )_i \left (\vec{\nabla} \times \vec{A} \right )_i = \epsilon_{ijk} \frac{\partial A_k}{\partial x_j} \epsilon_{imn} \frac{\partial A_n}{\partial x_m} = \left ( \delta_{jm} \delta_{kn} - \delta_{jn} \delta_{km} \right ) \frac{\partial A_k}{\partial x_j} \frac{\partial A_n}{\partial x_m}$$ $$= \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_m}{\partial x_j} \frac{\partial A_j}{\partial x_m} = \left ( \frac{\partial A_k}{\partial x_m} \right )^{2} - \frac{\partial A_j}{\partial x_j} \frac{\partial A_m}{\partial x_m} $$ And I'm stuck with both these terms. (I'm sorry for no rigour switching order of partials, but I couldn't come up with anything else). Where I messed up?","['vector-fields', 'multivariable-calculus', 'tensors', 'vector-analysis']"
4609163,(Uniqueness of) solution to $f'(t)=-\int_0^{f(t)}\!\mathrm dg(x)$,Let $g$ be a measure over the unit interval. Can we show that $f'(t)=-\int_0^{f(t)}\!\mathrm d g(x)$ has a unique nonzero solution $f$ ? (The boundary condition is that $f(0)>0$ and there is some $s>0$ with $f(t)=0$ for $t> s$ .),"['derivatives', 'ordinary-differential-equations', 'partial-differential-equations']"
4609217,Cut a number to a random integer between 0 and that number. Keep going until that number is 0. How many cuts do we need?,"Start with an integer like n = 100 and set it equal to a uniformaly random integer between [0,n] inclusive. Keep cutting it this way until n = 0 . What's the expected value of the number of cuts needed? For me, intuition gives an expected value of $\log_2 n ≈ 6.64$ , but empirical simulation in Python: import random
 
cuts = 0
expectedValue = 0
trials = 100000
 
for i in range(trials):
  startingValue = 100
  while startingValue > 0:
    startingValue = random.randint(0, startingValue)
    cuts += 1
 
expectedValue = cuts / trials
print(expectedValue) results in $≈6.18$ . Does there exist an explicit solution for n = 100 or for any integer n ?","['statistics', 'number-theory', 'probability-distributions', 'combinatorics', 'probability']"
4609235,"In a triangle, lines are drawn from each vertex to the opposite side. Can there be seven regions of integer area?","In a triangle, lines are drawn from each vertex to the opposite side. Can there be seven regions of integer area? Context My question was inspired by a similar question , which asks if a triangle is inscribed in a circle, can the four regions have integer area? My attempt I started by considering a simpler question: In a triangle, two lines are drawn, each one from a vertex to the opposite side. Can there be four regions of integer area? The answer is clearly yes. For example, if the triangle is equilateral, and the two lines go through the centre, then we can have four regions of area $1, 1, 2, 2$ : But when I consider three lines (and seven regions), the situation seems to be much more complicated, and I have not found a feasible approach.","['triangles', 'area', 'geometry']"
4609236,Can I use L'Hopital's rule when finding a derivative using the limit definition?,"When finding the derivative of $f(x) = \sqrt x$ via the limit definition, one gets $$f'(x) = \lim_{h \to 0} \frac{f(x+h) - f(x)}{h} = \lim_{h \to 0} \frac{\sqrt{x+h} - \sqrt x}{h}$$ For this, I could get the answer from applying L'Hopital's rule... but to me, this line of reasoning is a bit circular. I'm computing a derivative from first principles while needing to use derivative rules (from L'Hopital's) to compute this derivative. Can I use L'Hopital's rule when finding a derivative using the limit definition?","['limits-without-lhopital', 'limits', 'calculus', 'derivatives']"
4609269,"Why isn't there any method to solve integral, like differentiation has?","Differentiation of function has a method to solve, by limits $$ \frac{d(f(x))}{dx} = \lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ Is there any method by which we can solve integral without using antiderivative, like differentiation does?","['integration', 'definite-integrals', 'differential', 'limits', 'derivatives']"
4609272,"if deg(a)=0 in a graph, is ""a"" counted as an even vertex or not?","i was solving this test, and the answer of title affects the answer, so i was confused. Q) in a simple graph, p=10 and 2q/p=3. the graph has 3 isolated vertices and all other vertices have a degree of 5 or 4, how many even vertices does this graph have? 1. (5) 2. (6) 3. (7) 4. (8) thank you in advance (:","['graph-theory', 'discrete-geometry', 'discrete-mathematics', 'discrete-optimization']"
4609319,"Can we take the partial derivative of a function $f(x, y, z)$ with respect to $g(x, y, z)$","In Calculus 1 we differentiated $f(x)$ with respect to $g(x)$ that is, $\frac{df(x)}{dg(x)}$ by using chain rule: $$\frac{df(x)}{dx}\cdot\frac{dx}{dg(x)}= \frac{f'(x)}{g'(x)}$$ Is there something like this in partial derivatives too? For example: $$\frac{\partial f(x,y,z)}{\partial g(x,y,z)}$$ where $f(x, y, z)=xyz$ and $g(x, y, z)=x+y+z$","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
4609329,Why can they use continuity to compute this limit?,"(a) Use l'Hôpital's rule to find $\displaystyle\lim_{x\to0}\ln[(1+x)^{1/x}]$ . Solution: Since $\ln x$ is continuous, $$\lim_{x\to0}\ln\left((1+x)^{1/x}\right)=\ln\left(\lim_{x\to0}(1+x)^{1/x}\right)=1.$$ Therefore, $\displaystyle\lim_{x\to0}(1+x)^{1/x}=e$ . In the solution above, they've reasoned that because "" $\ln(x)$ "" is continuous, they are able to use limit laws to directly take the limit of the inner function. However, $\ln x$ is undefined and has a vertical asymptote at $x=0$ , so how are they able to still use continuity (and thus direct substitution) to compute this limit? If I were not allowed to use graphing software and had no idea how the graph of $\ln\left[(1+x)^{1/x}\right]$ looked like, how would I know that I can use continuity?","['limits', 'calculus', 'continuity']"
4609347,"A finite group of order $mn$ with $m,n$ relatively prime, together with subgroups of orders $m, n$.","Let $G$ be a finite group of order $mn$ with $(m,n) = 1$ . Assume that there exist subgroups $M,N$ of $G$ of orders $m$ and $n$ , respectively. Prove that $G$ is isomorphic to a subgroup of the symmetric group $S_{m+n}$ . It is easy to notice that $M\cap N = \{e\}$ since $m,n$ relatively prime, thus, $|MN|=\frac{|M||N|}{|M\cap N|} = mn$ . Hence, $G = MN = NM$ . To prove that $G$ is isomorphic to a subgroup of the symmetric group $S_{m+n}$ , I think I should construct a group action (G acting on some set of order m+n). And the set I think of is $A = M \cup N$ where identities in $M$ and $N$ respectively should be viewed as two different elements. Hence $|A| = m+n$ . If elements in $M$ commute with those in $N$ , I can define a group action $hk \cdot a = ha$ if $a \in M$ or $ka$ if $a \in N$ . (Here, $h \in M, k \in N$ , and $hk$ represents an element in $G$ ). However, I do not know how to prove the case where elements in $M$ do not commute with those in $N$ .","['group-isomorphism', 'abstract-algebra', 'symmetric-groups', 'group-theory', 'group-actions']"
4609350,"Explicit norm on $C([0,1])$ not dominated by $\|\cdot\|_\infty$?","Can we provide an explicit example of a norm $\|\cdot\|$ on the vector space of continuous functions $C([0,1])$ such that there is no constant $M>0$ with the property $\|\cdot\| \le M\|\cdot\|_\infty$ ? Some comments: It is shown here that, without AC, any complete norm on $C([0,1])$ is necessarily equivalent to $\|\cdot\|_\infty$ . So there is no explicit example of a complete norm $\|\cdot\|$ not dominated by $\|\cdot\|_\infty$ . A simple example of such a norm is $\|f\| := \|f\|_\infty + |\phi(f)|$ where $\phi$ is a discontinuous linear functional on $(C([0,1]),\|\cdot\|_\infty)$ . But of course we cannot construct such a functional $\phi$ on a Banach space without AC. I would assume therefore that the answer is no, and in that case I'm not looking for a rigorous proof or anything of the sort as I probably wouldn't be able to understand it anyway. For context, I was inspired by this question which inquires whether the set $$E = \left\{f \in C([0,1]) : \int_0^1 f(t)\,dt = 0\right\}$$ is closed in $C([0,1])$ . An answer pointed out that the OP initially did not specify the norm on $C([0,1])$ but then I realized I couldn't think of an example of a norm with respect to which $E$ wouldn't be closed. Of course it exists since $E$ is a kernel of a linear functional so it suffices to provide a norm which makes the functional discontinuous but this is probably impossible without AC.","['normed-spaces', 'real-analysis', 'functional-analysis', 'axiom-of-choice', 'set-theory']"
4609353,Fubini-Tonelli theorem,"I have the following question with me: Let $X, Y$ be independent real valued random variables and define $Z:=X+Y$ . Let $F_X, F_Y$ and $F_Z$ be the respective distribution functions of $X$ , $Y$ and $Z$ . Show that $$
F_Z(a)=\int F_X(a-y) P^Y(d y), \quad a \in \mathbb{R} .
$$ If $X$ has continuous density $f_X$ , show that $$
f_Z(a):=\int f_X(a-y) P^Y(d y), \quad a \in \mathbb{R}
$$ is the density of $Z$ .
If $X$ has continuous density $f_X$ and $Y$ has continuous density $f_Y$ , show that $$
f_Z(a):=\int_{-\infty}^{\infty} f_X(a-y) f_Y(y) d y, \quad a \in \mathbb{R}
$$ is the density of $Z$ . This is what I've tried: I can use the Fubini-Tonelli theorem to calculate $F_Z(a)$ . Also, integrating is easier than taking derivative. The following is the theorem:  Let $(E, \mathcal{E}, P)$ and $(F, \mathcal{F}, Q)$ be probability spaces.
a) Define $R(A \times B)=P(A) Q(B)$ for $A \in \mathcal{E}, B \in \mathcal{F}$ . R extends uniquely to a probability measure $P \otimes Q$ on $\mathcal{E} \times \mathcal{F}$ (it is called the product measure).
b) If $f$ is measurable and integrable (or positive) w.r.t. $\mathcal{E} \otimes \mathcal{F}$ , then the functions $$
\begin{aligned}
x & \mapsto \int f(x, y) Q(d y), \\
y & \mapsto \int f(x, y) P(d x)
\end{aligned}
$$ are measurable and $$
\begin{aligned}
\int f d P \otimes Q & =\int\left\{\int f(x, y) Q(d y)\right\} P(d x) \\
& =\int\left\{\int f(x, y) P(d x)\right\} Q(d y)
\end{aligned}
$$ But I'm not still not able to fully figure this out, and I would appreciate help. Thank you!!","['probability-distributions', 'fubini-tonelli-theorems', 'probability-theory', 'probability']"
4609380,Uniform limit of holomorphic functions on the unit disc,"my question is the following: Prove or disprove: There exists a sequence $\{f_n\}$ of holomorphic functions on the unit disc $\mathbb{D} := \{z:|z| < 1\}$ so that $f_n$ converges to $\bar{z}^3$ uniformly on the circle $C:=\{z:|z|=\frac{1}{2}\}$ . My attempt: I thought that the claim is wrong. My intuitive idea is to show that the uniform limit of holomorphic functions is holomorphic and $\bar{z}^3$ is not holomorphic. However, the question only claims uniform convergence on the circle C. As far as I understand, to say that the uniform limit of holomorphic function is holomorphic, we need uniform convergence in all compact subsets of a domain, it is not the case here. How should I attack this problem? Thanks in advance.","['complex-analysis', 'uniform-convergence']"
4609389,Confusion in limits with trigonometric functions,"Evaluate $$L=\lim_{r\to0} \frac{r\cos r}{r\cos r + \sin r}$$ In the solution it is written that as $r\to0$ , $\sin r = r$ and $\cos r = 1$ . Hence, we replace the trigonometric functions with $r$ and $1$ so that we can evaluate the limit easily. Therefore, $L=\lim_{r\to0} \frac{r\cdot1}{r\cdot1 + r}=\frac{1}{2}$ . Now, consider the limit $G =\lim_{r\to0} (\frac{1}{r^{2}}-\frac{1}{\sin^{2}r})$ . If we apply the above logic to this question, we will get $G=0$ , which is wrong. So why is the same logic not applicable in the second case?","['limits', 'trigonometry']"
4609410,How do I find poles and zeros from an ODE?,"Assume that you having an ODE. $$\dot x = x^3y x^2 + x + y + 10$$ How can I find the poles and zeroes here? If I was using a transfer function, $$G(s) = \frac{s^2 + 1}{s^2 + 0.1s + 50}$$ then the poles would be the roots of $0 = s^2 + 0.1s + 50$ and the zeros would be the roots of $0 = s^2 + 1$ But in this case, I don't have a transfer function. Just a regular ODE.","['control-theory', 'roots', 'ordinary-differential-equations', 'dynamical-systems']"
4609451,Confusion about the point at infinity with respect to inversion in geometry.,"I was studying inversion in Olympiad geometry, and they (Evan Chen's book EGMO) mentioned that we can extend the Euclidean plane by adding a point $P_{\infty}$ such that each line passes through it, and no circle passes through it. The reason for this they said was that now two parallel lines meet at that point only, and the center can go there on inversion. But now I have a really stupid confusion: Does this mean that all non parallel lines meet at two points and parallel lines meet at only one? I am very confused by this part now, I tried looking up some things on Wikipedia but they had defined very different things and it just made me more confused. I would really appreciate if anyone could clear this really dumb doubt of mine, Thank you!","['contest-math', 'definition', 'projective-geometry', 'geometry']"
4609474,Is $\sigma$-finiteness necessary for the uniqueness part of Hahn–Kolmogorov theorem?,"I'm reading Hahn–Kolmogorov theorem which states that ""if $\mu$ is $\sigma$ -finite, then the extension is unique"" . However, I did not use the assumption of $\sigma$ -finiteness in below theorem. Theorem Let $\mathcal X$ be an algebra on $X$ and $\mu:\mathcal X \to [0, \infty]$ . If $\nu, \nu'$ are measures on $\sigma (\mathcal X)$ such that $\nu|_{\mathcal X}  = \nu'|_{\mathcal X} = \mu$ , then $\nu=\nu'$ . Could you confirm if my proof is fine or I miss something...? Proof Let $\mathcal A := \sigma (\mathcal X)$ and $$
\mathcal D := \{A \in \mathcal A : \nu (A) = \nu' (A)\}.
$$ Then $\mathcal X \subset \mathcal D \subset \mathcal A$ . Let's prove that $\mathcal D$ is a monotone class . Let $(A_n) \subset \mathcal D$ such that $A_n \subset A_{n+1}$ . Let $A := \bigcup A_n$ . By continuity of measure from below, we have $$
\nu (A) = \lim_n \nu (A_n) = \lim_n \nu' (A_n) = \nu' (A).
$$ As such, $A \in \mathcal D$ . Let $(A_n) \subset \mathcal D$ such that $A_{n+1} \subset A_{n}$ . Let $A := \bigcap A_n$ . By continuity of measure from above, we have $$
\nu (A) = \lim_n \nu (A_n) = \lim_n \nu' (A_n) = \nu' (A).
$$ As such, $A \in \mathcal D$ . Let $m(\mathcal X)$ be the smallest monotone class containing $\mathcal X$ . By monotone class theorem , $m(\mathcal X) = \mathcal A$ . On the other hand, $m(\mathcal X) \subset \mathcal D$ . So $\mathcal A \subset \mathcal D$ . This completes the proof.",['measure-theory']
4609513,Variance of standardized linear regression coefficient,"Suppose we have standardized inputs (mean $0$ , variance $1$ ). So that $y=x_1\beta_1+x_2\beta_2+\varepsilon$ , (we have no $\beta_0$ due to standardization). With $\varepsilon \sim N(0,\sigma^{2})$ . Also note that we only have 2 variables. We wish to show that $Var(\beta_1)=\frac{\sigma^{2}}{1-r_{12}^{2}}$ , where $r_{12}$ is the sample correlation between $x_1,x_2$ . So I have arrived at $Var(\hat{\beta})=Var((X^{T}X)^{-1}X^{T}Y)=Var((X^{T}X)^{-1}X^{T}(X\beta+\varepsilon))=Var((X^{T}X)^{-1}X^{T}\sigma^2I) = \sigma^2(X^TX)^{-1}$ And I'm not exactly sure where to continue, any help would be appreciated.","['linear-regression', 'statistics', 'variance']"
4609536,"Solving the system $\cos(x)+\cos(x+y)=0$, $\cos(y)+\cos(x+y)=0$, where $0\leq x,y\leq 2\pi$ [duplicate]","This question already has answers here : How to find the stationary points of a function with trigonometric terms? (2 answers) Closed last year . Given: $$\cos(x)+\cos(x+y)=0$$ $$\cos(y)+\cos(x+y)=0$$ $$0\leq x,y\leq 2\pi$$ find all pairs $x,y$ such that the equations hold. My attempt: Looking at the first equation $$\cos(x)+\cos(x+y)=0$$ $$\cos(x)=-\cos(x+y)$$ note that $$\cos(a)=-\cos(a+\pi)$$ so we get $$\cos(x)=\cos(\pi+x+y)$$ And it seems we should apply the inverse. However, when applying the inverse we note that cos has a period of 2\pi, and I am unsure how to apply that concept in finding every root of this problem.",['trigonometry']
4609663,Finding the limit of $\lim_{x \to 0} \frac{\tan2x}{\sqrt{3x+1}-1}$,Finding this limit pretty hard because $\tan$ has fewer formulas (mentioned in my school book) unlike the other trigonometric functions like $\sin$ or $\cos$ . $$\lim_{x \to 0} \frac{\tan2x}{\sqrt{3x+1}-1}$$ I tried to solve it using factoring and I reached a level where I'm stuck at this $$\lim_{x \to 0} \frac{\tan2x  \left(\sqrt{3x+1}+1\right)}{3x}$$ I'm not quite sure if the progress is correct or not in the first place so help please to solve this.,"['limits', 'algebra-precalculus', 'trigonometry']"
4609679,Galois action on rational points vs. base change of a scheme,"Let $k\newcommand{\Spec}{\operatorname{Spec}}\newcommand{\id}{\operatorname{id}}$ be a field, let $K/k$ be a Galois extension, let $G = \operatorname{Gal}(K/k)$ , and let $X$ be a $k$ -scheme. I want to understand the connections between two different actions of $G$ on objects relating to $X$ . I will begin by stating their definitions and what I know about them. There are two natural ways to define a left action of $G$ on $X(K)$ . The first is to let $\sigma\in G$ act on a point $x\in X(K)$ by post-composition: to $\sigma$ , there is an associated morphism $\sigma^\flat\!:\Spec{K}\to\Spec{K}$ , and we set $\sigma x := x\circ\sigma^\flat$ . It is then not hard to check that this is a left group action. A different approach is to use the identification of $X(K)$ with pairs $(x,j)$ , where $x\in X$ and $j\!:\kappa(x)\hookrightarrow K$ is an embedding of the residue field at $x$ into $K$ . One can then define the action by $(x,j)\mapsto (x,j\circ\sigma)$ . Now, it is not too hard to check that these two approaches give the same action (for example by inspecting the proof of the identification used in the second approach). One can also define a right action of $G$ on the $k$ - scheme $X_K := X\times_k\Spec{K}$ (with the composition $X_K\to\Spec{K}\to\Spec{k}$ as the structure map) by assigning to $\sigma\in G$ the map $\id_X\times_k\sigma^\flat\!:X_K\to X_K$ which acts trivially on $X$ and applies $\sigma^\flat$ to $\Spec{K}$ . That this is a right action is not unexpected, since $\Spec$ is contravariant. Essentially what we now have is a map $\newcommand{\op}{\text{op}}\newcommand{\Aut}{\operatorname{Aut}}G^\op\to\Aut_k(X_K) = \{k\text{-isom. }X_K\to X_K\}$ . My problem is in trying to relate these two approaches. I'm aware that one can get a left action on $X_K$ by switching from $\sigma$ to $\sigma^{-1}$ , but this seems like the wrong direction to go. On the other hand, I have seen no other ways to get a left action out of the right action on $X_K$ . Basically, my question is: is it actually possible to relate the right action on $X_K$ to the left action on $X(K)$ , and if so, how? Furthermore, as this comes up in identifying $X(k)$ as the fixed points of the left action on $X(K)$ , is there an analogous such result for the action on $X_K$ ?","['algebraic-geometry', 'arithmetic-geometry']"
4609681,The taxonomy of real functions of one real variable,"In treatments like Spivak's Calculus , and in Pete Clark's notes here , the motivation is to describe a set of real functions of one real variable with ""sufficiently nice properties"" -- that is to say, a taxonomy of such functions. It seems to me that we can think of this taxonomy as analytic $\subset$ infintely differentiable $\subset$ n-times differentiable $\subset$ continuous $\subset$ general functions. I am trying (I think) to understand the jump from infinitely differentiable to analytic. I am at the end of Spivak's Chapter 24 where the statement is made that ""a convergent power series is always the Taylor series of the function which it defines"". That is (WLOG considering Maclaurin rather than Taylor series), if in some interval $J=(-R,R)$ we have that the function $f(x) = \sum_0^\infty a_n x^n$ converges, then it is in fact equal to its Taylor series $$a_n = \frac{f^{(n)}(0)}{n!}.$$ But as noted in Prof. Clark's notes, we can ask the more general question: Question 12.5. Let $f : I → R$ be an infinitely differentiable function and $T(x) = \sum_0^\infty \frac{f^{(n)}(0)}{n!} x^n$ be its Taylor series. We may then ask:
a) For which values of $x$ does $T(x)$ converge?
b) If for $x \in I$ , $T(x)$ converges, do we have $T(x) = f(x)$ ? I want to ask whether the following are the only possibilities other than $f$ being analytic (I think this means equal to it's Taylor series?) everywhere it's defined: The interval of convergence $J$ of the Taylor series of $f$ is a subset of $I$ (the interval on which $f$ is defined), but $f$ is given by its Taylor series on $J$ . I believe this is given e.g. by $f(x) = \textrm{ln}(1+x)$ . I have not yet studied later chapters in Spivak but I suspect this case arises when the complex version of the function encounters a singularity at that radius. The interval of convergence $J$ of the Taylor series of $f$ equals $I$ (the interval on which $f$ is defined), but $f$ is NOT given by its Taylor series on all of $J$ . I believe this is given e.g. by $f(x) = e^{-1/x^2} \ \textrm{for} \ x \neq 0, f(0) = 0$ . Further, with respect to question 2, I'm wondering if it's possible to comment on ""what goes wrong"". Prof. Clark provides sufficient conditions for analyticity in his Theorem 12.6, but I can't quite understand why something that has the limit of its Taylor remainder converge (so that the limit of Taylor polynomials converges) would equal anything other than the function from which the Taylor polynomials were constructed? I suspect this has something to do with uniform convergence but I'm not sure. Edit: With respect to the question in the last paragraph, I suppose a possibility is that the Taylor polynomials $P_{n,f}$ of $f$ converge to some function which is not $f$ , so that the remainder $R_{n,f}$ converges too (unlike in possibility 1 above, where I think the reason the sequence of Taylor polynomial does not converge is because the remainder blows up), but it does not converge to 0?",['calculus']
4609686,How to find the particular solution to this second degree differential equation $y'' - 6y' + 9y = 2xe^{2x}$?,"I have this solution: $$y'' - 6y' + 9y = 2xe^{2x}$$ The general solution to it is $$y(x) = C_1e^{3x} + C_2xe^{3x}$$ But I cannot figure out how to find the particular solution. This is what I did. I imposed this as particular solution: $$y_0(x) = e^{2x}.(ax+b)$$ and now I have to find $a$ and $b$ . So I take the first and second derivative of $y_0(x)$ and I replace it in original equation and now what I have is $$x(a-2)+b-2a=0$$ and even now I can not find the solution to $a$ and $b$ .
Could someone kindly help me find the solution to this problem?","['derivatives', 'ordinary-differential-equations']"
4609696,Relationship between the Radon Nikodym derivative with respect to two measures in both directions,"Background The Radon-Nikodym Theorem tells us that for two sigma finite measures $\mu$ and $\nu$ , that $\nu$ is absolutely continuous with respect to $\mu$ if and only if there exists a function $f$ such that it is the Radon-Nikodym derivative $\frac{d \nu}{d \mu}.$ In my course on Measure Theory (and the corresponding textbook “Measure Theory” by Donald Cohn), there is not much exploration into the Radon-Nikodym derivative itself and so I am left wondering more about when it exists and the implications of this. Question Assuming that the two measures $\mu$ and $\nu$ are both absolutely continuous between one another and are sigma finite on the measurable space $(\Omega, \mathscr{F})$ , then by the Radon-Nikodym Theorem, both $\frac{d \nu}{d \mu}$ and $\frac{d \mu}{d \nu}$ are guaranteed to exist. However, it isn't clear to me exactly what the relationship between the two derivatives will be (if there exits one). And if there is an easy way to calculate one from the other (which can sometimes be done with derivatives in other areas of mathematics). I would be grateful for any guidance here. I assume (if there exits a connection between the two) that this is just a standard result, however, I haven't been able to find anything on it online or in the textbooks that I have access to and so any help or references here would be appreciated.","['measure-theory', 'derivatives', 'probability-theory', 'probability', 'radon-nikodym']"
4609760,Prove / Disprove / Complete the proof That $f$ is Infinitely Differentiable,"THE NEW (MUCH SHORTER) POST My original post perhaps was too long for most people, and understandably not a lot of people tried to go over it. In this new post my question is much shorter please prove or disprove that following function is infinitely differentiable on $\mathbb{R}$ , or alternatively answer my original post (it is below here), which basically means helping me completing my proposed proof. Even if you don't want to answer my original post because it 's too long, it might be worth at least looking at some of it, because it could be useful. Let $E$ be some closed set of real numbers. Then, we define $f$ by 3 cases: $\quad$ (I) If $x \in E$ , $f(x) = 0$ . $\quad$ (II) If $x \in (a,b)$ , where $a,b \in E$ and $(a,b) \subseteq \mathbb{R} \setminus E$ , define $$F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right),$$ $\quad$ and then $$f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right),$$ $\quad$ where $X_{a,b}$ is some constant real based on $a, b$ , which you need to define appropriately (probably different for every $(a,b)$ ), to prove that $f$ is infinitely differentiable, or if you want to prove that $f$ is not infinitely differentiable, you need to show that this is true for every choice for $X_{a,b}$ . $\quad$ (III) If $E$ is bounded above, $M=\sup E$ , then $f(x)=e^{-\frac{1}{(x-M)^2}}$ for $x > M$ . Similarly, $f(x)=e^{-\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\quad N=\inf E$ . MY ORIGINAL POST: Baby Rudin Ex. 5.21 - Assessment of a Proposed Partial Solution and Help Completing It This exercise is driving me crazy.
I'm honestly ashamed to tell you how long I actually stack with this problem not continuing to study mathematics until I solve it. In many points a long the way I was very close to giving up and in many points I thought that I finally managed to solve the problem but then realized there is some mistake or a missing piece in the proof. The proposed solution here is the closest I managed to get at this point. At this point I decided it's time to ask for help on the last part missing from this proof and get some people to critic this partial proof in general. The Problem: Ex. 21, Chap. 5 Let $E$ be a closed subset of $\mathbb{R}$ . We saw in Exercise 22, Chap. 4, that there is a real continuous function $f$ on $\mathbb{R}$ whose zero set is $E$ . Is it possible, for each closed set $E$ , to find such an $f$ which is differentiable on $\mathbb{R}$ , or one which is $n$ times differentiable, or even one which has derivatives of all orders on $\mathbb{R}$ ? My Idea (Informally) I think such an infinitely differentable function does exist. I want to give you a visual explanation of how I thought of constructing this function before going to the formal solution. We obviously set $f$ to be 0 for all points of $E$ . For every open interval of points of $\mathbb{R} \setminus E$ , where the end points are points of $E$ , $f$ is a ""wave"" that smoothly approches 0 in the end points so that the derivative of any order on those end points is always 0. The shorter this open interval is, the smaller the maximum point of this ""wave"". If $E$ is bounded above, $f$ is just some increasing function after $\sup E$ , so that it smoothly approches 0 at $\sup E$ (the same goes if $E$ is bounded below).
I drew an illustration of this idea: The red parts are points of $f(E)$ , the blue parts are points of $f(\mathbb{R} \setminus E)$ . $P$ is a limit point of $E$ . My Partial Solution My proof uses two previous results from the book and two additional lemmas. I shall present them here (and prove the lemmas) before going to the main proof. Ex. 5.9. Let $f$ be a continuous real function on $\mathbb{R}$ , of which it is known that $f'(x)$ exists for all $x \neq 0$ and that $f'(x) \to 3$ as $x \to 0$ . Does it follow that $f'(0)$ exists? In the solution to this exercise we saw that $f'(0)$ does in fact exists and is 3. In general to any real continuous function $f$ on $\mathbb{R}$ for which the derivative is known to exist except for a specific point $a$ and $f'(x) \to D$ as $x \to a$ , we can use the same proof we use in this exercise to show that $f'(a)$ exists and $f'(a)=D$ . Theorem 4.15. If $f$ is a continuous mapping of a compact metric space $X$ into $\mathbb{R}^k$ , then $f(X)$ is closed and bounded. Thus, $f$ is bounded. Lemma 1. Let $f, g: \mathbb{R} \to \mathbb{R}$ differentiable functions. Then, $$\left(f(x)^{g(x)}\right)' = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right]$$ Proof. Let $h(x)=f(x)^{g(x)}$ , then $$\ln(h(x)) = \ln\left(f(x)^{g(x)}\right) = g(x)\ln(f(x)).$$ Differentiating both sides we get: $$\frac{h'(x)}{h(x)} = g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}.$$ Thus, $$h'(x) = f(x)^{g(x)}\left[g'(x)\ln(f(x)) + g(x)\frac{f'(x)}{f(x)}\right].$$ Lemma 2. Let $f,g : \mathbb{R} \to \mathbb{R}$ infinitely differentiable functions. Then, for every $n \in \mathbb{N}$ : $\quad$ (a) $f+g$ is infinitely differentiable and $$(f+g)^{(n)}(x)=f^{(n)}(x)+g^{(n)}(x)$$ . $\quad$ (b) $f\cdot g$ is infinitely differentiable and $$(f\cdot g)^{(n)}(x)=\sum_{i=1}^{2^n} f^{\left(\alpha_{n_i}\right)}(x)\cdot g^{\left(\beta_{n_i}\right)}(x),$$ $\quad$ where $\{\alpha_{n_i}\}$ , $\{\beta_{n_i}\}$ are sequences of non-negative integers. $\quad$ (c) $\frac{f}{g}$ is infinitely differentiable whenever $g(x) \neq 0$ and $$\left(\frac{f}{g}\right)^{(n)}(x)=\frac{F_n(x)}{g^{2^n}(x)},$$ $\quad$ where $F_n$ is some infinitely differentiable function. $\quad$ (d) $f\circ g$ is infinitely differentiable and $$(f\circ g)^{(n)}(x)=\sum_{j=1}^{z_n}\prod_{i=1}^{m_{n_j}}t_{n_{j_i}},$$ $\quad$ for some $z_n \in \mathbb{N}$ , $m_{n_1} and \dots, m_{n_{z_n}} \in \mathbb{N}$ and where each term $t_{n_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$ , $m$ a non-negative $\quad$ integer. Proof. We use induction for all 4 of the sections of this lemma. $\quad$ (a) For $n=1$ , $(f+g)'(x) = f'(x) + g'(x)$ . Suppose that $(f+g)^{(k)}(x)=f^{(k)}(x)+g^{(k)}(x)$ for some $k \in \mathbb{N}$ . Then, $$(f+g)^{(k+1)}(x)=\left(f^{(k)}(x)\right)' + \left(g^{(k)}(x)\right)' = f^{(k+1)}(x) + g^{(k+1)}(x).$$ $\quad$ (b) For $n=1$ , $(f \cdot g)'(x) = f'(x)g(x) + f(x)g'(x)$ . Suppose that for some $k \in \mathbb{N}$ , there are sequences $\{\alpha_{k_i}\}$ , $\{\beta_{k_i}\}$ of non-negative integers such that $(f\cdot g)^{(k)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x)$ . Then, $$(f\cdot g)^{(k+1)}(x)=\sum_{i=1}^{2^k} f^{\left(\alpha_{k_i}+1\right)}(x)\cdot g^{\left(\beta_{k_i}\right)}(x) + f^{\left(\alpha_{k_i}\right)}(x)\cdot g^{\left(\beta_{k_i}+1\right)}(x).$$ Now, let $$\alpha_{\left(k+1\right)_i} = \begin{cases}
\alpha_{k_i+1}, \quad 1 \leq i \leq 2^k\\
\alpha_{k_{\left(i-2^k\right)}}, \quad 2^k + 1 \leq i \leq 2^{k+1},
\end{cases}$$ $$\beta_{\left(k+1\right)_i} = \begin{cases}
\beta_{k_i}, \quad 1 \leq i \leq 2^k\\
\beta_{k_{\left(i-2^k\right)}+1}, \quad 2^k + 1 \leq i \leq 2^{k+1}.
\end{cases}$$ Thus, $$(f\cdot g)^{(k+1)}(x) = \sum_{i=1}^{2^{k+1}}f^{\left(\alpha_{(k+1)_i}\right)}(x)\cdot g^{\left(\beta_{(k+1)_i}\right)}(x).$$ $\quad$ (c) For $n=1$ , $\left(\frac{f}{g}\right)'(x) = \frac{f'(x)g(x)+f(x)g'(x)}{g^2(x)}$ . Suppose that for some $k \in \mathbb{N}$ , $\left(\frac{f}{g}\right)^{(k)}(x) = \frac{F_k(x)}{g^{2^k}(x)}$ , where $F_k$ is some infinitely differentiable function. Then, if we put $F_{k+1}(x) = F_k(x)g^{2^k}(x)-2^kg^{2^k-1}(x)g'(x)F_k(x)$ , we get the desired result that $$\left(\frac{f}{g}\right)^{(k+1)}(x) = \frac{F_{k+1}(x)}{g^{2^{k+1}}(x)}.$$ $\quad$ (d) For $n=1$ , $(f \circ g)'(x) = f'(g(x))g'(x)$ . Suppose that for some $k \in \mathbb{N}$ , $(f\circ g)^{(k)}(x)=\sum_{j=1}^{z_k}\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}$ for some $z_k \in \mathbb{N}$ , $m_{k_1} and \dots, m_{k_{z_k}} \in \mathbb{N}$ and where each term $t_{k_{j_i}}$ is either of the form $f^{(m)}(g(x))$ or $g^{(m)}(x)$ , $m$ a non-negative integer. Then, $(f\circ g)^{(k+1)}(x)=\sum_{j=1}^{z_k}\left(\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right)'$ . We then prove by induction $$\bullet \quad \left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left(\prod_{i=m+1}^{m_{k_j}}t_{k_{j_i}}\right)',$$ for every $m,j \in \mathbb{N}$ such that $j \leq z_k$ and $m < m_{k_j}$ . For $m=1$ , $\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = t_{k_{j_1}}'\prod_{i=2}^{m_{k_j}}t_{k_{j_i}} + t_{k_{j_1}}\left(\prod_{i=2}^{m_{k_j}}t_{k_{j_i}}\right)'$ . Suppose that $\bullet$ is true for some natural $m < m_{k_j}-1$ . Then, $$\begin{align*}
\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' 
& = \sum_{i=1}^{m}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m}t_{k_{j_i}}\left[t_{k_{j_{m+1}}}'\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right) + t_{k_{j_{m+1}}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'\right]\\
& = \sum_{i=1}^{m+1}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right) + \prod_{i=1}^{m+1}t_{k_{j_i}}\left(\prod_{i=m+2}^{m_{k_j}}t_{k_{j_i}}\right)'.
\end{align*}$$ Then, if we put $m=m_{k_j}-1$ we get that $$\left(\prod_{i=1}^{m_{k_j}}t_{k_{j_i}}\right)' = \sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right).$$ Since $t_{k_{j_i}}'$ is either of the form $f^{(u)}(g(x))g'(x)$ or $g^{(u)}(x)$ , $u \in \mathbb{N}$ , $$
(f \circ g)^{(k+1)}(x) = \sum_{j=1}^{z_k}\sum_{i=1}^{m_{k_j}}\left(\frac{t_{k_{j_i}}'}{t_{k_{j_i}}}\prod_{s=1}^{m_{k_j}}t_{k_{j_s}}\right)
$$ can clearly be arranged in our desired form. The main proof. Yes, such a function which is infinitely differentiable exists. We shall construct an example. Let $E \subseteq \mathbb{R}$ be closed. Defining $f$ For the trivial case that $E=\emptyset$ take $f(x)=1$ . Otherwise, we define $f$ by 3 cases: $\quad$ (I) If $x \in E$ , $f(x) = 0$ . $\quad$ (II) If $x \in (a,b)$ , where $a,b \in E$ and $(a,b) \subseteq \mathbb{R} \setminus E$ , define $$F_{a,b}(x) = \frac{\pi}{b-a}\left(x + \frac{3b-5a}{2}\right),$$ $\quad$ and then $$f(x) = X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right),$$ $\quad$ where $X_{a,b}$ is some constant real based on $a, b$ , which for now is irrelevant. We shall define $X_{a,b}$ later in the proof when its value will be $\quad$ relevant. $\quad$ (III) If $E$ is bounded above, $M=\sup E$ , then $f(x)=e^{-\frac{1}{(x-M)^2}}$ for $x > M$ . Similarly, $f(x)=e^{-\frac{1}{(x-N)^2}}$ if $E$ is bounded below and $\quad N=\inf E$ . Clearly $f$ is defined for every $x \in E$ . $f$ is also defined for every $x \in \mathbb{R} \setminus E$ because such $x$ have some neighborhood in $\mathbb{R} \setminus E$ , otherwise $x$ would be a limit point of $E$ and therefore contained in $E$ . $E$ is the zero-set of $f$ To show that $E=Z(f)$ , we need to show that $x \not\in E$ implies $f(x) \neq 0$ . For $x$ in case (II) $$* \quad \frac{3}{2}\pi < F_{a,b}(x) < \frac{5}{2}\pi, $$ therefore $f(x)>0$ . For $x$ in case (III) clearly $f(x)>0$ as well. $f$ is infinitely differentiable we treat points of $E$ and points of $\mathbb{R} \setminus E$ separately. Points of $\mathbb{R} \setminus E$ : $\quad$ For $x$ in case (II), $f(x)=X_{a,b}\cos^{F_{a,b}(x)}\left(F_{a,b}(x)\right)$ . To derive $f'(x)$ , we first note that $$F_{a,b}'(x)=\frac{\pi}{b-a}$$ and using lemma 1 $$\left(\cos^x(x)\right)' = \cos^x(x)\left(\ln\left(\cos x\right)-x\tan x\right).$$ We now prove by induction that for every $n \in \mathbb{N}$ , $f^{(n)}(x)$ exists and $f^{(n)}(x) = f(x)\cdot f_n(x)$ , where $f_n$ is some infinitely differentiable function. For $n=1$ , $$f'(x) = f(x)\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right]\frac{\pi}{b-a},$$ and we denote $f_1(x) = \frac{\pi}{b-a}\left[\ln\left(\cos F_{a,b}(x)\right) - F_{a,b}(x)\tan F_{a,b}(x)\right]$ . From $*$ it follows that $\cos F_{a,b}(x) > 0$ , hence $\ln\cos F_{a,b}(x)$ is well defined and is infinitely differentiable by lemma 2(d). Then, additionally using lemmas 2(a),(b),(d) we see that $f_1$ is infinitely differentiable. Suppose that for some $k \in \mathbb{N}$ $f^{(k)}(x)=f(x)f_k(x)$ , where $f_k$ is some infinitely differentiable function. Then, $$\begin{align*}
f^{(k+1)}(x)
&= f'(x)f_k(x) + f(x)f_k'(x)\\
&= f(x)(f_1(x)f_k(x) + f_k'(x)).
\end{align*}$$ Denoting $f_{k+1}(x) = f_1(x)f_k(x)+f_k'(x)$ , it's clear from lemmas 2(b),(c) that $f_{k+1}$ is infinitely differentiable. $\quad$ For $x$ in case (III), suppose without loss of generality that $E$ is bounded above and $x > M$ . Then, since $e^x$ and $-\frac{1}{(x-M)^2}$ are infinitely differentiable (for $x \neq M$ ) we can use lemma 2(d) to get that $f(x)$ is infinitely differentiable. Points of $E$ : We shall describe the area ""to the left"" and ""to the right"" of $x$ . Given $d > 0$ , we denote $l_d=(x-d,x)$ and $r_d=(x,x+d)$ . Then, we claim that there is some $\varepsilon_0>0$ , such that at least one of the following must be true: $\quad$ (1) $\quad$ $l_{\varepsilon_0} \subseteq E$ . $\quad$ (2) $\quad$ $l_{\varepsilon_0} \subseteq \mathbb{R} \setminus E$ . $\quad$ (3) $\quad$ For all $0<\varepsilon\leq \varepsilon_0$ , there's some $t \in l_\varepsilon$ such that $t \in E$ . And the same goes for ""the right side"" of $x$ . To show that one of these options must be true, suppose by contradiction that they're all false. Then, since (3) is false, (2) is true and that's a contradiction. $\quad$ We shall prove that for every non-negative integer $n$ , $f^{(n)}(t) \to 0$ as $t \to x$ . Then we use it to prove by induction that $f^{(n)}(x)=0$ and therefore $f^{(n)}(x)$ exists. Here is this induction proof: For $n=0$ we already know that $f(x)=0$ . Suppose that for some non-negative integer $k$ , $f^{(k)}(x)=0$ . Then, since $f^{(k)}(t) \to 0$ as $t \to x$ it follows that $f^{(k)}$ is continuous at $x$ and since we know that $f^{(k+1)}$ exists for points of $\mathbb{R} \setminus E$ , $f^{(k)}$ is continuous. Then, since $f^{(k+1)}(t) \to 0$ as $t \to x$ , using Ex.9 it follows that $f^{(k+1)}(x)=0$ . $\quad$ If (1) is true, then $f$ is $0$ constant in $l_{\varepsilon_0}$ , therefore for every $n \in \mathbb{N}$ $f^{(n)}$ is 0 constant as well. Thus, for every non-negative integer $n$ , $f^{(n)}(t) \to 0$ as $t \to x$ for $x \in l_{\varepsilon_0}$ . This argument of course can be applied to $r_{\varepsilon_0}$ as well. $\quad$ If (2) is true, then either $x=N=\inf E$ or $x=b$ for some $(a,b)$ as in case (II). If $x=b$ , denote $w(z)=X_{a,b}\cos^{F_{a,b}(z)}\left(F_{a,b}(z)\right)$ , for $z \in [a,b]$ (note that $w=f$ on $[a,b]$ ). Then, since $w$ is continuous on $[a,b]$ (composition of continuous functions is continuous, $\lim_{t \to x}f(t) = 0$ , for $t \in l_{\varepsilon_0}$ . Since $w^{(n)}(t) = w(t)\cdot f_n(t)$ and $w^{(n+1)}(t)$ exists for all non-negative integer $n$ and $t \in [a,b]$ , it follows that $w^{(n)}$ is continuous on $[a,b]$ , therefore $f^{(n)}(t) \to w^{(n)}(x) = 0$ as $t \to x$ for $t \in l_{\varepsilon_0}$ . If $x=N$ , then $f(t)=e^{-\frac{1}{(t-N)^2}}$ , for $t \in l_{\varepsilon_0}$ . Then, we show that $f^{(n)}(t) = f(t)\cdot p_n(t)$ , where $p_n(t)$ is some polynomial of $\frac{1}{t-N}$ . We use induction: For $n=1$ , $f'(t) = f(t)\cdot 2\left(\frac{1}{t-N}\right)^3$ . If for some $k \in \mathbb{N}$ , $f^{(n)} = f(t)p_k(t)$ , where $p_k(t) = \sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j$ , $i \in \mathbb{N}$ and $c_0, \dots, c_i \in \mathbb{R}$ . Then, $$\begin{align*}
f^{(k+1)}(t) 
&= f(t)2\left(\frac{1}{t-N}\right)^3\sum_{j=0}^{i}c_j\left(\frac{1}{t-N}\right)^j + f(t)\sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\\
&= f(t)\underbrace{\left[\sum_{j=0}^{i}2c_j\left(\frac{1}{t-N}\right)^{j+3} + \sum_{j=0}^{i}jc_j\left(\frac{1}{t-N}\right)^{j-1}\right]}_{\text{clearly a polynomial of } \frac{1}{t-N}}
\end{align*}$$ Now, if we prove that $\lim_{t \to N}\frac{f(t)}{|t-N|^m} = 0$ , for every $m \in \mathbb{N}$ , then clearly $f^{(n)}(t) \to 0$ as $t \to x$ for $t \in l_{\varepsilon_0}$ . $$
\lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{t \to N}\frac{e^{-\frac{1}{|t-N|^2}}}{|t-N|^m} = \lim_{h \to 0}\frac{e^{-\frac{1}{h^2}}}{h^m} = \lim_{n \to \infty}\frac{n^{\frac{m}{2}}}{e^n}.
$$ If $m=2z$ , for some $z \in \mathbb{N}$ , we apply l'Hôpital's rule $z$ times and get that $$\lim_{t \to N}\frac{f(t)}{|t-N|^m} = \lim_{n \to \infty}\frac{z!}{e^n} = 0.$$ If $m=2z-1$ , for some $z \in \mathbb{N}$ , we apply l'Hôpital's rule $z$ times and get that $$\lim_{t \to N}\frac{f(t)}{|t-N|^m} = 
\lim_{n \to \infty}\frac{\left(\frac{m}{2}\right)\left(\frac{m}{2}-1\right)\cdots\frac{1}{2}}{\sqrt{n}e^n} = 0.$$ In a very similar way we can prove this for $r_{\varepsilon_0}$ when either $x=a$ or $x=M=\sup E$ . The missing piece from the proof The last thing left to prove which I didn't mange to prove is that $f^{(n)}(t) \to 0$ as $t \to x$ for points $x \in E$ for which (3) is true. I should show how I tried to do it and what I did manage to show, that perhaps could be useful: $\quad$ Suppose (3) is true (but (1) is false). We should now define $X_{a,b}$ . Let $n_{a,b} \in \mathbb{N}$ be the smallest integer such that $\frac{1}{n_{a,b}} < b-a$ . Since $f_n$ is differentiable on $[a,b]$ , $f_n$ is continuous on $[a,b]$ , therefore by Theorem 4.15, there's some $M_n \in \mathbb{R}$ such that $\left|f_n(t)\right| \leq M_n$ for all $t \in [a,b]$ (notice that $f_n$ is independent of the value of $X_{a,b}$ ). Similarly, there is some $M_* \in \mathbb{R}$ such that $\left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right| \leq M_*$ for all $t \in [a,b]$ . Put $M_{a,b} = \max \{M_n|n \leq n_{a,b}\}$ . Finally, we define $$X_{a,b} = \frac{1}{n_{a,b}M_*M_{a,b}}.$$ We shall now use this definition of $X_{a,b}$ to demonstrate that for every $d > 0$ , there is some $\delta > 0$ such that if $t \in l_\delta$ and $t \not\in E$ , then $|f^{(n)}(t)|< d$ . Given $d > 0$ , there is some $x_d \in l_d$ , such that $x_d \in E$ . Put $\delta = x - x_d$ . If $t \in l_\delta$ and $t \not\in E$ , then $t \in (a,b)$ as in (II), where $b-a \leq \delta$ . Thus $$
|f^{(n)}(t)|=
X_{a,b}|f_n(t)|\left|\cos^{F_{a,b}(t)}\left(F_{a,b}(t)\right)\right|
\leq \frac{1}{n_{a,b}M_*M_{a,b}}M_*M_{a,b}
= \frac{1}{n_{a,b}}
< b-a \leq \delta
< d
.$$ For points $t$ for which (1) is true we know that $f^{(n)}(t) = 0$ , so there's no problem there. But the problem is points for which only (3) is true. We don't know that they are 0 (we only think they suppose to be 0), but how can we prove this? It seems a bit circular. What I'm Asking For I have a few questions / things I want to get in an answer to this question: General assessment / critique of the incomplete proof (ignore the missing piece): I would like to get your opinion about every aspect of this proof - soundness, rigor, style, clarity and any other possible aspect you can think of.
And most importantly is there some big unfixable mistake in this proof? Do you have a proposal to solve the missing piece? If needed feel free to change the definition of the coefficient $X_{a,b}$ as you see fit. If you think there's no way to solve this missing piece and this construction simply doesn't work, please don't suggest me a completely different solution to this exercise, I can look for solutions online, that's not a problem. Instead, please prove why this construction can't work for any choice of $X_{a,b}$ . How long does such a question should take to solve for a first time analysis student? Does it make sense that a single exercise would take so much time and effort, even when we talk about Rudin?","['proof-writing', 'smooth-functions', 'real-analysis', 'solution-verification', 'derivatives']"
4609792,Contest Math Question on Logarithms,"I am trying to solve a question from the AoPS Vol. 2 book.
The question is as follows: Suppose the $p$ and $q$ are positive numbers for which: $$\log_9 p = \log_{12}q = \log_{16}(p+q)$$ What is the value of $q/p$ ? (AHSME 1988) I only have knowledge of a few of the basic log identities/properties to work with, but they are not getting me anywhere, I find myself only coming up with some obvious equalities that do not lead me closer. One thing I tried was to rewrite the equalities as: $$\frac{\log p}{\log 9} = \frac{\log{q}}{\log 12} = \frac{\log(p+q)}{\log 16}$$ where $\log = \log_{10}$ . Then, rearranging the first equality shows: $$\log_q p = \log_{12} 9$$ I thought at first that I could just take $q=12$ and $p=9$ directly via substitution, but substituting in these values does not satisfy the equality given in the problem statement since $\log_{16} 21 \neq 1$ . (What's actually going on here by the way? Is it because you can have multiple values of $p$ and $q$ besides $12$ and $9$ that satisfy the equality?) I feel that I am missing some key step in my approach to be able to solve this that is beyond just manipulating the formulas/identities. I am wondering if someone can point me in the right direction about how to approach this, and perhaps explain how one would reach such an approach. Thanks.","['contest-math', 'algebra-precalculus', 'quadratics', 'logarithms']"
4609805,Behavior of the solutions to a system of linear differential equations,"Let $x(t)=(x_1(t),\cdots,x_n(t)):\mathbb{R}\rightarrow\mathbb{R}^n$ be differentiable and satisfy: $$x'(t)=Ax(t)$$ where $A=(a_{ij})_{n\times n}$ is a real matrix. I have known that if the real part of all eigenvalues of $A$ are strictly negative, then $\lim\limits_{t\rightarrow+\infty}x(t)=0$ . However, if we assume $\lim\limits_{t\rightarrow+\infty}x(t)=0$ and $x_1(t),\cdots,x_n(t)$ are linearly independent, what can we say about the matrix $A$ ? I guess the real part of all eigenvalues of $A$ should be strictly negative, but I cannot deal with the case that the eigenvalue $\lambda$ is purely imaginary number. In this case, $x_i(t)$ may be like this: $$c_1\sin \mu_1t+c_2\sin\mu_2t+\cdots+c_2\sin\mu_kt$$ where $\mu_k>0$ I guess this is impossible since $\lim\limits_{t\rightarrow+\infty}x(t)=0$ , but I don't know how to prove it. By the way, the original problem is that if all $a_{ij}\geqslant0$ and $\lim\limits_{t\rightarrow+\infty}x(t)=0$ , prove that $x_1(t),\cdots ,x_n(t)$ cannot be linearly independent. The solution is that use Perron's theorem to find a non-negative eigenvalue $\lambda$ and its eigenvector $(\alpha_1,\cdots ,\alpha_n)$ . Let $y(t)=\alpha_1x_1(t)+\cdots+\alpha_nx_n(t)$ and we will find that $y'=\lambda y$ and $\lim\limits_{t\rightarrow +\infty}y(t)=0$ , so $y(t)\equiv0$ , which means $x_1(t),\cdots,x_n(t)$ are linearly dependent.","['linear-algebra', 'ordinary-differential-equations', 'real-analysis']"
4609839,Canonicity of the push-pull map for sheaves of sets,"This question is Vakil 2.7.F (in the December 2022 version of the notes). Suppose we have the following commutative diagram of topological spaces: $$
  \require{AMScd}
  \begin{CD}
    W              @>{\beta'}>>  X \\
    @V{\alpha'}VV                @VV{\alpha}V \\
    Y              @>>{\beta}>   Z
  \end{CD}
$$ Suppose also we have a sheaf $\mathscr{F}$ on $X$ . Then, we define two maps from $\operatorname{hom}(\mathscr{F}, \mathscr{F})$ to $\operatorname{hom}(\beta^{-1}\alpha_*\mathscr{F}, \alpha'_*\beta'^{-1}\mathscr{F})$ . Given $f: \mathscr{F} \to \mathscr{F}$ , we get $\beta'^{-1}f: \beta'^{-1}\mathscr{F} \to \beta'^{-1}\mathscr{F}$ . We may then take the transpose of this map, apply $\alpha_*$ , apply commutativity, and finally take the transpose again to get the desired map (see Vakil for more details). Dually, you may also apply $\alpha_*$ first to get a second map. Call these maps $h_1, h_2$ . Vakil 2.7.F asks us to show that $h_1(\operatorname{id})=h_2(\operatorname{id})$ . You could obviously do this by passing to components and keeping track of all the functor applications and transposes, but this is super messy. Is there a nice, abstract nonsense proof of this fact?","['algebraic-geometry', 'sheaf-theory']"
4609840,Simplfying a trigonometric expression,"I would like to simplify: $$\frac{\cos^2(80)+5\sin^2(80)-3}{\cos(50)}$$ By using the fact that $\sin^2(\theta) + \cos^2(\theta) = 1$ , $$\frac{\cos^2(80)+5\sin^2(80)-3}{\cos(50)} = \frac{\cos^2(80)+5(1-\cos^2(80))-3}{\cos(50)} = -2\biggr(\frac{2\cos^2(80)-1}{\cos(50)}\biggr)$$ Since $2\cos^2(80)-1 = \cos(160)$ , $$-2\biggr(\frac{2\cos^2(80)-1}{\cos(50)}\biggr) = -2\frac{\cos(160)}{\cos(50)}$$ But I am not sure how we could simplify this further.",['trigonometry']
4609845,Closed-form of $\int_0^\infty \frac{t^s}{(e^t-1)^z}dt$,"I am looking for a closed form for the integral $$\int_0^\infty \frac{t^s}{(e^t-1)^z}dt$$ valid for $s,z$ being both complex numbers, hopefully using complex analysis. I have already evaluated this integral when $s$ is complex and $z$ is a positive integer. In that case, the result is $$\frac{\Gamma(s+1)}{\Gamma(z)}\sum_{k=1}^z s(z,k)\zeta(s-k+2),$$ where the coefficients $s(z,k)$ are the Stirling numbers of the first kind. Edit: as I said in the comments, I also found the closed form using
the generalized hypergeometric function $$\frac{\Gamma(s+1)}{z^{s+1}}\,_{s+2}F_{s+1}(-z,z,z,\dotsc;z+1,z+1,\dotsc;-1).$$ Unfortunately, this is an extension for $z$ but not for $s$ .","['integration', 'complex-analysis', 'analytic-number-theory']"
4609855,How to distinguish between 2 free products on the same number of generators,"I am currently learning algebraic topology (using Munkres/Hatcher) So far I've seen that the fundamental group of the ""figure 8 space"" is $\langle a, b \rangle$ or simply the free product on 2 generators. Then, we can also calculate the fundamental group of the Klein bottle which was $\langle a, b : aba = b \rangle$ (an application of Van-Kampen's theorem). I was wondering if this is enough information to distinguish the two spaces. Can I claim that these 2 free products are non-isomorphic? (the presentation with no relations seems to be different to one with relations). I know generators must be mapped to generators by an isomorphism but there may be different generating letters if I'm not mistaken (rather than just $a, b$ , they could be a combination of them). Do we have enough information from the fundamental groups to distinguish these spaces?","['group-theory', 'abstract-algebra', 'algebraic-topology']"
4609925,How can I solve $\lim_{x \to +\infty} \ln\left(\sinh x \right) \cdot \arctan \left( \frac{1}{x}\right)$?,I have to calculate limit: $$\lim_{x \to +\infty}\left(\sinh x\right)^{\arctan\frac{1}{x}}$$ I think I can transform it to: $$e^{\lim_{x \to +\infty} \ln\left(\sinh x \right) \cdot \arctan \left( \frac{1}{x}\right)}$$ But what to do now? I'm hopeless by limit: $$\lim_{x \to +\infty} \ln\left(\sinh x \right) \cdot \arctan \left( \frac{1}{x}\right)$$ Thank you for any ideas how to solve this limit. By the way using L'Hopital's rule is restricted for me in this example so I would like to find solution without need of it. I've already tried rewriting $\sinh x$ to exponential form but I think I can't help me anyhow. Even $\sinh x$ to $-i\sin ix$ identity didn't help me. Any other useful identity I haven't found too. Thanks to comments I can do another step: $$\lim_{x \to +\infty} \ln\left(\sinh x \right) \cdot \arctan \left( \frac{1}{x}\right) = \lim_{x \to +\infty} \ln\left(\sinh x \right) \cdot \frac{1}{x} = \lim_{x \to +\infty} \ln\left(\sinh^{\frac{1}{x}} x \right)$$ . Now a problem is to fit $\sinh^{\frac{1}{x}}x$ to $\left(1+\frac{1}{x}\right)^x$ . Is it possible when $\frac{1}{x}$ approaches to 0 and $sinh x$ approaches to infinity instead of one? Context of this question is that I am now preparing for exam and i found this example in  test from previous year. I am studing math first year so I have only few tools awaiable.,"['limits', 'analysis']"
4609931,Probability question from 2000 STEP III Exam (Question 13),"I got on the first part of the question fine but I'm really confused about the expression "" the last of the dice to show a six does so on the $r$ th roll"" in the second part of the question. I interpret this line as in the following situations but none make sense. If this is to ask the result of rolling the very last dice on the $r$ th roll, then wouldn't it be independent from $n$ and $r$ , and result simply be $p$ ? If this is to ask the probability of at least one six shows up on the $r$ th roll which we consider as the last roll, then wouldn't the probability simply be $1-q^n$ , which is independent from $r$ If this is to ask the probability that result of six would never occur on the rolls after the $r$ th roll, then wouldn't it simply be zero since the number of rolls would be infinity and no occurrence of a six would be impossible? (i) A set of $n$ dice is rolled repeatedly. For each die the probability
of showing a six is $p$ . Show that the probability that the first of
the dice to show a six does so on the $r$ th roll is $$ q^{nr}\left (
q^{-n} - 1\right ) $$ where $q = 1 − p$ . Determine, and simplify, an expression for the probability generating function for this distribution, in terms of $q$ and $n$ . The first of
the dice to show a six does so on the R th roll. Find the expected
value of $R$ and show that, in the case $n = 2$ , $p = 1/6$ , this value is
36/11. (ii) Show that the probability that the last of the dice to show a
six does so on the rth roll is $$ \left ( 1-q^{r} \right )^{n}-\left (
1-q^{r-1} \right )^{n}. $$ Find, for the case $n = 2$ , the probability
generating function. The last of the dice to show a six does so on the $S$ th roll. Find the expected value of $S$ and evaluate this when $p =
1/6$ .","['statistics', 'combinatorics', 'probability', 'dice']"
4609938,"If $f$ is increasing, then $g \ge 0$ and $\int_a^b g \mathrm d \lambda \le f(b)-f(a)$","Let $f:[a, b] \rightarrow \mathbb{R}$ and define $g:[a, b] \to \mathbb R$ by $$
g(x) :=
\begin{cases}
f^{\prime}(x) & \text{if } f \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
$$ Then $g$ is Borel measurable . Let $\lambda$ be the Lebesgue measure on $[a, b]$ . I would like to prove below result, i.e., Theorem If $f$ is increasing, then $g \ge 0$ and $\int_a^b g \mathrm d \lambda \le f(b)-f(a)$ . Could you confirm if my below attempt is fine? Proof It's clear that $g \ge 0$ . Let $$
g_n(x) :=
\begin{cases}
\frac{f\left(x+\frac{1}{n}\right)-f(x)}{\frac{1}{n}} & \text{if } f \text { differentiable at } x \in [a, b-\frac{1}{n}], \\
0 & \text {otherwise}.
\end{cases}
$$ Then $g_n \to g$ everywhere . Because $f$ is differentiable $\lambda$ -a.e. , we get $$
g_n =\frac{f\left(x+\frac{1}{n}\right)-f(x)}{\frac{1}{n}} \quad \text{for } \lambda \text{-a.e. } x \in \left [a, b-\frac{1}{n} \right].
$$ By Fatou's lemma, $$
\begin{align}
\int_a^b g \mathrm d \lambda &\le \liminf_n \int_a^b g_n \mathrm d \lambda  \\
&= \liminf_n \int_a^{b-1/n} g_n \mathrm d \lambda  \\
&= \liminf_n n \left [ \int_{a+1/n}^b f \mathrm d \lambda - \int_{a}^{b-1/n} f \mathrm d \lambda\right ]  \\
&= \liminf_n n \left [ \int_{b-1/n}^b f \mathrm d \lambda - \int_{a}^{a+1/n} f \mathrm d \lambda \right ] \\
&\le \liminf_n n \left [ \frac{f(b)}{n}- \frac{f(a)}{n} \right ] \text{ because } f \text{ is increasing} \\
&= f(b)-f(a).
\end{align}
$$ This completes the proof.","['measure-theory', 'lebesgue-measure', 'monotone-functions', 'real-analysis', 'derivatives']"
4609944,Spaces for which all transpositions are homeomorphisms,"The discrete, indiscrete, cofinite, cocountable, co-(any downwards closed collection of cardinals) topological spaces are part of the unique class of spaces such that all permutations of the space (bijections from the space to itself) are autohomeomorphisms. This is because their topologies may be defined purely in terms of cardinality, so everything is bijection-invariant. If we relax the requirement to just saying that all transpositions (i.e. permutations that just swap two elements) are homeomorphisms, do we get a broader class of spaces, or is this property equivalent to bijection-invariance?","['elementary-set-theory', 'general-topology']"
4609955,The $\lambda$-integrability of the derivative of a function of bounded variation,"Let $f:[a, b] \to \mathbb{R}$ and $\operatorname{Var}_{[a, b]} f$ its total variation . Let $\lambda$ be the Lebesgue measure on $[a, b]$ . I would like to prove below properties for functions of bounded variation, i.e., Theorem If $f$ is of bounded variation, then $f$ is differentiable $\lambda$ -a.e., The function $$
g(x) :=
\begin{cases}
f^{\prime}(x) & \text{if } f \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
$$ is $\lambda$ -integrable such that $\int_a^b |g| \mathrm d \lambda \leq \operatorname{Var}_{[a, b]} f$ . Could you confirm if my below attempt is fine? Proof By this lemma, there exist increasing functions $f_1, f_2:[a, b] \rightarrow \mathbb{R}$ such that $f=f_1-$ $f_2$ and $$
\operatorname{Var}_{[a, b]} f = f_1(b)-f_1(a)+f_2(b)-f_2(a).
$$ By this lemma, $f_1, f_2$ are differentiable $\lambda$ -a.e. Claim (1.) then follows. Let $$
g_1(x) :=
\begin{cases}
f_1^{\prime}(x) & \text{if } f_1 \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
$$ and $$
g_2(x) :=
\begin{cases}
f_2^{\prime}(x) & \text{if } f_2 \text { differentiable at } x \in (a, b),\\
0 & \text {otherwise}.
\end{cases}
$$ I proved that $g, g_1, g_2$ are Borel measurable. So $g, g_1, g_2$ are $\lambda$ -measurable . I proved that $g_1, g_2 \ge 0$ and $$
\int_a^b g_1 \mathrm d \lambda \le f_1(b)-f_1(a)
\quad \text{and} \quad
\int_a^b g_2 \mathrm d \lambda \le f_2(b)-f_2(a).
$$ Notice that $g=g_1-g_2$ $\lambda$ -a.e., so $$
\int_a^b |g| \mathrm d \lambda \le \int_a^b |g_1| \mathrm d \lambda + \int_a^b |g_2| \mathrm d \lambda = (f_1(b)-f_1(a))+(f_2(b)-f_2(a)) = \operatorname{Var}_{[a, b]} f < \infty.
$$ This completes the proof.","['bounded-variation', 'lebesgue-measure', 'derivatives', 'real-analysis']"
4609959,Kolmogorov complexity of primes vs. composites in the limit?,"If we take two natural numbers of roughly equivalent magnitude where one is prime and the other is composite, will the Kolmogorov complexity of one or the other tend to be higher as we approach infinity? I originally suspected primes would be incrementally more complex (I believe this is true for small numbers), but I have no idea about what to expect with arbitrarily large numbers. My one observation is that if you take $p\in\mathbb P$ large enough, then its K-complexity immediately has an upper bound of $K(\pi(p))+C$ , where $C$ is some constant necessary to encode the enumeration of primes, so we do know that every sufficiently large prime is at least somewhat compressible. Since many nearby composites are bound to be algorithmically random, perhaps there's a case to be made that primes are actually simpler in the limit. On the other hand, if you're considering the mean complexity over some range of numbers, it seems likely there are similar counterarguments about exploiting the factorability of composites.","['number-theory', 'elementary-number-theory', 'kolmogorov-complexity', 'prime-numbers', 'computer-science']"
4609976,Universal/general algebra: altering group operation definition,"Exercise 2.2.2 of An Invitation to Universal Algebra says: If $G$ is a group, let us define an operation $\delta_G$ on $|G|$ by $\delta_G(x, y) = x · y^{−1}$ . Does the pair $G' = (|G|, \delta_G)$ determine the group $(|G|, ·, {}^{−1}, e)$ ? (I.e., if $G_1$ and $G_2$ yield the same pair, $G'_1 = G'_2$ , must $G_1 = G_2$ ? Some students have asked whether by “=” I here mean “ $\cong$ ”.
No, I mean “=”.) So far I have been able to find an algorithm to recover $G$ from $G'$ (if one exists, which  I doubt). I did try the other way around for some simple examples. For $\mathbb{Z}_2$ , the group is unaffected by the $\delta$ operation since both members are already their own inverse. After applying the $\delta$ operation to $\mathbb{Z}_3$ , there is no right identity, so it isn’t even a group. Thank you for any advice.","['universal-algebra', 'group-theory', 'abstract-algebra']"
4609992,"Finding constants a and b, given a function and its inverse","Given a function and its inverse, where a and b are constants, find the constants a and b. $$h(x) = x + a $$ $$h^{-1}(x) = b(2x + 3)$$ I tried simultaneous equation(not quite sure is it done like this): I created an inverse from the first function, i.e h(x) = x + a $h(x) = x + a$ $h^{-1}(x) = x - a$ For the second inverse function, I rearrange it back to a function $h^{-1}(x) = b(2x + 3)$ $h(x) = \frac{x - 3b}{2b}$ Then I equate the inverse with the inverse function I created and the original function with the function $x - a = b(2x + 3)$ $x + a = \frac{x - 3b}{2b} $ From h(x), I rearrange to get b and substitued b into the inverse equation, which got me a = 0 or a = -1.5 $x + a = \frac{x - 3b}{2b} $ $b = \frac{x}{2x + 2a + 3}$ I'm stuck till this step. Can anyone please point out where my mistake is? Thank you.","['elementary-functions', 'functions', 'inverse-function']"
4610043,Limit of $u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy.$,"Let $f\in C_c(\mathbb{R}^n)$ , $n\ge 3$ , be a compact support function. We consider $u(x):=\int_{\mathbb{R}^n}\frac{f(y)}{|x-y|^{n-2}}\; dy.$ How can I prove that $u(x)\to 0,\quad\text{for}\; |x|\to\infty$ ?","['multivariable-calculus', 'real-analysis']"
4610055,Category of abelian groups is not equivalent to its opposite,"I want to show that $\operatorname{Ab}$ is not equivalent to $\operatorname{Ab}^{op}$ . Is it sufficient to say that $\operatorname{Hom}(\mathbb{Z}, \mathbb{Z}_2)$ is not isomorphic to $\operatorname{Hom}(\mathbb{Z}_2, \mathbb{Z})$ ? If yes then I don't understand why.","['abelian-groups', 'abstract-algebra', 'category-theory']"
4610091,"Cardinality of ""almost coinciding families""","Let's call $\textit{almost coinciding family}$ a family $\mathcal{D} \subseteq [\omega]^\omega$ such that for every two $A, B \in \mathcal{D}$ we have $\vert A  \Delta B \vert < \omega $ (meaning that the symmetric difference is finite. Can we, similarly to the case of $\textit{almost disjoint families}$ , find such a family of the cardinality of the continuum?","['elementary-set-theory', 'combinatorics']"
4610118,Suppose $|X_n|\leqslant Y$ a.s. for all $n\in\Bbb N$. Show that $\sup|X_n|\leqslant Y$ a.s. too,"Since $|X_n| \leq Y$ a.s. for all $n$ , we have that $\mathcal{P}\left(\bigcap_{n=1}^{\infty}\{|X_n|\leqslant Y\}\right) =1$ So now the solution asses that: $$\textstyle\mathcal{P}\left(\bigcap^\infty_{n=1}\{|X_n|\leqslant Y\}\right)=1=\mathcal{P}\big(\sup\{|X_n|\leqslant Y\}\big)$$ and clearly, the $\,\sup|X_n|\leqslant Y$ almost surely. What is not clear to me and so my doubt is: Why can I rewrite the intersection as the supremum?","['measure-theory', 'supremum-and-infimum']"
4610159,Definition of $\chi^2$-divergence between probability distributions,"$
% Some definitions
\def\D#1#2{\operatorname{D_{\chi^2}}(#1 \| #2)}
\def\Df#1#2{\operatorname{D}_{f}(#1 \| #2)}
\def\E#1#2{\operatorname*{\mathbb{E}_{#1}}\left[#2\right]}
\def\dee{\mathop{\mathrm{d}\!}}
\def\var#1{\operatorname{var}(#1)}
$ I'm interested in the $\chi^2$ -divergnce, as a way of comparing probability distributions. There are two ways I've seen it defined. Are they equivalent? Let $P,Q$ be two probability measures on a set $\mathscr X$ with some sigma algebra, and $p,q$ be their densities with respect to some dominating measure denoted $\dee x$ . The first definition I've seen is $$\D{P}{Q} \equiv \int_{\mathscr X}\frac{(p(x)-q(x))^2}{p(x)}\dee{x}$$ This definition makes the relationship with the Chi-squared test clear. The second definition I've seen is $$\D{P}{Q} \equiv \E{Q}{{\left(\frac{\dee{P}}{\dee{Q}}-1\right)}^2} = \int_{\mathscr X}{\left(\frac{\dee{P}}{\dee{Q}}-1\right)}^2\dee{Q}$$ where $\frac{\dee{P}}{\dee{Q}}$ is the RN-derivative of $P$ with respect to $Q$ .  This definition is used in the context of defining $\chi^2$ divergence as an $f$ -divergence ( $\Df{P}{Q}\equiv\E{Q}{f(\frac{\dee{P}}{\dee{Q}})}$ with $f(t)=(t -1)^2$ ). My question is: are these definitions completely identical, or does the fact that the second one makes no mention of a dominating measure mean that it is more general?","['chi-squared', 'measure-theory', 'probability-distributions', 'probability-theory']"
4610160,Failure of Banach Alaoglu in $L^1$,"We know that dual of $L^{\infty}(\mathbb{R})$ is not $L^{1}(\mathbb{R})$ and hence Banach Alaoglu theorem is not applicable for the $(L^1,L^{\infty})$ pairing. In other words if $\{f_n\}$ is a uniformly bounded sequence in $L^1(\mathbb{R})$ (i.e. $||f_n||_{L^1(\mathbb{R})} \leq C ),$ then there may not exist subsequence $f_{n_k}$ and an $f\in L^1(\mathbb{R})$ such that $\int\limits_{\mathbb{R}} f_{n_k}(x)g(x) dx \rightarrow \int\limits_{\mathbb{R}}f(x)g(x)dx$ for all $g\in L^{\infty}(\mathbb{R}).$ Is there any simple counter example which demonstrates this?","['measure-theory', 'functional-analysis', 'weak-convergence']"
4610172,How can i use compactness arguments here?,"Question: Let $F$ be a real Banach space, $\mu$ be a Borel probability measure on a compact Hausdorff space $X$ , and let $ f : X \rightarrow F$ be a continuous mapping. Then, using a compactness argument show the existence of a point $ y \in \overline{co}(f(X))$ such that $$ \Psi(y) = \int _X \Psi \circ f d\mu \quad \textrm{for every}\; \Psi \in F'.$$ How can I show this property? can you give me hint? We can use the following Theorem Theorem: Let $F$ be a real Banach space, $\mu$ be a Borel probability measure on a compact Hausdorff space $X$ , and let $ f : X \rightarrow F$ be a continuous mapping. Given $ \Psi_1 , \ldots \Psi_n \in F'$ let $$ \nu _j = \int _X \Psi \circ f d \mu \quad \textrm{for}\quad j = 1, \ldots ,n,$$ and let $T : F \rightarrow \mathbb{R}^n$ linear function defined by $$ T(y) = (\Psi_1(y),\ldots, \Psi_n(y)) \quad \textrm{for every} \quad y \in F.$$ then, $$ (\nu_1,\ldots,\nu_n) \in co(T\circ f(X)) = T(co(f(X))),$$ where $co(B)$ denotes the convex hull of the set B. I used this theorem like that For any $ \Psi \in F'$ , we take $$ \nu = \int _X \Psi \circ f d \mu$$ and let $ T: F \rightarrow \mathbb{R}$ be defined by $$ T(y) = \Psi(y). $$ Hence, applying above theorem we have $$ \Psi(\int_X f d\mu) = \int_X \Psi \circ f d \mu \in co(\Psi (f(X))) = \Psi (co(f(X)))$$ I stack here can you give me any hints ?","['banach-spaces', 'complex-analysis', 'polynomials', 'functional-analysis', 'bochner-spaces']"
4610173,Proof of $\int_0^1\ln\left(\frac{1}{\ln \frac{1}{x}}\right)dx=\gamma$,"After solving this problem I found out that: $$\int_0^1\ln^sxdx=(-1)^s\Gamma(s+1)\tag{1}$$ Where $\Gamma(s)$ is the gamma function, defined as $(s-1)!$ when $s\in\mathbb{Z}$ . $(1)$ can be proven by induction. Anyways, I realized that I could use this to prove that $$-\int_0^1\ln\left(\ln \frac{1}{x}\right)dx=\gamma$$ Where $\gamma$ is the Euler-mascheroni constant. Here is my proof: Take the derivative of $\int_0^1(-\ln x)^sdx$ with respect to $s$ . To do this, move the differentiation operator inside the integral sign, meaning that we need to take the derivative of $\ln^sx$ . This is the same as $e^{s\ln\ln \frac{1}{x}}$ . By chain rule we get that the derivative of $(-\ln x)^s$ is $\ln(-\ln x)(-\ln x)^s$ . Setting $s=0$ back in the original equation, we get that $\int_0^1\ln\frac{1}{\ln \frac{1}{x}}dx=-\gamma$ since the derivative of the analytic continuation of the factorial function at zero is $-\gamma$ . Multiplying both sides by $-1$ yields the desired expression. Are there other proofs to this expression?","['integration', 'definite-integrals', 'factorial', 'euler-mascheroni-constant', 'calculus']"
4610180,Is there a uniformly continuous function whose derivative is merely pointwise continuous? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question Does there exist a function from $\mathbb{R}$ to $\mathbb{R}$ which is uniformly continuous, and whose derivative exists everywhere, and whose derivative is continuous, but not uniformly continuous over the entire real line?","['continuity', 'derivatives']"
4610204,Calculating double integral using variables substitution,"$\displaystyle
  D = \left\lbrace \left. \rule{0pt}{12pt} (x,y) \; \right| \;  3 x^2 + 6 y^2 \leq 1  \right\rbrace$ Calculate $\displaystyle
   \iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{}$ . Attempt: $x=\frac{r}{\sqrt3}cost,y=\frac{r}{\sqrt6}sint \implies |J|=\sqrt{\frac{2}{3}}r$ $3 x^2 + 6 y^2 \leq 1 \implies 0\leq r \leq 1$ $\iint_D  \frac{ x^2 }{ ( 3 x^2 + 6 y^2 )^{ 3/2 } }  \; dx dy{} =\int _0^1\:\int _0^{2\pi }\:\frac{cos^2t\sqrt{2}}{3\sqrt{3}}dtdr = \frac{\sqrt{2}\pi }{3\sqrt{3}}$ My answer isn't corect , can't find out what is wrong. Appreciate any help.",['multivariable-calculus']
4610218,Slope of normal,"Let a curve given by $y=x+\cos(xy) $ . Find the slope of normal line at $(0,1) $ point.
My solution: $$f(x,y) =y-x-\cos(xy)=0$$ $$f'=-f_x/f_y=\dfrac{1-y\sin(xy)}{1+x\sin(xy)}$$ so $f'(0,1)=1$ and slope of normal line must be $-1$ .
Is it true? Any mistake? Thanks.","['solution-verification', 'derivatives']"
4610248,Induction and Universal Generalization,"Let $(\mathbb N,<)$ be the set of natural numbers as defined in the book Introduction to Set Theory by Hrbacek and Jech. Suppose you are asked to show that $n<m$ implies $n+1\leq m$ for all $m,n\in\mathbb N$ . Consider the following proof : Let $P(n,m)$ denote the property that $n<m$ implies $n+1\leq m$ . Fix some arbitrary $n\in \mathbb N$ . We will show that $P(n,m)$ holds for all $m\in \mathbb N$ by induction on $m$ . $P(n,0)$ holds vacuously since $n<0$ is false. Suppose $P(n,m)$ holds for some $m\in\mathbb N$ and suppose that $n<m+1$ . Then $n\leq m$ . If $n<m$ then $n+1\leq m $ by the induction hypothesis, and if $n=m$ then $n+1=m+1$ . In either case $n+1\leq m+1$ . By the induction principle we conclude that $P(n,m)$ holds for all $m\in \mathbb N$ . As $n$ is arbitrary the conclusion follows. It seems to me that this proof is ok. However the answer here seems to suggest that it is incorrect. Am I missing something?","['elementary-set-theory', 'proof-writing', 'solution-verification', 'induction']"
4610293,"If we define $\cos$ and $\pi$ based on the differential equation of a weight on a spring, how can we derive the properties of a triangle?","If we use the differential equation $\frac{d^2y}{dx^2} = -y$ , with initial conditions $y(0)= 1$ and $y'(0)=0$ , we can obtain a Maclaurin expansion for the solution. As it happens, this expansion is the cosine function, and the periodicity of the expansion is $2\pi.$ What if we defined cos and pi according to this formula? How could we then prove that the cosine function gives us the ratio of adj/hyp for a right angled triangle, and pi is the ratio of a circle's circumference to  its diameter? Of course we conventionally define cos and pi using circles and triangles, because mankind investigated those things thousands of years before we learned how to solve differential equations. I am just wondering how it would work the other way around.",['ordinary-differential-equations']
4610313,Contest Math Question: simplifying logarithm expression further,"I am working on AoPS Vol. 2 exercises in Chapter 1 and attempting to solve the below problem: Given that $\log_{4n} 40\sqrt{3} = \log_{3n} 45$ , find $n^3$ (MA $\Theta$ 1991). My approach is to isolate $n$ and then cube it. Observe: \begin{align*}
\frac{\log 40\sqrt{3}}{\log 4n} = \frac{\log 45}{\log 3n} \\
\log 40\sqrt{3}\log 3n = \log 45\log 4n\\
\log 40\sqrt{3} \cdot (\log 3 + \log n) = \log 45 \cdot (\log 4 + \log n)\\
\log n \cdot (\log 40\sqrt{3} - \log 45) =  \log 45\log 4 - \log 40\sqrt{3}\log 3
\end{align*} Dividing through and putting the coefficients as powers, we have: \begin{align*}
\log n &= \frac{\log 45^{\log 4} - \log \left[(40\sqrt{3})^{\log 3}\right]}{\log\left(\frac{40\sqrt{3}}{45}\right)}
=\frac{\log \left(\frac{45^{\log 4}}{(40\sqrt{3})^{\log 3}}\right) }{\log\left(\frac{40\sqrt{3}}{45}\right)} \\
&=\log \left(\frac{45^{\log 4}}{(40\sqrt{3})^{\log 3}}\right)^{\log\left(\frac{40\sqrt{3}}{45}\right)^{-1}}
\end{align*} which shows that \begin{align*}
n^3 = \left(\frac{45^{\log 4}}{(40\sqrt{3})^{\log 3}}\right)^{3\cdot\log\left(\frac{40\sqrt{3}}{45}\right)^{-1}}
\end{align*} Somehow it feels like this answer may be simplified further. Are the steps shown so far correct and can the answer be expressed in a better way?","['contest-math', 'algebra-precalculus', 'logarithms']"
4610314,Galois group of the quintic $x^5 -11x + 1$.,"Consider $f = x^5 -11x + 1 \in \mathbb{Q}[x]$ . I want to prove that its not solvable by radicals. I know that its solvable by radicals iff its galois group is solvable. My attempt was first to use the following: For any prime $p$ not dividing the discriminant of $f \in \mathbb{Z}[x]$ , the galois group of $f$ over $\mathbb{Q}$ contains an element with cyclic decomposition $(n_1,..,n_k)$ where $n_1,...,n_k$ are the degrees of the irreducible factors of $f$ reduced mod $p$ . Then, I could use this to determine the galois group of $f$ . However, the discriminant proved to be super hard to calculate (wolfram alpha works but its not intended to be used). So I am thinking that I got the wrong approach here. Any other hints?","['field-theory', 'galois-theory', 'abstract-algebra', 'galois-extensions', 'radicals']"
4610315,What is cardinality of a set of all abelian operations on $\mathbb{R}$ up to an isomorphism?,"Let $A$ be the set of all operations $+$ such that $(\mathbb{R},+)$ is abelian group. I will define relation $\sim$ on $A$ in a following way: $+_1\sim+_2, ~+_1, +_2\in A$ if and only if $(\mathbb{R},+_1)$ and $(\mathbb{R},+_2)$ are isomorphic. My question is then: What is cardinality of $A/\sim$ ? I know that $A/\sim$ has at least two elements, but I suppose there are a lot more of them. I have little experience with group theory and I don't have any idea how to solve this problem.","['group-theory', 'group-isomorphism', 'abelian-groups']"
4610345,Formalizing some items in an electrostatics computation,"Consider the question attached (Example 3.4 from Zangwill's Modern Electrodynamics , Ch 3). I can follow the solution quite easily using ""physics math"" but, having just recently finished Spivak's Calculus , I want to formalize the operations performed and am hoping someone can help me with two items in particular. Note that I will refer to the equations in the attached picture as (1) through (6), in the order they appear. The only niceties which I intend to ignore in this ""formalization"" are the rigorous developments of the improper integrals and also the $\epsilon_0$ factor. All integrands etc. are continuous so all of the theorems I invoke apply. For equation (3), I noted that Zangwill is choosing to evaluate the line integral along the path $ \boldsymbol\ell $ which is parametrized as $ \boldsymbol\ell  =  \boldsymbol\ell(r') = r' \mathbf{\hat{r}} $ in the conventional spherical coordinate system, and we are evaluating from $r'=\infty$ to $r' = r$ . But I wanted to challenge myself to compute this line integral via the ""opposite"" path and to show that the same result obtains (as it must, since the line integral from a point to a point is independent of the parametrization of the path which we choose). Thus I tried to consider the parametrization $ \boldsymbol\ell  =  \boldsymbol\ell(u) = r'(u) \mathbf{\hat{r}} $ where $r'(u) = -u$ (perhaps this is my first error -- this was what I thought was appropriate for a path which ""starts"" at infinity and goes radially towards 0) where we go from $u = -\infty$ to $u = -r$ . Thus $ d\boldsymbol\ell  =  \frac{d\boldsymbol\ell}{du}du = -\mathbf{\hat{r}}du $ . Would this process be correct if I plugged it into the LHS equality of equation (3)? More importantly, I want to follow the u-substitution which is performed using the physicist shorthand of ""changing the integration variable"". More rigorously (I think), I could observe that the middle expression in equation (3) is written as (I change the integration variable to $x$ so there is no confusion with derivatives) $$\int_\infty^r dx \ g'(x) (f\circ g)(x)$$ where $$f (x) = \int_0^{1/x} ds \ s^2 \rho(s)$$ $$g(x) = 1/x$$ $$g'(x) = -1/x^2$$ so that by the u-sub theorem (Spivak Theorem 19-2) I have $$\int_\infty^r dx \ g'(x) (f\circ g)(x) = \int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s)$$ Integrating by parts (Spivak Theorem 19-1), I find then that the integral is given by $$\int_{g(\infty)}^{g(r)} dx f(x)=\int_{0}^{1/r} dx \int_0^{1/x} ds \ s^2 \rho(s) = \left(x\int_0^{1/x} ds \ s^2 \rho(s) \right)\Big|_0^{1/r} - \\ \int_{0}^{1/r} dx \ x (-1/x^2) (1/x)^2 \rho(1/x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3}$$ where in the last step I've tried to use the fundamental theorem of calculus (Spivak Theorem 14-1) and the chain rule to evaluate the derivative with respect to $x$ of the inner integral. I can then observe that the second term integral in the above has an integrand of the form necessary for u-substitution, with $f(x) = x \rho (x), g(x) = 1/x, g'(x) = -1/x^2$ so that I obtain $$\frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \int_{0}^{1/r} dx \  \frac{\rho(1/x)}{x^3} = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} - \int_{\infty}^{r} dx \ x \rho(x) = \frac{\int_0^{r} ds \ s^2 \rho(s)}{r} + \\ \int_{r}^{\infty} dx \ x \rho(x) $$ which is the correct final answer. I'm hoping someone can check and/or set me straight with both steps.","['multivariable-calculus', 'electromagnetism']"
4610367,Issue with proof of theorem 2.1.11 in Audin-Damian,"The theorem in question is as follows: Let $V$ be a closed smooth manifold and $f: V\to \mathbb{R}$ a Morse function. Let $a$ be a critical point of $f$ with index $k$ and $\alpha=f(a)$ . Suppose that for some $\epsilon>0$ , $f^{-1}([\alpha-\epsilon, \alpha+\epsilon])$ is compact and does not contain any critical points distinct from $a$ . For any sufficiently small $\epsilon>0$ , the homotopy type of $V^{\alpha+\epsilon}=f^{-1}([\alpha+\epsilon,-\infty))$ is that of $V^{\alpha-\epsilon}$ with a cell of dimension $k$ attached. In the beginning of the proof, we wish to construct a function $F$ which coincides with $f$ outside of a neighborhood of $a$ where $F<f$ , such that $F^{-1}((-\infty, \alpha-\epsilon))$ is the union of $V^{\alpha-\epsilon}$ and a neighborhood of $a$ . This is done by taking a Morse chart $(U,h)$ of a neighborhood of $a$ and an $\epsilon>0$ such that $f^{-1}([\alpha-\epsilon,\alpha+\epsilon])$ is compact and $U$ contains a ball of radius $\sqrt{2\epsilon}$ with center $0$ and using a smooth function $\mu:\mathbb{R}^{\geq 0}\to \mathbb{R}^{\geq 0}$ such that \begin{align}
\mu(0)&>\epsilon\\
\mu(s)&=0 \text{ for } s\geq 2\epsilon\\
-1<\mu'(s)&\leq 0 \text{ for every } s
\end{align} In the Morse chart $h: U\to V$ where $U\subset \mathbb{R}^{n}=\mathbb{R}^{k}\times \mathbb{R}^{n-k}$ open, $f$ takes the form $f(h(x_-, x_+))=\alpha-||x_-||^2+||x_+||^2$ Using this $\mu$ , we can construct a function $$F(x)=\begin{cases} &f(x) &\text{ if } x\notin h(U)\\
&\alpha-||x_-||^2+||x_+||^2\mu(||x_-||^2+2||x_+||^2) &\text{ if } x=h(x_-,x_+)
\end{cases}$$ The authors claim that outside of the set $||x_-||^2+2||x_+||^2\leq 2\epsilon$ one has $F=f$ , but if $\mu(s)=0$ for $s\geq 2\epsilon$ , then for $||x_-||^2+2||x_+||^2>2\epsilon$ we have $\mu(||x_-||^2+2||x_+||^2)=0$ and $$F(x)=\alpha-||x_-||^2\neq \alpha-||x_-||^2+||x_+||^2=f(x)$$ Is there a detail that I'm missing, or is there a typo?","['smooth-functions', 'smooth-manifolds', 'morse-theory', 'analysis', 'differential-geometry']"
4610375,$G$ cannot have a subgroup $H $ with $|H| = n-1$,"I'm trying to prove a group $G$ cannot have a subgroup $H$ with $|H| = n-1$ if $|G|=n > 2$ . Here is my attempt: The order of subgroup must divide the order of its group, and thus we would have $(n - 1) \mid n$ . But if $n > 2$ then $n < k(n-1)$ for any $k>1$ , contradiction. Is that correct?","['number-theory', 'group-theory', 'abstract-algebra', 'finite-groups']"
4610394,Showing $x^4 + 1$ is irreducible in $\mathbb{Q}[x]$.,"Clearly, none of the roots are in $\mathbb{Q}$ so $f(x) = x^4 + 1$ does not have any linear factors. Thus, the only thing left to check is to show that $f(x)$ cannot reduce to two quadratic factors. My proposed solution was to state that $f(x) = x^4 + 1 = (x^2 + i)(x^2 - i)$ but $\pm i \not\in \mathbb{Q}$ so $f(x)$ is irreducible. However, I stumbled across this post $x^4 + 1$ reducible over $\mathbb{R}$... is this possible? with a comment suggesting that $x^4 + 1 = (x^2 + \sqrt{2}x + 1)(x^2 - \sqrt{2}x + 1)$ which turns out to be a case that I did not fully consider. It made me realize that $\mathbb{Q}[x]$ being a UFD only guarantees a unique factorization of irreducible elements in $\mathbb{Q}[x]$ (which $x^2 \pm i$ nor $x^2 \pm \sqrt{2} x + 1$ aren't in $\mathbb{Q}[x]$ ) so checking a single combination of quadratic products is not sufficient. Therefore, what is the ideal method for checking that $x^4 + 1$ cannot be reduced to a product of two quadratic polynomials in $\mathbb{Q}[x]$ ? Am I forced to just brute force check solutions of $x^4 + 1 = (x^2 + ax + b)(x^2 + cx + d)$ don't have rational solutions $(a,b,c,d) \in \mathbb{Q}^4$ ?","['irreducible-polynomials', 'number-theory', 'abstract-algebra', 'polynomials']"
4610400,A proof of a sufficient condition to have that $f : A\subset\mathbb{R}^n\to\mathbb{R}$ is differentiable,"Consider $f : O\subset\mathbb{R}^{n}\to\mathbb{R}$ a function continuously differentiable on $O$ an open set of $\mathbb{R}^n$ that is all its partial derivatives exist on $O$ and they are continuous. We want to show that it is a sufficient condition to have that $f$ is differentiable on $O$ . To do this, we consider the open set $N(X_0, r)$ where $X_0\in O$ . The idea is to use the theorem discussed here : Kind of Taylor expansion for functions of several variables? This theorem says the following : Consider $f:A\subset\mathbb{R}^n\to\mathbb{R}$ where all its partial derivatives exists on the open ball $N(X_o, r)$ with $X_0\in A$ . Consider $Z$ a vector of $\mathbb{R}^n$ with $\lVert Z\rVert\leq r$ . Then we have $f(X_0 + Z) = f(X_0) + \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ with $V_i=(z_1, ..., z_{i-1}, \theta z_{i}, 0, ..., 0),\quad 0<\theta<1$ Now, consider $Z\in\mathbb{R}^n$ such that $\lVert Z\rVert < r$ . Then, we can use the theorem  above at the point $X_0$ , it follows that $f(X_0 + Z) = f(X_0) + \sum_{i=1}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ . If we consider the approximation $f_{x_i}^{'}(X_0)$ instead of $f_{x_i}^{'}(X_0 + V_i)$ we can write the following : $\epsilon(X_0, Z) = f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i $ And then we make appear the equality of interest for the differentiability that is : $\epsilon_1(X_0, Z) =\frac{1}{\lVert Z\rVert}\left[ f(X_0 + Z) - f(X_0) - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right] $ Using the fact that $f(X_0 + Z) - f(X_0) = \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i$ we have : $\lvert\epsilon_1(X_0, Z)\rvert =\left\lvert\frac{1}{\lVert Z\rVert}\left[ \sum_{i}^{n}f_{x_i}^{'}(X_0 + V_i)z_i - \sum_{i=1}^{n}f_{x_i}^{'}(X_0)z_i\right]\right\rvert$ $\quad\quad\quad\quad\quad = \left\lvert\sum_{i=1}^{n}\frac{z_i}{\lVert Z\rVert}\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert $ $\quad\quad\quad\quad\;\;\;\leq\sum_{i=1}^{n}\left\lvert\frac{z_i}{\lVert Z\rVert}\right\rvert\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert\leq\sum_{i=1}^{n}\left\lvert\left[f_{x_i}^{'}(X_0 + V_i) - f_{x_i}^{'}(X_0)\right]\right\rvert $ But $\forall 1\leq i\leq n : \lVert V_i\rVert\leq\lVert Z\rVert\implies\lim_{Z\to 0_{\mathbb{R}^n}} V_i =0$ Thus using the continuity of the absolute value and of $f_{x_i}^{'}$ at $X_0$ we get $\lim_{Z\to 0_{\mathbb{R}^n}}\lvert\epsilon_1(X_0, Z)\rvert = 0 $ And this holds for all $X\in O$ , which concludes the proof. Is this seems correct or do you see some improvement possible for this proof ? Thank you a lot !","['multivariable-calculus', 'calculus', 'derivatives', 'analysis']"
4610453,Coming up with reasonable estimate for dice EV problem,"There's this question on 100-sided die probability: The question is as follows: You are given a 100-sided die. After you roll once, you can choose to either get paid the dollar amount of that roll OR pay one dollar for one more roll. What is the expected value of the game? There is no limit on number of rolls. There's an answer here: If the expected value of this game is $a$ , then at a die roll of $X$ you have the choice of either collecting $X$ or paying a dollar and restart, which gives you an expected value of $a-1$ .
To maximize the expected value, you should take $X$ if $X> a-1$ and start over if $X\le a-1$ (it does not really matter what we do when $X=a-1$ ).
We obtain therefore $$ a = \frac1{100}\left(\lfloor a-1\rfloor\cdot a+\sum_{k=\lfloor a-1\rfloor+1}^{100}k\right)
=\frac1{100}\left(\lfloor a-1\rfloor\cdot a+\frac{100\cdot101}{2}-\frac{\lfloor a-1\rfloor \cdot\lfloor a\rfloor}{2}\right).
$$ I find numerically (didn't do much code checking, but the results are somewhat plausible) $$a\approx87.3571 $$ which seems to be exactly (and of course the true result must be rational) $$a=87\frac{5}{14}.$$ But I'm sure you can do the justification after the fact, i.e. show that the strategy that consists in continuing until you roll at least $87$ gives you $87\frac{5}{14}$ as expected value. Question: Is there a good way to quickly estimate something reasonably close to $87$ without writing a program, using WolframAlpha, or doing a calculation using scratch paper?","['estimation', 'expected-value', 'combinatorics', 'inequality', 'probability']"
4610483,Finitely generated modules over local domains,"Let $R$ be an integral domain. Let $P$ be a finitely generated $R$ -module. The problem: If we additionally assume that $R$ is a local ring (that is, $R$ is a local domain), is the following statement true? "" $P$ is torsion-free if and only if $P$ is projective."" Some facts I know/encountered and the result of my search... I apologize if this might be messy but I want to ensure everybody is on the same page as me. If $R$ is just an integral domain then the statement is not necessarily true. If $R$ is a principal ideal domain (PID) then the statement is true (easily proven using the structure theorem of modules over PIDs). If $R$ is a local ring and we drop the integral domain assumption then the statement is not necessarily true. If $R$ is a discrete valuation ring (= local ring + PID) then the statement is true (since it is a module over PID). Discrete valuation rings are valuation rings but the converse is not necessarily true. Valuation rings are integral domains and local rings. A valuation ring is Noetherian if and only if it is a discrete valuation ring or a field. (Thus, the statement above is true for Noetherian valuation rings) An integral domain is a valuation ring if and only if it is a Bézout ring and a local ring. [Proposition 1.5, Krull] A local ring is a Bézout domain if and only if it is a valuation ring. A Bézout domain is a Prüfer domain. ""A finitely generated module M over a Prüfer domain is projective if and only if it is torsion-free."" (Converse direction ""A finitely generated torsion-free module $M$ over a Prüfer domain $R$ is projective."" proven in textbook [Theorem 2.7, Modules over Non-Noetherian Domains by Fuchs & Salce, original result proven by Kaplansky]) The conclusion I have from all the statements above is: $$\{ \textit{discrete valuation ring} \} \supset \{ \textit{valuation ring} \} \supset \{ \textit{local rings} \} \cap \{ \textit{Bézout rings} \} \supset \{ \textit{local rings} \}  \cap \{ \textit{Prüfer domain}\}$$ and any ring in any collection above will make the statement true. The subquestions I am currently thinking about in hopes to answer the question above: If $R$ is a local domain, then is $R$ a Prüfer domain? If this is true, then we are done. If it is not true, I am hoping to construct a counter-example... In case this question is simpler, are there examples of rings that is a local domain but not a valuation ring?","['modules', 'ring-theory', 'abstract-algebra', 'local-rings', 'commutative-algebra']"
4610532,Notation for the index of an element of a disjoint union,"Suppose I have a set $I$ and an $I$ -indexed family of sets $\{A_i\}$ . I can write the disjoint union (aka coproduct) as $$
\sum_{i\in I} A_i.
$$ An element of this set is a pair $(i,a)$ with $i\in I$ and $a\in A_i$ . My question is, is there an established / typical notation for getting the first element of this pair? If it was an element $(i,j)$ of $I\times J$ , then I would write $\pi_0$ , e.g. $\pi_0((i,j)) = i$ , but using $\pi_0$ looks a bit weird for the disjoint union, because the projection maps are part of the definition of the product, not the sum.","['elementary-set-theory', 'notation', 'category-theory']"
4610593,Convex optimization using constraint projection matrices,"I have a convex optimization of the form $$
\min_x \frac{1}{2} x^TAx-x^Tb \\
\text{s.t.}\ (I-P)x=0
$$ where $A$ is a $n$ by $n$ positive definite matrix, and $P$ is a $n$ by $n$ projection matrix (it has $p$ eigenvalues equal to zero, and $n-p$ eigenvalues equal to one) Intuitively, it seems like I can separate the subspaces by rewriting it to: $$
\min_x \frac{1}{2} x^TP^TAPx+\frac{\gamma}{2} x^T(I-P)^T(I-P)x-x^TP^Tb
$$ And since $P$ is a projection matrix, it is symmetric and idempotent, it can be simplified to: $$
\min_x \frac{1}{2} x^T(PAP+\gamma(I-P))x-x^TPb
$$ This results in an unconstrained optimization, and the result is obtained by solving: $$(PAP+\gamma(I-P))x=Pb$$ This formulation is particularly handy because the matrix $(PAP+\gamma(I-P))$ is still positive definite for any positive $\gamma$ value (which is in fact the eigenvalues of the constraint subspace), so it can be solved numerically using the conjugate gradient method. In practice, it works very well, but to be honest, I don't know if there is a flaw in my intuition, and I haven't found any resources on the subject. Is there a known method that I can use to demonstrate my intuition is correct?","['convex-optimization', 'numerical-optimization', 'linear-algebra', 'conjugate-gradient']"
4610627,A new trick for the Sophomore's dream?,"Can we try to have for $x \in(\alpha,\beta),\beta>1,\alpha\geq 1$ real numbers: $$x^{-x}=\sum_{n=1}^{\infty}a_{n}(c_n+x)^{b_{n}}e^{-d_n(c_n+x)^{b_{n}+1}}$$ Where $a_n,b_n,c_n,d_n\in(-\infty,\infty)$ You can find some material in my answer here Prove that $\int_0^\infty\frac1{x^x}\, dx<2$ . The goal in integrating easily the RHS is to give an infinite series for the integral : $$\int_{0}^{\infty}x^{-x}dx$$ Using also the Sophomore's dream . I also tried the Weierstrass's factorization theorem and there is a link with Stirling's series which have explicit representation . How to find a sequence $a_n,b_n$ ? Sides notes : A straight approach using Am-Gm and $a_n$ all equal doesn't work ! We have better result with $a_n$ non equal and for example : $$f\left(x\right)-x^{-x}<2\cdot10^{-4}$$ Where : $f\left(x\right)=\frac{\left(ex^{\frac{42}{100}+\frac{12}{1000}}e^{-x^{1+\frac{42}{100}+\frac{12}{1000}}}+\left(1+e^{-1}\right)ex^{\frac{40}{100}+\frac{12}{1000}}e^{-x^{1+\frac{40}{100}+\frac{12}{1000}}}+\left(1-e^{-1}\right)ex^{\frac{41}{100}+\frac{12}{1000}}e^{-x^{1+\frac{41}{100}+\frac{12}{1000}}}\right)}{3}$ Example with the generalized equality : $$\frac{\left(\frac{e^{d\left(1+c\right)^{a}}}{\left(1+c\right)^{\left(a-1\right)}}\left(x+c\right)^{\left(a-1\right)}e^{-d\left(x+c\right)^{a}}+ex^{0.4212}e^{-x^{1.4212}}\right)}{2}$$ Where : $d=1.4,c=-0.6,a=1.2$ Ps :State like this the problem was impossible on $x\in(1,\infty)$ so I split it .","['improper-integrals', 'sequences-and-series']"
4610629,Degree of a polynomial injective on complex upper half plane,"I'm trying to solve the following problem. Any polynomial function injective on the upper half plane has degree less than or equal to 2. This seems intuitively true, since polynomial of degree $n$ will approximately be an $n$ fold covering map. However I have no idea how to prove it explictely. I tried to show that polynomial of degree > 2 is not injective by finding two points having the same value, but it didn't work well. Any idea for this one?",['complex-analysis']
4610649,Existence of density of product of random variable and random vector,"Let $X$ be a random vector in $\mathbb R^d$ and $Y$ be a binary random variable on the same probability space, and with values in $\{\pm 1\}$ . Suppose that for every $y \in \{\pm 1\}$ : conditioned on the event $Y=y$ , (the distribution of) $X$ has density. Question. Is it true that the random vector $Z:=YZ$ has density ? Rough guess For any $y \in \{\pm 1\}$ , let $\pi_y := \mathbb P(Y=y)$ and let $f_y:\mathbb R^d \to \mathbb R_+$ be the density of $X$ conditioned on the event $Y=y$ . For any measurable $A \subseteq \mathbb R^d$ , one has $$
\begin{split}
\mathbb P(Z \in A) &= \sum_y \pi_y\mathbb P(yX \in A \mid Y = y) = \sum_y \pi_y\mathbb P( X \in yA \mid Y = y)\\
& = \sum_y\pi_y\int_{y A}f_y(b)\mathrm{d}b = \sum_y \pi_y \int_{A}f_y(ya)\mathrm{d}a,
\end{split}
$$ where we have used the change of variable $b=ya$ , which has Jacobian determinant $1$ .
Thus, it would seem that $Z$ has density given  by the explicit formula $f(a) := \sum_y \pi_y f_y(ya)$ .","['conditional-probability', 'statistics', 'probability-distributions', 'probability']"
4610651,Asymptotic Confidence Interval for ML-Estimate of Gamma-distribution,"Assume I have a sample of $n \in \mathbb{N}$ data points $x_1,\ldots, x_n$ , which are asummed to come from iid drawings of a Gamma-distribution. I may assume the shape paramter $k\in\mathbb{R}_+$ and the sample mean $\bar{x}$ , but not the sample variance, are known. To avoid confusion the Gamma(k, $\theta$ ) density function used here is $f(x) = \frac{1}{\Gamma(k) \theta^k} x^{k-1} e^{-\frac{x}{\theta}}.$ I am trying to calculate the Maximum-Likelihood Estimator $\hat{\theta}$ and then find an asymptotic 95% Confidence Interval for that estimate. The MLE was no problem but I'm not sure about the asymptotic CI and would be very grateful, for corrections if I made a mistake there. The likelihood function has the form $$\mathcal{L} = \prod_{i=1}^{n} f(x_i, \theta) = \left(\frac{1}{\Gamma(k)\theta^k}\right)^n \prod_{i=1}^{n} x_i^{k-1} e^{-\frac{x_i}{\theta}}.$$ From there we get the log-likelihood $$ \ell = \ln(\mathcal{L}) = (k-1) \sum_{i=1}^{n} \ln(x_i) - \sum_{i=1}^{n} \frac{x_i}{\theta} - n\cdot \ln(\Gamma(k)) - nk\cdot\ln(\theta).$$ Differentiating with respect to $\theta$ and then solving for it yieds the Maximum-Likelihood Estimator $$\hat{\theta} = \frac{\bar{x}}{k}.$$ To find now an asymptotic 95%-Confidence Interval for $\hat{\theta}$ I first calculated the mean and variance of the Gamma(k, $\theta$ ) distribution, which are $$\mathbb{E}[X] = k\theta, \quad\quad \text{Var}(X)= k^2 \theta.$$ Given that the sample variance is unknown, I first calculated the variance of my estimate and then an estiamte of that variance: $\begin{align*}
			\text{Var}(\hat{\theta}) =& Var(\frac{1}{k} \frac{1}{n} \sum_{i=1}^{n}X_i) \\
			=& \left(\frac{1}{kn} \right)^2 \sum_{i=1}^{n} \text{Var}(X_i) \\
			\overset{iid}{=}& \left(\frac{1}{kn} \right)^2 \cdot n \cdot k\theta^2 \\
			=& \frac{\theta^2}{kn} \; =: \sigma^2 \\
			\hat{\sigma^2} =& \frac{\hat{\theta^2}}{kn} \\
			=& \frac{(\frac{\bar{x}}{k})^2}{kn} \\
			=& \frac{1}{kn} \cdot \frac{1}{k^2} \frac{1}{n^2} \left(\sum_{i=1}^{n}x_i\right)^2 \\
			=& \frac{1}{k^3} \cdot \frac{1}{n^3} \left(\sum_{i=1}^{n}x_i\right)^2 \\
			=& \frac{1}{k^3n} \bar{x}^2
		\end{align*}$ From that I have constructed the asymptotic Confidence Interval $\begin{array}{lrcccl}
&z_{0.025} &\leq& \frac{\hat{\theta} - \mathbb{E}\left[\hat{\theta}\right]}{ \sqrt{ \frac{\hat{\sigma^2}}{n} } } &\leq& z_{0.975} \\
\Leftrightarrow & \hat{\theta} - z_{0.975} \sqrt{ \frac{\hat{\sigma^2}}{n} } &\leq& \theta &\leq& \hat{\theta} + z_{0.975} \sqrt{ \frac{\hat{\sigma^2}}{n} } \\
\Leftrightarrow & \frac{\bar{x}}{k} - 1.96 \sqrt{ \frac{\bar{x}^2}{k^3n^2}} &\leq& \theta &\leq& \frac{\bar{x}}{k} - 1.96 \sqrt{ \frac{\bar{x}^2}{k^3n^2}}
\end{array}$ Although I can't find any mistakes (though admittedly I'm also not an expert in statistics), the results from calculating $\hat{\sigma^2}$ on, seem a bit strange to me. As I said, I would be very glad if someone could check whether I indeed made a mistake or if this can be solved somewhat more elegantly.","['statistics', 'confidence-interval', 'maximum-likelihood']"
4610685,Optimizing a nonlinear function with both equality and inequality contraints,"I have the non-linear optimization problem $$\min f(a,b,c,x)=\int_0^R\frac{y}{1+(2ay+b)^2}dy =\frac{\log(\frac{(2aR+b)^2+1}{b^2+1})-2b(\arctan(2aR+b)-\arctan(b))}{8a^2}$$ subject to the constraints $$\begin{align}
g_1(a,b,c,x)&=2\pi (\frac{aR^4}{4}+\frac{bR^3}{3}+\frac{cR^2}{2})-V=0,\\
g_2(a,b,c,x)&=-(ax^2+bx+c)\leq 0,\ \ \ \ (0 \leq x \leq R)
\end{align}$$ where $R$ and $V$ are positive constants. I am ultimately looking for a triplet $(a,b,c)$ that minimizes $f$ and satisfies the constraints for all $x \in [0,R]$ . First of all, I noticed that the $x \in [0,R]$ part can be handled by replacing $g_2$ with a new function $g_2^\ast$ defined by $$g_2^\ast(a,b,c,x)=-(ax^2+bx+c)\cdot x(x-R)\leq 0.$$ It seems pretty clear that this would most likely not have a clean closed-form solution, and after some calculations, Sage seems to struggle to symbolically solve the system of equations given by even just forming the Lagrangian $$\mathcal{L}(a,b,c,\lambda)=f(a,b,c,x)+\lambda g_1(a,b,c,x)$$ for the equality constraint and taking $$\nabla \mathcal{L}(a,b,c,\lambda)=0.$$ From what I've learned so far, I'd assume that to solve the full problem with all the constraints implemented, I would form the (new, disregard the above example) Lagrangian $$\mathcal{L}(a,b,c,x,\lambda,\mu)=f(a,b,c,x)+\lambda g_1(a,b,c,x)+\mu g_2^\ast(a,b,c,x)$$ and solve the system given by $$\nabla \mathcal{L}(a,b,c,x,\lambda,\mu)=0.$$ I have concluded that this problem could probably only be solved numerically, and am looking for some method to do so. After some google searches, I have stumbled upon Sequential Quadratic Programming which (from what I've read off of Wikipedia) would allow one to numerically estimate solutions to nonlinear programming problems. Furthermore, I have found that since this problem involves both inequality and equality constraints, the usual method of Lagrange multipliers may not be sufficient, and that after an optimal solution is found numerically, checking it with the KKT conditions would verify its optimality(though not a necessary condition since my function is non-convex). As a result, it would be great if someone could direct me to any programs which could carry out these calculations or implement these algorithms for me, and also explain how to apply the KKT conditions here to verify the calculations after such numerical estimates have been made. I am very new to optimization, only just familiar with Lagrange multipliers, so please let me know if I have made mistakes or misunderstood something. Thanks in advance!","['multivariable-calculus', 'numerical-optimization', 'nonlinear-optimization']"
4610688,"$D: \Bbb R[x]\to \Bbb R[x]$ satisfies $D(fg)=D(f)g+fD(g), D(x)=1$.Can we show that $D$ is linear?","$D: \Bbb R[x]\to \Bbb R[x]$ satisfies $D(fg)=D(f)g+fD(g), D(x)=1$ .Can we show that $D$ is linear? Clearly, we have $D(x^k)=kx^{k-1}$ , $D(1)=0$ . But these two properties can ensure that $D$ is linear?","['derivatives', 'linear-algebra', 'polynomials']"
4610696,Which of the following statements is/are correct about a real-valued bounded function in $\mathbb{R}^2?$,"Let $f$ be a real-valued bounded function in $\mathbb{R}^{2}$ such that for all real $t,$ the functions $g_t(y)=f(t,y)$ and $h_t(x)=f(x,t)$ are non-decreasing. Then which of the following is/are correct? $f(x,x)$ is non-decreasing. $f$ can have uncountable number of discontinuities. $\lim\limits_{(x,y)\to(+\infty,+\infty)} f(x,y)$ exists. $\lim\limits_{(x,y)\to(+\infty,-\infty)} f(x,y)$ exists. Option $1$ seems correct to me since $f$ is non-decreasing in each co-ordinate. For 2) let $f(x,y)=\left\{\begin{align}
0, & \mbox{ if }~ x\leq0 \mbox{ or } y\leq0\\
1, ~& x>0\mbox{ and y>0}\end{align}\right.$ Then $f$ is bounded in $\mathbb{R}^2,$ is non-decreasing and is discontinuous at each point on x-axis as well as on y-axis. So option 2) is also correct. I am thinking that 3) and 4) should also be correct since $f$ is given to be bounded. However, I am not sure.","['examples-counterexamples', 'real-analysis', 'continuity', 'calculus', 'limits']"
4610707,"How can I prove that this subset $C$ is closed in $C[0,1]$?","Let $C=\{f\in C[0,1]: f(0)=f(1)\}$ . I need to show that $C$ is closed in $C[0,1]$ with respect to $||f||_{\infty}=\max(f)$ . I know that I can define $\phi(f)=f(0)-f(1)$ and then argue that $\phi$ is continuous and $C=\phi^{-1}(\{0\})$ . But I wanted to do it in another way: Proof Let $(f_n)_n$ be a sequence in $C$ s.t. $$f_n\stackrel{||\cdot||_\infty}{\longrightarrow}f\in C[0,1]$$ I want to show that $f\in C$ . But now consider $$\begin{align}|f_n(0)-f(0)|&\leq\max_x|f_n(x)-f(x)|\\&=||f_n-f||_\infty\rightarrow0\end{align}$$ Similarly for $f_n(1)$ . So we get $\lim_nf_n(0)=f(0)$ and $\lim_nf_n(1)=f(1)$ , but since the limit is unique and for all $n$ we have $f_n(0)=f_n(1)$ , we deduce that $f(0)=f(1)$ and therefore $f\in C$ . Now my question is does this work or am I wrong?","['analysis', 'solution-verification', 'functional-analysis', 'uniform-convergence', 'pointwise-convergence']"
4610719,A characterization of absolutely continuous functions,"Let $\lambda$ be the Lebesgue measure on $[a, b]$ . I'm trying to prove below result mentioned as Theorem 5.2 in my lecture note of gradient flows. Theorem A map $F:[a, b] \to \mathbb R$ is absolutely continuous if and only if there is a $\lambda$ -integrable $f:[a, b] \to \mathbb R$ such that $$
\int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b].
$$ In that case, $F$ is differentiable $\lambda$ -a.e. and $F'=f$ $\lambda$ -a.e. Could you confirm if my below attempt is fine? Proof Let $F$ be absolutely continuous. Then $F$ is of bounded variation. Then $F$ is differentiable $\lambda$ -a.e. and its derivative $F'$ is $\lambda$ -integrable . Let $G(x) := \int_a^x F' \mathrm d \lambda$ . Then $G$ is absolutely continuous. So $G-F$ is absolutely continuous. By Lebesgue differentiation theorem , $G$ is differentiable $\lambda$ -a.e. and $G' = F'$ $\lambda$ -a.e. It follows that $(G-F)' = 0$ $\lambda$ -a.e. Then $G-F$ is a constant. On the other hand, $G(a)=F(a)=0$ . So $G=F$ . Let $f:[a, b] \to \mathbb R$ be $\lambda$ -integrable such that $$
\int_a^x f \mathrm d \lambda = F(x)-F(a) \quad \forall x \in [a, b].
$$ Let $G(x) := \int_a^x f \mathrm d \lambda$ . Then $G$ is absolutely continuous. Then $F =G + F(a)$ is absolutely continuous. By Lebesgue differentiation theorem , $G$ is differentiable $\lambda$ -a.e. and $G' = f$ $\lambda$ -a.e. On the other hand, $F'=G'$ $\lambda$ -a.e.","['absolute-continuity', 'lebesgue-measure', 'derivatives', 'real-analysis']"
4610764,Why is implication used instead of conjunction in this instance of translating English into a logical statement?,"Can you please help me understand why implication was used instead of conjunction in the answer to this practice question? I have been struggling with nested quantifiers and when to use implication versus conjunction. I thought for sure this would be a case for conjunction. Thank you. Use predicates, quantifiers, logical connectives, and
mathematical operators to express the statement that every positive integer is the sum of the squares of four integers. my answer: $$ \forall x \exists a \exists b \exists c \exists d ((x > 0) \land (x = a^2 + b^2 + c^2 + d^2)) $$ correct solution from book: $$ \forall x \exists a \exists b \exists c \exists d ((x > 0) \implies x = a^2 + b^2 + c^2 + d^2) $$","['discrete-mathematics', 'logic-translation']"
4610773,Possible closed form of $\int_{0}^{1} \int_{0}^{1} \int_{0}^{1} \frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z$,"Reviewing the link , I consider to evaluate $$
\int_{0}^{1} \int_{0}^{1} \int_{0}^{1}
\frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z.
$$ And I quickly discover $$
\int_{0}^{1} \int_{0}^{1} \int_{0}^{1}
\frac{1}{(1+x^2)(1+y^2)(1+z^2)\sqrt{3-x^2-y^2-z^2} }\text{d}x\text{d}y\text{d}z\\
=\frac{9\sqrt{2}\pi }{4}\int_{\sqrt{\frac53}}^{\sqrt{2} } 
\frac{\arctan(x)}{\left ( 2x^2-3 \right )\sqrt{3x^2-5}  }\text{d}x
-\frac{\pi^3}{\sqrt{6} }+\frac{3\pi^2}{2\sqrt{6} }\arctan(2\sqrt{6} ).
$$ I don't know whether this is helpful or not. But I instinctively know it has a sufficient simple result, which only appears $\pi,\arctan$ and some quadratic irrationals. Hopefully you are glad to reach for my hand.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'multiple-integral']"
4610818,"Prove that $p_n\to 1/2,$ and $q_n/(1/4)^n\to -1$","Here is problem 10 of 13th Annual Harvard-MIT Mathematics Tournament Team Round A 2010 Call a 2n-digit base-10 number special if we can split its digits into two multisets of size n such that the sum of the numbers in the two sets is the same. Let $p_n$ be the probability that a randomly chosen 2n-digit number is special. We allow leading zeroes in 2n-digit numbers. Prove that $p_n\to 1/2$ as $n\to\infty$ . Let $q_n=p_n-1/2.$ Prove that $\lim\limits_{n\to\infty} q_n/(1/4)^n = -1.$ There is an official solution to this problem, though I don't understand it fully. I was therefore thinking of requesting an answer here in the hopes that I'd understand it better. For convenience, here is a link to the official solution. To reiterate, it is not necessary to answer all of the questions I ask here, though it may be easier than coming up with an alternative solution to this problem. The main thing I don't understand about the solution to part a is why their proof shows that $p_n$ converges to $1/2$ . The proof of the second part of this question given in the official solution is quite hard for me to understand. In particular, I have the following questions about the solution. It seems that I can't fully understand the proof of part (a), which is why I can't seem to prove claims that follow from a similar argument to part 1 in this question. Edit: I've removed some previous questions so that this post no longer has as many questions. Why is a number good if it has any odd digits? Again, this seems similar to the proof of part a). If there are no odd digits, why does the sum being divisible by 4 imply the number is good? The only change from the official solution that I think should be made is that 8-k should be replaced with 4-k. Here is my take of the case where the sum of the digits is congruent to 0 modulo 4 and all the digits are even, where balancing argument A refers to Apass.Jack's final algorithm in his solution: Also, even if there are no odd digits, if the sum of the digits is divisible by 4, then the number is balanced. Indeed, in this case, the number of even numbers congruent to 2 modulo 4 must be even. We first set aside 10 occurrences of the digits 0,4,6,8. We then perform balancing argument A on the remaining digits, which still have a sum congruent to 0 modulo 4. Then if $S_1$ and $S_2$ are the resulting sets of digits, and $s_i$ denotes the sum of all numbers in $S_i$ for $i=1,2,$ we have $s_1 + s_2 \equiv 0\mod 4$ and both $s_1$ and $s_2$ are even, so $s_1 - s_2\equiv -2s_2\equiv 0\mod 4.$ Let $d = |s_1-s_2|.$ Suppose $s_1 \leq s_2$ . We now add $(10-d/2)/2$ occurrences of the two numbers 4 and 6 and $(10+d/2)/2$ occurrences of the two numbers 0 and 8 to $S_1$ . Finally, we add $(10+d/2)/2$ occurrences of the two numbers 4 and 6 and $(10-d/2)/2$ occurrences of 0 and 8 to $S_2.$ Then the difference between the new sums $s_1$ and $s_2$ is now zero.","['contest-math', 'elementary-number-theory', 'calculus', 'combinatorics', 'probability']"
4610832,How to solve a second order ODE with Dirac delta inhomogeneity?,"I am working with the following second order ODE \begin{equation}
y''(x) - q² y(x) = F \delta(x - x_0)
\end{equation} on the domain $-L/2$ to $+L/2$ without specifying any boundary conditions for now. The solution to this problem I found in the literature is \begin{equation}
y(x) = A \cosh(qx) + B \sinh(qx) + \frac{F}{2q} e^{q|x-x_0|} .
\end{equation} However, I can not reconstruct this result. I am not sure how to split the solution into two parts before and after the Dirac spike.
Here is how far I have come:
The solution to the homogeneous equation can be obtained from the Ansatz \begin{equation}
y(x) = A e^{-qx} + B e^{qx} .
\end{equation} For the non-homogeneous equation I try variation of parameters starting from \begin{equation}
y(x) = C(x) e^{-qx} + D(x) e^{qx} .
\end{equation} Plugging this Ansatz into the ODE and introducing the constrain $C'(x) e^{-qx} + D'(x) e^{qx} = 0$ allows me to solve for the coefficients $C'(x)$ and $D'(x)$ \begin{equation}
C'(x) = - \frac{F}{2q} \delta(x - x_0) e^{qx} \quad \text{and} \quad D'(x) = \frac{F}{2q} \delta(x - x_0) e^{-qx} .
\end{equation} Integrating these equations then gives \begin{equation}
C(x_0) = - \frac{F}{2q} e^{qx_0} \quad \text{and} \quad D(x_0) = \frac{F}{2q} e^{-qx_0},
\end{equation} which do not depend on $x$ anymore, but only $x_0$ . This can now be used to obtain the full solution to the ODE \begin{equation}
y(x) = A e^{-qx} + B e^{qx} + \frac{F}{2q} \left( e^{q(x - x_0)} - e^{q(x_0 - x)} \right) .
\end{equation} I think the homogeneous part of mine is somewhat similar to the solution from the literature. I am more concerned what I missed in constructing the last term. Does anyone have any ideas on this?","['dirac-delta', 'ordinary-differential-equations']"
4610889,Bounded functions $f:\mathbb Z^n\to \mathbb R$ that are Discrete Harmonic at all but finite set of points .,"Question Let $n\in N$ , and let $S$ be a finite set in $Z^n$ . What is the dimension $d(n,S)$ of the space $F(n,S)$ of bounded functions $f:\mathbb Z^n\to \mathbb R$ that are discrete harmonic on $Z^n\setminus S$ ? Definition A function $f:\mathbb Z^n\to \mathbb R$ is discrete harmonic iff $$
  2nf(x) = \sum_{i=1}^n [f(x-e_i)+f(x+e_i)], \tag{*}\label{H}
$$ for all $x\in \mathbb Z^n$ ,
where $(e_1,\ldots,e_n)$ is the canonical basis of $\mathbb Z^n$ . Note that the condition \eqref{H} can be interpreted as that $f$ is a martingale w.r.t. a symmetric random walk on $\mathbb Z^n$ . The case $n=3, S=\{0\}$ This question was motivated by the question about existence of a bounded non-constant function $f:\mathbb Z^3 \to \mathbb R$ that is discrete harmonic except at the origin $(0,0,0)$ . An example of $f\in F(3,\{0\})$ is $f_1(x)$ equal to the probability that a symmetric 3D random walk starting from $x$ hits the origin. It can then be asked if all functions in $F(3,\{0\})$ can be obtained as linear combinations of $f_1(x)$ and the constant function $f_0(x)=1$ . The case $S=\{0\}$ For $n=1$ and $n=2$ the question is closely related to the related question with $S=\emptyset$ . A random walk in 1D and 2D is guaranteed to return to the origin. This suggest that $d(1,\{0\})=d(2,\{0\})=1$ . For $n\geq 3$ it is no more guaranteed that the random walk will be absorbed in at the origin and so there is at least one more dimension associated the ""value of never returning"". Can we conclude that $d(n,\{0\}) = 2$ for $n\geq 3$ ? The case $n=1$ Consider $S=\{a,b\}$ . If $b=a+1$ , then $f(a)=f(a-1)=\ldots$ and $f(b)=f(b+1)=\ldots$ , but $f(a),f(b)$ can be arbitrary. What is more, $f(x)$ has to be linear between the points $a$ and $b$ . For the general case of $S\subset \mathbb Z$ with $|S|\geq 2$ , $f$ is constant below $\min(S)$ and above $\max(S)$ , and is linear in between the points of $S$ .","['harmonic-functions', 'random-walk', 'functions', 'discrete-mathematics', 'probability']"
4610941,Perspective image of a cuboid,"You are given a perspective image $A'B'C'D'E'F'G'H'$ of a cuboid (rectangular prism) $ABCDEFGH$ .  The image is such that in the image, in any face, opposite sides are not parallel to each other .  In addition, the measure of the $AE$ (in the cuboid itself) is known.  (Note that $AE = BF = CG = DH $ ).  Can we find the measure of the other two dimensions $AB$ and $BC$ ? (Note that $AB = CD = FE = GH$ and that $BC = DA = EH = GF $ )","['projection', 'geometry']"
4610945,Dimension of the space of discrete harmonic functions $f:\mathbb Z^n\to \mathbb R$,"Fix $n\in \mathbb N$ . Let $F(n)$ be the space of discrete harmonic functions $f:\mathbb Z^n\to \mathbb R$ . What is the minimal $d \in \mathbb N$ (if there is one) such that there exists $B\subset F(n)$ with $|B|=d$ that every $f\in F(n)$ can be written as a linear combination of functions from $B$ ""subject to a translation"", i.e. there is $m\in \mathbb N$ and $(a^1,f^1,x^1),\ldots,(a^m,f^m,x^m) \in \mathbb R \times B \times \mathbb Z^n$ such that $$f(x) = \sum_{j=1}^m a^j f^m(x-x^m) \quad \forall x\in \mathbb Z^n. \tag{*}$$ A function $f:\mathbb Z^n\to \mathbb R$ is harmonic iff $$
  2nf(x) = \sum_{i=1}^n [f(x-e_i)+f(x+e_i)],
$$ for all $x\in \mathbb Z^n$ ,
where $(e_1,\ldots,e_n)$ is the canonical basis of $\mathbb Z^n$ . Note that $f$ being discrete harmonic can be interpreted as that $f$ is a martingale w.r.t. a symmetric random walk on $\mathbb Z^n$ . Case $n=1$ : It is easy to see that $f(x)$ has to have a constant growth, thus $F(1)$ is the space of all the linear functions $f:\mathbb Z\to \mathbb R$ . Case $n=2$ : Analogously, $F(2)$ also contains all the linear functions $f:\mathbb Z^2\to \mathbb R$ . However, the function $f^2_{12}(x) = (x_1)^2 - (x_2)^2$ is also discrete harmonic.  Does $f^2_{12}$ together with the basis of the space of linear functions $f:\mathbb Z^2\to \mathbb R$ span the whole $F(2)$ ? Clearly not, as @NeitherNor pointed out, there are other linearly independent functions in $F(2)$ and as @RyszardSzwarc brought up, their translations are also in $F(2)$ , thus the complicated definition of ""basis"" $B$ . Case $n\geq 3$ : Besides the linear functions also the functions $f^3_{ij}=(x_i)^2 - (x_j^2), i\neq j, \ i,j \in \{1,2,3\}$ are harmonic...","['harmonic-functions', 'random-walk', 'probability-theory', 'discrete-mathematics']"
4610952,ODE theory question,"I would like to get a hint for this question: ""let $p(x)$ , $q(x)$ , be continuous functions at $\mathbb{R}$ , and the ODE $y''+p(x)y'+q(x)y=0 $ . Prove that if $y_1(x) \neq 0$ , $y_2(x)$ solve the ODE, so $h(x) =\frac{y_2(x)}{y_1(x)}$ is a strictly monotone function or a constant function."" I didn't write my attempts because I barely know how to begin with this question. Thanks.",['ordinary-differential-equations']
4610962,How to get the distribution of a random variable from the distribution of their summation?,"Let ${\textstyle \{X_{1},\ldots ,X_{n}}\}$ be a sequence of independent and identical random variables. The distribution of $X_n$ is unknown. Assuming that we know the distribution of the following summation: $${S}\equiv \sum_{n=1}^{\infty}\frac{X_n}{n^2}$$ Would it be possible to find the distribution of $X_n$ from $S$ ?","['probability-distributions', 'central-limit-theorem', 'probability', 'random-variables']"
4610971,Which matrices can be embedded in flows?,"I've been thinking about matrices, recently, and wondering about a fairly simple but maybe hard-to-answer question. A real $n\times n$ matrix $A$ yields a discrete dynamical system on $\mathbb{R}^n$ , where from time $n$ to time $n + 1$ the vector $\mathbf{x}$ jumps to the vector $A\mathbf{x}$ . Under what conditions on $A$ can we ""interpolate"" this to a continuous-time dynamical system, i.e., find some vector field $\mathbf{X}$ such that the time-one flow of $\mathbf{X}$ takes any vector $\mathbf{x}$ to $A\mathbf{x}$ ? Here is what I think I know (but please correct me if I'm wrong). A smooth homomorphism $\phi\colon \mathbb{R} \to \text{GL}_n(\mathbb{R})$ always takes the form $t \mapsto e^{tB}$ for some $B \in \text{M}_{n\times n}(\mathbb{R})$ . Thus, there exists a flow through matrices whose time-one map equals $A$ if and only if $A = e^B$ for some matrix $B \in \text{M}_{n\times n}(\mathbb{R})$ . There are known conditions specifying when this is true; for instance, a paper by Walter Culver titled ""On the Existence and Uniqueness of the Real Logarithm of a Matrix"" says that such a $B$ exists if and only if $A$ is nonsingular, and each Jordan block of $A$ belonging to a negative eigenvalue occurs an even number of times. But do there exist matrices $A$ which are not the exponential of some other real matrix, but nevertheless are the time-one map of some more complicated, non-linear, flow? Thus, we are looking for a smooth homomorphism $\phi\colon \mathbb{R} \to \text{Diff}(\mathbb{R}^n)$ such that $\phi(1) = A$ , or equivalently (I think), a not-necessarily-linear vector field $\mathbf{X}$ whose time-one flow equals $A$ . By the way, I don't need this for any work I'm doing; I just got curious. In terms of my level, I went to grad school for math, but it was years ago and I don't remember things as well as I should. Thanks in advance for your help! Edit: As Alp Uzman points out in his answer, if $A$ embeds in a nonlinear flow, then clearly $A$ must commute with nonlinear diffeomorphisms. By Kopell’s Theorem (again, see Uzman’s answer), this is not possible if $A$ is a contraction and certain conditions on its eigenvalues hold. In dimension 1, the possibilities for diffeomorphisms commuting with any smooth contraction $f$ (not necessarily a linear map) are quite restricted, thanks to Kopell’s Lemma. It’s natural to ask whether this result can be generalized to higher dimensions. I posed this question on Mathoverflow years ago, and have not yet gotten an answer: https://mathoverflow.net/questions/168550/kopells-lemma-in-higher-dimensions","['vector-fields', 'linear-algebra', 'differential-topology', 'lie-groups', 'dynamical-systems']"
4610977,Simple subgroups and the Wielandt subgroup,"My goal is to prove the following: Let $G$ be a finite group and let $S \leq G$ be a simple subgroup. Suppose that $SH = HS$ for all subnormal subgroups $H$ of $G$ . Show that $S$ is contained in the Wielandt subgroup $\omega(G)$ of $G$ Note : $\omega(G) = \bigcap N_G(H)$ , where $H$ runs over the subnormal subgroups of $G$ . Here's my attempt: We know (from this paper by Wielandt ) that $\omega(G)$ contains every minimal normal subgroup of $G$ (as stated in the Encyclopedia of Math on the Wielandt subgroup). So it would suffice to prove that $S \subset \operatorname{Soc}(G)$ , since this is the subgroup generated by such minimal normal subgroups. Suppose, then, that this is not true. Then, we must have, by the simplicity of $S$ , $S\cap \operatorname{Soc}(G) = 1$ , which implies $S \cap M = 1$ for all minimal normal subgroups $M$ of $G$ . This feels like it can't happen, but I don't really know how to prove it. I haven't yet used $SH = HS$ , so that's probably where to go next, but I just don't see where this hypothesis comes in... I know, from this post , that it wouldn't be possible if $S$ were nonabelian and subnormal, but I don't think it's necessarily the case here. I don't actually need (or even really want) a complete solution, just a hint of what to do now, or if there is a better approach. Thanks in advance!","['finite-groups', 'simple-groups', 'normal-subgroups', 'socle', 'group-theory']"
4611002,Does type of moving average for an irregular time series exist already?,"Context I have an irregular time series where data points occur at irregular intervals of time. As a way to observe the behavior of this data over time, I want to use some type of moving average . The traditional suggestions for moving averages such as simple, weighted, or exponential, are not useful due to the irregular frequency of data points. Date Value 2022-12-04 10 2022-12-03 10 2022-12-02 10 2022-12-01 10 2020-01-01 1 This is an example of what could happen if I blindly applied a simple moving average. Suppose this example data represents a sequence of 5 data points that could occur within my data. $$
SMA_5 = \frac{10+10+10+10+1}{5}
$$ $$
SMA_5 = 8.2
$$ The simple moving average is dragged down by that last observation of 1, even though the observation occurred two years ago and is of little relevance now. What I want is a moving average that would somehow take into consideration how long it has been from time of evaluation to time of observation. A weighted or exponential moving average, therefore, is not useful since it would assign less weight to the last value for the sake of it being the 5th, and not because it was a long time ago. My Approach The solution that I've come up with involves the following steps. I apologize in advance, as I am sure the process is unorthodox. Computing $DaysAgo$ , a new column containing the number of days that has passed since the time of evaluation $$
DaysAgo = CutoffDate - ObservedDate
$$ Taking an arbitrary cutoff date of 2022-12-05, my sample data now looks like this. Date Value Days Ago 2022-12-04 10 1 2022-12-03 10 2 2022-12-02 10 3 2022-12-01 10 4 2020-01-01 1 1069 Computing $Weight$ , a new column containing a computed weight for each value $$
Weight = \frac{1}{\alpha*DaysAgo}+\frac{\alpha-1}{\alpha}
$$ $$
\alpha \geq 1
$$ The idea behind this formula is to let the weight of each observation model the rational function $y = \frac{1}{x}$ , so as to achieve decay over time reflected in the weight. $\alpha$ is there to have a parameter that lets us adjust the strength of the decay over time. The sample data now looks like this. Date Value Days Ago Weight 2022-12-04 10 1 1.0000 2022-12-03 10 2 0.5000 2022-12-02 10 3 0.3333 2022-12-01 10 4 0.2500 2020-01-01 1 1069 0.0009 Computing $WeightAdj$ , a new column containing the adjusted values of $Weight$ $$
WeightAdj = \frac{Weight}{\sum Weight}
$$ $$
WeightAdj = \frac{Weight}{1.0000+0.5000+0.3333+0.2500+0.0009}
$$ The sample data now looks like this. Date Value Days Ago Weight WeightAdj 2022-12-04 10 1 1.0000 0.4798 2022-12-03 10 2 0.5000 0.2399 2022-12-02 10 3 0.3333 0.1599 2022-12-01 10 4 0.2500 0.1199 2020-01-01 1 1069 0.0009 0.0004 The weight adjustment allows for the following. $$
\sum WeightAdj = 1
$$ Compute $ValueAdj$ , a new column containing the adjusted values of $Value$ $$
ValueAdj = Value * WeightAdj
$$ The sample data now looks like this. Date Value Days Ago Weight WeightAdj ValueAdj 2022-12-04 10 1 1.0000 0.4798 4.7978 2022-12-03 10 2 0.5000 0.2399 2.3989 2022-12-02 10 3 0.3333 0.1599 1.5993 2022-12-01 10 4 0.2500 0.1199 1.1995 2020-01-01 1 1069 0.0009 0.0004 0.0004 Finally, we compute the value of my custom moving average $$
MA_5 = \sum ValueAdj
$$ $$
MA_5 = 9.9960
$$ $$
\alpha = 1
$$ In which, we can see that the old observation barely affects the outcome. If I wanted the time decay to be weaker, however, all I have to do is adjust $\alpha$ . $$
MA_5 = 8.7284
$$ $$
\alpha = 2
$$ $$
MA_5 = 8.2795
$$ $$
\alpha = 10
$$ This seems to me like a novel solution to my problem. It lets me analyze my data over time if I'm willing to compute this moving average many times while shifting the cutoff date by, say, a day each time. Not to mention, this moving average doesn't freak out if you have many points that occurred at the exact same time, or a singular point that occurred a very long time ago. My Questions Does this type of moving average exist already? Does this type of moving average remind you of another one that is more efficient? Are there any cases where this moving average behaves unexpectedly? As I got to coding this solution in Python, I figured it would be a good idea to ask about it here first. I welcome any criticisms or anyone who's got a more elegant solution. Your thoughts are much appreciated, thanks.","['time-series', 'statistics']"
4611010,Is $(X_1+X_2+...+X_n)/n$ a random variable?,"If $X_1,X_2,...,X_n$ are i.i.d random variables and are all discrete/continuous, then is $(X_1+X_2+...X_n)/n$ also a random variable? My attempt: For continuous type, I guess it is a random variable. Since $Z=X+Y$ is a random variable, we can view $X_1+X_2+...+X_n=nZ$ , then we can get the CDF of it. After we got the CDF, we can get the PDF. But I stuck on the CDF Step, because at here it is in higher dimensions, so we can not use the classical method in two dimensions to get the pdf. And I am also not sure about the discrete  case. Could someone explain more to me? Please give me the answer about two cases. ( $X_1,X_2,...,X_n$ are all discrete and $X_1,X_2,...,X_n$ are all continuous) Moreover, if we just apply the definition of random variables(transfer the event to a real number), then maybe in both cases. They are random variables. But here we are trying to transfer multiple events? Will the sample space(collection of all outcomes) change to higher dimensions?(like the case in joint pmf/pdf)","['statistical-inference', 'statistics', 'probability-theory', 'probability', 'random-variables']"
4611013,Find the symmetric matrix given its eigenvalues and eigenvector.,"$A$ is a $3 \times 3$ symmetric matrix. It has the eigenvalue $\lambda_1 = 3$ with the eigenvector $$
\begin{bmatrix} 
1 \\ 0 \\ -1 
\end{bmatrix}
$$ and a double eigenvalue $\lambda_2 = -1$ .
Find the matrix $A$ . Since $A$ is symmetric, it can be diagonalized orthogonally, with an orthogonal matrix $P$ as $A = PDP^T$ . The matrix is made of the eigenvectors that are orthogonal to each other. What I then did was with the help of inner product, I got a vector that is orthogonal to $v_1$ and with the help of cross product I got another orthogonal vector. Those vectors are $$
\begin{bmatrix} 
1 \\ 1 \\ 1 
\end{bmatrix}
\quad\text{and}\quad 
\begin{bmatrix} 
1 \\ -2 \\ 1 
\end{bmatrix}
$$ so $$
A = PDP^{-1} 
= \begin{bmatrix} 
 1 &  1 & -5 \\ 
 1 & -5 &  1 \\ 
-5 &  1 & -5 
\end{bmatrix}. 
$$ But my answer is wrong obviously since $Av \neq \lambda v$ . The answer is $$
\begin{bmatrix} 
 1 &  0 & -2 \\ 
 0 & -1 &  0 \\ 
-2 &  0 & 1 
\end{bmatrix}
$$ where the vectors are $v_2 = \begin{bmatrix} 1 & 0 & 1 \end{bmatrix}^T$ and $\begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T$ . The answer sheet says that these vectors are orthogonal to $v_1$ . Also they didn't do $A = PDP^T$ instead they did $A = PDP^{-1}$ . Can anyone help me understand?","['eigenvalues-eigenvectors', 'orthogonality', 'matrices', 'linear-algebra', 'symmetric-matrices']"
4611026,"Set Theory - Cardinality - Having trouble ""building"" functions","While not a specific problem, trying to prove the cardinality of one set is equal to that of another one leaves me puzzled and confused. After looking at several problems and solutions, I don't know how to define an inverse function that satisfies (meaning, I'm having trouble thinking of such a function.) For example:
Prove that the sets below have equal cardinality: $\lbrace f:\mathbb{N} \rightarrow \lbrace 0,1\rbrace\rbrace, 
\text{and}\ 
\lbrace f:\mathbb{N} \to \lbrace 0,1\rbrace, \text{such that there's no}\ f(i) = f(i+1) = 0\rbrace$ What can I do in order to understand better? I'll be glad to hear what your experiences with this kind of problem are (function building).","['functions', 'set-theory']"
4611052,"In the group $G=\langle r,s,t\mid r^2=s^3=t^3=rst\rangle,$ the element $rst$ has order $2$","Formally, if $F$ is the free group with basis $X = \{r, s, t\}$ and $N$ is the normal subgroup generated by $R = \{r^2 s^{-3}, s^3 t^{-3}, t^{3} (rst)^{-1}\}$ , and $G = F/N$ , I want to show that the coset of $rst$ has order $2$ in $G$ . There are two parts to this: showing $rstrst = 1$ and showing $rst \neq 1$ . There must be some way to play with and combine the relations in just the right way to get $rstrst = 1$ , but it seems difficult to get inverses to even appear in the right places and amounts to cancel all the $r, s, t$ of positive exponent. So far I know $r^2 = s^3 = t^3 = rst = str = trs$ , $r = st$ , $s^2 = tr$ , $t^2 = rs$ , and so I try at random some manipulations like $rstrst = t^2 s^2 r = (s^{-1}r)^2 (rt^{-1})^2 (t^2 s^{-1}) = \cdots$ or similar, which has just resulted in wandering about aimlessly. On the other hand, to show $rst \neq 1$ , I have previously shown that $G/\langle rst \rangle \cong A_4$ , so I could exhibit a property that is different between $G$ and $A_4$ to say that $rst \neq 1$ , but such a property might already rely on $rst$ being not $1$ to demonstrate, as the groups share all other defining relations. Proving $rst \neq 1$ directly by showing $rst \notin N$ in $F$ seems way too messy, since the elements of $N$ are words on the conjugates of $R$ .","['group-presentation', 'combinatorial-group-theory', 'abstract-algebra', 'free-groups', 'group-theory']"
4611054,Show positivity of a function of two variables in the unit square.,"Let $$
f(x,y) =  x^3 (1 + y + y^2) + y^2 \Big[x^2  (5 + 2 y) + x  (-6 - 4 y + y^2) +  (1 + 3 y + y^2)\Big] 
$$ Show that $f(x,y) \ge 0 $ for $0\le x \le 1$ and $0\le y \le 1$ . Numerical evaluations seem to support the claim.  This is a cubic function in $x$ with one negative coefficient in the linear term, all other coefficients are positive. Since $f(x=0) > 0$ , the last two terms guarantee positivity for $0 < x < 1/6$ . The last three terms show a quadratic function which has its minimum always for $x^* \in [0 \quad 1]$ ,  namely at $x^* = \frac{6 + 4 y - y^2}{10 + 4 y }$ ; however, for small $y$ , the value of the sum of the three last terms is negative at $x^*$ . How to continue?","['multivariable-calculus', 'polynomials', 'inequality']"
4611065,Are there reasons to prefer one definition of the exponential function over the other?,"This question is motivated by curiosity and  I haven't much background to exhibit . Going through a couple of books dealing with real analysis, I've noticed that 2 definitions can be given of the exponential function known in algebra as $f(x)= e^x$ . One definition says : The exponential function is the unique function defined on $\mathbb R$ such that $f(0)=1$ and $\forall (x) [   f'(x)= f(x)] $ . The other one defines the exponential function as the inverse of the natural logarithm function . More precisely $(1)$ $\exp_a (x)$ is defined as the inverse of $\log_a (x)$ , $(2)$ then , $\exp_a (x)$ is shown to be identical to $a^x$ , and finally $(3)$ every function of the form : $a^x$ is shown to be a "" special case"" of the $e^x$ function. My question : (1) Do these definitions exhaust the ways the exponential function can be defined? (2) Are these definitions actually different at least conceptually  ( though denoting in fact the same object)? (3) Is there a reason to prefer one definition over the other? What is each definition good for?","['calculus', 'definition', 'exponential-function', 'real-analysis']"
