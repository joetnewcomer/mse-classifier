question_id,title,body,tags
4514780,Is there a way to calculate the maximum length of the repeating pattern in a rational number?,"I am working on a project for school in which I would need to sum up hundreds of rational values to over one trillion sig figs (done in code) however it is not practical to store each term in the sequence as the file will become to large, however because all of the numbers are rational they are either finite, which is better, or they are infinite, but still rational so  they have to repeat, so my question is: is there a mathematical way to tell the length of the pattern, for example 1/14 is 0.0[714285][714285] with the [714285] bit repeating infinite times. any help would be much appreciated
Thank you in advance!!","['rational-numbers', 'pattern-recognition', 'sequences-and-series']"
4514812,How does this definition of a topology on $X$ not cause $X \in X$?,"From Wikipedia: Formally, let X be a set and let $\tau$ be a family of subsets of X. Then $\tau$ is called a topology on X if: Both the empty set and X are elements of $\tau$ . Any union of elements of τ is an element of $\tau$ . Any intersection of finitely many elements of τ is an element of $\tau$ . I'm confused about (1). If $\tau$ is a family of subsets of X, and X is a member/element of $\tau$ then doesn't that suggest that X is a member of itself, which generally speaking isn't allowed?","['elementary-set-theory', 'general-topology']"
4514821,Does $\frac{\sin(tx)}{\sin(x)}$ have a name?,"Does the following function have a name? $$\operatorname{boxySine}(t,x) = \begin{cases}
    \frac{\sin(tx)}{\sin(x)} & x \neq 0 \\
    t & \text{otherwise}
\end{cases}
$$ It appears in describing the position of a point on a circular segment in terms of a position on the chord. For a circular segment of central angle $\theta$ and chord length $c$ the arc (or line segment if $\theta = 0$ ) can be described in terms of $\operatorname{boxySine}$ as $$
t \in [-1,1]
$$ $$
\alpha = \frac{\theta}{2}
$$ $$
x = \frac{c}{2}\cdot\operatorname{boxySine}\left(t,\alpha\right)
$$ $$
y= -c\cdot\sin\left(\alpha\right)\left(\left(\operatorname{boxySine}\left(\frac{t}{2},\alpha\right)\cdot\cos\left(\frac{\alpha}{2}\right)\right)^{2}-\left(\cos\left(\frac{\alpha t}{2}\right)\cdot \operatorname{boxySine}\left(\frac{1}{2},\alpha\right)\right)^{2}\right)
$$ An alternative way to parameterize a circular arc or line segment without using $\operatorname{boxySine}$ would also be an appropriate answer, as it would explain why it might not have a name. Desmos graph demonstrating that parameterization of an arc I called it $\operatorname{boxySine}$ because for small values of $t$ the graph of it looks boxy.","['osculating-circle', 'circles', 'trigonometry', 'indeterminate-forms', 'algebra-precalculus']"
4514836,Asymptotic estimation of an integral,"I have an integral of the form $$
I = \int\limits_0^1\int\limits^{1}_{0} \exp\left(\dfrac{vt}{(u+v+1)^2 + v^2} - vt\right) dudv.
$$ I intend to prove $$
I\leq c t^{-1}
$$ holds for the sufficiently large $t$ , where $c$ is a positive constant independent of $t$ .
By the way, the numerical integration shows that $c$ exists and is less than $9$ .
Can anyone give me some hints or references to prove this estimation?","['integration', 'calculus', 'definite-integrals', 'analysis']"
4514839,"Can we solve $\frac{dx}{dt} = 3x+3t$ by ""multiplying by $dt$""?","In preparation for ODE, I was just doing some practice problems when I came across this. $$\frac{dx}{dt} = 3x+3t$$ Although I looked at the solution where it states to do an integration factor, is there anything inherently wrong with simply multiplying dt on both sides and then taking the integral of both sides and then isolating $x$ ? I would appreciate any clarity on this matter. Thanks so much.","['alternative-proof', 'ordinary-differential-equations']"
4514852,Functions with a best linear approximation,"Let $f : \mathbb{R}^2 \to \mathbb{R}$ .
Fix a point $x_0$ throughout. Assume that all directional derivatives of $f$ exist and they are linear with respect to the direction, meaning $v \to D_{v}$ is  a linear map where $D_v$ is the derivative IN the direction $v$ .
Geometrically we get a plane that is the 'best' linear approximation of $f$ at $x_0$ . Even then $f$ may not be differentiable at $x_0$ even if it is continuous.
example $f(x, y) = \frac{x^3y}{x^4 + y^2}$ and defined $0$ at $0$ and take $x_0 = 0$ . So, such functions which have a 'best linear approximation' are a larger class than differentiable functions. One can check that they are closed under addition and multiplication. Is this class interesting? Is this notion of weaker derivative studied in the literature, because the examples for such functions that I could find (as above are not pathological also). Also the best linear appoximation can be made rigorous as if there is a linear $D_2$ such that $|f(x_0+h) - f(x_0) - D(v)h| < |f(x_0+h) - f(x_0) - D_2(v)h|$ for all $h$ sufficiently small then $D_2 = D$ (the linear function returning the directional derivatives). I understand this quesition is a bit vague but any comments or insights are also most welcome.","['multivariable-calculus', 'calculus', 'analysis', 'reference-request']"
4514949,How to write that the number in a singleton is larger than some other number?,"Let $X$ and $Y$ be arbitrary sets with arbitrary elements $x\in X$ and $y\in Y$ . Let $Y'\subset Y$ . Further, let $f:X\times Y\to\mathbb{R}$ be some function and let $f(x,Y')=\{f(x,y)\mid y\in Y'\}\subset\mathbb{R}$ . Thus, $f(x,Y')$ is a set of Real numbers. Then, if $f(x,Y')$ is the singleton containing just $z\in\mathbb{R}$ , we can simply write $f(x,Y')=\{z\}$ . If $f(x,Y')$ is a singleton whose unique element is equal or greater than $z\in\mathbb{R}$ , I have thought of writing $f(x,Y')\geqslant\{z\}$ . Unfortunately, I think my notation is wrong, because (unlike the relation $=$ ) the relation $\geqslant$ is usually only defined for numbers rather than sets of numbers (even if those sets are singletons!). I have thought of using the expressions $\min_{y\in Y'}\{f(x,y)\}\geqslant z$ or $\min\{f(x,y)\mid y\in Y'\}\geqslant z$ , but that seems too cumbersome for such a simple idea. Although, perhaps, the best is simply to write $f(x,y)\geqslant z$ for all $y\in Y'$ , isn't it? What would you do? Any help will be much appreciated.","['elementary-set-theory', 'notation']"
4515004,What is the formal definition of a continuous function?,"Thomas' Calculus 14th Edition gives the following definition of a continuous function: We define a continuous function to be one that is continuous at every
point in its domain. As an example, it declares the function f(x)=1/x a continuous function: The function ƒ(x) = 1/x (Figure 2.41) is a continuous function because
it is continuous at every point of its domain. The point x = 0 is
not in the domain of the function ƒ, so ƒ is not continuous on any
interval containing x = 0. Moreover, there is no way to extend ƒ to a
new function that is defined and continuous at x = 0. The function ƒ
does not have a removable discontinuity at x = 0. On the other hand, this document I found on the MIT Math portal, has this to say about that same function 1/x: The function 1/x is continuous on (0, ∞) and on (−∞, 0), i.e., for x >
0 and for x < 0, in other words, at every point in its domain.
However, it is not a continuous function since its domain is not an
interval. It has a single point of discontinuity, namely x = 0, and it
has an infinite discontinuity there. Unless I'm misreading something here, these two sources are in direct contradiction with each other. So my questions are: is f(x)=1/x a continuous or discontinuous function, and what is the generally accepted formal definition of an overall continuous function?","['limits', 'calculus', 'functions']"
4515061,Smallest algebra containing a function,"The question Let $f: X \to \Bbb C$ , then what is the smallest algebra $\mathcal{A}$ containing $f$ ? My attempt I claim that: $$\mathcal{A} = \{\sum \limits_{i=1}^{n} a_i f^i : [n \in \Bbb N, a_i \in \Bbb C]\}$$ In fact, it is clear that the set thus defined is an algebra (closure under addition and multiplication by a scalar can be proved directly while closure under multiplication may be proved considering two sums of degrees $m$ and $n$ and by proceeding by induction on $n$ , after having fixed $m$ ). Moreover, every function in $\mathcal{A}$ can be constructed using only $f$ , therefore every algebra containing $f$ must also include $\mathcal{A}$ . I would like to know if this answer is correct and satisfactory. As always, any comment or answer is welcome and let me know if I can explain myself clearer!","['functions', 'solution-verification', 'real-analysis']"
4515064,Examples of surjective sheaf morphisms which are not surjective on sections,"Let $X$ be a topological space, and let $\mathscr{F}, \mathscr{G}$ be sheaves of sets on $X$. It is well-known that a morphism $\varphi : \mathscr{F} \to \mathscr{G}$ is epic (in the category of sheaves on $X$) if and only if the induced map of stalks $\varphi_P : \mathscr{F}_P \to \mathscr{G}_P$ is surjective for every point $P$ in $X$, but the section maps $\varphi_U : \mathscr{F}(U) \to \mathscr{G}(U)$ need not be surjective. I know of a couple of examples from complex analysis: Let $X$ be the punctured complex plane, $\mathscr{F}$ the sheaf of meromorphic functions, $\mathscr{G}$ the sheaf of differential $1$-forms, and $\varphi$ the differential map; then $\varphi$ is epic and indeed the sequence $0 \to \mathscr{F} \to \mathscr{G} \to 0$ is even exact, but there are global sections of $\mathscr{G}$ which are not the image of a global section of $\mathscr{F}$, e.g. $z \mapsto \frac{1}{z} \, \mathrm{d}z$. Let $X$ be the punctured complex plane again, $\mathscr{F}$ the sheaf of meromorphic functions, $\mathscr{G}$ the sheaf of nowhere-zero meromorphic functions, and let $\varphi : \mathscr{F} \to \mathscr{G}$ be composition with $\exp : \mathbb{C} \to \mathbb{C}$; then $\varphi$ is epic but again fails to be surjective on (global) sections: after all, there is no holomorphic function $f : X \to \mathbb{C}$ such that $\exp f(z) = z$ for all non-zero $z$. Question. Are there simpler examples which do not require much background knowledge beyond knowing the definition of sheaves and stalks?",['sheaf-theory']
4515073,Why is the volume of a parallelogram $P$ formed from the unit square $I^2$ with an integer matrix $A$ equal to the number of integer points in $P$?,"I am aware of this existing post: Number of fixed points of a hyperbolic toral automorphism and its accepted answer: https://math.stackexchange.com/a/3371094/820472 . Unfortunately the last two proposed methods do not seem obvious to me and seem a bit disconnected ( note: what is and isn't connected is very subjective; here by connected I am referring to something that a first year undergraduate student would get after basic linear algebra). And then, while I like the method (Pick's formula) behind the first proposed solution it seems to assume a priori a relationship between the determinant of $A - I$ and the number of integer lattice points in a parallelogram. Question: My question is: Given an integer matrix $A$ such that $A - I$ is invertible, how do you justify that the number of integer points in a parallelogram spanned by $(A - I)(0, 1)$ and $(A - I)(1, 0)$ is equal to $\mathrm{det}(A - I)$ . Bonus question: Do we need that $A$ is an integer matrix? If it is the case, what do we know about a general real matrix $A$ ? Like a lower/upper bound for such integer points? Thanks for the help!","['determinant', 'integer-lattices', 'matrices', 'linear-algebra', 'quotient-spaces']"
4515091,A question about derivation of kalman gain,"I read the derivation of Kalman gain. In the derivation, we have: $$(\mathbf{H}\mathbf{P_{n,n-1}})^T = \mathbf{K_n}(\mathbf{H}\mathbf{P_{n,n-1}}\mathbf{H}^T+\mathbf{R_n})$$ where $\mathbf{H}$ is observation matrix, $\mathbf{P_{n,n-1}}$ is the predicted estimate uncertainty, $\mathbf{R_n}$ is the measurement uncertainty, and $\mathbf{K_n}$ is our desired Kalman gain. Then, $$\mathbf{K_n} = (\mathbf{H}\mathbf{P_{n,n-1}})^T(\mathbf{H}\mathbf{P_{n,n-1}}\mathbf{H}^T+\mathbf{R_n})^{-1}$$ My question is why $\mathbf{H}\mathbf{P_{n,n-1}}\mathbf{H}^T+\mathbf{R_n}$ must have inverse?","['statistics', 'kalman-filter', 'optimal-control', 'matrices', 'linear-algebra']"
4515098,Continuity of specific bivariate function,"Let $f:\Bbb R^2 \to \Bbb R$ be defined by the argument-value description: $$f(x, y) = (x+1)^y $$ then I should be able to show that $f$ is continuous on the unit square $[0,1]^2$ . However, so far, I did not manage to do it. I tried using the $\epsilon - \delta$ definition but it becomes very difficult very quickly. Therefore, I would like to ask if there is any easier way of showing it. As always any comment or answer is welcome and let me know if I can explain myself clearer!","['multivariable-calculus', 'calculus', 'continuity', 'real-analysis']"
4515117,"What are some other ways in which a parabola is ""between an ellipse and a hyperbola""?","What are some other ways in which a parabola is ""between an ellipse and a hyperbola""? On page 122 of Gilbert Strangs calculus text he writes:
""Throughout mathematics, parabolas are on the border between ellipses and hyperbolas."" Here are three ways in which we can think of a parabola as being inbetween an ellipse and a hyperbola: If we cut a cone with a horizontal plane, we get a circle. When we tilt the plane slightly, we get an ellipse. If we tilt the plane a lot, we get a hyperbola. When we tilt the plane so that its angle matches the slope of the cone, we get a parabola. The equation $Ax^2+Bxy+Cy^2=1$ produces a hyperbola if $B^2 > 4ac$ , an ellipse if $B^2-4AC <0$ and a parabola if $B^2-4ac=0$ In the polar form $r=\frac\ell{1+e\cos\theta}$ , we pass smoothly through a parabola when the eccentricity $e$ passes through $1$ with a fixed semi latus rectum $\ell$ . I'm suspecting there is at least one more way to understand why we think of a parabola as being inbetween an ellipse and a hyperbola, perhaps in terms of foci. Strang writes that the ""second foci of a parabola"" is located at ininfinity, and I'm not quite sure why this type of thinking makes sense and if we can somehow relate this foci at infinity to being an inbetween case of defining ellipses and hyperbola by their foci.","['euclidean-geometry', 'analytic-geometry', 'conic-sections', 'geometry']"
4515121,The meaning of 'definition' and usage of $:=$,"I'm aware of the idea of $:=$ as 'definition, similar to an equality',  but I'm confused about how it's used, and whether we mean to define the expression's value for all conditions. What do we mean by definition? If I define $y:=x^2,$ this makes sense, but what about $$x^2+y^2:=1\;?$$ What am I defining here? How does my 'definition' mean differently from simply stating an equality? How about $$\sin(x)^2+\cos(x)^2:=1\; ?$$ This always has the value of $1$ for any $x$ so how do I 'define' anything on this, it's either true or false, it's not my definition that makes it true, it is true always. How do we find truth values for statements given with ' $:=$ ', and what if I make a false equality for all $x$ like $$3x:=x+x$$ how do I 'define' something that can't be true? ADDENDUM I'm slightly struggling with these replies, because of my lack of understanding of what 'definition' means. Writing $x^2+y^2:=1$ , which you may technically do, is defining the entire symbol $x^2+y^2$ to be 1. The individual parts of the symbol, $x^2$ , + and $y^2$ might have no additional meaning. and You are defining something to be something else. If I define a twinkie to be something, any instance of the twinkie is that something. The cream filling or any other knowledge of a similar looking object mean nothing. I could have replaced the $x^2+y^2$ with a smiley face with $x^2$ and $y^2$ substituted for its eyes. It's just a symbol. What does this mean in terms of 'definition'? Surely we can define a relation (for example the unit circle) without having to somehow verge away from the expression having it's usual mathematical meaning? Is it ok to make an operation over some definition at the very first time you are defining it? This question seems to suggest that for some people, using the definition symbol suggests that what is on the left feels like 'some symbol' instead of the mathematical expression it should define.","['logic', 'notation', 'definition', 'algebra-precalculus', 'terminology']"
4515138,Graph Transformation,"Knowing dealing with graph transformations come handy MANY times. I searched on google to get a comprehensive graph transformation list but couldn't find one. Some good while back I learned them all with great enthusiasm but have now all forgotten them and like before, I'm not finding a great teacher neither a great search result. Can we have one here that will cover the following (at least): $$\text {For }y=f(x):$$ $y=f(x+c)$ $y=f(x)+c$ $y=f(cx)$ $y=cf(x)$ $y=f(-x)$ $y=-f(x)$ $y=f(|x|)$ $y=|f(x)|$ $y+c=f(x)$ $cy=f(x)$ $-y=f(x)$ $|y|=f(x)$ Kindly feel to provide for more in case I have missed out any thing. Thank you :) Note : Last few equations I know might be a repeat of the above cases but I have included them so as to clear any confusions.","['algebra-precalculus', 'functions', 'big-list', 'graphing-functions']"
4515147,Confusion on morphisms to projective spaces,"Hartshorne's theorem II.7.1 claims that if $\phi : X\rightarrow \mathbf{P}^n_k$ is a morphism of a $k$ -scheme into projective space, then $ \phi^*(\mathcal{O}(1)) $ is an invertible sheaf generated by global sections $s_i = \phi^*(x_i).$ Vakil's book, Theorem 16.4.1, claims that morphisms into projective space are in bijection with the data of (up to isomorphism) a line bundle on $X$ and $n+1$ global sections with no common zeros. This seems incompatible with Hartshorne's theorem--shouldn't Vakil require that that the $n+1$ global sections generate the line bundle? Indeed, Vakil's proof claims that if $\phi$ is the morphism determined by the line bundle $\mathcal{L},$ then $\mathcal{L} \cong \phi^*\mathcal{O}(1).$ I do not think Vakil properly proves this if we do not know that $s_i = \phi^*(x_i)$$ generate the global sections. But let's assume Vakil's proof is right. Wouldn't this immediately imply the line bundle is generated by global sections, since a sheaf and its pullback have isomorphic germs, isomorphic global sections, and the isomorphisms are compatible with the global sections to germs restrictions? It thus seems that Vakil proves a much stronger claim that any line bundle with $n+1$ global sections with no common zeroes is in fact generated by those global sections. This claim seems false. Question: Is a morphism into projective space equivalent to the data of a line bundle and $n+1$ sections generating the bundle, or just $n+1$ sections with no common zeroes? (It seems to me that this is what happens: given a line bundle and $n+1$ sections with no common zeroes, I can give a morphism into projective space. But the pullback of $\mathcal{O}(1)$ under that morphism is not always my original line bundle, but is instead some subbundle generated by the $n+1$ global sections.)",['algebraic-geometry']
4515157,Equivalence of additivity and homogeneity for $\mathbb{R} \rightarrow \mathbb{R}$ strictly increasing functions,"In my (sort of) intro to proofs course, we were given the following theorem without proof (which was then used in other proofs): If $f : \mathbb{R} \rightarrow \mathbb{R}$ is a strictly increasing function, the following statements are equivalent: (i) $\forall n \in \mathbb{Z} : \forall x \in \mathbb{R} : f(nx) = nf(x)$ (ii) $\forall x \in \mathbb{R} : f(x) = ax$ , where $a = f(1) > 0$ * (iii) $\forall x_1, x_2 \in \mathbb{R} : f(x_1 + x_2) = f(x_1) + f(x_2)$ *Since f goes from $\mathbb{R}$ to $\mathbb{R}$ , I believe (ii) is equivalent to degree 1 homogeneity. I managed to show two implications: (ii) $\implies$ (iii) $f(x_1 + x_2) = a(x_1 + x_2) = ax_1 + ax_2 = f(x_1) + f(x_2)$ (iii) $\implies$ (i) if $n \in \mathbb{Z}$ and $n > 0$ : $f(nx) = f(x + ... + x)$ [ $n$ times] $= f(x) + ... + f(x)$ [ $n$ times] $= nf(x)$ This is easy to extend to $n <= 0$ if you observe that additivity implies $f(0) = 0$ , because if it didn't, $f(x) = f(x + 0) = f(x) + f(0) \neq f(x)$ $f(-x) = -f(x)$ , since $f(x) + f(-x) = f(x-x) = f(0) = 0$ Now I need help figuring out how to show that (i) $\implies$ (ii). I'm sure I could use (i) to prove that $\forall q \in \mathbb{Q} : f(qx) = qf(x)$ , but I have no idea how to extend that to the reals. I suppose that's where the fact $f$ is strictly increasing becomes relevant? Thanks in advance!",['functions']
4515164,How far can we go with $\int_{0}^{\infty} \frac{\ln \left(1+x^n\right)}{1+x^{2}} dx \textrm{ where }n\in N ?$,"Latest Edit Great thanks to @Quanto for giving us the complete solution to ALL even powers using two wonderful factorizations \begin{align}
&1+x^{4m}= \prod_{k=1}^m \left(1+2x^2\cos\frac{(2k-1)\pi}{2m}+x^4 \right)\\
& 1+x^{4m+2}= (1+x^2)\prod_{k=1}^m \left(1+2x^2\cos\frac{2k\pi}{2m+1}+x^4\right)
\end{align} and made use of the post that $$\int_0^\infty \frac{\ln(1+2x^2\cos \theta +x^4)}{1+x^2}dx
=2\pi \ln\left(2\cos\frac{\theta}4\right)
$$ to build up two decent formula for ALL even powers as below: $$\begin{align}
&\int_0^\infty \frac{\ln(1+x^{4m})}{1+x^2}dx
=2\pi \ln \bigg( 2^m \prod_{k=1}^m \cos\frac{(2k-1)\pi}{8m}\bigg)\
\\
&\int_0^\infty \frac{\ln(1+x^{4m+2})}{1+x^2}dx
= \pi\ln2 + 2\pi \ln \bigg( 2^m \prod_{k=1}^m \cos\frac{k\pi}{2(2m+1)}\bigg)
\end{align}$$ which can be merged into a single formula below: $$
\boxed{\int_0^\infty \frac{\ln(1+x^{2n})}{1+x^2}dx =\frac{1-(-1)^n}{2} \pi \ln 2+2 \pi \ln \left[2^{\left[\frac{n}{2}\right]} \prod_{k=1}^{\left[\frac{n}{2}\right]}\left(\cos \frac{(n-2 k+1) \pi}{4 n}\right)\right]}
$$ where $n>1.$ Though the nut for odd powers is much harder, @J.G had cracked the nut partially by giving it a closed form with double sum. $$\boxed{\int_0^\infty\frac{\ln(1+x^n)}{1+x^2}dx=\frac{\pi}{4}\ln2+G+\sum_{k=0}^{(n-3)/2}\left(\pi\ln\left|2\sin\frac{\pi(2k+1)}{2n}\right|+\frac{\pi(n-4k-2)}{2n}\ln\left|\tan\frac{\pi(2k+1)}{2n}\right|+2\sum_{j\ge0}\frac{(-1)^{j+1}\cos\frac{\pi(2j+1)(2k+1)}{n}}{(2j+1)^2}\right)}$$ Would you like to try to simplify the sum over $j,\,k$ ? Backgound Couple months ago, I met the integrals $$
\int_{0}^{\infty} \frac{\ln \left(x\right)}{1+x^{2}} dx \stackrel{x\mapsto\frac{1}{x}}{=} -\int_{0}^{\infty} \frac{\ln \left(x\right)}{1+x^{2}} d x \Rightarrow \int_{0}^{\infty} \frac{\ln \left(x\right)}{1+x^{2}} dx =0 \tag*{}
$$ $$\textrm{  and}$$ $$
I(0)=\int_{0}^{\infty} \frac{\ln 2}{1+x^{2}} d x=\frac{\pi}{2} \ln 2
$$ Then I started to investigate, in my post 1 , $$I(1)= \int_{0}^{\infty} \frac{\ln (1+x)}{1+x^{2}}dx=  \frac{\pi}{4} \ln 2+G $$ Next integral $$I(2)=\int_{0}^{\infty} \frac{\ln \left(1+x^2\right)}{1+x^{2}} dx \stackrel{x\mapsto\tan {\theta}}{=} -2 \int_{0}^{\frac{\pi}{2}} \ln (\cos \theta) d \theta=\pi \ln 2 $$ The third one is split into two integrals, first of which refer to my post 2 , $$\begin{aligned} I(3)&=\int_{0}^{\infty} \frac{\ln \left(1+x^{3}\right)}{1+x^{2}} d x\\&= \underbrace{\frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}}}_{\frac{2 \pi}{3} \ln (2+\sqrt{3})-\frac{4}{3} G} d x+\underbrace{\int_{0}^{\infty} \frac{\ln (1+x)}{1+x^{2}}}_{\frac{\pi}{4} \ln 2+G} d x\\ &\boxed{\int_{0}^{\infty} \frac{\ln \left(1+x^{3}\right)}{1+x^{2}} d x =-\frac{G}{3}+\frac{\pi}{4} \ln 2 +\frac{2 \pi}{3}\ln(2+\sqrt{3})}\end{aligned}
$$ The fourth one comes immediately after the my post 3 which states that $$J(a)=\int_{0}^{\infty} \frac{\ln \left(1+2 x \sin a+x^{2}\right)}{1+x^{2}} d x =\pi\ln \left|2 \cos \frac{a}{2}\right|+|a| \ln \left|\tan \frac{a}{2}\right|-2 \operatorname{sgn} (a) \int_{0}^{\frac{|a|}{2}} \ln (\tan x) d x$$ $$
\begin{aligned}
I(4)&=\int_{0}^{\infty} \frac{\ln \left(1+x^{4}\right)}{1+x^{2}} d x\\&=\int_{0}^{\infty} \frac{\ln \left[\left(x^{2}+\sqrt{2} x+1\right)\left(x^{2}-\sqrt{2} x+1\right)\right]}{1+x^{2}} \\
&=J\left(\frac{\pi}{4}\right)+J\left(-\frac{\pi}{4}\right)\\& =2 \pi \ln \left(2 \cos \frac{\pi}{8}\right)\\& \boxed{\int_{0}^{\infty} \frac{\ln \left(1+x^{4}\right)}{1+x^{2}} d x =\pi \ln (2+\sqrt{2})}
\end{aligned}
$$ The harder one comes from the post 4 by @Quanto. $$ \begin{aligned}I(5)&=\int_0^1\frac{\ln(1+x^5)}{1+x^2}dx\\
&\boxed{= \frac15G -\frac{19\pi }{20}\ln 2+\frac{3\pi }{5}\ln 5+\frac{4\pi}5 \ln \left(1+\sqrt{1+\frac{2}{\sqrt{5}}}\right)+\frac{8\pi}5\ln \left(1+\sqrt{1-\frac{2}{\sqrt{5}}}\right)}\end{aligned}
$$ When the power of $x$ is raised to the power $6$ , I first split the integral into 2. $$\int_0^1\frac{\ln(1+x^6)}{1+x^2}dx=  \underbrace{\int_{0}^{\infty} \frac{\ln \left(x^{2}+1\right)}{1+x^{2}} d x}_{=\pi \ln 2}+  \underbrace{\int_{0}^{\infty} \frac{\ln \left(x^{4}-x^{2}+1\right)}{1+x^{2}} d x
}_{K} $$ For integral $K$ , we use the formula founded in my post 3 stating that $$J(a)=\int_{0}^{\infty} \frac{\ln \left(1+2 x \sin a+x^{2}\right)}{1+x^{2}} d x =\pi\ln \left|2 \cos \frac{a}{2}\right|+|a| \ln \left|\tan \frac{a}{2}\right|-2 \operatorname{sgn} (a) \int_{0}^{\frac{|a|}{2}} \ln (\tan x) d x$$ Then $$
\begin{aligned}
K &=\int_{0}^{\infty} \frac{\ln \left(x^{4}-x^{2}+1\right)}{1+x^{2}} d x \\
&=\int_{0}^{\infty} \frac{\ln \left(x^{2}+\sqrt{3} x+1\right)}{1+x^{2}} d x+\int_{0}^{\infty} \frac{\ln \left(x^{2}-\sqrt{3} x+1 \right) }{1+x^{2}} dx\\
&=J\left(\frac{\pi}{3}\right)+J\left(-\frac{\pi}{3}\right) \\
&=2 \pi \ln \left(2 \cos \frac{\pi}{6}\right) \\
&=\pi \ln 3
\end{aligned}
$$ $$
\boxed{ \int_{0}^{\infty} \frac{\ln \left(1+x^{6}\right)}{1+x^{2}} d x=\pi \ln 2+\pi \ln 3=\pi \ln 6 }
$$ From my recent post , $$
\boxed{\int_{0}^{\infty} \frac{\ln \left(x^{4}+b x^{2}+1\right)}{1+x^{2}} d x = 2 \pi \ln \left[2 \cos \left(\frac{1}{4} \cos ^{-1}\left(\frac{b}{2}\right)\right)\right]}
$$ $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln \left(x^{8}+1\right)}{1+x^{2}}=& \int_{0}^{\infty} \frac{\ln \left(x^{4}+\sqrt{2} x+1\right)}{1+x^{2}} d x +\int_{0}^{\infty} \frac{\ln \left(x^{4}-\sqrt{2} x+1\right)}{1+x^{2}} d x \\
=& 2 \pi \ln \left[2 \cos \frac{1}{4} \cos ^{-1}\left(\frac{\sqrt{2}}{2}\right)\right] +2 \pi \ln \left[2 \cos \frac{1}{4} \cos ^{-1}\left(-\frac{\sqrt{2}}{2}\right)\right] \\
=& 2 \pi \ln \left(2 \cos \frac{\pi}{16}\right)+2 \pi \ln \left(2 \cos \frac{3 \pi}{16}\right)\\
=& 2 \pi \ln \left(2^2 \cos \frac{\pi}{16} \cos \frac{3 \pi}{16}\right)\\=& 2 \pi \ln (\sqrt{2}+\sqrt{2+\sqrt{2}})
\end{aligned}
$$ Through the investigation, I found that it is very interesting to find their exact values as most of them are unexpectedly simple and decent except $I(5)$ . We can foresee the difficulty of the evaluation will be increased. My question is whether we can go further for $n=7$ and $n\geq 9.$ Your help and contributions will be highly appreciated.","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'trigonometric-integrals']"
4515175,"If graph has at least $\lfloor n^2/4\rfloor +1$ edges, then some edge appears in at least $(1/6-o(1))n$ triangles","Prove that every $n$ -vertex graph with at least $\lfloor
 n^2/4\rfloor+1$ edges contains some edge in at least $(1/6-o(1))n$ triangles, and that this constant $1/6$ is best possible. I have been thinking on this problem for a while and the following idea came to my mind. It is basically averaging argument since we need to show an existence of some edge which appears in $(1/6-o(1))n$ triangles. My attempt: For any edge $uv\in E(G)$ consider the following set $N(u)\cap N(v)=\{x\in V(G): xu,xv\in E(G)\}$ , i.e. this set represents all triangles where $uv$ is one of the sides. Let's apply averaging argument on the size of $N(u)\cap N(v)$ , i.e. consider the sum $$\sum \limits_{uv\in E(G)}|N(u)\cap N(v)|=\sum \limits_{uv\in E(G)}|\{x\in V(G): xu,xv\in E(G)\}|=$$ $$=\frac{1}{2}|\{(u,v,x)\in V^3: uv,xu,xv\in E(G)\}|=3|\Delta_G|,$$ where $|\Delta_G|$ is the number of triangles in graph $G$ . Here I am using the non-trivial fact that if $e(G)\geq \lfloor n^2/4\rfloor+1$ , then $|\Delta_G|\geq \lfloor
 n/2\rfloor$ .
Therefore, $$\frac{1}{e(G)}\sum \limits_{uv\in E(G)}|N(u)\cap N(v)|\geq \frac{3|\Delta_G|}{e(G)}\geq \frac{3\lfloor n/2 \rfloor}{e(G)}.$$ Hence there is an edge $uv\in E(G)$ such that $|N(u)\cap N(v)|\geq \frac{3\lfloor n/2\rfloor}{e(G)}$ . But of course it does not give the desired result. I was wondering is it possible to modify this method to solve the problem?","['graph-theory', 'combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4515202,Pointwise continuity of dominated non-negative function,"Let $M$ be a topological space and $f, g : M\rightarrow\mathbb{R}_+$ be two non-negative functions with $f(x_0)=g(x_0)=0$ for some $x_0\in M$ . Suppose further that $f(x)\leq g(x)$ for each $x\in M$ , and that $g$ is continuous at $x_0$ . I was wondering if this is sufficient to conclude that $f$ must also be continuous at $x_0$ ? Clearly this holds if $M$ is a metric space, but is it also true for general topological spaces (the topology on $\mathbb{R}_+$ shall always be the Euclidean (subspace) topology)?","['continuity', 'general-topology', 'functions', 'real-analysis']"
4515204,The Coin Problem (Probability),"Suppose there is a square room of side $d$ and we draw on the floor a circumference of radius $\frac{1}{5}d$ whose center concur with the center of the square room. If we throw a coin of radius $\frac{1}{10}d$ randomly, which is the probability that the coin doesn't touch the circumference? We are interested in the following event: $E$ : ""Coin doesn't touch the circumference"" The visual representation of the problem is the following: NOTE 1: There are some points of the floor that can not be the center of the coin because the walls of the room can not be traversed. All this points are ubicated in the pink region of the next picture. (Consequently, the pink region isn't part of the sample space) Consequently, we are going to determine which points of the square of side $\frac{8}{10}d$ verify this property: If the coin has its center in the green region, then it does not touch the circumference of radius $\frac{1}{5}d$ NOTE 2: All the points of the green region verify the previous condition. We are going to calculate the area of the square of side $\frac{8}{10}d$ and the green region: $A_{square} = (\frac{8}{10}d)^2 = \frac{64}{100}d^2 $ $A_{green} = \pi(\frac{1}{10}d)^2+[(\frac{8}{10}d)^2-\pi(\frac{3}{10}d)^2] = d^2(\frac{\pi}{100}+\frac{64}{100}-\frac{9\pi}{100})= (\frac{64-8\pi}{100})d^2$ Therefore, the probability of the event $E$ is the following: $P(E)=\frac{A_{green}}{A_{square}} = \frac{\frac{64-8\pi}{100}d^2}{\frac{64}{100}d^2} = \frac{64-8\pi}{64} = \frac{8-\pi}{8} = 0.6073009183... $ Is this reasoning correct? I'm not 100% sure.","['geometric-probability', 'probability-theory', 'probability']"
4515216,An equality about an infimum over a vector: $\inf_y = \inf_{\|v\|=1}\inf_t$,"I am reading convex optimization lecture notes , where I saw the equality as a part of the derivation: Here $f$ is a function and $\nabla f$ is the gradient. What seems to happen here is that the author replaces $(y-z) = t v$ with $\|v \| = 1$ and converts infimum over $y$ to double infimum (is this trivial? i.e. infimum over multiplication equals taking a double infimum?). What confuses me further here is, with that, now $z$ depends on $v$ and I don't understand how this works, or I'm overthinking. Could anyone provide a proof for this? Is this a well-known technique?","['convex-optimization', 'multivariable-calculus', 'supremum-and-infimum']"
4515218,Number of Critical Point of $|x-1|+|x|$,"Let $f(x) =|x-1|+|x|$ . Find the number of critical points of the function in its domain. According to me the definition of critical point is the point on the function where either $f'(x) =0$ or $f'(x) $ does not exist. In the interval $(-\infty, 0) $ $$f(x) =1-2x$$ $$f'(x) =-2 \neq 0$$ Similarly in interval $(1, \infty) $ $$f(x) =2x-1$$ $$f'(x) =2 \neq 0$$ The problem comes in the interval $[0, 1]$ . Firstly is it trivial the the derivative of the function does not exist at points $\{0, 1\}$ In the interval (0, 1) $$f(x) =1$$ $$f'(x) =0 \forall x \in (0, 1) $$ So the critical points to this function all reals in the interval $[0, 1]$ . Hence the function has an infinite number of critical points. But the answer to this problem is given as $2$ . Why is this happening? Is my definition  of critical points incomplete or wrong? Or maybe the answer to the problem is given wrong.","['calculus', 'derivatives', 'real-analysis']"
4515222,Product topology coarser than weak topology,"Let $\Gamma$ be a nonempty set and define the real vector space $$
c_{00}(\Gamma):=\{f: \Gamma\to \mathbf{R} \text{ finitely supported}\}.
$$ Now, let $V$ be a vector subspace of the algebraic dual $\mathbf{R}^\Gamma$ and suppose that $$
(c_{00}(\Gamma), V)
$$ is a dual pair (that is, a pair of vector spaces equipped with a bilinear map $\langle \cdot, \cdot \rangle: c_{00}(\Gamma)\times V\to \mathbf{R}$ such that both families of sections separate the points). Here, the duality map is $\langle x,y\rangle:=\sum_\gamma x(\gamma)y(\gamma)$ . Question. Is it true that the product topology $\tau$ on $c_{00}(\Gamma)$ is contained in the weak topology $\sigma(c_{00}(\Gamma), V)$ ? The answer is affirmative in all examples I have in mind. Attempt. Let $U$ be a basic $\tau$ -neighborhood of zero, so that there exist $n\ge 1$ , $\gamma_1,\ldots,\gamma_n \in \Gamma$ , and $\varepsilon>0$ such that $$
U:=\left\{x \in c_{00}(\Gamma): |x(\gamma_i)|\le \varepsilon \text{ for all } i=1,\ldots,n\right\}.
$$ Then, we have to show that $U$ is a weak neighborhood, that is, there exist $v_1,\ldots,v_k \in V$ and $\varepsilon_1,\ldots,\varepsilon_k>0$ such that $$
\left\{x \in c_{00}(\Gamma): \left|\sum_\gamma x(\gamma)v_j(\gamma)\right|\le \varepsilon_j \text{ for all } j=1,\ldots,k\right\}\subseteq U.
$$ How to do it? If the answer is affirmative, it should be related to the fact that $c_{00}(\Gamma)$ separates the points of $V$ . Ps. As it follows from Corollary 5.108 in ""Infinite dimensional analysis"" (Aliprantis and Border, 2006), $V$ separates the points of $c_{00}(\Gamma)$ if and only if $V$ is weak $^\star$ -dense in the algebraic dual of $c_{00}(\Gamma)$ . Added (failed) attempt. Denote by $\delta_{\gamma}$ the $\{0,1\}$ -valued function $\Gamma\to \mathbf{R}$ such that $\delta_\gamma(z)=1$ iff $z=\gamma$ .
Now, fix $U$ as above and a nonzero finitely supported sequence $x \in c_{00}(\Gamma)$ with $$
\mathrm{supp}(x)\subseteq \{\gamma_1,\ldots,\gamma_n\}.
$$ Since $V$ is weak $^\star$ -dense in the algebraic dual of $c_{00}(\Gamma)$ , for each $i=1,\ldots,n$ there exist $v_i \in V$ such that $$
\forall j=1,\ldots,n,\quad \left|\delta_{\gamma_i}(\gamma_j)-v_i(\gamma_j)\right|<\color{red}{\min\left\{\frac{\varepsilon}{4\sum_\gamma |x(\gamma)|}, \frac{1}{2}\right\}.}
$$ To conclude, fix $k=n$ , and suppose $x$ satisfies $$
\forall j=1,\ldots,n, \quad \left|\sum_\gamma x(\gamma)v_j(\gamma)\right|\le \varepsilon_j:=\frac{\varepsilon}{4}.
$$ Fix $j=i$ in the above inequality.
Then $$
\frac{\varepsilon}{4} \ge \left|x(\gamma_i)v_i(\gamma_i)\right|-\left|\sum_{\gamma\neq \gamma_i}x(\gamma)v_i(\gamma) \right|=
\left|x(\gamma_i)v_i(\gamma_i)\right|-\left|\sum_{j\neq i}x(\gamma_j)v_i(\gamma_j) \right|
$$ $$
> |x(\gamma_i)| \left(1-\frac{1}{2}\right)-\sum_{j\neq i}|x(\gamma_j)|\cdot \frac{\varepsilon}{4\sum_\gamma |x(\gamma)|}
\ge \frac{|x(\gamma_i)|}{2}-\frac{\varepsilon}{4}.
$$ This implies that $|x(\gamma_i)|<\varepsilon$ . By the arbitrariness of $x$ , we conclude that $x \in U$ . (The mistake here is that the red number should be defined independently of the sequence $x$ .)","['general-topology', 'functional-analysis', 'weak-topology']"
4515231,Show that for each $k \in \mathbb N$ with $k \geq 4$ there exists a prime number $p$ such that $\Omega (N_p) = k.$,"For $N \in \mathbb N,$ let $\Omega (N)$ denote the number of prime factors of $N$ counting multiplicities. For a prime number $p,$ let $N_p = p (p + 2) (p + 4).$ Show that for any $k \in \mathbb N$ with $k \geq 4$ there exists a prime number $p$ such that $\Omega (N_p) = k.$ What I can see is that $N_p$ has at least four prime factors (counting multiplicities) for $p \gt 3.$ But I don't see how it helps in solving the problem. Any help in this regard would be warmly appreciated. Thanks for your time.","['contest-math', 'number-theory', 'elementary-number-theory']"
4515234,How to calculate permutations with limited repetitions,"I have $246$ appointments, that can be scheduled in only $10$ timeslots.
However, $5$ time slots can take up to $30$ ( $s_1$ ) appointments, and the other $5$ can take up to $20$ ( $s_2$ ) appointments. How to calculate the Permutations with limited repetitions of this problem: $$n=10, r=246, s_1=30\text{ and }s_2=20$$ What if we simplified the problem to: $$n=10, r=246, s= 30\text{ for all slots}$$ If the $246$ are variables: $x_1, x_2, x_3,\ldots x_{246}$ , and the $10$ timeslots are: $y_1,y_2,\ldots y_{10}$ How to write an equation/constraint that limits the possible Permutations to be only the ones that satisfy the limited repetition (max $30$ of each time slot)?","['permutations', 'operations-research', 'discrete-mathematics', 'discrete-optimization', 'optimization']"
4515239,The formula to calculate the average cost of winning a game of chance,"As the title says, I'm trying to find the formula that calculates the average cost for a game of chance. Here are the rules of the game - There is a 25% chance to win a game The player wants to win exactly 3 games in one day . After that he stops playing. There is a maximum of 10 games per day The player pays for each game. The cost is 5 coins per game What is the average total amount of money the player would have to spend until he reaches a day in which he has won exactly 3 games? At first, I thought I could use the binomial distribution formula to solve the chance of getting exactly 3 wins in a day and use the result to calculate the average cost. $$E(X) = \binom{10}{3}0.25^{3}(0.75)^{7}$$ The result is ~ 0.2502, which means meeting our goal of exactly 3 wins approximately once every 4 games. That didn't seem right. Because the binomial distribution doesn't take into account the fact that the player stops playing after 3 wins. It assumes the player always plays 10 games every day . Since this didn't seem like the correct answer I wrote a little python program to approximate the true answer. The program simulates 10 million iterations with the rules above (the player has a maximum of 10 games per day, or 3 wins, whichever comes first. A game cost 5 coins and there's a 25% chance to win a game). These were the results - The player will have to play an average of 18.329 games until the goal of a day with exactly 3 wins is met Times that by the cost of 5 coins per game gives us an average cost of 91.64 coins The full output of the program can be seen here . So, is there a way to get to this answer without iterating through 10 million cases? Is there a formula to calculate this?","['expected-value', 'binomial-coefficients', 'game-theory', 'probability-theory', 'probability']"
4515255,Electric potential due to dipole layer,"In Classical Electrodynamics , Jackson derives the electric potential for a surface with a dipole charge. Here is his derivation. I will omit constants for brevity. Letting $D(\textbf{x}) := \lim_{d(\textbf{x}) \to 0} \sigma(\textbf{x}) d(\textbf{x})$ where $d(\textbf{x})$ is the local separation of $S$ and $S'$ with $S$ having charge density $\sigma(x)$ and $S'$ having equal and opposite charge density. The potential due to the two surfaces is: $$
\phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \frac{\sigma(\textbf{x'})}{|\textbf{x} - \textbf{x}' + \textbf{n}d|} da''
\tag{1}$$ where $\textbf{n}$ is the unit normal to the surface $S$ pointing away from $S'$ . He uses a Taylor expansion $$ \frac{1}{|\textbf{x} + \textbf{a}|} = \frac{1}{x} + \textbf{a} \cdot \nabla \Big( \frac{1}{x} \Big) \tag{2}$$ He says this is valid when $|\textbf{a}| \ll |\textbf{x}|$ (and I assume $x := |\textbf{x}|$ ). Then as $d \to 0$ (and I believe he redefines $\textbf{x} := \textbf{x} - \textbf{x}'$ and $\textbf{a} := \textbf{n}d$ ) he arrives at $$ \phi(\textbf{x}) = \int_S D(\textbf{x}') \textbf{n} \cdot \nabla'\Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da' \tag{3}$$ and since $\textbf{p} = \textbf{n}\ D\ da'$ then the potential at $\textbf{x}$ caused by a dipole at $\textbf{x}'$ is $$ \phi(\textbf{x}) = \frac{\textbf{p} \cdot (\textbf{x} - \textbf{x}')}{|\textbf{x} - \textbf{x}'|^3} \tag{4}$$ There are many steps I don't understand: From (1) Jackson used $\sigma(\textbf{x}')$ at $S$ and $- \sigma(\textbf{x}')$ at $S'$ . But, if $\textbf{x}'$ traces out $S$ wouldn't this be starting with the assumption that $S$ and $S'$ are the same surface? Why is $|\textbf{a}| \ll |\textbf{x}|$ a necessary assumption to use the Taylor expansion? The 1D case would be analogous to expanding the function $1/(x+a)$ and I do not see a reason that $a \ll x$ is necessary to do this. After substituting the Taylor expansion into (1) (and using $\textbf{x} := \textbf{x} - \textbf{x}'$ and $\textbf{a} := \textbf{n}d$ ) we get $$ \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'}) \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} + \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) \Big) da''$$ $$ \phi(\textbf{x}) = \int_S \frac{\sigma(\textbf{x}')}{|\textbf{x} - \textbf{x}'|} da' - \int_{S'} \sigma(\textbf{x'})  \frac{1}{|\textbf{x} - \textbf{x}'|} da''  - \int_{S'} \sigma(\textbf{x'}) \textbf{n}d \cdot \nabla \Big( \frac{1}{|\textbf{x} - \textbf{x}'|} \Big) da''$$ which somehow reduces to (3). It seems like Jackson cancelled the first two terms but how is this valid when we are integrating over $S$ in one and $S'$ in the other? Also, it seems like Jackson is missing a negative sign from the third term above. Also, the third term above differs from (3) in that he switches from integrating over $S'$ to $S$ . Is his change justified because after we do the limiting process the two surfaces coincide, allowing using swap? EDIT: I just realized these issues (besides the missing negative sign) can be resolved by doing the limiting process first and then expanding the integral into two terms. But this just begs the question; how do we know which order to do these steps when modeling with differentials and limiting processes? Should $\textbf{n}d$ be $\textbf{n}d(\textbf{x}')$ ? I just don't see the jump from (3) to (4) equation.","['electromagnetism', 'multivariable-calculus', 'taylor-expansion', 'vector-analysis', 'limits']"
4515299,Limit of $u_{n}=\frac{1}{n !} \sum_{k=0}^{n} k !$ [duplicate],"This question already has answers here : $\lim_{n\to\infty} \sum_{k=1}^n \frac{k!}{n!}$ (3 answers) Closed 1 year ago . I encountered a sequence $(u_n)_{n \in \mathbb{N}} $ defined as $$ u_{n}=\frac{1}{n !} \sum_{k=0}^{n} k ! $$ And I wonder what is the limit. It seems to be 1 but even Wolfram Alpha cannot figure it out. My first idea was to write $$ \frac{k!}{n!} = \frac{1}{(k+1)...(n-1)n}$$ and $$  (k+1)^{n-k} \leq (k+1)...(n-1)n \leq n^{n-k} $$ EDIT : I found a solution but Yanko found something more elementary. My solution : \begin{align}
  u_n &= 1 + \frac{1}{n} + \frac{1}{n(n-1)} + \frac{1}{n(n-1)(n-2)} + ... + \frac{1}{n!}   \\ 
&= 1 + \frac{1}{n} + (n-1) \times o(\frac{1}{n^2})  \\
&= 1 + \frac{1}{n}  + o(\frac{1}{n}) \\
\end{align} Thus all terms converge towards 0 except the first one, and the limit is 1.","['limits', 'analysis', 'sequences-and-series']"
4515300,"Minimal Definition of ""Finite Set""","Walter Rudin's Principles of Mathematical Analysis , page 25 has this definition: For any postive integer $n$ , let $J_n$ be the set whose elements are the integers $1,2,\ldots,n$ . $A$ is finite if $A\sim J_n$ for some $n$ (the empty set is also considered to be finite) Here $B\sim C$ means ``there exists a 1-1 mapping of $B$ onto $C$ ''. Wikipedia has the same definition: Formally, a set $S$ is called finite if there exists a bijection $f:S\rightarrow\{1,\ldots,n\}$ for some natural number $n$ . Question : is it not sufficient to stipulate that the map be 1-1? Requiring also that the map be onto seems to be unnecessary, and it seems like best practice to make minimal definitions.","['analysis', 'definition', 'functions', 'elementary-set-theory', 'set-theory']"
4515323,Is $h(x)=\frac{f(x)}{g(x)}$ injective?,"Let $f$ and $g$ be injective functions from $\mathbb{N}$ to $\mathbb{N}$ . Define $h$ , a function from $\mathbb{N}$ to $\mathbb{Q}$ by $h(x)=\frac{f(x)}{g(x)}$ for all $x \in \mathbb{N}$ . Is $h$ injective? My attempt: By the definition of injectivity, $f(x)=f(y) \implies x=y$ and $g(x)=g(y) \implies x=y$ Thus, $\frac{f(x)}{g(x)}=\frac{f(y)}{g(y)} \implies \frac{x}{x}=\frac{y}{y} \implies 1=1$ . Therefore, $h(x)$ isn't injective if $f=g$ .","['functions', 'real-analysis']"
4515327,Constant of integration for solving differential equation,"As I am progressing differential equations practice, I found myself at somewhat of a roadblock. The roadblock is essentially that let's say we have the following equation: $$\int x^2\,dx=\int y\,dy.$$ Now when we put the constants down after integration, such as: $$\frac{x^3}{3}+c_1=\frac{y^2}{2}+c_2,$$ would $c_1$ and $c_2$ not have to have the same value? Of course, the problem then becomes that the constant would cancel out, and there would be no constant in the solution. However, I was just wondering why the constants could be different, especially since we need to ensure that both left and right antiderivatives are the same function? In the case that the constants can be different, would someone be able to explain why that can be the case? Thanks so much.","['integration', 'ordinary-differential-equations']"
4515335,What is wrong with my solution to find the size of an angle of a triangle?,"The sides of a triangle $a$ , $b$ , and $c$ has the following lengths. $$a=x^2+x+1$$ $$b=2x+1$$ $$c=x^2-1$$ Since $a,b,c>0$ , $x>1$ . $$a-b=x(x-1)>0\ (\because x>1)$$ $$a-c=x+2>0$$ Therefore, $a$ is the longest side, and its opposite angle $A$ is the largest angle. Let $B$ and $C$ be the opposite angle of $b$ and $c$ , respectively. $$a:b:c=x^2+x+1:2x+1:x^2-1=A:B:C$$ $$A=(x^2+x+1)k$$ $$B=(2x+1)k$$ $$C=(x^2-1)k$$ $$A+B+C=(2x^2+3x+1)k=\pi$$ $$k=\frac{\pi}{2x^2+3x+1}$$ $$A=\frac{(x^2+x+1)\pi}{2x^2+3x+1}$$ The correct value of A is $\frac{2}{3}\pi$ , but the given solution doesn't produce the right answer. I got the correct value by an alternative method, but I can't figure out what's wrong with the solution above.","['triangles', 'trigonometry', 'angle', 'geometry']"
4515384,"A contour integration approach for $\int_{0}^{\infty} \frac{\ln \left(x^{4}+2 x^{2} \cos 2 a+1\right)}{1+x^{2}} \, d x$","Background In this week, I am tackling the integral $$\int_{0}^{\infty} \frac{\ln \left(1-x+x^{2}\right)}{1+x^{2}} d x$$ and found that a general formula below in my post , $$
\begin{aligned}
J(a)= \int_{0}^{\infty} \frac{\ln \left(1+2 x \sin a+x^{2}\right)}{1+x^{2}} d x=& \pi\left(\ln \left(2 \cos \frac{a}{2}\right)\right)+|a| \ln \left(\tan \frac{|a|}{2}\right)-2 \operatorname{sgn} (a) \int_{0}^{\frac{|a|}{2}} \ln (\tan x) d x
\end{aligned}
$$ by which, I found a decent formula for the quartic one, $$
\boxed{I(a)=\int_{0}^{\infty} \frac{\ln \left(x^{4}+2 x^{2} \cos 2 a+1\right)}{1+x^{2}} d x=2 \pi \ln \left(2 \cos \frac{a}{2}\right)}
$$ Proof: $$
\begin{aligned}
I(a)&=\int_{0}^{\infty} \frac{\ln \left(x^{4}+2 x^{2} \cos 2 a+1\right)}{1+x^{2}} d x\\
&=\int_{0}^{\infty} \frac{\ln \left[\left(x^{2}+1\right)^{2}+2 x^{2}(\cos 2 a-1)+1\right]}{1+x^{2}} d x\\
&=\int_{0}^{\infty} \frac{\ln \left[\left(x^{2}+1\right)^{2}-4 x^{2} \sin ^{2} a\right]}{1+x^{2}} d x\\
&=\int_{0}^{\infty} \frac{\ln \left(x^{2}+2 x \sin a+1\right)}{1+x^{2}}+\int_{0}^{\infty} \frac{\ln \left(x^{2}-2 x \sin a+1\right)}{1+x^{2}} d x\\& =J\left(\frac{a}{2}\right)+J\left(-\frac{a}{2}\right)\\& =2 \pi \ln \left(2 \cos \frac{a}{2}\right)
\end{aligned}
$$ For examples: $$
K=\int_{0}^{\infty} \frac{\ln \left(x^{4}+1\right)}{1+x^{2}} d x= I\left(\frac{\pi}{4}\right)=2 \pi \ln \left(2 \cos \frac{\pi}{8}\right)= \pi \ln (2+\sqrt{2})
$$ $$
\begin{aligned}
L=\int_{0}^{\infty} \frac{\ln \left(x^{4}+x^{2}+1\right)}{1+x^{2}} d x&=I\left(\frac{\pi}{6}\right)=2 \pi \ln \left(2 \cos \frac{\pi}{12}\right)=\pi\ln (2+\sqrt{3})
\end{aligned}
$$ $$
\begin{aligned}
M=\int_{0}^{\infty} \frac{\ln \left(x^{4}-x^{2}+1\right)}{1+x^{2}} d x&=I\left(\frac{\pi}{3}\right)=2 \pi \ln \left(2 \cos \frac{\pi}{6}\right)=\pi\ln 3
\end{aligned}
$$ $$
\begin{aligned}
N=&\int_{0}^{\infty} \frac{\ln \left(x^{8}+x^{4}+1\right)}{1+x^{2}} dx = L+M=\pi \ln (6+3 \sqrt{3})
\end{aligned}
$$ Last but not least, $$
\boxed{\int_{0}^{\infty} \frac{\ln \left(x^{4}+b x^{2}+1\right)}{1+x^{2}} d x = 2 \pi \ln \left[2 \cos \left(\frac{1}{4} \cos ^{-1}\left(\frac{b}{2}\right)\right)\right]}
$$ $$
\begin{aligned}
\int_{0}^{\infty} \frac{\ln \left(x^{8}+1\right)}{1+x^{2}}=& \int_{0}^{\infty} \frac{\ln \left(x^{4}+\sqrt{2} x+1\right)}{1+x^{2}} d x +\int_{0}^{\infty} \frac{\ln \left(x^{4}-\sqrt{2} x+1\right)}{1+x^{2}} d x \\
=& 2 \pi \ln \left[2 \cos \frac{1}{4} \cos ^{-1}\left(\frac{\sqrt{2}}{2}\right)\right] +2 \pi \ln \left[2 \cos \frac{1}{4} \cos ^{-1}\left(-\frac{\sqrt{2}}{2}\right)\right] \\
=& 2 \pi \ln \left(2 \cos \frac{\pi}{16}\right)+2 \pi \ln \left(2 \cos \frac{3 \pi}{16}\right)\\
=& 2 \pi \ln \left(2^2 \cos \frac{\pi}{16} \cos \frac{3 \pi}{16}\right)\\=& 2 \pi \ln (\sqrt{2}+\sqrt{2+\sqrt{2}})
\end{aligned}
$$ Eager to know whether it can be proved by contour integration. Your help and solutions are highly appreciated!","['integration', 'improper-integrals', 'calculus', 'trigonometric-integrals', 'complex-integration']"
4515396,Argue on elementary grounds that the average value of $x/y$ exceeds 1 if x and y are chosen randomly between 1 and 2,"Textbook problem: Two numbers, first $x$ and then $y$ , are chosen at random between $1$ and $2$ . What is the average value of the quotient $\frac{x}{y}$ ? Can you argue on elementary grounds that the answer must exceed 1? My answer: If $x$ has been picked, then the average of the quotient is $$
\int_1^2 \frac{x}{y}\,dy = x\ln(2)
$$ Averaging over possible values of $x$ yields $$
\int_1^2 x\ln(2)\, dx = \frac{3}{2}\ln(2) \approx 1.03
$$ But this is not an argument based on elementary grounds I don't think. Interpreting the average value of a function over an interval as being the height of the rectangle with base on the interval and with the same area as the area under the function over the interval leads me to the following argument. The average value of $x$ is $3/2$ . The quotient $x/y$ then ranges from $3/4$ to $3/2$ . We can underestimate the area under this curve by splitting it up into a rectangle with length $1$ and height $3/4$ and a right triangle on top of the rectangle whose hypotenuse is the tangent line to the curve $\frac{3}{2y}$ at the point $(3/2,1)$ . This gives a triangle with height $7/12$ and base $7/8$ . So an underestimate for the area under the curve would be $\left(1\cdot \frac{3}{4}\right) + \left(\frac{1}{2}\cdot \frac{7}{8}\cdot \frac{7}{12}\right) = \frac{193}{192}$ . Here is an illustration of my quotient function with $x = 3/2$ , the tangent line, and the rectangle: Since the base of the rectangle with the same area on $[1,2]$ has length $1$ its height must be at least $\frac{193}{192}$ to match the area under the graph.  Hence, the height, or average value, exceeds $1$ . Question: Does this seem to be an argument the author could be looking for instead of integrating? This is a single variable calculus text.  The section is ''The Average Value of a Function"". Note that not until the next chapter is probability introduced. Update: I have obtained a copy of the author's own solutions manual. Here is the official solution: To argue on elementary grounds that the answer must exceed 1, note that the outcomes $a/b$ and $b/a$ must occur equally often, so that the answer is certainly greater than the minimum over all $a$ and $b$ in $[1,2]$ of the average of these two numbers. Now this average is $$
\frac{1}{2}\left[\frac{a}{b} + \frac{b}{a}\right] = \frac{a^2 + b^2}{2ab}
$$ and this is always greater than 1 because $a^2 + b^2 - 2ab = (a-b)^2 > 0$ .","['average', 'calculus']"
4515434,Looking at a possible inconsistency in the areas of two right trapezoids forming a larger one,"Could anybody explain to me why the difference arises below? Area of right trapezoid for the total figure, i.e. $AEFD$ , is $131978$ For trapezoid $ABCD$ the area is $62196$ For trapezoid $BEFC$ the area is $57630$ The sum of areas for $ABCD$ and $BEFC$ should be equal to that of $AEFD$ , but clearly that's not the case here. Feel free to correct me if I'm wrong, but if not, where does the difference in values come from?","['quadrilateral', 'algebra-precalculus', 'area', 'geometry']"
4515446,How does the path connectedness in a topology book relate to the path connectedness described in a typical vector calculus book?,"A set $S$ is said to be connected (see [30a] ) if any two points in S can be connected by an unbroken curve lying entirely within $S$ . Conversely, if there exist pairs of points that cannot be connected in this way (see [30b] ), then the set is disconnected. Amongst connected sets we may single out the simply connected
sets (see [30c] ) as those that do not have holes in them. More precisely, if we picture the path connecting two points in the set as an elastic string, then this string may be continuously deformed into any other path connecting the points, without any part of the string ever leaving the set. Conversely, if the set does have holes in it then it is multiply connected (see [30d] ) and there exist two paths connecting
two points such that one path cannot be deformed into the other. Page-92, 93 , Neeedham Visual complex Analysis My doubt about this section came when I started reading about Topology from Munkres. The above text seems to be about topological space being path connected but my question is, how does one describe holes  (and classification of space based on that) as shown above using the topological path connectedness idea ? Path connectedness (deftn):The space $X$ is said to be path-connected if there is exactly one path-component, i.e. if there is a path joining any two points in $X$ source","['general-topology', 'algebraic-topology']"
4515456,Function unbounded on a neighbourhood then limit doesn't exist,"I came across a corollary which goes as follows: Let $D \subset \mathbb R $ . If $f:D \rightarrow \mathbb R$ is not bounded on $N(c,\delta)\, \cap \, D$ for some $\delta$ - neighbourhood $N(c,\delta)$ of c, then $\lim_{x \to c} f(x)$ does not exist on $\mathbb R$ . My question is, in this corollary shouldn't this condition of unboundedness apply on every $\delta$ - neighbourhood $N(c,\delta)$ of c. For example if f(x) = tan(x), and c = $\pi/4$ , D = $ [0,\pi/2) \cup (\pi/2,3\pi/2) $ , then for a $\pi/2$ -neigbourhood of $\pi/4$ , $N(\pi/4,\pi/2) $ . f is not bounded on the interval $ N(\pi/4,\pi/2)\, \cap \, D = [0,\pi/2) \cup(\pi/2,3\pi/4) $ , which would imply that $\lim_{x \to \pi/4} tan(x)$ doesn't exist. Which is not true Also, in this corollary how would it be affected if we consider a deleted neighbourhood?","['limits', 'analysis', 'real-analysis']"
4515472,Lebesgue measure and the curse of dimensionality (application),"Apologies for the cryptic, Harry Potter -esque title; I really did not know how to name this! (Very much open to suggestions of more informative titles.) Fix $\varepsilon>0$ and $a>0$ .
For each $n\in\{2,3,...\}$ , define $$
S_n := \left\{(x_1,...,x_n)\in [0,a]^n : \frac{e^{x_i}}{\sum_{j=1}^n e^{x_j}}< \varepsilon \ \forall i\in\{1,...,n\}\right\}
$$ I'm having difficulty calculating the Lebesgue measure (denoted by $\lambda(\cdot)$ ) of $S_n$ . Is this possible? If so, advice on how to do this would be very appreciated. If not, would there be another way for me to analytically characterize $\text{limit}_{n\to\infty} \frac{\lambda(S_n)}{a^n}$ ? (I.e. how ""large"" $S_n$ is relative to $[0,a]^n$ as $n$ becomes large?.) Note: Assume $\varepsilon$ is such that $S_n$ is non-empty for at least some $n\in\{2,3,...\}.$","['measure-theory', 'lebesgue-measure', 'applications', 'limits', 'recreational-mathematics']"
4515482,Number of solutions of $|\log_e{|x-2|}|+x^2-2x-35=0$,"Find the number of solutions of the given equation: $$|\log_e{|x-2|}|+x^2-2x-35=0$$ While studying the topic ""Graph Transformation"", this question cam up and the answer was said as 4. Out of curiosity, I went to Geogebra, and there I saw the answer should be 2 instead! Here's my simple attempt in GeoGebra for confirmation. Isn't there a way that complements the graph so as to confirm the answer? How can this question be solved without the use of graphs? Please help.","['algebra-precalculus', 'graphing-functions']"
4515484,When does a continuous function map loops to loops?,"Let $X$ be a space; let $x_o$ be a point of $X$ . A path in $X$ that begins and ends at $x_o$ is called a loop based at $x_o$ . Page-327 , Munkres I have before learned of continuous function presving connectedness and path connectedness, but what more conditions do we need on a function on top of continuity for it to preserve loops? [How I understood that path connectedness doesn't imply loop preservation]: If we have a continuous map $f:X \to Y$ , and a path in $X$ $a \to b \to c \to a$ , then we know by continuity preserving path connectedness that: $ a \to b$ gets sent to some path $a' \to b'$ $ b \to c$ gets sent to $ b'' \to c'$ $ c \to a $ gets sent to $ c'' \to a'' $ The issue is , I don't know if the double primed points are equal to the primed points. So, it doesn't make sense to say that just having continuity preserves loops.","['general-topology', 'algebraic-topology']"
4515543,"If $X$ is log-normal, find $\mathbb{E}(\min(X-4,1))$.","For a random variable $X$ , we have $\mathbb{E}(X)=4$ and that $\ln X$ follows normal distribution with variance $3$ . I need to find $\mathbb{E}(\min(X-4,1))$ and express it with $\Phi$ , the cumulative distribution function of the standard normal distribution. My attempt: $$
\begin{align}
\mathbb{E}(\min(X-4,1))
&=\int^\infty_{-\infty} \min(x-4,1)f_X(x)dx\\
&=\int^5_{-\infty}(x-4)f_X(x)dx+\int^\infty_5f_X(x)dx\\
\end{align}
$$ Here's where I'm stuck and i don't know what to do to express it with $\Phi$ , the CDF of the standard normal distribution. Any suggestions please?","['expected-value', 'statistics', 'normal-distribution']"
4515549,Integral: $\int_{0}^{1}\frac{\arctan^{2}\left(x\right)}{x}dx$,"Context : While working on a contour integral for fun, I stumbled upon the following integral: $$\int_{0}^{1}\frac{\arctan^{2}\left(x\right)}{x}dx.$$ I typed it into WolframAlpha and got that it equals $$\frac{1}{8}(4\pi C - 7\zeta{(3)}),$$ where $C$ denotes Catalan's Constant and $\zeta{(3)}$ denotes Apery's Constant. Attempt : Let's call the original integral $I$ . At first, I tried IBP, then letting $x = \tan{(\theta)}$ , then IBP again like this: $$
\eqalign{
I &= -2\int_{0}^{1}\frac{\arctan\left(x\right)\ln\left(x\right)}{1+x^{2}}dx \cr
&= -2\int_{0}^{\frac{\pi}{4}}x\ln\left(\tan\left(x\right)\right)dx \cr
&= 2\int_{0}^{\frac{\pi}{4}}\frac{x^{2}}{\sin\left(2x\right)}dx.
}
$$ At that point, I decided I was using IBP an unnecessary amount of times and figured there has to be a nicer solution. I also tried differentiating with respect to a parameter $a$ and defining $$J(a) = -2\int_{0}^{1}\frac{\arctan\left(x\right)\ln\left(ax\right)}{1+x^{2}}dx,$$ but I ended up circling back to where I started after doing a lot of grunt work. I also tried $$ -2\int_{0}^{\frac{\pi}{4}}x\ln\left(\tan\left(x\right)\right)dx = -2\int_{0}^{\frac{\pi}{4}}x\ln\left(\sin\left(x\right)\right)dx+2\int_{0}^{\frac{\pi}{4}}x\ln\left(\cos\left(x\right)\right)dx$$ and using Taylor Series and complex definitions of $\sin{(x)}$ and $\cos{(x)}$ , but I was getting a mess. Question : Does anyone know a nice way of solving the given integral? If it's not a pretty solution, it's fine. Any hints and help are appreciated.","['integration', 'calculus', 'definite-integrals']"
4515550,"Prove that certain integral operator on $L^1[0,\infty)$ is compact","I met the following exercise in functional analysis, which made me somewhat confused: Let $f\in L^1[0,\infty)$ (say complex valued, the usual $L^p$ space definition, usual Lebesgue measure on $[0,\infty)$ ). Consider the integral operator $$H_{f}:L^1[0,\infty)\rightarrow L^1[0,\infty),~\left(H_{f} g\right)(x)=\int_{0}^{\infty} f(x+y) g(y) d y.$$ Prove that $H_f$ is a compact operator. (Of course by Fubini it's bounded.) I was confused and had no idea how to prove this, since the compactness of integral operator is usually established for the dual index: c.f. Integral operator on $L^p$ is compact . So in the usual context, if the space is $L^p$ , then the kernel function is in $L^q$ . The exercise looks somewhat similar to something related to convolution, but the interval is not symmetric. I guess this requires some ""ad-hoc"" method to deal with. Or is there any background/famous theorem /concrete properties of $L^p$ spaces related to this example? (I'm a beginner in functional analysis course; if this question is too naive, I sincerely apologize.) Thanks a lot in advance for any suggestion or hint!","['lp-spaces', 'compact-operators', 'functional-analysis']"
4515565,How does one come up with the continued fraction for the arctangent? $\arctan x=\frac{x}{1+}\frac{x^2}{3+}\frac{(2x)^2}{5+}\cdots$,"$\newcommand{\K}{\operatorname{\large{K}}}$ The question has already been asked here . However, I find the accepted answer unsatisfactory since it details all the parts which are, in my opinion, trivial, and the one step it doesn't elaborate on is precisely the step I consider non-trivial. I am a bit stuck, so I'd like to ask here. I’ll be using Gauss’ Kettenbrücher K-notation for continued fractions, analogous to the $\sum,\prod$ notations. We know that: $$\arctan x=x-\frac{x^3}{3}+\frac{x^5}{5}-\cdots$$ For $|x|\lt1$ and we know Euler's continued fraction formula: $$a_0\left(1+\K_{n=1}^\infty\frac{-a_n}{1+a_n}\right)^{-1}=a_0+a_0a_1+a_0a_1a_2+\cdots$$ So it is quite easy to find $a_0=x,\,a_n=-\frac{2n-1}{2n+1}x^2$ as suitable for the arctangent, to get: $$\arctan x=x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}=\cfrac{x}{1+\cfrac{\frac{1}{3}x^2}{1-\frac{1}{3}x^2+\cfrac{\frac{3}{5}x^2}{1-\frac{3}{5}x^2+\cdots}}}$$ And by clearing the denominators we obtain: $$\tag{1}\arctan x=\cfrac{x}{1+\cfrac{x^2}{3-x^2+\cfrac{(3x)^2}{5-3x^2+\cdots}}}$$ But the accepted answer to the linked post claims a different fraction follows. Apparently, if one uses Euler's formula - with the same coefficients I used - it is a matter of simple algebra to arrive at: $$\tag{2}x\left(1+\K_{n=1}^\infty\frac{\frac{2n-1}{2n+1}x^2}{1-\frac{2n-1}{2n+1}x^2}\right)^{-1}\overset{?}{=}\cfrac{x}{1+\cfrac{x^2}{3+\cfrac{(2x)^2}{5+\cfrac{(3x)^2}{7+\cdots}}}}$$ This is corroborated by Wikipedia. Unfortunately I am at a complete loss as to how to go from $(1)$ to $(2)$ . One thing I have noticed - it suffices to show: $$\K_{n=1}^\infty\frac{(nx)^2}{2n+1}=\K_{n=1}^\infty\frac{((2n-1)x)^2}{(2n+1)-(2n-1)x^2}=\K_{n=1}^\infty\frac{(n^2x-(n-1)^2x)^2}{(n+1)^2-n^2-(nx)^2+((n-1)x)^2}$$ As $2n-1=n^2-(n-1)^2$ , which seems half-promising, but I just don't see the trick needed to procede. Other than equivalence transformations given by clearing denominators, I don't know how to manipulate continued fractions. What are we supposed to do here? Note: I see that it is also possible to prove this result, and many others, using the Gauss continued fraction identities for the hypergeometric functions. I have never studied the hypergeometric functions: if you can answer using them, I’d really appreciate it if the answer was written in such a way that avoids the general theory. But, that shouldn’t be necessary, since the accepted answer to the linked post claims the result follows from “algebra”.","['proof-explanation', 'real-analysis', 'trigonometry', 'limits', 'continued-fractions']"
4515567,Derivative of an implicit trigonometric function,"I was doing my homework which included finding the derivative of implicit function. One of them was $$3\sin(xy)+ 4\cos(xy) = 5$$ On first attempt I did this as follows $$\frac{d}{dx}(3\sin(xy)+ 4\cos(xy)) = \frac{d}{dx}(5)$$ $$[3\cos(xy)-4\sin(xy)][y+x\frac{dy}{dx}] = 0$$ $$\frac{dy}{dx}= \frac{-y}{x}$$ It matched the answer and I was happy. I don't know why but I wrote the original expression as this $$\frac{3}{5}\sin(xy)+ \frac{4}{5}\cos(xy) = 1$$ $$\cos(\alpha)\sin(xy)+ \sin(\alpha)\cos(xy) = 1$$ $$\sin(xy +\alpha)=1 \quad \quad \textrm{where} \quad\alpha = \arccos(\frac{3}{5})$$ Again from step 2 of finding derivative, $$[3\cos(xy)-4\sin(xy)][y+x\frac{dy}{dx}] = 0$$ $$[\frac{3}{5}\cos(xy)-\frac{4}{5}\sin(xy)][y+x\frac{dy}{dx}] = 0$$ $$[\cos(\alpha)\cos(xy)-\sin(\alpha)\sin(xy)][y+x\frac{dy}{dx}] = 0$$ $$[\cos(xy +\alpha)][y+x\frac{dy}{dx}] = 0$$ Since $\sin(xy +\alpha) =1 \implies \cos(xy+\alpha)=0$ what did in finding the derivate while going from step 2 to 3 was diving by zero which is not valid. So I cannot actually conclude that the derivative is $\frac{-y}{x}$ . But then why is the answer right(wolfram alpha says this too)? Is there any other way of finding this derivate? Thank you in advance.","['calculus', 'implicit-differentiation', 'derivatives']"
4515611,Localization of ground state Schrödinger eigenfunction,"Principle: The ground state eigenfunction of a Schrodinger operator is localized near the minima of the potential. I learned this principle in basic quantum mechanics courses (when solving the Schrödinger equation for various potentials, e.g. double well, periodic box, etc.). My question concerns how to quantify this statement. Let $I=[-1,1]$ and $V:I\to \mathbb{R}$ be a smooth periodic potential and let $$
L=-\frac{d^2}{dx^2}+V(x)
$$ be the corresponding Schrödinger operator on $I$ with periodic boundary conditions. Let $\lambda_1$ be the lowest eigenvalue of the eigenvalue problem $$
Lu=\lambda u
$$ on $I$ with periodic boundary conditions and let $u_1$ be the corresponding eigenfunction (the ground state eigenfunction). By the nodal domain theorem, $u_1$ has no zeros, hence we may assume $u_1>0$ . Suppose also that $\lambda_1\leq 0$ , which implies that $V_{\min}\leq 0$ .
Let $x_0\in I$ be the point at which the potential is maximized $$
V(x_0)=V_{\max}.
$$ Question: Is it true that $x_0$ minimizes $u$ , i.e. that $$
u(x_0)=u_{\min}?
$$ Numerical solutions I generated using Mathematica for various potentials indicate that the answer to the question is affirmative. How can one prove or disprove this. If the answer is negative, how can the ""principle"" written above be quantified? Any references would be appreciated. Thank you.","['partial-differential-equations', 'ordinary-differential-equations', 'sturm-liouville', 'eigenfunctions', 'spectral-theory']"
4515674,How does $s=r\theta$ take into account negative angles or angles greater than $2\pi$?,"Radian is defined as the angle subtended by the arc at the center of the circle. The formula of radian is $\theta=\dfrac sr$ where $s$ is the arc length and $r$ is the circle's radius. Since length of arc cannot be more than the circumference, how is a value of radian that is more than $2\pi$ adjusted in the formula? How about a negative value of radian? Is a scalar $k$ multiplied in the formula to compensate for negative value and for value more than to $2\pi\;?$ Is this formula correct? $$\theta=k\frac sr$$","['algebra-precalculus', 'angle']"
4515688,Cohomology exists if and only if its forced to,"When setting up the foundations of sheaf cohomology, one proves that flasque sheaves have no cohomology. The proof in Hartshorne is an induction, and relies on two properties of flasque sheaves: if $0 \to\mathcal{F}'\to\mathcal{F}\to\mathcal{F}''\to 0$ is exact and the first term is flasque, then the global sections sequence is exact; and that a quotient of a flasque sheaf by a flasque subsheaf is exact. But it seems plausible to me that there should be an argument using only the first property of flasque sheaves. Conjecture. If $\mathcal{F}'$ is a sheaf so that every exact $0 \to\mathcal{F}'\to\mathcal{F}\to\mathcal{F}''\to 0$ induces an exact sequence $0 \to\mathcal{F}'(X) \to\mathcal{F}(X)\to\mathcal{F}''(X)\to 0$ of global sections, then $\mathcal{F}'$ has trivial higher cohomology. Is this conjecture true? I think an attempt at contrapositive makes some progress. If $\mathcal{F}'$ has nontrivial $H^1,$ then by embedding into an injective sheaf $\mathcal{F}$ and taking the quotient, we always have an exact sequence $0 \to \mathcal{F}'(X)\to \mathcal{F}(X)\to\mathcal{F}''(X)\to H^1(X, \mathcal{F}')\to H^1(X, \mathcal{F}) = 0.$ If $\mathcal{F}(X)\to\mathcal{F}''(X)$ surjected, then by exactness we'd deduce that $H^1(X, \mathcal{F}') \to 0$ is injective, and so $H^1(X, \mathcal{F}') = 0.$ This takes care of the $H^1,$ and so if $\mathcal{F}'$ has the hypothesis of conjecture, then $H^1(X, \mathcal{F}') = 0$ by contrapositive. But can I say anything about vanishing of the higher cohomologies?",['algebraic-geometry']
4515695,"Is it sufficient to prove that $| f(x,y) – L | = 0 $ so that $\lim_{(x,y)\to(0,0)} f(x,y) =L$ is true?","Im trying to proove the following limit by the squeeze theorem: $$  \lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0 $$ I've seen all the related questions, and I can't understand the validity of applying absolute value as it follows: Of course we have that: $|\frac{xy}{\sqrt{x^2+y^2}}|≥0$ , and also $ |\frac{xy}{\sqrt{x^2+y^2}}|=  \frac{|x|\cdot |y|}{\sqrt{x^2+y^2}} ≤                    \frac{|x|\cdot |y|}{\sqrt{y^2}} = \frac{|x|\cdot |y|}{|y|}=|x|$ , Therfore, by the squeeze theorem: $$ 0≤|\frac{xy}{\sqrt{x^2+y^2}}|≤|x|  $$ $$ \lim_{(x,y)\to(0,0)}  0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤\lim_{(x,y)\to(0,0)} |x|  $$ $$ 0≤\lim_{(x,y)\to(0,0)}  |\frac{xy}{\sqrt{x^2+y^2}}|≤0  $$ So finally that should be sufficient to prove that the distance $| f(x,y) – L | = 0 $ and therefore, that should be enough to say that: $$\lim_{(x,y)\to(0,0)} \frac{xy}{\sqrt{x^2+y^2}} = 0$$ Then my question is: is this valid? Or am I committing a mathematical atrocity.  Thank you for your time and patience.","['limits', 'multivariable-calculus', 'absolute-value']"
4515702,Determine $\alpha$ that satisfies $P(|X|<1) = P(|X|>1)$,"Suppose that $X$ is uniformly distributed $[-\alpha, +\alpha]$ , where $\alpha > 0$ . Whenever possible, determine $\alpha$ so that the following are satisfied. $$P(|X| < 1) = P(|X| > 1)$$ Answer: $\alpha = 2$ . This is the question 4.20 from (Paul Meyer's ""Introductory probability and Statistics"", 2nd ed.) What I've managed to do was find the density function, which is: $\frac{1}{2\alpha}$ . And from there I applied in the given equation using the absolute value definition, obtaining: $$P(-1<X<1) = P(X>1) + P(X<-1)$$ $$\int_{-1}^{1}\frac{1}{2\alpha}d\alpha = 1 - P(X≤1) + P(X<-1)$$ But the outcome of this integral it's not a real value. What was my mistake here?","['statistics', 'uniform-distribution', 'probability-distributions', 'probability', 'random-variables']"
4515743,Topological closed immersions and direct / inverse image of sheaves,"Preamble which you can skip I am reading Proposition 2.24 in Liu's ""algebraic geometry and arithmetic curves"" book and there is a confusing reference to exercise 2.13 in the proof. In said exercise he claims that: the natural map $f^{-1}f_*\mathcal{F} \to \mathcal{F}$ is an isomorphism whenever $f$ is an open immersion and that the natural map $\mathcal{G} \to f_*f^{-1}\mathcal{G}$ is surjective whenever $f$ is a closed immersion. But in proposition 2.24 $f$ is a closed immersion and he is using the isomorphism $f^{-1}f_*\mathcal{F} \cong \mathcal{F}$ referring to the exercise which instead requires $f$ to be open. Question I believe the correct statement for closed immersions should be the following: Let $f:Y \to X$ be a topological closed immersion. Then: for every $\mathcal{F} \in \operatorname{Sh}(Y)$ , the natural map $f^{-1}f_*\mathcal{F} \to \mathcal{F}$ is an isomorphism and for every $\mathcal{G}\in \operatorname{Sh}(X)$ such that $\mathcal{G}_x =0$ iff $x \notin f(Y)$ the natural map $\mathcal{G}
 \to f_*f^{-1}\mathcal{G}$ is an isomorphism. What I said above follows, I believe, from the following fact about topological closed immersion: Let $f: Y \to X$ be a topological closed immersion and let $\mathcal{F}\in \operatorname{Sh}(Y),$ then $f_*\mathcal{F}_x = 0$ if $x \notin f(Y)$ and $f_*\mathcal{F}_x = \mathcal{F}_y$ if $x=f(y)\in f(Y).$ Q: Is my statement above correct? Is there a typo in Q.Liu's exercise?","['category-theory', 'algebraic-geometry', 'sheaf-theory', 'general-topology', 'schemes']"
4515759,$\phi \in L^1 \iff m(\{\phi \neq 0\})<\infty$,"Let $\phi$ be a positive, simple, function. Prove $\phi \in L^1 \iff m(\{\phi \neq 0\})<\infty$ . My attempt: $(\iff)$ $\{\phi \neq 0\}=\{\phi >0\}\cup \{\phi <0\}.$ Suppose $\phi :E\to [0,+\infty),$ then $\int_{E}\phi \:dμ =\int_{\{\phi >0\}\cup \{\phi <0\}\cup \{\phi = 0\}}\phi \:dμ=
\int_{\{\phi >0\}}\phi \:dμ+\int_{\{\phi <0\}}\phi \:dμ+ \int_{\{\phi =0\}}\phi \:dμ.$ $\phi $ is integrable at $\{\phi =0\}$ , so in order to $\phi \in L^1(E)$ it must be that $\int_{\{\phi >0\}}\phi \:dμ< \infty$ and $\int_{\{\phi <0\}}\phi \:dμ<\infty,$ but because $\phi$ is simple ( $\int_{\{\phi >0\}}\phi \:dμ =\sum a_im(\{\phi >0\})$ that means $m(\{\phi <0\})<\infty $ , $m(\{\phi >0\})<\infty \Rightarrow m(\{\phi \neq 0\})<\infty$ .","['measure-theory', 'solution-verification', 'real-analysis']"
4515772,How to obtain all solutions of separable differential equation where the existence and uniqueness theorem does not apply?,"From blackpenredpen 's February 2017 video on the existence & uniqueness theorem, the separable differential equation $$\frac{{\rm d} y}{{\rm d}x}= x\sqrt{y-3} \tag{1}$$ with the initial condition $y(4)=3$ has the only solutions $$y=\left(\frac{x^2}{4}-4\right)^2+3 \tag{2}$$ $$y(x)=3 \tag{3}$$ where the valid interval can be the whole real line. The solution is obtained by dividing both sides of $(1)$ by $\sqrt{y-3}$ and then taking the integral of both sides. For me, it seems that it has not been proven that there aren't any more solutions $y$ to $(1)$ such that there exists a $x$ for which $y(x)=3$ . I'm thinking when integrating both sides we first assume that $y(x) \neq 3$ so we can divide both sides. When this happens, we can not say we have covered all solutions $y$ of $(1)$ such that there is a $x$ for which $y(x)=3$ . This is evident from $(3)$ . I'm assuming obtaining $(2)$ was just a happy accident. In short the proof has 2 cases. First case assumes $y\neq3$ for all $x$ and covers all those solutions and the second case is the constant solution $y=3$ . I'm asking where the justification is that we don't need a third case for the solutions $y$ such that only for some $x$ in the valid interval is $y(x)=3$ . Background: Solving separable differential equations","['initial-value-problems', 'ordinary-differential-equations']"
4515775,What is $\Gamma(1/4)$? I'm trying to figure out how you would go about solving it,"We know that $$\Gamma(1/2) = \int e^{-x} x^{-1/2} \text{d}x$$ If I set $t^2 = x$ then we have $2\int e^{-t^2} \text{d}t$ We know that $$\frac{1}{\sqrt{\pi}}\int e^{-t^2} \text{d}x = \frac{1}{2}$$ This is simply half of the area under the graph of a normal distribution given that my parameters of integration are $(0, +\infty)$ . Hence $$2\int e^{-t^2} \text{d}x = 2 \left(\frac{1}{2}\sqrt{\pi}\right) = \sqrt{\pi}$$ Setting up the problem similarly for $\Gamma(1/4)$ would give us $$\int e^{-x}  x^{-3/4} \text{d}x$$ Then setting $t^4 = x$ gives us $$4\int e^{-t^4} \text{d}t$$ Not exactly sure where to go from here or if this even the right approach. Any help is appreciated. Thank you so much.","['gamma-function', 'definite-integrals', 'probability-theory']"
4515779,Probability Convergence of an average of random variables where $n \to \infty$ and $d \to \infty$,"Suppose $X_1^{(d)}, X_2^{(d)}, \cdots, X_n^{(d)} \stackrel{\text{i.i.d.}}{\sim} \mathbf{F}_d$ are $d$ -dimensional random variables such that there exists a function $g_d(\cdot,\cdot)$ taking values in $[-2,+2]$ , and satisfying $g_d(X_1^{(d)},X_2^{(d)}) \to 0$ in probability as $d \to \infty$ . Also, $\mathbb{E}[g_d(X_i^{(d)}, X_j^{(d)})] = 0$ , for all $i,j = 1,2,\cdots,n ~(i \neq j)$ . Define the following random variable: $$S_{n,d} = \frac{1}{\binom{n}{2}} \sum_{1 \leqslant i \neq j \leqslant n} g_d(X_i^{(d)},X_j^{(d)})$$ Can we comment on the probability convergence of $S_{n,d}$ as $n \to \infty$ and $d \to \infty$ simultaneously? NOTE: $g_d(\cdot,\cdot)$ is a function which doesn't vary over $d$ . e.g. $g_d(a^{(d)},b^{(d)})= \frac{||a^{(d)}+b^{(d)}||_2}{||a^{(d)}||_2+||b^{(d)}||_2}$ . The problem is that I am facing difficulty understanding whether the different $g_d(X_i^{(d)},X_j^{(d)})$ 's are independent or not. If they are independent, it would have been a straightforward application of WLLN which is not the case here. However, we have an additional information that each summand goes to $0$ in probability as $d \to \infty$ . I have a hunch that $S_{n,d} \to 0$ in probability as $n \to \infty$ and $d \to \infty$ . Any help will be appreciated.","['statistics', 'law-of-large-numbers', 'probability-theory']"
4515802,$3$D analog to the catenary,"Introduction : Given a plane $(2d)$ , two points $(0d)$ , and a curve $(""1d"")$ analogous to a chain (constant length, points can only rotate around their neighbouring points and cant distance or approach themselves to their neighbouring points, and it is non-self intersecting) hanging from the points, under an uniform gravitational field, we have a catenary. Question : Given the space $(3d)$ , two curves $(1d)$ , and a surface $(2d)$ (with analogous characteristics, e.g. if the lenght of the chain was fixed before, now the area of the surface is fixed), we have a... what? What is its name? [ Answer : In civil engineering this surface is called "" catenary membranes "" or "" catenary domes ""] Clarification : Illustration of the surface: Imagine I have a pair of curves with finite length, a mesh of inelastic strings (making tiny rectangular cells, for example), and the mesh is hanging from the curves under a uniform gravitational field A source for physics of the differential equation that leads to the catenary: Exercise 9 on p. 9 of Professor Shiffrin's DIFFERENTIAL GEOMETRY: A First Course in Curves and Surfaces Another source : https://parametricmonkey.com/2015/10/10/catenary-curves/","['physics', 'analytic-geometry', 'differential-geometry']"
4515811,Definition of the pushforward measure,"Given a map $f:X \to Y$ , and two $\sigma$ -algebras $\mathcal{A}$ and $\mathcal{B}$ on $X$ and $Y$ respectively, and a measure $\mu$ on $(X, \mathcal{A})$ , we can define the pushforward measure $f_\#\mu$ on $B \in \mathcal{B}$ as $$f_\#\mu(B):=\mu(f^{-1}(B)).$$ I can't seem to understand the most basic property of this measure, which is that for any measurable $g$ on $Y$ , we have $$\int g df_\#\mu=\int g \circ f d\mu.$$ I think I'm missing something really obvious here. My thought process: It suffices to prove the case for $g=1$ , as we can approximate by simple functions and then apply the monotone convergence theorem. So we want to show $$\int_{f(A)} df_\#\mu=\int_A  f d\mu,$$ where $A$ is the support of $f$ . We have $\int_{f(A)} df_\#\mu=f_\#\mu(f(A))=\mu(f^{-1}(f(A)))$ , and I can't see why this is equal to $\int_A  f d\mu$ .","['measure-theory', 'probability-theory', 'real-analysis']"
4515819,Convergence in operator norm equivalent to uniform convergence on bounded sets,"Let $E$ be a Banach space and let $\{T_n\}_{n=1}^{\infty}$ be a sequence of bounded linear operators on $E$ . I am trying to prove the following claim: Claim: $T_n \rightarrow T$ in operator norm (i.e. $\lim\limits_{n \to \infty} \|T_n - T\| = 0)$ if and only if $T_n \rightarrow T$ uniformly on every bounded subset of $E$ (i.e. $\lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x) \| = 0$ for every bounded subset $S \subset E$ ). I am having a little trouble with the ""only if"" part of the proof. Here is what I have so far: Proof attempt : ( $\implies$ ) Suppose that $T_n \rightarrow T$ in the operator norm. Let $S \subset E$ be a bounded set and let $x_0 \in E$ . Pick $r > 0$ large enough so that $S$ is contained in the closed ball $B_r(x_0) := \{x \in E: \|x-x_0\| \leq r \}$ . Since the map $x \mapsto \|x\|$ is continuous and $B_r(x_0)$ is compact, there exists an $M > 0$ such that $\|x\| \leq M$ for all $x \in B_{r}(x_0)$ . We then have $$ \|T_n(x) - T(x)\| \leq \|T_n - T \| \|x\| \leq \|T_n - T\| M \quad \forall x \in B_{r}(x_0). $$ Thus, $$\sup_{x \in S} \|T_n(x) - T(x)\| \leq \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| \leq \|T_n - T\| M.  $$ And since $\|T_n - T\| M \rightarrow 0$ as $n \to \infty$ , it follows that $\lim\limits_{n \to \infty} \sup\limits_{x \in S} \|T_n(x) - T(x)\| \to 0$ . So far so good (I think). Now here's the direction where I'm not so sure: $(\impliedby)$ Suppose $T_n \rightarrow T$ on every bounded subset of $E$ . Pick some $x_0 \in E$ . Then for each $r \in \mathbb{N}$ , we have $$ \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0. $$ Then taking the limit in $r$ , we have $$ \lim_{r \to \infty} \lim_{n \to \infty} \sup_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = 0. \hspace{1cm} (*)$$ Now the following would suffice to give the result: The limits in $(*)$ can be interchanged. $\lim\limits\limits_{r \to \infty} \sup\limits_{x \in B_{r}(x_0)} \|T_n(x) - T(x)\| = \sup\limits_{x \in E} \|T_n(x) - T(x)\|$ My main questions: Are 1) and 2) true? For 2) I am somewhat skeptical and am not sure how to rigorously justify interchanging the limits. For 2) I haven't quite proved it yet, but I am pretty confident it is true. (Or am I wrong?) Any insights would be appreciated.","['operator-theory', 'analysis', 'linear-algebra', 'functional-analysis', 'uniform-convergence']"
4515828,Prove $\int_{0}^{\infty}{e}^{a\cos\left(bx\right)}\sin\left(a\sin\left(bx\right)\right)\frac{\mathrm{d}x}{x}=\frac{\pi}{2}\left({e}^{a}-1\right)$,"Show that if $a > 0$ and $b > 0$ that \begin{align}
 \int_{0}^{\infty} {e}^{a \cos \left(b x\right)} \sin \left(a \sin \left(b x\right)\right) \frac{\mathrm{d}x}{x} = \frac{\pi}{2} \left({e}^{a} - 1\right) \\
\end{align} Attempt: I attempted to simplify the integral by putting it into a form of complex exponentials. \begin{align}
 & \int_{0}^{\infty} {e}^{a {e}^{i b x}} \frac{\mathrm{d}x}{x} \\
 = & \int_{0}^{\infty} {e}^{a \cos \left(b x\right) + i a \sin \left(b x\right)} \frac{\mathrm{d}x}{x} \\
 = & \int_{0}^{\infty} {e}^{a \cos \left(b x\right)} \cos \left(\sin \left(b x\right)\right) \frac{\mathrm{d}x}{x} + i \int_{0}^{\infty} {e}^{a \cos \left(b x\right)} \sin \left(a \sin \left(b x\right)\right) \frac{\mathrm{d}x}{x} \\
\end{align} We then have the result by linearity that \begin{align}
 & \int_{- \infty}^{\infty} {e}^{a {e}^{i b x}} - {e}^{a {e}^{- i b x}} \frac{\mathrm{d}x}{x} \\
 & = \int_{- \infty}^{\infty} {e}^{a \cos \left(b x\right)} \left({e}^{i a \sin \left(b x\right)} - {e}^{i a \sin \left(b x\right)}\right) \frac{\mathrm{d}x}{x} \\
 & = 2 i \int_{- \infty}^{\infty} {e}^{a \cos \left(b x\right)} \sin \left(a \sin \left(b x\right)\right) \frac{\mathrm{d}x}{x} \\
 & = 4 i I \\
\end{align} where \begin{align}
 I = \int_{0}^{\infty} {e}^{a \cos \left(b x\right)} \sin \left(a \sin \left(b x\right)\right) \frac{\mathrm{d}x}{x} \\
\end{align} I am confused because when I take the residue at $0$ , I find \begin{align}
 & {\text{Res}}_{z = 0} \frac{{e}^{a {e}^{i b z}} - {e}^{a {e}^{- i b z}}}{z} \\
 & = {e}^{a} - {e}^{a} = 0 \\
\end{align}","['integration', 'residue-calculus', 'improper-integrals', 'trigonometric-integrals']"
4515832,Show that $ \bar{X}_{k_n} ^n \rightarrow e^{cZ} \ \text{in distribution}$,"Suppose $X_1,...,X_n,...$ be positive, i.i.d. random variables. Define $$\bar{X_n} = \frac{1}{n} \sum_{i=1}^{n} X_i$$ . Assume that $E(X_1)=1$ , Var $(X_1)=\sigma^2 < \infty$ . We need to show that if $k_n \rightarrow \infty$ as $n \rightarrow \infty$ , then as $n \rightarrow \infty$ , $$ \bar{X}_{k_n} ^n \rightarrow e^{cZ} \ \text{in distribution}$$ where $Z \sim N(0, \sigma^2)$ , if $\frac{n^2}{k_n} \rightarrow c^2$ . I was trying to look at the MGF by expanding: $$M_{\bar{X}_{k_n} ^n}(t) = 1 + \frac{(\sum_{i=1}^{k_n} X_i)^n}{k_n ^n} .t + \frac{(\sum_{i=1}^{k_n} X_i)^{2n}}{2 k_n ^{2n}}. t^2 + ...$$ But I could not infer anything from here! Other approaches to crack this?","['statistics', 'asymptotics', 'convergence-divergence', 'probability-theory', 'probability']"
4515837,Statement about sigma field generated by two independent random variables,"Let $X_1, X_2, X_3$ be mutually independent random variables from $(\Omega,\mathcal{F},P)$ to $(\mathbb{R},\mathcal{B})$ . Show that $X_3$ is independent of $\sigma(X_1,X_2)$ , i.e., for all $B \in \mathcal{B}$ and $A \in \sigma(X_1,X_2)$ , $P(X_3^{-1}(B) \cap A) = P[X_3^{-1}(B)]P[A]$ . My attempt: We know from the mutual independence of $X_1,X_2,X_3$ that for all $B_1,B_2,B_3 \in \mathcal{B}$ , $$ P[X_1^{-1}(B_1) \cap X_2^{-1}(B_2) \cap X_3^{-1}(B_3)] = P[X_1^{-1}(B_1)]P[X_2^{-1}(B_2)]P[X_3^{-1}(B_3)] $$ Take an arbitrary $B_3 \in \mathcal{B}$ and $A \in \sigma(X_1,X_2)$ . Now, $\sigma(X_1,X_2) = \sigma(\sigma(X_1) \cup \sigma(X_2))$ . I have worked out three cases. First case: $A \in \sigma(X_1) \cup \sigma(X_2)$ . Since $X_3$ and $X_1$ (or $X_2$ ) are independent, $P[X_3^{-1}(B_3) \cap A] = P[X_3^{-1}(B_3)]P[A]$ , and we are done. Second case: $A =C_1 \cap C_2$ for some $C_1 \in \sigma(X_1)$ and $C_2 \in \sigma(X_2)$ . $C_1=X_1^{-1}(B_1)$ for some $B_1 \in \mathcal{B}$ and $C_1=X_1^{-1}(B_2)$ for some $B_2 \in \mathcal{B}$ . Hence, by independence of $X_1,X_2,X_3$ , $$P[X_3^{-1}(B_3) \cap (C_1 \cap C_2)] = P[X_3^{-1}(B_3)]Pr[C_1]Pr[C_2] = P[X_3^{-1}(B_3)]Pr[C_1 \cap C_2]$$ and we are done. Third case: $A =C_1 \cup C_2$ for some $C_1 \in \sigma(X_1)$ and $C_2 \in \sigma(X_2)$ . We can show that $A^c$ and $X_3^{-1}(B_3)$ are independent by case 2. Hence, $A$ and $X_3^{-1}(B_3)$ are independent. $\tag*{$\blacksquare$}$ How do we proceed for a general set $A$ in $\sigma(X_1,X_2)$ ?","['probability-theory', 'random-variables']"
4515854,Why the Kullback-Leibler Divergence is never negative,"The Kullback-Leibler Divergence for the continuous case for two probability densities $p$ and $q$ is $$D_\text{kl}\left(p,q\right) = \int_{x\in\chi}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x$$ . Then, how come that for any choice of $q$ , $D_\text{kl}\left(p,q\right)\geq0$ ? I will attempt to prove by absurd. Suppose that there exists a probability distribution $q$ in which $D_\text{kl}\left(p,q\right)<0$ . An initial guess would be to make $\forall x\in\chi: q(x)>p(x)$ But that doesn't hold since both $p$ and $q$ must have integrals adding up to one: $$\int_{x\in\chi}q(x)\text{d}x>\int_{x\in\chi}p(x)\text{d}x = 1 \unicode{x21af}$$ . So, $p$ and $q$ must intersect at least once, meaning that we can divide the integral into two parts, one in which $p\geq q$ and another with $p<q$ : $$D_\text{kl}\left(p,q\right) = \int_{x\in\chi_{q\geq p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x + \int_{x\in\chi_{q< p}}\left(p(x)\log(p(x)) - p(x)\log(q(x))\right)\text{d}x$$ . On the second term, choose $q$ so that it gets arbitrarily close to $p$ as possible $$\forall x\in\chi_{q<p} : q(x) = p(x)-\epsilon(x)$$ . Whereas in the first term, we want $q\geq p$ . However, if we choose $q>p$ : $$\int_{x\in\chi}q(x)\text{d}x = \int_{x\in\chi_{q\geq p}}q(x)\text{d}x + \int_{x\in\chi_{q<p}}q(x)\text{d}x>\int_{x\in\chi_{q\geq p}}p(x)\text{d}x + \int_{x\in\chi_{q<p}}p(x)\text{d}x - \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x$$ $$\int_{x\in\chi}q(x)\text{d}x>1- \int_{x\in\chi_{q<p}}\epsilon(x)\text{d}x$$ . We have an absurd on the limit $\epsilon(x)\to0$ . So, the only suitable choice for $q$ is $q=p$ .","['measure-theory', 'statistics']"
4515866,How to analytically solve a Second Order ODE? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I got the following ODE in my work. I haven't seen ODEs carrying trigonometric functions this way. See if anyone could help on solving it analytically. Here is the ODE $y''(x)+\sqrt{2} c \tanh \left(\dfrac{c x}{3 \sqrt{2}}\right) y'(x)=0$ where $c>0.$ The boundary conditions could be $y(\infty)=0, y(0)=1.$",['ordinary-differential-equations']
4515892,Can we define addition of numbers which are **NOT** eventually all zero as we go to the left?,"I am struggling to define addition of objects which are similar to decimal-expansions. In this post, we refer to the decimal-expansion-like things as "" wumbers "". Our goal is to write something like: $\forall v,w \in \mathbb{W}$ , $\quad v + w = \text{<SOMETHING>}$ . For any natural numbers, you can pad the left of the associated string with any number of zeros you like. $582 = 000582 = 0000000000000000000582$ However, my wumbers never stop having non-zero digits as you go farther and farther to the left. This is because a wumber is simply a sign ( + or - ) and a function from $\mathbb{Z}$ to the symbols $\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\end{Bmatrix}$ . I am most of the way to defining a reasonable form of addition, but I could use some help filling in the gaps. What is a "" Wumber ""? Definition of Wumber A wumber is an ordered pair $(F, S)$ such that $S$ is one of the symbols ""+"" or `""-"" and $F$ is a function from $\mathbb{Z}$ to the digits $\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\end{Bmatrix}$ such that $\not\exists z \in \mathbb{Z}: \forall Z \geq z, F(Z) = 9 \text{ or } \forall Z \leq z, F(Z) = 9$ . Definition of $\mathbb{W}$ $\mathbb{W}$ denotes the set of all wumbers. Difference Between Wumbers and Decimal Expansions The primary difference between the decimal-expansion of a real-number and a wumber is that the digits of a wumber are not eventually all zero as you go to the left. There exists a wumber $W$ such that $\sum_{z \in \mathbb{Z}}^{\text{ }} 10^{z}*W(z)$ is not defined because the coefficient for very large powers of $10$ is not zero. Definition of the degree of a Wumber Let $\text{deg}$ be a mapping from the set of all wumbers $\mathbb{W}$ to the set $\mathbb{Z} \cup \begin{Bmatrix} \infty \end{Bmatrix}$ is defined as follows: $\forall W \in \mathbb{W}$ , $\qquad \text{deg}(W) = $ $\qquad \qquad \begin{cases}
\infty,  & \text{if  } \forall n \in \mathbb{Z} \exists m \in \mathbb{Z}: W(m) \neq 0 \\
\text{min} \begin{Bmatrix} n \in \mathbb{Z}: \forall m \geq n, \quad W(m) = 0 \end{Bmatrix}  & \text{otherwise }
\end{cases}$ Converting Wumbers to Real Numbers $(W^{\mathbb{R}})$ Let $W \in \mathbb{W}$ . If $\exists d \in \mathbb{Z}$ such that $d = \text{deg}(W)$ then $W^{\mathbb{R}} = \sum_{z \in \mathbb{Z}}^{\text{ }} 10^{z}*W(z)$ $W^{\mathbb{R}}$ is the sum of $10^{k}*W(k)$ taken over all $k \in \mathbb{Z}$ Also, wumber $W$ is said to be a real wumber . Converting Real Numbers to Wumbers $(x^{\mathbb{W}})$ For any real number $x$ , $x^{\mathbb{W}}$ is the unique wumber $W$ such that $W^{\mathbb{R}} = x$ Definition of "" Natural Wumber "" For any real wumber $W \in \mathbb{W}$ , $W$ is natural if and only if $W^{\mathbb{R}}$ is a natural number. How do we Add Wumbers? An answer to this question is a non-trivial definition of addition which extends addition of real-numbers to $\mathbb{W}$ . $\forall V, W \in \mathbb{W}$ if $V$ and $W$ are real, then $V + W = V^{\mathbb{R}} + W^{\mathbb{R}}$ Example $\qquad$ Let $W = (``+"", F)$ such that $F$ is a mapping from $\mathbb{Z}$ to the digits $\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\end{Bmatrix}$ such that: $\qquad\qquad$ $F[0] = 5$ $\qquad\qquad$ $F[1] = 1$ $\qquad\qquad$ $F[-1] = 2$ $\qquad\qquad$ $F[a] = 0 \forall a \in \mathbb{Z}: \geq 2$ $\qquad\qquad$ $F[b] = 0 \forall b \in \mathbb{Z}b \leq -2$ Then, $F^{\mathbb{R}} = 15.2$ $\qquad$ Let $V = (""-"", G)$ such that $G$ is a mapping from $\mathbb{Z}$ to the digits $\begin{Bmatrix} 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\end{Bmatrix}$ such that: $\qquad\qquad$ $G[0] = 2$ $\qquad\qquad$ $G[1] = 4$ $\qquad\qquad$ $G[-1] = 6$ $\qquad\qquad$ $G[a] = 0$ $\forall a \in \mathbb{Z}: \geq 2$ $\qquad\qquad$ $G[b] = 0$ $\forall b \in \mathbb{Z}b \leq -2$ Then, $V^{\mathbb{R}} = 42.6$ So, $V + W = V^{\mathbb{R}} + W^{\mathbb{R}} = 42.6 + 15.2 = 57.8$ . The issue is when the co-efficient on large powers of $10$ are never eventually always zero... If you add a non-real wumber $V$ to a real wumber $W$ to produce non-real wumber $X$ then for all sufficiently large indices $k$ , the $X[k] = V[k]$ Suppose that we have $F$ from $\mathbb{Z}$ to the digits such that: $F(k) = 0$ for negative indices $k$ $F(k) = 0$ for odd positive indices $k > 0$ $F(k) = 1$ for even non-negative indices $k \geq 0$ Then $W = (""+"", F(k))$ is a non-real wumber . $W = \dots 101010101 \dots 10101.00000 \dots 0000 \dots$ You can add a number like $500$ or $\pi$ to a non-real wumber. $W + 500 + \pi = \dots 101010101 \dots 1010604.14519 \dots [\text{ more digits of } \pi] \dots$ After we define how to add a real number to a wumber we could define how to add any arbitrary pair of wumbers . If I ask you for the $k^{\text{th}}$ element of $V + W$ you should be able to: truncate $V$ at index $p$ truncate $W$ at index $p$ add the truncated wumbers together say that $\forall k, p \in \mathbb{Z}$ and $\forall V, W \in \mathbb{W}$ , $(V + W)[k] = T(V, p)[k] + T(W, p)[k]$ . The truncation $T$ of wumber $V$ at index $k$ has the properties: $T(V, k)[z] = V[z]$ for all $z \in \mathbb{Z}$ if $z \leq k$ $T(V, k)[z] = 0$ for all $z \in \mathbb{Z}$ if $z > k$ $T(V, k)$ is a real-wumber even if $V$ is a non-real wumber . Existence of a something we will call a "" Common Additive "" Instead of greatest-common-factor we could have a "" greatest-common-additive "". I am thinking that it is probably the case that $\forall V, W \in \mathbb{W}$ , if $V$ and $W$ are non-real then $\exists X \in \mathbb{W}$ such that: $X$ is a non-real wumber. $V = X + V^{\prime}$ $W = X + W^{\prime}$ $V^{\prime}$ is a real wumber $W^{\prime}$ is a real wumber Then, $V + W = (2*X) + (V^{\prime} + W^{\prime})$ $(2*X)$ is a non-real wumber. $(V^{\prime} + W^{\prime})$ is a real wumber. So, adding two non-real wumbers can be expressed as adding a real wumber to a non-real wumber . I am not sure how we would define $(2*X)$ for non-real $X$ . Maybe if $X$ was a ""binary wumber"" (a mapping from $\mathbb{Z}$ to $\begin{Bmatrix} 0, 1\end{Bmatrix}$ then $\forall k \in (2*X)(k) = X(k - 1)$ How can we define the addition of two non-real wumbers ? Was there a Question in There Somewhere ? An answer to this question is a non-trivial definition of addition which extends addition of real-numbers to $\mathbb{W}$ . How do you add two decimal expansions together when there exists non-zero co-efficient for $10^{k}$ where $k$ is very large.","['real-numbers', 'number-theory', 'combinatorics-on-words', 'decimal-expansion', 'sequences-and-series']"
4515906,"determine whether there exists $(x_1,\cdots, x_n)$ so that $x_{i+1} \in \{2x_i, 2x_i - 1, 2x_i - n, 2x_i - n-1\}\,\forall i$","Let $n$ be an even positive integer. Determine whether there exists a permutation $(x_1,\cdots, x_n)$ of $\{1,\cdots, n\}$ so that $x_{i+1} \in \{2x_i, 2x_i - 1, 2x_i - n, 2x_i - n-1\}\,\forall i$ (here $x_{n+1} = x_1$ ). It may be useful to find an Eulerian circuit in a directed multigraph, which is guaranteed provided the indegree of each vertex equals its outdegree. But I'm not sure how to find this multigraph. Write $n = 2m$ . We label the edges of the graph as $1,\cdots, 2m$ and the vertices as $1,\cdots, m$ . We want to ensure each edge appears in the Eulerian circuit exactly once. I think one should be able to let $x_i$ be the label of the $i$ th edge in the Eulerian circuit once the graph is defined properly. It seems in the answer below that there are two outgoing arcs from each vertex, because there's always two values that are ""in range."" For instance, if $1\leq v\leq m/2$ , then $2v-m < 1$ , so only $2v$ and $2v-1$ are between $1$ and $n=2m$ . If $v>m/2 + 1,$ then $2v-m$ and $2v-m-1$ are in range. If $v=(m+1)/2, 2v - 1 = m$ and $2v-m$ are in range. Edit: I have several questions about the answer posted below: If the indegree equals the outdegree for each vertex and the graph is connected, doesn't the graph have an Eulerian cycle? Also, how would one prove this (e.g. by induction on the number of edges perhaps). Why are the incoming arcs to vertex $v$ precisely $(\lfloor (v+1)/2 \rfloor , v), (\lfloor (v+m+1)/2\rfloor, v)$ ? From my understanding, for an incoming arc $(w,v)$ , we need $2w =v, 2w - 1 = v, 2w - m = v$ or $2v - m-1=v$ . If $v$ is odd, then we must have $2w-1=v$ or one of $2v-m-1=v, 2v - m = v$ , depending on whether $m$ is odd or not. Doesn't one require the digraph to be (weakly) connected for all m? And to prove this, perhaps one could show all vertices are reachable from vertex $(1,1+m)$ ? To restore a permutation from the Eulerian cycle, when exactly do we take the number $v$ and when do we take the number $v+m$ ? A small example might help (e.g. $m=3, n=6$ ). Also, just for additional info, I've added a proof of the weak connectivity of the graph (which I'm pretty sure should work regardless of the parity of m). We'll prove the weak connectivity by induction on the vertex number. Let the vertices be $v_1,\cdots, v_m$ , where $v_i$ represents the pair $(i,i+m)$ . We'll prove that $v_1$ can reach all vertices. For clarity, we let vertex $v_i$ be connected to vertex $v_{2i-1}, v_{2i}, v_{2i-m-1}, v_{2i-m},$ depending on which indices are in range. $v_1$ is connected to $v_1$ by definition. Assume the result holds for all smaller indices. We want to show it holds for $v_j$ where $1<j\leq m$ . Note that $v_j$ has in-neighbours $v_{\lfloor (j+1)/2\rfloor}$ and $v_{\lfloor (j+m+1)/2\rfloor}$ . $v_{\lfloor (j+1)/2\rfloor}$ is connected (by a directed path) to $v_1$ by the inductive hypothesis as $\lfloor (j+1)/2\rfloor \leq (j+1)/2 < j$ , and so $v_j$ is also reachable from $v_1$ , as required.","['contest-math', 'permutations', 'graph-theory', 'combinatorics', 'discrete-mathematics']"
4515942,What is the average annual increase in the pollination rate?,"This is an SAT practice-test question : The graph above shows the pollination rate of plants in a forested area every two years. Based on the trend line, what is the average annual increase in the pollination rate? a) $5\%$ b) $3.8\%$ c) $2.5\%$ d) $1.5\%$ I first calculated the total percentage increase from $2000$ to $2014$ $$90/53 = 1.698113...$$ Since I am looking for the average annual increase $A,$ I then find the total increase during $14$ years $$A^{14} = 1.698113.$$ Thus option (b) $A = 1.038547$ is my answer, but the answer key says that the answer is option (c). Is the answer key wrong, or have I made a mistake?","['algebra-precalculus', 'graphing-functions', 'percentages']"
4515968,construct the normaliser of a subgroup and then construct the subgroup,"There is a maximal nontoral (not contained in a conjugate of a fixed maximal torus) elementary abelian $2$ -subgroup of rank 6 in $G = PGL(8,7)$ . I denote this group by $A$ . Its normaliser $N_{G}(A)$ is $A.N$ where $N$ is a maximal subgroup of $Sp(6,2)$ of order $40320$ . And this $A.N$ is itself a subgroup of a Class 8 maximal subgroup of $G$ and is isomorphic to $SO_{8}^{+}(7).2$ , I believe. I've been trying to construct this $A$ in $G$ for two days. The following is what I did but didn't work. > d:=ClassicalMaximals(""L"",8,7:general:=true,classes:={8});
     > X:=GL(8,7);
     > G:=PGL(8,7);
     > ro:=hom<X->G|G.1,G.2>;
     > m3:=ro(d[3]);m1:=ro(d[1]);
     > SUBOFM3:=Subgroups(m3:OrderEqual:=2580480);

     >> SUBOFM3:=Subgroups(m3:OrderEqual:=2580480);
                 ^
     Runtime error in 'Subgroups': Cannot compute subgroups of all composition
     factors of this group. I intended to get this $A.N$ and then use the command of pCore to find A. I suppose I need to narrow down the searching range a bit... If I get the Sylow $2$ -sub of $m3$ and then try to find elementary abelian subgroups of order $2^6$ , it'd be killed by Magma. Any suggestions would be greatly appreciated!","['classical-groups', 'maximal-subgroup', 'finite-groups', 'magma-cas', 'group-theory']"
4516013,If a and b are the distinct roots of the equation $x^2+3^{1/4}x + 3^{1/2} =0$ then find the value of $a^{96}(a^{12}-1)+b^{96}(b^{12}-1)$.,"If $a$ and $b$ are the distinct roots of the equation $x^2+3^{1/4}x +
 3^{1/2} =0$ then find the value of $a^{96}(a^{12}-1)+b^{96}(b^{12}-1)$ My attempt:
LHS = $(a^{108}+b^{108})-(a^{96}+b^{96})$ $a^{n} + b^{n} = -[3^{1/4}(a^{n-1}+b^{n-1})+3^{1/2}(a^{n-2}+b^{n-2})]$ $a^{108} + b^{108} = -[(3^{1/4}(a^{107}+b^{107})+3^{1/2}(a^{106}+b^{106})]$ I suppose i could repeat this process until every term in is terms of $a^{96}$ and $b^{96}$ but that would be very tedious","['algebra-precalculus', 'roots', 'polynomials']"
4516025,How does the pushforward of the inverse metric relate to the inverse of the pullback metric for an embedding?,"I am learning some geometry and stumbled upon these two ways to obtain a different metric. For a smooth manifold embedding $\phi:N\to M$ suppose a non-degenerate, covariant metric $g_{ij}$ on the tangent space $TM$ which induces a pullback metric $g'_{i'j'}$ on the tangent space $TN$ by $\quad g'_{i'j'}=\dfrac{\partial \phi^{i}}{\partial x^{i'}}\,g_{ij}\,\dfrac{\partial \phi^{j}}{\partial x^{j'}}=[\mathrm J_\phi]^{i}_{i'}\,g_{ij}\,[\mathrm J_\phi]^{j}_{j'}\quad$ (components) $\quad [g']=[\mathrm J_\phi]^T\,[g]\,[\mathrm J_\phi]\quad$ (matrix) $\quad g'=\phi^*\,g\quad$ (geometric) where $\mathrm J_\phi$ is the Jacobian of $\phi$ . On $T^*M$ we have that $g_{ij}$ also induces a contravariant inverse metric $h^{kl}$ via $\quad g_{ij}\,h^{jl}=\delta_i^l\quad$ (components) $\quad [g][h]=[h][g]=\mathrm I\quad$ (matrix) $\quad [h] = [g]^{-1}\quad$ (matrix inverse) Further, on the codomain $\phi(N)$ of $\phi$ , this inverse metric $h$ can be pushed forward along $\phi^{-1}$ by $\quad h'^{k'l'}=\dfrac{\partial (\phi^{-1})^{k'}}{\partial x^{k}}\,h^{kl}\,\dfrac{\partial (\phi^{-1})^{l'}}{\partial x^{l}}=[\mathrm J_{\phi^{-1}}]^{k'}_{k}\,h^{kl}\,[\mathrm J_{\phi^{-1}}]^{l'}_{l}\quad$ (components) $\quad [h']=[\mathrm J_{\phi^{-1}}]\,[h]\,[\mathrm J_{\phi^{-1}}]^T\quad$ (matrix) $\quad h'={\phi^{-1}}_*\,h\quad$ (geometric) where the Jacobian $\mathrm J_{\phi^{-1}}$ can be obtained as the Moore-Penrose pseudoinverse ${\mathrm J_{\phi^{-1}}=(\mathrm J_{\phi}^T\,\mathrm J_{\phi}^{\vphantom{T}})^{-1}\,\mathrm J_{\phi}^T}$ . ( Update: here lies the mistake! As pointed out in the comments, this definition of the Moore-Penrose pseudoinverse does an orthogonal projection w.r.t. the standard metric and not w.r.t. the metric $g$ .) Now, it seems that for embeddings, this pushforward inverse metric $h'$ and the matrix inverse of the pullback metric $g'$ generally do not agree (given I have made no mistakes in my trials where they do agree for diffeomorphisms): $\quad g'_{i'j'}\,h'^{j'l'}\neq\delta_{i'}^{l'}\quad$ (components) $\quad [h'] \neq [g']^{-1}\quad$ (matrix) $\quad {\phi^{-1}}_*(g^{-1})\neq(\phi^*\,g)^{-1}\quad$ (geometric) Q: Which metric, the pushforward of the inverse metric or the inverse of the pullback metric is ""used"" on $T^*N$ ? (...and what is it used for in physics?) On one hand, the pushforward of the inverse metric ${\phi^{-1}}_*\,h$ preserves inner products on covectors from the reachable subspace ${\{\omega\in T_{\phi(p)}^* M\,|\,\phi^{-1\,*}(\phi^*\,(\omega))=\omega\}}$ . When $\mathrm P_\phi$ is a projection onto that subspace $\quad [\mathrm P_\phi]^i_k=[\mathrm J_{\phi^{\vphantom{-1}}}]^i_{i'}\; [\mathrm J_{\phi^{{-1}}}]^{i'}_k\quad$ (components) $\quad [\mathrm P_\phi]=[\mathrm J_{\phi^{\vphantom{-1}}}]\; [\mathrm J_{\phi^{{-1}}}]\quad$ (matrix) $\quad \mathrm P_\phi(\omega)=\phi^{-1\,*}(\phi^*\,(\omega))\quad$ (geometric) then we obtain $\quad \hphantom{={}}{\langle\, \mathrm P_\phi(\omega)\,,\,\mathrm P_\phi(\mu)\,\rangle}_h$ $\quad =(\omega_i\;[\mathrm P_\phi]^i_k)\;h^{kl}\;([\mathrm P_{\phi}]^{j}_l\;\mu_j)$ $\quad =(\omega_i\;[\mathrm J_{\phi^{\vphantom{-1}}}]^i_{i'}\; [\mathrm J_{\phi^{{-1}}}]^{i'}_k)\;h^{kl}\;([\mathrm J_{\phi^{{-1}}}]^{j'}_l\;[\mathrm J_{\phi^{\vphantom{-1}}}]^j_{j'}\;\mu_j)$ $\quad =(\omega_i\;[\mathrm J_{\phi^{\vphantom{-1}}}]^i_{i'})\; ([\mathrm J_{\phi^{{-1}}}]^{i'}_k\;h^{kl}\;[\mathrm J_{\phi^{{-1}}}]^{j'}_l)\;([\mathrm J_{\phi^{\vphantom{-1}}}]^j_{j'}\;\mu_j)$ $\quad =(\phi^*\,\omega)_{i'}\;({\phi^{-1}}_*\,h)^{i'j'}\;(\phi^*\,\mu)_{j'}$ $\quad ={\langle\,\phi^*(\omega)\,,\,\phi^*(\mu)\,\rangle}_{{\phi^{-1}}_*\,h}\quad$ (desirable property 1) On the other hand, the inverse of the pullback metric $(\phi^*g)^{-1}$ or ${g'}^{i',j'}$ can be used for raising and lowering indices in invariant expressions $\quad\langle u,v\rangle_{g'} =u^{i'}\,g'_{i'j'}\,v^{j'}=u_{k'}\,g'^{k',i'}\,g'_{i'j'}\,v^{j'}=u_{k'}\,v^{j'}\quad$ (desirable property 2) I think that both properties, preservation of covector inner products and preservation of invariants when raising and lowering indices are desirable. So currently my questions are Is there any advice how to deal with these two different inverse metrics on $T^*N$ ?
E.g., do we just have both of them and use one for inner products and the other one for raising and lowering? Which one is used to define the hodge operator on $T^*N$ then? Where else is preservation of the covector inner product necessary? Edit: I have previously displayed the pushforward along $\phi^{-1}$ wrong, it is updated to be ${\phi^{-1}}_*\,h$","['inner-products', 'pushforward', 'pullback', 'physics', 'differential-geometry']"
4516059,Who said differential dx is a very small quantity?,"Given a differentiable function $f(x)$ in a point $x_0$ , by definition, the differential $df$ is the difference in ordinates of the tangent line to curve in $x_0$ , evaluated at the point $x_0 + dx$ .
Now, $dx$ can be small or big, and if it is small enough, we know that this difference approximates: $f(x_0 + dx) - f(x_0)$ .
Question is: it's $dx$ just a quantity which can be taken big or small? (often taken small enough in applications)?","['integration', 'derivatives']"
4516060,The sigma algebra generated by a stochastic process is equivalently generated by finite intersections.,"I am reading a stochastic analysis script and in one proof the following 'fact' is used. Let $(\Omega, \mathcal{F}, \mathbb{P})$ be a probability space, $I \neq \emptyset$ an index set and $(S, \mathcal{S})$ a measurable space. For a process $X = (X_i)_{i \in I}$ consisting of $S$ valued $\mathcal{F}-\mathcal{S}-$ measurable random variables the following is claimed (without proof): Let $\mathcal{I} = \{\bigcap_{j \in J} X_j^{-1}(A_j)| J \subset I \text{ finite }, A_j \in \mathcal{S}\}$ , then $\sigma(\mathcal{I}) = \sigma(X_i, i \in I) (= \sigma( \bigcup_{i \in I} X_i^{-1}(\mathcal{S})))$ . The interesting direction is to show that every set $\bigcup_{i \in I} X_i^{-1}(A_i)$ , with $A_i \in \mathcal{S}$ is an element of $\sigma(\mathcal{I})$ . For uncountable $I$ , I wanted to proof it myself, but is this even correct? For example: Consider $(\Omega, \mathcal{F}, \mathbb{P}) = ([0,1], \mathcal{B}, \lambda)$ and $(S, \mathcal{S}) = (\mathbb{R}, \mathcal{B})$ and $X_i = 1_{\{i\}}$ , for $i \in I$ , where $I$ is some nonmeasureable subset of $[0,1]$ . Now, we have $\sigma(\mathcal{I}) \subset \mathcal{B}$ , but $\bigcup_{i \in I} X_i^{-1}(\{1\}) = \bigcup_{i \in I} \{i\}  = I \in \sigma(X_i, i \in I)$ . So, according to the above claim, we would have $I \in \mathcal{B}$ which is false. Am I missing something? If I am, how would you actually prove it?","['stochastic-analysis', 'stochastic-processes', 'measure-theory', 'probability']"
4516075,Why finite subgroups of a virtually cyclic group have bounded order?,"Let $G$ be an infinite, virtually cyclic group, i.e., $G$ has an infinite cyclic subgroup $H$ of finite index. I have a question: Why is there a finite bound on the orders of finite subgroups of $G$ ? What I tried : If $G$ is finite, then $|G|$ is a bound on the orders of its subgroups. But I don't know why is true about an infinite virtually cyclic group.",['group-theory']
4516081,Measure Theory - Modes of convergence of $f_n(x) = (1/x)(\sin(x))^n$,"Question: Consider the sequence of functions $f_n(x) = \frac{1}{x}(\sin(x))^n$ on $(0, \infty)$ , equipped with the Borel σ−algebra and Lebesgue measure $λ$ . Does it hold that $f_n \to 0$ as $n \to \infty$ for each of the following types of convergence: a) pointwise?
b) pointwise a.e?
c) in measure?
d) in $L^p$ for $1 < p < \infty$ ?
e) in $L^\infty$ ? My attempt: a) is False since for for example the x for which $\sin(x)=1$ , then $f_n$ does not converge to $0$ . b) is True because the reason a) was False is that it fails to converge on only a set of measure $0$ since it was countable. I'm struggling with c), d), and e). My thought on c) was to use that $\sin(x)/x$ is bounded by $x$ , and then it would mean that $\sin(x)^{n-1}$ would go to zero a.e. so it converges in measure, but I don't know if that is correct to think like that. On d) and e) I have no idea. Any suggestions for how to solve c),d), and e)?",['measure-theory']
4516087,Why is differentiability defined on multivariable functions this way?,"I am told that a function $f:\mathbb{R}^{n}\to \mathbb{R}$ is differentiable at a point $\mathbf{x}=(x_1,x_2,...,x_n)$ if $\Delta f$ is of form $$\sum_{i}^{n}\frac{\partial f}{\partial x_i}\Delta x_i+\sum_{i}^{n} \epsilon_i \Delta x_i$$ Where $$\lim_{\mathbf{\Delta x}\to \mathbf{0}}\epsilon_{i}=0$$ for every $1\leq i\leq n$ . It seems arduous to me to use such a definition. Why don't mathematicians define differentiability by the existence of partial derivatives, which seems natural? Now, I am aware that I can define things how I want to, but my goal is to understand why it is commonly defined this way. Are there some underlying properties that make this definition superior?","['multivariable-calculus', 'calculus']"
4516095,where formulas 1 and 2 came from?,"I am reading a textbook on differential equations.The chapter I am studying now is about solving differential equations using series.The book solves Legendre's differential equation $$(1-x^{2})y''-2xy'+v(v+1)y=0$$ using the series method. First, it assumes that the equation has a solution in the form of $$y=c_{0}+c_{1}x+c_{2}x^{2}+\cdots+c_{n}x^{n}+\cdots$$ Then takes the first and second derivatives of $y$ and puts them in the equation $(1-x^{2})y''-2xy'+v(v+1)y=0$ to get the coefficients $c_{i}$ .
we have $$y'=c_{1}+2c_{2}x+3c_{3}x^{2}+\cdots+nc_{n}x^{n-1}\cdots$$ and $$y''=2c_{2}+3\times 2c_{3}x+4\times 3c_{4}x^{2}+\cdots+n(n+1)c_{n}x^{n-2}+\cdots$$ Now put these expressions in the equation $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ 2c_{2}+3\times 2c_{3}x+4\times 3c_{4}x^{2}+\cdots+(n+2)(n+1)c_{n+2}x^{n}+\cdots$$ $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -2c^{2}x^{2}-\cdots-n(n-1)c_{n}x^{n}-\cdots$$ $$\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ -2c_{1}x-2\times 2c_{2}x^{2}-\cdots-2nc_{n}x^{n}-\cdots$$ $$+v(v+1)c_{0}+v(v+1)c_{1}x+v(v+1)c_{2}x^{2}+\cdots+v(v+1)c_{n}x^{n}+\cdots=0$$ Equating the coefficient of each power of x to zero, we have $$2c_{2}+v(v+1)c_{0}=0\Rightarrow c_{2}=-{{v(v+1)}\over{2}}c_{0}$$ $$3\times 2c_{3}-2c_{1}+v(v+1)c_{1}=0\Rightarrow c_{3}=-{{(v-1)(v+2)}\over{3!}}c_{1}$$ $$\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots\cdots$$ $$(n+2)(n+1)c_{n+2}+\left[{-n(n-1)-2n+v(v+1)}\right]c_{n}=0\Rightarrow c_{n+2}=-{{(v-n)(v+n+1)}\over{(n+2)(n+1)}}c_{n}$$ The last relation is a recursive relation. Using the recursive relation, all odd coefficients are expressed in terms of $c_{1}$ and all even coefficients are expressed in terms of $c_{0}$ and $c_{0}$ and $c_{1}$ are arbitrary constants.So we have: $$c_{4}=-{{(v-2)(v+3)}\over{4\times 3}}c_{2}={{(v-2)v(v+1)(v+3)}\over{4!}}c_{0}$$ $$c_{5}=-{{(v-3)(v+4)}\over{5\times 4}}c_{3}={{(v-3)(v-1)(v+2)(v+4)}\over{5!}}c_{1}$$ And the rest of the coefficients are calculated in the same way. By inserting these expressions for the coefficients into $y=c_{0}+c_{1}x+c_{2}x^{2}+\cdots+c_{n}x^{n}+\cdots$ we obtain $$y=c_{0}(1-{{v(v+1)}\over{2!}}x^{2}+{{(v-2)v(v+1)(v+3)}\over{4!}}x^{4}-+\cdots )$$ $$\ \ \ +c_{1}(x-{{(v-1)(v+2)}\over{3!}}x^{3}+{{(v-3)(v-1)(v+2)(v+4)}\over{5!}}x^{5}-+\cdots )$$ We notice that the general solution is in the form $$y(x)=c_{0}R_{v}(x)+c_{1}S_{v}(x)$$ where $R_{v}(x)$ and $S_{v}(x)$ are two linearly independent solutions.
If $v=n$ (a non-negative integer), then according to the recursive relation $c_{n+2}=-{{(v-n)(v+n+1)}\over{(n+2)(n+1)}}c_{n}$ , we have $$c_{n+2}=c_{n+4}=\cdots=0$$ As a result, when n is even, the series $R_{n}(x)$ is a polynomial of degree n and $S_{n}(x)$ will be a series, and when $n$ is odd, the series $S_{n}(x)$ will be a polynomial of degree $n$ and $R_{n}(x)$ will be a series. Therefore, for every non-negative integer n, either $R_{n}(x)$ or $S_{n}(x)$ (and not both) is a polynomial of degree n. These polynomials, after being multiplied by some constants, are called Legendre polynomials and are denoted by $P_{n}(x)$ . We write the recursive relation $c_{n+2}=-{{(v-n)(v+n+1)}\over{(n+2)(n+1)}}c_{n}$ in the form $$c_{n}=-{{(n+2)(n+1)}\over{(v-n)(v+n+1)}}c_{n+2}$$ Then we obtain all the non-zero coefficients in terms of $c_{v}$ . In this case, $c_{v}$ is an arbitrary constant and it is customary to choose $$c_{v}={{(2v)!}\over{2^{v}(v!)^{2}}}={{1\times 3\times 5\times\cdots\times (2v-1)}\over{v!}}$$ Now, according to relation $c_{n}=-{{(n+2)(n+1)}\over{(v-n)(v+n+1)}}c_{n+2}$ , we have: $$c_{v-2}=-{{v(v-1)}\over{2(2v-1)}}c_{v}=-{{v(v-1)}\over{2(2v-1)}}{{(2v)!}\over{2^{v}(v!)^{2}}}$$ $$=-{{v(v-1)(2v)(2v-1)(2v-2)!}\over{2(2v-1)2^{v}v(v-1)!v(v-1)(v-2)!}}$$ $$=-{{(2v-2)!}\over{2^{v}(v-1)!(v-2)!}}$$ And in a similar way $$c_{v-4}=-{{(v-2)(v-3)}\over{4(2v-3)}}c_{v-2}={{(v-2)(v-3)}\over{4(2v-3)}}{{(2v-2)!}\over{2^{v}(v-1)!(v-2)!}}$$ $$={{(2v-4)!}\over{2^{v}2!(v-2)!(v-4)!}}$$ and in the general case for $v-2k\geq 0$ we have $$c_{v-2k}=(-1)^{k}{{(2v-2k)!}\over{2^{v}k!(v-k)!(v-2k)!}}$$ and assuming that $v$ is a non-negative integer like $n$ , the resulting solution is called a Legendre polynomial of degree $n$ and we denote it by $P_{n}(x)$ and according to the relation $c_{v-2k}=(-1)^{k}{{(2v-2k)!}\over{2^{v}k!(v-k)!(v-2k)!}}$ , we have $$P_{n}(x)=\sum\limits_{k=0}^{\left[{{{n}\over{2}}}\right]}{(-1)^{k}{{(2n-2k)!}\over{2^{n}k!(n-k)!(n-2k)!}}}x^{n-2k}$$ So the general solution of a Legendre equation with $v=n$ is $$y=a_{1}P_{n}(x)+a_{2}Q_{n}(x)$$ where $P_{n}(x)$ is a polynomial of degree n which is calculated by the formula $P_{n}(x)=\sum\limits_{k=0}^{\left[{{{n}\over{2}}}\right]}{(-1)^{k}{{(2n-2k)!}\over{2^{n}k!(n-k)!(n-2k)!}}}x^{n-2k}$ and q is an infinite series. We call $P_{n}(x)$ 's Legendre polynomials and $Q_{n}(x)$ 's second type Legendre functions. So far everything is clear but after this book says:
As we said before, the general solution of Legendre's equation is in general form $$y(x)=c_{0}\left[{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{\infty}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it v}-{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm ))...(}{\it v}-{\rm 2}{\rm )}{\it v}{\rm (}{\it v}+{\rm 1}{\rm )(}{\it v}+{\rm 3}{\rm )...(}{\it v}+{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))}}\over{{\rm (}{\rm 2}{\it k}{\rm )!}}}}{\it x}^{{\rm 2}{\it k}}}\right]$$ $$\ \ \ \ \ \ \ \ \ +c_{1}\left[{{\rm x}+\sum\limits_{{\it k}{\rm =1}}^{\infty}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it v}-{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))...(}{\it v}-{\rm 3}{\rm )(v}{\rm -}{\rm 1)(}{\it v}+2{\rm )(}{\it v}+4{\rm )...(}{\it v}+2k{\rm )}}\over{{\rm (}{\rm 2}{\it k+1}{\rm )!}}}}{\it x}^{{\rm 2}{\it k+1}}}\right]$$ $$=c_{0}R_{v}(x)+c_{1}S_{v}(x)$$ and when $v=n$ ( $n$ a non-negative integer) either $R_{n}(x)$ or $S_{n}(x)$ is a polynomial of degree $n$ and as it was said before, we denote these polynomials with $P_{n}(x)$ . $P_{n}(x)$ can be obtained from the following equation: $$P_{n}(x)=\left\{{\matrix{
{{{R_{n}(x)}\over{R_{n}(1)}},even\ n}\cr
{{{S_{n}(x)}\over{S_{n}(1)}},\ odd\ n}\cr
}}\right.\ \ \ \ \ \ \ \ \ \ \ \ \ (1)$$ And the other solution, which is an infinite series and is denoted by $Q_{n}(x)$ , can be obtained from the following relation: $$Q_{n}(x)=\left\{{\matrix{
{R_{n}(1)S_{n}(x)\ \ \ ,even\ n}\cr
{-S_{n}(1)R_{n}(x),\ \ \ odd\ n}\cr
}}\right.\ \ \ \ \ \ \ \ \ \ \ \ \ (2)$$ This is where I run into trouble. The book does not explain where formulas 1 and 2 came from. These formulas are equivalent to say that for even ${\it n}$ , Legendre polynomials ${\it P}_{{\it n}}{\rm (}{\it x}{\rm )}$ are obtained from this formula: $${{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{{{{\it n}}\over{{\rm 2}}}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it n}-{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm ))...(}{\it n}-{\rm 2}{\rm )}{\it n}{\rm (}{\it n}+{\rm 1}{\rm )(}{\it n}+{\rm 3}{\rm )...(}{\it n}+{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))}}\over{{\rm (}{\rm 2}{\it k}{\rm )!}}}}{\it x}^{{\rm 2}{\it k}}}\over{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{{{{\it n}}\over{{\rm 2}}}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it n}-{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm ))...(}{\it n}-{\rm 2}{\rm )}{\it n}{\rm (}{\it n}+{\rm 1}{\rm )(}{\it n}+{\rm 3}{\rm )...(}{\it n}+{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))}}\over{{\rm (}{\rm 2}{\it k}{\rm )!}}}}}}={\it P}_{{\it n}}{\rm (}{\it x}{\rm )}$$ (provided that ${\it n}$ be even) for example: $${\it P}_{{\rm 4}}{\rm (}{\it x}{\rm )}={{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{{\rm 2}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it 4}-{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm ))...(}{\it 4}-{\rm 2}{\rm )}{\it 4}{\rm (}{\it 4}+{\rm 1}{\rm )(}{\it 4}+{\rm 3}{\rm )...(}{\it 4}+{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))}}\over{{\rm (}{\rm 2}{\it k}{\rm )!}}}}{\it x}^{{\rm 2}{\it k}}}\over{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{{\rm 2}}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it 4}-{\rm (}{\rm 2}{\it k}-{\rm 2}{\rm ))...(}{\it 4}-{\rm 2}{\rm )}{\it 4}{\rm (}{\it 4}+{\rm 1}{\rm )(}{\it 4}+{\rm 3}{\rm )...(}{\it 4}+{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))}}\over{{\rm (}{\rm 2}{\it k}{\rm )!}}}}}}={{{\rm 1}+{{-{\rm 1}{\rm \times}{\rm 4}{\rm \times}{\rm 5}}\over{{\rm 2}{\rm !}}}{\it x}^{{\rm 2}}+{{{\rm 2}{\rm \times}{\rm 4}{\rm \times}{\rm 5}{\rm \times}{\rm 7}}\over{{\rm 4}{\rm !}}}{\it x}^{{\rm 4}}}\over{{\rm 1}+{{-{\rm 1}{\rm \times}{\rm 4}{\rm \times}{\rm 5}}\over{{\rm 2}{\rm !}}}+{{{\rm 2}{\rm \times}{\rm 4}{\rm \times}{\rm 5}{\rm \times}{\rm 7}}\over{{\rm 4}{\rm !}}}}}={{{\rm 1}-{\rm 10}{\it x}^{{\rm 2}}+{{{\rm 35}}\over{{\rm 3}}}{\it x}^{{\rm 4}}}\over{{\rm 1}-{\rm 10}+{{{\rm 35}}\over{{\rm 3}}}}}={{{\rm 3}}\over{{\rm 8}}}{\rm (}{\rm 1}-{\rm 10}{\it x}^{{\rm 2}}+{{{\rm 35}}\over{{\rm 3}}}{\it x}^{{\rm 4}}{\rm )}={{{\rm 1}}\over{{\rm 8}}}{\rm (}{\rm 3}-{\rm 30}{\it x}^{{\rm 2}}+{\rm 35}{\it x}^{{\rm 4}}{\rm )}$$ and for odd ${\it n}$ ,    Legendre polynomials ${\it P}_{{\it n}}{\rm (}{\it x}{\rm )}$ are obtained from this formula: $${{{\rm x}+\sum\limits_{{\it k}{\rm =1}}^{\left[{{{n}\over{2}}}\right]}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it n}-{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))...(}{\it n}-{\rm 3}{\rm )(n}{\rm -}{\rm 1)(}{\it n}+2{\rm )(}{\it n}+4{\rm )...(}{\it n}+2k{\rm )}}\over{{\rm (}{\rm 2}{\it k+1}{\rm )!}}}}{\it x}^{{\rm 2}{\it k+1}}}\over{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{\left[{{{n}\over{2}}}\right]}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it n}-{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))...(}{\it n}-{\rm 3}{\rm )(n}{\rm -}{\rm 1)(}{\it n}+2{\rm )(}{\it n}+4{\rm )...(}{\it n}+2k{\rm )}}\over{{\rm (}{\rm 2}{\it k+1}{\rm )!}}}}}}={\it P}_{{\it n}}{\rm (}{\it x}{\rm )}$$ (provided that n be odd) In this formula, $\left[{{{n}\over{2}}}\right]$ is the largest integer smaller than ${{n}\over{2}}$ . for example: $${\it P}_{{\it 5}}{\rm (}{\it x}{\rm )}{\rm =}{{{\rm x}+\sum\limits_{{\it k}{\rm =1}}^{2}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it 5}-{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))...(}{\it 5}-{\rm 3}{\rm )(5}{\rm -}{\rm 1)(}{\it 5}+2{\rm )(}{\it 5}+4{\rm )...(}{\it 5}+2k{\rm )}}\over{{\rm (}{\rm 2}{\it k+1}{\rm )!}}}}{\it x}^{{\rm 2}{\it k+1}}}\over{{\rm 1}+\sum\limits_{{\it k}{\rm =1}}^{2}{{{{\rm (}-{\rm 1}{\rm )}^{{\it k}}{\rm (}{\it 5}-{\rm (}{\rm 2}{\it k}-{\rm 1}{\rm ))...(}{\it 5}-{\rm 3}{\rm )(5}{\rm -}{\rm 1)(}{\it 5}+2{\rm )(}{\it 5}+4{\rm )...(}{\it 5}+2k{\rm )}}\over{{\rm (}{\rm 2}{\it k+1}{\rm )!}}}}}}={{x+{{-1\times 4\times 7}\over{3!}}x^{3}+{{2\times 4\times 7\times 9}\over{5!}}x^{5}}\over{1+{{-1\times 4\times 7}\over{3!}}+{{2\times 4\times 7\times 9}\over{5!}}}}={{x-{{14}\over{3}}x^{3}+{{21}\over{5}}x^{5}}\over{1-{{14}\over{3}}+{{21}\over{5}}}}={{15}\over{8}}(x-{{14}\over{3}}x^{3}+{{21}\over{5}}x^{5})={{1}\over{8}}(15x-70x^{3}+63x^{5})$$ which is also true.
I want to prove these two formulas(1 & 2).If you can, please help me or if you know a textbook that mentions these two formulas, please introduce it to me. thanks","['special-functions', 'ordinary-differential-equations']"
4516103,Inequality of a linearized function,"The following inequality is used in one of the steps for the proof of the centre manifold theorem. Consider the function $$f(x)=Ax+\tilde{f}(x)$$ where $x\in \mathbb{R}^n$ , $f:\mathbb{R}^n \rightarrow \mathbb{R}^n$ is of class $C^k$ for some $k\geq 1$ and $f(0)=0$ , $Df(0)=A$ . The author during the proof of a lemma proceeds to state the following result. $$\|f(x)\|\leq\|x\|\sup_{s\in[0,1]}\|Df(sx)\|$$ The author has not defined how the norm of $Df(sx)$ is defined and I am not sure if that is required. I tried proving this for the one dimensional case by replacing all the norms with moduli but couldn't come up with a simplification for the RHS. The text which this is taken from is A. Vanderbauwhede, ""Centre manifolds, normal forms and elementary bifurcations"", Dynamics Reported, Vol 2 (1989), pg 95.","['ordinary-differential-equations', 'functional-analysis', 'mean-value-theorem', 'nonlinear-dynamics', 'dynamical-systems']"
4516129,$\rm Spec$ of an infinite product,"I know that the spectrum of an infinite product of rings is not an infinite disjoint union of spaces. I always see this fact being proven using compactness, but I would like to understand where the argument of the finite case fails. I started reasoning under the assumption that the ideals in a product of rings $R:=\prod _{i\in I}R_i$ , where the cardinality of $I$ is not finite, are exactly the subsets $\prod _{i\in I}J_i$ , with $J_i\subset R_i$ ideals; but at this point the argument for the finite case seems to work fine, so probably the initial assumption is wrong. I'm  not sure though: the product of ideals is an ideal in $R$ , so I just need to  know if any ideal  in $R$ is of this form. Let $J\subset R$ be an ideal, and let $J_i\subset R_i$ be the image of $J$ under the corresponding projection. Clearly $J\subset J':=\prod  _{i\in I}J_i$ .  Let $j:=(j_i)_{i\in I}$ be an element of $J'$ : then for every $i$ , the element of $J'$ whose coordinates are all $0$ , except for the $i$ -th one which is $j_i$ , is contained in $J$ . Can we say that $j\in J$ then? I'm not convinced, because the sum of an infinite number of  terms does not make  sense in general, but here maybe it does: the sum is defined coordinatewise, and at every coordinate we have a sum of all zeros but a term, that can be computed.","['algebraic-geometry', 'commutative-algebra']"
4516134,Why isn't this restriction of a homeomorphism open?,"[I put a PS at the bottom of the text, as I think problem's solved] There is a simple idea that a restriction of a homeomorphism to a subset of its domain yields a new homeomorphism onto its image. I am having problems with certain cases,  where I think that this applies, but my texts conclude that the result is just continuous and bijective. When the domain is in $R^n$ , domain invariance is then used. One example is this homeomorphic manifolds proof by John. He restricts 3 homeomorphisms, concludes that the restriction are homeomorphisms and that is it. I solved this question the same way, the maps are homeomorphic, why wouldn't the restriction of the middle homeomorphism $h$ not be ? But this proof does not seem to use domain invariance, so I wonder if it's correct. As usual, there probably is a simple insight here that I am lacking, but I am not getting myself on the right track, so any hint appreciated. What I have seen looks like john's answer, but was really:
Mand N are homeomorphic manifolds with dimensions m and n. Obtain a contradiction if m > n. $h:M\rightarrow N$ is a homeomorphism. for $h(x)  \in N$ there is an open V in N, an open $O_v$ in $R^n$ , and a homeomorphism $\psi_N :V\rightarrow O_v$ . $h^{-1}(V)$ is open in M, so there is an open $U\subset h^{-1}(V)$ around x, an open $0_u$ in $R^m$ and a homeomorphism $\psi_M : U \rightarrow O_u$ . Again, $h(U)\subset V $ is open, and the restriction $\psi_N|_{h(U)}$ is a homeomorphism onto its image $O_v'$ . My text says here that $\psi_M\ o\ h^{-1} \ o\ \psi_N|_{h(U)}^{-1}$ (edited: I should use $ \psi_N|_{h(U)} \ o\ h\ o\  \psi_M^{-1}$ ) is continuous and injective and uses domain invariance to conclude it is open.
I fail to see why the restriction of h to U would not be a homeomorphism, and the composition is not just a composition of three homeomorphisms, which is open, or what else is the reason that domain invariance is used.
The domain invariance proposition which is used is: any continuous injective function from an open $O\subset R^n$ into $R^n$ is open. PS: following freakish' hint, I think I found the answer that all the restrictions are indeed homeomorphisms, but it is not useful to know because I need an embedding into a higher dimension, and that is where I need Brouwer's domain invariance","['general-topology', 'differential-geometry']"
4516227,Help with proving existence of a certain set using Zorn's lemma,"As part of my homework I need to prove, using Zorn's lemma, that there exists a set $A \subseteq [0,1)$ so that for every real number $r \in \mathbb{R}$ , there exists a single element $a \in A$ that satisfies $r-a \in \mathbb{Q}$ . Here's what I've tried: first I defined a weak partial order $\prec$ that satisfies $A \prec B \iff B \subseteq A$ . Then I defined a set $Q = \{A \subseteq [0,1) \mid \forall r \in \mathbb{R}~ \exists a \in A. r-a \in \mathbb{Q}\}$ , proved it's not equal to $\emptyset$ , and tried to find an upper bound for every chain $C \subseteq Q$ . Here's where I got stuck. I thought $\cap C$ would work as an upper bound for every chain $C$ , and I've managed to prove $\forall c \in C. c \prec \cap C$ . The problem is I can't prove that $\cap C \in Q$ , that is, for every $r \in \mathbb{R}$ exists $a \in \cap C$ so that $r-a \in \mathbb{Q}$ . It seems to me that the way I defined the order, $\cap C$ should be inside $C$ , but I can't prove that either. Anyways, any help will be appreciated. Thanks in advance!",['elementary-set-theory']
4516260,"On the induction argument of the ""many paths to a basis"" theorem","I'm trying to make practical sense of the induction argument of the following famous theorem on the dimension of a finite-dimensional vector space. Theorem Let $n$ be a positive integer. If $V$ is a vector space containing two lists of vectors $x_1,\ldots,x_n$ and $y_1,\ldots,y_n$ of the same length $n$ , such that (1) $x_1,\ldots,x_n$ generate $V$ , and (2) $y_1,\ldots,y_n$ are independent, then both lists are bases of $V$ . Update: For your convenience I've updated the post adding the full proof. In the proof below, $\theta$ denotes the zero vector of $V$ . Proof. We have to show that list (1) is independent and list (2) is generating. The proof is by induction on $n$ . Let $F$ be the field of scalars. Suppose $n=1$ . By assumption, $V = Fx_1$ and $y_1\neq \theta$ . Say $y_1 = c x_1$ . Since $y_1$ is nonzero, both $c$ and $x_1$ are non zero. From $x_1\neq\theta$ we see that the list $x_1$ is independent. Since every vector is a multiple of $x_1 = c^{-1}y_1$ , and therefore of $y_1$ the list $y_1$ is generating. Let $n\geq 2$ and assume that the statement in the theorem is true for lists of length $n-1$ . We assert first that $x_1,\ldots,x_n$ are independent. Assume to the contrary that this list is linearly dependent, i.e. one of the $x_i$ is a linear combination of the others. Suppose, for illustration, that $x_n$ is a linear combination  of $x_1,\ldots,x_{n-1}$ . It follows that the list $x_1,\ldots,x_{n-1}$ generates $V$ ; for, its linear span includes $x_n$ as well as $x_1,\ldots,x_{n-1}$ , so it must be all of $V$ by (1). Since the list $y_1,\ldots,y_{n-1}$ is independent by (2), it follows from the induction hypothesis that $y_1,\ldots,y_{n-1}$ generate $V$ . In particular, $y_n$ is a linear combination of $y_1,\ldots, y_{n-1}$ , contradicting (2). The contradiction shows that $x_1,\ldots,x_{n}$ is independent, as asserted. The proof that $y_1,\ldots, y_n$ are generating will be accomplished by invoking the induction hypothesis in a suitable quotient space $V/M$ ; the first step is to construct an appropriate linear subspace $M$ . Express $y_n$ as a linear combination of $x_1,\ldots,x_n$ , say $$
y_n = c_1x_1+\cdots+c_nx_n.
$$ Since $y_n\neq \theta$ , one of the coefficients $c_i$ must be nonzero; rearranging the $x_i$ , we can suppose that $c_n\neq 0$ . It follows that $$
x_n = (-c_1/c_n)x_1+\cdots+(-c_{n-1}/c_n)x_{n-1}+(1/c_n)y_n,
$$ thus the linear span of the list $x_1,\ldots,x_{n-1},y_n$ includes all of the vectors $x_1,\ldots,x_n$ ; in view of (1) we conclude that $$
(*)\,\,\,\, x_1,\ldots,x_{n-1},y_n\,\,\text{generate } V.
$$ Let $M = Fy_n$ and let $Q:V\to V/M$ be the quotient mapping. Then $$
(3)\,\,\,\ Qx_1,\ldots,Qx_{n-1}\text{ generate } V/M
$$ (by (*)) and $$
(4)\,\,\,\ Qy_1,\ldots,Qy_{n-1}\text{ are independent}
$$ (by (2)), so by the induction hypothesis, both the lists (3) and (4) are bases of $V/M$ . In particular, the list (4) is generating for $V/M$ ; since $M$ is generated by $y_n$ , it follows (from another result not shown here) that $y_1,\ldots,y_n$ generate $V$ . Q.E.D. I'm stuck at the induction step (in bold). I do understand that the assumption is a necessary logical step which is given for truth without questioning, in order to show that what follows is true. My question is: if $V$ has basis $x_1,\ldots,x_{n-1}$ , how can it also have (a larger) basis $x_{1},\ldots,x_{n}$ ?","['induction', 'linear-algebra', 'vector-spaces']"
4516297,Correct use of separation of variables for second order DE?,Is this the correct use of separation of variables? I multiped both sides by dx^2 then integrated. Please let me know if you find any mistakes. Thanks for any help.,"['integration', 'calculus', 'ordinary-differential-equations']"
4516301,Spivak: Understanding computation of an upper bound for remainder of Taylor polynomial approximation of $\log{(1+x)}$.,"There is a particular calculation in Spivak's Calculus that I had to think about a lot. I think I finally understood it but I am not sure about why it was done this way. The calculation is related to another recent question that I asked about computing the remainder term in a Taylor polynomial approximation to $\log{(1+x)}$ . The conclusion of that question was how to compute the remainder term in the following expression, for $x\geq 0$ $$\log{(1+x)}=x-\frac{x^2}{2}+\frac{x^3}{3}-\frac{x^4}{4}+...+\frac{(-1)^{n-1}x^n}{n}+\frac{(-1)^n}{n+1}t^{n+1}, t\in (0,x)$$ where $$|R_{n,0}(x)|=\left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{n+1} \tag{1}$$ and $R_{n,0}(x)$ is the remainder term. Some work was involved in coming up with the remainder above (which is obtained from the integral form of the remainder), instead of just applying the formula for the Lagrange remainder (which I will do below). Then Spivak says and there is a slightly more complicated estimate when $-1<x<0$ (Problem 16). For this function the remainder term can be made as small as desired by choosing $n$ sufficiently large, provided that $-1<x\leq 1$ . The ""slightly more complicated estimate when $-1<x<0$ turns out to be $$\left | \frac{(-1)^n}{n+1}t^{n+1} \right |\leq \frac{x^{n+1}}{(1+x)(n+1)}\tag{2}$$ Let's see what happens if we use the Lagrange form of the remainder. We have $$R_{n,0}(x)=\frac{f^{(n+1)}(t)}{(n+1)!}(x-0)^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n \cdot n!}{(1+t)^{n+1}}\cdot \frac{1}{(n+1)!}\cdot x^{n+1}, t\in (0,x)$$ $$=\frac{(-1)^n x^{n+1}}{(1+t)^{n+1}(n+1)}, t\in (0,x)\tag{3}$$ the magnitude of which is smaller than $\frac{|x|^{n+1}}{n+1}$ , and we can see this also can be made as small as desired. Why didn't Spivak just use this remainder to make the points he made, namely that for $-1<x\leq 1$ the remainder is decreasing in $n$ ? He could have, correct? Now, the reason we need to come up with an upper bound for the remainder is so that we have an expression that is dependent on quantities we know ( $x$ and $n$ ), and not quantities we don't know ( $t$ ), correct? Here is how we obtain $(2)$ : For $-1<x\leq t\leq 0$ we have $$0<1+x\leq 1+t\leq 1$$ $$0\leq \frac{1}{1+t}\leq \frac{1}{1+x}$$ $$\left | \int_0^x\frac{u^n}{1+u}du\right |\leq \int_0^x \frac{|u|^n}{1+x}du\leq \frac{|x|^{n+1}}{(1+x)(n+1)}$$ But is the following calculation also correct? EDIT: no, the following is incorrect. The integral form of the remainder is $\int_0^x (-1)^n \frac{u^n}{1+u}du$ Assume $-1<x\leq 0$ , $n$ is odd and let $f(x)=-\frac{x^{n+1}}{n+1}$ . Then $$f(0)=0>\int_x^0 \frac{u^n}{1+u}du>\int_x^0 u^n du = -\frac{x^{n+1}}{n+1}=f(x)\tag{4}$$ And the Intermediate Value Theorem tells us that there exists some $t\in (x,0)$ such that $-\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du$ . Thus, the remainder term is $$R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du=(-1)\cdot \frac{t^{n+1}}{n+1}<0$$ $$-\frac{x^{n+1}}{n+1}<-\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0$$ Hence $$|R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1}$$ Now assume $n$ is even. Then $$f(0)=0<\int_x^0\frac{u^n}{1+u}du<\int_x^0 u^ndu=-\frac{x^{n+1}}{n+1}=f(x)$$ Again, the Intermediate Value Theorem tells us that there exists some $t\in (x,0)$ such that $-\frac{t^{n+1}}{n+1}=\int_x^0 \frac{u^n}{1+u}du$ . Therefore, $$R_{n,0}=(-1)^{n} \int_0^x \frac{u^n}{1+u}du= \frac{t^{n+1}}{n+1}<0$$ $$\frac{x^{n+1}}{n+1}<\frac{t^{n+1}}{n+1}=R_{n,0}(x)<0$$ Hence $$|R_{n,0}(x)|<\frac{|x|^{n+1}}{n+1}$$ EDIT: as per Ted Shifrin's comment, these calculations don't, in fact, work. The reason is that the integrals are all for $|x|<1$ , and so $\frac{1}{1+u}>1$ . Thus, inequalities like in $(4)$ don't work.","['integration', 'calculus', 'derivatives', 'taylor-expansion']"
4516307,"Find range of values of $a,b,c$ such that $ax^2+bx+c$ satisfies given conditions","Let $y = f(x) = ax^2+bx+c$ where $a\neq0$ . Find the range of values $a,b$ and $c$ such that it satisfies the following: $0 \le f(x) \le 1 \quad \forall \space x \in [0,1]$ What I have found so far? For $x=0$ , we get $0 \le c \le 1$ For $x=1$ , we get $0 \le a+b+c \le 1$ If roots exist, then they should lie outside the interval $[0,1]$ . Therefore, ${{-b - \sqrt{D}}\over{2a}} < 0$ and ${{-b + \sqrt{D}}\over{2a}} >1$ . I tried to solve these but didn't get any fruitful results. EDIT 1: As explained in comments by @insipidintegrator, this is only for distinct roots. Vertex of quadratic $f(x)$ is $\big( \frac{-b}{2a}, -\frac{D}{4a}\big)$ . If the x-coordinate of the vertex lies between 0 and 1, then y-coordinate should also be between 0 and 1. But I have no idea how to proceed from here. I want to find some lower/upper limits for $a,b,c$ or any kind of relation between them.  Any solution or hints for solving this problem is greatly appreciated.","['algebra-precalculus', 'quadratics', 'inequality']"
4516331,What does $\frac{X_n}n$ converge to in distribution for $X_n \sim \chi^2_n$?,"Problem I have been working through a probability question, where I am supposed to show that the following claim is not true. I believe I am supposed to use the Weak Law of Large Numbers in order to induce a contradiction, however, I'm not sure how to make progress here. Claim: For $X_n$ following the Chi Square distribution with $n$ degrees of freedom, $\frac{X_n}{n}$ converges in distribution to $X$ with $P(X=0)=1$ . We want to show that this claim is false. Attempt As $X_n$ follows the Chi Square distribution, we know that $$X_n=Z_1^2+Z_2^2+\cdots+Z_n^2$$ where each $Z_i$ follows the standard normal distribution. I don't think each $X_i$ will be independent because $$X_i=X_{i-1}+Z_i^2$$ However, without assuming independence, I'm completely unsure of how to to proceed from here. If we then assume independence, then the Weak Law of Large Numbers states: Weak Law of Large Numbers: for independent $X_1,  X_2, X_3$ ... with common mean and variance, then for all $\epsilon >0$ , the probability $$P \Big{(} \Bigl{|} \frac{\sum_{1}^nX_k}{n}-\mu \Bigl{|} \ge \epsilon \Big{)} \rightarrow 0$$ $$\text{or alternatively}$$ $$P \big{(}\bigl{|}X_n-X \bigl{|} \ge \epsilon \big{)} \rightarrow 0$$ We can rewrite $$P\big{(}\bigl{|}X_n-X \bigl{|} \ge \epsilon \big{)} = P \big{(} \bigl{|}X_n \bigl{|} \ge \epsilon \big{)}$$ as $X=0$ with probability $1$ . I am unsure of how to proceed from here (or if what I have done is correct). I would be grateful for any further guidance as I am not sure if my assumption of independence is correct.","['statistics', 'probability-distributions', 'law-of-large-numbers', 'convergence-divergence', 'probability']"
4516356,Spivak: How do we know $\arctan{x}$ and $\log{(1+x)}$ for all $x$ if we know these functions for $|x|<1$?,"In Ch. 20 of Spivak's Calculus , he shows that the remainder terms for $\arctan$ and $\log{(1+x)}$ become large with the order of the Taylor polynomial used to approximate these functions. Thus these approximations are of no use whatsoever in computing $\arctan{x}$ and $\log{(1+x)}$ .
This is no tragedy, because the values of these functions can be
found for any $x$ once they are known for all $x$ with $|x|<1$ . How do we find the values of these functions for any $x$ if we know the functions for $|x|<1$ ?","['integration', 'calculus', 'polynomials', 'taylor-expansion', 'derivatives']"
4516362,Question about the Inverse Fourier Transform of a particular function.,"I'm trying to find the IFT of the following function: $\hat{f}(\omega) = \frac{i\omega}{1+\omega^2}$ . I know about the Laplace Transform, so right away I thought about looking for an inverse of $\hat{f}$ involving $\cosh$ and then do a change of variables. I managed to arrive at $f(x) = \sqrt{2\pi} \cosh(x) \theta(x)$ , where $\theta$ is the Heaviside Function. Wolfram Alpha agrees that the Fourier Transform of that $f$ is $\frac{i\omega}{1+\omega^2}$ . However, when I use Wolfram Alpha to calculate $\hat{f}^{-1}$ , I have in return $g(x) = -\sqrt{\frac{\pi}{2}}e^{-x}[e^{2x}\theta(-x) - \theta(x)]$ . Both functions, $f$ and $g$ , are obviously not the same. Did I do something wrong? Is it normal for a function to have more than one inverse? In that case, if I was asked on an exam to find the IFT of a function, would both $f$ and $g$ be correct?","['fourier-analysis', 'fourier-transform', 'analysis', 'partial-differential-equations', 'signal-processing']"
4516445,"If $\sum\limits_{n=1}^{\infty}e_n x^n = 0$ always implies $\sum\limits_{n=1}^{\infty}e_n a_n = 0$, then $(a_n) = (x^n)$?","Let $x \in \left[\frac{2}{\sqrt{5}+1}, 1\right)$ and assume the real sequence $(a_n)$ satisfies $a_1 = x,$ For every sequence $(e_n) \subset \left\{-1, 0, 1 \right\}$ with $\sum\limits_{n=1}^{\infty}e_n x^n = 0$ we have $\sum\limits_{n=1}^{\infty}e_n a_n = 0.$ How can I show that this implies $(a_n) = (x^n)$ ? I can see why it's necessary to restrict the interval from which $x$ can be taken (For large/small enough $x$ the only way to get $\sum\limits_{n=1}^{\infty}e_n x^n = 0$ is to have $e_n = 0$ for all $n$ ), but I have no idea how to show the implication to be true in the given interval. As $\frac1x$ has to be between $1$ and $\frac{\sqrt{5}+1}{2}$ , there must be some connection to the golden ratio, but I don't see it yet. Any help is appreciated! EDIT: A proof was posted on AoPS a couple of months after I asked this question here.","['power-series', 'golden-ratio', 'sequences-and-series']"
4516448,Group in which every conjugacy class of subgroups is characteristic,"If $G$ is a (finite) group in which every subgroup is characteristic, then it's easy to show that $G$ is cyclic (and conversely of course). A more interesting generalization is to instead consider groups in which every conjugacy class of subgroups is characteristic. In other words, for any automorphism $\phi\colon G\to G$ and any subgroup $H\leqslant G$ there exists some $g\in G$ such that $\phi(H) = H^g$ . This class of groups is much larger. For instance, any group with trivial outer automorphism group satisfies this property. But there are also examples like $SL_2(\mathbb{F}_3)$ and $SL_2(\mathbb{F}_5)$ which have non-trivial outer automorphisms but every conjugacy class of subgroups is characteristic. I doubt there's anything like a classification of groups of this form since it's already not feasible to classify groups with trivial automorphism group. But these groups have cropped up in my research and I'd like to at least know: (1) Is there an established name for groups of this form? (2) Are there any papers that consider them? There are also some similar stronger properties that I would be equally interested in, like if any two isomorphic subgroups are conjugate or any two subgroups of equal order are conjugate (the above examples of $SL_2(\mathbb{F}_3)$ and $SL_2(\mathbb{F}_5)$ both satisfy these stronger properties).","['group-theory', 'reference-request']"
4516457,Surprising Patterns in Numbers whose Digit Sum is equal to their Square Root in an arbitrary Base.,"In base 10, $\sqrt{81} = 8 + 1 = 9$ . It turns out that 81 is the only number in base 10 that has this property. I wanted to find out if there are other numbers with this property in other bases. Mathematically, for a given base $b$ , I'm looking for numbers $n$ with digits $d_i$ in base $b$ such that $$
\sqrt{\sum^{\infty}_{i=0} d_i\cdot b^i} = \sum^{\infty}_{i=0} d_i
$$ I wrote this code to brute force the solutions in any given base. I've also uploaded the set of solutions for the first 5000 bases here . Here I don't consider 0 and 1 to be solutions The pattern arises when I tried [plotting the number of solutions for 1 million bases (plot only includes at most 5000 data points from each strip $[2^n, 2^{n+1}]$ ): There are distinct gaps in the data that appears to roughly correlate with powers of 2? Furthermore, the smallest base of each ""strip"" are bases 7, 31, 211, 2311, 30031, 510511, which are Euclid numbers , these bases have 5, 10, 21, 48, 96, and 196 solutions respectively. I've verified that base 9699691 has 397 solutions and base 223092871 has 784 solutions. Here's the data in numpy.array format I'm not quite sure what to make of this. On the one hand, the powers of two-like pattern makes me suspect that it could stem from a floating point precision error or a bug in the code. On the other, the manner in which the powers of two present themselves feels atypical for a computational error. My question is does this pattern actually exist, and if so I would appreciate any insight into why such a pattern arises from this problem. Edit:
It also appears that $(b-1)^2$ is always a solution as proven in the comments. Edit 2:
I plotted out the solutions $\sqrt{n}$ against the base $b$ , and there appears to be rays with varying slopes:","['number-theory', 'integers', 'prime-numbers', 'decimal-expansion']"
4516472,Is there a unicursal octagram?,"Tl;dr Is there such a thing as a unicursal octagram ? Long question The unicursal hexagram is the figure formed when you link all six vertices of a regular hexagon using a single continuous trace, rather than a set of overlaid triangles. Here is what it looks like: It even have some occult meanings and other nice symbolism. For reasons of arbitrary RPG system symmetries, I would like to know whether anyone has ever heard of a unicursal octagram , that is, the figure formed when you trace all the vertices of a regular octagon with a single trace. The rules are: The trace has a direction. That is, if the octagram is modelled as a graph, each vertex will have exactly one edge going towards it and exactly one edge going out of it. Crossings in the middle of the surrounding octagon are allowed, but more symmetric structures are preferred. Beauty is a goal. I have googled it several times, but the word ""unicursal"" seem to be very tightly bound to the word ""hexagram"" in Google's mind, so the octagram's more famous brother is always stealing the spotlight, so my google searches weren't very successful. Alternatively, I tried looking for questions here on MathSE about unicursal octagrams, but there weren't any. I figured I could ask one, then.","['graph-theory', 'geometry']"
4516569,Fiber of a morphism of spectra,"Let $f:R\to S$ be a ring homomorphism, and for a fixed prime ideal $\mathfrak p\subset R$ , let $p:R\to k(\mathfrak p)$ be the canonical  homomorphism, $k(\mathfrak  p)$ being the fraction field  of $R/\mathfrak p$ . Take the following (pushout) square: $\require{AMScd}$ $$\begin{CD}
R@>>p> k(\mathfrak p)\\
@VVfV @VjVV\\
S@>i>>S\otimes_R k(\mathfrak p)
\end{CD}$$ and consider its image (in $\rm Top$ ) under  the functor $\rm Spec$ (whose action I'll denote by $-^*$ ). So $p^*$ is injective, being its domain a singleton ( $R\neq 0$ ), and also $i^*$ is injective: $i=l\circ q$ , where $q:S\to S\otimes_R R/\mathfrak p$ is surjective, and $l: S\otimes _R R/\mathfrak p\to S\otimes_R  k(\mathfrak p)$ is a localization (respect to the subset of elements of the form $1\otimes \bar r$ , $r\notin \mathfrak p$ ) hence $q^*,l^*$ are injective. Until now, we can say that $i^*$ injects its domain into the fiber of $\mathfrak p\in \operatorname{Spec} R$ ; but I don't understand how to deduce the most important part, i.e. that the image of $i^*$ is the entire fiber (and so the square in $\rm Top$ is a pullback). I apologize for the third question of the same kind in  two days, but this passage (that I found on Clark's Commutative Algebra, 4.3) is where my confusion actually started. Thanks for your patience","['algebraic-geometry', 'commutative-algebra']"
4516611,Is the integral of the squared distance function smooth on a compact Riemannian manifold?,"Let $M$ be a compact Riemannian manifold and let $d$ be the natural distance on it. Is the function $$x \mapsto \int _M d(x,y) ^2 \, \mathrm d y$$ smooth? Had it not been for the integral, the map $x \mapsto d(x,y) ^2$ is smooth outside the cut locus of $y$ , for fixed $y$ . The problem is the integral, which makes $y$ vary, and I do not know how to investigate this. If the above function (let us call it $f$ ) is not smooth, is there any theoretical framework in which I could say that $$\Delta f = \int _M \Delta_x \, d(x,y)^2 \, \mathrm d y$$ where the lower index $x$ indicates the variable with respect to which the Laplacian acts? And what could be the meaning of the integrand? I have tried placing the problem in the context of distribution theory, but this did not allow me to slip the Laplacian inside the integral, which is what I am after.","['riemannian-geometry', 'lebesgue-integral', 'metric-spaces', 'smooth-manifolds', 'differential-geometry']"
4516639,Solving the equation $f(x) = \int_0 ^x \sqrt{4-2f(t)} dt$,"A continuous function $f(x)$ satisfies the following.
For all reals $x \le b$ , $f(x)=a(x-b)^2+c$ .
For all reals, $f(x) = \int_0 ^x \sqrt{4-2f(t)} dt$ . If $\int_0^6 f(x) dx = \frac{q}{p}$ , where $p,q$ are relatively prime positive integers, find $p+q$ My approach is as follow $f\left( x \right) = \int\limits_0^x {\sqrt {4 - 2f\left( t \right)} dt} $ $f'\left( x \right) = \sqrt {4 - 2f\left( x \right)} $ $\frac{d}{{dx}}f\left( x \right) = \sqrt {4 - 2f\left( x \right)} $ $\frac{{d\left( {f\left( x \right)} \right)}}{{\sqrt {4 - 2f\left( x \right)} }} = dx$ Not able to proceed from here",['functions']
4516652,Eliminate $\theta$ from these two equations,"If $$\operatorname{cosec}\theta-\operatorname{sin}\theta=m$$ and $$\operatorname{sec}\theta-\operatorname{cos}\theta=n$$ then eliminate $\theta$ . My work: Squaring both the equations and then adding them will yeild $$\operatorname{cosec}^2\theta+\operatorname{sec}^2\theta=m^2+n^2+3$$ or $$\operatorname{cos}^2\theta\operatorname{sin}^2\theta=\frac{1}{m^2+n^2+3}$$ or $$\operatorname{sin}^2 2\theta=\frac{4}{m^2+n^2+3}$$ or $$\theta=\frac12\cdot\operatorname{arcsin}\left(\frac{2}{\sqrt{m^2+n^2+3}}\right)$$ Now to eliminate $\theta$ , we will have to put its value in any of the two equations. But actually I don't know how to evaluate inverse trigonometric values, like that of $\theta$ in terms of $m$ and $n$ . And after that I guess it will be difficult to simplify the expression. Any help is greatly appreciated.","['trigonometry', 'inverse-function']"
4516673,How do you solve $\cos (3x) = \cos (x)$?,"I want to solve this problem: $\cos(3x) = \cos(x)$ and I’m stuck. I have tried to rewrite it to $4 \cos^3(x) - 3 \cos(x) = \cos (x)$ and then solve it. Add $3 \cos (x)$ to both sides and then divide by $4$ . And then I have this: $\cos^3(x) = \cos (x)$ . I don’t know what so do now or even if on the right track. Please help, thanks.",['trigonometry']
4516701,"If $A$, $B$ and $A\cap B$ are topological balls, is $A\cup B$ too?","I would be happy about an answer addressing general dimensions, but I am mostly interested in $d=4$ , so I will state the question for this case. Question: Given two closed sets $A,B\subset\Bbb R^4$ so that $A$ , $B$ and $A\cap B$ are homeomorphic to 4-balls, then is $A\cup B$ also homeomorphic to a 4-ball? If it helps, and to avoid pathologies, let's assume that everything is PL, i.e. all these sets are PL-homeomorphic to PL-balls. Some thoughts Seifert-van-Kampen suggests that the union has trivial fundamental group. That is a good start. I though about first proving that $B\setminus A\simeq D^4$ (the 4-ball) or that $\partial A\cap\partial B\simeq S^2$ (the 2-sphere), but both are not necessarily true, already in small dimensions.","['general-topology', 'low-dimensional-topology']"
4516709,Is this an easy generalization of the Hahn-Banach separation Theorem?,"The Hahn-Banach theorem goes like If $p:X\to\mathbb R$ is sublinear and $f$ is a linear functional on a subspace $Y\subseteq X$ bounded by $p$ (on $Y$ ), then there is an extension of $f$ to $X$ that is bounded by $p$ and matches with $f$ on $Y$ . I am trying to do something similar, let $\psi:X\to2^\mathbb R$ be such that for any $x\in X$ , $\psi(x)$ is a non empty-closed interval for any $x\in X$ , $a\in \mathbb R$ , $\psi(ax)=a\psi(x)$ for any $x,y\in X$ , $\psi(x+y)\subseteq \psi(x)+\psi(y)$ One can check that $x\to \sup \psi(x)$ and $x\to -\inf \psi(x)$ are both sublinear functions (over the extended real line). I want to know if the following holds : If $f$ is a linear function on a subspace $Y\subseteq X$ such that $f(x)\in\psi(x)$ for any $x\in Y$ , then there is an extension of $f$ to $X$ such that $f(x)\in\psi(x)$ for all $x\in X$ . In a sense this is a version of Hahn-Banach where the function is sandwiched between a sublinear and a (forgive the abuse of word) superlinear function. I have never seen anything like that, but I have this feeling that Hahn Banach is actually already enough to show that, I was thinking of using $p(x)=\sup \psi(x)-\inf\psi(x)$ as a semi-norm but it is hard for me to find a new $f'$ that is less than $p$ that would correspond one to one to $f$ in some way. Any thought is welcome. For the context I am trying to make progress on this question , however I think this new question has independent interest, the formulation is pleasant to me.","['elementary-set-theory', 'convex-analysis', 'hahn-banach-theorem', 'linear-algebra']"
4516720,Can the expectation of a random variable not exist?,"I saw a question like this. Solving that was not hard, because it only needs to show that the value of the next improper integral does not exist. $$
\int_0^\infty x\cdot\frac{2}{\pi(1+x^2)}dx=\infty
$$ It is understandable by formula, but not intuitively at all. From what I've learned so far, every random variable has an expectation, and any book never said that the average may not exist. Can this actually be? Of course, it is certain that a given function satisfies the definition of a random variable. But what does it mean that the expectation of a random variable does not exist?","['statistics', 'probability']"
