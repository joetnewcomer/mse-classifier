question_id,title,body,tags
2423055,"If $\Bbb R$ means all real number, then what does $\Bbb R^2$ mean? [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 6 years ago . Improve this question I am sort of baffled by this thing, already real number has every thing in it why is this concept of $\Bbb R^2$ ? What does it mean? What is its advantage?","['real-numbers', 'elementary-set-theory']"
2423062,How to calculate the determinant of this n by n matrix?,"Find the determinant of this n by n matrix. $$
\begin{pmatrix}
0 & x_1 & x_2 & \cdots& x_k \\
x_1 & 1 & 0 & \cdots & 0 \\
x_2 & 0 & 1& \cdots & 0 \\
\vdots& \vdots& \vdots& \ddots  & \vdots\\
x_k & 0 & 0 & \cdots& 1 \\
\end{pmatrix}
$$
where, $$ k=n-1 $$. I am new to matrices and determinants, but this is what I did:
I developed the determinant using the second column: $$
(-1)^2*x_1 
\begin{pmatrix}
x_1  & 0 & \cdots & 0 \\
x_2  & 1& \cdots & 0 \\
\vdots& \vdots& \ddots& \vdots \\
x_k  & 0 & \cdots& 1 \\
\end{pmatrix} + (-1)^3 *1
\begin{pmatrix}
0 & x_2 & \cdots & x_k \\
x_2  & 1& \cdots & 0 \\
\vdots& \vdots& \ddots& \vdots \\
x_k  & 0 & \cdots& 1 \\\end{pmatrix}
$$ the first determinant is triangular, so its equal to $ x_1 $ but this is where I got stuck. I don't know what to do with the second determinant. Any help is appriciated. Thanks","['matrices', 'linear-algebra', 'determinant']"
2423071,"If $\sum\limits^n_{r=1} r f(r) = n(n+1)$ and $f(1)= \dfrac12$, find $f(1000)$.","If for $f : \Bbb N \to \Bbb R$,
$$\sum\limits^n_{r=1} r f(r) = n(n+1)$$ 
and $f(1) = \dfrac12$ where $n \ge 2$, find $f(1000)$. I calculated some terms
$$f(1) = \dfrac12$$
$$2f(2) = 2\cdot 3 - \dfrac 12 = \dfrac {11}2$$ So, $f(1) + 2f(2) = 6$ $$3f(3) = 3\cdot 4 - 6 = 6$$
$$4f(4) = 4\cdot 5 - 6 - 6 = 8$$ Looking at the pattern, I hypothesise that $nf(n) = 2n$ for $n > 2$. I think this can be proved by induction if it is true. So $f(n) = 2$ for $n > 2$, thus $f(1000) = 2$. The answer given is $\dfrac 1{2000}$, which is so much different from my answer. I feel very stupid now, all I did is basic arthmetic operations to get my answer. I think I should go back to my brothers class to relearn these things. How did I go so wrong with my answer ?","['real-analysis', 'calculus', 'functions']"
2423096,Hahn-Banach From Systems of Linear Equations,"In this paper 1 on the history of functional analysis, the author mentions the following example of an infinite system of linear equations in an infinite number of variables $c_i = A_{ij} x_j$: \begin{align*}
\begin{array}{ccccccccc}
1 & = & x_1 & + & x_2 & + & x_3 & + & \dots  \\
1 & = &     &   & x_2 & + & x_3 & + & \dots \\
1 & = &     &   &     &   & x_3 & + & \dots \\
  & \vdots  &   &     &   &     &   &   &   \ddots 
\end{array} \to \begin{bmatrix} 1 \\ 1 \\ 1 \\ \vdots \end{bmatrix} = \begin{bmatrix}
1 & 1 & 1 & \dots \\
  & 1 & 1 & \dots \\
  &   & 1 & \dots \\
  &   &   & \ddots
\end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ \vdots \end{bmatrix} 
\end{align*} as an example of a system such that any finite truncation of the system down to an $n \times n$ system has a unique solution $x_1 = \dots = x_{n=1} = 0, x_n = 1$ but for which the full system has no solution. This book 2 has the following passage on systems such as this one: The Hahn-Banach theorem arose from attempts to solve infinite systems of linear equations... The key to the solvability is determining ""compatibility"" of the system of equations. For example, the system $x + y = 2$ and $x + y = 4$ cannot be solved because it requires contradictory things and so are ""incompatible"". The first attempts to determine compatibility for infinite systems of linear equations extended known determinant and row-reduction techniques. It was a classical analysis - almost solve the problem in a finite situation, then take a limit. A fatal defect of these approaches was the need for the (very rare) convergence of infinite products."" and then mentions a theorem about these systems that motivates Hahn-Banach: Theorem 7.10.1 shows that to solve a certain system of linear equations, 
  it is necessary and sufficient that a continuity-type condition be satisfied. Theorem 7.10.1 (The Functional Problem): Let $X$ be a normed space over $\mathbb{F} = \mathbb{R}$ or $\mathbb{C}$, let $\{x_s \ : \ s  \in S \}$ and $\{ c_s \ : \ s \in S \}$ be sets of vectors and scalars, respectively. Then there is a continuous linear functional $f$ on $X$ such 
  that $f(x_s) = c_s$ for each $s \in S$ iff there exists $K > 0$ such that 
  \begin{equation}
\left|\sum_{s \in S} a_s c_s \right| \leq K  \left\| \sum_{s \in S} a_s x_S \right\| \tag{1}, 
\end{equation}
  for any choice of scalars $\{a_s \ : \ s \in S \}$ for which $a_s = 0$ for all but finitely many $s \in S$ (""almost all"" the $a_s = 0$). Banach used the Hahn-Banach theorem to prove Theorem 7.10.1 but Theorem 7.10.1 implies the Hahn-Banach theorem: Assuming that Theorem 7.10.1 holds, let $\{ x_s \}$ be the vectors of a subspace $M$, let $f$ be a continuous linear functional on $M$; for each $s \in S$, let $c_s = f(x_s)$. Since $f$ is continuous, $(1)$ is satisfied and $f$ possesses a continuous extension to $X$. My question is: If you knew none of the theorems just mentioned, how would one begin from the system $c_i = A_{ij} x_j$ at the beginning of this post and think of setting up the conditions of theorem 7.10.1 as a way to test whether this system has a solution? How does this test show the system has no solution? How do we re-formulate this process as though we were applying the Hahn-Banach theorem? Does anybody know of a reference for the classical analysis of systems in terms of infinite products? 1 Neal L. Carothers: A Brief History of Functional Analysis . 2 Lawrence Narici, Edward Beckenstein: Topological Vector Spaces , 2nd Edition.","['infinite-product', 'hahn-banach-theorem', 'systems-of-equations', 'functional-analysis', 'topological-vector-spaces']"
2423121,Is $\sum_{n=2}^{\infty}\frac{(-1)^n}{\sqrt n+(-1)^n}$ convergent?,"Is the following series convergent? $$\sum_{n=2}^{\infty}\frac{(-1)^n}{\sqrt n+(-1)^n}$$ I think, the above series is divergent, since $$\sum_{n=2}^{\infty}\frac{(-1)^n}{\sqrt n+(-1)^n}=\frac{1}{\sqrt 2+1}-\frac{1}{\sqrt 3-1}+\frac{1}{\sqrt 4+1}-\frac{1}{\sqrt 5-1}+\dots\geq$$
$$\frac{-2}{(\sqrt 3-1)^2}+\frac{-2}{(\sqrt 5-1)^2}+\dots=\sum_{n=2}^{\infty}\frac{-2}{(\sqrt n-1)^2}$$ And the last series is divergent. IS my argument correct? Thanks.","['alternative-proof', 'sequences-and-series', 'analysis', 'proof-verification']"
2423154,First Betti number of a Reeb graph is not greater than that of the space?,"It is well-known that $\beta_1(R(f))\le\beta_1(X)$, where $\beta_1$ is the first Betti number (number of loops), $X$ is a topological space, $f:X\to\mathbb R$ a continuous funcion, and $R(f)$ its Reeb graph, i.e., the space of contours of $f$ (connected components of levels of $f$): see, e.g., Computational Topology: An Introduction by Edelsbrunner & Harer, page 141. However, the commonly given argument looks flawed to me: it is said that the dimensionality of the space of images of all 1-cycles of $X$ in $R(f)$ does not exceed $\beta_1(X)$, but why cannot $R(f)$ have other 1-cycles not induced from any 1-cycle of $X$? For example, the height function $f$ on a Warsaw circle (a topologist's sine curve including a segment at the ""bad"" place, not just a point; plus a piece of a circle) $X$: or even (EDIT 2: The left picture is not a counterexample, but the variant of the function at the right is.) is continuous and $R(f)$ is a circle, $\beta_1(R(f))=1$, while $\beta_1(X)=0$ ( here , Remark 2.7 on page 7). While it is true that the projection in $R(f)$ of any cycle in $X$ is zero, $R(f)$ has a cycle not projected from any cycle in $X$. Where am I wrong? And even if this particular counterexample is wrong, still how to prove that $R(f)$ does not have other cycles? EDIT 1: Here is the statement from the book: and then: So I want to understand whether the fact is true, and why.","['homology-cohomology', 'algebraic-geometry', 'computational-geometry', 'algebraic-topology', 'general-topology']"
2423174,How to calculate the directional derivative of $f(x)=1/|x|$,"I have the function $f:
\left(\Bbb{R}\setminus0\right)^n \to\mathbb{R}$ given that $n\ge 1$ defined by $$f(x) = \frac{1}{|x|}$$ where $|x| = (x_{1}^{2}+...+x_{n}^{2})^{1/2}$. I have to calculate the directional derivative in a generic $x\ne 0$ with the direction $v = \frac{\nabla f(x)}{|\nabla f(x)|}$. Well, I verified that $f$ is differentiable $\forall x\ne 0$. Then the directional derivative is $$\frac{\partial f}{\partial v} (x_0) = \left\langle \nabla f(x_0), \frac{\nabla f(x_0)}{|\nabla f(x_0)|}\right\rangle = \left|\nabla f(x_0)\right|$$ But now, if I have to calculate $|\nabla f(x_0)|$, how can I do it in $\mathbb{R}^n$?","['partial-derivative', 'calculus', 'functions']"
2423220,Thinking about the group algebra $k[G]$ as functions on $G$,"Given a group $G$ and field $k$ one can define the group algebra $k[G]$ in two ways: The underlying vector space of $k[G]$ is the free $k$-vector space on $G$, and the multiplication on $k[G]$ is the unique bilinear extension $k[G] \times k[G] \to k[G]$ of the group multiplication $G \times G \to G$. The elements of $k[G]$ are the functions $f \colon G \to k$ with finite support and the vector space operations are defined pointwise.
The multiplication is given by the convolution product
$$
     (f_1 f_2)(g)
  := \sum_{h \in G} f_1(h) f_2(h^{-1} g)
  \qquad
  \text{for all $g \in G$}.
$$ Since I first encountered the second definition I have always found it somewhat inferior to the first one.
I assumed that it exists only to define $k[G]$ without requiring the notion of a free vector space, made possible by hiding the free vector space on a set $X$ behinds its technical construction as functions $X \to k$ with finite support. But by now I have seen the second definition often enough to begin wondering if there is some benefit to taking it more seriously.
While thinking about this, I began to wonder if there is some deeper connection between the two definition than I have given it credit for. Question: What do we gain from thinking about the group algebra $k[G]$ as an algebra of functions on $G$, and how does this view point relate to the other definition? (I realize that it could turn out be more appropriate to ask two separate questions instead.
But I’m currently not sure how related the two are, and thus refrained from doing so.) So far my thoughts on this have been the following: According to the first definition, $G$ is a basis of $k[G]$.
Functions $G \to k$ are then “the same” as elements of $k[G]^*$;
the functions of finite support correspond to the image of the basis-dependent embedding $k[G] \hookrightarrow k[G]^*$, $g \mapsto \delta_g$.
This seems to suggest some kind of duality between the two definitions, coming from the usual duality pairing of $k[G]$ and $k[G]^*$.
(I know that for finite $G$ the Hopf algebra $k[G]$ is dual to the Hopf algebra of functions $G \to k$.
But I don’t know if this is of use here.) If $k[G]$ consists of functions on $G$, then I would strongly expect it to be contravariant in $G$.
That this is not the case seem to be thanks to the convolution product.
But I don’t really understand where the convolution product comes from when one doesn’t already have the first definition in mind. The requirement of finite support is needed to assure that the sum $\sum_{h \in G} f_1(h) f_2(h^{-1} g)$ is well-defined even for infinite groups.
It should be possible to lift this restriction if we have some notion of convergence for this sum.
While this seems to be a thing , I don’t know if this sheds any light on a possible deeper relation between the two definitions in the algebraic setting. Help is appreciated.","['intuition', 'abstract-algebra', 'group-rings']"
2423272,Does $\lfloor\frac{(n+1)^2}2\rfloor$ equal $\lfloor\frac{n^2}2\rfloor + n$?,"By expanding the left, I found that $\lfloor\frac{(n+1)^2}2\rfloor$ = $\lfloor\frac{n^2}2 + n + \frac{1}2\rfloor$. I am not sure how to relate $\lfloor n + \frac{1}2 \rfloor$ to n so as to show that the right is true/false. Any ideas?",['discrete-mathematics']
2423304,Question regarding Gambler's Ruin,"Consider a gambling process $(X_n)_{n∈\mathbb{N}}$ on the state space $S = {0, 1, . . . , N}$, with probability
$p$, resp. $q$, of moving up, resp. down, at each time step. For $x = 0, 1, . . . , N$, let $τ_x$
denote the first hitting time, $τ_x := \inf\{n ≥ 0 : X_n = x\}$ Let $p_x := P(τ_{x+1} < τ_0 | X_0 = x), x = 0, 1, . . . , N − 1$ Explain why $p_x$ satisfies the recursion $p_x = p + qp_{x−1}p_x$ for $x = 1, . . . , N − 1$. I apply the first step analysis, whereby $P(τ_{x+1} < τ_0 | X_0 = x) = P(τ_{x+1} < τ_0 | X_0 = x_1, X_1=x+1) \cdot P(X_1=x+1 |X_0 = x) + P(τ_{x+1} < τ_0 | X_0 = x_1, X_1=x-1) \cdot P(X_1=x-1 |X_0 = x)$ By the time homogeneous property, the equation reduces to, $P(τ_{x+1} < τ_0 | X_0 = x+1) \cdot P(X_1=x+1 |X_0 = x) + P(τ_{x+1} < τ_0 | X_0 =x-1) \cdot P(X_1=x-1 |X_0 = x)$ Given that initial state is $x+1$, we have already ""hit"" $x+1$ before hitting $0$. So the equation is now $1 \cdot p + P(τ_{x+1} < τ_0 | X_0 =x-1) \cdot q$ Now all left to do is to find $P(τ_{x+1} < τ_0 | X_0 =x-1)$ The Event $\{τ_{x+1} < τ_0 \}$ given that $X_0 =x-1$ can be pictured as paths with the following trends in the picture below. I think the graph on the right correctly portrays the 2 general trends of the outcomes in the event. It can be described as the paths starting at $x_1$, going up and down in any way except reaching $0$ before hitting $x+1$, and then eventually hitting either the absorption state $0$ or $N$. But, i was told that we should IGNORE the presence absorption state $N$ and do the following, Consider the graph on the left, First treat $x$ as an absorption state and thus the green line has probability $p_{x_1}$. Then treat $x+1$ as an absorption state and thus the blue line has the probability $p_{x}$. It stops here since it is an absorption state. Is the above approach valid? It seems very strange as $0$ and $N$ are defined to be the absorption states in the gambling process. If the above approach is valid, why so? (Edit: DEFINITELY NOT, Why did the person even tell me this). If not, what is the proper approach here? (Edit: The accepted answer).","['stochastic-processes', 'markov-chains', 'random-walk', 'probability']"
2423353,Solve for $x$: $\sqrt{1-(\frac{\sin x}{n})^2}\sqrt{1-(n\sin (A-\arcsin (\frac{\sin x}{n})))^2}=\cos x \cos (A-\arcsin (\frac{\sin x}{n}))$,Solve for $x$ in terms of the constants $n$ and $A$. $$\sqrt{1-(\frac{\sin x}{n})^2}\sqrt{1-(n\sin (A- \arcsin (\frac{\sin x}{n})))^2}=\cos x \cos (A- \arcsin (\frac{\sin x}{n}))$$ This gets really messy when I try and simplify and manipulate. I notice that there is a pair of difference-of-two-squares on the left. But multiplying out just makes things even messier.,"['algebra-precalculus', 'trigonometry']"
2423365,Naïve question about manifolds,"On this youtube video by XylyXylyX explaining curves on differentiable manifolds the following drawing is presented: with $(X,\mathcal T_X,\mathcal A)$ denoting a topological space, i.e. $(X,\mathcal T_X),$ with Housdorff, second countability and paracompactness, and an atlas, $\mathcal A;$ $f(\lambda)$ plotting the real line to the manifold: $\mathbb R \to X$, so as to parametrize the line in black on the manifold; and $\gamma$ and $\phi$ representing charts $X\to \mathbb R^2$ (or $\mathbb R^d)$ for different chart regions $U$ and $V.$ It is clear how after we land safely in Euclidean space through $\gamma$ and $\phi$ we can apply calculus; however, and before we get there (or to change coordinates) we have to go through $X$. And if $X$ is not in Euclidean space, What mathematical form does $f$ assume? It can't be $y = f(\lambda),$ which would imply coordinates. Can I have an example (other than the sterographic world map)?","['coordinate-systems', 'differential-geometry']"
2423429,Is my analogy of a directional derivative correct?,"I find analogies to real life are a good way for me to understand some faucets of multivariate calculus, and a wrong analogy can be an exact and glaring indication of misunderstanding. As far as I am aware, in 3D space, there is no total derivative because there is an input space rather than line, and so output $z$ no longer depends on one variable alone - the rate of a change of a slice of a surface in Euclidean space depends on which slice of the surface we take. For instance, if a skateboarder, from a bird's eye view, is directly left of a hill that he can ride down and to the right a ramp he can ride up, and if further up the $y$ axis from the skateboarder's location (above and below the hill and ramp respectively) is just flat asphalt, then if $x, y$ and $z$ represents length and the origin is centered at the skateboarder's location, the rate of change of $z$ (elevation) will be different as the skateboarder moves along $x$ at some velocity depending on where he is -- not changing on flat asphalt above him, but changing if moving between the hill and ramp. Because of that, there is no total derivative, and a different one per slice of $y$. And as such, we need partial derivatives -- finding the derivative with a constant $y$ slice and then taking the general case where $y$ is some constant -- then if we have $x$ and $y$ we can compute the derivative at that slice. Directional Derivatives seem as arbitrary as moving from partial differentiating along some straight line $x$ or $y$ and applying it so some arbitrary vector $\vec v$. I see it as being analogous to the scenario with before, but the ramp being, say, above and to the left of the skater (from a bird's eye view, so $+y$ away and $-x$ away). Taking the directional derivative of a vector causing the skateboarder to go up the ramp will cause a different relationship with $z$ if the velocity vector was some arbitrary vector moving on flat asphalt once again. Please critique my understanding by pointing out the flaws in my analogies if there are any, which is certainly likely.",['multivariable-calculus']
2423431,Probability function for distance d from the center of a point picked at random in a unit disk,"Assume there is a unit disk with radius = 1 and centered at $C$. Randomly and uniformly pick a point $P$ in the disk. What is the expected distance between $C$ and $P$? Solution: Since $P$ is $\bf{Uniformly Distributed}$, we know the probability is $\frac{1}{\pi}$, use polar coordinates substitution $x = r\cos{\theta}$ and $y = r\sin{\theta}$, we know $E[\sqrt{x^2+y^2}] = \frac{1}{\pi}\int_{0}^{2\pi}\int_0^1r*rdrd\theta = \frac{2}{3} $ Here is the problem. How do we generate a uniformly randomly distributed $P$ in real life? At my original thinking, there are two independent variables, $\theta$ and $r$, every point in the disk can be represented by these two variables. Thus, we uniformly pick an angle from $[0, 2\pi)$ and distance from $[0, 1)$. But in this way, the probability of choosing a point becomes $\frac{1}{2\pi}*1=\frac{1}{2\pi}$ , which is different from $\frac{1}{\pi}$ that I claimed before. Also, in this setup, the expected distance from any point to the center becomes $\frac{1}{2}$, since it is $[0, 1)$ uniform distribution. This contradiction gives me trouble and I can only conclude that the distance probability isn't uniform $[0, 1)$ distributed. Actually, from this link http://mathworld.wolfram.com/DiskPointPicking.html it actually says that ""The probability function for distance d from the center of a point picked at random in a unit disk is $P(d) = 2d$. Indeed, if this is the probability function for distance, the expectation is easy to calculate, $\int_0^1 2rdr=\frac{2}{3}$, which is the same as before. Also, the total probability becomes $\int_0^{2\pi} \int_0^1 2r(\frac{1}{2\pi})rdrd\theta = 1$. I know why $dxdy = rdrd\theta$ when transforming $x,y$ to $r, \theta$, but it is not that easy to imagine the distance is not uniformly distributed. Can someone give an easy to understand explanation? Follow-up question, what if the shape of object is more complicated? As an example, if on x-y plane, I draw a equilateral triangle and be asked to uniformly pick a point inside the triangle, how to do it? Previously, I was thinking use rotation matrix. Give the vector representation of two sides, each decides an angle (uniformly between $[0,\frac{\pi}{3}]]$ to rotate. But now I'm very worried that this way, it cannot generate really uniformed distributed points. What if the triangle is not symmetric?","['simulation', 'probability']"
2423433,Proof that a Octahedron graph is 3-colorable?,"I worked on a problem that gives an adjacency matrix and lets you draw a graph from that. After some time I found that the graph is a) planar and b) not any planar graph, but a Octahedron. The next part asked for a coloring of the graph and a proof thereof. We know that every planar graph can be colored in at most 4 colors by the 4-Color-Theorem, but after some trying I found out the above graph can be actually colored in 3 colors. Is there any straightforward proof of that, besides a proof by picture that just shows the actual coloring? It has to do with the triangles that exist in the graph, so I thought a bit about using some kind of structural induction that starts with one triangle and then adds more until the structure is complete, but I think that is also quite involved.","['graph-theory', 'proof-writing', 'planar-graphs', 'discrete-mathematics']"
2423501,"An $n×n$ chart, where n≥4 with ""+"",""−"" signs","On an $n\times n$ chart, where $n \geq 4$, stand ""$+$"" signs in the cells of the main diagonal and ""$-$"" signs in all the other cells. You can change all the signs in one row or in one column, from $-$ to $+$ or from $+$ to $-$. Prove that you will always have $n$ or more $+$ signs after finitely many operations. I am looking for a solution by induction. I tried to solve it but I wasn't able to continue to a solution.you can also get help from: https://artofproblemsolving.com/community/c6h609872 as it's available in links this question is from Russian Olympiad 2010. I tried to answer it by induction but the base case isn't trivial but I suppose it's true and I continued my solution: Suppose we have done a number of operations on the board. Clearly, we can assume that we did at most one operation on each row or column. By the hypothesis, the lower right $(n-1)\times(n-1)$ sub board contains at least $n-1$ pluses. If the union of the first row and the first column contains a plus, then we're done. Suppose otherwise. Assume WLOG that we did the operation on the first row, but not on the first column. Therefore we did the operation on all columns except the first, and we did not do the operation on any rows except the first. Now it is easy to check that the lower right $(n-1)\times(n-1)$ sub board contains at least $n$ pluses, and we're done. please help me to prove the base case.","['combinatorics', 'induction']"
2423523,$(n+1)$th prime $p_{n+1}$ less than or equal to $p_1p_2\dots p_n+1$,"I'm trying to understand some particulars of the following proposition: First, let me write down the theorem on which the subsequent proposition relies. Theorem. (Euclid). There are infinitely many primes. Proof : Suppose that there are finitely many primes, say, $p_1, p_2, \ldots, p_n$ . Consider $$m=p_1 p_2 \cdots p_n$$ Consider $$m=p_1 p_2 \cdots p_n+1\ge 2$$ By the Fundamental Theorem of Arithmetics, $m$ is a product of primes. Thus $p_k \mid m$ for some $1\le k \le n$ . Since $p_k\mid m$ and $p_k\mid p_1 p_2 \cdots p_n$ , we have that $p_k \mid (m-p_1 p_2 \cdots p_n)$ , i.e. $p_k\mid 1$ , which leads to a contradiction. Thus there are infinitely many primes. $$$$ Proposition : For $n\in\mathbb{N}$ , we have $p_n\le 2^{2^n}$ , where $p_n$ is the $n$ -th prime number. The proof by induction goes as follows. For $k=1$ , we have $p_1 = 2\le 2^2=4$ . Suppose that the result holds for $1\le k \le n$ . We have seen in the proof of the Theorem above that $$p_{n+1}\le p_1p_2\cdots p_n+1$$ Thus, by our induction hypothesis, $$p_{n+1}\le 2^{2^1} 2^{2^2}\cdots 2^{2^n}+1=2^{2^{n+1}-2}+1\le 2^{2^{n+1}}$$ By induction, proposition holds. What I don't seem to get is how it follows from the Theorem above that $$p_{n+1}\le p_1p_2\cdots p_n+1$$ Moreover, how can one immediately see (other than by proof by induction) that $2^1 + 2^2 + \cdots + 2^n = 2^{n+1}-2$ ? Would appreciate some clarifications.","['number-theory', 'analytic-number-theory', 'prime-numbers']"
2423581,Limit points of a sequence constructed from pi (if pi is normal),"I'm interested in this question apparently posed by John Nash, which I found in the book A Beautiful Mind. If you make up a bunch of fractions of pi $3.141592\ldots$. If you start
  from the decimal point, take the first digit, and place decimal point to
  the left, you get $.1$ Then take the next 2 digits $.41$ Then take the next 3 digits $.592$ You get a sequence of fractions between $0$ and $1$. What are the limit points of this set of numbers? I would like to know what can be deduced by assuming $\pi$ is normal? It seems to me that, if $\pi$ is normal, the sequence defined above must be non-convergent and therefore have at least two limit points (see the discussion here ). But I think by choosing carefully which subsequences to look for, you should be able to construct as many limit points as you like (again, if $\pi$ is normal). For example, you could look for subsequences bounded by $[0,1/3)$, $[1/3,2/3)$ and $[2/3,1]$ to find three distinct limit points. Could anyone tell me if this reasoning is correct?","['pi', 'sequences-and-series', 'calculus', 'limits']"
2423596,Jordan-Chevalley vs Jordan normal decomposition,"I am confused about a proof of the Jordan-Chevalley decomposition I was reading in Peterson's linear algebra book. Let $T : V \to V$ be an $n$-dimensional operator on a complex vector space. The Jordan-Chevalley decomposition tells us that $T = S + N$, where $S$ is diagonalisable, $N$ is nilpotent, and $SN = NS$. If
$$m_T(t) = (t - \lambda_1)^{m_1}\cdots(t - \lambda_k)^{m_k}$$
is the minimal polynomial of $T$, where the $\lambda_i$ are distinct, then we have a $T$-invariant decomposition
$$V = \ker(T - \lambda_1I)^{m_1} \oplus \cdots \oplus \ker(T - \lambda_kI)^{m_k}.$$
Then a matrix representation for $T$ can be written as a block diagonal matrix where the $i$th diagonal block is a matrix representation for $T$ restricted to $\ker(T - \lambda_i)^{m_i}$ (call this matrix $T_i$). Using the rational canonical form, one can see that $T_i$ is similar to
$$\left[\begin{matrix} \lambda & 1 & \cdots & 0 \\ 0 & \lambda \\ \vdots & & \ddots & 1\\ 0 & & & \lambda \end{matrix}\right].$$
This is a Jordan block, however. So I don't understand how the Jordan-Chevalley decomposition is different from the Jordan decomposition. Yet I know they can't be the same, because this proof seems to imply that there is only one block for each distinct eigenvalue, which is not necessarily true. What exactly is going on here?","['matrices', 'jordan-normal-form', 'linear-algebra']"
2423613,Why isn't the arc length of $\cos x$ equal to $\pm \sin x$?,"The arc length of some function $f(x)$ is given by $$L=\int{\sqrt{1+f'\left(x\right)^2}}dx$$
Plugging $\cos x$ in for $f$ and using basic algebra, this simplifies as follows
$$L=\int{\sqrt{1+\cos'\left(x\right)^2}}dx$$
$$L=\int{\sqrt{1-\sin\left(x\right)^2}}dx$$
$$L=\int{\sqrt{\cos\left(x\right)^2}}dx$$
$$L=\int{\pm\cos\left(x\right)}dx$$
$$L=\pm\sin x$$
Obviously this is wrong. The real answer involves elliptic integrals, but my question is, why does this approach give such an incorrect result? What am I overlooking?","['derivatives', 'arc-length', 'trigonometry', 'calculus', 'integration']"
2423631,Different results in the apparently same improper integral (wolfram),"I thought the both limits above would give the same results, but it doesn't. Can someone explain why?","['wolfram-alpha', 'real-analysis', 'limits', 'improper-integrals', 'integration']"
2423642,Conservation law(s) of chemical reaction $\rm A \longrightarrow B + 2C$,"I am trying to find the conservation law for the following chemical reaction: $$\rm A \longrightarrow B + 2C$$ where: A converts into $B$ and $C$ at a rate $k (k>0)$. $[A]_0$, $[B]_0$, $[C]_0$ are initial concentrations of $A$, $B$ and $C$ $[B]_0$ and $[C]_0$ are initially $\rm 0\  M$. I am thinking the conservation laws are: $[B] + [A] = [A]_0$ $[C] + 2[A] = 2[A]_0$ Am I correct? Or am I getting it wrong?","['chemistry', 'ordinary-differential-equations', 'dynamical-systems']"
2423648,How to find orthonormal frame of given metric?,"I want to find the orthonormal frame associated to the following Riemann metric on $\Bbb R^5$: $$g=\begin{pmatrix}
1+y^2+t^2 & yw &t&0&-y\\yw& 1+w^2+t^2&0&t&-w\\
t&0&1&0&0\\
0&t&0&1&0\\-y&-w&0&0&1
\end{pmatrix},$$
where $(x,y,z,w,t)$ is local coordinate chart. It is clear to me that $e_5=\frac{\partial}{\partial t},e_4=\frac{\partial}{\partial w},e_3=\frac{\partial}{\partial z}$ but I don't know how to compute the $e_1$ and $e_2$?","['riemannian-geometry', 'tensors', 'smooth-manifolds', 'maple', 'differential-geometry']"
2423658,Covariance of increasing functions of random variables,"Let $X$ be a random variable and $f, g: \mathbb{R} \rightarrow
 \mathbb{R}$ be increasing functions. Show that $cov(f(X), g(X)) \ge
 0$. The following hint was also provided: Assume $X$, $Y$ are independent and identically distributed, then show $E[(f(X)-f(Y))(g(X)-g(Y))] \ge 0$. My attempt: Since $f$ and $g$ are increasing, then $(f(x) - f(y))(g(x) - g(y)) \ge 0$ for all $x, y \in \mathbb{R}$. Thus, $E[(f(X) - f(Y))(g(X) - g(Y))] \ge 0$ by the monotonicity of expectations. Expanding, we get $E[f(X)g(X)]-E[f(X)g(Y)]-E[f(Y)g(X)]+E[f(Y)g(Y)] \ge 0$. Now due to independence, the LHS becomes $E[f(X)g(X)]-E[f(X)]E[g(Y)]-E[f(Y)]E[g(X)]+E[f(Y)g(Y)]$. Then due to identically distributed, the LHS further becomes $2E[f(X)g(X)]-2E[f(X)]E[g(X)]$. So together we have $2cov(f(X), g(X)) \ge 0$ and we are done. My main query: This whole proof relies on the fact that $X$ and $Y$ are i.i.d. How can we just simply assume this? If we didn't make this assumption, then we would never have been able to break up the expectations and collect like terms. Is this proof correct or do I need a proof that does not rely on the iid of $X$ and $Y$?","['expectation', 'probability', 'covariance']"
2423681,Interpretation of nabla followed by a dot (which is not meant as divergence),"I am studying a paper and the authors use $\nabla \cdot u$ for a real valued function $u\colon \mathbb{R}^n \to \mathbb{R}$ and I am quite confused how to interpret that, even after long searches in books and the internet. They use this construct also for other real valued functions at other positions in the paper, therefore I think its not a mistake. Can anyone see what is meant by that(I have no background in physics, maybe there is such a notation in physics)? Since $u$ is real valued I hardly can interpret it as divergence.","['physics', 'calculus', 'divergence-operator', 'analysis', 'vector-analysis']"
2423719,"$\mathcal{O}_{\mathbb{A}^n,x}\simeq \mathbb{C}[X_1,...,X_n]_{m(x)}$","I try to understand the proof of the following proposition: For every point $x=(x_1,...,x_n)\in \mathbb{A}^n$ there exists a
  natural isomorphism $$\mathcal{O}_{\mathbb{A}^n,x}\simeq \mathbb{C}[X_1,...,X_n]_{m(x)}$$ where $m(x)=(X_1-x_1,...,X_n-x_n)$. The proof starts with There exists a natural map from $\mathbb{C}[X_1,...,X_n]_{m(x)}$ to
  $\mathcal{O}_{\mathbb{A}^n,x}$ how associates to every fraction the
  germ of  the function that it defined. An element from $ \mathbb{C}[X_1,...,X_n]_{m(x)}$  has the form $\frac{p}{q}$ where $p,q\in \mathbb{C}[X_1,...,X_n]$, $q(x)\neq 0$. On the other side $\mathcal{O}_{\mathbb{A}^n,x}=\bigcup\limits _{x\in U} {\lbrace f\in \mathcal{O}_{\mathbb{A}^n}(U)\rbrace}/\sim$, where $g\in \mathcal{O}_{\mathbb{A}^n}(U)\sim h\in \mathcal{O}_{\mathbb{A}^n}(V)$ iff there exists $W\subseteq U\cap V$ s.t. $g=h$ on $W$. The isomorphism can be written explicitly?","['germs', 'sheaf-theory', 'algebraic-geometry']"
2423737,"$\sqrt{2^2} = 2$, $\sqrt{2^4} = 2^2 = 4$, $\sqrt{2^6} = 8 \neq 6$, $\ldots$","In general, if $t \in \mathbb{N}$, when is it true that
$$\sqrt{2^{2t}} = 2^t = 2t?$$ I know of course that it is true when $t - 1 = \log_{2}(t)$.  Are there other ( more arithmetic ) conditions, for which the equation $2^t = 2t$ holds, for $t \in \mathbb{N}$? It appears to be true when $t = 2^s$, for some $s \in \mathbb{N}$.","['algebra-precalculus', 'number-theory', 'diophantine-equations', 'elementary-number-theory']"
2423796,Why is this proof incorrect? (limit product is product of the limits),"I want to prove that if: $$\lim_{n \to \infty}s_n = L_1, \lim_{n \to \infty}t_n = L_2$$ then $$\lim_{n \to \infty}(s_n t_n) = L_1L_2$$ Wrong (?) proof: Fix $\epsilon >0$. By definition, there are integers $N_1,N_2$ such that: $$n>N_1 \implies |s_n-L_1|< \frac{\epsilon}{|s_n|+|L_2|}$$
$$n>N_2 \implies |t_n-L_2|< \frac{\epsilon}{|s_n|+|L_2|}$$ Hence, for $n > \max\{N_1,N_2\}$, we have: $$|s_nt_n - L_1L_2| = |s_n(t_n - L_2) + s_nL_2 - L_1L_2|$$
$$\leq |s_n||t_n - L_2| + |L_2||s_n - L_1|$$
$$< \frac{\epsilon}{|s_n|+|L_2|} (|s_n| + |L_2|) = \epsilon$$ I was taught that the $\epsilon$ can't depend on $n$, but I can't see why. What goes wrong? EDIT: I know how to fix the proof, I made a post on this one: Limit of product of sequences is the product of the limits of the sequences","['epsilon-delta', 'real-analysis', 'sequences-and-series', 'limits']"
2423820,"Euler characteristic of a pair of sheaves $(E,F)$?","I understand the notion of Euler characteristic of an algebraic variety $X$ (say) in terms of the dimensions of the cohomology groups of $X$. In Huybrecht's book ""The Geometry of Moduli Spaces of Sheaves"" he gives a definition of the Euler characteristic in terms of a pair of sheaves $(E,F)$. The definition reads 
$$
\chi(E,F) := \sum_i (-1)^i \text{dim Ext}(E,F)
$$
But I do not understand how this is related to the topological invariant of some underlying space. Are both $E$ and $F$ over $X$? Is $F$ a subsheaf of $E$? What is going on here exactly?","['algebraic-topology', 'sheaf-theory', 'algebraic-geometry']"
2423846,Distance between two lines in parametric form,"a) Parametrize the line $L$ through $P = (2, 1, 2)$ that intersects the
  line $x = 1 + t$, $y = 1 − t$, $z = 2t$ perpendicularly. b) Parametrize the $z$ axis. c) What is the distance from this line $L$ to the $z$-axis? My work For the part a), I got the equation was $x = 2 + \frac{t}{6}$ ; $y = 1 + \frac{5t}{6}$ ; $z = 2 + \frac{t}{3}$. Stuck with the last 2 parts. Anyone want to give this math novice a hand? :D","['multivariable-calculus', 'parametric', 'vectors', 'calculus']"
2423904,how to parametrize a curve in $\mathbb R^3$?,"How to parametrize $x^2+y^2+z^2=4$ and $x+z=2$ $x$, $y$ and $z$ should be function of $t$ I have tried to eliminate $z$ but it doesn't work",['multivariable-calculus']
2423921,Generalized limit in $l_\infty$ (Using: Hahn Banach Extension Theorem),"I am trying to solve the following problem (found in Maddox's book ""Elements of Functional Analysis"", page 128): So we have the function $p:l_\infty\rightarrow\mathbb{R}$ given by $$
p(x) = \inf\left\{\limsup_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}
$$ In which the infimum is taken over all finite sets of positive integers $\{i_1,\ldots,i_k\}$. I already showed that: $p$ is well defined in $l_\infty$ $p(\alpha x)=\alpha p(x)$, for all $x\in l_\infty$ and $\alpha >0$ $p$ is subadditive, i.e $p(x+y)\leq p(x)+p(y)$, for all $x,y\in l_\infty$ This part I'm not sure that it's the best way to do it: To obtain a functional as the problem states, I defined first another functional $g:c\rightarrow \mathbb{R}$ ($c$={space of convergent sequences}), given by: $$
g((x_n)_n) = \lim_{n\rightarrow\infty} x_n
$$ It was not difficult to show that: $g$ is linear $g(x)\leq p(x)$, for all $x\in c$. (Actually we'll have $g(x)=p(x)$) $g((1,1,1,\ldots))=1$ Now, the Hahn Banach Extension Theorem states that there is a functional $f:l_\infty\rightarrow \mathbb{R}$ such that: $f(x)=g(x)$, for all $x\in c$ $f(x)\leq p(x)$, for all $x\in l_\infty$ Now I just have to show that $f$ satisfies the conditions given by the problem, which are: $f(x)=1$ if $x_n= 1$ for all $n$ $f(x)\geq 0$ if $x_n\geq 0$ for all $n$ $f((x_2,x_3,\ldots))=f((x_1,x_2,\ldots))$ (1) is straightforward, since the condition is valid for $g$. My Difficulties: (2) I managed to demonstrate this one, but I'm not sure I'm correct, this is my solution: For every $x\in l_\infty$ we have $$
f(x) \leq p(x) = \inf\left\{\limsup_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}
$$ So we have \begin{align}
-f(x) = f(-x) \leq p(x) &= \inf\left\{\limsup_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k (-x_{n+i_j})\right) \right\}\\
&= \inf\left\{\limsup_{n\rightarrow\infty}\left(-\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}\\
&= \inf\left\{-\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}\\
&= -\sup\left\{\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}
\end{align} From which we get $$
f(x) \geq \sup\left\{\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}
$$ And clearly if $x_n\geq 0$ for all $n$, then $$
f(x) \geq \sup\left\{\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\} \geq 0
$$ Is this correct? (3) This is my biggest problem, I have little clue on how to show this, it is clear that in $c$ it is valid. I have the feeling that using the following inequality I can get somewhere: $$
\sup\left\{\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\} \leq f(x) \leq \inf\left\{\limsup_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}
$$ but I haven't been able to do much. If we define 
$$q(x)=\sup\left\{\liminf_{n\rightarrow\infty}\left(\frac{1}{k}\sum_{j=1}^k x_{n+i_j}\right) \right\}$$ I have no trouble in noticing that: \begin{align}
p((x_2,x_3,\ldots))=p((x_1,x_2,\ldots))\\
q((x_2,x_3,\ldots))=q((x_1,x_2,\ldots))
\end{align} If we had $p(x)=q(x)$ then it would be done, but I think this is not the case. Any suggestions?","['functional-analysis', 'sequences-and-series']"
2423926,Proof verification: $\frac{d}{dx}\sin^{-1}(x)=\frac{1}{\sqrt{1-x^2}}$,"In this question, I will use $\sin^{-1}(x)$ instead of $\arcsin(x)$ since to me it's simpler this way. Here's the proof that needs verifying: $$y=\sin^{-1}(x)$$ hence, $$x=\sin(y)$$ $$\frac{dx}{dy}=cos(y)$$ $$\frac{dy}{dx}=\frac{1}{cos(y)}$$ then, using the $\sin^2(y)+\cos^2(y)=1$ identity, $\cos(y)=\pm\sqrt{1-\sin^2(y)}$, or $\cos(y)=\pm\sqrt{1-x^2}$, given that $x=\sin(y)$. However, since $-\frac{\pi}{2}<\sin^{-1}(x)<\frac{\pi}{2}$, and $y=\sin^{-1}(x)$, and $\cos(y) > 0$ in the given interval, it can be inferred that the answer is: $$\frac{dy}{dx}=\frac{1}{\sqrt{1-x^2}}$$ the question is, is this proof correct, since at some point throughout this proof: it was implied that $$\frac{dy}{dx}=\sec(y)$$ and therefore that $$\sec(y)=\frac{1}{\cos(\sin^{-1}(x))}=\frac{1}{\sqrt{1-x^2}}$$ and I've proven numerically that this identity is true for the value $x=\frac{1}{2}$, but I don't know if this is true for every value of $-1 < x < 1$and if therefore, the means of proving I've used are false. TLDR: Is this proof correct and rigorous?","['derivatives', 'trigonometry', 'proof-verification']"
2423938,Prove $\lim_{x\to 2} \sqrt{x^2+5} = 3$,"As stated in the title, I need to prove that $\lim_{x\to 2} \sqrt{x^2+5} = 3$ using only the precise definition of a limit. For any given $\varepsilon \gt 0$, there exists a $\delta = $ Such that $0 \lt \lvert x-2 \rvert \lt \delta \Rightarrow \lvert \sqrt{x^2+5} \rvert \lt \varepsilon $ I've attempted to convert $\sqrt{x^2+5}$ into the following: $\sqrt{x^2+5} = \sqrt{(x^2-4)+9}\\
 \sqrt{x^2+5} = \sqrt{(x^2-4x+4)+4x +1} = \sqrt{(x-2)^2+4(x-2)+9} $ I have a hunch that I am heading in the wrong direction. Can I get some advice on what I might be doing wrong?","['radicals', 'epsilon-delta', 'limits']"
2423945,Proof by Definition: $X$ is a subset of $\bigcup \mathcal{P}(X)$,"I have to prove, only by definition (and valid forms of inference), that $X$ is a subset of the union of the power set of $X$, i.e. $X \subseteq \bigcup \mathcal{P}(X)$. I keep getting stuck. Does anyone have any suggestions at all?","['proof-writing', 'elementary-set-theory', 'discrete-mathematics']"
2423977,Show that the divergence operator is onto,"Came across this while reading Temam's text on Navier Stokes.
Lemma 2.4 states that the divergence operator $\nabla\cdot$ maps $\bf{H}_0^1(\Omega)$ $\textit{onto}$ $L^2(\Omega)/\mathbb{R}$ where the latter is the subspace of $L^2(\Omega)$ of elements $f$ satisfying $\int_\Omega f=0$. To prove this statement we consider $A=\nabla$ (the gradient operator) defined on $L^2(\Omega)$ (mapping into $\bf{H}^{-1}(\Omega)$). Earlier, it'd been shown that the operator $A$ restricted to $L^2(\Omega)/\mathbb{R}$ is injective. So if we simply set $R(A):=A(L^2(\Omega)/\mathbb{R})\,(=A(L^2(\Omega))$, we know that $A$ is an isomorphism between $L^2(\Omega)/\mathbb{R}$ and $R(A)$. So far, I don't think anything profound has been said. Now, the proof proceeds with the following statement (paraphrased): by transposition, the adjoint $A^*\in L(\bf{H_0^1(\Omega)}$ $,L^2(\Omega))$ is an isomorphism from the $\textit{orthogonal}$ of $R(A)$  $\textit{onto}$ $L^2(\Omega)/\mathbb{R}$. My question is, what's the justification for this last statement?? 
Perhaps I'm a little rusty, but I'm guessing I'm failing to make the necessary manipulations to relate the ranges of $A$ and $A^*$ and the orthogonal spaces of them etc. etc.","['functional-analysis', 'operator-theory', 'partial-differential-equations']"
2424005,What does infinity in complex analysis even mean?,"Ive always thought that infinity isn't really a number. It is just an idea - a name we attach to something that grows without bound. So in real analysis, when the terms of a sequence or partial sums of a sequence (series) keep increasing without an upper bound, we say the sequence or the series goes to infinity. Negative infinity is the same idea, but with a minus sign, that is negative terms, which keep decreasing without any lower bound go to $-\infty$. And this ""infinity"" object is bigger than any number you could possibly think of (because it isn't a number in itself). All fine. But attached with this idea of bigger than everything else, is the notion of big or small, i.e. order. However for complex numbers, there is no total order. We just can't compare any 2 given complex numbers and say which is ""bigger"". How, then is infinity thought of in complex analysis? It can't be an element that is bigger than all other elements, because ""bigger"" doesn't make any sense. And since there is only one infinity in the complex plane, unlike $+\infty$ and $-\infty$ in $\mathbb{R}$, does that mean that the complex infinity is more of a scalar (with only a modulus and no direction or argument) than a vector (like we can associate a vector with all other complex numbers)? I know I sound very confused. I am. Please shed some light.
Thanks.","['complex-analysis', 'infinity', 'complex-numbers']"
2424021,"If $X$ is separable, then the closed unit ball of $X^*$ is weak-star metrizable. Some calculus helps needed!","Here is my effort to show this fact and I will use ball $X^*$ to denote the closed unit ball of $X^*$. To show ball $X^*$ is weak star metrizable, we only have to show there is a metric $d$ on ball $X^*$ such that the topology induced by $d$ is the weak-star topology on ball $X^*$. Since $X$ is separable, ball $X$ is also separable. Thus there exists a countable dense subset $\{x_n\}$ in ball $X$. Now define the metric $d$ on ball $X^*$ by $$d(x^*,y^*)=\sum_{n=1}^{\infty}\frac{|\langle x_n,x^*-y^*\rangle|}{2^n},$$ where $x^*,y^*\in\text{ball $X^*$}$. Let $T$ be the topology induced by $d$ and $wk^*$ be the weak-star topology on $\text{ball $X^*$}$. Then we need to show $T=wk^*$. And I try to use net convergence to show topology equivalence. Let $x^*\in\text{ball $X^*$}$ and let $x_i^*$ be a net in $\text{ball $X^*$}$ such that $x_i^*\overset{wk^*}{\longrightarrow} x^*$. Then $\langle x_n,x_i^*\rangle\rightarrow\langle x_n,x^*\rangle$ for all $n$. Now Let $x^*\in\text{ball $X^*$}$ and let $x_i^*$ be a net in $\text{ball $X^*$}$ such that $x_i^*\rightarrow x^*$ in $(X,T)$. Then for each $\epsilon>0$, there exists $i_\epsilon\in\mathbb{N}$ such that $d(x_i^*,x^*)<\epsilon$ for all $i\geqslant i_\epsilon$; that is, for each $\epsilon>0$, $$d(x_i^*,x^*)=\sum_{n=1}^{\infty}\frac{|\langle x_n,x_i^*-x^*\rangle|}{2^n}<\epsilon.$$ Here I want to show $x_i^*\overset{wk^*}{\longrightarrow} x^*$ if and only if $x_i^*\rightarrow x^*$ in $(X,T)$. But I forget some knowledge in Calculus. Can somebody help me to show this please? Thank you so much!!","['functional-analysis', 'normed-spaces', 'metric-spaces', 'calculus']"
2424030,Prove $\lim_{x\to 1} \frac{x+2}{x^2+1}=\frac{3}{2}$,"The title is quite clear. I am required to prove $\lim_{x\to 1} \frac{x+2}{x^2+1}=\frac{3}{2}$ using the epsilon-delta definition of the limit. Given any $\varepsilon \gt 0$, there exists a $\delta =$ Such that $0 \lt \lvert x-1 \rvert \lt \delta \Rightarrow \lvert\frac{x+2}{x^2+1} -\frac{3}{2} \rvert \lt\varepsilon$ $\frac{x+2}{x^2+1} -\frac{3}{2}= \frac{-3x^2+2x+1}{2(x^2+1)}$ I have managed to express both the numerator and the denominator as such $-3x^2+2x+1 = -(x-1)(3x+1)\\
2(x^2+1) = 2x^2+2= 2(x-1)^2 + 4(x-1) +4$ Returning back to the fraction $\frac{-(x-1)(3x+1)}{2(x-1)^2 + 4(x-1) +4}$ I am unsure on how to continue and would really appreciate some guidance.","['epsilon-delta', 'limits']"
2424032,Understanding why the empty set is closed,"Definition . A set is called closed if its complement in $\mathbb{R}$ is open. In my lecture notes it says: $\emptyset$ is closed because $\emptyset = \emptyset \setminus \mathbb{R}$ and $\mathbb{R}$ is open. I think there is a typo because $\emptyset \neq \emptyset \setminus \mathbb{R}$, right? It should be $\emptyset = \mathbb{R} \setminus \mathbb{R}$. Can you please check this?","['general-topology', 'analysis']"
2424090,Give an example of a topological space $X$ and a finite subset $A$ of $X$ such that $\bar{A}$ is an infinite set,"Give an example of a topological space $X$ and a finite subset $A$ of $X$ such that $\bar{A}$ is an infinite set. I have thought a lot about this question and all I can think of is to consider $\mathbb{R}$ with the discrete topology. In this topology, the set $A=\left \{  1,2\right \}$ is open and $\overline{A}$ is infinite because all the sets have $1$ and $2$, which are infinite… Is this reasoning right? Many thanks.",['general-topology']
2424112,Infinite series involving Von Mangoldt's function,"I am trying to compute the following infinite sum $$\sum_{n=2}^\infty \,\frac{\Lambda(n)}{n^2 \ln n},$$ where $\Lambda(n)$ is Mangoldt's function. It seems to me that the result is strictly less than 1/2. Does somebody know a reference where that series is computed?","['number-theory', 'analytic-number-theory', 'sequences-and-series']"
2424180,$X$ is a topological space s.t. every continuous $f:X\rightarrow \mathbb{R}$ is bounded. Is X compact?,"A well-known basic topological result is that if $X$ is a compact topological space, then every continuous function $f:X\rightarrow \mathbb{R}$ is bounded. That raises the natural question - is the converse also true? Or maybe there exists a non-compact space that still holds that property, ""every continuous real function is bounded""?","['general-topology', 'compactness']"
2424220,Limit with complex exponential: using the fact that it is bounded,"Consider the following integral $$\int_{0}^{\infty} e^{(-a x)}e^{i(bx)}dx\,\,\,\,\,\,\,\,\,\,\,\,\,\, a,b \in \mathbb{R}\,\,\,\,\,\,\,\,\ a>0$$ When evaluated it becomes $$\frac{1}{ib-a}\lim_{c\to \infty} \bigg[e^{(-a x)}e^{i(bx)}\bigg]\,\, \bigg|_{0}^{c}=\frac{1}{ib-a}\bigg\{-1+\lim_{c\to \infty}e^{(-a c)}e^{i(b c)}\bigg\}=\frac{1}{a-ib}$$ 
Now the following limit is zero $$\lim_{c\to \infty}e^{(-a c)}e^{i(b c)}=0$$
But is it correct to say that it is zero because $$\lim_{c\to \infty}e^{(-a c)}=0 \,\,\,\,\,\mathrm{and} \,\,\,\,\, |e^{i(b c)}|<1 \,\,\,\,\, \forall c  \,\,\,\,?$$
i.e. because the limit of the real exponential is zero and the complex exponential is bounded ?","['complex-numbers', 'limits', 'calculus', 'complex-analysis', 'improper-integrals']"
2424234,Calculate inner product of two vectors,"I know that the title is vague, at least for what I am asking. 
Anyway, this is an exercise that I've been struggling with for the past two hours, it seems relatively simple on a first glance, and I've tried a method which proved to be wrong. I'll go through what I've tried, but before I do I will briefly provide an overview of the exercise: Let $ e_1, e_2, e_3$ be a basis in space $\mathbb R$ $ u = e_1 + 2e_2 +3e_3 $   and $ v = 2e_1  - 3e_2 + 4e_3 $ $ |e_1| = 3, |e_2| = 4, |e_3| = 2$ $ e_1  \cdot e_2 = -6 $ $ e_1 \cdot e_3 = 4  $ $ e_2 \cdot e_3 = 4$ Calculate $ u \cdot v $ so this is what I've tried doing: From $u \cdot v = |u|  |v|  \cos \theta$ I tried plugging the data we are given above into the forumla: $ \frac{u \cdot v}{|u||v|} = \cos\theta $ so one example would be: $ \frac{e_1 \cdot e_2}{|e_1| |e_2|} = \cos\theta $ $ \frac{ -6}{3 \times 4} = \frac{-1}{2}$ so $\cos\theta = -0.5$ of course the other two give me $\cos\theta = 0.5$ I then, individually, get the length of vector u and vector v by using
pythagoras: $ |u| = \sqrt{1^2 + 2^2 + 3^3 } = \sqrt{14} $ On a first glance, this does not seem right... applying the same 'logic' to $ |v| = \ldots = \sqrt{29} $ I then plug the data into the aforementioned formula ($u \cdot v = |u| |v| \cos\theta$ ) and basically get gibberish, it does not make any sense. I am doing something completely wrong, that I know, I am just not sure how to use the provided data to get an answer that is $ = 0 $ I'd sincerely love to get an answer as I've been on this exercise for over two hours to no avail. EDIT: Thank you all! It does make sense now that I look at it, and I somehow tried to over-complicate things (which is actually a real struggle of mine). However, Thanks to your help, I managed to solve it.","['linear-algebra', 'inner-products']"
2424251,"Is it true that $ \limsup_{T\to0+}F(0,t)=\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]$?","Consider a continuous function $F:\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}\to\Bbb R_{\ge0}$ where $\Delta_{\Bbb R_{\ge0}^2}:=\{(x,x):x\ge0\}$ is the diagonal of $\Bbb R_{\ge0}^2$. Suppose moreover $F$ symmetric, that is $F(t,s)=F(s,t)\;\;\forall (s,t)\in\Bbb R_{\ge0}\times\Bbb R_{\ge0}\setminus\Delta_{\Bbb R_{\ge0}^2}$ and $F(0,t)>0$ for all $t>0$. Is it true that
$$
\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]
=\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]\;\;?
$$
It seems clear that RHS is $\ge$ than LHS; is the converse true? I have a proof, but in the comments there is a counterexample: where is the problem? PROOF: We distinguish two cases for the value of RHS. Let us suppose first $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\in\Bbb R_{\ge0}$. If $c=0$ then the conclusion is trivial. Suppose thus $c>0$. Now for every $n\in\Bbb N$ there exists $\delta_n>0$ such that
$$
\left|\sup_{0\le s<t\le\delta_n}F(s,t)-c\right|<\frac1n
$$
and for every $n\in\Bbb N$, fixed $\delta_n$, there exists $\{(s_m^{(\delta_n)},t_m^{(\delta_n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that
$$
F(s_m^{(\delta_n)},t_m^{(\delta_n)})\stackrel{m\to+\infty}{\longrightarrow} \sup_{0\le s<t\le\delta_n}F(s,t).
$$
from which we get
$$
\lim_{n\to+\infty}F(s_n^{(\delta_n)},t_n^{(\delta_n)})
=\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=c\;
$$
(moreover it is clear that $s_n^{(\delta_n)},t_n^{(\delta_n)},\delta_n\to0$).
Then, by continuity, $\forall\varepsilon>0\;\;\exists N_{\varepsilon}\ge1$ such that
$$
\left|F(0,t_n^{(\delta_n)})-F(s_n^{(\delta_n)},t_n^{(\delta_n)})\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon}
$$
and
$$
\left|F(s_n^{(\delta_n)},t_n^{(\delta_n)})-c\right|<\frac{\varepsilon}2\;\;\;\;\;\forall n\ge N_{\varepsilon},
$$
from which we get
$$
\left|F(0,t_n^{(\delta_n)})-c\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon},
$$
and thus
$$
\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]
=\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(\delta_n)}}F(0,t)\right]
\ge \lim_{n\to+\infty}F(0,t_n^{(\delta_n)})=c
$$
which allows to conclude, since clearly $\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]\le c$. The case $\lim_{T\to0+}\left[\sup_{0\le s<t\le T}F(s,t)\right]=+\infty$ is only slightly different: since
$$
T\mapsto\sup_{0\le s<t\le T}F(s,t)
$$
is not decresing, it is clear that
$$
\sup_{0\le s<t\le T}F(s,t)=+\infty\;\;\;\forall T>0.
$$
In particular fixing $n\ge1$ we have that $\sup_{0\le s<t\le \frac1n}F(s,t)=+\infty$, thus there exists $\{(s_m^{(n)},t_m^{(n)})\}_{m\ge1}\subset\Bbb R_{\ge0}^2\setminus\Delta_{\Bbb R_{\ge0}^2}$ such that
$$
\lim_{m\to+\infty}F (s_m^{(n)},t_m^{(n)})=+\infty,
$$
and since this holds true for every $n\ge1$ we can write
\begin{equation}
\lim_{n\to+\infty}F (s_n^{(n)},t_n^{(n)})=+\infty.
\end{equation}
Now, being clearly $s_n^{(n)},t_n^{(n)}\to0$ as $n\to+\infty$ and since $F$ is continuous, $\forall\varepsilon>0$ $\exists N_{\varepsilon}\ge1$ such that
$$
\left|F(0,t_n^{(n)})-F(s_n^{(n)},t_n^{(n)})\right|<\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon};
$$
in particular we get
$$
F(0,t_n^{(n)})>F(s_n^{(n)},t_n^{(n)})-\varepsilon\;\;\;\;\;\forall n\ge N_{\varepsilon},
$$
from which 
$$
\lim_{n\to+\infty}F (0,t_n^{(n)})=+\infty.
$$
Then we can conclude as in the previous case:
$$
\lim_{T\to0+}\left[\sup_{0<t\le T}F(0,t)\right]
=\lim_{n\to+\infty}\left[\sup_{0<t\le t_n^{(n)}}F(0,t)\right]
\ge \lim_{n\to+\infty}F(0,t_n^{(n)})=+\infty.
$$ FINALLY: I wanted to treat my problem in general, but originally it was $$
F(s,t)=\frac{|f(t)-f(s)|}{|t-s|^{\lambda}}
$$
for some $0<\lambda\le1$ and for a function $f$ which is $\alpha$-Holder continuous for some $\alpha$. THE MOST IMPORTANT THING: If my proof is wrong, what kind of hypothesis we can put in order to obtain the result? THE GENESIS OF THIS POST: $f$ originally was the difference of two solutions of the following scalar SDE:
$$
x_t=\underbrace{\xi_0+\int_0^tb(s,x_s)\,ds+\int_0^t\sigma(s,x_s)\,dW_s^H}_{=:z_t}+y_t\;
$$
where
$$
y_t=\sup_{s\in[0,t]}\left(z_s\right)^{-}.
$$
and
$$
b:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R
$$
and
$$
\sigma:\Bbb R_{\ge0}\times\Bbb R\to\Bbb R
$$
are bounded measurable functions, called drift and diffusion coefficient respectively.
We will assume the following hypotesis on them: $$
|b(t,x)-b(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T]
$$ $$
|\sigma(t,x)-\sigma(t,y)|\le K_0|x-y|\;\;\forall x,y\in\Bbb R,\;\forall t\in[0,T]
$$ $$
|\sigma(t,x)-\sigma(s,x)|\le K_0|t-s|^{\nu}\;\;\forall x\in\Bbb R,\;\forall s,t\in[0,T]
$$
and $W^H$ is the fractional Brownian motion of Hurst index $1/2<H<1$; moreover the integral is the YOUNG INTEGRAL , which is defined as the limit of the usual Riemann Sums, or it can be equivalently viewed as follows (here we deal with deterministic case: we think to $W^H$ as a fixed path of the fBM): if $h$ is a $\lambda$-Holder real function, for $\lambda>1-H$, then it can be proved that
$$
\int_s^th_u\,dW_u^H=h_s(W_t^H-W_s^H)+\Lambda_{st}((h_c-h_a)(W_b^H-W_c^H))
$$
where $a,c,b$ are mute variables and $\Lambda:\mathcal{ZC}_3^{\mu}\to\mathcal{C}_2^{\mu}$ is the inverse of $\delta:\mathcal{C}_2^{\mu}\to\mathcal{ZC}_3^{\mu}$ which is defined as
$$
(\delta h)_{sut}:=-h_{ut}+h_{st}-h_{su}
$$
and
$$
\mathscr C_2^{\mu}:=\{h\in\mathscr{C}_{2}\;:\;\|h\|_{\mu}<+\infty\}
$$
where $\mathscr{C}_{2}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^2\to\Bbb R$ continuous and such that they vanish on diagonal.
Moreover
$$
\|h\|_{\mu}:=\sup_{\substack{s,t\in[0,T]\\s\neq t}}\frac{|h_{ts}|}{|t-s|^{\mu}}
$$
and
$$
\mathscr C_3^{\mu}:=\{h\in\mathscr{C}_{3}\;:\;\|h\|_{\mu}<+\infty\}
$$
where $\mathscr{C}_{3}$ is the $\Bbb R$-vector space of all functions $h:[0,T]^3\to\Bbb R$ continuous such that $h_{sut}=0$ whenever $s=u$ or $u=t$ and $\|\cdot\|_{\mu}$ is a suitable norm. Thus
\begin{align*}
f_t
&=x_t^{(1)}-x_t^{(2)}\\
&=\int_0^t(b(u,x_u^{(1)})-b(u,x_u^{(2)}))\,du+
\int_0^t(\sigma(u,x_u^{(1)})-\sigma(u,x_u^{(2)}))\,dW_u^H
+y_t^{(1)}-y_t^{(2)}
\end{align*} and
$$
F(0,t)=\frac{|f_t|}{t^H}
$$ IMPORTANT since the original exponent is $\rho>1$, then it would be enough to prove that this last limit is nonzero, in fact
$$
\frac{|f_t|}{t^{\rho}}=\frac{|f_t|}{t^{H}}\frac1{t^{\rho-H}}
$$","['continuity', 'real-analysis', 'limsup-and-liminf']"
2424290,Union of Two Measurable Sets is Measurable Using Diff. Def. of Measurability,"Use property (ii) of theorem 11 as the primitive definition of a measurable set and prove that the union of two measurable sets is measurable. Then do the same for property (iv). (ii) There is a $G_{\delta}$ set $G$ containing $E$ for which $m^*(G-E)=0$ . (iv) There is a $F_{\sigma}$ set $F$ contained in $E$ for which $m^*(E-F)=0$ . Suppose that $A$ and $B$ are measurable. Then there exist $G_\delta$ sets $G_1$ and $G_2$ such that $A \subseteq G_1$ and $B \subseteq G_2$ , and $m^*(G_1 - A) = 0 = m^*(G_2 -B)$ . Using the distributive laws of intersections and unions, we can show that $G_1 \cup G_2$ is $G_\delta$ set, which must contain $A \cup B$ . Moreover, $$(G_1 \cup G_2) - (A \cup B)$$ $$= (G_1 - (A \cup B)) \cup (G_2 - (A \cup B))$$ $$= ((G_1 - A) \cap (G_1 - B)) \cup ((G_2 - A) \cap (G_2 - B)) \subseteq (G_1 - A) \cup (G_2 - B),$$ and by monotonicity we get $$m^*((G_1 \cup G_2) - (A \cup B)) \le m^*((G_1 - A) \cup (G_2 - B)) \le m^*(G_1 - A) + m^*(G_2 - B) = 0$$ I believe the other proof is nearly identical: assume measurability of $A$ and $B$ , and get the $F_{\sigma}$ sets $F_1$ and $F_1$ ; the union is a $F_{\sigma}$ set, and at this point we are essentially done. So how does this sound?","['real-analysis', 'measure-theory']"
2424333,"Problem 12, chapter 1 of Walter Rudin's Functional Analysis","The question is to prove that the metric , defined below, induces usual topology on R :. $d_2(x,y)=|\phi(x)-\phi(y)|$ Where $\phi(x)=\frac{x}{(1+|x|)}$ and $d_1$ is the usual metric. The continuity of $\phi\times\phi$ and $d_1$ gives that $d_2$ is continuous when $\mathbb{R}$ is given usual metric, but I'm not able to do other way. Any help?","['functional-analysis', 'general-topology', 'metric-spaces']"
2424386,Kronecker quiver and tensor product,"Good day, based on representation theory of Assem, and the definition of tensor product I need to find $\varphi^1_{21}$ and its domain in the kronecker quiver. I did the next: but Im confused to find $F_0$ and therefore $F/F_0$ to deduce the domain $\varepsilon _2KQ\varepsilon _1\otimes  \varepsilon _1KQ\varepsilon _1$. Depends it of the field $K$ and of the external operation $ka$, $k\in K, a\in \varepsilon _2KQ\varepsilon _1$?","['tensor-products', 'abstract-algebra', 'tensors', 'quiver', 'representation-theory']"
2424388,"Find the center of the octagon, complex plane","The problem given read as follows, Two consecutive vertices of a regular convex octagon are $(1,2)$ and $(3,-2)$. Find the center of the octagon. I would like to know if my answer is correct. Here's what I have thought: I've drawn the complex plane and set the affixes given accordingly (I've called them $z_{1} = (1,2)$ and $z_{2}=(3,-2)$). Since $z_{1}, z_{2}, ... z_{8}$ compose a regular octagon, the angle between $z_{1}$ and $z_{2}$ must be $\frac{\pi}{4}$. So I have an angular relation at my disposal. Say $z_{0} = x+iy$ is the center of the polygon, then I have two vectors, namely $\vec{v} = \vec{z_{0} z_{2}}$ and $\vec{u} =  \vec{z_{0} z_{1}}$ and since I have an angle relation between them, I can write: $\vec{u} = \vec{v}.e^\frac{i \pi}{4} \iff z_{1}-z_{0} = (z_{2} - z_{0}).cis \left(\frac{\pi}{4} \right) (*)$ Substituting with my known affixes, I get: $1+2i-z_{0}=(3-2i-z_{0}).e^\frac{i \pi}{4}$ Working out some algebra, I've got: $z_{0} = \frac{3e^{\frac{i \pi}{4}} - 1}{e^\frac{i \pi}{4} - 1} + i\frac{-2-2 e^\frac{i \pi}{4}}{e^\frac{i \pi}{4} - 1}$ Converting from polar form to trigonometric form and doing a lengthy algebra, I've got: $\boxed{z_{0} = 2 + i \frac{\sqrt{2}}{2-\sqrt{2}}}$ I believe that my reasoning is correct, although substituting my result at $(*)$ I get: $\left(1 - \sqrt{2}, \frac{4-3\sqrt{2}}{2-\sqrt{2}}\right) \neq \left(\frac{3 \sqrt{2} - 2}{2}, \frac{1-2 \sqrt{2}}{2-\sqrt{2}}\right)$ I would appreciate if someone could help me figure out my mistake. Thanks, in advance.",['complex-analysis']
2424395,Counting the number of same elements in a sequence,"Let, $\mathcal{E'}$ is the set of all sequences where: Any element of $\mathcal{E'}$ is a subset of $\mathbb{N}\cup \{0\}=\mathbb{N}_1$. In any element of $\mathcal{E'}$, every distinct number appears finitely many times. Let, $\mathcal{E}$ is the set of all elements of $\mathcal{E'}$which are in non-decreasing order with entries from $\mathbb{N}_1$. ( We define a function $A:\mathcal{E'}\rightarrow \mathcal{E}$, where $A$ rearranges a element of $\mathcal{E'}$ in non-decreasing order.) Now choose an element(say, $S$) from $\mathcal{E}$. Let, $\rho$ is an operation on $S$, such that $\rho(S)=(f(n))_{n=0}^{\infty}$, where $f:\mathbb{N}_1\rightarrow \mathbb{N}_1$, with $f(n)=$ the of number of times $n$ appeared in the sequence $S$. Let $\mathcal{H}\subset \mathcal{E}$ is the set of all such $S\in \mathcal{E}$ for which $\rho(S)\in \mathcal{E'}$. Arranging elements of $\rho(S)$ in non-decreasing order, we will get an element of $\mathcal{E}$, which is $A(\rho(S))$. Essentially, if $A(\rho(S))\in \mathcal{H}$, we can get $\rho(A(\rho(S)))\in \mathcal{E'}$. Again, arranging elements of $\rho(A(\rho(S)))$ in non-decreasing order, we will get an element of $\mathcal{E}$, which is $A(\rho(A(\rho(S))))=(A\circ\rho)^2(S)$. We continue this process, and will get a set of sequences $((A\circ\rho)^m(S))_{m=0}^{\infty}\subset \mathcal{H}$ with $(A\circ\rho)^0(S)=\rho^0(S)=S$. Questions: Consider the set, $((A\circ\rho)^m(S))_{m=0}^{\infty}$. Let $N\in \mathbb{N}$. Can we find a $N$, for a given $S\in \mathcal{E}$ such that $(A\circ\rho)^N(S)=S$? Can we find a $S\in \mathcal{E}$, for each $N\in\mathbb{N}$ such that $(A\circ\rho)^N(S)=S$? I made this question actually, so if you find any problem, please let me know. Also see the MO post.","['sequences-and-series', 'elementary-set-theory']"
2424398,Partial derivative notation,"Suppose $F(x,y,z)=x+y+z$. Then the partial derivative of $F$ w.r.t. $x$ is $1$. (Most books don't mention that $y$ and $z$ are held constants and it's sort of implied.) But what if $z$ was a function of $x$? If $z=x$, then $F=2x+y$. In that case the answer should have been $2$. So what does the partial derivative w.r.t. $x$ mean? Do I always require a prior knowledge of the nature of the arguments $x$,$y$,$z$ in order to take partial derivatives?","['multivariable-calculus', 'partial-derivative']"
2424402,Is this an uncomputable function?,"""An unusual cubic representation problem"" by Bremner & Macleod (2014) describes an infinite set $S$ of even positive integers, such that for each $n\in S$, the equation
$$\frac{a}{b+c} + \frac{b}{a+c} + \frac{c}{a+b}=n\tag{1}$$
has positive integer solutions $(a>0,b>0,c>0).$ In particular, for each $n\in S$, there is a smallest such solution $(a,b,c)$, whose maximum element ($\max\{a,b,c\}$) has a number of digits we denote by $M(n)$. The function $M()$ is unbounded but not monotonic, with occasional very large increases; e.g. (from results in the paper),
$$M(4)=81\\
M(136)=26942\\
M(178)=398605460\\
M(198)=726373\\
M(896)\gt 2187147111901.
$$ Now, Equation (1) is shown to correspond to an elliptic curve
$$ y^2 = x^3 + (4n^2 + 12n − 3)x^2 + 32(n + 3)x\tag{2}$$
in which the coefficients are increasing functions of $n$, and [this Quora posting] asserts that ""The negative solution of Hilbert’s 10th problem means that the growth of the solutions as the coefficients get larger is an uncomputable function"", and that ""the correspondence 4→ 80-digit numbers, 178→ hundreds-of-millions-digit numbers and 896→ trillions of digits gives us a glimpse into the first few tiny steps of that monstrous, uncomputable function."" Question : Is $M()$ an uncomputable function? If so, how does this follow from the negative solution of Hilbert’s 10th problem , or how is it otherwise proved? I'm aware that certain one-parameter families of Diophantine equations (e.g. this one ) supply a negative solution to Hilbert's 10th problem, but is the present case such a family?","['number-theory', 'computability', 'elliptic-curves', 'diophantine-equations']"
2424424,Construction of exponential for an unbounded operator.,"I'm puzzling out the way one can determine $exp(iA)$ operator for an unbounded $A$. More precisely I would like to know, how could I deal with the momentum operator $p_x = -i\hbar\partial_x$. I'm trying to follow the instruction which I learned from Frederic Schullers lectures ( https://www.youtube.com/watch?v=GbqA9Xn_iM0 lecture 10 and 11) The instruction is as following: 1) Construct the positive real-valued measure $\mu_{\psi}$ using Stieltjes inversion formula : $\mu_{\psi}((-\infty,\ \lambda]) = \lim_{\delta\to 0^{+}}\lim_{\epsilon\to 0^{+}}\frac{1}{\pi} \int\limits_{-\infty}^{\lambda+\delta}dt \ Im<\psi, R_p(t + i\epsilon)\psi> $, where $R_p(z)$ is a resolvent operator. 2)Construct complex-valued measure $\mu_{\psi, \phi}$ by polarization formula: $\mu_{\psi, \phi}(\Omega) = \frac{1}{4} [\mu_{\psi+\phi}(\Omega) - \mu_{\psi-\phi}(\Omega)+i\mu_{\psi-i\phi}(\Omega)-i\mu_{\psi+i\phi}(\Omega)], \Omega$ is a Borel set in $\mathbb{R}$ 3) Construct projection-valued measure $P$ as following: $<\psi, P(\Omega)\phi> := \int \chi_\Omega d\mu_{\psi, \phi}$ 4) Calculate the integral: $exp(i\hbar\partial_x) := \int_{\mathbb{R}} e^{i\lambda}\ P(d\lambda)$ My achievments are really poor. Actually, I've just calculated the resolvent for the momentum operator:
$R_p(z) = \frac{i}{\hbar} \int\limits_{0}^{\infty}dt\ e^{izt\hbar^{-1}}u(t)$, where $u(t)\psi(x) = \psi(x-t)$.
But I got in trouble even at the first step while calculating Stieltjes integral. I've achieved such thing to calculate: $\frac{i}{h}\int\limits_{0}^{\infty}da\ e^{iza\hbar^{-1}}\ \int_{\mathbb{R}} dx\ \psi^{*}(x)u(a)\psi(x)$ and I don't know how to deal with it. Maybe someone tried this way and can give me some advices?
However, I bet it is not the shortest way to rigorously construct exponential of unbounded operator. Are they any other ideas to define exp? 
If they are, what is the main purpose of the spectral theorem and all of equastions I mentioned before. It seems to be not constructive in this case.","['functional-analysis', 'quantum-mechanics', 'measure-theory', 'unbounded-operators']"
2424432,Non-measurable set A such that $A \cap \mathbb{Q}^c$ is measurable,"Does there exist a non-(Lebesgue)measurable set $A \subset \mathbb{R}$ such that $B = \{x \in A : x \in \mathbb{R}\setminus\mathbb{Q} \}$ is measurable? My thoughts: the only non-measurable sets I've seen so far are the Vitali sets, so thinking along this line - are the irrational points in a Vitali set $V$ measurable? It would be enough to show that their complement is measurable - that is, is the set $\{x \in V : x \in \mathbb{Q}\}$ measurable? Since the set $V$ is obtained by taking the quotient group $\mathbb{R} \setminus \mathbb{Q}$, the rational points are all in a single equivalence class represented by, say, $0$, which certainly is measurable as it is a singleton. Is my thinking correct? If not, does there exist another example?","['real-analysis', 'measure-theory']"
2424444,domain and range of function $y= 1+\sum_{n=2}^\infty x^n$,"My friend give me a question to find domain and range  of
$y= 1+\sum_{n=2}^\infty x^n$ there is no more description about the problem, so  I think the domain of that function is all  $\mathbb{R} $ and range of that function is $ \mathbb{R}$ too but he told me that I was wrong, domain of that function is $-1 < x < 1 $ and range of that function is $y \leq -3  $ or $y  \geq 1  $  that was shocked me. I never knew whether domain of convergence or divergence function form will be an answer of this function. Moreover, according to WFA they said $y= 1+\sum_{n=2}^\infty x^n$ 
 equal to $y =  1 - \frac{x^2}{x-1}$ for  $\left | x \right | <1$  so domain of function should be   $ \left \{ x\in \mathbb{R} : x\neq 1 \right \}$, isn't it? Which one got the right answer and what is real definition of domain and range Sorry for my english, Thank you in advance.",['functions']
2424499,"The topology of Cantor set? How to prove $f:\{0,2\}^{\mathbb{N}}\rightarrow C$ the Cantor set, is a homeomorphism?","$P=\{0,2\}^{\mathbb{N}}$ and $C$ is Cantor sets.
Then $$f(\{a_n\})=\sum_{n=1}^\infty\frac{a_n}{3^n}$$ 
with $a_n\in\{0,2\}$ is a bijection from $P$ to $C$. My first question : If I choose the discrete topology of $\{0,2\}$ and choose the product topology in $P$, does $P$ also has the discrete topology? I think the answer is yes, because every point in $P$ as a set is an open set in $P$, then $P$ must has the discrete topology. Am I right? My second question : If I choose $C$ as subspace topology of $\mathbb{R}$, then must the topology of  $C$  not the discrete topology? I also think it's correct and $C$ is not discrete topology. Because every point in $C$ can't be written as an intersection of open interval and $C$. My third question : But I heard that $f$ is a homeomorphism of $P$ and $C$. How is it possilbe to have a homeomorphism from a discrete topology to a non-discrete topology? If this saying is right, how to prove the homeomorphism?","['general-topology', 'real-analysis', 'cantor-set']"
2424515,Showing a ring is DVR,"Here $k$ is an algebraically closed field. Consider $R=k[x,y,z]/(x^2+y^2+z^2-1)$ and $\mathfrak p=(x+iy,z-1)/(x^2+y^2+z^2-1)$ a prime ideal of $R$. I want to show that $R_{\mathfrak p}$ is a DVR. I know that a ring $A$ is DVR iff $A$ is Noetherian, local, one dimensional and normal. A ring is called a normal ring iff localisation at each of its prime ideals give integrally closed domains. Clearly $R_{\mathfrak p}$ is a Noetherian local ring. Now $\dim R_{\mathfrak p}=\operatorname{ht}\mathfrak{p}=\dim R-\dim R/{\mathfrak{p}}$. Here $R/\mathfrak{p}\cong k[x]$. Hence $\dim R_{\mathfrak p}=1$. So it is enough to show that $R_{\mathfrak p}$ is a normal ring. Clearly any integrally closed domain is a normal ring. So it is enough to show that this ring is integrally closed. I am stuck here. How do I show that $R_{\mathfrak p}$ is integrally closed? Thank you in advance for the help.","['algebraic-geometry', 'commutative-algebra']"
2424524,Cyclotomic Polynomials and GCD,"Since Cyclotomic polynomials are irreducible over $\mathbb{Q}$ , $\phi_n(x)$ , $\phi_m(x)$ are coprime as polynomials in $\mathbb{Z}[x]$ . Working over $\mathbb{Q}$ , $(\phi_n(x)$ , $\phi_m(x))=(1)$ . This implies that $(\phi_n(x), \phi_m(x))=(c)$ for some $c \in \mathbb{N}$ when this ideal is considered in $\mathbb{Z}[X]$ . Can this $c$ be evaluated as a function of $n,m$ ? What can be said about $f(x) = \gcd(\phi_n(x),\phi_m(x))$ , when the polynomials are considered as scalars, i.e. evaluated at some $x$ ? $\forall x: f(x) | c$ from question 1, but can something stronger be said? More concretely, what is the image of $f$ ? EDIT: This paper by Apostol, provided in the comments to Greg's answer, gives a pretty good answer, which Greg guessed. It basically calculates the resultant of 2 cyclotomic polynomials, which gives a number that is divisible by the optimal constant $c=(\phi_n(x), \phi_m(x))$ , and it is either 1 (when $\frac{n}{m}$ is not a prime power) or a power of a prime ( $p^{\phi(m)}$ when $\frac{n}{m}$ is a power of $p$ ). When it is $1$ (the common case), we have a full answer for both questions. When it is a prime power, we only have an upper bound for the multiplicity ( $\phi(m)$ , when $m|n$ ). This elementary paper gives some weaker result but it is simpler.","['cyclotomic-polynomials', 'polynomials']"
2424538,How to prove that $x^2 +1 \geq 2x$?,I am trying to prove that $x^2 +1 \geq 2x$ without using circular logic (meaning first assuming that this inequality is true and then moving to the $2x$ to the left side and factoring it). Thanks.,"['algebra-precalculus', 'inequality', 'proof-writing', 'quadratics']"
2424554,Prove $f(z)$ is continuous at $z_o$ iff its real and imaginary parts are continuous at $z_o$,"Was wondering if I could get some suggestions/second looks at my proof where $f(z) = f(x +iy) = u(x, y) + iv(x,y).$ I'm not sure about the right direction especially show $u, v$ are continuous. ($\implies$) Suppose $f(z)$ is continuous at $z_o = x_o + iy_o$. Then for all $\epsilon > 0, \exists \delta > 0$ so that if $|z - z_o| < \delta$ then $|f(z) - f(z_o)| < \epsilon$. Note that: $|u(x, y) - u(x_o, y_o)|, |v(x, y) - v(x_o, y_o)| \leq |f(z) - f(z_o)| \leq |u(x, y) - u(x_o, y_o)|+ |v(x, y) - v(x_o, y_o)|$ Since $f(z) - f(z_o) = u(x,y) + iv(x,y) - [u(x_o, y_o) + iv(x_o, y_o)] = [u(x,y) - u(x_o, y_o)] +i[v(x,y) - v(x_o, y_o)]$ And we know that $|Re z|, |Imz| \leq |z| \leq |Rez| + |Imz|$. Since $|(x,y) - (x_o, y_o)| \leq |z-z_o| < \delta$ then $|u(x, y) - u(x_o, y_o)| \leq |f(z) - f(z_o)| < \epsilon$ and so $u(x, y)$ is continuous at $z_o$. Similarly, since $|(x,y) - (x_o, y_o)| \leq |z-z_o| < \delta$ then $|v(x, y) - v(x_o, y_o)| \leq |f(z) - f(z_o)| < \epsilon$ and so $v(x, y)$ is continuous at $z_o$. ($\impliedby$) Suppose $u(x,y), v(x,y)$ are continuous. Then for $\epsilon_1, \epsilon_2 > 0, \exists \delta_1, \delta_2 > 0$ so that if $|(x,y) - (x_o, y_o)| < \delta_1$ then $|u(x,y) - u(x_o, y_o)| < \epsilon_1$. Likewise, if $|(x,y) - (x_o, y_o)| < \delta_2$ then $|v(x,y) - v(x_o, y_o)| < \epsilon_2$. Let $\epsilon > \epsilon_1 + \epsilon_2 > 0$ and take $\delta = \min\{\delta_1, \delta_2\}$. Then if $|z - z_o| < \delta$ this means $|(x,y) - (x_o, y_o)| < |z -
 z_o| < \delta_1, \delta_2$ so we have: $|f(z) - f(z_o)| \leq |u(x,y) - u(x_o, y_o)| + |v(x,y) - v(x_o, y_o)| < \epsilon_1 + \epsilon_2 < \epsilon \ \ \ \ \ \ \ \square$","['complex-analysis', 'proof-verification']"
2424562,Showing that the set of closed subsets of $\mathbb{R}$ generates the Borel set,"Let $\mathcal{B}(\mathbb{R})$ be the Borel $\sigma$-algebra on $\mathbb{R}$. Let $\mathcal{C}=\{\text{all closed subsets of $\mathbb{R}$}\}$ and $\mathcal{O}=\{\text{all open subsets of $\mathbb{R}$}\}$. I want to show that $\sigma(\mathcal{C})=\sigma(\mathcal{O})\;(\,=\mathcal{B}(\mathbb{R})\,)$ by showing $\sigma(\mathcal{C})\subset\mathcal{B}(\mathbb{R})$ $\mathcal{B}(\mathbb{R})\subset\sigma(\mathcal{C})$ The argument I'm trying to make is All open sets $\mathcal{O}$ are complements of closed sets. Since $\sigma(\mathcal{C})$ contains complements of closed sets, then $\mathcal{O}\subset\sigma(\mathcal{C})$. I'm stuck at trying to show that all sets in $\sigma(\mathcal{O})=\mathcal{B}(\mathbb{R})$ that are not in $\mathcal{O}$ are also in $\sigma(\mathcal{C})$.","['borel-sets', 'measure-theory']"
2424641,"Show that $\operatorname{cov}(x,a + by) = b \operatorname{cov}(x,y)$","Let $x$ and $y$ be jointly distributed numeric variables and let $z=a+by$ , where $a$ and $b$ are constants. Show that $\operatorname{cov}(x,z)=b\, \operatorname{cov}(x,y)$. Here's what I have so far, but then I got stuck. Finished.","['statistics', 'proof-writing', 'covariance']"
2424705,Intuition of dual space action in dual of $\ell_p$,"For $p>1,$ the sequence space $\ell_p$ is defined as $$\ell_p=\left\{ x=(x_i)_{i\in\mathbb{N}} : \sum_{i=1}^\infty |x_i|^p < \infty \right\}.$$ A classical duality theorem of $\ell_p$ space is that $\ell_p^* \cong \ell_q$ where $1/p + 1/q = 1$ and $\cong$ means isometrically isomorphic and $\ell_p^*$ means dual space of $\ell_p.$ One way to show the isometrically isomorphism is the following: For any $\eta=(\eta_i)_{i\in\mathbb{N}} \in \ell_q,$ define a map $\phi_{\eta}:\ell_q\to\mathbb{R}$ given by the dual space action, that is, $$\phi_\eta(\xi) = \sum_{i=1}^\infty \eta_i \, \xi_i$$ where $\xi=(\xi_i)_{i\in\mathbb{N}} \in \ell_q.$ Then one proceeds to show that $\|\eta\|_q = \| \phi_\eta \|$ to obtain into isometry from $\ell_q$ into $\ell_p*.$ Question : What is the intuition behind the dual space action? When I try to show that $\ell_q \cong \ell_p^*$ myself, I could not think of the action. So I am wondering what triggers one to think of the dual space action.","['functional-analysis', 'real-analysis', 'lp-spaces', 'dual-spaces']"
2424737,A smooth path passing through infinitely many points of a given sequence,"While doing my research I came across an interesting question. 
Let $\{\bf{x}_n \}\subset R^m$ be any sequence of points with $\bf{x}_n\rightarrow \bf{0}$, as $n \rightarrow \infty$. Is there a smooth path $c: [0,a) \rightarrow \mathbf{R}^m$, $a>0$, $c(0)=\bf{0}$, a (decreasing) sequence $\{t_k\}\subset (0,a)$, an (infinite) subsequence $\{\bf{x}_{n_k} \}\subset \{\bf{x}_n \}$ so that $c(t_k)=\bf{x}_{n_k}$? Can we make the path analytic at zero? Any help, or suggestion would be appreciated.",['analysis']
2424745,Integrate $\int\dfrac{1}{e^{2/y}}dy$,$$\int\dfrac{1}{e^\frac{2}{y}}dy$$ I'm trying to integrate $\frac{dy}{dt}=e^{2/y}$. I've separated the equation to $\frac{1}{e^{2/y}}dy=dt$ so that I can integrate each side with respect to the corresponding variables. Computing the integral of $dt$ is trivial but I'm confused on how to compute the integral of $\frac{1}{e^{2/y}}dy$.,"['derivatives', 'indefinite-integrals', 'integration', 'ordinary-differential-equations']"
2424808,Laplace Transform is difficult. What to do?,"I am trying to solve a differential equation that has cumulative normal distributions on the non-homogenous part: $$ay'' + by' + cy = \gamma e^{mx} \operatorname{erf}(\alpha x + \beta)$$ There are many more terms on the RHS, all of which are functions of erf. The Laplace Transform is easy, but the inverse is not. Matlab is not giving me an answer. Is there something I am missing to study? What can help me deal with these type of ""hard"" laplace/inverse laplace problems?","['ordinary-differential-equations', 'laplace-transform']"
2424827,Finding a substitution that eliminates the squared term from a cubic equation,"So I am a little stumped here, and it could be simple. I'm just not exactly sure how to approach this.
The question reads: Find $\omega_0$ in the set of Complex numbers, such that the substitution $z = \omega - \omega_0$ reduces the cubic equation $z^3 + Az^2 + Bz +C = 0$ into $\omega^3 -m\omega -n =0$ ... where I'm assuming those constants are real numbers. My first attempt was to just do a straight substitution of $z= \omega - \omega_0$ into the first equation, then expand it, and try to solve that down for what $\omega_0$ should be, but I started to realize that that's probably not the way.
Am I just missing something here?","['complex-analysis', 'polynomials', 'complex-numbers']"
2424908,Convergence of a recursively defined sequence,"Let a sequence $(a_n)_{n=0}^\infty$ be defined recursively $a_{n+1} = (1-a_n)^{\frac1p}$, where $p>1$, $0<a_0<(1-a_0)^{\frac1p}$. Let $a$ be the unique real root of $a=(1-a)^{\frac1p}$, $0<a<1$. It is clear $0<a_0<(1-a_0)^{\frac1p}\Leftrightarrow 0<a_0<a$. Prove 1) $a_{2k-2}<a_{2k}<a<a_{2k+1}<a_{2k-1}$ and $a_{2k+1}-a<a-a_{2k}$. 2) $\lim\limits_{n\to\infty}a_n=a$. Define $f(x):=(1-x)^{\frac1p}$. Consider $f^2$. When $p=2$, $a_{n+2}=f^2(a_n)=\big(1-(1-a_n)^{\frac12}\big)^{\frac12}$. $a_{n+2}>a_n\Leftrightarrow (1-a_n)(1+a_n)^2>1\Leftrightarrow a_n<(1-a_n)^{\frac12}$, and the conclusion is proved. But I am having difficulty generalizing this method to arbitrary $p>1$. I also suspect that there is a general method to solve this kind of problem.","['real-analysis', 'inequality', 'limits', 'fixed-point-theorems', 'sequences-and-series']"
2424924,A question about continuous curves in $\mathbb{R}^2$,"Let $f:[a,b]\longrightarrow\mathbb{R}^2$ be a continuous function such that 
$$f(a)=(0,0),\ f(b)=(0,1).$$
Is it true that there must exist $t_1,t_2\in [a,b]$ such that
$\displaystyle f(t_1)-f(t_2)=(0,\frac{1}{2})?$ If not, please give a counterexample.","['real-analysis', 'analysis']"
2424967,Weak topology = topology of pointwise convergence?,"In a book, I found the following theorem (slightly simplified here): Consider $\mathbb{R}$ with the Borel $\sigma$-field. Let $B$ be the set of all measurable, bounded, real-valued functions on $\mathbb{R}$. Let $M$ be the set of all finite, signed measures on $\mathbb{R}$. Let the weak topology on $B$ be defined to be the coarsest topology such that all functions $B \to \mathbb{R},\quad f \mapsto \int f \mathrm{d} \mu,\quad \mu \in M$, are continuous. Theorem: On any $\| \cdot \|_\infty$-bounded subset of $B$, the weak topology and the topology of pointwise convergence coincide. Proof. Since $M$ contains all one-point probability measures, convergence in the weak topology implies pointwise convergence. Conversely, since sets bounded in $\| \cdot \|_\infty$-norm are also pointwise bounded, we can deduce from Lebesgue's dominated convergence theorem that pointwise convergence implies convergence in the weak topology. $\square$ Now, I do not understand why having the same convergent sequences in this case should imply that the topologies are the same. One direction seems clear: The topology of pointwise convergence is coarser than the weak topology, since the topology of pointwise convergence is the initial topology w.r.t. the family of all functionals $f \mapsto \int f \mathrm{d} \delta_x$. Yet I have not been able to get rid of the sequences argument in the other direction, and, thinking about how open sets in $B$ look like in the two topologies, I am not even sure anymore that the theorem is correct. Questions: (a) Is the theorem correct?
(b) Is the proof correct, and, if yes, why?","['functional-analysis', 'pointwise-convergence', 'weak-convergence', 'measure-theory']"
2424974,symbol for the set of integers from 1 to N [duplicate],"This question already has an answer here : About Math notation: the set of the first $n$ natural numbers (1 answer) Closed 6 years ago . Is there a special symbol for the set: $$
\{1, 2, 3, \dots, n\}$$,
 or $$\{x| 1\leq x\leq n, n \in \mathbb{Z} \} $$?",['elementary-set-theory']
2424990,Characterizing BV functions via the Stieltjes integral,"Let $J=[a,b]$ be a compact interval and $f,g:J\rightarrow\mathbb{R}$ be two bounded functions. We say that $\int f dg$ exists (and equals $A$) if and only if the following condition holds: for any $\epsilon>0$, there exist a partition $P_\epsilon$ of $J$ such that for any refinement $P\supset P_\epsilon$, any Riemann-Stieltjes sum with repect to $(f,P,g)$ lies in the $\epsilon$-neighborhood of $A$. In Hildebrandt, T. H. (1938), ""Definitions of Stieltjes Integrals of the Riemann Type"", The American Mathematical Monthly, 45 (5): 265–278, it is stated without proof that If $\int f dg$ exists for every $f(x)$ continuous on $a\leq x\leq b$, then $g(x)$ must be of bounded variation. How do we prove such a statement? Should we use piece-wise linear functions alternating between 0 and 1 resembling step functions?? I don't see how I should proceed.. any advice would be welcome.","['riemann-integration', 'functional-analysis', 'integration', 'lebesgue-integral', 'bounded-variation']"
2425013,Is it true that $RP^3$ minus a point deformation retracts to a space homeomorphic to $RP^2$,"I have the following question: Is it true that $RP^3$ minus a point deformation retracts to a space
  homeomorphic to $RP^2$ My Efforts Answer is NO Assume on the contrary that $RP^3$ with a point deleted deformation retracts onto a space homeomorphic to $RP^2$. It implies that fundamental group of $RP^3$ minus a point is $\mathbb{Z}/2\mathbb{Z}$ Deleting a point from $RP^3$ is same as removing $2$ points from $S^3$. By stereographic projection $S^3$ minus a point is homeomorphic to $\mathbb{R}^3$. As $R^3$ minus a point is simply connected, it's fundamental group  is trivial. So we arrive at a contradiction.","['algebraic-topology', 'general-topology', 'homotopy-theory', 'proof-verification']"
2425019,A guess related to Lebesgue differentiation theorem,"When I read Lebesgue differentiation theorem, I suddenly have the following conjecture, which I can't prove or find a counterexample. Let $f\in L_{\mathrm{loc}}^1(\mathbb{R}^n)$. If 
  $$
\int_{B_r(x)} f(y)dy=0
$$
  holds for any $r\geq 1$ and $x\in \mathbb{R}^n$ , then can we say that $f(x)=0$ a.e. ? Please be careful that $r$ is larger than 1 , which prevents us from taking advantage of  Lebesgue differentiation theorem. When $n=1$, this seems to be true.","['harmonic-analysis', 'real-analysis', 'measure-theory']"
2425027,Are there infinitely many even integers $n$ such that $1+4\varphi(n)$ is a perfect square?,"Let $\varphi(n)$ the Euler's totient function. The sequence of even integers $n$ such that $$1+4\varphi(n)$$ is a perfect square starts as $$4, 6, 14, 18, 26, 28, 36, 42, 44, 50\ldots$$ Question. I've curiosity about this question: are there infinitely many even integers $n$ such that $1+4\varphi(n)$ is a perfect square? Many thanks.","['sequences-and-series', 'totient-function', 'elementary-number-theory']"
2425054,Coarea formula and its application,"I have a question about coarea formula. Last day, I found the following assertion in a paper: ""Since $f \in C(\bar{D})$ and $D$ is a bounded $C^1$ -domain, \begin{align*}
(1)\quad
\lim_{\varepsilon \to 0}\frac{1}{\varepsilon}\int_{D_{\varepsilon}}f\,dx=\int_{\partial D}f\,d\sigma,
\end{align*} where $D_{\varepsilon}=\{x \in \bar{D}:d(x,\partial D) \le \varepsilon\}$ and $\sigma$ is the surface measure on $\partial D$ ( $(d-1)$ -dim Hausdorff measure on $\partial D$ )."" Since the author state this assertion without proof, I want to know how to prove and generalize this assertion. My attempt Let $D$ be a bounded domain on $\mathbb{R}^d$ . That is, $D$ is a connected bounded open subset of $\mathbb{R}^d$ . We assume $m(\partial D)=0$ ( $m$ is the $d$ -dim Lebesgue measure). Define $F(x)=\inf_{y \in \partial D}|x-y|$ .
Note that $F$ is a Lipschitz continuous function on $\mathbb{R}^d$ and $|\nabla F|=1$ $m$ -a.e. Take $f \in L^{1}(\bar{D},m)$ and extend it on $\mathbb{R}^d$ by putting $f=0$ on $\mathbb{R^d}\setminus \bar{D}$ . Using coarea formula, we have $\int_{\mathbb{R}} \int_{\{F=s\}}|f|\,d\sigma\,ds<\infty $ and \begin{align*}
\int_{\{F>t\}}f\,dx=\int_{t}^{\infty}\left(\int_{\{F=s\}}f\,d\sigma \right)\,ds,\quad t \in \mathbb{R}.
\end{align*} Therefore, we have \begin{align*}
\frac{1}{\varepsilon}\int_{\{0<F \le \varepsilon\}}f\,dx=\frac{1}{\varepsilon} \int_{0}^{\varepsilon} \left(\int_{\{F=s\}}f\,d\sigma \right)\,ds.
\end{align*} Since $f=0$ on $\mathbb{R}^d \setminus \bar{D}$ , \begin{align*}
\frac{1}{\varepsilon}\int_{D_{\varepsilon}}f\,dx=\frac{1}{\varepsilon} \int_{0}^{\varepsilon} \left(\int_{\{F=s\}}f\,d\sigma \right)\,ds.
\end{align*} However, is $s \mapsto \int_{\{F=s\}}f\,d\sigma$ continuous? My question When is $s \mapsto \int_{\{F=s\}}f\,d\sigma$ continuous? Even if this is not true, under what conditions do we still have $$
\frac{1}{\varepsilon} \int_{0}^{\varepsilon} \left(\int_{\{F=s\}}f\,d\sigma \right)\,ds \to \int_{\{F=0\}}f\,d\sigma \, ?
$$ Even if $D$ is Lipschitz domain and $f$ is continuous, (1) must be valid. Thanks for any information. Information for (1) Recently, I found an extension of (1) in this paper: enter link
  description here . If you are interested, please look at Lemma 7.1
  in this paper.","['geometric-measure-theory', 'measure-theory']"
2425055,Spaces with finite bases are compact,"Let $X$ be a topological space with a finite basis $B$, and let $S$ be an open cover of $X$. Then every $U\in S$ is a finite union of members of $B$. Let $S'\subseteq S$ be a subcover such that members of $S'$ are disjoint. Then $S'$ is finite. Hence X is compact. How can I show that $S'$ exists? And why is $S'$ finite? I guess it'd also work if I can show that $S\subseteq B$.",['general-topology']
2425070,Is there a way to deal with this singularity in numerical integration?,"I would like to compute numerically, e.g., using the standard method of trapezes the following definite integral over the interface $[0,1]$ : $$
I = \int_0^1 \frac{f(x)}{\sqrt{1-x^2}} \, \mathrm{d} x \, , 
$$ where $f(x)$ is continuous in the interval [0,1]. 
It can readily be shown analytically that the integral is well convergent. 
However, when proceeding numerically, difficulties arise since the integrand diverges at the upper limit of integration for $x=1$ . I was wondering whether there exists a procedure that can help to remove the singularity in this integral.
Any help is highly appreciated Thank you, hartmut","['improper-integrals', 'numerical-methods', 'integration', 'definite-integrals']"
2425077,"If $\cos(z-x) + \cos(y-z) + \cos(x-y) = -\frac{3}{2}$, then $\sin x + \sin y + \sin z = 0 = \cos x + \cos y + \cos z $.","If $$\cos(z-x) + \cos(y-z) + \cos(x-y) = -\frac{3}{2}$$ then how can I show that the sum of cosines of each angle ($x$, $y$, $z$) and sines of each angle sum up to zero? i.e. $$\sin x + \sin y + \sin z = 0 = \cos x + \cos y + \cos z $$ I tried: • Expanded using $\cos(A-B) = \cos A\cos B+\sin A\sin B $, but it did nothing. After spending one hour to this problem, I thought that there must be a shorter and ideal way.",['trigonometry']
2425083,How many numbers smaller than one milion?,"How many numbers smaller than 1 000 000 : a) have digits in a non-decreasing order? b) contains exactly three digits 9 and have an odd sum of numbers? c) have digits in non-increasing order? For the a. solution Is it like: 
$\binom{6+10-1}{10-1}$
Am I right? Solution to the b. So the first solution is exactly 999, for the 4-digits number $\binom{4}{1}$$\binom{4}{1}$ +1 , for the 5-digits number $\binom{5}{1}$$\binom{4}{1}$$\binom{4}{1}$$\binom{4}{1}$","['combinatorics', 'discrete-mathematics', 'elementary-number-theory']"
2425167,Fixing a wobbly table - Revisited,"There is this video on the Youtube channel Numberphile in which it is argued that each wobbly four legged square table can be made stable by just minor adjustments in position, namely an at most $90^°$ turn around its center. Too long to read : I doubt the completeness of the presented solution. Hence I am not sure if every table can be stabelized in the demonstrated way. Of course Numberphile is a popular math channel and cannot go into much detail, but I think there might be much more than just some simple gap filling and formalization work to complete the proof. Question : Is the problem indeed harder than demonstrated in the video? Or is 
the solution complete and I can just not see how? How to fix it otherwise? There were some assumptions made and I will start here with formalizing the problem. At first, I assume that the ground on which the table is placed is given as a continuous function $f:\Bbb R^2\to\Bbb R$. For some point $(x,y)=p\in\Bbb R^2$ I will write $\bar p:=(x,y,f(p))\in\Bbb R^3$ for its lifting onto the ground in three dimensions. Now the problem of finding a stable position for the table can be given as follows: Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$. There are four points $(x_i,y_i)=p_i\in\Bbb R^2,i=1,2,3,4$ so that the points $\bar p_i$ form a unit-square. However, the demonstrated solution of the problem in the video aims for a stronger statement. For this, call a set $\{p_i\}$ of points $p_i\in\Bbb R^2,i=1,2,3$ squarable if there is a point $\diamond(p_i)\in\Bbb R^3$ so that $$\{\bar p_1,\bar p_2,\bar p_3,\diamond(p_i)\}$$ form the corners of a unit-square. So the video states Conjecture . Given a continuous function $f:\Bbb R^2\to \Bbb R$ and a squarable set $\{p_i^0\}$ of three points $p_i^0\in\Bbb R^2,i=1,2,3$. Then there are continuous functions $p_i:[0,1]\to\Bbb R^2,i=1,2,3$ so that it holds $p_i(0)=p_i^0$, the set $\{p_i(t)\}$ is squarable for all $t\in[0,1]$, there is a point $p_4\in\Bbb R^2$ with $\bar p_4=\diamond(p_i(1))$. Stated informally this means that a table in a possibly wobbly initial position (see 1.) can be moved continuously (see 2.) into a stable position (see 3.). Now the video explains how to choose the paths $p_i(t)$ in a clever way, so that 3. is satisfied automatically by the intermediate value theorem. In detail, they choose path $p_i(t),i=1,2,3$ to start in $p_i^0$ and aim for $p_{i+1}^0$ (assume $p_4^0$ is the projection of $\diamond(p_i^0)$ into the plane). They call this a rotation of the table around its center. Then they argue that condition 3. is satisfied for some time $t\in[0,1]$. I do not have any objections against this latter argument. I just realized that they completely skipped to argue that such paths even exist! In the end, condition 2. is a strong restriction and the existence is not obvious. The rest of this post contains some arguments with which I convinced myself that the problem might be non-trivial. At first I went one diemnsion lower. I tried to move a two dimensional table along a function-graph by keeping the legs on the ground. I asked: is it always possible to move the table from the left to the right without loosing touch with the ground? Lets formalize this. Again the ground is a continuous function $f:\Bbb R\to\Bbb R$. A point $p\in\Bbb R$ is lifted to $\bar p=(p,f(p))\in\Bbb R^2$. Instead of a unit-square, here we have to keep the touching point of the legs with the ground in the form of a unit interval, i.e. at distance one. Question : Given a continuous function $f:\Bbb R\to\Bbb R$ and $x_0,x_1,y_0\in\Bbb R$ with $\|\bar x_0-\bar y_0\|=1$. Are there continuous functions $x,y:[0,1]\to\Bbb R$ so that it holds $x(0)=x_0$, $y(0)=y_0$ and $x(1)=x_1$, it holds $\|\bar x(t)-\bar y(t)\|=1$ for all $t\in[0,1]$. Looking at the above sketch, this might seem trivial. However, when we choose functions like $f(x)=100 \sin(x^2)$ and want to move the table from $x_0=0$ to $x_1=100$, it is not clear that this can be done. Try it! I will now demonstrate you my approach to solve this question and the problems I encountered on the way. Define the function $d(x,y):=\|\bar x- \bar y\|$. Note that $d$ is a function $\Bbb R^2\to\Bbb R$, hence describes a height map over a two-dimensional area. The $x(t)$ and $y(t)$ in the above formulation are the positions of the table legs at each point in time. Condition 2. is then equivalent to $d(x(t),y(t))=1$. Hence we are interested in paths of constant ""height 1"" in the height map given by $d$. For example, the following figure shows on the left a plot of $f(x)=2\sin(x^2)$, and on the right the associated hightmap. The black points are all the points with $d(x,y)=1$. It looks pretty complicated. But the good thing is that there is indeed a path (the black curve) connecting the bottom of the plot with the top. The $x$- and $y$-coordinates of this path are the desired $x(t)$ and $y(t)$ functions of how to move the tables legs. The hard part is to show that such a path exists always. I tried to show this using some generalizations of the intermediate value theorem but there are sitations where this approach fails. And I was not able to proof that such situations cannot occure for this problem. The second observations are the closed loops in the right figure. They indicate positions in which the table can be placed, but from which he cannot escape. This was strong evidence for me that the problem is highly non-trivial. How do I know that such locked situations do not occure in the case of a square table? Why should I be able to rotate the table while keeping the legs on the ground? What if I am in such a separate connected component as in the above simplified problem?","['general-topology', 'real-analysis', 'puzzle', 'proof-explanation']"
2425176,Proof-verification: $R$ is transitive iff $R \circ R ⊆ R$.,"This is Velleman's exercise 4.3.8 ( And not a duplicate of "" $R$ is transitive if and only if $ R \circ R \subseteq R$ ""): Here's my proof of it: Proof. ($\rightarrow$) Suppose $R$ in transitive. Let $(x, z)$ be an arbitrary element of $R \circ R$. By the definition of composition there must be some $y$ such that $(x, y) ∈ R$ and $(y, z) ∈ R$. Since $R$ was transitive then from $(x, y) ∈ R$ and $(y, z) ∈ R$, we get $(x, z) ∈ R$. Since $(x, z)$ was arbitrary then we can conclude that $R \circ R ⊆ R$. ($\leftarrow$) Suppose $R \circ R ⊆ R$. Let $x$, $y$, and $z$ be arbitrary elements of the set $A$ such that $(x, y) ∈ R$ and $(y, z) ∈ R$. Now by the definition of composition we have $(x, z) ∈ R \circ R$. Hence from our hypothesis we get $(x, z) ∈ R$. Since $x$, $y$, and $z$ were arbitrary then $∀x ∈ A∀y ∈ A∀z ∈ A((x Ry ∧ yRz) → x Rz)$. Ergo $R$ is transitive. From ($\rightarrow$) and ($\leftarrow$), $R$ is transitive if and only if $R \circ R ⊆ R$. Is my proof correct? I'm particularly concerned with the part that I introduced the set ""A"" in the backward direction. Thanks.","['relations', 'function-and-relation-composition', 'elementary-set-theory', 'proof-verification']"
2425184,Solve the equation $18x^2-18x \sqrt{x}-17x-8 \sqrt{x}-2=0$,"Solve the following equation. $$18x^2-18x \sqrt{x}-17x-8 \sqrt{x}-2=0.$$ Taking $\sqrt{x}=t$ we get equivalent equation $18t^4 -18t^3 - 17t^2-8t-2=0$. From this point I have tried to factor it , write RHS as sum of two squares and its variants but nothing seem to work. Then putting the original equation in wolfram alpha I got solution $x=\frac{2}{9}(7+2 \sqrt{10})$. Can anyone suggest a method to solve it without wolfram alpha or any such computer method. Thanks in advance.","['algebra-precalculus', 'contest-math', 'polynomials', 'quadratics']"
2425224,"The space of specific $(1,1)$-tensors","Let $(M,g)$ be a Riemannian manifold and $X$ a non-vanishing parallel vector field, i.e. $\nabla_XX=0$ . Let $${\cal A}_X:=\{T\in\ ^1_1\!\otimes(TM)\,|\ T(X)=0\}.$$ It is easy to see that ${\cal A}_X$ is a vector space. Also if $T\in {\cal A}_X$ then $\mathscr{L}_X T \in  {\cal A}_X$ and $\nabla_X T \in  {\cal A}_X$ where $\mathscr{L}_X$ is Lie derivative along $X$ . Suppose $Q$ denote the Ricci operator defined by $g(QY,Z)=Ric(Y,Z)$ . If $QX=\lambda X$ for some smooth function $\lambda$ , I think $Q$ can be written as follow: $$Q=aI+b\omega\otimes X+\sum_ic_iT_i,\quad T_i\in {\cal A}_X,\,a,b,c_i\in C^\infty(M,\Bbb R),\, \omega=g(X,.)$$ Is this conclusion  correct? Can anyone give a simple proof? What we can say about $\dim({\cal A}_X)?$ Thanks in advance","['tensors', 'riemannian-geometry', 'differential-geometry']"
2425225,What does this notation mean? (See the picture.),It's a problem from a past-paper. So what is this notation tell me to do? How should I start solving this? Please tell me where should I start? I'm confused with the notation that $J$ points,"['derivatives', 'partial-derivative', 'notation', 'calculus']"
2425230,Why is it called hyper-plane? [duplicate],"This question already has an answer here : Why is a Hyperplane called a ""Hyper""plane? (1 answer) Closed 6 years ago . ""In geometry a hyperplane is a subspace of one dimension less than its ambient space."" However, the Greek prefix hyper- means ""'over', usually implying excess or exaggeration"" . So why do we call a hyperplane a hyperplane, while it actually has less dimensions than the original space? Shouldn't it be called hypoplane rather?","['terminology', 'convention', 'geometry']"
2425234,"Why, if principal ideals are different, they are not 2-sided?","There is an argument about ring ideals which I do not understand. Let $R$ be a ring with $1$,
  $$a=\begin{bmatrix}1&0\\0&0\end{bmatrix},$$ $I=\{a\}\cdot M_2(R)$,
  $J=M_2(R)\cdot\{a\}$. $I$ is a right ideal and $J$ is a left ideal of
  $M_2(R)$. Observe that, if $R$ is not trivial, $I\not=J$ and hence
  none of them is 2-sided. I do not understand this “hence”. How is $I\not=J$ relevant? I can prove that $J$ is not a right ideal in a concrete way: the right column of every member of $J$ is zero and $$\begin{bmatrix}1&0\\0&0\end{bmatrix}\cdot\begin{bmatrix}0&1\\0&0\end{bmatrix}
= \begin{bmatrix}0&1\\0&0\end{bmatrix}.$$ I guess I need to prove something like “if $J=S\cdot\{a\} $ is a right ideal, then $J=\{a\}\cdot S $” for an abstract ring $S$ with $1$. I proved the following. $1\in S$, $1\cdot a\in J $, $a\in J $. [Let $s\in S$ be given. $a\cdot s\in J $ because $J$ is a right ideal.] Hence $\{a\}\cdot S\subseteq J $. What next? [Update 2017-09-12 19:48:28+03:00. It seems that this is a mistake in the textbook. I publish the parameters of the textbook so the authors can fix it. Menini, Claudia, and Freddy Van Oystaeyen. Abstract Algebra: A Comprehensive Treatment. Marcel Dekker, 2004.]","['matrices', 'abstract-algebra', 'ring-theory', 'ideals']"
2425304,Identity for Hodge star on product manifold,"Let $X$ and $Y$ be Riemannian manifolds, and let $\omega \in \Omega^i(X)$ and $\eta \in \Omega^j(Y)$ be differential forms on $X$ and $Y$. I expect it is the case that
$$
*_{X\times Y}(\omega \wedge \eta) = (*_X \omega) \wedge (*_Y \eta)\,,
$$
where '$*$' denotes the Hodge star operation, and where in the expression $\omega \wedge \eta \in \Omega^{i+j}(X\times Y)$ we understand $\omega$ and $\eta$ to be pulled-back from $X$ and $Y$ under projection maps. The proof is likely quite straightforward, but I found it a bit fiddly, and so I am looking for a reference for this fact, or alternatively a nice proof.","['differential-forms', 'riemannian-geometry', 'differential-geometry']"
2425374,Example of a function in Riemannian manifold,"Is there any example of a function $f:M\rightarrow\mathbb{R}$ such that the following condition holds $$\nabla^2f_p(X,Y)\geq Ric_p(X,Y)f(p)\ \forall p\in M \text{ and } X,Y\in T_pM$$ where $\nabla^2$ is the Hessian operator and $Ric_p$ is the Ricci tensor at $p$. I know that every convex function in Euclidean space satisfies the condition but I can not find any other example in other Riemannian manifold. Please help.","['tensors', 'examples-counterexamples', 'riemannian-geometry', 'differential-geometry']"
2425383,How to get rid of the inductive step while proving $n!+2$ is divisible by $2$ for all integers $n \ge 2$ by induction?,"In the inductive step we assume that $k!+2$ is divisible by $2$. So
  $k!+2=2r$ for some $r \in \Bbb Z$. To show that $(k+1)!+2$ is divisible by $2$. Consider $(k+1)!+2=(k+1)k!+2=k\cdot k!+ k!+2=k\cdot k!+2r.$ From here I can only think that since $k \ge 2$, the number $k!$ is even. Thus $k!=2s$ for some $r \in \Bbb Z$. But I feel that by this logic, we don't need Principle of Mathematical Induction at all to solve this problem. Just apply this logic to the given statement. So should I just write $k\cdot k!+2r=k\cdot k\cdot (k-1)\cdot \cdot \cdot 2\cdot 1+2r=2(k\cdot (k-1) \cdot \cdot \cdot 1 + r)$ and finish it off? are there any alternate ways?","['induction', 'alternative-proof', 'discrete-mathematics']"
2425427,Explicit Gram Schmidt iteration,"Let $v_1,...,v_n$ be a set of linearly independent vectors in a real Hilbert space. Is there a way to write its Gram-Schmidt orthonormalization $w_1,...,w_n$ explicitly only in terms of $v_1,...,v_n$ and inner-products $\langle v_i,v_j\rangle$ in such a way that we do not have to expand anything and can compute $w_1,...,w_n$ directly. It is clear that this is possible, but most expressions for Gram-Schmidt involve the norm or recursive parts. Clearly, we start with $$w_1:=\frac{v_1}{\sqrt{\langle v_1,v_1\rangle}}$$
$$w_2:=\frac{v_2-\langle v_2,v_1\rangle v_1}{\sqrt{\langle v_2,v_2\rangle+\langle v_1,v_1\rangle-2\langle v_1,v_2\rangle}}$$ But is there also a general formula for $w_n$?","['real-analysis', 'functional-analysis', 'orthogonality', 'linear-algebra', 'vector-spaces']"
2425472,"If $x\sim\mathcal{N}(\mu, \sigma^2)$, what is $\mathbb{E}\left(\frac{\exp(x)}{a+\exp(x)}\right)$ for $a>0$?","Let $x\sim\mathcal{N}(\mu, \sigma^2)$ be a normal random variable. I'm interested in a new random variable, defined as 
$$
z = \frac{\exp(x)}{a+\exp(x)},\quad x\in\mathbb{R}, a>0.
$$
More specifically, I would like to find the closed form (if any) of its expected value, $\mathbb{E}[z]$, but I don't know how to proceed. As a first step I could use an intermediate random variable $y=\exp(x)$, which follows the log-normal distribution (since $x$ is normal) and its mean and variance are given w.r.t. to $x$'s mean and variance. But again, I'm not sure if that helps.","['real-analysis', 'normal-distribution', 'expected-value', 'probability', 'random-variables']"
2425503,Stopping Rules for Jacobi/Gauss-Seidel Iteration,"Suppose that $A\mathbf{x}=\mathbf{b}$ is a diagonally dominant linear system. We can use iterative methods that produce a sequence of approximations $\mathbf{x}^1,\mathbf{x}^2,\dots$ that converge to $\mathbf{x}$. Doing this for a small system it seems intuitive to stop when the components $x^i_j$ and $x^{i+1}_j$ differ by a tolerance $\varepsilon$ giving solutions correct to $\varepsilon$. While playing around with larger systems it was clear that this could stop well before convergence and is a very poor stopping rule (and doesn't give accurate answers as claimed). Researching a little further I see some slightly more sophisticated stopping rules (in the below the norms might be max norms or 2-norms): When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon.$ When $\|\mathbf{x}^{i+1}-\mathbf{x}^i\|< \varepsilon\|\mathbf{b}\|.$ When $\displaystyle \max_j\left|\frac{x_j^{i+1}-x_j^i}{x_j^{i+1}}\right|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon$. When the residual $\|\mathbf{r}\|=\|\mathbf{b}-A\mathbf{x}^i\|<\varepsilon\|\mathbf{b}\|$. I am interested in hearing the pros and cons of each. I am hoping to use one that is relatively easy to implement on VBA with perhaps 20 unknowns.","['numerical-linear-algebra', 'normed-spaces', 'algorithms', 'numerical-methods', 'linear-algebra']"
2425507,Convergence almost sure of the minimum of a succession of random variables,"I have a problem with this exercise. Let $(X_n)$ be an IID succession such that $X_n$ ~ $\mathcal{N}(0,1)$ and
set $Y_n := n\min(|X_1|, \dots, |X_n|)$.
Can anyone explain me intuitively why $Y_n$ does not converge almost surely? And what can be said about $\liminf_{n \to \infty} Y_n$? I have already shown that $Y_n$ converges to $\text{Exp}(\sqrt{\frac{2}{\pi}})$ in distribution. Thanks for your help.","['probability-theory', 'convergence-divergence', 'probability-distributions']"
2425524,Algebra of additive polynomials,"Let $\mathbb{F}_p$ be a finite field for an odd prime $p$ . Consider the ring $$\mathcal{L}(\mathbb{F}_p(X),Y)$$ of additive (or linearized) polynomials in $Y$ over the rational function field $\mathbb{F}_p(X)$ . That is, we are interested in polynomials of the form $$a_n(X)Y^{p^n} + a_{n-1}(X)Y^{p^{n-1}} + \dots + a_1(X)Y^{p} + a_0(X)Y$$ with $a_j(X) \in \mathbb{F}_p(X)$ for every $0 \leq j \leq n$ . Addition is the usual, while multiplication is now function composition, making $\mathcal{L}(\mathbb{F}_p(X),Y)$ a non-commutative ring with unity. My interest is regarding this non-commutativity of the multiplication in $\mathcal{L}(\mathbb{F}_p(X),Y)$ and its structure as a ring. What can we say about the structure of the multiplicative monoid of $\mathcal{L}(\mathbb{F}_p(X),Y)$ ? What is the center of $\mathcal{L}(\mathbb{F}_p(X),Y)$ ? It is clear that the center contains $\mathbb{F}_p Y$ , but is that the whole center? The same questions can be asked of the ring $\mathcal{L}(\mathbb{F}_p[X],Y)$ of linearized polynomials with coefficients now from the domain $\mathbb{F}_p[X]$ instead of the rational function field. What can we say about the structure of the multiplicative monoid of $\mathcal{L}(\mathbb{F}_p[X],Y)$ ? What is the center of $\mathcal{L}(\mathbb{F}_p[X],Y)$ ? Again, it is clear that the center contains $\mathbb{F}_p Y$ , but is that the whole center? This is only the surface. Even more interesting is how this multiplication behaves with regard to absolute irreducibility of the polynomials, its roots (in an extension of $\mathbb{F}_p(X)$ ), its splitting fields, etc. But I'll get to those later. Apologies if the question is broad, and will refine it as I get more familiar with the object . I am interested in all aspects of the structure of the non-commutative multiplication here, so any comprehensive references would be valuable. The ones I googled seem way too sophisticated and abstract (Drinfeld modules and the like) that I don't know where to even begin. Please feel free to share any interesting facts about the non-commutative ring $\mathcal{L}(\mathbb{F}_p[X],Y)$ (and its multiplicative monoid) that you know of. Thanks.","['finite-fields', 'polynomials', 'abstract-algebra', 'noncommutative-algebra', 'ring-theory']"
2425542,Apparent inconsistency of Lebesgue measure,"Studying the Lebesgue measure on the line I've found the following argument which concludes that $m(\mathbb{R}) < +\infty$ (where $m$ denotes the Lebesgue measure on $\mathbb{R}$ ). Obviously it must be flawed, but I haven't been able to find the flaw so far. Recall that for a Lebesgue measurable set $A$ we have by definition that $$
m(A)=\inf\left\{\sum_{n=1}^\infty(b_n-a_n):\cup_{n=1}^\infty(a_n,b_n]\supset A\right\}.
$$ Pick your favorite summable sequence of positive terms, $\{a_n=1/n^2\}_{n=1}^\infty$ for instance. We know the rational numbers are countable, so we can index them in a sequence $\{q_n\}_{n=1}^\infty$ . Now consider the intervals $$
I_n=\left(q_n-\frac{a_n}{2},q_n+\frac{a_n}{2}\right].
$$ As the rationals are dense in $\mathbb{R}$ we must have $\mathbb{R}\subset\cup_{n=1}^\infty I_n$ but then, having into account the definition of Lebesgue measure we have $$
m(\mathbb{R})\leq\sum_{n=1}^\infty\left(\big(q_n+\frac{a_n}{2}\big)-\big(q_n-\frac{a_n}{2}\big)\right)=\sum_{n=1}^\infty a_n<+\infty.
$$ For instance with $a_n=1/n^2$ we get $m(\mathbb{R})\leq\pi^2/6$ . As I said, I know this is flawed but I've spent almost two hours trying to find the flaw so any help would be appreciated!","['fake-proofs', 'lebesgue-measure', 'measure-theory']"
2425550,"Density of $X-Y$ where $X,Y$ are independent random variables with common PDF $f(x) = e^{-x}$?","$X,Y$ are independent random variables with common PDF $f(x) = e^{-x}$ then density of $X-Y = \text{?}$ I thought of this let $ Y_1 = X + Y$, $Y_2 = \frac{X-Y}{X+Y}$, solving which gives me $X = \frac{Y_1(1 + Y_2)}{2}$, $Y = \frac{Y_1-Y_2}{2}$ then I calculated the Jacobian $J = \begin{bmatrix} \frac{1+y_2}{2} & \frac{y_1}{2} \\ \frac{1}{2} & -\frac{1}{2} \end{bmatrix}$ so that $\left|\det(J)\right| = \frac{1+y_1+y_2}{4}$ and the joint density of $Y_1,Y_2$ is the following $W(Y_1,Y_2) = \left|\det(J)\right| e^{-(y_1+y_2)}$ when $y_1,y_2> 0$ and $0$ otherwise. Next I thought of recovering $X-Y$ as the marginal but I got stuck. I think i messed up in the variables. Any help is great!.","['exponential-distribution', 'probability', 'density-function', 'probability-distributions']"
2425572,Partial derivative of a function of a function,"Suppose $z=f(x+y)$. Taking partial derivatives w.r.t. $x$ on either side:
$\partial z/\partial x = \partial f(x+y)/\partial x$. Let us denote $x+y$ by $u$. So $\partial z/\partial x = \partial f(u(x,y))/\partial x$. How do I evaluate the R.H.S. The chain rule I know is for finding total derivatives and not partial derivatives.","['multivariable-calculus', 'partial-derivative']"
2425778,How to determine the $\lim_{n\to \infty} \frac{1+2^2+\ldots+n^n}{n^n}=1$. [duplicate],"This question already has answers here : Finding $\lim\limits_{n \to \infty}{\frac{1^1+2^2+3^3+\cdots+n^n}{n^n}}$ (4 answers) Closed 4 years ago . I stuck to do this, $$\lim_{n\to \infty} \frac{1+2^2+\ldots+n^n}{n^n}=1.$$ The only thing I have observed is $$ 1\le \lim_{n\to \infty} \frac{1+2^2+\ldots+n^n}{n^n}$$ I am unable to get its upper estimate so that I can apply Sandwich's lemma.","['sequences-and-series', 'limits']"
