question_id,title,body,tags
2828578,Interesting sequence involving prime numbers jumping on the number line.,"Udpate 4: Trying to characterize finite and infinite cycles. Update 3: All primes $a_0\ge29$ seem to either have infinite cycles or finite non-terminating cycles that converge to infinite cycles of other primes. More examples added at the end of the post (visualized as graphs). Also, renamed the post. Update 2: I've found it can be shown that $29$ and $31$ will never terminate. I believe the same can be done for all other primes. Explanation is added at the end of the post. Update 1: Turns out my initial question is a degenerate case of a broader observation. The broader observation (""Conjecture"") is added at the bottom of the post. Defining the sequence Pick any starting number $a_0\in\mathbb N$ on the number line. At $n$ th step (starting at step $n=1$ on your starting number): jump $a_0+n$ numbers to the right (add $a_0+n$) if current number is not composite jump $a_0+n$ numbers to the left (subtract $a_0+n$) if current number is composite If $a_0\lt0$, then reverse the condition (subtract for not composite and add for composite). If $a_0=0$, you stop jumping. By ""not composite"", it is meant ""prime or one"" (zero is stopping condition). For example, If we start with $a_0=6$, we have $(6)-7\rightarrow(-1)-8\rightarrow(-9)+9\rightarrow0$. This process can be defined as: Pick a starting number $a_0\in\mathbb N$ and define a sequence: $a_{n+1}=$ $$
\begin{cases} a_{n}+(a_0+n+1),  & \text{if $a_n>0$ and $a_n$
 is not composite or $a_n<0$ and $|a_n|$ is composite} \\ a_{n}-(a_0+n+1), &
 \text{if $a_n<0$ and $|a_n|$ is not composite or $a_n>0$ and $a_n$ is composite} \\ 0, & \text{if $a_n=0$} \end{cases} $$ ""Not composite"" in the above definition means that it ""is prime or is $1$"". ""Composite"" is then ""not prime and not one"", while zero ends the sequence. If there exists $k\in\mathbb N$ such that $a_k=0$, then we say the
  sequence terminates (converges to $0$). Examining the sequence It is easy to show that if $a_0$ is composite, the sequence always terminates: (One can write down the sequence for even $a_0$ and for odd $a_0$ to conclude:) $a_3=0$ if $a_0$ is odd and composite, or $a_0$ is even and $a_0+3$ is composite $a_{11}=0$ if $a_0$ is even and $a_0+3$ is not composite and $a_0+11$ is composite $a_{19}=0$ if $a_0$ is even and $a_0+3$ is not composite and $a_0+11$ is not composite But when $a_0$ is prime or equal to $1$ (not composite), I have (computed): $a_0=1 \rightarrow a_{14}=0, \space(1, 3, 6, 2, 7, 13, 20, 12, 3, 13, 24, 12, -1, -15, 0)$ $a_0=2 \rightarrow a_{8}=0, \space(2, 5, 9, 4, -2, -9, -1, -10, 0) $ $a_0=3 \rightarrow a_{6}=0, \space(3, 7, 12, 6, -1, -9, 0) $ $a_0=5 \rightarrow a_{22}=0, \space(5, 11, 18, 10, 1, 11, 22, 10, -3, -17, -32, -16, 1, 19, 38,\dots, 0) $ $a_0=7 \rightarrow a_{14}=0, \space(7, 15, 6, -4, 7, 19, 32, 18, 3, 19, 36, 18, -1, -21, 0) $ $a_0=11 \rightarrow a_{14}=0, \space(11, 23, 36, 22, 7, 23, 40, 22, 3, 23, 44, 22, -1, -25, 0) $ $a_0=13 \rightarrow a_{42}=0, \space(13, 27, 12, -4, 13, 31, 50, 30, 9, -13, -36, -12, 13, 39, 12,\dots, 0) $ $a_0=17 \rightarrow a_{38}=0, \space(17, 35, 16, -4, 17, 39, 16, -8, 17, 43, 70, 42, 13, 43, 74, 42, 9, \dots, 0) $ $a_0=19 \rightarrow a_{110}=0, \space(19, 39, 18, -4, 19, 43, 68, 42, 15, -13, -42, -12, 19, 51, 18,\dots, 0) $ $a_0=23 \rightarrow a_{106}=0, \space(23, 47, 72, 46, 19, 47, 76, 46, 15, -17, -50, -16, 19, 55, 18, \dots,0) $ Prime numbers $\ge29$ seem to never reach $0$. Are these truly the only primes that reach zero? Can we show that no other prime will reach zero? Update 1: After letting the sequence continue after numbers hit $0$, and observing sequences: (If $0$ in the above definition is treated as composite and does not terminate the sequence:) Conjecture: If we observe $a_0=a\ne23$, then: If $(a\gt19)$ is not prime , and $(a+4k)$ is prime number for some $k\in\mathbb N$, then the sequence starting with $a_0=a$  will hit $0$ exactly $k$ times for terms $a_3,a_7,a_{11},\dots,a_{4k-1}$, and no other terms. The case $k=0$ is a degenerate case of the previous claim and is: If $(a\gt19)$ is prime , we never hit $0$. It is easily shown that the above holds for $k\ne0$ (all cases of $k$ but the degenerate one) as odd composite numbers will reach zero at $a_3$ after which we have a new odd number, and the same thing will repeat unless we have a prime number; which causes the sequence to enter a cycle which seems to contains no further zeroes (entering the $k=0$ case). Trivial observation for cases not included above: If $(1\le a\le19)$ or $(a\gt2$ and is even $)$, $a_0=a$ will hit zero infinitely many times. This is true (can be easily shown) for even numbers since we know that even numbers $\gt2$ will reach zeroes at one of $a_3,a_{11},a_{19}$ terms for the first time, after which we are left with a new even number, which repeats the cycle indefinitely. It is true as well for first $19$ numbers and number $23$ since they are computed to continue at an even number after reaching their first zero, which enters the infinite cycle of the even numbers. It still remains to show that prime numbers $\ge29$ themselves will not reach zero, as those primes seem to enter cycles that do not contain zeroes. I suspect one should be able to show cycles that do not contain  zero for odd prime numbers, similar to the cycle for even numbers which revisits the zero infinitely many times. Update 2: I've observed that consecutive prime numbers reach same values infinitely many times. For example, primes $29,31$ will reach numbers ${25,26,27,28,29,30,31}$ infinitely many times as some $a_k$ members of their sequences. Turns out it can be shown that $29$ and $31$ will never reach $0$. This can be done by observing $a_k=29$ and assuming $k$ is even. By looking at all cases while writing out the sequence, eventually a full cycle will be shown and we can use induction. (Also for example; If you assume $59+k$ is prime and follow the blue arrow as shown below, that implies $k$ is even in the following nodes.) A simplified proof can be shown as a graph that generates the sequences for $29$ and $31$: Red arrows imply the previous node is composite, and blue arrows imply it is prime. Note that constants summed with $k$ in nodes are based on the starting position $a_k=29$ and assuming shorter paths back to $29$; taking longer paths results in larger summing constants in some nodes (meaning nodes being visited repeatedly or in different order following the longer paths before reaching $29$ and substituting $a_{k+l}=29$ with $a_k=29$ again); but the graph remains the same nonetheless. The nonconstant nodes clearly grow in absolute value for $k\ge0$ and $\ne0$. I believe we can construct a generating graph for all sequences starting with primes and show that the ones where $a_0\ge29$ will never reach $0$. Is it possible to prove this for all primes, rather than confirming it one by one for individual (finitely many) cases? Update 3: It seems that every starting prime number $a_0\ge29$ will either revisit itself infinitely many times (infinite cycle) for some $a_k$ terms, or revisit itself finitely  many times (finite cycle), and after that its sequence will converge to a sequence of some other prime; which again either has an finite or infinite cycle. We can revisit the $(29,31)$ example from above, where we have that the sequences of these two primes are the same infinite cycle. We can draw the same graph from above in a circular form: Nodes represent some terms $a_k$ of their sequences, and following the arrows you get the next term $a_{k+1}$. Nonconstant nodes have decimal names which were generated while drawing the graph. Constant nodes have their exact values; these are ${25,26,27,28,29,30,31}$ for this particular example, as already mentioned above. Nonconstant nodes $a_k$ are either always composite, or take both composite and prime values for different $k$; which is shown by having either one or two different paths leaving them. We can also look at an example involving finite cycles. Prime number $157$ has a finite cycle and its sequence converges to the prime number $163$. (Last time it is revisited is for $a_{116}=157$, and in total it is visited $23$ times) Prime numbers $(163,167)$ share the same finite cycle, and it converges to the prime number $173$. (Last time $a_0=163$ is revisited is for $a_{152}=163$, and in total it is visited $27$ times; Last time $a_0=167$ is revisited is for $a_{12}=167$, and in total it is visited $4$ times) Prime number $173$ has a finite cycle and its sequence converges to the prime number $179$. (Only time it is visited is for $a_{0}=173$, in total that's only once) Prime numbers $(179,181)$ share the same infinite cycle which can be displayed: We can also visualize the previous finite cycles of primes which eventually converge to the above cycle (redrawn again as the rightmost cycle): $157\rightarrow (163,167)\rightarrow173\rightarrow (179,181).$ Click the image to see it in full size. Another example; primes $(37,41,43)$ all share the same infinite cycle, and prime $47$ has a minimal finite cycle which converges to theirs: There is also an example such as: $359\rightarrow367\rightarrow373\rightarrow(379,383)\rightarrow389\rightarrow(397,401)\rightarrow(419,421)\leftarrow409$ Where we have two different sequences of finite cycles converging to the same infinite cycle. The longer sequence of cycles from this example can be seen below: It seems that primes sharing cycles and primes that are a part of connected cycles, are consecutive primes. All primes $359, 367, 373, 379, 383, 389, 397, 401, 409, 419, 421$ from the last example are consecutive. Seems like we could represent all sequences for all primes as one
  single graph, where consecutive primes build connected components of
  the graph. All primes $a_0\ge29$ seem to only revisit constant nodes $a_k$ that are $a_0\approx a_k$ their size. All other nonconstant nodes diverge to either $\pm\infty$, for $k\to\infty$. No $0$ nodes have been found so far, which would defy the $a_0\approx a_k$ observation, for constant $a_k$ ( by ""constant term of the sequence"" I mean ""recurring term of the sequence""; $a_k=a$ for different $k$ ). One can generate graphs for individual primes and prove this and the initial question (for finitely many terms); that primes $\ge29$ do not terminate (reach $0$). But I still do not know how can one prove this for all primes, rather than checking individual cases. If we look at sequences of primes, every prime eventually converges to some other prime. If we observe only the first next prime they visit, we can represent the convergence relations with a graph and build the forest of primes. For primes up to $10009$ we have: (Open the image in a new tab and zoom in) Update 4: Characterization of cycles I've noticed that the following seems to hold for prime $a_0$: If $a_0\ge29$ is a Sophie Germain Prime ($2a_0+1$ is prime), then: $a_0$ will have infinite cycle $\iff$ $a_0+2$ is prime $a_0$ will have a finite cycle that does not revisit $a_0$ $\iff$
  $a_0+2$ is not prime If $a_0\ge29$ is not a Sophie Germain Prime, then then $a_0$ will be revisited at least $2$ times. If $a_0\ge29$ is 1st term in Twin Prime pair ($a_0+2$ is prime), then: $a_0$ will have an infinite cycle $\iff$ $a_0$ is not a 3rd term of a
  prime quadruple . $a_0$ will have an finite cycle $\iff$ $a_0$ is a 3rd term of a
  prime quadruple . If we have a Prime Triplet starting with $a_0\ge29$, then: $a_0$ will have infinite cycle $\iff$ $a_0$ is not Median term of
  prime quintuplets . $a_0$ will have finite cycle $\iff$ $a_0$ is Median term of prime
  quintuplets . If we have tuples of $\ge4$ consecutive primes; that is: If $a_0\ge29$ is 1st term of a Prime Quadruplet or a longer Prime Constellation , then that seems to imply that $a_0$ will have an infinite cycle. Can we characterize the finite/infinite cycles more precisely; or find counterexamples to these claims?","['graph-theory', 'prime-numbers', 'sequences-and-series']"
2828582,"Continuous function on $[0,1]$, $f(0)=f(1)$","I came across this very interesting question, which seems to be partially answered in a couple posts around here: Let $f:[0,1]\rightarrow\mathbb{R}$ continuous such that $f(0)=f(1)$. Then for all $n>1\in \mathbb{N}$, there exists $x_n\in [0,1-1/n]$ such that $f(x_n)=f(x_n+1/n)$.
The case $n=2$ is simple and has been answered in this forum already. But for $n>2$ I can't seem to use the same logic (intermediate value theorem). Any ideas?","['continuity', 'general-topology', 'functions']"
2828587,Ranking participants based on tiers and totals,"I am trying to rank participants based on sets of data that I have. The data used is for a competition. In this competition, you can participate in X amount of events , at the end of the event you participate in you get ranked in a Tier (eg from Tier 1 to Tier 6). Example: Particpants  Tier 1 Tier 2 Tier 3 Total
John           2       3     2      7
Smith          1       2     4      7
Tom            3       1     1      5 In this example John was ranked in Tier 1 for 2 events out of the Total 7 he signed up for.
The issue I'm having is that I am unsure what formula to use that would rank the participants, the ranking would take into consideration the amount of events you participated in (the higher the better) and the Tier you got per event (the more you have in Tier 1 the better etc). Does such a formula exist? Please tell me if this seems to complex to calculate.","['statistics', 'page-rank']"
2828598,Is every norm increasing?,"Let $N$ be any norm on $\Bbb R^n$. Is it true that if $0 \leq a_i \leq b_i$ for $1 \leq i \leq n$, then $N(a_1, \ldots, a_n) \leq N(b_1, \ldots, b_n)$ ?
Clearly this is true for the norms $\| \cdot \|_p$ where $1 \leq p \leq \infty$, being defined as a composition of increasing functions. Any two norms are equivalent, but I don't see how my property is preserved under equivalence.","['normed-spaces', 'real-analysis']"
2828610,Point-free notation for function composition?,"Let $f$ and $g$ be a pair of functions mapping reals to reals. It is common to use the point-free notation $f\circ g$ to describe the function $h$ defined by $h(x)$ = $f(g(x))$. By ""point-free"" I mean the notation $f\circ{g}$ does not refer explictly to any function arguments $x$. Now consider $f:\mathbb{R}^2\to\mathbb{R}$ with $g:\mathbb{R}\to\mathbb{R}$ Is there a commonly used ""point-free"" notation for the composite function $h$ defined by $h(x,y) = f(x,g(y))$? That is, could we write $h$ in a generic way without reference to $x$ and $y$?","['notation', 'functions']"
2828617,Zeros of solution of a ODE,"Let $a$ be a real differentiable function $a(0)>0$ and $a(t)\geq 0$. Consider the solution $\phi$ of the differential equation
  $$x''+ ax=0$$
  with initial conditions $\phi(0)=1$ and $\phi'(0)=0$. Prove that there exist $t_0<0<t_1$ such that $\phi(t_0)=\phi(t_1)=0$. I don't know exactly what to do with this problem. It is clear that $0$ is local maximum of $\phi$ because of the initial conditions. Any hint?",['ordinary-differential-equations']
2828619,A question about the spectral theorem for unbounded self-adjoint operators,"The spectral theorem for unbounded operators says that if $A:D(A)\subset H\to H$ is a densely defined self-adjoint operator ($H$ Hilbert), and $f:\mathbb{R}\to \mathbb{R}$ is a Borel function bounded on $\sigma(A)$ then 
$$f(A) = \int_{\mathbb{R}} f(\lambda) dP(\lambda)$$
where $dP(\lambda)$ is the spectral measure of $A$. I naively thought that the spectrum of $f(A)$ is given by $f(\sigma(A))$ but then I came across the following.
Let $A = -\Delta: D(\Delta) \to L^2(\mathbb{R}^3)$, where $\Delta$ is the Laplacian. It  is well known that  $\sigma(A)= [0,+\infty)$. Let $\rho < 0$, then we can consider the self-adjoint operator
$$ (A-\rho)^{-1} = \int_{0}^{+\infty} (\lambda-\rho)^{-1}dP(\lambda) $$
This operator is the inverse of $A-\rho$, and by our naive assumption its spectrum should be $\frac{1}{\sigma(A)-\rho}$.
But from other theories we also know that the inverse $(A-\rho)^{-1}$ is a compact self-adjoint operator and thus its spectrum must be  a discrete set (contradiction). So, where have I made a mistake?","['functional-analysis', 'spectral-theory', 'operator-theory']"
2828629,Is there a surjective morphism $\widehat{\Bbb Z} \to \Bbb Z$?,"Consider the profinite completion $\widehat{\Bbb Z}$ of the additive group of $\Bbb Z$. Is there a surjective group morphism $s : \widehat{\Bbb Z} \to \Bbb Z$ ?
  If so, can we moreover assume that $s \circ i = \rm{id}_{\Bbb Z}$, where $i : \Bbb Z \to \widehat{\Bbb Z}$ is the canonical embedding? Clearly, there is no continuous map $\widehat{\Bbb Z} \to \Bbb Z$, otherwise $\Bbb Z$ would be compact and discrete... ; so $s$ can't be continuous. I also noticed that there is no surjective group morphism $\Bbb Z_p \to \Bbb Z$ (i.e. $\Bbb Z$ is not a quotient of the $p$-adic integers), for any prime $p$, since the former is $q$-divisible for any prime $q \neq p$, while the latter is not. Thank you!","['abelian-groups', 'group-theory']"
2828687,Forecasting seasonal data,"I am fairly new to time series modelling. Suppose that I have time series of electricity consumed per day of one company. The series is on workdays only and sometimes there is NA value because of a public holiday. I would like to forecast future electricity consumption. I am familiar with the ARIMA modelling in R, however to properly find seasonal component in R I need to (using auto.arima) specificy frequency of the data, which I believe is double seasonal - that is, depends on both workday and month of the year. Because I was unable to fit the ARIMA, I found a post about predicting a time series using linear regression. Thus my question is, whether the following is statistically correct approach. To deseason my data, I would start with a linear regression $$y_i = day_i + month_i + \epsilon_i,$$
which gives me estimates $y^{seasonal}_i$ of the seasonality of the data. Then I would model the rest of the series, that is $y_i - y^{seasonal}_i$ via ARIMA using Box-Jenkins for series without seasonal component, so basically to model the trend in the data. The forecasting would be simply to forecast from both of the models and sum it. Is my reasoning mathematically correct? Can it be used for correlated data?","['time-series', 'statistics']"
2828697,Universal Classes of Regular Graphs,"There is a small but well-known open problem in graph coloring called the Total Coloring Conjecture.  I worked on this problem several years ago and am interested in returning to it, potentially. For a finite graph $G$ with no multi-edges, I assume it is known what a proper coloring of its vertices is.  In the total coloring conjecture we also want to color the edges, i.e. assign a color to each edge such that no two edges sharing a common vertex are the same color.  A total coloring is a simultaneous proper coloring of the edges and the vertices so that, as well, no edge has the same color as one of its vertices. Let $\Delta(G)$ be the maximum valence of vertices of $G$.  The total coloring conjecture, by Vizing, states that for a graph $G$ the total coloring number, i.e. the minimal number of colors needed to totally color the graph, is at most $\Delta(G) + 2$. The method I was using was to supply total colorings of regular graphs and then embed $G$ into one.  There is a bit of difference between the even and odd cases concerning this, though it didn't cause problems in practice.  Given a homogeneous regular graph I was able to totally color a large number of these manually, even for high degrees, since the adjacency matrices are sort of 'modular' looking and thus susceptible to elementary arguments (putting colors of vertices on diagonals and colors of edges elsewhere). So I am wondering, is there a nice, well-behaved family of (preferably homogeneous) regular graphs for valence $n$ that contain all regular graphs of valence $n$?  What I am picturing are sort of 'longer and longer wreaths' of a single graph, chosen from some finite collection based on $n$, and some rules for braiding it together.  I suppose it is alright if the regular graphs are infinite. Does the question make sense?  Perhaps negative results would also be helpful, to evaluate the feasibility of this method.","['graph-theory', 'algebraic-graph-theory', 'combinatorics', 'coloring', 'infinitary-combinatorics']"
2828715,Operator norm (induced $2$-norm) of a Kronecker tensor,"Let $A \in \mathcal M(n \times n; \mathbb R)$ with $\rho(A) < 1$ . Then we know $I \otimes I - A^T \otimes A^T$ is invertible where $\otimes$ denotes kronecker product. Let $\text{vec}$ denote the vectorization operation and $\mathcal T = (I \otimes I - A^T \otimes A^T)^{-1} : \mathbb R^{n^2} \to \mathbb R^{n^2}$ . The operator norm of $\mathcal T$ is given by \begin{align*}
  \|\mathcal T\|_2 = \sup_{\|x\|_2=1} \|\mathcal Tx\|_2,
\end{align*} where $x \in \mathbb R^{n^2}$ . Let $\text{mat}$ denote the inverse operation of $\text{vec}$ , i.e., stacking the elements into a matrix. Let $X = \text{mat}(x)$ where $X \in \mathcal M(n \times n)$ . We note $\mathcal Tx$ is exactly the vectorization of $Y$ where $Y$ is the unique solution of \begin{align}
\label{eq:1}
\tag{$\star$}
A^T Y A + X = Y.
\end{align} The assumption on $A$ guarantees a unique solution. My question is whether we can take $X$ to be symmetric to determine the operator norm of $\mathcal T$ . That is, does the following hold \begin{align*}
\|\mathcal T\|_2 = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\},
\end{align*} where $Y$ is the solution to \eqref{eq:1} and $\mathbb S_n$ denotes the set of symmetric matrices. If we let $c = \sup \{\|Y\|_F: X \in \mathbb S_n, \|X\|_F=1\}$ , clearly $c \le \|\mathcal T\|_2$ . Will the other way hold? $\mathcal T$ seems to have nice structure and it makes me wonder whether we can just consider supremum over this proper subset. BOUNTY EDIT: This question has been a while. Loup Blanc has an excellent answer. I thought I understood his edit 2 before but actually it was misunderstanding. Now I am starting a bounty hoping someone (maybe loup himeself) could elaborate his Edit 2.","['kronecker-product', 'matrix-analysis', 'matrices', 'matrix-norms', 'linear-algebra']"
2828724,A strictly decreasing nested sequence of non-empty compact subsets of S has a non-empty intersection with empty interior.,"S is an Hausdorff topological space. A decreasing nested sequence of non-empty compact subsets of S has a non-empty intersection. In other words, supposing $C_{k}$ is a sequence of non-empty, compact subsets of a topological we know that
$\cap_{k\in N}C_{k}\not= \emptyset$ If I assume that the sequence is striclty decreasing $C_{k+1} \subset C_{k}$ can I say the following $Int(\cap_{k\in N}C_{k})= \emptyset$? remark: In my opinion $C_{k+1} \subset C_{k}$ does not imply that diameters of $C_{k}$ are strictly decreasing. Any simple proof? Thanks.","['general-topology', 'real-analysis', 'filters']"
2828740,"If an extension is locally Galois, is it Galois?","Consider an extension of number fields $L/K$ . Basically, my question is know whether this extension has a given property, if we assume that the completions at all finite places of the extension have the same property.
More precisely, Assume that for almost all finite prime $p$ of $K$ , there is a finite prime $P$ of $L$ above $p$ , such that $L_P / K_p$ is Galois. Does it follow that $L/K$ is Galois? One could make two variations: Make the stronger hypothesis that for all prime $p$ of $K$ and any prime $P$ above $p$ , the extension of local fields $L_P / K_p$ is Galois. Replace ""Galois"" by ""abelian"", both in the hypothesis and conclusion. Some thoughts: Recall that the converse holds : if $L/K$ is Galois, then so is any completion. We may write $L = K(a)$ for some $a \in L$ . Then I think that for any prime $p$ of $K$ , there is $P \mid p$ such that $L_P = K_p(a)$ (consider the minimal polynomial $f$ of $a$ over $K$ , then $a$ is a root of some irreducible factor of $f$ in $K_p[T]$ , and we have $L \otimes_K K_p \cong K_p[T]/(f) \cong \prod\limits_{P \mid p} L_P$ ). But in general, $K_p(a) \cap \overline{K}$ strictly contains $K(a)$ , so if we pick any $\sigma : L = K(a) \to \overline{K}$ and try to extend it to $K_p(a)$ in order to use the assumption, we are stuck. We cannot consider the variation ""replace Galois by solvable "". Indeed, any finite extension of local fields is solvable, while there are $S_5$ -extensions of number fields. For the variation 2., if we further assume that $L/K$ is Galois (not necessarily abelian), the question is just to deduce or not that $\mathrm{Gal}(L/K)$ is abelian, knowing that all the decomposition subgroups $D(P \mid p)$ are abelian.","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2828775,"integration by parts $\int \frac{x^{2}+4x}{x+2}\,dx$","$$\int \frac{x^{2}+4x}{x+2}\,dx$$ I have written it in this form: $$\int \frac{(x+2)^{2}-4}{x+2}\,dx$$: on this stage I try to do integration by parts, which gets me to : $$\frac{x^{2}}{2}+2x-4\ln\left | {x+2} \right |$$ but it's wrong for some reason. The right answer is : $$\frac{x^{2}+4x+4}{2}-4\ln\left | x+2 \right |$$ Where am I wrong?","['integration', 'definite-integrals', 'calculus']"
2828779,Variation of a Beatty Sequence,"I've been studying Beatty sequences lately, and, having read and understood a proof of Raleigh's theorem, I know that if $\lfloor \alpha x\rfloor$ is a Beatty sequence with $\alpha\gt 1$, then its complementary Beatty sequence is $\lfloor \beta x\rfloor$, where
$$\frac{1}{\alpha}+\frac{1}{\beta}=1$$
However, I'm having a little bit of trouble with the following: as a generalization of the Beatty sequence $\lfloor \phi n\rfloor$, I am considering the similar sequence $\lfloor \phi n-1/3\rfloor$ and trying to find its complement in the natural numbers. I believe that its complement is $\lfloor (\phi+1)n+2/3\rfloor$, but the method of proof used for Beatty's theorem doesn't work in this case. I used the second proof provided here . EDIT: My conjecture is false (see comments), but I would still like to find the complement of the originally mentioned sequence. Does anyone know how to find the complement of the following sequence?
  $$\{\lfloor \phi n-1/3\rfloor\}_{n=1}^\infty$$","['number-theory', 'sequences-and-series', 'irrational-numbers', 'elementary-number-theory']"
2828802,Angle definition confusion in Rodrigues rotation matrix,"The Rodrigues rotation formula gives us the following way to rotate a vector $\vec{v}$ by some angle $\theta$ about an arbitrary axis $\vec{k}$: $\vec{v}_\text{rot} = \vec{v}\cos(\theta) + (\vec{k} \times \vec{v})\sin(\theta) + \vec{k}(\vec{k}\cdot\vec{v})(1-\cos\theta)$ Let's call this the ""vector notation"" There is also a way to obtain the corresponding rotation matrix $\textbf{R}$, as such: $\textbf{R} = \textbf{I} + (\sin\theta)\textbf{K} + (1-\cos\theta)\textbf{K}^2$ $\>\>\>\>\>\downarrow$ $\vec{v}_\text{rot} = \textbf{R}\vec{v}$ where $\textbf{K}$ is the cross-product matrix of the rotation axis: $\textbf{K} = \begin{pmatrix} 0 & -k_z & k_y \\ 
                              k_z & 0 & -k_x \\ 
                              -k_y & k_x & 0\end{pmatrix}$ Let's call this the ""matrix notation"" My issue is that I cannot seem to get the ""vector"" and ""matrix"" notations to agree... Let's say we have the vector $\vec{v} = \begin{pmatrix} \sqrt{1/3} \\ \sqrt{1/3} \\ \sqrt{1/3} \end{pmatrix}$ That we would like to rotate to lie on the x-axis: $\vec{v}_\text{rot} = \begin{pmatrix} |\vec{v}| \\ 0 \\ 0 \end{pmatrix} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}$ then our axis and angle of rotation are: $\vec{k} = \dfrac{\vec{v}\times\vec{v}_\text{rot}}{|\vec{v}\times\vec{v}_\text{rot}|} = \begin{pmatrix} 0 \\ \sqrt{2}/2 \\ \sqrt{2}/2 \end{pmatrix}\>\>\>\>, \>\>\theta = \cos^{-1}\left( \dfrac{\vec{v}\cdot\vec{v}_\text{rot}}{|\vec{v}||\vec{v}_\text{rot}|}\right) = 0.95532 \text{rad}$ The ""vector notation"" statement above does indeed give the expected answer (I've implemented it in Python with NumPy ): import numpy as np
v = np.array([1/3, 1/3, 1/3])**(1/2)
vr_desired = np.array([1, 0, 0])
theta = np.arccos(np.dot(v, vr_desired) / np.linalg.norm(v)*np.linalg.norm(vr_desired))
k = np.cross(v, vr_desired) / np.linalg.norm(np.cross(v, vr_desired))

vr_according_to_vector_form = v*np.cos(theta) + (np.cross(k,v)*np.sin(theta)) + k*(np.dot(k,v))*(1.0-np.cos(theta))
print(""v_rot = {}"".format(vr_according_to_vector_form)) where the result is v_rot = [ 1.  0.  0.] as expected. Now, trying the ""matrix notation"": K = np.array([[0, -k[2], k[1]],[k[2], 0, -k[0]],[-k[1], k[0], 0]])
I = np.eye(3)
R = I + np.sin(theta)*K + (1-np.cos(theta))*(K**2)

vr_according_to_matrix_form = np.matmul(R,v)
print(""v_rot = {}"".format(vr_according_to_matrix_form)) I get the wrong answer: v_rot = [ 1.48803387  0.3660254   0.3660254 ] I can only imagine that the angle $\theta$ is being defined differently in the vector and matrix forms of the Rodrigues formula... I am confident in this diagnosis because if I rotate the axis of rotation itself, I do get the same axis back, which seems to indeed indicate that it is simply an issue with $\theta$ that I'm not understanding, and that there isn't any issue in my declaration of $\textbf{R}$... print(""Rk - k = {}"".format(np.matmul(R,k) - k)) gives Rk - k = [ 0.  0.  0.] FYI, I've been using the this wikipedia resource: https://en.wikipedia.org/wiki/Rodrigues%27_rotation_formula There, it says that $\vec{v}$ in the vector formula is rotated ""by an angle Î¸ according to the right hand rule,"", and in the matrix formula, it is rotated ""through an angle Î¸ anticlockwise about the axis $\vec{k}$"". Those sound like identical statements to me, though.","['matrices', 'rotations', 'python']"
2828815,"My own proof of ""Every neighborhood is open"" (Rudin's definition)","Previously, someone posted his/her proof of ""every neighborhood is open"" here . I am focusing on the same statement, but it seems that my approach is different from his/hers. Could someone comment on my own proof? Here we go: Let $X$ be a metric space and $p\in X$. Fix $r>0$. Define a neighborhood around $p$ as $N_r (p)=\{q\in X| d(p, q)<r\}$. Now, take $w = \frac{r}{2}$. Then, $N_w (p)\subset N_r(p)$. In other words, every point of $N_r(p)$ is interior. Therefore, $N_r (p)$ is open. Since $r$ is arbitrary, this proves every neighborhood is open. Update: Under my definition of ""neighborhood,"" can I prove in this way instead: For all $m\in N_r(p)$, take $w=\frac{râˆ’d(m,p)}{2}$. Then, $N_w(m)\subset N_r(p)$. Update2 From the hints given in comments, I solve it: Assume $m\in N_r(p)$ and $q\in N_w(m)$. Take $w=r-d(m, p)$. Then, $d(p, q)\leq d(p, m) + d(m, q)< d(p, m) + r-d(m,p) =r$. Thanks to @JosÃ©CarlosSantos and @fleablood !","['general-topology', 'real-analysis', 'proof-verification']"
2828842,"Proving that for a reductive group $G$, $GV_r$ is closed","Let $G$ be a reductive group over an algebrically closed field $\mathbb{K}$ acting on a finite dimensional vector space $V$. Let $\lambda$ be a one-parameter subgroup of $G$. Let's consider for $r \in \mathbb{N}$
$$
V_r = \{ v \in V \, | \, \forall t \in \mathbb{K}^*, \, \lambda(t) \cdot v = t^r v \}.
$$
Let's denote $GV_r = \{ g \cdot v \, | \, g \in G, v \in V_r \}$. I wanted to show that $GV_r$ is closed. But I think that it is not a straighforward result. Since even if $V_r$ is closed, that doesn't imply that $GV_{r}$ is closed. For example, we know that an orbit of an element is locally closed but not necessarily closed. In addition, I think that the fact that $G$ is reductive play a role in the proof, but I don't know how. Since, I'm a begginer in the field of Algebraic groups, I'd really want some help to prove that $GV_r$ is closed. Any hint or reference would be great. Thanks in advance for your enlightenment. K. Y.","['reductive-groups', 'algebraic-groups', 'algebraic-geometry']"
2828862,"Which function satisfies $ð‘“: \mathbb R â†’ [âˆ’2, 2]$ with $(ð‘“(0))^ 2 + (ð‘“ â€² (0))^ 2 = 85$,also $f$ is twice differentiable?","Let $f:\mathbb R\to[âˆ’2,2]\,$ be a twice differentiable function 
  with $$\big(ð‘“(0)\big)^2+\big(ð‘“â€²(0)\big)^2=85.$$ Which of the following statements are necessarily TRUE? (A) There exist ð‘Ÿ, ð‘  âˆˆ â„, where ð‘Ÿ < ð‘ , such that ð‘“ is one-one on
  the open interval (ð‘Ÿ, ð‘ ) (B) There exists $ð‘¥_0\in(âˆ’4, 0),\,$ such that $|f'(ð‘¥_0)|\le 1$ (C) $\lim_{x\to\infty}f(x)=1$ (D) There exists $\,\alpha\in(âˆ’4,4),\,$ such that $\,f(\alpha) + f''(\alpha)=0$ and $f'(\alpha)\ne 0$ Solution:I tried it by using the function $f'(x)=x^3+2x+9$ ,as it is satisfying $(ð‘“(0))^
2 + (ð‘“
â€²
(0))^
2
= 85$ . On checking all the 4 options on the chosen function ,i'm getting $A,D$ as correct options. My query is that  the chosen function is not satisfying the definition of function( every element  of the domain has a unique image in codomain ) as $f(3)=43$ is not in $[-2,2]$ . Please provide a function which staisfies the definition in the problem.","['derivatives', 'real-analysis', 'alternative-proof', 'calculus', 'solution-verification']"
2828892,Ideal generated by linear polynomials is radical?,"Is an ideal generated by linear homogeneous polynomials a radical ideal? I know that a linear variety in $\mathbb{P}^n$ is always isomorphic to an algebraic set of form $V_+(x_0,\ldots,x_r)$ . (Can this be useful?)","['ideals', 'algebraic-geometry', 'commutative-algebra']"
2828919,Is there a relation between the solutions to these two Lyapunov matrix equations?,"Let $A \in \mathcal M(n \times n; \mathbb R)$ with $\rho(A) < 1$. Let $X, Y$ be the solutions to the following Lyapunov matrix equations \begin{align*}
X &= A^T X A + Q \\
Y &= A Y A^T + Q
\end{align*} where positive definite matrix $Q$ is given. By the assumptions on $A$, the solutions have closed form \begin{align*}
X &= \sum_{k=0}^{\infty} (A^T)^kQA^k,\\
Y &= \sum_{k=0}^{\infty} A^k Q (A^T)^k
\end{align*} I am wondering whether there are relationships between $X$ and $Y$, such as spectrum, norms, etc.","['matrices', 'matrix-equations', 'matrix-analysis', 'linear-algebra']"
2828925,Prove that $GL_n(\mathbb{F}_p)$ contains an element of order $p^n-1$,"If $p$ is prime and $n$ is a positive integer, prove that GL$_n(\mathbb{F}_p)$ contains an element of order $p^n-1$. I am a bit stuck on how to get started on this problem. I found it in the Galois theory section on a list of past qualifying exams, so I'm guessing some kind of Galois theory should be involved, but I can't seem to get anywhere with it. I know that $\mathbb{F}_{p^n}$ and $\mathbb{F}_p^n$ are isomorphic as $\mathbb{F}_p$ vector spaces, so $Gal(\mathbb{F}_{p^n}/\mathbb{F}_p)$ is isomorphic to a cyclic subgroup of GL$_n(\mathbb{F}_p)$ of order $n$, though this doesn't seem too useful (but maybe I'm wrong!). My only other idea at this point is to try to find some $\mathbb{F}_p$-linear transformation $T$ of $\mathbb{F}_{p^n}$ with characteristic polynomial $x^{p^n-1}-1$ and then use Cayley-Hamilton to get that $T$ has order dividing $p^n-1$, and hopefully the definition of $T$ (whatever it may be) will show it's order can't be lower than $p^n-1$. Can anyone see a more clever way to attack this problem?","['finite-fields', 'group-theory', 'galois-theory', 'linear-algebra']"
2829013,Sign of codifferential,"I've read at least two conventions for the sign of the codifferential. The first of them
\begin{align}
\delta &= (-1)^{k}\star^{-1}\operatorname{d}\star\\
&= (-1)^{kn+n+1}\operatorname{sgn}(g)\star\operatorname{d}\star\tag{1}
\end{align}
comes from Let $M$ be a smooth orientable $n$-dimensional manifold with exterior derivative $\operatorname{d}:\Omega^{k}(M)\to\Omega^{k+1}(M)$, metric $g\in\Gamma(T^{0,2}M)$ and volume form $\mu\in\Omega^{n}(M)$. One extends the metric $g$ to a metric $g_{(k)}$ on each $\Omega^{k}(M)$ requiring that for every $\alpha,\beta\in\Omega^{k}(M)$ we have
  $$g_{(k)}(\alpha,\beta) := \frac{1}{k!} g^{a_1b_1}\cdots g^{a_kb_k}\alpha_{a_1\dots a_k}\beta_{b_1\dots b_k}$$
  and define the sign of the metric by
  $$\operatorname{sgn}(g) := g_{(n)}(\mu,\mu) = \pm 1$$
  Then define the Hodge dual to be the operator $\Omega^{k}(M)\to\Omega^{n-k}(M)$ such that for every $\alpha,\beta\in\Omega^{k}(M)$ we have
  $$\alpha\wedge\star\beta = g_{(k)}(\alpha,\beta)\mu$$
  which leads to
  $$\star\star = (-1)^{k(n-k)}\operatorname{sgn}(g)$$
  and this implies
  $$\star^{-1} = (-1)^{k(n-k)}\operatorname{sgn}(g)\star$$ Now if we take the definition
$$\delta\omega = (-1)^{k}\star^{-1}\operatorname{d}\star\omega$$
And we calculate the action of $\star^{-1}$ over $\operatorname{d}\star\omega$: If $\omega$ is of grade $k$, $\star\omega$ is of grade $n-k$ and $\operatorname{d}\star\omega$ is of grade $n-k+1$. Hence we have
\begin{align}
\star^{-1}\operatorname{d}\star\omega
&= (-1)^{(n-k+1)(n-(n-k+1))}\star\operatorname{d}\star\omega\\
&= (-1)^{nk+n+k+1}\star\operatorname{d}\star\omega
\end{align}
Now, multiplying by $(-1)^{k}$ we get
$$\delta\omega = (-1)^{kn+n+1}\operatorname{sgn}(g)\star\operatorname{d}\star\omega$$
Which is formula $(1)$. However, I've also encountered the formula
\begin{align}
\delta &= (-1)^{kn}\operatorname{sgn}(g)\star\operatorname{d}\star\tag{2}
\end{align}
Where does $(2)$ come from?","['differential-forms', 'hodge-theory', 'smooth-manifolds', 'differential-geometry']"
2829040,Geometric puzzles similar to the two-part tetrahedron?,"http://math.rice.edu/~nsfmli/curriculum/Lecluse/divide_the_tetrahedron_in_two_and_four_equal_pieces.pdf The following is a conceptually simple puzzle. Most 5 year olds will understand the objective (but fail to solve it) and most Ph.D's will do just as poorly as the 5 year old (unless they already know the solution, which anyone reading this post now knows). Hand a person a tetrahedron that has been cut in two equal pieces (the provided link has a printable foldable 2D net). Tell them to put the two pieces back together to form a tetrahedron. Then watch them struggle to put it together again, fail and give up (or worse yet, conclude it is impossible). I have this puzzle laying around on my desk and almost everyone has failed, including some very smart people. I gave it to someone with graduate studies in crystallography and they got it pretty quickly. As for the rest... fail. It is surprising to me that most people fail. There must be an optical illusion going on or something similar. Anyways, enjoy teasing your friends (or enemies). Question: Know any similar geometric puzzles that are easy to explain and hard to solve?","['puzzle', 'geometry']"
2829056,How do i find all possible ways of obtaining a total value of 40 cents from 5 cent and 8 cent stamps?,You have an inexhaustible supply of $5$-cent and $8$-cent stamps. List all possible ways of obtaining a total value of $40$ cents with these stamps. I used a probability tree to solve this problem. But I feel like there's a better way and the probability tree took too much time. There are more questions like this and they the cents turn into dollars and they keep getting bigger. I was wondering if anyone has a more efficient way?,"['discrete-mathematics', 'elementary-number-theory']"
2829074,What's the distribution of $L_1$ norm of Gaussian random variables?,"Suppose we have a Multivariate Gaussian random variable $X\sim N(\mu,\Sigma)$. What's the distribution of $\|X\|_1$? My thinking:
There are a lot of articles talking about the distribution of $\|X\|_2$, But how about $L_1$ norm? Finding distribution of distance from origin https://stats.stackexchange.com/questions/25358/what-is-the-distribution-of-the-euclidean-distance-between-two-random-points-in","['normed-spaces', 'probability', 'normal-distribution', 'probability-distributions']"
2829098,"If the sum of the tail of a series goes to $0$, must the series converge?","Suppose $\{a_n\}$ is a sequence of positive terms. It is a well known result that if the series $\displaystyle \sum_{k=1}^{\infty} a_k$ converges then $\displaystyle \lim_{m \rightarrow \infty} \displaystyle \sum_{k=m}^{\infty} a_k=0 $. Is the converse true? That is, is it true that if the tail of a series goes to zero, then the series must converge? My thoughts: If we let $S_n=\displaystyle \sum_{k=1}^{n} a_k$, then clearly $S_n$ is an increasing sequence, and for $n > m$ , we have $S_n - S_m = \displaystyle \sum_{k=m+1}^{n} a_k $ Then we want to show that if $\displaystyle \lim_{m\rightarrow \infty} (\displaystyle \lim_{n\rightarrow \infty} S_n - S_m ) = 0$ (or just if it exists) then $\displaystyle \lim_{n\rightarrow \infty} S_n < \infty$. Note that since $S_n$ is increasing, it is enough to show that it is bounded. I tried to show this by definition, but my issue is that I am not sure how to deal with that double limit. Any help would be really appreciate it. Thanks!","['real-analysis', 'sequences-and-series']"
2829109,Notation of Euler Lagrange Equations for multiple independent variables,"Let $x_{1}, ..., x_{n}$ be several independent variables. Let $u = u(x_{1}, ..., x_{n})$ be a function. Let $L(x_{1}, ..., x_{n},u,\partial_{1}u,...,\partial_{n}u)$ be the Lagrangian whose action we want to extremize. Euler Lagrange equation is stated as 
$$\frac{\partial L}{\partial u} ~=~ \sum_{i=1}^{n}\frac{\partial}{\partial x_{i}}\frac{\partial L}{\partial (\partial_{i}u)} .$$ The problem is that the partial derivative with respect to $x_i$ is not really a partial derivative, rather it includes both implicit and explicit dependence on $x_i$. The notation is really unclear on this issue. Is there a way of indicating this dependency in the notation? Or is there another way of writing Euler Lagrange Equation in which this issue is bypassed altogether?","['multivariable-calculus', 'calculus-of-variations', 'euler-lagrange-equation', 'notation']"
2829116,How does Rellichâ€“Kondrachov not lead to a contradiction?,"I'm currently working through Lawrence Evan's PDE's textbook, and in it, he states the Rellich-Kondrachov theorem: Assume $U$ is a bounded open set of $\mathbb R^n$, and $\partial U$ is $C^1$. Suppose $1 \leq p < n$. Then
$$W^{1,p}(U) \subset \subset L^p(U)$$
for each $1 \leq q < p^*$ where $p^*$ is the Sobolev conjugate of $p$. I am currently failing to see how this does not contradict the fact that a linear space is compact if and only if it is finite dimensional. In particular the argument I have in mind is that, if $\overline{W^{1,p}(U)}$ is compact, then so is the unit ball in $W^{1,p}(U)$, being a closed subset of a compact set. But, $W^{1,p}(U)$ is clearly not finite dimensional. Where does the above argument go wrong? Thanks! Edit: After doing some further digging, it seems like the main cause of my confusion is that the topologist's definition of compact embedding and the analyst's definition are not equivalent, so I was under the impression that the theorem said something that it didn't.","['functional-analysis', 'sobolev-spaces', 'partial-differential-equations']"
2829121,"Intuitively, what exactly does the ellipse equation mean?","I get how to derive the ellipse equation, but I'm struggling to understand what it means intuitively. You see, a circle equation can be understood very intuitively. The circle equation models how the radius of the circle can be represented using the Pythagorean theorem. But I don't understand what the ellipse equation means at such a level. Does it model how an ellipse can be drawn out using a stretched rope? What exactly does it model? Can someone please explain? Can you please explain it as simply as possible, as I'm still a beginner?","['algebra-precalculus', 'intuition', 'conic-sections']"
2829142,Evaluate $\lim_{n \to \infty} \int_{0}^{1} x^2 \left(1+\frac{x}{n}\right)^n dx$,Evaluate $$I=\lim_{n \to \infty} \int_{0}^{1} x^2 \left(1+\frac{x}{n}\right)^n dx$$ My try: we have $$I=\lim_{ n\to \infty} \frac{1}{n^n} \int_{0}^{1} \left((x+n)^2-2n(x+n)+n^2\right)(x+n)^ndx$$ $\implies$ $$I=\lim_{ n\to \infty} \frac{1}{n^n} \int_{0}^{1}(x+n)^{n+2}-2n(x+n)^{n+1}+n^2(x+n)^n dx$$ $\implies$ $$I=\lim_{ n\to \infty} \frac{1}{n^n} \left(\frac{(n+1)^{n+3}}{n+3}-\frac{2n \times(n+1)^{n+2}}{n+2}+\frac{n^2 \times (n+1)^{n+1}}{n+1}\right)$$ $\implies$ $$I=\lim_{ n\to \infty} (n+1)\frac{(n+1)^{n}}{n^n} \frac{n^2+n+2}{(n+1)(n^2+5n+6)}=e$$ But the answer given is $e-2$ Any thing went wrong?,"['algebra-precalculus', 'integration', 'definite-integrals', 'limits']"
2829145,Real Analysis - Uniform continuity,"Prove that if $f: (a,b) \rightarrow \Re$ is differentiable on the open interval $(a,b)$, and $f'(x)$ is bounded on the interval $(a,b)$, then $f$ is uniformly continuous on $(a,b)$. Also, prove the converse is false, that is, find a function that is uniformly continuous on $(-1,1)$ whose derivative is unbounded on $(-1,1)$. So I'm a little confused with how to approach this question but this is what I'm thinking so far. So for every $\epsilon \gt 0$ there is a $\delta \gt 0$ and there exists $(x,y) \in (a,b)$ such that $|x-y| \lt \delta$... and this is where I get stuck I know it's only the beginning but I don't know how to incorporate the differentiability of $f$ and I have only done uniform continuity on functions such as $1/x$ or others like that can someone guide me in the right direction?","['derivatives', 'real-analysis', 'continuity', 'proof-explanation', 'uniform-continuity']"
2829157,Continuous mapping onto itself s.t. $f(A) = A$ where $A$ is closed,"Let $f$ be a continuous mapping of a Hausdorff non-separable space $(X,\tau)$ onto itself. Prove that there exists a proper non-empty closed subset $A$ of $X$ such that $f(A) = A$ . [ Hint: Let $x_0 \in X$ and define a set $S = \{x_n : n \in \mathbb{Z}\}$ such that $x_{n+1} = f(x_n)$ for every integer n ] Is the above result true if $(X,\tau)$ is separable? (Justify your answer.) I can find some examples of such maps and closed sets. Let $f$ be a continuous function: $$f: \mathbb{R} \to \mathbb{R}$$ $$f(x) = -x$$ and let set $A = \{-1, 1\}$ . Then $A$ is closed, proper and non-empty and $f(A) = A$ . I need some help with general proof. If at some stage $f(x_n) = x_0$ then sequence $\{f(x_n)\}$ is finite and closed. Otherwise it's infinite countable. Can't guess how to use non-separability here ...","['continuity', 'general-topology', 'separable-spaces']"
2829209,Does the invertibility of the frame operator for a set $E=(e_j)_{j \in J}$ imply that $E$ is a frame?,"I'm reading a book about Time-Frequency Analysis and I have a question regarding the property weather or not a set is a frame for a Hilbert space: Let $H$ be a Hilbert space and $E= (e_j)_{j \in J} \subset H$ a subset of elements in $H$ ($J$ countable). We define the associated frame operator $S$ via $$
S:H \to H,  \ \ Sf=\sum_{j \in J} \langle f,e_j\rangle e_j
$$ Now assume that $S$ is a bounded operator then it is well known that $S$ is a positive operator. My question is: If we further assume that $S$ is invertible, can we conclude that an estimate of the form $$
A\|f\|^2 \le \langle Sf,f \rangle \leq B \|f\|^2
$$ holds for $0 < A \leq B ?$","['fourier-analysis', 'hilbert-spaces', 'harmonic-analysis', 'operator-theory', 'functional-analysis']"
2829233,Markov's Mouse and the Maze,"I recently started to learn about Markov Chains and had a problem regarding the expected time to absorption: Problem: Markov has an untrained mouse that he place in a maze. The mouse may move between adjoining rooms, or stay in the same room at any time-step. At the exit of the maze is an enormous piece of unscented cheese where the mouse may leave from and never return. Given the following transition matrix: $$\begin{pmatrix}\frac 1{10}&\frac 3{10}&\frac 35&0&0&0\\\frac 12&\frac 12 &0&0&0&0\\\frac 3{10}&0&\frac 1{10}&\frac 35&0&0\\0&0&\frac 3{10}&\frac 1{10}&\frac 3{10}&\frac 3{10}\\0&0&0&\frac 12&\frac 12&0\\0&0&0&0&0&1\\ \end{pmatrix}$$ (a) What is the probability it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ?? (b) How long before it leaves the maze, given it started in State $i \quad \forall \ i \in[0,5]$ ? My Attempt: Well first I made a diagram to get the understanding of the problem visually. I replace the number States to letters, i.e. State $1$ as State $A$ etc. and wrote down the transition probabilities from the matrix. (a) I think because there exists an absorbing state at the exit, then the probability of exiting must be $1$ a.s. . (b) I used First-Step-Analysis to find the expected time for each $i$. Let $v_i$ be the expected time to exit from starting position $i$, i.e. $v_i =\Bbb E_i[T_{\{5\}}]$. Then solving the system of equations: $$
\left\{
\begin{array}{ll}
v_A &=1+\frac 1{10} v_A+\frac 3{10} v_B+\frac 35 v_C \\
v_B &=1+\frac 12 v_A +\frac 12 v_B\\ 
v_C &=1+\frac 3{10} v_A+\frac 1{10} v_C+\frac 35 v_D \\
v_D &=1+\frac 3{10} v_C+\frac 1{10} v_D+\frac 3{10} v_E+\frac 3{10} v_F \\
v_E &=1+\frac 12 v_D+\frac 12 v_E\\
v_F &=0
\end{array} 
\right.
$$ So I get the following solution:
$$\Bbb E_A[T_{\{5\}}]=14; \quad \Bbb E_B[T_{\{5\}}]=16 \quad \Bbb E_C[T_{\{5\}}]=11 \frac 13 \quad \Bbb E_D[T_{\{5\}}]=8 \frac 13 \quad \Bbb E_E[T_{\{5\}}]= 10 \frac 13$$ I'm not $100\%$ sure about the part (b). My Question(s): I was wondering, is there another way to look at this problem without using Markov Chains or First-Step-Analysis? I feel like there are geometric random variables involved here. Also what would happen if there were 2 exits, could I apply the exact same principles or is something changing? Thank you in advance!","['markov-chains', 'probability', 'linear-algebra', 'random-variables']"
2829263,Solve $2xy+y^2-2x^2\frac{\mathrm{d}y}{\mathrm{d}x}=0$; $y=2$ when $x=1$,"$2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0$ ; $y=2$ when $x=1$ . My reference gives the solution $y=\dfrac{2x}{1-\log x}$ , but is it really the solution ? My Attempt $$
2xy+y^2-2x^2\dfrac{\mathrm{d}y}{\mathrm{d}x}=0\implies\dfrac{\mathrm{d}y}{\mathrm{d}x}=\dfrac{2xy+y^2}{2x^2}=\frac{y}{x}+\frac{1}{2}\frac{y^2}{x^2}\\
\text{Put }v=\frac{y}{x}\implies y=vx\\
\dfrac{\mathrm{d}y}{\mathrm{d}x}=v+x\dfrac{\mathrm{d}v}{\mathrm{d}x}=v+\frac12v^2\implies x\dfrac{\mathrm{d}v}{\mathrm{d}x}=\frac{1}{2}v^2\\
2\int v^{-2}dv=\int\frac{\mathrm{d}x}{x}\implies -\frac{2}{v}=\log|x|+C\\
\boxed{\frac{-2x}{y}=\log|x|+C}
$$ $y=2$ when $x=1$$\implies C=-1$ , $$
\frac{-2x}{y}=\log|x|-1\implies \color{red}{y=\dfrac{2x}{1-\log|x|}}
$$ How do I justify going from $y=\dfrac{2x}{1-\log\color{red}{|}x\color{red}{|}}$ to $y=\dfrac{2x}{1-\log x}$ ? What I understand I only have basic knowledge on differential equations, thus not familiar with the ideas like singularity and all. I think I only understand a hint, $$
y=\dfrac{2x}{1-\log|x|}\implies y=\begin{cases}\dfrac{2x}{1-\log x} \text{ for } x>0\\\dfrac{2x}{1-\log(-x)} \text{ for } x<0\end{cases}
$$ So the condition "" $y=2$ when $x=1$ "" does not include in the second case. Does this has something to do with my doubt ?","['logarithms', 'absolute-value', 'ordinary-differential-equations', 'calculus']"
2829266,Prove/disprove: $f+g$ and $f$ are differentiable at $x_0$ $\implies$ $g$ is differentiable at $x_0$,"Prove/disprove: $f+g$ and $f$ are differentiable at $x_0$ $\implies$ $g$ is differentiable at $x_0$ attempt Suppose $g$ is not differentiable at $x_0$. There are three cases: Case I: The two following one-sided limits exist, but not equal.$$\lim_{x\to {x_0}^+}\frac{g(x)-g(x_0)}{x-x_0}\neq \lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}$$ Case II: $g$ is not continuous at $x_0$ Case III: $g$ is undefined at $x_0$ Case I: $$\lim_{x\to {x_0}^-}\frac{(f+g)(x)-(f+g)(x_0)}{x-x_0}=\lim_{x\to {x_0}^-}\Big(\frac{f(x)-f(x_0)}{x-x_0}+\frac{g(x)-g(x_0)}{x-x_0}\Big)=\lim_{x\to {x_0}^-}\frac{f(x)-f(x_0)}{x-x_0}+\lim_{x\to {x_0}^-}\frac{g(x)-g(x_0)}{x-x_0}=L_f+L_{g^-}$$Similarly, with the other one-sided limit:$$\lim_{x\to {x_0}^+}\Big(...\Big)=...=L_f+L_{g^+}$$ Case II: For every type of discontinuity of $g$ we use one-sided limit rules and continuity of $f$ to show that $f+g$ is not continuous and therefore not differentiable. Case III: That leads to $f+g$ being undefined at $x_0$. comment I haven't yet found a counterexample. If this idea of a proof works, is there a shorter one?","['derivatives', 'real-analysis']"
2829304,Find the largest area of a rectangle inside a circular segment of $\frac{2\pi}{3}$.,"What is the largest area of a rectangle inside a circular segment of $\frac{2\pi}{3}$ and radius $r$? 
  One side of the rectangle lies on the circle's chord. 
  We want a geometrical solution (as opposed to analytic geometry or trigonometry). If $r$ is the radius, then the rectangle has an ""elevation"" of $\frac{r}{2}$ above the $x$ axis.
If $A$ is the upper right vertex of the rectangle and $O$ the center of the circle, also $\theta$ the angle of $OA (= r)$ with the $x$ axis, then the area is 
$$ 2r\cos Î¸ (r\sin Î¸-\frac{r}{2}) $$
I can't think of any purely geometrical solution.",['geometry']
2829310,"Find matrices that commute with $\operatorname{Diag}(1,1,-1)$.","Let, $A = \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1& 0 \\ 0 & 0 & -1 \end{bmatrix}$ . Then find $ S= \{B \in M_{3\times3}(\Bbb R) :AB=BA\}$ . My attempt: Bare computation yielded $$\left\{B\in M_{3\times3}(\Bbb R)\,\middle|\,B = \begin{bmatrix} c_1 & c_2 & 0 \\ c_3 & c_4& 0 \\ 0 & 0 & c_5 \end{bmatrix} , c_1,c_2,c_3,c_4,c_5 \in \Bbb R\right\} \subseteq S$$ but I am having trouble showing the reverse inclusion (if it is true!). I was also tempted to use eigenvalues and eigenspaces to reach some conclusion by looking at the diagonal form of $A$ (displaying its eigenvalues), but can't get my way through! Thanks in advance for help.","['eigenvalues-eigenvectors', 'matrices', 'linear-transformations', 'linear-algebra', 'vector-spaces']"
2829352,Bootstrap for Mean with 95% Confidence Interval,"I've been working through a book Modern Data Science with R and I have a conceptual question about bootstrapping and confidence intervals. Say you do a bootstrap for a mean 1000 times.  How do you get the 95% confidence interval?  According to the demonstration in the book, you simply calculate the .025, .975 quantile. Can anybody explain why this is so? I'm wondering why this process doesn't include the familiar steps of calculating a confidence interval like you'd do in a t-test. Just in case there are any R users that want a reference to a specific example of the book exercise I am working with, it is here: https://mdsr-book.github.io/instructor/foundations-ex.html I am using R and the data for the 2nd exercise is the Gestation dataset available in the MosaicData package. This question was prompted by the difference between the 1st exercise and the 2nd one. The 1st exercise simply asked to calculate a confidence interval which I solved simply with the t.test function. The 2nd exercise I first solved with the Mosaic package (following book demonstration) but didn't really know ""why"" the answer works. (Book showed the procedure but didn't explain) So I'm basically wondering WHY the 95% confidence interval can be obtained by getting 1,000 or so means with resampling (e.g. bootstrap) and then getting the appropriate quantile.",['statistics']
2829369,Is there any systematic way for transforming differential equation into Clairaut form?,"I have a few differential equations with their substitution to form Clairaut form, but is there any standard method to get the intuition of the substitution? examples: $y = 2xp + y^2 p^3$, for transforming , put $y^2 = Y$ $y + px = x^4 p^2$, for transforming , put $1/x = X$ $y = 3px + 6 y^2 p^2$,  for transforming , put $y^3 = Y$ and few other transformation cases. Note : $p = \frac{dy}{dx}$ I understand that reducing to Clairaut's form involves suitable substitution so as to bring it in the form of $Y = P X + f(P)$ but i am unable to form any intuition about what such substitutions might be , as the above equations seem complicated with more than one combination of variables and $p$. Help appreciated.",['ordinary-differential-equations']
2829410,Consider the next succession and prove by induction,"The exercise says: Knowing the next succession, $$a_1=1$$ $$a_{n+1}=\frac{2a_n}{(2n+2)(2n+1)}, n>=1$$ Prove by induction that $a_n=\frac{2^n}{(2n)!}$ What I've done so far is prove for $n=1$ For $n=1$ : $$a_1=\frac{2^1}{(2\times1)!}=\frac{2}{2}=1$$ Which is correct. Therefore, to prove the induction: $$a_{n+1}=\frac{2\times a_{n+1}}{(2(n+1)+2)(2n+1)}=$$ $$=\frac{2}{(2(n+1)+2)(2n+1)}\frac{2^{n+1}}{(2(n+1))!}=$$ $$=\frac{2^{n+2}}{(2n+4)(2n+3)}\times\frac{1}{(2(n+1))!}=$$ $$=\frac{2^{n+2}}{4n^2+6n+18n+12}=$$ $$=\frac{2^{n+2}}{4n^2+24n+12}=$$ $$=\frac{2\times2^{n+1}}{2(2n^2+12n+6)}=$$ $$=\frac{2^{n+1}}{2n^2+12n+6}$$ Is this correct? Do I have to simplify even more? Thanks","['induction', 'sequences-and-series', 'discrete-mathematics']"
2829414,Solve the differential equation $x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5$,"Solve the differential equation $$x^5\frac{d^2y}{dx^2}+3x^3\frac{dy}{dx}+(3-6x)x^2y=x^4+2x-5$$ In the solution of the above the question the author is trying to convert it into an exact differential equation by multiplying $x^m$ and comparing the given equation to $$P_{0}y''+P_{1}y'+P_{2}y=\phi(x)$$ To make it exact it tried to $P_{2}-P_{1}'+P_{0}''=0$ how is he getting to this equation ? Also further on using this equation a value of $m$ was derived and substituted , then a first integral was made. My question is what exactly is first integral and how is it relevant in solving ordinary differential equation . If someone can guide me to a good source or link I would be thankful Also the method shown above is it a standard method of solving ODE ? if yes could you also please let me know the name / link of a source from where I can read this method.",['ordinary-differential-equations']
2829421,Lagrange multiplier plus or minus,"I am working through different tutorials about Lagrange multiplier and came upon a problem. It seems like that there's different ways to solve. You have a given equation $f(x,y)$ and a constraint $g(x,y)$. A few tutorials then start to set the derivatives respect to $x$ and $y$ of $\lambda\,g(x,y)$ and $f(x,y)$ equal to each other and start solving for variables: $$f(x,y)= \lambda\,g(x,y) \tag{1}$$ or use a form of $$F(x,y,\lambda)=f(x,y)  - \lambda \,g(x,y)\tag2$$ which is basically the same since you can subtract the right hand side of $(1)$ and derive $(2)$. Other tutorials use a form of $$F(x,y,\lambda)= f(x,y) + \lambda\,g(x,y)\tag3$$ So the main difference is the minus and the plus in their form and I am not sure which one to use. I have found multiple examples for both, so it's not a single mistake by one of the tutors. So which one is the correct one: $(2)$ or $(3)$? sources for $(1)$, $(2)$: A , B sources for $(3)$: C , D","['multivariable-calculus', 'maxima-minima', 'lagrange-multiplier']"
2829437,Understanding the absolute value of a matrix.,"I believe that the absolute value of a matrix is defined as 
$$
|A|=\sqrt{A^{\dagger}A} \ .
$$
But the square root of a matrix is not unique wikipedia gives a list of examples to illustrate this. To understand this, how does one work out the absolute value of:
$$
A=\begin{pmatrix}1 & 0\\0 & -1\end{pmatrix}
$$
Clearly $A^{\dagger}=A$ so $|A|=\sqrt{A^2}$, but this is not necessarily $A$. I want to pick the identity in this case, since then the eigenvalues of $|A|$ are both 1 (and they were $\pm1$ for $A$). But mathematics is not about what I want. So what is $|A|$? Is it well-defined? And how do I do this operation in general, since my application for this is of course far more complex.","['matrices', 'linear-algebra', 'absolute-value']"
2829467,Is there an algebraic extension $K / \Bbb Q$ such that $\text{Aut}_{\Bbb Q}(K) \cong \Bbb Z$?,"Is there an algebraic field extension $K / \Bbb Q$ such that $\text{Aut}_{\Bbb Q}(K) \cong \Bbb Z$? Here I mean the field automorphisms (which are necessarily $\Bbb Q$-algebras automorphisms) of course. According to this answer , one can find some extension of $\Bbb Q$ whose automorphism group is $\Bbb Z$. But I've not seen that one can expect this extension to be algebraic. At least such an extension can't be normal, otherwise $\Bbb Z$ would be endowed with a topology turning it into a profinite group, which can't be countably infinite.
(So typically, if we replace $\Bbb Q$ by $\Bbb F_p$, then the answer to the above question is no , because any algebraic extension of a finite field is Galois). Thank you!","['abstract-algebra', 'galois-theory', 'extension-field', 'field-theory']"
2829473,Neural Network Gradient Descent of Cost Function Misunderstanding,"I've taken an interest in neural networks recently and have been progressing rather well but came to a standstill while learning about gradient descent (I've done multivariable calculus previously). Specifically, I struggle with this: Say our neural network is designed to recognise digits 0-9, and we have the MSE Cost function which, given a certain vector of weights and biases, after a large number of training examples, will spit out the average 'cost' as a scalar. I have read that now one must compute the gradient -$\nabla{C}$ in order to find the vector of greatest descent, but I am confused on this: How does one graph the cost function/find an explicit formula for the cost function/compute $\nabla{C}$ - these would all require one to try an infinite number of weights and biases surely...??","['optimization', 'gradient-descent', 'multivariable-calculus', 'machine-learning', 'neural-networks']"
2829508,"$f: [0,1] \to \mathbb{R}$ absolutely continuous, $f' \in \{0,1\}$ (a.e.), $f(0)=0$. Prove that for some measurable subset $A$, $f(x)=m(A \cap (0,x))$","Problem: Suppose that $f: [0,1] \to \mathbb{R}$ is absolutely continuous, $f' \in \{0,1\}$ (a.e.) and $f(0)=0$ . Prove that for some measurable subset $A \subset [0,1]$ and every $x \in [0,1]$ we have $f(x)=m(A \cap (0,x))$ I have proved it on my own. Please help me verify and tell me how much of the problem you think I have solved. (This was an exam problem and I'm trying to verify my given answer). I didn't have time to study derivatives for the real analysis exam and my argument might lack a bit of rigor, I think. Edit: I'm going to write down my proof as an answer to mark this question solved.","['real-analysis', 'absolute-continuity', 'lebesgue-integral', 'measure-theory', 'analysis']"
2829529,Relation between $ \bigcap_{i \in I}A_i $ and $ \bigcap_{i=1}^{n}A_i $,"Let I be a nonempty set and a family of sets 
such that every element of the family is a subset of U. $\mathcal F = \{A_i | i \in I\}$ I understand the meaning of this operation: $$
\bigcap_{i \in I}A_i
$$ That's the intersection of all the elements of the sets of the family $\mathcal F$. But I don't truly understand what this other operation means and what's the relation between the above one.
$$
\bigcap_{i=1}^{n}A_i
$$ I understand that this operation is also an intersection but what I am trying to understand is the relation between $\bigcap_{i \in I}A_i$ and $\bigcap_{i=1}^{n}A_i$ $$
\bigcap_{i=1}^{n}A_i
\subseteq\bigcap_{i \in I}A_i
$$ Is the relation above true?","['notation', 'elementary-set-theory']"
2829530,$2$-dimensional intermediate value theorem,"23/06/2018 I'm looking for a proof of the following result. I don't know if it is true or not, but it seems to be true. I have tried it from different points of view, but I can not formalize them correctly. My ideas were to apply the intermediate value theorem for each radius or maybe in a topological manner considering the closed sets $Z_2 := \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ and $Z := \{z\in\mathbb{C}\;|\;g(z) = 0\}$. Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous
  functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and
  define $g:= g_1 - g_2$. Suppose that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to
  $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$. Solved. It is false and a counterexample given by a collegue at the University of Barcelona (Jordi) is the following: $g_1(z) = 1$ for all $z\in \overline{\mathbb{D}}$ and $g_2 := g_2' + g_2''$, where
\begin{equation}
  g_2'(z) = \begin{cases}
0, & \text{if }|z| \leq b\\
|z| - b,& \text{if }|z| > b
\end{cases}
\end{equation}
and
\begin{equation}
  g_2''(z) = \begin{cases}
1 - \frac{2}{a}|z - a|, & \text{if }|z - a| \leq \frac{a}{2}\\
0,& \text{if }|z - a| > \frac{a}{2}
\end{cases}
\end{equation}
for some $0<b<1$ and $a < b/2$. 25/06/2018 I forgot an extra hypothesis, which is that the connected components of $Z_2$ are simply connected, so that this is the new statement: Let $g_1,g_2:\overline{\mathbb{D}}\to [0,\infty)$ be continuous
  functions, where $\mathbb{D}:=\{z\in\mathbb{C}\;|\;|z| < 1\}$, and
  define $g:= g_1 - g_2$. Assume that the connected components of $Z_2:= \{z\in\mathbb{C}\;|\;g_2(z) = 0\}$ are simply connected and that $g_2(0) = 0$; $g_2(z) > 0$ for all $z\in\mathbb{S}^1$; $g_1(z) > 0$ for all $z\in \overline{\mathbb{D}}$. Then, there exists some simple and closed curve (homeomorphic to
  $\mathbb{S}^1$) $\gamma\subset \overline{\mathbb{D}}$ such that $0\in \text{int }\gamma$; $g_2(z) > 0$ for all $z\in \gamma$; $g(z) > 0$ for all $z\in \gamma\cup\text{int }\gamma$.","['general-topology', 'complex-analysis', 'real-analysis', 'algebraic-topology']"
2829572,"$\int_\gamma P\,dx+Q\,dy$ as the area of a domain","Let $A$ be a domain in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve. Find two functions $P,Q:\mathbb R^2\to \mathbb R$ such that $\int_\gamma P\,dx+Q\,dy$ is the area of $A$. Is it sufficient to apply Green's theorem and find $P$ and $Q$ with $Q_x-P_y=1$? So, will $Q(x,y)=x, P(x,y)=x$ do the job?","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
2829644,A line integral as the average of squares of distances,"Let $A$ be a domain  in $\mathbb R^2$ whose boundary $\gamma $ is a smooth positively oriented curve and whose area is $|A|$. Find a function $F:\mathbb R^2\to \mathbb R$ such that $$\frac{1}{|A|}\int_\gamma  Fdx+Fdy$$ is the average value of the square of the distance from the origin to a point of $A$. I guess I should use Green's theorem at some point, but I don't know how exactly, and how to start.
The distance from $(x,y)\in A$ to the origin is $\sqrt{x^2+y^2}$. So the average I believe is $\frac{\sum_{(x,y)\in A} x^2+y^2}{|A|}$? I don't know where to get from this.","['multivariable-calculus', 'real-analysis', 'integration', 'calculus']"
2829666,Finding a Strict Lyapunov Function for $\ddot{x} + \dot{x} + x^3 = 0$,"Consider the system $\dot{x}=y$ and $\dot{y}=-x^3-y$. We know that the function $V(x,y)=0.5y^2+0.25x^4$ is a Lyapunov function for the system and thus, the system is stable. However, we know that the system is globally asymptotically stable (by looking at the vector field). How can we prove global asymptotic stability of this system?
Thank you, in advance, for your response!","['control-theory', 'ordinary-differential-equations', 'nonlinear-dynamics', 'stability-theory']"
2829676,Proof verification: prove inclusion between two sets,"Let $(X,\mu)$ be a measure space and $f_1,\cdots,f_d\in L^\infty(X)$. Set
$$g:=\displaystyle\sum_{k=1}^d|f_k|^2\;\;\text{and}\;\;c:=\|g\|_\infty.$$
and
$$A_\varepsilon=\left\{x\in X;\;\displaystyle\sum_{k=1}^d \left[\Re(f_k(x))\right]^2+\left[\Im(f_k(x))\right]^2>c-\varepsilon\right\}.$$ Let $\sigma:=\{a_1,b_1,\cdots,a_d,b_d\}$ be such that $a_i,b_i\in \mathbb{Q}_+$ for all $i$. Set
$$S_\sigma=\left\{x \in X;\; \left[\Re(f_k(x))\right]^2>a_k,\; \left[\Im(f_k(x))\right]^2>b_k,\;\;k=1,\cdots,d\right\}.$$
and
$$\mathfrak{F}=\left\{\{a_1,b_1,\cdots,a_d,b_d\}\subset \mathbb{Q}_+^{2d};\;\;\sum_{k=1}^d (a_k+b_k) > c-\varepsilon,\;\forall \varepsilon>0\right\}.$$ I want to show that
  $$A_\varepsilon \subset \bigcup_{\sigma \in \mathfrak{F}} S_\sigma,\;\forall \varepsilon>0\;?$$ If $x\in A_\varepsilon$ such that $\left[\Re(f_i(x))\right]$ and $\left[\Im(f_i(x))\right]$ are non-zero, then we can choose non negative rational numbers $a_i$ and $b_i$ such that $$ 0\lt     \left[\Re(f_i(x))\right]^2-  a_i\lt \frac{ \varepsilon}{2d}   \mbox{ and }   0\lt     \left[\Im(f_i(x))\right]^2 -b_i \lt \frac{ \varepsilon}{2d}          .$$ Hence we can set $\sigma:=\{a_1,b_1,\cdots,a_d,b_d\}$. Then $\sigma$ belongs to $\mathfrak{F}$ and $x$ belongs to $S_\sigma$. The problem if $x\in A_\varepsilon$ such that $\left[\Re(f_i(x))\right]=0$ or $\left[\Im(f_i(x))\right]=0$.","['measure-theory', 'elementary-set-theory']"
2829685,Hamiltonian cycle containing each edges of graph,"Let $n$ be the number of the vertices of a graph $G$. If the degree of each vertex of $G$ is at least $\frac{n+1}{2}$, prove that for an edge $e$ of $G$ there is  a Hamiltonian cycle containing $e$. I know that the theorem which says that if for every two vertices $x,y$ of a graph, $d(x)+d(y)\ge n$ then there is a Hamiltonian path. I tried to use the idea of the proof of this theorem, using maximal paths, but I could not conclude this assertion.","['combinatorics', 'graph-theory', 'extremal-graph-theory']"
2829686,Problem related to quadratic equations,"Let $f(x) = x^2 + ax + b$ be a quadratic polynomial in which $a$ and $b$ are integers. If for a given $n$ , $f(n)f(n+1) = f(m)$ for some integer $m$ then the value of $m$ is ? Do we have to substitute $x$ by $n$ and $n+1$ and then multiply it? or is there another method to get the solution because the mentioned process will give an extremely large equation. Answer is $n(n+1)+an+b$","['algebra-precalculus', 'polynomials', 'functions', 'quadratics']"
2829704,Is the set of conjugates of a subgroup a group?,"Let $G$ be a group and $H$ be any subgroup of $G$. Let $A = {gHg^{-1} \bar g \in G}$. Define a binary operation on $A$ by $(gHg^{-1})(hHh^{-1}) = ghHh^{-1}g^{-1} $. I want to show that $A$ is a gelroup under this operation. I proved that this operation satisfies three conditions of group operation(identity, inverse, and associativity). But I recognized that $A$ becomes a group if this operation is well-defined. How can I show this part? Or are there any counterexamples?",['group-theory']
2829710,Largest inscribed rectangle in sector,"What is the largest (by area) rectangle that can be inscribed in a circular sector wiith radius $r$ and central angle $\alpha$? I think I have an answer to this question, but would like it confirmed. It is well known that the largest rectangle in a semi-circle (where the central angle $\alpha = \pi$) has the dimensions $x= \sqrt{2}r$ and $y=\frac{\sqrt{2}}{2}r$. For sector angles larger than $\pi$, i.e for $\pi \lt \alpha \lt 2\pi$, this remains the largest rectangle, as no rectangle can bend around or encompass the circle's center (see figure below). For sectors smaller than a semi-circle, i.e. for $0 \lt \alpha \lt \pi$ we have the following situation: After setting up equations for x and y (dependent on $\theta$ and $\alpha$), setting area $A = xy$ and differentiating, I find that the maximum area rectangle occurs when $$\theta = \frac{\alpha}{4}$$
Can anyone confirm this is correct? As a test I set $\alpha = \pi$ (i.e the semi-circle) and this gives $\theta = \frac{\pi}{4}$ which is correct. An interesting sidenote (if the formula is correct) is that when $\alpha = \frac{\pi}{3}$ the maximum area rectangle is a square, for the first time since $\alpha = 2 \pi$. A follow-on question: Is the largest inscribed rectangle also the largest possible rectangle? I.e. could there be larger rectangles where every corner does not touch the sector?","['derivatives', 'circles', 'geometry']"
2829725,How many possible sets we can get?,"In how many ways can the power set of the set $\{1,2,\cdots ,n\}$ be partitioned into two sets $S_1,S_2$ such that; $$ \bigcup_{X\in S_1} X=\{1,2,\cdots ,n\}$$ where the minimum cardinality of each element of $S_1$ is $k>1$ For example when $n=3, k=2$, one acceptable $S_1$ would be $$S_1 = \Big\{\{1,2\},\{1,3\},\{1,2,3\} \Big\}$$ My question is: Can we find a general formula for number of such partitions as a function of $n$ and $k$? My attempt: I recently found that it has something to do with Dedekind numbers . If we
consider all possible strings of length $n$ ,place $1$ in the place of $k$-th number if $k$-th number is present in a $S_1$ and $0$ otherwise. Like, for $S_1= \{\{1,2\},\{1,3\}\}$ we have 110 and 101 . Now, consider a function which will give output $0$ or $1$ , when the string is accepted then output will be $1$ and if not then $0$. Observe that, if a string is accepted then also any string bigger than the accepted one is also a accepted string. So actually counting the number of functions $f:\{0,1\}^n\to \{0,1\}$ which are monotone are the number of possible such sets(haven't considered the condition that union of all subsets equals to $\{1,2,\cdots ,n\}$ ). Which is Dedekind number . Now, if consider the condition that the union covers the set $\{1,2\cdots ,n\}$ then we can use the method of counting number of onto functions and get the desired number of ways $$\sum_{i=0}^n(-1)^i\binom{n}{i}M(n-i)$$
where $M(n)$ is the $n$-th Dedekind number . Note:$S_2$ is of no use, just stated.",['combinatorics']
2829732,"Does the series $\sum \frac{x^n}{1+x^n}$ converge uniformly on $[0,1)$?","Consider the following series for $x \in [0,1)$:
$$
\sum \frac{x^n}{1+x^n}
$$
I figured that it converges, by using the ratio test: 
$$
\left| \frac{a_{n+1}}{a_n} \right| = \frac{x^{n+1}}{1+x^{n+1}} \cdot \frac{1+x^{n}}{x^n} = \frac{x+x^{n+1}}{1+x^{n+1}} < \frac{1+x^{n+1}}{1+x^{n+1}} = 1.
$$
with $a_n = \frac{x^n}{1+x^n}$. However, I would like to determine if this convergence is uniform on this interval. I know that for $a<1$ it is uniform on the interval $[0,a]$, since $g_n(x) = \frac{x^n}{1+x^n}$ is increasing on the interval, so we have for every $n$:
$$
|g_n(x)| \leq \frac{a^n}{1+a^n}
$$
and $\sum \frac{a^n}{1+a^n}$ converges, as just shown, so the convergence of $\sum g_k(x)$ is uniform by the Weierstrass M-test. However, is it uniform on $[0,1)$?","['real-analysis', 'uniform-convergence', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2829741,"Using different characterizations of compactness, continuity, etc.","I've been looking for simple problems online to improve my grasp on the basics of elementary analysis. I'm not sure how much context I should include to make this question understood, so I'll just include it all; my question will be at the very bottom of the post if the details aren't needed and you want to skip to it. I found the following problem, and thought of two ways to solve it: $\textbf{Problem}$: A real-valued function $f$ on a closed, bounded interval $[a,b]$ is said to be upper semicontinuous provided that for every $\epsilon>0$ and every $p\in[a,b]$, there is a $\delta=\delta(\epsilon,p)>0$ such if $x\in[a,b]$ and $|x-p|<\delta$ then $f(x)<f(p)+\epsilon$. Prove that an upper semicontinuous function is bounded above on $[a,b]$. $\underline{\text{First Proof}}$: The argument is by contradiction. Suppose that $f$ is not bounded above. Then, for each $n\in\mathbb{N}$, there exists $x_n$ such that $f(x_n)>n$. Since the sequence $(x_n)$ is bounded, it contains a convergent subsequence, $(x_{n_k})$, converging to some $x^*\in[a,b]$. Given $\epsilon>0$, there exists $\delta>0$ such that for all $x$ satisfying $|x-x^*|<\delta$, we have $f(x)<f(x^*)+\epsilon$. There exists $K$ such that for all $k\geq K$ we have $|x_{n_k}-x^*|<\delta$, so that $f(x_{n_k})<f(x^*)+\epsilon$ for all $k\geq K$. This is a contradiction, since we must have $f(x_{n_k})>n_k\geq k$ for all $k$. $\underline{\text{Second Proof}}$: Given an $\epsilon >0$, the set of open intervals $$\mathcal{O}=\big\{(p-\delta_{\epsilon,p}, p+\delta_{\epsilon,p}): p\in [a,b]\big\}$$ forms an open cover of $[a,b]$. Heine-Borel guarantees the existence of a finite subcover: $$[a,b] \subset \bigcup^{n}_{i=1}(p_i-\delta_{\epsilon,p_i}, p_i+\delta_{\epsilon, p_i}).$$ Let $f(p^*)=\max \{f(p_1),f(p_2),...,f(p_n)\}$. Then $f(x)<f(p^*)+\epsilon$ for all $x\in [a,b]$, so that $f(p^*)+\epsilon$ serves as an upper bound for $f$ on $[a,b]$. After doing the second proof and rereading the first proof, I realized (assuming the proofs are correct): Both rely on the compactness of the interval $[a,b]$. The first proof uses the sequential characterization of compactness; the second uses the finite subcover definition of compactness. I wondered: Can I use the sequential characterization to get a direct proof? I don't see how to do this, and doubt it can be done. So, here's my question: If a direct proof really $\textit{isn't}$ possible with the sequential characterization, is there some deeper reason for this? Does this suggest some underlying difference between the sequential characterization and the finite open subcover definition? More generally, it's impossible not to notice that there are often several equivalent characterizations of a single concept in analysis (e.g., the $\epsilon$-$\delta$ vs. the sequential vs. the open set definitions of continuity), and sometimes one is more appropriate than another. If it happens that for a given problem or a given theorem, one characterization of a concept only allows for indirect proof whereas another characterization allows for a direct proof, is there always some reason for this? Does this tell us anything significant about the differences between the characterizations, or about what assumptions underlie the different characterizations? I hope this question make sense. Please change the tag if another is more appropriate.",['real-analysis']
2829776,Cardinality of Cartesian product of infinite sets,"I want to find the cardinality of the cartesian product of N and P(Q), which is the power set of the rationals. I wish to compare it to the cardinality of P(NxQ), i.e, the power set of the cartesian product of N and Q. My intuition say that it's equal to the cardinality of R, am I correct ? Why ? Thank you","['cardinals', 'elementary-set-theory']"
2829789,Combinatorics: How many possible paths?,"Consider the following grid. We start at the bottom left corner.  We may only move one step up or one step right at each move.  This procedure is continued until we reach the top right corner.  How many different paths from the bottom left corner to the top right corner are possible? This is an excerise from Sheldon Ross' A First Course in Probability. The correct answer is $$\binom{7}{4}=35.$$
However, no explanation is given.  I understand that there are $7$ moves total that must be made:
$$3\ \text{steps up and}\ 4\ \text{steps to the right.}$$
Can anyone provide me with an explanation of how the author came up with this answer? Thanks so much!",['combinatorics']
2829792,"Closed unit ball in $ L^2 ([0, 1]) $ intersect the set of $1$-Lipschitz functions is compact in the $ L^2 $ norm.","Let $$K = \{f \in  L^2 ([0, 1])  \,\, | \,\, \|f\|_{L^2} \leq 1 \} $$$$ L = \{f:[0, 1] \to \mathbb C \,\, | \,\, |f(x) - f(y)| \leq |x - y| \,\, \forall x, y \in [0, 1]\} $$ The question is to show $ K \cap L $ is compact in $ L^2 ([0, 1]) $. I have shown that both $ K $ and $ L $ are closed in $ L^2 ([0, 1]) $, and that $ K $ is not compact in $ L^2 ([0, 1]) $. As $ L^2 ([0, 1]) $ is complete in its norm, it would suffice to show that $ K \cap L $ is totally bounded, but I'm not sure how to begin proving this.","['functional-analysis', 'compactness']"
2829855,How to calculate the integral $\int_{0.5}^{1} \frac{1}{\sqrt{2x-x^2}}$?,"I have the following integration problem: $$ \int_{0.5}^{1} \frac{1}{\sqrt{2x-x^2}} $$ And I can see I should probably be completing the square here. I may be missing something extremely obvious, but wouldn't this mean I have to take the negative outside the expression to do this? I need the $x^2$ term by itself, don't I? Any help would be greatly appreciated!","['integration', 'trigonometry', 'calculus']"
2829913,Realizable automorphism groups of graphs,Frucht's theorem states that every group can be realized as an automorphism group of a graph.  I am interested in which subgroups of $S_n$ can be realized as an automorphism group of a graph on $n$ vertices.  Are there any things we can say about these groups or their actions which don't tautologically follow from the definition of automorphism groups of graphs?,"['graph-theory', 'algebraic-graph-theory', 'group-theory', 'automorphism-group']"
2829928,"Prove that $n^{-1} \max\left(X_1,\cdots,X_n\right) \to 0$ in probability","Let $X_1,\cdots,X_n$ be sequence of positive, iid random variables such that $\mathbb{E} X_1 <\infty$. How can I show that $$\frac{1}{n}\max\left(X_1,X_2,\cdots,X_n\right)\to 0 \text{ in probability}$$ I can prove it assuming $\mathbb{E}X_1^2 < \infty$, but I don't know how to prove it with the given assumption. Note : While almost surely convergence implies convergence in probability, I am looking for a solution that does not take that route, and does not use Borel-Cantelli lemma.","['probability-theory', 'probability']"
2829976,Proving $\sum_{k=0}^{n-1} \text{cos}\left(\frac{2\pi k}{n} \right) = \sum_{k=0}^{n-1} \text{sin}\left(\frac{2\pi k}{n} \right) = 0$ [duplicate],"This question already has answers here : Proving $\sum_{x=0}^{n-1} \cos\left(k +x{2\pi\over n}\right) =\sum_{x=0}^{n-1} \sin\left(k +x{2\pi\over n}\right) =0. $ (3 answers) Closed 6 years ago . I would like to prove the following, $$\sum_{k=0}^{n-1} \text{cos}\left(\frac{2\pi k}{n} \right) = \sum_{k=0}^{n-1} \text{sin}\left(\frac{2\pi k}{n} \right) = 0$$ This is equivalent to showing that if we have a regular n-gon, all of the vectors in $\mathbb{R}^2$ pointing to every vertex cancel eachother out to $\textbf{0}$. It seems obvious enough and i've tried a number of examples for small $n$.  I am not sure how to start a proof though.","['circles', 'trigonometry', 'geometry', 'summation', 'vectors']"
2829981,Pointwise conformal vs. conformally diffeomorphic metrics in dimension 2,"Let $g$ be any Riemannian metric on the 2-sphere $S^2$ and let $g_0$ be the round metric (of constant curvature $K$, say). Is it true that there exists a smooth positive function $\lambda:S^2\to \mathbb{R}$ such that $g_0=\lambda^2g$? The version of the uniformization theorem that I am familiar with implies that there exists a diffeomorphism $f:S^2\to S^2$ and a smooth function $u:S^2\to \mathbb{R}$ such that the pullback metric $f^{\ast}g$ is conformal to the round metric, i.e. such that $g_0=u^2(f^{\ast}g)$. So my question is equivalent to the following question: can the diffeomorphism $f$ be taken to be the identity? The reason I think this might be true is that a 2-dimensional manifold has constant curvature iff it has constant scalar curvature. For $n\geq 3$, any metric on a closed $n$-manifold is pointwise conformal to a metric with constant scalar curvature (by the solution of the Yamabe problem). Does this also hold for $n=2$ and/or specifically $S^2$?","['riemannian-geometry', 'differential-geometry']"
2829988,What is the relative maximum or minimum and the point of inflection?,"$f(x) = ax^3+bx^2+cx +d$, determine a, b, c, and d such that the graph of $f$ has a extreme in $(0,3)$ and a point of inflection in $(-1,1)$. When is a quadratic I know that the formula $V=(\frac{-b}{2a},\frac{-\triangle}{4a})$ gives the maximum or minimum of the function, but for $f(x) = ax^3+bx^2+cx +d$, not seems the same. I tried to make $f(x) = x(ax^2+bx+c) +d$ but doesn't work and for the point of inflection  I know that is the point of the second derivative is equal zero and change of the signal, but how I can make the function change in (-1,1)?",['calculus']
2829990,"Limit: $ \lim_{n \to \infty} \int_{(0,1)^n} \frac{n}{x_1 + \cdots + x_n} dx_1 \cdots dx_n $","I want to calcurate $$ \lim_{n \to \infty} \int_{(0,1)^n} \frac{n}{x_1 + \cdots + x_n} \, dx_1 \cdots dx_n $$ I met this in studying Lebesgue integral. But, I don't know how to do at all. I would really appreciate if you could help me! [Add] Thanks to everybody who gave me comments, I can understand the following, \begin{align*}
\lim_{n \to \infty} \int_{(0,1)^n} \frac{n}{x_1 + \cdots + x_n} dx_1 \cdots dx_n
&=\lim_{n \to \infty} n\int_0^\infty \bigg(\frac{1-e^{-t}}{t}\bigg)^n\,dt 
\end{align*} and \begin{align*}
	\int_0^\infty \bigg(\frac{1-e^{-t}}{t}\bigg)^n\,dt 
  &=\frac{n}{(n-1)!}\sum_{i=0}^{n-1}{ n-1 \choose i} (-1)^{n-1-i} (i+1)^{n-2}\log(i+1)
\end{align*} and \begin{align*}
\int_0^\infty \bigg(\frac{1-e^{-t}}{t}\bigg)^n\,dt
&=\int_0^\infty \frac{z^{n-1}}{(n-1)!}\, \mathrm{Beta}(z,n+1)\,dz\\
&=n\,\int_0^\infty \frac{z^{n-1}}{z(z+1)\cdots(z+n)}\,dz
\end{align*} But,I can't calcurate these integral and the limit. Please let me know if you find out.","['real-analysis', 'integration', 'calculus']"
2830029,Show that $p_n^{1-\epsilon}\le n$ using PNT,"Assuming PNT $$\pi(x)\sim \frac{x}{\log{x}}$$
How can we show that given any $\epsilon>0$ $$p_n^{1-\epsilon}< n,$$ for all sufficiently large $n$ ($p_n$ denotes the $n^{th}$ prime.) My work:
Setting $x=p_n$ we get $\lim\limits_{n\to\infty}\dfrac{n\log{p_n}}{p_n}=1\Rightarrow 1-\epsilon<\dfrac{n\log{p_n}}{p_n}$, for sufficiently large $n$. Now if I can show that $$\dfrac{n\log{p_n}}{p_n}\le\dfrac{\log n}{\log{p_n}}$$
then the result follows. But I am unable to show the last inequality. Can someone please help with this? Other approaches are welcome as well.
[NOTE: Using this I want to show that $\lim\limits_{n\to\infty}\frac{\log{n}}{\log{p_n}}=1$ so please don't use that, although it can be derived independently] Thank you","['number-theory', 'analytic-number-theory', 'prime-numbers']"
2830033,Why $A$ invertible $\iff \det A\neq 0$,"Let $A$ a matrix $n\times n$ over $\mathbb R$. I'm trying to prove that $A$ is invertible $\iff\det A\neq 0$. If $A$ is invertible, there is $B$ s.t. $AB=I$, and thus $$\det(A)\det(B)=\det(AB)=\det(I)=1,$$
and thus $\det A\neq 0$. I have problem to prove the converse. Let $A$ s.t. $\det(A)\neq 0$. I would like to say that $$\det(A)\det(A)^{-1}=1.$$
I know that if $A^{-1}$ exist then $\det(A)^{-1}=\det(A^{-1})$, but since I have to prove that $A^{-1}$ exist, I can't use this formula... so how can I conclude ?",['linear-algebra']
2830066,Limits applications in geometric problem,"We have the following situation The goal is to find $$\lim_{a \to b } \frac{a-b}{c-d} $$ Thought As $a $ tends to $b$ then we see we are gonna have a rectangle which means that $2 \alpha $ is gonna tend to $ \frac{\pi}{2}$ . In other words, $\alpha $ is gonna tend to $\frac{ \pi }{4} $ . Now notice $$ a = \frac{ x }{ \tan \alpha }, b = \frac{ y }{ \tan \alpha} $$ and $$ d = \frac{ y}{\cos \alpha}, c = \frac{x}{\sin \alpha } $$ now $$ \lim_{a-b } \frac{ a-b}{c-d} = \lim_{\alpha \to \frac{\pi}{4} } = \frac{\frac{ x }{ \tan \alpha }-  \frac{ y }{ \tan \alpha}}{\frac{ y}{\cos \alpha}- \frac{x}{\sin \alpha }} = \frac{x-y}{\frac{2}{\sqrt{2}}x - \frac{2}{\sqrt{2}} y } = \frac{ \sqrt{2} }{2}  $$ Is this a correct solution? My","['limits', 'calculus', 'geometry']"
2830074,Confusion About Partial Fractions In Pre-Calculus,"I'm confused about partial fractions. Let's take the partial fraction $$\frac{8x+4}{x^2-4}.$$ We can break it up into $$\frac{8x+4}{x^2-4} = \frac{a}{x+2} + \frac{b}{x-2}.$$ Once we multiply both sides by $(x+2)$ and $(x-2)$, we get $8x+4=a(x-2)+b(x+2)$. My confusion comes on the two steps after all this. We first assume that $x=2$, making $a(x-2)=0$. Then, we solve for $b$. After that, we assume $x=-2$, making $b(x-2)=0$ and solve for $a$. My question is, how can we just assume that $x=2$, or -2, or whatever that makes the roots =0? Why would it be logical to make such an assumption here? EDIT: I think my confusion comes from 2 parts in particular. First, how do we know that $8x+4=a(x-2)+b(x+2)$ applies for all real values of x? What about the way this equation is written ensures that to be true? Second, how do we know that the values of a and b won't fluctuate, as we sub in different x-values? How do we know that a and b will be constant no matter what x-value we sub in? If we can know both, I think I can be convinced that the step above is legitimate. Thanks in advance!","['algebra-precalculus', 'self-learning']"
2830086,Proving Wald's identity for Brownian motion,"I'm trying to prove that for a Brownian motion $\big(B_t, \mathcal{F}_t \big)_{t\geq 0}$ and a stopping time $\tau$ satifying $\mathbb{E}[\tau]<\infty$, we have that $\mathbb{E}[B_\tau^2]=\mathbb{E}[\tau]$. I know that $U_t=W_t^2-t$ is a martingale, and It's enough for me to show that $\mathbb{E} \big[ U_\tau\big]=0 $. I define a sequence $U_{\tau \wedge n}$, and because it's a martingale I know by Doob's O.S.T that $\mathbb{E}[U_{\tau \wedge n}]=0$. Since $U_{\tau \wedge n}\rightarrow U_\tau$ almost surely, if I find a dominating function for  $U_{\tau \wedge n}$, I will obtain that: $0\equiv\mathbb{E}[U_{\tau \wedge n}]\rightarrow \mathbb{E}[U_\tau]$, which solves the problem. However I've been having problems finding a dominating function for $U_{\tau \wedge n}$. I do know that: $$\vert U_{\tau \wedge n}\vert \leq B_{\tau \wedge n}^2+\tau= B_\tau^2\cdot 1_{ \{ \tau \leq n\}}+ B_n^2\cdot 1_{ \{ \tau  > n\}}+\tau \leq B_\tau^2+ B_n^2\cdot 1_{ \{ \tau  > n\}}+ \tau$$ By Fatou's lemma I know that $B_\tau^2\in L^1$. Hence I already bounded the first and last term in $L^1$, and I just need to find an $L^1$ bound on $B_n^2\cdot 1_{ \{ \tau  > n\}}$ to conclude. I would very much appreciate any hint, as it seems to me as there is a little thing I am missing.","['stochastic-processes', 'probability-theory', 'stopping-times', 'martingales', 'brownian-motion']"
2830087,"pdf from mean, mode, median","I have been given the mean, median and mode of a function and have to find the probability density function. mean: $\gamma - \beta\Gamma_1$ median: $\gamma-\beta(ln2)^{1/\delta}$ mode: $\gamma-\beta(1-1/\delta)^{1/\delta}$ I am also given that 
$$\Gamma_k=\Gamma(1+k/\delta)$$
$$\Gamma(z)=\int_0^\infty t^{z-1}dt$$
$$ -\infty<x<\gamma, \beta>0, \gamma>0 $$ Now I understand  how to calculate the mean, mode and median when given a probability density function. However I'm struggling to go backwards. I initially tried to ""reverse"" the process by differentiating the mean or median however I know this is skipping the substitution over the given limit. I then looked for patterns with known distributions and realised they are from Weibull distribution however $\gamma-$. Does this mean essentially this is a typical Weibull distribution however shifted by $\gamma$ and therefore the pdf will be $\gamma-Weibull pdf""$","['means', 'statistics', 'density-function']"
2830093,Give an example where $\int_{\bar A} f$ exists but $\int_A f$ does not for a continuos $f$ on a bounded open subset $A$ of $\mathbb{R}^n $,"In the book of Analysis on Manifolds by Munkress, at page 121, it is asked that Let $A$ be a bounded open set in $\mathbb{R}^n $; let $f: \mathbb{R}^n
> \to \mathbb{R} $ be a bounded continuous function. Give an example
  where $\int_{\bar A} f$ exists but $\int_A f$ does not. I couldn't find any such example. I mean I do noticed that $A$ should not be rectifiable, and the only way I can think of that $f$ is not integrable on $A$ is that the set $E$ where the limit 
$$\lim_{x \to x_0} f(x) = 0$$ fails for $x_0 \in \partial A$ has a non-zero measure, but then this set $E$ also contained in $\bar A$, so in such a case $\int_{\bar A} f$ also wouldn't exists. In short, I'm looking for such an example. Note: I have read this answer, but I have no idea what the answerer is talking about, so I wanted to ask a new question including my own thoughts.","['real-analysis', 'integration', 'analysis', 'riemann-integration']"
2830101,"How many solutions of $x$ are available of the equation $3\tan x+x^3 =2 $ in range of $[0,\pi/4]$?","$3\tan(x)+x^3 =2$ in range of $[0,\frac{\pi}{4}$]? I mean how to start with this. I'm not given the calculator to put distinct values. So what's the key idea to solve it? Should I put $\tan(x) = x$ for smaller values of $x$? Or anything else?",['trigonometry']
2830117,norm of $AUx$ for unitary $U$,"Let $U$ be a unitary matrix. Does it mean that for every invertible matrix $A$, $$\Vert AUx\Vert \le \Vert Ax\Vert$$ for every vector $x$?","['matrices', 'normed-spaces', 'linear-algebra']"
2830125,Intersection of countable and uncountable sets,"I came across following problem: If $A$ is a countable set, and $B$ is an uncountable set, then the most we can say about $A\cap B$ is that it is Empty. Finite, countable, or uncountable. Countable. Uncountable. Countable or uncountable. Finite. At most countable. I understand it has to be countable, that is option 3. But the answer given was option 7 - ""at most countable"". I am confused what it means to say by prefix ""at most"". The explanation given was ""The intersection of A and B could be smaller than countable."" What does it means ""smaller than countable""? Empty and/or finite? Or is there something more to it?",['elementary-set-theory']
2830135,rank of a rectangular Vandermonde matrix,"Let the $m\times (n+1)$ rectangular Vandermonde matrix be $V$. More specifically, the matrix $V$ has the following form. $V=\begin{pmatrix}
1 & a_1 & \cdots & a_1^{n} \\ 
1 & a_2 & \cdots & a_2^{n}\\ 
\vdots& \vdots & \ddots  &\vdots \\ 
1 & a_m & \cdots & a_m^{n} 
\end{pmatrix}$
, where $m \geq n$. I want a proof that $ \operatorname{rank}(V)=n$, if and only if the $a_i$ take exactly $n$ different values. Can you recommend me any paper or book that has a formal proof?","['matrices', 'geometric-progressions', 'matrix-rank', 'linear-algebra']"
2830145,On the verification of an equality of sets,"Only one thing is not clear to me. Proposition. Let $\{f_n\}_{n\in\mathbb {N}}$ a sequence of integrable function no-negative such that $f_{n}\le f_{n+1}$ if
  \begin{equation}
\lim_{n\to\infty}\int_Xf_n\;d\lambda=0,
\end{equation}
  then 
  \begin{equation}
\lim_{n\to\infty}f_n<\infty\quad\text{a.e.}
\end{equation} proof. \begin{equation}
0=\lim_{n\to\infty}\int_Xf_n\;d\lambda=\sup_n\int_Xf_n \;d\lambda,
\end{equation}
then
\begin{equation}
\int_Xf_n\;d\lambda=0\quad\text{for all $n\in\mathbb{N}$},
\end{equation}
therefore $f_n=0$ a.e.for all $n\in\mathbb{N}$. Now
\begin{equation}
\lambda(\{x:f_n(x)>0\})=0.
\end{equation}
We observe that
\begin{equation}
\{x:\lim_nf_n(x)>0\}=\cup_n\{x:f_n(x)>0\}\quad(1)
\end{equation}
then
\begin{equation}
\lambda(\{x:\lim_nf_n(x)>0\})=\lambda(\cup_n\{x:f_n(x)>0\})=\lim_n\lambda(\{x:f_n(x)>0\})=0.
\end{equation}
Is it correct to proceed in this way for 1? If $x\in\cup_n\{x:f_n(x)>0\}$ $\Rightarrow$ $\exists n_0\in\mathbb{N}$ such that $f_{n_0}(x)>0$ $\Rightarrow$  $\sup_n\{f_n(x)>0\}$ $\Rightarrow$ $\lim_n f_n(x)=\sup_n\{f_n(x)\}>0$. Then
\begin{equation}
\bigcup_n\{x:f_n(x)>0\}\subseteq \{x:\lim_nf_n(x)>0\}.
\end{equation}
If $x\in\{x:\lim_nf_n(x)>0\}$ $\Rightarrow$ $\sup_n\{f_n(x)\}>0$ $\Rightarrow$ $\exists n_0$ such that $f_{n_0}(x)>0$ $\Rightarrow$ $x\in\cup_n\{x:f_n(x)>0\}$. Thanks!","['measure-theory', 'proof-explanation']"
2830167,Algebra and matrices.,"Q. Solve for $x$, $y$ and $z$ if $$(x+y)(x+z)=15, $$
$$(y+z)(y+x)=18, $$
$$(z+x)(z+y)=30. $$ Solution:
I expanded each equation above as : $$x^2+xz+yx+yz=15, \tag{1}$$
$$y^2+xz+yx+yz=18, \tag{2}$$
$$z^2+xz+yx+yz=30. \tag{3}$$ Then I subtracted $(1)-(3)$, $(2)-(1)$ and $(3)-(2)$; so I got the equations as below:
$$x^2-z^2=-15 \tag{4}$$
$$y^2-x^2=3 \tag{5}$$
$$z^2-y^2=12 \tag{6}$$ I then tried solving $(4)$,$(5)$,$(6)$ by using matrices, but I couldn't reach any solution. Please advise. Thank You.","['matrices', 'matrix-equations', 'algebra-precalculus']"
2830187,Finite summation of series involving binomial coefficients,"Considering the following summation of series:
$$S_n=\sum_{k=0}^{n}(-1)^k{{n}\choose{k}}\sum_{m=0}^{k}(-1)^m\frac{k!}{(k-m)!}b^{-m},$$
where $n$ is a non-negative integer, and $b$ is a known non-zero constant. I computed manually and got
$$S_1=b^{-1}, S_2=2b^{-2}, S_3=6b^{-3}.$$
Then I set a hypothesis of $S_n$:
$$S_n=n!\cdot b^{-n}.$$
However, I couldn't prove whether it is correct or not.
Could someone help me? Thanks very much indeed!","['summation', 'binomial-coefficients', 'sequences-and-series']"
2830194,Example of an ideal $I$ that is maximal in $R$ but not maximal in $R[x].$,"I was reading Dummit and Foote and the following remark showed up: Note that it is not true that if $I$ is a maximal ideal of $R$ then $I[x]$ is a maximal ideal of $R[x]$. If $I$ is maximal in $R$ then $R/I$ is a field and this means that $R[x]/I[x]$ is a field since $R[x]/I[x]\cong (R/I)[x].$ Beyond this, I am not sure how I can come up with an example of this claim.","['abstract-algebra', 'polynomials']"
2830221,Convergence of conditional expectations given a sequence of random variables,"I am trying to prove the following statemt: Let $(\Omega, \mathcal A, P)$ be a probability space. Let $(Z_n)_{n\in \mathbb N}$ be i.i.d. random variables with $Z_1 \in \mathcal L^1$. Let $\theta \in \mathcal L^1$ be independent from $(Z_n)_{n\in \mathbb N}$ and define $\forall n\in \mathbb N: Y_n := Z_n + \theta$. Then
  $$\mathbb E[\theta \mid Y_1, Y_2, \dots, Y_n] \longrightarrow \theta$$
  $P$-a.s. as $n\to \infty$. I was trying to apply LÃ©vy's martingale convergence theorem: $$\mathbb E[\theta \mid Y_1, Y_2, \dots, Y_n] \longrightarrow \mathbb E[\theta \mid \mathcal F_{\infty}]$$
where $\mathcal F_\infty $ is the smallest $\sigma$-algebra generated by all $Y_n$. Now it remains to show $\theta = \mathbb E[\theta \mid \mathcal F_\infty]$, but here is where I am stuck. Is this the right direction I am going?","['expectation', 'probability-theory', 'probability', 'martingales', 'convergence-divergence']"
2830224,Maximum the amount of coins the king can get,"Here is how it works: The king has $65$ citizens. Counting himself, each of them gets a coin every month. The king can give a suggestion of how the coins should be distributed, and evryone else cast a vote. It will pass if it has more than $50$% voted ""yes"". Every citizen is extremely selfish and does the following: if he/she gets more coin, then he/she will vote ""yes"". If he/she gets less coin, then he/she will vote ""no"". Otherwise, he/she won't vote at all. Question: What's the maximum amount of coin (out of 66) the king can get, and how is it achieved in minimal amount of suggestions? Apprently the answer seem to be 63, and it's pretty obvious, but how can one formally prove it? Also, how can I find the answer to the second question? Edit: here is an example that might help you to understand the question: Suggestion $1$: $33$ citizens gets 2 coins each ($33$ ""yes"" $32$ ""no"") Suggestion $2$: $17$ citizens gets 3/4 coins each ($17$ ""yes"" $16$ ""no"") Suggestion $3$: $9$ citizens gets 6/7 coins each ($9$ ""yes"" $8$ ""no"") Suggestion $4$: $5$ citizens gets 12/13 coins each ($5$ ""yes"" $4$ ""no"") Suggestion $5$: $3$ citizens gets 22 coins each ($3$ ""yes"" $2$ ""no"") Suggestion $6$: $2$ citizens gets 33 coins each ($2$ ""yes"" $1$ ""no"") Suggestion $7$: $3$ citizens didn't get any coin the first round gets one coin each. The king gets the rest of 63 coins. ($3$ ""yes"" $2$ ""no"")",['combinatorics']
2830245,Is this operator pseudodifferential or trace-class?,"Let $M$ be a closed manifold and $D$ a Dirac operator on $M$. Then (the closure of) $D^2+1$ has a bounded inverse 
$$(D^2+1)^{-1}:L^2\rightarrow H^2,$$ 
where $H^2$ is the second Sobolev space. In fact, $(D^2+1)^{-1}$ is a compact operator on $L^2$ by Rellich's lemma. Question 1: Is $(D^2+1)^{-1}$ a properly supported pseudodifferential operator if $M$ is non-compact? Question 2: Is $(D^2+1)^{-1}$ a trace-class operator on $L^2$ (when $M$ is compact)? Thoughts: I am not sure about the first question, but I believe the answer to the second question is no (although certainly one can raise $(D^2+1)^{-1}$ to a high enough power to get a trace-class operator), although I don't know how to show explicitly that it's not trace-class. Thanks!","['spin-geometry', 'operator-theory', 'functional-analysis', 'pseudo-differential-operators', 'differential-geometry']"
2830247,Is there any theorem on uniqueness of integrating factor for inexact ordinary differential equations?,"Do we have any theorem regarding  uniqueness of integrating factor for inexact ordinary differential equations? Anything like: If $f(v)$ and $g(v)$ are functions of $v$ and integrating factors for the given inexact ODE, then $f(v)=g(v)$",['ordinary-differential-equations']
2830249,How do I formally answer questions about maximal domain? [Verification],"I have the following task: ""State the maximal domain $D \subseteq \mathbb{R}^2$ of the following functions $f: D \rightarrow \mathbb{R}$ $\color{grey}{\textrm{and draw D in each case.}}$"" (I'll do this bit). $f(x,y) = \dfrac{xy}{x-y}$ $f(x,y) = x + \sqrt{x^2 + y^2}$ $f(x,y) = \dfrac{\tan{(xy)}}{x^2 - y^2}$ Whilst I think that I understand what is being asked, I don't think I could articulate my solution with the appropriate rigour such that I could present it. Is my approach for determining the maximal domains valid? As such, have i accurately represented my maximal domains with the correct notation? Progress so far: Generally, I have tried to identify subdomains in my functions, and then wish to state the domain of each whole function as the intersection of those subdomains. Here I would argue that the maximal domain is the intersection of the numerator and denominator. Thus, 
\begin{align*}
D_1 &= \{x,y: x,y \in \mathbb{R}^2\} \ \cap \{ x,y: x,y \in  \mathbb{R}^2 \cap (x \neq y)\} \\
D_1 &=  \{ x,y: x,y \in  \mathbb{R}^2 \cap (x \neq y)\} & \text{(Rule missing.)} \\
\end{align*} Here I would argue that the maximal domain is the intersection of the lone $x$ term and the term contained within the root function. Thus, \begin{align*}
D_2 &= \{x: x \in \mathbb{R} \} \cap \{x,y: x^2 + y^2 \in \mathbb{R_{\geq 0}} \} 
 \hspace{6.1cm} \\
D_2 &= \{x,y: (x \in \mathbb{R}) \cap (x^2 + y^2 \in \mathbb{R_{\geq 0}}) \} \\
\end{align*} Analogue to 1, except that another term needs to be introduced for the repeating asymptotes of the tangent function. Thus, \begin{align*}
D_3 &= \{ x,y: x,y \neq n\cdot \frac{\pi}{2} \in \mathbb{R^2} \ (n \in \mathbb{N}) \} \ \cap \{ x,y: x^2 - y^2 \neq 0 \} \hspace{2.6cm} \\
D_3 &= \{ x,y: x,y \neq n\cdot \frac{\pi}{2} \in \mathbb{R^2} \ (n \in \mathbb{N}) 
 \cap (x^2 - y^2 \neq 0) \} \\
\end{align*}","['elementary-set-theory', 'functions']"
2830279,On epimorphisms of groups.,"I want to prove by absurdity that every epimorphism of groups is a surjective morphism. So let $f\colon A\to B$ be an epimorphism of groups. Let by absurd assume that $f$ is non surjective, so that the image $f(A)$ of $f$ is properly contained in $B$ and let us also assume that $f(A)$ is not a normal subgroup of $B$, because in that case we can form the quotient group of classes of elements of $B$ modulo $f(A)$ and use the same argument that is used for abelian groups. So, the index of $f(A)$ in $B$ must be at least $3$, otherwise $f(A)$ would be normal in $B$. Let $f(A)$, $f(A)u$ and $f(A)v$ be three distinct cosets of $f(A)$ in $B$. Then, we consider the permutation $\sigma$ on $B$, defined by: $\sigma(xu)=xv$ for all $x\in f(A)$; $\sigma(xv)=xu$ for all $x\in f(A)$ and $\sigma(b)=b$ on the remaining part of $B$. Then, for all $x,b\in B$, let us define $\psi_b(x)=bx$ and $\overline{\psi}_b=\sigma^{-1}\circ\psi_b\circ\sigma$. Let us finally define $\psi,\overline{\psi}\colon B\to\operatorname{Sym}_B$ by $\psi(b)=\psi_b$ and $\overline{\psi}(b)=\overline{\psi}_b$. Since $\psi\circ f=\overline{\psi}\circ f$, then $\psi=\overline{\psi}$, because $f$ is assumed to be an epimorphism. So I need to prove that $\psi\neq\overline{\psi}$ to get a contradiction, but I can't. Can you please help me to show that actually $\psi\neq\overline{\psi}$.","['category-theory', 'abstract-algebra', 'group-theory']"
2830304,$CLT$ and $LLN$ give different results,"I tried to solve a problem two different ways and I got different results. Let $( X_i )_{i \in \mathbb{N}}$ be a series of independent, identically distributed random variables, with $\mathbb{E}[X_i] = 1$ and $\mathbb{V}[X_i] = 1$ Determine $$
\lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \leq \sqrt{n}\right)
$$ Here are the two approaches that I tried. Central limit theorem \begin{align*}
& \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \leq \sqrt{n}\right) \\
= {} & \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n (X_i - 1) \leq \sqrt{n} - \frac{n}{\sqrt{n}}\right) \\
= {} & \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n (X_i - 1) \leq 0\right) \\
= {} & \Phi_{0,1}(0) = \frac{1}{2}
\end{align*} Law of large numbers \begin{align*}
& \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \leq \sqrt{n}\right) \\
= {} & \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^n X_i \leq 1 \right) \\
\geq {} & \lim_{n \to \infty} \mathbb{P}\left(\frac{1}{n} \sum_{i=1}^n X_i = 1\right) \\
= {} & 1
\end{align*}
according to the strong law of large numbers. This then means that
$$
\lim_{n \to \infty} \mathbb{P}\left(\frac{1}{\sqrt{n}} \sum_{i=1}^n X_i \leq \sqrt{n}\right) = 1
$$ What am I doing wrong here? My understanding is that the CLT solution is correct, but I don't see what I did wrong with applying the law of large numbers either.","['probability-limit-theorems', 'probability', 'proof-verification']"
2830326,Is it legal to put $\tan$ on both sides of the equation?,"I have this equation and I need to find $x$ variable: $$1+\frac{\pi}{4}-x=\arctan x$$ can I put $\tan$ on RHS and LHS in order to find $x$: $$\tan( 1+\frac{\pi}{4}-x)=\tan(\arctan x)$$ Secondly, after I have $x$ on RHS, how am I supposed to find $x$ if the LHS has now become a complicated expression?","['trigonometry', 'calculus']"
2830333,"LovÃ¡sz local lemma, approximate weights","In the constructive form of the LLL we bound the expected time using a weight function $x:\mathcal{A}\rightarrow (0,1)$ that satisfies $\forall A\in \mathcal{A},\Pr [A] \leq x(A) \prod _{B\in \Gamma^+(A)}(1-x(B))$ and the running time  bound is given by 
$O(\sum_{A\in \mathcal{A}}\frac{x(A)}{1-x(A)})$
expected time To my understanding there is no nice way to generally compute $x$ or decided it doesn't exist. I'm interested in finding an approximation of $x$ . I.E a function $x:\mathcal{A}\rightarrow (0,1)$ that satisfies $\forall A\in \mathcal{A},\pmb{(1-\varepsilon )}\Pr [A] \leq x(A) \prod _{B\in \Gamma^+(A)}(1-x(B))$ for any $0.5>\varepsilon>0 $ .
running time should be polynomial in $1/\varepsilon,|\mathcal A|\;\;$and$\;\;\log (1/\min _A(\Pr[A]))$ Is such solution possible, And if so does anyone know of a similar question","['algorithms', 'probability', 'stochastic-approximation', 'discrete-mathematics']"
2830350,"Hartshorne Page 307, Proposition 3.1","I was reading the proof of the Proposition (3.1, IV) in Hartshorne 's Algebraic Geometry book. Proposition: Let $D$ be a divisor on a curve $X$ . Then: (a) the complete linear system $\vert D \vert$ has no base points if and only if for every point $P\in X$ , $$ \dim \vert D-P \vert = \dim \vert D \vert - 1;$$ (b) $D$ is very ample if and only if for every points $P, Q\in X$ (including the case $P=Q$ ), $$ \dim \vert D-P-Q \vert = \dim \vert D \vert - 2.$$ In the proof, I could not understand the line in the proof which says that if $D$ satisfies the condition (b), then we have $$ \dim \vert D-P \vert = \dim \vert D \vert - 1$$ for every $P\in X.$ (It may be silly question.) I would be thankful if someone could elaborate this sentence.","['algebraic-curves', 'algebraic-geometry']"
2830355,A characterization for idempotent lifting property,"Let $I$ be an ideal in a commutative ring $R$ with $1$ and let $g+I$ be an idempotent element of $R/I$. We say that this idempotent can be lifted  modulo $I$ in case there is an idempotent $e^2=e\in R$ such that $g + I = e + I$ ($g-e\in I$). Question: Is there any characterization for $I$ under which idempotents  lift modulo $I$, that is, every idempotent of $R/I$ lifts modulo $I$? And is there any comprehensive reference for this property?","['algebraic-geometry', 'abstract-algebra', 'ring-theory', 'commutative-algebra', 'idempotents']"
2830368,Minimize an integral vs Minimize the argument of a integral.,"I have some trouble understanding this apparently simple concept.
Let's say we have to do a minimization of an integral and I need to find L* 
$$ L^{*} = \min_{z(x)} \int f(x,z(x)) dx$$ Can I bring the min inside and thus write: $$ L^{*} = \int \min_{z(x)}  f(x,z(x)) dx$$ If so, why? 
Intuitively make sense, but I can't really see the mathematical reason behind.","['integration', 'optimization']"
2830373,Notion of derivative used in Petersen & Pedersen's Matrix Cookbook,"I am looking at the Matrix Cookbook . From my real analysis background, my understanding of calculating derivatives involving matrices is to use the FrÃ©chet derivative on the normed space $(\mathbb{R}^{n \times n}, \|\cdot\|_{op})$ and whatever the target space is, but I am having a hard time linking this to what is used in this book. For example, consider the matrix trace $\text{Tr}: \mathbb{R}^{n \times n} \rightarrow \mathbb{R}$. This is a linear map so the Frechet derivative in direction ${\bf{V}} \in \mathbb{R}^{n \times n}$ is just the linear map itself independent of the point ${\bf{X}} \in \mathbb{R}^{n \times n}$ so $$\text{d}\text{Tr}({\bf{X}}){\bf{V}} = \text{Tr}({\bf{V}})$$ whereas in the Matrix Cookbook, the following identity is stated $$\displaystyle\frac{\partial}{\partial {\bf{X}}}\text{Tr}({\bf{X}}) = {\bf{I}}$$ Which I suppose has the same property of being independent of ${\bf{X}}$ but it's not the same. Another example is the function $f({\bf{X}}) = {\bf{X}}^{-1}$, which has Frechet derivative $\text{d}f({\bf{X}}){\bf{V}} = -{\bf{X}}^{-1}{\bf{V}}{\bf{X}}^{-1}$, MC states an extremely similar looking identity:
$$\frac{\partial{\bf{X}}^{-1}}{\partial x} = -{\bf{X}}^{-1}\frac{\partial{\bf{X}}}{\partial x}{\bf{X}}^{-1}$$ My question is, what is the definition of $\displaystyle\frac{\partial}{\partial {\bf{X}}}$, $\partial{\bf{X}}$ and exotic expressions such as $\partial{\lambda_i}$ $\partial{\bf{v}}_i$ (where $\lambda_i, {\bf{v}}_i$ are the eigenvalues and vectors of a real symmetric matrix). I am also curious if there a nice geometric interpretation or analogue to derivatives in Banach spaces and why might these specialised derivatives be preferred over a Frechet derivative in applications. I have found a similar question with some answers here but I did not find these particularly enlightening, any insightful answers are much appreciated.","['derivatives', 'matrix-calculus', 'soft-question', 'linear-algebra', 'frechet-derivative']"
2830377,How is convolving Haar measure with itself over subsets enough to define subgroups?,"In this post , Terry Tao says that Gaussians are ""a subgroup of $\Bbb R$"". When you convolve Haar probability measure on a (compact) subgroup with itself, you get back the same measure, and this can in fact be used as a definition of such subgroups. If you convolve a Gaussian probability measure with itself, you almost get back the same Gaussian measure, but it has spread out by a factor of $\sqrt 2$. So Gaussians are in some sense a ""$\sqrt 2$-approximate group"". I don't understand this comment, or how it means that Gaussians can be viewed as a subgroup of $\Bbb R$. How can you define subgroups through this? Perhaps this? A subset, $H$ of a group $G$ with Haar measure $\mu$ is called a subgroup if $\mu |_H=\mu\ast\mu|_H$. How does this show that a Gaussian is a subgroup of $\Bbb R$?","['probability', 'group-theory', 'haar-measure']"
