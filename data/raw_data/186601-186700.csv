question_id,title,body,tags
3459910,"Find the maximum of the $f=\sum_{i=1}^{n}\left(\binom{a_{i}}{2}\cdot\sum_{j<k,j,k\neq i}a_{j}a_{k}\right)$","Let $a_{1},a_{2},\ldots,a_{n}$ be integers, such that $a_{i}\ge 0$ for $i=1,2,\cdots,n$ , and such that $\sum_{i=1}^{n}a_{i}=120$ . Find the maximum value of $$F=\sum_{i=1}^{n}\left(\binom{a_{i}}{2}\cdot\sum_{j<k,j,k\neq i}a_{j}a_{k}\right).$$ I tried the following: when $n\mid120$ , taking all of the $a_i$ to be equal, then $$F=n\binom{120/n}{2}\binom{n-1}{2}\left(\dfrac{120}{n}\right)^2=\dfrac{120^3(120-n)(n-1)(n-2)}{4n^3}.$$ If $n\le 6$ , then it's clear that $n=5$ gives the maximum value, which turns out to be $4769280$ . But I can't prove this is actually the maximum value.","['inequality', 'combinatorics']"
3459926,Precalc factoring $x^2+10x+25-9y^2$,Factor $$x^2+10x+25-9y^2$$ The solution is $$(x+5-3y)(x+5+3y)$$ I understand how to factor when there is only one variable $x$ but I am not sure how to complete this problem with the additional variable $y$ .,"['real-analysis', 'calculus', 'factoring', 'polynomials', 'algebra-precalculus']"
3460041,Almost sure convergence to mean with a logarithm,Let $(X_n)$ be i.i.d. random variables with mean $\mu$ . Show that $$\lim _{n \rightarrow \infty} \frac{1}{\log n} \sum_{i=1}^{n} \frac{X_{i}}{i}=\mu$$ almost surely. I am thinking of letting $T_n=\frac{1}{\log n} \sum_{i=1}^{n} \frac{X_{i}}{i}$ then applying Markov's inequality to get $$\Pr(|T_n - \mu|>\epsilon) \leq \frac{\mathbb{E}|T_n-\mu|}{\epsilon^2}$$ and somehow getting a summable sequence to apply the Borel-Cantelli Lemmas. But I don't really know how.,"['measure-theory', 'probability-theory']"
3460127,Tower game strategy,"We have 4 towers of cubes, each tower has one or more cubes (at least one). Two players play against each other. Each player in turn removes one cube from one of the towers, or two cubes from two towers - one from each tower of his choice. The game ends when the height of all towers is 0 (no more cubes). Wins the player who makes the last move. What strategy for winning the game could be developed?","['game-theory', 'combinatorics', 'combinatorial-game-theory']"
3460251,Unramified prime,"Let $f \in \mathbb{Z}[X]$ be a monic irreducible polynomial, n its degree, $\alpha$ a zero of $f$ in some extension field of $\mathbb{Q}$ , and $p$ a prime number not dividing the discriminant $\Delta(f)$ of $f$ . Denote by $t$ the number of prime ideals $\mathfrak{p}$ of $\mathbb{Z}[\alpha]$ with $p \in \mathfrak{p}$ . Prove that $\left( \dfrac{\Delta(f)}{p} \right) = (-1)^{n-t}$ . It is a generalization of a result in quadratic number field Proposition: Let $d \neq 1$ be squarefree and $p$ an odd prime. Then $p$ is split in $\mathbb{Z}[\sqrt{d}]$ for $\left(\dfrac{d}{p}\right)=1$ , inert for $\left(\dfrac{d}{p}\right)=-1$ and ramified for $\left(\dfrac{d}{p}\right)=0$ . The proposition can be deduced from Kummer-Dedekind theorem and explicit description of $\mathcal{O}_{\mathbb{Z}[\sqrt{d}]}$ that $\mathcal{O}_{\mathbb{Z}[\sqrt{d}]}=\mathbb{Z}[\sqrt{d}]$ for $d \equiv 2,3 \;(mod \;4)$ and $\mathcal{O}_{\mathbb{Z}[\sqrt{d}]}=\mathbb{Z}\left[\dfrac{1+\sqrt{d}}{2} \right]$ for $d \equiv 1 \; (mod \;4)$ . However I have stuck since we don't have explicit description of ring of integer of $\mathbb{Q}[\alpha]$ in general.","['number-theory', 'algebraic-number-theory']"
3460270,How to prove that $ -n \int _0 ^1 x^{n-1} \log(1-x)dx$ equals the $n$-th harmonic number?,"From (Almost) Impossible Integrals, Sums, and Series section 1.3: $$H_n = -n\int _0 ^1 x^{n-1} \log(1-x)dx$$ The proof of which was appetizingly difficult. I was unable to answer the follow-up challenge question, and do not have access to the given solution. It asks: Is it possible to [prove this equality] with high school knowledge only (supposing we know and use the notation of the harmonic numbers)? I've been fiddling with the integral on the right for an hour, but that $\log$ really throws a wrench in my plans.","['integration', 'harmonic-numbers', 'sequences-and-series']"
3460274,"Writing ""x is a perfect square or the square root of x is is irrational"" in predicate logic","I saw something similar that proved this statement here: Prove that the square root of a positive integer is either an integer or irrational I was wondering how to formally write the statement ""x is a perfect square or the square root of x is is irrational"" in predicate logic. My attempt is this: $\forall x\in \mathbb{Z^{\geq 0}}, (\forall a \in \mathbb{Z^{\geq 0}},\forall b \in \mathbb{Z^{\geq }}, \sqrt{x} \neq \frac{a}{b}) \lor (\exists d \in \mathbb{Z^{\geq 0}}, x = d^2)$","['predicate-logic', 'discrete-mathematics']"
3460294,Ito integral of sign function is a Brownian Motion,"Let $W(t)$ be a standard $\mathcal{F}_t$ -Brownian Motion. Let $$g(x)=\begin{cases}1 & \text{if } x\geq 0\\ -1 & \text{if }x<0 \end{cases}$$ Consider the process $B(t)=\int_0^tg(W(s))dW(s)$ . I want to show that $B(t)$ is also a Brownian Motion. My attempt: I can justify the start at the origin and a.s. $t$ -continuity as follows: Start at the origin: $B(0)=\int_{0}^0g(W(s))dW(s)=0$ . a.s. $t$ -continuity: as $B(t)$ is a stochastic integral, there exists a $t$ -continuous version of it, so $B(t)$ is almost surely continuous. Where I'm having a little bit of trouble is in showing the normality and independence of increments. Ideally, in my opinion, I would like to apply Ito's formula to get a relationship between $B(t)$ and $W(t)$ , or between $dB(t)$ and $dW(t)$ , so that by using the properties of $W(t)$ , I can say something about $B(t)$ , however, my issue is that $g$ is not twice-continuously differentiable (although it almost surely is). A way around this, to keep with the same strategy  would be to use the definition of the Ito integral as the limit of the integral for elementary functions, although seems like an unnecessary amount of work. Any hints or help is welcomed.","['stochastic-integrals', 'stochastic-processes', 'stochastic-analysis', 'probability-theory']"
3460311,Equivalent Formulations of the Open Mapping Theorem,"Different textbooks give different, but equivalent, formulations of the open mapping theorem, and I want to understand how all of these rigorously relate (and how they're all equivalent). I intuitively understand why they're equivalent, but want a deeper formal understanding. So, here are the various formulations: Rudin : Let $X,Y$ be Banach spaces, and $T \in \mathcal{L}(X,Y)$ (set of all bounded linear maps from $X$ to $Y$ ) be a surjective map. Then $T$ is an open map. Note that Rudin's formulation doesn't include the converse of this statement, although it can easily be checked. Royden : Let $X,Y$ be Banach Spaces and $T \in \mathcal{L}(X,Y)$ . Then $\text{Im}(T)$ is closed if and only if the operator $T$ is open. Brezis : Let $X,Y$ be Banach Spaces and $T \in \mathcal{L}(X,Y)$ . Then there is $r > 0$ such that $B_{r}^{Y}(0) \subset T(B^{X}_{1}(0)$ ), where $B^{E}_{r}(x)$ denotes the open ball of radius $r$ about $x$ in space $E$ . Intuitively, I can see how these formulations are equivalent. If the image if closed and the operator is open, the image is an open and closed space in $Y$ , which means that it's $Y$ itself (i.e. the map is surjective). Similarly, in the third formulation, if the image of unit ball contains a neighborhood of the origin, then by linearity all open sets will be mapped to open sets. I do, however, struggle with formalizing these ideas, and would appreciate if someone could (fairly rigorously) explain the equivalence of these notions.","['banach-spaces', 'functional-analysis']"
3460415,"Expected number of siblings, given the expectation of children in a family","The number of children in a family is a random variable $X$ , where $ \mathbb E(X) = 1.8 $ and $Var(X) = 0.36 $ . If we randomly choose a child, which is the expected number of its siblings? (it is given that the answer $> 0.8$ ) Let the number of a child's siblings be a random variable $Y$ , so we are looking for $ \mathbb E(Y) $ . If found that, $$
\mathbb E(Y) = \sum_{k=0}^{\infty}k \; \mathbb P(Y=k)
= \sum_{k=0}^{\infty}k \; \mathbb P(X=k+1) =
$$ $$
= \sum_{k=0}^{\infty}k \; \mathbb P(X=k)-\sum_{k=0}^{\infty}\mathbb P(X=k)
+\mathbb P(X=0)= \mathbb E(X)-1+\mathbb P(X=0)
$$ which is indeed greater than $0.8$ , if $\; \mathbb P(X=0)>0$ . But, I don't know how to make use of the variance to find the final answer. Thank you in advance","['variance', 'probability-distributions', 'expected-value', 'probability-theory', 'probability']"
3460434,Calculating $\int_0^1\frac{1}{1+x}\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)dx$,"How to prove in a simpe way that $$\int_0^1\frac{1}{1+x}\operatorname{Li}_2\left(\frac{2x}{1+x^2}\right)dx=\frac{13}{8}\ln2\zeta(2)-\frac{33}{32}\zeta(3)\ ?$$ where $\operatorname{Li}_2$ is the dilogarithm function. If we integrate by parts, the integral boils down to $$\int_0^1\left(\frac{1}{x}-\frac{2x}{1+x^2}\right)\left[2\ln(1-x)\ln(1+x)-\ln(1+x)\ln(1+x^2)\right]dx$$ and that seems long and complicated, any other ideas?","['integration', 'real-analysis', 'harmonic-numbers', 'calculus', 'polylogarithm']"
3460455,"Units of $\mathbb{Z}[\sqrt {-2}]$, and are $3,5$ irreducible in $\mathbb{Z}[\sqrt {-2}]$?","So for the first question I determined the units to be $+1$ and $-1$ . I assigned $x = a + b \sqrt{-2}$ and proceeded to show $b = 0$ so $a^2 = 1$ or $-1$ so $x = 1$ or $-1$ . For the second part I am not sure what to do. Looking at the norm function $N$ , we have $N(a + b\sqrt{-2}) = a^2 + 2b^2$ but I don't know how to go about with this information. I believe finding the norm would be important to determine the second part, right?","['ring-theory', 'abstract-algebra']"
3460473,"Deduce fundamental unit $\mathbb{Z}[\alpha]$ from fundamental unit of $\mathbb{Z}[\alpha,\beta]$","Let $R=\mathbb{Z}[\sqrt[3]{19}] =\mathbb{Z}[\alpha]\subset K=\mathbb{Q}(\sqrt[3]{19})$ . We know that $\mathcal{O}_K=R[\beta]=\mathbb{Z}[\alpha,\beta]$ where $\beta=\dfrac{\alpha^2-\alpha+1}{3}$ , c.f Example 3.7, page 34 . Furthermore, we have $1-\alpha-\beta$ is a fundamental unit of $\mathcal{O}_K$ but not in $R$ , c.f Example 7.3, page 75 . My question is How we can deduce fundamental unit of $\mathbb{Z}[\alpha], \mathbb{Z}[\beta]$ from fundamental unit of $\mathbb{Z}[\alpha,\beta]$ ? In 5.16, page 64 , the author use Regulator to implies that $R^*$ is of finite index in $\mathcal{O}_K$ but i don't really understand it. How can we get the bound for $n$ such that $(1-\alpha-\beta)^n$ to be a fundamental unit of $\mathbb{Z}[\alpha]$ ?","['number-theory', 'algebraic-number-theory']"
3460488,Probability that girl who answers the door is the eldest girl?,"You know that a family has 3 children. You walk up to and knock on the front door of their house. A girl answers the door. What is the probability that the she is the eldest girl among the children? Assume that all 3 children are home and equally likely to answer the door. (I appreciate that with questions like this, the wording can be so crucial. If the question is in any way ambiguous, I would be delighted to have an explanation of why that is the case!)","['recreational-mathematics', 'probability']"
3460564,Prove that $\mathbb{E}|X-Y|\geq \mathbb{E}|Y|$ if $X$ and $Y$ are independent and $\mathbb{E}{X} = 0$,"Given that $X$ and $Y$ are independent random variables and $\mathbb{E}{X} = 0$ , how to prove that this inequality $$\mathbb{E}|X-Y|\geq \mathbb{E}|Y|$$ holds? Surely, it's true that $\mathbb{E}|X-Y|\geq \mathbb{E}|X| - \mathbb{E}|Y|$ , but somehow the independence and $\mathbb{E}{X} = 0$ condition should sharpen this lower bound. However, I have no idea, how to use these conditions. Any help would be appreciated.","['expected-value', 'inequality', 'probability-theory']"
3460634,Linear independence of tensor product basis $\{ v_i \otimes w_j\}$ for $\{v_i\}$ and $\{w_j\}$ linearly independent.,"Show that the set $\{v_i \otimes w_j\}$ is a linear independent subset of $V\otimes W$ when $\{v_i\}$ and $\{w_j\}$ are independent subsets of V and W respectively. I want to find an error in a proof of this statement. I've found great proofs of this in the literature, for example these notes and this book . They all have the same core of the proof: take a linear form $\varphi_i: V\to \mathbb{F}$ such that $\varphi_i(v_j)=\delta_{ij}$ , which exists thanks to independence of $\{v_i\}$ . Then we have a linear function $\phi_i : V\otimes W \to W$ given by $\phi_i(v\otimes w)= \varphi_i(v)w$ which is as close to projection $V\otimes W \to W$ as one can hope to get. We can transport any linear relation from $V\otimes W$ to $W$ and use linear independence of $W$ . Details are in the links above. Okay, so here is the argument I am skeptical of: $\DeclareMathOperator{span}{span}$ We can assume that $\{v_i\}$ and $\{w_j\}$ are bases, instead we can consider $V'=\operatorname{span}\{v_i\}$ and $W'=\operatorname{span}\{w_j\}$ .
Let $F=F(\{v_i\}\times\{w_j\})$ be a free vector space with a basis $\{(v_i,w_j)\}$ . We define a bilinear function $\Phi:V\times W \to F$ by $$ \Phi(v,w) = \Phi\left(\sum_i a_iv_i,\sum_j b_jw_j\right) = \sum_{i,j}  a_ib_j(v_i,w_j)$$ As $\{v_i\}$ and $\{w_j\}$ are bases, therefore the coefficients $a_i,b_j$ are unique, so this function is well defined.
By the universal property, there is a linear function $\varphi: V\otimes W \to F$ such that $\varphi(v_i\otimes w_j)=\Phi(v_i,w_j)=(v_i,w_j)$ . We see that it is an isomorphism, because we can define $\phi^{-1}$ on a basis: $$\phi^{-1}((v_i,w_j))=v_i\otimes w_j$$ which is clearly a linear inverse. Therefore $F$ is isomorphic to $V\otimes W$ , so if $\{(v_i,w_j)\}$ is a basis for $F$ , $\phi[\{(v_i,w_j)\}] = \{v_i\otimes w_j\}$ must also be a basis. It uses no projection or a variant thereof, and proves the statement almost tautologically. I've tried to find the error, but I couldn't. I have a really hard time accepting this proof, because I haven't seen it anywhere (and there are plenty of places) and it really feels logically less demanding as there are no linear functionals used, the isomorphism $F\simeq V\otimes W$ is constructed directly. Actually, there are a few faulty proofs similar to the above that were already corrected on this site, namely here and here (one of the answers) . If the proof above is correct, can you direct me to the the sources or, ideally, explain why this proof don't appear in the sources I linked to?","['solution-verification', 'linear-algebra', 'tensor-products', 'tensors']"
3460651,Roots of equation form infinite sequence,"The sequence $a_n$ has the property that $a_n$ and $a_{n+1}$ are the roots of the equation $$x^2-c_nx+\frac{1}{3^n}=0$$ and $a_1=2$ . What is $\sum_{n=1}^{\infty}c_n?$ By Vieta's, $a_{n+1}=\frac{1}{3^na_n}$ and $c_n=a_n+a_{n+1}$ . Additionally listing the first numbers in $a_n$ $$2,\frac{1}{6},\frac{2}{3}, \frac{1}{18}, \frac{2}{9},\cdots$$ doesn't reveal anything (even though $c_n$ seems like a geometric sequence). Thanks!",['algebra-precalculus']
3460657,Rudin RCA Theorem 7.1,"In Rudin's Real and Complex Analysis, Theorem 7.1 is provided without a proof. Unfortunately, I have a difficulty in proving it by myself. It says: Suppose $\mu$ is a complex Borel measure in $R^1$ and \begin{equation*}
f(x) = \mu((-\infty,x)) \quad\quad (x\in R^1).
\end{equation*} If $x\in R^1$ and $A$ is a complex number, each of the following two statements implies the other: (a) $f$ is differentiable at $x$ and $f'(x) = A$ . (b) To every $\epsilon>0$ corresponds a $\delta>0$ such that \begin{equation*}
\left|\frac{\mu(I)}{m(I)} - A \right| < \epsilon
\end{equation*} for every open segment $I$ that contains $x$ and whose length is less than $\delta$ .
    Here $m$ denotes Lebesgue measure on $R^1$ . Here is my trial to prove that (a) imples (b). Assume (a) and let $\epsilon>0$ . Then, there exists $\delta>0$ satisfying \begin{equation*}
\frac{|f(x') - f(x) - A(x'-x)|}{|x'-x|} < \frac{\epsilon}{2}
\end{equation*} for all $0 < |x'-x| < \delta$ .
    Let $x-\delta < \alpha < x < \beta < x+\delta$ such that $\beta-\alpha < \delta$ .
    Then, \begin{equation*}
\frac{|f(\alpha) - f(x) - A(\alpha - x)|}{|\alpha - x|} = \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| < \frac{\epsilon}{2}
\end{equation*} and \begin{equation*}
\frac{|f(\beta) - f(x) - A(\beta - x)|}{|\beta - x|} = \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \frac{\epsilon}{2}.
\end{equation*} Thus, \begin{equation*}
\left|\frac{\mu([\alpha,\beta))}{\beta-\alpha} - A\right| \le \left| \frac{\mu([\alpha,x))}{x - \alpha} - A \right| + \left| \frac{\mu([x,\beta))}{\beta-x} - A \right| < \epsilon.
\end{equation*} Here is the point I stuck. I could not make $\mu([\alpha,\beta))$ to $\mu((\alpha,\beta))$ . I guess that either $\mu(\{\alpha\}) = 0$ or $\mu([\alpha_i,\beta)) \to \mu((\alpha,\beta))$ as $\alpha_i\to\alpha$ is necessary. Any help will be appreciated.","['measure-theory', 'analysis']"
3460749,Analysis Textbook like Zorich,"I'm an undergraduate student currently studying mathematical analysis. Our professor uses Zorich's Mathematical Analysis, but I found the text too difficult to understand. After exploring some textbooks, I found that Abbott was easier to follow, so I studied Abbott until I realized that there's a significant amount of content in Zorich that Abbott doesn't cover. So I was wondering if there's a book out there that covers as much content as Zorich but is more readable? Thank you for any help.","['book-recommendation', 'analysis']"
3460771,Line bundles over $\Bbb P^1_k$,"I am trying to understand the line bundle $O(1)$ over $\Bbb P^1_k$ and why $$ O(1)^{\otimes n} = O(n)$$ from Vakil's notes, p398, line 6 . His explanation is rather long, so I took a screen shot. I don't understand Vakil's explanation of why we have the equality because of the transition functions.","['quasicoherent-sheaves', 'tensors', 'vector-bundles', 'algebraic-geometry', 'tensor-products']"
3460835,Understanding a Topological proof of the infinity of primes,"Sorry if the question is too basic, I haven't taken any Topology lectures. I found this proof in ""Proofs from THE BOOK"" and don't quite understand the explanation of why $ N_{a,b} $ is closed, I hope you could spell it out for me. Thank you in advance.","['number-theory', 'general-topology', 'proof-explanation', 'prime-numbers']"
3460882,Average degree of graph from polyhedral complex,"Consider a graph constructed as follows. We begin with a pure polyhedral complex $C$ of dimension $d$ in $\mathbb{R}^d$ -- for our purposes this is just a finite collection $\{P_1, \dots, P_k\}$ of distinct $d$ -dimensional convex polytopes (""cells"") in $\mathbb{R}^d$ such that for any $P_i$ and $P_j$ , the intersection $P_i \cap P_j$ is a face of both $P_i$ and $P_j$ (possibly empty). Now we define the ""cell graph"" $G(C)$ to be the graph with vertices $P_1, \dots, P_k$ , and an edge between $P_i$ and $P_j$ if their intersection has dimension $d-1$ (i.e. their intersection is a facet of both). Let's say a graph is $d$ -cellular if it is the cell graph of some pure polyhedral complex of dimension $d$ . My question is: What is the best upper bound on the average degree of a $d$ -cellular graph (if there is one)? Note that when $d = 2$ , a $2$ -cellular graph is necessarily planar, and thus has average degree $< 6$ , and indeed there are $2$ -cellular graphs which have average degree arbitrarily close to $6$ : for example we can take the cell graph of a large bounded restriction of the regular hexagonal tiling of $\mathbb{R}^2$ . I would guess that a potential upper bound for the case $d=3$ is $14$ , asymptotically achieved by the Bitruncated cubic honeycomb (pictured below), but it's not immediately clear to me that there's any upper bound on the average degree for fixed $d \geq 3$ . Edit: As @quarague points out, the Triakis truncated tetrahedral honeycomb has a higher limit average degree of $16$ .","['polytopes', 'graph-theory', 'convex-geometry', 'combinatorial-geometry', 'combinatorics']"
3460901,Let $m_X(t)$ be the moment generating function of random variable $X$. Prove $m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!}$,"Let $m_X(t)$ be the moment generating function of random variable $X$ . Prove $m_X(t)=\sum_{k=0}^\infty E(X^k)\frac{t^k}{k!}$ So I have: $$
\begin{split}
m_X(t)
 &= \mathbb{E}\left[ e^{tX} \right]\\
 &= \mathbb{E}\left[ \sum_{k=0}^\infty \frac{(tX)^k}{k!} \right] \\
 &=\sum_{k=0}^\infty \mathbb{E}\left[ X^k \right] \\
 &=\sum_{k=0}^\infty \frac{t^k}{k!} \mathbb{E}\left[ X^k \right]
\end{split}
$$","['moment-generating-functions', 'statistics', 'probability-distributions']"
3460903,Order of the set of group homomorphisms from $\mathbb{Z}^n$ into an arbitrary finite group $G$.,"Question : Let a finite $G$ act on itself by conjugation, and let $N$ be the number of conjugacy classes. Find a formula for $|\mathrm{Hom}\left(\mathbb{Z}^n, G\right)|$ , denoting the set of group homomorphisms from $\mathbb{Z}^n$ into $G$ . Attempt : For $n=2$ , I got that Burnside's Lemma implies that $$
|G|\cdot N = \sum_{g\in G} |\mathrm{Fix}\left(g\right)|
$$ where $$
\begin{aligned}
\mathrm{Fix}\left(g\right) &= \{ h\in G\;|\;g\ast h = h \} \\
&= \{h\in G\;|\; ghg^{-1} = h \} \\
&= \{ h\in G\;|\; gh = hg \}
\end{aligned}
$$ Hence $$
\sum_{g\in G} |\mathrm{Fix}\left(g\right)| = | \{ (g,h) \in G\times G\;|\;gh = hg\} | = |\mathrm{Hom}\left(\mathbb{Z}^2, G\right)|
$$ My attempt (for $n=3$ ): My guess is that $|\mathrm{Hom}\left(\mathbb{Z}^n, G\right)| = |G|\cdot N^{n-1}$ but I do not know how to prove this or if it's even correct for non-abelian groups $G$ . My observation is that for abelian groups $N$ = $|G|$ , and for non-abelian groups, $N<|G|$ . Intuitively, I would think as $n$ grows large the permutations of objects in $G\times G\times...\times G$ ( $n$ times) such that they can be mapped onto by any homomorphism from an abelian group grows faster as $G$ becomes ""more abelian"", because of the property of homomorphisms. Construct the function $\Phi:\mathrm{Hom}\left(\mathbb{Z}^3, G\right)\rightarrow G\times G$ where $$
\Phi\left(\gamma\right) = \left(\gamma\left(1,0,0\right), \gamma\left(0,1,0\right), \gamma\left(0,0,1\right)\right)
$$ $\Phi$ uniquely describes all homomorphisms in the set that we want. We know that this is an injective function and the image consists of ordered triples that commute so: $$
|\mathrm{Hom}\left(\mathbb{Z}^3, G\right)| = |\{ \left(a,b,c\right)\in G\times G\times G\;|\; ab = ba, ac = ca, bc = cb\}|
$$ Now consider once again $\mathrm{Fix}\left(a\right)$ under the conjugacy action. Both $b$ and $c$ must be in that set, since $a$ commutes with both $b$ and $c$ . But in general that doesn't mean $b$ and $c$ commute with each other, but we require that otherwise it can't be a homomorphism. My first attempt was to find the size of $\mathrm{Fix}\left(a\right)$ first, and then find the size of $\mathrm{Fix}\left(a\right)\cap\mathrm{Fix}\left(b\right)$ . But I did not know how to do this or how it relates to the number of conjugacy classes. Not really sure how to proceed after this nor whether this is the right track.","['group-homomorphism', 'free-abelian-group', 'proof-verification', 'abstract-algebra', 'group-theory']"
3460980,Number of empty subway cars,"I have task: There are $9$ passengers and they get into empty $5$ -car subway train. What is the probability of the fact, that exactly two cars will be empty? I know that the power of all possibilities is $5^9$ . Then I thought to pick two empty cars that is ${5\choose 2}$ possibilities, then pick three people that get to the rest of the cars, that is $9\cdot 8\cdot 7$ and the rest of the people have $3^6$ possibilities. I checked it on my calculator and it seems that the power of this event is bigger than the power of $\Omega$ . What is wrong?","['combinatorics', 'probability']"
3461053,The equivalence relation on the power set,"Let $X$ be a set. We can define the equivalence relation on the power set $2^{X}$ in the following way: the sets $A,B\in 2^{X}$ are equivalent if the symmetric difference $A\triangle B$ is finite. Questions: Is there any information about this equivalence relation? Is there a standard notation for this equivalence relation?","['elementary-set-theory', 'notation']"
3461072,What is function $\sin$ without $x$?,"I'm solving a problem set in ODE: This is the first time in my life that I see such functions $\sin,\cos,\tan$ without argument $x$ . I would like to ask if $\sin,\cos,\tan$ mean $\sin x,\cos x,\tan x$ or they mean something else. Thank you so much!","['notation', 'ordinary-differential-equations']"
3461091,How do I solve recurrence relation without characteristic equation?,"Question: Solve the recurrence relation $\ a_n = 3a_{n-1} - 2a_{n-2} + 1 $ , for all $\ n \ge 2$ $\ a_0 = 2 $ $\ a_1 = 3 $ Write $\ a_n $ in terms of n I tried to solve this by finding the characteristic equation, $\ r^2 - 3r + 2 - 1 = 0 $ which is $\ r^2 - 3r + 1 $ . However, I can't simplify that further because of the ""+ 1"" unless I use the quadratic general formula... but the roots will be in fractions and they are definitely not correct compared to the answers.. So I tried to find $\ a_2, a_3, a_4 $ and so on... like this: $\ a_2 = 3a_1 - 2a_0 + 1 = 3(3) - 2(2) + 1 = 6 $ $\ a_3 = 3a_2 - 2a_1 + 1 = 3(6) - 2(3) + 1 = 13 $ $\ a_4 = 3a_3 - 2a_2 + 1 = 3(13) - 2(6) + 1 = 28 $ and so on... But it leads me to nowhere as I couldn't find any common pattern between $\ a_2, a_3, a_4 $ and so on, to derive $\ a_n $ ... How do I solve recurrence relations like this?","['recurrence-relations', 'discrete-mathematics']"
3461098,$y''+y\ln(1+x)=x+e^x$,"Seriously , I have no idea about the defintion of maximal unique solution for 2nd order ODE as asked in the asked and what $J$ it is asking? I did a lot of research about it and came across only first order ODE with maximal solutions examples on internet, which I dont understand them also. :(","['calculus', 'derivatives', 'ordinary-differential-equations']"
3461219,How to solve $y''(t) + 4 y (t) = 2t \cos (2t)$ on time in the exam?,"I'm solving this ODE, which is from last year final exam Solve in $\mathbb R$ the ODE $$y''(t) + 4 y (t) = 2t \cos (2t)$$ This ODE is not hard to solve, but it takes me a lot of time to compute the derivative $y',y''$ and simplifies them. This process is unfortunately likely to cause mistake. The last year exam contains a total of $4$ questions, and has a duration of $1$ hour and $30$ minutes. Solving this ODE is just one of $4$ questions. I would like to ask for a faster way to solve this ODE. Maybe it has a special characteristics that I'm unable to recognize. My attempt: The solution is of the form $$\begin{aligned} y (t) &= \big ( a \cos (2t) + b \sin (2t) \big ) + t(ct+d) \cos (2t) + t(gt+h) \sin (2t)\\
&= a \cos (2t) + b \sin (2t) + dt \cos (2t) +ht \sin (2t) + ct^2 \cos (2t) + gt^2 \sin (2t) \end{aligned}$$ where $a,b,c,d,g,h \in \mathbb R$ . It follows that $$\begin{aligned}
y' (t) &= (2b+d) \cos (2t) + (h-2a) \sin (2t)\\
       & \qquad + 2(h+c) t \cos (2t) +2(g-d) t \sin (2t)\\
       & \qquad + 2g t^2 \cos (2t)  -2c t^2 \sin (2t)
\end{aligned}$$ and $$\begin{aligned}
y'' (t) &= 2(2h-2a+c) \cos (2t) -2 (2b+2d-g) \sin (2t)\\
       & \qquad + 4(2g-d) t \cos (2t) -4(h+2c) t \sin (2t)\\
       & \qquad -4c t^2 \cos (2t)  -4g t^2 \sin (2t)
\end{aligned}$$ Hence $$\begin{cases}
2(2h-2a+c) + 4a &=0 \\
-2 (2b+2d-g) +4b &=0 \\
4(2g-d) + 4d &=0 \\
-4(h+2c) + 4h  &=0 \\
-4c +4c &=0 \\
-4g+4g  &=0 \\
\end{cases} \iff
\begin{cases}
c &=0 \\
g &=1/4 \\
h &=0 \\
d  &=1/8 \\
\end{cases}$$ In conclusion, the solution is $$y(t) = a \cos (2t) + b \sin (2t) + \frac{1}{8} t \cos (2t)   + \frac{1}{4} t^2 \sin (2t)$$","['alternative-proof', 'ordinary-differential-equations']"
3461262,Second order linear ODE with polynomial coefficients,"I am currently stuck at solving an ODE of the form \begin{equation}
0=\psi''(x)+(\varepsilon-(\alpha x^2-\beta)^2)\psi(x)
\end{equation} where $\alpha,\beta$ and $\varepsilon$ are real parameters.
Anticipating $\psi\sim\mathrm{e}^{-\alpha x^3/3}$ for large $x$ , I made the Ansatz $\psi(x) = f(x)\mathrm{e}^{-\alpha x^3/3}$ and obtain the ODE \begin{equation}
0=f''(x)-2\alpha x^2 f'(x)+(\varepsilon-\beta^2+2\alpha\beta x^2-2\alpha x)f(x).
\end{equation} Actually, I am looking for a normalized solution $\sim\mathrm{e}^{-\alpha|x|^3/3}$ , where extra care needs to be taken around $x=0$ . (I am a physics student and not so much concerned about mathematical rigor right now.)
But any solution would be a first step of course.
I tried a variety of things to solve the second equation: A powerseries approach $f(x)=\sum_{n=0}^\infty a_n x^n$ . I obtain \begin{equation}a_{n+2}=\frac{(\varepsilon-\beta^2)a_n-2\alpha(n+1)a_{n-1}+2\alpha\beta a_{n-2}}{(n+2)(n+1)}\end{equation} for $n\geq 2$ , which does not look very promising. I also checked A. Polyanins and V.F. Zaitsevs Handbook of Exact Solutions of Ordinary Differential Equations but failed to transform the above ODEs into one given in the book. Wolfram Mathematica was not helpful either. I transformed the equation into an system of first order linear ODEs with $g=f'$ . However such an approach is only helpful for constant coefficients, right? I am thankful for any ideas or hints. A particular solution would be very helpful of course, as it would allow me to reduce the order of the ODE and find all solutions.
Note that, while $\alpha$ and $\beta$ are given, I expect constraints on $\varepsilon$ for (normalized) solutions to exist, i.e. I would also appreciate a solution for special values of $\varepsilon$ , e.g. $\varepsilon=\beta^2$ . Sorry for any grammatical errors.
Thank you for your help!","['physics', 'ordinary-differential-equations', 'fundamental-solution']"
3461273,"Asymptotic behaviour of $P(S_n=k),$ where $S_n$ is a sum of $n$ i.i.d. discrete random variables and $k$ is fixed.","Let $\{X_i\}_{i=1}^\infty$ be a sequence of i.i.d. discrete random variables taking non-negative integer values and let $S_n=\sum_{i=1}^nX_i$ . Are there any theorems/results establishing exact asymptotic behaviour (say, in terms of mean and/or variance of $X_i$ 's) of $$P(S_n=k)$$ as $n\to\infty,$ where $k\in\mathbb{N}_0$ is fixed?","['probability-theory', 'probability']"
3461315,Can a function's derivative switch signs without crossing zero?,"So if you want to find a local minimum of a function, one way to do that would be to find an interval $[a,b)$ where the derivative is negative, and that the derivative in the interval $(b,c]$ is positive for some $c$ . $f'(b)$ needs to $0$ or not defined.
But my question is, is there a function where the derivative in $[a,b)$ is negative, and positive in $[b,c]$ ? That means there's a jump discontinuity in the derivative, so the original function would be piece-wise Thank you!","['calculus', 'functions', 'derivatives', 'functional-analysis']"
3461318,The number of edge cut sets simultaneously containing two different edges of complete graph,"Let $a$ , $b$ be two different edges in the complete graph $K_n$ . Try to find the number of edge cut sets that simultaneously contain $a$ and $b$ . Here, an edge cut of a connected graph $G$ is a set $S$ of $G$ 's edges such that $G-S$ is disconnected and $G-S'$ is connected for any proper subset $S'$ of $S$ . I try to calculate based on whether $a$ , $b$ have a common vertex, but I don't know the exact answer.","['graph-theory', 'discrete-mathematics']"
3461334,Do these definitions of irreducibility of algebraic sets coincide?,"I am reading the book Numerically solving polynomials systems with Bertini in which they define a manifold point $p^* = (p_1^*,\ldots,p_m^*)$ of an algebraic set $X$ to be a point in $X$ with an open neighborhood $U\subset X$ such that for some mapping $\Phi(z_1,\ldots,z_m)$ , $\Phi$ restricted to $U$ maps $U$ bijectively onto a neighborhood of the origin in $\mathbb{C}^k$ for some $k$ .
The set of manifold points of $X$ is denoted $X_{\text{reg}}$ . Now they say an affine complex algebraic set $X$ is irreducible if $X_{\text{reg}}$ is connected, i.e. $X_{\text{reg}}$ cannot be written as the union of two disjoint non-empty open subsets in $X_{\text{reg}}$ . However, in my algebraic geometry class, which is based on the book of Hartsthorne. An algebraic set $X$ is irreducible if it cannot be expressed as the union of two proper non-empty closed subsets of $X$ . I have not seen any mention of manifold points in Hartsthorne so far and am having trouble understanding which points of an algebraic set are manifold points (or rather which points are not) and therefore how these two definitions of irreducibility coincide.",['algebraic-geometry']
3461446,Is the space of all linear complex structures of $\mathbb{R}^{2n}$ an embedded submanifold of $GL_{2n}(\mathbb{R})$?,"Denote $G = GL_{2n}(\mathbb{R})$ , and let $F : G \to G$ be the map $F(X)=X^2$ . Let $\mathcal{J} = F^{-1}(-\operatorname{Id})$ , the space of all linear complex structures of $\mathbb{R}^{2n}$ . Is $\mathcal{J}$ an embedded submanifold of $GL_{2n}(\mathbb{R})$ ? $G$ acts on itself from the left by conjugation $A \overset{B\cdot}{\mapsto} BAB^{-1}$ , and under this action $\mathcal{J}$ is the orbit of the standard complex structure of $\mathbb{R}^{2n}$ : $$J_0 = \begin{pmatrix} 0 & -\operatorname{Id} \\ \operatorname{Id} & 0 \end{pmatrix}$$ The stabilizer of $J_0$ is a closed Lie subgroup which can be identified with $H = GL_n(\mathbb{C})$ . Therefore, the quotient $G/H$ has a smooth structure, and the orbit map $A \mapsto A J_0 A^{-1}$ transcends to a smooth, injective, $G$ -equivariant immersion $\iota : G/H \to G$ whose image is $\mathcal{J}$ . But, the action of $G$ is not proper, as the stabilizer of $J_0$ is not compact. So, we cannot deduce that $\iota$ is a proper map, and therfore an embedding. Is there any other way to show this is an embedding? Or is it wrong? Another way I tried to approach this is by using the fact that $F : G \to G$ is also $G$ -equivariant, and therefore has a constant rank on every orbit. But, this doesn't give me a constant rank in an open neighborhood of $\mathcal{J}$ , so I cannot argue it's a level set of a map with a constant rank.","['complex-geometry', 'lie-groups', 'differential-geometry']"
3461478,Find natural parameter of Gamma,"We start by proving that the gamma distribution is a member of the exponential family. It looks a bit different if either $\alpha$ or $\beta$ are known but below I showcase my proof if both are unknown $$ pdf: p(x|\beta, \alpha) = \frac{\beta^\alpha}{\Gamma (\alpha)} x^{\alpha-1} e^{-\beta x} $$ $$ log [p(x|\beta, \alpha)] = \alpha log \beta - log\Gamma (\alpha) + (\alpha - 1)logx - \beta x $$ $$ p(x|\beta, \alpha) = \frac{1}{\Gamma (\alpha) }exp [\alpha log \beta + (\alpha - 1)logx - \beta x] $$ If we define a member of the exponential family as $$ p_(x|\theta) = \frac{h(x)}{z(\theta)} exp(\eta(\theta)^T \cdot s(x)) \quad \quad \quad (1) $$ Then we can see that the gamma distribution is a member of the exponential family, since $$ h(x)=1, \quad z(\alpha, \beta) = \Gamma(\alpha), \quad \eta_1(\alpha, \beta)= \alpha log \beta, \quad s_1 (x) = 1 \\ \eta_2(\alpha, \beta)=(\alpha-1), \quad s_2(x)=logx, \quad \eta_3(\alpha, \beta) = -\beta, \quad s_3(x)=x $$ My question: Now, we are to find the natural parameter and describe the natural parameter space. Any help here would be greatly appreciated, since I have a hard time understanding the concept of natural parameters, how to calculate it and why we are interested in it. EDIT: I've tried reading up a bit, and my guess at the moment is that when I successfully write a p.d.f in terms of equation (1), then I can determine that the corresponding distribution is a member of the exponential family, and the function $\eta(\theta)$ is the natural parameter.","['gamma-distribution', 'statistics', 'probability-theory']"
3461531,Help with multivariable limit and differentiability.,"I have to determine differentiability at $(0,1)$ of the following function: $$f(x,y)=\frac{|x| y \sin(\frac{\pi x}{2})}{x^2+y^2}$$ The partial derivatives both have value $0$ at $(0,1),$ and both are continuous on that point (I think I've got this part right), so the function must be differentiable at $(0,1).$ But when I checked for differentiability using the definition, the limit that should be $0$ doesn't exist, so I assume I'm doing something wrong when computing the limit. The following limit has to be $0$ if the function is differentiable at that point $$\lim_{x,y\to(0,1)} \frac{|f(x,y)|}{\|(x,y)-(0,1)\|}$$ Doing the change $w=y-1$ we have: $$\lim_{x,y\to(0,0)} \frac{|x (w+1)\sin(\frac{\pi x}{2})|}{(x^2+(w+1)^2)\sqrt{x^2+w^2}}$$ and then computing the limit along the line $x=w,$ it has the value $\pi /2\sqrt{2}$ , which contradicts that the limit is $0.$ What am I doing wrong?","['real-analysis', 'multivariable-calculus', 'calculus', 'limits', 'derivatives']"
3461543,Relation between Infinitesimal Generator of Ito Difussion and Ito's Formula,"I am wondering how to relate the infinitessimal generator of a diffusion process and Ito's formula. Formally, consider the one-dimensional case, and let $dX_t=a(X_t)\,dt+b(X_t)\,dW_t$ , where $W_t$ is a standard brownian motion. I know that the infinitessimal generator, for $f$ suitable, is defined as $\mathcal{L}f(X_t)=\lim_{\delta\to 0} \frac{\mathbb{E}^{X_t}[f(X_{t+\delta})-f(X_t)]}{\delta}$ . I know from Oksendal's Book on SDEs that $\mathcal{L}f(X_t)=a(X_t)f'(X_t)+\frac{1}{2}b(X_t)^2f''(X_t)$ , and I'm wondering how to reconstruct this formula in a simple way from Ito's formula, by looking at infinitesimal changes (It is usual to express the generator in terms of infinitesimal changes without much justification in finance books). My attempt is the following. For $\delta$ arbitrarily close to zero, we can think of $\frac{1}{\delta}=\frac{1}{dt}$ and of $f(X_{t+\delta})-f(X_t)=df(X_t)$ . Then, by Ito's formula, $$df(X_t)=f'(X_t)\,dX_t+\frac{1}{2}f''(X_t)(dX_t)^2$$ So plugging back $dX_t$ and using Ito rules to compute $(dX_t)^2$ (i.e. $dt\,dW_t=(dt)^2=dW_t\,dt=0$ and $(dW_t)^2=dt)$ , we have $$df(X_t)=f'(X_t)(a(X_t)\,dt+b(X_t)\,dW_t)+\frac{1}{2} f''(X_t)b(X_t)^2 \, dt$$ So taking the expectation, with the observation that $\mathbb{E}^{X_t}(dW_t)=0$ , we end up with $$\mathbb{E}^{X_t}(df(X_t))=f'(X_t)a(X_t) \, dt+\frac{1}{2} f''(X_t)b(X_t)^2 \, dt$$ And thus $\frac{1}{dt}\mathbb{E}^{X_t}(df(X_t))=f'(X_t)a(X_t)+\frac{1}{2} f''(X_t)b(X_t)^2=\mathcal{L}f(X_t)$ My question is the following: If the reasoning is correct, what do I need to think of $\lim_{\delta\to 0}\frac{\mathbb{E}^{X_t}[f(X_{t+\delta})-f(X_t)]}{\delta}$ as $\frac{\mathbb{E}^{X_t}(df(X_t))}{dt}$ ? I've seen the latter used mainly in finance books but without much justification. I understand that I need some conditions to make the DCT or MCT work to exchange limit with expectation, but what else would I need to know that $f(X_{t+\delta})-f(X_t)\to df(X_t)$ as $\delta\to 0$ ? Can someone please help me to fill in the gaps that would make my reasoning completely formal?","['stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3461553,Forgetting about the Underlying Probability Space,"I have read that when dealing with random variables, one often forgets about the underlying probability space ( Wikipeida ). What is a good example (or a couple) of when one does this? I figure that there are two ways to go about this: (1) a stand-alone random variable where the probability space is too complicated or (2) two random variables on rather different probability spaces that have the same distribution, which motivates the study of the distributions themselves. By a good example, I mean one that would motivate the study of distributions without regard to the underlying probability space. I am not looking for an example that could just so happen be studied with distributions, but an example that shows that it is easier to study the random variable with respect to the distribution rather than the underlying probability space.","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
3461561,Smooth Approximation of Staircase Function [duplicate],"This question already has answers here : Equation for a smooth staircase function (6 answers) Closed 4 years ago . What is a function $f(x)$ that is smooth and approximates a staircase function (a function composed of a set of equally spaced jumps of equal length)? With this answer , I know of $f^n(x)$ , where n is an integer and $f(x)=x-\sin(x)$ , but it requires quite a large amount of terms to get to a desired sharpness. So, I'm looking for approximations that is not recursive and requires only a finite amount of terms.","['functions', 'approximation']"
3461582,"$\cos(x)=\frac{1}{n}, n=2k+1, k\in \mathbb Z_+$","Let $n\in N$ be odd. Prove that all solutions to the equation: $$
 \cos{(x)}=\frac{1}{n}$$ are irrational. (Poland, 1966) My work: $$\frac{1}{3}=\max\Bigg\{\frac{1}{n}:n=2k+1, k\in \mathbb Z+\Bigg\}$$ $$\cos(x)=\frac{1}{n} \in\left\langle 0, \frac{1}{3}\right]$$ $$\implies x\in\left\langle\frac{(4k-1)\pi}{2},-\arccos{\frac{1}{3}}+2k\pi\right]\cup\left[\arccos{\frac{1}{3}}+2k\pi,\frac{(4k+1)\pi}
{2} \right\rangle$$ Facts I took into account: Apart from some powers of irrational roots, a product of two irrational numbers is most likely to be irrational. If we subtract irrational numbers with different decimal parts or add two irrational numbers whose decimal parts aren't 'complementary' , the result will be irrational as well. $$x\in \mathbb R\setminus \mathbb Q\implies x\ne\frac{m}{n}, \forall m\in \mathbb Z,\forall n\in \mathbb N$$ I considered proving by contradiction but didn't know how to realize it (I wanted to prove $M(m,n)\ne1$ and square $\arccos{\frac{1}{n}}$ ) Another idea was to prove $\arccos{(kx)}, k\in \mathbb Z\setminus\{0\}$ . There are many tools I didn't know which to choose.","['contest-math', 'elementary-number-theory', 'trigonometry', 'real-analysis']"
3461584,Frequency of looped chains in an array of random integers,"Take an array of y random integers in the inclusive range 0 to 99, each integer written in the form ab, where a is the first digit in the integer and b the second. For example, integer 9 is written as 09 for this purpose, a=0 and b=9. Two of the array members a(1)b(1) and a(2)b(2) and are linkable if a(1)=b(2), or b(1)=a(2). Let us create a looped chain of linkable array members of length x. An example of a looped chain of length 4 is 09,95,57,70.  70 links back to 09. Additional rules: (i) in the random generation of array members, repeated integers have a separate member identity in the array (ii) array members are permitted to be members of multiple looped chains, but only to appear once in each chain Then: (I) For the defined array, what is the probability that there exists one or more looped chain(s) of length x, with x < y? (II) Is there a method to generate, or a formula for generating, estimates of how many different possible chains of length x exist within an array of length y? (III) Is there a formula/ method to generalise this in any number base?","['statistics', 'integers', 'probability']"
3461596,How to prove the relation $R$ is a function?,"Definition: The relation $R ⊆ A \times B$ is a function only if it satisfies the two properties: For every $x∈A$ there is $y∈B$ s.t. $x\mathrel{R}y$ ; if $x\mathrel{R}y$ and $x\mathrel{R}z$ then $y=z$ . The question: Prove that $R ⊆ A \times B$ is a function if and only if $\ker(R)$ is an equivalence relation. Note: $\ker(R) = R^{-1}\circ R$ , the composition of $R$ inverse and $R$ . I already proved that $\ker(R)$ is an equivalence relation, by proving for the relation the reflexive, symmetric and transitive properties. But I'm having trouble on how to prove that $R$ is a function.","['abstract-algebra', 'discrete-mathematics']"
3461636,Gauss's proof that the Digamma function equals $\int_0^{\infty}(\frac{e^{-t}}{t} - \frac{e^{-zt}}{1-e^{-t}})dt$.,"I was reading about the Digamma function, defined as: $$\psi(z) = \frac{d}{dx}\ln( \Gamma(z)) = \frac{\Gamma ' (z)}{\Gamma(z)}$$ And the following integral representation of $\psi(z)$ was given for $z:\Re(z) > 0$ : $$\psi(z) = \int_0^{\infty}\Big{(}\frac{e^{-t}}{t} - \frac{e^{-zt}}{1-e^{-t}}\Big{)}dt$$ The proof of which is written off as ""due to Gauss,"" but I cannot find his proof. What was Gauss's proof of this?","['integration', 'digamma-function', 'reference-request', 'complex-analysis', 'gamma-function']"
3461652,Sampling with and without replacement,"A box contains 5 tickets. An unknown number of them are red, the rest are green. Suppose that to start off with you think there are equally likely to be 0, 1, 2, 3, 4, or 5 red tickets in the box. Three tickets are drawn from the box between draws. The tickets are red, green, and red. Given this information, what is the chance that there are actually 3 red tickets in the box? a) with replacement (Answer: 0.36) b) without replacement (Answer: 0.4) So far I have done this: P(RRRGG|RGR) = P(RRRGG & RGR)/P(RGR) = P(RRRGG)/P(RGR), but I do not know what to do next because I keep getting the wrong answer.","['statistics', 'probability']"
3461670,Image of shape under Möbius transformation,"I am trying to describe the image of $\{z: |z-i|<1,Re(z)<0\}$ under the Möbius transformation $f(z)=\frac{z-2i}{z}$ . I would usually know how to describe this image by first considering the image of its boundary, and then a point on its interior. But I am having difficulty finding the image of the boundary. I know that line segments map to line or circle segments, and $f(0)=-\infty, f(i)=-1,f(2i)=0$ . So I think the line segment from $0$ to $2i$ maps to the line segment from $-\infty$ to $0$ , or $\mathbb{R}_{\leq0}$ . But I can't figure out what the circular arc from $0$ to $2i$ maps to. My only guess is to consider some point on this arc, for example $-1+i$ , which maps to $i$ under $f$ . But I don't think this means the arc maps to the line segment from $-\infty$ (in the imaginary direction) to $i$ .","['complex-analysis', 'mobius-transformation']"
3461707,Finding a parametre that satisfies an inequality,"For what values of $k>0$ does $$a^2+b^2+c^2+d^2+4(\sqrt3 -1)(abcd)^k\geq\sqrt{12(abc+abd+acd+bcd)}$$ hold for all $a,b,c,d\geq0$ satisfying $a+b+c+d=4$ ? On the one hand, I found a lower bound for the LHS by using an equivalent form of Turkevich's inequality : $3(a^2+b^2+c^2+d^2)\geq4(4-abcd)$ . On the other hand, for the RHS one can find an upper bound by using the famous ISL 1997 $64+44abcd\geq27(abc+abd+acd+bcd)$ . Yet, I am stuck, I cannot make any progress towards finding the range of $k$ .","['contest-math', 'multivariable-calculus', 'optimization', 'inequality', 'mixing-variables']"
3461791,"Etale covers of $\mathbb{G}_{m,k}$ in char 0","Let $k$ be a field of characteristic 0. It seems it is well known that étale covers of $\mathbb{G}_{m,k}=\operatorname{Spec}(k[T^{\pm 1}])$ are in bijection with étale covers of $\operatorname{Spec}(k(\!(T)\!))$ . I'd like to understand this by myself, as much as I can. This means I'm here mainly for hints. What is the good ""language"" to study étale covers of $\mathbb{G}_m$ ? I know that, in characteristic 0, connected étale covers of $\mathbb{A}^1_k$ are trivial, i.e. they are the same as connected étale covers of $k$ , i.e. they corresponds to finite (separable, but everything is separble in char 0) extensions of $k$ . This is proved, for instance in Galois Theory for Schemes by H.W. Lenstra (6.23) via ramification theory of valuations, with references to some chapters of Intro. to theory of algebraic functions in one variable by Chevalley. I don't know much of valuation theory and I'm wondering if this ""language"" may help me to classify étale covers of $\mathbb{G}_m$ . I believe it would help me indeed, but maybe there are some other kind of arguments that would solve my problem. (I apologize for the tag ""étale cohomology"", maybe it's an overkill, but it was the only tag with the word ""étale"")","['etale-cohomology', 'ramification', 'algebraic-geometry', 'valuation-theory', 'commutative-algebra']"
3461796,complex polynomials problem with a weird inequality,Show that if $P(z) = a_nz^n+a_{n-1}z^{n-1}+...+a_1z + a_0$ is a polynomial of degree n then there is $R_0>0$ such that for $|z| > R_0$ the polynomial can be bound from below as $|P(z)| > \frac{1}{2}|a_n||z|^n$ but what if you have the polynomial $P(z) = a_1z+a_0$ where $a_1 = -100$ and $a_0 = 100$ and we fill in $z = 1$ then we have the left side equal to $0$ and the right side equal to 50 so it does not hold. How does that work? I also do not understand what the $R_0$ adds to the problem. Do i use it somewhere in the proof because i do not see its relevance otherwise than stating that absolute value is bigger than 0 also.,['complex-analysis']
3461844,Sine-Gordon Transformation and Minlos' Theorem,"I'm currently reading Simon's book on functional integration and there is a section where it discusses the so-called sine Gordon Transformation. We have a symmetric function $V$ satisfying $$\sum_{i,j=1}^{n}z_{i}\bar{z}_{j}V(x_{i}-x_{j})\ge 0$$ for all $z_{1},...,z_{n}\in \mathbb{C}$ and $x_{1},...,x_{n}\in \mathbb{R}^{d}$ . This kind of function is called positive-definite. Now, Simon's book states: ""Any positive definite function is crying out to be a covariance of a Gaussian process. Thus, we construct a gaussian process $\{q(x)\}_{x\in \mathbb{R}^{n}}$ with covariance $V(x-y)$ and we use $d\mu(q)$ to denote the corresponding measure. (...) If $V$ is Hölder continuous, then one can prove a multidimensional Kolmogorov lemma and realize $d\mu$ on $C(\mathbb{R}^{d})$ and, in any event, by Minlos' Theorem, we can realize $d\mu$ on $\mathcal{S}'(\mathbb{R}^{d})$ ."" After that, he calculates (Here $\langle \cdot \rangle = \int \cdot d\mu$ ): $$\langle \exp (i\sum_{i=1}^{n}a_{i}q(x_{i}))\rangle = \exp(-\frac{1}{2}\sum_{i,j=1}^{n}a_i a_jV(x_{i}-x_{j})) \hspace{3cm} (1)$$ I know hardly anything about gaussian processes, so I don't quite understand what those $\{q(x)\}_{x\in \mathbb{R}^{d}}$ are or how can I write them explicitly but my question is: It seems that Simon's performing the integral on the left hand side of (1) on the space $\mathcal{S}'(\mathbb{R}^{d})$ , which is what I want to do too. But how come this integral make sense? I don't understand what it means to integrate this exponential on $\mathcal{S}'(\mathbb{R}^{d})$ . Besides, how can we conclude equality (1)? I don't see how it follows.","['stochastic-processes', 'probability-theory', 'functional-analysis']"
3461849,extreme simplification: $\frac{\sin(Nx)\sin((N+1)x)\sin(Mx)\cos((M+1)x)}{\sin^2(x)}$,"I am trying to simplify the function $$f_{N,M}(x)=\frac{\sin(Nx)\sin((N+1)x)\sin(Mx)\cos((M+1)x)}{\sin^2(x)},\qquad N,M\in\Bbb N$$ to the form $$f_{N,M}(x)=\sum_{k}\alpha_k^{(N,M)}\sin(2kx).$$ The reason I expect this to be possible is the $(N,M)=(3,5)$ case: $$f_{3,5}(x)=-\frac12\sin 2x-\frac12\sin4x+\sin8x+\frac32\sin10x+\frac32\sin12x+\sin14x+\frac12\sin16x,$$ and the unexplained answer here .  The $(N,M)=(3,5)$ case, included in an answer to the linked question, is apparently due to the formula for the sine $\sin x=\frac1{2i}(u-1/u)$ , where $u=e^{ix}$ . I've tried to apply this to the general case at hand: $$\begin{align}
f_{N,M}(x)&=\frac{\sin(Nx)\sin((N+1)x)\sin(Mx)\cos((M+1)x)}{\sin^2(x)}\\
&=\frac{2^{-1}(2i)^{-3}(u^{N}-u^{-N})(u^{N+1}-u^{-N-1})(u^M-u^{-M})(u^{M+1}+u^{-M-1})}{(2i)^{-2}(u-1/u)^2}\\
&=\frac1{4i}\frac{(u^{N}-u^{-N})(u^{N+1}-u^{-N-1})(u^M-u^{-M})(u^{M+1}+u^{-M-1})}{(u-1/u)^2}.
\end{align}$$ Then I defined the functions $$\begin{align}
a_j(x)&=x^j-\frac1{x^j}\\
r_j(x)&=x^j+\frac1{x^j}
\end{align}$$ so that $$4if_{N,M}(x)=\frac{a_N(u)a_{N+1}(u)a_M(u)r_{M+1}(u)}{a_1^2(u)}.$$ Letting the numerator be $\eta$ , we note that $$r_N(u)r_{N+1}(u)=a_{2N+1}(u)-a_1(u)$$ as well as $$r_M(u)a_{M+1}(u)=r_{2M+1}(u)-r_1(u)$$ so that $$\eta=a_{2N+1}(u)r_{2M+1}(u)-a_{2N+1}(u)r_1(u)-a_1(u)r_{2M+1}(u)+a_1(u)r_1(u).$$ We see that $$a_p(u)r_q(u)=4i\cos(px)\sin(qx)=2i(s_{q+p}+s_{q-p}),\qquad s_K=\sin Kx$$ so that $$\frac{\eta}{2i}=s_{2M+2N+2}+s_{2M-2N}+s_{2N}+s_2-(s_{2N+2}+s_{2M+2}+s_{2M}).$$ This almost seems like something I'm looking for, except for the denominator of $4if_{N,M}$ , namely $\sin^2x$ , which I can't seem to get rid of. Could I have some help? Thanks.","['trigonometric-series', 'trigonometry', 'sequences-and-series']"
3461864,True/false: $\det(A^2+I)\ge 0$ for every $3 \times 3$ matrix with real entries and rank $>0$,"I have the following proposition about which I have to say whether it is true or false. $\det(A^2+I)\ge 0$ for every $3 \times 3$ matrix with real entries and rank $>0$ . $I$ is the identity matrix. I tried brutal ways (taking a generic matrix, evaluating its square and adding $I$ ), but there are too many calculations and I feel that manner will not lead me to anything of interesting. I also tried to construct a counterexample, but nothing. I am not able to prove nor confute this assertion.","['matrices', 'matrix-rank', 'determinant']"
3461865,$\mathbb{S}^1$-action over $T^2$ is symplectic but not Hamiltonian,"I've read the following assertion in Ana Cannas' Lectures in Symplectic Manifolds : ""On the $2$ -torus $(T^2,d\theta_1\wedge d\theta_2)$ , the vector fields $X_1=\frac{\partial}{\partial\theta_1}$ and $X_2=\frac{\partial}{\partial\theta_2}$ are symplectic but not Hamiltonian."" Here's my attempt to prove it. I'm used to writing $T^2=\mathbb{R}^2/\mathbb{Z}^2$ , so I'll use $x,y$ for the coordinates and the symplectic form will be $dx\wedge dy$ . Clearly the flows for $X_1,X_2$ are $\phi_t^1(x,y)=(x+t,y)$ and $\phi_t^2(x,y)=(x,y+t)$ respectively, so $(\phi_t^1)^*(dx\wedge dy)=d(x+t)\wedge dy=dx\wedge dy$ and $(\phi_t^2)^*(dx\wedge dy)=dx\wedge dy$ . This means that both $X_1,X_2$ are symplectic. On the other hand, $i_{X_1}(dx\wedge dy)=dy$ and $i_{X_2}(dx\wedge dy)=dx$ , so it looks like both are also Hamiltonian. I guess what I'm doing wrong is that $dx,dy$ are perfectly ok in $\mathbb{R}^2$ but not in $T^2$ because of the quotient by $\mathbb{Z}^2$ . But in that case, is the first argument with the pullbacks also wrong?","['group-actions', 'symplectic-geometry', 'differential-geometry']"
3461873,"find a closed form expression for $\sum_{k=0}^n \left \lceil\sqrt{2k} \right\rceil, \quad n \ge 0$","The following is an exercise from my textbook. I can't really seem to find similar examples in the book and I find it a bit confusing. I need to find a closed form expression for $$\sum_{k=0}^n \left \lceil\sqrt{2k} \right\rceil, \quad n \ge 0$$ attempt thus far, Let $q=\lceil \sqrt{2k}\rceil$ Then, \begin{align*}
\sum_{k=0}^n \left \lceil\sqrt{2k} \right\rceil &= \sum_{0\leq k<n}\left \lceil\sqrt{2k} \right\rceil  \\
&= \sum_{k, q \geq 0}q [k<n][q-1 < \sqrt{2k} \leq q] \\
&= \sum_{k, q \geq 0}q [k<n][(q-1)^2 < 2k \leq q^2] \\
&= \sum_{k, q \geq 0}q [k<n][\frac{(q-1)^2}{2} < k \leq \frac{q^2}{2}] \\
&= \sum_{k, q \geq 0}q[\frac{(q-1)^2}{2} < k \leq \frac{q^2}{2} < n] \\
&= \quad ...
\end{align*}","['summation', 'number-theory', 'elementary-number-theory', 'closed-form', 'discrete-mathematics']"
3461939,Contadicts Radon-Nikodym Patrick Billingsley,"I am self-studying Probability Theory.
I am unable to solve the below problem ( Probability and Measure, 3e by Patrick Billingsley, Exercise 32.5","['self-learning', 'measure-theory', 'lebesgue-measure', 'probability-theory', 'radon-nikodym']"
3461953,"How can we show that $\int_{-\infty}^{\infty} f(x) \,df(x) = \frac12$?","How can we show that $\int_{-\infty}^{\infty} f(x) \,df(x) = \frac 1 2 $ , where $f$ is a continuous probability distribution function?","['integration', 'measure-theory', 'probability-theory']"
3461981,Confusion about proof that Hopf fibration is a bundle,"I am looking at page 2 of the lecture notes http://staff.ustc.edu.cn/~wangzuoq/Courses/16F-Manifolds/Notes/Lec15.pdf . Let $\{U_j\}_{1\leq j \leq n+1}$ denote the usual open covering of $\mathbb{CP}^{n}$ and let $\pi: S^{2n+1} \to \mathbb{CP}^n$ denote the natural projection. The author constructs local trivializations by defining $$
\Phi_{j}: \pi^{-1}\left(U_{j}\right) \rightarrow U_{j} \times S^{1}, \quad \Phi_{j}(z)=\left(\left[z^{1}: \cdots: z^{n+1}\right], \frac{z^{j}}{\left|z^{j}\right|}\right).
$$ The author then claims that the inverse of this map is given by $$
\Phi_{j}^{-1}\left(\left[z^{1}: \cdots: z^{n+1}\right], e^{i \theta}\right)=\frac{e^{i \theta}\left|z^{j}\right|}{\sqrt{\left|z^{1}\right|^{2}+\cdots+\left|z^{n+1}\right|^{2}} z^{j}}\left(z^{1}, \cdots, z^{n+1}\right).
$$ Is this well-defined, i.e., invariant under scaling by elements of $S^1$ ? It doesn't look so to me, but maybe I'm missing something.","['proof-explanation', 'general-topology', 'manifolds', 'geometry']"
3461994,A state of a C* algebra which preserves square of a self adjoint element,Let $ϕ$ be a state on a $C∗$ -algebra $ A$ . Suppose that for some selfadjoint element $a ∈ A$ one has $ ϕ(a^{2}) = ϕ(a)^2$ . Show that this implies that $ϕ(ab) = ϕ(ba) = ϕ(a)ϕ(b)$ for any $ b ∈ A$ . My step : easy to check that it is enough to prove the theorem in case $b=b^*$ . When we need to prove that $ϕ(ab) = \overline{ϕ(ab)}$ . But what's next? Could you take me some small hint?,"['c-star-algebras', 'functional-analysis', 'operator-algebras']"
3462030,Alternative definitions for Completeness in $\mathbb{R}^n$,"I'm reading my multivariable calculus lecture notes and have some questions $\dots$ Here are some Theorems mentioned in the notes: $$\boxed{\begin{align}\text{MCT (Monotone Convergence Theorem)}\\
\text{BST (Bounded Squence Theorem)}\end{align}}$$ Consider the case in $\mathbb{R}$ we have: $\text{Every bounded sequence in $\mathbb{R}$ has a subsequence that converges to a limit.}\tag*{BST}$ $\text{Every non-decreasing sequence of real numbers}\tag*{MCT}\\
\text{that is bounded above converges to a limit.}$ $\text{Any non-empty set of real numbers} \\\text{that has an upper bound must }\\\text{have a least upper bound in real numbers.}\tag*{Dedekind Completeness}$ You can check that these are all actually equivalences, in other words, that you can prove the completeness of $\mathbb{R}$ (the least upper bound property) from the MCT and that you can prove the MCT from the BST: $$\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$$ Hence in $\mathbb{R}$ the notion of completeness is equivalent to the Bounded Sequence Theorem. Finally notice that the statement of the Bounded Sequence Theorem no longer requires the definition of a least upper bound (or an order between vectors) and has a generalization to the Bounded Sequence Theorem in $\mathbb{R}^n$ as above. This allows us to define completeness of $\mathbb{R}^n$ in terms of the BST. The analogue of the completeness axiom in higher dimensions thus becomes: $\mathbb{R}^n$ is complete if Every bounded sequence in $\mathbb{R}^n$ has a convergent subsequence. $\dots$ We could also show that completeness is equivalent to the $\underline{\text{Intermediate Value Theorem}}$ , or to the statement that $\underline{\text{Every absolutely convergent sequence converges}}$ . I think it's saying since for $\mathbb{R}$ we have Dedekind Completeness (the least upper bound property) , which is hard to extend to $\mathbb{R}^n$ , but BST $\Leftrightarrow$ Completeness, so we can take BST as an alternative definition for Completeness. I believe $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ actually means: $$(\text{BST}\Leftrightarrow \text{MCT})\land(\text{BST}\Leftrightarrow\text{Completeness})\land(\text{MCT}\Leftrightarrow\text{Completeness})$$ And the note also listed the proof outline for $\mathbb{R}$ which might be: (Any cycle would work) $$(\text{BST}\Rightarrow\text{MCT})\land (\text{MCT}\Rightarrow\text{Completeness})\land(\text{Completeness}\Rightarrow\text{BST})$$ I'm not sure why it's not taking Cauchy Completeness for $\mathbb{R}^n$ , since $$\text{Cauchy Completeness}\Leftrightarrow\text{Dedekind Completeness}$$ Is MCT also an alternative definition for Completeness $?$ How does $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ generalise to $\mathbb{R}^n?$ My attempts: $\text{Every bounded sequence in $\mathbb{R}^n$ has a subsequence that converges to a limit.}\tag*{BST}$ $\text{Every non-decreasing sequence in $\mathbb{R}^n$}\tag*{MCT}\\
\text{that is bounded above converges to a limit.}$ $\text{Every Cauchy sequence in $\mathbb{R}^n$ converges}\tag*{Cauchy Completeness}$ In order to prove $\text{BST}\Leftrightarrow \text{MCT}\Leftrightarrow\text{Completeness}$ ,
maybe take the following proof outline: $$(\text{BST}\Rightarrow\text{Completeness})\land (\text{Completeness}\Rightarrow\text{MCT})\land(\text{MCT}\Rightarrow\text{BST})$$ For BST $\Rightarrow$ Completeness: First prove two lemmas $1.$ Every Cauchy sequence are bounded in $\mathbb{R}^n$ $2.$ Every Cauchy sequence in $\mathbb{R}^n$ has a convergent subsequce are convergent. Then we take arbitrary Cauchy sequence, apply lemma $1$ that it's bounded, then apply BST so it has a subsequence that converges to a limit, finally apply lemma $2$ have it converges, hence Completeness hold. Some details for each Lemma: Lemma $1$ : Cauchy sequence in $\mathbb{R}^n$ is bounded Proof. Assume $\{a_j\}_{j=1}^\infty$ is a Cauchy sequence $$\forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon)$$ Show it’s bounded $$\exists r>0,s.t.\forall j\in\mathbb{N},\Vert a_j\Vert <r$$ Let $S:=\{d\in\mathbb{R}:\exists i\in [1,k],s.t.\Vert 0-a_i\Vert= d\}$ Also $M:=\max(S)$ And $r:=M+\varepsilon$ $\underline{\text{Case} 1:j\le k}$ Then we have $$j\in[1,k]\Rightarrow \Vert 0-a_j\Vert \le M$$ $$\Rightarrow \Vert a_j\Vert =\Vert 0-a_j\Vert < M+\varepsilon=r$$ $$\Rightarrow \boxed{\Vert a_j\Vert <r}$$ $\underline{\text{Case} 2:j>k}$ Since $k\in[1,k]$ we have $$a_k\le M\Rightarrow\Vert 0-a_k\Vert \le M$$ And by assumption $$\Vert aj-a_k\Vert <\varepsilon$$ Together with Triangle inequality of norm in $\mathbb{R}^n$ implies $$\Vert a_j\Vert =\Vert 0-a_j\Vert \le\Vert 0-a_k\Vert +\Vert a_k-a_j\Vert <M+\varepsilon=r$$ $$\Rightarrow\boxed{\Vert a_j\Vert <r} \tag*{$\square$}$$ Lemma $2$ : If Any Cauchy sequence in $\mathbb{R}^n$ has a convergent subsequce then that Cauchy sequence is convergent. Proof. Assume $\{a_j\}_{j=1}^\infty$ is a Cauchy sequence in $\mathbb{R}^n$ that has a convergent subsequence $$\forall\varepsilon>0,\exists N\in\mathbb{N},s.t.\forall j,k\in\mathbb{N},(j,k\ge N\rightarrow\Vert a_j-a_k\Vert <\varepsilon)$$ $$\wedge\forall\varepsilon>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\varepsilon)$$ Show $\{a_j\}_{j=1}^\infty$ converges to the same point. $$\forall\varepsilon>0,\exists K> 0,s.t.\forall j\in\mathbb{N}(j\ge K\rightarrow\Vert a_{j}−L\Vert <\varepsilon)$$ By rearrange the assumption we have $$\forall\frac{\varepsilon}{2}>0,\exists N\in\mathbb{N},s.t.\forall j,i\in\mathbb{N},(j,i\ge N\rightarrow\Vert a_j-a_{j_i}\Vert < \frac{\varepsilon}{2})$$ $$\wedge\forall\frac{\varepsilon}{2}>0,\exists J> 0,s.t.\forall i\in\mathbb{N}(i\ge J\rightarrow\Vert a_{j_i}−L\Vert <\frac{\varepsilon}{2})$$ Let $K=\max\{N,J\}$ , so we can use both assumptions By Triangle inequality of norm in $\mathbb{R}^n$ implies $$\Vert a_j-L\Vert =\Vert a_j-a_{j_i}+a_{j_i}-L\Vert \le\Vert a_j-a_{j_i}\Vert +\Vert a_{j_i}-L\Vert <\varepsilon$$ $$\Rightarrow \boxed{\Vert a_j-L\Vert <\varepsilon}\tag*{$\square$}$$ Is the idea correct $?$ Thanks for your help.","['proof-verification', 'real-analysis', 'multivariable-calculus', 'definition', 'general-topology']"
3462061,Can every Borel set be written as a disjoint union of elements in the algebra of half open intervals?,"We know that the algebra $\mathcal{A}$ of finite disjoint unions of intervals of the form $(a,b]$ for $a, b \in \mathbb{R}$ generates the Borel $\sigma$ -algebra $\mathcal{B}_{\mathbb{R}}$ . Is it true that every Borel set $A \in \mathcal{B}_{\mathbb{R}}$ can be written as a countable disjoint union of elements in $\mathcal{A}$ ?","['measure-theory', 'real-analysis']"
3462085,What familiar group is G isomorphic to?,"Let $G$ be the quotient $F_2/\langle a^4,b^4,aba^{-1}b^{-1} \rangle.$ a) What is a simplified form of $ab^8a^5b^{10}$ ? b) What is a normal form for the elements of $G$ ? c) What familiar group is G isomorphic to? My attempt: The quotient is formed by the equivalence relations: $a^4 \equiv e, b^4 \equiv e,ab \equiv ba$ a) $ab^8a^5b^{10}=ab^4b^4aa^4b^4b^4b^2=a^2b^2.$ b) the normal form of elements is $a^ib^j$ , where $0 \leq i,j\leq 3$ , since if we have degree higher than 3, we can simplify it using the relations $a^4 \equiv e, b^4 \equiv e,ab \equiv ba.$ c) since there are 4 possible choices for $i$ and $j$ , I suppose it's isomorphic to $\mathbb Z_4 \times \mathbb Z_4.$ But I don't know how to formally prove that... How to define an mapping $\phi:G\rightarrow \mathbb Z_4 \times \mathbb Z_4$ and prove that it is actually isomorphism Can somebody check my attempt and help me out with part c)? Thanks in advance.",['abstract-algebra']
3462106,Eigenvalues of the hyperbolic Laplacian,"I am working on the following Linear Algebra problem: Let $k$ be a field, let $d$ be a positive integer, and let $P_d$ be the $k$ -vector space of polynomials of degree $\leq d$ in $k[x,y]$ . The hyperbolic Laplacian on polynomials of degree $d$ is the differential operator $D_d = -y^2(\frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2})$ . For each $d \leq 5$ , compute the eigenvalues of $D_d$ , and, for each eigenvalue, compute the dimension of the eigenspace. (Your answer will depend on $d$ .) Here is my work so far: Let $f(x,y) = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}x^iy^j \in k[x,y]$ be a $d$ -dimensional polynomial, where $a_{i,j} \in k$ . Then $D_d(f) = \lambda f$ $\Rightarrow -y^2(\frac{\partial^2f}{\partial x^2} + \frac{\partial^2f}{\partial y^2}) = \lambda f$ $\Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d i(1-i)a_{i,j}x^{i-2}y^{j+2} + \sum_{i = 0}^d \sum_{j = 0}^d j(1-j)a_{i,j}x^iy^j = \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j} \lambda x^iy^j$ $\Rightarrow \sum_{i = 0}^d \sum_{j = 0}^d a_{i,j}[i(1-i)x^{i-2}y^{j+2} + (j(1-j)-\lambda)x^iy^j] = 0$ From the above equation, I'm struggling with finding the solutions for the eigenvalues $\lambda$ and their corresponding eigenspace dimensions. How do I deconstruct the above equation for my solutions for $\lambda$ ? It seems a bit overwhelming. Thanks!","['linear-algebra', 'polynomials', 'eigenvalues-eigenvectors', 'vector-spaces']"
3462189,Is this tensor product of $ \mathbb Q(\sqrt { 2 }) $ a field?,"I am trying to prove that $\mathbb{Q}(\sqrt{2})\otimes_{\mathbb{Q}}\mathbb{Q}(\sqrt{2})$ is not a field, but I do not know which is the right isomorphism. I am thinking of $$\mathbb{Q}(\sqrt{2})[x]/(x^2-2)\cong\mathbb{Q}(\sqrt{2})\otimes_{\mathbb{Q}}\mathbb{Q}(\sqrt{2}).$$ Could someone help, please? Thank you so much.","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
3462196,Class groups and Iwasawa Theory,"In a paper by Iwasawa (On Cohomology Groups of Units in $\mathbb{Z}_p$ -Extensions, 1983) the following two facts have been mentioned without any explanation (or examples). I do not see how to justify these statements or construct examples. Let $k$ be a field of CM type and $K/k$ be the cyclotomic $\mathbb{Z}_p$ -extension. Let $\Gamma = Gal(K/k) \simeq
\mathbb{Z}_p$ and $C(p)$ be the $p$ -primary part of the ideal class
group of $K$ . 
Claim: Then, it is possible that $C(p)^{\Gamma}$ is infinite. Fix the ground field $k$ and find a $\mathbb{Z}_p$ -extension $K/k$ such that
the number of primes of $k$ that ramify in $K$ is minimum in the family of all $\mathbb{Z}_p$ -extensions over $k$ . Claim: Then, $C(p)^{\Gamma}$ is finite for this $K/
k$ .","['class-field-theory', 'number-theory', 'algebraic-number-theory']"
3462245,Is $\frac{1}{\alpha} \in \mathbb{Q}[\alpha]$ for irrational $\alpha$?,"I have been trying to pick up abstract algebra and just attempted an exercise from Landin's An Introduction to Algebraic Structures which asks to prove whether $\frac{1}{\pi} \in \mathbb{Q}[\pi]$ , and would like to ask a (slightly) more general question. Let $\alpha$ be an irrational number. Is $\frac1\alpha \in \mathbb{Q}[\alpha]$ ? $\mathbb{Q}[\alpha]$ being the ring of numbers of the form $a_0 +a_1\alpha+a_2\alpha^2\dots a_n\alpha^n$ with $a_i\in\mathbb{Q}$ . I'm really not sure where to begin, and haven't found a similar question on MSE (perhaps because the solution is simpler than I realize.) I am stuck on what we can conclude about $\frac{1}{\alpha}$ other than that it is an irrational number greater than $1$ . Clearly, $\alpha^k n \in \mathbb{Q}[\alpha]$ , but I'm reluctant to make any claims about $\frac{1}{\alpha}$ and $\alpha^k n$ , as I'm sure a solution would use other simpler methods. *edited title and parts of the body because $\alpha$ need not be less than one.","['ring-theory', 'irrational-numbers', 'abstract-algebra', 'polynomials']"
3462260,Conditional expected value variable change,"Assume that we have $X_1,X_2,X_3$ that are jointly Gaussian, $E(X_i)=0$ and with non-zero covariances. We also have: $$Y_1=X_1$$ $$Y_2=X_2−X_1$$ $$Y_3=X_3−X_2$$ With $0$ covariances among all $Y_i$ and $E(Y_i^2)=1$ Is the following operation valid: $$E(Y_3^2|X_1,X_2)=E(Y_3^2|Y_1,Y_2)=E(Y_3^2)=1$$ Also consider: $$E(Y_3^2|X_1,X_2)= E(X_3^2|X_1,X_2)-2E(X_3|X_1,X_2)X_2-X_2^2$$ Which should be impossible to solve with the current information. I know that in the case of jointly gaussian variables a correlation of $0$ implies independence and all $X_i$ are jointly gaussian but does this property extend to all the $Y_i$ ?","['expected-value', 'statistics', 'conditional-expectation', 'random-variables']"
3462265,Find the number of integral values of $x$ where $f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+...+(x-2401)^2|x-2500|$ has local minima.,Find the number of integral values of $x$ for which $f(x)=x^2|x-1|+(x-1)^2|x-4|+(x-4)^2|x-9|+.....+(x-2401)^2|x-2500|$ has local minima. My Attempt $$f(x)=\sum_{m=0}^{49}\left(x-m^2\right)^2\left|x-(m+1)^2\right|$$ It appears that the extrema would appear between $m^2$ and $(m+1)^2$ . So how to proceed after this.,"['maxima-minima', 'calculus', 'algebra-precalculus', 'real-analysis']"
3462286,proving that the function $f(x) = \lambda(S ∩ (S + x))$ is a continuous and $\lim_{x\to\infty} f(x) = 0$,"Let $S \subset \mathbb{R}$ be a Borel measurable subset with finite Lebesgue
measure. Show that the function $f(x) = \lambda(S ∩ (S + x))$ is a continuous
function of $x$ and that $\lim_{x\to\infty} f(x) = 0$ . $\textbf{My attempt:}$ I tried this but not sure and stuck (though I think in general I am in the right direction). I have to show that $\forall \epsilon>0, \forall x\in S$ , $\exists \delta>0$ s.t. if $|x-x_n|<\delta$ , then $|f(x)-f(x_n)|<\epsilon$ . So; assume that $|x-x_n|<\delta$ , fix $x\in S$ . I can write \begin{align}
f(x) 
& = \lambda(S ∩ (S + x)) =\int_\mathbb{R} \chi_{S ∩ (S + x)}(t)d\lambda(t)=\int_\mathbb{R} \chi_{S}(t) \chi_{(S + x)}(t)d\lambda(t)\\
& = \int_\mathbb{R} \chi_{S}(t) \chi_{S}(t-x)d\lambda(t)
\end{align} so \begin{align}
|f(x)-f(x_n)|
& =|\int_\mathbb{R} \chi_{S}(t) \chi_{S}(t-x)d\lambda(t)-\int_\mathbb{R} \chi_{S}(y) \chi_{S}(y-x_n)d\lambda(y)|\\
& = ...
\end{align} for the second part; \begin{align}
\lim_{x\to\infty}f(x)
& = \lim_{x\to\infty}\int_\mathbb{R} \chi_{S}(t) \chi_{S}(t-x)d\lambda(t)\\
& =\int_\mathbb{R} \chi_{S}(t) \lim_{x\to\infty}\chi_{S}(t-x)d\lambda(t) ~~~~ {DCT}\\
& = 0 ~~~~~~ \text{since} ~~~~ \chi_{S}(t-x)\to 0 ~~~~ \text{as} ~~~~x\to\infty
\end{align}","['measure-theory', 'lebesgue-measure', 'proof-verification', 'real-analysis', 'fubini-tonelli-theorems']"
3462372,"3D geometry-skew lines, distance of a point from a line","A straight line L  intersects perpendicularly both the line: $$\frac{(x+2)}{2} = \frac{(y+6)}{3}=\frac{(z-34)}{-10} $$ and the line: $$\frac{(x+6)}{4}=\frac{(y-7)}{-3}=\frac{(z-7)}{-2}$$ Then the square of the perpendicular distance of origin from L is I could find the shortest distance between the skew lines but I can't find the equation of line and also the position of origin. How is the origin related to this system, I have no idea. Please help.","['geometry', '3d']"
3462462,Calculate geodesics on SE(3),"I want wo calculate geodesics on SE(3). I'm not sure if my definition of a Riemannian metric on this manifold is correct. Any comments on my construction will be helpful thank you in advance. I know, that for SE(3) ist is not true, that oneparametersubgroups are geodesics https://mathoverflow.net/questions/81590/one-parameter-subgroup-and-geodesics-on-lie-group?rq=1 Here my thoughts:
To be able to talk about geodesics we have to define a Riemanian metric on SE(3). A Riemanian metric is in each point on the manifold a skalarproduct. For the identity element I can define a skalarprodukt by setting $g_{e}\left((v_{1},w_{1}),(v_{2},w_{2})\right):=\begin{pmatrix} v_{1} & w_{1} \end{pmatrix}\begin{pmatrix} A & B\\ B^\top & C \\ \end{pmatrix}\begin{pmatrix} v_{2} \\ w_{2} \end{pmatrix}$ where $(v_{1},w_{1}),(v_{2},w_{2})\in T_{e}G$ and $w$ the coordinates corresponding to the Rotation and $v$ the coordinates corresponding to the translation. I can use Left translation $L_{p}:x\mapsto px$ in order to define a Riemanian metric from this scalarproduct on the whole SE(3) by using the pullback $g_{p}((v_{1},w_{1}),(v_{2},w_{2})):=g_{e}(dL_{p^{-1}}((v_{1},w_{1})),dL_{p^{-1}}((v_{2},w_{2})))$ In order to calculate the geodesics I have to solve the second order differential equation called geodesic equation. Therefore I have to calculate the Christoffel symbols, therefore I have to calculate the coefficients of the metric $g_{ij}$ for any $p\in SE(3)$ . For $p=e=Id$ I already know by definition $(g_{e})_{ij}=\begin{pmatrix} A & B\\ B^\top & C \\ \end{pmatrix}$ I tried to use this article Pull-back metric to calculate the coefficients of the pullback metric:
This calculation looked as follows I use canonical coordinates on SE(3) that means local coordinates around the identity look like $x^{k}:=pr^{k}\circ \log$ where log is the Liegrouplogarithm and $pr^k$ the Projektion on the k-th coordinate. Around any other point, the local coordinates look like $y^{k}:=pr^{k}\circ \log\circ L_{p^{-1}}$ it holds $(g_{p})_{ij}=g_{p}(e_{i},e_{j})
=g_{e}(d_{p}L_{p^{-1}}(e_{i}),d_{p}L_{p^{-1}}(e_{j}))
=\frac{\partial (x^{k}\circ L_{p^{-1}})}{\partial y^{i}}\frac{\partial (x^{r}\circ L_{p^{-1}})}{\partial y^{j}}g_{e}(e_{k},e_{r}).$ Calculation of the derivatives: $\frac{\partial (x^{k}\circ L_{p^{-1}})}{\partial y^{i}}=\partial_{i}\left(x^{k}\circ L_{p^{-1}}\circ y^{-1}\right)=\partial_{i}\left(pr^{k}\circ \log\circ L_{p^{-1}}\circ L_{p}\circ \exp\right)=\delta_{ik}.$ inserting $(g_{p})_{ij}=\delta^{ik}\delta^{jr}(g_{e})_{kr}=(g_{e})_{ij}$ That would mean the coefficients are constant. Therefore the Christoffelsymbols would be zero, since for the calculation of the Christoffelsymbols I have to calculate the derivatives of g_{ij}. Then the geodesic equation would simplify to c''=0 that means the geodesics are one parameter subgroubs and this is wrong. Does anyone see my mistake?
 Thanks a lot!","['geodesic', 'geometry', 'lie-groups', 'differential-geometry']"
3462501,"If a random process $\{Y_t\}_{t\ge0}$ whose moment of first order is uniformly bounded, does $Y_t/t$ converges to $0$ a.s.?","I have problem in reading the paper in the proof Theorem 3.6 (in Page 14): http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.386.7227&rep=rep1&type=pdf I am puzzled here: it says $X(t)/t$ converges a.s. to $v$ , and $X(t)-R(t)$ is tight, then concluding that $R(t)/t$ converges a.s. to $v$ . I think the author want to say that he concluded $(X(t)-R(t))/t \rightarrow 0$ a.s. by the tightness, but I failed. There is one counterexample for uniformly bounded first order moment process $Y_n$ whose time is discrete and $Y_n/n$ doesn't converges to $0$ a.s.: $Y_n$ are mutually independent, and $P(Y_n=n)=1/n, P(Y_n=0)=1-1/n$ . Therefore, I think I need some property of the trajectory to have it, but the proof of the paper is really not clear to me. However, it is easy to see uniformly finite first moment obtains the convergence to $0$ in $L^1$ . It might cost you some time to read the paper, and it is very grateful for your help!","['stochastic-processes', 'probability-theory', 'probability']"
3462557,Is it necessarily true that $\lim\limits_{x \rightarrow \infty} f(x) = 0$?,Suppose $f : \Bbb R \longrightarrow \Bbb R$ be a continuous function such that $\int\limits_{0}^{\infty} f(x)\ dx$ exists finitely. If $\lim\limits_{x \rightarrow \infty} f(x)$ also exists finitely then is it necessarily true that $\lim\limits_{x \rightarrow \infty} f(x) = 0$ ? Any suggestion regarding this will be highly appreciated. Thank you very much for your valuable time.,"['limits', 'real-analysis']"
3462567,Characterizing the growth of the Fourier transform,"The Paley-Wiener theorem characterizes the speed of growth of the Fourier transform (say, of exponential type $A$ i.e. of growth essentially bounded by $exp(A \cdot \mathrm{Im}(z))$ ) in terms of the original function (smooth, supported in $(-A,A)$ ). In particular, a function compactly supported has its Fourier transform potentially growing exponentially. I would like to know if there are other theorems of this kind for non-compactly supported functions. In particular : what are the functions $f$ so that its Fourier transform is such that $\hat{f}(x)$ is dominated by $\exp(- A x)$ ? Thanks in advance, any reference is welcome.","['fourier-transform', 'fourier-analysis', 'functional-analysis']"
3462584,Similar matrices over $\mathbb{Z}/2\mathbb{Z}$,"Given the following matrices $P=\left( \begin{array}{rrr}
1 & -1 & 0 \\
0 & 2 & 5  \\
0 & 0 & 3 \\
\end{array}\right), Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
-1 & 4 & 0  \\
0 & 3 & 7 \\
\end{array}\right)$ , such that $P,Q \in M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , I have to find out whether the two matrices are similar or not. One problem I'm facing is that $\mathbb{Z}/2\mathbb{Z} = {0,1}$ , which implies that $-1,2,5,3,4,7 \notin \mathbb{Z}/2\mathbb{Z}$ , 
hence I'm assuming it is meant that $P=\left( \begin{array}{rrr}
1 & (-1\mod2) & 0 \\
0 & (2\mod2) & (5\mod2)  \\
0 & 0 & (3\mod2) \\
\end{array}\right)\\ Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
(-1\mod2) & (4\mod2) & 0  \\
0 & (3\mod2) & (7\mod2) \\
\end{array}\right)$ which would give us $P=\left( \begin{array}{rrr}
1 & 1 & 0 \\
0 & 0 & 1  \\
0 & 0 & 1 \\
\end{array}\right)\\ Q=\left( \begin{array}{rrr}
1 & 0 & 0 \\
1 & 0 & 0  \\
0 & 1 & 1 \\
\end{array}\right)$ But now the real problem arises, which is finding $R\in M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , such that $P=RQR^{-1}$ . One attempt was to try and solve a huge system of equations, arising ,for example, from $P=RQR^{-1}$ but that does not seem like the proper way to do this (it didn't work out). Another thing I noticed is that $P, Q$ have the same rank, however, I was unable to use this in order to find a solution. My last resort so far would be to simply try out various matrices in $M(3\times3, \mathbb{Z}/2\mathbb{Z})$ , but this can't be the only way to do this. Any help is appreciated, thanks.","['finite-fields', 'matrices', 'linear-algebra', 'linear-transformations', 'similar-matrices']"
3462855,How do I reference the only element in a singleton set?,"You can turn any element $n$ into a singleton set by adding braces, $\{n\}$ . Is there an inverse to this operation, such that if I know the set is a singleton set I can easily reference its element? For example, say I have the set $A = \{5,6,7,8\}$ , and then some process that iteratively eliminates all but one element of $A$ , and then I want to see what 10 plus the resulting element is, how could I write that? The only way I can think of is by referencing the first element by its index, e.g., $10 + A_1$ . Is there another way to do it though? In my case the elements of $A$ are already indices, so I would have to index an index, like $10 + x_{A_1}$ . Which is not the worst thing in the world, I was just wondering if there is a better way.","['elementary-set-theory', 'notation']"
3462877,Does there exist a sequence of positive numbers $a_{n}$ such that $\sum a_{n}$ is convergent and $\ln(n) a_{n}$ does not converge to zero?,"We have $a_{n}>0 $ and the series $ \sum a_{n} $ is convergent. 
Does it mean that $ a_{n}\times \ln{n}$ tends to 0, as n goes to infinity? I couldn't find any counterexample, no sequence of the form $\frac{1}{n^{m}}$ , where $m>1$ , works.
I also tried expressing $a_{n}$ as difference of two sums, but it didn't help. Is there a counterexample?","['sequences-and-series', 'real-analysis']"
3462900,Can Abel's test and Dirichlet test be used interchangably?,I have seen in most cases in series of constants and in series of functions that where Abel's test of convergence applies Dirichlet's test also applies and vice versa. This makes me to think whether the two tests are equivalent in the sense that  one is derivable from other.Is it true? Or there are cases where we can apply one but cannot apply other. Also is it so that one is more general and other is a corollary out of it.,"['metric-spaces', 'real-analysis', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence']"
3463016,Interpolation Capability of Deep Neural Networks of bounded height,"The Universal Approximation Theorem shows that deep neural networks can approximate any function in $C(\mathbb{R}^d,\mathbb{R}^n)$ uniformly on compacts.  I'm curious, can the collection of a neural networks with bounded height interpolate any finite set of points?","['machine-learning', 'statistics', 'neural-networks']"
3463102,Characteristic method to $u_t+au_x-u^2=1$,"Consider the following hyperbolic equation for $u=u(t,x)$ , $a\in\mathbb{R}$ $$ \frac{\partial u }{\partial t} + a\frac{\partial u }{\partial x} -u^2 = 1.  $$ I want to show that there is no global solution, that is, there exists a $t_*$ such that the solution $u$ goes to infinity as $t\rightarrow t_*$ . In order to do that I am thinking in showing that any two characteristic cruves intersec each other. My problem is to calculate the characteristic curves of the considered equation . Any help or idea is welcome! Thank you.","['characteristics', 'analysis', 'partial-differential-equations']"
3463103,"Showing that to any point on a curve there exists a number $ m \in \mathbb{Q} $ so that $(x,y)=(m^2-1,m^3-m)$","I have a question here, for which I don't have an idea how to prove it .. I want to show that to any rational point $ (x,y) \in \mathbb{ Q}^2 $ on a curve $$ C:= \{ (x,y) \in \mathbb{R}^2 | y^2=x^2+x^3 \} \subset \mathbb{R}^2 $$ there exists a number $m \in \mathbb{Q} $ so that $$ (x,y)= (m^2-1,m^3-m) $$ very thankful for any help!","['number-theory', 'elementary-number-theory', 'real-analysis']"
3463126,Is it possible to transform a cubic ODE to a set of quadratic ODEs?,"I am wondering if it is possible (or why it's not possible) to transform a set of cubic ODEs into a larger set of quadratic ODEs. Take as a simple example the ODE $$\dot{x}_1 = x_1^3$$ then can one rewrite this ODE in the form \begin{align}
\dot{x}_1 =& f_1(\mathbf{x})\\
\vdots\\
\dot{x}_n =& f_n(\mathbf{x})\\
\end{align} where each $f_i(\mathbf{x})$ is, at most, quadratic in the variables $\mathbf{x} = (x_1,...,x_k)$ ? A more general question is then: is it possible (and does there exist a general method) to transform a cubic ODE in $n$ dimensions to a quadratic ODE in $m>n$ dimensions?","['polynomials', 'ordinary-differential-equations']"
3463162,Uniform convergence of $f_n(x) := \frac{1}{1+n^2x^2}$ and $f_n(x) := \sum_{k=0}^n \frac{(x \sin x)^k}{k!}$,"I have to find out if the functions $f: \mathbb{R} \to \mathbb{R}$ converge uniformly on $M$ $$\text{(a) }f_n(x) := \frac{1}{1+n^2x^2}, M = [0, \infty)$$ $$\text{(b) }f_n(x) := \frac{1}{1+n^2x^2}, M = (0, \infty)$$ $$\text{(c) }f_n(x) := \frac{1}{1+n^2x^2}, M = [\delta, \infty) \text{ for } \delta > 0$$ $$\text{(d) }f_n(x) := \sum_{k=0}^n \frac{(x \sin x)^k}{k!}, M = [0, 1]$$ Regarding $\text{(a)}$ : For $x = 0$ we get a constant function, it converges towards $1$ . For $x != 0$ the denominator converges towards $\infty$ , so the function converges towards $0$ Since the function is continuous at $0$ (because $M = [0, \infty))$ , it converges uniformly Regarding $\text{(b)}$ : For $x = 0$ we get a constant function, it converges towards $1$ . For $x != 0$ the denominator converges towards $\infty$ , so the function converges towards $0$ Since the function is not continuous at $0$ (because $M = (0, \infty)) $ , it does not converge uniformly Regarding $\text{(c)}$ : Same reasoning as $\text{(b)}$ . --> Not converging uniformly Regarding $\text{(d)}$ : I think that it converges uniformly (because the sine function's derivative has absolute value of at most one to see that $|\sin(x) - \sin(y)| \le |x - y|.$ But I don't think that explanation works here. Can someone tell me if these are correct/incorrect?","['convergence-divergence', 'functions', 'uniform-convergence', 'real-analysis']"
3463207,Confusion about a probability question.,"$90$ students, including Joe and Jane, are to be split into three classes of equal size, and this is to be done at random. What is the probability that Joe and Jane end up in the same class? The answer given is $\frac{29}{89}$ . One explanation is ""put Joe in one class, then, Jane has a $\frac{29}{89}$ chance of choosing to be in the same class"". I also found the combinatorics answer $$\frac{3\binom{88}{28}}{\binom{90}{30}}.$$ I don't understand why it wouldn't just be $\frac 13$ . $9$ ways to split Jane and Joe up, and in $3$ of those $9$ ways, they are together. I don't see why the other $88$ students matter.","['combinatorics', 'probability']"
3463240,Proof for $\sum_{i=0}^n \frac{n \choose i}{{2n-1} \choose i} = 2$.,"I stumbled upon this strange equivalence for all integers $n>0$ : \begin{equation}
\sum_{i=0}^n \frac{n \choose i}{{2n-1} \choose i} = 2 \\
\end{equation} And tried numerous other integer triples $(a,b,c)$ to see if: $$\sum_{i=0}^n \frac{an \choose i}{{bn+c} \choose i}$$ Was ever constant for all $n$ , but could not find any. I am wondering how to prove the case for $(a,b,c)=(1,2-1)$ , and how to find other triplets. My attempt: I do not know many equivalences in combinatorics, but I began with: $$\frac{n \choose i}{{2n} \choose i} - \frac{n \choose {i+1}}{{2n} \choose i+1} = \frac{n!(2n-i)!}{(n-i)!2n!} - \frac{n!(2n-i-1)!}{(n-i-1)!2n!}$$ Mostly because I see a $2n-1$ appear, but am not sure if this is correct, and am not quite sure how to pick apart the right hand side further if it is.","['combinatorics', 'combinatorial-proofs']"
3463293,How far can I rewrite an equation before the derivative is not equivalent?,"I've run into a problem that I can't explain to my class. 
We are looking at the derivative for the equation $\frac{x}{y}+\frac{y}{x}=3y$ . We calculated it to be $\frac{y(x^2-y^2)}{x(3xy^2+x^2-y^2)}$ and we also verified it with Wolfram Alpha. A student thought about rewriting the original equation as $x^2+y^2=3xy^2$ by multiplying everything by $xy$ . I realize this adds to the domain and adds the point $(0,0)$ as a solution, but it's not differentiable at that point regardless as it's not continuous at that point. When we took the derivative of the rewritten equation and got $\frac{3y^2-2x}{2y-6xy}$ , which is not equivalent to our previous calculation. I can't figure out why the derivatives are so vastly different if all that was added to the original was a new point that's not differentiable to begin with. Any help is much appreciated!","['calculus', 'implicit-differentiation', 'derivatives']"
3463302,$ \pi(n) > \frac{n}{22 \ln(n)} $ simple proof?,"What is the easiest way to show that $$ n > 2 $$ $$ \pi(n) > \frac{n}{2 \ln(n)} $$ Where $\pi(n)$ is the prime counting function. I read a proof of the PNT with the zeta function but this statement is much weaker !! What is the shortest proof ? 
The simplest ?
The most elementary ? Do we use results from Mertens ? ( $\Pi ( 1 - 1/p) $ or $ \sum 1/p $ )
Do we need to use results of Mertens ?
Do we need to estimate $\sum \ln(p) $ ? How about the even weaker $$ \pi(n) > \frac{n}{22 \ln(n)} $$ Is that even easier ? Or not ?","['number-theory', 'elementary-number-theory', 'proof-writing', 'alternative-proof', 'prime-numbers']"
3463336,Proving a Wiener Process is a Martingale,"Let $W_t$ be a Wiener process. Let $X_t = W_t^2 − t$ . Show that { ${X_t
;t ≥ 0}$ } is a martingale with respect to the natural filtration. To prove that it is in fact a Martingale I must prove 2 properties: 1) $\mathbb E [|X_t|] < ∞ $ 2) $\mathbb E [X_t | F_s] = X_s$ for $ 0 ≤ s ≤ t$ For the first part I have: $$\mathbb E [|X_t|] = \mathbb E [|W_t^2 -t|] ≥ t \mathbb E [W_t^2] +t =2t $$ Although I am not sure if this is correct? For the second part I am unsure what the answer is, if anyone can help, I'd be very grateful.","['stochastic-processes', 'martingales', 'brownian-motion', 'probability-theory', 'filtrations']"
3463339,Proving that a list of perfect square numbers is complete,"Well, I have a number $n$ that is given by: $$n=1+12x^2\left(1+x\right)\tag1$$ I want to find $x\in\mathbb{Z}$ such that $n$ is a perfect square. I found the following solutions: $$\left(x,n\right)=\left\{\left(-1,1^2\right),\left(0,1^2\right),\left(1,5^2\right),\left(4,31^2\right),\left(6,55^2\right)\right\}\tag2$$ Is there a way to prove that this a complete set of solutions? So I mean that the solutions given in formula $(2)$ are the only ones? My work: We know that: $$
1 + 12x^2 \left(1+x \right) \ge 0
  \space \Longleftrightarrow \space
  x \ge -\frac{1+2^{-2/3}+2^{2/3}}{3}
  \approx -1.07245
\tag3
$$ So we know that for $x<-1$ there are definitely no solutions.","['number-theory', 'square-numbers', 'proof-writing', 'elliptic-curves']"
3463378,Gradient of $\mathcal{L}(W) = -\frac{n}{2}\left\{d\ln(2\pi)+\ln|C|+\mbox{Tr}(C^{-1}S)\right\}$ w.r.t. $W$,"I have the following function $\mathcal{L}(W)$ and I want to find the gradient with respect to $W$ , but I'm struggling with the matrix operations and derivations. $$\mathcal{L}(W) := -\frac{n}{2}\left\{ d\ln(2\pi) + \ln|C| + \mbox{Tr}(C^{-1}S) \right\}$$ where $C := WW^T + \sigma^2I$ . You can consider $S$ , which is positive definite, as constant. The gradient should be $$\nabla_W \mathcal{L}(W) = -n \left( C^{-1} S C^{-1} W - C^{-1} W \right)$$ but I cannot understand the steps that lead to it. If someone is interested, this is the probabilistic PCA, and you can find more information here .","['matrices', 'matrix-calculus', 'derivatives']"
3463399,Singular vectors of random Gaussian matrix with non-isotropic rows,"Suppose $G \in \mathbb{R}^{m \times n}$ has i.i.d. rows $g_i \sim \mathcal{N}(0, \Sigma)$ for some diagonal matrix $\Sigma = \text{diag}(\lambda_1,\dots,\lambda_n)$ where the diagonal entries satisfy $\lambda_1 \geqslant \dots \geqslant \lambda_n > 0$ . Can anything be said about the distribution of the columns of $V$ where $G = U\Lambda V^T$ is the singular value decomposition of $G$ ? The following question considers the case when the entries of $G$ are i.i.d. $\mathcal{N}(0,1)$ and shows that the singular vectors are uniformly distributed on a sphere of radius $1$ : Singular vector of random Gaussian matrix The proof capitalizes on the rotational invariance of the standard normal distribution but I don't think the same argument can be made here due to the form of $\Sigma$ . Any help or references would be greatly appreciated.","['svd', 'geometric-probability', 'linear-algebra', 'random-matrices', 'probability-theory']"
3463413,Given $2018 \times 4$ grids and tint them with red and blue. So that each row and each column...,"Given $2018 \times 4$ grids and tint them with red and blue. So that each row and each column has the same number of red and blue grids, respectively. Suppose there're $M$ ways to tint the grids with the mentioned requirement. Determine $M \pmod {2018}$ . Solution?: Each column can be colored such as $$(R,R,B,B),(R,B,R,B),(R,B,B,R),$$ $$(B,B,R,R),(B,R,B,R),(B,R,R,B)$$ Say the column colorings appears $a,b,c,a',b',c'$ respectively. Clearly we must have $a+b+c= a'+b'+c'$ since $R$ appears the same times as $B$ in first row. Simillary we have $a+b'+c' = a'+b+c$ which implies $a=a'$ . The same is true for $b=b'$ and $c=c'$ . So we have that $a+b+c=1009$ and thus $$M =\sum_{a+b+c=1009}\frac{2018!}{a!^2b!^2c!^2} \equiv ?\pmod{2018}$$ Edit: after Ross Millikan answer. My question here is: is this is correct and seeking for an alternative solution via generating functions .","['contest-math', 'proof-verification', 'combinatorics', 'discrete-mathematics', 'generating-functions']"
3463443,"$a,b$ and $c$ are roots of $x^3-x-1=0$. Find $a^{\frac{2}{3}}+b^{\frac{2}{3}}+c^{\frac{2}{3}}$","$a,b$ and $c$ are the roots of $$x^3-x-1=0$$ Find $$a^{\frac{2}{3}}+b^{\frac{2}{3}}+c^{\frac{2}{3}}$$ To solve this question I called the quantity A and I calculated $A^3$ . I have a feeling there is a better way. Question From Jalil Hajimir",['algebra-precalculus']
3463459,$n$-th power of stochastic exponential,"Let $X$ be a continuous semimartingale with $X_0 = 0$ and $n \geq 1$ . Prove that $$ \mathcal{E}(X)_t^n = \mathcal{E}(Y)_t $$ where $Y_t := nX_t + \frac{1}{2}n(n-1)\langle X \rangle_t$ My attempt If we take $f(\mathcal{E}(X)) = \mathcal{E}(X)^n$ , we can apply Ito's formula to $f$ to yield $$ f(\mathcal{E}(X)_t) - f(\mathcal{E}(X)_0) = \mathcal{E}(X)_t^n - 1 = \int_o^t \frac{\partial f(\mathcal{E}(X)_s)}{\partial(\mathcal{E}(X)_s)} d \mathcal{E}(X)_s  + \frac{1}{2} \int_0^t \frac{\partial^2 f(\mathcal{E}(X)_s)}{\partial^2(\mathcal{E}(X)_s)} d \langle \mathcal{E}(X) \rangle_s  = \int_0^t n \mathcal{E}_s^{n-1} d \mathcal{E}(X)_s + \frac{1}{2} \int_0^t n(n-1) \mathcal{E}(X)_s^{n-2} d\langle \mathcal{E}(X) \rangle_s$$ At this stage things look promising since the integer coefficients look like they are of the same form as what has to be proved. I also make use of one final trick : $d\mathcal{E}(X) = \mathcal{E}(X) d X$ as $\mathcal{E}(X)$ is a solution to the simple ""exponential SDE"", so I finally get $$ \cdots = \int_0^t n \mathcal{E}_s^{n} d X_s + \frac{1}{2} \int_0^t n(n-1) \mathcal{E}(X)_s^{n-2} d \langle \mathcal{E}(X) \rangle_s$$ But this is where I am stuck. Is this the right approach, or how do I proceed from here? Any help would be greatly appreciated. Update 1 I made some more progress, using the identity $$ \left\langle \int_0^t \xi d X \right\rangle = \int_0^t \xi^2 d \langle X \rangle $$ I can simplify the above to $$ \frac{1}{2} \int_0^t n(n-1) \mathcal{E}(X)_s^{n-2} d \langle \mathcal{E}(X) \rangle_s = \left\langle \frac{1}{2} \int_0^t n(n-1) \mathcal{E}^{\frac{n}{2} - 1}_s d \mathcal{E}(X)_s\right\rangle = \left\langle \frac{1}{2} \int_0^t n(n-1) \mathcal{E}^{\frac{n}{2}}_s d X_s \right\rangle = \frac{1}{2} \int_0^t n(n-1) \mathcal{E}^{n}_s d \langle X_s\rangle$$ At this stage I know I am very close, but I still don't see how to do it...","['stochastic-integrals', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
3463504,Why does the number of regions in a circle cut by chords joining $n+1$ points equal the number of regions in $\mathbb{R}^4$ cut by $n$ hyperplanes?,"It is well-known that if $n+1$ points are placed on a circle ( $n$ a nonnegative integer), the $\binom{n+1}{2}$ chords joining them cut the interior into $$1 + \binom{n+1}{2} + \binom{n+1}{4} = \sum_{k=0}^4 \binom{n}{k}$$ regions (in the general case where no three chords have a common intersection). This is also equal to the number of regions that $4$ -dimensional Euclidean space $\mathbb{R}^4$ is cut by $n$ general hyperplanes. (This is sequence A000127 .) (Equivalently, it is the number of regions cut by $n+1$ hyperplanes in $4$ -dimensional projective space $\mathbb{RP}^4$ .) The question: Is there a direct proof that these two numbers are the same, without explicitly counting them? For example, is there a natural bijection between the regions in the circle and the regions in $4$ -space?","['combinatorial-geometry', 'combinatorics']"
3463506,Rewriting expression to use at most one trigonometric function,I have the expression: $$41\sqrt{2}\cos(v) + 41\sqrt{2}\sin(v)$$ And I want to rewrite it like an expression in v ∈ R that contains at most one trigonometric function. What I have tried to do is: $$41\sqrt{2}(\cos(v) + \sin(v))$$ But now I don't know where to go from here. Is there a formula that I somehow can use to get further or can I move in the constants into the trigonometric functions sin and cos?,['trigonometry']
3463552,How would one show that $\textbf{Z}$ is countable?,"I am trying to show that the set of all integers $\mathbb{Z}$ is indeed countable which would mean that $|\mathbb{N}|=|\mathbb{Z}|$ . This would further imply that I have to find a bijection between the two sets, but I do not really know how to do that. I have tried with a function $f:\mathbb{N}\longrightarrow \mathbb{Z}$ such that all the odd numbers are mapped to the positive subset of $\mathbb{Z}$ , and all even numbers are mapped to the negative numbers. I think that this will produce a valid solution but I don't have the details yet, and I also do not really know how to account for zero in all of this. Any help on the matter would be much appreciated.","['elementary-set-theory', 'discrete-mathematics']"
3463562,How many 5 card hands do and don't contain the ace of clubs?,"I'm getting a bit confused here. I keep seeing two common answers for these types of situations: $51\choose5$ and $52\choose4$ . Which of these answer which question? Are there $51\choose5$ hands containing the ace of clubs, or are there $52\choose4$ (or neither)? Likewise, are there $51\choose5$ hands not containing the ace of clubs, or are there $52\choose4$ ? I'm really hoping one value answers one question, and the other value answers the other question, otherwise I'll be even more confused...","['discrete-mathematics', 'combinatorics', 'card-games', 'probability']"
3463597,Difficult probability question,"Four French teams make it to the final eight of the Champions League. The draw for the round is made at random. What is the probability that exactly one pairing has two french teams? ...
At first, I recognised that there could only either be 0 matching teams, 1 matching team, or two matching teams. If we denote french as $f$ and not french as $nf$ , we can see that with 0 matching teams, the order of the teams must be: $f$ v $nf$ , $f$ v $nf$ , $f$ v $nf$ , $f$ v $nf$ , with one matching team: $f$ v $f$ , $f$ v $nf$ , $f$ v $nf$ , $nf$ v $nf$ , and two matching teams: $f$ v $f$ , $f$ v $f$ , $nf$ v $nf$ , $nf$ v $nf$ , I then proceeded to calculate the probability of 0 matching teams and two matching teams occuring, and subtracting them from 1 to calculate the probability for 1 matching team. However, I was not sure what approach to take when calculating this probability. Thanks,
Field",['probability']
3463605,Intuitive explanation of why some autonomous differential equations go to infinity in finite time,"Take any differential equation of the form $$\frac{dy}{dx}=y^n$$ where $n > 1$ . The solution $y(x)$ will reach infinity at a finite value of $x$ . Assuming $y_0 =1 $ for all cases, here are a few examples: $$\frac{dy}{dx}=y^2$$ has the solution $$y=\frac{-1}{x-1}$$ which reaches its asymptote at $x=1$ . The DE $$\frac{dy}{dx}=y^{1.01}$$ has the solution $$y=\left(\frac{-100}{x-100}\right)^{100}$$ which reaches its asymptote at $x=100$ . If you take any DE of the form $$\frac{dy}{dx}=y^{1 + \epsilon}$$ where $\epsilon$ is a very small number, the solution is $$y=\left(\frac{-1}{\epsilon(x-\frac{1}{\epsilon})}\right)^{\epsilon^{-1}}$$ which eventually hits the vertical asymptote at the very large number $\frac{1}{\epsilon}$ This has always bugged me. Intuitively, one expects that the solutions to these equations will grow rapidly and aggressively, much faster than the exponential function. But it is not entirely obvious why they should reach an infinite value after a finite time, instead of say, grow like the Ackermann function or some other function that grows rapidly but stays strictly finite. Is there an intuitive argument for why these DEs are able to reach infinity in a finite timespan?","['infinity', 'asymptotics', 'ordinary-differential-equations', 'singularity']"
