question_id,title,body,tags
3615529,Is $\pi (x)=\operatorname{R}(x)-\sum_{\rho}\operatorname{R}(x^{\rho})$ correct at all?,"It should be the case that, in some appropriate sense $$\pi (x)\sim \operatorname{Ri}(x)-\sum_{\rho}\operatorname{Ri}(x^{\rho}) \tag*{(4)}$$ with $\operatorname{Ri}$ denoting the Riemann function defined: $$\operatorname{Ri}(x)=\sum_{m=1}^\infty \frac{\mu (m)}{m}\operatorname{li}\left(x^{\frac{1}{m}}\right). \tag*{(5)}$$ This relation $(4)$ has been called ""exact"" [in Ribenboim's The New Book of Prime Number Records ], yet we could not locate a proof in the literature; such a proof should be nontrivial, as the conditionally convergent series involved are problematic. In any case relation $(4)$ is quite accurate, and furthermore the Riemann function $\operatorname{Ri}$ can be calculated efficiently (...) The sum in $(4)$ over critical zeros is not absolutely convergent, and furthermore the phases of the summands interact in a frightfully complicated way. —from Journal of Computational and Applied Mathematics by Borwein et al. Of profound importance, Bernhard Riemann proved that the prime-counting function is exactly $$\pi (x)=\operatorname{R}(x)-\sum_{\rho}\operatorname{R}(x^{\rho})$$ where $$\operatorname{R}(x)=\sum_{n=1}^\infty \frac{\mu (n)}{n}\operatorname{li}\left(x^{\frac{1}{n}}\right),$$ (...) $\rho$ indexes every zero of the Riemann zeta function, and $\operatorname{li}\left(x^{\frac{1}{n}}\right)$ is not evaluated with a branch cut but instead considered as $\operatorname{Ei}\left(\frac{\rho}{n}\ln x\right)$ . Equivalently, if the trivial zeros are collected and the sum is taken only over the non-trivial zeros $\rho$ of the Riemann zeta function, then $\pi (x)$ may be written $$\pi (x)=\operatorname{R}(x)-\sum_{\rho}\operatorname{R}(x^{\rho})-\frac{1}{\ln x}+\frac{1}{\pi}\arctan\frac{\pi}{\ln x}.$$ —from Wikipedia's Prime counting function article Questions: According to Borwein and Ribenboim, the index in $(4)$ should run only over non-trivial zeros . According to Wikipedia, the index in $(4)$ should run over all zeros . Wikipedia states that if the sum runs only over non-trivial zeros, then we add the $\ln$ and $\arctan$ terms, which is even more confusing. So what's true? I'm pretty sure that Riemann did not prove the formula $(4)$ . He only proposed a ""weaker"" form of it, namely $$\pi (x)=\sum_{m=1}^\infty \frac{\mu (m)}{m}J\left(x^{\frac{1}{m}}\right)$$ where $$J(x)=\operatorname{li}(x)-\sum_{\rho}\operatorname{li}\left(x^{\rho}\right)+\int_x^\infty \frac{dt}{t(t^2-1)\ln t}-\ln 2$$ and where $\rho$ runs over all non-trivial zeros. The formula was proven by Mangoldt, not Riemann. The Wikipedia article is wrong in that historical fact, isn't it? Even though the proof of $(4)$ is nowhere to be found in the literature, Raymond Manzoni provided a partial proof here: Two Representations of the Prime Counting Function .
I call it partial because it is unknown whether the series converges at all: How could that be settled down? Note: When I refer to the formula $(4)$ in this question, I assume $=$ instead of $\sim$ .","['complex-analysis', 'convergence-divergence', 'riemann-zeta', 'prime-numbers']"
3615587,Knife inside the square [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question A square of side $n$ is drawn in the sand. A knife of length $k$ is thrown into the square. The tip of the knife lands inside the square, and then the knife falls down to the ground in a random direction. The location of the tip of the knife is uniformly distributed within the square, and the direction in which the knife falls to the ground is uniformly distributed in $(0,2\pi]$ . What is the probability that the entire length of the knife now lies inside the square? What happens when we replace the square with a circle of diameter $n$ ?","['geometry', 'probability']"
3615597,Prove that $\{A \in M_{2x2}: \text{rank}(A)=1\}$ is a 3-submainfold,"First of all, English is not my native language so I may use some wrong terms, if so, let me know. I'm studying submanifolds, and I need to prove that $$N:=\{A \in M_{2x2}: \text{rank}(A)=1\}$$ is a submanifold of $M_{2x2}$ with dimension 3. The only way I can think of is defining $f(A)=\det(A)$ so that if $N$ were $N=f^{-1}(\{0\})$ and $\nabla f(p)\neq 0 \enspace\forall p \in N$ , then this would be proved. However, that's obviously not the case, as $0\in f^{-1}(\{0\}), \enspace 0\notin N$ and $\nabla f(0)=0$ . I have tried to think any other function to solve this, for example $g(x, y, z, w) = (x*z - y*w, x^2+y^2+z^2+w^2)$ so that $N=g^{-1}(\{(0, x): x\neq 0\})$ but that proves nothing. Is there any function I could use to solve this? Or what method should I use? Thanks a lot!!","['submanifold', 'differential-geometry']"
3615603,Motivation/application of the characterisation of separable Hilbert spaces (and/or Sobolev spaces) to PDEs,"I recently came across this question in the context of a course on functional analysis. This question was posed by a friend of mine, and the Wikipedia pages and existing MO threads are too dense for me to understand. Some SE threads we've looked at include this , this , and this - I feel none of these really answer the essence behind our question (the last one comes close but only briefly touches on the question here). It is a basic result that separable Hilbert spaces are characterised by the existence of a countable basis, from which we can see that any (infinite-dimensional) separable Hilbert space is isometrically isomorphic to $\ell^2$ . What we are wondering is why this result is helpful - we have tried to explain the build up to that below. Classical PDE  systems assume that the defining equations are smooth, but it turns out that this is a really strong condition to enforce (for example, take a square wave in the wave equation). By relaxing this assumption, there are certain measure theoretic issues that we run into, and so we instead have to consider the equations in their equivalence classes (e.g. in $L^2$ , where for example the square wave is differentiable almost everywhere). In particular, the subspaces that these solutions live in are called Sobolev spaces, which are themselves Hilbert spaces. It turns out that for some values of $p$ , the Sobolev spaces are also separable, and so all the results from functional analysis can be applied to show existence and uniqueness of solutions to these linear PDEs. Additionally, it turns out that the same thing works for non-integral $p$ (Bessel spaces) which are also Hilbert space when $p = 2$ . The specific question that we still don't have a good answer to is Why do we care about separable Hilbert spaces being equivalent to $\ell^2$ ? What property of $\ell^2$ makes it so useful in solving systems of PDEs? It appears that the square summable sequences look similar to the Fourier series which are used to define Sobolev spaces, but this link is not made clear anywhere that we can find. In addition, even with such a link, we are unable to find an explanation for why/how this allows us to solve certain systems of PDEs. A good answer for this would be one which explains this link and motivation (to a background strong in algebra and Riemannian geometry and understanding graduate-level functional analysis and measure theory, but with a much more elementary foundation of PDEs), or otherwise recommends some light, introductory resources and a short summary of what is really going on here.","['reference-request', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'soft-question']"
3615702,Smoothness is the same as regularity at closed points for a finite type scheme of pure dimension $d$ over an algebraically closed field [duplicate],"This question already has an answer here : Problem with Jacobian criterion and regular local ring (1 answer) Closed 4 years ago . This is exercise 12.2.I in Vakil's notes: Suppose $X$ is a finite type scheme of pure dimension $d$ over an
  algebraically closed field $k$ . Show that $X$ is regular at its closed points if and
  only if it is smooth. For the forward direction, for a closed point $p$ , $\operatorname{dim}\mathcal{O}_{X,p}=d$ . From Exercise 12.1.G, one has that the Zasiki cotangent space at a closed point is given by the cokernel of the Jacobian matrix. If $X$ is regular at closed points, then corank of the Jacobian matrix is $d$ at all closed points, hence by Exercise 12.2.H, the Jacobian matrix has corank $d$ at all points. This shows the smoothness. Conversely, smoothness of $X$ implies the Jacobian has corank $d$ at all points, in particular closed points. Is the result true for $k$ not algebraically closed? I don't see why it's required that $k$ is algebraically closed.",['algebraic-geometry']
3615721,Choosing Bounds of Integration for a Triangle,"Suppose I have a region of integration boded by $x+y \le 8$ and $0 \le y \le x$ . I have graphed the bounds, and they form a right triangle with its hypotenuse following the $x$ -axis from $[0,8]$ and the right angle at $(4,4)$ . For clarity, the region I am looking at is the intersection of the red and green regions. I am having trouble figuring out what the bounds should be. I was thinking of splitting up the double integral into two double integrals. $\int^4_{0} \int^{y=x}_{y=0} f(x) dy dx + \int^8_{4} \int^{y=8-x}_{y=0} f(x) dy dx$ Is this correct?","['integration', 'multivariable-calculus', 'definite-integrals', 'bounds-of-integration']"
3615742,"If a,b,c be roots of $2x^3+x^2+x-1=0$ show that some expression is equal to 16.","If $a,b,c$ are roots of $2x^3+x^2+x-1=0,$ show that $$\bigg(\frac{1}{b^3}+\frac{1}{c^3}-\frac{1}{a^3}\bigg)\bigg(\frac{1}{c^3}+\frac{1}{a^3}-\frac{1}{b^3}\bigg)\bigg(\frac{1}{a^3}+\frac{1}{b^3}-\frac{1}{c^3}\bigg)=16$$ My attempt: Let $\frac{1}{a}=p,\frac{1}{b}=q,\frac{1}{c}=r$ $p+q+r=1$ $pqr=2$ $$pq+qr+rp=\frac{1}{ab}+\frac{1}{bc}+\frac{1}{ca}$$ $$=\frac{ab+bc+b^2}{(abc)^2}$$ $$=4\bigg(\frac{1}{pq}+\frac{1}{qr}+\frac{1}{q^2}\bigg)$$ $$pq+qr+rp=4\bigg(\frac{pq+qr+rp}{pq^2r}\bigg)$$ $$pq^2r=4$$ $$\implies q=2 \implies b=\frac{1}{2}$$ So p, r are roots of $x^2+x+1=0$ $\implies p^3=q^3=1$ But this condition gives a different value of the required expression, so what am I doing wrong? Please tell me the right solution.","['algebra-precalculus', 'solution-verification', 'roots', 'symmetric-polynomials']"
3615783,"What, if anything, is a square radian?","My 1st year Mathematics BSc course notes on circular motion use \begin{align}
\frac{d}{dt}(\sin\theta)
&=\frac{d}{dt}(\sin(\omega t))\tag{1.1}\\
&=\omega\cos(\omega t),\tag{1.2}\\
\frac{d^2}{dt^2}(\sin\theta)
&=\frac{d^2}{dt^2}(\sin(\omega t))\tag{2.1}\\
&=\frac{d}{dt}(\omega\cos(\omega t))\tag{2.2}\\
&=-\omega^2\cos(\omega t),\tag{2.3}
\end{align} where $\omega$ is angular speed, a scalar, with units $\text{rad}\cdot s^{-1}$ ; $t$ is time, a scalar, with units $s$ . From this it seems to me to follow that $\omega^2$ has units $(\text{rad}\cdot s^{-1})^2=\text{rad}^2\cdot s^{-2}$ . Is that right, and, if so, does a square radian have a physical meaning?","['dimensional-analysis', 'classical-mechanics', 'circles', 'geometry']"
3615821,Existence of regular hexagons in 3D space with integer-coordinate vertices,"I am intrigued by the existence of regular hexagons with vertices in $\mathbb{Z}^3$ . A simple example is the hexagon with vertices $$(0,−1,−1),(1,0,−1),(1,1,0), 
(0,1,1),(−1,0,1),(−1,−1,0).$$ Can anyone think of a more exotic (less obvious) example with a larger side length? i.e. one with a greater variety of entries for the $x$ , $y$ and $z$ coordinates. Here we only have $3$ different entries: $-1, 1, 0$ . I'd be interested to have a look at some more examples.","['geometry', '3d']"
3615836,Semisimple Lie algebra and Jacobson radical,"In the theory of Lie algebras, the radical $\mathrm{rad} (\mathfrak{g})$ of a Lie algebra $\mathfrak{g}$ is defined to be a (the) maximal solvable ideal of $\mathfrak{g}$ , and the Lie algebra $\mathfrak{g}$ is said to be semisimple if $\mathrm{rad} (\mathfrak{g}) = 0$ . On the other hand, in the theory of associative algebras, the Jacobson radical $\mathrm{rad} (A)$ of an algebra $A$ is the intersection of all maximal (left) ideal of $A$ , and the algebra $A$ is semisimple if $A$ is artinian and $\mathrm{rad} (A) = 0$ . (A semisimple algebra is a semisimple module (direct sum of simple modules) over itself.) Then, there rises two questions to me: Is the universal enveloping algebra $U (\mathfrak{g})$ artinian? Do this two kinds of semisimplicity coincide; that is, $\mathfrak{g}$ is semisimple iff $U (\mathfrak{g})$ is semisimple? If not, under which circumstances can we deduce the equivalence of semisimplicity?","['semisimple-lie-algebras', 'abstract-algebra', 'representation-theory', 'lie-algebras']"
3615897,"Density of $X_1 + \cdots+X_n$ when $X_i$'s are independent $U(-1,1)$ variables [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question How can we show for independent random variables uniformly distributed over $(1,-1)$ that $X_1 + \cdots+X_n$ has density $$\pi^{-1}\int^\infty_0 \left(\frac{\sin t}{t}\right)^n \cos tx \; dt \textrm{ for }n \geq 2\text{?} $$ This is   problem 26.6 from Billingsley  ""Chapter : CONVERGENCE OF DISTRIBUTIONS"" (3rd edition). I would really appreciate it, if you show it analytically .","['probability-distributions', 'uniform-distribution', 'probability-theory']"
3615926,How to solve the following radical equation?$\sqrt{2+\sqrt{2-\sqrt{2+\sqrt{2-x}}}} =x$,How to solve the following radical equation? $$\sqrt{2+\sqrt{2-\sqrt{2+\sqrt{2-x}}}} =x$$ I'm looking for an easy way to solve it.,"['algebra-precalculus', 'radicals']"
3616036,Show that $ Χ_n \rightarrow X$ (Challenging Problem?),"Suppose that $X_n$ assumes as values $γ_n + kδ_n$ , $k= 0 $ , $\pm 1 $ , ..., where $δ_n >0$ . Suppose that $δ_n \rightarrow 0$ and that , if $k_n$ is an integer varying with $n$ in such a way 
that $γ_n + k_n δ_n  \rightarrow x $ , then $P[ X_n = γ_n + k_n δ_n]{δ_n}^{-1} \rightarrow f(x)$ ,
where $f$ is the denstiy of a random variable $X$ . Show that $    Χ_n \rightarrow X$ . $$$$ Attempt : We construct a probability space and random
variables $U , X_1 , X_2 , X_3$ .. defined on that space with each $X_n$ having the distribution specified in the problem and with $U$ ~ Uniform $(0,1)$ independent of $X_n$ for all $n$ . Let $Y_n = X_n + δ_nU$ How can we: (i)Calculate  the density $f_n$ of $Y_n$ and then (ii)Show that $f_n(x) \rightarrow  f(x)$ as $n \rightarrow  \infty$","['self-learning', 'probability-theory', 'weak-convergence']"
3616090,Percolation theory critical density simple proof!,"Is there a simpler proof for the existence of infinite connected component in 2D lattice (percolation theory) if the probability of connection exceeds a critical threshold? Currently, I am reading the proof by Harry Kesten here . I am totally lost as I do not have a background to deal with that level of mathematics. I am hoping over the past 40 years someone might have come up with an easier proof. Any help is appreciated!","['network', 'graph-theory', 'probability-theory', 'percolation', 'probability']"
3616126,$\sigma$-algebra generated by a stochastic process,Let $x_t:\Omega \to \mathbb{R}$ a random variable and $\{x_t\}:\Omega \to \mathbb{R}^\mathbb{Z}$ a stochastic process. Can someone explain to me the notion of the $\sigma$ -algebra generated by a stochastic process? I understand that of the $\sigma$ -algebra generated by the random variable $x_t$ but I am struggling to extend it to stochastic processes. An example (maybe with discrete $\Omega$ ) would help too.,"['time-series', 'stochastic-processes', 'measure-theory']"
3616136,"Find all functions $f$ such that for all positive integers $x$, $y$, $f(xy)+f(x+y)=f(x)f(y)+1$.","Find all functions $f$ such that for all positive integers $x$ , $y$ , $$f(xy)+f(x+y)=f(x)f(y)+1\,.$$ I'm pretty sure that the only function that satisfies this equation in $f(x)=1$ . By considering $f$ as a sequence, I managed to show that $\forall x\in\mathbb{N}_{>0}$ , $$f(x)=\left(f(1)-\frac{1}{2-f(1)}\right)(f(1)-1)^{x-1}+\frac{1}{2-f(1)}\;,$$ but this doesn't really help me since I can't find the value of $f(1)$ .","['contest-math', 'functional-equations', 'functions', 'recursion']"
3616137,Critical strip is strict,"It is a well known fact that the Riemann $\zeta$ -function satisfies a functional equation , has all its non-trivial zeros in the critical strip $\{ s \in \mathbb{C} \mid 0 \leq \textrm{Re}(s) \leq 1\}$ , and that the zeros in this domain are symmetric in this domain, in the sense that for all $s$ in the critical strip: $\zeta(s)=0 \iff \zeta(1-s)=0$ . The above fact is proved in most introductions to analytic number theory or $L$ -series, such as Neukirch, Zagier, Lang etc. Now I have been told that one can prove that the critical strip is strict , i.e. $\zeta(s) \neq 0$ for all $s \in \{ s \in \mathbb{C} \mid \textrm{Re}(s)=1\}$ , which by the function implies that $\zeta(s)$ is also non-zero on the $\textrm{Re}(s)=0$ line. As far as I understand it, the above is quite a profound statement and is equivalent to the prime number theorem (?) My number theory professor mentioned this in passing. He is gone now and I have been unable to find a proof of the statement in the literature. So my question is: Does anyone know a textbook proof of the strictness of the critical strip, or at least a place in the literature where it is discussed? Many thanks.","['algebraic-number-theory', 'number-theory', 'reference-request', 'complex-analysis', 'abstract-algebra']"
3616273,How to compute the image of the fundamental group of a covering space of $S^1 \vee S^1$?,"In Allen Hather's Algebraic Topology , there is a section on the Covering Spaces of $S^1 \vee S^1$ . The following picture is inside. How do you rigorously compute $(1)$ - $(9)$ ? If we take $E$ to be our covering space and $e_0$ to the the dark vertex, then we need to compute $\pi_1(E,e_0)$ in order to know the number of generators for our covering space. This can be computed using van Kampen's Theorem. Then let $p:(E,e_0) \to (S^1 \vee S^1, x_0)$ be our covering map. We wish to find the presentation of the image $p_{*}(\pi_1(E,e_0))$ . What is the general method to compute $p_{*}(\pi_1(E,e_0))$ ? There is a very similar question here Covering spaces of $S^1 \vee S^1$ , but the answers do not suffice.","['fundamental-groups', 'abstract-algebra', 'algebraic-topology']"
3616279,By definition of $\mathbb E(X\mid \sigma(Y))$ calculate $\mathbb E(X\mid Y=y)$ when $X$and $Y$ are discrete random variables.,"By definition of $\mathbb E(X\mid \sigma(Y))$ i want to show $$\mathbb E(X\mid Y=y)=\sum x \mathbb P(X=x\mid Y=y)$$ when $X$ and $Y$ are jointly discrete  random variables.( Absolutely continues  case is here ).  I want to know is my steps right or no. I need some explanation in the steps from start to end(declared with sign ?) let $Y$ is a discrete random variable that take values $\{ a_1 ,a_2,\cdots ,a_n\}$ . So $\sigma(Y)=\sigma(\{a_1\},\cdots , \{ a_n\})$ (??) By definition $\forall A\in \sigma(Y)$ $$ \mathbb E \left( \mathbb E \color{red}{(}X\mid \sigma(Y)\color{red}{)}1_A\right)
=\mathbb E(X1_A)$$ since $A\in \sigma(Y)$ so $1_A$ is a function of $Y$ so i think i can write $$\mathbb E \left( \mathbb E \color{red}{(}X|\sigma(Y)\color{red}{)}1_B(Y)\right)=\mathbb E(X1_B(Y))$$ $$RHS=\mathbb E(X1_B(Y))=\sum_{y\in B} \sum_{x} x \mathbb P(X=x,Y=y) $$ $$LHS=\mathbb E \left( \mathbb E \color{red}{(}X\mid \sigma(Y)\color{red}{)}1_B(Y)\right)
=E \left( \mathbb E \color{red}{(}X\mid Y\color{red}{)}1_B(Y)\right)
$$ $$=\mathbb E \left( g(Y) 1_B(Y) \right)=\sum_{y\in B} g(y) P(Y=y)
=\sum_{y\in B} \mathbb E(X\mid Y=y) P(Y=y)
$$ By unifying $LHS$ and $RHS$ , $\forall B$ $$\sum_{y\in B}  \mathbb E(X\mid Y=y)  \mathbb P(Y=y)=\sum_{y\in B} \sum_{x} x \mathbb P(X=x,Y=y)$$ I think(??) for $y\in \{ a_1 ,a_2,\cdots ,a_n\}$ i can write (since equation is for all $B$ ??) $$ \mathbb E(X\mid Y=y)  \mathbb P(Y=y)= \sum_{x} x \mathbb P(X=x,Y=y)$$ so for $y\in \{ a_1 ,a_2,\cdots ,a_n\}$ $$ \mathbb E(X\mid Y=y) = \sum_{x} x \frac{ \mathbb P(X=x,Y=y)}{ \mathbb P(Y=y)}= \sum_{x} x \mathbb P(X=x\mid Y=y)$$ . This proof was for finite support(like $Y$ is binomial), is this valid
for countable support ?(like Poisson?). Thanks in advance for any help you are able to provide or any clarification.","['measure-theory', 'conditional-expectation', 'real-analysis', 'expected-value', 'probability-theory']"
3616290,A topology that is finer than a metrizable topology is also metrizable?,"If $\tau_{1}$ and $\tau_{2}$ are two topologies on a set $\Omega$ such that $\tau_{1}$ is weaker than $\tau_{2}$ (i.e. $\tau_{1}\subset\tau_{2}$ ) and $\tau_{1}$ is metrizable, is it then true that $\tau_{2}$ is also metrizable? My guess is that it is not true, since we do not know what the sets in $\tau_{2}$ look like, let alone whether they can contain 'open metric balls' or not. But maybe it is possible to adapt the metric so that the sets in $\tau_{2}$ can contain 'open metric balls'. Also, it may be the case that the converse is easier to work with: If $\tau_{2}$ is not metrizable, then $\tau_{1}$ is not metrizable. I find it very hard to think of any (counter)examples. Any help or hints would be greatly appreciated!","['metrizability', 'general-topology', 'metric-spaces']"
3616374,Any probability density can be written as a sum of Gaussian,"In Gaussian sum mixture model, any probability density function (pdf) can be written as a sum of Gaussian. Lets consider here any $n$ -dimensional vector $x$ follows Gaussian distribution with mean $\hat{x} \in \mathbb{R}^n$ , and covariance $P \in \mathbb{R}^{n\times n}$ can be written as \begin{equation}
p(x) = \mathcal{N}(x;\, \hat{x},\, P) = \sum_{i=1}^{N_f} w_{i} \mathcal{N}(x;\, \hat{x}_i, \, P_i),   
\end{equation} where $w_i$ represents the weight of the $i$ -th Gaussian pdf, and $\sum_{i=1}^{N_f} w_{i}=1$ . $\hat{x}_i$ and $P_i$ are the $i$ -th mean and covariance respectively. Here my specific question is how to generate $w_i$ , $\hat{x}_i$ and $P_i$ for any $N_f$ . Any kind of suggestion will be of great help.","['statistics', 'estimation', 'gaussian', 'probability-theory', 'probability']"
3616376,Finding Radius and period time of a limit cycle using Melnikov Integration,"The system of equations I am working on is the following: $$
\begin{align}
\dot{x}& = y \\
\dot{y}& = -\mu(x^2 + ax^4 - 1)y - x
\end{align}
$$ The question asks first to find the Hamiltonian of the system at $\mu = 0$ which I evaluate to be: $$
\begin{equation}
H(x,y) = \frac{y^2 - x^2}{2}
\end{equation}
$$ The proceeding parts are the ones I am having trouble with. This is the body of the question: ""Find an explicit expression for the trajectories solving this Hamiltonian
system. Choose suitable initial conditions having in mind
that in the next step you will integrate over a closed orbit."" Is this done by solving the system of equations when $ \mu = 0$ ? Doing so with initial conditions x(0) = 1 and y(0) = 1 results in: $$
\begin{align}
x(t)& = cos(t) + sin(t)\\
y(t)& = cos(t) - sin(t)
\end{align}
$$ After plugging this into the Hamiltonian and simplifying I arrive at: $$
\begin{equation}
H(x,y) = -sin(2t)
\end{equation}
$$ The next part which involves using the Melnikov method is as follows: ""Now consider a small positive $\mu$ . To determine the radius R and
period time T to lowest (zeroth) order in $\mu$ , evaluate the change $\Delta H$ in the Hamiltonian H(x; y) as you follow a trajectory governed
by the dynamics in the system of equations. Show that H must vanish
after following a limit-cycle trajectory one lap. Use this fact to find the radius R as a function of a."" When H completes one lap I assume that it goes from 0 to $\pi$ and $H = 0$ at $t = \pi$ . I am not sure how to find the radius and the period after this step. For example, I don't know how to handle $\mu$ being non-zero. Could someone give me some pointers as to how this should be done or if what I have done so far is correct?","['nonlinear-system', 'ordinary-differential-equations', 'dynamical-systems']"
3616390,Characterization of the measurable sets in the $\sigma$-algebra generated by cylindrical sets on $\mathbb{R}^{\mathbb{R}}$,"Let $\tilde\Omega_\mathbb{R}$ denote the collection of all functions $\tilde\omega : \mathbb{R} \to \mathbb{R}$ and $\mathcal{B}_\mathbb{R}$ the $\sigma$ -algebra generated by all cylindrical subsets of $\tilde\Omega_\mathbb{R}$ .  Similarly, let $\tilde\Omega_\mathbb{Z^+}$ denote the collection of all functions $\tilde\omega : \mathbb{Z^+} \to \mathbb{R}$ and $\mathcal{B}_\mathbb{Z^+}$ the $\sigma$ -algebra generated by all cylindrical subsets of $\tilde\Omega_{\mathbb{Z^+}}$ . My question concerns the following characterization of the sets in $\mathcal{B}_\mathbb{R}$ : $S \in \mathcal{B}_\mathbb{R}$ if and only if there exists $B \in \mathcal{B}_{\mathbb{Z^+}}$ and an infinite sequence of real numbers $t_1, t_2, \dots$ such that $$
 S = \left\{ \tilde\omega \in \tilde\Omega_\mathbb{R} \, : \, \big( \tilde\omega(t_n) \big)_{n \in \mathbb{N}} \in B \right\}
$$ I found this question that asks how to prove this characterization, and in the accepted answer an outline for the proof is provided.  The first step is to show that the collection of sets $\Sigma$ defined by $$
 \Sigma := \Big\{ S \subseteq \tilde\Omega_\mathbb{R} \, : \, \exists B \in \mathcal{B}_\mathbb{Z^+}, \, \{t_n\}_{n \in \mathbb{N}} \subset \mathbb{R} \text{ such that } S = \left\{ \tilde\omega \in \tilde\Omega_\mathbb{R} \, : \, \big( \tilde\omega(t_n) \big)_{n \in \mathbb{N}} \in B  \right\}  \Big\}
$$ is a $\sigma$ -algebra. I am having difficulty in proving that $\Sigma$ is closed under countable unions. The following is my attempt.  I thought it would be helpful to think in terms of projections.  Specifically, for a sequence of real numbers $\{t_n\}_{n \in \mathbb{N}}$ , define $\pi_{\{t_n\}_{n\in\mathbb{N}}} : \tilde\Omega_\mathbb{R} \to \tilde\Omega_\mathbb{Z^+}$ by $\pi_{\{t_n\}_{n\in\mathbb{N}}}(\tilde\omega):=\big( \tilde\omega(t_n) \big)_{n\in\mathbb{N}}$ .  Then $S \in \Sigma$ iff there exists $B \in \mathcal{B}_{\mathbb{Z}^+}$ and a projection $\pi_{\{t_n\}_{n\in\mathbb{N}}}$ such that $S = \pi_{\{t_n\}_{n\in\mathbb{N}}}^{-1}(B)$ .  Moreover, $\Sigma$ is the family of all such pre-images where $B$ and the sequence of times range over all possibilities: $$
\Sigma := \Big\{ S \subseteq \tilde\Omega_\mathbb{R} \, : \, \exists B \in \mathcal{B}_\mathbb{Z^+}, \, \{t_n\}_{n \in \mathbb{N}} \subset \mathbb{R} \text{ such that } S = \pi_{\{t_n\}_{n\in\mathbb{N}}}^{-1}(B) \Big\} = \bigcup_{B \in \mathcal{B}_{\mathbb{Z}^+}} \bigcup_{\{t_n\}_{n\in\mathbb{N}} \subset \mathbb{R}} \{\pi_{\{t_n\}_{n\in\mathbb{N}}}^{-1}(B)\}
$$ Let $\{S_n\}_{n\in\mathbb{N}} \subset \Sigma$ , so there exists $\{t_{n,k}\}_{n,k \in \mathbb{N}} \subset \mathbb{R}$ and $\{B_n\}_{n \in \mathbb{N}} \subset \mathcal{B}_{\mathbb{Z}^+}$ such that $$
\forall n \in \mathbb{N}, \qquad S_n = \pi_{\{t_{n,k}\}_{k\in\mathbb{N}}}^{-1}(B_n)
$$ We aim to show $\bigcup_{n\in\mathbb{N}} S_n \in \Sigma$ .  I thought about trying to diagonalize the array of times to form a sequence $\{\tau_n\}_{n\in\mathbb{N}}$ $$
 \tau_1 := t_{1,1}, \, \tau_2 := t_{1,2}, \, \tau_3 := t_{2,1}, \, \tau_4 := t_{3,1}, \, \dots
$$ and writing $\bigcup_{n\in\mathbb{N}} S_n = \pi_{\{\tau_n\}_{n\in\mathbb{N}}}^{-1}\left( \bigcup_{n\in\mathbb{N}} B_n \right)$ , but this doesn't sit right with me.  Does anyone have any suggestions? Thank you very much for your help!  Also, please give me feedback on how I can improve my questions on MSE as this is my first post!","['stochastic-processes', 'measure-theory', 'probability-theory']"
3616517,Using the linearization of scalar curvature operator to obtain the contracted second Bianchi identity,"I'm reading the book ""The Ricci Flow: An Introduction"" and I'm at the part where the authors prove the Bianchi identities using the diffeomorphism invariance of the curvature. I'm stuck on some computations from the paragraph below: Consider the scalar curvature operator $g \mapsto R_g$ and it's linearization $DR_g$ defined by $$D R_{g}(h)=-g^{i j} g^{k \ell}\left(\nabla_{i} \nabla_{j} h_{k \ell}-\nabla_{i} \nabla_{k} h_{j \ell}+R_{i k} h_{j \ell}\right)  \ \ \ \ \ (1)$$ for any $2$ tensor $h$ . Substituting $$h_{i j}=\left(\mathcal{L}_{X} g\right)_{i j}=\nabla_{i} X_{j}+\nabla_{j} X_{i}$$ (where $X$ is an arbitrary vector field) and commuting covariant derivatives yields $$\begin{align}
D R_{g}\left(\mathcal{L}_{X} g\right) &=-2 \Delta \nabla_{i} X^{i}-2 R_{i j} \nabla^{i} X^{j}+\nabla^{i} \nabla_{j} \nabla_{i} X^{j}+\nabla_{i} \nabla_{j} \nabla^{j} X^{i}   \ \ (2)\\
&=2 X^{i} \nabla^{j} R_{i j} \ \ (3)
\end{align}$$ I don't understand how to go from $(1)$ to $(2)$ . Commuting derivatives, we get (where I'm using the obvious notation $\nabla_{j, k} = \nabla_j \nabla_k$ ): $$\nabla_{j, k} X_{\ell} - \nabla_{k, j} X_{\ell} = R_{jks}^{\ell} X^{s}$$ and with some work we can substitute $h = \mathcal{L}_{X} g$ into $(1)$ and obtain: $$DR_g(h) = -g^{ij}g^{kl}\left( \nabla_{i}   \left(R_{jks}^{\ell} X^{s} \right)-  \nabla_{i}\left(R_{j{\ell}s}^{k} X^{s}\right)  +R_{i k} h_{j \ell}\right) $$ but I still can't get from here to $(2)$ . Nor can I see how $(3)$ follows from $(2)$ . I've been stuck at this for a while now and would really appreciate some help.","['riemannian-geometry', 'ricci-flow', 'geometry', 'manifolds', 'differential-geometry']"
3616641,Spivak's Calculus Chapter 3 Problem 9 (c),"I've been stuck with this 9 (c) problem: I think the floor function might be a counterexample since: if $f(x) = ⌊x⌋$ then $f = f²$ and the $C_{A}$ function just doesn't look like it. I checked the answer book and it says to apply (b). If you have a better answer or could explain to me how to apply (b), would be appreciated.
Thank you in advance.","['algebra-precalculus', 'functions']"
3616746,Curvature Formula Proof By Definition,"Question : Use Definition 3.2 to prove Theorem 3.4. Definition 3.2 “The signed curvature $k(s)$ of a plane curve $ \alpha: I \rightarrow \mathbb{R^2}, \alpha(u)=(x(u),y(u))$ is defined by $t’(s)=k(s)n(s)$ (where $t(s),n(s)$ are the unit tangent and normal vectors, respectively). Theorem 3.4 “Let $ \alpha: I \rightarrow \mathbb{R^2}, \alpha(u)=(x(u),y(u))$ , be a regular curve (not necessarily parametrised by arc length). Then: $$k(u)=\frac{x’(u)y’’(u)—x’’(u)y’(u)}{(x’(u)^2+y’(u)^2)^{\frac{3}{2}}}$$ Below is my attempt. I’m not sure why my denominator has the incorrect index. Clearly, it must be small faux pas but I can’t find it. I’ve looked at other proofs here relating to the curvature (as many as I could, there are a lot) but none seem to be exactly the same as this. I apologise in advance if this is a duplicate.","['arc-length', 'parametrization', 'curvature', 'differential-geometry']"
3616814,Integrating an ODE in terms of elliptic functions,"Consider the ODE for $w:\mathbb{C}\to \mathbb{C}$ \begin{align}
w''=2w^3+Aw+B &&(1)
\end{align} We can multiply it by $w'$ and then integrate to get \begin{align}
w'^2=w^4+Aw^2+2Bw+C &&(2)
\end{align} ( $A,B,C$ are complex constants) My question is how would one then integrate (2) - i.e. obtain an explicit expression for the solution $w$ ? In particular, equation (1) is found in `Ordinary Differential Equations' by Ince. It's listed as equation VIII in section 14.316. Ince states that it is ""integrable in terms of elliptic functions"".
Unfortunately, I cannot see how this would be done. As an example of what I'm looking for, in a problem related to the one above, one is able to transform the ODE \begin{align}
P'^2-P^4+\lambda P^2+\frac{B^2}{P^2}+2A=0 && (3)
\end{align} into the ODE defining the Weierstrass elliptic function $$\wp'^2=4\wp^3-g_2\wp-g_3$$ by taking $$\wp(x):=P(x)^2-\frac{\lambda}{3},$$ where $g_2, g_3$ are constants depending on $A,B,\lambda$ .
And so the general solution to (3) is $$P(x)=\pm\sqrt{\wp(x)+\frac{\lambda}{3}}$$","['integration', 'ordinary-differential-equations', 'elliptic-functions', 'nonlinear-system', 'complex-integration']"
3616844,Show constant function.,"There are two parts to this question: (a) Let $U \subseteq \mathbb{C}$ be a connected open set containing the closed unit disk $\overline{B(0,1)}$ . Let $f \in \mathcal{O}(U)$ be a holomorphic function such that for any $z \in U$ with $|z|=1$ , we have $f(z) \in \mathbb{R}$ . Show that $f$ is a constant. (b) Find a non-constant holomorphic function $f$ on $\mathbb{C} \setminus \{1\}$ such that for any $z \in \mathbb{C} \setminus \{1\}$ with $|z|=1$ , we have $f(z) \in \mathbb{R}$ . I am guessing this question might need maximum modulus theorem, or maybe harmonic functions since real-valued functions come into play? Thank you for any help!",['complex-analysis']
3616946,Domain of essential self-adjointness for $A\otimes 1+1\otimes A$,"Let $A$ be an unbounded self-adjoint operator acting on a Hilbert space $H$ (typically $L^2(\mathbb{R}^d)$ ). Then, using Stone's theorem, the operator $A^{\otimes 2}:=A\otimes 1+1\otimes A$ defines a self-adjoint operator on $H\otimes H$ , the domain of which might be difficult to determine. Is it however true that if $A$ is furthermore essentially self-adjoint on $\mathcal{C}$ , then $A^{\otimes 2}$ is essentially self-adjoint on $\mathcal{C}\otimes \mathcal{C}$ ?","['tensor-products', 'operator-theory', 'functional-analysis']"
3616975,Can you help with this proof that the $n$-th Bell number is bounded by $n!$ for all natural numbers $n$?,"I am trying to prove that an upper bound for the nth Bell number is n factorial. I am trying to do this by induction. Firstly, the nth Bell number is given by: $B_{n}=\sum\limits^{n-1}_{k=0} B_{k}{n-1\choose k}$ , for $n \geq 2$ and $B_{0}=B_{1}=1$ . My proof is as follows: Let the statement $S(n)$ be $B_{n}=\sum\limits^{n-1}_{k=0} B_{k}{n-1\choose k} \leq n!$ . (*) Clearly $S(2)$ is true. Assume $S(n)$ true for some $n>2$ . RTP: $S(n+1)$ true, i.e. $B_{n+1}=\sum\limits^{n}_{k=0} B_{k}{n\choose k} \leq (n+1)!$ I tried multiplying both sides of (*) by $n+1$ , so we get $(n+1)!$ on the RHS as needed, but then it is not clear that we get $\sum\limits^{n}_{k=0} B_{k}{n\choose k}$ on the LHS. Can anyone help me to complete this proof? 
I tried using different forms of the Bell number as well - like using Dobinski's formula. I also got nowhere. Thanks!","['statistics', 'inequality', 'combinatorics', 'sequences-and-series', 'bell-numbers']"
3617057,How many values of $n$ are there for which $n!$ ends in $1998$ zeros?,"How many values of $n$ are there for which $n!$ ends in $1998$ zeros? My Attempt:
Number of zeros at end of $n!$ is $$\left\lfloor \frac{n}{5}\right\rfloor+\left\lfloor\frac{n}{5^2}\right\rfloor+\dots$$ But is there a method to get converse","['summation', 'ceiling-and-floor-functions', 'factorial', 'elementary-number-theory', 'algebra-precalculus']"
3617216,Find $r$ which maximizes $\binom{20}r\binom{20}0+\binom{20}{r-1}\binom{20}1+\binom{20}{r-2}\binom{20}2+\cdots+\binom{20}0\binom{20}r$,The value of $r$ for which $$\binom{20}r\binom{20}0+\binom{20}{r-1}\binom{20}1+\binom{20}{r-2}\binom{20}2+\cdots+\binom{20}0\binom{20}r$$ is maximum is? I tried to wrap my head around the solution but I don't get it. Could someone help me with it in an easier way? The solution arbitrary begins by considering the expansion of $(1+x)^{20}$ and then multiplying it to itself. A more question-oriented solution would be appreciated. Thanks. $r$ has to be some sort of plain integer. This whole question isn't about negative or fractional indices.,"['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
3617224,How to show that the inverse of an isometry is also an isometry?,"I'm studying geometry and am learning about isometries. I'm self-studying with the book Modern Geometry with Applications (Jennings) in case anyone's curious. The textbook explains that: A function $f: \Bbb{E}^n \rightarrow \Bbb{E}^n$ is an isometry if for all points $P, Q \in \Bbb{E}^n$ , $$f(P)f(Q) = PQ$$ where "" $PQ$ "" refers to the distance between the two points. Assume that $f$ is an isometry and that it has an inverse function $f^{-1}$ . Show that $f^{-1}$ is also an isometry. I'm not sure how to even get started. I've been thinking about the definition of what an isometry is, and figured that in order for the inverse of an isometry to also be an isometry then if we were to write $g = f^{-1}$ : $$\forall_{P,\ Q \in \Bbb{E}}\ g(P)g(Q) = PQ$$ but that's where I just started from. Could anybody provide some tips or hints to move forward? Thanks.","['isometry', 'geometry']"
3617343,x cancels out when looking for extremum,"So this happenned when I was trying to find a maximum of a function $f(x)$ and I'm not sure how to interpret that. The calculations are correct, I checked multiple times. $$f'(x)=...=(y_1-y_2)(x-x+c)=0$$ x cancels out and it leaves me with a condition $y_1=y_2$ . 
Does it mean extremum does not exist or what do I make of that? If that's of any help this is the function. $\sigma_1,\sigma_2,\rho, y_1, y_2$ are given. $\sigma_1,\sigma_2, y_1, y_2$ are positive real numbers, $\rho$ is real and $|\rho| \leq 1$ . $$f(x)=\frac{(y_1-y_2)(x-\frac{\sigma_2^2-\rho\sigma_{1}\sigma_2}{\sigma_1^2+\sigma_2^2-2\rho\sigma_{1}\sigma_2})}{\sqrt{x^2(\sigma_1^2+\sigma_2^2-2\rho\sigma_{1}\sigma_2)+x(-2\sigma_2^2+2\rho\sigma_{1}\sigma_2)+\sigma_2^2}}$$","['calculus', 'functions', 'real-analysis']"
3617376,Algebraic Closure of Finite Field $\mathbb{F}_p$,"Dummit and Foote (in Chapter 14.3) construct the algebraic closure of the finite field $\mathbb{F}_p$ by the following union: $$\bar{\mathbb{F}}_p = \bigcup_{n \geq 1}\mathbb{\mathbb{F}}_{p^n}.$$ I'm having trouble seeing how we can take this infinite union, because to do so we have to view all the fields $\mathbb{F}_{p^n}$ as subsets of some larger object. Finite unions of the form $$\bigcup_{k=1}^n \mathbb{F}_{p^k}$$ make sense because all the fields in this union can be seen as subfields of a finite field of order $p^{n!}$ , but I'm having trouble with the infinite case. The authors write: ...given any two finite fields, $\mathbb{F}_{p^{n_1}}$ and $\mathbb{F}_{p^{n_2}}$ there is a third finite field containing (an isomorphic copy of) them, namely $\mathbb{F}_{p^{n_1n_2}}$ . This gives us a partial ordering on these fields and allows us to think of their union. How does the partial ordering give us a well defined union in the infinite case? Does it involve something like Choice or the principle of recursive definition? Is it true in general that a partial ordering on a set gives us a well-defined notion of a union even when the elements aren't literally sets contained in a larger set?","['elementary-set-theory', 'finite-fields', 'field-theory']"
3617377,Series representation of polynomial root,"Consider the quintic: $$(1+q)x^5-(2+3q)x^4+(1+3q)x^3-x^2+2x-1=0$$ for $q=0$ , this equation has a triple root: $$(1-x)^3(x^2+x+1)=0 \implies x=1.$$ Kopal (1959) then states that, for small $q$ , a series representation of this root is: $$x = 1-w+\frac{1}{3}w^2+\frac{1}{9}w^3+ ...,$$ with $$w^3 = \frac{q}{3(1+q)}.$$ How does one go about finding this expansion? A standard Taylor expansion does not work, as the derivative $x'(q)$ is infinite at $q=0$ . Also, we seem to be expanding in the cube root of the small number. How does $w$ given above surface from the polynomial? Am I missing something trivial here?","['quintics', 'roots', 'polynomials', 'sequences-and-series']"
3617401,Using the equation $ A^{-1} + B^{-1} = (A+B)^{-1} $ to find relations between det(A) and det(B),"We are given a condition, $$
A^{-1} + B^{-1} =  (A+B)^{-1}
$$ Further, $|A| =4$ and we are asked to find the value of |B|. I tried to simplify the LHS $$
A^{-1} + B^{-1}= B^{-1}BA^{-1} + B^{-1}AA^{-1}
$$ $$
A^{-1} + B^{-1}= B^{-1}(A+B)A^{-1}
$$ That only lead me to get $$
|A+B|^2=|A||B|
$$ Am stuck after this. Any help would be appreciated! Thanks.","['matrices', 'determinant', 'linear-algebra']"
3617404,"An application of Keith Conrad's "" Splitting field of $X^3-2$ over $\mathbb{Q}$ ""","After reading This Paper , I thought in the case of FLT for n = 3: Let $x^3+y^3+z^3=0$ ,  for $x,y,z$ relatively coprime integer solutions. If $3$ doesn't divide $x,y,z$ then $x^3+y^3+z^3\not\equiv{0}$ mod $9$ .  Because $(\mathbb{Z}/9\mathbb{Z})^3=\{0,1,-1\}$ . So, let's assume that $3$ divides $z$ . The pure cubic field $\mathbb{Q}(\sqrt[3]{2})$ ,  as Professor Keith Conrad explains,  has ring of integers $\mathbb{Z}(\sqrt[3]{2})$ and an integer basis is: $\left\lbrace 1,\sqrt[3]{2},\sqrt[3]{4}\right\rbrace$ .  The prime $3$ is ramified: $\pi^3\nu$ ,  for: $\pi=1+\sqrt[3]{2}$ and $\nu=\sqrt[3]{2}-1$ ,  such that $\nu$ is a unit in $\mathbb{Q}(\sqrt[3]{2})$ . We have: $x^3+y^3+z^3=0$ and $-z^3=x^3+y^3$ $\Rightarrow$ $-z^3+3y^3=x^3+4y^3$ .  The expression $x^3+4y^3$ factors in $\mathbb{Q}(\sqrt[3]{2})$ as: $(x+y\sqrt[3]{4})(x+\omega y\sqrt[3]{4})(x+\omega^2 y\sqrt[3]{4})\,=\,(x+y\sqrt[3]{4})(x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4})$ .  So: $-z^3+3y^3=(x+y\sqrt[3]{4})(x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4})$ (1) . (Where $\omega=(-1+\sqrt{-3})/2$ is a primitive cube root of unity). I will consider here two lemmas as proved. An impossibility of being rational numbers in $\mathbb{Z}(\sqrt[3]{2})$ implies the impossibility of being it in $\mathbb{Z}$ ( $\mathbb{Z}\subseteq\mathbb{Z}(\sqrt[3]{2})$ ) .  And the extension of arithmetic rules (for example: to be relatively coprime) of $\mathbb{Z}$ to $\mathbb{Z}(\sqrt[3]{2})$ . A)  If $x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}$ is coprime to $x+y\sqrt[3]{4}$ ,  it will be coprime to $(x+y\sqrt[3]{4})^2=x^2+2y^2\sqrt[3]{2}+2xy\sqrt[3]{4}$ .  Let's see it. The sum is: $2x^2+4y^2\sqrt[3]{2}+xy\sqrt[3]{4}$ and the difference: $-3xy\sqrt[3]{4}$ .  And only by $3$ we see $-3xy$ doesn't divide $2x^2$ either $4y^2$ or $xy$ . Because $3$ only divides $z$ . B)  Let's see it in $\mathbb{Q}(\sqrt[3]{2},\omega)$ .  If $x+y\sqrt[3]{4}$ is coprime to $x+\omega y\sqrt[3]{4}$ ,  its addition and its substraction will neither have common factors. The sum is: $2x+y\sqrt[3]{4}(1+\omega)$ and the difference: $y\sqrt[3]{4}(1-\omega)$ .  The expression $1+\omega$ is a unit in $\mathbb{Q}(\omega)$ and $1-\omega=-(\omega-1)$ divides $3=-\omega^2(\omega-1)^2$ .  Then $1-\omega$ doesn't divide $2x$ or $y\sqrt[3]{4}(1+\omega)$ .  It is only left to see with $x+\omega^2 y\sqrt[3]{4}$ .  Its sum is: $2x+y\sqrt[3]{4}(1+\omega^2)$ and its difference: $y\sqrt[3]{4}(1-\omega^2)$ .  But $1+\omega^2$ is also a unit in $\mathbb{Q}(\omega)$ (the conjugate of $1+\omega$ )  and $1-\omega^2=(1+\omega)(1-\omega)$ .  Then, for the same reason than before $x+y\sqrt[3]{4}$ is coprime to $x+\omega^2 y\sqrt[3]{4}$ and therefore to $x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}$ . As $3$ divides $-z^3$ (1) ,  then $\pi=1+\sqrt[3]{2}$ ,  where is prime in $\mathbb{Z}(\sqrt[3]{2})$ ,  must divide $x+y\sqrt[3]{4}$ or $x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}$ and only one. Let's check it out: $\dfrac{(x+y\sqrt[3]{4})(\pi^2\nu)}{\pi\cdot\pi^2\nu}\,=\,\dfrac{(x+y\sqrt[3]{4})(1-\sqrt[3]{2}+\sqrt[3]{4})}{3}\,=\,\dfrac{x-2y+(2y-x)\sqrt[3]{2}+(x+y)\sqrt[3]{4}}{3}$ Without loss of generality we assume $x^3+y^3+z^3=0$ $\Rightarrow$ $1-1+0\equiv{0}$ mod $3$ .  So: $x\equiv{1}$ mod $3$ and $y\equiv{-1}$ mod $3$ .  In this way, we observe that: $x-2y\equiv{0}$ mod $3$ , $2y-x\equiv{0}$ mod $3$ and $x+y\equiv{0}$ mod $3$ .  Therefore $\pi$ divides $x+y\sqrt[3]{4}$ . Now let's check this: $\pi$ doesn't divide $x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}$ . $\dfrac{(x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4})(\pi^2\nu)}{\pi\cdot\pi^2\nu}\,=\,\dfrac{(x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4})(1-\sqrt[3]{2}+\sqrt[3]{4})}{3}$ $=\,\dfrac{x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}-x^2\sqrt[3]{2}-2y^2\sqrt[3]{4}+2xy+x^2\sqrt[3]{4}+4y^2-2xy\sqrt[3]{2}}{3}$ $=\,\dfrac{x^2+2xy+4y^2+(2y^2-x^2-2xy)\sqrt[3]{2}+(x^2-xy-2y^2)\sqrt[3]{4}}{3}$ But: $x^2+2xy+4y^2\equiv{0}$ mod $3$ , $2y^2-x^2-2xy\equiv{0}$ mod $3$ and $x^2-xy-2y^2\equiv{0}$ mod $3$ .  This is a contradiction and tell us if $x^3+y^3+z^3=0$ ,  then $3$ divides $x,y$ and $z$ in $\mathbb{Z}(\sqrt[3]{2})$ . In particular, if $3$ divides $x+y\sqrt[3]{4}$ and $x^2+2y^2\sqrt[3]{2}-xy\sqrt[3]{4}$ ,  at A) we have seen that $3$ should also have divided $x,y$ ,  not just $z$ ;  and at B), in $\mathbb{Z}(\sqrt[3]{2},\omega)$ ,  that $1-\omega$ should have divided $x,y$ in addition to $z$ ,  ( $1-\omega$ is a factor of $3$ ) . But if $3$ divides $x,y,z$ in $\mathbb{Z}(\sqrt[3]{2})$ and consequently in $\mathbb{Z}$ ; then: $x^3=3^3x'\,^3\,,\,y^3=3^3y'\,^3$ and $z^3=3^3z'\,^3$ . And it there will be a sum $x'\,^3+y'\,^3+z'\,^3=0$ , for $x',y',z'$ integral relatively prime coefficients, less than $x^3+y^3+z^3=0$ .  And so on. Beginning an infinite decreasing. Hence $x,y,z$ cannot be rational. Is it correct? Have I used the concepts of the Paper well? Thank you in advance¡","['number-theory', 'algebraic-number-theory']"
3617409,Does joint continuity imply separate continuity?,"I know that separate continuity does not, in general, imply joint continuity but does the converse hold? Given $X,Y,Z$ topological spaces, if $f: X\times Y \to Z$ is continuous ( $X\times Y$ with the product topology) does it follow that $f$ is separately continuous?","['general-topology', 'analysis']"
3617617,Acceleration of a ball on a plane [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question I have a plane ( $ax+by+cz+d=0$ ) in a 3D world, and a gravity vector $\vec{g}$ (say it's $[0, 0, -9.81]$ .) How would I find the acceleration vector of an object on this plane, ignoring friction?","['vectors', 'classical-mechanics', 'geometry', 'linear-algebra', 'physics']"
3617623,"Let $f, g$ be two bijective continuous functions $f\left(g^{-1}(x)\right)+g\left(f^{-1}(x)\right)=2 x$","question - Let $f, g$ be two bijective continuous functions on $\mathbb{R}$ such that $$
f\left(g^{-1}(x)\right)+g\left(f^{-1}(x)\right)=2 x
$$ for all $x \in \mathbb{R} .$ Suppose there exists $x_{0} \in \mathbb{R}$ such that $f\left(x_{0}\right)=$ $g\left(x_{0}\right) .$ Prove that $f(x)=g(x)$ for all real numbers $x$ my try - by Putting $h(x)=f\left(g^{-1}(x)\right) .$ i get $h(x)+h^{-1}(x)=2 x .$ ....(1)
where H is also bijective and continuous ..... after this the hint says that Conclude that $h(x)=x+c .$ Show that $c=0$ and hence $f(x)=g(x)$ for all $x$ i am not getting how to prove that $h(x)=x+c$ ...i tried replacing x by h(x) in 1)...but did not able to conclude .... any help will be helpful 
thankyou","['contest-math', 'continuity', 'functions']"
3617696,Can you suggest some good optimization books?,"I am looking for optimization books. Can you suggest some good materials? First, I started with Convex Optimization by Stephen Boyd & Lieven Vandenberghe, but I don't like it because they don't give examples of proofs and techniques, only theory and talking. I need some classical books, for example I like books such as: Zorich, Kreyszig, Kolmogorov and Fomin. Please suggest books that have a similar style. Especially I need books to pursue research in Reinforcement Learning.","['optimization', 'book-recommendation', 'convex-analysis', 'functional-analysis']"
3617861,Proving sum of integrals is integral of sums.,"This comes from Spivak ""Calculus on Manifolds"" problem 3-3. I think I have proof but it seems too simple to be true so I would like others to look through it and maybe point me to some subtle issues. I want to prove that if $f$ and $g$ are integrable functions $A \to \mathbb{R}$ where $A$ is a closed rectangle in $\mathbb{R}^n$ , then $f+g$ is also integrable. Also, I would like to prove that the values are related in the following way. $$ \int_A f+g = \int_A f + \int_A g$$ I assume that the following has already been proven. Here, $L(f,P)$ is lower sum of $f$ for partition $P$ , $U(f,P)$ is upper sum of $f$ for partition $P$ . $$ L(f,P) + L(g,P) \leq L(f+g,P) $$ $$ U(f,P) + U(g,P) \geq U(f+g,P) $$ Then, because $f$ and $g$ are integrable, then for every $\varepsilon/2 > 0$ we can find partitions $P$ and $P'$ such that the following is true. $$ U(f,P) - \frac{\varepsilon}{2} \leq \int_A f \leq L(f,P) + \frac{\varepsilon}{2}$$ $$ U(g,P') - \frac{\varepsilon}{2} \leq \int_A g \leq L(g,P') + \frac{\varepsilon}{2} $$ Combining results, we get the following. $$ U(f,P) + U(g,P') - \varepsilon \leq \int_A f+\int_A g \leq L(f,P)+L(g,P')  +\varepsilon $$ Now, take refinement of $P$ , $P'$ and call it $P''$ . Then we have the following results for $f$ and similarly for $g$ . $$ U(f,P'') \leq U(f,P) $$ $$ L(f,P'') \geq L(f,P) $$ So, combining multiple results, we have the following inequalities. $$ U(f+g, P'') - \varepsilon \leq \int_A f+ \int_A g \leq L(f+g,P'') + \varepsilon$$ From here, first of all, we see that $f+g$ is integrable. $$ U(f+g, P'') - L(f+g, P'') \leq 2 \varepsilon $$ Now, consider $\mathrm{sup}(L(f+g,P'')) = \alpha$ . Assume that $\alpha < \int_A f+ \int_A g $ . 
But then take $\varepsilon = (1/2)(\int_A f+ \int_A g - \alpha) > 0$ . In that case, the following inequality shows that it contradicts that $\alpha$ is an upper bound. $$ \alpha < \frac{\alpha + \int_A f + \int_A g}{2} = -\frac{\int_A f + \int_A g - \alpha}{2} + \int_A f + \int_A g \leq L(f+g,P'') $$ Assume that $\alpha > \int_A f + \int_A g$ . But then take $\varepsilon = (1/2)(\alpha-\int_A f - \int_A g) > 0$ . In that case, the following inequality shows that it contradicts that $\alpha$ is the least upper bound. This is due to each upper sum is an upper bound for the set of lower sums. $$ \alpha > \frac{\alpha + \int_A f + \int_A g}{2} = \frac{\alpha - \int_A f - \int_A g}{2} +\int_A f + \int_A g \geq U(f+g,P'') $$ I would really appreciate your thoughts and comments.","['integration', 'multivariable-calculus']"
3617912,Correlation coefficient in finance,"An asset A has volatility estimated as σA = 0.2
An asset B has volatility estimated as σB = 0.4
The assets have covariance = σAB = -0.33
State the hypothesis of correlation and test this at a 10% significance level So firstly I worked out the correlation coefficient which I believe to be -4.125 I did this by p= $-0.33/(0.2*0.4)$ adding to this I am told by the teacher I need to revere engineer the formula for d using a value taken from the Normal table. However I’m even unsure what my normal table value is","['statistical-inference', 'statistics', 'finance', 'correlation', 'hypothesis-testing']"
3618009,A die is thrown $n$ times. What is the probability that $6$ appears even number of times?,"A die is thrown $n$ times. What is the probability that $6$ appears even number of times (for the purpose of task $0$ is even number)? The solution from my textbook is:
We have two hypotheses $H_1$ that the array starts with 6 and $H_2$ array does not start with 6; $p_n$ is the probability. $p_n=P(H_1)(1-p_{n-1})+P(H_2)p_{n-1}=\frac{1}{6}+\frac{2}{3}p_{n-1}$ . $p_1=\frac{5}{6}$ After solving this recursive relation we get $p_n=\frac{1}{2}(1+(\frac{2}{3})^n)$ . I know how to solve recursion I just don't understand how they got it (I know probabilities of hypothesis). Also, is there any other way to approach this task?","['conditional-probability', 'probability', 'recursion']"
3618048,Inequalities on $L_p$ norm of Bounded Functions,"Let $\mathcal{F}$ be a set of uniformly bounded measurable functions on interval $[0,1]$ with respect to the Lebesgue measure. Let $\tilde{\mathcal{F}} = \{e^f/\int e^f d\mu: f\in\mathcal{F}\}$ be the transformed density class. Let $\|\cdot\|_p$ be the $L_p$ norm for $1\leq p\leq\infty$ . For any $f\in\mathcal{F}$ , denote $\tilde{f} = e^f/\int e^f$ . Show that for any $f_1$ and $f_2$ in $\mathcal{F}$ , $$\| \tilde{f}_1 - \tilde{f}_2 \|_p\leq c_1 \|f_1 -f_2 \|_p $$ and $$\|\log(\tilde{f}_1) - \log(\tilde{f}_2) \|_p\leq c_2\|\tilde{f}_1 - \tilde{f}_2\|_p$$ for some constants $c_1$ and $c_2$ that only depends on $\sup_{f\in\mathcal{F}}\|f\|_\infty$ . In the paper the author says its ""easy"" to see these two inequalities but I've been struggling to show this explicitly for a while now. Any suggestions would be greatly appreciated. Edit: I think the uniform boundedness and the fact that $\exp(\cdot)$ and $\ln(\cdot)$ are Lipschitz should give the results. Note that $\exp(x)$ on a bounded interval is always Lipschitz; for $\ln(x)$ to be Lipschitz, need $x$ to be on $[a,\infty)$ for $a>0$ .","['normed-spaces', 'functional-analysis', 'analysis', 'real-analysis']"
3618086,Range of entire function. Picard's theorem,$f(z)$$=$ 2 $z^{14}$$cos^2z$ $f(z)$ is entire function. I want to figure out if $f$ ( $\Bbb C$ ) $=$$\Bbb C$ or there's a point $a$ such that $f$ ( $\Bbb C$ ) $=$$\Bbb C$ \ { $a$ }. So essentially I want to use Little Picard Theorem here. I thought about using the reflection principle. Let $f(z)$$=$$w$ .  So here $f$ ( $\bar{z}$ )= $\bar{f(z)}$ . This means that if $w$ if this exceptional point then $\bar{w}$ is too. I guess this implies that exceptional points can only be located on the real line? Can someone please correct me if I am wrong. Any help with solving this problem would be appreciated.,"['complex-analysis', 'entire-functions', 'complex-numbers']"
3618177,What are most ways to differentiate the same function?,"An example of a function that relates to my question: $f(x)=\frac{x^3\left(2x+1\right)^2}{x^2}$ with $x>0$ . That function could be differentiated (at least) 5 different ways using rules . Simplify, expand and differentiate each term: $f(x)=4x^3+4x^2+x$ then $f'(x)=12x^2+8x+1$ Product rule: $f(x)=x(2x+1)(2x+1)$ then $f'(x)=(1)(2x+1)(2x+1)+(x)(2)(2x+1)+(x)(2x+1)(2)=12x^2+8x+1$ Chain rule with product rule : $f(x)=x\cdot(2x+1)^2$ then $f'(x)=(1)(2x+1)^2+(x)(2)(2x+1)(2)=12x^2+8x+1$ Logarithmic differentiation: $\ln\left(f(x)\right)=\ln\left(x\left(2x+1\right)^2\right)$ then $f'(x)=\left(\frac{1}{x}+2\cdot\frac{1}{2x+1}\cdot2\right)\left[x\left(2x+1\right)^2\right]=12x^2+8x+1$ Quotient rule with product rule : $f'(x)=\frac{\left(\left(3x^2\right)\left(2x+1\right)^2+\left(x^3\right)\left(2\left(2x+1\right)2\right)\right)x^2-\left(x^3\left(2x+1\right)^2\right)2x}{\left(x^2\right)^2}=12x^2+8x+1$ (And then, there is the limit definition of the derivative) The purpose of the question is to introduce new ways of differentiating with functions that have been used previously. Also, students should think about which way is most efficient, but that only works if there are multiple (rule-based) ways of differentiating the same function.","['calculus', 'derivatives', 'chain-rule']"
3618203,"How prove this inequality $\sum_{i=1}^{n}\sum_{j=1}^{n}\text{lcm}(i,j)\le\frac{n^3}{5}(n+4)$?","Let $n$ be postive integers.  Show that $$\sum_{i=1}^{n}\sum_{j=1}^{n}[i,j]\le\dfrac{n^3}{5}(n+4)\,,$$ where $[a,b]$ denote the least common multiple of $a$ and $b$ . $S_1=1=\dfrac{1^3}{5}(4+1)=1$ Assume that $n>2$ is an integer such that $$S_{n-1}\leq \dfrac{(n-1)^3}{5}(n+3),$$ Then, $$S_{n}-S_{n-1}=n+2\,\sum_{k=1}^{n-1}\,\text{lcm}(k,n)\,.$$","['summation', 'number-theory', 'elementary-number-theory', 'gcd-and-lcm', 'inequality']"
3618207,Do the Generators of integral de Rham Cohomology always integrate to 1?,"Let $\omega$ be the Fubini-Study-Form on complex projective space $\mathbb{P}^N$ , normalized s.t. $\int_{\mathbb{P}^1}\omega =1$ , and let $1\leq n\leq N$ .
Then $\int_{\mathbb{P}^n}\omega^n =1$ . I've seen a differential geometry proof for that. But I want to ask about the connection to de Rham cohomology .
Is it true, that \begin{align*}
	\int_{\mathbb{P}^1}\omega =1 \Leftrightarrow \int_{\mathbb{P}^n}\omega^n =1 \Leftrightarrow \omega \text{ generator of } H^2\left(\mathbb{P}^N,\mathbb{Z}\right)?
	\end{align*} My ideas so far : (using Mumford's Algebraic Geometry I as a reference for de Rham cohomology) The $2n$ -form $\omega^{n}$ is a basis of $H^{2n}\left(\mathbb{P}^N,\mathbb{Z}\right)$ .
For the homology, $H_{2n}\left(\mathbb{P}^N,\mathbb{Z}\right)\cong \mathbb{Z}$ and is generated by the fundamental class $[\mathbb{P}^n]$ (the image of $\mathbb{P}^n$ under the homomorphism $H_{2n}\left(\mathbb{P}^n,\mathbb{Z}\right)\rightarrow H_{2n}\left(\mathbb{P}^N,\mathbb{Z}\right)$ ). Maybe because we have the bilinear map \begin{align*}
	H^{2n}\left(\mathbb{P}^N,\mathbb{Z}\right) \times H_{2n}\left(\mathbb{P}^N,\mathbb{Z}\right) \rightarrow \mathbb{R}
	\end{align*} the basis element of the cohomology space $\omega^n$ is mapped to a generator/ basis element of the image and the image is $\mathbb{Z}$ ? 
Maybe we can use the duality of the two spaces in some sense?
I also tried using de Rham's Theorem, but it only made sense for a topdimensional form, which is not of interest here.","['algebraic-topology', 'de-rham-cohomology', 'homology-cohomology', 'differential-forms', 'differential-geometry']"
3618208,Strictly increasing function from $\mathbb{R}$ into $\mathbb{R}\backslash\mathbb{Q}$,"It seems this question is duplicated...Still I would be grateful for any hint to my second question. Does there exist a strictly increasing function $f:\mathbb{R}\rightarrow\mathbb{R}\backslash\mathbb{Q}$ , i.e., from real numbers to irrational numbers? It cannot be surjective, since otherwise one can show that it is continuous and obtain a contradiction, but without assuming surjectivity I have not found a contradiction. I do have a candidate function with that property by translating decimal expansion to continued fraction. More precisely, define a function $f:(0,1)\rightarrow\mathbb{R}\backslash\mathbb{Q}$ as follows. Let $x\in(0,1)$ with decimal expansion $0.a_1a_2a_3...$ , define $f(x)$ to be $[0;1,1+a_1,1,1+a_2,...]$ . As far as I understand infinite continued fraction must represent irrational number, and it seems $f$ preserves order. Is this a correct example? More generally, what subset of reals can be the image of strictly increasing function?","['continued-fractions', 'functions', 'monotone-functions', 'real-analysis']"
3618240,Generate all nonisomorphic rooted trees from a vertex set with a common root,"For a vertex set $v = \left\{v_1...v_n  \right\}$ and a common root $r$ , is there an efficient (maybe $O(1)$ per tree) algorithm that generates all non-isomorphic trees on all nodes $v$ and with root $r$ . Two trees are isomorphic if all the parent-child relationships are the same, i.e. all equivalent nodes in the two trees have the same parent and the same children. Example: $v = \left\{v_1, v_2, v_3\right\}$ All trees should have the same root an the same node set. The following image shows some valid trees: Including a tree should lead to all other isomorphic trees being ignored Another SO post shows a implementation that finds unrooted topologies: https://codereview.stackexchange.com/questions/202773/generating-all-unlabeled-trees-with-up-to-n-nodes In one answer, there appears this algorithm for creating unrooted non-isomorphic topologies on $n$ nodes in constant time per tree. Robert Alan Wrights, Bruce Richmond, Andrew Odlyzko and Brendan D. Mckay (1986). “Constant time generation of free trees”. SIAM J. Comput. 15:2, pp. 540–548. I am not sure if it is possible to extend this algorithm efficiently to generate the desired rooted trees.","['graph-theory', 'trees', 'combinatorics', 'discrete-mathematics']"
3618255,Strong law of large numbers for triangular arrays,"Consider a triangular array $X_{n,1},\ldots,X_{n,n}$ of rowwise i.i.d. real random variables with $ \sup_{n \in \mathbb{N}} \mathbb{E}\vert X_{n,1} \vert < \infty$ and $ \lim_{n \rightarrow \infty} \mathbb{E} X_{n,1} := \mu < \infty $ exists. Does under the standing assumptions the strong law of large numbers hold, i.e. \begin{align}
\frac{1}{n} \sum_{i=1}^n X_{n,i} \stackrel{a.s.}{\longrightarrow} \mu \qquad \text{as } n \longrightarrow \infty?
\end{align} If yes and if it is not a trivial conclusion from a well known theorem, do you know some references where the statement is written down?","['law-of-large-numbers', 'probability-limit-theorems', 'probability-theory', 'probability']"
3618395,Does order matter when drawing balls from a bag?,"I have a question. Let's say we draw 5 balls from a bag. This bag has 20 balls. And each ball is numbered from 1 to 20. Case 1: The balls are drawn one at a time without replacement. Does order matter? Is 1,2,3,4,5 the same as drawing 5,4,3,2,1? Why/Why not? Case 2: The balls are drawn all at once without replacement. Is 1,2,3,4,5 the same as 5,4,3,2,1? Does order matter? Why/Why not? Case 3: The balls are drawn one at a time with replacement. Is 1,2,3,4,5 the same as 5,4,3,2,1? Does order matter? Why/why not? This is an example mentioned in Mathematics:A Discrete Introduction by Scheinerman. I am confused because I don't understand why the author states his intuitions the way he does.","['discrete-mathematics', 'probability']"
3618423,Problem on the general solution of the differential equation $ [y+xf(x^2+y^2)]dx+[yf(x^2+y^2)-x]dy=0 $,"This question has stumped me: $$ [y+xf(x^2+y^2)]dx+[yf(x^2+y^2)-x]dy=0  $$ I've tried finding $M_x-N_y$ thinking it would help find some integrating factor, but it came out to be 2 and I don't think I can use an integrating factor now. I've also tried diving by $x^2,xy$ , but I'm not sure how to proceed now.",['ordinary-differential-equations']
3618431,"Prove $\partial_t \int f(t,s) ds = \int \partial_t f(t,s)ds$ if $t \mapsto \partial_t f(t,s)$ exists almost everywhere for each $s$","Let $f(t,s)$ be a (jointly) continuous bounded function on $(a,b) \times (a,b) \subseteq \mathbb{R}^2$ . Suppose further than for each fixed $s \in (a,b)$ , the continuous function $t \mapsto f(t,s)$ is differentiable (Lebesgue) almost everywhere on $(a,b)$ and that $\partial_t f(t,s) \in L^1(a,b)_t$ . I would like to know whether the continuous function $t \mapsto \int_a^b f(t,s) ds$ is differentiable for almost all $t \in (a,b)$ with derivative equal to $\int_a^b \partial_t f(t, s) ds$ (note there is some subtlety here about showing $s \mapsto \partial_t f(t,s)$ is measurable for almost all $t$ ). The issue is that when looking at the quantity $$ \frac{1}{h} \int_a^b f(t+h,s) - f(t,s) ds,$$ we want to use a statement like ""for almost all $s$ , $\frac{f(t+h,s) - f(t,s)}{h}$ converges to $\partial_t f(t,s)$ as $h \to 0$ ,"" but the only thing we can assume is sort of the opposite (for each fixed $s$ , $\frac{f(t+h,s) - f(t,s)}{h} \to \partial_tf(t,s)$ for almost all $t$ ). Note that if each $t \mapsto f(t,s)$ is differentiable on all of $(a,b)$ , then by dominated convergence, there is no issue (apart from showing $s \mapsto \partial_t f(t,s)$ is measurable for each $t$ ). The reason I care about proving this proposition is because I am trying to establish the variation of parameters formula for Sturm Liouville problems having coefficients that are $L^1$ functions. So, in this situation, we can only expect our solutions to be differentiable almost everywhere. The function $f(t,s)$ is really the the primary fundamental matrix in disguise (i.e., for each $s$ , $t \mapsto f(t,s)$ is the solution of the ODE satisfying the initial condition $f(s,s)= 1$ ). Edit: It seems that the variation of parameters formula I was looking at (Theorem 1.3.1 in Zettl's book Sturm-Liouville Theory ) is not completely accurate. If you look at Theorem 3.1 of Coddington-Levinson, there is a simpler version of the formula which still solves the ODE (and therefore must be the unique solution) and in which the $t$ -dependence appears outside the integral only. So then there is no issue. It would still be interesting to know whether the above assertion is true, but I no longer need it for the application I had in mind.","['measure-theory', 'lebesgue-integral', 'ordinary-differential-equations']"
3618472,"Left invariant metric on $SL(n,\mathbb{R}$)","I'm vaguely familiar with the concept of invariant Riemannian metrics on Lie groups. I am mainly interested in $SL(n,\mathbb{R})$ but I guess the following is more general. To the best of my knowledge, for any Lie group $G$ there can always be chosen a left-invariant (or right) Riemannian metric (but usually not bi-invariant). Moreover, this Riemannian metric induces a left-invariant (""standard"", non Riemannian) metric $d_G$ on $G$ . My questions are: Are my statements above correct? I don't know any canonical reference for these, so I would appreciate any. Is the (non Riemannian) left invariant metric $d_G$ on $G$ unique in some sense (maybe up to equivalence or something)? $SL(n,\mathbb{R})$ is naturally embedded in $\mathbb{R}^{n^2}$ , on which all metrics are equivalent. How is the (non Riemannian) metric $d_{SL(n,\mathbb{R})}$ on $SL(n,\mathbb{R})$ relates to the metrics on $\mathbb{R}^{n^2}$ ? Is there any relation at all? Are there any interesting inequalities? Even the $n=2$ case is a starting point. I vaguely remember some identities somewhat looking like $\|M\|^2\approx \cosh d(M,I)$","['riemannian-geometry', 'metric-spaces', 'reference-request', 'lie-groups', 'differential-geometry']"
3618531,Circle Geometry Colinear Points Problem,"I was digging through some old questions I had from high school and I came across this circle geometry problem. . There were no solutions unfortunately. How can this be proven? Here are a few things I've tried: Pascal's theorem Proof that $\angle EBF = \angle EAF$ ABDC is a cyclic quadrilateral I have been unsuccessful with all of these attempts, however I may have missed something when attempting to prove these.","['power-of-the-point', 'euclidean-geometry', 'circles', 'geometry']"
3618568,Combinatorics and integer solutions to equations,"Here is the question: How many non-negative integer solutions are there to the equation $x_1 + x_2 + x_3 + x_4 = 74$ with each $x_j \leq 26$ ? We've been instructed to use the $C(n+r-1,r-1)$ identity for the amount of integer solutions for the equation $x_1 + x_2 + \cdots + x_r = n$ . They also provided a hint on how to start that makes no sense to me. Given hint for step 1: Let $U$ be the set of solutions without any restriction, and let $S_j$ be the set of solutions where $x_j > 26$ . The size of $S_1$ can be found by replacing $x_1$ with $y_1 = x_1 - 27 \geq 0$ and applying the given formula. What is the size of $S_1$ ? I am completely stumped on this question, and I'm not actually sure what the hint is actually pointing me towards. Any help or direction would be appreciated!","['combinatorics', 'discrete-mathematics']"
3618661,Explanation regarding the formula for the sum of interior angles of geodesic triangle on a cone,"I'm studying geometry and had a question regarding geodesic triangles and the sums of their interior angles. The book that I'm using is Modern Geometry with Applications (Jennings) if anyone's curious. The book illustrates the concept of the angles of a geodesic triangle on a curved surface not having to add up to $180°$ . They state that: Let $\theta$ be the angle subtended by the circular sector. If the vertex of the cone lies in the interior of $\triangle ABC$ you will find that $$\angle A + \angle B + \angle C = 540° - \theta$$ In particular $\angle A + \angle B + \angle C \gt 180°$ if $\theta \lt 360°$ . Here, the image that they used to illustrate this is a ""paper cone"" where you join the end points like follows: This isn't the exact image from the book, but I believe it's similar enough to get the point across. My question is, where did the $540°$ come from?","['triangles', 'geodesic', 'geometry']"
3618731,Find all the entire functions satisfying $f^2+g^2=1$ [duplicate],"This question already has an answer here : $f, g$ entire functions with $f^2 + g^2 \equiv 1 \implies \exists h $ entire with $f(z) = \cos(h(z))$ and $g(z) = \sin(h(z))$ (1 answer) Closed 4 years ago . Find all the entire functions (holomorphic functions on $\mathbb{C}$ ) $f,g$ satisfying $f^2+g^2=1$ . Of course $f,g$ can be constants satisfying $f^2+g^2=1$ . But if they are not constants, do they exist? Maybe Liouville Theorem can be used here to solve this problem, but how to use?","['complex-analysis', 'entire-functions']"
3618762,On $x^5+x+1\equiv 0 \mod p$,"Prove that if $x^2 \equiv p \mod{23}$ has no integer solutions then $x^5+x+1\equiv 0 \mod p$ has integer solutions.
I found this after I test for the first 10000 prime numbers, but I don't know how to prove this.","['number-theory', 'algebraic-number-theory']"
3618791,Prove or disprove that the sum is bounded by a constant,"Given $p\in[0,1]$ , prove or disprove that the sum $$\sum_{n=k}^\infty\sum_{j=0}^k\left(\matrix{n\\j}\right)p^j(1-p)^{n-j}$$ is bounded by a constant that does not depend on $k$ . The terms $\left(\matrix{n\\j}\right)p^j(1-p)^{n-j}$ do remind me of the binomial expansion. But using that, the most naive estimate is $$\sum_{j=0}^k\left(\matrix{n\\j}\right)p^j(1-p)^{n-j}\leq\sum_{j=0}^n\left(\matrix{n\\j}\right)p^j(1-p)^{n-j}=1$$ And then summing over $n$ still gives $\infty$ . How should I make the correct estimate? p.s. I am not absolutely sure that this holds . There is a proof that I was reading where such an estimate could yield the final result. I am a bit confused now because one comment says this is wrong while an answer seems to have proved this.","['sequences-and-series', 'estimation', 'real-analysis']"
3618850,"Continuity/Differentiability of ""Cosine Mixture Function""","In the paper ""A literature survey of benchmark functions for global optimization problems"" at this arxiv link https://arxiv.org/pdf/1308.4008.pdf , it describes the Cosine Mixture Function as ""Discontinuous, Non-Differentiable, Separable, Scalable, Multimodal"". The n-dimensional function is as follows: $$f(x)=-0.1\sum_{i=1}^n \cos{5\pi x_i}-\sum_{i=1}^n x_i^2$$ I'm not great at maths so it isn't immediately apparent where this function is discontinuous, and I can't see any areas where the function is undefined. Any help would be greatly appreciated as I'm pretty sure I am missing something.","['optimization', 'calculus', 'derivatives', 'continuity']"
3618854,How to define an total order on $P$?,"Consider the finite set $X = \{a_1, \cdots, a_n\}$ where $a_i \in \mathbb{R}$ for all $1 \leq i \leq n$ and let $P = \wp(X) - \{\emptyset\}$ . An exercise in my book of discrete mathematics has a question to define a total order $\sqsubseteq$ on $P$ , which in the particular case for $Y, Z \in P$ when Y and Z have a cardinality equal to 1, the order is the same to the usual order $\leq$ on $\mathbb{R}$ . How is the definition of $\sqsubseteq$ ?","['elementary-set-theory', 'order-theory', 'discrete-mathematics']"
3618855,"Find all functions $f$ such that for any rationals $x$ and $y$, $f(x+y)=f(x)f(y)-f(xy)+1$.","Find all functions $f$ such that for all rational numbers $x$ , $y$ , $$f(xy)+f(x+y)=f(x)f(y)+1\,.$$ I already asked the same question where $x$ and $y$ were integers, but this time I want to solve this functional equation over the rationals. The solutions when $x$ and $y$ are integers are $f(x)=1$ , $f(x)=x+1$ and $f(x)=\frac{1+(-1)^x}{2}$ . So far, I have been able to prove that $f(x)=1$ and $f(x)=x+1$ are also solutions over the rationals, but I am struggling to do the same for $f(x)=\frac{1+(-1)^x}{2}$ , and I don't actually know if this is a solutions over the rationals","['functional-equations', 'recreational-mathematics', 'functions', 'rational-numbers']"
3618961,Converting an ordinary differential equation to homogeneous form,"We have the following ODE: $$2xy'+y=y^2\sqrt{x-x^2y^2}$$ We are asked to substitute $$y(x)=z(x)^a$$ and then to find a value of the parameter $a$ that would turn the equation into a homogeneous equation, but I can't figure it out,I'm stuck at this point: $$y'=az^{a-1}z'$$ after plugging it and simplifying it a bit I'm getting: $$2axz'+z=z^{a+1}\sqrt{x-x^2z^{2a}}$$ Can't figure it out from here, would appreciate any directions, thanks",['ordinary-differential-equations']
3618990,Question about ring of prime order.,"Question : is a ring of prime order a field?  Must a ring of prime order contain a multiplicative identity? My attempt : I consider the ring $\mathbb{3Z}/\mathbb{9Z}$ I saw it has only three elements $0+\mathbb{9Z}$ , $3+\mathbb{9Z}$ and $6+\mathbb{9Z}$ . (These are only elements of $\mathbb{3Z}/\mathbb{9Z}$ because any coset of $\mathbb{9Z}$ in $\mathbb{3Z}$ must be equal to one of the above three cosets) Further, I saw $(3+\mathbb{9Z})(6+\mathbb{9Z})=18+\mathbb{9Z}=0+\mathbb{9Z}=\text{zero element in the ring   }\mathbb{3Z}/\mathbb{9Z}$ Hence $\mathbb{3Z}/\mathbb{9Z}$ has zero divisors and hence it is not an integral domain and hence not a field. For second part : clearly none of ( $0+\mathbb{9Z}$ , $3+\mathbb{9Z}$ and $6+\mathbb{9Z}$ )  these elements is multiplicative identity (unity) in $\mathbb{3Z}/\mathbb{9Z}$ and hence the ring of prime order need not have unity. But when I searched MSE, I saw a question with title “show that a finite ring of prime order must have a multiplicative identity” (here is link Finite rings of prime order must have a multiplicative identity ) So please tell me, am I wrong in the second part of the question? and please also verify my attempt for first part. Please help.","['ring-theory', 'finite-fields', 'abstract-algebra']"
3618997,Is there an odd solution of $\varphi(n)+n=\sigma(n)$?,"I want to show that the only solution of $$\varphi(n)+n=\sigma(n)$$ for a positive integer $n$ is $n=2$ . What I worked out is that we must have $$\varphi(n)>\frac{n}{2}$$ To show this assume $n$ is composite and its prime factors are $$p_1,\cdots,p_k$$ Then, the numbers $$1,\frac{n}{p_1},\cdots ,\frac{n}{p_k}$$ are distinct divisors from $n$ not equal to $n$ and the sum of those divisors is larger than the number of positive integers $m\in [1,n]$ not coprime to $n$ , since every such $m$ is a multiple of one of the prime factors $p_i$ of $n$ , the number of multiples of this $p_i$ is $\frac{n}{p_i}$ So we get $$n-\varphi(n)<1+\frac{n}{p_1}+\cdots \frac{n}{p_k}\le \sigma(n)-n$$ and together with $\varphi(n)+n=\sigma(n)$ we easily can derive the claimed inequality. This implies that an even composite solution cannot exist because of $\varphi(n) \le \frac{n}{2}$ for even $n$ , hence $2$ must be the only solution. But how can I show that no odd solution $n$ exists ? I strongly conjecture this because in this case $n$ must be a perfect square (because $\sigma(n)$ must be odd, if $n>1$ ) , and no perfect square $n\le 10^{16}$ does the job.","['number-theory', 'divisor-sum', 'totient-function', 'elementary-number-theory']"
3619008,"$2\left|f'(0)\right|=\sup_{z,\omega \in \mathbb{D}} \left|f(z)-f(\omega)\right|$ implies that $f$ is linear.","Suppose that $f:\mathbb{D}\to \mathbb{C}$ is holomorphic with $$2\left|f'(0)\right|=\sup_{z,\omega \in \mathbb{D}} \left|f(z)-f(\omega)\right|$$ Prove that $f$ is linear. My attempt Suppose that $$f(z)=\sum_{n=0}^\infty a_n z^n$$ Then $2\left|f'(0)\right|=2|a_1|$ , and \begin{align*}
\sup_{z,\omega \in \mathbb{D}} \left|f(z)-f(\omega)\right|
&\ge \sup_{|z|=1} \left|f(z)-f(-z)\right| \\
&=2|a_1|\sup_{|z|=1} \left|1+a_3z^2+a_5z^4+\cdots\right| \\
&\ge 2|a_1|
\end{align*} where we use the maximum modulus principle. But I got stuck to show that the equality holds precisely when $a_2=a_3=\cdots=0$ . Any hints would be highly appreciated.",['complex-analysis']
3619024,A Trigonometric Equation with different frequencies,"I have come across a system of trigonometric equations in $t$ , with parameters $x$ and $y$ . Though I know the first is not analytically solvable, my hope was to solve the second, and then substitute into the first. However, I am unable to solve the second equation, though I think it may be possible. Without further ado, the equations are: $$\left\{\begin{array}{l}\sin(t)\cos(t)-y\sin(t)-t+x=0\\\cos(2t)-y\cos(t)-1=0\end{array}\right.$$ Indeed, if one considers the first and second left hand sides, one can observe that the second is the derivative of the first with respect to $t$ . Not sure if that helps. Thanks a lot, and any help is appreciated.","['trigonometry', 'systems-of-equations']"
3619046,Chern character of $\mathrm{ch}(i_* \mathcal{O}_L)$ for $i: L \hookrightarrow X$ a line,"$\newcommand{\oh}{\mathcal{O}} \newcommand{\ch}{\mathrm{ch}} \newcommand{\td}{\mathrm{td}}$ Let $X$ be a Fano threefold of index $1$ and degree $d$ with ample class $H$ , so $K_X=-H$ and $H^3=d$ , and let $ i : L \hookrightarrow X$ be a line (with Hilbert polynomial $1+t$ ) in $X$ . I have read that $\ch(i_* \oh_L) = L + \frac{1}{2}P$ where $L$ and $P$ are the classes of a line and a point in the cohomology ring $H^*(X, \mathbb{Q})$ , respectively (note that $dL=H^2$ ). I'm trying to show this, but I'm making a mistake somewhere in my reasoning - my question is where? Note that $i$ is proper, so $i_!=i_* $ . Then by Grothendieck-Riemann-Roch we have $$ \ch(i_* \oh_L)  \td(X) = i_*(\ch(\oh_L) \td (L)) . $$ I know that the Todd class of such $X$ can be given by $\td(X) = (1, \frac{1}{2}H, \frac{1}{12}(H^2 + c_2(X)), \frac{1}{24} H \cdot c_2(X))$ . I also know that we have $H \cdot c_2(X) = 24$ (but I think I should put some class after $24$ here - perhaps $24P$ ?), so the last entry of the Todd class of $X$ is $1$ (or $P$ ?). Also, $\ch(\oh_L) = 1$ and since $L$ is an algebraic curve with $L \cong \mathbb{P}^1$ , we have $\td(L) = 1 + c_1(L) = 1 - K_L = 1 + 2P$ (where here, $P$ is a point in $L$ as opposed to in $X$ ). We also know that $i_* \oh_L$ is a sheaf on $X$ supported in codimension $2$ , so it follows that $\mathrm{rk}(i_* \oh_L ) = 0$ and $c_1 (i_* \oh_L ) = 0$ . Furthermore, by this question , $c_2(i_* \oh_L) = L$ (using standard short exact sequence with ideal sheaf of $L$ ). So really, the only unknown Chern class is the third one. For the right hand side, I'm not really sure how to interpret what $i_* $ does to the $1+2P$ inside it - I assumed that it means we now consider $P$ as a point in $X$ , so the left hand side would be $i_*(1+2P) = 1+2P $ where now $P$ is a point class in $X$ - is this correct? Putting what we have together, we get (I'm letting $c_2 := c_2(i_* \oh_L)$ and $c_3 := c_3(i_* \oh_L)$ here): \begin{align}   
( -L + \frac{1}{2} c_3 )( 1 + \frac{1}{2} H + \frac{1}{12}(H^2 + c_2(X)) + P) = 1 + 2P
\end{align} but this seems wrong, because this would mean that $L=0$ which is a contradiction, and also that $c_3=5P$ which doesn't match up. Thank you for any help!","['complex-geometry', 'vector-bundles', 'algebraic-geometry', 'sheaf-theory', 'schemes']"
3619050,Integrate $\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x$ and monotonicity of integrals,"I must evaluate $$\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x$$ My idea is that if we set $$L:=\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x$$ Then the integral must satisfy the equation $$\int_0^L \frac{1}{\sqrt{x}}\text{d}x=L$$ And we have that $$\int_0^L \frac{1}{\sqrt{x}}\text{d}x=\lim_{\varepsilon \to 0^+}\int_\varepsilon^L\frac{1}{\sqrt{x}}\text{d}x=2\sqrt{L}$$ So we have the equation $2\sqrt{L}=L$ , that leads to the solutions $L_1=0$ and $L_2=4$ ; now I suspect that $$\int_0^{\int_0^{\vdots}\frac{1}{\sqrt{x}}\text{d}x}\frac{1}{\sqrt{x}}\text{d}x > 0$$ So the only choice left if $L_2=4$ , but I don't actually know how to prove it rigorously; I'm sure that the last integral is $\geq0$ because $\sqrt{x}\geq0$ , but maybe it is $>0$ because the square root can't be $0$ being at the denominator. Two questions: 1) is my argument right? I'm not sure if it is rigorous, especially when I ""substitute"" the upper bound with $L$ ; maybe I can approach it with sequences. 2) In this case the square root was at the denominator so somehow I've excluded the fact that the integrand could be $\geq0$ (if my argument is correct), but in general how can I prove that if $f(x)>0$ then $\int_a^b f(x) \text{d}x >0$ and not $\int_a^b f(x)\text{d}x \geq 0$ (if this is true)? Thanks.","['definite-integrals', 'analysis']"
3619085,How to find general solution of a DE given its particular solutions?,"Can you suggest how can I find the general solution of the equation $f(x)y'' + g(x) y'+y = 1$ if $x^2$ , $x$ , $1$ are solutions of it. My idea is to make somehow a linear combination of the solutions like that $C_1 + C_2x +C_3x^2$ but it does require that this indeed general solution of the equation. Then I thought of finding the roots of the characteristic equation but I have functions, not constant coefficients.","['calculus', 'ordinary-differential-equations']"
3619098,"A ""new"" general formula for the quadratic equation?","Maybe the question is very trivial in a sense. So, it doesn't work for anyone. A few years ago, when I was a seventh-grade student, I had found a quadratic formula for myself. Unfortunately, I didn't have the chance to show it to my teacher at that time and later I saw that it was ""trivial"". I saw this formula again by chance while mixing my old notebooks. I wonder if this simple formula is used somewhere. The original method Let's remember the original method first: $$\color{#c00}{ax^2+bx+c=0, ~~\text {}~a\neq 0} \\ 4a^2 x^2+4abx+4ac =0 \\ 4a^2 x^2+4abx=-4ac \\ 4a^2 x^2+4abx+b^2=b^2-4ac \\ \left(2ax+b \right)^2 =b^2-4ac \\ 2ax+b= \pm \sqrt{b^2-4ac} \\ x_{1,2}= \dfrac{\pm\sqrt{b^2-4ac} -b}{2a} \\  \bbox[5px,border:2px solid #C0A000] {x_{1,2}=\dfrac{-b\pm\sqrt{b^2-4ac}}{2a}}$$ In fact, the ""meat"" of this method is as  follows: $$\color{#c00}{{ax^2+bx+c=0, ~~\text {}~a\neq 0}}\\x^2+\dfrac{b}{a}x+ \dfrac{c}{a}=0 \\\left (x+ \dfrac{b}{2a} \right)^2- \left (\dfrac{b}{2a} \right)^2+\dfrac{c}{a}=0 \\ \left (x+ \dfrac{b}{2a} \right)^2=\dfrac{b^2}{4a^2}-\dfrac {c}{a} \\ \left (x+ \dfrac{b}{2a} \right)^2=\dfrac{b^2-4ac}{4a^2} \\ x+ \dfrac{b}{2a}= \dfrac{\pm\sqrt{b^2-4ac}}{2a} \\ \bbox[5px,border:2px solid #C0A000] {x_{1,2}=\dfrac{-b\pm\sqrt{b^2-4ac}}{2a}}$$ Construction of the general formula Now, we know that if one of the roots for $ax^2+bx+c=0$ is $x = 0,$ then our equation is equivalent to $ax^2 + bx = 0.$ No special formula is required to solve the last equation. In this sense, I am setting off by accepting that $x \neq0.$ $$\color{#c00}{ax^2+bx+c=0, ~~\text {}~a\neq 0} \\ a+\dfrac {b}{x} +\dfrac{c}{x^2}=0 \\ \dfrac{c}{x^2}+\dfrac {b}{x} +a=0 \\ \dfrac{4c^2}{x^2}+\dfrac{4bc}{x}+4ac=0 \\ \dfrac{4c^2}{x^2}+\dfrac{4bc}{x}=-4ac \\ \dfrac{4c^2}{x^2}+\dfrac{4bc}{x}+b^2=b^2-4ac\\ \left( \dfrac {2c}{x}+b \right)^2=b^2-4ac \\ \dfrac {2c}{x}+b= \pm\sqrt{b^2-4ac} \\ \dfrac {2c}{x}=-b\pm\sqrt{b^2-4ac} \\ \color{#c00}{\bbox[5px,border:2px solid #C0A000]{x_{1,2}= \dfrac{2c}{-b\pm\sqrt{b^2-4ac}}}}$$ Proof of the general formula Let's rewrite the well-known general formula as follows: $$\dfrac{-b\color{red}{\pm}\sqrt{b^2-4ac}}{2a}=\dfrac{-b\color{red}{\mp}\sqrt{b^2-4ac}}{2a}$$ If we accept $c\neq0$ , then we have: $
\dfrac{2c}{-b\color{blue}{\pm}\sqrt{b^2-4ac}}=\dfrac{-b\color{red}{\mp}\sqrt{b^2-4ac}}{2a}\\
\begin{align}
\Longleftrightarrow  \left(-b\color{blue}{\pm}\sqrt{b^2-4ac}\right)  \times \left(-b\color{red}{\mp}\sqrt{b^2-4ac}\right) &=4ac\\
\Longleftrightarrow -\left(b\color{blue}{\mp}\sqrt{b^2-4ac}\right)  \times \left( -\left(b\color{red}{\pm}\sqrt{b^2-4ac}\right)\right)&=4ac\\
\Longleftrightarrow  \left(b\color{blue}{\mp}\sqrt{b^2-4ac}\right)  \times \left(b\color{red}{\pm}\sqrt{b^2-4ac}\right)&=4ac\\
\Longleftrightarrow b^2-\left(b^2-4ac\right)&=4ac\\
\Longleftrightarrow 4ac&=4ac
. \end{align}$ Insufficient point of the formula Since we have accepted $x \neq 0$ before, this formula cannot work completely for $c = 0.$ If $c=0$ , then we have: $x_1=\dfrac {0}{-2b}=0$ which imply, one of the roots is correct. $x_2=\dfrac {0}{0}=\text{undefined}$ which imply, the second root is incorrect. Curious points of the formula These are interesting points for an untutored person like me. On the other hand, they are trivial. If the $\Delta$ $\left(\text{Discriminant}\right)$ is zero, then there is exactly one real root, sometimes called a repeated or double root. $\Delta=b^2-4ac$ $~$ or $~$ $D=b^2-4ac$ $~$ and $~$ $D=0$ , then we have : From the formula $~$ $\color{blue}{x_{1,2}=\dfrac{-b\pm\sqrt{D}}{2a}}$ , $$\color{blue}{x}=x_1=x_2=\dfrac{-b}{2a}=\color{blue}{-\dfrac{b}{2a}}$$ From the formula $~$ $\color{#c00}{x_{1,2}= \dfrac{2c}{-b\pm\sqrt{D}}}$ , $$\color{#c00}{x}=x_1=x_2=\dfrac{-2c}{b}=\color{#c00}{-\dfrac{2c}{b}}$$ which both are equal. $$\begin{align} \color{blue}{x}=x_1=x_2=\color{blue}{-\dfrac{b}{2a}} \color{black}{=} \color{#c00}{-\dfrac{2c}{b}}\Longrightarrow b^2=4ac \Longrightarrow b^2-4ac=0.\end{align}$$ The original formula does not work for $a = 0$ . However, the alternative formula also works when $a = 0$ . The important point is that we should be careful not to make the denominator zero. In other words, If $a=0$ and $b>0$ then we write: $$x=\dfrac{2c}{-b-\sqrt{b^2}}=-\dfrac {c}{b}$$ If $a=0$ and $b<0$ then we write: $$x=\dfrac{2c}{-b+\sqrt{b^2}}=-\dfrac {c}{b}$$ My question Maybe in some special cases, can this formula be more useful than its own alternative? (I assume the formula I found here is correct.)","['soft-question', 'roots', 'algebra-precalculus', 'math-history', 'quadratics']"
3619174,Find inverse laplace transform of $f(s) = \frac{1}{(s-2)^2+9}$,"Find inverse laplace transform of $f(s) = \frac{1}{(s-2)^2+9}$ Here is what I have gotten from partial fractions. I observed that $s^2-4s + 13$ is irreducible (doesn't have real roots). $$\frac{1}{(s-2)^2 + 9} =  \frac{1}{s^2-4s + 13} = \frac{As + B}{s^2-4s + 13}$$ A=0, B=1 The corresponding value in my table is $e^{at}\sin(bt)$ for the corresponding laplace transform $\frac{b}{(s-a)^2 + b^2}$ . so plugging in A and B, I get $e^{0t}\sin(1\cdot t) = \sin(t)$ . However my textbook answer is $\frac{1}{3}e^{2t}\sin{3t}$ . I am not sure how to trouble shoot this problem. I am pretty sure I did the partial fractions correctly, but the textbook answer implies that my approach to finding the inverse laplace transform is very wrong because I have no idea where the $\frac{1}{3}$ comes from in the solution.","['laplace-transform', 'ordinary-differential-equations']"
3619179,Is there any closed embedding which is not cofibration?,"Is there any closed embedding which is not cofibration? I firstly think that if $X$ is Topologist's sine curve and $A$ is $(0,0)$ , then embedding $i:A\rightarrow X$ might satisfy this condition. However, I couldn't prove there is no retraction $r:X\times I\rightarrow (X\times 0)\cup(A\times I) $ .","['cofibrations', 'general-topology', 'retraction', 'algebraic-topology']"
3619182,Proving $\sum_{k=0}^n\sum_{l=0}^k{n \choose k}^2{k \choose l}{n \choose l}{2n-l \choose n}=\sum_{k=0}^n {n \choose k}^2{n+k \choose k}^2$ [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 years ago . Improve this question I'm trying to prove this identity $$\sum_{k=0}^n\sum_{l=0}^k{n \choose k}^2{k \choose l}{n \choose l}{2n-l \choose n}=\sum_{k=0}^n {n \choose k}^2{n+k \choose k}^2$$ but I don't even know from where to start.
Any ideas?","['summation', 'binomial-coefficients', 'combinatorics']"
3619207,"Given $X, Y$ non-empty sets, is there a surjective function between $X$ and the set $\{f:Y \rightarrow X: f \text{ is function}\}$?","I was wondering to use the inclusive function but that's if $A$ were a subset of $B$ , also remark that $A$ and $B$ are any non empty sets so if I try to use function $g:X \rightarrow Y$ I think is kinda use what I want to prove.","['elementary-set-theory', 'functions']"
3619214,Closed orbits in an elliptical pool table,"Imagine an elliptical billiard table, and a ball on its edge; we hit the ball so that it starts bouncing on the walls. Now, the study of the trajectories seems to be a fairly recent topic (for instance, they are tangent to certain caustic conics), but there’s one result I cannot seem to find anywhere on the web: If the ball at some point returns to its original position, will it then repeat the same trajectory? Because nothing guarantees that, a priori, the last segment is the reflection of the first one. I have looked at a small introduction page at Wolfram , and they do say the following: On an elliptical billiard table, the envelope of a trajectory is a smaller ellipse, a hyperbola, a line through the foci of the ellipse, or a closed polygon (Steinhaus 1999, pp. 239 and 241; Wagon 1991). The closed polygon case is related to Poncelet's porism. Poncelet’s porism seems to imply the existence of infinite such polygons, for every $n$ , but I’m not really sure how it answers the question as posed. And I couldn’t find the proofs by the authors referenced without buying their books. The only related question I could find on this site was this one , but it was dealing more with the existence of closed orbits for a given number of sides and seemed to assume that the trajectory would repeat itself. I’m fairly sure there’s a bruteforce way to do this, maybe with vectors or even complex numbers - and though I’m willing to accept those, I was hoping to find a purely geometric approach to this problem. To be honest, The only thing I could think of was to try and project the problem so the ellipse became a circle, but I couldn’t find something to characterize the projected path (since the angles of reflection aren’t invariant by projection, the new trajectory wouldn’t be a bouncing ball anymore). So my question is: if the ball returns to its starting position after a finite number of bounces, how do we prove that it will repeat that same trajectory henceforth?","['euclidean-geometry', 'conic-sections', 'geometry', 'reflection']"
3619364,Orbit of a vector under the action of a $2\times 2$ matrix,"Consider the following matrix $A=\pmatrix{0 &1\\
-1&\frac{1}{3}}$ and put $e_1:=(1,0)^t$ . How can I find the elements of the orbit $A^ne_1$ , for $n\in\mathbb N$ ? It seems to me that $A$ has infinite order in $SL_2(\mathbb R)$ and moreover I don't see any closed formula for the powers of $A$ .","['matrices', 'linear-algebra', 'geometry']"
3619399,Why does the Cantor-Bendixson cupcake theorem need transfinite induction?,"Recall the Cantor-Bendixson theorem: Let $X$ be a Polish space. For every closed subset $K \subseteq X$ , there is a unique disjoint sum decomposition $C \cup P = K$ where $C$ is countable and $P$ is perfect. Moreover, $P$ is exactly the condensation set of $K$ . I know two proofs of the above theorem. One is to define the Cantor-Bendixson derivative $K \mapsto K'$ by letting $K'$ be the accumulation set of $K$ , and define $K^{(\alpha)}$ by transfinite recursion on the Cantor-Bendixson derivative, taking intersections at limit stages. One only loses countably many points at each stage, and this process must terminate at some ordinal $\alpha_K < \aleph_1$ , the ""Cantor-Bendixson rank"" of $K$ . So $P = K^{(\alpha_K)}$ ; then $C$ must be countable by the above, and one easily checks that $P$ is the condensation set of $K$ . Another apparently does not use transfinite recursion at all. Let $P$ be the condensation set of $K$ . Since $K$ is second countable, it has a countable basis $U_n$ , and $C = \bigcup_{n: |U_n| < \aleph_1} U_n$ , hence $C$ is countable. Famously, the Cantor-Bendixson theorem (where $X = \mathbb R$ ) cannot be proven without appeal to $\Pi_1^1$ -comprehension. This is proven in reverse mathematics. In particular, $\Pi_1^1$ -comprehension is stronger than arithmetical transfinite recursion ( $ATR_0$ ). So one expects that ""transfinite induction is avoidable""; most famous theorems that require $ATR_0$ or a stronger system make some sort of appeal to the transfinite. Why is the Cantor-Bendixson theorem (for $X = \mathbb R$ ) so ridiculously strong? Is the notion of condensation set the problematic step, since it requires us to be able to say "" $x \in P$ iff for every $n > 0$ , the set $A(x, n) = \{y \in K: |x - y| < 1/n\}$ is not the image of any sequence""? (Since the elements of the ""set"" $A(x, n)$ are really sets of natural numbers, I can see this being problematic exactly as an artifact of the fact that we are using Goedel codes!) Or is the sneaky appeal to countable choice in the proof that $C$ is countable the reason why we need $\Pi_1^1$ -comprehension? Neither seems much stronger than arguments one makes in soft analysis all the time, so is the problem really that those other theorems of soft analysis haven't been studied so much in reverse mathematics, and the Cantor-Bendixson theorem is just infamous because logicians have thought to analyse it?","['reverse-math', 'general-topology', 'descriptive-set-theory']"
3619406,Name for largest subset of image whose preimage is contained in the original set?,Given a function $f:X \to Y$ and a subset $A \subseteq X$ let $A^*$ be the largest subset of $f(A)$ such that $f^{-1}(A^*) \subseteq A$ . Is there a name for $A^*$ ?,"['elementary-set-theory', 'functions']"
3619426,Techniques to Prove a Map Open or Closed,"I've been finding it hard to verify that given maps are open or closed in practice. For example, in his discussion of the Mobius bundle, Lee's Introduction to Smooth Manifolds says without justification that if $E$ is the quotient of $\mathbb{R}^2$ by the equivalence relation $(x,y) \sim (x',y')$ if and only if $(x',y') = (x + n, (-1)^n y)$ for some $n \in \mathbb{Z}$ , and $q : \mathbb{R}^2 \to E$ is the quotient map, then the restriction of $q$ to $[0,1] \times \mathbb{R}$ is closed and therefore also a quotient map. Since he doesn't go into much detail on this, I assume that verifying that the restriction is closed isn't hard, but I can't seem to find any way to do it without delving deep into the weeds and proving it straight from the definition. In general I have trouble proving that maps are open or closed; are there any tricks or overall methods I should be aware of?","['closed-map', 'general-topology', 'open-map']"
3619497,$\mathbb{R}$-bundle,"Why a principal $\mathbb{R}$ -bundle is always trivial? I know that a principal bundle of the form $(E,B,\pi,G)$ is trivial if and only if it admits a global section $f:B\to E$ . So which section should I take? Is there a simpler way to prove this property?","['principal-bundles', 'differential-geometry']"
3619551,"Let $G$ be a group and $x,y\in G$. Suppose $n,m\in\mathbb{Z}$. Is it possible to rewrite $[x^{n+1},y^{m+1}]$ as a conjugate of $[x,y]$?","Let $G$ be a group and $x,y\in G$ . Suppose $n,m\in\mathbb{Z}$ . Is it possible to rewrite $[x^{n+1},y^{m+1}]$ as a conjugate of $[x,y]$ ? I am looking for commutator identities for when we are dealing with powers of elements.","['group-theory', 'abstract-algebra']"
3619562,Why is the assumption $(dx)^2 = 0$ actually correct instead of just approximately correct?,"Imagine dividing a sphere into concentric spherical shells of thickness $dr$ and inner radius $r$ . The volume of each shell is $$dV = \frac{4\pi}{3}  [ (r + dr)^3 - r^3]$$ Expand the cubic expressions, we get: $$
(r + dr)^3 - r^3 = r^3 + dr^3 + 3r^2 dr + 3rdr^2 - r^3 = 3r^2dr + 3rdr^2 + dr^3
$$ Assuming that $dr^3 = 0$ and $rdr^2 = 0$ , we get: $$(r + dr)^3 -r^3= 3r^2dr$$ Thus, the volume of each shell is $dV = 4\pi r^2dr$ . If we integrate along the radius, then we get $$\int_0^R 4\pi r^2dr =  \frac43\pi R^3$$ This confirms that our analysis of the spherical shell volumes is correct. However, this analysis relies on the assumption that $dr^3 = 0$ and $rdr^2 = 0$ . My question is why are these assumptions correct? If we assume those values are zero, shouldn't the final value just be approximately correct by an infinitesimal amount instead of being absolutely correct?","['calculus', 'derivatives', 'volume']"
3619570,Continuation of proof of intersection of two unions.,"I'm having trouble continuing a proof of the intersection of two unions. What I'm basically trying to reach is $$(X \cup Y) \cap (X \cup Z) = X \cup (Y \cap Z)$$ Here is what I've been able to do so far: $$(x \in X \lor x \in Y) \land (x \in X \lor x \in Z)$$ $$\Bigl((x \in X \lor x \in Y) \land x \in X\Bigr) \lor \Bigl((x \in X \lor x \in Y) \land x \in Z\Bigr)$$ $$\Bigl((x \in X \land x \in X) \lor (x \in Y \land x \in X)\Bigr) \lor \Bigl((x \in X \land x \in Z) \lor (x \in Y \land x \in Z)\Bigr)$$ $$\Bigl((x \in X) \lor (x \in Y \land x \in X)\Bigr) \lor \Bigl((x \in X \land x \in Z) \lor (x \in Y \land x \in Z)\Bigr)$$ And I think if I were to change something in the latter part (after the union symbol in the middle), I would rather easily arrive at what I'm trying to reach, but I can't figure out what it is that I need to change. Thanks a lot!",['elementary-set-theory']
3619607,Show that $\int_a^b f(u) \int_a^u f(v) dv du=\int_a^b f(u) \int_u^b f(v) dv du$,"Let $f:[a,b]\rightarrow \mathbb{R}$ be integrable on $[a,b]$ . Show that ( $\forall u\in[a,b]$ ) $$\int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b f(u)\int_u^b f(v) dv\; du$$ What i've done so far: I suppose $\int f$ has $F$ as an antiderivative (which is not guaranted by $f$ being just integrable, but i can't think other way). So, $$\int_a^b f(u)\int_a^u f(v) dv\; du=\int_a^b F(u)f(u)\;du-F(a)\int_a^b f(u) du=\int_a^b F(u)f(u)\;du+F(a)^2-F(a)F(b)$$ $$\int_a^b f(u)\int_u^b f(v) dv\; du=F(b)\int_a^b f(u)\;du-\int_a^b F(u)f(u) du=
F(b)^2-F(a)F(b)-\int_a^b F(u)f(u)\;du$$ In terms of areas, for let´s say $f(x)=2x$ with $[-2,3]$ , and $F(x)=x^2$ what we're integrating from $-2$ to $3$ is: $$(F(u)-F(a))f(u)=
2(u^2-(-2)^2)u=2(u^2-4)u$$ and $$(F(b)-F(u))f(u)=2(9-u^2)u$$ If i graph these from $-2$ to $3$ , they don't seem related, but the area under both curves is the same. Can someone give me an approach. ¿Is this a known result? Thanks","['integration', 'multivariable-calculus', 'calculus']"
3619612,"Extreme on the multivariate function $f(x,y) = x^2+xy+y^2+y$","Find the extreme of $$f(x,y) = x^2+xy+y^2+y$$ I think from what I learn on the function with two variables, we need to find the second derivatives to solve this kind of question. However, after I figured out both $f_x$ and $f_y$ , $$f_x=2x+y, f_y=x+2y+1$$ I am stuck on what I should do next since this function will have $f_{xx}$ , $f_{xy}$ , $f_{yx}$ , $f_{yy}$ , four second derivatives. Can someone lead me out?","['optimization', 'multivariable-calculus']"
3619686,"Non-triviality of ""Weak closures of $*$-subalgebras are von Neumann algebras""","I suspect there is a slight error in Murphy's C*-algebras and Operator Theory: Murphy defines a von Neumann algebra on a Hilbert space $H$ as a $*$ -subalgebra of $B(H)$ that is strongly closed. I mention that because others define von Neumann algebras as those that are equal to their double commutants (if $id_H\in A$ , the definitions are equivalent by the double commutant theorem, but if not, they are not the same; we can reduce to this definition by considering the unit of $A$ though, which is a projection and compressing to that subspace). Anyway, after a number of results on von Neumann algebras and the strong and weak operator topologies, Murphy says ""If $A$ is a $*$ -subalgebra of $B(H)$ , then its weak closure is a von Neumann algebra."" and he refers to this as a simple observation BEFORE moving on to Kaplansky's density theorem. He also says that this will be used in the proof of Kaplansky's theorem (but I cannot spot where he uses it). I don't think this is trivial without Kaplansky's help. I mean, obviously, since convex sets have equal strong and weak closures, if $A$ is a $*$ -subalgebra of $B(H)$ then $\overline{A}^{WOT}=\overline{A}^{SOT}$ , so this is indeed strongly closed. By weak continuity of involution, it is a self-adjoint set. It is obviously a linear subspace. But why is this a subalgebra ? Using Kaplansky's theorem, I can see why this is true: If $u,v\in\overline{A}^{SOT}$ , then we can find a norm-bounded (by Kaplansky) net $(u_\lambda)\subset A$ with $u_\lambda\xrightarrow{SOT}u$ and let $(v_\lambda)\subset A$ be a net with $v_\lambda\xrightarrow{SOT}v$ . Then since multiplication restricted on $S\times B(H)\to B(H)$ where $S$ is a bounded subset of $B(H)$ is strongly continuous, we get that $uv\in\overline{A}^{SOT}$ and we are done. Is there something that obvious that I am missing out?","['von-neumann-algebras', 'c-star-algebras', 'functional-analysis', 'operator-algebras']"
3619754,Determine where the Functions are Continous,"$$f(x,y)=\begin{cases}\dfrac{xy}{x^2+y^2}&(x,y)\neq(0,0)\\
0 &(x,y)=(0,0)\end{cases} $$ I use graph tool to help me visualize this problem, and I think it is continuous for all the graph, but the answer is wrong. Can someone guide me how to solve this kind of question?","['continuity', 'multivariable-calculus']"
3619790,Convolution of Distribution with smooth function,"I meet an exercise in my homework:
Let $f\in \mathcal{D}'(\mathbb{R}^n)$ and $g\in \mathcal{D}(\mathbb{R}^n)$ , show that the biliner map $(f,g)\mapsto f*g $ is continuous in $f$ and $g$ , respectively. I have proved that for any fixed $f\in \mathcal{D}'(\mathbb{R}^n)$ , if $g_{i}\in \mathcal{D}(\mathbb{R}^n)$ such that $g_{i}\to 0$ in $\mathcal{D}(\mathbb{R}^n)$ , then $f*g_{i}\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ , but I don't know how to prove that for any fixd $g\in \mathcal{D}(\mathbb{R}^n)$ and $f_{i}\to 0$ in $\mathcal{D'}(\mathbb{R}^n)$ then $f_{i}*g\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ . Is this conclusion correct? Here is my approach: To prove $f_{i}*g\to 0$ in $\mathcal{E}(\mathbb{R}^n)$ , we need to show that, for any compact set $K\subset \mathbb{R}^n$ , and any multi-index $\alpha$ , $$\sup_{x\in K}|\partial^{\alpha}(f_{i}\ast g)|=\sup_{x\in K}|f_{i}\ast \partial^{\alpha}g|
=\sup_{x\in K}|\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0$$ holds for fixed $g\in \mathcal{D}(\mathbb{R}^n)$ as $i\to +\infty$ . But we only have $f_{i}\to 0$ in $\mathcal{D}'(\mathbb{R}^n)$ , it seems we can only show that, for any fixed $x\in K$ , $$ |\langle f_{i}(y),\partial_{x}^{\alpha}g(x-y)\rangle|\to 0$$ for $i\to +\infty$ . Can someone help me with the question above? Thank you very much!","['convolution', 'distribution-theory', 'functional-analysis', 'partial-differential-equations']"
3619811,Is there a specific term for a function that exists solely as a set of ordered pairs and cannot be described by a mathematical formula?,"I've been reading about the Axiom of Choice, and my current understanding is that we can assert a choice function exists even in cases where it may be impossible to construct a deterministic formula that produces the behavior we're looking for. This confused me at first (how can you have a function you can't write down?) but then I remembered that functions can be understood both intensionally (as a formula) and extensionally (as a set of ordered pairs), so I assume that these undefinable choice functions must exist in a purely extensional form, devoid of intensional content. If this understanding is correct, is there a specific term for these non-intensional functions? Or are these arbitrary mappings so common that there's no need to treat them as a special kind of function? Also: would it be correct to say that the Axiom of Choice is required if and only if we need to apply a choice function to an infinite number of sets and no intensionally-defined function is available?","['elementary-set-theory', 'axiom-of-choice', 'functions', 'terminology']"
3619869,"Acceleration, velocity and position vectors","Given a vector parametrization that describes the position of a particle $\vec{r}(t)$ , is it valid to say that $\vec{r} (t)$ is perpendicular to $\vec{r'} (t)$ through the following: $$
\frac{\mathrm{d}(\vec{r}(t) \cdot \vec{r}(t))}{\mathrm{d}t} = 0
$$ so through the product rule $$
\frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) + \vec{r}(t) \cdot \frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} = 0
$$ which means $$
2\frac{\mathrm{d}\vec{r}(t)}{\mathrm{d}t} \cdot \vec{r}(t) = 0
$$ so $\vec{r}(t) \cdot \vec{r'}(t) = 0$ , meaning that they are perpendicular to each other? Whenever I draw a curve, if I draw a position vector to a minima in that curve and then the velocity vector, they are not perpendicular to each other. I feel like I'm making a very simple mistake.",['multivariable-calculus']
3619887,Is it true that year $y$ is a leap year in the Gregorian calendar if and only if $y \in (4\mathbb{Z}) \Delta (100\mathbb{Z}) \Delta (400\mathbb{Z}$)?,"A leap year in the Gregorian calendar is any year that is divisible by 4, excluding the ones that are divisible by 100, but including the ones that are divisible by 400. So, now, my question is: Is it true that year $y$ is a leap year in the Gregorian calendar (proleptic if $y \le 1582)$ if and only if $y \in (4\mathbb{Z}) \Delta (100\mathbb{Z}) \Delta (400\mathbb{Z}$ ), where $\Delta$ denotes the symmetric difference of sets? Remember that the intersection of three sets is contained in their symmetric difference. Also, BC years are to be converted to the corresponding non-positive years using astronomical year numbering . Of course, $m$ is divisible by $n$ if and only if $m \in n\mathbb{Z}$ , so this statement may in particular be applied for $n \in \{4,100,400\}$ . While this statement is about individual factors, considering symmetric differences makes things more complicated.",['elementary-set-theory']
3619889,Deriving the Jacobian and Hessian of the nonlinear least-squares function,"I'm working on a problem that involves deriving the Jacobian and Hessian of the following nonlinear least squares function. I've been thinking of expanding the function with Taylor Series expansion then try to match and find the Jacobian and Hessian, but I'm stuck right now. $f(x) = \sum_1^m [y_i - (a_i^Tx)^2]^2$ Where $x \in \Bbb R^{n}$ , $a_i^T$ is the row vector in $x \in \Bbb R^{n}$ . Multiple sources have the derivations in a general form which doesn't allow me to find the gradients in a nice vector-matrix form. Any help/hint would be incredibly helpful.","['multivariable-calculus', 'matrix-calculus', 'linear-algebra', 'least-squares', 'partial-derivative']"
3619900,The component of $T − e$ containing $v$ has at least as many vertices as the component not containing $v$.,"$\textbf{Claim:}$ For every tree $T$ there exists a vertex $v$ such that for every edge $e$ the component of $T − e$ containing $v$ has at least as many vertices as the component not containing $v$ . I am trying to prove it by contradiction, but not able to see what property of tree to contradict! $\textbf{Observations:}$ 1) The vertex $v$ can't be a leaf vertex for a tree of large size. 2) $T - e$ has two components always. 3) We can always find a vertex $v$ in the tree such that the number of vertices on the left side of $v$ is almost same as the number of vertices on the right side. They atmost  differ by $1$ .","['graph-theory', 'combinatorics', 'discrete-mathematics']"
3619910,Analyzing the given condition for a twice differentiable function $(f(0))^2 + (f'(0))^2 = 85$. [duplicate],"This question already has answers here : For every twice differentiable function $f : \bf R \rightarrow [–2, 2]$ with $(f(0))^2 + (f'(0))^2 = 85$, which of the following statements are TRUE? (2 answers) Closed 4 years ago . We are given that a twice differentiable function, $f:\Bbb R\rightarrow[-2,2]$ satisfies the condition $$(f(0))^2 + (f'(0))^2 = 85$$ We are asked if exists a value of $x$ , say $α \in (-4,4)$ , for which $f(α) + f''(α) = 0$ and $f'(α) \neq0$ . (Please note we are merely asked about the existence of such value and not the value itself) My attempt : Suppose I take a function $$p(x) = (f(x))^2 + (f'(x))^2$$ Taking the derivative I obtained, $$p'(x) = 2f(x)f'(x) + 2f'(x)f''(x)$$ Which can be rewritten as $$p'(x) = 2f'(x)[f'(x)+ f''(x)]$$ So if I can somehow prove that p'(x) = 0 at some point and f'(x) is not zero at that point, I know that there is a value of α. But not sure how to proceed any further. Can I apply LMVT somewhere? Have I missed something? Any help would be appreciated!","['limits', 'calculus', 'derivatives', 'analysis']"
3619998,Derivative of log of Gaussian pdf?,"I am trying to find the derivative of the logarithm of a Gaussian pdf with mean $\mu$ and standard deviation $\sigma$ defined over $x \in \mathbb{R}$ , but I am not certain whether my derivation is correct. Could you please verify whether this is fine, and if not, tell me where the error lies? I would start with the Gaussian pdf: $$p(x)=\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)$$ Then apply the log and the derivative operator to both sides: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right)$$ Here we can split the innermost argument on the RHS into two separate logarithms: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}log\left(\frac{1}{\sqrt{2 \pi \sigma^2}}\right)+\frac{d}{dx}log\left(exp\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)\right)$$ Recognizing that the first RHS term is constant, its derivative becomes zero. In the second RHS term, the $log$ and $exp$ cancel out. $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{(x-\mu)^2}{2 \sigma^2}\right)$$ We can expand the numerator of the term in the RHS brackets: $$\frac{d}{dx}log\left(p(x)\right)=\frac{d}{dx}\left(-\frac{x^2-2x\mu-\mu^2}{2 \sigma^2}\right)$$ Which yields, after derivation: $$\frac{d}{dx}log\left(p(x)\right)=-\frac{2x-2\mu}{2 \sigma^2}$$ which can be simplified to: $$\frac{d}{dx}log\left(p(x)\right)=-\frac{x-\mu}{\sigma^2}$$ Is this correct? Or did I make any errors?","['derivatives', 'normal-distribution', 'logarithms']"
