question_id,title,body,tags
4418390,A linear algebra problem about linear subspace of $GL_n$,"Assume $A_1,\dots,A_n$ are fixed $n\times n$ real matrices and satisfy that for any nonzero vector $v ∈ \mathbb R^n$ , the vectors $A_1v,\dots,A_nv$ form a basis for $\mathbb R^n$ . Find the integers $n$ such that the matrices $A_1,\dots,A_n$ exist.
List examples of matrices $A_1,\dots, A_n$ for those $n$ . I found this problem in a book about linear algebra. I've solved the problem for $n=1,2,4$ and odd numbers before I posted the problem. At first I thought this problem is just about the matrices, but when observing the examples for $n=1,2,4$ I noticed that the problem has something to do with the product structure of $\mathbb R^n$ and this insight allows me to solve this problem.","['matrices', 'linear-algebra', 'problem-solving']"
4418442,Approximation by simple functions for $f\in L^1(\mu)$,"I'm solving a problem about measure space. Let $(X,\mathcal{M}, \mu)$ be a measure space, and let $f \in L^1(\mu)$ . Prove that, for all $\epsilon >0$ , there exist a simple measurable function $s$ such that $$\int_X |f-s| d\mu < \epsilon.$$ The simple function $s$ can assume negative value as well. My solution is using a decomposition, $f = f^+ + f^-$ , where $f^+ = max(f,0)$ and $f^- = max(-f,0)$ . Let $s_n^+ = \sum_{k=1}^{n2^n} \frac{k-1}{2^n} I(\frac{k-1}{2^n} \leq f^+< \frac{k}{2^n})$ . Then $s_n^+ \rightarrow f^+$ , and $s_n^+$ is non decreasing simple function. By def of $s_n^+$ , for given $\epsilon >0$ , we can take $N$ large enough so that $N > f^+(x), \frac{1}{2^N}<\frac{\epsilon}{2\mu(X)}$ . $n > N, \;\;|S_n^+ - f^+| \leq \frac{1}{2^n} < \frac{1}{2^N} < \frac{\epsilon}{2\mu(X)}, \;\; \int_X |f^+ - s_n^+ | d\mu \leq \frac{1}{2^n}\mu(X) < \epsilon.$ Apply same procedure on $s^-_n$ . And since \begin{align*}
\int_X | f-s|d\mu &= \int_X | f^+-f^- -s^+ +s^-|d\mu \\
& < \int_X |f^+-s^+| d\mu + \int_X |f^- - s^-|d\mu
\end{align*} We can have the conclusion. I think If $\mu(X) = \infty$ , my solution is bit awkward I guess. But I'm not sure how can I show that $\mu(X) < \infty$ on this condition or maybe I do not need that $\mu(X) < \infty$ . Am I wrong about my thought? or My solution is completely lost?","['measure-theory', 'lebesgue-measure', 'lebesgue-integral', 'measurable-functions']"
4418449,Inverse limit of affine algebraic groups,"Let $G_i$ be an inverse system of affine Algebraic groups (ie. Group objects in category of affine varieties). Is the inverse limit an affine algebraic group scheme? More precisely, does the inverse limit have a scheme structure? Let me explain the context. Let $\pi_1(X,x)$ be the fundamental group of a compact Kahler manifold. Let us consider the inverse system of representations of $\pi_1(X)$ ie. morphisms to affine algebraic groups $G_i$ . The inverse limit is called the pro-algebraic completion of the fundamental group and is denoted as $w_1(X,x)$ . My question is : Is $w_1(X,x)$ a scheme?","['fundamental-groups', 'algebraic-geometry', 'algebraic-groups']"
4418468,Prove that a list can be made of all the subsets of a finite set such that these conditions hold,"Prove that a list of subsets of a finite set can be made such that i. The empty set is the first in the list. ii. Every subset is used only once in the list. iii. Each subset in the list is obtained by either adding one element from the preceding subset, or removing one element. So I've been thinking about this puzzle a bit, and feel like I almost see the pattern, but I'm not sure yet how to generalize it. I've been coming at it from the angle of partitions by way of cardinality on the power set $\mathcal{P}(X)$ ( $X$ a finite set), and ways to ""distribute"" the various ""cardinality classes"" of elements of $\mathcal{P}(X)$ into the $2^{|X|}$ possible places of the list such that they fulfill the requirements. Example, $n=3$ : \begin{array} {|r|r|}
\hline \text{Cardinality of subset} & \text{# subsets in } \mathcal{P}(X) \\ 
\hline 0 & 1 \\ 
\hline 1 & 3 \\ 
\hline 2 & 3 \\ 
\hline 3 & 1 \\ 
\hline  
\end{array} Using this partition of $\mathcal{P}(X)$ , we can arrange the subsets in the following way which will fulfill the requirements when filled in with the appropriate $a_i \in \mathcal{P}(X)$ (each subset $a_i$ is identified by its cardinality): \begin{array} {|r|r|}\hline \text{List Entry} & a_1 & a_2 & a_3 & a_4 & a_5 & a_6 & a_7 & a_8 \\ \hline \text{Cardinality of }a_i & 0 & 1 & 2 & 1 & 2 & 1 & 2 & 3 \\ \hline  \end{array} One interesting thing I noticed is that the partitioning of the power set by cardinality of each element (i.e., subset of $X$ ) appears to follow the Fibonacci sequence, which may or may not be useful, but feels like it should be... \begin{array} {|r|r|}
\hline \text{Cardinality of } X & \text{Partition of } \mathcal{P}(X) \\ 
\hline 3 & 1,3,3,1 \\ 
\hline 4 & 1,4,6,4,1 \\ 
\hline 5 & 1,5,10,10,5,1 \\ 
\hline 6 & 1,6,15,20,15,6,1 \\ 
\hline  
\end{array} Anyway, I managed to find a solution for $n=4$ , using the partitioning method to determine where to put subsets of each cardinality in the sequence. The following I believe to be a framework for a solution for $n=4$ (replace the cardinality identifier placeholders with the appropriate subsets such that the intersections work out correctly): \begin{array} {|r|r|}\hline \text{List Entry} & a_1 & a_2 & a_3 & a_4 & a_5 & a_6 & a_7 & a_8 & a_9 & a_{10} & a_{11} & a_{12} & a_{13} & a_{14} & a_{15} & a_{16} \\ \text{Cardinality of $a_i$} & \hline 0 & 1 & 2 & 3 & 2 & 3 & 2 & 3 & 4 & 3 & 2 & 1 & 2 & 1 & 2 & 1 \\ \hline  \end{array} However, so far I've not been able to generalize this for any $n$ . I guess it gets a bit more complicated than just the cardinalities, because you also have to guarantee that in each pair of entries in the list you have $\geq 1$ element in the intersection of the two (since we can only add or remove one element at a time). I also think that the correct way to arrange the subsets may differ depending if $n$ is even or odd. So I was thinking if I can derive some fact about how often each element $x_n \in X$ must appear in each ""cardinality class"" of $\mathcal{P}(X)$ (so that we can say it's possible to choose an element from that class for the next list entry and still have a nonempty intersection), as well as an algorithm for placing the various elements of the cardinality classes among the list entries, then I could prove the result in general. Am I on the right track with any of this?","['contest-math', 'set-partition', 'elementary-number-theory', 'solution-verification', 'elementary-set-theory']"
4418492,Solving the ODE $\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy}$,"In my ODE course, I've stumbled upon the following ODE whose RHS is a rational function. $$\frac{dy}{dx}=-\frac{2x+3y+y^2}{x+2xy}.$$ It seems that the question is insinuating that we can solve using elementary tools. The first thing I thought is seeing if we get an exact form. Letting $N=x+2xy$ and $M=-(2x+3y+y^2)$ we have that $$M_y=-3-2y=-2-(1+2y)=-2-N_x$$ So the form $Mdx+Ndy$ is not exact. If I want to make it exact by multiplying by an integrating factor $\mu$ , then I must solve the PDE $$\mu_yM+\mu M_y=\mu_xN+\mu N_x=\mu_xN+\mu(-M_y-2)$$ But it doesn't seem I can get much from this. Am I missing some obvious trick? Only hints, no full answers please! I realized I made a mistake with the problem in fact the equation is $$\frac{dy}{dx}=-\frac{\color{red}{3}x+\color{red}{2}y+y^2}{x+2xy}.$$ This means that $$M_y=-2-2y=-2(1+y)=-2N_x.$$ But still that doesn't help. In any case, I will be interested in an answer to any of these equations.","['calculus', 'ordinary-differential-equations', 'partial-differential-equations']"
4418518,"Prove that $A-A = X$, where A is subset of Banach Space and dense $G_{\delta}$ set.","$X$ is Banach Space and $A\subseteq X$ . $A$ is dense $G_{\delta}$ set. We have to show that $A-A=X$ . Since $A$ is dense we can write $\bar{A}= X$ . So, it is enough to show that $A-A= \bar{A}$ . Here $A-A = \{y-z | y,z\in A\}$ . Consider $\tau$ is the induced topology by $X$ . Since $X$ is normable, then $\tau$ has a bounded convex nbd of 0. Let $V$ be a bounded, balanced, convex nbd of 0. So, $A= \cap_{r\in \mathbb{Q} } rV$ . From this I can claim that $A-A\subseteq \bar{A}$ . How to show other side? Any alternative methods also appreciated.","['banach-spaces', 'topological-vector-spaces', 'functional-analysis']"
4418523,"Angles of two connected convex quadrilaterals, all lengths given","Consider two convex quadrilaterals sharing one edge with lines meeting at the endpoints of this edge being collinear, so that the seven edges form another, larger convex quadrilateral (see picture). With all lengths but no angles given, these quadrilaterals are still perfectly defined. Now I want to calculate the angles for given lengths $a$ to $g$ (any angle, from there I can calculate the others) If you can help me, it's greatly appreciated! It's for a mechanism I want to build :)","['quadrilateral', 'geometry']"
4418543,continuity of a function defined in pieces,"Frequently i have to prove in topology case by case that some functions defined in pieces are continous. I would like to solve the general case so that i don't have to prove it everytime in some particular cases. Let $X$ and $Y$ be a Topological space. Let $X_1 \subseteq X$ and $X_2 \subseteq X$ be such that $X_1\ne \emptyset $ , $X_2\ne \emptyset $ and $X_1 \cup X_2 = X$ . Let $ g: X_1 \to Y$ and $h: X_2 \to Y$ be continous functions such that $g(x)=h(x) \forall x \in X_1 \cap X_2$ When does the function $f:X \to Y$ defined by $f(x)=g(x) \forall x \in X_1$ and $f(x)=h(x) \forall x \in X_2$ is continous? I don't think this is true in general but i suspect that to prove this fact i need something like a separable property on the domain. For example i think i need that for every point in $X_1$ that is not in $X_2$ i can find a neighbourhood (in $X$ ) of this point which is entirely contained in $X_1$ and vice versa. With this hypothesis i think i could prove that $f$ is continous in every point of $X$ . I would like to ask you if this intuition is right or if this statement is true in a more generale case.","['continuity', 'general-topology']"
4418557,When should I use partial fractions in generating functions,"I am currently studying generating functions and I don't understand why should I use partial fraction decomposition when solving $x^n$ coefficient of a question.
For example, in this function $$\frac{1}{(1-x)(1-2x)^2} $$ Why should I use partial fractions to divide it to this expression $$\frac{1}{1-x} - \frac{2}{1-2x} + \frac{2}{(1-2x)^2} = \sum_{i=0}^\infty \left(x^i - 2(2x)^i + 2 \binom{i+1}{1}(2x)^i\right) $$ to get the coefficient of $x^n$ and not transfer it to sums from the start like $\begin{array}{cc}\frac{1}{1-x} = \sum_{i=0}^\infty x^i & \frac{1}{(1-2x)^2} = \sum_{i=0}^\infty \binom{i+1}{1}x^i \end{array}$ Therefore, $$\frac{1}{(1-x)(1-2x)^2}  = \sum_{i=0}^\infty x^i * \sum_{i=0}^\infty \binom{i+1}{1}x^i$$ and then find the coefficient from convolution. Or this function instead of doing partial fraction I can do this $$\frac{1 - x^2}{(1-x)^3} = (1-x^2) * \sum_{i=0}^\infty \binom{i+2}{2}x^i$$ and use convolution again","['partial-fractions', 'discrete-mathematics', 'generating-functions']"
4418582,A question about the determinant of a binary matrix,"Is it true that for every positive integers $n$ and $m < n$ there exists a square matrix of order $n$ that contains only zeros and ones, whose columns contain exactly $m$ ones  (and hence $n(n-m)$ zeros) and whose determinant is not equal to zero?  It is clear that one can speak of rows instead of columns.  Here is an example of such a determinant for $n=5$ and $m=3$ : $$
\begin{vmatrix}
1 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 1 & 0 \\
1 & 0 & 1 & 0 & 1 \\
0 & 1 & 1 & 1 & 1 \\
1 & 1 & 0 & 1 & 1
\end{vmatrix}
$$ I see experimentally using MathCAD that it seems to be true.  But how to prove it?","['matrices', 'determinant', 'linear-algebra']"
4418658,Expected value and variance of number of coin flips until two consecutive tails are flipped,"I'm working with a problem from an old exam where one had to calculate the expected value and variance of the number of throws, let's call it $N$ , before we get two tails in a row. We also assume the coin to be fair, meaning the probability of getting head and tails is just as equal. For our sake, let's also form the events $T$ for flipping a tail, and $H$ for flipping a head. In order to calculate the expected value, we need to find the pmf of our stochastic variable $N$ . This can easily be done by first examining some base cases of $p(k):=P(N=k)$ . Furthermore, we have that $V_N \in \{2,3,\dots\}$ . For $N = 2$ , $P(N=2) = P(T \cap T) = 1/4$ trivially.
For $N = 3$ , $P(N=3) = P(H \cap T \cap T) = 1/8$ also trivially. From this we notice a pattern, before every ending $TT$ we have to place out a $H$ , meaning this position is always determined. For instance $N = 4$ , we have that the last three letters are $HTT$ , and for the first position, we have 2 choices, meaning $P(N=4) = 2 / 2^4 = 1/8$ So what about the case when $N = k$ ? We already know that the last three letters are determined. Meaning we have a total of $2^{k-3}$ choices left to do. But from this, we have to subtract the number of $TT$ - ""strings"" that may arise in the rest of our $k-3$ positions. However, from here, I struggle to find the number of combinations for which we don't get a $TT$ somewhere along the $k-3$ positions. I know that as soon as we get $T$ , we must choose $H$ , but as soon as we get $H$ , we have $2$ choices to make. Maybe this is a better way of tackling the problem instead of the method I used above. Still, I don't really see how to cover all the cases, and I'd be glad if anyone could share these details. Also, I'd be thankful if you didn't share a whole solution to the expected value and variance, since I'll try to solve it on my own. Thanks.","['expected-value', 'variance', 'probability']"
4418664,"Let $a$ be a set accumulation point $X$. Show that there is either an increasing sequence, or a decreasing sequence of...","Let $a$ be a set accumulation point $X$ . Show that there is either an increasing sequence, or a decreasing sequence of points $x_n \in X$ with $lim_{x_n}=a$ . Let $A_n = (a -\frac{1}{n}, a)$ and $B_n = (a, a +\frac{1}{n})$ , as $a \in A'$ then one of these sets has infinite elements of $A$ , if $A_n$ is infinite we can define $(x_n)$ increasing with $lim x_n = a$ otherwise we define $(y_n)$ decreasing, both with limit $a$ . That's right? is enough? Thanks.","['limits', 'solution-verification', 'problem-solving', 'real-analysis']"
4418671,Why arithmetic mean gives the closest number in a data set? [duplicate],"This question already has answers here : Arithmetic mean. Why does it work? (6 answers) Closed 2 years ago . I just enrolled in AP Statistics course this week. And, one thing that popped up a lot in descriptive statistics is the idea of the mean. I understand mean as the closest data point to all of the data points. But, I simply don't understand the rationale behind it. How the one who made this formula come to think about it? What's intuition? Can anyone resolve that for me? Why mean = Sum of observations / Number of observations?","['expected-value', 'statistics', 'average']"
4418679,How do I show that a continuous function preserves a.s. convergence?,"I have the following question: We have $(X_n)_{n},X$ a collection of real valued random variables which are defined in $(\Omega, F, \Bbb{P})$ . And $f:(\Bbb{R},B(\Bbb{R}))\rightarrow (\Bbb{R},B(\Bbb{R}))$ a continuous function.
I need to show that if $X_n\rightarrow X$ a.s. then $f(X_n)\rightarrow f(X)$ a.s. So I mean if we assume $X_n\rightarrow X$ a.s. this means that there exists $N\in F$ s.t. $\Bbb{P}(N)=0$ and for all $\omega \in \Omega \setminus N$ $$X_n(\omega)\rightarrow X(\omega)$$ I thought maybe I can apply $f$ to $N$ but this confuses me a bit since $f$ is not neccessairly defined on $N$ . Then I thought one could do it by contraposition but also there I got stuck. Could maybe someone help me?","['convergence-divergence', 'almost-everywhere', 'probability-theory', 'probability']"
4418690,Find the number of failed Secret Santa games for N people,"N people play Secret Santa. They draw their names out of a box where: A person draws a name from the box If they draw their own name the game is failed Else the next person draws a name and this process repeats I am trying to find how many failed games occurred for N people. At first I thought this could be simply calculated by doing permutations - derangements, however this has extra permutations with numbers/people after the collision (which does not need to be accounted for since the game has failed and should start again) I then looked at permutations with a single fixed point however the same issue arises. Example:
For 2 people either the first person draws the others name and second draws the first players name OR the first person pick there name and the game fails. Hence for N=2, the number of failed games is 1. Does anyone know how I could exclude these extra combinations from my approach or perhaps another way I could solve this problem?","['permutations', 'derangements', 'combinatorics']"
4418749,A direct proof to Fermat's Little Theorem,"Hello everyone I think I've proven Fermat's Little Theorem which I know it to be: $$x^p \equiv x \pmod p$$ I use the same trick I used to calculate sum formulas which I shared yesterday here. Proof: If $p=2$ the result is obvious (if a number is odd then it's square is odd and the same for even numbers), let's say $p$ is an odd prime for the proof. Notice that $$\sum_{i=1}^{x} i^p-{(i-1)}^p=x^p \tag{*}$$ This is the property we will use, let's expand that ${(i-1)}^p$ part, by binomial theorem we have $$(i-1)^p=\sum_{k=0}^{p} {p \choose k} \cdot i^{(p-k)}(-1)^k$$ Let's expand this for the first and last term to get: $$(i-1)^p={p \choose 0} \cdot i^{(p-0)}(-1)^0+\sum_{k=1}^{p-1}{p \choose k}.i^{(p-k)}(-1)^k+{p \choose p} \cdot i^{(0)}(-1)^p$$ Here as $p$ is an odd prime $(-1)^p=-1$ , if we substitute this in we get: $$(i-1)^p=i^p+ \left(\sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^k \right)-1$$ If we substitute this in $(*)$ we get: $$\sum_{i=1}^{x}\left( i^p-\left(i^p+\left( \sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^k \right)-1\right)\right)=x^p$$ If we do the algebra we get: $$\sum_{i=1}^{x}\left(- \sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^k \right)+\sum_{i=1}^{x}1=x^p$$ We can see that $\sum_{i=1}^{x}1=x$ , substituting this in gives us: $$\sum_{i=1}^{x}\left( \sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^{k+1} \right)+x=x^p$$ $$\sum_{i=1}^{x}\left( \sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^{k+1} \right)=x^p-x$$ Now here we know that when $p$ is a prime and $k$ is not $p$ or $0$ , the combination in ${p \choose k}$ is always divisible by $p$ , in that sum every combination satisfies this so everything that is summed there are divisible by $p$ , consequently so is their sum, then we have the following: $$p|\sum_{i=1}^{x}\left( \sum_{k=1}^{p-1}{p \choose k} \cdot i^{(p-k)}(-1)^{k+1} \right)$$ which means $p|x^p-x$ and we know that in modulus this implies $$x^p \equiv x \pmod{p}$$ Is this proof valid? Thank you for reading.",['number-theory']
4418754,Sources for reading about integration techniques,"I wanted to know about techniques like Contour Integration, Leibniz Rule and so on. Complex analysis textbooks, like other analysis textbooks, only discuss generalisations and simple integration using learned material. I came across the Leibniz rule somehow, but a systematic treatment in some textbook could not be found. So, I wanted to know if textbooks or papers exist which explain integration through these and other advanced techniques, as well as discuss integrals with fair amount of complexity that can be solved.",['integration']
4418768,Prove that 3 is not a quadratic residue of $2^n-1$ when n is odd.,"The problem is to show that 3 not a quadratic residue mod $2^n-1$ when n is odd and $n>\geq 3$ .
As for now I can see that $2^n-1$ is $1$ mod $3$ which means that $2^n-1$ has an even number of prime factors of the form $3k+2$ . However, I can't see if this is useful. Any suggestions on how to approach the problem?","['number-theory', 'quadratic-residues']"
4418823,How to simplify this differential equation of orbital motion,"While solving for expression of orbit of a body under central force , this method used some substitution to simplify given problem But I cannot understand the simplification of $\left(\frac{\mathrm{d} r}{\mathrm{~d} \theta}\right)^{2}=\left(\frac{L^{2}}{k m}\right)^{2} \frac{1}{u^{4}}\left(\frac{\mathrm{d} u}{\mathrm{~d} \theta}\right)^{2}$ into $u^{\prime}(\theta)^{2}=-u^{2}+2 u+\frac{2 E L^{2}}{m k^{2}}$ so, anyone can please describe it in bit more detailed steps","['physics', 'calculus', 'ordinary-differential-equations']"
4418841,Supremum norm on the space of measures?,"Let $C\equiv C([0,1];\mathbb{R}^n)$ be the space of all continuous paths in $\mathbb{R}^n$ , and $\|x\|:=\sup_{t\in[0,1]}|x_t|$ , and denote by $M_C$ the vector space of all signed (Borel) measures on $(C,\|\cdot\|)$ . Clearly, we can define $$\tag{1}\|\mu\|_\infty := \int_{C}\!\|x\|\,|\mu|(\mathrm{d}x) \quad \text{ for each } \mu\in M_C,$$ where $|\mu|$ is the total variation of $\mu$ . (So that, for example, $\|\mathbb{P}_Y\|_\infty= \mathbb{E}[\sup_{t\in[0,1]}\!|Y_t|]$ for $Y=(Y_t)_{t\in[0,1]}$ a continuous stochastic process in $\mathbb{R}^n$ .) I was wondering if (1) defines a norm on $M_C$ whose name you know?","['probability-theory', 'functional-analysis', 'measure-theory', 'real-analysis']"
4418866,Submartingale under equivalent change of probability measure,"Setting We work on a filtered probability space with finite time horizon $T$ and let $Q$ be a probability measure equivalent to $P$ . Let $\rho_t=E[dQ/dP |\mathcal{F}_t]$ , then $\rho$ is a martingale. Result A process $M$ is a $Q$ -martingale if and only if $\rho M$ is a $P$ -martingale. See for example Lemma 5.3.4 in this book . Question Does the same hold if we replace ""martingale"" by ""submartingale"", i.e. is $M$ a $Q$ -submartingale if and only if $\rho M$ is a $P$ -submartingale? Proof (this is the proof of Lemma 5.3.4 in the above book adapted to the submartingale setting) Let $M$ be a $Q$ -submartingale. Then $E[|\rho_t M_t|]=E[\rho_T |M_t|]=E_Q[|M_t|]<\infty$ , showing that $\rho_t M_t$ is $P$ -integrable. Next, for $s\leq t$ and $A\in\mathcal{F}_s$ , $$E[\rho_tM_t1_A]=E[\rho_TM_t1_A]=E_Q[M_t1_A]\geq E_Q[M_s 1_A]=E[\rho_T M_s 1_A]=E[\rho_sM_s 1_A].$$ It is not clear if the next step is indeed true. Thus $E[\rho_tM_t|\mathcal{F}_s]\geq \rho_sM_s$ , showing that $\rho M$ is a $P$ -submartingale. Proof (of the step) Let $N :=\rho M$ . We know $E[(N_t-N_s)1_A]\geq 0, \forall A\in\mathcal{F}_s$ . Define $D:=\{E[N_t|\mathcal{F}_s]-N_s=E[N_t-N_s|\mathcal{F}_s]<0\}\in\mathcal{F}_s$ . Thus $$0\leq E[(N_t-N_s)1_D]=E[E[N_t-N_s|\mathcal{F}_s]1_D].$$ This implies $P[D]=0$ and thus $E[N_t|\mathcal{F}_s]\geq N_s$ a.s.","['stochastic-processes', 'measure-theory', 'finance', 'probability-theory']"
4418968,"Why is $y'=\frac{x^2}{y^2}$ a Bernoulli equation and not a linear one, but $y'=-\frac{2y}{x}$ is Linear and not Bernoulli?","Im going through this book on differential equations: Differential Equations (Schaum's Outlines) 4th Edition In chapter 3, Supplementary Problems I have to determine whether an equation is homogeneous and/or linear, and if not linear, whether it is a Bernoulli equation. Definition of a Linear equation: $y' + p(x)y=q(x)$ Definition of a Bernoulli equation: $y' + p(x)y=q(x)y^n$ I understand why: $y'=\frac{x^2}{y^2}$ is a Bernoulli equation. It cannot be a Linear one because when we rearrange everything we get: $y' + 0 = x^2 \cdot y^{-2}$ So we get: $
n=-2 \\
p(x)=0 \\
q(x)=x^2
$ Because we have two $y$ 's (although with a - sign in their powers, it doesn't matter), so we cannot formulate a Linear equation. But here: $y'=-\frac{2y}{x}$ is not a Bernoulli equation but a Linear one. I can see why it is a Linear equation, if we rearrange things we get: $y' + 2\frac{1}{x}\cdot y = 0$ So we get: $
p(x)=2\frac{1}{x} \\
q(x)=0
$ But why cant it also be a Bernoulli equation with $q(x)=0$ ?",['ordinary-differential-equations']
4419069,The conditions for $\lim_{x\to\infty}{g(x)\bar{F}(x)}=0$,"Let $X$ be a nonnegative random variable with distribution $F$ , and $g$ is a continuous function such that $E[g(X)]< \infty$ . Is $$
\lim_{x\to\infty}{g(x)\bar{F}(x)} = 0
$$ always true, where $\bar{F}(x) = 1-F(x)$ is the survival function? If not, what other conditions should $g$ have?","['measure-theory', 'probability-theory', 'probability']"
4419072,How many $5$ letter words can be made from $15$ letter set where multiple conditions must be met,"a) How many $5$ -letter words can be made using letters from the $15$ letter set $\{A, B, C ... , O\}$ such that the letters are all different and in alphabetical order? b) How many are there if we add the condition that no word begins OR ends with a vowel? I understand part a) . It's just $\binom{15}{5}$ . But I am having trouble with b) I thought of creating two sets such as $A$ for all words that start with a vowel and set $B$ for all words that end in a vowel and then finding $A \cup B$ and subtract that from $\binom{15}{5}$ but I am not sure. Any help and guidance would be appreciated.","['inclusion-exclusion', 'combinatorics', 'discrete-mathematics']"
4419077,Proving a function is a bijection,"Let $f : \mathbb{C} \to \mathbb{C}$ $$f(x)=x^2 + 2x + 1$$ Prove whether f is a bijection or not. This is what I have so far Let $x_1,x_2 \in \mathbb{C}$ . suppose $f(x_1)=f(x_2)$ . $$x_1^2 + 2x_1 +1 = x_2^2 + 2x_2 + 1$$",['functions']
4419080,Existence of Continuous Function on a Complex Region,"I am working on the following problem: Let $\Omega = \mathbb C\backslash [-1,1]$ , i.e. deleting ``the line'' only, is there a function $f:\Omega\to \mathbb C$ such that $f$ satisfies $f(z)^2 = 1-z^2$ and is continuous on this region? My guess is that such a function would exist, but requires a piecewise definition. A candidate solution I have been working on is $f(z) = e^{\frac{1}{2}\log(1-z^2)}$ . The problem with this solution is having the domain, as I realize it is possible to have $1-z^2>1$ , where my solution is well-defined. Is there a way to work around this or should I try something else? Thanks!","['complex-analysis', 'continuity']"
4419154,"How to evaluate $\sum_{k=1}^{\infty}\frac{B\left(k, \frac{1}{2}\right)}{(2k+1)^2}$, where $B(x, y)$ is the Beta function?","I am trying to evaluate this sum: $$\sum_{k=1}^{\infty}\frac{B\left(k, \frac{1}{2}\right)}{(2k+1)^2}$$ where $B(x, y)$ is the Beta function.
Checking with WolframAlpha gives beautiful result: $4-4G$ where $G$ is the Catalan constant. I tried to use the integral definition of Beta function: $$B\left(k, \frac{1}{2}\right)=\int_{0}^{1}x^{k-1}(1-x)^{\frac{-1}{2}}dx,$$ so $$\sum_{k=1}^{\infty}\frac{B(k, \frac{1}{2})}{(2k+1)^2}=\sum_{k=1}^{\infty}\frac{1}{(2k+1)^2}\int_{0}^{1}x^{k-1}(1-x)^{\frac{-1}{2}}dx.$$ After changing the order of summation and integration: $$\int_{0}^{1}\frac{1}{\sqrt{1-x}}dx\sum_{k=1}^{\infty}\frac{x^{k-1}}{(2k+1)^2}.$$ And getting stuck, because the latter summation will lead to Lerch transcendent: $$\int_{0}^{1}\frac{\Phi\left(x,2,\frac{3}{2}\right)}{4\sqrt{1-x}}dx.$$ And I don't know how to progress further.","['integration', 'beta-function', 'calculus', 'sequences-and-series']"
4419211,How can a function be a set? [duplicate],"This question already has answers here : Confusion about the definition of function (2 answers) Closed 2 years ago . Intuitively, a function is a rule that takes something as an input and gives an output. It can also be stated as that a function is a rule that maps elements from a set to the elements of a different or the same set. Both the above statement can be regarded as almost same. Everything was a fine till I came to know the set-theoritic definition of a function. According to that, a function is a set of ordered pairs such that all the first component of pairs which come from the same set, is related to the second component of pairs which also come from a common set. But function is like something I stated above and set is just a collection of things having something common. How can set do the work of function? I understand that each function can correspond to a set of oredered pairs in which first component is input and second component is the output but still set can't be thought same as a function. Function is a rule. Set is just a collection. How does the intuition of function connect to the set theoritic definition of function? Apologies if I sound like a overthinker. Thanking in advance. EDIT: Function takes something and if function is defined as a set that it should behave in the same way but set does not behave in that way. Also, I think that the definition might be for checking whether something is function or not. Since we can create a unique set out of every function, so may be the definition is not for defining what a function is (because we understand that informally) but for defining what can be a function.","['elementary-set-theory', 'definition', 'functions']"
4419222,Derivative of Hadamard Product of two vectors,"How can I compute the following derivative? $$\frac{\partial(K u \circ T u)}{\partial u}$$ $K$ and $T$ are constant matrices, $u$ is an unknown vector. and $\circ$ is Hadamard product. my solution: $$\frac{\partial(K_{ij} u_j \circ T_{mn} u_n)}{\partial u_p} = K_{ij}\frac{\partial(u_j)}{\partial u_p}\circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \frac{\partial u_n}{\partial u_p} = K_{ij}\delta_{jp} \circ T_{mn} u_n+K_{ij} u_j \circ T_{mn} \delta_{np}=K_{ip} (\sum_n T_{mn} u_n)+T_{mp} (\sum_j K_{ij} u_j).$$ therefore it can be written as follow $$\frac{\partial(K u \circ T u)}{\partial u} = K^T(Tu)+T^T(Ku).$$ where $\square^T$ is the transpose of the matrix.","['hadamard-product', 'derivatives', 'vectors']"
4419223,Is it hard to tackle the integral $\int_{0}^{\infty} \frac{x^{2}}{\left(1+x^{4}\right)^{2}} d x?$,"Putting $ \displaystyle=4, \alpha=2, n=2 $ in my post , $$\int_{0}^{\infty} \frac{x^{n}}{\left(1+x^{m}\right)^{\alpha}}dx=\frac{\pi}{m(\alpha-1) !} \csc\frac{(n+1) \pi}{m}\prod_{k=1}^{\alpha-1}\left(\alpha-k-\frac{n+1}{m}\right)$$ we can conclude that $$
\begin{aligned}
I &=\frac{\pi}{4(2-1) !} \csc \frac{3 \pi}{4}\left(1-\frac{3}{4}\right) \\
&=\frac{\sqrt{2} \pi}{16}
\end{aligned}
$$ Question : Is there any other method? Your suggestion and alternative are warmly welcome.","['integration', 'trigonometry', 'definite-integrals']"
4419226,"Integral of function for $ f : [−1, 1] → \mathbb{R}$","$$ f : [−1, 1] → \mathbb{R}$$ What is the value of $$ \int_{-1}^{1} f (x^2 -1) dx$$ My approach If i look at the function then it is forming $f(x) = -\left| {x} \right|+1$ in the domain of $[-1,1]$ If i analyze the function then i get $f(x) = x+1$ for $x\in[-1,0]$ and $f(x) = -x+1$ for $x\in[0,1]$ . If i continue solving this i get $2$ i.e. $(2+ \frac{1}{3} -\frac{1}{3})$ but the answer is $\frac{2}{3}$ .","['calculus', 'functions']"
4419249,Which shape is the worst to pack in $\mathbb{R}^n$?,"Yesterday I bought a box of chocolates and remarked with a friend how they had just put enough in to get above a certain transparent window to the inside but everything else was empty space. I added that this was probably particularly bad because it doesn't seem like the spherical-shaped chocolates would pack very well (in comparison to, say, cubes). I wondered if in fact the sphere is the worst ""shape"" to pack, but I'm not entirely sure what the most natural formalisation of this question is. The following is my first attempt, which I have no idea how to prove, but I am open to suggestion for better formalisations: We will call a tile of $\mathbb{R}^n$ a convex compact subset and we will say two tiles are almost disjoint if their intersection is a subset of their boundaries. I can't necessarily remember what little measure theory I knew but probably we want some kind of normalisation condition and I think convex compact subsets are measurable, so let's say we also want tiles to have (Lebesgue) measure one. Is it true that the tile with least packing density for any $n$ is the $n$ -dimensional closed ball (of appropriate size to have volume 1)? If we remove the convexity condition but say, we still want our tiles to be connected, my thoughts are that there is in fact no tile of least packing density as we can just take $n$ -dimensional (closed) annuli which are thinner and thinner with increasing radii to retain having volume 1 and these have arbtrarily low packing density. Is this correct also? EDIT: Based on a suggestion in the comments, I wish to clarify that ""packing"" here permits tiling by translation and rotation of the tiles.","['spheres', 'convex-analysis', 'geometry', 'packing-problem']"
4419306,"How can I solve $\int (f^2 - x f_x f ) \, dx$?","I am trying to solve the following integral $$\int (f^2 - x f_x f ) \, dx$$ where $f:=f(x)$ is a differentiable function and $f_x = \frac{df}{dx}$ . By substituting $u = \frac{f^2}{x^2}$ , we obtain $du= \frac{2}{x^3} (x f_x f - f^2) \, dx$ and then $$\int (f^2 - x f_x f ) \, d x = -\int \frac{x^3}{2} \, du.$$ Now, using integration by parts, we get $$\int (f^2 - x f_x f ) \, d x = - \frac{x}{2}f^2 + \frac{3}{2} \int f^2 \, dx.$$ I'm unable to proceed further from this point. How can I solve this integral? Thanks in advance.","['indefinite-integrals', 'calculus', 'derivatives']"
4419307,Directional derivative definition versus gradient,"Given the following scalar field $$f(x,y) = \begin{cases}
           \frac{y^3}{x^2+y^2} & (x,y)\ne(0,0) \\
            0 & (x,y)=(0,0)
           \end{cases}$$ find its directional derivative in the direction of $(3,2)$ at the point $(0,0)$ . First way I wanted to do this was with the gradient. However, as neither partial derivative exists at $(0,0)$ , I need to use their limit form instead: $$\frac{\partial}{\partial x}f(x,y)=\lim_{h\to 0}\frac{f(h,0)-f(0)}{h}=\lim_{h\to 0}\frac{f(h,0)}{h}=\lim_{h\to 0}\frac{0}{h^3}=0$$ $$\frac{\partial}{\partial y}f(x,y)=\lim_{k\to 0}\frac{f(0,k)-f(0)}{k}=\lim_{k\to 0}\frac{f(0,k)}{k}=\lim_{k\to 0}\frac{k^3}{k^3}=1$$ Yielding my gradient at $(0,0)$ : $$\nabla f = \vec{(0, 1)}$$ Using this to calculate the directional derivative with $u = (3,2)$ , I get $$\nabla_{u} f(0,0)=\nabla f_{(0,0)}\frac{u}{|u|}=\frac{1}{\sqrt{13}}(3,2)\cdot{}(0,1)=\frac{2}{\sqrt{13}}$$ However, if I use the directional derivative definition here: $$\lim_{h\to 0}\frac{f(\vec{a}+h\vec{u})-f(\vec{a})}{h|u|}=\lim_{h\to 0}\frac{f(h(3,2))}{h\sqrt{13}}=\lim_{h\to 0}\frac{\frac{(2h)^3}{(3h)^2+(2h)^2}}{h\sqrt{13}}=\frac{8}{13\sqrt{13}}$$ which was the gradient answer but cubed. What should I expect from these answers? And what have I done wrong that yields these different but similar answers?","['scalar-fields', 'multivariable-calculus', 'partial-derivative', 'limits', 'derivatives']"
4419312,Entropy and probabilistic Algorithms,"Recall entropy, from basic information theory: The entropy of a probability distribution $D$ on a finite set $X$ is $$H(D)=\sum_{x\in X}{p(x) \cdot \log_2{\!(1/p(x))}}$$ I was able to prove that the maximum entropy of any distribution over $[n]$ is $\log{\!(n)}$ and it is achieved by the uniform distribution.  Also, I was able to prove that when we group 2 parameters from $D$ —​let's say, $D=\{p_1,\dots,p_n\}$ and $D'=\{p_1,\dots,p_{n-2},p_{n-1} + p_n\}$ —​then $H(D')\leq H(D)$ . I need to show using the above that if I have a biased coin with probabilities $p$ and $1 − p$ of each outcome, that in order to obtain a length- $k$ sequence of unbiased coin-flips, then you need on average to use $k/H(p, 1 − p)$ tosses of
your biased coin. My calculations are as follows:-
for the biased coin, in order to make an unbiased coin from it then I flip the coin twice, if it lands on different faces then I take the first face as the answer , otherwise, I have the same result in the 2 tosses then I toss again.
the expectation of this is $1/(2p(1-p))$ so in order to get $k$ tosses the expected value would be $k/(2p(1-p))$ and the entropy $$H(p, 1 − p) = p\cdot\log((1-p)/p) + \log(1/(1-p))$$ I can't make a connection between the entropy and my answer.","['entropy', 'probability', 'information-theory']"
4419318,How many more odd divisors are there than even divisors?,"Let $f(k)$ be the number of odd divisors of $k$ and $g(k)$ be the number of even divisors. Define $F(n) = \sum_{k \le n} f(k)$ and $G(n) = \sum_{k \le n} g(k)$ . Thus $F(n)$ and $G(n)$ are the total number of odd and even divisors of natural numbers up to $n$ . Experimental data show that $$
\lim_{n \to \infty}\frac{F(n)  - G(n)}{n} = \log 2
$$ Question : Is the above limit true? Motivation : For a different question I had written a program to find length of the period $l_p$ of $1/p$ . It is known that $l_p|p-1$ so we only need to search among the divisors of $p_1$ to find the smallest divisor $d$ such that $10^d - 1$ is divisible by $p$ . This computation is slow but I observed that overall the program runs much faster if we first scan through even divisors first and only if we do not find a $d$ then we search through odd. This is because about $2/3$ of the divisors of $p-1$ seems to be even. This led me to investigate the proportion of odd and even divisors among natural numbers. Source code: p = 1
step = target = 10^6
odd = even = 0

while True:
    d = divisors(p)
    l = len(d)
    i = 0
    while i < l:
        e = d[i]
        if e%2 == 1:
            odd = odd + 1
        else:
            even = even + 1
        i = i + 1
        
    if even > odd:
        print(""Found"", p, odd, even)
    
    if p >= target:
        t = odd + even
        print(p, odd, even, odd/t.n(), even/t.n(), (odd - even)/p.n())
        target = target + step
        
    p = p + 1","['number-theory', 'divisibility', 'elementary-number-theory', 'prime-numbers']"
4419342,How do I show that the following function is analytic using the Morera theorem?,Let $f:\Bbb{C}\rightarrow \Bbb{C}$ be a continuous function which is analytic on $\Bbb{C}\setminus \Bbb{R}$ . I need to use the Morera theorem to show that $f$ is analytic on $\Bbb{C}$ . In the lecture the Prof. gave us the hint that we should decompose rectangle into three rectangles. Up to now I have done the following: Let $z_0\in \Bbb{C}$ . Take $B_r(z_0)\subset \Bbb{C}$ be an open disk. By assumption $f$ is continuous on $B_r(z_0)$ . Let me define $R$ to be an arbitrary closed rectangle in $B_r(z_0)$ . Now to conclude I only need to show that $$\int_{\partial R} f(z) dz=0$$ Here I think I need the hint but I don't see how it works. Could maybe someone help me? Thanks for your help,"['complex-analysis', 'analysis', 'analytic-functions']"
4419420,How many multiples of 7 contain a 7 in their decimal representation?,"The first few multiples of $7$ that contain a $7$ in their decimal representation are $7, 70, 77, 147, 175, 217 ..$ ( OEIS A121027 ). My question is how many such numbers exist below $n$ : $$
f(n) = \left| \left\{ x \lt n : x \equiv 0 \bmod 7 \land \text{$x$ contains a '7'} \right\} \right|
$$ It seems that $f(7 \cdot 10^c) = 10^c - 9^c$ , e.g. there are exactly $10^3 - 9^3 = 271$ multiples of $7$ that contain a $7$ below $7000$ . A proof for this has been given on MathOverflow . Using the above fact, it is also easy to find $f(n)$ for any $n$ starting with a $'7'$ . $$\forall m<10^c, f(7 \cdot 10^c + m) = 10^c - 9^c + \lceil m/7 \rceil $$ I'd like to know how many such numbers exist below $n$ in general, in the form of a (possibly recursive) function.","['number-theory', 'recursive-algorithms', 'recursion', 'algorithms']"
4419480,Examples of non-self-induced algebras,"Let $A$ be a (possibly non-unital) algebra over $\mathbb C$ .  We say that $A$ is self-induced if the product map $m:A \otimes_A A \rightarrow A$ is an isomorphism.  Here $A \otimes_A A$ is the balanced tensor product, the quotient of $A\otimes A$ by the linear span of elements of the form $ab\otimes c - a\otimes bc$ . This notion seems to have been introduced by Gronbaek in Morita equivalence for self-induced Banach algebras and was further studied by Meyer in Smooth and rough modules over self-induced algebras .  If $A$ is unital, or more generally, has local, one-sided, units, then $A$ is self-induced.  I am interested in non-trivial examples of $A$ which are not self-induced. What is an example of $A$ which is not self-induced, but such that the product map is surjective, and such that the product is non-degenerate (so for each non-zero $a\in A$ there are $b,c\in A$ with $ba\not=0, ac\not=0$ ). I have tagged this functional analysis, as the notion seems to have arisen in the context of topological algebras (and so people working in this area might know examples).  But the question does not ask about the topological case.  I would be interested to know if this idea is studied in non-topological contexts under a different name?","['abstract-algebra', 'functional-analysis', 'algebras']"
4419497,Permuting subgroups with the same finite index,"Suppose that we have a finitely generated residually finite group $G = \langle g_1,\ldots,g_r \rangle$ and $H$ is a subgroup of $G$ with finite index $m$ . Let $\phi$ be an automorphism on $G$ . Question : What is the bound for the smallest $n \in \mathbb{N} \setminus \{0\}$ , such that $\phi ^n(H) = H$ ? My thought so far : We know that $G$ has at most $(m!)^r$ number of subgroups with index $m$ , so $n \leq (m!)^r$ . I was wondering if the bound can be improved, it seems unlikely for the automorphism to go through all the subgroups of this index. Is it possible to show that $n$ is bounded by a polynomial (or even linear function) in $m$ , i.e. $ n = \mathcal{O}_r(m^d)$ for some $d$ ?","['permutations', 'combinatorial-group-theory', 'abstract-algebra', 'geometric-group-theory', 'group-theory']"
4419513,How can I solve the differential equation $f'(x) + f\left( x^2\right) =0$?,"I was looking at this AOPS thread which dealt with the following question: Find all functions $f:(1, \infty) \to \mathbb{R}$ that satisfy $$f(x) -f(y) = (y-x)f(xy)  $$ I attempted to solve (at least part of) this question, and in one of my attempts I encountered a path that seemed promising, but I couldn't manage to finish the solution. Here's what I did: Rewriting the functional equation as $$
\frac{f(x) - f(y)}{x-y} = -f(xy)
$$ the LHS is the slope of a secant line. This makes me want to take the limit $y\to x$ on both sides to get some derivatives. So assuming $f$ is differentiable on the domain of interest, this gives $$
f'(x) = -f\left(x^2\right) \tag{1}
$$ as a differential equation whose solutions form a subset of differentiable solutions to our original functional equation. As shown in the AOPS thread, the family of solutions to the original problem is indeed $f(x) = \frac{c}{x}$ , with $c$ some constant, and this family of functions do satisfy that $\frac{\mathrm{d}}{\mathrm{d} x}\frac{c}{x} = -\frac{c}{x^2}$ as the differential equation suggests, so all seems good up to this point. Even though using the ansatz $f(x) = \frac{c}{x}$ to verify the differential equation $(1)$ does work, I don't know how to solve the differential equation directly. I thought about introducing a substitution of the form $u(x) = x^2$ , but then the LHS of $(1)$ would end up with things like $f'(\sqrt{u})$ which again doesn't seem very useful. Is it possible to solve the differential equation $(1)$ directly? And if the answer is yes, what is the procedure with which you can go about solving a differential equation like this? Thank you!","['contest-math', 'functional-equations', 'calculus', 'ordinary-differential-equations']"
4419565,Is this ambiguity in the use of prime notation to denote a derivative?,"Consider the function $f(x)$ and let $g(x)=f(cx)$ . By the definition of derivative $$f'(x)=\frac{df(x)}{dx}=\lim\limits_{h \to 0} \frac{f(x+h)-f(x)}{h}\tag{1}$$ and so the definition of $f'(cx)$ is $$f'(cx)=\frac{df(cx)}{d(cx)}=\lim\limits_{h \to 0} \frac{f(cx+h)-f(cx)}{h}\tag{2}$$ Also $$g'(x) = \lim\limits_{h \to 0} \frac{g(x+h)-g(x)}{h}$$ $$=\lim\limits_{h \to 0} \frac{f(c(x+h))-f(cx)}{h}$$ $$=\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{h}$$ $$=c\lim\limits_{h \to 0} \frac{f(cx+ch)-f(cx)}{ch}$$ $$=cf'(cx)$$ Hence $$g'(x)=cf'(cx)$$ I am not sure if I am seeing an inexistent ambiguity, but $f'(cx)$ seems like ambiguous notation. As defined above in $(2)$ , $f'(cx)$ means the derivative of $f$ relative to $cx$ , evaluated at a point we call $cx$ , . Note that this is different than the derivative of $f$ relative to $x$ evaluated at a point $cx$ : this derivative is $g'(x)=\frac{df(cx)}{dx}$ . In ""prime"" notation, how do we denote this latter derivative? It would seem to be $f'(cx)$ , but I think either this is incorrect, or the definition given in $(2)$ is somehow non-standard. For example, let $$f(x)=3x^3$$ $$g(x)=f(cx)=3c^3x^3$$ Then $$f'(x)=\frac{df(x)}{dx}=9x^2$$ $$\left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2\ \ (=f'(cx)???)$$ $$g'(x)=\frac{df(cx)}{dx}=c\frac{df(cx)}{d(cx)}=c\cdot f'(cx)=c\cdot 9c^2x^2= 9c^3x^2\tag{3}$$ In this example, $f'(cx)=\frac{df(cx)}{d(cx)}=9c^2x^2$ according to definition I gave in $(2)$ . But perhaps more intuitively, it could also be $f'(cx)= \left.\frac{df(x)}{dx}\right \vert_{x=cx}=9c^2x^2$ Note that both uses of $f'(cx)$ lead to the same result in this example. Which use of $f'(cx)$ is the ""correct"" or ""standard"" one?","['notation', 'calculus', 'derivatives']"
4419566,Sequence $x\mapsto x^a$ cyclic in 3 directions: $s_0^{b^ic^jd^k}\bmod N$. How to find member $ijk$ if projection to 1D not possible due mixed factors,"Summery: Can we determine $i,j,k$ for $s_{ijk}$ $$s_{ijk} \equiv s_0^{\beta^i\gamma^j\delta^k}\mod N$$ $$N = P\cdot Q \cdot R$$ $$P = 2\cdot p \cdot p_{big} +1 $$ $$Q = 2\cdot q \cdot q_{big} +1 $$ $$R = 2\cdot r \cdot r_{big} +1 $$ $$p = 2\cdot p_1\cdot p_2\cdot p_3 +1$$ $$q = 2\cdot q_1\cdot q_2\cdot q_3 +1$$ $$r = 2\cdot r_1\cdot r_2\cdot r_3 +1$$ $$\alpha = 16 \cdot p_{big} \cdot q_{big} \cdot r_{big}$$ $$\beta \equiv \alpha^{2 p_2 p_3 q_2 q_3 r_2 r_3} \mod \phi(N)$$ $$\gamma \equiv \alpha^{2 p_1 p_3 q_1 q_3 r_1 r_3} \mod \phi(N)$$ $$\delta \equiv \alpha^{2 p_1 p_2 q_1 q_2 r_1 r_2} \mod \phi(N)$$ $$|\{s_0 \mapsto s_0^\beta\}| = B =p_1q_1r_1$$ $$|\{s_0 \mapsto s_0^\gamma\}| = C =p_2q_2r_2$$ $$|\{s_0 \mapsto s_0^\delta\}| = D =p_3q_3r_3$$ $$|\{s_0 \mapsto s_0^\alpha\}| = |\{s_0 \mapsto s_0^{\beta\delta\gamma}\}| = L_{all} = A = B\cdot C \cdot D$$ $$\text{with a suitable } s_0 = n^\alpha \text{ , } n \in [2,N-1] \text{ (not all $n$ work)}$$ faster than $O((\sqrt[3]{A})^2)$ or even faster than $O(\sqrt{A})$ steps of computation with knowing only the factors $p,q,r$ (and $2$ ) of $\phi(N)$ $$\phi(N) = 2 \cdot 2 \cdot 2 \cdot p \cdot q \cdot r \cdot p_{big} \cdot q_{big} \cdot r_{big}$$ (and also knowing the prime factors of $p,q,r$ , the start value $s_0$ , the mod value $N$ and exponents $\beta,\gamma,\delta$ with their inverses, the structure size $B,C,D$ and with this also $A$ ) In more detail: Given a sequence like $x\mapsto x^\alpha$ but cyclic in 3 directions. A sequence member $s_{ijk}$ can be defined with $s_{ijk} \equiv s_0^{\beta^i\gamma^j\delta^k}\bmod N$ based at a starting value $s_0$ and three exponents $\beta,\gamma,\delta$ with their related exponents $i,j,k$ (one for each direction). Together they can describe a set of unique $B \times C \times D$ sequence members for well chosen parameters $\alpha,\beta,\gamma,\delta,N,s_0$ . ( $B$ would be the sequence length of $x \mapsto x^\beta \bmod N$ and $C$ of $x \mapsto x^\gamma \bmod N$ and $D$ of $x \mapsto x^\delta \bmod N$ and $x \mapsto x^\alpha \mod N$ would have length $B\cdot C\cdot D$ ) Given now a member $s_{ijk}$ I'm looking for the (computationally) fastest way to determine the indices $i,j,k$ . The difficulty seems to be related to chosen parameter. For some parameter a member $s_{ijk}$ can be projected to $3$ sequence with length $B,C$ and $D$ which makes solving the discrete logarithm with baby-step-giant-step-algorithm much easier. For some parameter it can not (Question: Or can it?) We pick a number $N = P\cdot Q\cdot R$ with $P,Q,R$ primes $$P = 2\cdot p+1$$ $$Q = 2\cdot q+1$$ $$R = 2\cdot r+1$$ with $p,q,r$ different primes and $\phi(\cdot)$ containing 4 primes each: $$p = 2\cdot p_1\cdot p_2\cdot p_3 +1$$ $$q = 2\cdot q_1\cdot q_2\cdot q_3 +1$$ $$r = 2\cdot r_1\cdot r_2\cdot r_3 +1$$ All odd prime factors are unique. (*) To get a sequence of length $L_{all}=(p_1p_2p_3)\cdot (q_1q_2q_3)\cdot (r_1r_2r_3)$ we set $\alpha=16$ . The sequence $x \mapsto x^\alpha \bmod N$ will have that many unique members for a starting value $x_0 \equiv s_0 \equiv n^\alpha \bmod N, n\in [2,N-1]$ (with some exceptions we won't focus here). Depending on starting value it can be part of one out of 8 disjoint sequences with that length (or some special case). To get a structure $B\times C \times D$ which is cyclic in 3 directions we can set the exponents $\beta, \gamma, \delta$ to different powers of $\alpha$ . For those powers we can use the prime factors of $L_{all}$ . If we set them to $$\beta_{easy} \equiv \alpha^{q_1 q_2 q_3 r_1 r_2 r_3} \mod \phi(N)$$ $$\gamma_{easy} \equiv \alpha^{p_1 p_2 p_3 r_1 r_2 r_3} \mod \phi(N)$$ $$\delta_{easy} \equiv \alpha^{p_1 p_2 p_3 q_1 q_2 q_3} \mod \phi(N)$$ we would get a structure $(B_{easy}=p_1p_2p_3) \times (C_{easy}=q_1q_2q_3) \times (D_{easy}=q_1q_2q_3)$ . All members $m_{easy}$ of this structure can be projected to a member of 3 much shorter sequences with $$|\{\forall m_{easy}: m_{easy}^p \bmod N\}|=B$$ $$|\{\forall m_{easy}: m_{easy}^q \bmod N\}|=C$$ $$|\{\forall m_{easy}: m_{easy}^r \bmod N\}|=D$$ This would make finding $ijk$ much easier. But if we mix those prime factors e.g.: $$\beta \equiv \alpha^{p_2 p_3 q_2 q_3 r_2 r_3} \mod \phi(N)$$ $$\gamma \equiv \alpha^{p_1 p_3 q_1 q_3 r_1 r_3} \mod \phi(N)$$ $$\delta \equiv \alpha^{p_1 p_2 q_1 q_2 r_1 r_2} \mod \phi(N)$$ We would get a structure $(B=p_1q_1r_1) \times (C=p_2q_2r_2) \times (D=p_3q_3r_3)$ . Given a member $m$ of that structure with $$m = s_{ijk} \equiv s_0^{\beta^i\gamma^j\delta^k}\mod N$$ there is no exponent $\xi$ which is able to project such member to a one-directional sequence of size $B,C$ or $D$ . $$\not\exists \text{ }  \xi \text{ with: } \text{ } |\{\forall m: m^\xi \bmod N\}| \in \{B,C,D\}$$ Or at least I found none in some tests (no proof here). Question : Given such a member $m = s_{ijk}$ and a starting value $s_0$ , the exponents $\beta, \gamma, \delta$ , the mod value $N$ , the structure size $B,C,D$ with their prime factors. Which would be the (computationally) fastest way to determine $i,j,k$ in $$m = s_{ijk} \equiv s_0^{\beta^i\gamma^j\delta^k}\mod N$$ or in other words is there some simplification for that problem which makes it (much more) faster solvable than the (advanced) naive way: surface around $s_0$ with $s_0^{\beta^i\gamma^j}\bmod N$ line starting at $m$ with $m^{\delta^k} \bmod N$ until match in between them This would take $O(B\cdot C + D)$ steps of calculation (we assume constant time for power/multiplication and other base operations in between values $<N$ ). Can it done faster/better? Some more details: Besides the exponents $\beta, \gamma, \delta$ their inverses $\beta^{-1}, \gamma^{-1}, \delta^{-1}$ are also known. So $(m^\beta)^{\beta^{-1}} = m \mod N$ . But we can assume the prime factorization of $N$ and with this $\phi(N)$ is not known. For simplification left out above but the true prime factors of $N$ will have an additional big factor like $P = 2 \cdot p \cdot p_{big} +1$ and with this $\alpha$ will be $\alpha = 16 \cdot p_{big} \cdot q_{big} \cdot r_{big}$ . (while factors $p,q,r$ will be relatively small - that't the indented purpose, big factors $2\cdot 3\cdot g+1$ with $g$ some big safe prime - Is there a better way?) Dimensions $B,C,D$ can be assumed to be (almost) the same size. Related to (*): The condition of all odd prime factors from $p,q,r$ being unique is not complete. Some test cases did not work out. Hints about this would be nice too. We can not reduce it to single series of length $B,C,D$ but we can project it to a smaller sub-structure, e.g.: $$\{\forall m: m^p \bmod N\} \equiv (q_1r_1) \times (q_2r_2) \times (q_3r_3)$$ $$\{\forall m: m^{p\cdot q} \bmod N\} \equiv (r_1) \times (r_2) \times (r_3)$$ But can this be reverted to help finding $i,j,k$ ? Alternative way of solving: Besides projecting the member to 3 shorter sequences we could also project it to a single sequence with length $L_{all}$ . We can use the exponent $\lambda = \beta\cdot \gamma\cdot \delta$ and find a solution for $m \equiv s_0^{\lambda^y} \bmod N$ . For $i,j,k$ we use $i = y \bmod B$ and $j = y \bmod C$ , $k = y \bmod D$ . If we know $\phi(N)$ this should take $O(\sqrt{L_{all}})$ steps of computation with baby-step-giant-step-algorithm. Or could we also use something similar to the Pohlig–Hellman algorithm here? However if we use some additional factors $p_{big}, q_{big}, r_{big}$ as described above we only know some factors of $\phi(N)$ . Without knowing it $\lambda^y$ with $y\approx \sqrt{L_{all}}$ can get very big. Would knowing $\alpha$ help here? Is there any alternative faster way than $O(\sqrt{L_{all}})$ ? Example: without extra big factors: $N=10009867024679 = 7823\cdot 25919 \cdot 49367 = (2\cdot p+1)\cdot (2\cdot q+1)\cdot (2\cdot r+1)$ $p = 3911 = 2\cdot 5\cdot 17 \cdot 23 +1$ $q = 12959 = 2\cdot  11\cdot 19\cdot 31 +1$ $r = 24683 = 2\cdot  7\cdot 41\cdot 43 +1$ $\beta = 4240894939664$ , $\gamma = 1339883950832$ , $\delta=8727881742696, \alpha=16$ with an extra factors (but only some small, but still bigger in $\phi(\phi(N))$ : $N=20006771790898198589661718867638862554111657299 $ $N =2033131686658427\cdot 2843113624018043 \cdot 3461125068714059$ $N=(2\cdot p \cdot p_{big}+1)\cdot (2\cdot q\cdot q_{big}+1)\cdot (2\cdot r\cdot r_{big}+1)$ $p = 46762309367 = 2\cdot 2579\cdot 2999\cdot 3023 +1$ $q = 52104123887 =  2\cdot 2819\cdot 2963\cdot 3119+1$ $r = 53539663223 =  2\cdot 2879\cdot 2903\cdot 3203+1$ $p_{big} = 21739 = 2\cdot 3\cdot 3623 +1$ $q_{big} = 27283 =  2\cdot 3\cdot 4547+1$ $r_{big} = 32323 =  2\cdot 3\cdot 5387+1$ $\alpha = 1411000697115152 = 16 \cdot p_{big} \cdot q_{big} \cdot r_{big}$ $\beta=647805671078700581866355815418189019913379384$ $\beta = \alpha^{2\cdot 2999\cdot 3023\cdot 2963 \cdot 3119 \cdot 2903 \cdot 3203} \bmod \phi(N)$ $\gamma=15486888170768439649918711963029782135114744176$ $\delta=10588302512744441040460723396975909732625316616$ Prime factors from $\phi(\phi(N))$ are either $2,3$ or a prime $2\cdot s+1$ with $s$ a prime.","['modular-arithmetic', 'number-theory', 'cryptography', 'discrete-mathematics', 'sequences-and-series']"
4419584,"For any $A\in M_n(\mathbb{R})$, there is a $B$ s.t. $\operatorname{rank}(A)+\operatorname{rank}(B)=n$ and $AB=0$.","This is a qualifying exam question which I have little experience approaching. This is certainly an exercise in linear algebra so I would like to obtain a method in solving similar questions. Let $M_n(\mathbb{R})$ be the ring of $n\times n$ matrices over $\mathbb{R}$ . If $A\in M_n(\mathbb{R})$ , then there exists a $B\in M_n(\mathbb{R})$ s.t. $\operatorname{rank}(A)+\operatorname{rank}(B)=n$ and $AB=0$ . Also does there exist a $B$ with the same properties such that $BA=0$ as well? As far as I can tell, there is a similar question , but the question is over $\mathbb{C}$ coefficients so this is not a repeat question. My approach is as follows. Since we work over $\mathbb{R}$ , we should focus on the Rational Canonical Form. So write $A$ as a direct sum of companion matrices. The rank is preserved under similarity classes. The rank of $A$ is the sum of the ranks of the companion matrices. So the problem reduces to proving the result for a companion matrix. Let $A=\begin{pmatrix}
0 & \dots & \dots & -a_1\\
1 & 0 & \dots &-a_2\\
\ddots &\ddots & \ddots & \ddots \\
0 & \dots & 1 &-a_{n-1}
\end{pmatrix}$ be a companion matrix. This has either rank $n-1$ or $n-2$ . Then there should be choices of $B$ which work. Of course, this would be a tedious computation. My question. Is there a better approach to this question? For example, some way to relate $M_n(\mathbb{R})$ to $M_n(\mathbb{C})$ so that I can use Jordan canonical forms instead?","['matrices', 'matrix-rank', 'linear-algebra']"
4419594,Show dominated convergence to exchange the order of differentiation and integration,"Problem Let $\text{Ai}: \mathbb{R} \to \mathbb{R}$ with $$\text{Ai}(x) = \frac{1}{\pi} \Re \int_{0}^{\infty} \omega e^{ -\frac{t^3}{3} + i x \omega t}  dt$$ be the Airy function with $\omega := e^{\frac{i\pi}{6}}$ . Prove that $\text{Ai}''(x) = x\text{Ai}(x)$ . My approach We have $\omega = e^{\frac{i\pi}{6}} = \cos\frac{\pi}{6} + i\sin\frac{\pi}{6} = \frac{\sqrt{3}}{2} + \frac{1}{2} i$ and $-\frac{t^3}{3} + i x \omega t = -\frac{t^3}{3} - \frac{xt}{2} + \frac{xt\sqrt{3}}{2}i$ . We also have $\omega^3 = e^{\frac{i\pi}{2}} = i$ . Let $f: \mathbb{R}_0^+\times\mathbb{R}\to\mathbb{C}$ with: \begin{align}
&f(t,x) = \omega e^{ -\frac{t^3}{3} + i x \omega t}\\ &\frac{\partial}{\partial x}f(t,x) = i \omega^2 t e^{ -\frac{t^3}{3} + i x \omega t} &&\left| \frac{\partial}{\partial x}f(t,x) \right| = t e^{-\frac{t^3}{3} - \frac{xt}{2}}\\
&\frac{\partial^2}{\partial^2 x} f(t,x) = - i t^2 e^{ -\frac{t^3}{3} + i x \omega t}
&&\left| \frac{\partial^2}{\partial^2 x}f(t,x) \right| = t^2 e^{-\frac{t^3}{3} - \frac{xt}{2}}
\end{align} I need to show the following equation in order to progress: $$ \frac{\partial^2}{\partial^2 x} \int_0^\infty f(t,x) dt = \frac{\partial}{\partial x} \int_0^\infty \frac{\partial}{\partial x} f(t,x) dt = \int_0^\infty \frac{\partial^2}{\partial^2 x} f(t,x) dt $$ For this I need to show dominated convergence for $\frac{\partial}{\partial x}f(t,x)$ and $\frac{\partial^2}{\partial^2 x}f(t,x)$ , a.i. I need to find a function $g_1:\mathbb{R}_0^+ \to \mathbb{R}$ with $\left| \frac{\partial}{\partial x}f(t,x) \right| \leq g_1(t)$ for all $t \in \mathbb{R}_0^+$ and $x \in \mathbb{R}$ . I also need to find a function $g_2:\mathbb{R}_0^+ \to \mathbb{R}$ with $\left| \frac{\partial^2}{\partial^2 x}f(t,x) \right| \leq g_2(t)$ for all $t \in \mathbb{R}_0^+$ and $x \in \mathbb{R}$ . However, $x\in\mathbb{R}$ can be arbitrarely small making $\frac{\partial}{\partial x}f(t,x)$ and $\frac{\partial^2}{\partial^2 x}f(t,x)$ arbitrarely large at some point. I am unable to find functions $g_1(t)$ , $g_2(t)$ to show dominated convergence. What am I missing? Note This problem was already discussed here , but the one particular point which I am interested in was handwaved.","['integration', 'complex-analysis', 'complex-integration', 'airy-functions']"
4419596,"$\forall x\in \mathbb{R}, |f'(x)| \leqslant C |f(x)|$ implies $f=0$","Question Let $f\in \mathcal{C}^1$ a function such that $f(0)=0$ and $\forall x\in \mathbb{R}, |f'(x)| \leqslant C |f(x)|$ .
Prove that $f=0$ My attempt If there exists $a$ such that $f(a)\neq 0$ , then in a neighborhood of $a$ we can write $-c \le \frac{f'}{f} \le c$ and we can integrate between $a$ and $x$ in this neighborhood : $-c(x-a) \le \ln(f(x)) - \ln(f(a)) \le c(x-a)$ so $\exp(-c(x-a)) \le \frac{f(x)}{f(a)} \le \exp(c(x-a)) (*)$ . Without loosing generality we can suppose $a>0$ . Let $b = \max \{ a > x \ge 0, f(x)=0\}$ (exists since $f(0)=0$ ). Then when $x\rightarrow b$ we get in $(*)$ : $\exp(-c(b-a)) \le 0$ . This is impossible. So we get the result. First I'm not sure of my proof. Then I think there might be other proofs, and surely easier ones. Could someone help ?","['integration', 'roots', 'analysis', 'real-analysis', 'derivatives']"
4419600,"Find $x_1,x_2$ such that $\min_{x_1,x_2} x_1^2+2x_1x_2$ where $x_1,x_2$ are subject to constraint $x_1^2x_2 \ge 10$.","I am attempting to solve a constrained optimization problem using Lagrange multipliers but am getting lost on how to resolve the equations the gradients output. The problem is the following: Find $x_1,x_2$ such that $\min_{x_1,x_2} x_1^2+2x_1x_2$ where $x_1,x_2$ are subject to constraint $x_1^2x_2 \ge 10$ . I have changed the constraint into the equality $x_1^2x_2-10-s^2=0$ and attained the gradients which result in 4 equations and 4 unknowns: \begin{align}
x^2_1x_2-10-s^2 &= 0 \\
2x_1+2x_2 &= \lambda (2x_1x_2) \\
2x_1 &= \lambda x_1^2 \\
0 &= \lambda(-2s)
\end{align} But I am unsure of how to proceed from here. Additionally, I am struggling to find the dual problem.","['optimization', 'multivariable-calculus', 'lagrange-multiplier']"
4419618,Integral involving beta function (continuous version of binomial distribution),"I'm hoping someone can help me out with a problem that's consumed me for a while. I'm working with a continuous version of the binomial distribution defined for real $n > 0$ and continuous $0 < \mu < 1$ : $$
P(x) = n \mu^{nx} (1 - \mu)^{n - nx} \frac{\Gamma(n + 1)}{\Gamma(nx + 1) \Gamma(n - nx + 1)}
$$ (The $n$ adjusts for rescaling: the binomial distribution takes in $k$ , but we take in $nx$ with $x \in [0, 1]$ .) My goal is to find a way of computing $\int_0^1 P(x)\,dx$ or $$
\int_0^1 n \mu^{nx} (1 - \mu)^{n - nx} \frac{\Gamma(n + 1)}{\Gamma(nx + 1) \Gamma(n - nx + 1)} \, dx
$$ What I've tried: It's possible to convert this to the beta function and rewrite in terms of a trigonometric integral found here to get $$\int_{x=0}^1 \int_{\theta=0}^{\pi/2} \mu^{nx} (1 - \mu)^{n - nx} \frac{2^{n+1}}{\pi} \cos^n \theta \cos ((2nx - n)\theta)\,d\theta\,dx$$ Then, Mathematica eliminates $x$ and produces $$
\int_{0}^{\pi/2} \frac{2^n \cos^n \theta \left[\theta (\mu^n + (1 - \mu)^n) \sin (n\theta) + ((1 - \mu)^n - \mu^n) \tanh^{-1}(1 - 2\mu) \cos n \theta \right]}{\pi \left(\theta^2 + \left[ \tanh^{-1} (1 - 2u) \right]^2\right)} \, d\theta
$$ which is a pretty interesting integral that's vaguely Bessel-esque (if you graph this for $(n, \mu) = (100, 0.5)$ it's surprising that the answer is almost exactly 1), but not something I've managed to make any progress on. It seems natural to try and transform the integral so $x, \mu$ are over a different interval than $[0, 1]$ , but nothing I've tried there has worked. This integral isn't exactly like the incomplete beta function (this is almost exactly the PDF of $\text{Beta}(\alpha = nx + 1, \beta = n(1 - x) + 1)$ applied to $\mu$ , instead of switching $\mu$ and $x$ which is what the incomplete beta function can help with.) Any ideas? I'm beat.","['integration', 'statistics', 'definite-integrals', 'gamma-function', 'beta-function']"
4419628,Show the function between the dihedral groups is well defined,"Suppose that $n = dm$ where $d$ and $m$ are positive integers with $m\ge 3$ . Consider the dihedral group $D_n = \langle \{\mu, \rho\}\rangle,$ where $|\mu| = 2$ , $|\rho| = n$ and $\rho\mu = \mu\rho^{−1}$ , and the dihedral group $D_m = \langle \{s, r\}\rangle,$ where $|s| = 2$ , $|r| = m$ and $rs = sr^{−1}$ . Define $\psi : D_n \to D_m$ by $ψ(\mu^a\rho^b)=s^ar^b$ , for any integers $a,b$ . Show that $\psi$ is well-defined. Here's the stuff I noticed: different values of $a,b$ can give the same group element $\mu^a\rho^b$ , and I need to show that they also give the same $\psi(\mu^a\rho^b)$ . if $n$ is not a multiple of $m$ , then $\psi$ not well-defined. So here is what I did so far, (tried to make a proof sketch): from integer division, there exists unique integers $i, j, s, t$ with $0 \le i < 2$ and $0 \le j < n$ and $a = i + 2s$ and $b = j + nt$ . So, the group element $\mu^a\rho^b=\mu^{i+2s}\rho^{j+nt}$ uniquely determined by $i$ and $j$ , since changing $s$ and $t$ won't make a difference. So, I think I need to show that $\psi(\mu^a\rho^b)$ depends only on $i$ and $j$ , and not on $s$ or $t$ . (this is what I'm having a hard time doing.)","['permutations', 'group-isomorphism', 'dihedral-groups', 'abstract-algebra', 'group-theory']"
4419661,Find the number of rearrangements of AABBBCCDDD where there are no two consecutive As or Bs,"I tried using inclusion exclusion where set $A$ is all the rearrangements where two $A$ s are consecutive and set $B$ where two $B$ s are consecutive. However, I got $8400$ which is incorrect. I think it has to do with there being $3$ $B$ s so I am somehow counting them wrong. This was my process: Total rearrangements of $AABBBCCDDD:$ ${10\choose 2,3,2,3} = 25200$ $\left|A\right| = {9\choose 1,3,2,3} = 5040$ $\left|B\right| = {9\choose 2,1,1,2,3} = 15120$ $\left|AB\right| = {8\choose 1,1,1,2,3} = 3360$ $A\cup B = 25200 -(5040+15120)+3360 = 8400$ Any help or guidance would be extremely appreciated.","['inclusion-exclusion', 'combinatorics']"
4419665,$f_n(x) = n^{1/p}\chi(nx)$ in $L^p(\mathbb{R})$ weakly converges to 0 for $1 \leq p <\infty$,"I am trying to show that $f_n(x) = n^{1/p}\chi(nx)$ in $L^p(\mathbb{R})$ weakly converges to 0 when $1<p<\infty$ . Here I have let $\chi$ denote the characteristic function on $[0,1]$ . So far I have noticed that for $g\in L^q(\mathbb{R})$ , for $q$ the conjugate pair of $p$ , $$\int_\mathbb{R} f_n(x) g(x) d\mu(x) = \int_{[0,1/n]}n^{1/p}g(x) d\mu(x) = n^{1/p}\int_{[0,1/n]}g(x) d\mu(x)$$ From here, it seems like we should use some kind of change of variables like $x = u/n$ . However, this does not seem to yield the final result. Also, why does the weak convergence fail when $p =1$ ? Thank you in advance!","['measure-theory', 'lp-spaces']"
4419703,Probability of a series of numbers,"I’m trying to find the probability of getting nine numbers in a row. If $1-9$ are the numbers in question, what is the probability of guessing each of the nine correct. There can be duplicates and the order matters. Such as what is the probability of getting $145347671$ ?","['probability-theory', 'probability']"
4419711,Show that $\operatorname{Log}\left ( 1+\frac{1}{z} \right )=\operatorname{Log}(z+1)-\operatorname{Log}(z)$,"It is well-known that whenever $a,b$ are non-zero complex numbers, then $\operatorname{Log}(a/b)=\operatorname{Log}(a)-\operatorname{Log}(b)+2\pi i k$ for some integer $k$ . Here $\operatorname{Log}$ denotes the principal logarithm. I read somewhere without explanation that $$
\operatorname{Log}\left ( 1+\frac{1}{z} \right )=\operatorname{Log}(z+1)-\operatorname{Log}(z)\tag{*}
$$ for all $z\in \mathbb{C}\setminus (-\infty,0]$ . Is this valid in general? If so, why? I checked it myself to insert some values of $z$ in $\mathbb{C}\setminus (-\infty,0]$ and compare on both sides, and they look good. However, I can't seem to prove it in general, or that $k$ should be $0$ in that case (where $a=z+1$ and $b=z$ ). Update : I have tried a slightly different way, though I might err:
Write $x=a+ib$ , then $$
\frac{z+1}{z}=1+\frac{a}{a^2+b^2}+i\frac{-b}{a^2+b^2}.\tag{**}
$$ If $a>0$ , (*) is valid, as equality holds whenever $\Re z+1,\Re z>0$ . If $a=0$ , then (**) is reduced to $1-i/b$ , so $\operatorname{Log}(\frac{z+1}{z})=\ln|1-i/b|+\arctan(-1/b)$ , while $\operatorname{Log}(z+1)-\operatorname{Log}(z)=\ln|1-i/b|+\arctan(b)-\operatorname{Arg}(ib)$ . $\qquad$ If $b>0$ , then the latter would be equal to $\ln|1-i/b|+\arctan(b)-\pi/2$ . $\qquad$ For $b<0$ , we would instead get $\ln|1-i/b|+\arctan(b)+\pi/2$ . In both cases, I suspect that $\arctan(-1/b)$ is equal to $\arctan(b)-\pi/2$ for $b>0$ and equal to $\arctan(b)+\pi/2$ for $b<0$ . I checked it with Graph calculator, and left hand side and right hand side seem to coincide. How to prove it rigorously? See update 2 below. If $a<0$ , replace $a$ by $-a'$ for $a'>0$ , then (*) is still valid for $a'>0$ . Update 2 : Define $f(x)=\arctan(-1/x)-\arctan(x)$ for $x\neq 0$ . Then, we see that $f'(x)=0$ , so $f$ must surely be constant. But, since $f(1)=-\pi/2$ , it follows that $f(x)=-\pi/2$ for all $x\neq 0$ . But, this is not entirely true, as $f(x)$ is $\pi/2$ for negative $x$ . Where is the flaw in my argument?","['complex-analysis', 'logarithms']"
4419774,Cardinality of $(A\times B)\cup(B\times A)$,"Let $A$ and $B$ be finite sets and let $a=|A|$ , $b=|B|$ and $c=|A\cap B|$ . Write an expression in terms of $a$ , $b$ and $c$ that is equal to $|(A\times B)\cup(B\times A)|$ for every choice of $A$ and $B$ . If possible I would like to have some guidelines.","['elementary-set-theory', 'discrete-mathematics']"
4419784,Integrating factor $e^{\int \frac{1}{x} dx}$ in differential equation,"When integrating $\int \frac{1}{x} dx$ , we typically write the integrated expression as $ln|x| + C$ . The absolute value of the $x$ -variable is introduced to account for the scenario where we have $ln(-x)$ and $x$ is a negative number. In a calculus book I am currently working with, the following differential equation is used in an example: $$xy'+y=3x^{2} +4x, x\neq 0$$ In this case the integrating factor becomes $e^{\int \frac{1}{x} dx}$ . The textbook example then states that $e^{\int \frac{1}{x} dx}=e^{ln x} = x$ . Question : Why is it that we do not have to take the absolute value of the $x$ -variable here? I could understand this if it was explicitly stated that $x>0$ in the given problem, but this is not stated. All we know is that $x\neq0$ . So how does this account for the second scenario outlined above? If anyone can explain this to me, then I would greatly appreciate it!","['integration', 'ordinary-differential-equations', 'logarithms']"
4419803,Solve $y’’ – 4y’ + 5y = 4e^{2x}\sin(x)$ using $\mathcal D$ operator,"Hello – I am working through the following question and get stuck at step 6. Could someone please advise in simple terms which I can hopefully understand. Thanks $$y'' – 4y' + 5y = 4e^{2x}\sin(x)$$ Step one – Order equation so that differential operator is in front of the RHS of the equation $\newcommand{\D}{\mathcal D}$ $$1 = \frac 1 {\D^2 – 4\D + 5}  \cdot 4e^{2x}\sin(x)$$ Step two – move constant and exponential in front of the $\D$ operator $$1 = 4e^{2x}\cdot \frac 1 {\D^2 – 4\D + 5}\cdot \sin(x)$$ Step three – calculate $a$ Because of $e^{2x}$ , $a = 2$ , and because of $\sin(x)$ , $a = 2 + i$ . Step four – insert $a$ into the $\D$ operator and then calculate to see if it equals zero \begin{align}
1 &= 4e^{2x}\cdot \frac 1 {(2+i)^2 – 4(2+i) + 5}\cdot \sin(x)
\\
&= 4e^{2x}\cdot \frac 1 {(4+4i+4-8-4i+5)}\cdot\sin(x)
\\
&= 4e^{2x}\cdot \frac 1 {(0)}\cdot \sin(x)
\end{align} Step 5 – because there is a zero, note $a = 2$ therefore make it $\D+2$ \begin{align}
1 &= 4e^{2x}\cdot \frac 1 {(\D + 2)^2 – 4(\D + 2) + 5}\cdot \sin(x)
\\
&= 4e^{2x}\cdot\frac 1 {\D^2 + 1}\cdot\sin(x)
\end{align} What do I do for step 6? Please explain in simply terms and assume my calculus knowledge is low.","['calculus', 'ordinary-differential-equations']"
4419819,Functions $f(x)=\log_x(a)$ and $f(x)=\sqrt[x]{a}$,"I was refreshing my memory on the power, exponential, nth-root, and logarithmic functions when I realised that there is a whole pair of functions missing. If I know the exponent, I can make $y=x^a$ or its inverse $y=\sqrt[a]{x}$ .
If I know the base I can make $y=a^x$ and its inverse $y=\log_a(x)$ .
But what if I know the power and I want to define a relationship between the exponent and the base? It wasn't too hard to figure out that they should be $y=\log_x(a)$ and $y=\sqrt[x]{a}$ . But I don't think I ever ran across these back when I was in school, and it sparked a lot of questions. Do these functions have names? What (if anything) are they used for? Is there a reason why we don't teach them alongside those other related functions?","['radicals', 'functions', 'logarithms']"
4419825,Lie group of biholomorphisms of complex manifolds,"I am running into conceptual problems trying to understand complex Lie groups, and more specifically, the Lie group of automorphisms $\text{Aut}(X)$ of a complex manifold $X$ . Already in this question I was pointed out that certain topological restrictions are needed in order for $\text{Aut}(X)$ to be a complex Lie group. In this sense, Kobayashi's Transformation Groups in Differential Geometry shows that for a compact complex manifold, $\text{Aut}(X)$ is a complex Lie group, whose Lie algebra corresponds to the holomorphic vector fields on $X$ . The following is a list of concepts that are not very detailed in the book, because it is assumed that the reader is already familiar with them. Concepts I want to learn about How to calculate the complex Lie algebra? : In the real $C^\infty$ case, we can calculate the Lie algebra of a Lie group ""by taking derivatives"". For example, if we can represent the relevant Lie group as matrices, taking derivatives near the identity matrix with the relevant restrictions, e.g. $\det(A(t))=1$ for $\text{SL}(n,\mathbb{R})$ ,  gives you elements of the Lie group.
Here, real 1-parameter subgroups are needed. How does one calculate similar Lie
algebra elements for complex Lie groups? Do I need $\mathbb{C}^*$ 1-parameter subgroups à la Geometric Invariant Theory? Or should I parametrize w.r.t. a real parameter $t$ ? Do complex Lie algebra elements correspond to left-invariant holomorphic vector fields as in the real Lie group theory? Left-invariance should still hold ""by forgetting the holomorphic structure"", I suppose. I assume, with no actual grounds for stating this, that the real (by forgetting holomorphic structure on $G$ ) and complex Lie algebra are related via complexification. This is true at the level of tangent space at the identity: does it hold for the Lie bracket as well? Compact real Lie groups are very important and handy: invariant measures (Haar), metrics, and Cartan 1-forms can be constructed. However, the only compact complex Lie groups are torii. Are there any generalizations of the above tools to non-Abelian complex Lie groups? I suppose some may arise via complexification of compact real groups . Specific computations I want to be able to perform As an illustration of Point 1 above, say we have a $\text{PGL}(2,\mathbb{C})=Aut(\mathbb{CP}^1)$ , a complex Lie group of automorphisms of complex dimension 3. We know then that the action of this complex Lie group on the projective line gives a representation of Lie algebras $$
\text{Lie}(PGL(2,\mathbb{C}))\rightarrow H^0(\mathbb{CP}^1, T^{1,0}\mathbb{CP}^1).
$$ Consider the subgroup of diagonal automorphisms $$
\begin{pmatrix}
1 & 0\\ 0 &\lambda
\end{pmatrix}
$$ for $\lambda\in\mathbb{C}^*$ . I want to be able to conclude in some rigorous way that the vector field $z \frac{\partial}{\partial z}$ is the Lie algebra element generated by the above elements, where $z=Z^0/Z^1$ is the homogeneous coordinate in the complex projective line. Should I take derivative w.r.t. $t$ of $\lambda(t) = 1 + t$ ? Am I able to change direction of the 1-parameter subgroup $\lambda_w(t) = 1 + w t$ for some $w\in \mathbb{C}^*$ ? Any feedback on misinterpretations, conceptual errors and references to complement Kobayashi's book are more than welcome!","['lie-algebras', 'kahler-manifolds', 'complex-geometry', 'lie-groups', 'differential-geometry']"
4419849,Examples of continua that are contractible but are not locally connected at any point,"A continuum is a compact, connected, metrizable space. What are examples of continua that are contractible but nowhere locally connected, meaning that no point has a neighbourhood basis consisting of connected sets? The following is an example of a contractible nowhere locally connected metrizable space, but I'm not aware of any compact examples (every segment on the ""main line"" has segments sticking out on a dense set).","['continuum-theory', 'general-topology', 'metric-spaces', 'examples-counterexamples']"
4419866,Number of pairwise overlapping balls in n dimensions,"I am looking for a closed form formula for the maximum number of balls in $n$ dimensions with the following conditions: All balls must have a non-empty pairwise intersection There must be no point inside more than 2 balls (i.e., no 3 way intersection) In 1D, we do not really have balls, but we could think of the intersection as the region between G and F, so we have at most 2 balls In 2D, we have the following, I do not see a way to have more than 4 balls In 3D, it is possible to fit 5 balls, like this So, the question is 1. whether these numbers are correct for 1-3 dimensions and whether there is a general formula to compute the maximum number of balls.","['euclidean-geometry', 'geometry']"
4419869,"Do the groups $\operatorname{SL}$, $\operatorname{PGL}$, and $\operatorname{PSL}$ over a field $K$ always have the same automorphism group?","Let $K$ be a field, then $\operatorname{GL}(n,K)$ consists of the $n\times n$ invertible matrices, $\operatorname{SL}(n,K)$ consists of the $n\times n-$ matrices with determinant $1$ , and $\operatorname{PGL}(n,K) = \operatorname{GL}(n,K)/\{rI_n: r\in K^\times\}$ , $\operatorname{PSL}(n,K) = \operatorname{SL}(n,K)/\{rI_n: r^n=1\}$ . Now I'm interested in the automorphism groups $\operatorname{Aut}(\operatorname{SL}(n,K))$ , $\operatorname{Aut}(\operatorname{PGL}(n,K))$ , and $\operatorname{Aut}(\operatorname{PSL}(n,K))$ . I've searched for small $n$ and $|K|$ using GAP and found that they are the same. So are these three groups really isomorphic? Edit 1: I realized that $\operatorname{SL}(n,K)$ , $\operatorname{PGL}(n,K)$ , and $\operatorname{PSL}(n,K)$ are in many cases isomorphic by themselves. For $K$ finite, this happens if and only if $\gcd(n,|K|-1)=1$ (so for $n=3$ , the first nontrivial cases are $|K|=4$ and $|K|=7$ ). Edit 2: I think my conjecture is very likely to fail if $K$ behaves not so well. So please assume that $K$ is finite if necessary.","['automorphism-group', 'group-theory', 'group-isomorphism']"
4419910,"If $\vert A\vert+\vert B\vert =0,$ then What is the value of $\vert A+B\vert$?","There are two square matrices $A$ and $B$ of same order such that $A^2=I$ and $B^2=I,$ Where $I$ is a unit matrix.If $\vert A\vert+\vert
 B\vert =0,$ then find the value of $\vert A+B\vert ,$ here $\vert
 A\vert$ denotes the determinant of matrix $A$ Solution: Since $A^2=I$ then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is $\lambda_A^2-1=0\implies \lambda_A=+1,-1$ . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So $\vert A\vert =(+1)(-1)=-1\tag{1}$ . Similarly,Since $B^2=I$ then Cayley-Hamilton theorem implies that the characteristic polynomial of last equation is $\lambda_B^2-1=0\implies \lambda_B=+1,-1$ . Since the product of Eigenvalues of a matrix is equal to the determinant of the matrix,So $\vert B\vert =(+1)(-1)=-1\tag{2}$ . Equations $(1)$ & $(2)$ implies that $\vert A\vert +\vert B\vert=-2$ But,it is given that $\vert A\vert+\vert B\vert =0$ Please point out my mistake??","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'solution-verification', 'linear-algebra']"
4419927,Can One Prescribe a Solid Angle to the Vertices of the Platonic Solid,"Can One Prescribe a solid angle to the vertices of the platonic solid? The canonical example given to demonstrate the concept of a solid angle gives one a conical spherical cap whose base encloses some area. The solid angle is the higher-dimensional analogue of the plane angle and is prescribed to the vertex of this cone-like cap. The solid angle, Ω, is then defined in relation to the area, A, enclosed by the base and r, the spherical radius: Ω=𝐴/𝑟^2
. Can this same concept be used to ascribe a solid angle to the vertices with regular solid such as the platonic solid? It is an almost trivial fact that the sum of the angles of a quadrilateral in regular flat space that is 360 degrees or 2π radians. Is there, for example, a higher-dimensional analog for a sum of solid angles at the vertices within a regular cube?","['trigonometry', 'geometry']"
4419939,Confidence interval of transformed random variable,"Let $X$ be a Normal random variable of mean $\mu$ and variance $\sigma^2$ . Also let $g\colon \mathbb{R}\to\mathbb{R}_{>0}$ be a positive strictly increasing bijective function. I would like to find the symmetric 0.95 ""confidence interval"" of $g(X)$ centred by its mean, that is, find $a$ such that $$
\mathbb{P}\big(z - a \leq g(X) \leq z + a\big)=0.95,
$$ where $z = \mathbb{E}[g(X)]$ is the mean. Suppose that such interval exists. However, I failed to compute the interval in closed-form. Observe that $$
\mathbb{P}\big(z - a \leq g(X) \leq z + a\big) = \mathbb{P}\big(g^{-1}(z - a) \leq X \leq g^{-1}(z + a)\big),
$$ so essentially, we want to find the $a$ by solving $$
\mathrm{CDF}(g^{-1}(z + a)) - \mathrm{CDF}(g^{-1}(z - a)) = 0.95,
$$ where $\mathrm{CDF}$ is the CDF of $X$ . But how to find a closed-form solution/approximation to this equation? By ""approximation"", I mean an analogy of $\mu \pm 1.96\sigma$ for $\approx0.95$ of Normal.","['statistics', 'confidence-interval', 'probability-distributions', 'probability-theory', 'probability']"
4419949,Bayes's rule and probability theory,"I'm trying to solve such a task: Preliminary tests of students showed that 30% of them are very well prepared to the exam, 50% are prepared quite well and 20% are prepared somehow. For the first group of students probability of getting the highest grade is 90%, for the second group of students it's 70% and for the last one — only 30%. A random student got the highest grade.
What's the probability that he was very well prepared? This is how I see the task: G1 - 30%, G2 - 50%, G3 - 20% (groups prepared very well, quite well and somehow) H - event to get highest grade P(H|G1) - 90%, P(H|G2) - 70%, P(H|G3) - 30% I should find P(G1|H) - ? P(G1|H) = P(H|G1)xP(G1) / P(H) P(H) = (0.3 x 0.9)+(0.5 x 0.7)+(0.2 x 0.3) = 0.68 P(G1) = 1/3 (probability to chose one of 3 groups) P(G1|H) = 0.9*0.33 / 0.68 = 0.44 My answer is not correct. What is wrong with my assumptions/calculations?","['integration', 'conditional-probability', 'calculus', 'bayes-theorem', 'probability-theory']"
4419979,Riemann Integral vs. Scalar Field Integral vs. Vector Field Integral (In single variable case),"I am a bit confused about the relation between these three integrals. As my textbook doesn't seem to draw a very clear distinction between them. [this is somewhat related to this question Are regular/Riemann integrals a special case of a line integral? ] Suppose there is a single-variable real-valued function $y=f(x)$ The Riemann integral is then: $$\int_a^bf(x)dx,\ \ \ \text{where$\int_a^b=-\int_b^a$}$$ The Vector Field line integral is then: $$\int_a^bf(x)\vec j\cdot \vec idx=0,\ \ \ \text{where$\int_a^b=-\int_b^a$}$$ The Scalar Field line integral is then: $$\int_a^bf(x)|dx|,\ \ \ \text{where$\int_a^b=\int_b^a$}$$ The Scalar Field Integral has the same absolute value as the Riemann Integral, but different in the property $\int_a^b=-\int_b^a$ The Vector Field Integral has the same property $\int_a^b=-\int_b^a$ as the Riemann Integral, but different in the absolute value (in this particular case, the value of vector integral is zero). [Q1]: Are these three types of integral inherently different things? Or they can be treated as special cases of a generalized form [Q2]: When parametrize a two variable scalar field line integral. $$\int_Cf(x,y)ds=\int_a^bf(t)\left\| {\,\vec r'\left( t \right)} \right\|dt$$ My textbook assumed that the parametrized integral is a Riemann integral, but why?","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4420005,Convergence of Euler's Method,"I am to show the IVP as defined by $$y' = ay + b, \quad y(0) = y_0$$ converges using Euler's method. To do this, I need to show that $$\lim_{h \to 0} u_n = y_n$$ where $u_n$ is the approximated value and $y_n$ is the exact value (given by $y(t) = (y_0 + \frac{b}{a}) e^{at} - \frac{b}{a}$ ). Then $$y_0 = y_0$$ and $$y_1 = y_0 + hf(t_0, y_0) = y_0 + hay_0 + hb$$ $$y_2 = y_1 + hf(t_1, y_1) = y_1 + hay_1 + hb = y_0 + hay_0 + hb + ha(y_0 + hay_0 + hb) + hb$$ However, when I go up until $y_n$ , I can't find a function that yields the sequence. I have tried FindSequenceFunction but it didn't help either. How can I find such a function? If there's no such function, is there perhaps a better way to show the convergence? Edit: Using @Lutz Lehmann's hint; $$ y_1 = q y_0 + d $$ $$ y_2 = q y_1 + d = q(qy_0 + d) + d$$ $$ y_3 = q y_2 + q = q(q(qy_0 + d) + d) + d $$ Continuing this, don't we obtain $$ y_n = q^n y_0 + q^{n - 1} d + d $$ where $d = hb$ and $q = (1 + h)$ and $$ \lim_{h \to 0} (1+h) y_0 + (1+h)^{n -1} + hb = y_0$$ I must be missing something critical.","['numerical-methods', 'ordinary-differential-equations', 'eulers-method']"
4420020,Significance of Cauchy's theorem.,"I am studying complex analysis. In the beginning of the chapter on integration theory there is a theorem known as Cauchy's theorem. It states that: If $\Omega\subset \mathbb C$ be open and $f:\Omega\to \mathbb C$ be a holomorphic function and $C$ be a closed curve in $\Omega$ whose interior is also in the set, then $\int\limits_C f(z)dz=0$ . I want to know the significance of this theorem. I have tried to understand on my own but I want to make sure if it is all I need to understand. Cauchy's theorem basically says that if I integrate a holomorphic function along a closed curve, then the integral will be $0$ , this basically means that given any two points in the set $\Omega$ , I can choose whichever path I want between them for integrating between the two points without affecting the result. That means integral of a holomorphic function is path independent. Am I correct? Is there anything I am yet to realize about this theorem?","['complex-analysis', 'motivation', 'intuition']"
4420109,Regular pentagon folding a strip,"For young students it is an interesting surprise to discover that a knot tied  in a strip of paper is a regular pentagon. I'm interested to find a simple, but rigorous, geometrical proof of this ""experimental"" fact. The classical book  Mathematical Models by H. Martyn Cundy and A.P. Rollett ( cited in Wikipedia )  extend a similar construction to other  regular poligons, but don't gives a proof. It is easy to prove that in a regular pentagon a diagonal is parallel to the opposite side, but here we have to prove the converse.","['knot-theory', 'geometry', 'geometric-construction']"
4420139,Proof there are no perfect groups of order 3024,"How can I prove that there are no perfect groups of order $3024$ ? My attempt is the following: Each non-trivial finite perfect group admits a non-abelian simple quotient. This holds because if the group $G$ is simple, we have done, otherwise has to admit a normal subgroup $K$ . However $G/K$ is a non-trivial perfect group and or it’s simple or admits a normal subgroup $H$ . Then $K_1:=\pi^{-1}(H)$ is a normal subgroup of $G$ containing $K$ , where $\pi \colon G\to G/K$ is the projection map. Moreover the order of $K_1$ is strictly greater than the order of $K$ . Thus in a finite number of steps we get a normal subgroup of $G$ such that the quotient has to be simple and not-abelian. The only not-abelian simple groups dividing $3024$ are $PSL(2,\mathbb{F}_7)$ and $PSL(2, \mathbb{F}_8)$ . In the second case we have that the group $G$ of order $3024$ is an extension of $PSL(2, \mathbb{F}_8)$ by a kernel $K$ of order $6=3\cdot 2$ . The kernel has to admit a $3-$ Sylow normal subgroup $S$ , that is characteristic in $K$ , and so normal in $G$ . So $G/S$ is a perfect group of order $3024/3=1008$ but there are not perfect groups of order $1008$ . The second case is quite difficult.  Suppose that the group $G$ of order $3024$ is an extension of $PSL(2, \mathbb{F}_7)$ by a kernel $K$ of order $18=3^2\cdot 2$ . The kernel has to admit a $3-$ Sylow normal subgroup $S$ , that is characteristic in $K$ , and so normal in $G$ . So $G/S$ is a perfect group of order $3024/9=336$ . The only perfect group of order $336$ is $SL(2, \mathbb{F}_7)$ . Thus seems that there is not a contradiction. However I’m sure that there not exist perfect groups of order $3024$ , so someone of you does know some hint that can help me to solve this problem?","['finite-groups', 'simple-groups', 'normal-subgroups', 'sylow-theory', 'group-theory']"
4420181,Does the closure of a set of positive Lebesgue measure contain an open set?,Let $L^n(A)$ denote the $n$ -dimensional Lebesgue measure of a set $A \subset \mathbb R^n$ . Suppose that $L^n(A)>0$ . Is it true that the closure of $A$ contains an open set?,['measure-theory']
4420225,if P(A)$\in$P(B) which of the following is true ? 1) A$\in$B or 2) A$\subseteq$B,"So my question is this, assume for some sets A,B : ${\mathcal{P}(A)}$$\in$${\mathcal{P}(B)}$ which of the following is true? $A$$\in$$B$ $A$$\subseteq$$B$ By the assumtion of ${\mathcal{P}(A)}$$\in$${\mathcal{P}(B)}$ $\implies$ ${\mathcal{P}(A)}$$\subseteq$$B$ . Also for every set $A$$\subseteq$$A$ $\implies$ $A$$\in$${\mathcal{P}(A)}$ if $A$$\in$${\mathcal{P}(A)}$ and ${\mathcal{P}(A)}$$\subseteq$$B$ $\implies$ $A∈P(A)$$\subseteq$$B$ $\implies$ $A$$\in$$B$ Counterexample: Let $A$ = $\{$ 1 $\}$ Let $B$ = $\{$$\emptyset$ , $\{$ 1 $\}$$\}$ so ${\mathcal{P}(A)}$ = $\{$$\emptyset$ , $\{$ 1 $\}$$\}$ and ${\mathcal{P}(B)}$ = $\{$$\emptyset$ , $\{$$\emptyset$$\}$ , $\{$$\{$ 1 $\}$$\}$ , $\{$$\emptyset$ , $\{$ 1 $\}$$\}$$\}$ so the assumption of ${\mathcal{P}(A)}$$\in$${\mathcal{P}(B)}$ holds
but $A$$\not\subseteq $$B$ . Thank you Marc van Leeuwen","['elementary-set-theory', 'discrete-mathematics']"
4420227,"Is the canonical map $(\prod_{i \in I} V_i)\otimes (\prod_{j \in J} W_j)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j$ injective?","Let $k$ be a field and $\{V_i\}_{i \in I}$ and $\{W_j\}_{j \in J}$ be a collection of $k$ -vector spaces. We have a canonical map $$\left(\prod_{i \in I} V_i\right)\otimes \left(\prod_{j \in J} W_j\right)\to \prod_{(i,j)\in I\times J} V_i\otimes W_j: (v_i)_{i \in I}\otimes (w_j)_{j \in J}\mapsto (v_i\otimes w_j)_{(i,j)\in I\times J}.$$ Is this map injective in general? I can prove this for the direct sum, because there I can simply use a basis, however the direct product doesn't have a nice basis so I don't know how to proceed here. I tried tricks with linear independency, linear functionals etc. Thanks in advance for any help!","['abstract-algebra', 'linear-algebra', 'tensor-products']"
4420266,$a_n$-Consistency of the Largest Order Statistic (Jun Shao Example 2.34),"I am learning about different modes of consistency in statistics. I am having a hard time understanding the concept of $a_n$ - consistency . Here's the definition: Let $\{a_n\}$ be a sequence of positive constants diverging to $\infty$ . $T_n(X)$ is called $a_n$ - consistent for $\vartheta$ iff $a_n[T_n(X) - \vartheta] = O_p(1)$ w.r.t. any $P \in \mathcal{P}$ , where $X = (X_1, \cdots, X_n)$ is a sample from a population $P \in \mathcal{P}$ , $\vartheta$ is the parameter vector associated with the population $P$ , and $T_n(X)$ is a point estimator of $\vartheta$ for every $n$ . In a follow-up example, he showed that the largest order statistics $X_{(n)}$ is $n^{(m+1)^{-1}}$ -consistent when $X_1, \cdots, X_n$ are i.i.d. with a continuous c.d.f. $F$ satisfying $F(\theta) = 1$ for some $\theta \in \mathcal{R}$ , $F(x) < 1$ for any $x < \theta$ , the $i$ -th order left hand derivative of $F$ at $\theta$ exists and vanishes for any $i \leq m$ , and the $m$ -th order left hand derivative of $F$ exists and is non-zero. $m$ is a nonnegative integer. I have two questions about this example: Shao quoted a ""fact"" that $P(n[1-F(X_{(n)})] \geq s) = (1 - s/n)^n$ . However, it doesn't seem very intuitive to me how I can get to this ""fact"". How do I derive this? The second question is, how do I use the ""fact"" given in 1., and the Taylor expansion of $$1 - F(X_{(n)}) = \frac{(-1)^mF^{(m+1)}(\theta-)}{(m+1)!}(\theta - X_{(n)})^{m+1} + o(|\theta - X_{(n)}|^{m+1}) \; a.s.,$$ to get to the conclusion that $(\theta - X_{(n)})^{m+1} = O_p(n^{-1})$ ? The definition $O_p$ is: $X_n = O_p(Y_n)$ iff, for any $\epsilon > 0$ , there is a constant $C_{\epsilon} > 0$ such that $\sup_n P(\|X_n\| \geq C_{\epsilon}|Y_n|) < \epsilon$ , where $X_n$ is a random vector and $Y_n$ is a random variable. Thank you very much. Here is the screenshot of the example:","['statistics', 'probability-distributions', 'asymptotics', 'limits', 'probability-theory']"
4420305,Uniform continuity and derivatives [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Is there any example of a three times differentiable and uniformly continuous function $f: \Bbb R \rightarrow \Bbb R$ such that $f^{(3)}$ is bounded but $f^{(2)}$ isn't?","['examples-counterexamples', 'real-analysis', 'continuity', 'functions', 'derivatives']"
4420327,"If $f(x)$ is analytic and non-negative, does convergence of $\int_a^{\infty} f(x)\,dx$ imply $\lim_{x\to\infty} f(x)=0$?","It is well known that improper integrals don't have to satisfy $\lim_{x\to\infty} f(x)=0$ in order for $\int_a^{\infty} f(x)\,dx$ to converge, for instance $f(x)=\sin(x^2)$ . It is also possible to construct such non-negative $f(x)$ , and even continuous and non-negative, by taking $f(x)$ to be $0$ everywhere except for triangular spikes with appropriate converging areas. I was wondering whether we can get such $f(x)$ which is smooth or even analytic. It seems clear that it should be possible to modify the triangular spikes example and get such a smooth $f(x)$ by using bump functions. However, what if we wish for $f(x)$ to be analytic? Bump functions no longer come to the rescue. To summarize: does there exist a non-negative $f(x)$ which is analytic in $[a,\infty)$ and such that $\int_a^{\infty} f(x)\,dx$ converges, but such that $f(x)$ does not converge to $0$ at infinity?","['improper-integrals', 'real-analysis', 'calculus', 'riemann-integration', 'analytic-functions']"
4420333,"In a group, when does $|ab| = \mathrm{lcm}(|a|, |b|)$","In a finite abelian group $G$ where $a$ has order $m$ and $b$ has order $n$ , I was able to prove that $\mathrm{lcm}(m,n) \mid |ab|$ by proving that $(ab)^{\mathrm{lcm}(m,n)} = e$ . I know that it is not true in general that $|ab| \mid \mathrm{lcm}(m,n)$ , but I'm trying to motivate a counterexample. My thought was that the following formula relating the lcm and gcd is useful: $$
\mathrm{lcm}(m,n) = \frac{mn}{\mathrm{gcd}(m,n)}.
$$ If $m$ and $n$ are relatively prime, then $\mathrm{lcm}(m,n) = mn$ . That doesn't tell me anything necessarily about the order of $ab$ , however, unless it is true in general that if $\gcd(a,b) = 1$ , then $|ab| = |a||b|$ . In that case, we have $\mathrm{lcm}(m,n) = |ab| = mn$ if and only if $m$ and $n$ are relatively prime, and I should look for a counterexample where two elements have relatively prime order. I'd appreciate if someone could comment a bit more on this. I'm especially interested in how to motivate counterexamples. Though I'm able to follow those I read, I have trouble coming up with them on my own.","['proof-explanation', 'gcd-and-lcm', 'finite-groups', 'group-theory', 'abelian-groups']"
4420356,Show that $4\leq |z+3|+|z^2-z+4|\leq 11$ where $|z|=2$,"Let $z\in \mathbb{C}$ and $|z|=2$ .Show that $$4\leq |z+3|+|z^2-z+4|\leq 11$$ Is there a non-calculus way to do this My Attempt: I used method which involved calculus. Let $z=2(\cos t+i\sin t)$ $\Rightarrow|z+3|=\sqrt{13+12\cos t}$ $\Rightarrow|z^2-z+4|=2|4\cos t-1|$ Let $f(x)=\sqrt{13+12x}+2|4x-1|$ where $x\in [-1,1]$ Now here $f(-1)=f(1)=11$ $f'(x)=\frac{6}{\sqrt{13+12x}}-8<0$ for $x\in [-1,\frac{1}{4})$ and $f'(x)=\frac{6}{\sqrt{13+12x}}+8>0$ for $x\in (\frac{1}{4},1]$ So, clearly $x=\frac{1}{4}$ is a critical point as $f(x)$ is not differentiable at $x=\frac{1}{4}$ . $f(\frac{1}{4})=4$ So, $4\leq f(x)\leq 11$","['maxima-minima', 'algebra-precalculus', 'geometry', 'complex-numbers']"
4420383,Why are the cardinalities of these subsets not adding up to the universal set?,"Given a set of six letters $S$ = {A, B, C, D, E, F}, I am trying to find how many lists of four letters could be made that have exactly no repeated letters, exactly two repeated letters, exactly three repeated letters, and exactly four repeated letters. The set of $6^4=1296$ possible lists in total represents the universal set of lists and has cardinality of $|S| = 1296$ . Let $U$ = the universal set, and let $X_a$ represent the subsets of $U$ where $a$ is the number of repeated letters in the elements of the subset.  For example, ABCA $\in X_2$ and DAFE $\in X_0$ . By the addition principle, since $\bigcap_{a \in {\{0, 2,3,4\}}}X_a =\emptyset$ , then $|\bigcup_{a \in \{0, 2, 3, 4\}}X_a|=|U|=1296$ .  But this does not seem to be the case when I attempt to solve this problem.  Please let me know where I go wrong. In order to find $|X_0|$ , it is a permutations problem. $6*5*4*3=360$ , so there are $360$ lists of four letters with exactly no repeated letters. $|X_2|$ and $|X_3|$ are where I am less confident in my method. In order to find $|X_2|$ , there are $4 \choose 2$ ways to choose two spots of the four possible in the list.  That means for any given spot in the list, there are $6$ letters that spot can be, but for another spot, but exactly one other spot must be that same letter.  So for any given choice of two spots in the list such as the first spot and the second spot, $6*1*5*4$ ways of arranging the letters for that specific choice of spots in the list.  Because of this, I believe that $|X_2|$ = ${4 \choose 2} * 6*5*4*1=720$ . In order to find $|X_3|$ , there are $4 \choose 3$ ways to choose three spots of the four possible in the list.  I believe that the same logic as the previous step applies, where there are $6*1*1*5$ possible ways of choosing letters for each choice of three spots in the list.  Following this logic, there should be $|X_3|$ = ${4 \choose 3} * 6*5*1*1=120$ . Lastly, to find $|X_4|$ , there are only six possible ways to arrange these letters where all four are the same:
AAAA, BBBB, CCCC, DDDD, EEEE, and FFFF, so $|X_4|=6$ . After all of this $|X_0|+|X_2| + |X_3| + |X_4| = 360+720+120+6=1206$ which is $90$ off from what it should be.  Can someone please explain what I've done wrong?","['elementary-set-theory', 'combinatorics', 'permutations']"
4420384,"Range of $\sqrt{a^2 + (1 - b)^2} + \sqrt{b^2 + (1 - c)^2} + \sqrt{c^2 + (1 - d)^2} + \sqrt{d^2 + (1 - a)^2}$ on $0 \le a,$ $b,$ $c,$ $d \le 1.$","Let $0 \le a,$ $b,$ $c,$ $d \le 1.$ Find the possible values of the expression $$\sqrt{a^2 + (1 - b)^2} + \sqrt{b^2 + (1 - c)^2} + \sqrt{c^2 + (1 - d)^2} + \sqrt{d^2 + (1 - a)^2}.$$ I tried to use some inequalities to find the bounds of the expression, but it didn't really work. Also, I don't know calculus yet, so please keep the responses and hints non-calc. Thanks in advance!!","['algebra-precalculus', 'radicals', 'upper-lower-bounds', 'inequality']"
4420451,Controllability for second order coupled system,"I have the following system: $\ddot{y_1}=-y_1+\alpha y_2+u_1$ $\ddot{y_2}=-y_2+\alpha y_1-2u_2$ I am trying to answer 4 questions: For what values of $\alpha$ is the system controllable For what values of $\alpha$ is the system controllable from $u_1$ alone For what values of $\alpha$ is the system controllable from $u_2$ alone For what values of $\alpha$ is the system controllable if $u_1=u_2$ My workings: First, let's convert it into state space form by setting $x_1=y_1, x_2=y_2,x_3=\dot{y_1},x_4=\dot{y_2}$ We get $\begin{bmatrix}\dot{x_1}\\\dot{x_2}\\\dot{x_3}\\\dot{x_4}\\\end{bmatrix}=\begin{bmatrix}0&0&1&0\\0&0&0&1\\-1&\alpha&0&0\\\alpha&-1&0&0 \end{bmatrix}\begin{bmatrix}x_1\\x_2\\x_3\\x_4\\\end{bmatrix}+\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix}\begin{bmatrix}u_1\\u_2\\\end{bmatrix}$ . To answer 1) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0&0\\0&0\\1&0\\0&-2\end{bmatrix}$ . This matrix appears to always be full rank. To answer 2) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\1\\0\end{bmatrix}$ . This matrix appears to be full rank only when $\alpha\ne0$ . To answer 3) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\0\\-2\end{bmatrix}$ This matrix appears to be full rank only when $\alpha\ne0$ . To answer 4) We check controllability by checking the rank of $[B,AB,A^2B,A^3B]$ with $B=\begin{bmatrix}0\\0\\1\\-2\end{bmatrix}$ This matrix appears to be full rank only when $\alpha\ne0$ . Are the answers really this trivial or am I making a mistake somewhere?","['ordinary-differential-equations', 'control-theory', 'linear-control', 'optimization', 'dynamical-systems']"
4420485,"The given function $\phi$ is not an isomorphism because it is not surjective. But it's not injective either, right?","Here's the problem I did for homework from A First Course in Abstract Algebra, 7th Edition by John B. Fraleigh. I just want to check if my reasoning is correct on problem number 15 from Section 3: Let $F$ be the set of all functions $f$ mapping $\mathbb{R}$ into $\mathbb{R}$ that have derivatives of all orders. Determine whether the given map $\phi$ is an isomorphism of the first binary structure with the second. If it is not an isomorphism, why not? $\langle F,+\rangle$ with $\langle F,+\rangle$ where $\phi(f)(x)=x\cdot f(x)$ Given the binary structures $\langle S,*\rangle$ and $\langle S',*'\rangle$ , in our class we define an isomorphism $\phi:S\to S'$ as a one-to-one correspondence such that $\phi(x*y)=\phi(x)*'\phi(y)$ for all $x,y\in S$ . I checked whether $\phi$ in the given problem is injective and concluded it is not by doing the following: Assume $\phi(f)(x)=\phi(g)(x),\exists f,g\in F.$ $\begin{align} x\cdot f(x)&=x\cdot g(x)\\
x\cdot f(x)-x\cdot g(x)&=0 \\
x(f(x)-g(x))&=0\end{align} $ $\implies x=0$ or $f(x)=g(x)$ . Since $f(x)=g(x)$ need not hold if $x=0,$ $\phi$ is not injective. Therefore, $\phi$ is not an isomorphism since it is not a one-to-one correspondence. However, the solution manual said: It is not an isomorphism because $\phi$ does not map $F$ onto $F$ . Note that $\phi(f)(0)=0\cdot f(0)=0.$ Thus there is no element of $F$ that is mapped by $\phi$ into the constant function $1$ . It seems Fraleigh used a similar line of reasoning to show $\phi$ is not surjective that I did when I concluded $\phi$ is not injective. Is my answer correct too? Thanks for your help!","['group-isomorphism', 'calculus', 'functions', 'abstract-algebra', 'binary-operations']"
4420551,Free abelian group generated by a set,"Let $S$ be a set. Then one can construct the free abelian group with basis $S$ as the set of all functions from $S \to \mathbb{Z}$ with finite support: $$\mathbb{Z}^{(S)}:=\{f:S \to \mathbb{Z} \ | \ \mathrm{supp}(f) \ \text{has finite cardinality}\}.$$ One can define an addition on $\mathbb{Z}^{(S)}$ the usual way $$+:\mathbb{Z}^{(S)} \times \mathbb{Z}^{(S)} \to \mathbb{Z}^{(S)}, \ (f,g) \mapsto f+g$$ with $(f+g)(s):=f(s)+g(s)$ for all $s \in S$ . This gives $\mathbb{Z}^{(S)}$ the structure of an abelian group. Now, every $f \in \mathbb{Z}^{(S)}$ can be written uniquely as $$f=\sum_{\{x \in S \ | \ f(x) \neq 0 \}} f(x) 1_x.$$ However, this does technically not have $S$ as a basis, but rather the functions $1_x$ for all $x \in S$ . Surely, those are in $1:1$ correspondence with the elements of $S$ , but does that allow for writing $\sum_{\{x \in S \ | \ f(x) \neq 0\}} f(x)x$ , or is that incorrect? I have often seen this formal sum definition, which is why I suspect that it is correct, but I am unsure. I don't know what the existence of a set of formal sums would be, if there is any. An alternative notation is to write $f$ as $$f=\sum_{x \in S} f(x)1_x$$ and mention that this sum is indeed finite. However, isn't this rather also a bit inaccurate? As a somewhat related question: I wanted to remind myself why one can write $f=\sum f(x)1_x$ despite not ""knowing"" the $f(x)$ . This was because one knows those exist and satisfy the condition that for all $s \in S$ $f(s)=(\sum f(x)1_x)(s)$ . ""Not knowing the elements"" was a silly sidethought I had, I just wanted to make sure if my explanation here is correct, since this is kind of related.","['free-abelian-group', 'notation', 'abstract-algebra', 'linear-algebra', 'elementary-set-theory']"
4420630,Convergence Analysis of A Vector Sequence with Discretization Recurrences (with Toy Examples),"I'm confused by how to analyse if a vector sequence is convergent or not. Here I first post the original problem as follows: (Although this post is long, the problem meaning is easy to understand but how to prove it is very hard for me.) (You may first jump over the original problem below and take a glance at my toy example .) 1. Problem: Given a sequence length value $T\in\mathbb{N}$ , a threshold $K\in\mathbb{N}$ and target summation $S\in\mathbb{N}$ , an N-dimensional vector sequence $\{\mathbf{v}_t\}_{t=0}^{T}$ is produced by: The first item $\mathbf{v}_0\in\mathbb{R}^N$ is given by random generation. All its elements are non-negative and their summation is a non-negative integer denoted by $S\in\mathbb{N}$ that satisfies: $0\le S\le KN$ ; The subsequent items is produced by the following steps (procedure): $~~~~~~~$ (1) Initialize $\mathbf{v}_t:=\mathbf{v}_{t-1}$ ; $~~~~~~~$ (2) Calculate the average error between the element summation and $S$ , i.e., let $\delta_t:=\text{average}(\mathbf{v}_t)-(S/N)$ ; $~~~~~~~$ (3) Force the summation of its all elements to be $S$ , i.e., $\mathbf{v}_t:=\mathbf{v}_t-\delta_t$ , here each element of $\mathbf{v}_t$ is reduced by $\delta_t$ ; $~~~~~~~$ (4) Force all elements to be integers in $[0,K]$ , i.e., $\mathbf{v}_t:=\text{round}(\text{clip}_{0,K}(\mathbf{v}_t))$ , where $\text{round}(\cdot)$ and $\text{clip}_{0,K}(\cdot)$ are element-wise operator, they calculate the new vector element values dimension-by-dimension. The complete recurrence formula should be: $$
\mathbf{v}_t:=\text{round}(\text{clip}_{0,K}(\mathbf{v}_{t-1}-[\text{average}(\mathbf{v}_{t-1})-(S/N)])).
$$ Is $\{\mathbf{v}_t\}_{t=0}^{T}$ convergent or not when $T\rightarrow\infty$ ? (Some above expressions may be informal but do not affect the meaning of this problem) There is a toy example (created by myself) that may be helpful to understand the original problem: 2. My Example (may be a little weird but not impactive): I have three children ( $N=3$ ) and ten stamps ( $S=10$ ). My children weigh $90$ kg, $6$ kg and $4$ kg (the first one may be a teenager and the latter two may be little baby boys), respectively, so at the beginning, I want to give them $9$ , $0.6$ and $0.4$ stamps, respectively. But there are three constrains. Firstly, I think a child can not have more than four stamps ( $K=4$ ). Secondly, the stamps can only be given one-by-one and one single stamp can not be cut, divided or shared (the stamps given to each child must be with a non-negative integer number). Thirdly, I only have ten stamps in total, and I want to try my best to distribute them all (the summation of all elements should be $S$ or close to $S$ ). So I perform the above mentioned procedure as follows: $~~~~~~~$ (1) I initialize the stamp number vector: $(9, 0.6, 0.4)$ ; First round: $~~~~~~~$ (2) I calculate the average error $\delta$ , and found that it is zero; $~~~~~~~$ (3) I clip each element into $[0,4]$ and round them, then I get: $(4, 1, 0)$ ; Second round: $~~~~~~~$ (4) I calculate the average error $\delta=(5-10)/3=-1.67$ ; $~~~~~~~$ (5) I correct the stamp number vector: $(4, 1, 0)-(-1.67)=(5.67,2.67,1.67)$ ; $~~~~~~~$ (6) I clip each element into $[0,4]$ and round them, then I get: $(4, 3, 2)$ ; Third round: $~~~~~~~$ (7) I calculate the average error $\delta=(9-10)/3=-0.33$ ; $~~~~~~~$ (8) I correct the stamp number vector: $(4, 3, 2)-(-0.33)=(4.33,3.33,2.33)$ ; $~~~~~~~$ (9) I clip each element into $[0,4]$ and round them, then I get: $(4, 3, 2)$ . I find that the stamp vector sequence is converged, so I stop, give the children four, three and two stamps, respectively, and keep one stamp. 3. My Efforts (may be not a correct idea): I'm trying to prove that the average error sequence $\{\delta_t\}$ is absolutely convergent, i.e., I was trying to show that: $$
0\le |\delta_{t}|\le|\delta_{t-1}|,
$$ but I was failed. And I think if $\{\delta_t\}$ is convergent, we can not say that $\{\mathbf{v}_t\}$ is convergent. May be I need to prove that $\{\delta_t\}$ is convergent to zero? However, I empirically find that $\{|\delta_t|\}$ may only be convergent to a non-negative number in most cases (please see below). Nevertheless, I simulated the absolute average error sequence $\{|\delta_t|\}$ by a toy Python program and observed that $\{|\delta_t|\}$ may be convergent: import numpy as np
from matplotlib import pyplot as plt

N = 100  # vector dimension
T = 30  # max sequence length
S = 51200  # target sum of vector elements
K = 1024  # max value of each element

for _ in range(1000):  # 1000 test instances
    v = np.random.rand(N)  # uniformly distributed random vector
    v = (v / np.sum(v)) * S  # normalize to make the element sum be S
    abs_delta_list = []  # all absolute values of deltas
    for t in range(T + 1):  # t = 0, 1, 2, ..., T
        delta = np.mean(v) - (S / N)  # average error
        v -= delta  # force the sum to be S
        v = np.round(np.clip(v, 0, K))  # force all elements to be intergers in [0, K]
        abs_delta_list.append(np.abs(delta))  # collect abs(delta)
    plt.plot(abs_delta_list, color='blue')  # plot curve

plt.xlabel('t')
plt.ylabel('abs(delta)')
plt.savefig('abs_delta.png', dpi=600) I obtained: $(N=100, T=30, S=5120, K=1024)$ $(N=10, T=30, S=5120, K=1024)$ (Obviously, $\delta_1$ is zero). 4. Update: I've solved this problem with the help of @antkam.","['integer-programming', 'discrete-mathematics', 'discrete-optimization', 'convergence-divergence', 'sequences-and-series']"
4420646,Determine at what points the complex function $f\left(z\right)=e^{2x}\cos3x+ie^{3x}\sin2y$ is differentiable.,"Determine at what points the complex function $$f(z)=e^{2x}\cos3y+ie^{3x}\sin2y$$ is differentiable. The function is differentiable at $z_0 \in \mathbb C$ if $f$ is defined on a neighborhood of $z_0$ contained in $D_f=\mathbb C$ (in this example this condition is satisfied) and $\lim_{z \to z_0}\frac{f\left(z\right)-f\left(z_{0}\right)}{z-z_{0}}$ exists, but here computing the limit is difficult, so what's the the alternative solution? And generally when we are asked to find all points that a specific function is differentiable at such points what should we do?","['complex-analysis', 'cauchy-riemann-equations', 'derivatives', 'complex-numbers']"
4420719,derivative of matrix with respect to vector,"I need to calculate the derivative of matrix w.r.t. vector. < Given Equation > 1) $\mathbb Y = \mathbb A \mathbb X$ ,where $\mathbb A$ : (n $\times$ n) matrix $\mathbb X$ : (n $\times$ 1) vector. 2)
all elements in $\mathbb A$ and $\mathbb X$ are the function of $z_i$ , where $\mathbb Z = [z_1\ z_2\ \cdots\ z_m]^\top$ In other words, $\mathbb Y(z)=\mathbb A(z) \mathbb X(z)$ < Problem definition > I want to calculate the following partial derivative: $\frac{\partial \mathbb Y}{\partial \mathbb Z}$ , which yields a (n $\times$ m) matrix From the general derivation rule for multiplication, it looks like the rule can be expanded (with some modifications) to the matrix/vector version, $\frac{\partial \mathbb Y}{\partial \mathbb Z}
= 
\frac{\partial (\mathbb A \mathbb X)}{\partial \mathbb Z}
=
\frac{\partial \mathbb A}{\partial \mathbb Z}\mathbb X
+
\mathbb A \frac{\partial \mathbb X}{\partial \mathbb Z}$ However, the above rule is wrong, as you can easily see that the first term's dimension doesn't coincide with (n $\times$ m). I want to calculate the derivation without explicitly calculating all elements in the output $\mathbb Y$ .
How can I solve this problem?","['matrices', 'matrix-calculus', 'linear-algebra', 'partial-derivative', 'derivatives']"
4420750,"Show $\lim_{n\to \infty} \int_n^\infty \left(1+\frac{x}{n} \right)^n e^{-x} \, dx=0.$","I want to show $$\displaystyle\lim_{n\to \infty} \int_n^\infty \left(1+\frac{x}{n} \right)^n e^{-x} \, dx=0.$$ Hint is given ; HINT : $\displaystyle\lim_{n\to \infty} u_n=0,$ where $u_n=\displaystyle\max_{x\geqq n}\left(1+\frac{x}{n}\right)^n e^{-x}.$ So far, I tried to do simply by $\epsilon-n.$ Let $\epsilon >0.$ Since $\lim u_n=0,$ there is $N\in \mathbb N$ s.t. $n\geqq N \Rightarrow u_n<\epsilon.$ When $n\geqq N$ , $\left| \int_n^\infty \left(1+\frac{x}{n}\right)^n e^{-x} \, dx \right| \leqq \int_n^\infty \left| \left(1+\frac{x}{n}\right)^n e^{-x} \right| \, dx =\int_n^\infty \left(1+\frac{x}{n}\right)^n e^{-x}  \, dx \leqq \int_n^\infty  u_n \, dx < \int_n^\infty \epsilon \, dx.$ This fails because the upper limit of integral is $\infty.$ I don't know how I should handle the upper limit $\infty.$ I'd like you give me any idea.","['limits', 'calculus', 'improper-integrals']"
4420777,"Using differentiation under the integral sign to evaluate $\int\ln(x^n+1) \,\mathrm dx$ [closed]","Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 2 years ago . Improve this question This is my first stack exchange question, so sorry if it is not neat. So for $$\int\ln(x^n+1)\,\mathrm dx,
$$ I tried doing $\ln(a(x^n + 1))$ where $a = 1$ and differentiating under the integral. So I set the integral is $Q(a)$ and get $$
Q^\prime(a)=\int 1/a\,\mathrm dx
$$ because we treat $(x^n + 1)$ like a coefficient. And then $Q^\prime(a) = dQ/da=x/a$ , so $Q = x\ln(a) + C$ but setting $a = 1$ you get $Q(1) = C$ and the derivative of $C$ is $0$ , which isn't $\ln(x^n + 1)$ . So what did I do wrong?","['integration', 'calculus', 'derivatives']"
4420798,Square and equilateral triangles,"CDEF is a square, CBF and BEA are equilateral triangles. Find angle $x$ . What I have found at the moment: $\angle BEF =15^{\circ}, \angle AED =135^{\circ} $ I managed to find the length of $BE$ and use the Law of Cosines in $\triangle ADE$ to find $AD$ and to use the Law of Cosines one more time to get the $\cos \angle ADE$ , but I didn't get good numbers. Maybe there should be an analytical way to solve the problem.","['triangles', 'trigonometry', 'geometry', 'plane-geometry']"
4420799,The categorical role of $\overline{\mathbb{N}}$ and sequences in the category of topological spaces,"If you have a nice topological space, say, one in which the topology comes from a metric, then every topological property (like being continuous, closed, open,...) can be described in terms of sequences. As sequences in $X$ correspond to maps from the one point compactification $\overline{\mathbb{N}}=\mathbb{N}\cup\{\infty\}$ to $X$ , this means that the space $\overline{\mathbb{N}}$ is somehow import in the overall category. Hence, my question is Is there a categorical notion which describes the role of $\overline{\mathbb{N}}$ in the category of ""nice"" topological spaces? For example, the functor of points $\operatorname{Hom}(\overline{\mathbb{N}},-)$ reflects isomorphisms, which is nice but I don't know if it is enough to recover the important properties about sequences or to characterize $\overline{\mathbb{N}}$ (maybe it is enough).","['general-topology', 'metric-spaces', 'category-theory']"
4420800,Is the union of a set of ordinals a nonlimit ordinal?,"Let $A$ be an arbitrary set of nonlimit ordinals less than a given $\kappa$ , and let the union $A$ be also less than $\kappa$ . Does this union have to be a nonlimit ordinal if $\kappa=\omega$ ; $\kappa$ is an uncountable cardinal. In the first case, the answer is yes? $A$ must be a subset of omega such that $\bigcup A < \omega$ . So $A$ must be finite? And then the union will be a limit ordinal? In the second case, I tried to consider $\aleph_1$ . But here it is unclear how to describe its subsets $A$ for which $\bigcup A < \aleph_1$ ...","['elementary-set-theory', 'ordinals']"
4421042,Finding a spanning tree with at least 100 leaves,"I have the following graph theory problem: In a country there are pairs of towns connected by roads in such a
way that you can get from any town to any by those roads. The
president of the country had ordered to build several new towns and
roads so that you could still get from any town to any by
roads. As it turned out, there are now $100$ roads that do not share
ends pairwise, connecting newly built towns to the older ones. Proof
that you can close a number of roads so that there would be
left only a single path between each two of the towns, and also that there
would be no less than $100$ towns that would have only a single road
leading out from them. I came up with the following: First, let's translate the problem to the language of graph theory.
Obviously, the towns and roads here are vertices and edges of an undirected graph, let's call it $G$ . The ""get from any town to any"" condition means that $G$ is connected. Next, we make a union with an nonempty set of vertices and edges $U$ such that $G \cup U$ is connected. This graph has $100$ edges each without a common end with one another, and each of those edges connects a vertex from $V(G)$ to a vertex from $V(U)$ . What needs to be proven is that this graph has a spanning tree (given by the ""single unique path"" condition) that has at least $100$ leaves. Such a tree does exist since the graph is connected. Second, I tried to consider the $100$ edges we have (let's call this edge set $F$ ). They don't have any ends in common, and each edge is incidental to a vertex $x \in V(G)$ of an arbitrary degree $\geq2$ and a vertex $y \in V(U)$ that is either a leaf vertex (deg. of $1$ ) or not. If it is a leaf vertex, then we don't need to do anything with it and we just add it to the number of needed leaves for the proof. If it isn't (i.e it has more than one vertex adjacent to it), then we consider two cases: a) $y$ has other leaves adjacent to it; b) it doesn't and each other adjacent vertex is of degree $\geq2$ . In the first case, we have one or more leaves found, and in the second case we remove all edges adjacent to $y$ except $xy \in F$ and now it is a leaf itself. Thus, we should be able to attain at least $100$ leaves. The final wanted condition is that we need the resulting graph to be a spanning tree. I'm not sure if I can just say that next we arbitrarily delete edges to attain it (would that be rigorous enough?), or if we need to construct a Trémaux tree or something along those lines. So, I would like to know if the first part of the solution (about edges and leaves) is sufficient enough for a proof and if it is correct at all, and would also like some help on the second part (about a spanning tree).","['trees', 'graph-theory', 'graph-connectivity', 'combinatorics', 'discrete-mathematics']"
4421049,A mapping of two sequences with no overlaps and partial assignments,"I want to characterize a correspondence mapping of two sequences $\psi : A \rightarrow B$ for an article that I am writing. I need help describing the function class.  I think this is an injective, non-surjective, order isomorphism . Please correct me on this? In the figure below I have visually described the constraints on $\psi$ adopting the representation language of the following article: https://www.mathsisfun.com/sets/injective-surjective-bijective.html Constraints on the mapping, $\psi$ . The gray arrows within each sequence indicate ordering within each sequence. This information is repeated in the text. Constraints: First and last elements of each sequence must correspond.  Given that $A$ has length $n$ , and $B$ has length $m$ : It can be said that $A(1) \widehat{=} B(1)$ and $A(n) \widehat{=} B(m)$ . Every element in $A$ has at most one element in $B$ . Not every element in $A$ can be assigned. The elements of $B$ can have at most one assignment. The assignments $A_i \rightarrow B_j$ cannot cross other assignments. That is if $A_i \widehat{=} B_j$ then $A_{i+1} \rightarrow B_k,\ \exists k > j$ Thank you.","['order-theory', 'functions', 'monotone-functions']"
4421075,Conditional expectation of the smaller uniform random variable,"Suppose $U_1, U_2 \sim \mathcal{U}[0,1]$ and both random variables are independent. I want to compute $E[U_1 \mid U_1 < U_2]$ . I think the correct ansewr is $$
E[U_{1}\mid U_{1}<U_{2}]=E[U_{1}\mid U_{1}=\min\{U_{1},U_{2}\}]=E[\min\{U_{1},U_{2}\}]=\int_{0}^{1}2(1-x)xdx=\frac{1}{3}.
$$ But, I was wondering why the following reasoning leads to incorrect solution. If we fix $U_2 =u$ , then $E[U_1\mid U_1 <u]=u/2$ . Each $u$ happens equally likely, i.e., $U_2\sim \mathcal{U}[0,1]$ , so $E[U_1\mid U_1 < U_2] = E[U_2]/2=1/4$ . Or, more formally, $$
E[U_{1}\mid U_{1}<U_{2}]=\int_{0}^{1}\int_{0}^{u}\frac{x}{u}dxdu=\int_{0}^{1}\frac{u}{2}du=\frac{1}{4}.
$$ Where did I make a mistake?","['statistics', 'conditional-expectation']"
4421127,Find all non-similar solutions of matrix equation,"Find all unique $($ not conjugate to each other by an element of $GL_2(\mathbb{Z}))$ matrices $A \in M_2(\mathbb{Z})$ , such that $A ^ 2 - 4A - I = 0$ , where $I$ is the identity matrix. From the equation it can be easily seen that matrix is a solution iff it has trace equal to $4$ and determinant equal to $-1$ . The main problem is to find all conjugacy classes. Since $\mathbb{Z}$ is not a field, Jordan, Frobenius and Smith normal forms are not applicable. I also tried to brute-force the problem and solve the equation $CA = BC$ , where $A$ and $B$ are two distinct solutions and $C \in GL_2(\mathbb{Z})$ , but this boils down to the system of 3 diophantine equations with 4 variables, one of which is not even linear (as in this paper, page 3) and I have no idea how to check existence of solutions for that. Another approach that I tried (again, without luck) was to decompose $A$ and $B$ into a product of $GL_2(\mathbb{Z})$ generators and check that they differ by a cyclic permutation (as listed here , point 6) but I was not able to do that in general for this family of matrices. I have also found a theorem (in this presentation by Svetlana Katok, slide 10), which is very cryptic to me, but I think it is impossible to use in practice. I feel that the answer is $\pmatrix{1 & 2 \\ 2 & 3}$ and $\pmatrix{0 & 1 \\ 1 & 4}$ because I could not find any counterexamples. Is there any simple way to find all conjugacy classes?","['matrix-equations', 'abstract-algebra', 'linear-algebra']"
4421140,The sample space of two six-sided dice and the idea that sets are unordered,"Take the common probability theory example of two six-sided dice. I saw the sample space for this example written as $$\{ \{1, 1\}, \{1, 2\}, \{2, 1\}, \dots, \{5, 6\}, \{6, 5\}, \{6, 6\} \}$$ But we know that sets are unordered. This means, for instance, that $\{1, 2\}$ is the same as $\{2, 1\}$ , and $\{5, 6\}$ is the same as $\{6, 5\}$ . So shouldn't this actually just be $$\{ \{1, 1\}, \{1, 2\}, \dots, \{5, 6\}, \{6, 6\} \}$$ ?",['probability-theory']
4421157,"Why do many people in math use the phrase ""almost always"" when referring to irrational numbers vs rational numbers? (from a language perspective)","I understand what they mean by ""almost always."" E.G.: Statement $A$ is true $\forall x∈ \mathbb{I}$ . Statement $A$ is false $\forall x∈ \mathbb{Q}$ . Therefore statement $A$ is almost always true... But isn't that being purposefully coy? Instead of using the label of ""almost always true,"" why not just be explicit?","['elementary-set-theory', 'convention', 'logic', 'terminology']"
4421171,Are these two recursive formulas known in the literature?,"First let me introduce the two recursive relations: $$\int_0^1 x^{n-1}\ln^a(1-x)dx=f(a,n),$$ where $$f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)  H_n^{(a-j)},\quad f(0,n)=\frac1n.\tag{1}$$ Cases using Mathematica: $$(-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n,$$ where $$g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)},\quad g(0,n)=1.\tag{2}$$ Cases using Mathematica: Question : Are $(1)$ and $(2)$ known in the literature? If so, any reference? Proof of $(1)$ : Take the logarithm of both sides of $$\operatorname{B}(m,n)=\Gamma(n)\prod_{k=0}^{n-1} \frac{1}{k+m},$$ we have \begin{gather*}
\ln\operatorname{B}(m,n)=\ln\Gamma(n)+\ln\prod_{k=0}^{n-1}\frac{1}{k+m}\\
\left\{\text{ use $\ln\prod a_n=\sum \ln(a_n)$}\right\}\\
=\ln\Gamma(n)-\sum_{k=0}^{n-1} \ln(k+m).
\end{gather*} Differentiate both sides with respect to $m$ , $$\frac{\frac{\partial}{\partial m}\operatorname{B}(m,n)}{\operatorname{B}(m,n)}=-\sum_{k=0}^{n-1} \frac{1}{k+m}$$ or \begin{equation}
\frac{\partial}{\partial m}\operatorname{B}(m,n)=-\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m}.
\end{equation} Let's keep differentiating w.r.t $m$ : \begin{equation}
\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)=\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}-\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} \begin{equation}
\frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)=-2\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}+2\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}\\
-\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} \begin{equation}
\frac{\partial^4}{\partial m^4}\operatorname{B}(m,n)=6\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^4}-6\frac{\partial}{\partial m}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^3}\\
+3\frac{\partial^2}{\partial m^2}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^2}
-\frac{\partial^3}{\partial m^3}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{k+m},
\end{equation} so in general $$\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\operatorname{B}(m,n)\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}.$$ Let $m$ approach $1$ and call $\displaystyle\left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m\to1}=f(a,n)$ , $$f(a,n)=(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}f(j,n)H_n^{(a-j)}$$ and the proof finishes on observing \begin{gather*}
\left.\frac{\partial^a}{\partial m^a}\operatorname{B}(m,n)\right|_{m=1}=\left.\int_0^1\frac{\partial^a}{\partial m^a} x^{n-1}(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\
=\left.\int_0^1 x^{n-1}\ln^a(1-x)(1-x)^{m-1}\mathrm{d}x\right|_{m=1}\\
=\int_0^1 x^{n-1}\ln^a(1-x)\mathrm{d}x=f(a,n)	
\end{gather*} and $f(0,n)=\int_0^1 x^{n-1}\mathrm{d}x=\frac1n.$ Proof of $(2)$ : We have $$\frac{1}{(1-x)^m}=(1-x)^{-m}=\sum_{n=0}^\infty \binom{m+n-1}{n}x^n$$ Take the $a$ -th derivative of both sides w.r.t $m$ $$(-1)^a\frac{\ln^a(1-x)}{(1-x)^m}=\sum_{n=0}^\infty \frac{\partial^a}{\partial^am}\binom{m+n-1}{n} x^n$$ Let $m\to 1$ and call $\displaystyle\left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1}=g(a,n)$ $$(-1)^a\frac{\ln^a(1-x)}{1-x}=\sum_{n=0}^\infty g(a,n) x^n$$ Note that $$\frac{\partial}{\partial m}\binom{m+n-1}{n}=\binom{m+n-1}{n}\left(\psi(m+n)-\psi(m)\right)=\binom{m+n-1}{n}\sum_{k=0}^{n-1}\frac{1}{k+m}$$ which can be generalized to $$\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}.$$ and so $$\left.\frac{\partial^a}{\partial m^a}\binom{m+n-1}{n}\right|_{m\to 1}$$ $$=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}\left.\frac{\partial^j}{\partial m^j}\binom{m+n-1}{n}\right|_{m\to 1}\,\left.\sum_{k=0}^{n-1} \frac{1}{(k+m)^{a-j}}\right|_{m\to 1}$$ or $$g(a,n)=-(a-1)!\sum_{j=0}^{a-1}\frac{(-1)^{a-j}}{j!}g(j,n)H_n^{(a-j)}$$ and the proof completes on observing $$g(0,n)=\left.\binom{m+n-1}{n}\right|_{m\to 1}=\binom{n}{n}=1$$ Note: A recursive relation, similar to $(2)$ , was introduced by @Marko Riedel here .","['integration', 'logarithms', 'recurrence-relations', 'reference-request', 'harmonic-numbers']"
4421195,Why does adding/subtracting/multiplying/dividing a number from both sides of an equality maintain the solution set?,"So if you add/divide/subtract/multiply both sides of an equation, obviously the equality is maintained since you're doing the ""same thing"" to both sides. But does preserving the equality but changing the number on either side of the equality change the solution set? I know the answer is no but what's the reason? My thinking is choose an arbitrary solution of $x$ in equation $A$ , and show it must satisfy equation $B$ , and vice versa. Is there a general rule/proof? I think it's easy to see with simple equations such as $2x + 82 = 7$ but what about one with many variables?",['algebra-precalculus']
4421209,Root finding and automatic differentiation,"Consider the equation $z = f (z, x)$ . We would like to find $z^{\star}$ for $f$ such that $z^{\star} = f (z^{\star}, x)$ . One way to do this problem is through naive iteration: $z^{(k + 1)} = f (z^{(k)},
x)$ ; stop when $z^{(k + 1)} \approx z^{(k)}$ . A faster way is to arrange the equation as $g (z) = f (z, x) - z$ . This
allows us to use Newton's root finding method: $$z^{(k + 1)} = z^{(k)} - \left( \frac{\partial g (z^{(k)})}{\partial z}
   \right)^{- 1} \cdot g (z^{(k)})$$ Having found $z^{\star}$ , let's say
we would like to find the the derivative of some loss function $l$ with
respect to $x$ . This can be done as: $$\frac{\partial l}{\partial x} = \frac{\partial l}{\partial z^{\star}}
   \frac{\partial z^{\star}}{\partial x} = \frac{\partial l}{\partial
   z^{\star}} \cdot - \left( \frac{\partial g}{\partial z^{\star}} \right)^{-
   1} \frac{\partial g}{\partial x}$$ Let's say we would to do this
using a software package that implements automatic differentiation like
PyTorch or JAX. Automatic differentiation has a foward pass and a backward pass. In the
forward pass we simply iterate through $z^{(k + 1)} = z^{(k)} - J (z^{(k)})^{-
1} \cdot g (z^{(k)})$ and save the output of each iteration. In the
backward pass, we evaluate the derivative $\frac{\partial l}{\partial x}$ through each of the iterations we had done in the forward pass. We can
think of this as unrolling the forward pass and passing $\frac{\partial l}{\partial x}$ from the output to the input. What I've just described is the standard way automatic differentiation
is used with the backpropagation algorithm. The problem here is that
we need to backpropagate through all the steps we did in the forward
phase. This is not only time consuming, but requires us to store the
outputs of all the iterations since they are needed in the backward
pass. I was reading this tutorial: http://implicit-layers-tutorial.org/implicit_functions/ , where the
author says, if we do implicit differentiation, then we don't need to
save the intermediate values because the only Jacobian (the term $\frac{\partial
g}{\partial z^{\star}}$ ) we need is the Jacobian at the solution point.
This is kind of a big deal because it means if you can reformulate your
function as an implicit function, then the backward phase of the
backpropagation becomes free. My issue is I don't fully understand why
we can avoid backpropagating through the solver by defining the function
as an implicit function. I would appreciate if someone can enlighten me.","['machine-learning', 'newton-raphson', 'derivatives', 'backpropagation']"
