question_id,title,body,tags
4531988,Prove random variable is in a set a.s and determine its distribution,"This was one of the questions in my exam that i did not figure out. For $\epsilon \in ]0,1[$ , $\operatorname{f}:[0,1]\to[0,\infty[$ is a continuous function with $$\operatorname{f}(x) \le \min(x,1-x) ,\forall x \in[0,1].$$ $(Z_i)_i$ , $i\in\mathbb{N}$ are i.i.d. random variables on probability space $(\Omega,\mathscr{F},\mathbb{P}$ ) with $$\mathbb{P}(Z_1=-1)=\frac{1}{2}=\mathbb{P}(Z_i=1)$$ We have Filtration $\mathscr{F}_n:=\sigma(Z_1,...,Z_n)$ for all $n\in\mathbb{N}$ and $\mathscr{F}_0:=\{\emptyset,\Omega\}$ , also recursively, $$X_n:=X_{n-1}+Z_n\cdot{f(X_{n-1})},X_0:=\epsilon.$$ Questions: a) Prove  that $X_n$ is a [0,1]-martingale with respect to $\mathscr{F_n}$ , and $(X_n)_n$ converges to a random variable $X_\infty$ almost surely. ( $X_n$ takes value in [0,1]) b) $\pmb{S}:=\{x \in[0,1]\mid\operatorname{f}(x)=0\}$ .
Prove: $\mathbb{P}(X_\infty\in \pmb{S})=1$ c) Assume that $\operatorname{f}(x)\gt0 $ for all $x\in]0,1[$ and $\operatorname{f}(0)=\operatorname{f}(1)=0 $ . Determine the distribution of $X_\infty$ . I finished a), which gives existence of $X_{\infty}$ . My idea : If $X_{\infty}$$\in$$\pmb{S}$ , we must have $\operatorname{f}(X_\infty)$$\in$ { $0,1$ }(if my opinion is correct ), that's where I get stuck, how to prove that. Since $\operatorname{f}(x)\le \min\{x,1-x\}$ , we have $\operatorname{f}(x)\le\frac{1}{2}$ , recursively $X_n$ converges to 0 or 1. How can I prove it in the right format? Assumption in c) is obvious, is that helpful to determine the distribution and how to find it. Thanks in advance!","['probability-limit-theorems', 'probability-distributions', 'martingales', 'probability-theory', 'random-variables']"
4531994,Solve in exact form: $x^6 - x^5 + 4 x^4 - 4 x^3 + 4 x^2 - x + 1=0$ ( WolframAlpha failed),"Solve the polynomial in closed form: $$x^6 - x^5 + 4 x^4 - 4 x^3 + 4 x^2 - x + 1=0$$ WolframAlpha obviously failed. I tried several ways: I tried the Rational Root Thereom, but there is no rational root. I tried possible factorisations $$x^6-x^5+4x^4-4x^3+4x^2-x+1= (x^2+a_1x+a_2)(x^4+a_3x^3+a_4x^2+a_5x+a_6)$$ and $$x^6-x^5+4x^4-4x^3+4x^2-x+1= (x^3+a_1x^2+a_2x+a_3)(x^3+a_4x+a_5)$$ But, the expansions are terrible!","['irreducible-polynomials', 'roots', 'polynomials', 'radicals', 'algebra-precalculus']"
4532048,"Spivak, Ch. 20, Problem 9c: Is there a Typo in this question item or not?","The following problem is from Chapter 20 of Spivak's Calculus . I've asked a separate question regarding the comment at the end of item $(c)$ . In the accepted answer to that question, the author of the response claims that item $(c)$ itself has a typographical error. I had already come up with a proof for that item, however, and my question now is if such a proof is incorrect. In other words, was the author of the aforementioned answer correct in his claim. (a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of
all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial
in $x-a$ ]. Again, give a more direct proof, using the obvious facts
about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ The claimed typo seems to be that $x\to a$ in the limits instead of $x\to 0$ . My attempt at a proof $$p(x)=\sum\limits_{i=1}^n a_i(x-a)^i$$ $$p(q(x)+R(x))=\sum\limits_{i=0}^n a_i(q(x)+R(x)-a)^i=\sum\limits_{i=0}^n a_i \sum\limits_{j=0}^i \binom{i}{j} (q(x)-a)^{i-j} R(x)^j$$ $$=\sum\limits_{i=0}^n a_i(q(x)-a)^i + \sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j$$ $$=p(q(x))+\bar{R(x)}$$ where $\bar{R}(x)=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j}(q(x)-a)^{i-j}R(x)^j$ Note that $(q(x)-a)^{i-j}$ is polynomial, hence continuous everywhere and $\lim\limits_{x\to 0} (q(x)-a)^{i-j}$ exists. Thus, $\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=\sum\limits_{i=0}^n a_i \sum\limits_{j=1}^i \binom{i}{j} \cdot \lim\limits_{x\to 0} (q(x)-a)^{i-j} \cdot \frac{R(x)^j}{(x-a)^n}=0$ Is this proof correct?","['integration', 'calculus', 'solution-verification', 'taylor-expansion', 'derivatives']"
4532073,Binomial Formula Combinatorial Proof [duplicate],"This question already has answers here : Proving $\sum^n_{k=0} k^2 {n\choose k} = (n+n^2)2^{n-2}$ (2 answers) Closed 1 year ago . Find a formula (not involving $\sum$ ) for the function $\mathbb{N} \to \mathbb{N}$ given by: $$f(n):= \sum\limits_{k = 0}^n {n \choose k} k^2.$$ Current thinking: To get the formula for $f(n):= \sum\limits_{k = 0}^{n} {n \choose k} {k}$ , I differentiated which gives $$\frac{\mathrm d}{\mathrm dx}(1+x)^n = n(1+x)^{n-1}$$ $$\frac{d}{dx}\sum\limits_{k = 0}^{n} {n \choose k} {k} = \sum\limits_{k = 0}^{n} {n \choose k} {kx^{k-1}}$$ Can I just differentiate twice in a similar manner if I wanted to get $f(n):= \Sigma_{k = 0}^{n} {n \choose k} {k^2}$ ?","['binomial-coefficients', 'combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4532097,"Can the fact ""If $\phi$ is a Hermitian linear functional on a $C^*$-algebra $A$ then $||\phi|| = \sup\{\phi(x):x = x^*, ||x|| = 1\}$"" be generalized?","This is Proposition 13.3 on Page 78 in An Introduction to Operator Algebras by Kehe Zhu. The statement is as follows: Proposition 13.3. If $\varphi$ is a Hermitian linear functional on a $C^*$ -algebra $A$ , then $\Vert \varphi \Vert = \sup\{\varphi(x):x = x^*, \Vert x\Vert \leq 1\}.$ where a linear functional $\varphi: A \rightarrow \mathbb{C}$ on a $C^*$ -algebra $A$ is Hermitian if $\varphi(x^*) = \overline{\varphi(x)}$ . I wonder whether this can be generalized to the case that the ""target"" of the linear map $\varphi$ is a general $C^*$ -algebra, say, $B$ ? I give the definition below: A linear map $\varphi: A \rightarrow B$ between two $C^*$ -algebras $A$ and $B$ is said to be Hermitian if $\varphi(x^*) = \varphi(x)^*$ . and I guess Proposition 13.3 can be generalized as the following statement: If $\varphi: A \rightarrow B$ is a Hermitian linear map between two $C^*$ -algebras $A$ and $B$ , then $\Vert \varphi \Vert = \sup\{\Vert\varphi(x)\Vert:x = x^*, \Vert x\Vert \leq 1\}$ . It is evident that in the above statement, the right-hand side is less than or equal to the left-hand side, because by definition $\Vert\varphi\Vert = \sup\{\Vert\varphi(x)\Vert: \Vert x\Vert \leq 1\} $ (without the extra limitation $x = x^*$ ).
However, I have no idea how to obtain the inequality in the reverse direction , because in Zhu's book he involved the properties of complex numbers such as rotations, etc. Could anyone present a proof of this statement or a counterexample to disprove this statement (although I may feel a bit disappointed)? Any help is appreciated.","['c-star-algebras', 'functional-calculus', 'operator-algebras', 'operator-theory', 'functional-analysis']"
4532099,Expectation of the exponential of the amount of time a Brownian motion spent in half space,"Let $(B)_{t\geq 0}$ be a standard Brownian motion and let $c\in\mathbb{R}$ . Can we calculate the expectation of the exponential of the time $B$ will be spent above $c$ , $$\mathbb{E}\left[\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\}\right]?$$ My thought is: Let $L_t:=\exp\left\{\int_0^t\mathbb{1}_{\{B_u>c\}}du\right\}$ , it is a non-decreasing process, hence, a sub-martingale. Next step is to identify its Doob-Meyer decomposition, and this is where I cannot proceed.","['probability', 'stochastic-analysis', 'stochastic-processes', 'brownian-motion', 'stochastic-calculus']"
4532120,Initial-value problem with a forcing function that has step discontinuity,"Context I am solving an equation of motion derived from a Lagrangian that has a discontinuous step in its potential at the origin. In other words $$V(x) = \left(V_2-V_1\right)H(x) + V_1.$$ Based on this context, my question regards how to solve an initial-value problem to obtain $\dot{x}$ . Bare in mind that I can solve this problem using conservation of energy, but that won't teach me anything new with respect to how to utilize the Lagrangian formalism. Question I would like to solve the following initial value problem $$   m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta(x)$$ with the initial conditions $$x(t_o) = x_o,~\text{where}~x_0<0,~\text{and}~\dot{x}(t_o) = v_o,~\text{where} ~v_0>0.$$ [In truth, I am interested only in $\dot x(t)$ ]. My Answer For all values $x$ excepting at $x=0$ , the mass is moving with constant velocity. As a consequence, I may write that $x=v\left(t-t_o\right)+x_o$ . Therefore, $$   m\,   \ddot{x}    =  - \left[V_2 - V_1\right]\delta\left(v\left(t-t_o\right)+x_o  \right).$$ Now, I integrate both sides with respect to time. I find $$ m\, \int    \ddot{x} \,dt   =  - \left[V_2 - V_1\right] \int   \delta\left(v\left(t-t_o\right)+x_0  \right)\,dt.$$ Upon integrating the left-hand side, and re-adjusting the right-hand side, I obtain $$ m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(v\,t-\left(v\,t_o-x_o\right)  \right)\,dt.$$ Now, I do a change of variables. Namely, that $s = v_o\,t$ . I find that $$ m\, \dot{x}  + k_1  =  - \left[V_2 - V_1\right] \int   \delta\left(s-\left(v_o\,t_o-x_o\right)  \right)\,\frac{ds}{v_o}.$$ Erroneously taking the integral on the right-hand side, I incorrectly find that $$ m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(s-\left(v_o\,t_o-x_o\right)  \right) + k_2 .$$ Now, since $s = v_o\,t$ ,  I have that $$ m\, \dot{x}  + k_1  =  - \frac{\left[V_2 - V_1\right]}{v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k_2 .$$ Ultimately, for an as yet unknown constant $k$ , I arrive at $$  \dot{x}(t)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right) + k  .$$ Upon satisfying the satisfy the boundary condition, $$ \dot{x}(t_o)    =  - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t -t_o\right) +x_o \right) + k = k = v_o .$$ So, I erroneously find that $$
\boxed{
 \dot{x}(t)    =  v_o - \frac{\left[V_2 - V_1\right]}{m\,v_o} H\left(v_o\left(t-t_o\right) +x_o \right)  .
}
$$ Remarks I am not satisfied with my approach. It seems unwieldy, and is erroneous. If you have a more direct approach to solve the problem, then please consider posting it.","['dirac-delta', 'ordinary-differential-equations', 'initial-value-problems', 'euler-lagrange-equation', 'physics']"
4532181,A precalculus solution for $x^6 - 3 x^4 + 2 x^3 + 3 x^2 - 3 x + 1 =0$,"Using algebra (precalculus) and suggest the solution method for the polynomial $$x^6 - 3 x^4 + 2 x^3 + 3 x^2 - 3 x + 1 =0$$ I'm solving problems on polynomials.  I'm stuck here. My attempts. First, I tried the Rational root theorem, then I failed.
Then I tried factorise the polynomial e.g. $(x^2+ax+b)(x^4+cx^3+dx^2+ex+f)$ , but I failed again. At the end I tried $$P(x)/x^3=x^3-3x+2+\frac 3x-3\frac {1}{x^2}+\frac {1}{x^3}=x^3+\frac {1}{x^3}-3\bigg(x-\frac 1x\bigg)-\frac {3}{x^2}+2=0$$ I failed again.","['irreducible-polynomials', 'algebra-precalculus', 'roots', 'polynomials']"
4532183,Convergence of Hilbert transform of a converging sequence,"Fix $n$ , and consider random variables $x_1, \dots, x_n$ whose joint p.d.f. is $p_n$ . Assume that the empirical distribution of $x_1, \dots,x_n$ converges weakly almost surely to the probability density function function $\rho(x)$ . Moreover, suppose that $x$ 's are bounded. Let $\hat{\rho}_n (y) = \frac{1}{n} \sum_{i =1}^n \delta(y - x_i)$ , which is a p.m.f., and is random itself. Then, $$
\hat{\rho}_n (y) \to \rho(y) \hspace{5pt} \text{weakly, almost surely under $p_n$}
$$ Define the Hilbert transform of $\rho$ as $H[\rho](z) = {\rm PV} \frac{1}{\pi} \int \frac{\rho(x)}{z-x} \, dx$ . How can I show that $\lim_{n \to \infty} \mathbb{E}\Big[ \frac{1}{n} \sum_{i=1}^n H[\rho](x_i) \Big] = \int H[\rho](x) \rho(x) \, dx$ ?
where the expectation is w.r.t. to $p_n$ .","['measure-theory', 'probability-theory', 'weak-convergence', 'cauchy-principal-value']"
4532205,Why is $\sin^2 \pi/(4n)\leq 1/n^2$?,"In Jeffrey Bub's Bananaworld $^1$ , there's a remark (p. 101) that $$\sin^2 \frac{\pi}{4n} \;\leq\; \frac{1}{n^2} \;,$$ where $n$ is any positive integer. It's been decades since I studied trig, although I have used it occasionally with the help of review and references.  An appendix to an earlier chapter in Bananaworld reviews some useful trigonometric identities, but I can't see how to derive the above statement from them.  I've scanned through the Wikipedia List of trigonometic identities to see whether I could see any obvious quick way of deriving Bub's claim, but I didn't succeed.  I am sure this isn't a difficult problem, but I don't see how to get a solution, and I'm not sure where to look without doing extensive study.  It's kind of an unusual trigonometric inequality, it seems to me, since one side is not trigonometic. Since the sine expression only applies to positive values less than or equal to $\pi/4$ , all I need to understand is why $$\sin \frac{\pi}{4n} \;\leq\; \frac{1}{n} \;.$$ For $n=1$ , it's obvious that $\sin \pi/4 \leq 1$ .  Similarly, for $n=2$ , I can just see by looking at a figure that $\sin \pi/8 \leq 1/2$ .  As $n$ increases, both $\sin \pi/(4n)$ and $1/n$ decrease, and when I calculate values for $\sin \pi/(4n)$ and $1/n$ for many values of $n$ , $\sin \pi/(4n)$ is always smaller.  However, I don't see how to prove that it must be smaller. That's as far as I've gotten.  It must not be very hard to show that Bub's inequality is correct, since he didn't even provide a hint as to why it holds.  The book requires a little bit of mathematical sophistication, but not a lot of specific mathematical knowledge beyond basic algebra.  I'm willing to do the work to figure out why the inequality holds if I have some idea where to start, so a hint might be all I need.  (A full proof is fine, too, if someone prefers to provide that.)  Thanks. $^1$ Bananaworld is a book on quantum entanglement.  The inequality is a small step in a short argument.",['trigonometry']
4532270,Equation for graph of constant helming,"Constant helming is (or was) a tactic used by ships in wartime to frustrate torpedo attacks by submarines.  It is a modified form of zigzagging.  It differs from zigzagging in that every time the helmsman completes a new zig, instead of bringing the rudder amidships, he maintains a slight angle on the rudder.  If an ordinary zigzag course results in a sine curve , then constant helming results in a modified sine curve that does not have a point of inflection at x=π.  Instead of inflecting at x=π, the curve continues to bend until the next zig, when it begins to curve in the opposite direction. So if zigzagging can be described by y=sin(x), then what equation describes constant helming?   I presume the constant angle maintained on the helm after reaching maxima and minima at x=π/2, 3π/2, etc., until the next zig, is a parameter in this equation.","['calculus', 'graphing-functions', 'trigonometry', 'parametric']"
4532292,Does $ln(x) +2$ composed with itself infinitely have a limit?,"I was bored and playing around with my calculator, and I decided to take a number, take its natural log, add 2, and then repeat. I tried numbers as small as 5 and numbers larger than a billion, and they all seemed to end the same way. The numbers basically stopped decreasing when they get to between 1 and 4, and then stay in that interval. If $f^{{\circ}n}(x)$ is $f(x)$ composed with itself $n$ times and $f(x)=ln(x)+2$ , then does this limit exist? $$\lim_{n\to{\infty}}f^{{\circ}n}(x)$$ And if it does, what happens if you replace 2 with an arbitrary constant? By the way, the last Calc class I completed was Calc 2, if you need to know what math level I'm at.","['limits', 'calculus']"
4532308,"Behaviour of $u_{n}=\frac{u_{n-1}+u_{n-2}+n}{\gcd\left(u_{n-1},u_{n-2},n\right)}$","Let $u_1=u_2=1$ and $$u_{n}=\frac{u_{n-1}+u_{n-2}+n}{\gcd\left(u_{n-1},u_{n-2},n\right)}$$ Then I suspect that $$\lim_{n\rightarrow\infty}\frac{u_{n+1}}{u_{n}}=\frac{1+\sqrt{5}}{2}$$ which is equivalent to the fact that $\gcd\left(u_{n-1},u_{n-2},n\right)=1$ for $n$ large enough. It seems that this property holds for any starting integer values $(u_1,u_2)$ . Does anyone see how to prove it? Proof of the equivalence. If $\lim_{n\rightarrow\infty}\frac{u_{n+1}}{u_{n}}=\frac{1+\sqrt{5}}{2}=\phi$ and since we have $\frac{n}{u_{n-1}}\rightarrow0$ then we get $$\frac{1+\frac{u_{n-2}}{u_{n-1}}+\frac{n}{u_{n-1}}}{\gcd\left(u_{n-1},u_{n-2},n\right)}	\sim\frac{1}{\gcd\left(u_{n-1},u_{n-2},n\right)}\left(1+\frac{1}{\varPhi}\right)\sim\varPhi\ \left(n\rightarrow\infty\right)$$ and necessarily we have $\gcd\left(u_{n-1},u_{n-2},n\right)\rightarrow1$ . Since it is an integer value it must be $1$ for $n$ large enough. In the other way if $\gcd\left(u_{n-1},u_{n-2},n\right)=1$ for $n>n_{0}$ then we have for $n>n_{0}$ $$u_{n}=u_{n-1}+u_{n-2}+n$$ and $u_{n}=a\varPhi^{n}+b\varPhi^{-n}+cn+d$ for some constants $(a,b,c,d)$ with $a>0$ . I add that if we replace $n$ with a function $f(n)$ sufficiently regular like $f(n)=n^2$ i.e. $u_{n}=\frac{u_{n-1}+u_{n-2}+f(n)}{\gcd\left(u_{n-1},u_{n-2},f(n)\right)}$ the property seems to persist. Thank you ps: here the values of $n$ such that $\gcd\left(u_{n},u_{n-1},n+1\right)>1$ and the corresponding gcd. No gcd>1 after $n=22933$ until $n=10^6$ (5,5),(6,2),(9,3),(12,2),(27,3),(48,3),(69,3),(90,3),(111,3),(135,3),(159,3),(165,3),(171,3),(177,3),(183,3),(204,3),(225,3),(249,3),(270,3),(273,7),(291,3),(300,5),(306,17),(327,3),(333,3),(357,21),(363,3),(369,3),(393,3),(399,3),(420,3),(426,3),(430,5),(447,3),(471,3),(495,3),(519,3),(525,3),(546,3),(570,3),(594,9),(618,3),(624,3),(645,3),(669,3),(690,3),(714,3),(738,3),(744,3),(750,15),(771,3),(777,3),(801,3),(807,3),(820,5),(834,3),(840,3),(864,3),(885,3),(891,3),(897,3),(903,3),(924,3),(945,3),(951,3),(960,5),(972,9),(993,3),(1017,3),(1038,3),(1059,3),(1075,5),(1080,3),(1095,5),(1116,3),(1122,3),(1143,9),(1167,3),(1191,3),(1212,3),(1236,3),(1242,3),(1248,3),(1260,5),(1278,3),(1280,5),(1358,7),(1428,7),(1444,19),(16781,173),(16800,3),(16806,3),(16808,11),(16814,7),(16830,15),(16836,3),(16857,3),(16863,3),(16870,5),(16890,3),(16911,3),(16932,3),(16953,3),(16977,3),(17001,3),(17007,3),(17028,33),(17034,3),(17050,5),(17055,3),(17061,3),(17067,3),(17073,3),(17079,3),(17103,3),(17109,3),(17130,3),(17136,9),(17160,3),(17166,3),(17175,5),(17178,7),(17187,3),(17195,5),(17208,3),(17220,5),(17221,17),(17241,3),(17262,3),(17286,3),(17300,5),(17304,3),(17325,3),(17331,3),(17355,3),(17376,3),(17400,3),(17424,3),(17445,3),(17466,3),(17487,3),(17511,3),(17532,3),(17538,3),(17544,3),(17550,3),(17556,3),(17562,3),(17586,9),(17610,3),(17616,3),(17622,3),(17628,3),(17633,7),(17652,3),(17673,3),(17685,5),(17697,3),(17718,3),(17725,5),(17733,3),(17754,3),(17766,7),(17775,3),(17790,5),(17810,5),(17811,3),(17835,3),(17856,3),(17862,3),(17868,3),(17892,3),(17898,3),(17919,3),(17940,3),(17961,3),(17967,3),(17988,3),(17994,3),(18015,3),(18039,3),(18063,3),(18081,7),(18084,3),(18108,3),(18125,5),(18132,3),(18138,3),(18162,3),(18164,19),(18168,3),(18189,3),(18195,3),(18201,3),(18225,3),(18249,3),(18273,3),(18279,3),(18300,3),(18321,3),(18340,7),(18342,3),(18348,3),(18354,3),(18368,7),(18378,3),(18402,3),(18408,3),(18414,3),(18435,3),(18441,3),(18465,3),(18471,3),(18480,5),(18492,3),(18498,3),(18515,5),(18522,3),(18528,3),(18535,5),(18555,3),(18561,3),(18565,5),(18582,3),(18606,3),(18615,5),(18618,3),(18642,3),(18648,3),(18654,3),(18660,3),(18681,3),(18702,3),(18723,3),(18729,3),(18750,3),(18774,9),(18798,3),(18810,5),(18828,3),(18849,3),(18870,3),(18876,3),(18882,3),(18906,3),(18912,3),(18918,3),(18924,3),(18928,7),(18948,3),(18972,3),(18978,3),(18999,3),(19020,3),(19026,9),(19032,3),(19038,3),(19059,3),(19080,3),(19086,3),(19103,7),(19110,3),(19115,5),(19128,3),(19134,3),(19140,3),(19164,3),(19170,3),(19176,3),(19182,3),(19206,9),(19227,3),(19248,3),(19254,3),(19275,3),(19285,7),(19296,3),(19320,3),(19341,3),(19347,3),(19353,3),(19359,3),(19360,5),(19365,3),(19386,3),(19392,3),(19413,3),(19419,3),(19425,3),(19446,3),(19467,3),(19473,3),(19479,3),(19485,3),(19506,3),(19515,5),(19527,3),(19548,3),(19558,7),(19569,3),(19575,3),(19596,3),(19602,3),(19626,3),(19635,11),(19647,3),(19653,3),(19659,3),(19665,3),(19669,13),(19671,3),(19677,3),(19698,3),(19715,5),(19725,3),(19746,3),(19752,3),(19776,3),(19782,3),(19788,3),(19809,3),(19830,3),(19851,3),(19872,3),(19896,3),(19920,3),(19935,5),(19953,3),(19959,3),(19965,5),(19968,3),(19989,3),(20013,3),(20034,3),(20058,3),(20064,3),(20085,3),(20106,3),(20125,5),(20127,3),(20148,3),(20154,3),(20175,3),(20199,3),(20220,3),(20244,3),(20268,3),(20274,3),(20295,3),(20316,3),(20322,3),(20328,3),(20334,3),(20355,3),(20376,3),(20382,3),(20390,5),(20406,3),(20412,3),(20418,3),(20442,3),(20460,11),(20481,3),(20487,3),(20493,3),(20499,3),(20523,3),(20544,3),(20550,3),(20571,3),(20577,3),(20583,3),(20604,3),(20610,3),(20634,3),(20658,3),(20679,3),(20700,3),(20721,3),(20727,3),(20748,3),(20769,3),(20790,9),(20796,3),(20817,3),(20838,3),(20846,7),(20862,3),(20883,3),(20907,3),(20913,3),(20919,3),(20925,5),(20928,3),(20952,3),(20958,3),(20982,3),(21006,3),(21020,5),(21027,3),(21028,7),(21035,5),(21048,3),(21054,3),(21060,3),(21084,21),(21105,3),(21129,3),(21153,3),(21159,3),(21165,3),(21171,3),(21192,3),(21213,3),(21230,5),(21237,3),(21243,3),(21264,3),(21270,15),(21291,3),(21312,3),(21333,3),(21339,9),(21345,3),(21351,3),(21357,3),(21378,3),(21399,21),(21420,3),(21425,5),(21438,3),(21462,3),(21476,7),(21483,3),(21500,5),(21510,3),(21534,3),(21555,3),(21576,3),(21595,7),(21597,3),(21621,3),(21630,7),(21642,3),(21648,3),(21654,3),(21660,3),(21665,5),(21666,3),(21687,3),(21708,3),(21714,3),(21720,3),(21725,5),(21735,23),(21750,3),(21756,3),(21762,3),(21783,3),(21807,3),(21828,3),(21852,3),(21876,3),(21880,5),(21900,3),(21924,3),(21930,3),(21945,11),(21963,3),(21984,3),(21990,3),(22005,5),(22026,3),(22047,3),(22071,3),(22077,3),(22083,3),(22089,3),(22100,5),(22104,3),(22125,3),(22146,3),(22170,3),(22180,5),(22191,3),(22197,3),(22203,3),(22209,3),(22220,5),(22224,3),(22245,3),(22269,3),(22280,5),(22284,3),(22305,3),(22326,3),(22332,3),(22341,11),(22353,3),(22359,3),(22365,9),(22386,3),(22407,3),(22417,29),(22428,9),(22434,3),(22435,7),(22455,3),(22461,3),(22467,3),(22473,3),(22494,3),(22500,3),(22521,3),(22527,3),(22535,5),(22548,3),(22554,7),(22572,3),(22593,3),(22603,7),(22614,3),(22638,3),(22659,3),(22680,3),(22686,3),(22710,3),(22734,3),(22755,3),(22765,5),(22776,3),(22797,3),(22821,3),(22827,7),(22835,5),(22842,3),(22848,3),(22855,5),(22875,3),(22880,11),(22932,7)","['elementary-number-theory', 'sequences-and-series']"
4532312,$U_{n} + \lfloor \sqrt{U_{n}} \rfloor = k^{2}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . This post was edited and submitted for review last year and failed to reopen the post: Original close reason(s) were not resolved Improve this question I've been stuck, I tried to prove it by extracting the explicit formula but i got nowhere.","['ceiling-and-floor-functions', 'analysis', 'sequences-and-series']"
4532322,A question about following ODE.,"I was reading ""Ricci flow, an introduction"" by B. Chow and D. Knopf and I was stuck by the equation on Page 13. As below, $x,y,z>0$ satisfying \begin{equation}
\begin{array}{ll}
\dfrac{\mathrm{d}x}{\mathrm{d}t} \!\!\!\!\!&=-8+4\dfrac{y^2+z^2-x^2}{yz},\\
\dfrac{\mathrm{d}y}{\mathrm{d}t} \!\!\!\!\!&=-8+4\dfrac{x^2+z^2-y^2}{xz},\\
\dfrac{\mathrm{d}z}{\mathrm{d}t} \!\!\!\!\!&=-8+4\dfrac{x^2+y^2-z^2}{xy}.
\end{array}
\end{equation} We assume that $x(0)\geqslant y(0)\geqslant z(0)$ , then we can calculate that \begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}(x-z)=4(x-z)\frac{y^2-(x+z)^2}{xyz}.
\tag{$\ast$}
\end{equation} Then the author conclude that $x(t)\geqslant y(t)\geqslant z(t)$ persist for as long as the solution exists. Here is my question : how can we judge $x(t)\geqslant y(t)\geqslant z(t)$ from $(\ast)$ ? My attempt is \begin{equation}
\frac{\mathrm{d}}{\mathrm{d}t}(\mathrm{ln}|x-z|)=4\frac{y^2-(x+z)^2}{xyz}.
\end{equation} But then what should I do?","['analysis', 'ordinary-differential-equations']"
4532352,Finite moment generating function near zero but not subexponential,"A centered random variable $X$ with moment generating function (MGF) $M_X(t) := \text E[e^{tX}]$ is subexponential if $\log M_X(t) \leq ct^2$ for some $c > 0$ on some neighborhood $(-\delta, \delta)$ of zero. Are there any examples of random variables with finite MGFs in some neighborhood of zero that are not subexponential? Relatedly, how quickly can the MGF grow near zero while still being finite, and, for a given growth rate, how to find a random variable with that MGF?","['moment-generating-functions', 'probability-distributions', 'probability-theory', 'real-analysis']"
4532364,Conjecture about product of lengths in a circle,"In a circle of radius $r>1$ , point $P$ is a distance $\sqrt{r^2-1}$ from the centre of the circle. From $P$ , draw $n$ line segments to the circle, such that the angles between neighboring line segments are equal. (There is more than one way to do this, because all the line segments can be rotated together; any of these ways will do.) Here is an example with $r=\sqrt2$ and $n=12$ . Call the lengths of the line segments $l_1, l_2, l_3, ..., l_n$ . I am trying to prove (or disprove) the following conjecture: $\lim\limits_{n\to\infty}\prod\limits_{k=1}^n l_k=1$ My attempt If $n$ is even, then consider an arbitrary chord $AB$ through $P$ , and diameter $CD$ through $P$ . Using the intersecting chords theorem , we have $$(AP)(PB)=(CP)(PD)=(r-\sqrt{r^2-1})(r+\sqrt{r^2-1})=1$$ $$\therefore\prod\limits_{k=1}^n l_k=1$$ $$\therefore \lim\limits_{n\to\infty}\prod\limits_{k=1}^n l_k=1$$ If $n$ is odd, then we can easily verify that $\prod\limits_{k=1}^n l_k$ does not always equal $1$ . For example, let $r=\sqrt2$ and $n=3$ , and let one of the line segments go through the centre of the circle. We can easily calculate that $\prod\limits_{k=1}^n l_k=\frac{1}{2}(3+3\sqrt2-\sqrt5-\sqrt{10})\approx 0.922$ . But since the product with even $n$ always equals $1$ (with or without taking the limit), it seems plausible that the product with odd $n$ approaches $1$ as $n\to\infty$ . But I have not found a convincing way to prove this. (This question was inspired by this related question and @person's answer.)","['infinite-product', 'circles', 'geometry']"
4532407,Is there always a general formula for the linear transformation that maps polynomials to one of their antiderivatives?,"In Friedberg's Linear Algebra this example of a linear transformation comes up: $T:P_2(R) \rightarrow P_3(R)$ , such that $T(p)=q$ , with $q(x)=\int_0^x{p}$ , where $P_n(R)$ is the vector space of all polynomials over the reals of degree up to $n$ . At first my intuition suggested that, given any of a polynomial's antiderivatives, there should exist a linear transformation that maps that polynomial to that specific antiderivative, and that that would be accomplished by suitably varying the integral's lower bound in the preceding expression. However, consider the polynomial $f$ such that $f(x)=x$ , and take its integral for an arbitrary lower bound $C$ : $$g_C(x)=\int_C^x f=\frac{x^2}{2}-\frac{C^2}{2}$$ In other words, $g_C$ is of the form $g_C(x)=\frac{x^2}{2}-D$ , for $D\geq0$ . So no matter what lower bound we choose, we'll never obtain an antiderivative for which $g_c(0)>0$ , which seems quite surprising. Despite this, nothing stops us from defining a linear transformation $U$ such that $U(f)=h$ , with $f(x)=x$ and $h(x)=\frac{x^2}{2}+1$ , and analogously for the other basis vectors, whichever basis we might be using. I wasn't able to, however, find a general expression written in terms of basic operations and integrals that could express what this linear transformation does, as can be done for $T$ , which, again, seems to me quite surprising. Put informally, why should certain antiderivatives be ""privileged"" with respect to others in the realm of linear transformations and their expressions? Or am I just not seeing what this general expression should be? Is there one?","['integration', 'linear-algebra', 'linear-transformations']"
4532477,"prove that if m is even, $m=n$ iff there exists a function $g:A \to A$ so that $g(g(a)) = f(a)$ for all a in A","Let $A = \{1,2,\cdots, m+n\}$ , where m and n are positive integers and let the function $f:A \to A$ be defined by $f(i)=i+1$ for $i \in A\backslash \{m,m+n\}, f(m)=1, f(m+n)=m+1$ . Prove that if m is even, $m=n$ if and only if there exists a function $g:A \to A$ so that $g(g(a))=f(a)\,\forall a\in A.$ For the only if direction, note that the function $g(i) = m+i$ for $1\leq i\leq m, g(m+i)=i+1$ for $1\leq i\leq m-1, g(2m)=1$ works. For the converse, let $M=\{1,2,\cdots, m\}$ and observe that $f$ acts the same way on M as the cycle $(1\,2\,\cdots\,m).$ Also, note that for any $i\in M,$ there exists a smallest $j\ge 1$ so that $f^j(i)=i$ , where $f^j$ denotes the j fold composition of f with itself. Indeed, in the sequence $i,f(i),f^2(i),\cdots $ there is a repeated element. Since the sequence lies in M, we can find indices $j < k$ so that $f^j(i) = f^k(i)\Rightarrow f^{k-j}(i) = i$ since f is injective (we only need the fact that f is injective on M since f maps M to M). Also, it seems that $g(M)\cap M = g(A\backslash M)\cap (A\backslash M)=\emptyset$ but I'm not sure how to prove this. If I can show that $A\backslash M $ has the same cardinality as M, I'm done. For instance, from the injectivity of g and the above claims, since $g$ maps $M$ to $A\backslash M, |M|\leq |A\backslash M|$ and similarly $|A\backslash M|\leq |M|$ .","['contest-math', 'functional-equations', 'combinatorics', 'discrete-mathematics']"
4532505,Minimum Probability of Intersection of 3 events theory,"Say you have $P(A) = P(B) = P(C) = 0.9$ , but they are not necessarily independent events. What is the minimum probability of $P(A ∩ B ∩ C)$ ? For example, we know that $P(A ∪ B) = P(A) + P(B) - P(A ∩ B) \leq 1$ , so $P(A ∩ B) \geq P(A) + P(B) - 1 = 0.9 + 0.9 -1 = 0.8$ So the min probability of $P(A ∩ B)$ is $0.8$ I can only find the maximum probability being $0.8$ because $P(A ∩ B ∩ C) \leq P(A ∩ B), P(B ∩ C), P(A ∩ C) = 0.8$ The working I've done so far: $0 \leq P(A ∪ B ∪  C) \leq1$ $0 \leq P(A) + P(B) +P(C) - P(A ∩ B) - P(A ∩ C) - P(B ∩ C) + P(A∩B∩C) \leq 1$ $-2.7 \leq P(A ∩ B ∩ C) - P(A ∩ B) -  P(A ∩ C) -  P(B ∩ C) \leq -1.7$ $1.7 \leq P(A ∩ B) +  P(A ∩ C) + P(B ∩ C) - P(A ∩ B ∩ C) \leq 2.7$ $P(A ∩ B ∩ C) + 2.7 \geq P(A ∩ B) +  P(A ∩ C) + P(B ∩ C)$ $P(A ∩ B ∩ C) \geq P(A ∩ B) +  P(A ∩ C) + P(B ∩ C) - 2.7$ Since, $0.8\leq P(A ∩ B)\leq 0.9$ $2.4 \leq P(A ∩ B) +  P(A ∩ C) + P(B ∩ C) \leq 2.7$ So $P(A ∩ B ∩ C) >= 2.4 - 2.7$ ? or $2.7 - 2.7$ ? Which is just $0$ ?","['conditional-probability', 'optimization', 'statistics', 'probability']"
4532538,What is an easy way to understand the Intermediate value theorem?,"Let $f(x) = \sqrt{x^2-4}$ , Use Intermediate value theorem to show that $f(x) = 5\sin x$ has a solution for $2<x<3$ , The never truly understand the theorem fully, so I am working on examples to see if makes sense. so, $\sqrt{x^2-4} = 5\sin x$ , $g(x) =\sqrt{x^2-4} - 5\sin x = 0$ $g(2) = \sqrt{2^2 -4} - 5\sin (2) = -4.546 (<0)$ $g(3) = \sqrt{3^2-4} - 5\sin(4) = 1.5304 (>0)$ I am just following step by step from what I learned how to solve it, but I never knew the meaning of doing it. Example, Why must we find $g(2), g(3)$ ? and also $g(2)= -4.546 (<0)$ what does this values and $<0$ really mean? And from here, I need to apply the theorem to get to a conclusion which Im unclear of as I don't really understand the theorem","['limits', 'functions']"
4532543,"$G = HN$ a group, $N$ normal and $N \cap H =1$. If the conjugacy of $H$ on $N$ has a orbit with all of the non trivial elements: $|N|$ is a prime","Let $G$ be a finite group, $H, N \leqslant G $ with $N$ normal in $G$ such that $G=HN$ and $N \cap H =\{1\}$ . Suppose that all of the non-trivial elements of $N$ are in a single orbit for the conjugacy action of $H$ on $N$ , then: exists a prime $p$ such that $\forall x \in N$ , $x \neq 1$ we have $|x|=p$ . So I can deduce that the conjugacy action of $H$ on $N$ has two orbits: $\{1\}$ and $N \setminus \{1\}$ and if $x \in N \setminus \{1\}$ , for the orbit-stabilizer theorem we have that $|N|-1 = [H:H_x]$ , so $|N|-1$ divides the order of $H$ , hence $|G| = |N|(|N|-1)k$ . Now, to show that $|x| = p\,\,$ , I can imagine that the strategy would be to show that $|N|$ is the prime $p$ . Any hint will be appreciated since I am stuck here, thank you.","['finite-groups', 'abstract-algebra', 'normal-subgroups', 'group-theory', 'group-actions']"
4532550,What values of $a$ make $x=a^x$ solvable?,"I was wondering what the solution to $x=e^x$ was, but then I graphed $y=x$ and $y=e^x$ and saw that they didn't intersect. I assume they wouldn't intersect for a base greater than $e$ either. So, I wanted to know what values of $a$ would give the equation $x=a^x$ a solution. The first thing you notice is that $x>0$ because $x=a^x>0$ . Also, the values $a\in(0,1)$ should work based on the shape of exponential decay (and $a=1$ of course), but the challenge is for $a>1$ . The conclusion I reached was that the rest of the values for $a$ should be inside $(1,e)$ . My reasoning uses the fact that $x>\ln(x)$ on the domain of $\ln(x)$ . If you rearrange $x=a^x$ you get $$x=a^x$$ $$\ln(x)=\ln(a^x)$$ $$\ln(x)=x\ln(a)$$ $$\frac{\ln(x)}{\ln(a)}=x$$ In order for this equation to be unsolvable, the function on the left must never intersect the function on the right, so one must always be greater. Because $\ln(x)$ grows more slowly, x would have to be the greater function. This leads to the inequality $$x>\frac{\ln(x)}{\ln(a)}$$ $$x\ln(a)>\ln(x)$$ This inequality is automatically true if $x\ln(a)>x$ . Dividing on both sides gives $ln(a)>1\rightarrow a>e$ . Therefore when $a>e$ the equation can't be solved. But what subinterval of $(1,e)$ contains the a-values I'm looking for?",['functions']
4532572,Derivation of the behavior of solutions to $\tan x = x$,"This question is related to Chapter IV, Note IV.36 of Flajolet & Sedgewick's Analytic Combinatorics, and The question : Sum of the squares of the reciprocals of the fixed points of the tangent function as well: Let $x_k$ be the $k^{th}$ positive root of the equation $\tan z = z$ .  Then, the sum $S_r = \sum_k x_k^{-2r}$ are rational numbers for $r \ge 1$ .  For instances, $S_1 = 1/10$ , $S_2 = 1/350$ , $S_3 = 1/7875$ (from Note IV.36, pp. 269.) I follow the approach in the book for Bernoulli numbers (Chapter IV, IV.6.1, pp. 268.)  Consider the function: \begin{align}
f(z) = \frac{1}{\tan z - z} \tag{1}
\end{align} It is obvious that $\{x_k\}$ are poles of $f(z)$ , with $k \in \mathbb{Z} \backslash \{0\}$ .  Further, \begin{align}
Res[f(z);z=x_k] &= \frac{1}{\frac{d}{dz}(\tan z - z)|_{z=x_k}} \\
&= \frac{1}{\tan^2 x_k} \\
&= \frac{1}{x_k^2} \tag{2}
\end{align} Hence, \begin{align}
\frac{1}{\tan z - z} \sim \frac{1}{x_k^2}\cdot \frac{1}{z-x_k}, \text{ for } z \to x_k \tag{3}
\end{align} By the Cauchy's coefficient formula, we have \begin{align}
f_n = [z^n]f(z) &= \int_C \frac{f(z)}{z^{n+1}} dz \tag{4} \\
&= -\sum_{k \in \mathbb{Z} \backslash \{0\}} \frac{1}{x_k^2}\cdot \frac{1}{x_k^{n+1}}
\end{align} where $C$ is a contour encircling all poles.  Since $f(z)$ is an odd function, the poles are in pairs like $\pm x_k$ .  Then, $f_n = 0$ , when $n$ is even.  As a result, \begin{align}
f_{2n-1} &= -2 \sum_{k=1}^\infty x_k^{-2(n+1)}, \text{ for } n \ge 1 \tag{5}
\end{align} Therefore, the following relation is established: \begin{align}
S_r = \sum_{k=1}^\infty x_k^{-2r} = -\frac{1}{2}f_{2r-3}, \text{ for } r > 1 \tag{6}
\end{align} From the expansion of (1), that is, \begin{align}
\frac{1}{\tan z - z} = \frac{3}{z^3} - \frac{6}{5z} - \frac{1}{175}z - \frac{2}{7875}z^3-\frac{37}{3031875}z^5+O(z^6) \tag{7}
\end{align} It shows that $S_2$ and $S_3$ calculated from (6) are correct.  But $S_1$ cannot be calculated from (6) since $f_{-1}$ is related to the pole at $z=0$ , which is excluded from (6).
Then, for $S_1$ , I follow the approach in an answer to The question to consider the integral in the region including $z=0$ .  That is, \begin{align}
\int_C f(z)dz = 2i\pi \sum_{k \in \mathbb{Z}} Res[f(z); z=x_k] \tag{8}
\end{align} From the expansion in (7), we have $Res[f(z);z=0]=-\frac{6}{5}$ .  Together with (2), (8) becomes \begin{align}
\int_C f(z)dz = 2i\pi \left(-\frac{6}{5} + 2\sum_{k=1}^\infty x_k^{-2}\right) \tag{9}
\end{align} Since it is found that (details in an answer to The question ) \begin{align}
\int_C f(z)dz = -2i\pi \tag{10}
\end{align} (9) becomes \begin{align}
-\frac{6}{5} + 2\sum_{k=1}^\infty x_k^{-2} = -1
\end{align} Or \begin{align}
S_1 = \sum_{k=1}^\infty x_k^{-2} = \frac{1}{10} \tag{11}
\end{align} This result is well known as proved in The question . It looks the flow of logic leading to the results (6) and (11) are both correct.  However, the point I don't understand is: both (4) and (9) include an integral around the same contour and both give the same result ( $-2i\pi$ ).  Nevertheless, by Cauchy's residue theorem, they equal to the sum of different sets of residues - one with the pole at $0$ (9) while the other without (4).  The difference in residue set is due to the difference in the regions in concern ( $\mathbb{C}$ vs. $\mathbb{C} \backslash \{0\}$ .)  But why the contour integrals of a function at these different regions give the same result? Could you please indicate what is wrong in the above logic?  Thank you.","['residue-calculus', 'asymptotics', 'complex-analysis', 'power-series', 'complex-integration']"
4532616,Strengthened form of weak convergence,"Let $(\Omega,\mathcal{F},\mathbb{P})$ be a probability space, $\mathcal{G} \subseteq \mathcal{F}$ be a subalgebra, $X$ be a real-valued random variable and $X_1,X_2,..$ be $\mathcal{G}$ -measurable random variables s.t. $$
\mathbb{E}[Uf(X_n)] \rightarrow \mathbb{E}[Uf(X)]
$$ holds for all continuous bounded $f:\mathbb{R} \rightarrow \mathbb{R}$ and $\mathcal{G}$ -measurable bounded $U:\Omega \rightarrow \mathbb{R}$ . Does this imply $$
\mathbb{E}[Uf(X_n)] \rightarrow \mathbb{E}[Uf(X)]
$$ for all continuous bounded $f:\mathbb{R} \rightarrow \mathbb{R}$ and $\mathcal{F}$ -measurable bounded $U:\Omega \rightarrow \mathbb{R}$ ? It would be very useful for me if this holds, but I can find neither a proof nor a counter example. I would be greatful for a help.","['weak-convergence', 'conditional-probability', 'probability-theory', 'probability', 'random-variables']"
4532621,"Finding the total derivative of $g(x)=(x,f(x))$ step by step.","Suppose that $f:\mathbb R^n\to \mathbb R^k$ is differentiable. Define $g:\mathbb R^n\to \mathbb R^{n+k}$ as $g(x)=(x, f(x))$ . Then, the total derivative of $f$ at some $x\in \mathbb R^n$ is to be determined. I think the question above needs a little explanation. $x\in \mathbb R^n$ is an $n-$ tuple and therefore looks like $(x_1,x_2,...,x_n)$ and similarly for $f(x)$ so $g(x)=((x_1,...,x_n), (f_1(x),...,f_k(x))\in \mathbb R^n\times \mathbb R^k.$ But here in order that the question makes sense, let's identify $\mathbb R^n\times \mathbb R^k$ by $\mathbb R^{n+k}$ (I actually don't understand why it can be done. I do understand that $\mathbb R^{n+k}$ is isomorphic to $\mathbb R^n\times \mathbb R^k$ as vector spaces but that doesn't seem the reason for such replacement. One possible reason could be: norms on a finite dimensional vector spaces are equivalent. But here we have two isomorphic vector spaces so I don't understand how to see the validity of this step. So let me take such replacement for granted for now.). Let $f(x)=(f_1(x),f_2(x),...,f_k(x))$ , where $f_i:\mathbb R^n\to \mathbb R$ . $g$ is differentiable as each of its coordinate map is differentiable. So $g(x)=(x_1,x_2,...,x_n, f_1(x),f_2(x),...f_k(x))$ . Writing Jacobian matrix for $g$ at $x$ (it will be an $(n+k)\times n$ matrix) gives: $\begin{bmatrix}1 &0&0&\cdots &0\\
0&1&0&\cdots&0\\
\cdots&\cdots&\cdots & &\cdots\\
\cdots&\cdots&\cdots & &\cdots\\
 0&0&0&\cdots &1\\
\frac{\partial f_1(x)}{\partial x_1}&\frac{\partial f_1(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_1(x)}{\partial x_n}\\
\frac{\partial f_2(x)}{\partial x_1}&\frac{\partial f_2(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_2(x)}{\partial x_n}\\
\cdots&\cdots&\cdots & &\cdots\\
\cdots&\cdots&\cdots & &\cdots\\
\frac{\partial f_k(x)}{\partial x_1}&\frac{\partial f_k(x)}{\partial x_2}&\cdots&\cdots& \frac{\partial f_k(x)}{\partial x_n}\end{bmatrix}$ But this does not look correct as it has $f_i$ 's, which I created for convenience and they were not given in the original problem. So how do I get the correct total derivative? I'm having difficulty understanding this subject.","['multivariable-calculus', 'calculus', 'differential-topology', 'real-analysis']"
4532669,Lévy's continuity theorem for moment generating functions?,"Is there theorem similar to Lévy's continuity theorem , but considering moment generating functions (mgf) instead of characteristic functions? For example, suppose $M_n(t) = \mathbb E e^{tX_n}$ are mgf of $X_n$ , which are all finite on $(-\varepsilon, \varepsilon)$ for some fixed $\varepsilon > 0$ . Suppose the $\lim_{n\to\infty}M_n(t) = M(t)$ exists for all $t \in (-\varepsilon, \varepsilon)$ , and $M(t)$ is continuous. Could we conclude that $M$ is mgf, and $X_n$ converge in distribution to $X$ with distribution characterized by $M$ ? Moment generating functions and characteristic functions are quite similar in definition. On top of that, once they both exist, each of them determines distribution. It seems reasonable to have analogue theorem for mgf.","['probability-theory', 'weak-convergence']"
4532676,Proving $E(X)=0 \Rightarrow P(X=0)$,"Let $X$ be a random variable that takes on values in $[0,\infty]$ . I
want to show that $E(X)=0$ implies $P(X=0)=1$ . I came up with an attempt presupposing that $X$ is discrete. We have: $$\mathbb{E}(X)=\sum \limits_{\omega\in [0,\infty]}X(\omega)P(\{\omega\})=\sum \limits_{k=0}^{\infty}kP(X=k)=0\cdot P(X=0)+\underbrace{1\cdot P(X=1)+2\cdot P(X=2)+\cdots }_{=0}=0$$ And on the other hand $\sum \limits_{k=0}^{\infty}P(X=k)=1$ , this means $P(X=0)$ . Is this correct? And what about the continuous case?","['expected-value', 'probability-theory', 'probability']"
4532680,Proving using Mathematical Induction from my discrete math class [duplicate],"This question already has answers here : Prove that for every positive integer $n$, $1/1^2+1/2^2+1/3^2+\cdots+1/n^2\le2-1/n$ (4 answers) Closed 1 year ago . This is a practice exercise from our class about proving inequalities using mathematical induction. I've been stuck on the last step for quite a while now. This is the Question. ""Prove that $\sum_{k=1}^n\frac{1}{k^2}=\frac{1}{1^2}+\frac{1}{2^2}+\cdots+\frac{1}{n^2}\le 2-\frac{1}{n}$ whenever $n$ is a positive integer."" This is my attempt to prove it. Step 1: Base case $(n=1)$ $\sum_{k=1}^1\frac{1}{(1)^2}\le2-\frac{1}{1}$ $1\le1$ Step 2: Induction hypothesis Assume that $\sum_{k=1}^n\frac{1}{k^2}\le 2-\frac{1}{n}$ is true for $n=c$ Then, $\sum_{k=1}^c\frac{1}{k^2}\le 2-\frac{1}{c}$ Step 3: prove that it is true for $n=c+1$ $\sum_{k=1}^{c+1}\frac{1}{k^2}$ $\le$ $2-\frac{1}{c}+\frac{1}{(c+1)^2}$ $\le 2+\frac{-(c+1)^2+c}{c(c+1)^2}$ $\le 2+\frac{-(c^2+2c+1)+c}{c(c+1)^2}$ $\le 2+\frac{-c^2-2c-1+c}{c(c+1)^2}$ $\le 2-\frac{c^2+c+1}{c(c+1)^2}$ $\le 2-\frac{c^2+c}{c(c+1)^2}-\frac{1}{c(c+1)^2}$ $\le 2-\frac{c(c+1)}{c(c+1)^2}-\frac{1}{c(c+1)^2}$ $\le 2-\frac{1}{c+1}-\frac{1}{c(c+1)^2}$ I'm Stuck on this step.","['algebra-precalculus', 'induction']"
4532753,Six-sided and four-sided dice question contradiction,"Suppose we have two fair dice, with six sides (numbered 1 to 6) and with four sides (numbered 1 to 4). Suppose we pick a die at random and throw it, the result is announced to be 2 and the die is discarded. Then we pick the remaining die and throw it. What is the expectation of the second throw? On the one hand, us getting 2 on the first throw, does not provide us with information on whether the first die was six-sided or four-sided, so we could argue that by total expectation the expectation of the second throw is just the average of expectations: $$
E(X)=(3.5)\frac{1}{2} + (2.5)\frac{1}{2}=3
$$ On the other hand, we know that we got 2 on the first throw, so the expectation of the second throw is: $$
E(X)=(1)\frac{2}{9} + (2)\frac{1}{9} + (3)\frac{2}{9} + (4)\frac{2}{9} + (5)\frac{1}{9} + (6)\frac{1}{9}=3.22
$$ It seems to me that there is a contradiction?","['dice', 'probability']"
4532759,Evaluating Surface Integral with Divergence Theorem,"Evaluate $\displaystyle\int_S \mathbf{F\cdot n}\ dS$ over the entire surface of the region above the $xy$ plane bounded by the cone $z^2=x^2+y^2$ and the plane $z=4$ if $\mathbf F=x\hat i+y\hat j+z^2\hat k$ . Solution: By the divergence theorem, since $\nabla \cdot\mathbf F=2+2z$ , we have that $$\begin{align}
\int_S \mathbf{F\cdot n}\ dS&=\int_A \int_{z=\sqrt{x^2+y^2}=r} ^4 (2+2z)\ dz\ dx\ dy\\
&=\iint_A [2z+z^2]_r ^4 \ dx\ dy\\ 
&=\int_\theta^{2\pi} \int_{r=0} ^{4} (24-2r-r^2)r\ dr\ d\theta\\
&=\int_{\theta=0}^{2\pi}\left[24\frac{r^2}{2}-2\frac{r^2}{3}-\frac{r^4}{4}\right]_0^4\\
&=\int^{2\pi}_0 \frac {256}{3}\ d\theta\\
&=\frac{512}{3} \pi
\end{align}$$ The given answer is $\dfrac{ 128\pi}{3}$ . Where did I mistake, exactly?","['divergence-theorem', 'multivariable-calculus', 'surface-integrals', 'vector-analysis']"
4532802,Probability that notes are in the same scale,"What is the probability that a random sequence of notes (on the 12-note chromatic scale) of length n is in the same major scale? Quite some time ago, I came across a song written by converting pi to base-12. (The song didn’t go on forever, obviously, but it was really cool.) The guy who wrote the song claimed that pi is a “musical number”. I wanted to make it a project to figure out whether pi is truly a musical number. That is, if pi in base-12 begins with a string of digits that, when converted to musical notes, stay in the same scale longer than statistically expected. This is a big topic depending on what types of scale(s) one could choose and what key transitions would be allowable, but I thought major would be a good place to start. Haven’t gotten very far mathematically, mainly because I can’t figure out exactly what to do or whether this is an easy or difficult problem. Edit: Since I saw this in the comments—which scale is not specified in advance. So you have the notes, and then you see if they fit ANY scale.","['music-theory', 'pi', 'probability']"
4532807,"For the function $y=\frac{\sqrt{x}}{x-e^x}$, prove that there is one horizontal asymptote at $y=0$","$$y=\frac{\sqrt{x}}{x-e^x}$$ Proving vertical asymptotes are easy but when it comes to proving horizontal or oblique ones, I often get stuck. This function seems to intersect its asymptote at the origin so substituting in $0$ to show an error does not work. So how does one prove these kinds of things, is there a common way that people use? Right now, I am suspecting something to do with limits(correct me if I am wrong) but have no idea how to progress with that way.","['limits', 'algebra-precalculus', 'functions']"
4532842,What is the inverse limit of algebraic stacks?,"Let $S$ be a base scheme. Let $(F_{i},f_{ii'})_{i\in I}$ be a directed inverse system of algebraic spaces over $S$ . Then if each $f_{ii'}$ is affine, the inverse limit $\lim_{i}F_{i}$ exists as an algebraic space. Let $({\cal{X}}_{i},g_{ii'})_{i\in I}$ be a directed inverse system of algebraic stacks over $S$ . Then as the case of algebraic spaces, is the inverse limit $\lim_{i}\cal{X}_{i}$ exists as an algebraic stack? If not, when $\lim_{i}\cal{X}_{i}$ is an algebraic stack? In fact, it seems that there is no reference concerning the inverse limit $\lim_{i}\cal{X}_{i}$ of algebraic stacks. I have no idea what is the definition of the limit $\lim_{i}\cal{X}_{i}$ . Is it simply a small limit in $Cat$ ? This is what I found in Xinwen Zhu's paper (arXiv:1707.05700v1).","['algebraic-stacks', 'algebraic-geometry']"
4532871,What is largest possible sum of all interoir angles of a triangle on a torus?,"As discussed in the question Triangles on a Torus , the outside of a torus has positive Gaussian curvature. A triangle inscribed on the outer surface will have interior angles which add to be greater than 180 degrees. By adjusting the geometry of the Torus, and the points which define the triangle, what is the maximum possible sum of interior angles of the triangle? I suspect the fringe case occurs with a horn torus with vertices that lie on the axis between positive and negative curvature. Edit: “Inside of the Torus” refers to the side of the torus facing inward “toward the donut hole.” Alternatively, it is simply the surface of the torus with negative Gaussian curvature. Edit 2: I’m referring to one single triangle drawn anywhere on the surface of the torus, not breaking the surface into a number of smaller triangles","['surfaces', 'curvature', 'differential-geometry']"
4532885,Proof of a neat pattern in polynomials,"Let $f_1:\mathbb{R}\to\mathbb{R}$ such that $$f(x) = ax + b\space\space \forall\space x \in \mathbb{R}$$ It can be easily verified that $$f(x)-2f(x-1)+f(x-2)=0 \space \forall \space x \in 
\mathbb{R}---(I)$$ Now, $\space$ let $f_2:\mathbb{R}\to\mathbb{R}$ such that $$f(x) = ax^2+bx+c \space \forall\space x\space \in \mathbb{R}$$ After some trial and error, I found that $$f(x)-3f(x-1)+3f(x-2)-f(x-3) = 0 \space \forall\space x\space \in \mathbb{R}---(II)$$ One could see the coefficients of terms in the expansion of $(1-x)^2$ being the coefficients of terms in $(I)$ in the same order, and the coefficients of terms in the expansion of $(1-x)^3$ being the coefficients of terms in $(II)$ in the same order. I tried to continue the above observation: Let $f_{4}:\mathbb{R}\to\mathbb{R}$ such that $$f(x) = ax^3 + bx^2 + cx + d\space\space \forall\space x \in \mathbb{R}$$ It can be verified that $$f(x)-4f(x-1)+6f(x-2)-4f(x-3)+f(x-4) = 0\space \forall\space x\space \in \mathbb{R}---(III)$$ And voila, the coefficients of terms in the expansion of (1-x)^4 are the coefficient of terms in $(III)$ in the same order. I have been trying to extend this result in general to $f_{n}$ where $n \in \mathbb{N}$ , where $\mathbb{N}$ is the set of natural numbers, have tried to approach this problem via several methods but couldn't get anywhere. Any hint towards proving the general result(if at all the result is true) will be appreciated Thank You PS: $a,b,c,d \in \mathbb{R}$ and $a \neq 0$","['algebra-precalculus', 'functions', 'polynomials', 'binomial-coefficients']"
4532895,If $u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt$ then $u(x)$ equals,"If $u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt$ then $u(x)$ equals $(1)\frac{e^x}{x}$ $(2)\frac{x}{e^x}$ $(3)xe^x$ $(4)\frac{1}{xe^x}$ we have, $$u(x)-\sin(x)=2\int_{0}^x \cos(x-t)u(t)dt\tag{A}$$ Using Leibnitz's rule of differentiation under integral sign,we get $$u'(x)-\cos(x)=2[-\int_{0}^x \sin(x-t)u(t)dt+u(x)]$$ $$\implies u'(x)-u(x)-\cos(x)=-2\int_{0}^x \sin(x-t)u(t)dt$$ Using Leibnitz's rule of differentiation under integral sign,we get $$u''(x)-u'(x)+\sin(x)=-2\int_{0}^x \cos(x-t)u(t)dt\tag{B}$$ Using equations $(A)$ and $(B)$ ,we get $$u''(x)-u'(x)+\sin(x)=\sin(x)-u(x)$$ $$u''(x)-u'(x)+u(x)=0\tag{C}$$ Auxiliary equation for $(C)$ is $m^2-m+1=0\implies m=\frac{1\pm \iota\sqrt 3}{2}$ So, $u(x)=c_1\cos(\frac{\sqrt 3}{2})x+c_2\sin(\frac{\sqrt 3}{2})$ ,where $c_1,c_2$ are arbitrary constants. which is not in any of the option,what is wrong with my solution?","['trigonometry', 'integral-equations', 'ordinary-differential-equations']"
4532917,What does one do when Wolframalpha fails to solve an integral?,"I'm looking for an analytic solution to this integral: $$
\int\sqrt{\left(\frac{2 d}{(1 + 4 x ^ 2)^{\frac{3}{2}}} + 1\right) \frac{2}{\sqrt{1 + 4 x ^ 2}}}dx, d > -0.5
$$ Wolframalpha fails to find a solution to this, and using quadratures for evaluation is too slow for my use case since I need this for a real-time application. What does one do in such cases? My intuition is to try to find a function that approximates the integral decently enough on a graphic calculator, but it's quite challenging to start from scratch. The challenging part of this integral is the $+ 1$ in the first term. Without it, the solution is trivial. Update: The closest approximation I found is this: $$
\frac{6 x \sqrt{\frac{2 d}{\sqrt{1 + 4 x ^ 2}} + 1}}{\sqrt[4]{9 ^ 2 x ^ 2 + 8 ^ 2} + 2}
$$ How could I improve the accuracy of this?","['integration', 'numerical-calculus', 'calculus']"
4532925,Noncommutative Ring with only Left Identity,Does there exist a noncommutative ring $R$ without an identity and an element $e\in R$ such that $ex = x$ for all $x\in R$ ? (i.e. $xe \neq x$ for some $x\in R$ ).,['abstract-algebra']
4532949,How can a permutation act on a set if there is no ordering inherent in a set?,"I'm hoping this isn't a stupid question. I'm currently pondering the notion of a ""permutation of a set"". For the first time, this idea seemed a little strange to me, because there is no ordering inherent in a set, and I think of a permutation as a reordering. Perhaps I shouldn't think of permutation as a reordering? Like, a transposition acting on a set would literally be  making an element $\alpha$ into an $a$ and $a$ into an $\alpha$ (where $a,\alpha \in S$ ). Like I said, hope this isn't a dumb question, I guess im just aware that I know some people are really into foundations of math and stuff and maybe someone can clarify what's wrong with my thinking....","['elementary-set-theory', 'logic', 'permutations']"
4532961,Unlabelled simple graphs with $k$ edges,"Let $G(n,k)$ denote the set of unlabeled simple graphs (i.e. without loops). Prove that for $k<\frac{1}{2}\binom{n}{2}$ we have $|G(n,k)|\leq |G(n,k+1)|$ . A naive constructive argument on $k$ by taking a graph on $G(n,k)$ and adding an edge to lift it to $G(n,k+1)$ doesn't work since two different graphs in $G(n,k)$ can become isomorphic when you add an edge. Is there a simple proof to this without resorting to counting polynomials and exact formulas?","['graph-theory', 'combinatorics']"
4533061,Existence of a bijective choice function $f:\tau\rightarrow \mathbb{R}$,"If $\tau$ is the set of open sets on the real line, it is known that $|\tau| = |\mathbb{R}|$ (see this question). Thus, it is feasible that there might exist a bijective choice function $f:\tau\rightarrow \mathbb{R}$ . A choice function $f$ on $\tau$ is a function such that $\forall U\in \tau, f(U)\in U$ . Does such a bijection exist?","['axiom-of-choice', 'general-topology', 'set-theory']"
4533081,Radical in polynomial ring over $\mathbb{Q}$ implies radical in polynomial ring over $\mathbb{C}$?,"Let $f_1,\ldots,f_s\in\mathbb{Q}[x_1,\ldots,x_n]$ be polynomials such that the ideal $\langle f_1,\ldots,f_s\rangle$ is radical. Viewing each $f_i$ as an element of $\mathbb{C}[x_1,\ldots,x_n]$ , is it true that $\langle f_1,\ldots,f_s\rangle$ generated as an ideal in $\mathbb{C}[x_1,\ldots,x_n]$ is also radical? For example, the ideal generated by $x^2+1$ in $\mathbb{Q}[x]$ is radical (it is prime) and even though the ideal generated by $x^2+1$ in $\mathbb{C}[x]$ is not prime, it is still radical . Indeed, if $f^n\in\langle x^2+1\rangle\subset\mathbb{C}[x]$ then $f^n$ has $x^2+1$ as a factor, so it has $x-i$ and $x+i$ as factors, which is only possible if $f$ had $x-i$ and $x+i$ as factors to begin with. Hence $f$ has $x^2+1$ as a factor so that $f\in\langle x^2+1\rangle\subset\mathbb{C}[x]$ .","['ring-theory', 'abstract-algebra', 'commutative-algebra', 'ideals']"
4533110,When is asymptotic stability preserved in this sequence of dynamical systems?,"Setup: Suppose that for each $a>0$ , $(a,0)$ is an asymptotically stable steady state of the autonomous system $$
\begin{cases}
\frac{d}{dt}{z}_1(t) = f_1\big(z_1(t),z_2(t);a\big)\\
\frac{d}{dt}{z}_2(t) = f_2\big(z_1(t),z_2(t);a\big)\\
\end{cases} \qquad (*)
$$ where $f_i:\mathbb{R}^2\times$ $\mathbb{R}_{>0}$ $\to\mathbb{R}$ is continuously differentiable $(i=1,2)$ . Remark: note that this implies that -- given system $(*)$ -- the basin of attraction of $(a,0)$ $$ \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=a \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\} $$ has strictly positive Lebesgue measure $\forall a>0$ ( details ). Assume that $\lim\limits_{a\to\infty}f_{i}(\cdot;a)=\hat{f}_{i}(\cdot)\in\mathcal{C}$ pointwise ( $i=1,2$ ),
and define the autonomous system $$
\begin{cases}
\frac{d}{dt}{z}_1(t) = \hat{f}_{1}\big(z_1(t),z_2(t)\big)\\
\frac{d}{dt}{z}_2(t) = \hat{f}_{2}\big(z_1(t),z_2(t)\big)\\
\end{cases} \qquad (**)
$$ Question: Are any additional assumptions needed for the following statement to be true? $``$ Assuming $(z_1(t),z_2(t))$ obey system $(**)$ , the set $$ \left\{\vec{z}_0\in\mathbb{R}^2: \big(z_1(0),z_2(0)\big)\,\texttt{=}\,\vec{z}_0 \Rightarrow \lim_{t\to\infty}z_1(t)=\infty \text{ and } \lim_{t\to\infty}z_2(t)=0 \right\} $$ has strictly positive Lebesgue measure. $""$ If so, which assumptions are sufficient (and necessary) for this to be true?""","['measure-theory', 'ordinary-differential-equations', 'stability-in-odes', 'limits', 'dynamical-systems']"
4533115,method of moments poisson distribution not unique,"We know that MOM estimate may not be unique. The most common example is Poisson distribution. From my lecture notes, it said if we only consider $m_1 = \mu_1'$ , then we have $\hat{\lambda} = \bar{X}$ . While if we consider $m_1 = \mu_1'$ and $m_2 = \mu_2'$ together, then $m_2 = \lambda + m_1^2 =\lambda + \bar{X}^2$ , which implies $\hat{\lambda} = m_2 - m_1^2$ . My question is why I cannot write $m_2 = \lambda + \lambda^2 $ , so this will be a new estimate.",['statistics']
4533148,Covering unit square with discs; systems of degree-2 polynomials,"Given a unit square and $n$ identical circles/discs, what is the smallest radius $r_n$ for which the circles can fully cover the square? For $n=4$ , the proven minimal solution is $r_4 = 1/(2\sqrt2) \approx 0.35355$ For $n=5$ , the proven minimal solution is $r_5 \approx 0.32616$ For $n>5$ , well, read on. The context behind this is actually pretty thick, and includes: Last week's Riddler Classic This 2000 paper discussing this covering up to $n=30$ , which has been linked in other questions My solution in Geogebra to the $5$ -circle case Fig 1: $5$ -Circle solution with analytic solution (OC) Fig 2: $6$ -Circle solution from Nurmelas & Ostergard , with labels The $5$ -circle result has an analytical solution--though not one that yields a value except via numerical methods--via the following system of polynomials: $$\begin{cases}
a^2 + \frac14 = 4r_5^2 \\
(1-a)^2 + b^2 = 4r_5^2 \\
(2a+2r-1)^2 + (2b-1)^2 = 4r_5^2
\end{cases}
$$ The solution is that $a$ is the smaller real root of: $$64a^6 -144a^5 + 209a^4 -196a^3 +154a^2 -92a +21 = 0$$ and the radius is $r_5 = \textstyle \frac12 \sqrt{a^2 + \frac14}$ . Edit: the full minimal polynomial for $r_5$ is degree 12, with $r_5$ being the smaller positive real root: $$4294967296 r^{12} - 33554432 r^{10} + 213975040 r^8 - 37961728 r^6 - 13421056 r^4 - 152384 r^2 + 180625=0$$ Now the $6$ -circle solution ought to also have an analytical solution, I would think--if nothing else, a high-degree polynomial. Consider Fig 2. Each of the line segments must be the same length as each of the others--those lengths being the radius of the identical circles. The figure has $C_2$ symmetry. Also note that the sets of four segments in the top left and bottom right connect the four points of a rectangle. I've added labels to the original figure from the paper. Using those distance labels, this system of equations ought to give us a solution: $$\begin{cases}
a^2 + b^2 = 4r_6^2 \\
(1-a)^2 + c^2 = 4r_6^2 \\
d^2 + f^2 = r_6^2 \\
(c - b - d)^2 + (a - f)^2 = r_6^2 \\
(1 - c - b - d)^2 + (1 - a - f)^2 = r_6^2 \\
b+c+2d=1
\end{cases}
$$ The first two equations describe the circles on the corners; the next three describe the segments pointing southeast, northwest, and northeast from the center of the bottom-middle circle, which has its center at $(b+d, f)$ . Nurmelas and Ostergard give the radius as $r_6 \approx 0.29873$ . However, both Wolfram Alpha and SageMath return little other than not-smallest solution where we cut the square into six rectangles and circle each of those. Everything else they return either has complex numbers or negative distances. I'd appreciate any help toward (A) dude, you made a tiny error right here, fix that, or (B) the better way to do it is this , or (C) this won't give you anything solvable, sorry. I'm actually rather surprised there seem to have been no further papers/computations on this since 2000. Unfortunately, the authors don't give their algorithm or the coordinates of the centers, so it's hard to determine just what they have. My math kingdom for a ""Supplementary Material"" link.","['recreational-mathematics', 'systems-of-equations', 'geometry']"
4533270,Is there difference between how we treat 'free variable' in basic logic and the kind we come across in elementary mathematics?,"This question may come across as quite contrived so let me give you an example we might find in a typical (simple) example, let $x$ and $y$ be elements of the reals (they can be considered for all values in the reals) for example for a well defined $f$ we might have $y=f(x)$ which means for every admissible value of $x$ there exists a value of $y$ and they can form a set of pairs (in this case, the graph of $f$ ) forming the set: $[(x,y)|y=f(x)]$ . In this case we have limited our values such that $y=f(x)$ is always true, for every $x$ we must consider a certain $y$ and we have no freedom to change this. In the case of the basic quantification logic I've done it seems different for proper 'free variables' for free variables we denote a domain of discourse $D$ and each variable is free to vary inside it, for example $x$ and $y$ are now free to vary however we can still have $y=f(x)$ but this becomes a subset of the possibilities of values for $x$ and $y$ , now we can define something like $y=f(x)$ and consider the values of $x$ and $y$ that make it true, such as proving the existence of at least one pair in the graph by writing: $∃(y)∃(x)(y=f(x))$ we can also say: $∀(x)∃!(y)(y=f(x))$ And with the graph of the function, $G_f$ $∀((x,y)∈G_f(y=f(x))$ Is there a difference here between the first and second case? Can we make a distinction between a 'free variable' and an elementary variable?","['predicate-logic', 'quantifiers', 'logic', 'functions', 'algebra-precalculus']"
4533296,Does this number have degree $>2$ over $\mathbb{Q}(i)$?,"I’m thinking about the number $x=r \alpha e^{2i\pi \theta}$ where $$
r = \sqrt{\frac{\sqrt{2}}{3-\sqrt{3}}},\quad \alpha = 1-\frac{\sqrt{3}}{2} + \frac{i}{2},\quad \theta = \frac{1}{48}.
$$ Each one is algebraic over $\mathbb{Q}(i)$ and hence so is the product $x=r\alpha e^{2i\pi/48}$ . We also have the relation $$
r^2 e^{4i\pi\theta}(1+i\alpha^2) = 1.
$$ Question: Is it possible that $x$ satisfies a degree $2$ polynomial over $\mathbb{Q}(i)$ ? Can I say for certain that it is not degree $2$ over $\mathbb{Q}(i)$ ? Context: I'm interested in studying the dynamical behaviour of certain diagonal orbits in $\operatorname{SL}_2(\mathbb{C})/\operatorname{SL}_2(\mathbb{Z}[i])$ . The behaviour of an orbit generated by an upper triangular unipotent matrix is known to be related to the continued fraction expansion of the top right entry. In particular the 'quadraticness' of such an entry would have dynamical implications - See, for example, the beginning remarks of section 5 in these notes by Hensley which say that quadratic numbers have expansions which are eventually periodic. Edit: Thanks so much for your answers. For anyone wondering about where these constants come from, $r$ is the constant appearing Minkowski's theorem on minimal values of two binary complex linear forms (evaluated on $\mathbb{Z}[i]$ ). See Satz LXII on page 218 of Minkowski's Diophantische Approximationen. There are some other (other than qudraticness) algebraic criteria which guarantee my desired orbit behaviour so I will try and explore those now.","['number-theory', 'continued-fractions', 'algebraic-number-theory']"
4533320,Find f(32) given f(x) = f(x-1) + 2^(f(x-1)+1) and f(0) = 5 [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question I would like to find f(32) given: f(x) = f(x-1) + 2^(f(x-1)+1) f(0) = 5 I figured that this function grows tetrationally (if that is a word) but I don't really know Where I got this equation from: In the collectable trading card game Magic: The Gathering, there are these cards: Miirym, sentinel wyrm , Astral Dragon , Parallel Lives If Astral Dragon where to come into play with Miirym and Parallel Lives already on the battlefield and all of the tokens Astral Dragon created where copies of Parallel Lives, you would end up with f(32) Parallel Lives in play. Edit: I am not looking for a specific answer, just a rough estimate of how large that number is. I know it is larger than 2^^32 bit is it Larger than 2^^33? 2^^34?","['functions', 'tetration', 'recursion']"
4533345,Why does $\mathbb{R}P^3/\mathbb{S}^1 \cong \mathbb{S}^2$ hold?,"Good evening. I recently stumbled across the claim $$\mathbb{R}P^3/\mathbb{S}^1 \cong\mathbb{S}^2,$$ which is not really clear to me. Here, $\mathbb{R}P^3$ denotes real projective space of dimension $3$ . I know that $\mathbb{R}P^3 \cong\mathbb{S}^3/\mathbb{Z}_2$ (more or less per definition) and $\mathbb{S}^3/\mathbb{S}^1 \cong\mathbb{S}^2$ (due to Hopf), but I'm unsure how exactly to proceed. Edit The context in which $\mathbb{R}P^3/\mathbb{S}^1$ arises is the following: The space of parametrized great circles on $\mathbb{S}^2$ is diffeomorphic to $\mathbb{R}P^3$ (which can be shown by identifying it first with the unit tangent bundle $S\mathbb{S}^2$ , and then using $S\mathbb{S}^2 \cong SO(3) \cong \mathbb{R}P^3$ ) and admits a $\mathbb{S}^1$ -action by time-shift, i.e. by unparametrizing the great circles.","['general-topology', 'differential-topology', 'algebraic-topology', 'projective-space']"
4533364,A question on warped product Riemannian metric,"I am studying a text of Riemannian geometry where the author says: Let the product manifold $M=I\times \tilde{M}$ where $I$ is an interval of $\mathbb{R}$ and $\tilde{M}$ is an $(n-1)-$ dimensional manifold equipped with a Riemannian metric $\tilde{g}$ . A warped product Riemannian metric on $M$ can be written as $$g= dr\otimes dr + w(r) \tilde{g}$$ where $r$ is the standard coordinate on $I$ and $w(r)$ is a positive function. If $F$ and $\tilde F$ denote norms induced by the Riemannian inner products on $M$ and $\tilde M$ respectively, we get $$F(v)=\sqrt{v^1v^1+w(r)\tilde{F}^2(\tilde{v})},$$ where $\tilde{v}$ is projection of v on $\tilde{M}$ .
We generalize it to the form $$F=\sqrt{w(v^1, r, \tilde{F})}. $$ Since F is positive homogeneous of degree one,  we have $$F = \tilde F.  \sqrt{w(\frac{v^1}{\tilde{F}}, r, 1)}, $$ for $\tilde{F} \neq 0$ . Since $\{v\in T_x M \mid \tilde{F}=0\}$ is a one dimensional subspace of $T_x M$ ( $x\in M$ ), the norm $F$ can be determined by its values on the open subset $\{v\in T_x M \mid \tilde{F} \neq 0\}$ by the continuity. I don't know which theorem is used that this result came up? Can someone explain it for me?","['normed-spaces', 'riemannian-geometry', 'differential-geometry']"
4533391,Limit of a product of two functions is $0$,"If $f(x)$ and $g(x)$ two real valued functions such that $\lim\limits_{x\to c}f(x)$ exists and is finite, $g(x)$ is bounded and $$\lim_{x\to c}f(x) g(x) =0$$ then prove that $\lim\limits_{x\to c}f(x)=0$ or $\lim\limits_{x\to c}g(x)=0$ or both. My try: let $\lim\limits_{x\to c}f(x)\neq 0$ then $$\lim\limits_{x\to c} g(x) =\frac{\lim\limits_{x\to c}f(x) g(x)   }{\lim\limits_{x\to c}f(x)   }$$ and hence we have $$\lim_{x\to c} g(x) =0  $$ What if $\lim\limits_{x\to c}g(x)\neq 0$ ? Thank you.","['limits', 'calculus', 'functions', 'real-analysis']"
4533393,"$ f\in L^1(\mathbb{R}) $, $ f $ is continuous at $ 0 $, $ \widehat{f}\geq 0 $, show that $ \widehat{f}\in L^1(\mathbb{R}) $.","Assume that $ f\in L^1(\mathbb{R}) $ and $ f $ is continuous at the point $ 0 $ . If $ \widehat{f}(\xi)=\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dx\geq 0 $ for any $ \xi\in\mathbb{R} $ , show that $ \widehat{f}\in L^1(\mathbb{R}) $ . It can be get that $$
\int_{\mathbb{R}}|\widehat{f}(\xi)|d\xi=\int_{\mathbb{R}}\int_{\mathbb{R}}f(x)e^{-2\pi ix\xi}dxd\xi.
$$ I do not how to use the condition that $ f $ is continuous at $ 0 $ . Can you give me some references or hints?","['harmonic-analysis', 'fourier-analysis', 'analysis', 'real-analysis']"
4533445,Find the domain to make the integral max,"Find the domain $D \subset \mathbb{R}^2$ such that the value of the integral $$
\iint\limits_D (1-x^2-y^2) \,\mathrm{d}A
$$ is maximum. I only have basic definitions and properties of Darboux integral on a general domain; no Fubini yet and no change of variables.  I'm thinking to use the following facts: If $A \subset B$ , $f$ is integrable on $A$ and $B$ with $f \ge 0$ , then $\int\limits_A f \le \int\limits_B f$ . If $f \le g$ , then $\int\limits_D f \le \int\limits_D g$ . Applying to this problem, I'm going to find the maximum area of $D$ such that $1-x^2-y^2 \ge 0$ .  That means $D$ must be a disk center $O$ and radius $1$ . Am I right?  I feel like my work is not concrete at all, not mathematically reasoning. What should I do?  Thank you very much.","['multivariable-calculus', 'multiple-integral', 'area']"
4533463,Canonical inverse for Laplacian operator in $C^\infty(\mathbb R^n)$,"Let $f \in C^\infty_c(\mathbb R^n)$ , we know $$(Af)(x) := (f * \Gamma)(x) - (f * \Gamma)(0)$$ solves the Poisson's equation on $\mathbb R^n$ $$\Delta u = f.$$ where $\Gamma(x)$ is the fundamental solution of the Laplcace equation. This means the operator $A$ defined above is a right inverse to the Laplacian operator. Of course there are many other right inverses, but I think the operator $A$ is somewhat a ""canonical"" right inverse. For general $f\in C^\infty(\mathbb R^n)$ , the expression $f * \Gamma$ does not make sense. However, I believe (at least intuitively) there are still many right inverses to Laplacian operator in this case. And I guess $$(Bf)(x) := \lim_{R\to \infty} \Big((f\chi_{B_R} * \Gamma)(x) -  (f\chi_{B_R} * \Gamma)(0)\Big) $$ is well-defined and $B$ is again a ""canonical"" right inverse to Laplacian. Can I actually prove it? This may be a silly question. So I'm very sorry if it wasted your time.","['integration', 'harmonic-functions', 'analysis', 'laplacian', 'partial-differential-equations']"
4533475,"Determine if weighted graph can be physically constructed, treating weight as Euclidean distance (ie check if subset of distances is self-consistent)","Suppose we want to position some points in space, given that we know at least some of the distances between them. How can we determine if this is possible? And if it is possible, can we determine the least dimension in which it is possible and algorithmically find a possible (clearly not unique) solution? This is basically the problem of physically constructing a weighted graph, where the edges represent distances between two vertices, and we don't care how far apart are any pairs of vertices not joined by an edge. Clearly we should check that the given distances satisfy the triangle equality, but for a fixed dimension this itself is not enough. Suppose we know that a subset of three points A, B and C each lie a (Euclidean) distance of one from each other. Then there is no way to position them in one dimension. However, in two-dimensional space they can be physically constructed, as the vertices of an equilateral triangle. So there was no violation of the triangle inequality, which means the one-dimensional issue we hit was some other problem. Similarly if point D must be at unit distance from A, B and C then two dimensions are not enough, but a three-dimensional tetrahedron would resolve the problem. If A, B, C, D, E must all be at unit distance from each other, then even three dimensions are not enough... An algebraic way to look at the three-point case is that equations $(x_A - x_B)^2 = 1^2$ , $(x_B - x_C)^2 = 1^2$ , $(x_C - x_A)^2 = 1^2$ are inconsistent. But we can extend to another dimension, $(x_A - x_B)^2 + (y_A - y_B)^2 = 1^2$ etc, to make them consistent (albeit under-determined). So long as there is no violation of the triangle inequality, can we always keep adding extra dimensions until the system of  simultaneous quadratic equations becomes consistent? Is there an algorithm to determine whether the equations are consistent? (Bear in mind we will not necessarily know the distance between each pair of points.) Is there instead a more geometric approach we can use? Related questions: Define positions of a set of points given (only) the distances between them (but unlike my question, all distances are given rather than only only a subset of them), When can we achieve given distances between four points? and When can a weighted graph be embedded in a metric space? (not necessarily Euclidean, and which has nice albeit trivial answer). This interesting question didn't receive an answer, but looks at sets of distances rather than assigning the distances to have to lie between particular vertices: Sets of distances between points that do not form distinct shapes . Closely related question on CS Theory SE: Best way to determine the minimum dimension of a structure given only distances between points (partially answers my question in terms of finding the minimum dimension, but not determining whether it's possible in some dimension- thanks @D.W. for finding this one).","['euclidean-geometry', 'systems-of-equations', 'graph-theory', 'geometry', 'quadratics']"
4533506,"if $f=f(x,y)$ where $x=u^2 - v^2$ and $y = \frac{u}{v}$ what is $f_{uu}$ and $f_{vv}$","I have tried to calculate this question and am not sure about my process being correct.  If you have the time, can you tell me if my answer is correct or not and if not where I have made my mistake(s).  Thanks in advance. $$
	\begin{align*}
		f &= f(x.y), x = u^2 - v^2, y = \frac{u}{v}\\
		&\text{find $f_{uu}$ and $f_{vv}$}\\
		\frac{\partial x}{\partial u} &= 2u\\
		\frac{\partial x}{\partial v} &= -2v\\
		\frac{\partial y}{\partial u} &= \frac{1}{v}\\
		\frac{\partial y}{\partial v} &= -\frac{u}{v^2}\\
		\frac{\partial f}{\partial u} &= \frac{\partial f}{\partial x} \frac{\partial x}{\partial u} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial u}\\
		&=2u\left(\frac{\partial f}{\partial x}\right) + \frac{1}{v}\left(\frac{\partial f}{\partial y}\right)\\
		\frac{\partial}{\partial u}\left(f\right) &= 2u\frac{\partial}{\partial x}\left( f \right) + \frac{1}{v}\frac{\partial}{\partial y}\left( f \right) \tag{1} \label{equation0}\\
		\frac{\partial^2 f}{\partial u^2} &= \left(\frac{\partial}{\partial u}\right) \left(\frac{\partial f}{\partial u}\right)\\
		&=2\left(\frac{\partial f}{\partial x}\right) + 2u\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial x}\right) 
		+ \frac{1}{v}\left(\frac{\partial}{\partial u}\right)\left(\frac{\partial f}{\partial y}\right) \tag{2} \label {equation1}\\
		&\text{Use equation $\eqref{equation0}$ to find $\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x} \right)$ and 
			$\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y} \right)$.}\\
		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial x}\right)
		&= 2u\left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) + 
		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\
		&= 2u\frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x} \tag{3} \label{equation2} \\
		\frac{\partial}{\partial u} \left(\frac{\partial f}{\partial y}\right)
		&= 2u \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) + 
		\frac{1}{v} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\
		&= 2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2} \tag{4} \label{equation3} \\		
		&\text{Substitute $\eqref{equation2}$ and $\eqref{equation3}$ into $\eqref{equation1}$.}\\
		\frac{\partial^2 f}{\partial u^2} &= 2 \left(\frac{\partial f}{\partial x}\right) + 2u \left(2u \frac{\partial^2 f}{\partial x^2} + \frac{1}{v} \frac{\partial^2 f}{\partial y \partial x}\right) + \frac{1}{v} \left(2u \frac{\partial^2 f}{\partial x \partial y} + \frac{1}{v} \frac{\partial^2 f}{\partial y^2}\right)\\
		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y}
		+ \frac{1}{v^2} \frac{\partial^2 f}{\partial y^2}\\
		&\text{Assuming $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$}\\
		&= 2 \frac{\partial f}{\partial x} + 4u^2 \frac{\partial^2 f}{\partial x^2} + \frac{4u}{v} \left( \frac{\partial^2 f}{\partial y \partial x} \right)	+ \frac{1}{v^2} \left(\frac{\partial^2 f}{\partial y^2}\right)\\
		\frac{\partial f}{\partial v} 
		&= \frac{\partial f}{\partial x} \frac{\partial x}{\partial v} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial v}\\
		&=-2v\left(\frac{\partial f}{\partial x}\right) - \frac{u}{v^2} \left(\frac{\partial f}{\partial y}\right)\\
		\frac{\partial}{\partial v}\left(f\right) &= -2v \frac{\partial}{\partial x}\left( f \right) -  \frac{u}{v^2}\frac{\partial}{\partial y}\left( f \right) \tag{5} \label{equation4}\\
		\frac{\partial^2 f}{\partial v^2} &= \left(\frac{\partial}{\partial v}\right) \left(\frac{\partial f}{\partial v}\right)\\
		&=-2\left(\frac{\partial f}{\partial x}\right) - 2v \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial x}\right) + \frac{2u}{v^3}\left(\frac{\partial f}{\partial y}\right) - \\
		& \frac{u}{v^2} \left(\frac{\partial}{\partial v}\right)\left(\frac{\partial f}{\partial y}\right) \tag{6} \label {equation5}\\
		&\text{Use equation $\eqref{equation4}$ to find $\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x} \right)$ and $\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y} \right)$.}\\
		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial x}\right)
		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial x}\right) - 
		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial x}\right) \\
		&= -2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x} \tag{7} \label{equation6} \\
		\frac{\partial}{\partial v} \left(\frac{\partial f}{\partial y}\right)
		&= -2v \left(\frac{\partial}{\partial x}\right) \left(\frac{\partial f}{\partial y}\right) - 
		\frac{u}{v^2} \left(\frac{\partial}{\partial y}\right) \left(\frac{\partial f}{\partial y}\right) \\
		&= -2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2} \tag{8} \label{equation7} \\		
		&\text{Substitute $\eqref{equation6}$ and $\eqref{equation7}$ into $\eqref{equation5}$.}\\
		\frac{\partial^2 f}{\partial v^2} &= -2 \left(\frac{\partial f}{\partial x}\right) - 2v \left(-2v \frac{\partial^2 f}{\partial x^2} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y \partial x}\right) +
		\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) \\
		&- \frac{u}{v^2} \left(-2v \frac{\partial^2 f}{\partial x \partial y} - \frac{u}{v^2} \frac{\partial^2 f}{\partial y^2}\right)\\
		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} + \frac{2u}{v} \frac{\partial^2 f}{\partial y \partial x} - \frac{2u}{v^3} \frac{\partial f}{\partial y} + \frac{2u}{v} \frac{\partial^2 f}{\partial x \partial y} \\
		&+ \frac{u^2}{v^4} \frac{\partial^2 f}{\partial y^2}\\
		&\text{Assuming $\frac{\partial^2 f}{\partial x \partial y} = \frac{\partial^2 f}{\partial y \partial x}$}\\
		&= -2 \frac{\partial f}{\partial x} + 4v^2 \frac{\partial^2 f}{\partial x^2} - \frac{4u}{v} \left(\frac{\partial^2 f}{\partial y \partial x}\right) +\frac{2u}{v^3} \left(\frac{\partial f}{\partial y}\right) +  \frac{u^2}{v^4} \left(\frac{\partial^2 f}{\partial y^2}\right)\\
	\end{align*}
$$","['partial-derivative', 'multivariable-calculus', 'chain-rule']"
4533508,"What are the solutions in numbers of $xyz | x^n + y^n + z^n$, $n$ prime.","What are the integers $(x,y,z)\in \mathbb Z^*$ such that $xyz$ divide $x^n + y^n + z^n$ , for prime $n$ ? I have no other motivation for that problem but its inherent beauty and interest.
Note that it can be assumed without loss of generality that $x\geq |y|\geq |z|$ . Here is what I've obtained so far: If $xyz$ divides $x+y+z$ and $n$ is odd, $xyz$ divides $x^n + y^n + z^n$ . Since the set of solution $(x,y,z)$ of the former relation for which $x\geq |y|\geq |z|$ is $$\{(2,1,1), (1,1,1), (3,2,1), (x,1,-1), (x,-1,1):\ x\in \mathbb N^*\} \cup \{(x,y,z): x+y+z = 0\},$$ these are also solutions of the proposed equation. The proof by induction of the above proposition is based on the following formula: $$x^n+y^n+z^n = (x^{n-1}+y^{n-1}+z^{n-1})(x+y+z) - (x^{n-2}+y^{n-2}+z^{n-2})(xy+xz+yz) + (x^{n-3}+y^{n-3}+z^{n-3})xyz.$$ Regarding the set of solutions of the equation $xyz|x+y+z$ , this is not entirely trivial but is nevertheless straightforward. So, let me call the above solutions the ""straightforward solutions"". My question is: do there exist solutions that are not straightforward? I guess that the case $n=2$ , not dealt with in the above proposition, is very different from the case "" $n$ odd"". EDIT: to avoid solutions by scaling (see the answer of Qiaochu Yuan), I should probably have added the condition $\gcd (x,y,z) = 1$ . I suggest to restrict the problem to that case from now on. That is, the question which is the closest to what I intended is: do there exist globally coprime solutions that are not straightforward?","['number-theory', 'algebraic-number-theory', 'elementary-number-theory']"
4533509,An interesting sum identity: $\sum \frac 1 {k_1\cdots k_m} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k^m} \binom n k$,"Fix a positive integer $m$ .  I am trying to prove
\[\sum \frac 1 {k_1\cdots k_m} = \sum_{k=1}^n \frac{(-1)^{k-1}}{k^m} \binom n k\]
where the sum on the left is taken over all integer $m$ -tuples $(k_1,\dots,k_m)$ such that $1\le k_1\le \cdots\le k_m\le n$ .  This comes immediately after having proved that
\[ \sum_{k=1}^n b_k/k = \sum_{k=1}^n \frac{a_k}k \binom n k \]
where $b_n = \sum_{k=1}^n a_k\binom n k$ .  There's a strong similarity, so I suspect that I should use this result to prove the above.  In particular it suggests that we take $a_k/k = (-1)^{k-1}/k^m$ which is to say $a_k = (-1)^{k-1}/k^{m-1}$ .  If this is the right path then we want to show that
\[ \sum\frac 1 {k_1\cdots k_m} = \sum_{k=1}^n \frac{b_k}k = \sum_{k=1}^n \frac 1 k \sum_{j=1}^k a_j\binom{k}j = \sum_{k=1}^n \frac 1 k \sum_{j=1}^k \frac{(-1)^{j-1}}{j^{m-1}}\binom{k}j\]
Now trying to rearrange and reindex the sums basically just seems to get you back into the proof that $\sum b_k/k = \sum a_k\binom n k /k$ so that seems like a dead end. I'm guessing that somehow, instead, we should find a more direct reason why these are equal, perhaps by splitting fractions.  For instance, $\frac 1 {2\cdot 3} = \frac{1}{2}-\frac{1}{3}$ but there is no such simple splitting for $\frac 1 {2\cdot 4}$ . The right-hand side of the above seems to suggest that perhaps we fix a $1\le k \le n$ as the least number in $(k_1,\dots,k_m)$ , and then use something that looks vaguely like inclusion-exclusion. From here the question looks like ""Is there a reason why, if we collect all the terms of $\sum \frac 1 {k_1\cdots k_m}$ with a fixed $k_1$ , that the sum of these terms is $\frac 1 k \sum_{j=1}^k \frac{(-1)^{j-1}}{j^m}\binom k j$ ?"" Just to test the idea out, if we set $n=4,m=2$ and $k_1=2$ then we would be looking at the terms
\[ \frac{1}{2\cdot 2}+\frac{1}{2\cdot 3}+\frac{1}{2\cdot 4} \]
and on the other hand we would be trying to see whether this is the same as
\[ \frac 1 2 \sum_{j=1}^2 \frac{(-1)^{j-1}}{j^2}\binom 2 j = \frac 1 2 \cdot \left( 2 - \frac 1 4  \right)\]
A quick computation shows these are not equal, so then I'm not sure if I made a mistake along the way, or if the entire plan of attack is flawed. If the entire plan is flawed, I don't have a plan B.","['summation', 'binomial-coefficients', 'combinatorics']"
4533542,Is there an algorithm to check whether a subset generates a group?,"I’m new to abstract algebra, so recently, I came up with a curious question: Given a group $G$ and a subset $S$ of $G$ , is there a general algorithm to decide whether $\langle S \rangle = G$ ? A special case which I find interesting is when $G = S_n$ (finite symmetric groups), can we still come up with such algorithm to decide this problem? If possible, is the algorithm efficient (i.e in polynomial time)? (I think if the search space $G$ is finite, I might be able to come up with an exponential time brute force algorithm which takes $g_1, g_2 \in S$ and put $g_1g_2, g_2g_1$ back to $S$ repeatedly until there is no more new elements; not sure though if this is correct.) A concrete formulation of this question is more like: “Does the problem of checking whether $S$ generates $G$ in P, EXP, or even decidable or not?” for $G = S_n$ , for finite $G$ , and for general $G$ .","['permutations', 'abstract-algebra', 'algorithms', 'group-theory', 'computational-complexity']"
4533561,Proof Surjective function with no ''given'' function,"$f_1 : \mathbb{R}^2 \longrightarrow \mathbb{R}$ and $f_2 : \mathbb{R}^2 \longrightarrow \mathbb{R}$ and define $f: \mathbb{R}^2 \longrightarrow \mathbb{R}^2$ as $f(x,y) = (f_1(x,y),f_2(x,y))$ . Question: If $f$ is surjective. Proof that $f_1$ and $f_2$ are also surjective. So what I thought was that if $f$ is surjective, this means that for all $b$ (element of $\mathbb{R}^2$ ) there exists an $a$ (element of $\mathbb{R}^2$ ) so that $f(a)=b$ . However, I don't know how to continue this proof without having a function as I can't find an inverse function. I would like some help. Thanks !","['real-numbers', 'functions']"
4533603,A unit speed differentiable curve which avoids a null set of directions,"For each $x \in \mathbb{R}^2$ , let $N_x$ be a null set of the unit circle $S^1$ (with respect to $1$ -dimensional Lebesgue measure). Further given that for each $u \in S^1$ , we have that $u \not \in N_x$ for almost all $x \in \mathbb{R}^2$ . Does there necessarily exist a unit speed differentiable curve $\gamma : (-\varepsilon, \varepsilon) \rightarrow \mathbb{R}^2$ such that: $\gamma'(t) \not \in N_{\gamma(t)}$ for all $t \in (-\varepsilon,\varepsilon)$ ?","['measure-theory', 'differential-geometry', 'real-analysis']"
4533628,"Express $T$, $N$ and $B$ unitary vectors in terms of $\sigma$ (not necessary an arc length parametrization).","Let $\sigma:[a, b] \to C \subset \mathbb{R}^n$ a parametrization of a curve $C$ . I want to express $T$ , $N$ and $B$ unitary vectors in terms of $\sigma$ . The main problem is that I cannot find a short way to express these unitary vectors. For example: $$T' = \Big(\frac{1}{|| \sigma' ||}\Big) \sigma''- \Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big) \sigma' = \Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)$$ and $$|| T' || = \sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2}$$ Then $$N = \frac{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x', \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y', \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)}{\sqrt{\Bigg(\Big(\frac{1}{|| \sigma' ||}\Big)x''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)x'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)y''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)y'\Bigg)^2+\Bigg( \Big(\frac{1}{|| \sigma' ||}\Big)z''-\Big(\frac{\sigma'' \cdot \sigma'}{|| \sigma' ||^3}\Big)z' \Bigg)^2}}$$ Is it a short way to write $N$ and $B$ in terms of $\sigma$ ?","['multivariable-calculus', 'calculus']"
4533640,How can this equation be simplified to give $y$?: $x = \frac{(-1)^y ( 5 (-1)^y y - y + (-1)^y - 1))}4$,"I'm trying to convert this equation to the form $y = ...$ , but I am stuck. It seems the $y$ -root of $(-1)^y$ is not $-1$ , but is instead a beast. Here is the overall equation: $$x = \frac{(-1)^y  ( 5  (-1)^y  y - y + (-1)^y - 1))}4$$ Note: y is an integer, x is an integer. I could be open to x needing to be a complex number as long as there are solutions where x ∈ { 3+0i, 5+0i, 7+0i, ...} and y ∈ { 5, 8, 11, ...}. Note: the point is to avoid using separate equations for even vs. odd, but to have one equation that handles both. That's why the first equation has (-1)^n in it; it makes the equation = y when y is even, and (3y+1)/2 when y is odd. However that trick is not as helpful when we only care about every third number instead of every second number. Context: I'm an old man trying to refresh my math skills by learning about groups and branch groups. I'm not sure how much extra context you want. Trying to build a map between 2n+1 and 3n+2, kind of.","['algebra-precalculus', 'inverse-function', 'transcendental-equations']"
4533715,Sampling from intersection of sphere and simplex,"I need to isotropically sample vectors in $\mathbb{R}^d$ of given Euclidean norm $C$ with a restriction that components are positive and add up to 1. Can someone suggest a method to do it? I tried sampling from sphere/plane intersection and rejecting points outside of the positive orthant, but the efficiency is bad. Unless $d$ is very low, this has almost 100% probability of rejection. Visualized below for $d=3$ .","['statistics', 'mathematica', 'probability', 'sampling']"
4533724,Is this a paradox about probability of a fair coin at very large numbers of flips?,"The binomial formula for the probability of x heads on n flips with a probability of 0.5 is: $$\displaystyle \frac{\frac{n!}{x!(n-x)!)}}{2^{n}}$$ For the probability of getting exactly n/2 heads, this becomes: $$\displaystyle\frac{\frac{n!}{(\frac{n}{2})!(\frac{n}{2})!}}{2^{n}}$$ I don't know how to evaluate this in the limit that n approaches infinity, but I used a spreadsheet's binomdist function (x,n,0.5,false) to calculate the probability of exactly 50% heads on n flips (n = 2x), where x ranges from ten to a billion, with steps increasing by powers of 10.  Up to 1 billion heads on 2 billion flips, the probability of the 50% heads decreases per the table below. Heads Flips Probability 10 20 0.176197052 100 200 0.05634847901 1000 2000 0.01783901115 10000 20000 0.005641825312 100000 200000 0.001784121886 1000000 2000000 0.000564189513 10000000 20000000 0.0001784124094 100000000 200000000 0.00005641895828 1000000000 2000000000 0.00001784124116 It seems that the probability of an outcome with exactly 50% heads decreases as the number of flips increases.  This makes sense to me because the number of categories of outcomes that are near (but not equal to) 50% heads increases with increasing numbers of flips, so some of the most central probability would be reapportioned to the near-neighbor categories.  I also know that the likelihood of getting extreme results decreases as n increases, so that more of the probability is centralized around the 50% heads rather than the tails. However, from another point of view, it seems surprising.  If the trend continues in the limit that n approaches infinity, then the probability of getting exactly 50% heads is zero when the coin is flipped an infinite number of times.  This contradicts how I had thought expectation values work.  I had thought that by flipping a coin an infinite number of times, the limiting-case behavior equates to the expectation value.  The expectation value of heads for the coin is 0.5, but the binomial function suggests that we never reach it. I recognize that the probability distribution becomes increasingly centralized about 50% in a symmetrical way.  However it seems that it never actually lands there, even after an infinite number of flips, which seems paradoxical. Am I understanding this correctly?","['probability-limit-theorems', 'probability-distributions', 'probability']"
4533726,Open subfunctors of Affine scheme is not affine,"So I was given this problem by my teacher, let $\text{Aff}_k$ be the opposite category of commutative $k$ -algebras or $\text{CAlg}_k^{op}$ . One can define a functor $\text{Aff}_k\to Fun(\text{CAlg}_k, Set)$ by the definition $$A\mapsto h_A=\hom_{\text{CAlg}_k}(A, -)$$ Call functors of this form affine schemes. Now we can make a definition - Definition : Given an affine scheme $X=h_A$ define an open subfunctor corresponding to a subset $I\subset A$ as the functor $$D(I)(R)=\{a:A\to R | Ra(I)=R\}\subset h_A(R)=\hom_{\text{CAlg}_k}(A, R)$$ Now the problem states - If $A=k[X, Y]$ and $I=\{X, Y\}$ then show that $D(I)$ is not an affine scheme. That is, there is no $k$ -algebra $B$ such that $D(I)\cong h_B$ . To me it seems like a different version of the problem - ""The scheme $S=\mathbb{A}_k^2\setminus \{(0,0)\}$ is not affine."" As I understand that in that case one shows that the coordinate ring of $S$ is isomorphic to $k[X, Y]$ again which leads to a contradiction. Now what I tried was consider $X=h_A$ to be an affine scheme. Then $$\hom_{Fun(\text{CAlg}_k, Set)}(X, \mathbb{A}^1)=\hom_{Fun(\text{CAlg}_k, Set)}(h_A, h_{k[T]})=\hom_{\text{CAlg}_k}(k[T], A)\cong A$$ So if I can show $\hom_{Fun(\text{CAlg}_k, Set)}(D(I), \mathbb{A}^1)\cong k[X, Y]$ then we are done. But I am not sure how to proceed with this proof. Any help would be appreciated.","['affine-schemes', 'algebraic-geometry']"
4533770,"Integral bounds for a square with vertices $(\pm2,0), (0,\pm2)$","In exercise X §1 1(e) from ""Calculus of several variables"" by Serge Lang (Third Edition), we are asked to Use Green's theorem to find the integral $\int_C y^2 \;dx+x\;dy$ When C is the following curve (taken counterclockwise) ... (e) The square with vertices $(\pm2,0), (0, \pm2)$ . I took this to mean $$
\int_{-2}^0\int_{-2-x}^{2+x}(1-2y)dy\;dx+
\int_{0}^2\int_{x-2}^{2-x}(1-2y)dy\;dx
$$ which comes out to 0 according to my calculations. Checking in the back the answer was given as 8. I checked my work again and didn't find any error, but according to the explanation for the answer, see photograph below, the integral is just $$
\int_{-2}^{2}\int_{-2}^{2}(1-2y)dy\;dx
$$ I think the book's answer is wrong, but I am asking for your opinion because I would like to check that I haven't missed some obvious thing.","['multivariable-calculus', 'solution-verification']"
4533782,How to linearize this bernoulii ODE?,"I have a Bernoulli ODE problem: $$y'-xy=xy^{\frac{3}{2}}.$$ To solve it I will linearize the ODE. Let $z=y^{1-\frac{3}{2}}=y^{-\frac{1}{2}}$ then $y=z^{\frac{1}{2}}$ and $y'=\frac{1}{2} z^{-\frac{1}{2}} z'$ . Substitute in the ODE we have \begin{align}
			& \frac{1}{2} z^{-\frac{1}{2}} z' - x z^{\frac{1}{2}} =x \left( z^{\frac{1}{2}}\right) ^{\frac{3}{2}}\nonumber\\
			\iff & \frac{1}{2} z' - x z =x  z^{\frac{3}{8}} \nonumber\\
			\iff &  z' - 2 x z =2x  z^{\frac{3}{8}}.\label{p1ba}
		\end{align} But in the last ODE I can't get the linear ODE. What should be letting $z(x)$ such that the ODE can linear?",['ordinary-differential-equations']
4533791,Problem with several Simson lines in need of complete geometry solution,"The problem: $A,B,C,A’,B’,C’$ are concyclic. The three Simson lines of $A,B,C$ about $\triangle A'B'C'$ intersect at $D,E,F$ . The three Simson lines of $A’,B’,C’$ about $\triangle ABC$ intersect at $D’,E’,F’$ . Show that $D,E,F,D’,E’,F’$ are concyclic. Moreover, if $H,H’$ are the orthocenters of $\triangle ABC,\triangle A’B’C’$ . Show that the center of $\odot(DEFD’E’F’)$ is the midpoint of $HH’$ . It might be no good drawing the graph. When orthocenters and Simson lines appear at the same time, I think of the Steiner theorem, which states that $A,B,C,D$ concyclic, $H$ is the orthocenter of $\triangle ABC$ , then the midpoint of $DH$ is on the Simson line of $D$ . There’s also a property that might help: Let $A,B\in\odot O$ . The intersection angle of Simson lines of $A,B$ equals to the inscribed angle of $\overset{\LARGE\frown}{AB}$ in $\odot O$ . I’ll show my complex number method. (Barely needs figure) First let $\odot{ABC}$ be the unit circle. There’s a formula of Simson line of $P$ and $\triangle ABC$ that is $$Pz-ABC\overline z=\frac1{2P}(P^3+\sigma_1P^2-\sigma_2P-\sigma_3).$$ So we can get the Simson line of $A$ and $B4$ then put them together to find intersection. \begin{cases}Az-A’B’C’\overline z=\frac1{2A}(A^3+\sigma_1A^2-\sigma_2A-\sigma_3),\\
Bz-A’B’C’\overline z=\frac1{2B}(B^3+\sigma_1B^2-\sigma_2B-\sigma_3).\end{cases} Subtract, and it is not hard to solve $\displaystyle z=D=\frac12(A+B+\sigma_1+\frac{\sigma_3}{AB}).$ The midpoint of $HH’$ is $M=\dfrac12(A+B+C+A’+B’+C’).$ Then $$|D-M|=\frac12|-C+\frac{A’B’C’}{AB}|=\frac12|ABC-A’B’C’|.$$ Thus all of $DM,EM,FM,D’M,E’M,F’M$ equal to this. $\#$",['geometry']
4533866,Coproduct in the category of compact abelian groups,"Denote by Ab the category of abelian groups and group homomorphisms and by CAb the category of compact Hausdorff abelian groups and continuous group homomorphisms. It follows from Pontryagin duality that CAb is equivalent to Ab $^{op}$ , where $^{op}$ denotes the opposite category. This is a consequence of the fact that a locally compact group is compact if and only if its dual is discrete and viceversa. Now, Ab has both products and coproducts, so the same is true for Ab $^{op}$ and therefore also for CAb . Products in CAb are just the usual direct products endowed with the product topology. On the other hand, I find it much harder to understand coproducts in this category. Given a family $(G_i)_{i\in I}$ of compact abelian groups, one way to describe their coproduct is the following: $$\coprod_{i\in I}G_i=\widehat{\prod_{i\in I}\widehat G_i}$$ where the hat denotes the Pontryagin dual and the product is endowed with the discrete topology. Is there a ""more concrete"" way to describe the coproduct in CAb , that does not make use of duals?","['harmonic-analysis', 'topological-groups', 'category-theory', 'group-theory', 'abelian-groups']"
4533948,Density of hitting time for a two-sided barrior for Brownian motion with drift,"For the following Brownian motion with drift $X_t = X_0 + \mu t + \sigma B_t$ where $\mu \in \mathbb{R}$ , $ \sigma > 0$ and $X_0 \in \mathbb{R}$ which is a solution to the stochastic differential equation: \begin{align*}
dX_t = \mu dt + \sigma dB_t \ .
\end{align*} I am interested in finding the probability density funtion of the following stopping time: \begin{align*}
\tau = \inf[t: X_t \notin (a,b) ] \ .
\end{align*} Where we for the initial position have $X_0 \in (a,b)$ . The corresponding forward Kolmologov equation to the above SDE is: \begin{align*}
\frac{\partial u}{\partial t} = \frac{\sigma^2}{2} \frac{\partial^2u}{\partial x^2} - \mu  \frac{\partial u}{\partial x} \ .
\end{align*} From the answers in the link Density of first hitting time of Brownian motion with drift it should be possible to find the density of $\tau$ as a function of $t$ by solving the forward Kolmologov equation with the initial condition $u(x,t=0) = \delta(x-x_0)$ and with boundary condition $u(a,t)=u(b,t) = 0$ for all $t>0$ . I have already tried the same strategy as mentioned in the link, but it doesn't seem to work. Do anyone have a suggesten to another strategy to solve this problem?","['stochastic-differential-equations', 'partial-differential-equations', 'stopping-times', 'brownian-motion', 'probability-theory']"
4533970,Degree of Hodge bundle,"Let $H$ be the Hilbert scheme parametrizes subschemes of $\mathbb P^{5g-6}$ with Hilbert polynomial $p(m)=(6g-6)m+(1-g)$ (for example, curves with genus $g$ embedding by the canonical bundle to the 3rd power), and let $\pi: U \to H$ be the universal family. Let $\omega=\omega_{U/H}$ be the relative dualizing sheaf. I want to know that how to compute the self intersection number $det(\pi_* \omega)^n$ ? Where $n$ is the dimension of $H$ . Although this is may not be well defined on every point, those bad ones are of higher codimension, so I believe this number is still well defined. Could anyone give a comment or reference on this question? Thanks in advance.","['algebraic-curves', 'algebraic-geometry', 'moduli-space']"
4533980,Non solvable group requiring more than 2 generators,"The smallest group requiring more than 2 generators is $$
C_2 \times C_2 \times C_2
$$ but that group is abelian. The smallest non abelian groups requiring more than 2 generators are groups with quotient $ C_2^3 $ , such as $ D_8 \times C_2 $ and $ Q_8 \times C_2 $ . But these extensions of $ C_2^3 $ are still solvable. What is the smallest finite group $ G $ which is not solvable and requires more than two generators? This question is the same as The smallest group with 3 generators but with ""non abelian"" replaced by ""non solvable"" Kenta S points out that $$
C_2^3 \times A_5
$$ is order 480 and non solvable and requires at least 3 generators (since $ C_2^3 $ is a quotient). I have a feeling that we can do better  and $ C_2^2 \times A_5 $ is also minimal 3 generated (EDIT: my feeling about $ 2^2A_5 $ was wrong see the answer from ahulpke or for an explicit 2 generation of $2^2A_5$ see answer from Parcly Taxel)(Also note that ahulpke found another non-solvable group of size 480 which is minimal 3 generated, namely $2^2S_5$ ).","['group-theory', 'finite-groups']"
4534046,Optimizing a Chernoff bound with Bernstein's condition,"Let $X$ be a real valued, zero mean random variable satisfying berstein's condition with parameter $b$ so that the moment generating function satisfies: $$\mathbb E\left [ \exp \left ( \lambda X\right )\right ]
\leq \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} \right )$$ for all $\lambda \in (0, 1/b)$ where $\sigma^2$ is the variance of $X$ . Using this to evaluate a chernoff bound for $X$ : $$\mathbb P (X \geq t) \leq \inf_{\lambda \in (0, 1/b)} \exp \left ( \frac{\lambda^2\sigma^2/2}{1-\lambda b} -\lambda t\right )$$ All that remains is to choose an appropriate value of $\lambda$ . The literature usually chooses $\lambda^* = t/(bt + \sigma^2)$ but it's not clear to me how this value is chosen or why it is a good choice. Interestingly, the argument of the exponential is $0$ when $\lambda =  2t/(2bt + \sigma^2)$ , which looks like the choice of $\lambda^*$ from the literature (but with a scaling on $t$ ). Is the process to identify $\lambda^*$ basically: 1. find a root, 2. choose a scaling of $t$ that minimizes the value of the RHS but still satisfies the constraint $\lambda \in (0, 1/b)$ ?","['statistics', 'probability', 'inequality']"
4534071,Global sections of pushforwards,"Let $X$ and $Y$ be projective schemes over $\mathrm{Spec}A$ , where $A$ is a ring. Let $\pi:X\rightarrow Y$ be a morphism. Let $\mathscr F$ be a coherent sheaf on $X$ . When is it true that $\Gamma(X,\mathscr F)\simeq \Gamma(Y,\pi_*\mathscr F)$ ?","['algebraic-geometry', 'coherent-sheaves']"
4534090,"Can we show: $\int_{-\pi/2}^{\pi/2}\sin\cos\tan x\cosh\sin\tan x\,\mathrm{d}x=\pi\sin(1/e)$ with contours?","$\newcommand{\d}{\,\mathrm{d}}$ By converting $\sin,\cos$ into their exponential forms and expanding the exponential into a series, it can be quickly shown using “normal” techniques that: $$\int_0^\infty\frac{\sin\cos x\cosh\sin x}{1+x^2}\,\mathrm{d}x=\frac{\pi}{2}\sin(1/e)$$ Which is equivalent to: $$\int_{-\pi/2}^{\pi/2}\sin\cos\tan(\vartheta)\cosh\sin\tan(\vartheta)\d\vartheta=\pi\cdot\sin(1/e)$$ This seems amenable to using contour integration (this approach is harder but of mathematical and - for me - educational interest). A notable complication is that this integral is improper. If we pass to $z=e^{it}$ , we must also take an improper contour integral, i.e: For $\varepsilon>0$ small, let $\gamma_{\varepsilon}$ be the contour $t\mapsto e^{it}$ restricted to $[\varepsilon-\pi/2,\pi/2-\varepsilon]$ . Then it suffices to show: $$2\pi i\cdot\sin(1/e)=\lim_{\varepsilon\to0^+}\oint_{\gamma_{\varepsilon}}2\sin\cosh\frac{z^2-1}{z^2+1}\cos\sinh\frac{z^2-1}{z^2+1}\cdot\frac{1}{z}\d z$$ We can use $\sin(a+b)+\sin(a-b)=2\sin(a)\cos(b)$ and $\cosh(x)+\sinh(x)=e^x,\,\cosh(x)-\sinh(x)=e^{-x}$ and instead ask to show: $$2\pi i\cdot\sin(1/e)=\lim_{\varepsilon\to0^+}\oint_{\gamma_{\varepsilon}}\frac{\sin(\exp(r(z)))+\sin(\exp(-r(z)))}{z}\d z=:\lim_{\varepsilon\to0^+}J_{\varepsilon}$$ Where $r(z)=1-2(1+z^2)^{-1}$ . What is promising about this: if one were to close the contour in such a way that it winds once around the origin, then the integral would evaluate to $2\pi i[\sin(e)+\sin(1/e)]$ by the residue theorem, $r(0)=-1$ . The standard thing to do here might be to add the contours $\gamma_{\pm}:[0,\pi]\to\Bbb C$ , $t\mapsto\pm i\mp\varepsilon e^{it}$ . Then: $$J_{\varepsilon}=2\pi i[\sin(1/e)+\sin(e)]+\oint_{\gamma_+\cup\,\gamma_-}$$ So we “just” need to show: $$\lim_{\varepsilon\to0^+}\oint_{\gamma_+\cup\,\gamma_-}\frac{\sin\exp(r(z))+\sin\exp(-r(z))}{z}\d z=-2\pi i\cdot\sin(e)$$ Furthermore, $r$ is even and $\gamma_+(t)=-\gamma_-(t)$ so  this symmetry allows us to instead show: $$\tag{$\ast$}\lim_{\varepsilon\to0+}\oint_{\gamma_+}\frac{\sin\exp(r(z))+\sin\exp(-r(z))}{z}\d z\overset{?}{=}-\pi i\cdot\sin(e)$$ But this is very difficult (for me). Substituting $z=\gamma_{\pm}(t)$ gives a very ugly mess. The asymptotics of the integrand are essentially $\sin\exp(O(\varepsilon^{-2}))$ and upon expansion into real and imaginary parts, we get a mess of $\sin(a)\cosh(b)+\cdots$ where $b=\sin(\sin(\sin(\cdot)))$ (more or less) and the asymptotics of that seem intractable. Equally, I might be overthinking this and getting lost in irrelevant details. A realised (?) version of our goal: $$\pi\cdot\sin(e)=\lim_{\varepsilon\to0^+}\varepsilon\cdot\int_0^\pi\frac{\sin\exp\left(1+\frac{2}{\varepsilon\cdot e^{it}(2i-\varepsilon\cdot e^{it})}\right)+\sin\exp\left(-1-\frac{2}{\varepsilon\cdot e^{it}(2i-\varepsilon\cdot e^{it})}\right)}{i\cdot e^{-it}-\varepsilon}\d t$$ The question: does anyone know how to continue, i.e. how to successfully show $(\ast)$ ? I’d really like to learn from this, as an example of more advanced asymptotic analysis than I’m used to. But perhaps it really is too difficult: in which case, I’d be happy to see alternative contour methods. Many thanks.","['complex-analysis', 'contour-integration', 'asymptotics']"
4534198,"Let $E,F$ be finite dimensional Banach spaces and define $\delta(E,F)=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\}$","Let $E,F$ be finite dimensional Banach spaces and define $\delta(E,F):=\inf\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is an isomorphism}\}$ . Prove that, $\delta(E,F)=1$ iff $E$ and $F$ are isometric. For any isomorphism $T:E\to F$ , $1=\lVert id_F\rVert=\lVert T\circ T^{-1}\rVert\le \lVert T\rVert\lVert T^{-1}\rVert$ , this implies $\delta(E,F)\ge 1$ . Suppose $T:E\to F$ is an isometry then $\lVert T\rVert=1=\lVert T^{-1}\rVert\implies\delta(E,F)\le1\implies\delta(E,F)=1$ . I'm stuck with the converse part. But I have observed the following- If there is an isomorphism $T:E\to F$ such that $\lVert T\rVert\lVert T^{-1}\rVert=1$ , then $$\lVert T^{-1}(Tx)\rVert\le \lVert T\rVert^{-1}\lVert Tx\rVert\implies \lVert T\rVert\lVert x\rVert\le \lVert Tx\rVert\le\lVert T\rVert\lVert x\rVert\implies \lVert Tx\rVert=c\lVert x\rVert\implies \lVert (c^{-1}T)x\rVert=\lVert x\rVert$$ where $c=\lVert T\rVert>0$ . Hence, $S:=c^{-1}T$ is isometry between $E$ and $F$ . But I don't know whether the set $\{\lVert T\rVert\lVert T^{-1}\rVert|\ T:E\to F\text{ is isomorphism}\}$ is closed or not, if yes then $1=\delta(E,F)$ belong to the set and the above observation will complete the proof. Can anyone help me to finish the proof? Thanks for your help in advance.","['operator-theory', 'isometry', 'functional-analysis', 'analysis']"
4534217,When are two functions related by a coordinate change? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Consider two $\underline{\text{injective}}$ functions $X: R^m \to R^n$ and $Y: R^m \to R^n$ with $m<n$ . How do I decide if there exists a (smooth, invertible) coordinate transformation $\sigma: R^m\to R^m$ such that $X=Y\circ \sigma$ ?","['coordinate-systems', 'functions', 'functional-analysis']"
4534232,Find the all real roots of the polynomial $x^6+3 x^5+3 x-1=0$ in closed form,"Find the all real roots of the polynomial $$x^6+3 x^5+3 x-1=0$$ in exact form. WolframAlpha gives only numerical results. I've asked a few similar questions before . The source of the problem comes from the algebra precalculus workbook (but not homework workbook). The rational root theorem doesn't work. Because, $x=1$ and $x=-1$ are not roots. Wolfram says , we have only $2$ real roots. What kind of factorization should I try?Can a factorization of the form $P(x)=(x^3+ax^2+bx+c)(x^3+dx^2+ex+f)$ work? I also tried the trick $\frac {P(x)}{x^n}$ , but I failed.","['irreducible-polynomials', 'algebra-precalculus', 'factoring', 'polynomials']"
4534269,Representing $\frac{1}{x^2}$ in powers of $(x+2)$,"I was asked to represent $f(x)=\frac{1}{x^2}$ in powers of $(x+2)$ using the fact that $\frac{1}{1 − x} = 1 + x + x^2 + x^3 + ...$ . I am able to represent $\frac{1}{x^2}$ as a power series, but I am struggling withdoing it in powers of $(x+2)$ . This is what I attempted. $I.$ Firstly, I used the simple expansion $$\ln(x+1) = \sum_{n=1}^\infty (-1)^n\frac{x^n}{n}$$ which comes from the fact that $\frac{1}{1 − x} = 1 + x + x^2 + x^3 + ...$ . I then noticed that $$\ln(x) = \ln(1+(x-1))=\sum_{n=1}^\infty (-1)^n\frac{(x-1)^n}{n}$$ $II$ . $-f(x)=-\frac{1}{x^2}$ is the second derivative of $\ln(x)$ , so that $$\frac{d}{dx}\sum_{n=1}^\infty (-1)^n\frac{(x-1)^n}{n}= \sum_{n=1}^\infty(-1)^n \frac{(x-1)^{n-1}}{n^2}$$ and $$\frac{d}{dx} \sum_{n=1}^\infty(-1)^n \frac{(x-1)^{n-1}}{n^2} = \sum_{n=1}^\infty(-1)^n \frac{(x-1)^{n-2}}{n^2(n-1)}=-\frac{1}{x^2}$$ from what plainly follows that $$\frac{1}{x^2} = -\sum_{n=1}^\infty(-1)^n \frac{(x-1)^{n-2}}{n^2(n-1)}$$ $III.$ Of course, this expansion is not in powers of $(x+2$ ). What I pressumed would be the logical thing to do is to repeat these steps but, instead of the using $\ln(1 + (x-1))$ , using the equivalent $$\ln(-2 + (x+2))=\ln((-2)(1+\frac{x+2}{(-2)})) = \ln(-2)+\ln(1+\frac{x+2}{(-2)})$$ I expected to be able to use this because the second term is of the form $\ln(1+ u)$ where $u$ is some function of $x$ , and we know the expansion of such expression. However, $\ln(-2)$ is nonsense, since $\ln(x)$ is defined only for $\mathbb{R}^+$ . Is there an alternative, better way to do this or am I missing something? Thanks.","['calculus', 'logarithms', 'taylor-expansion', 'sequences-and-series']"
4534306,Proving a chromatic number upper bound for a graph with a planar subgraph,"I have the following problem: Let $G$ be a graph such that for any partition of its vertices into two sets, the induced subgraph on either of the sets is going to be planar. Prove that $\chi(G) \leq 11$ I cannot use the four-color theorem for this problem, so I suppose I need to use some conditions for planarity like the Kuratowski's theorem, but I'm not sure how.","['coloring', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'planar-graphs']"
4534327,Symmetries of Tetrahedron,"I know there are 12 rotational symmetries of a regular tetrahedron ${T}$ (including the identity $e$ ), but I can only find 9 of them. The first 8 are the $r = \pi/3$ rad rotations with 1 vertex fixed at a time. There are 4 vertices of interest, and there can only be 2 angles of rotation for each case ( $r=\pi/3, r^2 = 2\pi/3$ , with $r^3=e$ ), so then there are $2\cdot 4 = 8$ reflectional symmetries, plus the identity $e$ , this gives $8+1=9$ symmetries. What are the other 3? And I know there are 12 reflectional symmetries of $T$ as well.
I know I can reflect about the axes through the top vertex and the midpoints of bottom edge (there are 3 such axes). I can reflect about the axes that pass through a bottom vertex to the midpoint of the opposite face (3 more axes). Each reflection can only be done once, so this gives me 6 reflections. What could the other six be?","['group-theory', 'symmetry', 'geometry']"
4534342,Is $A\times B\times C$ equal to $(A\times B)\times C$?,"I'm rather new to set theory, and I have a question about Cartesian product. Let $A$ , $B$ , and $C$ be sets. Is $A\times B\times C$ equal to $(A\times B)\times C$ ? I think the answer is false . My understanding is as follows: For example, let $A=\{0\}$ , $B=\{1\}$ , and $C=\{2\}$ . Then $$A\times B\times C=\big\{(0, 1, 2)\big\},$$ whereas $$(A\times B)\times C=\big\{((0, 1), 2)\big\}.$$ I believe these are different, but it seems counter-intuitive to me that one additional pair of parenthesis can change the entire expression. Is my understanding correct?",['elementary-set-theory']
4534387,Nature of ODE $\dot x=x^2-\frac{t^2}{1+t^2}$,"Discuss the equation $\dot{x}=x^2-\frac{t^2}{1+t^2}$ . Make a numerical analysis. Show that there is a unique solution which asymptotically approaches the line $x=1$ . Show that all solutions below this solution approach the line $x=$ $-1 .$ Show that all solutions above go to $\infty$ in finite time. This is a problem of Teschl ODE qn 1.30 For the numerical analysis, we have the nullclines are $x_{+}=\frac{t}{\sqrt{\left(1+t^{2}\right)}}$ and $x_{-}=-\frac{t}{\sqrt{\left(1+t^{2}\right)}}$ which are supersolution and subsolution respectively which has given me 3 regions see the picture attached. We can focus on $t\geq 0$ part only. Now I can also see from xpp that the behaviour of the solution goes in the way I am unable to make the proof rigorous. I know the theorem: Let $x_{+}(t), x_{-}(t)$ be super, sub solutions of the differential equation $\dot{x}=f(t, x)$ on $\left[t_0, T\right)$ , respectively. Then for every solution $x(t)$ on $\left[t_0, T\right)$ we have $$
x(t)<x_{+}(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \leq x_{+}\left(t_0\right)
$$ respectively $$
x_{-}(t)<x(t), \quad t \in\left(t_0, T\right), \quad \text { whenever } \quad x\left(t_0\right) \geq x_{-}\left(t_0\right)
$$ but I am unable to find other good super and subsolutions to make it work. Let me know if I need any other theorems. Also, my guess is that the one unique solution starts at an irrational initial point $\alpha$ which is in between $0.544592<\alpha<0.54453$ . I got that while playing with xpp.
see this More Information after trying: So I am guessing that $x=-\sqrt{-c+\frac{t^{2}}{1+t^{2}}}$ is a super solution or not, where $c\in (0,1)$ . If it is so then it will help me with part c of the problem. But the equation is getting cubic. If you are also trying in the same direction let me know.","['ordinary-differential-equations', 'analysis', 'numerical-methods', 'nonlinear-dynamics', 'dynamical-systems']"
4534445,Formulae for projectile motion with resistance proportional to velocity,"What are the formulae for resisted projectile motion in which the resistance is proportional to the velocity? I have a problem where my answers don't match up with the textbook answers and I need to verify with formulae. I need the velocities and positions with respect to time for both components, for future readers as well. The projectile is launched from the ground, at an angle $\theta$ and velocity $u$ :","['integration', 'projectile-motion', 'ordinary-differential-equations']"
4534482,Can I recover the Zariski open subobjects from the Grothendieck topology they generate?,"Let $\mathbf{cRing}$ be a category of commutative rings and let $\mathbf{Set}$ be a category of sets relative to which $\mathbf{cRing}$ is small (Grothendieck universes). The opposite $\mathbf{Aff}$ of the category of commutative rings becomes a site when we equip it with the Grothendieck topology generated by the pretopology consisting of families $\{\,R \to R[s_i^{-1}]\,\}_i$ such that the $s_i$ generate the unit ideal $(1)$ of $R$ . The topology is subcanonical and the Yoneda embedding makes $\mathbf{Aff}$ a full subcategory of the topos $\operatorname{Sh}(\mathbf{Aff})$ . I denote the Yoneda embedding by $R\mapsto \operatorname{Spec}R$ . I call the objects of the presheaf category $\operatorname{Pr}(\mathbf{Aff})$ Z-functors, and I call the objects of the sheaf topos Zariski-local Z-functors. The Grothendieck topology on $\mathbf{Aff}$ induces a Lawvere-Tierney topology $j: \Omega \to \Omega$ on the subobject classifier of the presheaf topos and the Zariski-local Z-functors are precisely the sheaves for this topology. The open subfunctors of an affine Z-functor $\operatorname{Spec}R$ are by definition (lecture notes by Marc Nieper-Wißkirchen) those of the form $DI\hookrightarrow \operatorname{Spec}R$ , where $I$ is an ideal of $R$ and $(DI)A = \{\phi^*: R \to A\,|\,\text{$\phi^*I$ generates (1)\}}$ . The closed subfunctors of $\operatorname{Spec}R$ are up to isomorphism of the form $\operatorname{Spec}R/I \to \operatorname{Spec} R$ . Once it is clear what the open/closed subobjects of the representable are, one can define topologies on all Z-functors and one can define schemes. Question: Is there a way to recover the open and closed subobjects from the topology $j$ on the presheaf category? Edit: I am sorry, I left out important context. I am reading A functional approach to General Topology , and on page 114 section 2.5. the authors hint that it is possible to get a notion of closed maps from a topology on an elementary topos. According to them the closed maps $f: X \to Y$ are those for which both image $\Sigma_f$ and preimage $f^*$ in the subobject fibration commute with the closure operator induced by $j$ . I would like if someone with experience to tell me if this will (probably) give me the closed maps I want or something else entirely.","['grothendieck-topologies', 'category-theory', 'zariski-topology', 'algebraic-geometry', 'schemes']"
4534513,Does convergence of arithmetic mean imply convergence of geometric mean?,"Let $x_n$ be a sequence of positive real numbers, which is not convergent. ( $x_n$ does not converge to a finite number, nor to infinity). Define $$ A_n = \frac{x_1 + x_2 + \cdots + x_n}{n} \quad \text{ and }\quad G_n = \sqrt[n]{x_1x_2...x_n}.$$ Does the convergence of $A_n$ imply the convergence of $G_n$ or vice versa? I know that if $x_n \to L \in \mathbb{R}\cup\{\infty\}$ , then both $A_n,G_n$ converge to $L$ . But here I assume $x_n$ is not convergent. I also wonder if adding a boundedness assumption on $x_n$ changes anything.","['means', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4534521,When is a line really straight?,"I have researched the issue and found some confusion, which seem determined by the vagueness of the definitions, or even absence of definition (in a comment in link 1): the term line is used both for any line and, mostly  as a synonym of straight line a straight line is also defined in US as: ""* a set of points extending in both directions containing the shortest path between any two points on it.*"", (quoted here !: What is a straight line? ), but this applies only to a plane, and is usually considered infinite ( https://en.wikipedia.org/wiki/Line_(geometry) ): "" In geometry, a line is an infinitely long object with no width, depth, or curvature "", according to that Meridians should NOT be classified as lines but as segments the University of Cornell site ( https://pi.math.cornell.edu/~dwh/books/eg99/Ch02/Ch02.html ) says: "" Great circles are those circles which are the intersection of the sphere with a plane through the center of the sphere. Examples: Any longitude line and the equator are great circles on the earth ... The first step to understanding this problem is to convince yourself that great circles are straight lines on a sphere "". and on this site ( Straight Line on a Sphere? ) it is stated  that only great circles are straight lines, but this extends to a curved plane the property of a flat plane. The shortest distance between 2 points is a real straight line going through the earth does this picture show that Parallels are as straight ad the Equator? questions: I am sure the confusion stems from the fact that Euclid was concerned with lines on a plane, and modern geometric ignored the problem. Am I wrong? why shouldn't the Tropic of Cancer line (and other parallels) be considered  a straight line? by the same principle can we define Parallel as really parallel lines? a line can bend on the Euclid (xy axes) plane and so it is a curved line, but if it is straight and it (or the whole plane) bends on the z axis must be considered straight or curved? I think it should be considered straight, and the Euclid line should be renamed, e.g. fully straight",['geometry']
4534562,Connection between thinking of an ellipse as a squished circle and the formal definition,"If I think of an ellipse fundamentaly as a squished circle. For exemple, I have a initial circle $x^2+y^2=1$ and I morph into $x^2+ 2y^2=1$ . How can I see, intuitively, that for all points on the curve, the sum of the two distances to the focal points is a constant like the formal definition of an ellipse demands?","['analytic-geometry', 'conic-sections', 'circles', 'geometry', 'intuition']"
4534660,Show that $|x_{k+1}-x_k| \leq 1$ (for $0<k<n$) implies $\sum_{k=1}^n |x_k| - \left|\sum_{k=1}^n x_k\right|\leq\lceil(n^2-1)/4\rceil$.,"Let $n\ge 1$ be a positive integer and let $x_1,\cdots, x_n$ be real numbers so that $|x_{k+1}-x_k|\leq 1$ for $k=1,2,\cdots, n-1$ . Show $$\sum_{k=1}^n |x_k| - \left|\sum_{k=1}^n x_k\right|\leq \left\lceil \frac{n^2-1}4\right\rceil.$$ Observe that by replacing $(x_1,\cdots, x_n)$ with $(-x_1,\cdots, -x_n)$ , which changes neither the condition that $|x_{k+1}-x_k|\leq 1$ for $1\leq k < n$ nor the inequality to be proven, we can assume there are at most as many positive as negative terms (e.g. one would do such a replacement if this is not the case). So if $P$ denotes the multiset of positive $x_k$ 's and $N$ denotes the multiset of negative $x_k$ 's, then $|P|\leq (n-1)/2$ . Let $(a_1,\cdots, a_n)$ be a permutation of $(x_1,\cdots, x_n)$ in nondecreasing order. Then the elements of $P$ are $a_{k_0+1}\leq a_{k_0+2}\leq \cdots \leq a_{k_0+l}$ for some $l\ge 0$ and some $k_0 > 0$ . I think that for any $1\leq i\leq n-1,$ there exist adjacent terms $x_j$ and $x_k$ so that $x_j\leq a_i$ and $x_k\ge a_{i+1}$ . So $0\leq a_{i+1}-a_i\leq 1$ by the problem condition. But how would one prove this? With the above claim and with notation defined so that $\sigma(P)$ is the sum of the terms in $P$ and $\sigma(N)$ is the sum of the terms in $N$ , we then have that the LHS equals $(\sigma(P) - \sigma(N)) - |\sigma(P) + \sigma(N)|.$ But I'm not sure how to simplify this either.","['permutations', 'algebra-precalculus', 'summation', 'inequality']"
4534666,Which integer matrices are $k$th powers for all $k$?,"Problem 11401 of AMM (the American Mathematical Monthly ) states: Let $A$ be a nonsingular square matrix with integer entries. Suppose that for every positive integer $k$ , there is a matrix $X$ with integer entries such that $X^k = A$ . Show that $A$ must be the identity matrix. What if we assume $\det A=0$ ?  For example, any idempotent matrix $A=A^2$ is a $k$ th power for all $k$ , but are there any other matrices?","['matrices', 'linear-algebra', 'contest-math']"
4534674,"Prove there exist numbers $a_i, b_j$ so that $x_{ij}=a_i + b_j$ for all $1\le i,j\le n$ if any full tour costs the same","In a country with $n$ towns the cost of travel from the $i$ -th town to the $j$ -th town is $x_{ij}$ . Suppose the total cost of any route passing through each town exactly once and ending at its starting point doesn't depend on which route is chosen. Prove there exist numbers $a_1,\cdots, a_n, b_1,\cdots, b_n$ so that for all $1\leq i, j \leq n, x_{ij}=a_i + b_j$ . Let $f(a,b)=x_{a1}+x_{1b}-x_{ab}$ where $a,b,1$ are distinct. I think $f(a,b)$ is independent of $a$ and $b$ but I'm not sure how to show this. I know $f(a,b)=f(b,c)$ when $a,b,c,1$ are all distinct, and this can be shown by assuming WLOG that $a<b<c$ and considering the two routes $a,b,1,c,2,\cdots, a-1,a+1,\cdots, b-1,b+1,\cdots, c-1,c+1,\cdots, n,a$ and $a,1,b,c,2\cdots, a-1,a+1\cdots, b-1,b+1,\cdots, c-1,c+1,\cdots, n,a.$ The difference between their costs is $x_{ab}+x_{b1}+x_{1c}-x_{a1}-x_{1b}-x_{bc},$ which must be zero by the problem assumption, and rearranging yields $f(a,b) = f(b,c)$ . But is it true that $f(a,b)=f(b,a)$ for $a,b,1$ distinct (*), and if so how would one show this? I tried a similar approach to above, but I didn't make much progress. Provided (*) holds, we have for $a,b,c,d,1$ all distinct that $f(a,b)=f(b,c)=f(c,d) = f(d,c), f(a,b)=f(b,a)=f(a,c)=f(c,a), f(a,b)=f(b,c)=f(c,b),$ which proves the desired independence. Provided $f(a,b)$ is independent of $a$ and $b$ , for any $1\lt i < j \leq n$ , we have $x_{ij} = x_{i1} + x_{ij} - x_{i1} + x_{1j}-x_{1j} = -f(i,j) + x_{i1} + x_{1j}$ . So letting $F$ denote the constant value of $f(i,j)$ , we can let $a_i = -F + x_{i1}$ and $b_i = x_{1i}$ for $i>1$ and $a_1 = 0, b_1 = F$ .","['contest-math', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'recreational-mathematics']"
4534677,"Why is $\textbf{E}[X + Y \mid X,Y] = X + Y$?","Intuitively, it seems obvious, but I am struggling to prove it for the case where $X_1,...,X_n$ are continuous random variables. I am aware that $E[c(X)|X]=c(X)$ . So how would one show that $E[c(X_i)|X_1,...,X_n]=c(X_i)$ and that $E[c(X_i)+a(X_j)|X_1,...,X_n]=c(X_i)+a(X_j)$ for $i,j=1,...,n$ ? The reason I am asking is that, given a linear regression model $y = \beta_0 + \beta_1 x_1 + \beta_2 x_i + u$ where $y,x_1, x_2, u$ are random variables, it is often said that $E[y|x_1,x_2]=\beta_0+\beta_1 x_1 + \beta_2 x_2 + E[u|x_1,x_2]$","['statistics', 'conditional-expectation']"
4534721,Integrability condition in Jensen's inequality,"I'm reading about Jensen's inequality from a lecture note : If $X$ is a random variable and $\psi: \mathbb{R} \rightarrow \mathbb{R}$ is convex and such that $\mathbb{E}(|\psi(X)|)<\infty$ , then $$
\psi(\mathbb{E}(X)) \leq \mathbb{E}(\psi(X)) .
$$ In particular, $|\mathbb{E}(X)| \leq \mathbb{E}(|X|)$ . It seems there is no guarantee that "" $\psi(X)$ is integrable implies $X$ is integrable"". As such, we don't know whether $\mathbb{E}(X)$ and thus $\psi(\mathbb{E}(X))$ are well-defined or not. Could you confirm if my understanding is correct?","['inequality', 'jensen-inequality', 'probability-theory']"
4534722,What does normalizing a real valued function mean?,"I might be wrong because I didn't find a proper definition in a google search but what I have found is that if we have a real-valued function $f:[0,1]\to\mathbb{R}$ defined by $f(x)=x^2$ then normalizing this function means the integral value should be $1$ .Like here if I integrate then we have $\int_{0}^{1}x^2dx=\frac{1}{3}$ so normalizing this function means we want another function say $g$ which is isomorphic to $g$ such that $\int_{0}^{1}g(x)=1$ . If whatever I have written is correct then I want to know why we are doing this. If whatever I have written is not correct then please if someone gives the correct meaning or definition of normalizing a function and also how to normalize such function, that will be a great help, Thanks.","['functions', 'functional-analysis', 'terminology', 'real-analysis']"
4534729,"Does $W^{1,1}([a,b])$ compactly embed in $L^1([a,b])$?","I'm trying to prove the Remark at page 274 of the second edition of the Book ""Partial Differential Equations"" by Evans Lawrence, the Remark uses the Theorem 1 at page 272 (Rellich-Kondrachov Compcactness Theorem), here I quote the theorem, the remark and the definition of compactly embedding Definition of compactly embedding Let $X$ be a Banach space with norm $||\,\cdot\,||_X$ , let $Y$ be a Banach space with norm $||\,\cdot\,||_Y$ .
One says that $X$ compactly embeds into $Y$ , which is denoted by the simbol $X \subset\subset Y$ , if and only if $X \subset Y$ there exists $C > 0$ such that $||u||_Y \leq C||u||_X$ $\forall u \in X$ each bounded sequence in $X$ has a convergent subsequence in $Y$ (Rellich-Kondrachov Compactness Theorem)
Assume $U$ is a bounded $C^1$ open set of $\mathbb{R}^n$ , suppose $1 \leq p < n$ , then $W^{1,p} \subset\subset L^q(U)$ for each $1 \leq q < p^*$ Remark. Observe that since $p^* > p$ and $p^* \to \infty$ as $p \to n$ , we have in particular $W^{1,p}(U) \subset\subset L^p(U)$ for all $1 \leq p \leq \infty$ Observe that if $n < p \leq \infty$ , this follows from Morrey's inequality and the Ascoli-Arzela compactness criterion) The problem is that the proof of the Remark doesn't work when $p = n  = 1$ , this is because you can't use Rellich Kondrachov Theorem because there is no $p$ such that $1 \leq p < n$ because $n = 1$ , I asked to my professor how to prove the Remark in this case and he told me that the proof of the Rellich-Kondrachov Compactness Theorem (the one on the Evans' Book) can be modified to prove the Remark in this case, but I don't get how to do it. Therefore my question is : Is it true that $W^{1,1}(U)$ compactly embeds in $L^1(U)$ when $U$ is an open $C^1$ bounded subset of $\mathbb{R}^1$ ?? How can I prove it or disprove it?? Clearly it would be enough to prove the statement in the case where $U = (a,b)$ , so you can assume it if you wish.","['banach-spaces', 'measure-theory', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
4534791,Show absolute value of determinant of a prime matrix is a perfect square.,"Assume we have a matrix $A$ of the following form $$ 1\leq i,j \leq n \hspace{0.5cm} A = [a_{ij}] \in M_n(\{0,1\}) \hspace{0.4cm}a_{ij} = \begin{cases} 1 \hspace{0.3cm}i+j \in  \mathbb{P} \\ 0 \hspace{0.3cm} i + j \notin  \mathbb{P} \end{cases} $$ where $\mathbb{P}$ denotes the set of prime numbers. How can I prove the absolute value of its determinant is a perfect square? So far, I've managed to figure out it's a symmetric matrix and has zeros on its diagonal except at $a_{11}$ , which isn't much, and I haven't even made use of the prime property.","['determinant', 'matrices', 'square-numbers', 'linear-algebra', 'prime-numbers']"
4534888,"find numbers x,y,z that achieve maximum possible value of $E(x,y,z) = \alpha(x-y)^2 + \beta(y-z)^2 + \gamma (z-x)^2$","Let $\alpha, \beta, \gamma$ be positive real numbers and let $[a,b]$ be an interval. Find numbers $x,y,z\in [a,b]$ so that $E(x,y,z) =  \alpha(x-y)^2 + \beta(y-z)^2 + \gamma (z-x)^2$ is maximum. I think we can assume WLOG that $\alpha \leq \beta\leq \gamma.$ But how would one justify the above assumption? Also, I think one can assume $x\leq y\leq z$ . As for the justification, under the assumption that $x\leq y\leq z,$ one can see that $E(x,y,z) \ge E(y,x,z)$ and $E(x,y,z) \ge E(z,y,x).$ But $E(x,y,z) - E(z,y,x) = (\alpha - \beta)((x-y)^2 - (y-z)^2),$ and it's not clear that this value is nonnegative (if one only makes the above two additional assumptions). With the above two assumptions, note that a convex function on a bounded interval attains its maximum at one of the endpoints. It might be useful to define a function like $f_1(y) = \alpha(y-a)^2 + \beta(y-b)^2,$ which can be used to maximize $E(a,y,b)$ where $y$ ranges over $[a,b]$ . But must the maximum value of $E$ be obtained at a point of the form $(a,y,b)$ or $(b,y,a)$ ?","['inequality', 'real-analysis', 'calculus', 'optimization', 'algebra-precalculus']"
4534907,Is the pdf at infinity equal to 0?,"Is it always the case that given a continuous probability density function we have: $$\lim_{x\to\infty} f(x) = 0$$ If not, what are necessary and sufficient conditions so that is the case?","['probability-theory', 'probability']"
