question_id,title,body,tags
4914417,How to come up with a â€œgerrymanderingâ€ index / â€œroughnessâ€ index of some closed $E \subseteq \mathbb{R}^n$ of finite area,"The answer to this question is not objective, but I believe this to be an interesting mathematical problem, which has different approaches one may take to arrive at an answer, and Iâ€™m interested to see what others may come up with. Gerrymandering is the process of drawing boundaries of constituencies in such a manipulative way such that a certain political party or group is favoured disproportionately to their size. Often a district can very obviously be seen to be gerrymandered if it appears to be an â€œuglyâ€, â€œroughâ€, â€œunnaturalâ€ shape - for example, very uneven borders and panhandles that stick out of it. The 12 Most Gerrymandered Districts In America - Ranker gives the following example of Marylandâ€™s 3rd district: In reality, things may be more complicated and a â€œniceâ€ looking shaped district may actually be heavily gerrymandered if it happens to over/underrepresent a certain demographic. For our purposes we will ignore such complexity and focus solely on the shape of a district. Formally, let $E \subseteq \mathbb{R}^2$ be a closed set of finite area. Iâ€™m wondering if we can come up with some index $$g: \{ \text{such subsets $E \subseteq \mathbb{R}^2$} \} \to [0,1]$$ such that a â€œnicely shapedâ€ $E$ , such as a near-circle/square/etc. has $g(E) \approx 0$ , and a â€œroughly shapedâ€ $E$ , such as the shape of Marylandâ€™s district 3 above, has $g(E) \approx 1$ . Some properties $g$ should have are that it should be invariant under scaling/reflection/rotation/translation of the shape $E$ , as such transformations make the shape no better or worse. An example of an attempt An attempt at a solution to this problem is to let $g(E)$ be one minus the probability that, picking two points $x_1, x_2 \in E$ uniformly at random, that the line connecting $x_1$ and $x_2$ lies entirely within $E$ . One positive of this index is that all convex $E$ have $g(E) = 0$ , i.e. not at all gerrymandered - a value we may want since convex sets have smooth boundaries, and are path-connected. $g$ is also invariant under rotations/translations/etc. A disadvantage of this index is a district may be considered heavily gerrymandered if there is a large hole, or several large holes within it, perhaps representing large lakes/seas in a real world setting. Also, â€œsmoothâ€ districts that are very much non-convex, such as a banana-shaped district, may be considered heavily gerrymandered. What Iâ€™m looking for in an answer Iâ€™m wondering if: There are any standard indexes for this problem or similar problems that someone has previously come up with Anyone has a particularly interesting idea for such an index. NB. We may wish to extend this question to $E \subseteq \mathbb{R}^n$ rather than $E \subseteq \mathbb{R}^2$ in a natural way.","['recreational-mathematics', 'mathematical-modeling', 'problem-solving', 'geometry']"
4914418,Direction for solving 3rd order non-linear ode $f(f''' - f'') + f'^2 = 0$,"What the title says - I was playing around with an equation and I ended up with the following $$ f(f''' - f'') + f'^2 = 0$$ It's nice because all the coefficients are $1$ I think, but that might not be exactly true either because of how it's multiplied. I've played around with it a bit but it's weird enough that I can't really make much progress on this. I did find that a solution for this is $f = c$ where $c$ is a constant. Then I get $c(0) + 0 = 0$ which is true but also doesn't feel like it's a complete solution. Is there a chance I could get some advice on how to proceed with this?","['calculus', 'ordinary-differential-equations']"
4914457,$10$-digit numbers divisible by $66667$.,"Let us consider the set $S$ of $10$ -digit numbers whose digits are only allowed to be taken from the set $\{3,4,5,6,7,8\}$ . Let $S'$ be the subset of $S$ such that its elements are divisible by $66667$ . Then what is the cardinality of $S'$ ? I think this problem is similar to This one , so I try to solve this problem as follows: Let $abcdefghij$ be an element of $S'$ . Then we have that $3\cdot 66667\cdot abcde-2\cdot(100000\cdot abcde+fghij)$ is also divisible by $66667$ . We can thus write $$
abcde=2\cdot fghij+k\cdot 66667,
$$ where $k$ is an integer. Note that both $5$ -digit numbers are greater than $30000$ and are
less than $100000$ , so we further see that it is only possible for the the cases when $k=0,-1,-2$ . Unfortunately, I got stuck here. I think the factor $2$ before $fghij$ is annoying, and I have no idea how to step forward then. Thanks in advance for any helpful answer.","['elementary-number-theory', 'algebra-precalculus', 'combinatorics']"
4914465,Associativity of infinite products,"It is well-known that if $\sum_{n=1}^\infty a_n$ is an absolutely convergent complex series and $\mathbb N$ is partitioned as $J_1,J_2,\dots$ , then the series $\sum_{j\in J_n}a_j$ for all $n$ and $\sum_{n=1}^\infty\sum_{j\in J_n}a_j$ are both absolutely convergent, and $\sum_{n=1}^\infty a_n=\sum_{n=1}^\infty\sum_{j\in J_n}a_j$ . I am trying to make sense of the same identity for infinite products. The convention I'm using is the following The product $\prod_{n=1}^\infty z_n$ is called convergent if there exists an $n_0$ such that the partial products $\prod_{j=n_0}^nz_j$ converge to a non-zero number as $n\to\infty$ . Write $z_n=1+w_n$ , the product is said to converge absolutely if $\prod_n(1+|w_n|)$ converges. It is a theorem that an absolutely convergent product is convergent and that $\prod_n(1+|w_n|)$ converges if and only if $\sum_n|w_n|$ does. Suppose $\prod_{n=1}^\infty z_n$ converges absolutely and $\Bbb N=\bigcup_{n=1}^\infty J_n$ is a partition, I want to show that $$\prod_{n=1}^\infty z_n=\prod_{n=1}^\infty\prod_{j\in J_n}z_j,$$ with the RHS converging absolutely. By the characterisation of absolute convergence, we know that $\sum_{n=1}^\infty|z_n-1|$ converges, so the series $\sum_{j\in J_n}|z_j-1|$ converge for all $n$ and hence all the products $\prod_{j\in J_n}z_j$ converge absolutely. What I'm having trouble with is showing $\prod_{n=1}^\infty\prod_{j\in J_n}z_j$ converges (absolutely or not) and showing it is equal to the original product. By the characterisation of absolute convergence, it would be enough to show that $\sum_{n=1}^\infty|(\prod_{j\in J_n}z_j)-1|$ converges, but this has no discernible relation to the series $\sum_{n=1}^\infty\sum_{j\in J_n}|z_j-1|$ , which we do know converges by the ""associativity"" for absolutely convergent series.","['complex-analysis', 'absolute-convergence', 'sequences-and-series', 'infinite-product', 'convergence-divergence']"
4914472,What is the geometric explanation for why the shape operator is symmetric?,"The shape operator at acts on the tangent space of a manifold at $p$ : $S_p: T_pM \rightarrow T_pM$ By: $$S_p(v) = D_v(n(p))$$ That is, it sends the normal vector at $p$ to it's directional derivative in the direction of the input vector. The proof that this map is symmetric follows easily from the fact that partial derivatives commute, but I was hoping that somebody here could tell me why the geometric reason why this map is symmetric w.r.t. an inner product on the tangent space: $$S_p(V) \cdot U = V \cdot S_p(U)$$ The eigenvalues of this map are the principle curvatures and the direction of the eigenvectors are the principle directions.","['submanifold', 'riemannian-geometry', 'differential-geometry']"
4914495,Crazy integral with nested radicals and inverse sines,"Recently a friend who is writing a book on integrals added this problem to his book: $$\int_{0}^{1}\arcsin{\sqrt{1-\sqrt{x}}}\ dx=\frac{3\pi}{16}$$ After a while, when trying to generalize, I was able to find the following integrals: $$\int_{0}^{1}\arcsin{\sqrt{1-\sqrt{1-\sqrt{x}}}}\ dx=\frac{61\pi}{256}$$ $$\int_{0}^{1}\arcsin{\sqrt{1-\sqrt{1-\sqrt{1-\sqrt{x}}}}}\ dx=\frac{12707\pi}{65536}$$ For this last integral, simply apply a u-substitution with the term inside the arcsin: $$\int_{0}^{1}16u(1-u^2)(1-(1-u^2)^2)(1-(1-(1-u^2)^2)^2)\arcsin{u}\ du$$ Applying integration by parts and some algebra: $$\int_{0}^{1}\frac{(1-(1-(1-u^2)^2)^2)^2}{\sqrt{1-u^2}}\ du$$ With $\sin{t} = u$ , the integral becomes: $$\int_{0}^{\frac{\pi}{2}}(1-(1-\cos^4{t})^2)^2\ dt$$ Which can be easily calculated with some formulas for the beta function. However, when trying to solve the problem for n radicals, I was not able to solve this last step in a more general way, furthermore, the integrals with more radicals quickly become too large to calculate manually. Is it possible to calculate integrals for n radicals with some more simplified expression?","['integration', 'nested-radicals', 'inverse-trigonometric-functions', 'definite-integrals']"
4914555,"If sets $X$ and $Y$ have at least two elements each, then $X \cup Y \preceq X \times Y$","This is a problem from Derek Goldrei's Classic Set Theory, a book that I am currently working through. This is in a chapter titled 'Cardinals (without the Axiom of Choice)' in case that context is necessary. If sets $X$ and $Y$ have at least two elements each, then $X \cup Y \preceq X \times Y$ I've been struggling to come up with an injection to prove this. I understand what it's saying is analogous to saying that $x + y \leq xy$ when $x$ and $y$ are both greater than or equal to $2$ , and I thought by writing that proof out I would get some insight into proving the original statement, but that did not happen. I have written out a few finite examples and of course the injections are immediately spotted, but I haven't been able to form a general function. Going straight from $X \cup Y$ to $X \times Y$ doesn't seem feasible because there will be multiple ordered pairs for any given element in $X \cup Y$ . So I believe there will be some intermediate set(s) that I need to map $X \cup Y$ to in order to eventually map to $X \times Y$ such that all of the intermediate mappings are injections. I'm not necessarily looking for a full solution, more so a nudge in the right direction. Thanks.","['elementary-set-theory', 'intuition']"
4914605,Decomposition of function into products,"Given a single variable function $f(x)$ , is there a way of decomposing it into the product of a family of function. Something similar to, $$f(x) = \prod_n p^{a_n}_n(x)$$ I am trying to find the multiplicative counterpart to Fourier or Taylor expansions. Is this useful? What is the nearest thing that comes to mind on seeing this kind of thing? (a friend of mine pointed at the Dyson series)","['functions', 'fourier-series', 'infinite-product', 'physics', 'mathematical-physics']"
4914648,I think I don't truly understand Cauchy's Integral theorem,"Cauchy's theorem states that closed line integral of some holomorphic functions yields zero, in some good regions (i.e. simply connected domain). More explicitly, $$
\oint_\gamma f(z) d z=0.
$$ Many textbooks use Goursat's theorem or Green's theorem to prove this. I understand both proofs and can write it myself. Example: in Stein and Shakarchi's Complex Analysis the proof that the integral over a triangle of a holomorphic function is zero is due to a limiting argument. While this proof is elegant and slick, it doesn't give me any intuition of why the integral is zero. Are there any intuitive interpretations of this theorem that might help my understanding? Something that might explain why these integrals turn out to be zero while others are not?","['complex-analysis', 'contour-integration', 'complex-integration']"
4914671,Show that $\displaystyle\lim_{n \to \infty} \prod_{k=1}^{n} \left(1-2^{k-1-n}\right)$ exists and strictly positive.,"I have a question regarding limit calculations. As part of something I'm working on, I faced the challenge of calculating the limit $$
\lim_{n \to \infty}\prod_{k = 1}^{n} \left(1 - 2^{k-1-n}\right)
$$ This was challenging for me and I couldn't solve this by now, although numerical calculation shows the limit exists and is strictly positive $\left(\approx 0.289\right)$ . I couldn't prove it. For my purposes it is sufficient to show the limit exists and is strictly positive. I will appreciate any help.","['analysis', 'real-analysis', 'calculus', 'sequences-and-series', 'limits']"
4914688,Making clear the definition of 'affine variety' in Mumford's book.,"I am reading ""The Red Book of Varieties and Schemes"" by Mumford. In section 4 the author defines the term affine variety : An affine variety is a topological space $X$ plus a sheaf of $k$ -valued functions $\mathcal{O}_X$ on $X$ which is isomorphic to an irreducible algebraic subset of some $k^n$ plus the sheaf just defined. There are a few things that are not clear to me. When it says ""which is isomorphic to"", what is the ""which"" referring to? To the whole pair $(X,\mathcal{O}_X)$ or just $X$ ? In any case, what is exactly the notion of ""isomorphism"" in this definition (and between what elements are we considering such isomorphism)? Is ""the sheaf just defined"" refering to the sheaf of regular functions $\mathcal{O}_X$ ? In the book, no other sheaf is considered (apart from the examples given after the definition of sheaf). I feel like I need some insight overall. Is the advantage of this definition that we ""forget"" about the ambient space $k^n$ and we look at affine varieties as topological spaces + a sheaf? If yes, why does the sheaf provide that much information? Thanks a lot. Comments to give some context to the topic, even if they are not directly related to the question, are welcome.","['affine-varieties', 'definition', 'algebraic-geometry', 'intuition', 'sheaf-theory']"
4914694,Parabola as a limit of sequence of function $f(x)=\sum_{k=-n}^{n} |x+k|$,"I was fooling around with the following expression: $|x-3|+|x-2|+|x-1|+|x|+|x+1|+|x+2|+|x+3|$ the graph looks something like this: similarly for y= $\sum_{i=-5}^{5} |x-i|$ Now if we increment by only 0.01(ie. $y= \sum_{i=-10}^{10} |x-0.01i|$ but vary i from  -10 to 10 we get: Now my question is , do these graphs $ approximately$ represent parabolas as the value of i gets larger and the increment (call it h) gets smaller? I think of something to do with reimann sums but my efforts till now have proved to be futile. Because this sequence of functions looks like parabolas to me. Or are they completely different like $cosh(x)$ ?Or maybe thereis no pattern at all and this is just a coincidence?","['limits', 'calculus', 'functions']"
4914726,How to derive likelihood function,"I have been struggling a lot with the concept of likelihood and I'd really appreciate it if someone could verify if my understanding is correct and give input. If I understand this correcly, we pick a point on x and then asks ourselves given the data on this exact x point in this case $x_0$ , what is the likelihood that we observe a data point t, as in which parameter are most likely to obsreved and if we keep doing that for all x points then we can approximate the function for the likeli hood?","['statistics', 'bayesian', 'machine-learning', 'maximum-likelihood', 'probability']"
4914807,Convergence of a Non-linear Recursive Sequence with Fractional Exponent,"I am exploring a recursively defined sequence involving non-integer exponents, and I aim to find or confirm the limit of $\left\{b_n\right\} \text { as } n \rightarrow \infty \text {. }
$ The sequence is defined as follows: Let $x_1, x_2, \ldots, x_t$ be initial fixed positive real numbers, and define the sequence $\{b_n\}$ by: For $n \leq t$ , $b_n = x_n$ . For $n > t$ , $$
  b_n = b_{n-1}^p + b_{n-2}^p + \cdots + b_{n-k}^p,
  $$ where $p$ is a real number with $0 < p < 1$ . Progress and Approach: Given the concavity of the function $f(x) = x^p$ for $x > 0$ and $0 < p < 1$ , we observe that $f(x) \leq x$ for $x \geq 1$ . This suggests that each term in the sequence contributes progressively less as it grows larger, potentially indicating boundedness or stabilization of $\{b_n\}$ . Assuming the sequence might converge to a limit $L$ , we can expect $L$ to satisfy the steady-state condition: $$
  L = k L^p.
  $$ Solving this leads to: $$
  L = k^{1/(1-p)}.
  $$ This suggests a fixed point, which we hypothesize might act as an attractor for the sequence. To further validate this, I considered the function $f : [0, \infty)^k \to [0, \infty)$ defined by: $$
  f(x_1, \ldots, x_k) = x_1^p + \ldots + x_k^p.
  $$ Given that the derivative $\frac{d}{dx}(x^p) = p x^{p-1}$ is less than 1 for all $x > 1$ and $p < 1$ , I aimed to examine if $f$ acts as a contraction mapping under a suitable metric. Defining the metric as: $$
  d((x_1, \ldots, x_j), (y_1, \ldots, y_j)) = \max |x_i - y_i|,
  $$ one needs to show that: $$
  |f(x_1, \ldots, x_j) - f(y_1, \ldots, y_j)| \leq C \max |x_i - y_i|,
  $$ where $C < 1$ . This would imply that $f$ gradually brings terms closer together, suggesting convergence towards the fixed point $L$ . Initially, setting $M = \max(x_1, \ldots, x_t)$ , it leads to: $$
  b_{t+1} \leq k M^p
  $$ and following the recurrence: $$
  b_{t+2} \leq k \cdot (k M^p)^p = k^{1+p} M^{p^2}
  $$ indicating each term might be bounded by a decreasing sequence as $M^{p^n}$ approaches zero for large $n$ , given $p^n$ â€™s exponential decay. Considering these, how can I rigorously prove the hypothesized convergence towards $L = k^{1/(1-p)}$ , or assess how the sequence will converge/bound instead? I would greatly appreciate any insights, corrections, or suggestions on methods to rigorously confirm whether $\{b_n\}$ converges and to what limit.","['limits', 'convergence-divergence', 'upper-lower-bounds', 'sequences-and-series']"
4914865,What probability distribution function is this?,"This is sort of a followup to this question (I'll mention everything relevant in this post though so no need to click link). Main Question: I was trying to study a random variable $Y$ . I will describe it here: Suppose we flip a fair coin $L$ times. Then, we let the random variables $X_1, \ldots, X_s$ represent the counts of consecutive runs. For example, if $L = 5$ , and I flip $$HHTHH,$$ then $X_1 = 2$ (two heads), $X_2 = 1$ (followed by one tails), $X_3 = 2$ (followed by two heads). In this example, we let $s$ be the count of the number of consecutive runs (3 in this example). Now, I want to understand the behavior of $Y = \prod (X_i+1)$ as $L \to \infty$ . Specifically, I want to understand how the pdf/cdf of the limiting probability distribution looks like. Originally, I thought this would be a messy probability distribution with nothing interesting about it. However, when I graphed the pmf (histogram with about 200 bins for the log of the value of the RV) for $L = 17$ or $18$ , I got something cool (shown below). Can someone provide some insight into what $Y$ , or $Pr(Y < a)$ is? Thanks! ðŸ˜ Please explain this like Iâ€™m an undergrad who kinda paid attention in probability class for the first half of the semester. One suggestion for a possible answer (though I am slightly skeptical) : $$Pr(Y < a) \approx Pr\left(\log \prod_{i=1}^{\left\lfloor\frac{L}{2}\right\rfloor} 1 + Geo(1/2) < \log(a)\right)$$ One thing that was suggested was to model each $X_i$ as a geometric random variable $G_i$ . Then, if we let $Z_s = \prod\limits_{i=1}^{s}(G_i + 1)$ , then we take $\log(Z_s)$ , we can use the central limit theorem and send $s \to \infty$ . It was then suggested that since $s$ will be about $L/2$ , we can approximate the distribution (pdf/cdf) of $Y$ with $Z_{\lfloor L/2 \rfloor}$ . While this explains the lognormal shape, I would now like to know why it is this is the case. Specifically, why can we approximate the $X_i$ 's with i.i.d. geometric random variables as $L \to \infty$ (necessary for CLT) and why the ""distribution of $Y$ converges to the distribution of $Z_{\lfloor L/2 \rfloor}$ ."" Specifically, it was suggested that $\mathbb{P}(Y< a)=\mathbb{P}(\log Z_{L/2}< \log  a)
=\mathbb{P}\left(\frac{\log Z_{L/2}-L\mu/2}{\sigma\sqrt{L/2}}< \frac{\log a-L\mu/2}{\sigma\sqrt{L/2}}\right)\asymp \Phi\left(\frac{\log a-L\mu/2}{\sigma\sqrt{L/2}}\right)$ This does explain the lognormal shape as expected. If this is the wrong way to approach this problem, please explain why and what I should do instead. And if it is correct, can someone work out the details of this (law of large numbers + CLT analysis details) and as a followup, when is it in general where we can make these approximations? Graph of pmf (looks like lognormal?) Below, I graphed a rough histogram of the distribution of the value of the random variable when $L$ is around 17. Some of the details might be off but theres is some lognormal shape to it and I wish to understand where this shape comes from and whether the random variable $Y$ tends to something nice like that below. THIS GRAPH IS WRONG, NEW GRAPH COMING SOON (BUT STILL LOGNORMAL)","['central-limit-theorem', 'probability-distributions', 'combinatorics', 'probability', 'random-variables']"
4914875,When is an ergodic Ito diffusion also mixing,"If $(X_t,t\geq0)$ denotes the strong solution to the Ito SDE $\mathrm{d}X_t=b(X_t)\mathrm{d}t+\sigma(X_t)\mathrm{d}W_t$ for $W_t$ the standard $d$ -dimensional Brownian motion and $b,\sigma$ satisfying the usual conditions with $X_0=x$ and $\sigma$ additionally satisfies the uniform hypoelliptic assumption. Additionally, we assume that $X_t$ is ergodic in the sense that the semigroup $P_t$ converges to the invariant probability measure $\mu$ in total variation norm. I am curious what conditions we need to impose on the process in order to obtain that $X_t$ additionally satisfies mixing properties (for example one of the mixing concepts in Basic Properties of Strong Mixing Conditions. A Survey and Some Open Questions ). Intuition says that there must exist some argument to argue that indeed ergodicity induces sufficient ""regularity"" on the trajectories to ensure some form of mixing. For example, example 2.2.18 in the book Stochastic
Processes and Long Range Dependence argues that a centered stationary Gaussian process $(X_n,n\in\mathbb{Z})$ is ergodic if and only if it is weakly mixing. Indeed the class of Gaussian processes could be considered one of the ""nicest"" processes to work with, yet the Ito diffusion inherets many of the welcoming properties of the Brownian motion, and so I feel that a similar result should exist for the Ito diffusion (perhaps under additional (mild) assumptions). Could my intuition be right, and does anyone know any sources which might contain more information?","['ergodic-theory', 'stochastic-processes', 'markov-process', 'probability-theory', 'mixing']"
4914877,Independent random variables with $X^2 + Y^2 =1$,Does there exists independent non-constant  random variables with $X^2 + Y^2 =1$ ?  I think not because intuitively if there is a relation between them it must mean they are dependent but I can't think of a proof.,"['independence', 'probability-theory', 'random-variables']"
4914922,Ring homomorphisms between finite fields,"I'm confused by this problem (A-52) from J.S. Milne - Field and Galois Theory Let $E$ and $F$ be finite fields of the same characteristic. Prove or disprove these statements. (a) There is a ring homomorphism of $F$ into $E$ iff $|E|$ is a power of $|F|$ . (b) There is an injective group homomorphism of the multiplicative group of $F$ into the multiplicative group of $E$ iff $|E|$ is a power of $|F|$ . According to the solution (a) is false (b) is true (the solution only says this). Say $\textrm{char} \,E = \textrm{char} \,F = p$ , $p$ prime. I agree that (b) is true as this follows from the fact that $p^a - 1 | p^b - 1$ iff $ a | b$ . But why is (a) false? If such a ring Hom exists it induces an injective group Hom between the multiplicative groups, so by (b) $|E|$ is a power of $|F|$ . On the other hand if $|E| = p^{c\times a}$ and $|F| = p^{a}$ then $E = \mathbb{F}_{p^{c\times a}} \supset \mathbb{F}_{p^{a}} = F$ and the field inclusion is such Hom. What am I missing? (It may be something very dumb ...)","['field-theory', 'group-theory']"
4914924,Finding a function which is $L^1$ not $L^2$ and the integral is bounded by the square root.,"So I have been trying to solve the following this past exam problem: Find $f\in L^1(\mathbb{R})$ , not $L^2(\mathbb{R})$ with the property: $$
\int_{A}|f(x)|dm\leq \sqrt{m(A)}\quad\text{ for all } A\in\mathcal{L} \text{ with } m(A)<\infty
$$ I tried to start solving the problem for a $A$ a bounded interval on $\mathbb{R}$ and then use some approximation theorem. The best function I found so far is $$
f(x)=\frac{e^{-\sqrt{x}}}{2\sqrt{x}}
$$ Then we can see that $f\in L^1( \mathbb{R})$ , not $L^2(\mathbb{R})$ , because of $1/x$ problems, and for an interval $I=[a,b]$ with $0\leq a\leq b$ : $$
\int_{I}f\leq \int_I \frac{1}{2\sqrt{x}}=\sqrt{b}-\sqrt{a}\leq \sqrt{b-a}=\sqrt{m(I)}
$$ Now also we notice that $F=-e^{-\sqrt{x}}$ is an antiderivative for f. So if we have two disjoint intervals $I_1=[a,b]$ and $I_2=[c,d]$ then: \begin{equation}
\int_{I_1\cup I_2}f=e^{-\sqrt{a}}-e^{-\sqrt{b}}+e^{-\sqrt{c}}-e^{-\sqrt{d}}\quad (1)
\end{equation} So we would like to prove that $$
e^{-\sqrt{a}}-e^{-\sqrt{b}}+e^{-\sqrt{c}}-e^{-\sqrt{d}}\leq \sqrt{b-a+d-c}=\sqrt{m(I_1\cup I_2)}
$$ I tested some values on desmos and it seems to hold but I can't get a direct proof, maybe the convexity of $e^{-\sqrt{x}}$ could give something. If this is solved then we could just approximate the measurable set by a finite number of disjoint intervals and get the result for f, then just do $g(x)=f(|x|)$ to get it for $\mathbb{R}$ . In general if this doesn't work I believe that the idea of trying $g(x)/2\sqrt{x}$ should give an example for ""very decaying"" g.","['integration', 'measure-theory', 'lp-spaces', 'real-analysis']"
4914948,"How many rolls are sufficient to ensure, with probability 99%, that the sum is greater than 100?","I roll a pair of fair dice $n$ times, and calculate the sum of all $2n$ faces which come up: Suppose each roll of each die is independent of other rolls. How many rolls are sufficient to ensure, with probability $99\%$ , that the sum is greater than $100?$ . I've calculated that the expected value and variance of sum are $\mathbb{E}(X) = 7n\text{ and } \mathrm{Var}(X)=\frac{35}{6},$ where $X$ represents the random variable of sum of the $2n$ faces. I think I need to find a lower bound on $\operatorname{Pr}(X > 100)$ . I've attempted applying Markov's inequality: $\operatorname{Pr}(X \geq 100)\leq \frac{\mathbb{E}(X)}{100}$ , which isn't anywhere near what I want. Any tips $?$ .","['variance', 'expected-value', 'dice', 'probability', 'random-variables']"
4914991,What distribution does the height of both men and women follow?,"It is often said that the height of men and that of women follow normal distribution with different means and variances. As graphs suggest, it appears true. Then, what is the whole distribution of both men and women? Is it normal distribution or other distribution?
(I'm asking a distribution the height of randomly chosen people follow, including both men and women.) I thought it is related to the reproductive property of the normal distribution, but it appears slightly different.","['statistics', 'variance', 'normal-distribution', 'means']"
4915045,Taking the inverse (not the reciprocal) of both sides of an inequality,"This is something I'm having a hard time finding online, but say we know that $f(x) > g(x)$ (for all inputs $x > a_{0}$ for some $a_{0}$ ), then would it always be true that $f^{-1}(x) < g^{-1}(x)$ (for all inputs $x > b_{0}$ for some $b_{0}$ )? For example, let $f(x) = 2^{x}$ and $g(x) = x^{2}$ . $f^{-1}(x) = \log_{2}(x)$ and $g^{-1}(x) = \sqrt{(x)}$ If I plot on desmos I see $\sqrt{(x)}$ eventually exceeds $\log_{2}(x)$ , so hence we can say $f^{-1}(x) < g^{-1}(x)$ for this example Since this is just the definition of Big-O, could we say $f = O(g) => f^{-1} = \Omega(g^{-1})$ Not sure how to prove it but isn't it intuitive from the fact that the inverse is just reflecting over $y = x$ , so if $g$ is above $f$ , then after reflecting $g^{-1}$ is below $f^{-1}$","['inverse-function', 'asymptotics', 'functions', 'inverse', 'inequality']"
4915051,Inclusion-Exclusion confusion: Meeting exactly one friend in lunch time during a semester,"From Trotter's Combinatorics Textbook, I was working on this problem from his Inclusion-Exclusion Chapter: A graduate student eats lunch in the campus food court every Tuesday over the course of a $15$ week semester. [They are] joined each week by some subset of a group of six friends from across campus. Over the course of a semester, [they] ate lunch with each friend 11 times, each pair 9 times, and each triple 6 times. [They] ate lunch with each group of four friends 4 times and each group of five friends 4 times. All seven of them ate lunch together only once that semester. Did the graduate student ever eat lunch alone? If so, how many times? The solution mainly involves making your sets $A_{i}$ as the set of weeks he ate with friend $i$ , and doing inclusion-exclusion from there. (See Chapter 7 Solutions, Question 9: https://trotter.math.gatech.edu/math-3012/toppage.html ) Now I tried to answer my own follow-up question of how many days he eat with exactly 1 friend. Since I know that $|A_{1} \cup \dots \cup A_{6}| = 14$ , I could just try to do: $|A_{1} \cup \dots \cup A_{6}| - |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})|$ Now the formula for $|(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})|$ I couldn't find online or in the textbooks, so I tried to just count it out myself and noticed a pattern that I think generalizes: If I add up all the sets that are intersections of 2 friends, I've overcounted the sets that are intersections of 3 friends ${3 \choose 2} = 3$ times, so I have to subtract those sets and their quantities 2 times for them to be counted exactly once! Then for the sets 4-friend-intersections, I first counted them ${4 \choose 2}$ times, then subtracted them $2 \cdot {4 \choose 3}$ times, hence they've been counted $ 6 - 8 = -2$ times, so I have to add those sets and their quantities 3 times. You continue this logic and you get the coefficients should be +1, -2, +3, -4, +5. In particular I mean that: $$ |(A_{1} \cap A_{2}) \cup \dots \cup (A_{5} \cap A_{6})| = \sum|(A_{i} \cap A_{j})| - 2 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k})| + 3 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l})| - 4 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m})| + 5 \cdot \sum |(A_{i} \cap A_{j} \cap A_{k} \cap A_{l} \cap A_{m} \cap A_{n})|$$ But when I try plugging in the numbers I get -16: ${6 \choose 2}9 - 2 {6 \choose 3} 6 + 3  {6 \choose 4}  4 - 4  {6 \choose 5}  4 + 5  {6 \choose 6}  1 = -16$ which is weird since I use this same method for this problem ( Combinations' Problem ) and I get 7 and hence 13 - 7 = 6 times they ate with exactly 1 friend alone, which is correct. Am I missing something here? I've been think about this problem for a while now but am really stuck. Kindly please help here.","['discrete-mathematics', 'inclusion-exclusion', 'combinatorics', 'probability']"
4915054,Integral involving composition of cosine and log,"I am trying to prove that $$\int_{0}^1 \frac{1}{x}\log{(1-x^2)}\cos{(p \log{x})}dx = \frac{1}{p^{2}}-\frac{\pi}{2p}\coth\left(\frac{p\pi}{2}\right),\quad p \in \mathbb{R}$$ where $\coth {x} =  \frac{e^{2x}+1}{e^{2x}-1}$ . At $p=0$ , we take the limiting value. It is taken from ""Table of Integrals, series, and products"" 7th edition by Gradshteyn and Ryzhik. Where they state that on page 595 , $\int_{0}^1 \frac{1}{x}\log{(1-x^2)}\cos{(p \log{x})}dx = \frac{1}{2p^{2}}+\frac{\pi}{2p}\coth\left(\frac{p\pi}{2}\right)$ which seemed incorrect to me as the right side is mostly positive. I don't have a complete proof, but I think the correct approach would be integration by parts, $$\int_{0}^1 \frac{1}{x}\log{(1-x^2)}\cos{(p \log{x})}dx = \left. \ln(1-x^2)\frac{\sin(p\ln(x))}{x}\right|_0^1-\frac{1}{P}\int_0^{1}\frac{-2x}{1-x^2}\sin{(p\ln{x})}dx\\ = \frac{2}{P}\int_0^{1} \frac{x}{1-x^2}\sin{(p\ln{x})} \overset{x=e^{-t}}{=} -\frac{2}{p}\int_0^{\infty} \frac{\sin{pt}}{e^{2t}-1}dt.$$ where I am currently stuck at the last integral. Feynman's trick is not gonna work as the integral is gonna be unbounded.  How should I proceed from here? Edit: Maybe the approach is related to this solution which uses contour integration.","['integration', 'improper-integrals', 'trigonometry', 'definite-integrals']"
4915207,solution-verification Determine the position of M,"The problem In the regular quadrilateral prism, $ABCDA'B'C'D'$ the edge of the base is equal to $4 \sqrt{6}$ , and the volume is $1152$ . Determine the position of the point M on the edge CC', so that the planes $(BA'D)$ and $(MBD)$ are perpendicular. The idea drawing enter image description here I let O be the center of square ABCD. If those planes are perpendicular, then the angle they form is 90. Using the theorem of the 3 perpendicular we get that $MO\perp DB, A'0\perp BD$ and the common side of both planes $(BA'D)$ and $(MBD)$ , which make the angle between them be $\angle A'OM=90$ We can calculate using the volume and the edge of the base that the lateral edge is $12$ , so AA'=12 and we can also calculate $AO=4\sqrt3$ , from here we get that $\angle A0A'=60=> \angle COM=30, => MC=4 $ so we determined the pos of M The thing is I don't know if this is the only solution... I don't know if M can be outside of CC'
Thank you!","['solution-verification', 'geometry']"
4915210,Folding a circle [duplicate],"This question already has answers here : Is circle the only shape that can remain convex after folding? (3 answers) Closed last month . I was studying folding of circles out of boredom, when I noticed that folding a circle never concludes with a form where the circle intercepted itself. If you fold a rectengular piece of paper for example, for some particular folding, the paper will intercept with itself, but the cirlce does not do that. I was able to proof that this is always the case, but I would really like to show that the circle is the only figure with this property, or isnt, i dont know. Can someone help? I dont know if there is a field which studys such concepts, I am new to mathematics of this kind.","['circles', 'geometry']"
4915301,Convergence of weighted sum to Brownian Motion,"Let $\{\varepsilon_t\}_{t = 1}^T$ be a sequence of iid random variables such that $\varepsilon_t \sim N(0, \sigma^2)$ and $\sigma^2 > 0$ . Then it is known that (see 17.3.6 in James Hamilton's Time Series Analysis) $T^{-1/2}\sum_{t = 1}^{[sT]}\varepsilon_t \to \sigma B(s)$ , where $s \in [0, 1]$ , $[sT]$ is the largest integer less than or equal to $sT$ , and $B(s)$ is a standard brownian motion. The convergence is weak and follows by the functional central limit theorem. What would the corresponding statement look like for $T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t$ , where $\{w_t\}_{t = 1}^T$ is some sequence. What conditions need to be placed on this sequence? My conjecture is that $T^{-1/2}\sum_{t = 1}^{[sT]}w_t\varepsilon_t \to \lambda B(s)$ , where $\lambda = \sigma \sqrt{\sum_{t = 1}^\infty w_t^2}$ , so that the required condition would be $\sum_{t = 1}^\infty w_t^2 < \infty$ . However, I am not familiar enough with the concepts to show it. I know this discussion is not fully formal. I am only looking for references, and likewise informal answers. Thanks in advance!","['statistics', 'stochastic-processes', 'time-series', 'brownian-motion', 'probability']"
4915309,43 cookies are randomly given to 10 children. What's the probability each child receives at least 2 cookies?,"I wanted to ask 1) if I've solved this puzzle problem correctly, and 2) if there is a shorter or more elegant approach. There are 43 cookies to be given out at random to 10 children. What is the probability that each child gets at least 2 cookies? First, notice that this is a classic multinomial setup. The multinomial distribution can be thought of as giving the probability
of observing the outcome $i \in \{ 1, ..., k \}$ coming up $x_i$ times when rolling a $k$ -sided die $n$ times, and is a generalization of the Binomial distribution. We have that the probability of each outcome $i$ coming up on a single roll is given by $\pi_i$ . If the so-called die is fair, $\pi_i = \pi_{i'}\; \forall i, i' \in \{ 1, ..., k\}$ or the die is unfair and $\pi_i$ is not necessarily equal to $\pi_i'$ . In all cases, we assume $\sum \pi_i = 1$ . Let $X = (X_1, ..., X_k)$ be a $\text{Multinomial}(n, k, \pi)$ distributed where $\pi = (\pi_1, ..., \pi_k)$ . The density of the Multinomial distribution is \begin{align}
P(X = x) & = {n \choose x_1,...,x_k!} \prod_{i=1}^k \pi_i^{x_i} \\\\
& = \frac{n!}{x_1!\cdots x_k!}\pi_1^{x_1} \cdots \pi_k^{x_k}.
\end{align} Here, let $X_1, ..., X_{10}$ denote how many of the $n=43$ cookies each of the $k=10$ children received. Let $p_n$ denote the probability that all 10 children receive at least 2 cookies each given that $n$ are given out uniformly at random. Notice that $$
{\small
X_1, ..., X_{10} \Bigg\vert \sum_{i=1}^{10} X_i = 43 \sim 
\text{Multinomial}(n = 43, k = 10, \pi_i = 1/10),}
$$ where $\pi_i = 1/10$ indicates that the distribution of cookies
is uniformly random (e.g., equal probability) across the 10 children. The Poisson and Multinomial distributions have an interesting relationship.
When the outcomes $X_1, ..., X_k$ are such that $X_i \sim \text{Poisson}(\lambda_i)$ ,
then $\sum_{i=1}^k X_i \sim \text{Poisson}(\sum_{i=1}^k \lambda_i).$ One can easily derive via the definition of conditional probability that $$(X_1, ..., X_k) {\Bigg\vert} \sum_{i=1}^k X_i = N = n \sim \text{Multinomial}(n, k, \pi),$$ where $\pi = \left(\frac{\lambda_1}{\sum \lambda_i}, ..., \frac{\lambda_k}{\sum \lambda_i}\right)$ . \begin{align}
P(X = x \Big \vert N = n) & = \frac{P(X = x, N = n)}{P(N = n)}  \\\\ 
& = \frac{P(X = x)}{P(N = n)} \\\\ 
& = \left( \frac{e^{-\sum \lambda_i} \prod \lambda_i^{x_i}}{\prod x_i!} \right) \Bigg / \left( \frac{e^{-\sum \lambda_i} (\sum \lambda_i)^{n}}{n!} \right) \\\\ 
& = {n \choose x_1, ..., x_k} \frac{\prod \lambda_i^{x_i}}{\left( \sum \lambda_i \right)^n} \\\\ 
& = {n \choose x_1, ..., x_k} \left( \frac{\lambda_i}{\sum \lambda_i} \right)^{x_i} \\\\ 
& \sim \text{Multinomial}(n, k, \pi).
\end{align} Since no information was given to us about how it came about that $n = 43$ cookies
were given out, let's assume that it was the result of a $\text{Poisson}(\lambda)$ process.
This implies that the $X_i$ are independently and identically distributed
as $\text{Poisson}(\lambda/10)$ by a similar argument in the reverse direction. $$P(X = x \Bigg| \sum X_i = n) = \frac{P(X = x, \sum X_i = n)}{\underbrace{P(\sum X_i = n)}_{\text{Poisson}(\lambda)}},$$ and $X = x \implies \sum X_i = \sum x_i = n$ , so $P(X = x, \sum X_i = n) = P(X = x)$ as
long as $n = \sum x_i$ . \begin{align}
\therefore \;\; P(X = x) & = P(X = x \Bigg| \sum X_i = n) \cdot P(\sum X_i = n) \\\\ 
& = \left[ {n \choose \pi_1, ..., \pi_k} \prod \pi_i^{x_i} \right] \cdot \left( \frac{e^{-\lambda} \lambda^{\sum x_i}}{\sum x_i! } \right) \\\\ 
& = \frac{n!}{x_1!\cdots x_k!} \pi_1^{x_1}\cdots \pi_k^{x_k} \left( \frac{e^{-\lambda} \lambda^{n}}{n!} \right).
\end{align} Assume as given in the problem that the
cookies are uniformly randomly given out, and $\pi_i = \pi_{i'}\;  \forall i, i' \in \{ 1, ..., k \}$ ; this
single probability must be $1/k$ (or in our case, $k = 10$ children). \begin{align}
P(X = x) & = \frac{e^{-\lambda}}{x_1! \cdots x_k!} \left( \frac{\lambda}{k} \right)^{\sum x_i} \\\\
& = \prod_{i=1}^k \frac{e^{-\lambda/k} \left( \frac{\lambda}{k} \right)^{x_i}}{x_i!}, \\\\ 
\text{a product of the }&\text{density for $k$ iid Poisson}\left(\frac{\lambda}{k}\right) \text{ variables}.
\end{align} We said that if $n$ cookies are given out, then there's a $p_n$ probability
that the 10 children all receive 2+ cookies. Then without conditioning on the number of cookies given out, the probability that all kids receive 2+ cookies is given by $$\mathbb{E}\left[\mathbb{E}[p_n | N = n]\right] = \sum_{n=0}^\infty \frac{e^{-\lambda} \lambda^n}{n!} p_n.$$ On the other hand, since the $X_1, ..., X_k$ are iid Poisson, the probability
that all 10 kids
receive 2+ cookies is $P(X_1 \geq 2)^{10} = (1-P(X_1 \leq 1))^{10} = 
(1-e^{-\frac{\lambda}{10}} - \frac{\lambda}{10}e^{-\frac{\lambda}{10}})^{10}$ . Now we have that $$ \sum_{n=0}^\infty \frac{e^{-\lambda} \lambda^n}{n!} p_n = 
\left[1-e^{-\frac{\lambda}{10}} - \frac{\lambda}{10}e^{-\frac{\lambda}{10}}\right]^{10}.
$$ Now, recall that $e^{x}$ has a series expansion. If we
multiply both sides by $e^{\lambda}$ and by $43!$ , we should find that the coefficient on the left-hand-side series for $\lambda^{43}$ is just $p_n$ , and the right-hand-side series coefficient for $\lambda^{43}$ gives an expression we can evaluate for $p_{43}$ .
They must be equal, because in order for two convergent power series
to coincide on a non-empty interval, their coefficients must be equal. So what we're going to do is expand the function $e^{x} = \sum_{t=0}^\infty \frac{x^t}{t!}$ wherever we see it
on the modified right-hand-side and use that to identify the coefficient of $\lambda^{43}$ ,
which is $p_{43}$ . However, this is a bit hard to do by hand, so we'll use the symbolic algebra library sympy in Python to do it for us. # Python code
from sympy import symbols, exp, factorial, series, Pow

lambda_ = symbols('lambda')

inner_expression = 1 - exp(-lambda_/10) - (lambda_/10)*exp(-lambda_/10)

raised_expression = inner_expression**10 

complete_expression = exp(lambda_) * raised_expression

expanded_series = series(complete_expression, x=lambda_, n=44).removeO()

coeff_lambda_43 = expanded_series.coeff(lambda_**43)

p_43 = factorial(43) * coeff_lambda_43
p_43 $$\frac{38360235213946776318553037176114920309}{78125000000000000000000000000000000000} \approx 0.491
$$ Thus we conclude that if 43 cookies are given out to 10 children uniformly at random, then
the probability that each child receives at least 2 cookies is $\approx .491.$ Let's see if we can confirm that via a simple simulation in R: # R code
set.seed(1234)

num_trials <- 100000  # Number of simulations
num_children <- 10    # Number of children
num_cookies <- 43     # Number of cookies

results <- replicate(num_trials, {
  cookies <- sample(1:num_children, num_cookies, replace = TRUE)
  counts <- table(factor(cookies, levels = 1:10))
  all(counts >= 2)
})

prob_estimate <- mean(results)
var_estimate <- var(results) / num_trials

prob_estimate
var_estimate > prob_estimate
[1] 0.49178
> var_estimate
[1] 2.499349e-06 The point-estimate is off by 0.0007689893. I'm not sure if the way I've calculated the variance is appropriate.","['poisson-distribution', 'solution-verification', 'combinatorics', 'multinomial-distribution', 'probability']"
4915394,Defining curvature in two ways,"Given a Riemannian manifold $(M,g)$ one often defines the Curvature of the Levi-Civita connection $\nabla$ as $$R(X,Y)=\nabla_X\nabla_Y - \nabla_Y\nabla_X - \nabla_{[X,Y]}$$ for vector fields $X$ and $Y$ . However, some authors define the curvature as $$R=\nabla \circ \nabla.$$ These two definitions should coincide, but I have some difficulties proving this. Does anyone here know a resource that goes over this?","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
4915418,"How do I find the angle alpha given that I have a scalene triangle, and only know the length of one side of that triangle?","[scalene triangle where only one angle is known, and one side is known. There is a possible right triangle drawn as apart of the smallest side of the scalene triangle.][1] Hey all, having trouble with a problem and figured I'd ask here for advice/help. It's been some time since I've done geometry quite like this so bare with me. I have a scalene triangle where one of the angles is known, which we'll say is $\theta = 20^{o}$ . AB and CB are unknown in this setup, but CD = 2, and BD is also unknown. With the information provided is it possible to figure out $\alpha$ , AB, and CB? Sorry if I haven't explained this well, the photo included should provide a bit more clarity.","['trigonometry', 'geometry']"
4915432,Why don't we apply the principle of indifference here?,"The Problem You and friend play a game where you both select an integer 1-100. The winner receives 1$ from the loser. The winner is the one who chooses a number that is either (i) exactly two less than the other player's number, or
(ii) strictly greater than the other player's number (if they choose a number that is two less than your, they win) Assume you and your friend play optimally. What is the optimal strategy here, and what is $Var(X)$ , where $X$ is the random value you choose according to this optimal distribution? My Questions (i) I haven't yet been exposed to too many of these questions regarding ""optimal strategies"". So I'm a little unsure on how to think when I see one. Are these really probability questions? If so, can someone tell me in a general way how I should think about these kinds of problems? (ii) The only question I've seen that was similar to this one (in that it was an ""optimal strategy"" question as well) was the popular problem 3.4 in Mark Joshi's book ( Fail to understand 'The indifference criterion means that $1p_1=2p_2=3p_3$.' ). It utilized the indifference criterion. Why can't the same principle be applied here? What's different between that problem and this problem? In case you don't know which problem I'm talking about, I will reproduce it here: We play a game: I pick a number from 1 to 100. If you guess correctly, I pay you $n and zero otherwise. How much would you pay to play this game?","['game-theory', 'probability']"
4915475,Confusion on reading multiple ways of p implies q,"So these are the different ways of expressing the conditional statement, and I got these from Rosen's Discrete Math Textbook: So I want to be clear on a couple of things: When we read all these statements, can we read them in long form by adding the phrase ""is true"" next to/after the q and p? Like could ""if p, then q"" be read as ""if p is true, then q is true""? If q if p is the same thing as saying if p then q or p -> q , then would p only if q be the same thing as saying if q is true, then p is true ? From that perspective I have a hard time seeing how p -> q means the same thing as p only if q , especially when drawing out the truth table for it. I'm not sure if this is a language issue or something but it just feels weird how by adding the word ""only"" next to if in p only if q makes a different meaning from q if p . Probably my most important question: I'm really having a hard time seeing why q unless (not P) is the same as p -> q, especially when I draw out the truth table for it where the columns don't match up: p q p -> q q unless (not p) T T T T T F F F F T T F F F T T Since we know ""unless"" means ""except on the condition that"", p being false makes (not p) true, so q must be false then. Similarly p being true makes it that (not p) is false, so q should still be true then. Hence I filled the truth table accordingly. I also saw someone else on MSE recommend that we instead use (not p) unless q instead, but when making the truth table I just get the same result Kindly please help clarify my doubt here.","['propositional-calculus', 'solution-verification', 'logic', 'discrete-mathematics']"
4915508,"Measurability of $\|f(\cdot, x_{2})\|_{L^\infty(X_{1})}$ (proof of Minkowski's inequality)","I am trying to prove Minkowski's inequality for integrals in the case $p = \infty$ . Suppose $(X_{1}, \mu_{1})$ and $(X_{2}, \mu_{2})$ are two $\sigma$ -finite measure spaces and $f(x_{1}, x_{2})$ is a non-negative measurable function on $X_{1}\times X_{2}$ then $$\Big\|\int f(x_{1}, x_{2})\:\text{d}\mu_{2}\Big\|_{L^{\infty}(X_{1})}\leq \int \|f(x_{1}, x_{2})\|_{L^\infty(X_{1})}\:\text{d}\mu_{2}.$$ I am having trouble at the very first step in showing that $x_{2}\mapsto \|f(\cdot, x_{2})\|_{L^{\infty}(X_{1})}$ is measurable. Some thoughts: My idea is to write $\|f(\cdot, x_{2})\|_{L^{\infty}(X_{1})}$ as the limit of a sequence of measurable functions. (i) The answer posted here suggests that it might be useful to use the fact that $\|g\|_{\infty} = \lim_{p\rightarrow\infty}\|g\|_{p}$ under certain conditions. However, the answer here suggests that in the case when $\|g\|_{\infty} < \infty$ then we need in addition that $\|g\|_{p_{0}} < \infty$ for some $p_{0}\in(0, \infty)$ (which may not be the case). Could we somehow make use of the $\sigma$ -finiteness of the spaces? (ii) The answer here suggests that if $h$ is a non-negative measurable function then $$\|h\|_{\infty} = \sup\Big\{\int hg\:\text{d}\mu : g\geq 0, \|g\|_{1} = 1\Big\}.$$ In this case, for $\mu_{2}$ -a.e. $x_{2}$ we may write $$\|f(\cdot, x_{2})\|_{L^{\infty}(X_{1})} = \lim_{m\rightarrow\infty}\int f(x_{1}, x_{2})g_{m}(x_{1}, x_{2})\:\text{d}\mu_{1}$$ for some sequence of non-negative functions $\{g_{m}\}$ such that for each $m$ and $\mu_{2}$ -a.e. $x_{2}$ we have $\|g(\cdot, x_{2})\|_{L^{1}(X_{1})}\leq 1$ . However, I am not able to make this argument work since it is not clear that the functions $g_{m}$ are measurable with respect to the product measure on $X_{1}\times X_{2}$ . Any help is appreciated. Thank you.","['measure-theory', 'lp-spaces', 'real-analysis']"
4915534,Prove that $|z^2+1|\le 2$ implies $|z^3+3z+2|\le 6$,"Show that $$\{z \in \mathbb{C}: |z^2+1|\le 2 \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \} \tag{*}$$ (In other words: Let $z\in \mathbb{C}$ satisfy $|z^2+1|\le 2$ . Prove that $|z^3+3z+2|\le 6.$ ) This problem was asked by mengdie1982 roughly a day ago(relative to posting this question), but was closed due to lack of work. This has been bothering me because of how simply it has been stated, however even after hours of work I am not close to proving this mathematically. Geometrically the statement (*) is true, as seen below. $\hspace{4cm}$ Here are some estimates: From reverse triangle inequality, $|z^2+1| \le 2$ implies $|z| \le \sqrt{3}$ with equality at $z=i\sqrt{3}$ . From triangle inequality, $|z^3+3z+2| \le |z|^3+3|z|+2 \le 6\sqrt{3}+2 \approx 12.4$ . A better bound can also be achieved by factorization, $$|z^3+3z+2| = |z(z^2+1)+2(z+1)| \le |z||z^2+1|+2(|z|+1) \le 4\sqrt{3}+2 \approx 8.93. $$ Another approach I took was to show that for any $r<6$ , $$\{z \in \mathbb{C}: |z^2+1|\le 2 \} \nsubseteq \{z \in \mathbb{C} : |z^3+3z+2|\le r \}  $$ which explains the pinching behavior between the sets near the point $(1,0)$ . Define $$L=\left(\frac{2}{2-r+\sqrt{8-4r+r^{2}}}\right)^{\left(\frac{1}{3}\right)},\quad  z_0 = \frac{1}{2}\left(1+L-\frac{1}{L}\right), $$ then with much effort, it can be shown that $z_0$ satisfies $|z^2+1|\le 2$ , but it is not present in the other set. This implies that (*) can be reworded as  finding the largest $r>0$ such that $$\{z \in \mathbb{C}: |z^2+1|\le r \} \subseteq \{z \in \mathbb{C} : |z^3+3z+2|\le 6 \}$$ or it is possible that there is an easier solution which I am missing.","['inequality', 'real-analysis', 'complex-analysis', 'optimization', 'algebra-precalculus']"
4915535,Prove that any 2x2 nilpotent matrix N is similar to zero matrix or 0 & 1 \\ 0 & 0,"The question is to prove that any nilpotent matrix $N \in Mat_{2\times2}(\mathbb{C})$ is similar to \begin{equation}
\begin{pmatrix}
0 & 0 \\
0 & 0
\end{pmatrix}
\end{equation} or \begin{equation}
\begin{pmatrix}
0 & 1 \\
0 & 0
\end{pmatrix}
\end{equation} I've known that $N^2=0$ , and the eigenvalues of $N$ are $0$ . But I'm still confused about how I can reach the conclusion of similarity. Thanks for any help!","['matrices', 'linear-algebra']"
4915568,The conjugate of a cosine series is a sine series,"In Katznelson's An Introduction to Harmonic Analysis the author defines the conjugate $\widetilde{S}$ of a trigonometric series $$S \sim \sum_{n = \infty}^\infty a_n e^{inx}$$ by $$\widetilde{S} \sim \sum_{n = \infty}^\infty -i \operatorname{sgn}(n) \ a_n e^{inx}.$$ Exercise 1.3. asks to formally check that if $S \sim \sum a_j \cos{jx}$ then $\widetilde{S} \sim \sum a_j \sin{jx}$ . My attempt. Using Euler's formula, we may formally write \begin{align}
S \sim \sum_{j = -\infty}^\infty a_j \cos{jt}
= \frac{1}{2}\left(\sum a_j e^{ijt} + \sum a_{-j} e^{ijt}\right)
= \sum \frac{a_j + a_{-j}}{2} e^{ijt}.
\end{align} Therefore, \begin{align}
    \widetilde{S} &\sim \sum_{j = -\infty}^\infty  \operatorname{sgn}{j} \frac{a_j + a_{-j}}{2i} e^{ijt} \\
    &= \frac{1}{2i} \left(\sum \operatorname{sgn}(j) a_{j} e^{ijt} + \sum \operatorname{sgn}(j) a_{-j} e^{ijt}\right) \\
    &= \sum a_j \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i}\\
    &= \sum_{j = -\infty}^{-1} a_{j} \frac{\operatorname{sgn}(j) e^{ijt} + \operatorname{sgn}(-j) e^{-ijt}}{2i} + \sum_{j = 1}^\infty a_j \frac{e^{ijt} - e^{-ijt}}{2i} \\
&= \sum_{j = -\infty}^{-1} a_{j} \sin{(-jt)}+ \sum_{j = 1}^\infty a_j \sin{jt}
\end{align} I'm not sure how to conclude from here. Is this even correct?","['complex-analysis', 'trigonometric-series', 'harmonic-analysis']"
4915607,Solving inverse trigonometric equation involving arccotangent.,"$\tan^{-1}(x+1)+\cot^{-1}(\frac{1}{x-1})=\tan^{-1}(\frac{8}{31})$ One thing to clear is that the range of arccotangent is $(0,\pi)$ . I am so sad because I can't use Wolfram to cross check my answers.
I tried to solve the problem and got two values of $x=\frac{1}{4},-8$ But I had used the tangent function on both sides. So there were high chances that extraneous roots have been generated. So I tried to put $x=\frac{1}{4}$ and $-8$ . But in both cases I came to the end value $\pi+\tan^{-1}(\frac{8}{31})$ which is not equal to $\tan^{-1}(\frac{8}{31})$ Moreover to my surprise both the answer are given as the correct answers in the answer key. I don't trust it though. Please guide me where I am going wrong. The issue is the arccotangent's range. Or it would have been way easier. Edit 1: My solution: Edit 2: Check if x= 1/4 is valid or not","['inverse-trigonometric-functions', 'algebra-precalculus', 'trigonometry']"
4915701,Gain in SLLN if instead of iid we require same E,"Usually, the SLLN (say, Kolmogorov's or Etemadi's) are stated for iid random variables. This said, they could also be stated by requiring just uniform and finite variance and expected value across the sequence. That is, that $$
\mathbb{E}[X_i]=\mu\qquad \mathrm{Var}[X_i] = \sigma^2 \qquad\forall i\in\mathbb{N}.
$$ My question is: modellistically speaking, what do we really gain by this generalization? What are some (practical) situation where this generalization actually matters and we could not have used iid instead? Thanks.","['law-of-large-numbers', 'mathematical-modeling', 'probability-theory']"
4915705,Let $(x_t: t\geq 0) $ be the Brownian motion. $E(f| x_t)=0$ implies $f=0$?,"Let $(x_t: t\geq 0)$ be the standard 1-dimensional Brownian motion on the Wiener space $(\Omega, \Sigma, d\mu)$ . We assume $\Sigma$ is the (complete) $\sigma$ -algebra generated by all $(x_t: t\geq 0)$ . And we denote by $\sigma(x_t)$ the (complete) sub $\sigma$ -algebra generated by $x_t$ . Suppose $f\in L^1(\Omega)$ such that for all $t\geq 0$ the conditional expectation $$\mathbb E(f| \sigma(x_t))=0.$$ Can we conclude that $f=0$ ? If we replace $\sigma(x_t)$ by the (complete) sub $\sigma$ -algebra generated by $(x_s: 0\leq s\leq t)$ . Then the similar statement is true by Martingale convergence theorem.","['conditional-expectation', 'brownian-motion', 'probability-theory']"
4915724,"Compute the cohomology ring of $G=\langle u,v\mid uvuv=vuvu\rangle$.","I am trying to compute the integral cohomology ring of the following group: $$G=\langle u,v\mid uvuv=vuvu\rangle$$ First, I managed to compute the cohomology groups: $$H^0(G,\mathbb{Z})=\mathbb{Z}e_0,H^1(G,\mathbb{Z})=\mathbb{Z}e_1\oplus\mathbb{Z}e_2,H^2(G,\mathbb{Z})=\mathbb{Z}e_3$$ and $H^i(G,\mathbb{Z})=0~\forall~i\geq 3$ . Hence, $H^\ast(G,\mathbb{Z})=\mathbb{Z}e_0\oplus\mathbb{Z}e_1\oplus\mathbb{Z}e_2\oplus\mathbb{Z}e_3$ . To understand the ring structure we need to compute the cup product of the generators, which are: $$e_i\smile e_i=0~\forall~i=0,1,2,3$$ $$e_0\smile e_i=e_i~\forall~i=0,1,2,3$$ $$e_1\smile e_2=2e_3$$ and $e_i\smile e_j=0$ for every other $i,j$ . I would like to know whether $H^\ast(G,\mathbb{Z})$ is isomorphic to a well known ring, such as a polynomial ring or some exterior algebra.","['group-theory', 'ring-theory', 'group-cohomology']"
4915763,Conjectured connection between $e$ and $\pi$ in a semidisk.,"A semidisk with diameter $\dfrac{e}{\pi}n$ is divided into $n$ regions of equal area by line segments from a diameter endpoint. Here is an example with $n=6$ . Consider the $n$ arcs between neighboring line segment endpoints. Let $P(n)=\text{product of arc lengths}$ . Is the following conjecture true: Conjecture: $\lim\limits_{n\to\infty}P(n)=\dfrac{\pi}{2}$ In other words, if the average arc length is $\color{red}{\dfrac{e}{2}}$ then the product of arc lengths approaches $\color{red}{\dfrac{\pi}{2}}$ . Evidence for my conjecture I got the following approximations. $P(1)\approx 1.359$ $P(2)\approx 1.438$ $P(3)\approx 1.469$ $P(6)\approx 1.507$ $P(12)\approx 1.528$ $P(24)\approx 1.543$ $P(48)\approx 1.552$ $P(96)\approx 1.558$ It seems that if the average arc length is less than $\dfrac{e}{2}$ then the product approaches $0$ , and if the average arc length is greater than $\dfrac{e}{2}$ then the product approaches $\infty$ . What makes this difficult What makes my conjecture difficult for me to prove or disprove, is that I cannot find exact expressions for the arc lengths. For example, in the example shown above with $n=6$ , the length of the longest arc is $\dfrac{3ex}{\pi}$ where $x-\sin x=\frac{\pi}{6}$ . I am aware of Kepler's equation , but that doesn't seem to help. Related questions This question is essentially the converse of my question , ""Product of areas in a disk"". I asked about the product of another kind of arc length related to a circle, in my question , ""Another interesting property of $y=2^{n-1}\prod_{k=0}^n \left(x-\cos{\frac{k\pi}{n}}\right)$ : product of arc lengths converges, but to what?"". I recently asked a related question , ""Product of lengths in a disk of area $\pi/e$ "".","['conjectures', 'arc-length', 'circles', 'infinite-product', 'limits']"
4915779,Why is the derivative of $x^x$ not $\ln(x)(x^x)$?,To do this I did $u=x$ and $\frac{dy}{du}=\ln(u)u^x$ and $\frac{du}{dx}=1$ and used the chain rule to get $\ln(x)(x^x)$ . A teacher explained that if $f(x)=\exp(g(x))$ then $f'(x)=g'(x)\exp(g(x))$ and used $\exp(\ln(x)x)$ . However I do not understand why my method of using the chain rule is incorrect and I do not understand the previous rule either. I have also seen a method using multivariable calculus that I do not understand either.,"['multivariable-calculus', 'calculus']"
4915800,"Equivalent characterisation of the space $L^p_{\operatorname{loc}}(\Omega)$, where $\Omega$ is a non-empty open subset of $\mathbb R^n$?","Consider the euclidian space $\mathbb R^n$ , where $n \in \mathbb N$ is an arbitrary integer, equipped with the usual Lebesgue measure. Moreover, let $\Omega \subset \mathbb R^n$ denote an arbitrary non-empty open set. In most textbooks, the space $L^p_{\operatorname{loc}}(\Omega)$ is defined as follows: Definition . Given $1 \leqslant p < \infty$ , the space $L^p_{\operatorname{loc}}(\Omega)$ consists of all Lebesgue measurable functions $f : \Omega \to \mathbb R$ that satisfy $$ \int_K |f(x)|^p \, dx < \infty, $$ for every compact set $K \subset \Omega$ . Now, the following equivalent characterisation of $L^p_{\text{loc}}(\mathbb R^n)$ is well known: $$ \tag{1} f \in L^p_{\text{loc}}(\mathbb R^n) \iff f \, \chi_B \in L^p(\mathbb R^n), \quad \text{ for every ball } B \subset \mathbb R^n. $$ A simple proof. Suppose firstly that $f \in L^p_{\text{loc}}(\mathbb R^n)$ and consider an arbitrary ball $B \subset \mathbb R^n$ . Clearly, $\overline B$ is a compact subset of $\mathbb R^n$ and it contains $B$ . Consequently, we obtain $$ \int_{\mathbb R^n} |f(x)|^p \, \chi_B(x) \, dx \leqslant \int_{\overline B} |f(x)|^p \, dx < \infty, $$ where the last inequality follows from the hypothesis $f \in L^p_{\text{loc}}(\mathbb R^n)$ . On the other hand, suppose now that $f \, \chi_B \in L^p(\mathbb R^n)$ , for every ball $B \subset \mathbb R^n$ and consider an arbitrary compact set $K \subset \mathbb R^n$ . It is well known that there exists $R > 0$ such that $K \subset B(0,R)$ . Therefore, it follows that $$ \int_K |f(x)|^p \, dx \leqslant \int_{B(0,R)} |f(x)|^p \, dx = \int_{\mathbb R^n} |f(x)|^p \, \chi_{B(0,R)} (x) \,dx < \infty, $$ where the last inequality follows from the hypothesis $f \, \chi_B \in L^p(\mathbb R^n)$ , for every ball $B \subset \mathbb R^n$ . Now comes my question. Question. Is it possible to build the analogous of $(1)$ to $\Omega$ instead of $\mathbb R^n$ ? That is, is it possible to guarantee something along the lines of $$ f \in L^p_{\text{loc}}(\Omega) \iff f \chi_B \in L^p(\Omega), \quad \text{ for every ball } B \subset \mathbb R^n \quad ? $$ Thanks for any help in advance.","['lebesgue-measure', 'lebesgue-integral', 'lp-spaces', 'solution-verification', 'functional-analysis']"
4915813,"Is it true that |X| vanishes at infinity, then X is integrable?","Suppose $X$ is a random variable on $(\Omega,\mathscr{F},\mathbb{P})$ . If there exists $M>0$ , such that for all $\lambda>0$ , we have $\mathbb{P}[|X|>\lambda]\le\frac{M}{\lambda}$ , then is it true that $X$ is integrable? This condition is weaker than the absolute continuty: Does absolute continuity of integral imply integrability on finite measure space , but compared to absolute continuity, the condition above gives a explicit inequality. So, I'm not sure if it is true. My attempt: $$\mathbb{E}[|X|]=\int_0^{+\infty}\mathbb{P}(|X|>\lambda) \ d\lambda\le M\int_0^{+\infty}\frac{1}{\lambda}\ d\lambda,$$ but the right side is infinite.","['measure-theory', 'statistics', 'probability', 'real-analysis']"
4915817,Show that $f(x)=\int_{0}^{x}\sin(t)g(x-t)dt$ is 2 times differentiable and that $f''+f=g$ [duplicate],"This question already has answers here : Differential Equation: Am I missing a trick? (3 answers) Closed last month . Given g(x) a continuous function on $\mathbb{R}$ , show that $f(x)=\int_{0}^{x}\sin(t)g(x-t)dt$ is 2 times differentiable and that $f''+f=g$ . This problem reminds me another one where $$f(x)=\int_{0}^{x}\sin(t)(x-t)dt$$ and so $$f'(x)=\left(\int_{0}^{x}\sin(t)xdt-\int_{0}^{x}\sin(t)tdt\right)'= \int_{0}^{x}\sin(t)dt+x\sin(x)-\sin(x)x=-\cos(x)+1$$ But here, we do not have a way to split the $g(x-t)$ .","['integration', 'derivatives']"
4915873,Perspective on Differentials Generating Transformations of Functions - with Fourier Transformations $e^{a\partial_x} f(x) = f(x+a)$,"An analytic function $f(x)$ can be transformed by the exponential of a differential operator.
The most known and easiest example is $
e^{a \partial_x} f(x) = f(x+a)
$ Generally this is shown by Taylor series.
Because $f$ is analytic it has a Taylor series $T^{\infty} [f,a](x) = \sum_{n=0}^\infty \frac{f^{(n)}(a)}{n!}(x-a)^n$ . Now the object $e^{a \partial_x}$ is interpreted by Taylor expansion: $e^{a \partial_x} f(x) = \sum_{n=0}^\infty \frac{a^n}{n!} \partial_x^nf(x) = \sum_{n=0}^\infty \frac{f^{(n)}(x)}{n!}a^n = T^{\infty} [f,x](a+x) = f(x+a)$ This is for example used in Quantum Mechanics:
By Noether-Theorem the momentum operator $\hat{p}$ generates translation.
In real space representation the momentum operator is a differential operator $\hat{p} = -i\hbar \nabla$ .
This leads us to $\psi(x+a) = e^{a \frac{i}{\hbar}\hat{p}}\psi(x)$ . The argument can be expanded to more then one dimension and I believe that the factors are simply for dimension and unitarity. From this simple case we can expand to more general differential operators for analytic and invertible functions $g$ : $e^{\frac{d}{dg(x)}}f(x) = f(G(x))$ Using $f(x) = f(g^{-1}(g(x)))$ , we can see that $G(x) = g^{-1}(g(x)+1)$ .
Because of Cauchy-Eulers differential equations, we may be interested in $e^{\frac{d}{d(ln(x)/a)}}=e^{ax\frac{d}{dx}}$ . So $G(x) = e^{a( lnx/a +1)}= e^a x$ , and $x \frac{d}{dx}$ gives us scaling instead of translation. Now the main content: I want to present an easier argument for the translation by exponentiation of differentiation and then use the same argument for the scaling operator. Based on the momentum operator argument, I will look at this operator in momentum space ( $k$ -space).
As Quantum Mechanics (and Fourier Transform) deals with Schwartz functions, so smooth and rapidly decreasing functions, this argument is just for a (very important) subspace of analytic functions. We need two pieces of information before the argument becomes evident:
Be $F(k) = \mathcal{F}(f(x))(k)$ the Fourier transform of f, given it exists and I will be making use of the unitary angular frequency convention. $1. \mathcal{F}(f(x+a))(k) = e^{ i a k} F(k)$ $2. \mathcal{F}(f'(x))(k) = ik F(k) $ $\implies$ $\mathcal{F}(e^{a\frac{d}{dx}} f(x))(k) = e^{a i k }F(k) = \mathcal{F}(f(x+a))(k)  \\ \implies e^{a \frac{d}{dx}} f(x) = f(x+a)$ The last step is also a property of Schwartz functions.
That I can transform the exponent as shown can be seen by examining the series representation.
I believe this to be a neat proof and would like to have some feedback on the argumentation, possible gaps and additional steps! Now as a last step, the application to the scaling operator. $3. \mathcal{F}(f(a x))(k) = \frac{1}{a} F(\frac{k}{a}), a>0$ $4. \mathcal{F}(xf'(x) )(k) = \frac{1}{\sqrt{2\pi}} \mathcal{F}(x)(k) * \mathcal{F}(f'(x))(k) = \frac{1}{\sqrt{2\pi}} (i \sqrt{2\pi} \delta'(k) )*(ik F(k) ) = - (kF(k))' = - F(k) - k F'(k) = (-1-k\frac{d}{dk})F(k)$ $\implies$ $ \mathcal{F}(e^{a x\frac{d}{dx}} f(x))(k) = e^{-a-k\frac{d}{dk} }F(k) = e^{-a}F(e^{-a}k) \\ \iff e^{a x\frac{d}{dx}} f(x) = f(e^a x) $ Thanks for reading! I'm open to questions and comments, so I can close remaining argumentative gaps. I hope you liked my perspective on this transformation and find these ideas useful.","['fourier-transform', 'functions', 'solution-verification', 'functional-analysis', 'transformation']"
4915899,What do $du$ and $dv$ mean when used in an equation without an integral sign?,"Main Question How do I evaluate the expression $L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}$ , given values for $L(u, v)$ , $M(u, v)$ , and $N(u, v)$ ? Background While working on a personal project, I came upon this definition from Wikipedia for the second fundamental form (lightly paraphrased): Let $\mathbf{r} = \mathbf{r}(u,v)$ be a regular parametrization of a surface $S$ in $â„^3$ , where $\mathbf{r}$ is a smooth vector-valued function of two variables. It is common to denote the partial derivatives of $\mathbf{r}$ with respect to $u$ and $v$ by $\mathbf{r}_{u}$ and $\mathbf{r}_{v}$ . Regularity of the parametrization means that $\mathbf{r}_{u}$ and $\mathbf{r}_{v}$ are linearly independent for any $(u,v)$ in the domain of $\mathbf{r}$ , and hence span the tangent plane to $S$ at each point. Equivalently, the cross product $\mathbf{r}_{u} Ã— \mathbf{r}_{v}$ is a nonzero vector normal to the surface. The parametrization thus defines a field of unit normal vectors $\mathbf{n}$ : $$
\mathbf{n} =\frac{\mathbf{r}_{u} Ã— \mathbf{r}_{v}}{|\mathbf{r}_{u} Ã— \mathbf{r}_{v}|}
$$ The second fundamental form is usually written as $$
\mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}
$$ where the coefficients $L$ , $M$ , and $N$ at a given point can be computed with the aid of the dot product as follows: $$
L = \mathbf{r}_{uu} â‹… \mathbf{n}, \quad M = \mathbf{r}_{uv} â‹… \mathbf{n}, \quad N = \mathbf{r}_{vv} â‹… \mathbf{n}
$$ The surface $S$ with which I am dealing is is a smooth, closed, convex surface parametrized as $$
\begin{alignat}{3}
X(Î¸, Ï†) &= \sin(Î¸) â‹… \cos(Ï†)\:& â‹… &\:h(Î¸, Ï†) + \cos(Î¸) â‹… \cos(Ï†)\:& â‹… &\:h_Î¸(Î¸, Ï†) - \frac{\sin(Ï†)}{\sin(Î¸)} â‹… h_Ï†(Î¸, Ï†), \\
Y(Î¸, Ï†) &= \sin(Î¸) â‹… \sin(Ï†)\:& â‹… &\:h(Î¸, Ï†) + \cos(Î¸) â‹… \sin(Ï†)\:& â‹… &\:h_Î¸(Î¸, Ï†) + \frac{\cos(Ï†)}{\sin(Î¸)} â‹… h_Ï†(Î¸, Ï†), \\
Z(Î¸, Ï†) &= \cos(Î¸)\:& â‹… &\:h(Î¸, Ï†) - \sin(Î¸)\:& â‹… &\:h_Î¸(Î¸, Ï†),
\end{alignat}
$$ given a support function $h(Î¸, Ï†)$ and its partial derivatives $h_Î¸(Î¸, Ï†) = \frac{âˆ‚ h}{âˆ‚ Î¸}$ and $h_Ï†(Î¸, Ï†) = \frac{âˆ‚ h}{âˆ‚ Ï†}$ , with spherical coordinates $0 â‰¤ Î¸ â‰¤ Ï€$ and $0 â‰¤ Ï† â‰¤ 2 Ï€$ . I also know that the unit normal vector of $S$ at a point $P(u, v) = \bigl(X(u, v), Y(u, v), Z(u, v)\bigr)$ is $$
\mathbf{n}(u, v) = \bigl(\sin(u) â‹… \cos(v), \,\sin(u) â‹… \sin(v), \,\cos(u)\bigr)
$$ From all of the above, and with the help of WolframAlpha, I've worked out that $$
\begin{align}
L &= -h(u, v) - h_{uu}(u, v), \\
M &= \cot(u) â‹… h_{u}(u, v) - h_{uv}(u, v), \\
N &= -\sin(u)^2 â‹… h(u, v) - h_{vv}(u, v)  - \cos(u) â‹… \sin(u) â‹… h_{u}(u, v)
\end{align}
$$ However, it's been a while since my calculus education, and I can't remember what it means to have $du$ and $dv$ in an equation like $\mathrm{I\!I} = L\,du^{2} + 2 M\,du\,dv + N\,dv^{2}$ without integral signs. I've tried searching online, but haven't found a solid answer. Are they just there as a reminder that, for example, $L$ is a product of the second derivative of $h$ with respect to $u$ , without actually indicating a mathematical operation to be carried out on $L$ ? Are they saying I should integrate, with the integration signs being implied? Or do they mean something else?","['multivariable-calculus', 'calculus']"
4915913,How to evaluate $\int_0^\infty (\frac{\tanh x}{x^3} - \frac{\operatorname{sech}^2 x}{x^2}) dx$? [duplicate],"This question already has answers here : Integral $\int_{0}^{\infty}\frac{1}{x^2}(\frac{1}{\cosh^2x}-\frac{\tanh{x}}{x})dx$ [duplicate] (2 answers) Closed last month . The integral $$\int_0^\infty (\frac{\tanh x}{x^3} - \frac{\operatorname{sech}^2 x}{x^2}) dx$$ appears when one tries to calculate the jump in specific heat due to the superconducting phase transition using BCS theory. In Carsten Timm's lecture notes on superconductivity, the value of the integral is quoted to be $\frac{7\zeta(3)}{\pi^2}$ without proof. How to evaluate this integral?","['integration', 'definite-integrals']"
4915948,How to evaluate $\int_0^1 \ln ^3(1+x) \ln (1-x) d x$?,"QUESTION :How to evaluate $$\int_0^1 \ln ^3(1+x) \ln (1-x) d x$$ ? I'm not sure of the closed form of the integral, as I haven't evaluated it yet. However, after evaluating the integral $$\int_0^1 \ln (1+x) \ln (1-x) \, dx,$$ I thought about integrating $$\int_0^1 \ln^3(1+x) \ln (1-x) \, dx.$$ I am also interested in evaluation of $$\int_0^1 \ln^n(1+x) \ln (1-x) \, dx.$$ If possible Here is my attempt \begin{aligned}
& I=\int_0^1 \ln ^3(1+x) \ln (1-x) d x \\
& \text { Let }: a=\ln (1-x) \wedge b=\ln (1+x) \\
& \text { and } \quad b^3 a=\frac{1}{8}(a+b)^4-\frac{1}{8}(a-b)^4-a^3 b \\
& \Rightarrow I=\frac{1}{8} \underbrace{\int_0^1 \ln ^4\left(1-x^2\right) d x}_A-\frac{1}{8} \underbrace{\int_0^1 \ln ^4\left(\frac{1-x}{1+x}\right) d x}_B \\
& -\underbrace{\int_0^1 \ln ^3(1-x) \ln (1+x) d x}_C \\
& A=\frac{1}{2} \int_0^1 x^{-\frac{1}{2}} \ln ^4(1-x) d x=\left.\frac{1}{2} \mathrm{~B}^{(4)}\left(\frac{1}{2}, a\right)\right|_{a=1} \\
& =\frac{1}{2} \mathrm{~B}\left(\frac{1}{2}, a\right)\left\{\left[\psi^{(0)}(a)-\psi^{(0)}\left(a+\frac{1}{2}\right)\right]^4\right. \\
& +6\left[\psi^{(1)}(a)-\psi^{(1)}\left(a+\frac{1}{2}\right)\right]\left[\psi^{(0)}(a)-\psi^{(0)}\left(a+\frac{1}{2}\right)\right]^2 \\
& +4\left[\psi^{(2)}(a)-\psi^{(2)}\left(a+\frac{1}{2}\right)\right]\left[\psi^{(0)}(a)-\psi^{(0)}\left(a+\frac{1}{2}\right)\right] \\
& \left.+3\left[\psi^{(1)}(a)-\psi^{(1)}\left(a+\frac{1}{2}\right)\right]^2+\psi^{(3)}(a)-\psi^{(3)}\left(a+\frac{1}{2}\right)\right\}\left.\right|_{a=1} \\
& \text { But }: \psi^{(0)}=-\gamma \wedge \psi^{(0)}\left(\frac{3}{2}\right)=2-2 \ln (2)-\gamma \\ \text{and to $n \in \mathbb{N}^{+}$:}\\
& \star \psi^{(n)}(1)-\psi^{(n)}\left(\frac{3}{2}\right)=(-1)^{n+1} n!\left[2^{n+1}+\left(2-2^{n+1}\right) \zeta(n+1)\right] \\
& \Rightarrow A=(2 \ln (2)-2)^4+6(4-2 \zeta(2))(2 \ln (2)-2)^2 \\
& +4(12 \zeta(3)-16)(2 \ln (2)-2)+3(4-2 \zeta(2))^2+96-84 \zeta(4) \\
& \Rightarrow A=-48 \ln ^2(2) \zeta(2)+96 \ln (2)(\zeta(3)+\zeta(2))+16 \ln ^4(2) \\
& -64 \ln ^3(2)+192 \ln ^2(2)-54 \zeta(4)-96 \zeta(3)-96 \zeta(2) \\
& -384 \ln (2)+384 \ldots(\alpha) \\
&
\end{aligned} As you can see, I was able to solve the integral $A$ , but I don't have any ideas for integrals $B$ and $C.$","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4915968,Find optimal control for cooling a cup of coffee,"I have the following problem: Exercise 2. - A cup of coffee is initially at 100Â°C, and we want to lower its temperature to 0Â°C as quickly as possible by adding a fixed amount of milk. If $x(t)$ is the temperature of the coffee and milk mixture, the cooling law is given by $$
x'(t) = -x(t) - 25u(t) - \frac{1}{4}x(t)u(t),$$ where $u(t)$ is what is being added of milk, restricted to $0 \leq u(t) \leq 1$ and also $$
\int_0^T u(t)\,dt = 1,
$$ a) Reason that the optimal control should be of the form $$
u(t) = \begin{cases}
        0, & 0 < t < t_0, \\
        1, & t_0 < t < T,
    \end{cases}
    $$ for a certain $t_0$ .  b) Taking into account the previous section, find the optimal control. Using a) it easy to compute $t_0$ and $T$ , for which I obtained $t_0=0.697$ and $T=1.697$ . But I don't know how to proceed with section a). I have the constraints $x(0)=100$ and $x(T)=0$ , the Hamiltonian of the system is $$H=u+p(-x-25u-xu/4),$$ so the equation for $p$ is on the form $$p'=-H_x=p(1+u/4),$$ but since $u$ also depends on $t$ I don't know how to  continue. Any help will be very appreciated. Thanks in advance! EDITED: I managed to prove the following : We define the Hamiltonian (H) for our system: $$ H = \lambda(t) \left(-x(t) - 25u(t) - \frac{1}{4}x(t)u(t)\right) + \mu u(t), $$ where $\lambda(t)$ is the constant (or adjoint cost) multiplier and $\mu$ is the multiplier associated with the constraint $\int_0^T u(t) \, dt = 1$ . The maximum principle states that the optimal control $u^*(t)$ maximizes the Hamiltonian $H$ at each time instant $t$ : $$ u^*(t) = \arg\max_{0 \leq u \leq 1} H. $$ We calculate the derivative of the Hamiltonian with respect to $u$ $$ \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu. $$ The decision of whether $u(t) = 0$ or $u(t) = 1$ at a given moment depends on the sign of the derivative $$ \frac{\partial H}{\partial u} = \lambda(t) \left(-25 - \frac{1}{4}x(t)\right) + \mu $$ $$\frac{\partial H}{\partial u} > 0 \implies u^*(t) = 1$$ $$\frac{\partial H}{\partial u} < 0\implies u^*(t) = 0$$ I'm not really sure about the last two statements. However I don't know how to conclude the problem. Please help. Thanks in advance!","['optimal-control', 'optimization', 'ordinary-differential-equations']"
4916033,Reducing product of powers of logarithm,"I am trying to show that $$(\log(a))^n (\log(b))^m = P(\log(a^ib^j)), \quad i,j \in \{-1,0,1\}$$ where $P$ is a polynomial and $n \ge m \ge 1$ are natural numbers. Using Binomial identities for the cases $(n,m) \in \{(1,1),(2,1),(2,2)\}$ , I was able to obtain $$ \log(a)\log(b) = \frac{1}{2}\left[ (\log(ab))^2-(\log(a))^2-(\log(b))^2 \right] \\ 
\left(\log\left(a\right)\right)^{2}\log\left(b\right) = \frac{1}{6}\left((\log\left(ab\right))^{3}-(\log\left(ab^{-1}\right))^{3}-2(\log\left(b\right))^{3}\right)\\
\left(\log\left(a\right)\right)^{2}\left(\log\left(b\right)\right)^{2}= \frac{1}{12}\left(\log\left(ab\right)^{4}+\log\left(ab^{-1}\right)^{4}-2\log\left(a\right)^{4}-2\log\left(b\right)^{4}\right)
$$ Is this true for larger values of $n$ and $m$ ? For $n=3,m=1$ , I am stuck at $$\log\left(a\right)^{3}\log\left(b\right) = \frac{1}{8}\left(\log\left(ab\right)^{4}-\log\left(ab^{-1}\right)^{4}-8\log\left(a\right)\log\left(b\right)^{3}\right).$$ I am not sure how to get rid of the third term on the right side. For which $(n,m) \in \mathbb{N} \times \mathbb{N}$ is the given statement true? Edit : $P$ can be written as $$P = \sum_{k=1}^{M} \sum_{i,j \in \{-1,0,1 \}}a_{i,j,k} (\log(a^ib^j))^k.$$ Addendum: A great idea by @Jair to simplify notation is to substitute $x=\log{a}$ and $y=\log{b}$ . Then the above three equations can be written as $$ xy = \frac{1}{2}\big( (x+y)^2-x^2-y^2\big) \\ x^2y = \frac{1}{6}\left((x+y)^3-(x-y)^3-2y^3 \right) \\ 
x^2y^2 = \frac{1}{12}\left( (x+y)^4+(x-y)^4-2x^4-2y^4\right)$$ and $P =  \sum_{k=1}^{M} \sum_{i,j \in \{-1,0,1 \}}a_{i,j,k} (ix+jy)^k$ . The problem can be rephased as: Does their exist $M \in \mathbb{N}$ such that for $n \ge m \ge 1$ , $$ x^n y^m = \sum_{k=1}^{M} \sum_{i,j \in \{-1,0,1 \}}a_{i,j,k} (ix+jy)^k$$","['algebra-precalculus', 'binomial-coefficients', 'binomial-theorem', 'logarithms']"
4916064,Using Cauchy's estimate to show a function is constant,"If $f$ is entire function satisfying $|f(z)|\leq |z|^{1/2}\log(1+|z|+|z|^2)$ . Show that $f(z)=c$ for some $c\in \mathbb{C}$ . My approach: Let $p\in \mathbb{C}$ and $\mathbb{D}_r(p)\subset \mathbb{C}$ . Then $f$ is continuous on the closure of the disc and holomorphic on the interior, because it is entire. Now, I apply Cauchy's estimate. For $n=1$ we have the following $$|f^\prime(p)| \leq \dfrac{1}{r}\sup_{|z-p|=r}|f(z)|\leq \dfrac{1}{r} \sup_{|z-p|=r}|z|^{1/2}\log(1+|z|+|z|^2)$$ since this is true for all $r>0$ . I just take $r\to \infty$ and we have $f ^\prime (p)=0$ . Completing the proof. I am not sure the part where I take $r$ to infinity is allowed since the sup also depends on $r$ ?",['complex-analysis']
4916068,Can the Euclidean unit interval have a finer connected topology?,"Consider the unit interval $X = [0,1]$ equipped with the Euclidean topology $\tau_E$ , and consider some other topology $\tau_F$ on $X$ that is strictly finer than $\tau_E$ . Does there exist such a topology $\tau_F$ for which $(X,\tau_F)$ is a connected space? I suspect there is either 1) a simple, abstract way of showing this topology cannot exist, or 2) a somewhat complicated example showing it can exist. PARTIAL RESULTS: Consider a process by which (at least) one subset of $X$ is added to the Euclidean topology to form $\tau_F$ . No such subset could be a finite subset of $X$ , as finite subsets of $X$ are closed with respect to the Euclidean topology and therefore clopen in $(X,\tau_F)$ , disconnecting the space. Therefore, $\tau_F$ cannot contain any finite subsets of $X$ . Now consider adding (at least) one subset interval of $X$ that is not in $\tau_E$ to $\tau_E$ to form $\tau_F$ . Such a subset cannot be an open interval, as this is already included in $\tau_E$ . Such a subset cannot be a closed interval, and the resulting set would be clopen in $\tau_F$ , disconnecting the space. Lastly, it cannot be a half-open interval $[a,b)$ or $(a,b]$ , as the space can always be split into two disjoint open sets of $\tau_F$ of the following form: $$X = [0,a)\cup\left([a,b)\cup(\frac{a+b}{2},1]\right) = [0,a)\cup[a,1]$$ or $$X = \left([0,\frac{a+b}{2})\cup(a,b]\right)\cup(b,1] = [0,b]\cup(b,1]$$ As a result, $\tau_F$ must also not contain any half-open/half-closed or closed intervals.","['general-topology', 'connectedness']"
4916102,Justification for differentiation under integral,"Given a function I seek to find its derivative $$f(x) = \int_{\frac{1}{x}}^{\frac{e^x}{x}} \frac{\cos(xt)}{t} \, dt, \quad (x>0)$$ My question is regarding the justification of the differentiation under the integral sign rather than how to do it.
The theorems that I have at hand are the following. $\textbf{Theorem 1}$ Let $[a,b]$ compact interval, $J$ open interval, suppose that $f(x,t), \frac{\partial f}{\partial t}(x, t)$ continuous for $(x,t)\in [a,b]\times J$ . Then $F(t) = \int_{a}^{b}\frac{\partial f}{\partial t}(x, t) \,dx\quad t\in J$ . $\textbf{Theorem 2 (For generalised domains)}$ Suppose there are majorants $g(x), h(x)$ such that $\left| f(x,t) \right| \le g(x), \left| \frac{\partial f}{\partial t}(x, t) \right| \le h(x)$ and $\int_Ig(x)\,dx < \infty,\quad \int_Ih(x)\,dx < \infty $ . We form $F(t) = \int_If(x,t)\, dx$ then $F(t)$ is differentiable and $F'(t) = \int_I\frac{\partial f}{\partial t}(x, t) \,dx$ . Now if we come back to the function I gave as an example if $x<\infty$ then the interval is compact it is also clear that the integrand and its derivative are continuous on that interval hence differentiation is justified as I understand. Now consider $x \to \infty$ if we use the second theorem a possible majorant is $\frac{1}{t}$ but leads to $x<\infty$ . As I understand this is not possible as $x$ here is a variable. A similar issue occurs with bounding its derivative. Could anyone clarify what I am misunderstanding, or perhaps I am choosing the wrong majorants? Perhaps I have to assume that the interval is not generalised. Thanks for any ideas and clarifications!","['integration', 'real-analysis', 'calculus', 'vector-analysis', 'derivatives']"
4916105,Systems of equations involving unison and intersection,"While studying systems of linear equations in linear algebra, i see that the general definition is that:                                                                over a field F a standard system of equations of n variables is that of the form $$a_1x_1+a_2x_2+...+a_nx_n=b$$ , where $x_1, x_2,...,x_n$ are the variables, $a_1, a_2,..., a_n$ are the coefficients and b is the constant term. I could assume a field F where addition is $\cup$ and multiplication is $\cap$ , as $A\cup \varnothing=A$ just like $a+0=a$ ,0 is $\varnothing$ and 1 is (?), this is another question, is there a one in this set, and an inverse? Assuming there are, and even if there is not, what would a linear equation and a system of those look like including the solutions to them? Is there a one, and are there inverses using only $\cup$ and $\cap$ , if there are, what would a system of equations look under that field. Can linear equations be solved not under a field? And can these equations be solved not under a field? If yes, what would the solution look like?","['elementary-set-theory', 'systems-of-equations', 'linear-algebra', 'set-theory']"
4916137,Anything interesting known about this generalization of even and odd functions?,"Let $n \in \mathbb N$ . Let's say a complex function $f: U \rightarrow \mathbb C$ is ""of type $k \pmod n$ "" if for one (and hence every) primitive $n$ -th root of unity $\omega$ , $$f(\omega z) = \omega^k f(z)$$ for all $z\in U \subseteq \mathbb C$ . (Obviously the domain $U$ has to be closed under multiplying with $\omega$ .) For $f$ analytic around $0$ , this is equivalent to: In the Taylor expansion $f(z) = \sum a_m z^m$ , all $a_m$ except possibly those with $m \equiv k \pmod n$ are zero. The sum of two functions of type $k \pmod n$ is again of type $k \pmod n$ . The product of a function of type $k \pmod n$ and a function of type $l \pmod n$ is of type $(k+l) \pmod n$ . Also, the derivative of a differentiable function of type $k \pmod n$ is of type $k-1 \pmod n$ . For $n=2$ , this retrieves the classical ""even"" and ""odd functions"" taught to this day in high schools. I just stumbled upon these , or shifts of them, as rare exceptional solutions to functional identities which otherwise usually have no interesting solutions. I wondered if there is a better name for them, and if they are useful in some theory I have missed. (This seems to be at least one level of difficulty below modular forms of weight $k$ , but maybe somebody can take me by the hand and show a connection to those. Or to the seemingly very different generalization here , maybe bringing in representation theory of the cyclic group $\mathbb Z/n$ .)","['even-and-odd-functions', 'complex-analysis', 'roots-of-unity', 'modular-forms', 'analytic-functions']"
4916212,Pigeonhole Principle Question: Place 1600 points inside a unit square such that there is at least one point inside every rectangle of area 1/200,"Tim wants to place $1600$ points inside a unit square such that there is at least one point inside every rectangle of area $\frac{1}{200}$ and with sides parallel to those of the
square. Is it possible to do it? If your answer is yes, please give an example and show that it works. If your answer is no, please prove it is impossible. A rectangle of area $\frac{1}{200}$ may be $\frac{1}{10} \times \frac{1}{20}$ or $\frac{1}{200} \times \frac{1}{1}$ or $\frac{1}{1} \times \frac{1}{200}$ , ... . In my research, to make sure that every rectangle of area $\frac{1}{1} \times\frac{1}{200}$ or $\frac{1}{200} \times \frac{1}{1}$ contains at least one point, we need to make sure that in every row and column of the grid of $200 \times 200$ must contains at least one point. This strategy needs $200 \times 200=40,000$ points. I would appreciate any help, especially a hint or approach that I can use to find the rest of the solution on my own!","['contest-math', 'pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
4916221,Proving a (Representing Utility) Function is Continuous,"This question was migrated from MathOverflow because it can be answered on Mathematics Stack Exchange. Migrated last month . I sincerely apologize for posting such a long question. The question involves a complicated proof of a theorem in mathematical economics. I feel it will be better for me to state my question first. I will add all related background information afterwards. So here is my question: I need to prove that a function $\alpha(x)$ is continuous at all $x\in\mathbb{R}^L_+$ . Since every point of $\mathbb{R}^L_+$ is a limit point, I could proceed to show that for any sequence $\{x^n\}_{n=1}^{\infty}$ with $x=\lim_{n\to\infty}x^n$ we have $\lim_{n\to\infty}\alpha(x^n)=\alpha(x)$ . So, consider a sequence $\{x^n\}_{n=1}^{\infty}$ such that $x=\lim_{n\to\infty}x^n$ . Suppose that I can show the sequence $\{\alpha(x^n)\}_{n=1}^{\infty}$ has a convergent subsequence. Moreover, suppose that I can show all convergent subsequences of $\{\alpha(x^n)\}_{n=1}^{\infty}$ converge to $\alpha(x)$ . Does this imply that $\lim_{n\to\infty}\alpha(x^n)=\alpha(x)$ ? If so, why is that? As you can see, it seems that the proof below says ""yes"" to the above question. But I am confused, because every convergent subsequence converges to the same point does not necessarily imply the original sequence converges to that point (for example, consider the sequence $0,1,0,2,0,3,0,4,\dots$ ). I think we should instead prove that every subsequence of the original sequence converges to the same point. But I couldn't figure out how to do that. Am I missing anything? How should we fix this? Thanks a lot for any help! As I mentioned in the beginning, the question is from a proof of a theorem in mathematical economics. So let me state the theorem together with its proof in this section. In the next section, I will provide related notations and definitions. Theorem $\quad$ Suppose that the rational preference relation $\succsim$ on $X=\mathbb{R}^L_+$ is monotone and continuous. Then there exists a continuous utility function $u(x)$ that represents $\succsim$ . Before we prove the theorem, we need a lemma. Lemma $\quad$ For a monotone, continuous, and rational preference relation $\succsim$ on $X=\mathbb{R}^L_+$ , $y\geq x$ implies $y\succsim x$ . Proof of lemma $\quad$ Let $\succsim$ be a monotone, continuous, complete, and transitive preference relation on $X=\mathbb{R}^L_+$ . Suppose first that $y\gg x$ ; that is, $y_i>x_i$ for all $i=1,\dots,L$ . Then by the definition of monotonicity, $y\succ x$ and thus $y\succsim x$ . Now suppose that $y_i=x_i$ for some $i\in\{1,\dots,L\}$ and $y_k>x_k$ for all $k\neq i$ . (So $y\geq x$ .) Let $\{y^n\}$ be a sequence of points in $\mathbb{R}^L_+$ such that $y^n_k=y_k$ for $k\neq i$ and $y^n_i=y_i+\frac{1}{n}$ . Let $\{x^n\}$ be a sequence of points in $\mathbb{R}^L_+$ such that $x^n_k=x_k$ for $k\neq i$ and $x^n_i=x_i+\frac{1}{n+1}$ . Then for each $n$ , $y^n_k = y_k > x_k = x^n_k$ and $y^n_i = y_i + \frac{1}{n} = x_i + \frac{1}{n} > x_i + \frac{1}{n+1} = x^n_i$ . Thus $y^n\gg x^n$ , and monotonicity implies $y^n\succ x^n$ , and so $y^n\succsim x^n$ . So we have constructed a sequence of pairs $\{(x^n,y^n)\}$ with $y^n\succsim x^n$ for all $n$ , $x=\lim_{n\to\infty}x^n$ , and $y=\lim_{n\to\infty}y^n$ . Then continuity implies $y\succsim x$ . Now we prove the theorem. Proof $\quad$ We proceed our proof with the help of Figure 1. Denote the diagonal ray in $\mathbb{R}^L_+$ (the locus of vectors with all $L$ components equal) by $Z$ . It will be convenient to let $e$ designate the $L$ -vector whose elements are all equal to 1. Then $\alpha e\in Z$ for all nonnegative scalars $\alpha\geq0$ . By the above lemma, for every $x\in\mathbb{R}^L_+$ , we have $x\succsim 0$ . Also note that for any $\overline{\alpha}$ such that $\overline{\alpha}e\gg x$ (as drawn in the figure), we have $\overline{\alpha}e\succ x$ and so $\overline{\alpha}e\succsim x$ by monotonicity. Then, monotonicity and continuity can then be shown to imply that there is a unique value $\alpha(x)\in[0,\overline{\alpha}]$ such that $\alpha(x)e\sim x$ . Formally, this can be shown as follows: By continuity, the upper and lower contour sets of $x$ are closed. We show that the set $A^+ = \{\alpha\in\mathbb{R}_+:\alpha e\succsim x\}$ and $A^- = \{\alpha\in\mathbb{R}_+:x\succsim\alpha e\}$ are nonempty and closed. To see the nonemptiness of $A^+$ , note that for any $x=(x_1,\dots,x_L)\in\mathbb{R}_+$ , we can always choose an $\alpha' = \max\{x_1,\dots,x_L\}+1$ , so that $\alpha'e\succ x$ . Thus $A^+\neq\emptyset$ . To see the nonemptiness of $A^-$ , simply pick $\alpha''=0$ , so that $x\succsim0=\alpha''e$ would imply $A^-\neq\emptyset$ . We now prove the closedness of $A^+$ and $A^-$ . Let $\alpha$ be a limit point of $A^+$ . Then there exists a sequence $\{\alpha_n\}$ in $A^+$ such that $\alpha = \lim_{n\to\infty}\alpha_n$ and $\alpha_ne\succsim x$ for all $n$ . Consider the sequence of pairs $\{(x,\alpha_ne)\}$ with $\alpha_ne\succsim x$ , $x = \lim_{n\to\infty}x$ , and $\alpha e = \lim_{n\to\infty}\alpha_ne$ . By continuity, we have $\alpha e\succsim x$ , so that $\alpha\in A^+$ . Hence every limit point of $A^+$ is a point of $A^+$ , which means $A^+$ is closed. A similar argument would show that $A^-$ is closed. Finally, not that by completeness of $\succsim$ , $\mathbb{R}_+=(A^+\bigcup A^-)$ . Then the nonemptiness and closedness of $A^+$ and $A^-$ , together with the fact $\mathbb{R}_+$ is connected, imply that $A^+\bigcap A^-\neq\emptyset$ . Thus there exists a scalar $\alpha$ such that $\alpha e\sim x$ . Furthermore, by monotonicity, $\alpha_1e\sim\alpha_2e$ whenever $\alpha_1>\alpha_2$ . Hence there can be at most one scalar satisfying $\alpha e\sim x$ . This scalar is $\alpha(x)$ . We now take $\alpha(x)$ as our utility function; that is, we assign a utility value $u(x)=\alpha(x)$ to every $x$ . This utility level is also depicted in Figure 1. We need to check two properties of this function: that it represents the preference $\succsim$ (i.e., that $\alpha(x)\geq\alpha(y)$ if and only if $x\succsim y$ ) and that it is a continuous function. That $\alpha(x)$ represents preferences follows from its construction. Formally, suppose first that $\alpha(x)\geq\alpha(y)$ . Then either $\alpha(x)>\alpha(y)$ or $\alpha(x)=\alpha(y)$ . If $\alpha(x)>\alpha(y)$ , then $\alpha(x)e = (\alpha(x),\dots,\alpha(x)) \gg (\alpha(y),\dots,\alpha(y)) = \alpha(y)e$ . By the monotonicity, this implies that $\alpha(x)e\succ\alpha(y)e$ , and so $\alpha(x)e\succsim\alpha(y)e$ . If $\alpha(x)=\alpha(y)$ , then $\alpha(x)e=(\alpha(x),\dots,\alpha(x)) = (\alpha(y),\dots,\alpha(y))=\alpha(y)e$ . The above lemma implies that $\alpha(x)e\succsim\alpha(y)e$ . Therefore, $\alpha(x)e\succsim\alpha(y)e$ . Since $x\sim\alpha(x)e$ and $y\sim\alpha(y)e$ , we have $x\succsim y$ . Suppose, on the other hand, that $x\succsim y$ . Then $\alpha(x)e\sim x\succsim y\sim\alpha(y)e$ . Assume to the contrary that $\alpha(y)>\alpha(x)$ . Then $\alpha(y)e\gg\alpha(x)e$ , and monotonicity would imply that $y\sim\alpha(y)e\succ\alpha(x)e\sim x$ , a contradiction. Therefore, $\alpha(x)\geq\alpha(y)$ . (My question above essentially begins from here:) (Cont'd) We now argue that $\alpha(x)$ is a continuous function at all $x$ . Since every point of $\mathbb{R}^L_+$ is a limit point of $\mathbb{R}^L_+$ , we shall prove for any sequence $\{x^n\}$ with $x=\lim_{n\to\infty}x^n$ , we have $\lim_{n\to\infty}\alpha(x^n)=\alpha(x)$ . Hence consider a sequence $\{x^n\}$ such that $x=\lim_{n\to\infty}x^n$ . We note first that the sequence $\{\alpha(x^n)\}$ must have a convergent subsequence. By monotonicity, for any $\epsilon>0$ , $\alpha(x')$ lies in a compact subset of $\mathbb{R}_+$ , $[\alpha_0,\alpha_1]$ , for all $x'$ such that $\|x'-x\|\leq\epsilon$ (see the figure below). Since $\{x^n\}_{n=1}^{\infty}$ converges to $x$ , there exists an $N$ such that $\alpha(x^n)$ lies in this compact set for all $n>N$ . But any infinite sequence that lies in a compact set must have a convergent subsequence. What remains is to establish that all convergent subsequences of $\{\alpha(x^n)\}_{n=1}^{\infty}$ converges to $\alpha(x)$ . To see this, suppose otherwise: that there is some strictly increasing function $m(\cdot)$ that assigns to each positive integer $n$ a positive integer $m(n)$ and for which the subsequence $\{\alpha(x^{m(n)})\}_{n=1}^{\infty}$ converges to $\alpha'\neq\alpha(x)$ . We first show that $\alpha'>\alpha(x)$ leads to a contradiction. To begin, note that monotonicity would then imply that $\alpha'e\succ\alpha(x)e$ . Now, let $\hat{\alpha} = \frac{1}{2}[\alpha'+\alpha(x)]$ . The point $\hat{\alpha}e$ is the midpoint on $Z$ between $\alpha'e$ and $\alpha(x)e$ (see the second figure). By monotonicity, $\hat{\alpha}e\succ\alpha(x)e$ . Now, since $\alpha(x^{m(n)})\to\alpha'>\hat{\alpha}$ , there exists an $\overline{N}$ such that for all $n>\overline{N}$ , $\alpha(x^{m(n)})>\hat{\alpha}$ . Hence, for all such $n$ , $x^{m(n)}\sim\alpha(x^{m(n)})e\succ\hat{\alpha}e$ (where the latter relation follows from monotonicity). Because preferences are continuous, this would imply that $x\succsim\hat{\alpha}e$ . But since $x\sim\alpha(x)e$ , we get $\alpha(x)e\succsim\hat{\alpha}e$ , which is a contradiction. The argument ruling out $\alpha'<\alpha(x)$ is similar. Thus, since all convergent subsequences of $\{\alpha(x^n)\}_{n=1}^{\infty}$ must converge to $\alpha(x)$ , we have $\lim_{n\to\infty}\alpha(x^n)=\alpha(x)$ , and we done. Some related notations and definitions: Notation $\quad$ $\mathbb{R}^L_+ = \left\{x\in\mathbb{R}^L:x_l\geq0\ \text{for}\ l=1,\dots,L\right\}$ . Notation $\quad$ Let $x,y\in\mathbb{R}^N$ so $x=(x_1,\dots,x_N)$ and $y=(y_1,\dots,y_N)$ . Then (i) $x\geq y$ means $x_n\geq y_n$ for all $n=1,\dots,N$ ; (ii) $x\gg y$ means $x_n>y_n$ for all $n=1,\dots,N$ . Definition $\quad$ The preference relation $\succsim$ on $X$ is rational if it possesses the following two properties: (i) Completeness . For all $x,y\in X$ , we have $x\succsim y$ or $y\succsim x$ (or both). (ii) Transitivity . For all $x,y,z\in X$ , if $x\succsim y$ and $y\succsim z$ , then $x\succsim z$ . Definition $\quad$ The preference relation $\succsim$ on $X$ is monotone if $x\in X$ and $y\gg x$ implies $y\succ x$ . Definition $\quad$ The preference relation $\succsim$ on $X$ is continuous if it is preserved under limits. That is, for any sequence of pairs $\{(x^n,y^n)\}_{n=1}^{\infty}$ with $x^n\succsim y^n$ for all $n$ , $x=\lim_{n\to\infty}x^n$ , and $y\lim_{n\to\infty}y^n$ , we have $x\succsim y$ . Reference $\quad$ Mas-Colell, Andreu, Michael D. Whinston, and Jerry R. Green (1995). Microeconomic Theory . New York: Oxford University Press. Update (5/13/2024, 9:16 PM EDT) $\quad$ As you can see, the statement of the theorem here as well as the proof I presented in this post is different from the original one (Proposition 3.C.1) in the book Microeconomic Theory . This is because I have revised and corrected certain parts of the original proof. But I couldn't figure out this final step of proving $\{\alpha(x^n)\}_{n=1}^{\infty}$ converges to $\alpha(x)$ . As is pointed out by @StevenLandsburg, the idea of proving every subsequence of the original sequence $\{\alpha(x^n)\}_{n=1}^{\infty}$ won't work. I would really appreciate it if someone could offer a strategy to fix the proof of the convergence of $\{\alpha(x^n)\}_{n=1}^{\infty}$ to $\alpha(x)$ ! Thank you very much!","['economics', 'continuity', 'decision-theory', 'sequences-and-series']"
4916264,Calculate $\sum_{n=1}^\infty (1-\alpha) \alpha^nP^{*n}(x)$,"$$
\mbox{Let}\quad
P'(x)=\sum_{j=1}^n a_j\frac{a^jx^{j-1}}{(j-1)!}e^{-ax},x\geq 0
$$ be the density function of a mixture of Erlangs and let $\alpha\in(0,1)$ : Is is possible to determine an analytic expression for $$
\sum_{n=0}^\infty (1-\alpha) \alpha^nP^{*n}(x)$$ where $P^{*n}$ is the $n-$ fold convolution of $P$ with itself and $P^{*0}(x)=0$ ?. I was thinking about calculating $\mathcal L^{-1}([\mathcal L(P)]^n)$ , where $\mathcal L$ is the Laplace transform, but this looks really hard.","['integration', 'laplace-transform', 'convolution', 'real-analysis', 'probability-theory']"
4916273,Least number of circles required to cover a continuous function on a closed interval.,"Now asked on MO here . This question is a generalisation of a prior question . Given a continuous function $f :[a,b]\to\mathbb{R}$ , what is the least number of  circles with radius $r$ required to cover the graph of $f$ ? It is easy to prove (by using the extreme value theorem) that only finitely many circles  are required to cover the graph of $f$ . But how can I find the least number of circles? I don't think a closed form exists (I also think Fourier series might be a part of the solution to this problem but I couldn't reach am algorithm using it), but is there another solution, like an indefinite integral? If there isn't, is there an algorithm that can solve this problem?","['geometry', 'real-analysis', 'calculus', 'closed-form', 'algorithms']"
4916282,How to prove if $e<a<b$ then $a^b>b^a$,How to prove if $e<a<b$ then $a^b>b^a$ Thus far I got: $a^b>b^a$ $e^{\ln(a^b)}>e^{\ln(b^a)}$ $\frac{e^{b\cdot \ln(a)}}{e^{a\cdot \ln(b)}}>1$ $e^{b\cdot \ln(a)-a\cdot \ln(b)}>1$ $b\cdot \ln(a)>a\cdot \ln(b)$ We also know $b>\ln(b)>\ln(a)>1$ and $b>a>\ln(a)>1$,"['exponentiation', 'algebra-precalculus', 'inequality']"
4916301,Hausdorff's Division of the Circle into Congruent Pieces,"I was reading Andersen & Jessen's (1948) "" On the Introduction of Measures in Infinite Product Sets "", where they provide an example of an infinite product that can't be assigned a countably additive measure (i.e. a space for which the Kolmogorov Extension Theorem failsâ€“Â see this post ). Beginning in section 4 on page 5, they write: Let $C$ denote a circle of length $1$ ... Let $C$ be divided in Hausdorff's manner into disjoint sets $\cdots, C_{-1}, C_0, C_1, \cdots$ which are congruent by rotation. I can't seem to find any references to this ""Hausdorff division"" of the circle into countably many congruent pieces. Does anyone know what this is referring to?","['measure-theory', 'math-history', 'probability-theory', 'terminology']"
4916314,constraints on the sum and product of roots of quadratic equation assuming less than unity roots,"I am solving a math contest problem. Assume we have the quadratic equation $x^2+a_1x+a_2=0$ where $a_1,a_2\in \mathbb{R}$ are real numbers. The roots of this equation can be found as (from equation it can be inferred that $a_1 = -(p_1+p_2), a_2 = p_1p_2$ ) $$p_1,p_2 = -\frac{a_1}{2}\pm \sqrt{\frac{a_1^2}{4}-a_2}$$ Now assume that $|p_1|< 1, |p_2|< 1$ , in other words the module of the roots is lesser than unity (in the case of real roots, it translates to the fact that their absolute value is less than one). The claim is twofold $|a_2| < 1$ $|a_1| < a_2+1$ proving the first one very easy since $|a_2|=|p_1p_2| \le |p_1||p_2|< 1$ . However, the second one is almost impossible to prove! If we assume the roots are real, then by using $|p_i|< 1$ we can say $$-1< -\frac{a_1}{2}\pm \sqrt{\frac{a_1^2}{4}-a_2}< 1 \Rightarrow -1+\frac{a_1}{2} < \pm \sqrt{\frac{a_1^2}{4}-a_2}< 1 + \frac{a_1}{2}$$ then this gives two inequalities which can be manipulated to get similar results with a lot of mental gymnastics! But I don't know what to do in general! Is there any simpler way to reach the conclusion? What can be done in general case (complex conjugate roots and real distinct roots)? Thank you! =================================Edit=============================== For the complex conjugate pair, we can write $p_1 = p_r+ip_i, p_2 = p_1-ip_i$ and by substitution we get two true statements $$(1+p_r)^2+p_i^2 > 0 , (p_r-1)^2+p_i^2 > 0$$ and this prove the complex case. However, I am trying to find a way to drive the second statement using the given assumptions! Is there a way?","['contest-math', 'roots', 'calculus', 'quadratics', 'complex-numbers']"
4916316,Not sure if this is a valid way of proving local maximum,"I am unsure if my argument makes sense:
Let $f(x) = 0$ if $x$ is irrational and $f(x) = \frac{1}{q}$ if $x = \frac{p}{q}$ with $p,q$ in lowest terms. The exercise asks to find all local maximum and minimum:
I see that the irrationals are all local minimum and that in particular the natural numbers are all local maximum. Now, I convinced myself that any other rational is indeed also a local maximum. However the proof is what I am not so sure about... I said that given a rational number $x = \frac{p}{q} $ , if you look at numbers that are not further than $\delta := Min(|p'/q'-x|,gcd(p',q') = 1, q' < q) $ then by definition of $\delta$ there will be no number with greater value of $f$ .","['solution-verification', 'analysis', 'real-analysis']"
4916349,Justify if there exist a differentiable function f such that $|f(x)|<2$ and $f(x)f'(x)\geq \sin(x)$,"The problem states: Justify if there exist a differentiable function f such that $|f(x)|<2$ and $f(x)f'(x)\geq \sin(x)$ for every $x\in \mathbb{R}$ . What I got by another question I made about a related result: $f(x)f'(x)=(\frac 12f(x)^2)'$ so $(\frac 12f(x)^2)'=\sin(x)+c(x)\implies \frac 12f(x)^2=C(x)-\cos(x)$ now, $|f|<2$ so $\sqrt{f(x)^2}<2$ or equivalently $\frac{f(x)^2}{2}<2$ . Putting this result on the above equations gives $C(x)<2+\cos(x)$ . In other words, we got a function that has to be lower than 1 in some points like $x=\pi$ , but, at the same time $C(x)-\cos(x)\geq 0$ so $C(x)\geq \cos(x)$ . Finishing my reasoning, we are looking for an increasing function $C(x)$ such that $C(0)\geq1$ and $C(\pi)<1$ which is impossible. Is this right?",['derivatives']
4916353,Does a nonzero section induce Isomorphism of Locally Free Sheaves?,"Suppose I have two locally free sheaves $E, F$ of the same rank on a scheme $X$ . And suppose that I have a nowhere vanishing section $s \in H^0(E \otimes F)$ . That is, for all $x \in X$ , we have that $s_x \notin m_x(E \otimes F)_x$ . This section induces a map $s \colon E^\vee \to F$ . Since $s$ is nowhere vanishing is the induced map an isomorphism? Or do I have to assume the induced map is surjective?",['algebraic-geometry']
4916389,Product of lengths in a disk of area $\frac{\pi}{e}$,"A disk of area $\dfrac{\pi}{a}$ is divided into $n$ regions of equal area by line segments from a point on the edge. Here is an example with $n=8$ . Let $P(a,n)=\text{product of lengths of the line segments}$ . What is $\lim\limits_{n\to\infty}P(e,n)$ ? ( $e$ is Euler's constant) Why I chose $a=e$ Here are plots of $P(a,n)$ against $n$ with $a=0.99e$ , $\space a=e$ and $\space a=1.01e$ . It seems that if $a<e$ then $\lim\limits_{n\to\infty}P(a,n)=\infty$ , and if $a>e$ then $\lim\limits_{n\to\infty}P(a,n)=0$ . So $a=e$ seems to be a critical value. Here are the $P(e,n)$ values that I got; some of these appear in the graph above. $P(e,2)\approx1.2131$ $P(e,3)\approx1.3682$ $P(e,4)\approx1.4937$ $P(e,5)\approx1.6007$ $P(e,6)\approx1.6946$ $P(e,7)\approx1.7790$ $P(e,12)\approx2.1120$ $P(e,24)\approx2.6452$ $P(e,48)\approx3.3194$ $P(e,96)\approx4.1717$ $P(e,192)\approx 5.2476$ (I calculated these values ""manually"": that is, I used desmos to get an approximate solution to $xâˆ’\sin x=\frac{2k\pi}{n}$ with individual $k$ values, then I used those $x$ values and Excel to approximate $P(e,n)$ .) What makes this difficult What makes my question difficult for me, is that I cannot find exact expressions for the lengths. For example, with $a=e$ and $n=8$ , the length of the shortest line segments is $\frac{2}{\sqrt{e}}\sin \left(\frac{x}{2}\right)$ where $x-\sin x=\frac{\pi}{4}$ . I am aware of Kepler's equation , but that doesn't seem to help. Context This question was inspired by the following remarkable fact: If $n$ evenly spaced points are drawn on a unit circle, and line segments are drawn from one point to each of the other points, then the product of lengths of the line segments equals $n$ ( proof ). Related question: Conjectured connection between $e$ and $\pi$ in a semidisk Update @Carl Schildkraut's answer shows that $\lim\limits_{n\to\infty}\frac{P(e,n)}{n^{1/3}}=\frac{e^{1/2}}{6^{1/3}}\approx0.9073$ . Here is a plot of $\frac{P(e,n)}{n^{1/3}}$ against $n$ .","['circles', 'geometry', 'infinite-product', 'limits', 'trigonometry']"
4916399,"Analytic $f: \mathbb{D} \to \mathbb{D}$, $f(0)=0$, and $f$ has five zeros in $\overline{\frac{1}{2}\mathbb{D}}$","Suppose $f: \mathbb{D} \to \mathbb{D}$ is a holomorphic function and $f(0)=0$ . The function $f$ has a total of five zeros (counting multiplicities) in the closed half-disc $\overline{\frac{1}{2}\mathbb{D}} = \{z \in \mathbb{C}: |z| \leq \frac{1}{2}\}$ . How large can $|f'(0)|$ be? Schwarz Lemma $f: \mathbb{D} \to \mathbb{C}$ is analytic with $f(0) = 0$ and $|f(z)| \leq 1$ on $\mathbb{D}$ , then $|f(z)| \leq |z|$ for all $z \in \mathbb{D}$ and $|f'(0)| \leq 1$ It would be almost indisputable that Schwarz lemma will be of use here. Now, given that $f$ has five zeros in the domain $\overline{\frac{1}{2}\mathbb{D}}$ , it's no question that Rouche's theorem will likely also be of use here. Since $f$ maps $\mathbb{D}$ to itself, then implicitly we also have $|f(z)| \leq 1$ for $z$ on $\mathbb{D}$ . Then $f$ satisfies Schwarz's lemma, and so an upper bound for $|f'(0)|$ is $1$ . Perhaps this bound can be improved. But I have not used the fact that $f$ has five zeros in $\overline{\frac{1}{2}\mathbb{D}}$ . Again, I know this must be Rouche's theorem, but I'm not sure how to apply it.","['complex-analysis', 'rouches-theorem']"
4916408,"Different Formulations of Differentials as Generating Transformations: $e^{t A}f(x) = h(x,t); A=\frac{\partial_x}{g'(x)} \& h(x,t)=f(g^{-1}(g(x)+t))$","For context and introduction please see: Perspective on Differentials Generating Transformations of Functions - with Fourier Transformations $e^{a\partial_x} f(x) = f(x+a)$ Here I used Fourier transformations (instead of the classical series argument) to show that $\exp{(a\partial_x)}\,f(x) = f(x+a) \iff \exp{(i ak)} \,\mathcal{F}(f(x))(k) = \mathcal{F}(f(x+a))(k)$ . From this one can substitution to arrive at $e^{\frac{t}{g'(x)}\frac{d}{dx}}f(x) = f(G_t(x))$ with $G_t(x) = g^{-1}(g(x)+t)) \quad \quad \mathbf{(1)}$ The user LL 3.14 presented another perspective, that is seemingly more known to arrive at another general form of this transformation by exponentiation. It utilizes differential equations instead of power series:
Let's define the solution to $\partial_t h = A h$ with $h(0) = f$ at time $t$ to be given by $h(t) := e^{t A}f \quad \quad \mathbf{(2)}$ I now want to show that both approaches lead to the same results. I want to transform $f \mapsto f \circ G$ via the exponent given in $(1)$ , they give this solution by $h(x,t)$ .
Leading me to $h(x,t)= f(G_t(x)) \quad \quad \mathbf{(3)}$ . I will know try to show that this form of $h$ solves the characteristic equations: $\partial_t h = \partial_t f(G_t(x)) = f'(G_t(x)) \partial_t G_t(x) =  \frac{f'(G_t(x))}{g'(G_t(x))}$ $A h = \frac{1}{g'(x)}\frac{d}{dx} h(x,t) = \frac{1}{g'(x)} f'(G_t(x)) \frac{g'(x)}{g'(G_t(x))} = \frac{f'(G_t(x))}{g'(G_t(x))}$ $h(t=0) = f(G_0(x))$ with $G_0(x) = g^{-1}(g(x))) = x$ So they are the same. I used $f'(u)$ to refer to the total derivative in dependency of one parameter $u$ .
The differentiation for $t,x$ is analogous, as it depends only linearly in $t$ but from $g(x)$ the second calculation includes an additional $g'(x)$ to the $\frac{1}{g'(G_t)}$ factor. The given variant relys on $g$ being an invertible and differentiable function (as almost all differentiable functions are localy invertible, this is enough for my purpose). That $g \in C^1$ has to be assumed to make sense of $\frac{d}{dg}$ , which is the core idea leading me to this observations. The case that $g$ is not invertible in an enviroment arount $0$ , $g=\, const.$ , can easily be dealt with as a special case. Now I have convinced myself that both formulations are equivalent, but are you, dear reader, satisfied with this 'proof'?
What am I missing? I have a few question left: Is my form of $h(x,t) = f(G_t(x))$ the only possible choice (for $A$ depending on differential operators and invertible functions) It should be, as the solution to the differential equation should be unique, as well as the description described by $g$ and $A$ !? *What properties should the operator $A$ has? It doesn't need to be hermitian or bounded as can be seen by the example, the defining differential equation is linear... I hope the used function spaces are clear from their use, just assume what is necessary. If there is something special about these assumptions you can note them.
Thank you!","['fourier-transform', 'functions', 'solution-verification', 'functional-analysis', 'transformation']"
4916455,Probability distribution of sum of many differently weighted coins,"Imagine that we have $100$ coins, which each have a random weight between $0$ and $1$ $\left(~\mbox{but we know the weights}~\right)$ : That is to say, we have $100$ known numbers $p_{1}, p_{2},\ldots, p_{100} \in \left[0, 1\right]$ such that coin $n$ has probability $p_{n}$ of flipping heads, which we treat as
an outcome of $1$ , and probability $1 - p_{n}$ of flipping tails,
which we treat as an outcome of $0$ . We want the probability distribution of the sum of these $100$ coins. How can we compute it efficiently, exactly if possible or approximately if not possible $?$ .","['probability-distributions', 'probability']"
4916460,Expectation of a piecewise const approximation based on Beta distribution,"Let $X_1, X_2 \stackrel{\text{iid}}{\sim}\mathrm{Uniform}(0,1)$ and then sort $X_1,X_2$ to get $X_{(1)} < X_{(2)}$ . Based on the pdfs of $X_{(i)}$ , we know $X_{(1)} \sim \mathrm{Beta}(1,2)$ and $X_{(2)} \sim \mathrm{Beta}(2,1)$ , with $\mathbb{E}(X_{(1)}) = \frac{1}{3}$ and $\mathbb{E}(X_{(2)}) = \frac{2}{3}$ . Consider the following piecewise constant approximation for the function $f(x) = x$ . Sample two points $x_1, x_2$ uniformly on $(0, 1)$ and them sort them to get $x_{(1)}$ and $x_{(2)}$ , $x_{(1)} <  x_{(2)}$ . Denote the expectation as $\mathbb{E}_1(\|f - c\|_2^2) = \mathbb{E}_1(\displaystyle\int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt)$ , where $c = \frac{1}{x_{(2)}-x_{(1)}}\displaystyle\int_{x_{(1)}}^{x_{(2)}} f(t)\; dt$ is a constant. Sample $y_{(1)}$ from $\mathrm{Beta}(1,2)$ and $y_{(2)}$ from $\mathrm{Beta}(2,1)$ . Denote the Expectation as $\mathbb{E}_2(\|f - c\|_2^2) = \mathbb{E}_2(\displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt)$ , where $c = \frac{1}{y_{(2)}-y_{(1)}}\displaystyle\int_{y_{(1)}}^{y_{(2)}} f(t)\; dt$ is a constant. (Note: the definitions of $\mathbb{E}_1$ and $\mathbb{E}_2$ are identical; the only difference is the method of obtaining the points $x_{(i)}, y_{(i)}$ .) Compare $\mathbb{E}_1(\|f - c\|_2^2)$ and $\mathbb{E}_2(\|f - c\|_2^2)$ . It's observed from the numerical experiments that $\mathbb{E}_1(\|f - c\|_2^2) < \mathbb{E}_2(\|f - c\|_2^2)$ . This result confuses me a lot. First, I expect them to be equal because both $x_{(i)}$ and $y_{(i)}$ from the same Beta distribution as discussed above. Second, even if $\; \mathbb{E}_1 \neq \mathbb{E}_2$ , since $y_{(1)}$ could be greater than $y_{(2)}$ , $\displaystyle\int_{y_{(1)}}^{y_{(2)}} (f(t)-c)^2 \; dt$ could be negative for some $y_{(i)}$ . However, we know $\int_{x_{(1)}}^{x_{(2)}} (f(t)-c)^2 \; dt > 0$ .
Therefore, shouldn't it be $\mathbb{E}_1(\|f - c\|_2^2) > \mathbb{E}_2(\|f - c\|_2^2)$ ? I am confused by this result. Did I miss something? My experiments were done using Matlab with the built-in functions $\textit{rand}$ and $\textit{betarnd}$ .","['statistics', 'uniform-distribution', 'approximation', 'probability-distributions', 'order-statistics']"
4916472,"Trying to prove that if $\frac{f(x+1)}{f(x)} = b,$ where $b$ is constant, then $f(x)$ is exponential.","I know how to prove the converse, but I'm curious about how to prove it in the other direction. So far, what I have is this: Assume that $\frac{f(x+1)}{f(x)} = b.$ Then, \begin{align}
f(x+1)&=bf(x) \\
f(x+2)&=b^{2}f(x) \\
\vdots \\
f(x+k)&=b^{k}f(x).
\end{align} Any tips are appreciated. Thank you.","['algebra-precalculus', 'proof-writing', 'discrete-mathematics']"
4916476,Computing torsion subgroup of elliptic curve,"Compute the torsion subgroup of the elliptic curve $y^2=x^3+5x^2+3x+7$ . I am only used to computing torsion groups when our equation is in 'short Weirstrass form'; i.e. $y^2=x^3+Ax+B$ for integer $A,B$ . In that case, we can reduce over primes $p$ of good reduction, and use the fact that this $\mathcal{E}(\mathbb{Q})$ injects into $\mathcal{E}(\mathbb{F}_p)$ . Here is the solution given by my instructor: Reduce mod $3$ to get $y^2=x^3+2x^2+7$ . The RHS has no solutions over $\mathbb{F}_3$ so must be squarefree, and this reduction is therefore smooth. One can check to see there are $5$ points over $\mathbb{F}_3$ , so the torsion group has order dividing $5$ . Therefore we only need to check that there is a point in the torsion group with order exactly $5$ instead of infinite order. $(1,4)$ is obviously a point on the curve, and by point duplication we can how it has order $5$ . This solution confuses me for the following reasons: Why didn't we just write $y^2=x^3+2x^2+1$ modulo $3$ ? This gives a different answer? What exactly is meant by 'smooth' reduction? Is this just a synonym for good reduction? If so how have they deduced this from the fact there are no solutions on the RHS? In general, is the strategy behind computing the torsion subgroup (save for using Nagell-Lutz) to find the primes of good reduction, compute the sizes of the groups manually for small enough primes $p$ , and then try to spot points on the curve; then, showing they have certain order? i.e. if we know the torsion group has order dividing, say, $4$ , then we ought to find a point of order $4$ to show the torsion group has order exactly $4$ ?","['algebraic-curves', 'elliptic-curves', 'algebraic-geometry', 'solution-verification', 'modular-forms']"
4916546,What is the correct particular solution(and solved coefficients) for this non-homogenous recurrence relation?,"I need some help with the following non-homogenous recurrence relation, $$a_{n+3} - 3a_{n+2} + 3a_{n+1} - a_n = 3 + 5n$$ The characteristic equation is third order with repeated root $1$ . I know that our homogeneous solution is $a_n^{(h)} = A + Bn + Cn^2$ .
We were told that this should be our particular solution $a_n^{(p)} = Dn^3+En^4$ .
Could you explain me how did we get it?
Thank you.","['recurrence-relations', 'discrete-mathematics']"
4916574,Is it possible to evaluate $ \int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt $,"This question was in my Calc 2 final and I was stumped with these limits. First, I'm confused since $f(t)$ isn't defined for $-5 \leq t \leq 5$ . My teacher suggested I treat it as an improper integral, but I'm not sure that works out. What I tried was to find the antiderivative first by trig substitution: Consider $\int\frac{1}{t^4 \sqrt{t^2-25}} dt$ If $\sqrt{t^2-25}=5\tan \theta$ , $t=5\sec\theta$ and $dt=\sec \theta \tan \theta d \theta$ We have $\int\frac{5\sec \theta \tan \theta}{(5\sec \theta)^4 5 \sec \theta} d\theta = \frac{1}{5^4}\ \int\frac{1}{\sec^3 \theta} d \theta$ With $u$ -sub and trig identities, we get $=\frac{1}{5^4}\ [\sin \theta - \frac{1}{3}\sin^3 \theta] $ In terms of $t$ : $= \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ]$ Back to our integral: $\int_{0}^{1} \frac{1}{t^4 \sqrt{t^2-25}} dt = \frac{1}{5^4}\ [ \frac{\sqrt{t^2-25}}{t}\ - \frac{1}{3}\ (\frac{\sqrt{t^2-25}}{t}\ )^3 ]^1_0 $ And of course I run into the same issue of having a division by zero. If I take the limit, then I find it diverges. Looking forward to comments! Please let me know if I'm making mistakes or conceptual errors. Thanks!","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'limits']"
4916586,Sum of iid random variables,"Iid random variables $Y_1, Y_2, \dots, Y_{50}$ take only the values $0$ , $1$ and $2$ with probabilities $\mathbb{P}(Y_i = 0) =
\mathbb{P}(Y_i = 1) = \frac{4}{9}, \mathbb{P}(Y_i = 2) = \frac{1}{9}$ .
Find the probability that the sum of these random variables will be $45$ . Now here's my idea: let $i$ be the number of $2$ 's ( $i$ is an integer between $0$ and $22$ ), then we must have $45 - 2i$ $1$ 's. There is $C^i_{50}$ ways to choose $Y_i$ that are gonna be equal to $2$ and $C^{45 - 2i}_{50 - i}$ ways to choose $Y_i$ that are gonna be equal to $1$ . The rest are going to be $0$ . Since we will have $i$ $2$ 's, $45 - 2i$ $1$ 's there is going to be $5 + i$ $0$ 's (since we have $50$ variables in total).
Hence we have: $$\sum \limits_{i = 0}^{22} C^i_{50} C^{45 - 2i}_{50 - i} \mathbb{P}(Y_i = 2)^{i} \mathbb{P}(Y_i = 1)^{45 - 2i} \mathbb{P}(Y_i = 0)^{5 + i} = $$ $$\sum \limits_{i = 0}^{22} C^i_{50} C^{45 - 2i}_{50 - i} (\frac{1}{9})^{i} (\frac{4}{9})^{45 - 2i} (\frac{4}{9})^{5 + i} = $$ $$(\frac{4}{9})^{45} (\frac{4}{9})^{5} \sum \limits_{i = 0}^{22} C^i_{50} C^{45 - 2i}_{50 - i} (\frac{1}{9})^{i} (\frac{4}{9})^{- 2i} (\frac{4}{9})^{i} = $$ $$(\frac{4}{9})^{50} \sum \limits_{i = 0}^{22} C^i_{50} C^{45 - 2i}_{50 - i}$$ Now what surprises me is that there are no probabilities left inside the sum. Am I doing everything correctly? And if so, how can I make this sum prettier (i.e. get a number instead of a sum)? Thanks!","['binomial-coefficients', 'probability', 'random-variables']"
4916596,Question regarding multiplication table of group of odd order.,"Question Let $G$ be a group of odd order. Then show that the diagonal elements of its multiplication table contain each element only once. Attempt What I get ultimately is that $a^{2}c^{2} = e$ for some $a,c \in G$ . And if my group were abelian, it would yield that $c$ is the inverse of $a$ , but I don't see any contradiction here. Ideally, I feel I need to show if the diagonal could contain same elements twice, then somehow a subgroup of order two would exist, which would be a contradiction. Kindly help me go ahead.","['group-theory', 'abstract-algebra', 'finite-groups']"
4916609,Equivalence between definitions of orthogonal groups of minus type: $GO_{2n}^{-}(q)$,"I am working with the general orthogonal group of minus type over a finite field $F_q$ with $q=p^f$ elements. In general, finite classical groups may be studied from different perspectives: as the fixed point subgroups under a Steinberg endomorphism of a semisimple algebraic group, or as groups fixing a certain classical form. In this particular case I am considering two definitions.
The first is taken from the book of Malle and Testermann "" Linear algebraic groups and finite groups of Lie type "".
They define the general orthogonal group $GO_{2n}$ over an algebraically closed field $k = \overline{F}_p$ as the group of isometries $$GO_{2n}=\{A \in GL_{2n} \mid f(Ax)=f(x) \text{ for all }x \in k^{2n} \},$$ where $f:k^{2n} \rightarrow k$ is the non-degenerate quadratic form: $f(x_1,...,x_{2n})=x_1x_{2n}+x_2x_{2n-1}+...+x_{n}x_{n+1}.$ Then, they consider the following element $$
  g :=\left( \begin{array}{cccc}
    I_{n-1}   & & & \\
       & 0 &1 &  \\
       & 1 &0 & \\
       &&&  I_{n-1} 
  \end{array} \right) \in  GO_{2n},
     $$ Moreover, let $F: GO_{2n} \rightarrow GO_{2n}$ , $(a_{ij}) \mapsto (a_{ij}^q)^{-tr} $ , i.e. the composition of the map sending a matrix to the inverse of its transpose with the standard $q$ -Frobenius endomorphism. The general orthogonal group of minus type over the field $F_q$ is defined as the fixed point subgroup of $gF$ : $$GO_{2n}^{-}(q)=(GO_{2n})_{gF}.$$ I'd like to prove that this coincides with the isometry group of a non-denegerate orthogonal form with Witt defect -1, defined on $V=F_q^{2n}$ .
In particular in this context I am working with the basis $$\mathcal{B}^{-}=(e_1,f_1,...,e_m,f_m)$$ and the quadratic form $Q$ such that: $$Q(e_i)=Q(f_i)=(e_i,u_m)=(f_i,u_m)=(e_i,v_m)=(f_i,v_m)=0$$ $$(e_i,f_j)=\delta_{ij}, Q(u_m)=Q(v_m)=1, (u_m,v_m)=\zeta^2+\zeta^{-2}$$ where $\zeta \in F_q^2 \setminus F_q$ satisfies $\zeta^{q+1}=1$ . I am pretty stuck in proving that the first  definition is equivalent to the second (I am not interested in this particular $Q$ , I am also looking for the most suitable $Q$ for proving the equivalence, first of all reording the basis $\mathcal{B}^{-}$ should work better). Any hint is greatly appreciated.","['finite-groups', 'abstract-algebra', 'linear-algebra', 'group-theory', 'lie-groups']"
4916641,$\left |\int _{X} log(|f|)d\mu \right | < \infty$ if $\int_{X} |f| d\mu <\infty $?,"Is the following true?:
Assume that $\mu$ is any positive measure with $\mu (X)=1$ and that, if $E_{0}=\left \{ x\in X :|f(x)|=0 \right \}$ , we have $\mu (E_{0})=0$ . Then $$\left |\int _{X} log(|f|)d\mu \right | < \infty \Leftarrow \int_{X} |f| d\mu < \infty.$$ I was trying to use the following sets $E_{1}=\left \{ x\in X: |f(x)|\geq 1 \right \}$ and $F=\left \{ x\in X: 0<|f(x)|< 1 \right \}.$ Then we have $$
\left |\int _{X} log(|f|)d\mu \right | = \left |\int_{E_{0}}log(|f|)d\mu + \int_{E_{1}}log(|f|)d\mu + \int_{F}log|f|d\mu \right | \\ \le \int_{E_{1}}|f|d\mu +\left 
|\int_{F}log|f|d\mu \right |.
$$ If I would be able to determine that $\left |\int_{F}log|f|d\mu \right | <\infty$ I would be done. I have the suspicion that it is true since $\left |\int_{[0,1]}log(x)dx \right | < \infty$ . But how can I do this with $f$ ? Thanks in advance for any help.","['measure-theory', 'lebesgue-integral']"
4916671,Flux calculation - what did I do wrong?,"The exercise asks to calculate the flux of $\mathbf{F}=(4x,4y,z^2)$ through the surface $x^2+y^2=25, 0\le z \le 2$ I calculated using Gauss' theorem and  I obtained $500\pi$ , which is, as the teacher says, the correct answer. The exercise also asks to calculate it as $$\iint_S \mathbf{F}Â·d\mathbf{S}=\iint_S\mathbf{F}(\mathbf{S})Â·\mathbf{n}\ dS$$ Using cylindrical coordinates: $\mathbf{S}=(5\cos\theta, 5\sin\theta, z), \theta \in [0,2\pi], z \in [0, 2]$ And $\mathbf{F}(\mathbf{S})=(20\cos\theta, 20\sin\theta, z^2)$ And the unitary normal vector of the surface is $\mathbf{n}= \frac{\mathbf{S}_\theta \times \mathbf{S}_z}{\lVert \mathbf{S}_\theta \times \mathbf{S}_z \rVert}=(5\cos\theta,5\sin\theta,0)$ And $dS=5d\theta dz$ So $$\iint_S \mathbf{F}Â·d\mathbf{S}=\int_0^{2\pi} \int_0^2(20\cos^2(\theta)+20\sin^2(\theta))5dzd\theta=400\pi\neq500\pi$$ What did I do wrong?","['integration', 'multivariable-calculus', 'divergence-theorem', 'definite-integrals']"
4916715,how to evaluate $\int_0^1{\ln ^3\left( 1-x \right) \ln ^2\left( 1+x \right) \text{d}x}$,"Integral: how to evaluate $$\int_0^1{\ln ^3\left( 1-x \right) \ln ^2\left( 1+x \right) \text{d}x}$$ Same context I'm not sure of the closed form of the integral, as I haven't evaluated it yet. However, after evaluating the integral $$\int_0^1 \ln (1+x) \ln (1-x) \, dx,$$ I thought about integrating $$\int_0^1 \ln^3(1-x) \ln^2(1+x) \, dx.$$ I am also interested in evaluation of $$\int_0^1 \ln^n(1-x) \ln^m(1+x) \, dx.$$ If possible My work replace $x\rightarrow\frac{1-x}{1+x}$ , there is $$\int_0^1{\ln ^3\left( 1-x \right) \ln ^2\left( 1+x \right) \text{d}x}=2\underset{I}{\underbrace{\int_0^1{\frac{\ln ^2\left( \dfrac{2}{1+x} \right) \ln ^3\left( \dfrac{2x}{1+x} \right)}{\left( 1+x \right) ^2}\text{d}x}}}$$ Expand the integral \begin{align*} &\ln ^2\left( \frac{2}{1+x} \right) \ln ^3\left( \frac{2x}{1+x} \right) =-\ln ^2\text{2}\ln ^3\left( 1+x \right) +\text{2}\ln\text{2}\ln ^4\left( 1+x \right) -\ln ^5\left( 1+x \right) \\ &+\text{3}\ln ^2\text{2}\ln ^2\left( 1+x \right) \ln \left( 2x \right) -\text{6}\ln\text{2}\ln ^3\left( 1+x \right) \ln \left( 2x \right) +\text{3}\ln ^4\left( 1+x \right) \ln \left( 2x \right) \\ &-\text{3}\ln ^2\text{2}\ln \left( 1+x \right) \ln ^2\left( 2x \right) +\text{6}\ln\text{2}\ln ^2\left( 1+x \right) \ln ^2\left( 2x \right) -\text{3}\ln ^3\left( 1+x \right) \ln ^2\left( 2x \right) \\ &+\ln ^2\text{2}\ln ^3\left( 2x \right) -\text{2}\ln\text{2}\ln \left( 1+x \right) \ln ^3\left( 2x \right) +\ln ^2\left( 1+x \right) \ln ^3\left( 2x \right) \end{align*} Evaluation of $I_1$ $$I_1=-\ln ^22\int_0^1{\frac{\ln ^3\left( 1+x \right)}{\left( 1+x \right) ^2}\text{d}x}=\frac{\ln ^52}{2}+\frac{\text{3}\ln ^42}{2}+\text{3}\ln ^32-\text{3}\ln ^22$$ Evaluation of $I_2$ $$I_2=\text{2}\ln 2\int_0^1{\frac{\ln ^4\left( 1+x \right)}{\left( 1+x \right) ^2}\text{d}x}=-\ln ^52-\text{4}\ln ^42-\text{12}\ln ^32-\text{24}\ln ^22+\text{24}\ln 2$$ Evaluation of $I_3$ $$I_3=-\int_0^1{\frac{\ln ^5\left( 1+x \right)}{\left( 1+x \right) ^2}\text{d}x}=-60+\frac{\ln ^52}{2}+\frac{\text{5}\ln ^42}{2}+\text{10}\ln ^32+\text{30}\ln ^22+\text{60}\ln 2$$ Evaluation of $I_4$ \begin{align*} I_4&=\text{3}\ln ^22\int_0^1{\frac{\ln ^2\left( 1+x \right) \ln \left( 2x \right)}{\left( 1+x \right) ^2}}\text{d}x\\&=\text{3}\ln ^22\int_0^1{\frac{\ln ^2\left( 1+x \right) \ln x}{\left( 1+x \right) ^2}}\text{d}x+\text{3}\ln ^32\int_0^1{\frac{\ln ^2\left( 1+x \right)}{\left( 1+x \right) ^2}}\text{d}x \end{align*} $$I_{41}=\int_0^1{\frac{\ln ^2\left( 1+x \right) \ln x}{\left( 1+x \right) ^2}}\text{d}x=\frac{1}{4}\zeta \left( 3 \right) +\frac{\pi ^2}{6}-\frac{1}{3}\ln ^32-\ln ^22-\text{2}\ln 2$$ $$I_{42}=\int_0^1{\frac{\ln ^2\left( 1+x \right)}{\left( 1+x \right) ^2}\text{d}x}=1-\frac{1}{2}\ln ^22-\ln 2$$ Expanding the integral does not seem like a great way to evaluate it. I did not provide explanations for some smaller integrals due to the length of the post. Closed form by Mathematica \begin{align*}
&-120 + 6 \pi^2 + \frac{\pi^4}{30} + 96 \log(2) - 6 \pi^2 \log(2) - \frac{1}{30} \pi^4 \log(2) \\
&\quad - 48 \log^2(2) + 2 \pi^2 \log^2(2) + 16 \log^3(2) - \frac{2}{3} \pi^2 \log^3(2) \\
&\quad - 3 \log^4(2) + \frac{3}{5} \log^5(2) + 24 \text{Li}_4 \left( \frac{1}{2} \right) + 24 \text{Li}_5 \left( \frac{1}{2} \right) \\
&\quad + 45 \zeta(3) - 2 \pi^2 \zeta(3) - 24 \log(2) \zeta(3) + 12 \log^2(2) \zeta(3) + \frac{3}{5} \zeta(5)
\end{align*}","['integration', 'calculus', 'definite-integrals', 'closed-form']"
4916734,Radon Nikodym derivative and distribution function,"Let $\mathfrak{B}$ be the Borel $\sigma$ -algebra over $\mathbb{R}$ and $\beta$ the  Borel-Lebesgue measure over $\mathfrak{B}$ . Let $\mu$ be a Borel measure over $\mathbb{R}$ s.t. the distribution function $$\varphi:\mathbb{R}\rightarrow\mathbb{R}, \hspace{10px} \varphi(x) =\begin{cases}
    -\mu((x,0]), &x<0,\\
    0, &x=0,\\
    \mu((0,x]), &x>0
\end{cases}$$ is differentiable and $\varphi'$ is continuous. I want to prove that $\varphi'(x) = \frac{d \mu}{d \beta}(x)$ is the Radon-Nikodym derivative. Here is my attempt: If $\varphi'$ is continuous then it is integrable so $$
F(x) = \int^x_a \varphi(y) dy
$$ is differentiable almost everywhere and $F'(x) = \varphi$ . Is this a step in the right direction?","['integration', 'complex-analysis', 'measure-theory', 'radon-nikodym']"
4916748,Show that $\int_0^{1/2}\log(n(\arcsin x-\arcsin(x-1/n)))\mathrm dx$ converges to $\frac12-\frac14\log(27/4)$ as $n\to\infty$.,"Show that $\lim\limits_{n\to\infty}\int_0^{1/2}\log\left(n\left(\arcsin x-\arcsin\left(x-\frac{1}{n}\right)\right)\right)\mathrm dx=\frac12-\frac14\log\frac{27}{4}$ . Here is the graph of $y=\log\left(n\left(\arcsin x-\arcsin\left(x-\frac{1}{n}\right)\right)\right)$ for $0<x<\frac12$ with $n=10^5$ . This claim has numerical evidence , but Wolfram does not evaluate the limit, indefinite integral, nor definite integral. I'm at a loss of what to do. Context (optional reading) In a semicircle with diameter $na$ , draw chords of lengths $a,2a,3a,\dots,na$ that meet at one of the endpoints of the semicircle. Here is an example with $n=6$ . Consider the $n$ arcs between neighboring chord endpoints. Numerical investigation led me to the following claim: If $a=\frac{3\sqrt3}{2e}$ then as $n\to\infty$ the product of the arc lengths converges to unity. This claim is equivalent to the claim about the integral in this question. Edit I made a calculation mistake (in one step I used the diameter instead of the radius). So it is not true that ""If $a=\frac{3\sqrt3}{2e}$ then as $n\to\infty$ the product of the arc lengths converges to unity"" (and that claim is not equivalent to the claim in this question). After correcting my mistake, I have: If $a=\frac{2}{e}$ then the product of arc lengths is $\prod\limits_{k=1}^n \frac{2n}{e}\left(\arcsin\left(\frac{k}{n}\right)-\arcsin\left(\frac{k-1}{n}\right)\right)$ , which seems to converge to $1.225\dots$ (I ask about the limit here ). Anyway, I still learned something from the answers to my question.","['integration', 'definite-integrals', 'calculus', 'infinite-product', 'limits']"
4916773,"Expected value of stopping time of non symmetric random walk$E[\tau_{a,b}]$ is finite","Suppose we have a random walk $S_n$ that increases by $1$ with probability $p \ne \frac{1}{2}$ and decreases by $1$ with probability $1-p$ . And let $a,b \in \mathbb{N}$ .
How can I show that the expected value of $\tau_{a,b} := \inf \{n \ge 0: S_n \in \{-a,b\}\}$ ist finite.
In lecture it was stated that this is easy to see, however I do not know where to start proving this. Any help would be appreciated.","['stopping-times', 'probability-theory', 'probability']"
4916776,If $f(1)=f(2)=0$ then exist $c$ such that $cf''(c)+2f'(c)=0$,"Problem. Let the function $f$ be continuous on $[0,2]$ , second-order differentiable on the interval $(0,2)$ and satisfy the condition $f(1)=f(2)=0$ . Prove that there exists a real number $c$ in $(0,2)$ such that $$ cf''(c)+2f'(c)=0 $$ My idea is use MVT to prove that $f'(c)=\frac{-f(0)}{2}$ and use idea from the problem If $f(0)=f(1)=f(2)=0$ , $\forall x, \exists c, f(x)=\frac{1}{6}x(x-1)(x-2)f'''(c)$ , but I stuck at proving it. Can someone help me please, thank you.",['real-analysis']
4916813,Expected Number of Flips and Probability in a Coin Toss Experiment,"I'm currently studying probability and I've come across a problem that I'm finding quite challenging. I would appreciate any help or guidance. Problem Statement: A fair coin is continually flipped until both heads and tails have appeared. I need to find: (a) The expected number of flips
(b) The probability that the last flip lands on heads My Attempt: For part (a), I understand that the expected value is the long-run average or mean of a random variable. Since the coin is fair, the probability of getting heads or tails is 0.5. However, I'm not sure how to apply this concept when the coin is flipped until both heads and tails have appeared. For part (b), I'm a bit confused. My initial thought was that since the coin is fair, the probability that the last flip lands on heads would be 0.5. But I'm not sure if this is correct because the experiment doesn't stop until both heads and tails have appeared. Background: I'm an undergraduate student majoring in Mathematics. I've taken courses in Calculus and Linear Algebra, and I'm currently taking a course in Probability and Statistics. I'm familiar with the basics of probability, but this problem seems to involve concepts that I haven't fully grasped yet. I found this problem in my textbook (unfortunately, I don't have the name of the book right now), in the chapter on expected values. I've tried to solve it using the concepts explained in the book, but I'm stuck. I would really appreciate it if someone could explain the solution in a way that a beginner in probability could understand. Thank you in advance for your help!","['expected-value', 'probability-distributions', 'probability-theory', 'probability']"
4916830,Two runners start on a track and run at different speeds. When will they next meet?,"Two runners start running laps at the same time, from the starting position. George runs a lap in $50$ seconds; Sue runs a lap in $30$ seconds. When will the runners next be side by side? George's speed: $\dfrac{1}{50}$ lap per second Sue's speed: $\dfrac{1}{30}$ lap per second We are looking for the time $t$ when they next meet. How many laps George has run after time $t$ : $\dfrac{t}{50}$ laps How many laps Sue has run after time $t$ : $\dfrac{t}{30}$ laps This is where I get confused. The solution says: They will next be even when Sue has run exactly one more lap, that is, when $\dfrac{t}{30}-\dfrac{t}{50}=1$ . What is the justification to say they will be next to each other when Sue has run exactly one more lap? To me this just came out of the blue.",['algebra-precalculus']
4916833,The number of real roots of the equation $\displaystyle 1+\frac{x}{1}+\frac{x^2}{2}+\cdots \cdots +\frac{x^{2012}}{2012}=0$,"The number of real roots of the equation $\displaystyle 1+\frac{x}{1}+\frac{x^2}{2}+\cdots \cdots +\frac{x^{2012}}{2012}=0$ What I try : Let $ $\displaystyle f(x)=1+\frac{x}{1}+\frac{x^2}{2}+\cdots+\frac{x^{2012}}{2012}.$ Then $\displaystyle f'(x)=1+x+x^2+x^3+\cdots +x^{2011}$ And for maxima and minima put $f'(x)=0$ Then from above equation $x=1$ is not a root of that equation So we have $\displaystyle \frac{x^{2012}-1}{x-1}=0\Longrightarrow x^{2012}=1$ We get $\displaystyle x=\bigg(1\bigg)^{\frac{1}{2012}}$ Also $\displaystyle f''(x)=1+2x+3x^2+\cdots +2011x^{2010}$ And at $\displaystyle f''\bigg((1)^{1/2012}\bigg)>0$ So minimum of $f(x)$ occur at $x=(1)^{1/2012}$ I.e no real solution occur. Is my process is right , Please have a look And if it is wrong , please explain me right solution. Thanks",['functions']
4916869,Simplification of square roots as denominators with limits,"This might be a basic question but it has been the major issue to solving problems I seem to have when it comes to calculus. Consider this limit $$\lim_{\alpha \to \infty}\left[\tanh \alpha\right]= \lim_{y\to \infty}\left[\frac{y}{\sqrt{1+y^2}}\right]=\lim_{y \to \infty}\left[\frac{y}{\sqrt{1+y^2}} \times \frac{(\frac{1}{y})}{(\frac{1}{y})}\right] =\lim_{y \to \infty} \left[\frac{1}{ \sqrt{\frac{1}{y^2} + 1} }\right]=1$$ What is in particular confusing with me is that I always get confused how one manages to ""put"" $ \frac{1}{y} $ inside the square root to simplify $y^2$ . I think that I know the algebra rules, but this is something that I'm quite sure I haven't encountered so far. I haven't found any axiom or theorem proving that this is ""allowed"", although it obviously works. To make it more legible I'm in particular asking how did we get this to simplify: $$\lim_{y \to \infty}\left[\frac{y}{\sqrt{1+y^2}} \times \frac{(\frac{1}{y})}{(\frac{1}{y})}\right] =\lim_{y \to \infty} \left[\frac{1}{ \sqrt{\frac{1}{y^2} + 1} }\right]=1$$","['limits', 'calculus', 'limits-without-lhopital', 'algebra-precalculus']"
4916979,"Logarithmic inequality involving $a_1, a_2, ..., a_n$","Given the real numbers $a_1, a_2,...,a_n$ all greater than $1$ , such that $\prod_{i=1}^{n} a_i=10^n$ , prove that: $$\frac{\log_{10}a_1}{(1+\log_{10}a_1)^2}+\frac{\log_{10}a_2}{(1+\log_{10}a_1 + \log_{10}a_2)^2}+...+\frac{\log_{10}a_n}{(1+\sum_{i=1}^{n} \log_{10}a_i)^2}\le\frac{n}{n+1}$$ My first try was with proof by induction. It did not work. My second attempt goes as follows:
Let`s define the numbers $x_1,x_2,...,x_n$ , such that $x_1=\log_{10}a_1, x_2=\log_{10}a_2,...,x_n=\log_{10}a_n$ . This implies that every number $x_i>0$ , where $i=1,2,\cdots,n$ ; and that $\sum_{i=1}^{n} x_i=n$ , so the inequality becomes: $$\frac{x_1}{(1+x_1)^2}+\frac{x_2}{(1+x_1 +x_2)^2}+...+\frac{x_n}{(1+\sum_{i=1}^{n} x_i)^2}\le\frac{n}{n+1}.$$ $\phantom{2}$ Then I tried to identify another expression for each of the fractions, so that we can work with them. My best guess was that for every $x_1,x_2,...,x_n$ , $$\frac{x_n}{(1+\sum_{i=1}^{n} x_n)^2}< \frac{1}{1+\sum_{i=1}^{n} x_n}$$ so that the inequality can be rewritten as: $$\frac{1}{1+x_1}+\frac{1}{1+x_1+x_2}+...+\frac{1}{1+\sum_{i=1}^{n} x_n}\le\frac{n}{n+1}$$ But now I am stuck. My biggest problem is that the denominators contain multiple values of $x_n$ . Is there a way or strategy to handle such problems more efficiently?","['algebra-precalculus', 'logarithms', 'inequality']"
4916988,Combinatorial proof that $\sum_{k=1}^{n} k {2n \choose n+k}=\frac{1}{2}n{2n \choose n}$ [duplicate],"This question already has answers here : A conjecture including binomial coefficients (2 answers) Closed last month . I'd like to find a combinatorial/algebraic proof of the identity: $$\sum_{k=1}^{n}k{2n \choose n+k}=\frac{1}{2}n{2n \choose n}$$ The only proof of this that I've been able to find on the Internet, uses the Egorychev method , which is a bit excessive in my opinion. I'll post the answer I came up with. Hint: it involves non-returning walks on the number-line. In addition, I wonder if there are even simpler ways of showing this?","['combinatorial-proofs', 'random-walk', 'combinatorics-on-words', 'binomial-coefficients', 'combinatorics']"
4916993,Can there be more than one global maximums of a function?,"The definition(wiki): A real-valued function $f$ defined on a domain $X$ has a global (or absolute) maximum point at $xâˆ—$ , if $f(xâˆ—) \geq f(x)$ for all $x$ in $X$ . My question is can there be more than one global maximum(or minimum) of a function? The definition does not suggest that there is only 1 global maximum(or minimum). Example: let $f(x) = x^2$ on the domain $X = [-2,2]$ , thus the global maximum is $f(-2) = 4 = f(2)$ as by definition $f(-2)  = f(2) \geq f(x) \ \forall x\in X$ , thus we have 2 global maiximums. I tried to confirm this but there are so many sorces saying that there can only be one: Example1 , Example2 . Am I misunderstanding something? I would appreciate any insight you can offer.","['maxima-minima', 'calculus']"
4917001,"Equivalence of definitions of ""standard Borel space""","I met the following definition of standard Borel spaces in Durrett's probability theory book (slightly rephrased): $(S,\mathcal{S})$ is said to be standard Borel if it is isomorphic (as a measurable space) to a Borel subset of $\mathbb{R}$ equipped with its Borel sigma algebra. However, this is different from the other definitions I've seen in other sources: e.g., ""being isomorphic to a separable complete metric space with the Borel Ïƒ-algebra"". Durrett's definition relies on a special space, namely $\mathbb{R}$ . Are these two definitions equivalent? I tried to check a descriptive set theory book, but I cannot find out a definite answer. Thanks in advance!","['descriptive-set-theory', 'measure-theory', 'probability-theory']"
4917007,Confusion of definition of covariant derivative along curve,"I am currently studying Riemannian geometry, and have come across the following proposition: Proposition. Let $(M,g)$ be a Riemannian manifold with Levi-Civita connection $\nabla$ and let $\gamma:I\to M$ be a $C^1$ -curve in $M$ . Then there exists a unique operator $$\frac{\mathrm{D}}{\mathrm{d}t}:C^\infty_\gamma(\mathrm{T}M)\to C^\infty_\gamma(\mathrm{T}M),$$ such that for all $\lambda,\mu\in\mathbb{R}$ , $f\in C^\infty(I)$ , $X,Y\in C^\infty_\gamma(\mathrm{T}M)$ , we have $$\frac{\mathrm{D}(\lambda X+\mu Y)}{\mathrm{d}t}=\lambda\frac{\mathrm{D}X}{\mathrm{d}t}+\mu\frac{\mathrm{D}Y}{\mathrm{d}t}$$ $$\frac{\mathrm{D}(fX)}{\mathrm{d}t}=f'X+f\frac{\mathrm{D}X}{\mathrm{d}t}$$ for each $t_0\in I$ , there exists an open subinterval $J$ of $I$ such that $t_0\in J$ and if $\bar{X}\in C^\infty(\mathrm{T}M)$ is a vector field with $\bar{X}_{\gamma(t)}=X(t)$ for all $t\in J$ , we have $$\frac{\mathrm{D}X}{\mathrm{d}t}(t_0)=(\nabla_{\dot{\gamma}}\bar{X})_{\gamma(t_0)}.$$ In the above, $C^\infty_\gamma(\mathrm{T}M)$ refers to all smooth vector fields along $\gamma$ , i.e. smooth $X:I\to\mathrm{T}M$ with $\pi\circ X=\gamma$ , and $C^\infty(\mathrm{T}M)$ refers to all smooth vector fields on $M$ . My problem with this proposition is that I don't quite think 3 makes sense, and I'm trying to figure out what it should be or how it should be interpreted. To see my problem, note first that the Levi-Civita connection on $(M,g)$ is a map \begin{align*}
\nabla:C^\infty(\mathrm{T}M)\times C^\infty(\mathrm{T}M)&\to C^\infty(\mathrm{T}M), \\
(X,Y)&\mapsto \nabla_XY.
\end{align*} Consequently the expression $\nabla_{\dot\gamma}\bar{X}$ does not make sense, as $\dot\gamma$ is not in $C^\infty(\mathrm{T}M)$ . Even identifying $\dot\gamma$ with $t\mapsto(\gamma(t),\dot\gamma(t))$ we still only have a vector field along $\gamma$ , i.e. an element of $C^\infty_\gamma(\mathrm{T}M)$ . So the only way to try to make sense of this is to try to interpret $\dot\gamma$ as a vector field on $M$ . If $\gamma$ is injective, this is not a problem. Simply note that $\gamma(I)$ defines a submanifold of $M$ locally, and so we can extend the map $\gamma(I)\to\mathrm{T}M$ , $p\mapsto(p,\dot{\gamma}(\gamma^{-1}(p))$ to a vector field on $M$ locally around each point, and this vector field would then take the place of $\dot\gamma$ in $\nabla_{\dot\gamma}\bar{X}$ , and everything would make sense. The problem is, however, that if we do not assume $\gamma$ to be injective, we may have self-intersections, and for a given point $p\in\gamma(I)$ with $t_1,t_2\in I$ such that $\gamma(t_1)=\gamma(t_2)=p$ , we may have that $\dot{\gamma}(t_1)\neq\dot{\gamma}(t_2)$ , meaning that the above construction would be ill-defined. So to get around this, I assume the point is to choose $J\subseteq I$ sufficiently small so that $\gamma\vert_{J}$ is injective, and then do the above construction locally. Is this what is meant by the above? So could be write 3 instead as for each $t_0\in I$ , there exists an open subinterval $J$ of $I$ such that $t_0\in J$ , $\gamma\vert_J$ is injective, and if $\bar{X}\in C^\infty(\mathrm{T}M)$ is a vector field with $\bar{X}_{\gamma(t)}=X(t)$ for all $t\in J$ , we have $$\frac{\mathrm{D}X}{\mathrm{d}t}(t_0)=(\nabla_{\dot\gamma\circ(\gamma\vert_J)^{-1}}\bar{X})_{\gamma(t_0)}.$$ or something along those lines?","['riemannian-geometry', 'vector-fields', 'connections', 'smooth-manifolds', 'differential-geometry']"
4917018,Understanding the Series Summation: $\alpha+2\alpha^2+3\alpha^3+...=\frac{\alpha}{(\alpha-1)^2}$ [duplicate],"This question already has answers here : Formula for the summation of the series $a+2a^2+3a^3+...$ upto nth term? [duplicate] (4 answers) Closed last month . I've been working on understanding series and their summations, and I came across this particular series: $$\alpha+2\alpha^2+3\alpha^3+...$$ The solution provided states that the sum of this series is $$\frac{\alpha}{(\alpha-1)^2}$$ , but I'm having trouble understanding why this is the case. My Work: I tried to approach this problem by comparing it to the geometric series summation formula, which is $$\frac{a}{1-r}$$ where 'a' is the first term and 'r' is the common ratio. However, I'm not sure how to apply it here since the coefficients of $$\alpha$$ are increasing. Background: I'm currently an undergraduate student taking a course in Calculus II. We've covered sequences and series, including geometric series and the tests for convergence. Definitions: Here, $$\alpha$$ is a real number and the series is presumably infinite. I would appreciate any help or guidance on how to approach this problem. Thank you in advance!","['summation', 'sequences-and-series']"
4917042,Solve $\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x}$.,"This problem gives me a lot of trouble. I am asked to solve the differential equation $$\frac{dy}{dx}=\frac{2x^2-xy+y^2}{y-x}.$$ Note that this is non-linear, non-homogeneous, non-exact, and inseparable for variables $x$ and $y$ . This is a problem on my final exam. I gave attempts to test for known form of equations and substitutions but they yield nothing.","['calculus', 'derivatives', 'ordinary-differential-equations']"
4917067,the smallest volume of the cube that can contain 9 unit volume cubes,"I have been thinking about this question for some time now. If the question was about 8 cubes, the answer is obvious as we can just fit them inside a cube of side $2cm$ and hence volume $8cm^3$ . However, how can we add another cube to this with minimal volume increase in the larger cube? I came up with an answer that is less than the obvious $27cm^3$ . What I did was trying to reduce the problem to 2 dimensions and think of how large does a unit square has to be so that I can fit 5 unit squares in it. Well, I can put four squares in the corners of the larger square and I can fit a square in the middle with the vertices of those squares touching the middle of the side of the central square (I attached my drawing to represent it). This gives a side of the square measuring $(2 + \sqrt2/2)cm$ (simple calculation). This is less than the obvious answer of $3cm$ . Now we can translate this idea to three dimensions and make a cube where we place one unit cube on each vertex of the large cube, and in the bottom (take 4 cubes touching the same face) we place a unit cube so that it touches the bottom face as well forming the solution I found for 2d. This gives us a solution of $(2 + \sqrt2/2)^3 cm^3$ which is less than the obvious $27cm^3$ . So here is the question: can I prove this solution is optimal? First, I did not find a way to prove that the 2d solution is optimal, even though intuitively it seems like it is. The total area of the square required for my solution is approximately $7.33$ , so the ""remaining"" area not covered by squares (approximately $2.33$ ) gives ""room"" for another square to fit if there is an arrangement allowing it. Secondly, if I prove the solution in 2d is optimal, I could only say that the 3d ""translation"" of the 2d idea is also optimal if the projected vision in each face of the cube resulted in the optimal 2d solution. But such is not the case. But intuitively it seems to me that this is also the optimal solution for 3d. Is there a way to prove it? Thanks in advance!","['geometry', '3d']"
4917084,A parent wants to give his horses to their three children [duplicate],"This question already has answers here : Mathematical Fallacy - The $17$ camels Problem. (2 answers) Closed last month . A parent leaves their horses to their three children in their will. The oldest gets half of the horses, the middle gets a third, and the youngest gets a ninth. The parent has $17$ horses so they couldn't be divided wholly according to their will. A friend of the parent solved this problem by lending one of their horses to make the total $18$ , so the oldest gets $9$ horses, the middle gets $6$ horses, and the youngest gets $2$ horses, and each gets a little more than they were entitled to. The remaining $1$ horse is returned to the parent's friend. What is wrong here? Solution: $\dfrac{1}{2}+\dfrac{1}{3}+\dfrac{1}{9}=\dfrac{17}{18}$ , so after giving away the horses to the children, there should be $\dfrac{1}{18}$ of the horses left, but after the parent's friend takes their horse back there is none left. I'm a bit confused by this word problem. Nothing seems wrong to me, and each child actually gets a little more than they were entitled to. Is the solution saying the friend of the parent should leave that last horse because according to the will $\dfrac{1}{18}$ of the horses are left untouched? But that horse originally belongs to the friend.",['algebra-precalculus']
4917107,$\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right)$,"I'm trying to evaluate this: $$\lim_{x\to 0}\arcsin\left(\frac{\arccos\left(x\right)}{\pi}+\frac{\arccos\left(x^{2}\right)}{\pi}\right)$$ Now, calculating the for $$\lim_{x\to 0}\frac{\arccos\left(x\right)}{\pi}$$ This should equal to $\frac{1}{2}$ And similarly, $$\lim_{x\to 0}\frac{\arccos\left(x^2\right)}{\pi}$$ should equal to $\frac{1}{2}$ . And if I understand correctly, $\lim_{x\to 0}$ returns the actual value, not approximately, but the actual value that the function is approaching. So, here $\frac{1}{2} + \frac{1}{2} = 1$ And, $$\arcsin\left(1\right) = \frac{\pi}{2}$$ which is my answer. Is this valid?
The answer given in my textbook is that the limit is not defined. But, I don't understand that.","['limits', 'calculus']"
4917170,Expressing integral based on the change of variables in double integral,"Let B be the region in the first quadrant bounded by the curves $xy=1$ , $xy=3$ , $x^2âˆ’y^2=1$ , and $x^2âˆ’y^2=4.$ Evaluate $\iint_{B} (x^2 + y^2 ) dx dy$ using the change of
variables $u=x^2âˆ’y^2$ , $v=xy.$ The Jacobian is $J = 2(x^2+y^2)$ . But I have a difficulty to express the new transformed integral .It must be $$\iint_{D^*}f (x(u, v), y(u, v)) J dudv$$ $$ \iint_{D^*} ?  2(x^2+y^2) dudv$$ What is placed in the $f(x(u, v), y(u, v))$ in the solutions book is $1/2$ .But I can understand why? How can I  express $x^2+y^2$ based on $u=x^2âˆ’y^2,v=xy$ ? is $$ \iint_{D^*} \frac{x^2+y^2}{  2(x^2+y^2)} dudv = \iint_{D^*} \frac{1}{  2} dudv$$ ?","['integration', 'multivariable-calculus', 'change-of-variable']"
