question_id,title,body,tags
47121,Asymptotic approximation for confluent hypergeometric function,"I have the following nasty expression that I would like to expand in powers of $\frac{1}{N}$: \begin{align}
\frac{2^{\frac{3}{2}} 3^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{2+N}{4}\right) 
                 \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{1}{2},\frac{3r^2}{2u} \right) 
                 -\sqrt{6} r \cdot  \Gamma \left( \frac{4+N}{4} \right) \cdot {}_1F_1 \left( \frac{4+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr]
}{N \cdot  u^{\frac{1}{2}} \Biggl[ \sqrt{u} \cdot \Gamma\left(\frac{N}{4}\right) \cdot {}_1F_1 \left( \frac{N}{4},\frac{1}{2},\frac{3r^2}{2u} \right) -\sqrt{6} r \cdot  \Gamma \left( \frac{2+N}{4} \right) \cdot {}_1F_1 \left( \frac{2+N}{4},\frac{3}{2},\frac{3r^2}{2u} \right) \Biggr]}
\end{align} where you can assume that $N \in \mathbb{N}$ (but could be analytically continued to $\mathbb{R}^+$), $u \in \mathbb{R}^+$, and $r \in \mathbb{R}^+$. Furthermore, ${}_1F_1$ is the confluent hypergeometric function sometimes written as $M(a,b,z)$. Using a different route I have obtained a value for the limit $N \to \infty$, but I'd like to a) reproduce this result using the above expression and b) find the $O\left(\frac{1}{N}\right)$ corrections. So far I have tried numerous identities from the NIST Handbook of Mathematical Functions, but I simply seem to lack the experience to make real progress.
If anyone knows a solution or has an idea of how to proceed next, I'd greatly appreciate their help. With best regards, Jan","['special-functions', 'limits']"
47125,numerical integration of equations of motion of large system of particles with lubrication forces,"I have a large system of solid particles moving in the liquid. I use traditional Newtonian equations of motion for the particles. There are many different interaction forces between particles and the most severe are ""lubrication forces"". These are forces which dependent on the distance between the two particles and their velocities and the force goes to infinity when the particles get into touch. Which integration technique should I use to have as few time steps while stable? I have implemented Runge-Kutta-Fehlberg method with adaptive time step following this article: http://www.trentfguidry.net/post/2009/10/09/Runge-Kutta-Fehlberg.aspx but it produces a large number of steps even if I allow for large errors. I compare it with and Eulerian method where I estimate the future step length based on the allowed force changes. Such a dummy method produces less steps than the Runge-Kutta-Fehlberg and is stable but still is slow... Any suggestions are appreciated! PS: For those who are familiar with it. I use Lattice Boltzmann as fluid dynamics solver and Immersed Boundary Method for particles representation.","['ordinary-differential-equations', 'numerical-methods']"
47144,Factoring Quadratics,"Is there a method to find which numbers to use when simplifying quadratics? For example $x^2 + 5x + 6$ is easy enough to find, but what if I have bigger numbers, or I have this quadratic expression: $x^2 + 7x – 6$, then it gets very hard to think of a valid simplification. EDIT:
Here's an example:
$(x^2 + 5x + 6)/(x + 2)$. When simplifying the first half we get $(x + 2)(x + 3)/(x + 2)$, then we can cancel out $(x + 2)$ and the answer would be $(x + 3)$. The question is: How to always find the right numbers which to put in? (in this example they where 2 and 3)","['factoring', 'quadratics', 'algebra-precalculus', 'polynomials']"
47152,A problem in Linear algebra,"Suppose $A$ is a $n$ by $n$ matrix with entries $a_{ij}$ such that 
$$|a_{ii}|>\sum_{k\neq i}|a_{ki}|$$ for $i=1,2,...,n$,  prove $A$ is invertible.",['linear-algebra']
47171,Term for Tetrahedron with Three Right Angles at a Point,"Is there a name for the tetrahedron/pyramid (four vertices, four triangular faces, six edges) where three edges meet orthogonally at a point? Three of the faces are right triangles. Another description: taking $\vec e_1,\vec e_2,\vec e_3$ as the standard basis vectors of $\mathbb{R}^3$, the vertices are $\vec 0,c_1\vec e_1,c_2\vec e_2,c_3\vec e_3$. The faces are composed of the coordinate planes with the plane $x/c_1 + y/c_2 + z/c_3 = 1$.","['geometry', '3d', 'solid-geometry', 'euclidean-geometry', 'terminology']"
47173,Parametric integral with abs(),"I've got the integral
$$\int_B|\sin(x)-y|dx dy$$
with
$$B=\{0\leq x\leq \pi, 0\leq y\leq 1\}$$
My problem is how handle the abs in the integral. I'd probably go with two integrals (one when the abs is negative, one if it's positive), but I suppose there's a nicer solution.
Note: I know how to deal with parametric integrals, the question is how to integrate the absolute value.","['multivariable-calculus', 'calculus']"
47175,Does Birkhoff - von Neumann imply any of the fundamental theorems in combinatorics?,"I recently had the occasion to think about Hall's Marriage Theorem for the first time since my undergraduate combinatorics class more than a decade ago.  Reading the wikipedia article linked above, I was interested to see that it is regarded as equivalent to several other fundamental theorems in combinatorics, for instance Dilworth's Theorem on posets of finite width and König's Theorem on matching in (finite?) graphs.  I was able to track down the proofs of these equivalences online. However, the article also claims that Hall's Marriage Theorem is equivalent to the (König-)Birkhoff - von Neumann Theorem , which asserts that a real matrix is doubly stochastic iff it is a convex combination of permutation matrices (if you know the Minkowski-Krein-Milman theory of extreme points in convex sets, then an equivalent assertion is that the doubly stochastic matrices form a compact convex subset of $M_n(\mathbb{R})$ with extreme points precisely the permutation matrices, and since there are finitely many extreme points one has in fact a convex polytope, the Birkhoff polytope ). But the attribution here is strange: the only citation is to this series of slides by R.D. Borgersen , for which the title is ""Equivalence of seven major theorems in combinatorics""...but it only shows some of the implications!  In particular, it is not shown that Birkhoff - von Neumann implies any of the theorems as above.  So now we get to the question in my title: does it? As a secondary question, is there a nice source which treats all these theorems (maybe not Birkhoff - von Neumann, but the other five or six, depending on how you count) at once?  In particular much of the literature I've found seems to be a bit sloppy on whether the structures involved need to be finite: it seems that ""some, but not all"" finiteness conditions are needed for these theorems to hold and that the folkloric equivalence is understood to apply in the finite case only. Is this correct?","['graph-theory', 'order-theory', 'birkhoff-polytopes', 'combinatorics']"
47176,Find all bijections $ \mathbb{E}^1 \rightarrow \mathbb{E}^1 $ that preserve euclidian metric,"My question is: how do we find all bijections $\mathbb{E}^1\to\mathbb{E}^1$ that preserve the Euclidean metric? If we have a metric-preserving bijective mapping $ f: \mathbb{E}^1 \rightarrow \mathbb{E}^1 $ then $ \forall x,y \in \mathbb{E}^1 \ |x-y| = |f(x) - f(y)| \Rightarrow f'(x)=\lim_{\delta \rightarrow 0}{\frac{f(x+\delta)-f(x)}{\delta}} = \pm 1 \ \forall x \in \mathbb{E}^1 $. It means that $f(x)$ is a shift by a constant (possibly with a reflection): $ f(x) = \pm x + c, \ c \in \mathbb{E}^1 $, because $ ( f(x) - (\pm x+a) )'=0 , \ a \in \mathbb{E}^1$. Is my solution correct? And could you propose a more geometric solution?",['geometry']
47177,(Organic) Chemistry for Mathematicians,"Recently I've been reading ""The Wild Book"" which applies semigroup theory to, among other things, chemical reactions. If I google for mathematics and chemistry together, most of the results are to do with physical chemistry: cond-mat, fluids, QM of molecules, and analysis of spectra. I'm more interested in learning about biochemistry, molecular biology, and organic chemistry — and would prefer to learn from a mathematical perspective. What other books aim to teach (bio- || organic) chemistry specifically to those with a mathematical background?","['noncommutative-algebra', 'chemistry', 'finite-groups', 'semigroups', 'group-theory']"
47188,Conformally immersed Riemann surfaces and foliations,"I want to show that conformally immersed Riemann surfaces in $\mathbb{R}^4$ are leaves of a 2-foliation $\mathcal{F}$. I start with the generalized Weierstrass representation of the surfaces: take 4 holomorphic functions $ \{\phi(z)_\alpha, \psi(z)_\alpha\},~\alpha=1,2$ that satisfy a Dirac equation $\partial_z \phi_\alpha=p\psi_\alpha,~\partial_{\bar{z}}\psi_\alpha=-p\phi_\alpha$ with real-valued $p(z,\bar{z})$. These define a conformal immersion into $\mathbb{R}^4$ with coordinates $X_a(z,\bar{z}), a=1,2,3,4$ that satisfy $dX_1=\frac{i}{2}(\bar\phi_1\bar\phi_2+\psi_1\psi_2)dz+c.c.$ $dX_2=\frac{1}{2}(\bar\phi_1\bar\phi_2-\psi_1\psi_2)dz+c.c.$ $dX_3=-\frac{1}{2}(\bar\phi_1\psi_2+\bar\phi_1\psi_2)dz+c.c.$ $dX_4=\frac{i}{2}(\bar\phi_1\psi_2-\bar\phi_1\psi_2)dz+c.c.$ (for details on this ""generalized Weierstrass representation"", see Konopelchenko & Landolfi ). Call this ""immersed submanifold"" $\Sigma$. Now I want to show these surfaces define a foliation, so I want to show the field of tangent planes is integrable. In these local coordinates I can write a vector field as $ V=V^a \frac{\partial}{\partial X^a}=V^a(\frac{\partial z}{\partial X^a}\frac{\partial }{\partial z}+\frac{\partial \bar{z}}{\partial X^a}\frac{\partial}{\partial \bar{z}})$ It seems pretty obvious to me then that $[V,W]$ is a vector field on $\Sigma$ if $V,W$ are, and by Frobenius' Theorem this defines a 2-foliation. On the other hand, I know that any old surface in a smooth manifold does not necessarily define a 2-foliation and it didn't look like I did anything special except use coordinates which depend smoothly on the complex coordinate $z$. So am I right about this or did I do something strange?","['riemann-surfaces', 'differential-geometry']"
47194,Trouble solving a ODE,"What I'm trying to solve is $$ \frac{dx}{dy}=\frac{-x- y^3}{y - x^3}.$$ I've tried with substituting with $ v = \frac{x}{y} $ and $ u = \frac{y}{x},$ but even with that I still can't separate the variables. Using the second substitution I get $$ ( u + x^3u^3 - x^2 + x )  dx + (u + u^4x^2) du = 0$$ I tried working with this but found no way to simplify or advance from this in any way. I plugged it in Wolfram Alpha and of course it gave a nice looking solution to this (an ugly one for the first form of the equation though), so I know I'm missing something. Any help would be greatly appreciated.",['ordinary-differential-equations']
47198,"Infinite series $\sum_{n=1}^{+\infty} a_n x^n$ is convergent when $x=-3$, and it is divergent when $x=20$","Assume that the series $\displaystyle\sum_{n=1}^{+\infty} a_n x^n$ is convergent when $x=-3$, and it is divergent when $x=20$ Is the series $\displaystyle\sum_{n=1}^{+\infty} a_n (-2)^n$ divergent? Is the series  $\displaystyle\sum_{n=1}^{+\infty} a_n 24^n$ convergent? I figure I could use a comparison test for the second one, but the first one isn't positive, so I'm not sure.","['sequences-and-series', 'power-series', 'convergence-divergence', 'calculus']"
47209,Convergence of Sequences,"I came across the following problems on convergence of sequences during the course of my self-study of real analysis: Suppose $a_n \to a$. Define $$s_n = \frac{1}{n}\sum_{k=1}^{n} a_k$$ Prove that $s_n \to a$. So $(a_n-a)$ is a null sequence. I want to show that $(s_n-a)$ is a null sequence. By a previous exercise, I know that $(x_n)$ is a null sequence $\implies$ $(y_n)$ is a null sequence where $y_{n} = (x_1+ \cdots+ x_n)/n$. So can we do something analogous to ""adding $a$ to both sides"" to get the desired result? Show that the sequence $$a_n = \left(1- \frac{1}{2} \right) \left(1- \frac{1}{3} \right) \cdots \left(1- \frac{1}{n+1} \right)$$ is convergent. So $a_1 = \frac{1}{2}$, $a_2 = \frac{1}{3}, \dots, a_n = \frac{1}{n+1}$. So I conjecture that $(a_n)$ is a null sequence. In other words, for each $\epsilon >0$, $|a_n| \leq \epsilon$ for all $n>N$. Let $\epsilon = \frac{1}{n}$. Choose $N = n+1$. Then the convergence follows? Prove that the sequence $$a_n = \frac{1}{n+1}+ \frac{1}{n+2} + \dots + \frac{1}{n+n}$$ is convergent to a limit $\leq 1$. So $a_{n} < a_{n+1}$ for all $n$. Then I need to show that it is bounded above by $1$. To show this should I consider $(1-a_n)$? All the terms are $<1$.",['real-analysis']
47211,Scaling a histogram in the face of outliers,"I am trying to figure out how to display a histogram of a digital image in the face of massive outliers (lots of shadows, highlights, or lots of anything inbetween).  If I simply choose the bin with the most entries to be the '100% height' of my fixed display area the rest of the bins are dwarfed and you can't really get any useful information by looking at it. I attempted to find the standard deviation between the bins then only include bins within a certain number of std devs from the average when picking the '100% height' bin, but it didn't turn out too well in the general case... certain number of std devs worked well for some images and not others. A good example of what I want is Photoshop's histogram, but I'm not sure how they accomplish this and I've come up short on the google.  Does anyone have any advice?","['statistics', 'image-processing']"
47212,Is the product of ideals commutative?,"My algebra book introduces sum, intersection and product of ideals (in non-commutative rings), and then says that all three operations are commutative and associative, without proof. I see no reasons why the product of ideals should be commutative, but I wasn't able to find a counterexample either.",['abstract-algebra']
47220,Parabolic shape in Bow (not arrow!),"This is what I am thinking for some days. And I think here are some experts who can answer this question. If I bend any stick made with material that uniform density and its shape is cylindrical (more like stick than real cylinder), will the shape of it converge to parabola? If so, how long it'll be? I mean from the time I start to bend it and to the time It breaks. Between these two times when this will form a real parabola?","['ordinary-differential-equations', 'conic-sections', 'physics']"
47226,$F_\sigma$-subsets in a normal space can be separated,"Let $X$ be a normal Hausdorff space and let $C,D$ be two $F_{\sigma}$ subsets of $X$
such that $\overline{C} \cap D = \emptyset$ and $C \cap \overline{D} = \emptyset$. Prove there exists disjoint open subsets $U,V$ such that $C \subset U$ and $D \subset V$. No idea how to show this. Can you please help?",['general-topology']
47227,How do I determine if the vectors lie on a plane in an $N\times N$ matrix?,"In a $2\times2$ matrix, it is quite easy to see if the vectors lie on a plane or not. By vector, I mean the columns of the matrix. I usually determine if the numbers are of a certain multiple. From there, I can judge that the columns of the matrix lie on a plane and that means its determinant is zero and therefore not invertible. So for instance: 
$$
\left |\begin{bmatrix}
2 & 4\\ 
4 & 8
\end{bmatrix}  \right | = 0
$$
Both vectors $\begin{bmatrix}
2\\ 4
\end{bmatrix}$ and $\begin{bmatrix}
4\\ 8
\end{bmatrix}$ lie on the same plane because the second vector is just 2 times of the first vector. But in a $3\times3$ or $N\times N$ matrix, how can I get to know if a matrix is invertible or not by observing it without going through to calculate its determinant? For example in a matrix say $\begin{bmatrix}
1 & 4 & 7\\ 
2 & 5 & 8\\ 
3 & 6 & 9
\end{bmatrix}$, the numbers are not of a common multiple of each other. But still, the determinant of this matrix gives zero, which means the matrix is not invertible. But I wouldn't have known this without going through the trouble to calculate its determinant value. Since the numbers are not of a multiple, I am also not sure if the columns of the matrix lie on the same plane or not. Thanks for any help.","['vector-spaces', 'matrices', 'linear-algebra']"
47230,Value of $\lim_{n\to\infty}{(1+\frac{2n^2+\cos{n}}{n^3+n})^n}$,"How should one go about computing $$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^n}\quad?$$ What surprised me about this is that 
$$\lim_{n\to\infty}{\left(1+\frac{2n^2+\cos{n}}{n^3+n}\right)^\frac{n^3+n}{2n^2+\cos{n}}}=1$$(according to wolfram ), instead of $e$, which is what I expected. Could someone comment on that also?",['real-analysis']
47234,What does Trotter Product Formula mean?,"For some reason, I have to work with Trotter product formula recently, but I do not have a strong background in functional analysis. The following is the statement of the formula from MathWorld When A and B are self-adjoint operators, 
  $$
e^{t(A+B)} = \lim_{n \to +\infty}(e^{tA/n}e^{tB/n})^n
$$ My questions are: What does the exponential of an operator mean precisely? How to interpret the convergence? In terms of some norm?","['semigroup-of-operators', 'functional-analysis']"
47237,Question about proof for $S_4 \cong V_4 \rtimes S_3$,"Let $V_4=\{I, (12)(34), (13)(24), (14)(23)\}$ . In my book they give the following proof for $S_4 \cong V_4 \rtimes S_3$ : Let $j: S_3 \rightarrow S_4: p \mapsto  \left( \begin{array}{cccc}
1 & 2 & 3 & 4 \\
p(1) & p(2) & p(3) & 4 \end{array} \right)$ Clearly, $j(S_3)$ is a subgroup $S_4$ isomorphic with $S_3$ , hence $j $ is injective. We identify $S_3 $ with $ j(S_3$ ).
Also $V_4 \triangleleft S_4$ and clearly $V_4 \cap S_3 = \{I\}$ .
We now only have to show that $S_4 = V_4S_3$ . Hence $V_4\cap S_3 = \{I\}$ , we know that $\#(V_4S_3) = \#V_4 \#S_3 = 4 \cdot 6 = 24 = \#S_4$ thus $S_4 = V_4S_3$ , which implies that $S_4 \cong V_4 \rtimes S_3$ . However, I am wondering what the function $j$ is actually used for in the proof? (I do not see the connection.)","['semidirect-product', 'symmetric-groups', 'abstract-algebra']"
47246,When could these be equal? Differentials,"given $ax^2y''+bxy'+cy=0$ a,b, and c are real. x is positive. I want to show that 
$$a \frac{d^2y}{dv^2} +(b-a)\frac{dy}{dv} +cy =0.$$
This is a problem in a elementary text, I have found a more rigours method online, but I'm trying to make sense of the last step in my own less rigours idea. Using the coefficients $a$, $b$, and $c$, I observe that if this is true then
$$xy'=\frac{dy}{dv}\quad\text{and}\quad
x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv},$$
so if I show the two statements above are true I will have justified the subsitution. Let $\ln x = v$
$$\frac{1}{x} = \frac{dv}{dx}$$
$\ln$ is one to one so it has an inverse and 
$$\begin{align*}
x &= \frac{dx}{dv}\\
xy'&= \frac{dx}{dv} \frac{dy}{dx}\&&\text{by the chain rule}\\
xy'&= \frac{dy}{dv}.
\end{align*}$$
Great one down one to go. Continuing with the above...
$$x \frac{dy}{dx} = \frac{dy}{dv}.$$
I  take the derivative with respect to $x$ of both sides, using the product rule on the right.
$$x \frac{d^2y}{dx^2} + \frac{dy}{dx} = \frac{d}{dx}  \left( \frac{dy}{dv} \right)$$
I dont feel so good about the right side, but I  keep going anyway. $$x \frac{d^2y}{dx^2} = \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx}$$
since $x$ is $e^v$, $x$ is its own derivative with respect to $v$. I multiply through by $x$:
$$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dx} \frac{dv}{dx}$$
chain rule on the far right. $$x^2 \frac{d^2y}{dx^2} = x \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$
I'm so close but I'm stuck! all I want to say is:
$$x^2y''= \frac{d^2y}{dv^2} - \frac{dy}{dv}$$
and it is very sugestive to write:
$$x^2 \frac{d^2y}{dx^2} = \frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) - \frac{dy}{dv}$$
but what could it mean to cross out the $dx/dx$? that is in my way? as I  was told before here such operations are ""dubious"" though I'm still trying to grasp why. But I  know this is true so it must be the case that for these functions
$$\frac{dx}{dv} \frac{d}{dx}  \left( \frac{dy}{dv} \right) = \frac{d^2y}{dv^2}$$
perhaps I  can say that the kind of functions that work in the above are just the kind I'm working with and then I  would be done?",['ordinary-differential-equations']
47266,Proving that $\det{(A^i_j})= \sqrt{ |\det{(G)}|}$,"Let $V$ be an $n$-dimensional vector space and let $(v_1, \dots, v_n)$ denote any oriented basis for $V$. Also, let $g$ be an inner product on $V$ and let $G$ denote the Gram matrix of inner products $G = [g(v_i, v_j)]$. I am trying to show that if $v_j = A^k_je_k$, where $e_k$ denotes a basis that is orthonormal with respect to g, then $\det{(A^i_j)} = \sqrt{G}$. I believe I have found a useful intermediate result, but I'm not really sure how to close the deal. For vectors $v_i$ and $v_j$ we have: $$
g(v_i, v_j) = g(A^k_i e_k, A^r_j e_r) = A^k_iA^r_j \delta_{kr} = \sum\limits_{m=1}^n A^m_iA^m_j = \langle A_i | A_j\rangle
$$ where $A_k$ denotes the $k^{th}$ column of $A$ and $\langle\cdot | \cdot\rangle$ denotes the standard Euclidean inner product. Therefore, the matrix $G$ is given by $$
G = [\langle A_i|A_j \rangle]
$$ At this point, I'm not sure what to do. I'm thinking there's some essential fact I need to know in order to continue. So, my question is, am I on the right track and if so what should my next step be? Edit: I updated this question to change the assumption that the $e_i$ are the standard basis vectors to the assumption that the $e_i$ are orthonormal with respect to $g$",['linear-algebra']
47272,Accelerating Convergence of a Sequence,"Suppose I had a monotonically increasing sequence $\{d_{n}\}$ which is also bounded above. The $d_{n}$'s satisfy a given recurrence, however computationally they tend very slowly to the limit. What are some ways which I can use to speed up the convergence of this sequence computationally? I am computing these $d_{n}$ in PARI/GP if that is useful information.","['computational-mathematics', 'sequences-and-series', 'algorithms', 'numerical-methods']"
47285,Fundamental Theorem of Calculus,"The Fundamental Theorem of Calculus says the following: Theorem. If $f$ is the derivative of $F$ at every point on $[a,b]$ , then under suitable hypotheses we have that $$\int_{a}^{b} f(t) \ dt = F(b)-F(a)$$ Theorem. If $f$ is integrable on $[a,b]$ , then under suitable hypotheses we have that $$\frac{d}{dx} \int_{a}^{x} f(t) \ dt = f(x)$$ I am trying to put myself in the shoes of Poisson, Cauchy and Riemann. The first theorem is basically saying that to find the area under a curve, we need to find any anti-derivative and evaluate it at the endpoints? The second theorem is saying that we can view the integral as a function of $x$ and take its derivative to get $f(x)$ . Wasn't the goal of Poisson, Cauchy and Riemann to find the area under a curve? So they hypothesized the first theorem and then only later proposed the second theorem? Both theorems deal with finding the area under a curve (i.e. they are equivalent)? Do these theorems still hold under other types of integration (i.e. the Lebesgue integral)?","['soft-question', 'real-analysis']"
47293,Qualitative interpretation of Hilbert transform,"the well-known Kramers-Kronig relations state that for a function satisfying certain conditions, its imaginary part is the Hilbert transform of its real part. This often comes up in physics, where it can be used to related resonances and absorption. What one usually finds there is the following: Where the imaginary part has a peak, the real part goes through zero. Is this a general rule? And are there more general statements possible? For Fourier transforms, for example, I know the statement that a peak with width $\Delta$ in time domain corresponds to a peak with width $1/\Delta$ (missing some factors $\pi$, I am sure...) in frequency domain. Is there some rule of thumb that tells me how the Hilbert transform of a function with finite support (e.g. with a bandwidth $W$) looks like, approximately? Tanks,
Lagerbaer","['integral-transforms', 'functional-analysis', 'signal-processing']"
47306,"""Resultant"" of three polynomials","The resultant $\operatorname{Res}(f,g)$ of two polynomials over a field $k$ is a polynomial in the coefficients of $f$ and $g$ which enjoys the property of being nonzero if and only if $f$ and $g$ have no common root in an algebraic closure $\overline{k}$ of $k$. Does there exist a similar construction for three polynomials? There seems to be none. I would like to suggest the following conjecture: Conjecture : there does not exist a function $\operatorname{Res}(f,g,h)$ of three polynomials $f,g,h \in k[x]$, which is a polynomial in the coefficients of $f,g,h$, having the property of being zero if and only if the polynomials $f,g,h$ have a common root in an algebraic closure $\overline{k}$ of $k$.","['abstract-algebra', 'polynomials']"
47312,Orthonormal basis error diagonally dominant?,"I'm working on an error estimate for a numerical method, and in the process I've stumbled across the following abstract inequality which I think is true, but am having a hard time proving. Suppose $\{\phi_i\}_{i=1}^N$ and $\{\psi_i\}_{i=1}^N$ are orthonormal bases of $\mathbb{R}^N$ with $(\phi_i,\psi_i) \ge 0$, and call the ""error"" between these basis vectors $e_i=\phi_i-\psi_i$. Is there a constant $C$ independent of N such that
$$
\sum_{i \neq j} (e_i,e_j) \le C \sum_i ||e_i||^2?
$$
I've written a matlab script to test it for thousands of random sets of orthonormal vectors and found no counterexamples so far for C=1.","['linear-algebra', 'inequality', 'analysis']"
47314,Trig identity $1+\tan x \tan 2x = \sec 2x.$,"I need to prove that: $$1+\tan x \tan 2x = \sec 2x.$$ I started this by making sec 1/cos and using the double angle identity for that and it didn't work at all in any way ever. Not sure why I can't do that, but something was wrong. Anyways I looked at the solutions manual and they magic out $$1 + \tan x \tan 2x = 1+\tan x\left(\frac{2 \tan x}{1-\tan ^2x}\right) $$ which I recognize as the double angle forumla sort of, I just don't understand why I can use that and how they magiced it into this. It is just too difficult to think of an equation in an equation in an equation. I tried working with this and got nowhere near the right answer.",['trigonometry']
47321,"Proof of $ \text{Var}\,\left(\sum_{i=1}^{n}g(X_i)\right)=n\left(\text{Var}\,g(X_1)\right).$","I have a question about part of a proof of a Lemma in a book (Casella's Statistical Inference) I'm reading. This it how it goes. Let $X_1, \cdots ,X_n$ are a random sample from a population and let $g(x)$ be a function such that $\mathbb{E}g(X_1)$  and $\text{Var}\,g(X_1)$ exist. Then $$ \text{Var}\,\left(\sum_{i=1}^{n}g(X_i)\right)=n\left(\text{Var}\,g(X_1)\right).$$ So this is how I proceeded to to prove it. Since the $X_i's$ are independent, we have that $$
\begin {align*} 
  \text{Var}\,\left(\sum_{i=1}^{n}g(X_i)\right)&= \text{Var}\,g(X_1)+\cdots +\text{Var}\,g(X_n)\\
&= n\text{Var}\, g(X_1). \end {align*}$$ 
where the last equality holds because the $X_i's$ are identically distributed. 
Can I do this? I'm asking this because the proof in the book started by using the definition of the variance and somewhere along the lines involved the covariance matrix. Thanks.",['statistics']
47329,Calculating heavy numbers in a given range?,"I had an interview question a couple days ago involving the calculation of the amount of heavy numbers between in a given range. I came up with a basic solution, which was to iterate over the entire range and test for the heaviness of each number. I realized right away that its a terribly inefficient method of solving the problem. In the interest of educating myself I have been tackling the problem today to try and learn more about this type of problem. Its tough for me, because I don't have a super strong mathematics background but I'm trying to learn. Basically, the problem goes like this: Take any 2 numbers from 0 to 200,000,000. In that range, calculate the number of ""heavy numbers"". This is calculated by adding together all of the component digits and then dividing by the number of components. If the average is greater than 7, the number is considered heavy. Example: 1234: 1+2+3+4/4 = 2.5 (not heavy)
8996: 8+9+9+6/4 = 8 (heavy) So given the range 1002 - 12089, my naive approach is to iterate over each number between and calculate its weight. This approach quickly falls apart when dealing with very large number ranges. I've done quite a bit of searching via Google and Bing and beyond a couple of posts where people have copy/pasted verbatim the question, I find almost no information about this kind of problem. I suspect that this type of problem is called something other than ""heavy numbers"" but I'm at a loss as to what that the terminology is. Here is a link to one such discussion: http://groups.google.com/group/algogeeks/browse_thread/thread/a1b824107afe3801 I'm hoping that someone who is familiar with type of scenario can explain alternative ways to solve the problem in a simple manner, and that I might ask follow up questions until I understand. Thanks for reading.",['combinatorics']
47343,Are there always singularities at the edge of a disk of convergence?,"Take a function that is analytic at 0 and consider its Maclaurin Series.  Here are some examples I'll refer to: $$\frac{1}{1-x} =\sum_{n=0}^\infty x^n$$
$$\frac{1}{1+x^2} =\sum_{n=0}^\infty(-1)^nx^{2n}$$
$$\ln(1-x) =-\sum_{n=1}^\infty\frac{x^n}{n}$$
$$\sqrt{1-x} =1-\sum_{n=1}^\infty\frac{(2n-2)!}{2^{2n-1}n!(n-1)!}x^n$$ Each of these series has a radius of convergence of 1.  And each function either $\bullet$ has a singularity along the edge of the disk of convergence (at 1, $\pm i$, and 1 in the first three examples respectively) or $\bullet$ has a derivative with a singularity along the edge of the disk of convergence (the last example is this way at 1). My question is: Suppose a function $f$ is analytic at 0 and its Maclaurin Series has a radius of convergence $r<\infty$.  Does it have to be the case that some derivative (0th, 1st, 2nd, ...) of $f$ blows up somewhere along the edge of the disk of convergence?","['sequences-and-series', 'convergence-divergence', 'calculus', 'complex-analysis', 'taylor-expansion']"
47345,Number of ways of distributing $n$ identical objects among $r$ groups,"I came across this formula in a list: 
The number of ways of distributing $n$ identical objects among $r$ groups such that each group can have $0$ or more $(\le n)$ objects I know that standard way of doing this is to solve the problem of distributing n identical objects and $(r-1)$ partitions among themselves which can be done in $C(n+r-1,r-1)$ ways. But I am unable to prove to myself why it is not $(r+1)^n$. 
Because each of the n objects has r+1 choices, either group1, group2,... group r or none at all.",['combinatorics']
47350,Proof of the independence of the sample mean and sample variance,"I've been trying to establish that the sample
mean and the sample variance are independent. One motivation is to
try and write the sample variance, $S^{2}$ as a function
of $\left\{ X_{2}-\bar{X},X_{3}-\bar{X},\cdots,X_{n}-\bar{X}\right\} =A$
only. Then we proceed by showing that $A$ and $\bar{X}$ are independent ( which I'm unable to show ),
which then implies the independence of $S^{2}$ and $\bar{X}.$ I would appreciate it if the good people of M.SE would help guide me in the right direction. Thanks. Edit: The random samples $X_1,\cdots,X_n$ are from an $N(\mu, \sigma)$ distribution.",['statistics']
47356,"Are ""$n$ by $n$ matrices with rank $k$"" an affine algebraic variety?","Identify the set of all complex $n$ by $n$ matrices with $\mathbb{C}^{n^2}$.  We say a subset $S \subset \mathbb{C}^{n^2}$ is an affine algebraic variety if $S$ is the common zero set of a collection (possibly infinite or uncountable) of polynomials in $n^2$ complex variables.  Some examples that satisfy this definition are ""matrices with determinant 1"" and ""matrices with rank at most $k$"". A non-example is ""matrices with rank $n$"", because it is the complement of the affine algebraic variety ""matrices with rank at most $n - 1$""; any affine algebraic variety is necessarily closed in the Euclidean topology, so its complement (which is open in the Euclidean topology) cannot be an affine algebraic variety. However, I'm having trouble formulating a statement about ""matrices with rank $k$"" for $1 < k < n$. Is the set of complex $n$ by $n$ matrices of rank $k$, where $1 < k < n$, an affine algebraic variety?",['algebraic-geometry']
47360,Urysohn's function on a metric space,"Let $(X,d)$ be a metric space and $A\subset B\subset X$. $A$ is closed, $B$ is open. If there are developed methods to find at least one (or describe the whole class) of Urysohn's functions for $A$ and $B^c$? Edited: Martin has already presented a nice example of an Urysohn's function. Now I would like to focus on my second question about the whole class $\mathcal U(A,B^c)$ of Urysohn's functions. I had a function $u$ which is $0$ on $B^c$ and $1$ on A - and I would like to check if it is continuous (so if it is in $\mathcal U(A,B^c)$ class). If there are any sufficient conditions to prove that this function is $\mathcal{U}(A,B^c)$ which allow me pass direct verifying of continuity of $u$, or this question is meaningless and all that I can do - is to verify continuity without theory of Urysohn's functions?","['general-topology', 'separation-axioms', 'metric-spaces', 'functional-analysis']"
47375,"1-Variable Real Analysis True/False questions on Supremum, Infimum, and Inequalities with them","(S. Abbott. Understanding Analysis 1 ed. pp 18 question 1.3.9) is asking me to answer the following questions without any formal proofs. I have some intuition for them, but I was hoping to get some external input as well. It would be great if you could maybe give some rigorous explanations to the intuition behind. a) A finite, nonempty set always contains its  supremum. - I think this is True. b) If $a < L$ for every element $a$ in the set $A$, then sup$A < L$. - I think this is False, because it could be $\leq$. c) If $A$ and $B$ are sets with the property that $a < b$ for every $a \in A$, and every $b \in B$, then it follows that sup$A < $ inf$B$. - I think that again, it's $\leq$ and not strictly less. d) If sup$A$ = $s$, and sup$B$ = $t$, then sup($A+B$)=$s+t$. $A+B={a+b | a \in A, b \in B}$. - I think it's True, after thinking of several examples with sets that do and do not contain their suprema, but I am not sure. e) If sup$A \leq$ sup$B$, then there exists an element $b \in B$ that is an upper bound for $A$. - I thought this was False, because we can set $B=A$, and the condition on superma would hold, yet pick such an $A$ that doesn't contain its own supremum. Thank you!",['real-analysis']
47385,Is there a natural number between $0$ and $1$?,"Is there a natural number between $0$ and $1$? A proof, s'il vous plaît, not your personal opinion. (Assume the Peano Postulates.)","['natural-numbers', 'logic', 'elementary-set-theory']"
47395,The Duals of $l^\infty$ and $L^{\infty}$,"Can we identify the dual space of $l^\infty$ with another ""natural space""?  If the answer is yes, what can we say about $L^\infty$ ?
By the dual space I mean the space of all continuous linear functionals.","['lp-spaces', 'dual-spaces', 'functional-analysis', 'banach-spaces']"
47401,Hom is a left-exact functor,"If $$
0 \to A \to B\to C,$$ is a left exact sequence of $R$ -module, then for any $R$ -module $M$ , $$
0 \to \operatorname{Hom}_R(M,A)\to \operatorname{Hom}_R(M,B)\to \operatorname{Hom}_R(M,C),
$$ is left exact. I proved the above, and highlighted what I'm a little unfamiliar with: Let $$
0 \to A\ \xrightarrow{i}\ B\ \xrightarrow{f}\ C,
$$ and $$
0 \to \operatorname{Hom}(M,A)\ \xrightarrow{\operatorname{Hom}(M,i)}\ \operatorname{Hom}(M,B)\ \xrightarrow{\operatorname{Hom}(M,f)}\ \operatorname{Hom}(M,C).
$$ We need to show that $$
\ker\left(\operatorname{Hom}(M,f)\right)=\operatorname{Hom}(M,i)(\operatorname{Hom}(M,A)).
$$ Let $i \circ \varphi \in \operatorname{RHS}$ . Then $f \circ i \circ \varphi : M \to C$ is $0$ since $f \circ i \circ \varphi(M) \subseteq f( i(A)) = f(\ker(f))=0$ . Conversely, let $\psi \in \operatorname{LHS}$ . Then $f \circ \psi = 0$ so that $ f(\psi(M))=0$ . Hence $\psi(M) \subseteq \ker(f)=i(A)$ . Since the image of $\psi$ is contained in the image of $i$ , we may factor $\psi$ as $\psi=i \varphi$ with $\varphi : M \to A$ . (Here is my trial, but I'm not fully understanding this: Since $i$ is injective, $i(A)$ is isomorphic with $A$ . So $i^{-1}(\psi (M)) \subseteq A$ and if we let $\varphi=i^{-1} \psi$ , then $\psi = i \varphi$ .) And I have one more question: The above looks very messy, especially the notation. Is there a better proof/understanding about it?","['exact-sequence', 'abstract-algebra', 'homological-algebra', 'category-theory', 'modules']"
47411,What is the tensor product of a sheaf and a module?,"The following object was studied in section III.12 of Hartshorne's Algebraic Geometry book. 
Let $A$ be a noetherian ring, $Y=\mathrm{Spec}A$, and $M$ be an $A$-module.  Let $f:X\to Y$ be a morphism, and $\mathcal{F}$ be a quasicoherent sheaf on $X$. Then Hartshorne writes $\mathcal{F}\otimes_A M$ without defining what it is (technically, he already used it in Coroally III.9.4). Here is what I have guessed.  For each affine $U\subset X$, $A_U:=\mathcal{O}_X(U)$ is an $A$-algebra. So we have a presheaf by assigning $U$ with the module $\mathcal{F}(U)\otimes_A M$.  Then we will call the associated sheaf of this presheaf $\mathcal{F}\otimes_A M$. But what puzzles me is that in Proposition 12.2, it was claimed that if $C^*(\mathfrak{U}, \mathcal{F})$ is the Čech complex of $\mathcal{F}$, then for any $i_0, \ldots, i_p$, we have $$\Gamma(U_{i_0,\ldots, i_p}, \mathcal{F}\otimes_A M)=\Gamma(U_{i_0,\ldots, i_p}, \mathcal{F})\otimes_A M.$$
In other words, it seems to me that the book claims that the presheaf that I have defined is in fact a sheaf, which is not obvious to me at all.  Could anyone explain what I am missing here? Edit: In proposition 12.2, it was assumed that $\mathcal{F}$ is flat over $Y$.",['algebraic-geometry']
47414,Involuted vs Idempotent,"What is the difference between an ""involuted"" and an ""idempotent"" matrix? I believe that they both have to do with inverse, perhaps ""self inverse"" matrices. Or do they happen to refer to the same thing?","['linear-algebra', 'terminology']"
47418,Finding the Laurent series,"While studying i got this exercise and would like some pointers where i'm going wrong. Find the Laurent Series at $z_0=2i$ $$f(z) = \frac{1+z}{z^2+4}+e^z$$ I've tried the following: $$f(z)=(1+z)\frac{1}{(z+2i)(z-2i)}+e^z$$ and the series for $$\frac{1}{(z+2i)(z-2i)} = \frac{1}{z-2i}\frac{1}{4i}\frac{1}{1+(\frac{z-2i}{4i})}
 = \displaystyle\sum\limits_{k=0}^{+\infty} \frac{(-1)^k}{(4i)^{k+1}}(z-2i)^{k-1}$$ after this i'm stumped in trying to put everthing has a power of $z-2i$ $$f(z) = (1+z)\displaystyle\sum\limits_{k=0}^{+\infty} \frac{(-1)^k}{(4i)^{k+1}}(z-2i)^{k-1}+\displaystyle\sum\limits_{k=0}^{+\infty}\frac{z^k}{k!}$$ I have several questions regarding this: 1 - should i try to put $\displaystyle \frac{1+z}{z^2+4}$ has a sum of simple fractions? 2 - is there even a Laurent series for $e^z$ around $z_0=b$ since $e^z$ doesn't have any singularities? 3 - What should i do to that $(1+z)$?","['sequences-and-series', 'complex-analysis']"
47419,Using quaternions instead of 4x4 matrices for transformations,"I'm interested in implementing a clean solution providing an alternative to 4x4 matrices for 3D transformation. Quaternions provide the equivalent of rotation, but no translation. Therefore, in addition to a Quaternion, you need an additional vector of translations $(t_x,t_y,t_z)$. I have always seen it stated that you need 12 values for the matrix representation, and only 7 for the quaternion-based representation. What I don't understand is how to manipulate the translation values. For rotation of a quaternion, no problem. For a vector $v$, an axis vector $x$, and an angle $a$:
$$q = \cos\left(\frac{a}{2}\right) + x \cdot \sin\left(\frac{a}{2}\right)$$
To rotate the vector:
$$v' = qvq^{-1}$$
For multiple rotations, you can apply the transformations to the quaternion, and only when you have the final rotation do you have to apply it to the data. This is why matrix transformation is so nice in 3d graphics systems. Ok, so now if translation enters into it, what do I do? A given vector transformation is:
$$T = (t_x,t_y,t_z)$$
$$v' = qvq^{-1} + T$$
If I want to apply a rotation and translation operation to this, I would have to modify $T$ and $q$. What should the result be?","['matrices', 'quaternions', 'transformation']"
47422,Nested sets convergence,"Define $\xi\in C^1([-1,1]\times[-1,1])$ such that
$$
\int\limits_{-1}^1 \xi(x,y)\,dy = 1
$$
for all $x\in[-1,1]$ and $\xi\geq 0$. Put $A_0 = [0,1]$ and $$A_{n+1} = \left\{x\in A_n:\int\limits_{A_n}\xi(x,y)\,dy = 1\right\}.$$ Are there methods to find the rate of convergence of $\lambda(A_{n}\setminus A_{n+1})$ where $\lambda$ is a Lebesgue measure? Maybe you can refer me to the relevant literature? There are at least two kinds of situation: for some $N$  we have $A_N = \emptyset$, then $A_{N+1} = A_N$ and $\lambda(A_n\setminus A_{n+1}) = 0$ for $n\geq N$. for some $N$  we have $A_N = A_{N+1}$, then again $\lambda(A_n\setminus A_{n+1}) = 0$ for $n\geq N$. Can you help me to construct an example for the third case, namely when $A_n\neq A_{n+1}$ for all $n\geq0$ (of course if such an example exists)? We can also assume for this example that $\xi\in \operatorname{Lip}([-1,1]\times[-1,1])$, not necessary differentiable. I am mostly interested if it is possible to find a Lipschitz $\xi$ such that $A_n$ coincide with iterations of Smith-Volterra-Cantor set (Fat Cantor Set)?","['stochastic-processes', 'general-topology', 'measure-theory', 'probability', 'functional-analysis']"
47430,"Is Fourier series an ""inverse"" of Taylor series?","I've understood Taylor series as being the representation of a ""transcendental"" function, using power functions with coefficents represented by appropriate derivatives. (Or maybe it is the MacLauren series, where $\cos x= 1-\frac{x^2}{2!}+\frac{x^4}{4!}+...$) I've understood Fourier series as being the representation of periodic linear functions using integral coefficients multiplied by transcendental functions. Is my understanding correct in either or both cases? And if one consists of derivatives and the other of integrals, does that mean that Fourier series are the converse (or ""inverse"") of Taylor series?","['fourier-series', 'sequences-and-series', 'taylor-expansion']"
47433,Origin of the name 'test functions',"This is a very simple question really: where did the name 'test functions', used nowadays when speaking of infinitely differentiable and compactly supported functions, come from? More to the point: is there a mathematical reason these functions are called that way?","['functional-analysis', 'math-history', 'partial-differential-equations', 'distribution-theory', 'terminology']"
47440,an $L^\infty$ norm equal to a supremum,"My question arose while studying an article which finds the $K$-functional for the pair of spaces $L^1,L^\infty$, so it's related to interpolation theory, but I think it can be solved with some $\inf,\sup$ manipulations. I'm sorry if the tags aren't correct. Consider $(X,\Sigma, \mu)$ an arbitrary measure space. For each measurable function $f: X \to \Bbb{C}$ and $\alpha \geq 0$ define 
$$ f_*(\alpha)=\mu(\{ x \in X : |f(x)|>\alpha \}),$$
the distribution function of $f$. For any measurable function $f:X \to \Bbb{C}$ for which there exists $\alpha>0$ with $f_*(\alpha)<\infty$, define $f^* :(0,\infty) \to [0,\infty)$ by
$$ f^*(t)=\inf \{ y>0 :f_*(y) \leq t\}.$$ It can be proved from the definitions that for every $t>0$ we have
$$ f^*(f_*(t))\leq t,\ f_*(f^*(t))\leq t.$$ Moreover, the function $f^*$ is continuous from the right. Both of the functions $f_*,f^*$ are non increasing. What I need to prove is that $$\sup_{t>0} f^*(t)= \| f\|_\infty,$$ for every function $f \in L^\infty$. The inequality $\leq $ is straight from the definition. The other one I can't seem to get, and the text says that it's pretty hard, but with ""a little more effort"" it can be done. I haven't succeeded.","['real-analysis', 'analysis']"
47441,A sine product with (almost) integer values,"Let n, k be integers, $n>1$ and $k \perp n$ denote that k, n  are coprime and let $S_n = \{1 \le k \le \lfloor n / 2 \rfloor :  k  \perp n \}.$ Then 
$$ n \left( \prod_{k \in S_{n}} \sin \left( k \frac {\pi}{n} \right) \right)^{-2}  \in \mathbb{Z}. $$ I think this is surprising but I have no proof.","['analysis', 'number-theory']"
47448,Connection between eigenvalues and eigenvectors of a matrix in different bases,If you have a matrix $A$ you can find its eigenvalues and eigenvectors. If you represent this matrix relative to another basis $\mathcal{D}$ you can again find its eigenvectors and eigenvectors. My questions What is the connection between the eigenvalues and eigenvectors of this same matrix in different bases? Why is this so? And how do you interpret the eigenvalues and eigenvectors in these different bases? Could you please also provide some intuition and an example? Thank you! EDIT I created a follow-up question with a more complicated example here: Example of eigenvectors in different bases (follow-up question),"['linear-algebra', 'eigenvalues-eigenvectors']"
47459,Smith normal form of graded modules. (Major edit),"Ok, this is a major rewriting of my previous entry which no one answered. Let us have two graded $F[t]$-modules M and N with bases $m_1, \ldots, m_m$
and $n_1, \ldots, n_n$, respectively, and $F$ is a field ($F[t]$ is a PID). Note that $m_i$ and $n_j$ are homogeneous elements. We are now interested in calculating the image of a certain 0-degree graded homomorphism from $M$ to $N$. Note that 
$$M \simeq F[t]\deg(m_1)\oplus\cdots\oplus F[t]\deg(m_m)$$ and similarly for $N$. The grading is the standard grading, i.e. $t(c_0, c_1, \ldots) = (0,c_0, c_1, \ldots)$. Now let $\partial$ be our homomorphism and the basis elements of $M$ and $N$ with degrees in () are given by: 
$$ab(1), bc(1), cd(2), ad(2), ac(3)$$
for $M$
and 
$$a(0), b(0), c(1), d(1)$$
for $N$. Then $\partial$ is defined by 
$$\partial(ab) = t^{\deg(ab)-\deg(b)}b - t^{\deg(ab)-\deg(a)}a = tb-ta$$
and exactly the same for the rest, 
$$\partial(bc) = c - tb$$
$$\partial(cd) = td - tc$$
$$\partial(ad) = td - t^2a$$
$$\partial(ac) = t^2c - t^3a$$ Now sorting the basis elements of $N$ in descending order $d,c,b,a$ we can represent $\partial$ by $$
 \begin{pmatrix}
  * & ab & bc & cd & ad & ac \\
  d & 0 & 0 & t & t & 0 \\
  c & 0 & 1 & t & 0 & t^2 \\
  b & t& t & 0 & 0 & 0 \\
  a & t & 0 & 0 & t^2 & t^3
 \end{pmatrix}
$$ Using column operations we can keep homogeneous bases and reduce the matrix to column echelon form $$
 \begin{pmatrix}
  * & cd & bc & ab & z_1 & z_2 \\
  d & t & 0 & 0 & 0 & 0 \\
  c & t & 1 & 0 & 0 & 0 \\
  b & 0& t & t & 0 & 0 \\
  a & 0 & 0 & t & 0 & 0
 \end{pmatrix}
$$ where $z_1 = ad - cd - t\cdot bc - t\cdot ab$ and $z_2 = ac - t^2\cdot bc - t^2\cdot ab$ form homogenous basis for the kernel. Note that we have that for an entry in the matrix that the degree of the element at that position + the degree of the row basis element = the degree of column basis element. Now the author of the paper argues:
The pivots in column-echelon form are the same as the diagonal elements in Smith normal form. Moreover, the degree of the basis elements on pivot rows is the same in both forms. With proof:
Because of our sort, the degree of row basis elements is monotonically decreasing from the top rown down. Within each fixed column $j$ the degree of the column basis element is constant equal to $c$ and therefore $\deg\partial_{i,j} = c - \deg(\text{row}~i)$. Therefore, the degree of the elements in each column is monotonically increasing with row. We may eliminate non-zero elements below pivots using row operations that do not change the pivot elements or the degrees of the row basis elements. We then place the matrix in diagonal form with row and column swaps. $\square$ How is it possible to do row operations WITHOUT altering the degree and keeping a homogeneous basis element? Am I missing something obvious? Note that this proof shall hold for any such $\partial$ where the degree of the row + degree of element is equal to the degree of the column. Another example would be 
$$\begin{pmatrix} * & ab \\ a & t \\ b & t^2 \end{pmatrix}$$
where $\deg(ab) = 3, \deg(a) = 2, \deg(b) = 1$. How would even that be possible... What we are really interested in is the image of $\partial$. This becomes $$\deg(d)tF(t)\oplus \deg(c)F(t)\oplus \deg(b)tF(t)$$ according to the statement and matrix above. I do, however, believe that the result is true and I think I can give a proof for it: Assume that we have column-echelon-form. Then the degree of the homogenous elements along each column increases as we go top rown down. We may also assume that along each row the degree of the pivot element is greater than the other elements on the row, if not, use column operations to remove the element with greater degree. This gives us a matrix of the form (assuming just 2 elements for simplicitiy) $$ \begin{pmatrix} * & m_1 & m_2 \\ n_1 & t^{\beta_1^1}  & 0 \\ n_2 & t^{\beta_2^1} & t^{\beta_2^2} \end{pmatrix}$$
where $\beta_2^2 \geq \beta_2^1 \geq \beta_1^1$. Then use $n_1$ to remove first coordinate of $n_2$. The image then becomes: $$m_1 \to t^{\beta_1^1}n_1\cdot f(t)$$ where $f(t) \in F(t)$. 
$$m_2 \to t^{\beta_2^2}(n_2 - n_1\cdot t^{\beta_2^1 - \beta_1^1})g(t)$$ where
$g(t) \in F(t)$. Writing in terms of the basis elements of the codomain we have image equal to $$n_1(t^{\beta_1^1}f(t) - t^{\beta_2^2 +\beta_2^1-\beta_1^1}g(t))$$
$$n_2t^{\beta_2^2}$$ and since $\beta_2^2 \geq \beta_1^1$ we have that this is isomorphic to
$$\deg(n_1)t^{\beta_1^1}F(t)\oplus \deg(n_2)t^{\beta_2^2}F(t)$$
and the proof generalizes in an obvious way to higher dimensions. Non-pivot rows are skipped. Source: http://comptop.stanford.edu/preprints/persistence1.pdf Chapter 4.1","['matrices', 'algebraic-topology']"
47471,"Four men, hats and probability","I encountered the four men in hats puzzle for the first time today. My question is about a realisation I (think I) had while arriving at the solution, but I have no idea whether I've made a mistake somewhere. Before I got to the actual answer, I was thinking about each man's chances of correctly guessing his hat colour. My thought process went something along these lines: At first blush, D has the best chances, as he can see the most. Hang on - D has only a 1 in 2 chance, the same as anyone else. Anyone else? No - C can see that B has a white hat, so he knows there's only one other. It can be on one of three heads, so if he guesses that his own is black, his chances of being right are 2 in 3. What threw me about this, assuming that last realisation isn't flawed in some way, is that D can also see that B has a white hat. Somehow, C has a better chance of being right, even though D (in some sense) has more knowledge, at least for this configuration of hats. What am I missing? UPDATE Thanks for the answers so far. I don't think I was very clear on what I was asking though: I did manage to solve the puzzle, but what confused me was the realisation I outlined above while getting there. Forgetting the original goal of the puzzle (how do the men reason about the solution) and only taking into consideration their chances of successfully guessing , how do I explain the fact that C's chances seem better than D's, even though D can see everything that C can, and more?","['puzzle', 'intuition', 'probability']"
47477,Number of occurrences of the digit 1 in the numbers from 0 to n,We have a function: $f(n)$ = number of occurrences of the digit $1$ in the numbers from $0$ to $n$. Example: $f(12) = 5$ It is obvious that $f(1)=1.$ Question: Which is the next number for which $f(n) = n$?,['sequences-and-series']
47481,Questions on scheme morphisms,"I have some questions on scheme morphisms. I ask pardon for posting them in one thread as they are most likely not worth to be distributed into several threads. Let $X=Spec R$ be a noetherian scheme. For a maximal ideal $m$ of $R$ one has a morphism $Spec (k(m))\to X$ induced by the ring homomorphism $R\to R_m\to R_m/R_mm=R/m$ which is a closed immersion. It maps the only point of $Spec (k(m))$ to $m$. If one takes a prime ideal $p$ instead of a maximal ideal, there is a scheme morphism induced by $R\to R_p\to R_p/R_p p=Quot(R/p)$. Is the image of the only point $(0)$ of $Spec (Quot(R/p))$ the generic point of the irreducible subscheme of $X$ correspoinding to $p$? For a prime ideal $p$ one has, as above, scheme morphisms
$$
Spec(Quot(R/p))\xrightarrow{g_p} Spec(R_p)\xrightarrow{f_p} X
$$
and if $p$ is maximal $Spec(Quot(R/p))$ can be thought of as a point. What is $Spec(R_p)$ geometrically? Is $f_p$ or $g_p$ respectively an open/closed immersion? How can the image of $f_p$ or $g_p$ respectively be thought of? If one has a morphism $f:Y\to X$, the base change of $f$ by $Spec (k(m))\to X$ is the fiber of $f$ at the point $m$. Is the base change of $f$ by $Spec(R_m)\to X$ a kind of 'thickened fiber'? In how far can I reconstruct the morphism $f$ if I know what it does on the topological spaces and if I know all the fibers or 'thickened fibers' respectively?","['commutative-algebra', 'affine-schemes', 'algebraic-geometry']"
47486,Matrix Representation of Inner Product,"In my readings I have on several occasions encountered references to a linear algebra theorem that runs as follows: Let $g$ be a non-degenerate inner product on the real vector space $V$. Then, there exists a basis $e_1, \dots, e_n$ such that the matrix of $g$ is diagonal and whose diagonal entries are all either $-1$ or $1$ Despite having encountered this result several times, unfortunately, I have not had the luck of finding a precise reference to where this theorem is stated/proved. Can anyone provide a reference that proves this claim?","['linear-algebra', 'reference-request']"
47494,Magical Loaded Dice and expected values,"Let's say we have a magical biased die that allows us to set the probability of every side to whatever we want (nonzero and add up to 1, of course).  And let's say this die enforces the condition that the value of every roll must be between 1 and previousRoll + 1 . What would we set the bias on the sides to be to keep the expected value (mean over a large number of rolls) at 3.5, the same as an unbiased non-magical die? More interestingly, what if the die has an arbitrary number of sides?  What if it's output is continuous?","['statistics', 'dice', 'probability', 'probability-theory']"
47497,Computing Curvatures,"What are some manifolds other than products of space forms for which the various curvature quantities can be computed easily? I'm interested in odd (real) dimensions just as much even, so I'd like to stay away from complex manifolds.","['riemannian-geometry', 'differential-geometry']"
47507,If $\int_0^\infty {e^{-\lambda t}f(t){\rm d}t} = 0$ for all $\lambda >0$ then $f=0$ a.e.?,"I am studying a paper in which the author uses something like that: Let $f$ be a bounded and Lebesgue measurable function. If $$\int_0^\infty {e^{-\lambda t}f(t)\,{\rm d}t} = 0 
\qquad\text{for all} \qquad \lambda \gt0$$ then $f=0$ almost everywhere. Could you point me to a proof of this theorem? It is very important for me. Thanks!","['integration', 'real-analysis']"
47509,"""Counting Tricks"": using combination to derive a general formula for $1^2 + 2^2 + \cdots + n^2$","I was reading an online article which confused me with the following. To find out $S(n)$, where $S(n) = 1^2 + 2^2 + \cdots + n^2$, one can first write out the first few terms: 0 1 5 14 30 55 91 140 204 285 Then, get the differences between adjacent terms until they're all zeroes: 0 1 5 14 30 55 91 140 204 285 1 4 9 16 25 36 49 64 81 3 5 7 9 11 13 15 17 2 2 2 2 2 2 2 all zeroes this row Then it says that therefore we can use the following method to achieve $S(n)$: $S(n) = 0 {n\choose 0} + 1 {n\choose 1} + 3 {n\choose 2} + 2 {n\choose 3}$. I don't understand the underlying mechanism. Someone cares to explain?",['combinatorics']
47520,Expectation of an event,"Let $A$ be an array of length 1000 with all entries 0. I want to fill up $A$ with ones using the following approach: At each iteration I take three random integers $(j_1,j_2,j_3)$ from [1,1000] with replacement and do the following: Set $A[j_1]=1$ 2a. If $A[j_2]=1$ and $A[j_3]=0$, then set $A[j_3]=1$ 2b. if $A[j_2]=0$ and $A[j_3]=1$, then set $A[j_2]=1$ 2c. If $A[j_2]=A[j_3]=0$, do nothing What is the expected number of such trials to fill up A with all ones? With replacement means $j_2$ may be equal to $j_3$ or any previous $j_2$ etc.",['probability']
47526,How many little balls can fit in a container?,"Today, I went to grocery store (named H-E-B) and I got a irresistible offer. Buy 2 Nesquik cereal boxes and get a scholar kit (pencils, erasers, crayons, etc..) with the theme of the movie Kung Fu Panda 2 and also have the opportunity to get an iPad 2 if I win this little game: Having $n$ players and a rectangular transparent box with measures $w$ for width, $l$ for length and $h$ for height find out how many balls (spouse balls are spherical) with radio $r$ are in the box if the box is full of balls. Each player have to give a guess, and wins the player who give the most correct answer. The correctness is giving in this way $|PlayerAnswer - RealAnswer|$, You win automatically if your correctness is $0$. Then my question is: Does anyone have a good approach to solve this problem? Special note: In the real game you don't know the measures. Update : Video related","['geometry', 'packing-problem']"
47532,Sample: don't confuse measurements with actual values?,"In Wikipedia's article on Sample there is the following remark: ''Note that a sample of random variables (i.e. a set of measurable functions) must not be confused with the realizations of these variables (which are the values that these random variables take). In other words, $X_i$ is a function representing the measurement at the $i$-th experiment and $x_i = X_i(ω)$ is the value we actually get when making the measurement.'' I'm afraid I don't understand this passage, can anybody please explain the point?","['probability-theory', 'sampling', 'probability']"
47534,How to prove Campanato space is a Banach space,"Let $p\ge1$, $\mu\ge0$ and $\Omega$ be a bounded open set in $\mathbb{R}^n$. Campanato space embraces all $u$'s which $$[u]_{p,\mu}=[u]_{p,\mu;\Omega}=\sup_{\substack{x\in\Omega\\0<\rho<\mathrm{diam}\Omega}}\left(\rho^{-\mu}\int_{\Omega_\rho(x)}\left|u(y)-u_{x,\rho}\right|^p\,dy\right)^{\frac{1}{p}}<+\infty,$$
 where $\Omega_\rho(x)=\Omega\cap B_\rho(x)$ ($B_\rho(x)$ denotes a ball centered at $x$ with a radium $\rho$) and
$$u_{x,\rho}=\frac{1}{\left|\Omega_\rho\right|}\int_{\Omega_\rho(x)}u(y)\,dy, $$ equipped with a norm
$$\|u\|_{L^{p,\mu}}=\|u\|_{L^{p,\mu}(\Omega)}=[u]_{p,\mu;\Omega}+\|u\|_{L^p(\Omega)}.$$ Let $\{u_k\}$ be a Cauchy sequence in Campanato space, one can determine a $u$ because of the completeness of $L^p$ space. What are the next steps to prove that $u$ is also the right limit of $\{u_k\}$ in the sense of $\|\cdot\|_{L^{p,\mu}(\Omega)}$ and therefore Campanato space is complete? Thank you~","['functional-analysis', 'banach-spaces', 'analysis']"
47541,Set of all compact operators $K(H)$ is the unique ideal in $B(H)$?,I want to show that the set of all compact operators $K(H)$ is the unique ideal in $B(H)$. Is there any relation between invertibility and compactness of an operator?,"['compact-operators', 'operator-theory', 'ideals', 'hilbert-spaces', 'functional-analysis']"
47543,Getting the inverse of a lower/upper triangular matrix,"For a lower triangular matrix, the inverse of itself should be easy to find because that's the idea of the LU decomposition, am I right? For many of the lower or upper triangular matrices, often I could just flip the signs to get its inverse. For eg: $$\begin{bmatrix}
1 & 0 & 0\\ 
0 & 1 & 0\\ 
-1.5 & 0 & 1
\end{bmatrix}^{-1}=
\begin{bmatrix}
1 & 0 & 0\\ 
0 & 1 & 0\\ 
1.5 & 0 & 1
\end{bmatrix}$$
I just flipped from -1.5 to 1.5 and I got the inverse. But this apparently doesn't work all the time. Say in this matrix: 
$$\begin{bmatrix}
1 & 0 & 0\\ 
-2 & 1 & 0\\ 
3.5 & -2.5 & 1
\end{bmatrix}^{-1}\neq 
\begin{bmatrix}
1 & 0 & 0\\ 
2 & 1 & 0\\ 
-3.5 & 2.5 & 1
\end{bmatrix}$$
By flipping the signs, the inverse is wrong.
But if I go through the whole tedious step of gauss-jordan elimination, I would get its correct inverse like this: $\begin{bmatrix}
1 & 0 & 0\\ 
-2 & 1 & 0\\ 
3.5 & -2.5 & 1
\end{bmatrix}^{-1}=
\begin{bmatrix}
1 & 0 & 0\\ 
2 & 1 & 0\\ 
1.5 & 2.5 & 1
\end{bmatrix}$
And it looks like some entries could just flip its signs but not for others. Then this is kind of weird because I thought the whole idea of getting the lower and upper triangular matrices is to avoid the need to go through the tedious process of gauss-jordan elimination and can get the inverse quickly by flipping signs? Maybe I have missed something out here. How should I get an inverse of a lower or an upper matrix quickly?","['matrices', 'linear-algebra']"
47544,"Double dual of the space $C[0,1]$","The second dual or double dual of the space of all continuous functions on $[0,1]$, $C[0,1]$ is von Neumann algebra. Can anyone help me identifying this space?","['operator-theory', 'operator-algebras', 'functional-analysis']"
47545,A double integral (differentiation under the integral sign),"While working on a physics problem, I got the following double integral that depends on the parameter $a$ : $$I(a)=\int_{0}^{L}\int_{0}^{L}\sqrt{a}e^{-a(x-y+b)^2}dxdy$$ where $L$ and $b$ are constants. Now, this integral obviously has no closed form in terms of elementary functions. However, it follows from physical considerations that the derivative of this integral $\frac{dI}{da}$ has a closed form solution in terms of exponential functions. Unfortunately, my mathematical abilities are not good enough to get this result directly from the integral. So, how does a mathematician solve this problem?","['multivariable-calculus', 'closed-form', 'integration']"
47547,Is there a continuous bijection from $\mathbb{R}$ to $\mathbb{R}^2$,"I need a hint. The problem is: is there a continuous bijection from $\mathbb{R}$ to $\mathbb{R}^2$ I'm pretty sure that there aren't any, but so far I couldn't find the proof. My best idea so far is to consider $f' = f|_{\mathbb{R}-\{*\}}: \mathbb{R} - \{*\} \to \mathbb{R}^2 - \{f(*)\}$, and then examine the de Rham cohomologies: $$H^1_{dR}(\mathbb{R}^2 - \{f(*)\}) = \mathbb{R} \  \xrightarrow{H^1_{dR}(f')} \ 0 = H^1_{dR}(\mathbb{R} - \{*\}),$$ but so far I failed to derive a contradiction here. Am I on the right path? Is it possible to complete the proof in this way e.g. by proving that $H^1_{dR}(f')$ must be a mono? Or is there another approach that I missed?",['general-topology']
47549,Linear algebra: rank,"Let $A:E\rightarrow F$ be a Linear Transformation between finite dimensional vector spaces, with $\mathrm{Rank}(A)=r$ and $\dim E=n$, $\dim F=m$. Prove that there are basis in $E$ and $F$ such that the matrix of $A$ has $a_{11}=\cdots=a_{rr}=1$ and $a_{ij}=0$ everywhere else, as entries. I thought in the change of basis $ap=qa'$ where $a'$ would be the matrix we want but as I got no information about $a$, $p$, $q$ then this is not a way out definetely. Then as the rank is the maximum number of independent columns and rows I thought I could just erase the ones that are linear dependent but this doesn't guarantee me that the transformation would be the same transformation without the deleted linear dependent columns and rows. A hint would be apreciated, Thanks in advance.",['linear-algebra']
47571,Curvature and connections in principal G-bundles,"Let $E \rightarrow F$ be a principal $G$-bundle. Let $\alpha$ be a connection 1-form with values in the Lie algebra of $G$. Let $\omega$ denote the curvature 2-form of connection $\alpha$. We know that $\omega =0$ everywhere i.e., $\alpha$ is a flat connection if and only if the distribution $\ker (\alpha)=\{v_x \in T_x E |   \alpha(v_x)=0 \ ; x\in E \} $ on $E$ is completely integrable. Now, suppose we have a norm on the space of 2-forms. We start with an ""almost flat connection 
$\alpha$ "" i.e., the curvature form $|\omega| < \epsilon$ everywhere, for sufficiently small $\epsilon$.  Is it true that the distribution $\ker(\alpha)$ on $E$ is ""close"" to a completely integrable distribution? and ""close""  in what sense? I have a feeling that with appropriate notion of ""closeness"" of distributions the above question has an affirmative answer. While I am trying to show that it is so, I have a difficulty deducing any useful information about connection 1-form from bounds on curvature. Thanks in advance for bringing in any new insight. I have posted this question on MO also. https://mathoverflow.net/questions/68794/almost-flat-connections-on-principal-g-bundles","['curvature', 'differential-geometry']"
47582,Why is it that Complex Numbers are algebraically closed?,"I find it curious that Complex Numbers give enough flexibility to be algebraically closed, where the reals, rational numbers do not.  For the reals it is easy to see that they cannot be used to solve equations like $x^2 + 1 =0$.  Geometrically, one can look at the number line as see that any $x$ squared yields a positive number which when added to one cannot get you back to zero.  In the complex case, however, we are working with the plane.  In this case exponents stretch and rotate any given $x$.  It is easy to therefore see in the particular circumstance that if $x=i$ that $x^2$ rotates it to $-1$ which when added to one yields the desired result (i.e. $0$).  So because the Complex Numbers are algebraically closed, I conclude that any polynomial equation with complex coefficients my be solved by choosing one or more $x$'s in the plane and rotating them and stretching them such that they will combine using the given coefficients to produce the RHS. Question: Why is it that we do not need a larger space than the plane to solve Complex polynomial equations? I have tried to find a sufficient answer through Google, but was not able.  I also searched M.SE and could not find a sufficient answer.  I am not a mathematician, so I am looking for an intuitive answer if possible.","['complex-numbers', 'intuition', 'abstract-algebra']"
47594,Plane intersecting line segment,"I have a plane which is represented as a 3d point $\vec{p}$ with a normal $\hat{n}$. I also have a line segment specified by two points $\vec{v_1},\vec{v_2}$ . I want to get the intersection point (if any). Here's what I have: $${\rm dist}_{v_1} = \hat{n} \cdot (\vec{v_1} - \vec{p})$$
$${\rm dist}_{v_2} = \hat{n} \cdot (\vec{v_2} - \vec{p})$$ If ${\rm dist}_{v_1}\cdot {\rm dist}_{v_2} \leq 0$ then I know there is an intersection because the distances are signed and the product of numbers with opposite signs is negative. Zero is included to cover the case when an endpoint is exactly on the plane. Then: $$\hat{x} = \frac{\vec{v_2} - \vec{v_1}}{\left|\vec{v_2} - \vec{v_1}\right|}$$
$$\cos\theta = \hat{n}\cdot\hat{x}$$ Now I test $\cos\theta$. If zero then I choose one (of both) the endpoints as the intersection point. If non-zero I proceed to find the intersection point $\vec{v}$:
$$\vec{{v}} = \vec{v_2} - \hat{x}({\rm dist}_{v_2} / \cos\theta)$$ I'd like to know if my solution can be reworked to be more ""elegant"" without the last check of $\cos\theta$ being non-zero to avoid a divide-by-zero?","['geometry', '3d']"
47600,"Where is the name ""coset"" in group theory from?","One of the most important application of "" coset "", I think, is to prove the Lagrange's theorem , which was not originally stated in the group theoretic terms. In some textbooks I  have read about abstract algebra, I didn't find any history about ""coset"". Here are my questions : Where is the concept ""coset"" from? And what was it originally used for?","['terminology', 'math-history', 'group-theory', 'abstract-algebra']"
47603,What is a good number theoretic interpretation of primitive geodesics on the modular surface?,"Given $SL_2( \mathbb{Z})$, what interpretation is available for the hyperbolic elements? What is true, if we consider a congruence subgroup? I heard that there is a connection with certain class numbers.",['number-theory']
47608,Mathematical description of a random sample,"Mathematical description of a random sample: which one is it and why? $X_1(\omega), X_2(\omega), ..., X_n(\omega)$, where $X_1, ..., X_n$ are different but i.i.d. random variables. $X(\omega_1), X(\omega_2), ..., X(\omega_n)$, where $X$ is a (single) random variable.","['probability-theory', 'random-functions', 'probability']"
47618,Definition of the gradient for non-Cartesian coordinates,"The gradient of a function $f: \mathbb{R}^n \to \mathbb{R}$ is defined as the vector of the partial derivatives: $$ \nabla f = \left(\frac{\partial f}{\partial x_1}, ..., \frac{\partial f}{\partial x_n}\right)$$ Recently, I have become somewhat confused over this definition since I realized that if, for example, $f$ is defined in spherical coordinates $(r, \theta, \phi)$ , the gradient is given as $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{1}{r} \frac{\partial f}{\partial \theta}, \frac{1}{r \sin \theta} \frac{\partial f}{\partial \phi} \right)$$ rather than $$ \nabla f = \left(\frac{\partial f}{\partial r}, \frac{\partial f}{\partial \theta}, \frac{\partial f}{\partial \phi} \right)$$ I have two questions regarding this: Is a scalar-valued function in spherical coordinates still considered to be  $f: \mathbb{R}^3 \to \mathbb{R}$, or is $\mathbb{R}^3$ reserved for Cartesian coordinates? Does the ""partial derivative"" definition of the gradient in fact require Cartesian coordinates?",['multivariable-calculus']
47625,Linear Algebra: determine whether the sets span the same subspace,"So I am stuck on 51 here: 51. Determine whether the sets $S_1$ and $S_2$ span the same subspace of $\mathbb{R}^3$:
  $$\begin{align*}
S_1 &= \Bigl\{ (1,2,-1),\ (0,1,1),\ (2,5,-1)\Bigr\}\\
S_2 &= \Bigl\{ (-2,-6,0),\ (1,1,-2)\Bigr\}
\end{align*}$$ What I did to solve it was to multiply each vector in set $S_1$ by $C$, add the vectors together and set them to zero. Once I reduced the resulting matrix I got the following result:
$$\begin{align*}
c_1&=-2t\\
c_2&=-t\\
c_3&=t
\end{align*}$$ So this result would be linearly dependent. Then I did the same thing for the second set and I got that $c_1=c_2=0$ which means that it is linearly independent. In conclusion, I said that set $S_1$ and $S_2$ does not span the same subspace of $\mathbb{R}^3$. The book says they do. Could anyone point out where I went wrong? Thanks.","['vector-spaces', 'linear-algebra']"
47627,Solving a simple recurrence relation,"I have the following recurrence relation: $a_0=1$ $a_{n}=pa_{n+1}+qa_{n-1}$ Where $p+q=1$. This relation arises in analyzing a ""gambler's ruin"" situation. It is claimed that the general solution is $A+B(q/p)^i$ but I fail to see why (trying the usual method of solving the characteristic equation does not seem to work for me). Also, and this is maybe even more interesting to me - what is the solution if the relation is finite, i.e. if we have $a_{k}=a_{k-1}$ for some $k$ and onwards?","['recurrence-relations', 'probability']"
47644,Various definitions of group action,"Sorry for the long post but this is a personal piece of maths, and I needed to be more precise as possible. There exists a well known equivalence between the category of $G$-sets and the category of functors $Fun(G,\mathbf{Sets})$ (viewing $G$ as a category with a single object $*$): given a set $X$ just consider the unique functor sending $*$ into $X$; functoriality determines the well known classical permutation representation $G\to Sym(X)$, which easily leads to the equivalent notion of an action as a map $G\times X\to X$ with suitable properties. It is also well known that the notion of ""action"" of a ""group object"" can be stated in any category with finite products and a terminal object (say, $1$). I would like to relate the two notions: I started noticing that the first relation lacks of elasticity, being formulated in the particular case of the category of sets. My first task is hence to generalize the notion of action of a group on some $X$, object in $\mathbf C$ (category w. finite products and a terminal object). It seems to me that I can define it as a functor $G\to \mathbf C$ sending $*$ into $X$: Functoriality allow me to think that any $g\in G$, seen as an isomorphism $g\colon *\to *$, corresponds via $F$ to an isomorphism in $\mathbf C$, then $G$ is related to a subset(subgroup) in $\hom_\mathbf C(X,X)$. So, it seems we recovered the notion of permutation representation in this more general context. Futhermore, it seems to me that when we consider, say, a functor $F\colon G\to \mathbf{Top}$ ($G$ a group in that category, i.e. a topological group), mere functoriality allows me to say that $G\times X\to X$ is a continouos map (similarly consider then the subcategory of $\mathbf{Top}$ made by differentiable manifolds, $G$ is then a Lie group and the action a smooth map). Can we go any further? Moerdijk defines the action of a groupoid on a space $\mathbf G=(s,t:G_\text{mor} \to G_\text{ob})$ as an arrow $\mu\colon G_\text{mor}\times_{G_\text{ob}} E\to E$ with suitable properties. This is partly similar to an idea I got yesterday, thinking to a suitable way to expand the notion of $G$ acting on something. In a few words, I start identifying $G$ and $\hom(*,*)$, groups in set-theoretic sense wrt the composition. Say that an action of $G$ on an object $X$ in $\mathbf C$ is a function $\hom(*,*)\times \hom(1,X)\to \hom(1,X)$ which is an action in the classical set-theoretic sense: is it formally correct? Can this approach be applied in a ""real case""? Can you provide me a reference for the definition I gave of Moerdijk groupoid-action, which I read dunno-where on MO? Thanks everybody.","['category-theory', 'group-theory']"
47648,Derivative of inverse quadratic function of a matrix,"I have been stuck with the following derivative for some time:
$$
\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}}
$$, where $\mathbf{b}\in\mathbb{R}^{M\times1}$, $\mathbf{X}\in\mathbb{R}^{M\times N}$ and $\mathbf{C}\in\mathbb{R}^{N\times N}$ and $\mathbf{C}$ is symmetric. I had a look in the Matrix Cookbook , but I am still not sure how to deal with the inverse of a matrix in the second order form. Is it correct to apply the chain rule?
$$\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{X}} = 
\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}}\cdot 
\frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}.$$ In this case, the first partial derivative will be: 
$$
\frac{\partial\,\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-1}\mathbf{b}}{\partial\,\mathbf{XCX}^\mathrm{T}} = 
-(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^\mathrm{-T}\mathbf{b}\mathbf{b}^\mathrm{T}(\mathbf{X}\mathbf{C}\mathbf{X}^\mathrm{T})^{-\mathrm{T}}
$$ 
(using Eq. 55, from 1 ). The second part, $\frac{\partial \, \mathbf{XCX}^{\mathrm{T}}}{\partial \, \mathbf{X}}$, will be similar to a fourth-rank tensor. How can I arrive at a result that is a $M\times N $ matrix? I would really appreciate if someone could help me with this or provide some piece of advice.","['matrices', 'linear-algebra']"
47663,Avoidance of two permutations,"Let $1 \leq m \leq n, \sigma \in S_n, \pi \in S_m$. The permutation $\sigma$ avoids $\pi$ if no subset $\{j_1 < \cdots < j_m\} \subseteq \{1,\cdots,n\}$ exists, so that for all $1 \leq i < l \leq m$ applies
    $$\sigma(j_i) < \sigma(j_l) \Leftrightarrow \pi(i) < \pi(l)$$ Prove that if a permutation $\sigma \in S_n$ avoids the permutation 123 it avoids the permutation 3124 as well (one-line-notation used for permutations). Hi! I first reflected on what the definition of the avoiding implies for $\sigma$ (apart from $|3124| = m = 4 \Rightarrow n \geq 4$). The only fact I discovered is that according to the definition $\sigma$ contains one ascension at most and this should ""cover"" the last part $124$ of $3124$. Could you please help me to go on? Thanks in advance!","['permutations', 'discrete-mathematics']"
47693,A question about prime elements in integral domains,"I have to show the following: Let $p \in R\setminus\{0\}$ then: $p$ is prime element in $R$ if and only if $(p)$ is a prime ideal in $R$. I have real problems doing so. I tried the following: $\Rightarrow$ let $p$ be a prime element in $R$, then we know that if $p \mid ab$ then $p\mid a$ or $p\mid b$. Also, we know that $$(p) = Rp = \{ rp \mid r \in R\},$$ then we know that $(p)$ is prime ideal because $rp \in (p) \Rightarrow p \in (p)$. Now how do i show that $(p)\neq R$. Also, the last step feels strange, as this seems to imply that $(a)$ is prime ideal for any $a\in R$ if $(a) \neq R$? with the $\Leftarrow$ direction I do not know how to start, could I get any hints? Thanks!",['abstract-algebra']
47700,"Algebraic geometry ""20 questions""","I'm trying to see if it's possible to do an ""algebraic geometry 20-questions game"" On an index card there is printed the equation for some algebraic
variety $W$, in this case, let's say it's the zero-set of $x^{7}y^{3} - y^{7}z^{3} + z^{7}x^{3} = 0$. In the setup of this game, there are three sorts of questions: Allowed questions: what is the number of rational points on the surface? What are the homology/cohomology/homotopy groups of the surface? In general, these questions are about some property of the algebraic images of $W$. These questions are encouraged in the context of the game. What is the Kodaira dimension of $W$? They do not have to be yes/no questions. Not allowed questions: ""Is some point $(x,y,z,w)$ a part of this surface?"" (one could ask this many many times and build up a picture of the surface) Discouraged questions: ""Is $W$ given by the zero-set of $x^{7}+y^{4}z^{8}-xzw^{5}=4$?"" (I say discouraged because the point of this exercise is not to brute force an answer, but questions like this are appropriate at the end, when the answer could be yes) The goal of the game is to determine what $W$ is explicitly (or, more generally, the variety that the asker has in mind), or as Zev puts it ""Is there a finite list of invariants of a variety that determine it completely?"" If such a game is possible, could someone run through a hypothetical transcript of one? (or, to up the level of abstraction: what strategy would you use to play it?) If such a game is not possible, please explain why not. EDIT: Clarification: I could have asked ""There is an unknown variety $W$: and all that can be determined about it are its invariants, can we tell explicity what sort of variety it is?"", but ""all that can be determined"" is somewhat arbitrary, so I used the frame of a game to provide a reason that there would be  limits to the information available about the variety in question. I'm more interested in the machinery of algebraic geometry that would provide strategies for reducing the number of questions a player would need to ask to determine the variety in question than special cases that reduce to ""I'm thinking of a number"".  In the case of a twenty questions style game: there is an explicit algebraic variety that the one player has in mind, and the other players here need some reasonable strategy for determining what sort of variety it is. (asking a countably infinite number of questions is not an option.)",['algebraic-geometry']
47703,"How common is the use of the term ""primitive"" to mean ""antiderivative""?","I don't know if this should actually be asked on the English stackexchange.  It seemed like I would find better answers here. I have all but finished an undergraduate degree in mathematics in the United States, but I have never once heard the term ""primitive"" to mean ""antiderivative"" until recently, when someone from Europe pointed it out to me.  According to him, it's a common term there.  So I was wondering if people could give me an idea of how common this term is, and where.  I know for sure that if someone says ""primitive"" to a math student in the US, that the student won't know what he is talking about. Does the reverse hold for ""antiderivative"" (or the also common ""integral"") elsewhere?","['terminology', 'analysis']"
47708,"Evaluating $\int_{1}^{\infty}\exp(-(x(2n-x)/b)^2)\,\mathrm dx$","$$I_1=\int_1^{\infty}\exp\left(-\left(\frac{x(2n-x)}{b}\right)^2\right)\mathrm dx,$$ 
I set 
$$t=\frac{x(2n-x)}{b},$$ 
and, solving for $x$ and $dt$ I got 
$$I_1=\frac{b}{2 n} \int_1^{\infty} e^{-t^2}\left(1-\frac{bt}{n^2}\right)^{\frac12}\mathrm dt.$$ I then expand 
$$\left(1-\frac{bt}{n^2}\right)^{\frac12} \approx 1+\frac{bt}{2n^2}$$ 
in the first two terms of Binomial series, and obtain something like 
$$I_1 \approx \frac{b}{2n}\left(\frac{\sqrt{\pi}(1-\mathrm{erf}(1))}{2}+\frac{b}{4n^2e}\right).$$ I am not 100% sure about this derivation (although the result is sensible), especially about the substitution, it seems I could misused integration of Gaussian function here.","['improper-integrals', 'calculus', 'integration', 'special-functions', 'definite-integrals']"
47717,Notation for a certain kind of discrete measure,"Suppose $\phi:\mathbb{R}^n \rightarrow \mathbb{R}$ is smooth, $Z=\{x: \phi(x)=0\}$ and $D\phi\neq0$ on $Z$. Is anyone familiar with use of the notation $dZ$ for the measure 
 $$\sum_{x \in Z} |D\phi(x)|\delta_x,$$
where $\delta_x$ is the Dirac measure at $x$? If so, can you explain it?","['multivariable-calculus', 'measure-theory', 'calculus']"
47739,Laurent series for an infinite sum of functions,"I have an infinite sum of analytic functions that is guaranteed to converge for every $x$, except for $x=0$:
\begin{equation}
g(x) = \sum_{n=1}^\infty f_n (x)
\end{equation}
I want to expand the function $g(x)$ in a Laurent series around $x=0$. For example if $f_n(x)=e^{-nx^2}$, then:
\begin{equation}
g(x)=\frac{1}{e^{x^2}-1} \approx \frac{1}{x^2}-\frac{1}{2}+\frac{x^2}{12} ...
\end{equation}
The problem is that in general I don't have a closed-form expression for $g(x)$. However, I think I should still be able to obtain a closed-form expression for the coefficients of the Laurent series. Taylor-expanding $f_n(x)$ is not good enough, since the terms of the Taylor series do not necessarily converge in $n$ (in the above example they don't).","['sequences-and-series', 'convergence-divergence', 'functions', 'taylor-expansion']"
47748,Fractions with radicals in the denominator,"I'm working my way through the videos on the Khan Academy, and have a hit a road block. I can't understand why the following is true:
$$\frac{6}{\quad\frac{6\sqrt{85}}{85}\quad} = \sqrt{85}$$","['fractions', 'arithmetic', 'algebra-precalculus']"
47755,Eigenvectors of a normal matrix,"According to the spectral theorem every normal matrix can be represented as the product of a unitary matrix $\mathbb{U}$ and a diagonal matrix $\mathbb{D}$
 $$\mathbb{A} = \mathbb{U}^H\mathbb{D}\mathbb{U}$$
meaning that every normal matrix is diagonalizable. Does it necessarily mean that the unitary matrix has to be composed from the eigenvectors of $\mathbb{A}$ ? 
I presume that not, because then the eigenvectors of every normal matrix would form an orthonormal set (rows and columns of a unitary matrix are orthonormal in $\mathbb{C}^n$). So am I right that only the set of eigenvectors of a hermitian (or symmetric while in $\mathbb{R}^n$) matrix is orthonormal?","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
47761,Fermat's Last Theorem: implications (there is no new proof),"I am not experienced in Number Theory but what I know is that some results of this filed are applicable in other areas, e.g. algebra. For sure FLT made (and makes) people be interested in Number Theory leading to the development of new methods which can be themselves applied not only for the proof of FLT (like financial problems motivated somehow development of stochastic analysis). I am interested - if there are applications or implications of FLT itself? More precisely: if the fact ""for each $n\geq3$ there are no integer solutions of $a^n+b^n=c^n$"" leads to solutions of problems which are not in the field of Number Theory? I would specify that I wonder about some problems which are already formulated: since FLT is known for more then 300 years I am pretty sure that there were formulated hypothesis which follow from FLT directly (if there are such hypothesis not in the field of Number Theory).","['math-history', 'number-theory']"
47764,Can a real symmetric matrix have complex eigenvectors?,"A Hermitian matrix always has real eigenvalues and real or complex orthogonal eigenvectors. A real symmetric matrix is a special case of Hermitian matrices, so it too has orthogonal eigenvectors and real eigenvalues, but could it ever have complex eigenvectors? My intuition is that the eigenvectors are always real, but I can't quite nail it down.","['eigenvalues-eigenvectors', 'symmetric-matrices', 'matrices', 'linear-algebra', 'complex-numbers']"
47772,"Definite integral: $\int^{4}_0 (16-x^2)^{\frac{3}{2}}\,dx$","The following integral can be computed using the substitution $x = 4\sin\theta~$ and then proceeding with $dx = 4\cos\theta~ d\theta~$ , and evaluating the integral of $\cos^4\theta$ : $$\int^{4}_0 (16-x^2)^{\frac{3}{2}}\,dx$$ However, how would one approach this, using integration by parts or otherwise, to exploit the property of $\displaystyle\int^{4}_0 \sqrt{16-x^2}\,dx$ simply being a circle quadrant of radius $4$ units? Note: The final answer is $48\pi$ .","['trigonometry', 'calculus', 'circles', 'integration']"
47778,Understanding and using the transfer-matrix-method,"Let $G = (V,E,\Phi)$ be a weighted directed graph and $\mathcal{W}' : E \rightarrow \mathbb{C}$ the weighting. Let additionally $m = \# V$, $E_m$ the $m \times m$ identity matrix. Let $v,w \in V$ be in a fixed order in $V$ so $v$ is the i -th and $w$ the j -th element of $V$. Then applies $$f_{vw}(x) = (-1)^{i+j} \det((E_m - xA)^{(j,i)}) / \det(E_m-xA).$$ Example : Let $L$ be the set of all words over the alphabet $\Sigma = \{a,b\}$ that no not contain ""bb"". The following unique finite state-machine with the start state $S = q_0$ and final states $T = \{q_0,q_1\}$ accepts exactly these language: we now use a weighting $W'(e) = 1$. The matrix of the graph is given by
  $$\mathcal{A} = \left(
\begin{array}{cc}  1 & 1 \\
                   1 & 0 
\end{array}
\right) 
$$
  while the first row and column relate to the node $q_0$ and the second row and column to the node $q_1$. Then applies for $f_L(x) = \sum_{n \geq 0} \sum_{w \in L \atop |w| = n} x^n$ that $f_L(x) = f_{q_0q_0}(x)+f_{q_0q_1}(x)$.
  Because of $$\det(E_2-Ax) = (1-x)-x^2$$  we get using the the transfer-matrix-method (see definition above) $$f_{q_0q_0}(x) = (-1)^{1+1} \det((E_2-Ax)^{(1,1)})/\det(E_2-Ax) = 1/(1-x->x^2).$$ 
  $$f_{q_0q_1}(x) = (-1)^{1+2} \det((E_2-Ax)^{(2,1)})/\det(E_2-Ax) = (-1) \cdot (-x)/(1-x-x^2).$$ 
  Therefore applies
  $$f_L(x)=f_{q_0q_0}(x) +f_{q_0q_1}(x) = \frac{1+x}{1-x-x^2}.$$ Exercise Let $g_n$ be the amount of words of the length $n$ over the alphabet $\Sigma = \{a,b,c\}$ that do not contain $ab,ac,bc$ or $ba$. Use the transfer-matrix-method. (a) Prove that $\sum_{n\geq 0} g_n t^n = \frac{1+t}{(1-t)^2}$ (b) Identifiy an explicit formula for $g_n, n \geq 0$ Hi! Sorry for the long introduction, but I just was not sure if your definitions and conventions match with the ones I have to use. We didn't get more information about the ""transfer-matrix-method"" then I wrote above, and I still don't get it completely. I created the finite state machine for the given language as The matrix of the corresponding graph according to the example would be 
$$\mathcal{A} = \left(
\begin{array}{ccc}  1 & 1 & 0 \\
                   1 & 1 & 1 \\
1 & 0 & 1
\end{array}
\right) 
$$. $\begin{eqnarray*}
f_{q_0 q_0}(x) &=& (-1)^{1+1} \det((E_m - xA)^{(1,1)}) / \det(E_m-xA) \\
&=& \det \left(\left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right)
- \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right)^{(1,1)}
\right) \\ && / \det \left( \left(\begin{array}{ccc} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1\end{array} \right)
- \left(\begin{array}{ccc} x & x & 0 \\ x & x & x \\ x & 0 & x\end{array} \right)\right) \\
&=& \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}
 \right)^{(1,1)} \\ &&/ \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}
 \right) \\
 &=& \det \left( \begin{array}{ccc} (1-x) & (-x) \\ 0 & (1-x) \end{array} \right) / \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}
 \right) \\
 &=& (1-x)^2 / ((1-x)^3 + (-x)^3 - (-x)(-x)(1-x)) \\
 &=& \frac{(1-x)^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\
 f_{q_0 q_1}(x) &=& (-1)^{1+2} \det((E_m - xA)^{(2,1)}) / \det(E_m-xA) \\
 &=& (-1) \det \left( \begin{array}{ccc} (1-x) & (-x) & 0 \\ (-x) & (1-x) & (-x) \\ (-x) & 0 & (1-x) \end{array}
 \right)^{(2,1)}  / \det(E_m-xA) \\
  &=& (-1) \det \left( \begin{array}{cc} (-x) & 0 \\ 0 & (1-x)\end{array}
 \right)^{(2,1)}  / \det(E_m-xA) \\
 &=& (-1) (-x)(1-x) / -x^3 -x^2 (1-x)+(1-x)^3 \\
 &=& \frac{(1-x)x}{-x^3 -x^2 (1-x)+(1-x)^3} \\
 f_{q_0 q_2}(x) &=& (-1)^{1+3} \det((E_m - xA)^{(3,1)}) / \det(E_m-xA) \\
 &=&  \det \left( \begin{array}{cc} (-x) & 0 \\ (1-x) & (-x) \end{array}
 \right)  / \det(E_m-xA) \\
 &=& \frac{x^2}{-x^3 -x^2 (1-x)+(1-x)^3} \\
 f_{q_0 q_0} + f_{q_0 q_1} + f_{q_0 q_2} &=& \frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3}
\end{eqnarray*}
$ So $\frac{(1-x)^2 + (1-x) x+ x^2}{-x^3 -x^2 (1-x)+(1-x)^3}$ should the answer to (a), shoudn't it? Unfortunately (according to Wolfram Alpha) it isn't. Is there still anything wrong? Thanks in advance!","['discrete-mathematics', 'automata', 'abstract-algebra', 'matrices', 'linear-algebra']"
47792,How many non-isomorphic binary structures on the set of $n$ elements?,"This question is originated from Fraleigh's Abstract Algebra, Ex3.34 . The exercise is for the case of $n=2$. The answer is 10, and the below is my solution about it. Let the set be $\{{ a,b \}}$. If we let $f$ be the non-identity isomorphism($f(a)=b, f(b)=a$), then 4 binary structures are invariant under $f$: If you set $a*a$ and $a*b$ then the rest are determined since $f(a)*f(a)=b*b$ and $f(a)*f(b)=b*a$. So the number of non-isomorphic binary structures is $4+ \frac {16-4} 2 = 10$. Is there any generalization of this on $n$ elements? It seems a little complicated for me. I tried to find something on google, but I can't find out.","['group-theory', 'abstract-algebra', 'combinatorics']"
47800,Median of a set of Integer,"The median of a set containing odd number of integer is the middle element after sorting. What about the case of even number of terms. Some places mention of average of the two middle value, but I heard that precise definition allows any number from the left middle number to the right middle number. For example, {1 2 3 4}, any number between and including 2 and 3 is valid.",['statistics']
47810,Functor of order $n$ as in Mumford's Abelian varieties,"In section II.6 ""the theorem of the cube I"" of Mumford's ""Abelian Variety"" book, Mumford introduced the notion of functor of order $n$. Here is part of the remark immediately below the statement of theorem. Let $T$ be a contravariant functor on the category of complete varietties into the Category $\underline{\mathrm{Ab}}$ of abelian groups. Let $X_0,\ldots,X_n$ be any system of complete varieties, $x_i^0$ a bse point of $X_i$ and let $\pi_i: X_0\times\ldots\times X_n\to X_0\times\ldots\times \widehat{X_i}\times \ldots X_n$ ($\widehat{X_i}$ indicating the omission of the $i$-th factor $X_i$) be the projection map, and $\sigma_i: X_0\times\ldots\times \widehat{X_i}\times \ldots X_n \to X_0\times\ldots \times X_n$ the inclusion map using the base point. Consider the homomorphisms $$\alpha_T^n: \prod_{i=0}^n T(X_0\times\ldots\times \widehat{X_i}\times \ldots X_n) \to T(X_0\times\ldots \times X_n),$$
$$ \beta_T^n:  T(X_0\times\ldots \times X_n)\to \prod_{i=0}^n T(X_0\times\ldots\times \widehat{X_i}\times \ldots X_n),$$
defined in the natural way. One then prove by an easy induction on $n$ that we have natrual  splitting $$T(X_0\times\ldots \times X_n)=\mathrm{Im}\alpha\oplus \ker \beta.$$
The functor is said to be order $n$ if $\alpha $ is surjective or equivalently, $\beta$ is injective. Now if $T_i, i=1,2,3$ are contravariant functors on complete varieties into $\underline{\mathrm{Ab}}$, and $T_1\to T_2$ and $T_2\to T_3$ are natrual transformations such that $T_1\to T_2\to T_3$ is an exact sequence, and if $T_1$ and $T_3$ are of order $n$ so is $T_2$, as follows from the exactness of $$0=\ker\beta^n_{T_1}(X_0\times\ldots \times X_n)\to \ker\beta^n_{T_2}(X_0\times\ldots \times X_n) \to\ker\beta^n_{T_3}(X_0\times\ldots \times X_n)=0.$$ There are two things that I am having troube with.  First, I have tried a couple times to work out this ""easy"" induction but have not much success so far.  The base case is easy. For $n=2$, I got $\mathrm{Im}{\alpha}\cap \ker\beta=0$ part. But not much else. Secondly, I don't see where the exactness of this sequence involving the $\ker\beta_{T_i}$'s come from. It feels to me that one is claiming that 
if we have a commutative diagram with exact rows, then the kernel of the vertical maps fit into a sequence that's exact in the middle, which is certainly not true, as one can easily take $B'=C'=0$ and produce counter examples. $$\begin{array}{cccc}
A&\to & B & \to &  C\\
\downarrow& & \downarrow & & \downarrow\\
A' &\to & B' & \to &  C'\\
\end{array}$$",['algebraic-geometry']
47821,Choose K items from N in a circle,"In how many ways can we choose k items from n distinct items put in a circle. Then further extension of the question, if I want to find out the number of ways to choose k items from n distinct items in a circle such that no 2 are together. Thanks",['combinatorics']
47835,Proof for law of complex exponents using only differential equation,"I just read that an elegant proof exists that the law of exponents also holds for complex numbers ($a,b,z$ all complex): $$e^{a+b}=e^ae^b,$$ which only uses the definition that $$y=e^{zt}$$ is a solution to $$dy/dt=zy,$$ with initial condition $y(0)=1$, so in particular $e^z=y(1).$ I can only find a proofs which use the trig-representation of complex numbers. Can anybody help? Thank you!","['complex-numbers', 'ordinary-differential-equations', 'complex-analysis', 'exponentiation']"
47837,noetherian induction,"So I think I've misunderstood the principle of Noetherian induction as stated in the Hartshorne exercise II.3.16, or his statement is slightly incorrect. He says: ""Let $X$ be a Noetherian topological space, and let $\mathscr{P}$ be a property of closed subset of $X$. Assume that for any closed subset $Y$ of $X$, if $\mathscr{P}$ holds for every proper closed subset of $Y$, then $\mathscr{P}$ holds for $Y$. (In particular, $\mathscr{P}$ must hold for the empty set.) Then $\mathscr{P}$ holds for $X$."" Why does $\mathscr{P}$ hold for the empty set? What if $\mathscr{P}$ is the property of being nonempty and $X = \varnothing$?","['induction', 'elementary-set-theory']"
47861,Gauss's Theorem vs. Stokes's Theorem?,"What's the difference between Gauss' Theorem and Stokes' Theorem? Does Gauss's Theorem take an integral over an ""inner product"" derivative while Stokes's Theorem takes an integral over an exterior derivative? And is ""divergence"" associated with Gauss's Theorem and ""curl"" associated with Stokes's Theorem? And does ""divergence"" refer to movements of (e.g. fluids) TO (and from) a surface, while curl refers to movements AROUND a surface)?",['multivariable-calculus']
