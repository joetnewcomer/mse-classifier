question_id,title,body,tags
1162824,"Inductive proof, algebra step","I have to prove by induction that 
$$1^2+2^2+3^2+ \cdot \cdot \cdot + n^2 = {n(n+1)(2n+1)\over 6}$$
Base step: $$1^2 = {1(1+1)(2\bullet1 +1)\over 6}$$
$$1^2= {6\over 6} = 1$$
Then I use this  $$1^2+2^2+3^2+ \cdot \cdot \cdot + n^2 +(n+1)^2 = {n(n+1)(2n+1)\over 6}+(n+1)^2$$
to show that
 $$1^2+2^2+3^2+ \cdot \cdot \cdot + n^2 + (n+1)^2 = {(n+1)((n+1)+1)(2(n+1)+1)\over 6}$$
$$=$$
$$1^2+2^2+3^2+ \cdot \cdot \cdot + n^2 + (n+1)^2 = {(n+1)(n+2)(2n+3)\over6}$$ I'm stuck with the algebra part, could someone go through the steps of how 
$${n(n+1)(2n+1)\over 6}+(n+1)^2 = {(n+1)(n+2)(2n+3)\over 6}$$","['induction', 'discrete-mathematics']"
1162860,Really cute counting problem (Finding the $k$'th $n$-Champkowski number),A friend of mine gave me this problem I wanted to share. We call a positive integer  a $n$-Champkowski number if the sum of its digits in base $n$ is a multiple of $n$. Give the fastest method to find the $k$'th $n$-Chapkowski number you can.,"['number-theory', 'combinatorics']"
1162867,$f_n(x)=n\sin^{2n+1}x\cos x$. Find $\lim_{n\to\infty}f_n(x)$,"Let $f_n(x)=n\sin^{2n+1}x\cos x$. Then find the value of $\lim\limits_{n\to\infty}\displaystyle\int_0^{\pi/2}f_n(x)\;dx-\displaystyle\int_0^{\pi/2}(\lim\limits_{n\to\infty}f_n(x))\;dx$ My Thoughts: If the function is uniformly convergent, the answer should be zero. To check for uniform convergence, I need to do Weierstrass's M-Test. A ratio test says $f_n(x)$ is convergent provided $x<\pi/2$. How do I properly solve this problem ? Please help.","['definite-integrals', 'calculus', 'uniform-convergence', 'limits']"
1162884,Calculating Inverse Laplace Transform of stretched exponential,"I am trying to solve a Laplace transform problem that has gotten way over my head in terms of complex analysis knowledge. I would like to solve the Inverse Laplace Transform $(s\rightarrow t)$ of $$\frac{1}{s^{\alpha+1}}\exp(-s^{\alpha}),\qquad \alpha\in(0,1)$$ I have tried a few things so far and have run into the limits of my knowledge. This will involve an integral of the form $$\frac{1}{2\pi i}\lim_{T\rightarrow\infty}\int_{c-iT}^{c+iT}\,ds\,\frac{1}{s^{\alpha+1}}\exp(-s^{\alpha}+st)$$ I feel like some kind of residue evaluation is the only way to approach this. Naively, to calculate the residue at $s=0$, I'd expand the exponential and look for the term proportional to $s^{-1}$ (there is only one such term), and the proportionality factor is my residue. However, I am concerned about the branching induced by the fractional power of $\alpha$, and I am not sure how to treat that in my evaluation of the residue. I also tried the substitution $u=s^{-\alpha}$, which leads to the integral $$\int_{\gamma}\,du\,\exp(-\frac{1}{u}+t(\frac{1}{u})^{1/\alpha}),$$
where the contour integral is now around the circle of radus 1/2 centered at $u=1/2$. This doesn't help much with the branching issue, and I don't know how to deal with the essential singularity at the origin--is the contour just deformed to include (or not include) this singularity? I have a bad feeling that the fractional power might make my desired integrals undefined. In that case I have tried some method of converting the Laplace transform of a function $LT[f(t)](s)$ to the Mellin transform $MT[f(t)](q)$ by the integral $$MT[f(t)](q) = \frac{1}{\Gamma(1-q)}\int_{0}^{\infty}\,ds\, s^{-q}LT[f(t)](s),$$ though I am worried about convergence issues here, too. However, it might be the only way to proceed in order to get some reasonably tractable/numerically solvable analytic expression or integral.","['laplace-transform', 'complex-analysis', 'contour-integration']"
1162903,"If $X\geq0$ is a random variable, show that $\lim\limits_{n\to\infty}\frac1nE\left(\frac{1}{X}I\left\{X>\frac{1}{n}\right\}\right)=0$ [duplicate]","This question already has an answer here : Show $\lim_{n \to \infty} n^{-1} E \left( \frac{1}{X}1_{[X>n^{-1}]} \right) =0$ (1 answer) Closed 3 years ago . If $X\geq0$ is a random variable then show that:$$\lim_{n\to\infty} \frac{1}{n} \cdot E\bigg(\dfrac{1}{X}I\bigg\{X>\dfrac{1}{n}\bigg\}\bigg)=0$$ A hint would be most appreciated. I have studied measure theory, but I presume this can be solved using simple analysis.","['convergence-divergence', 'random-variables', 'expectation', 'probability-theory', 'probability']"
1162938,Sum of Banach valued Borel measurable functions need not be Borel measurable?,Sum of Banach valued Borel measurable functions need not be Borel measurable when the Banach space is not separable. Any references to this result? Many thanks!,"['separable-spaces', 'measure-theory', 'reference-request', 'banach-spaces']"
1162951,Detection of Cycles without a Center in an ODE,"In my classes in dynamical systems theory, we were taught how to detect cycles or cyclic behavior in an ODE (be it dampened, sustained or growing) around a fixed point by looking at the eigenvalues for that point (their imaginary component in particular). But in higher dimensional systems cycles need not have a center, and in practice one often does not even always know the location of a center even if it exists. Are there any non-local techniques for identifying cyclic behavior in an ODE which would allow me to detect a cycle without a center? Would such techniques similarly allow me to identify strange attractors?","['dynamical-systems', 'ordinary-differential-equations', 'chaos-theory']"
1162965,Find the expected number of people who select their own name tag,"At a party n people throw their name tags on a table. The name tags
are mixed up and then each people randomly and simultaneously selects one.
Find the expected number of people who select their own name tag. Since the people choose simultaneously, I assumed that each has  $\frac{(n-1)!}{n!}$ or $\frac{1}{n}$ chance of selecting their own, thus expected number of people who get their own name tag is $ n\cdot \frac{1}{n} = 1 $. Am I correct in this?","['statistics', 'probability']"
1162968,Characterization of pre-orders,"Let $X$ be an arbitrary set, and $\leq$ be a pre-order on $X$. Does there always exist $u:X\to \Bbb R$ such that $x'\leq x''$ iff $u(x') \leq u(x'')$? If this is not true in general, is that true under some additional assumptions? Let us endow now $X$ with some $\sigma$-algebra, and let $\mathcal P(X)$ be the set of all probability measures on $X$. We define a preorder on $\mathcal P(X)$ in terms of stochastic dominance: $p'\leq p''$ iff there exists a coupling $P\in \mathcal P(X\times X)$ with marginals $p',p''$ such that $P(\leq) = 1$. Given that for $(X,\leq)$ there exists $u$ as in $(1)$, is that true that we can always refine $u$ to make sure that $p'\leq p''$ iff $\int u\;\mathrm dp'\leq \int u\;\mathrm dp''$?","['probability-theory', 'game-theory', 'measure-theory', 'order-theory']"
1162972,Hodge star related question,"If we have expression (1) $$\star (F \wedge d\alpha)$$ where $$ (F \wedge d\alpha)$$ is a $2$-form field strength ($F$ and $d\alpha$ are 1 forms)and $\star$ represents Hodge star . How can we simplify this rather more? I want to exterior derive expression (1) and was wondering if I can simplify it before applying exterior derivative on it? To elaborate my question: The Hodge dual on, let's say, $d(\alpha)$ where $\alpha$is any complex function can easily be deducted as one applies the Hodge Duality rule. Why? Because $d(\alpha)$ can be written as $\partial _x \alpha dx + \partial _y \alpha dy + \partial _z dz$ and we know how to go from this expression to its dual because we know that $\star dx$ let's say is $dy \wedge dz$ in 3 spatial coordinates, but here we don't know how to apply the rule..",['differential-geometry']
1163006,Resolvent of a Markov process,"I have a question about theory of Markov processes. Let $(\Omega,\mathcal{F},P)$ be a probability space. Let $E$ be a Hausdorff topological space and $\mathcal{B}(E)$ be its Borel $\sigma$-algebra and $m$ denotes a $\sigma$-finite positive measure on $(E,\mathcal{B}(E))$.   Let $X$ be a Markov process in a continuous time with value in the set $E$. We assume the following conditions holds : $\mathcal{B}(E)=\sigma(C(E))$ where $C(E)$ denotes the set of all continuous ($\mathbb{R}$-valued) function on $E$ For all $\omega \in \Omega$, $t \mapsto X_{t}(\omega)$ is right continuous on $[0,\infty[$ $P_{x}[X_{0}=x]=1\quad({}^{\forall} x\in E)$ In the above-mentioned setting, we define
\begin{align*}
R_{\alpha}f(x):=E_{x}\left[ \int_{0}^{\infty} e^{-\alpha t} f(X_{t})dt\right]\quad(\alpha>0,x \in E, f \in \mathcal{B}(E)^{+})
\end{align*} My question : Since $m$ is $\sigma$-finite, there exists $\varphi \in \mathcal{B}(E)$ such that $\varphi(x)>0$ for all $x \in E$ . For this $\varphi$, $R_{1}\varphi(x)>0,\,x \in E$ is hold? My attempt : Case1: $\varphi$ is continuous Since $t \mapsto X_{t}(\omega)$ is right continuous and $P_{x}[X_{0}=x]=1\quad({}^{\forall} x\in E)$, it is obvious that  $R_{1}\varphi(x)>0$ forall $x \in E$ . Case2 $\varphi$ is not continuous Although I think $\varphi$ is a kind of continuous function by the condition $\mathcal{B}(E)=\sigma(C(E))$, I don't know how to the proper way to use this condition. Thank you in advance.","['probability-theory', 'stochastic-processes', 'markov-process']"
1163017,Summation of $\tan^{-1}$ series,"I am given
$$S=\sum\limits_{n=1}^{23}\cot^{-1}\left(1+ \sum\limits_{k=1}^n 2k\right)$$ On expanding the sigma series becomes 
$$S= 23\cot^{-1}(3)+22\cot^{-1}(5) + \cdots + \cot^{-1}(47)$$
And in tan form as
$$S= 23\tan^{-1}(1/3)+22\tan^{-1}(1/5) + \cdots + \tan^{-1}(1/47)$$
How to sum this series?","['trigonometry', 'inverse', 'sequences-and-series', 'trigonometric-series']"
1163032,"Geometric & Intuitive Meaning of $SL(2,R)$, $SU(2)$, etc... & Representation Theory of Special Functions","Many special functions of mathematical physics can be understood from the point of view of the representation theory of lie groups. An example of the power of this viewpoint is given in my question here . The gist of the theory is as follows: The general set up here is that we have a group acting on a space $X$, and we look at the space of functions on $X$
  (let me write it $\mathcal F(X)$). Then there is a natural representation of $G$ on $\mathcal F(X)$. (a) If $ X = G = S^1$ (the circle group, say thought of as $\mathbb R/\mathbb Z$) 
  acting on itself by addition, then the solution to the
  problem of decomposing $\mathcal F(S^1)$ is the theory of Fourier
  series.  (Note that a function on $S^1$ is the same as a periodic
  function on $\mathbb R$.) (b) If $ X = G = \mathbb R$, with $G$ acting on itself by addition,
  then the solution to the above question (how does $\mathcal{F}(\mathbb{R})$ decompose under the action of $\mathbb R$) is the theory of the
  Fourier transform. (c) If $ X = S^2$ and $G = SO(3)$ acting on $X$ via rotations, then
  decomposing $\mathcal F(S^2)$ into irreducible representations gives
  the theory of spherical harmonics. (This is an important example in
  quantum mechanics; it comes up for example in the theory of the
  hydrogen atom, when one has a spherical symmetry because the electron
  orbits the nucleus, which one thinks of as the centre of the sphere.) (d) If $ X = SL_2(\mathbb R)/SL_2(\mathbb Z)$ (this is the quotient of
  a Lie group by a discrete subgroup, so is naturally a manifold, in
  this case of dimension 3), with $G = SL_2(\mathbb R)$ acting by left
  multiplication, then the problem of decomposing $\mathcal F(X)$ leads
  to the theory of modular forms and Maass forms, and is the first
  example in the more general theory of automorphic forms. The same idea can explain how Bessel, Hypergeometric functions etc... arise geometrically. Again my question has an example of this. This table: lists 7 groups and the differential equation each is related to. My question is asking for an intuitive way to think about the 7 groups listed and what they geometrically mean, and why they relate to some invariance in Laplace's equation $\nabla^2 u = 0$. I even think there are a few different interpretations, e.g. in terms of homogeneous spaces, hyperbolic spaces, etc... so I'm kind of lost. To give an example of what I'm hoping for, take the Euclidean Group $E_2$, it can be represented by matrices of the form $$ g(x,y,\theta)  =  \left( \begin{array}{ccc}
\cos(\theta) & - \sin(\theta) & x \\
\sin(\theta) & \cos(\theta) & y \\
0 & 0 & 1 \end{array} \right)$$ Intuitively this is the group of plane motions, the group of all translations and rotations in the plane. If Laplace's equation models a problem with cylindrical symmetry, i.e. Laplace's equation is invariant under the group of plane motions, classically we end up having to solve Bessel's equation, but group-theoretically we can solve the problem with representation theory this magical way instead. That shows how Bessel's equation is just another way to say: ""find a function in the plane such that when we shift it right, then shift it back left again, all locally (i.e. differentially) in polar coordinates, we get the same function"". The same model holds for all equations in that table, but I don't know the geometry or pictures allowing me to see it. Some present confusion of mine: When I look at $SL(2,R)$ and it's matrix representation I have no intuitive picture of the group it's representing or any geometric picture (like a cylinder) to think of that tells me when solving $\nabla^2 u = 0$ will result in the Hypergeometric equation, for example. If I try to think of $SO(3)$ as the group of rotations of a sphere I get Gegenbauer functions, but Gegenbauer polynomials give Legendre polynomials as a special case. However they also arise from the $SU(2)$ spinor representation of $SO(3)$. My guess is that this all links together because rotations can be decomposed into products of reflections, and spinor representations arise from this simple idea. But is it okay to think of $SO(3)$ as a group of 3-D Rotations and $SU(2)$ as the reflections that generate those rotations?","['special-functions', 'ordinary-differential-equations', 'representation-theory', 'lie-groups']"
1163092,Prove that function with positive second derivative grows faster than a linear function,"The statement of the problem is as follows, Let $f: \mathbb{R} \rightarrow \mathbb{R}$ be differentiable, and $f''(0)$ exists and is greater than $0$. We also have $f(0) = 0$. Prove that there exists an $x > 0$ such that $f(2x) > 2f(x)$. I have tried doing this based on the sequential definition of limits and the definition of the derivative but I have only been able to show that there exists $x > 0$ such that $f(2x) > f(x)$. EDIT: There was an additional condition I forgot to include, $f(0) = 0$",['analysis']
1163127,"How to factorise $x^4 - 3x^3 + 2$, so as to compute the limit of a quotient?","Question: Find the limit: $$\lim_{x \to 1}\frac{x^4 - 3x^3 + 2}{x^3 -5x^2+3x+1}$$ The denominator can be simplified to: $$(x-1)(x^2+x)$$
However, I am unable to factor the numerator in a proper manner (so that $(x-1)$ will cancel out) I know upon graphing that the limit is $5\over4$. What should I do here? Note: To be done without the use of L'Hospital Rule","['limits-without-lhopital', 'calculus', 'limits']"
1163134,Is it possible to solve for x in an equation with both x and ln(x)?,"I have an equation which has the form $$x=y-c_1\ln(c_2y+c_3)+c_1\ln(c_2x+c_3)$$ where $c_1,c_2,c_3$ are constants, and $y$ is another variable. I am trying to solve it for $x$ but I'm not sure if it's possible. Any tips?",['algebra-precalculus']
1163179,Proximal Operator of $ f \left( U \right) = -\log \det \left( U \right) $,"This is an assignment problem which I failed to solve in a couple of days. Denote the set of all $n \times n$ symmetric matrices and the set of all $n \times n$ symmetric positive definite matrices by $\mathbb{S}^n$ and $\mathbb{S}^n_{++}$ respectively. Let $f: \mathbb{S}^n_{++} \rightarrow \mathbb{R}$ be defined by
$$f(U) = -\log \det (U) \text{ for } U \in \mathbb{S}^n_{++}$$
We are asked to find the proximal mapping $\text{prox}_f$ of $f$. My question is: What is the domain of $\text{prox}_f$? It is not mentioned in the problem. I suppose it is $\mathbb{S}^n$ and I will explain it later. The function $g(U;X) = -\log \det(U) + \frac{1}{2}\|U - X\|_F^2$ is differentiable in each entry $U_{ij}$ of $U$. If I did not make mistakes in the calculation, the minimization problem 
$$ \text{prox}_f(X) = \text{argmin}_{U \in \mathbb{S}^n_{++}} g(U;X) $$
can be formulated as
$$ 0 = \frac{\partial g(U;X)}{\partial U_{ij}} = -U^{-1}_{ji} + U_{ij} - X_{ij} = -U^{-1}_{ij} + U_{ij} - X_{ij}$$
and therefore $U = \text{prox}_f(X) \in \mathbb{S}^n_{++}$ should satisfy
$$U - U^{-1} = X$$
This also sees why $X$ has to be symmetric. But I cannot proceed from here: How do we calculate $U$ from $X$ in practice? Thanks in advance. Any comments or hints are welcome.","['optimization', 'proximal-operators', 'matrices', 'linear-algebra', 'convex-optimization']"
1163180,Help with understanding an example from the book 'Fooled by Randomness',"This is an example from the book ""Fooled by Randomness"": (...)We know a priori that he is an excellent investor, and that he will be expected to earn a return of 15% in excess of Treasury bills, with a 10% error rate per annum (what we call volatility). It means that out of 100 sample paths, we expect close to 68 of them to fall within a band of plus and minus 10% around the 15% excess return, i.e., between 5% and 25% (to be technical; the bell-shaped normal distribution has 68% of all observations falling between -1 and 1 standard deviations). It also means that 95 sample paths would fall between -5% and 35%. A 15% return with a 10% volatility (or uncertainty) per annum translates into a 93% probability of success in any given year. But seen at a narrow time scale, this translates into a mere 50.02% probability of success over any given second. Table 3.1 Probability of success at different scales Scale      Probability
1 year     93%
1 quarter  77%
1 month    67%
1 day      54%
1 hour     51.3%
1 minute   50.17%
1 second   50.02% How do I calculate the probability of success at different scales (Table 3.1)?
E.g. where does 77% for a quarter come from?","['probability-distributions', 'probability']"
1163184,when is $n!+10$ a perfect square?,When is $n!+10$ is a perfect square ? I have tried and found that only for $n=3$ is $n!+10$ a perfect square. Is there any other solution to this?,"['factorial', 'square-numbers', 'discrete-mathematics', 'number-theory']"
1163205,"Example 4.21 in Baby Rudin: How is the map $f^{-1}$ not continuous at the point $(1,0) = f(0)$?","Let $f \colon [0,2\pi ) \to \{ (x,y) \in \mathbb{R}^2 \mid x^2 + y^2 = 1\}$ be defined as 
$$f(t) = ( \cos t , \sin t) \ \ \mbox{ for all } \ t \in [0, 2\pi).$$ 
Then the map $f$ is bijective and continuous, as asserted by Rudin. But how do we rigorously show that the inverse map is not continuous at the point $(1,0) = f(0)$? Intuitively, it is obvious since if we approach the point $(1,0)$ along that part of the unit circle which lies in the fourth quadrant, we can make the  distance in $\mathbb{R}^2$ between $f(t)$ and $f(0)$ as small as we please, but the distance in $\mathbb{R}$ between $t= f^{-1}\left(f(t)\right)$ and $0= f^{-1}\left(f(0)\right) = f^{-1}\left( (1,0) \right)$ approaches $2\pi$. But how to formulate this into a rigorous, $\epsilon$-$\delta$ argument?","['continuity', 'real-analysis', 'analysis', 'metric-spaces', 'uniform-continuity']"
1163225,"Properties of Sobolev spaces $W^{k,\infty}(\Omega)$","I'm looking for different properties of spaces $W^{k,\infty}(\Omega)$ for bounded domain $\Omega \subset \mathbb R^n$ and $k \geq 1$ that I couldn't find in literature. References are wery welcome. Extension. Theorem 12.15 of Giovanni Leoni ""A first course in Sobolev spaces"" tells us that if $\Omega$ has Lipshitz boundary then there exists a continuous linear extension operator from $W^{1,\infty}(\Omega)$ to $W^{1,\infty}(\mathbb R^n)$. What about spaces $W^{k,\infty}$ for $k \geq 2$? Does there exist a continuous extension operator in the case of $C^k$ boundary $\partial \Omega$? Trace. Theorem 5, p. 131 of Evans ""Measure theory and fine properties of functions"" tells us that $f \in W^{1,\infty}_\text{loc}(\Omega)$ if and only if $f$ is locally Lipshitz in $\Omega$. If I'm not mistaken, combining this with the above extension result we obtain that for any $f \in W^{1,\infty}(\Omega)$ we can define its trace on $\partial D$ as an element of $W^{1,\infty}(\partial \Omega)$ if $\partial \Omega \in C^1$. To do this we extend $f$ to $\widetilde f \in W^{1,\infty}(\mathbb R^n)$ so that $\widetilde f|_{\partial D}$ is the restriction of the Lipshitz function which is Lipshitz and hence $W^{1,\infty}(\partial D)$. Is it true in general that the trace operator continuously maps $W^{k,\infty}(\Omega)$ to $W^{k,\infty}(\partial \Omega)$ if $k \geq 1$, $\partial \Omega \in C^k$? (Intuitively it must be true since taking trace on $W^{k,p}$ ""costs"" 1/p smoothness). Extension from boundary. Let $f \in W^{1,\infty}(\partial D)$. Then (as above) $f$ is Lipshitz. Then by the Kirszbraun theorem $f$ can be extended to a Lipshitz $\widetilde f$ (the extension not necessarily linear) on $\mathbb R^n$ and hence to $\widetilde f|_\Omega \in W^{1,\infty}(\Omega)$. Does the same result hold for $f \in W^{k,\infty}(\partial D)$, $k \geq 2$? Any comments are very welcome.","['reference-request', 'sobolev-spaces', 'functional-analysis']"
1163260,Random directions on hemisphere oriented by an arbitrary vector,"Hy, i'm writing a raytracer, and for that I need to generate n random vectors that are inside an hemisphere oriented by the surface normal. Ideally, I would also like being able to restrict the rays so that the angle with the normal is <= some alpha. I don't require mathematics strictness, it can be an approximation. What I have been doing is creating an arbitrary tangent frame, and a random vector where the x and y coordinate go from -1 to 1 and the z coordinate from 0 to 1, and then multiplying it with the tangent frame, but for some reason this is making the rays to go along a direction, instead of covering the entire hemisphere.
Here's some pseudo code, based on what i'm using, but removed the HLSL clumsy things: dir = normal

ray.x = GetRandomFloat(-1, 1)
ray.y = GetRandomFloat(-1, 1)
ray.z = GetRandomFloat(0, 1)

N = abs(normal)

if( N.z > N.x and N.z > N.y )
    rt = (1, 0, 0)
else
    rt = (0, 0, 1)

rt = normalize(rt - normal.xzy * dot(rt, In))
rtb = cross(rt, normal.xzy)

rt.xyz = rt.xzy
rtb.xyz = rtb.xzy

ray = normalize(ray.x * rbt + (ray.y * rt + (ray.z * dir))) If someone can provide some insight into this, I would be grateful. It doesn't need to be code, even a math formula will do. Also, distribution doesn't need to be uniform, just look ""good"" and 1/0 or 0/0 in some cases are allowed, as they map to INF or NaN, and I can handle that without problems. EDIT:
While a PDF that fits this is interesting, it will require a random number generator based on a PDF, and that's really expensive, and not actually what I'm looking for. EDIT 2:
If it can be done, it would be better not to use inverse trigonometric functions, even if it sacrifices uniformity EDIT 3: Tried Tryss method, but the results are the same, here's a screen to show what I mean: It is clear that the rays aren't generated along all directions.
I'm using the second method to generate the rays, and then using the same process I had before to orientate it, that is constructing a basis change matrix from the normal and an arbitrary tangent vector. That might be what is wrong, can anyone provide some insight if the code I have posted earlier actually orients the directions from the z up hemisphere to the hemisphere oriented by the vector ""normal"" EDIT 4: I'll try to make the problem more clear. I want to generate n random rays, in an hemisphere oriented by an arbitrary direction. I do not require an uniform distribution. The first part has been answered, taking x = rand(-1,1) y = rand(-1,1) z = rand(0,1) and then normalizing, while not uniform, produces acceptable results and is fast enough. Now the second part is transforming that vector so that it is oriented by an arbitrary direction, in a way that (0,0,1) -> (nx,ny,nz) , where (nx,ny,nz) is the arbitrary vector.
The coordinate system for the final vector and the arbitrary directions is a left handed system where x is left and right, z is front and back and y is up and down.
I know that this matrix does the transformation I want, because I do the same coordinate change for normal mapping ( http://en.wikipedia.org/wiki/Normal_mapping ) (bitangent.x, bitangent.y, bitangent.z)
(tangent.x  , tangent.y  , tangent.z  )
(normal.x   , normal.y   , normal.z   ) Where bitangent is normal X tangent The problem now is that I only have the ""normal"" vector, so I need to generate an arbitrary ""tangent"" vector to construct that matrix. How can I do that, in a way that the rays get properly transformed ( they don't biased to a direction, like in the screen I showed) ?
The code I was using earlier is at the top of the page, but it produces the biasing I want to avoid.","['monte-carlo', 'linear-algebra', 'probability', 'random']"
1163269,"can the emphasis on ""smallest"" in the monotone class theorem be ignored in applications?","The monotone class theorem states that for any algebra of sets $\cal A$ one can construct the smallest monotone class generated by this class ${\cal M}(\cal A)$. This smallest monotone class is also the smallest sigma-algebra $\Sigma(\cal A)$ generated by $\cal A$, and ${\cal M}(\cal A)=\Sigma(\cal A)$. However, I noticed that in applications of the theorem in various mathematical proofs I have studied authors ignore the fact that the monotone class should be the smallest. For example, assume that the goal is to prove that a certain class of sets, e.g. $\cal B$, is a sigma-algebra. Authors just prove that this group of sets is a monotone class, and then by invoking the monotone class theorem, without showing that $\cal B$ is indeed the smallest monotone class, just conclude that this class has to be also a sigma-algebra. Is it in general possible to ignore the ""smallest"" requirement when using the monotone class theorem, or are there circumstances one has to be aware of that make such use possible? EDIT: The background to the question can be found here . The post discusses a theorem where the ""smallest"" requirement is being ignored.  It provides an example.","['monotone-class-theorem', 'measure-theory']"
1163306,$y''+y'^{2}+y=0$ equation solution,How would you solve this differential equation $y''+y'^{2}+y=0$? I can't apply the ansatz method (or more formally apply the characteristic polynomial method). Thanks,['ordinary-differential-equations']
1163318,Difference between sum and direct sum,What is the difference between sum of two vectors and direct sum of two vector subspaces? My textbook is confusing about it. Any help would be appreciated.,"['vector-spaces', 'direct-sum', 'linear-algebra', 'vectors']"
1163331,How to prove this limit is $0$?,"Let $f:[0,\infty )\rightarrow \mathbb{R}$ be a continuous function such that: $\forall x\ge 0\:,\:f\left(x\right)\ne 0$. $\lim _{x\to \infty }f\left(x\right)=L\:\in \mathbb{R}$. $\forall \epsilon >0\:\exists x_0\in [0,\infty)$ that $0<f\left(x_0\right)<\epsilon $. Prove that: $$L=0$$ So far, I thought about assuming that $L\ne 0$ to get a contradiction. Assume that $f\left(0\right)<0$. Now, choose an arbitrary $\epsilon $>$0$, and we know that $\exists x_0>0$ such that $0<f\left(x_0\right)<\epsilon$. $f$ is continuous, so from the Intermediate value theorem, $\exists c\in \left[0,x_0\right]: \space f\left(c\right)=0$, and it's a contradiction to (1). So, $f\left(0\right)\ge 0$. (Actually, I think it means that $\forall c\in \left[0,\infty \right): \space f\left(c\right)\ge 0$, and it contradicts that $L<0$). I get stuck while proving the case when $L>0$. Can someone guide me? Thanks in advance!","['continuity', 'functions', 'limits']"
1163335,If $ z_{0}=x_{0}+iy_{0}$ satisfy the equation $2\left|z_{0}\right|^2=r^2+2.$ Then $\left|\alpha \right| = $,"Let a Complex no. $\alpha$ and $\displaystyle \frac{1}{\bar{\alpha}}$ lie on the Circles $\left|z-z_{0}\right|=r$ and $\left|z-z_{0}\right|=4r^2$ respectively. If $\displaystyle z_{0}=x_{0}+iy_{0}$ satisfy the equation $2\left|z_{0}\right|^2=r^2+2.$ Then $\left|\alpha \right| = $ $\bf{My\; Solution::}$ If $\alpha$ lies on $\left|z-z_{0}\right|^2 = r^2\;,$ Then Put $z=\alpha$ in Given equation. So We get $\left|\alpha-z_{0}\right|^2=r^2.......................................................(1)$ Similarly If $\displaystyle \frac{1}{\bar{\alpha}}$ lies on $\left|z-z_{0}\right|^2 = 4r^2\;,$ Then Put $\displaystyle z=\frac{1}{\bar{\alpha}}$ in Given equation. So We get $\displaystyle \left|\frac{1}{\bar{\alpha}}-z_{0}\right|^2=4r^2\Rightarrow \left|1-\bar{\alpha}z_{0}\right|^2=4r^2 \cdot \left|\bar{\alpha}\right|^2=4r^2 \cdot \left|\alpha\right|^2.....................(2)$ Now Subtract $(2)$ from $(1)\;,$ We Get.... $\Rightarrow \displaystyle \left|\alpha-z_{0} \right|^2 -\left|1-\bar{\alpha}z_{0}\right|^2=r^2-4r^2 \left|\alpha \right|^2$ $\displaystyle \Rightarrow \left|\alpha \right|^2+\left|z_{0}\right|^2-\alpha \bar{z_{0}}-\bar{\alpha}z_{0}-1-\left|\alpha\right|^2\left|z_{0}\right|^2+\alpha \bar{z_{0}}+\bar{\alpha}z_{0}=r^2-4r^2\left|\alpha \right|^2$ $\displaystyle \Rightarrow \left|\alpha \right|^2-1+\left|z_{0}\right|^2-\left|\alpha\right|^2\left|z_{0}\right|^2=r^2-4r^2\left|\alpha^2\right|$ $\displaystyle \Rightarrow \Bigg(\left|z_{0}^2\right|-1 \Bigg)\cdot \left(1-\left|\alpha ^2\right|\right)=r^2-4r^2 \left|\alpha\right|^2$ Now Using $\displaystyle \left|z_{0}\right|^2 = \left(\frac{r^2+2}{2}\right).$ and Put into above above equation. $\displaystyle \Rightarrow \Bigg(\frac{r^2}{2}\Bigg)\cdot\left(1-\left|\alpha ^2\right|\right)=r^2-4r^2 \left|\alpha\right|^2 $ $\displaystyle \Rightarrow \left(r^2\right)\cdot \left(1-\left|\alpha ^2\right|\right)=2r^2-8r^2 \left|\alpha\right|^2.$ $\displaystyle \Rightarrow r^2-r^2\left|\alpha \right|^2=2r^2-8r^2 \left|\alpha\right|^2$ So we get $\displaystyle 7r^2 \left|\alpha \right|^2 = r^2\Rightarrow \left|\alpha\right| = \frac{1}{\sqrt{7}}$ . My Question is can we solve the above question any other method, If yes then please explain me. Thanks.","['complex-numbers', 'algebra-precalculus']"
1163353,Projection onto subspaces - point to line projection,"In the following document about projection onto subspaces, the author is computing the transformation matrix to project a vector $b$ onto a line formed by vector $a$. Since the projected vector $p$ is on the $a$ line, therefore it can be expressed as $p = \bar{x}a$. The projection ""line"" which is the vector $b - p$ is orthogonal to $a$. This means the dot-product of these two is zero: $$a \dot{} (b - p) = a^T(b - \bar{x}a) = 0$$ It comes to the conclusion that $\bar{x} = \frac{a^T b}{a^T a}$ so $$p = ax = a \frac{a^T b}{a^T a}$$ (Which I fully understand) Then the objective is to find the projection matrix P such as $p = P b $. The author wrote that $p = \bar{x}a = \frac{aa^Ta}{a^Ta}$ and so $P = \frac{aa^T}{a^Ta}$ How does he deduced this last part? I cannot see how he goes from 
$$p = \bar{x} a = \frac{a^T b}{a^T a} a$$ to $$p = \frac{aa^Ta}{a^Ta}$$ And even then, why would $P$ be $\frac{aa^T}{a^Ta}$ since there are absolutely no $b$ in this last expression to identify a $p = Pb$ ?","['projective-space', 'matrices', 'linear-algebra']"
1163379,Marbles in a bag (Combinatorics),"There are 10 numbered from 1 to 10 marbles. The marbles are placed in an opaque bag and shuffled. One random marble is taken out, its number is written on a piece of paper, marble is then returned to the bag, marbles are shuffled again. Procedure is repeated, until 25 records are accumulated on the paper. Question №1: what is the probability that paper now contains all numbers from 1 to 10, at least once? Q №2: what is the average amount of numbers to be written in such a manner, until all numbers from 1 to 10 are recorded at least once? P.S. I found correct numbers by using the Monte Carlo method, but interested in purely mathematical solution. Update Since asking this, I've questioned friends and collegues, tried to receive correct solution for #1 at different websites, but it all failed for me. The Emperor of Ice Cream's answer, simply doesn't seem to be entirely correct, as it's outcome is fairly far from my simulation results (which was conducted again on rewired algorithm, only to see the same outcome).","['probability', 'combinatorics']"
1163417,Subset $A\subset\mathbb R$ such that for any interval $I$ of length $a$ the set $A\cap I$ has Lebesgue measure $a/2$,Is there a subset $A\subset\mathbb R$ such that for any interval $I$ of length $a$ the set $A\cap I$ has lebesgue measure $a/2$? Can it be constructed explicitly?,"['measure-theory', 'lebesgue-measure']"
1163423,Domain and Range of functions,"What is the best way to find the range of a function? For example if we have a function given:
$$y = \frac{\sqrt{x+3}}{x^3 - 2x^2 - 8x}$$ The domain is easy enough to find:
Ensure that the term inside the square root is positive and that the denominator is not zero. However, how would we find the range of the function? Apart from graphing the function. I would like to do it algebraically.",['functions']
1163457,prove that every lossless compression algorithm must result in increasing the file size for some inputs.? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Using Pigeonhole Principle prove that every lossless compression algorithm must result in increasing the file size for some inputs.?","['pigeonhole-principle', 'discrete-mathematics']"
1163470,"Euler-Lagrange Equation and ""Eigen Value ""","The Eigen value $\lambda(t)$ which is characterised by the Rayleigh quotient (where $t$ is a scalar variable): 
$$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy }{\int_{\Omega_t} u^2 dy}$$ 
$\lambda(\Omega_t) = \mathbf{min} \{R(v, \Omega_t) : v \in H^{1,2}(\Omega_t)\}$ 
The minimiser $u*$ of the Rayleigh quotient satisfies :
$$\left. \frac{d}{ds} R(u* + s\varphi) \right \vert_{s=0} =0 $$
Solving it , the conditions i get are $$\Delta u^* + \lambda^* (\Omega_t) u^* =0 \quad \textrm{in} \quad  \Omega_t $$ $$\partial_{\nu} u^* =  0 \quad \textrm{in} \quad \partial \Omega_t$$ $\textbf{Similarly, I want to find the conditions that are}$ 
$ \textbf{fulfilled by the minimizer $u^*$ for the Rayleigh  quotient defined as follows .}$ $$R(u,\Omega_t)= \frac{\int_{\Omega_t} |\nabla u|^2 dy + \alpha \oint_{\partial \Omega_t} u^2 dS}{\left(\int_{\Omega_t} u^q dy \right)^{2/q}}$$ 
$\textbf{My Attempt: }$
We have $$\lambda(x, \Omega_t) = \frac{\int_{\Omega_t} {\mid \nabla u(x) \mid  }^2 \, dx + \alpha \oint_{\partial \Omega_t} u^2\, ds} { \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} $$ 
Corresponding eigen value equation is given by , 
Let $t \in \mathbb{R} , \varphi \in H^{1,2}\Omega$ . If $\lambda(x, \Omega_t) $ is the eigen value then the following holds 
$\frac{d}{ds} R(u_t + s \varphi , \Omega_t) \rvert_{s=0}$
$$\frac{d}{ds} \left(\frac {\int_{\Omega_t} {\mid \nabla (u+s\varphi) \mid}^2\,dx}{\left(\int_{\Omega_t} {\mid u +s\varphi \mid}^q \, dx \right)^{2/q}} \right) = \frac{d}{ds} \left (\frac{\int_{\Omega_t} {\mid \nabla u \mid}^2 + 2 s \nabla u \nabla \varphi + s^2 {\mid \nabla \varphi \mid}^2 \, dx} {\left( \int_{\Omega_t} (u+s \varphi )^q  \, dx\right )^{2/q}} \right)_{s=0}  $$ $$=\frac{2 \int_{\Omega_t} \nabla u \nabla \varphi\,dx }{ \left( \int_{\Omega_t} u^q  \, dx\right )^{2/q}} - 2 \left( \int_{\Omega_t} {\mid u \mid }^q\right)^{\frac{-2}{q} -1 } \int_{\Omega_t} u^{q-1} \varphi \, dx  \int_{\Omega_t} {\mid \nabla u \mid}^2 dx  ... (1*) $$ Evaluating next term, 
$$\frac{d}{ds} \left( \frac{\alpha \oint_{\partial \Omega_t} (u+s \varphi)^2 \, ds}{ \left(\int_{\Omega_t} (u+s\varphi)^q \, dx \right )^{2/q} }\right )_{s= 0} $$ $$= \frac{2 \alpha \int_{\partial \Omega_t} u \varphi  \, ds - 2 \alpha \oint_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} u^q \, dx \right )^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left( \int_{\Omega_t} u^q \, dx \right )^{2/q} } ... (2*) $$ Adding $(1)$ and $(2)$ gives : $$\frac{2  \int_{\Omega_t} \nabla u \nabla \varphi dx - \int_{\Omega_t} {\mid \nabla u \mid}^2 dx \left( \int_{\Omega_t} u^q  \, dx \right )^{-1}  \int_{\Omega_t} u^{q-1} \varphi \, dx + \alpha \int_{\partial \Omega_t} u \varphi \, ds - \alpha \int_{\partial \Omega_t } u^2 \, ds \left( \int_{\Omega_t} u^q  dx \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx }{ \left(\int_{\Omega_t} {\mid  u\mid }^q\, dx \right)^{2/q}}$$ Using integration by parts yields, $$\frac{-2 \int_{\Omega_t} \Delta u \varphi dx + 2 \int_{\partial \Omega_t} \partial_{\nu} u \varphi \, ds  - 2 \left ( \int_{\Omega_t} u^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx \int_{\Omega_t} {\mid \nabla u \mid}^2 \, dx   +2 \alpha \int_{\partial \Omega_t} u \varphi \, ds }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$
-\frac{2 \alpha \int_{\partial \Omega_t} u^2 \, ds \left(\int_{\Omega_t} {\mid u \mid}^q \right)^{-1} \int_{\Omega_t} u^{q-1} \varphi \, dx  }{\left(\int_{\Omega_t} {\mid  u\mid }^q \, dx \right)^{2/q}}$$ $$ \Delta u + \lambda \mu (u) u^{q-1} = 0 \mathtt \;on\; \Omega_t $$ $$ \partial_{\nu} u + \alpha u = 0 \mathtt \; on \; \partial \Omega_t $$ Where $$\mu(u) = \left ( \int_{\Omega_t} u^{q-1} \, dx \right )^{-1} $$ Can someone check if the resulted equations are the right necessary conditions ? I am looking also for the sufficient conditions so as to know for which $q \in \mathbb R$  the minimum even exists .","['multivariable-calculus', 'functional-analysis', 'calculus-of-variations']"
1163481,Picard iteration for a system,"I have the following system: $$y'(t)=x^2(t)-x(t)$$ $$x'(t)=y(t)$$ It comes from the second order ode $$x''(t)=x^2(t)'x(t)$$ I am asked to do the first four Picard iterations starting from the solution 
$$\phi_0 (t)= \bigg(\frac{-1}{2},0 \bigg)$$ I can do Picard iterations for a simple first order ode, but I am not able to generalize it to a system where the two equations depend on each other, and I cant find any examples or theory that tells the algorithm to help me in this case.",['ordinary-differential-equations']
1163501,Is $\int_0^\infty x^{a-1} (1-x)^{b-1} e^{t-cx} dx$ integrable?,"I am trying to evaluate the  integral below. Is it even integrable? (Online integral solvers  e.g. WolframAlpha could not solve the indefinite or the definite integral.) $$\int_0^\infty x^{a-1} (1-x)^{b-1} \, e^{t-cx} dx$$ where $a > 0$, $b > 0$, $c > 0$, and $t$ is any real number.","['definite-integrals', 'improper-integrals', 'probability-distributions', 'integration']"
1163545,"Seven line segments, with lengths no greater than 10 inches, and no shorter than 1 inch, are given. [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question Seven line segments, with lengths no greater than 10 inches, and no shorter than 1 inch, are given. Show that one can choose three of them to represent the sides of a triangle.",['combinatorics']
1163616,Expected rolls to get 3 of any number,"Suppose I am rolling a die repeatedly, and I keep a tally of how many times each number has come up. As soon as a number has come up 3 times, the game is over.
It does not need to be 3 times in a row - the tally just needs to reach 3. What is the expected number of rolls in a given game? From simulation, I get an answer of approximately 7.29, but I'm trying to figure out how to solve it exactly. I'm having trouble even beginning to frame this, so any help would be appreciated.",['probability']
1163650,"If $\mathbb{R}^n=U\cup V$ for path-connected $U,V$ then $U\cap V$ is path-connected.","Suppose that we can write $$
\mathbb{R}^n=U\cup V,
$$
where $U,V$ are open and path-connected. It is easy to show using Mayer-Vietoris sequence that $U\cap V$ is path-connected as well. Is there a proof which does not use homology, ideally something just using point-set topology?","['general-topology', 'algebraic-topology']"
1163651,Row space and Column space and product of matrices question,"The theorems state that the $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and the $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$. When doing $A \cdot B$, it's the dot product of the rows of $A$ by the columns of $B$. If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{RowSpace}(A \cdot B) \subseteq \operatorname{RowSpace}(B)$ and not $\operatorname{RowSpace}(A)$? Also, If its the rows of $A$ by the columns of $B$, why is it that $\operatorname{ColumnSpace}(A \cdot B) \subseteq \operatorname{ColumnSpace}(A)$ and not $\operatorname{ColumnSpace}(B)$?","['matrices', 'linear-algebra']"
1163671,How to prove that the lines in a polygonal approximation of a simple closed curve do not intersect as n gets large,"I have the following exersice which I have no idea how to approach. Let γ=$φ[0, L]$ to $R^2$ be a closed simple curve parametrized by arc length. Now we set $si$ = $iL/n$ for $i = 0, 1, . . . , n$, let $ Pn (γ) = (φ (s0), φ (s1), . . . , φ (sn))$
be the $n$-th polygonal approximation of γ. Show that for $n$ sufficiently large, $Pn(γ)$ has no self-intersections,
that is, the interiors of the straight line segments joining $φ(si)$ and $φ(si+1)$ for
$0 ≤ i ≤ n$ are disjoint The result is intuitive however I have no idea how to begin proving this. Does anyone have any idea? Thanks in advance",['geometry']
1163680,Birkhoff representation of a stochastic matrix,"From the Birkhoff theorem , it is known that every doubly stochastic matrix can be written as a convex combination of permutation matrices, although this representation might not be unique. Assume that a stochastic matrix is given. How can I find a permutation matrix that has a nonzero weight in at least one convex representation of the given stochastic matrix? For example, if $$P = \begin{bmatrix} \frac 12 & \frac 12\\ \frac 12 & \frac 12\end{bmatrix}$$ then $P$ can be written as follows $$P = \frac 12 \begin{bmatrix} 1 & 0\\ 0 & 1\end{bmatrix} + \frac 12 \begin{bmatrix} 0 & 1\\ 1 & 0\end{bmatrix}$$ In this case, both permutation matrices in the right-hand side have the property that I wish because they receive nonzero weight in at least one convex representation of $P$. Is there any simple algorithm to rapidly find at least one such permutation matrix for a given stochastic matrix?","['permutations', 'stochastic-matrices', 'birkhoff-polytopes', 'matrices', 'convex-hulls']"
1163773,When does Separation of Variables yield basis of solution set?,"In mathematical physics, one often employs the technique 'Separation of Variables' to find the full solution set to some linear partial differential equation. For instance, consider the differential equation (1D heat equation): $$\frac{\partial u}{\partial t} - \frac{\partial^2 u}{\partial x^2} = 0$$ The driving premise behind 'separation of variables' is that solutions of the following form make up a basis (are total in the sense that infinite, converging sums are allowed) for the entire solution space:
$$u(x,t) = X(x)T(t)$$ Let this set of solutions be denoted $U$. Let the full set of solutions be denoted $S$. Is there a proof that $\overline{\mathrm{lin}}(U) = S$? What is the complete classification of PDEs for which this is true? I have another way of wording this question which may give some insight. Let $X$ be the vector space of $L^2$ functions of $x$, and $T$ the vector space of $L^2$ functions of $t$. I think that $X \otimes T$ is the $L^2$ space of functions of $(x,t)$. The Separation of Variables technique assumes that the subspace of $X \otimes T$ corresponding to the solution set is given by some $X_1 \otimes T_1$ where $X_1$ and $T_1$ are subspaces of $X$ and $T$ respectively, as these spaces admit bases of form $\{f_1 \otimes g_1, f_2 \otimes g_2 , \dots\}$. However, not every subspace of $X \otimes T$ is of this form, as I found in this question . Given a PDE, how do we know when the solution set will be of the special form $X_1 \otimes T_1$?","['functional-analysis', 'partial-differential-equations']"
1163781,Evaluate the following sum using a combinatorial argument [duplicate],This question already has answers here : Evaluate $\sum_{k=0}^{n} {n \choose k}{m \choose k}$ for a given $n$ and $m$. (5 answers) Closed 9 years ago . Evaluate the following sum using a combinatorial argument: $$ \sum\limits_{k=0}^n {n \choose k} {m \choose k} $$ Can someone push me in the right direction with this? I thought for combinatorial proofs there has to be a left side and a right side where one side can be used to form a question? (if that makes sense? haha) Is there a difference with combinatorial arguments? Any help would be greatly appreciated.,['combinatorics']
1163808,Apparent counter example to Stoke's theorem?,"I think I found an apparent contradiction to Stoke's theorem with this 2-differential form $M= \overline{B^{2}}- \{ 0 \}$, $\partial M = S^1$, $$\omega = \frac{x~dy-y~dx}{x^2+y^2}$$ defined in $\mathbb{R}^2 - \{0\}$ and then pullbacked to  $ \overline{B^{2}}- \{ 0 \}$ $d \omega = 0$ So that by Stoke's Theorem $$ \int\limits_{\partial M} \omega = \int\limits_{M}  d \omega = 0 $$ But direct calculation shows that $\int\limits_{S^1} \omega \neq 0 $","['differential-forms', 'integration', 'differential-geometry']"
1163824,Trees have a vertex of degree $1$,"Prove that any tree has a vertex of degree $1$. Let graph $G=(V,E)$ have $n$ vertices and $m$ edges where $m<n$. We need to prove that the minimum degree of, $\delta (G)=1$ Since G is connected then there exists a path from $u$ to $v$ such that $u,v \in V$. Is what I have said so far correct? I really don't know how to carry it on. I need a formal answer.","['graph-theory', 'combinatorics']"
1163847,"if $x^3-x\in\mathbb{Z}$ and $x^4-x\in\mathbb{Z}$ for some $x\in\mathbb{R}$, then $x\in\mathbb{Z}$.","Assume that $x^3-x\in\mathbb{Z}$ and $x^4-x\in\mathbb{Z}$ for some $x\in\mathbb{R}$.
Prove that $x\in\mathbb{Z}$. my attempt:
Let $a=x^3-x$ and consider polynomial $X^3-X-a$, then $x$ is a root of it and if $x\in\mathbb{Q}$, then obviously $x\in\mathbb{Z}$.
But it doesn't give anything if $x\notin\mathbb{Q}$, so it's surely not the right way.","['polynomials', 'contest-math', 'number-theory']"
1163860,About the series $\sum_{n\geq 0}\frac{1}{(2n+1)^2+k}$ and the digamma function,"Let we provide a closed form for 
$$ S_k = \sum_{n\geq 0}\frac{1}{(2n+1)^2+k} $$
for $k>0$ in terms of elementary functions. It is quite easy to check that $S_k$ can be computed in terms of the digamma function $\psi(x)$, but it is also true that:
$$ \int_{0}^{+\infty}\frac{\sin(mx)}{m}e^{-\sqrt{k}\,x}\,dx =\frac{1}{m^2+k}\tag{1} $$
and that, almost everywhere:
$$ \sum_{n\geq 0}\frac{\sin((2n+1) x)}{2n+1} = \frac{\pi}{4}(-1)^{\left\lfloor\frac{x}{\pi}\right\rfloor}, \tag{2}$$
hence:
$$\begin{eqnarray*} S_k &=& \frac{\pi}{4}\int_{0}^{+\infty}(-1)^{\left\lfloor\frac{x}{\pi}\right\rfloor} e^{-\sqrt{k}\,x}\,dx = \frac{\pi}{4}\sum_{n\geq 0}(-1)^n \int_{n\pi}^{(n+1)\pi}e^{-\sqrt{k}\,x}\,dx \\&=&\frac{\pi}{4\sqrt{k}}\sum_{n\geq 0}(-1)^n\left(e^{-n\pi\sqrt{k}}-e^{-(n+1)\pi\sqrt{k}}\right)\\&=&\frac{\pi\left(1-e^{-\pi\sqrt{k}}\right)  }{4\sqrt{k}}\sum_{n\geq 0}(-1)^n e^{-n\pi\sqrt{k}}=\color{red}{\frac{\pi}{4\sqrt{k}}\cdot\frac{e^{\pi\sqrt{k}}-1}{e^{\pi\sqrt{k}}+1}}.\tag{3} \end{eqnarray*}$$ Does this identity provide something interesting about special values of the digamma function?","['closed-form', 'special-functions', 'sequences-and-series', 'calculus', 'polylogarithm']"
1163866,Weak $k$-compositions with each part less than $j$,"I am trying to figure out a problem from Richard Stanley's $\textit{Enumerative Combinatorics}$, which has to do with weak compositions of $n$ (sequence of nonnegative integers whose sum adds up to $n$). The problem is as follows: Let $\kappa(n,j,k)$ be the number of weak compositions of $n$ into $k$ parts, each part less than $j$. Give a generating function proof that
$$\kappa(n,j,k)=\displaystyle \sum_{r+sj=n}(-1)^s\binom{k+r-1}{r}\binom{k}{s},$$ where the sum is over all pairs $(r,s)\in \mathbb{N}^2$ satisfying $r+sj=n$. I see an alternating sum, so naturally I think about the Principle of Inclusion-Exclusion. I thought to consider the number of all weak compositions of $n$, which is $\binom{n+k-1}{k-1}$ and then remove all weak compositions whose largest part is $n$, $n-1$, $\ldots$, or $j$. I have made one observation, which is that $$ \text{the number of weak compositions with largest part $j$}=\kappa(n,j+1,k)-\kappa(n,j,k),$$ unless somehow I am mistaken (in that case, please let me know). However, this led me to some ridiculous computations. Perhaps someone else has a suggestion? Thank you for any help offered!","['generating-functions', 'discrete-mathematics', 'number-theory', 'combinatorics']"
1163885,Quaternions as a counterexample to the Gelfand–Mazur theorem,It seems that by the Gelfand–Mazur theorem quaternions are isomorphic to complex numbers. That is clearly wrong. So where is the catch? I think that I found the problem but it seams so subtle that would like to have confirmation from someone else and I would like to know where the proof of Gelfand–Mazur blows up.,"['banach-algebras', 'functional-analysis']"
1163888,Axiom of Completeness to prove intermediate value theorem,"I am having a little trouble understanding one of the steps in this proof.
From Stephen Abbott's Analysis : Using AoC to prove the IVT: TO simplify matters, consider $f$ as a continuous function which satisfies $f(a)<0<f(b)$
and show that $f(c) =0 $ for some $c \in (a,b)$. First let Clearly we can see $K$ satisfies the axiom of completeness, and thus we can let $c = \sup K$ There are three cases to consider: $f(c) > 0, f(c)<0 \space \text{and} \space f(c) = 0$ Part where I have difficulty understanding the proof . The author states: ""Since $c$ is the least upper bound of $K$, we can rule out the first two cases.
"" I don't understand how we can conclude this immediately? Thinking about it, if $f(c)>0$, then $c \notin K$ so this would contradict $c$ being a least upper bound.(Right?) However, how can we rule out $f(c)<0$? Here $c \in K$, which is a valid least upper bound?",['real-analysis']
1163940,Is it ever easier to show differentiability than continuity?,"I'm TAing a course right now in multivariable calculus and in the lecture notes the professor gave the students the theorem stating that differentiability implies continuity, as well as another theorem that says a function $F(x,y)$ is differentiable at a point $(a,b)$ if $\partial_x F$ and $\partial_y F$ exist and are continuous at $(a,b)$. I was trying to think of an example where the continuity of $F(x,y)$ is not obvious, but the continuity of the partial derivatives is more apparent and came up with nothing.",['multivariable-calculus']
1163959,Local extension of smooth funtion to a embedded manifold,"I'm trying to proof the following problem from Lee's Book: Suppose $M$ is a smooth manifold and $S\subseteq M$ is a smooth submanifold. Show that $S$ is embedded if and only if every $f\in C^\infty(S)$ has a smooth extension to a neighborhood of $S$ in M. I've shown that $S$ embedded $\Longrightarrow$ Every $f\in C^\infty(S)$ has a smooth extension to a neighborhood of $S$ in M. But the converse it's being hard to see. The book gives us a hint, but I have no ideia how this can be helpful. The hint is: If $S$ is not embedded, let $p\in S$ be a point that is not in the domain of any slice chart. Let $U$ be a neighborhood of $p$ in $S$ that is embedded, and consider a function $f\in C^\infty(S)$ that is supported in $U$ and equal to $1$ at $p$ . Maybe a contradiction argument be necessary, but I can't realise this. How can I prove this? Any kind of help will be useful.","['smooth-manifolds', 'functions', 'problem-solving']"
1163962,Does such a series exist?,"This question came up as some puzzle. Does there exist a sequence of real numbers ${c_j}$ such that $\sum{c_j^m} = m$ for all positive integers $m$? I argue no.  Suppose there exists such a sequence.  Then, we must have $\sum{c_j}^2 = 2$.  Thus, we need $|c_j| \leq \sqrt{2}$ for all $j$.  Similarly, we would need $\sum c_j^4 = 4$, and so $|c_j| \leq (4)^{\frac{1}{4}}$, for all $j$.  Continuing this procedure for all even $m$, we get that $|c_j| \leq 1$ for every $j$ from the limit. But now, if that is the case, we have that $c_j^2 \geq c_j^4$ for every $j$, and so $2 = \sum c_j^2 \geq \sum c_j^4 = 4$, a contradiction.  Thus, no such sequence can exist. Does this seem correct?  It seems strange that I only care about the even integers.  If correct, can you think of any other ways to solve this?","['sequences-and-series', 'calculus']"
1163980,Critical Points of a smooth map on SO(n),"I am given the following map
$f:SO(n) \rightarrow \mathbb{R}$, $f(X) = Tr(DX)$ where $D$ is a diagonal matrix $\{d_1,\ldots,d_n\}$, $1<d_1<\cdots <d_n$. I need to find the critical points of this map, and the index of $f$ at those points. If I am right, $df = f$ since $f$ is a linear map, but I can not find any critical point.
Do I need to use local charts for $SO(n)$?","['derivatives', 'differential-geometry']"
1164022,"If a nonempty subset of integers is bounded from below, it has a minimum","Let $A$ be a non-empty subset of $\mathbb Z$. Suppose there exists $s \in \mathbb Z$ such that $s \le a$, for all $a \in A$. Show that $A$ has a minimum. I was assuming induction would be used for this proof since that is what we just covered, but it doesn't seem to apply. The main thing throwing me off is that $s$ is in $\mathbb{Z}$ not $A$.","['elementary-number-theory', 'discrete-mathematics']"
1164028,Can an unstable limit cycle be contained directly within a stable one?,"Can both the alpha and omega point sets of a trajectory be part of two different limit cycles? I.e. can trajectories being 'repelled' from one limit cycle be pulled into an 'attracting' (stable) limit cycle? The reading I've done seems to indicate no, but I don't understand why.","['dynamical-systems', 'nonlinear-system', 'ordinary-differential-equations']"
1164031,"Show that if $b_k\uparrow \infty$ and $\Sigma_{k = 1}^\infty a_kb_k$ converges, then $b_m \Sigma_{k = m}^\infty a_k → 0$ as $m → \infty$.","Suppose that $\Sigma_{k=1}^\infty a_k$ converges. Prove that if $b_k\uparrow \infty$ and $\Sigma_{k = 1}^\infty a_kb_k$ converges, then $b_m \Sigma_{k = m}^\infty a_k → 0$ as $m → \infty$. Attemtp: Suppose $\Sigma_{k=1}^\infty a_k$ converges, and $\Sigma_{k = 1}^\infty a_kb_k$ also.Then we know by Abel's Formula that the sequences converge only iff its partial sums converge. If we let $\Sigma a_k = \Sigma \frac{a_kb_k}{b_k}$, then because $b_k$ is increasing we have $\frac{1}{b_k}$ is decreasing to zero. So we almost have a telescoping series. Then let $c_k = \Sigma_{ j = k}^{\infty} a_jb_j$. Then $\Sigma_{k = n}^m a_k = \Sigma \frac{a_kb_k}{b_k}= \Sigma_{k = n}^m \frac{c_k - c_{k+1}}{b_k}$ I don't know how to continue. I am having trouble applying Abel's Formula and the subscripts are confusing. Can someone please help me? I am suppose to use Abel's Formula.  Thank you very much. Abel's Formula: Let $a_k,b_k$ be real sequences, and for each pair of integers $n \geq m \geq 1$ set $A_{n,m} = \Sigma_{k =m}^n a_k$
$\Sigma_{k = n}^m a_kb_k = A_{n,m}b_n - \Sigma_{k = m}^{n-1} A_{k,m}(b_{k+1} -b_k)$","['convergence-divergence', 'sequences-and-series', 'real-analysis', 'analysis']"
1164035,Regular way to fill a $1\times1$ square with $\frac{1}{n}\times\frac{1}{n+1}$ rectangles?,"Locked . This question and its answers are locked because the question is off-topic but has historical significance. It is not currently accepting new answers or interactions. The series $$\sum_{n=1}^{\infty}\frac{1}{n(n+1)}=1$$ suggests it might be possible to tile a $1\times1$ square with nonrepeated rectangles of the form $\frac{1}{n}\times\frac{1}{n+1}$. Is there a known regular way to do this? Just playing and not having any specific algorithm, I got as far as the picture below, which serves more to get a feel for what I am looking for. I think some theory about Egyptian fractions would help. It's nice for instance in the center where $\frac13+\frac14+\frac16+\frac14=1$. And on the right edge where $\frac12+\frac13+\frac16=1$. Side note: The series is $\left(\frac11-\frac12\right)+\left(\frac12-\frac13\right)+\left(\frac13-\frac14\right)+\cdots$. The similar looking $\left(\frac11-\frac12\right)+\left(\frac13-\frac14\right)+\left(\frac15-\frac16\right)+\cdots$ sums to $\ln(2)$, and there is a nice picture for that, if you interpret $\ln(2)$ as an area under $y=
\frac{1}{x}$:","['egyptian-fractions', 'sequences-and-series', 'visualization']"
1164115,Product unbounded operators,"Let $A : D(A) \subset H \rightarrow H$ be unbounded and $B$ be a bounded operator, both of them are self-adjoint, then $(AB)^* = B^*A^*$ and $(BA)^* = A^*B^*$, right? I just wanted to be sure that I am correct about this.","['operator-theory', 'spectral-theory', 'real-analysis', 'analysis', 'functional-analysis']"
1164130,How to obtain the inverse of a function?,"I am trying to solve this $$f(x)= \frac{-5}  {x^2 + 1}$$ The solution is $f^{-1} = \pm\frac{\sqrt{-x-5}} {\sqrt{x}}$ I have done 
$$x = \frac{-5}  {y^2 + 1}$$
$${(x)(y^2 +1)} = {-5} $$
$${y^2 +1} = {\frac{-5}{x}} $$
$${y^2} = {\frac{-5}{x} - 1} $$
$${y} = \sqrt{\frac{-5}{x} - 1} $$ But I do not get the final answer as yo can see, I need to use the inverse to get the range of the original function,can someone please guide me in how to solve this exercise","['algebra-precalculus', 'functions']"
1164138,Ratio test with limsup vs lim,"Could I prove that the ratio test still works using $\limsup(\frac{a_{n+1}}{a_n})$ instead of $\lim(\frac{a_{n+1}}{a_n})$?
I think for $\limsup<1$ I could show that for $\epsilon>0, N>1 \limsup(\frac{a_{n+1}}{a_n})<1-\epsilon.$ From there I can solve that $\lvert a_k\rvert<(1-\epsilon)^{k-N}\lvert a_N\rvert$ for $k>N$. Thus, by comparison test the left-hand side will converge absolutely. Is this enough to show I can use the ratio test with $\limsup$?","['ratio', 'convergence-divergence', 'real-analysis', 'limsup-and-liminf']"
1164147,Spectral Measures: Stone's Formula,"Hilbert Space: $\mathcal{H}$ Hamiltonian:
$$H:\mathcal{D}(H)\to\mathcal{H}:\quad H=H^*$$ Spectral Measure:
$$E:\mathcal{B}(\mathbb{R})\to\mathcal{B}(\mathcal{H}):\quad H=\int\lambda\mathrm{d}E(\lambda)$$ Resolvent Map:
$$R(z):=(H-z)^{-1}=\int\frac{1}{\lambda-z}\operatorname{dE(\lambda)}$$ Stone's Formula:
  $$\frac12\{E[a,b]+E(a,b)\}\\
=\lim_{\varepsilon\to0^+}\frac{1}{2\pi i}\int_a^b\{R(s+i\varepsilon)-R(s-i\varepsilon)\}\operatorname{ds}$$ (Bounded Interval!) How to check this by spectral measures?","['operator-theory', 'spectral-theory', 'hilbert-spaces', 'functional-analysis']"
1164151,"Theory and problems book in euclidean, affine, and projective geometry","Could you recommend a rich , clear , and complete theory book on euclidean, affine and projective spaces (i.e., ""geometry""); and an interesting exercise book full of non-trivial problems and exercises ?","['geometry', 'projective-geometry', 'affine-geometry', 'reference-request', 'soft-question']"
1164183,"How to evaluate $I=\int\limits_0^{\pi/2}\frac{x\log{\sin{(x)}}}{\sin(x)}\,dx$","Prima facie, this integral seems easy to calculate,but alas, this not's case $$I=\int\limits_0^{\pi/2}\frac{x\log{\sin{(x)}}}{\sin(x)}\,dx$$
 The numerical value is I=-1.122690024730644497584272...
 How to evaluate this integral?
By against,I find:
 $$I=\int\limits_0^{\pi/2}\frac{x\log{\sin{(x)}}}{\sin(2x)}\,dx=-\frac{\pi^3}{48}$$","['improper-integrals', 'closed-form', 'integration', 'definite-integrals', 'trigonometric-integrals']"
1164196,integral of complex conjugate times the differential is purely imaginary,"I am trying to figure out why $\int_C\overline{f(z)}f'(z)dz$ is purely imaginary (C is a closed curve).  I was told to use the Cauchy-Riemann equations to show that the real part of the integrand is an exact differential. I wrote down that $\int_C\overline{f(z)}f'(z)dz = \int_C(u(x,y)-v(x,y))(\frac{du}{dx}+\frac{dv}{dx}) $ The real part of this is $u(x,y)\frac{du}{dx}+v(x,y)\frac{dv}{dx}$.  I can apply the C-R equations, but I don't see how that would make this part dissapear. EDIT: For clarification, $f(z)$ is analytic in the region in which C lies. EDIT2: So integrals of exact differentials over closed curves always vanish.  This means that I need to prove that $u(x,y)\frac{du}{dx}+v(x,y)\frac{dv}{dx}$ is a exact differential.",['complex-analysis']
1164202,Elementary tensors [duplicate],"This question already has answers here : Showing that a function is a tensor (2 answers) Closed 9 years ago . I need to determine whether the following function is tensor on $\Bbb R^4$ and express it in terms of elementary tensors. Can someone please help me with it? I do not know what elementary tensor means either. 
$$f(x,y)=3x_1y_2+5x_2 x_3$$ Thanks in advance!","['differential-geometry', 'manifolds', 'functions', 'linear-algebra', 'tensors']"
1164208,Spectral Measures: Helffer-Sjöstrand,"Given a Hilbert space $\mathcal{H}$. Consider a Hamiltonian:
$$H:\mathcal{D}(H)\to\mathcal{H}:\quad H=H^*$$ Regard a function:
$$f\in\mathcal{C}^\infty_0(\mathbb{R}):\quad f(\mathbb{R})\subseteq\mathbb{R}$$ And an extension:
$$f_E\in\mathcal{C}^\infty_0(\mathbb{C}):\quad f_E\restriction_\mathbb{R}=f\quad\bar{\partial}f_E\restriction=0$$ Then one has:
$$f(H)=-\frac{1}{\pi}\int_\mathbb{C}\overline{\partial}f_E(z)R(z)\mathrm{d}\lambda_\mathbb{C}(z)$$
How can I check this?","['operator-theory', 'spectral-theory', 'hilbert-spaces', 'functional-analysis']"
1164336,Is this a valid way of proving the well ordering principle?,"The well ordering principle states that every non-empty subset of $\mathbb{N}$ must have a first element (i.e. a minimum). For my proof, I suppose there exists a set $\varnothing \neq S \subseteq \mathbb{N}$ that doesn't have a minimum. I, then, consider the set $R = \mathbb{N} - S = \{x\in \mathbb{N} \; \vert \; x\notin S\}$. The proof follows with proving $R = \mathbb{N}$ via induction: $0\in R$, because, otherwise, $S$ would have a minimum (because $0\leq n \; \forall n \in  \mathbb{N}$). I suppose that every $k<n$ is in $R$ (i.e. every $k<n$ isn't in $S$). If $n$ was to be in $S$, $S$ would have a minimal element. Therefore, $n \in R$. This proves that $R= \mathbb{N}$, it follows that $S = \varnothing$. Absurd . Is this a valid proof?","['elementary-set-theory', 'proof-verification', 'order-theory']"
1164342,Combinatorics question with locks.,"I have a question regarding the following problem. A crime detective is able to dust a lock for finger prints and he determines that the numbers: 3,4,5 and 8 have been pressed repeatedly and the other numbers have not.  If the lock is a standard 10 digit number pad, and the combination contains 5 places, how many different possible password could he have to try before finding the correct one? The following is my claim. It is natural to think that the four numbers are pressed repeatedly, therefore each numbers are used at least once. So, using the fundamental counting principle with 5 places, I would say the first number could be any of the 4 numbers. This is where I get iffy. The next number may or may not be the repetition of the previous, so I am not sure whether to call it 4 choices or 3 choices. Ignoring my confidence, I think the answer is $4*4!$ which implies the fact that there are 24 combinations of numbers with 4 choices of numbers that could have been repeated. What concerns me is the position of the repeated number, though. I know that the combinations of the lock is sensitive to order, so I am not sure if the second number is repeated, third number is repeated, etc.... and I cannot tell numerically if that makes a difference or not. Can I have some confirmation?","['algebra-precalculus', 'probability', 'combinatorics']"
1164362,What is the trick to identify which of these are true,I was hoping to not have to create a bunch of fictional sets so that I can solve this problem. Any trick or rules to this? The sets A and B are subsets of a universal set U. Which of the following relations is always true? (a) $A'∩ B' ⊂ (A ∩ B)'$ (b) $B ∩ A ⊂ B$ (c) $(A ∪ B) ⊂ A' ∩ B'$ (d) $A ∪ B ⊂ A ∩ B$,['elementary-set-theory']
1164398,Why is the derivative of a vector orthogonal to the vector itself?,"$R(t) \cdot R'(t) = 0$, which is what every source I can find tells me. Even though I understand the proof I don't understand the underlying concept. If $R(t)\cdot R'(t) = 0$, then $R'(t)$ is orthogonal to $R(t)$, right? But you use the same derivative to find the tangent of a curve. Then somehow if you differentiate the tangent itself, you get the normal to the curve. I really can't wrap my head around this. Could someone help me understand?","['orthogonality', 'calculus', 'vectors', 'derivatives']"
1164443,How to come up with relation in induction hypothesis for strong induction,"Note: This problem is from Discrete Mathematics and Its Applications [7th ed, prob 2, page 341] . Problem: Let $P(n)$ be the statement that a postage of n cents can be formed using just 4-cent stamps and 7-cent stamps. The parts of this exercise outline a strong induction proof that $P(n)$ is true for $n \geq 18$ . a) Show statements $P(18)$ , $P(19)$ , $P(20)$ , $P(21)$ are true, completing the basis step of the proof b) What is the inductive hypothesis of the proof? c) What do you need to prove in the inductive step? d) Complete the inductive step for $k \geq 21$ . I am currently working on 4d. I am trying to apply what I learned from Brian M. Scott Strong Induction My work(off Brian's Model): $P(n)$ is the assertion that a postage of n cents can be formed using just 4-cent stamps and 7-cent stamps. I' am given $P(18), P(19), P(20)$ , and $P(21)$ to get the induction started. Now I assume for some $n \geq 21$ , $P(k)$ is true for each $k \leq n$ . This is my induction hypothesis and my task in the induction step is to prove $P(n+1)$ So I have to prove $n + 1 = 7d + 4c$ , with d and c being some natural number. Where would I go from here? What Brian did in the last problem was use the relationship that a domino causes the domino three after it to fall to show that the n+1 domino has a domino three before it that is in the assumption. How would you apply this idea here? There isn't really that sort of relationship here except except if you consider 18, 19, 20, and 21 are separated by one. Would you use that?","['proof-writing', 'discrete-mathematics', 'proof-verification', 'induction']"
1164448,$\exists x P(x)\land\exists x Q(x)$ is not logically equivalent to $\exists x(P(x)\land Q(x) )$,"The textbook states that the solution is: Let P(x) be ""x is positive"" and Q(x) is ""x is negative"". The domain is
  integers. This shows $\exists x P(x)\land\exists x Q(x)$ is True and shows $\exists x(P(x)\land Q(x) )$ is False. I take this to mean that $\exists x P(x)\land\exists x Q(x)$ is translated into English as ""There exist positive integers and there exist negative integers"", which is obviously true. I think that $\exists x(P(x)\land Q(x) )$ means ""There exists an integer that is positive and negative"" which is False. Does all this mean what I think it means?","['logic', 'quantifiers', 'discrete-mathematics']"
1164510,About idempotent and invertible matrix,"So I get a square matrix $A$ is idempotent when $A^2 = A$. How can you prove that if $A$ is an $n \times n$ matrix that is idempotent and invertible, then 
$A = I$?? I'm quite confused on this. Any help much appreciated, Thanks.","['matrices', 'linear-algebra']"
1164603,Series $\frac{1}{4}+\frac{1\cdot 3}{4\cdot 6}+\frac{1\cdot 3\cdot 5}{4\cdot 6\cdot 8}+\cdots$,"Find the sum of the series to infinity$$\frac{1}{4}+\frac{1\cdot 3}{4\cdot 6}+\frac{1\cdot 3\cdot 5}{4\cdot 6\cdot 8}+\cdots$$ Attempt- I wrote the general term as $$\frac{\binom{2n}{n}}{2^{2n}\cdot (n+1)}$$
I don't know what to do next",['sequences-and-series']
1164660,Identifying the cotangent bundle of the flag variety,"Suppose $G$ is a Lie group (or I guess a linear algebraic group), $P \subset G$ a Lie subgroup with Lie algebras $\mathfrak{g}$ and $\mathfrak{p}$ respectively. In Chriss and Ginzburg's book ""Representation theory and complex geometry"" they have the following result (Lemma 1.4.9): There is a natural $G$-equivariant isomorphism
  $$ T^*(G/P) \cong G \times_P \mathfrak{p}^\perp $$
  where $\mathfrak{p}^\perp$ is the annihilator of $\mathfrak{p}$ in $\mathfrak{g}^*$ and $P$ acts on $\mathfrak{p}^\perp$ by the coadjoint action. They give a proof however I don't understand it at all. I will will reproduce it for convenience below (with some annotations that are my own!). I was hoping someone could help me fill in the details. I am actually interested in the algebraic version of the result but help understanding the proof in either the differential category or the algebraic one would be very useful! Proof . Let $e = 1 \cdot P/P \in G/P$ be the base point. We have $T_e(G/P) = \mathfrak{g}/\mathfrak{p}$ and $T_e^*(G/P) = (\mathfrak{g}/\mathfrak{p})^* = \mathfrak{p}^\perp \subset \mathfrak{g}^*$. ( NOTE: ok up to here I understand, but the rest I have no idea ). It follows that, for any $g \in G$
$$T_{g \cdot e}^*(G/P) = g\mathfrak{p}^\perp g^{-1}. $$ Question 1: So I understand that somehow I am meant to be able to transfer the tangent space at $e$ to the point $g \cdot e$ using the $G$ action but I'm unsure how to write this down. This shows that the vector bundles $T^*(G/P)$ and $ G \times_P \mathfrak{p}^\perp $ have the same fibers at each point of $G/P$, hence are equal as sets ( NOTE: ok I am happy with this if I can understand the above ). To prove that they are isomorphic as manifolds, one can refine the argument as follows. Consider the trivial bundle $\mathfrak{g}_{G/P} = G/P \times \mathfrak{g}$ on $G/P$ with fiber $\mathfrak{g}$. The infinitesimal $\mathfrak{g}$-action on $G/P$ gives rise to a vector bundle morphism $\mathfrak{g}_{G/P} \longrightarrow T(G/P)$. Question 2: I understand that each element of the Lie algebra $X \in \mathfrak{g}$ gives rise to a vector field $\xi_X$, is the image of $(gP,X)$ under this map simply $\xi_X(gP)$? It is clear that the kernel of this morphism is the sub bundle $E \subset \mathfrak{g}_{G/P}$ whose fiber at a point $x \in G/P$ is the isotropy Lie algebra $\mathfrak{p}_x \subset \mathfrak{g}$ at $x$. This gives an isomorphism $T(G/P) \cong \mathfrak{g}_{G/P}/E$. Question 3: I don't know what is meant by the isotropy Lie algebra at the point but if I am correct in question 2 I would guess this is simply the tautological thing  - i.e. the elements of the lie algebra that act infinitesimally by zero? Further, the description of the fibers of $E$ gives an isomorphism $E \cong G \times_P (\mathfrak{g}/\mathfrak{p})$. Question 4: I don't understand this at all Hence, $T(G/P) \cong G \times_P (\mathfrak{g}/\mathfrak{p})$, Question 5: Didn't we just say that $T(G/P)$ was a quotient by this bundle? and the result follows by taking duals on each side. ( NOTE: That I am happy with ).","['differential-geometry', 'algebraic-geometry', 'representation-theory', 'algebraic-groups', 'lie-groups']"
1164670,"$k$ such that $n,2n,\dots,kn$ have odd sum of digits","is the following statement true or not? for any $k\in\mathbb{N}$ there exists $n\in\mathbb{N}$ such that all numbers $n,2n,\dots,kn$ have odd sum of digits? I have no idea... it may turn out hard (for even sum of digits it would be very easy)","['elementary-number-theory', 'contest-math', 'number-theory']"
1164679,Probabilistic method: vertex disjoint cycles in digraphs,"Let us say that a di-graph is $k$-regular if every vertex has precisely $k$ out-edges. The following theorem appears in a book I am currently studying Theorem. Every $k$-regular graph $D$ has a collection of $r = \lfloor k/(3 \log{k}) \rfloor$ vertex-disjoint cycles. The proof in the book goes as follows. Color the vertices of $D$ choosing colors from $\{1,\ldots,r\}$ uniformly at random. For a vertex $v \in V(D)$ define the event $A_v$ that $v$ does not have any out-neighbor of the same color. The author then claims it is enough to show $$Pr[\cap_{v \in V(D)} \overline{A}_v] > 0,$$ and argues how to derive this bound using the Lovasz Local Lemma. What I am wondering is: Is there any reason we are disregarding to estimate the probability that
  each color $r$ is represented in the coloring of $D$?","['graph-theory', 'probabilistic-method', 'probability', 'combinatorics']"
1164683,Question about right and left cosets.,"I want to do a question about how my algebraic structures professor defined left and right cosets. I'll write here his way to present them. We first talked about quotient group. Let $G$ be a group, $H\leq G$, and we want to build $G\ /\ H$. We look the particular case of $\mathbb{Z}\ /\ n\mathbb{Z}$ to make the generalization. After a little explanation of the last group, he defined two relations: $\sim$ and $\approx$: Let $G$ be a group. Let $H\leq G$. Let $g_1$ and $g_2$ $\in G$. We say that 
\begin{equation}
g_1\sim g_2\ \ \text{ if }\ \  g_1\ g_2^{-1}\in H,
\end{equation} and \begin{equation}
g_1\approx g_2\ \ \text{ if }\ \   g_2^{-1}g_1\ \in H.
\end{equation} After that, we proved that they are equivalence relations, and then he defined the quotient groups on whom we were interested: \begin{equation}
G\ / \sim\  =  G\ /\ H,\\
G\ / \approx\  = H\ \backslash\ G.
\end{equation} We say that $G\ /\ H$ is the set of the right equivalence classes (I think that in english it's called right coset), and then $H\ \backslash\ G$ is the left coset. Now it comes the part that I don't understand: Let $g \in G\ /\ H$. The equivalence class of g is: \begin{equation}
[g]=\{ g'\in\  G \mid  g' \sim g \}= \{ g' \in\ G \mid (g')^{-1} \in\ H\}=\{ g' \in\ G \mid g\in Hg \}= Hg
\end{equation} The equivalence classes of the elements of $H\ \backslash\ G$ are similar. My question is: how he can say that \begin{equation}
\{ g'\in\  G \mid  g' \sim g \}= \{ g' \in\ G \mid (g')^{-1} \in\ H\}?
\end{equation} As far as I'm concerned, $\ g'\sim g \implies g'g^{-1} \in\ H$. He can say from this that $(g')^{-1}\! \in H$? Sorry about this long explanation, but I wanted you to know how my professor deduced these quotient groups, because I haven't seen it in any group theory book. I hope you understood it clearly, despite my english. Thank you!","['equivalence-relations', 'group-theory', 'abstract-algebra']"
1164692,Prove that a sequence $a_n$ converges iff $a_n^3$ converges,"I want to prove that $A\ sequence\ a_n\ converges\ \longleftrightarrow\ a_n^3\ converges$ If $a_n$ converges, then by arithmetics, $a_n^3$ converges. Now let $a_n^3$ converge to a real $L$. Take the function $f(x) = \sqrt[3]{x}$, which is well defined for each $x \in \mathbb{R}$. Then we know that $\forall n\in \mathbb{N}, f(a_n^3) = \sqrt[3]{a_n^3} = a_n$. $f$ is continuous, so by taking the limit: $\lim_{n\to\infty}a_n = \lim_{n\to\infty}f(a_n^3) = f(\lim_{n\to\infty}a_n^3) = f(L) = \sqrt[3]{L}$. Does this proof hold?
And also, I was wondering how can you prove it from definition, using $\epsilon$ notation? Thanks!","['sequences-and-series', 'calculus', 'proof-verification', 'limits']"
1164710,Linearity of convergence in probability,"I am trying to prove the following statement. Let $X_n \rightarrow X$ and $Y_n \rightarrow Y$, both in probability. Then $aX_n + bY_n \rightarrow aX + bY$ in probability for  $a,b \in \mathbb{R}$ s.t. $a,b \neq 0$. The statement I need to prove is
$$\forall{\varepsilon} >0 \qquad \lim_{n\rightarrow\infty} P\{\lvert aX_n + bY_n - aX - bY \rvert > \varepsilon\} = 0$$ Fix $\varepsilon > 0$. Note that
$$\lvert aX_n + bY_n - aX - bY \rvert \leq \lvert a\rvert\lvert X_n - X\rvert + \lvert b\rvert\lvert Y_n - Y\rvert$$
Therefore
$$P\{\lvert aX_n + bY_n - aX - bY \rvert > \varepsilon\} \leq P\{\lvert a\rvert\lvert X_n - X\rvert + \lvert b\rvert\lvert Y_n - Y\rvert > \varepsilon\}$$ Here comes the part which I feel uncomfortable about. $$P\{\lvert a\rvert\lvert X_n - X\rvert + \lvert b\rvert\lvert Y_n - Y\rvert > \varepsilon\} = P\{\lvert a\rvert\lvert X_n - X\rvert \vee \lvert b\rvert\lvert Y_n - Y\rvert > \frac{\varepsilon}{2}\}$$
Now I let $n$ go to infinity and claim that the RHS goes to $0$. I base this claim on the intuition that $\lvert X_n - X\rvert$ and $\lvert Y_n - Y\rvert$ converge in probability to $0$. Therefore their ""worst case"" combination must do the same. Could someone help me fix this proof or post a better one? Thanks a lot.","['convergence-divergence', 'probability', 'proof-verification']"
1164779,Uniformly continuous maps between topological groups,"Let $G$ be a topological group. For every neighbourhood $U$ of the identity, let $L_U$ be the set of all pairs $(x,y) \in G \times G$ such that $x^{-1} y \in U$. For topological groups $G$ and $H$, a map $f \colon G \to H$ is called left uniformly continuous if for every open neighbourhood $V$ of the identity in $H$ there exists an open neighbourhood $U$ of the identity in $G$ such that $(f(x),f(y)) \in L_V$ for all $(x,y) \in L_U$. Is the map $G \times G \to G$, $(x,y) \mapsto x^{-1}y$ left uniformly continuous?
That is for any open neighbourhood $W$ of the identity does there exists neighbourhoods $U,V$ of the identity such that $(x_1^{-1}x_2, y_1^{-1}y_2) \in L_W$ whenever $(x_1,x_2,y_1,y_2) \in L_{U \times V}$. If not, is there a nice example to show this is not true?","['topological-groups', 'continuity', 'analysis', 'group-theory', 'uniform-continuity']"
1164793,Rational parametrization of algebraic variety,"Suppose I have some algebraic variety (i. e. solution of system of polynomial equations). Sometimes I can find rational parametrization of it - for example, it case of circle defined by $x^2 + y^2 - 1 = 0$ I can parametrize all solutions $x = \frac{2t}{1+t^2}, y = \frac{1-t^2}{1+t^2}.$ Sometimes I can't. So does algebraic geometry (or maybe other mathematics) provide general setting for decision whether or not given algebraic variety (not only curve or surface) has rational parametrisation? Or these varieties are exactly rational varieties ? If so, it's not clear for me, so could anybody explain the link between this object and my question.",['algebraic-geometry']
1164803,Taylor series for $\frac{1}{(1+x)^t}$,"I'm having some trouble finding the Taylor series for the following function at zero (Maclaurin series).
\begin{equation}
\frac{1}{(1+x)^t}
\end{equation}
Where $t$ is a constant that is greater than zero.","['sequences-and-series', 'calculus', 'real-analysis', 'derivatives', 'taylor-expansion']"
1164823,Computing the exterior derivative of a wedge product,How can we prove the following relation for differentiating the wedge product of a p-form $\alpha_p$ and a q-form $\beta_q$ ? $$d(\alpha_p\wedge\beta _q)=d\alpha_p\wedge\beta_q+(-1)^{p}\alpha_p\wedge d\beta_q$$,['differential-geometry']
1164877,Don't understand proof that interior of a set is open,"Say we want to show that the interior of a set $A$ is open. If $x \in Int(A)$ , then there exists an open ball $B_r(x) \subseteq A$ . Since $B_r(x)$ is open, $y \in B_r(x)$ also has an open ball $B_s(y) \subseteq B_r(x) \subseteq A$ , so $y \in Int(A)$ . Now, somehow we have to show that the ball $B_r(x) \subseteq Int(A)$ , and that would complete the proof. All of the proofs I read say this is obvious, but I don't see how $B_r(x) \subseteq Int(A)$ immediately follows here.","['general-topology', 'elementary-set-theory', 'real-analysis', 'solution-verification']"
1164921,Intersection of 8 spheres: find the volume,"From a long time ago, I remember a puzzle asking for the common area available to four cows: each cow is attached to a different corner of a square with a rope that has the same length as the sides of the square. One of several 'cow problems' and in this case, it's just the area of intersection of four unit circles, with centers on the four corners of a unit square. Through either geometry or calculus, the area is found to be $1+\pi/3-\sqrt{3}$. The higher-dimensional analogue could be to ask for the volume of intersection of eight unit spheres, with centers on the eight corners of a unit cube. You could describe it as the 'fly zone' of eight flies, attached to... etc. Geometry-wise it's not as simple to sketch/imagine the solid in question and calculus-wise, it's not that simple to set up the right integral. At least not to me, any ideas? Or is this a known problem, if someone has a reference? Addendum : and I guess you could even try generalizing this to the volume of intersection of hyperspheres on the vertices of a hypercube, but I'd already be happy with some input on the 3D-case :-).","['geometry', 'puzzle', 'calculus']"
1164929,The automorphism group of Prüfer group,"Let $G = C_{p^\infty}$ be the Prüfer group, where $p$ is a prime. Is clearly that $G$ is a torsion group. In general, if $G$ is a torsion group, it is not true that $Aut(G)$ is a torsion group. But in the specific case where $G = C_{p^\infty}$, is true that $Aut(G)$ is a torsion group? Remark: $Aut(G)$ is the group of automorphisms of $G$.",['group-theory']
1164954,Complex exponential has $1$ as Lipschitz constant.,"(In the following, Lipschitz constant does not mean ""best Lipschitz constant"".) I've just read this in a book that I highly regard: Moreover, by mean value theorem, $u\to e^{iu}$ is Lipschitz continuous with Lipschitz constant $1$. How does the author infer this from the mean value theorem ? The theorem applies only for real-valued functions. The mean value theorem shows nontheless that $\sin$ and $\cos$ have Lipschitz constant $1$, and therefore $u\to e^{iu}$ has Lipschitz constant $2$. How can this be lowered  to $1$ ?","['lipschitz-functions', 'real-analysis']"
1164973,"For which values of $a$, $b$ and $c$, if $a + b = c$, then $\frac{1}{a} + \frac{1}{b} = \frac{1}{c}$?","I have a problem in my homework, which I have tried to solve, but I have just ideas, no real mathematical solutions. The problem is the following: Suppose we have three real numbers $a$, $b$, and $c$ which satisfy the
  equation: $$a + b = c$$ Is it then also true that: $$\frac{1}{a} + \frac{1}{b} = \frac{1}{c}$$ or not? Or is it only true for some particular choice of $a$, $b$, and
  $c$, and which would that be? My ideas: I noticed immediately that all $a$, $b$ and $c$ must be different from $0$, because otherwise we would have $\frac{1}{0}$ in the second equation, and that's not defined, as everybody knows. I tried to form a system of equations with the equations given in the specification of the problem: $$\begin{cases} a + b = c \\ \frac{1}{a} + \frac{1}{b} = \frac{1}{c} \end{cases}$$ Since we have 3 variables ($a$, $b$ and $c$), I am not sure if this system of equations can bring me to some solution. I have tried to replace $a + b$ in the second equation: $$b(a + b) + a(a + b) = ab$$
$$ba + b^2 + a^2 + ba = ab$$ We can simplify to: $$ba + b^2 + a^2 = 0$$ Now, I would not know how to proceed, and sincerely I don't know if my solution (ideas) is correct or not, or how far is it from the real solution. My guess is that there's no values for $a$, $b$ or $c$ such that the 2 equations are valid.",['calculus']
1164977,How to prove the infinite number of sides in a circle?,"I was in geometry class today when I came across the following formula for the external angle of a regular polygon with n sides:
$$Ea = \frac{360º}{n}$$ So I thought if $$ n\rightarrow\infty $$ then $$ Ea\rightarrow0$$
Thus if a circle is a polygon with an infinite number of sides it's external angles would approach 0.
I then tried to do the reverse way, trying to figure out a way to prove the premise that a circle has n = infinity; however I could not prove it. In this sense, how do you prove the infinite number of sides in a perfect circle?","['geometry', 'infinity', 'circles']"
1165034,How to show that the topology is compatible with the metric?,"This is in the contest of Toplogical-Vector-Spaces, but can be interepreted as a simply topology question. For my matter, assume $\|\cdot\|_n$ is a countable family of seminorms, and define $$d(x,y)=\sum 2^{-n} \frac{\|x-y\|_n}{1+\|x-y\|_n}.$$ It is quite easy to see that this is a metric, and I want to show that it is compatible with the topology generated by this local-sub-base at $0$: $$\left\{x\in X\mid\:\|x\|_i < \frac{1}{n}\right\}\text{ (for all }i,n\text{)}$$ How am I supposed to do this? By definition, I believe i'm supposed to show that they have the same open sets, but pointing out some kind of function between those doesn't seem right to me. Thanks!","['general-topology', 'topological-vector-spaces', 'functional-analysis']"
1165053,Limit of naturals at infinity and uniform continuity,"I need to prove the following: Let $ f:\mathbb{R}\to\mathbb{R}$ be continuous. $\lim_{n\to\infty}f(n) = \infty$ , $n\in\mathbb{N}$ . Let $f$ be uniformly continuous, prove that $\lim_{x\to\infty}f(x)=\infty,\ x\in\mathbb{R}$ It's clear to me why uniform continuity is required, for example: $f(x) = xsin(\frac{(4x-3)\pi}{2})$ $\forall n\in\mathbb{N},\ f(n) = n$ But clearly $f$ does not tend to infinity for all $x$ . I just can't get the uniform continuity into anything that will get me closer to solving this. Help please!","['uniform-continuity', 'calculus', 'limits']"
1165063,"Numbers $m,n$, such that $a^m+ab+b^n$ is always composite","Are there integers $m,n\ge 2$, such that $$a^m+ab+b^n$$ is composite for all
 integers $a,b\ge 2$ ? I checked the pairs with $2\le m\le 100$ and $2\le n\le 100$ and always found a prime
 of the form $a^m+ab+b^n\ ,\ a,b\ge 2$ Due to symmetry, $m\le n$ can be assumed.","['prime-numbers', 'number-theory']"
1165066,Fubini's theorem for conditional expectations,"I need to prove that if $E \int_a^b |X_u|\,du = \int_a^b E|X_u|\,du$ is finite then: $$E\left[\left.\int_a^b X_u\,du \;\right|\; \mathcal{G}\right] = \int_a^b E[X_u \mid \mathcal{G}]\,du.$$ I just dont have any idea how to approach this problem.","['probability-theory', 'stochastic-processes', 'conditional-expectation', 'probability']"
1165090,"a, b, c, d are reals and a < b < c < d. express the set $[a,c] \cap [b, d]$ as difference of two intervals.","I'm struggling to solve the problem stated above. To help clarify the question I let a = 1, b = 2, c = 3,and d = 4. If that were the case then the interval I  am interested is [b, c]. What does it mean to express that as the difference of two intervals?",['discrete-mathematics']
1165094,geometric proof of $2\cos{A}\cos{B}=\cos{(A+B)}+\cos{(A-B)}$,"I have seen geometric proof of identities
$$\cos{(A+B)}=\cos{A}\cos{B}-\sin{A}\sin{B}$$
and
$$\cos{(A-B)}=\cos{A}\cos{B}+\sin{A}\sin{B}$$ By adding two equation, $$2\cos{A}\cos{B}=\cos{(A+B)}+\cos{(A-B)}$$. But how to prove this by geometry? Thank you.","['geometry', 'trigonometry']"
1165126,Find a linear operator s.t. $A^{2}=A^{3}$ but $A^{2}\neq A$?,"From Halmos's Finite-Dimensional Vector Spaces, question 6a section 43, the section after projections. Find a linear transformation A such that $A^{2}(1-A)=0$ but A is not idempotent (I remember A is idempotent iff it is a projection). I had no luck.","['matrices', 'linear-algebra', 'examples-counterexamples']"
1165134,Getting a specific formula for a sequence.,"If: $$a_0 = \frac{5}{2}, a_k = a_{k-1}^{2} - 2$$ for $k \ge 1$. How do I get a general formula for $a_k$? With induction proof. I even tried calculating $a_1, a_2 ...$: $$a_0 = \frac{5}{2}$$ $$a_1 = \frac{17}{4}$$ $$a_2 = \frac{273}{16}$$ $$a_3 = \frac{74017}{256}$$ I will treat the numerator and denominator seperately. I see that for the denominator. $$d = 2^{2^k}$$ Now to the numerator: I cant get it. I tried, $$2^{2^{k}} + 1$$ but it doesnt work for $k=2$. But it is shifted $1$, meaning for $n=2$, I got the value of $n=1$.","['sequences-and-series', 'calculus', 'contest-math', 'real-analysis', 'analysis']"
1165181,"Suppose that $f : X \to Y$ is a one-to-one function and $A, B\subset X$ with $f(A) = f(B)$. Then $A = B$.",Suppose that $f:X\rightarrow Y$ is a one-to-one function and $A$ and $B$ are subsets of $X$ such that $f(A) = f(B)$. Then $A = B$. I either have to prove if this is true or give a counter example if this is false... suggestions on where to start would be helpful!,['elementary-set-theory']
