question_id,title,body,tags
909228,Infinite Series $\sum_{n=1}^\infty\frac{H_n}{n^32^n}$,"I'm trying to find a closed form for the following sum
$$\sum_{n=1}^\infty\frac{H_n}{n^3\,2^n},$$
where $H_n=\displaystyle\sum_{k=1}^n\frac{1}{k}$ is a harmonic number. Could you help me with it?","['closed-form', 'sequences-and-series', 'zeta-functions', 'harmonic-numbers', 'real-analysis']"
909233,If $G/K\cong H/K$ must $G\cong H$?,"Let K be a normal subgroup of $H$ and $G$ so that $G/K$ is isomorphic to $H/K$, must we have $G\cong H$? I can't really tell you what I have tries since I haven't really done anything worth telling. Thanks in advance.","['group-theory', 'abstract-algebra']"
909239,What is the relationship between the trace/norm of a quaternion and the definition in field theory?,"I'm having some trouble figuring out the relationship between the trace/norm of a quaternion element and the definition of trace/norm in the extensions of vector spaces. According to my number theory textbook, the Norm of a quaternion $\alpha=a+bi+cj+dk$ is $N\alpha=a^2+b^2+c^2+d^2$ and the trace of $\alpha$ is $Tr(\alpha)=2a$. I've been trying to relate this to the definition given by another book on field theory, which says we can create a linear map that represents multiplication by a quaternion element such as the matrix $$m = \left(\begin{matrix} a & b &c&d\\ -b&a&-d&c\\-c&d&a&-b\\-d&-c&b&a \end{matrix}\right)$$ and that if $m:V\rightarrow V$ is a linear operator on a finite dimentional vector space V over F, the trace of $m$ is the trace of the matrix $m$ and the norm is the norm of the matrix $m$. However, the trace and norm of this matrix are inconsistent with the first definition. (The trace of the matrix is $4a$ and the norm is the $(a^2+b^2+c^2+d^2)^2$. Is one of my definitions incorrect, or is there something more to it?","['vector-spaces', 'extension-field', 'abstract-algebra', 'algebraic-number-theory', 'field-theory']"
909242,Self studying higher mathematics?,I'm fairly well-versed in calculus but I would like to explore beyond calculus. I have looked into the basics of some topics in higher mathematics such as group theory and abstract algebra and they intrigue me. I am wondering if there are any recommended methods or resources I should use to learn more about these topics. And is there any recommended starting point?,"['soft-question', 'group-theory', 'abstract-algebra']"
909276,$\mathbb{P}(O\oplus O(-1))\simeq \mathbb{P}^1\times \mathbb{P}^1$?,"Let $X=\mathbb{P}^1$. I am looking at $\mathbb{P}(O_X\oplus O_X(-1))$ and can see that it is the blow up of the projective plane at one point. I also see that it is a $\mathbb{P}^1$-bundle over $X$, but can't quite see if it's isomorphic to $\mathbb{P}^1\times \mathbb{P}^1$ or not. Can anyone give some insight?",['algebraic-geometry']
909288,Why is $W_n(k)$ the unique flat lifting of a perfect field $k$ over $\mathbf{Z}/p^n$?,"Let $k$ be a perfect field of characteristic $p>0$ and denote by $W_n(k)$ the ring of Witt vectors over $k$ of length $n$. In their article on the decomposition of the de Rham complex, Deligne and Illusie claim that $W_n(k)$ is the unique flat lifting of $k$ over $\mathbf{Z}/p^n$ (1.3). I know how to justify that $W_n(k)$ satisfies these properties, but it is not clear to me why it should be (up to isomorphism) the only one satisfying them. Translating this into commutative algebra, we need to prove that if $A$ is a flat $\mathbf{Z}/p^n$-algebra such that its reduction modulo $p$ is isomorphic to $k$, then we have $A \cong W_n(k)$. Could someone shed a light on this?","['commutative-algebra', 'algebraic-geometry', 'positive-characteristic']"
909301,How to solve $e^x=x$?,"I mean, how to solve $e^x=x$ in the complex plane? Is there a solution?
I know there is no real solution to this because apparently, $y=x$ and y= $e^x$ have no interception.",['algebra-precalculus']
909319,Find circles that completely cover a polygon minimizing the amount of space covered outside the polygon,I have an arbitrary polygon that I need to roughly represent using circles. Any point inside the polygon must lie inside a circle. There will be points outside the polygon that will fall under a circle. I want to minimize this as much as possible. I believe there will be a trade off between the number of circles used and the amount of space covered outside the polygon. I am limited to the number of circles I can use. It will be in the order of 1 to 10. Ideally I would like an algorithm that takes the number of circles and gives me ideal positions and radius' Here is the simplest solution. Place a circle at the center of the polygon with a radius equal to the furthest point from the center. It looks something like this: Ideally I would like something like this: Is there an algorithm to solve a problem like this? Does this problem have a name? For context I am working with GeoFences which can be defined as points with a radius. The solution needs to know when you enter the polygon.,"['geometry', 'polygons', 'circles']"
909326,Heuristic for Dirichlet's Theorem on Arithemtic Progression,"If we let $\pi_{a,d}(x) = \{p \leq x: p \mbox{ prime, } p \equiv a \bmod d\}$ then it is a well known result that if $(a,d)=1$ then $$\lim_{x \to \infty} \frac{\pi_{a,d}(x)}{\pi(x)} = \frac{1}{\phi(d)}$$ That is, the primes fall evenly into each congruence class mod $d$ that infinitely many of them can fall into. My question is, aside from the proof of this fact, why would one think this should be true? Are there any simple heuristics that would get this? Or, is there a heuristic that would suggest primes favour one congruence class over another but fails one way or another? Any reference or answer would be appreciated.","['prime-numbers', 'number-theory']"
909366,$T^2\times S^n$ is parallelizable,"This is taken from a UCLA Geometry/Topology qualifying exam. How would one prove that $T^2\times S^n$ is parallelizable for all $n\geq 1$? Is there a way to find $n+2$ linearly independent vector fields? I am trying to think of the simplest case $n=2$ where $S^2$ is not parallelizable, but $T^2\times S^2$ has to be in some way. I would appreciate a general strategy to treat such problems.","['differential-topology', 'geometry', 'manifolds']"
909408,Is 641 the Smallest Factor of any Composite Fermat Number?,"Consider the sequence $a_n = 2^{2^n}+1$ of so-called Fermat numbers. It's well known that $a_5$ isn't prime ($a_5 = 641 \cdot 6700417$, this is due to Euler). What I want to know about this sequence is the smallest factor of any of its composite terms. Is it simply 641, or does a later composite term in the sequence have a smaller factor? I originally wanted to ask if there had been any effort to solve similar problems for other sequences as well, but that's not something I feel MSE would want. If someone has any info on this, however, they should feel free to comment or add it to their answer!","['factoring', 'prime-factorization', 'sequences-and-series', 'number-theory']"
909436,hint with an exercise algebra,"I'm stuck with the following I hope someone could help me. Let $N$ a normal subgroup of $G$. Show that if $[G:N]=4$, exists a normal subgroup $M$ of $G$ s.t. $[G:M]=2$. My idea: Since $G/N$ has orden 4, either is isomorphic to $\mathbb{Z}/4$ or $\mathbb{Z}/2 \times\mathbb{Z}/2$ and in any case we have a subgroup of order $2$ and for instance the same applies to $G/N$ but from here I'm not sure how to get the normal subgroup $M$ of $G$ which have exactly two cosets. any idea? I think I got it.  $G \to_\pi G/H\to _f\{\text{klein 4 group or cyclic group}\}$. Either if we have a cyclic group  or the Klein 4 group  (both commutatives) we can find a subgroup of order $2$, say $H$ (which obviously is normal), then $f^{pre}[H]=K\lhd G/H$, and define $M =\pi^{pre}[K]\lhd G$ (because the canonical map is a surjecion). To conclude we claim that $[G:M]= 2$. Since $4=[G:N]=[G:M][M:N]=[G:M]2$, we're done. I think is correct, thanks","['self-learning', 'group-theory', 'abstract-algebra']"
909472,Find the probability of winning at this lottery.,"So, the problem I found goes like this: You have $n$ different numbers, numbered from $ 1 $ to $n$. You can
  randomly choose $m$ (different) of them. The computer also randomly
  selects $m$ (different) of them. If you and the computer have exactly
  $k$ common numbers, then you win a certain amount of money. The problem asks us to find the probability of winning. I have solved some easier problems involving probabilities. But here, the only thing I could think of was that the probability for a certain sequence of $m$ numbers to emerge is: $$ \frac{1}{\dbinom{n}{m}} $$ How do you solve it? I'm on my way of getting used to this type of problems and I could really use some help.","['probability', 'problem-solving']"
909497,Sequence with equidistant terms,"Consider the sequence $(u_{n})_{n \in \mathbb{N}}$ given by : $$ u_{0} \in \mathbb{Z} \quad \mathrm{and} \quad \forall n \in \mathbb{N}, \, \vert u_{n+1}-u_{n} \vert = 1.$$ Is the sequence $(w_{n})_{n \in \mathbb{N}} = \displaystyle \Big( \frac{u_{n}}{n+1} \Big)_{n \in \mathbb{N}}$ convergent ? I have proved that $(w_{n})_{n \in \mathbb{N}}$ is bounded ($\forall n \geq n_{0}, \, 0 \leq w_{n} \leq 2$). I tried to see whether it is increasing or decreasing but we do not have enough information on $(u_{n})_{n \in \mathbb{N}}$ to conclude. I also tried to prove that $(w_{n})_{n \in \mathbb{N}}$ is a Cauchy sequence but I don't think it is one. My intuition is that $(w_{n})_{n \in \mathbb{N}}$ is convergent but I can't prove it. Edit : To prove that $(w_{n})_{n \in \mathbb{N}}$ is bounded, here is what I did : let $n \in \mathbb{N}$, $$ u_{n} - u_{0} = \sum_{k=1}^{n} u_{k}-u_{k-1} $$ Then, $$
\begin{align*}
\vert u_{n}-u_{0} \vert &= {} \Big\vert \sum_{k=1}^{n} u_{k}-u_{k-1} \Big\vert \\[2mm]
 &\leq \sum_{k=1}^{n} \vert u_{k}-u_{k-1} \vert = n \\
\end{align*}
$$ As a consequence, $\vert u_{n}-u_{0}\vert \leq n \leq n+1$. It leads to : $\vert u_{n} \vert \leq \vert u_{0} \vert + (n+1)$. Therefore : $$ \bigg\vert \frac{u_{n}}{n+1} \bigg\vert \leq \frac{\vert u_{0} \vert}{n+1} + 1 $$ Since the sequence $\displaystyle \Big( \frac{\vert u_{0} \vert}{n+1} \Big)_{n \in \mathbb{N}}$ is convergent to $0$, there exist a $n_{0} \in \mathbb{N}$ such that : $$ \forall n \geq n_{0}, \, \frac{\vert u_{n} \vert}{n+1} \leq 2. $$ Edit 2 : To prove that $(w_{n})_{n \in \mathbb{N}}$ is a Cauchy sequence, here is what I tried : let $(p,n) \in \mathbb{N}^{2}$ such that $p > n$, $$ 
\begin{align*}
\frac{u_{p}}{p+1} - \frac{u_{n}}{n+1} &= {} \frac{(n+1)u_{p} - (p+1)u_{n}}{(n+1)(p+1)} \\[2mm]
 &= \frac{(n+1)\bigg( \displaystyle \sum_{k=n+1}^{p} (u_{k}-u_{k-1}) + u_{n} \bigg) - (p+1)u_{n}}{(n+1)(p+1)} \\[2mm]
&= \frac{\displaystyle (n+1)\sum_{k=n+1}^{p} (u_{k}-u_{k-1}) - (p-n)u_{n}}{(n+1)(p+1)} \\
\end{align*}
$$ Therefore, $$ \Bigg\vert \frac{u_p}{p+1} - \frac{u_n}{n+1} \Bigg\vert \leq \frac{(p-n)(n+1+\vert u_{n} \vert)}{(n+1)(p+1)} $$ But I couldn't go any further.","['sequences-and-series', 'real-analysis']"
909502,Examples for infinite Hamiltonian group having a infinite order 2-group as a subgroup,"During teaching some basic concepts about a Hamiltonian group, I was asked about an infinite sample. According to what D.J. Robinson cited, we have a very good frame for this kind of interesting group: (Dedekind-Baer): A Hamiltonian group $G$ can be written as
                  $${G={Q}_8\times\mathbb{Z}_{2}^{k}\times H.}$$ wherein $H$ is an abelian group with all its elements of odd order. Obviously, while $G$ is finite then $|G|=2^{3+k}|H|$ and so $Q_8$ is the smallest one in this class of groups. In that time, we thought of an example of infinite one and then concluded that we could take $H=\mathbb{Z}(p^{\infty})$ where $p$ is odd prime. Do you know any other infinite $H$ satisfying the above definition? Thanks for the time.","['group-theory', 'abstract-algebra']"
909521,Find the sum of a convergent series using a well-known function,"I found this series in my calculus book: $$\sum_{n=1}^{\infty}(-1)^{n+1} \frac{1}{5^nn}$$
The directions are in the title of this question, but I can't think of any functions whose power series looks anything like that when evaluated at a point. Hints are appreciated, because I like to work these out on my own.",['sequences-and-series']
909539,How to show that $\lim \frac{1}{n} \sum_{i=1}^n \frac{1}{i}=0 $? [duplicate],"This question already has answers here : Prove that $\lim_{n\to\infty} H_n/n = 0$ ($H_n$ is the $n$-th harmonic number) using certain techniques (4 answers) Find $\lim_{n\to\infty} (1+\frac{1}{2}+...+\frac{1}{n})\frac{1}{n}$ (6 answers) Closed 6 years ago . Show that 
$$\lim \frac{1}{n} \sum_{i=1}^n \frac{1}{i} =0 $$ I've proved that this sequence converges (it is bounded and decreasing). NOW, I need to find a sequence that is bigger than this one and goes to zero. Maybe something using geometric serie of 1/2 Thanks in advance!","['harmonic-numbers', 'convergence-divergence', 'sequences-and-series', 'limits']"
909547,Does simply-connected imply measurable?,"The famous examples of non-measurable sets involve a sophisticated selections of points from a ball (or another object). This raises the following question: if a certain object in a Euclidean space is simply-connected, does it imply that it is Lebesgue-measurable? (BACKGROUND: I am doing a geometry-related research, where most shapes are simple polygons and circles. But when I try to extend the results to more general shapes, I keep running into problems related to measurability. So, I want to know if assuming that my shapes are simply-connected will save me from this trouble).","['geometry', 'measure-theory']"
909583,Direct proof of principle of transfinite induction,"This is a problem from the book Set theory by You-Feng Lin. Principle of Transfinite Induction Let $(A,\le)$ be a well-ordered set. For each $x \in A$, let $p(x)$ be a statement about $x$. If for each $x \in A$, the hypothesis ""$p(y)$ is true for every $y \lt x$"" implies that ""$p(x)$ is true,"" then $p(x)$ is true for every $x \in A$. I'm trying to prove this theorem directly using this lemma. Let $(A, \le)$ be a well-ordered set, and let $\mathscr T$ be a family of segments of $A$ such that (1) any union of members of $\mathscr T$ belongs to $\mathscr T$. (2) if $A_x \in \mathscr T$, then $A_x \cup \{x\} \in \mathscr T$. Then $\mathscr T$ contains all segments of $A$. How may I use this lemma to prove the principle of transfinite induction? I'm not sure how to form a family of segments to satisfy those conditions, and how that would guarantee the theorem. Any help?","['transfinite-recursion', 'induction', 'well-orders', 'elementary-set-theory']"
909598,Complement of a solid genus-2-handlebody in $S^3$,I'm not sure if this is a stupid question or not but is the complement of a solid genus-2-handlebody in $S^3$ also a solid genus-2-handlebody? Thanks!,"['general-topology', 'low-dimensional-topology', 'geometric-topology', 'algebraic-topology']"
909603,Number of intersections of two closed loops on a genus zero surface,"I have stumbled onto the following fact and I am quite helpless in seeing why this is true (although I can agree intuitively). Let $M$ be a surface of genus zero (open or closed, with or without boundary). The claim is now that there can't be two closed, smooth and transversal loops $\alpha, \beta : S^1 \rightarrow M$ with 
non-zero intersection number . (One loop is required to be simple closed. I am however not sure if this does really matter.) The intersection number is meant to be the oriented intersection number of transversal loops (cf. chapter 3 of Differential topology by Guillemin and Pollack). It basically adds up the signs of each of the intersections. The sign of an intersection is set to $+1$ if loop $\alpha$ crosses loop $\beta$ from left to right and to $-1$ if $\alpha$ crosses $\beta$ from right to left. Transversality and smoothness of the loops is required to define this intersection number. Intuitively it it clear that I can't draw two closed loops on a genus zero surface where
$\alpha$ enters the right side of $\beta$ more often than it enters the left side . Does anyone know about a reference where I can read up on this? Many thanks! :)","['differential-geometry', 'general-topology', 'intersection-theory', 'differential-topology', 'surfaces']"
909637,"Inequality of numerical integration $\int _0^\infty x^{-x}\,dx$.","There was a friend asking me how to prove $$\int_0^\infty x^{-x}\,dx<2$$ Mathematica showed that its approximate value is 1.99546, so I think it isn't easy to solve it, can you provide me some ideas about this question?","['definite-integrals', 'calculus', 'integral-inequality']"
909640,"Boundary Layer, leading order, Pertubation Theory, Differential Equations","I have got the following problem, taken from Multiple Scale and singular perturbation methods, Kevorkian & Cole book, page 94, exercise 1.b.: Find the leading order of the problem: $\varepsilon y''+ \frac{1}{2}y^2y'-y=0 $, with  $0<x<1$ and $y(0)=A$,  $y(1)=B$, and A,B independent of $\varepsilon$ . I have tried to use the boundary layer method in order to find an outer and inner solution and then make the respective matching to find some constants which appear. The inner layer is at some point inside the interval if $B^2-A^2\ne4$. When doing the expansion for the inner solution I have got the following equation: $Y_0'+ \frac{1}{6}Y_0^3=K$. This integral is not trivial but it can be solved by partial-fraction decomposition to get an implicit solution with depends in two different constants, namely $K$ and $C$. I have got $\frac{x}{6}+C=\frac{1}{3K^{\frac{2}{3}}}\left[-\ln\left(K^{\frac{1}{3}}-Y_{0}\right)+\frac{1}{2}\ln\left(K^{\frac{2}{3}}+K^{\frac{1}{3}}Y_{0}+Y_{0}^{2}\right)+\sqrt{3}\tan^{-1}\left(\frac{2Y_{0}}{\sqrt{3}K^{\frac{1}{3}}}+\frac{1}{\sqrt{3}}\right)\right]$ The problem here is that in order to do the matching, I should get an explicit solution $Y_0$ in order to find the constants. Can someone give a hint? Is there maybe another approach to solve this problem? Thank you.","['asymptotics', 'ordinary-differential-equations', 'perturbation-theory']"
909644,How to predict next number from a given set of measurement data?,"I have to do some experiment and measure it on a specific time 
0, 3, 6, 9, 12, 18, 24, 36 months
$$
\begin{array}{l|c|c|c|c|c|c}
\text{Month}	&ID1 &    ID2	&    ID3	&    ID4	&    ID5	&    ID6\\\hline
0	   & 101.7&	102.6&	101.7	&100.5	&100.4	&103.4\\
3	    &103.4	&103.3	&101.4	&101.7&	100.5	&101.2\\
6	    &100.7	&103.7	&101.6	&102	   & 102.9&	102\\
9	    &100.2	&100.6&	101.2&	97.9&	98.7	&99.5\\
12	   & 99.8	&100.1&	x  &     98.6&	100.4	&100.1\\
18	    &98.7&	x&		x	&	x      & x       &x\\
24	    &101.3	&x		&x	&	x &      x&       x\\
36	    &100.8	&x		&x&		x       &x  &     x\\
\end{array}
$$
Basically, all the numbers are from the measurement. So, could anyone please suggest how do I predict the numbers represented as ""x"" here ? Assume each ID are different type of items/products/equipment to be measured. I would prefer to use the arithmetic mean and variance for the prediction. However, if anyone could suggest better idea, it would be nice. Thank you in advance,
PTP",['statistics']
909680,How would I find a point on the y-axis equidistant from two other points?,"The points are$$(5,-5) and (1,1)$$ I tried doing this visually and came up with (0,-5). This wasn't correct once I applied the distance formula to check the distance between that point and the two others.",['algebra-precalculus']
909712,"Evaluate $\int_0^{{\pi}/{2}} \log(1+\cos x)\, dx$","Find the value of $\displaystyle \int_0^{{\pi}/{2}} \log(1+\cos x)\ dx$ I tried to put $1+ \cos x = 2 \cos^2 \frac{x}{2} $, but I am unable to proceed further.
I think the following integral can be helpful:
 $\displaystyle \int_0^{{\pi}/{2}} \log(\cos x)\ dx =-\frac{\pi}{2} \log2 $.","['calculus', 'integration', 'definite-integrals', 'trigonometry', 'real-analysis']"
909733,A group with five elements is Abelian [duplicate],"This question already has answers here : Is a group of prime-power order always abelian? (3 answers) Groups of prime order are cyclic (2 answers) Closed 2 years ago . I tried to prove the following theorem: A group with five elements is abelian. I know only the definition of a group and a subgroup but no more.(this is a problem from Topics in Algebra by IN Herstein). I tried a few things but to no avail. Suppose that the group is $G=\{e,g_1,g_2,g_3,g_4\}$. $g_1g_2$ cannot be $g_1$ or $g_2$ or else one of them will be the identity,$e$.So, $g_1g_2$ is either $g_3$ or $g_4$ or $e$. Case I: $g_1g_2=e$.(so, $g_2g_1=e$).
Then $g_1g_3=g_2$ or $g_4$($g_1g_3\not=e$,or else $g_2=g_3$). Say,$g_1g_3=g_2$. That is, $g_1=g_3^{-1}g_2\implies g_1g_2=g_3^{-1}g_2^2=e$ or in other words, $g_2^2=g_3$. Using this, $g_3g_1=g_2^2g_1=g_2(g_2g_1)=g_2$ and we have $g_1g_3=g_1g_2^2=g_2$. Maybe, this will lead to a solution but I was wondering if there is a much smarter way to do it as this leads to a lot of casework! .",['group-theory']
909741,"How to prove $\int^{\pi/2}_0 \log{\cos{x}} \, \mathrm{d}x = \frac{\pi}{2}\log\left(\frac12\right)$","ALREADY ANSWERED I was trying to prove the result that the OP of this question is given as a hint. That is to say: imagine that you are not given the hint and you need to evaluate : $$I = \int^{\pi/2}_0 \log{\cos{x}} \, \mathrm{d}x \color{red}{\overset{?}{=} }\frac{\pi}{2} \log{\frac{1}{2}} \tag{1}$$ How would you proceed? Well, I tried the following steps and, despite it seems that I am almost there, I have found some troubles: Taking advantage of the fact: $$\cos{x} = \frac{e^{ix}+e^{-ix}}{2}, \quad \forall x \in \mathbb{R}$$ Plugging this into the integral and performing the change of variable $z = e^{ix}$, so the line integral becomes a contour integral over a quarter of circumference of unity radius centered at $z=0$ , i.e.:
$$ I = \frac{1}{4i} \oint_{|z|=1}\left[ \log{ \left(z+\frac{1}{z}\right)} - \log{2} \right] \, \frac{\mathrm{d}z }{z}$$ $\color{red}{\text{We cannot do this because the integrand is not holomorphic on } |z| = 1 }$ Note that the integrand has only one pole lying in the region enclosed by the curve $\gamma : |z|=1$ and it is holomorphic (is it?) almost everywhere (except in $z =0$), so the residue theorem tells us that: $$I = \frac{1}{4i}  \times 2\pi i \times \lim_{z\to0} \color{red}{z} \frac{1}{\color{red}{z}} \left[ \underbrace{ \log{ \left(z+\frac{1}{z}\right)} }_{L} - \log{2} \right] $$ As I said before, it seems that I am almost there, since the result given by eq. (1) follows iff $L = 0$, which is not true (I have tried L'Hôpital and some algebraic manipulations). Where did my reasoning fail? Any helping hand? Thank you in advance, cheers! Please note that I'm not much of an expert in either complex analysis or complex integration so please forgive me if this is trivial. Notation:  $\log{x}$ means $\ln{x}$. A graph of the function $f(z) = \log{(z+1/z)}$ helps to understand the difficulties: where $|f(z)|$, $z = x+i y$ is plotted and the white path shows where $f$ is not holomorphic.","['residue-calculus', 'integration', 'complex-analysis', 'contour-integration']"
909755,Solving for an angle,"I was never good in trigonometry. I have a rectangle with dimensions $L_1$ and $W_1$.  I want to rotate it so that it fits inside another rectangle with dimensions $L_2$ and $W_2$. I need to find the angle. I have worked out the formula that I need is:
$$L_1 \cos\theta + W_1 \sin \theta = L_2$$ I don't know how to solve for $\theta$. I understand that there won't be a solution for $$L_2 > \sqrt{L_1^2 + W_1^2}$$",['trigonometry']
909783,Prove that the series is convergent: 1- (1+1/3)/2 + (1+1/3+1/5)/3-.....,"I can see that this is an alternating series with the $n$-th term $$(-1)^{n+1}\frac{1+\frac13+\frac15+\cdots+ \frac{1}{2n-1}}{n}.$$ What test can I apply to show that it converges? Also, it converges absolutely and not conditionally, right? Thanks for any help.","['convergence-divergence', 'sequences-and-series', 'analysis']"
909786,Winning strategies in multidimensional tic-tac-toe,"This question is a result of having too much free time years ago during military service.
One of the many pastimes was playing tic-tac-toe in varying grid sizes and dimensions, and it lead me to a conjecture.
Now, after several years of mathematical training at a university, I am still unable to settle the conjecture, so I present it to you. The classical tic-tac-toe game is played on a $3\times3$ grid and two players take turns to put their mark somewhere in the grid.
The first one to get three collinear marks wins.
Collinear includes horizontal, vertical and diagonal lines.
Experience shows that the game always ends in a draw if both players play wisely. Let us write the grid size $3\times3$ as $3^2$.
We can change the edge length by playing on any $a^2$ grid (where each player tries to get $a$ marks in a row on the $a\times a$ grid).
We can also change dimension by playing on any $a^d$ grid, for example $3^3=3\times3\times3$.
I want to understand something about this game for general $a$ and $d$.
Let me repeat: The goal is to make $a$ collinear marks. I assume both players play in an optimal way.
It is quite easy to see that the first player wins on a $2^d$ grid for any $d\geq2$ but the game is a tie on $2^1$.
The game is a tie also on $3^1$ and $3^2$, but my experience suggests that the first player wins on $3^3$ but the game ties on $4^d$ for $d\leq3$.
It seems quite credible that if there is a winning strategy on $a^d$, there is one also on $a^{d'}$ for any $d'\geq d$, since more dimensions to move in gives more room for winning rows. This answer to a related question tells that for any $a$ there is $d$ so that there is a winning strategy on $a^d$. This brings me to the conjecture: There is a winning strategy for tic-tac-toe on an $a^d$ grid if and only if $d\geq a$. (Refuted by TonyK's answer below.) Is there a characterization of the cases where a winning strategy exists?
It turns out not to be as simple as I thought. To fix notation, let
$$
\delta(a)=\min\{d;\text{first player wins on }a^d\}
$$
and
$$
\alpha(d)=\max\{a;\text{first player wins on }a^d\}.
$$
The main question is: Is there an explicit expression for either of these functions?
  Or decent bounds?
  Partial answers are also welcome. Note that the second player never wins, as was discussed in this earlier post . A remark for the algebraically-minded:
We can also allow the lines of marks to continue at the opposite face when they exit the grid; this amounts to giving the grid a torus-like structure.
Now there are no special points, unlike in the usual case with boundaries.
Collinear points on a toric grid of size $a^d$ corresponds to a line (maximal collinear set) in the module $(\mathbb Z/a\mathbb Z)^d$.
(If $a$ is odd, then $a$ collinear points in the mentioned module add up to zero, but the converse does not always hold: the nine points in $(\mathbb Z/9\mathbb Z)^3$ with multiples of three as all coordinates add up to zero but are not collinear.)
This approach might be more useful when $a$ is a prime and the module becomes a vector space.
Anyway, if this version of the game seems more manageable, I'm happy with answers about it as well (although the conjecture as stated is not true in this setting; the first player wins on $3^2$).","['tic-tac-toe', 'game-theory', 'combinatorial-game-theory', 'linear-algebra', 'modules']"
909794,Show that the value of $\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}}$ when $x=0$ is $\frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2$,"The question originally asks you to prove that if $y=\sin^{-1}(x)+(\sin^{-1}(x))^2$ that: $(1-x^2)y''-x y'$ is independent of $x$. I get that $(1-x^2)y''-x y'=2$ hence proving the first part. The second part asks you prove that for $n>1$ that $$(1-x^2)\frac{\text{d}^{n+2}y}{\text{d}x^{n+2}}-x(2n+1)\frac{\text{d}^{n+1}y}{\text{d}x^{n+1}}-n^2\frac{\text{d}^{n}y}{\text{d}x^{n}}=0$$ 
which again is fine (I did it using General Leibniz rule ). The problem I'm having is in the next part, it says show that the value of $\frac{\text{d}^{2r+1}y}{\text{d}x^{2r+1}}$ when $x=0$ is: $$\frac{1}{2^{2r}}\left(\frac{(2r)!}{r!}\right)^2$$ I can't seem to be able to show it, I'm sure it must be something with a double factorial in since it the formula seems very similar to it but I'm not too sure how to go about it.","['calculus', 'algebra-precalculus', 'derivatives']"
909801,"Mathematical Analysis 2nd ed. - Apostol, Exercise 4.2","If $a_{n+2}=(a_{n+1}+a_n)/2$ for every $n\geq1$, show that $a_n \to (a_1+2a_2)/3$. Now, starting from the fact that $a_{n+2}-a_{n+1}=(a_n-a_{n+1})/2$ and using $b_n=a_{n+1}-a_n$ I obtain $$b_{n+1}=\frac{1}{2}b_n ⇒ b_{n+1}=\frac{1}{2^n}b_1$$ Now I sum the two sides in this way $$\sum_{k=2}^{n+1}b_k=\frac{1}{2}\sum_{k=1}^n b_k$$
Substituting back $b_k$and trying to find the limit of this sequence as $n\to\infty$ I get $$l=2a_2-a_1$$ It seems I am clearly wrong. I can prove that the sequence is Cauchy and converges (because we are in Euclidean space) but I cannot find the correct limit.  Why? Edit: Since I discovered why I got the wrong limit (read the comments), I would like to know if I could have solved this problem without using that ""summation trick"", which I deem not very intuitive. This sequence is an oscillating one, hence we cannot use the monotone convergence theorem. Thank you for your help.",['sequences-and-series']
909827,Notation of list expansion to a tuple,"I have a set $S$ that I want to expand to a $|S|$-tuple. How is the notation for that? Currently I have something like that:
$$
T = (f(x) : x \in S)
$$
An example:
$$
S = (A,B,C)\\
T = (f(A), f(B), f(C))
$$
In Mathematica the Map Function does what I want. So basically I want to expand a set of elements and add an entry to a tuple for every element of the set. I need the same functionality a summation does, without building the final sum - if that is of any help. Can I express this the way I did? UPDATE As I want the order of the resulting tuple to be preserved, it is necessary to map from a tuple to a tuple - so $S = (A,B,C)$ instead of $S = \{A,B,C\}$. Also if you're having the same issue, the Wikipedia Set-builder notation is a good place to start with.","['notation', 'elementary-set-theory']"
909852,Surface Integral over a sphere,"Suppose $f(x,y,z)=g\left(\sqrt{x^2+y^2+z^2}\right)$, where $g$ is a function of one variable such that $g(2)=-5$.  Evaluate $$\iint_S f ~dS,$$where $S$ is the sphere $x^2+y^2+z^2=4$. Now, I reasoned as follows. If $g(2)=-5$ then when $\sqrt{x^2+y^2+z^2}=2 \implies x^2+y^2+z^2=2$ the function $f$ takes the value of $-5$. Hence, since $S$ is the sphere of radius $4$ centred at the origin, we have \begin{align*}\iint_S f ~dS&=-5\iint_\Omega \left\|\frac{\partial\mathbf r}{\partial\phi }\times\frac{\partial\mathbf r}{\partial\theta } \right\|~dA,\end{align*}where $\mathbf{r}(\phi,\theta)$ is a parametrisation of $S$ using spherical coordinates: $$\mathbf{r}(\phi,\theta)=\left(2\sin\phi\cos \theta,2\sin\phi\sin\theta,2\cos\phi\right), ~(\phi,\theta)\in[0,\pi]\times[0,2\pi].$$ Hence \begin{align*}\iint_S f ~dS&=-5\int\limits_0^{2\pi}\int\limits_0^\pi 4\sin\phi~d\phi d\theta\\&=-80\pi.\end{align*} Can somebody please verify that my reasoning (especially in the first steps) is correct? The minus sign confuses me for some reason; can surface integrals be negative?","['multivariable-calculus', 'integration', 'surfaces', 'solution-verification']"
909860,Range of a trigonometric function,"Question:
Prove that:
$$0 \leq \frac{1 + \cos\theta}{2 + \sin\theta}\leq \frac{4}{3}$$ I have absolutely no idea how to proceed in this question. Please help me!","['trigonometry', 'functions']"
909871,Expressing $ 12\sin( \omega t - 10) $ in cosine form,"$$ 12\sin( \omega t - 10) $$ I understand how it's solved when using the graphical method, however I'm having trouble understanding something about the trigonometric identities method. The solution in the text book goes like this (It wants positive amplitudes) : (All angles are in degrees) $$ 12\cos( \omega t - 10 - 90) $$ $$ 12\cos( \omega t - 100) $$ I know that in order to convert from sine to cosine angle you either add or subtract $90$ degrees. What I don't understand is whether I should add or subtract to get the equivalent with positive amplitude. The way I approach this is that I imagine the graph where $+\cos \omega t$ is the positive $x$-axis, $-\cos \omega t$ is the negative $x$-axis, $+\sin \omega t$ is the negative $y$-axis and $-\sin \omega t$ is the positive $y$-axis. Since I want to change from positive amplitude sine to positive amplitude cosine I add $90$ degrees. But apparently that is incorrect. Please explain this to me.","['trigonometry', 'algebra-precalculus']"
909885,Why is $0/0$ not $\Bbb R$? [duplicate],"This question already has answers here : Division by zero (16 answers) Closed 9 years ago . Whenever I asked my grade school and college teachers why $\frac00$ is undefined, they would show me a graph of $\frac1x$ and point out the vertical asymptote at $0$, noting that ""Since it approaches $-\infty$ from the left and $\infty$ on the right, it could be $-\infty$, $\infty$, or anything in between. So, we say it's undefined."" My question is, then, why isn't it $\Bbb R$, or even $\{\Bbb{R,C}\}$?",['algebra-precalculus']
909892,Linear algebra - Memorising proper definitions of homomorphism types,"I am reading a book about linear algebra. On the basis of this book, I worked out the terminology below. Problem: To me, it looks like Wikipedia defines homomorphism differently. Apart from that: Do you agree with the following definitions of the homomorphism subtypes?
If so, is there a trick to memorise them? Let $V_1, V_2$ be vector spaces over a common field.
We consider a function
$f : V_1 \rightarrow V_2$.
Now, $f$ is a homomorphism iff $f$ is linear (linear-algebra-linear, not calculus-linear). epimorphism =
homomorphism +
surjective monomorphism =
homomorphism +
injective isomorphism =
epimorphism +
monomorphism endomorphism =
homomorphism +
(domain = codomain) automorphism =
endomorphism +
isomorphism The article Algebra homomorphism enumerates (in its first sentence) homogeneity and additivity but also a third property. The third property seems to be missing in my definition (definition based on the book). By the way, should I use the term map instead of function ?","['vector-spaces', 'linear-algebra', 'abstract-algebra']"
909896,Why $E[X|\mathcal{G}]=X$ if $X$ is $\mathcal{G}$-measurable?,"If $X$ is a $\mathcal{G}$-measurable random variable, why $E[X|\mathcal{G}] = X$? I know the intuition (basicly we're conditioning on the same informations on which $X$ is defined, $\sigma(X)$, we can't gain any different information), but I don't know how to prove it rigourously","['probability-theory', 'conditional-expectation', 'random-variables', 'expectation']"
909930,Elliptic curves as $\mathbb{C}^*/\mathbb{Z}$,"I apologize in advance if my question is rather trivial, but i have trouble understanding a basic fact about elliptic curves.. I have always wrote an elliptic curve $E$ as $\mathbb{C}/\Lambda$, where $\Lambda$ is a lattice of rank 2, i.e. $\Lambda=\mathbb{Z}+\mathbb{Z}\tau$, $\tau\in\mathbb{H}$ the upper half plane. Now I have read the equality $E=\mathbb{C}/\Lambda=\mathbb{C}^*/\mathbb{Z}$ where the last quotient is given by multiplication with $t^n$, $t=e^{i2\pi\tau}$. My question is: how can $\mathbb{C}/\Lambda$ and $\mathbb{C}^*/\mathbb{Z}$ be the same thing? the two actions seem really different to me..","['algebraic-geometry', 'elliptic-curves']"
909954,Density of Pythagorean triples,"We define a Pythagorean triple as a triple $<a,b,c>$ such that $a,b,c\in \mathbb N$ and $a^2+b^2=c^2$. In order to avoid duplicates, we say that a triple $<a,b,c>$ is legit iff $b>a$. Let $\mathcal P$ be the set of all legit Pythagorean triples. We define $$L_{PT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P\wedge b\leq N\}$$ (If it's more convinient we can define it for $b^2\leq N$, $c\leq N$ or $c^2\leq N$). What is the density of $|L_{PT}^N|$ as a function of $N$? e.g. is $|L_{PT}^N|=\Theta(N^2)?\Theta(N)?$ We say that a triple $<a,b,c>$ is minimal if $gcd(a,b,c)=1$.
Let $\mathcal P_M$ be the set of all legit, minimal triples. Let $$L_{MPT}^N=\{<a,b,c> | <a,b,c> \in  \mathcal P_M\wedge b\leq N\}$$ What is the density of $|L_{MPT}^N|$ as a function of $N$? e.g. is $|L_{MPT}^N|=\Theta(N)?$","['asymptotics', 'linear-algebra', 'pythagorean-triples', 'combinatorics']"
909977,Hard Definite integral involving the Zeta function,Prove that: $$\displaystyle \int_{0}^{1}\frac{1-x}{1-x^{6}}{\ln^4{x}} \ {dx} = \frac{16{{\pi}^{5}}}{243\sqrt[]{{3}}}+\frac{605\zeta(5)}{54} $$ I was able to simplify it a bit by substituting ${y = -\ln{x}}$ and some further mathematical manipulation but was not able to get the correct form.,"['sequences-and-series', 'riemann-zeta', 'calculus', 'integration', 'definite-integrals']"
909985,continuity single and multivariable function simple question,"Why $$f(x,y) =\begin{cases} \frac{xy^2}{x^2 +y^2} \mbox{ for } (x,y)\neq (0,0) \\ 0  \mbox{ for } (x,y)= (0,0)\end{cases}$$ is continuous and $$f(x) =\begin{cases} 2 \mbox{ for } 0>=x>10 \\ 5  \mbox{ for } x>=10\end{cases}$$ seem to be discontinuity? Or am I wrong? This is related to my previous question continuity single variable function and multivariable funtion and its parcial derivatives","['multivariable-calculus', 'continuity']"
910010,"Probability of events in an infinite, independent coin-toss space","I am studying Steven E. Shreve's Stochastic Calculus book. Example 1.1.4 (p.4-6) constructs a probability measure on the space of infinely many coin tosses $\Omega_\infty$. In the example the $\sigma$-algebras for $n$ coin tosses are defined like this $\mathcal{F}_0 = \{\emptyset, \Omega\}$, $\mathcal{F}_1 = \{\emptyset, \Omega , A_{H}, A_{T}\}$, where $A_H$ is the set of all sequences beginning with head. For three coin tosses we get $\mathcal{F}_3 = \{\emptyset, \Omega ,A_H, A_T, A_{HH}, A_{HT}, \ldots\}$ Now the authors state By continuing this process, we can define the probability of every set that can be described in terms of finitely many tosses. And later: We create a $\sigma$-algebra, called $\mathcal{F}_\infty$ by putting in every set that can be described in terms of finitely many coin tosses and then adding all other sets required in order to have a $\sigma$-algebra. It turns out that once we specify the probability of every set that can be described in terms of finitely many coin tosses, the probability of every set in $\mathcal{F}_\infty$ is determined. I find this puzzling! Why do finite descriptions suffice? For example I don't understand how the probability of the event ""infinitely many heads"" is determined. I would guess it has probability 1 but how can I conclude this from the finite cases? How is this done for general elements $A\in\mathcal{F}_\infty$? Edit 2: Can I argue like this: for $A\in (\mathcal{F}_\infty\setminus (\bigcup_{n=1}^\infty F_n))$, the complement $A^C$ is in some $\mathcal{F}_m$ and therefore $\mathbb{P}(A) = 1-\mathbb{P}(A^C)$?","['measure-theory', 'probability']"
910020,"Show that if $G$ is a group of order $168$ that has a normal subgroup of order $4$ , then $G$ has a normal subgroup of order $28$","Show that if $G$ is a group of order $168$ that has a normal subgroup of order $4$ , then $G$ has a normal subgroup of order $28$. Attempt: $|G|=168=2^3.3.7$ Then number of sylow $7$ subgroups in $G = n_7 = 1$ or $8$. Given that $H$ is a normal subgroup of order $4$ in $G$. If we prove that $n_7$ cannot be $8$, then $n_7=1$ and as a result, the sylow $7$ subgroup $K$ is normal.Hence, $HK$ will also be a normal subgroup of $G$ and since, $H \bigcap K = \{e\} \implies |HK|=28$. Now, suppose $n_7=8$ and hence, $K_1 \cdots K_8$ are the $8$ cyclic subgroups of order $7$. Each $K_i$ has $\Phi(7)=6$ elements of order $7$ . Hence, total elements of orders $=7$ in the $K_i's$ are $6.8=48$ How do I move forward and bring a contradiction somewhere? Thank you for your help.","['sylow-theory', 'finite-groups', 'group-theory', 'abstract-algebra']"
910021,Computing $\sum_{i=0}^{\infty}\frac{i}{2^{i+1}}$,"I came across this while trying to solve Google's boys & girls problem , and although I know now it's not the right approach to take, I'm still interested in summing
$\sum_{i=0}^{\infty}\frac{i}{2^{i+1}}$. Apparently it should be 1..but I'm having a tough time seeing this, especially since $\sum_{i=0}^{\infty}\frac{1}{2^{i+1}}=1$. I know it's a little elementary.. but I just can't figure out where I'm going wrong.. $$\sum_{i=0}^{\infty}\frac{i}{2^{i+1}}
=\frac{1}{2}\sum_{i=0}^{\infty}\frac{i}{2^{i}}
=\frac{1}{2}\sum_{j=0}^{\infty}\sum_{i=j}^{\infty}\frac{1}{2^{i}}
=\frac{1}{2}\sum_{j=0}^{\infty}\left(\sum_{k=0}^{\infty}\frac{1}{2^{k}} - \sum_{i=0}^{j-1}\frac{1}{2^{i}}\right)
=\frac{1}{2}\sum_{j=0}^{\infty}\left(2 - \frac{1-\frac{1}{2^j}}{1/2}\right)$$
$$=\frac{1}{2}\sum_{j=0}^{\infty}\left(2\frac{1}{2^j}\right)=2 \ne1$$","['summation', 'combinatorics']"
910098,Trigonometric equation $2\sin x+\cos x+1=0$,"I have to calculate $\dfrac{d}{dx}\dfrac{1+\cos x}{2+\sin x}=0$. I have already simplified to: $2\sin x+\cos x+1=0$, but I have no idea how to go further.. Could someone give a hint?",['trigonometry']
910117,"Show that for any $g \in L_{p'}(E)$, where $p'$ is the conjugate of $p$, $\lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx$","Let $1 < p < \infty, f_k \in L_p(E), k = 1, 2, ..., $ and $\lim_{k \rightarrow \infty}f_k(x) = f(x)$ a.e., $\sup_{1 \leq k<\infty}||f_k||_p \leq M$.  Show that for any $g \in L_{p'}(E)$, where $p'$ is the conjugate of $p$, $\lim_{k \rightarrow \infty}\int_Ef_k(x)g(x)dx = \int_Ef(x)g(x)dx$ This is from a past qual.  Not really sure what to do.  Thought about using dominated convergence theorem.  I did show that is $E$ is finite, $f_k \rightarrow f$ in $L_1(E)$ by vitali convergence theorem.  I think I was able to extend this to $m(E) = \infty$.  But, this doesn't show $f_k \rightarrow f$ in $L_p$ which is more useful.  Any suggestions?","['measure-theory', 'real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
910179,Part of proof that $d^2\omega=0$,"The following comes from the proof in differentiable manifolds that $d^2\omega=0$. Let $f$ belong to the set of $0$-forms. From definition I have that $\displaystyle df = \frac{\partial f}{\partial x^j}dx^j$ Then from the definition for $d$ of a one-form, apparently we have that $\displaystyle d(df)=d\Big(\frac{\partial f}{\partial x^j}\Big) \wedge dx^j$. I cannot see how this can been derived. I tried $\displaystyle d(df)=d\Big(\frac{\partial f}{\partial x^j} dx^j \Big)$. You then use some kind of product rule? The definition I have been given is if $\displaystyle \omega = \frac{1}{k!} \omega_{i_1 ... i_k} dx^{i_1}\wedge...\wedge dx^{i_k}$ then its derivative is $\displaystyle d\omega = \frac{1}{k!} d\omega_{i_1\cdots i_k} \wedge dx^{i_1}\wedge\cdots\wedge dx^{i_k}$","['exterior-algebra', 'manifolds', 'derivatives', 'differential-geometry']"
910184,Finding a cubic polynomial whose splitting field over $\mathbb{Q}$ equals $\mathbb{Q}(a)$ if $a$ is any of its roots,"Question: Let $\alpha$, $\beta$ and $\gamma$ be the roots of a rational cubic polynomial $q$. Can we find a (non-trivial) example where the splitting field of $q$ over $\mathbb{Q}$ equals $\mathbb{Q}(\alpha), \mathbb{Q}(\beta)$ and $\mathbb{Q}(\gamma)$? Comments: This is from an old algebra exam. Clearly one solution (the one given by the professor) is $q(x)=(x-1)^3$ where the splitting field is $\mathbb{Q}(1) = \mathbb{Q}$, but I was wondering: does there exist a $q$ as described above so that $\mathbb{Q}(\alpha) = \mathbb{Q}(\beta)$  = $\mathbb{Q}(\gamma)$ but still so that they are different from $\mathbb{Q}$? If not, can we prove that it does not exist?","['galois-theory', 'field-theory', 'abstract-algebra', 'number-theory']"
910190,When is the solution to a n initial value problem matrix differential equation invertible?,"Suppose $A (t,s)$ a $n\times n$ matrix is the solution of the initial value problem below, where $B_s$ is also an $n\times n$ matrix, invertible for all $s$: $$\dfrac{d A(t,s)}{ds} = B_s A(t,s)$$
$$ A(t,t) =I_n$$ I know that if $B$ is constant, the solution is invertible as an exponential matrix. If $B_s$ is not constant, is the solution still invertible? My answer is yes since by the Magnus expansion the solution is of the form $A(t,s) = exp(\Omega(t,s))A(t,t)$ which is again is invertible because it's an exponential and $A(t,t)=I_n$. Can anyone confirm if my argument is correct? or is there any other way to see it without using the Magnus expansion argument? Thanks","['matrices', 'linear-algebra', 'ordinary-differential-equations']"
910202,Groups - Prove that every element equals inverse of inverse of element,"This is my first proof about groups. Please feed back and criticise in every way (including style & language).
Axiom names ( see Wikipedia ) are italicised . We use $^{-1}$ to denote inverse elements; $e$ denotes the identity element. Let $(G, \cdot)$ be a group.
By $\textit{identity element}$, $G \ne \emptyset$.
Now, let $a \in G$.
By $\textit{inverse element}$, $a^{-1} \in G$ and $(a^{-1})^{-1} \in G$.
It remains to prove that $(a^{-1})^{-1} = a$.
\begin{equation*}
\begin{split}
a &= a \cdot e && \text{by }\textit{identity element} \\
  &= a \cdot \Big(a^{-1} \cdot (a^{-1})^{-1}\Big) && \text{by }\textit{inverse element} \\
  &= (a \cdot a^{-1}) \cdot (a^{-1})^{-1} && \text{by }\textit{associativity} \\
  &= e \cdot (a^{-1})^{-1} && \text{by }\textit{inverse element} \\
  &= (a^{-1})^{-1} && \text{by }\textit{identity element}
\end{split}
\end{equation*}
QED PS: What I do know is that I have a personal preference for details and explicitness :-(","['abstract-algebra', 'group-theory', 'proof-verification', 'soft-question', 'proof-writing']"
910206,Minimizing an error function by deriving a system of linear equations,"Consider the following formula: $$E(\mathbf{w}) = \frac{1}{2}\sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}^2$$ where $\mathbf{w}$ is a vector of weights; $x_n$ and $t_n$ come from two vectors of length $N$; and $y$ is a polynomial: $$y(x,\mathbf{w}) = \sum_{j=0}^M w_jx^j$$ My task is to show a system of equations which yield weights $\mathbf{w} = \{w_i\}$ that minimize E. I reckoned I should differentiate and set the derivative to 0: $$ \frac{dE}{dw} = \sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}\times\frac{dy}{dw}$$ $$ \frac{dE}{dw} = \sum_{n=1}^{N}\{y(x_n,\mathbf{w})-t_n\}\times\sum_{i=0}^{M}x_n^j$$ $$\sum_{n=1}^{N}\{\sum_{j=0}^{M}w_jx_n^j-t_n\}\times\sum_{i=0}^{M}x_n^j = 0$$ The solution says to do what I did, except differentiate ""with respect to $w_i$"". It offers the following expression: $$\sum_{n=1}^{N}(\sum_{j=0}^{M}w_jx_n^j-t_n) x_n^i = 0$$ I take it that each $i$ yields another equation, hence this approach leading to a system of equations. There are two things I don't understand: Why is there not a summation over the $x_n^i$ values at the end? I thought differentiating $y$ would remove the weights but retain the summation. The inner summation uses a $j$ though the outer uses a $i$. Why are they not the same symbol? Though I know if they were both $j$ we would be left with just one equation, I don't understand how they can be different.","['linear-algebra', 'error-function', 'derivatives']"
910220,What happens to a function when it is undefined?,"If I have the function $$f(x) = {x^2 - 2 \over    x + \sqrt 2}$$ this is undefined for $x = -\sqrt 2$, am I correct? Since the denominator would be zero. But the numerator is a difference of squares $x^2-2=(x - \sqrt 2)(x+\sqrt 2)$. Now the second factor is equal to my denominator, so those cancel and I am left with $(x - \sqrt 2)$, and this is defined at $x=-\sqrt 2$, is it not? If I put $x = -\sqrt 2$, the result is simply $-\sqrt 2 - \sqrt 2=-2\sqrt 2$. What am I doing wrong here?","['algebra-precalculus', 'functions']"
910226,Can I turn $Ax=b$ into $Ax=0$?,"For a system of equations $$
\begin{bmatrix}d_1 & d_2 & \dots & d_n \end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \end{bmatrix} = d_{n+1}
$$ where each $d$ is a column of (possibly noisy) data and each $u$ is a scalar unknown, is the correct approach to call this $Ax=b$ and solve as $x = A\setminus b$, or to rearrange it into an $Ax=0$ form as below, $$
\begin{bmatrix}d_1 & d_2 & \cdots & d_n & d_{n+1}\end{bmatrix} \begin{bmatrix}u_1\\u_2\\ \vdots \\ u_n \\ -1\end{bmatrix} = 0
$$ using SVD to find the vector of unknowns, then normalizing by its final element? In matlab these give different answers, but I don't know if the difference is in noise or implementation or if there's some theory that I'm missing.","['linear-algebra', 'svd', 'homogeneous-equation', 'numerical-methods']"
910247,Banach Spaces: Totally Bounded Subsets,"As an easy consequence of Riesz' lemma it is known that infinite dimensional Banach spaces possess bounded subsets that fail to be totally bounded. On the other hand in finite dimensional Banach spaces any bounded subset happens to be totally bounded as well. So the question arises wether totally bounded subsets always sit within finite dimensional spaces, or put in equivalent form: Is there an infinite dimensional space which possesses a totally bounded subset not lying entirely in a finite dimensional subspace?","['functional-analysis', 'banach-spaces']"
910257,If $H$ is a normal subgroup of a finite group $G$ and $|H|=p^k$ for some prime $p$. show that $H$ is contained in every sylow $p$ subgroup of $G$,"If $H$ is a normal subgroup of a finite group $G$ and $|H|=p^k$ for some prime $p$. show that $H$ is contained in every sylow $p$ subgroup of $G$ Attempt: $|H|=p^k \implies |G|=p^{n_1} q^{n_2} r^{n_3} \cdots~~|~~n_1 \geq k$ Now, $H$ is also a normal sub group of $G$ and we need to show that $H$ is contained in every $p$ sylow subgroup of $G$ which means in all of $H_{p^{n_1}}, H_{q^{n_2}},H_{r^{n_3}},  \cdots $ Unfortunately, I am not able to decide the strategy to move ahead. How do I move forward? Thank you for your help.","['sylow-theory', 'finite-groups', 'group-theory', 'abstract-algebra']"
910301,"Vertical asymptote, yes or no?","I am working on a problem that will highlight the importance of accuracy and the flaw in approximating certain numbers (very basic stuff). Say you have the following function $$f(x)=\frac{x^2 - b^2}{x + b}$$ If I were to draw this graph, I would get a straight line with a ""gap"" at $x = -b$. But if I were to approximate $b$ in the following manner $$\tilde{f}(x)=\frac{x^2 - b^2 }{x + \tilde{b}}$$ where $\tilde{b}$ is equal to some sort of approximation of $b$ (so imagine $b=\pi$ and $\tilde{b}=3.1415$). Now, if I were to draw this graph, the result would be a straight line like the one I previously drew, but the closer I got to $-\tilde{b}$, the graph would (depending on which direction I came from) jump in two directions. I no longer have that ""unspottable gap"" that I had previously. Why not? Why does it now jump in two directions instead of just behaving like a straight line (but with that ""gap"" at $-\tilde{b}$ still being there, just not visible)? And which one of these would have ""a vertical asympote"" and which one would not?",['functions']
910313,An interesting property of symmetric real matrices with row and column sums zero,"Let $A$ be an $n \times n$ real symmetric matrix with row and column sums zero. For example,
$$
A=\begin{bmatrix}1 & -2 & 1\\
-2 & 1 & 1\\
1 & 1 & -2
\end{bmatrix}.
$$
I have the following interesting observation about $A$ in general. Claim: Suppose $\mathrm{rank}(A)=n-1$, and let $v_1, v_2,\dots ,v_{n-1}$ be the $n-1$ normalized eigenvectors (with unit length) corresponding to the $n-1$ nonzero eigenvalues. Let $\mathbf{V}=[v_1,\dots,v_{n-1}]$ be an $n\times(n-1)$ matrix of which each column $i$ is the eigenvector $v_i$. We have
$$
I-\mathbf{V}\mathbf{V}^{T}=\frac{1}{n}\begin{bmatrix}1 & \dots & 1\\
\vdots & \vdots & \vdots\\
1 & \dots & 1
\end{bmatrix},
$$
where $I$ is the identity matrix, and $n$ is the number of columns. As for our particular $A$ in the display, we have
$$
\mathbf{V}=\begin{bmatrix}\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\
-\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{6}}\\
0 & -\frac{2}{\sqrt{6}}
\end{bmatrix}.
$$
One can easily verify the above claim for this example. I have randomly generated many such matrices, and the claim holds. So it might be correct. My question is how to prove it. After spending many hours, I have made little progress so far. The only thing meaningful I have found is that any eigenvector of $A$ must sum to be zero, because $0=1^TAv=\lambda 1^Tv$. Here $\{\lambda, v\}$ denotes a generic pair of eigenvalue and eigenvector. An additional observation is that all cofactors of $A$ are identical. But these observations are far from enough to understand this claim. Any thought is welcomed. Thanks.",['matrices']
910322,Geometric Product,"I have a problem with the geometric product: In my book the unit trivector is defined like this: $(e_{1}e_{2})e_{3}=e_{1}e_{2}e_{3}$
But that would mean $(e_{1}e_{2})e_{3}= (e_{1} \wedge e_{2})\cdot e_{3}+(e_{1} \wedge e_{2} \wedge e_{3})$ But I thougt it is just $e_{1} \wedge e_{2} \wedge e_{3}$? I could somehow imagine in my head that the plane spanned by $e_{1} \wedge e_{2}$ is perpendicular to the line $e_{3}$ but I'm not sure that it works like that. Is that right? Would make sense.
Ok but why is this true $(e_{1}\wedge e_{2})e_{1}=(-e_{2}e_{1})e_{1}=-e_{2}e_{1}e_{1}=-e_{2}$ because $(e_{1}\wedge e_{2})e_{1}=(e_{1} \wedge e_{2})\cdot e_{1}+ e_{1}\wedge e_{2} \wedge e_{1}$ where the last term is zero. I don't know how the term $e_{1} \wedge e_{2}$ interacts as dot product with $e_{1}$. Similar is $(e_{1}\wedge e_{2})(e_{2}\wedge e_{3})=e_{1}e_{3}$ why isn't this just zero? Because $(e_{1}\wedge e_{2})(e_{2} \wedge e_{3})=(e_{1}\wedge e_{2}) \cdot (e_{2} \wedge e_{3})+e_{1}\wedge e_{2} \wedge e_{2} \wedge e_{3}$ any help?","['geometric-algebras', 'linear-algebra', 'differential-geometry']"
910334,Question about a passage in the Bicommutant Theorem's proof.,"In the Averson's book, in the proof of the Von Neumann's Bicommutant theorem there is this passage: ($A $ is a self-adjoint algebra of operators in $L(H)$) ""Let $\xi_1$ be an element of the Hilbert space $H$ and let $P$ be the projection onto the closed subspace $[A\xi_1 ]$. Note first that $P$ commutes with $A$. Indeed the range of $P$ is invariant under $A$; since $A=A^\ast$, so is the range of $P^\perp=I-P$ and this implies $P\in A'$"" I didn't understand how is used the self-adjointness of $A$ in order to prove that $P^\perp$ have invariant range. When we have proved that the ranges of $P$ and $P^\perp$ are invariant i.e. $AP(H)=P(H)$ and the same with $P^\perp$ how can we conclude that $P \in A'$ (the commutant of $A$)?","['abstract-algebra', 'operator-algebras', 'von-neumann-algebras', 'spectral-theory', 'functional-analysis']"
910338,exercise on pointwise convergence of an (easy) function.,"Exercise 6.2.5. Taken from understanding analysis of Stephen Abbott For each n $\in N$, define $f_n on \ R$ by $$f_n(x) = \begin{cases} 1, & \mbox{if} \ |x| \ge 1/n \\ n|x|, & \mbox{if} \ |x| < 1/n \end{cases}$$ Find the pointwise limit of $(f_n)$ on $R$. I would like to know how to properly think about this, because thus far to demonstrate pointwise convergence I have always taken the limit as n goes to infinity of a function. The Definition of pointwise convergence made it sensible to use this approach. If I naively try to apply that to the conditional statements on x, I obtain $$f(x) =\begin{cases} 1, & \mbox{if} \ |x| \ge 0 \\ n|x|, & \mbox{if} \ |x| < 0 \end{cases}$$ But this seems wrong because no absolute value of x $\in R$ can be less than 0. Moreover I am unsure how to treat $n|x|$ because $n$ goes to infinity but $|x|$ is always smaller than n. So what is the right way to think about this problem? and how does it agree with the def. of pointwise convergence? Thank you in advance","['functions', 'self-learning', 'real-analysis']"
910354,"Which of the following are true and which are false? Let A = {0,1,2,3,4} and let B = P(A) (the power set of A) Confirm my answers.","Can someone confirm my answers? (a) 1∈A (c) {1}∈A (e) {1}⊆A (g) A∈B (b) 1∈B (d) {1}∈B (f) {1}⊆B (h) B⊆B ================================================== (a) is true because 1 is an element (member) of A. (b) is false because the elements of B are precisely the subsets of A, but 1 is an element, not a subset, of A. (c) is false because {1} is not an element (member) of A; instead, {1} is a subset of A. (d) is true because the elements of B are precisely the subsets of A, and {1} is a subset of A. (e) is true because {1} is a subset of (i.e. is contained in) A. (f) is false because 1 is an element of {1}, but as we saw in b), 1 is not an element of B; so {1} is not a subset of B. (g) is true because the elements of B are precisely the subsets of A, and clearly A is a subset of itself. (h) is true since B is clearly a subset of itself.",['elementary-set-theory']
910355,Derivative of the Inverse Cumulative Distribution Function for the Standard Normal Distribution,"As the title says, I am trying to find the derivative of the inverse cumulative distribution function for the standard normal distribution. I have this figured out for one particular case, but there is an extra layer of complexity that has be stumped. Let $0 \le p \le 1$ and let $z = \Phi^{-1}(p)$, where $\Phi^{-1}(p)$ is the inverse cumulative distribution function for the standard normal distribution. Then: $$\frac{\partial \Phi^{-1}(p)}{\partial p} = \left(\frac{\partial \Phi(z)}{\partial z}\right)^{-1},$$ where $\Phi(z)$ is the cumulative distribution function for the standard normal distribution. This yields: $$= \left(\frac{1}{\sqrt{2\pi}} \exp(-z^2/2) \right)^{-1} = \frac{\sqrt{2\pi}}{\exp(-z^2/2)}.$$ I think/hope this is right so far. But now I have $p_1$ and $p_2$ and I need to find the derivative of $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_1}$$ and $$\frac{\partial \Phi^{-1}\left(\frac{p_1}{p_1+p_2}\right)}{\partial p_2}.$$ Any help would be appreciated.","['normal-distribution', 'inverse', 'derivatives']"
910366,Condensing Fractional Logarithms,Does the following condense to the following: $\log_2z+(\log_2x)/2+(\log_2y)/2 = \log_2(z\sqrt{x}\sqrt{y})$ or to $\log_2(z\sqrt{xy})$ ?,"['logarithms', 'algebra-precalculus']"
910373,"There is no continuous mapping from $L^1([0,1])$ onto $L^\infty([0,1])$","There is no continuous mapping from $L^1([0,1])$ onto $L^\infty([0,1])$. Proof: suppose $T:L^1 \rightarrow L^\infty$ continuous and onto. $L^1$ is separable, let $\{f_n\}$  be a countable dense subset. If $T$ is onto, for each $g\in L^\infty$, $g = Tf$ for some $f\in L^1$, and we can approximate $g$ with $Tf_{n_k}$ since $T$ is continuous. We get the contradiction that $\{Tf_n\}$ is a countable dense subset of $L^\infty$. Is this okay? thank you very much!","['lp-spaces', 'proof-verification', 'functional-analysis', 'real-analysis']"
910380,Integral of $\ln(x)\operatorname{sech}(x)$,"How can I prove that:
$$\int_{0}^{\infty}\ln(x)\,\operatorname{sech}(x)\,dx=\int_{0}^{\infty}\frac{2\ln(x)}{e^x+e^{-x}}\,dx\\=\pi\ln2+\frac{3}{2}\pi\ln(\pi)-2\pi\ln\!\Gamma(1/4)\approx-0.5208856126\!\dots$$
I haven't really tried much of anything worth mentioning; I've had basically no experience with $\ln\!\Gamma$.","['improper-integrals', 'hyperbolic-functions', 'integration', 'definite-integrals', 'gamma-function']"
910385,Showing Grothendieck's Vanishing Theorem provides a strict bound,"The following result is due to Grothendieck: If $X$ is a noetherian topological space of dimension $n$ , then for all $i>n$ and all sheaves of abelian groups $\mathscr{F}$ on $X$ , we have $H^i(X,\mathscr{F})=0$ . Exercise III.2.1 in Hartshorne is designed to show that the bound is strict.  I will restate the exercise here. (a) Let $X=\mathbb{A}_k^1$ be the affine line over an infinite field $k$ .  Let $P,Q$ be distinct closed points of $X$ , and let $U=X-\{P,Q\}$ .  Show that $H^1(X,\mathbb{Z}_U)\ne 0$ . (b) More generally, let $Y\subseteq X=\mathbb{A}_k^n$ be the union of $n+1$ hyperplanes in general position, and let $U=X- Y$ .  Show that $H^n(X,\mathbb{Z}_U)\ne 0$ . Here, $\mathbb{Z}_U:=j_!(\mathbb{Z}|_{U})$ is the sheaf obtained by extending $\mathbb{Z}|_U$ by zero outside of $U$ , where $j:U\to X$ is inclusion.  Similarly, for the inclusion $i:Y\to X$ where $Y := \{P, Q\}$ , define $\mathbb{Z}_Y:=i_*(\mathbb{Z}|_Y)$ .  I've been able to solve $(a)$ by making use of the exact sequence of sheaves on $X$ $$0\to\mathbb{Z}_U\to\mathbb{Z}\to\mathbb{Z}_Y\to 0$$ This gives rise to a long exact sequence in cohomology, from which we find $H^1(X,\mathbb{Z}_U)\ne 0$ . I'm having trouble with part $(b)$ and would appreciate some help.  I'm trying to proceed by induction, with the base case given by part $(a)$ .  It seems to me that the inductive step should make use of the fact that if $Y=H_1\cup\ldots\cup H_{n+1}$ is the union of $n+1$ hyperplanes in general position in $\mathbb{A}_k^n$ , then $Y-(H_2\cup\ldots\cup H_{n+1})$ is the complement of $n$ hyperplanes in general position in $H_1=\mathbb{A}_k^{n-1}$ , but I can't seem to work it out.  Thanks in advance for any help.","['homology-cohomology', 'sheaf-theory', 'algebraic-geometry', 'sheaf-cohomology']"
910404,Mixed partial derivatives are different,"Let $f: \Bbb R^2 \to \Bbb R$ be defined as $$f(x) = \left\{ \begin{matrix} x_1^2 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2^2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right), & x_1 x_2 \neq 0, \\
0, & x_1 x_2 = 0. \end{matrix} \right.$$ Notation: $D_j f$ means the partial derivative with respect to the $j$-th coordinate. I have shown that $D_2 D_1 f(0) \neq D_1 D_2 f(0)$. My question : we know $f(x) = 0$ whenever $x_1 = 0$ or $x_2 = 0$. This implies that $D_1 f(0) = D_2 f(0) = 0$ applying the definition. However, would this mean that $D_1 f(x)$ or $D_2 f(x)$ equals zero whenever $x_1 = 0$ or $x_2 = 0$, in the inclusive sense of ""or""? I have used that $D_i f$ is not necessarily zero unless at the origin to show that the mixed partials are different. The expressions for them are $$\begin{align}
D_1 f(x) & = 2x_1 \operatorname{arctan} \left( \frac{x_2}{x_1} \right) - x_2, \\
D_2 f(x) & = x_1 - 2x_2 \operatorname{arctan} \left( \frac{x_1}{x_2} \right).
\end{align}$$ Applying the definition again to this I found $D_2 D_1 f(0) = -1$ and $D_1 D_2 f(0) = 1$.","['multivariable-calculus', 'real-analysis', 'analysis']"
910408,"Geometric interpretation of the norm $\|\vec x\|={(|x_1|+|x_2|)\over 3}+{2\max(|x_1|,|x_2|)\over 3}$","Let $p:\mathbb R^2 \to \mathbb R$ be a norm so that $$
\|\vec x\|
={(|x_1|+|x_2|)\over 3}+{2\max(|x_1|,|x_2|)\over 3}
={{\|\vec x\|_1\over 3}}+{2\|\vec x\|_\infty\over 3}.
$$ I need to graph the neighbourhood of radius $1$ around $(0,0)$ : $V_1 ((0,0))$ with this norm, but I don't even know the points that are in this neighbourhood I really don't know how to geometrically visualize it . I tried to separate the norm in to parts: I want that to find all $(x_1, x_2) \in \mathbb{R}^2$ that satisfy $$
{(|x_{1}|+|x_{2}|)\over 3}+{2\max(|x_1|,|x_2|)\over 3}
< 1
$$ so: $$
\frac{|x_1|+|x_2|}{3}
< \frac{1}{2}
\qquad \text{and} \qquad
\frac{2\max(|x_1|,|x_2|)}{3} < {1\over 2}.
$$ I know that the first inequality is a rotated square (geometrically) and the second one is a square, but from this point I don't see how to find the points that satisfy the given norm and visualize it geometrically.","['multivariable-calculus', 'normed-spaces', 'real-analysis']"
910439,Does $\sum_{i=1}^\infty a_i/i < \infty$ imply that $a_i$ has Cesaro mean zero?,If $(a_i)_{i=1}^\infty$ is a sequence of positive real numbers such that: $$ \sum_{i=1}^\infty \frac{a_i}{i} < \infty. $$ Does this mean that the sequence $(a_i)_{i=1}^\infty$ has Cesaro mean zero? As in $$ \lim_{n\to\infty} \frac{1}{n} \sum_{i=1}^n a_i = 0.$$,"['cesaro-summable', 'sequences-and-series']"
910441,Simplify a boolean algebra expression: xy + xz' + x'yz,"I need to simplify xy + xz' + x'yz into xz' + yz . I know that these expressions are equal in truth value, but I'm not sure how to simplify the first to get the second. Here are the steps I can do: 1) xy + xz' + x'yz 2) y(x + x'z) + xz' 3) y((x + x')(x + z)) + xz' 4) y(x + z) + xz' But that is where I get stuck. Any help you can give me would be great. Thanks.","['boolean-algebra', 'discrete-mathematics']"
910468,Using Direct Proof. $1+2+3+\ldots+n = \frac{n(n + 1)}{2}$ [duplicate],This question already has answers here : Proof that $1+2+3+4+\cdots+n = \frac{n\times(n+1)}2$ (36 answers) Closed 9 years ago . I need help proving this statement. Any help would be great!,['discrete-mathematics']
910479,Simplifying a product written in Capital Pi Notation,"I'm having some trouble figuring out how to simplify Capital Pi Notation. What I tried was to expand the multiplication with various n and tried to find a pattern. Could someone point me in the right direction on how to approach these problems? 
$$\prod_{k=2}^{n} \left(1 - \frac{1}{k^2}\right)$$","['algebra-precalculus', 'products']"
910483,Can one deduce whether a given quantity is possible as the area of a triangle when supplied with the length of two of its sides?,"Recently I have found a question like following: In triangle $ABC$, $AB=AC=2$. Which of the following could be the area of
  triangle $ABC$? Indicate all possible areas: [A] $0.5$ [B] $1.0$ [C] $1.5$ [D] $2.0$ [E] $2.5$ [F] $3.0$ From my points of view, I can only guess one answer if I assume the triangle is a right angle triangle. In that case, the area will be $2$. But the answer showed the result, [A][B][C][D]. So, my question this is there any axiom that the area of the right angle will be the highest area of any type of triangle with the expressing two lengths of it? Thanks in advance.","['geometry', 'triangles']"
910521,How to determine whether three ellipses have at least one common intersection point or not?,How to establish a criterion described in equation so that it is easy to determine whether three ellipses have common intersection area (point) or not? Update,"['geometry', 'algebraic-geometry']"
910539,Intuition behind the definition of linear transformation,"I have studied that given vector spaces $V_1$ and $V_2$, a function $T:V_1 \rightarrow V_2$ is called a linear transformation of $V_1$ into $V_2$, if following two properties are true for all $u, v \in V_1$ and scalar $c$: $(1)$:  $T(u+v) = T(u) + T(v)$ and $(2):  T(cu) = c T(u) $. My questions are $1$: What is the geometrical interpretation of properties $1$ and $2$ which says that $T$ preserves additivity and scalar multiplication . I am not able to see this geometrically. What is the meaning of preserving additivity and scalar multiplication. $2$: At some place I have studied that a linear transformation will be linear if it sends each line to line and planes to planes and so on. How can we interpret this based on these two properties. I need help to understand this. Thank you very much for your time..","['linear-transformations', 'linear-algebra', 'intuition']"
910552,"$\sin(nx)$ does not contain Cauchy subsequence in $L^p([0,2\pi]) $ for $1\leq p < \infty$","$\sin(nx)$ does not contain Cauchy subsequence in $L^p([0,2\pi]) $ for $1\leq p < \infty$ My attempt: Set $f_n(x) = \sin(nx)$. Argue by contradiction, suppose there exists a Cauchy subsequence $f_{n_k}$ in $L^p$ for some fixed $p$, then $f_{n_k}$ converges strongly to some $f \in L^p$, this $f$ must be the zero function since we know that $f_{n_k}$ converges weakly to zero in $L^p$. But $f\equiv 0$ is impossible since $||f_{n_k}||_p$ is bounded below by a positive number. To see this, we look at the preimage 
$$\{{|f_{n_k}|}^p \geq \big( \frac{\sqrt 2}{2}\big)^p \} = \{{|f_{n_k}|}\geq \frac{\sqrt 2}{2} \} $$
the measure of this set is bounded below by $\pi$. By Chebyshev's Inequality ${||f_{n_k}||_p}^p$ is bounded below by $\pi\big( \frac{\sqrt 2}{2}\big)^p $. Is this okay? thank you very much!","['lp-spaces', 'proof-verification', 'functional-analysis', 'real-analysis']"
910589,List of functions $f(cx) = C\cdot f(x)$,"I was looking for some complex functions f(x), which satisfies the condition: $$\exists (c, C) \in \Bbb C^2 \backslash\{(1,1)\}, \forall x \in \Bbb C, f(cx) = C\cdot f(x)$$ Till now I have got $$\begin{array}{c|c}
\text{Function, $f(x)$}&\text{Remarks}\\
x^d&\text{d is some constant}\\
\log(d^x)&\text{d is some constant}\\
sin(x\pi)&-\\
cos(x\pi)&-\\
tan(x\pi)&-\\
|x|&-
\end{array}$$ which satisfies this condition. But I am looking for more complex functions which satisfies the same condition.","['special-functions', 'functional-analysis', 'functions']"
910597,$\int \sqrt{1+\sin ^2 x} dx$ an elliptic integral?,"It seems to be an elliptic integral of the second kind, but when $k=i$? This is going by the definition that $E(\theta,k)=\int_{0}^{\theta} \sqrt{1-k^2 \sin^2x}dx$. That seems a bit off. Or is this not one at all due to the indefinite nature of the integral?",['integration']
910601,Probability: the average times to make all the balls the same color,"Suppose there are n balls with different colors with each other in a bag.
In one loop, One take two balls in sequence out of the bag and replace them with two balls with the same color of the first ball. 
Q: how many loops does it take to make all the balls the same color on average?",['probability']
910603,"If $a_n\to0$, there exists $\pm$ such that $\sum\limits_n\pm a_n$ converges [duplicate]","This question already has an answer here : Forcing series convergence (1 answer) Closed 9 years ago . Our Analysis I lecturer in his last lecture for the course gave us a problem to think about. I've been thinking about it for a while and has been bothering me for some time.
It looks like a generalisation of a theorem( or lemma) in the real 1D case. The problem is as follow: Let $(a_n)$ be a complex sequence tending towards $0$. Show that there exists a sequence $\delta_n\in\{-1,1\}$ for all $n$ such that $\sum\limits_n\delta_na_n$ converges. This is true in the case for real sequence. My only vaguely hopeful line of attacking the problem is to reduce it down to a finite case problem as follows.
Suppose we have $A=\{a_i:n\le i\le m\}$ such that $a_i\le\epsilon$ for some $\epsilon$. 
Then I wish to show that I can choose $\delta_n$ such that the partial sum is always bounded by some multiple of $\epsilon$ (say $\sqrt2$ or possibly $2$).
I suspect this is true but I'm not entirely sure how one goes about doing this since the size of our set A is arbitrary.","['complex-analysis', 'real-analysis']"
910622,Equivalence class for the relation of having the same set of prime divisors,"For an integer $n\in \mathbb{N}$ define $P(n) = \{p : p \mid n \text{, where $p$  is a prime} \}$. For example $P(12)=\{2,3\} $ and $P(1)=\emptyset$. Question: Consider the relation $R$ on $\mathbb{Z}$ defined by $n R m$ $\iff$ $P(n) = P(m)$. Show that $R$ is an equivalence relation. My answer: Let $a=b \pmod m$ and $b=c \pmod m$
Thus, $a-b=0 \pmod m$ and $b-c=0\pmod m$
Combining, $a-c=0 \pmod m$
Thus, $a=c \pmod m$ $\rightarrow$ How do I begin to describe the equivalence class [2] for the relation $R$? I don't quite understand... Any help or advice is appreciated.","['equivalence-relations', 'elementary-number-theory', 'elementary-set-theory', 'prime-factorization']"
910635,"Power of a matrix, given its jordan form","Can someone please explain how to find the power of a matrix $A$, given $A=MJM^{-1}$ where the matrix $J$ is in the Jordan canonical form? Or else please explain how to find the powers of a  matrix $J$ that is in the Jordan canonical form.",['linear-algebra']
910640,Proof that group is commutative if every element is its inverse (feedback wanted),"This is one of my first proofs about groups. Please feed back and criticise in every way (including style & language).
Axiom names ( see Wikipedia ) are italicised . $e$ denotes the identity element. Let $(G, \cdot)$ be a group.
We assume that every element is its inverse.
It remains to prove that our group is commutative.
Non-trivially, $\textit{associativity}$ implies that parentheses are unnecessary.
Therefore, we do not use parentheses,
we will not use $\textit{associativity}$ explicitly. By $\textit{identity element}$, $G \ne \emptyset$.
Now, let $a, b \in G$.
By assumption, $$aa = e \text{ and } bb = e. \quad \text{(I)}$$
By $\textit{closure}$, $ab \in G$.
So, by assumption, $$abab = e.\quad \text{(II)}$$
It remains to prove that $ab = ba$.
\begin{equation*}
\begin{split}
ab &= aeb && \quad\text{by }\textit{identity element} \\
   &= aababb && \quad\text{by (II)} \\
   &= ebabb && \quad\text{by (I)} \\
   &= ebae && \quad\text{by (I)} \\
   &= bae && \quad\text{by }\textit{identity element} \\
   &= ba && \quad\text{by }\textit{identity element}
\end{split}
\end{equation*}
QED","['abstract-algebra', 'group-theory', 'proof-verification', 'soft-question', 'proof-writing']"
910650,What is the Singular Value Decomposition for the Zero Matrix?,"I am interested in the singular value decomposition of a matrix: $\mathbf{M} = \mathbf{U} \mathbf{S} \mathbf{V}^T$. Suppose $\mathbf{M} = \mathbf{0}$ (zero matrix) and square.  Clearly, $\mathbf{S} = \mathbf{0}$, but what about $\mathbf{U}$ and $\mathbf{V}^T$ -- Do they have defined values? When use MATLAB, $\mathbf{U} = \mathbf{V} = \mathbf{I}$, but is this a definition or pure luck?  If so, why is it defined as such, since any matrix for $\mathbf{U}$ or $\mathbf{V}$ will satisfy the decomposition - it does not even need to be orthogonal.  Is there some sort of proof that it must indeed fall to a specific value? >> [U, S, V] = svd(zeros(3))

U =

     1     0     0
     0     1     0
     0     0     1


S =

     0     0     0
     0     0     0
     0     0     0


V =

     1     0     0
     0     1     0
     0     0     1","['linear-algebra', 'svd']"
910678,$\int f = \lim\int f$ but $\int_{E}f\neq\lim\int_{E} f_{n}$,"This is exercise 2.13 in Folland's Real Analysis textbook Let $(X, \mathcal{M})$ be a measurable space. Suppose $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int
 f=\lim\int f_{n}<\infty$. Then $\int_{E} f = \lim \int_{E} f_{n}$ for
   all $E\in\mathcal{M}$. However, this need not be true if $\int
 f=\lim\int f_{n}=\infty$. The first part already been asked and answered here in MSE. As for the counter-example, let $f_{n}=n\chi_{(0, 1/n)}+\chi_{(1,\infty)}$, $f=\chi_{(1, \infty)}$ and $E=(0, 1)$. Then, $f_{n}\to f$ pointwise, $\int f = \lim\int f_{n}=\infty$, yet $\int_{E} f=0\neq 1=\lim \int_{E} f_{n}$. So the exercise is solved. Now my question is: Can we find  $\{f_{n}\}\subset L^{+}$, $f_{n}\to f$ pointwise, and $\int
 f=\lim\int f_{n}=\infty$, and a subset $E$ such that $\lim\int_{E} f_{n}=\infty$ but $\int_{E} f <\infty$? Notation: Here $L^{+}$ is the set of all Borel measurable functions from a space $X$ to $[0, \infty]$. (In the above example, I used $X=\mathbb{R}$ for the domain).","['measure-theory', 'real-analysis']"
910679,Exploiting structure in multilinear equations,"I'm wondering if there are any standard techniques for exploiting structure in multilinear equations. An example of what I have in mind is solving $A_{ab} X_{bc} A_{cd} (B_{ad} B_{bc} + B_{ac} B_{bd}) = RHS_{ad}$ for X. Here, A, B, and X are n-by-n matrices and there is summation over b,c. Of, course, one can view this as a large equation system $M vec(X) = vec(RHS)$ where M has dimension n^2 by n^2. But I'd like to somehow exploit the structure of M to reduce the rank.","['linear-algebra', 'multilinear-algebra']"
910709,If $ab+bc+ca=0$ then the value of $1/(a^2-bc)+1/(b^2-ac)+1/(c^2-ab)$ is,If $ab+bc+ca=0$ then the value of $1/(a^2-bc)+1/(b^2-ac)+1/(c^2-ab)$ is...,['algebra-precalculus']
910734,Solution of partial difference equation,"I want to find the explicit solution of the following difference equation $e_{i,j+1}=re_{i-1,j}+(1-2r)e_{i,j}+re_{i+1,j}+km_{i,j}$ where $r>0$, $k>0$ and $m_{i,j}$ are known and $e_{i,0}=0$. My approach is that as in the solution of PDE by seperation of variables, i.e, $e_{i,j}=X_{i}T_{j}$ but I couldn't go further. Is there any other approach to solve this?","['finite-differences', 'ordinary-differential-equations', 'discrete-mathematics', 'recurrence-relations']"
910803,Firm Non Expansiveness in the Context of Proximal Mapping / Proximal Operators,"$\newcommand{\prox}{\operatorname{prox}}$ Probably the most remarkable property of the proximal operator is the fixed point property: The point $x^*$ minimizes $f$ if and only if $x^* = \prox_f(x^*) $ So, indeed, $f$ can be minimized by find a fixed point of its proximal operator. 
See Proximal Algorithms by Neal Parikh and Stephen Boyd . Question 1 . In the paper given above, author is saying: If $\prox_f$ were a contraction, i.e., Lipschitz continuous with
constant less than $1$ , repeatedly applying $\prox_f$ would find a (here,
unique) fixed point Why the bound on the first-order derivative guarantees finding a fixed point by repeatedly applying proximal operator? Question 2 . Let me quote a paragraph from the same paper: It turns out that while $\prox_f$ need not be a contraction (unless $f$ is strongly convex), it does have a different property, firm nonexpansiveness, sufficient for fixed point iteration: $\|\prox_f(x) - \prox_f(y)\|^2_2 \le (x-y)^T (\prox_f(x) - \prox_f(y))$ $\forall x,y \in \mathbb{R}^n$ Firmly nonexpansive operators are special cases of nonexpansive operators (those that are Lipschitz continuous with constant 1). Iteration of a general nonexpansive operator need not converge to a fixed point: consider operators like $-I$ or rotations. However, it tunrs out that if $N$ is nonexpansive, then the operator $T = (1-\alpha)I + \alpha N$ , where $\alpha \in (0,1)$ has the same fixed points as $N$ and simple iteration of $T$ will converge to a fixed point of $T$ (and thus of $N$ ), i.e. the sequence: $x^{k+1} := (1-\alpha)x^k +\alpha N(x^k)$ will converge to a fixed point of $N$ . Put differently, damped iteration of
a nonexpansive operator will converge to one of its fixed points. Operators in the form $(1-\alpha)I + \alpha N$ , where $N$ is non-expansive and $\alpha \in (0,1)$ , are called $\alpha$ -averaged operators. Firmly nonexpansive operators are averaged: indeed, they are precisely the (1/2)-averaged
operators. Why ""unless $f$ is strongly convex""? What is the intuition behind the given expression for firm nonexpansiveness? How can you show that firm nonexpansive operators are $\alpha$ -averaged with $\alpha = \frac{1}{2}$ ? Is anyone aware of the proof of why proximal map is firm nonexpansive?","['multivariable-calculus', 'convex-analysis', 'fixed-point-theorems', 'proximal-operators', 'convex-optimization']"
910809,Counting bounded integer solutions to $\sum_ia_ix_i\leqq n$,"I want to find the number of nonnegative integer solutions to $$x_1+x_2+x_3+x_4=22$$ which is also the number of combinations with replacement of $22$ items in $4$ types. How do I apply stars and bars for this? What if there is an inequality or the lower bounds on the $x_i$ are not zero? More generally, what do I use if the $x_i$ are multiplied by integer constants, such as in this equation? $$3x_1+2x_2+x_3+x_4=47$$","['integer-partitions', 'combinations', 'faq', 'diophantine-equations', 'combinatorics']"
910821,What is the probability of my sum reaching exactly 10? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 9 years ago . Improve this question I throw a 6-sided dice (with values: 0,1,2,3,4,5) multiple times and add each value to a sum, which is 0 in the beginning. What is the probability of my sum reaching exactly 10, 11, 12, 13, 14? After reaching a requested sum, the sum will return to it's original 0 value. E.g: 5 + 5 = 10, and afterwards the sum returns to 0. 
Also, the probability for each number on the dice is different (it's not a fair dice).","['dice', 'probability']"
910843,Integral of $1/(1+x \tan(x))^2$,How would you solve the following integral? $$\int \frac{1}{(1+x\tan(x))^2} dx$$ Any help would be appreciated.,['integration']
910844,Contour Integral $ \int_{0}^1 \frac{\ln{x}}{\sqrt{1-x^2}} \mathrm dx$,"I need help evaluating this with contour integration
$$
\int_{0}^{1}{\ln\left(\,x\,\right)\over
\,\sqrt{\vphantom{\large A}\,1 - x^{2}\,}}\,{\rm d}x
$$
I am not sure as to how to work with the branch cuts of both
$\ln\left(\,x\,\right)$ and $\sqrt{1 - x^{2}}$ Second part is to evaluate
$$ \int_{0}^1 \frac{\sqrt{\,\vphantom{\large a}\ln\left(\,x\,\right)}}
{\sqrt{\vphantom{\large A}\,1 - x^{2}\,}} \mathrm dx$$","['definite-integrals', 'logarithms', 'branch-cuts', 'complex-analysis', 'contour-integration']"
910894,Help with finding a vertex,"I am having trouble with this math problem. The coordinate of the vertex of $f(x) = ax^2 + bx +c, a \ne 0$, is______. Now I know that to find the $x$ coordinate I would use this formula. $x = \frac{-b}{2a}$ However I am not sure how to find the y coordinate. I also don't know how to find the $x$ coordinate without numbers for $b$ and $a$. I assume that it would just be $\frac{-b}{2a}$ but I am not sure. Any help would be appreciated, thank you for your time.",['algebra-precalculus']
910898,Algebraic process to find numbers so that $xy=45$ and $x+y=18$,"Can someone help me with the following question? The sum of two numbers is $18$ and their product is $45$. Find the numbers. I know that the answer is $15$ and $3$. But how do I find that answer algebraically?
It tried doing it as $x+y=18$ and $xy=45$ and then $x= 18-y$ and I substituted in $xy=45$. I got it as $(18-y)y =45$, and I don't know what to do next.","['quadratics', 'algebra-precalculus', 'systems-of-equations']"
910916,Confused about the $\pm$ sign?,"I have multiple questions about the $\pm$ sign, since it seems to confuse me in general... Question 1: Say I have $15=\pm(a+x)$, Can I use the distributive property so it becomes $15=\pm a \pm x$? Or does that mean I went from 2 to 4 solutions? I get very confused when I encounter this sign in equations I need to simplify. Whenever I need to deal with these kind of situations I tend to just turn it into two equations, $15= a+x$ and $15=-a-x$. But if I were to do that with bigger equations that still need to be simplified it means I'm wasting alot of time since I'm doing twice the labor... Question 2: Does the $\pm$ sign only make sense in equations, or can they be used in normal expressions as well (e.g. $\pm y + 3$). But then I wonder, in what scenario would you do something like this? Question 3: $\sqrt {x^2}+37=y+40$ Say I were to simplify $\sqrt{x^2}$ in that equation, I don't know where I should put the $\pm$ sign. Where would I put it? The extra terms 37 and 40 confuse me...","['notation', 'arithmetic', 'algebra-precalculus']"
910917,Expected length of a random vector,"I meet a basic definition about the expected length of a random vector when reading 
a paper: What is ""expected length"" How to roughly derive both equations (yellow part) 
(Is that Gamma function?) Thanks","['statistics', 'integration', 'random-variables']"
