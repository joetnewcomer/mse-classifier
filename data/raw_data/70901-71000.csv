question_id,title,body,tags
867275,To solve $y'=\frac{\cos y+y\cos x}{x\sin y-\sin x}$,"$$y'(x)=\frac{\cos (y(x))+y(x) \cos (x)}{x \sin (y(x))-\sin (x)}$$
I am self-learning the differential equation from a textbook and I need some help with above equation. I learned I-factor (for 1st order linear ODE), Bernoulli's, and Riccati's. So the problem should be solved by them. The book doesn't have any examples using trig functions. I tried wolfram alpha but the website could not give me ""step-by-step"". Does anyone know why? Wolfram Alpha never had this problem where it could not give ""step-by-step"".",['ordinary-differential-equations']
867276,Find roots of $3z^{100} - e^z$ in the unit disc.,"This question was given in an exam in complex analysis:
Let $f \left( z \right) = 3z^{100} - e^z$. Find all of $f$'s roots in $D \left ( 0,1 \right)$ and show that they are simple roots. I've seen these types of questions with polynomials and the usual technique is to use Rouche's Theorem but it seems that the conditions are not satisfied here. Neither function $3z^{100}$ nor $e^z$ is strictly greater than the other in the unit disc. EDIT: I've misinterpreted the conditions for Rouche's theorem. As mentioned in the comments, the theorem only requires that one function be strictly greater than the other on the boundary of the compact domain (in this case, the unit circle) and indeed that condition is met.","['roots', 'complex-analysis']"
867291,Exceptional locus of birational morphism is a divisor.,"Let $f: V\to W$ be a proper birational morphism of smooth varieties, in a paper I'm reading the author claims that the exceptional locus of $f$ (i.e. the inverse image of the smallest closed set of $W$ outside of which $f$ is an isomorphism) is an effective Cartier divisor, but I don't know how to prove it. I think that in the case of quasi-projective varieties the result is true using Chow's lemma and blow-up approximation(in the sense of theorem 7.17 Chapter 2 of Hartshorne) but I'm not sure if that suffices for the general case.",['algebraic-geometry']
867299,Flex point on an elliptic curve,"I have just started working through Pete Clark's elliptic curve notes, which are available here: http://alpha.math.uga.edu/~pete/EllipticCurves.pdf Early on, in section 2.1 on page 6, it is shown that the group law on $E(K)$ actually gives a group. Our distinguished point, $O$ , is not assumed to be a flex point. Exercise 2.1 asks to show the following are equivalent: i) $O$ is a flex point of the cubic curve $E$ : i.e., the tangent line to $O$ intersects $O$ with multiplicity exactly 3.
ii) $O+O=O$ .
iii) For all $A\in E(K)$ , the inverse of $A$ lies on the line from $A$ to $O$ . Is part ii) correct? It was shown just before this exercise that $O$ is the identity for this group, and it isn't assumed to be a flex point. If $O$ is not a flex point, then to find $O+O$ we take the tangent at $O$ , which by assumption intersects some other point $S\in E(K)$ with $S\neq O$ . We then take the line from $S$ to $O$ and define $O+O$ to be the unique third point on this line. But this should just be $O$ . Is this correct?","['algebraic-geometry', 'elliptic-curves']"
867302,Using decimals of $\pi$ to store data,"I read recently about an idea to, instead of storing actual data, converting the data to a string of digits and then store the index of where this pattern occurs in some number, for example $\pi$. The idea being that the index of the data would take up less storage space than the actual data. Of course, we don't know whether $\pi$ is a normal number and hence we do not know if every finite decimal pattern occurs, but let's assume for the moment that it does (or one simply changes to some proven normal number, like the Copeland-Erdős constant ). The thing that struck me was whether the index of the data might actually be a larger number than the data itself . Does there exist some measure of the probability of finding a decimal sequence of length $n$ before the $m$:th decimal place? For $\pi$ in this case, I doubt there's a general formula. Would it depend on the base? Information or references to other, similar ideas are also very welcome. (Yes, I understand that this method is very impractical for everyday use, I just found the idea intriguinng.)","['information-theory', 'irrational-numbers', 'number-theory']"
867312,Show that the semigroup S(t) here described is a contraction semigroup,"Definition 1 : Let $H$ be a Hilbert space. A strongly continuous semigroup is a family
$\{S(t)\}_{t \ge 0}$ of continuous linear operators $S(t): H \rightarrow
H$ such that $S(0)=I$, where $I$ is the identity operator. $S(t)S(s)=S(t+s)$ for all $t,s \ge 0$. $t\mapsto S(t)x$ is continuous on $[0,\infty)$ for all $x\in H$. A contraction semigroup on a Hilbert space $H$ is a semigroup whose norm
is less or equal than 1; as in $\forall t \in \mathbb{R}^+: \|S(t)\| \le 1.$ Let $S(t)$ be a strongly continuous semigroup defined on a Hilbert space $H$ satisfying:
\begin{align*}
\left\|\int_0^t S(\tau) x\,d\tau\right\|_H \le t\|x\|_H
\end{align*}
How can I show that $S(t)$ is a contraction semigroup, i.e $\|S(t)\| \le
1$ for all $t \ge 0$? I tried to prove it by contradiction. I supposed that $\exists t_0 \in
\mathbb{R}^+: \|S(t_0)\| \gt 1$ but then I noticed that I couldn't use
the inequality because $\left\|\int_0^{t_0} S(\tau) x\,d\tau\right\|_H$ is
always $\le t_0\|x\|_H$ and not greater than anything else. Many thanks in advance, -- Cesar","['semigroup-of-operators', 'functional-analysis']"
867326,Intersection of subset of a natural number a member of the subset?,"I am working with the set theoretic natural numbers and trying to do an exercise from Halmos's Naive Set Theory. The exercise is to show that if $E$ is a nonempty subset of a natural number, then there is $k\in E$ such that for every $m\neq k$ in $E$, we have $k\in m$. I am trying to show that $\bigcap E$ is the $k$ we seek. I have already shown that $\bigcap E$ has the ""minimality"" property, and am now just trying to prove that $\bigcap E \in E$, but am stuck. How should I go about this?",['elementary-set-theory']
867329,Solving base e equation $e^x - e^{-x} = 0$,"So I ran into some confusion while doing this problem, and I won't bore you with the details, but it comes down to trying to solve $e^x - e^{-x} = 0$. I know to solve it, we can rewrite it as $e^x - \frac{1}{e^x} = 0$ and then get LCD so form $\frac{e^{2x} - 1}{e^x} = 0$ and then rewrite it as $e^{2x} = 1$, take the natural logarithm of both sides and it becomes $2x = \log(1)$ or when $x = 0$ (if anything up there is wrong, please tell me) My problem is when I try to do an alternative. Starting with $e^x - e^{-x} = 0$, I try to add to both sides to get $e^x = e^{-x}$, and then take the natural logarithm of both sides to get $x = -x$, which is not a true statement. Could someone explain to me what I'm doing wrong, please? Thanks in advance","['exponentiation', 'exponential-function', 'algebra-precalculus', 'transcendental-equations']"
867332,Matrix restoring (modulo n),"Let $m,n\geqslant 2$, and $A\in \mathcal{M}_n(\mathbb Z)$ such that $\det A \equiv 1 \pmod m$. Does it (necessarily) exist $M\in \mathrm{GL}_n(\mathbb Z)$ such that $A\equiv M \pmod m$?","['modular-arithmetic', 'elementary-number-theory', 'abstract-algebra', 'matrices', 'linear-algebra']"
867347,Can the complement of a subset be realized as a limit or colimit?,"Let $X$ and $Y$ be two sets, and let $f:X\rightarrow Y$. Now consider the posets $(\mathcal{P}(X),\subseteq)$ and $(\mathcal{P}(Y),\subseteq)$ as categories. The induced functions $f^*$ (preimage function), $f_!$ (forward image function), and $f_*$ (the function which sends $B\subseteq X$ to the largest subset $C$ of $Y$ such that $f^*(C)\subseteq B$) can then be interpreted as functors between the said categories. These functions also lead to a nice couple of adjunctions; namely, we have $f_!\dashv f^*\dashv f_*$ Since unions can be realized as colimits and intersections as limits, we get a nice categorical explanation for why $$f^*\left(\bigcup_{C\in\mathcal{C}}C\right)=\bigcup_{C\in\mathcal{C}}f^* (C)\quad\text{and}\quad f^*\left(\bigcap_{C\in\mathcal{C}}C\right)=\bigcap_{C\in\mathcal{C}}f^*(C)$$ However, we also have that $$f^*(Y-B)=X-f^*(B) \text{ for all } B\subseteq Y$$ This leads me to wonder the following: Can complements be realized as a limit or a colimit? Any help is appreciated.","['category-theory', 'elementary-set-theory']"
867350,How do I solve $y' = \sin(x - y)$?,"How can I solve the differential equation:
$$y'=\sin(x-y)$$ Could I do this? $$\frac{dy}{dx}= \sin x \cos y - \sin y \cos x$$ But, how would I continue?",['ordinary-differential-equations']
867370,Differentiating Integrals,"This problem appears as example 2d of Chapter 5 in ""A First Course in Probability - Ross, 8th ed."" Suppose that if you are s minutes early for an appointment, then you incur the cost cs, and if you are s minutes late, then you incur the cost ks. Suppose also that the travel time from where you presently are to the location of your appointment is a continuous random variable having probability density function f . Determine the time at which you should depart if you want to minimize your expected cost. If we let X denote travel time, and you leave t minutes before your appointment, then your cost, $C_t(x)$ is given by: $C_t(x)$ = c(t - X) if X $\le$ t $C_t(x)$ = k(X - t) if X $\ge$ t Therefore, E[$C_t(x)$] = $\int_0^tC_t(x)f(x)dx$ = $\int_0^tc(t - x)f(x)dx$ + $\int_t^{\infty}k(x - t)f(x)dx$ = ct$\int_0^tf(x)dx$ - c$\int_0^txf(x)dx$ + k$\int_t^{\infty}xf(x)dx$ - kt$\int_t^{\infty}f(x)dx$ The value of t that minimizes E[$C_t(x)$] can be obtained by: $\frac{d}{dt}$E[$C_t(x)$] =  ct*f(t) + c*F(t) - ct*f(t) - kt*f(t) + kt*f(t) - k[1 - F(t)] = (k + c)F(t) - k Could someone please explain the steps involved in this differentiation?","['statistics', 'derivatives']"
867374,Riemann Sums as in Königsberger Analysis 1,"Intro : I must take a small detour here which is only relevant if you do not know the book itself and care about my background. I am working with Königsberger Analysis I (can be found on Springerlink). Currently I am in Chapter 11 which focusses on integration. Königsberger takes the following approach to introduce Integral Calculus: He defines step functions $\varphi: [a,b] \to \mathbb{C}$ such that for all $x \in (x_{k-1},x_k)$ the step function $\varphi$ is constant $c_k$ and then defines the Integral of step functions: $$\int_a^b \varphi(x)dx := \sum_{k=1}^n c_k \Delta x_k $$ He defines regulated functions $f:I \to \mathbb{C}$ on an Intervall $I$ such that for all $x \in (a.b)$ the left-sided limit and the right-sided limit exists. He introduces the reader to the 'approximation theorem' Approximation theorem : A function $f$ on a compact Intervall $[a,b]$ is a regulated function if and only if for all $\epsilon > 0$ there exists a step function $\varphi $ such that $|f(x)-\varphi(x)| \leq \epsilon$ for all $x \in [a,b]$ Corollary to the theorem he shows that for regulated functions $f$ the following limit always exists and defines this as the integral of $f$ over $[a,b]$ $$ \int_a^b f(x) dx := \lim_{n \to \infty} \int_a^b \varphi_n(x)dx  $$ My problem : I believe to understand the above topics and associated proofs 'quite well'. However in the last section of the chapter Königsberger tries to make the connection of regulated functions to the Riemann Sum with the following theorem: Theorem : Let $f: [a,b] \to \mathbb{C}$ be a regulated function. Then for all $\epsilon > 0$ there exists a $\delta >0$ such that for all partitions $Z$ of $[a,b]$ with $\max \Delta x_k \leq \delta$ and arbitrary $\xi_k \in [x_{k-1},x_k]$ the following holds: $$ \left| \sum_{k=1}^n f( \xi_k) \Delta x_k - \int_a^b f(x) dx \right| \leq \epsilon $$ I struggle with the first part of the proof. (Page 216) Proof : Königsberger suggest to first verify the theorem for step functions rather than regulated functions and then use the approximation theorem. He says that showing it for step functions should be done by induction after the number of ""jump points"" $m$ (translator suggests saltus and jump discontinuity ) and that it is very easy. I only care about the first induction steps $m=0$ and $m=1$ which Königsberger describes as trivial, for $m=1$ he mentions to choose $\delta:= \epsilon / 4 \| \varphi\|$ and my problem is I really don't see how he obtains these things. After this I can complete the proof on my own because he's very thorough from that point on.","['self-learning', 'real-analysis', 'analysis']"
867388,Birthday Problem - Company Stats Strange or Average?,"I had a birthday problem question that I'm really interested in knowing the answer for: In a group of 2,000 people, what is the probability of one day during the year that no one has that particular day of birth?  What about the probability of having 10 days not having a birthday within them? For the real life story, I work for a company which we have just discovered that there are 10 particular days which no one has a birthday on.  We thought it strange, but we were wondering if this was actually an expected outcome.  I didn't know where else to turn.  Thanks in advance for your input!","['coupon-collector', 'probability', 'birthday']"
867398,What is $\frac{dXX^T}{dX}$?,"Given $X \in \mathbb{R}^{n \times r}$, what is $$\dfrac{dXX^T}{dX}?$$ I'm aware it is a order 4 tensor.","['matrix-calculus', 'linear-algebra']"
867402,Does Strong Convergence in $L^1$ Imply Weak Convergence in $L^2$?,"If I have $f_n \to f$ in $L^1(D)$, where $D \subset \mathbb{R}$ is compact, is it accurate to say $f_n \rightharpoonup f$ in $L^2(D)$? The argument is as follows: consider a simple function $\phi = \sum_i a_i \chi_{D_i}$. Then
\begin{align*}
\lim_{n\to \infty} \int_D \phi f_n & = \lim_{n\to\infty} \sum_i a_i \int_{D_i} f_n \\
& = \sum_i a_i \int_{D_i} f \\
& = \sum_i \int_{D_i} a_i f \\
& = \int_D \phi f 
\end{align*} Then weak convergence would follow from density of simple functions in $L^2$. This seems to make sense, but I couldn't find this result anywhere else - seems surprising for a result that appears so elementary.","['lp-spaces', 'weak-convergence', 'functional-analysis', 'real-analysis']"
867408,To prove this complex polynomial has all zeros on unit circle,"I'm trying to prove a self-inversive polynomial $P(z) = \sum\limits_{n=0}^{N-1}a_nz^n$ has all its roots on the unit circle. The coefficients are such that $ a_n = e^{j(n-\frac{N-1}{2})\pi u_0} - \beta e^{j(n-\frac{N-1}{2})\pi u_1}$ and $0 \leq n \leq N - 1$ These coefficients satisfy $a_n = a^*_{N-1-n}$ i.e. $P(z)$ is self-inversive. The necessary and sufficient condition for a self-inversive polynomial to have all roots on the unit circle is that $P'(z)$ has all its roots in $|z| \leq 1$. I considered Eneström–Kakeya theorem to show $P'(z)$ has all its roots inside unit circle, but the theorem extended for complex polynomial doesn't seem to be valid for the above polynomial. I'm unable to make headway in trying to prove $P(z)$ has all roots on unit circle although numerical experiments show the roots are on unit circle and infact roots of $P'(z)$ are inside the unit disk. Please provide me with any suggestions on how to approach the proof. 
Thanks","['roots', 'complex-analysis', 'polynomials']"
867421,The rows continue to be different to each other [duplicate],This question already has answers here : Why does Bondy's Theorem work? (2 answers) Closed last year . In each position of an $n \times n$ matrix there is a number. We know that all the rows of the matrix are different from each other. Show that we can delete a column so that the rows of the matrix that remain continue to be different to each other. I have absolutely no attempt. Any advice would be appreciated!,"['graph-theory', 'contest-math', 'combinatorics']"
867433,Square root each term (clarification on polynomials?),"So I'm in Algebra 2, and right now we're learning about conic sections (circles/ellipse/etc). I thought some problems in the workbook looked weird, like this one: $\y^2 = x^2 + 16 By my understanding, I should be able to take the square root of the ""y"", ""x"", and ""16"", leaving me with ""y = x + 4,"" since I'm taking the square root of each term (""distributing""?), but my teacher said I have to solve the polynomial first.
 Back in Algebra 1, I was also confused when we started factoring & canceling polynomials, because the teacher never explained when we could/couldn't cancel terms. Could I have some clarification on how polynomials are defined? [And also when to use the principal root?] EDIT: Thanks for the answers guys. Sorry if I wasn't specific in my question (I'm not exactly good at expressing my thoughts). I understood that the two were different (linear/hyperbolic). The question in the book was y^2 - x^2 = 16 . When I moved ""x^2"" to the other side, I assumed that it's not directly attatched to ""16."" In a generic situation, what indicates a polynomial (parentheses)?","['arithmetic', 'algebra-precalculus']"
867461,A closed form for $\int_{0}^{\pi/2} x^3 \ln^3(2 \cos x)\:\mathrm{d}x$,"We already know that \begin{align} 
\displaystyle & \int_{0}^{\pi/2} x \ln(2 \cos x)\:\mathrm{d}x  = -\frac{7}{16} \zeta(3), 
\\\\ &  \int_{0}^{\pi/2} x^2 \ln^2(2 \cos x)\:\mathrm{d}x  = \frac{11 \pi}{16} \zeta(4).  \end{align} Does the following integral admit a closed form? \begin{align} \displaystyle &  \int_{0}^{\pi/2} x^3 \ln^3(2 \cos x)\:\mathrm{d}x  \end{align}","['calculus', 'integration', 'definite-integrals', 'logarithms', 'trigonometry']"
867462,Blocking lines of length $5$ in a $7 \times 8$ matrix; how can we know the solutions have a specific form?,"A friend shared with me the following puzzle he encountered in a Chinese math competition: In a $7 \times 8$ matrix, we place tokens so that any straight line of length $5$ (horizontal, vertical, or on either diagonal) intersects at least one token.  What is the smallest number of tokens required? I'm not asking for a solution to this problem. I know the solution, I want to know the following: Q : I found, by computation, the possible solutions all have a particular form (I list them below).  Can we prove this? (In a way other than ""I tried all the other possibilities"".) To illustrate the problem, This arrangement doesn't work
$$\begin{array}{|ccccccc|}
\hline
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \bullet & \cdot & \cdot \\
\bullet & \bullet & \bullet & \bullet & \cdot & \bullet & \bullet & \bullet \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\hline
\end{array}$$
since e.g. there is the line
$$\begin{array}{|ccccccc|}
\hline
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \square & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \square & \bullet & \bullet & \cdot & \cdot \\
\bullet & \bullet & \bullet & \bullet & \square & \bullet & \bullet & \bullet \\
\cdot & \cdot & \cdot & \cdot & \bullet & \square & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \square & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\hline
\end{array}.$$ This arrangement works
$$\begin{array}{|ccccccc|}
\hline
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\bullet & \bullet & \bullet & \bullet & \bullet & \bullet & \bullet & \bullet \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\
\hline
\end{array}$$
and uses $14$ tokens (which is not the minimum). The solutions are ( spoiler alert ): It requires $11$ tokens, realized by these matrices: $\begin{array}{cc} \begin{array}{|ccccccc|} \hline \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \hline \end{array} & \begin{array}{|ccccccc|} \hline \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \hline \end{array} \\ \begin{array}{|ccccccc|} \hline \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \hline \end{array} & \begin{array}{|ccccccc|} \hline \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \hline \end{array} \\ \begin{array}{|ccccccc|} \hline \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \hline \end{array} & \begin{array}{|ccccccc|} \hline \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \hline \end{array} \\ \begin{array}{|ccccccc|} \hline \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \hline \end{array} & \begin{array}{|ccccccc|} \hline \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot \\ \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot \\ \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet & \cdot \\ \cdot & \cdot & \cdot & \cdot & \bullet & \cdot & \cdot & \cdot \\ \cdot & \cdot & \bullet & \cdot & \cdot & \cdot & \cdot & \bullet \\ \hline \end{array} \\ \end{array}$","['puzzle', 'discrete-mathematics', 'contest-math', 'combinatorics']"
867470,Why are there only a few known Ramsey numbers?,"Can someone explain in a simple way why there are so few known exact Ramsey numbers ? I guess it's because there are no efficient algorithms for this task, but are there so many combinations to test? And an additional question: How are the bounds determined? Why do the bounds, that are known, have those values? Why not i.e. try to take a lower number for the upper bound for some 2-coloring?","['np-complete', 'ramsey-theory', 'combinatorics']"
867481,"If two sets have the same cardinality, then so do their power sets. Converse can't be answered?","The following is my rewrite of this proof for the following assertion : For infinite sets $A, B$, $|A| = |B| \Longrightarrow  \require{cancel} \cancel{\Longleftarrow} |P(A)| = |P(B)|$. $\bbox[2px,border:1px solid black]{\text{ Proof Strategem : }}$ We are given that $f :A \rightarrow B$, is a bijection. How do we construct a bijection between subsets of A and subsets of B? We need a rule whereby, if we take one subset of A, then we can use the rule to uniquely construct a unique subset of B. My intuition on this is to take all the elements of the subset of A, and map them under $f$ to elements of B, which will form a subset of B. $\bbox[2px,border:1px solid black]{\text{ Proof : }}$  In mathematical notation, we define $g$ as: 
$g : P(A) \rightarrow P(B)$ by means of $\color{#009900}{g(S) = \{f(x) : x ∈ S\}} \; \forall S \subseteq A$. $\bbox[2px,border:5px solid grey]{\text{ $g$ onto ? }}$ For all $T \subseteq B$, does there exist $S \subseteq A$ such that $\color{#009900}{g(S) = \{f(x) : x ∈ S\}} \; = T$? Since $f^{-1}$ exists, define $S = \{f^{-1}(x) : x ∈ T\}.$ We must prove $\color{#009900}{\{f(x) : x ∈ S\}} = T$. $\bbox[5px,border:1px solid grey]{T \subseteq g(S)}$ Take $y \in T \iff  f^{-1}(y) ∈ S \iff  \color{ #FF4F00}{f}[f^{-1}(y)] ∈ \color{ #FF4F00}g[S] \iff y \in g[S]. $ $\bbox[5px,border:1px solid grey]{{g(S)} \subseteq T}$ Take $y \in g(S)$ which means there exists $x \in S \ni f(\color{#C154C1}{x}) = y $. Since $x \in S$ means there exists $t \in T \ni \color{#C154C1}{f^{-1}(t) = x}$, substitute this into the previous equation: $f(\color{#C154C1}{f^{-1}(t)}) = y \implies t = y $. Since $t \in T$, thus $y \in T$. $\bbox[2px,border:5px solid grey]{\text{ $g$ 1-1 ? }}$
Suppose we have $G,H ⊆ A$ such that: $g(G) = g(H)$ which means $\{f(x) : x ∈ G\} = \{f(x) : x ∈ H\}$. $\bbox[5px,border:1px solid grey]{\{f(x) : x ∈ G\}  \subseteq \{f(x) : x ∈ H\}}$ Take $g \in G \implies f(g) ∈ \quad g(G) = g(H) \quad \implies f(g) ∈ g(H)$. Thus there exists a $h ∈ H$ such that  $f(g) = f(h)$ $\implies g = h$ because $f$ is a bijection.  Since $g ∈ G$, thus $g ∈ H$. We can prove the other direction by the same argument, just with $G$ and $H$ swapped around. $1.$ In the proof for $g$ onto, how would you (fore)know (ie: divine or presage) to define $S = \{f^{-1}(x) : x ∈ T\}$? $2.$ Would someone please explain the step $f^{-1}(y) ∈ S \iff  \color{ #FF4F00}{f}[f^{-1}(y)] ∈ \color{ #FF4F00}g[S] $? $3.$ Are there easier proofs? The result seems intuitive but the proof is complex. $4.$ Are there any pictures? $5.$ My course doesn't include $ZFC$. So what's the intuition why the converse is false ?","['intuition', 'elementary-set-theory', 'visualization']"
867516,"surjective, but not injective linear transformation","$T$ is a transformation from the set of polynomials on $t$ to the set of polynomials on $t$.  So, the input to $T$ should be a polynomial, and the output should be some other polynomial.  Two common linear transformations are differentiation and integration from $t=0$.  Namely, we can describe differentiation operator $T(p) = \frac{dp}{dt}$ by saying that if $p(t) = a_0 + a_1 t + \cdots + a_n t^n$, then 
$$
T[p(t)] = a_1 + 2a_2 t + \cdots + na_n t^{n-1}
$$
Similarly, we can describe the operator $T(p) = \int_0^t p(x)\,dx$ by saying that if $p(t) = a_0 + a_1 t + \cdots + a_n t^n$, then 
$$
T[p(t)] = a_0t + \frac {a_1}2 t^2 + \cdots + \frac {a_n}{n+1} t^{n+1}
$$ How can i  prove that the first operator is surjective, but not injective, while the second is injective, but not surjective. Some help please.","['calculus', 'algebra-precalculus', 'derivatives']"
867524,Particular solution of the recurrence equation $u_{n+2} + u_n = \sqrt{2}\cos[(n-1)\pi/4]$,"I would like to solve the equation xx recurrence using the operator $E$, ie,
$$
(E^2 + 1)u_n = \sqrt{2}\cos[(n-1)\pi/4] \quad \Rightarrow \quad u_n = \dfrac{1}{E^2 + 1}\{\sqrt{2}\cos[(n-1)\pi/4]\}
$$
I'm having trouble making sense of the term $\dfrac{1}{E^2 + 1}$. Thanks for any help.","['sequences-and-series', 'discrete-mathematics']"
867526,The well-ordering principle can be used to show that there is a unique gcd of two positive integers...,"Question The well-ordering principle can be used to show that there is a unique gcd of two positive integers. Let $a$ and $b$ be positive integers, and let $S$ be the set of positive integers of the form $as+bt$ , where $s$ and $t$ are integers. a) Show that $S$ is non-empty. b) Use the well-ordering property to show that $S$ has a smallest element $c$ . c) Show that if $d$ is a common divisor of $a$ and $b$ , then $d$ is a divisor $c$ . d) Show that $c|a$ and $c|b$ . e) Conclude from (c) and (d) that the greatest common divisor of $a$ and $b$ exists. Finnish the proof by showing that this gcd of two positive integers is unique. This is my first use of well-ordering principle so I need
confirmation. More about my problem of this question in the section Problem below. My Attempt a) $S$ is non-empty since $s$ and $t$ in $as+bt$ can be any non-negative integers. b) From (a), since $S$ is non-empty set of positive integers, by Well-Ordering principle there is a least element $c$ . c) Since $d|a$ and $d|b$ , $$a = dk_1, k_1 \in \mathbb{Z}$$ $$b = dk_2, k_2 \in \mathbb{Z}$$ Thus, $$c = as + bt = (dk_1)s + (dk_2)t = d(k_1s) + d(k_2s) \equiv 0 (\bmod d)$$ d) Suppose $c \nmid a$ , then $a = qc + r, 0 < r < c$ . Suppose now $r \in S$ , this is a contradiction since since we've already established that $c$ is the least element in set $S$ . e) Existence: We have shown a least element $c$ in a non-empty set $S$ , such that $c\mid a$ and $c \mid b$ . Other divisors such as $d$ , divides $a, b, c$ , thus $d \leq c$ , hence gcd $c$ exist. Uniqueness: Suppose $\exists e \mid \gcd(a, b) = e$ then $$e \mid a, e \mid b \rightarrow e \mid c,$$ which is true if and only if $e \leq c$ , considering $\gcd(a, b) = e$ , thus $e = c$ . Problem: (a) and (b) was my very first use of well ordering principle (at least consciously) thus I need confirmation its being used right. (c) I'm quite confident so its unlikely that its wrong. (d) I'm a little shaky on that, is that also a correct use of well ordering principle? (e) I can't find anything wrong with it, but if its wrong just point it out.",['discrete-mathematics']
867585,"Using Green's theorem, with holes in region","I've just learned Green's theorem and I need a little help in solving a problem! I need to calculate  $\oint_c  \vec{F} d \vec{r}  $, when the vector field $ \vec{F} =( \frac{y}{x^2+y^2}+ \frac{y}{(x-2)^2+y^2})\hat{i}-(\frac{x}{x^2+y^2}+ \frac{x-2}{(x-2)^2+y^2})  \hat{j}$ . Curve $C$ has origin at $(0,0)$, and has radius of 10, and circulates counterclockwise. My professor taught how to solve this, but I didn't quite get it. She told us to use Green's theorem. However, the circle with radius 10 has two holes- at (0,0) and at (2,0). So I'm not sure how to solve this. She said to divide the circle into parts, with the holes circulating clockwise.... but how do I do this???And also that I should think of  $\vec{F}=\vec{F}_1+\vec{F}_2$, so $\vec{F}_1=\frac{y}{x^2+y^2}\hat{i}-\frac{x}{x^2+y^2}\hat{j}$ and $\vec{F}_2=\frac{y}{(x-2)^2+y^2}\hat{i}-\frac{x-2}{(x-2)^2+y^2}  \hat{j}$ I know that if I use Green's theorem, the answer would be $0$ anyways because $\frac{\partial N}{\partial x}- \frac{\partial M}{\partial y}= 0$ but I would like to know how to use Green's theorem when the region has holes...","['multivariable-calculus', 'calculus']"
867602,Trouble computing this double integral,"$$\iint_R xe^{xy}~\mathrm{d}A \qquad 0\le x\le 2 \quad 0 \le y \le 1$$ Today I started learning about double integrals on a class I am taking, had good understanding on single-variable integrals but I simply have no idea on what to do here. I am only able to do some simple excercises were I can obviously separate the terms without ""mixing"" the variables, then move them out of one integral, for example: $$\iint_R (x^2y) ~\mathrm{d}y~\mathrm{d}x \qquad 0\le y\le 1 \quad 1 \le x \le2$$
$$=\int_1^2 \left[ x^2\int_0^1 y~\mathrm{d}y \right]~\mathrm{d}x$$ And then it gets easy to do. But on that first one I don't know how to separate them. Could anyone give me a hint on what the next step is?","['definite-integrals', 'multivariable-calculus', 'calculus', 'integration']"
867610,Locally integrable function with a uniform bound...,"I'm a bit lost...  I have a measure space $(\Omega,\mathcal{B}(\Omega),\mu)$  where $\mathcal{B}(\Omega)$ is a Borel set. Let $f$ be a real-valued measurable function on $\Omega$ and $\mathcal{K}$ be the set of all compact sets in $\Omega$. If
$$
\sup_{K\in\mathcal{K}}\int_K \lvert f(x) \rvert\, \mu(dx) < +\infty\,, 
$$ do we have $f\in L^1(\Omega,\mu)$? We can add any nice property on $\Omega$ like locally compact, or $\mu$ being $\sigma$-finite, etc. Thanks in advance... Maybe a hint because it is not clear at  all for me. Sincerely.","['measure-theory', 'integration', 'real-analysis', 'analysis']"
867621,Triangle inequality and homomorphisms,"Here is my situation: I have two homomorphisms $f$ and $g$ from a group $A$ into the complex numbers $\mathbb{C}$. I know that they are 'close' on a subset $B \subseteq A$. More formally there is an $\epsilon > 0$ so that $|f(b)-g(b)| < \epsilon$ for all $b \in B$. Now let $a \in A$ be fixed. I am wondering if there is a $\delta > 0$ such that $|f(ab) - g(ab)|<\delta$ for all $b \in B$. That is are $f$ and $g$ also 'close' on $aB$? I know that $|f(ab)-g(ab)| = |f(a)f(b)-g(a)g(b)|$ and I feel like I have seen a triangle inequality argument that can be applied to things of this form in the past. However, I do not remember the details and I have been unable to reconstruct it myself. Help would be much appreciated.","['inequality', 'group-theory', 'analysis']"
867637,How do I find out whether three 3D vectors can form a right angled triangle?,"I am asking this question for my son who is about finish the twelfth grade. I have already seen this question , however that did not actually answer my query. I have three vectors, \begin{align*}
  \vec{A} &= 3\hat{i} - 2\hat{j} + \hat{k}\\
  \vec{B} &= \hat{i} - 3\hat{j} + 5\hat{k}\\
  \vec{C} &= 2\hat{i} + \hat{j} - 4\hat{k}
\end{align*} and I need to find out whether they can form a right angled triangle. One way to attack the problem will be to find out the  length
of the vectors. $|A|^2 = 9+4+1 = 14$ $|B|^2 = 1+9+25 = 35$ $|C|^2 = 4+1+16 = 21$ And then apply Pythagoras theorem, $|B|^2 = |A|^2 + |C|^2 = 35$ . Also, we need to check whether the angle between these $\vec{A}$ and
$\vec{C}$ is a right angle, $\vec{A}\cdot\vec{C} = 6-2-4 = 0$. Now, what is wrong if I do not use Pythagoras and find out the
three angles between the pairs of vectors and then simply check that
one is a right angle and the sum of the three angles is $180^\circ$?
Definitely at the same time I will need to check that the sum of two sides 
is larger than the largest side.",['geometry']
867638,Geometric Intuition for Dihedral Group Automorphisms,"I noticed the other day that the automorphism group of the dihedral group $D_{2n}$ (of order $2n$) is $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$, the group of affine transformations of the $\mathbb Z$-module $\mathbb Z/n\mathbb Z$. By ""affine transformation"" I mean an invertible function $\mathbb Z/n\mathbb Z\to \mathbb Z/n\mathbb Z$ of the form $x\mapsto ax + b$ (so $a\in(\mathbb Z/n\mathbb Z)^\times$ and $b\in\mathbb Z/n\mathbb Z$); the group multiplication in $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is function composition. Both $D_{2n}$ and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ have a natural geometric interpretation: $D_{2n}$ represents the symmetries of the $n$-gon and $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ represents a nice class of transformations of the ""space"" $\mathbb Z/n\mathbb Z$. The geometric interpretation of $\operatorname{Aff}(\mathbb Z/n\mathbb Z)$ is admittedly more abstract than the first, but I don't think it should be discounted. At any rate, I understand the algebraic reasons why $\operatorname{Aut}(D_{2n}) \cong \operatorname{Aff}(\mathbb Z/n\mathbb Z)$, but it seems like there might also be geometric reasons. Is there an intuitive geometric explanation for why we should expect the automorphism group of a dihedral group to be an affine group? Does anyone know of an insightful way to visualize automorphisms of the dihedral group as affine transformations?","['geometry', 'intuition', 'group-theory', 'abstract-algebra']"
867652,Why do positive definite symmetric matrices have the same singular values as eigenvalues?,I realize that this is because when the eigenvalues are either 0 or 1 they will have the same square root. But why does this happen?,"['positive-definite', 'eigenvalues-eigenvectors', 'svd', 'symmetric-matrices', 'linear-algebra']"
867653,Prove a statement with elements for Set Theory,"I am stuck on this proofing question and I would like some clarification. Q: $A\subseteq B \iff A\cap B^{\prime} = \emptyset$ I already proved that LHS goes to RHS, but I am confused for the other way around because the textbook answer key gives a weird answer. It says that for $A\cap B^{\prime}=\emptyset$, let $x$ be an element of $A$. If x isn't an element of $B$ then, then $x$ is an element of $B^{\prime}$, therefore $x$ is an element of $A\cap B = \emptyset$. Hence $x$ is an element of $B$ and $A$ is a subset of $B$. I am mainly confused about how they say $x$ isn't an element of $B$ and then all of a sudden say $x$ is an element of $B$...? How could it be both an element and not an element of $B$??","['discrete-mathematics', 'elementary-set-theory']"
867663,How to integrate these integrals? $\int{\frac{1}{(x^x-x^{-x})}} dx$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 9 years ago . Improve this question Problem : $$\int{\frac{1}{(x^x-x^{-x})}} dx$$ I need answer about the following problem.
Please help . I will be grateful to you. 
Thanks.","['integration', 'analysis']"
867667,"Choice function for a collection of nonempty subsets of $\{0,1\}^\omega$ [duplicate]","This question already has answers here : Closed 11 years ago . Possible Duplicate: Finding a choice function without the choice axiom Is possible to construct a Choice function for a collection of nonempty subsets of $X=\{0,1\}^\omega$? (Without AC) Attempt: We know an explicit injection $f\colon \mathbb{N}\to X$ is so easy to construct it, I think that a possible choice function $c$ is one which $c(X)=f(1)$, $c(X\setminus\{f(\{1,\ldots,n\})\} )=f(n+1)$, but I don't know how extend it (and I don't know if is possible to do) to all collection $P(X)\setminus\{\emptyset\}$. This exercise comes from Munkres Topology chapter 1 section 9","['elementary-set-theory', 'axiom-of-choice']"
867676,Prove $\int_0^{\infty} \left(\sqrt{1+x^{4}}-x^{2}\right)\ dx=\frac{\Gamma^{2}\left(\frac{1}{4}\right)}{6\sqrt{\pi}}$,"I have in trouble for evaluating following integral $$\int_0^{\infty}  \left(\sqrt{1+x^{4}}-x^{2}\right)\ dx=\frac{\Gamma^{2}\left(\frac{1}{4}\right)}{6\sqrt{\pi}}$$ It seems really easy, but I don't know how to handle it at all.
(The results are well known, here I tried to evaluate it but I failed) I tried to use the relation $$\sqrt{1+x^{4}}-x^{2}=\frac{1}{\sqrt{1+x^{4}}+x^{2}}$$ but I couldn't find the desired results.","['definite-integrals', 'improper-integrals', 'calculus', 'integration']"
867697,How do I prove $\int_{0}^{2a}f(x) dx = \int_{0}^{a}f(x)dx +\int_{0}^{a}f(2a-x)dx$,"I am stuck on this one. I was able to prove $\int_{0}^{a}f(x) dx = \int_{0}^{a}f(a-x)dx $ just by using substitution with $t = a - x$ , but I'm not sure about this one. I've tried quite a few subsitutions, such as $t = 2a - x$ , but none seem to work?","['definite-integrals', 'calculus', 'integration']"
867768,Geometric series of matrices,"I am currently reading 'Vector Calculus, Linear Algebra, and Differential Forms: A Unified Approach' by J. Hubbard and B. Hubbard.  In the first chapter, there is the proposition: Let A be a square matrix. If $|A|<1$, the series $$S=I+A+A^2+\cdots$$
  converges to $(I-A)^{-1}$. The proof first shows that
$$S_k(I-A)=I-A^{k+1}$$
and similarly
$$(I-A)S_k=I-A^{k+1}$$
where $S_k$ is the sum of the first $k$ terms in the series.  Then it shows that
$$|A^{k+1}|\leq|A|^{k+1}$$
and according to the proof in the book, it can be said from this that $\lim_{k\to \infty}A^{k+1}=0$ when $|A|<1$.  Consequently, $S(I-A)=I$ and $(I-A)S=I$.  Therefore $S=(I-A)^{-1}$. However I do not understand how it can be said that $\lim_{k\to \infty}A^{k+1}=0$ when $|A|<1$.  In addition, all similar propositions I have found on the internet state $|\lambda_i|<1$ as the necessary and sufficient condition.  How does this relate to the condition, $|A|<1$?","['matrices', 'sequences-and-series']"
867793,"Find the maximum value of $xy^2z^3$ given that $x^2 + {y}^2 + {z}^2 = 1$, using AM-GM",I've been struggling with this equation and how to find the maximum value it can take: Maximise $xy^2z^3$ given that $x^2+y^2+z^2 = 1$ The question is from the book Introduction to Inequalities - CJ Bradley. Any help would be much appreciated! Thanks,"['inequality', 'algebra-precalculus']"
867818,Show solvability of ODE without explicitly calculating solution,"Show that 
$$
 u + u^{(4)} - u^{(2)} = f
$$
has a solution $u \in H^4(\mathbb R)$ (without explicitly calculuting it) for every $f \in L^2(\mathbb R)$! What criteria for solvability for such ODE's exist?","['ordinary-differential-equations', 'functional-analysis', 'analysis']"
867843,"Learning Fibre Bundle from ""Topology and Geometry"" by Bredon","Bredon defines bundle projection in the following way: $\bf13.1.$ Definition. Let $X,B$ and $F$ be Hausdorff spaces and $p:X\to B$ a map. Then $p$ is called a bundle projection with fiber $F$, if each point of $B$ has neighborhood $U$ such that there is a homeomorphism $\phi:U\times F\to p^{-1}(U)$ such that $p(\phi\langle b,y\rangle)=b$ for all $b\in U$ and $y\in F$. That is, on $p^{-1}(U)$, $p$ corresponds to projection $U\times F\to U$. Such a map $\phi$ is called a trivialization of the bundle over $U$. Then he defines Fibre Bundle $\bf13.2.$ Definition. Let $K$ be a topological group acting effectively on the Hausdorff space $F$ as a group of homeomorphisms. Let $X$ and $B$ be Hausdorff spaces. By a fiber bundle over the base space $B$ with total space $X$, fiber $F$ and structure group $K$, we mean a bundle projection $p:X\to B$ together with a collection $\Phi$ of trivializations $\phi:U\times F\to p^{-1}(U)$, of $p$ over $U$, called charts over $U$ such that: each point of $B$ has a neighborhood over which there is a chart in $\Phi$; if $\phi:U\times F\to p^{-1}(U)$ is in $\Phi$ and $V\subset U$ then the restriction of $\phi$ to $V\times F$ is in $\Phi$; if $\phi$, $\psi\in\Phi$ are charts over $U$ then there is a map $\theta:U\to K$ such that $\psi\langle u,y\rangle=\phi\langle u,\theta(u)(y)\rangle;$ and the set $\Phi$ is maximal among collections satisfying 1, 2, and 3. The he remarks about the condition 3. He says the map $\theta :U \rightarrow K $ exists. The only important thing we demand is the continuity of $\theta$. And the justification is $\quad$ Let us investigate the meaning of condition $(3)$ of Definition 13.2. Given charts $\phi$ and $\psi$ over $U$, $\phi^{-1}\psi:U\times F\to U\times F$ is a homeomorphism commuting with the projections to $U$. Thus we can write $$\phi^{-1}\psi\langle u,y\rangle=\langle u,\mu\langle u,y\rangle\rangle,$$ where $$\mu:U\times F\to F$$ is the composition $p_F\circ\phi^{-1}\psi$ with the projection $p_F:U\times F\to F$, and hence is continuous. Then $\theta:U\to K$ is given by $$\theta(u)(y)=\mu\langle u,y\rangle.$$ My confusion is the map $\theta : U \rightarrow K$ is not very clear to me. He defines how $\theta(u)$ will act on an element of $y\in Y$  $\theta(u)(y)=\mu \langle u,y\rangle$. From that why should we get a map from U to K . I think the effectiveness of the action of $K$ on $F$ is vital. ( I guess one should use the fact $K$ acts on $F$ as same as there is an group homomorphism from $K$ to $\operatorname{Aut}(F)$ and the map is injective because the action is effective.)","['sheaf-theory', 'principal-bundles', 'algebraic-geometry', 'vector-bundles']"
867855,Solutions for $ \frac{dy}{dx}=y $?,"Al-right, this may be a very basic question but I'm confused about this. We all know that one differential equation can only have one solution. Consider: $$ \frac{dy}{dx}=y $$ The solution is: $$ y= e^x = c(1 + x +x^2/2 + ... ) = c( 1 + \int 1\times dx + \int \int 1 \times dx^2  + ...$$ where c is a constant. Wouldn't this work as a solution as well? $$ y = c(... -1/x^2 + 1/x + \log(x) + (x \log(x) - x) + ... = c \times (...+ \frac{d^2 (1/x)}{dx^2} + \frac{d (1/x)}{dx} + (1/x) + \int 1/x \times dx + ... )$$ Can someone tell me why the seond solution is wrong?",['ordinary-differential-equations']
867859,How to solve a non-homogeneous second-order linear difference equation with both a forward and a backward difference?,"Quite a long title for this:
I'm looking for the general solution of the following difference equation:
$$ax_{t+1} -bx_t + x_{t-1} = c + u_t$$
where $a,b,c$ are real constants and $u_t$ is a bounded stochastic disturbance (e.g. a uniformly distributed random variable between -1 and 1 and the $u_t$ are iid). This is the class of difference equations, that Woodford (2003, Interest and prices, chapter 7) solves as the law of motion of Lagrange multipliers $x$. Unfortunately, he just states ""Well, folks, this is the result"" but does not bother explaining or even stating his approach. Now, I'm pretty sure this problem can be tackled with the usual approach for second-order non-homogeneous difference equations with constant coefficients, i.e. the solution should be looking like this (the condition for two distinct real roots of the CE is fulfilled):
$$ x_t = Am_1^t + Bm_2^t + x^*$$ Is this true or am I missing something?","['economics', 'recurrence-relations', 'discrete-mathematics']"
867897,Implicit Derivative approaches,"Sorry for my excessive verboseness... Here's the equation as given: $$x = 10 + \sqrt{x^2 + y^2}$$ Here are my direct implicit steps without modifying original equation: $$\eqalign{
\dfrac{\mathrm d}{\mathrm dx}\left(x\right)& = \dfrac{\mathrm d}{\mathrm dx}\left(10 + \sqrt{x^2 + y^2}\right)\\
\dfrac{\mathrm dx}{\mathrm dx}& = \dfrac{\mathrm d}{\mathrm dx}\left(10 + \sqrt{x^2 + y^2}\right)\\
1 &= \dfrac{\mathrm d}{\mathrm dx}\left(10 + \sqrt{x^2 + y^2}\right)\\
1 &= \dfrac{\mathrm d}{\mathrm dx}\left(10\right) + \dfrac{\mathrm d}{\mathrm dx}\left(\sqrt{x^2 + y^2}\right)\\
1& = 0 + \dfrac{\mathrm d}{\mathrm dx}\left(\sqrt{x^2 + y^2}\right)\\
1 &= \dfrac{\mathrm d}{\mathrm dx}\left(\sqrt{x^2 + y^2}\right)\\
1 &= \dfrac{\mathrm d}{\mathrm dx}\left(\left(x^2 + y^2\right)^{1/2}\right)\\
1 &= \dfrac12 \left(x^2 + y^2\right)^{-1/2} \cdot\dfrac{\mathrm d}{\mathrm dx}\left(x^2 + y^2\right)\\
1 &= \dfrac 1{2 \sqrt{x^2 + y^2}}\cdot\dfrac{\mathrm d}{\mathrm dx}\left(x^2 + y^2\right)\\
1 &= \dfrac 1{2 \sqrt{x^2 + y^2}}\cdot\left(\dfrac{\mathrm d}{\mathrm dx}\left(x^2\right) + \dfrac{\mathrm d}{\mathrm dx}\left(y^2\right)\right)\\
1 &= \dfrac 1{2 \sqrt{x^2 + y^2}}\cdot\left(2x\cdot\dfrac{\mathrm d}{\mathrm dx}\left(x\right) + 2y\cdot\dfrac{\mathrm d}{\mathrm dx}\left(y\right)\right)\\
1 &= \dfrac 1{2 \sqrt{x^2 + y^2}}\cdot\left(2x\cdot\dfrac{\mathrm dx}{\mathrm dx} + 2y\cdot\dfrac{\mathrm dy}{\mathrm dx}\right)\\
1 &= \dfrac 1{2 \sqrt{x^2 + y^2}}\cdot\left(2x + 2y\cdot\dfrac{\mathrm dy}{\mathrm dx}\right)\\
1 &= \dfrac {2x}{2 \sqrt{x^2 + y^2}} + \dfrac {2y}{2 \sqrt{x^2 + y^2}}\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
1 &=\dfrac x{\sqrt{x^2 + y^2}}+ \dfrac{y}{\sqrt{x^2 + y^2}}\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
1 -\dfrac x{\sqrt{x^2 + y^2}} &= \dfrac{y}{\sqrt{x^2 + y^2}}\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
\sqrt{x^2 + y^2}\cdot\left(1 - \dfrac x{\sqrt{x^2 + y^2}}\right) &= y\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
\sqrt{x^2 + y^2} - \dfrac{x\cdot\sqrt{x^2 + y^2}}{\sqrt{x^2 + y^2}} &= y\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
\sqrt{x^2 + y^2} - x &= y\cdot\dfrac{\mathrm dy}{\mathrm dx}\\
\left[\dfrac{\sqrt{x^2 + y^2} - x}y\right] &= \dfrac{\mathrm dy}{\mathrm dx}}$$ This result matches http://symbolab.com However, Wolfram gives: $$\frac{dy}{dx}= \frac{-10}y$$ In an effort to get to Wolfram's result, I tried isolating y first: $$\eqalign{
x &= 10 + \sqrt{x^2 + y^2}\\
10 + \sqrt{x^2 + y^2} &= x\\
\sqrt{x^2 + y^2} &= x - 10\\
\sqrt{x^2 + y^2}^2 &= \left(x - 10\right)^2\\
\sqrt{x^2 + y^2}^2 &= \left(x - 10\right)\left(x - 10\right)\\
\sqrt{x^2 + y^2}^2 &= x^2 - 20x + 100\\
x^2 + y^2 &= x^2 - 20x + 100\\
y^2 &= x^2 - 20x + 100 - x^2\\
y^2 &= -20x + 100}$$ I'm guessing this next step could be problematic by not taking $\pm\sqrt n$ into account. $$\eqalign{y &= \sqrt{-20x + 100}\\
&= \left(-20x + 100\right)^{1/2}}$$ Proceeding to implicit derivative processing: $$\eqalign{
\dfrac{\mathrm d}{\mathrm dx}\left(y\right) &= \dfrac{\mathrm d}{\mathrm dx}\left(\left(-20x + 100\right)^{1/2}\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac{\mathrm d}{\mathrm dx}\left(\left(-20x + 100\right)^{1/2}\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac12 \cdot\left(-20x + 100\right)^{-1/2}\cdot\dfrac{\mathrm d}{\mathrm dx}\left(-20x + 100\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac1{2\sqrt{-20x + 100}}\cdot\dfrac{\mathrm d}{\mathrm dx}\left(-20x + 100\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac1{2\sqrt{-20x + 100}}\cdot\left(\dfrac{\mathrm d}{\mathrm dx}\left(-20x\right) + \dfrac{\mathrm d}{\mathrm dx}\left(100\right)\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac1{2\sqrt{-20x + 100}}\cdot\left(20\dfrac{\mathrm d}{\mathrm dx}\left(x\right) + 0\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac1{2\sqrt{-20x + 100}}\cdot\left(20\dfrac{\mathrm dx}{\mathrm dx}\right)\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac1{2\sqrt{-20x + 100}}\cdot20\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac{20}{2\sqrt{-20x + 100}}\\
\dfrac{\mathrm dy}{\mathrm dx} &= \dfrac{10}{\sqrt{-20x + 100}}}$$ I'm not sure if these next $\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2$ steps are allowed, but it did lead to a simpler result even though it never matched the Wolfram result: $$\eqalign{
\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 &= \left(\dfrac{10}{\sqrt{-20x + 100}}\right)^2\\
\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 &= \dfrac{10^2}{\sqrt{-20x + 100}^2}\\
\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 &= \dfrac{100}{-20x + 100}\\
\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 &= \dfrac{100}{-20\left(x + 5\right)}\\
\left(\dfrac{\mathrm dy}{\mathrm dx}\right)^2 &= -\dfrac5{x + 5}\\
\dfrac{\mathrm dy}{\mathrm dx} &= \sqrt{-\dfrac5{x + 5}}}$$ No matter what I try, I can't figure out at all how Wolfram got such a simple result. So which is the true derivative, and proper approach?","['calculus', 'derivatives']"
867912,Solving inhomogenous ODE,"I have an inhomogenous  ODE. The main issue here is variables are matrices. It is bit of matrix calculus.    A solution would be highly appreciated interms of x . I guess we can use same methods for solving ODEs but have to be careful because these are matrices $R'(x)-(C_1 +C_2 x) R(x) = R_1-C_1 R_0\, x $ where  except x rest are $3\times 3$ matrices means $C_1,C_2,R'(x),R(x)$  all are matrices. x is a scalar variable. $ C_1,C_2,R_0 $ are constant $3\times 3$  matrices . .$C_1$ and $C_2$ are skew symmetric matrices","['ordinary-differential-equations', 'algorithms', 'matrices', 'matrix-calculus', 'matrix-equations']"
867938,Is a norm on $R^n$ linear?,"I was reading the book Linear Algebra Done Right by Axler. In the chapter on inner product space (Ch.6), he defines the norm of x on $R^n$ space as: $||x|| = \sqrt{x_1^2 + ... + x_n^2}$ and says: ""The norm is not linear on $R^n$.  To inject linearity into the discussion, we introduce the dot product."" I don't see why the norm is not linear. If I check a multiplicity for $R^2$, using a scalar of 3 for example $3||x|| =? ||3x||$ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{(3 x_1)^2 + (3 x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =? \sqrt{9(x_1^2 + x_2)^2} $ $3 \sqrt{x_1^2 + x_2^2} =  3 \sqrt{x_1^2 + x_2^2} $ Why is the norm not linear? Axler then says: ""Also, if $y \in R^n$ is fixed, then clearly the map from $R^n$ to $R$ that sends $x \in R^n$ to $x \cdot y$ is linear."" Why is the dot product linear if the norm isn't? Regards,
Madeleine.","['normed-spaces', 'linear-algebra', 'inner-products', 'functions']"
867945,What is the sum of this series: $\frac{2}{\pi}\sum_{k=1}^{\infty}\frac{(-1)^{k-1}}{2k-1}$,Can anyone help me with this? What is the sum of this series: $\frac{2}{\pi}\sum_{k=1}^{\infty}\frac{(-1)^{k-1}}{2k-1}$ I got it after plugging $x=-1$ in a Fourier series Thank you!,"['fourier-series', 'sequences-and-series']"
867961,"Surface area of sphere $x^2 + y^2 + z^2 = a^2$ cut by cylinder $x^2 + y^2 = ay$, $a>0$","The cylinder is given by the equation $x^2 + (y-\frac{a}{2})^2 = (\frac{a}{2})^2$. The region of the cylinder is given by the limits $0 \le \theta \le \pi$, $0 \le r \le a\sin \theta$ in polar coordinates. We need to only calculate the surface from a hemisphere and multiply it by two. By implicit functions we have: $$A=2\iint\frac{\sqrt{\left(\frac{\partial F}{\partial x}\right)^2 + \left(\frac{\partial F}{\partial y}\right)^2 + \left(\frac{\partial F}{\partial z}\right)^2}}{\left|\frac{\partial F}{\partial z} \right|} dA$$ where $F$ is the equation of the sphere. Plugging in the expressions and simplifying ($z \ge 0)$, we get: $$A=2a\iint\frac{1}{\sqrt{a^2 - x^2 - y^2}} dxdy$$ Converting to polar coordinates, we have: $$A = 2a \int_{0}^\pi \int_{0}^{a\sin(\theta)} \frac{r}{\sqrt{a^2 - r^2}} drd\theta$$ Calculating this I get $2\pi a^2$. The answer is $(2\pi - 4)a^2$. Where am I  going wrong?",['calculus']
867967,"Why Limit of $0/x$ is $0$, if $x$ approaches $0$? [duplicate]","This question already has answers here : Limit of $0/x$ as x goes to 0 (3 answers) Closed 9 years ago . It might be a silly question, but to me it is not obvious why the following expression holds: $$
\lim\limits_{x\rightarrow 0}\frac{0}{x}=0 ?
$$","['calculus', 'limits']"
867977,weak convergence of probability measures and unbounded functions with bounded expectation,"Assume that $\mu^n$ are probability measures on $R$ that convergence weakly(-*) to $\mu$, i.e for all $f \in C_b (R)$ (bounded and continuous), we have that $\int f(x) \mu^n(dx) \rightarrow \int f(x) \mu(dx)$. Assume that $g \in C(R)$ is an unbounded function (for example $g(x)=x$) but assume that the integral of $\mu^n$ wrt to $|g|$ is uniformly bounded, i.e. $\sup_n  \int |g(x)| \mu^n(dx) \leq C$ Let us call $g_k(x)$ the function such that $g_k(x)=g(x)$ for $x \in [-k,k]$ and $g_k(x) = g(k)$ for $x>k$ and similar for $x<-k$. Hence $g_k$ is bounded and continuous, hence $\int g_k(x) \mu^n(dx) \rightarrow \int g_k(x) \mu(dx)$ for each $k$. Further we have by dominant converging theorem that for each $n$ we have $\int g_k(x) \mu^n(dx) \rightarrow \int g(x) \mu^n(dx)$ Question Does this also imply that $\int g(x) \mu^n(dx) \rightarrow \int g(x) \mu(dx)$ ? Alternative question If thats wrong for general $g$ is it true for the example $g(x)=x$?","['functional-analysis', 'weak-convergence', 'convergence-divergence', 'probability']"
867992,Function whose gradient is of constant norm,"Let $f:\mathbb R^n\rightarrow \mathbb R$ be a smooth function such that $\|\nabla f(x)\|=1$ for all $x\in \mathbb R^n$ and $f(0)=0$. I would like to prove that $f$ is linear. I first looked at the solution of the O.D.E. $$\dfrac{d\gamma}{dt}(t)=\nabla f(\gamma(t))$$
I noticed that there exists only one solution $\gamma_x$ passing through $x\in \mathbb R^n$ at $t=0$, and such a solution is defined over $\mathbb R$. I also proved that $$\gamma_x(t)=x+t\nabla f(x)$$ and $$f(\gamma_x(t))=f(x)+t.$$ Furthermore, the following hint is given : Show that if $f(x)=f(y)$ then $\langle \nabla f(x),x-y\rangle=\langle \nabla f(y),x-y\rangle=0$. I did it, but now, I don't understand why it follows that $f$ is linear. Edit : I write here the proof of the hint; Let $c:[0,1]\rightarrow \mathbb R^n$ be the (usual) parametrization of the segment $[x+t\nabla f(x),y]$. Since $f(x)=f(y)$, we get $$\begin{align*}|t|=|f(\gamma_x(t))-f(x)| & =|f(x+t\nabla f(x))-f(y)|\\ & =|\int_0^1 \langle \nabla f(c(s)),c'(s)\rangle ds| \\ & \leq \int_0^1 \|c'(s)\|ds \\ & =\|y-x-t\nabla f(x)\|\end{align*}$$
From this inequality, which is true for any $t\in \mathbb R$ and using the fact that $\nabla f(x)$ is a unitary vector, it follows that :
$$t^2\leq \|x-y\|^2 + t^2 +2t\langle x-y,\nabla f(x) \rangle$$
Dividing by $t$ and taking limits to $\pm \infty$ gives the hint.","['ordinary-differential-equations', 'differential-geometry', 'riemannian-geometry', 'gradient-flows', 'real-analysis']"
868000,Alternate translation for: “Every real number except zero has a multiplicative inverse.”,"A given text states, “Every real number except zero has a multiplicative inverse"" (where mul-
tiplicative inverse of a real number x is a real number y such that xy = 1). It offers the following translation: $$\forall x((x\neq 0) \rightarrow \exists y(xy = 1)).$$ I personally translated the statement as: $$\forall x \exists y((x\neq 0)\rightarrow (xy = 1)).$$ Are these two statements logically equivalent?
My reasoning being, for every real number x, there exists a real number y, such that if x does not equal zero, then the product of x and y equals 1.","['logic', 'quantifiers', 'discrete-mathematics']"
868012,Problem in set theory,"Let $h:\mathbb R\to \mathbb R$ such that $\forall x\in \mathbb R\Rightarrow h(x)>0$. Prove that there exist $A\subset \mathbb R$ and $\epsilon>0$ such that $A=_c\mathbb R$ and $\forall x\in A\Rightarrow h(x)>\epsilon$. What i did so far is this. By contradiction, suppose that $\forall A\subset \mathbb R$ or $\forall \epsilon >0$ we have that $A<_c\mathbb R$ and that there is a $x\in A\Rightarrow h(x)\leq \epsilon$. Because $\forall A\subset \mathbb R\Rightarrow A<_c\mathbb R$ is false we have that  for $ A\subset \mathbb R $ and $\forall \epsilon >0\Rightarrow \exists x\in A :0<h(x)\leq \epsilon$. Because $A<_c\mathbb R\Rightarrow A<_c\mathbb R_+$ and thus $h|_A$ cannot be a surjection. But we have that $\forall \epsilon >0\Rightarrow \exists x\in A :0<h(x)\leq \epsilon$. Does this imply that $h|_A$ is a surjection to $\mathbb R_+$? In general i need help with this problem. Any solutions? Should i use somehow the Axiom of Choice?","['elementary-set-theory', 'real-analysis']"
868026,"Given a polygon of n-sides, why does the regular one (i.e. all sides equal) enclose the greatest area given a constant perimeter?","This doesn't require much more than the title. I just need an explanation, but an algebraic proof would be a bonus. We can demonstrate this for quadrilaterals, a square is best as shown by this graph - the area peaks when both sides are equal at 250.","['geometry', 'polygons', 'area']"
868042,Is for open connected $U$ the set $U_\varepsilon$ for small $\varepsilon$ connected?,"Let for an open connected subset $U\subset \mathbb R^n$ and a number $\varepsilon >0$:
$$
U_\varepsilon=\{x\in U: dist (x, \partial U)> \varepsilon \}.
$$
Then  $U_\varepsilon$ is open but in general not connected. 
But I suppose that for sufficiently small $\varepsilon >0$ also  $U_\varepsilon$ is connected. How to prove its? Edit.
Let's assume that additionally $U$ is bounded. Is then $U_\varepsilon$ connected?","['general-topology', 'connectedness', 'metric-spaces', 'euclidean-geometry']"
868066,Stuck trying to prove that $e^{-x^{-2}}$ is $C^{\infty}$ [duplicate],"This question already has answers here : Infinitely differentiable functions: how to prove that $e^\frac{1}{x^2-1}$ has derivative of any order? (2 answers) Closed 9 years ago . This is Spivak's Calculus on Manifolds ex. 2-25, he says Define $f:\mathbb{R}\to \mathbb{R}$ by $f(x) =  \left\lbrace
  \begin{array}{l}
     e^{-x^{-2}} &\text{ if } x \neq 0\\
     0 &\text{ otherwise } \\
  \end{array}
  \right.$. Show that $f$ is $C^{\infty}$ anf $f^{(i)}(0)=0\; \forall i$. I haven't found a way to get a general solution. I only managed to prove: $(1)$ $f$ is continuous at $0$. This follows from $f(0)=0$ and $\lim_{x\to0} f(x)=\lim_{x\to 0}e^{-x^{-2}}=\lim_{x\to 0}\displaystyle\frac{1}{e^{1/x^2}}=0$ since $\displaystyle\frac{1}{x^2}\to+\infty$ if $x\to 0$ then $e^{1/x^2}\to+\infty$ if $x\to 0$, which means that $\displaystyle\frac{1}{e^{1/x^2}}\to 0$ if $x\to 0$ . $(2)$$f'(0)=0$. I got this by calculating the limit $f'(0)=\lim_{h\to 0}\displaystyle\frac{f(0+h)-f(0)}{h}=\lim_{h\to 0}\displaystyle\frac{f(h)}{h}=\lim_{h\to 0}\displaystyle\frac{e^{-h^{-2}}}{h}=\lim_{h\to 0}\displaystyle\frac{1/h}{e^{h^{-2}}}$. The last one follows from L'Hôpital: $\lim_{h\to 0}\displaystyle\frac{\frac{d}{dh}(1/h)}{\frac{d}{dh}(e^{h^{-2}})}=\lim_{h\to 0}\displaystyle\frac{(-1/h^2)}{(e^{h^{-2}})(-2h^{-3})}=\lim_{h\to 0}\displaystyle\frac{h}{2e^{h^{-2}}}=\displaystyle\frac{1}{2}\left(\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}\right)\left(\lim_{h\to 0} h\right) = 0$. $(3)$ $f''(0)=0$ I found that $\displaystyle\frac{df}{dx} =  \left\lbrace
  \begin{array}{l}
     2x^{-3}e^{-x^{-2}} &\text{ if } x \neq 0\\
     0 &\text{ otherwise } \\
  \end{array}
  \right.$, then  I can get $f''(0)$ computing the limit $\lim_{h\to 0}\displaystyle\frac{f'(0+h)-f'(0)}{h} = \lim_{h\to 0}\displaystyle\frac{f'(h)}{h} = \lim_{h\to 0}\displaystyle\frac{2e^{-h^{-2}}}{h^4}= 2\lim_{h\to 0}\displaystyle\frac{1/h^4}{e^{h^{-2}}}=-2\lim_{h\to 0}\displaystyle\frac{-4h^{-5}}{-2h^{-3}e^{h^{-2}}}\\=4\lim_{h\to 0}\displaystyle\frac{1}{h^2e^{h^{-2}}}=4\lim_{h\to 0}\displaystyle\frac{h^{-2}}{e^{h^{-2}}})=-4\lim_{h\to 0}\displaystyle\frac{-2h^{-3}}{e^{h^{-2}}(-2h^{-3})}=-4\lim_{h\to 0}\displaystyle\frac{1}{e^{h^{-2}}}=0$, after using L'Hôpital twice. But still I don't have a clue to prove that $f^{(i)}(0)=0$ for every $i$. One of the ideas I managed was to find a general form for $f^{(i)}$ but after I got $f'$ doesn't seem possible to do so considering that I would have to apply the product rule each time.","['derivatives', 'limits']"
868087,Find the value of $\frac{S_{5}S_{2}}{S_{7}}$,"If $a$, $b$, $c$ $\in \mathbb R$, we define $S_{k}=\frac{a^k+b^k+c^k}{k}$ (where $k$ is a non-negative integer). Given that $S_{1}=0$, find the value of $$\frac{S_{5}S_{2}}{S_{7}}$$ I tried: We are given that $S_{1}=0$ i.e. $a + b + c=0$ $\implies a^3+b^3+c^3=3abc$ or $S_{3}=abc$. Similarly, for getting a relation for $S_{2}$ I squared the given condition. However, for higher powers, finding a condition becomes tedious. Even after raising the given equation to power $5$ and $7$ and then substituting for $S_{5}$ and $S_{2}$, I'm not arriving at a particular answer. However, I think that the fact that the degree of numerator and denominator in $\frac{S_{5}S_{2}}{S_{7}}$ is equal can somehow be used but I'm not getting it. Please Help! Thanks!","['factoring', 'exponentiation', 'algebra-precalculus', 'polynomials']"
868121,Singularities at roots of unity,"I want to construct a function $f$ with the following properties:
$f$ has a singularity at $z=1$, and for any $\zeta = e^{2\pi i\frac{a}{b}}$ with $(a,b)=1$, then
$$\lim\limits_{x\to1^-}\frac{f(\zeta x)}{f(x)}=\frac{1}{b}$$
E.g. $f(-x)/f(x)\to1/2, f(ix)/f(x)\to1/4$ etc... I think it is likely such a function exists. For instance, let 
$$f(z)=\sum\limits_{k=1}^{\infty}\frac{z^k}{1-z^k}$$
Then plotting the graph of $|(f(0.99\exp(2\pi ix))|$ for $0<x<1$ shows Which appears linear! (linear by denominators, i.e. satisfying my conditions). Yet when I try to compute in individual cases, the ratios $\frac{f(\zeta x)}{f(x)}$, they do not seem to obey my conditions. (Or perhaps a software error. Maple has given me tantrums before). My question is, can anybody find a function satisfying my conditions? If not, any related information would be welcome. I know deep down, that the growth rate of a function near roots of unity is wholey determined by the size of the denominator in 
$$\zeta = e^{2\pi i\frac{a}{b}}$$
But I would like to prove it! So any insights I would like to hear.","['asymptotics', 'roots-of-unity', 'complex-analysis']"
868163,The area of circle,"The question is to prove that area of a circle with radius $r$ is $\pi r^2$ using integral. I tried to write $$A=\int\limits_{-r}^{r}2\sqrt{r^2-x^2}\ dx$$
but I don't know what to do next.","['calculus', 'integration', 'definite-integrals', 'area', 'circles']"
868174,Transition kernel that is not Markov,"Let $(X,\mathcal{F})$ and $(Y,\mathcal{G})$ be two measurable space. A transition kernel $K$ is a function $K : X \times \mathcal{G} \to \overline{\mathbb{R}}_+$ suche that $K(\cdot,B)$ is measurable and $K(x,\cdot)$ is a measure for every $B \in \mathcal{G}$ and $x \in X$.
We said that is a markov kernel if the measures $K(x,\cdot)$ are probability measures. I was having problems to prove that the function $$x\to K(x,C^1(x))$$ is measurable for every $C \in \mathcal{F} \otimes \mathcal{G}$ in the general case. When we have a markov kernel is easy to prove this using $\pi-\lambda$ theorem but in the not finite case I don't know if is even true. Any help or reference will be appriciated Coment: If we prove that $x\to K(x,C^1(x))$ is measurable and we have a measure $\mu$ in $X$ we can define the measure $$\nu(C)=\int_X K(x,C^1(x))d\mu(x)$$ and it will be the unique measure in $\mathcal{F} \otimes \mathcal{G}$ such that $$\nu(A\times B)=\int_A K(x,B)d\mu(x)$$","['stochastic-processes', 'measure-theory']"
868199,"Clairauts ""equality of mixed partial derivatives"" theorem (interpretation)","So I know how to prove this theorem via limits or whatever and I'm okay with that. What I'm not okay with is the interpretation. I just can't visualise how this is true in 3d space, any ideas? How do you guys interpret this theorem? My guess is that this theorem is implying that change looks the same from all directions. But even if this is true, I would like to know how this can be visualised. Or tell me if this is really not important and I should just trust what the theorem says? Thanks","['multivariable-calculus', 'calculus', 'partial-derivative']"
868224,Is this Goldbach-type problem easy to solve?,"Problem : Given an odd prime number $p$,   are there odd prime numbers $q$, $p'$, $q'$  such that $\{p,q\} \neq \{ p',q'\}$  and   $p+q = p'+q'$ ? This comment informs that it's an obvious corollary of the Polignac's conjecture . This conjecture is still open, and my problem seems much weaker, so that I ask for a proof.","['prime-numbers', 'goldbachs-conjecture', 'number-theory']"
868282,Use discrete proof to show that $\int f^2 \int g^2 \geq (\int fg)^2$,"One proof of the Schwarz inequality on $\mathbb{R}^n$ is to note that $$(\sum x_i^2)(\sum y_i^2) = (\sum x_i y_i)^2 +
    \sum_{i<j}(x_iy_j - x_jy_i)^2.$$ Spivak's Calculus , 4th ed., exercise 13-39(b), asks us to adapt this discrete proof to a proof that 
$$\left( \int_a^b fg \right)^2 \leq \left( \int_a^b f^2 \right) \left(  \int_a^b g^2 \right)$$
for $f,g$ Riemann integrable functions on $[a,b]$. Here is my attempt; I wonder if this is the way you would do it: First, I claim a lemma: Lemma: To show that $\left( \int_a^b fg \right)^2 \leq \left( \int_a^b f^2 \right) \left(  \int_a^b g^2 \right)$, it is sufficient to show that $\left( \mathcal{R}(fg,P)\right)^2 \leq \mathcal{R}(f^2, P)\mathcal{R}(g^2,P)$ for any tagged partition $P$, where $\mathcal{R}$ is the Riemann sum. Now note that 
$$\sum A_i^2 (t_i - t_{i-1})\sum B_i^2 (t_i - t_{i-1}) = \sum \left( A_iB_i (t_i -t_{i-1})\right)^2 \\ + \sum_{i<j}(t_i - t_{i-1})(t_j - t_{j-1})(A_iB_j-A_jB_i)^2.$$
Let $A_i =f(x_i)$ and $B_i = g(x_i)$, and indeed we see that the inequality holds for all tagged partitions $P$. Is this how you would do it? Another question: it would be more elegant if I were able to adapt this proof to show that if $f \neq \lambda g$, and $f,g$ are continuous, then the inequality is strict. But all the ways I'm thinking of to show this are getting hopelessly complicated. Does anyone have a suggestion? (I can think of other ways to show that the inequality is strict, but I'd like to adapt this proof to do so.) Addendum: @ThisIsMuchHealthier provided an excellent answer below, which relies on the pointwise equality
$$f(x)^2g(y)^2 + f(y)^2g(x)^2 = 2f(x)g(x)f(y)g(y) + (f(x)g(y)-f(y)g(x))^2.$$ How can we adapt this to a proof for $f,g:\mathbb{R}\to \mathbb{C}$? The analogous equality would read
$$|f(x)|^2|g(y)|^2 + |f(y)|^2|g(x)|^2 = 2 \Re \left( f(x)\overline{g(x)}\overline{f(y)}g(y) \right) + |f(x)g(y)-f(y)g(x)|^2.$$
We'd like to turn $2 \Re \left( f(x)\overline{g(x)}\overline{f(y)}g(y) \right) $ into $2f(x)\overline{g(x)}\overline{f(y)}g(y)$ somehow, but it's not clear how to do that. There is a discrete version of this proof for $\mathbb{C}^n$ that relies on 
$$(|x_1|^2 + \dotsb +|x_n|^2)(|y_1|^2 + \dotsb + |y_n|^2) = |x_1\overline{y_1} + \dotsb + x_n\overline{y_n}|^2 + \frac12 \sum_{i, j} |x_iy_j - x_jy_i|^2,$$
which suggests that there should be a way to adapt this for the integral. Any ideas? Edit: I think the answer is that $2 \Re \left( f(x)\overline{g(x)}\overline{f(y)}g(y) \right)$ is sufficient, because 
$$\int_a^b\int_a^b 2 \Re \left( f(x)\overline{g(x)}\overline{f(y)}g(y) \right) dxdy= 2 \Re \int_a^b\int_a^b f(x)\overline{g(x)}\overline{f(y)}g(y)dxdy \\=  2  \int_a^b\int_a^b f(x)\overline{g(x)}\overline{f(y)}g(y)dxdy,$$
because
$$\int_a^b\int_a^b f(x)\overline{g(x)}\overline{f(y)}g(y)dxdy = \int_a^b f(x)\overline{g(x)}dx \int_a^b \overline{f(y)}g(y)dy\\ = \left( \int_a^b f(x)\overline{g(x)}dx\right) \overline{\left( \int_a^b f(y)\overline{g(y)}dy \right)}\\=\left| \left( \int_a^b f(x)\overline{g(x)}dx\right) \right|^2$$
is real. Addendum 2 : The user 900 Sit-ups a day (formerly ""This is much healthier"") challenged me to come up with a similar identity for vector-valued functions $\mathbf{f},\mathbf{g}$. We have the identity
$$(f_i(t)g_j(s)-f_j(s)g_i(t))^2 = f_i(t)^2g_j(s)^2 + f_j(s)^2g_i(t)^2 - 2f_i(t)g_i(t)f_j(s)g_j(s).$$
Summing over all $i,j$ gives
$$|\mathbf{f}(t)|^2|\mathbf{g}(s)|^2+|\mathbf{f}(s)|^2|\mathbf{g}(t)|^2=2\langle \mathbf{f}(t), \mathbf{g}(t) \rangle \langle \mathbf{f}(s),\mathbf{g}(s) \rangle + \sum_{i,j}(f_i(t)g_j(s) - f_j(s)g_i(t))^2 \\
\int_a^b \int_a^b |\mathbf{f}(t)|^2|\mathbf{g}(s)|^2+|\mathbf{f}(s)|^2|\mathbf{g}(t)|^2 dsdt= \int_a^b \int_a^b 2\langle \mathbf{f}(t), \mathbf{g}(t) \rangle \langle \mathbf{f}(s),\mathbf{g}(s) \rangle \\+ \sum_{i,j}(f_i(t)g_j(s) - f_j(s)g_i(t))^2ds dt \\
2\int_a^b |\mathbf{f}|^2 \int_a^b |\mathbf{g}|^2 = 2 \left( \int_a^b \langle \mathbf{f},\mathbf{g} \rangle\right)^2 + \int_a^b \int_a^b \sum_{i,j}\left(f_i(t)g_j(s) - f_j(s)g_i(t) \right)^2ds dt\\
\int_a^b |\mathbf{f}|^2 \int_a^b |\mathbf{g}|^2 =  \left( \int_a^b \langle \mathbf{f},\mathbf{g} \rangle\right)^2 + \frac12 \int_a^b \int_a^b \sum_{i,j}\left(f_i(t)g_j(s) - f_j(s)g_i(t) \right)^2ds dt$$","['definite-integrals', 'riemann-sum', 'calculus', 'integration']"
868299,Expected maximum of a sequence of i.i.d. Poissons,"Let $X_i \sim \mathrm{Pois}(1)$ be a sequence of $n$ i.i.d. random variables (with Poisson distribution with parameter 1). I'm interested in the asymptotic behavior of $$\mathbb E[\max_{i \in \{1\ldots n\}}X_i],$$ i.e., the expected maximum value of the sequence for large $n$. The exact answer is $$\sum^\infty_{k = 0}\left[1-\left(\sum_{i=0}^k \frac{e^{-1}}{i!}\right)^n\right],$$ but I'm not really sure how to massage this into something that's easy to work with. I think the results I'm looking for are in a paper called ""A note on Poisson maxima,"" but I can't find a copy of it online.","['poisson-distribution', 'probability-distributions', 'probability', 'order-statistics']"
868336,Determinant of sum of orthogonal matrix with rank-$1$ matrix,"What is the determinant of the sum of two matrices $$\det (G + S)$$ where $S$ is all zeros except for a single column of $1$'s? $$S = \begin{bmatrix}
0 & ... & 0 & 1 & 0 & ... & 0 \\
0 & ... & 0 & 1 & 0 & ... & 0 \\
\vdots & & \vdots & \vdots & \vdots &  & \vdots \\
0 & ... & 0 & 1 & 0 & ... & 0 \\
\end{bmatrix}$$ I understand this can be solved by breaking up the determinant into columns, but I am unsure of how to do this. Also, $S$ is clearly singular - is there a general rule for the determinant of the sum of a singular and non-singular matrix (i.e. $G$ orthogonal $S$ singular)? Any help greatly appreciated Im essentially asking what happens to the determinant of a matrix when you add $1$ to each entry in a column. Specifically, I am interested in the case where $G$ is orthogonal with $\det(G) = -1$. I would also be interested in the case where we add $1$ to just a single entry.","['matrices', 'linear-algebra', 'determinant']"
868349,Global Max and Min Problem,"I'm working on a problem which asks me to find local and global extrema of the following function. $$f(x,y) = x^2y^2e^{(-x^2 - 2y^2)}$$ I went through and found all of the relevant partial derivatives. \begin{align*}
f_x &= (2xy^2)(e^{(-x^2 - 2y^2)}) + (x^2y^2)(e^{(-x^2 - 2y^2)})(-2x)\\
f_x &= (e^{(-x^2-2y^2)})(2xy^2 -2x^3y^2)\\
\\
f_y & = (2x^2y)(e^{(-x^2-2y^2)}) + (x^2y^2)(e^{(-x^2-2y^2)})(-4y)\\
f_y &= (e^{(-x^2-2y^2)})(2x^2y-4x^2y^3)\\
\\
f_{xx} &= (e^{(-x^2-2y^2)})(-2x)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(2y^2 -6x^2y^2)\\
f_{xx} &= (e^{(-x^2-2y^2)})(-10x^2y^2 + 4x^4y^2 + 2y^2)\\
\\
f_{yy} &= (e^{(-x^2-2y^2)})(-4y)(2x^2y-4x^2y^3) + (e^{(-x^2-2y^2)})(2x^2 - 12x^2y^2)\\
f_{yy} &= (e^{(-x^2-2y^2)})(-20x^2y^2 + 16x^2y^4 + 2x^2)\\
\\
f_{xy} &= (e^{(-x^2-2y^2)})(-4y)(2xy^2 -2x^3y^2) + (e^{(-x^2-2y^2)})(4xy-4x^3y)\\
f_{xy} &= (e^{(-x^2-2y^2)})(-8xy^3 + 8x^3y^3 +4xy - 4x^3y)\\
\end{align*} However, I'm not sure what to do after this. I thought I was  supposed to set $f_x$ and $f_y$ equal to 0 but I don't know how to solve the equations that I get. Can someone please help me? Did I make a mistake while I was determining my partial derivatives? EDIT: I made a mistake calculating the partial derivatives and I edited that",['multivariable-calculus']
868373,A 1-form on a smooth manifold is exact if and only if it integrates to zero on every closed curve,"I am stuck on the following problem, which comes from a old qualifying exam. Prove that a 1-form $\phi$ on $M$ is exact if and only if for every closed curve
c, $\int_{c} \phi =0$. One way is an application of Stokes' theorem,
if $\phi = df$ then $\int_{c}\phi = \int_{\partial C}df = 0$ since $\partial B=\emptyset$. I don't know how to do the other direction. I made an attempt as follows: Choose any $x_0 \in M$ define a function, $f(x)=\int_{x_0}^{x}\phi$. This makes sense since the integral is path independent. Now I want to prove that $f$ is smooth and $df=\phi$. I can't do either. Thanks","['differential-forms', 'manifolds', 'integration']"
868386,Unique line through two points in projective space,"I'm trying to solve exercise I.3.15 in Hartshorne's Algebraic Geometry. The question starts as follows: Projection from a point: Let $ \mathbb{P}^{n } $ be a hyperplane in $ \mathbb{P}^{n+1 } $ and let $ P \in \mathbb{P}^{n +1 } - \mathbb{P}^{n }$. Define a mapping $ \varphi : \mathbb{ P}^{n+1 } - \{P \} \rightarrow \mathbb{P}^{n } $ by $\varphi(Q) =$ the intersection of the unique line containing $ P$ and $Q $ with $ \mathbb {P}^ {n }$. I haven't been exposed to projective geometry prior to this. The question assumes that there is a unique line and the line intersects with the hyperplane uniquely. I'm trying to show this, but I inevitably end up with $ n$ linear equations that are difficult to deal with. Furthermore, some notes online suggest that a transformation can make the hyperplane $x_0 = 0 $ and the point $(1:0 : \cdots 0) $. I can see how a transformation can move the hyperplane, but I can't come up with a transformation that simultaneously moves both the point and the hyperplane. Could you please help me with the following questions: How to show that a unique line passes through the two points and intersects the hyperplane in one point. How to transform the projective space so that the hyperplane is $x_{ 0} =0$ and the point is $(1:0 : \cdots 0) $. Thank you",['algebraic-geometry']
868399,Upper bound on cardinality of a field,"Is there an upper bound on the cardinality of a field? The ""biggest"" fields I know are the field of real numbers, or the field of complex numbers. Is there a field with cardinality greater than continuum?","['cardinals', 'elementary-set-theory', 'model-theory', 'field-theory']"
868400,Showing that Y has a uniform distribution if Y=F(X) where F is the cdf of continuous X,"Let $X$ be a random variable with a continuous and strictly increasing c.d.f. $F$ (so that the quantile function $F^{−1}$ is well-deﬁned). Deﬁne a new random variable $Y$ by $Y = F(X)$ . Show that $Y$ follows a uniform distribution on the interval $[0, 1]$ . My initial thought is that $Y$ is distributed on the interval $[0,1]$ because this is the range of $F$ .  But how do you show that it is uniform?","['probability-theory', 'probability-distributions', 'statistics']"
868409,Is there an analytic function $f : \mathbb{D} → \mathbb{D}$ with $f(0) = 1/2$ and $f′(0) = 3/4?$,"(a) Let $\mathbb{D}$ denote the unit disk. Is there an analytic function $f \colon \mathbb{D} \to \mathbb{D}$ with $f(0) = 1/2$ and $f′(0) = 3/4?$ Either find such a function $f$ or explain why it does not exist. (b) Answer the same question for $f(0) = 1/2$ and $f′(0) = 4/5.$ It seems like I could use the Schwarz lemma, but that is not working out so well.  Any suggestions?  Thanks.",['complex-analysis']
868434,How to differentiate $\lim\limits_{n\to\infty}\underbrace{x^{x^{x^{...}}}}_{n\text{ times}}$? [duplicate],"This question already has answers here : Derivative of $x^{x^{\cdot^{\cdot}}}$? (2 answers) Closed 9 years ago . Let $$f(x)=\lim\limits_{n\to\infty}\underbrace{x^{x^{x^{...}}}}_{n\text{ times}}$$
Is it possible to find $f'(x)$. If yes, please show all steps.","['tetration', 'convergence-divergence', 'derivatives', 'limits']"
868437,"Chern classes of tautological bundle over the Grassmannian G(2,4)","I've the following problem: I know how to calculate Chern classes of the tautological bundle over the Grassmannian $G=G(2,4)$ using the Schubert calculus. If I am right, the Chern character should be
$$1-\sigma_1+\sigma_{1,1}$$
where $\sigma_{i_1,i_2}$ indicates the Schubert cycle corresponding to the Schubert variety $\Sigma_{i_1,i_2}=\{\Lambda\in G\mid\dim(V_{2-i_j+j}\cap\Lambda)\geq j\;\forall j\}$ (here $\{V_j\}\subset V$ is a flag in the 4-dimensional vector space $V$). Now these classes are element in the Chow ring, but I want to work with classes in the integral cohomology group. How can I do this translation? Thank you!","['schubert-calculus', 'algebraic-geometry']"
868455,"Galois closure of $\mathbb{C}(T,\sqrt{T^2+T+1})$ over $\mathbb{C}(T^3)$","I'm trying to solve the following problem, but it's too difficult for me. Let $\mathbb{C}(T)$ be the rational function field over the complex field $\mathbb C$ , and put $L:=\mathbb{C}(T,\sqrt{T^2 + T +1})$ , $K:=\mathbb{C}(T^3)$ . Let $M$ be a Galois closure of $L$ over $K$ . (1) Calculate $[M:K]$ . (2) Find all intermediate fields of $L$ and $K$ . What should I do first?","['abstract-algebra', 'field-theory']"
868474,What does the space of non-diagonalizable matrices look like?,"Let $k$ be a field  $\mathbb C$. Consider the action of $G=GL_n(k)$ by conjugation on the set of $n\times n$ matrices over $k$. The collection $X$ of matrices with repeated eigenvalues over $\overline k$ is a subvariety (as it is the zero set of the discriminant of the characteristic polynomial), and moreover it is preserved by $G$. If we let $k^n\subset X$ be the diagonal matrices with repeated roots, then $Y=X\setminus G(D)$ is the set of non-diagonalizable matrices, and also has an action of $G$. If $k=\overline k$, then every $G$-orbit contains an element in Jordan normal form, and by scaling the off-diagonal entries, we remain in the same conjugacy class, and so we see the corresponding diagonal matrix is in the closure of the orbit. Therefore $Y$ is dense in $X$.  This allows one to compute the dimension of $Y$ (I think). However, I'm not really sure what else to say in describing $Y$. What does $Y$ look like?  I know this is a little vague, but I'm not really sure what a reasonable reformulation would be. Are there good decompositions of $Y$ that help in understanding its structure?  Is it smooth? Is it a manifold?  Can we calculate useful invariants of $Y$, such as the cohomology?  Are we better off understanding the individual orbits? Are there other group actions on $Y$ which elucidate its structure?","['matrices', 'linear-algebra', 'algebraic-geometry']"
868484,A field with characteristic $0$ contains $\mathbb Q$,"To prove that a field $F$ with characteristic $0$ contains $\mathbb Q$ , the following lemma is used. Lemma: Let $R$ be a ring with unity. If the characteristic of $R$ is $0$ , then $R$ contains a subring isomorphic to $\mathbb Z$ . Solution: Let $S$ be a subring of $F$ that is $\approx \Bbb Z$ . Let $T = \{ab^{-1}~~|~~a,b \in S, b \neq 0\}$ . Then it is stated that $T \approx \mathbb Q$ . If we take a map $: ab^{-1} \rightarrow a/b$ . Then, clearly, $T \approx \mathbb Q$ under multiplication. Does isomorphism under addition need to be shown as well? My book further states that the intersection of the subfields of a field is a subfield (called the prime subfield), so there exists a subfield (EDIT: and a prime subfield) isomorphic to the rationals. I don't clearly understand this paragraph, did they just try to show that $T$ is a subfield of $F$ ? The subfield test says that a subset $A$ of a field $F$ forms a subfield iff $(i) ~~a-b \in A ~~\forall~~a,b \in A$ $(ii)~~ab^{-1} \in A ~~\forall~~a,b \in A$ But, we don't know if $ef^{-1}-gh^{-1} = e (f^{-1}-e^{-1}gh^{-1})\in T?~~|~~ e,f,g,h \in S?$ . I don't think $T$ should be a sub field. What is the book trying to say? Thank you for your help ..","['ring-theory', 'abstract-algebra']"
868485,about the intersection of nested intervals,"Consider a sequence $\{a_n\}$ (we have not informations about its convergence) and moreover consider a sequence of semi-open intervals of $\mathbb R$: $$\left[\frac{a_0}{2^0},\frac{a_0+1}{2^0}\right[\supset \left[\frac{a_1}{2^1},\frac{a_1+1}{2^1}\right[\supset\cdots\supset\left[\frac{a_n}{2^n},\frac{a_n+1}{2^n}\right[\supset\cdots$$ Can I conclude that the intersection is only a point ? Be aware of the fact that I can't use the Cantor intersection theorem since my intervals are not closed! Many thanks in advance.","['general-topology', 'real-analysis']"
868493,Space of Alternating $k$-Tensors Notation,"I will be taking a Differential Geometry class in the Fall, so I decided to get somewhat of a head start by going through Spivak's ""Calculus on Manifolds."" Before reading, though, I saw the Addenda at the end, which stated that his notation $\Lambda^{k}\left(V\right)$ for the space of alternating $k$-tensors was incorrect, although it is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for fin. dim. $V$ (and, after a little more digging around on Wikipedia , is naturally isomorphic to $\left(\Lambda^{k}\left(V\right)\right)^{*}$ in general). Is the notation suggested by Spivak, $\Omega^{k}\left(V\right)$, standard or is there some other notation that is typically used? EDIT: To quote Spivak: ""Finally, the notation $\Lambda^{k}\left(V\right)$ appearing in this book is incorrect, since it conflicts with the standard definition of $\Lambda^{k}\left(V\right)$ (as a certain quotient of the tensor algebra of $V$). For the vector space in question (which is naturally isomorphic to $\Lambda^{k}\left(V^{*}\right)$ for finite dimensional vector spaces $V$) the notation $\Omega^{k}\left(V\right)$ is probably on the way to becoming standard."" I don't know if this is still the case, though, or if his use of $\Lambda^{k}\left(V\right)$ became the standard.","['notation', 'tensors', 'differential-geometry']"
868510,Direct product of finitely many Noetherian non-unital rings is Noetherian,"Let $A_1, A_2,...,A_n$ be Noetherian rings (not necessarily unital). Is the direct product $A:=A_1×A_2×⋯×A_n$ necessarily a Noetherian ring? If $A_1, A_2,...,A_n$ are unital, then one can prove that the ideals of $A$ have the form $I_1 \times I_2 \times \dotsc \times I_n$, where $I_k$ is an ideal of $A_k$, and then show that indeed $A$ is Noetherian. But what about the general question?","['ring-theory', 'abstract-algebra', 'noetherian']"
868533,"Prerequisites to ""Applications of Lie Groups to Differential Equations""","I'm currently a 4th year student at a university. I've become close with a professor and we talked about the topic of lie groups in differential equations. He then offered to do a reading course with me, after he sent me a few papers about the topic of lie group analysis in epidemic models. Since it has been approved I got cold feet. In all honesty, my experience in analysis is not very good. I have only taken a 200-level Analysis course with Steven Lay's book and barely got through. The textbook we are using is Applications of Lie Groups to Differential Equations by Peter J Olver . The textbook says they only assume an elementary understanding of analysis. I have taken a lot of algebra (rings, fields, groups, galois theory) and have done much better in those classes. I'm not entirely worried about the groups/algebra part. I did go over some of the theorems Olver stated we should know in the opening and I understood the statements and proofs quite easily. Anyway, my question is how should I prepare for this course? Is there a sort of textbook which will go over preliminaries to this textbook? Is there other more applied/easier textbooks to go along with this textbook?","['ordinary-differential-equations', 'lie-algebras', 'lie-groups', 'soft-question']"
868545,"$\int_0^1f(x)dx = 2, \int_0^1g(x)dx = 1, \text{and} \int_0^1[f(x)]^2 dx ≤ C$ for some constant $C > 4.$","Suppose $f$ and $g$ are nonnegative measurable functions on the interval $[0,1],$ with the properties $$\int_0^1 f(x)\,dx = 2, \int_0^1g(x)\,dx = 1, \text{ and }\int_0^1[f(x)]^2 dx \le C$$ for some constant $C > 4.$ Let $E = \{x∈[0,1]:f(x)>g(x)\}$. Show that $E$ has measure $m(E) \ge 1/C.$ I am not sure what I should use for this.  Maybe Holder?  Any suggestions?","['measure-theory', 'real-analysis', 'analysis', 'lebesgue-integral', 'lebesgue-measure']"
868559,"Difficult infinite integral involving a Gaussian, Bessel function and complex singularities","I've come across the following integral in my work. $$\intop_{0}^{\infty}dk\, e^{-ak^{2}}J_{0}\left(bk\right)\frac{k^{3}}{c^{2}+k^{4}}
 $$ Where $a$,$b$,$c$ are all positive. I've seen and evaluate similar integrals without the denominator using resources like Watson's Theory of Bessel Functions, but I've had no luck finding anything resembling this integral. It would get me a very awesome result if I were to evaluate this. Does anyone have any ideas on how to approach it? edit: Some of my attempts so far include: 1)Using ""Infinite integrals involving Bessel functions by an improved approach of contour integration and the residue theorem""  by Qiong-Gui Lin. This (like other residue theorem approaches) doesn't seem to work since the gaussian blows up on the imaginary axis. 2)I recall seeing expressions like $Z_u(ax)X_v(b\sqrt x)$ where Z,X are some variation of bessel functions in Gradshteyn, Ryzhik. This inspired me to write $e^{-ax^2}=\sum_{k=-\infty}^{\infty}I_{k}(-ax^{2})$ and substitute $t=x^2$, then integrate term by term. This hasn't gotten me anywhere either.","['definite-integrals', 'bessel-functions', 'special-functions', 'integration']"
868578,Do differentiable functions preserve measure zero sets? Measurable sets?,"Consider Lebesgue measure on $\mathbb{R}$ and let $f:\mathbb{R}\to\mathbb{R}$ be differentiable.  Does $f$ necessarily preserve measure zero sets?  Does $f$ necessarily preserve measurable sets? Note that if $f$ is $C^1$ then $f$ preserves measure zero sets since $C^1$ functions are locally Lipschitz.  Therefore $C^1$ functions also preserve measurable sets since a measurable set is the union of an $F_\sigma$ set and an measure zero set, and continuous functions preserve $F_\sigma$ sets.  More generally if $f$ is absolutely continuous on each interval then $f$ preserves both measure zero sets and measurable sets. However, I'm not sure about the differentiable case.  I would guess that the answer to both questions is no.  I'm interested in a counter example or proof in each case.","['measure-theory', 'examples-counterexamples', 'real-analysis']"
868580,Calculus Implicit Differentiation and Concavity,Consider the relation $4x^2 - y^2 = -2$ (a) Use implicit differentiation to calculate $dy/dx$ and find all critical points of the curve. (b) Calculate the second derivative and determine the function's concavity at each critical point. (c) Graph and clearly label the relation using (a) and (b). What type of curve is described by the relation? So this seems like an easy question but there are somethings that confuse me: I did part (a) getting the derivative as $\large\frac{4x}{y}$  and to find the critical points I guess its just at $x = 0$. For part (b): the second derivative I calculated to be $(4y^2 - 16x^2)/y^3$ but I don't know how to determine the functions concavity at the critical point cause when I plug in $x = 0$ I still have another variable... so help me with that please... After doing that I wouldnt mind some help on how to draw the graph :p,"['implicit-differentiation', 'calculus', 'derivatives']"
868582,Help with composite functions?,Suppose that $u$ and $w$ are defined as follows: $u(x) = x^2 + 9$ $w(x) = \sqrt{x + 8}$ What is: $(u \circ w)(8) = $ $(w \circ u)(8) = $ I missed this in math class. Any help?,['algebra-precalculus']
868595,How to express membership to at least $m$ sets in a sequence of sets.,"Suppose we have a sequence of sets $(A_{n})$. Pick some positive integer $m$. How would you express the set of all points that belong to at least $m$ sets in the sequence $(A_{n})$? I tried toying around with combinations of $\bigcup$'s and $\bigcap$'s, but to no avail. (I was thinking about the meaning of $\limsup A_{n}$ and $\liminf A_{n}$ and trying to modify it.) Any hints, help would be great!","['elementary-set-theory', 'real-analysis']"
868598,∅ ⊆ { ∅ } Is this true or false?,True or false? Im guessing true because an empty set is a subset of every set. Is this a correct assumption? The only time an empty set is not a subset of something is when its a proper subset of an empty set correct?,['elementary-set-theory']
868604,Sum of Nonnegative Matrix and Diagonal Matrix,"Setup: Let $D = D^T > 0$ be a positive definite and diagonal $n\times n$ matrix, and let $A = A^T \in \mathbb{R}^{n\times n}$ be nonnegative with zero diagonals. That is, $a_{ij} \geq 0$ for $i\neq j$ and $a_{ii} = 0$. In my particular case, $A$ is an adjacency matrix of an undirected graph. Question : Give necessary and sufficient conditions under which $D - A$ positive definite. Failing that, what's a necessary condition so I can understand the possible gap between necessity and sufficiency. Partial Answer : A sufficient, but I believe not necessary, condition is that $\min_i D_{ii} > \rho(A) = \lambda_{\rm max}(A)$, where the last equality follows from the Perron theorem. From the Disc Theorem, another sufficient condition is that $D_{ii} \geq \sum_{j=1}^n a_{ij}$ with strict inequality in at least one row. Happy Thinking,
-John","['matrix-decomposition', 'matrices', 'linear-algebra', 'algebraic-graph-theory']"
868615,"In a class of 65, there are twice as many maths students as biology students.","I have a task: In a class of $65$, there are twice as many maths students as biology students. If $12$ biology students do not take maths and $15$ students take neither of these subjects, how many students take maths but not biology? A. 7 B. 19 C. 26 D. 31 E. This is impossible I found out that the result is $31$ (Answer D) but only by using the answer as help.  Is there another mathematical way? I tried out a lot but I couldn't find out. Thanks!",['elementary-set-theory']
868635,How find the value of the $x+y$,"Question: let $x,y\in \Bbb R $, and such
  $$\begin{cases}
3x^3+4y^3=7\\
4x^4+3y^4=16
\end{cases}$$ Find the $x+y$ This problem is from china some BBS My idea: since
$$(3x^3+4y^3)(4x^4+3y^4)=12(x^7+y^7)+x^3y^3(9y+16x)=112$$
$$(3x^3+4y^3)^2+(4x^4+3y^4)^2=9(x^6+y^8)+16(y^6+x^8)+24x^3y^3(1+xy)=305$$ then I can't  Continue","['algebra-precalculus', 'systems-of-equations']"
868643,Integration by Euler's formula,"How do you integrate the following by using Euler's formula, without using integration by parts? $$I=\displaystyle\int \dfrac{3+4\cos {\theta}}{(3\cos {\theta}+4)^2}d\theta$$ I did integrate it by parts, by writing the $3$ in the numerator as $3\sin^2 {\theta}+3\cos^2{\theta}$ , and then splitting the numerator. But can it be solved by using complex numbers and the Euler's formula?","['calculus', 'integration', 'complex-analysis']"
868682,Continuous function which has only rational values. [duplicate],"This question already has answers here : Is a rational-valued continuous function $f\colon[0,1]\to\mathbb{R}$ constant? (5 answers) Closed 9 years ago . If $f:[a,b]\to\mathbb{R}$ is a continuous function and $f(x)\in\mathbb{Q}$ for all $x\in[a,b]$ then what can say about $f$? My try: I think f should be constant, if it is not constant then it contradicts the continuity. Can anyone prove that f is constant?","['functions', 'real-analysis']"
868695,How to find sums like $\sum_{k=0}^{39} \binom{200}{5k}$,"How do I find sums like these?-- $$S=\displaystyle\sum_{k=0}^{39} \dbinom{200}{5k}$$ that is, when there is a summation of binomial coefficients, but with jumps of some terms..?","['summation', 'calculus', 'binomial-coefficients']"
868713,Sets as extremely trivial groups,"A group is a structure defined upon an underlying set which is endowed with a single binary operator that has some rules attached to it. I was wondering whether one could describe a set itself as being a group defined upon itself such that the binary operator is ""empty""/""null"" and thus no real structure is established. To me, it makes some sense to build structures up by making the relations increasingly interesting: the most basic entity could be taken to be the set (endowed with the empty operator), and then a magma is pretty much the next most trivial, and then monoids, groups, rings, and fields emerge (in increasing order of ""complexity"" of structure in some sense; note that I skipped some stuff, ignored others, and eventually introduced multiple operators per structure). On the other hand, I am having trouble conceptualizing sets to have even just the empty operator. Elements of sets do not interact with one another and even the empty operator would have to take an ordered pair of input and map it to... I am not sure what, maybe to ""undefined"" or something? These two views seem to oppose each other. I prefer the former and view the latter as just some technical problems. But how can one be rigourous in this treatment?","['elementary-set-theory', 'abstract-algebra', 'magma']"
868720,"Integration by substitution, why do we change the limits?","I've highlighted the part I don't understand in red. Why do we change the limits of integration here? What difference does it make? Source of Quotation: Calculus: Early Transcendentals, 7th Edition, James Stewart","['definite-integrals', 'calculus', 'integration']"
868735,Is $\sum\limits_{n=1}^\infty \sin{\frac{(-1)^{n+1}}{n}}$ convergent?,"$$
\mbox{Is}\quad
\sum_{n=1}^\infty \sin\left(\left[-1\right]^{n + 1} \over n\right)
\quad\mbox{ convergent ?.}
$$ $$
\mbox{I know that }\quad\sum_{n=1}^\infty \frac{(-1)^{n+1}}{n}\quad
\mbox{is convergent.}
$$ $\frac{\sin x}x $  goes to 1 as x goes to $0$. So I feel that the series should be convergent, but  I can't prove it rigorously.","['sequences-and-series', 'convergence-divergence', 'calculus', 'divergent-series']"
868746,Fibers of toric morpisms,"Let $f: X(\Delta_1) \to X(\Delta_2)$ be a toric morphism of toric varieties, and let $\sigma \subset \Delta_2$ be a cone, then for any point in the corresponding orbit $x \in O(\sigma)$ the fiber $f^{-1}(x)$ is the same. Is there a way to describe this fiber in terms of fans? Is it true that the fibers are always toric varieties? In this case is there a way to see corresponding fan?","['algebraic-geometry', 'reference-request', 'toric-geometry']"
868785,How many positive integer solutions are there to the equality $x_1+x_2+...+x_r= n$?,"The original problem is there are $r$ identical boxes and $n$ identical balls.  Every box is nonempty.  Then how many ways of putting balls in boxes? It is equivalent to the problem of finding integer solutions for the equality:
$$x_1+x_2+...+x_r= n$$
and $x_i>0$ for all $1\leq i\leq r$. Editted by @Sil: Replaced with equality to avoid confusion that this question deals with $x_1+x_2+\dots+x_r\leq n$, which it does not!","['balls-in-bins', 'discrete-mathematics', 'combinatorics']"
868787,Dual of $l^\infty$ is not $l^1$,"I know that the dual space of $l^\infty$ is not $l^1$, but I didn't understand the reason.
Could you give me a example of an $x \in l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is not a linear bounded functional on $l^\infty$, or maybe an example of a $x \notin l^1$ such that if $y \in l^\infty$, then $ f_x(y) = \sum_{k=1}^{\infty} x_ky_k$ is a linear bounded functional on $l^\infty$?","['lp-spaces', 'dual-spaces', 'functional-analysis', 'banach-spaces']"
