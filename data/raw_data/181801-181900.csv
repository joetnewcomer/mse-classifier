question_id,title,body,tags
3317035,Borel $\sigma$-algebra of a Borel subset,"Let $(X, \tau)$ be a topological space. Then $\sigma(\tau)$ is the Borel $\sigma$ -algebra on $(X, \tau)$ . For any subset $Y \subseteq X$ the subspace topology on $Y$ is $\tau|Y = \{ G \cap Y \mid G \in \tau \}$ and the trace $\sigma$ -algebra on $Y$ is $\sigma(\tau)|Y = \{ B \cap Y \mid B \in \sigma(\tau) \}$ . It holds $\sigma(\tau|Y) = \sigma(\tau)|Y$ . If $Y \in \sigma(\tau)$ then $\sigma(\tau)|Y \subseteq \sigma(\tau)$ , hence $\sigma(\tau|Y) \subseteq \sigma(\tau)$ . Consider $X = \mathbb{R}^2$ , $\tau_e$ the Euclidean topology and $\tau_S$ the Sorgenfrey plane topology (generated by semi-open rectangles $[a, b) \times [c, d)$ ). Then $\tau_e \subsetneq \tau_S$ (open rectangles $(a,b) \times (c,d)$ can be written as a union of semi-open rectangles) but $\sigma(\tau_e) = \sigma(\tau_S)$ (since $[a, b) \times [c, d) \in \sigma(\tau_e)$ ). Consider the antidiagonal $Y := \{ (x, -x) \mid x \in \mathbb{R} \}$ . Then $Y$ is a $\tau_e$ -closed subset of $X$ , hence a $\tau_S$ -closed subset. For any $x \in \mathbb{R}$ it holds $\{ (x, -x) \} = ([x, x+1) \times [-x,-x+1)) \cap Y \in \tau_S|Y$ , i.e. every point in $Y$ is $\tau_S|Y$ -open in $Y$ . Therefore, $\tau_S|Y = \mathcal{P}(Y)$ is the discrete topology, hence $\sigma(\tau_S|Y) = \mathcal{P}(Y)$ . Now, since $Y$ is $\tau_S$ -closed in $X$ , we have $Y \in \sigma(\tau_S)$ and therefore $\sigma(\tau_S|Y) \subseteq \sigma(\tau_S) = \sigma(\tau_e)$ , hence $\mathcal{P}(Y) \subseteq \sigma(\tau_e)$ . But this is a contradiction (e.g. by comparing the cardinalities: $|Y| = \frak{c}$ , hence $|\mathcal{P}(Y)| = 2^{\frak{c}}$ while $|\sigma(\tau_e)| = \frak{c}$ because $\sigma(\tau_e)$ is generated by countably many sets (the open rectangles with rational endpoints); see also here ). What am I missing?",['measure-theory']
3317051,Prove that $\lim_{y\to0}\lim_{x\to0}(x+y\sin\frac{1}{x}) $ doesn't exist,"Actually the full problem states: given a function $f(x, y) = x + y\sin\frac{1}{x}$ prove that $\lim_{x\to0}\lim_{y\to0}f(x, y)$ and $\lim_{(x, y) \to(0,0)}f(x, y) $ exist and that $\lim_{y\to0}\lim_{x\to0}f(x, y)$ doesn't exist. The first two weren't a problem really, first one is just direct calculation the other one is applying the three limits theorem. The third one confuses me. I remember there being a theorem that states something like the following: $\lim_{x \to x_0}f(x) = A$ if and only if for every sequence of real
  numbers $(x_n)$ where $\forall n \quad x_n \neq x_0$ such that $\lim_{n \to \infty}x_n = x_0$ , then $\lim_{n \to \infty}f(x_n) = A$ However, in the textbook they seem to be using some other property that they haven't mentioned so far. I'm quoting now: We will prove that $\lim_{x->0}f(x, y)$ cannot exist, therefore the
  entire limit cannot exist. Let's look at the sequences $a_n=(\frac{1}{n\pi})$ and $b_n =(\frac{1}{2n\pi + \frac{\pi}{2}})$ . $$\lim_{n \to
> \infty}f(a_n, y) = \lim_{n \to \infty}(\frac{1}{n\pi} +0\cdot y) = 0$$ $$\lim_{n \to \infty}f(b_n, y) = y$$ Therefore the limit cannot exist. So it kind of looks like some application of the previous theorem? But still then, it looks different because the theorem doesn't state anything about the existence of the limit as far as I understood. Is this some separate theorem that I've missed? Thanks.","['limits', 'multivariable-calculus']"
3317079,how to compute a differential?,"Let $$l:\text{Imm}(S^{1},\mathbb{R^2}) \to \mathbb{R}$$ be $$l(c)=\int_{S^1}{|c_{\theta}|d\theta}$$ and its diffrential is $$dl(c)(h)=\int_{S^1}{\frac{<h_{\theta},c_{\theta}>}{|c_{\theta}|}}d\theta$$ Can you please, explain how this differential is computed?","['differential', 'derivatives', 'differential-geometry']"
3317098,"Family of $r$-subsets, all pair-wise intersecting in $s$ elements","Let $X$ be some $n$ -element set and $\,\mathcal I\subseteq\mathcal P(X)$ a family of subsets with the following properties: all $I\in\mathcal I$ are of size $r$ any two $I,J\in\mathcal I$ intersect in exactly $s$ elements. In particular, I do not require that every element of $X$ appears in some subset of $\mathcal I$ . I am interested in the maximal size of $\mathcal I$ . I wonder what are the key words for such type of questions? I know of Erdos-Ko-Rado typed problems , block designs , etc., but none of these fits well. This question is similar, but askes for intersections $\ge s$ instead of $=s$ . More concrete, I want to know how large $\mathcal I$ can be if $4s=2r=n$ . If $\#(n)$ denotes this maximal size, then $$\#(4)= 3, \qquad\#(8)\in\{7,8\}, \qquad\#(2^k)\ge 2^k-1. $$ What in general?","['combinatorial-designs', 'extremal-combinatorics', 'reference-request', 'combinatorics', 'terminology']"
3317109,Calculating Range of a Function,"I am doing problems in the chapter Functions. I have enclosed images of the question and my solution for the problem. Question: (Kindly ignore the question code - [16JM120379]) My Solution: Clearly, there is no such option in the given question. So, did I go wrong anywhere? If yes, please tell where I went wrong. Or, is the question having a missing option? Kindly guide me how to approach this problem.","['trigonometry', 'functions']"
3317152,Tom Apostol mathematical analysis exercise 5.28 Prove the following theorem do not use L‘Hospital rule,"5.28
  Let $f$ and $g$ be two functions having finite nth derivatives in (a,b).for some interior point c in (a,b),assumes that $f(c)=f’(c)=\dots=f^{n-1}(c)=0$ ,and that $g(c)=g’(c)=\dots=g^{n-1}(c)=0$ ,but $g^{n}(x)$ is never zero in (a,b),show that $$\lim_{x\to c}\frac{f(x)}{g(x)}=\frac{f^{n}(c)}{g^{n}(c)}$$ Note. $f^{(n)}$ and $g^{(n)}$ are  not assumed to be continous at c. $Hint.$ Let $$F(x)=f(x)-\frac{(x-c)^{n-1}f^{(n-1)}(c)}{(n-1)!},$$ I research for n=1. $F(x)=f(x)-f(c),G(x)=g(x)-g(c)$ ,use Cauchy mean value theorem. I got $f(x)g’(x_1)=f’(x_1)g(x)$ where $x_1$ is interior to the interval joining $x$ and $c$ $$|\frac{f(x)}{g(x)}-\frac{f’(c)}{g’(c)}|=|\frac{f’(x_1)}{g’(x_1)}-\frac{f’(c)}{g’(c)}|$$ But $f’(x),g’(x)$ are not assumed to be continuous at $c$ . I don’t know how to continue.Could you help me ?pleasa do not use L Hospital because this exercise in which chapter isn’t teach L Hospital. Thanks in advance!!! PS,The problem in the book where emphasis that $f^{n}(x),g^{n}(x)$ is not assumed to be continuous at point c","['calculus', 'derivatives', 'means']"
3317227,Orbits of vectors under the action of $\mathrm{GL}_n(\mathbb Q)$,"Context. While working on a larger proof, I would love to have the following lemma, but I can't even decide if it's true or not. The question. We consider the action of $\mathrm {GL}_n(\mathbb Q)$ on $\mathbb R^n$ such that $\varphi\cdot x=\varphi(x)$ for $\varphi\in \mathrm {GL}_n(\mathbb Q)$ and $x\in\mathbb R^n$ . Let $A$ be a subspace of $\mathbb R^n$ of dimension $2$ . Does there exist $u,v\in A$ linearly independent such that $v$ is in the orbit of $u$ under the action of $\mathrm{GL}_n(\mathbb Q)$ ? Remarks. We can reformulate the question this way: does there exist a rational transformation $\varphi\in\mathrm{GL}_n(\mathbb Q)$ such that $\varphi(u)=v$ ? I managed to prove this result for $n=3$ by constructing a rational rotation which sends $u$ to $v$ . With a reasoning of cardinality, we can prove that this result is false if we fix $u\in A$ , and we try to find $\varphi\in\mathrm{GL}_n(\mathbb Q)$ and $v\in A$ such that $v=\varphi(u)$ . If $A$ contains a rational vector $x$ , the symmetry with respect to $x$ will do the trick, so we can assume that $A$ does not contain any rational vector. Moreover, if $A$ intersect non-trivially a rational plane $B$ , then we can consider the rotation of axis $B^\perp$ (of dimension $n-2$ ) and of angle $\pi/2$ , and a little work shows this result. So we can assume now that for all rational planes $B$ , $A\cap B=\{0\}$ . I have no idea if this result is even true when $n\geqslant 4$ . Any hints, references or proofs would be much appreciated.","['geometry', 'linear-algebra', 'linear-transformations', 'group-actions', 'rational-numbers']"
3317228,How many divisors of $\phi(m)$ do not divide $m-1$?,"Lehmer's totient problem asks if there exists a composite number $m$ such that $\phi(m)$ divides $m-1$ . Lower bounds on $m$ has been established but we do not know if a solution exists. Clearly, if we find a divisor of $\phi(m)$ which does not divide $m-1$ then $m$ cannot be a solution of the problem. I started counting the number of divisors of $\phi(m)$ which do not divide. Soon I noticed pattern: Composites of the form $m = 8k+3$ has at least $4$ divisors of $\phi(m)$ which do not divide $m-1$ Composites of the form $m = 6k+5$ has at least $5$ divisors of $\phi(m)$ which do not divide $m-1$ Composites of the form $m = 12k+7$ has at least $6$ divisors of $\phi(m)$ which do not divide $m-1$ Composites of the form $m = 20k+3$ has at least $7$ divisors of $\phi(m)$ which do not divide $m-1$ Composites of the form $m = 24k+23$ has at least $8$ divisors of $\phi(m)$ which do not divide $m-1$ $$
\cdots
$$ Composites of the form $m = 48k+11$ has at least $14$ divisors of $\phi(m)$ which do not divide $m-1$ In fact each of the above arithmetic progressions is the smallest (lowest coefficient of $k$ ) for a given $n$ and there could be more than one arithmetic progression with the above property. Claim : For $n \ge 1$ there is an arithmetic
  progression $m_k = ak + b $ such that if $m_k$ composite then there are 
  at least $n$ divisors of $\phi(m_k)$ which do not divide $m_k-1$ . Question : Can we find a counter example i.e. is there an integer $n$ for which at we cannot find such an arithmetic progression?","['number-theory', 'divisibility', 'elementary-number-theory', 'prime-numbers']"
3317271,"Can I say ""if a sequence is not bounded above, then it is divergent to positive infinity"" without explicitly saying it's eventually increasing?","I recently learned the following theorem about bounded sequences: If a sequence is eventually increasing and not bounded above, then it is divergent to positive infinity. If a sequence is eventually decreasing and not bounded below, then it is divergent to negative infinity. Do I have to say ""eventually increasing"" / ""eventually decreasing"" when stating this theorem? What if I just say: If a sequence is not bounded above, then it is divergent to positive infinity. If a sequence is not bounded below, then it is divergent to negative infinity. Am I correct in assuming that the sequence being eventually increasing/decreasing is implied by the sequence not being bounded? Are there any drawbacks to the more concise statement that I may not be aware of?","['calculus', 'definition', 'sequences-and-series']"
3317278,Finding the normal to a curve of an implicit function using differentiation,"The question asks to find the tangent and normal to the curve of the following equation at the point ( $\sqrt3, 2$ ): $$x^2-\sqrt{3}xy+2y^2=5 $$ I began by finding the first derivative of this, which I found to be: $$ \frac{dy}{dx}=\frac{\sqrt3y-2x}{-\sqrt3x+4y}$$ The slope was obviously zero at the given point. Hence, $$y=mx+c$$ $$ y=c$$ $$ \therefore c = 2$$ $$ \therefore y = 2$$ However, when I cannot seem to find the correct answer for the equation fo the normal. The answer in my book states the equation for the normal is $x=\sqrt3$ , my attempt at finding the slope of the normal to find its equation was: $$m_1m_2=-1$$ $$m_1 = 0$$ $$ \therefore m_2 = undefined$$ This clearly does not correspond with the book's answer and I do not understand why my method to to find the slope of the normal does not work. Perhaps the formula I used is not feasible for implicit equations? Could someone enlighten me of my mistake and show me how to obtain the correct answer?","['implicit-function', 'tangent-line', 'multivariable-calculus', 'implicit-differentiation', 'slope']"
3317301,"Continuity of $x\sin\frac{1}{y}$ at $(x, 0)$","I need to check if function is continue in $(x,0)$ $$f(x,y) =
\left\{
	\begin{array}{ll}
		x\sin\frac{1}{y}  & \mbox{if } y \ne 0 \\
		0 & \mbox{if } y = 0
	\end{array}
\right.$$ Can someone help me understand if I approached this correctly? First, I check the following limit $$\lim_{(x,y) \to (x,0)} x\sin\frac{1}{y}$$ Which does not exist. I can write it as $$\lim_{(x,y) \to (x,0)} x \lim_{(x,y) \to (x,0)} \sin\frac{1}{y}$$ The first limit tends to $x$ itself, the second one diverges. For $x=0$ the limit $$\lim_{(x,y) \to (0,0)} x\sin\frac{1}{y} = 0$$ Indeed, by the squeeze theorem I can write $$-1\leq\sin\frac{1}{y} \leq 1 $$ $$-x \leq x\sin\frac{1}{y} \leq x$$ Thus reducing the limit to $$ \lim_{(x,y) \to (0,0)} x = 0$$ Thus $f(x,y)$ is continuous $\{ \forall (x, y) \in \Re^2 \mid x \ne 0 \}$ I'm a beginner, thank you in advance!","['multivariable-calculus', 'calculus', 'continuity']"
3317310,Permutations on $[2^k]$ And the Existance of Permutation Polynomials,"Fix $k \geq 2$ and let $[n]$ denote the set $\{0, 1, \ldots, n-1\}$ . A polynomial $p(x) = \sum_{i=0}^d a_i x^i$ with integer coefficients in $[2^k]$ is a permutation polynomial modulo $2^k$ if $p(x) \mod 2^k$ permutes the elements of $[2^k]$ . It is known (due to Rivest) that $p(x)$ is a permutation polynomial modulo $2^k$ if and only if $a_1$ is odd and $\Delta_1$ and $\Delta_2$ are even, where $\Delta_1 = a_2 + a_4 + \ldots$ and $\Delta_2 = a_3 + a_5 + \ldots$ . I am interested in the opposite question. In particular, given a permutation $\pi: [2^k] \to [2^k]$ , under what conditions does there exist a permutation polynomial $p$ modulo $2^k$ such that $p$ and $\pi$ produce the same permutation? I suspect that there are permutations that have no such polynomial, but I haven't been able to find or construct a class of examples.","['permutations', 'finite-groups', 'polynomials', 'symmetric-groups', 'group-theory']"
3317352,Chracterization proper maps in terms of nets,"Let $X,Y$ denote arbitrary topological spaces. A map $f:X\to Y$ is called proper if for all compact $K\subset Y,$ the pre-image $f^{-1}(K)\subset X$ is also compact. A net $\{x_\alpha\}_{\alpha\in A}\subset X$ goes to infinity if for all compact $K\subset X,$ there exists $\alpha_0\in A$ such that $x_\alpha\not\in K$ for all $\alpha\ge\alpha_0.$ I propose the following equivalent notion of properness: A map $f$ is proper if any only if for every net $\{x_\alpha\}_{\alpha\in A}\subset X$ that goes to infinity, the net $\{f(x_\alpha)\}_{\alpha\in A}\subset Y$ also goes to infinity. Wikipedia asserts that this equivalency holds if we assume that $X,Y$ are metric spaces and we replace nets with sequences. I can prove one direction of the general characterization, namely the ""only if"" direction: Proof by contrapositive. Let $\{x_\alpha\}_{\alpha\in A}\subset X$ denote a net that goes to infinity such that $\{f(x_\alpha)\}_{\alpha\in A}$ does not go to infinity. Let $K\subset Y$ denote a compact such that for all $\alpha_0\in A,$ there exists $\alpha\ge\alpha_0$ with $x_\alpha\in f^{-1}(K).$ Since $\{x_\alpha\}$ must eventually leave every compact, then $f^{-1}(K)$ cannot be compact. As for the converse direction, I do not know. Here is one thing that might help: In any topological space $X$ with at least one compact subset that is not itself compact, I can construct a non-empty net that goes to infinity. Indeed, let the directed set $A$ be the collection of all compact subsets, ordered by inclusion. For all $K\in A,$ choose $x_K\in X$ so that $x_K\not\in K.$ Clearly $\{x_K\}_{K\in A}$ goes to infinity. I believe that a variation of this construction will do the trick for the converse direction.","['general-topology', 'nets', 'compactness']"
3317365,"Showing $\left<a,b,c : a^2=b^3=c^5=abc \right>/\left<abc\right>$ is finite","I have had hard time to show that $$G=\left<a,b,c : a^2=b^3=c^5=abc  \right>/\left< abc \right>$$ is a finite group. I guess that $G=\mathbb{Z}_2*\mathbb{Z}_3*\mathbb{Z}_5$ . Even in this situation, it is not clear if it is finite or not. Could anybody help me for this problem?  Thanks in advance! Edit  : Apparently, this question has been answered in Proving Finiteness of Group from Presentation . However, the answer given in the link used the property of triangle group which is not given in Dummit and Foote algebra. Since the prerequisite of the given problem is the Dummit and Foote's Algebra book, I think there should be another way to solve it. So.. does anybody know how to solve this in different way with the answer of given link?","['combinatorial-group-theory', 'group-presentation', 'group-theory', 'finite-groups']"
3317386,How do I solve this exponential-surdic equation?,"I have been trying to solve a problem today, but have had very little to no progress. The problem is:
Find the real values of $x$ satisfying $$(26 + 15\sqrt{3})^x + 6(2 + \sqrt{3})^x + (2 - \sqrt{3})^x - 5(7 + 4\sqrt{3})^x = 5$$ Clear explanations would surely be appreciated.","['exponentiation', 'algebra-precalculus']"
3317414,"How to map $\alpha x$ and $(1 - \alpha)x$ for $\alpha \in (0,1)$ to the same point","Can you please help me with a problem from real function analysis? Let $p$ be a continuous, increasing function such that $p(s) \in (0,1)$ for $s > 0$ . Let $f$ and $g$ be two functions defined on $\mathbb{R}_+$ such that $f(s) = s p(s)$ and $g(s) = s - f(s) = s(1-p(s))$ . I am trying to find two nondecreasing functions $F$ and $G$ such that $F(f(s)) = G(g(s))$ for $s > 0$ . We can easily see that $0 < f(s) < s$ , $0 < g(s) < s$ , and for example that $|f(s) - \frac{s}{2}| = |g(s) - \frac{s}{2}|$ . One solution is $F = f^{-1}$ and $G = g^{-1}$ assuming that $p$ is such that $f$ and $g$ are both nondecreasing, their inverses exist (what else?). Such $F$ and $G$ work fine for, for example, $p(s)=s/(1+s)$ , but not for $p(s) = s^2/(1+s^2)$ , since $g$ is not increasing for all $s > 0$ in the latter case. However, I cannot find any $F$ and $G$ that would work for a general increasing function $p$ . (One of them, either $F$ or $G$ , can be Identity, if it helps). Any ideas? Many thanks in advance! EDIT: As pointed out by Adam Latosiński, the answer is negative for general $p$ . Is it possible to find $F$ and $G$ , other than inverses of $f$ and $g$ , in the case when $p$ is increasing and such that $f$ and $g$ are increasing as well?","['functions', 'projection', 'monotone-functions', 'real-analysis']"
3317443,Decompose projection matrix into a matrix and its pseudoinverse,"While researching regression, I encountered the situation where I am trying to reconstruct a data matrix $X \in \mathbb{R}^{n\times p}$ from the $n \times n$ hat matrix $H = X(X'X)^{-1}X' = XX^{+}$ (where $A^{+}$ is the Moore-Penrose inverse of $A$ ). Note that I only have access to the hat matrix, not the original data $X$ . So I have only $H$ , but I do know the dimensions ( $n$ and $p$ ) of the $X$ matrix, and I know that it is full column rank (i.e., $n > p$ and $\text{rk}(X) = p$ ). I know about $H$ that it is symmetric, idempotent, and positive semidefinite (it is a projection matrix). Is there a way to decompose the matrix $H \to XX^{+}$ in this context? Any approximate solutions where I could define the approximation error would also be very much appreciated. Some steps I already went through: $$H = XX^+$$ $$HX = X$$ $$(I_n - H)X = 0$$ and from there I realised that $X$ has something to do with the nullspace of $(I_n - H)$ .","['matrices', 'projection-matrices', 'matrix-decomposition']"
3317447,"Conjecture about $(0,1)$-matrices","Let $A$ be an $m$ by $n$ $(0,1)$ -matrix. For $1\leq i \leq m$ and $1\leq j \leq n$ , let $f(A,i,j)$ be the number of entries in $A$ not in row $i$ , not in column $j$ , and not equal to $a_{ij}$ . I would like a proof or counterexample to the following conjecture: If $A$ is not all 1's or all 0's, then there exist $i$ and $j$ such that $f(A,i,j)\geq \frac{(m-1)(n-1)-1}{2}$ . Example 1: For $A=\begin{bmatrix} 1 & 0 & 1 & 0\\0 & 1 & 0 & 1 \\1 & 0 & 1 & 0\\0 & 1 & 0 & 1 \\\end{bmatrix}$ , we have $f(A,1,1)=4\geq\frac{3\cdot3-1}{2}$ . Example 2: For $A=\begin{bmatrix} 0 & 1 & 0 & 0 & 1\\1 & 0 & 0 & 1 & 0 \\1 & 0 & 1 & 0 & 1\\0 & 0 & 0 & 0 & 1\\\end{bmatrix}$ , we have $f(A,1,2)=6\geq\frac{3\cdot 4-1}{2}$ .","['matrix-analysis', 'combinatorics']"
3317455,Asymptotics of Mathieu functions,"Given the Mathieu equation, $\frac{d^2w}{dz^2}+(a-2q\cos 2z)y=0$ , I have the solution \begin{align}
y(t)=A_1C(a,q,z)+A_2S(a,q,z)\ ,
\end{align} where $C(\cdot,\cdot,\cdot)$ and $S(\cdot,\cdot,\cdot)$ are the Mathieu cosine and sine solutions, respectively. Does an expression exist describing the asymptotics of the solution as $z\to\pm\infty$ ? I've tried looking at the Digital Library of Mathematical Functions , but they only have the asymptotic expansions for small and large $q$ . I know that one can let $z\to\pm iz'$ to find the modified Mathieu functions. An asymptotic series exists for this case, but it is only for $\Re z'\to\infty$ , so I don't think it is valid to take the asymptotic form of the modified Mathieu equation and let $z'\to\pm z$ as this would correspond to $\Re z'\to\pm i\infty$ . I've tried looking at academic papers, but they mainly deal with asymptotics of $q$ , rather than $z$ . This makes me wonder, is the asymptotic form of the Mathieu cosine and sine functions known for $z\to\pm\infty$ ?","['asymptotics', 'special-functions', 'ordinary-differential-equations']"
3317459,Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function. Part II: Electric Boogaloo,"This is a follow up to my previous question: Expectation of nonnegative random variable when passed through nonnegative increasing differentiable function I am now wanting to establish a follow up to the above problem. Specifically, if $X$ is a nonnegative random variable and $g:\mathbb{R}\rightarrow\mathbb{R}$ is a nonnegative, strictly increasing, differentiable function, then $$\mathbb{E}g(X)<\infty \iff \sum_{n=1}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)<\infty$$ I believe I can show the inequality when $g(x)=x^{p}$ for $p\in\mathbb{N}$ , but the case of a general $g$ is more mysterious to me. My attempt for the converse proceeds in the following way: If you assume that the series converges then (by the linked question) \begin{equation}
\mathbb{E}g(X) =  g(0)+\int_{0}^{\infty}g^{\prime}(X)\mathbb{P}(X>x)dx \\ 
=  g(0)+\sum_{n=0}^{\infty}\int_{n}^{n+1}g^{\prime}(x)\mathbb{P}(X>x)dx \\
\leq  g(0)+\sum_{n=0}^{\infty}(g^{\prime}(n+1)+g^{\prime}(n))\mathbb{P}(X>n) \\
=  g(0)+\left(\sum_{n=0}^{\infty}g^{\prime}(n+1)\mathbb{P}(X>n)\right)+\left(\sum_{n=0}^{\infty}g^{\prime}(n)\mathbb{P}(X>n)\right).
\end{equation} However I am unsure how to proceed from here. I don't see how the middle series would converge without more assumptions on $g$ . Any help with the equivalence in general would be appreciated.","['expected-value', 'probability', 'random-variables']"
3317466,"Proof that $\lim_{(x,y)\to (0,1)} e^{xy} = 1$","Here is my proof: Let $\epsilon > 0$ be given. There exists $\delta >0$ such that $$|e^{xy}-1| < \epsilon$$ whenever $0<|x|<\delta$ and $0<|y-1|<\delta$ . Using exponential series, we write $$|e^{xy}-1|=|(1+xy+\frac{x^2y^2}{2!}+...)-1|$$ $$=|xy+\frac{x^2y^2}{2!}+\frac{x^3y^3}{3!}+...|$$ $$\le|xy|+|\frac{x^2y^2}{2!}|+|\frac{x^3y^3}{3!}|+...$$ $$\le|xy|+|x^2y^2|+|x^3y^3|+...$$ $$\lt\delta|y|+\delta^2|y|^2+\delta^3|y|^3+...$$ Let $\delta\le\frac12\implies\frac12\lt y\lt\frac32$ $$\therefore|e^{xy}-1|\lt\frac32\delta+(\frac32\delta)^2+(\frac32\delta)^3+...$$ $$= \frac{3\delta}{2-3\delta}$$ So, we choose $$\delta = min\{\frac12,\frac{2\epsilon}{3(\epsilon+1)}\}$$ $$\implies|e^{xy}-1|\lt\epsilon$$ whenever $$0<|x|<\delta $$ and $$0<|y-1|<\delta$$ Q.E.D. I would also like to know if there are any other ways to prove this.","['epsilon-delta', 'multivariable-calculus', 'proof-verification', 'real-analysis']"
3317501,Show that a homomorphism from $SL_2(\mathbb{Z})$ to $ SL_2(\mathbb{F}_p)$ is surjective,"The homomorphism sends the entries of matrices in $SL_2(\mathbb{Z})$ to their congruence classes mod $p$ . After a lot of work I could prove that a matrix $$\begin{pmatrix}a & b\\\ c & d\end{pmatrix}$$ in $ SL_2(\mathbb{F}_p)$ has a preimage in $SL_2(\mathbb{Z})$ if $a$ and $c$ are relatively prime and I can compute it. But I can't prove the general case and the method I used to prove the preceding assertion was so complicated that it can't be the right way to approach the problem. So there has to be another, more elegant, approach. I attacked the problem head on and ended up with a system of diophantine equations which was difficult to solve as one of the equations was quadratic.","['matrices', 'number-theory', 'abstract-algebra', 'field-theory']"
3317540,Intuition Behind Generalized Stokes Theorem,"Consider the Generalized Stokes Theorem: \begin{equation}
\int_Md\omega = \int_{\partial{M}} \omega
\end{equation} Here, $\omega$ is a k-form defined on $R^n$ , and $d\omega$ (a k+1 form defined on $R^n$ ) is the exterior derivative of $\omega$ . Let M be a smooth k+1-manifold in $R^n$ and $\partial{M}$ (the boundary of M) be a smooth k manifold. I know that the above theorem is simply a generalization of well-known vector calculus theorems. However, I am looking for the intuition behind the Generalized Stokes Theorem itself. I started off by defining the exterior derivative at a point p in $R^n$ as: \begin{equation}
d\omega_p =\lim_{|vol|\to 0}\frac{\int_{\partial{vol}} \omega}{|vol|}
\end{equation} In this case, "" $vol$ "" represents a k+1 ""parallelpiped"" in $R^n$ that contains point p (with $|vol|$ being its ""volume""). $\partial{vol}$ represents the boundary of this k+1 ""parallelpiped"", a k ""parallelpiped"" itself. With this definition (assuming it is correct), can we say that $\omega$ represents an infinitesimal ""flux"" element through $\partial{vol}$ which would imply that $d\omega_p$ is simply the ""flux density"" at a point p? If the above is true, can we take the idea that (when applying the Generalized Stokes Theorem) the interior ""fluxes"" through each $\partial{vol}$ within M cancel out leaving us with the total ""flux"" out of $\partial{M}$ as the intuition behind the Generalized Stokes Theorem? Any help is much appreciated.","['differential-forms', 'multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3317549,Submanifold of $\mathbb{R}^3$ with tangent space spanned by given vector fields,"Question: This is part of a problem from an old qualifying exam that I am having a little trouble with, so any help is greatly appreciated! In the problem, we are given two vector fields on $\mathbb{R}^3$ expressed by $$X = x(2y + \cos(y))\frac{\partial}{\partial x} - \frac{\partial}{\partial z},\qquad Y = \frac{\partial}{\partial y} + \frac{\partial}{\partial z} $$ and we need to write down a submanifold $N$ of $\mathbb{R}^3$ containing the point $p = (0,1,0)$ such that there is an open neightborhood $U_p$ around $p$ such that for every point $q\in U_p$ we have $T_qN = \text{span}\{X\vert_q, Y\vert_q\}$ , but I can't figure out a good way of going about this. My Attempt: My first thought was to check and see if $X$ and $Y$ commute: they are obviously linearly independent at $p = (0,1,0)$ , so if they commute, there is some coordinate representation of $N = (s^1,s^2,s^3)$ such that for a chart $(V_p, (s^i))$ of $N$ centered at $p$ , we have that $X = \frac{\partial}{\partial s^1}$ and $Y = \frac{\partial}{\partial s^2}$ [see Theorem 9.46 of Lee's Intro to Smooth Manifolds]. However, when I compute the Lie bracket $[X, Y]$ , I get $$\begin{align*}
[X,Y] &= X(1)\frac{\partial}{\partial y} + X(1)\frac{\partial}{\partial z} - Y(x(2y + \cos(y)))\frac{\partial}{\partial x} + Y(1)\frac{\partial}{\partial z}\\
&=0 + 0 - (2x-x\sin(y))\frac{\partial}{\partial x}+0\\
&=(x\sin(y) - 2x)\frac{\partial}{\partial x}\\
&\not= 0,
\end{align*}$$ so the vector fields don't commute. At this point I got stuck. I did compute that the flow $\theta_t$ of $X$ is given by $$\theta_t(x,y,z) = (xe^{2yt + t\cos(y)}, y, z - t),$$ and that the flow $\psi_t$ of $Y$ is given by $$\psi_t(x,y,z) = (x, y + t, z + t),$$ but I don't really know how to use this information to describe $N$ .","['vector-fields', 'submanifold', 'differential-geometry']"
3317562,Induced metric on $S^2$ from pullback of metric on $\mathbb{R}^3$,"I'm going over some GR from more of a differential geometry perspective and had a quick question about a simple calculation - my differential geometry background isn't too strong so I apologise if any of the terminology is incorrect, but I'd be grateful for any clarification. I'm following an example in Appendix A of Sean Carroll's Introduction to GR, where there is a map $\phi:M \to N$ , given by $$ \phi(\theta,\phi) = (\sin\theta \cos \phi , \sin \theta \sin \phi, \cos \phi) ,
$$ and $M=S^2$ is a submanifold of $N =\mathbb{R}^3$ (i.e. the two-sphere embedded in $\mathbb{R}^3$ ). The coordinates on the manifolds are $x^{\mu} = (\theta,\phi)$ on $M$ , and $y^{\alpha} = (x,y,z)$ on $N$ . The induced metric on $M$ is just the pullback of the flat-space metric $\phi^*g$ , which is given by the formula $$ 
(\phi^*g)_{\mu \nu} = \frac{\partial y^{\alpha} }{\partial x^{\mu}} \frac{\partial y^{\beta} }{\partial x^{\nu}} g_{\alpha \beta}.
$$ I understand how to calculate the individual Jacobian matrices of partial derivatives $\frac{\partial y^{\alpha} }{\partial x^{\mu}}$ (e.g. just using $y^1 = \sin\theta \cos \phi $ , $y^2 = \sin \theta \sin \phi$ and $y^3 = \cos \phi$ as defined by $\phi$ ), however I was confused as to how to treat the full expression above. The Jacobian matrices are $2 \times 3$ matrices, so the second has to be transposed to be a $3 \times 2$ matrix in order to give the required $2 \times 2$ metric $g_{\mu \nu}$ . My question is, in the pullback equation above, how does the metric $g_{\alpha \beta}$ act on the Jacobian $\frac{\partial y^{\beta} }{\partial x^{\mu}}$ , and how should I be writing this down? I can see that $y^{\beta}$ should be replaced with $y^{\alpha}$ , but should any indices be lowered, and how should I interpret the metric tensor transposing the Jacobian matrix? Edit - Ted Shriffin's comments are correct, the last component of the map should be $\cos \theta$ not $\cos \phi$ , and all my matrices should be transposed.",['differential-geometry']
3317583,Trying to understand a proof of Cantor's theorem: why was $B$ defined this way?,"This comes from the textbook: Edward A. Scheinerman - Mathematics: A Discrete Introduction-Cengage Learning (2012) I understand everything in the proof except for why Dr. Scheinerman defined the set $B$ as he did. Informally, he says that B is a set that $f$ ""misses"", i.e. there is no $a \in A $ such that $f(a) = B$ . How does the formal definition of $B$ he gives capture this idea? The definition of $B$ is $B = \{x \in A: x \notin f(x) \}$ , which I read as: "" $B$ is the set of all elements $x$ in $A$ that are not in the subsets they map to under $f$ "". Is that an accurate reading? If so, how does that definition encapsulate all the sets that $f$ ""misses""?",['discrete-mathematics']
3317618,Plain integer partitions of $n$ using $r$ parts,"Division of number $n$ on parts $a_1,...,a_r$ where $a_1 \le ... \le a_r$ we call a plain if $a_1 = 1$ and $a_i - a_{i-1} \le 1$ for $2 \le i \le r$ . Find enumerator (generating function) for plain divisions. my try The hint was to use bijection between plain divisions and some commonly know enumerator. I tried to use enumerator of divisions on different parts: $$ (1+x)(1+x^2)...(1+x^r)$$ where number of plain divisions is $$[x^n](1+x)(1+x^2)...(1+x^r) $$ let function $$f(n,r) = [x^n](1+x)(1+x^2)...(1+x^r) $$ For some first divisions it works. For example: $$f(4,3) = 1 $$ $$f(6,3) = 1 $$ $$f(11,5) = 2$$ But when I tried to find bijection, I failed. I found that this function isn't correct because $f(15,6) = 4$ but should be equal to $3$ because: $$15 = 1,1, 2, 3, 4, 4 \\
15 = 1, 2, 2, 3, 3, 4\\
15 = 1, 2, 3, 3, 3, 3 $$ . There I stucked.","['integer-partitions', 'combinatorics', 'discrete-mathematics']"
3317623,Probability of getting a sufficiently long piece from multiple stick breaking,"This question asks: Start with a stick of length $1$ . Repeatedly remove some fraction $U$ of the remaining stick, where $U$ is uniform in $[0,1]$ . What is the probability that at least one of the removed pieces has length at least $\frac12$ ? Eventually I managed to solve it, but the result was not very enlightening. Thus I present here a generalisation of the question where $\frac12$ is replaced with an arbitrary $0\le x\le1$ . Let $f(x)$ be the probability that we eventually break off a stick of length at least $x$ ; it has been shown that for $\frac12\le x\le1$ , $f(x)=-\ln x$ . What, then, is $f(x)$ for $0\le x\le\frac12$ ? This generalisation is more difficult because now we can cut off more than one stick longer than $x$ . Let's use joriki's approach to the source question: when $x\le\frac12$ either we cut off a piece longer than $x$ at the start, with probability $1-x$ , or we leave a stick of length $t$ and replace $x$ with $x/t$ : $$f(x)=1-x+\int_{1-x}^1f(x/t)\,dt$$ This is the same formula as in joriki's answer except that the lower bound $x$ has become $1-x$ . We can make the same manipulations that follow there to  get $$f(x)=1-x+x\int_x^{x/(1-x)}\frac{f(u)}{u^2}\,du$$ $$1-f(x)=x\left(1-\int_x^{x/(1-x)}\frac{f(u)}{u^2}\,du\right)$$ and eventually the functional differential equation $$x(1-x)^2f''(x)=f'\left(\frac x{1-x}\right)-f'(x)(1-x)^2$$ but how would I proceed from here? I've determined the following approximate values for $f(x)$ with one million trials for each $x$ . .49 .712676
.48 .732251
.47 .751341
.46 .769896
.45 .787595
.44 .805675
.43 .822355
.42 .838964
.41 .854620
.40 .869991
.39 .883803
.38 .897808
.37 .910511
.36 .923427
.35 .934614
.34 .945107
.33 .954406
.32 .963026
.31 .969796
.30 .976313
.29 .981963
.28 .986094
.27 .989759
.26 .992816
.25 .995101
.24 .996743
.23 .997929
.22 .998779
.21 .999327
.20 .999609
.19 .999815
.18 .999924
.17 .999958
.16 .999990
.15 .999995
<=.14 1.000000","['functional-equations', 'probability']"
3317636,On the definition of a smooth function on an arbitrary set,"Suppose that $A \subseteq \mathbb{R}^{n}$ is arbitrary, and $f :A \rightarrow \mathbb{R}$ . Then $f$ is defined as smooth as long as for each point $x \in A$ , there exists an open set $U$ of $\mathbb{R}^{n}$ containing $x$ and a smooth function $g:U\rightarrow\mathbb{R}$ which agrees with $f$ on $A$ . Now, I always assumed that the extending function did not matter, so that if $g$ and $g'$ are two smooth extensions as described above, then we are okay in just defining, for each component $i$ , $$ \frac{\partial f}{\partial x_{i}}(x)=\frac{\partial g}{\partial x_{i}}(x)=\frac{\partial g'}{\partial x_{i}}(x) $$ After further thought, this obviously is not the case. Consider the set $A=\{0\}$ , and define the function $f :A \rightarrow \mathbb{R}$ as $f(0)=0$ . Then by definition, both $g(x)=x^{2}$ and $g'(x)=x$ are smooth extensions of $f$ , which produce different derivatives at $x=0$ . What must we then have to ensure that the choice of extension does not matter? Do we need that the interior of $A$ be non-empty in order to exploit the continuity of derivatives? Thanks in advance!","['real-analysis', 'multivariable-calculus', 'calculus', 'functional-analysis', 'differential-topology']"
3317685,Every prime ideal in $\mathbb{Z}[x]$ is generated by at most two elements [duplicate],"This question already has an answer here : Classification of prime ideals of $\mathbb{Z}[X]$ (1 answer) Closed 2 years ago . I am trying to prove Every prime ideal in $\mathbb{Z}[x]$ can be generated by at most two elements. Since I have not seen any prime ideal generated by 3 elements, this statement seems true to me. But I cannot find a way to prove it. Thanks for any help in advance!","['ring-theory', 'abstract-algebra', 'maximal-and-prime-ideals', 'ideals']"
3317706,Proving the Continuity of $e^x$,"Can one say that $e^x$ is the sum of an infinite number of terms (Taylor expansion), every term being a continuous polynomial in itself, the sum of all the terms is continuous and so $e^x$ is continuous?
Thanks in advance","['limits', 'calculus', 'continuity']"
3317729,"What does the notation [n] in the phrase ""For any subset S of [n]""?","I'm trying to understand the sensitivity of boolean functions conjecture... But alas, every paper/blog... uses this notation without defining it. (And it is almost impossible to google for notation) Example of use","['discrete-mathematics', 'information-theory']"
3317735,Show that $f^{-1}(f(E))=E$,"Also yesterday I had a question on proving: If takes $f [-1,1]$ to $[-1,1]$ then $f^{-1}(f(0))= 0$ . Where $0$ is in brackets and is a set. one-one and onto proofs It was proved by showing that $f(f^{-1}(0)) = 0$ so is it logically equivalent to showing $f^{-1}(f(0))= 0$ . I think that in my book it does confirm that but it can be easy to misinterpret things, want verification Prove: $f^{-1}(f(E))=E$ for all subsets $E$ of $X$ Proof: $f(E) = y \in Y : y=f(x)$ for some $x \in E$ $f^{-1}(E) = x \in X : f(x) = y$ for some $y \in Y$ $f^{-1}(f(E))= f^{-1}(y \in Y : y=f(x)$ for some $x \in X$ = $x \in X :$ $f(x)=y$ for some $y \in Y$ = $E$ What I am trying to say is that the output values from $f(E)$ when plugged into $f^{-1}(E)$ should lead to the corresponding x-values of E which means it should equal E?","['functions', 'proof-verification', 'real-analysis']"
3317746,"Must a ring (commutative, with 1), in which every non-zero ideal is prime, be a field?","An early exercise in Irving Kaplansky's commutative rings asks: Let R be a ring. Suppose that every ideal in R (other than R) is prime. Prove that R is a field. This is easy if we assume the zero ideal is prime. But is this assumption necessary? If every non-zero ideal is prime, then for any non-unit $x \in R$ and with $x^{n+1} \ne 0$ we must have $\langle x \rangle \subseteq \langle x^{n+1} \rangle$ , which requires the existence of an element $y$ satisfying: $$
x(1-x^ny) = 0
$$ The collection of these and similar relations on the elements seems rather restrictive, but I would appreciate a simple and incisive argument to show that the condition that all non-zero ideals are prime can only be met by rings with trivial spectrum, or, if my guess is incorrect and this is untrue, a counter-example.","['maximal-and-prime-ideals', 'examples-counterexamples', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3317814,Probability of unique winner in a coin flipping game (limit of a recursive sequence),"Suppose we have a coin flipping game involving $n$ players.  In each round everyone still playing flips a fair coin, and the players whose coin comes up tails are eliminated.  The game continues until at most one player is still alive, and they are declared the winner. Now, it is possible that the game does not end with a winner (e.g. if $n=2$ and both players get tails on their first flip).  Let $f(n)$ denote the probability that a game with $n$ players has a winner.  We have $f(0)=0$ and $f(1)=1$ .  For $n>1$ it follows from considering the binomial distribution that $$f(n) = \sum_{k=0}^{n}\frac{\binom{n}{k}}{2^{n}}  f(k) $$ (Here $\binom{n}{k}/(2^n)$ represents the probability $k$ players survive the current round), which can be rearranged as $$f(n) = \sum_{k=0}^{n-1} \frac{\binom{n}{k}}{2^n-1} f(k)$$ Using this formula we can compute $f(n)$ recursively. $$\begin{array}{cc} n & f(n) \\ 0 & 0 \\ 1 & 1 \\ 2 & 2/3 \\ 3 & 5/7 \\ 4 & 76/105 \\ 5 & 157/217 \\
\vdots & \vdots \\ 
20 & 0.7213 \end{array}$$ The sequence of numerators doesn't seem to be in OEIS, nor does the sequence $a_n=f(n)(2^n-1)(2^{n-1}-1)\dots(3)(1)$ from clearing all the denominators in the recursion. Is there a way of analytically determine the limit (if it exists) of $f(n)$ as $n$ goes to infinity?  It seems from calculation to be about $0.7213$ , though I'm not confident in digits beyond that due to error propagation as the recursion continues.","['recurrence-relations', 'probability', 'real-analysis']"
3317836,Inscribed Equilateral Triangle Trig - Expression for P/A,"Problem: The vertices of an equilateral triangle, with perimeter P and area A , lie on a circle with radius r . Find an expression for $\frac{P}{A}$ in the form $\frac{r}{k}$ , where k ∈ Z+. Hi, I'm having a bit of trouble solving this problem. What I've tried is using the formula for the area of an equilateral triangle (a = $\frac{3\sqrt{3}}{4}$ $r^2$ ). Since one side is equal to $\frac{P}{3}$ , I inserted that into the formula a = $2rsin(60)$ to get $P=3(2rsin(60))$ . That meant that $\frac{P}{A}$ -> $\frac{3(2rsin(60))}{\frac{3\sqrt{3}}{4}r^2}$ = $\frac{2\sqrt{3}}{3\sqrt{3}r}$ = $\frac{2}{3r}$ . But I don't think this is the correct answer because when I put them back into the formulas I get different values for the radius! If anyone can help and explain what I did wrong it would help a lot. Thanks!","['trigonometry', 'geometry']"
3317846,Finding absolute maximum and minimum values on circular bounded region,"Find the absolute maximum and minimum values of $f(x,y)=4x^2y$ on the set $S=\{(x,y):x^2+y^2\le1\}$ . I am confused as to how to check the boundary of the circular region. I tried subtracting the formulas, i.e. $z = 4(x^2)y - 9 = x^2 + y^2$ and got the critical points of $(\frac{1}{2\sqrt 2} , \frac{1}{4})$ and $(-\frac{1}{2\sqrt 2} , \frac{1}{4})$ but this seems to be incorrect? Any help would be much appreciated, preferably building on the method I used (unless it is a completely wrong approach). Thank you so much!","['optimization', 'multivariable-calculus', 'maxima-minima', 'derivatives']"
3317886,Can MAGMA compute Auslander-Reiten sequences in group algebras?,"I'd like to ask the following MAGMA question: Given a non-projective $kG$ -module $M$ , where $G$ is a finite group and $k$ is a finite field whose characteristic divides $|G|$ , can MAGMA compute the left and right almost split sequences of $M$ ? $\tau (M) $ is easy, since $\tau (M)\cong {\Omega} ^2(M)$ , but how to compute the middle terms? Thanks for the help. EDIT (12th August 2019): Remark: In the GAP-package qpa it is done in the following way, but I'm not yet familiar enough with MAGMA to do the transfer (characters following the symbol $\#$ denote a comment): $\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$$\#$ $\#$$\#$ AlmostSplitSequence( $<M>$ ) $\#$$\#$ $\#$$\#$ This function finds the almost split sequence ending in the module $\#$$\#$$<M>$ , if the module is indecomposable and not projective. It returns $\#$$\#$ fail, if the module is projective. The almost split sequence is $\#$$\#$ returned as a pair of maps, the monomorphism and the epimorphism. $\#$$\#$ The function assumes that the module  is indecomposable, and $\#$$\#$ the range of the epimorphism is a module that is isomorphic to the $\#$$\#$ input, not necessarily identical. $\#$$\#$ InstallMethod( AlmostSplitSequence,""for a PathAlgebraMatModule"",
true, [ IsPathAlgebraMatModule ], 0, function( M )

local K, DTrM, f, g, PM, syzygy, G, H, Img1, zero,
genssyzygyDTrM, VsyzygyDTrM, Img, gensImg, VImg,
stop, test, ext, preimages, homvecs, dimsyz, dimDTrM,
EndDTrM, radEndDTrM, nonzeroext, temp, L, pos, i; $\#$ $\#$ ToDo: Add test of input with respect to being indecomposable. $\#$ K := LeftActingDomain(M);
if IsProjectiveModule(M) then
    return fail;
else
DTrM := DTr(M); $\#$ $\#$ creating a short exact sequence 0 -> Syz(M) -> P(M) -> M -> 0 $\#$ f: P(M) -> M, g: Syz(M) -> P(M) $\#$ f := ProjectiveCover(M);
g := KernelInclusion(f);
PM := Source(f);
syzygy := Source(g); $\#$ $\#$ using Hom(-,DTrM) on the s.e.s. above $\#$ G := HomOverAlgebra(PM,DTrM);
H := HomOverAlgebra(syzygy,DTrM); $\#$ $\#$ Making a vector space of Hom(Syz(M),DTrM) $\#$ by first rewriting the maps as vectors $\#$ genssyzygyDTrM := List(H, x -> Flat(x!.maps));
VsyzygyDTrM := VectorSpace(K, genssyzygyDTrM); $\#$ $\#$ finding a basis for im(g*) $\#$ first, find a generating set of im(g*) $\#$ Img1 := g*G; $\#$ $\#$ removing 0 maps by comparing to zero = Zeromap(syzygy,DTrM) $\#$ zero := ZeroMapping(syzygy,DTrM);
Img := Filtered(Img1, x -> x <> zero); $\#$ $\#$ Rewriting the maps as vectors $\#$ gensImg := List(Img, x -> Flat(x!.maps)); $\#$ $\#$ Making a vector space of $\#$ VImg := Subspace(VsyzygyDTrM, gensImg); $\#$ $\#$ Finding a non-zero element in Ext1(M,DTrM) $\#$ i := 1;
stop := false;
repeat
    test := Flat(H[i]!.maps) in VImg;
    if test then
        i := i + 1;
    else
        stop := true;
    fi;
until stop;

nonzeroext := H[i]; $\#$ $\#$ Finding the radical of End(DTrM) $\#$ EndDTrM := EndOverAlgebra(DTrM);
radEndDTrM := RadicalOfAlgebra(EndDTrM);
radEndDTrM := List(BasisVectors(Basis(radEndDTrM)), x ->
FromEndMToHomMM(DTrM,x)); $\#$ $\#$ Finding an element in the socle of Ext^1(M,DTrM) $\#$ temp := nonzeroext;
L := List(temp*radEndDTrM, x -> Flat(x!.maps) in VImg);
while not ForAll(L, x -> x = true) do
    pos := Position(L,false);
    temp := temp*radEndDTrM[pos];
    L := List(temp*radEndDTrM, x -> Flat(x!.maps) in VImg);
od; $\#$ $\#$ Constructing the almost split sequence in Ext^1(M,DTrM) $\#$ ext := PushOut(g,temp);
return [ext[1],CoKernelProjection(ext[1])];

fi;
end
); EDIT(9th April): I posted a similar question on MO: https://mathoverflow.net/questions/356800/can-magma-compute-almost-projective-kg-homomorphisms","['representation-theory', 'group-theory', 'magma-cas', 'finite-groups']"
3317916,In how ways is it possible to borrow up to five books from a friend who has five different books?,"My friend got 5 different books. He does not mind how many books I can select to borrow each time - even if I borrow none. How many different selections are possible? I am thinking that this is a ""combination"" problem since the order of the books does not matter. So I can make these selections:
5 books OR 4 books OR 3 books ...etc OR zero books.
So there would be 6 different selections possible - but this was the wrong answer. Where have I gone wrong?","['combinations', 'combinatorics']"
3317941,What is the difference between codomain and range?,"My book says that if there is a linear transformation $T: V \to V'$ , then $V'$ is the codomain of $T$ but it also says that $T[V]$ is the range of $T$ . $T[V]$ the same as $V'$ ?","['functions', 'linear-algebra', 'linear-transformations']"
3317966,"How do I sketch $y = -2\sin\big(2(x+\frac{π}{6})\big)+1$ between $[-π,π]$ without any fuss?","I've looked at a bunch of videos on youtube, asked multiple teachers and tutors yet everyone just kind of vaguely told me stuff to do and I didn't understand at all. The parts that mess me up is finding the $x$ -intercepts between the domain (especially in the negative section), fitting the graph onto an axis with a scale, and determining the end shapes. I'd like an in-depth explanation on how to sketch this step by step, that I can follow and be successful with other graphs as well.","['trigonometry', 'graphing-functions']"
3317987,Is there a name for this subgroup property?,"Suppose that I have a subgroup $H \le G$ such that for any $h \in H$ , $[h]_G \cap H = [h]_H$ . Is there a name for this property? Here, $[h]_G$ means the $G$ -conjugacy class of $h$ and $[h]_H$ is the $H$ -conjugacy class of $h$ .","['group-theory', 'definition', 'finite-groups', 'terminology']"
3318019,Finding range of given function.,"This was a question which came in my test. I didn't give the test. Some students were discussing how hard it was. I gave it a try at home. Question Find the no. of different values functions takes. $$f(x)=\bigg\lfloor \frac{x^2}{2005} \bigg\rfloor$$ Here $x \in [1,2005]$ and $x \in \mathbb N$ The way I tried this question: I assumed that (which will eventually turn out to be wrong) this function would produce some repeated values in starting i.e. for $x\in [1,44]$ the function will give $0$ , 44 times. And then the function would start giving values which won't repeat itself. We need to find the point upto which the function repeats the values. As lets say last repeated value it gave was(mere random example) $405$ at $x=900$ . So answer would be $(2005-900)+405$ . As the before $x=900$ function will give values from $[0,405]$ and after that the remaining numbers will give different values as they won't repeat any values they give. How is my assumption wrong: Actually the function gives some repeated values in the start and after wards it starts to give a values which are repeated and unrepeated both intermingled in each other. And after that some values skip, some repeat and some neither skip nor repeat i.e. come exactly once. Thank you for any help provided. Please help me solving this question. Post script: Also this question had 3 minutes time alloted to it. We had to find the number of proper divisors of the number of different values function takes. This hidden piece contains answer. And the answer was 2. Now if number of proper divisor is 2, the number is obviously a square of a prime number. Isn't that a big hint? I don't see how.","['number-theory', 'functions', 'elementary-number-theory', 'real-analysis']"
3318026,Write a homogeneous polynomial of degree $d$ as a sum of $d$-th power of linear polynomials,"I learned that the Warning rank of a homogeneous polynomial $h\in \mathbb{C}[x_1, \cdots, x_n]_d$ of degree $d$ is defined by the smallest
number of summands such that $h$ can be expressed as a sum of $d$ -th powers of linear polynomials. For example, $XY=(\frac{X}{2}+\frac{Y}{2})^2+(i\frac{X}{2}-i\frac{Y}{2})^2$ so the Warning rank of $XY$ is $2$ . But how do we know that any homogeneous polynomial of degree $d$ can be writte as a sum of $d$ -th powers of linear polynomials? I read a proof for the case of elementary symmetric polynomials, but I have no idea how to get the generalized result for arbitrary homogeneous polynomials. I know that the dimension of $\mathbb{C}[x_1, \cdots, x_n]_d$ is $\binom{n+d-1}{d}$ . I tried to find $\binom{n+d-1}{d}$ linearly independent polynomials each of which is a $d$ -th power of a linear polynomial, but there is no progress so far.","['symmetric-polynomials', 'linear-algebra', 'polynomials', 'combinatorics']"
3318043,Probability that a five-card poker hand contains cards of different kind?,"I got a question similar to this on searching. Here. The correct answer is: $\frac{\binom{13}{5} \cdot 4^5}{\binom{52}{5}}$ However, I tried this: $ \Large\frac{\binom{13}{1}\binom{12}{1}\binom{11}{1}\binom{10}{1}\binom{9}{1} \cdot 4^5}{\binom{52}{5}} $ I see that $\binom{13}{1}\binom{12}{1}\binom{11}{1}\binom{10}{1}\binom{9}{1}$ equals $\frac{13!}{8!}$ which means this multiplication generates a permutation and needs to be divided by $5!$ .
So, although I know this solution isn't right, I'm not entirely sure why it is wrong either. Why isn't the solution I tried working according to the product rule?","['discrete-mathematics', 'combinatorics', 'probability']"
3318054,Formal argument countability,"Show that if $X$ is non empty and countable then the following set is countable: $\{\{ x \} : x \in X \}$ . Now the way I did this is by defining $A$ map $f$ from the set to $X$ by $f(\{x\}) = x$ which is clearly injective and because $X$ is countable, the set is therefore countable. How can this be generalized? Is it true to say that if I have a countable set $X$ and a collection of subsets of $X$ , $U$ then $U$ is also countable?","['elementary-set-theory', 'proof-verification']"
3318103,Prove that $\lim_{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!} = e$,"Define $e, e'$ by $$e: =\lim _{n \rightarrow \infty} \left(1+\frac{1}{n}\right)^{n} \quad \text{and} \quad e' := \lim _{n \rightarrow \infty} \sum_{k=0}^{n} \frac{1}{k!}$$ Prove that $e' \in \mathbb R$ and $e' = e$ . Could you please verify whether my attempt is fine or contains logical gaps/errors? Any suggestion is greatly appreciated! PS: I've not learned about derivative and logarithmic function yet. My attempt: To make the presentation easier to follow, I set $$e_n :=  \left(1+\frac{1}{n}\right)^{n} \quad e'_n := \sum_{k=0}^{n} \frac{1}{k!}$$ Clearly, $e'_n + 1/((n+1)!) = e'_{n+1}$ and thus the sequence $(e'_n)$ is increasing. On the other hand, $e'_n = 2 + \sum_{k=2}^{n} \frac{1}{k!} \le 2 + \sum_{k=1}^{n} \frac{1}{k(k+1)} = 2+(1-\frac{1}{n+1}) = 3 - \frac{1}{n+1} < 3$ . So the sequence $(e'_n)$ is bounded from above and thus $e'$ is well defined. By binomial theorem, $e_n = \sum_{k=0}^n {n \choose k} \frac{1}{n^k}$ where ${n \choose k} \frac{1}{n^{k}}=\frac{1}{k !} \frac{n \cdot(n-1) \cdot \cdots \cdot(n-k+1)}{n \cdot n \cdot \cdots \cdot n} \leq \frac{1}{k !}$ . As such, $e_n \le \sum_{k=0}^n \frac{1}{k !} = e'_n$ and so $e \le e'$ . Next we prove that $e \ge e'$ . For $n \ge m$ , we have $$\begin{aligned} e_{n}=\left(1+\frac{1}{n}\right)^{n} &=\sum_{k=0}^{n} {n \choose k} \frac{1}{n^{k}} \geq \sum_{k=0}^{m} {n \choose k} \frac{1}{n^{k}} \\ &=1+\sum_{k=1}^{m} \frac{1}{k !} \frac{n (n-1) \cdots (n-k+1)}{n \cdots n} \\ &= 1+\sum_{k=1}^{m} \frac{1}{k !} \left[ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right] \end{aligned}$$ For $n \ge m$ , I set $x_{n,m} = 1+\sum_{k=1}^{m} \frac{1}{k !} \left [ 1 \cdot\left(1-\frac{1}{n}\right) \cdots\left(1-\frac{k-1}{n}\right) \right]$ . Then $e_n \ge x_{n,m}$ and thus $e = \lim _{n \rightarrow \infty} e_n \ge \lim _{n \rightarrow \infty} x_{n,m} = e'_m$ . As such, $e \ge e'$ . This completes the proof.","['constants', 'proof-verification', 'sequences-and-series', 'real-analysis']"
3318109,Result of Parallel transport along geodesic on $U(n)$,"I am reading this paper Conjugate Gradient Algorithm for
Optimization Under Unitary Matrix
Constraint . It basically describes a variant of the conjugate gradient method when optimizing for unitary matrices which takes advantage of the Lie group structure of $U(n)$ . The question however is basic differential geometry and maybe Lie theory and this is what I am struggling with. Specifically, the following is stated: Now, the parallel transport of a tangent vector $\tilde{X} = XW \in T_W U(n) , X \in  \mathfrak{u}(n)$ , w.r.t. the Riemannian connection along the
geodesic $$ \mathcal{G}_W (t) = \exp(tS) W,\quad S\in \mathfrak{u}(n), \quad t\in \mathbb{R} \tag{1}$$ is given by $$\tau \tilde{X}(t) = \exp(tS/2) \; X \; \exp(−tS/2) \;G_W(t) \tag{2} $$ where $\tau$ denotes the parallel transport. Here, $\mathfrak{u}(n) $ denotes the Lie algebra to $U(n)$ and the geodesic $\mathcal{G}_W(t)$ is such that it emanates from $W$ in the direction of the vector $(dR_W)_e(S)=SW \in T_WU(n)$ . I should further note that $U(n)$ was equipped with a bi-invariant metric! My questions: From the formula of the geodesic $\mathcal{G}_W(t)$ , it seems to be that one can transport every vector on the Lie group just by left/right translation to every point. This seems much more natural than parallel transport. In the case of a bi-invariant metric, does it differ from parallel transport (which I understand depends on the geometry crucially) Could someone walk me trough how one obtains the formula for the parallel transport, namely (2)? In particular, this actually looks a like a right translation of the vector $\exp(tS/2) \; X \; \exp(−tS/2)$ ,  too, i.e. $$ \tau \tilde{X}(t) = dR_{G_W(t)}\left(\exp(tS/2) \; X \; \exp(−tS/2)\right) \tag{3} $$ and the thing in the bracket is somehow the adjoint map but I still cannot puzzle these things together.","['optimization', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3318111,Does every group of order $n!$ have a subgroup of order $n$?,"Even with little/no knowledge of the symmetric group of degree $n$ , I think that Cayley theorem is sufficient to infer that such group must have a proper subgroup of order $n$ , for any $n>2$ . I was wondering what about groups of order $n!$ : do they always have a proper subgroup of order $n$ ? This is the case for $S_n$ . This is also the case for $C_6$ , which complete the survey for $n=3$ ,  but I can't even approach $n=4$ to look for counterexamples.","['group-theory', 'abstract-algebra', 'finite-groups']"
3318125,What is the expected number of coin tosses it would take to get N many heads OR N many tails?,"Where we do NOT require that the heads or tails be consecutive (though they may be!) Obviously, this expectation, $E[T]$ , is bound as follows: $N < E[T] < 2N - 1$ And obviously $E[T] = \sum_{i=N}^{n=2N-1}i*P[Game \ Ends \ On \ i^{th} \ Round]$ , where $\sum_{i=N}^{n=2N-1}P[Game \ Ends \ On \ i^{th} \ Round] = 1$ But how would one find such a probability for an arbitrary $i \in \{N, N+1, ..., 2N-1\}$ ?",['probability']
3318160,Example of a quasi-open set and how the capacity of the set fits in it?,"While going through some research papers, I came across a result of D. Bucur where the existence of a minimizer for the general $k^{th}$ eigenvalue of the Dirichlet Laplacian among a class of quasi-open sets with fixed measure was shown. I have looked into the definition of quasi-open sets from A. Henrot, Extremum problems for eigenvalues of elliptic operator . But I was unable to find any insight as to what kind of sets one might expect to be quasi-open. First, in Definition 2.4.3, they defined the capacity of a set as Let $D$ be a bounded open set in $\mathbb{R}^n$ . For any compact subset $K$ in $D$ , define $$\text{cap}_D(K)= \inf \left\{\int_D|\nabla v|^2: v\in C_0^{\infty}, v\geq 1 \text{ surrounding } K \right\}$$ The definition can be extended for an open subset $\omega$ of $D$ as $$\text{cap}_D(\omega):= \sup \{\text{cap}_D(K): K \text{ compact, } K\subset \omega\}$$ Quasi-open sets as in Definition 2.4.4 is defined as follows, A subset $\Omega$ of $D$ (a bounded open set in $\mathbb{R}^n$ ) is quasi-open if there exist a decreasing sequence of open sets $\omega_n$ such that $$\displaystyle \lim_{n\to +\infty} \text{cap}_D(\omega_n)=0$$ where $\Omega \cup \omega_n$ is open for all $n$ and $\text{cap}_D(\omega_n)$ is the capacity of $\omega_n$ relative to $D$ . I would love to get an insight on how the capacity of a set fits in the definition of quasi-open sets. Also, it would be helpful if I can get a detailed example of a quasi-open set or a reference to find one. I would also appreciate if someone can provide me with some intuition as to how to look at the idea of capacity of a set and which aspect of a set does it actually measure.","['metric-geometry', 'analysis', 'reference-request', 'partial-differential-equations', 'general-topology']"
3318167,Graph with $2n$ vertices and $n^2+1$ edges has at least $n$ triangles.,"I have the following graph theory problem. Problem. Let $n>1$ be an integer.  Given a simple graph $G=(V, E)$ with $|V|=2n$ and $|E|=n^2+1$ . Prove that there at least $n$ triangles in the graph $G$ . Clearly, from Turan's theorem we obtain that there is at least one triangle in $G$ , but it isn't sufficient to solve the problem.
For some small $n$ it can be proved by casework. It is also clear that the bound is sharp. Namely, in graph $K_{n,n}$ with one additional edge there are exactly $n$ triangles. How can we approach this problem?","['graph-theory', 'extremal-graph-theory', 'combinatorics', 'extremal-combinatorics']"
3318220,Romyar Sharifi's notes: Group and Galois Cohomology; ideal vs submodule?,"Lecture Notes Let $G$ be a group.  Below, the author talks about strictly $\Bbb{Z}$ -group rings. Definition 1.1.3. (i) The augmentation map is the homomorphism $\varepsilon : \Bbb{Z}[G] \to \Bbb{Z}$ given by $$
\varepsilon(\sum\limits_{g \in G} a_g g) = \sum\limits_{g \in G} a_g.
$$ (ii) The augmentation ideal $I_G$ is the kernel of the augmentation map $\varepsilon$ . It's intuitively clear to me that this is a $\Bbb{Z}$ -module homomorphism, and not a ring homomorphism.  Thus any kernel is therefore a $\Bbb{Z}$ -submodule.  So why here is it called an ideal ?","['group-homomorphism', 'modules', 'ring-theory', 'abstract-algebra', 'ideals']"
3318242,Volume of an evolving set,"Given a set $ \Omega \subset \mathrm{R}^n$ with smooth boundary I consider a family of sets $ \Omega(t)$ , $t>0$ small, such that $ \partial \Omega(t) = \{ g(x,t): x \in \partial \Omega \}$ . Here $g$ is given by $$
g(x,t) = \nu(x)(h_1(x)t+h_2(x)t^2)
$$ Here $\nu$ is the normal vectorfield on $ \partial \Omega$ and $h_1(x),h_2(x) \in C^{\infinity}(\partial \Omega)$ .
Using the formulae for the first and second variation of volume $$
vol(\Omega(t))'= \int_{\partial \Omega}< X_0, \nu> \,d\mu_{\partial \mu}
$$ $$
vol(\Omega(t))'= \int_{\partial \Omega} < X_0, \nu>' + < X_0, \nu> div_{\partial \Omega} X   \,d\mu_{\partial \mu}
$$ where $F:(-\epsilon, \epsilon)\times R^n \to R^n$ is the variation such that $F(t, \Omega)= \Omega(t) $ and $\frac{\partial F}{\partial t}=X_t$ I was able to prove $$
vol(\Omega(t)) = vol(\Omega) + \int_{\partial \Omega} th_1(x)+ \frac{t^2}{2}(h_1(x)+ 2h_2(x)) dx + \mathcal{O}(t^3).
$$ But what I am in fact interested in is $$
vol(\Omega(t)\backslash \Omega).
$$ I suspect that we have $$
vol(\Omega(t)\backslash \Omega) = \int_{\partial \Omega} (th_1(x)+ \frac{t^2}{2}(h_1(x)+ 2h_2(x)))_+ dx
$$ where $(...)_+$ is the positive part. 
Does anyone have an idea how I could prove this?","['integration', 'calculus-of-variations', 'differential-geometry']"
3318254,"If $f$ takes every value at most $k$ times, then f is differentiable almost everywhere.","I am stuck at the following problem, I got in an old question paper (real analysis). Let $k>0$ be a natural number and Let $f$ be a continuous function on real line such that $f$ takes any value at most $k$ times. Show that $f$ is differentiable almost everywhere. My hunch is that we can divide the real line (except a few exceptional points) into intervals such that on each of those intervals $f$ is one-to-one and therefore monotonic and hence differentiable almost everywhere. 
But, I am not able to make this idea concrete. Any help would be appreciated.","['measure-theory', 'derivatives', 'almost-everywhere', 'real-analysis']"
3318286,Integral $\int_0^\infty \frac{1-e^{-x}\cos bx}{x} dx$,"How can I evaluate this? $$\int_0^\infty \frac{1-e^{-x}\cos bx}{x} dx$$ One standard technique is the residue theorem, and one immediately notices that the integrand is entire.( $x=0$ is a removable singularity.) However, it is not plausible to deform the contour properly to make this integral easier.","['integration', 'improper-integrals']"
3318338,The number of roots of a polynomial $p(x)=x^{12}+x^8+x^4+1$ in $\mathbb{F}_{11^2}$.,"I am trying to count the number of roots of a polynomial $p(x)=x^{12}+x^8+x^4+1$ in $\mathbb{F}_{11^2}$ . Of course, I can plug every element in $\mathbb{F}_{11^2}$ into $x$ in $p(x)$ . But is there any simpler way to do this? Thanks in advance!","['field-theory', 'abstract-algebra', 'roots']"
3318349,Algebraic proof of finiteness of von Dyck groups?,"The von Dyck groups are quotients of the free group on two generators defined as $$D(l, m, n) = \langle  x, y | x^l = y^m = (xy)^n = 1\rangle$$ where $l, m, n \geq 2$ . These groups are the symmetries of a particular triangulation of a (possibly non-Euclidean) surface with triangles of angles $\pi/l, \pi/m, \pi/n$ , and  are finite if and only if $$\frac{1}{l} + \frac{1}{m} + \frac{1}{n} > 1,$$ in which case the surface is a sphere, which obviously has a finite set of symmetries. (The possible values of $(l, m, n)$ are $(2, 2, n)$ where $n \geq 2$ and $(2, 3, n$ ) where $3 \leq n \leq 5$ , plus permutations.) There was a question yesterday from someone who asked if $D(2, 3, 5)$ was finite and couldn't find a proof working from the group presentation alone. Is there an argument distinguishing the finite from the infinite von Dyck groups that works purely algebraically, rather than appealing to geometric properties of the group action? I suspect that these arguments would be difficult to find because the finite and infinite groups have superficially similar presentations, but I can't shake the feeling that there should be one.","['group-theory', 'abstract-algebra']"
3318353,How to define a recursively defined set properly?,"We can define sets recursively. For example we can say $x\in S\iff x=1 \vee \exists y\in S: y+2=x$ But how can we write $S$ ? I.e. how can we describe $S$ in a way we are normaly used to describe a set. I.e. extensionaly or intensionaly? In the form $S=\{x\in\mathbb{N}|\phi(x)\}$ (intensionaly)? Because I think that that recursively defining a set S implies a intensional definition of a subset of the superset of $S$ (The subset is $S$ ) in this case it is $\mathbb{N}$ . (Reasonably the set can also be written extensionally $S=\{1,3,5,....\}$ but this is not the point of my question I am interested in the implicit  intensional definition given by the recursion) I have made a previous example with propositional formulas in a language, A language consists of an ALPHABET and a GRAMMAR. An alphabet $\mathcal{A}$ is a union of three different sets, we will call the element of an alphabet symbols. The first set are the symbols for the propositional variables like $A,B,C....$ , the second set is the set of logical symbols $T,F,\wedge,\vee,\implies,\iff$ and the third set are non-logical symbols like $(,)$ The set of all propositional formulas  is similair to the set $S$ above because there are some initial elements and some elements we can derive from the initial elements recursively. Let $\mathcal{F}$ be the set of all propositional formulas. We want
  to define this set in such a way if we take a element from
  it: $\phi\in\mathcal{F}$ then we want to say that this is equivalent
  to the desired statement : $\phi\in \{1\}\times V\vee \exists!
 \psi_1,\psi_2\in\mathcal{F},a\in\{\vee,\implies,\wedge,\iff\}: \phi=(a,\psi_1,\psi_2)$ . $V$ is the set of all propositional variables
  we have defined extensionaly in beforehand: $V=\{A,B,C,D,E,...\}$ . For the sake of simplicity I have just looked at the set of binary operators but one can also do the same for other operators and also can use another symbols like $\{1,2,3,4\}$ . The important thing is that we can distinguish in this case the triples from one another. I have simplified even further and assumed in the following that $\implies$ is the only logical operator we have to consider and which happens to be a binary logical operator. $$\mathcal{F}=\bigcup_{n\in\mathbb{N_0}}T_n$$ $$T_0= \{1\}\times V\quad\text{and}\quad T_n=\bigcup_{(j,k)\in\{1,...,n-1\}^{2}}\{2\}\times T_{n-1}\times T_j\cup \{2\}\times T_k\times T_{n-1}$$ My question is first of all whether my definition of $\mathcal{F}$ makes sense, i.e. whether I actually have defined all propositional
  formulas? Whether there is alternative that is so general that it does not make
  use of the natural numbers because this definition was motivated by
  someone elses answer to an old question of mine and I am not sure if
  his understanding of $\mathcal{F}$ matches my definition and
  unfortunately this person does not react to my comment anymore (link
  to the question: Induction over propositional formulas ).
  And he seems to make no use of natural numbers. He explicitly said structural induction and not induction over the natural numbers. Whether taking an element from this defined set is equivalent to the desired statement and how I can prove it, especialy the uniqueness. Finally I want to ask whether I can somehow derive a general
  definition of all inductively defined sets from this example.
  Because every inductively defined set in its nature has initial
  elements which could be described with a generl $T_0$ and some
  advanced elements which are in some $T_n$ and deduced from the initial
  elements.","['proof-verification', 'logic', 'recursion', 'definition', 'elementary-set-theory']"
3318386,"Showing $\det\big[ (B+K)^{-1} (A+K) \big] = O(1) $ when $A,B$ are rank 1 updates of $I_n$ and $K$ is symmetric PD with positive entries","In general, given $n$ define $m_A, m_B \in\{1,...,n-1\}$ by $$ m_A = floor(a \times n) $$ $$ m_B = floor(b \times n ) $$ where the constants $a,b \in (0,1)$ are independent of $n$ with $a \ne b$ . Define two matrices as rank 1 updates of the identity matrix: $$A=I_n +u_A u_A^\top\; \text{where}\; (u_A)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_A \\ 1 & \text{else} \end{array}\right.,$$ $$B=I_n +u_B u_B^\top\; \text{where}\; (u_B)_i=\left\{\begin{array}{cc} 0, & i\leq n-m_B \\ 1 & \text{else} \end{array}\right.$$ or equivalently, \begin{equation} 
A =
\begin{pmatrix}
I_{n-m_A} & 0 \\
0 & I_{m_A} + J_{m_A} \\
\end{pmatrix},
B =
\begin{pmatrix}
I_{n-m_B} & 0 \\
0 & I_{m_B} + J_{m_B} \\
\end{pmatrix},
\end{equation} where $J_m$ is a $m \times m$ matrix of ones. My goal Now, let $K$ be a $n \times n$ symmetric positive definite matrix with positive entries. My goal is to show that $\det\left[ (B+K)^{-1} (A+K) \right]$ is $O(1)$ as $n \to \infty$ . Hence, I would like to find bounds which are $O(1)$ . Findings so far From link1 , I know that 1 as an eigenvalue of the matrix $B^{-1}A$ has multiplicity $n-2$ . From link2 , I also know that $\det(B^{-1}A) =\frac{m_A+1}{m_B+1}$ and $\det(A^{-1}B) =\frac{m_B+1}{m_A+1}$ . Thank to the suggestion ( link3 ) by @Semiclassical, $$\det[(B+K)^{-1})(A+K)]
=\frac{\det(A+K)}{\det(B+K)}
=\frac{\det(K+I_n+u_A u_A^\top)}{\det(K+I_n+u_B u_B^\top)}
=\frac{(1+u_A^\top(K+I_n)^{-1} u_A)\det(K+I_n)}{(1+u_B^\top(K+I_n)^{-1} u_B)\det(K+I_n)}=\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}$$ where the third equality holds due to the identity $\det(X+uv^\top)=(1+u^\top X^{-1}v)\det X$ . My attempts and Questions (Question 1) Through numerical experiments in Matlab, I found candidate bounds that seem to work for various versions of $K$ (the Matlab code can be found below).  So my question is: is the following statement true for all $n$ and $K$ (any symmetric positive definite matrix with only positive entries)? I. If $m_B<m_A$ , then \begin{align*}
\det (A^{-1}B)
\leq 
\det\left[ (B+K)^{-1} (A+K) \right] 
\leq 
\det (B^{-1}A)
\end{align*} II. If $m_B>m_A$ , then \begin{align*}
\det (B^{-1}A) 
\leq 
\det\left[ (B+K)^{-1} (A+K) \right] 
\leq 
\det (A^{-1}B)
\end{align*} or equivalently, I. If $m_B<m_A$ , then \begin{align*}
\frac{1+m_B}{1+m_A} 
\leq
\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}
\leq 
\frac{1+m_A}{1+m_B}
\end{align*} II. If $m_B>m_A$ , then \begin{align*}
\frac{1+m_A}{1+m_B} 
\leq 
\frac{1+u_A^\top(K+I_n)^{-1} u_A}{1+u_B^\top(K+I_n)^{-1} u_B}
\leq 
\frac{1+m_B}{1+m_A} 
\end{align*} where $\frac{1+m_A}{1+m_B}\approx \frac{1+a\times n}{1+b \times n}=\frac{1/n + a}{1/n +b}$ and $\frac{1+m_B}{1+m_A} \approx \frac{1/n+b}{1/n+a}$ are $O(1)$ , so the inequalities would imply that $\det\left[ (B+K)^{-1} (A+K) \right]=O(1)$ which is my goal. (Question 2) Are there any other bounds for $\det\left[ (B+K)^{-1} (A+K) \right]$ that are $O(1)$ (possibly obvious bounds that I am missing)? Note I initially thought a sharper bound by $1$ might be possible, but it was not. Suppose $m_B<m_A$ . It is not guaranteed that $u_A^T(K+I_n)^{-1}u_A -u_B^T(K+I_n)^{-1}u_B \geq 0$ . To see this, for instance, consider the example provided here with the matrix $$K =
\begin{bmatrix}
1 & 1 & 1\\
1 & 100 & 99\\
1 & 99 & 100\\
\end{bmatrix}, \\
$$ and the vectors $u_A = (0, 1, 1)$ and $u_B =(0, 0, 1)$ . This means that the sharper lower bound by $1$ : \begin{align*}
\frac{1+m_B}{1+m_A} 
<
1
\leq
\frac{1+u_A^T(K+I_n)^{-1}u_A}{1+u_B^T(K+I_n)^{-1}u_B}
\end{align*} is not possible. However, the proposed bounds by $\frac{1+m_B}{1+m_A}$ and $\frac{1+m_A}{1+m_B}$ still work even with the $K$ , $u_A$ , and $u_B$ in the example above. Code Matlab code for a fixed $n$ : % 1. Specify n,a,b 
n=5;
a=0.7;b=0.3;
mA=floor(a*n);
mB=floor(b*n); 
% 2. Define matrices A and B 
% Define a vector uA whose first n-mA entries = 0 and the last mA entries =1   
uA=ones(n,1);uA(1:n-mA)=0; 
A=eye(n)+uA*uA';
% Do the same for B
uB=ones(n,1);uB(1:n-mB)=0; 
B=eye(n)+uB*uB';
% 3. Define a (this can be any) symmetric PD matrix K with positive entires 
K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n); 
% 4. Check that det(A) = m_A +1. Same for B.
det(A)
mA+1
det(B)
mB+1
% 5. Compare three items
(mB+1)/(mA+1)
det(inv(B+K)*(A+K))
(mA+1)/(mB+1) Matlab code for varying $n$ : n_grid=10:100:1000;
a=0.7;b=0.3;
for i=1:length(n_grid)
   n=n_grid(i);
   mA=floor(a*n);
   mB=floor(b*n);
   uA=ones(n,1);uA(1:n-mA)=0; 
   A=eye(n)+uA*uA';
   uB=ones(n,1);uB(1:n-mB)=0; 
   B=eye(n)+uB*uB';
   K = rand(n,n);K = 0.5*(K+K'); K = K + n*eye(n); 
   determinant(i) = det(inv(B+K)*(A+K));
   det_invBA(i)=(mA+1)/(mB+1); % determinant of inv(B)*A
   det_invAB(i)=(mB+1)/(mA+1); % determinant of inv(A)*B
end 

figure
plot(n_grid,determinant,'*');xlabel('n');
hold on 
plot(n_grid,det_invBA,'*');
hold on
plot(n_grid,det_invAB,'*');
legend('det (B+K)^{-1}(A+K)','det B^{-1}A','det A^{-1}B');
xlim([n_grid(1),n_grid(end)]);xlabel('n')
title(['a =',num2str(a),'  b =',num2str(b)] );","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'inverse', 'upper-lower-bounds']"
3318400,"The ""most likely"" value of a random variable S with a binomial distribution with parameters $n$ and $p$ is $S = \langle np \rangle$?","I am working through Example 11.1.3 on p. 353 of the Second Edition of Thomas & Cover's ""Information Theory"" : We show that $\frac{1}{n+1} 2^{nH(k/n)} \le \binom{n}{k} \le 2^{nH(k/n)}$ . I am following the proof of the upper bound fine, but I do not understand the initial claim used to prove the lower bound. It says: For the lower bound, let $S$ be a random variable with a binomial distribution with parameters n and p. The most likely value of $S$ is $S = \langle np \rangle$ . This can easily be verified from the fact that $\frac{P(S = i+1)}{P(S = i)} = \frac{n-i}{i+1} \frac{p}{1-p}$ and considering the cases when $i < np$ and when $i > np$ . Then, since there are $n + 1$ terms in the binomial sum, $1= \sum_{k=0}^n \binom{n}{k} p^k(1−p)^{n−k} \le (n+1) \max_k \binom{n}{k}p^k(1−p)^{n−k} = (n+1)\binom{n}{\langle np \rangle} p^{\langle np \rangle} (1-p)^{n - \langle np \rangle}$ Probability is not my strong suit and I am not sure what it means by the ""most likely"" value of $S$ is $S = \langle np \rangle$ . I Googled ""mode of binomial distribution"" and found that the mode is $\lfloor (n+1)p\rfloor$ or $\lceil(n+1)p\rceil - 1$ . Is $\langle np \rangle$ common notation for this or am I misunderstanding what is being said? This notation is not explained in the ""list of symbols"" in the back of the textbook either. Any insight would be appreciated.","['binomial-distribution', 'probability-distributions', 'probability-theory', 'information-theory']"
3318452,Sums preserve the property that all stationary points are global minima?,"Let $f: \mathbb R^n \rightarrow \mathbb R$ and $g: \mathbb R^n \rightarrow \mathbb R$ be continuously differentiable functions whose sublevel sets are compact. Suppose that $\nabla f(x) = 0$ for some $x \in \mathbb R^n$ if and only if $x$ is a global minimum of $f$ . Suppose the same for $g$ . Then, is it true that $\nabla(f + g)(x) = 0$ for some $x$ if and only if $x$ is a global minimum of $f + g$ ? It seems to be true if both $f$ and $g$ are convex. What if one of them is not convex?","['optimization', 'multivariable-calculus', 'real-analysis']"
3318485,How many real roots does $x^4 - 4x^3 + 4x^2 - 10$ have?,"How many real roots does this polynomial have? $$x^4 - 4x^3 + 4x^2 - 10$$ Because non-real roots come in pairs, it must have 4, 2 or 0 real roots.
Following Descartes' rules of signs, it either has one negative (real) number and one or three positive numbers. How can I tell if it has 2 or 4 real roots? Thank you very much in advance.","['algebra-precalculus', 'quartics', 'roots', 'polynomials']"
3318487,In how many ways we could divide 300 same balls into three boxes so in each box not will be more than 180 balls,"In how many ways we could divide 300 same balls into 3 boxes so in each box will not be more than 180 balls. Is it correct if I want to sum all the options with $n+k+1\choose n-1$ and subtract all the ""bad"" options? so we have $n+k+1\choose n-1$ = $300+3-1\choose 300-1$ and we need to subtract ""bad"" options (how to calculate them?) What the right way to do it?","['combinatorics', 'discrete-mathematics']"
3318503,To clarify my doubt over my wrong approach in combination question,The question is: A box contains 5 different red and 6 different white balls. In how many ways can 6 balls be selected so that there are at least two balls of each colour. I know this an easy question and we can easily do this by taking different cases and adding them like:- $ { 5 \choose 2 }×{6\choose4}+  { 5 \choose 3}× { 6 \choose 3 }+ { 5 \choose 4 }× { 6 \choose 2 } =425$ But i want to know that why my first approach is wrong that is since we want two red balls and two white balls so we take $ { 5 \choose 2 }×{6\choose2}$ and now the remaining two balls could be taken from the remaining 7 balls (11-4) so the final answer according to me - $ { 5 \choose 2 }×{6\choose2}×{7\choose2}$ Please tell what is wrong with my approach I am very confused (the correct answer is 425).,"['combinations', 'combinatorics']"
3318508,What does Hardy mean in this lemma?,"I am concerned with the paper ""Oscillating Dirichlet's Integrals"" by G.H. Hardy (Quarterly Journal of Pure and Applied Mathematics, Vol. XLIV, pg 1-40). I don't understand Lemma 1, but I suppose this may be a problem of English comprehension? First, some notation. Hardy defines (for positive functions) $f\prec g$ (and $g\succ f $ ) to mean that $g/f \to \infty $ (as $x\to 0$ ), $f \asymp g$ to mean that $f/g \in (\delta, \Delta)$ for some constants $0<\delta <\Delta$ , $f\sim Ag$ to mean that $f/g\to A$ (for an unspecified constant $A$ , that may change from line to line) and $f\sim g$ to mean that $f/g\to 1$ (the usual asymptotic notation). Then he further states that he uses the symbols $\delta,\Delta$ to mean two different things (which cannot both occur at the same time): he writes $$ \log(1/x)^\Delta \prec (1/x)^\delta$$ to mean that the statement holds for any positive numbers $\Delta\gg1$ sufficiently large and $\delta \ll 1$ sufficiently small. At the same time, he would write $$ (1/x)^\delta \prec f \prec (1/x)^\Delta$$ to mean that there exists $\delta,\Delta>0$ such that this statement is true. Now the lemma (Lemma 1). He has already made some assumptions on the functions so that it is always true for any $f,g$ under consideration that one of $f\prec g, f \succ g, f \sim Ag$ is true. Lemma 1. If $f\succ 1, \phi \succ 1$ , then either $f\succ \phi^\Delta$ or there is a number $a$ ( $a\ge 0$ ) such that $f=\phi^a f_1$ , where $\phi^{-\delta} \prec f_1 \prec \phi^{\delta}$ . A similar result holds when $f\prec 1, \phi \prec 1$ . (Proof) For if it is not true that $f\succ \phi^\Delta$ , we can find numbers $\alpha$ such that $$ f \prec \phi^\alpha$$ and we can divide the positive real numbers $\alpha,$ including zero, with at most one exception, into two classes such that for one class $f\succ \phi^\alpha$ , and for the other $f\prec \phi^\alpha$ . There is at most one number, viz. $a$ , the number which divides the two classes, for which $f\sim A\phi^\alpha$ . If $f\sim A \phi^a$ , $a$ belongs to neither class. If, however $a$ belongs to one class or the other, and we put $f = \phi^a f_1$ , it is clear that $\phi^{-\delta}\prec f_1 \prec \phi^{\delta}$ . Thus the result of the lemma is true in either case. Some notes - ""viz."" means ""namely"" It is written $\phi^{-\delta} f_1\prec \phi^\delta$ in the original lemma statement in the paper...surely this is wrong, and I have corrected it in the above. I'm quite sure that I transcribed the $\alpha s$ and $a$ s correctly, but I cannot say for sure due to the quality of my digital copy, which I include a clip of here if there is any doubt - https://i.sstatic.net/KWaUE.png . Questions (at long last) In the lemma statement, which interpretation of $\delta,\Delta$ is being used? I guess it is $f\succ g^\Delta$ for all $\Delta \gg 1$ ? and $\phi^{-\delta} \prec f_1 \prec \phi^{\delta}$ for $\delta \ll 1$ ? The other interpretation seems impossible when he also claims later that there are $\alpha $ such that $f \succ \phi^a$ . When he claims there is at most one $a$ , is he implicitly saying also that the existence of $a$ is trivial and deserves no mention? I guess he would define $a$ as a supremum/infimum? How does he claim that there is at most one $a$ such that $f\sim A \phi^a$ , and then write "" If $f\sim A\phi^a""$ ?","['proof-explanation', 'oscillatory-integral', 'asymptotics', 'real-analysis']"
3318530,"Determine all sequences $a_1, a_2, a_3, . . . $ of nonnegative integers such that $a_1 \lt a_2 \lt a_3 \lt · · ·$","Determine all sequences $a_1, a_2, a_3, . . . $ of nonnegative integers such that $a_1 \lt a_2 \lt a_3 \lt · · ·$ and $a_n$ divides $a_{n-1}+n$ for all $n\ge2$ . I know that one obvious possible sequence is $a_n=a_{n-1}+n$ but I don't know how to prove this is the only one or if there is more from the 2018 SAMO senior round 3 http://www.samf.ac.za/content/files/QuestionPapers/s3q2018.pdf","['number-theory', 'problem-solving', 'sequences-and-series']"
3318549,Undestanding the Martingale Representation Theorem,"Theorem (Representation Theorem):Let $X$ be a binary model and let $V_T$ be an $\mathscr{F}_T$ -measurable random variable. Then there exists a bounded predictable process $H$ and a $v_0\in\mathbb{R}$ with $V_T=v_0+(H.X)_T$ . Proof : We show that there exists $\mathscr{F}_{T-1}$ -measurable random variables $V_{T-1}$ and $H_T$ such that $V_T=V_{T-1}+H_T(X_T-X_{T-1})$ . By a backward induction, this yields the claim.
Since $V_T$ is $\mathscr{F}_T$ -measurable, by the factorisation lemma, there exists a function $g_{T}:\mathbb{R}^T\to\mathbb{R}$ with $V_T=g_T(X_1,...,X_{T})$ . Define: $X^{\pm}_T=f_T(X_1,...,X_{T-1},\pm 1)$ and $V^{\pm}_T=g_T(X_1,...,X_{T-1},X^{\pm}_T)$ . Each of these four random variables is manifestly $\mathscr{F}_{T-1}$ -measurable. Hence we are looking for solutions $V_{T-1}$ and $H_T$ of the following system of linear equations: $V_{T-1}+H_T(X^{-}_T-X_{T-1})=V^{-}_T\\V_{T-1}+H_T(X^{+}_T-X_{T-1})=V^{+}_T$ By construction, $X^+_{T}-X^{-}_T\neq 0$ if $V^{+}_T-V^{-}_T\neq 0$ . Hence we can solve the system and get: $H_t=\begin{cases}
\frac{V^{+}_T-V^{-}_T}{X^+_{T}-X^{-}_T}, & \mbox{ if }X^+_{T}\neq X^{-}_TE\\
0, & \mbox{ else }\end{cases},$ and $V_{T-1}=V^{+}_T-H_T(X^{+}_T-X_{T-1})=V^{-}_T-H_T(X^{-}_T-X_{T-1})\:\:\:\:\:\:\:\blacksquare$ I am trying to understand this proof but I do not know if I understood it right. Questions: 1) When it starts proving by backward induction. Does it mean that $V_T=V_{T-1}+H_T(X_T-X_{T-1})$ is assumed to be true? 2 ) Why is $X^+_{T}-X^{-}_T\neq 0$ if $V^{+}_T-V^{-}_T\neq 0$ true? Why is not the other way around? 3 )Why is the proof complete after solving the system and obtain $H_T$ ? Thanks in advance!","['martingales', 'probability-theory']"
3318564,assume $f(4 x-3)+f(3-4 x)=4 x$ find the $f(x)$,"assume $f(4 x-3)+f(3-4 x)=4 x$ . find the $f(x)$ I did this: $\begin{aligned}
&	t=4 x-3
\\&f(t)+f(-t)=t+3
\\&f(-t)+f(t)=-t+3
\\\Rightarrow &f(t)+f(-t)=3
\end{aligned}$ and stucked here.",['functions']
3318579,How to perform updates on multivariate normals?,"Suppose I have the following model: $$\begin{aligned}
\mathcal L &\sim \mathcal N \left(\vec {\mu_L}, \mathbf \Sigma_L \right) \\
\mathcal X &\sim \mathcal N \left(\vec 0, \mathbf \Sigma_X \right) \\
\mathcal P &= \mathcal L + \mathcal X \\ \\
\end{aligned}$$ Or, equivalently: $$\begin{aligned}
\mathcal L &\sim \mathcal N \left(\vec {\mu_L}, \mathbf \Sigma_L \right) \\
\mathcal P | \mathcal L = \vec l &\sim \mathcal N \left(\vec {l}, \mathbf \Sigma_X \right) 
\end{aligned}$$ What's $\mathcal L | \mathcal P = \vec p$ distributed as, in general? In the one-dimensional case, it is very easy to see through straightforward algebraic manipulation that: $$\mathcal L | \mathcal P = p \sim \mathcal N \left(
\frac{\frac 1 {\sigma_L^{2}} \mu_L + \frac 1 {\sigma_X^{2}} p}{\frac 1 {\sigma_L^{2}} + \frac 1 {\sigma_X^{2}}}, \left(\frac 1 {\sigma_L^{2}} + \frac 1 {\sigma_X^{2}}\right)^{-1}
\right)$$ That is, the posterior mean is the average between $\mu_L$ and $p$ weighted by the precisions of each distribution, and the posterior precision is the sum of the precisions of each distribution (so the posterior variance is its inverse). Algebraic manipulation seems like it shouldn't be the only way to get to this result, though. What other way is there? And how do I use it to generalise this result to higher dimensions? It seems to me that the result should be something like: $$\mathcal L | \mathcal P = \vec p \sim \mathcal N \left(
\frac{\mathbf\Sigma_L^{-1}\vec{\mu_L} + \mathbf\Sigma_X^{-1}\vec p}{|\mathbf\Sigma_L^{-1}| + |\mathbf\Sigma_X^{-1}|}, \left(\mathbf\Sigma_L^{-1} + \mathbf\Sigma_X^{-1}\right)^{-1}
\right)$$ But I have no idea if that's actually the case or how to prove it if so.","['statistics', 'probability-distributions', 'bayesian', 'normal-distribution', 'probability']"
3318597,On completing the square and symmetry,"The quadratic form $f(x,y)=x^2+xy+y^2$ appears quite often on the site here. It is provably positive definite by completing the square: $$f(x,y)=\left(x+\frac y2\right)^2+\frac {3y^2}{4}$$ and this is the only analysis ever really given. I have never seen the symmetric version $$f(x,y)=\frac 34(x+y)^2+\frac 14(x-y)^2$$ quoted in an answer (either to the expression being positive, or to any other question on the site). This refers the original expression to principal axes and retains the symmetry between the two variables, so in principle ought to be preferred. Clearly the first version is adequate for many purposes - but is there a reason why the symmetric version does not feature? Note: This is related to other questions on quadratic forms - the general theory identifies canonical decomposition, but pragmatic methods seem to dodge this, and find other expressions adequate.","['algebra-precalculus', 'soft-question', 'quadratic-forms']"
3318604,Is the space of maps which satisfy this vanishing condition finite-dimensional?,"Let $\mathbb{D}^n \subseteq \mathbb{R}^n$ be the closed $n$ -dimensional unit ball. Let $h:\mathbb{D}^n \to \mathbb{R}^{k}$ be smooth, and suppose that $h(x) \neq 0$ a.e. on $\mathbb{D}^n$ . Set $$V_h=\{  \,\,f \in C^{\infty}(\mathbb{D}^n;\mathbb{R}^{k}) \, \,\,| \, \,  (df_x)^T\big(h(x)\big)=0 \, \text{ for every }\, x \in \mathbb{D}^n \, \} $$ $V_h$ is a real vector-space. Is it always finite-dimensional? Can it be infinite-dimensional for some $h$ ? Edit: Pozz showed nicely that when $k=1$ , $V_h$ always coincides with the space of constant functions, and that for $k>1$ , $V_h$ might be infinite-dimensional (e.g. if $h$ is a constant function). Is there ever a case where $V_h$ is finite-dimensional when $k>1$ ? I suspect that the answer is negative, but I don't know how to prove this.","['smooth-functions', 'real-analysis', 'multivariable-calculus', 'linear-algebra', 'transpose']"
3318605,Minimal set of $n\times n$ matrices whose products generate all $n\times n$ matrices with a single $1$,"Let $n \in \mathbb{N}^*$ be the dimension of the matrices. Let $M_{i,j}$ be the $n\times n$ matrix with $1$ at position $(i,j)$ and 0 elsewhere. Let $S_n$ be the set containing all the $n^2$ matrices $M_{1,1}$ , ..., $M_{n, n}$ . For instance, $S_2$ is $$\left\{ \begin{pmatrix} 1 & 0 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 1 \\ 0 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 1 & 0 \end{pmatrix}, \begin{pmatrix} 0 & 0 \\ 0 & 1 \end{pmatrix}\right\}.$$ I am interested in the smallest number of matrices that I need to take such that I can generate all the others by multiplying them. For example, If I take $$A_1 := \begin{pmatrix} 0 & 1 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, A_2 := \begin{pmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, A_3 := \begin{pmatrix} 0 & 0 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, A_4 := \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 1 & 0 & 0 \end{pmatrix}.$$ Then, I have $$A_1 \cdot A_3 := \begin{pmatrix} 1 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{pmatrix}, A_3 \cdot A_1 := \begin{pmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{pmatrix}, A_3 \cdot A_2 := \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{pmatrix}$$ $$A_4 \cdot A_1 := \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 1 & 0 \end{pmatrix}, A_4 \cdot A_2 := \begin{pmatrix} 0 & 0 & 0 \\ 0 & 0 & 0 \\ 0 & 0 & 1 \end{pmatrix}.$$ Therefore, all the elements of $S_3$ are of the form $A_k$ or $A_k \cdot A_p$ . But maybe if I take matrices that are not in $S_3$ to be those $A_k$ 's, I can use less matrices to generate $S_3$ . So, the question is Given $n$ , what is the minimal number of matrices $A_1, ..., A_m$ such that $$M_{i, j} \in S_n \Rightarrow (\exists k : M_{i,j} = A_k) \text{ or } (\exists k, p : M_{i,j} = A_k\cdot A_p)?$$ Partial result: I already know that I can take $2n - 2$ matrices to generate $S_n$ . Specifically, I can take $M_{1, j}$ for $2\le j \le n$ and $M_{i, 1}$ for $2\le i \le n$ as I did in the example above for $S_3$ . But, again, maybe I need less matrices. In particular, if I use generators that are not elements of $S_n$ themselves, does it improve something?","['matrices', 'additive-combinatorics', 'combinatorics', 'extremal-combinatorics']"
3318612,Evaluate the following limit: $\lim_{k \to \infty} \int_0^1 e^{- (k^2x^2/2)} dx$,"$\mathbf {The \ Problem \ is}:$ Find $$\lim_{k \to \infty} \int_0^1 e^{- (k^2x^2/2)} dx$$ Actually, I have been expanding $e^{- (k^2x^2/2)}$ to find out the limit, but I can't approach further. I think it's divergent . For any help, thanks in advance !!!","['limits', 'real-analysis']"
3318622,Bijection between Dyck paths with $k$ peaks and $n+1-k$ peaks,"A Dyck path of order $n$ can be thought of as a sequence of $n$ $0$ 's and $n$ $1$ 's such that in any initial segment, the $1$ 's do not outnumber the $0$ 's. A peak is a $0$ which is immediately followed by a $1$ . It can be shown that the number of Dyck paths of order $n$ with exactly $k$ peaks is counted by the Narayana numbers, $$
N(n,k) = \frac1n\binom{n}k\binom{n}{k-1}.
$$ Using the above formula, it is easy to show that the Narayana numbers are symmetric in $k$ , in the sense that $N(n,k)=N(n,n+1-k)$ . However, the corresponding symmetry between Dyck paths with $k$ peaks and Dyck paths with $n+1-k$ peaks is not so clear (to me). What is a bijection between Dyck paths of order $n$ with $k$ peaks and Dyck paths of order $n$ with $n+1-k$ peaks?","['catalan-numbers', 'combinatorics', 'combinatorial-proofs']"
3318632,How to join two 3D lines with a third line that connects to each with tangent arcs?,"I need help trying to find the equation of a 3rd line that connects two lines that are already defined in 3d space. The third line has to connect to the first two with ""fillet""/tangent arcs, both of the same radius. See the picture for clarifications. I do NOT want the first two lines to change in any way. Their start and end points need to stay exactly where they are. Knowns: Arc radius and all parameters of the two already defined lines...start and end points, start/end tangent.","['linear-algebra', 'geometry']"
3318646,Are functions that sum to zero over vertices of similar polygons identically zero?,"This is a generalization of problem A1 from the 2009 Putnam competition. The original problem asks for proof that any function $f:\mathbb{R}^2\rightarrow \mathbb{R}$ that sums to zero over the vertices of any square is necessarily zero everywhere. That is, if for any square with vertices $A$ , $B$ , $C$ , and $D$ , \begin{equation}
f(A)+f(B)+f(C)+f(D)=0
\end{equation} then $f(P)=0$ for any point $P\in \mathbb{R}^2.$ You can find the solution to this question here: Putnam 2009 A1 Points in a plane . My question is whether this result can be generalized to any family of similar polygons. If $\Sigma_{i}f(A_i)=0$ whenever $A_1,...,A_n$ are the vertices of a polygon similar to a given polygon, does $f$ need to be $0$ everywhere? This can be proven for regular $n$ -gons by a method I describe in the comments. The proof for equilateral triangles amounts to adding the vertices of the three blue equilateral triangles in the picture and then subtracting the vertices of the the two red triangles leaving $3f(A)=0$ and, since $A$ is arbitrary, proving $f$ is zero everywhere. Essentially the same argument applies to any $n$ -gons if you draw them evenly arranged around the center point. It doesn't matter if they overlap.","['contest-math', 'symmetry', 'geometry', 'polygons']"
3318656,"If f(x_n) converges whenever (x_n), then f is continuous.","This is the original question: Let $f: X\to Y$ be a map between metric spaces.  Prove that if $(f(x_n))_{n=0}^\infty$ converges in $Y$ whenever $(x_n)_{n=0}^\infty$ converges in $X$ then $f$ is continuous.
  [Note: it is not given that $f(x_n) → f(x)$ whenever $x_n → x$ .] I have spent quite a bit of time on this problem. I finally looked it up and have been reading the solution in the link below: for every convergent sequence $x_n$, $f(x_n)$ also converges. Does this imply continuity of f? I am confused by the solution given. I understand that they proved that $f(x_n)$ does not converge to $f(x)$ , but this doesn't prove that $f(x_n)$ does not converge at all. If we want to use the contrapositive, shouldn't be have to show that if $f$ is discontinuous, then there is some sequence $(x_n)$ that converges 
in $X$ but $f(x_n)$ does not converge in $Y$ . That is, we have to show that for some convergent sequence, there is some $\varepsilon$ such that for all $n$ greater than some $N$ , $f(x_n)>\varepsilon$ . Any clarification or hints would be greatly appreciated.","['continuity', 'convergence-divergence', 'analysis']"
3318676,How many times should I perform an experiment to get a high level of accuracy?,"I have an experiment with 4 factors (independent variables). The first and second variable have 8 levels, the third 5 and the fourth only 3 levels. Theres one dependent variable. How many times should I perform the experiment in order to get a high level of accuracy? For instance a 95%. I would appreciate the mathematical procedure behind this calculation. Thank you.","['statistical-inference', 'statistics', 'experimental-mathematics', 'mathematical-modeling']"
3318684,Proof of equivalence between variational principle and Euler-Lagrange equations on a manifold,"Let M be some manifold, and TM the tangent bundle. Let $\gamma : [a,b] \to M$ be a smooth curve on M defined on an interval on $\mathbb{R}$ . Let $J$ be another interval in $\mathbb{R}$ containing 0. A 'deformation of $\gamma(t)$ with fixed endpoints' is a curve $\overline{\gamma}:[a,b]\times J \to M : (t,\epsilon) \mapsto \overline{\gamma}_{\epsilon}(t)$ such that $\overline{\gamma}_{0}(t)=\gamma(t), \forall t \in [a,b]$ $\overline{\gamma}_{\epsilon}(a) = \gamma(a)$ and $\overline{\gamma}_{\epsilon}(b) = \gamma(b)$ for all $\epsilon \in J$ Let L be a lagrangian, i.e. a smooth map $L : TM \to \mathbb{R} : (p,\dot{p}) \mapsto L(p,\dot{p})$ . For $M = \mathbb{R}^n$ it is simple to prove that $\gamma$ fulfills the variational principle $$\left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_a^b L(\overline{\gamma}_{\epsilon}(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt = 0$$ for every deformation of $\gamma$ , if and only if $\gamma$ satisfies the Euler-Lagrange equations $$\frac{d}{dt}\frac{\partial L}{\partial \dot{p}}(\gamma(t),\dot{\gamma}(t)) - \frac{\partial L}{\partial {p}}(\gamma(t),\dot{\gamma}(t)).$$ My question In many references (any book on geometric mechanics), it is stated that this equivalence hold on any manifold - not just the euclidean case. And in several places (for examples Marsden and Ratiu's book on geometric mechanics) I have seen it stated that this can be prooved in coordinates. However, this is only done for the case where $\gamma$ is contained in a single chart. I am trying to prove, or looking for a reference that proves, the general case. Preferably in coordinates, or in a relatively 'simple' intrinsic way. Can anyone help with this? My attempt Say we want to prove the following direction; let $\gamma : [a,b]\to M$ fulfill the Euler-Lagrange equation. I.e. it fulfills the equation in every chart. We want to show that the variation of the integral is 0. Choose a cover of M , and let $\gamma$ be covered by 3 charts, as in the figure below (copied from the book 'Geometric mechanics and symmetry' by Holm et al). Then its deformations (for small enough $\epsilon$ ) is also covered by these charts. Depiction of a curve and deformations covered by 3 charts Choose one such deformation. Then we can split it into three subcurves defined on the intervals $[a,t_1],[t_1,t_2],[t_2,b]$ , respectively, such that each is contained in a single chart. Likewise, we can split up the integral into three integrals \begin{align}
\left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_a^b L(\overline{\gamma}_{\epsilon}(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt =& \left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_a^{t_{1}} L(\overline{\gamma}_{\epsilon}(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt \\
&+ \left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_{t_{1}}^{t_{2}} L(\overline{\gamma}_{\epsilon}\nonumber(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt \\
&+ \left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_{t_{2}}^b L(\overline{\gamma}_{\epsilon}(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt.
\end{align} In each integral, we can use the coordinates of the suitable chart. However, for each such curve/deformation in $\mathbb{R}^n$ , the endpoints will not be fixed, except at a and b . From the proof of the equivalence on $M = \mathbb{R}^n$ , one can deduce that if an arbitrary deformation $\overline{g} : [T_1,T_2]\times J \to \mathbb{R}^n$ (not necessarily with fixed endpoints) fulfills the E-L equations, then \begin{align*}
\left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_{T_1}^{T_2} L(\overline{g}_{\epsilon}(t),\dot{\overline{g}}_{\epsilon}(t)) dt = \left[ \frac{\partial L}{\partial \dot{p}}(g(t),\dot{{g}}(t)) \cdot \left.\frac{d}{d\epsilon}\right|_{\epsilon=0} \overline{g}_{\epsilon}(t)\right]_{T_1}^{T_2} 
\end{align*} This can be used on the previous equation to get \begin{align}
\left. \frac{d}{d\epsilon} \right |_{\epsilon=0} \int_a^b L(\overline{\gamma}_{\epsilon}(t),\dot{\overline{\gamma}}_{\epsilon}(t)) dt =& \left[ \frac{\partial L'}{\partial \dot{p}}(\gamma(t)',\dot{{\gamma}}'(t)) \cdot \left.\frac{d}{d\epsilon}\right|_{\epsilon=0} \overline{\gamma}'_{\epsilon}(t)\right]_{a}^{t_1}  \\
&+ \left[ \frac{\partial L''}{\partial \dot{p}}(\gamma(t)'',\dot{{\gamma}}(t)'') \cdot \left.\frac{d}{d\epsilon}\right|_{\epsilon=0} \overline{\gamma}_{\epsilon}''(t)\right]_{t_1}^{t_2} \\
&+ \left[ \frac{\partial L'''}{\partial \dot{p}}(\gamma(t)''',\dot{{\gamma}}(t)''') \cdot \left.\frac{d}{d\epsilon}\right|_{\epsilon=0} \overline{\gamma}_{\epsilon}'''(t)\right]_{t_2}^{b} 
\end{align} where the clumsy '-notation denotes that in each term of the sum we use a different coordinate representation of L , $\gamma$ and $\overline{\gamma}$ , since they belong to different charts. In the case where $M = \mathbb{R}^n$ we can use a single chart, so the sum telescopes. But on a general manifold, the sum does not necessarily telescope due to the different coordinate maps. Is there a way to fix this?","['calculus-of-variations', 'classical-mechanics', 'differential-geometry']"
3318700,Using roots of unity to factorize polynomials,"Engel's Problem Solving Strategies has the following exercise (p.260 #28). Show $x^4 +x^3+x^2+x+1 \hspace{0.1cm}\big{|}\hspace{0.1cm}x^{44}+x^{33}+x^{22}+x^{11}+1$ . I've been working through these problems and have seen the following strategy implemented quite a bit. Essentially it boils down to the following: Let $\omega^5=1$ be a fifth root of unity excluding $1$ ; then any $\omega$ satisfying this equation is simultaneously a root for the polynomial $x^4 +x^3+x^2+x+1$ and $x^{44}+x^{33}+x^{22}+x^{11}+1$ , so claim follows. Here is my question. Is it true that for every value of $n$ , the roots of unity $\omega$ satisfying $\omega^n=1$ , excluding $1$ , are exactly the roots of the polynomial $\sum_{j=0}^{n-1}x^j$ ? Or, e.g., only prime $n$ ? Thank you.","['complex-analysis', 'number-theory', 'polynomials']"
3318740,Getting bounds from Riemann sums,"$y=\sum_{i=0}^9\frac{1}{10\sqrt 3} \frac{1}{1+(\frac{i}{10\sqrt 3})^2}$ and $x=\sum_{i=1}^{10}\frac{1}{10\sqrt 3} \frac{1}{1+(\frac{i}{10\sqrt 3})^2}$ . Prove that $x<\frac{\pi}6<y$ $\frac{x+y}2<\frac{\pi}6$ My try : I have shown that $x,y$ are respectively lower and upper Riemann sum of the function $$I=\sqrt 3 \int_0^1\frac{dx}{3+x^2}=\frac{\pi}6.$$ So we are done with part a). How to deal with part b). Here $$x+y=\frac34\frac{1}{10\sqrt 3}+2\sum_{i=1}^{9}\frac{1}{10\sqrt 3} \frac{1}{1+(\frac{i}{10\sqrt 3})^2}+\frac{1}{10\sqrt 3}$$","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'riemann-integration']"
3318742,Are there any efficient algorithms for deciding whether a finitely presented group is Abelian?,Given a finitely presented group (finite number of generators and finite number of relations) is there any efficient algorithms which detects whether the group is Abelian or not? (So the problem is to check whether the commutators can be generated by the relations or not.),"['combinatorial-group-theory', 'group-theory', 'computer-science']"
3318757,Proof verification for function property,"May I have a proof verification? Suppose $f:A \rightarrow B$ .Let $C,C_1,C_2$ be subsets of $A$ and $D,D_1,D_2$ be subsets of $B$ . Prove $f^{-1}(B\setminus D)=A\setminus f^{-1}(D)$ Attempt Let $x \in f^{-1}(B\setminus D)$ then $f(x) \in B$ and $f(x) \notin D$ Thus $x \in f^{-1}(B) \subseteq A$ and $x \notin f^{-1}(D)$ so $x \in A \setminus f^{-1}(D)$ Now suppose $x \in A \setminus f^{-1}(D)$ thus $x \in A$ and $x \notin f^{-1}(D)$ Then $f(x) \in f(A) \subseteq B$ and $f(x) \notin D$ so $f(x) \in B \setminus D$ And $x \in f^{-1}(B \setminus D)$","['elementary-set-theory', 'proof-verification']"
3318772,Calculation of This Pullback of a Differential Form,"Consider the differential form $f dx$ in $\Omega^1 (\mathbb{R}^2)$ , where $f \in C^{\infty }(M)$ . $\Omega^1 (\mathbb{R}^2)$ has basis $\{ dx, dy \}$ . I am looking for a rigorous calculation of the pullback of $dx$ to $\mathbb{R}$ by some smooth map $\phi : \mathbb{R} \rightarrow \mathbb{R}^2$ . Its basic but I want to do it rigorously. I think $f dx$ should pull back to $p \mapsto f \circ \phi (p) \frac{\partial \phi_1 } {\partial t}(p) dt$ in $\Omega^1 (\mathbb{R})$ , but I'm not sure. If someone gives me a brief outline of how to do this calculation, I can fill in the details for myself.","['pullback', 'de-rham-cohomology', 'differential-forms', 'differential-geometry']"
3318791,How would you prove an equality of sums of set cardinalities?,"I know how to prove equality in general sets using basic logic (for example: how to prove that for subsets $A,B,C$ of a universal set $U$ , $A \cap(B \cup C)=(A \cap B) \cup(A \cap C)$ ). I also understand that to show equality in the cardinality between two sets, one must show a bijection between them. However, I don't understand how to prove something like: $|C \cap A|+|C \cap B|-|A \cap B| = |(C\cap A)\cup(C\cap B)|-|(A \cap B)\setminus C |$ . I know this must be true after exhausting  every possibility with venn diagrams so I guess this counts as a proof but I am wondering if there is a better way to prove these kinds of questions (more insightful proofs using basic logic).","['elementary-set-theory', 'proof-writing']"
3318813,Inequality for polynomial value from Rudin,"I can't follow a proof in Rudin's book. If $$
P(z)=z^n+a_{n-1}z^{n-1}+ ... +a_0
$$ is a polynomial with complex coefficients, then $$
r>1+|a_{n-1}|+...+2|a_0|
$$ implies $|P(r)|>|P(0)|$ .","['algebra-precalculus', 'polynomials', 'inequality']"
3318822,Help for simplification,"The known datapoints are $\left[ \begin{matrix}
n_x & s_x & a_x & p_x \\
n_y & s_y & a_y & p_y \\
n_z & s_z & a_z & p_z \\
0 & 0 & 0 & 1 
\end{matrix}\right]$ where $$\begin{align}
n_x &=           -\sin\phi_1\sin\phi_2 + \cos\phi_1\cos\phi_2\cos\theta_1 \\
n_y &= \phantom{-}\cos\phi_1\sin\phi_2+ \sin\phi_1\cos\phi_2\cos\theta_1\\
n_z &= -\cos\phi_2\sin\theta_1 \\[6pt]
s_x &= -\sin\phi_1\cos\phi_2 - \cos\phi_1\sin\phi_2\cos\theta_1 \\
s_y &= \phantom{-}\cos\phi_1\cos\phi_2 - \sin\phi_1\sin\phi_2\cos\theta_1 \\
s_z &= \phantom{-}\sin\phi_2\sin\theta_1 \\[6pt]
a_x &= \cos\phi_1\sin\theta_1 \\
a_y &= \sin\phi_1\sin\theta_1 \\
a_z &= \cos\theta_1 \\[6pt]
p_x &= h_x + r_1\cos\phi_1\cos\theta_1 +r_2\left(- \sin\phi_1\sin\phi_2 + \cos\phi_1\cos\phi_2\cos\theta_1\right) + d_2\cos\phi_1\sin\theta_1  \\
p_y &= h_y + r_1\sin\phi_1\cos\theta_1 + r_2\left(\phantom{-}\cos\phi_1\sin\phi_2 + \cos\phi_2\sin\phi_1\cos\theta_1\right)  + d_2\sin\phi_1\sin\theta_1 \\
p_z &= h_z - r_1\sin\theta_1 - r_2\cos\phi_2\sin\theta_1 + d_2\cos\theta_1 
\end{align}$$ $px$ , $py$ , and $pz$ can be simplified to $$\begin{align}
p_x &= h_x + r_1\cos\phi_1\cos\theta_1 +r_2 n_x + d_2 a_x\\
p_y &= h_y + r_1\sin\phi_1\cos\theta_1 +r_2 n_y + d_2 a_y\\
p_z &= h_z - r_1\sin\theta_1 + r_2 n_z+ d_2a_z 
\end{align}$$ I've gotten my simplification to here but I keep finding it hard to find an elegant way to remove the other sine and cosine terms so that we're able to solve the unknown $(h_x, h_y, h_z, r_1, r_2, d_2)$ relative to 6 or more datapoints matrices.","['trigonometry', 'systems-of-equations']"
3318835,incremental computation of standard deviation (batch by batch),"I found this link to compute standard deviation when we add samples one by one. If we add samples batch by batch ( $N_B$ ), how could we incrementally calculate the variance?","['computational-mathematics', 'statistics', 'variance', 'algorithms']"
3318870,Why can't $\int x(x-3)^8\ dx$ be integrated by parts?,"I've been given the task of integrating the following equation $$\int x(x-3)^8dx$$ but they've all been saying that it must be done via substitution and is impossible to do view integration by parts. I'm wondering why that is. Video of solution by substition: https://youtu.be/CXgsorgesS0?t=109 Attempt to solve it by parts: $$\int x(x-3)^8dx$$ $$ v=x,u'= (x-3)^8 $$ $$ u'=1, u = 1/9(x-3)^9$$ $$\int x(x-3)^8dx = (1/9)x(x-3)^9 -\int 1/9(x-3)^9$$ $$=(1/9)x(x-3)^9 -(1/90)(x-3)^{10}+C$$","['integration', 'calculus']"
3318904,show that $n\Upsilon_{n-1} \equiv -1 \pmod{n}$ iff $n$ is prime,"The Bernoulli numbers $B_n$ . where all numbers $B_n$ are zero with odd index $n>1$ . first values are given by $B_{0} = 1$ , $B_{1} = -1/2$ , $B_{2} = 1/6$ , $B_{3} = -1/30$ . Agoh conjecture: let $n$ be a positive integer with $n \ge 2$ , then $$nB_{n-1} \equiv -1 \pmod{n}\iff n\text{ is prime} $$ The idea of ​​ $\Upsilon$ number comes from my power sum formula and negative values of zeta function Definition let $m$ be a non-negative integer $$\zeta(-m)=(-1)^{m}\frac{B_{m+1}}{m+1}=\sum_{b=1}^{m+1} \Upsilon_b\sum_{i=0}^{b-1} (-1)^{i}(b-i)^{m}\binom{b-1}i$$ so we can calculate values of $\Upsilon_b$ , if we substitute value $\zeta (0)=-1/2$ gives $\Upsilon_1=-1/2$ . Again we can calculate next value using or substituting previous values of $\Upsilon_b$ . first values are given by $$\begin{align*}
\Upsilon_1&=-\frac{ 1}{2},\\
\Upsilon_2&=\frac{ 5}{12},\\
\Upsilon_3&=-\frac{ 3}{8},\\
\Upsilon_4&=\frac{ 251}{720},\\
\Upsilon_5&=-\frac{ 95}{288},\\
\Upsilon_6&=\frac{ 19087}{60480},\\
\Upsilon_7&=-\frac {5257}{17280}.
\end{align*}$$ hypothesis: let $n$ be a positive integer with $n \ge 2$ , then $$n\Upsilon_{n-1} \equiv -1 \pmod{n} \iff n\text{ is prime} $$ Else show that hypothesis is equivalence to agoh's conjectures Else 
    show that if n is prime then $n\Upsilon_{n-1} \equiv -1 \pmod{n} $ Steven Clark had verified conjecture $n\Upsilon_{n-1} \equiv -1 \pmod{n}\iff n\in \mathbb{P}$ for $2 \leq n \leq 500 $ More simply Let $D_{m,b}=\sum_{i=0}^{b-1} (-1)^{i}(b-i)^{m}\binom{b-1}i$ So $D_{m,m+1}=m!$ $$\Upsilon_{m+1}=\frac{(-1)^{m}B_{m+1}-(m+1)(\Upsilon_1 D_{m,1}+\Upsilon_2 D_{m,2}+\cdots+\Upsilon_m D_{m,m})}{(m+1)!}$$ Please see my post on MO","['number-theory', 'bernoulli-numbers', 'elementary-number-theory', 'prime-numbers']"
3318926,Compute $2\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^4}+\sum_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^3}$,"How to prove that $$2\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^4}+\sum_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^3}=7\zeta(7)+\frac{7}{4}\zeta(3)\zeta(4)-\frac32\zeta(2)\zeta(5)\tag{1}$$ where $H_n^{(p)}=1+\frac1{2^p}+\cdots+\frac1{n^p}$ is the $n$ th generalized harmonic number of order $p$ . You can find the proof of the equality above in the book (Almost) Impossible Integrals, Sums and Series page 297 using pure series manipulations but is it possible to prove it using integration or any other way? All approaches are appreciated. In case you are curious about the result of each sum, you can find them also in the book $$\sum_{n=1}^\infty\frac{H_nH_n^{(2)}}{n^4}=2\zeta(2)\zeta(5)+\frac34\zeta(3)\zeta(4)-\frac{51}{16}\zeta(7)$$ $$\sum_{n=1}^\infty\frac{H_nH_n^{(3)}}{n^3}=\frac{81}{8}\zeta(7)-\frac{11}{2}\zeta(2)\zeta(5)+\frac14\zeta(3)\zeta(4)$$ but again, our main problem here is to prove the equality in (1) in different ways. Thanks","['integration', 'harmonic-numbers', 'calculus', 'polylogarithm', 'sequences-and-series']"
3318935,How to find number of words made using letters of word 'EQUATION' if order of vowels do not change,"Find number of words made using letters of word 'EQUATION' if order of vowels do not change. My attempt:-  since we do not have to change the order of the vowels  hence, 
             _E_U_A_I_O_
we have $6$ places to fill the remaining letters. Therefore total number of cases $= C(6,3) \times 3! + 6 \times 3!= 156$ But the answer is $336$ .","['permutations', 'binomial-coefficients', 'combinatorics']"
3318936,Use proof by contradiction to prove markov's inequality,"Markov's inequality says: $P(X\geq \alpha ) \leq \frac{E[X] }{\alpha} $ I know the actual proof of this theorem, but i was talking to someone who used a proof by contradiction to prove this. Suppose $P(X\geq \alpha ) > \frac{E[X] }{\alpha} $ and assume that X can only take on values larger than alpha, then $\alpha>E[X]$ is the contradiction. I think this is incorrect because it does not take into account the possibility that there is no relationship between the left and right hand side at all. Is this right? Under what circumstances can you use proof by contradiction?","['statistics', 'probability']"
3318966,Measurable functions with arbitrarily small periods are constant,"Let $f:\mathbb{R}\to \mathbb{R}$ be a measurable function that has arbitrarily small periods, that is, for any $\varepsilon>0$ there is $0<T<\varepsilon$ such that $f(x+T)=f(x)$ for all $x\in\mathbb{R}$ . I need to show that $f$ is constant, at least almost everywhere. Can someone find an elementary proof of this ? One proof : I found a proof in which I used distribution theory. First replace $f$ by $f_A:= f\mathbf{1}_{\vert f\vert\leq A}$ for $A>0$ to get a bounded function.
Therefore, $f$ belongs to $L_{loc}^1(\mathbb{R})$ and can be seen as an element of $\mathcal{D}'(\mathbb{R})$ . Take $T$ a period of $f$ and denote by $\tau_Tf:= f(\cdot-T)$ the translation operator by $T$ . Take $\varphi$ as a test function, and compute \begin{aligned}
\left<f, \varphi \right> &=\left<\tau_T f, \varphi \right> \\ &= \left< f,\tau_{-T} \varphi \right>
\end{aligned} Thus, for $T$ arbitrarily small $$\left<f,\tau_T \varphi - \varphi \right> = 0.$$ Then, one uses the fact that if $\underset{n\to\infty}{\lim}T_n =0$ , then $$\underset{T_n\to0}{\lim} \frac{\tau_{T_n}\varphi - \varphi}{T_n} = \varphi'\quad \text{in}\ \mathcal{D(\mathbb{R})}.$$ Passing to the limit in the above inequality one gets that $$\left<f', \varphi \right> = 0 $$ for any test function $\varphi$ . Thus $$f' = 0\quad\text{in}\ \mathcal{D'}(\mathbb{R}).$$ By a classical result, this implies that $f$ is constant almost everywhere. Motivation: In a paper called ""On the approximation of Lebesgue integrals by Riemann sums"", Jessen proves the following theorem. Let $f$ be a $L^1(\mathbf{T})$ function. Denote its Riemann sum $$f_n:= \frac{1}{n}\sum_{k=1}^n f(x+\frac{k}{n}).$$ Then $$\underset{n\to\infty}{\lim} f_{2^n}(x) = \int_0^1 f\quad a.e.x $$ Jessen considers $$\phi(x):=\overline{\lim} f_{2^n}(x)$$ which is a $2^{-n}$ periodic function for all $n$ , hence an almost everywhere constant function. The proof reduces to show that this constant is $\int_0^1 f$ .","['periodic-functions', 'distribution-theory', 'measurable-functions', 'real-analysis']"
3318968,Direction of a Point in a Vector Field,"Consider the system of first order ODEs $$x'=x(y-1) \ \ y'=y(2-x^2-y). \tag{1}$$ For the equilibrium points $(\pm 1,1)$ , these give stable spirals. I am trying to determine the direction of rotation of this spiral. I considered a point, $$(x,y)=(2,2),$$ and by substituting this into $(1)$ , I found that $$x'>0, \  \ y'<0.$$ By constructing an arrow diagram as shown below, I believed that the point $(x,y)=(2,2)$ rotates anticlockwise (and hence the spiral rotates anticlockwise). But the solution in my books states this is in fact a clockwise rotation.","['vectors', 'ordinary-differential-equations', 'dynamical-systems']"
3318996,How to find whether $f(x)=\frac{2x(\sin(x)+\tan(x))}{2[\frac{x+2\pi}{\pi}]-3}$ is a many-to-one function or not?,"Question: How to find whether the function $$f(x)=\frac{2x(\sin(x)+\tan(x))}{2[\frac{x+2\pi}{\pi}]-3}$$ is a
many-to-one function or not? where [.] represents greatest integer function or floor function. Graph of $f(x)$ : Clearly, any horizontal line cuts the graph at more than one point, thus the function is many-to-one. I solved this without using the graph by taking an example, i.e., $f(0)=f(\pi)=0$ , thus the function is many-to-one. But, I wish to solve this problem algebraically(without taking any set of examples). Usually, we take derivative and see whether the sign of derivative changes or not to determine whether the function is many-to-one or not. But here due to the presence of floor function, I am unable to calculate the derivative. Is there any other way to prove the function is many-to-one, as solving by using graphing calculator is easier but can't be used in the exam.",['functions']
3319046,Intersection (set theory) how to write as a logical statement,"I am having troubles writing down the definition of the intersection of two sets $A$ and $B$ (that is not empty). I thought of $$(∃x)(∃y)(x∈A∧y∈B∧x=y)\enspace\text{or} \enspace(∃x)(∃y)((x∈A∧y∈B)⇒x=y)$$ but I am not sure, which of them, if any, is correct. Thank you!","['elementary-set-theory', 'logic']"
3319047,"Is there a name for the following ""special function""?","I'm considering a series of polynomial functions defined as follows: let $q\in \mathbb{R}$ , $q\neq 0$ . For each $n\geq 1$ let $$
f_n(x):=\prod_{i=1}^nq^ix(1-q^ix).
$$ This series of functions looks like Hermite functions but not the same. My question is: is the above function a well-known special function? If yes, where can I find its properties?","['special-functions', 'functions', 'polynomials']"
3319098,In how many ways 5 couples can be seated around a circular table with some conditions attached?,"In how many ways 5 couples can be seated around a circular table such that men and women sit alternatively and no person sits adjacent to his or her spouse ? Edit: The chairs are alike! 
I have gone through few answers to the question where 5 couples are arranged in such a way that they don't sit together using inclusion and exclusion principle but I am not getting my way around when men and women also have to sit alternatively!",['combinatorics']
