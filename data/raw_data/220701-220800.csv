question_id,title,body,tags
4516727,Solve this integral of degree $2043$,Find the value of $$\frac{\displaystyle\int_0^1(x+1)^{1010}\:\:dx}{\displaystyle\int_0^1(x^{2043}+1)^{1010}\:\:dx}$$ I evaluated the value of the numerator as $$\frac{2^{1011}-1}{1011}$$ But can't do the same for the denominator. Any help is greatly appreciated.,"['integration', 'calculus', 'real-analysis']"
4516744,Is $\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)\cong\mathrm{Spec}(\overline{\mathbb Q}\otimes\overline{\mathbb Q})$ a group scheme,"This question gives a homeomorphism $\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)\cong\mathrm{Spec}(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q})$ , sending $\sigma\in\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)$ to $\mathfrak p_\sigma:=\ker(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q}\to\overline{\mathbb Q}:x\otimes y\mapsto x\sigma(y))$ . Does the group structure on $\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)$ give $\mathrm{Spec}(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q})$ the structure of a group scheme? First of all, the swapping morphism $\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q}\to \overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q}:x\otimes y\mapsto y\otimes x$ takes $\mathfrak p_\sigma$ to $\mathfrak{p}_{\sigma^{-1}}$ , so it should be taken as the inverse map. The problem is, I don't think $\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q}$ can be given the structure of a $\overline{\mathbb Q}$ -algebra structure such that the swapping morphism is a $\overline{\mathbb Q}$ -homomorphism. That is, I doubt $\mathrm{Spec}(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q})$ can be given the structure of a group scheme over $\overline{\mathbb Q}$ . But over $\mathbb Q$ , the fibered product $$\mathrm{Spec}(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q})\times_{\mathrm{Spec}\mathbb Q}\mathrm{Spec}(\overline{\mathbb Q}\otimes_\mathbb Q\overline{\mathbb Q})$$ is homeomorphic to $\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)^3$ , not $\mathrm{Gal}(\overline{\mathbb Q}/\mathbb Q)^2$ . Is there any way to remedy this situation?","['galois-theory', 'group-schemes', 'algebraic-geometry', 'schemes']"
4516768,Possible value of angles in parametrization of $2\times 2$ unitary matrices,"I am doing parametrization of $2\times 2$ matrices from book 'The Unitary and Rotation Groups' by F.D. Murnaghan.
Writing the unitary matrix as $U = \begin{bmatrix} a & c \\ b & d \end{bmatrix}$ and applying the condition $U^\dagger U = I_{2\times 2}$ we get equations like $$a^*a + b^*b = 1$$ $$c^*c + d^*d = 1$$ $$a^*c + b^*d = 0$$ where $a, b, c, d \in \mathbb{C}$ . So, we can write for the first equation: $$ a = e^{\iota \alpha_1}\cos\phi$$ and $$ b = e^{\iota \alpha_2}\sin \phi$$ Then what are the bounds on the values of $\alpha_1, \alpha_2 \ \& \  \phi ?$ I can understand that $\phi$ can vary from $-\pi$ to $\pi$ , but so should $\alpha_1$ and $\alpha_2$ . But the book says that the latter angles can have values between $-\pi/2$ to $\pi/2$ and terms them as latitude angles, while $\phi$ as longitude angle. Why is there a constraint on the value of $\alpha$ 's?","['unitary-matrices', 'group-theory', 'linear-algebra', 'parametrization']"
4516782,Does this generalization of matrix exponential formula make sense?,"In a linear algebra class we defined the exponential of some invertible linear matrix $M$ through a power series $$
\exp M =\sum_{k=0}^\infty \frac{M^k}{k!},
$$ where if one had to write the explicit matrix indices they would have $$
[\exp M]_{ij} =\text{I}_{ij}+M_{ij}+\frac{1}{2!} \sum_m M_{im}M_{mj}+\frac{1}{3!} \sum_{mn} M_{im}M_{mn}M_{nj}+\cdots,
$$ here the discrete sums run over the dimension of the matrix $M$ . I was wondering if there is a continuum generalization of such sum/expression, where the discrete sum is replaced by integrals over some domain $D$ . Say that $M$ becomes some function of two variables $m(x, y)$ , then what is the sum (and when does it converge?) $$
\delta(x-y)+m(x,y)+\frac{1}{2!}\int_D dz\, m(x,z)m(z,y)
+\frac{1}{3!}\int_{D^2} dz\, dw\, m(x,z)m(z,w)m(w, y)+\cdots$$ where in place of the identity matrix I wrote Dirac's delta function. Is this completely crazy or not? Does the sum equal some sort of $\exp m$ ? Thank you!","['integration', 'calculus', 'linear-algebra', 'exponential-function']"
4516783,A 'measure' on $\mathcal{P}(\mathbb{R})$,"Question: Is there function $\mu : \mathcal{P}(\mathbb{R}) \to [0, \infty]$ with the following properties: $\mu$ is countably additive. (on disjoint sets) $\mu((a, b])) = b-a$ , i.e., it extends the usual length function.
? Notice that I am not putting the usual translation invariant condition. The reason for this question is to try to understand what leads to the failure of existence of an actual measure on power set of $\mathbb{R}$ . Is the translation invariance enough of an obstruction? An answer to this question will resolve this. From the standard counterexample : Vitali Sets it is not clear since we use all the aspects of a measure to get to the contradiction. Of course if we further remove the condition that $\mu$ extends the usual length function (2) then the dirac measure is an example but so far I have no counterexample when only translation invariance is removed. Addition : One way to see that size of $\mathcal{P}(\mathbb{R})$ is not a problem by itself, since the set of all lebesgue measurable sets has the same cardinality as $\mathcal{P}(\mathbb{R})$ . This can be seen by taking a lebesgue null subset of $\mathcal{R}$ which has same cardinality as $\mathbb{R}$ , for example, the Cantor Set .","['real-numbers', 'measure-theory', 'set-theory']"
4516790,Asymptotic expansion of integral involving an ArcTan,"The function I want to study reads $$
f(y)= \frac{\sqrt{1+y^2}}{\pi^2 y} \int_0^\infty du \left\lbrace \arctan \left[\frac{2y}{u^2(1+y^2)+1-y^2 }\right] \right\rbrace^2 \, ,
$$ and it is well-defined for any real $y$ (actually the integral diverges for $y=0$ , but this divergence is tamed by the prefactor so that $f(0)=0$ , and $f(y)\simeq y/\pi$ for small $y$ ). By computing the integral numerically I learn that $$
\lim_{y\to \infty}f(y) = 1 \, ,
$$ so I am interested in its asymptotic behavior for large $y$ , like $$f(y)\simeq 1-g(y) \, ,$$ for some function $g(y)$ (vanishing in the limit $y\to\infty$ ) which I want to determine. Clearly a blind Taylor expansion does not work, because not all the terms in the series converge. Do you have any suggestion? EDIT: The branch of the $ArcTan$ is chosen so that it returns a number in $[0,\pi]$ . EDIT 2: To further clarify which branch of the $ArcTan$ I am interested in, here I am plotting the integrand $ArcTan$ (without squaring it) for two sample values of $y$ . For $y=1$ , in particular, the argument of $ArcTan$ is always positive, so it returns $\pi/2$ at $u=0$ and then it decays to zero at large $u$ . With this choice, $f(y)$ grows monotonically from $0$ to $1$ , with no peak (and no other surprises) in the middle.","['functions', 'definite-integrals', 'approximation', 'asymptotics']"
4516809,Find all functions for which their inverse equals the derivative,I was wondering how you could find all functions $f$ for which it holds true that $f^{-1} = f'$ . I found the solution $\varphi^{1-\varphi}x^{\varphi}$ for which this holds true. Are there any other solutions and is there a closed form for all solutions for this equation? Thanks in advance.,"['golden-ratio', 'functions', 'derivatives', 'inverse-function']"
4516828,$\{T = A^*A - AA^*: A \in \mathcal B(\mathcal H)\}$ is closed in the norm topology but $\{T = A^*A-AA^*: A \in \mathcal K(\mathcal H)\}$ is not,"I would like to show that: The set of all self-commutators is closed under the norm-topology, while the set of all self-commutators of compact operators is not so. in the setting of infinite-dimensional, separable Hilbert spaces. Recall that an operator $T \in \mathcal B(\mathcal H)$ is said to be a self-commutator if there exists an operator $A \in \mathcal B(\mathcal H)$ such that $$T = [A^*,A] = A^*A- AA^*$$ Also, $[A^*,A]$ is called the self-commutator of $A$ . The norm topology is (clearly) metrizable, and so it is enough to check sequential closedness. For the first claim, i.e., $\{T = A^*A - AA^*: A \in \mathcal B(\mathcal H)\}$ is closed in the norm topology - consider sequences $\{T_n\}_{n\ge 1}$ and $\{A_n\}_{n\ge 1}$ in $\mathcal B(\mathcal H)$ , such that $T_n = [A_n^*,A_n]$ , and $T_n \xrightarrow{n\to\infty} T$ in the norm topology, i.e., $\|T_n - T\| \xrightarrow{n\to\infty} 0$ . What is a suitable candidate $A \in \mathcal B(\mathcal H)$ that may satisfy $T = [A^*,A]$ ? Even though the sequence $\{T_n\}_{n\ge 1}$ is assumed to converge in norm, it is not clear that $\{A_n\}_{n\ge 1}$ converges also. For the second claim, it is enough to find a counterexample, i.e., specific sequences $\{T_n\}_{n\ge 1}$ and $\{A_n\}_{n\ge 1}$ of compact operators, such that $T_n = [A_n^*,A_n]$ such that $T_n \xrightarrow{n\to\infty} T$ in the norm topology, but $T$ is not a self-commutator. I haven't made much progress beyond what's stated above, and I'll appreciate any help. Thanks!","['hilbert-spaces', 'operator-theory', 'functional-analysis', 'analysis']"
4516831,Determinant of certain rank-2 bundle on product of curves,"Let $X_1,X_2\subset\mathbb{P}^n$ be two disjoint smooth projective and irreducible curves. Then we have a $\mathbb{P}^1$ -bundle $B$ on the product $X_1\times X_2$ defined by $$B=\{(p,q,r)\in X_1\times X_2\times \mathbb{P}^{n}\mid r\in\textrm{Span}(p,q)\}.$$ The variety $B$ has natural maps $\pi_1:B\to X_1$ , $\pi_2:B\to X_2$ and $\pi_0:B\to \mathbb{P}^n$ . The preimage of $X_i$ under $\pi_0$ is a  divisor $E_i$ on $B$ for $i=1,2$ . We further have a divisor $D$ on $B$ corresponding to the pull-back of the determinant of the corresponding rank-2 vector bundle on $X_1\times X_2$ . Can we express the divisor class of $D$ in terms of pull-backs of divisors under the $\pi_i$ ( $i=0,1,2$ ) and the divisors $E_1,E_2$ ?","['vector-bundles', 'algebraic-geometry']"
4516840,Is my understanding of limits correct?,"I want to explain the basic concept of limits to see if my understanding is correct or not. If we have a function in the form of a fraction and for a value of $x$ the numerator and denominator $=0$ ,  the graph of the function will have a perforation. In order to find the coördinates of the perforation we can use limits to rewrite the function in a form so that we can put in the value of $x$ that would have resulted in $0$ . Because of the limits we can rewrite, for example, $f(x)= \frac {x^2 - 5x + 6}{x-2}$ = $\frac {(x-3)(x-2)}{x-2}$ to $\displaystyle \lim_{x \to 2} x-3 = -1$ We normally are not allowed to divide by $x-2$ because we do not know if $x-2$ could equal $0$ . In this case we know $x=2$ will result in $0$ , so the limit basically says that we take a $x$ that is very close to $2$ , like $1.9999999999999$ but does not equal $2$ and therefore we can divide by $x-2$ and simplify the function to a form where we can input $x=2$ Please correct me if I am wrong and apologies for some of the formatting, I do not know how to format the limit correctly.",['limits']
4516899,"Am I getting this right about equations, functions and logic?","When I square the equation $ x=2 $ I get the equation $ x^2=4 $ and I would write $$
x=2 \Rightarrow x^2=4.
$$ This implication is true for all $x$ in $\mathbb R$ . In other words, the sentence $$\forall x \in \mathbb R: (x=2 \Rightarrow x^2=4)$$ is true. Of course I could also think of this predicate $$
x^2=4 \Rightarrow x=2 ,
$$ let's call it $A(x)$ . There are numbers which turn $A(x)$ into a true sentence but also numbers which turn $A(x)$ into a false sentence, for example $A(-2)$ is a false sentence . So the sentence $$\forall x \in \mathbb R: (x^2=4 \Rightarrow x=2)$$ is false. The squaring of the equation $ x=2 $ is actually me using the function $h:t\mapsto t^2$ on both sides of $x=2$ . And I know that $x=2 \Rightarrow x^2=4$ is true for all $x$ in $\mathbb R$ because $t_1=t_2 \Rightarrow f(t_1)=f(t_2)$ is true for all $t_1$ and $t_2$ . This $$t_1=t_2 \Rightarrow f(t_1)=f(t_2)$$ is true for all $t_1,t_2$ because that's just what functions do; taking an input and giving out an output. For this $$t_1=t_2 \iff f(t_1)=f(t_2)$$ to be true for all $t_1,t_2$ , the function $f$ has to be injective. So when I'm doing my operations on my equations, I can only use the $ \iff $ arrow when I am applying an injective function on both sides of the equations. A simple example would be $$
x=2 \iff x+x=2+x.
$$ Here I used the injective function $f:t\mapsto t+x$ on both sides of the equation. And because I used this injective function on both sides of the equation I know that the sentence $$
\forall x \in \mathbb R: (x=2 \iff x+x=2+x)
$$ is true. Are all these thoughts correct?","['propositional-calculus', 'functions', 'logic']"
4516944,"Plotting modular forms in SageMath, what does this mean?","I'm in the process of learning modular forms, and I thought it would be fun to visualise them. I stumbled upon the great David Lowry-Duda and now I would like to produce similar plots of modular forms as he does. That being said, I have a snip of code in SageMath and I was wondering if it would be possible to explain what's going on in each row. The code sniplet is from Lowry-Duda's website and a similar code can be found in this post. Htoq = lambda x: exp(2*CDF.pi()*CDF.0*x) 
DtoH = lambda x: (-CDF.0*x + 1)/(x - CDF.0)
Dtoq = lambda x: Htoq(DtoH(CDF(x)))

f = ModularForms(group=1, weight=12).newforms()[0].q_expansion(100).truncate() # what is newforms and q_expansion? Why do we want that?
P = ccomplex_plot(lambda x: +Infinity if abs(x) >= 0.99 else f(Dtoq(x)), (-1,1),(-1,1),
             plot_points=500, aspect_ratio = 1, figsize=[5,5]) What I've understood, this plots a Poincare disk (because of the lambda x: +Infinity if abs(x) >= 0.99 else f(Dtoq(x)) ?). But I'd appreciate anyone that could walk me through the code as I don't have anyone with knowledge in this around me.","['number-theory', 'modular-forms', 'graphing-functions', 'sagemath']"
4516963,How to prove $\lim\limits_{x \to \infty} \frac{\pi\sqrt{x}}{\sin{(\frac{\pi}{\sqrt{x}} })} - x = \frac{\pi^2}{6}$?,"As we know, the result of some expressions and series is equal to $\frac{\pi^2}{6} $ that the most important of them is $\zeta(2)$ Now, I have founded an equation whose limit at point $x=\infty$ is equal to $\frac{\pi^2}{6} : $ $$\lim_{x \to \infty}  \frac{\pi\sqrt{x}}{\sin{(\frac{\pi}{\sqrt{x}}  })} - x = \frac{\pi^2}{6}$$ I want to know how can we prove it?","['riemann-zeta', 'limits', 'constants', 'pi']"
4516964,Why do we need to include the factor $a_{n}^{2n-2}$ in the discriminant of a polynomial?,"Question: Why do we need to include the factor $a_{n}^{2n-2}$ in the discriminant of a polynomial? Here is the definition of the discriminant ( $\Delta$ ) in terms of the roots $r_1,r_2,...$ : $$ \Delta=a_{n}^{2n-2}\prod_{i<j}(r_i-r_j)^2 $$ for the polynomial $a_nx^n+a_{n-1}x^{n-1}+...+a_0$ . If the polynomial equation has non-real coefficients (in particular if $a_n$ is not real), then it is pointless to tell whether $\Delta>0$ and so in general I can't tell the number of real roots using this way for a polynomial equation with non-real coefficients. However, for a polynomial equation with only real coefficients, even if we just consider $\prod_{i<j}(r_i-r_j)^2$ , where $a_{n}^{2n-2}$ is not included, we still have: $$\Delta >0, \text{if the number of complex roots} \equiv 0\mod 4$$ $$\Delta =0, \text{if there is a multiple root}$$ $$\Delta<0, \text{otherwise}$$ So, why isn't this the definition of discriminant? What difference does the factor $a_{n}^{2n-2}$ make in the non-real coefficients case? Any help will be appreciated!","['discriminant', 'real-analysis', 'complex-analysis', 'polynomials', 'algebra-precalculus']"
4516986,Need explanation for generalization of the associative law for sets,"I am currently reading Naive Set Theory by P. R. Halmos and I don't seem to understand the notation he uses to generalize the associative law for sets: ∪ k∈K A k = ∪ j∈J (∪ i∈I j A i ). I understand the left-hand side, and of the right-hand side only all thats included in the brackets. I don't seem to connect what the union outside the brackets of the RHS means. Perhaps I have met the notation before, and maybe I'm just not seeing the picture, but I've been looking at this formula for a long time now.
Thank you in advance.
​","['predicate-logic', 'logic', 'discrete-mathematics', 'elementary-set-theory', 'set-theory']"
4516999,Is there a formula for $\nabla^n(fg)$?,"Applying the ordinary Leibniz rule multiple times leads to the general Leibniz rule for $n$ th derivatives of $fg$ , namely $(fg)^{(n)}=\sum_{k=0}^n\binom{n}{k}f^{(n-k)}g^{(k)}$ . Is there an analogous formula for repeated gradients/divergences of a product of scalar functions? That is, $\nabla^n(fg)=$ ? Of course $\nabla^nf$ is understood to mean $\cdots\nabla(\nabla\cdot(\nabla f))$ , with $\nabla$ appearing $n$ times, either as a gradient or divergence according to what makes sense. While some identities involving $\nabla$ look very similar to the Leibniz rule, the problem is that $\nabla(X\cdot Y)\neq(\nabla X)Y+X\nabla Y$ . (If equality held here, I think a form of the generalized Leibniz rule would follow easily.) Let's agree to keep expressions like this unevaluated, so that no curls appear, nor any other operations other than gradients and divergences. I.e., the final formula can (and probably must) contain expressions like $\nabla[(\nabla f)\cdot(\nabla g)]$ . That said, if there is an alternative notation (e.g., tensor indices) that makes the formula easier to write, that would also be fine.","['divergence-operator', 'multivariable-calculus', 'derivatives', 'vector-analysis']"
4517005,Finding the equation of a plane given three points,"Below is a problem I did from a Calculus text book. My answer matches the
back of the book and I believe my answer is right. However, the method
I used is something I made up. That is, it is not the method described
in the text book. Is my method correct? Problem: Find the plane through the points $(1,1,-1)$ , $(2,0,2)$ and $(0,-2,1)$ . Answer: The general form of a plane is: $$ Ax + By + Cz = D$$ Sometimes the following constrain is added: $$ A^2 + B^2 + C^2 = 1$$ By inspection, we can see this plane is not parallel to the x-axis, the y-axis or the z-axis. Hence,
we can assume that the plane is of the form: $$ Ax + By + Cz = 1 $$ Now we setup the following system of linear equations. \begin{align*}
A + B - C &= 1 \\
2A + 2C &= 1 \\
-2B + C &= 1 \\
\end{align*} To solve this system of equations, we get rid of $A$ and $B$ in the first equation. \begin{align*}
2A &= 1 - 2C \\
A &= \frac{ 1 - 2C }{2} \\
-2B &= 1 - C \\
B &= \frac{ C - 1 }{2} \\
\left( \frac{ 1 - 2C }{2} \right) + \left( \frac{ C - 1 }{2} \right)  - C &= 1 \\
1 - 2C + C - 1 - 2C &= 2 \\
- 2C + C - 2C &= 2 \\
-3C &= 2 \\
C &= -\frac{2}{3} \\
B &= \frac{ -\frac{2}{3} - 1 }{2} = -\frac{2}{6} - \frac{1}{2} \\
B &= -\frac{5}{6} \\
A &= \frac{ 1 - 2\left(  -\frac{2}{3} \right)  }{2} = \dfrac{1 + \dfrac{4}{3} }{2} \\
A &= \dfrac{7}{6}
\end{align*} Hence the equation is: $$ \left( \dfrac{7}{6} \right) A + \left( -\frac{5}{6} \right) B + \left(  -\frac{2}{3} \right) C = 1  $$ Clearing the fraction, we get the final answer of: $$ 7A - 5B - 4C = 6 $$ As pointed out by Paul, the correct answer is: $$ 7x - 5y - 4z = 6 $$","['multivariable-calculus', 'solution-verification', 'geometry']"
4517024,Properties within a single chart on a topological manifold and smoothness,"Reading on Wikipedia it is stated that A topological manifold looks locally like a Euclidean space in a
rather weak manner: while for each individual chart it is possible to
distinguish differentiable functions or measure distances and angles,
merely by virtue of being a topological manifold a space does not have
any particular and consistent choice of such concepts.[7] Within a single chart, is there a well-defined concept of distance? Further, if this is the case, what other structures, that would otherwise be traditionally be added to a topological manifold, like a symplectic structure, is present in a single chart? As a second question, I am trying to understand exactly what goes wrong for performing calculus when the transition maps of a manifold are not smooth? i.e what exactly does the smoothness of transition maps bring? [7] Kervaire, M. (1961). ""A Manifold which does not admit any differentiable structure"". Comment. Math. Helv. 35 (1): 1–14. doi:10.1007/BF02565940. S2CID 120977898.","['differential-topology', 'differential-geometry']"
4517029,Does there exist a finite set of solutions to integrals such that any function composed of elementary functions is integrable?,"For indefinite integrals whose solutions cannot express with elementary functions, special functions are often defined, such as those shown below. $$ \mathrm{Si}(x) = \int_0^x\!\frac{\sin t}{t}\,\mathrm{d}t \qquad \mathrm{Li}(x) = \int_0^x\!\frac{1}{\ln t}\,\mathrm{d}t \qquad \mathrm{S}(x) = \int_0^x\!\sin(t^2)\,\mathrm{d}t $$ Through definining special functions as non-elementary integrals there exist cases where other functions also become integrable in terms of these newly defined special functions. An example is shown for the hyperbolic sine integral, whose solution is often given as a special function; however, this special function can be given in terms of the sine integral. $$ \mathrm{Shi}(x) = \int_0^x\!\frac{\sinh t}{t}\,\mathrm{d}t = \frac{\mathrm{Si}(ix)}{i} $$ Let $E$ be the set of all possible functions which can be created through combinations of the elementary functions. Examples are shown below for members of the set $E$ . $$ \frac{\sin x}{e^x} \qquad e^{x^2}\ln x \qquad \cosh\left(\frac{x^2 + 1}{\log_{10}(x)}\right)$$ Does there exist a finite set $S$ of defined solutions to non-elementary integrals, such that any function in the set $E$ can have its indefinite integral expressed as a combination of the functions in $S$ and the elementary functions?","['integration', 'galois-theory', 'elementary-functions', 'indefinite-integrals', 'differential-field']"
4517051,"Minimum spanning forest, where each tree has the same number of vertices.","Given a connected Graph $G(V,E)$ with weights $w\colon E\to\mathbb{N}$ and $|V|=kn$ . How can I find the minimum spanning forest $T_1,T_2, \dots, T_n$ where each tree $T_i$ has exactly $k$ vertices? I wonder if this problem is P or NP, I am particularly interested in the case $k=3$ .","['graph-theory', 'np-complete', 'combinatorics', 'computational-complexity', 'decision-problems']"
4517087,$g(x) = \lim \limits_{t \to x}f(t)$. Show $g$ is continuous on $\mathbb{R}$.,"Let the function $f : \mathbb{R} \to \mathbb{R}$ , $\exists \lim \limits_{x \to c}f(x)$ for all $c\in\mathbb{R}$ . Define $g : \mathbb{R} \to \mathbb{R}$ by $g(x) = \lim \limits_{t \to x}f(t)$ . Show $g$ is continuous on $\mathbb{R}$ . I tried the different proof comparing the method solution suggested. From here my proof begins. $(pf)$ Fixed $\forall c \in \mathbb{R}$ , Say $\lim \limits_{x \to c}f(x) = g(c)$ . By definition,
For all $\epsilon >0$ , $\exists \delta >0$ $s.t.$ $\forall x(0 < \vert x-c \vert <\delta  \Rightarrow \vert f(x) - g(c) \vert < \epsilon )$ ( $*$ ). Considering the $ I = \{x \vert \vert x-c \vert <\delta \}$ $\vert g(x)-g(c) \vert = \vert \lim \limits_{t \to x}f(t) - \lim \limits_{t \to x}g(c) \vert = \vert \lim \limits_{t \to x}(f(t)-g(c)) \vert <\epsilon$ , $\forall x \in I$ by the $(*)$ Hence $g$ is continuous at $\forall c\in \mathbb{R}$ (In other words, $g$ is continuous on $\mathbb{R}$ ) Is my proof right? If not, Please let me know which point I made mistakes. Regards.","['limits', 'solution-verification', 'real-analysis']"
4517102,Gelfands Trigonometry $\sin(\alpha - \beta) = \sin \alpha \cos \beta - \sin \beta \cos \alpha$,"Trying Prove the identity $\sin(\alpha - \beta) = \sin \alpha \cos \beta - \sin \beta \cos \alpha$ using the figure provided in Gelfands trigonometry. What I have so far $\sin(\alpha - \beta) = \frac{CD}{AC} = \frac{PQ}{AC} = \frac{BQ}{AC} - \frac{BP}{AC}$ $\sin(\alpha) = \frac{BQ}{AB} \implies AB\sin(\alpha) = BQ$ $\sin(\alpha - \beta) = \frac{AB\sin(\alpha)}{AC} - \frac{BP}{AC}$ $\frac{AB}{AC} = \frac{1}{\cos(\beta)}$ #corrected $\sin(\alpha - \beta) = \frac{\sin(\alpha)}{\cos(\beta)} - \frac{BP}{AC}$ # corrected Im stuck on what to do with $\frac{BP}{AC}$ . I've seen the posts here about the derivation of $\sin(\alpha + \beta)$ from the same diagram and I understand that proof perfectly well, but I am stuck on this one.",['trigonometry']
4517133,Is taylor series also an orthogonal projection of a infinitely differentiable function on some subspace?,"I'm wondering if there exists some inner product $\langle \cdot,\cdot \rangle$ defined on all real infinitely differentiable functions such that $$1, x, x^2, x^3, \ldots$$ are orthonormal w.r.t this inner product? If there does exist such an inner product, denote $$U_j=\text{span}(1,x,\ldots,x^j).$$ Then, is it true that, for an arbitrary infinitely differentiable function $f$ , $P_{U_j}(f)$ is $f$ 's $j$ th order taylor expansion at $x=0$ ?","['orthonormal', 'analysis', 'linear-algebra', 'taylor-expansion', 'functional-analysis']"
4517141,Using the divergence theorem to prove that $\frac{1}{|B_R(0)|} \int_{B_R(0)} M \textbf{y} . \textbf{y} dy = \frac{R^2}{ n + 2} \text{trace}(M)$,Here is the question I am trying to solve letter $(b)$ of it: Here is a solution to it: Which is a very very long solution. Does anyone have a more elegant and succinct solution please?,"['divergence-theorem', 'analysis', 'real-analysis', 'matrices', 'multivariable-calculus']"
4517151,Group cohomology of the product of cyclic groups,"Let $m \geq 2$ be a positive integer and $C_m$ be a finite cyclic group of order $m$ . My question : How to compute the cohomology groups $H^i(C_m \times C_n, A)$ for $i > 0$ in the following two cases: $A = \mathbb{Z}$ is a trivial $C_m \times C_n$ -module, $m=n$ is even and $A = \mathbb{Z}$ with an action of $C_m$ given by $k \cdot z = (-1)^k z$ . EDIT Sept. 14 : I have almost figure out the result via Approach 4 , after having been collaborating with my roommate, by taking a particular resolution. I shall sort them out in a few weeks and post it as an answer. Thank you all for your attention! My attempts : I have obtained a bunch of partial results, but failed to the general result. Attempt 1 : (Kunneth formula) In the first case when $A=\mathbb{Z}$ is trivial, one may apply the Kunneth formula to get the cohomology group, but the discussion is too tedious, full of mess with parity on $m$ and $n$ . More unsatisfactory to me is that the Kunneth's formula in my mind is the Exercise 6.1.8 in Weibel's book: Kunneth's formula : There is a split short exact sequence $$
0 \rightarrow \bigoplus_{p+q=n} H^p(G,\mathbb{Z}) \otimes H^q(H,\mathbb{Z}) \rightarrow H^n(G \times H, \mathbb{Z}) \rightarrow \bigoplus_{p+q=n+1} \mathrm{Tor}_{1}^{\mathbb{Z}}(H^p(G,\mathbb{Z}),H^q(H,\mathbb{Z})).
$$ where $\mathbb{Z}$ above are trivial modules. , which only treats the trivial coefficient case. Attempt 2 : (Hochschild-Serre spectral sequence) Taking $G=C_m \times C_n$ and $H=C_n$ , we have the convergent spectral sequence $$
\mathcal{E}: E_2^{p,q} = H^p(C_m, H^q(C_n, \mathbb{Z})) \Rightarrow H^{p+q}(C_m \times C_m, \mathbb{Z}).
$$ Since $G$ is an abelian group and $\mathbb{Z}$ is a trivial module, the conjugation actions of $G/H \cong C_m$ on the cohomology groups $H^q(C_n, \mathbb{Z})$ are trivial. So we explicitly compute the $E_2$ -page as $$
\begin{matrix}
 \mathbb{Z}/n & 0 & \mathbb{Z}/d & 0 & \mathbb{Z}/d & \cdots \\
 0 & 0 & 0 & 0 & 0 & \cdots \\
 \mathbb{Z}/n & 0 & \mathbb{Z}/d & 0 & \mathbb{Z}/d & \cdots \\
 0 & 0 & 0 & 0 & 0 & \cdots \\
E_2^{0,0}=\mathbb{Z} & 0 & \mathbb{Z}/m & 0 & \mathbb{Z}/m & \cdots \\
\end{matrix}
$$ Then $E_{\infty}^{p,q} = E_{2}^{p,q}$ by carefully checking the direction of arrows. Then we know directly: $H^0(C_m \times C_n, \mathbb{Z}) = \mathbb{Z}$ , $H^{\text{odd}}(C_m \times C_n, \mathbb{Z}) = 0$ . But for cohomology group of even degree, I can only obtain a filtration of the cohomology: $$
H^{2i}(C_m \times C_n, \mathbb{Z}) \supseteq \ast \supseteq \ast \cdots \supseteq \ast = 0
$$ where the subquotients are $\mathbb{Z}/m$ , $\mathbb{Z}/d$ for $(i-1)$ times and one $\mathbb{Z}/n$ . Then if everything splits , then $$
H^{2i}(C_m \times C_n, \mathbb{Z}) = \mathbb{Z}/m \oplus (\mathbb{Z}/d)^{i-1} \oplus \mathbb{Z}/n.
$$ But how can I see the splitting of this filtration ? Attempt 3 : (Inflation-restriction exact sequence) Actually the inf-res sequence is derived from the above spectral sequence, yet I still tried to gain something from it. We can only use the $n=1$ version of the inf-res sequence, which gives $$
0 \rightarrow H^1(C_n, \mathbb{Z}) \xrightarrow{\mathrm{inf}} H^1(C_m \times C_n, \mathbb{Z}) \xrightarrow{\mathrm{res}} H^1(C_m, \mathbb{Z})^{C_n} \xrightarrow{\mathrm{transgression}} H^2(C_n, \mathbb{Z}) \rightarrow \cdots.
$$ This turns into $$
0 \rightarrow 0 \xrightarrow{\mathrm{inf}} H^1(C_m \times C_n, \mathbb{Z}) \xrightarrow{\mathrm{res}} 0 \xrightarrow{\mathrm{transgression}} 0.
$$ hence $$
H^1(C_m \times C_n, \mathbb{Z}) \cong 0.
$$ This can be seen from the spectral sequence above. So unfortunately, I got nothing new. ( EDIT : I have corrected the calculation here, as I mixed up the homology and cohomology of cyclic groups) These are all my attempts by now, yet unfortunately they all fail to give a complete result, or even a promising method to treat the second case where $\mathbb{Z}$ is not a trivial module. So is there any way to settle down these cohomology groups? Or could someone provide some reference on the result of the cohomology group? Thank you so much for answering and commenting! :) EDIT : The comment by Dietrich Burde is quite useful, but I would like to see a more direct approach since this question only appears in a final exam of an elementary course on homological algebra, so it wouldn't require so much machinery that hard to establish during a two-hour exam. So Approach 4 : can we create an explicit resolution to do the job? Yet the standard resolution is quite complicated. :( Here I found a resolution provided by Sasha: https://mathoverflow.net/questions/36730/describe-the-second-cohomology-group-h2z-n-times-z-n-k . But I cannot found the pattern for the general term of the resolution.","['homological-algebra', 'spectral-sequences', 'abstract-algebra', 'group-cohomology']"
4517218,Fisher information of product model,"Consider the following regular statistical model: $(\mathbb{R}^n, \mathcal{B}(\mathbb{R}^n), \mathbb{P}_{\theta}: \theta \in \mathbb{R}_+ )$ . Assume $\mathbb{P}_{\theta}$ has the density $$\rho(\theta,.) = \frac{1}{\theta} f\left(\frac{x}{\theta}\right)$$ , where $f(x)>0$ and $f'(x)$ exists. I want to show that the fisher- information is given by: $$I(\theta) = \int_{\mathbb{R}}\frac{n}{\theta^2} \frac{(xf'(x)+f(x))^2}{f(x)} dx $$ Therefore I have to compute $$E[U_{\theta}^2]$$ $U_{\theta}$ is the score, which is given by: $$U_{\theta} = \partial_{\theta} ln(\rho(\theta,x))$$ for $x \in \mathbb{R}^n$ Then we have: $$\partial_{\theta}  ln(\prod_{i=1}^n \rho(\theta,x_i)) = \sum_{i=1}^n\partial_{\theta}  \frac{1}{\theta} f\left(\frac{x_i}{\theta}\right) = 
 \sum_{i=1}^n  \frac{\frac{-1}{\theta^2}f\left(\frac{x_i}{\theta}\right)+ \frac{1}{\theta}f'\left(\frac{x_i}{\theta}\right)\frac{-x_i}{\theta^2}}{\frac{1}{\theta}f\left(\frac{x_i}{\theta}\right)}$$ How can I go on from there. How can I make it independent from $i$ as the solution suggests to compute $$E[U_{\theta}^2]$$","['expected-value', 'fisher-information', 'statistics', 'density-function']"
4517287,"When should continued fraction expansions which start from an original value, converge to that same value? $\tan x=x/1-x^2/3-x^2/5-\dots$","$\newcommand{\K}{\operatorname{\large\mathcal{K}}}$ I am asking about a very common practice in proofs, that I see online, concerning continued fractions. There is an implicit assumption which I’d like to examine: a concrete, concise question can be found in the middle section, but I think a lot of context is necessary. Here is what I mean: sometimes, when dealing with continued fractions, we begin with the answer , and then recursively expand the answer, ad infinitum, implicitly assuming that the limit will be the same. I occasionally edit this post to add new thoughts, and sufficient conditions (a necessary one would be very well-received!) in the middle section. To clarify, here is a trivial example: $$\begin{align}1&=\cfrac{2}{3-1}\\&=\cfrac{2}{3-\cfrac{2}{3-1}}\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-1}}}\\&\overset{?}{=}\cdots\\&=\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}}\end{align}$$ We started with $1$ , then expressed it as a ratio $\frac{b_1}{a_1+\alpha_1}$ . We then expanded $\alpha_1$ as a ratio $\frac{b_2}{a_2+\alpha_2}$ ... et cetera. We can show that (Gauss Kettenbrücher K-notation, similar to $\sum,\prod$ notation) $\K_{n=1}^\infty\frac{b_n}{a_n}$ is convergent, and because we obtained this continued fraction from a sequence of expressions, all equal to $1$ , we have reason to believe that the limiting fraction is also equal to $1$ . However, in so doing we lose the helper $\alpha_n$ terms, so I feel, a priori , we cannot be certain that the limit is actually $1$ . It is, as it happens... the convergents are $-\K_{n=1}^m\frac{-2}{3}=1-\frac{1}{2^{m+1}-1}$ , which obviously tend to $1$ . But I ask about the general procedure. For example, Wikipedia’s article on Gauss’ continued fraction uses recurring expansions, just like how we expanded “ $1$ ” above, to derive continued fractions for ratios of hypergeometric functions. I like this derivation, but there is always the concern that, even if the continued fraction is convergent, the limiting value might not be the value we started with! A simple example to demonstrate this concern: in the above expansion of $1$ , we could just as easily write: $$\begin{align}2&=\frac{2}{3-2}\\&=\cfrac{2}{3-\cfrac{2}{3-2}}\\&=\cdots\\&\overset{!?}{=}\cfrac{2}{3-\cfrac{2}{3-\cfrac{2}{3-\ddots}}}\end{align}$$ But we already know that that converges to $1$ , not to $2$ ! The core question. My analysis, and a concrete setup: Let $L$ be any nonzero real number. Suppose we find a sequence of nonzero reals $(a_k),(b_k),(\alpha_k)$ with the property that, for all $n\in\Bbb N$ : $$L=\K_{k=1}^n\frac{b_k}{a’_k}$$ Where $a’_k=a_k$ except for $a’_n:=\alpha_n$ . Suppose further that: $$\lim_{n\to\infty}\K_{k=1}^n\frac{b_k}{a_k}=L’\in\Bbb R\setminus\{0\}$$ That is, the continued fraction converges. We want to know if $L’=L$ . If $p_n,q_n$ are the numerators and denominators of the $n$ th convergent $\K_{k=1}^n\frac{b_k}{a_k}$ (with no simplification!) then we know that (writing $\alpha_n=\alpha_n/1$ ): $$L=\frac{p_n+\alpha_np_{n-1}}{q_n+\alpha_nq_{n-1}}$$ For every $n$ . The difference $|L-L’|$ is equal to: $$\left|\frac{q_n\left(\frac{p_n}{q_n}-L’\right)+\alpha_nq_{n-1}\left(\frac{p_{n-1}}{q_{n-1}}-L’\right)}{q_n+\alpha_nq_{n-1}}\right|=\left|\left(\frac{p_n}{q_n}-L’\right)+\cfrac{1}{1+\cfrac{q_n}{\alpha_nq_{n-1}}}\left(\frac{p_{n-1}}{q_{n-1}}-\frac{p_n}{q_n}\right)\right|$$ For every $n$ . Thus I can hope to send $n\to\infty$ - so long as $\frac{q_n}{\alpha_nq_{n-1}}$ is bounded away from $-1$ , it is easy to see all terms tend to zero, whence $L=L’$ . However, there’s the rub - can we necessarily say that there is some $\delta>0$ , that for any $n\in\Bbb N$ , $\left|\frac{q_n}{\alpha_nq_{n-1}}+1\right|>\delta$ ? One quick observation is that, if all $(a_k),(b_k),(\alpha_k)$ are positive, then the answer is a definite: ‘yes’. Another observation, inspired by the partial answer below, is that I really only need to find a subsequence $(n_k)$ along which $\frac{q_{n_k}}{\alpha_{n_k}q_{n_k-1}}$ is bounded away from $-1$ . Note: if no such subsequence can be found, that actually implies the stronger assertion that: $\lim_{n\to\infty}\alpha_n\frac{q_{n-1}}{q_n}=-1$ as a strong limit. Furthermore, we see that, in order for $L$ to be finite and well defined, if $q_n/\alpha_n q_{n-1}$ comes arbitrarily close to $-1$ , then $\frac{\alpha_n p_{n-1}}{q_n}$ comes arbitrarily close to $-L’$ . I’ve tried using that to obtain a contradiction, but I’ve had no such luck yet. Furthermore, $\alpha_np_{n-1}/p_n$ would also tend to $-1$ . A new observation: if $\alpha_n\to0$ , as happens in the case of Lambert’s tangent continued fraction, and if one can be sure that $|q_n|\ge|q_{n-1}|$ for large $n$ , or at least along a subsequence (more generally, we only need to assume the ratio $q_{n-1}/q_n$ is bounded) - which is often the case - then $\alpha_n q_{n-1}/q_n$ tends to $0$ , and is in particular bounded away from $-1$ . This is not a necessary condition, since my trivial example involving $1=3/2-\cdots$ has $\alpha_n=-1$ for all $n$ . If the $\alpha_n$ are constant, then so long as we can say $|q_n|$ is sufficiently larger than $|q_{n-1}|$ (eventually, perhaps along a subsequence) then it will also be possible to bound away from $-1$ (more generally, the condition would be the unwieldy: “ $q_{n-1}/q_n$ is bounded away from the constant $-1/\alpha_n$ ”). See the counterexample expansion of $2$ : $-1/\alpha=\frac{1}{2}$ in this case, and $1/2$ is precisely the limiting ratio of $\frac{q_{n-1}}{q_n}$ , explaining why convergence fails. I don’t however think that this is anywhere near a complete, or usefully general, list. There is surely more to this story. My question : can we say more about the general case? I’d be happy to see relatively simple, easy-to-apply conditions, other than the conditions I’ve already supplied. As I have shown, and more recently G. Edgar has shown, counterexamples do exist. A less trivial, and more motivating, example: through some series manipulations and divisions of the Maclaurin expansions for $\sin,\cos$ , we can find: $$\tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{P_1/P_2}}}$$ Where: $$P_m:=\sum_{n=0}^\infty(-1)^n\frac{x^{2n}}{(2(n+m)+1)!}\prod_{k=1}^m2(n+k)$$ It can be shown that, using identical manipulations: $$\frac{P_m}{P_{m+1}}=(2m+3)-\frac{x^2}{P_{m+1}/P_{m+2}}$$ So, inductively, a continued fraction for $\tan x$ is born: $$\tan x=\cfrac{x}{1-\cfrac{x^2}{3-\cfrac{x^2}{5-\cfrac{x^2}{7-\ddots}}}}$$ This continued fraction can be shown to converge - but does it necessarily converge to $\tan x$ ? The concern being, again, that we have lost the helper $\alpha_m:=-x^2/(P_m/P_{m+1})$ terms.","['limits', 'continued-fractions', 'sequences-and-series', 'real-analysis']"
4517305,Evaluate the following humongous expression,PROBLEM : Evaluate $$\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2$$ CONTEXT : I saw a very interesting and yet intimidating question on the internet: Find the value of $$\frac{16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy\:\:\left(\frac{\displaystyle\sum_{n=-\infty}^{\infty}\frac{1}{1+n^2}}{\operatorname{coth}(\pi)}\right)^2}{\displaystyle\sum_{n=1}^{\infty}\frac{1}{n^2}}+5$$ I just know or rather heard that (though I don't know the proof) $$\sum_{n=1}^{\infty}\frac{1}{n^2}=\frac{{\pi}^2}{6}$$ and (I calculated it) $$16\displaystyle\int_0^\pi\int_0^1x^2\cdot\operatorname{sin}(y)\:\:dxdy=\frac{32}{3}$$ but I can't calculate the value of the expression written in the big brackets. Any help is greatly appreciated.,"['integration', 'summation', 'definite-integrals', 'real-analysis', 'multivariable-calculus']"
4517341,In how many ways can 3 distinct numbers fill 6 blanks (each repeated exactly twice)?,"I have a basic knowledge of permutations and combinations but using different approaches to this problem lead me to different answers.
Basically, there are 6 blanks and 3 distinct digits a,b, and c. All three of them occupy 2 blanks. For example, some possible combinations are: a a b b c c a b c a b c a b c c b a Please mention the approach used and why the approach is logical and the others are not. Edit: My Approaches:- We can first fill the 2 a's in any of the 6 blanks using 6C2 (as both are same). Then we can fill the 2 b's in any of the the remaining 4 blanks using 4C2. Then we can multiply them (there is no need to fill up c's as 2C2 is 1). The answer comes out to be 90. We can take the a's ,b's and c's as a1, a2, b1, b2, c1, c2 and find the number of permutations. That comes out to be 6P6 = 720. Since we a1 and a2 are same(similarly b1 and b2), we divide by 2 to get 360 (avoiding repetition). Edit2:
Upon seeing the comments, I realized that in approach2: a1 a2 b1 b2 c1 c2 a2 a1 b1 b2 c1 c2 a1 a2 b2 b1 c1 c2 a2 a1 b2 b1 c1 c2 a1 a2 b1 b2 c2 c1 a2 a1 b1 b2 c2 c1 a2 a1 b2 b1 c2 c1 a1 a2 b2 b1 c2 c1 are same. So, I should divide by 8 to get 90. So, is 90 the correct answer?","['permutations', 'combinations', 'combinatorics', 'decimal-expansion']"
4517347,Planar quadrangulations with only non-convex faces,"Do there exist planar quadrangulations that only consist of non-convex quadrilaterals? If so, do these types of quadrangulations have any special properties? I can only think of one example of this type of a quadrangulation, and it has only two faces and five vertices. When going past this, does a quadrangulation necessarily always have at least one convex face?","['graph-theory', 'combinatorics', 'geometry', 'discrete-mathematics']"
4517370,Length of shortest hamiltonian path in a circle,"Let's say I have a circle of radius $r$ . I will place $N$ points inside this circle, and then find the shortest hamiltonian path going through all these points. Of course, I know that this shortest hamiltonain path will be shorter than $2 r\cdot N$ But is there a way to have a better upper-bound ? Are there research papers about this kind of problem ? Edit The problem can be stated as follow: let $D = \{z \in \mathbb{C}, |z| \leq 1\}$ $$u_n = \sup_{(p_i)\in D^n}(\min_{\sigma \in S_n}\left(\sum_{k=1}^n|p_{\sigma(i+1)}-p_{\sigma(i)}|\right))$$ Find the best possible bound for $u_n$","['geometry', 'hamiltonian-path', 'combinatorial-geometry', 'upper-lower-bounds', 'optimization']"
4517392,Why do we need the Hartman-Grobman theorem & the Stable Manifold Theorem to prove that any sink is asymptotically stable & source/saddle is unstable?,"I am reading Perko's book on Differential Equations and Dynamical Systems (3e) and I have the following question: Why do we need the Hartman-Grobman theorem and the Stable Manifold Theorem to prove that any sink is asymptotically stable and source/saddle is unstable? Why is Hartman-Grobman not enough? The passage in question is on p.130: The following is said earlier, which I think makes implicitly use of Hartman-Grobman: Many thanks in advance! Attempt of an answer: Sink: If we have a sink, then the eigenvalues of $Df(x_{0})$ are all less than zero. To show asymptotical stability I would apply the diffeomorphism from H-G-thm, i.e. $$\lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||.$$ Then (hopefully) by continuity we have $$\lim_{t\to\infty}||\Phi_{t}(x)-x_{0}||=\lim_{t\to\infty}||H^{-1}\circ e^{At}H(x)-x_{0}||=0,$$ since $\lim_{t\to\infty}e^{At}H(x)=H(x_{0})$ . Saddle/Source: If we have a saddle then there are eigenvalues with positive and negative real part. Hence, $E^{s},E^{u}\neq\emptyset$ . By the Stable Manifold Theorem it follows that $W^{s},W^{u}\neq\emptyset$ , too, since they are of the same dimension as $E^{s},E^{u}$ respectively.  Hence, the stable manifold guarantees the existence of a trajectory that leaves any $B_{\epsilon}(x_{0})$ . But then we can not have stability. I hope it makes sense. Theorems and Definitions:","['ordinary-differential-equations', 'dynamical-systems']"
4517429,Sum of independent Gamma distributions is a Gamma or a normal distribution?,"Let $ {\textstyle \{X_{1},\ldots ,X_{n},\ldots \}}$ be a sequence of independent random variables, each of those random variable follow a Gamma distribution. For the summation of those random variable: $ {\displaystyle {\bar {X}}_{n}\equiv {\frac {X_{1}+\cdots +X_{n}}{n}}}$ Question: Is the summation of independent Gamma distributions a Gamma distribution or a normal distribution ? Here is the confusion: the summation of Gamma distribution should still be a Gamma distribution. On the other hand, central limit theorem says that such summation should approach a normal distribution. which of those viewpoint is correct ?","['probability', 'random-variables']"
4517447,Why does the Laurent series require negative powers in order to represent complex functions?,"I briefly studied Laurent Series during a Complex Analysis course and was told that they are a generalisation of the Taylor Series. The intuition behind why the Taylor Series can represent any smooth function is clear to me, though I do not understand why the Laurent Series requires negative powers to represent complex functions and I don't believe this was ever explained to us. Any assistance would be great!","['complex-analysis', 'functions', 'laurent-series']"
4517467,Maximal spectrum as affine scheme,"Let $A$ be a ring and let $X=M{\rm Spec}(A) \subset{\rm Spec}(A)$ be the set of maximal ideals of $A$ with the induced Zariski topology. Can $X$ be an affine scheme? That is, can we find a ring $B$ such that $X$ is homeomorphic to ${\rm Spec}(B)$ ? We know that this is true when $X$ is a closed subset, but what about in general? Can we find any resrtictions of $A$ such that $X$ would be an affine scheme?","['commutative-algebra', 'affine-schemes', 'algebraic-geometry', 'general-topology', 'schemes']"
4517511,How to derive the equation for geodesic deviation starting from the geodesic equation?,"A family of geodesics can be parametrised $x^a =  x^a(s, t)$ where $s$ is the distance along a geodesic and the parameter $t$ speciﬁes the geodesic. For each $t$ the geodesic equation is $$\frac{\partial^2 x^a}{\partial s^2}+\Gamma_{bc}^a\frac{\partial x^b}{\partial s}\frac{\partial x^c}{\partial s}=0\tag{1}$$ Partially diﬀerentiate this equation with respect to $t$ to obtain the equation of geodesic deviation $$\frac{D^2 w^a}{\partial s^2}=-R_{bcd}^a u^bw^cu^d\tag{2}$$ where $u^a = \partial x^a/\partial s\,$ and $w^a=\partial x^a/\partial t$ . So taking the time derivative of $(1)$ gives $$\frac{\partial}{\partial t}\left(\frac{\partial u^a}{\partial s}\right)+\left(\frac{\partial}{\partial t}\Gamma_{bc}^a\right)u^bu^c+\Gamma_{bc}^a\frac{\partial u^b}{\partial t}u^c+\Gamma_{bc}^au^b\frac{\partial u^c}{\partial t}=0\tag{a}$$ But right away I feel totally stuck as I don't understand what to do next in order to reach the desired expression, $(2)$ . Could someone please provide hints or tips on how to proceed? Just for reference, I have typeset the full solution as given by the author below: Differentiating the geodesic equation with respect to $t$ $$\frac{\partial^2 w^a}{\partial s^2}+\partial_d\Gamma_{bc}^aw^du^bu^c+2\Gamma_{bc}^a\frac{\partial w^b}{\partial s}u^c=0\tag{3}$$ Now $$\frac{D^2w^a}{\partial s^2}=\frac{\partial}{\partial s}\frac{Dw^a}{\partial s}+\Gamma_{bc}^a\frac{D w^b}{\partial s}u^c$$ $$=\frac{\partial}{\partial s}\left(\frac{\partial w^a}{\partial s}
+\Gamma_{bc}^aw^bu^c\right)+\Gamma_{bc}^a\left(\frac{\partial w^b}{\partial s}+\Gamma_{de}^bw^du^e\right)u^c$$ $$=\frac{\partial^2 w^a}{\partial s^2}+\left(\partial_d\Gamma_{bc}^a\right)u^dw^bu^c+\Gamma_{bc}^aw^b\frac{\partial u^c}{\partial s}+2\Gamma_{bc}^a\frac{\partial w^b}{\partial s}u^c+\Gamma_{bc}^a\Gamma_{de}^bw^du^eu^c$$ $$=\left(\partial_d\Gamma_{bc}^a\right)u^dw^bu^c-\Gamma_{bc}^a\Gamma_{de}^cu^du^ew^b+\Gamma_{bc}^a\Gamma_{de}^bw^du^eu^c+\frac{\partial^2w^a}{\partial s^2}+2\Gamma_{bc}^a\frac{\partial w^b}{\partial s}u^c,$$ using $\partial u^c/\partial s=-\Gamma_{de}^cu^du^e$ . Using $(3)$ this can be written as $$\frac{D^2 w^a}{\partial s^2}=\left(\partial_d\Gamma_{bc}^a\right)u^dw^bu^c-\Gamma_{bc}^a\Gamma_{de}^cu^du^ew^b+\Gamma_{bc}^a\Gamma_{de}^bw^du^eu^c-\partial_d\Gamma_{bc}^aw^du^bu^c.$$ All indices apart from $a$ are dummy indices. In the first two terms on the right-hand side swap $b$ and $c$ indices. In the second two terms swap the $c$ and $d$ indices. This yields $$\frac{D^2 w^a}{\partial s^2}=\left(\partial_d\Gamma_{bc}^a\right)u^dw^cu^b-\Gamma_{bc}^a\Gamma_{de}^bu^du^ew^c+\Gamma_{bd}^a\Gamma_{ce}^bw^cu^eu^d-\partial_c\Gamma_{bd}^aw^cu^bu^d$$ $$=-\left(\partial_c \Gamma_{bd}^a-\partial_d\Gamma_{bc}^a+\Gamma_{ec}^a\Gamma_{db}^e-\Gamma_{ed}^a\Gamma_{cb}^e\right)u^bw^cu^d=-R_{bcd}^au^bw^cu^d$$ Looking at the solution above, I really don't know how eqn. $(3)$ in the solution was found by differentiating eqn. $(1)$ and moreover, why can I not arrive at the same eqn. via $(\mathrm{a})$ ?","['proof-explanation', 'tensors', 'calculus', 'general-relativity', 'differential-geometry']"
4517527,Find solution of particular integral transform,"Here's a (not so) fun problem I've encountered trying to complete a proof. I have some constants $b > 0$ , $k < 0$ , and $c_1, c_2 \in \mathbb R$ , and I need to find a function $f:[0, b] \to \mathbb R$ and constant $\lambda \in \mathbb R$ such that $$
c_1 \int_0^x e^{k(x - y)} f(y) ~dy + c_2 \int_x^{b} e^{-k(x - y)} f(y) ~dy = \lambda e^{-kx},
$$ for all $x \in [0, b]$ . Does anyone have an approach to find $f$ ?","['integration', 'problem-solving', 'functional-analysis', 'integral-transforms']"
4517597,"Conditions for convergence of a derivative, given the function itself is convergent","Suppose $\{f_n\}_{n \in \mathbb{N}}$ is a family of bounded, differentiable, monotone increasing functions on $[0,1]$ , which converge uniformly to a limit $f$ .  Also, suppose we know that $f_n'$ is $\alpha$ -Lipschitz continuous for some constant $\alpha$ (not depending on $n$ ).   I want to analyze the differentiability of $f$ and the convergence of $f_n'$ , if this is even possible. Of course, the monotone increasing property of $f_n$ implies that $f$ is also monotone increasing, so it is differentiable Lebesgue almost surely on $[0,1]$ .  Is it possible to say that $f_n'$ converges to $f'$ pointwise, wherever $f'$ exists (even if only on a subsequence)?","['functional-analysis', 'analysis', 'real-analysis']"
4517615,"If for $x\in(\frac{1}{2},\infty)$ we have $f'(x)=(e^x-1)(x-2)(x-3)$. Show that there exist exactly two roots of $f''(x)=0$ in the given domain","Let $f:\left(\frac{1}{2},\infty \right)\to \mathbb{R}$ be a function such that $f'(x)=(e^x-1)(x-2)(x-3)$ . Show that there exist exactly two roots of $f''(x)=0$ in the given domain. My Attempt I evaluated $f''(x)=e^x(x-2)(x-3)+(e^x-1)(x-2)+(e^x-1)(x-3)$ and then plugged in the values $1,2$ and $3$ and obtained $f''(1)=3-e>0$ $f''(2)=(e^2-1)(-1)<0$ $f''(3)=e^3-1>0$ So one can observe that there is at least one root on both of the intervals $(1,2)$ and $(2,3)$ . But how does one claim that there are exactly two roots in the given domain","['rolles-theorem', 'calculus', 'derivatives', 'real-analysis']"
4517626,Real solutions to the depressed cubic equation [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How can I find the allowed domain to this depressed cubic inequality $$x^3 - 3 x + 2 \cos(\frac{3 \sqrt{3} n}{2}) \geq 0$$ where $n$ is a real non-negative number. Using Cardano's method, I can obtain some solutions for $x$ ( $x_1$ , $x_2$ , and $x_3$ ) that should be real and not complex. How can the solutions $x_j$ to define the feasible region satisfying the inequality for fixed $n$ ?","['cubics', 'algebra-precalculus', 'roots']"
4517653,Bounding Binomial distribution tail,"I have a random variable $X\sim \text{Bin}(3k, q)$ where $q < \frac{1}{2}$ and $q(1-q) \leq \frac{1}{5}$ . I want to show that for $k=O(\log_2 r)$ I can bound the probability $\mathbb{P}(X\geq k)$ by $\frac{1}{2^r}$ . I tried several upper bounds for the tails of binomial distributions I found online but none of them seems to do the trick. Any help would be appreciated!","['binomial-distribution', 'probability', 'upper-lower-bounds']"
4517656,Doubt on connection notions for the tangent bundle $T\mathcal{M}$,"I) Bundles The tangent bundle $T\mathcal{M}$ forms a introductory pathway for the theory of bundles. Probably the reason to introduce the notion of bundles via tangent bundles relies on its simplicity given the notion: ""the tangent bundle is a collection of all tangent spaces $\displaystyle T\mathcal{M} = \cup T_{p}\mathcal{M}$ . But, there is more on bundle theory: the principal bundles $P$ . Now, a particular structure that the one can construct given a principal bundle $P$ is the so called associated bundle $A_{P}$ . Given a particular principal bundle called frame bundle $Fr(\mathcal{M})$ , its associated bundle structure is the very tangente bundle $A_{Fr(\mathcal{M})} = T\mathcal{M}$ II) Connections Historically the Levi-Civita Connection occurs when the one study two structures: the base manifold $\mathcal{M}$ and the tangent bundle $T\mathcal{M}$ , and its local form (a.k.a. covariant derivative) is (with abuse of notation): $$\nabla_{\mu} := \partial_{\mu} \pm \Gamma_{\mu \gamma}^{\nu} \tag{1}$$ the connection coeficients are the famous Christoffel symbols. But, when the one starts to work with the Frame bundle structure, the notion of a connection map on $Fr(\mathcal{M})$ ""changes"" to the famous spin connection . Eventually, the tangent bundle is viewed as a associated bundle and then the covariant derivative changes to: $$D_{\mu} := \partial_{\mu} + \omega_{\mu \gamma}^{\nu} \tag{2}$$ The $(2)$ occurs every time when the one is dealing with gauge theory and, in particular, with the Dirac operator $\gamma^{\mu}D_{\mu}$ . III) My Question So, it seems that $T\mathcal{M}$ can be viewed as a ""standalone"" structure and as a associated bundle. Both of the cases, its a vector bundle, but the notion of the covariant derivative seems to ""change"". My question is: how can the same structure $T\mathcal{M}$ render different notions of covariant derivatives ( $(1)$ and $(2)$ )?","['principal-bundles', 'connections', 'smooth-manifolds', 'vector-bundles', 'differential-geometry']"
4517658,Irreducibility of polynomials in $\mathbb{F}_p[x][y]$,"Suppose we have a polynomial $P(x,y)$ in $\mathbb{F}_p[x][y]$ that is irreducible as a polynomial in $y$ . I guess that $P(x^p,y)$ is also irreducible as a polynomial in $y$ . How can I prove it? Thank you in advance! Edit, after Kenta's comment: What if I add the condition that
there exists an $n$ not divisible by $p$ such that the coefficient of $y^n$ in $P$ is nonzero?","['irreducible-polynomials', 'abstract-algebra']"
4517687,Cauchy sequences in a linear normed space form a subspace of the space of bounded sequence,"$\mathbb{X}$ is a linear normed space. The set $b(\mathbb{X})$ of sequences $(x_n)_{n\geq1}$ with values in $\mathbb{X}$ , that are bounded, and $$||(x_n)_{n\geq1}||_{*}=\sup_{n\geq 1}{||x_n||}<\infty$$ We check easily that $b(\mathbb{X})$ is a normed linear space. I want to prove that Cauchy sequences in $\mathbb{X}$ form a subspace say $b_c(\mathbb{X})$ , of $b(\mathbb{X})$ . I know that every Cauchy sequence is bounded, so $b_c(\mathbb{X})$ is a subset of $b(\mathbb{X})$ . If $x_n\in b(\mathbb{X})$ , then $x_n+y_n \in b_c(\mathbb{X})$ and $\alpha x_n \in b_c(\mathbb{X})$ . Therefore, $b_c(\mathbb{X})$ is a algebraic subspace of $b(\mathbb{X})$ . But I don't know how to prove $b_c(\mathbb{X})$ is closed.","['cauchy-sequences', 'functional-analysis', 'real-analysis']"
4517697,"Given $a^4+8b=4(a^3-1)-16\sqrt 3$ and $b^4+8a=4(b^3-1)+16\sqrt 3$, find $a^4+b^4$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Given $a^4+8b=4(a^3-1)-16\sqrt 3$ and $b^4+8a=4(b^3-1)+16\sqrt 3$ , find $a^4+b^4$ I tried adding and subtracting both equations, but didn't get anywhere. Would appreciated any ideas. Thanks!",['algebra-precalculus']
4517745,Gradient of ${\bf x}^\top {\bf A}^{1/2} {\bf x}$ with respect to $\bf A$,"How to calculate the gradient $\nabla_{\bf A} \left( {\bf x}^\top {\bf A}^{1/2} {\bf x} \right)$ , where $\bf x$ is $N \times 1$ column vector and $\bf A$ is $N \times N$ symmetric positive matrix? The difficulty is that there is ${\bf x}^\top {\bf A}^{1/2} {\bf x}$ rather than ${\bf x}^\top {\bf A} {\bf x}$ . Motivation I want to calculate the gradient of the vector Gaussian distribution $\mathcal{N} \left( \mathbf{y} \mid \mathbf{A}^{\frac{1}{2}}\mathbf{x}, \mathbf{I} \right)$ w.r.t. $\mathbf{A}$ , where $\mathbf{I}$ is an identity matrix, and $$ \mathcal{N} \left( \mathbf{y} \mid \mathbf{A}^{\frac{1}{2}},\mathbf{I} \right) = (2\pi)^{-\frac{N}{2}} \exp \left( -\left\|\mathbf{y}-\mathbf{A}^{\frac{1}{2}}\mathbf{x} \right\|_2^2 \right) $$ where $\|\cdot\|_2$ denotes the $\ell_2$ norm. Its difficulty is to calculate the term $$\nabla_{\mathbf{A}} \left( \mathbf{y}^T\mathbf{A}^{\frac{1}{2}}\mathbf{x} \right)$$ This term can be similarly solved by $\nabla_{\mathbf{A}}\left( \mathbf{x}^T\mathbf{A}^{\frac{1}{2}}\mathbf{x} \right)$ . And I believe this term exists.","['scalar-fields', 'normal-distribution', 'matrices', 'matrix-calculus', 'derivatives']"
4517763,How are the following integrals equivalent?,"Several textbooks I've read make the claim that for any $f: \mathbb{R} \to \mathbb{R}$ $$ \frac{1}{k!}\int_{0}^{\infty} x^k f(x) dx =\int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1}  $$ where we assume both sides of the equality are finite (e.g $f$ is continuous and compactly supported). I would preferably like to start with the left hand side (LHS) and obtain the right hand side (RHS) because the LHS is the ""natural"" integral to look at in the context of these textbooks. However, given that this equality is true, it seems easier to start with the right hand side. (If you have a motivated proof that starts with the LHS and gets to the RHS I would love to see that). Starting with the RHS, we can substitute $ x = x_1 + \dots x_k + x_{k+1}$ and have $dx = dx_k $ . Therefore, $$\int_{[0,\infty)^{k+1}} f(x_1+x_2+\dots + x_{k+1}) \ dx_1 \dots dx_{k+1} = \int_{[0,\infty)^k} \int_{x_1+\dots + x_k}^{\infty} f(x) \ dx   \ dx_1 \dots dx_k$$ We are integrating over the set $\{ x \in \mathbb{R} \ \text{and}\  x_i \in \mathbb{R}| 0 \leq x_i \leq \infty \ \text{and} \ (x_1 + \dots + x_k) \leq x \leq \infty \} $ I was thinking of changing the order of integration because that worked in the case $k=1$ , but I'm running into trouble with doing that for general $k$ . Any ideas?","['integration', 'real-analysis', 'multivariable-calculus', 'multiple-integral', 'linear-algebra']"
4517777,Maximum and minimum values of probability with two events,"Question : Consider two events $a$ and $b$ such that $P(a) = \alpha$ and $P(b) = \beta$ . Given only that knowledge, what is the maximum and minimum values of the probability of the events $(a \cap b$ ), and $(a \cup b)$ . Can you characterize the situations in which each of these extreme values occur? My take: If $a$ and $b$ are disjoint, $(a \cup b)$ will have the maximum value of $1$ and $(a \cap b)$ will have the minimum value of $0$ . if $a$ and $b$ are identical, ( $a \cap b$ ) will have the maximum value of $1$ and $(a \cup b)$ will have the minimum value of $0$ . I am not sure if my explanation for this question is correct or not, if it is wrong please guide me towards the answer. Thanks!","['probability-theory', 'probability']"
4517787,When can one justify switching the limit and infinite sum?,"I want to prove the following theorem $$\lim_{N\to\infty}\frac{\displaystyle\sum_{k=1}^N f(a_k)}{N}=\sum_{n=1}^\infty f(n)P(a_k=n)$$ Where $a_k$ is a sequence of non-negative integers and $P(a_k=n)$ is the probability that $a_k=n$ over all $a_k$ 's. $f(n)$ is an arbitrary function to the non negative real numbers. I want to know what a sufficient (maybe even necessary) condition is that this theorem holds. Here is what I was able to prove so far $$\lim_{N\to\infty}\frac{\displaystyle\sum_{k=1}^N f(a_k)}{N}=\lim_{N\to\infty}\frac{1}{N}\sum_{n=1}^\infty f(n)||a_k=n||_{k \le N}$$ where $||a_k=n||_{k \le N}$ is the number of $a_k$ 's with $k \le N$ for which $a_k=n$ . This is true since all that was done here is groupe together equal terms. Lets now assume the sum and the limit can be switched around (here is where I miss a proof), therefore we have $$\sum_{n=1}^\infty f(n)\lim_{N\to\infty}\frac{||a_k=n||_{k \le N}}{N}=\sum_{n=1}^\infty f(n)P(a_k=n)$$ I tried proving the missing step using Tannery's theorem but couldnt do it. So my question basically becomes, what conditions do $a_k$ and $f(n)$ have to meet in order to be able to switch the limit and sum? Any help is appreciated and thank you in advance!","['limits', 'infinity', 'sequences-and-series']"
4517935,How can I prove the following equation for the angle between two vectors?,"I'm new in the Mathematics forum, I hope such questions are accepted. This is a problem I had to solve in a job interview, I still can't find the answer, which I guess it's pretty basic. Given two vectors $x,y$ , prove that the angle between them can be calculated as: $$\theta = 2\arctan\left(\frac{\Big| |x| y- |y| x\Big|}{\Big| |x| y+ |y|x\Big|}\right)$$ I already tried writing $x$ and $y$ as vectors in the complex plane, I tried some graphical ""methods"" and using various trigonometric identities, but nothing got me close to an equation similar to that. A second question asked what numerical advantages does this formula give with respect to the usual $\theta=\arccos\left(\frac{{x}\cdot{y}}{|x||y|}\right)$ . I thought that $\arctan$ can accept any input, while $\arccos$ limits them in $[-1,1]$ , but this doesn't sound like a numerical matter. Moreover, I see a subtraction in the first equation, which I guess might lead to some loss of significance problems. Is the first equation really numerically superior than the second one?","['analytic-geometry', 'numerical-methods', 'vectors', 'geometry']"
4517969,Stuck at the beginning of the proof of uniform convergence in probability.,"If $X_1, \ldots, X_n$ are i.i.d. $p(x; \theta_0) \in \{p(x; \theta) : \theta\in \Theta\}$ , $\Theta$ is compact, $\log p(x; \theta)$ is continuous in $\theta$ for all $\theta \in \Theta$ and all $x \in X$ , and if there exists a function $d(x)$ such that $|\log p(x; \theta)| \le d(x)$ for
all $\theta \in \Theta$ and $x \in X$ . and we also know that the uniform continuity also implies that $\Delta(x, \delta) = \underset{{\{(\theta_1,\theta_2):||\theta_1−\theta_2||<\delta\}}}{\sup} \log p(x; \theta_1) − \log p(x; \theta_2)|\to 0$ as $\delta \to 0 $ My question is why do we have $\Delta(x, \delta) \le 2d(x)$ and
how to prove that , by the dominated convergence theorem we have that $E_{\theta_0}[\Delta(X, \delta)] \to 0$ as $\delta \to 0$ . thank you very much in advance","['uniform-convergence', 'probability-theory', 'probability', 'real-analysis']"
4518011,"For $u_{n+1}=\frac12 \arctan(u_n)$, why $u_n \sim 2^{-n}u_0/\sqrt{1+\frac89u_0^2(1-4^{-n})}$?","This question is about another post, specifically answered by Lutz Lehmann. Here is the post : Asymptotic expansion of $u_{n + 1} = \frac12 \arctan(u_n)$ I will re-wrote his answer : $$
\frac1{u_{n+1}^2}=\frac4{u_n^2(1-\frac13u_n^2+\frac15u_n^4\mp...)^2}
=\frac4{u_n^2}+\frac83-\frac4{15}u_n^2+O(u_n^4)\tag1
$$ Thus for a first approximation use $$x_{n+1}=4x_n+\frac83\iff x_{n+1}+\frac89=4(x_n+\frac89)$$ so that $$u_n^{-2}\sim x_n=4^n(x_0+\frac89)-\frac89.\tag2$$ This gives as first approximation $$
u_n\sim \frac{2^{-n}u_0}{\sqrt{1+\frac89u_0^2(1-4^{-n})}}.\tag3
$$ I have a problem through his proof. And that is $u_n^{-2}\sim x_n$ . I tried to explain it : $\forall n\in \mathbb N, v_n = u_{n+1}^{-2}-4u_n^{-2}-8/3$ , $x_n = u_n^{-2} + w_n$ . We want to find a $(w_n)_{n\in\mathbb N}$ such that $w_n = o(u_n^{-2})$ and we have : $$x_{n+1}=u_{n+1}^{-2} + w_{n+1} = 4u_n^{-2}+8/3+v_n+w_{n+1} = 4x_n+8/3+v_n+w_{n+1}-4w_n$$ So $w_{n+1} = 4w_n -v_n$ . But that's here where it becomes quite difficult. I have not been able to show that $w_n = o(u_n^{-2})$ . I want to know if there is a better explanation for this approximation.","['limits', 'trigonometry', 'asymptotics', 'sequences-and-series']"
4518028,Definition of $p$-adic formal scheme,"Could someone please provide a precise definition of a $p$ -adic formal scheme $X$ over a ring $A$ ?  Is it a formal scheme over $A$ which is locally isomorphic to $\operatorname{Spf}(B)$ , where the completion is taken with respect to the ideal $(p) \subset B$ ? Moreover, is there a relation to schemes over $p$ -adic fields or rings?  e.g. can one embed $\mathbb{Z}_p$ -schemes fully faithfully in $p$ -adic formal schemes?","['rigid-analytic-spaces', 'algebraic-geometry']"
4518030,Combinations - excluding one from each group,"I have 8 women and 6 men and need to form a committee with 3 women and 3 men.  If one man and one woman refuse to work together how many committees can be formed? Determine the number of committees excluding the 1 man and 1 woman: $\binom{7}{3}\binom{5}{3}  = 350$ Determine the valid combinations including all the women and excluding the one man: $\binom{8}{3} \binom{5}{2}  = 560$ Determine the valid combinations including all the men and excluding the one woman: $\binom{7}{2}\binom{6}{3}  = 441$ Find the total: 350 + 560 + 441 = 1351 Obviously this is wrong because the total number of committees, if everyone is willing to work with everyone is: $\binom{8}{3}\binom{6}{3}  = 1120$ Can anyone help with how I am supposed to be solving this? Thanks",['combinatorics']
4518036,Alternate forms of the Cantor Function? (aka Devil's Staircase),"$\def\R{\mathbf{R}}$ $\def\Q{\mathbf{Q}}$ Background: Using properties of uniform convergence and the definition of the function, in an exercise I proved that the Cantor Function is increasing, is differentiable except on the Cantor set, where it is just continuous.
It was a very weird property that it somehow manages to increase from $0$ to $1$ while having a derivative of $0$ wherever it is defined.
In some intuitive way I felt like the Cantor set had the ability to push the function from $0$ to $1$ , since the function only strictly increased near cantor set points.
My question is, which other sets are able to do this? Or rather, what sort of properties must the set satisfy to be able to construct such a function like this? The Question. Say we are given a function $f:\R \to \R $ , and a subset $A \subseteq \R$ , with the following properties: $f$ is increasing $f$ is continuous The derivative $f'(x) = 0$ for all $x \notin A$ , and $f$ is not differentiable for any $x\in A$ . $f$ is not constant. Then what can we say about the set $A$ ? (Open ended) We know that $A$ must be uncountable. Proof. $f$ is not constant, hence the range has at least two distinct values, say $a$ and $b$ . (assume $a<b$ WLOG). Since $f$ is continuous, it has the intermediate value property and hence the range contains $[a,b]$ . $f$ is increasing, but constant everywhere except on $A$ . So to ""bring"" $f$ from $a$ to $b$ , the elements of $A$ must map to every number in $[a,b]$ Since $[a,b]$ is uncountable, it follows that $A$ must be as well. Q.E.D. Are there any other characteristics that we can deduce of $A$ ? I wonder if $A$ can be the irrationals.","['general-topology', 'cantor-set', 'derivatives', 'real-analysis']"
4518052,Number of real roots of a separable real polynomial doesn't change under small perturbations,"Say we have a polynomial with real coefficients and no repeated roots.  Knowing that the roots of a polynomial vary continuously in the coefficients (so long as we don't change the degree), it seems intuitive that all sufficiently close polynomials will have the same number of real roots, because in order to make two real roots non-real, or vice versa, you'd have to first bring them together in order to satisfy the constraint that the non-real ones must be conjugates, and it's not too hard to see that all sufficiently close polynomials will also have no repeated roots (nonzero discriminant is an open set). However I'm at a loss as to how to formalize the ""you'd have to bring them together"" argument above.  That repeated roots can be avoided is easy because we have the discriminant as noted above, and looking at the sign of the discriminant shows the number of real roots won't change mod 4, but I'm not sure how to do better than that algebraically.  It would be nice if Sturm's Theorem could be used but I don't expect that the polynomial quotients involved would be continuous at all the relevant points. Is there any sort of nice algebraic way to do this, or is the best way to formalize the intuitive argument using continuity of roots?  In the latter case, how would you actually prove that you have to bring two of them together? (A possible workaround I thought of for the algebraic approach would be to use a different, more general Sturm chain -- in particular, for polynomials of degree n, consider $\mathbb{R}(a_0,\ldots,a_n)$ ($a_0,\ldots,a_n$ indeterminates) and the generic polynomial $a_n x^n+\ldots+a_0$ and its derivative, take quotients and remainders there, and only afterward plug in the real numbers, thus getting rid of discontinuous-degree-change issues.  However this, or at least this particular variant, doesn't seem to actually work, as far as I can tell -- checking by hand the degree 3 case seems to indicate that this won't work since anything that's an intermediate initial coefficient will end up getting divided by, and the first one of those is $\frac{2}{9}\frac{a_2^2}{9a_3}-\frac{2}{3}a_1$, which can still be 0 without either $a_3$ or the discriminant being 0, which are the two things that can obviously be safely divided by.)","['abstract-algebra', 'roots', 'polynomials', 'analysis']"
4518073,Vectors with $\pm 1$-s and the Combinatorial Nullstellensatz,"Let $S$ be a set of $k < n$ vectors of length $n$ , each having each entry equal to $1$ or $(-1)$ . Prove that there is a vector of length $n$ with each entry $1$ or $(-1)$ which is not orthogonal to any of the vectors in $S$ . The idea is to use the Combinatorial Nullstellensatz but I cannot find a suitable polynomial - it should contain something like $\prod_{i=1}^k (x \cdot v_i)$ , where $x = (x_1,\ldots,x_n)$ are the variables and $v_1,\ldots, v_k$ are the given vectors, but I cannot think of an additional suitable term. Any help appreciated!","['extremal-combinatorics', 'vectors', 'combinatorics', 'polynomials']"
4518083,Exercise 4.3.13 on Frobenius morphisms in Qing Liu's Algebraic Geometry,"Let $X$ be a smooth morphism over a scheme $S$ of positive characteristic $p > 0$ .
The morphism $F_S: S \to S$ induced by the ring homomorphism $O_S \to O_S : a \mapsto a^p$ the absolute
Frobenius of $S$ . We let $X^{(p)} $ denote
the fibered product $X \times_S S$ , where the second factor $S$ is endowed
with the structure of an S-scheme via the absolute
Frobenius $F_S : S \to S$ . The relative Frobenius $F_{X/k} : X \to X^{(p)}$ fits in following diagram in Definition 33.36.4 from Stacks Project. Exercise 4.3.13 in Qing Liu's Algebraic Geometry and Arithmetic Curves
asks of us to prove (a) Show that $X^{(p)} \to S $ is smooth
(use Proposition 3.38).
(b) Let us suppose that $S$ is the spectrum of a field $k$ . Let $F_{X/k} : X \to X^{(p)}$ be the relative Frobenius.
Show that $F_{X/k}$ is flat at the rational
points of $X$ (use Proposition 2.27).
(c) Show that $F_{X/k}$ is faithfully flat, that is flat and surjective and I'm having trouble in solving some parts there. (a) follows simply from smoothness of $X \to S$ by base change. (b) could work like this: $X$ is smooth and therefore at every point
rational point $x$ the $\mathfrak{m}$ -adic completion of the stalk $O_{X,x}$ at it's maximal ideal $\mathfrak{m}$ is $$\hat{O}_{X,x} \cong k[[T_1,..., T_d]]  $$ with $d = \dim O_{X,x}.$ Therefore at $x$ the map on stalks $F_{X/k}: O_{X,x} \to O_{X,x}$ induces
a map on the completions $k[[T_1,..., T_d]]$ which should be determined
by $T_j \mapsto T_j^p$ . Why is the last map flat and how can I deduce the flatness of $F_{X/k}$ at $x$ ?
If $d=1$ then $k[[T_1]]$ is principal ideal domain and since the map $k[[T_1]] \to k[[T_1]], T_1 \mapsto T_1^p$ is
torsion free therefore flat and we win. What about case $d > 1$ und how to
deduce flatness of $F_{X/k}$ in $O_{X,x}$ . (c) No idea how to prove that $F_{X/k}$ flat. Flatness commutes base change
so that it's sufficient to show it at closed points. But not all closed points are
rational, so I not see how (b) helps.","['algebraic-geometry', 'flatness', 'schemes']"
4518091,Obtaining $\zeta(4)$ from specific derived expression,"I started from the expansion of cot(x) by the Mittag-Leffler theorem, where: $$\cot(x) = \frac{1}{x} + \sum_{k = 1}^{\infty} \frac{2x}{x^{2} - k^2 \pi^2}.$$ After  splitting up $\cot(x)$ , taking the derivative of both sides, and some algebra, I eventually was left with the following expression: $$-1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2 
  \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2} $$ where $x = 0$ gives $\zeta(2)$ . I am now trying to obtain the exact value for $\zeta(4)$ through this expression. To do this, I am attempting to once again take the derivative and then set $x = 0$ . My work is shown below. $$-1 = 6\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} + 4x^2 
  \left [ \sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 - 4x^2\sum_{k = 1}^{\infty} \frac{1}{({x^{2} - k^2 \pi^2})^2} \Longrightarrow $$ $\Longrightarrow 0 = -12x\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + 8x^2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4x}{(x^{2} - k^2 \pi^2)^3} \right ] \Longrightarrow
$ Dividing by $x$ : $\Longrightarrow0 =-12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] ^2 + \\8x\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] \times \frac{d}{dx}\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{x^{2} - k^2 \pi^2} \right ] - 8x \left [\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(x^{2} - k^2 \pi^2)^2} \right ] \left [ \displaystyle\sum_{k = 1}^{\infty} \frac{-4}{(x^{2} - k^2 \pi^2)^3} \right  ] \Longrightarrow
$ Setting $x = 0$ : $\Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 \pi^2)^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 \pi^2} \right ] ^2 \Longrightarrow
$ Multiplying everything by $\pi^4$ : $\Longrightarrow 0 = -12\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(- k^2 )^2} + 8\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{- k^2 } \right ] ^2 \Longrightarrow
$ $\Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \displaystyle\sum_{k = 1}^{\infty} \frac{1}{ k^2 } \right ] ^2 \Longrightarrow
$ Solving for ζ(4): $\Longrightarrow 0 = 3\displaystyle\sum_{k = 1}^{\infty} \frac{1}{(k^2 )^2} - 2\left [ \frac{\pi^2}{6} \right ] ^2 \Longrightarrow
$ $\Longrightarrow \displaystyle\sum_{k = 1}^{\infty} \frac{1}{k^4 } = \frac{\pi^4}{54}$ However, my answer is wrong. The correct result would be $\frac{\pi^4}{90}$ . Any help on this would be greatly appreciated.","['riemann-zeta', 'derivatives', 'mittag-leffler-function']"
4518104,Inscribing an ellipse in an irregular convex pentagon,"Using the methods of projective geometry, identify the unique ellipse that is inscribed in a given convex pentagon. Suppose the vertices of the pentagon are: $(1, 0), (4, 2), (3, 6), (-1, 5), (-1, 1)$ .  Find the equation of the unique inscribed ellipse that is tangent to all five sides of this convex pentagon.","['projective-geometry', 'conic-sections', 'geometry']"
4518137,How to find this indefinite integral? $\int\frac{1+x^4}{(1-x^4)\cdot \sqrt{1+x^4}}dx$,"I am thinking of a trig sub of $x^2 = \tan{t}$ but its not leading to a nice trigonmetric form, which i can integrate. Our teacher said that it can be computed using elementary methods, but I'm unable to think of the manipualtion.",['calculus']
4518186,Self adjoint inverse,"I'm reading some functional analysis and it's after lunch so I can't think. Can someone please help with this quick question. I have the following: Let $T\in B(H)$ be a self adjoint operator and $\varepsilon>0$ . Let $p$ be the spectral projection of $T$ corresponding to the Borel subset $[-\varepsilon,\varepsilon]$ (that is, the indicator function of $[-\varepsilon,\varepsilon]$ applied to $T$ by Borel functional calculus). Then the following operator $$S = (1-p)T+\varepsilon p$$ has a self adjoint bounded inverse. Why? Is there an obvious inverse I am missing, or is this some bounded inverse theorem (or one of the other functional analysis theorems) business.","['operator-theory', 'functional-analysis', 'functional-calculus', 'operator-algebras']"
4518199,Semi-orthogonal tall matrices with zero column-means,"For a semi-orthogonal $m\times n$ matrix $\mathbf{A}$ with $m\gt n$ , such that $\mathbf A^T\mathbf A=\mathbf I_n$ , can we prove or disprove the proposition that, for some given $m, n$ an $\mathbf A$ exists such that $$\sum_{i=1}^m a_{ij} = 0\qquad\forall j \in [1, n]$$ Further, from an optimization perspective, regardless of whether the above proposition holds or not: Starting from an arbitrary semi-orthogonal matrix $\mathbf A_{m\times n}$ which does not meet zero column mean criteria, how close can we approach the criteria? The other way around could be to start with a matrix with zero column means and optimize $\Vert\mathbf A^T\mathbf A-\mathbf I\Vert_F$ under the constraint. Could a Lagrangian do the trick here? Say, if we start with the orthogonality, we could begin with $\mathbf{QR}$ decomposition of a random matrix and optimize $\Vert\mathbf Q^T\mathbf Q-\mathbf I\Vert_F$ subject to $\sum a_{ij}$ . But how exactly do we proceed for the multidimensional case? Alternatively under a statistical view, assuming $n$ zero-centered distributions each with sample size $m$ , can we somehow optimize an equivalent distance metric? I am not too sure about this intuition. I also looked into possibility of using normalized Hadamard matrices , although they are surely known to exist only when $m=2^k$ , and possibly also exist whenever $m=4k$ . I also found an outdated paper discussing the weights (total number of 1s) in these matrices and they provide upper and lower bounds for many cases, but their definition of weights is not columnar. Hadamard matrices that fit my query must only have $m/2$ weight per column in at least $n$ columns. They are not easy to find in the first place for an arbitrary space and I suspect, even harder to optimize as we have to start with binary elements. Approaching from a combinatorial perspective, I looked into a related concept from coding theory, the so called equidistant constant weight codes but could not glean much from it. Then also, my problem is not necessarily combinatorial in nature although such a solution would be equally acceptable. Kindly pardon any mistakes in text or notations. An easy to understand explanation would be preferred and much appreciated. Thanks.","['statistics', 'orthogonal-matrices', 'linear-algebra', 'optimization', 'coding-theory']"
4518281,How to prove inequalities in geometry,"Outline: In a triangle $\triangle ABC$ let $\overline{AB}$ be the longest of the three sides. Let $G$ be the centroid of $\triangle ABC$ and $M$ the midpoint of $\overline{AB}$ . Furthermore, let a point $D$ outside the triangle be given. The following inequalities now apply: The distance from $D$ to the vertices of the triangle is less than $1$ . The side $\overline{AB}$ is bigger than $\sqrt{\frac{2}{3}}$ but less than $\sqrt 2$ . The median $\overline{CM}$ is bigger than $\frac{\sqrt 2}{2}$ . Prove: $\overline{DG}<\frac{\sqrt 2}{2}$ . I experimented a bit with Geogebra and found out by measuring side lengths that the statement holds. I can't find much on the internet about these kinds of problems with geometric inequalities. Therefore, I would be interested in what possibilities there are to approach such problems. I have tried to make progress with the triangle inequality and Ptolomew's inequality, but have not yet reached my goal, because the estimates were always too inaccurate.
Edit: The best possible bound to prove is $DG<\frac{\sqrt 7}{3}$ .","['inequality', 'geometry', 'triangles', 'geometric-inequalities', 'trigonometry']"
4518301,Tips or steps on how to visualise a multivariable function,"Question . My doubt is about what steps do you take when given a multivariable function like $z^2 = x^2 - y^2$ , to know how it looks like in general. For example if it looks like a bunch of waves, or a mountain, or a ellipsoid, or an sphere, etc. I want to improve in being able to visualize most functions, of course not knowing exactly how they are but a general shape or knowledge about it. I know it's needed a lot of practice but I'm asking for tips/steps not another thing.","['multivariable-calculus', 'calculus', 'functions']"
4518352,BLUE in correlated case,"Let $X_1,...,X_n$ be real-valued observations such that $E(X_j)=\mu$ , $V(X_j)=\sigma^2$ and the correlation coefficient between $X_i$ and $X_j$ is $r_{ij}$ , $1 \le i < j \le n$ . Assume that $r_{ij}$ 's are known. Derive the Best Linear Unbiased Estimator (BLUE) $\hat{\mu}$ of $\mu$ . My approach so far Suppose $T = \sum_{j=1}^{n} c_j X_j$ is the BLUE. Then $E(T)=\mu \implies \sum_{j=1}^{n} c_j =1$ . Also, we need to minimize $V(T) = \sum_{j=1}^{n} c_j ^2 V(X_j) + 2 \sum \sum_{i<j} c_i c_j \sigma^2 r_{ij}$ with respect to $c_1,...,c_n$ . I tried Lagrange Multipliers but the entire thing becomes too much tedious and I could not get a closed-form solution for the $c_j $ 's. Is there any other approach to this?","['statistical-inference', 'statistics', 'estimation', 'probability-theory', 'probability']"
4518421,"Can we write square roots in a fraction separately? eg, $\sqrt{\frac{9-x^2}{x-2}}$ vs $\frac{\sqrt{9-x^2}}{\sqrt{x-2}}$","I was doing some questions related to functions when I came across this question where they gave us two functions as $$\sqrt{\frac{9-x^2}{x-2}}\quad\text{and}\quad\frac{\sqrt{9-x^2}}{\sqrt{x-2}}$$ I can see that those two are a little different, but I have learned that we write $$\sqrt\frac{a}{b}=\frac{\sqrt a}{\sqrt b}$$ I am a little confused: is there a specific condition in which we can apply this?","['notation', 'algebra-precalculus', 'radicals']"
4518427,How to compute the gradient of any loss function,"I have the following loss function to minimize : $\hat{\mathbf{A}} = \arg \min_{\mathbf{A}} \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{M}\right)}\parallel}_{F}^{2} + \frac{1}{2}{\parallel{\mathbf{W} - \mathbf{L}\left(\left(  \mathbf{D}\mathbf{A}\right)\odot\mathbf{H}\right) \mathbf{S}}\parallel}_{F}^{2}$ Where ${\parallel\mathbf{X}\parallel}_{F}^{2} =Tr(\mathbf{XX^T})$ is the Frobenius norm, and $\odot$ denote Hadamard product (element-wise product). For the first term : $\mathbf{Y}\in\mathbb{C}^{a\times l}$ , $\mathbf{K}\in\mathbb{C}^{a\times b}$ , $\mathbf{D}\in\mathbb{C}^{b\times d}$ , $\mathbf{A}\in\mathbb{C}^{d\times l}$ and $\mathbf{M}\in\mathbb{C}^{b\times l}$ . For the second term : $\mathbf{W}\in\mathbb{C}^{b\times c}$ , $\mathbf{L}\in\mathbb{C}^{b\times b}$ , $\mathbf{H}\in\mathbb{C}^{b\times l}$ and $\mathbf{S}\in\mathbb{C}^{l\times c}$ . I know when the loss function is on the form : $J(X) = \frac{1}{2}{\parallel{\mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)}\parallel}_{F}^{2}$ then its derivative is : $\nabla J(X) = K^{H}\left( \mathbf{Y} - \mathbf{K}  \left(  \mathbf{X}\odot\mathbf{M}\right)\right)\odot \mathbf{M}$ But in this case it is so difficult. Is there any way to compute its gradiant ?","['optimization', 'gradient-descent', 'matrix-calculus', 'derivatives']"
4518430,It is always true that X < Y. Is it possible for X and Y to be independent? Why?,X and Y are random variables. It is always true that X < Y. Is it possible for X and Y to be independent? Why?,"['statistics', 'independence', 'random-variables']"
4518485,Does linear combination of i.i.d. variables being normal implies normality of original distribution?,"Suppose $X_1, X_2 \ldots X_n$ are i.i.d., and for some real coefficients $a_i$ we have $$
\sum_{i=1}^n a_i X_i \overset D =\mathcal N(0, 1).
$$ Is it necessary, that $X_1$ has normal distribution? My attempt: note that not all $a_i$ are zero. If all $a_i$ were equal, then using characteristic functions we would have $$ \varphi_{X_1} (a_1 t)^n = \exp\left(-\frac {t^2} 2\right)\quad
\implies \quad \varphi_{X_1} (t) = \exp\left(-\frac {t^2} {2n a_1^2}\right)$$ so $X_1$ must have normal distribution. But we can't quite use this approach for different $a_i$ .","['independence', 'probability-distributions', 'probability-theory', 'normal-distribution']"
4518503,Check if point is inside a circular cone in rational^3 domain.,"In 3d Euclidean space, limited to rational Cartesian coordinates, we have a circular cone. Cone's tip is at the origin point $p_0$ . We know coordinates of $3$ points on cone's surface: $p_1,p_2,p_3$ . We can assume that $\{p_0, p_1, p_2, p_3\}$ forms a tetrahedron with non zero volume. How to test if given point $p_4$ is inside the cone? I know we could project points $p_1,p_2,p_3,p_4$ onto a sphere centered at $p_0$ , resulting in $q_1,q_2,q_3,q_4$ accordingly,
then calculate circumcenter from $q_1,q_2,q_3$ projections in order to get cone's axis vector $c$ . Then check if dot product of cone's axis $c$ with any of $q_1, q_2, q_3$ projections is smaller than dot product of axis with $q_4$ projection. Unfortunately, calculating projections makes use of square root which produces real numbers while we are limited to rational numbers only.","['geometry', 'rational-numbers']"
4518507,Conditional covariance of two independent normal variables when their sum is fixed,"I am reading through Brady Neal's ""Introduction to Causality"" course textbook and have got to Section 3.6 where Berkson's paradox is discussed. Neal provides the following toy example: $$
X_{1} = \mathcal{N}(0,1) \\ 
X_{3} = \mathcal{N}(0,1) \\
X_{2} = X_{1} + X_{3}
$$ He then proceeds to compute the covariance of $X_{1}$ and $X_{3}$ as a sanity check: $$
\text{Cov}(X_{1}, X_{3}) = \mathbb{E}[X_{1}X_{3}] - \mathbb{E}[X_{1}]\mathbb{E}[X_{3}] = 
\mathbb{E}[X_{1}X_{3}] = \mathbb{E}[X_{1}]\mathbb{E}[X_{3}] = 0
$$ where we used independence. Next Neal computes the conditional covariance given that $X_{2} = x$ . $$
\text{Cov}(X_{1}, X_{3} \,|\, X_{2} = x) = \mathbb{E}[X_{1}X_{3} \,|\, X_{2} = x] = 
\mathbb{E}[X_{1}(x  - X_{1})] = x\mathbb{E}[X_{1}] - \mathbb{E}[X^{2}_{1}] = -1
$$ Is this correct? When I do my own calculation I seem to get the following result: $$
\text{Cov}(X_{1}, X_{3} \,|\, X_{2} = x) = \mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] - \mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{3}\,|\,X_{2}=x]
$$ Consider each factor separately in the second term: $$
\mathbb{E}[X_{1} \,|\, X_{2} = x] = \mathbb{E}[X_{1} \,|\, X_{1} + X_{3} = x] =
\mathbb{E}[x - X_{3}] = x - \mathbb{E}[X_{3}]
$$ Likewise we have $$
\mathbb{E}[X_{3} \,|\, X_{2} = x] = x - \mathbb{E}[X_{1}]
$$ Multiplying both terms we have: $$
\mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{3}\,|\,X_{2}=x] = (x - \mathbb{E}[X_{3}])(x - \mathbb{E}[X_{1}]) = x^{2}
$$ Now consider the first term: $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] =
\mathbb{E}[X_{1}X_{3}\,|\, X_{1} + X_{3} = x] = \\
\mathbb{E}[X_{1}(x - X_{1})] =
x\mathbb{E}[X_{1}] - \mathbb{E}[X_{1}^{2}] = 0 - 1 = -1
$$ Putting everything together we have: $$
\text{Cov}(X_{1}, X_{3} \,|\, X_{2} = x) = \mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] - \mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{3}\,|\,X_{2}=x] = -1 - x^{2}
$$ Am I doing something wrong? I am concerned the author is forgetting that the expectations in the second term are conditional leading them to set the second term to zero as in the unconditioned case. I may also be using the wrong definition for conditional covariance, although no explicit definition is provided in the book. Note that this example is an attempt to model a collider where $X_{1}$ and $X_{3}$ are parents of $X_{2}$ . EDIT: Both myself and the textbook are wrong! Thanks to Henry for pointing this out, whose answer I have accepted below. I thought I would correct my approach using Henry's working to highlight my errors. As before we have: $$
\text{Cov}(X_{1}, X_{3} \,|\, X_{2} = x) = \mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] - \mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{3}\,|\,X_{2}=x]
$$ Let's deal with the second term first. Clearly we have: $$
\mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{1}\,|\,X_{2}=x] = 
\mathbb{E}[X_{1}\,|\,X_{2}=x]^{2}
$$ Applying the first formula derived by Henry in this question we have $$
\mathbb{E}[X_{1}\,|\,X_{2}=x]^{2} = \frac{x^{2}}{4}
$$ Now for the first term we have $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \mathbb{E}[(x-X_{3})X_{3}\,|\, X_{2} = x] = x\mathbb{E}[X_{3}\,|\, X_{2} = x] - \mathbb{E}[X_{3}^{2}\,|\, X_{2} = x]
$$ Note how the conditional in the expectation remains as $X_{3}$ is still conditioned on $X_{2}$ . This is what caused the issue with my analysis! Following a similar logic as above with $X_{3}$ in place of $X_{1}$ we have: $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \frac{x^{2}}{2} - \mathbb{E}[X_{3}^{2}\,|\, X_{2} = x]
$$ Adding and subtracting $\mathbb{E}[X_{3}\,|\,X_{2} = x]^{2}$ we have $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \frac{x^{2}}{2} - (\mathbb{E}[X_{3}^{2}\,|\, X_{2} = x] - \mathbb{E}[X_{3}\,|\,X_{2} = x]^{2}) - \mathbb{E}[X_{3}\,|\,X_{2} = x]^{2}
$$ Observe that the term in the brackets is simply the conditional variance of $X_{3}$ . Hence, using the second identity provided by Henry in the aforementioned question we have: $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \frac{x^{2}}{2} - \frac{1}{2} - \mathbb{E}[X_{3}\,|\,X_{2} = x]^{2}
$$ Recall that we calculated the leftover term (with $X_{1}$ in place of $X_{3}$ . Plugging in our solution we have: $$
\mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] = \frac{x^{2}}{2} - \frac{1}{2} - \frac{x^{2}}{4} = \frac{x^{2}}{2} - 1/2
$$ Putting everything together we end up with: $$
\text{Cov}(X_{1}, X_{3} \,|\, X_{2} = x) = \mathbb{E}[X_{1}X_{3}\,|\, X_{2} = x] - \mathbb{E}[X_{1} \,|\,X_{2}=x]\mathbb{E}[X_{3}\,|\,X_{2}=x] \\ = \frac{x^{2}}{2} - 1/2 - \frac{x^{2}}{4} = -1/2
$$ Finally, we arrive at the correct result! Note that I have left some details out regarding how the conditional expectations and variances used from Henry's question are calculated. Although, I believe a question which presents the working for a similar problem is linked there. I may add these derivations later but for now I am happy to assume that Henry is a divine oracle capable of correctly computing the conditional moments of normal distributions :).","['causality', 'statistics', 'probability']"
4518531,Another definition of Riemann integrability,"This is the definition of Riemann integrability in terms of Riemann sums instead of upper and lower Darboux sum definition. I want to show using ONLY this definition that The Dirichlet function $f : [0, 1] → \mathbb R$ , defined by $$f(x) = \begin{cases}
1, & x ∈  \mathbb Q \\
0, & x ∈ [0, 1] - \mathbb Q
\end{cases}$$ is not Riemann Integrable. How do we do that? How do we negate the statement? Can anyone help?","['integration', 'riemann-integration', 'analysis', 'real-analysis']"
4518533,integration with respect to time,"I am pretty sure this is a really basic question, but after the summer, not using integrals / derivatives at all, I just can not remember how to do math anymore. I have a function for acceleration that I need to integrate with respect to time $t$ in order to obtain speed. My acceleration function is (for angle $\phi$ ) $$\ddot \phi = -2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)}$$ where $$\ddot \phi = \frac{d^2}{dt^2}\phi $$ and $$\dot \theta = \frac{d}{dt}\theta $$ and $$\dot \phi = \frac{d}{dt}\phi$$ and so on. so I would need to obtain the $\dot \phi$ by integrating the acceleration once. $$\dot \phi = \int \ddot \phi dt= \int \Big(-2 \frac{\dot r}{r} \dot \phi - \dot \phi \dot \theta \frac{cos(\theta)}{sin(\theta)} \Big) dt$$ I just do not remember at all how to start to process this problem. I know it's almost a bit shameful but indeed could use a helping hand here. (I have 2 similar equations to be solved for $\ddot \theta$ and $\ddot r$ but I am pretty sure I get these done when I once remember how to calculate things) Thank you so much if you could help me out. Those 2 other equations are following : (if someone is interested) $$\ddot{r} = r \dot \theta ^2 + r\dot\phi^2 sin^2(\theta) -\frac{GM}{r^2}$$ $$\ddot \theta = \dot \phi^2 sin(\theta)cos(\theta) - 2 \frac{\dot r}{r} \theta$$","['integration', 'multivariable-calculus', 'calculus', 'derivatives']"
4518561,A map which is homeomorphism and isometry but not diffeomorphism,"The question comes from this about metric space which is also smooth manifold. Existence of a Riemannian metric inducing a given distance. Alexandrov proved that Suppose that $(𝑀,𝑑)$ is a locally compact finite dimensional path-metric space with curvature locally bounded above and below and extendible geodesics. Then $𝑀$ is homeomorphic to a smooth manifold $𝑀′$ and, under this homeomorphism, the distance function
is isometric to the distance function coming from a Riemannian metric $g$ on $𝑀′$ , the regularity of the metric tensor $g$ is $𝐶_{1,\alpha}$ My question is that: Does there exist cases where a smooth manifold $M$ , which is metric space, is homeomorphic and isometric to another smooth manifold $M'$ , which is Riemannian metric space, and this map is not diffeomorphism? Can you write it in explicit formulas?","['riemannian-geometry', 'metric-geometry', 'metric-spaces', 'geometry', 'differential-geometry']"
4518636,Prove/disprove that $U$ and $V$ are disjoint.,"Let $x, y \in \mathbb{R}^{n}$ such that $x \neq y, -y$ and $|x| = 1 = |y|$ , and take $r = \min \{ |x - y|, |x + y|, 1\}$ . Let $U = \bigcup_{0 \neq \lambda \in \mathbb{R}}{B_{|\lambda| r}(\lambda x)}$ and $V = \bigcup_{0 \neq \lambda \in \mathbb{R}}{B_{|\lambda| r}(\lambda y)}$ . I'm trying to prove that $U$ and $V$ are disjoint by assuming that $z \in U, V$ , which implies that there exist nonzero $\lambda_{x}, \lambda_{y} \in \mathbb{R}$ such that $|z - \lambda_{x} x| < |\lambda_{x}| r$ and $|z - \lambda_{y} y| < |\lambda_{y}| r$ . But I'm stucked at this point because I don't know how to arrive at a contradiction. But $U$ and $V$ being disjoint is only a hypothesis, and so $U$ and $V$ might not be necessarily disjoint. I really appreciate it if someone could give an idea/hint on how to prove this, or if someone could come up with a counterexample.","['general-topology', 'normed-spaces', 'metric-spaces', 'analysis']"
4518642,$\frac{dy}{dx}$ Vs. $\dfrac{1}{\frac{dx}{dy}}$,"Today, my friend gave me a question which is stated below: On the curve $x^3 = 12y$ , the abscissa changes at a faster rate than the ordinate. Then find the interval in which $x$ belongs to. I did it as follows: Differentiating $x^3 = 12y$ wrt $y$ , $$3x^2\frac{dx}{dy} = 12$$ $$\implies \frac{dx}{dy} = \frac{12}{3x^2}\quad, x \not = 0$$ $$\implies \frac{dx}{dy} = \frac{4}{x^2}\quad, x \not = 0$$ Now $\frac{dx}{dy}$ must be greater than $1$ so that the rate of change of abscissa is greater than rate of change of ordinate. So, $$\dfrac{4}{x^2} > 1\quad, x \not =  0\implies \boxed{x \in (-2, 2) - \{0\}}$$ which is correct according to our book. Whereas, my friend solved it as follows: Differentiating $x^3 = 12y$ wrt $x$ , $$\implies 3x^2 = 12 \dfrac{dy}{dx}$$ $$\implies \dfrac{3 x^2}{12} = \dfrac{dy}{dx}$$ $$\implies \dfrac{x^2}{4} = \dfrac{dy}{dx}$$ Now, $\frac{dy}{dx}$ must be less than $1$ so that the abscissa changes at a faster rate than the ordinate. So, $$\frac{x^2}{4} < 1 \implies\boxed{ x \in (-2, 2)}$$ What's the mistake? Whose answer is correct?","['calculus', 'derivatives']"
4518655,(Pseudo-)tensor densities as sections of bundles,In the wiki article on tensor densities ( https://en.wikipedia.org/wiki/Tensor_density ) it is mentioned that this notion may be understood in terms of sections of tensor product of the density bundle with tensor bundles. I would like to clarify if I am correct in thinking that (due to the determinant in transition functions being/not being taken with absoulte value) this only works for pseudotensor densities -- tensor densities are instead sections of tensor product of just the determinant bundle with tensor bundles?,"['tensors', 'differential-geometry']"
4518662,Generalizing a formula for the sum of three sines,"For the sum of two cosine functions , we have the formula: $$A\sin \alpha + B\sin \beta = (A+B)\sin\left(\frac{\alpha+\beta}{2}\right)\cos\left(\frac{\alpha-\beta}{2}\right)+(A-B)\sin\left(\frac{\alpha-\beta}{2}\right)\cos\left(\frac{\alpha+\beta}{2}\right)$$ We can write this as: $$A\sin \alpha + B\sin \beta = \sum_{k=1}^2\lambda_k(A,B)P_k^{(2)}(\alpha,\beta)$$ where $\lambda_i(A,B)$ are linear functions in the coefficients $A,B$ , and $P_k^{(2)}(\alpha,\beta)$ are quadratic trigonometric polynomials, namely: $$\lambda_1(A,B) = A+B, \qquad \lambda_2(A,B) = A-B$$ $$P_1^{(2)} = \sin\left(\frac{\alpha+\beta}{2}\right)\cos\left(\frac{\alpha-\beta}{2}\right), \qquad P_2^{(2)} = \sin\left(\frac{\alpha-\beta}{2}\right)\cos\left(\frac{\alpha+\beta}{2}\right)$$ For three sine functions , I conjecture that there is a formula of the form: $$A\sin \alpha + B\sin \beta + C\sin \gamma = \sum_{k=1}^2\lambda_k(A,B,C)P_k^{(3)}(\alpha,\beta,\gamma)$$ $\lambda_k(A,B,C)$ linear funtions in $(A,B,C)$ , and $P_k^{(3)}(\alpha,\beta,\gamma)$ cubic trigonometric polynomials. My second attempt was to use the formula: $$\begin{align} \prod_{k=1}^n \cos \theta_k & = \frac{1}{2^n}\sum_{e\in S} \cos(\epsilon_1\theta_1+\cdots+\epsilon_n\theta_n) \\[6pt]
& \text{where }e = (\epsilon_1,\cdots,\epsilon_n) \in S=\{1,-1\}^n
\end{align}$$ thus we have $\epsilon_i = \pm 1$ , this can be used for computing a sum of three cosines, which can then be easily converted into a sum of three sines. Note : The problem arises wave mechanics whether it would be interesting to sum up an arbitrary number of complex exponential functions as pre-factor + complex exponential: $$\sum_{i=1}^n A_i e^{ik_ix} = B(A_1,\dots,A_n;k_1,\dots,k_n;x) e^{i(k_1+\dots+k_n)x/2}$$","['calculus', 'trigonometry', 'fast-fourier-transform', 'sequences-and-series']"
4518679,Calculate object position in a perspective view,"I am trying to detect a court field from a video stream via machine learning.
In my scenario I have a court field with known dimensions: width: 10m height: 20m (10m per field) height of the net: 0,88m I am already able to detect the upper and lower bounds of the court, as well as the top edge of the net. As the bottom edge of the net contains no usable visual cues, I am trying to calculate the bottom edge based on the known dimensions. In the following picture you can see the detected lines in black. I want to calculate the red line which maps to the middle line of the court. The perspective is not fixed, as the perspective might slightly change in the video stream or between streams. Thanks in advance for your input!",['trigonometry']
4518686,Multiple Graph Transformations,"$y = x^n $ is transformed to $0.5(3x+2)^n - 1$ . Describe the transformations. Not too sure on this one, but I factorised $3x+2$ to $3(x+2/3)$ , giving me a translation $-2/3$ to the left followed by a stretch scale factor $1/3$ parallel to x-axis. After that, it would be stretch scale factor $1/2$ parallel to y-axis followed by a translation 1 unit down. This definitely doesn't seem right though. My only other thought would be to find the inverse of $3x+2$ which is $(x-2)/3$ . Cheers guys","['graphing-functions', 'functions', 'solution-verification', 'linear-transformations', 'transformation']"
4518698,modular forms mod p and their liftings,"Deligne in ""Courbes elliptique: Formulaire (d’après J. Tate)"" computes the ring of modular forms mod $p$ . He proves that if $p>3$ , then the space of modular forms is isomorphic to $\mathbb{F}_p [c_4, c_6]$ but, for example, for $p=3$ one has $\mathbb{F}_3 [b_2, \Delta]$ where $b_2$ is of weight $2$ and $\Delta$ is of weight $12$ . In particular, there is a nonzero weight $2$ modular form mod $3$ . This modular form should not lift integrally and my question is: does it lift mod $9$ ? If yes, what is the minimal $k$ such that $b_2$ does not lift mod $3^k$ ?","['number-theory', 'algebraic-geometry', 'elliptic-curves']"
4518749,Noetherian and Hausdorff space,"A Noetherian space satisfies that any non-empty set of open subsets must have a maximal element. But a Hausdorff space satisfies that, for $x\neq y$ two different points, there exists open neighborhoods $U,V$ containing $x,y$ respectively such that $U\cap V=\emptyset$ . My question is: when we consider the set of open subsets $$\{U,V\}$$ there is no maximal element in this set. So how can a Noetherian space be Hausdorff too? Thank you!","['general-topology', 'algebraic-geometry', 'noetherian']"
4518793,High probability concentration bound for norm of multivariate normal distribution,"Given a multivariate normal random variable $X \sim \mathcal{N}(0, \Sigma_{d\times d})$ , I am looking for an concentration bound of the following form: $$\mathbb{P}( \|X\|_2 \leq C) \ge 1- \delta.$$ What can we have for $C$ here? I am thinking $C$ can be a polynomial of $d$ , $\log(d/\delta)$ and other terms such as eigenvalues or trace of $\Sigma_{d\times d}$ . The closest I could find was if $\eta \sim \mathcal{N}(0, \Lambda^{-1})$ , then $$\mathbb{P}( \|\eta\|_\Lambda \leq c \sqrt{d \log (d/\delta)} \ge 1 - \delta,$$ where $c$ is a constant.","['measure-theory', 'concentration-of-measure', 'probability-theory', 'normal-distribution']"
4518798,Statement about primitive sets,"After watching Numberphile´s video with Jared Litchman about primitive sets I started playing around with this notion (in case you didn't see it: a set $S\subset \mathbb{N}$ is a primitive set if no number in the set divides another one) and I thought about the following statement that I believed to be true: Given a set $S\subset \mathbb N$ let $P(S)=\{p\in\mathbb P:\;\exists a\in S\;\;\text{such that}\;\;p\mid a\}$ (with $\mathbb P$ the set of prime numbers). The claim is that given a primitive set $S$ such that $P(S)$ is finite, then $S$ is finite. The thing is that I could only give a proof for this if $P(S)=2$ , I'll put my proof below (it isn't really elegant or anything though), but I couldn't generalize it for bigger cases (not even if $P(S)=3$ ); so I'd really appreciate if someone notices how to generalize it, or if you have a proof for bigger cases. My proof: The idea is to suppose that $S\subset \mathbb N$ is an infinite set such that $P(S)$ has two elements, and prove that $S$ can't be a primitive set.
Let $P(S)=\{p,q\}$ , then we can describe $S$ as follows $S=\{p^\alpha q^\beta:\;(\alpha,\beta)\in I\}$ for some $I\subset \mathbb N^2$ (notice that there is a trivial one-to-one correspondence between $S$ and $I$ , so I might use alternatingly $p^\alpha q^\beta$ and $(\alpha, \beta)$ ). Let $\alpha_0=\min\{\alpha\in \mathbb N/\;\exists \beta\in \mathbb N:\;(\alpha,\beta)\in S\}$ , and $\beta_0$ be such that $(\alpha_0,\beta_0)\in S$ , and consider $S'=S\setminus\{(\alpha_0, \beta_0)\}$ . Now, given $(\alpha,\beta)\in S'$ ,  if $\beta\geq \beta_0$ , the problem is finished since we know that $\alpha\geq \alpha_0$ , and then we'd have that $p^{\alpha_0}q^{\beta_0}\mid p^\alpha q^\beta$ , and then $S$ isn't primitive; so, by way of contradiction let us assume that for all $(\alpha, \beta)\in S'$ we have that $\beta<\beta_0$ . Lemma: given $N\in \mathbb N$ there exists $\alpha>N$ and $\beta\in\mathbb N$ such that $(\alpha,\beta)\in S'$ . Let's assume that there is an $N\in \mathbb N$ such that the statement isn't true, then, given $(\alpha, \beta)\in S'$ we have that $\alpha_0\leq\alpha\leq N$ and $0\leq \beta<\beta_0$ , and this implies that $\#S'\leq (N+1)\beta_0$ , which is absurd since the set is infinite, so the lemma is true. Going back to what we are trying to prove, let $\beta_1=\min\{\beta\in\mathbb N/\;\exists\alpha:\;(\alpha, \beta)\in S'\}$ and let $\alpha_1$ be such that $(\alpha_1,\beta_1)\in S$ . Then by the lemma there exists $\alpha>\alpha_1$ and $\beta\in\mathbb N$ such that $(\alpha,\beta)\in S'$ . Lastly, as $\alpha>\alpha_1$ and $\beta\geq \beta_1$ we have that $p^{\alpha_1}q^{\beta_1}\mid p^\alpha q^\beta$ , which proves that $S'$ isn't primitive, and therefore $S$ neither.","['number-theory', 'prime-numbers']"
4518803,Infinite product of Banach algebra is a Banach algebra or not,"First I am trying to show infinte product of banach spaces is banch or not. It is in the book Functional Analysis by John B. Conway . It is proposition 4.4 in chapter 3. It says that if $\{ \mathcal{H}_i. : i\in I\}$ be a collection of Banach spaces and let $\mathcal{H}=\oplus _p \mathcal{H}_i, 1 \leq p < \infty $ . Then $\mathcal{H}$ is also Banach space. Where $\mathcal{H}=\oplus _p \mathcal{H}_i= \{ x\in \prod_i \mathcal{H_i}: \|x\|={[\sum_i\|x(i)\|^p]}^{\frac{1}{p}} < \infty \}$ . My problem: Let $\{x_n\} \subset \mathcal{H} $ be a Cauchy sequence. Then each $\{ x_n(i) \}$ will be also a Cauchy sequence in $\mathcal{H}_i$ .lets say $x_n(i)$ converges to $x(i)$ so I will get $x=\prod_i x(i)$ . Now I am struggling to show $x\in \mathcal{H}$ and even if I show that I am unable to show $x_n$ converges to $x$ in this norm. If I take $p=\infty$ then I was able to show that thing but here it is problematic. Can anyone help me to arrange $\epsilon$ and $\delta$ . Now If I have all $\mathcal{H}_i$ are banach algebras how to define multiplication of $\mathcal{H}$","['banach-spaces', 'operator-theory', 'functional-analysis', 'analysis']"
4518809,How to reduce the mean on a dataset without changing the order of the data,"I created a list of anime I have completed and for each anime that I have completed I give it a score based on how good I thought it was. Now over time there are like 200+ anime in that list. But I now feel like my scores are very inflated. I want to change my scores such that it reduces the mean score a bit. Also whatever I do to the score of a single anime must be done to all the scores on the list to keep it fair. So I was wondering if there is a mathematically correct way of changing each score in such a way that for any 2 anime A and B, if A had a higher score than B before the transformation, it must have a higher score than B after the transformation. Some simple transformation like reducing every score by some amount 'x' would not work since the minimum score you can have is 0 and if there is an anime which had score < x before the transformation then after the transformation it will end up having a negative score which is not allowed. I just want some way to shift the whole bell curve to the left a bit properly. There has to be a proper function that can do this right? I am not that good at math so please forgive me if my description of the problem is not technical enough. Edit: Thanks to bobeyt6 for suggesting I should divide every value with a number to reduce the mean. But I still have a question. If I want my mean to go to a specific number. Lets say my mean right now is 7.6 and I want it to be exactly 5 after I do the transformation, how do I pick which number I should divide with?",['statistics']
4518832,Relationship between GIT and coarse moduli spaces,"I'm trying to understand how a generic algebraic geometer constructs coarse moduli spaces.  I'm familiar with the definition, and how it is usually quite involved to show that a space has the coarse moduli property.  It seems that Mumfords GIT takes care of this somehow, although it seems to me that GIT is a way to construct 'nice' quotients.  How does this interact with the moduli property?","['geometric-invariant-theory', 'algebraic-geometry', 'moduli-space']"
4518846,How to construct the unity partition on $\mathbb{S}^1$?,"How to construct the unity partition on $\mathbb{S}^1$ ? I was reading the construction of unity partition on $\mathbb{S}^1$ given in https://en.wikipedia.org/wiki/Partition_of_unity . I would like to understand the steps please. First, I know that $\mathbb{S}^1=\left \{(x,y)\in \mathbb{R}^2:x^2+y^2=1\right \}$ and I need to find two functions $\psi _i:\mathbb{S}^1\to [0,1]$ , $i=1,2$ and for every $p\in \mathbb{S}^1$ we need that $\psi _1(p)+\psi _2(p)=1$ . Wikipedia used the function $\Phi (t)=e^\frac{1}{t^2-1}$ , for $t\in (-1,1)$ . Now i want to connect that function with the functions $\psi _i$ . They choose $\psi _1=\Phi$ and $\psi _2=1-\Phi$ , but I do not understand why it works since the domain of $\psi _i$ need to be $\mathbb{S}^1$ instead of $(-1,1)$ . Can somebody help me to understand what is the idea behind of this or I do not get it?  Thank you.","['integration', 'multivariable-calculus', 'calculus']"
4518847,Yet another proof of the Abelianess of groups of order $255$?,"Seeking for one more proof to this claim (namely, that a group of order $255=3.5.17$ is Abelian), I'm looking for an argument (if any) which rules out the case $P\cap Z(G)=\{1\}$ , where $P$ is the (one) normal subgroup of order $17$ of $G$ . If I could get this, I think that $G/P$ cyclic and $P$ central would lead to the desired result (likewise the usual argument about $G/Z(G)$ ). What I know . The nontrivial elements of $P$ must lay in two conjugacy classes (of $G$ ) of size $3$ and in two other of size $5$ (overall $1+3+3+5+5=17$ ). How can I complete this argument?","['abelian-groups', 'group-theory', 'cyclic-groups', 'finite-groups']"
4518856,Spot it! and geometry over finite fields.,"Problem: The party game “Spot It!” features 55 cards, each of which has eight symbols printed on it, in such a way that any two cards have exactly one symbol in common. (In the game, each player looks at a pair of cards and tries to find their common symbol.) The “Junior” version of the game has 30 cards with six symbols each. How do you use geometry over finite fields, as in the parts above, to build decks of cards with the required property? (Note: Spot It decks don’t quite have the optimal number of cards.) My work: This question is the third part of a problem. I already solved the two parts before where I prove the following formulas: (I) The number of $m$ -dimensional subspaces of an $n$ -dimensional vector space over a field with order $q$ is $$\frac{(q^n - 1)(q^n - q)\ldots(q^n - q^{m-1})}{(q^m - 1)(q^m - q)\ldots(q^m - q^{m-1})}$$ (II) The number of two-dimensional spaces that contain a given 1-dimensional subspace is $$\frac{q^n - q}{q^2 - q} = \frac{q^{n-1} - 1}{q - 1}$$ So I am supposed to use those formulas or a similar method to solve the problem. I already read this but none of the answers talks about the approach I am looking for, the most similar thing I was able to find is Example 4 of this answer but still doesn't solve my doubts. I tried thinking of a field $F$ whose order $q$ is the number of symbols, then the set of all cards are all elements of $F^8$ where any two vectors only share a single coordinate or equivalently, a symbol. Since any two cards must share a single symbol or equivalently, a one-dimensional subspace of $F^8$ $$\frac{q^{7} - 1}{q - 1} = 2 \implies q^7 - 2q + 1= 0$$ but then I realised that the argument was wrong and didn't make much sense and that is why the equation yields wrong answers for the number of total symbols. This section of the Wikipedia article about projective planes seems useful but I don't know how to apply it to the problem. I would appreciate any hints on how to get started. Thanks in advance.","['linear-algebra', 'card-games', 'combinatorics']"
4518857,Obtaining Classical Wiener Space from abstract Wiener measure,"The question I'm working on understanding the Abstract Wiener Space construction and wanted to rederive the defining property of the classical counterpart, $$\require{cancel} \xcancel{\xi_{t+s} - \xi_t}\ B_{t+s} - B_t\sim \mathcal{N}(0, s) \quad(\text{for } s> 0), \tag{1}$$ from the Abstract Wiener Space as constructed on the Wikipedia page and here . The setup That is, I assume to know that $\xi \in W^{2,1}_0[0,T] =: \mathcal{H}$ , i.e., $\xi: [0,T] \to \mathbb{R}$ is an absolutely continuous path with square-integrable first derivative and $\xi(0) = 0$ . The inner product on $\mathcal{H}$ shall be defined as $$(\xi, \zeta) = \int_0^T \dot{\xi}(t) \dot{\zeta}(t) \,\mathrm{d}t, \tag{a}$$ and from the construction we know there exists a measure $\mu$ which acts on the algebraic dual space $E^a$ and which has the properties (Defs. 20 and 25 of Velhinho) $$\forall\, \xi \in \mathcal{H}: \quad \chi(\xi) := \int_{E^a} e^{i\phi(\xi)}\, \mathrm{d}\mu(\phi) = e^{-\frac{1}{2}(\xi,\xi)} \tag{b}$$ and (Theorem 11, which I believe is a special case of the cylinder set measure property, correct me if I'm wrong) $$\forall\, \xi \in \mathcal{H}, A \in \mathcal{B}(\mathbb{R}): \quad \mu_\xi(A) := \mu(\{\phi \in E^a \mid \phi(\xi) \in A \}) = \frac{1}{\sqrt{2\pi (\xi,\xi)}} \int_A e^{-\frac{x^2}{2(\xi,\xi)}} \,\mathrm{d}x. \tag{c}$$ My attempt at a solution ( see edit below! ) I have the following idea, but I'm not sure if it's right: What we want to show in (1) is equivalent to $$P(\{\xi(t+s) - \xi(t) \in A\}) = \frac{1}{\sqrt{2\pi s}}\int_A e^{-\frac{x^2}{2s}} \,\mathrm{d}x$$ for any Borel set $A \in \mathcal{B}(\mathbb{R})$ .
We may note that $$\xi(t+s) - \xi(t) =  \int_t^{t+s} \dot{\xi}(\tau)\, \mathrm{d}\tau = \int_0^{T} \dot{f}(t) \dot{\xi}(\tau)\, \mathrm{d}\tau \quad \text{with } \dot{f}(\tau) = 1_{[t,t+s]}(\tau).$$ Now, since every $\xi \in \mathcal{H}$ will also have a dual element $\phi_\xi \in E^a$ and $f$ as implicitly defined above is a valid element of $\mathcal{H}$ , we can swap the roles of $\xi$ and $f$ to define $$\xi(t+s) - \xi(t) =: \phi_\xi(f).$$ Then we can apply (c) to find that $$\mu_f(A) = \frac{1}{\sqrt{2\pi (f,f)}}\int_A e^{-\frac{a^2}{2(f,f)}}\,\mathrm{d}x = \frac{1}{\sqrt{2\pi s}}\int_A e^{-\frac{a^2}{2s}}\,\mathrm{d}x,$$ where we used $(f,f) = s$ via definition (a). This looks like the right result, but the way there seems a bit odd, and it also glosses over the fact that the set that is measured in (c) includes $\phi$ that are not duals $\phi_\xi$ of some $\xi$ . Is it obvious that these will contribute with measure zero? Feel free to point me to a good introductory text that gives more context, if necessary! Edit: Revised attempt I found part of the problem:
In (1), $\xi$ was the Brownian motion itself, whereas in (a) through (c), $\xi$ was an element of the Cameron–Martin Hilbert space. However, the Brownian motion sample paths are not elements of the Hilbert space, but rather of the Banach space $E^a$ . Thus, I should replace (1) with $$B_{t+s} - B_t \sim \mathcal{N}(0, s)$$ to make the distinction between $\xi$ and $B$ obvious. Then the rest of the argument continues as before, i.e. $$
B(t+s) - B(t) = \int_0^T \dot{B}(\tau) \dot{\xi}(\tau)\, \mathrm{d}\tau \quad \text{with } \dot{\xi}(\tau) = 1_{[t,t+s]}(\tau)
\implies \mu_\xi(A) = \frac{1}{\sqrt{2\pi s}} \int_A e^{-x^2/(2s)} \,\mathrm{d} x,
$$ but we now face a different problem, namely that $B(\tau)$ is a.s. not differentiable and that hence the integral that I just wrote does not exist for a.e. $B$ . I've considered interpreting $\dot{B}$ in the distributional sense, but $\xi$ is not a test function, so that doesn't seem to work, I don't know if there's some closure/completion/limiting property that can be used instead.","['wiener-measure', 'stochastic-processes', 'measure-theory']"
4518884,"If $\frac{x^2+y^2+x+y-1}{xy-1}$ is an integer for positive integers $x$ and $y$, then its value is $7$.","I saw this on quora
and haven't been able to solve it. If $\dfrac{x^2+y^2+x+y-1}{xy-1}$ is an integer for positive integers $x$ and $y$ ,
then its value is $7$ . If $y=1$ this is $\dfrac{x^2+x+1}{x-1}
= x+2+\dfrac{3}{x-1}$ which is an integer only when $x=2$ or $x=4$ and has value $7$ . Looking at the values from 2 to 20
for $x$ and $y$ ,
this is an integer only for $x=2, y=12$ (and $x=12, y=2$ ).
This value is 7. So it looks like this might be correct,
and I don't know how to show it.","['elementary-number-theory', 'algebra-precalculus', 'vieta-jumping']"
4518907,Convergence almost surely of the sample mean,"In a probability textbook I have been working through, I came across the following exercise involving almost sure convergence for the sample mean of a given sequence of random variables and was unsure how to proceed with the problem. For an independent sequence of random variables, for each value of $i\geq 1$ , $\mathbb{P}(X_i=i^2-1)=i^{-2}$ and $\mathbb{P}(X_i=-1)=1-i^{-2}$ . The sequence of random variables $\displaystyle \frac{1}{n}\sum \limits _{i=1}^nX_i$ converges almost surely to a constant. Find this constant. The progress that I have made on the problem is that we know that $$\sum \limits _{i=1}^\infty \mathbb{P}(X_i\neq -1)=\sum \limits _{i=1}^\infty i^{-2}<\infty .$$ And so by the first Borel-Cantelli Lemma, the probability of this event occurring infinitely often is equal to $0$ . For almost sure convergence, we need to show that $$\mathbb{P}\left (\left \{\left |\overline X_n-\overline X\right |\geq \varepsilon \right \}_{\text{io}}\right )=0$$ (where $\text{io}$ refers to an event that occurs “infinitely often”) However, it is unclear to me how the progress I have made helps us in this particular instance and would be grateful for any additional guidance.","['statistics', 'almost-everywhere', 'convergence-divergence', 'probability', 'random-variables']"
4518931,find all n so that eventually all lamps will be turned off,"We have $n\ge 2$ lamps $L_1,\cdots, L_n$ in a row, each of them being either on or off. Every second we simultaneously modify the state of each lamp as follows: If the lamp $L_i$ and its neighbours (only one neighbour for $i=1$ or $i=n$ , two neighbours for other $i$ are in the same state, $L_i$ is switched off; Otherwise, $L_i$ is switched on. Initially, all the lamps are off except the leftmost one. For example, if we take $n=4$ and let the binary sequence $b_1\cdots b_n$ represent the current configuration, where $b_i=1$ if lamp $L_i$ is on and $0$ otherwise, then we initially have the consecutive configurations $1000, 1100, 0110, 1111, 0000.$ Find all $n\ge 2$ so that eventually all lamps will be turned off. Note that no odd value of n will work, as the proof below shows. Indeed, one can prove by induction on the number of steps (after the first step, when the sequence consists of $2$ ones followed by $n-2$ zeroes) that the sequence always has a form consisting of blocks of ones of even length where consecutive blocks are separated by blocks of zeroes of even length ( $\ge 2$ ). All entries between the second and second last entries of any block will become 0's in the next step. Then the first and last entries will be ones. So each block of zeroes will have its first and last entries replaced by 1's and hence remain even in length while each block of ones will have all but two entries replaced with zeroes. But the two entries that remain at one, assuming the block of ones is preceded and followed by blocks of zeroes, form two groups of two with the closest element in the neighbouring block of zeroes. In the special case when the block includes the first position, the first position becomes zero. If the block of ones includes the last position, the last position becomes zero. And so if the block of ones includes the first and last positions, it becomes zero in the next move. Otherwise, assume WLOG that a block of zeroes of even length follows the block and the block includes the first position. Then the block is replaced by a block of two ones, one following the block and one in the last position of the block. This shows that after one step the described form is maintained. So in particular, the above proof shows that there will always be a block of ones of even, positive length, assuming one never obtains a full block of ones of even length. Hence no odd value of n will allow all lamps to eventually be turned off. Also the next proof shows that any power of two will work: For notational purposes, let for a digit $ x, x^{(i)}$ denote the digit $x$ repeated $i$ times in a sequence. We can prove by induction that for $n = 2^k,$ we can get a sequence ending in $1^{(2^k)}$ after $2^k - 1$ steps. For $n = 2^{k+1},$ by the inductive hypothesis, we can get the block $1^{(2^{k})}0^{(2^k)}$ because the last one after the mth step is at position m + 1 (and all other entries are zeroes, so they can effectively be ignored). This takes $2^k - 1$ steps by the inductive hypothesis. Then we can get $0^{(2^k-1)} 11 0^{(2^k-1)}$ after one step. Then one can prove that after $2^{i-1}$ steps, starting with a block of size $2^i$ in the center, we can get a block of size $2^{i+1}$ in the center. In the base case where $i = 1,$ we get $0^{(2^k-2)}11110^{(2^k-2)}$ after one more step. Assuming the claim holds for i, starting with a block of ones size $2^(i+1)$ in the center, after the first step, we get the sequence $110^{(2^(i+1) - 2)} 11$ in the center. Then after the mth step, we replace the first and last zeroes in the center with 1's and we replace the zeroes right before and after the sequence with ones, assuming there are still zeroes. So after each step we reduce the length of the zero block in the center by 2 (except for the first one, in which case the length of the zero block becomes $2^{i+1}-2$ for the first time). Hence after $2^i$ steps the zero block in the center becomes a one block of size $2^{i+1}$ and $2^{i+1}$ ones are added to the sides, forming a block of size $2^{i+2}$ . Hence we take $2^k - 1 + 1 + 2 + ... + 2^{k-1} = 2^{k+1}-2$ steps to get to the all ones block of size $2^{k+1}$ . One last step brings us to $2^(k+1)- 1$ steps in total and we get the all zeroes block. But what about other even values of $n$ ? Obviously once a configuration repeats, the sequence of configurations represented in binary will be periodic from that point on. $n=6$ is impossible: we have the sequence of configurations $100000, 110000, 011000, 111100, 000110, 001111, 011000, 111100,$ which is periodic with period 4. Note that the sequence is also symmetric; if one starts with the reverse of a sequence and the terms of the original sequence are $B_1,B_2,\cdots $ , one gets the mirror image back at each step. That is if the new sequence is $A_1,A_2,\cdots $ , then $A_i$ is the reverse of $B_i$ for all $i$ . To prove this, we do it by induction on the number of steps, with the proof for the step being the following: consider a configuration represented in binary, say $B_i.$ Write $B_i = b_1\cdots b_n$ . Note that for $2\leq i \leq n-1,$ lamp $L_i$ is switched on or off iff lamp $L_{n+1 - i}'$ is switched on/off, where $L_{k}'$ is the kth lamp represented by the configuration $A_i$ . For instance, if $L_i$ is switched off, then $L_i$ had the same state as its neighbours $L_{i-1}$ and $L_{i+1}$ . So $L_{n+1 - (i-1)}$ and $L_{n-(i+1)}$ have the same state as $L_{n+1 - i}$ by induction, and hence $L_{n+1 - i}$ is switched off. The other cases are similar. And the case where $i=1$ or $n$ is also similar. For $n=10,$ we get $1000000000, 1100000000, 0110000000, 1111000000, 0001100000, 0011110000, 0110011000, 1111111100, 0000000110$ , and this is also impossible.","['contest-math', 'combinatorics', 'discrete-mathematics', 'sequences-and-series', 'induction']"
4518937,Convergence of $1-\frac{1 \times 3}{3!} + \frac{1 \times 3 \times 5}{5!} +....+ \frac{1 \times 3 \times 5 \times ... \times(2n-1)}{(2n-1)!} - ...$,"I was asked to prove whether $$1-\frac{1 \times 3}{3!} + \frac{1 \times 3 \times 5}{5!}- \frac{1 \times 3 \times 5 \ \times 7}{7!} +....+ \frac{1 \times 3 \times 5 \times ... \times(2n-1)}{(2n-1)!} - ...$$ converges absolutely, conditionally or diverges, and was wondering whether my proof is correct or not. I began by noticing that $$1-\frac{1 \times 3}{3!} + \frac{1 \times 3 \times 5}{5!}- \frac{1 \times 3 \times 5 \ \times 7}{7!} +....+ \frac{1 \times 3 \times 5 \times ... \times(2n-1)}{(2n-1)!} -...$$ $$= 1- \frac{1}{2} + \frac{1}{2\times4} - \frac{1}{2 \times 4 \times 6}+...+\frac{1}{(2n)\times(2n-2)... \times2} -...$$ In other words, each $n$ th term is the inverse of the product of all even natural numbers, and thus the initial sum is simply $ 1 + \sum a_n$ with $$a_n := (-1)^n \frac{1}{\prod_{j=1}^n2j}$$ We can show that $$\lim_{n\to\infty} \frac{|a_{n+1}|}{|a_n|} = \lim_{n\to\infty} \frac{\prod_{j=1}^{n}2j}{\prod_{j=1}^{n+1}2j}=\lim_{n\to\infty} \frac{1}{2(n+1)} = 0 < 1$$ and therefore the series converges absolutely according to the ratio test. Is this proof correct?","['calculus', 'solution-verification', 'sequences-and-series', 'limits', 'convergence-divergence']"
4518961,"Let $A\lhd G$ for a finite, nontrivial nonabelian group $G$. Suppose that $A$ and $G/A$ are abelian. If for each $a\in A-\{1\}, C_G(a)=A$, then $A=G'$","I'm working on proving the following problem: Let $A \lhd G$ for a finite, nontrivial, nonabelian group $G$ . Suppose that $A$ and $G/A$ are abelian. If for each $a \in A - \{1\}$ , $C_G(a) = A$ , then $A = G'$ . I can't seem to prove this.  The way I've been trying to prove this is to assume that $G' \subsetneq A$ , and using the fact that $C_G(a) = A$ to force a contradiction by showing that $G/G'$ is nonabelian, but nothing I'm trying is working. Maybe there is a direct way to show that each element of $A$ is a commutator of some elements of $G$ . Any suggestions would be appreciated.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4518991,Applying the Ricci identity on the Cotton tensor,"The Schouten tensor is given by $$
P_{i j}=\frac{1}{n-2}\left(R_{i j}-\frac{1}{2(n-2)} R g_{i j}\right).$$ I want to compute the divergence of the Bach tensor in dimension $4$ . From this post, we know that the Bach tensor is divergence-free in dimension $4.$ I want to show it explicitly. The divergence of the back tensor $$
\begin{aligned}
\nabla^{j} B_{i j} &=\nabla^{j} ( \nabla^{k} \nabla_{k} P_{i j}-\nabla^{j} \nabla^{k} \nabla_{j} P_{i k})+\nabla^{j}\left(P^{k l} W_{i k j l}\right)  \\
&= (\nabla^{j} \nabla^{k} \nabla_{k} P_{i j}-\nabla^{k} \nabla^{j} \nabla_{k} P_{i j})+\nabla^{j}\left(P^{k l} W_{i k j l}\right) \end{aligned} .$$ The second term requires the divergence of the Weyl tensor, and I have a nice formula for the divergence of the Weyl tensor. I'm struggling to compute the following portion: $$
\begin{aligned}
(\nabla^{j} \nabla^{k} \nabla_{k} P_{i j}-\nabla^{k} \nabla^{j} \nabla_{k} P_{i j}) \end{aligned} .$$ On page 126 of Jiaqi Chen's Doctoral thesis , he says the Ricci identity gives us $$
\begin{aligned}
(\nabla^{j} \nabla^{k} \nabla_{k} P_{i j}-\nabla^{k} \nabla^{j} \nabla_{k} P_{i j}) \end{aligned} $$ equals to $$(\nabla^{j} P^{k l}) R_{i k j l}.$$ I could not derive this equality by the Ricci Identity . Could you help me establish the equality that Jiaqi Chen claimed? Thanks so much. Note that here our connection $\nabla$ is torsion-free and metric compatible. Update: Finally, I did the calculation and it was nasty. I'm still wondering how Dr. Chen did this in just two lines. I'm still looking for a better/short calculation.","['conformal-geometry', 'partial-differential-equations', 'differential-geometry']"
4519009,Proving that the exponential satisfies the following sum equation,"I was thinking about how $(\sum_{n=0}^{\infty} \frac{1}{n!})^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}
$ and was wondering if there existed any other sequences that satisfied this besides the exponential. My gut instinct tells me no, but I'm not sure how to prove it. $(\sum_{n=0}^{\infty} a_n)^x = \sum_{n=0}^{\infty} x^n a_n
$ I managed to show $a_{n+1} < \frac{a_n}{x}$ as $n \to \infty$ for any positive $x$ , but I have absolutely no idea how to go any further.","['power-series', 'summation', 'exponential-function', 'sequences-and-series']"
4519039,"Find a two-dimensional minimal sufficient statistic for $(\theta,j)$.","Let $(X_1,X_2,...,X_n)$ be a random sample from a distribution with discrete probability $f_{\theta,j}$ , where $\theta \in (0,1)$ , $j=1,2$ , $f_{\theta,1}$ is the Poisson distribution with mean $\theta$ , and $f_{\theta,2}$ is the binomial distribution with size $1$ and probability $\theta$ . Find a two-dimensional minimal sufficient statistic for $(\theta,j)$ . Approach: The joint distribution of $X$ is given as $$f_{\theta}(x) = \left[ \frac{e^{- \theta} \theta ^{\sum X_i}}{\prod_{i=1}^{n} X_i !} I_{\{j=1\}}. \theta^{\sum X_i}(1-\theta)^{n-\sum X_i} I_{\{j=2\}} \right]$$ which can be re-arranged as: $$\left[ \frac{e^{-\theta}}{\prod_{i=1}^{n} x_i !} I_{\{j=1\}} (1-\theta)^n I_{\{j=2\}} \right]. \exp\{ \sum X_i \log \theta. I_{\{j=1\}} + \sum X_i \log \left( \frac{\theta}{1-\theta} \right) I_{\{j=2\}} $$ . Seems like $\sum X_i$ is minimal sufficient but turns out it is not sufficient for $(\theta,j)$ .
Intuitively, we should have a two-dimensional statistic to minimal sufficient but I don't have a clue about how to find it.","['statistical-inference', 'statistics', 'probability-theory', 'probability']"
