question_id,title,body,tags
4849040,Proving existence of powers of 10 in a sequence,"I have been given the sequence of numbers $$a(n) = 2^{2n-1}-n$$ and I want to know if any of its members is a power of 10 (other than 1).
The only thing I've figured out is that the $n$ for such a number must be even and not a multiple of five. By searching, I know that there are no powers of 10 in the first 10000 terms, but I don't know how to prove that there are none, if that is the case.","['number-theory', 'perfect-powers', 'sequences-and-series']"
4849079,"A property of centers of circumcircle, incircle and nine point circle","About 2 hours ago I came up with a very cool feature of Euclidean geometry, but it's much more difficult than I can handle, any help would be appreciated. If we draw the circumcircle, the incircle and the nine point circle, then the circle passing through the centers of these three circles is congruent with the nine point circle if and only if it is tangent to the circumcircle. Is this property already known? If the answer is yes, please include references that you can remember. How do we prove that anyway?","['euclidean-geometry', 'geometry']"
4849094,"How do I express the region bounded by $z=x^2, z+y=1, z-y=1$ as a z-simple triple integral?","I've been able to figure out the y-simple and x-simple triple integrals, but I'm having trouble with the z-simple one. I split the region in half over the x-axis, from what I can see, the projection onto the xy-plane of this is a rectangle of $[-1,1] \times [0,1]$ and $x^2 \leq z \leq 1-y$ .","['integration', 'multivariable-calculus', 'volume']"
4849124,I need help evaluating the integral $\int_{-\infty}^{\infty} \frac{\log(1+e^{-z})}{1+e^{-z}}dz$,"I was playing around with the integral: $$\int_{-\infty}^{\infty} \frac{\log(1+e^{-z})}{1+e^{-z}}dz$$ I couldn't find a way of solving it, but I used WolframAlpha to find that the integral evaluated to $\zeta(2)$ or $\frac{\pi^2}{6}$ . How would I go around finding the solution? My experience with integrals is very limited, but I think the solution can possibly be found using the polylogarithm (saw a similar integral use it), but I don't know much of the function so maybe someone on here could help me. I initially tried rewriting the $\frac{1}{1+e^{-z}}$ as a geometric series, but I don't think that works. Then I looked at finding the poles and drawing a contour, but I also don't think that works. Is there some identity that can be used here, which I happen not to know? Any help is appreciated, especially if you could show your working out step by step.","['integration', 'definite-integrals', 'contour-integration', 'polylogarithm', 'zeta-functions']"
4849141,"Difference between 2 solutions, one is correct and the other is wrong, in a permutation problem","The question is:
From 4 oranges, 3 bananas and 2 apples, how many selections of 5 pieces of fruit can be made, taking at least 1 of each kind? My attempt at this is as follow: Choose 1 orange from 4: $\binom{4}{1}$ = 4 ways. Choose 1 banana from 3: $\binom{3}{1}$ = 3 ways. Choose 1 apple from 2: $\binom{2}{1}$ = 2 ways. Choose 2 fruits from the remaining (3 oranges, 2 bananas, 1 apple): $\binom{6}{2}$ = 15 Multiplying these together: $4 \times 3 \times 2 \times 15 = 360$ which is incorrect because this exceed the total number of selections of any 5 fruits without any restriction: $\binom{9}{5}$ . However, other solutions that I have found either: Add up different combinations of the fruit: Is there a shortcut to this combination problem? Take the total number of selections, then subtract each selection that fails to have at least one of a kind. Both of these solutions lead to 98.
I understand the 2 correct solutions but I am being torn up not being able to find out the flaw of my own attempt at the solution. Can someone please point out the difference in my incorrect solution and the correct ones? Why is it wrong?
Thank you so much in advance.","['combinations', 'combinatorics', 'discrete-mathematics']"
4849161,Discretization Error of Mirror Descent,"It is well known that for sufficiently differentiable functions $f$ and small $\eta>0$ the iterate given by gradient descent $$ x_{k+1}=x_k-\eta \nabla f(x_k)$$ is within $\mathcal O(\eta^2)$ of the gradient flow solution $$\dot x(t)=-\nabla f(x(t)), \hspace{1cm}x(0)=x_k$$ which is a consequence of Taylor's Theorem. Now Mirror Descent is a generalization of Gradient Descent, where one considers a strongly convex potential $\psi:\mathbb R^d\rightarrow \mathbb R$ and performs the update $$x_{k+1}=(\nabla\psi)^{-1}\bigg(\nabla\psi(x_k)-\eta \nabla f(x_k)\bigg).\hspace{2cm}(MD)$$ One may similarly derive a continuous-time dynamic given by noting that $$\frac{\nabla \psi(x_{k+1})-\nabla\psi(x_k)}{\eta}=-\nabla f(x_k)$$ and thus the most natural candidate for an ODE describing Mirror descent is given by $$\frac{d}{dt}\nabla\psi(x(t))=-\nabla f(x(t))$$ which in turn yields $$\dot x(t)=-\nabla^2\psi(x(t))^{-1}\cdot \nabla f(x(t)). \hspace{2cm}(CMD)$$ Question: Can we derive a similar error estimate between the Mirror Descent updates (MD) and the continuous-time Mirror Descent (CMD)? (I am particularly interested in the case $\psi(x)=p^{-1}\Vert x\Vert_p^p$ .) ( You may assume $\psi$ to be as smooth as you need it to be ) The naive approach of performing a Taylor Approximation on (CMD) does not seem to yield the desired bound.","['gradient-flows', 'ordinary-differential-equations', 'optimization', 'numerical-methods', 'gradient-descent']"
4849170,"Equivalence of ""area metric"" and Hausdorff metric","I have been studying Viro's textbook ""Elementary Topology"" and in the process of solving it I came across the following problem (in the textbook itself it has the number $4.Px$ ): Prove that on the set of convex polygons the metric $d_\triangle$ is equivalent to the Hausdorff metric. The metric $d_\triangle$ is defined on the set of bounded polygons in the plane and maps a pair of elements of this set to the area of their symmetric difference. The Hausdorff metric is defined on the set of bounded closed subsets of an arbitrary metric space and looks like this: $d_\rho(A,B)=\max \lbrace \underset{a\in A}{\sup}\rho(a,B), \underset{b\in B}{\sup}\rho(b,A) \rbrace $ for bounded closed subsets $A,B$ of the metric space $(X,\rho)$ . To be honest, I don't have any ideas for a solution. The mechanism of applying the classical approach of proving the equivalence of two metrics (namely, demonstrating the inclusion of the first topology in the other and of the second topology in the first) seems unclear to me here, for it is not even fully visual or at least intuitive to see what the open ball represents on each of these metrics.","['general-topology', 'metric-spaces']"
4849185,Does a non-measurable set that is not contained in a null set always contain a set of positive measure?,"My specific problem is with the Lebesgue measure, but out of curiosity I would also appreciate insights on general measure spaces. Suppose I am given a non-measurable set $A$ with the property that there exists no zero measure set which contains all of $A$ . Is it then true that there exists a measurable subset of $A$ with non-null measure? For my specific use case, the underlying space can even be finite-dimensional, in case that makes a difference. I am a graph theorist and woefully uneducated in measure theory.","['measure-theory', 'lebesgue-measure']"
4849208,"Calculate $I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$","Question Evaluate $$I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(xy)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ My try $$I = \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy + \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ $$+ 2 \int_{0}^{\infty} \int_{0}^{\infty} \frac{\ln(x) \cdot \ln(y)}{(x^2 + x + 1)(y^2 + y + 1)} \,dx \,dy$$ $$= 2 \int_{0}^{\infty} \frac{\ln(x)}{x^2 + x + 1} \,dx \cdot \int_{0}^{\infty} \frac{1}{y^2 + y + 1} \,dy \quad (\text{symmetry})$$","['integration', 'definite-integrals', 'improper-integrals', 'multivariable-calculus', 'calculus']"
4849254,Prove that if $AB = BA$ then $\log{(AB)} = \log{(A)} + \log{(B)}$,"Background I am solving an assignment on the fundamentals of Quantum Computing which mostly has questions on Linear Algebra. All matrices and vectors are over complex domains . Question Define the exponent of a matrix as $$e^{\lambda A} = \sum_{i = 0}^{\infty}\frac{\lambda^i}{i!}A^i$$ where $A^0 = \mathbb{I}$ or the identity operator. Given a matrix $B$ , the matrix logarithm of $B$ is defined as the matrix $A$ such that $e^A = B$ . Prove that $\log$ is a non-unique function (i.e. for every $B$ , there are several $A$ that satisfy the matrix formula given). Prove that for two matrices, if $AB = BA$ , then $\log(AB) = \log(A) + \log(B)$ . My approach For this part, I am firstly not sure about the existence of an $A$ , it doesn't seem obvious to me why such an $A$ should always exist and if so, how to compute it. Nonetheless, assuming that we have a solution to $\log(B) = A$ , then I try to prove that I can derive a family of solutions from it (this is my intuition) $$\therefore B = \mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots $$ Assume we have an $A' = A + 2\pi i \mathbb{I}$ such that $$ e^{A'} = \mathbb{I} + (A+2\pi i)\mathbb{I} + \frac{({A + 2\pi i\mathbb{I}})^2}{2!} + \frac{({A + 2\pi i\mathbb{I}})^3}{3!} + \ldots $$ Since $A$ and $\mathbb{I}$ always commute, we can write the sum as (the proof is just algebraic manipulation and application of the binomial theorem, so I've skipped it) $$e^{A'} = \left(\mathbb{I} + A + \frac{A^2}{2!} + \frac{A^3}{3!} + \ldots\right)\left(\mathbb{I} + \frac{2\pi i}{1!}\mathbb{I} + \frac{(2\pi i)^2}{2!}\mathbb{I} + \frac{(2 \pi i)^3}{3!} \mathbb{I} + \ldots \right)$$ $$\therefore e^{A'} = e^A \mathbb{I} \left(1 + \frac{2\pi i}{1!}+ \frac{(2\pi i)^2}{2!}\ + \frac{(2 \pi i)^3}{3!} + \ldots \right)$$ $$\therefore e^{A'} = e^A \mathbb{I} e^{2 \pi i} = B$$ Similarly $\forall n \in \mathbb{Z}, \exp{(A + 2n \pi i \mathbb{I})} = B$ This finishes the proof Assume $e^X = AB, e^Y = A, e^Z = B$ To prove that $X = Y + Z$ We write out the expansions of $A$ and $B$ and then use the fact that $AB = BA$ $$AB = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2YZ + Z^2}{2!} + \frac{Y^3 + 3Y^2Z + 3YZ^2 + Z^3}{3!} + \ldots$$ $$BA = \mathbb{I} + (Y + Z) + \frac{Y^2 + 2ZY + Z^2}{2!} + \frac{Y^3 + 3ZY^2 + 3Z^2Y + Z^3}{3!} + \ldots$$ Put $AB - BA = \mathbb{O}$ $$\therefore (YZ - ZY) + \frac{1}{2!} (Y^2Z - ZY^2) + \frac{1}{2!}(YZ^2 - Z^2Y) + \ldots = 0$$ This is where I am stuck Obviously if $Y$ and $Z$ commute, then the above expression equates to 0 and thus from the expansions of $AB$ and $BA$ , if $Y$ and $Z$ would commute, $AB = BA = e^{Y+Z} = e^X$ and thus our proof is complete. The question is: how do I prove that this is the only case where the summation is 0? If $Y$ and $Z$ don't commute, then is it possible to proceed with the proof? Are there any alternate approaches?","['matrices', 'exponentiation', 'linear-algebra', 'quantum-mechanics']"
4849287,Formula to calculate height $x$ depth cutting area of a band saw with $4$ wheels,"I need help, I am trying to create a formula to calculate the height ( $H$ ) for a bandsaw design. The blade length ( $L$ ) shown in red color, desired cutting depth (W), and wheel diameters $A$ , $B$ , $C$ , $D$ will all be known. The outer edge of the wheels will always align outward horizontally and inward vertically (see 'Cutting Area'). $A$ , $B$ , $C$ diameters will be the same diameter with each other and are all smaller than $D$ . I appreciate the help, I am stuck on this one.","['trigonometry', 'circles', 'geometry']"
4849316,Invertible matrices acting on set of subspaces of the same dimension,"I'm reading a paper and have stumbled upon the following phrases (everything over an arbitrary field $\mathbb{K}$ ): ""As the natural action of $\operatorname{GL}_n(\mathbb{K})$ on the set of linear hyperplanes of $\mathbb{K}^n$ is transitive..."" ""Let $H$ be a subspace of $\mathbb{K}^n$ of codimension $2$ . Since $\operatorname{GL}_n(\mathbb{K})$ acts transitively on the set of $(n − 2)$ -dimensional linear subspaces of $\mathbb{K}^n$ , we may also assume that $H$ is the subspace defined by the following system of
two (independent) linear equations... "" What is this action exactly? The first thing I could think of was that it was simply permutation matrices permuting the basis vectors of each subspace in the set, and my initial reaction was to choose (e.g. for the hyperplane case) the orbit of the vectors with last coordinate $0$ , whence (by permutation), we obtain the subspaces with $0$ each of the $n$ coordinates. However, I stumbled upon the following obstacles: I think (I may be wrong on this) that, for this to be valid, we first need to choose a basis for each subspace, and I don't know if this is permitted. Even after choosing a basis, I can see how to include subspaces, whose basis is a subset of the standard basis, in the orbit, but there are infinitely many hyperplanes and permutting a subset of the standard basis to get a not-so-nice looking basis for another subspace is not clear to me. As I'm writing 2), I realise that I have probably made a choice of basis implicitly when I'm speaking of permutation matrices: They contain $0$ 's and $1$ 's, so they are supposed to permute the vectors of the standard basis, so now I'm asking myself how would we go about permuting vectors of a non-standard basis. Off the top of my head, I can think of Gram-Schmidt, but this does not feel like it solves the problem completely. I'm sorry about the mumbo-jumbo, but I'm really trying to understand this intuitively, so I may be overthinking quite a bit. Is there any merit in my thoughts, or the action does not have anything to do with permutation matrices whatsoever?","['permutation-matrices', 'matrices', 'linear-algebra', 'group-theory', 'group-actions']"
4849390,"If $X+Y$ is independent from $X$ and independent from $Y$, then $X+Y$ is deterministic","Let $X,Y$ be two real random variables such that $X+Y$ is independent from $X$ and also independent from $Y$ . Show that $X+Y$ is deterministic If $X$ and $Y$ have a variance, then we can compute : $$\mathbb E[(X+Y)^2] = \mathbb E[ X (X+Y)] + \mathbb E[Y(X+Y)] =  \mathbb E[ X ]\mathbb E[X+Y] + \mathbb E[Y]\mathbb E[X+Y] = (\mathbb E[X+Y])^2$$ and therefore $\operatorname{Var}(X+Y) = 0$ and $X+Y$ is almost surely constant. In the general case however, I don't see how to prove this. Using the characteristic function $f_X(t) = \mathbb E[e^{itX}]$ , I can show that : $$\forall t,s\in\mathbb R,f_X(t) f_{X+Y}(s) = f_{X+Y}(s+t) f_Y(-t)$$ I am not sure if and how this helps, though.","['fourier-analysis', 'probability-theory', 'probability']"
4849495,Integrating general powers of the Mahalanobis norm with respect to spherical measure,"Let $S^{d-1}$ denote the $d$ -dimensional sphere, and $\sigma_{d-1}$ denote the corresponding spherical measure. I am wondering how to go about solving the following integral for $c>0$ $$
\int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u),
$$ where $M$ is a positive definite and symmetric matrix. The obvious first step is to substitute $v=Mu$ but I am not quite sure how to work with the spherical measure, even for the simpler setting of $c=1$ . In the setting where $M=I$ then $\|Mu\|= \|u\|=1$ whenever $u \in S^{d-1}$ , so here we have $$
\int_{S^{d-1}} \|M u\|_2^{2c} d \sigma_{d-1}(u)
=
\int_{S^{d-1}} \|u\|_2^{2c} d \sigma_{d-1}(u)
=
\int_{S^{d-1}} d \sigma_{d-1}(u)
= S(d)
$$ where $S(d)$ is the surface area of the $d$ -dimensional sphere. Any hints or references to relevant material for dealing with the more general setting would be appreciated!","['integration', 'definite-integrals', 'multivariable-calculus', 'calculus', 'spherical-coordinates']"
4849528,My solution seems too long - feedback welcome. Show $f(f^{-1}(S))=S \iff f$ is surjective.,"Below is my attempt at a standard exercise on basic set theory and functions. There are 3 reasons for my request for feedback on my solution: I am self-teaching and that means I often think I am correct when I am not. My solution seems much longer than any exercise I've done so far in Terence Tao's Analysis I which suggests I've missed something important. I am not learning much from the many answers to the same exercise on this site. I want to check my thinking here, which is different from the thinking behind those answers there. (part of) Exercise 3.4.5 from Tao Analysis I 4ed Let $f : X \to Y$ be a function from one set $X$ to another set $Y$ . Show that $f(f^{-1}(S))=S$ for every $S \subseteq Y$ if and only if $f$ is surjective. My Solution Strategy: We need to prove both: Part 1 $f(f^{-1}(S))=S \implies f$ is surjective. Part 2 $f$ is surjective $\implies f(f^{-1}(S))=S$ . Part 1 (proves contrapositive) Let's start with $f(f^{-1}(S))=S \implies f$ is surjective. This is a statement of the form $A \implies B$ , which we'll prove by showing the logically equivalent $\neg B \implies \neg A$ . So, let's consider that $f$ is not surjective. This means there exists an $s \in S$ for some $S \subseteq Y$ , such that $f(x) \neq s$ for all $x \in X$ . The inverse image $f^{-1}(S)$ is, by definition, the set $\{x \in X : f(x) \in S\}$ . If we apply $f$ to the elements of this set, we obtain the set $\{f(x) \in S\}$ . However, this set does not contain $s \in S$ because we have already defined it such that $f(x) \neq s$ for all $x \in X$ . Therefore, $f$ not surjective implies $f(f^{-1}(S)) \neq S$ . This is logically equivalent to $f(f^{-1}(S))=S \implies f$ . Part 2 Now let's show $f$ is surjective $\implies f(f^{-1}(S))=S$ . To do this we need to show that under the assumption $f$ is surjective, both of the following are true: $f(f^{-1}(S)) \subseteq S$ $S \subseteq f(f^{-1}(S))$ Let's start with the first $f(f^{-1}(S)) \subseteq S$ . If $y \in f(f^{-1}(S))$ then $y=f(x)$ for some $x \in f^{-1}(S)$ . By definition $f^{-1}(S)$ is the set $\{x \in X: f(x) \in S\}$ . So if $f(x) \in S$ then $y=f(x) \in S$ . That is, $y \in f(f^{-1}(S)) \implies y \in S$ . This is equivalent to $f(f^{-1}(S)) \subseteq S$ . Note we didn't need $f$ to be surjective for this. Now let's consider $S \subseteq f(f^{-1}(S))$ . Since $f$ is surjective, for every $y \in S$ there exists an $x \in f^{-1}(S)$ such that $f(x)=y$ . Here, $x \in f^{-1}(S)$ gives us $y = f(x) \in f(f^{-1}(S))$ . Hence, $y \in S \implies y \in f(f^{-1}(S))$ . This is equivalent to $S \subseteq f(f^{-1}(S))$ . Having shown both $f(f^{-1}(S)) \subseteq S$ , and $S \subseteq f(f^{-1}(S))$ , we can conclude $f(f^{-1}(S))=S$ . This was under the assumption that $f$ is surjective, so we have $f$ is surjective $\implies f(f^{-1}(S))=S$ . Finally, having shown $f(f^{-1}(S))=S \implies f$ is surjective, and $f$ is surjective $\implies f(f^{-1}(S))=S$ , we can say $f(f^{-1}(S))=S \iff f$ is surjective. $\square$","['elementary-set-theory', 'functions', 'solution-verification']"
4849529,Geometry of electrodynamics,"In this question, I'd like to go over the physics - math dictionary occurring in the geometric structure (Principal bundle/spin bundles etc.) of Maxwell electrodynamics and the Dirac field. Consider the action for classical electrodynamics on Minkowski spacetime $(M,\eta)$ : $$S_{QED} = \int_{M} \big[ -\frac{1}{4} F_{\mu \nu} F^{\mu \nu} + \psi^{\dagger} \gamma^{0}(i\gamma^{\mu} D_{\mu} - m) \psi \big] dV_{\eta}$$ where the $\gamma$ symbols are the Dirac matrices, $D_\mu = \partial_\mu - ieA_\mu$ is a so-called gauge covariant derivative in which $A_\mu$ is the electromagnetic four potential . (I will italicize physics terminology that we will translate). The above theory is a $U(1)$ - gauge theory , i.e. a geometrical physical theory formulated using a principal bundle - thus one which must ""respect"" the mathematical redundancy of describing a principal bundle (its isomorphism class). This is analogous to the fact that isometric Lorentzian manifolds (if there are fields, we pull these back naturally too) describe the same spacetime (and fields). Namely that $$S_{EH}[(M,g)] = S_{EH}[(N,h)]$$ when $(M,g)$ is isometric to $(N,h)$ . Anyway here is the structure which I so far see regarding QED before quantization: We have a principal $U(1)$ bundle over $\mathbf{R}^4$ (so necessarily trivial $\equiv$ there exists a global smooth section) on which there is an principal $U(1)$ connection $\omega$ . Now  a trivialization of this bundle $(E,\pi,M)$ over a neighboorhood $\mathcal{U} \subset M$ is called a local gauge . A local gauge transformation is a change in this trivialization and hence equivalently the related section (the local smooth section equivalent to the trivialization, which can also be called a local gauge or mathematically, a trivializing section). Anyway, it is a standard fact that such local gauge transformations are in one to one correspondence with bundle automorphisms of $\pi^{-1}(\mathcal{U})$ . The $A_\mu$ above is the pullback of the principal $U(1)$ connection to the trivializing neighborhood by some trivializing section, so $s^{*}\omega$ . As in the above discussion, if one wants to do a local gauge transformation, one needs only change the local trivializing section. So we replace $s$ by the section $s^g$ where pointwise it is defined by $s^g = s \cdot g$ where $g: \mathcal{U} \to E$ is a smooth function (all local sections are given this way). From here we get the famous formula $$(s^g)^{*}\omega = Ad(g)^{-1} s^{*} \omega + g^{-1} dg$$ .
This is the transformation formula for the connection one forms . Next, recall the Faraday tensor is simply the pullback of the curvature of $\omega$ . Question one Given this set up, how do we show geometrically that the Faraday tensor term is ""gauge invariant"". Question two Considering the Dirac term is geometrically formulated in terms of a (associate) spin bundle and all that machinery, how does the gauge transformation act on that end? Also what is the gauge covariant derivative geometrically?","['principal-bundles', 'gauge-theory', 'mathematical-physics', 'differential-geometry']"
4849540,"Expected value of $1+1+1+\cdots+1$ (with $n$ ""$1$""s) if each '$+$' is independently deleted with probability $1/2$","Problem from an old exam: We have a sequence of symbols $1+1+1+1+1+\cdots+1+1$ , consisting of $n$ ones and $n-1$ plus signs. Each of the plus signs is independently deleted with a probability of $\frac{1}{2}$ . Let $X$ be the value of the resulting expression. For example, if $n=5$ and we delete all the plus signs except the second one, then $X=11+111=122$ . (a) Calculate $E(X)$ . (b) Provide a formula for $\operatorname{Var}(X)$ . It suffices to provide a formula that is a sum of a polynomial number of terms in compact form (such a sum does not need to be calculated). For $n = 2$ , the expected value should be $ \frac{11+2}{2} = 6.5 $ , for $n = 3$ it should be $\frac{3+12+12+111}{4} = 34.5 $ , for $n = 4$ it should be $\frac{4+13+13+13+22+112+112+1111}{8} = 175 $ . The only thing that comes to my mind is something like $\mathbb{E}[X] = \sum_{i=1}^{n} \mathbb{E}[X_i],$ where $\mathbb{E}[X_i] = 1 + \sum_{j=1}^{i-1}10^j\cdot\frac{1}{2^j}.$ But this obviously gives results that are too large. How can I approach it?","['combinatorics', 'probability']"
4849627,"What's the area of the triangle in this geometry problem? I think I can solve it, but it's way too convoluted...","I am trying to solve this geometry problem from an exam. The exam is supposed to be 3 hours long,  and this is supposed to be 1 out of 10 problems. So given that, the solution should be something quick. However, the only solution I've been able to find is extremely convoluted and realistically I'd never be able to give an answer in less than 3 hours. So, I'm looking for a simpler solution ( the intended solution ) which I should have been able to do in the exam. Here's the problem: Consider this illustration: In the image, the outer shape is a square with side length $8$ , a circle is drawn so that is is tangent to two sides of the square at points $G$ and $F$ , then the line $AH$ is drawn so that it is tangent to the circle at point $I$ and also such that the length of the segment $AI$ is also equal to $8$ . Lastly the line $BJ$ is drawn such that it is tangent to the circle at point $K$ . Find the area of the blue triangle, that is the area of $\triangle{AJB}$ . My approach so far is to divide the square into 9 different areas : I also added the segment $EL$ such that it is the continuation of the line $GE$ . From here on out when I say $a_{i}$ , I am referring to the area of region $a_{i}$ , $r$ will be the radius of the circle, $x$ will be the length of segment $JI$ and $y$ will be the length of segment $IH$ . So we want to find $a_{9}$ Notice then that: $$
a_{1} = a_{2} = \frac{r(8-r)}{2} \\
a_{3} = r^2 \\
a_{4} = a_{5} = \frac{ry}{2} \\
a_{6} = a_{7} = \frac{rx}{2} \\
a_{8} = \frac{8(8-r-y)}{2} =  4(8-r-y) \\
a_{1} + \cdots + a_{9} = 8*8 = 64 \\
a_{1} + \cdots + a_{8} = 4r + ry + rx + 32 - 4y = 32 + r(4 + x) + y(r - 4) \\
a_{9} = 64 - (32 + r(4 + x) + y(r - 4)) = 32 - r(4 + x) + y(4 - r)
$$ So from here really all we need to do is find $x, y, r$ and problem solved, right? Finding $r$ is pretty easy, notice that line segment $AE$ can be thought of as the hypotenuse of $\triangle{AIE}$ and the hypotenuse of $\triangle{AEL}$ . Using Pythagoras, we have $8^2 + r^2 = (8-r)^2 + (8-r)^2$ . Solving for $r$ we get two solutions: $16+8\sqrt{3}, 16-8\sqrt{3}$ and since $r$ is lesser than $8$ , then the second one is our value of $r$ . To find the value of $y$ , notice that by Pythagoras on $\triangle{ACH}$ , we have: $(8 + y)^2 = 8^2 + (8-r-y)^2$ , and since we know the value of $r$ we can solve for $y$ and we get $y = \frac{16\sqrt{3}-24}{3}$ . From here, I don't actually know how to find the value of $x$ , so I went back to our expression for the desired area $a_{9} = 32 - r(4 + x) + y(4 - r)$ , plugged in the values of $r$ and $y$ to make the area a function of $x$ , then I found another function of $x$ for the area, using Heron's formula. Notice the perimeter of the desired triangle is equal to $8 + (8 - x) + (8 - r + x)$ , and when we plugged for $r$ we get that the semiperimeter $S$ should be equal to $S = 4 \sqrt{3} + 4$ . Since the sides of the triangle have length $8, 8-x, 8-r+x$ and since we have precise values for the semiperimeter and $r$ , we can sort of make a function for the area based on values of $x$ . If we make these two functions equal to each other and solve the equation, we get two values which are possible candidates for $x$ . Problem is, these values are super convoluted, involve really large numbers and a bunch of radical expressions, which is why I gave up on this solution, realistically if it wasn't for Symbolab solving the equation for me, I probably wouldn't have time to do so in the exam. So that's why I am writing this post : I would like for you guys to help me come up with a concrete value for $x$ in an easier way, or maybe a completely different way of computing the area that doesn't involve $x$ , anything that is simpler than what I've been doing. Thank you so much for reading all of this, I know that it's kind of a long post.","['area', 'circles', 'geometry', 'triangles', 'problem-solving']"
4849631,Existence of Malthusian parameter,"Consider a continuous time point process $\eta(t)$ representing the number of points in the interval $[0, t]$ . Let $\eta(\infty)$ be distributed as the total number of children of a particle. Define $\mu(t)=\mathbb{E}[\eta(t)]$ and so $\mu(\infty)=\int_0^{\infty}\mu(dt)$ . The Malthusian parameter, if it exists, is defined as the solution $\alpha$ of $$
1=\int_0^{\infty} e^{-\alpha t}\mu(dt).
$$ I want to show that if $\mu(\infty)\in(1,\infty)$ then the Malthusian parameter $\alpha$ satisfying the integral must exist but I'm completely unsure how to proceed. It looks like it's presented as a part of Proposition $2.2$ on page $6$ of this paper but I don't see a proof included. Any hints, sources or other questions on this platform related to this are welcome.","['stochastic-processes', 'measure-theory', 'point-processes', 'statistics']"
4849664,Does such a convergent series exist?,"This is the question that arose in my mind while contemplating the convergence of this series: $$\sum_{n\geq 1} \frac{\sin(\sqrt{n})}{n}$$ In this problem, if we assume $A_N =  \sum_{n=1}^{N}\sin(\sqrt{n})$ , and consider the method of partial sums: $$\sum_{n=1}^{N} \frac{\sin(\sqrt{n})}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)}$$ Then, due to the estimation of $A_N = O(\sqrt{N})$ , we know that the series converges. At this point, I have a question regarding the summation of general form like $\sum_{n\geq 1} \frac{a_n}{n}$ , we can make the same assumption that $A_N = \sum_{n=1}^{N}a_n$ , and apply the method of partial summation $$\sum_{n=1}^{N} \frac{a_n}{n} = \frac{A_N}{N}+\sum_{n=1}^{N-1}\frac{A_n}{n(n+1)}$$ In this case, if $|A_N| \gg N$ , then this method cannot determine the convergence of the series; it can only conclude that the series is not absolutely convergent. For example, let $A_N = (-1)^N N$ , in this case, the series does not converges. However, does these exist $|A_N| \gg N$ such that the series $\sum_{n=1}^{\infty} \frac{a_n}{n}$ conditionally converges?","['sequences-and-series', 'examples-counterexamples', 'real-analysis']"
4849704,The proof for every even degree derivative being divisible by the factorial of the degree,"So in the process of proving the Maclaurin series for $$\frac{1}{x^2+1}$$ I tried to do prove that for all odd degrees of the derivative, the numerator contains x in every number so when 0 is input into the function, the result is 0, so the summation results in only x's that contain even integer powers. How would I go about proving that for those given derivatives of an even degree, they always contain the factorial of the degree? eg. $$f^{6}(x)=(6!)\frac{(7x^6-35x^4+21x^2-1)}{(x^2+1)^7}$$ I've tried doing induction as below
By assuming that $$\frac{d^{2k}}{dx^{2k}}(\frac{1}{x^2+1})=(2k)!f(x)$$ and assuming true for n=k+1 by saying $$\frac{d^{2k+2}}{dx^{2k+2}}(\frac{1}{x^2+1})=(2k+2)!g(x)$$ I've tried substituting $$(2k+2)!g(x)=(2k)!(2k+1)(2k+2)g(x)$$ with $$f''(x)$$ and ended with $$g(x)(2k+1)(2k+2)=f''(x)$$ but was not really able to do anything with it. Am I doing something wrong? Thanks.","['derivatives', 'taylor-expansion']"
4849780,Why does Wolfram Alpha give a wrong answer to a linear ODE with constant coefficients?,"This is very odd as it is a really simple ODE and the interpretation of the input seems correct, yet Wolfram Alpha produces rubbish: $$x'' - 2x' + x = t, \quad x(0) = 1.$$ Wolfram Alpha claims that the solution to this is $x(t) = c_1e^tt$ ( Link ). For reference, it gets it correct if you either put none or two initial conditions ( Link , Link ). Is this a known issue?","['initial-value-problems', 'wolfram-alpha', 'ordinary-differential-equations']"
4849798,"What is the volume of this shape, and how can I calculate it?","I have this shape with $7$ vertexes as specified. You start with a unit square, then at one back edge, two vertexes go directly above the bottom two with a height of $h$ . Then a special final vertex lies halfway between the previous two (in the $xy$ plane), but only half as high. I decided to keep $h$ as a variable. But how do I determine its volume? I can't figure out a way to deconstruct it into simpler shapes such as a pyramid.","['geometry', 'volume']"
4849820,Find the distance between 2 lines,"Question Let the cube $ABCDA' B' C' D'$ be where the points $M$ and $P$ are the midpoints of the edges $(AB)$ and $(B B')$ and $P'$ and $N'$ the centers of the faces $A' B' C' D'$ respectively $CDD' C'$ .  Calculate the distance between the lines $M P'$ , and $P N'$ , according to the edge of the cube $AB = a$ , where $a$ is strictly positive real number. My idea I know that this type of problems  are solved expressing the volume in $2$ ways. So using one of the volume formula  we get that $$
V(MP'N'P)=\frac{N'P\cdot MP'\cdot \operatorname{dis}(MP',N'P)\sin\angle (MP', N'P)}{6}
$$ We can easily express that $MP'=N'P=\frac{a \sqrt{5}}{2}$ and $ \angle (MP', N'P)= \frac{\sqrt{21}}{50}$ and to find the distance wanted i have to express the volume of the tetrahedron again. I thought of showing what procent of its volume is from the volume of the cube which is equal to $a^3$ , but i dont know how. Hope one of you can help me! Thank you!",['geometry']
4849866,Inequality related to function $f(x)=x\ln x-\frac{x^2}e+ax$,"I'm trying to solve a hard question about function and here is the question below: For the given function $f(x)=x\ln x-\frac{x^2}e+ax$ (where $a$ is a parameter), we have such property: $$\exists\;x_1,x_2\,(x_1\not=x_2)\quad s.t.\quad f(x_1)=f(x_2)\quad f'(x_1)=f'(x_2)$$ In this case, prove the inequality $$(a+1)^2x_1x_2-\sqrt{x_1x_2}\lt\frac34$$ To work this one out, I have tried a few things and finally got this $$x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$$ Can anyone help me continue this? (I'll put my work below.) My work of proving $x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$ First, I calculate the derivative. $$f'(x)=\ln x-\frac{2x}e+a+1$$ Because of the property of $f(x)$ , we have the following equations: $$\begin{cases}
f(x_1)=f(x_2)\Rightarrow x_1\ln x_1-\frac{{x_1}^2}e+ax_1=x_2\ln x_2-\frac{{x_2}^2}e+ax_2\\
f'(x_1)=f'(x_2)\Rightarrow\ln x_1-\frac{2x_1}e=\ln x_2-\frac{2x_2}e
\end{cases}$$ Simplify the second equation, we get $$x_1-x_2=\frac e2(\ln x_1-\ln x_2)\tag{*}\label{*}$$ Simplify the first one, we have $$x_1\ln x_1-x_2\ln x_2=\frac1e(x_1^2-x_2^2)-a(x_1-x_2)$$ $$\Rightarrow x_1\ln x_1-x_2\ln x_2=\frac1e(x_1-x_2)(x_1+x_2)-a(x_1-x_2)$$ $$\Rightarrow x_1\ln x_1-x_2\ln x_2\stackrel{\eqref{*}}=\frac12(\ln x_1-\ln x_2)(x_1+x_2)-\frac{ae}2(\ln x_1-\ln x_2)$$ $$\Rightarrow ea(\ln x_1-\ln x_2)=-(x_1-x_2)(\ln x_1+\ln x_2)$$ $$\Rightarrow 2a(\ln x_1-\ln x_2)\stackrel{\eqref{*}}=-(\ln x_1-\ln x_2)(\ln x_1+\ln x_2)$$ $$\Rightarrow \ln x_1x_2=-2a$$ $$\Rightarrow x_1x_2=\frac1{e^{2a}}$$ Then, let's calculate a vague range of a. $$f''(x)=\frac1x-\frac2e$$ So at $x=\frac e2$ , $f'(x)$ has a maximum. $$[f'(x)]_{max}=f'(\frac e2)=-\ln2+1+a$$ In order to satisfy the property of $f(x)$ , it is obvious that there must be an interval where $f'(x)\gt0$ . So we have $$[f'(x)]_{max}=-\ln2+1+a\gt0$$ $$\Rightarrow a\gt\ln2-1$$ $$\Rightarrow x_1x_2=\frac1{e^{2a}}\lt\frac{e^2}4$$","['real-analysis', 'calculus', 'functional-analysis', 'inequality', 'derivatives']"
4849902,Functional Analytical definition of no arbitrage,"Let $ {(S_t)}_{t\in[0,+\infty[} $ be a semimartingale and ${(x_t)}_{t \in[0,+\infty[}$ an admissible strategy. We denote by $(x.S)_{+\infty}=\lim \int_{0}^{t} x_u dS_u$ if such limit exists, and by $K_0$ the subset of $L^0(\Omega,\mathcal{A}_{\infty},P)$ containing all such $(x . S)_{+\infty}$ . Then we define: $C_0=K_0-L^0_+(\Omega,\mathcal{A}_{\infty},P)$ $C=C_0\cap L^{\infty}_+(\Omega,\mathcal{A}_{\infty},P)$ The market model satisfies: $\bullet$ the no-arbitrage condition (NA) if and only if $C \cap L^\infty(\Omega, \mathcal{A}_\infty, P) = \{0\}$ ,  and $\bullet$ the no-free-lunch-with-vanishing-risk condition (NFLVR) if and only if $\overline{C} \cap L^\infty(\Omega, \mathcal{A}_\infty, P) = \{0\}$ . There is no arbitrage if only if $C \cap L^{\infty}(\Omega,\mathcal{A}_{\infty},P)=\{0\}$ Question I am having a hard time making sense of this. If I remove from $K_0$ all a.s. positive functions, I mean $L^0_{+}$ , then $C_0\cap L^{\infty}_{+}$ must be zero once $L^{\infty}\subset L^0$ and hence $L^{\infty}_+\subset L^0_+$ .
How is this supposed to be a condition for arbitrage? This is always zero by the definition of the spaces.
Am I understanding this correctly?","['stochastic-analysis', 'stochastic-processes', 'finance', 'functional-analysis']"
4849904,Why does $x^{\log_a y} = y^{\log_a x}$ ? (intuitive reason),"So I have seen mathematical proofs for this. They used other logarithmic identities to get this result. I am looking for a more intuitive approach to this and not just equations which lead to this. edit: okay so what I mean by intuitive is that,
x^{power of a which gives y} = y^{power of a which gives x}.
So is there a way to make sense of it using pure logic and not identities?","['intuition', 'algebra-precalculus', 'discrete-mathematics', 'logarithms']"
4849977,"An accurate, rapidly converging estimate of $\sum_{j = 1}^{\infty}\frac 1 {j^2}$ (the Basel Problem) using only elementary calculus","Although the Basel Problem $$\zeta(2) = \sum_{j = 1}^{\infty}\frac 1 {j^2}$$ took a hundred years to solve analytically, using elementary calculus we can easily get strikingly accurate bounds: $$\zeta(2) = \sum_{j = 1}^{n}\frac 1 {j^2} + \frac {t(n)}{n} + \frac{1 - t(n)}{n+1}$$ (for all $n$ ) for some $$0 < t(n) < 1.$$ Proof: This follows from $$\sum_{j = n}^{\infty}\frac 1 {j^2} = \int_n^\infty \frac 1 {\lfloor x \rfloor^2} \quad dx.$$ This formula gives at the midpoint ( $t = \frac 1 2$ ) a very accurate estimate, with $< 0.23\%$ relative error after just a few terms ( $n = 4$ ).  It seems to be both simpler and more accurate than some of the other elementary approaches tried : What's striking is that $t$ clearly seems to approach $1/2$ as $n \to \infty$ : as this plot suggests: Question Can we prove (or even just explain intuitively) that as $n \to \infty, t(n) \to 1/2$ ? Can we derive a formula or estimate for $t(n)$ (such as e.g. $t(n) = \frac 1 2 - \frac 1 {n^{6/5}} + ...$ )? Can this be used to solve the Basel Problem?","['number-theory', 'asymptotics', 'real-analysis', 'analytic-number-theory', 'sequences-and-series']"
4850004,Can a Brownian Motion be scaled by a random time still be a brownian motion?,"I was reading the proof that if $\tau=\sup\{s\in [0,1]:B_{s}=0\}\wedge 1$ . Then $\bigg(\frac{1}{\sqrt{\tau}}W_{\tau t}\bigg)_{t\in [0,1]}$ is a Standard Brownian Bridge in $[0,1]$ from here . In the proof (Lemma 15 and Lemma 16), they are using that for a standard Brownian Motion $Y_{t}$ , we have that $(\sqrt{\tau}\cdot Y_{t/\tau})_{t\in [0,1]}$ is again a Standard Brownian Motion. However, I have not encountered such a scaling by a Random Time before. $\tau$ is also not a stopping time. So my question is whether $(\sqrt{\tau}\cdot Y_{t/\tau})_{t\in [0,1]}$ is a Brownian Motion or not and if the above is indeed true, then how
should I go about proving this? Firstly, I don't see how this is a Gaussian process. To be honest, I don't even see even for a fixed $t$ , how is $\sqrt{\tau} Y_{t/\tau}$ has normal distribution. So I cannot go about computing the covariances. However, I can easily see that conditioned on $\tau$ , we have that it is a Brownian Motion by usual scaling invariance. Can anyone provide a proof or a reference or more details for this ?","['probability-distributions', 'stochastic-processes', 'brownian-bridge', 'brownian-motion', 'probability-theory']"
4850029,"Canonical way to construct $R$ with $\mathrm{Spa}(A, A^+) \simeq \mathrm{Spec}(R)$?","In this Scholze-Weinstein's note on $p$ -adic geometry, there's a Huber's theorem (Theorem 2.3.3): Adic spectrum is spectral, i.e. $\mathrm{Spa}(A, A^+)$ is homeomorphic to $\mathrm{Spec}( R)$ for some $R$ (which says that $\mathrm{Spa}$ is not that weird space). Is there a canonical/functorial way to attach $R$ from $A$ ? I can't find a proof in the lecture note (maybe I missed), and I also checked Huber's original paper (Continuous Valuations), but I'm not sure the proof gives canonical construction or not.","['ring-theory', 'algebraic-geometry', 'rigid-analytic-spaces']"
4850084,Term for injective/one-to-one with respect to just one of multiple variables?,"As I understand it , a function $z=f(x,y)$ would be one-to-one or injective if there is only one unique $(x_1,y_1)$ pair which yields some value $f(x_1,y_1)=z_1$ . In this definition, the pair of variables $(x,y)$ is the entity that maps one-to-one to $z$ . Is there a ""partial"" equivalent to this that treats $x$ and $y$ separately? I want to say something like "" $z=f(x,y)$ is one-to-one with respect to $x$ for all $y$ "" , meaning that for any choice of $y_1$ in the domain of $y$ , $f(x,y_1)$ maps $x$ to $z$ one-to-one. This property would mean that I can always ""invert"" a function $z=f(x,y)$ into $x=f'(z,y)$ , but I can't necessarily do the same for $y=f''(x,z)$ . In general , for a multi-variable function , replace "" $y$ "" with ""all other variables."" Is there a concise terminology for this?","['notation', 'multivariable-calculus', 'functions', 'inverse', 'terminology']"
4850105,2-factors with many cycles,"Petersen's theorem states that every cubic, bridgeless graph contains a perfect matching. Let $G$ be a cubic bridgeless graph, and let $M$ be a perfect matching. Clearly $E(G)-M$ is a 2-factor of $G$ (or, equivalently, $E(G)-M$ induces a subgraph composed of vertex-disjoint cycles covering all vertices of $G$ ). A graph may have different 2-factors. I'm interested in 2-factors with many cycles. What is known about this? In particular, is there a constant $c$ such that every cubic bridgeless graph with $n$ vertices has a 2-factor with at least $cn$ cycles? I was able to find results where the goal was to avoid/force cycles of a given size, or to bound the number of cycles from above, but not from below.","['graph-theory', 'matching-theory', 'combinatorics']"
4850150,How can I properly show that the Residual Sum of Squares can be written in matrix form?,"First, I apologize if I am violating any community laws or have improper tags I am a newbie. However, I am tasked with showing, $$RSS(\boldsymbol{\beta}) = \sum_{i=1}^{N} (y_i - \beta_0 - \sum_{j =1}^{n}\beta_{j}x_{ij})^2$$ to be written in matrix form as $$RSS(\boldsymbol{\beta})  = (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})^T (\boldsymbol{y} - \boldsymbol{X}\boldsymbol{\beta})$$ I just want to ensure my thought process is accurate or if you can point me in the right direction. My solution: Let $\boldsymbol{y}$ be a column vector of size $N \times 1$ . Let $\boldsymbol{\beta}$ be a column vector of size $(n+1) \times 1$ . Finally, let $X$ be a matrix of size $N \times (n+1)$ then, $$X \boldsymbol{\beta}=\begin{bmatrix}
    1 & x_{11} & x_{12} & \dots  & x_{1n} \\
    1 & x_{21} & x_{22} & \dots  & x_{2n} \\
    \vdots & \vdots & \vdots & \vdots & \vdots \\
    1 & x_{N1} & x_{N2} & \dots  & x_{Nn}
\end{bmatrix} \cdot \begin{bmatrix}
    \beta_0 \\
    \beta_1 \\
    \vdots \\
    \beta_n 
\end{bmatrix} = \begin{bmatrix}
    \beta_0 + x_{11}\beta_1+ x_{12}\beta_2+...+x_{1n}\beta_n \\
    \beta_0 + x_{21}\beta_1+ x_{22}\beta_2+...+x_{2n}\beta_n \\
    \vdots \\
    \beta_0 + x_{N1}\beta_1 + x_{N2}\beta_2+...+x_{Nn}\beta_n
\end{bmatrix}$$ Resulting in a $N \times 1$ column vector containing our predicted values. Then we can find the residuals as such, $$ u = \boldsymbol{y} - X\boldsymbol{\beta}$$ where $u$ is an $ N \times  1$ column vector.Therefore, we can rewrite the Residual Sum of Squares in matrix form as follows, $$RSS(\boldsymbol{\beta}) = (\boldsymbol{y}-X\boldsymbol{\beta})^T(\boldsymbol{y} - X\boldsymbol{\beta}) = u^Tu$$ . Since $(\boldsymbol{y}-X\boldsymbol{\beta})$ is a column vector we need to transpose it to multiply it by itself.","['statistics', 'regression-analysis']"
4850198,For what integers $n$ does $\varphi(n)=n-5$?,"What I have tried so far: $n$ certainly can't be prime. It also can't be a power of prime as $\varphi(p^k)=p^k-p^{k-1})$ unless it is $25=5^2$ . From here on, I am pretty stuck. I tried considering the prime factorization of $n$ , or perhaps looking for some contradiction, which would show $n$ must be a power of a single prime, but cannot make progress: Let $n=p_1^{e_1}...p_k^{e_k}$ . Then $$n(1-\frac 1{p_1})...(1-\frac 1{p_k})=p_1^{e_1}...p_k^{e_k}-5.$$ Do I need to consider modulo 5? I also thought about considering whether or not $5\mid n$ or whether $n$ is even/odd, but cannot seem to make progress. I found this similar question asked on StackExchange a while ago, but since the RHS is $n-2$ , they could consider restrictions with parity. I also don't get why they could assume that $n=pq$ is a product of exactly 2 distinct primes only for the contradiction.","['number-theory', 'prime-factorization', 'elementary-number-theory', 'prime-numbers']"
4850218,X and Y are both $N\times N$ matrices. How many of the $N^2$ equations in $XY=YX$ regarding the elements of X and Y are independent?,"I tried the case of $N=2$ : Let $X=\begin{pmatrix}a&b \\ c&d\end{pmatrix},Y=\begin{pmatrix}e&f \\ g&h\end{pmatrix}$ . $XY=YX$ means that $$
\begin{pmatrix}
 a e+b g & a f+b h \\
 c e+d g & c f+d h \\
\end{pmatrix}=
\begin{pmatrix}
 a e+c f & b e+d f \\
 a g+c h & b g+d h \\
\end{pmatrix}$$ which is $N^2=4$ equations about $a,b,c,d,e,f,g,h$ , But only $3$ of them are independent. $tr(XY-YX)=0$ , so at most $N^2-1$ equations are independent. Is $N^2-1$ the answer?","['matrices', 'linear-algebra']"
4850223,"Solve $a=\frac{b+c}{1+b^2c^2},b=\frac{a+c}{1+a^2c^2}, c=\frac{a+b}{1+a^2b^2}$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Let $a,b,c> 0$ such that $$a=\frac{b+c}{1+b^2c^2},b=\frac{a+c}{1+a^2c^2}, c=\frac{a+b}{1+a^2b^2}$$ Prove that the only solution to this system of equations is $$a=b=c=1$$ I am getting this answer using numerical simulations, but haven't been able to prove this algebraically. EDIT: I am specifically looking for an argument that utilizes inequalities, and is an equality only for $a=b=c=1$ .","['algebra-precalculus', 'systems-of-equations', 'inequality']"
4850236,if motion in infinitely small time interval is linear then why a tangent to a circle has only one common point with the circle,I cant understand the infinitely small linear relation in calculus. I don't yet know calculus. But my physics book use the conclusions from it without explanation. Edit as asked by someone: Instant is an infinitely small interval of time curve can change direction during that interval. Since point is smaller than displacement at an instant. I am getting even more confused.,"['calculus', 'infinitesimals']"
4850248,How many positive elements in a diagonal matrix $D$ do we need to make $A+D$ positive definite for a real symmetric $A$ with $m$ negative eigenvalues,"Consider a $n$ -dimensional real symmetric matrix $A\in\mathbb{R}^{n\times n}$ having exactly $m$ negative eigenvalues and $n-m$ positive eigenvalues ( $1\leq m\leq n$ ). Let $D\in\mathbb{R}^{n\times n}$ be a diagonal matrix with non-negative elements. My question is: To make $A+D\succ0$ , i.e. positive definite, at least how many positive elements of $D$ do we need? For $n=1, 2$ , it is easy to show that the answer is exactly $m$ . Does this conclusion still hold for $n\geq3$ ? Any ideas and suggestions are welcomed. Thanks in advance!","['matrices', 'hessian-matrix', 'matrix-equations', 'eigenvalues-eigenvectors']"
4850255,Proof of Theorem 1.1 of Analytic Number Theory by Iwaniec & Kowalski,"I am not clear about the proof of Theorem 1.1 in the book `Analytic Number Theory' by the authors Iwaniec & Kowalski. They say that if a multiplicative function $f$ satisfies $$\sum_{n\le x}\Lambda_f(x) = \kappa\log x+O(1)$$ where $\Lambda_f(n)$ is given by corresponding Dirichlet series $-D'_f(s)/D_f(s)$ , $\kappa>-1/2$ is a constant, with $$\sum_{n\le x}|f(n)|\ll(\log x)^{|\kappa|},$$ then the Euler product $$\frac{D_f(s)}{\zeta(s+1)^{\kappa}} = \prod_{p}\left(1-\frac{1}{p^{s+1}}\right)^\kappa\left(1+\frac{f(p)}{p^s}+\frac{f(p^2)}{p^{2s}}+\cdots\right)$$ has limit $$\prod_{p}\left(1-\frac{1}{p}\right)^\kappa\left(1+f(p)+f(p^2)+\cdots\right)$$ as $s\to 0+$ , without further explanation (it is previously known that the function $D_f(s)/\zeta(s+1)^\kappa$ tends to a limit as $s\to 0+$ ). But I cannot see why. Below is my trial. (1) By means of Abel summation, it is known that if a Dirichlet series $D_g(s)$ converges at $s=s_0$ , then it is uniformly convergent in the sector $\{s\in\mathbb{C}|\textrm{Arg}(s-s_0)<\theta\lor s=s_0\}$ for a fixed $\theta\in(0,\pi/2)$ . Hence if we can show that the given Euler product converges absolutely(in the sense that we take absolute value for inner series, too), then we can write it as a corresponding Dirichlet series, and hence use continuity of the Euler product to conclude. But I couldn't show that the product converges at $s=0$ . I don't even know the series $1+f(p)+f(p^2)+\cdots$ converges or not. (2) I took logarithmic derivative of the quotient and see if the corresponding Dirichlet series converges (although it may not have direct relation...). Then we have $$-\frac{D'_f}{D_f}(s)+\kappa\frac{\zeta'}{\zeta}(s+1) = \sum_{n=1}^{\infty}\frac{\Lambda_f(n)-\Lambda(n)/n}{n^s},$$ where the coefficient has partial sum of growth $O(1)$ by the first condition (and by Mertens theorem $\sum_{n\le x}\Lambda(n)/n = \log x+O(1)$ ). This does not help too, since it only says that the series converges for $\Re s>0$ . Or by Abel summation this says that the LHS above is bounded near $s=0$ , but this is not enough as we don't have any information about possible continuation of $D_f(s)$ beyond the half-plane $\Re s>0$ . I am aware of the duplicate Question about a proof in Iwaniec-Kowalski's Analytic Number Theory book , but this does not answer my question, and it's too old post, so I cannot expect further answer even if I have written a comment. If you want, I can list my doubts on the answer above.","['analytic-number-theory', 'limits', 'euler-product', 'dirichlet-series']"
4850273,How can we solve the system with 6 variables? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I'm doing some GMAT exercises and this question came up to me. Unfortunately, I cannot figure out the final result. Mike and Jenny are buying fruits. Mike bought total 11 oranges and
apples, which cost Mike 2.25 dollar. Then Jenny bought total 7 oranges and apples, which cost her 1.35 dollar. If the price of an orange is higher than apple, What is the price of an orange? There are 5 given answers: $$0.1 $0.15 $0.2 $0.25 $0.3 ==================== Here are what I've tried to solve. Let's call: the price of an orange is $P_{o}$ the price of an apple is $P_{a}$ the quantity of orange Mike bought is $O_{1}$ the quantity of apple Mike bought is $A_{1}$ the quantity of orange Jenny bought is $O_{2}$ the quantity of apple Jenny bought is $A_{2}$ Here are the system I have: $P_{o} \cdot O_{1} + P_{a} \cdot A_{1}$ = 2.25 $O_{1} + A_{1} = 11 <=> O_{1} = 11 - A_{1}$ $P_{o} \cdot O_{2} + P_{a} \cdot A_{2}$ = 1.35 $O_{2} + A_{2} = 7 <=> O_{2} = 7 - A_{2}$ Using some eliminations, I have: $P_{o} \cdot (11 - A_{1}) + P_{a} \cdot A_{1} = 2.25$ $P_{o} \cdot (7 - A_{2}) + P_{a} \cdot A_{2} = 1.35$ $\iff$ $11 \cdot P_{o} - A_{1} \cdot (P_{o} - P_{a}) = 2.25$ $7 \cdot P_{o} - A_{2} \cdot (P_{o} - P_{a}) = 1.35$ I'm currently stuck here because I've tried to eliminate variables but it just goes around then came back to this. Can you guys help me please?","['word-problem', 'algebra-precalculus', 'systems-of-equations']"
4850331,Is it true that $A\in\{A\cup B\}$,"I'm studying topology, and I've encountered a set $$\mathcal{T}_{\mathcal{B}} = \bigg\{\bigcup_{i\in I}B_i:B_i\in\mathcal{B}\bigg\},$$ where $\mathcal{B}$ is a basis for a topology, and $I$ is an index set. The text then proves that $\mathcal{T_{\mathcal{B}}}$ is a topology.
As part of the proof, the following is said; ""Let $B_1,B_2 \in \mathcal{T_{\mathcal{B}}} $ (...)"". This has me a bit confused, as I'm not sure I understand if the individual sets of the union are actually in $\mathcal{T_{\mathcal{B}}}$ . My thought process is as follows: Let $A$ and $B$ be non-empty sets. Obviously, $A\subset A$ and $A\subset A\cup B$ . Further, $A\in \{A\}$ . But is $A\in\{A\cup B\}$ ? I can't seem to find a clear answer. As an example, let $A=\{1,2\}$ , $B=\{2,3\}$ , and $S=\{A\cup B\}$ . Then $S=\{\{1,2\}\cup\{2,3\}\}=\{\{1,2,3\}\}$ . From what I understand, this set contains one element, and that element is $\{1,2,3\}$ . If this is correct, then how is the statement from the text justified?","['elementary-set-theory', 'general-topology', 'set-theory']"
4850353,Intersection of a circle in 3D with horizontal planes?,"Let's assume that we have two vectors in $\mathbb R ^3$ , $v_1$ and $v_2$ , such that $v_1 \perp v_2$ and $|v_1|=|v_2|=1$ . Since the two vectors emerge from the origin, they can define a circle in $\mathbb R^3$ that includes both vectors and is perpendicular to their cross product. Now, the question is how to compute all points on the circle that have a specific $z$ -value, or in other words, all points that are formed by intersecting the non-horizontal circle at the origin with a horizontal plane with a specific $z$ -value determining its height. Later addition: Reading this problem can cause confusion because of me adding specifiers like 'horizontal' without me providing what is the coordinate system we work on.
In my system, X is right, Y is front, and Z is up. So saying a 'horizontal' plane means: the plane that is parallel to the X and Y (right and front) axes and is defined by only is Z value as its height.","['vectors', '3d', 'circles', 'geometry', 'trigonometry']"
4850433,"Let $\bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$ where $n=1,2,\ldots$. Then find $\lim_{{n\to\infty}} P(\bar X_n=2)$","Let $\{X_n\}_{n\geq1}$ be a sequence of iid random variables having a common density function $$f(x) = \begin{cases} 
    xe^{-x} & \text{if } x\geq0 \\
    0 & \text{otherwise}
\end{cases}$$ Let $\bar X_n = \frac{1}{n}\sum_{i=1}^{n} X_i$ where $n=1,2,\ldots$ . Then find $\lim_{{n\to\infty}} P(\bar X_n=2)$ . My try My first thought as well but I think the problem is deeper than that. I think that as the n tends towards infinity the probability of the the sample mean converging to the population mean is 1. Looking at proving this. By the Central Limit Theorem the sample mean distribution can be approximated by a Normal distribution with $$\mu = 2,~\sigma = \sqrt{\dfrac{2}{n}}$$ As $n\to \infty$ this becomes a delta function centered at $2$","['statistics', 'solution-verification', 'probability']"
4850456,Aproximating difference equation by differential equation,"I have the following difference equation $$ C_{n+1} = C_n + \frac{1}{4C_n}$$ with $C_0 \in \mathbb{R}^+$ given and am interested in the speed at which $C_n$ diverges to $\infty$ as $n \to  \infty$ . Up until now, I failed to derive an explicit solution for $C_n$ and I am also not sure whether a nice formula exists (if so, I would of course be interested). However, solving the corresponding differential equation $C' = \frac{1}{4C}$ yields the solution $$ C(t) = \sqrt{\frac{2C_0^2 + t}{2}},$$ i.e., an expression of order $O(\sqrt{t})$ . When plotting the first terms $C_n$ this pattern also appears. I was wondering whether there is some general theory on how to approximate the asymptotics of the solution of a difference equation by the using the solution of the differential equation and how it can be used in this case.","['calculus', 'ordinary-differential-equations']"
4850464,$x^5 = 9^x$ Solution by Derivatives (Where Did I Go Wrong)?,"Where is my solution to the following problem wrong? Problem: $x^5 = 9^x$ Solve for x. $5x^4 = 9^x * \ln(9)$ d/dx both sides. $5x^4 = x^5 * \ln(9)$ As stated in the problem 9^x is equal to x^5 so substitute that in here. $5 = x * \ln(9)$ Divide x^5 by x^4 and get x. $5 / \ln(9) = x$ Isolate variables. But this is wrong! Where did I go wrong?  Was it in my derivation?  d/dx 9^x is 9^x * ln(9) so my derivative tables tell me.
Was it in substituting x^5 for 9^x?  That seems allowed as long as the equation has a solution.  And it does.  It's just not x = 5/ln(9).","['calculus', 'derivatives']"
4850495,"A square contains many random points. From each point, a disc grows until it hits another disc. What proportion of the square is covered by the discs?","A square lamina contains $n$ independent uniformly random points. At a given time, each point becomes the centre of a disc whose radius grows from $0$ , at say $1$ cm per second, and stops growing when the disc hits another disc. Consider the discs after all growth has stopped. Here is an example with $n=20$ . Let $P=$ proportion of the square that is covered (i.e. occupied) by the discs. In the example above, $P\approx0.383$ . What does $P$ approach as $n\to\infty$ ? The shape of the lamina (for example, square) does not matter, since we are taking $n\to\infty$ . The rate of growth (for example, $1$ cm per second) does not matter, as long as all the discs start growing at the same time and grow at the same rate. My thoughts I tried to find the probability that a new random point in the square lies in one of the existing discs. I also tried to find the average area of a disc. But I haven't succeeded. These questions seem to be complicated by the fact that the size of a point's disc is determined not only by the point's distance to its neighbors, but also its neighbors' distances to their neighbors, and so on. Context This question was inspired by another question , ""A disc contains $n$ random points. Each point is connected to its nearest neighbor. What does the average cluster size approach as $n\to\infty$ ?"". Both of these questions are about inherent properties of the $2D$ Poisson process.","['geometric-probability', 'circles', 'geometry', 'expected-value', 'limits']"
4850562,Proof of Positivity for Solutions in Ordinary Differential Equations (ODEs),"Let $x(t)$ be the solution of the initial value problem: $$
\dot{x}(t) = f(x(t)); \; \; x(0) = x_0
$$ I have made the following asumption during my work: If $x_0 \geq 0$ , and $f(0) \geq 0$ , then $x(t) \geq 0$ for $t \geq 0$ . It intuitivelly makes sense: starting with x positive, it might decrease until $x=0$ . But as soon as it reaches that point, $\dot{x} = f(0) \geq 0$ , so it either ""grows back up"" or just stay at $0$ . I feel like there should be a simple proof for this, but I cannot find one. Also, if it not true, what additional conditions should I add? Maybe  Lipschitz continuity of $f$ ? Thanks in advance.","['initial-value-problems', 'proof-writing', 'ordinary-differential-equations', 'dynamical-systems']"
4850598,Tietze transformations and the translation of words,"I am new to combinatorial group theory and have a question regarding Tietze transformations. Say I have a group G, finitely presented by $\langle X | R \rangle$ , and a sequence of (finitely many) Tietze transformations ending in $\langle X' | R' \rangle$ , which in turn finitely presents a group $G'$ . Further, say I want to translate a word $w$ (written in $X$ and its inverses) from $\langle X | R \rangle$ to an equivalent word $w'$ in $\langle X' |R' \rangle$ (i. e. a word $w'$ denoting the same group element in $G'$ as $w$ denoted in $G$ , where $G \cong G'$ ). Since Tietze transformations in general produce isomorphisms, would I just need to know the mapping of the generators of $X$ to the generators of $X'$ in order to aquire an equivalent word (i. e. one of many representations of the equivalent group element in $G'$ written in $X'$ and its inverses)? How would I need to account for a discrepancy in the numbers of generators in both presentations, in order to translate $w$ correctly? Or to break down my question into single Tietze steps: When adding a new generator $y$ , say to $X = \{ x_1, x_2, ... , x_n \}$ what do I map to the new generator $y$ ? As I understand, $y$ can be written in the generators $x_1, x_2, ..., x_n$ (or their inverses). Would it be necessary to map the preimage of y to y in order to translate the word $w$ ? I would assume it only being necessary, if the letter $y$ itself is to be part of the translation, since $y$ itself can be written in $x_1, x_2, ..., x_n$ (or their inverses)? When cancelling a generator, say $y$ (assuming there exists the relevant relator, say $ys^-1$ , in $R$ to do so), I would need to map $y$ to $s$ ( $s$ being written in $x_1, x_2, ...$ or their inverses). So I would need a map from all $x_i$ to $x_i$ as well as from $y$ to $s$ . When adjusting only $R$ with a Tietze transformation the normal closure stays the same. So I would assume there is no mapping necessary for a translation of $w$ . My question being, is my understanding correct, or am I missing something (or several things)?","['combinatorial-group-theory', 'group-presentation', 'group-theory', 'transformation']"
4850599,Wronskian & Linear independence of functions,"We usually study wronskian concerned with solutions of ODE, but using this idea to check linear dependence of any two differentiable functions seems Okay because it is the determinant of a Matrix having functions & their derivatives.If two rows/columns are Linearly dependent, then Determinant is zero.
The Wronskian for two real-valued differentiable functions f and g is defined by $W(f(t),g(t))$ = $\begin{vmatrix} f(t) & g(t)\\ f'(t) &  g'(t)\\ \end{vmatrix}$ = $f(t)g'(t)-f(t)g(t)$ . If f and g are linearly dependent on some interval $I$ , then, WLOG we may assume $f = cg$ for some scalar $c$ , so that the Wronskian becomes $W(f(t),g(t))$ = $\begin{vmatrix} cg(t) & g(t)\\ cg'(t) & g'(t)\\ \end{vmatrix}$ = $c.g(t)g'(t)-c.g'(t)g(t)=0$ on $I$ . In other words, if $f$ and $g$ are linearly dependent on $I$ , the Wronskian $W(f(t),g(t))$ is identically $0$ on $I$ . Now consider this question Let $f, g : [- 1, 1] \to \mathbb{R}, f(x) = x ^ 3$ and $g(x) = x ^ 2|x|$ .
And options are (a) $f$ and $g$ are linearly independent on $[- 1, 1]$ (b) $f(x)  g' (x) - f'(x)g(x)$ is NOT identically zero on $[- 1, 1]$ (c) There exist continuous function $p(x)$ and $q(x)$ such that $f$ and $g$ satisfy $y'' + py' + qy = 0$ on $[- 1, 1].$ I had a discussion with my friend over this question, I said that the functions are Linearly Independent (abbrivated as LI) on $[-1,1]$ because we cannot write these functions as a scalar multiple of one another, now if they are LI their wronskian should not be zero. But my friend says define $$g(x) = \begin{cases} x³ & x≥0\\-x³ & x<0 \end{cases}$$ Now wronskian is for $x ≥0$ , $\begin{vmatrix} x³ & x³\\ 3x² &  3x²\\ \end{vmatrix}$ = $0$ and for $x<0$ , $\begin{vmatrix} x³ & -x³\\ 3x² &  -3x²\\ \end{vmatrix}$ = $0$ ,So it is zero on $[-1,1]$ So they are LI. Then, I said that you are calculating wronskian of a piecewise function over two different regions, then how can you combine the two individual results for the whole domain because LI and LD is domain based property. If we are going to check LI and LD on individual points in $\mathbb{R}$ then every function is LD because every point in $\mathbb{R}$ can be written as a scalar multiple of one another. I added, In Option B , they aren't asking to calculate wronskian, they are just asking that term to be NOT zero. Option C I'm not sure about this but my view is that a piecewise function cannot be a solution to a differential equation. Kindly help, what is correct explanation for this question.","['functions', 'wronskian', 'ordinary-differential-equations']"
4850662,What makes the Mean Squared Error so special compared to other upper bounds?,"Let $X$ be some measure space (esp probability space), $Y$ some metric space and we consider $g:X\to Y$ as an estimator for $f:X\to Y$ . It seems convenient to consider an estimation as 'good' in case for any small $\epsilon > 0$ the measure $ \mu(\{d(f,g) > \epsilon\}) $ is small. Using Markov's Inequality we have for any $h:\mathbb{R}\to \mathbb{R}$ that is positive and non decreasing on $\mathbb{R}_{\geq 0}$ that: $$ \mu(\{d(f,g) > \epsilon\}) \quad \leq \quad  \frac{1}{h(\epsilon)} \int_X h(d(f,g))\; d\mu $$ Thus, there is a lot of choices for upper bounds choosing different $h$ (x^n, exp, ..). Nevertheless, it seems that mostly only the Mean Squared Error of $f$ and $g$ (where $h=x^2$ ) is considered and we try to find a $g$ that minimizes the Mean Squared Error. What makes it so special compared to other choices? Why don't we use another $h$ and find an estimator $g$ minimizing $\int_X h(d(f,g))\; d\mu$ ?","['optimization', 'statistics', 'estimation']"
4850672,A necessary condition for non-vanishing of irreducible characters of $S_n$,"The following is Corollary 2.4.9 from the book ""The Representation theory of the Symmetric Group"" by James and Kerber. For partition $\alpha,\beta \vdash n$ , $\chi_{\alpha}(\beta)\neq 0 \implies \beta \leq (h_{11}^{\alpha}, h_{22}^{\alpha},\ldots)$ . Here $\chi_{\alpha}$ is the irreducible character of $S_n$ corresponding to $\alpha$ and $\leq$ denotes the dominance order and $h_{ij}^{\alpha}$ is the hook length at $(i,j)$ -th node of the Young diagram of $\alpha$ . The proof is not given in the book probably because it is not difficult, but I haven't been able to find a proof. Here is what I tried : Let $\beta \not\leq (h_{11}^{\alpha}, h_{22}^{\beta},\ldots)$ , then we want to prove that $\chi_{\alpha}(\beta)=0$ . The idea is to show that $\beta$ contains a cycle whose length doesn't appear as a hook length of some node in the Young diagram of $\alpha$ . Then the recursive Nakayama-Murnaghan rule immediately yields the result. Also notice that for any partition $\alpha$ , $h_{11}^{\alpha}$ cannot be too small. Let $P(n)$ be the set of all partition of $n$ . Then we can consider the map $\Delta:P(n)\to P(n)$ given by $\alpha \mapsto (h_{11}^{\alpha}, h_{22}^{\alpha},\ldots)$ . If $n=6$ , one can check that the image of $\Delta$ is the set $\{(6), (5,1), (4,2)\}$ . This shows that the least possible value of $h_{11}^{\alpha}$ is 4. In general, it seems that for any $\alpha \vdash n$ , $h_{11}^{\alpha}\geq \frac{n}{2}+1$ if $n$ is even and $h_{11}^{\alpha}\geq \frac{n+3}{2}$ if $n$ is odd. From this it seemed to me that if $\beta=(\beta_1,\beta_2,\ldots)$ and $\beta \not\leq (h_{11}^{\alpha},h_{22}^{\alpha},\ldots)$ , then $\beta_1>h_{11}^{\alpha}$ . If it were to be true then since $h_{11}^{\alpha}$ is the highest hook lenght in $\alpha$ , it would have followed that $\beta_1$ cannot be a hook length in the diagram of $\alpha$ . But the conclusion that $\beta_1>h_{11}^{\alpha}$ can be easily seen to be false. For example - If $\alpha=(5,3,3)\vdash 11$ , then taking $\beta=(6,5)$ , it is clearly seen that $\beta\not\leq (7,3,1)=(h_{11}^{\alpha},h_{22}^{\alpha},h_{33}^{\alpha})$ but $\beta_1=6<h_{11}^{\alpha}$ . One can deduce some more restriction on the partition $(h_{11}^{\alpha}, h_{22}^{\alpha},\ldots,)$ . For example $h_{ii}^{\alpha}\leq h_{jj}^{\alpha}-2$ where $j=i-1$ and so on. But with all these I haven't been able to get the correct argument. I think there is some easy observation that I
am overlooking. Any help will be highly appreciated. Thanks in advance.","['finite-groups', 'representation-theory', 'combinatorics', 'symmetric-groups', 'group-theory']"
4850724,Confusing statement in stats book,"I just picked up a book, and while I was skimming through it one statement caught my eye. In Chapter 2, page 47 of Statistical Inference it reads: If X is a random variable with cdf $F_X(x)$ then any function of X, say g(X), is also random variable. I am not quick to assume that book is in the wrong, but it is the case now isn't it? It should only hold for discrete $X$ or by adding assumption that $g$ is measurable. A bit off topic: this book is recommended a lot and presented as rigorous ( for example here ), but all the books I've read on stats completely disregard measure theory and skip technical aspects. Is it just a normal approach in statistics and I am not missing out by going with such books, or should i get something more technical?","['statistics', 'book-recommendation']"
4850751,How can I calculate the determinant without knowing every element of the matrix? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question I was given the following matrix and asked only to find the determinant. As far as I know we need to know every element of the matrix in order to do that. But maybe I'm missing something. Any help is appreciated. Thanks in advance.","['matrices', 'determinant', 'linear-algebra']"
4850777,Finding the derivative of $\sqrt x$ geometrically,"I've been smacking my head against a wall on this for a couple hours now. Here is a square, it as an area of x and a side length of √x . If the side length increases by some dx , what is the corresponding change in area aka df Drawing of the square and subsequent calculations: The expected answer is $$\frac{1}{2\sqrt{x}}$$ I have tried to intuitively reason my way out of this. df is the increase area for a given dx as dx approaches 0. Therefore the areas of the two rectangles plus the small square is equal to df . Area of the two rectangles $$2\sqrt{x} \cdot dx$$ Area of the small square $$dx^2$$ Therefore df being the total area of the increase is $$2\sqrt{x} \cdot dx + dx^2$$ We discount the addition of $dx^2$ because it is of negligible size. And that leaves us with the rate of increase of df per dx $$\frac{df}{dx} = \frac{2\sqrt{x} \cdot dx}{dx}$$ $$\frac{df}{dx} = 2\sqrt{x}$$ Where have I gone wrong? I feel like it's something obvious... Thank you in advance. Related Question: I'm trying to prove the derivative of $\sqrt{x}$ using geometry. Helpful Youtube video https://www.youtube.com/watch?v=S0_qX4VJhMQ&list=PLZHQObOWTQDMsr9K-rj53DwVRMYO3t5Yr&index=4","['calculus', 'derivatives', 'geometry']"
4850797,Schubert classes appearing in the class of certain subvarieties of incidence variety,"The above picture comes from Fulton's ""Introduction to Intersection Theory in Algebraic Geometry"". The variety $I$ is the partial flag variety $F(0,d;n)$ , also known as the incidence variety of points on $d$ -planes in $\mathbf P^n$ . In other words, points in $I$ are pairs $(p,P)$ where $P$ is a $d$ -plane and $p$ is a point on $P$ . We assume $0<d<n$ . The Chow ring of $I$ has a basis given by classes of the cycles of Schubert varieties, or simply Schubert classes. We can concretely label these classes by $(a_0,\dots,a_d; k)$ , where $a_0,\dots,a_d$ is a strictly increasing sequence of integers in $\{0,\dots,n\}$ , and $k$ is an integer in $\{0,\dots,d\}$ . Fulton uses the notation $(a_0,\dots,\overset{*}{a_k},\dots,a_d)$ . Ignore his use of the notation $\{1,\dots,\overset{*}{1},0,\dots,0\}$ . The dimension of such a class is $k+\sum\limits_{i=0}^d (a_i-i)$ . Since $V'$ has codimension $d+1$ , the class of $[V']$ is a $\mathbb Z$ -linear combination of Schubert classes of codimension $d+1$ . It is clear to me that all of the $\mu_k$ have codimension $d+1$ . However, it is not so clear why there should be no other Schubert classes of codimension $d+1$ appearing in the expansion of $[V']$ . First, it is not the case that the $\mu_k$ are the only Schubert classes of codimension $d+1$ . Indeed, assuming that $d+2\leq n$ , we have $(n-d-2, n-d+1, \dots, n; 1)$ has codimension $d+1$ . Then, in order to verify that the other codimension $d+1$ Schubert classes don't appear in $[V']$ , one would have to understand what all possible codimension $d+1$ Schubert classes are and how they interact with $V'$ . Is this even possible? Am I missing something?","['algebraic-geometry', 'intersection-theory', 'schubert-calculus']"
4850818,An integral of a differential equation that's troubling me [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 months ago . Improve this question I am facing a problem in differential equations. $$(x-x^3)dy = (y+yx^2-3x^4)dx\tag{Question}$$ I am completely recognisant that I can use the linear differential equation form here, as I have shown below: $$\frac{dy}{dx}=\frac{y+yx^2-3x^4}{x-x^3}=y\frac{1+x^2}{x-x^3}-\frac{3x^3}{1-x^2}$$ $$\frac{dy}{dx}-y\frac{1+x^2}{x-x^3}=-\frac{3x^3}{1-x^2}$$ From the above LDE form, I can deduce the below $$Integrating\space factor = e^{-\int\frac{(1+x^2)}{x-x^3} \,dx}\tag{Problematic one}$$ Now I tried catching WolframAlpha (I did not know how to proceed, but eventually I recognized the partial fraction present), and it gave me the answer for the integrating factor as $$\frac{1-x^2}{x}$$ This would further complicate things as the final solution would look like: $$y\times\frac{1-x^2}{x}=-\int\frac{3x^3}{1-x^2}\times\frac{1-x^2}{x}dx\tag{Problematic one}$$ which makes it an excessively lengthy problem. This problem is from the JEE Mains 2021 where barely 3 minutes is given for a problem and this would be quite solvable if and only if Physics and Chemistry were over in an hour or so. I am a 12th grader where we learn only substitution, partial fractions and integration by parts here. Any suggestions for alternative methods to do this faster? EDIT:
After the complete error rectification now the problem seems fine. I will be able to proceed.","['integration', 'indefinite-integrals', 'integrating-factor', 'ordinary-differential-equations']"
4850861,Prove the integral bound $\int_0^1 \sqrt{\frac{1-x^2}{1+x^2}} d x>\ln 2$,"Prove that $$I=\int_0^1 \sqrt{\frac{1-x^2}{1+x^2}} d x>\ln 2$$ My try: $$\begin{aligned}
& \forall x \in(0,1), \sqrt{1-x^2}>1-x^2 \\
& \Rightarrow \frac{\sqrt{1-x^2}}{\sqrt{1+x^2}}>\frac{1-x^2}{\sqrt{1+x^2}} \\
& \Rightarrow I>\int_0^1 \frac{\left(1-x^2\right) d x}{\sqrt{1+x^2}} \\
& \Rightarrow I>J-W
\end{aligned}$$ Where $J=\int_{0}^{1} \frac{1}{\sqrt{1+x^2}}\:dx,\:W=\int_{0}^{1} \frac{x^2}{\sqrt{1+x^2}}\:dx$ Now we have $$\begin{aligned}
& J=\left.\ln \left(x+\sqrt{1+x^2}\right)\right|_0 ^1 \\
& \Rightarrow J=\ln (\sqrt{2}+1)
\end{aligned}$$ Also: $$\begin{aligned}
W & =\int_0^1 \frac{x^2 d x}{\sqrt{1+x^2}} \\
\Rightarrow W & =\int_0^1 \sqrt{1+x^2} d x-J \\
\Rightarrow W & =\frac{x}{2} \sqrt{1+x^2}+\left.\frac{1}{2} \ln \left(x+\sqrt{x^2+1}\right)\right|_0 ^1-J \\
\Rightarrow W & =\frac{1}{\sqrt{2}}+\frac{1}{2} \ln (\sqrt{2}+1)-\ln (\sqrt{2}+1) \\
\Rightarrow W & =\frac{1}{\sqrt{2}}-\frac{\ln (\sqrt{2}+1)}{2}
\end{aligned}$$ Finally $$\begin{aligned}
I & >J-W \\
\Rightarrow \quad & I>\frac{3}{2} \ln (\sqrt{2}+1)-\frac{1}{\sqrt{2}}=0.6149
\end{aligned}$$ So I missed to reach the bound of $\ln 2$","['integration', 'calculus', 'definite-integrals', 'inequality']"
4850922,If $G\to\mathbb{Z}$ and $H\to\mathbb{Z}$ are surjective homomorphisms prove that exists $G\times H\to\mathbb{Z}$ with finitely generated kernel.,"Let $G$ and $H$ be two finitely generated groups. If $\varphi_G:G\to\mathbb{Z}$ and $\varphi_H:H\to\mathbb{Z}$ are surjective homomorphisms prove that exists $\varphi:G\times H\to\mathbb{Z}$ with finitely generated kernel. I read this fact on a paper but without proof, so I thought that this must be either easy or well-known, however I failed. The idea I had was that $G=\ker(\varphi_G)\rtimes\mathbb{Z}$ and $H=\ker(\varphi_H)\rtimes\mathbb{Z}$ so $K=(\ker(\varphi_G)\times\ker(\varphi_H))\rtimes \mathbb{Z}$ is finitely generated. So I was trying to find a map $\varphi$ with $K$ as its kernel, but I failed. The ideas of maps I thought were: $$\varphi(g,h)=\varphi_G(g)+\varphi_H(h),\varphi(g,h)=\varphi_G(g)-\varphi_H(h)$$ or $$\varphi(g,h)=\varphi_G(g)\varphi_H(h)$$ but none of them seemed to work and I don't know any other possibilty for $\varphi$ . Thanks for your help.","['group-homomorphism', 'finitely-generated', 'group-theory']"
4850931,Properties of tangent functor,"When I was learning differential geometry, I was told that differential of a function can be viewed as a functor, i.e. sending manifold into tangent space and function into its differential. Unfortunately, I believe that it's simply abstract nonsense, since I cannot find any references on the properties of tangent functor, for example, when it's exact, when it keeps injective ( a bit wired, since there are piles of injection, immersed or embedded for example ), or whether it keeps the limit or colimit. ( for finite product, it's evident ) So is there any reference for the properties of tangent functor? Thanks!","['tangent-spaces', 'category-theory', 'differential-geometry']"
4850953,Alternative of exterior power as a tensor algebra,"This question is related to my previous question . The $n$ -th exterior power of a vector space (over some field of characteristic zero) $U$ is a pair $(\wedge^{n}, \bigwedge^{n}U)$ where $\bigwedge^{n}U$ is a vector space over the same field and $\wedge^{n}: \overbrace{U\times \cdots \times U}^{\text{$n$ times}} \to \bigwedge^{n}U$ is an alternating $n$ -linear map satisfying the universal property. It can be proved that the $n$ -th exterior power of $U$ , if it exists, is unique up to isomorphisms. To prove that it in fact exists, one can explicitly construct a pair with the desired properties. In the literature, this is most commonly done by taking $\bigwedge^{n}U = \bigotimes^{n}U/K_{n}(U)$ , that is, the quotient space of the tensor algebra with the subspace $K_{n}(U)$ generated by products of the form $u_{1}\otimes \cdots \otimes u_{n}$ with $u_{i} = u_{j}$ for at least one pair of indices $i \neq j$ . One important result then is that such $\bigwedge^{n}U$ is isomorphic to $\text{Alt}^{n}(U)$ , the space of alternating $n$ -tensors. In other words, one can identify: $$\wedge^{n}(u_{1},...,u_{n}) = u_{1}\wedge\cdots \wedge u_{n} = \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)}.$$ There is one thing that bothers me and, after a quick search on MSE, it seems that it bothers a lot of other students as well. I still did not find a clear explanation of it, this is why I am asking this question. For simplicity, assume that $U$ is a finite-dimensional vector space over $\mathbb{C}$ . Let $f: U\times \cdots \times U \to \text{Alt}^{n}(U)$ be given by: $$f(u_{1},...,u_{n}) := \frac{1}{n!}\sum_{\sigma \in S_{n}}\text{sign}(\sigma)u_{\sigma(1)}\otimes \cdots \otimes u_{\sigma(n)}$$ be the alternating map. As the name suggests, this is an alternating $n$ -linear map and, from my previous question, it satisfies the universal property. In other words, $f$ can be used to define $\bigwedge^{n}U$ ; simply take $\bigwedge^{n}U = \text{Alt}^{n}(U)$ and $\wedge^{n} = f$ . However, this is not the standard construction and many posts on MSE claim that this is not a good way of defining $\bigwedge^{n}U$ . But I don't understand why. To be fair, some of the questions I saw about this topic have a more abstract setting; one takes $U$ to be a $\mathbb{K}$ -module. I don't have much background in algebra yet, but maybe in these more abstract settings these constructions are not equivalent for some reason. Concerning this topic and in the case of vector spaces , I have the following questions: Are these constructions equivalent or not? Can I use the alternating map $f$ to define my exterior power $\bigwedge^{n}U$ ? If so, does it work only in finite dimension or not? (It does not seem to me that it depends on the dimension but I might be missing something). Is there any real advantage of defining $\bigwedge^{n}U$ as $\bigotimes^{n}U/K_{n}(U)$ instead of $\bigwedge^{n}U = \text{Alt}^{n}(U)$ ?","['differential-geometry', 'abstract-algebra', 'linear-algebra', 'differential-topology', 'exterior-algebra']"
4851045,Sum of arrival times of Chinese Restaurant Process (CRP),"Suppose that a random sample $X_1, X_2, \ldots$ is drawn from a continuous spectrum of colors, or species, following a Chinese Restaurant Process distribution with parameter $|\alpha|$ (or equivalently $X_1,\ldots,X_n \mid P \sim P$ where $P$ is a Dirichlet process $\operatorname{DP}(\alpha)$ with atomless measure $\alpha$ , because the CRP is the unique urn process associated with the DP). Therefore we assume that $X_i$ takes values in some measurable space $(S, \mathcal{S})$ ; for simplicity take $S = \mathbb{R}^{+}$ and $\mathcal{S}$ equal to the Borel sigma algebra on $\mathbb{R}^{+}$ . Assume that we keep track of the unique species $X_j^*$ . We may do so by noting the arrival time of the $j$ th new color. Thus define $T_1=1$ and $$T_j=\min \left\{n: \sum_{i=1}^n D_i=j \: \: \text{for} \: \: D_i=1 \:\: \text{if} \: \: X_i \notin\left\{X_1, \ldots, X_{i-1}\right\}\right\}$$ and setting $X_j^*=X_{T_j}$ . A sequence $X_1, \ldots, X_n$ will contain a random number of unique species, usually denoted by $K_n$ . My question is: are there know results to compute: $$ \mathbb{E}\left[\sum_{j=1}^{K_n} T_j \right]$$ The same question is also posted on MathOverflow given its difficulty: here . PS: also upper bounds of the above quantity are of interest to me. PS2: the probability of a ""new draw"" for the Dirichlet process, i.e. $X_i \notin\left\{X_1, \ldots, X_{i-1}\right\}$ , is given by $\frac{|\alpha|}{|\alpha|+i}$ . Moreover the distinct values in a random sequence $X_1, X_2, \ldots$ sampled from a distribution generated from a $\mathrm{DP}(\alpha)$ -prior with atomless base measure form an i.i.d. sequence from $\bar{\alpha}$ .","['statistics', 'bayesian', 'combinatorics', 'nonparametric-statistics', 'probability-theory']"
4851046,Maximize smallest distance between any two of n points on a disk,"How do you distribute n points in a disk such that the smallest distance between any two of the points is maximized? Before 7, the solution is as simple as evenly distributing them along the perimeter of the disk, but at 7 the distance between two points distributed like that falls to 0.867, making it then more efficient to put one point in the center. But what about 8? 9? Is there a general answer for arbitrary n? If not a general solution for the arrangement, what about for the maximum smallest distance?","['circles', 'geometry']"
4851049,How do we know that recursion and induction are valid techniques in ZFC? How do we prove them? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 months ago . Improve this question I don't see which axioms would give rise to them.  Replacement uses a particular wff, so it must be of finite length, but I don't see how a finite wff can deal with all natural numbers.  People often describe recursive functions as wff, but I don't know why they are.","['elementary-set-theory', 'set-theory']"
4851087,"Suppose $|g(x)| \le x^4$, then find $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}$","Suppose $|g(x)| \le x^4$ ,for all $x$ , then find $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}$ . I did it as follows: $0 \le |g(x)| \le x^4$ , for all $x$ $0 \le \left|\frac{g(x)}{x}\right| \le |x^3|$ for all $x\neq 0$ Also, $\lim\limits_{x \rightarrow 0}0=0$ and $\lim\limits_{x \rightarrow 0}|x^3|=\lim\limits_{x \rightarrow 0}x^3=0$ . Then by the Squeeze Theorem, we have $\lim\limits_{x \rightarrow 0}\frac{g(x)}{x}=\lim\limits_{x \rightarrow 0}\left|\frac{g(x)}{x}\right|=0$ . I think it's right. I'm just not sure if I can consider $x \neq 0$ in the Squeeze Theorem.","['limits', 'calculus']"
4851140,"Find all positive sequneces $(s_n)_n$ such that: $\forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2$","The goal is to find all positive sequneces $(s_n)_n$ such that: $$
\forall n\geq 1, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2.
$$ My attempt is the following: Let us consider a sequence $(s_n)_n$ satisfying: $$
s_n>0, \; \sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^n s_k\right)^2,
$$ for all $n\geq 1$ . From the last equality we can remark: $$
\sum_{k=1}^n s_k^3 = \left(\sum_{k=1}^{n-1} s_k+ s_n\right)^2= \left(\sum_{k=1}^{n-1} s_k\right)^2+s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k.
$$ This implies that $$
\sum_{k=1}^n s_k^3 =\underbrace{\left(\sum_{k=1}^{n-1} s_k\right)^2}_{\sum_{k=1}^{n-1} s_k^3} +s_n^2 + 2s_n \sum_{k=1}^{n-1} s_k.
$$ So we get that $$
s_n^3 - s_n^2 - 2s_n \sum_{k=1}^{n-1} s_k= 0 \implies s_n^2 - s_n -2 \sum_{k=1}^{n-1} s_k = 0,
$$ since $s_n>0$ . However I cannot find the sequences $(s_n)_n$ that satisfy the above equality.","['calculus', 'sequences-and-series', 'real-analysis']"
4851146,Show that the recursive sequence $x_{k+1} = |x_k - \frac{x_k}{1-2M^2x_k^2}|$ is monotone,"I'm doing some exercises for an upcoming exam, and as part of a larger problem, I want to show that the given recursive sequence: $$x_{k+1} = \left|x_k - \frac{x_k}{1-2M^2x_k^2}\right|$$ is monotonly increasing if $$|x_0| \geq \frac 1{2M}$$ and $M>1$ . I'm pretty sure that induction is the right approach, but I can't get the induction step to work. I tried messing around with the inverse triangle equation , but I couldn't get far. Do you have any pointers on how to approach the problem?","['newton-raphson', 'induction', 'monotone-functions', 'analysis']"
4851160,Is a vector equal to the vector sum of it's components along any three non-coplanar unit vectors?,"Suppose $a,b$ and $c$ are the components of a vector P along three non-coplanar unit vectors A , B and C respectively. Then, is the below equality true ? $$\textbf{P}=a\textbf{A}+b\textbf{B}+c\textbf{C}$$ I know that it's true if : $$\textbf{A}=\hat{i}$$ $$\textbf{B}=\hat{j}$$ $$\textbf{C}=\hat{k}$$ where $\hat{i},\hat{j},\hat{k}$ are unit vectors along $x,y$ and $z$ axis respectively. But I am not sure if this holds for any general case (like what if these unit vectors are not perpendicular etc. etc. ). Context: My textbook assumed the above equality true in general to solve a question which was to prove that : $$\vec{p}\times\vec{q}=\vec{a}[a\space p\space q] +\vec{b}[b\space p\space q] +\vec{c}[c\space p \space q].  $$ where $\vec{a},\vec{b}$ and $\vec{c}$ are any three coplanar unit vectors and $[x\space y\space z]$ denotes the scalar triple product. Here's the original :","['vectors', 'linear-algebra', 'geometry']"
4851172,Is every multiplicative subgroup of $M_n(R)$ a subgroup of $GL_n(R)$?,"I'm afraid this turns out to be a really stupid question, but let's observe that the set of $n\times n$ matrices with coefficients in a ring $R$ can contain groups that (1) don't necessarily share the same identity with $GL_n(R)$ , and (2) are not included in $GL_n(R)$ . For example, the trivial subgroup consisting of the zero matrix (together with matrix multiplication). I was wondering whether every such ""multiplicative"" group of $n\times n$ matrices with coefficients in a ring $R$ is isomorphic to a subgroup of $GL_n(R)$ . In other words, does $GL_n(R)$ contain (an isomorphic copy of) every subgroup of $M_n(R)$ ? Again, by subgroup I mean a group that may not share the same identity element in $GL_n(R)$ . Would my question make more sense if ""subgroup"" was replaced with ""finite subgroup""","['group-theory', 'abstract-algebra', 'linear-algebra']"
4851244,"When is a field ""compatible"" with an abelian group?","Let $F$ be a field and $G$ an abelian group. We say that $F$ and $G$ are ""compatible"" if and only if there exists a function $M:F\times G\to G$ such that $M(\lambda,gh)=M(\lambda,g)M(\lambda,h)$ for all $\lambda\in F$ , $g,h\in G$ $M(\lambda+\mu,g)=M(\lambda,g)M(\mu,g)$ for all $\lambda,\mu\in F$ , $g\in G$ $M(\lambda\mu,g)=M(\lambda,M(\mu,g))$ for all $\lambda,\mu\in F$ , $g\in G$ $M(1,g)=g$ for all $g\in G$ $M(0,g)=\varepsilon$ for all $g\in G$ , where $\varepsilon$ is the identity of $G$ If $F$ and $G$ are compatible, then $(F,G)$ (or $\{F,G,M\}$ , or $\langle F,+_F,\cdot_F,G,+_g,M\rangle$ , or however else you care to denote it) forms a vector space, whose field is $F$ , whose vectors are $G$ with vector addition given by the group operation, and scalar multiplication given by $M$ . In fact, this is the definition of a vector space - a vector space consist precisely of a field and an abelian group related by a function like the one described. Question: What methods can be used to efficiently decide the compatibility of a given field with an arbitrary abelian group? There is a trivial case. If we take $F^X$ to be the group of functions from $X$ to $F$ with pointwise addition, and $G\cong F^X$ , then $F$ and $G$ are compatible by $M(\lambda,\varphi^{-1}(g))(x)=\lambda \varphi^{-1}(g)(x)$ , where $\varphi:F^X\to G$ is any group isomorphism. Every vector space for which I have a name is of this form, but, if I'm not mistaken, one can easily construct one which isn't by matching a group with an involution to a field whose additive group has none.","['vector-spaces', 'field-theory', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4851251,General formula for the sum of these coefficients,"I have non-negative coefficients $a_0, \ldots, a_T$ whose sum is one. I would now like to create new coefficients $$
b_k \propto a_k\left(1 - \frac{k}{T+1}\right) \qquad k=0, \ldots, T
$$ that also sum up to one. Of course, one way to do so, is to simply divide them by their sum. Since $T$ could be quite large, I wonder if there is a neat expression for the sum $$
\sum_{k=0}^T b_k = \sum_{k=0}^T a_k\left(1 - \frac{k}{T+1}\right),
$$ so that I can use that in practice. You can simplify this into $$
\sum_{k=0}^T b_k = 1 - \frac{1}{T+1}\sum_{k=0}^T a_k k
$$ but I couldn't simplify it further.","['algebra-precalculus', 'abstract-algebra', 'discrete-mathematics']"
4851276,Counting one pair poker hands (five cards per hand),"I understand how to find the number of one-pair poker hands there are, but I am being asked to explain how the following logic is incorrect: Choose any card ( $52$ ways), then choose another card of the same value to make the pair ( $3$ ways), then choose three more cards of different values ( $48$ ways, $44$ ways, $40$ ways). The answer is $52 \cdot 3 \cdot 48 \cdot 44 \cdot 40 = 13'178'880$ . I can't quite figure out how to explain why this is wrong besides the order of cards drawn doesn't matter. I also need to explain why the wrong answer is a multiple of the right answer ( $1'098'240$ ).","['combinations', 'combinatorics']"
4851299,Automorphisms of CW complexes and fixed points,"Let $X$ be a CW complex, and let $F:X\to X$ be an homeomorphism that sends each cell onto some cell; notice that we could say that $F$ is an automorphism of the CW complex since it preserves its cell decomposition. Assume that $F$ fixes some cell $e$ , that is, $F(e)=e$ . Does $F$ necessarily have a fixed point? So far, I've considered the following. Let $n$ be the dimension of $e$ . If $n=0$ , then the answer to the question is trivially yes. If $n=1$ , the answer is also yes, because either the closure $\overline{e}$ of $e$ is homeomorphic to $[0,1]$ , in which case we can use the Brouwer fixed theorem in a rather elementary way, or to a circle, in which case the only $0$ -cell attached to the circle is necessarily fixed. For $n>1$ , the situation is not as obvious to me. If one assumes that the CW is contractible, there is a solution here, which apparently is related to the Lefschetz fixed point theorem, a generalization of the Brouwer fixed point theorem: A version of Brower's fixed point theorem for contractible sets? I'm not exactly sure how to tackle the general case. Any help is appreciated!","['fixed-points', 'fixed-point-theorems', 'cw-complexes', 'general-topology', 'algebraic-topology']"
4851361,What is the largest known prime of the form $(3^n + 1)/4$? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 months ago . Improve this question Consider integers of the form $(3^n + 1)/4$ . I was able to find some primes. examples : $n = 13,23,43$ all give primes. It seems similar to Fermat primes or Mersenne primes. So how many of $(3^n + 1)/4$ are prime ? Are there infinitely many ? Or only finite ? Since this question may be to hard ; What is the largest known prime of the form $(3^n + 1)/4$ ? Any sources ?","['exponentiation', 'number-theory', 'examples-counterexamples', 'reference-request', 'prime-numbers']"
4851392,$e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}}$ - Find $x$ (G.H. Hardy),Approximate the large positive root of the equation $$e^{e^x} = 10^{10} x^{10}e^{10^{10} x^{10}}$$ (attributed to G.H. Hardy) This is a great problem which I can find no verifiable source or discussion of online.  Is there a source where Hardy actually asks this question?  How did Hardy (or others) approach it?,"['contest-math', 'calculus', 'reference-request', 'real-analysis']"
4851427,Geometric interpretation of Hermite's identity,"I came across an elegant interpretation of the well known reciprocity law for the floor function in the USSR problem book and I was wondering if there was a similar sort of interpretation for Hermite's identity; Reciprocity law interpretation: (I'm sorry if I'm not supposed to attach images for this kind of thing, but I honestly do not know how to explain it better than the book itself.) I've seen both an elementary proof for Hermite's identity and one using the periodicity of the function $f(x)=[x]+\left[x+\dfrac{1}{n}\right]...+\left[x+\dfrac{n-1}{n}\right]-[nx]$ . Does anyone have a proof that is more geometric in flavor, similar to the one above, for Hermite's identity?","['number-theory', 'ceiling-and-floor-functions', 'geometry']"
4851443,Solving $\sin x= 2\sin(x-30^\circ) \sin(40^{\circ})$,"To solve $\sin x= 2\sin(x-30^\circ) \sin(40^{\circ})$ I expanded RHS by using $\sin(\alpha-\beta)=\sin\alpha\cos\beta-\sin\beta\cos\alpha$ , $$\sin x= 2\sin(40^{\circ})\times\left(\frac{\sqrt3}2\sin x- \frac12\cos x\right)$$ $$\left(\sqrt3 \sin(40^{\circ})-1\right)\sin x= 
\sin(40^{\circ})\cos x$$ $$\cot x= \sqrt 3- \csc(40^{\circ})$$ But from here I'm not sure how to continue. I've also tried using $2\sin\alpha\sin\beta= \cos(\alpha-\beta)-\cos(\alpha+\beta)$ for the RHS which result in, $\sin x= \cos(x-70^\circ)-\cos(x+10^\circ)$ but don't know how to continue.",['trigonometry']
4851495,What am I doing wrong with trying to find the intersection equation between an offset Ellipse and a Ray?,"In relation to another, recent post, I am having trouble finding the intersection equation between an offset Ellipse and a Ray (line restricted by a point). Due to the equations being necessary for a computational application, the parameters cannot be replaced by actual values, this is done in the computations. The Ray is defined as follows: $$
\begin{cases} 
 x = o_1 + m \cdot v_1, \\ 
 y = o_2 + m \cdot v_2, 
\end{cases}
$$ $O = (o_1, o_2)$ is the origin, or restriction point of the ray, and $$
V = 
\begin{bmatrix}
v_1 \\ 
v_2
\end{bmatrix}
$$ The Ellipse is defined as follows: $$
\frac{(x-h)^2}{a^2} + \frac{(y - k)^2}{b^2} = 1,
$$ where $a$ and $b$ are the eccentricity in $x$ and $y$ , $h$ and $k$ the offset in $x$ and $y$ from the origin. To simplify the algebra, I converted the parametric equation of the ray to a cartesian equation: $$
y = \frac{v_2}{v_1} x + o_2 - \frac{v_2}{v_1} o_1,
$$ and then set $$ 
c = \frac{v_2}{v_1}, d = o_2 - c\cdot o_1.
$$ The final result is this: $$ 
x = 
\frac{b^2h-a^2c(d-k) \pm ab\sqrt{b^2+a^2c^2 -(ch + d - k)^2}}{b^2+a^2c^2},\ y = cx+d.
$$ This equation should give the intersection between a line $y = cx + d$ and the earlier defined ellipse. however, when setting up this equation in Geogebra, I do not get the expected results. I have checked and redone the algebra multiple times, and I cannot find the error or bad approach angle.
The geogebra setup is here: https://www.geogebra.org/calculator/zpvnjtkj EDIT: the equations are for use in code, where the variables will be replaced each time the equation is used and would be ""known"". the code is for a 2D light propagation, with reflections from elliptical and linear mirrors. Due to accumulating errors, the equation has to be as good as possible and testing using geogebra to check if the calculated intersections are on the ellipse circumference is not working. I will redo the algebra by solving for $m$ or $t$ , to use better notation. EDIT 2: After constructing the ellipse, Ray and intersection equation solving for the factor of $V$ , I found something interesting: If a or $b$ are bigger or smaller than 1, the intersection is off. However, if $a$ and $b$ are equal to 1: the intersection is correct. https://www.desmos.com/calculator/7fhdwyzwm8 Edit 3: Below is the desmos graph when a and b are equal to 1 or -1: $a$ and $b$ are equal to 1 or -1"" /> Below is the same graph when b is not equal to 1 or minus 1: It might be slightly hard to see, but the blue point on the green line is the point the intersection equation finds. in the first image, the point is on the ellipse periphery. $a$ and $b$ are both equal to 1. in the second image, the point is not on the ellipse periphery. $b$ in the second image is equal to $-1.76$","['analytic-geometry', 'systems-of-equations', 'solution-verification', 'geometry']"
4851507,Uniform law of large numbers for ergodic stationary sequence,"I am trying to apply a uniform law of large numbers, which is stated in Lemma 7.2 of ""Econometrics"" by Fumio Hayashi. The starting point is the stochastic process $\{x_t\}$ , which we assume is stationary ergodic. The theorem then states (using different notation than me) that under some regularity conditions, the loss function \begin{equation}
L_T(\theta) = \frac{1}{T} \sum_{t=1}^T \ell(x_t; \theta)
\end{equation} converges in probability to $\mathbf{E}[\ell(x_t; \theta)]$ , uniformly in $\theta$ . The problem, however, is that my loss function is on the form \begin{equation}
L_T(\theta) = \frac{1}{T} \sum_{t=1}^T \ell(x_1, \dots, x_t; \theta)
\end{equation} This loss arises from the log-likelihood of a sequence model: $\ell(x_1, \dots, x_t; \theta) = \log p_{\theta}(x_t \, | \, x_1, \dots, x_{t-1})$ . Do there exist similar results, or assumptions I can make to apply this result in my setting? I would guess that I need to assume that the effect of $x_{t-s}$ on $\ell(x_1, \dots, x_t; \theta)$ decays with $s$ in some way (maybe some kind of geometric bound on the covariance of $x_t$ could achieve this?), but I am not able to turn this into sufficiently precise statements as of yet.","['statistics', 'probability-limit-theorems', 'ergodic-theory', 'law-of-large-numbers', 'probability-theory']"
4851527,Fourier Transform of Gaussian with imaginary pole,"I'm struggling at the moment with the following integral, which is essentially an inverse Fourier transform of a Gaussian with a single pole on the imaginary axis: $$I(Q) = \frac{1}{2}\int_{-\infty}^{\infty} dx \frac{e^{ix a} e^{-\beta x^2}}{Q^2+x^2} $$ A solution to this integral is given in Ryzhik & Gradshteyn (7th ed, pg 504, 3.954) as $$I = \frac{\pi}{4Q}e^{\beta Q^2}\left[2\cosh{Qa} - e^{-Qa}\text{erf}\left(Q\sqrt{\beta}-\frac{a}{2\sqrt{\beta}}\right)- e^{Qa}\text{erf}\left(Q\sqrt{\beta}+\frac{a}{2\sqrt{\beta}}\right)\right]$$ I've been trying to figure this solution out, as I will eventually need to take a similar integral with more complicated poles, so this is a good starting integral to solve. However, I don't intuitively see how to get to this result in R&G. I've tried rectangular contours but can't seem to see the line of thought to take me to this result. Any help would be very much appreciated! Thanks :)","['integration', 'definite-integrals', 'complex-analysis', 'contour-integration', 'gaussian-integral']"
4851557,Example for $[G : G\cap H] \neq [H:G\cap H]$ with isomorphic subgroups $G \cong H$?,"For isomorphic finite subgroups $G,H$ of a group $A$ it holds $[G : G\cap H] = [H: G\cap H]$ , since $[G : G\cap H] = \frac{|G|}{|G\cap H|} = \frac{|H|}{|G\cap H|} =[H: G\cap H]$ . Does this also hold for general isomorphic subgroups which may not be finite? Does it hold if one of the indices is finite? I think it does not hold in general, unless the isomorphism $G \to H$ takes $G\cap H$ to $G\cap H$ . Concretely, has someone an example for a group $A$ with subgroup $G$ such that $[G : G \cap (aGa^{-1})] \neq [aGa^{-1} : G \cap (aGa^{-1})]$ for some $a \in A$ ?","['quotient-group', 'group-theory', 'group-isomorphism', 'examples-counterexamples']"
4851586,Definitions of coercivity - functional analysis,"In my PDE courses I've come across two different definitions or coercivity of a functional $\mathit{F}: \mathit{H} \rightarrow \mathbb{R}$ where $\mathit{H}$ is a Hilbert space. Definition 1 : For the product space $\mathit{H} \times \mathit{H}$ for some Hilbert space $\mathit{H}, \mathit{F}: \mathit{H} \times \mathit{H} \rightarrow \mathbb{R}$ , $\mathit{F}$ is coercive means that there exists a positive constant $K$ , so that $\mathit{F}(x,x) \geq K (x,x)$ with $(.,.)$ being the inner product. (It can also be defined more generally for a normed space, https://mathworld.wolfram.com/CoerciveFunctional.html ). Definition 2 : $\mathit{F}$ is called coercive, if for some $a \in \mathbb{R},$ the corresponding sublevel set is non-empty and bounded. Question 1 : Are these two definitions equivalent in some way? Question 2 : I've seen another definition where the Hilbert space is $\mathbb{R}^n$ where coercivity is defined as the property in Definition 2 , but holding for every $a \in \mathbb{R}.$ Why is this difference?","['coercive', 'calculus-of-variations', 'hilbert-spaces', 'functional-analysis', 'partial-differential-equations']"
4851638,"A square contains many random points. From each point, a disc grows until it hits the nearest neighboring point. What is the total area of the discs?","A unit square lamina contains $n$ independent uniformly random points. Each point is the centre of a disc whose perimeter touches the nearest neighboring point. Here is an example with $n=20$ . In this example, the total area of the discs (i.e. the sum of areas of the discs, not the area of the union of the discs) is approximately $1.0105$ . What does the total area of the discs approach, as $n\to\infty$ ? The answer is $1$ , i.e. the area of the square itself. I give a non-intuitive proof below. But since this result is quite elegant, I am looking for an intuitive explanation. My non-intuitive proof As $n\to\infty$ , the points approach a 2D Poisson process with intensity $n$ . For a chosen point, let $x$ be the distance to its nearest neighbor, which has density function $$f(x)=2n\pi xe^{-n\pi x^2}$$ So the total area of the discs approaches $$n\int_0^\infty \pi x^2 f(x)\mathrm dx=1$$ by integration by parts twice. Remarks The shape of the lamina (e.g. square) does not matter, since we are taking $n\to\infty$ . The answer is the same in one, two or three dimensions. (In one dimension, the nearest neighbor distance density function is $f(x)=2ne^{-2nx}$ . In three dimensions , it is $f(x)=4n\pi x^2e^{-\frac{4n\pi}{3}x^3}$ .) This question was inspired by a related question , ""A square contains many random points. From each point, a disc grows until it hits another disc. What proportion of the square is covered by the discs?"".","['circles', 'geometry', 'expected-value', 'poisson-process', 'intuition']"
4851640,$\partial_{z}$ and $\partial_{\bar{z}}$: what are these vector fields from a geometrical point of view?,"In complex analysis, we are taught that instead of coordinates $x$ , $y$ on the complex plane, one can use $z$ , $\bar{z}$ , then, for instance, the Cauchy-Riemann conditions become $\frac{\partial }{\partial \bar{z}}f(z, \bar{z})=0$ , and $\Delta = \frac{1}{4}\frac{\partial^2}{\partial z \partial \bar{z}}$ . This was explained to me in the following way: we simply perform a change of coordinates $z = x + y$ , $\bar{z}=x-y$ on $\mathbb{R}^2$ . However, if this was the case, functions with $\frac{\partial }{\partial \bar{z}}=0$ would be constant along lines $y=-x+C$ , which is clearly not true (for instance, this would mean isolated zeros or isolated singularities are impossible). In general, this means that $\partial_{\bar{z}}$ can't be understood as a directional derivative in the complex plane, i.e. as a vector field on the complex plane with real coefficients, because holomorphic functions would then be constant along the integral lines of this vector field. So, I guess, either the derivative or the change of coordinates should be understood in some other sense. Reading old MSE answers to similar questions, I understood that a) This confusion about the meaning of $\partial_{\bar{z}}$ is very common among people studying complex analysis, including, apparently, those who answer questions about Wirtinger derivatives (because often the answers boil down to treating $\partial_{z}$ and $\partial_{\bar{z}}$ as real vector fields). b) The correct answer (as opposed to hand-waving the issue) has something to do with the structure of complex manifold on $\mathbb{C}$ . So my question is: where can I find an elementary introduction to the theory of complex manifolds that would carefully explain how $\partial_{z}$ and $\partial_{\bar{z}}$ fields work? Edit: Perhaps, I should explain that my naive understanding of complex manifolds leads to further confusions. For instance, $\mathbb{C}$ is a one-dimensional complex manifold. The experience teaches me it should have one linearly independent vector at each tangent space, not two, which raises more questions about what $\partial_{z}$ and $\partial_{\bar{z}}$ really are.","['complex-analysis', 'complex-geometry', 'complex-manifolds', 'reference-request']"
4851652,Evaluating $\sum\limits_{n=0}^\infty\Gamma(in+1)$,"Our goal is evaluating: $$S=\sum_{n=0}^\infty(in)!= \sum\limits_{n=0}^\infty\Gamma(in+1) = 1.66159412484058… - 0.09593230517926 …i $$ which hypergeometric function generalizations cannot express. Attempting to rewrite it using $\Gamma(n)$ ’s integral representation does not work either as the geometric series converges for all $0\le x$ . Even using a regularization and Cauchy principal value at $x=1$ : $$S= \sum_{n=0}^\infty\int_0^\infty e^{-x}x^{i n}dx\mathop=^\text{reg}\int_0^\infty\frac{e^{-x}}{1-x^i}dx\mathop\approx^\text{PV}0.5-0.041380i$$ gives incorrect numerical results. The last idea was to convert it to a Barnes type integral, like with a Ramanujan interpolation based formula. With $\Gamma(in+1)$ , the result diverges, but using $\Gamma(1-in)$ : $$\bar S=1+\frac i2\int_{c-i\infty}^{c+i\infty}((\cot(\pi z)+i)\Gamma(1-i z)dz\iff S=1-\frac i2\int_{ci-\infty}^{ci+\infty}(\coth(\pi z)-1)\Gamma(z+1)dz$$ Another useful formula may be $\Gamma(1-in)=-\pi i\frac{\operatorname{csch}(\pi n)}{\Gamma(i n)}$ . Finally, Since $\lim_\limits{n\to\infty}\frac{2\ln(\text{Re}(\Gamma(in+1)))}{\pi n}=-1$ , the sum’s real part roughly converges at $e^{-\frac\pi2n}$ speed. However, not much so far helps evaluate the sum. What can we do?","['integration', 'factorial', 'gamma-function', 'sequences-and-series', 'complex-numbers']"
4851709,"Two definitions of mapping being equivalent, which one is correct and why?","The following are two definitions of the concept that two mappings $ f:A\mapsto B $ and $ g:A^\prime\mapsto B^\prime $ are equivalent. Def1 : Both the domains and codomains are the same, i.e., $A=A^\prime, B=B^\prime$ , and $\forall x\in A, f(x) = g(x)$ . Def2 : Only the domains are the same , i.e., $A=A^\prime$ , and $\forall x\in A, f(x) = g(x)$ . We see that Def2 in principle only requires that $f(A) \subseteq B\bigcap B^\prime$ , which is looser than Def1 which requires $f(A) \subseteq B=B^\prime$ I am asking which one is correct and why. To be more precise, I haven't encountered those two definitions written explicitly. I am inferring Def1 from some other definitions. One example is the morphism in the category theory discussed below. The second one comes from my naive argument that mapping takes one thing to another, so as long as with the same input we obtain the same output then no one can differentiate those two mappings at all. Example in category theory The definition of the category requires that if $A\neq A^\prime$ or $B\neq B^\prime$ , then $\mathrm{Mor}(A,B)$ and $\mathrm{Mor}(A^\prime,B^\prime)$ are disjoint. Let's take the category $\mathrm{\mathbf{Grp}}$ as an example. The morphism here is the group homomorphism, which down in the deep is just a mapping. If $A\neq A^\prime$ , then both definitions of mapping equivalence indicate that $\mathrm{Mor}(A,B) \bigcap \mathrm{Mor}(A^\prime,B^\prime) = \emptyset$ . But what if $A = A^\prime, B\subsetneq B^\prime$ ? Under Def2 we would then have $\mathrm{Mor}(A,B) \subsetneq \mathrm{Mor}(A^\prime,B^\prime)$ . Clearly, Def1 is what is implicit in the definition of category. Function understood as a subset of Cartesian product Another example is an alternate interpretation of function as a subset of Cartesian product. $f:A\mapsto B$ can be defined as a subset of $A\times B$ , where $\forall a\in A$ there exists one and only one elements in $f\subseteq A\times B$ whose first element is $a$ . Under this interpretation, Def1 of mapping equivalence requires not only that this subset $f$ is the same but also that it comes out of the same total set $A\times B$ . This seems a little bit over-definition for me.","['equivalence-relations', 'definition', 'functions', 'elementary-set-theory', 'soft-question']"
4851727,Show $f^{-1}(f(S))=S$ if and only if $f$ is injective. (stuck on how injectivity helps) [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 months ago . Improve this question This question is about a standard exercise but my reasons for asking here are: I am self-teaching and too often I think I understand something but in fact I don't. The answers to the same problem on this site don't help with my specific question here. My solution is long - so I suspect I have missed something important. Problem, part of Exercise 3.4.5 Tao's Analysis I 4th Ed. Let $f : X \to Y$ be a function from one set $X$ to another set $Y$ . Show $f^{-1}(f(S))=S$ for every $S \subseteq X$ if and only if $f$ is injective. Because I want to isolate the root cause of any misunderstanding, I will write the solution in small steps. Strategy We need to prove both: Part 1: $f^{-1}(f(S))=S \implies f$ is injective. Part 2: $f$ is injective $\implies f^{-1}(f(S)) = S$ . Part 1 Let's start with $f^{-1}(f(S))=S \implies f$ is injective. This is a statement of the form $A \implies B$ , which we'll prove by showing the logically equivalent $\neg B \implies \neg A$ . So, let's consider $f$ is not injective. This means there could be an $x \in S$ and an $x' \in X \setminus S$ such that $f(x)=f(x')$ . Question 1: Is this a valid argument to start with? We could have $x \in S$ and $x' \in S$ such that $x \neq x$ but $f(x)=f(x')$ . The inverse image $f^{-1}(f(S))$ is, by definition, the set $\{x \in X: f(x) \in f(S)\}$ . Using our definitions of $x$ and $x'$ , both qualify to be in this set as $f(x')=f(x) \in f(S)$ . However $x' \notin S$ , which tells us $f^{-1}(f(S)) \neq S$ . Therefore, $f$ not injective implies $f^{-1}(f(S)) \neq S$ . This is logically equivalent to $f(f^{-1}(S))=S \implies f$ is injective. Question 2: Even if this prove by contradiction is correct, is the the simplest way? Part 2 Now let's show $f$ is injective $\implies f^{-1}(f(S))=S$ . To do this we need to show that under the assumption $f$ is injective, both of the following are true: $S \subseteq f^{-1}(f(S))$ $f^{-1}(f(S)) \subseteq S$ Let's start with the first $S \subseteq f^{-1}(f(S))$ . If $x \in S$ , then $f(x) \in f(S)$ .  By definition of inverse images, the set $f^{-1}(f(S))$ is $\{x \in X: f(x) \in f(S)\}$ . Therefore $x$ is in this set, because it meets the membership criteria, $f(x) \in f(S)$ . So we have $x \in S \implies x \in f^{-1}(f(S))$ . This is equivalent to $S \subseteq f^{-1}(f(S))$ . Note we didn't need injectivity to show this. Question 3: Is this correct that we didn't need injectivity? Comment I am least sure of the following. Now let's consider $f^{-1}(f(S)) \subseteq S$ . Consider an $x \in X$ that is a member of $f^{-1}(f(S))$ . By definition of inverse images, this means $f(x) \in f(S)$ . Every $y \in f(S)$ has an $s \in S$ such that $f(s) = y$ . Question 4: I think this is wrong. This looks like surjectivity which is not relevant to this problem. Suppose there is another $x' \in X \setminus S$ that is a member of $f^{-1}(f(S))$ . That would require $f(x') \in f(S)$ . This means there is a $y \in f(S)$ where $f(x) =y$ and $f(x')=y$ . This contradicts the injectivity of $f$ . There can only be one $x \in X$ that maps to a given $y \in f(S)$ and since we know $s \in S$ is such that $f(s) \in f(S)$ , then that $x$ must be in $S$ . Hence $f^{-1}(f(S)) \subseteq S$ .","['elementary-set-theory', 'functions', 'solution-verification']"
4851758,Asymptotic distribution of $\frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m 1(X_i > Y_j)$,"Suppose that $X_1, ..., X_n$ are i.i.d. draws of some random variable $X$ ; likewise, $Y_1, ..., Y_m$ are i.i.d. draws of some random variable $Y$ . If we want to estimate $P(X > Y)$ , we may consider the fraction of all the $(X_i, Y_j)$ pairs which have the property that $X_i > Y_j$ . That is, consider $$\frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m 1(X_i > Y_j) = \frac{1}{n m} \sum_{i, j} 1(X_i > Y_j)$$ where $1$ is the indicator function. One sees that this is an unbiased estimator of $P(X > Y)$ : $$ E[\frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m 1(X_i > Y_j)] = \frac{1}{n m} \sum_{i=1}^n \sum_{j=1}^m E[1(X_i > Y_j)] = \frac{nm}{nm}P(X > Y) = P(X > Y)$$ I have two (related) questions, however: (1) Can one show that our estimator is consistent using (something like) the law of large numbers? I'm a bit confused here since the LLN usually applies to a simple sample average (with a single summation). (2) Can one show that our estimator is asymptotically normal using (something like) the central limit theorem?","['statistics', 'probability']"
4851784,closed form for limit?,Consider the function $$  f(x)=\lim_{k \to \infty}\bigg(\int_0^x \sum_{n=1}^k e^{\frac{\log n}{\log r}}~dr \bigg)\bigg( \int_0^1 \sum_{n=1}^k e^{\frac{\log n}{\log r}}~dr \bigg)^{-1} $$ I want to find a closed form for $$\sum_{n=1}^\infty  f\bigg( e^{-\sqrt{\log n}} \bigg). $$ I simplified this to $$\sum_{n=1}^\infty f\bigg( e^{-\sqrt{\log n}} \bigg)=\sum_{n=1}^\infty \bigg( \lim_{k \to \infty}\bigg(\int_0^{e^{-\sqrt{\log n}}} \sum_{n=1}^k e^{\frac{\log n}{\log r}}~dr \bigg)\bigg( \int_0^1 \sum_{n=1}^k e^{\frac{\log n}{\log r}}~dr \bigg)^{-1}\bigg)$$ After simplyfying more I arrived at: $$ = 1/2+1/4\bigg(\lim_{k\to\infty}\bigg( \sum_{n=1}^k e^{-2\sqrt{\log n}} \bigg)\bigg(\sum_{n=1}^k \sqrt{\log n}~K_1\big(2\sqrt{{\log n}} \big) \bigg)^{-1}\bigg). $$ Is there a closed form for the limit in parentheses? $K_1$ is the modified bessel function. But I'm unable to make progress on simplifying the last limit. I did think to look at the asymptotic expansion for large $x$ here: $$K_{\nu}(x)\sim \sqrt{\frac{\pi}{2x}}e^{-x}\Big(1+\frac{4\nu^2-1}{8x}+\frac{(4\nu^2-1)(4\nu^2-9)}{2!(8x)^2}+\ldots\Big).$$ Then some calculations based on the first 2 terms of the expansion seem to suggest $$\sum_{n=1}^\infty f\bigg( e^{-\sqrt{\log n}} \bigg)=1/2$$ because we basically have $$  1/2+C/4\bigg( \lim_{k\to \infty} \frac{\sum_{n=1}^k e^{-2\sqrt{\log n}}}{\sum_{n=1}^k(\log n)^{1/4}e^{-2\sqrt{\log n}}}  \bigg)  $$,"['calculus', 'functions', 'closed-form', 'sequences-and-series', 'limits']"
4851822,What translating a vertex does to a function in completed square form [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 5 months ago . Improve this question I just wanted to see if I was thinking about this in the right way Say we have the function $f(x)=2x^2-4x-7$ In compledted square form that gives us $f(x)=2(x-1)^2-9$ this gives $(1,-9)$ as the coordinates of the vertex now lets say we have to translate that vertex to the point $(-1,-5)$ so we're mapping all x values by negative two and all y values by 4. so then would our function become $y+4=2(x-2)^2-4(x-2)-7$ giving us a copmpleted square form of $f(x)=2(x-3)^2-13$ is that the right way to be thinking about this ?","['functions', 'transformation']"
4851827,Geometry of a stool leg,"I am building a stool with cross-braced legs and I am trying to figure out how to properly compute the size of the sides.
I made the schema below: ABCD is a rectangle of length L and width l ; the inner shape is the leg, so GH is parallel to EF ; w is the width of the leg. I am looking for a and b such as: BG = BE = b and DH = 2xDF = 2a . My intuition is that for a given L , l and w , there should a unique a and b that satisfy the constraints. I first tried to calculate the area of ABCD in two ways: A_1 = L.l A_2 = sum of the area of all the inner right triangles and rectangle. Then solve for A_1 = A_2 . But A_2 becomes rapidly quite hairy so I don't think it is the right approach. Any suggestion? PS: for those curious about the stool, you can see it on this picture .",['geometry']
4851883,How would you prove that the orthocentres of triangles with one moving point form a circular arc?,"Recently, I was working on some olympiad geometry problems, and I noticed that if two points $A,B$ are fixed on a circle $\omega$ , then a variable point $C$ on $\omega$ defines a set of orthocentres of $\Delta ABC$ , and they appear to be on a circle (which also passes through $A,B$ ). An image from a reproduction on GeoGebra is below (orthocentres are $I,D,J,H$ ): I have tried to prove this using similarity, congruence, circle theorems, Euler line, etc., but haven't been able to make progress. Does anyone know of a proof of this or the name of the theorem, if it exists?","['euclidean-geometry', 'triangle-centres', 'circles', 'geometry']"
4851900,Simple formula for the probability that a random walk with non equal steps does not hit zero?,"I've been interested with gambler-ruin-type problems with steps of unequal size. In doing so, I stumbled upon a very curious observation that I cannot manage to prove. For TL;DR, see the conjecture below. Here's the setting:
Let $d\in\{1,2,3,\ldots\}$ , and define i.i.d. random variables $X_i$ with distribution $$\mathbb P\{X_i=1\}=p\qquad\qquad\mathbb P\{X_i=-d\}=1-p.$$ I'm interested in the random walk $$S_n=S_0+X_1+\cdots+X_n.$$ So long as $p>\frac{d}{d+1}$ , we have that $\mathbb E(X_i)>0$ , and thus the random walk drifts off to $+\infty$ with probability 1.
I'm interested in the probability that the random walk starting at some $k\geq 1$ never touches zero before it diverges: $$P(k)=\mathbb P\{\forall n:~S_n>0~|~S_0=k\}.$$ By standard first-step analysis (i.e., $P(k)=pP(k+1)+(1-p)P(k-d)$ ) we can in principle recursively solve for $P(k)$ in terms of $P(1)$ , i.e., there is some constant $c_k$ such that $$P(k)=c_kP(1),\qquad k\geq1.\tag{$\ast$}$$ As far as I can tell, there is no simple formula for $c_k$ for arbitrary $k$ when $d\geq2$ . However, for finite $k$ not too large, $c_k$ can be calculated using a computer. That said, this leaves out the value of $P(1)$ , which is of course required to calculate $(\ast)$ . At first, I was resigned to just approximate $P(1)$ numerically by solving the problem $$P^N(k)=\mathbb P\{S_n\text{ hits $N$ before $0$}~|~S_0=k\}$$ using a computer; in doing so, we can
use the boundary condition $P^N(N)=1$ to solve for $P^N(1)$ . If $N$ is very large then $P^N(1)\approx P(1)$ . As I did this, I was struck by how often I would get seemingly extremely simple results for what $P(1)$ supposedly is, such as $0.2000000000$ , $0.3333333333$ , or $0.6666666667$ (I was instead expecting stuff like $0.5437927472$ or whatever). After playing around with the values of $p$ and $d$ , I eventually came up with the following ""conjecture,"" which shows remarkable agreement with the numerics I just mentioned no matter what I put for $p$ and $d$ : Conjecture. For every $d\in\{1,2,3,\ldots\}$ and $\frac{d}{d+1}<p<1$ , $$P(1)=\mathbb P\{\forall n:~S_n>0~|~S_0=1\}=1+d-\frac dp=1-d\frac{1-p}{p}.$$ The simplicity of this formula leads me to believe that the must be a simple argument to compute $P(1)$ , but I haven't been able to figure it out.","['random-walk', 'probability-theory', 'probability']"
4851909,What are the continuous outer automorphisms of the general linear group?,"Is the only continuous outer automorphism of $\operatorname{GL}(n, \mathbb{R})$ the transpose inverse map $g \mapsto (g^\intercal)^{-1}$ ? If not, what other continuous outer automorphisms are there?","['automorphism-group', 'general-linear-group', 'continuity', 'group-theory', 'lie-groups']"
4851929,Archimedes method to estimate $\pi$?,"Does anyone know if there are additional steps beyond what is pictured below, after the triangle at point $K$ is found to estimate $\pi$ ? This seems to improve a $6$ sides polygon to a $12$ sided one, but I read that he found $\pi$ using $n$ sided polygons. Reference",['trigonometry']
4851938,Revisited question Lebesgue Measure of Symmetric Difference is 0,"Following this question: Dudley Section 3.4, Problem 1: Lebesgue Measure of Symmetric Difference is 0 Let E be a Lebesgue measurable set, such that for all $x$ in a dense subset of $\mathbb{R}$ , $m(E\Delta (E+x)) =0$ . Show that either $m (E)$ or $m(\mathbb{R}/E) =0$ . I did not understand the first answer in this question. Can anyone add some details for that answer? Or another approach.. Proof: I try to use the contradiction by following two results from For a set of positive measure there is an interval in which its density is high, $\mu(E\cap I)> \rho \mu(I)$ : Let $E$ be Lebesgue measurable, with $\mu(E)>0$ (here $\mu$ denotes the Lebesgue measure). Then: for any $0<\rho<1$ , there exists an open interval $I$ such that $\mu(E \cap I)> \rho \cdot \mu(I)$ . the set $E-E = \{x-y : x, y \in E\}$ contains an (open) interval centered at $0$ (in particular, if $\rho > 3/4$ , the text I am using suggests that $(-\frac{1}{2} \mu(I), \frac{1}{2}\mu(I)) \subseteq E-E$ ). Now, suppose that $m(E)>0$ and $m(E^c)>0$ . Then for $\rho=\frac{3}{4}$ so that there is $I_1$ with $$
\mu(E\cap I_1)>\frac{3}{4}\mu(I_1)
$$ and apply the result to $E^c$ to get there is a interval $I_2$ so that $$
\mu(E\cap I_2)<\frac{1}{4}\mu(I_2)
$$ Then by the second result, we have there is an open interval so that $$
(-0.5m(I_1), 0.5m(I_2))\subset E-E
$$ and $$
(-0.5m(I_2), 0.5m(I_2))\subset E^c-E^c.
$$ Since $m(E\Delta (E+q))=0$ (i.e., $E=E^c$ a.e.), then for $E\cap (I_1\cup I_2)$ we have $$
E\cap (I_1\cup I_2)=(E+q)\cap (I_1\cup I_2) a.e.
$$ Thus, $m(E\cap (I_1\cup I_2))=m((E+q)\cap (I_1\cup I_2))$ Then for $x\in (-0.5m(I_1), 0.5m(I_2))$ , we have $x=a-b$ for some $a, b\in E$ , and for $y\in (-0.5m(I_2), 0.5m(I_2))$ , we have $y=c-d$ for some $c,d \in E^c$ .","['measure-theory', 'real-analysis']"
4851949,The $\int \frac{x^2}{x^2+x-2}$ only seems to exist for parts of its domain. But it must have an integral for its entire domain. What's happening here? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 5 months ago . Improve this question Suppose $$
F(x)=\int \frac{x^2}{x^2+x-2}\,dx.
$$ When I use Desmos to plot this integral it will give me this: The integral $F(x)$ seems to only exist between the two asymptotes that $\frac{x^2}{x^2+x-2}$ has: one at $x=1$ and another at $x=-2$ . Why is this? My textbook and maple gives the answer as $x-\frac{4}{3}\ln(2+x)+\frac{1}{3}\ln(x-1)$ which seems to exist only in in the interval $(1,\infty)$ . This function looks like the purple curve in the following image. This does coincide with the graph Desmos draws as the integral, but only if I make the lower bound of the integral be some value between the rightmost asymptote and $+\infty$ . This is what it looks like when I do that: My questions are as follows... Why does desmos only plot a small interval of the entire domain of the function? Why is my textbook's answer, $x-\frac{4}{3}\ln(2+x)+\frac{1}{3}\ln(x-1)$ , not a function with a domain of interval $(-\infty,+\infty)$ ? Does a function exist whose domain is from $-\infty$ to $+\infty$ that is this integral? If not, why? When integrating rational functions is this a common occurrence? I would appreciate an answer that I could understand as a Calculus 2 student. Thanks! Here it is the link to Desmos graph I mention:","['integration', 'calculus']"
4851964,"Integral inequality : for $f\in C^{4}[0,1]$ with $f(0)=f(1)=f^\prime(0)=f^\prime(1)=0$, show that $\int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192$","I was interested in the problem stated below: Problem 0 . $f\in C^{4}[0,1]$ with $f(0)=f(1)=f^\prime(0)=f^\prime(1)=0$ , show that $\int_0^1|\frac{f^{(4)}(x)}{f(x)}|dx\geq 192$ This problem is assigned as supplementary problems based on the easier one : Problem 1 : $f\in C^{2}[0,1]$ with $f(0)=f(1)=0$ , show that $\int_0^1|\frac{f^{\prime\prime}(x)}{f(x)}|dx\geq 4$ I proved the easier one in this way: Proof of Problem 1: $$\int_{[0,1]}|f''(x)|dx\geq \int_{[\theta_1,\theta_2]}|f''(x)|dx\geq \left|\int_{[\theta_1,\theta_2]}f''(x)dx\right|=|f^\prime(\theta_2)-f^\prime(\theta_1)|$$ As $f$ is continuous on a compact set, we can find $x_0$ such that $f(x_0)>f(x)$ for any $x\in [0,1]$ . By the mean value theorem, assign $\theta_1$ and $\theta_2$ above to be: $$\frac{f(x_0)-f(0)}{x_0-0}=f^\prime(\theta_1);\quad \frac{f(x_0)-f(1)}{x_0-1}=f^\prime(\theta_2)$$ Then we have: $$|f^\prime(\theta_2)-f^\prime(\theta_1)|=\left|\frac{f(x_0)}{x_0(1-x_0)}\right|\geq 4|f(x_0)|$$ In sum, we have: $$\int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x)}\right|\geq \int_{[0,1]}\left|\frac{f^{\prime\prime}(x)}{f(x_0)}\right|\geq 4$$ However for Problem 0 , I tried MVT and Taylor expansions on $0$ and $1$ but find no clues to continue. Any reasonable thoughts are welcome! Thank you!","['mean-value-theorem', 'analysis', 'taylor-expansion', 'numerical-calculus', 'numerical-methods']"
4852024,Convergence of $\int_{-\infty}^{\infty} \frac{\sin^2 x}{x}dx$,"I would like to understand whether this improper integral $$\int_{-\infty}^{\infty} \frac{\sin^2 x}{x}dx$$ converges or diverges. On one hand, if I fix $\alpha>0$ , I note that $$\int_{-\alpha}^{\alpha} \frac{\sin^2 x}{x}dx=0$$ since the integrand is odd, and the integration interval is symmetric with respect to the origin. On the other hand, however, Mathematica, when given the command Integrate[x*(Sin[x]/x)^2, {x, -[Infinity], [Infinity]}], responds that the integral does not converge on $(-\infty, \infty)$ . Does anyone have any suggestions to offer?","['integration', 'calculus', 'trigonometry']"
4852036,Inequalities for sum of $k$ smallest degrees of a graph,"As part of a homework assignment, I am doing a proof for a generalised variant of Karger's algorithm and am stuck at a particular step. I have proven that for a graph $G=(V,E)$ [writing $n=|V|$ ] with a minimum $k$ -cut $C$ , $$|C| \leq \sum\limits_{l=1}^{k-1} d_l$$ where $d_1\leq d_2\leq\ldots$ are the degress of $v_1,v_2,\ldots\in V$ . I need some way of estimating an upper bound on $\frac{|C|}{|E|}$ in terms of $n$ and $k$ alone. For instance in the case of minimum two-cuts (what are normally just called ""minimum cuts"") we can show $$|C| \leq d_1 \leq \frac{1}{n}\sum\limits_{l=1}^n d_l = \frac{2|E|}{n} \Rightarrow \frac{|C|}{|E|} \leq \frac{2}{n}$$ Any help would be appreciated (there is a strong possibility of an "" XY problem ""-situation here but please give it a go anyways).","['graph-theory', 'discrete-mathematics', 'computer-science']"
4852100,Is there a non-measurable set that you can add to a sigma-algebra and nevertheless remain consistent?,"Suppose we have a $\gamma : 2^X \to \mathbb{R}_{\geqslant 0}$ that is monotonic ( $A \subseteq B \implies \gamma(A) \leqslant \gamma(B)$ ), semi- $\sigma$ -additive ( $A \subseteq \bigcup_{i \in \mathbb{N}} A_i \implies \gamma(A) \leqslant \sum_{i \in \mathbb{N}} \gamma(A_i)$ ) and satisfies $\gamma(\varnothing) = 0$ ; in our class we called that pre-measure though it seems to be a slightly unconventional wording. Let's call $A \subseteq X$ $\gamma$ -measurable iff the Carathéodory's criterion holds: $\forall B \subseteq X \quad \gamma(B) = \gamma(B \cap A) + \gamma(B \cap \bar{A})$ , where $\bar{A}$ denotes complement $X \setminus A$ . Denote with $\Sigma$ the totality of all $\gamma$ -measurable $A \subseteq X$ . In our class we proved that $\Sigma$ is a sigma-algebra and $\gamma|_{\Sigma}$ is a $\sigma$ -additive measure on it (looks kind of similar to Carathéodory's extension theorem ). Nothing special at the first glance. However, it is unclear why should $\Sigma$ be maximal such $\sigma$ -algebra that this restriction (or rather extension if $\gamma$ is an outer measure of some $\mu$ defined on a smaller domain) turns out to be possible. It could appear as that a contradiction to the Carathéodory's criterion 'breaks' additivity but that's not necessarily true: maybe the counterexample set $B$ doesn't belong to a $\sigma$ -algebra generated by $\Sigma \cup \{ A \}$ , where $A$ is the non-measurable set in question, thus making no need to check properties of the measure on it. If it's true and $\Sigma$ is indeed not maximal, then there exist unmeasurable sets that paradoxically can me assigned a measure without causing any contradictions. That sounds... odd.","['measure-theory', 'outer-measure']"
