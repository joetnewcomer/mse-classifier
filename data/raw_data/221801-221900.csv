question_id,title,body,tags
4545096,Find a set of vectors with special property,"Given vectors $\alpha_1, \dots , \alpha_{m}$ in an $n$ -dimensional Euclidean space, such
that $(\alpha_i, \alpha_j) \leq 0$ for $i\neq j$ and $(a_i,a_i)\neq 0$ . Find the maximum value of $m$ . The answer is $2n$ . It's easy to know $m\geq 2n$ . Let $e_1,\dots,e_n$ be the orthonormal basis of $V$ . And take $e_{n+i}=-e_i.$ Then $(e_i,e_j)\leq 0,i\neq j.$ But I don't know how to show $m\leq 2n.$","['linear-algebra', 'vector-spaces']"
4545101,Why am I getting $\beta=90^{\circ}$,"Consider the geometry below, where the small circle is touching both semi circles of radius $5$ and the side of the square. Find the radius of the small circle. My try: $M$ and $N$ are centers of semicircles and $G$ is center of small circle indicated below.Let $r$ be the radius of small circle. We have $$\beta=\frac{3\pi}{4}-\alpha$$ So we have $$\sin \beta=\frac{\sin \alpha+\cos \alpha}{\sqrt{2}}$$ Also $$\sin \beta=\frac{r}{5-r}=\frac{\sin \alpha+\cos \alpha}{\sqrt{2}}\tag{1}$$ By cosine law $$\cos \alpha=\frac{(5-r)^2+(5 \sqrt{2})^2-(5+r)^2}{2(5 \sqrt{2})(5-r)}=\frac{5-2 r}{\sqrt{2}(5-r)}\:\Rightarrow \sin \alpha=\sqrt{\frac{2(5-r)^2-(5-2 r)^2}{2(5-r)^2}}=\frac{\sqrt{25-2r^2}}{\sqrt{2}(5-r)}$$ Hence from $(1)$ we get $$\frac{(5-2 r)+\sqrt{-2 r^2+25}}{\sqrt{2}(5-r)}=\frac{r}{5-r}$$ Solving the above equation we get $r=\frac{5}{2}$ , but in that case $\beta=90^{\circ}$ . Where I went wrong?","['trigonometry', 'circles', 'geometry', 'tangent-line']"
4545146,understanding surface element in $n \geq 4$ dimensions in proof of Mean value property,"I took a basic multivariable calculus class as an undergrad where I saw Green's theorem, Stokes theorem, and Divergence theorem without proof. I know how to use and make sense of them in dimension $n \leq 3$ . I have recently started learning PDEs from Evans and take the 'Guass-Green' theorem for granted since I haven't learnt about differential forms/manifolds yet. $$\int_{U}{\frac{\partial u}{\partial x_i}}dx=\int_{\partial U}u\nu^{i}dS\;\;\;\;(i=1,\ldots,n),$$ I am trying to understand the proof of MVP for a harmonic function $u:U \to \mathbb{R}$ , where $U \subseteq \mathbb{R}^n$ and $u \in C^2(\overline{U})$ which says $$u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma $$ for all balls $\overline{B(x,r)} \subseteq U$ where $d\sigma$ is the surface element. The proof begins by fixing $x$ and defining $$ \phi(r):= \def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma$$ and showing $\phi'(r)=0$ to get $$\phi(r)=\phi(0)=u(x)=\def\avint{\mathop{\,\rlap{-}\!\!\int}\nolimits} \avint_{\partial B(x,r)} u(y) d\sigma$$ . My question is what is the expression for $d \sigma$ ? How does it depend on $y$ in the above integrals? I know when using hyperspeherical coordinates, we get $$dy=dy_1dy_2 \ldots dy_n=r^{n-1}\sin^{n-2}(\theta_1)\sin^{n-3}(\theta_2) \ldots \sin^{n-2}(\theta_{n-2})dr d\theta_1d\theta_2 \ldots d\theta_{n-1}=r^{n-1}dr d\sigma$$ How can I write $d \sigma$ explicitly in terms of $y$ and make sure I am using change of variables correctly?","['integration', 'measure-theory', 'harmonic-analysis', 'multivariable-calculus', 'partial-differential-equations']"
4545162,Are balanced measures isotropic?,"Consider a Borel measure $\mu$ on $\mathbb R^n$ ( $n\geq2$ ) of finite second moment, i.e. such that $$
\int_{\mathbb R^n} \|x\|^2 \,\mathrm d\mu(x) < \infty.
$$ It is said balanced in the direction $e_1$ (the first unit-vector) if for all Borel set $A\subset\mathbb R^{n-1}$ $$
\int_{\mathbb R\times A} x_1\,\mathrm d\mu(x)=0.
$$ Intuitively, no matter the section $A$ considered, the infinite beam of section $A$ , axis $e_1$ and weight distribution $\mu$ is balanced. Likewise, $\mu$ is said balanced in the direction $u$ (unit-vector) if $\phi_*\mu$ is balanced in the direction $e_1$ , where $\phi$ is a rotation that maps $u$ to $e_1$ . Finally, $\mu$ is said balanced if it is balanced in all directions, and isotropic if $\phi_*\mu=\mu$ for all linear isometry $\phi$ (including reflections). Obviously if $\mu$ is isotropic, it is balanced. The question is: ""If $\mu$ is balanced, is it necessarily isotropic?"" It would seem so, but I cannot come up with a convincing argument.","['measure-theory', 'geometric-measure-theory', 'rotations']"
4545176,"If $\alpha+\beta+\gamma=\pi$, then does $\sqrt{\sin\alpha}+\sqrt{\sin\beta}+\sqrt{\sin\gamma}$ reach its maximum when $\alpha=\beta=\gamma$? [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question In any given triangle ( $\alpha+\beta+\gamma= \pi$ ), the following inequality holds: $$\sin {\alpha}+\sin{\beta}+ \sin{\gamma} \leq \frac{3 \sqrt{3}}{2}$$ with the maximum value of $\frac{3 \sqrt{3}}{2}$ reached at $\alpha=\beta=\gamma= \frac{\pi}{3}$ . Does it, then, follow from the above that the expression $$\sqrt{\sin {\alpha}}+\sqrt{\sin{\beta}}+ \sqrt{\sin{\gamma}}$$ also reaches its maximum value at $\alpha=\beta=\gamma= \frac{\pi}{3}$ ? Why?","['trigonometry', 'triangles', 'summation']"
4545183,What is the $n^{th}$ derivative of $f(x)=\sin^2x$?,"What is the $n^{th}$ derivative of $f(x)=\sin^2x$ ? I am not allowed to use anything fancy, just the process of mathematical induction and the basic rules for calculating derivatives. (I haven't studied the Leibniz rule for example) So we have $$f'(x)=2\sin x(\sin x)'=2\sin x\cos x=\sin2x\\f''(x)=(\sin2x)'=\cos2x(2x)'=2\cos2x\\f'''(x)=(2\cos 2x)'=2(\cos2x)'=-4\sin 2x\\f^{(4)}(x)=-8\cos2x$$ Is it possible to make an assumption for the n-th derivate (that we will of course prove using induction) that doesn't include looking at different cases (I mean when $n$ is even and odd and etc)? In other words, I want to find a single formula parametrized with $n$ for the n-th derivative. Thanks!","['calculus', 'derivatives']"
4545206,Minimum of a function -approximate value,"Consider the function below: $f(x)=\frac{x}{1-\left(1-a^{\frac{1}{x}}\right)^{\frac{1}{d}}}$ , $x\geq 1$ . We also have that $0<a<<1$ , $d>>1$ . By plotting on Matlab, I can clearly see that this function has an absolute minimum.
However, I am not able to calculate analytically this minimum. I tried via derivation, but the resulting expression has not straightforward zeros. I don't necessarily need the exact value of the minimum, an approximate value would also be fine. Anyone can help or suggest a strategy?","['calculus', 'functions']"
4545269,complex analysis/ integral: $\int_0^\infty \frac{1-\cos x }{x^2}dx$,"I have a question about an example in Stein/Shakarchi. Actually there is another thread here on SE Integrating $\int_0^\infty \frac{1-\cos x }{x^2}dx$ via contour integral. Regarding the integral $\int_0^\infty \frac{1-\cos x }{x^2}dx$ , I understand the indented semicircle contour, the division into 4 integrals, I also understand it until letting $R \rightarrow \infty$ and then applying ML estimation. However afterwards, I can not follow it anymore, where did the integrals of the two horizontal lines go? Do they cancel each other out (if yes, how can I see it?) and why do they write $f(z)$ as $f(z)=\frac{-i z}{z^2} + E(z)$ ? It would be really kind if someone could explain these last steps to me","['complex-analysis', 'contour-integration']"
4545283,Checking my proof that $\lim_{n \to \infty} 1/(2n - 3) = 0$,"My goal is to prove that the sequence $(x_n)$ defined as $x_n := \frac{1}{2n-3}$ converges and the limit is $0$ . In addition to determining if my proof is correct I also wanted to know if there is an alternative way to do the proof, maybe with an inequality that holds for all $n \in \mathbb{N}$ , still just using the definition of the limit of a sequence. My proof: Let $\varepsilon > 0$ . By the Archimedean Property there exists some $K_{*} \in \mathbb{N}$ such that $1/\varepsilon < K_{*}$ , therefore $1/K_{*} < \varepsilon$ . Let $K = \max\{K_{*}, 3\}$ and suppose $n \geq K$ . Then $$\bigg|\frac{1}{2n-3} - 0 \bigg| = \bigg|\frac{1}{2n-3}\bigg| \leq \bigg|\frac{1}{n}\bigg| = \frac{1}{n} \leq \frac{1}{K} < \varepsilon,$$ where the first inequality holds for all $n \geq 3$ .","['limits', 'solution-verification']"
4545287,Almost Diophantine approximation,"We have an algebraic number $a$ and a real number $b$ . Can the following inequality have infinitely many solutions for $n \in \mathbb{N}$ ? $$ \{an\} \in [b - \frac{1}{2^n}, b + \frac{1}{2^n}] $$ Where ${x}$ denotes the fractional part of x, ${x} = x - [x]$ . Background . I encountered this problem when I was dealing with some kronecker approximations, I wanted to show that for a given point $(b_1t \mod 2\pi, b_2t \mod 2\pi)$ where $b_1,b_2$ are algebraic numbers who are linearly independent over rational numbers, one can not get exponentially close to a given fixed point $(\phi_1,\phi_2) \in [-1,1]^2$ . Some special cases of this problem could be solved by Baker but I was unable to solve it in the general case.","['number-theory', 'algebraic-number-theory', 'approximation', 'diophantine-approximation']"
4545334,Measure Theory: How to compute the conditional expectation of max of dice tosses?,"Consider a dice with $f$ faces and let $(X_n)_{1 \le n \le N}$ be the outcomes of the tosses. For $1 \le n \le N$ we set $\mathcal{F}_n = \sigma(X_1,\ldots,X_n)$ and additionally $\mathcal{F}_0 = \{\emptyset, \Omega \}$ . Consider the RVs $Z_0 := 0$ and $Z_n := \max_{1 \le k \le n} X_k \quad (1 \le n \le N)$ . Compute the conditional expectation $\mathbb{E}(Z_n \mid \mathcal{F}_{n-1})$ for $(1 \le n \le N)$ . I understand that intuitively $Z_n$ describes precisely the maximum number yielded by the $n$ th dice toss when we know the results of the previous $n-1$ tosses. I furthermore suppose that we can model this setting as a measure space $(\Omega, \mathcal{A}, \mathbb{P})$ via $\Omega := \{1,\ldots,f\}$ and consider $X_1,\ldots,X_N$ as uniformly distributed independent canonical RVs, i.e. $X_i(\omega) := \omega$ . For the $\sigma$ -Algebra $\mathcal{A}$ I suppose it is best to set $\mathcal{A} = \mathcal{P}(\Omega)$ . However, I do not see how to transfer my ""continuous"" definition (see below) of condtional expectation $\mathbb{E}(X \mid \mathcal{F})$ over to this discrete case. Let $(\Omega, \mathcal{A}, \mathbb{P})$ be a probability space and let $X$ be a real RV with $E(\lvert X \rvert) < \infty$ . $\mathbb{E}(X \mid \mathcal{F})$ is uniquely defined as a RV via the conditions $\mathbb{E}(X \mid \mathcal{F})$ is $\mathcal{F}$ measurable, $\mathbb{E}(X \mid \mathcal{F}) \in L^1(\mathbb{P})$ $\int_A \mathbb{E}(X \mid \mathcal{F}) d \mathbb{P} = \int_A X d \mathbb{P}$ for all $A \in \mathcal{F}$ . Could you please help me?","['conditional-expectation', 'measure-theory', 'probability-theory']"
4545347,Equivalence between $L^{p_0}+ L^{p_1}$ and Orlicz space $L^\Psi$ with suitable $\Psi$,"This is from Exercise 24, Chapter 1, in Stein and Shakarchi's Functional Analysis . First a few definitions. $L^{p_0}+L^{p_1}$ is defined as the vector space of measurable functions $f$ on a measure space $X$ , that can be written as a sum $f=f_0+f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . Define $$\|f\|_{L^{p_0}+L^{p_1}}=\inf\{\|f_0\|_{L^{p_0}}+\|f_1\|_{L^{p_1}}\},$$ where the infimum is taken over all decomposition $f=f_0+f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . Suppose $\phi(t)$ is continuous, convex, increasing function on $[0,\infty)$ , with $\phi(0)=0$ and $\phi(t)$ is not the constant function 0. Define $$L^{\phi}=\{f\hspace{0.2cm} \text{measurable:}\int_{X}\phi(|f(x)|/M)d\mu<\infty, \text{for some}\hspace{0.2cm} M>0\}$$ and $||f||_{\phi}=\inf_{M>0}\int_{X}\phi(|f(x)|/M)d\mu\le1$ . Suppose $1\leq p_0<p_1<\infty$ and define $$\Psi(t) = \int_0^t \psi(u) du \quad \text{where} \; \psi(u) =
\begin{cases}
  u^{p_1-1} & \text{if $u<1$} \\
  u^{p_0-1} & \text{if $1\leq u<\infty$}
\end{cases}$$ Show that $L^\Psi$ with its norm is equivalent to the space $L^{p_0} + L^{p_1}$ . In other words, there exist $A, B > 0$ , so that $$A \| f \|_{L^{p_0} + L^{p_1}} \leq \| f \|_{L^\Psi} \leq B \| f \|_{L^{p_0} + L^{p_1}} .$$ The left half of the inequality is easy to prove, as $\Psi(|f|/M)$ effectively defines a decomposition of $f$ . I cannot figure out how to prove the right half of the inequality.","['functional-analysis', 'analysis', 'orlicz-spaces']"
4545359,How to compute the characteristic function $\mathbb{E}(e^{itXY})$ via the conditional characteristic function $\mathbb{E}(e^{itXY} \mid X)$?,"Let $X, Y$ be independet $\mathcal{N}(0,1)$ RVs. Compute the conditional expectation $\mathbb{E}(e^{itXY} \mid X)$ and use this to compute $\mathbb{E}(e^{itXY})$ . My attempt was the following: Using a well-known lemma from probability theory we have $$\mathbb{E}(e^{itXY} \mid X) = \mathbb{E}(e^{itxY})$$ Now, using the definiton of expected value and the assumption that $Y$ is standard normal, we get $$ = \int_{-\infty}^{\infty} e^{itxy} \frac{1}{\sqrt{2\pi}} e^{-y^2/2} dy$$ , but I do not see how to continue from here. Could you please give me a hint?",['probability-theory']
4545361,"Find the maximum and minimum of $f(x,y) = x^2 -xy +y^2$ subject to constraints $|x|+|y|\leq1$.","Find the maximum and minimum of $f(x,y) = x^2 -xy +y^2$ subject to constraints $|x|+|y|\leq1$ . The constraint of this exercise came out of left field for me, I'm not sure how to handle it. Note, we have not gone through Lagrangian multipliers yet, so an approach without them would be preferable. To get started somewhere, I began by computing the gradient, obtaining the following: $$[2x-y \hspace{3mm} 2y-x]$$ Thus we can see that for a critical point, we require that $2x=y$ and $2y=x$ , which leads to $x=y=0$ and $(0,0)$ is a critical point contained inside our restriction. But then I'm not sure how to proceed. Apologies for the lack of an attempt, but I'm simply not sure how to go about this in a systematic way, so any input would be much apriciated!","['multivariable-calculus', 'calculus']"
4545364,"Possible ""clever"" ways to solve $x^4+x^3-2x+1=0$, with methodological justification","Solve the quartic polynomial : $$x^4+x^3-2x+1=0$$ where $x\in\Bbb C$ . Algebraic, trigonometric and all possible methods are allowed. I am aware that, there exist a general quartic formula. (Ferrari's formula). But, the author says, this equation doesn't require general formula. We need some substitutions here. I realized there is no any rational root, by the rational root theorem. The harder part is, WolframAlpha says the factorisation over $\Bbb Q$ is impossible. Another solution method can be considered as the quasi-symmetric equations approach. (divide by $x^2$ ). $$x^2+\frac 1{x^2}+x-\frac 2x=0$$ But the substitution $z=x+\frac 1x$ doesn't make any sense. I want to ask the question here to find possible smarter ways to solve the quartic.","['contest-math', 'irreducible-polynomials', 'substitution', 'algebra-precalculus', 'quartics']"
4545377,Bringing limits inside functions when limits go to infinity,"A standard result says that under suitable conditions to make sure the functions are defined where they need to be, we can write $$\lim_{x \to c} f(g(x)) = f \left( \lim_{x \to c} g(x) \right)$$ as long as $f$ is continuous at $\lim_{x \to c} g(x)$ . But if the inside function is going to infinity, the continuity condition makes no sense, yet it is still common to say something like $$\lim_{x \to c} f(g(x)) = \lim_{t \to \infty} f(t) $$ This is more the ""substitution"" method of evaluating the limit, which I also see used in the standard case instead of formally bringing limits inside of functions. But I've never liked it, because it never felt like substituting was using a limit theorem which I knew to be true in all situations. My question: what is the most general version of the theorem here? In the second formula, does something need to be continuous in some sense?",['calculus']
4545435,"If there is a bijection between $\mathbb R$ and $\mathbb R^3$, why is it said that it takes 3 numbers to determine a point in three-dimensional space?","Looking up definitions of the mathematical concept of three-dimensional space, the notion that it takes three independent real numbers to specify a point in it seems to be the defining property of the ""three-dimensional"" in ""three-dimensional space"". I was very surprised to learn however that there is a bijection between $\mathbb R$ and $\mathbb R^n$ for any finite $n$ , such as $3$ . In other words, it is possible to specify any point in three-dimensional space with a single real number, as could be done for any other finite-dimensional space. This seems like a huge disconnect between how we perceive mathematics and the physical world to be connected, and the definition of three-dimensional space itself. What gives? What is then the fundamental property that makes three-dimensional space different from, say, two-dimensional space?","['geometry', '3d', 'real-analysis']"
4545473,Strengthening Dilworth's theorem in a certain poset,"I am currently working on my discrete math problem. Let's define a consecutive chain $C = (a_1, a_2, \cdots)$ of poset $(P, \leq)$ as a chain with the property that $\not\exists z (a_i \leq z \leq a_{i+1})$ , where $z \neq a_i, a_{i+1}$ . The poset I am considering is $(\mathcal{F}, \subseteq)$ where $\mathcal{F} \subseteq 2^{[n]}$ with the following property. If $A, C \in \mathcal{F}$ and $A \subseteq B \subseteq C$ , then $B \in \mathcal{F}$ . If I can prove the following version of Dilworth's theorem on $(\mathcal{F},\subseteq)$ , I will be done. Let $r$ be the length of the largest antichain of $\mathcal{F}.$ Then $\mathcal{F}$ can be partitioned into $r$ consecutive chains. I have read the original proof of Dilworth's theorem by Galvin. It was by the induction on the size of the poset. However I have no idea how to modify it the prove my claim. Is that generalized version of Dilworth' theorem false? Even when restricted to the case that the poset is $(\mathcal{F}, \subseteq)$ ? Thanks in advance for any form of help, hint, or solution.","['combinatorics', 'extremal-combinatorics', 'discrete-mathematics']"
4545474,"Integration of vector field $(y,-y,1)$ over paraboloid $z=1-x^2-y^2$","The question asks to integrate the vector field $$
F(x,y,z)={1\over\sqrt{x^2+y^2}}(y,-y,1)
$$ over the paraboloid $$z=1-x^2-y^2.$$ over the area defined by $0\leq z\leq1$ which is $x^2+y^2\leq1$ .
Using $X=(x,y,1-x^2-y^2)$ as the surface I obtained $$
\begin{align}
{\partial X\over\partial x}&=(1,0,-2x)\\
{\partial X\over\partial y}&=(0,1,-2y)\\
N={\partial X\over\partial x}\times
{\partial X\over\partial y}&=(2x,2y,1)\\
\end{align}
$$ and the integral of $F$ is then $$
\iint_R F(x,y,z)\cdot N dx\;dy
=
\int_0^{2\pi}\int_0^1
{2xy-2y^2+1\over r} r\;dr\;d\theta\\
=\frac13\int_0^{2\pi}(2\sin\theta\cos\theta-2\sin^2\theta+1) \;d\theta\\
=\frac13\int_0^{2\pi}(\sin2\theta+\cos2\theta)\;d\theta=0.\\
$$ However, the book gives the answer $4\pi/3$ . I've been over the calculations repeatedly without avail. 🥴 Can anyone point out my error? This is exercise 12 of XII §3 of ""Calculus of Several Variables"" by Serge Lang, third edition, on page 341.",['multivariable-calculus']
4545495,The continuity of this two variable function. Is this continuous?,"I have come across with this problem in studying partial differential equation. Let $f$ be a continuous and periodic function on $\mathbb R$ with period $2\pi$ , and denote $D=\{(x,y)\mid x^2+y^2<1\}.$ And let $u:D\to \mathbb R$ be continuous and $C^2$ class on $D$ , and satisfy $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0,$$ where I use the polar coodinates $x=r\cos a, y=r\sin a$ for $u(x,y)$ . Then, if I define $v: \overline D \to \mathbb R$ by $$v(x,y)=v(r\cos a, r\sin a):=\begin{cases}u(r\cos a, r\sin a) & \mathrm{if} \ (x,y)\in D\\ f(a) &\mathrm{if} \ (x,y)\in \overline D \setminus D=\partial D\end{cases}$$ , is $v$ continuous on $\overline D$ ? On $D$ , $v$ is continuous from the supposition of continuity of $u$ , so the problem is the continuity at $\partial D.$ I expect this holds because $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0$$ means that if $a\fallingdotseq b$ and $r\fallingdotseq 1$ , then $u(r\cos a, r\sin a)\fallingdotseq f(b)\fallingdotseq f(a)$ . ( $f(b)\fallingdotseq f(a)$ follows from the continuity of $f$ .) So I tried to prove but it doesn't seem to work. Let $(c,d)\in \overline D$ . If $(c,d)\in D$ , $v$ is continuous at $(c,d)$ due to the continuity of $u$ . So I'll consider the case $(c,d)\in \partial D.$ Let $\epsilon>0.$ $(c,d)$ is on the unit circle so I can write $$c=\cos \xi, d=\sin \xi.$$ From $$\lim_{\delta \to 0}\underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)|=0,$$ there is $\delta_1$ s.t. $$|\delta|\leqq \delta_1\Rightarrow \underset{1-\delta\leqq r<1}{\sup_{|a-b|\leqq \delta}}|u(r\cos a,r\sin a)-f(b)| <\epsilon.$$ And from the continuity of $f$ , there is $\delta_2>0$ s.t. $$|\eta-\xi|<\delta_2 \Rightarrow |f(\eta)-f(\xi)|<\epsilon.$$ Let $\delta_3:=\min\{\delta_1, \delta_2\}$ , and let $(x,y)\in \overline D$ satisfy $|(x,y)-(c,d)|<\delta_3.$ Denote $x=r\cos a, y=r\sin a.$ Then, if I could show $|v(x,y)-v(c,d)|<\epsilon$ , the proof will finish. If $(x,y)\in \partial D$ , then $|v(x,y)-v(c,d)|=|f(a)-f(\xi)|$ , so $|a-\xi|<\delta_3$ is desired, but this doesn't seem to work because $|a-\xi|<|(x,y)-(c,d)|$ doesn't hold. (Actually, the opposite inequality holds.) If $(x,y)\in D$ , I want to do \begin{align}
|v(x,y)-v(c,d)|&=|u(x,y)-f(\xi)|\\
&\leqq\underset{1-\delta_3\leqq r<1}{\sup_{|a-b|\leqq \delta_3}}|u(r\cos a,r\sin a)-f(b)|
\end{align} ,
so I have to check $|a-\xi|\leqq\delta_3$ and $1-\delta_3\leqq r<1$ I get $1-\delta_3\leqq r<1$ since $r<1$ follows from $(x,y)\in D$ and $1-\delta_3\leqq 1-|(x,y)-(c,d)|\leqq r.$ Thus, in both cases $(x,y)\in \partial D$ and $(x,y)\in D$ , $|a-\xi|<\delta_3$ is desired. I'd like you to share the idea for showing $|a-\xi|<\delta_3$ . Another proof for the continuity of $v$ is also welcomed.","['continuity', 'multivariable-calculus', 'real-analysis']"
4545522,"Is $x_{j} \bmod v$ dense in $[0,v]$?","Suppose that $(x_{j})_{j \in \mathbb{N}}$ is an unbounded sequence inside $\mathbb{R}$ . Does there exist a $v>0$ so that $x_{j} \bmod v$ is dense in $[0,v]$ ? Note here that $a \bmod b := a- \lfloor \frac{a}{b}\rfloor b$ . $\textbf{Example}$ : For the sequence $(j)_{j \in \mathbb{N}}$ , $j \bmod \sqrt{2}$ is dense in $[0,\sqrt{2}]$ . I added the unbounded assertion as the sequence $((\frac{1}{2})^{j})_{j \in \mathbb{N}}$ is not dense modulo $v$ for any $v > 0$","['real-numbers', 'sequences-and-series', 'real-analysis']"
4545545,Is it possible to visualize the theorems and proofs in functional analysis?,"I am an MSc.second year student and just started studying functional analysis.I understand the proofs of the theorems in this topic but since I cannot visualize those things,so I forget the proofs easily as I do not get what is going on actually and just understand the proof step by step.So,my question is: Is it possible to visualize the theorems and proofs in functional analysis? First difficulty I face is due to not knowing the standard normed spaces very well.For example,I do not know how $L_p[0,1],1\leq p<\infty$ and $L_\infty[0,1]$ spaces behave unlike I know for $\mathbb R^n$ and $\mathbb C^n$ .These spaces seem to much abstract to work with.Also finding counterexamples in the spaces $\ell_p,\ell_\infty$ seem to be difficult.Compared to that working with $(C[0,1],\|.\|_\infty)$ is quite easy. Also it is difficult to work with infinite dimensional normed spaces because we are used to finite dimensional normed spaces and try to visualize the subspaces by lines or planes $($ as we have in $\mathbb R^3)$ ,but things are not so simple in infinite dimensional cases.So,I fail to create a mental picture of a normed subspace of a normed space.This troubles me a lot. Then comes theorems like uniform boundedness principle and open mapping theorem.As constructing examples in functional analysis is not so easy,I face problems in finding intuition behind the proofs we are taught in class. So,I am struggling a lot in functional analysis.Can someone tell me a way out?I mean can someone tell me a proper way to study functional analysis and visualize these things?","['visualization', 'motivation', 'functional-analysis', 'intuition', 'soft-question']"
4545562,Probability of Adjacent Birthdays,"Recall the birthday problem, where only 23 people are required for a >50% chance that at least two share the same birthday. What is the new probability if we want at least two people out of twenty-three to share the same birthday or have adjacent birthdays? January 1 and December 31 are considered adjacent, and we don't consider February 29. Similar to the birthday problem, I thought it would be easier to consider the probability that nobody has matching or adjacent birthdays, but as I was setting up the products, I realized that there are different amounts of available dates, depending on when the birthdays are. For ease of writing, let the dates be represented as 1, 2, 3, 4, ..., 365. Say the first person's birthday is 1. If the second person's birthday is 3 or 364, then there are 360 remaining choices for the third birthday, but if the second person's birthday is not in (364, 365, 1, 2, 3), then there are 359 remaining choices for the third person's birthday. Considering all these possibilities for 23 people is tedious, however. I also tried approaching this from a stars and bars perspective. Arrange 23 bars in a circle, put one star automatically between each bar, leaving 339 possible dates to be inserted. However, this also is difficult because there's an ordering. If three bars are Mar 1, Mar 14, and Mar 20, you can't stick Aug 5 in there. So, I was at a loss to figure it out intuitively. On the other hand, I generated a pattern by starting small (5 days and 2 people, for example), and finding a function out of OEIS. The answer to the problem with 365 days and 23 people is $$1 - \frac{23!}{365^{23}} \left( \binom{343}{23} - \binom{341}{21} \right) \approx 0.8879096$$ Let $r(n,k)$ be the rising factorial, $n(n+1)(n+2)...(n+k-1)$ . The answer can also be written as $$1 - \frac{23!}{365^{23}} \left( r(321, 23) - r(321, 21) \right) \approx 0.8879096$$ The $321$ comes from $365 - 2(23-1)$ , which is due to the pattern I found. My question is, what's the intuitive way to approach this problem, or how do I actually derive the formula to get my answer?","['combinatorics', 'birthday', 'probability']"
4545604,To prove: $e^{\csc^2\theta}\sin^2\theta+e^{\sec^2\theta}\cos^2\theta\gt e^2$,"Question: Statement I: If $0\lt\theta\lt\frac\pi2$ , then $e^{\csc^2\theta}\sin^2\theta+e^{\sec^2\theta}\cos^2\theta\gt e^2$ Statement II: AM of positive numbers is always greater than or equal to their GM. A) Both the statements are true and Statement II is correct explanation of Statement I. B) Both the Statements are true and Statement II is not the correct explanation of Statement I. C) Statement I is true and Statement II is false. D) Statement I is false and Statement II is true. My Attempt: I can see that both the statements are true. I wonder how to prove Statement I. Answer given is option A. $\frac{e^{\csc^2\theta}\sin^2\theta+e^{\sec^2\theta}\cos^2\theta}2\ge\sqrt{e^{\csc^2\theta}\sin^2\theta e^{\sec^2\theta}\cos^2\theta}$ Thus, $$RHS=\sqrt{e^{\frac1{\sin^2\theta}+\frac1{\cos^2\theta}}\sin^2\theta\cos^2\theta}\\ =e^{\frac1{\sin\theta\cos\theta}}\sin\theta\cos\theta$$ Not able to conclude.","['trigonometry', 'exponential-function', 'means', 'inequality']"
4545618,"How ""much"" non-injective is a continuous map from the Möbius strip to the real plane?","We all know that there exists no injective continuous map from the Möbius strip $M$ to the real plane $\mathbb{R}^2$ . However, I am interested in quantifying this non-injectivity when we let $M$ be given as a subset of $\mathbb{R}^{3}$ , more precisely, $$M=\{\varphi(v,u)\}_{v\in[-1,1],\;u\in[0,2\pi)}$$ where $\varphi(u,v):=\left[\left(1+\frac{v}{2}\cos\frac{u}{2}\right)\cos u,\;
\left(1+\frac{v}{2}\cos\frac{u}{2}\right)\sin u,\;\frac{v}{2}\sin\frac{u}{2}\right]$ . In particular, let $F:M\to\mathbb{R}^{2}$ be continuous. Is it true that there exists some constant $c>0$ , independent of $F$ , such that $$F(x)=F(y)\;\;\text{for some}\;\;x,y\in M\;\;\text{with}\;\;|x-y|>c$$ ? Put it another way, I am asking whether the infimum below is positive or null, $$\delta(M,\mathbb{R}^{2}):=\inf_{F\in\mathcal{C}(M,\mathbb{R}^{2})}\;\;\sup_{x\in M,\;y\in F^{-1}(x)}\;|x-y|.$$ For instance, for the unit sphere $S^{2}\subset\mathbb{R}^{3}$ , by the Borsuk-Ulam theorem we know that $\delta(S^{2},\mathbb{R}^{2})=2$ , since every continuous map $S^{2}\to\mathbb{R}^{2}$ fails to be injective at two antipodal points. My guess is that $\delta(M,\mathbb{R}^{2})$ is also positive, but I am not able to prove. My intuition tells me that for any $F\in\mathcal{C}(M,\mathbb{R}^{2})$ there exists a point $x$ on the boundary $\partial M$ and a point $y$ on the middle line $\ell:=\{\varphi(0, u)\}_{u\in[0,2\pi)}$ such that $F(x)=F(y)$ . The idea comes from the fact that, regardless of $F$ , both $F(\partial M)$ and $F(\ell)$ are closed loops in the plane, but I couldn't proceed any further from here... If you have any ideas please let me know!","['general-topology', 'analysis']"
4545644,Proof of The Chain Rule,"I’m self-studying Stewart’s calculus and I went back to review the proof of the chain rule, but I keep getting confused about a particular line. The proof starts as follows: $\Delta y = f(x+\Delta x) - f(x)$ : this part is clear to me. $\displaystyle{\lim_{\Delta x \to 0}} [\Delta y/\Delta x] = f’(a)$ : this part is clear to me. let $\varepsilon$ be a number such that $\varepsilon = (\Delta y/\Delta x) - f’(a)$ : not exactly sure why they decided to go in this direction. How does one go about logically arriving at this step? $\Delta y = \varepsilon \Delta x + \varepsilon f’(a)$ : this is clear to me. If we define $\varepsilon$ to be 0 when $\Delta x = 0$ then $\varepsilon$ becomes a continuous function of $\Delta x$ : I’m also confused here. How can we define $\varepsilon$ to be 0 when $\Delta x = 0$ ? Not sure what that means. Further, if $\Delta x = 0$ then wouldn’t $\varepsilon = 0$ (no change in the difference quotient) $- f’(a) = -f’(a)$ ? #3 and #5 are preventing me from proceeding. Can anyone help out?","['proof-explanation', 'calculus', 'derivatives', 'chain-rule']"
4545687,can a bounded ratio of two polynomials have unbounded derivative?,"Let $P(x_1,\cdots,x_m)$ and $Q(x_1,\cdots,x_m)$ be two polynomials and assume that $$f(x_1,\cdots,x_m)=\frac{Q(x_1,\cdots,x_m)}{P(x_1,\cdots,x_m)}$$ is bounded over some region $D\subseteq \mathbb{R}^m$ .
Can $f$ have an unbounded derivative over $D$ ? By unbounded I mean that $|\nabla f|\rightarrow \infty$ somewhere in $D$ .","['derivatives', 'polynomials']"
4545693,"How many subsets of the set {1, 2, ... , 9} contains exactly two odd numbers?","I have encountered this problem in one of my books on discrete mathematics: How many subsets of the set {1, 2, 3, 4, 5, 6, 7, 8, 9} contains exactly two odd numbers? There is no restriction on how many even numbers the subset can contain. I quickly realized that the number subsets of the set is so large that is stupid to solve it by just writing and counting it. I solved it using Python instead, like you can see in the attached image file. But now I want to know how you can solve it with just math? There must be a proper way to solve problems like these. If I programmed it correctly then the correct answer is 160. Please help, thanks.",['combinatorics']
4545696,Find the variance for the number of runs,"A biased coin is tossed $n$ times and heads shows up with probability $p$ on each toss.  Let us call  a sequence of throws which result in the same outcomes a run , so that for example, the sequence HHTHTTH contains five runs. If $R$ is a r.v. representing the number of runs then $\mathbb{E}(R) = 1+(n-1)2pq$ . I want to work out the variance $var(R)$ . To do this I would like to use that $var(R) = var(R-1) = \mathbb{E}(R-1)^2 - (\mathbb{E}(R-1))^2$ . Let $I_j$ be the indicator function of the event that the outcome of the $(j+1)$ th toss is different from the outcome of the $j$ th toss. $I_j$ and $I_k$ are independent if $|j-k| > 1$ , so that \begin{equation*}
\begin{aligned}
\mathbb{E}(R-1)^2 ={} & \mathbb{E}\left\{\left(\sum_{j=1}^{n-1}I_j\right)^2\right\} \\
= {} &\mathbb{E} \left(\sum_{j=1}^{n-1} I_{j}^{2}+2 \sum_{j=1}^{n-2} I_{j} I_{j+1}+2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}\right).
\end{aligned}
\end{equation*} Now $\mathbb{E}(\sum_{j=1}^{n-1} I_{j}^{2}) = (n-1)2pq$ and $\mathbb{E}(2 \sum_{j=1}^{n-2} I_{j} I_{j+1}) = (n-2)2pq$ . We also have that $\mathbb{E}(2 \sum_{j=1}^{n-3} \sum_{k=j+2}^{n-1} I_{j} I_{k}) = (n-3)(n-4)(2pq)^2$ I believe. But now I have lost confidence and I am not sure how to get the final result for the variance. Is my approach correct and what should it be in the end?",['probability']
4545745,"Maximal inequality of iid random variables $\{X_{ij}\}_{1\leqslant i,j \leqslant n}$","Suppose that $\{X_{ij}\}_{1\leqslant i,j\leqslant n}$ are iid random variables with $\mathbb{E}(X_{11})=0$ and $\mathrm{Var}(X_{11})=1$ , does the following convergence hold: $$
\max_{1\leqslant  j\leqslant n}\biggl\{\frac{1}{n^2}\sum_{1\leqslant i\neq i'\leqslant n}(X_{ij}X_{i'j})\biggr\} \to 0 \qquad \text{almost surely}?
$$ Comment: I have also posted this question on mathoverflow according to @D.R.'s suggestion. Background I am reading the AoP paper ""Limit of the smallest eigenvalue of a large dimensional sample covariance matrix"" by Z. Bai and Y. Yin (1993). Their Lemma 2 states a generalization of the well-known Marcinkiewicz-Zygmund strong law of large numbers to the case of multiple arrays of iid random variables. [Lemma 2 in Bai and Yin (1993) ] Let $\{\xi_{ij},i,j=1,2,\ldots\}$ be a double array of iid random variables and let $\alpha>1/2,\beta\geqslant 0$ and $M>0$ be constants. Then as $n\to\infty$ , $$
\max_{j\leqslant Mn^{\beta}} \biggl|n^{-\alpha}\sum_{i=1}^n (\xi_{ij}-c)\biggr|\to0\quad \text{almost surely},
$$ if and only if $$
(i)\quad \mathbb{E}|\xi_{11}|^{(1+\beta)/\alpha}<\infty
$$ $$
(ii)\quad c =  \left\{
\begin{array}{ll}
      \mathbb{E} \,\xi_{11},& \text{if }\alpha\leqslant 1, \\
      \text{any number}, &\text{if }\alpha>1.
\end{array} 
\right.  
$$ By our assumptions and taking $\alpha=\beta=M=1$ , $\xi_i=X_{ij}^2$ in this lemma, we have $$
\max_{j\leqslant n}\biggl|\frac{1}{n}\sum_{i,j}X_{ij}^2-1\biggr|\to0\quad \text{almost surely}.
$$ This result is for square terms. I wonder if there is a similar result for the cross terms $$
\max_{1\leqslant  j\leqslant n}\biggl\{\frac{1}{n^2}\sum_{i\neq i'}(X_{ij}X_{i'j})\biggr\} \to 0 \qquad \text{almost surely}?
$$ Attempt I can prove that $(1/n^2)\sum_{i\neq i'}(X_{ij}X_{i'j})\to 0\; a.s.$ for any fixed $j$ . But I do not know how to deal with the problem with "" $\max$ "". For fixed $j$ , $$
\mathrm{Var}\Bigl(\sum_{i\neq i'}X_{ij}X_{i'j}\Bigr)=2\sum_{i\neq i'}\mathrm{E}\bigl(X_{ij}^2\bigr)\cdot\mathrm{E}\bigl(X_{i'j}^2\bigr)=2(n^2-n),
$$ then by Chebyshev's inequality, for any $\varepsilon>0$ , $$
\Pr\biggl(\frac{1}{n^2}\sum_{i\neq i'}(X_{ij}X_{i'j})>\varepsilon\biggr) =O\Bigl(\frac{1}{n^2}\Bigr),
$$ which is summable. Hence, by using the Borel-Cantelli lemma, we have $$
\frac{1}{n^2}\sum_{i\neq i'}(X_{ij}X_{i'j})\to 0\qquad
 \text{almost surely}.$$ If we consider $\max_{1\leqslant j\leqslant n}$ , and use the trivial inequality to bound it, we have $$
\Pr\biggl(\max_{1\leqslant  j\leqslant n}\biggl\{\frac{1}{n^2}\sum_{i\neq i'}(X_{ij}X_{i'j})\biggr\}> \varepsilon\biggr)
\leqslant n\cdot \Pr\biggl(\frac{1}{n^2}\sum_{i\neq i'}(X_{i1}X_{i'1})>\varepsilon\biggr)=O\Bigl(\frac{1}{n}\Bigr),$$ which means $$
\max_{1\leqslant j\leqslant n}\biggl\{\frac{1}{n^2}\sum_{i\neq i'}(X_{ij}X_{i'j})\biggr\}\to 0\qquad
 \text{in probability}.\tag{*}
$$ How can we improve the result (*) to ""almost surely""?","['law-of-large-numbers', 'analysis', 'inequality', 'probability-theory', 'probability']"
4545772,"If $\lim_{t\to +\infty} \int_{0}^{\pi} f(x)e^{xt} \, dx=0$ then $f=0$?","Question, Let $f \in L^2(0, \pi)$ such that $\lim_{t\to +\infty} \int_{0}^{\pi} f(x)e^{xt} \, dx=0$ . Is it necessary that $f$ is identically zero almost everywhere? If true, how to use the fact $\lim_{t\to +\infty} e^{xt} = +\infty, \hspace{0.5em} \forall x>0$ ? If $f$ is positive, we can use that $\forall x>0, \hspace{0.5em} e^{tx}\geq t$ for large $t$ and we deduce $$ \int_{0}^{\pi} f(x)e^{xt} \, dx
\geq t\int_{0}^{\pi} f(x) \, dx
\to +\infty$$ if $\int_{0}^{\pi} f(x) \, dx  \neq 0$ . But we know $\int_{0}^{\pi} f(x) \, dx = 0 \iff f = 0 \hspace{0.5em} \text{a.e.} $","['integration', 'functional-analysis']"
4545816,Find angle $\alpha$ in $\triangle ABC$,"As title suggests, the goal is to find $\alpha$ from the given $\triangle ABC$ with some given angles and sides. I'm posting this here to look for better solutions, as my own, which I'll post as well, is rather complicated.","['triangles', 'euclidean-geometry', 'trigonometry', 'geometry']"
4545936,How many subsets of set contains at least one even number,"I have this set: {1, 2, 3, 4, 5, 6, 7, 8, 9} And I want to know how many subsets of it contains at least one even number (There is no restriction on odd numbers). I have tried to solve it but I'm not sure if it's correct so I would appreciate some feedback and especially if the answer is correct. First I thought that the subset with only even numbers is {2, 4, 6, 8} with a total of 4 elements. This can then be formed in $4! = 4 \times 3 \times 2 \times 1 = 24$ ways. We also have the subset of odd numbers {1, 3, 5, 7, 9} which means that there are $2^5=32$ (because $2^n$ ) ways of forming subsets of that. If we then multiply these two number $24 \times 32 = 768$ and that is our answer. Is this correct? If not can you please explain what im doing wrong.",['combinatorics']
4546005,Second derivative and stationary points equalling zero,"If $f'(x_{s})=0$ (and $x_{s}$ is therefore a stationary point) is it always true that : $$f''(x_{s})\ne0\,\, \forall x_{s}$$ Is this true for every $f(x)$ and if so how could I go about proving it?","['calculus', 'derivatives']"
4546027,If I roll 2 dice and then add those 2 numbers together. What is the probability that the sum is $\le 5$ or that the sum is $\ge 11$,I want to solve this dice problem but I'm not sure if my solution is the most efficient way. If I roll one dice and then one more and then add those 2 numbers together. What is the probability that the sum is $\le 5$ or that the sum is $\ge 11$ ? I basically wrote out all possible occasions and checked how many of the possibilities were less than or equal to 5 or greater than or equal to 11. 10 possibiliteis has a sum of 5 or less and 3 had a sum of 11 or more. And this is out of $6 \times 6 = 36$ total possibilities. So $\frac {10}{36} + \frac {3}{36} = \frac {13}{36}$ Is there a better way of solving this?,['probability']
4546040,Tensorfields and Multilinear maps correspondence,"So we defined a (r,s) tensor field $w$ on a smooth manifold $M$ as a choice of (r,s) tensors for every point $p \in M$ living over the respected tangent spaces.
So for every $p \in T_pM$ , $w(p)$ is an Element of $\bigotimes^r T_pM \otimes \bigotimes^s T_pM^*$ .
We also mentioned in our lecture, that these tensor fields correspond uniquely to $C^\infty(M)$ mulitlinear maps: $TM^* \times \dots TM^* \times TM \times \dots \times TM \to C^\infty(M)$ . We did not proove this theorem, but we kept defining tensor fields via their induced multilinear maps. Now I want to understand, how to define this correspondence.
My idea was, that these multilinear maps themselves are elements of the tensor space $\bigotimes^r TM \otimes \bigotimes^s TM^*$ ( $C^\infty(M)$ Tensor Module), by the usual identification of tensors and multilinear maps on the dual spaces. Now let $w$ be such a $C^\infty(M)$ multilinear maps as described above, then it can be written in the form: $w=\sum h_1 \otimes \dots \otimes h_r \otimes g_1 \otimes \dots \otimes g_s$ where the tensor product is meant as a module tensor product. The $h_i, g_j$ are vector and covector fields ( $(1,0)$ and $(0,1)$ tensor fields), to get a tensor field in the definition I could define: $w(p)= \sum h_1(p) \otimes \dots \otimes h_r(p) \otimes g_1(p) \otimes \dots \otimes g_s(p)$ where $h_1(p)$ is an abuse of notation, but uses implicitly the isomorphism from vector fields to $(1,0)$ tensor fields given by the canonical pairing. I have meanwhile found a proof, which uses the fact that the multilinear map only depends on Values of the vector fields at $p$ . I did not use that, so I am now wondering why my construction does not work, can you tell me in which step things go wrong?",['differential-geometry']
4546089,joint density problem from P1 probability actuary book - solving density function of the subtraction of random variables,"Let X and Y be two random variables with joint density function $$f_{XY} $$ .
Compute the pdf of $$ U = Y − X $$ I've looked at this problem multiple times and I keep getting a different answer than the book. For my joint density function I'm getting the following: $$f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{X,Y}(x, a+x) dx = \int_{-\infty}^{\infty} -f_{X,Y}(y-a, y) dy $$ If X and Y are independent I'm getting the following: $$ f_{Y −X}(a)= \int_{-\infty}^{\infty} f_{Y}(a+x) \cdot f_X(x) dx =  \int_{-\infty}^{\infty} -f_{X}(y-a) \cdot f_Y(y) dy $$ The book gives this answer: $$ f_{Y −X}(a) = \int_{-\infty}^{\infty} f_{X,Y} (y − a, y)dy $$ Moreover, the book says If X and Y are independent then $$  f_{Y −X}(a) = \int_{-\infty}^{\infty} f_X(y − a)f_Y (y)dy = \int_{-\infty}^{\infty}f_X(y)f_Y (a + y)dy $$ would anyone kindly explain how the book got this answer and am I wrong? I was simply following the same approach as if we were to calculate $$ U = X+Y $$ but it seems something is really off with my answer...","['multivariable-calculus', 'probability-distributions', 'probability-theory']"
4546102,"Analyze the property of $\frac{\log\left(1+p - \frac{1}{1+(1-p)x}\right)}{\log(x)},\;\;x>1,\;\;0<p<1$","Consider the function $$f(x) = \dfrac{\log\left(1+p - \frac{1}{1+(1-p)x}\right)}{\log(x)},\;\;\;x>1,\;\;0<p<1.$$ I guess the function increases firstly and then decreases. But I don't know how to prove this property. I tried computing the derivative but it's too messy to analyse. I am wondering if there is any simple method to analyse it. It is easy to see there is an upper bound on the function $f(x)$ , for example, $f(x) \leqslant \dfrac{\log(2)}{\log(x)}$ and I want to find a tighter bound on $f(x)$ . Firstly, we can show that $f(x) \leqslant \dfrac{\log(2x-2\sqrt{x}+1)}{\log(x)}-1,\quad \forall p \in (0, 1)$ . Define $\;g(x) = \dfrac{\log(2x-2\sqrt{x}+1)}{\log(x)},\;\;\forall x >1.\;$ Then we just need to bound $g(x)$ .","['monotone-functions', 'real-analysis', 'calculus', 'functions', 'derivatives']"
4546113,Bochner integral: Is $f=g$ $\mu$-a.e. if their integrals are equal on every measurable set?,"Let $(X, \mathcal F, \mu)$ be a $\sigma$ -finite complete measure space, and $(E, |\cdot|_E)$ a Banach space. Assume $f,g:X \to E$ are $\mu$ -integrable such that $$
\int_A f \mathrm d \mu = \int_A g \mathrm d \mu \quad \forall A \in \mathcal F.
$$ Here we use the Bochner integral . If $E = \mathbb R$ , then it's well-known that $f=g$ $\mu$ -a.e. The proof in this case uses the natural order on $\mathbb R$ . I would like to ask of if $f=g$ $\mu$ -a.e. in case $E$ is a general Banach space. Thank you so much!","['integration', 'banach-spaces', 'measure-theory', 'functional-analysis', 'bochner-spaces']"
4546125,Cauchy-Schwarz inequality for positive semidefinite matrices,"I have a vector space $V$ of finite dimension $n$ over the field $\mathbb R$ . I define the following product $$u\otimes v=\sum_{i=0}^\infty b_iu_iv_i^T$$ where $b_i\in\mathbb R^+$ and all $u_i$ and $v_i$ are elements of the vector space $V=\mathbb R^n$ . In the context of this formula, we use $u$ (and $v$ ) to mean the infinite list of $u_i$ (and $v_i$ ). This product defines a matrix in $\mathbb R^{n\times n}$ , given a choice of basis in $V$ . In particular, $u\otimes u$ will be (symmetrical) positive semidefinite. We can assume that $\sum b_i$ converges and that all $u_i$ and $v_i$ are bounded (the $L^2$ norm of each vector is bounded). That ensures that $u\otimes v$ converges. If we choose $n=1$ , the product $u\otimes v$ simplifies to a scalar product and we can easily show using Cauchy-Schwarz that: $$(u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u)\ge0$$ How would I prove a similar inequality in general, for any finite $n$ ? As $b_i>0$ for all $i$ , we have both $u\otimes u$ and $v\otimes v$ (symmetrical) positive semidefinite matrices so the inequality should be understood as meaning that $(u\otimes u)-(u\otimes v)(v\otimes v)^{-1}(v\otimes u)$ is also (symmetrical) positive semidefinite.","['matrices', 'cauchy-schwarz-inequality', 'linear-algebra']"
4546136,Subtracting a number (expected value) from a function (random value),"While self studying probability, I came across this formula for variance of random variable: $$\operatorname{Var}(X)=E\left[(X-E(X))^2\right]$$ However, what I understood from earlier definitions is that: $X$ is random variable which is a measurable function $E(X)$ is the expected value or mean which computes to a single value (or number) In the above formula, it seems that we are subtracting this single value (or number) $E(X)$ from a function $X$ (in the part $(X-E(X)$ ). From the examples following the definition in the book, I could understand that while applying the formula, we subtract the same expected value from each possible value $x$ in codomain of $X$ . So does Mathematics allows subtracting number or any single value from a function? If yes, is the meaning always in the sense similar to vector addition/subtraction with scalar, where too we do add/subtract scalar to each item in vector. I'm comparing this from my Programming experience where the functions and numbers are generally considered separate types which can't do algebra with each other.","['functions', 'variance', 'probability-theory']"
4546145,Why are there lots of groups with order $2^n$?,"I've noticed that, in the OEIS sequence A000001, lots of record high values are held by powers of $2.$ The records are held by only $1, 4, 8, 16, 24, 32, 48, 64, 128, 256, 512,$ and $1024.$ The only record holders that aren't powers of two are $24$ and $48.$ Can anyone tell me why this is so?","['finite-groups', 'oeis', 'abstract-algebra', 'group-theory', '2-groups']"
4546183,"Evaluate $\lim_{(x,y)\to (0,0)}{\frac{x^3 y}{x^2 - y}}$.","The way I did it is that $$\left|\frac{x^3 y}{x^2 - y}\right|\leq \left|\frac{x^3}{x^2 - y^2}\right|$$ for small values of $x$ . Hence, if the limit of the expression on the right exists, then the limit of the expression on the left is the same by the Squeeze Theorem. Since the limit goes to $(0,0)$ , we can use a polar coordinate substitution. Thus, let $x=r\cos \theta$ and $y = r\sin \theta$ . Then \begin{align*}
\lim_{(x,y)\to (0,0)}{\left|\frac{x^3}{x^2 - y^2}\right|}
&= \lim_{r\to 0}{\left|\frac{r^3 \cos^3 \theta}{r^2\left(\cos^2 \theta - \sin^2 \theta\right)}\right|}
\\
&=\frac{\cos^3 \theta}{\cos 2\theta}\cdot \lim_{r\to 0}{r}
\\
&=0,
\end{align*} meaning the original limit (in the title) is also $0$ , by the Squeeze Theorem. As far as I can tell, this works. However, I wanted to know if there is a simpler way to do this problem, especially since the ""small $x$ "" criterion does not have a 'nice' boundary, such as $x\in (-1,1)$ . So any advice on alternative ways of doing this problem, maybe with the limit laws, would be appreciated.","['limits', 'multivariable-calculus']"
4546219,Does there always exist a unique point with minimal average distance to any curve?,"Given any curve $\gamma:[0,1]\to\mathbb{R}^2$ , we can consider the average distance from any point $x\in\mathbb{R}^2$ to the curve, $\int_0^1d(x,\gamma(t))dt$ . I am interested in the points $x\in\mathbb{R}^2$ which minimize this distance. It is easy to prove that such points exist by a compactness argument, but are they always unique? This problem can be expressed much more generally: given a compactly supported probability measure $m$ on $\mathbb{R}^2$ , is there always a unique point $x$ that minimizes $\int_{\mathbb{R}^2}d(x,y)dm(y)$ ? In this case the general answer is no, for example, we can consider a measure where the points $(0,0)$ and $(0,1)$ each have probability $\frac{1}{2}$ : then any point between them minimizes the average distance. But are there some simple sufficient conditions that eliminate ""trivial"" counterexamples like that one? (e.g. the measure not being supported in two points or something more restrictive if necessary). The case of the first paragraph with a curve $\gamma$ can be seen as a concrete case of a compactly supported probability measure, just by defining a measure $m$ in $\mathbb{R}^2$ by $m(A)=\mu(\gamma^{-1}(A))$ , where $\mu$ is the usual Lebesgue measure in $[0,1]$ . If the uniqueness part is true, it would have some cool consequences: for example, if the curve is a regular polygon parametrized by arc length, then the unique point at minimal average distance has to be the center of the polygon. The same would apply to other curves whose isometry group (that is, the group of isometries $\phi:\mathbb{R}^2\to\mathbb{R}^2$ such that $\phi\circ\gamma=\gamma$ ) fixes just $1$ point.","['measure-theory', 'curves', 'probability-distributions', 'geometric-measure-theory', 'analysis']"
4546269,How to solve $\lim\limits_{x\to 1} \frac{1-\cos(\sin(x^3-1))}{x^3-1}$ without L'Hospital's Rule?,"I am asked to solve $\lim\limits_{x\to 1} \frac{1-\cos(\sin(x^3-1))}{x^3-1}$ without using L'Hospital's Rule. I'm not sure how to go about it. There is an indeterminate form $(\frac{0}{0})$ at $x=1$ and L'Hospital's Rule seems like the best course of action. I tried to multiply by $\frac{1+\cos(\sin(x^3-1))}{1+\cos(\sin(x^3-1))}$ to get $\lim\limits_{x\to 1} \frac{1-\cos^2(\sin(x^3-1))}{x^3-1}\cdot\frac{1}{1+\cos(\sin(x^3-1))} = \lim\limits_{x\to 1} \frac{\sin(\sin(x^3-1))}{x^3-1}\cdot \sin(\sin(x^3-1))\cdot\frac{1}{1+\cos(\sin(x^3-1))}$ I'd try to get a limit of the form $\lim\limits_{u\to 0} \frac{\sin(u)}{u}$ because I know that that limit is $1$ , but the problem is that sine is composed with itself. Any help would be appreciated!","['limits', 'calculus', 'limits-without-lhopital', 'trigonometry']"
4546310,Defining the sheaf of bigraded homotopy groups in motivic homotopy theory.,"I have been learning motivic homotopy theory from these notes and on page 154 (page 8 of the pdf), the author defines $\pi_{p,q}(E)$ where $E$ is an $(s,t)$ -bispectrum. He defines it as the sheaf of bigraded stable homotopy groups associated to the presheaf $$
U\mapsto \text{co}\hspace{-0.2cm}\lim_{m\geq -q}\text{Hom}_{SH^{\mathbb{A}^1}_s(k)}(S^{p-q}_S\wedge S_T^{q+m}\wedge\sum_s^\infty U_+, E_m)
$$ However, this is a presheaf valued in sets, so even if we sheafify, we will get a sheaf valued in sets. Is there some group structure on the Hom sets that I am missing?","['higher-homotopy-groups', 'higher-category-theory', 'algebraic-geometry', 'stable-homotopy-theory', 'algebraic-topology']"
4546324,How do I stop overcomplicating proofs?,"I'm a third year student majoring in Math.
Whenever I sit down and try to prove something, I just don't know what and where to start with. The first proofs course I took was graded very strictly so missing a very tiny detail made me lose a lot of marks (which does make sense since it is an introductory class to proofs and the ""little details"" could have been not ""little""). But after that, I just get way too anxious when I do proofs because I don't know what kind of detail I would be missing. I end up completing the proofs by getting a lot of hints on where to start, and it takes way too much time for me to do a single proof (almost 2-3 days per one theorem). And because I don't want to get the proofs wrong, I keep searching up resources to do the proofs; so I kind of end up not doing the proofs myself. But when I see the ""solutions"" to the proofs, I realize they were very simple and I have been over-complicating it a lot. I really love math and I want to be able to really understand courses like Real Analysis, and how scared I am with proofs definitely is an issue that I want to overcome. So my question is (i) If you have gone through this stage, how did you overcome? (ii) Are there any general tips on starting proofs? Thanks.","['proof-writing', 'soft-question', 'real-analysis']"
4546368,"How are the words ""extend"", ""extending"", ""extension"" used in these particular math contexts.","In the following three examples, it has the words: extends, extension or extending.  I often see these three words in the context of mapping. Can someone tell me how these words are used and what they mean in the three particular context.  Is it related to the concept of function extension or group or field extension, etc.  I put all three examples in one post since the three words are close in meaning. I place the word in boldface in all three cases. Example 1: A group $F$ is said to be free on a subset $X\subset F$ if, given any group $G$ and any map $f:X\rightarrow G$ , there is a unique homomorphism $f':F\rightarrow G$ $\textbf{extending}$ $f$ , that is, having the property that $f'(x)=f(x)$ for all $x\in X.$ or the following maps $\text{inc}:X\rightarrow F,$ $f:X\rightarrow G,$ and $\exists !f':F \dashrightarrow G$ are commutative.  Then $X$ is called a basis of $F$ and $|X|$ the rank of $F$ , written $r(F).$ Example 2: Deduce that $S_{6}=\{{t'}_{1},\ldots,{t'}_{5}\}$ and that the map $(1 2)\mapsto {t'}_{1}$ $(2 3)\mapsto {t'}_{2}$ $(3 4)\mapsto {t'}_{3}$ $(4 5)\mapsto {t'}_{4}$ $(5 6)\mapsto {t'}_{5}$ $\textbf{extends}$ to an automorphism of $S_6$ $\text{Example 3:}$ Let $G$ be an $\textbf{extension}$ of $A_6$ by an exterior automorphism of $S_6$ of order $2$ . How does this group look like, is it just $S_6$ , or where is the difference? Thank you in advance","['group-theory', 'functions', 'terminology']"
4546386,Functional distance in a manifold,"In Riemannian Geometry of Peter Petersen , he gave a definition of functional distance in a manifold $M$ as following: $$
d_F(p,q)=\sup\{|f(p)-f(q)|\ |f:M\to \mathbb{R} \text{ has } |\nabla f|\leq 1 \text{ on } M\}.
$$ He said that the distance is always smaller than the arclength distance. But I can't understand it.","['riemannian-geometry', 'differential-geometry']"
4546389,Double dual of a subsheaf of a vector bundle,Let $F$ be a non-trivial proper subsheaf of a vector bundle $E$ over a smooth projective surface $X$ . Is it necessarily true that $F^{**} \subset E$ ? I can construct a map from $F^{**} \to E$ from dualizing twice the sequence $0 \to F \to E \to E/F \to 0$ . But why in the left we should end up with $0$ ?,['algebraic-geometry']
4546484,"How to evaluate this improper integral $\int_{-\infty}^\infty x^2 e^{-x^2}\cos x \, dx$?","How to evaluate $$
\int_{-\infty}^{\infty} x^2 e^{-x^2} \cos x \, dx?
$$ My Attempt: Since the integrand is an even function, we have $$
\int_{-\infty}^{\infty} x^2 e^{-x^2} \cos x \, dx = 2 \int_0^{\infty} x^2 e^{-x^2} \cos x \, dx. \tag{1} 
$$ Since $-1 \leq \cos x \leq 1$ and $x^2 e^{-x^2} > 0$ for all $x$ such that $0 \leq  x < \infty$ , we have $$
-x^2 e^{-x^2} \leq x^2 e^{-x^2} \cos x \leq x^2 e^{-x^2},
$$ which implies $$
-\int_0^{\infty} x^2 e^{-x^2} \, dx \leq \int_0^{\infty} x^2 e^{-x^2} \cos x \, dx \leq \int_0^{\infty} x^2 e^{-x^2} \, dx, 
$$ and hence $$
-2 \int_0^{\infty} x^2 e^{-x^2} \, dx \leq 2 \int_0^{\infty} x^2 e^{-x^2} \cos x \, dx \leq 2 \int_0^{\infty} x^2 e^{-x^2} \, dx.
$$ Now if we can show that $$
\int_0^{\infty} x^2 e^{-x^2} \, dx = 0, 
$$ then would obtain $$
\int_{-\infty}^{\infty} x^2 e^{-x^2} \cos x \, dx = 0.
$$ [ Refer to (1) above.] Am I right? If so, then what next? How to proceed? Or, is there another better way around this problem?","['integration', 'improper-integrals', 'definite-integrals', 'analysis', 'real-analysis']"
4546523,sheaves on a scheme,"I work within the framework of Demazure & Gabriel`s book. (My schemes are all functors). Let $X$ be a scheme. I can define an underlying topological space $|X$ | of $X$ . Its points are equivalence classes of field valued generalised points $\text{Spec }K\to X$ , and its topology is induced by the open subfunctors of $X$ . As for any topological space I get a topos of sheaves $\text{Sh}(|X|)$ . Let $\text{Aff}$ be the opposite of the category of rings equipped with the Zariski Grothendieck topology. Every scheme is a sheaf on $\text{Aff}$ . What is the relationship between the comma category $\text{Sh}(\text{Aff})/X$ and the category $\text{Sh}(|X|)$ ? It seems like every object $F\to X$ of the slice category gives me an object of $\text{Sh}(|X|)$ by letting its set on the open subfunctor $U$ be the set of sections of $U\to F$ . Is there a construction in the other direction? Is it an equivalence?","['grothendieck-topologies', 'quasicoherent-sheaves', 'category-theory', 'algebraic-geometry', 'topos-theory']"
4546553,correspondence between locally free sheaf of finite rank equipped with flat connection and the local system,"The $(X,C^\infty)$ is a smooth manifold then we have the following theorem which says the category of locally free sheaves of finite rank with flat
connection is equivalent to the category of $\Bbb{R}$ -local system. The functor between the category of locally free sheaves with flat connection and the category of the local system as shown in this post , takes the kernel of the connection and gives $\mathcal{L} = \ker \nabla$ . I try to prove that $\mathcal{L}$ is a local system. In the language of a vector bundle, the kernel of the connection is some parallel section of bundle, if the connection is flat we can parallel transport along the coordinate axis, since the commutation relation $$[\nabla_{\partial_i},\nabla_{\partial_j}] = 0$$ we can prove that this process really produces a section such that $\nabla s = 0$ that independent order of translation. (See for example Lee's Riemannian manifold book Lemma 7.8). However, I don't know how to prove it is a local system after that. (That is I want to show $\mathcal{L}|_U \cong \underline{\Bbb{R}}^k$ )","['complex-geometry', 'algebraic-geometry', 'smooth-manifolds', 'differential-geometry']"
4546588,"Is sup of the solutions in $t$ to $\int_{B(x,t)}f\,dV=t^n$ Borel measurable?","I have no idea how to find an answer to the following question: Let $f:\mathbb{R}^{n} \rightarrow [0,\infty)$ be a smooth, non-negative function such that for any $x\in\mathbb{R}^{n}$ , there exists $t_x\in (0,\infty)$ such that $$\int_{B(x,t_x)}fdV=t^{n}_x.$$ Define the function $r:\mathbb{R}^n \rightarrow (0,\infty]$ by $r(x) = \sup \left\{t\in(0,\infty)\,:\,\int_{B(x,t)}fdV=t^{n}\right\}$ . My question: Is $r$ Borel measurable? Any hint would be appreciated.","['measure-theory', 'real-analysis']"
4546620,Determinant of a sum of two matrices [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question For a matrix $X$ with $\det X = 0$ , what should be the constraints on $Y$ such that $\det (X+Y) = 0$ ? On obvious choice would be $Y=\alpha X$ , but can one say something more than this?","['matrices', 'determinant']"
4546626,2-transitive permutation groups of small degree,"I've recently been learning about 2-transitive permutation groups. To better my understanding of the classification of 2-transitive groups I'm trying to learn about 2-transitive permutation groups of small degree, say $ n \leq 30 $ . To make this slightly more manageable I've confined myself to odd degree. Is this the full list of 2-transitive permutation groups for odd degree $ n \leq 30 $ (excluding $ A_n,S_n $ )? $ n $ is the degree $ k $ is the transitivity $n$ $k$ Group 5 2 $AGL(1,5)$ 7 2 $AGL(1,7)$ 7 2 $PSL(3,2)$ 9 2 $AGL(1,9)=3^2:8$ 9 2 $AGL(2,3)=3^2:2S_4$ 9 2 $ASL(2,3)=3^2:2A_4$ 9 2 $3^2:Q_8$ 9 2 $3^2:2D_8$ 9 3 $PSL(2,8)$ 9 3 $P\Gamma L(2,8)$ 11 2 $AGL(1,11)$ 11 2 $PSL(2,11)$ 11 4 $M_{11}$ 13 2 $AGL(1,13)$ 13 2 $PSL(3,3)$ 15 2 $PSL(4,2)\cong A_8$ 15 2 $A_7$ 17 2 $AGL(1,17)$ 17 3 $PSL(2,16)\leq G \leq P\Gamma L(2,16)$ (3 total) 19 2 $AGL(1,19)$ 21 2 $PSL(3,4)\leq G \leq P\Gamma L(3,4)$ (4 total) 23 2 $AGL(1,23)$ 23 4 $M_{23}$ 25 2 $ASL(2,5)\leq G \leq A \Gamma L(2,5)$ (7 total) 25 2 $AGL(1,25)$ 25 2 $A\Gamma L(1,25)$ 27 2 $AGL(3,3)$ 27 2 $ASL(3,3)$ 27 2 $AGL(1,27)$ 27 2 $A\Gamma L(1,27)$ 29 2 $AGL(1,29)$ Edit: I updated the list using the answer/comments/GAP code from comments Edit 2: reader beware I think I fixed the degree 27 groups but seems like my degree 25 groups still aren't right One simple pattern we see here is that $ AGL(1,n) $ as a subgroup of $ S_n $ is $ 2 $ -transitive if and only if $ n $ is a prime power.","['permutations', 'gap', 'group-theory', 'finite-groups']"
4546638,How can I prove $(k^2 -4)x^2 +4k$ can be a perfect square only if $k$ is a perfect square?,"Proof $(k^2 - 4)x^2 +4k$ can be a perfect square only if $k$ is a perfect square. $k$ and $x$ are positive integers. I have come to this formula while solving 1988 IMO #6 without using Vieta jumping, which means $$ \frac{x^2+y^2}{xy+1} = k $$ can be expressed as $$ y = \frac{1}{2} \left(k x - \sqrt{4 k - 4 x^2 + k^2 x^2} \right) $$ I thought if $4 k - 4 x^2 + k^2 x^2$ , which is $(k^2 -4)x^2 +4k$ is a perfect square, eventually, $k$ will be an integer.","['number-theory', 'vieta-jumping', 'quadratics']"
4546650,Is there any nontrivial group that is isomorphic to its outer automorphism group?,"I know that there are lots of nontrivial groups that are isomorphic to their automorphism groups like $S_3$ . Is there any nontrivial group that is isomorphic to its outer automorphism group? Is there any nontrivial finite group isomorphic to both its inner automorphism group and its outer automorphism group? If so, what is the classification of such groups?","['automorphism-group', 'group-theory', 'abstract-algebra', 'finite-groups']"
4546655,Method to approximate continuous functions by smooth functions?,"Sorry that this is informal, I don't know what formalizes this idea. Let's say we have a function, which, like $|x|$ has many ""sharp corners"" or even perhaps cusps. Well, I don't like those sharp corners because they're not infinitely differentiable. Is there a method to approximate a zig-zag or cusp-filled function by a smooth function with rounded corners?","['functions', 'approximation-theory', 'real-analysis']"
4546677,Understanding why a sequence of random variables is not identically distributed,Consider the sequence $(X_n)_n$ of mutually independent v.a. whose probability law is defined for $P(X_n=-\sqrt n)= P(X_n=\sqrt n)= \frac{1}{2}$ Why they are not identically distributed?,"['statistics', 'probability-distributions', 'probability']"
4546683,"When differentiating $4x^2 + 3y^2 -3xy$ with respect to $x$, why does $3y^2$ 'disappear'?","When differentiating $4x^2 + 3y^2 -3xy$ with respect to $x$ , why does $3y^2$ 'disappear'? Not too sure where this $3y^2$ goes, or why $3xy$ can turn into just $3y$ . Would love an explanation behind this.","['economics', 'derivatives']"
4546726,wave equation with moving end,"Solve the wave equation with moving end: $$
\begin{aligned}
u_{t t} &=c^{2} u_{x x} \text { for } x>0, t>0 \\
u(x, 0) &=\varphi(x), u_{t}(x, 0)=\psi(x) \text { for } x \geq 0 \\
u(0, t) &=f(t) \text { for } t \geq 0
\end{aligned}
$$ Suppose $x_{0} \geq c t_{0}$ . Now $\left[x_{0}-c t_{0}, x_{0}+c t_{0}\right]$ lies entirely on the nonnegative part of the $x$ - axis, and not enough time has passed for the initial displacement $f(t)$ at time zero to reach any point in this interval. In this case, $u\left(x_{0}, t_{0}\right)$ is not influenced by $f$ and the d'Alembert formula holds. $$
u\left(x_{0}, t_{0}\right)=F\left(x_{0}-c t_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0} \geq c t_{0}
$$ $F$ and $B$ are the forward and backward waves. $\psi(x)$ are not defined for $x_{0}-c t_{0}<x<0$ ; hence $F\left(x-c t_{0}\right)$ is not defined. However, putting $x=0$ yields $$
u\left(0, t_{0}\right)=f\left(t_{0}\right)=F\left(-c t_{0}\right)+B\left(c t_{0}\right) .
$$ This suggests that we can extend $F$ to this negative value by defining $$
F\left(-c t_{0}\right)=f\left(t_{0}\right)-B\left(c t_{0}\right) .
$$ Both function values on the right are well defined. Further, since $t_{0}$ can be any positive number, we can think of $c t_{0}$ as any positive number and use this equation as a model to define $$
F(-x)=f\left(\frac{x}{c}\right)-B(x)
$$ for any positive number $x$ . This extends $F$ to negative values. For simplicity, we are using the same symbol $F$ for the extended function. Now put, for $x_{0}-c t_{0}<0$ $$
\begin{aligned}
F\left(x_{0}-c t_{0}\right) &=F\left(-\left(c t_{0}-x_{0}\right)\right) \\
&=f\left(\frac{c t_{0}-x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right)
\end{aligned}
$$ or $$
F\left(x_{0}-c t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right) .
$$ Substituting this into d'Alembert's solution, we have $$
u\left(x_{0}, t_{0}\right)=f\left(t_{0}-\frac{x_{0}}{c}\right)-B\left(c t_{0}-x_{0}\right)+B\left(x_{0}+c t_{0}\right) \text { for } x_{0}-c t_{0}<0
$$ In view of the definition of the backward wave $B$ , this equation can be written $$
\begin{aligned}
u\left(x_{0}, t_{0}\right) &=f\left(t_{0}-\frac{x_{0}}{c}\right)+\frac{1}{2}\left(\varphi\left(x_{0}+c t_{0}\right)-\varphi\left(c t_{0}-x_{0}\right)\right) \\
&+\frac{1}{2 c} \int_{c t_{0}-x_{0}}^{x_{0}+c t_{0}} \psi(s) d s \text { for } x_{0}<c t_{0}
\end{aligned}
$$ We have used the zero subscript to discuss the solution at a particular point and maintain $x$ and $t$ as variables. However, we now drop the subscript and write the solution at any $(x, t)$ with $x \geq 0, t \geq 0$ : $$ \bbox[5px, border:2px solid black]
{\begin{aligned}
u(x, t) &=\frac{1}{2}(\varphi(x-c t)+\varphi(x+c t)) \\
&+\frac{1}{2 c} \int_{x-c t}^{x+c t} \psi(s) d s \text { for } x \geq c t\\
u(x, t) &=f\left(t-\frac{x}{c}\right)+\frac{1}{2}(\varphi(x+c t)-\varphi(c t-x)) \\
&+\frac{1}{2 c} \int_{c t-x}^{x+c t} \psi(s) d s \text { for } x<c t
\end{aligned}}
$$ I didn't understand the bolded lines, like why we split the domain in $x< ct \& x\geq ct$ ? And what was mean by, and not enough time has passed for the initial displacement $f(t)$ at time zero to reach any point in this interval. In this case, $u\left(x_{0}, t_{0}\right)$ is not influenced by $f$ and the d'Alembert formula holds. I copy the whole page of Beginning partial differential equations by Peter V. O'Neil , page: 134, section: 4.5 Any help will be appreciated, Thanks in advance.","['wave-equation', 'analysis', 'partial-differential-equations']"
4546730,Trying to solve $\frac{1}{\sigma\sqrt{2\pi}}\int_r^\infty xe^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx$,"I'm looking to simplify/solve $$\frac{1}{\sigma\sqrt{2\pi}}\int_r^\infty xe^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}dx$$ , where $\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x-\mu}{\sigma})^2}$ is the pdf of the normal distribution. I tried to look at solving it with substitution, but I struggle at a certain point. So if I define $u=-\frac{1}{2}(\frac{x-\mu}{\sigma})^2$ , then $du = -\frac{x-\mu}{\sigma^2}dx$ Now, I want to come to: $$\frac{1}{\sigma\sqrt{2\pi}}\int_r^\infty e^udu$$ but is the following correct? $$\mu(1-F(r))- \frac{1}{\sigma^3 \sqrt{2\pi}}\int_r^\infty e^udu$$ , where $F(.)$ is the CDF of the normal distribution. I believe I'm making a mistake here, but just don't see what. Any help or hints are appreciated. EDIT I’m not looking for a closed form solution but a solution in terms of the pdf and cdf of the normal distribution.","['probability-theory', 'normal-distribution']"
4546803,How to find the number of integral solutions of a given equation analytically?,"I was given $$\tan^{-1} x+\cot^{-1} y= \tan^{-1} 3$$ and asked to find the number of integral solutions. I was able to find two, which are $(1,2) ; (2,7)$ which was obtained by solving the given equation by taking the tangent of  both sides and writing $\cot y \\$ as $  \tan \frac{1}{y}$ after which I used trial and error.
However, this doesn't avoid the possibility of missing solutions. What's the analytical method of solving such problems? I have come across this question here on solving equations using Simon's favourite factoring technique but, I don't really get that. Hence I'm asking here.","['algebra-precalculus', 'integers', 'trigonometry', 'real-analysis']"
4546804,Find angle $x$ in convex Quadrilateral,"Fairly simple question, in my opinion. Below is a given quadrilateral $ABCD$ and the goal is to solve for the angle labeled $x$ in the diagram. I'm going to post my own approach as an answer. I'm unsure if my solution or my answer are correct, or if there are other better ways to arrive at an answer. Please share your solutions as well!","['euclidean-geometry', 'trigonometry', 'geometry']"
4546813,Error in the tree diagram of the chain rule?,"I was given a problem, we define: $$z(x,y) = x\sin(y^2), y(x) = 2x+1.$$ We wish to use the chain rule to compute $\frac{\partial z}{\partial x}.$ I know, I can get the correct answer using the Jacobian; however, the tree diagram fails me I this regard because, $$ \frac{\partial z}{\partial x} = \frac{\partial z}{\partial x} + \frac{\partial z}{\partial y}\frac{\partial y}{\partial x}.$$ Obviously, we have a problem here. Am I misunderstanding something?",['multivariable-calculus']
4546868,When are positive elements mapped to $1$ by same pure states,"Let $\mathcal{A}$ be a unital C*-algebra, and $0\leq a\leq b\in\mathcal{A}$ be 2 positive elements with unit length, $\|a\|=\|b\|=1$ . Show $\|b-(1-a)\|=1$ . I'm trying to prove by finding a pure state $f$ such that $f(a)=f(b)=1$ . Does it always exist?","['c-star-algebras', 'functional-analysis', 'operator-algebras']"
4546891,$L^{p_0} \cap L^{p_1}$ and $L^{q_0} + L^{q_1}$ are duals of each other,"This is from Exercise 26, Chapter 1, in Stein and Shakarchi's Functional Analysis. Suppose $1 < p_0, p_1 < \infty$ and $1/p_0+ 1/q_0 = 1$ and $1/p_1 + 1/q_1 = 1$ . Show that the Banach spaces $L^{p_0} \cap L^{p_1}$ and $L^{q_0} + L^{q_1}$ are duals of each other up to an equivalence of norms. Below are the definitions of $L^{p_0} \cap L^{p_1}$ and $L^{p_0} + L^{p_1}$ . Define the norm of $f \in L^{p_0} \cap L^{p_1}$ as $$\|f\|_{L^{p_0} \cap L^{p_1}} = \|f\|_{L^{p_0}} + \|f\|_{L^{p_1}}.$$ $L^{p_0}+L^{p_1}$ is defined as the vector space of measurable functions $f$ on a measure space $X$ , that can be written as a sum $f=f_0+f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . Define $$\|f\|_{L^{p_0}+L^{p_1}}=\inf\big\{\|f_0\|_{L^{p_0}}+\|f_1\|_{L^{p_1}}\big\},$$ where the infimum is taken over all decomposition $f=f_0 + f_1$ with $f_0\in L^{p_0}$ and $f_1\in L^{p_1}$ . What is meant by ""dual space"" is as follows. For every bounded linear functional $l$ on $L^{p_0}+L^{p_1}$ there is a unique $g \in L^{q_0} \cap L^{q_1}$ so that $$l(f) = \int_X f(x)g(x) d\mu(x), \quad \text{for all $f \in L^{p_0}+L^{p_1}$}$$ Moreover, $\|l\|_{(L^{p_0}+L^{p_1})*} = \| g \|_{L^{p_0} \cap L^{p_1}}$ . By modifying Lemma 4.2 in the textbook, it is not too hard to prove the case above, i.e., $L^{p_0} \cap L^{p_1}$ is the dual space of $L^{p_0} + L^{p_1}$ . I have trouble to prove the opposite, that $L^{p_0} + L^{p_1}$ is the dual space of $L^{p_0} \cap L^{p_1}$ .","['banach-spaces', 'lp-spaces', 'functional-analysis', 'real-analysis']"
4546925,What is a simple (not many relators) presentation of the Monster group?,I know that the Monster group is the largest sporadic finite simple group. Is there any simple presentation of the Monster group? $79$ relators or less is preferable.,"['group-presentation', 'sporadic-groups', 'simple-groups', 'abstract-algebra', 'group-theory']"
4546932,Is this notation appropriate for multiple integrals of this form?,"I am trying to write the following integral in an easier notation $$
\int_0^\infty\ldots \int_0^\infty f(x_1,\dots,x_k)\,dx_1\ldots dx_k
$$ For example $$
\int_0^\infty\ldots \int_0^\infty \prod_{i=3}^k e^{-x_i} \,dx_3\ldots dx_k
$$ Is the notation $$
\int_{\mathbb{R}_{+}^k} f(x_1,\dots,x_k)\,dx_1\ldots dx_k
$$ correct? Also, is it appropriate to write $dx_1\ldots dx_k$ in a simpler form?","['integration', 'multivariable-calculus', 'notation']"
4546938,is the absolute value of function $H^s$ also in $H^s$?,"If $f$ and $g$ are (complex-valued) functions in the Sobolev space $H^s(\mathbb{R}^d)$ then any linear combination $f+\alpha g$ with $\alpha\in\mathbb{C}$ is also in $H^s(\mathbb{R}^d)$ since it is a vector space and, if $s>d/2$ , it can be shown that the product $fg$ is in $H^s(\mathbb{R}^d)$ with $$
\|fg\|_{H^s(\mathbb{R}^d)}\lesssim \|f\|_{H^s(\mathbb{R}^d)}\|g\|_{H^s(\mathbb{R}^d)}
$$ I was wondering, however, what can be said about the absolute value of a function in $H^s(\mathbb{R}^d)$ . Is it true that if $f\in H^s(\mathbb{R}^d)$ then also $|f|\in H^s(\mathbb{R}^d)$ ? Here $|\cdot|$ denotes the absolute value in $\mathbb{C}$ since $f$ is possibly complex-valued. I've found two similar questions already (see 1 or 2 ) but none solved in full generality. If someone could help me by giving a hint or providing a reference I'd very much appreciate it, thank you :)","['fractional-sobolev-spaces', 'sobolev-spaces', 'functional-analysis']"
4546954,Using dot produt or element-wise as multiplication for vectorized multivariables functions in chain rule?,"Should we use dot product or Hadamard product (element-wise) for vectorized mutlivariable functions with the chain rule ? I'm struggling to find the correct operation rule between gradient and Jacobian for the chain rule. I have the following expression : for $
x = \left[ \begin{matrix}
x_1 \\
x_2 \\
x_3 \\
\end{matrix} \right] \in \mathbf{R}^3
$ , and for $a, b, c \in \mathbf{R}$ , $w = \left[ \begin{matrix}
a \\
b \\
c \\
\end{matrix} \right] \in \mathbf{R}^3
$ $$
v_4 = g(w, x) \in \mathbf{R}^3
$$ $$
v_5 = f(w, v_4) \in \mathbf{R}^3
$$ $$
v_6 = L(v_5) \in \mathbf{R}
$$ $$
v_7 = x \times v_6 = x \times L(f(w, g(w, x))) \in \mathbf{R}^3
$$ Where I considered vectorized functions defined as following : for example, if $f: (w \in \mathbf{R}^3, x \in \mathbf{R}) \mapsto a \times_{\mathbf{R}} x + b \times_{\mathbf{R}} x^2 + c$ the vectorized function is $(w \in \mathbf{R}^3, x \in \mathbf{R}^3) \mapsto \left[ \begin{matrix}
f'(x_1) \\
f'(x_2) \\
f'(x_3) \\
\end{matrix} \right]$ with $f'$ being the scalar function. Then, for the differentiation we can consider the use of the Jacobian (which is then a diagonal matrix). Here : $$
g(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R}
$$ $$
f(w, x) : \mathbf{R}^3 \times \mathbf{R} \rightarrow \mathbf{R}
$$ $$
L(x) : \mathbf{R}^3 \rightarrow \mathbf{R}
$$ The chains rule give us : $$
\frac{\partial{v_7}}{\partial{a}} = x \left( \frac{\partial{L(v_5)}}{\partial{v_5}} \left( \frac{\partial{f(w, v_4)}}{\partial{v_4}} \frac{\partial{g(w, x)}}{\partial{a}} + \frac{\partial{f(w, v_4)}}{\partial{a}}  \right) \right)
$$ which in term of objects give us : $$
\left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{1} \left( \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot \end{matrix} \right] \times_{2} \left(
\left[ \begin{matrix} 
\cdot & 0 & 0 \\
0 & \cdot & 0 \\
0 & 0 & \cdot \\
\end{matrix} \right] \times \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] + \left[ \begin{matrix} \cdot \\ \cdot \\ \cdot  \end{matrix} \right] \right) \right)
$$ The question here is : Should I be using the dot product for $\times_{2}$ or matrix multiplication by using the transpose of the $\frac{\partial{L(v_5)}}{\partial{v_5}}$ (or using the row column convention for the gradient which may also impact the form of $\frac{\partial{g(w, x)}}{\partial{a}} = \left[ \begin{matrix} \frac{\partial{g_1(w, x_1)}}{\partial{a}} \\ \frac{\partial{g_2(w, x_2)}}{\partial{a}} \\ \frac{\partial{g_3(w, x_1)}}{\partial{a}} \end{matrix} \right]$ ) or the element-wise product which may result in having $\left[ \begin{matrix} x_0 & x_0 & x_0 \\ x_1 & x_1 & x_1 \\ x_2 & x_2 & x_2 \end{matrix} \right]$ instead of $x \times_{1} ...$ , by defining for any constant $e \in \mathbf{R}$ and vector $v \in \mathbf{R}^3$ , $v \times e = v \times \left[ \begin{matrix} e \\ e \\ e \end{matrix} \right]$ ?","['jacobian', 'multivariable-calculus', 'matrix-calculus', 'gradient-descent', 'chain-rule']"
4546956,show that some pair of 8 points in the unit square will be at most $\sqrt{5}/4$ units apart,"Show that some pair of 8 points in the unit square will be at most $\sqrt{5}/4$ units apart. Choose coordinates so that the unit square is centered at the origin. Consider the seven circles with radius $\sqrt{5}/8$ centered at $(\pm 3/8, \pm 1/4), (0, \pm 1/2), (0,0).$ Some two of the two points must lie in the same circle. But how would one prove that the circles cover the entire unit square formally? I know that if two circles intersect, the distance between the centers is less than the sum of their radii. So circles $C_1$ and $C_2$ intersect, where $C_1,C_2$ are centered at $(-3/8, -1/4), (0,-1/2).$","['contest-math', 'pigeonhole-principle', 'geometry']"
4547024,About the algebraic character of the solution of $\cos x=x$ [duplicate],"This question already has an answer here : is the unique solution of $\cos t = t$ a transcendental number? (1 answer) Closed 12 months ago . The equation $x=\cos x$ is well-known because some facts. For example, with an old calculator, you can find approximations of the solution by typing any number and pressing the $\cos$ button repeatedly. With Bolzano-Rolle combo, it is not difficult to show that this solution exists and is unique. My question: Is there some work about the rationality or trescendality of this solution ? Of course, $x$ is in radians.","['trigonometry', 'irrational-numbers', 'transcendental-numbers']"
4547110,An interesting sum involving binomial coefficients $\sum_{k=1}^n\frac{(-1)^{k+1} 2^{2k}}{k}\binom{n}{k}\binom{2k}{k}^{-1}$,"The following sum appears in Brychkov, Marichev, and Prudnikov,  Integrals and Series, Vol 1, 4.2.8, #25 $$\sum_{k=1}^n\frac{(-1)^{k+1} 2^{2k}}{k}\binom{n}{k}\binom{2k}{k}^{-1}= 2 H_{2n} - H_n$$ where $H_n$ are the harmonic numbers . Here is what I've tried: We have the series expansion : $$\sum_{n\ge 1} \frac{2^{2n-1} }{n \binom{2n}{n}} x^{2n-1} = \frac{\arcsin x}{\sqrt{1-x^2}}$$ and so $$\sum_{n\ge 1} \frac{2^{2n-1} }{n \binom{2n}{n}} x^{n} = \frac{\sqrt{x} \arcsin \sqrt{x}}{\sqrt{1-x}}$$ Now we can use the relation between the generating functions for the binomial transform $$G(x) = \frac{1}{1-x}\cdot F(-\frac{x}{1-x})$$ Now things get a bit fuzzy : it's not clear what will be the expansion after the transformation, and how that involves harmonic numbers. Also, here is the Taylor expansions involving harmonic numbers $$\sum_{n\ge 1} H_n x^n= \frac{1}{1-x} \log \frac{1}{1-x}$$ and from here using $x \mapsto \pm \sqrt{x}$ and averaging, we can get the series $$\sum_{n \ge 1} H_{2n} x^n $$ Any feedback is appreciated! $\bf{Added:}$ Thank you all for all the great answers! I've learned a lot. I will try to share a part of what I've learned from your answers: The binomial transform at the level of (exponential) generating series is very powerful. I have to get more comfortable with formulas. There are other interesting formulas that use the Beta integrals to express the inverse of binomial coefficients as an integral--- very useful. I've learned a new formula from this question , indicated by @Marko Riedel: We have the identity in $\alpha$ $$\sum_{k=1}^n \frac{\binom{\alpha + n-k}{n-k}}{\binom{\alpha + n}{n}} \cdot \frac{1}{k} = \sum_{k=1}^n \frac{1}{\alpha + k}$$ This can also be rewritten as $$\sum_{k=1}^n \frac{\binom{n}{k}}{\binom{\alpha + n}{k}} \cdot \frac{1}{k} = \sum_{k=1}^{n} \frac{1}{\alpha + k}$$ or with $\alpha = \beta- n$ , $$\sum_{k=1}^n \binom{n}{k} \frac{1}{k \binom{\beta}{k} } = \sum_{k=0}^{n-1} \frac{1}{\beta- k}$$ If we take $\alpha = -\frac{1}{2}-n$ in the formula we get our formula ( we have $\binom{-\frac{1}{2}}{k} = \frac{(-1)^k \binom{2k}{k}}{2^{2k}}$ ) $\bf{Added:}$ This formula (slightly modified) at 4.2.8. #27 appears in the Volume but only for natural values of $m$ .","['summation', 'harmonic-numbers', 'binomial-coefficients', 'taylor-expansion', 'sequences-and-series']"
4547182,Does 'weak' Steinhaus Property imply Steinhaus Property?,"Steinhaus Property : A subset $A$ of $\mathbb{R}$ is said to have the Steinhaus Property if $A-A$ contains an interval around the origin. Weak Steinhaus A subset $A$ of $\mathbb{R}$ is said to have the weak Steinhaus Property if $A-A$ contains an interval. Does Weak-Steinhaus imply Steinhaus? Countable Sets are neither Steinhaus nor weak Stienhaus. Cantor Set is Steinhaus and any set with positive measure is Steinhaus. Vitali set is not Weak Steinhaus. ( $V-V$ has no rationals for any Vitali set $V$ ). The following is also not weak Steinhaus: Consider $\mathbb{R}/A$ where $A = \{ r + n\sqrt{2} , r \in \mathbb{Q}, n \in \mathbb{Z} \} $ . Take a selector, say $B$ . Take $A_1 = \{ r + n\sqrt{2}, r \in \mathbb{Q}, 2n \in \mathbb{Z} \}$ Take $A_2 = \{ r + n\sqrt{2},  r \in \mathbb{Q}, (2n+1) \in \mathbb{Z} \}$ Now, it is easy to check that $B + A_1$ and its complement $B+A_2$ are both not weak Steinhaus. Any subset of a set which is not weak Steinhaus is also not weak Steinhaus. Note that $0 \in A-A$ . Is there a set which is weak Steinhaus but not Steinhaus?","['elementary-set-theory', 'measure-theory', 'lebesgue-measure']"
4547310,Conditional expectation of symmetric statistic has the same distribution when conditioning on iid random variables,"Suppose I have iid random variables $\{X_1,...,X_n\}$ and a statistic $S(X_1,...,X_n)$ that is symmetric (i.e. if $ (\sigma_{i})_{i=1}^n $ is a permutation of $\{1,...,n \} $ , then $S(x_1,...,x_n) = S(x_{\sigma_1},...,x_{\sigma_n} )$ ). $\textbf{How do I show that $\mathbb{E}(S(X_1,...,X_n)|X_i) \overset{d}{=} \mathbb{E}(S(X_1,...,X_n)|X_j)?$}$ , i.e. the conditional expectations have the same distribution? Related link: Conditional expectation of iid random variables $\textbf{motivation:}$ Efron and Stein (1981) used this ""identity"" https://pdodds.w3.uvm.edu/research/papers/others/1981/efron1981a.pdf","['conditional-probability', 'conditional-expectation', 'probability-theory']"
4547366,"Prove that $ \lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0.$","I have a continuous family of functions $\rho_t(x)$ that converge pointwise to a Gaussian $$
\lim_{t\to\infty}\rho_t(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2}.
$$ I would like to prove that $$
\lim_{t\to\infty} \frac{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\cos(\omega xt)\, dx}{\int_{-\infty}^\infty\rho_t(x)\,e^{xt}\, dx}=0.
$$ Usually, one would argue that the numerator has a rapidly oscillating function at large $t$ multiplied by a function that is becoming smooth and should decay to zero, but the $e^{xt}$ term makes that argument difficult. You can assume $\rho_t(x)$ is well-behaved (whatever smoothness condition is necessary).","['integration', 'central-limit-theorem', 'large-deviation-theory', 'fourier-transform', 'limits']"
4547382,Find the length of perpendicular DF.,"The question is from the topic Similarity of Triangles . It goes like this: $ABCD$ is a quadrilateral with $\angle A= \angle C = 90°$ . AE = 5, BE = 12 and AC = 21. Find the length of DF. The figure:- My attempt:- AB = 13 (Pythagoras Theorem) EC = AC - AE = 16 BC = 20 (Pythagoras Theorem) Let EF = a, so FC = 16 - a Let AD = z and CD = y Let DF = x (5 + a)² + x² = z² 25 + a² + 10a + x² = z² ----(1) x² + (16 - a)² = y² 256 + a² - 32a + x² = y² ----(2) z² + 169 = BD² = y² + 400 z² - y² = 231 Substituting (1) and (2):- 25 + a² + 10a + x² - x² -256 - a² + 32a = 231 42a = 462 $\therefore a = 11$ EF = a = 11 and FC = 16 - a = 5 I don't know what to do now.","['triangles', 'geometry']"
4547400,What's the lower bound for a string of characters to contain all pairs of n different letters?,"Problem: Given that you have n different characters, find the least length needed for a string of characters to contain all possible pairs from those n characters with the order of pairs not mattering, that is, ab is the same as ba . I'm having trouble trying to make a general approach to the problem. Examples: With the letters a, b, c, so n = 3: one the strings with the least length containing all possible pairs (ab, bc, ac) would be 'abca' (which has ab, bc, and ca which is equal to ac). So the least length needed is 4. I also solved this for n = 6, so with letters a, b, c, d, e and f: one the strings with the least length containing all possible pairs (ab, ac, ad, ae, af, bc, bd, be, bf, cd, ce, cf, de, df, ef) would be 'abcadeafbdcefcebfd'. So the least length needed is 18. I currently don't know how to prove these are the least lengths needed, and also don't know how to approach this in a general manner. I would gladly appreciate it if someone could give me some insight on how to solve this. Sorry if my english isn't good enough to translate the problem, I've tried my best. Thanks everyone.","['permutations', 'combinatorics']"
4547432,How to reasonably (numerically) estimate $n\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t+\ldots+{{t^{m-1}}\over{(m-1)!}}\right)\right)^n\right)dt$?,"Recently in doing some expected value calculations, I've derived the following two integrals: $$6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t\right)\right)^6\right)dt$$ $$6\int_0^\infty\left(1-\left(1-e^{-t}\left(1+t + {{t^2}\over2}\right)\right)^6\right)dt$$ Plugging these into Wolfram Alpha can get us numerical answers. For instance, the first integral comes out to about $24.1$ , and the second integral comes out to about $32.7$ . However, I'm wondering if anyone can give a reasonable numerical estimate for these $2$ integrals from first principles (pencil and paper) without using a calculator, Wolfram Alpha, or a computer. I've tried but made little to no progress, and I consulted some nearby PhD students and they didn't know either, so asking here.","['integration', 'estimation', 'real-analysis', 'calculus', 'probability']"
4547459,Self-similar solutions for a particular parabolic system,"Consider the parabolic system \begin{align}
   \begin{cases}
   u_t - \Delta\Big((a_1 + a_{11} u + a_{12} v) u\Big) = 0, & t >0, \ x \in \mathbb R^n \\
   v_t - \Delta\Big((a_2 + a_{22} v + a_{21} u) v\Big) = 0, & t >0, \ x \in \mathbb R^n 
   \end{cases}
\end{align} where $a_1,a_2,a_{11},a_{22},a_{12},a_{21}$ are non-negative constants and the unknowns are $u: (0,\infty)\times \mathbb R^n \to \mathbb R$ and $v: (0,\infty)\times \mathbb R^n \to \mathbb R$ . Main Question: Formally, can we compute a (possibly radial) self-similar solution, that is a solution of this system the form $$\left(\frac{1}{t^{\gamma}} u(x/t^{\alpha}), \ \frac{1}{t^{\kappa}} v(x/t^{\beta})\right)$$ for suitable $\alpha$ , $\beta$ , $\gamma$ , $\kappa$ ? Subquestion: Partial results assuming something (reasonable) extra on the coefficients are also very welcome. Remark 1. The computation that I'm looking for in the system above is very classical in the particular case of the heat equation $u_t -\Delta u = 0$ (i.e. $a_{11} = a_{12} = a_{2} = a_{22} = a_{21} = 0$ ). For your convenience, I'm typing it up below (following what can be found in Evans' book):
Let us look for a solution of the form \begin{equation*}
u(x, t)=\frac{1}{t^{\alpha}} v\left(\frac{x}{t^{\beta}}\right) \quad\left(x \in \mathbb{R}^{n}, t>0\right)
\end{equation*} where the constants $\alpha, \beta$ and the function $v: \mathbb{R}^{n} \rightarrow \mathbb{R}$ must be found.
Plugging this into the heat equation, we arrive at \begin{equation*}
\alpha t^{-(\alpha+1)} v(y)+\beta t^{-(\alpha+1)} y \cdot D v(y)+t^{-(\alpha+2 \beta)} \Delta v(y)=0
\end{equation*} for $y:=t^{-\beta} x$ . In order to transform this into an expression involving the variable $y$ alone, we take $\beta=\frac{1}{2}$ , so that the equation reduces to \begin{equation*}
\alpha v+\frac{1}{2} y \cdot D v+\Delta v=0
\end{equation*} Assuming also that $v$ is radial -- that is, $v(y)=w(|y|)$ for some $w: \mathbb{R} \rightarrow \mathbb{R}$ , we get \begin{equation*}
\alpha w+\frac{1}{2} r w^{\prime}+w^{\prime \prime}+\frac{n-1}{r} w^{\prime}=0
\end{equation*} for $r=|y|, '=\frac{d}{d r}$ . Setting $\alpha=\frac{n}{2}$ , we get \begin{equation*}
\left(r^{n-1} w^{\prime}\right)^{\prime}+\frac{1}{2}\left(r^{n} w\right)^{\prime}=0
\end{equation*} Thus \begin{equation*}
r^{n-1} w^{\prime}+\frac{1}{2} r^{n} w=a
\end{equation*} for some constant $a$ . Assuming $\lim _{r \rightarrow \infty} w, w^{\prime}=0$ , we conclude $a=0$ ; whence \begin{equation*}
w^{\prime}=-\frac{1}{2} r w
\end{equation*} But then for some constant $b$ \begin{equation} 
w=b e^{-\frac{r^{2}}{4}}
\end{equation} In conclusion, we arrive at the expected Gaussian profile $$\frac{b}{t^{n / 2}} e^{-\frac{|x|}{4 t}}$$ Remark 2 For the same question for the Porous Medium Equation $u_t - \Delta u^m = 0$ , see Section 4.4 Source-type solutions. Selfsimilarity from page 69 of https://verso.mat.uam.es/~juanluis.vazquez/BKPME2006six.pdf","['heat-equation', 'reference-request', 'multivariable-calculus', 'calculus', 'partial-differential-equations']"
4547486,Is there a surjective $C^{\omega}$ map from the 2-sphere to the 2-torus?,"Is there a surjective $C^{\omega}$ map from the 2-sphere to the 2-torus? More explicitly, is there a surjective real-analytic mapping $f \colon S^2 \to T^2$ where $$S^2 = \{x \in \mathbb{R}^3 : \|x\| = 1\}, \quad T^2 = \{x \in \mathbb{R}^4 : x_1^2 + x_2^2 = x_3^3 + x_4^2 = 1\}?$$ There are $C^{\infty}$ surjective maps from the 2-sphere to the 2-torus, but all of the constructions I know of are not analytic. Attempt: Suppose such $f$ exists, and consider the projection $P(x_1, x_2, x_3, x_4) = (x_1, x_3)$ .  Then $P \circ f \colon S^2 \to \mathbb{R}^2$ is an analytic surjective map from the sphere to the closed square with vertices $(\pm 1, \pm 1)$ .  I would guess that no such map exists.","['analytic-functions', 'differential-topology', 'analyticity', 'differential-geometry']"
4547508,How many hands does it take to draw all 52 different cards from an infinite deck of cards? [duplicate],"This question already has an answer here : Expected number of times a set of 10 integers (selected from 1-100) is selected before all 100 are seen (1 answer) Closed last year . Say you have a deck of cards 52 distinct cards labeled numbers 1 through 52. Each turn you are given a hand of 5 different cards, each of which are labeled 1–52. Each card has an equal chance of being drawn. Each turn all cards labeled 1–52 are in play; ex: if you draw a 7 one turn, you can draw the 7 again on a future turn. How many expected turns does it take until you have collected all cards labeled 1 through 52?","['statistics', 'probability-distributions', 'coupon-collector', 'card-games', 'probability']"
4547522,Examples of functions with linearity that are not polynomials or derivatives?,"Derivatives, integral and limits have this linearity feature right? Where $f(x+y) = f(x) + f(y)$ (I’m not sure if ‘linearity’ is the proper term). Can someone give examples of more types of functions with this feature? Because the only ones I could think of were linear polynomials or something like sums. Thank you!","['functional-equations', 'real-analysis', 'calculus', 'functions', 'linear-algebra']"
4547524,Is there a continuous open surjective map from the 2-sphere to the 2-torus?,"Is there a continuous open surjective map from the 2-sphere $S^2$ to the 2-torus $S^1 \times S^1$ ? [Some thoughts: Since both spaces are compact, any continuous surjective map is a quotient map.  There are many such maps, but not all of them are open.  Consider projecting the 2-sphere to a disk, distorting the disk into $[0,1]^2$ and then mapping $[0,1]^2$ to the 2-torus (this last mapping is not open).]","['connectedness', 'open-map', 'general-topology', 'differential-topology', 'algebraic-topology']"
4547534,$x \leq y \leq z \iff |x-y|+|y-z|=|x-z|$,"I want to prove one statement. if $x,y,z \in \mathbb R$ $x \leq y \leq z \iff |x-y|+|y-z|=|x-z|$ So with $\Rightarrow$ I don't have a problem. But with another direciton... The solution says: To establish the converse, show that $y<x$ and $y>z$ are impossible. For example,if $y<x \leq z$ , it follows from what we have shown and the given relationshipthat $|x−y|= 0$ , so that $y=x$ , a contradiction. I didn't understand anything. How did we get this contradiction? Why are $y<x$ and $y>z$ impossible, if it's possible. Because if $y<x$ and $y>z$ , then $z<y<x$ and we have $|x-y|+|y-z|=|x-z|$ $x-y+y-z=x-z$ $x-z=x-z$ and it seems ok. Where did I make a mistake or where my missunderstanding? Thank you for help!","['absolute-value', 'analysis', 'real-analysis']"
4547541,"Show that $\int_{0}^{2}\sqrt{x^2+1}\,dx$ is at least $2\sqrt{2}$ without evaluating the integral","Problem: Show that $\int_{0}^{2}\sqrt{x^2+1}\,dx$ is at least $2\sqrt{2}$ without evaluating the integral By breaking the integral into two parts, and with some substitution of variables, I was able to obtain: $\int_{0}^{2}\sqrt{x^2+1}\,dx = \int_{0}^{1}\sqrt{(x-1)^2+1}+\sqrt{(x+1)^2+1}\,dx$ Then I took the derivative of the integrand (on the right side of above equation) with respect to $x$ and was able to show through much algebra that it was greater than $0$ for $0<x<1$ . From this I concluded that the integrand is increasing over the interval $(0,1)$ and therefore the integrand is at least $2\sqrt{2}$ (i.e., the value of the integrand at $x=0$ ) over the interval $[0,1]$ , and therefore: $\int_{0}^{2}\sqrt{x^2+1}\,dx \geq 2\sqrt{2}$ This seems sound, but I wonder if there is some other observation to be made that would provide a simpler or more elegant solution. Any ideas? Thanks!","['integration', 'calculus', 'recreational-mathematics']"
4547557,When is a function a Radon-Nikodym derivative?,"Or, stated another way, what requirements on $f$ makes $\mu(A) = \int_A f(x) \nu(x) dx$ a measure? (Here, $A$ is any measurable subset of $\mathbb{R}$ and $\nu$ is a measure on $\mathbb{R}$ .) I know the Radon-Nikodym theorem says that for any $\mu$ and $\nu$ that are measures on the same space, with $\mu \ll \nu$ , there exists a Radon-Nikodym derivative $f$ for which the above holds, and furthermore $f$ is Borel-measurable. But for a given $\nu$ , would the converse hold? I.e. for any Borel-measurable $f$ , would the $\mu$ defined above be a measure? It does make sense that $\mu \ll \nu$ , since if $\nu(A)=0$ for any $A$ , then $\mu(A) = 0$ too. Attempt at proof: for any $A \subseteq \mathbb{R}$ , $\mu(A) \geq 0$ only if $f(x) \geq 0$ whenever $\nu(A) > 0$ . $\mu(\varnothing) = 0$ since an integral over the empty set is always 0. for a disjoint set of sets $E_k \subset \mathbb{R}$ , the measure of the union is $\mu(\cup_{k=1}^\infty E_k) = \sum_{k=1}^\infty \mu(E_k)$ by linearity of integrals. So, it sounds like we need $f(x) \geq 0$ for all $x\in A$ such that \nu(A) > 0$. Is this the one requirement?","['measure-theory', 'probability-theory', 'radon-nikodym']"
4547563,Regular pentagon and the 42° angle,"Let ABCDE be a regular pentagon. If $\overline{BF} = \overline{BC}$ , calculate $\alpha$ . Using some trigonometry, it's a pretty simple exercise as you can reduce your problem to: $\dfrac{\sin(66°)}{\sin(42°+\frac{\alpha}{2})} = \dfrac{\sin(108°-\alpha)}{\sin(36°+\frac{\alpha}{2})}$ . Sadly, I've been having some problem to prove it geometrically. Any ideas? Thanks in advance.",['geometry']
4547564,"Theorem about fractional iterated functions : is it true, and if yes has it already been discovered?","So, I've been playing with tetration for quite a while now, and I've noticed the method I've been using can possibly be used in any other iteration of a function that satisfy this condition : $f(+\infty)$ converges to a finite value. For clarification, $f^n(x) = f(f(f(...x)))$ n times. So here's the theorem : for any function $f$ where $f^n(x)$ equals the nth iteration of $f(x)$ , where $n$ is a integer, and that the infinite iteration of $f$ equals a finite value $\tau$ , $$f^k(x) = \lim_{n\rightarrow+\infty}(f^{-n}((f^n(x)-\tau)f'(\tau)^k +\tau))$$ for any complex number $k$ if $f'(\tau)$ doesn't equal $1$ or $0$ . I've noticed that in this case, the ratio between $\frac{f^{n+1}(x)-\tau}{f^{n}(x)-\tau}$ ALWAYS approaches $f'(\tau)$ when $n$ approaches $+\infty$ , no matter what. So because we have a ratio, we can then calculate the $(n+k)$ th iteration of the function, and by taking the proper amount of time the inverse function, we can have the nth iteration of the function. If the function approaches $\tau$ from the top or bottom, the ratio is positive, if it ocilates around $\tau$ , it's negative, meaning the kth iteration will often result in complex results. So is this true? (I'm unable to demonstrate it), and if it is, has it already been discovered?","['limits', 'functions']"
4547569,"Conjecture on the prime factorisation of the $\operatorname{lcm}(1,\dots,k) \pm 1$, $k \geq 2$","For $k \in \mathbb{N},\, k \geq 2$ , define $S(k) := \operatorname{lcm}(2,\dots,k)$ . $S$ stands for ""superprimorial"", as it is a name I've seen come up once before and that I kinda like: it can be written as the (finite) product $S(k) = \prod_{p \in \mathbb{P}} p^{\max\{\alpha \in \mathbb{N} \,|\, p^\alpha \,\leq\, k\}} = \prod_{p \in \mathbb{P}} p^{\big\lfloor \frac{\ln(k)}{\ln(p)} \big\rfloor}$ where $\mathbb{P}$ is the set of positive prime numbers, and since the sequence of the exponents is decreasing when $p$ increases, it is a product of primorials, a ""super"" primorial so to speak. Below are my two personal conjectures, based on manual realisation for smaller $k$ and a rudimentary Python program I made for bigger but still small $k$ . I tried searching on this site among other $\operatorname{lcm}$ and number theory posts to see if this was answered or at least already talked about, to no avail. I'm rather new here though so maybe I missed some? Conjectures: $(1)$ For $k \neq 8$ , $S(k) + 1$ and $S(k) - 1$ are squarefree, i.e. for all $p \in \mathbb{P}$ , $p^2$ does not divide $S(k) \pm 1 $ . (EDIT: known to be false sadly) $(2)$ For all $k \geq 2$ , $S(k) + 1$ and $S(k) - 1$ each have at most $\bigg\lfloor \frac{\ln(k)}{\ln(2)} \bigg\rfloor = \max\{\alpha \in \mathbb{N} \,|\, 2^\alpha \,\leq\, k\}$ prime divisors counted with multiplicity. $(2)$ would imply, for example, that $S(2^{100}) + 1$ , which is a pretty big number, would have at most $100$ prime divisors, which, while not that small for the purpose of finding prime candidates, is still really small. Plus, the fact that this concerns both $S(k) + 1$ AND $S(k) - 1$ could potentially lead towards the twin prime conjecture or something...? $k = 8$ stands out since $S(8) + 1 = 841 = 29^2$ is not squarefree, and of course if $(1)$ holds then there's no need of talking about multiplicity in $(2)$ for $k \neq 8$ .
Do note that $(1)$ and $(2)$ still remain independent questions though, as in, unless the proof of either of them proves the other one, $(1)$ and $(2)$ should not imply each other (I don't think?). Here are the questions I have in mind right now, having written those conjectures: A) The kinda obvious question: do we know those assertions to be true? Or any partial version, like ""true for big enough $k$ "", ""an infinity of $k$ "", and so forth...? If not, maybe there are counterexamples other than $k = 8$ that I missed, which is very possible? B) At the moment I'm not really knowledgeable on number theory proofs: any recommendations on results and/or techniques that might help me start? For $(2)$ , I've tried upper bounding the number of divisors of $S(k) \pm 1$ by the number of numbers coprime to $S(k)$ ( $S(k)$ being coprime to them, their divisors also have to be coprime to $S(k)$ ) due to the definition of $S(k)$ making it so that a ""lot"" of numbers are not coprime to $S(k)$ AND that it is ""easy"" to evaluate $\varphi(S(k))$ due to the multiplicativity of $\varphi$ (Euler's totient function for those unaware), and I've thought of removing the numbers between $S(k)$ and $S(k)/k$ since all of them are guaranteed to not be ""divisor candidates"", but besides that I'm sort of at a loss. As for $(1)$ , I'm aware of the existence of the Möbius function and the fact there are quite a few results involving it, but I don't really know if that'll really help considering the challenge of the "" $\pm 1$ ""... (Hopefully my English is correct enough. Any advice on the form of the post itself is also welcome.) EDIT: It would seem that conjecture $(2)$ is false by looking at $S(359) - 1$ which has $10$ prime factors (thanks a lot Peter)! Yet, it could still be interesting to know if such counterexamples are rare, or if a different upper bound could still apply, and so on, so I'll leave conjecture $(2)$ 's false statement on the post.","['number-theory', 'recreational-mathematics', 'gcd-and-lcm', 'prime-factorization']"
4547586,Invariance of a symmetric sum,"Consider two matrices $M$ and $N$ of same dimension, such that $L=MN+NM$ is the symmetric sum in the sense that interchanging $M$ and $N$ does not change $L$ . Is there any transformation $T[M]=M^\prime$ and $T[N]=N^\prime$ such that $M^\prime N^\prime+N^\prime M^\prime=MN+NM$ ? In other words, the symmetric sum is invariant under the transformation $T$ .","['matrices', 'linear-transformations']"
4547588,Is $w$ Killing for some semi-Riemannian metric?,"Consider the vector field $w=(x\log x,-y\log y,-z\log z)$ for $0<x,y,z<1.$ I'm wondering if $w$ is Killing for some semi-Riemannian metric. If we consider a lower dimensional version of $w$ i.e. $v=(x\log x,-y\log y)$ then it's not hard to show that $v$ is Killing and preserves $g=\frac{dxdy}{xy}.$ Furthermore it can be shown that the pair $(M,g)$ is equivalent to $\Bbb M^{1,1}$ (Minkowski plane). A quick sketch of how to see the equivalence is to solve for the integral curves of $v$ and then inspect them, revealing that they are rectangular hyperbolae under a change in coordinates. Or you can start with the Minkowski metric in standard coordinates, perform a rotation, then pushforward the metric to get $g.$ A similar approach with $w$ yields integral curves of the form $(e^{-t_1e^{s}},e^{-t_2e^{-s}},e^{-t_3e^{-s}})$ parametrized by $s\in (-\infty,\infty)$ and parameters $t_1,t_2,t_3.$ Inspecting this parametrisation leads me to believe that these integral curves foliate a semi-Riemannian manifold that I think would be equivalent to $\Bbb M^{2,1}.$ Am I on the right track to showing that the integral curves of $w$ foliate a semi-Riemannian manifold equivalent to a component of Minkowski $3-$ space?","['vector-fields', 'semi-riemannian-geometry', 'differential-geometry']"
4547672,Asymptotic Joint Distribution of Variance and Ratio of Mean and Standard Deviation,"I am working on an exercise from a textbook about asymptotic distributions and I happen to see the question and I am interested in knowing how to solve it. The question is: If $X_i$ are i.i.d with mean $\mu$ , variance $\sigma^2$ , assuming that $\mathrm{E}(X^4_1)<\infty$ . I am interested in deriving the asymptotic distribution of $(S^2_n, \bar X_n/S_n)^\top$ . $\bar X_n$ is the sample mean and $S^2_n$ sample variance. Here's what I did so far: By the bivariate CLT, we have that $\sqrt{n}\left(\begin{pmatrix}
S^2 \ \\ \overline{X}/S  \end{pmatrix} 
-
\begin{pmatrix}
\mu \\ \sigma^2  
\end{pmatrix}\right)\stackrel{d}{\rightarrow} N(0, J_{g(a)} \Sigma J'_{g(a)}) $ If we let $g(u,v) = \begin{pmatrix}
v-u^2 \\ \frac{u}{\sqrt{v-u^2}}
\end{pmatrix} $ then we have $$ 
J 
=
\begin{pmatrix}
-2u & 1 \\ 
\frac{v}{(v-u)\sqrt{v-u^2}} & \frac{-u}{2(v-u^2)^{3/2}}
\end{pmatrix} 
\text{ so that } J_{g(a)}= 
\begin{pmatrix}
0 & 1 \\ 
\frac{1}{\sigma } & 0
\end{pmatrix}
\text{ where } 
a = \begin{pmatrix}
0\\\sigma^2
\end{pmatrix},  \text{ and }   
\Sigma 
= 
\begin{pmatrix}
\sigma^2  & \mu_3  \\ 
\mu_3 & \mu_4-\sigma^4
\end{pmatrix}.
$$ Covariance Matrix: $$J_{g(a)} \Sigma J'_{g(a)} =
\begin{pmatrix}
0 & 1 \\ 
\frac{1}{\sigma } & 0
\end{pmatrix}
\begin{pmatrix}
\sigma^2  & \mu_3  \\ 
\mu_3 & \mu_4-\sigma^4
\end{pmatrix}
\begin{pmatrix}
0 & 1 \\ 
\frac{1}{\sigma } & 0
\end{pmatrix}
=
\begin{pmatrix}
\mu_4 - \sigma^4 & \frac{\mu_3}{\sigma} \\ 
\frac{\mu_3}{\sigma} & 1
\end{pmatrix}
$$ $$ \begin{pmatrix}
S^2 \ \\ \overline{X}/S  
\end{pmatrix} 
\stackrel{\text{asym}}{\sim} 
N\left(
\begin{bmatrix}
\mu \\ \sigma 
\end{bmatrix}; \ 
\frac{1}{n}
\begin{bmatrix}
\mu_4 - \sigma^4 & \frac{\mu_3}{\sigma} \\ 
\frac{\mu_3}{\sigma} & 1
\end{bmatrix}
\right)
$$","['statistics', 'probability-distributions', 'solution-verification', 'asymptotics']"
4547690,Rate of convergence with Weak Law of Large Numbers,"Assume $X_i$ are i.i.d with mean $μ$ and the fourth moment of $X_i$ exists. Let $δ < 2$ . Show $n^δ Pr(| X_n − μ| >) → 0$ as $n → ∞$ . I know $Pr(| X_n − μ| >)→ 0$ as $n → ∞$ by the WLLN, but I have no idea what to do with $δ$ , since that value doesn't converge. I have thought about using Slutsky's Theorem, but again, $n^δ$ doesn't really converge to anything, so I don't know what to do with this.","['statistics', 'law-of-large-numbers']"
4547693,What idea of integration were Newton and Leibniz using?,The integral that is taught in calculus courses is the Riemann Integral. Which presumably is named after Bernhard Riemann. But Riemann was born $99$ years after Newton died. So what kind of integration were Newton and Leibniz using? I ask this because we are told that Newtons advisor discovered the Fundamental Theorem of Calculus. But that would require knowledge of integration. So what idea did Newton and Leibniz have of integration if Newton's advisor managed to discover the Fundamental Theorem of Calculus.,"['integration', 'calculus', 'math-history']"
4547722,"Cardinal Arithmetic, Examples","Below you find an excerpt of my notes about cardinal arithmetic. I self-study set theory, so I need someone to check if my notes are correct since I have no teacher and most books leave you alone of how specifically things work. For instance in my book (Deiser, Einführung in die Mengenlehre) the author only gives the result without any explanation. I just wanted to give me some examples of how this kind of arithmetic works. Can you look if the formalities are correct and if my reasons I give once in a while (usually below the critical formula) are correct too? If not, I‘d appreciate some reasoning about what seems suspicious.","['elementary-set-theory', 'functions', 'set-theory']"
4547784,find the number of interesting permutations,"A permutation $a_1,\cdots, a_{2022}$ of $1,2,\cdots, 2022$ is called interesting if $|a_i - i|$ is a constant for all i. Find the number of interesting permutations. Clearly the identity permutation is interesting. Replace $2022$ with $n$ . I think that if $|a_i-i|\neq 0$ , then it must equal $1$ (1). As an example, if $|a_i-i| = 4$ , then $a_1 = 5, a_2 = 6,, a_3 = 7, a_4 = 8,$ and $a_5 = 9$ or $1$ . But I'm not sure how to get a contradiction from this. Note that once we show (1), it is easy to show that there are no interesting nonidentity permutations if $n$ is odd since $n = \sum_{i=1}^n (a_i-i) = 0\mod 2$ for any interesting nonidentity permutation. Edit: the original proof seems like it can be modified slightly to solve the general case. First, we'll prove more formally that permutations get split into cycles of length $2k$ . We indeed have that for $1\leq i\leq k, |a_i - i| = k$ implies that $a_i = i+k$ since $a_i = i-k$ is impossible ( $a_i$ is positive). For $2k+1\leq i, |a_i - i| = k\Rightarrow a_i \ge k+1,$ and hence for $k+1\leq i\leq 2k,$ we must have $a_i = i-k.$ Then $\{1,\cdots, 2k\} = \{a_1,\cdots, a_{2k}\}.$ We claim that the statement P(x) holds where P(x) states that $\{x(2k) + 1,\cdots, (x+1)(2k)\} = \{a_{x(2k)+1},\cdots, a_{(x+1)(2k)}\}$ for all $x\ge 0$ (and we trim the indices if they're out of bounds or we can extend the permutation past n by ignoring extra elements). Assuming that $P(y)$ holds for all $y < x, x\ge 1$ , we then have for $1\leq i\leq k, a_{(x+1)(2k)+i} = (x+1)(2k)+i + k$ since $(x+1)(2k)+i-k$ was already taken by some other elements by the inductive hypothesis. Also, for $k+1\leq i\leq 2k, a_{(x+1)(2k)+i} = (x+1)(2k)+i-k$ since none of these elements can be achieved by other $a_i$ 's using similar reasoning to the base case. Hence by induction we have that $P(y)$ always holds provided y is in range and so for an interesting permutation we must have $2k | n$ (note that at any point in the construction, we also have $\{x(2k) + 1,\cdots, x(2k)+i\} \not\subseteq\{a_{x(2k)+1},\cdots, a_{x(2k)+i}\}$ for any $1\leq i < 2k$ so the permutation wouldn't be surjective if it were cut short because $2k\not\div n$ . This would be shown more formally by adding this claim to the inductive hypothesis). The above proof also shows that for an even divisor $2k$ of $n$ , the permutation satisfying $|a_i-i| = k$ for all i is in fact unique (well technically we need to add this assumption to the inductive hypothesis but that's it); hence the result.","['permutations', 'contest-math', 'recurrence-relations', 'combinatorics', 'discrete-mathematics']"
4547879,Why is my counting wrong? Counting the number of distinct permutations such that no two adjacent letters are the same,"There are three islands labelled $A$ , $B$ , and $C$ . A grasshopper is on island $A$ and hops to one of the two other islands every minute. In how many different ways can the grasshopper end up at island $C$ after seven minutes? Proposed solution: Consider the incomplete sequence $A$ _ _ _ _ _ _ $C$ . We consider the equivalent problem of finding the total permutations possible of the above sequence when inserting the letters $A$ , $B$ , or $C$ into one of the slots such that the following is obeyed: Adjacent slots do not contain the same letter In the first four slots, each slot has $2$ possibilities for a letter. Regarding the antepenultimate slot, there are two cases: containing $A$ or $B$ implies the penultimate slot must contain $B$ or $A$ respectively. That is, the choice of letter is fixed. containing $C$ implies the penultimate slot must contain $A$ or $B$ . That is, there are $2$ choices of letter. Adding the two cases and subtracting off what I think we are overcounting: $2^6 - 2^4$ is my answer. However, by explicit counting I arrive at $43$ . I want a combinatorial answer so why is my counting wrong?",['combinatorics']
