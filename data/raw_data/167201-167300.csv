question_id,title,body,tags
2917120,Collection vs. set of subsets vs. set of all subsets vs. family of sets,"Collection vs. set of subsets vs. set of all subsets? These get mixed. But the idea that I have is: collection : a set of subsets, where order is ignored. So it's like a weaker form of set. set of subsets : could be a collection, however since this is set of sub sets , then one may not know whether the weakened properties of collections suffice. Or whether one actually has set of collections . set of all subsets or the power set: this could be a collection, but it's not required to be one, because one could refrain from using the term collection and rather use subset. family of sets or family of subsets is by definition a collection $F$ of subsets of a given set $S$. So it seems like here collection is the same as set of subsets, but also family of subsets. Or also, family is a set. I wonder what sense does it make to use the terms collection and family at all? If they are sets, then why not call them sets?","['elementary-set-theory', 'terminology']"
2917133,Necessity of uniform integrability in martingale convergence theorem,"Statement of theorem: If $X_n$ is a uniformly integrable martingale, then $\lim_n X_n$ exists a.s. and in $L^1$ , and $$X_n=E(\lim_{n \to \infty} X_n \mid \mathcal F_n) \quad \text{a.s.}$$ I can't think of an example of a martingale that is not uniformly integrable for which $E(\lim_n X_n|\mathcal F_n)\neq X_n$ and $\lim_n X_n$ exists. For instance, let $X_1=1$ , and for $n\ge2$ , $X_n=X_{n-1}+\epsilon_n$ , where $\epsilon_n=n$ with probability 1/2, $\epsilon_n=-n$ with probability 1/2, and $\epsilon_n\perp\epsilon_m$ for $n\neq m$ . This is a martingale that is not u.i., but $\lim_n X_n$ does not exist. Any help greatly appreciated!","['martingales', 'uniform-integrability', 'probability-theory', 'examples-counterexamples']"
2917173,Conformal Killing Equation,"I saw this come up in a lecture on conformal field theory, and was a bit skeptical of the claim. So, given some conformal Killing field, $$
\mathcal{L}_{\xi}~g = \lambda g
$$
it was said the following is true
$$
\mathcal{L}_{\nabla^2\xi}g=g\nabla^2\lambda
$$
Where $\nabla^2=\nabla^a\nabla_a$ (a contraction of covariant derivatives).
I have tried writing things out but can't get anywhere.  I end up with a bunch of Riemann tensor terms that don't cancel. So, first off is this even true, and if so how can I show it?","['laplacian', 'riemannian-geometry', 'differential-geometry']"
2917181,"Find particular solution of $y''+4y=12$ if the point $(0,5)$ has horizontal tangent line","Find the particular solution of $$y''+4y=12$$ if the point $(0,5)$ has horizontal tangent line (parallel to the $x$-axis). I know the general solution of $y''+4y=12$ is $$y=C_1\cos{(2x)}+C_2\sin{(2x)}+3\quad\text{for some}~C_1,C_2\in\mathbb R.$$ Now we have to find the particular solution but I don't know if the initial conditions are correct: $$\begin{cases}y''+4y=12\\\color{red}{y(0)=5}\\\color{red}{y'(0)=k,\quad\text{for some}~k\in\mathbb R}.\end{cases}$$ Are the $\color{red}{\text{initial conditions}}$ correct? Thanks!",['ordinary-differential-equations']
2917183,Two definitions of the space of pointed Riemann surfaces,"I was reading Wendl's notes on closed holomorphic curves, and could not seem to figure out the following assertion. I think I am missing some fact or the other about Riemann surfaces. Let $\mathcal{M}_{g,m}$ be the space of pointed Riemann surfaces. To construct this, take the space of tuples $(\Sigma, j, \Theta)$ where $\Sigma$ is a topological closed surface of genus $g$, $j$ is a complex structure, and $\Theta$ is an ordered tuple of $m$ points in $\Sigma$. Then, quotient out by the equivalence relation $(\Sigma, j, \Theta) \sim (\Sigma', j', \Theta')$ if and only if there exists some biholomorphic diffeomorphism $\phi: \Sigma \to \Sigma'$ such that $\phi$ maps $\Theta$ to $\Theta'$ in an order-preserving manner. We can define a related space as follows. Fix a surface $\Sigma$ and a tuple of $m$ points $\Phi$. Let $\text{Diff}_+(\Sigma, \Phi)$ be the set of all orientation-preserving diffeomorphisms on $\Sigma$ that restrict to the identity map on $\Phi$. Furthermore let $J(\Sigma)$ be the space of complex structures on $\Sigma$. Then one can take the quotient $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$. The claim is that $\mathcal{M}_{g,m}$ is homeomorphic to $J(\Sigma)/\text{Diff}_+(\Sigma, \Phi)$, likely by the map $[(\Sigma, j, \Theta)] \to [j]$ in one direction and $[j] \to [(\Sigma, j, \Phi)]$ in the opposite direction. Right off the bat, to show that the forward map is injective, we must show that, for any complex structure $j'$ and points $\Theta$, the tuple $(\Sigma, j', \Theta)$ is equivalent to $(\Sigma, j', \Phi)$. This reduces to the following statement: There exists a biholomorphic automorphism on $\Sigma$ that takes the set of points $\Theta$ to $\Phi$? I am confused because this statement is certainly not true in general. For example, if we take $g = 0$ and $m = 4$, and $j$ the standard complex structure on $\Sigma = S^2$, the biholomorphic automorphisms are Mobius transformations. These are determined by the image of $\{0, 1, \infty\}$, so the action of the Mobius transformations is not transitive on quadruples of points.","['complex-analysis', 'riemann-surfaces', 'symplectic-geometry']"
2917229,"Show that a subspace is dense in $L^2[0,1]$","Let $f \in L^1[0,1]$, but $f \notin L^2[0,1]$. Consider the subspace $X$ of $L^2[0,1]$ such that $X= \{\phi \in L^2[0,1]: \int f \phi = 0\}$. Want to show that $X$ is dense in $L^2[0.1]$. I tried proving this by checking that $\langle f, g \rangle, \forall g \in X$ implies $f = 0$. However, this does not seem like a plausible way of doing this. I am also hinted by using the theory of densely defined operator, but I only know of this by its definition.",['functional-analysis']
2917230,Question about indexes in sum,"Let $k_1=2$ and $k_2=3$ and $\mu_1=1$  and $\mu_2=3$. I would like to evaluate the following sum
\begin{align}
   S=&\sum_{i=1}^{2}
           \sum_{j=1}^{k_i}\\
   \times& \sum_{\begin{matrix}
   n_1+n_2=k_i-j\\
   n_i=0
\end{matrix}}^{}\prod_{\begin{matrix}
   l=1\\
   l\neq i
\end{matrix}}^{2} \binom{k_l+n_l-1}{n_l}\frac
{u_l^{k_l}}
{(\mu_l-\mu_i)^{k_l+n_l}}
\end{align}
My question is how we get $n_1$ and $n_2$ and  evaluate the above sum?
What is the meaning of this summation and it name where only $k_j$ and $\mu_j$ are known?
$$
\sum_{
\begin{matrix}
   n_1+n_2+\cdots n_n=k_i-j\\
   n_i=0
   \end{matrix}}^{}
$$
Does any one have work with this sum or have some examples about it?
This is independent part, I found the same sum in Leibniz formula which is the derivation of product of $m$ functions given by
$$
(v_1v_2v_3\cdots v_m)^n=\sum_{
\begin{matrix}
   t_1+t_2+\cdots t_m=n\\
   t_1,t_2,\cdots t_m\geq0
   \end{matrix}}^{}\frac
{n!}
{t_1!t_2!\cdots t_m!}v_1^{(t_1)}v_2^{(t_2)}\cdots v_m^{(t_m)}
$$
Just how we can found all $t_i$ or $n_i$ and the sum?","['summation', 'derivatives']"
2917252,Differentiating closed formulas in formal power series,"In Herbert Wilfs' book gfology, the generating function is defined ""formally"" as If $\displaystyle f = \sum_{i \geq 0} a_i x^i$, and $\displaystyle g = \sum_{i \geq 0} b_i x^i $, we define $\displaystyle f+g := \sum_{i \geq 0} (a_i + b_i) x^i$ $\displaystyle fg := \sum_{i \geq 0}(\sum_{m+n = i} a_mb_n)x^i$. $\displaystyle f' := \sum_{i \geq 0} ia_{i+1} x^i$ (Call this $\star$) This is fine and I understand this and have no problem with this. But when we say something like this in a formal setting: $\displaystyle e^x = \sum_{i \geq 0} \frac{x^n}{n!}$ Or, $\displaystyle \frac{1}{\sqrt{1-4x}} = \sum_{k \geq 0} \binom{2k}{k} x^k 
$ (Call this equation $\spadesuit$) My question is: How is $e^x$ or $\displaystyle \frac{1}{\sqrt{1-4x}}$ defined in a formal setting ? Are they defined as the generating functions of the sequences I mentioned ? If it's defined like that, then how can you proceed to ""differentiate"" both sides of $\spadesuit$ to obtain $\displaystyle \frac{2}{\sqrt{(1-4x)^3}} = \sum_{k \geq 0} k \binom{2(k+1)}{k+1} x^k $ I understand how you formally differntiate the right hand side of $\spadesuit$ since that's defined in $\star$, but how the formal differentiation of $\frac{1}{\sqrt{1-4x}}$ defined (especially when limits may not make much sense working on some rings)?","['generating-functions', 'combinatorics', 'taylor-expansion', 'real-analysis']"
2917253,How to show orthogonality with respect to the Euclidean inner product involving a curve in $\mathbb{R}^n$ and a point not on the curve,"Specifically, the question is as follows: Let $f: \mathbb{R} \rightarrow \mathbb{R}^n$ be a differentiable mapping (a curve) with
  $f^\prime(t) \ne 0$ for all $t \in \mathbb{R}$. Let $p$ be a fixed point not on the image $f$. If $q = f(t_0)$ is the point of the curve closest to $p$,
  that is $| p - q| \le |p - f(t)|$ for all $t \in \mathbb{R}$,
  show that the vector $(p - q)$ is orthogonal to the curve
  at $q$. Here the orthogonality is with respect to the usual Euclidean inner product. I've also been given the following hint: Differentiate the function
  $\varphi(t) = | p - f(t)|^2$. I understand that $p$ and $q$ are points in $\mathbb{R}^n$ and that $(p-q)$ is the vector that is between $p$ and $q$. I also understand that to show $(p - q)$ is orthogonal to the curve
at $q$, I need to show that $\langle(p-q),f'(q)\rangle=0$, as orthogonality to the curve at a point is given by a $0$ inner product with the tangent to the curve at the point. My confusion lies in understanding what $\varphi(t)$ and it's derivative represent, as well as how to use them. Correct me if I'm wrong, but it seems like $\varphi(t)$ is a parabaloid (or some step function eqivalent if $f$ is discontinuous) where the vertex is the minimum value, $|p-q|$ (i.e. when $t=t_0$). So what information do I acquire from taking the derivative that I can utilize to solve this problem? In a more direct sense, I know that I want to show that $\langle(p-q),f'(q)\rangle=0$, so, equivalently, I want to show that $\sum_{i=1}^n(p_i-q_i)q_i=0$. Expanding and differentiating, I have that $\varphi'(t)=2\sum_{i=1}^nf'(t)_i(f(t)_i-p_i)$. But I don't see what to do from here. I'd like assistance in interpreting this problem (ideally geometrically and algebraically) so that I can develop a solution. This is homework, so I request guidance only.","['multivariable-calculus', 'real-analysis']"
2917291,Limit of composite function,"I know that $$\lim_{x \rightarrow a} (fg(x)) = f\left(\lim_{x \rightarrow a} g(x)\right)$$ provided that $\lim_{x \rightarrow a} g(x)$ exists and $f$ is continuous at this limit point. Normally when we say that a limit exists, we don't refer to it being infinity. So I am wondering if this applies when $\lim_{x \rightarrow a} g(x) = \infty$ It would seem so, because when I evaluate $$\lim_{x \rightarrow \infty} \tan(\sinh x) = \lim_{x \rightarrow \infty} \tan(x)$$ since $\lim_{x \rightarrow \infty} \sinh x = \infty$ . Even though, we don't evaluate $\tan$ at $\infty$, it is still that same idea... So I have two questions: is what I wrote above correct for that specific example? Can someone write out the conditions clearly for the case where $\lim_{x \rightarrow a} g(x) = \infty$ (I haven't been able to find it online) Thanks",['limits']
2917298,"Loney: If $\alpha$, $\beta$, $\gamma$ are the roots of $x^3 + px^2 + qx + p = 0$, then $\tan^{-1}\alpha + \tan^{-1}\beta + \tan^{-1}\gamma = n\pi$","If $\alpha, \beta, \gamma$ are the roots of the equation $$x^3 + px^2 + qx + p = 0,$$
  prove that $$\tan^{-1}\left(\alpha\right) + \tan^{-1}\left(\beta\right) + \tan^{-1}\left(\gamma\right) = n\pi$$ 
  except in one particular case. This question is from S. L. Loney's 'Plane Trigonometry' page 327 q13. It may be useful to note that this section utilises $$\tan\left(\alpha + \beta + \gamma + ...\right) = \frac{s_1 - s_3 + s_5}{1 - s_2 + s_4 - \cdots}$$
where $s_1 =$ the sum of the tangents of the separate angles, $s_2 =$ the sum of the tangents taken two at a time, $s_3 =$ the sum of the tangents taken three at a time, and so on. I do not know where I should start with this question.","['trigonometry', 'polynomials']"
2917343,"What is the standard topology of real line? Why is $(0,1)$ called open but $[0,1]$ not open on this topology?","For a topological space $(X,\tau)$, the topology $\tau$ on the set $X$ is a family of subsets called open sets, if $X$, $\emptyset$, any union of the subsets, and any finite intersection of the subsets are in $\tau$. This could be a definition of the term ""open sets"" with respect to topology. On the real line, $X = \mathbb{R}$, we say $(0,1)$ is ""open"" based on the definition above, because $\tau_1 = \{\emptyset, \mathbb{R}, (0,1)\}$ can be a topology. However, we say $[0,1]$ is ""not open"", though $\tau_2 = \{\emptyset, \mathbb{R}, [0,1]\}$ can also be a topology that satisfies all of the properties. I think $[0,1]$ is not called open just because the $\tau_2$ topology is not so-called a standard topology of real line, correct? I remember the definition of open set for a metric space $(X,d)$ is, we say a subset $S \subset X$ is open if $\forall x \in S$, $\exists \epsilon > 0$ s.t. $B_{\epsilon}(x):=\{y \in X \;|\; d(x,y) < \epsilon\} \subset S$. Is this exactly the standard topology of real line I am asking for, if we collect all of these subsets of $\mathbb{R}$? In other words, we can define open sets in such a way, and collect all these subsets of $\mathbb{R}$ to make a topology, called standard topology. Am I right? One more question, it seems natural if we take the power set of $\mathbb{R}$, i.e., the discrete topology instead. Why is it not ""standard""?","['general-topology', 'real-analysis']"
2917360,Street Combinatorics - 6 by 7 grid,"You go to school in a building located six blocks east and seven blocks north
of your home. So, in walking to school each day you go thirteen blocks. All streets in a rectangular pattern are available to you for walking.
In how many different paths can you go from home to school, walking only thirteen
blocks? I want to say that the answer can be found knowing that there are $6!$ ways east and $7!$ ways north. Then, the answer would be $6!+ 7!$ . I feel like this is way too simple of a solution to be correct.",['combinatorics']
2917378,A first countable hemicompact space is locally compact,"Prove: A first countable hemicompact space is locally compact. A topological space $(X,\tau)$ is said to be hemicompact if it has a sequence of compact subsets $K_n$ , $n \in \mathbb{N}$ , such that every compact subset $C$ of $(X,\tau)$ satisfies $C \subseteq K_n$ , for some $n \in \mathbb{N}$ . In locally compact space each point $x$ has a compact nbhd $C$ , s.t. $x \in U \subseteq C$ , where $U$ is open. By first countability, there exists a monotonic decreasing sequence $U_n$ of open nbhds of point $x$ . By hemicompactness, there exists an increasing monotonic sequence $K_m$ of compact sets containing $x$ . My intuition tells me that $U_n \subseteq K_m$ , for some $n,m \in \mathbb{N}$ , and $K_m$ is a compact nbhd of $x$ , but I can't formalize this. Am I on the right path?","['general-topology', 'first-countable', 'compactness']"
2917380,Connected levels and polynomials submersions,"Is it true that a polynomial submersion $ p: \mathbb{R}^2 \to \mathbb{R}$ of degree $n$ has at most $n$ connected components on each level? I think I have a proof, can someone point me out any mistakes? Let $c\in \mathbb{R}$ . Since $p$ is a submersion, $p-c$ doesn't have a $(x^2+y^2-R)$ factor for any $R\in \mathbb{R}^+$ .
Also, each connected component of $p^{-1}(c)$ must intersect $x^2+y^2-R=0$ at least twice, for every $R$ big enough. By Bezout theorem, the system \begin{cases} p=c \\ x^2+y^2=R\end{cases} has at most $2n$ real solutions, hence $p^{-1}(c)$ has at most $n$ connected components.","['algebraic-curves', 'algebraic-geometry']"
2917409,Why does $\int_{-\pi}^{\pi}\ln(a^2+e^2+2ea\cos(\theta))d\theta = 4\pi$ only for $-e\leq a\leq e$,"I've been given this integral from a friend. 
$$\int_{-\pi}^{\pi}\ln(a^2+e^2+2ea\cos(\theta))d\theta = 4\pi$$  for $-e\leq a\leq e$. 
He said I could use complex analysis to solve it. I'm just starting to learn complex analysis and evaluating real integrals using it. I'm not entirely sure exactly what I'm doing, but I'll show you what I've got. Using $$\int_{\Gamma} \frac{\ln(z+e)}{z} dz=2\pi i$$ where $\Gamma$ is the circle with radius $a$ traveling counterclockwise around the origin. Letting $z=ae^{i\theta}$ $dz=aie^{i\theta}$ and $-\pi \leq\theta\leq\pi,$ $$2\pi=\int_{-\pi}^{\pi} \ln(ae^{i\theta}+e)d\theta$$ Rewriting this (I'm pretty sure) can be written as: $$\int_{-\pi}^{\pi}\frac{1}{2}\ln(a^2+e^2+2ea\cos(\theta))+i\arctan(\frac{a\sin(\theta)}{a\cos(\theta)+e})d\theta$$ Since that arctan part is odd it evaluates to zero so the only part that matters is the real part so, $$4\pi=\int_{-\pi}^{\pi}\ln(a^2+e^2+2ea\cos(\theta))d\theta$$ Why does the integral only converge to $4\pi$ when $-e\leq a\leq e$?","['integration', 'complex-analysis', 'calculus']"
2917439,"What is the conditional probability that the number of heads equals the number showing on the die, conditional on knowing that the die showed 1?","Suppose we flip two fair coins and roll one fair six-sided die. What is the conditional probability that the number of heads equals the number
showing on the die, conditional on knowing that the die showed 1? Let's define the following: $A=\{\text{#H = # on die}\}$ $B=\{\text{# on die = 1}\}$ We want to find: $P(A|B)=\dfrac{P(A\cap B)}{P(B)}=\dfrac{P(\{\text{#H = # on die}\} \cap \{\text{# on die = 1}\})}{P(\{\text{# on die = 1}\})}$ I drew a tree diagram to help me: Basically we toss two coins, and the final toss can match with either number on the dice. Now $P(B)=1/6=(1/2)(1/2)(1/6)(4)$ For the numerator, we want $\{\text{#H = # on die}\} \cap \{\text{# on die = 1}\}$. The more restrictive here is that the die must roll a 1: The orange show the only two paths that are the intersection. This equals $(2)(1/2)(1/2)(1/6)=1/12$ Therefore our $P(A|B)=(1/12)/(1/6)=(1/2)$ as our final answer. Is this correct? I'm wondering if it makes sense because the dice rolled a $1$, and there are only two options on a coin, that thus the 50/50.","['conditional-probability', 'statistics', 'probability']"
2917450,Reflections along root vectors in a basis generate the Weyl group?,"Let $\Phi$ be a root system of type ADE, $\Lambda$ be the lattice in the Euclidean space spanned by $\Phi$ . If $\Lambda$ is spanned by $\{v_i\}\subset \Phi$ (as a $\mathbb Z$ -module), and let $\sigma_i$ be the reflection along hyperplane associated to $v_i$ . Is it true that $\{\sigma_i\}$ generate the Weyl group $W(\Phi)$ ? My approach I know there is a proposition says reflections along vectors in a base (of a root system, or in some book called simple system) generate the full $W(\Phi)$ . So I tried to show $\{v_i\}$ contains a base, but have no idea how to do this. More precisely, I don't know how to use the condition $\{v_i\}$ generate $\Lambda$ . If this too general, what about the case $\Phi=E_6$ ? If $6$ vectors $v_1,\ldots,v_6$ in the root system generate the lattice, is it true that after possibally replacing $v_i$ by $-v_i$ for some $i$ , they will form a base of $E_6$ ?","['integer-lattices', 'root-systems', 'linear-algebra', 'lie-algebras']"
2917493,"$\mathbb{HP}^2$, exotic 7-spheres, and Bott manifolds","I am looking for some explanation how $\mathbb{HP}^2$ , exotic 7-spheres, and Bott manifolds are related? And how the construction of a Bott manifold is related to $\mathbb{HP}^2$ and exotic 7-spheres? p.s. The description I found is Kervaire-Milnors work on homotopy spheres and the description by Johannes Ebert . But this description is too dense and too quick to me, could you provide more details in a slow manner? Johannes Ebert said : ""A textbook reference is Kosinski: ''Differential manifolds''. In section IX.8 (Theorem 8.7), you find the statement that there exists an 8-dimensional manifold which is almost parallelizable (i.e. parallelizable away from a point) whose signature is $8 \cdot 28$ . Because this $M$ is almost parallelizable, $p_1 (TM)=0$ , and from the formulae for the $\hat A$ -class and the $L$ -class, you get that $\hat{A} =1$ . Typically, one wants that the signature is zero, and this you can achieve by connected sum with $8 \cdot 28$ copies of $\overline{HP^2}$ . How is $M$ constructed? You take the $E_8$ -plumbing manifold $V$ . It is a $3$ -connected $8$ -manifold which is parallelizable, which has signature $8$ and whose boundary is a homotopy sphere. In fact, $\partial V$ generates the group $\Theta_7 \cong Z/28$ of exotic $7$ -spheres. Now you form the boundary connected sum of $28$ copies of $V$ ; the boundary of the resulting manifold is the standard $S^7$ , and you glue in a copy of $D^8$ to obtain $M$ .""","['geometric-topology', 'surgery-theory', 'manifolds', 'differential-topology', 'differential-geometry']"
2917506,"Let $\alpha,\beta,\gamma$ be cardinal numbers where $\aleph_0\le\gamma$, $\alpha+\beta=\gamma$, and $\alpha<\gamma$. Then $\beta=\gamma$","Let $\alpha,\beta,\gamma$ be cardinal numbers. We define $\alpha+\beta=|S|$ , where $S=A\cup B$ with $|A|=\alpha$ , $|B|=\beta$ , and $A\cap B=\emptyset$ . If $\alpha\le\beta$ and $\aleph_0\le\beta$ , then $\alpha+\beta=\beta$ . If $\alpha+\beta=\gamma$ , $\aleph_0\le\gamma$ , and $\alpha<\gamma$ , then $\beta=\gamma$ . My attempt: Lemma: $\aleph_0\le\beta\implies \beta+\beta=\beta$ (I presented a proof here ) We first prove claim 1: We have $\beta\le\alpha+\beta\le\beta+\beta$ and $\beta+\beta=\beta$ by Lemma . Thus $\alpha+\beta=\beta$ . We next prove claim 2: We have $\alpha+\beta=\gamma$ , then $\beta\le\gamma$ . Assume the contrary that $\beta\neq\gamma$ , then $\beta<\gamma$ . If $\alpha\le\beta$ , then $\alpha+\beta=\beta$ by Claim 1. It follows that $\alpha+\beta=\beta<\gamma$ , which is a contradiction. If $\beta<\alpha$ , then $\alpha+\beta=\alpha$ by Claim 1. It follows that $\alpha+\beta=\alpha<\gamma$ , which is a contradiction. Hence $\beta=\gamma$ . Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help!","['elementary-set-theory', 'proof-verification']"
2917524,Calculate$\int\limits_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}}dx$ [duplicate],This question already has an answer here : Calculate $ \int_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}} dx $ (1 answer) Closed 5 years ago . Calculate $$\int\limits_{-2}^{0} \frac{x}{\sqrt{e^x+(x+2)^2}}dx$$ First I tried the substitution $t=x+2$ and obtained $$\int\limits_{0}^{2} \frac{t-2}{\sqrt{e^{t-2}+t^2}}dt$$ and than I thought to write it as $$\int\limits_{0}^{2} (t-2)\frac{1}{\sqrt{e^{t-2}+t^2}}dt$$ and use the fact that $$2(\sqrt{e^{t-2}+t^2})'=\frac{1}{\sqrt{e^{t-2}+t^2}} \cdot(e^{t-2}+2t)$$ Using integration by parts we get that we have to calculate (excluding some terms we know) $$\int\limits_{0}^{2} \sqrt{e^{t-2}+t^2}\cdot \frac{6e^{t-2}-2te^{t-2}+8}{(e^{t-2}+2t)^2}dt$$ which is uglier then the initial problem. Do you have any idea how to solve the problem?,"['integration', 'calculus', 'definite-integrals']"
2917554,Find the number of 5-member committees which include at least two Republicans,"There are 10 Republicans, 8 Democrats  and 2 Independent legislators eligible for committee membership. How many 5-member committees exist which include at least two Republicans? My  Work : $C(20,5)-(C(10,5)+C(10,4))$ $(\;All\; 5-member\; committees\;)\;-\;(\;committiees\; with \;no\; Republicans\;+\;committees\; with\; one \;Republican \;)$ Explanation: There are $C(20,5)$ 5-member committees which will be subtracted from sum of two numbers in order to achieve the answer; C(10,5) is number of 5-member committees which include no Republicans (I've only counted Democrats and Independents ($8+2$)then found number of 5-member committees ) and C(10,4) is number of 5-member committees with one Republican (Like before I've excluded Republicans but counted 4-member committees then  added one Republican to each subset) Book's Answer: It's just said $C(10,2)C(18,3)$","['binomial-theorem', 'combinatorics']"
2917582,Why $-\ln|\cos x| = \ln|\sec x|$ [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question I'm currently learning derivatives and antiderivatives. In a solution to one of the question, it mentions $$-\ln|\cos x| +C=\ln|\cos x|^{-1}+C= \ln|\sec x|+C$$ I am puzzled by what rules this transformation uses. Could someone please kindly point it out to me. Edit 1: I think my main confusion is the use of absolute value and natural log in one place.","['integration', 'derivatives']"
2917617,Proving there are as many generalized eigenvectors as algebraic multiplicity eigenvalue without the Jordan canonical form,"I'm reading some treatment of generalized eigenvectors in a differential equations book. They want to derive that there are as many generalized eigenvectors to a certain eigenvalue $\lambda_i$, as the algebraic multiplicity of this eigenvalue $\lambda_i$. This result is used to proof that there is a basis transformation putting the matrix in the Jordan canonical form. It goes as follows: let $p(x)=\prod_{i=1}^q(x-\lambda_i)^{m_i}$ be the characteristic polynomial of some $\mathbf{A}\in\mathbb{R}^{n\times n}$, thus $\sum_{i=1}^qm_i=n$, with $m_i$ the algebraic multiplicity of eigenvalue $\lambda_i$. Then, by Cayley-Hamilton, $$\mathbf{0}=\prod_{i=1}^q(\mathbf{A}-\lambda_i\mathbf{I})^{m_i},$$ and from here it is concluded that $S_i=\{\mathbf{v}:(\mathbf{A}-\lambda_i\mathbf{I})^{m_i}\mathbf{v}=\mathbf{0}\}$ is a vector subspace of dimension $m_i$. I don't get where this last assertion comes from. I have seen a (completely different) proof of the Jordan normal form theorem before. I would proof Cayley-Hamilton from the Jordan normal form theorem, and because eigensystems of similar matrices are equal, we can easily see that $S_i$ is a subspace of dimension $m_i$ using the Jordan normal form of $\mathbf{A}$. Can anyone explain me how this last conclusion is made without the Jordan normal form theorem?","['jordan-normal-form', 'generalized-eigenvector', 'linear-algebra']"
2917649,A way of understanding the $dx$,"In an attempt to explain the concept of the infinitesimal change, I have defined it as such : Given an interval of size $L$, we could express $L$ as follows :$L=\alpha.dx$ thus, theoretically, $\alpha = \frac{L}{dx}$ could be seen as the total number of $x$s since $dx$ is infinitesimally small. Do you think this to be a conceptually wrong explanation?","['calculus', 'derivatives']"
2917652,Gradient estimate for convex function,"Consider $\varphi:\mathbb{R}^n\rightarrow \mathbb{R}$ a convex, twice differentiable function with the gradient $\nabla \varphi$ Lipschitz-continuous. Suppose the function achieve a minimum in $\mathbb{R}^n$. We can write the following gradient system:
\begin{equation}
\begin{cases}
\dot{x}(t) = -\nabla\varphi(x(t))\\
x(0) = x_0 \in \mathbb{R}^n
\end{cases}
\end{equation}
From a much more general case it is known that the following estimate is true:
\begin{equation}
\|\nabla \varphi(x(t))\|\leq \frac{C}{t}
\end{equation}
for a certain constant $C$. I would like to prove this inequality without using the general case, but I haven't found anything yet. How can I prove it?","['derivatives', 'ordinary-differential-equations', 'real-analysis']"
2917657,About the universal property of initial and final topologies,"I have recently seen in my topology course that if $X$ is any set, Given a family of functions $\{f_i : X \to Y_i\}_{i \in I}$, with $(Y_i, \tau_i)_i$ topological spaces, the initial topology on $X$ with respect to this family is the one generated by $\{f_i^{-1}(U_i) : i \in I, U_i \in \tau_i \}$, which is the coarsest such that the functions $f_i$ are continuous, and verifies $h : Z \to X$ continuous if and only if $f_ih$ is continuous for all $i$ in $I$. In a similar way, given a family $\{f_i : Y_i \to X\}_{i \in I}$, the final topology $\tau$ in $X$ is defined by $U \in \tau $ if and only if $f_i^{-1}(U) \in \tau_i \ (\forall i \in I)$. This is the finest topology such that the family is countinuous, and $h : X \to Z$ is continuous if and only if $hf_i$ is for all $i \in I$. It was also mentioned that the reciprocal of both statements is true, i.e. that if a topological space $(X, \tau)$ verifies $h : Z \to X$ (resp. $h:X \to Z$) continuous if and only if $f_ih$ (resp. $hf_i$) continuous for all $i$, for all $h$, it has the initial (resp. final) topology with respect to the family $(f_i)_i$. It is easy to see taking $h \equiv id $ that the given topology $\tau$ is finer/coarser than the initial/final topology, because all the functions $f_i$ are continuous. Any hints on how to prove the other inclusion?",['general-topology']
2917663,"what does ""arc"" in arcsin, arccos, arctan stands for [duplicate]","This question already has answers here : Etymology of $\arccos$, $\arcsin$ & $\arctan$? (3 answers) Closed 1 year ago . I was just wondering, what does the ""arc"" in arcsin, arccos, arctan stands for? Is there any particular reason why it is named the way it is?",['trigonometry']
2917666,Minimum number of balanced partitions,"For any multiset $x_1,x_2,\ldots,x_{2n}$ of positive real numbers, a partition into two nonempty subsets $(A,B)$ is called ""balanced"" if $\text{sum}(A)\geq\text{sum}(B)-\max(B)$ and $\text{sum}(B)\geq\text{sum}(A)-\max(A)$. What is the minimum number of balanced partitions, in terms of $n$? For the case that all numbers are equal, a partition is balanced if and only if it puts $n$ numbers in each part. So there are $\binom{2n}{n}$ balanced partitions. I conjecture that this is also the minimum. The reason is that if the numbers are not equal, there is more ""advantage"" to be gained by subtracting the max, which should give more balanced partitions.","['binomial-coefficients', 'combinatorics', 'extremal-combinatorics']"
2917712,Integral $\int_0^1 \frac{x\ln\left(\frac{1+x}{1-x}\right)}{\left(\pi^2+\ln^2\left(\frac{1+x}{1-x}\right)\right)^2}dx$,"The goal is to show (preferably without contour integration, as my knowledge is pretty limited there, but if you can do it that way there is no problem to share it) that: $$I= \int_0^1 \frac{x\ln\left(\frac{1+x}{1-x}\right)}{\left(\pi^2+\ln^2\left(\frac{1+x}{1-x}\right)\right)^2}\mathrm dx=\frac{1}{240}$$ 
I am still trying to find more relevant approaches, but so far I thought of: Considering $$J(a)= \int_0^1 \frac{x\ln\left(\frac{1+x}{1-x}\right)}{a^2+\ln^2\left(\frac{1+x}{1-x}\right)}dx$$ We have that $J'(\pi)=-2\pi I$, also $$J(a)=\int_0^\infty e^{-at}\int_0^1 x\sin\left(t\ln\left(\frac{1+x}{1-x}\right)\right)dx dt $$ Which is nice, however I couldnt solve the inner integral. Another approach would be to let $\ln\left(\frac{1+x}{1-x}\right)=t \rightarrow x=\frac{e^t-1}{e^t+1}\rightarrow dx=\frac{2e^t}{(e^t+1)^2}$ 
$$I=2\int_0^\infty \frac{e^t(e^t-1)}{(e^t+1)^3}\frac{t}{(\pi^2+t^2)^2}dt$$ This doesnt look nice, but notice that $$\frac{d^2}{dx^2} \left(\frac{1}{1+e^x}\right)=\frac{e^x(e^x-1)}{(e^x+1)^3}$$ Integrating by parts two times I get $$I=\frac{1}{\pi^2} +24 \int_0^\infty \frac{x^2-\pi^2}{(x^2+\pi^2)^4}\frac{x}{1+e^x}dx$$ Now I am thinking to use series for $\frac{x}{1+e^x}$, but I dont know a series that satisfies it, similary we have for $\frac{x}{e^x-1}$ in terms of Bernoulli numbers. Could you land some help for this integral?","['integration', 'definite-integrals']"
2917716,To what extent are homeomorphisms just deformations?,"Background. It is often said that two spaces are homeomorphic if, roughly speaking, one space can be continuously deformed into the other without any tearing and gluing. It is then emphasized that this is more of a guiding principle than a solid fact. Indeed, a well-known 'counterexample' to this heuristic would be double-twisted Möbius strip $M$ , which is homeomorphic to the cylinder $C$ , but when we visualize the two spaces in $\mathbb{R}^3$ , we find that we cannot deform into each other without tearing. Why does this happen? In a sense this is because $\mathbb{R}^3$ does not have enough room for the desired deformation; there simply aren't enough dimensions to 'untwist' the double twist without tearing things up. This leads us to the following: Preliminary question. Suppose I were to embed $M$ and $C$ in $\mathbb{R}^n$ for some large $n$ . Can I now deform one into the other without tearing or gluing? The answer turns out to be yes, and in fact I believe $n = 4$ already suffices. If you care for the informal proof, read on; if not, scroll down to the main part. Let us start with a suitable embedding of a double twist. This picture visualizes an embedding of the double twist within $\mathbb{R}^4$ , where the first three dimensions are spatial, and the fourth dimension is represented by different colours. (The choice of colours may be unfortunate, but I did not have any other pens.) Now imagine moving the 'left strip' and the 'right strip' closer together, like so. Notice that the 'double twist' starts to resemble a loop. Now, if we move the strips yet closer, they will start to overlap spatially. But notice that the colours will remain different on the overlap so that no actual tearing or gluing occurs. Now keep going until the two strips fully overlap each other. At this point the colors have aligned, so that the twist has turned into a 'loop'. This loop is undesirable, but the solution to that issue is simple. During the proces in which we let the strips overlap, we simply let the size of the loop go to zero. At the very moment that the overlap is complete (and the colors have aligned), the size of the loop reaches, and as such the loop will vanish altogether. 'QED'. To our main question. We see that our counterexample is no longer a counterexample. We might therefore wonder if this is part of a general phenomenon. Let us make this more precise. Let $X$ be a topological space, and let $f : X\to \mathbb{R}^n$ and $g : X \to \mathbb{R}^n$ be two embeddings. Denote by $\varepsilon_{n,m} : \mathbb{R}^n \to \mathbb{R}^{n+m}$ the embedding which sends a point $x$ to $(x,0)$ . Let's say $f$ can be deformed by embeddings into $g$ if there exists an $m$ and a homotopy from $H : X \times I \to \mathbb{R}^{n+m}$ from $\varepsilon_{n,m} \circ f$ to $\varepsilon_{n,m} \circ g$ such that for all $t\in [0,1]$ , the map $H_t : X \to \mathbb{R}^{n+m}$ is a continuous embedding. Question. Suppose $X$ is a reasonable space (say, a compact manifold with boundary). Let $f$ and $g$ be two embeddings of $X$ into some $\mathbb{R}^n$ . Can $f$ always be deformed by embeddings into $g$ ? This question can be generalized and specialized in many ways, so feel free to change some of its details.","['geometry', 'geometric-topology', 'general-topology', 'low-dimensional-topology', 'algebraic-topology']"
2917742,Find Order And Degree of a Differential Equation,$\left\{ 1 + \left( \frac { d y } { d x } \right) ^ { 2 } \right\} ^ { \frac { 3 } { 2 } } = \frac { d ^ { 2 } y } { d x ^ { 2 } }$ what is the degree and order for above equation well according to my knowledge the order be should $2$ and degree should be $\frac { 3 } { 2 }$ is my answer right or wrong ?,"['differential', 'ordinary-differential-equations']"
2917820,Can the long line be embedded in the ordered plane?,"It is a well known result that the long line (namely, the topological space $S_\omega \times [0, 1)$ in the order topology, where $S_\omega$ is the minimal uncountable well-ordered set) cannot be embedded in Euclidean space of any dimension. Is it possible, however, to embed the long line in $\Bbb{R}^2$ in the lexicographic order topology? What about the ordered square? My intuition about this space is that it is composed of an ""uncountable number of real lines"" which makes me conjecture it could be possible to embed it in such a space; as it stands, I'm not yet able to give a rigorous proof of the matter - or the opposite.","['general-topology', 'order-topology']"
2917887,Do Arithmetic Mean and Geometric Mean of Prime Numbers converge?,"I was looking at a list of primes. I noticed that $ \frac{AM (p_1, p_2, \ldots, p_n)}{p_n}$ seemed to converge. This led me to try $ \frac{GM (p_1, p_2, \ldots, p_n)}{p_n}$ which also seemed to converge. I did a quick Excel graph and regression and found the former seemed to converge to $\frac{1}{2}$ and latter to $\frac{1}{e}$. As with anything related to primes, no easy reasoning seemed to point to those results (however, for all natural numbers it was trivial to show that the former asymptotically tended to $\frac{1}{2}$). Are these observations correct and are there any proofs towards: $$
{
\lim_{n\to\infty} \left( \frac{AM (p_1, p_2, \ldots, p_n)}{p_n} \right)
= \frac{1}{2} \tag1 
}
$$ $$
{
\lim_{n\to\infty} \left( \frac{GM (p_1, p_2, \ldots, p_n)}{p_n} \right)
= \frac{1}{e} \tag2
}
$$ Also, does the limit $$
{
\lim_{n\to\infty} \left( \frac{HM (p_1, p_2, \ldots, p_n)}{p_n} \right) \tag3
}
$$ exist?","['number-theory', 'elementary-number-theory', 'prime-numbers']"
2917904,Let $A$ be a set. Define a set B such that $|A|=|B|$ and $A\cap B=\emptyset$,"Let $A$ be a set. Define a set B such that $|A|=|B|$ and $A\cap B=\emptyset$. My attempt: Let $B=\{\{A\}\cup a\mid a\in A\}$. It's clear that $|A|=|B|$. Next we prove $A\cap B=\emptyset$. If $A\cap B\neq\emptyset$, then there exists $c$ such that $c\in A$ and $c\in B$. Since $c\in B$, then $A\in c$. Thus $A\in c\in A$, which contradicts Axiom of Regularity. Hence $A\cap B=\emptyset$. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help!",['elementary-set-theory']
2917939,Hilbert-Chow morphism $Hilb^2(C)\to Sym^2(C)$ for $C$ a singular curve,"I would like to know if there is anything known about the Hilbert-Chow morphism for the case of a singular curve. Due to Fogarty, for any number $n$ and any smooth projective variety $X$, there is a morphism $$ \alpha: Hilb^n(X)\to Sym^n(X),\; Z\mapsto \sum\limits_{p\in X}l_p(Z)\cdot p, $$ where $l_p(Z)$ denotes the length of the subscheme $Z$. In more geometric terms, this is saying that $Hilb^n(X)$ arises as blow-up of $Sym^2(X)$ along the diagonal. What happens, if $n=2$ and $X=C$ a singular curve? Since I just want to get some intuition about what might happen for singular curves, feel free to only say something about $C$ being a nodal curve. I am also thankful to any kind of reference and/or suggestions for reading.",['algebraic-geometry']
2917962,Use of implication vs. such that.,"Given any two rational numbers $a$ and $b$, it follows that $ab$ is rational. $\forall$$a$,$b$ $s.t.$ (($a$,$b$ $\in$ $\mathbb{Q}$) $\implies$ ($a$$b$ $\in$ $\mathbb{Q}$)) But I've seen it written: $\forall$$a$,$b$ $\in$ $\mathbb{Q}$ $s.t.$ ($a$$b$ $\in$ $\mathbb{Q}$)","['elementary-set-theory', 'predicate-logic']"
2917982,Is this domain Lipschitz? Does Poincaré–Wirtinger inequality apply?,"Suppose $R_2 > R_1 > 0$ and consider $\Omega \subset \mathbb{R}^2$ such that $$
\Omega:=\left(B_{R_2}(0)\setminus B_{R_1}(0)\right)\setminus L 
$$
where $L:= \{(x,y)\in\mathbb{R}^2\,|\,x < 0, y = 0\}$. Is this domain Lipschitz? I have little background in this area, but having looked at the definition of a Lipschitz domain here ,it seems to me that it is isn't, since e.g. if we take $p = (-R',0) \in \partial\Omega$ where $R' = \frac{R_1+R_2}{2}$ and consider any possible $C$ (as in the definition given there) we cannot satisfy what is being said about $\Omega\cap C$. In the end I would like to apply Poincaré–Wirtinger inequality which is defined here for $p=2$ as
$$
\|u-u_{\Omega}\|_{L^2(\Omega)} \leq C \|\nabla u\|_{L^2(\Omega)},
$$
where $u \in W^{1,2}(\Omega)$.
In the setup I am working with I specifically exclude $L$ so across it the function is allowed to jump (which would exclude it from the space if $\Omega$ included $L$). Am I correct in thinking that thus the inequality applies? My hand-wavy intuition is that in fact for any $x\in L \cap\left(B_{R_2}(0)\setminus B_{R_1}(0)\right)$ we define 'traces' from above ($x^+$) and below ($x^-$) as something like
$$
u(x^\pm) = \lim_{R\to 0^\pm}u(x_1,x_2+R)
$$
and thus in some sense these are 'separate' boundaries when looking from above and from below. Sadly I have next to no knowledge of Measure Theory and would appreciate if somebody could clear my doubts. Thank you!","['measure-theory', 'partial-differential-equations', 'lipschitz-functions', 'real-analysis']"
2918029,functional $F(a) = \int_{0}^{\infty}e^{-kx}\operatorname{ln}(a(x))dx$ maximization,"Good afternoon. I try to find  a function $a(x)>0$ subject to $\int_0^\infty a(x) dx=1$ maximzing the following functional
$$
F(a) = \int\limits_{0}^{\infty}e^{-kx}\operatorname{ln}(a(x))dx.
$$
But I don't know how to do this problem. Because formally we should find a derivate with respect to function. I will be gratefull for hints ideas and literature recomendation.","['optimization', 'maxima-minima', 'functional-analysis', 'real-analysis']"
2918031,Why should one use the quotient rule instead of the power rule to differentiate a quotient?,"There's a lot of emphasis on the difference quotient, as it's on the AP test and all that, but honestly using the power rule plus the product rule is soo much easier and gives you the same answer and slope. What am I missing here? For example, if you wanted to differentiate 
$$f(x)=\frac{x^2+4x-2}{x-1},$$
you could either do 
$$f'(x)=\frac{d}{dx}[(x^2+4x-2)(x-1)^{-1}]=(2x+4)(x-1)^{-1}-(x^2+4x-2)(x-1)^{-2}, $$
or you could do
$$
f'(x)=\frac{(x-1)(2x+4)-(x^2+4x-2)(1)}{(x-1)^2}.
$$","['calculus', 'derivatives']"
2918063,"Are ""most"" sets in $\mathbb R$ neither open nor closed?","It seems intuitive to believe that most subsets of $\mathbb R$ are neither open nor closed. For instance, if we consider the collection of all (open, closed, half-closed/open) intervals, then one can probably make precise the notion that ""half"" of all intervals in this collection are neither open nor closed. (Whether this will amount to a reasonable definition of what it means for most subsets to be neither open nor closed might be up for debate.) If this intuition is correct, is there a way to formalise it? If not, how would we formalise its being wrong? To be clear, I am happy for a fairly broad interpretation of the term ""most"". Natural interpretations include but are not limited to : Measure-theoretic (e.g. is there a natural measure on (a $\sigma$-algebra on) the power set of $\mathbb R$ that assigns negligible measure to $\tau$?) Topological (e.g. is there a natural topology on the power set of $\mathbb R$ where $\tau$ is meagre, or even nowhere dense?) Set-theoretic (e.g. does the power set of $\mathbb R$ have larger cardinality than $\tau$?) Here, $\tau$ is (obviously) the Euclidean topology. Actually, that last version of the question in parentheses might have the easiest answer: Let $\mathcal B$ be the Borel sets on $\mathbb R$. We have that $|\tau| \le | \mathcal B | = | \mathbb R | < \left| 2^{\mathbb R} \right|$. (For details on the equality, see here . For a much simpler proof, see this answer .) Are there alternative ways to make this precise?","['real-numbers', 'measure-theory', 'descriptive-set-theory', 'real-analysis', 'general-topology']"
2918196,Weierstrass approximation theorem. Approximation of |x|.,"I'm studying a proof of the Weierstrass approximation theorem that requires an uniform approximation using polynomials of the function |x| in the interval $[-1,1]$ i.e. we need a sequence of polynomials that converge to |x| in the supreme norm. One way to do this is to define for $t\in[0,1]$ the following sequence: First set $P_0(t)=0$ for all t, and then using induction define $$P_{n+1}(t)=P_n(t)+\frac{1}{2}(t-P_n(t)^2).$$ The next step would be to prove using induction that for all $t\in[0,1]$, $$0\leq\sqrt{t}-P_n(t)\leq\frac{2\sqrt{t}}{2+n\sqrt{t}},$$ but i have not been able to do this. Can someone help me???","['weierstrass-approximation', 'approximation-theory', 'absolute-value', 'analysis']"
2918219,Partial Derivative of a double finite summation.,"How might i go about finding the derivative the following double sum $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\:$$ My inital idea was to end put the summation terms and $a_{kj}$ outside of the derivative, and then look for special cases where i=j i=k and i=j=k. However i'm quite stuck in the way of expressing it. Basically this is as far as i got. $$\frac{\partial }{\partial x_i}\left(\sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}x_kx_j\right)\: = \sum _{k=1}^n\:\sum _{j=1}^n\:a_{kj}\frac{\partial \:}{\partial \:x_i}\left(x_kx_j\right)\:$$","['partial-derivative', 'calculus', 'derivatives', 'summation']"
2918223,Limit of quotient,"I recently learned that when you are solving for the limit of a quotient, you have to divide everything by the highest number in the denominator, like this $$ \lim_{x \to \infty} \frac{\sqrt{4 x^2 - 4}}{x+5} = \lim_{x \to \infty} \frac{\sqrt{4 - \frac{4}{x^2}}}{1 + \frac{5}{x}} = 2.$$ But I don't quite understand how when $x \to - \infty$ , the answer changes to $-2$, since when you divide everything by the highest power, it gives 
$$\frac{\sqrt{4-\frac{4}{x^2}}}{1+\frac{5}{x}},$$ meaning that even if you put negative infinity in the place of $x$, it still only gives $0$ and leaves $2$ as the final answer. Why does the answer change to $-2$? I understand that it must be $-2$ when I look at the graph, I just don't understand the algebra part of it.",['limits']
2918232,"MLE of $\mu$ given $X_1\sim \mathsf N(\mu, 4)$ and $X_2\sim \mathsf{N}(\mu, 16)$","Let $X_1\sim \mathsf N(\mu, 4)$ and $X_2\sim \mathsf{N}(\mu, 16)$
  where $X_1$ and $X_2$ are independent.  Find the maximum likelihood
  estimator $\hat{\mu}$ of $\mu$ if it exists. We have $$\begin{align*}
L(\mu\mid x_1, x_2)
&=\frac{1}{\sqrt{8\pi}}e^{-\frac{(x_1-\mu)^2}{8}}\cdot \frac{1}{\sqrt{32\pi}}e^{-\frac{(x_2-\mu)^2}{32}}\\\\
&=\frac{1}{\sqrt{8\pi}\sqrt{32\pi}}e^{-\frac{(x_1-\mu)^2}{8}-\frac{(x_2-\mu)^2}{32}}
\end{align*}$$ Then $$logL(\mu\mid x_1, x_2)=log\left(\frac{1}{\sqrt{8\pi}\sqrt{32\pi}}\right)-\frac{(x_1-\mu)^2}{8}-\frac{(x_2-\mu)^2}{32}$$ Then we set $$\frac{\partial log(L)}{\partial\mu}=\frac{1}{4}(x_1-\mu)+\frac{1}{16}(x_2-\mu)=0$$ Solving for $\mu$ I get that $\hat{\mu}=\frac{4}{5}x_1+\frac{1}{5}x_2$ This doesn't satisfy my intuition. I would have thought $\hat{\mu}=\frac{x_1+x_2}{2}$ Checking that this is in fact a maximum, I get that $$\frac{\partial}{\partial\mu}\left(\frac{1}{4}(x_1-\mu)+\frac{1}{16}(x_2-\mu)\right)=-\frac{5}{16}\lt0\text{ } \checkmark$$ Is this a valid solution? Edit: I think the MLE makes sense actually. It gives more weight to the random variable with a lower variance.","['probability-theory', 'probability', 'maximum-likelihood']"
2918309,"If I subtract 1 from the n,n entry of a Pascal Matrix, why does the determinant become zero?","In problem 19.2 , how did the author reach this statement: ""Since the n, n entry multiplies its cofactor positively, the overall determinant drops by 1 to become 0."" I was able to solve it a different way, but I feel that my approach was less efficient. I'd like to understand the solution's approach.
Here is what I did: $det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&20\end{bmatrix}) - det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix})$ . and $ det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\0&0&0&1\end{bmatrix}) = (-1)^3 * det(\begin{bmatrix}0&0&0&1\\1&1&1&1\\1&2&3&4\\1&3&6&10\end{bmatrix}) = (-1)^3 * (-1) * det(\begin{bmatrix}1&1&1\\1&2&3\\1&3&6\end{bmatrix})$ . therefore $det(\begin{bmatrix}1&1&1&1\\1&2&3&4\\1&3&6&10\\1&4&10&19\end{bmatrix}) = 1 - 1 = 0 $ . It feels like the solution skipped doing this step, I don't get it.","['matrices', 'determinant', 'linear-algebra']"
2918323,Does representation irreducibility ensure non-zero determinant?,"If a set of matrix representation $\{M(g)\}$ for a group $G$ is irreducible, what can we say about their determinant for every $g\in G$? Are they all of non-zero determinant? Thank you very much! Cheers,
Collin P.S.: I'm a physics graduate student. So please use as little math terminology as possible, I would really appreciate that!","['matrices', 'group-theory', 'determinant', 'finite-groups']"
2918359,A group of order $36$ has either a normal Sylow $2$-subgroup or a normal Sylow $3$-subgroup,Let $G$ be a group of order $36$ and given that $G$ has $4$ Sylow $3$-subgroups. Then  I have to  show that Sylow $2$-subgroup is unique. So far we proved that $G$ has a normal subgroup of order $3$. Couldn't solve that it has a unique Sylow $2$-subgroup. Help me. Thanks.,"['group-theory', 'abstract-algebra', 'finite-groups', 'sylow-theory']"
2918366,$\int_0^\frac{\pi}{2}\frac{\ln(\sin(x))\ln(\cos(x))}{\tan(x)}dx$,"I have the problem below:
$$\int_0^\frac{\pi}{2}\frac{\ln(\sin(x))\ln(\cos(x))}{\tan(x)}dx$$
I have tried $u=\ln(\sin(x))$ so $dx=\tan(x)du$ so the integral becomes:
$$\int_{-\infty}^0u\ln(\cos(x))du$$
but I cannot find a simple way of getting rid of this $\ln(\cos(x))$ I also tried using the substitution $v=x-\frac{\pi}{2}$
so the integral becomes:
$$\int_{-\frac{\pi}{2}}^0\frac{\ln(\cos(v))\ln(\cos(v+\frac{\pi}{2}))}{\tan(v+\frac{\pi}{2})}dv$$
but this does not seems to lead anywhere useful EDIT to follow up $$B(\alpha,\beta)=\int_0^{\pi/2}\sin^{\alpha-1}(x)\cos^{\beta+1}(x)dx$$
$$\frac{\partial^2}{\partial_\alpha\partial_\beta}B(\alpha,\beta)=\int_0^{\pi/2}\sin^{\alpha-1}(x)\cos^{\beta+1}(x)\ln(\sin(x))\ln(\cos(x))dx$$
so I see that when $\alpha\to1$ and $\beta\to-1$ that this is the form that we want, so do we now have to take partial derivates of the non-integral form of the beta function then take the double integral for our chosen values?",['integration']
2918374,Rank of an NxN matrix,"This question very similar to this one . I have this problem I'm working on. I think I have it figured out but I just want to make sure I am not misunderstanding how rank works. The question goes like this, Consider an nXn matrix where the elements go from 1 to $ n^2 $ as you read across and then down.  For instance a 3X3 matrix where n = 3 looks like this: $ \left( \begin{array}_
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9 \end{array} \right)$ What is the rank of the nXn matrix, as a function of n, for n>=2? Drawing out the matrices for n=2 to n=5 I can spot a pretty obvious pattern in the matrix construction and the whole thing can be written in terms of n: $ \left( \begin{array}_
1 & 2 & 3 & ... & n  \\
n+1 & n+2 & n+3 & ... & 2n  \\
2n+1 & 2n+2 & 2n+3 & ... & 3n  \\
3n+1 & 3n+2 & 3n+3 & ... & 4n  \\
... &... &... & ... & ... \\
n^2-n+1  & n^2-n+2 & n^2-n+3 & ... & n^2 \end{array} \right)$ From this I can spot that as soon as you get past the 3rd row, you can sum the prior rows to cancel out that row. For instance take row 4 from above for the first column.
If I take $ row 3 + row 2 - row 1 $ I get this value $ (2n+1) + (n+1) - (1) = 3n+1 $ Which is the next rows value in row 4. And that can be carried out on the rest of the columns to produce the same result. So for any arbitrary row k beyond row 3, I can make that row all zeros by doing a row reduction of: R k -> R k - R k-1 - R k-2 + R k-3 Thus the maximum rank this matrix can have is 3 correct? Is this a reasonable way to solve this problem or there a more direct approach I should be taking?","['matrices', 'matrix-rank', 'linear-algebra']"
2918397,"conditional probability: game with three different bags, balls in bags","We play a game with three bags of balls, where the game is best of five, and one wins when they select three winning balls. A bag is selected at the beginning of a game, and it does not change between selections within games. There are three bags described as follows, and selecting each is equally likely. • The ""lucky"" bag, where selecting a winning ball has a 3/4 chance. • The ""impartial"" bag, where selecting a winning ball has a 1/2 chance. • The ""unlucky"" bag, where selecting a winning ball has a 1/4 chance. Answer the following questions: What is the probability that the game ends in the first three consecutive draws? This seemed to mean you select three winning balls in a row or three losing balls. By the law of total probability:$$
P(\text{Game ends in 3 draws}) = \frac{1}{3}(\frac{3^3}{4^3} + \frac{1^3}{2^3} + \frac{1^3}{4^3}) + \frac{1}{3}(\frac{1^3}{4^3} + \frac{1^3}{2^3} + \frac{3^3}{4^3}) = \frac{3}{8}$$ What is the probability that the you lose the game? This seems to be the probability that you draw exactly three losing balls (regardless of the game ending before five balls are drawn, but this is where I'm confused). Again by the law of total probability: $$
P(\text{Lose the game}) = \frac{1}{3}(\frac{1^3}{4^3} + \frac{1^3}{2^3} + \frac{3^3}{4^3}) = \frac{3}{16}$$ What is the probability that you selected one winning ball and three losing balls? By the law of total probability: $$
P(\text{1 winning ball, 3 losing balls}) = \frac{1}{3}(\frac{1^3}{4^3}\cdot\frac{3}{4} + \frac{1^4}{2^4} + \frac{3^3}{4^3}\cdot\frac{1}{4})$$ Given that you selected one winning ball and three losing balls, what is the probability that you were drawing from the ""lucky"" bag? By the definition/formula for conditional probability, seems like this could be $$\frac{1}{4^3}\frac{3}{4}$$ divided by the answer to the part right above (i.e. the probability that we draw one winning ball and three losing ones).","['conditional-probability', 'statistics', 'probability-theory', 'probability']"
2918449,(Multivariable Calculus) Convert $\rho = \sin \phi$ to cylindrical and rectangular,"Question: Consider the surface given in spherical coordinates by $\rho = \sin(\phi)$. Convert to rectangular coordinates and cylindrical coordinates. Identify the surface. By graphing the function, I've found that it is a horn torus (circles in cross section are tangent to each other). Using some conversion formulas, I got this: $$r = \sin^2(\phi)$$
$$\theta = \theta$$
$$z = \frac{\sin(2\phi)}{2}$$ And then for rectangular (using those cylindrical values): $$x = \sin^2(\phi)\cos(\theta)$$
$$y = \sin^2(\phi)\sin(\theta)$$
$$z = \frac{\sin(2\phi)}{2}$$ I would  say this is wrong as I'm probably supposed to get it in terms of $(r, \theta, z)$, and then also into $(x, y, z)$. Unless this is correct? Thanks for the reading.","['cylindrical-coordinates', 'multivariable-calculus', 'proof-verification', 'spherical-coordinates']"
2918467,Proof Verification $\sum_{k=0}^n \binom{n}{k} = 2^n$ (Spivak's Calculus),"$$\sum_{k=0}^n \binom{n}{k} = 2^n$$ I'll use induction to solve prove this.  Then $$\sum_{k=0}^n \binom{n}{k} = 2^n = \binom{n}{0} + \binom{n}{1} + ... + \binom{n}{n - 1} + \binom{n}{n}$$ First prove with n = 1 $$\binom{1}{0} + \binom{1}{1} = 2^1$$ Since $$\binom{1}{0} = \binom{1}{1} = 1$$ it's true. Now suppose that is true with $n$ if is true with $n + 1$ Then, multiply both sides by two $$2(2^n) = 2(\binom{n}{0} + \binom{n}{1} + ... + \binom{n}{n - 1} + \binom{n}{n})$$ $$2^{n+1} = 2\binom{n}{0} + 2\binom{n}{1} + ... + 2\binom{n}{n - 1} + 2\binom{n}{n}$$ $$2\binom{n}{0} + 2\binom{n}{1} + ... + 2\binom{n}{n - 1} + 2\binom{n}{n} = \binom{n}{0} + \binom{n}{0} + \binom{n}{1} + \binom{n}{1} + ... + \binom{n}{n} + \binom{n}{n} $$ The first term have two equal term, then, you sum the last one with the first one of the next term, and you'll get this $$\binom{n}{0} + \binom{n}{1} +... + \binom{n}{n-1} + \binom{n}{n}$$ If we use this equation (Already proved) $$\binom{n}{k-1} + \binom{n}{k} = \binom{n+1}{k} $$ Of course, we'll have two term without sum, one $\binom{n}{0}$ and $\binom{n}{n}$ We can write these two term like this $$\binom{n}{0} = \binom{n}{n} = \binom{n+1}{0} = \binom{n+1}{n+1}$$ Then, we get $$2^{n+1} = \binom{n+1}{0} + \binom{n+1}{1} + ... + \binom {n+1}{n+1}$$ And it's already proved. Note: Just if we take $0! = 1$ I have to prove these too. $\sum_{k}^n \binom{n}{m} = 2^{n-1}$ If $m$ is even. And 
$\sum_{j}^n \binom{n}{j} = 2^{n-1}$ If $j$ is odd. Then, I just said that If 
$$\sum_{m}^n \binom{n}{m} + \sum_{j}^n \binom{n}{j} = \sum_{k=0}^n \binom{n}{k} $$ Then $$2^{n - 1} + 2^{n - 1} = 2^n$$ Which is true, then, I already prove this. And I have a last one. $$\sum_{i=0}^n (-1)^i\binom{n}{i} = 0$$ if n is odd. Then $$\binom{n}{0} - \binom{n}{1} + ... + \binom{n}{n-1} - \binom{n}{n} = 0$$ And that's can be solve knowing that $$\binom{n}{k} = \binom{n}{n - k}$$ And if n is even $$\binom{n}{0} - \binom{n}{1} + ... - \binom{n}{n-1} + \binom{n}{n} = 0$$ That means that every negative term if when n is odd, then, we can use our two last prove to prove it If
$$\sum_{m}^n \binom{n}{m} - \sum_{j}^n \binom{n}{j} = 0$$ Then $$2^{n-1} - 2^{n-1} = 0$$ Which is true. And that's it, I want to know if my proves are fine and are rigorous too and what is the meaning of every combinatorics prove . I want to know too better approaches to prove these (Or forms more intuitive)","['proof-explanation', 'proof-writing', 'combinatorics', 'binomial-coefficients']"
2918470,Upper bound of sub-gaussian norm of bounded random variable?,"I am reading the High-Dimensional Probability by Dr.Roman Vershynin , where I stuck on some statement at page 28. where state as below: Any bounded random variable $X$ is sub-gaussian with: $$\newcommand\norm[1]{\left\lVert#1\right\rVert}
\norm{X}_{\psi_2}\leq \frac{\norm{X}_{\infty}}{\sqrt{\log2}}
$$ where $\newcommand\norm[1]{\left\lVert#1\right\rVert}
\norm{X}_{\psi_2}$ is the sub-gaussian norm define as: $$\newcommand\norm[1]{\left\lVert#1\right\rVert}
\norm{X}_{\psi_2} =\inf \left\{ t>0 : \mathbb{E} \left[\exp{\left(\frac{X^2}{t^2}\right)} \right]   \leq 2 \right\}
$$ where $\newcommand\norm[1]{\left\lVert#1\right\rVert}
\norm{X}_{\infty} :=( \mathbb{E} |X|^p)^{1/p}$ as $p \to \infty$ I can see how why the  bounded random variable is sub-gaussian (hoeffing lemma ),but How could I see this upper bound of sub-gaussian  norm?","['statistics', 'probability', 'random-variables']"
2918511,Are there other self-similar functions like $e^x$ and $\cos x$? [duplicate],"This question already has answers here : Find $f$ where $f'(x) = f(1+x)$ (3 answers) Closed 5 years ago . (counting $\sin x$ as a variation of $\cos x$). They are self-similar in that their derivative is also a function of themselves. Crucially, this sort of feedback means that higher derivatives are also similar, resulting in curves that are smooth in what seems to me to be a unique way. For the examples in the title: $f'(x) = f(x)$ is true for $f(x)=e^x$ $f'(x) = f(x+\pi/2)$ is true for $f(x)=\cos x$ EDIT: That's for the two functions given in the title. The question is: are there any other functions that are similar to their derivative in some other way? Just one example is enough (the ""duplicate"" doesn't address this). BTW It seems to me that these are the only solutions to these differential equations, and e.g. a different offset for the second one just changes the period of the $\cos$ function. But I don't see how to show this - or even how to think about it. BTW this was inspired by Euler's equation, relating $\cos, \sin,$ and $e$. I think the fundamental connection is that they all are self-similar, and the rest is clever bit-twiddling (like how integers alternate even/odd, and $f(x) = (-1)^x$ alternates positive/negative).","['algebra-precalculus', 'derivatives', 'ordinary-differential-equations']"
2918535,"In non-standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points?","For standard analysis, an answer (in the concept of limit) is here . However, when dealing with infinitesimals in non-standard analysis, that limit technique will not work. So in non standard analysis, should we necessarily consider derivative as slope between two infinitesimally apart points? Or should we still consider it as slope at ""a point in hyper real number line $(\varepsilon)$""? If the latter is true, please give a reason.","['calculus', 'derivatives', 'nonstandard-analysis']"
2918540,Showing Probability Measure is Nonatomic,"Here is the problem I am working on: Let $(\Omega, \mathcal{F}, P)$ be a probability space, and suppose there are independent events $A_1, A_2,...$ such that if $ \alpha_n = \min\{P(A_n), 1 - P(A_n)\}$ then $\sum \alpha_n = \infty$. Show that $P$ is nonatomic. Here's what I have so far: Set $B_n$ equal to whichever of $A_n$ or $A_n^c$ has smaller probability. Then $P(B_n = \alpha_n.$ Then
$$
P(B_1 \cap \cdots \cap B_n) = \prod \alpha_j \leq \prod (1 - \alpha_j)
\leq e^{\sum \alpha_n} \rightarrow 0.
$$ I have a hint that says I should consider sets of the form 
$$
\{ \omega: \sum_n I_{A_n}(\omega) 2^{-n} \leq x \}
$$
where $I$ is the indicator function of the set  $A_n$. I think the idea is to then show that for any set $B$ the function $P(B \cap A_x)$ is continuous in $x$, which shows that the measure is non-atomic (by the Intermediate Value Theorem), but I don't see how to do this. Could someone help me show this?","['measure-theory', 'probability-theory', 'real-analysis']"
2918566,$p$-adic integers $\mathbf{Z}_p$ and $\mathbf{Z}_q$ homeomorphic for $p \neq q$?,"Obviously, $\mathbf{Z}_p \not\cong \mathbf{Z}_q$ ($p$-adic integers: $\mathbf{Z}_p = \varprojlim_n\mathbf{Z}/p^n$) for $p \neq q$ as (topological or abstract) groups, but are they homeomorphic as topological (profinite) spaces?",['general-topology']
2918636,Solving homogeneous differential equation $\frac{dy}{dx}=\frac{x^2+8y^2}{3xy}$.,"Solve the differential equation. Use the fact that the given equation is homogeneous
  $$
\frac{dy}{dx}=\frac{x^2+8y^2}{3xy}
$$ First I multiply the right side by
$$
\frac{\frac{1}{x^2}}{\frac{1}{x^2}}
$$
Then
$$\frac{dy}{dx}=\frac{1+\frac{8y^2}{x^2}}{\frac{3y}{x}}
$$
$$
Let: v=\frac{y}{x}
$$ $$
\frac{dy}{dx}=v+x\frac{dv}{dx}
$$
Then substitute
$$
\frac{dy}{dx}=\frac{1+v^2}{3v}$$
$$
\frac{1+v^2}{3v}=v+x\frac{dv}{dx}$$ How do I find the solution from here?",['ordinary-differential-equations']
2918643,$\Bbb N\times\Bbb N$ is countably infinite,"$\Bbb N\times\Bbb N$ is countable. In this proof, I try to not use The Fundamental Theorem of Arithmetic, so the proof may be needless long :) My attempt: We define $f:\Bbb N\times\Bbb N\to\Bbb N$ by $$f(i,j)=i+\sum\limits_{k=0}^{i+j}k$$ Next we prove that $f$ is injective. For $(i_1,j_1),(i_2,j_2)\in \Bbb N\times\Bbb N$, $f(i_1,j_1)=f(i_2,j_2)\iff i_1+\sum\limits_{k=0}^{i_1+j_1}k = i_2+\sum\limits_{k=0}^{i_2+j_2}k$. Without loss of generality, we can assume that $i_1+j_1\le i_2+j_2$. Thus we have only two cases. $i_1+j_1 = i_2+j_2$ Then $\sum\limits_{k=0}^{i_1+j_1}k = \sum\limits_{k=0}^{i_2+j_2}k$ and thus $i_1=i_2$. It also follows that $j_1=j_2$. Hence $(i_1,j_1)=(i_2,j_2)$. $i_1+j_1 < i_2+j_2$ $i_1+\sum\limits_{k=0}^{i_1+j_1}k = i_2+\sum\limits_{k=0}^{i_2+j_2}k \iff \sum\limits_{k=0}^{i_2+j_2}k - \sum\limits_{k=0}^{i_1+j_1}k = i_1-i_2\iff \sum\limits_{k = i_1+j_1+1}^{i_2+j_2}k=i_1-i_2$. $\sum\limits_{k = i_1+j_1+1}^{i_2+j_2}k \ge i_1+j_1+1 \iff i_1-i_2\ge i_1+j_1+1 \iff 0\ge -i_2\ge j_1+1$. This inequality is clearly a contradiction. To sum up, we have only one possible cases in which $i_1+j_1 = i_2+j_2$. Thus $f(i_1,j_1)=f(i_2,j_2)\iff (i_1,j_1)=(i_2,j_2)$. Hence $f$ is injective and $\Bbb N\to\Bbb N$ is countable. Does this proof look fine or contain gaps? Do you have suggestions? Many thanks for your dedicated help!","['elementary-set-theory', 'proof-verification']"
2918671,Intuition behind a random variable being a measurable function with a specific example.,"I'm just starting to learn measure theory and we have defined a random variable $X$ as a measurable function from some set $\Omega$ to some other set $E$ which is usually $\mathbb{R}$. We defined $\mathbb{P}(X \in S)$ for $S$ measurable in $E$ as being $\mu(\{\omega: X(\omega) \in S\})$ where $\mu$ is the probability measure on $\Omega$. I'm struggling to understand how this relates to the ""usual"" pre-measure theory definition of a random variable. For example if we had a standard Gaussian R.V what would $\Omega$ be and what would the map $X$ be? My intuitions tell me we should take $\Omega$ to be $(0,1)$ with the standard measure and then $X$ sends $\omega \in \Omega$ to $\inf\limits_{x \in \mathbb{R}} \{x: \omega \leq F(x)\}$ where $F$ is the C.D.F of the Gaussian. However I found a very similar question on this site which said we take $\Omega$ to be $\mathbb{R}$ and $X$ as the identity map which didn't really make sense to me. Any clarifications are much appreciated.","['measure-theory', 'random-variables']"
2918726,Finding $f$ satisfies $\limsup_{n\to\infty}\frac{\tau(n)}{f(n)}=1$,"Is there a known result on finding the function $f$ satisfies $$\limsup_{n\to\infty}\frac{\tau(n)}{f(n)}=1?$$
  where $\tau(n)$ is the number of factor(s) of $n$. Related: Prove that $d(n)\leq 2\sqrt{n}$ It shows that $f(n)=O(\sqrt n)$. Some Ideas Also, letting $n=2^m,m\in\mathbb Z$ gives $f(n)=\Omega(m)$, or $f(n)=\Omega(\ln n)$. Factorize $n$ into $\prod_ip_i^{\alpha_i}$ gives $\tau(n)=\prod_i(\alpha_i+1)$. So probably the next step to do is to estimate $p_i$. Wikipedia gives $p_n\le n\ln n+n\ln\ln n$ when $n\ge 6$. Can we use this result to give the better bounds for $f$?","['number-theory', 'limits', 'limsup-and-liminf']"
2918754,"Number of sequences formed from $1, 1, 1, 2,3,4,5,6$ in which all three $1$s appear before the $6$","I want to find number of sequences which contains $2,3,4,5,6$ exactly once and $1$ exactly three times. Also all three $1$s should be placed before $6$. Example $1)1,1,1,2,3,4,5,6\\2)1,2,3,4,1,1,6,5$ I think that $6$ can not be at $1$st, $2$nd, and $3$rd place.
If we fix $6$ at fourth place then all three $1$'s are fixed at first three places and there are $4!$ such sequences possible. How to calculate for $6$ be fixed at $5$th to $8$th place?","['permutations', 'combinatorics']"
2918770,Spivak Physics for Mathematicians - Help understanding the derivative of a one-parameter family of linear transformations.,"Most physics textbooks I've consulted only confused me more with regards to how to understand rotating coordinate systems so I consulted Spivak's book but I have 2 questions; in particular regarding the derivative near the bottom of the page. $r' = B(\rho') + B'(\rho)$ Even if we write it out in uncondensed notation, the formula reads $r'(t) = B(t)(\rho'(t)) + B'(t)(\rho(t))$. Firstly I am not sure how he gets this formula, because it seems like he used a product rule of sorts, which I am not sure how/why it even applies. And secondly, I'm not sure how to interpret the term $B'(t)$, because from my understanding, $B$ is a function from $\mathbb{R}_{\geq0}$ into Hom$(\mathbb{R}^3)$, so how can we differentiate such an object? I know that the derivative of a function from say from $\mathbb{R}^n$ into $\mathbb{R}^m$ is a linear transformation, but what about the case where the target space is a general vector space? So if someone could explain this step in more detail, that would be much appreciated :)","['physics', 'derivatives', 'coordinate-systems', 'linear-transformations']"
2918771,"Is there conflict between the law of excluded middle and ""no set is its own member""?","And if yes, then how can it be resolved? As far as I know in standard set theory it's true that ""no set is its own member"".
Also in standard logic the law of excluded middle is true, either $A$ or $\lnot A$. Now let's consider simple sentence ""Entity $X$ is either a real number or not a real number"".
This can be restated in terms of set theory as ""Element $x$ either belongs to set $\mathbb{R}$ or
to set non-$\mathbb{R}$"". Looks like a tautology, doesn't it? But I will show you that it's not because there is one 
thing that isn't a member of either set. Namely, it's set ""non-$\mathbb{R}$"". This can't belong to
set $\mathbb{R}$ because it isn't a real number. This also can't belong to set non-$\mathbb{R}$ (i.e. it can't belong
to itself) because of ""no set is its own member"". Thus ""Element $x$ either belongs to set $\mathbb{R}$ or to set non-$\mathbb{R}$"" isn't a tautology.","['elementary-set-theory', 'propositional-calculus', 'logic']"
2918773,Find the entire function $f$.,"Suppose that $f:\mathbb{C}\to\mathbb{C}$ is entire and that $f(x,y)=u(x,y)+iv(x,y)$. If $u^2-v^2\geq x^2-y^2$ for all $z=x+iy$, what information can we have about $f$? It seems Liouville's theorem must apply somewhere here.
Now the inequality above gives us that the real part of $f(z)^2$ is greater than the real part of $z^2$ for all $z\in\mathbb{C}$. But how to proceed from here? Hints would be great.","['complex-analysis', 'entire-functions']"
2918788,Decomposition of polynomials like $1+x^4$,"What is the ""trick"" to get from $1+x^4$ to $$(x^2-\sqrt{2}x+1)(x^2+\sqrt{2}x+1)?$$ Of course I can calculate it's true, but I don't understand what steps to take to get from one to the other. Next to this specific question, I am also looking for the general rules for such decompositions? Cheers!","['algebra-precalculus', 'factoring', 'polynomials']"
2918874,There are infinity many numbers when added to Three distinct integers making them pair-wise relatively prime,"Given $0<a<b<c$ three distinct integers, prove that there exists infinity many numbers $n$ such that $a+n,b+n,c+n$ are relatively prime to each other. For the case when $(a,b)=(b,c)=(a,c)=1$ which means they are already relatively prime the answer is easy, i found that $n= (c-b)(c-a)(b-a)k $ for $k \in \mathbb{N} $  works, for instance $gcd((c-b)(c-a)(b-a)k + a, (c-b)(c-a)(b-a)k +b) = (b-a,(c-b)(c-a)(b-a)k +a)=  (b-a,a) = (b,a) = 1$ And so for the rest. But what to do when one or more are not relatively prime ? How to solve it in  more general way?","['number-theory', 'elementary-number-theory']"
2918903,How does the focus-directrix definition of a conic section apply to a circle?,"One of the definitions of a conic section is that the conic section is a locus of points whose distance to the focus $F$ is a constant multiple of distance between them and the directrix $D$, i.e. $e = \frac{d(P,F)}{d(P, D)}$ Where $e$ is eccentricity. It is said that the eccentricity of a circle is $0$, which means: $0 = \frac{d(P,F)}{d(P, D)}$ therefore, $d(P,F)=0$ What confuses me about this, is that, unlike with other conic sections, where there would be infinite number of points $P$ which satisfy the given equation, it seems that the only point that satisfies this equation is the point $P=F$. Yet, the circle is composed of infinite points, which is contradictory. Can someone explain what is the geometric interpretation of this, and where I'm wrong?","['curves', 'analytic-geometry', 'conic-sections', 'geometry']"
2918975,Riemann sum of $x\cdot \ln(x)$,"I did not find any information regarding this Riemann sum anywhere: Riemann sum of $f(x)=\begin{cases} 0& x=0 \\ x\cdot \ln(x)& \text{otherwise}\end{cases}$ in the interval $[0, 1]$. I don't want the answer given to me, I'm only looking for a hint that could guide me in the right direction. I have already proved that the function is continuous in this interval, i.e can be integrated in it also.","['riemann-sum', 'real-analysis']"
2919004,How is 0 a limit point of $\{1/n\}_{n=1}^{\infty}$?,"I'm working through Principles of Topology and can't wrap my head around how $0$ is a limit point of $\{1/n\}_{n=1}^{\infty}$. For reference, this is on page 44 of the 2016 edition. The book provides the following definition/theorem & proof: Definition : A point x in $\mathbb{R}$ is a limit point or accumulation point of a subset A of $\mathbb{R}$ provided that every open set containing x contains a point of A distinct from x. Theorem 2.9: A real number x is a limit point of a subset A of $\mathbb{R}$ if and only if $d(x, A\setminus\{x\}) = 0$. Proof: Suppose first that x is a limit point of A and let $\epsilon$ be a positive number. Then the open set $(x-\epsilon, x+\epsilon)$ contains a point y of A distinct from x. Since $y\in(x-\epsilon,x+\epsilon)$, then $d(x,y) < \epsilon$ so $d(x, A\setminus\{x\}) < \epsilon$. Since the latter inequality holds for all $\epsilon>0$, then $d(x, A\setminus\{x\}) = 0$. Suppose now that $d(x, A\setminus\{x\}) = 0$ and consider an open set O containing x. Then O contains an open interval $(x-\delta, x+\delta)$ for some positive number $\delta$. Since $d(x, A\setminus\{x\}) < \delta$ and the interval $(x-\delta, x+\delta)$ consists precisely of all points at a distance less than $\delta$ from x, then $(x-\delta, x+\delta)$ must contain a point z in $A\setminus\{x\}$. Thus $$z \in (x-\delta, x+\delta) \subset O$$ and $z \neq x$ since $z\in$ $A\setminus\{x\}$. Hence every open set containing x contains a point of A distinct from x, and x is a limit point of A. $$\tag*{$\Box$}$$ Additionally, the book defines distance between a real number and a non-empty subset of $\mathbb{R}$ as:
$$d(a, B) = glb\{|a-b|:b \in B\}$$ and provides a few examples, e.g.: $d(0, [1,2]) = d(0, (1,2)) = 1$ $d(1, [1,2]) = d(1, (1,2)) = 0$ So far, so good. The book then notes: $0$ is the only limit point of $\{1/n\}_{n=1}^{\infty}$. Note in this case that the limit point is outside the set. My question is... how? Say $x = 0$ and you choose $\epsilon = 3$, and you follow along the proof:
$$(x-\epsilon, x+\epsilon) = \{x \in \mathbb{R}: (x-\epsilon) < x < (x+\epsilon)\}$$ which certainly does contain a point $y = 1$ where $y \in \{1/n\}_{n=1}^{\infty}$, with $y$ being distinct from $x$. What I don't understand: if $A = \{1/n\}_{n=1}^{\infty}$, how does $d(x, A\setminus\{x\}) = 0$? As far as I can tell, since $0 \notin A$, then $A\setminus\{x\}$ is equivalent to $\{1/n\}_{n=1}^{\infty}$, and the distance between $0$ and $A$ would be $1$: $$d(0, [1, \frac 12, \frac 13, \frac 14 ...)) = 1$$ What might I be missing here?","['general-topology', 'metric-spaces', 'real-analysis']"
2919027,"$T: L^2[0,1] \to L^2[0,1]$, $Tf(x)= \frac{1}{x}\int_{0}^x f(y)$, is a bounded but not compact operator.","To show that the image of $T$ lies in $L^2$ , and derive its bound, I tried the following: $$\|Tf\|_{2} = \left(\int|\int \frac{1}{x}f(y)dy|^2dx\right)^{\frac{1}{2}} \leq \int \sqrt{\int|\frac{1}{x}f(y)1_{[0,x]}(y)|^2dx}dy = \int_{0}^{1} \sqrt{ \frac{1}{y}-1 }|f(y)|.$$ Then probably using Holder's inequality, but this does not seem to work. Then to show it is not a compact operator, I want to construct a bounded sequence $f_n$ where $Tf_n$ does not have a convergent subsequence.","['operator-theory', 'lp-spaces', 'compact-operators', 'functional-analysis']"
2919041,Gaining an appreciation for homology class representatives in $\mathbb CP^n$.,"Given a compact oriented submanifold $N \subset M$ one says that $N$ represents a homology class in $M$ by taking $i_*(\tau_N)$ where $i_*$ is induced by inclusion and $\tau_N$ is the fundamental class of $N$ chosen according to orientation. There are some cases where this is completely clear. For example, $S^1$ represents a generator in $\mathbb C \setminus \{0\}$, or $\mathbb CP^1$ represents a homology class in $\mathbb CP^2$ by the $CW$-structure. However, there are some more mysterious cases for me. For example, a degree $3$ complex projective curve should be $3 \cdot [\mathbb CP^1] \in H_2(\mathbb CP^2).$ But these are tori (when they are elliptic curves) so up to homology, $[T^2] =3 \cdot [\mathbb CP^1]$. Probably a satisfactory answer (assuming that I'm thinking about this correctly) would include something like: Can one prove that a torus represents $3 \cdot [\mathbb CP^1] \in
 \mathbb H_2(\mathbb CP^2)$ geometrically? or a reference pointing to how one can begin to do these types of geometric calculations.","['geometry', 'reference-request', 'manifolds', 'homology-cohomology', 'algebraic-topology']"
2919044,Calculating Fisher Information for Bernoulli rv,"Let $X_1,...,X_n$ be Bernoulli distributed with unknown parameter $p$. My objective is to calculate the information contained in the first observation of the sample. I know that the pdf of $X$ is given by $$f(x\mid p)=p^x(1-p)^{1-x}$$, and my book defines the Fisher information about $p$ as $$I_X(p)=E_p\left[\left(\frac{d}{dp}\log\left(p^x(1-p)^{1-x}\right)\right)^2\right]$$ After some calculations, I arrive at $$I_X(p)=E_p\left[\frac{x^2}{p^2}\right]-2E_p\left[\frac{x(1-x)}{p(1-p)}\right]+E_p\left[\frac{(1-x)^2}{(1-p)^2}\right]$$ I know that the Fisher information about $p$ of a Bernoulli RV is $\frac{1}{p(1-p)}$, but I don't know how to get rid of the X-values, since I'm calculating an expectation with respect to $p$, not $X$. Any clues?","['expected-value', 'statistics', 'probability-distributions']"
2919063,Applying Rolle's theorem,"This is the first time that I'm doing a proof-problem on my own and I'm not really sure how to check if my answer is correct, so I was wondering if someone could tell me with certainty if my proof is correct. The problem states: Let $f:\mathbb{R} \to \mathbb{R}$ be a twice differentiable function (on its entire domain) such that $f(0) = f(1) = f(2)$. Prove that there exists some point $x_0 \in (0, 2)$ such that $f''(x_0) = 0$. So I developed my proof based on Rolle's theorem that states that if a function $f$ is continous on a closed interval $[a, b]$ and differentiable on an open interval $(a, b)$ and if $f(a) = f(b)$ then there exists some point $c \in (a, b)$ such that $f'(c) = 0$. Proof: $f$ is differentiable $\forall x \in \mathbb{R}$, meaning $f$ must be differentiable on $(0, 1)$ and $(1, 2)$ $f$ is differentiable $\forall x\in\mathbb{R}$ meaning $f$ must be continuous $\forall x \in \mathbb{R}$ meaning $f$ must be continuous on $[0, 1]$ and $[1, 2]$. $f(0) = f(1)$ and $f(1) = f(2)$. From 1., 2. and 3. we conclude that $f$ satisfies the conditions of Rolle's theorem for both of these intervals, meaning:
$\exists c_1 \in (0, 1) : f'(c_1) = 0$ and $\exists c_2 \in (1, 2) : f'(c_2) = 0$ Let $F(x) = f'(x)$. We know that the original function is twice differentiable $\forall x \in \mathbb{R}$, therefore $F(x)$ is differentiable on $\mathbb{R}$ meaning it must be differentiable on $(c_1, c_2)$ also meaning it must be continuous on $[c_1, c_2]$. Now, since $f'(c_1) = f'(c_2) = 0$, the function $F(x)$ satisfies the conditions of Rolle's theorem (on the interval $(c_1, c_2)$) meaning : $\exists x_0 \in (c_1, c_2) \subset(0, 2): F'(x_0) = f''(x_0) = 0$.","['proof-verification', 'real-analysis']"
2919081,"Why does ""Naive Set Theory"" by Halmos allow the universal set despite admitting its non-existence?","Halmos openly says that the universe doesn't exist: there is no universe But several pages later Halmos says: In order to record the basic facts about complementation as simply as possible, we assume nevertheless (in this section only) that all the sets to be mentioned are subsets of one and the same set E and that all complements (unless otherwise specified) are formed relative to that E. In such situations (and they are quite common) it is easier to remember the underlying set E than to keep writing it down, and this makes it possible to simplify the notation. An often used symbol for the temporarily absolute (as opposed to relative) complement of A is A′. The following rule from the book killed all my hopes that E isn't the universal set: $$E^{\prime} = \varnothing$$ This seems like blatant self-contradiction with previous statement that there is no universe. I hope to resolve it, but I have almost no ideas what it can mean. My only guess is that it's some kind of ""lie to children"", when relatively simple lie is told because the teacher believes that his/her student(s) can't yet comprehend the whole truth. P.S. In my other question I got comments that can probably shed some light on the problem. looking at the image you posted, I believe you are confusing two different scenarios. In many applications of set theory, there is a universal set at hand. For example, if we are studying natural numbers, the universal set for that purpose may be the set of all natural numbers. All of the rules for set complement assume that a temporary universal set has been chosen in that way. The other context is in studying formal set theory, such as ZFC. In this context, there is no single universal set that contains all sets.( Carl Mummert ) And another: If your book isn't specifically on mathematical set theory, then the complement is usually taken with respect to a not explicitly stated super set U. E.g. the complement of the set of even numbers is probably meant to be the set of odd numbers, as the super set might be the natural numbers. This should be clear from the context. ( M.Winter ) These comments give weak hope, but alas they don't seem to solve the conflict. If anything, they create new one: conclusion from them is in contradiction with statement that absolute coplement of E is the empty set. For example, let's assume that E is equal to set of natural numbers N. It contains only natural numbers as its elements, but not sets of natural numbers (like it doesn't contain set {1,2,3} as its element). Thus complement of E wouldn't be empty as Halmos assures, it would contain such sets like {1,2,3}, {1,7,65382, 3235464567765}, etc. as its elements. Even worse, I doubt that complement of set E under such circumstances would even be a set in the first place, because it would contain everything that doesn't belong to set E, including set E and complement of set E itself . P.P.S. As Eric Wofsey pointed out, I missed consequences of phrase "" all the sets to be mentioned are subsets of E"". Indeed, then our complement set can't contain elements that aren't elements of E. In other words, our complement set of E must be subset of E! But at the same time they must have no any common elements. Complement of set E can't contain element that isn't element of set E, thus the only alternative is not contain any element at all, i.e. be the empty set because intersection of any set with empty set is empty set. And of course, under such considerations E doesn't necessarily contain itself as its member. @EricWofsey Thanks Eric! I think the question is solved. Halmos doesn't really allow the universe, I just got wrong conclusions from rule $E^{\prime} = \varnothing$ .",['elementary-set-theory']
2919096,How can I solve the following higher order ODE?,"I am trying to solve a system of two second-order ODEs. After separating them, I obtained a fourth-order independent ODE as illustrated below. I wonder if there is a specific technique to solve it. $$y^{(4)}+\frac{a_1}{x} y^{(3)}+\frac{a_2}{x^2}y^{(2)}+a_3y^{(2)}+\frac{a_4}{x}y^{(1)}+a_5y=0$$",['ordinary-differential-equations']
2919193,Variance of $X-Y$ cannot be negative,"Suppose we have 2 non independent variable X and Y. Since $$Var(X-Y)= Var(X) + Var (Y) -2Cov(X,Y)$$ Would it be possible for the above to be negative in the case when $$Var(X) + Var(Y) < 2 Cov(X,Y)$$?","['statistics', 'covariance']"
2919210,An example of a group with a topology,Do you know an example of a group with a topology satisfying both the following two conditions the product is separately continuous but not jointly continuous the inversion map is continuous .,"['topological-groups', 'examples-counterexamples', 'continuity', 'abstract-algebra', 'general-topology']"
2919270,Continuity vs. strong continuity,"Why is this condition stronger than continuity? By definition, a map is continuous iff the preimage of an open set is open. That's precisely the condition in the cited definition, isn't it?","['general-topology', 'logic']"
2919283,"Open set in the complex plane, then the set of conjugates is also open","Take a set $A \subset \mathbb{C}$ such that $A$ is open. Show that the set $\{z \in \mathbb{C} : \bar{z} \in A\}$ is also open. I have been using the definition of the open ball, but seem to be confusing my self. Any hints on how to show this?","['complex-analysis', 'general-topology', 'analysis']"
2919316,Continuity of $2$ variable function in $R^2$ when one variable function is differentiable.,"I have this question in an assignment and I am unable to figure it out. ""Suppose $f(x,y)$ is a function defined in $R^2$.  Set $g(x) = f(x, 0)$, $h(y) = f(0, y)$.  If $g$ and $h$ are differentiable at $0$ as functions in one variable does it follows that f is continuous at the origin?  (If your answer is ""yes"", provide a proof; if your answer is ""no"", construct a counterexample.)"" I was able to construct a counterexample to show that it does not follow that $f$ is differentiable at the origin but I am not sure if how that relates to the continuity of $f$. Thanks in advance.",['multivariable-calculus']
2919326,If $f$ is bijective then show it has a unique inverse $g$,"Given a function $f:X\rightarrow Y$, an inverse function $g$ of $f$ is a function from $Y$ into $X$ such that $\left ( f(g(y)) \right )=y$ for all $y \in Y$. I have to show that $g$ is a unique inverse function of $f$ if $f$ is bijective. My attempt: Since $f$ is bijective, we know that there exists a $y \in Y$ with $f(y)=x$ for every $x \in X$ (from surjectivity). Furthermore, there is only one such $y$ such that $f(y)=x$ since $f$ is injective as well. Let $g(x)=y$. Then $g(f(y))=y$ for any $y$ and $f(g(y))=x$ for any $x$. So $g$ is an inverse of $f$. To show that $g$ is unique, suppose for contradiction, $f$ has another inverse $h$. Also suppose $g \neq h$. Then there would be some $x$ for which $g(x) \neq h(x)$. However, this $x$ must be $f(y)$ for some $y \in Y$. Since $g$ is an inverse of $f$, we must have $g(x) = g(f(y))=y$. Similarly, $h(x)=y$. Therefore, there is no $x$ such that $g(x) \neq h(x)$. So $g$ is unique. Does this proof seem correct?","['functions', 'proof-verification', 'discrete-mathematics']"
2919335,"For non-differentiable functions, does fundamental theorem of calculus give the subgradient?","As we know, if $f:R→R$ is continuous, and $F(x)=\int_0^xf(y)dy$, then $F$ is differentiable and $F'(x)=f(x)$. (Fundamental Theorem of Calculus) Consider the case that $f$ is not continuous, let $F(x)=\int_0^xf(y)dy$. Assume $f$ is increasing thus $F$ is convex. Now $F$ doesn't have to be differentiable but it has subgradients (subderivatives). Is there any relationship between $f$ and subgradient of $F$? Is it true? Can we prove this: $c$ is a subgradient of $F(x)$, if for any $y<x,f(y)<c$ and for any $y>x, f(y)>c$. Here is an example to justify this proposition. Suppose $f(x)=1$ for $x>=0$, $f(x)=-1$ for $x<0$. $F(x)=\int_0^xf(y)dy=|x|$. The proposition holds in this example.","['subgradient', 'calculus', 'analysis']"
2919344,$\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx$ (Now solved) [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question does not appear to be about math within the scope defined in the help center . Closed 5 years ago . Improve this question I have this integral:
$\require{cancel}$
$$\xcancel{I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx}$$
and the clue given for solving it is:
$$\int_a^bf(x)dx=\int_a^bf(a+b-x)dx$$
so,
$$\xcancel{\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3-x)}}dx=\int_2^4\frac{\sqrt{\ln(9+x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(x-3)}}dx=\int_2^4\frac{\sqrt{ln(x+9)}\left(\sqrt{\ln(3+x)}-\sqrt{\ln(x-3)}\right)}{\ln(3+x)-\ln(x-3)}dx}$$
where shall I go from here (if this is the right approach)? EDIT
if we take the equation to actually be:
$$I=\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx$$
this removes any problems involving $\ln(0)$ as pointed out by users below If we now use the same method as I outlined above we obtain:
$$\int_2^4\frac{\sqrt{\ln(9-(6-x))}}{\sqrt{\ln(9-x)}+\sqrt{\ln(3+x)}}dx=\int_2^4\frac{\sqrt{\ln(9-x)}}{\sqrt{ln(3+x)}+\sqrt{\ln(9-x)}}dx$$
and we can see that the bottom of the fraction is the same on the left and the right, so my first idea would be to add the two sides, giving:
$$2I=\int_2^4\frac{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}{\sqrt{\ln(3+x)}+\sqrt{\ln(9-x)}}dx=\int_2^41dx=2$$
$$\therefore I=1$$","['integration', 'calculus']"
2919421,Making a Venn diagram with four circles - impossible,"A Venn diagram is a diagram that shows all possible logical relations between a finite collection of different sets. ( https://en.m.wikipedia.org/wiki/Venn_diagram ) It is easy to make a Venn diagram for three sets. Just draw three circles as shown in the figure. For four sets, it is getting more difficult. There are lot of possible ways making a Venn diagram for 4 sets (just check the wikipedia or imagine with four ellipses), but it is impossible to make it with four circles. But why? I only know that it is impossible. I can prove it but my solution is really complicated (maybe wrong as well), but I think there might be an easy, beautiful solution. Please help! Thanks in advance!","['elementary-set-theory', 'geometry']"
2919426,Lebesgue measure on countable union,"Given the Borel set $A=\cup_{n=1}^{\infty}[n,n+1/n]$ how do I find its Lebesgue measure? My attempt: Given $\lambda$ we have that 
\begin{align*}
\lambda(\cup_{n=1}^{\infty}[n,n+1/n])&=\lambda([1,2]\cup[2,2+1/4])+\sum_{n=3}^{\infty}\lambda([n,n+1/n]) &&\text{Additive}\\
&=\lambda([1,2+1/2]) +\sum_{n=3}^{\infty}\frac{1}{n}&&\\
&=1+1/2+\sum_{n=3}^{\infty}\frac{1}{n}&&\\
&=\sum_{n=1}^{\infty}\frac{1}{n}
\end{align*} The harmonic series diverges, hence $\lambda(A)=+\infty$. Is this true? Any help is appreciated.","['borel-sets', 'general-topology', 'lebesgue-measure', 'measure-theory']"
2919430,Sum equals the product,"A while ago, just playing with some numbers I noticed that $1+2+3=1\cdot2\cdot3$, so I started thinking about the non-zero integer solutions of the equation $$\prod_{i=1}^na_i=\sum_{i=1}^na_i$$ For example, for $n=2$, the only solution is the pair $(2,2)$, for $n=3$ the only solutions are $(1,2,3)$ and $(-1,-2,-3)$ and that's what I have by now, the problem is, I solved the case $n=2$ using divisibility and the case $n=3$ I proved that if $|a_1|\leq|a_2|\leq|a_3|$, then for $a_2>2$ there was no solution, so I just analyzed every case. Can someone help me with the general case?",['number-theory']
2919498,Application of Rouché's theorem outside the unit circle; Confusion about argument principle,"I am preparing for my function theory exam and came across this problem. Let $f(z) := z^8 - 8z^5+2$. I want to count the number of roots in B:= $\big\{z \ \big| \left| z-2 \right| < \frac{1}{2} \big\}$. From plotting $f$ I know it has one root in $B$. Using that for $z \in B$ we have $\frac{3}{2} < |z| < \frac{5}{2}$, and trying out several candidates for $g$, I sought upper and lower bounds such that one of the following equations is satisfied on $\partial B$: If $|f|<|g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (f+ g, B)$, and if $|f-g| < |f| + |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$, while in class we also used if $|f+g| < |g|$ on $\partial B$, then  $\#_{roots} \ (f, B) = \#_{roots} (g, B)$. Taking a closer look, $g$ has to have one complex root within $B$. I can't even think of a candidate right now. I don't see a clear path to the solution of my problem. I believe I have a hard time understanding the argument principle, due to it's various equivalent forms. Practically, do I chose one of its inequalities and stick to it or is it better to chose it depending on the problem I am trying to solve? I know there are $5$ roots within the unit circle. And I know I can find 3 more roots in $B_2(0)$.","['complex-analysis', 'rouches-theorem']"
2919507,Fixed points of an endomorphism of a ring,"Let $R$ be a commutative $k$-algebra, where $k$ is a field of characteristic zero.
Let $f$ be a $k$-algebra endomorphism of $R$. ($f$ is not assumed to be either injective nor surjective). Are there additional conditions (on $R$ or on $f$) that guarantee that $f$ has a fixed point $r \in R-k$? Namely, $r \in R-k$ such that $f(r)=r$. I do not mind to restrict my question to the special case $R=\mathbb{C}[x,y]$ and $f$ having an invertible Jacobian. (Below I am trying to solve the case $R=\mathbb{C}[x]$ and $f$ having an invertible Jacobian). Remarks: (1) By definition, $f(\lambda)=\lambda$ for every $\lambda \in k$, hence I required that $r \in R-k$. (2) 'fixed point theorems' tag shows that there are several nice theorems concerning fixed points of functions, but additional structures on $R$ are needed. (3) Perhaps this question is relevant. (Notice that here the set of fixed points of $f$ is a $k$-subalgebra of $R$, but it may happen that it equals $k$; the point is to find a fixed point outside $k$). (4) See also this paper , in which the following is proved: If $f:k[x,y] \to k[x,y]$ has an invertible Jacobian and is not an automorphism, then $\cap f^i(k[x,y])=k$. Then it follows that if $f$ has a fixed point $r \in k[x,y]-k$, then 
$r \in \cap f^i(k[x,y])$, so $\cap f^i(k[x,y]) \neq k$, hence $f$ is an automorphism of $k[x,y]$. The special case $R=\mathbb{C}[x]$: Consider $R=k[x]$ and $f: x \mapsto a_mx^m+\cdots+a_1x+a_0$, $a_i \in k$. If $f$ has an invertible Jacobian, namely, $ma_mx^{m-1}+\cdots+a_1= (a_mx^m+\cdots+a_1x+a_0)' \in k^{\times}$, then $a_2=\ldots=a_m=0$, $a_1 \in k^{\times}$, $a_0 \in k$, so  $f: x \mapsto a_1x+a_0$; for convenience, write $f: x \mapsto ux+v$. If $r=\sum_{j=0}^{n} b_jx^j$ is a fixed point of $f$, then
$\sum_{j=0}^{n} b_j(u^jx^j+\cdots+v^j)= \sum b_j(ux+v)^j= \sum b_jf(x)^j=f(\sum b_jx^j)=f(r)=r=\sum_{j=0}^{n} b_jx^j$ Therefore, $b_nu^n=b_n$, so $b_n(u^n-1)=0$, hence $u^n=1$. (Also, if $v \neq 0$, then $\sum_{j=0}^{n}b_j=b_0$, so $\sum_{j=1}^{n}b_j=0$). We obtained that $f$, having an invertible Jacobian, has a fixed point $r \in R-k$ if $f: x \mapsto ux+v$ with $u^n=1$ for some $n$ (and other conditions). For example, $k=\mathbb{C}$, $f: x \mapsto -x$, here $n=2$, so we take $r$ of degree $2$: $r=x^2$:
$f(r)=f(x^2)=f(x)^2=(-x)^2=x^2=r$. Another example: $k=\mathbb{C}$, $f: x \mapsto ix$, here $n=4$, so we take $r$ of degree $4$: $r=x^4$:
$f(r)=f(x^4)=f(x)^4=(ix)^4=(i^4)x^4=x^4=r$. Thank you very much!","['fixed-point-theorems', 'algebraic-geometry', 'commutative-algebra']"
2919511,Finding the limit $f(2x)-f(x)$,"Let $f:(0,+\infty) \rightarrow \mathbb{R}$ differentiable such that $\left | f'(x) \right | \leq \frac{1}{x^2} ,$ for every $x>0$ Prove that $\lim_{x\rightarrow\infty} \left |f(2x)-f(x) \right|=0$ Here's my attempt: $$\lim_{x\rightarrow\infty}\left |f(2x)-f(x) \right|= \lim_{x\rightarrow\infty}\frac{\left |f(2x)-f(x) \right|\cdot(2x-x)}{2x-x}=\left |f'(x) \right|\cdot x\leq \frac{1}{x^2}\cdot x=\frac{1}{x}$$ Now using the squeeze theorem we get that the limit is $0$ I know though that there's something wrong with my solution.. What do you think?","['limits', 'derivatives', 'real-analysis']"
2919562,Proving $(p \land \lnot q) \rightarrow p$ is a tautology using logical equivalences,"I'm very new to discrete math and propositional calculus. 
I keep getting lost trying to prove the following propositional formula is a tautology using equivalencies. $$(p \land \lnot q) \rightarrow p$$ Edit : I solved it; see my answer below.","['propositional-calculus', 'logic', 'discrete-mathematics']"
2919587,Why are these sets saturated open sets?,"How did the author find out that typical saturated open sets are those depicted? For me it's not clear why those are saturated open sets. How do I see it? For definition, see About saturated sets Similarly, I can't see why these sets are saturated open:",['general-topology']
2919600,Can we tell if a function has a max or min by looking along specific directions?,"Suppose we have a smooth function $f$ from $\mathbb R^n\to\mathbb R$ such that $\nabla f(0)=0$, and we want to check if $f$ has a local maximum at $0$ (as opposed to a local min or a saddle point). For any vector $v$ in $\mathbb R^n$, we can form a function $g_v:\mathbb R\to\mathbb R$ defined by $g_v(x)=f(vx)$. Intuitively, we are looking at the behavior of $f$ along the $v$ direction. If $f$ has a local max at zero, then of course $g_v$ also has a local max at zero. My question is, is the converse true? If the single-variable function $g_v$ has a local max at $0$ for every possible direction $v$, does $f$ also have a local max at $0$? If the Hessian of $f$ at zero has full rank, then this question is easily answered as yes. By taking $v$ to be in turn each of the eigenvectors of the Hessian, the fact that $g_v$ has a local max means that the corresponding eigenvalues are all negative, and so $f$ has a local max at $0$. But if some of the eigenvalues are $0$, then I'm not sure how to analyze it. As a first step in a proof, I might think: well, for any fixed direction, there is some radius $r$ such that as long as $v$ is within distance $r$ of the origin, then $f(v)\le f(0)$. But since the $r$ depends on the direction, we can't necessarily find a single $r$ which works for all directions, so that might allow for a counter-example. As a bonus, if it does hold for smooth functions, what about functions that are merely differentiable? Continuous? Or even all functions?","['maxima-minima', 'multivariable-calculus']"
2919662,The notion of limsup for sets,"I was working through The Borel-Cantelli lemma from Real Analysis problem book and ran into the following comment: Let $\{E_k\}_{k\geq1}$ is a countable family of measurable subsets of $\mathbb{R}^d$ and that $\sum \limits_{k=1}^{\infty}m(E_k)<\infty$. Let $$E=\{x\in \mathbb{R}^d: x\in E_k, \text{for  infinitely many} \ k\}=\limsup\limits_{k\to \infty}(E_k).$$ I know the notion of $\limsup$ and $\liminf$ for sequences from $[-\infty,+\infty]$. But here $E_k$ are sets (NOT numbers!). How did they get that $$\{x\in \mathbb{R}^d: x\in E_k, \text{for  infinitely many} \ k\}=\limsup\limits_{k\to \infty}(E_k)?$$ This equality seems to me quite weird. Would be very grateful for explanation! EDIT: Is there some essential difference between $\limsup$ of sequence of sets and reals?","['elementary-set-theory', 'limsup-and-liminf', 'measure-theory', 'real-analysis']"
2919672,Equivalent definition of a quotient map,"I'm trying to understand why the two definitions of a quotient map are equivalent. Suppose $p:X\to Y$ is a quotient map in the first definition. Then certainly $p$ is continuous and maps all open sets to open sets (in particular it maps saturated open sets to open sets). But the reverse implication is not clear. Suppose $p$ is continuous and maps saturated open sets to open sets. There are two things to prove: 1) $p$ is surjective. 2) if $U\subset X$ is an arbitrary open set (not necessarily saturated), then $p(U)\subset Y$ is open. How do I show this?",['general-topology']
2919706,Vector spaces. When in the real world are we checking if it's a vector space or not?,I am reading this text: When in the real world are we checking to see if sets are vector spaces or not? The examples above seem like really specific sets... Are there any places where we redefined scalar multiplication like this?,['linear-algebra']
2919725,Time complexity analysis of finding the largest prime divisor,"I am trying to understand the time complexity of the following solution to finding the largest prime factor of a positive integer. https://www.geeksforgeeks.org/find-largest-prime-factor-number/ Let's suppose a number is already prime, say $N = 6247$. According to the algorithm we loop at most $\sqrt(N)$ times looking for a prime to divide out.  However, in cases where $N$ is not prime, we have an inner while loop that divides out all occurrences of a given prime. How can we prove that the inner loop doesn't increase the asymptotic complexity of the algorithm? More rigorously, if $N = p_{1}^{a_{1}} p_{2}^{a_{2}}...p_{n}^{a_{n}},$ prove that $\sum_{i=1}^{n}a_{i} = O(\sqrt(N)).$","['computational-complexity', 'discrete-mathematics', 'algorithms']"
2919733,How many solutions to $z^{6!}-z^{5!}\in \mathbb{R}$ and $|z|=1$,"How many solutions $\Im(z^{720}-z^{120})=0$  where $|z|=1$?  (AIME I $2018$ #$6$) The following is my interpretation of the first solution. The next step is to see how many solutions to $\sin(720\theta)-\sin(120\theta)=0$. (1) One way to do this is consider $\sin(6\omega)-\sin(\omega)$, which has $12$ solutions. (2) For each solution to (2), corresponds $120$ solutions to (1). I understand that. But how do I know that $sin(6\omega)=sin(\omega)$ has $12$ solutions? I'd say during a real contest I could come up with everything but the $12$ solutions part. I tried sketching both graphs on the same plane for $0<x<2\pi$ and I only got 9 intersections, while I can see all $12$ clearly on Desmos, so my drawing is unreliable.",['trigonometry']
2919854,Equation with one variable unknow how to solve easily,"I have $$\left|\begin{array}{ccc}
x & -2 & 3x-6 \\
2x & \phantom{-}0 & 2-x \\
-x & \phantom{-}5 & x-2 
\end{array}\right| = 0$$ How can I solve this with a fast way? I thought I can do 
$$x-2+3x-6=0 \implies 4x=-8 \implies x=-2$$ 
and I will continue with the other two, but I don't know if I am right.","['linear-algebra', 'ordinary-differential-equations']"
