question_id,title,body,tags
1034109,Why does my professor say that writing $\int \frac 1x \mathrm{d}x = \ln|x| + C$ is wrong?,"My professor says that writing this is convenient
$$\int \frac 1x \mathrm{d}x = \ln|x| + C\tag{1}$$
but wrong, since it should be written as:
$$\int \frac 1x \mathrm{d}x = \begin{cases}\ln x + C &x > 0\quad(\star)\\[0.2em]
\ln(-x) + C &x < 0\end{cases}$$ I was wondering why is that the case. I thought that the two were equivalent, as one can see by the definition of absolute value.
In $(\star)$ the equality sign is dropped because the logarithm is not defined in $0$, but that would be the case with $(1)$ as well.","['logarithms', 'calculus', 'integration', 'indefinite-integrals']"
1034150,Is there a specific name for this set of square-rooted primes?,"Consider the set of all the primes numbers (± square rooted) and all the irrational numbers that can be formed under their addition (only the addition of finitely many elements is allowed, i.e. no limits). We also include 0, as the identity element into this set. Is this set a group? I believe yes. For example $7^{0.5}+3^{0.5}+5^{0.5}$ can be formed by adding $7^{0.5}+ 7^{0.5}+3^{0.5}+5^{0.5}+11^{0.5}$ and $-11^{0.5}$, (and infinitely many other other pairs of elements). However, I have a doubt. Say we want to form $11^{0.5}$ from $5^{0.5}$ without resorting to the addition of $-5^{0.5}+11^{0.5}$. I believe that no finite sum of irrational primes (hence element) of the set exists such that $x=-5^{0.5}+11^{0.5}$. So now I wonder, what could this set be called (I haven't formally studied mathematics. Can it be regarded as a group, for I am not sure if it can fairly said to satisfy the latin square property. I am tempted to say that is does, but then again, is there a very specific name for the group I seem to have described, with its particular characteristic of ""fundamental"" elements and ""constructed"" elements? Thank you very much. Please ask if any clarifications are needed.","['elementary-set-theory', 'group-theory']"
1034156,Ways to prove $ \int_0^\pi \frac{\sin^2 nx}{\sin^2 x} dx= n\pi$,"In how many ways can we prove the following theorem? $$I(n):= \int_0^\pi \frac{\sin^2 nx}{\sin^2 x} dx= n\pi$$ where $n$ is a nonnegative integer. The proof I found is by considering $I(n+1)-I(n)$ , which can be reduced to $$
g(n):= \int_0^\pi \frac{\sin(2 n x) \cos x}{\sin x}dx
$$ I then showed that $g(n)=g(n+1)$ , with $g(n) = ng(1) = n\pi$ . This completes the proof. I was wondering if there is a more direct way to prove it. By 'direct' I mean without deriving auxiliary recursions.","['definite-integrals', 'trigonometry', 'trigonometric-integrals', 'integration']"
1034210,What is the Moore-Penrose pseudoinverse for a hermitian block-matrix with one zero block?,"Given a block matrix of the form \begin{pmatrix}
  A & B^* \\ B & 0
\end{pmatrix} where $A$ is singular (otherwise one could simply use the well-known block matrix inverse), is there a simpler formula for the Moore-Penrose inverse than the general one ?","['matrices', 'pseudoinverse', 'inverse']"
1034229,What's a good primer from linear algebra to spherical harmonics?,"I need a topic, a primer, that will be able to introduce me to spherical harmonics and how to translate and use them with the usual tools of linear algebra and calculus, namely matrices, polynomials and derivatives for example . In other words, I would like to know enough to handle and compute harmonics. What topics do you suggest I should touch to get up and running with spherical harmonics starting with a linear algebra and calculus background ?","['calculus', 'linear-algebra', 'harmonic-analysis', 'reference-request', 'spherical-harmonics']"
1034247,How does $\int_0^\infty e^{-t^4}dt = \Gamma (\frac{5}{4}) ?$,"My text book claims that $$\int_0^\infty e^{-t^4}dt = \Gamma \left(\frac{5}{4}\right).$$ I fail to see this. By the definition of the gamma function we have
$$\Gamma (z) = \int_0^\infty t^{z-1}e^{-t}dt.$$ Thus, plugging in $\frac{5}{4}$ for $z$ should give us
$$\begin{array}{ccc}
\Gamma\left(\frac{5}{4}\right) & = & \int_0^\infty t^{\frac{5}{4}-1}e^{-t}dt \\
& = & \int_0^\infty t^{1/4}e^{-t}dt. \\
\end{array}$$
This algebraically implies that $$t^{1/4}e^{-t} = e^{-t^4} \Leftrightarrow t^{1/4} = e^{-t^3}.$$ And from here I just get into a messy algebra scenario using logarithms that just really doesn't get me anywhere (as far as solving for $t$ goes). So what am I missing here?","['calculus', 'integration', 'exponential-function', 'real-analysis', 'gamma-function']"
1034261,"let $f$ be analytic and bounded above, can I prove f is constant?","I've read up on Lioville's theorem and I was wondering if this could also be proved using the theorem: let $f$ be analytic on $\mathbb{C}$ and let $K>0$ be s.t. $|f| \geq K$, could I prove using Liouville's theorem that $f$ is constant? I tried by defining $q(z) = 1/f(z)$ and then $|q(z)| \leq 1/K$ but we don't know if $q$ is enitre",['complex-analysis']
1034275,The supermum of E,"Let $f\ [0,1]\longrightarrow [0,1]$ be increasing function. let: $$E=\{x\in [0,1] \mid f(x)\geq x \} $$ Show that $E$ has a supermum $b$ and that $f(b)= b$ . we have $x\leq 1$ since $f$ is inceasing $f(x)\leq f(1)$ and $x\leq f(x)$ i don't know how to answer that question
any help would be apppreciated","['functions', 'supremum-and-infimum', 'real-analysis', 'analysis']"
1034276,Fourier Transform of Newton's Law of Cooling,"I am attempting to solve Newton's Law of Cooling differential equation with Fourier Transforms for a high school math report. Can Fourier Transforms be used to solve first-order ODEs? The equation is: $\frac{dT}{dt} = -kT + kT_a$, where $k$ and $T_a$ are constants. Since the Fourier Transform is a linear operator, I took the transform of each of the three terms: $$F\{T'\} = F\{-kT\} + F\{kT_a\}$$ $$F\{T'\} =  -kF\{T\} + kT_aF\{1\}$$ The Fourier transform of 1 should be $\delta(s)$, so: $$F\{T'\} = -kF\{T\} + kT_a\delta(s)$$ The transform of the derivative of a function, using $s$ instead of $\omega$ in the formulas, is $F\{T'\} = 2\pi isF\{T\}$. Substituting this into the original equation, $$2\pi isF\{T\} = -kF\{T\} + kT_a\delta(s)$$ Adding the terms with the transform should yield $$2\pi isF\{T\} +kF\{T\} = kT_a\delta(s)$$ $$F\{T\}(2\pi is +k) = kT_a\delta(s)$$ $$F\{T\} = \frac{kT_a\delta(s)}{2\pi is + k}$$. The formula for the Inverse Fourier Transform is $f(t) = \int_{-\infty}^{\infty} e^{2\pi ist} F(s) ds$. Thus $$T = \int_{-\infty}^{\infty} \frac{e^{2\pi ist} \delta(s) kT_a}{2\pi is + k}ds$$, which I evaluated to $T_a$, dropping the integral and plugging in 0 for $s$ due to the $\delta(s)$ function...however, the actual solution to the differential equation is $T_a + Ae^{-kt}$, where A is a constant. Where did I go wrong?","['ordinary-differential-equations', 'fourier-analysis']"
1034278,"If a linear operator has an adjoint operator, it is bounded","This is a question I'm struggling with for a while: Let $H$ be a Hilber space. Let  $T,S: H\rightarrow H$ be linear operators (not neccessarily bounded) such that for every $x,y\in H$: $\langle Tx,y\rangle=\langle x,Sy \rangle$. Prove $T,S$ are bounded. What I did so far: First attempt: $||Tx||^2=\langle Tx,Tx\rangle=\langle x,STx\rangle\le||x||*||STx||\le||x||*||S||*||Tx||$ I'm not entirly sure I'm allowed to do the last inequallity, since $||S||$ might be $\infty$. It follows $||Tx||\le ||x||*||S||$ and analogly $||Sx||\le ||x||*||T||$ From the first I'd get $||T||\le ||S||$ and from the second $||S||\le||T||$. Therefore $||S||=||T||$.
I don't know where to go from here. Second Attempt:
assume  $T$ isn't bounded, therefore so is $S$, therefore there are series $(x_n),(y_n)\subset H$ such that $||x_n||=||y_n||=1$ and $\lim ||Tx_n||=\infty,\lim ||Sy_n||=\infty$. I'll keep it going soon, but it didn't go so well.. I'd love some guide. Not neccessarily the whole solution, but something that would tell me what i'm missing. Thanks :)","['operator-theory', 'real-analysis', 'adjoint-operators', 'hilbert-spaces', 'functional-analysis']"
1034288,Prove every prime ideal of a ring is a radical ideal.,"this is my attempt: Since $R$ is commutative, we let $I$ to be a prime ideal of $R$, the for $a,b\in R$,then the product $ab$ we must have that $a\in I$ or $b \in I$, by definition of a prime ideal. On the other hand,we have: $radI = \{r\in R | r^n\in I $ for some $n\in \mathbb{N}\}$. so the argument goes by induction on $(ab)^n$, for $n=1$ is just the definition of a prime ideal, so we assume that its true for $n$. now for $n+1$, we have the product: $(ab)^{n+1}= (ab)^n(ab)= (a^{n}a)(b^{n}b)$, but if $a\in I$, then $a^n\in I$, similarly if $b\in I$, then $b^n\in I$, therefore we thus have that $radI=I$. Now, i need to give an example that the converse is false, so if $a,b\in radI$, then $a^n\in I$ and $b^n\in I$, so when we consider the product $a^nb^n=(ab)^n$, we thus have that $a^n\in I$ and $b^n\in I$, therefore $I$ cant be a prime ideal. My question is if this is the correct proof, especially in the converse.","['commutative-algebra', 'ring-theory', 'ideals', 'abstract-algebra']"
1034325,How do you solve a 2nd order differential equation of the form $v = v' - v'' +C^t +D^{t+E}$,"I've been working on an economic simulator for a game I've been making and in order to simulate the velocity of money, I created the differential equation of the form $v = v' -v'' + C^t + D^{t+E}$. However, after creating it, I realized that I didn't actually know how to solve it (the most advanced course in mathematics I have taken is Calculus BC), and neither Wolfram Alpha nor Sage seem to be able to use a method to either solve or approximate the solution. Thanks for any help in advance!",['ordinary-differential-equations']
1034332,Continuous functions and IVT,"Suppose that $f$ is continuous on $[0,1]$ and $f(0) < 0$ and $f(1) > 1$. Show that there is some $x \in (0,1)$ such that $f(x) = x^{2014}$. I am guessing that here we should use the Intermediate Value Theorem. It states: Suppose that $f$ is continuous on $[a,b]$ and $k$ is between $f(a)$ and $f(b)$, then there exists $c \in (a,b)$ such that $f(c) = k$. But from there I am not sure how to prove that $f(x) = x^{2014}$. Could anyone help me out please? Thanks.","['real-analysis', 'analysis']"
1034333,How is number of conjugacy class related to the order of a group?,"Let $c(G)$ denote the number of conjugacy classes of a group $G$. I have to show that 
$$\lim_{n \to \infty} \inf _{|G|=n}c(G)=\infty.$$ That is, I have to show that $\exists $ a function $f:\mathbb{N} \to \mathbb{N}$ such that if $c(g)\leq n, $ then $|G|\leq f(n).$ I know that if $G$ is a finite group such that any two non-identity elements in $G$ are conjugate, then $G$ is isomorphic to $\mathbb{Z}_2$. The proof is simple, infact: It can easily be shown that $G$ acts transitively by conjugation on $G \setminus \{e\}$. Then, by  orbit-stabilizer theorem, the claim follows. I wonder if similar idea could be useful somehow. But I can't think of any to prove the claim above. Please help!","['group-theory', 'abstract-algebra']"
1034335,What is the difference between convergence of a sequence and convergence of a series?,"I'm preparing for my calculus exam and I'm unsure how to approach the question: ""Explain the difference between convergence of a sequence and convergence of a series?"" I understand the following: Let the sequence $a_n$ exist such that $a_n =\frac{1}{n^2}$ Then $\lim_{n\to\infty} a_n=\lim_{n\to\infty} \frac{1}{n^2}=0$ therefore $a_n$ converges to $0$. And the series $\sum_{i=1}^{n}a_n=1 + \frac{1}{4} + \frac{1}{9} + \frac{1}{16} + ... +\frac{1}{n^2}$ And by the $n$-th term test, this series converges. But, I don't understand why or how the convergence between the series and the sequence is different. I looked online and I find a lot of answers on how to determine convergence or divergence, but the only difference I've found is that you use limits to test sequences and series have more complex testing requirements. Please help!","['sequences-and-series', 'convergence-divergence', 'calculus']"
1034340,"Reworking $\sum_{n \leq x} \frac{1}{n^s}$, where $n$ is relatively prime to some fixed $k$","For a fixed integer $k \geq 1$ and real $s>0$ I want to rework the partial sums $$\sum_{\substack{
  n \leq x \\
  \text{gcd}(k,n) = 1
}}
\frac{1}{n^s}$$ in such a way that I can find an asymptotic formula for them.  I've tried to express the $\text{gcd}(k,n) = 1$ condition in various ways involving $\mu$; for example,
$$\sum_{\substack{
  n \leq x \\
  \text{gcd}(k,n) = 1
}}
\frac{1}{n^s} = \sum_{n\leq x}\frac{1}{n^s}\sum_{d | \text{gcd}(k,n)} \mu(d),$$ since $\sum_{d | \text{gcd}(k,n)} \mu(d)$ is equal to 1 if $k$ and $n$ are relatively prime, and 0 if they aren't. But I can't see where to go from here; the difficulty seems to be that $k$ isn't an index of the summation. (This is exercise 3.12 in Apostol's Introduction to Analytic Number Theory.)","['asymptotics', 'summation', 'number-theory']"
1034343,Prove $W \cap W^\perp =\{\vec{0}\}$,"If $W$ is a subspace of $\mathbb{R}^n$, then $W^\perp = \overline{W} =
 \{v \cdot w = 0, \forall w \in W\}$ Prove $W \cap W^\perp = \{\vec{0}\}$. How do I fully prove this intersection is $\vec{0}$? I showed $\vec{0}$ is in $W$, $\overline{W}$, since they are both subspaces of $\mathbb{R}^n$, thus $\vec{0}$ is a subset of $W \cap \overline{W}$  which is also a subset of $\vec{0}$. Then I must show $u = \vec{0}$ to complete. I have let $u$ be an element of  $W \cap \overline{W}$  which implies $u$ is an element of the zero vector (because they are subsets of each other). But does this show $u$ must be equal to $\vec{0}$? If not, how do I show that part?","['vector-spaces', 'linear-algebra']"
1034362,"$\phi(v)/\Phi(v)$ is decreasing for $\phi$ and $\Phi$ being the PDF and CDF of $N(0,1)$","Let $\phi(v)$ and $\Phi(v)$ denote, respectively, the PDF and CDF of the standard normal distribution. How would one show that
$$
\frac{\phi(v)}{\Phi(v)}
$$
is decreasing? I tried the quotient rule but couldn't handle the term $\Phi(v)\phi'(v)$.","['probability-theory', 'inequality', 'algebra-precalculus', 'integration']"
1034409,Circle radius as variable,"I am confused. How is $y^2 + x^2 =3x$ a circle? Can someone please help me try to understand why the above a circle, or is it just a typo?","['analytic-geometry', 'geometry']"
1034455,Defeating Russell's paradox,"I am not very big in mathematics yet(will be hopefully), naive set theory has a problem with Russell's paradox , how do they defeat this sort of problem in mathematics? Is there a greater form of set theory than naive set theory that beats this problem? (Maybe something like superposition if is both or neither)?",['elementary-set-theory']
1034471,"If f ' = 0, then f is constant?","I'm a little confused.  After finishing the online multi-variable calculus course from the MIT OCW offerings (I wanted to brush up on the subject more concretely, after my Analysis II course), I looked at another brisk course on a single-variable calculus course. The hope was to revisit calculus, after a couple years of rigorous analysis courses. But, my question is:  in the MIT OCW course, the prof. had mentioned a few times that: If the first order derivatives of f were 0, then f is not actually constant, but rather it is just not changing, to first order.  There are obviously higher order terms in its Taylor development. But when I return to single-variable calculus, I have seen several times now that some theorems and proofs argue that if f' = 0, then f is constant.  But isn't f just...not changing, to first order - and that it's not actually constant?  It may or may not have higher order, non-vanishing terms in its Taylor development. Thanks in advance,","['multivariable-calculus', 'calculus', 'partial-derivative', 'derivatives', 'taylor-expansion']"
1034520,Prove $(b-a)\cdot f(\frac{a+b}{2})\le \int_{a}^{b}f(x)dx$,"Let $f$ be continuously differentiable on $[a,b]$. If $f$ is concave up, prove that $$(b-a)\cdot f\left(\frac{a+b}{2}\right)\le \int_{a}^{b}f(x)dx.$$ I know that (and have proved) $$(b-a)\cdot f\left(\frac{a+b}{2}\right)= \int_{a}^{b}f(x)dx$$ for any linear function on $[a,b]$. Also, the graph of $f$ lies above the tangent line at $(\frac{a+b}{2},f(\frac{a+b}{2}))$.","['inequality', 'calculus', 'integration']"
1034541,Proving $f(z)$ entire function in complex analysis,"If $f\in C(\Bbb C)\cap H(\Bbb C\backslash \delta B_1(0))$ then $f\in H(\Bbb C)$ [C means continuous, $\Bbb C$ means complex plane, H means analytic and $\delta$ means boundary] I don't even know where to start. Thank you...",['complex-analysis']
1034579,"Prove reduction formula for $\int \cos^n (x)\sin^m (x) \, dx$","$$\displaystyle\int \:\sin^n\left(x\right)\cos^m\left(x\right)\mathrm  dx=\frac{\sin^{n+1}x\cos^{m-1}x}{m+n}+\frac{m-1}{m+n}\int \:\sin^nx\cos^{m-2}x\,\mathrm dx$$ I have been trying to solve for over a week now can someone please help me.","['calculus', 'integration', 'indefinite-integrals']"
1034612,Dimension of irreducible variety,"Why is the dimension of intersection, $V\cap H$, of $m$-dimensional irreducible variety $V$ and a hyperplane given by $\dim(V\cap H)$ of dimension $m-1$?","['commutative-algebra', 'algebraic-geometry']"
1034632,the first $2k$ terms of the power series of $\sec x + \tan x$ at $x=-\pi/2$,"We know the power series of $\sec x+\tan x$ is as follows, $f(x)=\sum_{n\geq 0}\frac{E_n}{n!}x^n$, where $E_n$ is Euler Zigzag numbers and clearly the radius of convergence of $f(x)$ is $\pi/2$. Suppose $f_k=\sum_{n\geq 0}^{2k-1}\frac{E_n}{n!}x^n$, i.e., the sum of the first $2k$ terms. Obviously $f(x)$ diverges at $-\frac\pi2$. However it seems $\lim_{k\rightarrow \infty}f_k(-\frac{\pi}2)$ converges. I calculated first several terms of $f(x)$ and found $f_k(-\frac\pi 2)$ seems to converge to $-\frac2\pi$. But I have no idea to prove it. Can someone give me some hints? Thanks in advance!","['power-series', 'trigonometry', 'complex-numbers']"
1034653,Specific piecewise-function SAT2 question,"Taken from Barron's SAT Math Level 2 prep book: If f(x) = i, where i is an integer such that i ≤ x < i + 1, the range of f(x) is (A)  the set of all real numbers
       (B)  the set of all positive integers
       (C)  the set of all integers
       (D)  the set of all negative integers
       (E)  the set of all nonnegative real numbers Here's how I tried to solve it: i has to be an integer, which eliminates answer choices A and E. I then tried to manipulate the inequality to focus it on i : i ≤ x AND x < i + 1 x - 1 < i therefore: x - 1 < i ≤ x Now there are no constraints on our x so whatever its value is, i is going to be an integer less than or equal to x and more than x-1, which led me to answer choice C.
The correct answer according to Barrons is D. I can't think of a reason why i has to be a negative integer. So why is it so and is there a better way of solving it (using a graphic calculator is allowed.)? EDIT: I apologize. Just checked a newer edition and apparently this is a typo and should be C. Sorry.","['algebra-precalculus', 'functions']"
1034654,"0 to the power of 0, what does the essential discontinuity actually look like?","So having watch this clip by Numberphile which explains why $0^0$ is undefined https://www.youtube.com/watch?v=BRRolKTlF6Q And also this Link And also this Zero to the zero power – is $0^0=1$? I understand how when given a function of the form $x^y$ then you have the following results $$\lim_{x\rightarrow 0} x^0=1,$$ $$\lim_{y\rightarrow 0^+} 0^y=0,$$ and both the video and the mathforum link (and numerous others) mentioned about how when approaching form different directions you get different answers However, both google and mathematica failed to actually show what the discontinuity look like While a similar problematic case of "" $\frac{0}{0}$ "" indeterminate form gives rather consistently the shape of the essential singularity attempt to do so for the case $x^y$ in google and mathematica does not really illuminate the shape of the essential singularity By approaching zero from the x axis, and also analysing the x and y partial derivatives of the function $x^y$ I can clearly see a jump However I don't quite get what the result for the x^y cases as you approach from directions that are not x or y axis geometrically look like For example, for the "" $\frac{0}{0}$ "" case it is easy to see why Numberphile said the value of $\frac{0}{0}$ depends on the angle you approach the origin But for $0^0$ ... While algebraically the limits clearly evaluated to different values depending on how we approach it, geometrically it does not seeemed to agree with what the limits said (the curve look smooth and continuous...probably due to the limitation of the graphing programs) ""==============================================="" So to sum up my question, what does the discontinuity of $0^0$ actually look like, is it like an oscillation, a jump, a dot or something more complicated? ""==============================================="" UPDATE to ask for clarification, which should be in the comment section had it has a ""posting image function"") Using the answers of Aes and Meelo, and examining the plots of the curves and parametric curves used by them to investigate the limiting behavior, I got the following So the singularity has a shape of the vertical line interval (0,1) as mentioned by Aes and its neighborhood is as mentioned by Meelo has a very steep but non vertical ""fault like structure""near the x axis which using the general form of a curve that can give limits that are between 0 and 1, as shown by Aes, explains why we need to travel along these curves (so that part of our journey is on that steep fault) in order not not get dragged into the value of 1 Or in short, based on the answers, is my understanding as shown below the correct way to understand it?","['calculus', 'visualization', 'real-analysis']"
1034665,"How to put a matrix in Jordan canonical form, when it has a multiple eigenvalue?","Put the matrix $$\begin{bmatrix} 3 & -4\\ 1 & -1\end{bmatrix}$$ in Jordan Canonical Form.  Moreover, find the appropriate transition matrix to the basis in which the original matrix assumes its Jordan form. I'm having a lot of trouble with this.  I know that the eigenvalue has multiplicity two and is $\lambda = 1$.  I can find the first eigenvector, which is: \begin{bmatrix}
        2 \\
        1  \\
        \end{bmatrix}
I'm having trouble finding the second since both eigenvalues tell us the same thing.  But I'm not nearly as concerned about the eigenvectors as I am about what to do after. If anyone could explain thoroughly the next steps involved (not necessarily the answer but how to obtain it), I would be forever grateful.  This homework is in 2 days and it may determine my grade letter.","['jordan-normal-form', 'matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
1034676,What is the purpose of Jordan Canonical Form?,"I don't claim at all to be an expert on this topic. In many (advanced) linear algebra textbooks for undergraduates, I usually find something about the ""Jordan Canonical Form"" of a matrix. What is the purpose of such a form? I have taken a usual first-course in linear algebra (did another semester with Axler, but I don't claim to be an expert) and have taken abstract algebra (most familiar with group and ring theory) and have briefly skimmed through linear algebra books covering this material, but I don't quite understand the ""big picture"" idea, i.e., why is this useful in application? One person once told me it is the ""most straightforward and useful algorithm for solving systems of linear equations, once you get beyond 3 variables or so,"" but maybe I'm missing something, since I usually don't see anything like what this person described to me in the linear algebra books I have. Most textbooks I've seen tend to have a more theoretical focus on this topic. Also, any suggested texts which have good coverage on this topic would be very helpful.","['jordan-normal-form', 'matrices', 'linear-algebra']"
1034701,Do real-analytic functions always extend uniquely to complex-analytic functions on $\mathbb{C}$?,"A function $f(x)$ is an real function and analytic in an open interval of $x$-axis or the whole $x$-axis. Is there only unique way to analytically extend it to the whole complex plane? I know identity theorem for holomorphic functions, but it requires that two functions equal in 
an open and connected set, while here I only need they equal in an open interval of x-axis which is a closed set of complex plane.","['analyticity', 'complex-analysis']"
1034736,"If the sum of two i.i.d. random variables is normal, must the variables themselves be normal?","It is well known that if two i.i.d. random variables are normally distributed, their sum is also normally distributed. Is the converse also true? That is, suppose $X$ and $Y$ are two i.i.d. random variables such that $X+Y$ is normal. Is it necessarily the case that $X$ and $Y$ are also normal? Thoughts: If the pdfs of $X$ and $Y$ are both $f$, the pdf of $X+Y$ is $f*f$, and its Fourier transform is $\hat f^2$. So if $\hat g=\hat f^2$ is a Gaussian function, then $\hat f$ could be any pointwise product of $\sqrt{\hat g}$ and an even function $\mathbb R\mapsto\{-1,1\}$. But it is not clear that any of those correspond to probability distribution functions, which must be nonnegative everywhere.","['probability-theory', 'normal-distribution', 'probability-distributions']"
1034768,"Combinations of fruits and their ""nutrients""","As a computer scientist and not a mathematician, I know not some of the formal language to describe my problem, so I'll present it in a word problem form. Maybe someone can help me hone my search and vocabulary. Suppose we have a set of fruits, costs, and their associated provisions Apples cost \$1 and provide 1HP, 3MP, 2STA Oranges cost \$2 and provide 2HP, 3STA Pears cost \$3 and provide 2MP, 4STA If I require 10HP, 7MP, and 12STA, how do I optimize for the following: Generate fruit combinations which go as minimally in excess of the requirements (you know, because wasting fruit is bad) Consume as few fruits as possible to fully satisfy the requirements Spend as little money as possible to fully satisfy the requirements The problem illustrated is obviously trivial, but if the number of fruits can approach dozens or even hundreds, and the number of required parameters HP, MP, STA, etc. can also reach dozens, the combinatorial space becomes computationally prohibitive, so I need another approach. What are some possible alternatives that may not be perfect, but give an asymptotically increasingly accurate answer? Some approach thoughts I've had but haven't fully pursued due to intuited problems with the approach: Brute force (the permutations become innumerable and the computation time even worse) Multidimensional hill-climbing algorithm with a distribution of starting points (not sure what ""up"" means in this approach) Linear Programming (The selection process doesn't appear linear? Maybe I don't understand the formal definition of linear here) Network Flow (not sure how to encode edges, and it looks like edges end up being as numerous as in the brute force approach)","['integer-programming', 'ordinary-differential-equations', 'linear-programming', 'combinatorics']"
1034785,Pronunciation of `Rng` - the non-unital Ring,"I chuckled the first time I heard that a Ring without a multiplicative identity (R i ng without the i ) is called a Rng (pronounced wrong ). According to Wikipedia , it's pronounced rung . How is Rng pronounced in research/academia?","['ring-theory', 'rngs', 'abstract-algebra']"
1034830,What is the limit of this trig function?,"How do I find $$\lim_{x \to \pi/4}{\frac{\cos x-\frac{1}{\sqrt2}}{x-\frac\pi4}}$$? I've tried setting the denominator equal to $h$, then replacing $x$ in terms of $h$, but I still don't know how to manipulate it to look like $\frac{\cos h-1}h$.","['trigonometry', 'calculus']"
1034855,Why do non-real solutions of a polynom occur pairwise complex-conjugated?,"So if I have a polynom with real coefficients and the solution $x+iy$, why is $x-iy$ always a solution too? Let $z$ and $w$ be complex numbers, with $w^{\ast}$ = complex-conjugated of $w$, then
$$(z-w)(z-w^{\ast}) = z^2 -2z\mathrm{Re}(w) + |w|^2$$
All coefficients are real, but how do I prove the other direction?",['algebra-precalculus']
1034867,"Laurent series, integral over the annulus, radii","We are given $$f = \sum_{n= - \infty} ^{\infty} a_n (z-z_0)^n \in \mathcal{O} ( \text{ann} (z_0, r, R)), \ \ 0<r<R< \infty. $$ Prove that $$\frac{1}{\pi} \int _{ann (z_0, r, R)} |f(z)|^2 d \lambda(z) = \sum _{n \neq -1} \frac{R^{2n+2} - r^{2n+2}}{n+1}|a_n|^2 + 2 \log \frac{R}{r}|a_{-1}|^2.$$ I know that $$a_n = \frac{1}{2 \pi i} \int_{\partial B(z_0, \rho)} \frac{f(s)}{(s-z_0)^{n+1}}ds, \ \ \rho \in (r,R), \ n \in \mathbb{Z}$$ We know that the series above is convergent, so $R = \frac{1}{\limsup_ {n \rightarrow \infty} \sqrt[n]{|a_n|}}$ and $r = \limsup_ {n \rightarrow \infty} \sqrt[n]{|a_{-n}|}$. In the series $$\sum _{n \neq -1} \frac{R^{2n+2}}{n+1}|a_n|^2, \ \ \sum _{n \neq -1} \frac{r^{2n+2}}{n+1}|a_n|^2$$ we have $b_n = \frac{|a_n|^2}{n+1}$ and radii of convergence are $$R'=\frac{1}{\limsup_ {n \rightarrow \infty} \sqrt[n]{\frac{|a_n|^2}{n+1}}} \ge \frac{1}{\limsup_ {n \rightarrow \infty} \sqrt[n]{|a_n|^2}} \ge \frac{1}{\limsup_ {n \rightarrow \infty} \sqrt[n]{|a_n|}}^2 = R^2,$$ so $R' \ge R^2$, and similarly, $$r' = \limsup_ {n \rightarrow \infty} \sqrt[n]{\frac{|a_{-n}|^2}{n+1}} \le \limsup_ {n \rightarrow \infty} \sqrt[n]{|a_{-n}|^2} \le r^2.$$ So the two series have radii of convergentce $R', r'$, assuming they have the form $\sum b_n (z-z_0)^n$ with $z=R^2$ or $-r^2$. Does that make sense? Could you tell me how to prove the equality of the integral and the series?","['laurent-series', 'analyticity', 'complex-analysis']"
1034875,Solve $3^a-5^b=2$ for integers a and b.,"So I have got that (a,b)=(1,0),(3,2) are solutions for the equations, and maybe the only one.","['exponential-diophantine-equations', 'number-theory']"
1034880,Poisson Process: Finding the sum of interarrivial time,"One hundred items are simultaneously put on a life test. Suppose the
  lifetimes of the individual items are independent exponential random
  variables with mean $200$ hours. The test will end when there have
  been a total of $5$ failures. If $T$ is the time at which the test
  ends, find $E[T]$. My attempt:
Let $T_i$ be the interarrivial time between the $(i-1)$th failure and $i$th failure. Then we have $$T= \sum_{i=1}^5 T_i$$ which is quickly followed by $$E[T]= \sum_{i=1}^5 E[T_i].$$ I guess there is no problem above. I now claim that interarrivial time is independent exponential r.v.. Thus, I have $E[T_i] = \frac{1}{200}$, which differs from the book answer: $E[T_i] = \frac{200}{101-i}$. Can anyone point out my mistake? Thanks in advance.",['probability-theory']
1034887,Finding inverse of a function $h(x) = \frac{1-\sqrt{x}}{1+\sqrt{x}}$,"I have a function: $$h(x) = \frac{1-\sqrt{x}}{1+\sqrt{x}}$$ With just pen and paper, how can I determine if there exists an inverse function? Am I supposed to sketch it on paper to see if it can have an invers? Or is there another/simplier way to do it? How would I go about solving it? This is what I did:
$$h(x) = \frac{1-\sqrt{x}}{1+\sqrt{x}} = y$$
$$y(1 + \sqrt{x}) = 1-\sqrt{x}$$
$$y + y\sqrt{x} = 1-\sqrt{x}$$
$$\text{Cancel out roots:}$$
$$y^2 + y^2x = 1+x$$
$$y^2 + y^2x - x = 1$$
$$y^2 + x(y^2 - 1) = 1$$
$$x(y^2 - 1) = 1 - y^2$$
$$x = \frac{1 - y^2}{y^2 - 1}$$
$$\text{Swap x and y and we get the inverse function:}$$
$$f^{-1}(x) = \frac{1 - x^2}{x^2 - 1}$$
$$\text{But the correct answer is supposed to be:}$$
$$f^{-1}(x) = \frac{(1 - x)^2}{(1 + x)^2},  -1< x \le 1$$ What am I doing wrong?","['inverse', 'algebra-precalculus', 'functions']"
1034901,A converse of schur's lemma,"Suppose $\rho: G \rightarrow GL(V)$ is a representation. and if $T: V \rightarrow V$ is a linear operator such that $T\circ \rho_g= \rho_g\circ T$ for all $g\in G$ implies $T=k\cdot Id$ for some number $k$. (i.e. $T$ is $G$-invariant/ $G$-intertwining implies $T$ is a homothety). Prove that $\rho$ is irreducible. Attempt: Suppose $W$ is a vector subspace of $V$ such that $\rho_g (W) \subseteq W$ for all $g\in G$, I want to prove that $W=\{0\}$ or $V$. The only theorem I have learnt to prove irreducibility is $\langle \chi,\chi \rangle=1$ but I dont think it is useful here. But then I have no idea how to make use of the condition ""the only $G$-interwining linear operator is homothety"".  Please helps","['representation-theory', 'abstract-algebra']"
1034943,Is there really anything wrong with Bourbaki's Set Theory?,"Recently I have started reading Bourbaki's Theory of Sets on my own. Regarding one of the explanations of a concept when I went to a Professor of our college, he asked me why I was wasting my time reading a book which contains so many pitfalls . Besides, he also told me that Bourbaki's treatment of Set Theory is wrong . When I asked him about examples, he told me that he thought I would be able to find it myself. Now this is something that is highly contradicting because here in D. Miller 's answer I found that, ...Bourbaki is very far from being suitable for everyone. That said, if you're looking for a reference that's axiomatic, super-abstract, and works in greatest possible generality, and is also crystal clear and careful, Bourbaki is the place to go. ... And this contradicts the expression I got from my professor. Besides, I himself wasn't able to find out any 'pitfall' in the book so far. Is there really anything wrong with Bourbaki's Set Theory? Update Though Asaf Karagila was very patient with me and answered all my queries, I will be very glad if someone who has gone through Bourbaki's Theory of Sets answers it in detail.","['elementary-set-theory', 'soft-question']"
1034994,Sum of all triangle numbers,"Does anyone know the sum of all triangle numbers? I.e $1+3+6+10+15+21\cdots$ I've tried everything, but it might help you if I tell you one useful discovery I've made: I know that the sum of alternating triangle numbers, $1-3+6-10\cdots$ Is equal to $\dfrac{1}{8}$ and that to change $1+3+6\cdots$ Into $1-3+6\cdots$ You would subtract $6+20+42+70\cdots$ which is every other triangular number (not the hexagonals) multiplied by two. $\dfrac{1}{8}$ plus this value is $1+3+6+10+\cdots$ A final note: I tried to split the triangle numbers into hexagonals and that series and then I got the squares of the odd numbers. Using dirichlet lambda functions This gave me 0 but I don't think this could be right. A number of other sums gave me $\dfrac{1}{24}$ and $\dfrac{3}{8}$ but I have no idea","['divergent-series', 'summation', 'sequences-and-series']"
1035014,Indefinite Integral of Reciprocal of Trigonometric Functions,How to evaluate following integral $$\int \frac{\mathrm dx}{\sin^4x+\cos^4x\:+\sin^2(x) \cos^2(x)}$$ Can you please also give me the steps of solving it?,"['calculus', 'integration', 'indefinite-integrals']"
1035028,Question about complex exponentation,We define: $z^w:=e^{wLogz}$ but in general for complex numbers $wLogz \ne Log z^w$. (Even if we take principial branch). So what is the reasoning for it?,['complex-analysis']
1035040,Nth Derivative of a fucntion,Find the $N^{th}$ derivative of $$f(x) = \sqrt{\frac {1-x}{1+x}}$$ I have got $1^{st}$ derivative as: $\frac{-1}{(1-x)^{1/2}(1+x)^{3/2}}$ and $2^{nd}$ derivative as: $\frac{1-2x}{(1-x)^{3/2}(1+x)^{5/2}}$ and $3^{rd}$ derivative as: $\frac{-6x^2+6x-3}{(1-x)^{5/2}(1+x)^{7/2}}$ I can see how will the denominator gets it form but can you help me with the numerator? Thanks. :),['derivatives']
1035045,"Find $a,b$ to make $V$ a Lyapunov function","Given $V(x,y)=ax^2+by^2$ I'm asked to find $a$ and $b$ to make $V$ a Lyapunov function for the following systems: $(1)$\begin{cases} x'= -x^\color{red}{3}+xy^2 \\ y'= -\color{red}{2}x^2y-y^3\end{cases} Here I have $\dot{V}(x,y)=2ax(-x^2+xy^2)+2b(-y^3)=2ax^3+2ax^2y-4bx^2y^2-2by^4$. The first two termins can take positive and negative values depending on $x$, then here I would set $a=0$; then $b>0$ to make it $\dot{V}(x,y)\leq 0$. $(2)$\begin{cases} x'= -x^\color{red}{3}/2+2xy^2 \\ y'= -y^3\end{cases} In this case is $\dot{V}(x,y)=2ax(-x^2/2+2xy^2)+2b(-y³)=-ax^3-x^2y^2-2by^3$. For the middle term is $-x^2y^2\leq 0$, but $-ax^3-2by^3$ could be less or greater than zero for any $a,b$ chossing an appropiate $x,y$. Then in this case $a=b=0$ is the only possible solution? UPDATE: I copied the problem from my notes instead of the original problem, doing this I messed up some of the exponents. The fixed exponents are those in red, which I before miscopied as 2; I also missed a constant.","['ordinary-differential-equations', 'solution-verification']"
1035049,Confusing Analysis proof,"I have a question about a proof of the Beltrami-Enneper theorem:
In the following $\nu$ is the surface-normal and $e_1,e_2,e_3$ the Frenet 3-frame. It states: Every asymptotic curve $c: I \rightarrow S \subset \mathbb{R}^3$  ( $II(c',c')=0$, where $II$ is the second fundamental form) with curvature $\kappa \neq 0$ and torsion $\tau$ satisfies the equation $\tau^2=-K$, where $\tau$ is the torsion of the curve and $K$ the Gauß-curvature of the surface $S$. Proof: Let $c(s)$ be an aymptotic curve with $II(c',c')=0$. Then the normal curvature of $c$ vanishes. Hence, $e_2$ is tangential to the surface (so far is everything alright), but now it goes on with, then $e_3 = \nu$. 
I mean, I see that $e_3$ is then either $+ \nu$ or $- \nu$, but I don't see why the $+$ sign is already clear here. Then it goes on like $\tau= \langle e_2',e_3 \rangle = \langle e_2', \nu \rangle $ which is just definition and the fact we just proved, but now it is claimed that this is equal to $II(e_1,e_2).$ Then the author claims that 
$K = Det(II)/Det(I) = II(e_1,e_1)II(e_2,e_2) - (II(e_1,e_2))^2 = 0 - \tau^2.$ Apparently, the determinant of the metric tensor is supposed to be equal to one here(why?). You find this proof in this book: It is theorem 3.19 on page 85: see here If anything is unclear, please let me know.","['differential-geometry', 'surfaces', 'real-analysis', 'analysis']"
1035053,Measure-theoretic analog of homeomorphism and isometry,"If $(X,\tau_X)$ and $(Y,\tau_Y)$ are topological spaces and $f:X\to Y$ is a continuous bijective function between them such that $f^{-1}$ is also continuous, then the two topological spaces are said to be homeomorphic and they can be essentially identified through $f$, because their topological properties are the same. Similarly, if $(X,d_X)$ and $(Y,d_Y)$ are metric spaces and $f:X\to Y$ is a bijective isometry between them that preserves distance, then the two metric spaces can be essentially identified through $f$, because their metric properties are the same. Now, suppose that $(X,\Sigma_X)$ and $(Y,\Sigma_Y)$ are measurable spaces and $f:X\to Y$ is a measurable bijective function between them such that $f^{-1}$ is also measurable. Can one say that the two spaces can be essentially identified from a measure-theoretic point of view? Is there a distinguished name for this phenomenon (analogous to “homemorphic” and “isometric” in the previous two cases)? (Please refrain from using category-theoretic arguments in your answer, I'm not familiar with it. Thank you!)","['measure-theory', 'reference-request']"
1035099,Notation laplace operator squared $\Delta^2$,I have the following expression (in a numerical context) $$\Delta_h u(x) = \Delta u(x) + \frac{h^2}{12} \Delta^2 u(x) + O(h^4)$$ The $\Delta$ is the Laplace operator so $\Delta u = u_{xx}+u_{yy}$. But what is $\Delta^2$? In the context it would make sense (but it is not really a strong indication) for it to be $$\Delta^2 u = u_{xxxx}+u_{yyyy}$$ but when I first saw it I just thougth it was $$\Delta^2 u = \Delta (\Delta u) = \Delta (u_{xx}+u_{yy}) = u_{xxxx}+2u_{xxyy}+u_{yyyy}.$$ Which one is correct? Can you provide references where one or the other is used?,"['notation', 'multivariable-calculus', 'laplacian', 'numerical-methods']"
1035183,Character of dual Representation?,"Let $G$ be a finite group and consider the group ring $\mathbb C[G]$. If $M$ is a $\mathbb C[G]$-module consider the dual representation in $M^*=\operatorname{Hom}(M, \mathbb C)$ given by $$(g\cdot f)(m)=f(g^{-1}\cdot m).$$ How can I show $$\chi_{M^*}(g)=\chi_M(g^{-1}),$$ where $\chi_M$ and $\chi_{M^*}$ are the associated characters of the given representations?","['representation-theory', 'abstract-algebra']"
1035202,Let $f$: $X \to Y$ be a function. Show that if $f$ is injective then $f(A \cap B) = f(A) \cap f(B)$ for sets $A \subseteq X$ and $B \subseteq X$.,"Let : $X \to  Y$ be a function. Show that if $f$ is injective then $f(A \cap  B) = f(A) \cap  f(B)$ for
sets $A \subseteq  X$ and $B \subseteq  X$. My answer : Suppose $f$ is injective and $f(x) \in   f(A \cap  B) \Leftrightarrow  x \in  A \cap  B  \Leftrightarrow  x \in  A$ and $x \in  B \Leftrightarrow $ $  f(x) \in  f(A)$ and $f(x) \in  f(B) \Leftrightarrow  f(x) \in  f(A) \cap  f(B)$. Therefore as each step is an equivalence it can be read backwards so $f(A \cap  B) \subseteq  f(A) \cap  f(B)$ and $f(A) \cap  f(B)$ $ \subseteq  f(A \cap  B)$ meaning $f(A \cap  B) = f(A) \cap  f(B)$. What I don't understand is where this proof breaks down if $f$ is not injective ?","['elementary-set-theory', 'functions']"
1035212,Beautifully looking little geometry/trigonometry problem,"Given triangle ABC, a,b,c as its sides, p is a half perimeter, such that $\dfrac{p-a}{11}=\dfrac{p-b}{12}=\dfrac{p-c}{13}$. We need to find $(\tan\dfrac{A}{2})^2$ (A)$\dfrac{143}{432}$  B)$\dfrac{13}{33}$ C)$\dfrac{11}{39}$ D)$\dfrac{12}{37}$ I've tried to substitute the area of the triangle as $S=(1/2)(ab\sin C)=(1/2)(ac\sin B)=(1/2)(bc\sin A)$ and I know that $(\tan A/2)^2=(1-\cos A)/(1+\cos A)$ And some other formulas, but how is $\tan A/2$  derived from the given? Sorry, the problem's given was confused, p is half-perimeter.","['geometry', 'trigonometry', 'algebra-precalculus']"
1035213,Does $\sum |\sin n| / n$ converge? [duplicate],"This question already has answers here : Covergence test of $\sum_{n\geq 1}{\frac{|\sin n|}{n}}$ (3 answers) Closed 9 years ago . How can I prove if the following series converges? $$\sum_{n\geqslant1} \frac{|\sin n|}{n}$$ I can't use differential or integral calculus. I've tried using Dirichlet and Cauchy tests, but they didn't get me anywhere.","['sequences-and-series', 'real-analysis']"
1035217,"Preorder traversal, inorder traversal, postorder traversal","a) preorder traversal b) inorder traversal c) postorder traversal Ok, a) r,j,h,g,e,d,b,a,c,f,i,k,m,p,s,n,q,t,v,w,u b) a,b,d,c,e,g,f,h,j,i,r,s,p,m,k,n,v,t,w,q,u c) a,b,c,d,e,f,g,h,i,j,v,w,t,u,q,n,s,p,m,k,r I'm definitely doing some mistakes here, but that was my try. Can anyone help me with this?","['trees', 'discrete-mathematics']"
1035234,Definition of a Manifold from Guillemin Pollack,"I have been studying differential topology from Guillemin and Pollack (GP).
Unlike many other books that define differentiable manifolds using maximal atlases GP starts by saying $ X \subset R^{N}$ for some ambient space $R^{N}$ and then goes on to define a $k$ dimensional manifold. But I know that this containment comes due to a weak version of Whitney's theorem. Later on when they prove Whitney's theorem it is done so by induction on $N >= 2k+1$. But how to I justify that $ X \subset R^{N}$ in the first place? How can it just be assumed in definition like that? I need help getting from the general definition of manifolds using atlases to the weak version of Whitney. Thanks","['differential-topology', 'multivariable-calculus', 'differential-geometry']"
1035244,How to prove there is no bijection between a set and its second power set?,"Let $X$ be a set. How can I show that there is no bijection between $X$ and $P(P(X))$, the powerset of the powerset of $X$. I know that there is no bijection between $X$ an $P(X)$, due to Cantor's theorem. Since $P(P(X))$ has a 'larger' cardinality than $P(X)$, the result should follow immediately. I just have trouble writing down the proof.",['elementary-set-theory']
1035282,Why those division by zero are formalized?,"Easy example first: $f(x) = nx$ $f'(x) = (f(x+0)-f(x))/0 = (nx+0n-nx)/0 = (0n)/0 = n$ Hard one: $f(x) = a^x$ $f'(x) = (f(x+0)-f(x))/0 = (a^{x+0}-a^x)/0 = (a^x(a^0-1))/0 = (a^x(e^{\ln(a^0)}-1))/0 = (a^x(e^{0\ln(a)}-1))/0$ Place $e^x = 1 + x + x^2/2 + ... + x^k/!k$ in $f'(x)$. $f'(x) = (a^x(1 + 0\ln(a) + ... - 1))/0 = (a^x(0\ln(a) + ...))/0 = a^x(\ln(a) + (...)/0)$ Discard values that are $0$ that we can't rescue them with that $/0$. $f'(x) = a^x(\ln(a)) = a^x\ln(a) $ Why is this working? Is there a name for this type of equations? (Or type of a way of solving equations) This way of solving involves expanding $x$ of $x/0$ until some $*0$ to elimnate the outside $/0$. But as you see this is not exactly limit, but I tagged it because it's similar.","['nonstandard-analysis', 'infinitesimals', 'limits']"
1035294,Find $E(X)$ and $Var(X)$,"In a box there are $30$ balls, $20$ are black and $10$ are red. Let $X$ be the number of red in a selection of two balls drawn without replacement then $$X=I_1 + I_2$$ where $I_1 = 1$ if red is drawn first, else 0 the same thing applies to $I_2$ in the second draw. Find $$E(I_1),\,\, E(I_2),\,\, E(I^2 _1),\,\, E(I^2 _2),\,\, E(I_1 I_2), \,\,E(X)$$ and $Var(X)$.","['statistics', 'probability']"
1035308,Computing $\lim_{x\to\infty}{3^x\over \sqrt {9^x - 4^x}}$,Compute $$\lim_{x\to\infty}{3^x\over \sqrt {9^x - 4^x}}$$ Can one use end behavior to solve this? I.E. $$\lim_{x\to\infty}{\sqrt{3^{2x}\over (3^{2x} - 2^{2x})}}$$ and therefore divide $3^{2x}$ by $3^{2x}$ ?  leaving us with an anwser of $1$ ?,"['calculus', 'limits']"
1035322,Implicit Function Theorem exercise,"I did an exercise in the book Vector Calculus [Marsden & Tromba] and I check my answer in the book Vector Calculus Study Guide and Solutions Manual [Karen Pao, Frederick Soon] but my answer is not the same and I don't understand why. Is it possible to solve the system of equations
  \begin{align}
xy^2+xzu+yv^2 & = 3 \\
u^3yz+2xv-u^2v^2 &= 2
\end{align}
  for $u(x,y,z)$, $v(x,y,z)$ near $(x,y,z)=(1,1,1)$, $(u,v)=(1,1)$? Compute $\partial u/\partial y$ at $(x,y,z)=(1,1,1)$. This is my answer:
\begin{align}
f_1(x,y,z,u,v) &= xy^2+xzu+yv^2-3 \\
f_2(x,y,z,u,v) &= u^3yz+2xv-u^2v^2-2
\end{align}
$$\Delta =
 \frac{\partial(f_1,f_2)}{\partial (u,v)} =
 \begin{vmatrix} xz & 2vy \\ -2uv^2+3u^2yz & -2u^2v+2x\end{vmatrix}$$
$$\left.\Delta\right|_{(x,y,z,u,v)=(1,1,1,1,1)} = \begin{vmatrix} 1 & 2 \\ 1 & 0\end{vmatrix} = -2 \neq 0$$
Thus, it is possible to solve in terms of $x$, $y$, $z$ at the given point. To compute $\partial v/\partial y$ I derive implicitly:
$$
\begin{align}
\frac{\partial f_1}{\partial y} =
 2xy+xz\frac{\partial u}{\partial y}+v^2+2yv\frac{\partial v}{\partial y} &= 0 \\
\frac{\partial f_2}{\partial y} = 3u^2yz\frac{\partial u}{\partial y}+u^3z+2x\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0
\end{align}
$$
For $(x,y,z)=(1,1,1)$ we have:
$$
\begin{align}
2+\frac{\partial u}{\partial y} + v^2+2v\frac{\partial v}{\partial y} &= 0 \\
3u^2\frac{\partial u}{\partial y}+u^3+2\frac{\partial v}{\partial y}-2uv^2\frac{\partial u}{\partial y}-2u^2v\frac{\partial v}{\partial y} &= 0
\end{align} \tag{1}\label{1}
$$ And, after a boring time, I get:
$$\frac{\partial v}{\partial y} = \frac{6u^2+3u^2v^2-u^3-4uv^2-2uv^4}{2+4uv^3-2u^2v-6u^2v}$$ But in the Solutions Manual , they substitute also $(u,v)=(1,1)$ in $\eqref{1}$(which is much faster) and their answer is
$$
\frac{\partial v}{\partial y} = -1
$$ If that's correct, I don't understand it because in the exercise they ask us to compute $\partial v/\partial y$ at $(x,y,z)=(1,1,1)$","['multivariable-calculus', 'calculus', 'real-analysis', 'analysis', 'implicit-function-theorem']"
1035355,Solving $\sin(2v) = \sin(v)$,"$$\sin(2v) = \sin(v)$$
Why can't this equation be solved by setting:
$$2v = v + 2\pi n \quad \leftrightarrow  \quad v = 2\pi n\\2v = \pi - v + 2\pi n \quad\leftrightarrow \quad 3v = \pi + 2\pi n  \quad \leftrightarrow \quad v = \frac{\pi}{3} + \frac{2\pi n}{3}$$ This is how I've seen other similar equations being solved. Please note that I was able to solve the equation by setting $\sin(2v) = 2\sin(v)\cos(v)$, I just want to know why this particular equation not can be solved the way I presented. The correct solutions are (according to my literature): $v = \pi n$ or $v = \pm \frac{\pi}{3} + 2\pi n$","['trigonometry', 'algebra-precalculus']"
1035358,Mapping the open ball to itself?,"How to prove that there exists a continuous function $f:B^2 \to B^2$ without constant points? Here, $B^2$ is the unit open ball. I guess $f$ can be for example like this $f: re^{iax} \to re^{ibx} $ but in what sense it does not include constant points?","['general-topology', 'continuity', 'algebraic-topology', 'functions']"
1035372,"Ordinary differential equations of the form $M(x,y)dx+N(x,y)dy=0$ question","An ODE of the form $M(x,y)dx+N(x,y)dy=0$ is called ""good"" if $\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$ We are given the differential equation $(3x^2y+2xy+y^3)dx+(x^2+y^2)dy=0$. This ODE is not ""good"". We are asked to find $\mu (x,y)$ such that: $$\mu (x,y)(3x^2y+2xy+y^3)dx+\mu (x,y)(x^2+y^2)dy=0,  (*)$$ 
is ""good"". What I did: if the equation $(*)$ is good then $\mu_y (x,y) M(x,y)+\mu (x,y)M_y (x,y)=\mu_x (x,y)N(x,y)+\mu (x,y)N_x (x,y)$ so we get $\mu_y(x,y)(3x^2y+2xy+y^3)+\mu(x,y)(3x^2+2x+3y^2)=\mu_x(x,y) (x^2+y^2)+2x
\mu(x,y)$ And now I'm stuck. Even if we were to guess $\mu_x(x,y)=0$ or $\mu_y(x,y)=0$ we will never get something like $\frac{\mu_y}{\mu}=\phi(y)$ or $\frac{\mu_x}{\mu}=\psi(x)$. $\mu$ seems to depend on both variables and unless the above restrictions apply (which they don't here) I don't know how to find $\mu$. Please help.","['ordinary-differential-equations', 'calculus', 'integration', 'derivatives']"
1035389,Help finding the residue of $1/(z^8+1)$,"Help finding the residue of $1/(z^8+1)$ I'm integrating over $\{ Re^{it} | 0 \leq t \leq \pi \}$, and I found 4 simple poles at $z_0=e^{in\pi/8}$ where $n = 0,...,3$ and I'm trying to calculate $res(1/(z^8+1),z_0)$ 
calculating this: $$\lim_{z\to z_0} (z-z_0)f = \lim_{z\to z_0}\frac{z-z_0}{1+z^8},$$ now are there any algebra tricks I can do to simplify this?","['residue-calculus', 'complex-analysis']"
1035393,Minimal polynomials and cyclic subspaces,"I'm trying to make my way through two problems in Curtis's Linear Algebra , chapter 25.  One of the two problems is this one, #5: Prove that $V$ is cyclic relative to a linear transformation $T \in \mathcal{L}(V)$ if and only if the minimal polynomial of $T$ is equal to the characteristic polynomial. Part of the issue is that I'm not really sure what it's asking. The statement that $V$ is cyclic seems to suggest that the entire vector space (say it has dimension $n$) is generated by some vector $v \in V$, along with $Tv, T^2v, \ldots T^{n-1}v$; Curtis's definition of a cyclic subspace (as opposed to a space, though I can't see why there should be a difference) seems to reinforce this: A $T$-invariant subspace $V_1$ of $V$ is called cyclic relative to $T$ if $V_1 \neq 0$ and there exists a vector $v_1 \in V_1$ such that $V_1$ is generated by $\{ v_1, Tv_1, T^2v_1, \ldots T^kv_1 \}$ for some $k$. However, I've seen several examples that seem to directly contradict this interpretation. For example, we recently had a take-home midterm that required we find elementary divisors, rational canonical form, etc., of a $6\times 6$ matrix; a classmate who received a perfect score on her test found that her matrix's characteristic polynomial was equal to its minimal polynomial — it was $(x+8)^2 (x+14)^2 (x^2+61x+80)$ — and yet the vector space could be written as $\left< v_6 \right> \oplus \left< v_4 \right> \oplus \left< -v_2+v_3+v_6 \right>$ — i.e., as the direct sum of three cyclic subspaces, not as one cyclic subspace. So that suggests that ""$V$ is cyclic"" might mean the direct sum of cyclic subspaces. However, while working on the exercise that follows (#6), I came across this: $$S= \left(\begin{array}{ccc}
-1 & 0 & 0 & 0\\
0 & 1 & 0 & 0\\
0 & 0 & 1 & 0\\
0 & 0 & 0 & 2
\end{array}\right)$$ Here, the characteristic polynomial is not equal to the minimal polynomial, which is $m(x) = (x-1)(x+1)(x+2)$. The eigenspace associated with $\lambda = 1$ has dimension $2$. And so it seems as though, if $V$ has basis $\{ v_1, v_2, v_3, v_4 \}$, we can write $V = \left< v_1 \right> \oplus \left< v_2 \right> \oplus \left< v_3 \right> \oplus \left< v_4 \right> $. This would seem to contradict my second interpretation of the original question. Obviously I'm missing something. Could someone fill me in on what it might be? (I've tried to be thorough, but I'm also trying not to make this ridiculously long; if I've left out necessary information, I'll be happy to append it.)",['linear-algebra']
1035406,Is every regular polygon the unit ball for some norm?,"For every regular polygon, is there a norm such that the polygon is it's unit ball centered on 0?","['geometry', 'normed-spaces', 'functional-analysis']"
1035450,Solving a recurrence with diagonalization?,"Considering the recurrence $F_n=F_{n-1}+3F_{n-2}-3F_{n-3}$ where $F_0=0$, $F_1=1$ and $F_2=2$. Use diagonalization to find a closed form expression for $F_n$. So I first continued the recurrence to find $F_3=5$, $F_4=8$, $F_5=17$ ... etc From this is I got a vector with three consecutive terms in the recurrence to be $u_k = \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix}$ From here get a matrix $A$  from the terms of the recurrence: $A = \begin{pmatrix} F_4 & F_3 & F_2 \\ F_3 & F_2 & F_1 \\ F_2 & F_1 & F_0 \end{pmatrix} = 
\begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix}$ Then the action of $A$ on $u_k$ would produce the $u_{k+1}$ term. [Correct me if I am wrong.] So, $Au_k = \begin{pmatrix} 8 & 5 & 2 \\ 5 & 2 & 1 \\ 2 & 1 & 0 \end{pmatrix} \begin{pmatrix} -3F_{n-3} \\ 3F_{n-2} \\ F_{n-1}\end{pmatrix} = 
\begin{pmatrix} -24F_{n-3} + 15F_{n-2} + 2F_{n-1} \\ -15F_{n-3} + 6F_{n-2} + F_{n-1} \\ -6F_{n-3} + 3F_{n-2} + 0 \end{pmatrix}$ Then $u_0 = \begin{pmatrix} 2 \\ 1 \\ 0 \end{pmatrix}$ From this point on I am unsure how to follow through with completing the problem. I have followed this Fibonacci Recurrence Problem - Most Helpful to figure out what to do next but have fallen short. I see that the next step should be to find $\det(A - \lambda I)$ : $0 = \det(A - \lambda I) = \begin{vmatrix} 8-\lambda & 5 & 2 \\ 5 & 2-\lambda & 1 \\ 2 & 1 & -\lambda \end{vmatrix}$ $=(8-\lambda)\begin{vmatrix} 2-\lambda & 1 \\ 1 & -\lambda\end{vmatrix} - 5\begin{vmatrix} 5 & 1 \\ 2 & -\lambda\end{vmatrix} + 2\begin{vmatrix} 5 & 2-\lambda \\ 2 & 1\end{vmatrix}$ $= (8-\lambda)(-2\lambda^2-1)-5(-5\lambda-2)+2(1+2\lambda)$ $= -16\lambda^{2}-8 + 2\lambda^3 + \lambda + 29\lambda + 12$ $= 2\lambda^3-16\lambda^2+30\lambda+4$ $= 2(\lambda^3-8\lambda^2+15\lambda+2)$ $= 2((\lambda-5)(\lambda-3)\lambda+2)$ From the source above, I can't figure out what the next steps should be? Can anyone help? Thanks!","['diagonalization', 'linear-algebra', 'recurrence-relations']"
1035469,Geometric interpretation of analyticity?,"Suppose the real valued functions $u(x,y)$ and $v(x,y)$ are continuous and have continuous first order partial derivatives in a domain $D$. If $u$ and $v$ satisfy the Cauchy Riemann equations at all points of $D$, then the complex function $f(z)= u+iv$ is analytic in $D$. Could someone please give me a geometric interpretation of the theorem above? The Cauchy Riemann equations can be interpreted as saying the the gradient of $u$ and $v$ must be perpendicular for the function to be differentiable.","['multivariable-calculus', 'calculus', 'intuition', 'complex-analysis']"
1035479,Solve the recurrence $T(n)=aT(n-1)+bn$,"I have to solve the following recurrence, given $T(1)=1$, $$T(n)=aT(n-1)+bn$$ I have done the following: $$T(n)=aT(n-1)+bn \\ =a^2T(n-2)+ab(n-1)+bn \\ =a^3T(n-3)+a^2b(n-2)+ab(n-1)+bn \\ = \dots \\ =a^iT(n-i)+b\sum_{k=0}^{i-1} a^k (n-k)$$ $n-i=1 \Rightarrow i=n-1$ $$T(n)=a^{n-1}T(1)+b \sum_{k=0}^{n-2}a^k (n-k)=a^{n-1}+bn\sum_{k=0}^{n-2}a^k-b \sum_{k=0}^{n-2}k a^k \\ =a^{n-1}+bn \frac{a^{n-1}-1}{a-1}-b \frac{(n-2)a^n-(n-1)a^{n-1}+a}{(a-1)^2}$$ Could you tell me if my result is correct?","['recurrence-relations', 'discrete-mathematics', 'solution-verification']"
1035484,baire category and the union of dyadic balls of rational center,"Suppose that $\{r_n\}_{n=0}^\infty$ is an enumeration of $\mathbb{Q}^N$ and $U = \bigcup_{n=0}^\infty B(r_n,2^{-n})$.  We can use a trivial measure theory argument to prove that $U \neq \mathbb{R}^N$. I was poking around in Real Analysis by Carothers in the section on Baire category (before measure theory has been developed in the book), and I noticed that there's a problem there that asks to prove that $U \neq \mathbb{R}^N$.  There are no hints, but given the placement in the book, the goal is presumably to use Baire category. I have never seen a non-measure based proof of this result, so I gave it a try.  I'm a bit stuck, though.  It's clear that Baire tells us that one of the sets $\bigcup_{n=k}^\infty B(r_n,2^{-n})$ for $k \in \mathbb{N}$ must not be dense, but I'm not sure how to use this information to prove the result.  Any thoughts would be greatly appreciated. EDIT: Here are the details on my above claim.  Set $U_k = \bigcup_{n=k}^\infty B(r_n,2^{-n})$, which is clearly open.  Suppose, BWOC, that each $U_k$ is dense in $\mathbb{R}^N$.  Then Baire says that $\bigcap_{k=0}^\infty U_k$ is dense in $\mathbb{R}^N$, but on the other hand by construction $\bigcap_{k=0}^\infty U_k = \varnothing$.  This contradiction means we must have $E_k$ not dense for some $k$. EDIT 2: As pointed out in the comments below, this argument doesn't seem to fly.  So, back to square one...","['baire-category', 'functional-analysis', 'real-analysis']"
1035521,Proving $\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0$ using limit definition,"Prove $\displaystyle\lim_{n\to\infty} \left(\frac {2}{3}\right)^n=0$ with the definition of limit. From the definition and since $n\in\mathbb N$ I get that ${\Large\mid} \left(\frac {2}{3}\right)^n{\Large\mid}=\left(\frac {2}{3}\right)^n$ but now I'm not sure what to do, I don't see how taking a $\log$ here would help. I thought of different approach, proving by induction that $\left(\frac {2}{3}\right)^n < \frac 1 n$: The first steps are trivial, then for $n+1$: $\frac {1}{n+1}>\frac {2}{3}\left(\frac {2}{3}\right)^n$, then from the induction hypothesis: $\frac {1}{n+1}>\frac {2}{3}\frac 1 n$ and we get that this true for $n>2$ so we can choose $N_{\epsilon}=\frac 1 {\epsilon}$. Is there another way that does not involve induction?","['epsilon-delta', 'calculus', 'limits']"
1035537,Geometric interpretation of Cauchy-Goursat Theorem?,"This theorem seems almost magical. The algebraic derivation doesn't really provide any insight into why it works. So could someone give me a geometric interpretation of it? This: Geometrical Interpretation of the Cauchy-Goursat Theorem? is a similar question but uses another, incorrect, definition of the theorem.","['intuition', 'complex-analysis', 'contour-integration']"
1035575,Circle to circle homotopic to the constant map?,"How to prove that a continuous function, homotopic to the constant map $f:S^1\to S^1$  (a) has a fixed point and that (b) has a point $x$, such that $f$ maps $x$ to its antipodal point $-x$?","['general-topology', 'algebraic-topology', 'functions']"
1035577,Transcendence Degree of the Function Field of Meromorphic Functions over $\mathbb{C}$,"What is the transcendence degree of the field of meromorphic functions over $\mathbb{C}$? By a cardinality argument (meromorphic functions are determined by their image under a countable dense subset and $\mathbb{C}^{\mathbb{Q}}$ has the same cardinality as $\mathbb{C}$), I know a transcendence basis has cardinality at most $|\mathbb{C}|$ but I am unsure if it is exactly this size.","['complex-geometry', 'transcendence-theory', 'complex-analysis', 'field-theory']"
1035604,Differential Equations Constant,"The function $y(x)$ satisfies the linear equation $$y'' + p(x)y' + q(x)y = 0.$$ The Wronskian $W(x)$ of two independent solutions, denoted $y_1(x)$ and $y_2(x)$ , is defined to be $$W(x) = \left|\begin{array}{ccc} y_1 & y_2 \\ y_1' & y_2' \end{array}  \right|.$$ Let $y_1(x)$ be given. Use the Wronskian to determine a first-order inhomogeneous differential equation for $y_2(x)$ . Hence, show that $$y_2(x) = y_1(x) \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t . $$ I get by expanding the determinant and using an integrating factor $$ \frac {y_2(x)} {y_1(x)} =  \int \frac {W(x)}{y_1(x)^2} \mathrm d x $$ hence $$ \frac {y_2(x)} {y_1(x)} -  \frac {y_2(x_0)} {y_1(x_0)} =  \int_{x_0}^x \frac {W(t)}{y_1(t)^2} \mathrm d t . $$ But why can I just decide $y_2(x_0)=0$ if $x_0$ is arbitrary? And if it's for a specific value of $x_0$ (the question is rather vague about this...) then how do I know that in general there exists $x_0$ such that $y_2(x_0)=0$ ?",['ordinary-differential-equations']
1035605,"Prove $x \geq \sin x$ on $[0,\pi/4]$",As the title says.. it says to use the mean value theorem but I don't see how that's applicable. Thank you,"['trigonometry', 'inequality', 'real-analysis']"
1035619,Geometric Interpretation of Antiderivative?,Could someone please give me a geometric interpretation of the above theorem?,"['multivariable-calculus', 'calculus', 'intuition', 'complex-analysis']"
1035634,Spectrum of a Self-Adjoint Operator is Real,"Preparing for an exam in functional analysis, I'm trying to show that for a self-adjoint operator $A$, $\sigma(A) \subset \mathbb{R}$. I came across the following proof in the book (or rather, lecture notes) we're using for the course. The proof is even stronger, giving bounds for the spectrum. However, I have issue with the proof. Here is the proof: Let $m = \inf_{||x||=1}{\langle Ax, x \rangle}$ and $M= \sup_{||x|=1}{\langle Ax, x \rangle}$. Let $\lambda \in \mathbb{C} \backslash [m,M]$. Define $d = \mathrm{dist}(\lambda, [m,M])$. By the Cauchy Schwarz inequality we have $||(A- \lambda I)x|| \ge |\langle (A- \lambda I)x, x \rangle| =|\langle Ax, x \rangle - \lambda| \ge d$. Thus $||A-\lambda I||$ is bounded below and is hence invertible. My problem wiht this proof is 2-fold. First, this seems to assume that (if $||x||=1$, then) $\langle Ax,x \rangle \in \mathbb{R}$. Further, I don't see how this uses the fact that $A$ is self-adjoint! If I had to guess, $A$ self-adjoint implies that $\langle Ax,x \rangle \in \mathbb{R}$ (when $||x|| = 1$), but I don't seem to be able to figure out how to make that connection. Any help is appreciated. Thanks in advance.","['operator-theory', 'functional-analysis']"
1035659,Transpose Operator is diagonalizable?,"Let $T \colon \mathbb{M}_{n\times n}(\mathbb{R}) \to \mathbb{M}_{n\times n}(\mathbb{R})$ the linear operator such that $T(M)=M^t$, where $M^t$ is the transpose of the matrix $M$. Prove that $T$ is diagonalizable. I thought I'd follow the path that $T^3=T$ but I have not resolved the problem.",['linear-algebra']
1035661,Why is $f(x) = x + \frac{1}{x}$ a mapping contraction?,"Why is $f(x) = x + \frac{1}{x}$ a mapping contraction? The metric space in question is $[1,\infty)$. Also, if this were a contraction, wouldn't it have a fixed point by Banach's theorem? It looks to me like it's not a contraction because for example if I take $x=5$ and $x'=6$:
$$d\left(5+\frac{1}{5},6+\frac{1}{6}\right) > 1 = d(5,6)$$ Would appreciate any help.",['ordinary-differential-equations']
1035676,Strange double integral,"What is wrong with this computation of $\int_0^1\int_{-y}^y \sqrt[3]{x} \, dx \, dy$? I'm considering real functions only.
Since $x^{4/3}$ is an antiderivative of the integrand, we will get $\frac{3}{4}[x^{4/3}]_{-y}^y =\frac{3}{4}(y^{4/3}-(-y)^{4/3})=\frac{3}{4}(y^{4/3}-y^{4/3})=0$. Thus $\int_0^1 \int_{-y}^y \sqrt[3]{x} \, dx \, dy=0$.
However, maple is giving  me a complex (nonzero) number as the answer. Why is that? Any hint?",['multivariable-calculus']
1035703,An equivalent for $\sum_{n=0}^{\infty} e^{-x\sqrt{n}}$ as $x$ tends to $0^+$,"I would like to obtain an equivalent form for $$
f(x)=\sum_{n=0}^{\infty} e^{-x\sqrt{n}}
$$ as $x \rightarrow 0^+$. I tried without success to ""remove"" the $\sqrt{\cdot}$ in the summand by summing over some new index $p$ writing $\displaystyle \sum_{n=0}^{\infty} =\sum_{p=0}^{\infty}\sum_{k=p^2}^{(p+1)^2-1}$. Thanks for your help.","['sequences-and-series', 'calculus', 'integration', 'definite-integrals', 'asymptotics']"
1035754,GCD Direct Proof,"I need to show that if $a,b,c$ are ints such that $\gcd(a,b) = 1$ and $c|(a+b)$ , then $\gcd(c,a) = \gcd(c,b) = 1$ I want to try and prove this directly because I think it will be more straightforward then a indirect. Also, I believe this has something to do with relatively prime numbers. The help is appreciated! $$$$","['elementary-number-theory', 'proof-writing', 'gcd-and-lcm', 'discrete-mathematics']"
1035758,Showing that all monotone functions are integrable,"I am given the following proof: Theorem. All monotone functions are integrable. Proof. Without loss of generality, assume that $f$ is increasing on an interval $\left[a, b \right]$. 
Thus, $f(a) \le f(x) \le f(b)$, and $f$ is bounded on $\left[a, b \right]$. 
Given $\varepsilon >0$, there exists $k > 0$ such that \begin{equation*}
k \left[f(b) - f(a) \right] < \varepsilon.
\end{equation*} Let $P = \left\lbrace x_0, x_1, \dots, x_n \right\rbrace$ be a partition of $\left[a, b \right]$ such that $\Delta x_i \le k$ for all $i$. Since $f$ is increasing, it follows that \begin{equation*}
m_i = f(x_{i-1}) \ \text{and} \ M_i = f(x_i), \quad i = 1, 2, \dots, n.
\end{equation*} Where $m_i$ is the greatest lower bound of $f$ on $\left[ x_{i-1}, x_i \right]$, and $M_i$ is the least upper bound of $f$ on $\left[ x_{i-1}, x_i \right]$. $U(f, P) - L(f, P) = \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] \Delta x_i$ $\le k \sum_{i=1}^n \left[ f(x_i) - f(x_{i-1}) \right] (*)$ $= k \left[f(b) - f(a) \right] $ $< \varepsilon.$ By Theorem 7.1.9 $f$ is integrable on $\left[ a, b \right]$. In the case that $f$ is monotone decreasing, we may use the same argument on $-f$. I am just wondering if somebody could explain how we get from the line marked by (*) to the line after that.","['integration', 'real-analysis']"
1035815,Finding the limit of a function with ArcTan,"I've found difficulties finding this limit ( without using Taylor series approximation , as it's intended for the secondary-school ): $$
\lim_{x\ \to\ \infty}\left[\,
{x^{3} \over \left(\,x^{2} + 1\,\right)\arctan\left(\,x\,\right)} - {2x \over \pi}
\,\right]
$$ Thanks.","['trigonometry', 'limits']"
1035820,$4^{2^n}+2^{2^n}+1$ is Divisible by $7$,"I have one question. How do I prove that $$4^{2^n}+2^{2^n}+1$$ is Divisible by $7$ ?
thanks in advances.",['number-theory']
1035822,"very short and basic question, is $(1,1]$ empty or is it $\{1\}$","Title says it all really. I was asked what is the union and intersection of all the sets $A_n=(1/n,1]$ where $n$ is natural. Right off the bat, $A_1=(1,1]$. is this an empty set? or is it $\{1\}$. I am unsure. On one end, it says ""does not include 1"" but on the other end it does so...",['elementary-set-theory']
1035823,Existence of strictly positive probability measures,Let $X$ be a Hausdorff space (or let's even assume it is metrizable). A strictly positive measure on $X$ assigns positive measure to any non-empty open subset of $X$. What conditions on $X$ ensure the existence of a strictly positive probability (or equivalently $\sigma$-finite) measure? Is there a standard way to construct such a measure if the existence condition is satisfied?,"['general-topology', 'measure-theory', 'probability-theory']"
1035836,"$f: \mathbb{R}^2\to \mathbb{R}^2$ is differentiable, and satisfies an inequality that involves its partials - show that f is a bijection.","Suppose that $f: \mathbb{R}^2\to \mathbb{R}^2$ is differentiable, and the partial derivatives of the components $f_1$, $f_2$ satisfy $$max(|\frac{\ df_1}{dx} -1|, |\frac{df_1}{d_y}|, |\frac{df_2}{d_x}|, |\frac{df_2}{d_y}-1|) <10^{-10}.$$ Prove that f is a bijection.  Note: f is not assumed to be continuously differentiable. Any ideas on how to tackle this problem? We don't have an explicit function given for f, unfortunately.  And I can't assume that f $\in$ $C^1$, let alone assume that f is as smooth as we want it to be, so that rules out the usage of the Inverse and Implicit Function Theorems. Thanks in advance, edit:  I can view the objects that we are taking the maximum of...as a row vector.  Then express this row vector as a 2x2 matrix of the form: $$
        \begin{bmatrix}
        \frac{\ df_1}{dx} -1 & \frac{df_1}{d_y} \\
         \frac{df_2}{d_x}& \frac{df_2}{d_y}-1 \\
               \end{bmatrix}
$$ I observe that this is just the total derivative matrix of the function f, minus the identity matrix, i.e. we have g:= Df - I. Using the inequality that we are given, we have that the sup norm of the matrix < $10^{-10}$. Where can I go from here?","['multivariable-calculus', 'vector-analysis', 'partial-derivative', 'real-analysis', 'derivatives']"
1035837,Solving this trigonometric task: $\sqrt{3}\cos{\theta}-\sin{\theta}=R\cos(\theta+\alpha)$ [duplicate],"This question already has answers here : Express $\sqrt{3}\sin\theta - \cos\theta$ as: $a\cos (\theta + \alpha) $ (5 answers) Closed 6 years ago . Find the values of $R$ and $\alpha$ in the identities below, given that $R>0$ and $\alpha$ is an acute angle. $$\sqrt{3}\cos{\theta}-\sin{\theta}=R\cos(\theta+\alpha)$$ I'm a bit confused by this task. How should I start? I have
$$
\cos(a+b) = \cos(a)\cos(b)-\sin(a)\sin(b).
$$ If I square anything, I can use the trig identity 
$$
\sin^2(x) + \cos^2(x) =1.
$$",['trigonometry']
1035878,Convert form English to logical symbols.,"I have a logical argument in English which says. All Humans are Mortal. Zeus is not Mortal. therefore Zeus is not Human. And I tried to convert it from English to logic. and did this h = is Human, z = is Zeus, m = is Mortal h $ \rightarrow $ m z $ \rightarrow $ ~m $ \therefore $     z $ \rightarrow $ ~h and did it wrong answer as my truth table shows this argument is valid but the original English argument according to Venn Diagram is invalid that meant my conversion was wrong. O.K. then I tried with this h ^ m z ^ ~m $ \therefore $     z ^ ~h And this time I can't get both premise true in a single row in my truth table, I mean both premise are never true at a same time. What is the actual way to solve this problem with logical symbols. (by the way this is not my homework assignment it's just an exercise in Venn Diagram Section of my course and I thought to do it with logical symbols too but could not)",['discrete-mathematics']
1035893,Show that it is the solution of the recurrence,"I have to show that the solution of the recurrence $$X(1)=1, X(n)=\sum_{i=1}^{n-1}X(i)X(n-i), \text{ for } n>1$$
is $$X(n+1)=\frac{1}{n+1} \binom{2n}{n}$$ I used induction to show that. I have done the following: For $n=0$ : $X(1)=1 \checkmark $ We assume that it stands for each $1 \leq k \leq n$:
$$X(k)=\frac{1}{k}\binom{2(k-1)}{k-1} \ \ \ \ \ (*)$$ We want to show that it stands for $n+1$: $$X(n+1)=\sum_{i=1}^{n} X(i)X(n+1-i)=\sum_{i=1}^{n} \frac{1}{i}\binom{2(i-1)}{i-1}\frac{1}{n+1-i}\binom{2(n-i)}{n-i}=\sum_{i=1}^{n}\frac{1}{i}\frac{(2(i-1))!}{(i-1)!(2(i-1)-(i-1))!}\frac{1}{n+1-i}\binom{(2(n-i))!}{(n-1)!(2(n-i)-(n-i))!}=\sum_{i=1}^{n}\frac{(2(i-1))!}{i!(i-1)!}\frac{(2(n-i))!}{(n-i+1)!(n-i)!}$$ How could I continue??","['induction', 'recurrence-relations', 'discrete-mathematics']"
1035919,Converse of Galois Theorem.,If $F\subset K\subset E$ field extension such that $K\subset E$ and $F\subset K$ is both finite and Galois extensions then $F\subset E$ is Galois extension. My intuition says that this is false but I cannot find a counterexample for this.,"['galois-theory', 'abstract-algebra']"
1035933,Chapter dependency tree for Hartshorne's Algebraic Geometry,"I'm self-studying Hartshorne's Algebraic Geometry and I need some guidance. I've studied chapter I (varieties) and sections 1, 2 and 3 of chapter II (schemes). Do I need to study all sections in chapter II before moving on to chapter III (cohomology)? Or is it the case that some sections in chapter II can be skipped in a first reading? What's the optimal strategy for studying the sections of the book? I'm asking because in some other books I studied, more advanced sections where tagged as optional for a first reading. I found that getting back to them after getting a comprehensive understanding of the essentials much more efficient than attacking advanced sections in the middle of a first reading. Thank you","['algebraic-geometry', 'reference-request']"
1035946,Intuition behind Laurent's theorem?,"Taylor series has a pretty nice intuitive explanation. If you know the position, velocity, acceleration and so on of a particle you can predict it's location at any time. Does a similar intuitive explanation exist for Laurent series and the theorem above (the one that describes the coefficients of the series as a line integral)?","['multivariable-calculus', 'complex-analysis']"
1036001,"Show that $u(x)=\ln\left(\ln\left(1+\frac{1}{|x|}\right)\right)$ is in $W^{1,n}(U)$, where $U=B(0,1)\subset\mathbb{R}^n$.","The entire problem statement is: Let $n>1$ and let $U=B(0,1)\subset\mathbb{R}^n$. Show that $u:U\to\mathbb{R}$ given by 
$$u(x)=\ln\left(\ln\left(1+\frac{1}{|x|}\right)\right)$$ is in $W^{1,n}(U).$ My attempt at the proof is as follows: To show that $u\in W^{1,n}(U)$ it suffices to show that $|Du|\in L^{n}(U)$. Consider
$$u_{x_i}=\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{1+\frac{1}{|x|}}|x|^{-2}\frac{x_i}{|x|},$$
which simplifies to,
$$u_{x_i}=\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{1+\frac{1}{|x|}}\frac{x_i}{|x|^3}.$$
Thus,
$$|Du|=\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{1+\frac{1}{|x|}}\frac{1}{|x|^2}$$
which can be manipulated to,
$$|Du|=\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{1+|x|}\frac{1}{|x|}.$$ Moreover, since $U=B(0,1)$ we can bound the second term from above which gives,
$$|Du|\leq\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{|x|}$$ Thus, we can instead show that
$$\int_U\left(\frac{1}{\ln\left(1+\frac{1}{|x|}\right)}\frac{1}{|x|}\right)^n<\infty.$$ Converting this into polar coordinates we have with $r=|x|$,
$$\int_0^R\int_{\partial B(0,r)}\left(\frac{1}{\ln\left(1+\frac{1}{r}\right)}\frac{1}{r}\right)^n\,dSdr=\int_0^R\left(\frac{1}{\ln\left(1+\frac{1}{r}\right)}\frac{1}{r}\right)^n\int_{\partial B(0,r)}dSdr$$
Note that $\int_{\partial B(0,r)}\,dS=n\alpha(n)r^{n-1}$, which is the surface area of $B(0,r)$. Thus, we have then
$$n\alpha(n)\int_0^R\left(\frac{1}{\ln\left(1+\frac{1}{r}\right)}\frac{1}{r}\right)^nr^{n-1}\,dr=n\alpha(n)\int_0^R\left(\frac{1}{\ln\left(1+\frac{1}{r}\right)}\right)^n\frac{1}{r}\,dr$$ And so this is where I get stuck in the problem, since I don't how I can evaluate that last integral to show it's finite. Thank you for any help or feedback!","['functional-analysis', 'sobolev-spaces', 'polar-coordinates', 'integration']"
1036019,"If $G$ has no proper subgroup, then $G$ is cyclic of prime order","This is something I'm supposed to be able to prove for an upcoming test, but I can't find anything to help me prove this in my notes or the chapter, which is on cosets and Lagrange's theorem. If all I start with is the group having no proper subsets, then that means any subset is the whole group. By Lagrange's theorem, that means the index is $1$, but that doesn't get me anywhere. Where do I start?","['group-theory', 'abstract-algebra']"
1036021,Laplace transform and Fourier transform of kernel,"Suppose $p_t(x)=\frac1{\sqrt{2\pi t}}e^{-\frac{x^2}{2t}}$ is the Gaussian density. $p_t(x)$ is also the Green kernel of the heat equation in 1D: $(\partial_t-\frac12\Delta)u=0$. The Fourier transform of $p_t(x)$ is $\hat p_t(\xi)=e^{-\frac{t}2|\xi|^2}$. Then the Laplace transform at 1 has the special bound
$$\int_0^\infty e^{-t}|\hat p_t(\xi)|^2dt=\int_0^\infty e^{-t}e^{-t|\xi|^2}dt\lesssim \frac1{1+|\xi|^2}. $$
In this bound, we see that this integration decay as $|\xi|^{-2}$ when $|\xi|\to\infty$. Note also that the Laplace transform is closely related to resolvent of the operator $\Delta$. Suppose now that we replace $\Delta$ by a ""nice"" (symmetric) operator $A$ with symbol $\psi(\xi)$. What are the characteristics of $A$ so that we have a similar bound
$$\int_0^\infty e^{-\lambda t}|e^{-t\psi(\xi)}|^2dt\lesssim \frac1{1+|\xi|^a} $$
for some $\lambda>0$ and $a>0$. Is there a method to identify the decay rate $a$? PS: I know this is probably well known in semigroup theory, but I couldn't find any exact reference.","['probability-theory', 'semigroup-of-operators', 'operator-theory']"
1036033,Show stable node or spiral cannot occur,"If I have the equation:
$$\ddot{x} + f(\dot{x}) + g(x) = 0$$
where $f$ is even and $f$ and $g$ are both smooth, how do I show that the equilibrium points cannot be stable nodes or spirals? What I've done so far is:
$$\dot{x} = y$$
$$\dot{y} = F(x,y) = -f(y) - g(x)$$
I can try to take the Jacobian at the fixed points
$$J = \begin{pmatrix}
0 & 1 \\
\frac{\partial F}{\partial x} & \frac{\partial F}{\partial y}
\end{pmatrix}$$
but without knowing the values of $\frac{\partial F}{\partial x}$ or $\frac{\partial F}{\partial y}$ I can't tell what type of fixed points can occur. My best guess is that, based on the information in the question, there must be some restriction on the values $\frac{\partial F}{\partial x}$ or $\frac{\partial F}{\partial y}$ can take, which would then rule out the possibility of stable nodes or spirals. However, I can't seem to find any such restrictions. Any help would be appreciated!",['ordinary-differential-equations']
