question_id,title,body,tags
3321483,How many ways are there to pick a red ball second from a bucket of 10 blue balls and 10 red balls (no replacement)?,"This example is taken from my textbook: ""A bag contains twenty balls; ten of the balls are painted red and ten are painted blue. Two balls are drawn from the bag. Suppose we do not replace the first ball once it is drawn. "" ""There are $20 * 19 = 380$ different ways to draw one ball and then draw a second from those that remain."" This makes sense. ""There are $10*19=190$ ways to pick a ball such that the first ball is red."" This also makes sense. ""Likewise, there are $190$ ways to pick a ball such that the second ball is red."" This doesn't make sense to me. How can you be certain of this probability, since it is dependent on your first pick? You have $20$ choices for your first pick (though $190$ isn't a multiple of $20$ ...), since it can be anything. After that, it has to be a red ball...but the probability of that is dependent on what color you picked previously. Maybe the author implied that there are $190$ ways to pick a red ball second assuming you picked a blue one first. But even then, wouldn't the answer be $10 * 10$ , because you have $10$ ways to pick a blue ball first, and then $10$ ways to pick a red ball after that?","['discrete-mathematics', 'probability']"
3321512,Are there an infinite number of primes which are any multiple of $n$ apart? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Are there an infinite number of primes which are any multiple of $n$ apart? That is take $n\in \mathbb{N}$ , then is there an infinite number of primes which are separated by $\textbf{any}$ of the numbers in the below set $$ \{n,2n,3n,4n,\ldots \}. $$","['number-theory', 'prime-gaps', 'prime-numbers']"
3321549,Why $a_{-1}$ term of Laurent series may not be residue?,"Suppose $f(z)$ is analytic on $0<|z-z_0|<R$ . And we find a Laurent's series for $f(z)$ on annulus $r<|z-z_0|<R$ where $r$ may not be $0$ . Then it is said that $a_{-1}$ of such Laurent's series may not be residue unless $r=0$ (Residue is defined as $Res(f,z_0)=\frac{1}{2 \pi i}\int_\gamma f(z)dz$ for any enclosed curve $\gamma$ on $0<|z-z_0|<R$ ). I find this hard to understand. In particular, fix an enclosed curve $\gamma'$ contained in $r<|z-z_0|<R$ where the Laurent's series applies. Substitute $f$ with this Laurent series into $Res(f,z_0)=\frac{1}{2 \pi i}\int_{\gamma'} f(z)dz$ . Then integral of all except the $a_{-1}/(z-z_0)$ terms should evaluate to $0$ as we have finished substitution and are just evaluating the integral with Cauchy formula (and thus are no longer concerned by where Laurent's series apply). In the end since $Res(f,z_0)=\frac{1}{2 \pi i}\int_\gamma f(z)dz$ takes same value for all $\gamma$ in $0<|z-z_0|<R$ , our result based on $\gamma'$ applies in general. Where is the mistake in this proof? Note 1. 
I have checked explicit expression for $a_{-1}$ in a Laurent expansion where $r>0$ .  I think $a_{-1}$ should be residue. The statement in Note 2 may be false. I hope the author or somebody with expertise in complex analysis could confirm. Note 2.
The original statement I was referring to can be found in Simon's answer in this post: Calculate residue at essential singularity : ""In fact the residue of $f(z)$ at an isolated singularity $z_0$ of $f$ is defined as the coefficient of the $(z-z_0)^{-1}$ term in the Laurent Series expansion of $f(z)$ in an annulus of the form $0 < |z-z_0|<R$ for some $R > 0$ or $R = \infty$ . If you have another Laurent Series for $f(z)$ which is valid in an annulus $r < |z-z_0|< R$ where $r > 0$ , then it might differ from the first Laurent Series, and in particular the coefficient of $(z-z_0)^{-1}$ might be different, and hence not equal to the residue of $f(z)$ at $z_0$ .""","['complex-analysis', 'laurent-series', 'complex-numbers']"
3321595,"If a and b are two positive integers, prove $a^2-4b \neq 2$","I know that there are answers for this question, but I didn't come across the following proof, and I was wondering if it was correct: I re-arranged the equation: $a^2 \neq 2 \pmod 4$ Case 1: when a is even: let $a = 2k$ where $k \in Z$ $(2k)^2 \neq 2\pmod4$ $4k^2 \neq 2\pmod4$ $4(k^2) \neq 2\pmod4$ the statement is true because LHS does not have a remainder 2. Caes 2: when a is odd let $a = 2k+1$ where $k \in Z$ $(2k+1)^2 \neq 2\pmod4 $ $4k^2 + 4k + 1 \neq 2\pmod4$ $4(k^2 + k) + 1 \neq 2\pmod4$ the LHS has a remainder of 1 when divided by 4 and the RHS has a remainder of 2 which does not equal $\therefore$ the statement is true","['elementary-number-theory', 'proof-verification', 'discrete-mathematics']"
3321611,Help differentating $f(x) = \sqrt\frac{x^2-1}{x^2+1}$,"The equation I'm trying to differentiate is, $ f(x) = \sqrt\frac{x^2-1}{x^2+1}$ and I know the answer is meant to be $$=\frac{\frac{x\sqrt {x^2+1}}{\sqrt {x^2-1}}-\frac{x\sqrt {x^2-1}}{\sqrt {x^2+1}}}{x^2+1}$$ But when I do the working out I get this $$=\frac{(x^2-1)^\frac{1}{2}}{(x^2+1)^\frac{1}{2}}$$ $$=\frac{\frac{1}{2}(x^2-1)^\frac{-1}{2}\cdot2x\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot\frac{1}{2}(x^2+1)^\frac{-1}{2}\cdot2x}{x^2+1}$$ simplify $$=\frac{x(x^2-1)^\frac{-1}{2}\cdot(x^2+1)^\frac{1}{2}-(x^2-1)^\frac{1}{2}\cdot x(x^2+1)^\frac{-1}{2}}{x^2+1}$$ $$=\frac{\frac{\sqrt {x^2+1}}{x\sqrt {x^2-1}}-\frac{\sqrt {x^2-1}}{x\sqrt {x^2+1}}}{x^2+1}$$ As you can see two of my $x$ 's are in the wrong location, and I just can't figure out what I'm doing wrong.  Any help as to what steps I'm doing wrong or missing would be much appreciated.",['derivatives']
3321630,Calculate number of elements in a set,"How many natural numbers are there below 1000 that are multiples of 3 or that contain 3 in any digit of the number? My effort : Here we need to calculate union of two set. First set is natural number which are multiple of 3. So it's cardinality will be the nearest integer of 1000/3, which will be 333. But I am confused with second set. Any help/hint in this regards would be highly appreciated. Thanks in advance!",['elementary-set-theory']
3321643,How to maximize area of a square inscribed in a equilateral triangle?,"We have an equilateral triangle and want to inscribe a square, in such way that maximizes the area of the square. I sketched two possible ways, not to scale and not perfect. Note I am not sure if the second way will really have all square corners touching the triangle sides. The second case appears to have bigger side-lengths of the square, so bigger area. But I do not know how to determine the angles involved. How to solve this?","['euclidean-geometry', 'area', 'geometry', 'maxima-minima', 'triangles']"
3321645,Variance of a non-homogeneous Poisson process,"I am trying to derive the mean and variance of a non-homogeneous Poisson process. For a homogeneous Poisson process with parameter $\lambda$ , our class notes show the following derivation for the mean, $\mathbb{E}[N(s)]$ , and variance, $var[N(s)]$ , of the number of events over a duration of $s$ : $
\begin{align}
\quad \mathbb{E}[N(s)] 
&= \sum_{n=0}^{\infty} n \cdot P(N(s) = n) \\
&= 0 + \sum_{n=1}^{\infty} n \cdot P(N(s) = n) \\
&= \sum_{n=1}^{\infty} n e^{-\lambda s} \frac{(\lambda s)^n}{n!} \\
&= \sum_{n=1}^{\infty} e^{-\lambda s} \frac{(\lambda s)^n}{(n-1)!} \\
&= (\lambda s) (e^{-\lambda s}) \sum_{n=1}^{\infty} \frac{(\lambda s)^{n-1}}{(n-1)!} \\
&= (\lambda s) (e^{-\lambda s}) (e^{\lambda s}) \\
&= \lambda s
\end{align}
$ $
\begin{align}
\quad \mathbb{E}[\left ( N(s) \right ) \left ( N(s) - 1 \right )]
&= \sum_{n=0}^{\infty} n(n-1) \cdot P(N(s) = n) \\
&= 0 + 0 + \sum_{n=2}^{\infty} n(n-1) e^{-\lambda s} \frac{(\lambda s)^n}{n!} \\
&= \sum_{n=2}^{\infty} e^{-\lambda s} \frac{(\lambda s)^n}{(n-2)!} \\
&= (\lambda s)^2 (e^{-\lambda s}) \sum_{n=2}^{\infty} \frac{(\lambda s)^{n-2}}{(n-2)!} \\
&= (\lambda s)^2 (e^{-\lambda s}) (e^{\lambda s}) \\
&= (\lambda s)^2
\end{align}
$ $
\begin{align}
\quad var[N(s)]
&= \mathbb{E}[(N(s))^2] - \left ( \mathbb{E}[N(s)] \right )^2 \\
&= \mathbb{E}[(N(s))^2] - \mathbb{E}[N(s)] + \mathbb{E}[N(s)] - \left ( \mathbb{E}[N(s)] \right )^2 \\
&= \mathbb{E}[(N(s))^2 - N(s)] + \mathbb{E}[N(s)] - \left ( \mathbb{E}[N(s)] \right )^2 \\
&= \mathbb{E}[(N(s))(N(s) - 1)] + \mathbb{E}[N(s)] - \left ( \mathbb{E}[N(s)] \right )^2 \\
&= (\lambda s)^2 + \lambda s - (\lambda s)^2 \\
&= \lambda s
\end{align}
$ where we used the Taylor expansion $e^x = \sum_{n=0}^{\infty} \frac{x^n}{n!}$ in the infinite sums. When I try to apply this to a non-homogeneous process with parameter $\lambda(t)$ that is dependent on $t$ , I get the mean: $
\begin{align}
\quad \mathbb{E}[N(s)] 
&= \lim_{\delta r \to 0} \sum_{r=0}^s \left ( \sum_{n=0}^{\infty} n \cdot P(N_r(\delta r) = n) \right ) \\
&= \int_0^s \left ( \sum_{n=0}^{\infty} n \cdot P(N_r(\delta r) = n) \right ) \\
&= \int_0^s \left ( \sum_{n=1}^{\infty} n e^{-\lambda(r) dr} \frac{(\lambda(r) dr)^n}{n!} \right ) \\
&= \int_0^s \left ( \lambda(r) dr \sum_{n=1}^{\infty} e^{-\lambda(r) dr} \frac{(\lambda(r) dr)^{n-1}}{(n-1)!} \right ) \\
&= \int_0^s \left ( \lambda(r) dr \right ) \left ( e^{-\lambda(r) dr} \right ) \left ( e^{\lambda(r) dr} \right ) \\
&= \int_0^s \lambda(r) dr \\
&= \Lambda(t, s)
\end{align}
$ But when I try to derive the $\mathbb{E}[\left ( N(s) \right ) \left ( N(s) - 1 \right )]$ term, I get stuck: $
\begin{align}
\quad \mathbb{E}[\left ( N(s) \right ) \left ( N(s) - 1 \right )]
&= \int_0^s \left ( \sum_{n=0}^{\infty} n(n-1) \cdot P(N_r(dr) = n) \right ) \\
&= \int_0^s \left ( \sum_{n=2}^{\infty} n(n-1) e^{-\lambda(r) dr} \frac{(\lambda(r) dr)^n}{n!} \right ) \\
&= \int_0^s \left ( \left ( \lambda(r) dr \right )^2 e^{-\lambda(r) dr} \sum_{n=2}^{\infty} \frac{(\lambda(r) dr)^{n-2}}{(n-2)!} \right ) \\
&= \int_0^s \left ( \left ( \lambda(r) dr \right )^2 \left ( e^{-\lambda(r) dr}  \right ) \left ( e^{-\lambda(r) dr} \right ) \right ) \\
&= \int_0^s \left ( \lambda(r) dr \right )^2 \\
&=^? \left ( \int_0^s \lambda(r) dr \right )^2
\end{align}
$ If the last line is true, then the variance becomes $\Lambda(t, s)$ like I would expect. However, I feel like this is generally not true, and to be honest I'm not even sure how to interpret the square inside of the integral, which is making me question my derivation altogether...","['poisson-distribution', 'poisson-process', 'probability']"
3321655,Is Correlation a linear operator? and is it a measure in terms of measure thoery?,"Let X, Y be two vectors belong to $R^n$ , and $X_{c}$ and $Y_{c}$ are the centered versions of X and Y. Cov(X,Y) = $\frac{<X-\bar{X}, Y-\bar{Y}>}{n-1}$ and Corr(X,Y) = $\frac{<X_{c},Y_{c}>}{||X_{c}||*||Y_{c}||}$ . Is corr a linear operator in terms of algebra? and Is corr a measure in terms of measure theory? And how to verify that?
Comparing 0.1, 0.5, 0.9 of corr, does the difference between 0.1 and 0.5 and the difference between 0.5 and 0.9 tell the same magnitude of difference in the correlation of data?","['statistics', 'linear-algebra', 'probability', 'real-analysis']"
3321693,Example of a function that's not periodic but its derivative is periodic?,"Show by example a function where $\frac{df}{dx}$ is periodic but $f$ is not periodic. The book I'm reading has only shown me the derivatives of sine, cosine, tangent, secant, cosecant, and cotangent. As far as my knowledge goes, all these functions are periodic, and all their derivatives are periodic. I'm unable to think of an example where the function $f$ is not periodic but its derivative is periodic. I considered how $\cot x$ is not defined for all odd multiples of $\frac{\pi}{2}$ but $-\csc^2x$ (its derivative) is defined for these values. But I know that that doesn't make the cotangent function non-periodic by any means. Any help would be greatly appreciated. Thanks.","['periodic-functions', 'calculus', 'trigonometry']"
3321706,Associativity with families in Halmos' Naive Set Theory,"I'm going through Halmos' naive set theory and am having trouble with the statement on page 35 in the Families section. We have the family $\{I_j\}$ with domain $J$ , so in other words $I$ is a function $I: J \rightarrow $ some other set. We are also given $K = \bigcup\limits_{j} I_j$ ; and let $\{A_k\}$ be a family with domain $K$ (so $A: K \rightarrow$ some other set). It then asserts $\bigcup\limits_{k \in K} A_k = \bigcup\limits_{j \in J} \left(\bigcup\limits_{i \in I_j} A_i\right)$ My question: why is the last union operator is iterating over $i \in I_j$ ? Since $K$ is the union of the family $\{I_j\}$ , I think that also means $K$ is the image of the function $I$ . So each $I_j$ is a member of $K$ . Perhaps worth noting is that everything in Halmos so far is a set, numbers haven't been constructed yet. So the $I_j$ 's make up the members of $K$ , but each $I_j$ is itself a set with an unknown count of elements. To construct the set $\bigcup\limits_{i \in I_j} A_i$ , $A_i$ would have to be defined on the elements of each individual $I_j$ . But the elements $i \in I_j$ are not the same thing as the elements $I_j$ $\in K$ . The domain of $A$ is given as $K$ . So the arguments of $A$ are $I_j \in K$ , not $i \in I_j$ . Have I misunderstood something about families? The book goes on to say this equality is the generalization of the associative law for unions. But I'm not sure how this demonstrates that since I'm not sure that $A$ is defined on the elements of $I_j$ .",['elementary-set-theory']
3321716,Simplify $ \frac{ \sqrt[3]{16} - 1}{ \sqrt[3]{27} + \sqrt[3]{4} + \sqrt[3]{2}} $,"Simplify $$ \frac{ \sqrt[3]{16} - 1}{ \sqrt[3]{27} + \sqrt[3]{4} + \sqrt[3]{2}} $$ Attempt: $$ \frac{ \sqrt[3]{16} - 1}{3 + \sqrt[3]{4} + \sqrt[3]{2}} =  \frac{ \sqrt[3]{16} - 1}{ (3 + \sqrt[3]{4}) + \sqrt[3]{2}} \times \frac{ (3 + \sqrt[3]{4}) - \sqrt[3]{2}}{  (3 + \sqrt[3]{4}) - \sqrt[3]{2}}  $$ $$ =    \frac{ (\sqrt[3]{16} - 1) [(3 + \sqrt[3]{4}) - \sqrt[3]{2}]}{ (3 + \sqrt[3]{4})^{2} - 2^{2/3}}  $$ $$ =     \frac{ 3 \sqrt[3]{16} - 3\sqrt[3]{4} + \sqrt[3]{2} + 1}{ (9 + 5 \sqrt[3]{4} + \sqrt[3]{16}) } $$ From here on I don't know how to continue. I can let $a = \sqrt[3]{2}$ , but still cannot do anything.","['radicals', 'field-theory', 'linear-algebra', 'extension-field', 'algebra-precalculus']"
3321724,Subgroups of $G\times G$ that are isomorphic with $G$,"Let $G$ be a finite group. I want to find all subgroups $H$ of $G\times G$ such that $H\cong G$ . It's easy to find three such subgroups: (1) $\{(g,1):g\in G\}$ , (2) $\{(1,g):g\in G\}$ and (3) the diagonal $\Delta(G) = \{(g,g): g\in G\}$ . More generally, for any $\sigma \in \mathrm{End}(G)$ , $\{(g,g^\sigma):g\in G\}$ and $\{(g^\sigma,g):g\in G\}$ are also subgroups of $G$ that are isomorphic with $G$ . Therefore, we have $2|\mathrm{End}(G)|-1$ such subgroups. If $G$ is decomposable, say $G = H\times K$ , then $H_1\times K_2\cong G$ is also a subgroup of $G_1\times G_2 = (H_1\times K_1)\times (H_2\times K_2)$ , where $G_1,G_2\cong G$ , $H_1,H_2\cong H$ and $K_1,K_2\cong K$ . Hence we have more such subgroups. My questions: Are there more such subgroups that are not of types above? And what is the total number of such subgroups of $G\times G$ (given $G$ )?","['group-theory', 'abstract-algebra', 'finite-groups']"
3321737,Number of particular paths of even length,"Let $W_n=\{ w=a_1a_2\cdots a_{2n}: a_i\in\{1,-1\},\ a_1+\cdots+a_j\geq 0,\ \forall\ 1\leq j\leq 2n\}$ . Let $W_n^k=\{w\in W_n:w\ \text{does not cross the line}\ y=k  \}$ . Is there any way to find $\# W_n^k$ ? Example: if $n=2$ , then $\# W_2=6$ and $\# W_2^1=1$ and $\#W_2^2=4$ .",['combinatorics']
3321743,How many N by N adjacency matrices exist with maximum degree of m?,"I want to calculate the number of adjacency matrices corresponding to graphs with $N$ nodes satisfying all of the following properties: There are no isolated nodes (each row of the matrix must have at least one 1). The maximum degree for each node is at most $m \leq N$ (For example, if $m=3$ , then each row of the matrix can have at most three 1s). The adjacency matrix represents an undirected graph. My attempts thus far : I know that the number of adjacency matrices for an undirected graph with $N$ nodes is $2^{N(N-1)/2}$ , but I have not been able to incorporate the remaining assumptions.","['discrete-mathematics', 'graph-theory', 'linear-algebra', 'combinatorics']"
3321756,"Given $x, y$ that $xy-\frac{x}{y^2}-\frac{y}{x^2}=3$, work out $xy-x-y$.","Today I had a competition in Xiamen, China. I know how to do the questions except this strange equation. Given $x, y$ that $xy-\dfrac{x}{y^2}-\dfrac{y}{x^2}=3$ , work out $xy-x-y$ . Such a strange question, right? I have found the integral solution are $0$ when $x=y=2$ and $3$ when $x=y=-1$ , but I think there are infinitely many solutions but I can’t really prove it at that time. Can you guys help me?","['contest-math', 'algebra-precalculus']"
3321770,Using the Mean Value Theorem to Prove an Inequality involving the second derivative [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Consider the twice-differentiable function $f$ such that $f(0) = 0$ , $f(1/2) = 1/2$ , and $f'(0) = 0$ . Prove that $f''(x)$ is greater than or equal to 4 for some $x$ in the domain $[0, 1/2]$ . NOTE: Please don't just use one example of a possible function for $f$ and solve it using that example as I want to be able to generalize it to all cases. EDIT: Sorry if it wasn't clear - the question is asking to prove that the second derivative of $f(x)$ ( $f''(x)$ ) is greater than or equal to 4. I've tried solving for $f'(x)$ from $f(0)$ and $f(1/2)$ , and then trying to disprove a counterexample to the inequality we are given (trying to prove that $f''(x)$ cannot be less than 4), but that's as far as I've gotten. EDIT2: I've just realized that the question did not ask for all x, but only some x. This was a huge mistake on my part, so I'm extremely sorry to all those who have helped thus far.","['calculus', 'derivatives']"
3321799,"If $A\in {\mathbf F_2}^{n\times n}$ is symmetric, its diagonal is in the span of its columns","Let $A$ be an $n\times n$ symmetric matrix with entries in $\mathbf F_2$ (the field with 2 elements, also referred to as $\mathbb Z/2\mathbb Z$ ). Prove that the diagonal of $A$ is in the span of its columns. If the diagonal is only zeros the problem is trivial, but I haven't made any significant progress in the general case...","['matrices', 'number-theory', 'linear-algebra', 'symmetric-matrices']"
3321831,"a finite variety and its ""vanishing ideal""","Here is a problem from Ernst Kunz's Introduction to Commutative Algebra and Algebraic Geometry : Let $K$ be an infinite field, $V \subset \mathbb{A}^n(K)$ a finite set of points. It's ideal $\mathfrak{J}(V)$ in $K[X_1, ..., X_n]$ is generated by $n$ polynomials. Hint: Interpolation. note: $\mathfrak{J}(V)$ consists of all $F \in K[X_1, ..., X_n]$ with $F(x) = 0$ for all $x\in V$ A similar question can be found here . The solutions given above just answers why any finite variety can be writen as the zero locus of n polynomials. But further questions: do the n polynomials constructed in the link generate the vanishing ideal? And that's my question really concerned. Is the condition ""the field K infinite"" necessary? How do we use it? I've tried some simple cases: $n = 1$ and $V$ is a single point. For other cases, I don't have any accessible solutions. Anyone can help? Prefect the solution in the link or give an alternative approach to the problem. Thank you!","['algebraic-geometry', 'polynomials', 'commutative-algebra']"
3321839,"Section 3.1.1. ""Complete Integrals"" from Evans PDE book","Question about an observation taken from p. 92-93 of the book {1} in the title.
With $$
F(Du,u,x) = 0 \tag{1}
$$ We denote a non linear first order PDE. Here $u$ is the scalar function of vector variable $x$ and $Du = (u_{x_1}, \ldots, u_{x_n})$ is the gradient. From the book NOTATION. We write $$(D_a u, D^2_{xa}u) = 
\begin{pmatrix}
u_{a_1} & u_{x_1a_1} &\dots & u_{x_n a_1} \\
\vdots &   \vdots & \ddots & \vdots \\
u_{a_n} & u_{x_1a_n} &\dots & u_{x_n a_n}
\end{pmatrix}_{n \times (n+1)} \tag{2}
$$ where $u(x;a)$ is a solution for $F$ parametrized by $a \in A \subset \mathbb{R}^n$ . DEFINITION. A $C^2$ function $u = u(x;a)$ is called complete integral in $U\times A$ provided (i) $\quad u(x;a)$ solves $(1)$ for each $a \in A$ (ii) $\quad \text{rank}(D_a u, D^2_{xa}u) = n \quad (x \in U, a \in A)$ After this definition we have Interpretation. Condition (ii) ensures $u(x;a)$ ""depends on all the $n$ independent parameters $a_1,\ldots,a_n$ "". To see this suppose $B \subset \mathbb{R}^{n-1}$ is open, and for each $b \in B$ assume $v = v(x;b)$ , $(x \in U)$ is a solution of $(1)$ . Suppose also there exists a $C^1$ mapping $\psi : A \to B$ , $\psi = (\psi^1,\ldots,\psi^{n-1})$ such that $$
u(x;a) = v(x;\psi(a)) \quad (x \in U, a \in A) \tag{3}
$$ That is, we are supposing the function $u(x;a)$ ""really depends only on the $n-1$ parameters $b_1,\ldots, b_{n-1}$ "". But then $$
u_{x_i a_j}(x;a) = \sum_{k=1}^{n-1} v_{x_i b_k}(x;\psi(a)) \psi_{a_j}^k(a) \quad (i,j = 1,\ldots, n) \tag{*}
$$ Consequently $$
\det(D_{xa}^2 u) = \sum_{k_1,\ldots, k_n = 1}^{n-1} v_{x_1 b_{k_1}} \ldots v_{x_n b_{k_n}} \det \begin{pmatrix}
\psi_{a_1}^{k_1} & \ldots & \psi_{a_n}^{k_1} \\
 & \ddots &  \\
\psi_{a_1}^{k_n} & \ldots & \psi_{a_n}^{k_n}
\end{pmatrix} \tag{**}
$$ I don't get how $(**)$ follows from $(*)$ . To me $(*)$ is the $(i,j)$ entry of matrix defined as product between two matrices namely $$
V(x;a) = 
\begin{pmatrix}
v_{x_1,b_1}(x;a) & \ldots & v_{x_1,b_{n-1}}(x;a) \\
\vdots & \ddots & \vdots \\
v_{x_n,b_1}(x;a) & \ldots & v_{x_n,b_{n-1}}(x;a)
\end{pmatrix}
$$ and $$
\Psi(a) = 
\begin{pmatrix}
\psi_{a_1}^1(a) & \ldots & \psi_{a_n}^{1}(a) \\
\vdots & \ddots & \vdots \\
\psi_{a_1}^{n-1}(a) & \ldots & \psi_{a_n}^{n-1}(a)
\end{pmatrix}
$$ So $$
D^2_{xa} u = V(x;a) \Psi(a)
$$ But I cannot manage from this to derive the determinant, because the matrices are not square, but rectangular so I cannot apply the Binet rule for the product of determinants. Any suggestion? Also continuing with the chapter it seems to me that in order to use this method I'd need to find a complete integral first, so I can generate other solutions. However at this point the question is should I use the characteristic method described later to find a complete integral first? {1} L.C. Evans, Partial Differential Equations , 2nd ed., American Mathematical Soc., 2010. Update : I'd still like to understand where (**) comes from, however I've found a workaround that doesn't use any explicit computation. I can use the dimension theorem to reach the same conclusion, since I can regards $V(x;a)$ and $\Psi(a)$ as linear maps. Since $V(x;a) : \mathbb{R}^{n-1} \to \mathbb{R}^n$ and $\Psi(a) : \mathbb{R}^n \to \mathbb{R}^{n-1}$ then we must have $rank(V(x;a)) \leq n - 1$ and $rank(\Psi(a)) \leq n - 1$ , more specifically we have $dim(Ker(\Psi(a))) \geq 1$ (which means an entire subspace of dimension at least $1$ is mapped to $0$ ). This yields to state $dim(ker(V(x;a)\Psi(a))) \geq 1 \Rightarrow rank(V(x;a)\Psi(a)) \leq n - 1$ which in turnes yields $rank(D^2_{xa}u) \leq n - 1$ , since $D^2_{xa}u : \mathbb{R}^n \to \mathbb{R}^n$ then the determinant must be 0. I think the arguments works fine, but I think Evans uses computation like (**) in the book so understanding where it comes from might make my life easier in the future.","['proof-explanation', 'proof-verification', 'linear-algebra', 'partial-differential-equations']"
3321906,"Alice, Bob and his 1956-Triumph [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Alice challenges Bob with a puzzle and Bob accepts it even before Alice told him specifically what it is about :) He must secretly write down a list (list A) of 20 positive rational numbers, not necessarily different from each other and put the list in a sealed envelope. Then Bob must give Alice a list (list B) of different numbers, each of which can be either one of the numbers in his list A, or the sum of more than one numbers in list A. Then Alice must try to find the numbers of list A. If she manages to find 2 or more sets of 20 numbers (from the numbers of list B), by which she can guess the numbers of list A, then Bob must donate her his priceless 1956 Triumph TR3. If, however, by the numbers in list B there is only one way to guess the 20 numbers in list A, then Bob will pay Alice one dollar for each of the numbers in list B. What is the minimum number that Bob must pay to Alice (to save his Triumph)? This was given to me as a challenge from a friend. I am obsessed with maths and combinatorics but, alas, with this one I can't even think of where to start from! (and I'm not even sure what category to assign it to!! I chose ""combinatorics"" only by intuition!!)",['combinatorics']
3321952,Show that it is possible to color the edges of $K_n$ with at most $3 \sqrt n$ colors so that there are no monochromatic triangles.,"Show that it is possible to color the edges of $K_n$ with at most $3 \sqrt n$ colors so that there are no monochromatic triangles. (This was previous question and I get an explanation in the comments: Does this problem make a sense? I would expect at least, not at most. Where is my thinking wrong? ) Here is a solution: Actually, the problem is trivial. Proceed inductively. Just split $K_n$ in to two parts with ${n\over 2}$ elements in both if $n$ is even or ${n-1\over 2}$ and ${n+1\over 2}$ elements if $n$ is odd. Color all edges between this parts with one color. In one part we will not use more than $3\sqrt{n+1\over 2}$ colors. So we have used $$3\sqrt{n+1\over 2} +1$$ colors and all we have to check if $$3\sqrt{n+1\over 2} +1\leq 3\sqrt{n}$$ which is true. Since I found this here, problem 41 I wonder how to solve it with a probabilistic method ?","['graph-theory', 'probabilistic-method', 'discrete-mathematics']"
3322029,"I need to prove that $\lim_{(x, y)\to(0,0)}\frac{|x|\log(1+y)}{\sqrt{x^2+y^2}}=0$","I need to prove the following limit $$\lim_{(x, y)\to(0,0)}\frac{|x|\log(1+y)}{\sqrt{x^2+y^2}}=0$$ Using the Squeeze Theorem $$0\le|\frac{|x|\log(1+y)}{\sqrt{x^2+y^2}}|\leq\frac{|x|\log(1+y)}{y}\to0$$ For $(x,y)\to(0,0)$ Here I have used the fact that $$\sqrt{x^2+y^2}\ge\sqrt{y^2}=y$$ Such that $$\frac{1}{\sqrt{x^2+y^2}}\le\frac{1}{y}$$ And the fact that $$\lim_{y\to0}\frac{\log(1+y)}{y} =0$$ So I can conclude that $$0\le\lim_{(x, y)\to(0,0)}\frac{|x|\log(1+y)}{\sqrt{x^2+y^2}}\le\lim_{(x, y)\to(0,0)}\frac{|x|\log(1+y)}{y}\le0$$ And thus by the Squeeze Theorem $$\lim_{(x, y)\to(0,0)}\frac{|x|\log(1+y)}{\sqrt{x^2+y^2}}=0$$ Is my proof correct?","['multivariable-calculus', 'limits', 'calculus', 'proof-verification']"
3322057,Concerning details of the proof of Hartshorne II 8.15,"Here are the statement and proof Hartshorne II 8.15. I got the following questions concerning the proof: Concerning non-singularity of abstract variety, it is required to prove the iff relation for every point of $X$ , why the consideration only at the closed points can be generalized? I cannot see how (8.14A) can be applied explicitly. Below are the statements of (8.14A) and (Ex. 5.7): They are likely to be trivial questions for most of you, but it will help a lot for me to understand the context. Thank you very much in advance.","['algebraic-geometry', 'commutative-algebra']"
3322072,"Group multiplication and taking of inverses are continuous with respect to the topology on $PSL(2,\mathbb{R})$","I am currently working with the book ""Fuchsian groups"", by Svetlana Katok and am trying to solve a few of the provided exercises. I understand $PSL(2,\mathbb{R})$ can be represented as the quotient group $SL(2,\mathbb{R})/(±Id)$ and $PSL(2,\mathbb{R})$ is a topological group endowed with a quotient topology. Also it is clear to me how to show that the multiplication and inverses are continuous for $SL(2,\mathbb{R})$ since it is a subset of $\mathbb{R}^4$ . However, how would one now show that the group multiplication and inverse are continuous with respect to the topology on $PSL(2,\mathbb{R})$ ?","['hyperbolic-geometry', 'geometry', 'topological-groups']"
3322094,Dense subset of cut locus,"Given a complete Riemannian manifold $M$ and point $p\in M$ , denote $\mathrm{Cut}_p$ the cut locus of $p$ and $\mathrm{Cut}_p^1\subset \mathrm{Cut}_p$ the set of points $q$ which are connected to $p$ by more than one length minimising geodesic. According to a remark in Sakai's Riemannian geometry book (Rmk. 4.9), the latter forms a dense subset - but I don't understand why. Question: Why is $\mathrm{Cut}_p^1\subset \mathrm{Cut}_p$ dense? (I use density in this answer on MO. Comments on how to avoid this property to prove regularity of Riemannian distance function are also very welcome.)","['riemannian-geometry', 'differential-geometry']"
3322138,"Rate of change with $f(x)=4x^2-7$ on $[1,b]$","As part of a textbook exercise I am to find the rate of change of $f(x)=4x^2-7$ on inputs $[1,b]$ . The solution provided is $4(b+1)$ and I am unable to arrive at this solution. Tried: $f(x_2)=4b^2-7$ $f(x_1)=4(1^2)-7=4-7=-3$ If the rate of change is $\frac{f(x_2)-f(x_1)}{x_2-x_1}$ then: $\frac{(4b^2-7)-3}{b-1}$ = $\frac{4b^2-10}{b-1}$ This is as far as I got. I tried to see if I could factor out the numerator but this didn't really help me: $(4b^2-10)=2(2b^2-5)$ If I substitute this for my numerator I still cannot arrive at the provided solution. I then tried isolating b in the numerator: $4b^2-10=0$ $4b^2=10$ $b^2=10/4$ $b=\frac{\sqrt{10}}{\sqrt{4}}=\frac{\sqrt{10}}{2}$ This still doesn't help me arrive at the solution. How can I arrive at $4(b+1)$ ?",['algebra-precalculus']
3322144,"let $x > -1$ prove for all $n \geq 1$, $(1+x)^n \geq 1+nx$",I tried to prove this by induction: Base case for n = 1 satisfies $\because$ $1+x$ = $1+x$ I.H: $(1+x)^k \geq 1 + kx$ Inductive Step for $k+1$ : $(1+x)^{k+1} \geq 1 + (k+1)x$ $(1+x)^k (1+x) \geq 1 + kx + x$ $(1+kx) (1+x) \geq 1 + kx + x$ by I.H. $kx^2 + kx + x + 1 \geq 1 + kx + x$ $kx^2 \geq 0$ which is true for $x>-1$ therefore proved? Is this the correct way to prove by induction?,"['proof-verification', 'discrete-mathematics']"
3322191,Can the Leibniz integral rule be used on integrals with singular endpoints?,"I'd like to know if the Leibniz integral rule has an extension or generalization that can handle convergent integrals whose endpoints are singular. This post attempts to ask a similar question but doesn't give a good example. Here's my example: $$\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx=\frac{\pi}{2a}.$$ Suppose we take $d/da$ of both sides. Clearly we have: $$\frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\frac{-\pi}{2a^2}.$$ But the Liebniz integral rule seems to give: $$\frac{d}{da}\left(\int_a^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx\right)=\int_a^\infty \frac{a}{x(x^2-a^2)^{3/2}}\,dx-\frac{1}{a\sqrt{a^2-a^2}}.$$ Of course, the rightmost term is a problem, which results from the fact that the endpoint of the original integral was singular. Suppose the original integral weren't so easily solvable and we actually needed the Leibniz integral rule to make progress—is there a valid way to apply it? One idea I have is to change the original integral to $$\lim_{\epsilon\to 0^+}\int_{a+\epsilon}^\infty \frac{1}{x\sqrt{x^2-a^2}}\,dx$$ and then take the derivative using the Leibniz rule, which should result in a difference of two expressions that individually diverge as $\epsilon\to 0$ , but whose difference converges. But this would require justifying bringing the derivative $d/da$ inside the $\lim_{\epsilon\to 0}$ operation, which would require a uniform convergence proof. Is there a shortcut that gets around this difficulty, or a guarantee that this maneuver is always allowed?","['integration', 'improper-integrals', 'real-analysis']"
3322216,"What are the eigenvalues of $X = xx^{T}, x\in\mathbb{R}^{d}$? [duplicate]","This question already has answers here : Eigenvalues of the rank one matrix $uv^T$ (3 answers) Closed 4 years ago . I'm given the matrix $X = xx^{T}\in\mathbb{R}^{d \ x \ d}, x\in\mathbb{R}^{d}$ .
Does somebody know how to compute $\lambda_{max}(X)$ or $\lambda_{min}(X)$ ?
I only want to know these two eigenvalues, the others are not really important. I seem to be stuck. I'm thankful for any answer.",['linear-algebra']
3322232,How to derive $ \frac ab -x= \frac {c-xd}{b}+\left(\frac{b-d}{b}\right) \left(\frac{a-c}{b-d} - x \right) $,$$ \frac ab -x= \frac {c-xd}{b}+\left(\frac{b-d}{b}\right) \left(\frac{a-c}{b-d} - x \right)  $$ It is easy to check it by computing right hand side. It feels unnatural and a little magical. I could't derive it starting from LHS This identity is used in the proof of Stolz Cesaro theorem ( https://ru.wikipedia.org/wiki/Теорема_Штольца ). It is in russian I understood with the  help of google translate .,['algebra-precalculus']
3322249,"Prove: there exists 3 sets: $A, B, C \subseteq \mathbb{N}$ such that: $A\cap B\cap C =\emptyset$ and $|A|=|B|=|C|=\aleph_0$?","Prove: there exists 3 sets: $A, B, C \subseteq \mathbb{N}$ such that: $A\cap B= B \cap C = A \cap C = A\cap B\cap C = \emptyset$ and $|A|=|B|=|C|=\aleph_0$ ? also, the sets must exists: $$|\mathbb{N} \setminus {A}| = \aleph_0$$ $$|\mathbb{N} \setminus {B}| = \aleph_0$$ $$|\mathbb{N} \setminus {C}| = \aleph_0$$ in other words, I'm looking for a way to substitute $\mathbb{N}$ into 3 disjoint sets, such that the cardinality of each of them is equal to the cardinality of $\mathbb{N}$ EDIT: the point for this question is to prove a lemma (finding $A, B, C$ as described), as given the set: $$ M = \{ A \in P(A) \vert \ \ |A| = \aleph_{0} \ \land \ |A^c| = \aleph_{0} \}$$ Prove that: $|M| = \aleph$ by finding 3 sets, such that $|A| = |B|=|C| =|\aleph_0$ , then I could determine that: $(B\cup C) \in M$ as $|B \cup C| =\aleph_0$ ,
and: $(B\cup C)^c = \mathbb{N}\setminus(B \cup C) = A$ , as $|A| = \aleph_0$ . using the lemma, I'd argue that for every set $\beta \subseteq B: \ (\beta\cup C) \subseteq (B\cup C) \Longrightarrow \ \forall \beta: (\beta \cup C) \subseteq M$ . this is true because $|C| =\aleph_{0}, \ \forall \beta: |\beta \cup C| = \aleph_{0}$ and $ A \subseteq (\beta \cup C)^{c} \Longrightarrow |(\beta \cup C)^{c}| = \aleph_0$ . finally, the collection of all $\beta$ sets is: $\{\beta | \beta \subseteq B \}$ , Hence $\{\beta | \  \beta \subseteq B \} = P(B)$ . Notice that $|P(B)| = 2^{\aleph_{0}} = \aleph$ this means that $\left(P(B) \cup C \right) \subseteq M \Longrightarrow \ 
 \aleph =|\left(P(B) \cup C \right)| \leq |M|$ . because $M \subseteq P(\mathbb{N}) \Longrightarrow |M| \leq |P(\mathbb{N})| = \aleph$ , hence, by Cantor Bernstein theorem we conclude that $|M| =\aleph$ , as wished.","['elementary-set-theory', 'cardinals', 'proof-verification', 'discrete-mathematics']"
3322294,Are these new series formulae for $\zeta(2)$?,"Let $\zeta(n)$ denote the Riemann zeta function defined for positive integers greater than $1$ by its usual infinite series. Thus, $\zeta(2)=\sum_{k=1}^\infty\frac{1}{k^2}$ . Many formulae exist involving $\zeta(2)$ , including the Apéry-like fast-converging series: $$
\zeta (2)=3\sum _{{n=1}}^{{\infty }}{\frac  {1}{n^{{2}}{\binom  {2n}{n}}}}.
$$ Recently I have found the following similar-looking series: $$
\zeta (2)=\frac83\sum _{{n=1}}^{{\infty }}{\frac  {2^{n-1}}{n^{{2}}{\binom  {2n}{n}}}},
$$ $$
\zeta (2)=\frac94\sum _{{n=1}}^{{\infty }}{\frac  {3^{n-1}}{n^{{2}}{\binom  {2n}{n}}}}
$$ and $$
\zeta (2)=\frac43\sum _{{n=1}}^{{\infty }}{\frac  {4^{n-1}}{n^{{2}}{\binom  {2n}{n}}}}.
$$ Are these series already known? A quick internet search yields no such results. EDIT forgot to add the second series.","['riemann-zeta', 'sequences-and-series']"
3322346,Eigenvalues of rank-$1$ update,"If I have a diagonal matrix with rank- $1$ update $$D + u v^T$$ what can I say about its eigenvalues? I know from Two matrices that are not similar have (almost) same eigenvalues that every eigenvalue of $D$ with multiplicity $m > 1$ will occur in $D + uv^T$ at least $m-1$ times. I am wondering what can be said about the remaining eigenvalues, and in particular, how do they scale with $u$ and/or $v$ . For example in the following Mathematica code: dim = 50;
SeedRandom[1]

Diag = DiagonalMatrix[Flatten[RandomInteger[{0, 10}, {1, dim}]]];

u = ConstantArray[{1}, dim];
v = List /@ RandomReal[{0, 100}, {dim}];
vT = Transpose[v];
uvT = Transpose[u.vT];

Eigenvalues[Diag]
Round[Eigenvalues[Diag + uvT], 0.01]

(*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 
3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0}*)

(*{2340.33, 9.85, 9., 8.79, 8., 7.75, 6.98, 6., 6., 5.85, 5., 5., 5., 
5., 5., 4.68, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3., 2.4, 
2., 2., 2., 1.55, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.3, 0., 0., 
0., 0., 0., 0., 0.}*) one can clearly see that the eigenvalue with multiplicity $m>1$ occurred in the perturbed case again at least $m-1$ times while every other eigenvalue lifted only slightly. If I now chose v to be of higher magnitude: v = List /@ RandomReal[{10^9, 10^10}, {dim}]; for some reason the eigenvalues change only insignificantly: (*{10, 9, 9, 8, 8, 7, 6, 6, 6, 5, 5, 5, 5, 5, 5, 4, 4, 4, 4, 4, 4, 3, 
3, 3, 3, 3, 3, 3, 2, 2, 2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 
0, 0, 0, 0, 0}*)

(*{2.60337*10^11, 9.86, 9., 8.79, 8., 7.76, 6.97, 6., 6., 5.84, 5., 5., 
5., 5., 5., 4.67, 4., 4., 4., 4., 4., 3.59, 3., 3., 3., 3., 3., 3., 
2.4, 2., 2., 2., 1.56, 1., 1., 1., 1., 1., 1., 1., 1., 1., 0.31, 0., 
0., 0., 0., 0., 0., 0.}*) except for the first eigenvalue that blows up. Is there a mathematical argument for why most of the eigenvalues change only so slightly even when choosing $v$ to be so large?","['matrices', 'linear-algebra', 'mathematica', 'eigenvalues-eigenvectors']"
3322353,Calculate mean curvature of surface,"I've got the following surface $M=\bigl\{(x,y,z) \mid e^z=\frac{\cos x}{\cos y}\bigr\}\subset \mathbb{R}^3$ where $x,y \in \bigl(-\frac{\pi}{2},\frac{\pi}{2}\bigr)$ and want to calculate the mean curvature of it. To do this I used the parametrisation $X(u,v)=(u,v,\ln(\frac{\cos u}{\cos v}))$ but this leads to to a complete mess when using the well known formula $\dfrac{1}{2}\dfrac{eG-2fF+gE}{EG-F^2}$ where $E,F,G$ is from the first and $e,f,g$ is from the second fundamental forms. Someone know how to do this so it doesn't go off the handle? I have Gauss map given by $$N(X(u,v))=(\tan u,-\tan v, 1) \dfrac{1}{\sqrt{\dfrac{-\sin^2u}{\cos^2u}+\dfrac{-\sin^2v}{\cos^2v}+1}}.$$","['minimal-surfaces', 'differential-geometry']"
3322357,How is the matrix identity $\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B)$ proved?,"The Wikipedia page about the determinant mentions the following matrix identity $$\det\begin{pmatrix}A&B\\B&A\end{pmatrix}=\det(A+B)\det(A-B),$$ valid for squared matrices $A$ and $B$ of the same size. How is this result proved?","['matrices', 'determinant', 'linear-algebra', 'block-matrices']"
3322368,"Is there a Mellin transform or an analogue on $L^2([0,2\pi])$ or $\ell^2(\mathbb{Z})$?","From Wikipedia, the Mellin transform is an isometry $M : L^2(\mathbb{R}^+) \mapsto L^2(\mathbb{R})$ , $$\{M f\} (s) := \frac{1}{\sqrt{2\pi}} \int_{\mathbb{R}^+} x^{-1/2 + \mathrm{i} s} f(x) dx.$$ https://en.wikipedia.org/wiki/Mellin_transform Does anyone know if there is an analogue of this transformation on $L^2([0,2\pi])$ or $\ell^2(\mathbb{Z})$ ? Thanks !","['fourier-analysis', 'analysis', 'complex-analysis', 'mellin-transform', 'integral-transforms']"
3322394,Norm of a block of matrix operator,"Let $(\mathcal{H}_1,\langle \cdot\mid \cdot\rangle_1), (\mathcal{H}_2,\langle \cdot\mid \cdot\rangle_2), \cdots, (\mathcal{H}_d,\langle \cdot\mid \cdot\rangle_d)$ be complex Hilbert spaces and let $\mathbb{H}=\oplus_{i=1}^d\mathcal{H}_k$ . Let $\mathbb{T}= (T_{ij})_{d \times d}$ be an operator matrix on $\mathcal{B}(\oplus_{i=1}^d\mathcal{H}_k)$ and $\tilde{\mathbb{T}} = (\|T_{ij} \|_{\mathcal{B}(\mathcal{H}_j,\mathcal{H}_i)})_{d\times d}$ its block-norm matrix.
  Why $$\|\mathbb{T}\|_{\mathcal{B}(\oplus_{i=1}^d\mathcal{H}_k)} \leq \| \tilde{\mathbb{T}} \|?$$ Attempt: Let $x=(x_1,\cdots,x_d)\in \oplus_{i=1}^d\mathcal{H}_k$ . Then, $$
\|\mathbb{T}x\|^2=\sum_k\left\|\sum_jT_{kj}x_j\right\|_k^2\leq\sum_k\left(\sum_j\|T_{kj}\|_{\mathcal{B}(\mathcal{H}_j,\mathcal{H}_k)}\,\|x_j\|_j\right)^2
.
$$ On the other hand, $$\| \tilde{\mathbb{T}} \|=\sup_{\|x\|_{\mathbb{R}^d}}\| \tilde{\mathbb{T}}x\|.$$ For all $x=(x_1,\cdots,x_d)\in \mathbb{R}^d$ we have $$
\|\tilde{\mathbb{T}}x\|^2=\sum_k\left|\sum_j\|T_{kj}\|x_j\right|^2.
$$","['operator-theory', 'functional-analysis']"
3322407,Can a group have a cyclical derived series?,"Given any group $G$ , one can consider its derived series $$G = G^{(0)}\rhd G^{(1)}\rhd G^{(2)}\rhd\dots$$ where $G^{(k)}$ is the commutator subgroup of $G^{(k-1)}$ . A group is perfect if $G=G^{(1)}$ and thus has constant derived series, and solvable if its derived series reaches the trivial group after finitely many steps. Is it possible for a group’s derived series to be cyclical, i.e. that $G \cong G^{(n)}$ for some $n>1$ and $G\not\cong G^{(k)}$ for all positive $k<n$ ? Note that such a group could not be finite, solvable, nor co-Hopfian.","['derived-subgroup', 'group-theory', 'abstract-algebra', 'infinite-groups']"
3322443,What's the meaning of a $d$ with a stroke when using Leibniz notation?,"I'm studying engineering and there's a physics teacher that strikethroughs the derivative $d$ when writing an expression for power (which is work over time). I know physics teachers are known to abuse mathematical notation, but this intrigued me as I had never seen it used and couldn't find anything online. So the expression she writes is: $đW/dt = ...$ What does it mean for the $d$ to be struck like this?","['notation', 'physics', 'calculus', 'derivatives']"
3322451,Open Set in $\mathbb{Q}_p$-rational points of a torus is Zariski-dense in the torus.,"Suppose $K$ is a finite field extension of the p-adic numbers $\mathbb{Q}_p$ . Let $T$ be the algebraic torus over $\mathbb{Q}_p$ obtained by the Weil Restriction of scalars from $K$ to $\mathbb{Q}_p$ of the multiplicative group $\mathbb{G}_m$ i.e. Res $_{K/\mathbb{Q}_p}(\mathbb{G}_m)$ . Suppose $U$ is a non-empty open set of $K^*=T(\mathbb{Q}_p)$ in the $p$ -adic topology on $K^*$ . Then why is $U$ Zariski-dense in the torus $T$ ? This came up in chapter III of Serre's Abelian $l$ -adic Representations and Elliptic Curves , right after the definition of locally algebraic $p$ -adic representations . Any help would be appreciated!","['algebraic-groups', 'p-adic-number-theory', 'zariski-topology', 'algebraic-geometry', 'lie-groups']"
3322465,"There is $f$ such that $\int_0^1 f dx = \lim_{c \downarrow 0} \int_c^1 fdx$, but for $|f|$ this limit does not exist. How is that possible?","If $f$ in integrable on some interval $[a,b]$ then we know that $\lvert f \rvert $ is also integrable on that same interval. There is a problem in Rudin's Principles of Mathematical analysis such that we construct an $f$ where $$\int_0^1 f dx = \lim_{c \downarrow 0} \int_c^1 fdx$$ exists and yet for $\lvert f \rvert$ this limit fails to exist. How does this not contradict the implication above? One such constuction is to set $f(x) = (-1)^{k+1}(k+1), \forall x \in (\frac{1}{k+1},\frac{1}{k}]$ .","['integration', 'improper-integrals', 'real-analysis']"
3322474,Calculus - indefinite integration,"The integral in which I am interested in is $$\int x(x^3+1)^{33}\mathrm{d}x$$ I tried to solve by substituting $x^2 = t$ , but it didn't help. I find a solution by expanding it with the help of binomial expansion. Can anyone help me with any other method like substitution, by parts?","['integration', 'indefinite-integrals', 'calculus', 'discrete-mathematics']"
3322492,Prove: $A\cap (B - C) = (A \cap B) - (A \cap C)$,"Prove: $A \cap (B - C) = (A \cap B) − (A \cap C)$ I can understand this using Venn Diagrams, however I am struggling to translate this into a formal proof.","['elementary-set-theory', 'proof-writing', 'proof-verification', 'discrete-mathematics']"
3322498,Why does adding $\sup$ render the following probability measures not equal,"I cannot wrap my head around why the following is not equivalent: $\lim\limits_{n \to \infty} P(\sup\limits_{m \geq n}\vert X_{m}- X\vert > \epsilon )=0\iff \lim\limits_{n \to \infty} P(\vert X_{n}- X\vert > \epsilon )=0$ Obviously $\Rightarrow$ is trivial as $P(\sup\limits_{m \geq n}\vert X_{m}- X\vert > \epsilon )\geq P(\vert X_{n}- X\vert > \epsilon )$ and I cannot understand why the $\Leftarrow$ is not necessarily true. Let's say we get the following relationship from the fact that $\lim\limits_{n \to \infty} P(\vert X_{n}- X\vert > \epsilon )=0$ : say $P(\vert X_{n}- X\vert > \epsilon )\leq \frac{1}{n}(*)$ for any $n \in \mathbb N$ . Surely this means that for any $m \geq n$ that $P(\vert X_{m}- X\vert > \epsilon )\leq \frac{1}{m}\leq \frac{1}{n}$ and hence $P(\sup\limits_{m \geq n}\vert X_{m}- X\vert > \epsilon )\leq \frac{1}{n}$ Fair enough, $(*)$ may not always hold, however, my intuition tells me that: when $n$ gets larger in terms of the RV's $(X_{n})_{n}$ and only a certain number (decreasing) of outcomes $\omega$ so that: $\vert X_{n}(\omega)- X(\omega)\vert > \epsilon$ is satisfied then surely the exact same would hold for $\sup\limits_{m \geq n}\vert X_{m}(\omega)- X(\omega)\vert > \epsilon$ . Someone please explain why I am wrong","['measure-theory', 'convergence-divergence', 'probability-theory', 'probability', 'random-variables']"
3322515,Find the expression of an angle within an equation that contain sin(angle) and cos(angle),"Let's consider the following equation: $$R = \frac{p  D\cos\theta}{p - D \sin\theta}$$ Where $R,p,D>0$ and $-\pi/2\le\theta\le0$ . The goal is to transform this formula in order to write something like $\theta = f(R,p,D)$ My first try was to write the equation like below: $$Rp = D(p\cos\theta + R\sin\theta)$$ Then considering the case where $p=R$ , I was able to do this: \begin{align}
 R & = D(\cos\theta + \sin\theta) \\
 & = D\sqrt{2}[\sin(\pi/4)\cos\theta + \cos(\pi/4)\sin\theta] \\
 & = D\sqrt{2}\sin(\pi/4 + \theta) \\
 \theta& = \sin^{-1}\left(\frac{R}{D\sqrt{2}}\right) - \frac{\pi}{4} \\
\end{align} I can do the same considering the angles $\pi/3$ and $\pi/6$ but in all the cases I had to fix the relation between $p$ and $R$ which is not what I want. I also tried to use the following equation: $\cos(x)=\frac{1-t^2}{1+t^2}$ , $\sin(x)=\frac{2t}{1+t^2}$ where $t = \tan(\frac{x}{2})$ I ended with a second degree equation $$(\frac{R}{D} - 1)t^2 - 2\frac{R}{p}t + (\frac{R}{D} - 1) = 0$$ I calculated the solution  but I obtained wrong values when testing with the initial formula. probably I missing something on the road. Is there a magic step or an equation that can help me find $\theta = f(R,p,D)$ ? For the reference, this question is a extension of my answer in StakOverflow and below is the figure from where the initial equation was extracted. Probably I didn't extract the good equation and there is a better one that suits my needs.",['trigonometry']
3322556,Jensen's inequality over definite positive matrices.,From the fact  that inversion operator $A  \to A^{-1}$ is convex over the set of  positive definite matrices. I would like to know if it is correct to use  Jensen's inequality with the standard Lowner order. I.e $$ (E(X))^{-1} \leq E(X^{-1}) $$ Does the following equation also hold? $$\lambda_{max}(E(X)) \leq E(\lambda_{max}(X))$$ where $\lambda_{max}$ stands for the maximum eigenvalue of a matrix.,"['expected-value', 'jensen-inequality', 'probability-theory', 'positive-definite']"
3322611,Can $AB = \gamma BA$ for matrices $A$ and $B$,"For what values of $\gamma \in \mathbb{C}$ do there exist non-singular matrices $A , B \in \mathbb{C}^{n \times n}$ such that $$AB = \gamma BA \,?$$ So far what I have done shown that $\gamma$ must be an nth root of unity, by considering the determinant. $$det(AB) = det(\gamma BA)$$ $$det(A)det(B) = \gamma ^n det(B)det(A)$$ Now since both $A$ and $B$ are non singular we have $det(A) \neq 0$ and $det(B) \neq 0$ So: $$\gamma ^n =1$$ .
I also know that $$tr(AB - \gamma BA)=0$$ $$tr(AB) - \gamma tr(BA)=0$$ $$tr(AB)\big(1-\gamma \big) = 0$$ Clearly we can assume $\gamma \neq 1$ since surely we can find $A$ and $B$ such that they commute so we conclude that $tr(AB) = tr(BA) = 0$ Now i'm thinking that we can find matrices $A$ and $B$ for any $\gamma = e^{\frac{2 \pi i}{n}}$ such that $AB = \gamma BA$ but I cannot think of a way of constructing them. Does anyone have any ideas? thanks in  advance!","['matrices', 'linear-algebra']"
3322612,Lucas sequence - how to know which terms to calculate when testing if a number is prime?,"To check if a number is a Lucas pseudoprime , a Lucas sequence is computed. The Lucas sequence is based on a recurrence relation, but there's a method involving inspecting the bits of the number being checked for primatliy to know which terms in the sequence to compute. How exactly does inspecting the bits work? Wikipedia gives the example For example, if n+1 = 44 (= 101100 in binary), then, taking the bits
  one at a time from left to right, we obtain the sequence of indices to
  compute: $1_2$ = 1, $10_2$ = 2, $100_2$ = 4, $101_2$ = 5, $1010_2$ = 10, $1011_2$ = 11, $10110_2$ = 22, $101100_2$ = 44. Therefore, we compute U1, U2, U4, U5, U10,
  U11, U22, and U44 It says ""taking the bits one at a time"" but the example clearly is not doing that. Though the end result makes sense because the index each term is either double or one more from the previous, and we have formals to easily compute that. However in example code I've seen it appears one bit at a time is inspected. So how exactly are the bits used to decide which terms to compute? 
Are the terms known and advanced or determined while checking each individual bit?","['number-theory', 'primality-test', 'algorithms']"
3322616,$4^n - 1$ in base $2$,How would you answer this question. Write $4^n - 1$ in base $2$ . Given the context of the question I assume that $n \in \mathbb{Z}$ . I'm  fine with converting regular numbers to and from different bases but I don't know what a sufficient answer to this question would be. Please help. Thank you.,['discrete-mathematics']
3322638,How do I evaluate the antiderivative of $e^{cos(x)}$?,"Functions that do not have an elementary antiderivative can be evaluated by generating a Taylor series, provided the function is infinitely differentiable and uniformly convergent in its domain. However, some functions do not have a simple general form for its Taylor series. For example, take the following integral: $$ \int e^{cos(x)} dx $$ As the integral has no elementary antiderivative, we need to use a Taylor series to evaluate it. The Maclaurin series for $ cos(x) $ is: $$ cos(x) = \sum_{n=0}^{\infty} \frac {(-1)^n x^{2n}} {(2n)!} $$ Since $ cos(0) = 1 $ , we need to use the Taylor series of $ e^{x} $ centered at $ a = 1 $ . $$ e^{x} = \sum_{n=0}^{\infty} \frac {e (x-1)^{n}} {n!} $$ Utilizing the property of substation in Taylor series, we can generate one for $ e^{cos(x)} $ : $$ e^{cos(x)} = \sum_{n=0}^{\infty} \frac {e (cos(x)-1)^{n}} {n!} $$ Thus, the Taylor series of $ e^{cos(x)} $ is: $$ e^{cos(x)} = \sum_{n=0}^{\infty} \frac {e ((\sum_{n=0}^{\infty} \frac {(-1)^n x^{2n}} {(2n)!})-1)^{n}} {n!} $$ Clearly, this Taylor series is complex, with a series nested within in a series. I am confused as to how to compute the following integral: $$ \int e^{cos(x)} dx = \int \sum_{n=0}^{\infty} \frac {e ((\sum_{n=0}^{\infty} \frac {(-1)^n x^{2n}} {(2n)!})-1)^{n}} {n!} dx $$ The series can be simplified as such: $$ \int e^{cos(x)} dx = \int \sum_{n=0}^{\infty} \frac {e} {n!} ((\sum_{n=0}^{\infty} \frac {(-1)^n x^{2n}} {(2n)!})-1)^{n} dx $$ $$ \int e^{cos(x)} dx = \sum_{n=0}^{\infty} \frac {e} {n!} \int (- \frac{x^{2}}{2!} + \frac{x^{4}}{4!} - \frac{x^{6}}{6!} + \: ...)^{n} dx $$ Although a Taylor series can be integrated term-by-term, how does one integrate an infinite sum raised to a power? Is there a different approach to take, such as finding a general Maclaurin series for the function $ e^{cos(x)} $ by taking the derivatives and deciphering the pattern? EDIT: 
I decided to approach the antiderivative in a different way. 
Since the Maclaurin series for $ e^{x} $ is: $$ e^{x} = \sum_{n=0}^{\infty} \frac {x^{n}} {n!} $$ The series for $ e^{cos(x)} $ can be expressed as: $$ e^{cos(x)} = \sum_{n=0}^{\infty} \frac {(cos(x))^{n}} {n!} $$ $$ e^{cos(x)} = \sum_{n=0}^{\infty} \frac {{cos}^{n}(x)} {n!} $$ Thus the antiderivative of $ e^{cos(x)} $ can be evaluated: $$ \int e^{cos(x)} dx = \sum_{n=0}^{\infty} \frac {1} {n!} \int {cos}^{n}(x) $$ We can apply the reduction formula for $ \int {cos}^{n}(x) dx $ : $$ \int e^{cos(x)} dx = \sum_{n=0}^{\infty} (\frac {1} {n!}) (\frac {1} {n} {cos}^{n-1}(x) sin(x) + \frac {n-1} {n} \int {cos}^{n-2}(x) dx) $$ Hence, the antiderivative of $ e^{cos(x)} $ is as follows: $$ \int e^{cos(x)} dx = \sum_{n=0}^{\infty} \frac {1} {n \cdot n!} {cos}^{n-1}(x) sin(x) + \frac {n-1} {n \cdot n!} \int {cos}^{n-2}(x) dx $$ The reduction formula is not valid for the initial term, so it must be evaluated separately. $$ \int e^{cos(x)} dx = \int {cos}^{0}(x) dx + \sum_{n=1}^{\infty} \frac {1} {n \cdot n!} {cos}^{n-1}(x) sin(x) + \frac {n-1} {n \cdot n!} \int {cos}^{n-2}(x) dx $$ $$ \int e^{cos(x)} dx = c + x + \sum_{n=1}^{\infty} \frac {1} {n \cdot n!} {cos}^{n-1}(x) sin(x) + \frac {n-1} {n \cdot n!} \int {cos}^{n-2}(x) dx $$ I still have run into a roadblock though. My answer is not a Taylor series, as it is written as a summation of a string of trigonometric terms. Thus, I do not know how to determine the interval of convergence. If I evaluate the first few derivatives of $ e^{cos(x)} $ at the center point $ a=0 $ , I can generate the following Maclaurin series: $$ e^{cos(x)} = e- \frac {e} {2} x^2 + \frac {e} {6} x^4 - \frac {31e} {720} x^6 + \ldots \: $$ Thus, the antiderivative of $ e^{cos(x)} $ can be expressed as an infinite sum: $$ \int e^{cos(x)} dx = c + ex - \frac {e} {6} x^3 + \frac {e} {30} x^5 - \frac {31e} {5040} x^7 + \ldots \: $$ Is there a general pattern to the following Maclaurin series? If so, can a convergence test be applied to determine the interval of convergence? If there series is not convergent across all real numbers, the Taylor series is not valid. When this is the case, are there other methods to evaluate antiderivatives that do not require generating a Taylor series?","['special-functions', 'indefinite-integrals', 'taylor-expansion', 'sequences-and-series']"
3322695,Linear regression: equivalence of forms of the minimum variance affine unbiased estimator,"Background Consider the linear regression model: $$y = X\beta + e\\E[e] = 0 \quad E[ee^T] = V$$ It is well known that the minimum variance affine unbiased estimator (MVAUE) of $\beta$ exists if and only if $X$ has linearly independent columns. In this case, the MVAUE is unique and given by $$\hat\beta = My = \arg \min_\beta \, (y - X\beta)^T V_0^+ (y - X\beta)$$ where $$
\begin{align}
M &:= (X^T V_0^+ X)^{-1} X^T V_0^+ \\
V_0 &:= V + XUX^T
\end{align}
$$ and $U$ is any positive semidefinite matrix such that $\mathrm{col}\, X \subseteq \mathrm{col}\, V_0$ , where $V_0 := V + XUX^T$ . The superscript ""+"" denotes Moore-Penrose inverse. Chapter 4 (in particular, section i) of C. R. Rao's ""Linear Statistical Inference and it's Applications"" and Chapter 13 of Magnus and Neudecker's ""Matrix Differential Calculus with Applications in Statistics and Econometrics"" are two good references on the subject, for the curious. My question I want to demonstrate that the matrix $$M = [X^T (V + XUX^T)^+ X]^{-1} X^T (V + XUX^T)^+$$ is independent of the particular choice of $U$ , provided that $X$ has linearly independent columns and that $U$ satisfies the aforementioned conditions (though I would be happy with a solution that strengthened the assumption on $U$ to $U > 0$ ). I have solved the problem for two subcases, $V = 0$ and $V > 0$ , and I offer these solutions below. A proof for the case where $V \geq 0$ is singular, but nonzero, has been elusive. Partial solution: Assume $V = 0$ and $U > 0$ We will show that $M = X^+$ . First note that $MX = I$ . Therefore $XMX = X$ and $MXM = M$ . Now observe $$
\begin{align}
X^T(XUX^T)^+X &= (U^{-1}X^+XU) X^T(XUX^T)^+X (U X^T X^{+T} U^{-1}) \\
&= (U^{-1}X^+) (XU X^T) (XUX^T)^+ (X U X^T) (X^{+T} U^{-1}) \\
&= (U^{-1}X^+) (XU X^T) (X^{+T} U^{-1}) \\
&= U^{-1} U U^{-1} \\
&= U^{-1}
\end{align}
$$ Therefore $$
\begin{align}
XM &= X [X^T (XUX^T)^+ X]^{-1} X^T (XUX^T)^+ \\
&= XUX^T (XUX^T)^+
\end{align}
$$ is symmetric, due to the properties of $(XUX^T)^+$ . We have demonstrated that $M$ satisfies all four conditions required to be the Moore-Penrose inverse of $X$ . Partial solution: Assume $V > 0$ and $U > 0$ Applying the Woodbury matrix identity gives $$
\begin{align}
(V + XUX^T)^{-1} &= V^{-1} - V^{-1}X(X^TV^{-1}X + U^{-1})^{-1}X^TV^{-1} \\
(V + XUX^T)^{-1}X &= V^{-1}X(X^TV^{-1}X + U^{-1})^{-1}U^{-1} \\
X^T(V + XUX^T)^{-1}X &= X^TV^{-1}X(X^TV^{-1}X + U^{-1})^{-1}U^{-1} \\
[X^T(V + XUX^T)^{-1}X]^{-1} &= U + (X^TV^{-1}X)^{-1}
\end{align}
$$ Combining these results in the correct manner (I'll omit the details for now, for the sake of brevity, but I would be happy to give them upon request), one can derive $$M = (X^TV^{-1}X)^{-1}X^TV^{-1}$$","['statistics', 'regression', 'matrices', 'pseudoinverse', 'linear-algebra']"
3322723,"Collection of intervals covers $[0,1]$?","For each $n=1,2,3,...$ and each $m=0,1,2,...,n-1$ , let $$K^n_m = \left[ \frac{3m-n+1}{n} , \frac{3m-n+2}{n} \right] \subset (-1,2). $$ I am struggling these with two questions for quite some time: (1) $ \ $ The collection $\{ K^n_m \} \setminus \{ K^1_0 \}$ covers the interval $[0,1]$ ? In case the answer for (1) is yes, then (2) $ \ $ For each $ \ x \in [0,1] \ $ is there an infinite number of $ \, K^n_m \, $ such that $ \ x \in K^n_m \ $ ?","['real-numbers', 'rational-numbers', 'real-analysis']"
3322733,"The question asks to find three functions, $f(n). g(n), h(n)$ that satisfy the following conditions","The question asks to find three functions, $f(n). g(n), h(n)$ that satisfy the following conditions: the conditions are: $f \not \in O(h)$ $g \in \Omega (h)$ $(f-g) \in O (h)$ $(f-g) \not \in \Omega (h)$ from these four condition I've gathered that $f>h$ , $g>h$ and $f-g<h$ So my answer is $h = n$ , $f=n^2+3$ , $g=n^2$ . Is my answer correct?:","['asymptotics', 'discrete-mathematics']"
3322738,Bézout's theorem intuition,"I am not taking algebraic geometry or trying to prove anything. I'm just looking for a simple intuitive understanding of Bézout's theorem for the case when one of the curves is a constant function. From https://en.wikipedia.org/wiki/B%C3%A9zout%27s_theorem#Examples we have the example: ""Two distinct non-parallel lines (in the same plane) always meet in exactly one point. Two parallel lines intersect at a unique point that lies at infinity."" But take y = 1 and y = x. These two lines meet once at (1,1), but the product of the degrees of these curves is 0*1 = 0. I feel like I'm missing something very simple or don't understand the hypotheses of the theorem... can someone please help explain?
Thanks!",['algebraic-geometry']
3322765,Find integral area with respect to x,"The question is finding the area enclosed by the curves x= $y^2$ and x+2y=8 using both x and y integrals Graph for reference Purple is x+2y=8, red is x= $y^2$ First I found the limits by letting x=8-2y. This gave the equation $y^2$ +2y-8 which gave y=-4 and y=2. Putting back into equation gives x=4 and x=16 I calculated with y integral being $$\int_4^2 (8-2y)-y^2 \,$$ ( lower integral is -4, I can't seem to express) Now I would like to ask how would you calculate using the x integral I had a think about this and got but the answers x and y integrals are different so I think theres a mistake somewhere but I don't know what I did wrong.","['integration', 'area', 'definite-integrals', 'analysis']"
3322768,Differential equations book that explains differential forms,"All books that I have read so far say something like this: From your calculus course you probably know differential forms: $$M(x,y)dx + N(x,y)dy = 0$$ This is really bothering me. Even though I am fine with intuition that $y' = \frac{dy}{dx}$ can be thought of as a ratio, all analysis and calculus books kept telling me that we can't do that. Thats why differential equation above has no meaning for me right now. So my question is, are there books on DEs that give justification for using $dx$ and $dy$ separately?",['ordinary-differential-equations']
3322810,"$f(f(x)) = 1 + x^2$, then what is f(1)?","I get $f(f(a)) = a^2 + 1 = f(f(-a))$ , and so $f(a)^2 + 1 = f(a^2 + 1) = f(-a)^2 + 1$ , so $f(a) = f(-a)$ or $f(a) = -f(-a)$ , but then I donot know what to do next. Thanks for any help.","['functional-equations', 'functions']"
3322857,Maximizing $ 3^{\sin^2{\theta}} \cdot 27^{\cos^2{\theta}} + 8^{\sin{\theta}}\cdot 16^{\cos{\theta}} $,"Find the maximum value of $$ 3^{\sin^2{\theta}} \cdot 27^{\cos^2{\theta}} + 8^{\sin{\theta}}\cdot 16^{\cos{\theta}} $$ Where does maximum of the expression occur? I can find maxima of individual terms easily, but since they occur at different values of $\theta$ , that is not getting anywhere. If I differentiate, it becomes very tedious. Is there any clever rearrangement or logic, without calculus, which will get me max value of this expression? I tried arithmetic mean greater than geometric mean but could not get anywhere. Thank you.",['trigonometry']
3322862,Is there a closed form for $\int_a^b\frac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}}$?,"I have solved $$\int^3_1\dfrac{{\rm arccosh}x}{\sqrt{(x-1)(3-x)}}{\rm d}x=4G$$ where $G$ is Catalan constant, by rewriting it into double integral $$\iint_{[0,\pi]\times[0,\pi]}\ln(2-\cos x-\cos y){\rm d}x{\rm d}y$$ and   computing it using Leibniz integral rule. Now my question here is: Is there a closed form for integral $$\int_a^b\dfrac{{\rm arccosh}x}{\sqrt{(x-a)(b-x)}}{\rm d}x$$ as $b>a>0$ ?
  Or it is just a coincidence?","['integration', 'definite-integrals', 'real-analysis', 'closed-form', 'sequences-and-series']"
3322907,Convergence of measure topology,"Let $(\Omega,\mu)$ be a $\sigma$ -finite measure space. Let $1\leq p<\infty.$ Suppose $T:L^p(\Omega)\to L^p(\Omega)$ be a bounded positive function, i.e. it takes nonnegative functions to nonnegative functions. Suppose he restriction of $T$ on any $L^p(A)$ , where A has finite measure, is continuous in topology of convergence in measure for both domain and codomain ( $L^p(A)$ has the $\sigma$ algebra as the restriction of the whole $\sigma$ -algebra). Does it imply that $T:L^p(\Omega)\to L^p(\Omega)$ is continuous in the topology of convergence in measure?","['general-topology', 'lp-spaces', 'measure-theory']"
3322951,Show that the orthogonal projection onto $Range T$ is equal to $T(T^\ast T)^{-1}T^\ast$ given that $T: V \to W$ is injective,"Given $V$ and $W$ as finite-dimensional inner product spaces and an injective linear map $T: V \to W$ , how can we show that $$P_{Range T} = T(T^\ast T)^{-1}T^* \in \mathcal{L}(W)$$ where $P_{Range T}$ is the orthogonal projection onto $Range T$ and $T^\ast$ is the adjoint of $T$ ? Supposedly this result can be used to derive a useful matrix formula for orthogonal projection, but  I'm not sure how to figure out how we can show the equality above. Does $(T^\ast T)^{-1}$ even make sense? How do we show that? And what is the intuition of the above formulation? Among my ideas are trying to show that the right hand side is in the range of $T$ (which it seems to be) and trying to utilize the fact that $P_{Range T}^2 = P_{Range T}$ . But I'm not sure how to proceed from there.",['linear-algebra']
3322989,Help showing that $ (D \setminus A) \setminus (C \setminus B) = D \setminus (A \cup (B \cup C)) $,"I wanted to show that $$ (D \setminus A) \setminus (C \setminus B) = D \setminus (A \cup (B \cup C)) $$ Since I did not know how to approach this, I let wolfram alpha do the work and got the following results: https://www.wolframalpha.com/input/?i=(D%5CA)%5C(C%5CB)%3DD%5C(A+union+B+union+C) It says: undetermined (can be true or false). So, it can be true or false. But on what does it depend whether the solution is true or false? If I take for example the sets: $$ A=\{1,2,3,4,5\} $$ $$ B=\{2,1,3,4,6\} $$ $$ C=\{3,1,2,4,7\} $$ $$ D=\{4,1,2,3,8\} $$ then $$ (D \setminus A)\setminus (C\setminus B) = \{8\} $$ $$ D \setminus (A \cup B \cup C) = \{8\} $$ But what would be cases, in which this relation does not hold?",['elementary-set-theory']
3323005,Can $e^x$ be expressed as a linear combination of $(1 + \frac x n)^n$?,"Can $e^x$ be expressed as a linear combination of $(1 + \frac x n)^n$ ? In other words, does there exist an infinite sequence $(a_k)_{k \in \mathbb N_0}$ such that $$e^x = a_0 + \sum_{1 \leq k < \infty} a_k \left(1 + \frac x k\right)^k$$ for all $x \in \mathbb R$ ? Call the series on the right $s(x)$ . I can answer the question in the negative when the series is absolutely convergent. In the conditionally convergent case, I'm not so sure. My thoughts were to use the fact that: $$e^{x - \frac{x^2}{2k}} \leq (1+ \frac x k)^k \leq e^{x}$$ and use the lower bound when $a_k$ is negative, and the upper bound when $a_k$ is positive. This gets stuck because it's not always the case that if some $b_k$ is a decaying sequence then $\sum_{k} \frac{b_k}{k}$ is convergent. The strengthened inequality $$e^{x - \frac{x^2}{2k}} \leq (1+ \frac x k)^k \leq e^{x - \frac{x^2}{2k} + \frac{x^3}{3k^2}}$$ looks like it might make more progress... [EDIT 2019/08/14 14:00 GMT] This is the solution in the absolutely convergent case, given by lemmas 1 and 2 : Definition : Let $s(x) = a_0 + \sum_{1 \leq k < \infty} a_k \left(1 + \frac x k\right)^k$ . Lemmas and proofs follow: Lemma 1 : If $s(x)$ converges absolutely for some $x\geq 0$ , then $s(x)$ converges absolutely for all $x \geq 0$ . Proof Pick an $x_0 \geq 0$ for which $s(x_0)$ converges absolutely. By the condition stated in the lemma, the series $\sum_{1 \leq k < \infty} |a_k| \left|1 + \frac {x_0} k\right|^k$ must converge. We also observe that $|a_k| \leq |a_k| \left|1 + \frac {x_0} k\right|^k$ is true for all $k$ . So by the Direct Comparison Test, the series $\sum_{0 \leq k < \infty} |a_k|$ must also converge. In other words, $s(0)$ is absolutely convergent. Consider now any $x \geq 0$ . The series $\sum_{0 \leq k < \infty} |a_k| e^{x}$ converges because it is equal to $e^{x} \sum_{0 \leq k < \infty} |a_k|$ , which we proved to be convergent in the previous paragraph. We observe that $|a_k| \left|1 + \frac {x} k\right|^k \leq |a_k| e^{x}$ is true for all $k$ . So by the Direct Comparison Test, the series $|a_0| + \sum_{1 \leq k < \infty} |a_k| \left|1 + \frac {x} k\right|^k$ must also converge. So by the definition of absolute convergence, we have that $a_0 + \sum_{1 \leq k < \infty} a_k \left(1 + \frac x k\right)^k=s(x)$ converges absolutely, where $x \geq 0$ was arbitrary. $\blacksquare$ Lemma 2 : If $s(x)$ converges absolutely when $x \geq 0$ , then for large enough $x$ we have that $e^x > s(x)$ . Proof Let $z_n(x) = |a_0| + \sum_{1 \leq k < n} |a_k| \left(1 + \frac x k\right)^k$ . Pick some $\epsilon < \frac 1 2$ . Observe that there must be a large enough $n$ such that $z_\infty(0) - z_n(0) \leq \epsilon$ . Using the triangle inequality, we have that: $$\begin{aligned}
|s(x)| &\leq z_\infty(x)\\
&\leq z_n(x) + (z_\infty(x) - z_n(x))\\
\end{aligned}$$ Since $z_n(x)$ is a polynomial, there is a large enough $X$ such that all $x \geq X$ it's true $z_n(x) < \epsilon \cdot e^x$ . So we have that $$\begin{aligned}
|s(x)| &<\epsilon\cdot e^x + (z_\infty(x) - z_n(x))\\
&\leq \epsilon\cdot e^x + (z_\infty(0) - z_n(0)) e^x\\
&\leq \epsilon\cdot e^x + \epsilon\cdot e^x\\
& = 2\epsilon \cdot e^x\\
&< e^x.
\end{aligned}$$ The claim above that $z_\infty(x) - z_n(x) \leq (z_\infty(0) - z_n(0)) e^x$ follows from $$\begin{aligned}
&|a_k| \left(1 + \frac x k\right)^k \leq |a_k| e^x\\
\implies &\sum_{k \geq {n+1}}\left(1 + \frac x k\right)^k \leq \sum_{k \geq {n+1}}|a_k| e^x\\
\implies & z_\infty(x) - z_n(x) \leq (z_\infty(0) - z_n(0)) e^x
\end{aligned}$$ We are done. $\blacksquare$","['conditional-convergence', 'exponential-function', 'sequences-and-series']"
3323018,"Negating ""$\forall n\in \Bbb N$ and $\forall\varepsilon>0$, $\exists A\in\mathcal A$ such that $n<|a|<n+\varepsilon$ for infinitely many $a\in A$.""","Let $\mathcal A$ be a collection of subsets of $\Bbb R$ . "" $\forall n\in \Bbb N$ and $\forall \varepsilon>0$ , $\exists A\in\mathcal A$ such that $n<|a|<n+\varepsilon$ for infinitely many $a\in A$ ."" What is the negation of above statement? Is this the correct negation? "" $\exists n\in \Bbb N$ and $\exists\varepsilon>0$ such that for each $A\in \mathcal{A}, 
n<|a|<n+\varepsilon$ holds for finitely many $a\in A$ ."" Some of my friends say that negation is: "" $\exists n\in \Bbb N$ and $\exists\varepsilon>0$ such that for each $A\in \mathcal A, 
n\ge|a|$ or $|a|\ge n+\varepsilon$ holds for infinitely many $a\in A$ ."" Which one of these is correct?","['quantifiers', 'logic', 'real-analysis', 'elementary-set-theory', 'logic-translation']"
3323031,An interesting algorithm about prime numbers that I thought today,"I thought up the following algorithm today: Choose $a_1\in\mathbb{Z}^+\setminus\{1\}$ . Then let $a_{n+1}=a_n+p_n$ , where $p_n$ is the largest prime factor of $a_n$ . The algorithm is easy, but I have the following questions: $1)$ Is the sequence $\{p_n\}$ monotonically increasing? $2)$ Are there infinitely many primes in the sequence $\{p_n\}$ ? $3)$ Find all $a,b\in\mathbb Z^+$ such that there are infinitely many primes of the form $ak+b$ where $k$ is a nonnegative integer in the sequence $\{p_n\}$ . I thought up a solution for $1)$ and $2)$ , as follows: $1)$ Assume the contrary, i.e. $p_n>p_{n+1}$ for some $n$ . As $p_n|a_n$ , $p_n|a_n+p_n\iff p_n|a_{n+1}$ . So $p_n\le p_{n+1}$ . A contradiction rises. $2)$ Yes. Assume the contrary, there are finitely many primes in the sequence $\{p_n\}$ . Then as $\{p_n\}$ is monotonically increasing,  So there exist an $m$ such that $\forall i\ge m, p_i=p_m$ . So $a_i=a_m+(i-m)p_m$ . Also, we let the smallest prime larger than $p_m$ be $p$ . But as $(p, p_m)=1$ , $\exists i<p+m$ such that $p|a_i$ . A contradiction rises. I think the above solutions seems correct, but can you help to verify? Also, can someone help me to do $3)$ ? Any help is appreciated!","['number-theory', 'proof-writing', 'proof-verification', 'algorithms', 'prime-numbers']"
3323043,Function is differentiable at a point so its differentiable in a region from the point,"Given some function $f: I \subseteq\mathbb R \rightarrow \mathbb R$ , Which is differentiable twice at some point $a\in I$ .
Can one say that there is a region around the point where the function is differentiable twice, without any other information? so I assume that its true because if we look at the first derivative which is: $lim_{h\rightarrow0} \frac {f(a+h)-f(a)}h $ then obliviously we can ""take"" h to be smaller as we want and then I can assume that if the limit exists then it exists at some region of that point a. so I guess the same goes for the second derivative.","['differential', 'calculus', 'derivatives']"
3323044,An element not in this (baby) cylindrical sigma-algebra?,"Consider $\{0,1\}^\mathbb N$ , a generic element of which I denote by $x= \{x(i)\}_{i \in \mathbb N}$ . Three possible sigma-algebras on this set are : The sigma-algebra $\mathcal E$ generated by the singletons. The cylindrical sigma-algebra $\mathcal F$ generated by $\{x(i)^{-1}(\{j\}) : i \in \mathbb N, j \in \{0,1\}\}$ The powerset $\mathcal G$ They verify $\mathcal E \subset \mathcal F \subset \mathcal G$ , and the first inclusion is strict since $\{0,1\}^\mathbb N$ is not countable. The sigma-algebra generated by the singletons is the set of the countable or co-countable  subsets. So $x(1)^{-1}(\{0\})$ is an element of $\mathcal F \setminus \mathcal E$ . Similarly, in case the inclusion $\mathcal F \subset \mathcal G$ is strict, can one construct an (as explicit as possible) element of $\mathcal G \setminus \mathcal F$ ? I thought about transposing the classical example of the Vitali set of the real line but this does not seem so easy... Also, the kind of example here Cylindrical sigma algebra and continuous functions. does not directly apply in the very simple setting we consider.","['general-topology', 'probability']"
3323105,"Non-unique factorization of ideals in $\mathbb{Z}[t,t^{-1}]$","Edited version: In a Dedekind domain $R$ , every nonzero proper ideal factors uniquely as a product of prime ideals. If $R$ is a Noetherian domain, then by this post any ideal $I$ which does factor into a product of primes, does so uniquely. The ideal class monoid of $R$ is the quotient of the monoid of nonzero ideals of $R$ under multiplication by the equivalence relation $I\sim J$ if there exist $x,y$ so that $(x)I=(y)J$ . If $R$ is also a UFD, irreducible elements are prime, so all nonzero proper principal ideals factor into a product of primes. Therefore, if $I$ factors as a product of primes, it can be factored as a principal ideal times some unique nonprincipal prime ideals $p_1,\dots,p_n$ , and since $(x)I$ also has a unique prime factorization for all nonzero $x\in R$ its ideal class will consist of $\{(x)p_1\cdots p_n:x\in R\}$ . I am wondering if this ever doesn't happen, ie. if there can be an ideal class which is not of the form $\{(x)I\}$ for some ideal $I$ . By the above discussion a necessary requirement is that $I$ does not factor as a product of prime ideals. The ring I'm interested in is $\mathbb{Z}[t,t^{-1}]$ , which has trivial Picard group, so the only invertible ideals are already principal. Another way to think about this question is the following: consider a graph $\Gamma$ with vertices the nonzero ideals of $R$ and a directed edge $I\rightarrow J$ whenever there is some nonzero $x\in R$ so that $J=(x)I$ . The connected components of this graph are the ideal classes of $R$ . Maximal ideals are ""roots"", so is any product of nonprincipal prime ideals, and by the above they are the only roots of their connected components. I would like an example of an ideal class which has more than one root. Otherwise, every ideal class has a unique root $I$ , and the only ideals $J\sim I$ are of the form $J=(x)I$ , which is not very interesting. A case of two roots in the same component will yield an equation $(x)I=(y)J$ , where $I$ and $J$ are not both principal ideal multiples of a third ideal $K$ , so this is a particular way an ideal can have distinct factorizations, hence the original question. Thanks for any comments or questions! [Original post: I know that unique factorization of nonzero proper ideals in an integral domain $R$ is equivalent to being a Dedekind domain, and that my ring of interest, $\mathbb{Z}[t,t^{-1}]$ , is not Dedekind, because it has dimension 2 and Dedekind domains have dimension 1. So $R=\mathbb{Z}[t,t^{-1}]$ must have a nonzero proper ideal which factors non-uniquely. What is an example? I would think that such an ideal would not factor as a product of prime ideals, e.g. $(4, t+1)$ appears to be unfactorable, yet is not prime. My strategy so far has been to find unfactorable ideals $I$ , $J$ so that $(f)I=(g)J$ for some nonzero $f,g\in R$ . Really what I am interested in is an example of an ideal class in the ideal class monoid of $R$ which is not of the form $\{(f(t))I:f(t)\in R\}$ for some ideal $I$ , but my hope is that an ideal which factors non-uniquely will supply such an example.]","['ideal-class-group', 'number-theory', 'abstract-algebra', 'factoring', 'commutative-algebra']"
3323106,Does the Monty Hall problem occur on this situation?,"Lets say, I have the following situation: I know, that an alarm will go on on a certain day. It will go on on any day from Monday to Sunday. On the week before, I know that the possibility is equal (1/7). My question:
When it's Wednesday, two days have passed. Is the possibility for the alarm getting on still 1/7 or is it changed? And why or why not? I think the possibility didn't change, because it seems like to be like the monty hall problem.","['monty-hall', 'probability']"
3323133,Questions about the Schroeder-Bernstein Theorem,"The following is the Schroeder-Bernstein Theorem in Real Analysis with Real Applications by Donsig and Davidson p. $63$ : There are certain parts of the proof that I'm having trouble understanding. When I try drawing a diagram using Figure $2.6$ as a reference, for example, I'm unable to complete the diagram using a finite number of points. That is, using Figure $2.6$ I replaced the ""ellipsis block"" in both $A$ and $B$ by $A_{4}$ and $B_{4}$ . Then I assigned exactly one point to each of $A_{1},A_{2},A_{3},A_{4},$ and $A_{5}$ . I repeated the process with $B$ and assigned exactly one point to each $B_{1},B_{2},B_{3},B_{4},$ and $B_{5}$ . But then applying the recursive process in the given proof, I can't apply the function $f$ to $A_{4}$ since this would imply the existence of some subset $B_{5}$ of $B$ which doesn't exist in the diagram that I've constructed. I'd like to know where I'm going wrong or what important assumption I'm missing. Also, I'm unsure what $(fg)^{i-1}$ is supposed to represent. Is it supposed to be some sort of composite function made up of both $f$ and $g$ , raised to the power $i-1$ ?","['elementary-set-theory', 'proof-explanation']"
3323174,"Finite Sets, Equal Cardinality, Injective $\iff$ Surjective.","This proof seems odd to me. I have come to the conclusion that I will use induction. I would like to see a smoother way or just some improvements on my technique. Let $f:A\rightarrow B$ be a function between two finite sets of equal cardinality. Show that $f$ is surjective if and only if it  is injective. To start, I will show that a surjection implies an injection using induction. I will dismiss the cases that both sets are empty or contain one element as being trivial (essentially vacuously true). Assume $|A| \geq 2$ , $|B| \geq 2$ , $|A| = n = |B|$ , and $f:A \rightarrow B$ is a surjection. For the base case, let $n = 2$ . There are two elements in both $A$ and $B$ . Due to surjection, every element $b \in B$ must be mapped to, through $f$ , by at least one element $a \in A$ . If each of the two elements in $B$ were mapped to by the same element in $A$ , the definition of function would be violated. Therefore, they are mapped to by unique elements in $A$ . Thus, for $f(p), f(q) \in B$ , if $f(p) = f(q)$ , it must be true that $p = q$ so $f$ is injective. Now assume that the surjection implies an injection for $n \geq 2$ . We must show this to be true for $|A| = n + 1 = |B|$ . Since it is true for $|A| = n = |B|$ , the $n + 1$ case represents the addition of one new element to both $A$ and $B$ . The new element in $B$ cannot be mapped to any other element in $A$ except for the new one. If mapped to by an old one, the definition of function would be violated. It must be mapped to by something since $f$ is surjective, hence it must be the new element. Finally, the new element in $A$ cannot be mapped to an old element in $B$ because it is unique and the previous $B$ was shown to be injective. $$\blacksquare$$ This is a very wordy and awkward proof in my opinion. I have been out of proofs for a long time. I would like to see one that is more clear or seek validation if there isn't. I know that I have only completed half of the proof and have yet to go the other way.","['elementary-set-theory', 'proof-writing', 'functions', 'proof-verification']"
3323175,How to find the shaded region [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 4 years ago . Improve this question Find the area of the blue shaded region of the square in the following image: [Added by Jack:] The area of the triangle in the middle of the square is given by $$
4.8\times 6=28.8\  (cm^2)
$$ Other than this, it seems difficult to go further with the given information. It seems that one has yet to use the assumption of ""square"". How can one solve this problem?","['area', 'geometry']"
3323193,Is a Chern class a property of a vector bundle or of a section of the bundle?,"Coming from a physics point of view, I learned from Green, Schwarz, Witten that (for instance) ""the second Chern class of an $SU(N)$ gauge field"" is an integral class of $H^4(M,\mathbb R)$ for a manifold $M$ . In particular, if $A = A^a_\mu(x) dx^\mu T^a$ (where $T^a$ is Lie-algebra-valued and repeat indices are summed) is a particular $SU(N)$ gauge field (i.e. a section of the vector bundle $E$ of Lie-algebra valued one-forms over $M$ ), then we can form the field strength $F = dA + A\wedge A$ , and the second Chern class is $\text{Tr} F\wedge F/ 8\pi^2$ . So a Chern class is a property of a section of a vector bundle . Learning Chern classes from Hatcher's book, I read that Chern classes assign to ""each vector bundle $E\to B$ a class $c^{2i} \in H^{2i}(B, \mathbb Z)$ "". So the Chern class is a property of a vector bundle . Is there some canonical choice of section (gauge field) made in the latter that is not made in the former, physics, definition? Any conceptual clarification would be helpful.","['mathematical-physics', 'vector-bundles', 'algebraic-topology', 'differential-geometry']"
3323239,$f(z)=z^n+a_{n-1}z^{n-1}+\cdots+ a_0\in\mathbb Z[z]$ has all its roots on the unit circle. Prove that any root of $f(z)=0$ is a root of unity.,"Suppose that $f(z)=z^n+a_{n-1}z^{n-1}+\cdots+ a_0\in\mathbb Z[z]$ has all its roots on the unit circle in the complex plane. Prove that any root of $f(z)=0$ is a root of unity. This question has been asked before, yet it links to another MO post which proves a stronger result: Let $f$ be a monic polynomial with integer coefficients in $x$ . If all roots of $f$ have absolute value at most $1$ , then $f$ is a product of cyclotomic polynomials and/or a power of $x$ (that is, all nonzero roots are roots of unity). David E Speyer gave a short and relatively elementary proof. But other answers, and most likely the standard approaches, involve Galois theory. So I am looking for other methods proving the statement which requires the roots lying on the unit circle without invoking Galois theory. Thank you.","['complex-analysis', 'number-theory', 'polynomials']"
3323265,Norm of Finite Product of Normed Spaces generates Product Topology,"Let $(X_i,||\cdot||_i)$ be normed spaces where $i=1,2,...,n \in \Bbb Z_+$ . Let $X$ be the product space of these normed spaces equipped with product topology. For $x=(x_1,x_2,...,x_n) \in X$ where $x_i \in X_i$ $(i=1,2,...,n)$ set: $$||x|| := \max \{||x_1||_1,||x_2||_2,...,||x_n||_n\}$$ How to prove that this norm $||\cdot||$ generates the product topology on $X$ ?","['general-topology', 'normed-spaces', 'functional-analysis', 'products']"
3323277,Application of maximum modulus theorem to $f(z)$ and $\frac{1}{f(z)}$,"Let $G\subseteq \mathbb{C}$ be a bounded domain  with $0 \in G$ and $f:\bar{G}\rightarrow\mathbb{C}$ is continous. Furthermore let $f$ be holomorphic in $G$ and let $|f(z)| \geq e^{Re (z)}$ for all $z\in\partial G$ and $|f(0)|<1$ . I've got to show that $f$ has a zero. My attempt: Suppose $f(z)$ has no root. So that $|f(z)|>0$ is always fullfilled.
Since the conditions for the maximum modulus theorem are fullfilled we apply it to $f(z)$ and $\frac{1}{f(z)}$ (which is also holomorphic on $G$ as $f(z) \neq 0$ ) That is: $0 \leq |e^{Re (z)}|\leq |f(z)| \leq f(C_1)$ for some $C_1 \in \partial G$ , and $0 \leq | \frac{1}{f(z)}|\leq  |\frac{1}{e^{Re (z)}}|\leq \frac{1}{f(C_2)}$ for some $C_2 \in \partial G$ Now in the second row we get for $z=0$ : $\frac{1}{f(0)}\leq 1$ which is contradiction to the assumption that $|f(0)|<1$ . Is this proof correct? I mean can $0 \in \partial G$ be true when only $0\in G$ is assumed? Thank you for your help.","['complex-analysis', 'maximum-principle']"
3323317,Some distributions / auto-correlations associated with irrational numbers,"Given a number $x\in [0, 1]$ , let us consider the sequence $z_n=\{b^n x\}$ where the curly brackets represent the fractional part function, and $b>1$ is an integer. In particular, $\lfloor b z_n\rfloor$ is the $n$ -digit of $x$ in base $b$ . The following property is true for most real numbers $x\in [0, 1]$ , for all positive integers $k$ , though there are infinitely many exceptions (all rational numbers are exceptions): $c(x,k) = \mbox{Correl}(z_n,z_{n+k}) = b^{-k}$ . The correlation here is an empirical auto-correlation of lag $k$ computed on the observed values in the sequence $z_n$ . This result is true for all numbers $x$ but a set of Lebesgue measure zero. Not sure if it is a well known result or not, but I formally proved it, and this is not the object of this question. Empirical evidence also suggests it is correct. This result is true only for normal numbers, that is, in a nutshell, numbers having a uniform distribution on $[0, 1]$ for $z_n$ (the vast majority of numbers.) Not all irrational numbers are normal, for instance $0.101001000100001...$ is irrational but not normal in base $2$ . The contants $\pi,\log(2), e, \sqrt 2, \sin(1)$ are believed to be normal, after extensive statistical testing up to 22 trillions of digits, though there is no proof. Now is the interesting part of the discussion. I am doing some tests, computing the following correlations for some number $x$ , with $b=2$ and $f(n)=n$ : $g(x,k) =\mbox{Correl}\Big(\{xf(n)\},\{b^k xf(n)\}\Big), k=0, 1, 2 \cdots.$ You would also expect, if $f(n)$ is a well behaved sequence of integers, say $f(n) = n$ , that $g(x,k) = c(x, k)$ . My question is whether you can find an irrational number $x$ that is non-normal, for which $g(x,k) \neq b^{-k}$ for at least some values of $k$ , say $k=1, 2, 3$ or $4$ . Here we can use $b=2$ for simplicity. All the irrational numbers that I tested so far (even weird ones) seem to satisfy $g(x,k) = b^{-k}$ , and none of the rational numbers I tested do. I am very interested in finding an irrational number (obviously it would be a non-normal one) for which this equality is NOT satisfied. A positive answer to my question may lead to new criteria to characterize normal numbers.","['statistics', 'number-theory', 'irrational-numbers', 'stochastic-processes', 'sequences-and-series']"
3323330,"Evaluating the integral: $ I = \int e^{\frac xa} \sin x \, \mathrm dx$","Evaluating the integral: $$ I = \int e^{\frac xa} \sin x \, \mathrm dx \tag {1}$$ This question was asked in CBSE Board 12th Grade (India). So, here was the approach I made. Proposition 1: $$ for, \, y= u(x), \forall \, x \in \mathbb{R} $$ $$ \int e^{\frac xa} u(x) \, \mathrm dx =  a e^{\frac xa} \left ( au(x) - a^2\dfrac{\mathrm du(x)}{\mathrm dx} + a^3\dfrac{\mathrm d^2u(x)}{\mathrm dx^2} - \dots \right ) \quad \dots\tag {*} $$ Proof: This can easily be proved by applying by parts in LHS and subtracting it with RHS to a quantity which can be made small than any other assignable quantity as required. So, using the same to evaluate the integral $(1)$ , we get: $$I = ae^{\frac xa} \left ( (\sin x) - (\cos x) + (-\sin x) - (-\cos x) + (\sin x) - (\cos x) + (-\sin x) - (-\cos x) + (\dots) \right) $$ Clearly, the repetitions of sine and cosine functions inside the brackets in RHS are cancelling each other, so irrespective of the value of $x$ , the series should converge to '0'. $$\therefore I = 0$$ But, wait, the integrand is continuous and is strictly increasing and strictly decreasing for particular intervals of $x$ . This is enough to show that my answer is wrong, but what I missed? Edit: This question is more like why my approach failed then What is the correct way to find the solution of the question Edit 2: Thanks to @J.G for pointing out that my proposition had issues. I've fixed that part now :)","['integration', 'indefinite-integrals', 'proof-verification']"
3323345,$ \mu((A\setminus B)\cup (B\setminus A))=0$ implies $\mu(A)=\mu(B)$,"Let $(\Omega,\mathcal{F},\mu)$ be a measure space and let $A,B \in \mathcal{F}$ . Show that if $ \mu((A\setminus B)\cup (B\setminus A))=0$ then $\mu(A)=\mu(B)$ . My attempt Firstly, $ \mu((A\setminus B)\cup (B\setminus A))=0 \implies \mu(A\setminus B)=- \mu(B\setminus A)$ by the finite additivity of the measure (and the disjointness of the sets). It implies that $\mu(A\setminus B)=\mu(B\setminus A)=0$ by the nonnegativeness of the measure. If $A\cap B=\emptyset$ , then $\mu(A)=\mu(B)=0$ . Update Define $C=A\cap B$ . It holds that $$C\subseteq A \implies \mu(A\setminus C)+\mu(C)=\mu(A)$$ and $$C\subseteq B \implies \mu(B\setminus C)+\mu(C)=\mu(B)$$ . Now $A\setminus C=(A\setminus A)\cup (A\setminus B)=A\setminus B$ . Similarly, $B\setminus C=B\setminus A$ . Both are zero, by the above consideration. Hence $\mu(A)=\mu(B)=\mu(C)$ , as desired.","['measure-theory', 'proof-verification']"
3323389,Steady states of $u_t= u_{xx}+\pi^2u$,"I just put the following one-dimensional reaction-diffusion equation in Mathematica: $$u_t= u_{xx}+au$$ with $\Omega=(0,1)$ with Dirichlet boundary conditions. When $a<9$ , no matter the initial condition I choose, the solution decays to $0$ : (time interval=[0,20]) But for $a>10$ the solution grows to infinity: However , when I choose exactly $a=\pi^2$ , every smooth initial condition I tried seems to give rise to a constant solution, here's for example $u_0(x)=-x^2+x$ : It gives the same result for all functions I tried like $u_0(x)=-3x^2+3x$ or $u_0(x)=\sin(\pi x)$ . I tried to find analytically the steady states of $u_t= u_{xx}+\pi^2u$ and I found all the functions of the form $u_0(x)=B\sin(\pi x)$ . But why a function like $u_0(x)=-x^2+x$ seems to be also a steady state in the simulation? Could it be that $u_0(x)=-x^2+x$ gives rise to a solution that converges immediately to a $B\sin(\pi x)$ function?","['heat-equation', 'steady-state', 'ordinary-differential-equations', 'partial-differential-equations']"
3323442,Binomial random variable with coin flips. How do we get the formula $P(X=h) = \binom{n}{h}p^h(1-p)^{n-h}$?,"Let's say a coin produces $H$ (for heads) with probability $p$ (and thus it produces $T$ with probability $p-1$ . Let $X$ denote the number of $H$ s we see and let $h$ be an integer. Then, if we flip the coin $n$ times: $P(X=h) = \binom{n}{h}p^h(1-p)^{n-h}$ I'm not quite understanding where this formula is coming from. I can see that there are $\binom{n}{h}$ ways to choose a $h$ -length subset of an $n$ -length set (i.e. there are $\binom{n}{h}$ $n$ -length sequences where we see $h$ heads), but why do we multiply that with the probability? Furthermore, why do we multiply the probabilities of heads and tails together? I get that coin flipping is an independent repeated trial, but I'm not sure how to apply that here (or whether it's even relevant). Any help is appreciated!","['binomial-coefficients', 'discrete-mathematics', 'probability']"
3323489,The need for outer measure when extending measure,"I am curious why Ash et al. (1999) introduces the outer measure when the authors try to extend the measure to a large class of sets. Here is the basic roadmap they take: Begin with $\mathscr{F}_0$ , a field of subsets of a set $\Omega$ . Let $P$ be a probability measure on $\mathscr{F}_0$ . Consider increasing sequences of sets where the limiting sets may not belong to $\mathscr{F}_0$ , but establish that if the two limiting sets $A\subset A'$ , then $\lim_{m\rightarrow\infty}P(A_m)\leq\lim_{n\rightarrow\infty}P(A'_n)$ . If both sequences increase to the same limit, it is equality. The authors produce a larger set, $\mathscr{G}$ , which is the collection of all limits of increasing sequences of sets in $\mathscr{F}_0$ , essentially the collection of all countable unions of sets in $\mathscr{F}_0$ . Because in (1), we established how the probability measure $P$ behaves in the limit of increasing sequences, this first extension of $P$ to $\mathscr{G}$ is natural. This extended measure is denoted as $\mu$ on $\mathscr{G}$ . Now, the authors extend $\mu$ to the class of all subsets of $\Omega$ , which also seems natrual, but the authors comment that the extension will NOT be countably additive on all subsets, but only on a smaller $\sigma$ -field. Then, they introduce the outer measure definition as. $\mu^*(A)=\inf\{\mu(G):G\in\mathscr{G},A\subset G\}.$ My Question: Why do we introduce this $\mu^*$ in step 3 of the extension of measures? Also, is this $\mu^*$ pictorially putting smaller boxes (i.e. bunch of subsets of $\Omega$ over $A$ ) to cover $A$ ? Reference: $\textit{Probability and Measure Theory}$ (Robert B. Ash and Catherine A. Doleans-Dade), Harcourt/Academic Press, 1999.",['measure-theory']
3323500,quasi-Frobenius iff Frobenius iff $\operatorname{ann}(m)\cong R/m$. its relation to algebraic geometry,"This is related to Weibel, Homological Algebra, Chpt 4, Sec 2, Vista 4.2.7. Assume $R$ commutative. If $R$ is a $0-$ dimensional local ring with maximal ideal $m$ , then $R$ is quasi-Frobenius iff $\operatorname{ann}(m)\cong R/m$ . $\textbf{Q:}$ How do I prove above assertion? My first thought is to use show $R$ is a Frobenius $R/m=k$ algebra.(i.e. $R\cong \operatorname{Hom}_k(R,k)$ ) If this is the case, then it follows that $R$ is self injective by hom-tensor adjunction with hom preserving injective objects. Since $R$ is local 0-dimensional, it follows that $R$ is artinian which makes filtration $0=m^{n+1}\subset m^n\subset\dots m\subset R$ finite length and it gives the splitting of $R\cong k\oplus_{i\leq n}\frac{m^i}{m^{i+1}}$ as a $k$ -vector space. However, I have to note that first fact $k$ comes from identification of $k=R/m$ which contains $1$ . Thus, I could not realize first factor $k$ as $\operatorname{Ann}(m)$ . I do not know whether I am even on the right track. $\textbf{Q':}$ The book correction part indicates if $R$ is finite dimensional over field $k$ , then $R$ is a Frobenius algebra over $k$ iff $R$ is quasi-Frobenius. It is clear that if $R$ is a Frobenius algebra over $k$ , then $R$ is self injective which implies $R$ is quasi-Frobenius. How do I get reversed implication? $\textbf{Q'':}$ The book says $R$ is Gorenstein if $\operatorname{id}(R)$ injective dimension is finite. Since in the commutative case, I have $\operatorname{id}(R)$ identified with Krull dimension. So $R$ s has finite krull dimension which means $R$ may not be $0-$ dimensional. ""If $R$ is a $0$ -dimensional local ring with maximal ideal $m$ , then $R$ is quasi-Frobenius iff $\operatorname{ann}_R(m)\cong R/m$ . This recognition criterion is at the heart of current research into the Gorenstein rings that arise in algebraic geometry."" Why this criterion is at the heart of Gorenstein rings arising in algebraic geometry?","['homological-algebra', 'algebraic-geometry', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3323520,"Understanding a complex number proof that there is a convex equiangular $1990$-gon with sides $1^2$, $2^2$, $\ldots$, $1990^2$","IMO 1990, Problem B3 reads: Prove that there exists a convex $1990$ -gon such that all its angles are equal and the lengths of the sides are the numbers $1^2$ , $2^2$ , $\ldots$ , $1990^2$ in some order. I have an issue understanding the solution by Robin Chapman , reproduced here: ""In the complex plane we can represent the sides as $p_n^2w^n$ , where $p_n$ is a permutation of $(1, 2, \ldots , 1990)$ and $w$ is a primitive $1990$ th root of unity. The critical point is that $1990$ is a product of more than $2$ distinct primes: $1990 = 2\cdot 5\cdot 199$ . So we can write $w = -1\cdot a\cdot b$ , where $-1$ is primitive $2$ nd root of unity, $a$ is a primitive $5$ th root of unity, and $b$ is a primitive $199$ th root of unity. Now given one of the $1990$ th roots we may write it as $(-1)^ia^jb^k$ , where $0 < i < 2$ , $0 < j < 5$ , $0 < k < 199$ and hence associate it with the integer $r(i,j,k) = 1 + 995i + 199j + k$ . This is a bijection onto $(1, 2, \ldots , 1990)$ . We have to show that the sum of $r(i,j,k)^2 (-1)^ia^jb^k$ is zero. We sum first over $i$ . This gives $-9952 \times \text{sum of $a^jb^k$}$ which is zero, and $-1990 \times \text{sum $s(j,k) a^jb^k$}$ , where $s(j,k) = 1 + 199j + k$ . So it is sufficient to show that the sum of $s(j,k) a^jb^k$ is zero. We now sum over $j$ . The $1 + k$ part of $s(j,k)$ immediately gives zero. The $199j$ part gives a constant times $b^k$ , which gives zero when summed over $k$ ."" I did not grasp the details of the solution well, especially how the summations are parted and computed. I would thank any body elucidating the solution in detail.","['contest-math', 'geometry', 'complex-numbers']"
3323630,How to understand this derivation - derivative of double dot product,"I am looking at a derivation from a continuum mechanics book and I am not sure I understand how the author goes from step 2 to step 3.  The author goes like this: $\frac{\partial tr[\bar{\textbf{C}}]}{\partial \textbf{C}}$ $\frac{\partial [I_3^{-1/3}\textbf{C}:\textbf{I}]}{\partial \textbf{C}}$ $I_3^{-1/3}\textbf{I}-\frac{1}{3}I_3^{-4/3}I_3\textbf{C}^{-1}(\textbf{C}:\textbf{I})$ I would think it would be easier to go from the second step to: 3'. $\frac{\partial [I_3^{-1/3}I_1]}{\partial \textbf{C}}$ Because $\textbf{C}:\textbf{I} = tr[\textbf{C}] = I_1$ Is this a valid move? If I do that I end up at the same final answer as the author, but I want to make sure it isn't blind luck.  If we follow the author's approach, it looks like one needs to apply the product rule to the three terms that are each a function of $\textbf{C}$ but I am not sure how this works out.  For example, applying the product rule to the second step gives: 2'. $\frac{\partial I_3^{-1/3}}{\partial \textbf{C}}\textbf{C}:\textbf{I}+I_3^{-1/3}\frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I}+I_3^{-1/3}\textbf{C}:\frac{\partial \textbf{I}}{\partial \textbf{C}}$ The third term will be zero because the identity matrix is independent of $\textbf{C}$ .  But does the author's step 3 imply that $\frac{\partial \textbf{C}}{\partial \textbf{C}}:\textbf{I} = \textbf{I}$ ?  I thought that $\frac{\partial \textbf{C}}{\partial \textbf{C}}$ was the 4th order identity? The final expression is: $I_3^{-1/3}(\textbf{I}-\frac{1}{3}I_1\textbf{C}^{-1})$ So in summary, my questions are: Is my method (3') valid? 
Do I understand correctly what the author's work implies about the inner product of a 4th order identity with a 2nd order identity being a second order identity? Thanks","['tensors', 'derivatives']"
3323674,Solve the initial value problem $xyy' + xy' = 1$ and $y(1) = 0$,"I can solve the differential equation, which is $y + y^{2}/2 = \ln(x) + C$ . But I cannot solve the IVP because I can't isolate for $y$ and find the value of $C.$","['initial-value-problems', 'ordinary-differential-equations']"
3323680,How to isolate $x$ when $\cos(x/2)\cos(3x)/2−3\sin(x/2)\sin(3x)= 0$,"Asked to find all relative and absolute extrema of $f(x) = \sin\left(\frac{1}{2}x\right)\cos(3x)$ on the interval $[0,\pi]$ . I've gotten the derivative, $$f'(x)=\frac{\cos(\frac{x}{2})\cos(3x)}{2} - 3\sin(\frac{x}{2})\sin(3x),$$ just fine, but how would I proceed to isolate x after this? I noticed that the situation is similar to the sine angle sum identity, but I don't know if there's an identity for an equation with the form $$h\cos(\alpha)\cos(\beta) - k\sin(\alpha)\sin(\beta)$$ and can't find anything online. I don't have much experience working with Euler's Formula in this context but know enough that I could probably follow an explanation based on it.","['calculus', 'trigonometry']"
3323698,Question on isogeny of abelian varieties and principal homogeneous spaces,"I know that for an abelian variety $A$ over an algebraically closed field $k=\overline k$ and an ample line bundle $L$ on $A$ , one can define an isogeny $\phi_L \colon A \rightarrow \mathrm{Pic}^0(A)$ by $x \mapsto \tau_x^* L \otimes L^{-1}$ , where $\tau_x$ denotes the translation by $x$ . First, I assume that this goes through verbatim over a non algebraically closed field, assuming that $L$ is defined over $k$ . Is it right, or am I missing something? Second, does the above construction work if $A$ is just a principal homogeneous space over an abelian variety? That is, $k$ is not algebraically closed, $A$ has no $k$ -rational points, but $A(\overline{k})$ is an abelian variety.
In this situation, do I still get a finite dominant morphism $\phi \colon A \rightarrow \mathrm{Pic}^0(A)$ ? My guess is that the same construction should still work, again provided that $L$ is a polarization defined over $k$ . As I am not very comfortable with non-closed fields, I would really appreciate some feedback on what I sketched above.","['algebraic-geometry', 'proof-verification', 'abelian-varieties', 'group-schemes']"
3323702,"prove by induction: $\forall n \in N, \exists k \in N: 165^{2n} - 1 = 166k$","I am trying to prove by induction: $\forall n \in N, \exists k \in N: 165^{2n} - 1 = 166k$ . But I've never come across having to possibly induct on two variables? The base case for $n=1$ is true for $k = 164$ With the inductive hypothesis: assume $165^{k} - 1 = 166k$ we will prove that $165^{2k+2} - 1 = 166k$ . Where can I start on the inductive step?","['proof-explanation', 'proof-writing', 'proof-verification', 'discrete-mathematics']"
3323722,Constructions | Olympiad Geometry,"Let $ABCD$ be a parallelogram with no angle equal to $60^\circ$ . Find all pairs of points $(E,F)$ such that $AE = BE$ , $BF = CF$ , and triangle $DEF$ is equilateral. So far after making a couple of diagrams, I've figured out that $DEF$ is equilateral only when $ABE$ and $BFC$ are equilateral as well. But I don't know how to prove it. We could use similarity or rotation somewhere here, but I don't see how. We know that the locus of points E and F lie on the perpendicular bisectors of AB and BC respectively.","['contest-math', 'euclidean-geometry', 'vectors', 'geometric-construction', 'geometry']"
3323732,Find B such that $\det(B)>0$ but there is no real $A$ with $\exp{(A)}=B$,I have proved that $\exp(\text{tr}(A)) = \det (\exp (A))$ . An easy corollary is that $\exp (A)$ has positive determinant whenever $A$ is real. Now the question asks us to find a B such that $\det(B)>0$ but there is no real $A$ with $\exp{(A)}=B$ . I have no idea how to proceed so it would be greatly appreciated if someone could drop me a hint.,"['matrices', 'matrix-exponential', 'linear-algebra']"
3323743,Derangement formula for repeated permutation,"I need general formula for repeated permutation: For any set of $n$ numbers $\{1,2,3,\ldots,n\}$ , the formula for its number of derangements is given by the recursion $$!n=(n-1)(!(n-1)+!(n-2)).$$ Here, the numbers are distinct from one another (no number is repeated in the set). Is there any general formula for the number of derangements when the numbers are repeated? For example, for a multiset like $ \{1,1,2,2,3,3,4,5\} $ . Thanks in advance.","['permutations', 'derangements', 'combinatorics']"
3323753,How to get the derivative of an average?,"I was curious about how to derive the derivative of an average. More specifically: $$\mu = \frac{1}{m}\sum_{i = 1}^m x_i$$ $$\frac{\partial \mu}{\partial x_i} =\ ?$$ My derivation is as follows: $$
\begin{align}
\mu & = \frac{1}{m}(x_1 + x_2 + \cdots + x_m) \\
\partial \mu / \partial x_i& = \frac{1}{m}(1 + 1+\dots + 1) \\
& = 1
\end{align}
$$ but I'm not sure if this is correct...",['derivatives']
3323761,Is a (finite) group determined by its subgroups?,"Motivation I think of the ""structure"" of a topological space $X$ as being the limit operator on functions $I\to X$ where $I$ could be the natural numbers or another topological space -- in this sense, a topological homomorphism (continuous function) $f$ is a function that commutes with the limit operation $f(\lim x)=\lim f(x)$ , similar to how a group homomorphism commutes with group multiplication $f(\mathrm{mult}(x,y))=\mathrm{mult}(f(x),f(y))$ and a linear transformation commutes with linear combination. Nonetheless, it can be shown that this structure can be determined uniquely by the set of open sets on $X$ . One may also understand these open sets to be the ""sub-(topological spaces)"" of $X$ as the topology of $X$ is inherited by them exactly (well, the closed sets are also a ""dual"" kind of sub-topological spaces). Similarly, given a set $V$ and a list of subsets that we call ""subspaces"" (which would have to satisfy some properties), one can determine the vector space up to isomorphism (i.e. we can find its dimension). I wonder if something like this can be done with groups. Given a set $G$ and a list of subsets we call its ""subgroups"", can we determine the group up to isomorphism? At least for finite sets? Example given the set $\{0, 1, 2, 3\}$ , we'd be given the following ""subgroup structure"" on it: $\{\{0\},\{0,2\},\{0,1,2,3\}\}$ , and the group being described is $C_4$ . The positions of 1 and 3 aren't determined, but the group is still determined to isomorphism.","['group-theory', 'finite-groups']"
3323783,Integration of $ \int x^{2} \sqrt{2x-6} dx $,"$$ \int x^{2} \sqrt{2x-6} dx = ?$$ My Attempt: by partial integration $$ \int x^{2} \sqrt{2x-6} dx = \frac{x^{2} (2x-6)^{3/2}}{3}- \frac{2}{3} \int x(2x-6)^{3/2}dx$$ continuing partial integration $$ = \frac{x^{2} (2x-6)^{3/2}}{3}- \frac{2}{3} \left[ \frac{x(2x-6)^{5/2}}{5} - \int \frac{(2x-6)^{5/2}}{5} dx\right]  $$ $$ = (x^{2}/3)(2x-6)^{3/2} - (2x/15)(2x-6)^{5/2} + (2/105)(2x-6)^{7/2} + C$$ Is this the correct and best/simplest answer? Strangely, the multiple choices only include answers in the form: $$ A(2x+6)^{7/2} + B(2x+6)^{5/2} + C(2x+6)^{3/2} + D$$ where $A,B,C,D$ are constants.","['integration', 'calculus', 'algebra-precalculus']"
3323879,The differential of $\lVert D^{1-\lambda} U^* D^{2\lambda}\ UD^{1-\lambda} \rVert_F^2$ with respect to $\lambda$,"Let a square matrix $A=WDV^*$ by the SVD where $D$ is diagonal with positive entries, $U=V^*W$ is unitary, and $0<\lambda<1$ .  let $$ f_{(\lambda)} = \lVert D^{1-\lambda} U^* D^{2\lambda}\  UD^{1-\lambda} \rVert_F^2 = \operatorname{tr}\Big(D^{2-2\lambda} U^* D^{2\lambda}\ U \ D^{2-2\lambda} U^* D^{2\lambda}\ U\Big) $$ Please help with the differential $\frac{\partial}{\partial\lambda} f_{(\lambda)}$ . I'm thinking in the Frobenius product direction, but I'm not able to get it done. Thanks in advance. Post-edit: Thanks to @greg for pointing my attention to $\operatorname{tr}(M^*M)\ne\operatorname{tr}(M^2)$ since $M=D^{2-2\lambda} U^* D^{2\lambda}\ U$ is not normal. Again, the Frobenius norm is a real-valued function. Therefore, to ensure real-valued trace, will the following be correct? (I implemented some test cases in MATLAB to check the equality) . $$ \begin{align} f_{(\lambda)} & \ne \operatorname{tr}\Big((D^{2-2\lambda} U^* D^{2\lambda}\ U)^2\Big) \\
&=\operatorname{tr}\Big((D^{2-2\lambda} U^* D^{2\lambda}\ U)\ (D^{2-2\lambda} U^* D^{2\lambda}\ U)^*\Big) \\
&=\operatorname{tr}\big(D^{4-4\lambda} U^* D^{4\lambda}\ U\big) \end{align} $$ If it's the right way to state it, please help with the differential $\frac{\partial}{\partial\lambda} f_{(\lambda)}$ with respect to this $f_{(\lambda)}$ version.","['frobenius-method', 'trace', 'matrices', 'linear-algebra', 'derivatives']"
3323947,Is the set of all invertible 2x2 matrices a subspace of all 2x2 matrices?,"Is the set of all invertible 2 x 2 matrices a subspace of all 2 x 2 matrices? If not, can someone give me a counterexample to disprove this statement.","['matrices', 'linear-algebra']"
3323951,Show that the number of elements of $X$ belonging to a least $r$ equals to $\sum_{k=r}^n(-1)^{k-r}{k-1\choose r-1}S_k$,"Show that the number of elements of $X$ belonging to a least $r$ of the sets $A_1,\ldots,A_n\subset X$ is $$\sum_{k=r}^n(-1)^{k-r}{k-1\choose r-1}S_k.$$ $S_k$ is defined here as: $$ \sum_{1 \le i_1 < \cdots< i_k \le n} |A_{i_1} \cap ... \cap A_{i_k}|$$ My try. From link we know that number of elements of $X$ belonging to $r$ sets is $$ L(k) = \sum_{i=k}^n (-1)^{i-k} \binom{i}{k}S_i $$ So number of elements of $X$ belonging to a least $r$ of the sets is equal to $$L(k) + L(k+1) + L(k+2) + \cdots + L(n) $$ So let do this: \begin{align}  & \binom{r}{r}S_r - \binom{r+1}{r}S_r + \color{red}{\binom{r+2}{r}}S_r -... \pm \binom{n}{r}S_n + \\  & 0 + \binom{r+1}{r+1}S_r - \color{red}{\binom{r+2}{r+1}}S_r +... \mp \binom{n}{r+1}S_n +\\ 
& 0 + 0 + \color{red}{\binom{r+2}{r+2}}S_r -... \mp \binom{n}{r+2}S_n + 
\\&\vdots\\\\ &   0+0+\color{red}{0}+0+0+0+0+0+\cdots \pm S_n \end{align} I think that summing by cols can give me proof. But I have some troubles with proof that: $$\sum _{k=0}^t (-1)^k \binom{r+t}{k+r} = \binom{r+t-1}{r-1}.$$","['summation', 'combinatorics', 'discrete-mathematics', 'inclusion-exclusion']"
3323972,"Einstein field equation,pde and differential geometry","I'm a math undergraduate student with some interest in mathematical physics with basic knowledge of partial differential equation. When I was reading a wikipedia article about einstein field equation,it said when fully written out, the EFE are a system of ten coupled, nonlinear, hyperbolic-elliptic partial differential equations. my question is if einstein equation is a partial differential equation, why can't you solve it normally,why do you need tensor analysis/riemannian geometry for, and can any partial differential equation be written using the languange of tensor, differential geometry,etc? I apologize for my minimal understanding of this subject, but I haven't learn any tensor calculus yet","['partial-differential-equations', 'general-relativity', 'mathematical-physics', 'differential-geometry']"
