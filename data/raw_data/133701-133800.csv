question_id,title,body,tags
2100660,Determine the number of subsets without using tree diagram,"Q. Determine the number of subsets
of {3, 7, 9, 11, 24} with the property that the sum of the
elements in the subset is less than 28. My approach: First I calculated the total number of subsets that is $2^\left | S \right |$ , which in this case is $2^5$=32. Then from the total number of subsets I subtract those subsets which give sum $\geq $ 28 , to get the subset whose sum $\geq $ 28 , I include 24 in each of the subsets $\left \{24, \texttt{_,_,_,_ }  \right \}$  so by filling the rest of the remaining four places will give $2^4$ subsets, these $2^4$ subsets has sum $\geq $ 28 , so we subtract these subsets from the total number of subsets to get the subsets that has sum $\leq $ 28 that is $2^5$- $2^4= 16$ . But in the $2^4$ subsets that we have calculated includes a subset $\left \{24, \texttt{3}  \right \}$ whose sum $\leq $ 28 , so we subtract one from the $2^4$ subsets, which is $2^4-1=15$ is the number of subsets whose sum $\geq $ 28 So we get $2^5-(2^4-1)=17 $       as the answer which is the total number of subsets whose sum $\leq $ 28 Is this approach correct to solve these type of questions?","['combinatorics', 'discrete-mathematics']"
2100677,differentiation under the integral sign - moments,"I want to find $$\mu_r' = \int_0^\infty y^r\theta e^{-\theta y}dy $$ A day ago I read something here on MSE about differentiation under the integral sign. I am not sure of how it works, however I tried to differentiate wrt $r$ for $r$ times, but I just get a huge expression. Considering $\theta e^{-\theta y}$ a constant wrt $r$ I get first $ry^{r-1}$ then $y^{r-1}+r(r-1)y^{r-2}$ then $(r-1)y^{r-2}+(r-1)y^{r-2}+(ry^{r-2}+r(r-1)(r-2)y^{r-3})$ and so on. I can see there must be a pattern, but I can't find it. How can I find the general formula for $\mu_r'$ ? Edit I am pretty sure that the solution is something of the form $$\frac{r!}{\theta^r}$$ as this is what you get when doing each case separately Edit 2 As seen in the answers below my derivative is wrong, still I have no clue on how to solve the problem","['derivatives', 'integration', 'moment-generating-functions']"
2100699,Alternate topological definition of continuity,"The standard topological definition of continuity is as follows: Definition: Continuity Let $(X, \mathcal{T}_X)$ and $(Y, \mathcal{T}_Y)$ be topological spaces. A function $f : X \to Y$ is said to be continuous if for each open subset $V$ of $Y$ , the set $f^{-1}(V)$ is an open subset of $X$ . In other words $f$ is continuous if for each $V \in \mathcal{T}_Y$ , we have $f^{-1}(V) \in \mathcal{T}_X$ . But can we alternatively define continuity in the following way: Possible Alternate Definition? : $f$ is continuous if for each $W \in \mathcal{T}_X$ , we have $f(W) \in \mathcal{T}_Y$ But if we take $W = f^{-1}(V) \in \mathcal{T}_X$ , then $f(W) = f(f^{-1}(V)) \subset V \in \mathcal{T}_Y$ , but I don't think that $f(f^{-1}(V))$ needs to be open in $Y$ (as $f(f^{-1}(V)) \subset V$ ) , which leads me to believe that we cannot define continuity in the proposed way above. Am I correct, or can we define continuity in the following way? If we cannot define continuity in the following way, are there any other (perhaps more intuitive) reasons why we cannot define continuity in arbitrary topological spaces in the proposed way above.","['intuition', 'continuity', 'definition', 'general-topology', 'open-map']"
2100704,Regarding the dual cone in a normed space,"$X$ is a normed space and $C\subset X$ a convex and closed cone ( $\lambda C\subset C\ \forall \lambda\geq0$ ), and $C':=\{x'\in X':x'(x)\geq0\ \forall x\in C\}$ the dual cone of $C$ . I want to show: $(i):\quad C\neq X\Rightarrow C'\neq\{0\}$ $(ii):\quad x'(x)\geq0\ \forall x'\in C'\Rightarrow x\in C$ Because in class we talked about separation theorems, my approach so far was: Let $C\neq X$ then $\exists x_0\notin C$ . Then $\{x_0\}$ is closed and convex, the Hahn-Banach-Separation-Theorem provides $x_0'$ such that $x_0'(x_0)<\inf_{x\in C}x_0'(x)\leq0$ . This is were I'm stuck...","['functional-analysis', 'normed-spaces', 'separation-axioms']"
2100722,"Prob. 5, Chap. 4 in Baby Rudin: Continuous extension of a function defined on a closed set","Here is Prob. 5, Chap. 4 in the book Principles of Mathematical Analysis by Walter Rudin, 3rd edition: If $f$ is a real continuous function defined on a closed set $E \subset \mathbb{R}^1$ , prove that there exist real continuous functions $g$ on $\mathbb{R}^1$ such that $g(x) = f(x)$ for all $x \in E$ . (Such functions $g$ are called continuous extensions of $f$ from $E$ to $\mathbb{R}^1$ .) Show that the result becomes false if the word ""closed"" is omitted. Extend the result to vector-valued functions. Hint: Let the graph of $g$ be a straight line on each of the segments which constitute the complement of $E$ (compare Exercise 29, Chap. 2). The result remains true if $\mathbb{R}^1$ is replaced by any metric space, but the proof is not so simple. First of all, we show that the word ""closed"" is essential. Let $E = (-\infty, 0) \cup (0, +\infty)$ , and let $f \colon  E \to \mathbb{R}^1$ be defined as $$ f(x) = \frac{1}{x} \ \mbox{ for all } x \in E.$$ Then the set $E$ is not closed in $\mathbb{R}^1$ , but there is no continuous function $g \colon \mathbb{R}^1 \to \mathbb{R}^1$ such that $g(x) = f(x)$ for all $x \in E$ , although $f$ is certainly continuous. Am I right? Now we show the main result: First of all, here is Exercise 29, Chap. 2 in Baby Rudin, 3rd edition: Prove that every open set in $\mathbb{R}^1$ is the union of an at most countable collection of disjoint segments. ... Since $\mathbb{R}^1 - E$ is open in $\mathbb{R}^1$ , it is the union of an at most countable collection of disjoint segments $\left\{ \left(a_n, b_n \right) \right\}_{n \in K}$ , where $K$ is either the set $J = \left\{ 1, 2, 3, \ldots \right\}$ or $K$ is some $J_N = \left\{ 1, \ldots, N \right\}$ for some $N \in \mathbb{N}$ ; moreover, $\left( a_m, b_m \right) \cap \left( a_n, b_n \right) = \emptyset$ for any two distinct $m, n \in K$ ; finally some $\left( a_n, b_n \right)$ can possibly be infinite. Now since $$\mathbb{R}^1 - E = \cup_{n \in K} \left( a_n, b_n \right),$$ each of the (finite) endpoints of these segments is an element of set $E$ . Now if some segment $\left( a_n, b_n \right)$ has $-\infty$ as its left endpoint, then we put $g(x) = f \left( b_n \right)$ for all $x \in \left( a_n, b_n \right)$ . If some segment $\left(a_m, b_m \right)$ has $+\infty$ as its right endpoint, then we put $g(x) = f\left( a_m \right)$ for all $x \in \left( a_m, b_m \right)$ . And, for any segment $\left( a_n, b_n \right)$ , where $-\infty < a_n < b_n < +\infty$ , we put $$ g(x) = f\left( a_n \right) + \left[ \frac{ f\left( b_n \right) - f\left( a_n \right)}{b_n - a_n} \right] \left( x - a_n \right) $$ for all $x \in \left( a_n, b_n \right)$ . I hope I've used the hint given by Rudin correctly. If so, then how to rigorously show that the resulting real function $g$ defined on $\mathbb{R}^1$ is continuous on all of $\mathbb{R}^1$ ? This is intuitively clear though, but how does the countability of these segments become significant? Up to this point, Rudin hasn't stated any result about the continuity of a function defined using several continuous functions as pieces, like our function $g$ is defined here. Moreover, are there any other possibilities for $g$ besides the one we have defined above? Now we state the generalization of the above result for vector-valued functions. Let $E$ be a closed set in $\mathbb{R}^1$ , and let $\mathbf{f}$ be a continuous function defined on $E$ with values in some $\mathbb{R}^k$ . Then there are continuous functions $\mathbf{g}$ defined on $\mathbb{R}^1$ with values in the same $\mathbb{R}^k$ such that $\mathbf{g}(x) = \mathbb{f}(x)$ for all $x \in E$ . In order to prove this generalized result, let's put $\mathbb{f}(x) = \left( f_1(x), \ldots, f_k(x) \right)$ for all $x \in E$ , where $f_1, \ldots, f_k$ are real continuous functions defined on $E$ , and each of these functions we can continuously extend to all of $\mathbb{R}^1$ , thereby getting a continuous extension $\mathbf{g}$ of $\mathbf{f}$ from $E$ to all of $\mathbb{R}^1$ . Is this reasoning correct? Last but not the least, I have the following query. Let $\left( X, d_X \right)$ and $\left( Y, d_Y \right)$ be metric spaces, let $E \subset X$ such that $E$ is closed in $\left( X, d_X \right)$ , and let $f$ be a continuous mapping of the induced metric space $E$ into the metric space $Y$ . Then can we find---or prove the existence of---a continuous mapping $g$ of $X$ into $Y$ such that $g(x) = f(x)$ for all $x \in E$ ? If so, then how to find such a map or prove it exists? If not, then what is the general result that Rudin is referring to and how to come up with the not-so-simple proof that he has alluded to?","['real-analysis', 'calculus', 'continuity', 'metric-spaces', 'analysis']"
2100726,Statistics Homework Question (Binomial Distribution),"Problem: In a binomial experiment with 45 trials, the probability of more than 25 successes can be approximated by $P(z > \frac{(25-27)}{3.29}$) What is the probability of success of a single trial of this experiment? Choices: 0.07 0.56 0.79 0.61 0.6 My Solution: Since the experiment can be approximated by a normal distribution, I evaluated P(z) using NormalCdf on my calculator. $$P(z > \frac{25-27}{3.29}) = NormCdf(\frac{25-27}{3.29},\infty, 0, 1) = 0.72834 $$ Then, I set up the binomialCdf expression as $binomCdf(45, p, 26, 45)$ using 26 as the minimum number of successes as the question specifies it to be more than 25. Through inspection, I found p to be 0.61 . However, the answer key says otherwise. Can someone check the error in my work? Thank you!",['statistics']
2100775,Mathematica gives: $\int_{0}^{\infty}{\cos(x^n)-\cos(x^{2n})\over x}\cdot{\ln{x}}\mathrm dx={12\gamma^2-\pi^2\over 2(4n)^2}$,"How do we show that the given result by Mathematica is correct? $$\int_{0}^{\infty}{\cos(x^n)-\cos(x^{2n})\over x}\cdot{\ln{x}}\mathrm dx={12\gamma^2-\pi^2\over 2(4n)^2}\tag1$$
  $n>0$ Where $\gamma=0.577216...$ I would try substitution, because it may help to simplify the problem into a manage integral to deal with. $u=x^n$ $du=nx^{n-1}dx.$ $${1\over n}\int_{0}^{\infty}{\cos(u)-\cos(u^2)\over u^{1\over n}}\cdot{\ln{u^{1\over n}}}{\mathrm dx\over u^{n-1\over n}}={12\gamma^2-\pi^2\over 2(4n)^2}$$ Simplified to $${1\over n^2}\int_{0}^{\infty}{\cos(u)-\cos(u^2)\over u}\cdot{\ln{u}}\mathrm du={12\gamma^2-\pi^2\over 2(4n)^2}$$ We can remove $\ln{u}$ by doing another substitution $v=\ln{u}$ $udv=du$ $${1\over n^2}\int_{-\infty}^{\infty}{\cos(e^v)-\cos(e^{2v})\over e^v}\cdot{v}\cdot{e^v}\mathrm du={12\gamma^2-\pi^2\over 2(4n)^2}$$ Then we finally simplified to $$={1\over n^2}\int_{-\infty}^{\infty}v\cos(e^v)\mathrm dv -{1\over n^2}\int_{-\infty}^{\infty}v\cos(e^{2v})\mathrm dv$$ At this stage I would apply integration by parts but it seems to show a problem for me to do. So I need some help. Thank you.","['calculus', 'euler-mascheroni-constant', 'improper-integrals', 'integration', 'definite-integrals']"
2100793,How to access an element of a set?,"Let us say that I have a set $A=\{1,2, 3\}$. Now I need to access, say, the element $3$ of $A$. How do I achieve this? I know that sets are unordered list of elements but I need to access the elements of a set. Can I achieve this with a tuple? Like $A=(1, 2, 3)$, should I write $A(i)$ to access the i-th element of $A$? Or is there any other notation? If I have a list of elements, what is the best mathematical object to represent it so that I can freely access its elements and how? In programming, I would use arrays .","['notation', 'elementary-set-theory']"
2100807,How to prove that $f(x)=|x|$ is not surjective?,"Let $f:\mathbb{R}\to\mathbb{R}$ be defined as $f(x)=|x|$. How to prove that $f$ is not surjective? I need some help with this exercise. Suppose $f$ is surjective.
Be $a\in\mathbb{R}$ , $\exists\;b\in\mathbb{R} $ such that $f(b)=a$, well. i consider two cases. Case 1: $b>0$, then $f(b)=b=a$ Case 2: $b<0$, then $f(b)=-b=a\Rightarrow b=-a$ Okay, in this case i have a problem because i didnt see the contradiction, can someone help me?","['algebra-precalculus', 'functions']"
2100828,How to express $\sqrt{2-\sqrt{2}}$ in terms of the basis for $\mathbb{Q}(\sqrt{2+\sqrt{2}})$,"I've already shown that $\sqrt{2-\sqrt{2}}\in\mathbb{Q}(\sqrt{2+\sqrt{2}})$ but I want to write $\sqrt{2-\sqrt{2}}=a+b\alpha+c\alpha^2+d\alpha^3$ where $\alpha=\sqrt{2+\sqrt{2}}$. By squaring $\sqrt{2-\sqrt{2}}$ I get a system of four equations with four variables, but everything I try (Sage, for instance) takes too long to solve it. Is there a simpler way to write it in terms of the basis?","['abstract-algebra', 'galois-theory', 'field-theory', 'linear-algebra']"
2100861,The direct sum $\oplus$ versus the cartesian product $\times$,"In the case of abelian groups, I have been treating these two set operations as more or less indistinguishable. In early mathematics courses, one normally defines $A^n := A\times A\times\ldots\times A$; however in, for example, the fundamental theorem of finitely generated abelian groups, we normally write that every such group is isomorphic to one of the form
$$
\mathbb{Z}^n \oplus \mathbb{Z}_{r_1} \oplus \cdots \oplus \mathbb{Z}_{r_t} 
$$
where $\mathbb{Z}^n$ now means $\mathbb{Z}\oplus\mathbb{Z}\oplus\cdots\oplus\mathbb{Z}$. From an intuition perspective, and in the sense of sets, is this more or less the same as $\mathbb{Z}\times\mathbb{Z}\times\cdots\times\mathbb{Z}$? (Bear in mind I am normally using these ideas in relation to homology groups.)","['abelian-groups', 'group-theory']"
2100878,Does the converse of the chain rule hold in general?,"I've found the following theorem for single variable real valued functions: The converse of the chain rule: Suppose that $f,g$ and $u$ are related so that $f(x)=g(u(x))$. If $u$ is continuous at $x_0$, $f'(x_0)$ exists and $g'(u(x_0))$ exists and is non-zero; then $u'(x_0)$ is defined and we have: $$f'(x_0)=g'(u(x_0))u'(x_0)$$ I'm interested in knowing if this ""other converse"" also holds: Suppose that $f,g$ and $u$ are related so that $f(x)=g(u(x))$. If $g$ is continuous at $u(x_0)$, $f'(x_0)$ exists and $u'(x_0)$ exists and is non-zero; then $g'(u(x_0))$ is defined and we have: $$f'(x_0)=g'(u(x_0))u'(x_0)$$ If anyone is interested here is the proof of the first one (page 12), I tryed to proved the second one in an analogous way but I didn't succeed. For my immediate purposes I would be happy with knowing it holds, and in case it doesn't some counterexample could be instructive. I'm also interested in knowing if those statements generalises in the obvious way to functions from $\mathbb{R}^n$ to $\mathbb{R}^m$. Thanks in advance","['multivariable-calculus', 'chain-rule', 'calculus']"
2100884,Why is the second derivative of this function a straight line?,"This might be an unusual question but I was wondering why the 2nd derivative of this function is a straight line? I kind of have the feeling this is not that easy to answer. But it kind of struck me that it is exactly linear. Here's a picture: I mean, yes mathematically you can say that it just is as it is, but is there also an intuitive answer to it? Thank you for answering! Regards!","['derivatives', 'polynomials', 'curves']"
2100919,Find nearest whole integer(s) on function,"I have two functions as follows: $$a=\frac{5}{16}(x+2y)$$
$$b=-\frac{5}{16}(x-2y)$$ The user may enter any arbitrary number for (x, y) . However only values which produce whole integers of a and b are considered valid. Given these conditions, is it possible to find the nearest valid (x, y) to the user's inputted x and y which would produce whole numbers for a and b . I thought representing both equations as planes and finding the closest point on them to a specified point using Lagrange multiplication. But wasn't sure exactly how to do so, to produce the results I was looking for. Edit: Follow-up question found here","['integers', 'functions']"
2100930,Is there a standard notation to define a tuple?,"So I have a matrix $A=[a_{ij}]_{\forall\,i,j\in\{1,\dots,n\}}$. I have no problem to define a set $S$ as $S:=\{j: j=\text{argmax}_{j'}\; a_{ij'},\forall i\in\{1,\dots,n\} \}$ for example. In fact, I have seen sets defined this way many times. Now my problem is with tuple. Is it correct to use the same for tuple. So, can I say ""let $A$ be the tuple $A:=(j: j=\text{argmax}_{j'}\; a_{ij'},\forall i\in\{1,\dots,n\} )$."" or this makes no sense? EDIT In fact, I have a bad time finding some mathematical object that is like array in programming language. So I can access elements of that object , find its size, add elements, remove elements, etc. Is there any mathematical object like this?","['notation', 'elementary-set-theory', 'definition']"
2100943,Uniform Boundedness principle for bounded linear maps from Frechet Space into a Banach Space,"I am looking for a proof of the uniform boundedness principle where the domain is a Frechet space, instead of the usual setting of a Banach Space. This is used in proving the space of tempered distributions is complete but I can't find a proof of it anywhere. When I try to prove it myself I get stuck on the final part(which uses the scaling property of linear maps). Does anyone have a proof that they could share?","['functional-analysis', 'real-analysis', 'distribution-theory']"
2100966,Find closest whole integer in equation,"Given the following equations: $$a=\frac{py+qx}{2pq}$$
$$b=\frac{py-qx}{2pq}$$ Where p and q are some real constant number. And $(x, y)$ are some arbitrary real number. Any number can be inputted as $(x, y)$ but only those which produce whole integers for $a$ and $b$ respectively, are considered valid. Given these conditions, how can I find the valid $(x_1, y_1)$ values whose euclidean distance to the inputted $(x, y)$ is minimal? E.g. for clarification. Given $p=1.6$ and $q=0.8$. The user inputs $(x,y) = (0, 1.5)$. This produces $(a,b)=(0.9375,0.9375)$ as this is not an integer value it is not considered valid. The next closest input which would produce an integer would be (0, 1.6). We'll call this $(x1,y1) = (0,1.6)$. Given the user inputs $(x,y)$ how can I return the closest valid position of $(x1,y1)$?. This is a follow-up question with better clarifcation from the previous one here .","['real-numbers', 'functions']"
2100975,"Let $f: [0 ,1] \to \mathbb{R} $ be continuous, prove $\lim_{n\to \infty} \int_0^1 f(x^n)dx = f(0)$","Let $f: [0,1] \to \mathbb{R}$ be continuous, prove $\lim_{n\to \infty} \int_0^1 f(x^n)dx = f(0)$ This makes some sense looking at it. I have only the Regulated Integral definition of integration to work with : https://en.wikipedia.org/wiki/Regulated_integral It uses sequences of step functions with a partition over a closed interval. So I need to solve the integral prior to taking the limit but not sure how to really get what I need. Since $f $ is continuous function there exists a sequence $(\varphi_n)_{n \in \mathbb{N}}$ of step functions such that $\lim_{n \to \infty} \sup_{x \in [0,1]} \mid f(x) - \varphi_n(x)\mid \,= 0$. Moreover, $\int_0^1 f(x)dx := \lim_{m \to \infty} \int_0^1 \varphi_m(x)dx $ so it should be the case that $\int_0^1 f(x^n)dx := \lim_{m \to \infty} \int_0^1 \varphi_m(x^n)dx $ so I assume that it would be that $\lim_{n \to \infty } \int_0^1 f(x^n)dx := \lim_{n \to\infty} (\lim_{n \to \infty} \int_0^1 \varphi_m(x^n)dx )$ Now, $ \int_a^b \varphi(\eta)d\eta := \sum_{j=0}^{N}\varphi(\eta_j)(\sigma_{j+1}-\sigma_j)$ where $(\sigma_j)_{j=0}^{N+1})$ is a partition of $[a,b]$ and $\eta_{j} \in (\sigma_j,\sigma_{j+1})$ such that each block of the partition is constant so choice of $\eta_j$ is not particularly important. So should have $ lim_{n \to \infty}(lim_{m \to \infty}\int_a^b \varphi_m(\eta^n)d\eta := lim_{n \to \infty}(\lim_{m \to \infty}\sum_{j=0}^{N}\varphi_m(\eta_j^n)(\sigma_{j+1}-\sigma_j))$. This is where I get stuck.","['real-analysis', 'integration', 'analysis']"
2101001,How do you solve $(i^{i})^{i}$? [duplicate],"This question already has answers here : What is $i^i$ and $(i^i)^i$ (3 answers) Prove that $i^i$ is a real number (6 answers) Closed 7 years ago . I am trying to solve this problem, and show that it does not necessarily equal $i^{-1}$. My method so far is
$$(i)^{i} \\
=e^{(i)\ln{(i)} }
i = \cos(\theta)+i\sin(\theta)
\Rightarrow \theta = \frac{\pi}{2}\pm2n\pi, n \in \mathbb{Z} \\
\Rightarrow i = e^{i\frac{\pi}{2}\pm2in\pi} \\
\Rightarrow \ln{(i)} = i\frac{\pi}{2}\pm2in\pi \\
\Rightarrow e^{(i)\ln{(i)} } = e^{(i)(i\frac{\pi}{2}\pm2in\pi)}\\
=e^{(-\frac{\pi}{2}\pm2n\pi)}$$ However when I raise this to the power of $i$ once more, the answer ends up becoming $i^{-1}$ once more. What am I missing here? If someone can please show me where I'm going wrong that would be fantastic!","['complex-analysis', 'complex-numbers']"
2101056,How to compute the formula of common tangent plane of three spheres,"Now there are three spheres, $s_0(x_0,y_0,z_0,r_0), s_1(x_1,y_1,z_1,r_1), s_2(x_2,y_2,z_2,r_2)$. $(x_i,y_i,z_i)$ represents the center of the $i$th sphere,and $r_i$ is the radius. I want to calculate the formula of the common tangent planes of the three spheres. I don't know how. Please tell me the answers if you know. Thanks very much!",['geometry']
2101076,Fourier transform in the tempered distributions space,"I'm trying to prove that the following formula holds in the tempered distributions space $S'(\mathbb{R}^n)$:
$$\widehat{\exp(-a|x|^2)}=\left(\frac{\pi}{a}\right)^{\frac{n}{2}}\exp\left(-\frac{\pi^2|\xi|^2}{a}\right)\hspace{0.1cm};\hspace{0.1cm} Re(a)\geq0\hspace{0.1cm};\hspace{0.1cm} a\neq0,$$
where $\sqrt{a}$ is defined as the branch with $Re(a)>0$. I have no problem when I consider $a\in\mathbb{R}$ because in that case $\exp(-a|x|^2)\in L^1(\mathbb{R}^n)$ and its Fourier transform (given by that formula) coincides with the transform in the tempered distributions sense. But I don't know what to do in the general case ($a\in \mathbb{C}$). The book (Introduction to Nonlinear Dispersive Equations - Linares,Ponce) suggest to use an analytic continuation argument, but I don't really see how to do it. Can anyone help me with this, please? Thanks.","['functional-analysis', 'complex-analysis', 'distribution-theory', 'fourier-transform']"
2101096,Prove if $f(0) = 0$ then $\lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0$ for regulated function $f$,"Prove if $f(0) = 0$ then $\displaystyle\lim_{x \to 0^+}x\int_x^1 \frac{f(t)}{t^2}dt = 0$ for regulated function $f$ A regulated function is a function $f$ on $[a,b]$ such that $\exists$ a sequence $(\varphi_n)_{n \in \mathbb{N}}$ of step functions such that $\displaystyle \lim_{n \to \infty} \sup_{x \in [a,b]} \lvert f(x) - \varphi_n(x) \rvert = 0$ and $\forall x \in (a,b)$ the left and right limits exist, also left limit of $b$ & right limit of $a$. Also given the assumption that $f$ is continuous at $0$ which is said to be redundant. So we know $f$ is continuous on $[0,1)$. For $\varepsilon > 0, \exists \delta > 0, \lvert x \rvert < \delta \implies \lvert f(x) - f(0) \rvert = \lvert f(x) \rvert < \varepsilon$ Intuitively, I know the $x$ outside the integral goes to $0$ so as long as the integral itself converges then we can get the desired $0$ as the limit. so get $\displaystyle\lim_{x \to 0^+}x \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt = 0 \cdot \lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt$ so we need $\displaystyle\lim_{x \to 0^+}\int_x^1 \frac{f(t)}{t^2}dt < \infty$ Now I'm stuck because if $x = 0$ then $\displaystyle \frac{f(0)}{0} = \frac{0}{0}$","['real-analysis', 'integration', 'analysis']"
2101116,"How many subsets have a given sum, if the subsets can only contain at most $m$ elements?","Let set $S = \left \{ 1,2,3,4,5,6,\ldots,10,11,12 \right \}$. Find how many sets are possible with a given sum and and upto given $m$. For Ex : Take Sum = 6 and $m$ = size = $4$ The possible sets are :  $\left \{ 2,4 \right \}, \left \{ 1,3,2 \right \}, \left \{ 1,4,1 \right \}, \left \{ 1,1,1,3 \right \}, \ldots$ but  $\left \{ 1,1,1,1,2 \right \}$ is not valid as $m = 4$ and this contains $5$ elements How many sets are possible with $m = 5$ and sum = $8$ ? Is there any generalized idea to calculate sets ?","['combinatorics', 'multisets', 'elementary-set-theory']"
2101118,"What are $x$, $y$ and $z$ if $\frac{x}{y + z} + \frac{y}{x + z} + \frac{z}{x + y} = 4$ and $x$, $y$ and $z$ are whole numbers?","What are $x$, $y$ and $z$ if $$\dfrac{x}{y + z} + \dfrac{y}{x + z} + \dfrac{z}{x + y} = 4$$ and $x$, $y$ and $z$ are whole numbers? MY ATTEMPT Let $u = x + y + z$.  Then the equation can be rewritten as
$$\dfrac{x}{u - x} + \dfrac{y}{u - y} + \dfrac{z}{u - z} = 4$$ Suppose I set
$$1 = \dfrac{x}{u - x} = \dfrac{y}{u - y}$$
and
$$2 = \dfrac{z}{u - z}.$$ Then I get
$$x = y + z$$
$$y = x + z$$
$$z = 2(x + y),$$
so that
$$z = 0 = x + y,$$
which is impossible. Next, suppose I set
$$\dfrac{4}{3} = \dfrac{x}{u - x} = \dfrac{y}{u - y} = \dfrac{z}{u - z}.$$ Then I get
$$4(u - x) = 3x$$
$$4(u - y) = 3y$$
$$4(u - z) = 3z$$
so that
$$12u - 4(x + y + z) = 3(x + y + z)$$
which implies that
$$12u = 7(x + y + z) = 7u$$
from which it follows that
$$u = 0.$$
This is, again, impossible. Alas, here is where I get stuck.  Any hint(s) will be appreciated.","['algebra-precalculus', 'recreational-mathematics', 'diophantine-equations']"
2101139,Weighted Average from Errors,"This should be a really easy problem, but I want to know the right way to do it instead of making something up. Data: Given the data set ( the unit is nano meters ):
$$
D = \{ 3386, 3290, 3372, 3450 \}
$$
The following is the respective errors:
$$
\sigma_D = \{ 50, 180, 42, 100 \}
$$ Question: How do I find the weighted average?","['statistics', 'average']"
2101142,Prove that there are at least 100 pairs of usable boots,"A store has 200 boots of size A, 200 boots of size B and 200 boots of size C. Among these 600 boots, 300 are of the left foot and 300 are of the right foot. Knowing that usable pairs of boots have the same size and are for different feet, prove that it's possible to find at least 100 pairs of usable boots.","['combinatorics', 'pigeonhole-principle']"
2101143,Roots of a quintic equation,How can I find the roots of a quintic equation. However my original question is what is the sum of the roots of $x^5+3x^2+7=0$.,['algebra-precalculus']
2101150,Poisson Equation - $L^2$ boundary regularity,"Let $\mathbb{R}_{+}^n = \{(x',x_n): x' \in \mathbb{R}^{n-1}, x_n > 0 \}$ be the half space. It is known that if $u \in H_0^1(\mathbb{R}_{+}^n)$ is a weak solution to  $-\Delta u = f$ with $f \in L^2(\mathbb{R}_{+}^n)$ then in fact $u \in H^2(\mathbb{R}_{+}^n)$. Furthermore if $f \in H^1$ then $u \in H^3$, etc. This second part can be shown as follows: first show that for $u\in H_0^1\cap H^2$, the tangential derivatives $\partial_j u \in H_0^1(\mathbb{R}_+^n)$ (ie for $j\neq n$) is a weak solution to poisson equation, thus by first part $\partial_j u \in H^2$. Secondly, we need to just show $\partial_n^2 u\in H^1$, which comes from $-\partial_n^2 u = \partial_1^2 u + \cdot \cdot \cdot + \partial_{n-1}^2u + f \in H^1$. Now suppose we only have that $f \in L^2$ with $\partial_j f \in L^2$ for some $j$. I would like $\partial_j u \in H^2$. In the case when the derivative is tangential to boundary, ie $j\neq n$ then this extra smoothness is carried over to $u$ as explained above. But what happens if $j=n$, ie the extra smoothness is normal to the boundary? The argument above requires tangent derivatives to control the normal derivative so it doesn't work. Is there some other way to show that that the additional normal smoothness on $f$ carries over to $u$, or is this in fact false?","['functional-analysis', 'regularity-theory-of-pdes', 'elliptic-equations', 'partial-differential-equations']"
2101156,How to solve this mathematical puzzle?,"10 investors bought 3 stocks each and any two of them bought at least one stock in
common. For the stock owned by most people, what is the minimum number of people who have bought it? I tried applying the concepts of set theory as well as pigeon hole principle, but cannot really wrap my head around it.","['puzzle', 'combinatorics', 'pigeonhole-principle', 'maxima-minima']"
2101191,"All subrings of ring are division rings, then is a field.","My professor put this problem in a list of $200$ problems for the examen: A ring $R$ is called periodic if for each $x\in R$ exist $n\geq 2$ such that $x^n=x$ . Starting from the Jacobson Theorem (All periodic rings are commutative) conclude that: Any finite division ring is a field. If $R$ is a ring such that any subring is a division ring, then $R$ is a field. The first one is the Wedderburn theorem, and I found proofs in books and in internet, but the second one I don't know how to proceed with it. I would appreciate if anyone can give me a hint or a reference to read the proof.","['finite-fields', 'abstract-algebra', 'ring-theory']"
2101284,"Obvious but unprovable: If $f'$ is the zero function on $\mathbb Q$, then $f$ is constant [duplicate]","This question already has an answer here : Is a function whose derivative vanishes at rationals constant? [duplicate] (1 answer) Closed 7 years ago . Let $f:\mathbb Q\to\mathbb R$ be a uniformly continuous function and assume that $f'(x)=0$ for all $x\in\mathbb Q$. That $f$ is constant is obvious...and, as far as I can tell, unprovable. Please tell me that I'm wrong!","['derivatives', 'real-analysis']"
2101295,Differentiate to get error on weighted mean (summation fraction),"I need help differentiating a fraction containing two summations. Given the weighted mean $
\bar{x} = \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^{2}}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^{2}}$ where $x_{i}$ and $\sigma_{i}$ are uncorrelated.
The error propagation formula $\sigma_{f}^2 = \sigma_{f}^2 \left(\frac{\delta f }{\delta x}\right)^2 +  \sigma_{f}^2 \left(\frac{\delta f }{\delta y}\right)^2 + ...$ yields the error on $\bar{x}$ as:
$\sigma_{\bar{x}}^2 = \sum\limits_{i=1}^{\rm N} \left(\frac{\delta \bar{x} }{\delta x_{i}}\right)^2  \sigma_{i}^2$ The literature* solves the derivative as: $\frac{\delta \bar{x} }{\delta x_{i}} = \frac{\delta }{\delta x_{i}} \frac{\sum\limits_{i=1}^{\rm N} x_{i}/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2}
	= \frac{1/\sigma_{i}^2}{\sum\limits_{i=1}^{\rm N} 1/\sigma_{i}^2}$ I cannot figure out how this result is achieved. By my working, if $\sigma_{i}$ can be treated as a constant, then $\frac{\delta \bar{x} }{\delta x_{i}}=1$. Am I missing something? I have tried the Quotient rule but I don't see how it applies in this case. Eqn. 4.19, Bevington, Data Reduction and Error Analysis for the Physical Sciences http://astro.cornell.edu/academics/courses/astro3310/Books/Bevington_opt.pdf","['means', 'summation', 'ordinary-differential-equations', 'average']"
2101297,Possible alternate proof of uniqueness of power series?,"I want to show that if $\sum_{k=0}^\infty a_kx^k = 0$ on $[0,1]$, then $a_k=0 \forall k\in\mathbb{N}$. I'm aware of the standard proof, but wanted to try another argument. We know that a polynomial of degree $k$ has at most k roots, so the polynomial $\sum_{k=0}^N a_kx^k$ has at most $N$ roots. Now if we let $N\rightarrow \infty$ we know that we can have at most ""$\aleph_0$ roots"", but for the function to be identically zero we must have uncountably many roots. I know that this argument is rubbish, but is there a way (by transfinite induction/Baire Category or something else) to make this idea rigorous?",['real-analysis']
2101303,Show $\text{det}(A)$ is divisible by $2^{n-1}$,Let $A_{n \times n}$ be a $n$ by $n$ matrix with all entries equal to $\pm 1$. I want to show $\text{det}(A)$ is divisible by $2^{n-1}$. How can I appraoch this?,"['matrices', 'linear-algebra', 'determinant']"
2101311,A tough integral : $\int_{0}^{\infty }\frac{\sin x \text{ or} \cos x}{\sqrt{x^{2}+z^{2}}}\ln\left ( x^{2}+z^{2} \right )\mathrm{d}x$,"Recently, I found these two interesting integrals in Handbook of special functions page 141. $$\mathcal{I}=\int_{0}^{\infty }\frac{\sin(ax)}{\sqrt{x^{2}+z^{2}}}\ln\left ( x^{2}+z^{2} \right )\mathrm{d}x$$ $$\mathcal{J}=\int_{0}^{\infty }\frac{\cos(ax)}{\sqrt{x^{2}+z^{2}}}\ln\left ( x^{2}+z^{2} \right )\mathrm{d}x$$ In this book, it gives the answer below $$\mathcal{I}=\frac{\pi }{2}\left (\ln\frac{z}{2a}-\gamma   \right )\left [ I_0\left ( az \right )- \mathbf{L}_0\left ( az \right )\right ]+\frac{1}{4\pi }G_{24}^{32}\left ( \frac{a^{2}z^{2}}{4}\middle|\begin{matrix}
 \dfrac{1}{2},\dfrac{1}{2} \\ 
0,0,\dfrac{1}{2},\dfrac{1}{2}
\end{matrix} \right )~~~,~~~\left (a,\Re z>0  \right )$$ $$\mathcal{J}=\left ( \ln\frac{z}{2a}-\gamma  \right )K_0\left ( az \right )~~~,~~~\left ( a,\Re z>0 \right )$$ where $I_0(\cdot)$ is modified bessel function of the first kind , $\mathbf{L}_0(\cdot)$ is modified struve function , $G_{pq}^{mn}(\cdot)$ is meijer-G function and $K_0(\cdot)$ is bessel function of rhe second kind. So, I tried to figure out how to get the answer. My attempt: Let $x=z\tan t$ , we have \begin{align*}
\mathcal{I}&=2\int_{0}^{\frac{\pi }{2}}\sin\left ( az\tan t \right )\ln\left ( z\sec t \right )\sec t\, \mathrm{d}t\\
&=2\int_{0}^{\frac{\pi }{2}}\sin\left ( az\tan t \right )\ln\left ( \sec t \right )\sec t\, \mathrm{d}t+2\ln z\int_{0}^{\frac{\pi }{2}}\sin\left ( az\tan t \right )\sec t\, \mathrm{d}t
\end{align*} Hence, define $$\mathcal{I}\left ( m \right )=\int_{0}^{\frac{\pi }{2}}\sin\left ( az\tan t \right )\sec^mt\, \mathrm{d}t$$ then using the taylor series of $\sin x$ we get \begin{align*}
\mathcal{I}\left ( m \right )&=\sum_{k=0}^{\infty }\left ( -1 \right )^{k}\frac{\left ( az \right )^{2k+1}}{\left ( 2k+1 \right )!}\int_{0}^{\frac{\pi }{2}}\tan^{2k+1}t\sec^mt\, \mathrm{d}t \\
&=\sum_{k=0}^{\infty }\left ( -1 \right )^{k}\frac{\left ( az \right )^{2k+1}}{\left ( 2k+1 \right )!}\int_{0}^{\frac{\pi }{2}}\sin^{2k+1}t\cos^{-2k-m-1}t\, \mathrm{d}t
\end{align*} By using the same way we get $$\begin{align*}
\mathcal{J}\left ( m \right )&=\sum_{k=0}^{\infty }\left ( -1 \right )^{k}\frac{\left ( az \right )^{2k}}{\left ( 2k \right )!}\int_{0}^{\frac{\pi }{2}}\tan^{2k}t\sec^mt\, \mathrm{d}t \\
&=\sum_{k=0}^{\infty }\left ( -1 \right )^{k}\frac{\left ( az \right )^{2k}}{\left ( 2k \right )!}\int_{0}^{\frac{\pi }{2}}\sin^{2k}t\cos^{-2k-m}t\, \mathrm{d}t
\end{align*}$$ But how to evaluate the last integral, it seems can't be expressed by Beta function. If I'm doing the wrong way, is there another way to solve the problem. Any help will be appreciated!","['bessel-functions', 'integration', 'definite-integrals', 'calculus']"
2101320,Limit of a function with powers (L'Hopital doesnt work),"I have a little problem with limit of this function: $\lim_{x \to \infty} x^2(2017^{\frac{1}{x}} - 2017^{\frac{1}{x+1}})$ I have tried de L'Hopital rule twice, but it doesn't work. Now I have no idea how to do it.",['limits']
2101332,Is it possible to have a $3 \times 3$ matrix that is both orthogonal and skew-symmetric?,Is it possible to have a $3 \times 3$ matrix that is both orthogonal and skew-symmetric? I know it has something to do with the odd order of the matrix and it is not possible to have such a matrix. But what is the reason?,"['matrices', 'orthogonal-matrices', 'linear-algebra', 'skew-symmetric-matrices']"
2101344,Limit $\lim_{x \to 0} \frac{\sin[x]}{[x]}$,"I am trying to find the limit  of  $$\lim_{x \to 0} \frac{\sin[x]}{[x]}$$ where [.] represents the greatest integer function.
I tried to take up an infinitesimally small number $h$ and took up the  Right Hand Limit and Left Hand limit $$\lim_{x \to 0^+}\frac{\sin[x]}{[x]}$$
$$\Rightarrow \lim_{h \to 0} \frac{\sin[h]}{[h]}$$ I am stuck over here, though I know that $$\lim_{x \to 0}\frac{\sin x}{x}=1$$ But here I see that since $h$ is a very small positive number$[h]$ itself becomes zero and we get $$\Rightarrow  \frac{\sin 0}{0}.$$ Does this shows that the RHL doesn't exist or am I at fault somewhere?","['ceiling-and-floor-functions', 'trigonometry', 'limits']"
2101381,The differential of an almost everywhere constant function is almost everywhere zero?,"Let $f: \mathbb{R}^n \to \mathbb{R}$ be measurable and differentiable almost everywhere. Is it true that $df=0 $ a.e on $\{f=0\}$? If $f \in W^{1,p}$ then it's known to be true- but the proof* uses the fact that weak derivatives behave well w.r.t fundamental theorem of calculus, so I am guessing that withouth this assumption this should not hold in general. *(It is Theorem 4.4 (iv) in ""Measure theory and
fine properties of functions"", revised ed)","['measure-theory', 'calculus']"
2101403,"Haar system forms an orthonormal system in $L_2[0,1]$","Haar wavelets are defined as: $$
\psi_{0,0}(t) = 
\begin{cases}
1, \text{ for } 0<t< 1/2\\
-1, \text{ for } 1/2<t<1 \\
0, \text{ otherwise }
\end{cases}
$$
And for $n \geq 0$, $0 \leq k < 2^n$
$$\psi_{n,k} = 2^{n/2} \psi_{0,0}(2^n t -k).$$ I was able to prove the orthonormality of those functions, and tried to approximate polynomials and sin/cos functions, since I know that those are a basis, but both methods failed. This result has been proven by B.S. Kashin, A.A. Saakyan, ""Orthogonal series"" , Moscow (1984), but Russian isn't my forte. Full derivations and general pointers are appreciated. I encountered these in the context of Brownian motion. Edit: Wolfram alpha provides this graph of first few functions",['functional-analysis']
2101415,Real logarithms of special orthogonal matrices,"In continuation to the following question : Let $P\in\text{SO}(2)$ and let $X$ be a real matrix such that $P = e^X$. Is $X$ necessarily skew-symmetric? No. The above-mentioned thread gives counter examples. However, are there counter examples in which $P$ is neither $I_2$ nor $-I_2$? More generally, in higher dimension, what can be said about special orthogonal matrices that have non-skew-symmetric real logarithms?","['matrices', 'matrix-exponential', 'lie-groups']"
2101465,Show us how to prove that $\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0$,I saw these two intgerals and would like to know if they are correct. $$\int_{0}^{\infty}{\sin(e^{-\gamma}x)}\cdot{\ln{x}\over x}\mathrm dx=0\tag1$$ $$\int_{0}^{\infty}{\sin\left(\sqrt{x}^{\sqrt{2}}\right)}\cdot{\ln{x}\over x}\mathrm dx=-\pi\gamma\tag2$$ Where $\gamma$ is the Euler Mascheroni constant Here I ignored the limits I apply sub: to $(1)$ $u=\ln{x}$ $xdu=dx$ $$I = \int{ue^{-u}\sin(e^{u-\gamma})}du\tag3$$ Apply integration by parts to $(3)$ $$\int{ue^{-u}}du=-e^{-u}(1+u)$$ $$I={-e^{-u}(1+u)\sin(e^{u-\gamma})}+\int{e^{-u}(1+u)\cos(e^{u-\gamma})}du$$ Encounter more harder than before. Please show us how to prove $(1)$ and $(2)$,"['improper-integrals', 'integration', 'definite-integrals', 'calculus']"
2101495,Trigonometric functions for complex numbers,"I thought I had defined what cos(ai) and sin(ai) was earlier today when I did the following: $e^{(vi)} = \cos(v) + i\sin(v)$ If we let $v = ai$, where a is real, we get: $e^{(aii)} = \cos(ai) + i\sin(ai) = e^{(-a)}$ Since $e^{(-a)}$ is a real number, the $i\sin(ai)$ must be 0, and therefor $\cos(ai)$ must be $e^{(-a)}$. So I have concluded that $\cos(ai) = e^{(-a)}$ and $\sin(ai) = 0$ I am pretty sure this is wrong since I have seen different answers online, but I would like to know what I did wrong. Thanks","['trigonometry', 'complex-numbers']"
2101519,Automorphisms of the group $\operatorname{GL}_n \mathbb C$,"I'm interested in the determination of a group $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$. A class of examples of automorphisms of $\operatorname{GL}_n \mathbb C$ is given by conjugations $c_g$, for $g \in \operatorname{GL}_n \mathbb C$:
$$ c_g(A) = gAg^{-1}.$$ These automorphisms form a normal subgroup of $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$, a group of inner automorphisms. But for $n > 2$, we have an example of an outer automorphism:
define
$$ \iota(A) = (A^T)^{-1} = (A^{-1})^T.$$ Why we can't have $\iota = c_g$, for some $g \in G$?
Take $\lambda \in \mathbb C$ such that $\lambda^n = 1, \lambda^2 \neq 1$.
Then:
$$\iota(\lambda I) = \lambda^{-1} I,$$ (which isn't equal to $\lambda I$ because of $\lambda^2 \neq 1$) but $$ c_g(\lambda I) = \lambda I,$$ for all $g$.
My questions are, in an order of increasing difficulty: 1) Is $\operatorname{Aut}( \operatorname{GL}_2 \mathbb C) = \operatorname{Inn}( \operatorname{GL}_2 \mathbb C)$? 2) Is $ \operatorname{Aut}( \operatorname{GL}_n \mathbb C)/ \operatorname{Inn}( \operatorname{GL}_n \mathbb C) = 
\{\mathrm{id}, \iota \} \simeq \mathbb Z / 2\mathbb Z $? 3) What is $\operatorname{Aut}( \operatorname{GL}_n \mathbb C)$? I would be really grateful if someone could give me a good reference in which all of these questions are discussed (especially if it's done for general fields).","['matrices', 'abstract-algebra', 'group-theory']"
2101532,Primes satisfy $pq+2=rt$,"Are there infinitely many primes $p, q, r, t$ such as $pq+2=rt$ $3\times{11}+2=5\times{7}$ $5\times{11}+2=3\times{19}$ I guess this may be an open question?","['number-theory', 'prime-numbers']"
2101573,Proving 20-th cyclotomic field has class number one.,"(This is related to one of my previous questions; reading is not required though) I'm still banging my head on the following exercise. Consider the primitive $n$ -th root of unity $\zeta_{n} := exp(\frac{2\pi i}{n})$ . Show that the number field $K := \mathbb{Q}(\zeta_{20})$ has class number one. In the exercise, the following hint is given: Show that it suffices to show that any prime ideal above the primes $2,3,5,7,11$ is principal. We know that the quadratic subfields of $\mathbb{Q}(\zeta_{20})$ are $\mathbb{Q}(\sqrt{-5})$ , $\mathbb{Q}(\sqrt{5})$ , $\mathbb{Q}(i)$ . The prime 2 may be treated via $\mathbb{Q}(i)$ . For 3 and 7, observe that $\omega_1^2 + \omega_2^2 = 3$ and $\omega_1^4 + \omega_2^4 = 7$ , where $\omega_1 := (1+\sqrt{5})/2$ and $\omega_2 := (1-\sqrt{5})/2$ . For 5 show that the norm from $\mathbb{Q}(\zeta_{20})$ to $\mathbb{Q}(\zeta_{5})$ of $(\zeta_5 + \zeta_5^{-1})+\zeta_5^2\cdot i$ is $1-\zeta_5$ . For 11, first determine its prime factors in $\mathbb{Q}(\zeta_5)$ . What I have accomplished so far : I have shown via Minkowski-bound that it suffices to show that any prime ideal above the primes $2,3,5,7,11$ is principal. I also managed to show that any prime ideal above $2$ or $11$ must be principal. So it remains to show that for the primes $3,5,7$ , obviously using the hints above. However I don't really seem to get to the point where the hints make sense to me. If it helps, I also computed the inertia/decomposition fields: Given a prime number $p$ , let $r$ denote number of prime ideals of $\mathbb{Z}[\zeta_{20}]$ above $p$ . Then let $e$ be ramification index and $f$ the inertia degree. For primes $3,5,7$ get: Prime 3 : $r=2, e=1, f=4$ . Decomposition field = $\mathbb{Q}(\sqrt{-5})$ , inertia field = $K$ Prime 5 : $r=2, e=4, f=1$ . Decomposition field = inertia field = $\mathbb{Q}(i)$ Prime 7 : $r=2, e=1, f=1$ . Decomposition field = $\mathbb{Q}(\sqrt{-5})$ , inertia field = $K$ Thanks for any help in advance, I'd really like to close this chapter.","['number-theory', 'maximal-and-prime-ideals', 'algebraic-number-theory']"
2101606,Formula for midpoint in hyperbolic 3-space,"Consider the hyperbolic 3-space $(\mathbb{H}^3,ds^2)$ with $$\mathbb{H}^3:=\{(x,y,z)\in\mathbb{R}^3|z>0\}, \quad ds^2=\frac{dx^2+dy^2+dz^2}{z^2}$$ Geodesics for this space are circular arcs normal to $\{z=0\}$ and vertical rays normal to $\{z=0\}$. Given any two points, for example $p1=(2,-1,3),p_2=(1,2,4)$ (I've chosen them so they don't lie on a vertical ray) is it possible to obtain a closed formula for the coordinates of the midpoint $m$ of $p_1$ and $p_2$? (i.e. the point $m$ is such that $d(m,p_1)=d(m,p_2)=d(p_1,p_2)/2$ where $d$ is the metric induced by $ds^2$) I just can think of this method: given $p_1$ and $p_2$ find the plane $H$ orthogonal to $\{z=0\}$ and which contains both $p_1$ and $p_2$. Then in $H$ find the circular arc $g:[0,d(p_1,p_2)]\rightarrow H$ orthogonal to $\{z=0\}$ with $g(0)=p_1$, $g(1)=p_2$ and $|\dot g(t)|_{ds^2}=1$. Finally $m=g(d(p_1,p_2)/2)$. But my method is rather long and complicated. Do you know of a better one which could be written directly as a formula? I'm interested in the case of $(\mathbb{H}^3,ds^2)$ to find a method which could be generalized for other riemannian and metric spaces.","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
2101607,How to tell if a cubic curve in the projective plane is irreducible,"Let $\mathcal{C}$ be a cubic curve in the complex projective plane given in terms of its defining equation. How can I systematically tell if $\mathcal{C}$ is irreducible? By “systematically” I mean “without being clever”, e.g. without eyeballing the polynomial and finding a factorization this way. However, the solution does not necessarily have to be algorithmic. Some facts (please correct me if I’m wrong): $\mathcal{C}$ is non-singular $\Rightarrow \mathcal{C}$ is irreducible $\mathcal{C}$ has a ordinary double point (a “node”) $\Rightarrow \mathcal{C}$ is irreducible $\mathcal{C}$ has a triple point $\Rightarrow \mathcal{C}$ is reducible $\mathcal{C}$ has more than one singular point $\Rightarrow \mathcal{C}$ is reducible But what if $\mathcal{C}$ has a non-ordinary double point? And are there other ways of telling if a curve is irreducible than considering its singularities?","['algebraic-curves', 'projective-space', 'projective-geometry', 'algebraic-geometry']"
2101611,Element prime order (EPO) group,"A  finite  group  $G$  is  called  EPO-group  if every  non-identity  element  of  $G$  has  prime  order. For a given finite group $G$, let $\pi (G)$ denote the set of all prime divisors of $|G|$. Does there exist  non-isomorphic EPO-groups $G_1$, $G_2$ of same order with $|\pi(G_i)|\geq 2$ , $i=1,2$ ?","['finite-groups', 'group-theory', 'group-isomorphism']"
2101626,Does every set of orthogonal coordinates in 2D Euclidean space yield a conformally-separable metric?,"Consider the 2D Euclidean plane in Cartesian coordinates which has the metric
$$ds^2 = dx^2 + dy^2$$
If we transform into polar coordinates , we obtain the metric in the form
$$ds^2 = r^2 (\frac{1}{r^2} dr^2 + d\vartheta^2)$$
If we transform into parabolic coordinates , we obtain the metric in the form
$$ds^2 = (\sigma^2 + \tau^2)(d\sigma^2 + d\tau^2)$$
Etc. etc. In any case, it seems that every orthogonal coordinate transformation into coordinates $w,z$ transforms the metric into a ""conformally-separable"" form
$$ds^2 = \Omega^2(w,z) (W(w) dw^2 + Z(z) dz^2)$$
That is, each of the diagonal metric components are, up to a common conformal factor $\Omega^2$, functions only of the respective coordinates. (This means that e.g. the Laplace equation will be separable in these coordinates.) Is this the case for any analytic 2D orthogonal coordinates? (Can you give a nice canonical reference for further reading?) My attempts: I tried to approach this question by first investigating the question: does every orthogonal coordinate transformation generated by a holomorphic function yield such a metric? Consider a holomorphic function $f(z)$ with $x=\Re (f(z)),\, y=\Im (f(z))$ and the transformed coordinates are $w=\Re (z),\,v=\Im (z)$. This gives us the coordinate transform as $x=x(w,v),\,y=y(w,v)$ The holomorphic nature of $f(z)$ means that it fulfills the Cauchy-Riemann equations which, in this case, can be written as
$$\partial_w x= \partial_v y$$
$$\partial_v x= -\partial_w y$$
By transforming the Euclidean metric using this relation we obtain
$$d s^2 = [(\partial_w x)^2 + (\partial_w y)^2] d w^2 + [(\partial_v x)^2 + (\partial_v y)^2] d v^2$$
which can be also rewritten as
$$d s^2 = |\partial_w f|^2 d w^2 + |\partial_v f|^2 d v^2$$
However, it is not clear how does this amount to the conformal-separable form...","['riemannian-geometry', 'coordinate-systems', 'euclidean-geometry', 'orthogonality', 'holomorphic-functions']"
2101637,"How to prove the number of all maps of the kind $f : \{1, 2, ..., n\} \to \{1, 2\}$ is $2^n$, for all $n \in\mathbb{N}$?","How can I prove ""The number of all maps of the kind $f : \{1, 2, ..., n\} \to \{1, 2\}$ is $2^n$, for all $n \in\mathbb{N}$? Using induction?
Thanks :)","['combinatorics', 'induction', 'functions', 'discrete-mathematics']"
2101653,A\(B∩C) = (A\B) ∪ (A\C) how to prove?,I have to prove that. While I know this is true by thinking about it I'm having a lot of trouble actually writing the proof,['elementary-set-theory']
2101702,Continued fraction for some integrals by Ramanujan,"While browsing through some letters of Ramanujan to G. H. Hardy I came across these two formulas (which are as mysterious as any other formula of Ramanujan): $$4\int_{0}^{\infty}\frac{xe^{-x\sqrt{5}}}{\cosh x}\,dx = \cfrac{1}{1+}\cfrac{1^{2}}{1+}\cfrac{1^{2}}{1+}\cfrac{2^{2}}{1+}\cfrac{2^{2}}{1+}\cfrac{3^{2}}{1+}\cfrac{3^{2}}{1+}\cdots\tag{1}$$ $$2\int_{0}^{\infty}\frac{x^{2}e^{-x\sqrt{3}}}{\sinh x}\,dx = \cfrac{1}{1+}\cfrac{1^{3}}{1+}\cfrac{1^{3}}{3+}\cfrac{2^{3}}{1+}\cfrac{2^{3}}{5+}\cfrac{3^{3}}{1+}\cfrac{3^{3}}{7+}\cdots\tag{2}$$ From these it appears that Ramanujan had general formulas for the integrals $$I_{m, n} = \int_{0}^{\infty}\frac{x^{m}e^{-x\sqrt{n}}}{\cosh x}\,dx,\,J_{m, n} = \int_{0}^{\infty}\frac{x^{m}e^{-x\sqrt{n}}}{\sinh x}\,dx\tag{3}$$ and for some values of $m, n$ he was able to express these as continued fractions. I am totally perplexed by these formulas. Do we have any proofs of these or perhaps any reference which treats the integrals $I_{m, n}, J_{m, n}$? Just for clarity the notation $$\cfrac{a_{1}}{b_{1}+}\cfrac{a_{2}}{b_{2}+}\cfrac{a_{3}}{b_{3}+}\cdots$$ consumes less space in typing compared to the more cumbersome but easier to understand continued fraction $$\cfrac{a_{1}}{b_{1} + \cfrac{a_{2}}{b_{2} + \cfrac{a_{3}}{b_{3} + \cdots}}}$$","['integration', 'definite-integrals', 'continued-fractions']"
2101706,Existence of a potential for given exact form satisfying symmetry conditions,"Suppose that $\omega$ is an exact differential form satisfying $L_X \omega=0$ for $X$ in some Lie algebra of vector fields. When can I find $\eta$ such that $\omega=d \eta$ and $L_X \eta=0?$ What are the obstructions for this problem? Motivating example: Manifold $\mathbb R^2$, $\omega=dx \wedge dy $. I can find $\eta = \frac{1}{2} ( x dy - y dx )$ which is rotationally symmetric, $\eta = x dy$ which is invariant with respect to  translations in $y$ and $\eta= -y dx$ which is invariant with respect to translations in $x$ direction. Finding $\eta$ with all these symmetries is impossible. Formulation in the language of cohomology: Since Lie derivatives commutes with exterior derivative, algebra of differential forms invariant under an action of given group forms a chain subcomplex of de Rham complex. The question essentially asks what can be said about its cohomology. As noted in comments by Ted Shifrin, in the case of compact groups invariant potential can be found by averaging action of a group on one potential (with respect to Haar measure). This is nice and useful point, but now case of noncompact groups remains.","['homology-cohomology', 'lie-derivative', 'differential-forms', 'symmetry', 'differential-geometry']"
2101745,What's the derivative of: $ \sqrt{x+\sqrt{{x}+\sqrt{x+\cdots}}}$?,"let $ y=\displaystyle \sqrt{x+\sqrt{{x}+\sqrt{x+\cdots}}}$, i'm really interesting to know how do I find : $\displaystyle \frac{dy}{dx}$ ?. Note: I have used the definition of derivative of the square root function but i don't succed . Thank you for any help","['derivatives', 'real-analysis', 'nested-radicals', 'functions']"
2101750,Continuous functions define a topology?,"The WP article on general topology has a section titled "" Defining topologies via continuous functions ,"" which says, given a set S, specifying the set of continuous functions $S \rightarrow X$ into all topological spaces X defines a topology [on S]. The first thing that bothered me about this was that clearly this collection of continuous functions is a proper class, not a set. Is there a way of patching up this statement so that it literally makes sense, and if so, how would one go about proving it? The same section of the article has this: for a function f from a set S to a topological space, the initial topology on S has as open subsets A of S those subsets for which f(A) is open in X. This confuses me, because it seems that this does not necessarily define a topology. For example, let S be a set with two elements, and let f be a function that takes these elements to two different points on the real line. Then f(S) is not open, which means that S is not an open set in S, but that violates one of the axioms of a topological space. Am I just being stupid because I haven't had enough coffee this morning?",['general-topology']
2101768,How many ways are there to place $l$ balls in $m$ boxes each of which has $n$ compartments (2)?,"I asked a question, but have not got any answer. So, I decided to break the question down into more simple questions. Here, you may see the main question , here the first part of question . Assume that there are $m$ boxes each of which contains $n$ compartments. There are also $l$ balls, where  $\ m\ <\ l\ <\ (m−1)n+2$. Moreover, the balls are the same. $\bullet\ $ I would like to know how many ways there are to place the balls in the boxes such that 1) Each compartment can hold up to one ball. 2) Each box must have at least one ball. 3) There are exactly $j$ boxes which contain only one ball. Without the constraint 3, you may see some answers here which I am not sure about the correctness. For sure, there some $j$s for which constraint 3 can be satisfied. I think $2m-l \leq j\leq \lfloor \frac{mn-l}{n-1}\rfloor $. I highly appreciate any help in advance.","['combinatorics', 'probability', 'discrete-mathematics']"
2101811,Divide a square into rectangles where each occurs once in each orientation,"A $26 \times 26$ square divides into different rectangles so that each occurs exactly twice in different orientations. I've also found a solution for the $10 \times 10$ square, but no others.  Are there any other squares that can be divided into a finite number of rectangles so that each occurs exactly twice in different orientations?","['combinatorics', 'graph-theory', 'tiling', 'recreational-mathematics']"
2101867,Change the order of integration if the inner integral is to a power,"Is there anything I can do with a double integral of the following form? $$ \int_0^1 \left(\int_0^x f(j) dj\right)^\alpha g(x) dx$$ For $\alpha = 1$, I can do $$ \int_0^1 \int_0^x f(j) dj g(x) dx = \int_0^x f(j) \int_0^1    g(x) dx dj$$, where the inner integral has a well defined solution. For general $\alpha$, I know that there is a difference between $x_1^a + x_2^a$ and $(x_1 + x_2)^a$, but is there perhaps a trick I can use to still change the order of integration?","['multivariable-calculus', 'integration', 'definite-integrals']"
2101871,"On average, how many times will I need to roll a six-sided die before I see ten ONES in total?","On average, how many times will I need to roll a six-sided die before I see ten ONES in total? I'd like to be able to extend my knowledge to answer other questions in the same form, such as, ""If only 15% of fish are within the legal size limit, how many fish should I need to catch before I get five keepers? Thanks kindly",['probability']
2101884,Scaling a 3d projection matrix to be equal to another projection matrix,"This problem has been eating at me for a little while now, and it's extremely frustrating. First off, let me begin with explaining the matrices I am using: Unity calculates culling matrices incorrectly. From the matrix, it seems to generate the actual occlusion culling itself scaled proportionally to the projection matrix, at least when it comes to the near clipping plane. This can typically be fixed by simply calculating a new projection matrix with half the near clip plane distance, but my problem is unfortunately not that simple. I want to calculate an oblique culling matrix (that is, with the near plane not perpendicular to the camera), and this near-clip-doubling is making it impossible to do as no matter what method I use to calculate the matrix, it is proportionally incorrect. I cannot figure out how to counter-compute against the scaling being caused by Unity's incorrect calculation, as I am simply not familiar enough with linear algebra and matrix math to intuitively understand the problem. Here is the code that is being used to calculate the oblique matrix //clipPlane is a vector4 in camera space (direction + distance from camera)
Matrix4x4 CalculateObliqueOcclusion(Matrix4x4 projection, Vector4 clipPlane, float signSide) {
    //signSide determines whether it is the near or the far clipping plane that moves to the clipPlane
    Vector4 q = projection.inverse * new Vector4(
         signSide,
         signSide,
        1.0f,
        1.0f
    );
    Vector4 c = clipPlane * (2F / (Vector4.Dot(clipPlane, q)));
    //Left/Right sizing
    projection[2] = c.x - projection[3];
    //Top/Down sizing
    projection[6] = c.y - projection[7];
    //Near/Far clipping plane adjustment. Used to move far clipping plane.
    projection[10] = (c.z - projection[11]);
    //Scale adjustment- used to move near clipping plane
    projection[14] = (c.w - projection[15]);
    return projection;
} Here is a video demonstrating the issue. the clipping plane is being generated correctly off of the object, but as you can see, the culling matrix is wildly out of control. Occlusion matrix being calculated incorrectly In this example, the invisible ""box"" that I am grabbing is being used to calculate the near plane of the projection matrix to be used — this code is working properly. The green box is a rendering of the volume of the occlusion culling matrix, which as you can see appears to scale with the rear plane by a factor of two.","['projection-matrices', 'matrix-equations', 'matrices', '3d', 'linear-algebra']"
2101896,What is the meaning of a differential in terms of an exact differential?,"As I understand it a differential is an outdated concept from the time of Liebniz which was used to define derivatives and integrals before limits came along. As such $dy$ or $dx$ don't really have any meaning on their own. I have seen in multiple places that the idea of thinking of a derivative as a ratio of two infinitesimal change while intuitive is wrong. I understand this, and besides I am not even really sure if there is a rigorous way of saying when a quantity is infinitesimal. Now on the other hand, it have read that you can define these differentials as actual quantities that are approximations in the change of a function. For example for a function of one real variable the differential is the function $df$ of two independent real variables $x$ and $Δx$ given by: $df(x,Δx)=f'(x)Δx$ How this then reduces to $df = f'(x)dx$ and again what $dx$ means I dont understand.
It seems to me that it is simply a linear approximation for the function at a point $x$. However there's no mention of how large or small $dx$ must be, it seems to be just as ill defined as before and I have still found other places referring to it as an infinitesimal even when it has been redefined as here. Anyway ignoring this, I can see how this could then be extended to functions of more than one independent variable $y = f(x_1,....,x_n)$ $dy = $$\frac{df}{dx_1}dx_1\ +\ .... \ +\frac{df}{dx_n}dx_n\ $ However then the notion of exact and inexact differentials are brought up. This seems like its unrelated but that raise the question of what a differential means in this case. All this comes from a course I am taking in Thermal Physics. ] 2 If anyone can enlighten me as to what the concept of differentials means or perhaps direct me towards a book or website where I can study it myself I would be very grateful. An explanation of Schwarz' Theorem in this context would be great too.","['multivariable-calculus', 'linear-approximation', 'calculus']"
2101897,Show that an open linear map between normed spaces is surjective.,"Let $X,Y$ be normed spaces and $T:X\to Y$ is an open linear map. Show that $T$ is surjective. In  order to show $T$ is surjective let's take $y_0\in Y$ and assume the contrary that $Tx\neq y_0\forall x\in X$. Now taking $x_0\in X\implies Tx_0\neq y$. Also $T(B(x_0,r))$ is open. $X=\cup_{n\in \Bbb N}B(x_0,n)\implies T(X)\subset \cup_{n\in \Bbb N} T(B(x_0,n))$. I am unable to find any contradiction.Can someone kindly help?","['functional-analysis', 'normed-spaces', 'linear-transformations', 'open-map']"
2101910,change in the data value to give a lower/equal mean than the median,"Does anyone knows, how to change (or which ONE VALUE of data do I have to change), so that the mean of this data are: 1. lower than the median, and 2. as equal as the median. For example if I have the data: 1, 1.2, 1.4, 1.6, 1.6, 1.8, 2.0, 2.6, 2.8, 2.8, 3. I've read some on the internet which says that: median + (new value - old value)/number of value (or we called n)= mean then I've tried to make a new mean value which is lower than the old mean. But it's always give a negative result for the ONE NEW VALUE (of the data). Is it true? So I can't have any ONE NEW POSITIVE VALUE (to make the mean lower/equal than the median)?","['means', 'statistics', 'data-analysis', 'median']"
2101912,"Evaluate $\int x^2e^{\frac{x^2}2} \, dx$","$$\int x^2e^{\frac{x^2}2} \, dx$$ I do not need to evaluate $\int e^{\frac{x^2}{2}}$ in a numeric method. If we take $u=x^2, u'=2x$  $v'=e^{\frac{x^2}{2}},v=\int e^{\frac{x^2}2}\,dx$ we get: $$\int x^2e^{\frac{x^2}{2}} \, dx = x^2\int e^{\frac{x^2}{2}} \,dx-2\int \left( x \int e^{\frac{x^2}2} \,dx\right) dx\text{?}$$ How can the solution be $\displaystyle xe^{\frac{x^2}2}-\int e^{\frac{x^2}{2}}dx$?",['integration']
2101922,Restricing domains for universal and existential quantifiers,"I am currently using Rosen's ""Discrete Mathematics and Its Applications"" (7th ed.) for my discrete mathematics course. We recently talked about quantifiers, more specifically the universal and existential quantifier. What confuses me is when we started talking about restricting the domain (i.e. instead of all real numbers, only all real positive numbers), where does the conditional statement come from when restricting the domain of a universal quantifier and where does the conjunction come from when restricting the domain of a existential quantifier? This link did help: Universal and Existential quantifier in Propositional logic However, the part that confuses here is how are the domains restricted in the answer given in the link above?","['predicate-logic', 'logic', 'discrete-mathematics']"
2101943,Possible projective duality between two determinantal formulas for triangle area,"The shoelace formula for the area of a polygon in terms of consecutive vertices is well-known. In the particular case of a triangle, this may be written using a 3-by-3 determinant as $$\text{area of triangle}=\frac{1}{2}\begin{vmatrix} x_1 & y_1 & 1 \\ x_2 & y_2 & 1 \\ x_3 & y_3 & 1\end{vmatrix}$$
where $(x_k,y_k)_{k=1,2,3}$ are the three vertices. Several proofs have appeared on this site already. What may be surprising is that there's another determinantal formula for the area in terms of the three lines . Specifically, a triangle with lines $a_k x+b_k y+c_k=0$ for $k=1,2,3$ satisfies $$\text{area of triangle}=\frac{1}{2C_1C_2C_3}\begin{vmatrix} a_1 & b_1 & c_1 \\ a_2 & b_2 & c_2 \\ a_3 & b_3 & c_3\end{vmatrix}^2$$ where $C_1,C_2,C_3$ are the cofactors of the third column. (An older question has multiple proofs.) The parallels between these formulas intrigue me: Both express the area of a triangle in terms of a determinant, but one in terms of points and the other in terms of lines . This is reminiscent of the duality of lines and points in projective geometry. Hence my question: Can these the two formulas indeed be understood through projective duality?","['projective-geometry', 'determinant', 'euclidean-geometry', 'triangles', 'geometry']"
2101953,"$G$ be a non-measurable subgroup of $(\mathbb R,+)$ ; $I$ be a bounded interval , then $m^*(G \cap I)=m^*(I)$?","Let $G$ be a non-measurable subgroup of $(\mathbb R,+)$ , $I$ be a bounded interval , then is it true that $m^*(G \cap I)=m^*(I)(=|I|)$ ? where $m^*$ denotes the Lebesgue outer measure","['measure-theory', 'real-analysis', 'group-theory', 'outer-measure']"
2101956,Lipschitz continuity implies differentiability almost everywhere.,"I am running into some troubles with Lipschitz continuous functions. Suppose I have some one-dimensional Lipschitz continuous function $f : \mathbb{R} \to \mathbb{R}$. How do I prove that its derivative exists almost everywhere, with respect to the Lebesgue measure? I found on other places on the internet that any Lipschitz continuous function is absolutely continuous, and that this directly implies that the functions is differentiable almost everywhere. I don't quite see how this argument goes, though. Any help with giving such a proof, or redirecting me to a source where I can find one, would be greatly appreciated.","['derivatives', 'almost-everywhere', 'lipschitz-functions']"
2101962,Sum the series: $1+\frac{1+3}{2!}+\frac{1+3+3^2}{3!}+\cdots$,We have the series$$1+\frac{1+3}{2!}+\frac{1+3+3^2}{3!}+\cdots$$How can we find the sum$?$ MY TRY: $n$th term of the series i.e $T_n=\frac{3^0+3^1+3^2+\cdots+3^n}{(n+1)!}$. I don't know how to proceed further. Thank you.,"['real-analysis', 'sequences-and-series']"
2101990,Hilbert-Modules and Cohens factorization,"Let $B$ be a $C^*$-algebra and $E$ a right Hilbert-$B$-Module. If I take an approximate unit $(e_i)_i$ in $B$ then for all $x \in E$ we have $x\cdot e_i \rightarrow x$ in $E$ by a simple calculation.  I think by Cohens factorization Theorem we should have $E \cdot B:=\{x \cdot b | x \in E, b \in B\}=E$. But on the Wikipedia page it only says that $E \cdot B$ is dense. Now I am a little worried that I got that wrong. I would be grateful if somebody could clarify that for me. Thank you","['functional-analysis', 'hilbert-modules', 'operator-algebras']"
2102017,Need help solving this Discrete Math problem,"How many natural numbers smaller than $10\,000$ that have no digit $0$ and the sum of their digits equals $9$? I am really having a hard time solving this question. Any help would be appreciated!","['combinatorics', 'discrete-mathematics']"
2102083,Completing the square for quadratic equation,"I'm learning algebra (at age $66$) with home schooling materials. I am doing well and enjoying it. My question is this: The material says I can use "" Completing the square "" to solve any quadratic equation. I solved this equation 
$$6x^2+24x=0$$ 
by factoring (got answers $0$ and $-4$). But when I try and solve the same equation by completing the square I get a different answer ($\pm 12$). I must be doing something wrong but can't figure it out as I think I'm using the right steps to complete the square. Any help appreciated.",['algebra-precalculus']
2102138,Does a linear map preserving norm also approximately preserve inner products?,"Suppose I have distribution on linear map  $D_{\epsilon,\delta}$  over $\mathbb{R}^{d\times d}$ such that for any $0<\epsilon,\delta<1/2$ we have $\forall x$,
$$\Pr_{A \sim D_{\epsilon,\delta}}(\lvert \lVert Ax\rVert^2- \lVert x\rVert^2\rvert>\epsilon \lVert x\rVert^2)<\delta$$
Now given this can I say that $A$ drawn from $D_{\epsilon,\delta}$ also preserves inner product in the following sense
$$\Pr_{A \sim D_{\epsilon,\delta}}(\left\lvert \langle Ax_1,Ax_2\rangle-\langle x_1,x_2\rangle\right\rvert>\epsilon\lVert x_1\rVert\lVert x_2\rVert)<\delta$$
I tried using polarization identity to get the result, I have following thing so far
\begin{align*} 
\langle Ax_1,Ax_2\rangle&=\frac{1}{4}(\lVert A(x_1+x_2)\rVert^2-A(x_1-x_2)\rVert^2) \\ 
&\le\frac{1}{4}((1+\epsilon)\lVert x_1+x_2\rVert^2-(1-\epsilon)\lVert x_1+x_2\rVert^2)\\
&=\langle x_1,x_2 \rangle+\frac{\epsilon}{2}(\lVert x_1\rVert^2+\lVert x_2\rVert^2)
\end{align*}
Now I am not able to go further as  $\lVert x_1\rVert^2+\lVert x_2\rVert^2\geq 2\lVert x_1\rVert\lVert x_2\rVert$.
Any help, comments, hints are greatly appreciated. Thanks.","['functional-analysis', 'probability-theory', 'hilbert-spaces']"
2102150,Orthogonal group acts on symmetric matrices,"Let $O_n(\mathbb{R})$ act on $Sym_n(\mathbb{R})$, the symmetric matrices with real entries, via $S \mapsto A^T SA$ for $A \in O_2(\mathbb{R})$ and $Sym_n(\mathbb{R})$. What is the space of orbits $O_n(\mathbb{R})/Sym_n(\mathbb{R})$ as a set (and what is a basis for the topology)? I know that we can diagonalize a symmetric matrix $A$ with $Q\in O_n(\mathbb{R})$ such that $QSQ^{-1}$ is diagnonal but I don't know how to continue. Thanks a lot for your help!","['general-topology', 'linear-algebra']"
2102155,How can we characterize polynomials in $\mathbb{R}^2$ that are harmonic,"How can we characterize polynomials $p(x,y)$ in  $\mathbb{R}^2$ ( in two variables ) that are harmonic (that is $\Delta p(x,y) = 0$)?","['real-analysis', 'polynomials', 'partial-differential-equations', 'calculus', 'complex-analysis']"
2102195,Picking zero out of zero elements,"How can this expression$$\binom{0}{0}=1$$
be logical? I do not know if this is good example: Probability of selecting one blue candy from a jar, which contains 5 green candies. $$\frac{\binom{0}{0}}{\binom{5}{1}}=\frac{1}{5}$$","['combinatorics', 'binomial-coefficients']"
2102240,Proof concerning eigenvalues,"I have the following problem which I don't know how to solve, any help is appreciated. Let $A,B$ and $C$ be $n \times n$ matrices. Suppose that $B$ and $C$ are symmetric. Consider the matrix
  $$
M = \begin{bmatrix} A & B \\ C & -A^T \end{bmatrix}
$$ Show that if $\lambda$ is an eigenvalue of $M$ then so is $-\lambda$. My idea: I know that for $\lambda$ to be an eigenvalue of $A$, $det(A-λI)$ has to be zero, then you can work out this determinant and get the eigenvalues. However I don't know if you need to solve this matrix like that since its entries aren't numbers but matrices. Besides that, I'm quite sure this isn't the way to tackle this problem since you don't need the values of the eigenvectors but you just have to show that if $\lambda$ is an eigenvalue of M then so is $-\lambda$. Thanks in advance :)","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2102241,Why is $\sum^\infty_{n=0}\sum^n_{k=0}\frac{b^k}{k!}\cdot\frac{a^{n-k}}{(n-k)!}=\sum^\infty_{n=0}\frac{(a+b)^n}{n!}$?,"The tutor of my algorithms class used the following equation today: $\sum\limits^\infty_{n=0} \sum\limits^n_{k=0} \frac {b^k} {k!} \cdot \frac {a^{n-k}} {(n-k)!}$ = $\sum\limits^\infty_{n=0} \frac {{(a+b)}^n} {n!}$ When I asked him why we are allowed to use this he just told me that you usually prove it in one of your first maths lectures. However, I am still too stupid to see why this should be true. A hint would be really nice. I don't know if that is misleading but after having some thoughts about the equation I figured that the part $\sum\limits^n_{k=0} \frac {b^k} {k!} \cdot \frac {a^{n-k}} {(n-k)!}$ is a Cauchy-Product. After finding that out I wrote the first terms of the sum on a piece of paper and always saw that they look pretty similar to the binomial theorem. But I can't find a way to do it for an infinite amount of terms. Just for the given ones I wrote down.","['binomial-theorem', 'binomial-coefficients', 'sequences-and-series']"
2102242,Spivak's proof of Inverse Function Theorem.,"This question concerns the Inverse Function Theorem done in the book. Spivak states that $$|D_jf^i(x) - D_jf^i(a)| < 1/2n^2$$ for all $i,j$ and $x \in U$ Here $U$ is a closed rectangle set, $n$ is an integer. The lemma 2.10 (on the previous page) states that Let $A \subset \mathbb{R}^n$ be a rectangle and let $f: A \to \mathbb{R}^n$ be continuously differentiable. If there is a number M such that $|D_jf^i(x)| \leq M$ for all $x$ in the interior of $A$, then $$|f(x) - f(y)| \leq n^2 M|x - y|$$ He define $g(x) := f(x) - x$ and states that, for $x_1, x_2 \in U$, we have $|f(x_1) - x _1 -(f(x_2) - x_2)| \leq \frac{1}{2} |x_1 - x_2|$ and it's ok for me, because $|D_jg^i(x) - D_jg^i(a)| \leq \frac{1}{2n^2} n^2 |x_1 - x_2| = \frac{1}{2} |x_1 - x_2|$ by the lemma, but we can do this only if $|D_jg^i(x)| \leq M$, my doubt is why is this valid?","['multivariable-calculus', 'calculus', 'proof-explanation']"
2102252,Definition of a subbasis for a topology,"Definition (Subbasis): A subbasis , $\mathcal{S}$ , for a topology on $X$ , is a collection of subsets of $X$ , whose union equals $X$ . $$\mathcal{S} = \left\{S_{\alpha} \subset X \ \middle| \ \bigcup_{\alpha} S_{\alpha} = X \right\}$$ This is the definition of a subbasis that I've taken from Munkres: Topology - A First Course . But by this definition if $X$ is a set, then a subbasis for a topology on $X$ could be $\mathcal{S} = \{X\}$ (as $X \subset X$ for any $X$ ). But then then the basis $\mathcal{B}$ generated by this subbasis, would be $\mathcal{B} = \{X\}= \mathcal{S}$ , as the only finite intersection of elements of $\mathcal{S}$ is $X\cap X = X$ . But then we have for the topology $\mathcal{T}$ generated by this basis $\mathcal{B}$ , $$\mathcal{T} = \{X\}$$ as the only possible union of basis elements is $X$ . But $\mathcal{T}$ cannot be a topology as $\emptyset \not\in \mathcal{T}$ , reaching a contradiction. Have I made a mistake somewhere? If not then how can this definition of a subbasis be correct?","['general-topology', 'elementary-set-theory', 'definition']"
2102277,Solve trigonometric equation $ \cos x - x \sin x=0 $,"I've been studying trigonometric functions lately and there's one problem I didn't manage to solve. It states that f is a function defined as following:
$$ f : \mathbb R_+\to \mathbb R $$
$$     x  \mapsto x\cos x $$
I'm supposed to find the smallest extremum of that function, therefore I calculated the derivative which is:
$$ f'(x) = \cos x - x\sin x $$
So, in order to find the extremum, we know that we should solve for x the equation f'(x)=0 the thing that I was unable to do. In an attempt to simplify that equation, I obtained $ \cos x= \frac{x}{\sqrt{x^2+1}} $ which just got the things more complicated. Using a graph calculator, I found out that the point I'm looking for had the coordinates (0.8603, 0.6522) but I still didn't know which path is to be taken in order to get to that result. Can you please help me? And it would be awesome if you give me a general method of solving these kinds of equations.",['trigonometry']
2102307,Is there any continuous function that satisfies the following conditions?,"Is there any continuous function  $y=f(x)$ on $[0,1]$ that satisfies the following conditions?
$$
f(0) = f(1) = 0,
$$
and
$$ 
f(a)^2-f(2a)f(b)+f(b)^2<0,
$$
for some $0<a<b<1$. I tried to test with several functions (with different $a,b$) but non of them satisfied. Any help is appreciated. Thank you in advance.",['calculus']
2102359,Understanding Theorem 19.2 in Munkres,"Theorem 19.2 in Munkres Suppose the topology on each space $X_{\alpha}$ is given by a basis $\mathcal{B}_{\alpha}$. The collection of all sets of the form $$\prod_{\alpha \in J} B_{\alpha}$$ where $B_{\alpha} \in \mathcal{B}_{\alpha}$ for each ${\alpha}$, will serve as a basis for the box topology on $\prod_{\alpha \in J} X_{\alpha}$ The problem I'm having with understanding the theorem is due to the use of the indices. From what I understand the above theorem is saying that the collection of all possible sets formed by: Taking one arbitrary basis element from each arbitrary basis $\mathcal{B}_{\alpha}$, and then taking the product of each of those basis elements forms a basis for the box topology $\prod_{\alpha \in J} X_{\alpha}$. Is my understanding correct? 
I apologize in advance if this question is somewhat vague.","['general-topology', 'elementary-set-theory']"
2102366,Composition of holomorphic functions and order of zeros,"Consider the holomorphic functions $f_1 \in H(\Omega_1)$ and $f_2 \in H(\Omega_2)$, where $\Omega_1, \Omega_2 \subset \mathbb{C}$ and $f_1(\Omega_1) \subset \Omega_2$. Let $$g = f_2 \circ f_1.$$ Consider $z_0 \in \Omega_1$ and $z'_0 = f_1(z_0) \in \Omega_2$. Suppose $z'_0$ is a zero of order $k$ for $f_2$. Is $z_0$ a zero of order $k$ for $g$? If not what is it? Do we need to deal with the case where the derivatives of $f_1$ vanish separately?","['complex-analysis', 'complex-numbers']"
2102379,Construct a large set such that any k+l elements span the vectorspace,"Let $\mathbb{F}_q^k$ be a  $k$-dimensional vectorspace over a finite field, and let $l \ge 0$ be an integer. The question is how to construct a (maximally) large set $A \subset \mathbb{F}_q^k$ such that if you pick any $k+l$ distinct elements from $A$, then these points span $\mathbb{F}_q^k$. This question is a more general version of my previous question , which handled the case $l=0$. In more geometric terms the question sounds like this: Construct a large set of points in $\mathbb{P}^{k-1}(\mathbb{F}_q)$ such that no $k+l$ lie on the same hyperplane. In the answer to my previous question it was pointed out that for the case $l=0$ these objects are called $\mathcal{K}$-arcs and they are well known. I believe for $l>0$ these objects are called $(\mathcal{K},k+l)$-arcs, but I could not find any construction for them.","['finite-fields', 'combinatorics', 'finite-geometry', 'vector-spaces']"
2102461,Proof of basis for a box topology,"Theorem: Suppose the topology on each space $X_{\alpha}$ is given by a basis $\mathcal{B}_{\alpha}$. The collection of all sets of the form $$\prod_{\alpha \in J} B_{\alpha}$$ where $B_{\alpha} \in \mathcal{B}_{\alpha}$ for each ${\alpha}$, forms a basis for the box topology on $\prod_{\alpha \in J} X_{\alpha}$ My Attempted Proof The topological space we are dealing with here is $\left(\prod_{\alpha \in J} X_{\alpha}, \mathcal{T}_B\right)$, where $\mathcal{T}_B$ is the box topology on  $\prod_{\alpha \in J} X_{\alpha}$. Let $\mathcal{B}_{\text{box}}$ denote the standard basis for the box topology. Put $$\mathcal{F} = \left\{ \ \prod_{\alpha \in H} B_{\alpha} \ \middle| \ B_{\alpha} \in \mathcal{B}_{\alpha} \ \text{for each $\alpha$} \ \right\}.$$ Then take $U \in \mathcal{T}_B$, and take $x \in U$. Since $U = \bigcup_{ B \in \mathcal{K}}B$ where $B = \prod_{\alpha \in J}U_{\alpha}$ and $U_{\alpha}$ is open in $X_{\alpha}$ and $\mathcal{K} \subset \mathcal{B}_{\text{box}}$ we then have $x \in B$ for some $B \in \mathcal{K}$. In other words $x \in \prod_{\alpha \in J}U_{\alpha}$ for some indexing set $J$. Now since each $U_{\alpha}$ is open in $X_{\alpha}$, we have $U_{\alpha} = \bigcup_{\gamma \in I}B_{\gamma}$ where $B_{\gamma}$ are basis elements of $X_{\alpha}$. Therefore $$x \in \prod_{\alpha \in J}\left(\bigcup_{\gamma \in I}B_{\gamma}\right)_{\alpha}$$ By elementary set theory we have $$\left(\bigcup_{i \in I} V_i\right) \times \left(\bigcup_{j \in J} W_j\right) = \bigcup_{\langle i, j \rangle I \times J}\left(V_i \times W_j\right)$$ So that $$\prod_{\alpha \in J}\left(\bigcup_{\gamma}B_{\gamma}\right)_{\alpha} \  = \  \bigcup_{\langle i_1, i_2, ... \rangle \in I_1 \times I_2 \times ...} \underbrace{\left(B_{i_1} \times B_{i_2} \times  ...\right)}_{|J| \ \text{times}}$$ Choose $H = I_1 \times I_2 \times I_3 \times ...$ Then $\exists \prod_{\alpha \in H}B_{\alpha} \in \mathcal{F}$ such that $x \in \prod_{\alpha \in H}B_{\alpha}$. It also follows that $$\prod_{\alpha \in J}\left(\bigcup_{\gamma}B_{\gamma}\right)_{\alpha} \subset U$$ Since $\prod_{\alpha \in J}\left(\bigcup_{\gamma}B_{\gamma}\right)_{\alpha} = \prod_{\alpha \in J}U_{\alpha} = B \subset U$. Thus it follows that $\mathcal{F}$ is a basis for the box topology on $\prod_{\alpha \in J} X_{\alpha}$. $\square$ Is my above proof correct and rigorous? Are there areas of it that I can improve on? Can this be proved in an easier way? I apologize in advance if the proof is long or messy (especially with the indices) If my proof is incorrect however, or if it is not rigorous please let me know!","['alternative-proof', 'proof-verification', 'proof-writing', 'elementary-set-theory', 'general-topology']"
2102463,Equivalence of Rudin's Definition 4.33 (infinite limits and limits at infinity)?,"In Rudin's ""Principles of Mathematical Analysis"", Definition 4.33 (p98) states: Definition 4.33 $\quad$ Let $f$ be a real function defined on $E.$  We say that $f(t)\to A$ as $t\to x$, where $A$ and $x$ are in the extended real number system, if for every neighborhood $U$ of $A$ there is a neighborhood $V$ of $x$ such that $V \cap E$ is not empty, and such that $f(t)\in U$ for all $t\in V\cap E, t\ne x.$ I'm a little confused however by the remark right beneath this definition: A moment's consideration will show that this coincides with Definition 4.1 when $A$ and $x$ are real. My question is actually three-fold: 1) In Definition 4.1 (quoted below), it is explicitly stated that $x$ ($p$) has to be a limit point of $E$.  Can this be inferred from Definition 4.33?  It appears to me that according to Definition 4.33 the limit of a function can also be defined even if $x$ is an isolated point of $E.$  For example, let $E=\{0\}\cup \{1\} \cup [3,4]$, and $f(0)=f(1)=0, f(t)=t, t\in [3,4]$.  Then with $V$ being a neighborhood of radius $2$ around $0$, we have $f(t)\to 0$, as $t\to 0,$ don't we? 2) Similarly, for limits at infinity, i.e. $x=\infty$, Definition 4.33 doesn't require the domain $E$ of $f$ to be unbounded, does it?  For example, if $f(x)=1$ for $x\in[0,1]$, then we may also say $f(x)\to 1$ as $x\to \infty$? (e.g. with $V=(0.5, \infty)$ we clearly satisfy Definition 4.33.) 3) Should we or should we not add these assumptions (or implicitly assume them) when using Definition 4.33?  (i.e. $x$ has to be limit point of $E$, or $E$ is unbounded above or below.) Thanks a lot! Definition 4.1 $\quad$ Let $X$ and $Y$ be metric space; suppose $E \subset X$, $f$ maps $E$ into $Y$, and $p$ is a limit point of $E$.  We write $f(x)\to q$ as $x\to p$, or $\lim_{x\to p}f(x)=q$ if there is a point $q \in Y$ with the following property: For every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),q)<\epsilon$ for all points $x\in E$ for which $0<d_X(x,p)<\delta$.","['real-analysis', 'analysis']"
2102470,How can two matrices cancel each other out when there is a matrix in between?,"I'm watching videos of Gilbert Strang's linear algebra lectures. In lecture 17, where he goes over orthonormal bases and the Gram-Schmidt process, he proves $$ A^TB = A^T\left( b - \frac{A^Tb} {(A^TA)} A\right) = 0 $$ by having $$ A^T \frac{A^Tb} {(A^TA)} A $$ cancel out into A T b. I don't know how the A T A on the top was allowed to cancel out with the A T A on the bottom if there is an A T b sandwiched in between the A T and the A on the numerator. Shouldn't the A T and the A on the numerator not be allowed to multiply each other? I wondered if the A T on the numerator can cancel out with the A T on the denominator and the same for the two A's, but I do not know if this violates the order of operations for matrix multiplication. If I were to write (A T A) -1 rather than have the (A T A) on the denominator below the (A T b), where would it go?","['matrices', 'projection-matrices', 'gram-schmidt', 'linear-algebra']"
2102490,What do you call the property where a set is closed under an operation infinitely many times?,"My understanding of the closure of a set $S$ under an operation $\oplus$ is that applying $\oplus$ to elements of $S$ yields only other elements of $S$. However, in my undergraduate topology class we recently came across the concept that, although a given topology $\mathcal{T}$ is closed under union and intersection, while the union of infinitely many elements of $\mathcal{T}$ is necessarily a member of $\mathcal{T}$, the intersection is not. Is there a concise way to express this? (i.e., "" $\mathcal{T}$ is infinitely closed under the operation $\cup$"", or something like that)","['general-topology', 'binary-operations', 'elementary-set-theory']"
2102494,Naturality of tensors in Differential Geometry,"I have always had a hard time understanding the big picture of tensors and tensor fields . I have no problem understanding why low type tensors and tensor fields such as
\begin{align}
\text{$scalars$ and $smooth$ $functions$ --- type $(0,0)$,}\\
\text{$vectors$ and $vector$ $fields$ --- type $(1,0)$,}\\
\text{$covectors$ and $differential$ $forms$ --- type $(0,1)$,}\\
\text{$linear$ $transformations$ and $vector$-$field$ $morphisms$ --- type $(1,1)$,}\\
\text{$inner$ $products$ and $Riemannian$ $metrics$ --- type $(0,2)$ }
\end{align}
are so useful and natural. As these objects all share a lot of algebraic structure I understand the reason to encapsulate them within the notion of tensor, in an algebraic context. But, from an Analysis-Geometry setting, they are way different objects, so I see no natural reason to join together all these objects and expect the resulting object (tensors) to be of so much use in Differential Geometry. Yet they are, and they are everywhere!! Q: Am I missing some reason of Analytic-Geometric character which motivates this generalization? If not, how does it turn out that an object whose generalization seem to be natural only algebraically end up playing such a central role in Differential Geometry? Maybe I am just underestimating the role that the algebraic structure play in this context?","['tensors', 'differential-geometry', 'soft-question', 'motivation']"
2102510,Why the limit does not exist,Why the limit of this greatest integer function doesn't exist: $$\lim_{x\to0}[x]$$ My attempt: L.H.L.= $$\lim _{x\to0^-}[x]$$ $$=\lim_{h\to0}[0-h]$$ $$=\lim_{h\to0}[0-0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$ R.H.L.= $$\lim _{x\to0^+}[x]$$ $$=\lim_{h\to0}[0+h]$$ $$=\lim_{h\to0}[0+0]$$ $$=\lim_{h\to0}[0]$$ $$=0$$,['limits']
2102517,How to draw a regular pentagon with compass and straightedge,"I remember reading that Gauss managed to construct a regular pentagon with just a compass and straightedge, but I don't remember the particulars of how he did this. Could someone help me out and give me instructions on how to do this?","['geometric-construction', 'polygons', 'geometry']"
2102542,Does this Limit approach $1$ or $\frac{1}{2}$,"Evaluate $$L=\lim _{x \to 0}\left(\frac{e^x}{e^x-1}-\frac{1}{x}\right)$$ I tried in two methods Method $1$. $$L=\lim_{x \to 0}\left(\frac{\frac{e^x}{x}}{\frac{e^x-1}{x}}-\frac{1}{x}\right)$$ But $$\lim_{x \to 0}\frac{e^x-1}{x}=1$$ So $$L=\lim_{x \to 0}\left(\frac{e^x}{x}-\frac{1}{x}\right)=\lim_{x \to 0}\frac{e^x-1}{x}=1$$ Method $2$.  By taking LCM we get $$L=\lim_{x \to 0}\frac{xe^x-e^x+1}{xe^x-x}$$ Since the limit is in $\frac{0}{0}$ form using L'Hopital's Rule twice we get $$L=\lim_{x \to 0}\frac{xe^x}{xe^x+e^x-1}$$ i.e., $$L=\lim_{x \to 0}\frac{xe^x+e^x}{xe^x+2e^x}=\frac{1}{2}$$ I am sure that second method is correct, but i want to know whats is wrong in first method","['algebra-precalculus', 'derivatives', 'limits']"
2102582,The general solution of $y''=-\sin y$,"When I asked Mathematica to solve the ODE
$$y''=-\sin y \tag{1} $$
I got the solutions
$$y=\pm 2 \text{am}\left(\frac{1}{2} \sqrt{\left(c_1+2\right) \left(t+c_2\right){}^2}|\frac{4}{c_1+2}\right), \tag{2} $$
where $\text{am}(u|m)$ is the Jacobi Amplitude function . I wonder why there is a $\pm$ ambiguity here (I believe it's related to the square root), since the equation $(1)$ is explicit. P.S. Using the rules $c_1 \mapsto -2+4c_1^2,c_2 \mapsto c_2/c_1$ one gets the equivalent form
$$y= \pm2 \text{am}\left(\sqrt{\left(t c_1+c_2\right){}^2}|\frac{1}{c_1^2}\right)$$
and if one cancels the square root with the square, noticing that am is odd in the first argument one gets an even nicer form
$$y=\pm2 \text{am}\left(c_1t +c_2|\frac{1}{c_1^2}\right).$$
However, my question still remains: Why is there an ugly $\pm$ in the solution if the ODE is written explicitly in the form $y''=f(x,y,y')$? Is there a nicer form for the general solution? Thank you!","['ordinary-differential-equations', 'elliptic-functions']"
2102637,How does this converging function appear to diverge?,"I found something while trying to analyze the integral $\displaystyle\int_0^\infty \frac{\sin(mx)} {x}\mathrm  dx $ Let us suppose we already know that the value of the integral is $\pi/2$ when $m>0$, $0$ when $m=0$ and $-\pi/2$ when $m<0$. Let $f(m)=\displaystyle\int_0^\infty \dfrac{\sin(mx)} {x} \mathrm dx$ Thus $f'(m)=\displaystyle\dfrac{\mathrm d}{\mathrm dm}\int_0^\infty \frac{\sin(mx)} {x} \mathrm dx=\int_0^\infty \frac{\partial}{\partial m}\dfrac{\sin(mx)} {x}\mathrm  dx=\int_0^\infty \cos(mx)\mathrm  dx=\lim\limits_{x \to \infty} \frac{\sin(mx)}{m}$, which does not converge. However, $f'(m)$ should be $0$ where $m\neq0$, as value of the integral remains constant So we get $0= \lim\limits_{x \to \infty} \dfrac{\sin(mx)}{m}$, where $m\neq0$ How does this happen? Please note that I am a high school student and do not know complex analysis. Can we assign value $0$ to $\lim\limits_{x \to \infty}{\sin(x)}$ as the average value of Sine function remains zero even when $x$ approaches infinity?","['definite-integrals', 'convergence-divergence', 'calculus']"
2102688,Existence of $\alpha\in \Bbb C$ such that $f(\alpha)=f(\alpha+1)=0$.,Let $f(x)\in \Bbb Q[x]$ be an irreducible polynomial over $\Bbb Q$ . Show that there exists no complex number $\alpha$ such that $f(\alpha)=f(\alpha+1)=0$ . Following @quasi; Let $f(x)=a_0+a_1x+a_2x^2+\dots +a_nx^n$ . Define $g(x)=f(x+1)-f(x)$ Then $g(\alpha)=0$ .Also $g(\bar \alpha)=0\implies g(x)=(x-\alpha)^a(x-\bar \alpha)^b h(x);h(x)\in \Bbb C[x]$ . What to do now?Please help.,"['irreducible-polynomials', 'abstract-algebra', 'proof-verification']"
2102701,Pole of the function $f(z)=\frac{z}{(1-e^z).\sin z}.$,"Let , $f(z)$ be a meromorphic function given by $$f(z)=\frac{z}{(1-e^z).\sin z}.$$Prove or disprove that "" $z=0$ is a pole of order $2$ "". 1st Argument : Firstly we can rewrite the given function as $\displaystyle f(z)=\frac{z}{\sin z}.\frac{1}{1-e^z}$. Now $\displaystyle \frac{z}{\sin z}$ has a removable singularity at $z=0$. Also , $\displaystyle \frac{1}{1-e^z}$ has a pole of order $1$ at $z=0$. So finally the function has a pole of order $1$ at $z=0$. 2nd Argument : But we know that  "" A function $f$ has a pole of order $m$ if and only if $(z-a)^mf(z)$ has a removable singularity at $z=a$."" So here , if we can show that $\displaystyle z^2f(z)=\frac{z^3}{(1-e^z).\sin z}=g(z)(say)$ has a removable singularity at $z=0$ then we can show that $f$ has a pole of order $2$ at $z=0$. Now , as $\displaystyle \lim_{z\to 0}z.g(z)=0$ , so $g$ has a removable singularity  at $z=0$ and consequently $f$ has a pole of order $2$ at $z=0$. I'm confused that which is correct ? I could not find any mistake in both of my arguments. Can anyone detect the fallacy ?","['complex-analysis', 'complex-numbers', 'meromorphic-functions']"
2102702,Intuitive explanantion for random vectors and estimation theory,"Suppose $X$ is a random vector and $X \sim f_\theta(x)$. I have a question about random vectors. Q1. Why do we consider each entry of a random vector to be $X_i \sim f_\theta(x_i)$? I think I can visualise it as being a vector in space, so its exact position is random. Each of its coordinates will be randomly chosen. Each entry has a probability distribution. Is this the correct intuition? I think I am getting confused by the use of $f_\theta(x_i)$, so can someone explain? Q2. Further, is there some intuition behind why joint probability density becomes the product when each entry is independent? Q3. Why is each observation considered to be an entry of a random vector during parameter estimation? In this case, why can't each entry be drawn simply from a distribution $X_i \sim f_\theta(x_i)$ which is exactly the same for all $i$? Why do we assume that the random variable is different for different measurements?","['statistics', 'statistical-inference', 'random-variables']"
2102718,Volume and area form for the circle and sphere using pullback and integration.,"I was trying to compute the area of the sphere using calculus and my knowledge of differential form as follow : Consider the two form $\omega = dx \wedge dy$, we want to use this form to find out the area of a disk. We define a parametrization of the sphere as follows $F(r,\phi) \rightarrow (rcos\phi, rsin\phi)$ So we have : \begin{align}
\int_{\mathbb{S}^2} \omega = \int_{[0,R]}\int_{[0,2\pi]} F^* \omega = \\
\int_{[0,R]}\int_{[0,2\pi]}rdr \wedge d\phi = \\
\int_{[0,R]}\int_{[0,2\pi]} rdrd\phi = \pi R^2
\end{align} I was hoping to apply the same principle to find the circumference of the circle but I think I ultimately miss some technicalities. The circumference of the circle is a 1-dimensional manifold so I am trying to define a one-form on it.I do not know how to proceed from here. Do I have to find a 1-d parametrization of the circle ? Is the one form $dx$ the one to integrate ? I'm trying to compute the circumference of the circle using pullback an forms. So I can understand the mechanics of integrating forms to find volumes ( and hopefully move to more exotic manifolds like the area or volume of a torus ). EDIT : 
My reasoning is as follows. To find the circumference of a circle, let's define a mapping $F:\mathbb{R} \rightarrow \mathbb{R}$ that parametrize it. I was thinking of defining $F:[0,\frac{\pi}{2}] \rightarrow \mathbb{R}$ and multiply the result by 4 ( since the mapping parametrize a quarter of the circle ). Then proceed to integrate $\int_{[0,\frac{\pi}{2}]} F^*(dx)$ Unfortunately the few mapping I tried fail to provide me the right answer. EDIT2 : Using stereo coordinates (attempt) Let's consider the map $F:[0,\frac{\pi}{4}] \rightarrow \mathbb{R}$ the stereo projection of $\frac{1}{8}$ of the circumference of the circle,  defined as $F(\alpha)=Rtan(\alpha)$ Now we have $\int_{\mathbb{\frac{S^1}{8}}} dx = \int_{[0,\frac{\pi}{4}]} F^*(d\alpha) = R\frac{\pi}{4}$ which when multiplied by 8 gives us the result $2\pi R$","['differential-forms', 'integration', 'differential-geometry']"
