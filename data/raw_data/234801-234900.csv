question_id,title,body,tags
4907778,"Prove that $\lim_{(x,y) \to (0,0)}(\frac{xy}{x^2 + y^2})^{x^2}$ does not exist","Which sequences should one choose to effectively prove that $$\lim_{(x,y) \to (0,0)}\left(\frac{xy}{x^2 + y^2}\right)^{x^2}$$ does not exist? Obviously, we deal with $$\lim_{(x,y) \to (0,0)}x^2*ln\left(\frac{xy}{x^2+y^2}\right).$$ I tried to substitute $x=y=\frac{1}{n}$ and $x=\frac{1}{n}$ , $y=\frac{1}{n^2}$ ( $n$ approaches infinity), but in both cases $x^2$ seems to drag the whole thing to zero?
Thanks!",['limits']
4907841,"Why is $\int_0^\infty(\frac{1}{1+x^a}-\frac{1}{1+x^b})\frac{dx}{x}=0$ for all a,b>0?","I found an equation similar to the one in the question in a table of integrals, and WolframAlpha confirms that the equation holds for all values I've tried, but I haven't been able to prove it. I tried using the substitution $u=x^a,\frac{du}{au}=\frac{dx}{x}$ on the divergent integral $$\int_0^\infty\frac{dx}{(1+x^a)x},$$ which gives $$\int_0^\infty\frac{du}{(1+u)au},$$ but this implies that $$\int_0^\infty \left(\frac{1}{1+x^a}-\frac{1}{1+x^b}\right)\frac{dx}{x}=\left(\frac{1}{a}-\frac{1}{b}\right)\int_0^\infty\frac{du}{(1+u)u}$$ for all a,b, which is not in general $0$ .","['integration', 'improper-integrals', 'definite-integrals']"
4907883,Population size change with equally probable growth and decline,"There is a population that is equally likely to grow by $10\%$ or decrease by $10\%$ every day. What can we say about it in a year's time: will it be bigger, smaller, or the same as it was in the beginning? And in one year and one day? Let the population size initially be equal to $x$ . Then after first day we have in average: $x\cdot \frac{1}{2} \cdot (1+0.1+1-0.1) = x$ , which corresponds with initial value. After second day average population size is equal: $x\cdot \frac{1}{4} \cdot ((1+0.1)^2+2\cdot (1+0.1)\cdot (1-0.1) + (1-0.1)^2) = x$ Third step gives: $\frac{1}{8} \cdot x \cdot((1+0.1)^3+3\cdot((1+0.1)^2) \cdot (1-0.1)+3 \cdot ((1-0.1)^2) \cdot (1+0.1)+(1-0.1)^3) = x$ However, when deriving the last formula, I separated the intermediate states up-down from down-up, etc, and final states such as up-up-down from up-down-up and down-up-up, etc. If this was not done, I received a different answer: the population was not stationary: $\frac{1}{6} \cdot x \cdot((1+0.1)^3+2\cdot((1+0.1)^2) \cdot (1-0.1)+2 \cdot ((1-0.1)^2) \cdot (1+0.1)+(1-0.1)^3) > x$ How is it correct to calculate the number of paths here: taking into account the order or without taking order into account? And is it true, if the order is taken into account, that I will always get the binomial formula and in the end the answer to the problem will be ""The population does not change the number on average""? Thank you","['calculus', 'binomial-coefficients', 'probability']"
4907886,Walters Ergodic Theory Theorem 2.4: Is this condition necessary?,"Theorem 2.4. of Walters' An Introduction to Ergodic Theory reads as follows: Let $(X_1,\mathbb{B}_1,\mu_1),(X_2,\mathbb{B}_2,\mu_2)$ be probability spaces. Suppose that $V:L^2(m_2)\to L^2(m_1)$ is a bijective linear map such that: $(Vf,Vg)=(f,g)$ for all $f,g\in L^2(m_2)$ . $V,V^{-1}$ map bounded functions to bounded functions. $V(fg)=(Vf)(Vg)$ if $f,g$ are bounded. Then $V$ is induced by an isomorphism $\Phi:(\overline{\mathbb{B}}_2,\overline{m}_2)\to(\overline{\mathbb{B}}_1,\overline{m}_1)$ of measure algebras in the sense that $V_{\chi_\overline{B}}=\chi_{\Phi(\overline{B})}$ for all $\overline{B}\in\overline{\mathbb{B}_2}$ . My question is, is the condition 2 really necessary for the theorem to be true? I am not sure where in the proof this condition is used. At some point he uses the bounded convergence theorem, but it does not seem that condition 2 is necessary for that as all the functions involved are characteristic functions, so they are automatically bounded by $1$ .","['measure-theory', 'ergodic-theory']"
4907912,A seeming example of a group whose subgroup lattice is lower semimodular but not consistent: where's my error?,"Corollary 5.3.12 in Schmidt's ""subgroup lattices of groups"" states that if groups $A,B$ have lower semimodular subgroup lattices, then so does their direct product $A \times B$ . This paper examines groups whose subgroup lattices are consistent , ie. for any two subgroups $A,B \le G$ , if $A$ is join-irreducible in $G$ then $\left<A \cup B\right>$ is join-irreducible in the interval lattice $[B,G]$ .The authors claim that every lower semimodular subgroup lattice must be consistent. Here's my problem, though: both $S_3$ and $C_2$ have lower semimodular (in fact: even modular), subgroup lattices, and lower semimodularity does indeed seem to hold for $S_3 \times C_2$ , which is isomorphic with the dihedral group of order 12 . However, in this diagram , and every other diagram I was able to find, $\left<s\right>$ and $\left<sr\right>$ are both join-irreducible (in fact: even atoms), but $\left<s,sr\right>=D_{12}$ is join irreducible neither in $[\left<s\right>,D_{12}]$ nor $[\left<sr\right>,D_{12}]$ . Hence $D_{12}$ doesn't seem to be consistent. I seriously doubt any of the linked sources contain factual errors, so... where's my mistake?","['group-theory', 'finite-groups', 'lattice-orders']"
4907984,"Let $G$ be a graph with the property: for any odd cycles $C_1, C_2$ of graph $G$, it holds that $V(C_1) \cap V(C_2) \neq \emptyset$.","Let $G$ be a graph with the property: for any odd cycles $C_1, C_2$ of graph $G$ , it holds that $V(C_1) \cap V(C_2) \neq \emptyset$ . (a) Let $C$ be an odd cycle in graph $G$ . Prove that $\chi(G - V(C)) \leq 2$ . (b) Prove that $\chi(G) \leq 5$ . Attempt: (a) To prove that $\chi(G - V(C)) \leq 2$ , we need to show that the chromatic number of the graph resulting from removing the vertices of any odd cycle $C$ from $G$ is at most 2. Given $C$ , let $G' = G - V(C)$ be the graph obtained by removing the vertices of $C$ from $G$ . We aim to show that $\chi(G') \leq 2$ . Since $C$ is an odd cycle, $G'$ remains connected because the removal of $V(C)$ cannot disconnect $G$ due to the given property that any pair of odd cycles in $G$ share at least one vertex. Now, I consider a maximal path $P$ in $G'$ . Since $P$ is a path, its endpoints are of degree 1 in $G'$ . Now I suppose $P$ has length $k$ . Since $P$ is maximal, the neighbors of the endpoints of $P$ must all lie on $P$ . Because $P$ is a path, each of its vertices has at most two neighbors on $P$ . Have I started o. k.? Is this the right path? (b) To prove that $\chi(G) \leq 5$ , where $\chi(G)$ denotes the chromatic number of the graph $G$ , I use a proof by contradiction. I assume, for the sake of contradiction, that $\chi(G) > 5$ . This implies that there exists a coloring of $G$ using at least $6$ colors. Let $c: V(G) \rightarrow {1, 2, 3, 4, 5, 6}$ be such a coloring. Now I consider the induced subgraph $G_i$ of $G$ consisting of all vertices colored with color $i$ , for $i = 1, 2, 3, 4, 5, 6$ . Since $\chi(G) > 5$ , at least one of these induced subgraphs, say $G_6$ , must be non-empty. Since $G_6$ is non-empty, it must contain at least one vertex, say $v$ . Now, consider the neighborhood of $v$ in $G$ , denoted as $N(v)$ . Since $G_6$ is an induced subgraph, all vertices in $N(v)$ must be colored with colors other than $6$ . I have the same questions as above ... And help would be appreciated.","['graph-theory', 'coloring', 'discrete-mathematics']"
4908054,General Formula for $\sin^n(x)+\cos^n(x)$,"I wanted to derive an expression for $\sin^n(x)+\cos^n(x)$ and thought I should start of by deriving a few basic powers myself, and finding a pattern.
Apart from our usual $\sin^2(x)+\cos^2(x)=1$ , I derived the following myself, Subsequently, I attempted to find a general form for odd numbers. $$\sin^{2n-1}+\cos^{2n-1}=(1-\sum_{i=1}^{n-1} (\sin(x)\cos(x))^i),n\in\mathbb{N},n\neq1$$ And even powers, by induction I had assumed the general form was: $$\sin^n(x)+\cos^n(x)=1-\frac{n}{2}\sin^2(x)\cos^2(x),n\in\mathbb{N}$$ But, on checking this on Geogebra, I found these only hold for $n\in{2,3}$ Does anyone know a general formula for a sum of powers of sine and cosine?
I've been unable to find a clear pattern.",['trigonometry']
4908058,Derive the use of the Jacobi Amplitude function with the nonlinear pendulum diffequation,"I've been playing around with the pendulum differential equation ${\theta}''+\frac{g}{L}\sin{\theta}=0$ , and have found many general solutions using the Jacobi Amplitude function $\text{am}(u,m)$ often multiplied with a trig function using the integral that evaluates period time. I'm somewhat of a beginner to the topic only being fluent in the first three courses of calculus as I am working on this for a physics project. All of the derivation videos I've watched make sense, but always arrive at the idea that these equations are 'really hard to solve' and only reveal how to derive the period time. I've tried applying generalizing $\theta$ to a power series and trying to get a solution for the coefficiens of $\theta$ , $a_n$ , but I couldn't solve this because of the nonlinear term.
What I have so far is this: $$
\text{let } \theta(t)=\sum_{n\ge0} a_nt^n
\\\text{if }\theta(0)=\theta_0=a_0 \text{ and }\theta'(0)=0=a_1
\\\text{so }\theta(t)=\theta_0+a_2t^2+a_3t^3+...+a_nt^n=\theta_0+\sum_{n\ge2}a_nt^n
\\\text{and }\theta''(t)=2a_2+6a_3t+12a_4t^2+20a_5t^3+...+n(n-1)a_nt^{n-1}
\\\text{then substituting}\sum_{n\ge0}n(n-1)a_nt^{n-2}+\frac{g}{L}\sin({\theta_0+\sum_{n\ge2}a_nt^n})=0
$$ how can I go from here to find that the coefficients of $a_n$ match the coefficients of some elliptic function. Also let me know if the concept for 'finding' an elliptic function in a diffeq is misconstrued or if a better approach without generalizing to a power series exists. Let me know if this question isn't clear so I can help you help me ;). PS, this is my first stack exchange post, so please let me know if there are problems with my latex or other things, thanks","['physics', 'elliptic-functions', 'elliptic-integrals', 'ordinary-differential-equations']"
4908098,Solve $\Bigl(2 \sin(x)D^2+2 (\cos(x)+\sin(x))D\Bigr)y+2y\cos(x)=\cos(x)$,"How to solve the following differential equation? $$2 \sin(x)\frac{d^2 y}{dx^2}+2 \cos(x) \frac{dy}{dx}+2 \sin(x) \frac{dy}{dx}+2y \cos(x)=\cos(x)$$ This was given to me by one of my friends as a friendly challenge, but it appears that he himself doesn't know the solution. What we tried was various sorts of trigonometric solutions but that didn't work out. I think power series methods would be overkill. Even symbolic languages including Mathematica and mathdf can't find the solution to this problem. I also tried putting various ansatz based on a numerical solution but they all seem to result in inconsistencies, but this might be a very easy problem. How to approach this? Also, tips for attacking some problems are welcome. Apologies for my silliness, because I am new to math and just entered high school. Thanks a lot in advance.","['calculus', 'ordinary-differential-equations', 'substitution']"
4908111,Limit of integral of sum of cosine functions by CLT?,"I want to show that $$\lim_{n\to \infty} (2\pi)^{-d}n^{d/2}d^{-2n}\int_{[-\pi, \pi]^d} (\cos(x_1)+\cdots +\cos(x_d))^{2n} dx_1\cdots dx_d =2(d/4\pi)^{d/2}$$ holds. How do I prove this?
It seems that the central limit theorem can be used for this problem, but I don't know how to apply it. Or is there another good way to prove it?","['integration', 'central-limit-theorem', 'lebesgue-integral', 'real-analysis', 'probability-theory']"
4908114,Is the largest root of a random polynomial more likely to be real than complex?,"Posted on MO since it is unanswered in MSE It is known that the number of real roots of a random polynomial with real coefficients is much smaller than the number of complex roots. WLOG, assume that the coefficients are uniformly random in $(-1,1)$ for if not then we can divide each coefficient by the coefficient with the largest absolutely value to scale each coefficient to $(-1,1)$ . Then the number of real roots of a polynomial of degree $n$ is asymptotic to $\displaystyle \frac{2\log n}{\pi} + o(1)$ . Similar asymptotics hold for other distribution of the coefficients however for the rest of this post we assume that the coefficients are uniformly random in $(-1,1)$ . This means that the number of complex roots is approximately $\displaystyle n - \frac{2\log n}{\pi}$ . Definition 1 : The largest root of a polynomial is the root with the largest modulus. Definition 2 : The smallest root of a polynomial is the root with the smallest modulus. The above graph shows the roots of a polynomial of degree $101$ ; the largest root is in the top right corner in green. Is the largest or the smallest root more likely to be complex or real? The naive guess is that the largest or the smallest root is more likely to be complex than real because there are exponentially more complex roots than real roots as seen from the above asymptotic. However, experimental data shows that Probability that the largest root is real is equal to the probability that the smallest root is real and this probability is greater than that of either of them being complex. This probability decreases to $1/2$ as $n \to \infty$ as shown in the above graph (created using a Monte Carlo simulation with $10^5$ trails for each value of $n$ ). Note : Instead of uniform distribution, if we assume that the coefficients are normally distributed with mean $0$ and standard deviation $1$ and scaled to $(-1,1)$ , the above observation and limiting probabilities hold. It is counter intuitive that despite being much exponentially fewer in number, real roots are more likely to contain the largest as well as the smallest roots. In this sense, the largest as well as the smallest roots is biased towards reals . Question 1 : What is the reason for this bias? Question 2 : Does the probability that the largest (or the smallest) root of a polynomial of degree $n$ is real approach $\frac{1}{2}$ as $n \to \infty$ ? Update 2-May-2024 : We can quantify the observed bias as follows. Let $P(L|R)$ be the probability that a root is the largest given that it is real and let $P(L|C)$ be the probability that a root is the largest given that it is complex. Similarly, let $P(S|R)$ be the probability that a root is the smallest given that it is real and let $P(S|C)$ be the probability that a root is the smallest given that it is complex. Then the experimental data says that $$
P(L|R) = P(S|R) \approx \frac{\pi}{4\log n},
$$ $$
P(L|C) = P(S|C) \approx \frac{\pi}{2n\pi - 4\log n}.
$$ Related : What is the probability that the absolute value of the roots of a polynomial of degree $n$ is greater than $x$ ?","['roots', 'polynomials', 'limits', 'algebra-precalculus', 'probability']"
4908117,Expanding $\sin(ab)$ in terms of $\sin a$ and $\sin b$.,"Motivation behind the question: I've wondered since grade 10 that if $\sin(a+b),\sin(a-b)$ have formulas in terms of $\sin(a)$ and $\sin(b)$ , then why not $\sin(ab)$ . But I couldn't find any such information sadly. My attempt :- $$\sin(ab)=\sin\bigg(\frac{(a+b)^2}{4}-\frac{(a-b)^2}{4}\bigg)$$ This can be expanded as $$\sin \bigg(\frac{a+b}{2}\bigg)^2\cos \bigg(\frac{a-b}{2}\bigg)^2-\cos \bigg(\frac{a+b}{2}\bigg)^2\sin \bigg(\frac{a-b}{2}\bigg)^2$$ Now even if I try to expand the squares, I get something in the form of $\sin(a^2)$ , and thus I can't run from multiplication. Any help, or closed form solution will be greatly appreciated.","['algebra-precalculus', 'trigonometry']"
4908141,What's the optimal area a rectangle can have inside this parabola?,"If a parabola is of the form $\frac{1}{8}$ ${(42x-15x^2)}$ , what would be the optimised area a rectangle can have inside it if the bottom (width) of the rectangle is aligned on the X axis i.e the y isn't less than zero? Edit: Thank you to everyone for your help!","['optimization', 'derivatives', 'geometry']"
4908185,Laplace Transform Differential equations,"Given Equation is $(D^4-a^4)y = x^4$ Solving the auxiliary equation gives $D = a,-a,+ai,-ai$ So the homogeneous solution is $y_h = c_1e^{ax} + {c_2e^{-ax}} + c_3sin(ax) + c_4 cos (ax) $ Taking the Laplace transform $ Y(s) = \frac{24}{s^5(s^4-a^4)}$ Using the incomplete partial fraction method is a lengthy process as $s^5$ is in the denominator. Finding the particular integral using inverse differential operator seems to be the easier method. Is there a quicker way to solve this equation using the Laplace transform method?? If we use convolution, since we already know the homogeneous solution, does that help us in some way??","['laplace-transform', 'ordinary-differential-equations']"
4908346,Similarity between Combinatorial Series and Recursions,"I was evaluating the sum $$a_n = \sum_{k=0}^{n} {2n+1 \choose 2k+1} 2^{3k}$$ The method I went about was treating $$a_n = \frac{1}{2\sqrt{2}} \sum_{k=0}^{n} {2n+1 \choose 2k+1} (2\sqrt{2})^{2k+1} \\ b_n = \frac{1}{2\sqrt{2}} \sum_{k=0}^{n} {2n+1 \choose 2k} (2\sqrt{2})^{2k}$$ Obviously, by the binomial theorem $$a_n + b_n = \frac{1}{2\sqrt{2}} (2\sqrt{2}+1)^{2n+1} \\ a_n - b_n = \frac{1}{2\sqrt{2}} (2\sqrt{2}-1)^{2n+1}$$ This gives a closed form for both $a_n$ and $b_n$ as $$a_n = \frac{1}{4\sqrt{2}} \left((2\sqrt{2}+1)^{2n+1} + (2\sqrt{2}-1)^{2n+1} \right) \\ b_n = \frac{1}{4\sqrt{2}} \left((2\sqrt{2}+1)^{2n+1} - (2\sqrt{2}-1)^{2n+1} \right)$$ But this form seemed familiar to the general solution of a homogeneous second order recursion, the recursions being: $$a_{n+2} - 18a_{n+1} + 49a_n = 0, a_0 = 1, a_1 = 11 \\ b_{n+2} - 18b_{n+1} + 49b_n = 0, b_0 = \frac{1}{2\sqrt{2}}, b_1 = \frac{25}{2\sqrt{2}}$$ Is there a more ""elegant"" way to reach this recursion without finding the closed form of $a_n$ or $b_n$ ? (finding just the characteristic roots of the recursion also works) Also, to what extent can we generalize this?","['summation', 'recursion', 'recurrence-relations', 'binomial-coefficients', 'sequences-and-series']"
4908348,A mysterious limit: probability that a triangle captures the centre of a circle.,"On a circle, choose $6n$ $(n\in\mathbb{Z^+})$ uniformly random points and label them $a_0,a_1,a_2,\dots,a_{6n-1}$ going anticlockwise, with $a_0$ chosen randomly. Draw three chords: Chord $a_0 a_{3n}$ Chord $a_n a_{4n}$ Chord $a_{2n} a_{5n}$ Here is an example with $n=5$ . The rightmost point is $a_0$ . The centre of the circle is shown. Let $P(n)=$ probability that the triangle formed by the three chords contains the centre of the circle. What is $\lim\limits_{n\to\infty}P(n)$ ? Intuition When I came up with this question, I had no intuition as to whether $P(n)$ should increase or decrease, as $n$ increases. Intuitively, as $n$ increases, the expected area of the triangle should decrease, which tends to make $P(n)$ decrease. But at the same time, the triangle should ""get closer"" to the circle's centre (e.g. the expectation of the distance between the triangle's centroid and the circle's centre, should decrease), which tends to make $P(n$ ) increase. It was not clear to me which of these opposing factors should dominate. Simulations Simulations, with $5\times10^6$ sets of three chords for each value of $n$ , yielded the following estimated probabilities: $P(1)\approx 0.0625\space$ (I prove that $P(1)=\frac{1}{16}$ below.) $P(2)\approx 0.0741\overset{?}{=}\frac{2}{27}$ $P(3)\approx 0.0786$ $P(4)\approx 0.0809$ $P(5)\approx 0.0824$ $P(6)\approx 0.0832$ $P(10)\approx 0.0849$ I also got $P(100)\approx 0.0872$ based on a simulation with $10^6$ sets of three chords. I tried to get an approximation for $P(1000)$ , but my computer started overheating (I'm using Excel for my simulations). Here is a plot of estimated $P(n)$ against $n$ . So as $n$ increases, it seems that $P(n)$ is approaching some number strictly between $0$ and $1$ . Due to the naturalness of my question's geometric construction, I suspect that $\lim\limits_{n\to\infty}P(n)$ has a closed form, and it should be an interesting number worth finding. I also suspect that $P(n)$ is a rational sequence. My attempt I have only been able to prove that $P(1)=\frac{1}{16}$ . Here is my proof. Suppose that: When we choose the six random points on the circle, instead of having a continuous distribution of points to choose from, we have $2k$ evenly spaced points to choose from, where $k$ is a large integer. The circumference of the circle is $2k$ . A point can be chosen more than once. Let $x=$ distance from $a_0$ to $a_3$ along the circle going anticlockwise. Let $y=$ distance from $a_2$ to $a_5$ along the circle going anticlockwise. Let $z=$ distance from $a_1$ to $a_4$ along the circle going anticlockwise. If the triangle contains the centre of the circle, then either $x\le k$ and $y\le k$ and $z\ge k$ , or $x\ge k$ and $y\ge k$ and $z\le k$ . These two configurations are shown respectively below, where $a_0$ is the rightmost point in each diagram. By symmetry, each of these happens with equal probability, so we have: $$P(1)=2\times P(x\le k \land y\le k\land z\ge k)$$ To see these distances more clearly, imagine cutting the circle at $a_0$ and straightening the circle into a line segment, so that the points from left to right are $a_0, a_1, a_2, a_3, a_4, a_5$ . To understand the next part, it may be easier to first work with specific numbers, so let's suppose $k=100$ , and suppose $\color{red}{x=10}$ . Then $90\le y \le 100$ . If $y=90$ , there is only $\color{blue}{1}$ possible combination of locations for $a_1$ and $a_4$ (because we require $z\ge 100$ ). If $y=91$ then there are $(1)+(1+2)=\color{blue}{4}$ possible combinations of locations for $a_1$ and $a_4$ . If $y=92$ then there are $(1)+(1+2)+(1+2+3)=\color{blue}{10}$ possible combinations of locations for $a_1$ and $a_4$ . $\cdots$ If $y=100$ then there are $(1)+(1+2)+(1+2+3)+\cdots+(1+2+3+\cdots+11)=\color{blue}{286}$ possible combinations for $a_1$ and $a_4$ . So for $\color{red}{x=10}$ , the total number of possible combinations for $a_1$ and $a_4$ is $\color{blue}{1+4+10+\dots+286}=1001$ . The numbers $1,4,10,\dots,286$ are the tetrahedral numbers . The sum of the first $x+1$ tetrahedral numbers is $\binom{x+4}{4}$ . So the total number of combinations of $a_1, a_2, a_3, a_4, a_5$ that satisfy $(x\le k \land y\le k\land z\ge k)$ is $\sum\limits_{x=0}^{k}\binom{x+4}{4}$ . So the probability that $(x\le k \land y\le k\land z\ge k)$ is $\sum\limits_{x=0}^{k}\binom{x+4}{4}$ divided by the total number of ways to choose $a_1, a_2, a_3, a_4, a_5$ , which is $\binom{2k}{5}$ . To change from a discrete back to a continuous distribution of available points along the circle, we take the limit as $k\to\infty$ . Remembering that $P(1)=2\times P(x\le k \land y\le k\land z\ge k)$ , we have: $\begin{align}
P(1)&=2\lim\limits_{k\to\infty}\frac{\sum\limits_{x=0}^k\binom{x+4}{4}}{\binom{2k}{5}}\\
&=2\lim\limits_{k\to\infty}\frac{\sum\limits_{x=0}^k\frac{x^4}{4!}}{\frac{2^5}{5!}k^5}\\
&=2\lim\limits_{k\to\infty}\frac{\frac{1}{4!}\cdot\frac{1}{5}k^5}{\frac{2^5}{5!}k^5}\\
&=\frac{1}{16}
\end{align}$","['integration', 'geometric-probability', 'circles', 'limits', 'probability']"
4908354,Show that if $E(X_n|\mathcal{F}_n)\rightarrow_p 0$ then $X_n\rightarrow_p 0$.,"Let $\{X_n\}_{n \in \mathbb{N}}$ be a sequence of non-negative integrable random variables.
i. Show that $X_n\rightarrow_p 0$ if and only if $E(\min(X_n,1))\rightarrow 0$ .
ii. Show that if $E(X_n|\mathcal{F}_n)\rightarrow_p 0$ then $X_n\rightarrow_p 0$ and converse is not true. Proof of i. We know that \begin{align*}
\lim_n E[\min(X_n,1)]&=\lim_n \int^1_0P(X_n\geq x)dx \\
&= \int^1_0\lim_nP(X_n\geq x)dx\\
&=0
\end{align*} I am struggling with ii.). I think I should use the result of i.) to prove ii.) Proof of ii.  (Using Sangchul Lee's hint) Using result of part i. $\mathbb{E}[X_n\mid\mathcal{F_n}]\rightarrow_p 0 \iff \mathbb{E}\Big(\min\{\mathbb{E}[X_n\mid\mathcal{F_n}],1\}\Big)\rightarrow 0$ \begin{align*}
\mathbb{E}[\min\{X_n,1\}]
=
\mathbb{E}\Big(\mathbb{E}[\min\{X_n,1\}\mid\mathcal{F_n}]\Big)
\leq
\mathbb{E}\Big(\min\{\mathbb{E}[X_n\mid\mathcal{F_n}],1\}\Big)\rightarrow 0
\end{align*} So $\mathbb{E}[\min\{X_n,1\}]\rightarrow 0$ , which implies $X_n\rightarrow_p 0$ by another application of (i.).","['conditional-probability', 'measure-theory', 'probability-theory']"
4908389,multiple general solutions for this differential equation,"In the differential equation below, one way to solve it is to replace y with cos(u) or sin(u). But, when I tried both cos(u) and sin(u), general solutions for y seem different. Is this expected or did I do something wrong? $ y{}' = \sqrt{1-y^{^{2}}}$ By the way, I got the following answers. When I replace y with sin(u), y=sin(x+c) where c is constant. When I replace y with cos(u), y=cos(-x+c) where c is constant.",['ordinary-differential-equations']
4908420,Definite Integral $\int_{0}^{\infty} \frac{\log(x)}{(1+x+y)^{5/2}} \mathrm{d} x$,"My Attempt The integral I'm trying to solve is \begin{align*}
    I_1 &= \int_{0}^{\infty} \frac{\log(x)}{(1+x+y)^{5/2}} \mathrm{d} x.
\end{align*} Letting $u=\frac{1}{(1+x+y)^{3/2}}$ , we have \begin{align*}
    I_1 &= \frac{2}{3} \int_{0}^{(1+y)^{-3/2}} \log \left( \frac{1-u^{2/3}(1+y)}{u^{2/3}} \right) \mathrm{d} u\\
    &= \frac{2}{3} \int_{0}^{(1+y)^{-3/2}} \log \left[ \frac{(1+u^{1/3}\sqrt{1+y})(1-u^{1/3}\sqrt{1+y})}{u^{2/3}} \right] \mathrm{d} u\\
    &= \frac{2}{3} \biggl[ \underbrace{\int_{0}^{(1+y)^{-3/2}} \log(1+u^{1/3}\sqrt{1+y}) \mathrm{d} u}_{I_2} \, + \, \underbrace{\int_{0}^{(1+y)^{-3/2}} \log(1-u^{1/3}\sqrt{1+y}) \mathrm{d} u}_{I_3} \biggr.\\
    &\qquad \biggl. - \, \frac{2}{3} \underbrace{\int_{0}^{(1+y)^{-3/2}} \log(u) \mathrm{d} u}_{I_4} \biggr].  
\end{align*} I have found that \begin{align*}
    I_2 &= (1+y)^{3/2} \log(2+y) - \frac{1}{(1+y)^2} \left[ \frac{1}{3} (y+2)^3 - \frac{3}{2} (y+2)^2 + 3(y+2) - \log(y+2) - \frac{11}{6} \right]
\end{align*} and \begin{align*}
    I_4 &= (1+y)^{3/2} \left[ \frac{3}{2} \log(1+y) - 1 \right].
\end{align*} The following is my attempt at solving $I_3$ . Letting $v=1-u^{1/3}\sqrt{1+y}$ , we have \begin{align}
    \nonumber I_3 &= \frac{1}{(1+y)^{3/2}} \int_{-y}^{1} 3(v-1)^2 \log(v) \mathrm{d} v\\
    \nonumber &= \frac{1}{(1+y)^{3/2}} \left\{ \left[ \log(v) \left( v^3-3v^2+3v \right) \right]_{-y}^{1} - \int_{-y}^{1} \left( v^3 -3v^2 + 3v \right) \cdot \frac{1}{v} \mathrm{d} v \right\}\\
    \nonumber &= \frac{1}{(1+y)^{3/2}} \left\{ \left[ 0-\log(-y) \left( -y^3-3y^2-3y \right) \right] - \int_{-y}^{1} \left( v^2-3v+3 \right) \mathrm{d} v \right\}\\
    \nonumber &= \frac{1}{(1+y)^{3/2}} \left\{ -\log(-y) \left( -y^3-3y^2-3y \right) - \left[ \frac{1}{3}v^3 - \frac{3}{2} v^2 + 3v \right]_{-y}^{1} \right\}\\
    &= \frac{1}{(1+y)^{3/2}} \left[ -\log(-y) \left( -y^3-3y^2-3y \right) - \frac{11}{6} - \frac{1}{3} y^3 - \frac{3}{2} y^2 - 3y \right]. \tag{1}
\end{align} Question Before dealing with the main integral ( $I_1$ ), I have proved that \begin{align*}
    \int_{0}^{\infty} \frac{\log(x)}{(1+x)^{5/2}} \mathrm{d} x = \frac{4}{3} [\log(2)-1],
\end{align*} that is $I_1$ with $y=0$ . Now, I would like to prove that \begin{align*}
    I_1 &= \frac{2\log\left(y+1\right)+4\log\left(2\right)-4}{3\left(y+1\right)^\frac{3}{2}}.
\end{align*} In (1), I encountered a $\log(-y)$ . Does this mean that I have to restrict $y$ to $y \leq 0$ ? How could I deal with this? Can anyone help me with this problem? I am open to any suggestions/solutions, including that for the very first integral in the question. Any help/feedback is much appreciated. Thank you!","['integration', 'calculus', 'definite-integrals']"
4908443,How to solve for the critical points of $f(x) = 3x - \tan(x)?$,"I've been trying for quite a while now to try and solve the problem: The function $f(x) = 3x - \tan(x)$ has two critical points in the interval $\pi/2 < x < \pi/2$ . Find the x-coordinates of the extrema's. My attempt: $ f(x) = 3x - \tan(x)$ I took the derivative: $f'(x) = 3 - \frac{(\cos^2x + \sin^2(x))}{\cos^2(x)}$ Set the derivative function to zero: $0 = 3 - \frac{1}{\cos^2(x)} \iff$ $\cos(x) = \pm\sqrt\frac{1}{3} \iff $ $x = \arccos(\pm\sqrt\frac{1}{3})$ The only problem now is that, when I take the negative root, I do not match up with the real answer. The real answer is approximately $-0.9553$ , meaning taking the negative of the first root. It perfectly makes sense graphically, I have done that on Desmos. However, computationally, it is not really matching up. Could someone please explain how to solve the problem computationally without the error I made. It would be sincerely appreciated. Thank you!","['optimization', 'calculus', 'trigonometry']"
4908464,Line integral of a vector field over a triangle [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last month . Improve this question I am trying to solve this line integral, but I get different answers when doing a normal line integral and using the vector form of green's theorem and am wondering why this is the case? I got 4 as my answer solving the line integral and 12 using green's theorem.
I am also confused as to why the integral has $d\mathbf{s}$ , and not $d\mathbf{r}$ , so any clarification on that would be awesome.
The question states: Evaluate $\int_{C} \mathbf{F}\cdot d\mathbf{s}$ where $\mathbf{F} = \langle 2z,8x-3y,3x+y \rangle $ and $C$ is the triangle with vertices $(1,0,0), (0,1,0), (0,0,2)$ Attempt with line integrals: Parametrizing $C$ into $C_1 = <1-t,t,0>$ , $C_2 = <-,1-t,2t>$ , and $C_3 = <5,0,2-2t>$ where $0\le t \le 1$ $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \sum_{i=1}^{3}\int_{C_i} \mathbf{F}\cdot d\mathbf{s} = \int_{C_1} \mathbf{F}\cdot d\mathbf{s} + \int_{C_2} \mathbf{F}\cdot d\mathbf{s} + \int_{C_3} \mathbf{F}\cdot d\mathbf{s}$$ $$\int_{C_1} \mathbf{F}\cdot d\mathbf{s} = \int_{0}^{1} \mathbf{F}(\mathbf{r}(t))\cdot \mathbf{r}\prime(t)dt$$ $$\int_{0}^{1} <2(0),8(1-t)-3(t),3(1-t)+(t)>\cdot<-1,1,0> dt$$ $$\int_{0}^{1} 8-8t-3t dt = \frac{5}{2}$$ With $C_2$ and $C_3$ being $$\int_{C_2} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2}$$ and $$\int_{C_3} \mathbf{F}\cdot d\mathbf{s} = -1$$ So $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \frac{5}{2} + \frac{5}{2} - 1 = 4$$ With Green's Theorem: $$\int_{C} \mathbf{F}\cdot d\mathbf{s} = \iint_{D}curl\mathbf{F}\cdot k dA$$ $$curl\mathbf{F} = <1,-1,8>$$ $$\iint_{D}curl\mathbf{F}\cdot k dA = \iint_{D}8 dA$$ $$= 8 \iint_{D}dA$$ where $\iint_{D}dA$ would be the area of the triangle area of triangle $$A = \frac{1}{2}|<-1,1,0>\times<-1,0,2>| = \frac{\sqrt{9}}{2} = \frac{3}{2}$$ and so $$8 \iint_{D}dA = 8 * \frac{3}{2} = 12$$ Any help on how to solve this would be greatly appreciated, thanks!","['vector-fields', 'multivariable-calculus', 'line-integrals', 'greens-theorem']"
4908480,For which values of $a$ there is no relative extremum?,"In a previous question , I explored the conditions under which the function $ f(x, y) = (y - x^2)(y - ax^3) $ does not have a relative extremum at the origin. I've found a similar but distinct problem formulation to be equally interesting. Here's the modified version: Let $f : \mathbb{R}^2 \to \mathbb{R}$ be a function defined by $f(x,y) = (y - x^2)(y - ax^2)$ at each $ (x,y) \in \mathbb{R}^2$ , where $a \in \mathbb{R}$ . What are the values of $ a $ for which $ f$ does not have a relative extremum at $(0,0)$ ? Here is my solution attempt:
To find the values of $ a$ for which $ f $ does not have a relative extremum at $ (0, 0) $ , it is necessary to analyze the critical points of $ f $ at $ (0, 0) $ . $f(x, y) = \frac{y}{y^2} - axy^2 + 2xy + ax^4$ $\frac{\partial f}{\partial x} = -2(a+1)xy + 4ax^3$ $\frac{\partial f}{\partial y} = 2y - 2(a+1)x^2$ $\frac{\partial^2 f}{\partial x^2} = -2a + 12ax^2$ $\frac{\partial^2 f}{\partial y^2} = 2$ $\frac{\partial^2 f}{\partial x \partial y} = -2a + 2x$ $H = \begin{bmatrix} -2a + 12ax^2 & -2a + 2x \\ -2a + 2x & 2 \end{bmatrix}$ $H_1(0,0) = 0$ , first principle minor. $H_2(0,0) = 0$ , second principle minor. I have outlined my analysis and calculations above to determine the values of $a$ for which the function $f$ does not have a relative extremum at $(0, 0)$ . But I could not reach any conclusions. I would greatly appreciate it if members of the community could review my approach and calculations. Are there any errors or aspects I might have overlooked? Thank you in advance for your insights and assistance.","['multivariable-calculus', 'calculus']"
4908485,"How many ways to derange $n$-numbers, ignoring the direction?","Derangement is a permutation of the elements of a set in which no element appears in its original position. In other words, a derangement is a permutation that has no fixed points.
The recursive relationship is $!n = (n-1) \cdot \left( !(n-1) + !(n-2) \right)$ for $n\ge 2$ , with $!0 = 1$ and $!1=0$ , (also we have $!2 = 1$ , $!3 = 2$ , $!4 = 9$ , $!5 = 44$ etc).
There's an explicit formula for derangement, that is $!n = n! \sum_{i=0}^n \frac{(-1)^i}{i!}$ . Each derangement can be seen as, with numbers $1$ , $2$ , ..., $n$ put on a circle, draw $n$ arrows among them, each number has one arrow going out and one going in. Now if we ignore the direction of the arrows, that is, change the arrows to non-directional lines, how many patterns will it be, to line up $1$ , $2$ , ..., $n$ numbers on a circle, with each number has two lines? Call it as $f(n)$ ,
we have $f(1)=0$ , $f(2)=1$ , $f(3)=1$ , $f(4)=6$ . For example, for 3 numbers on a circle there’s only one pattern: $(1-2-3)$ ; for 4 numbers there are 6 patterns, namely $(1-2, 3-4)$ , $(1-3, 2-4)$ , $(1-4, 2-3)$ , $(1-2-3-4)$ , $(1-2-4-3)$ , and $(1-3-2-4)$ . So is there an explicit formula for $f(n)$ ?","['derangements', 'combinatorics']"
4908491,Is every $\alpha$-Hölder continuous function of bounded variation absolutely continuous?,"Let $f:[a,b] \to \mathbb{R}$ be $\alpha$ -Hölder continuous function of bounded variation, does it follow that $f$ is absolutely continuous ? Here $\alpha \in (0,1)$ is fixed . Here are some sources : If $f$ is $\alpha$ -Hölder continuous for $\alpha \geqslant 1$ , then the absolute continuity follows straight from the definition. If $f$ is only $\alpha$ -Hölder continuous for $\alpha \in (0,1)$ but not necessarily of bounded variation, the Weirstrass function ( Hölder continuity of Weierstrass Function ) is a counterexample. If $f$ is only (uniformly) continuous and of bounded variation, then the devil's staircase is a counterexample. This counterexample is not $\alpha$ -Hölder continuous, see https://mathoverflow.net/questions/45020/non-h%C3%B6lder-continuous-devils-staircases .","['analysis', 'real-analysis', 'continuity', 'absolute-continuity', 'functional-analysis']"
4908510,Finitely generated abelian groups with the same finite quotients,"Let $\Gamma$ and $\Delta$ be two finitely generated abelian groups. Therefore, by the classification theorem of finitely generated abelian groups, we can assume that $\Gamma \cong \mathbb{Z}^r \oplus T_1$ and $\Delta \cong \mathbb{Z}^s \oplus T_2$ , where $T_1$ and $T_2$ are finite abelian groups. I am assuming that $\Gamma$ and $\Delta$ have the same finite quotients and I want to prove that $\Gamma \cong \Delta$ . To do this, first of all, I want to prove that $r=s$ . I want to procede in the following way: if by contradiction $r>s$ , then I can choose a large prime $p$ such that $p$ does not divide $|T_1||T_2|$ , and construct a finite quotient $(\mathbb{Z}/p\mathbb{Z})^r$ that cannot be a quotient of $\Delta$ . How can I prove that $(\mathbb{Z}/p\mathbb{Z})^r$ is not a quotient of $\Delta$ ? I am thankful for any hint.","['finite-groups', 'profinite-groups', 'group-theory', 'abstract-algebra', 'abelian-groups']"
4908550,"$G$ 3-connected graph, $xy \in E(G)$. $G'$ graph obtained from $G$ by removing the edge $xy$ and merging the vertices $x$ and $y$ into one vertex.","Let $G$ be a $3$ -connected graph and $xy \in E(G)$ . Let $G'$ be the graph obtained from $G$ by removing the edge $xy$ and merging the vertices $x$ and $y$ into one vertex. Prove: The graph $G'$ is $3$ -connected if and only if the graph $G - $ { $x, y$ } is $2$ -connected. Attempt: I know that a graph $G$ is $k$ -connected if it has at least $k + 1$ vertices and for every subset $A \subseteq V(G)$ , $|A| < k$ , the graph $G-A$ is connected. Since $G - $ { $x, y$ } is $2$ -connected, removing any single vertex does not disconnect it. Now, I consider any subset $A$ of vertices in $G'$ such that $|A| < 3$ and I need to show that $G' - A$ is connected. How do I continue? Similarly I got stuck on the other implication. Any help would be appreciated.","['graph-theory', 'graph-connectivity', 'discrete-mathematics']"
4908592,Question on Complex Integral with Polar Form,"Let $f$ be a complex-valued integrable function. Write the complex number $\int fd\mu$ in its polar form, letting $w$ be a complex number of absolute value 1 such that \begin{align}
\int fd\mu = w\left|\int fd\mu\right|.
\end{align} Then, \begin{align}
\left|\int fd\mu\right| = w^{-1}\int fd\mu = \int(w^{-1}f)d\mu = \int\mathfrak{R}(w^{-1}f)d\mu \leq \int |f|d\mu.
\end{align} I have difficulty understanding why $\int(w^{-1}f)d\mu = \int\mathfrak{R}(w^{-1}f)d\mu$ is true. Could someone please help me out? Thanks a lot in advance! I want to write $f=u+iv$ and $w=\cos\theta-i\sin\theta$ . Then $w^{-1} = \cos\theta-i\sin\theta$ and \begin{align}
w^{-1}f = (u\cos\theta+v\sin\theta)+i(v\cos\theta-u\sin\theta).
\end{align} I couldn't see why $\mathfrak{I}(w^{-1}f) = v\cos\theta-u\sin\theta = 0$ . Let $(X,\mathscr{A},\mu)$ be a measure space. A complex-valued function $f$ on $X$ is integrable if its real and imaginary parts $\mathfrak{R}(f)$ and $\mathfrak{I}(f)$ are integrable; if $f$ is integrable, then its integral is defined by \begin{align*}
    \int fd\mu = \int\mathfrak{R}(f)d\mu + i\int\mathfrak{I}(f)d\mu.
\end{align*}","['integration', 'measure-theory', 'analysis', 'real-analysis', 'complex-analysis']"
4908608,"Disproving surjectivity of $f : \Bbb Z \times \Bbb Z \rightarrow \Bbb Z$, $f(u,v) = 3u + 6v$","A function $f : \Bbb Z \times \Bbb Z \rightarrow \Bbb Z$ is defined as $f(u,v) = 3u + 6v.$ Is the function surjective? Prove it. I had the following proof. Proof Pick $x = 2$ , then $3u + 6v = 2 \Rightarrow 3(u + 2v) = 2$ Let $y = u + 2v$ $\exists y \in \Bbb Z \times \Bbb Z$ . Thus $3y = 2 \Rightarrow y = \frac{2}{3}$ . This is a contradiction because $\frac{2}{3} \not\in \Bbb Z \times \Bbb Z.$ The function is therefore not surjective. I am a novice at this whole LaTex thing and relatively new to proofs and these surjective proofs are killing me I can't seem to get anything right. Any help is greatly appreciated.","['proof-writing', 'functions', 'solution-verification']"
4908647,What is the reason for these strange oscillations? Issue with Desmos?,"Take a partition of $\Bbb R^2_{\gt 0}$ by the union of functions indexed by real $t\ge 0$ $$\mathcal F:=\bigg \lbrace \mathcal M[\chi_t(x)]\cup \mathcal M\bigg[\frac{1}{1-\chi_t(x)}\bigg] \bigg \rbrace$$ where $$\mathcal M[\chi_t(x)]:=\int_{(0,1)} \chi_t(x)x^{s-1}~dx=  2\sqrt{\frac{t}{s}}K_1(2\sqrt{ts})=\Phi_t(s)$$ and $$\mathcal M\bigg[\frac{1}{1-\chi_t(x)}\bigg] :=\int_{(0,1)} \frac{1}{1-\chi_t(x)}x^{s-1}~dx= \Psi_t(s)= \sum_{n=0}^\infty \Phi_{tn}(s)=\sum_{n=0}^\infty 2\sqrt{\frac{tn}{s}}K_1(2\sqrt{ts})$$ for $K_1$ Bessel function. Consider the functions indexed again by $t$ $$H_t(x)=-\frac{\Phi'{_t}(s)}{\Phi{_t}(s)\Psi_t(s)}  $$ This set of functions forms a real analytic partition (even a foliation) of $\Bbb R^2_{\gt 0}$ we have that $$\bigcup_{t\ge 0} H_t(x)=\Bbb R^2_{\gt 0}$$ But I am curious, what is the reason for the oscillations in the plot? Is this an issue with Desmos? Here is a picture of the partition. Link to the plot: Desmos Plot .","['integration', 'definite-integrals', 'graphing-functions', 'real-analysis', 'sequences-and-series']"
4908650,Show a ring is commutative if $r^2 = r + r$,"The claim is that any ring $R$ in which for all $r \in R$ we have that $rr = r + r$ , must be commutative. No assumptions are made about $R$ having multiplicative identity or being commutative. I was told from a professor that it was true. So far, I've found that a ring with this property cannot have a $1$ element distinct from $0$ since $1^2 = 1 + 1 = 1$ , meaning $1 = 0$ . I also have that every element is nilpotent with a degree at most $3$ . This comes from \begin{equation*}
(r+r)(r+r) = (r+r) +(r+r) \\
r^2 + r ^2 + r^2 + r^2 = r^2+r^2 \\
r^2 + r^2 = 0 \\
r^4 = 0.
\end{equation*} Expanding $r^3$ then gives \begin{equation*}
r^3 = r(r^2) \\
r^3 = r(r + r) \\
r^3 = r^2 + r^2 \\
r^3 = 0.
\end{equation*} The other more notable result that I've managed to prove is that $ab = -ba$ for all $a, b \in R$ . This can be found from expanding \begin{equation*}
a^2 + b^2 = (a + a) + (b + b) \\
a^2 + b^2 = (a + b) + (a + b) \\
a^2 + b^2 = (a + b)(a + b) \\
a^2 + b^2 = a^2 + ab + ba + b^2 \\
ab = -ba
\end{equation*} It feels like I'm really close but I'm not really sure where to go from here. From the above, it seems like for R to commute you would need that $ab + ab = (ab)^2 = 0$ . I've continued to look at other expansions from the $r^2 = r + r$ identity and have been able to develop some other equations, but have had no success in proving commutativity yet.","['ring-theory', 'rngs', 'abstract-algebra', 'noncommutative-algebra']"
4908662,Determining Jordan canonical form(JCF) of an operator given by complex differentiation.,"Let $W$ be the subspace of $\Bbb C$ linear combination of the following functions: $$f_1(z)=\sin z,\qquad f_2(z)=\cos z,\qquad f_3(z)=\sin2z,\qquad f_4(z)=\cos2z.$$ Let $T$ be the linear opeartor on $W$ given by complex differentition.Which of the following statements are true? $1$ .Dimension of $W$ is $3$ . $2$ .The span of $f_1$ and $f_2$ is a Jordan block of $T$ . $3$ . $T$ has two Jordan blocks. $4$ . $T$ has four Jordan blocks. Since $f_1(z)=\sin z$ , $f_2(z)=\cos z$ , $f_3(z)=\sin2z$ and $f_4(z)=\cos2z$ are linearly independent,hence dimension of $W$ is $4$ .So option (1) is incorrect. Also $W$ is $T$ invariant (as $f'_1(z)=\cos z$ , $f'_2(z)=-\sin z$ , $f'_3(z)=2\cos2z$ , $f'_4(z)=-2\sin2z$ ) Now for other options one should find Jordan canonical form of $T$ and for Jordan canonical form one must have eigenvalues but I am unable to find eigenvalues of $T$ .","['jordan-normal-form', 'linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
4908681,"How could we approximate $\int \frac{W(t) }{1+W(t)}\,\, \frac {\sin(t)} t \, dt$?","In a now deleted post appeared an interesting integral. $$I=\int\frac 1 x \,\,\sin \left(\frac{\log (x)}{x}\right) \,dx$$ which does not make (too much) problems from a numerical point of view. As one can expect, the plot of the integrand is not the most pleasnat we could find. $$\frac{\log(x)}x=-t \quad\implies\quad x=\frac{W(t)}{t}\quad \implies \quad $$ $$I=\int \frac{W(t) }{1+W(t)}\,\, \frac {\sin(t)} t \, dt=\text{Si}(t)-\int \frac{1 }{1+W(t)}\,\, \frac {\sin(t)} t \, dt$$ which is much more pleasant to look at and even easier to integrate nmerically. My question is : how could we approximate the integrand to have a decent approximation for the integral between $k\pi$ and $(k+1)\pi$ ? $k$ being a non negative integer. Edit Thanks to @Hume2's answer, the problem now reduces to $$I=W(t)\,\sin(t)-\int W(t)\,\cos(t)\, dt$$ $$J_k=-\int_{k\pi}^{(k+1)\pi} W(t)\,\cos(t)\, dt$$ Asymptotically, it seems that $$|J_k| \sim \frac{1}{2 k}+\frac{3}{4k^2}+\frac{19}{4k^3}+O\left(\frac{1}{k^4}\right)$$","['integration', 'trigonometry', 'lambert-w']"
4908701,Showing explicitly that every 2-form on a plane minus the origin is exact,"Consider $X=\Bbb R^2-0$ . Since it deformation retracts onto the unit circle $S^1$ , the second de Rham cohomology of the manifold $X$ is zero. This means that every 2-form $hdxdy$ on $X$ is exact. But can we find an explicit 1-form $fdx+gdy$ on $X$ satisfying $d(fdx+gdy)=hdxdy$ , i.e. $g_x-f_y=h$ ? I cannot see how to define $f$ and $g$ from $h$ .","['smooth-manifolds', 'multivariable-calculus', 'vector-analysis', 'differential-topology', 'differential-forms']"
4908758,Find the number of pairs of two consecutive zeros,"the problem We call a binary sequence of length $n$ a string with $n$ digits of $0$ or $1$ . For such a sequence, $A$ ,
of finite length, $f(A)$ represents a transformation where every 1 in $A$ becomes $0,1$ and every $0$ of $A$ becomes $1,0$ . e.g $f((1,0,1))=(0,1,1,0,0,1)$ Find the number of pairs of two
consecutive zeros from $f^{(n)}((1))$ , where $f^{(n)}((A))$ is the sequence obtained by transforming $A$ $n$ times. my idea I think first of all we should try some values for $n$ and try finding a rule value of n how A looks like 0 1 1 0,1 2 1,0,0,1 3 0,1,1,0,1,0,0,1 4 1,0,0,1,0,1,1,0,0,1,1,0,1,0,0,1 As we can see I think if we have a zero and a one consecutivly we can make a pair of 3 consecutive zero. Although this pair will be destroyed next transformation. I don't know what to do forward! Hope one of you can help me! Thanks!","['binary', 'combinatorics']"
4908806,Example of an infinite compact measurable space,"Let $X$ be a nonempty set with a $\sigma$ -algebra $\mathcal{A}$ . The notion of $\sigma$ -algebra strictly lies between Boolean algebras and complete Boolean algebras. Clearly, $\mathcal{A}$ is a distributive lattice (moreover a $\sigma$ -frame). A measurable space $(X,\mathcal{A})$ is said to be compact if $\mathcal{A}$ is a compact lattice. Any set $X$ with a finite $\sigma$ -algebra $\mathcal{A}$ is a compact measurable space. I want the following example. Does there exist an infinite compact measurable space? By infinite I mean that the $\sigma$ -algebra $\mathcal{A}$ has infinite cardinality.","['measure-theory', 'lattice-orders', 'borel-measures']"
4908876,"Prove that for every graph $G$, it holds that $|V(G)| \geq \kappa(G) \cdot (\text{diam}(G) - 1) + 2$. Find example for equality.","Prove that for every graph $G$ , it holds that $|V(G)| \geq \kappa(G) \cdot (\text{diam}(G) - 1) + 2$ . For $k \in \{1,2\}$ and $d \geq 2$ , find an example of a $k$ -connected graph with diameter $d$ for which the equality in the above estimate holds. Attempt: I was able to prove this theorem (I help with this link: Min. number of vertices in graph as function of $\kappa(G)$ and $\operatorname{diam}(G)$ ) , but I am unable to find an example of an equality above.","['graph-theory', 'graph-connectivity', 'discrete-mathematics']"
4908913,Mathematical proof that the earth is round.,"Aristotle in his treatise on the heavens proves that the earth is round as follows : he observed lunar eclipses and noticed that only a round sphere could imply a circular shadow, so the earth is round. In modern terms, he says that if $X\subseteq\mathbb{R}^3$ is a variety (differential or algebraic, as you prefer) such that the image of $X$ under all the projections $\pi:\mathbb{R}^3\longrightarrow\mathbb{R}^2$ is a disk then $X$ is a round ball. Do you know a proof of this fact ?","['algebraic-geometry', 'geometry', 'differential-geometry']"
4908958,On the order of growth of entire functions,"Definition. Let $f: \mathbb{C} \to \mathbb{C}$ be an entire function. The order of growth of $f$ , denoted by $O_G(f)$ , is defined as \begin{equation}
    O_G(f) := \inf \left\{r > 0: \exists A, B > 0 \,(\text{depending on } r) \text{ such that } |f(z)| \leq Ae^{B|z|^{r}} \text{ for all } z \in \mathbb{C} \right\}.   \hspace{1cm} (1)
\end{equation} My question: If $f$ is entire and $O_G(f) = \rho$ , do there exist constants $A,B > 0$ such that \begin{align*}
    \hspace{2cm} |f(z)| \leq Ae^{|z|^{\rho}} \quad \text{ for all } z \in \mathbb{C} \;?  \hspace{2cm} (2)
\end{align*} (In other words: does the set on the RHS of (1) contain its infimum?) My thinking so far: For each $n \in \mathbb{N}$ there exist constants $A_n > 0$ and $B_n > 0$ such that \begin{align*}
    |f(z)| \leq A_n e^{B_n |z|^{\rho + \frac{1}{n}}} \quad \text{for all } z \in \mathbb{C}. 
\end{align*} Now if the sequences $(A_n)_{n=1}^{\infty}$ and $(B_n)_{n=1}^{\infty}$ are bounded, then we can let $A := \sup_{n} A_n$ and $B := \sup_n B_n$ , and we will have \begin{align*}
    \hspace{2cm} |f(z)| \leq A e^{B |z|^{\rho + \frac{1}{n}}} \quad \text{for all } z \in \mathbb{C}, \hspace{2cm} (3)
\end{align*} and then taking $n \to \infty$ gives (2). But must there exist bounded sequences $(A_n)_{n=1}^{\infty}$ and $(B_n)_{n=1}^{\infty}$ satisfying (2)? If not, what would be a counterexample?","['complex-analysis', 'asymptotics']"
4908959,"What are these infinite sums of powers of integers, $n^p$, multiplying a quadratic in the Bessel function $J_n(nx)$ and its derivative $J'_n(nx)$?","What are explicit elementary functions of real $x$ , for $0 < x < 1$ , if they exist, for $p=1$ and $p=3$ of $$\sum_{n=1}^\infty n^p [J_n(nx)]^2$$ $$\sum_{n=1}^\infty n^{p+1}J_n(nx)J'_n(nx)$$ $$\sum_{n=1}^\infty n^p [J'_n(nx)]^2$$ where $J_n(z)$ is the Bessel function of the first kind, and $J'_n(z)$ is its derivative with respect to its argument, $z = nx$ . P.C. Peters and J. Mathews, Physical Review 131, 435-440 (1963), Appendix, showed how to get these sums for even $p=2$ and $p=4$ , but I don't see how to use their procedure for odd values of the exponent $p$ .","['summation', 'real-analysis', 'functions', 'sequences-and-series', 'bessel-functions']"
4909002,Prove $\left(x_1^2+\cdots+x_n^2\right)^3\ge8\left(x_1^3x_2^3+\cdots+x_{n-1}^3x_n^3\right)$,"Let $x_1$ , $x_2\dots$ , $x_n$ be non-negative real numbers, prove that $$\left(\sum_{i=1}^nx_i^2\right)^3\ge8\sum_{i=1}^{\color{red}{n-1}}x_i^3x_{i+1}^3.$$ I have a brutal proof by induction. Assume the inequality is true for $n$ , then for $n+1$ , pick the smallest variable $x_t$ . I will only demonstrate the most difficult case, that is when $x_t$ is in the middle. Let $m^2=x_t^2+x_{t+1}^2$ , after applying the $n$ -case to variables $(x_1,\dots,x_{t-1},m,x_{t+2},\dots,x_{n+1})$ , we are left to prove that $$m^3x_{t-1}^3+m^3x_{t+2}^3\ge x_{t-1}^3x_t^3+x_t^3x_{t+1}^3+x_{t+1}^3x_{t+2}^3.\\\\\iff\left(x_t^2+x_{t+1}^2\right)^3\left(x_{t-1}^3+x_{t+2}^3\right)^2\ge\left(x_{t-1}^3x_t^3+x_t^3x_{t+1}^3+ x_{t+1}^3 x_{t+2}^3\right)^2.$$ Let $x_{t-1}=b+x$ , $x_t=b$ , $x_{t+1}=b+y$ , $x_{t+2}=b+z$ , then the inequality above becomes $$23 b^{12}+(78 x+60 y+78 z) b^{11}+\left(141 x^2+252 y x+126 z x+72 y^2+141 z^2+198 y z\right) b^{10}+\left(152 x^3+468 y x^2+126 z x^2+396 y^2 x+126 z^2 x+378 y z x+44 y^3+152 z^3+360 y z^2+234 y^2 z\right) b^9+\left(105 x^4+516 y x^3+42 z x^3+720 y^2 x^2+126 z^2 x^2+378 y z x^2+372 y^3 x+42 z^3 x+378 y z^2 x+594 y^2 z x+12 y^4+105 z^4+390 y z^3+423 y^2 z^2+138 y^3 z\right) b^8+\left(42 x^5+360 y x^4+780 y^2 x^3+42 z^2 x^3+126 y z x^3+660 y^3 x^2+42 z^3 x^2+378 y z^2 x^2+594 y^2 z x^2+216 y^4 x+126 y z^3 x+594 y^2 z^2 x+558 y^3 z x+42 z^5+270 y z^4+456 y^2 z^3+246 y^3 z^2+36 y^4 z\right) b^7+\left(7 x^6+144 y x^5+540 y^2 x^4+700 y^3 x^3+14 z^3 x^3+126 y z^2 x^3+198 y^2 z x^3+378 y^4 x^2+126 y z^3 x^2+594 y^2 z^2 x^2+558 y^3 z x^2+72 y^5 x+198 y^2 z^3 x+558 y^3 z^2 x+324 y^4 z x+7 z^6+108 y z^5+315 y^2 z^4+262 y^3 z^3+63 y^4 z^2\right) b^6+\left(24 y x^6+216 y^2 x^5+480 y^3 x^4+396 y^4 x^3+42 y z^3 x^3+198 y^2 z^2 x^3+186 y^3 z x^3+126 y^5 x^2+198 y^2 z^3 x^2+558 y^3 z^2 x^2+324 y^4 z x^2+12 y^6 x+186 y^3 z^3 x+324 y^4 z^2 x+108 y^5 z x+18 y z^6+126 y^2 z^5+180 y^3 z^4+66 y^4 z^3\right) b^5+\left(36 y^2 x^6+192 y^3 x^5+270 y^4 x^4+132 y^5 x^3+66 y^2 z^3 x^3+186 y^3 z^2 x^3+108 y^4 z x^3+21 y^6 x^2+186 y^3 z^3 x^2+324 y^4 z^2 x^2+108 y^5 z x^2+108 y^4 z^3 x+108 y^5 z^2 x+18 y^6 z x+21 y^2 z^6+72 y^3 z^5+45 y^4 z^4\right) b^4+\left(32 y^3 x^6+108 y^4 x^5+90 y^5 x^4+22 y^6 x^3+62 y^3 z^3 x^3+108 y^4 z^2 x^3+36 y^5 z x^3+108 y^4 z^3 x^2+108 y^5 z^2 x^2+18 y^6 z x^2+36 y^5 z^3 x+18 y^6 z^2 x+12 y^3 z^6+18 y^4 z^5\right) b^3+\left(18 y^4 x^6+36 y^5 x^5+15 y^6 x^4+36 y^4 z^3 x^3+36 y^5 z^2 x^3+6 y^6 z x^3+36 y^5 z^3 x^2+18 y^6 z^2 x^2+6 y^6 z^3 x+3 y^4 z^6\right) b^2+\left(6 y^5 x^6+6 y^6 x^5+12 y^5 z^3 x^3+6 y^6 z^2 x^3+6 y^6 z^3 x^2\right) b+x^6 y^6+2 x^3 y^6 z^3\ge0,$$ which is true. Are there any nice proofs?","['algebra-precalculus', 'inequality']"
4909003,Calculating $\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta$ using Cauchy's Residue Theorem,"Calculate the following integral using the residue theorem. $$\int_{0}^{2\pi} \frac{\sin(\theta) + \cos(2\theta)}{2 + \sin(2\theta)}d\theta$$ This was my attempted method: Let $z=e^{i\theta}$ , $dz=izd\theta$ $\sin(\theta)=\frac{1}{2i}(z-\frac{1}{z})$ , $\sin(2\theta)=\frac{1}{2i}(z^2-\frac{1}{z^2})$ , $\cos(2\theta)=\frac{1}{2}(z^2+\frac{1}{z^2})$ By substitution, $$\oint_{|z|=1}\frac{\frac{1}{2i}(z-\frac{1}{z})+\frac{1}{2}(z^2+\frac{1}{z^2})}{2+\frac{1}{2i}(z^2-\frac{1}{z^2})}\frac{dz}{iz}=
\oint_{|z|=1}\frac{z^4-iz^3+iz+1}{z(z^4+4iz^2-1)}dz
$$ From the denominator, we have poles at $z=0, \sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle-\frac{\pi}{4}, \sqrt{\left( 2+\sqrt3 \right)}\angle\frac{3\pi}{4}$ Only $z=0, 
\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}, 
\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}$ are located within the contour path $|z|=1$ .
Rewriting the denominator, $$z(z^4+4iz^2-1)=z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i)$$ Around $z=0$ , $$\oint_{|z|=1}\frac{1}{z}\frac{z^4-iz^3+iz+1}{z^4+4iz^2-1}=2\pi i(-1)$$ Around $z=\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4}$ , $$\oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.59386\angle5.1039^\circ)$$ Around $z=\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4}$ , $$\oint_{|z|=1}\frac{1}{(z-\sqrt{\left( 2-\sqrt3 \right)}\angle\frac{3\pi}{4})}\frac{z^4-iz^3+iz+1}{z(z-\sqrt{\left( 2-\sqrt3 \right)}\angle-\frac{\pi}{4})(z^2+(2+\sqrt3)i)}\approx2\pi i(0.699\angle-4.3336^\circ)$$ Adding them up, the resulting sum is $\approx2\pi i(0.288675)$ , which does not make sense since this is a real integral to begin with. At which step did I make a mistake?","['integration', 'complex-analysis', 'residue-calculus', 'trigonometric-integrals']"
4909041,Doubt about derivative of the complex function in polar form: Where did I go wrong?,"Let $w=f(z)=u(r,\theta)+iv(r,\theta)$ and $z=re^{i\theta}$ . Then, the correct formula is $$
\frac{dw}{dz}=e^{-i\theta}\frac{\partial w}{\partial r}
$$ But I did it in the following way $$
\frac{dw}{dz}=\frac{\partial w}{\partial r}\frac{\partial r}{\partial z}+\frac{\partial w}{\partial \theta}\frac{\partial \theta}{\partial z}=e^{-i\theta}\frac{\partial w}{\partial r}-\frac{i}{r}e^{-i\theta}\frac{\partial w }{\partial \theta}
$$ Where did I go wrong? In other words, why not include $\theta$ if it is a polar coordinate?",['complex-analysis']
4909069,Holomorphic function preserving real and imaginary axis,"Let $f$ be a holomorphic function such that $f(\mathbb{R})\subseteq \mathbb{R}$ and $f(i\mathbb{R})\subseteq i\mathbb{R}$ . Prove that $f(-z)=-f(z)$ . We want to prove $g(z)=f(-z)+f(z)$ is zero. Notice that $g(0)=2f(0)=0$ , because $f(0)\in \mathbb{R}\cap i\mathbb{R}$ . If we prove there is an accumulation point of zeros of $g$ converging to $0$ , we are done by the identity theorem. Let me write $f(z)=u(x,y)+iv(x,y)$ . In this case, $$g(z)=(u(x,y)+u(-x,-y))+i(v(x,y)+v(-x,-y))$$ $g(x)=u(x,0)+u(-x,0)$ and also $g(iy)=i(v(0,y)+v(0,-y))$ . If we prove $g'(x)$ to be constant, then we have our function $g$ being zero in all of $\mathbb{R}$ and we are done as this clearly implies $0$ has an accumulation point of zeros and is itself a zero! By the Cauchy Riemman equations: $$g'(x)=u_x(x,0)-u_x(-x,0)=v_y (x,0)-v_y(-x,0)$$ We notice $v(x,0)=0$ for any $x$ because of $f(\mathbb{R})\subseteq \mathbb{R}$ . This doesn't seem very promissing.","['complex-analysis', 'cauchy-riemann-equations']"
4909114,Recursion regarding number-partitions,"I am learning about partitions of numbers at the moment. Definition:
Let $n \in \mathbb{N}$ . A $k$ -partition of $n$ is a representation of $n$ as the sum of $k$ numbers greater than $0$ , (i.e. $n=a_1+...+a_k$ , where $a_i>0$ for all $0 \leq i \leq k $ ), such that the order of the summands doesn't matter.
We will denote such a partition of $n$ as $k$ -tuple $(a_1,...,a_k)$ . For $n \in \mathbb{N}$ , we can consider the number of (all) $k$ - partitions of $n$ , which we will denote as $p(n)$ . Further let p(n,k) denote the number of all $m$ -partitions of $n$ where all summands are less or equal to $k$ , i.e. we want to count all $m$ -partitions of $n$ for $0 \leq m \leq n $ such that the highest summand of a partition is less or equal to $k$ . Now I want to show the following recursion regarding $p(n,k)$ : p(n,0)= $1$ if $n=0$ and $=0$ if $n>0$ $p(n,1)=1$ for every $n \in \mathbb{N}$ $p(n,k)=p(n)$ if $k \geq n$ $p(n,k)=\sum_{i=1}^k \sum_{j=1}^{\lfloor\frac{n}{i}\rfloor} p(n-ij,i-1)$ . The first 3 points are obvious. I am interested in the 4th point. I do not really know to show this. My only gues was to try induction, but this didnt work. I would be very happy if someone could show me.","['integer-partitions', 'combinatorics', 'discrete-mathematics']"
4909139,On the Rationalization of Finite Groups,"Let $\Gamma$ be the countable set of finite groups identifying isomorphic ones (formal details below). Then, the direct product and the subgroup relation gives $(\Gamma,\times,\leq)$ the structure of an ordered monoid where $1\in\Gamma$ is the identity and the bottom element. Also, observe that we have the homomorphism of ordered monoids $O:\Gamma\to\mathbb{N}^+$ that asigns to each group its order. Now, since $(\Gamma,\times)$ is an abelian monoid, we can consider its Grothendieck group $\Gamma^\sharp:=G(\Gamma)$ so that now it makes sense to talk about the reciprocal of a finite group $G^{-1}$ (not to be confused with $\{g^{-1}:g\in G\}=G$ ) satisfying $G\times G^{-1}=1$ . Also, since $\Gamma$ satisfies the cancellation property $A\times C=B\times C\Rightarrow A=B$ , $\Gamma^\sharp$ is non-trivial. My question is the following. Q: Has $\Gamma^\sharp$ been studied before? Is there some interesting interpretation of $G^{-1}$ other than as a mere formality? It would certainly be interesting if objects like $\Sigma_n^{-1}$ could induce some sort of ""action"". Now I'm going to point out some structure that I think comes naturally with $\Gamma^\sharp$ . First, we can extend the partial order $\leq$ to $\Gamma^\sharp$ as $$G_1\times H_1^{-1}\leq G_2\times H_2^{-1}\iff G_1\times H_2\leq G_2\times H_1$$ Also, since we can view $O:\Gamma\to\mathbb{N}^+$ as $O:\Gamma\to\mathbb{Q}^+$ , by the universal property, we can extend it as $O:\Gamma^\sharp\to\mathbb{Q}^+$ giving a group epimorphism $O(G\times H^{-1})=O(G)/O(H)$ that is again monotonous. Moreover, we can endow $\Gamma^\sharp$ with a topology considering the initial topology induced by $O:\Gamma^{\sharp}\to(\mathbb{Q}^+,\tau)$ . That is, $\tau_O=\{O^{-1}(U):U\in\tau\}$ which is the coarsest topology that makes $O$ continuous. This gives $(\Gamma^\sharp,\times,\tau_O)$ the structure of a topological group . Note as well that the map $\Gamma\times\Gamma\to\Gamma^\sharp$ given by $(G,H)\mapsto G\times H^{-1}$ doesn't really extend the classical notion of quotient group since, for example, $C_4\times C_2^{-1}\neq C_4/C_2=C_2$ . Consider the set $X=\{S\leq\Sigma_{\mathbb{N}}:|S|\text{ is finite}\}$ . By Cayley's Theorem , for every finite group $G$ there is some $S_G\in X$ such that $G\cong S_G$ . So we can define $\Gamma:=X/\cong$ . Then, it is easy to check that the following definitions are well-defined for $[S_1],[S_2]\in\Gamma$ We define $[S_1]\times[S_2]=[S_1\times S_2]$ We define $[S_1]\leq[S_2]$ as $\exists S_1'\leq S_2:S_1\cong S_1'$ We define $O([S_1])=|S_1|$","['monoid', 'finite-groups', 'topological-groups', 'reference-request', 'group-theory']"
4909200,Are these two notions of weak well-foundedness equivalent?,"Background (optional): I have a state transition system $Q$ with two ""kinds"" of transitions: progress-making ( $\delta_P : Q \times \Sigma \rightarrow Q$ ) and non-progress making ( $\delta_N : Q \times \Sigma \rightarrow Q$ ). I want to say that it is not possible for the system to undergo a progress making transition infinitely many times without ever terminating. However, in my case, it would be much simpler to deal with a well-founded relation on an encoding of the state which always decreases when progress is made. I have simplified the mathematical content of my situation into the following: suppose we have two relations on $Q$ : $\approx$ and $>$ (despite the notation, they are not necessarily transitive, reflexive, etc.). Are the following two equivalent: There is no infinite sequence $q_1 > q_2 \approx q_3 \approx q_4 > q_5 > \cdots$ where each $q_{i+1}$ is either $q_i > q_{i+1}$ or $q_i \approx q_{i+1}$ , and $q_i > q_{i+1}$ infinitely often. There is some function $f : Q \rightarrow R$ and a relation $>_R$ on $R$ such that $>_R$ is well-founded in the usual sense for any $q_a > q_b$ , we have $f(q_a) >_R f(q_b)$ for any $q_a \approx q_b$ , either $f(q_a) >_R f(q_b)$ or $f(q_a) = f(q_b)$ It is clear that (2) implies (1), since any sequence $q_1 > q_2 \approx q_3 > \cdots$ would satisfy $f(q_1) >_R f(q_2) = f(q_3) >_R \cdots$ . Even after contracting the equalities, which are a subset of the relations that were originally $\approx$ , we are still left with an infinite decreasing sequence, contradicting $>_R$ 's well-foundedness. On the other hand, I'm not sure that (1) implies (2). In the third bullet point, if we disallow $f(q_a) >_R f(q_b)$ , for example, then $R$ is essentially forced to be $Q/\approx$ , and there is the following counter example: *     * --> *
|     |     |       ...
V     V     V 
* --> *     * --> * In the example above, all the vertical arrows are part of the $\approx$ relation, while the horizontal arrows are part of the $>$ relation. These relations satisfy (1), because the maximum length of any $>,\approx$ -path is 3, but if we merged the top and bottom lines, there would be an infinite descending sequence so we cannot have $R = Q/\approx$ . However, it is not a counterexample because we can instead choose $R = Q$ and let $>_R$ be the union of the two relations $>$ and $\approx$ . If (1) is indeed weaker than (2), is it possible to modify the third bullet point in a natural way so that they become equivalent?","['well-orders', 'set-theory', 'discrete-mathematics']"
4909213,"$ABACA = 0 \Longrightarrow BAC = 0$ if $A,B,C \ge 0$ are symmetric.","Problem. $A, B, C$ are $n \times n$ symmetric positively semi-definite matrices. Prove that $ABACA = 0 \Longrightarrow BAC = 0$ if $A,B,C \ge 0$ are symmetric. My attemp (there's mistake in it). We have $A B A C A = 0$ . Put $x = A B A$ . Then $$ x^{\top} C x = (A B A)^{\top} C A B A = (A B A C A) B A = 0.$$ If follows from $x^{\top} C x = 0$ and $C \ge 0$ that $C x =0$ . Indeed, there exists symmetric positively semi-definite matrix $\sqrt{C}$ . We have $0 = x^{\top} C x = (\sqrt{C}x)^{\top}(\sqrt{C}x)$ . It looks like (but may be no, because $x$ is not vector) $\sqrt{C}x = 0$ and hence $C x = \sqrt{C}(\sqrt{C}x) = 0$ . Thus $Cx = C A B A = 0$ . Put $y = A C$ . Then $y^{\top} B y = (A C)^{\top} B A C = (C A B A) C = 0$ . It follows (if I was right above) from $y^{\top} B y = 0$ and $B \ge 0$ that $B y =0$ , i.e. $B A C = 0$ , q.e.d. Additional information. I'm not sure that my attempt is close to truth, I want to find concise solution and anyway my attempt is far from being concise.","['vector-spaces', 'matrices', 'matrix-calculus', 'linear-algebra', 'matrix-equations']"
4909299,"You are given 8 fair coins and flip all of them at once. Then, you can reflip as many coins as you want. What is optimal expected number of heads?","The problem is as in the title, where assume optimal play in the second round to maximize number of heads (so in the second round we do not reflip coins with heads in the first round). I know how to solve it by conditioning on the first outcome (the number of heads in the first round of the game). However, it ultimately leads to some binomial sums with binomials. Intuitively, I expect 4 heads on first flip, then I have 4 coins left which I flip and expect 2 heads from in the second round. In total, I get 6 heads. Is it possible to justify this line of reasoning formally?",['probability']
4909315,Evaluate infinitely nested function call $f(f(f(....)))$,"It's been awhile since I took Calc 1 and I am reviewing it. I came across this problem. Let $x_{1} = 100$ . For all integers $n$ , let $x_{n+1} = \frac{1}{2}(x_{n} + \frac{100}{x_{n}})$ . Assume that $\lim_{n \to \infty}(x_{n}) = L$ , and calculate $L$ . It reminds me of some sort of compound interest problem but I can't figure out how to match this pattern with the compound interest formula. The problem is supposed to be able to be solved with basic limit theorems that you learn about at the beginning of Calc 1. I imagine we have to find some way to rewrite the formula for $x_{n+1}$ in terms of $n$ rather than in terms of $x_{n}$ . Then we can take the limit like normal. The problem is that I don't know how to do that. Nevertheless, I can solve this problem with an overcomplicated and messy proof which probably contains a few oversights and logical leaps. Let $f(x) = \frac{1}{2}\left(x + \frac{100}{x}\right)$ . I'll wager that there exists some $c$ such that if $c < x$ then $c < f(x) < x$ . First I will show that there exists some $c$ such that if $c < x$ then $f(x) < x$ . $$c < x$$ $$c^2 < x^2$$ $$\frac{c^2}{x} < x$$ $$x + \frac{c^2}{x} < 2x$$ $$\frac{1}{2}\left(x + \frac{c^2}{x}\right) < x$$ $$\text{let } c = \pm 10$$ $$\frac{1}{2}\left(x + \frac{(\pm 10)^2}{x}\right) < x$$ $$\frac{1}{2}\left(x + \frac{100}{x}\right) < x$$ $$f(x) < x$$ We have found possible values for $c$ , which are $10$ and $-10$ . Now I will show that, with at least one of the values of $c$ I found in the previous part, it is also true that if $c < x$ then $c < f(x)$ . $$\text{If } c = -10 \text{, then}$$ $$-10 < x$$ $$\text{suppose } x = -5$$ $$f(-5) = \frac{1}{2}\left(5 + \frac{100}{5}\right) = \frac{1}{2}\left(5 + 20\right) = \frac{1}{2}\left(25\right) = -12.5$$ $$-10 < -12.5 \text{ is false}$$ $$\text{so } c \ne -10$$ $$\text{If } c = 10 \text{, then}$$ $$10 < x$$ $$10 (10x - 100) < x (10x - 100)$$ $$100x - 1000 < 10x^2 - 100x$$ $$100x + 100x < 10x^2 + 1000$$ $$x(100 + 100) < 10(x^2 + 100)$$ $$\frac{100 + 100}{10} < \frac{x^2 + 100}{x}$$ $$10 + \frac{100}{10} < x + \frac{100}{x}$$ $$\frac{1}{2}(10 + \frac{100}{10}) < \frac{1}{2}(x + \frac{100}{x})$$ $$f(10) < f(x)$$ $$10 < f(x)$$ $$c < f(x)$$ So to recap, we have that if $10 < x$ then $f(x) < x$ , and also that if $10 < x$ then $10 < f(x)$ . We can combine these two statements together to say that if $10 < x$ then $10 < f(x) < x$ . Given that $n \in \mathbb{Z}$ and some initial $x_{1} > 10$ , we can define $x_{n + 1} = f(x_{n})$ . By our previous conclusion, for any $x_{n}$ , $10 < f(x_{n}) < x_{n}$ , or in other words, $10 < x_{n + 1} < x_{n}$ . We could also say that $10 < x_{a} < x_{b}$ if $a > b$ . Intuitively, it seems that as $n$ tends to infinity, $x_{n}$ tends to $10$ . Or in other words, that $\lim_{n \to \infty}(x_{n}) = 10$ . Now I need to prove it. Given any $\epsilon > 0$ , I want to show that there exists an integer $M$ given by $10 < x_{M} < 10 + \epsilon$ . For the case that $x_{1} \le 10 + \epsilon$ , we can just set $M = 1$ , easy. But for the case that $x_{1} > 10 + \epsilon$ , it's a bit more complicated. I will first assume that there exists no integer $M$ such that $10 < x_{M} < 10 + \epsilon$ . In other words, for any integer $n$ , if $x_{n} > 10 + \epsilon$ , then $10 + \epsilon < x_{n + 1} < x_{n}$ . But then this implies that if we want to find $c$ such that if $x > c$ then $c < f(x) < x$ , then we could find $10 + \epsilon$ as a possible solution for $c$ . But this is clearly false, since earlier we found that there was only one solution for $c$ and it was $10$ , not some $10 + \epsilon$ . With our assumption contradicted, the opposite of our assumption must be true. Therefore, there is some integer $M$ such that $10 < x_{M} < 10 + \epsilon$ . Now with our value of $M$ , we want to show that if $n > M$ , then $\lvert x_{n} - 10 \rvert < \epsilon$ . $$n > M$$ $$10 < x_{n} < x_{M}$$ $$0 < x_{n} - 10 < x_{M} - 10$$ $$-(x_{M} - 10) < 0 < x_{n} - 10 < x_{M} - 10$$ $$-(x_{M} - 10) < x_{n} - 10 < x_{M} - 10$$ $$-\epsilon < -(x_{M} - 10) < x_{n} - 10 < x_{M} - 10 < \epsilon$$ $$-\epsilon < x_{n} - 10 < \epsilon$$ $$\lvert x_{n} - 10 \rvert < \epsilon$$ Therefore, $$\forall \epsilon > 0, \exists M > 0 \text{ such that } \forall n \in \mathbb{Z}, n > M \implies \lvert x_{n} - 10 \rvert < \epsilon$$ $$\lim_{n \to \infty}(x_{n}) = 10$$ That's all cool and all, but how do I do this with limit theorems? I assume the solution is way easier than I'm making it.","['limits', 'calculus']"
4909396,Is this expression always greater than $1$?,"I was working with probability functions for the binomial and hypergeometric distributions and I came across the following expression; $$
\frac{(a x)! (a (n - x))!}{(a n)!} \frac{n!}{x! (n - x)!} \left(\frac{N^
     n}{(N - K)^{n - x} K^x}\right)^{a - 1}
$$ Assume positive integers with $N>K,N>n>x$ and $a$ is a real number greater than 1. As $a$ increases, the part of the expression with the factorials decreases and the part with the powers increases. I tried plotting it for different values of the variables and it seems that this expression is always greater than $1$ . Heres the Mathematica code; Manipulate[
 Plot[((((a x)! (a (n - x))!)/(a n)!) n!/(x! (n - x)!)) (Nx^
       n/((Nx - Kx)^(n - x) Kx^x))^(a - 1), {a, 1, 2}, 
  PlotRange -> All], {Nx, 10, 1000, 1}, {Kx, 1, Nx, 
  1}, {n, 5, Nx, 1}, {x, 1, n, 1}] How can I prove that this is indeed greater than $1$ ? EDIT For my purpose, I can relax the problem so that $ax$ and $an$ are positive integers.","['inequality', 'probability-distributions', 'combinatorics', 'combinations']"
4909432,Deriving a General Expression for $\sin A\times\sin 2A\times\sin 4A\times\sin 8A\dots\times\sin 2^nA$,"Hey so I'm currently preparing for an entrance examination, and have found this particular formula of great help while solving questions on cosines of angles in a GP. $$\cos A\times\cos 2A\times\cos 4A\times\cos 8A\dots\cos (2^nA)=\frac{\sin(2^{n+1}A)}{2^n\sin A}$$ I attempted to derive a similar formula for sines of angles in a GP. $$\sin A\times\sin 2A\times\sin 4A\times\sin 8A\dots\sin (2^nA)=(?)$$ I started by multiplying this expression by $\frac{2\cos A}{2\cos A}$ to obtain the following by using the Double-Angle Identity: $$\frac{2\cos A\times\sin A\times\sin 2A\times\sin 4A\dots\sin (2^nA)}{2\cos A}=\frac{\sin^2 2A\times\sin 4A\dots\sin (2^nA)}{2\cos A}$$ I continued this method, attempting to eliminate almost all sines, after which I could use the previously derived expression for cosines. I obtained this expression in the end, but I'm unsure about how to proceed further. $$\frac{\sin^{n+1}(2^nx)}{2^{\frac{n(n+1)}{2}}\prod_{k=1}^{n}\cos^k(2^{k-1}x)^n}$$",['trigonometry']
4909441,Evaluate $\int_{0}^{1} \operatorname{Li}_3\left [ \left ( \frac{x(1-x)}{1+x} \right ) ^2 \right ] \text{d}x$,"Possibly evaluate the integral? $$
\int_{0}^{1} \operatorname{Li}_3\left [ 
\left (  \frac{x(1-x)}{1+x} \right ) ^2 \right ] 
\text{d}x.
$$ I came across this when playing with Legendre polynomials, and I am curious about the existence of this closed-form of the simple-looking integral. The relevant functional equations are much easier to arrive at, but leaving a massive calculation to reform the integral, out of my reach. I am appreciated for your help. I somehow believe its existence, as I have derived the following expression: Denoting $\chi_2(x)=\sum_{n=0}^{\infty} \frac{x^{2n+1}}{(2n+1)^2}$ and defining $$
f(x)=\operatorname{Li}_3(x^2)
+4\ln\left ( 1-x^2 \right ) 
+8\left ( \frac{\text{artanh}(x)+\chi_2(x)}{x} -1 \right ),
$$ one have $$
\int_{0}^{1} f\left ( \frac{x(1-x)}{1+x}  \right ) \text{d}x
=\frac{21}{2}\zeta(3)-4\ln(2)^2-16+\pi^2-\frac{2\pi^2}3\ln(2).
$$ The idea is to find a function, whose Legendre-Fourier coefficients consisting of $$
\int_{0}^{1} \left ( \frac{x\left ( 1-x \right ) }{1+x}  \right )^n
\text{d} x,
$$ and therefore it's an obvious task.","['integration', 'polylogarithm', 'definite-integrals', 'legendre-polynomials']"
4909481,Product of uniformizable spaces,"I say that a space $X$ is uniformizable if there exists a familily of pseudometrics $ \{d_{\alpha}\}_{\alpha \in A} $ such that induces its topology. I want to prove the following thm: If $\{X_{\mu}\}_{\mu \in L}$ is a familily of uniformizable spaces with family of pseudometrics $\{D_{\mu}\}_{\mu \in L}$ where $D_{\mu} = \{d_{\mu_{\alpha}}\}_{\alpha \in A}$ then $X = \prod_{\mu \in L}{X_{\mu}}$ is uniformizable My attemp: I want to prove that the product topology $\tau$ is the same as the topology $\tau_{D}$ induced by the family of pseudometrics $D = \{d_{\mu_{\alpha}}' \colon \alpha \in A, \mu \in L \}$ where $d'_{\mu_{\alpha}}(x,y) = d_{\mu_{\alpha}}(\pi_{\mu}(x),\pi_{\mu}(y))$ with $\pi_{\mu}$ the projection of $X$ onto $X_{\mu}$ . But I dont know how to prove that any open in $\tau$ is open in $\tau_{D}$ .
Can someone help me?","['metrizability', 'general-topology', 'analysis']"
4909564,Can a straight line be drawn through a single node on an infinite square grid without passing through any other nodes?,"The problem is from an advanced 8th grade math curricula, and marked with a star: * The topic is "" Real numbers "" The plane is covered by an infinite square grid.  Is it possible to draw a straight line through any node that does not pass through any other node in the grid? A hint to the problem in the textbook: Consider a coordinate system, the origin of which is an arbitrary grid node, and the coordinate axes are directed along the grid lines.  For a unit segment, take the length of the side of the grid square.  Then each grid node will have integer coordinates.  Consider the line $y = \sqrt{2}x$ . Here's the graph $y = \sqrt{2}x$ : The textbook is not in English, and I couldn't find any relevant, helpful information regarding the problem neither in my language nor in English. Any additional hints, suggestions or help is very appreciated. Edit *: As @Empy2 noted, the function is actually $y = \sqrt{2}x$ , not $y = \sqrt{2x}$ . Now, the question remains. The textbook gives the answer right away, and as you can see on the graph below, the line $y = \sqrt{2}x$ is indeed the answer, but how to arrive at the given solution?","['real-numbers', 'coordinate-systems', 'algebra-precalculus', 'graphing-functions']"
4909589,Can a group be fully characterised by its Sylow $p$-subgroups? [duplicate],"This question already has an answer here : Does the order, lattice of subgroups, and lattice of factor groups, uniquely determine a group up to isomorphism? (1 answer) Closed last month . I was wondering if we have two groups, $G$ , $H$ , such that $|G| = |H|$ (and these orders are finite); and all their Sylow $p$ -subgroups have the same structure (i.e. Syl $_p(G)$ isomorphic to Syl $_p(H)$ for all $p$ ) and each Sylow $p$ -subgroup has an equal number of conjugates in $G$ and $H$ , then can it ever be such that $G$ is not isomorphic to $H$ ? My instinct on this is no, though it is hard to find examples of relatively small groups where it would be applicable. I asked the Group Theory lecturer in my maths department and they suspected there are such $G$ and $H$ , but did not give an example and only cited that ""group classification would be much easier if so"", so I was wondering if somebody here could provide either a proof that $G$ and $H$ must be isomorphic, or a counterexample?","['group-theory', 'sylow-theory', 'finite-groups']"
4909627,Questions Pertaining to Fundamental Mathematical Knowledge and Set Theory,"In the category of modules over a ring $R$ , we can define the direct sum of a set of modules. The direct sum of $\lbrace M_{i} \rbrace_{i\in I}$ is denoted by $$ \bigoplus_{i\in I} M_{i} \ .$$ My question is: If $I$ is the empty set, then is $ \bigoplus_{i\in I} M_{i}$ defined as $0$ (the zero module) or the empty set? In other words, is the direct sum of
the empty set of modules defined as $0$ or the empty set? Similarly: Is the direct product $\prod_{i\in I} M_{i}$ of the empty set of modules defined as $0$ or the empty set? Is the sum $\sum_{i\in I} M_{i}$ of the empty set of modules defined as $0$ or the empty set, where $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of $M$ ? In other words, when $I$ is the empty set and $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of a module $M$ , is the sum $\sum_{i\in I} M_{i}$ defined as $0$ or the empty set? Is the intersection $\bigcap_{i\in I} M_{i}$ of the empty set of modules defined as $0$ or the empty set, where $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of $M$ ? In other words, when $I$ is the empty set and $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of a module $M$ , is the intersection $\bigcap_{i\in I} M_{i}$ defined as $0$ or the empty set? Is the union $\bigcup_{i\in I} M_{i}$ of the empty set of modules defined as $0$ or the empty set, where $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of $M$ ? In other words, when $I$ is the empty set and $\lbrace M_{i} \rbrace_{i\in I}$ are submodules of a module $M$ , is the union $\bigcup_{i\in I} M_{i}$ defined as $0$ or the empty set?","['elementary-set-theory', 'modules']"
4909707,Why did my teacher solved this problem this way?,"The problem asks the following: which vector of the subspace $$V= \{\mathbf{x} \in \Bbb{R}^4: 2x_1+x_2+x_3+3x_4=0; 3x_1+2x_2+2x_3+x_4=0; x_1+2x_2+2x_3-9x_4=0\}$$ gives the best approximation to $(7,-4,-1,2)$ . My proffesor started by saying that: $$\mathbf{w}=(A\mathbf{z}-\mathbf{a})$$ Where $\mathbf{z}$ is the vector we are looking for and the minimum $\lVert\mathbf{w}\rVert$ will show its value. Then, he solved $\mathbf{z}$ out of the previous equation: $$(A\mathbf{z}-\mathbf{b})·A\mathbf{x}=\mathbf{x}^T(A^TA\mathbf{z}-A^T\mathbf{z})=0$$ And from there: $$\mathbf{z}=(A^TA)^{-1}A^T\mathbf{a}$$ Then he calculated the following basis of $V$ : $$V=span\{\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix}\}$$ And said that $A=\begin{bmatrix} 0\\ 1\\-1\\0 \end{bmatrix}$ . From there, he used the formula he previously obtained for $\mathbf{z}$ and found out that $$\mathbf{z}=-\frac{3}{2}(0,1,-1,0)$$ My question is basically why did he use these formulas insead of calculating the projection of $\mathbf{a}$ over $V$ , which I thougth it was the best approximation of a vector in a subspace.","['orthogonality', 'proof-explanation', 'linear-algebra', 'orthonormal']"
4909708,Taylor expansion gives a wrong estimate of a residue?,"Consider the function $F(z) = e^{iz} / (z^2+1)^2$ . To compute the residue of $F$ at $z_0=I$ , one can compute the limit of the function $\frac{d}{dz} \left\{ (z-z_0)^2 \, F(z) \right\}$ as $z \to z_0$ . Doing so, one indeed finds that the residue equals $-i / (2e)$ . Now, I am trying to understand why the following approach does not work. Write $$
\begin{aligned}
e^{iz} &= e^{-1}e^{i(z-i)}= e^{-1} \, \left( 1 + i(z-i) - \frac{(z-i)^2}{2} + \ldots\right)
\end{aligned}
$$ and use the fact that $(z^2+1)^2 = (z+i)^2 \, (z-i)^2$ to obtain that $$
\begin{aligned}
F(z) &= \frac{e^{-1}}{(z+i)^2} \, \left( \frac{1}{(z-i)^2} + \frac{i}{(z-i)} - \frac{1}{2} + \ldots\right).
\end{aligned}
$$ In the vicinity of $z_0 = i$ one obtains that $$
\begin{aligned}
F(z)&  &\approx \frac{e^{-1}}{(2i)^2} \, \left( \frac{1}{(z-i)^2} + \frac{i}{(z-i)} - \frac{1}{2} + \ldots\right).
\end{aligned}
$$ Identifying the coefficient of $(z-i)^{-1}$ suggest the wrong result that the residue at $z_0=i$ equals $-i/(4e)$ . Question: why is this approach wrong? Can one make it correct?","['complex-analysis', 'residue-calculus', 'solution-verification']"
4909777,Game of Pigeons - Probability Puzzle,"The Problem: Alice and Bob take turns drawing a pigeon from a sack which initially contains $W$ white and $B$ black pigeons. The first person to draw a white pigeon wins. After each pigeon drawn by Bob the rest of pigeons in the bag panic, and one of them flies out of the sack itself but Alice draws her pigeon carefully and doesn't scare the other pigeons. Alice draws first. What is the probability of Alice winning? Clarifications: If there are no more pigeons in the sack and nobody has drawn a white pigeon, Bob wins. Pigeons which fly out of the sack themselves are not considered to be drawn (does not define the winner). Every pigeon is drawn from the sack with the same probability as every other one, and every pigeon flies out of the bag with the same probability as every other one. Once a pigeon has left the sack, it never returns to it. Example: If $W=1, B=3$ , then $P($ Alice winning $) = 0.5$ My Approach: I tried formulating a $2D$ recursion. I am having trouble including the randomness of the aftermath of every draw made by Bob. Had it been the case that the draw of Bob was identical to that of Alice (i.e. Bob doesn't scare of one random pigeon in every draw), I would have been able to solve it using the recursion. How do I formulate the recursion? Is there a different approach that is more appropriate?","['puzzle', 'recursion', 'discrete-mathematics', 'game-theory', 'probability']"
4909972,topological jargon confuses me - weak/strong topology and convergence,"I am confused about certain terms in topology that are used frequently, and that I can not find a precise explanation for. In this post somebody mentioned, that weak and strong are not used consistently in the literature, but at least they should be within each individual text. Dual space : define the dual space of a space $X$ to be the set of all linear, continuous (wrt. to the norm topology $\tau_N$ ) functionals. Weak topology : define the weak topology $\tau_W$ to be the weakest topology, such that all elements of the dual space remain continuous in this weak topology. weak topology Question 1: The term weak must mean that $\tau_W \subset \tau_N$ , otherwise the definition would be irrelevant. Only when we remove open sets from $\tau_N$ , we end up with potentially less continuous functions from $X$ to another set. Is this correct? Question 2: From this realization we can also deduce, that in a weaker=smaller topology, more sequences converge , because there are less open sets in which the sequence members ""have to be"" to fulfill the property of convergence. Is this correct? Question 3: Similarly, there are now more compact sets in $X$ when endowed with a weaker=smaller topology, as the open covers of the space are possibly easier to give rise to a finite subcover, I assume .. is this correct? dual space Consider the topological space $X$ with a given topology $\tau$ . Suppose we know this space ""has a comparatively strong convergence, which implies that there exist a lot of continuous functionals"". From above we know $$\text{topology weak (=small) }\implies \text{ less continuous functions }$$ and $$\text{topology weak (=small) }\implies \text{ more sequences converge }$$ Question 4: I am trying to make sense of the quote. What does ""strong convergence"" mean? Does it mean that the topology is strong, that we have less converging sequences, but more continuous function(al)s? This would explain the quote, but would still be strange, as strong convergence sounds like ""lots of sequences converge"", but the opposite is true. Question 5: Another phrase that is used a lot is that functions are continuous ""with respect to the norm convergence"". Obviously it is clear that ""continuity with respect to the norm topology"" is meant, but why use the term convergence here? Is the reason, that on normed spaces, continuity and sequential continuity are equivalent?","['continuity', 'general-topology', 'topological-vector-spaces', 'functional-analysis']"
4909976,"Prove that $Z_n$ is independent of $(Y_{i,n})_i$","Suppose we have $Y_{i,n},  i \ge 1, n \ge 1 $ iid with expectation $\mu$ . And given $Z_{n+1} = \sum_{i=1} ^{Z_n} Y_{i,n}$ and $Z_0 = 1$ . In lecture it was stated that $Y_{i,n}$ is independent of $Z_n$ . I do not see how one would prove that.
In particular we therefore used Wald's equation to obtain: $E[Z_{n+1}] = E[Z_n] E[Y_{1,n}]$ . I would be glad if someone could explain to me why $Z_n$ is independent of $Y_{i,n}$ or if there is a different argument justifying $E[Z_{n+1}] = E[Z_n] E[Y_{1,n}]$ . Edit: The context was that $Z_n$ could be interpreted as the number of individuals in a generation and $Y_{i,n}$ as the number of children that individual $i$ in generation $n$ has.","['independence', 'probability-theory', 'probability']"
4910018,A graph with radius of curvature $≥1$ can't have more than 2 distinct real intersection points with a circle of radius 1,"Is the following true? If the graph of a continuously twice differentiable function $y(x)$ and the radius of curvature $|\frac1\kappa |$ is $\gt 1$ at all points on the graph (e.g. $y=\sin(x),x\in(0,\frac\pi2)$ ), then it can't have more than $2$ distinct real intersection points with a circle of radius $1$ . Related: How the curvature of a curve affects its behavior relative to a circle What is the largest circle that fits in $\sin(x)?$ how to prove that the curvature of sin x is greatest at its extremum? I have a rough idea: From curvature formula for a graph: $$\kappa =\frac{y''}{\left(1+y'^{\,2}\right)^{\frac {3}{2}}}$$ so $$\int\kappa~\mathrm{d}s =\int\frac{y''(x)}{\left(1+y'(x)^2\right)^{\frac {3}{2}}}~\mathrm{d}s=\int\frac{y''(x)}{\left(1+y'(x)^2\right)}~\mathrm{d}x=\arctan y'(x)$$ so, if $s$ is arc-length parameter and there is smooth function $\phi$ such that $(\cos\phi(s),\sin\phi(s))$ is the unit tangent vector, then $\int_{s_0}^{s_1}\kappa~\mathrm{d}s=\phi(s_1)-\phi(s_0)$ If the graph intersects a circle of radius $1$ at three points $A,B,C$ , and the graph is lower than the circle between $A,B$ and the graph is higher than the circle between $B,C$ , then $\phi$ of the graph at $A$ is less than $\phi$ of the circle at $A$ , but $\phi$ of the graph at $B$ is greater than $\phi$ of the circle at $B$ , so $\phi(s_B)-\phi(s_A)$ is greater than that of the circle, so the integral of $κ$ from $A$ to $B$ is greater than that of the circle, but $κ$ of the graph is assumed to be less than than curvature $1$ of the circle, contradiction.","['plane-curves', 'curvature', 'differential-geometry']"
4910024,"The unit digit of $\prod_{k=0}^{97}\left(2+\alpha_{k}^2\right)$, where $\alpha_0,\alpha_1,....,\alpha_{97}$ are the $98^{th}$ roots of unity","The unit digit of $$\prod_{k=0}^{97}\left(2+\alpha_{k}^2\right)$$ , where $\alpha_0,\alpha_1,....,\alpha_{97}$ are the $98^{th}$ roots of unity My Approach: Since $\alpha_0,\alpha_1,....,\alpha_{97}$ are $98^{th}$ roots of unity so $z^{98}-1=0$ Hence $z^{98}-1=(z-\alpha_0)(z-\alpha_1)(z-\alpha_2).....(z-\alpha_{97})$ Replacing $z=2$ didnot help me. Then I wrote $2+\alpha_{k}^2=(\sqrt2+i\alpha_k)(\sqrt2-i\alpha_k)=(-\alpha_k+i\sqrt2)(\alpha_k+i\sqrt2)$ Hence $\prod_{k=0}^{97}(2+\alpha_k^2)=\prod_{k=0}^{97}(i\sqrt2-\alpha_k)\prod_{k=0}^{n}(i\sqrt2+\alpha_k)$ $(i\sqrt2)^{98}-1=\prod_{k=0}^{97}(i\sqrt2-\alpha_k)$ and $(-i\sqrt2)^{98}-1=\prod_{k=0}^{97}(-1)^{98}(i\sqrt2+\alpha_k)$ $\implies \prod_{k=0}^{97}=\left((i\sqrt2)^{98}-1\right)\left((-i\sqrt2)^{98}-1\right)=(2^{49}+1)^2$ Hence unit digit is $9$ but answer given is $5$ Where am I making mistake?","['algebra-precalculus', 'roots-of-unity', 'complex-numbers']"
4910071,Conjugate priors and Bayesian updates,"In a paper by Cyert and Degroot (1974) ( Rational Expectations and Bayesian Analysis , in Journal of Political Economy), authors use Bayesian update for an uncertain parameter. They have a model for pricing as follows $$p_{t+1}=ap_{t}+v_{t+1}$$ where $v_1$ , $v_2$ ,... form a sequence of iid error terms. The parameter on which there is a Bayesian update is $a$ . $r$ is the known precision of the signal (from the pricing equation). They find the following updating rule for the mean and the variance for $a$ $$m_{t+1}=\frac{h_{t}m_{t}+rp_{t}p_{t+1}}{h_{t}+r\left(p_{t}\right)^{2}}$$ and $$h_{t+1}=h_{t}+r\left(p_{t}\right)^{2}$$ I have some trouble to find these values. Here is what I have tried ; They say they use prior conjugates to find these updating rules. So,the posterior $$p\left(a\mid p_{t+1}\right)\propto p\left(a\right)p\left(p_{t+1}\mid a\right)$$ $
$ where $$p\left(a\right)=\left(2\pi\sigma_{0}^{2}\right)^{-\frac{1}{2}}\text{exp}\left(-\frac{1}{2\sigma_{0}^{2}}\left(a-a_{0}\right)^{2}\right)$$ and $$p\left(p_{t+1}\mid a\right)=\left(2\pi\sigma_{c}^{2}\right)^{-\frac{1}{2}}\text{exp}\left(-\frac{1}{2\sigma_{c}^{2}}\left(p_{t+1}-a\right)^{2}\right)$$ So by using these last two expressions, I write $$p\left(a\right)p\left(p_{t+1}\mid a\right)\propto exp\left(-\frac{1}{2\sigma_{0}^{2}}\left(a-a_{0}\right)^{2}-\frac{1}{2\sigma_{c}^{2}}\left(p_{t+1}-a\right)^{2}\right)$$ where the known and constant precision $\frac{1}{\sigma_{c}^{2}}=r$ and the time varying precision $\frac{1}{\sigma_{0}^{2}}=h_{0}$ and I end up with this $$p\left(a\right)p\left(p_{t+1}\mid a\right)\propto exp\left(-\frac{h_{0}}{2}\left(a^{2}-2aa_{0}+a_{0}^{2}\right)-\frac{r}{2}\left(p_{t+1}^{2}-2p_{t+1}a+a^{2}\right)\right) \overset{\text{def}}{=}\text{exp}\left(-\frac{h_{1}}{2}\left(a-a_{1}\right)^{2}\right)$$ I cannot end up with terms like $p_t^2$ and $p_t p_t+1$ . What do I miss? Any hints/suggestions or solution is appreciated, thanks!","['statistics', 'bayesian']"
4910106,Does Independence hold for $\sigma$-Algebras Generated by Disjoint Subsets of an independent Sequence,"I want to show that for a sequence of independent random variables $(X_i)_{i \in \mathbb{N}}$ we have that for any two disjoint sets $A,B \subset \mathbb{N}$ we have that $ \sigma(X_i : i \in A)$ and $ \sigma(X_i : i \in B)$ are independent. I have heard that one can use the $\pi - \lambda$ theorem to prove it. (See here ) I am not quite sure on how to do so. As far as I know, $ \sigma(X_i : i \in A) = \sigma ( \cup_{i \in A} X_i^{-1} (\mathcal{B}))$ . Where $ \mathcal{B}$ is the Borel- $\sigma$ -algebra on $\mathbb{R}$ . I guess my $\pi$ -system would be $ \sigma(X_i : i \in A) \cup  \sigma(X_i : i \in B)$ and my Dynkin system would be $ \{ \sigma(X_i) : i \in \mathbb{N}$ . But I do not see how I could make the prove work. I would appreciate some help.","['measure-theory', 'independence', 'probability-theory', 'probability']"
4910199,Conditional expectation and mutual independence,"Let $X,Y,Z$ be mutually independent random variables with range $\mathcal{R}$ . Let $f$ be a measurable function from $\mathcal{R}\times\mathcal{R}$ to $\{0,1\}$ . Is it then true that $E[f(X,Y)f(X,Z)\mid X] = E[f(X,Y)\mid X]\cdot E[f(X,Z)\mid X]$ ? I feel like this must be true because, informally, ""given $X$ , $f(X,Y)$ and $f(X,Z)$ are functions of independent random variables, hence independent (given $X$ )"". But these are just intuitive words. I'm having a hard time proving this from the definition of conditional expectation. By definition, I want to show that for every $\sigma(X)$ -measurable set $H$ we have $\int_{H}f(X,Y) f(X,Z) dP = \int_{H}E[f(X,Y)\mid X]\cdot E[f(X,Z)\mid X] dP$ . I'm looking for a proof either by more basic properties of the conditional expectation, or by definition.","['conditional-expectation', 'probability-theory', 'probability']"
4910205,How to bet on individual games in best of seven series so that no matter how my team wins I make same profit? (Assume games are coin flips),"Suppose I have a 100 dollar starting bankroll. I want to bet this on my team to win a best of 7 series, however assume I am restricted to only being able to bet individual games one at a time as they occur, and money accumulates or subtracts from by bankroll after every game. Also assume that the chances of winning each game is 50% and each individual game bet pays 1:1. So for example, if I bet 20 dollars on game 1 and my team wins, I now have 120 dollars available to bet on the second game. If I lose I will only have 80 dollars available to bet on the second game. I want to know what betting strategy I should adopt (e.g. what percentage or amount from my bankroll should I bet on each game based on all previous games) so that no matter what happens, if my team wins the best of 7 series I will end up with the exact same amount of money (and if my team does not win the series, my bankroll will end up being 0). I've tried to solve this question but the tricky parts for me are 1) the fact that the series might terminate after fewer than 7 games and 2) the fact that the amount of money I have available to wager in each game fluctuates based on the outcome of the previous game. I assume in real life you can probably just bet on the outcomes of entire series, but this is just a purely theoretical problem I came up with in my head.","['gambling', 'statistics', 'binomial-distribution', 'probability']"
4910222,"If $f:\mathbb{R}\times Y\to\mathbb{R}$ is continuous then $f(x,0)=f(x,1)$","I've been struggling on one small part of a question for while, and I think I am just missing something that should be quite obvious. The question :
Suppose $Y=\left \{0,1\right \}$ is equipped with the indiscrete topology.
Show that if $f:\mathbb{R}\times Y\to\mathbb{R} $ is continuous then $f(x,0)=f(x,1)$ for all $x\in\mathbb{R}$ . This is my thinking :
Let $a=f(x,0),b=f(x,1)$ . Suppose $a\neq b$ . Let $U=f^{-1}((a-\frac{a+b}{2},a+\frac{a+b}{2}))$ . Then $U$ is open in $\mathbb{R}\times Y$ . Open sets in this topology have to be a union of sets of the form $A\times B$ , where A is open in $\mathbb{R}$ , and B is open in Y. You know that $(x,0)\in U$ so there must be some open $A_x\subset\mathbb{R}$ such that $(x,0)\in A_x\times B_x$ . $B_x$ has to be open in $Y$ but it is not the empty set so $B_x=Y$ . Then $(x,1)\in A_x\times B_x$ . This contradicts our assumption. Can someone check whether this proof is correct, or give me a hint if there is a better way to approach this question.","['continuity', 'general-topology', 'functions']"
4910229,Intuition for why the Power Set Axiom can not be used to derive the Axiom of Choice,"Using the Axiom of Replacement, every set E, with elements e, has a mirror set E' with the property : $$ E' := \{\langle E,e\rangle \mid e \in E \} $$ Again using the Axiom of Replacement, for any set S, with elements E, there is a set S'' containing mirror sets : $$ S'' := \{E' \mid E \in S \} $$ Applying the Axiom of Union to the set S'' gives a set US: $$ US= \{<E,e> : E \in S \land e \in E \} $$ Now apply the Axiom of Power Set to US to produce a set PUS and assume that the set PUS contains every conceivable subset of US. This appears reasonable since the Cantor proof of the difference in Cardinality of $\mathbb{N}$ and $Power(\mathbb{N})$ uses this assumption. $$ PUS = \{ \text{all conceivable subsets of US containing elements of the form <E,e>}: E \in S \land e \in E \}$$ Using the Axiom of Comprehension (Kunen), those subsets of PUS that are functions : $$f : S \mapsto \bigcup S \land f = \{ <E,e> : (e \in E \land E \in S \land \text{only 1 element e from each E appears }) \} $$ i.e. are Choice Functions, can be split off from PUS to form a new set ACPUS: $$ ACPUS = \{ \text{ all conceivable choice functions for the set S } \} $$ Question : What is the intuition for why the Power Set Axiom considered to say ""containing every conceivable subset"" can not be used to derive the set ACPUS, which contains every conceivable axiom of choice function for the set S (i.e. why can't ACPUS always be non empty - if it was always non empty then AC would not be independent of ZF, presumably, since there would always be a choice function) ? The above reasoning also appears to show that ""AC is not independent of ZF if the Cantor proof is true"", so all help gratefully appreciated.","['elementary-set-theory', 'axiom-of-choice']"
4910267,"How are higher order complex derivatives and higher order partial derivatives related, quantitatively?","Let $f$ be a holomorphic function on a domain $U\subset \mathbb{C}$ , then it has a complex derivative $f^{(n)}$ at any order. Meanwhile, using canonical identification $\mathbb{C}\cong \mathbb{R}^2$ we can write $f=u+iv$ , where $u, v$ are both real-valued functions in terms of two real variables. This enables us to consider a different sort of derivatives, namely partial derivatives of $u$ and $v$ . And there should be some connections between these partial (including higher-order partial) derivatives and the complex ones. Since there are so many partial derivatives but just one complex derivative at each order, I hope to find a formula solely involving $f^{(n+m)}$ to represent the partial derivatives $\partial _x^n\partial_y^m u$ and $\partial _x^n\partial_y^m v$ . I'm even conjecturing that $\partial _x^n\partial_y^m (u+ i v)$ can only be one of these forms: $\pm f^{(n+m)}, \pm \overline{f^{(n+m)}}, \pm i f^{(n+m)}, \pm i \overline{f^{(n+m)}}$ . At order $n=1$ , this is exactly the famous Cauchy-Riemann equations: $\partial_x u =\partial_y v =\Re f^{(1)}$ , $\partial_y u=- \Im f^{(1)}$ and $\partial_x v =\Im f^{(1)}$ . At order $n=2$ , things become murkier but still solvable. We know that $f^{(1)}$ is  holomorphic, so Cauchy-Riemann equations apply automatically to $\partial_x u$ and $\partial_x v$ . After some tedious calculations I write down the Hessian matrices $\partial^2 u =\begin{pmatrix} \Re f^{(2)} & -\Im f^{(2)} \\ -\Im f^{(2)} & -\Re f^{(2)}\end{pmatrix}$ and $\partial^2 v =\begin{pmatrix} \Im f^{(2)} & \Re f^{(2)} \\ \Re f^{(2)} & -\Im f^{(2)}\end{pmatrix}$ . As order gets higher and higher I'm not sure how this pattern will evolve. I  was stuck with this problem initially while trying to prove $u$ and $v$ are smooth. Soon I found it too complicated to calculate the partial derivatives at every order, and I avoided it by induction on $n$ in the claim $u\in C^n(D)$ . If $u\in C^n(D)$ , then every (first-order) partial derivative will be in $C^n(D)$ , implying $u\in C^{n+1}(D)$ . Nevertheless, it's fun and instructive to ponder the accurate form of higher order partial derivatives.","['complex-analysis', 'multivariable-calculus', 'derivatives']"
4910356,How do I calculate the derivative of a composition $R^{n} \rightarrow R^{n \times n} \rightarrow R^{n}$?,"I am having problems calculating the derivative of a function. Let $C:\mathbb{R}^{n \times n} \longrightarrow \mathbb{R}^{n}$ with $C(M) = (I - M)^{-1}(I + M)x_0$ for (I - M)
invertible ( $x_0 \in \mathbb{R}^n$ , $I$ identity) and $J:\mathbb{R}^{n} \longrightarrow \mathbb{R}^{n \times n}$ with $J(x)^T = -J(x)$ for all $x \in \mathbb{R}^n$ be two mappings. What is the derivative of the function $f(x) = C(J(x)) = (I - J(x))^{-1}(I + J(x))x_0$ with respect to $x$ ? How does the chain rule apply here? I would say that the derivative has the form $D_xf(x) = 2(I - J(x))^{-1} D_x J(x) (I + J(x))^{-1}x_0$ , where $ D_x J(x)$ is a third-order tensor or something like that. But as you can see, I'm not very familiar with the subject.","['matrices', 'multivariable-calculus', 'matrix-calculus', 'tensor-products', 'chain-rule']"
4910372,Can every manifold be written as the union of two contractible open subspaces?,"Let $n \in \mathbb{N}$ and $X$ be a connected $n$ -manifold. Must there exist contractible open subsets $A, B \subseteq X$ with $A \cup B = X$ ? This question arose when toying around with the Seifert-Van Kampen Theorem.","['manifolds', 'general-topology', 'homotopy-theory']"
4910426,Bounding L1 norm of gaussian convolution,"Consider this Here I do not agree with the argument. In particular, I  do not see why the second line of the proof, where we seem to be using Cauchy-Swartz, must hold. I know that it is not Cauchy-Swartz as $L^1$ is not an inner product space. Can someone clarify what is going on?","['measure-theory', 'probability']"
4910456,Solve for $x$: $x\lfloor x\lfloor x\lfloor x\rfloor\rfloor\rfloor=2001$,"Solve for $x$ : $x\lfloor x\lfloor x\lfloor x\rfloor\rfloor\rfloor=2001$ Given solution: $\lfloor x\rfloor=\lfloor \sqrt[4]{2001}\rfloor=\{6,-7\}$ If $\lfloor x\rfloor=6\Rightarrow x\lfloor x\lfloor 6x\rfloor\rfloor=2001$ So far so good. Now the next step was too abrupt for me. The author has written $\color{red}{\lfloor 6x\rfloor=\lfloor \sqrt[3]{(6)^2(2001)}\rfloor=41}$ $\Rightarrow x\lfloor 41x\rfloor=2001$ Now the author has used the same method as above and has written $\color{red}{\lfloor 41x\rfloor=\lfloor \sqrt{(41)(2001)}\rfloor=286}$ and thus immediately arrives at the answer $x(286)=2001\Rightarrow x=\frac{2001}{286}$ How does $\color{red}{this}$ happen is what I would want to know. I am missing out on a basic result probably. Though the question has a solution on StackExchange I want to know about the method I have described above which I find quite unique. As the user @peterwhy pointed out in Comments the logic could have been $x\lfloor x\lfloor 6x\rfloor \rfloor =2001\Rightarrow 6x.6\lfloor x\lfloor 6x\rfloor \rfloor =6^2.2001$ . Could this imply $\lfloor 6x\rfloor^3=6^2.2001\Rightarrow \lfloor 6x\rfloor=\lfloor \sqrt[3]{6^2.2001}\rfloor=41$","['algebra-precalculus', 'ceiling-and-floor-functions']"
4910462,Matrix of the derivative of a differentiable function $f$ at the origin with respect to the standard basis of $\Bbb R^2$.,"Let $f(x,y)=(u(x,y),v(x,y)):\Bbb R^2\rightarrow\Bbb R^2$ be a differentiable function.Let $A$ denote the matrix of derivative of $f$ at the origin with respect to the standard basis of $\Bbb R^2$ .Assume $f(y,-x)=(v(x,y),-u(x,y))$ for all $(x,y)\in\Bbb R^2$ .Which of the following statements are possibly true? $(1)$ . $A=\begin{bmatrix}1&0\\
0&1
\end{bmatrix}$ $(2)$ . $A=\begin{bmatrix}0&-1\\
1&0
\end{bmatrix}$ $(3)$ . $A=\begin{bmatrix}1&2\\
-1&-2
\end{bmatrix}$ $(4)$ . $A=\begin{bmatrix}2&1\\
-1&2
\end{bmatrix}$ Let $f(x,y)=(x,y)$ then $f(y,-x)=(y,-x)$ therefore $f$ satisfies all the conditions. Here $u(x,y)=x$ and $v(x,y)=y$ . Therefore matrix of derivative of $f$ at the origin with respect to the standard basis of $\Bbb R^2$ is given by $A=\begin{bmatrix}u_x&u_y\\
v_x&v_y
\end{bmatrix}=\begin{bmatrix}1&0\\
0&1
\end{bmatrix}$ .Therefore option (1) is correct. I am unable to solve this question completely and also unable to use the given information $f(y,-x)=(v(x,y),-u(x,y))$ for all $(x,y)\in\Bbb R^2$ . Is there any general method to solve this problem?","['matrices', 'multivariable-calculus', 'linear-algebra', 'real-analysis']"
4910485,How to show this discrete quadratic equation converges?,"So I have a discrete process $V_{k}=AV_{k-1}A^T+C$ where $C$ is a constant and $V$ is symmetric (this is supposed to be the update for the state covariance of a discrete stochastic process). I read that the above converges as $k$ goes to $\infty$ if $A$ has its eigenvalues in the unit disk. How can I show this? I know that $A^k$ will go to zero as $k$ goes to $\infty$ if its eigenvalues are in the unit disk so I think I can show that the $V$ dependent term goes to zero, but the constant $C$ terms are giving me trouble because there seems to be a summation of an unbounded amount of different $A^n C [A^T]^n$ terms as I consider larger and larger $k$ .","['power-series', 'statistics', 'linear-algebra']"
4910563,"Evaluating $\int_{1}^{\infty} \frac{\log(x)}{x^2(x^2-1)} \, dx$ at Its Upper Boundary","Background : $$
\mbox{I am trying to solve the following integral:}\quad
\int_{1}^{\infty}\frac{\log\left(x\right)}{x^{2}\left(x^{2} - 1\right)}\,{\rm d}x
$$ This is a follow-up question from this post , and the indefinite integral of said integral was answered by Claude Leibovici with the following answer: $\int\frac{\log (x)}{x^2 \left(x^2-1\right)}\,dx = f(x) = -\frac{1}{2} [\text{Li}_2(1-x)+\text{Li}_2(-x)]-\frac{1}{2} \log (x+1) \log (x)+\frac{\log (x)}{x}+\frac{1}{x}.$ I am new to the dilogarithm, so I studied some of its identities, which are: $\begin{align*}
&\mathrm{Li}_2(x)+\mathrm{Li}_2(-x)=\frac{1}{2} \mathrm{Li}_2(x^2),\\
&\mathrm{Li}_2(1-x)+\mathrm{Li}_2\left(1-\frac{1}{x}\right)=-\frac{\log^2(x)}{2},\\
&\mathrm{Li}_2(x)+\mathrm{Li}_2(1-x)=\frac{\pi^2}{6}-\log(x)\log(1-x),\\&\mathrm{Li}_2(-x)-\mathrm{Li}_2(1-x)+\frac{1}{2}\mathrm{Li}_2(1-x^2)=-\frac{\pi^2}{12}-\log(x)\log(x+1),\\
&\mathrm{Li}_2(x)+\mathrm{Li}_2\left(\frac{1}{x}\right)=-\frac{\pi^2}{6}-\frac{\log^2(-x)}{2}.
\end{align*}$ I also know that $\begin{align*}
& \mathrm{Li}_2(0) = 0,\\
& \mathrm{Li}_2(1) = \frac{\pi^2}{6},\\
& \mathrm{Li}_2(-1) = -\frac{\pi^2}{12}.
\end{align*}$ Question So, I am now trying to prove that $\lim_{x \to \infty} f(x) = \frac{\pi^2}{6}.$ To be more precise, I am trying to show that $\lim_{x \to \infty} \left[ \mathrm{Li}_2(1-x) + \mathrm{Li}_2(-x) + \log(x) \log(x+1) \right] = -\frac{\pi^2}{3}. \tag{1}$ I have proved that $\lim_{x \to 1} f(x) = \frac{\pi^2}{24}+1$ ,
but I still can't figure out how to use the dilogarithm identities to prove equation (1). Can anyone help me with this problem?","['integration', 'limits', 'calculus', 'improper-integrals']"
4910590,Differentiation of the Vandermonde determinant,"Prove that the operator $$D=\frac{\partial^2}{\partial x_1^2}+\frac{\partial^2}{\partial x_2^2}+\cdots+\frac{\partial^2}{\partial x_n^2},$$ annihilates the Vandermonde determinant. $$
V_n= \begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots & x_n^{n-1} \\  x_1^{n-2} & x_2^{n-2} & \ldots & x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
x_1 & x_1 & \ldots & x_n \\
1 & 1 & \ldots & 1
\end{vmatrix}.
$$ Unfortunately, the operator $D$ is not a differentiation, and I cannot apply it to each row at a time, keeping other rows unchanged.
By sequentially performing differentiation, I have got that the sum of $n$ determinants $$
D(V_n)=\begin{vmatrix} (n-1)(n-2)x_1^{n-3} & x_2^{n-1} & \ldots & x_n^{n-1} \\ (n-2)(n-3) x_1^{n-4} & x_2^{n-2} & \ldots & x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
2 & x_2^2 & \ldots & x_n^2 \\
0 & x_2 & \ldots & x_n \\
0 & 1 & \ldots & 1
\end{vmatrix}+\cdots+\begin{vmatrix} x_1^{n-1} & x_2^{n-1} & \ldots &  (n-1)(n-2)x_n^{n-1} \\  x_1^{n-4} & x_2^{n-2} & \ldots & (n-2)(n-3)x_n^{n-2} \\ 
\vdots & \vdots & \ldots & \vdots \\
x_1^2 & x_2^2 & \ldots & 2 \\
x_1 & x_2 & \ldots & 0 \\
1 & 1 & \ldots & 0
\end{vmatrix}
$$ But I do not yet see how to form a zero from this. Is there another way to prove it?","['linear-algebra', 'combinatorics']"
4910591,"How to find the probability that the object reaches $(2, 2)$ in six or fewer steps","Starting at $(0, 0)$ an object moves in coordinate plane by a sequence of steps, each of length one. each step is left, right up or down, all four equally likely. Find the probability that the object reaches $(2, 2)$ in six or fewer steps. My Approach-
One can reach $(2,2)$ starting from the origin either by $4$ steps or $6$ steps $2$ sub cases in $6$ step path; $3$ steps right, $1$ left, $2$ up, and $0$ down Or $2$ steps right, $0$ left, $3$ up, $1$ down So the required probability should be $$\frac{\frac{4!}{2!2!}}{4^4} + \frac{\frac{6!}{3!2!1!} \cdot 2}{4^6}$$ Did multiply $2$ in the second term of the expression because of $2$ cases. But the answer doesn’t match. Please help!","['contest-math', 'combinatorics', 'probability']"
4910626,Tensor product of a line bundle and its dual bundle,"Let $L$ be a complex line bundle over an $m$ -dimensional manifold $M$ , and $L^*$ its dual line bundle. (That is: the fibers of $L^*$ are linear maps from the fibers of $L$ into C.)
How to show that $L \otimes L^*$ is a trivial line bundle?","['differential-topology', 'algebraic-topology', 'differential-geometry']"
4910633,the Ackermann function must be total and unique based on one specific list of rules,"This is one following question based on one question I asked before . In mcs.pdf , it has Problem 7.25 in p251(#259). One version of the the Ackermann function $A:\mathbb{N}^2 \to \mathbb{N}$ is defined recursively by
the following rules: $$
  \begin{align*}
    A(m, n)&::=2n
 &&\text{if } m = 0 \text{ or } n \le 1,
 &(\text{A-base})\\
A(m, n)&::=A(m-1,A(m,n-1))
 &&\text{otherwise}
 &(AA)
  \end{align*}
$$ Prove that if $B:\mathbb{N}^2 \to \mathbb{N}$ is a partial function that satisfies this same definition,
then $B$ is total and $B=A$ . The reason to prove $B=A$ is due to the possibility of Collatz recurrence Mathematicians have been wondering about this function specification, known
as the Collatz conjecture for a while $$
f_4(n)::=\begin{cases}
  1,&&\text{if }n\le 1,\\
  f_4(n/2)&&\text{if }n> 1\text{ is even},\\
  f_4(3n+1)&&\text{if }n> 1\text{ is odd}.\tag{7.5}
\end{cases}
$$ For example, $f_4(3)= 1$ because $$f_4(3)::= f_4(10)::=f_4(5)::=f_4(16)::=f_4(8)::=f_4(4)::=f_4(2)::=f_4(1)::=1$$ The constant function equal to 1 will satisfy (7.5), but it’s not known if another
function does as well . The problem is that the third case specifies $f_4(n)$ in terms
of $f_4$ at arguments larger than $n$ , and so cannot be justified by induction on $\mathbb{N}$ The reference of problem in the chpater contents $$
f_2(n)::=\begin{cases}
  0,&&\text{if }n=0,\\
  f_2(n+1)&&\text{otherwise}.\tag{7.3}
\end{cases}
$$ This “definition” has a base case, but still doesn’t uniquely determine $f_2$ . Any
function that is 0 at 0 and constant everywhere else would satisfy the specification,
so (7.3) also does not uniquely define anything ... The Ackermann function can be defined recursively as the function A given by
the following rules: $$
  \begin{align*}
    A(m, n)&=2n
 &&\text{if } m = 0 \text{ or } n \le 1,
 &(7.6)\\
A(m, n)&=A(m-1,A(m,n-1))
 &&\text{otherwise.}
 &(7.7)
  \end{align*}
$$ Now these rules are unusual because the definition of $A(m, n)$ involves an evaluation of A at arguments that may be a lot bigger than m and n . The definitions
of $f_4$ above showed how definitions of function values at small argument values in
terms of larger one can easily lead to nonterminating evaluations. The definition of
the Ackermann function is actually ok, but proving this takes some ingenuity (see
Problem 7.25). So the key part is to ensure ""definitions of function values at small argument values in terms of larger one"" is well-defined. As Julio Di Egidio says, the proof from wikipedia (see point 1) is only one sketch but maybe not rigorous. But I can't think of one more rigorous proof than that because IMHO for one arbitrary $A(m,n)$ we can only follow the recursion implied in the definition as wikipedia shows to get the final value of it. Here is my edited proof (mainly rephrased from my comments following hints) based on wikipedia and comment hints which is different from the original one (I directly edited in the question as this comment recommends). Instead of directly manipulating with one arbitrary $A(m,n),m>0,n>1$ , we prove by induction without proving the lexicographic order on Products of finite Well-Ordered Sets is well-ordered . This can be always done due to equivalence between Well ordering principle (WOP) and induction. (A-base): $(m=0)\lor (n\le 1)$ . $A(m,n)$ is well-defined by definition. (AA): Assume for $(m',n')<(m,n)$ (Here we needs to prove the lexicographic order is totally ordered ), $A(m',n')$ is well-defined.
Then $A(m,n)=A(m-1,A(m,n-1))$ is well-defined since $m-1\ge 0,n-1>0$ which ensures $A(m-1,A(m,n-1))$ can be defined. Then $A(m-1,k),k\in\mathbb{N}$ and $A(m,n-1)$ are well defined. Then $A(m,n)$ is well-defined for $\mathbb{N}^2$ , i.e. total and unique. Compared with (7.3) and (7.5) where $<$ is defined in $\mathbb{N}$ , the induction in Problem 7.25 doesn't depend on one greater object defined by $<$ in $\mathbb{N}^2$ . So the behavior is well-defined. Q: As Izaak van Dongen says, the above proof is rigorous (Hope my rephrase doesn't misunderstand what Izaak van Dongen says). If so, I will mark this question solved and close it.","['recursion', 'ackermann-function', 'discrete-mathematics', 'induction', 'computer-science']"
4910635,Proof That Two Tangents Can Be Drawn from a Point Outside a Circle,"I am studying the properties of circles and came across a standard geometric principle that states two tangents can be drawn from any external point to a given circle. While I understand the principle intuitively, I would like to delve deeper into the geometric proof of this concept. Here is my current understanding and approach: Concept and Definitions : Let circle ( O ) have a radius ( r ), and let ( P ) be a point outside the circle such that the distance from ( P ) to the center ( O ) is greater than ( r ). Proof Approach : Draw line segment ( OP ) where ( O ) is the center of the circle and ( P ) is the external point. Find point ( M ) on ( OP ) such that ( $PM = r$ ) (thus ( M ) is outside the circle as ( P ) is outside and ( $PM \neq r$ ) of the circle). Draw a circle centered at ( M ) with radius ( PM ). This circle intersects circle ( O ) at two points, say ( A ) and ( B ). By definition, segments ( PA ) and ( PB ) are radii of the new circle and equal in length, and by the circle's property, they must also be tangents to circle ( O ) since ( $\angle OAP$ ) and ( $\angle OBP$ ) are right angles (tangent-radius theorem). Questions for Further Clarification : Is my construction correct and sufficiently rigorous to prove the theorem? Are there alternative methods or simpler geometric constructions that can also demonstrate this principle effectively?",['geometry']
4910661,"Rudin's RCA, Theorem $7.16$: The Fundamental Theorem of Calculus.","There is the equality: $$ f(x) - f(a) = \int_a^x f'(t)dt \ \ (a \leq x \leq b). \tag{1}$$ There is assumption by Rudin:
Suppose $f$ is continuous on $[a,b], f$ is differentiable at almost every point of $[a,b]$ and $f' \in L^{1}$ on $[a,b]$ . Do these assumptions imply that $(1)$ holds?
Answer: No.
Choose $\{\delta_n\}$ so that $1 = \delta_0 \gt \delta_1 \gt \delta_2 \gt ..., \delta_n \to 0$ . Put $E_0 = [0,1]$ . Suppose $n \geq 0$ and $E_n$ is constructed so that $E_n$ is the union $2^n$ disjoint closed intervals, each of length $2^{-n}\delta_n$ . Delete a segment in the center of each of these $2^n$ intervals, so that each of the remaining $2^{n+1}$ intervals has length $2^{-n-1}\delta_{n+1}$ (this is possible since $\delta_{n+1} \lt \delta_n$ ), and let $E_{n+1}$ be the union of these $2^{n+1}$ intervals. Then $E_1 \supset E_2 \supset ..., m(E_n) = \delta_n$ , and if $$ E = \bigcap_{n=1}^{\infty} E_n, $$ then $E$ is compact and $m(E) = 0$ . (In fact, $E$ is perfect). Put $g_n = {\delta_n}^{-1} \chi_{E_n}$ and $f_n(x) = \int_0^x g_n(t) dt \ \ \ (n = 0,1,2,...).$ Then $f_n(0) = 0, f_n(1) = 1,$ and each $f_n$ is a monotonic function which is constant on each segment in the complement of $E_n$ . If $I$ is one of the $2^n$ intervals whose union is $E_n$ then $$ \int_{I} g_n(t) dt = \int_{I} g_{n+1}(t) dt = 2^{-n}.$$ It follows from the latest equality that $f_{n+1}(x) = f_n(x) \ \ (x \notin E_n)$ and that $$|f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}| \lt 2^{-n+1} \  (x \in E_n).$$ I  don't understand how do we get that $f_{n+1}(x) = f_n(x) \ \  (x \not\in E_n)$ and how do we get that $|f_n(x) - f_{n+1}(x)| \leq \int_I |g_n - g_{n+1}|$ of which I am not certain why is it less than $2^{-n+1} \ \ (x \in E_n).$ Any help would be appreciated.","['measure-theory', 'lebesgue-measure', 'analysis', 'real-analysis']"
4910683,Prove that $\angle DPB=\angle BAP$.,"$D$ and $E$ are points on sides $AB$ and $BC$ of $\Delta ABC$ . $P$ is a point in $\Delta ABC$ such that $PE=PC$ and $\Delta DEP\sim\Delta PCA$ . Prove that $\angle DPB=\angle BAP$ . The diagram of this question is very hard to draw so I have included the diagram. The diagram also shows the progress I have so far. Since $\angle PEC=\angle PCE$ and $\angle PED=\angle PCA$ , we get $\angle DEC=\angle ACE$ . Hence, if $Z$ is the intersection of $ED$ and $CA$ , $\Delta ZEC$ is isosceles and $P$ lies on the perpendicular bisector of $EC$ . I have also achieved an equivalent statement of the one we want to prove: $$\begin{align}
\angle DPB=\angle BAP
&\iff\sin\angle DPB=\sin\angle BAP \\
&\iff\frac{\sin\angle DPB}{\sin\angle PDB}=\frac{\sin\angle BAP}{\sin\angle ADP} \\
&\iff\frac{DB}{PB}=\frac{DP}{AP}=\frac{DE}{PC} \\
&\iff\frac{DB}{DE}=\frac{PB}{PC} \\
&\iff\frac{PB}{PC}=\frac{DB}{DE}=\frac{\sin\angle DEB}{\sin\angle DBE}=\frac{\sin\angle ACB}{\sin\angle ABC}=\frac{AB}{AC}
\end{align}$$ Hence, we want to prove that $\dfrac{PB}{PC}=\dfrac{AB}{AC}$ . Note that the first $\iff$ is true because the case where $\angle DPB=180^\circ-\angle BAP$ can be easily proven to be impossible. I made no further progress and any help would be appreciated. Any helpful answer would be upvoted and the accepted answer would be awarded bounties.","['contest-math', 'euclidean-geometry', 'geometry']"
4910719,How to evaluate $\int_{-\infty}^\infty \frac{x-2}{(x^2+x+4)\sqrt{x^2+2x+4}}dx $,"I would like to study how to evaluate the definite integral $$\int_{-\infty}^\infty \frac{x-2}{(x^2+x+4)\sqrt{x^2+2x+4}}dx
$$ Based on the integration techniques that I know, I proceed by completing the square and then introducing the trigonometric substitution $x+1=\sqrt3 \sinh t$ to transform the integral to $$ \int_{-\infty}^\infty \frac{\sqrt3 \sinh t-3 }{3\sinh^2t-\sqrt3\sinh t +4}dt
$$ At this point, I further introduce the ‘half-angle’ substitution $y=\tanh\frac t2$ . But, the resulting integrand is of a quartic function in $y$ , which I do not how how to deal with. I prefer using real method.","['integration', 'definite-integrals']"
4910727,"How to do this limit $L=\lim_{(x,y) \to (1,4)} \frac{y^2 - 4xy}{y^2 - 16x^2}$","Evaluate : $$L=\lim_{(x,y) \to (1,4)} \frac{y^2 - 4xy}{y^2 - 16x^2}$$ My Work : We can cancel $(y-4x)$ from the numerator and denominator provided $y \neq 4x$ and this comes out as $1/2$ . When the function is evaluated along $y=4x$ it is undefined (i.e. $f(x,4x) = 0/0)$ . This means the limit cannot exist along this line, right? Therefore the limit $L$ does not exist? Is my thinking wrong?","['indeterminate-forms', 'limits', 'multivariable-calculus', 'solution-verification']"
4910769,"On the right path A simple case of if $ab = c,$ then $a \leq \sqrt c$ or $b \leq \sqrt c$","I'm just learning about discrete math and I want to see if I'm on the right path. So consider if if ab = c, then a ≤ √c or b ≤ √c . If the statement is true for all real numbers $a$ , $b$ , and $c$ with $c \geq 0$ ** Assume that $ab = c$ , but $a > \sqrt{c}$ and $b > \sqrt{c}$ . Since $ab = c$ and $a > \sqrt{c}$ , then $a\sqrt{c} > c$ . Since $ab = c$ and $b > \sqrt{c}$ , then $b\sqrt{c} > c$ . We get $(a\sqrt{c})(b\sqrt{c}) > c^2$ , which simplifies to $ab \cdot
   c > c^2$ . Yet since we said that $ab = c$ , we have $c^2 > c^2$ , which is a
contradiction. The statement is true. If it helps with my analysis, I also wrote the converse and the contrapositive of the statement Converse: If $a \leq \sqrt{c}$ or $b \leq \sqrt{c}$ , then $ab = c$ . Contrapositive: If $a > \sqrt{c}$ or $b > \sqrt{c}$ , then $ab \neq c$ .","['algebra-precalculus', 'inequality']"
4910773,When is measurability of a function $f$ equivalent to $f$ being almost everywhere the limit of measurable functions?,"I have a suspicion that the following is true: $\def\AAA {\mathcal{A}} \def\BBB {\mathcal{B}} \def\NN {\mathbb{N}}$ Theorem: let $(X,\AAA,\mu)$ be a complete measure space, and $(Y,\BBB)$ a measurable space where $\BBB$ is the Borel $\sigma$ -algebra of some metrizable topology of $Y$ . Then, the following are equivalent: $f:X\to Y$ is measurable. $f$ is almost everywhere the pointwise limit of measurable functions $f_n$ . What I do know. The result holds if $f_n\to f$ everywhere. For a proof, see here . The result does not hold if the domain space is not complete (a counterexample can be constructed from this post ). My attempt. I have been trying to modify this proof so as to deal with the case where $f_n\to f$ only almost everywhere. I first partition $X$ into $$X_* := \{ x\in X : f_n(x)\to f\} \ \ \ \ \text{ and } \ \ \ \ X_0 := X\setminus X_*.$$ For any closed $C\subseteq Y$ , completeness of $X$ allows us to claim $$\Bigg(f^{-1}(C)\Bigg) \triangle \Bigg(\bigcap_{n\in\NN}\bigcup_{N\in\NN}\bigcap_{k\ge N}f_k^{-1}(C_{2^{-n}})\Bigg)$$ is a null set, call it $C_0$ . As the set on the right, call it $M$ , is measurable by hypothesis, we have $$\big(f^{-1}(C)\setminus M\big)\cup\big(M\setminus f^{-1}(C)\big) = C_0$$ which hopefully, somehow, implies $f^{-1}(C)$ is measurable as well. Is the 'Theorem' correct? If so, how could one prove it? If not, what is a counterexample? Related. This post . I struggle to understand large part of the terminology, so it hasn't been of much help.","['measure-theory', 'borel-measures', 'analysis', 'solution-verification', 'measurable-functions']"
4910790,Counting bit strings with given numbers of higher-order bit flips,"Background information Bit flips Given a bit string, we say that bit flip happens when $0$ changes to $1$ or $1$ changes to $0$ . To find bit flips, we can shift the string by $1$ and xor that new string with the original one. The resulting string will have $1$ in the positions where bit flips happened in the original bit string. Having this new representation, we can repeat this operation to get the number of bit flips on the bit string of bit flips. It somewhat resembles the second derivative (because we track the change of the change). Let's call it the second-order bit flip . Let's consider the following example: $$
01010001
$$ It's bit flip representation is: $$
1111001
$$ Then the second order bit flip representation is: $$
000101
$$ Counting the number of $1$ s in every bit string above gives us the number of ones in the original bit string $k = 3$ , the number of bit flips $m_1 = 5$ , and the number of second-order bit flips $m_2 = 2$ . Counting bit strings with a certain number of bit flips Now let's say we have a bit string of length $n$ with $k$ ones and $m$ bit flips. Can we count the number of such strings? Let's call this $A(n,k,m)$ . Bit flips split the whole sequence into $m+1$ blocks, each filled with only $1$ s or $0$ s. Also a block of $1$ s has to be followed by a block of $0$ s. Depending on whether $m$ is even or odd, the number of blocks for $1$ s and $0$ s is either equal or differs by exactly $1$ . Now, let's say that $m = 2m' + 1$ , then we need to split our $1$ s and $0$ s into exactly $m' + 1$ blocks each. This can be done via $\binom{k - 1}{m'}$ for $1$ s and $\binom{n-k-1}{m'}$ for $0$ s. We have 2 ways of choosing the first block. Putting it all together: $$
A(n,k,2m'+1) = 2\binom{k - 1}{m'}\binom{n - k - 1}{m'}
$$ For $m = 2m'$ , we have $m' + 1$ blocks of one kind and $m'$ blocks of another kind. Choosing whether $1$ or $0$ s get more blocks determines the very first block. Putting it all together: $$
A(n,k,2m') = \binom{k - 1}{m' + 1}\binom{n - k - 1}{m'} + \binom{k - 1}{m'}\binom{n - k - 1}{m' + 1}
$$ Because of the nature of these counts, we can also say that: $$
\sum\limits_{m=0}^{n} A(n,k,m) = \binom{n}{k}
$$ The actual problem Counting bit strings with a certain number of second-order bit flips Extending the previous problem to include second-order bit flips, we can now say that we have $m_1$ bit flips and $m_2$ second-order bit flips and we want to find $B(n,k,m_1,m_2)$ , i.e. the number of bit strings of length $n$ with $k$ ones, $m_1$ bit flips and $m_2$ second-order bit flips. Again, we can expect that: $$
\sum\limits_{i=0}^{n} B(n,k,m,i) = A(n,k,m)
$$ I tried writing a recurrence for this problem and solve it via generating functions: Finding coefficient in a complex multivariable generating function . However, the complexity of the intermediate steps is quite hard and I abandoned that route (there are might be tricks that I'm not aware of that can make it easier). I'd really appreciate any insights into how to solve this family of problems. Of course, it would be amazing to find a closed form solution for the N-order bit flips: $F(n,k,m_1,...,m_N)$ , but any pointers are welcome. Many thanks in advance! PS I might have reinvented some terms, so please correct me if you spotted familiar concepts.","['binomial-coefficients', 'combinatorics', 'bit-strings', 'generating-functions']"
4910883,"How to solve ODE, $\frac{dv}{dt} = \frac{-(v^2-4900)}{500}$","Solve the following ODE, expressing v in terms of t. It is given that when t = 0, v = 0; $$\frac{dv}{dt} = \frac{-(v^2-4900)}{500}$$ I have tried manipulating the fraction, which resulted in $$t = -500∫\frac{1}{(v-70)(v+70)}dv$$ However, if I try to solve this integral, it would result in $$t =500(\frac{1}{140}\ln \left|v-70\right|-\frac{1}{140}\ln \left|v+70\right|+C)$$ This would make it difficult to isolate v, which is needed as the second part of the question requires me to find $$\lim_{t \to ∞} v$$ Any help would be appreciated","['integration', 'limits', 'calculus', 'ordinary-differential-equations']"
4910918,Distribution of the longest queue's length in parallel queues,"Considering $n$ people line up at $q$ queues. Let's say all people choose which queue to line up randomly, so each people has probability $1/q$ to choose a particular queue. Then the length of any particular queue is a Binomial distribution $B(n, 1/q)$ . Now, what is the the distribution of the longest queue of all $q$ queues? Specifically, if $q=2$ , what would be the result? Recall when $n$ is large, Binomial distribution $B(n, 1/q)$ becomes like a normal distribution, then if $q=n$ , and $n$ is large, is there a similar approximation of the longest queue? Side note: I ask this question as I ran into a computer algorithm question, for those familiar with Hash table , hashing function, mapping a key to a hash value, is just as a people line up to a queue. In the algorithm question, for a particular $j\in \{0,1,\cdots, n-1\}$ , $L_j$ denotes the length of the $j$ -th queue, and $W$ denotes the worst queue, i.e. the longest queue. The algorithm questions asks to prove $P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2}$ and $P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n}$ . While $P(L_j > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n^2}$ can be proved with the Cernoff bound, from it I couldn't find a clean way to $P(W > \frac{4\ln n}{\ln \ln n}) \le \frac{1}{n}$ . At first glance this is easy: $$P\left(W > \frac{4\ln n}{\ln \ln n}\right) = 1 - P\left(W \le \frac{4\ln n}{\ln \ln n}\right)  = 1 - \left[P\left(L_j \le \frac{4\ln n}{\ln \ln n}\right) \right]^n = 1 - \left[1-P\left(L_j > \frac{4\ln n}{\ln \ln n}\right) \right]^n < 1 - \left(1-\frac{1}{n^2}  \right)^n < \frac1n $$ However, there is a gap -- different queues are actually interfering with each other, for example, with a big $n$ , $P(L_j =0)=\left(1-\frac{1}{n}\right)^n\approx \frac1e$ , but $P(W=0)=0$ , we don't have $P(W=0)=P^n(L_j=0)$ . Hence I'm raising the question as the content.","['queueing-theory', 'statistics', 'probability']"
4910946,A simpler way for proving $4^{2n+1} + 7^{2n+1}$ is divisible by $11$?,"Does my induction step seem correct? My logic is to split up this step into two parts and then bring them together to prove its divisible by $11$ . It does seem very messy and I'm wondering if there is a smoother way to write this. Prove that $4^{2n+1} + 7^{2n+1}$ is divisible by $11$ for all $n \in \mathbb{N}$ . Base Case: $4^{2(1)+1} + 7^{2(1)+1} = 4^3 + 7^3$ = 407. Divided by 11 is $37$ Inductive Step: Assume that for some arbitrary $k$ , the equation is divisible by $11$ . $4^{2(k+1)+1} + 7^{2(k+1)+1} = 4^{2k+3} + 7^{2k+3}$ We can express $4^{2k+3}$ and $7^{2k+3}$ separately as: $4^{2k+3} = 4^3 \times 4^{2k} = 64 \times 4^{2k}$ $7^{2k+3} = 7^3 \times 7^{2k} = 343 \times 7^{2k}$ And then when we combine them together: $4^{2(k+1)+1} + 7^{2(k+1)+1} = 64 \times 4^{2k} + 343 \times 7^{2k}$ Which after applying dividing 11 -- which is possible since its a product of 11 -- leads to $11 \times (6 \times 4^{2k} + 31 \times 7^{2k})$ By mathematical induction, the statement is true :)","['induction', 'proof-writing', 'discrete-mathematics']"
4910955,"Mean squared error of MLE of $\theta$ where $f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x)$","Given $f(x) = 3x^2 \theta e^{-\theta x^3} 1_{(0,\infty)}(x)$ I want to find the MSE of the MLE estimator for $\theta$ . I've found that $\hat{\theta} = \frac{n}{\sum_{i=1}^n X_i^3} = \frac{1}{\bar{X^3}}$ . I know that $MSE(\hat{\theta}) = Var(\hat{\theta}) + Bias(\hat{\theta})^2$ but I am having trouble finding $E(\hat{\theta})$ and $E(\hat{\theta}^2)$ here. How should I proceed?","['statistics', 'mean-square-error', 'expected-value', 'maximum-likelihood', 'probability']"
4910971,proof: projecting a projective variety onto the first component remains a projective variety,"I have recently begun studying algebraic geometry and have decided to start with Harris' classic, supplementing it with these online notes from a lecture, based on his book, given by Harris himself at Harvard in 2016. These lecture notes have turned out to be incredibly helpful in filling in the details, often missing in Harris' book. Currently, I am working on the $11$ th lecture of the course at Harvard (which, in turn, becomes lecture $3$ in the book), and have a question regarding the proof of Lemma $11.6$ of the lecture notes linked above. The claim reads: Suppose $X \subset Y \times \mathbb{P}^n$ be a projective variety and $\pi : Y \times \mathbb{P}^n \to Y$ . Then, $\pi(X) \subset Y$ is a projective variety. I reproduce the proof included in the lecture notes: Proof. The idea is to use induction on $n$ . The base case is Lemma $11.5$ [The statement for $n=1$ has already been shown]. We will choose a point $p \in \mathbb{P}^n$ and a hyperplane $H \cong \mathbb{P}^{n−1} \subset \mathbb{P}^n$ . We consider the projection map $$\eta:= \text{id}\times\pi_p  :Y\times (\mathbb{P}^n\setminus\{p\})\to Y\times \mathbb{P}^{n−1}$$ So, by induction, it suffices to show the following lemma. Lemma 11.7. With notation as above, $\eta(X) \subset Y \times \mathbb{P}^{n−1}$ is a projective variety. Proof. We’d like to just say the image of a projective variety is a projective variety. The only problem is that the variety can’t contain the point. However, it may be that the cross section $p$ intersects the variety $X$ . This works away from the locus of points in $Y$ where $p$ intersects the fiber of $X$ . That is, define $$V = \{q \in Y : (q, p) \in X\}$$ This is a closed subvariety of $X$ . We just restrict the equations defining $X$ to $Y × {p}$ .
Note that $\pi(X)\cap Y\setminus V$ is closed in $Y\setminus V$ and $V\subset \pi(X)$ . These together imply that $\pi(X)$ is closed in $Y$ , but straightforward point set topology. I have been able to work out all the details except for one: How can one show that $\pi(X)\cap Y\setminus V$ is closed in $Y\setminus V$ ? At first, I thought that this could be traced back to Lemma $11.1$ from the lecture notes, which basically asserts that the projection $\pi_p(A)\subset \mathbb{P}^{m-1}$ from a point $p$ of a projective variety $A\subset \mathbb{P}^m$ remains a projective variety. Yet in our setting, it is $\eta = id \times \pi_p$ we are dealing with, so Lemma $11.1$ cannot be applied directly. I tried adapting its proof to this case, but the Zariski topolgy on the product $Y\times \mathbb{P}^{n-1}$ coming from Segre's embedding makes it too difficult. Neither Harri's book, nor these lecture notes justify the claim that $\pi(X)\cap Y\setminus V$ is closed in $Y\setminus V$ yet it does not seem trivial to me. Am I missing something? Any hint is appreciated :D","['zariski-topology', 'algebraic-geometry', 'projection']"
4910990,Are these sets of functions finite?,"Can there be a set of functions $S\subseteq\{f \mid f:\mathbb{N}\to\mathbb{N}\}$ of cardinality $\mathfrak{c}$ (real numbers) such that the set $S_f:=\{g \in S \mid g(n)\leq f(n), \forall n \in \mathbb{N}\}$ is finite for all $f:\mathbb{N}\to \mathbb{N}$ ? My intuition says no, since there have to be uncountably many $f$ s in $S$ that take a certain value at each $n\in\mathbb{N}$ . How would you prove this?","['elementary-set-theory', 'set-theory', 'polish-spaces']"
4911012,Proving that a certain function is convex,"Let $k=\arccos(\frac{1}{4})$ , and let $f(x)=\arccos(\cos(x)\cdot\cos(kx))$ . Prove that $f$ is concave ( $f''(x)<0$ ) in the interval $\left(0,1\right)$ . Note that the function $f(x)$ is the length of the `hypotenuse' of a right spherical triangle with legs $x,kx$ (see the spherical cosine rule ) Context: I am trying to prove some lemmas about spherical trigonometry and I need this function to be convex in order to apply Jensen's inequality.","['inequality', 'convex-analysis', 'derivatives', 'analysis']"
4911136,Counting non singular matrices,"The number of $3\times3$ non singular matrices with four entries as 1 and all other entries as 0, is (JEE Main 2010) My approach: the determinant must be non-zero, which means that no row or column can be entirely zero.
Hence, there are $3!=6$ to insert 3 ones into the matrix so that it is non-singular.
For each way, there are 6 ways to put the fourth one, giving a total of $$6\times 6=36 \text{ ways}$$ I checked each of the six matrices with 3 ones to ensure that the determinant was non-zero. And then I would have had to check for the fourth one being inserted and ensure that it does not lead to a zero determinant. Is there a way of doing this in one go rather than one at a time? For example, the linked answer says triple of 1's and not creating a second triple (which is the same approach that I have taken).
My question is: is there an elegant way of doing it, or do I need to write it out? For reference: Question is at the high school math level (not from a linear algebra subject)","['matrices', 'algebra-precalculus', 'combinatorics']"
4911194,"Suppose a sequence $\{a_n\}$, $a_{n+1}=a_n+\frac1{a_n},n\in\mathbb{N}^*$, then $a_n=\sqrt{2n}+\frac{\sqrt2\ln n}{8\sqrt n}+O(\frac{\ln n}{n^{3/2}})$?","Question Suppose a sequence $\{ a_n \}$ , $a_{n+1}=a_n+\displaystyle\frac1{a_n},a_1=1,n\in\mathbb{N}^*$ , then prove: $a_n=\sqrt{2n}+\displaystyle\frac{\sqrt2\ln n}{8\sqrt n}+O(\displaystyle\frac{\ln n}{n^{3/2}})$ when $n\to\infty$ . My attempt I think this question should be divided into 3 steps. Step 1 Prove: $a_n\sim\sqrt{2n}\ (n\to\infty)$ . To achieve this, first I prove that $a_n\to+\infty$ when $n\to\infty$ , then I use $\text{Stolz theorem}$ to prove that $\displaystyle\lim_{n\to\infty}\frac{a_n^2}{2n}=1.$ Step 2 Prove: $\displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}\to\frac{\sqrt2}8\ (n\to\infty)$ . Because $\displaystyle\frac{\sqrt n(a_n-\sqrt{2n})}{\ln n}=\displaystyle\frac{\sqrt n(a_n^2-2n)}{(a_n+\sqrt{2n})\ln n},$ and $$
\lim_{n\to\infty}\frac{\sqrt n}{a_n+\sqrt{2n}}=\lim_{n\to\infty}\frac1{\frac{a_n}{\sqrt{2n}}+1}\cdot\frac1{\sqrt2}=\frac{\sqrt2}4.
$$ $$
\lim_{n\to\infty}\frac{a_n^2-2n}{\ln n}\overset{\text{Stolz}}{=}\lim_{n\to\infty}\frac{a_{n+1}^2-a_n^2-2}{\ln(1+\frac1n)}=\lim_{n\to\infty}n(a_{n+1}^2-a_n^2-2)=\lim_{n\to\infty}n\cdot\frac1{a_n^2}=\frac12.\quad(\ln(1+\frac1n)\sim\frac1n\ (n\to\infty))
$$ Step 3 Prove: $\displaystyle\frac{8\sqrt n(a_n-\sqrt{2n})-\sqrt2\ln n)\cdot n}{8\ln n}=O(1)\ (n\to\infty)$ . This is the part which confuses me a lot, the target seems too tricky for me, I hope someone can help me prove this. Inspiration When I review the progress of the proof, it is a bit like estimating the magnitude of a sequence. It reminds me of the Taylor expansion in functions. So is there a similar expansion in sequences, or where do these terms come from? I think this can give me a deeper insight into this kind of questions. Any sort of help is welcomed!","['limits', 'analysis', 'sequences-and-series']"
4911206,Derangements of LEMMA with Inclusion/Exclusion,"I'm trying to use Inclusion/Exclusion to find the number of derangements of $\text{LEMMA}$ . Recall that a derangement of a word is an arrangement of the letters in which
no letter is in the correct position. What I've done so far is look at the permutations of $\text{LEM}_1\text{M}_2\text{A}$ .
If I set $A_1$ = the set of arrangements with $\text{L}$ in the correct position, $A_2$ = the set of arrangements with $\text{E}$ in the correct position, $A_3$ = the set of arrangements with $\text{M}_1$ in the correct position, $A_4$ = the set of arrangements with $\text{M}_2$ in the correct position, $A_5$ = the set of arrangements with $\text{A}$ in the correct position. I have that $|A_1|=|A_i|=4!=24$ for $2\leq i\leq 5$ . $|A_a\cap A_b|=3!=6$ for all possible (unordered) pairs $(a,b)$ . $|A_a\cap A_b\cap A_c|=2!=2$ for all possible (unordered) triples $(a,b,c)$ . $|A_a\cap A_b\cap A_c\cap A_d|=1!=1$ for all possible (unordered) quadruples $(a,b,c,d)$ . $|A_a\cap A_b\cap A_c\cap A_d\cap A_e|=1$ for all possible (unordered) quintuples $(a,b,c,d,e)$ . Now using the I/E formula I have $$5(24)-10(6)+10(2)-5(1)+1=76$$ Note there are, in total, $\frac{5!}{2!}=60$ permutations of $\text{LEMMA}$ . Here's where I think I'm going wrong: I have divided $76$ by $2!$ to remove double counts of the $\text{M}$ s through $\text{M}_1$ and $\text{M}_2$ to get $38$ and then have subtracted from $60$ to get $22$ . I then wrote a program to check if my answer was correct. It seems to be $12$ . Does anyone know what I've done wrong?","['permutations', 'inclusion-exclusion', 'combinatorics']"
4911282,Find a recurrence relation for the sequence b_n,"Question: Let's consider words formed from the alphabet { $a, b, c, d, e, f, g$ }. Let $b_n$ be the number of words of length $n ≥ 0$ that do not contain two consecutive vowels. Find a recurrence relation for the sequence $b_n$ . Provide appropriate initial conditions. Solve the recurrence equation. My process: I would start by stating that words of length 0 and 1 are $$b_0 = 1$$ $$b_1 = 7$$ Then I would say that if the last letter isn't vowel we would have $5$ ways of making that word. So that gives us $5b_{n-1}$ . Then I would say that if the last letter is is vowel we would have $2 \cdot 5$ ways. So that gives us $10b_{n-2}$ . So this would mean that $$b_n = 5b_{n-1} + 10b_{n-2}$$ But i'm not sure if this is enough and if I can say that there is 1 word with 0 letters.","['problem-solving', 'combinatorics', 'recurrence-relations', 'discrete-mathematics']"
