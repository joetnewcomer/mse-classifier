question_id,title,body,tags
3427795,Find all $n≥1$ natural numbers such that : $n^{2}=1+(n-1)!$,Problem : Find all $n≥1$ natural numbers such that : $n^{2}=1+(n-1)!$ My try : $n=1$ we find : $1=1+1$ $×$ $n=2$ we find : $4=1+1$ $×$ $n=3$ we find : $9=1+2$ $×$ $n=4$ we find : $16=1+6$ $×$ $n=5$ we  find : $25=1+24$ $√$ Now how I prove $n=5$ only the solution ?,"['number-theory', 'elementary-number-theory']"
3427800,Hypergeometric Hypothesis Testing,"Suppose I have a jar with 9000 balls, each ball is either black or
  red. I pull a sample of 6000 and observe that 53% (3180) are red. I
  want to conduct a hypothesis test where $H_{0}:=$ Less than 50% of
  balls in the jar are red. However, I am willing to alter that hypothesis if there is a more reasonable way to formulate it that will 'essentially mean the same thing'. I have done some research to try and figure out the best way to go about this, and I discovered the hypergeometric test for over/under representation. The hypergeometric test uses the hypergeometric distribution to measure
  the statistical significance of having drawn a sample consisting of a
  specific number of $k$ successes (out of $n$ total draws from a population of size $N$ containing $K$ successes. In a
  test for over-representation of successes in the sample, the
  hypergeometric $p$ -value is calculated as the probability of randomly
  drawing $k$ or more successes from the population in $n$ total draws. In a test for under-representation,
  the $p$ -value is the probability of randomly drawing $k$ or fewer successes. According to this, I should take $N = 9000, k = 3180, n = 6000, K < 4500$ and then if $f(n,k)$ is the PDF of a hypergeometric distribution with $N = 9000$ and $K < 45000$ , I find my $p$ -value as $$P = \sum_{i = 3180}^{6000} f(i,6000).$$ Does this make sense? How do I handle the fact that $K < 4500$ , should I do the summation for each value of $K < 4500$ ? Or would it make sense to set $K = 4500$ Does my set up accurately reflect the hypothesis I set out to test? Should I alter my approach, or perhaps there is a better hypothesis I could formulate? I have almost no statistics background, just one or two classes when I was in undergrad, so I am not only lacking the ability to set up and solve this problem, but also need help interpreting the results and meaning. Thanks!","['statistical-inference', 'statistics', 'hypothesis-testing']"
3427802,"Find all the errors, if any, in the following L'Hospital's rule argument","Let $f(x)=e^{-2x}(\cos x+2\sin x)$ and $g(x) = e^{-x}(\cos x+ \sin x).$ Find all the errors (if any) in the following L'Hôpital's rule argument: $\lim\limits_{x\to \infty}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)}=\lim\limits_{x\to \infty} \dfrac{5}{2}e^{-x}=0.$ Here's my work. Recall the requirements for L'Hôpital's Rule: To argue that $\lim\limits_{x\to c}\dfrac{f(x)}{g(x)}=\lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)},$ the following must be true: $1.$ $f(x)$ and $g(x)$ are differentiable on an open interval $I,$ but not necessarily at some point $c.$ $2.$ $\lim\limits_{x\to c}f(x)=\lim\limits_{x\to c}g(x)=0$ or $\pm \infty.$ $3.$ $g'(x)\neq 0\;\forall x\in I, x\neq c.$ $4.$ $\lim\limits_{x\to c}\dfrac{f'(x)}{g'(x)}$ exists. We show that $f(x)$ and $g(x)$ are differentiable on $(-\infty, \infty).$ We have that $f'(x) = e^{-2x}(-2(\cos x+2\sin x) +(-\sin x+2\cos x))=-5e^{-2x}\sin x\;\forall x\in \mathbb{R}.$ Also, $g'(x)=e^{-x}(-(\cos x+\sin x)+(-\sin x+\cos x))= -2e^{-x}\sin x\;\forall x\in\mathbb{R}.$ Note that when $g(x)=0,\dfrac{f(x)}{g(x)}$ is undefined. This occurs when $\cos x + \sin x = 0\Rightarrow \tan x = -1\Rightarrow x = \dfrac{3\pi}{4}+2n\pi,n\in\mathbb{Z}.$ Let $x_0$ be such that $\tan x_0 = -1.$ We thus have that $f(x_0)=e^{-2x_0}(-\dfrac{\sqrt{2}}{2}+\sqrt{2})$ and $g(x_0)=0.$ Hence $\dfrac{f(x_0)}{g(x_0)}$ is indeterminate.  Also, consider when $x_1= \tan^{-1} \left(-\dfrac{1}{2}\right)+2n\pi.$ Then $\dfrac{f(x_1)}{g(x_1)}=\dfrac{e^{-2x_1}\left(\cos \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)-2\sin \left(\tan^{-1}\left(\dfrac{1}{2}\right)\right)\right)}{e^{-x_1}[\cos (\tan^{-1} (\frac{1}{2}))-\sin (\tan^{-1}(\frac{1}{2}))]}\\
=e^{-x_1}\dfrac{\frac{2}{\sqrt{5}}-\frac{2}{\sqrt{5}}}{\frac{2}{\sqrt{5}}-\frac{1}{\sqrt{5}}}=0.$ Hence $\dfrac{f(x)}{g(x)}$ is not indeterminate for all $x\in\mathbb{N}$ such that $x=\tan^{-1} (-\dfrac{1}{2})+2n\pi.$ Now consider $g'(x)=-2e^{-x}\sin x.$ $g'(x)=0$ whenever $\sin x=0$ as $e^{-x}\neq 0\;\forall x\in \mathbb{R}.$ Thus, $g'(x)=0\Leftrightarrow x=n\pi,n\in\mathbb{N}.$ So this is another error. From above, we have that $\lim\limits_{x\to \infty}\dfrac{f'(x)}{g'(x)}$ does not exist since it is undefined whenever $x=n\pi,n\in\mathbb{N}$ and equal to $\lim\limits_{x\to \infty}\dfrac{-5e^{-2x}\sin x}{-2e^{-x}\sin x}=\lim\limits_{x\to \infty}\dfrac{5}{2}e^{-x}=0$ whenever $x\neq n\pi.$","['calculus', 'proof-verification', 'derivatives']"
3427821,Is it possible to determine the given matrix is positive semidefinite under these conditions?,"Suppose I have a $2^n$ by $2^n$ symmetric matrix M. I know the following facts are true about $M$ . The diagonal of M is $n+1$ , which is strictly larger than any other non-diagonal entry. The sum of each row of the matrix M is exactly $2^n$ The value of non-diagonal entry cannot be smaller than $-n+1$ Each row contains the same elements (but the order is different so that $M$ is symmetric) I really hope to conclude $M$ is positive semidefinite, but I have to admit this may not be true. I know the fact that if each diagonal entry is greater than the sum of the absolute values of all other entries in the corresponding row, $M$ would be positive definite. However, we cannot use here because of fact 2. On the other hand, fact 2 implies $M$ has eigenvector 1 with eigenvalue $2^n$ and there cannot be too many negative entries in $M$ . I wonder if these conditions can be sufficient for me to draw such a conclusion. Courant fischer theorem seems helpful here since we can express the smallest eigenvalue as $\min_{v \perp 1} \frac{v^{T}Mv}{v^{T}v}$","['linear-algebra', 'positive-semidefinite']"
3427839,"Question about linear algebra in Benson's book ""Representations theory of elementary abelian groups and vector bundles""","The following proposition is in Benson's book: Proposition 4.1.2 Suppose $k$ is algebraically closed. Let $A_1,A_2\in M_{n\times m}(k)$ , regarded as maps from $k^m$ to $k^n$ . Then:
  (1) Suppose that forall $\lambda,\mu\in k$ , not both zero, $\lambda A_1+\mu A_2$ is injective. Then $$\bigcap_{(\lambda,\mu)\neq (0,0)}Im(\lambda A_1+\mu A_2)=0$$ (2) Suppose that forall $\lambda,\mu\in k$ , not both zero, $\lambda A_1+\mu A_2$ is surjective. Then $$\sum_{(\lambda,\mu)\neq (0,0)}Ker(\lambda A_1+\mu A_2)=k^m$$ This is very interesting proposition and the proof in the book is very beautiful. The proof in the book used field extension. I am wondering if this can be proved by methods of linear algebra. Any helps will be appreciated. The following is proof in the book: Th4.1.1 is that the rank is preserved when consider field extension and take algebraic independent variables.","['representation-theory', 'field-theory', 'algebraic-geometry', 'abstract-algebra', 'linear-algebra']"
3427905,A coin game and a crazy recurrence,"Consider the following game between two people. You start with a large stack of $n$ fair coins and flip all of them. For all coins that come up heads, toss those coins out the window and pass the remaining coins to the other player. Repeat this process until some player flips no heads. If a player fails to flip any heads, that player loses. Question: Given a (large) value of $n$ , determine which player is more likely to win this game. Let $P_n$ be the probability of losing this game when you go first and start with $n$ coins. Then $P_0=1$ (you can’t flip heads if there aren’t any coins) and $P_n$ satisfies the following recurrence: $$P_n=1-\frac{1}{2^n}\sum_{k=1}^n \binom{n}{k}P_{n-k}$$ While analyzing this, I found out pretty quickly that $P_n$ converges to $1/2$ , so I let $P_n=1/2+Q_n$ and found the following recurrence for $Q_n$ : $$Q_n=-\frac{1}{2^n}\sum_{k=1}^n \binom{n}{k}Q_{n-k}$$ Some initial values of $Q_n$ , starting with $Q_0=1/2$ , are $$\frac{1}{2},\space 0,\space -\frac{1}{4},\space \frac{1}{64},\space \frac{5}{256},\space ....$$ Of course, the sign of $Q_n$ determines which player the game favors. Running a Python script, I determined that $Q_n$ changes signs after $n=8,17,35,72,145,291,583$ (at which point the numbers got too big for my program to handle, since it was dealing with binomial coefficients). I know that these values at which sign changes occur are $\sim c\cdot 2^k$ for some constant $c$ . What I need help with: Can anyone calculate the value of $c$ , or come up with a clever way to calculate the sign of $Q_n$ for arbitrary large $n$ ? What I know already: If it helps, I’ve found out that the EGF (exponential generating function) of $Q_n$ , denoted $f$ , satisfies the following functional equation: $$f(2x)=(1-e^x)f(x)$$ However, I’m not sure how to use this fact. Also, I know that the expected duration of the game (that is, the number of turns before someone loses) is asymptotically $\log_2(n)$ for large $n$ . However, this fact is almost trivial because we expect half of the coins to come up heads (and be thrown out) after each turn.","['asymptotics', 'recurrence-relations', 'stochastic-processes', 'sequences-and-series', 'probability']"
3427915,What is the difference between complex numbers and 2D vectors?,"This is a follow-up to a previous question regarding complex numbers. Many people there compared complex numbers to vectors, and there was disagreement about what the difference was. Some options people have: Complex numbers and vectors are exactly identical. The $i$ in complex numbers changes the way it describes the space, if so, how and why? Complex numbers are 2D but can be used cleverly to describe larger dimensional systems. If so how and why? What are the differences?","['geometry', 'vectors', 'complex-numbers']"
3427946,Why the associative axiom of groups not stated as f * ( g * h ) * = ( f * g ) * h = ( f * h) * g?,"I am reading Alan Beardon's ""Algebra and Geometry"" and it says: (2) for all f , g and h in G , f * ( g * h) = ( f * g ) * h . (2) is called the associative law , and it says that f * g * h is uniquely defined regardless of which two operations * we choose to do first. The point here is that * only combines two objects at a time, and we have to apply it twice (in some order) to obtain f * g * h . There are exactly two ways to do so and the two must always yield the same results. The issue I have is that I do not see how is it that there are only two ways to combine two elements out of three . For as I understand I could make six different combinations (if I do not assume commutative property): (1) ( f * g ) * h (2) ( f * h ) * g (3) ( g * f ) * h (4) ( g * h ) * f (5) ( h * g ) * f (6) ( h * f ) * g And even if I assume that there is a commutative property, that I have not seen stablished in the definition so far, there would still be three different combinations: (1) ( f * g ) * h (2) ( f * h ) * g (3) ( g * h ) * f Can you please explain to me how is it that there are only two ways to apply a binary operation over three elements and thus defining the associative axiom shall be expressed as f * ( g * h) = ( f * g ) * h and not as ( f * g ) * h = ( f * h ) * g = ( g * h ) * f ?","['group-theory', 'abstract-algebra', 'combinatorics']"
3428023,"Evaluate $\int x^2\ln (1+x) \, dx$ as a power series: why is just $n$ ok?","This is a different question than the previous one I posed pertaining to the same textbook problem. I do not understand the justification in step seven for the exponent not changing. If you add $1$ to all the numbers that make up the set of $n$ , then you must cancel out this effect by turning every $n$ into $n-1$ so that the series remains the exact same. You must have $(-1)^{n-1}$ . What am I missing?","['power-series', 'sequences-and-series']"
3428066,Creating an Gaussian Orthogonal Matrix,"I am working on a project where I need to create a special type of orthogonal matrix, such that all of the rows and all columns are orthogonal, but each entry in the matrix is drawn from a Gaussian distribution with a mean of $0$ and variance of $1$ . Is this possible, and if so what would be an easy way to do it?","['normal-distribution', 'matrices', 'orthogonal-matrices', 'linear-algebra', 'random-matrices']"
3428150,Place the counters on a $4\times 4$ grid such that no column or row has the same color counters,"There is a $4\times 4$ grid posted on the wall. Find the number of ways of placing two identical red counters and two identical blue counters on four different squares of the grid such that no column or row has two counters of the same color. My solution: place the first blue counter $B_1$ : there are 16 ways. Next, place $B_2$ : there are 9 ways. After, I split into cases: -The first case: $R_1$ is placed not coinciding with the cross of $B_1$ or $B_2$ . -The second case: $R_1$ coincides with exactly one cross of $B_1$ or $B_2$ . -The third case: $R_1$ coincides with the crosses of $B_1$ and $B_2$ . I find my solution is quite long and complicated. Therefore, I want to refer to another solution. Please everyone help me.","['combinatorics', 'discrete-mathematics']"
3428165,The convergence of $\sum_1^{+\infty}b_n$ follows from the convergence of $\sum_1^{+\infty}a_n$ given that $\frac{a_n}{b_n}\to1$.,"It is true that if $$
\sum_1^{+\infty}a_n\qquad\text{and}\sum_1^{+\infty}b_n
$$ satisfies $$
\lim_{n\to+\infty}\frac{a_n}{b_n}=1,
$$ then the convergence of $\sum_1^{+\infty}b_n$ follows from the convergence of $\sum_1^{+\infty}a_n$ ? What I know is that if there are both positive series, this claim is true.",['analysis']
3428173,Special property of multiples of 6,"Can it be proved that if $n$ is a multiple of $6$ , then there exists a subset of the proper divisors of $n$ that add up to $n$ ?","['number-theory', 'elementary-number-theory']"
3428189,Wreath product is associative but regular wreath product is not associative.,"I am currently reading An Introduction of Theory of Groups by Rotman. In page 174, Theorem 7.26 states that wreath product is associative. But in the next page, regular wreath product $W=D \wr_r Q$ is defined when $Q$ -set acting on itself by left multiplication. Note that $|D\wr_r Q|=|D|^{|Q|}|Q|$ . So regular wreath product is not associative when all groups are finite because $$|T \wr_r (D \wr_r Q)|\neq |(T \wr_r D) \wr_r Q|$$ But if regular wreath product is a special case of wreath product, the operation should be also associative.","['wreath-product', 'group-theory', 'abstract-algebra']"
3428213,Find a restricted domain so that the function is injective,"The sinus function is not injective when the domain is the whole $\mathbb{R}$ . To find a restricted domain where the function is injective, can we do that only using the graph or is there also an other way? Such a resticted domain is for example $\left[-\frac{\pi}{2}, \frac{\pi}{2}\right ]$ . How can we find it?","['calculus', 'functions', 'trigonometry']"
3428214,"Is the converse of if $a=b$, then $f(a)=f(b)$ not necessarily true?","If $a=b$ , then $f(a)=f(b)$ is the definition of a function. However, its converse if $f(a)=f(b)$ , then $a=b$ is not necessarily true. The converse is only true if $f(x)$ is a ""one-to-one"" function, right? Edit: This is because if $f(x)$ is not a ""non-one-to-one"" function, then there will be multiple inputs that correspond (map to) the same output, so we cannot assume that $f(a)=f(b)$ implies $a=b$ since there will be multiple input values that satisfy the equation (produce said output). To assume so would ""miss"" the other solutions (input values). For example, $f(x)= x^2$ $x^2=2^2\longrightarrow x=2$ while true misses the other solution (input value) $x=-2$ which produces the same output. Is this correct?","['algebra-precalculus', 'functions']"
3428272,Evaluate $\sum_{n=1}^{\infty} \left( \frac{\pi}{6} - \int_{0}^{n} \frac{\sqrt{3} (2x+1)}{(x^2+x+3)^2+3} dx \right)$,"While I was taking some mathematics competition, I faced following problem, but I couldn't solve it.
The problem was: Let $\displaystyle f(x) = \frac{\sqrt{3} (2x+1)}{(x^2+x+3)^2+3}$ . Evaluate the sum: $$\sum_{n=1}^{\infty} \left( \frac{\pi}{6} - \int_{0}^{n} f(x) dx  \right).$$ I tried to solve this problem by first evaluating integral of $f$ .
Then, I obtained following sum, but I couldn't evaluate it. $$ \sum_{n=1}^{\infty} \left( {\pi \over 2} - \arctan({n^2+n+3 \over \sqrt{3}}) \right)$$ It would be very thankful if someone give me hint or solution to this problem. Note : This problem is from Korean College Mathematics Competition.","['calculus', 'summation', 'sequences-and-series']"
3428325,Does $\mathbb{R} \cong \mathbb{R}^2$ imply $\mathbb{N} \cong \mathbb{N}^2$?,"It's not like we can say that, because $\mathbb{N}$ is a subset of $\mathbb{R}$ , the restriction of a bijection between $\mathbb{R}$ and $\mathbb{R}^2$ is going also to be a bijection between $\mathbb{N}$ and $\mathbb{N}^2$ (although such a bijection may exist). There are other ways to prove that $\mathbb{N} \cong \mathbb{N}^2$ , but is it possible to show this using the fact that $\mathbb{R} \cong \mathbb{R}^2$ ?",['elementary-set-theory']
3428408,Proof of a Zeta function identity,"How do I show that $$
\sum_{n\geq 1} \frac{\zeta(2n)}{n(2n+1)}=\ln\frac{2\pi}{e}.
$$ I found this equation in my homework. I tried to integrate Zeta function's generating   function twice, but the result has Li function in it. Is there any simple method to prove it?",['complex-analysis']
3428417,"find a condition on B,C so set difference is associative (A\B)\C=A\(B\C)","Im trying to find a some conditions on the sets $B,C$ so the following will be correct: for all $A$ , $(A\setminus B)\setminus C= A\setminus (B\setminus C)$ if and only if    ** condition ** (set theory)",['elementary-set-theory']
3428424,Inner product on fiber of vector bundle over manifold.,"Fix $n\in\mathbb{N}$ . A vector bundle of rank $n$ is a smooth map $\pi:E\rightarrow B$ between manifolds such that $\forall p\in B: E_p := \pi^{-1}(p)$ is an $n$ -dimensional vector space and $\forall p\in B$ , there exists a neighborhood $U$ of $p$ and a diffeomorphism $\psi:E\mid_U:=\pi^{-1}(U)\rightarrow U\times \mathbb{R}^n$ such that $\operatorname{pr}_1\circ\psi = \pi$ and $\psi\mid_{E_q}:E_q\rightarrow\{q\}\times\mathbb{R}^n$ is a vectorspace isomorphism for all $q\in U$ . Suppose $E$ is a vector bundle over a manifold $M$ . (I suppose by this they mean there exists $\pi:E\rightarrow M$ as above). Prove that for all $x\in M$ one can construct an inner product (symmetric, positive definite, bilinear form): $g_x : E_x\times E_x\rightarrow \mathbb{R}$ which depends smoothly on $x$ . "" $g$ depends smoothly on $x$ "" means: $g(v,w)$ is a smooth function on $M$ for all smooth sections $v,w$ of $E$ . I have tried to construct this norm by using the standard norm on $\mathbb{R}$ . Take $x\in M$ . There is an open neighborhood $U$ of $x$ for which there exists a diffeomorphism $\psi$ as above. Let us define $g_x((a,b)) = <\operatorname{pr}_2(\psi(a)),\operatorname{pr}_2(\psi(b))>_\mathbb{R}$ . Since this is symmetric, $g_x$ will be symmetric. Since this is positive definite, $g_x$ will also be positive definite. Now for bilinearity. For $a_1,a_2\in E_x$ , we have $\psi(a_1)+\psi(a_2) = \psi(a_1+a_2)$ , since $\psi$ is an isomorphism. And since $\psi(a_i)\in\{x\}\times\mathbb{R}^n$ . $\operatorname{pr}_2(\psi(a_1)+\psi(a_2))=\operatorname{pr}_2(\psi(a_1))+\operatorname{pr}_2(\psi(a_2))$ . Since we have a vector space isomorphism we can also show this for scalar multiplication. I am unsure however what this field of scalars is exactly. Then for the final part I need to show that $g$ depends smoothly on $x$ . But here I am lost completely. How can we evaluate a section in $g$ ? As far as I know there is no correspondence between sections and elements of $E_x$ . A hint I was given is that we can use partitions of unity, but I have no clue how this ties in with what I have constructed.",['differential-geometry']
3428439,quadratic twists of $P_{\mathbb{Q}}^1$,"I have seen somewhere else that if $C$ is a conic in $P_{\mathbb{Q}}^2$ ,then it is a quadratic twist of $P_{\mathbb{Q}}^1$ . So I want to know how to define the quadratic twist of $P_{\mathbb{Q}}^2$ (maybe for a general variety?) and why is this curve isomorphic to $C$ (as an algebraic variety)? Thanks!","['algebraic-curves', 'algebraic-geometry']"
3428461,"Take twenty distinct integers not greater than 70: show that among their pairwise differences, at least four are equal.","I got this problem in a mathematics class. I've already seen how to solve it and fully understand the proof that is most commonly seen online (for example on this website ). However, I'm having trouble understanding whether my proof (which is simpler to my opinion) wouldn't work: Let's name A:= { $a_1$ , ..., $a_{20}$ } $\subset$ {1, ..., 70}  the set of distinct integers. Let's name D:= {| $a_i$ - $a_j$ | : 1 $\le$ i $\neq$ j $\le$ 20} $\subset$ {1, ..., 69} the set of pairwise differences By a simple combinatorial argument, we know that if there aren't any equivalences in the set D then card(D)= ${20 \choose 2}$ = 190 Therefore, if we show that we actually have card(D) $\le$ 190-4 = 186, we've shown there's at least 4 equivalences in the set D. But as a matter of fact, D $\subset$ {1, ..., 69} implies that card(D) $\le$ 69. Hence this proves the result. Where have I gone wrong in my reasoning?","['combinatorics', 'discrete-mathematics']"
3428466,Find the range of $f(x)=2|{\sin x}|-3|\cos x|$,"Find the range of $f(x)=2|{\sin x}|-3|\cos x|$ My attempt is as follows:- $$f(x)=\sqrt{13}\left(\dfrac{2}{\sqrt{13}}\cdot|\sin x|-\dfrac{3}{\sqrt{13}}\cdot|\cos x|\right)$$ Let's assume $z=\dfrac{2}{\sqrt{13}}\cdot|\sin x|-\dfrac{3}{\sqrt{13}}\cdot|\cos x|$ $$f(x)=\sqrt{13}z$$ Let's find the range of $z^2$ $$z^2=\dfrac{4}{13}\cdot\sin^2x+\dfrac{9}{13}\cdot\cos^2x-\dfrac{6}
{13}|\sin2x|$$ $$z^2=1-\dfrac{6}{13}\cdot|\sin2x|$$ $$z^2=1-\dfrac{6}{13}\cdot[0,1]$$ $$z^2=1-\left[0,\dfrac{6}{13}\right]$$ $$z^2=\left[\dfrac{7}{13},1\right]$$ As $0$ is not there in the range of $z^2$ , so we can say $z\in \left[-1,-\dfrac{\sqrt{7}}{\sqrt{13}}\right]\cup \left[\dfrac{\sqrt{7}}{\sqrt{13}},1\right]$ Hence $y \in \sqrt{13}\left(\left[-1,-\dfrac{\sqrt{7}}{\sqrt{13}}\right]\cup \left[\dfrac{\sqrt{7}}{\sqrt{13}},1\right]\right)$ $$y\in[-\sqrt{13},-\sqrt{7}]\cup[\sqrt{7},\sqrt{13}]$$ But actual answer is $[-3,2]$","['trigonometry', 'functions']"
3428467,"Given measurable sets $E_k$, find a measurable set $A$ with $0<\mu(E_k\cap A)<\mu(E_k)$","Let $\mu$ be the Lebesgue measure on $\mathbb{R}^n$ and $E_k\subseteq\mathbb{R}^n,k\in\mathbb{N}$ be measurable sets each having positive Lebesgue measure. How to find a measurable set $A\subseteq\mathbb{R}^n$ so that $0<\mu(E_k\cap A)<\mu(E_k)$ for every $k\in\mathbb{N}$ ?","['measure-theory', 'lebesgue-measure']"
3428470,"Flow of Hamiltonian vector fields, time dependent flow","I have trouble understanding the notion of time dependent flows of Hamiltonian vector fields: Let $(M, \omega)$ be a symplectic manifold, $H:M \rightarrow M$ a Hamiltonian function. question: In my lecture, we haven't properly defined what Hamiltonian function means. Is it correct, that it's just a smooth function? Now let $X_H$ be the Hamiltonian vector field which means $\omega(X_H,.)=-dH$ ). Then the flow of $X_H$ , $\varphi_t$ , is the Hamiltonian flow. Now I know the definition of the flow from differential geometry. Here it means that $X_H(\varphi_t)= \dfrac{d}{dt} \varphi_t$ . Now we have the notion of a time- $t$ map, which I don't understand. 
For example, I don't understand the following exercise: Let $\varphi_t:(M, \omega) \rightarrow (M, \omega)$ be the family of diffeomorphisms determined by the time-dependent Hamiltonian function $H : [0, 1] \times M \rightarrow \mathbb{R}$ via $\overset{.}{\varphi} = X_{H_{t}} \circ \varphi_t$ . For each $t \in (0,1)$ , write $\varphi_t$ as time one map of a family of diffeomorphisms
determined by a new Hamiltonian function built from $H$ . I don't understand this exercise at all. As I see it, you can get a family of flows $\{\varphi^{s}_t\}_{s \in [0,1]}$ such that for each fixed $s$ , $\varphi^s_t$ is the Hamiltonian flow of the function $H_s: M \rightarrow M$ . 
But I don't see why the parameter $t$ of the flow here is identified with the parameter $t \in [0,1]$ of the function $H$ .","['symplectic-geometry', 'hamilton-equations', 'differential-geometry']"
3428502,Definition of $L^p$ space and almost everywhere defined functions,"Given $(\Omega, M, \mu)$ a measure space, one defines $L^p$ to be the quotient of the set of all measurable functions $f$ from $\Omega$ to the extended real line such that $|f|^p$ has finite integral, with respect to, $~$ the the 'almost everywhere equal' equivalence relation. I cannot understand why it is a vector space. Take $f,g$ in $L^p$ . Their sum is defined as the equivalence class of the sum of any two representatives $h,k$ of respectively $f$ and $g$ . $h,k$ are functions defined on $\Omega$ , and are almost everywhere finite, however their sum may be only almost everywhere defined. So what do do? I guess one could extend $h+k$ on all of the domain by just setting to $0$ (or any other value) the sum every time it is not defined. This would still yield a well defined sum of equivalence classes. But for the integral? Can I define the integral of an almost everywhere defined function to be the integral of the function on the set on which it is defined?","['measure-theory', 'real-analysis']"
3428516,"In probabilistic questions with ""real life"" context, why can we ignore defining the sample space?","Consider, for the sake of argument, the following question: we are coloring every side and every diagonal of a regular hexagon with one of three colours (say white $W$ , red $R$ and black $B$ ). Everytime each colour is equally probably and colorings are independent. Let $X$ denote number of triangles in one colour. Compute $E(X)$ . I'd be satisfied with a solution going like: enumerate all 15 considerated edges with numbers from $1$ to $15$ and all 20 triangles with numbers from $1$ to $20$ . Step 1: define a ""compelling"" sample space, like a product space of 15 spaces $\Omega=\{W, R, B\}$ with $\frac{1}{3}$ probability for every singleton. Step 2: define random variables $X_{1}, X_{2}, \dots, X_{20}$ setting values $1$ if corresponding triangle is in one colour, $0$ otherwise. Then $X=X_{1}+\dots X_{20}$ and we proceed from here. Usually we ignore step $1$ and go directly to step $2$ . I'd kindly ask you to tell me why is it considered as as good of a solution. When dealing with similar questions, do we start by assuming that there is some ""compelling"" (what would that even mean?) sample space? How else would one be able to define random variables? Does one usually postulate anything when begins to model using probability theory? Also, in the usual way we continue by kind of guessing (without explicit sample space isn't that so? Or maybe it is arbitrary and equivalent to defining sample space?) distributions of defined random variables and say things like: $P(X_{i}=1)=\frac{1}{3} \cdot \frac{1}{3}$ because u fix one colour of one edge and then other two edges have $\frac{1}{3}$ probability of having this colour and those two colorings are independent, so we multiply. Why are those kind of solutions correct, formally? With sample space defined it seems so much more elegant and precise to me. Is it just my lack of practice? Why should i solve problems that way? Any help would be much appreciated. I'd like the answers to be as technical as it is necessary. My knowledge exclude things like Markov chains, martingales, stochastic processes (i.e. Kolmogorov's existence theorem) and statistics, but please use it if it's helpful, i'll then just get back to it again in the future.","['probability-theory', 'probability']"
3428579,Kind of convergence theorem for nets,"It is well known that the Lebesgue Dominated Convergence Theorem does not hold for nets (there are problems concerning countability). However, in the following particular case, is there any possible hypothesis, which I could add to the integrand function (actually on $f$ ) to switch the order of the limit and integral? $$
\lim_{\alpha \in A} \int_He^{-if(P_{\alpha}x)}d\mu=\int_H\lim_{\alpha \in A} e^{-if(P_{\alpha}x)}d\mu
$$ Here $H$ is a Hilbert space ( $\dim(H)=\infty$ ), $\mu$ some complex measure and $P_{\alpha}$ are projections converging strongly to the identity.","['lebesgue-measure', 'lebesgue-integral', 'functional-analysis', 'convergence-divergence', 'nets']"
3428643,Is it possible to give an elementary evaluation for this sum: $ \sum_{n=0}^{\infty}(x^{(2^{-n})}-1)$,"I've been thinking about this sum for a little while. I came up with it based on the observation that the  product: $$\prod_{n=0}^{\infty}x^{(2^{-n})}$$ converges to $x^2$ , and if we take the natural log of this product then we get: $$\sum_{n=0}^{\infty}\ln(x^{2^{-n}})$$ which converges to $2\ln(x)$ . The sum of interest comes by using the linear approximation $ln (x) \approx x-1$ . Intuition says that since the linear approximation becomes very good as $x \rightarrow 1$ , then the sum: $$f(x) = \sum_{n=0}^{\infty}(x^{(2^{-n})}-1)$$ should also converge. This can be confirmed rigorously using the ratio test. This observation lead me to ask whether there was any way to give an elementary evaluation of this sum for any value other than one, where all there terms are clearly zero. Using Taylor expansion at one, I showed that over the open interval from 0 to 2, this is equal to the sum: $$\sum_{n=0}^{\infty}(x-1)^n \sum_{j=0}^{\infty}
\prod_{i=0}^{n-1}(2^{-j}-i)$$ which though more complicated, has the advantage that the internal product and sum can be evaluated for a given value of $n$ by expanding the product, then breaking the sum into several geometric series and using the summation formula for geometric series to evaluate the individual geometric series. This altogether means that it is possible with only finitely much work to find rational estimates for $f(x)$ when $x$ is rational, which might lead to useful conjectures.",['sequences-and-series']
3428677,Differentiability of complex line integral,"Take $f:D\subseteq\mathbb{C}\rightarrow \mathbb{C}$ to be holomorphic on some path connected $D$ . Suppose there exists a function $F:D\rightarrow \mathbb{C}$ , such that for all $z,z'\in D$ and any path $\gamma:[0;1]\rightarrow D$ with $\gamma(0)=z$ and $\gamma(1)=z'$ we have: $$\int_\gamma f(z)\, dz = F(z')-F(z)$$ Can we show that $F$ is differentiable and $F'=f$ on $D$ (so $F$ is a complex antiderivative of $f$ ) without using Cauchy's integral formula or equivalence between the notions holomorphic and analytic? I already tried fixing one point of the path and then splitting it up (or alternatively the whole integral) into real and imaginary part to be able to use some real analysis but I didn't find something useful.Any ideas? Regards","['integration', 'path-connected', 'complex-analysis', 'line-integrals', 'derivatives']"
3428682,Every compact connected hypersurface with enough symmetry is a sphere,"I would like to prove that: Lemma. Let $M\subset\mathbb R^{n+1}$ be a connected and compact $n$ -dimensional smooth submanifold (i.e. a hypersurface). Suppose that for every direction, there exists a hyperplane such that $M$ reflected at this hyperplane equals $M$ . Or put more formally, for every $v\in\mathbb R^{n+1}$ with $|v|=1$ , there exists a constant $c(v)\in\mathbb R$ such that $M$ reflected at $$\{x\in\mathbb R^{n+1}\mid \langle x,v\rangle = c(v)\}$$ is equal to $M$ . Then $M$ is a sphere. It is said that this Lemma is due to Hopf, but I don't know how to prove it. I found the following paper: ALEKSANDROV’S  THEOREM:  CLOSED  SURFACES  WITHCONSTANT  MEAN  CURVATURE in which Lemma 1.4 is almost identical to my Lemma. But I can't understand the proof. Also, the Lemma 1.4 has the additional assumption that $M$ is a closed hypersurface. How can I prove this Lemma?","['submanifold', 'surfaces', 'differential-geometry']"
3428727,Find the units digit of $572^{42}$,"The idea of this exercise is that you use the modulus to get the right answer.
What I did was: $$572\equiv 2\pmod {10} \\
572^2 \equiv 2^2  \equiv 4\pmod{10} \\
572^3 \equiv 2^3  \equiv 8\pmod{10} \\
572^4 \equiv 2^4  \equiv 6\pmod{10} \\
572^5 \equiv 2^5  \equiv 2\pmod{10} \\
572^6 \equiv 2^6  \equiv 4\pmod{10} \\
(...)$$ I can see that this goes 2,4,8,6 and then repeats. I remember that the gist of the exercise is to find the remainder based on this repetition. How do I do that? I know that $572^{42} \equiv 2^{42}\equiv ? \pmod {10}$ . How do I simplify that 42 and answer this using that repetition?","['elementary-number-theory', 'modular-arithmetic', 'discrete-mathematics']"
3428778,Can I recognize when there is a group with given element degree sequence?,"Let $G$ be a group of order $p^n$ , where $p$ is prime, and let $n_k$ be the number of elements of order $p^k$ in $G$ . By analogy with the degree sequence of a graph, let's use the not-really-accurate term ""degree sequence of $G$ "" for this sequence. (Side question: is there a standard name for this? Not knowing one, I could not do very much advance investigation on my own.) Is there an algorithm which, given a finite sequence $n_i$ , efficiently decides whether this is the degree sequence of any group? For example, given the sequence $\langle 1, 5,2\rangle$ or $\langle1,1,2,4\rangle$ , the answer is yes ( $D_8$ and $Z_8$ respectively), but for $\langle 1,4,3 \rangle$ the answer is no. (The analogous problem for graph degree sequences is called the Erdős–Gallai theorem, and is efficiently decided by the Havel-Hakimi algorithm.) One can of course ask a more general version of the same question for a non- $p$ -group, with the input $n_i$ being the number of elements of order $i$ rather than the number of order $p^i$ .","['group-theory', 'finite-groups', 'algorithms']"
3428804,Easier proof of Tietze theorem in metric case.,"In the Wikipedia's page of Tietze theorem is written that the first proof of this theorem was found for finite dimensional real vector spaces by Brouwer and Lebesgue, than for metric spaces by Tietze and finally for normal spaces by Urysohn. My question is if there exist proofs of this theorem in the metric spaces setting that are easier than the general one (as for the Urysohn lemma) or even in the case of finite dimensional real vector space.","['alternative-proof', 'general-topology', 'metric-spaces']"
3428840,Compute $\int_0^{\pi/2} x^2\left(\sum_{n=1}^\infty (-1)^{n-1} \cos^n(x)\cos(nx)\right)dx$,"How to prove $$I=\int_0^{\pi/2} x^2\left(\sum_{n=1}^\infty (-1)^{n-1} \cos^n(x)\cos(nx)\right)dx=\frac16\left(\frac{\pi^3}{12}-\pi\operatorname{Li}_2\left(\frac13\right)\right)$$ This problem is proposed by Cornel which can be found here where he suggested that the problem can be solved with and without harmonic series. Here is my approach but I got stuck at the blue integral: Using the common identity $$ \sum_{n=1}^{\infty}p^n \cos(nx)=\frac{p(\cos(x)-p)}{1-2p\cos(x)+p^2}, \ |p|<1$$ Set $p=-\cos(x)$ we get $$ \sum_{n=1}^{\infty}(-1)^n \cos^n(x) \cos(nx)=-\frac{2\cos^2(x)}{1+3\cos^2(x)}=-\frac23+\frac23\frac1{1+3\cos^2(x)}$$ Multiply both sides by $-x^2$ then integrate from $x=0$ to $\pi/2$ we get $$\int_0^{\pi/2} x^2\left(\sum_{n=1}^\infty (-1)^{n-1} \cos^n(x)\cos(nx)\right)dx=\frac23\int_0^{\pi/2} x^2dx-\frac23\color{blue}{\int_0^{\pi/2}\frac{x^2}{1+3\cos^2(x)}dx}\\=\frac{\pi^3}{36}-\frac23\left(\color{blue}{\frac{\pi^3}{48}+\frac{\pi}{4}\operatorname{Li}_2\left(\frac13\right)}\right)=\frac{\pi^3}{72}-\frac{\pi}{6}\operatorname{Li}_2\left(\frac13\right)$$ I have two Questions: 1) Can we evaluate $I$ in a different way? 2) How to finish the blue integral? My try to the blue integral is using integration by parts $$\int\frac{dx}{1+3\cos^2(x)}=\frac12\tan^{-1}\left(\frac{\tan(x)}{2}\right)=-\frac12\tan^{-1}\left(2\cot(x)\right)$$ which gives us $$\int_0^{\pi/2}\frac{x^2}{1+3\cos^2(x)}dx=\frac{\pi^3}{16}-\int_0^{\pi/2}x\tan^{-1}\left(\frac{\tan(x)}{2}\right)dx$$ Or $$\int_0^{\pi/2}\frac{x^2}{1+3\cos^2(x)}dx=\int_0^{\pi/2}x\tan^{-1}\left(2\cot(x)\right)dx$$ I also tried the trick $x\to \pi/2-x$ but got complicated Proof of the identity: \begin{align}
\sum_{n=0}^\infty p^ne^{inx}&=\sum_{n=0}^\infty\left(p e^{ix}\right)^n=\frac{1}{1-pe^{ix}},\quad |p|<1\\&=\frac{1}{1-p\cos(x)-ip\sin(x)}=\frac{1-p\cos(x)+ip\sin(x)}{1-2p\cos(x)+p^2}\\
&=\frac{1-p\cos(x)}{1-2p\cos(x)+p^2}+i\frac{p\sin(x)}{1-2p\cos(x)+p^2}
\end{align} By comparing the real and imaginary parts, we get $$\sum_{n=\color{blue}{0}}^\infty p^n \cos(nx)=\frac{1-p\cos(x)}{1-2p\cos(x)+p^2}\Longrightarrow \sum_{n=\color{blue}{1}}^\infty p^{n-1} \cos(nx)=\frac{\cos(x)-p}{1-2p\cos(x)+p^2}$$ and $$\sum_{n=\color{red}{0}}^\infty p^n \sin(nx)=\frac{p\sin(x)}{1-2p\cos(x)+p^2}\Longrightarrow \sum_{n=\color{red}{1}}^\infty p^n \sin(nx)=\frac{p\sin(x)}{1-2p\cos(x)+p^2}$$","['integration', 'harmonic-numbers', 'calculus', 'polylogarithm', 'sequences-and-series']"
3428846,A family of distributions induced by a metric,"I'm interested in the family of distribution which can be expressed in the following form $f(x|\mu)=C \exp(-d(x,\mu))$ where $\mu$ is a parameter of the distribution and $d(*,*)$ is a metric. 
Normal distribution and Laplace distribution satisfy this property.
I was wondering if this family has already been studied and what is its name.",['statistics']
3428922,Adjoint of a nonlinear operator?,"For any linear operator $A$ , the adjoint $A^*$ is defined as a linear operator that satisfies $$\langle v, Au\rangle = \langle A^*v, u\rangle$$ Moreover, one has that $(A^{*})^{*} = A$ (proof here ). When things are nonlinear, this seems to result in weird conclusions. Let $F$ be an operator that is not linear. Can one define the adjoint of $F$ and if yes how? If one goes along with the usual definition, then we see that $\begin{align}
\langle x, F(y+z)\rangle &= \langle F^*x, y+z\rangle \\
&= \langle F^*x, y\rangle +\langle F^*x, z\rangle \\
&= \langle x, F(y) + F(z)\rangle
\end{align}$ I'm not sure if this explicitly contradicts the assumption that $F$ is nonlinear but it seems to? I did assume that $(F^*)^* = F$ but the linked proof above doesn't seem to require linearity so this seems okay. In general, is it possible to define the adjoint in the sense of $\langle v, Fu\rangle = \langle F^*v, u\rangle$ for nonlinear $F$ ?","['adjoint-operators', 'linear-algebra']"
3428932,Why does $\left(x \cdot \tan\left(\frac{1}{x}\right)-1\right)^{-1}$ asymptotically approach $3x^2 - 6/5$?,"I noticed that $\lim_{x \to \infty}\tan\left(\frac{1}{x}\right)*x = 1$ and I was wondering how fast it approaches $1$ . I looked at $\frac{1}{\tan\left(\frac{1}{x}\right)*x-1}$ and found that this grows slower than $x^3$ , so to find what polynomial degree it grows as fast as, I plugged it into $\lim_{x \to \infty} \frac{\ln\left(f\left(x\right)\right)}{\ln\left(x\right)} = \lim_{x \to \infty} \frac{\ln\left(\frac{1}{\tan\left(\frac{1}{x}\right)*x-1}\right)}{\ln\left(x\right)}=2$ to find that it grows around as fast as $x^2$ . Then I tried plugging $\lim_{x \to \infty}\left(\tan\left(\frac{1}{x}\right)*x-1\right)*x^2$ into Wolfram|Alpha and it produced $1/3$ . It was not able to provide any steps. How is this limit calculated? Using this, the next question I come upon is how $\lim_{x\to\infty}3x^2-\frac{1}{\tan\left(\frac{1}{x}\right)*x-1} = 6/5$ is calculated. Why does $\left(\tan\left(\frac{1}{x}\right)*x-1\right)^{-1}$ asymptotically approach $3x^2 - 6/5$ ?","['limits', 'trigonometry']"
3429075,Evaluate $\lim_{x\to 0} \cot ^2 (x)-\frac{1}{x^2}$,"Evaluate $\lim\limits_{x\to 0} \cot ^2 (x)-\dfrac{1}{x^2}.$ I was thinking of using L'Hôpital's Rule, but things got very, very ugly so I wasn't able to solve it. I know from experimentation that the limit is $-\dfrac{2}{3}$ though. edit: i do not want to use a taylor expansion to solve this. That's too easy.","['limits', 'calculus', 'derivatives', 'real-analysis']"
3429123,Showing a function is harmonic in the unit disc but vanishes on the boundary.,"this is my first post, so please forgive me for any slip-ups. This is an exercise from Stein and Shakarchi ""Complex Anaylsis"" chapter 8 on conformal mappings. The question reads as follows: Prove that a function $u$ defined by $$u(x,y) = \Re\left(\frac{i+z}{i-z}\right) \quad \text{ and } \quad u(0,1) = 0$$ is harmonic in the unit disc and vanishes on its boundary. Note that $u$ is not bounded in $\mathbb{D}$ , the unit disc. To show that a function is harmonic I've been trying to show $\frac{\partial^2u}{\partial x^2}+\frac{\partial^2u}{\partial y^2}= 0$ which would mean I have to calculate those partial derivatives after plugging in $z=x+iy$ . Although this is a gross computation I have managed to show it is true, however I would be curious to know of any cleverer ways of doing this. I've never been good at finding slick calculus tricks. I've found that usually the trick to finding properties of a function on the unit circle is to analyze the behaviour of $u(e^{i\theta}$ ). I also considered that harmonic functions are holomorphic, but this hasnt really gotten me anywhere. My Question : I'd like to know what the trick is to showing this function vanishes on $\partial\mathbb{D}$ . Does it have anything to do with conformal mappings? Since $u$ isn't bounded on $\mathbb{D}$ does that make it a conformal map to $\mathbb{C}$ since it's holomorphic? Any hints are apppreciated.",['complex-analysis']
3429173,Alternative method for expanding $\dfrac{1}{1+\sin(x)}$,"I was browsing on topics about Taylor series and come across this thread: Maclaurin series of $\frac{1}{1+\sin x}$ In it, I come across this answer by Marc van Leeuwen. I have two questions regarding his answer. The first is what is $1$ mod $x^5$ ? The second is how does he develop the equation $a+b$ , $b+c$ , $-\dfrac{1}{6}a+c+d$ and so on? The third is why he sets all these equations equal zero? I have seen this method of obtaining indeterminate coefficient of power series but I still don't have an exact understanding of it.","['power-series', 'calculus', 'sequences-and-series']"
3429187,Difference quotient for Hölder continuous functions,"Let $\Omega\subset\mathbb{R}^n$ be a bounded open set and $u\in C^\alpha_{\mathrm{loc}}(\Omega)$ . For $h>0$ , $1\leq k\leq n$ , let $$D_k^hu(x)=\frac{u(x+he_k)-u(x)}{h}$$ where $e_k$ is the $k$ -th coordinate vector. Suppose for each $\Omega_0\Subset\Omega$ and $k$ , there is a constant $C$ s.t. $\|D^h_ku\|_{C^\alpha(\Omega_0)}\leq C$ for all small $h$ . Then $u\in C^{1,\alpha}_{\mathrm{loc}}(\Omega)$ and in fact $\|D_ku\|_{C^\alpha(\Omega_0)}\leq C$ . Is this statement true? If so, how can I prove it? By Arzelà–Ascoli we know that there is a sequence $h_j\to0$ such that $D^{h_j}_ku$ converges uniformly. But why does this even imply $u\in C^1$ ?","['analysis', 'function-spaces', 'partial-differential-equations', 'regularity-theory-of-pdes', 'holder-spaces']"
3429211,On the determinant of a certain matrix with non negative integer entries with fixed row sum,Let $d$ be a positive integer. Let $n \ge 3$ be an integer. Let $A$ be a $n \times n$ matrix with non-negative integer entries such that each row sum is $d$ and the determinant is also $d$ . Let $B$ be a $n \times n$ matrix whose first column has all the same positive integer entries say $m$ and all the other entries of the matrix are $0$ . Then is it true that $\det (A+B)=d+m$ ?,"['matrices', 'linear-algebra', 'combinatorics']"
3429229,Convergence in $L^1$ and Boundedness in $L^p$ Implies Convergence in $L^p$,"Let $u_n = u_n(t,x)$ be a sequence of functions, $u_n : (0, \infty) \times \mathbb{R}^N \rightarrow \mathbb{R}$ , such that $u_n(t)$ converges to a function $u(t)$ in the $L^1(\mathbb{R}^N)$ norm for all $t \geq 0$ . I.e. $ ||u_n(t,\cdot) - u(t, \cdot)||_{L^1(\mathbb{R}^N)} \rightarrow 0$ . Also, assume the set of $u_n(t) $ is bounded in the $L^p(\mathbb{R}^N)$ norm, for all $p \in [1, \infty)$ . To Prove: $u_n(t) \rightarrow u(t)$ for all $t \geq 0$ in the $L^p(\mathbb{R}^N)$ norm, for all $p \in [1, \infty)$ . My Work So Far By convergence in $L^1$ , we know there exists a subsequence $u_{n_k}$ such that $u_{n_k}(t,x) \rightarrow u(t,x)$ pointwise, for almost all $x \in \mathbb{R}^N$ , for all $t \geq 0$ . By boundedness of $u_n(t) $ in $L^p$ , we know there exists $\text{lim inf}_{n_k \rightarrow \infty} ||u_{n_k}(t)||_{L^p} < \infty$ . Thus, by Fatou's Lemma, $ ||u(t)||_{L^p} \leq \text{lim inf}_{n_k \rightarrow \infty} ||u_{n_k}(t)||_{L^p} < \infty$ for all $t \geq 0$ . In particular, $u(t) \in L^p( \mathbb{R}^N)$ for all $t \geq 0$ . This is the most concrete progress I can make. I would like to proceed as follows, but an unsure if this is a dead end: Let $f_{n_k} (t,x) := |u_{n_k}(t, x) - u(t, x)|$ . Then $f_{n_k}(t)$ is a sequence of functions in $L^p( \mathbb{R}^N)$ , for all $p \in [1, \infty)$ and all $t \geq 0$ , and we have $f_{n_k}(t) \rightarrow 0$ in $L^1$ , for all $t \geq 0$ , and $f_{n_k} (t,x) \rightarrow 0$ pointwise for almost all $x \in \mathbb{R}^N$ and all $t \geq 0$ . I would here like to use Lebesgue's Dominated Convergence Theorem somehow to finish the proof, but am having trouble finding a function which dominates $f_{n_k}$ . Could someone please tell me how to find such a function, or if a different method is required. Thank you","['integration', 'real-analysis', 'lp-spaces', 'functional-analysis', 'convergence-divergence']"
3429302,"Let $g = x^3 + 1$ and $f = x^2 + 1$ How to show that there doesn't exist function $k$, such that $g = k \circ f$?","The task is Let $\mathcal F = \{f \mid f : \mathbb R \rightarrow \mathbb R\}$ and
  define relationship $R$ on $\mathcal F$ as follows: $$R = \{(f,g) \in \mathcal F \times \mathcal F \mid \exists h \in
 \mathcal F (f = h \circ g)\}$$ Let functions $f,g,h$ be the functions from $\mathbb R$ to $\mathbb R$ defined by the formulas $f(x) = x^2 + 1$ , $g(x) = x^3 + 1$ , $h(x) = x^4
 + 1$ . Prove that $hRf$ , but it is not the case that $gRf$ . I was able to show that $hRf$ . Let $k \in \mathcal F$ such that $$k(x) = x^2 - 2x + 2$$ Then $f(x) = k(g(x)) = (k \circ g)(x)$ for all $x$ . Now my question is, how to show that $(g,f)$ is not in R?","['elementary-set-theory', 'functions']"
3429351,Is there a mathematical operator that will turn a matrix into its absolute values?,"For a given matrix $X=\left(x_{i,\,j}\right)_{(i,\,j)}$ ,
I am searching for a mathematical operator that will give me another matrix $Y$ with the absolute values of $X$ , i.e.: $Y=\left(|x_{i,\,j}|\right)_{(i,\,j)}$ . It would be nice, if I could write something like $Y=|X|$ but I dont think, that this is a valid notation.
Is there any convention how to do so for matrices (or vectors)? Or do I have to define my own operator like $\tilde{X}:=\left(|x_{i,\,j}|\right)_{(i,\,j)}$ ? Thank you! :)","['matrices', 'notation', 'absolute-value']"
3429361,Conformal Group when p+q>3,"I am reading Martin Schottenloher's A Mathematical Introduction to Conformal Field Theory. On page 15, edition 2, or page 13 edition 1, he writes that for the case conformal killing factor is $0$ , which implies $$ X_{\mu , \nu} + X_{\nu , \mu} = 0.$$ This he claims implies $$ X^{\mu}_{,\nu} = 0.$$ I do not understand how he reached this claim. Then further he writes that this implies $$ X^{\mu}(q) = c^\mu + w^{\mu}_{\nu} q^{\nu}. $$ Which does not make sense at all, as for the above form of $ X^{\mu}(q)$ $$ X^{\mu}_{,\nu} = w^{\mu}_{\nu}.$$","['conformal-geometry', 'conformal-field-theory', 'mathematical-physics', 'differential-geometry']"
3429457,Sylow $p$-subgroup is cyclic,"Let $G$ be a group such that $|G|=p^{\alpha}m$ , and let $P$ be a Sylow $p$ -subgroup of $G$ . Suppose that $x\in P$ is such that $x$ has order $p$ and that $C_{G}(x)$ has cyclic Sylow $p$ -subgroups. Then $P$ is cyclic. My attempt (edited): If $x\not\in Z(P)$ , choose $y\in Z(P)$ of order $p$ . Such $y$ exists since $Z(P)\supsetneq \{1\}$ . Then $yxy^{-1}x^{-1}=1$ , so that $yx=xy$ . Since $\langle x\rangle\cap \langle y\rangle\leq\langle y\rangle$ , then $\langle x\rangle\cap\langle y\rangle$ is either trivial or $\langle y\rangle$ . It cannot be the latter for otherwise $x\in\langle y\rangle\subseteq Z(P)$ , a contradiction. Hence $\langle x,y\rangle\cong\langle x\rangle\times\langle y\rangle$ is a $p$ -subgroup of $C_{G}(x)$ and so is cyclic by hypothesis. Comments (edited): I have a feeling that I should conclude that $\langle x,y\rangle$ is not cyclic, but I am not sure how to. With this $\langle x,y\rangle$ not being cyclic, we get a contradiction so that $x\in Z(P)$ . How then would we be able to conclude that $P\subseteq C_{G}(x)$ as I feel that it should go in this direction. I would appreciate any hints to point me in the right direction for this problem.","['group-theory', 'abstract-algebra', 'sylow-theory', 'cyclic-groups']"
3429592,Checking if a finite subset is a subgroup,"It happens that, $G$ being a group, we want to check if a finite subset $\{a_{1}, \ldots , a_{n}\}$ of $G$ , with $n \geq 1$ , is a subgroup of $G$ . It is a subgroup if and only if for every $i, j$ in $\{1, \ldots , n\}$ , $a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ . (A nonempty finite submagma of a group is a subgroup.) If I'm not wrong, it is sufficient to verify that for every $i, j$ in $\{1, \ldots , n\}$ such that $i \leq j$ , $a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ , where we can clearly assume that the $a_{i}$ 's are pairwise distinct. (Edit : Thus (if I'm not wrong), instead of verifying that, for every $i, j$ such that $i \leq j$ , the elements $a_{i} a_{j}$ AND $ a_{j} a_{i}$ are in $\{a_{1}, \ldots , a_{n}\}$ , we need only to show that $ a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ . So, we avoid almost half of the calculations.) I have a proof (I give it below), but it is perhaps too complicated. Thus, my questions are : 1° is the statement correct ? 2° if the statement is correct, is the proof accurate ? 3° if the proof is accurate, is it the simplest possible ? 4° if the statement is correct, is it useful ? 5° do you know a reference to the literature about this question ? Definition. Let $G$ be a group. We define a semistable sequence of elements of $G$ as a nonempty finite sequence $(a_{1}, \ldots , a_{n})$ of pairwise distinct elements of $G$ such that for every $i, j$ in $\{1, \ldots , n\}$ such that $i \leq j$ , $a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ . Our problem is to prove that if $(a_{1}, \ldots , a_{n})$ is a semistable sequence of elements of $G$ , then $\{a_{1}, \ldots , a_{n}\}$ is a subgroup of $G$ . Step 1. Let $(a_{1}, \ldots , a_{n})$ be a semistable sequence of elements of a group $G$ , let $i$ be an index in $\{1, \ldots , n\}$ . Then $a_{i}$ is of finite order ( $\leq n$ ) and for every $s$ in $\mathbb{Z}$ , $a_{i}^{s}$ is in $\{a_{1}, \ldots , a_{n}\}$ . The function $a \mapsto a^{-1}$ is a permutation of $\{a_{1}, \ldots , a_{n}\}$ . Proof. Let us prove that for every natural number $r \geq 1$ , $a_{i}^{r}$ is in $\{a_{1}, \ldots , a_{n}\}$ . It is true if $r = 1$ . Assume that it is true for a natural number $r$ . Then $a_{i}^{r} = a_{j}$ for some $j$ . It implies $a_{i}^{r+1} = a_{i}a_{j}$ and also $a_{i}^{r+1} = a_{j}a_{i}$ . The first of these two results gives $a_{i}^{r+1} \in \{a_{1}, \ldots , a_{n}\}$ if $i \leq j$ and the second result also gives $a_{i}^{r+1} \in \{a_{1}, \ldots , a_{n}\}$ if $j \leq i$ . By induction on $r$ , we conclude that (1) for every natural number $r \geq 1$ , $a_{i}^{r}$ is in $\{a_{1}, \ldots , a_{n}\}$ . In particular, the $n+1$ elements $a_{i}, a_{i}^{2}, \ldots , a_{i}^{n+1}$ are all in $\{a_{1}, \ldots , a_{n}\}$ . Since $\{a_{1}, \ldots , a_{n}\}$ has only $n$ elements, there are at least two exponents $r$ and $t$ in $\{1, \ldots , n+1\}$ such that $a_{i}^{t} = a_{i}^{r}$ and this implies that $a_{i}$ is of finite order ( $\leq n$ ). Thus, for every $s$ in $\mathbb{Z}$ , $a_{i}^{s}$ is of the form $a_{i}^{r}$ with some $r \geq 1$ ,thus, in view of (1), $a_{i}^{s}$ is in $\{a_{1}, \ldots , a_{n}\}$ for every $s$ in $\mathbb{Z}$ . It is true in particular for $s=-1$ , thus the inverse of each element of $\{a_{1}, \ldots , a_{n}\}$ is in $\{a_{1}, \ldots , a_{n}\}$ , so the  function $a \mapsto a^{-1}$ is a permutation of $\{a_{1}, \ldots , a_{n}\}$ . Step 2. Let $(a_{1}, \ldots , a_{n})$ be a semistable sequence of elements of a group $G$ , let $i$ be an index in $\{1, \ldots , n\}$ . The two following conditions are equivalent : (i) for every $j \in \{1, \ldots , n\}$ , $a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ ; (ii) for every $j \in \{1, \ldots , n\}$ , $a_{j} a_{i}$ is in $\{a_{1}, \ldots , a_{n}\}$ . Proof. Let's define (temporarily) a left universal of $(a_{1}, \ldots , a_{n})$ as an $a_{i}$ such that condition (i) is satisfied, i.e. such that for every $j \in \{1, \ldots , n\}$ , $a_{i} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ and let's define a right universal of $(a_{1}, \ldots , a_{n})$ as an $a_{i}$ such that condition (ii) is staisfied, i.e. such that for every $j \in \{1, \ldots , n\}$ , $a_{j} a_{i}$ is in $\{a_{1}, \ldots , a_{n}\}$ . Thus, the statement amounts to say that the left universals are exactly the right universals. Let us prove that if $a_{i}$ is a left universal, then $a_{i}^{-1}$ is a right universal. Since $a_{i}$ is a left universal, we have $a_{i}a_{j} \in \{a_{1}, \ldots , a_{n}\}$ for every $j$ . We saw at step 1 that the function $a \mapsto a^{-1}$ is a permutation of $\{a_{1}, \ldots , a_{n}\}$ , thus $a_{j}^{-1} a_{i}^{-1} \in \{a_{1}, \ldots , a_{n}\}$ for every $j$ . Still because $a \mapsto a^{-1}$ is a permutation of $\{a_{1}, \ldots , a_{n}\}$ , $a_{j}^{-1}$ runs over $\{a_{1}, \ldots , a_{n}\}$ as does $a_{j}$ , thus, for every $j \in \{1, \ldots , n\}$ , $a_{j} a_{i}^{-1} \in \{a_{1}, \ldots , a_{n}\}$ . Since (step 1) $a_{i}^{-1}$ is in $\{a_{1}, \ldots , a_{n}\}$ , this proves that (1) if $a_{i}$ is a left universal, $a_{i}^{-1}$ is a right universal. Similarly, (2) if $a_{i}$ is a right universal, $a_{i}^{-1}$ is a left universal. Now let us prove that the inverse of a left universal is also a left universal. Let $a_{i}$ be a left universal, let $j$ an index in $\{1, \ldots , n\}$ . If $r$ is a natural number such that $a_{i}^{r} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ , we have $a_{i}^{r} a_{j} = a_{k}$ for some $k$ , whence $a_{i}^{r+1} a_{j} = a_{i}a_{k}$ . Since $a_{i}$ is a left universal, the right member is in $\{a_{1}, \ldots , a_{n}\}$ , thus $a_{i}^{r+1} a_{j}$ is in $\{a_{1}, \ldots , a_{n}\}$ . By induction on $r$ , we conclude that for every natural number $r \geq 1$ , $a_{i}^{r}$ is a left universal. Since (step 1) $a_{i}$ is of finite order, $a_{i}^{-1}$ is of the form $a_{i}^{r}$ with a natural number $r \geq 1$ , thus $a_{i}^{-1}$ is a left universal. We thus proved that (3) if $a_{i}$ is a left universal, $a_{i}^{-1}$ is also a left universal. Similarly, (4) if $a_{i}$ is a right universal, $a_{i}^{-1}$ is also a right universal. Let $a_{i}$ be a left universal. In view of (3), $a_{i}^{-1}$ is also a left universal. Thus, in view of (1), $a_{i}$ is a right universal. Thus every left universal is a right universal. Similarly, every right universal is a left universal. As noted, this proves step 2. Definition. Let $(a_{1}, \ldots , a_{n})$ be a semistable sequence of elements of a group $G$ . We define a universal of $(a_{1}, \ldots , a_{n})$ as an $a_{i}$ such that the two equivalent conditions of step 2 are satisfied. Theorem. Let $(a_{1}, \ldots , a_{n})$ be a semistable sequence of elements of a group $G$ . The set $\{a_{1}, \ldots , a_{n}\}$ is a subgroup of $G$ . Proof. Since $(a_{1}, \ldots , a_{n})$ is a semistable sequence of elements of a group $G$ , we have $a_{1} a_{j} \in \{a_{1}, \ldots , a_{n}\}$ for every $j$ in $\{1, \ldots , n\}$ , thus $a_{1}$ satisfies condition (i) of step 2 on $a_{i}$ , thus (1) $a_{1}$ is a universal of $(a_{1}, \ldots , a_{n})$ . If $n \geq 2$ , we have $a_{2} a_{j} \in \{a_{1}, \ldots , a_{n}\}$ for every $j \geq 2$ , because $(a_{1}, \ldots , a_{n})$ is a semistable sequence, and we have also $a_{2} a_{1} \in \{a_{1}, \ldots , a_{n}\}$ , because of (1). Thus, if $n \geq 2$ , (2) $a_{2}$ is a universal of $(a_{1}, \ldots , a_{n})$ . Similarly, we deduce from (1) and (2) that (if $n \geq 3$ ) $a_{3}$ is a universal, and so on. Thus $a_{1}, \ldots , a_{n}$ are all universals, thus $\{a_{1}, \ldots , a_{n}\}$ is a subgroup of $G$ . Edit. The proof of relation (3) in step 2 (if $a_{i}$ is a left universal, $a_{i}^{-1}$ is also a left universal) can be simplified. Since $a_{i}$ is a left universal, we have $a_{i} A = A$ , where $A$ denotes the set $\{a_{1}, \ldots , a_{n}\}$ . Left multiplication by $a_{i}^{-1}$ gives $A = a_{i}^{-1} A$ . Since, from step 1, $ a_{i}^{-1}$ is in A, this shows that $a_{i}^{-1}$ is a left universal. Similarly, the inverse of a right universal is a right universal. Edit (September 14, 2021) I discover that the statement demonstrated here is a theorem of G. Horrocks. It is the subject of Exercise 101 in J.S. Rose, A Course on Group Theory, p. 42.",['group-theory']
3429604,"How to show: $y'(t)=y(t)\sin(\frac{1}{y(t)})$ has one well defined solution $\in C^1(\mathbb{R},\mathbb{R})$","We are supposed to show: $y'(t)=y(t)\sin(\frac{1}{y(t)})$ has one well defined solution $\in C^1(\mathbb{R},\mathbb{R})$ for some initial value $y(0)=y_{0}>0$ . There is also a hint: solve on a local interval with explicit bounds, then try to construct a global solution. I have experimented with using Picard-Iteration to find solutions for explicit bounds, but I haven't seen a pattern. Obviously if $y_0=\frac{1}{n\pi}$ we get that a solution is the constant function $\varphi(x)=\frac{1}{n\pi}$ . For other bounds, I can't even solve using iteration. I don't see how I could use any other method. We can't use separation of variables as we only have one. It's not a linear differential equation. What am I not seeing here?","['derivatives', 'ordinary-differential-equations', 'real-analysis']"
3429623,"Is the union of $\emptyset$ with another set, $A$ say, disjoint? Even though $\emptyset \subseteq A$?","Is the union of $\emptyset$ with another set, $A$ say, disjoint?  Even though $\emptyset \subseteq A$ ? I would say, yes - vacuously.  But some confirmation would be great.",['elementary-set-theory']
3429671,Finding all quotients of the braid group $B_5$ up to order $720$,"Original Question. I am trying to find all quotients of the braid group $B_5$ on five strands up to order $6!=720$ . From previous very slow implementations I found that there are not many. Some results I have at my disposal are the following. All cyclic groups $C_n = \mathbb Z/n\mathbb Z$ are quotients of $B_5$ since the abelianization of $B_5$ is isomorphic to the integers $\mathbb Z$ . With the same argument we obtain that non-cyclic abelian groups cannot be quotients of $B_5$ . One can construct an automorphism, namely conjugation by the image of $\delta = \sigma_1 \cdots \sigma_{n-1}$ , of order divisible by $n$ , yielding that all quotients of $B_5$ must have order divisible by $5$ . Applying this to the subgroups $B_3$ and $B_4$ yields that any quotient of $B_5$ must have order divisible by $60$ , which drastically (but not drastically enough) decreases the number of groups to check. The permutation group $S_5$ is a quotient of $B_5$ by modding out the squares of the generators. Implementations showed that a subdirect product $A_5 \rtimes C_4$ and a direct product $C_3 \times S_5$ are also quotients of $B_5$ , but I do not know the explicit homomorphism. What I tried is to use the function AllSmallGroups in GAP, which really does return all groups of the desired order very efficiently (presumably because they are stored in a pre-computed list). I then checked whether the list returned by GQuotients was non-empty, which is apperently quite expensive to compute. Considering I have to check thousands of groups this is a problem. One approach I might have is to filter out the groups that have normal subgroups such that the quotient is non-cyclic and has order not divisible by $60$ . Sadly, the method StructureDescription takes ages to run on groups of the magnitudes of orders I am intersted in, so I cannot really check this efficiently. Thus, to answer my question I guess it would be enough to answer the following. How can you (more efficiently than using StructureDescription or the like) check whether a group returned by AllSmallGroups (or something similar) satisfies that the orders of all the quotients by its normal subgroups are divisible by $60$ ? Update. Alright, I managed to solve it. Here's the result: All non-cyclic quotients of order less than $720$ are either $A_5 \rtimes \mathbb Z/2^k \mathbb Z$ or $C_{2k+1} \times S_5$ . What it took was the following (thanks @AlexanderKonovalov): Get a list of IDs of SmallGroup s with IdsOfSmallGroups (less demanding on my workspace memory than using AllSmallGroups ). This gave me about $3000$ groups. Filter out the abelian groups: very easy with the precomputed IsAbelian attribute. I'm actually not sure how important that was since it only reduced my count from about $3000$ to about $2900$ groups, but still. Getting the normal subgroups of all those groups with NormalSubgroups and checking whether all quotients were either cyclic or had order divisible by $60$ . Throwing the rest away left me with $46$ groups. I then let my computer run for the whole day using GQuotients . It was able to check a remarkable $25$ groups during this time. I got a bit annoyed and ran StructureDescription on the group it was currently working at and found that it was working on a group that was somehow a direct product of $\mathbb Z/2\mathbb Z$ with some matrix group (which was previously identified as not a quotient). Since it only took a few seconds to run StructureDescription I decided to let it run on all of the remaining groups, and, miraculously, it finished this feat in half a minute or so. By hand I removed the ones that split as a direct product or a subdirect product the normal subgroup of which I knew was not a quotient, and had to run GQuotients on a single remaining group, which turned out to be not a quotient. What occurred to me a little bit too late is that I could have introduced a step 2.5 checking whether the groups in question had a generating set consisting of only two elements. This would have reduced to groups from $2900$ to about $2000$ , which would have saved maybe $5$ minutes of computing normal subgroups of stuff. But it turns out that all of the $46$ groups that required me to check by hand would not have been excluded by this step, so this is more of a side note.","['gap', 'group-theory', 'abstract-algebra', 'knot-theory', 'braid-groups']"
3429789,"Let $G$ be a group with identity element $e$, let $a \in G$ have order $n$. Let $m$ be an integer then prove that $a^m =e$ iff $n$ divides $m$","I have encountered this question in a book. I have no clue how to approach the question. How should I go about it? I was thinking of maybe writing m in terms of n, since we have m ≡ 0(mod n) . I tried to contradict it by taking some n which does not divide m. I am not sure if that is the correct procedure","['gcd-and-lcm', 'modular-arithmetic', 'discrete-mathematics']"
3429819,Irrationality of $1+1/10+1/11+1/100+...$,"Let $n$ 'th smallest whole number with digits strictly $0$ or $1$ be expressed by $f(n)$ . While all of these numbers look binary, we are working in entirely base 10. Is it possible to prove that $$\sum_{n=2}^\infty f(n) =\frac{1}{1}+\frac{1}{10}+\frac{1}{11}+\frac{1}{100}+\frac{1}{101}+\text{...}\approx 1.238... $$ is irrational? I have been working on trying to move this sum into a different form, and only partially succeeded. This was done by grouping the summed numbers in differing ways. $$\sum_{n=2}^\infty f(n) = \sum_{m=1}^\infty\text{ }\text{ }\sum_{n={\lfloor \log f(m)\rfloor+1}}^\infty\frac{1}{10^{n}+f(m)}$$ While my goal with this change was to compare with various Lambert Series, I found myself a little bit lost. Any and all help is greatly appreciated.","['elementary-number-theory', 'sequences-and-series']"
3429876,Having trouble understanding condition for Rolle's Theorem (russian translation),"I understand the conditions for Rolle's theorem in english : a function $f$ has to be continous on $[a,b]$ , differentiable on $(a,b)$ , and $f(a)=f(b)$ . But I'm studying in Russian, and they write the conditions like this: a) function $f$ is continous on $[a,b]$ , b) function $f$ has at all points of the interval $(a,b)$ a finite or definite sign infinite derivative, c) $f(a)=f(b)$ . I'm having trouble understanding point b), what does 'definite sign infinite derivative' mean? Is there a difference between that, and simply being differentiable on $(a,b)$ ? Maybe the translation isn't quite right, my russian isn't perfect, but I think that's what it means in English. All I could think of was, maybe they are refering to when the limit of $f(x)$ when $x \to a$ or $x \to b$ is equal to $\pm\infty$ , but then $f$ wouldn't be continous on $[a,b]$ . So, I'm lost. Do you guys have any ideas? Thanks a lot in advance.","['calculus', 'real-analysis']"
3429915,Can the coefficients for the Fourier transform of the Chebyshev polynomials be further simplified?,"In an article by Fokas and Smitheman, it is shown, among others things, how the finite Fourier transform of the Chebyshev polynomials can be computed. They show that, when $T_{m}(x)$ is the $m$ 'th Chebyshev polynomial of the first order and $$\hat{T}_{m} (\lambda) = \int_{-1}^{1} e^{-i \lambda x} T_{m} (x) dx, \quad \lambda \in \mathbb{C}, \quad m=0,1,2,\dots $$ then $$\hat{T}_{m} (\lambda) = \sum_{n=1}^{m+1} \alpha_{n}^{m} \Bigg{[} \frac{e^{i \lambda}}{(i \lambda)^{n}} + (-1)^{n+m} \frac{e^{i \lambda}}{(i \lambda)^{n}} \Bigg{]} $$ with $\alpha_{1}^{m} = (-1)^{m} $ , $\alpha_{2}^{m} = (-1)^{m+1}m^{2}$ and $$ a_{n}^{m} = (-1)^{m+n-1}2^{n-2}m \sum_{k=1}^{m-n+2} \binom{n+k-3}{k-1} \prod_{j=k}^{n+k-3} (m-j) \quad \text{for } n=3, 4, \dots, m+1. $$ Question : I wonder whether the expression for $\alpha_{n}^{m}$ -- when $3 \leq n \leq m+1$ -- can be either simplified further, or be cast into sequences of numbers that are already named or familiar?","['fourier-analysis', 'chebyshev-polynomials', 'sequences-and-series']"
3429946,"Do the closed unit disk $D$ and $f(D)$ intersect, if $||f(x)-x||\le2$ for all $x\in D$?","In $\Bbb R^n$ let $D=\{x:||x||\le1\}$ , and let $f:D\to\Bbb R^n$ be continuous with the property that $||f(x)-x||\le2$ for all $x\in D$ . Is it true that $D\cap f(D)\neq\varnothing$ (where $f(D)=\{f(x):x\in D\}$ ) ? In other words, is there $x\in D$ such that $f(x)\in D$ ? Clearly $f:D\to3D:=\{3x:x\in D\}$ , and if $g(x)=\frac{f(x)}3$ then $g:D\to D$ and $g$ has a fixed point but I do not see if I could employ this to answer my question. 
If $f$ is contracting, or if $f(D)\subseteq D$ then we are lucky (since $f$ would have a fixed point). If $f^{-1}$ exists (and is continuous) then we are lucky again, since $f^{-1}:3D\to D\hookrightarrow3D$ has a fixed point $p$ , which necessarily would be a fixed point of $f$ too. (Then $p=f(p)\in D\cap f(D)\neq\varnothing$ .) If $f$ is a translation distance $2$ (any direction) then $x$ and $f(x)$ would be two diametrically opposite points of $D$ , for some $x$ , so $f(x)\in D\cap f(D)\neq\varnothing$ . I came up with this question since it relates (I would think) to a possible ""topological"" proof of another question I posted earlier (which, in turn, relates to yet another question someone else posted). Given a positive semi-definite matrix $B$ does there exist a non-zero vector $z$ with all components non-negative such that all components of $Bz$ are non-negative?","['metric-spaces', 'fixed-point-theorems', 'continuity', 'general-topology', 'algebraic-topology']"
3429963,"Formally, what is the plus/minus $\pm$ object?","My guess is this: $$
\pm: F \to F\times F
$$ where $F$ is a field. For instance, $\pm 2$ maps $2 \in \mathbb{R}$ to the tuple $(2,-2) \in \mathbb{R}\times \mathbb{R}$ . Is it correct to think of the $\pm$ in this manner?","['notation', 'functions']"
3430014,Relation of regular map with the map of tangent Cones,"I am reading Chapter 1 of J.S. Milne notes.
Link is here: https://www.jmilne.org/math/CourseNotes/LEC.pdf I am confused on Example 2.7(a) on page 18 of the notes.  So the situation is we have a regular map $$\phi: \mathbb{A}^{1}\rightarrow V $$ $$t \rightarrow (t^{2}-1,t(t^{2}-1))$$ where $V$ is the variety defined by $Y^{2}-X^{3}-X^{2}=0$ . I guess I am confused on two things.  The first one feels obvious as I feel I am just missing some trivial thing.  However, for the second question, I feel I am missing something important. Why exactly is the tangent cone to $\mathbb{A}^{1}$ at the point $Q=1$ defined to be $k[s]$ where $s$ is the class of $T-1$ in $m_{Q}/m_{Q}^{2}$ ? Also, I am having trouble seeing how a regular map $\phi$ induces a map of tangent cones.  So my question how does $\phi$ determine that $x$ maps to $2s$ and $y$ maps to $2s$ ? Here are my thoughts so far. For question 1, from definition, if our variety is $V=Spec(k[X_{1},...,X_{n}])/\frak{a}$ , then we take the initial part of the polynomials defined in the ideal $\frak{a}$ .  However, for question 1, isn't affine space $\mathbb{A}^{1}$ defined by the zero polynomial.  Of course, the definition is defined for the origin so as $Q=1$ , it would be $Spec(k[T-1]/(0))$ . But $\frak{a}=0$ still so I feel so I am not sure what the tangent cone is. In other words, initial part of $0$ is $0$ right? For question 2, I am just not sure how to go from a regular map to maps of the tangent cone.  I looked on Tangent Spaces and Morphisms of Affine Varieties which gives me an idea of how regular map induces a map of tangent spaces through the Jacobian.  However, I am not sure how a regular map induces a map of tangent cones.  I tried playing around with variables to see how to even get $2s=2(T-1)$ . So as $m_{Q}=T-1$ the quotient $m_{Q}/m_{Q}^{2}$ is $(T-1)/(T^{2}-2T+1)$ .
From the regular map, $x$ maps to $T^{2}-1$ which is $2T-1-1$ which is $2(T-1)=2s$ . I am assuming that is how we figure out where $x$ maps to.  Now, as $y$ maps to $t(t^{2}-1)$ , we do not get the same result. My guess is since on the tangent cone on $V$ , $y=x$ or $y=-x$ , we get $y$ maps to $2s$ or $-2s$ . I feel I am missing something important for question 2.","['affine-varieties', 'algebraic-geometry']"
3430054,Proving that a combinatorial expression is an integer without combinatorics,"The number of ways to tile an $m \times n$ rectangle with dominos is $$S_{m,n} = \prod_{j=1}^{\lceil \frac{m}{2} \rceil} \prod_{k=1}^{\lceil \frac{n}{2} \rceil}\left( 4 \cos^2 \frac{\pi j}{m+1} + 4 \cos^2 \frac{\pi k}{n+1} \right) $$ How can we prove that this expression is an integer without appeal to its combinatorial interpretation? It's been suggested to me that since the expression above is an algebraic integer, it suffices to show that it is rational using Galois theory, but I am not familiar enough with Galois theory to produce such an argument. Edit: Following the hint below, using $\cos(\theta) = \frac{e^{i\theta} + e^{-i\theta}}{2}$ , we have that $\cos^2(\theta) = \frac{e^{2i\theta}  + e^{-2i\theta} + 2}{4}$ with which we can rewrite $$S_{m,n} = \prod_{j=1}^{\lceil \frac{m}{2} \rceil}\prod_{k=1}^{\lceil \frac{n}{2} \rceil} \left(e^{\frac{2\pi i j}{m+1}} + e^{-\frac{2\pi i j}{m+1}}  + e^{\frac{2 \pi i k}{n+1}} + e^{-\frac{2 \pi i k}{n+1}} +4 \right) =\prod_{j=1}^{\lceil \frac{m}{2} \rceil}\prod_{k=1}^{\lceil \frac{n}{2} \rceil} \left(\zeta_{m+1}^j + \zeta_{m+1}^{-j} + \zeta_{n+1}^k + \zeta_{n+1}^{-k} +4 \right) $$ Clearly, $S_{m,n} \in \mathbb{Q}[\zeta_{m+1},\zeta_{n+1}]$ where $\zeta_k = e^\frac{2\pi i }{k}$ . Then $\Gamma(\mathbb{Q}[\zeta_{m+1},\zeta_{n+1}]/\mathbb{Q}) \subseteq (\mathbb{Z}/m \mathbb{Z})^\times \times (\mathbb{Z}/n \mathbb{Z})^\times$ generated by the automorphisms $\zeta_{m+1} \mapsto \zeta_{m+1}^a$ for $1 \leq a < m+1$ relatively prime to $m+1$ and $\zeta_{n+1} \mapsto \zeta_{n+1}^b$ for $1 \leq b < n+1$ relatively prime to $n+1$ . From here, I don't know how to show that $S_{m,n}$ is invariant under this action, as (for example) $\zeta_{m+1}^{\lceil \frac{m}{2} \rceil -1} \mapsto \left(\zeta_{m+1}^{\lceil \frac{m}{2} \rceil -1}\right)^2$ gives rise to a term outside the index of the product. We can rewrite $$S_{m,n} = 2\prod_{j=1}^m \prod_{k=1}^n \left(\cos \frac{\pi j}{m+1} + i \cos \frac{\pi k}{n+1} \right) $$ using the properties of the cosine. We further expand this as above: $$S_{m,n} = \prod_{j=1}^m \prod_{k=1}^n \left(e^{i\frac{\pi j}{m+1}} + e^{-i\frac{\pi j}{m+1}} + e^{i\frac{\pi k}{n+1}} + e^{-i\frac{\pi k}{n+1}} \right)  = \prod_{j=1}^m \prod_{k=1}^n (\zeta_{2m+2}^j + \zeta_{2m+2}^{-j} + \zeta_{2n+2}^k + \zeta_{2n+2}^{-k})$$ but we have the same problem here as above, there are too many automorphisms.","['galois-theory', 'combinatorics', 'tiling']"
3430063,Find rectangle vertices from 4 points located at rectangle faces.,"If I have 4 points: $P_a$ , $P_b$ , $P_c$ , and $P_d$ . And each of these points lies on a different face of a rectangle, how do I find the vertices ( $V_1$ , $V_2$ , $V_3$ , and $V_4$ ) of this rectangle? Rectangle with vertices v1,v2,v3 and v4 Thank you!","['euclidean-geometry', 'rectangles', 'geometry', 'geometric-construction']"
3430110,Cross-entropy and determinant of cross-covariance matrix,Can the cross-entropy between two multivariate gaussians be expressed using the determinant of cross-covariance matrices?,"['statistics', 'entropy', 'information-theory']"
3430140,The set {x ∈ $R^n$: $\sum_{i=1}^n x_i = 1$},"I don't really understand the notation of this set: {x ∈ $R^n$ : $\sum_{i=1}^n x_i = 1$ }
what would be exemplary numerical values for this set?
Would it be e.g. [0.3 ; 0.7] or e.g. [0.1 ; 0.2 ; 0.3 ; 0.4]?",['elementary-set-theory']
3430164,Minimums of $-g(x)\cos(2 \pi h(x))$?,"Let $f(x) = -g(x)\cos(2 \pi h(x))$ , where $h(x)$ and $g(x)$ are both
continuous and invertible functions. Let the ""attraction basin"" of a minima of $f(x)$ be defined as the
set of points which lead to that minima when gradient descent is
performed. Basically,  the region around the minima where if you ""let
a ball go"", it would ""roll down"" to the minima. (Not sure how better
to explain it, if this is confusing or ambiguous let me know). Finally, let $L(f(x))$ be a function that returns the local minima of
the attraction basin of x. For example, if $f(x) = -\cos(2\pi x)$ , then minimas occur at integer coordinates so $L(f(x)) = round(x)$ . If $x = 0.4$ , then $L(f(0.4)) = round(0.4) = 0$ . If you performed gradient descent at 0.4, you would reach 0. I am trying to figure out how to make this work for $f(x) = -g(x)\cos(2\pi h(x))$ . Without the $g(x)$ , it is simple and $L(f(x)) = h^{-1}(round(h(x)))$ , because $h(x)$ is invertible. However, with the $g(x)$ , I am stumped. How can I figure out $L(f(x))$ for that? In other words, how can I predict where the minimas and their attraction basins are? I know that $f'(x) = 2\pi g(x) h'(x) \sin(2\pi h(x)) - g'(x) \cos(2\pi h(x))$ , and the minimas come when $f'(x) = 0$ . What are necessary conditions on $g(x)$ such that we can we predict the minimas of $f(x)$ ? Any help is very, very much appreciated. Thanks!","['optimization', 'maxima-minima', 'analysis', 'real-analysis']"
3430171,Monotonicity of function averages,"Please let me know if you know an answer to this problem. May be you could provide a reference to some publication on this topic? Let $f(x)$ be a real-valued  strictly convex function on $[0, 1]$ . For any integer $k$ between $0$ and some positive integer $n$ let $x_k =k/n$ . Consider the average value function $g(n)= \sum_{k=0}^{n}f(x_k)/(n+1) $ . Is it true that $g(n)$ is decreasing? Thank you!","['inequality', 'analysis', 'real-analysis', 'functions', 'average']"
3430205,"What are the no of ways in which we can place 7 apples in 5 containers, given neither apples nor containers are identical.","What are the no of ways in which we can place $7$ apples in $5$ containers such that each container contains at least $1$ apple, given neither apples nor containers are identical My attempt is as follows: As containers are not identical, so let's enumerate them as $C_1,C_2,C_3,C_4,C_5$ and as apples are not identical, so let's enumerate them as $A_1,A_2,A_3,A_4,A_5,A_6,A_7$ Now let's try to fill one apple in all the $5$ containers. No of ways to fill one apple in container $C_1=7$ No of ways to fill one apple in container $C_2=6$ No of ways to fill one apple in container $C_3=5$ No of ways to fill one apple in container $C_4=4$ No of ways to fill one apple in container $C_5=3$ Let's multiply all of them to get the no of ways in which we can fill $1$ apple in each of the container $=7\cdot6\cdot5\cdot4\cdot3=2520$ Now in each of the $2520$ ways, $2$ apples will be left at the end, now as all the containers are containing at least one apple, we can put the remaining $2$ apples in any of the containers. So no of ways to place remaining $2$ apples in any of the containers $=5\cdot5=25$ So $2520\cdot25=63000$ should be the answer.But actual answer is $11760$ . Where am I making the mistake. I tried to find it but didn't get any breakthroughs.","['permutations', 'combinations', 'combinatorics']"
3430206,Stability Analysis Near Fixed Point with Suspected Centers,"$$
\begin{cases}
 \dot{x} = -y\\
 \dot{y} = x - \frac{1}{2}x^2 + \frac{1}{2} y^2 \\
\end{cases}
$$ I'm working on this system right now, and am unsure how to analyze it near the fixed point $(0,0)$ . The eigenvalues of the linearized system have non-zero real part, so linearization is inconclusive. I then plotted it in Python and some online tools to see what I should expect , and consistently saw a continuum of centers near the origin. I'm used to this signalling a Hamiltonian system, but it isn't here. Moreover, since the periodic orbits are not limit cycles, Poincaré-Bendixson theorem isn't useful to my knowledge. I'm thinking that there should be an invariant of the system that describes these orbits. That is, some $G(x, y)$ such that $\dot{G} = 0$ , or at least a (not strict) Lyapunov Function. But guessing the form is proving difficult. The orbits seem to collapse into circles near the origin, so I suspected that it would be something like: $$G(x, y) = x^2 + y^2 + f(x, y)$$ Where $f(x, y) = o(x^2 + y^2)$ . This will be positive in a neighborhood of the origin (excluding the origin). With $f(x, y) = -x^3$ , we get: $$\dot{G} = (2x - x^2)(-y) + 2y(x - \frac{1}{2}x^2 + \frac{1}{2}y^2 ) = y^3 $$ Which doesn't seem like a bad start, but isn't there yet. I believe that $f(x, y)$ will require both $x$ and $y$ dependence, but again I'm not sure how yet.","['stability-in-odes', 'periodic-functions', 'ordinary-differential-equations', 'lyapunov-functions']"
3430274,$X^{2n} - 2X^n -7$ irreducible over $\mathbb{Q}$?,I read something somewhere that would imply $X^{2n} - 2X^n -7$ is irreducible over $\mathbb{Q}$ . Is there an easy way to prove this?,"['irreducible-polynomials', 'abstract-algebra', 'polynomials']"
3430331,Can we differentiate Fermat's little theorem?,"From Fermat's little theorem and factor theorem, for any $x \in \mathbb{Z}/p\mathbb{Z}$ , $$x(x-1)(x-2)\cdots(x-p+1)\equiv x^p-x$$ is satisfied. If we take derivative of this, we get $$\sum_{i=0}^{p-1}\prod_{j=0,i\neq j}^{p-1}(x-j)\equiv -1.$$ This formula can be derived by another way. \begin{align}&\sum_{i=0}^{p-1}\prod_{j=0,i\neq j}^{p-1}(x-j)\\
\equiv&\prod_{j=0,j\neq x}^{p-1}(x-j) +\sum_{i=0,i\neq x}^{p-1}\frac{1}{x-i}\prod_{j=0}^{p-1}(x-j) \\
\equiv&\prod_{j=1}^{p-1}j+\sum_{i=1}^{p-1}i\prod_{j=0}^{p-1}(x-j)\\
\equiv& -1
\end{align} I prefer former way (because it's simpler!). However, I don't know why I can apply derivative although it's mod p. Can we prove it's ok to apply derivative?","['number-theory', 'derivatives']"
3430453,closed rectangle of content zero,"How can I prove that set of the form $$[a_1,b_1]\times[a_2,b_2]\times...\times[a_n,b_n]$$ is not a content-zero set, where $a_i<b_i\forall 1\leq i\leq n$ . What I tried I know the proof for $n=1$ case, I tried to mimic the proof for general $n$ . But that seems difficult. I also tried to get a contradiction to the fact that content of 1 closed interval is not zero, but all in vain. Please help.","['measure-theory', 'real-analysis']"
3430486,Torsion Intuition,"Here is the formula that my instructor gave me to solve for torsion in Calc3: $\tau = \frac{-d\vec{B}}{dS} \cdot{\vec{N}}$ However, I'm having some trouble understanding the intuition behind this formula. I understand that Torsion measures the twist of a function through space, kinda like the normal measures the turn of a function. For example, if we have a spring, then the rise of the spring would represent the torsion. However, I do not understand where the $\cdot{\vec{N}}$ part comes from. Physically, a dot product represents the magnitude of the component of a vector that lies on another vector. But in this case, why do we care about the component of the derivative of the $\vec{B}$ that's on the Normal vector?","['multivariable-calculus', 'vector-analysis', 'differential-geometry']"
3430537,"Show that the graph of a function has measure zero (Stein & Shakarchi, Analysis III, Exercise 2.7)","In the book ""Real Analysis III"" by Stein and Shakarchi, on page 91, the problem 7 of the chapter 2 asks: Let $\Gamma\subset\mathbb{R}^d\times\mathbb{R}$ , $\Gamma=\{(x,y)\in\mathbb{R}^d\times\mathbb{R} : y = f(x)\}$ , and assume $f$ is measurable on $\mathbb{R}^d$ . Show that $\Gamma$ is a measurable subset of $\mathbb{R}^{d+1}$ , and $m(\Gamma)=0$ . I am simply wondering if my proof is correct, as every solution I find online is way more complicated. It goes like this: Define the new function $F:\mathbb{R}^{d+1}\to\mathbb{R}$ given by $F(x,y)=f(x)$ . The Corollary 3.7 in the book states precisely that since $f$ is measurable, the new function $F$ is also measurable on $\mathbb{R}^{d+1}$ . Therefore, the set $\Gamma = \{F\leq 0\}\cap \{F\geq 0\}$ is measurable, as the intersection of two measurable sets. Now, to compute $m(\Gamma)$ , notice that $$ m(\Gamma) = \int_{\mathbb{R}^{d+1}} 1_{\Gamma}(x,y)\,dx\,dy = \int_{\mathbb{R}^d}\int_{\mathbb{R}}1_\Gamma(x,y)\,dy\,dx$$ by Fubini's theorem. But when $x$ is fixed, $1_\Gamma(x,y)=1_{\{f(x)\}}(y)$ , so $$\int_{\mathbb{R}}1_\Gamma(x,y)\,dy = \int_{\mathbb{R}} 1_{\{f(x)\}}(y)\,dy = m(\{f(x)\})=0$$ and so $$m(\Gamma) = \int_{\mathbb{R}^d} 0\,dx = 0.$$ As I asked at the beginning, is my reasonning correct?","['measure-theory', 'proof-verification', 'real-analysis']"
3430580,Always increasing condition of a function,"In a classroom I was told today that the function is always increasing if $f'(X)\ge0$ . Interestingly my teacher took a function as $f(x)=x^3+3x^2+3x+5$ which is always increasing although at $x=-1$ , derivative will be zero. Now this equal to zero disturbed me a lot. Suppose there is a function which is increasing and then for some continuous interval it gets constant and after that interval it again starts increasing. Now $f'(x)\ge0$ will be satisfied in such a case, but is such a function always increasing, obviously its no. I tried to google it but couldn't find such a case. What am I missing here?","['calculus', 'functions', 'derivatives']"
3430590,"If the set of homomorphisms from a group $G$ to a group $H$ is in bijection with $H$ for any $H$, does $G = \mathbb{Z}$?","It is not so hard to show that a homomorphism from the integers to a group $H$ must be of the form $$f(m) = x^m \quad \text{ for some } x \in H$$ Consequently, we get a one-to-one correspondence between $\text{Hom}(\mathbb{Z}, H)$ and $H$ itself (note that I'm using $\text{Hom}(G, H)$ to mean the set here; I'm not implying it has a group structure). However, I'm wondering if the converse is true: if $\text{Hom}(G, H)$ is bijective with $H$ for any group $H$ , must we have $G = \mathbb{Z}$ ? To prove this, I thought I'd show $G$ has to be infinite and cyclic. I got the infinite part; if we take $H = \mathbb{Z}_p$ , then choose any nontrivial homomorphism $f$ (which must exist since $p > 1$ ) and any $g \in G$ that does not get sent to the identity under $f$ , and we get $$\{f(g), f(g^2),\ldots, f(g^p)\}$$ are all distinct. Thus, so are $\{g, g^2,\ldots, g^p\}$ , so $G$ contains elements of order at least $p$ for any $p$ and is thus infinite. However, I'm struggling to get that $G$ is cyclic (if it's even true!), and I'd be interested to see if anyone has any thoughts. Thanks!","['group-theory', 'abstract-algebra']"
3430646,Problems finding a Lyapunov function,"I've been trying to find the Lyapunov function for the following system, $\dot x= x^{2}-x-y$ , $\dot y=x$ . Since Ive tried all typical ones, and some variations I havent been able to conclude that the derivative of the function is $<0$ . The main question, I guess, is how to face the problem of finding a Lyapunov function when some term is even, which is the case of $\dot x$ . Probably there is a clever way to find this kind of functions and for sure Im missing it so, any help would be incredibly appreciated <3","['stability-in-odes', 'calculus', 'stability-theory', 'ordinary-differential-equations']"
3430671,"Blow-up, strict transform and tangent cone (Gathmann Notes, Exercise 9.22)","I'm studying Gatmann's Notes (version of 2014) https://www.mathematik.uni-kl.de/~gathmann/de/alggeom.php I'm currently reading the Chapter 9. Birational Maps and Blowing Up. I'm trying to do exercise 9.22 which appears to be important. Exercise 9.22 (Computation of tangent cones). Let $I\trianglelefteq K[x_1,\dots,x_n]$ be an ideal, and assume that the corresponding affine variety $X=V(I)\subseteq \mathbb{A}^n$ contains the origin. Consider the blow-up $\tilde{X}\subseteq \widetilde{\mathbb{A}^n}\subseteq \mathbb{A}^n\times \mathbb{P}^{n-1}$ at $x_1,\dots,x_n$ , and denote the homogeneous coordinates of $\mathbb{P}^{n-1}$ by $y_1,\dots,y_n$ . (a) By example 9.15 we know that $\widetilde{\mathbb{A}^n}$ can be covered by affine spaces, with one coordinate patch being \begin{align}
\mathbb{A}^n&\to \widetilde{\mathbb{A}^n}\subseteq \mathbb{A}^n\times \mathbb{P}^{n-1}\\
(x_1,y_2,\dots,y_n)&\mapsto((x_1,x_1y_2,\dots,x_1y_n),(1:y_2:\dots:y_n)).
\end{align} Prove that on this coordinate patch the blow-up $\tilde X$ is given as the zero locus of the polynomials \begin{equation}
\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}
\end{equation} for all non-zero $f\in I$ , where $\min\deg f$ denotes the smallest degree of a monomial in $f$ . (b) Prove that the exceptional hypersurface of $\tilde X$ is \begin{equation}
V_p(f^{in}:f\in I)\subseteq \{0\}\times \mathbb{P}^{n-1}
\end{equation} where $f^{in}$ is the initial term of $f$ , i.e. the sum of all monomials in $f$ of smallest degree. Consequently, the tangent cone of $X$ at the origin is \begin{equation}
C_0X=V_a(f^{in}:f\in I)\subseteq \mathbb{A}^n.
\end{equation} (c) If $I=(f)$ is a principal ideal prove that $C_0X=V_a(f^{in})$ . However, for a general ideal $I$ , show that $C_0X$ is in general not the zero locus of the initial terms of a set of generators for $I$ . I'm stuck at (a), which I think it's related to (b) and (c). I have done the following but it might be wrong: First, I'm gonna state two Lemmas that I think are right Lemma 1: Let $X=X_1\cup\dots\cup X_r$ be the decomposition of a Noetherian space into irreducible subspaces. If $A$ is a closed subset of $X$ such that for each $i=1,\dots,n$ , $X_i\not\subseteq A$ , then $X\setminus A$ is dense in $X$ . Lemma 2: If $f\neq 0$ , then \begin{equation}
\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}\notin (x_1).
\end{equation} Let's call $\phi:\mathbb{A}^n\to \widetilde{\mathbb{A}^n}$ the morphism defined in (a). If $\pi:\widetilde{\mathbb{A}^n}\to \mathbb{A}^n$ is the map associated to the blow-up, I believe I can prove the following equality \begin{equation}
\phi(V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\})\setminus V(x_1))=\pi^{-1}(X\setminus\{0\})\cap \phi(\mathbb{A}^n).
\end{equation} From there, If I could prove that \begin{equation}
\overline{V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\})\setminus V(x_1)}=V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\}),
\end{equation} then the exercise would be done just by taking closures. I think Lemma 1 and 2 come into play here. The problem is that $V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\})\setminus V(x_1)$ might not be dense in $V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\})$ because it may happen that $X_i\subseteq V(x_1)$ for some irreducible component $X_i$ of $V(\frac{f(x_1,x_1y_2,\dots,x_1y_n)}{x_1^{\min\deg f}}:f\in I\setminus\{0\})$ . Again, I might have made a mistake, so please read critically. I am mainly interested in (a), but a complete answer is also welcome.","['algebraic-geometry', 'blowup']"
3430679,Calculating probability of game ending after $n$ flips,"Two players A and B flip a coin sequentially. The game finishes when the sequence TTH is formed and player A wins or the sequence HTT is formed and player B wins. What is the probability that the game will finish at the $n$ -th flip? What I did: A wins iff the sequence is $n-1$ T's followed by a single H : $\frac{1}{2^n}$ B wins iff the sequence ends with HTT and we have no two consecutive T's in the first $n-3$ flips: this happens (I think) with probability $\frac{F_{n-2}}{2^n}$ where $F_{n}$ is the $n$ -th Fibonacci number. I proved this by induction. Thus, the total probability is $\frac{F_{n-2}+1}{2^n}$ Can someone verify that this is correct and/or share how you would solve this problem? If the answer is correct, then by summing over all $n$ we can obtain an interesting identity involving the Fibonacci numbers!","['fibonacci-numbers', 'markov-chains', 'probability']"
3430711,Challenging Integral: $\int_0^\infty\frac{\ln(2+x)\operatorname{Li}_2(-x)}{x(2+x)}dx$,"Prove that $$\int_0^\infty\frac{\ln(2+x)\operatorname{Li}_2(-x)}{x(2+x)}dx=\frac32\operatorname{Li}_4\left(\frac12\right)-\frac{111}{32}\zeta(4)-\frac78\ln2\zeta(3)-\frac98\ln^22\zeta(2)+\frac1{16}\ln^42$$ where $\operatorname{Li}_r(x)=\sum_{n=1}^\infty\frac{x^n}{n^r}$ is the polylogarithm function and $\zeta$ is the Riemann zeta function. This problem is created by Cornel and can be found here . I managed to solve it but my solution turned out really long and I used results of many harmonic series, so is there an elegant way that spares us the tedious calculations? My solution will be posted soon in the answer section as its too long to be posted here.  Thank you.","['integration', 'real-analysis', 'harmonic-numbers', 'calculus', 'sequences-and-series']"
3430750,Intro analysis question related to MVT,"Q) Let $f$ be a real-valued function which is continuous in a
  neighbourhood $N$ of some point $c∈R$ . Suppose that $f$ is
  differentiable on N\ {c} and that $\lim_{x->c} f'(x)=L$ . Show that $f$ is differentiable at $c$ with $f'(c) = L$ . My try: $f$ is differentiable at $c$ if the limit $\lim_{x->c+}(f(x)-f(c))/(x-c)$ exists. As $f$ is differentiable in $(c, c+h)$ , from MVT, there is some $y\in (c, c+h)$ such that interval such that $f'(y)=\frac{f(c+h)-f(c)}{c+h-c}=L$ . This implies that: $\lim_{x->c} f'(y)=\lim_{x->c} L = f'(c)=L$ . I know this isn't correct, but I don't know where I'm going wrong.","['limits', 'derivatives', 'analysis', 'real-analysis']"
3430785,Solving the integral $\int \ln\left ( \frac{1+\sqrt{1-x^2}}{1-\sqrt{1-x^2}} \right ) \frac{dx}{1-x^2}$,"I'm trying to solve the following indefinite integral: $$I = \int \ln\left ( \frac{1+\sqrt{1-x^2}}{1-\sqrt{1-x^2}} \right ) \frac{dx}{1-x^2}$$ The integral is a general case which comes from a physics problem of potential in a capacitor with a rod inside. I tried to figure out any plausible substitution or a transform by introducing a special function, but failed to do that. An approach I tried was by introducing a function: $h(z) = \mathrm{ln}|z| +i\phi$ for $0 < \phi < 2\pi$ . An approach with a special function seemed applicable to the integral, such as the Dilogarithm . Update-1 :
Applying numerical integration from $0$ to $1$ , I obtained (via Matlab, for self-check): fun = @(x) log((1+sqrt(1-x.^2))./(1-sqrt(1-x.^2))).*(1./(1-x.^2))
q = integral(fun,0,1)
>> 4.9348 Which is $\frac{1}{2} \pi^2$ .","['integration', 'special-functions', 'substitution', 'indefinite-integrals', 'complex-integration']"
3430812,Taking a bizarre limit,"Consider the set of integers, $\Bbb{Z}$ . Now consider the sequence of sets which we get as we divide each of the integers by $2, 3, 4, \ldots$ . Obviously, as we increase the divisor, the elements of the resulting sets will get closer and closer. Question: In the limit as $\text{divisor}\to\infty$ , what will the ""limiting"" set be? 
(I don't think it could be $\Bbb{R}$ .)",['limits']
3430882,Finding the number of continuous functions,"Question: Find the number of continuous function(s) $f:[0, 1]\to\mathbb{R}$ satisfying $$\int_0^1f(x)\text{d}x=\frac{1}{3}+\int_0^1f^2(x^2)\text{d}x$$ My approach: I put $x^2=t$ , giving $2x\text{d}x=\text{d}t$ , but I am not able to find/ proceed further. Can anyone help please?","['real-numbers', 'continuity', 'functions', 'definite-integrals']"
3430891,How to find a limit of this?,"I'm trying to solve this problem, $\lim_{x\to 0+}(\sin^2(4x))^{\sin^{-1}(2x)}$ .
It's indeterminate form so I used L'Hospital's rule but I'm stuck at here; $\lim_{x\to 0+}\ln(\sin^2(4x))^{\sin^{-1}(2x)}=\lim_{x\to 0+} \frac{2\ln(\sin(4x))}{1/\sin^{-1}(2x)}=\lim_{x\to 0+}\frac{4(\sin^{-1}(2x))^2\sqrt{1-4x^2}}{\tan x} $ Could you help me? Thank you in advance.","['indeterminate-forms', 'limits', 'calculus']"
3431062,How is Laplace transform more efficient?,"I wrote an answer on Laplace Transform , following a series of lectures by Prof.Ali Hajimiri (kindly take a look at the answer, my question is entirely based on that answer). In this answer, though I was able to arrive at the Laplace transform with operational calculus, I had a hard time figuring out what ""applying Laplace Transform on both sides"" meant, from the standpoint of this answer . I mapped out the steps involved in solving a differential equation, $y'(t)=x(t)$ with the operator-method and using the Laplace transform. Operator Method The Operator method: Assuming the inputs $x(t)$ and $y(t)$ are the impulse responses of a system with system operators $X(D)$ and $Y(D)$ , will imply, $Y(D)=H(D)X(D)$ , and then we can find $y(t)$ by reverse mapping. So a convolution in time domain $y(t)=h(t) * x(t)$ ( $h(t)$ is the impulse response of the system) becomes multiplication in ""operator"" domain $Y(D)=H(D)X(D)$ Using Laplace Transform Laplace Transform method : While taking laplace transform on both sides of a differential equation, $y'(t)=x(t)$ in this example, we are assuming y(t), is the impulse response of a system say sys1 and x(t) is the impulse response of another system say sys2 . $Y(D)$ be the system operator of sys1 . Now we need $y'(t)$ , so the system operator becomes $DY(D)$ to produce $y'(t)$ . And $X(D)$ be the system operator of sys2 . Since $y'=x$ the two machines $DY(D)$ and $X(D)$ must be the same. Now if we give an input $e^{st}$ to sys1 and sys2 , we know the system operator for sys1 becomes $sY(s)$ and for sys2 it's $X(s)$ . Now as these two systems are equal, $sY(s)=X(s)$ which implies that $Y(s)=\dfrac{X(s)}{s}$ . Now reverse map $Y(s)$ to obtain $y(t)$ . Is my interpretation correct? As far as I can see, Laplace transform has just complicated things. How does it simplify the process. Why do we use Laplace transform? I understand that this question is highly specific. But if someone can help, it would be highly helpful. Thanks in advance. Example: Consider a differential equation, $y'' + 3y' + 2y = e^{-3t}u(t)$ , with zero initial conditions, where $y$ is function of $t$ and $u(t)$ is the step function, $i)$ Operator Method: $$(D^2+3D + 2)y = e^{-3t}u(t)$$ $$y=\left( \dfrac{1}{D^2+3D + 2}\right) e^{-3t}u(t)$$ Expressing $y$ and $e^{-3t}$ in terms of $\delta(t)$ $$Y(D)\delta(t) = \left( \dfrac{1}{D^2+3D + 2}\right) \left( \dfrac{1}{D+3} \right) \delta(x)$$ $$\implies Y(D) = \dfrac{1}{(D+1)(D+2)(D+3)}$$ $$Y(D) = \dfrac{1/2}{D+1} - \dfrac{1}{D+2}+\dfrac{1/2}{D+3} \tag*{...(1)}$$ Reverse mapping, $$y(t)= \left(\dfrac{e^{-t}}{2} - e^{-2t} + \dfrac{e^{-3t}}{2}\right)u(t)$$ $ii)$ Using Laplace transform: Taking Laplace transform on both sides, $$s^2Y(s) + 3sY(s) + 2Y(s) = \dfrac{1}{s+3}$$ $$Y(s) = \dfrac{1/2}{s+1} - \dfrac{1}{s+2}+\dfrac{1/2}{s+3}$$ We could've replaced $D$ by $s$ , in $(1)$ since the input is of the form $e^{st}$ Now, taking inverse Laplace transform on both sides, $$y(t)= \left(\dfrac{e^{-t}}{2} - e^{-2t} + \dfrac{e^{-3t}}{2}\right)u(t)$$ Note, if the input contained any of the natural frequencies ( $e^-t$ or $e^-2t$ ), we can't use the Laplace transform to obtain the response, but we can still use the operator method . And we can obtain only the forced response with the Laplace, but the operator method also  gives us the natural repsonse. Which again leads me to the question, why is Laplace transform more efficient? In-short, give me an example where, an ODE is unsolvable with the operator-method, but becomes a piece of cake with the Laplace transform. I don't exactly intend to prove that the operator method is superior or something, but I want to know why Laplace is preferred over the operators? In what way, does that work around simplify the journey? When I look at it, we actually need not even take Laplace on both sides to get H(s) we just have to find H(D) and replace D by s, which seems to be relatively easy, isn't it? Edit: Following are the thing(s), I could see, When we use Laplace transform, we have a way to analyse the system's response to various frequencies, just by looking at the transfer function(D replaced by s in system function), as it's a function of input frequency $s$ . That's the only advantage as far as I can see, but I guess I'm not experienced enough to appreciate its significance. The function $e^{st}$ is an Eigen function for the differential operator and the Laplace Transform performs a change of basis in disguise. I don't know how to relate this linear algebra interpretation to this map of operations. Does this mapping of the operations shed some light on how the change of basis takes place, or is it completely unrelated(it can't be)? Someone shared a link, Operator Calculus . That article really gave some insight as to why Laplace overtook Operators, from an historical perspective . Primarily because(as far as I can see) Heaviside really thought it was contemptuous to ""prove"" his methods. But is there no mathematical convenience at all? Sub-question: What actually allows us to use the operator like a variable? Specifics on operator-method: What is the operator method? How to handle time delays? How to handle initial conditions?","['ordinary-differential-equations', 'operator-theory', 'laplace-transform', 'calculus', 'linear-algebra']"
3431118,"Conjugacy classes of GL(3, 2)","I need to find all conjugacy classes of GL(3, 2) in order to determine their cardinality and give a representative for each of them. I know that the number of conjugacy classes should be 6 and that one of these is surely the one containing just the identity. However, I don't understand how to find the five remaining. I understood the definition of conjugacy classes, but I don't know how to proceed; since we have 168 elements in the group I don't think I should just try randomly. Can someone please explain me what should I do, if there is a strategy or something? Also, I ask you please not to be rude even if the answer is obvious or easy. I'm not very good at Algebra and so I'd like to have detailed answers in order to understand. Thanks.","['matrices', 'group-theory', 'abstract-algebra']"
3431149,Where is Axiom of Choice used in the proof of Riesz and/or Hahn-Banach extension theorems?,"I read the proof of the Hahn-Banach extention theorem on wikipedia and see no mention of the axiom of choice. The proof goes by constructing an appropriate convex cone and then invoking the Riesz extension theorem, whose proof is based on transfinite induction. However, I've heard several times that proving the Hahn-Banach extension theorem (for topological vector spaces) requires AC or some slightly weaker version (ultrafitrations). I conclude that if the proofs on wikipedia are correct, then there must be a disguised use of AC somewhere (in Riesz of Hahn-Banach extension theorem). Question So, where is AC used in the proof of the Riesz or Hahn-Banach extension theorems ?","['banach-spaces', 'topological-vector-spaces', 'reference-request', 'functional-analysis', 'hahn-banach-theorem']"
3431184,"Degree of extension $\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p))/\mathbb{Q}(\cos(2\pi/p))$.","Let $p$ be an odd prime number. I want to compute the degree $$
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))].
$$ I already showed that $$
[\mathbb{Q}(\cos(2\pi/p)):\mathbb{Q}] = \frac{p-1}{2},
$$ so it will be helpful to know $$
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}].
$$ We know that the polynomial $$
X^2+\cos^2(2\pi/p)-1 \in \mathbb{Q}(\cos(2\pi/p))[X]
$$ vanish on $\sin(2\pi/p)$ , but I don't how to show that this polynomial is irreducible over $\mathbb{Q}(\cos(2\pi/p))$ . I believe that $$
[\mathbb{Q}(\cos(2\pi/p),\sin(2\pi/p)):\mathbb{Q}(\cos(2\pi/p))]=2,
$$ but I cannot see why it is possible or not to write $$
\sin(2\pi/p) = \sum_{k=0}^{n} a_k\cos^k(2\pi/p)
$$ for some $a_0,a_1,\dots,a_n\in\mathbb{Q}$ . Any help will be appreciated!","['field-theory', 'abstract-algebra', 'extension-field']"
3431191,Surjective function: Discrete Maths,"How to know whether this function is surjective or not? I know that we have represent x in terms of y and then substitute some value of y for which the domain of x is not satisfied, but for this sum, how to represent x in terms of y?",['discrete-mathematics']
3431203,"Convergence of $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$, with $a_1=\sqrt{7}$, $a_2 = \sqrt{7-\sqrt{7}}$?","How to prove the convergence of the real sequence $\{a_n\}$ ,  which is defined by $a_{n+2} = \sqrt{7-\sqrt{7+a_n}}$ ,  with $a_1=\sqrt{7}$ , $a_2 = \sqrt{7-\sqrt{7}}$ ?  Furthermore, how to verifty that 2 is the limit?","['limits', 'sequences-and-series', 'recurrence-relations', 'real-analysis']"
3431214,"Find all triples of non-negative real numbers $(a,b,c)$","Find all triples of non-negative real numbers $(a,b,c)$ such that: $a^2+ab=c$ $b^2+bc=a$ $c^2+ca=b$ . This question was problem number of 3 in the RMO(India) Olympiad in 2019 held on $10^{th}$ November. link . My attempt- Assume $a \geq b\geq c$ , $\therefore a^2 \geq b^2$ and $ab\geq bc$ . Adding these two, $a^2+ab\geq b^2+bc$ implies $c \geq a$ . Which can be possible only if $a=c$ , which also implies $a=b=c$ . Substituting in the base equations, $a^2+a^2=a$ $\therefore (a,b,c)=(0,0,0)$ or $(0.5,0.5,0.5)$ I want to know if my method is correct because it seems a lot different than the one provided in the solutions.","['contest-math', 'algebra-precalculus', 'systems-of-equations', 'inequality']"
3431223,Turkey TST 2014 Day 3 Problem 9 Worms that are allowed to move one way,"Turkey TST 2014 Day 3 Problem 9( I've read the official answer, and I find the official answer incorrect） At the bottom-left corner of a $2014\times 2014$ chessboard, there are some green worms and at the top-left corner of the same chessboard, there are some brown worms. Green worms can move only to right and up, and brown worms can move only to right and down. After a while, the worms make some moves and all of the unit squares of the chessboard become occupied at least once throughout this process. Find the minimum total number of the worms. There in AOPS Some of these discussions, however, are just as problematic as the official answers：I found this somewrong: In the final calculation of the green and Brown crawler through the grid, according to the inclusive-exclusive principle to subtract duplicate. The answer is that the repeated squares do not overlap by default when calculating repetitions. For example, the Green Reptilian AB has crossed C, and the Brown Reptilian D has crossed C, twice, in the manner of the answer, but actually it should have been once The following is an official translation of the Chinese version","['contest-math', 'combinatorics']"
3431315,"$A$ reduced implies $A\otimes_k K$ reduced, for $k$ perfect.","Let $k$ be a perfect field and $k\subset K$ any field extension. Let $A$ be any reduced $k$ -algebra, if it helps we may assume it is finitely generated but the result should be true regardless. How can we prove that $A\otimes_k K$ is also reduced? Here, reduced just means it has no nonzero nilpotent elements, i.e. $Nil(A)=0$ . This question is related to the so-called geometric reducedness of $A$ over $k$ .","['extension-field', 'ring-theory', 'algebraic-geometry', 'commutative-algebra']"
3431457,Recursively enumerable set meets the intersection of subsets,"Write $\phi_p$ for the function defined by the program $p$ , as in these examples on Wikipedia . Suppose that sets $X_i\subset \mathbb N, i=1,2$ satisfy $$\text{ if the domain } W_p=\{x:\phi_p(x)\downarrow\} \text { of }\phi_p \text{ is infinite, then } W_p\cap X_i\ne \emptyset$$ and also $X_i^c$ is infinite and $X_i=W_{p_i}$ for some $p_i$ (they are 'recursively enumerable'). How to prove that $X_1\cap X_2$ also has these properties? The second two properties are clear. But the first isn't. The conditions in the display only guarantee that these is some $x_1\in W_p\cap X_1$ and $x_2\in W_p\cap X_2$ but it may be the case that $x_1\in X_1-X_2$ and $x_2\in X_2-X_1$ so that neither $x_1$ not $x_2$ lies in $X_1\cap X_2$ . How to produce a common $x$ which lies in $W_p\cap X_1\cap X_2$ ?","['elementary-set-theory', 'logic', 'recursion', 'computability']"
3431469,Characterise polynomial roots as intersections of solution sets of its real and imaginary parts,"Consider a polynomial equation $p(z)\equiv\sum_k \alpha_k z^k=0$ for $\alpha_k\in\mathbb C$ . We can always understand this equation as a system of two polynomial equations, given by real and imaginary parts of $p(z)=0$ : $$
p(z)=0\Longleftrightarrow \begin{cases}\operatorname{Re}[p(z)]=0, \\ \operatorname{Im}[p(z)]=0.\end{cases}
$$ As a trivial example, the solutions of $z^2-1=0$ are the intersections of the surfaces described by $x^2-y^2-1=0$ and $xy=0$ . We can therefore understand geometrically the solutions of $p(z)=0$ as intersections of two algebraic curves. For example, generating random polynomials of degree $4$ and plotting the curves corresponding to real and imaginary parts of each, we get curves like the following ones: Code used to generate plots: With[{exprs = Total[
      RandomComplex[{-1 - I, 1 + I}, 5] z^Range[0, 4]
    ] /. {z -> x + I y} // Expand // ReIm // FullSimplify[#, {x, y} \[Element] Reals] &
  },
  ContourPlot[Evaluate@Thread[exprs == 0], {x, -5, 5}, {y, -5, 5},
    PlotPoints -> 50, MaxRecursion -> 4, ImageSize -> 200
  ]
] Where in each figure the blue line is the solution set of $\Re(p(z))=0$ and the orange line that of $\Im(p(z))=0$ . From these figures, we can clearly see that there are always $4$ intersections of blue and orange curves, consistently with the fundamental theorem of algebra. Can anything be said about these curves from a purely geometrical point of view?
Or more generally, can we prove the fundamental theorem of algebra by purely geometrical considerations on the types of pairs of algebraic curves that can be produced by a single complex polynomial?","['algebraic-geometry', 'polynomials', 'geometry']"
3431485,Are these infinite groups decomposable?,"I have been asked to: Decide whether the following groups are decomposable: (a) - $(\mathbb{R^*}, \cdot)$ (b) - $(\mathbb{C}, +)$ (c) - $(\mathbb{Q^*}, \cdot)$ (d) - $(\mathbb{Q}, +)$ I would like a hint for item (a). I believe I was able to do itens (b), (c) and (d). Regarding item (a), I tried to decompose $\mathbb{R^*}$ in rationals and irrationals (but this failed, since the irrationals are not a subgroup) or into algebraic and transcendental numbers (which also fails, since the transcendental numbers are not a subgroup). I also thought about showing that if $\mathbb{R^*} = A \times B$ then $A$ and $B$ do not intersect trivially (thus showing that the group is indecomposable), but I couldn't prove this idea. Regarding item (b), I decomposed $\mathbb{C}$ into $\mathbb{R}$ and $i\mathbb{R} = \{iy \ | \ y \in \mathbb{R} \} $ . Regarding item (c), I wrote that $\mathbb{Q^*} = \langle \ p \ | \ p \  \text{is a prime} \rangle = \langle 2 \rangle \ \oplus \ \langle \ p \ | \ p \  \text{is an odd prime} \rangle $ . EDIT: As pointed in the comments, this decomposition is for the multiplicative group of positive rational numbers. A correct decomposition would be, for instance, $\mathbb{Q^*} = \langle 2, -1 \rangle \ \oplus \ \langle \ p \ | \ p \  \text{is an odd prime} \rangle $ . Regarding item (d), I proved that the group is indecomposable by proving that two non-trivial subgroups don't intersect trivially. My reasoning was the same as in: Why is the additive group of rational numbers indecomposable? . Can anyone give me a hint for item (a)? Thanks in advance.","['group-theory', 'abstract-algebra', 'infinite-groups']"
3431534,Linear Operator in a Banach Space,"I'm trying to get my head around linear operators and their usage with Banach spaces. Could someone help me understand how some properties relate to the following operator? We have Banach space $L^2$ , whose elements are real valued sequences $x = (ξ_{j} ) = (ξ_{1}, ξ_{2}, . . .)$ such that $\sum_{j=1}^∞ |ξ_{j}|^2 < ∞$ , Define the operator $T : L^2 → L^2$ as: $(T x) = (ξj/j)$ , for every $x = (ξ_{j} ) ∈ L^2$ That is, $T x = (ξ_{1},ξ_{2}/2, ξ_{3}/3, . . . ,)$ for every $x = (ξ_{1}, ξ_{2}, ξ_{3}, . . .) ∈ L^2$ It is easy to show that the operator T is linear (as it satisfies the definition). I think that the inverse of $T$ is $T^{-1} x = (j ξ_{j})$ , how can I show that T is both injective and surjective? Furthermore, how would one go about showing that T is bounded, continuous and then show what $||T||$ is?","['operator-theory', 'functional-analysis', 'analysis']"
3431568,Simplifying the result of integration of $\int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx}$,"Evaluate: $$
\int\frac{e^x + e^{3x}}{1-e^{2x}+e^{4x}}\mathop{dx}
$$ I'm trying to simplify my answer so that it matches the keys section, no success so far. The integral itself is pretty simple. Factor $e^x$ in the denominator and then make an obvious substitution: $$
I = \int \frac{e^x(1 + e^{2x})}{1-e^{2x}+e^{4x}}\mathop{dx}\\
t = e^x\, , dt = e^x\mathop{dx}\\
$$ Thus: $$
\begin{align}
I&=\int \frac{1+t^2}{1-t^2 + t^4}\mathop{dt} \\
&={1\over 2}\int\left(\frac{1}{t^2 + \sqrt3t + 1} + \frac{1}{t^2-\sqrt3t+1}\right)\mathop{dt} \\
&={1\over 2}\int\left(\frac{1}{\left(t+{\sqrt3\over 2}\right)^2+{1\over 4}} + \frac{1}{\left(t-{\sqrt3\over 2}\right)^2+{1\over 4}}\right)\mathop {dt}
\end{align}
$$ Which after some further substitutions yields: $$
\boxed{I = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3)}\tag1
$$ However, the answer section suggests that: $$
I = \arctan(2\sinh x)\tag2
$$ Which matches my answer up to a constant , $-{\pi \over 2}$ in this case. Even though the answer is correct, I would still like to see how I could arrive from $(1)$ to $(2)$ , I've given it several tries without any luck. I would appreciate it if someone could show me why: $$
\arctan(2\sinh x) = \arctan(2e^x+\sqrt3) + \arctan(2e^x-\sqrt3) - {\pi\over 2}
$$ Thank you! As pointed out in the comments by @mickep , there is a way to directly arrive at the desired result. I would like to elaborate on it here. Instead of factoring $e^x$ one could factor $e^{2x}$ which would give: $$
\begin{align}
I &= \int \frac{e^{2x}(e^{-x}+e^{x})}{e^{2x}(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\
&= \int \frac{e^{-x}+e^{x}}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\
&= \int \frac{2(e^{-x}+e^{x})}{2(e^{-2x} - 1 + e^{2x})}\mathop{dx}\\
&= \int \frac{2\cosh x}{e^{-2x} - 1 + e^{2x}}\mathop{dx} \\
&= \int \frac{2\cosh x}{4{e^{-2x} - 2e^{x}e^{-x} + e^{2x}\over 4} + 1}\mathop{dx}\\
&= \int \frac{2\cosh x}{(2\sinh x)^2 + 1}\mathop{dx}
\end{align}
$$ Now using a substitution $t = 2\sinh x$ , one may obtain: $$
I = \int \frac{\mathop{dt}}{t^2 + 1} = \arctan(t) = \arctan(2\sinh x) + C
$$ By this approach, we have arrived at the desired result.","['algebra-precalculus', 'real-analysis']"
3431615,Differentiability of the operator norm,"My question is simple. Given finite-dimensional real Banach spaces $V, W$ , is the operator norm on $\mathcal{L}(V, W) \setminus \{ 0 \}$ differentiable? I know the standard Euclidean norm would be, but I don’t know what to do with this.","['derivatives', 'normed-spaces', 'linear-algebra']"
3431662,How to prove that this generalized polynomial has exactly one root?,"I am studying the function $U: \mathbb{R}_{\ge 0} \to \mathbb{R}$ given by \begin{align}
U(l) &= (1-r^l) \left[ T - \tau n^l \left( 1 - \left( 1 - \frac{1}{\tau n^l} \right)^T \right)  \right] \\
&=(1-r^l)\sum_{i=2}^T \binom{T}{i}(-1)^i\frac{1}{(\tau n^l)^{i-1}}
\end{align} where $0 < r < 1$ and $\tau \in\mathbb{Z}_{\ge 1}$ and $n, T \in \mathbb{Z}_{\ge 2}$ are fixed, with $T>\tau$ . This is a utility function in an application. It seems that, regardless of the values of the parameters $r,l,T$ , and $\tau$ (within their respective bounds), $U(l)$ starts at zero (for $l=0$ ), and for some $l_0>0$ increases in $[0,l_0]$ , has a unique local (and global) maximum at $l=l_0$ and is decreasing in $[l_0,\infty]$ and tends to zero as $l\to\infty$ . (See example graph in link below.) Example graph of U I want to prove the part that $U'(l)=0$ has exactly one root (at  some $l_0>0$ ). I have not gotten anywhere with the equation $U'(l)=0$ as is. Neither with $U'(l_0)=U'(l_1)=0$ for $l_0 \ne l_1$ leading to a contradiction.
For $T=2$ it can be solved analytically. Yet another idea is therefore a proof by induction over $T$ , but I cannot see how $U'(l)|_{T=t}$ having exactly one zero leads to $U'(l)|_{T=t+1}$ having exactly one zero. However, I have been able to prove that $U'(0)>0$ and that there exists an $L>0$ such that $U'(l)<0$ for all $l\ge L$ . By the intermediate value theorem, $U'$ (which is continuous) must then have a zero in $(0,L)$ . The question is how to prove that it is only one. If one could establish $U'(l_0)=0 \Rightarrow U''(l_0)<0$ we would be done, since $U'$ is continuous, but I cannot see how to use $U'(l_0)=0$ to prove $U''(l_0)<0$ . First try: The variable substitution $\tau n^l=x$ induces the new function $u: \mathbb{R}_{\ge\tau} \to \mathbb{R}$ given by \begin{align}
u(x) &= (1-\tau^B x^{-B}) \left[ T - x \left( 1 - \left( 1 - \frac{1}{x} \right)^T \right)  \right] \\
&= \sum_{i=2}^T(-1)^i\binom{T}{i} x^{-i+1} - \sum_{i=2}^T(-1)^i\binom{T}{i} \tau^B x^{-i+1-B}
\end{align} (where $B = -\frac{\log r}{\log n} > 0$ )
whose derivative $u'(x)$ (after a binomial expansion) can be written $$
u'(x)=p(x)+q(x)
$$ where $p(x)=\sum_{i=2}^T p_i x^{-i}$ and $q(x)=\sum_{i=2}^T q_i x^{-i-B}$ are generalized polynomials with $$
p_i=(-1)^i (-i+1) \binom{T}{i} \quad \text{and} \quad q_i=-(-1)^i (-i+1-B) \binom{T}{i} \tau^B.
$$ For example, if $\tau = 2, n = 10,
r = 0.7$ and $T = 5$ , then (with rounded off values) $B=0.15$ and $$
u'(x) = -10x^{-2} + 12.86x^{-2.15} + 20x^{-3} - 23.99x^{-3.15} - 15x^{-4} + 17.56x^{-4.15} + 4x^{-5} - 4.63x^{-5.15}.
$$ Since $u'(x)$ is a generalized polynomial, I thought first of using 
[Graham J.O. Jameson, Counting zeros of generalised polynomials:
Descartes’ rule of signs and Laguerre’s extensions , The Mathematical
Gazette 90 (2006), no. 518, 223–234]. Unfortunately, neither the generalized Decartes' rule of signs (Theorem 3.1)
nor the theorem about sign changes in partial coefficient sums for zeros in $(1,\infty)$ (Theorem 4.7) is of any use as $u'(x)$ in general has more than one (cumulative) sign change. Second try: Use the same substitution as above and write the equation $u'(x)=0$ as $g(x)=x$ and check if $g:\mathbb{R}_{\ge\tau}\to\mathbb{R}_{\ge\tau}$ is a contraction (i.e. there is some real number $0\le k<1$ such that $|g(x)-g(y)|<k|x-y|$ for all $x,y\in\mathbb{R}_{\ge\tau}$ ). If so, it would follow from Banach's fixed point theorem that $u'(x)=0$ has exactly one root. Unfortunately $g'(\tau)$ is not $<1$ in general, so $g$ is not a contraction. Are there any other ways or perhaps different approaches altogether to prove that $U'(l)$ (or $u'(x)$ ) has exactly one zero? The second challenge is to prove that $l_0$ increases with $T$ (while keeping all the other parameters fixed). From simulations, this seems to be the case, but I currently have no idea how to prove it.","['calculus', 'derivatives', 'polynomials', 'roots']"
