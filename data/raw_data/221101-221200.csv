question_id,title,body,tags
4526188,What happens with indexes in this proof?,"I am trying to understand a proof we did in our discrete math class. Theorem: For $n \geq 0,$ $$x^{\overline{n}} = \sum_k s(n,k) x^k,$$ where $x^\overline{n} = x (x+1)\cdots (x+n-1)$ and $s(n,k)$ is the Stirling number of the first kind. Proof:
We prove the theorem by induction. For $n=0$ we get $1$ on both sides of the equation. So it remains to show $n-1 \to n$ . $$ 
\begin{align}
x^{\overline{n}} &= x (x+1) \cdots (x+n-1) \\
&= (x+n-1)\cdot x^{\overline{n-1}} \\
&= (x+n-1)\sum_ks(n-1,k)x^k \\
&= x \sum_ks(n-1,k )x^k + (n-1)\sum_ks(n-1,k)x^k \\
&= \sum_k s(n-1,k)x^{k+1} + (n-1) \sum_k s(n-1,k)x^k  \\
&\stackrel{?}{=} \sum_k s(n-1,k-1)x^k  + \sum_k (n-1) s(n-1,k)x^k \\
&= \sum_k (s(n-1, k-1) + (n-1)s(n-1,k))x^k \\
&= \sum_k s(n,k)x^k 
\end{align}$$ The part which confuses me is marked with "" $?$ "". It seems like there was some shift in indexes, which is further made unclear because the notation on $\sum$ simply states $k$ instead of where the $k$ is running from/to. This is how I understand it: $k$ runs from $1$ to $n$ (or simply $0$ for $n=0$ ). So we have: $$
\begin{align}
&\sum_{k=1}^{n} s(n-1,k)x^{k+1} + (n-1) \sum_{k=1}^n s(n-1,k)x^k \\
=& \sum_{k=2}^{n+1} s(n-1,k-1)x^k  + \sum_{k=1}^n (n-1) s(n-1,k)x^k 
\end{align}$$ So how can we add these two sums together? What happens with the indexes and what am I doing wrong?","['summation', 'proof-explanation', 'combinatorics', 'discrete-mathematics', 'stirling-numbers']"
4526191,Why the norm is $>1$ for one projection operator?,"Let $P: c \to c$ be a projection operator onto $c_0 \subseteq c$ . That is, $P$ is a bounded operator such that $P^2=P$ and $\operatorname{Im} P=c_0$ . We are asked to show that $\lVert P\rVert > 1$ . My attempt was:
For any $w\in c$ we have that $$\lVert Pw \rVert= \lVert P^2 w\rVert \leq \lVert P\rVert^2 \lVert w\rVert$$ this implies that $\frac{\lVert Pw \rVert}{\lVert w\rVert}\leq \lVert P\rVert^2$ or equivalently $1\leq \lVert P\rVert$ . I do know why the inequality is strict. Hence, the other side we know that for all $w\in c $ then $w=w-P(w)+P(w)$ where $w-P(w)\in \ker P$ and $P(w)\in c_0= \operatorname{Im} P$ , so $\lvert c \rvert = \lvert \ker P \rvert + \lvert c_0 \rvert > \lvert c_0\rvert$ where i mean $\lvert c \rvert := \operatorname{dim}c$ . I guess that I need to find some particular element in the domain for attach the grater dimension or something like that. I am not sure if $c_0$ are the sequence that converge to $0$ for this reason i did no try other thing with sequences. I will appreciate any hint please. Best","['operator-theory', 'functional-analysis', 'operator-algebras']"
4526192,Inversion of Fourier Transform,"Let $\mu$ be a finite measure on the sigma algebra $\mathcal{B}$ of the borel set (over the real line). The Fourier-Transform of $\mu$ is defined to be the function $\hat{\mu} \; : \; \mathbb{R} \to \mathbb{C}$ such that $\hat{\mu}(t) := \int_{\mathbb{R}}{e^{itx} d\mu(x)}$ Let $\mu,\nu$ be two finite measure on $\mathcal{B}$ . I want to show that if $\hat{\mu} = \hat{\nu}$ then $\mu = \nu$ . To prove it if you wish you can use the following lemma Let $\mu,\nu$ be two finite measure on $\mathcal{B}$ such that $\mu(\, [a,b) \, ) = \nu( \, [a , b) \, )$ for any $a < b$ then $\mu = \nu$","['measure-theory', 'probability-distributions', 'lebesgue-integral', 'fourier-transform', 'real-analysis']"
4526200,"If $x = 0$, then $x/x$ is undefined, right?","It's been a while since I worked with math, but I stumbled upon a rather 'simple' equation and the ways of solving it had me thinking for a bit. Consider this equation: $$x(1-x) = x(2 - \sqrt{1-x})$$ Any normal person would consider making the problem easier by dividing both left and right side of the equal sign by $x$ , but bear in mind that one of the solutions might be $0$ , so this is what makes me wonder, why would it be allowed to even divide by $x$ here, if one of its solutions might be $0$ then that is that $0/0$ is undefined... right? Kindly refresh my memory.",['algebra-precalculus']
4526220,Is this Matrix Indefinite or Positive Semi Definite?,"Consider the following matrix: $ A=\begin{pmatrix}
1&1&1\\ 
1&1&1\\ 
1&1&0
\end{pmatrix} $ The eigenvalues are $\lambda_1=0$ , $\lambda_2=1-\sqrt{3}≤0$ and $\lambda_3=1+\sqrt{3}≥0$ . But when computing the principal minors we obtain: $A_1=1≥0$ $A_2=\begin{vmatrix} 1&1\\  1&1 \end{vmatrix} = 0$ $A_2= \begin{vmatrix} 1&1&1\\ 1&1&1\\ 1&1&0 \end{vmatrix} = 0$ So all leading principal minors are ≥ 0, but we have two eigenvalues with different sign and the third one is zero... Im really confused, thank you!","['matrices', 'multivariable-calculus', 'linear-algebra', 'optimization', 'hessian-matrix']"
4526237,Proving or disproving that $M$ is a submanifold of the flag manifold of $G$,"Let $G$ be a compact connected Lie group and let $T$ be a maximal torus of $G$ .  Denote by $\mathcal{F}=G/T$ the flag manifold of $G$ . Let $\theta : G \rightarrow G$ be an involution on $G$ . This involution induces an involution on the Lie algebra of $G$ , that we denote also by $\theta$ . Consider the natural action of $G$ on $\mathcal{F}$ . Let $x=gT \in \mathcal{F}$ , denote by $G_x$ the stabilizer of $x$ in $G$ and denote by $\mathfrak{g}_x$ its Lie algebra. (Note that $G_x =gTg^{-1}$ ). Consider the set $M:= \lbrace x \in \mathcal{F}, \theta(\mathfrak{g}_x)=\mathfrak{g}_x \rbrace 
$ . $\textbf{Question}:$ Prove or disprove that $M$ is a submanifold of $\mathcal{F}$ . So far I didn't find any example for which $M$ is not a submanifold, and my thoughts are: Since the group $G$ acts transitively on the set $\tilde{\mathfrak{g}}:=\lbrace \mathfrak{g}_x , x \in \mathcal{F}\rbrace $ (this is because $\tilde{\mathfrak{g}}$ is the set of all cartan subalgebras of $\mathfrak{g}$ ) and since the set $M$ is a fixed point set of the involution $\theta$ , then using this On the proof of that fixed point set of an involution is a submanifold ), I conclude that $M$ is a submanifold. is this correct ? $\textbf{Edit:}$ $T$ is assumed to be stable by $\theta$ .","['lie-algebras', 'lie-groups', 'differential-geometry']"
4526244,What is the probability that this happen P(A)+P(B)−2P(A ∩B),"I think that my question has a bad structured, but my question is in base the next. Let A and B be any sets. Show that the probability that exactly
one of the events A or B occurs is: $$P(A)+P(B)−2P(A\cap B)$$ I thought that this is possible with The inclusion-exclusion principle, is it right or I need other thing?","['statistics', 'inclusion-exclusion', 'probability-theory', 'probability']"
4526256,Can a limit exist at $x\to a$ but not equal to $f(a)$?,"In the calculus course I'm taking, the conditions for a continuous function are said to be when a.) $\lim_{x\to a}f(x)$ exists and b.) $\lim_{x\to a}f(x)$ = $f(a)$ And it seems a little redundant, since if a limit exists at $x\to a$ , wouldn't it always equal to $f(a)$ ? Are there situations where that is not the case? I apologize if this was a bad question/ formatted bad, I'm pretty new to math and this site in general.","['limits', 'calculus']"
4526293,Does a non-conjugate cut point imply closed geodesics?,"Let $(M, g)$ be a complete Riemannin manifold and suppose $p, q \in M$ re points s.t. $d_g(p, q)$ is equal to the distance from $p$ to its cut locus, with $q \in \text{Cut}(p)$ . If $q$ is not conjugate to $p$ along some minimizing geodesic segment, does it follow that there exist exactly two minimizing geodesic segments $\gamma_1, \gamma_2 : [0, b] \to M$ from $p$ to $q$ s.t. $\gamma_1'(b) = - \gamma_2'(b)$ ? My first thought was to make a closed loop going from $p$ to $q$ along $\gamma_1$ and then from $q$ to $p$ along $-\gamma_2$ . Then I wanted to deform this loop into one of the geodesics to obtain a Jacobi field vanishing at $q$ , but I fail to see how each curve would be a geodesic, or how this depends on $\gamma_1'(b) \neq -\gamma_2'(b)$ . Then I started wondering if there was something missing from the problem. If I reverse the direction of $\gamma_2$ and concatenate both geodesics, I would obtain a closed geodesic. This seems to imply that on any complete manifold where the closest point on the cut locus of $p$ to $p$ is not conjugate to $p$ , we would have closed geodesics. Is this even true?","['geodesic', 'differential-geometry']"
4526295,Question about solution of wave equation in traveling wave.,"I have a question about pg.10 from Stein and Shakarchi's fourier analysis. We must now connect this result with our original problem, that is, the physical motion of a string. There, we imposed the restrictions $0 \leq x \leq \pi$ , the initial shape of the string $u(x, 0) = f (x)$ , and also the fact that the string has fixed end points, namely $u(0, t) = u(\pi, t) = 0$ for all $t$ . To use the simple observation above, we first extend $f$ to all of $\mathbb{R}$ by making it odd on $[−\pi,\pi]$ , and then periodic in $x$ of period $2\pi$ , and similarly for $u(x, t)$ , the solution of our problem. Then the extension $u$ solves the wave equation on all of $\mathbb{R}$ , and $u(x, 0) = f(x)$ for all $x \in \mathbb{R}$ . Therefore, $u(x, t) = F (x + t) + G(x − t)$ , and setting $t = 0$ we find that $$F(x) + G(x) = f(x).$$ Since many choices of $F$ and $G$ will satisfy this identity, this suggests imposing another initial condition on $u$ (similar to the two initial conditions in the case of simple harmonic motion), namely the initial velocity of the string which we denote by $g(x)$ : $$\frac{\partial u}{\partial t}(x,0) = g(x),$$ where of course $g(0) = g(\pi) = 0$ . Again, we extend $g$ to $\mathbb{R}$ first by making it odd over $[−\pi,\pi]$ , and then periodic of period $2\pi$ . The two initial conditions of position and velocity now translate into the following system: $$F(x) + G(x) = f(x), \quad F'(x) − G'(x) = g(x).$$ It seemed okay to me when it first assumed $f(x)$ to be odd (although I'm not sure why this is allowed), when it assumed the velocity $g(x)$ to be also odd, I don't understand why this can be assumed. Just by assuming $f(x,t)=\sin(x+t)$ it tells me $g(x,t)$ has to be even. Can I get more explanation on why these assumptions are valid? Thank you.","['functional-equations', 'fourier-analysis', 'functions', 'wave-equation', 'partial-differential-equations']"
4526357,Proof by induction for infinite unions/intersections,"Say I want to prove the following statement: The union of a countable collection of countable sets is countable. Imagine I wrote a proof that went something like this: let $E$ be a collection of countable sets, that I can index $E=\{E_1, E_2,...\}$ with the natural numbers, and show for all $n \in \mathbb{N}$ , $\bigcup_{i<n}E_i$ is countable. I understand that this is an invalid argument. The argument above only shows for all finite collections, as we never show $P(n) \implies P(\infty)$ ; that is, we never actually consider the entire collection of sets. Now, consider a different problem. Say I want to prove the Nested Intervals Theorem( https://math.gmu.edu/~dsingman/315/sect1.6nounc.pdf ). The infinite intersection of sets is defined as $\bigcap^\infty E_i=\{ a: a\in E_i \hspace{.2cm}\forall i \in \mathbb{N}\}$ . I believe that an induction argument does work here. If I prove that there is some real number $a$ such that for all $n \in \mathbb{N}$ we have $a \in \bigcap^n_1 E_i$ , then I think that I prove $a \in \bigcap^\infty E_i$ . I think the above works because this argument: assume $a \not \in \bigcap^\infty E_i$ , then $\{i \in \mathbb{N}: a \not \in E_i \} \not = \emptyset$ . let $k$ be the least such element of this set. then, $a \not \in E_k$ but $a \in E_{k-1}$ which contradicts the proof by induction . Of course I have searched through related post. I remain unsatisfied. The best answers on this post, for example, does not agree with me: How is an induction 'proof ' of 'The principle of nested closed intervals' different that the standard proof? . Are there any errors in my thinking?","['elementary-set-theory', 'proof-explanation', 'induction', 'real-analysis']"
4526388,A corollary of Van der Waerden's theorem.,"The following is the well known Van der Waerden theorem: Th. Given positive integers $k,r$ there exists positive integer $N=N(k,r)$ such that if $\{1,2,...,N\}$ is $r$ -colored then there exists a monochromatic AP of length $k$ . Now consider the following statement $S(k,m)$ : Th. Given any two positive integers $k,m$ there exists a positive integer $p=p(k,m)$ such that for any $p$ positive integers $a_1<a_2<...<a_p$ with the property that $|a_{j+1}-a_j|\leq m$ for all $j=1,2,...,p-1$ ,there exists an AP within $a_1,a_2,...,a_p$ of length $k$ . Now our instructor proved that Van der Waerden $\implies S(k,m).$ I have some difficulty in understanding the proof.I think I am not getting the main philosophy.I give the proof below: Proof: Assume that Van der Waerden theorem is true. Let $p=p(k,m)=N(k,m)-(m-1)$ ,we will show that this $p$ does the job. Let $A_0=\{a_1,a_2,...,a_p\}$ be a set of positive integers with the property that $|a_{j+1}-a_j|\leq m$ for all $1\leq j\leq p-1$ . Define $A_1=\{a_1+1,a_2+1,...,a_p+1\}\setminus A_0$ $A_2=\{a_1+2,a_2+2,...,a_p+2\}\setminus A_0\cup A_1$ $\vdots$ $A_{m-1}=\{a_1+(m-1),...,a_p+(m-1)\}\setminus A_0\cup A_1\cup\dots\cup A_{m-2}$ Then $\{a_1,a_1+1,...,a_p+(m-1)\}=A_0\cup A_1\cup\dots \cup A_{m-1}$ $($ Why? $)$ Now $a_p+(m-1)-a_1+1=a_p+m-a_1\geq p+a_1-1+m-a_1=N(k,m)$ $($ Why? $)$ which completes the proof. $($ Why? $)$ I do not really understand what is going on here.I have marked the lines I do not understand by writing 'Why?'.Please go through the proof and help me to digest the essence of the proof.","['proof-explanation', 'intuition', 'combinatorics', 'discrete-mathematics']"
4526397,Please help me evaluate the following integral analytically [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question $$\int_{0}^{5} \left(\arccos\left(\frac{2}{x+2}\right)\right)^2 \, dx
$$ I tried evaluating this integral using traditional techniques such as integration by parts,  u-substitution and trigonometric identities, however,
I was unsuccessful. If this integral is analytically derivable, please specify the technique or analysis framework that could yield results, including multivariable calculus, complex analysis or tricks (such as Feynman's trick). I would prefer a closed-form solution to this integral if possible, for the positive real numbers. Thanks in advance!","['definite-integrals', 'complex-analysis', 'multivariable-calculus', 'calculus', 'indefinite-integrals']"
4526437,Is that true that 2-sphere in $\Bbb R^4$ is like circle in $\Bbb R^3$ (having hole),"Circle in $\Bbb R^2$ is a closed curve but viewing it in $\Bbb R^3$ we know that one can enter from one side and go into hole of circle then one can leave the hole of circle. I think all spheres have such property but I can't justify this claim intuitively or in language of homology groups. For example $\Bbb S^2$ looks has no hole to enter into its hole but I think it is possible in $\Bbb R^4$ . I don't know how to justify this? I think that the phrase ""to enter into its hole"" has no vague point because fortunately sphere has only one hole; i.e. $n$ -dim hole at least on even-dimensional spheres. Sorry for my non standard question.","['general-topology', 'homology-cohomology', 'algebraic-topology']"
4526494,What is wrong in this solution of $\sec x + \csc x = 2 \sqrt{2}$,"Find number of solutions in the interval $[0,2\pi]$ of the equation - $$\csc x + \sec x = 2 \sqrt{2}.$$ $⇒ \dfrac{1}{\sin x}+ \dfrac{1}{\cos x }= 2 \sqrt2$ $⇒ \dfrac{\sin x +\cos x}{\sin x  \cos x} = 2 \sqrt 2 ⇒ (\sin x+\cos x)^2=8\sin^2x\cos^2x⇒1+2\sin x \cos x=8\sin^2x \cos^2x$ $⇒\sin x \cos x=-1/4 ,\sin x \cos x=1/2⇒2\sin x \cos x=-1/2 , 2\sin x\cos x=1$ $⇒\sin2x =-1/2 , \sin2x=1.$ But if we check manually , $\sin2x=-1/2$ is not a solution. $⇒\sin 2x =1⇒2x=(-1)^{n}\dfrac{\pi}{2}+n\pi⇒x=(-1)^{n}\dfrac{\pi}{4}+\dfrac{n\pi}{2}⇒$ For $[0,2\pi], \boxed{x=\dfrac{\pi}{4},\dfrac{5 \pi}{4}}$ are solutions. But if we check the graph neither are the solutions . But $\pi /4 $ is a solution but $5\pi /4 $ is not a solution if we plug values manually. Can someone point out the mistake? Thanks $\text{References:}$ Link for above graph : https://www.desmos.com/calculator/eaakqvd9y5","['trigonometry', 'systems-of-equations']"
4526565,If $P(P(x)-1) = 1 + x^{36}$ then $P(2)=$,Is it possible to solve this problem?Can anyone give me some hints? If $P(P(x)-1) = 1 + x^{36}$ then $P(2)=$ What I did so far. $P(P(2) - 1) = 1+2^{36}$ $P(P(1) - 1) = 1+1 = 2 \Rightarrow P^{-1}(2) = P(1) - 1$ $P(P(0) - 1) = 1+0 = 1 \Rightarrow P^{-1}(1) = P(0) - 1$ $P(P(-1) - 1) = 1+1 = 2 \Rightarrow P(-1)=P(1)$ $P(P(-2) - 1) = 1+2^{36} \Rightarrow P(-2)=P(2)$ $P(x)=1+ \left\{ P^{-1}(x+1) \right\}^{36}$ and $P(2) = 1 + \left\{ P^{-1}(3) \right\}^{36}$ I don't see any connection from the expressions above.,"['algebra-precalculus', 'functions', 'polynomials']"
4526582,Is the natural map composited by direct image functor and inverse image functor a natural isomorphism?,"Assume $\pi: \mathrm{X}\to \mathrm{Y}$ is a continuous map of topological space, then for sheaves respectively on them, the direct image functor is defined as: $\pi_*\mathscr{F}(\mathrm{V})=\mathscr{F}(\pi^{-1}(\mathrm{V}))$ for $\mathrm{V}\subset\mathrm{Y}$ ; The inverse image functor is defined as: $\pi^{-1}\mathscr{G}(\mathrm{U})=(\underrightarrow{\lim}_{\mathrm{V}\supset\pi(\mathrm{U})}\mathscr{G}(\mathrm{V}))^{\mathrm{sh}}$ for $\mathrm{U}\subset\mathrm{X}$ . The two functors are adjoint ( $\pi^{-1}$ is left-adjoint, $\pi_*$ is right-adjoint). Now question is: whether the unit $\epsilon: \pi^{-1}\pi_*\to\mathrm{id}$ is natural isomorphism or not? The question comes from the book The Rising Sea: Foundations of Algebraic Geometry, page 92 and page 136. Page 92: Note that we then get canonical maps $\pi^{-1}\pi_*\mathscr{F}\to\mathscr{F}$ (associated to the identity in $\mathrm{Mor_Y}(\pi_*\mathscr{F},\pi_*\mathscr{F})$ ) and $\mathscr{G}\to\pi_*\pi^{-1}\mathscr{G}$ (associated to the identity in $\mathrm{Mor_X}(\pi^{-1}\mathscr{F},\pi^{-1}\mathscr{F})$ ). I didn't know what this means, so I skipped. But later I met the other sentence (Page 136): ...more precisely, is an isomorphism $\mathscr{O}_\mathrm{Y}\to \pi_*\mathscr{O}_\mathrm{X}$ of sheaves on $\mathrm{Y}$ , or equivalently by adjointness, $ \pi^{-1}\mathscr{O}_\mathrm{Y}\to\mathscr{O}_\mathrm{X}$ of sheaves on $\mathrm{X}$ . Why here says ""equivalently""? Is the latter map the composition $\pi^{-1}\mathscr{O}_\mathrm{Y}\to\pi^{-1}\pi_*\mathscr{O}_\mathrm{X}\to\mathscr{O}_\mathrm{X}$ of sheaves on $\mathrm{X}$ ? If the former is an isomorphism is equivalent to the latter is, whether this means that $\pi^{-1}\pi_*\mathscr{O}_\mathrm{X}\to\mathscr{O}_\mathrm{X}$ is an isomorphism? Associated with the sentence I didn't understand on page 92 showed above, then I come up with the question (I guess maybe that sentence can help, so I show it here). If the answer is not, then what the author really means here? Thank you for helping me! Through the example given by the first comment, I thought of a further question, which is also more closer to the context of the book: If the map $\pi$ is a homeomorphism, then whether the unit $\epsilon: \pi^{-1}\pi_*\to\mathrm{id}$ is natural isomorphism or not? The last question's answer is Yes, as the second comment said. Now my doubts about this are all solved. Thanks again for helping me!","['algebraic-geometry', 'category-theory']"
4526607,Why define norm of linear map as $\sup_{\|x\|=1}\|Ax\|=\|A\|$?,"I studied basic topology and abstract algebra(junior course) before studying multivariable analysis. In abstract algebra, we saw $\mathbb R$ -vector space $\mathfrak M(m, n)=\{A\mid A\text{ is }m\times n\text{ matrix over }\mathbb R\}$ as $\mathfrak L(\mathbb R^n, \mathbb R^m)=\{L:\mathbb R^n\to \mathbb R^m\mid L \text{ is }\mathbb R\text{-linear map}\}$ and subset(subring) of $\mathbb R^{mn}$ . Then, in this case, norm of $m\times n$ matrix $A$ would be $\|A\|=\sqrt{\sum_{1\le i, j\le n}a_{ij}^2}$ , as element of Euclidean space. But, in PMA, Rudin says norm of linear map $L$ as $\|L\|=\sup_{\|x\|=1}\|Lx\|$ . Why should it be defined like this? Is this because the matrix acts as an operator acting on Euclidean space, not as an element of a simple set?","['multivariable-calculus', 'analysis']"
4526621,Papa Rudin $1.33$ theorem,"This is the definition which we need for the proof of the theorem : There is the theorem: If $f$ $\in$ $L^1(\mu)$ , then $|\int_X f d\mu|$ $\leq$ $\int_X |f| d\mu $ . There is the proof: Put $z$ $=$ $\int_x f d\mu$ . Since $z$ is a complex number, there is a complex number $\alpha$ , with $|\alpha|$ $=$ $1$ , such that $\alpha$$z$ $=$ $|z|$ . let $u$ be the real part of $\alpha$$f$ . Then $u$ $\leq$ $|\alpha f|$ $=$ $|f|$ . Hence $|\int_X f d\mu|$ $=$ $\alpha$ $\int_X f d\mu$ $=$ $\int_X \alpha f d\mu $ $=$ $\int_X u d\mu $ $\leq$ $\int_X |f| d\mu $ . I don't understand how we conclude that there is a complex number $\alpha$ , with $|\alpha|$ $=$ $1$ , such that $\alpha z $ $=$ $|z|$ , hence I also don't understand why should $|\alpha|$ be equal of $1$ . Any help would be appreciated.","['lebesgue-integral', 'analysis', 'real-analysis', 'complex-analysis', 'complex-numbers']"
4526635,"How many elements of the form $a^{m_1} b^{n_1} a^{m_2} b^{n_2} a^{m_3} b^{n_3}$ in $G:=\langle a,b \mid a^2, b^4, (ab)^3 \rangle?$","I am trying to show that $G:=\langle a,b \mid a^2, b^4, (ab)^3 \rangle$ is of order at most 24. (To prove that it is a presentation of $S_4$ .) I thought of using the homomorphism $f$ sending $a$ to $(1 2)$ and $b$ to $(1 2 3 4)$ . The image of this homomorphism generates $S_4$ , and it satisfies the relations of $G$ . Hence the homomorphism $f$ extends to a homomorphism $f’$ making the following diagram commute Claim: $f’$ is an isomorphism to its image $f’(G)=S_4$ . Since $f’$ is surjective and its image has order $24$ , it suffices to show that $|G|\le 24$ . Now, any word in $G$ in $a$ and $b$ can be written in the form $a^{m_1} b^{n_1} a^{m_2} b^{n_2} a^{m_3} b^{n_3}$ , or $b^{n_1} a^{m_1} b^{n_2} a^{m_2} b^{n_3} a^{m_3} $ where $m_i \in \{0,1\}$ and $n_i\in \{0,1,2,3\}$ . Now there is $2(2^3\times 4^3)=2(512)=1024$ elements, but since $(ab)^3=1$ , then $G$ ’s elements should be way less than $1024$ , but I am not sure how to show that. Any hint would be appreciated.","['symmetric-groups', 'group-presentation', 'group-theory', 'combinatorial-group-theory']"
4526636,Upper bound on the elements of a Collatz cycle,"Let $T$ be the (shortcuted) Collatz map: $T(x) = x/2$ when $x$ is even and $T(x) = (3x+1)/2$ when $x$ is odd. Böhm and Sontacchi, 1978 show that if $x = T^n(x)$ then $x < 3^n$ with $x,n\in\mathbb{N}^+$ . Can this bound be improved to $2^n$ ? This question is relevant when analysing Collatz cycles in binary as it asks if iterates in a positive integer cycle of size $n$ can always be represented on $n$ bits or not. We originally asked the question in this paper which studies Collatz in base 2 and 3 -- this is not meant to do self-promotion but merely to give more context on this question. Arguably, the question is interesting per se as it is generally hard to give any kind of non-trivial results concerning Collatz cycles (such as Eliahou, 1993 or [Steiner, 1977]). Thank you very much in advance,","['collatz-conjecture', 'discrete-mathematics']"
4526694,Ball pulling game,"There are $16$ red and $15$ white balls in a basket. You and the dealer take turns drawing $2$ random balls from the basket (and do not return it back). If the balls are of different colors, the dealer adds one more white ball to the basket, if both balls are of the same color, then the dealer adds one more red ball to the basket. You win if the last ball in the basket is white, the dealer wins if the last ball in the basket is red. Estimate the probability of your winning. $1$ move Probability of white-white: $\frac{15}{31} \cdot \frac{14}{30}$ Probability of red-red: $\frac{16}{31} \cdot \frac{15}{30}$ So, the probability that a red ball will be added after the first move is: $\frac{15}{31} \cdot \frac{14}{30} + \frac{16}{31} \cdot \frac{15}{30} = \frac{15}{31} $ Probability of white-red: $\frac{16}{31} \cdot \frac{15}{30}$ So, the probability that a white ball will be added after the first move is: $2 \cdot \frac{16}{31} \cdot \frac{15}{30} = \frac{16}{31} $ Mathematical expectation of white balls after the first move: $15 - 2 \cdot \frac{15}{31} \cdot \frac{14}{30} + \frac{16}{31} \cdot (1-1) = 14 + \frac{17}{31} =  \frac{451}{31}$ Mathematical expectation of red balls after the first move: $16 - 2 \cdot \frac{16}{31} \cdot \frac{15}{30} + \frac{15}{31} \cdot 1 - \frac{16}{31} \cdot 1  = 15 + \frac{14}{31} =  \frac{479}{31}$ $2$ move Probability of white-white: $\frac{\frac{451}{31}}{30} \cdot \frac{\frac{451-31}{31}}{29}$ Probability of red-red: $\frac{\frac{479}{31}}{30} \cdot \frac{\frac{479-31}{31}}{29}$ So, the probability that a red ball will be added after the second move is: $\frac{\frac{451}{31}}{30} \cdot \frac{\frac{451-31}{31}}{29} + \frac{\frac{479}{31}}{30} \cdot \frac{\frac{479-31}{31}}{29} = \frac{404012}{29 \cdot 30 \cdot 31^2}$ The probability that a white ball will be added after the second move is: $2 \cdot \frac{\frac{451}{31}}{30} \cdot \frac{\frac{479}{31}}{29} = \frac{432058}{29 \cdot 30 \cdot 31^2}$ Then I can calculate the mathematical expectation of the balls of each color on the second move, and so on, however, there are too many rounds in the game for a direct calculation. It probably needs to do some kind of limit jump or find a recursive dependency, but I'm stuck and having trouble with that. I would appreciate any help.","['probability-theory', 'probability']"
4526695,"Ensuring that a curve $\gamma : [0,1] \to M$ lies on an integral manifold of a distribution $E \subset TM$","Let $M$ be a smooth manifold, $E \subset TM$ a smooth distribution of codimension $k$ , and $\gamma : [0,1] \to M$ a smooth curve whose tangent vector is in $\gamma^\star E$ , and whose individual points are contained in integral manifolds of $E$ . Can we deduce that $\gamma$ is entirely contained in a single connected integral manifold of $E$ ? If $E$ is integrable, then the answer is “yes”, and I can prove it as follows. For each $t \in [0,1]$ , take a distinguished chart in which $E$ is defined by $dx^1 = \dots = dx^k = 0$ . Then $\gamma$ is locally contained in a single plaque of this chart. Then we have an open cover of $\gamma$ by plaques. Take a finite subcover, and their union is a connected integral manifold of $E$ containing $\gamma$ . But what if $E$ is not integrable? EDIT: Rewrote whole question. Never mind, I am no longer in a hurry for an answer. I just realized that I can prove the global Frobenius theorem (every involutive distribution gives rise to a foliation) using only connectedness of the integral manifolds, not path-connectedness.","['foliations', 'tangent-bundle', 'differential-geometry']"
4526717,Conditional expectation between lognormal random variables,"My question is whether the conditional expectation between lognormal random variables $Y$ and $X$ , i.e $\mathbb{E}(Y|X)$ has a closed form linear (or non-linear) expression similar to Gaussian random variables. Recall that if $(Z,W)$ are jointly normal, then $\mathbb{E}(Z|W)=\beta_0 +\beta_1W$ , where $\beta_0=\mathbb{E}(Z)-\beta_1\mathbb{E}(W)$ and $\beta_1=\frac{Cov(Z,W)}{Var(W)}$ . Can we use this result by transforming lognormals to normals and then after using the result for normals, transform it back to logs?","['statistics', 'conditional-probability', 'probability-distributions', 'normal-distribution', 'conditional-expectation']"
4526747,"Minimum spanning forest, for a complete graph.","Given a complete Graph $G(V,E)$ with $|V|=kn$ and weights $w:E→N$ that satisfies ""Triangular Inequality"". That is, for any $v_1, v_2, v_3 \in V$ , $$w(v_1,v_2)\le w(v_1,v_3)+w(v_3,v_2).$$ Can I find the minimum spanning forest $T_1,T_2,…,T_n$ where each tree $T_i$ has exactly $k$ vertices? For $k=2$ , the problem becomes the minimum-weight perfect matching problem, and we can use the Blossom algorithm. I wonder for $k>2$ this problem is P or NP, I am particularly interested in the case with $k=3$ .","['graph-theory', 'np-complete', 'combinatorics', 'computational-complexity', 'decision-problems']"
4526753,In Euclidean space can you always find a sequence approaching a limit point along a line?,"I have what seems like a very simple question: Suppose I have an open set $X \subset R^n$ and a limit point $z$ of $X$ . I would like to find a sequence of points $z_1,z_2,\ldots \subset X$ approaching $z$ along a straight line ; i.e. along the line segment connecting $z_1$ and $z$ . I believe I can do it if $X$ itself is convex but I don't know that. Of course this is all local so if there exists a sequence of open neighborhoods $U_1,U_2,\ldots$ of $z$ such that, for all $k > n_0$ , $U_k \bigcap X$ is convex then it should be doable as well but I'm not sure if that's the case (even in Euclidean space topology can be tricky). Any pointers on how to approach this problem would be appreciated. Edit : @Apass.Jack has graciously taken the time to shown that in general this is not possible, but is true if $X$ is convex. To see if perhaps I can use that result, I will give an example of an $X$ I had in mind (sorry if this should be a different question, I can create one if necessary). $X$ is the image, by a ""projectivization"" map $\pi$ , of a properly embedded two dimensional surface $L \subset P^3$ (here $P$ is the strictly positive reals so $P^3$ is the strictly positive quadrant of $R^3$ ). Specifically, if $p \in L$ then $\pi(p) = (p_1,p_2,p_3)/(p_1+p_2+p_3) \in S^2$ , where $S^2$ is the two-dimensional simplex. I can show this map, restricted to $L$ , is of full rank everywhere, hence is open. I don't know that $X$ is convex, however, which is the problem (seems like it might be, though).",['general-topology']
4526772,Find the range of $f(x)=\frac1{1-2\sin x}$,"Question: Find the range for $f(x)= 1/(1-2\sin x)$ Answer : $ 1-2\sin x \ne 0 $ $ \sin x \ne 1/2 $ My approach: For range : $ -1 ≤ \sin x ≤ 1 $ $ -1 ≤ \sin x < 1/2$ and $1/2<\sin x≤1 $ , because $\sin x≠1/2$ $ -2 ≤ 2\sin x <1 $ and $ 1< 2\sin x≤2 $ $ -1 < -2\sin x ≤2 $ and $ -2≤ -2\sin x<-1 $ $ 0 < 1-2\sin x ≤3   $ and $ -1≤ 1-2\sin x<0 $ I am stuck at the last step. If I take reciprocal i.e., $\frac 1{1-2\sin x}$ I get : $ \frac10 < \frac1{1-2\sin x} ≤\frac13   $ and $ -1≤ \frac1{1-2\sin x} < \frac 10 $ $1/0$ can be interpreted as infinity so the second equation gives range of $f(x)$ as $[-1,∞)$ . But the correct answer is : $(−∞,−1]∪[\frac13,∞)$ Any alternative solutions are welcome :) P.S. : Also $\frac1{1-2\cos x}$ will also have the same range, right? As $\sin x$ and $\cos x$ both lie between $[-1,1]$ . So the answer will proceed in similar fashion to the given one.",['functions']
4526784,"From a point $P=(3,4)$, perpendiculars PQ and PR are drawn to line $3x+4y-7=0$ and a variable line $y-1=m(x-7)$ respectively","From a point $P=(3,4)$ , perpendiculars PQ and PR are drawn to line $3x+4y-7=0$ and a variable line $y-1=m(x-7)$ respectively, find the maximum area of $\triangle PQR$ . On EduRev website , they have taken the point R to be $(7,1)$ . Why? Can't the foot of the perpendicular be some other point? On toppr website , they are taking general PR in terms of m. And then they say for maximum area, $PR=5$ . For this, they are comparing the slope of PQ. That's more confusing. Can anyone explain this? Thanks.","['analytic-geometry', 'geometry', 'plane-geometry']"
4526790,Why is $\frac{\partial^2 f}{\partial x\partial x}$ equal to $\frac{\partial^2 f}{\partial x^2}$ and not $\frac{\partial^2 f}{(\partial x)^2}$?,"I know that this is a borderline pedantic question, but is there any other reason than a convention why usual calculus and differential equation texts say that $\frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{\partial x^2}$ and not $\frac{\partial^2 f}{\partial x\partial x} = \frac{\partial^2 f}{(\partial x)^2}$ ? Or are there some meaningful set of rules with which you can manipulate the differential forms $\partial x$ so that the $\frac{\partial}{\partial x}\frac{\partial f}{\partial x} = \frac{\partial^2 f}{\partial x^2}$ doesn't feel that arbitrary (although I suppose that this is a question of preference)?","['notation', 'calculus', 'partial-derivative', 'math-history', 'derivatives']"
4526791,Compute the MLE of variance for $f(x) = 3x^3 /\theta^3$,"We are given $X_1, X_2,\dots, X_n$ that are independent and with the same disturbution function $f(x) = 3x^2 /\theta^3$ for $ 0 \leq x \leq \theta$ , where $\theta$ is an unknown positive variable.
Using the maximum likelyhood method find : An MLE for $\theta$ An MLE for the variance of Xi's My answer to 1 is the well -known method : $L(\theta) = \prod_{i=1}^nf(X_i:\theta) \implies $ $$
L(\theta) = (\frac{3}{\theta^3})^n\prod_{i=1}^{n}x_i^3
$$ Now this has a well-known answer : $\hat \theta = \max_{1\leq i\leq n}x_i$ My problem is question 2 : I haven't got the concept of MLE quite well . I computed $\sigma^2 = Var(Xi) = \frac{3\theta^2}{5}$ . When we are trying to find an MLE for $Var(X_i)$ is it like we are trying to find an MLE for $\theta^2$ ? And if we already have an MLE for $\theta$ can't we just say that the MLE for $\theta^2$ is $\hat \theta^2$ ? So I would answer something like : $\hat Var(X_i) =\frac{3}{5}\hat \theta^2 $ but I am not sure at all . Even if question 1) wasn't present , the only way I know to use this method is to -blindly - take the definition of maximum likelyhood function (which hopefully will be a function  of the parameter I want to estimate) $L(\theta) = \prod_{i=1}^nf(X_i:\theta) $ . So I thought I could maybe write $f(x) $ as a function of $3/5 \theta^2$ : $$f(x:\frac{3}{5}\theta^2) = \sqrt{\frac{3}{5}}\frac{3x^3}{\sqrt{\frac{3}{5}\theta^2}^3}$$ Then I would have : $$L(\frac{3}{5}\theta^2) = (\sqrt{\frac{3}{5}}\frac{3}{\sqrt{\frac{3}{5}\theta^2}^3})^n\prod_{i=1}^{n}x_i^3 \implies $$ $$L(\sigma^2) = \Big(\sqrt{\frac{3}{5} }\frac{3}{\sigma^2}\Big)^n\prod_{i=1}^{n}x_i^3  $$ So then the answer would be just $\hat {\sigma^2} = \max\limits_{1 \leq i \leq n}{x_i}$ ?","['statistics', 'parameter-estimation', 'probability-theory', 'maximum-likelihood']"
4526800,Definition of the omega-limit set,"The $\omega$ -limit set of the set $B$ is defined as $\omega(B) = \bigcap_{n \geq 0} \overline{\bigcup_{k \geq n} T^k(B) }$ , where $T$ is some continuous mapping. What is the intuition behind this definition? I found this image in the Wiggins's book ""Introduction to Applied Nonlinear Dynamical Systems and Chaos"" and it shows that the $\omega$ -limit set of point $x$ is a point $x_0$ on the curve $\gamma$ ): To me, it is clear  that $y \in \omega(B) $ iff there exists sequence $x_j \in B$ and sequence of integers $n_j \rightarrow \infty$ such that $T^{n_j} x_j  \rightarrow y  $ as $j \rightarrow \infty$ . But I don't completely understand the definition from beginning, why do we need union and then intersection? Thanks a lot in advance.","['limits', 'calculus', 'dynamical-systems']"
4526814,Does Cantor's set contain a copy of each finite set?,"Let $C$ denote the (usual) Cantor's set in the interval $[0,1]$ . If $S=\{x_1,x_2,\cdots,x_n\}$ is a finite set of points in $\mathbb R$ , is it true that $aS+b\subset C$ for some $a\neq 0$ and $b\in\mathbb R$ ? (i.e. $C$ contains a copy of $S$ ) For the fat Cantor-type sets the answer is yes (from Steinhaus Theorem), since in this case the fat Cantor-type set has positive Lebesgue measure. Also it is known that there exists Lebesgue measure zero sets that contain a copy of each finite set.","['measure-theory', 'cantor-set', 'real-analysis']"
4526941,Solution Verification: Is the set of all finite subsets of $\mathbb N$ countable or uncountable?,"I'm pretty sure this is right, just need a quick look over. It's countable. The set $S$ of all finite subsets of $\mathbb N$ is $\bigcup_{k \in \mathbb N} U_k$ where $U_k$ is the set of subsets of size $k$ . If we prove $U_k$ is countable, we then know that $S$ is countable because the countable union of countable sets is itself countable. To show that $U_k$ is countable, note that we can view $U_k$ as ordered k-tuples $(u_{1},\cdots,u_{k})$ such that $u_i < u_{i+1}$ for $1 \leq i \leq k$ because every element in the subset must be distinct and thus must have a strict order. Also note that $\mathbb N^k = \underbrace{\mathbb N \times \cdots \times \mathbb N}_{\text{k copies}}$ is countable because the Cartesian product of a finite number of countable sets is countable. Since $U_k$ is a subset of $N^k$ , we know that $U_k$ is countable because the subset of a countable set is itself countable. Since $U_k$ is countable, we have showed that $S$ is countable.","['solution-verification', 'discrete-mathematics']"
4526946,Finite simple groups of order $p+1$,"Are there any well known patterns about which finite simple groups have order $ p+1 $ for $ p $ a prime? Here is a list of all non-cyclic simple groups of order up to 100,000 and whether they have order p+1 (there are 31 such groups, 16 have order $ p+1 $ ) $ PSL_2(5) $ , $p=59$ $ PSL_2(7) $ , $p=167$ $ PSL_2(9) $ , $p=359$ $ PSL_2(8) $ , $p=503$ $ PSL_2(11) $ , $p=659$ $ PSL_2(13) $ , $p=1091$ $ PSL_2(17) $ , $p=2447$ $ A_7 $ , $2519$ not prime $ PSL_2(19) $ , $3419$ not prime $ PSL_2(16) $ , $p=4079$ $ PSL_3(3) $ , $5615$ not prime $ PSU_3(3) $ , $p=6047$ $ PSL_2(23) $ , $6071$ not prime $ PSL_2(25) $ , $7799$ not prime $ M_{11} $ , $p=7919$ $ PSL_2(27) $ , $9827$ not prime $ PSL_2(29) $ , $12,179$ not prime $ PSL_2(31) $ , $p=14,879$ $ PSL_4(2) $ , $20,159$ not prime $ PSL_3(4) $ , $20,159$ not prime $ PSL_2(37) $ , $p=25,307$ $ PSU_4(2) $ , $p=25,919$ $ Suz(8) $ , $29,119$ not prime $ PSL_2(32) $ , $32,735$ not prime $ PSL_2(41) $ , $p=34,439$ $ PSL_2(43) $ , $p=39,731$ $ PSL_2(47) $ , $51,887$ not prime $ PSL_2(49) $ , $58,799$ not prime $ PSU_3(4) $ , $62,399$ not prime $ PSL_2(53) $ , $p=74,411$ $ M_{12} $ , $95,039$ not prime","['group-theory', 'simple-groups', 'finite-groups']"
4526950,"Showing that $\cos(u_1,u_3)=\cos(u_1,u_2)\cos(u_2,u_3)$ for nearby $u_i$","Edit if we replace rotations with ""add isotropic noise"", this relation can be proven using Chebychev inequality as shown here . The $\pi/4$ angle seems to be connected to forgetting of starting point. In high dimensions, random rotations seem to keep iterates $u_1,u_2,...,$ roughly along the same line (hence triangle inequality for cosines becomes equality), until $\pi/2$ angle is reached at which point the process becomes ergodic. Suppose I start with vector $u_1$ in $d$ dimensions and obtain $u_{i+1}$ by performing a sequence of $i$ small rotations in $d$ dimensions. For $d=100$ , the following gives a good approximation, within 0.1% of true value in expectation. $$\cos(u_1,u_4)=\cos(u_1,u_2)\cos(u_2,u_3)\cos(u_3,u_4)$$ where $$\cos(x,y)=\frac{\langle x, y\rangle}{\|x\| \|y\|}$$ ""Small rotation"" of $v$ is done by sampling entries $z$ from standard normal, and rotating $v$ in the plane defined by vectors $v,z$ by $\theta$ radians. This identity works for $\theta_i\le\pi/4$ and breaks down for $\theta$ slightly above $\pi/4$ . How can this be justified? Why is $\pi/4$ special?","['concentration-of-measure', 'probability-theory']"
4526975,Is a smooth function with bounded partial derivatives defined on an open set in Euclidean space bounded?,"This question is a follow up to this one , which was answered essentially in the negative. So I am trying to find another way. I have a connected bounded open set $X \subset R^n$ ; then I know it is also path-connected. I also have a smooth (say at least $C^2$ ) function $u$ on $X$ to the positive reals $P$ . I also know that the absolute value of the partials $\partial u/\partial x_i,\; 1 \leq i \leq n$ are uniformly bounded on $X$ . Finally, I have a limit point $z$ of $X$ and a sequence $\{z_n\} \subset X$ approaching $z$ . I would like to show that $\{u(z_n)\}$ is bounded (so I can extend it). I don't know that $X$ is convex, so I can't use that. Instead, my idea is to integrate $u$ along paths $l(z_1,z_k) \subset X, \; k = 2,3,\ldots$ , where $l(z_1,z_k)$ is of course the path in $X$ connecting $z_1$ and $z_k$ . I would then show that this sequence of integrals is bounded by using the boundedness of the partials of $u$ . I could then conclude that $u(z_n)$ itself is bounded. There's only one problem : While I believe, because $u$ is $C^2$ , that $l(z_1,z_k)$ has finite length, I don't know that $\lim_{k \rightarrow \infty} l(z_1,z_k)< \infty$ (so I can't use the typical argument that the value of the integral is bounded by the bound on the partials times the length of the domain of integration). Is there any way to show that in this particular situation the limit of the length of the paths is finite? Perhaps by showing there is a neighborhood $U(z)$ of $z$ such that $u$ is bounded on the ""last piece"" $U(z) \bigcap X$ ? I am wary of drawing any conclusions in this situation because I know how crazy paths (even in $R^n$ ) can be. Any ideas appreciated. Really. Edit : Thanks to Paul Frost for suggesting edits (above in bold) to make things clearer. The partials being bounded uniformly means that there exist positive constants $K_i,\; 1 \leq i \leq n$ such that $\lvert \partial u(x)/\partial x_i(x) \rvert < K_i,\; \forall x \in X, 1 \leq i \leq n$ . Edit : Martin R seems to have provided a clever counter-example showing the answer to my question is no. I am still hopeful of getting the result I need as $X$ is actually the image, by an open map (a projection), of an $n$ -dimensional integral manifold $L$ . Given bounds on the partial derivatives of $L$ (and perhaps an additional restriction on the second derivative) I can use estimates on the minimum size of the image, in $X$ , of any open ""ball"" in $L$ to show that, for every $x \in X$ , if $U_x$ is a neighborhood of $x$ completely contained in $X$ then it must contain a ball of minimum radius (I have questions about these estimates, I will create another post to pose them). This obviously would get around counter examples such as the one provided by Martin R. My thanks to all who posted and tried to help.","['integration', 'general-topology']"
4527012,"If $f'$ is integrable on $[a,b]$, what can we say about $f$?","Suppose $f:[a,b]\to\mathbb R$ is a differentiable function with $f'\in L^1 [a,b]$ , that is, $f$ has a derivative that is integrable on $[a,b]$ . What are some properties $f$ must have? If any are known, some defining properties would be nice, that is, properties $P$ such that $f$ has $P$ if and only if $f'$ is integrable on $[a,b]$ . One thing we can say is that $f$ must be Lipschitz-continuous. This shouldn't be the best we can do: Lipschitz-continuity follows from the boundedness of $f'$ , but we know from Darboux's theorem that $f'$ also has the intermediate value property, a very restrictive property. I'm not sure how we could leverage this to get some insight into the nature of $f$ . Any ideas?","['integration', 'calculus', 'derivatives', 'real-analysis']"
4527016,What is the probability that the committee will be made up of equal numbers of men and women?,"I have the following exercise of multiple option: You have four men and five women to make a committee of four people. If the committee is constituted by taking random people, what is the probability that the committee will be made up of equal numbers of men and women? a) $\frac{1}{5}$ b) $\frac{1}{2}$ c) $\frac{4}{9}$ d) $\frac{20}{21}$ My solution:
There's $\binom{9}{4}$ ways to select committee, i.e., there's 126 ways to select committee.  Now, selecting equal number of man and women in group of 4 implies 2 men and 2 women so selecting 2 men of 4 is $\binom{4}{2}=6$ selecting 2 women of 5 is $\binom{5}{2}=10$ so $P(A)= \frac{6 \cdot 10}{126}=\frac{60}{126} = \frac{10}{21}$ but this is not an option ... I don't understand if I am making a mistake. Thanks for your help.","['combinatorics', 'probability']"
4527035,Triangle ABC with $\angle{ACB} = 3\angle{ABC }$ and $AB = \frac{10}{3}BC$. Find $\cos{A}\cos{B}\cos{C}$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question assume that $\angle{ABC}=y, \angle{ACB}=3y, BC=3x,$ and $AB=10x$ then by sine rule, i obtain following $ \frac{10x}{\sin{3y}}=\frac{AC}{\sin{y}}=\frac{3x}{\sin{4y}}$ by cosine rule in try to figure out AC $AC=\sqrt{109x^2-60x^2\cos{y}}$ i have no idea how to combine these two informations in order to solve the problem",['trigonometry']
4527080,"Spivak, Ch. 20, Problem 9c: Understanding comment at end of problem item.","(a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of
all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial
in $x-a$ ]. Again, give a more direct proof, using the obvious facts
about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ Also note that if $p$ is a polynomial in $x-a$ having only terms of
degree $>n$ , and $q$ is a polynomial in $x-a$ whose constant term is $> 0$ , then all terms of $p(q(x-a))$ are of degree $>n$ . My question regards the comment at the end of item $(c)$ . Let $p$ and $q$ be polynomials in $x-a$ such that terms in $p$ have degree $>n$ , and the constant term in $q$ is $0$ . $$p(x)=\sum\limits_{i=n}^{m_1} a_i (x-a)^i$$ $$q(x)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i$$ What is $q(x-a)$ ? Is it $$q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-2a)^i$$ Or is it just $$q(x-a)=\sum\limits_{i=1}^{m_2} b_i (x-a)^i$$ In the first case we have $$p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-2a)^j -a \right )^i$$ In the second we have $$p(q(x-a))=\sum\limits_{i=n}^{m_1} a_i \left ( \sum\limits_{j=1}^{m_2} b_i (x-a)^j -a \right )^i$$ Is this sort of what the comment is saying? If so, is the next step to somehow conclude that all $(x-a)$ factors have a degree $>n$ in this second expression?","['integration', 'calculus', 'solution-verification', 'taylor-expansion', 'derivatives']"
4527113,Evaluating a binomial sum involving $1/m^m$,"I was wondering whether there is a closed form (or asymptotic expression) for the following binomial sum: $$\sum_{m=0}^n \frac{1}{m^m} \binom{n}{m},$$ where we use the convention $0^0=1$ . I feel like during my university studies we were taught a trick to solve problems like this by playing around with the binomial theorem, exponentials and sometimes calculus. However, nothing I've tried so far seems to work. At the very least, a non-trivial upper bound would be handy. Edit: Thanks all for the answers, they're very helpful and it's so fun to see all the different approaches","['real-analysis', 'analytic-number-theory', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics']"
4527137,Definition of limits in metric spaces Tao's Analysis II,"In the third edition of Tao's Analysis II he gives the following definition of limiting value of a function: Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, let $E$ be a subset of $X$ , and let $f:X\rightarrow Y$ be a function. If $x_0\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_0
$ in $E$ , or write $\lim_{x\to x_0 ; x\in E} f(x) =L$ , if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),L)< \epsilon$ for all $x\in E$ such that $d_X(x,x_0) < \delta$ But in the corrected third edition he changes the definition slightly: Let $(X,d_X)$ and $(Y,d_Y)$ be metric spaces, let $E$ be a subset of $X$ , and let $f:E\rightarrow Y$ be a function. If $x_0\in X$ is an adherent point of $E$ , and $L\in Y$ , we say that $f(x)$ converges to $L$ in $Y$ as $x$ converges to $x_0$ in $E$ , or write $\lim_{x\to x_0 ; x\in E} f(x) =L$ , if for every $\epsilon > 0$ there exists a $\delta > 0$ such that $d_Y(f(x),L)< \epsilon$ for all $x\in E$ such that $d_X(x,x_0) < \delta$ Basically he changes the domain of $f$ , my question is: what is the motivation in this change? The first definition looks more useful to me, since the idea of taking a subset $E$ of $X$ and the notation $\lim_{x\to x_0 ; x\in E} f(x) =L$ is to be able of taking limits in subsets of $X$ , also we could perform this idea with the second definition, if we have a function $f:X\rightarrow Y$ and a subset $E$ of $X$ is enought to take the restriction $f|_E$ , but we can perform the same idea with the first definition with no need of these ""extra steps"".","['definition', 'metric-spaces', 'real-analysis']"
4527151,Distributions of the form $u_f$ are dense in $\mathcal{D}'(\mathbb{R}^n)$,"Let $f \in L^1_{loc}$ , and define $u_f \in \mathcal{D}'(\mathbb{R}^n)$ to be any distribution of the form $$\langle u_f, \phi\rangle = \int f\phi.$$ I would like to show that the collection of all such $u_f$ is dense in the space of distributions $\mathcal{D}'(\mathbb{R}^n)$ . The motivation for this question came form this Wiki on distributions , which provides a proof of the formula for the differential operator acting on distributions. What bothers me is that the proof relies on representing the distribution as an integral, but clearly not every distribution can be represented this way (take the Dirac- $\Delta$ function for example). The article hints that collection of $u_f$ mentioned above is dense in the space of distributions, so it suffices to consider this case and from there I assume one can make a limiting argument. How would one go about proving this?","['operator-theory', 'derivatives', 'functional-analysis', 'distribution-theory']"
4527163,"Spivak, Ch. 20, Problem 9d: Understanding Solution Manual Proof","The following problem is from Chapter 20 of Spivak's Calculus , ""Approximation by Polynomial Functions"". My question is about item $(d)$ , and I have previously asked a question about the comment at the end of item $(c)$ . (a) Problem $7(i)$ amounts to the equation $$P_{n,a,f+g}=P_{n,a,f}+P_{n,a,g}$$ Give a more direct proof by writing $$f(x)=P_{n,a,f}(x)+R_{n,a,f}(x)\tag{1}$$ $$g(x)=P_{n,a,g}(x)+R_{n,a,g}(x)\tag{2}$$ and using the obvious fact about $R_{n,a,f}+R_{n,a,g}$ . (b) Similarly, Problem $7(ii)$ could be used to show that $$P_{n,a,fg}=[P_{n,a,f}\cdot P_{n,a,g}]_n$$ where $[P]_n$ denotes the truncation of $P$ to degree $n$ , the sum of
all terms of $P$ of degree $\leq n$ [with $P$ written as a polynomial
in $x-a$ ]. Again, give a more direct proof, using the obvious facts
about products involving terms of the form $R_n$ . (c) Prove that if $p$ and $q$ are polynomials in $x-a$ and $\lim\limits_{x\to 0} \frac{R(x)}{(x-a)^n}=0$ then $$p(q(x)+R(x))=p(q(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to 0} \frac{\bar{R}(x)}{(x-a)^n}=0$$ Also note that if $p$ is a polynomial in $x-a$ having only terms of
degree $>n$ , and $q$ is a polynomial in $x-a$ whose constant term is $0$ , then all terms of $p(q(x-a))$ are of degree $>n$ . (d) If $a=0$ and $b=g(a)=0$ , then $$P_{n,a,f\circ g}=[P_{n,b,f}\circ P_{n,a,g}]_n$$ Here is what the solution manual says Writing $$f(x)=P_{n,0,f}(x)+R_{n,0,f}(x)$$ $$g(x)=P_{n,0,g}(x)+R_{n,0,g}(x)$$ we have $$(f\circ g)(x)=P_{n,0,f}(P_{n,0,g}(x)+R_{n,0,g}(x))+R_{n,0,f}(g(x))$$ $$=A+B$$ Part $(c)$ shows that $$A=P_{n,0,f}(P_{n,0,g}(x))+\bar{R}(x)$$ where $$\lim\limits_{x\to a} \frac{\bar{R}(x)}{(x-a)^n}=0\tag{3}$$ and the
remark added at the end of $(c)$ shows that $$\lim\limits_{x\to a}
 \frac{B}{(x-a)^n}=0\tag{4}$$ Then, applying $(c)$ once again, we have $$(f\circ g)(x)=(P_{n,0,f}\circ P_{n,0,g})(x)+\bar{\bar{R}}(x)\tag{5}$$ where $\lim\limits_{x\to a} \frac{\bar{\bar{R}}(x)}{(x-a)^n}=0$ . It
follows, just as in part $(b)$ , that $P_{n,0,f\circ g}=[P_{n,0,f}\circ
 P_{n,0,g}]_n$ Everything up to $(3)$ is fine. How do we obtain $(4)$ ? My attempt at understanding it is: Since $R_{n,0,f}$ is polynomial in $x-a=x$ , composed of a single term of degree $n+1$ , and $g(x-a)=g(x)$ is such that the constant term in its Taylor polynomial is zero ( $g(a)=0$ by assumption), then as per the comment at the end of $(c)$ all of the terms in $R_{n,0,f}(g(x))$ are of degree $>n$ . Hence $$\lim\limits_{x\to a} \frac{R_{n,0,f}(g(x))}{(x-a)^n}=0$$ My other question is: how exactly is part $(c)$ applied again to reach $(5)$ ? Ie, in the context of part $(c)$ , when are the polynomials $p$ and $q$ here in part $(d)$ ? EDIT: Clarifications required by the bounty The solution manual sets $a=b=g(a)=0$ . I'd like to see a proof of $(d)$ that does not require $a=0$ or $b=0$ , only $b=g(a)$ . I will write an answer specifying my current understanding of the problem and solution.","['integration', 'proof-explanation', 'calculus', 'taylor-expansion', 'derivatives']"
4527179,Strange Result in Contour Integration: Possible Mistake?,"(Context) I tried out this proof I made up for fun: Let $a \geq 0.$ Prove that $$\int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \pi\sin\left(\frac{1}{e^{a}}\right).$$ I put this integral and my answer in Desmos with a slider $a$ , and it seems like Desmos is numerically approximating both expressions such that they're both off by a bit, probably around $0.1$ or $0.01$ . (Also, I was treating $\infty$ as a huge number like $3000$ because Desmos sometimes has trouble dealing with it.) (Attempt) Let that integral equal $I$ . Rewriting it gives us $$\int_{-\infty}^{\infty}\frac{\exp\left(-\sin\left(ax\right)\right)\sin\left(\cos\left(ax\right)\right)}{x^{2}+1}dx = \Im \int_{-\infty}^{\infty}\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}dz.$$ Let $f(z) = \frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}$ and traverse counterclockwise along this curve $C:= \left[-R,R\right] \cup \Gamma$ , where $C$ is a semicircle on and above the real axis in the complex plane and $R$ is a large radius approaching $\infty$ . The set of singularities is $\left\{i,-i\right\}$ (after setting the denominator equal to $0$ ). Notice that $i$ lives in $C$ , so we can make it our pole. By Cauchy's Residue Theorem, we can rewrite $\oint_C f(z)dz$ as $$2\pi i\text{Res}(f(z), z=i) = \int_{\Gamma}f(z)dz + \int_{-R}^R f(z)dz.$$ Solving the residue, we get $$\text{Res}\left(\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}, z=i\right) = \frac{\exp{(ie^{ia(i)})}}{\frac{d}{dz}\left(z^2+1\right)\Big|_{z=i}} = \frac{\exp\left(ie^{-a}\right)}{2i}.$$ As for the contour integral, it converges to $0$ as $R \to \infty$ . Notice if $z$ is on $\Gamma$ , then $|z| = R$ . Observe from using one of the Triangle Inequalities that $$\left|\frac{\exp\left(ie^{iaz}\right)}{z^{2}+1}\right|\le\frac{\left|\exp\left(ie^{iaz}\right)\right|}{\left|\left|z\right|^{2}-1\right|}=\frac{1}{R^{2}-1}.$$ Using the ML-Inequality, observe that $$0 \leq \left|\int_{\Gamma}f(z)dz\right| \leq |f(z)|\left(\frac{2\pi R}{2}\right) \leq \frac{\pi R}{R^2-1}.$$ Taking $R\to\infty$ , we can use the Squeeze Theorem to get $$\lim_{R\to\infty}\left|\int_{\Gamma}f(z)dz\right| = 0,$$ which implies $$\lim_{R\to\infty}\int_{\Gamma}f(z)dz = 0.$$ Going back to $\oint_{C} f(z)dz$ , we take $R\to\infty$ and $\Im$ on both sides to get $$
\eqalign{
\Im\lim_{R\to\infty} 2\pi i\text{Res}(f(z), z=i) &= \Im\lim_{R\to\infty} \int_{\Gamma}f(z)dz + \Im\lim_{R\to\infty} \int_{-R}^R f(z)dz   \cr
\Im\lim_{R\to\infty}\int_{-R}^{R}f(z)dz &= I \cr
&= \Im\left(\pi \exp{\left(ie^{-a}\right)}\right) \cr
&= \pi\sin{\left(\frac{1}{e^a}\right)}. \cr
}
$$ (Question) Is there something wrong with my proof? I've checked this over and over and I want to say there's nothing wrong and that I didn't miss anything trivial. Still, the approximations Desmos is giving me are sort of bothering me, even though I know Desmos isn't always reliable when evaluating things at $\infty$ . And also, the proof doesn't seem to hold true if $a$ is negative. If my proof is correct, then how come it breaks when $a$ is negative? If someone can shed some light, that would be great. Thank you in advance.","['integration', 'improper-integrals', 'complex-analysis', 'calculus', 'contour-integration']"
4527249,Rudin Exercise 18 Chapter 8 - Elegant solution?,"Exercise : Define $$f(x) = x^3 - \sin^2(x)\tan(x)$$ $$g(x) = 2x^2 - \sin^2(x) -x\tan(x)$$ Find out, for each of these two functions, whether it is positive or negative for all $x \in (0, \pi/2)$ , or whether it changes sign. Prove your answer. The problem The standard solution to this exercise requires to compute the derivatives of $f$ and $g$ as many times as six and five respectively, however I think that such an answer is neither illuminating nor typical for Rudin's exercises. While trying to solve it by myself I would have guessed the solution to rely on some clever inequality involving sine and tangent. In particular, I have noticed that the presence of $x^3$ and $2x^2$ is not random and this is something that the iterated derivatives do not highlight. Indeed, by the Mean Value Theorem, one can link the behavior of sine and tangent of $x$ to that of $x$ multiplied by some constant, so that, for instance, $\sin^2x\tan x$ behaves like a multiple of $x^3$ . I would like to know whether there is an actual elegant solution or if this exercise were mere computation. As always, any comment or answer is much appreciated and let me know if I can explain myself clearer!","['inequality', 'real-analysis', 'calculus', 'trigonometry', 'derivatives']"
4527253,Sufficient condition for existence of vectors,"Suppose that $A\in\mathbb{R}^{m\times m}$ , $B\in\mathbb{R}^{m\times n}$ , $C\in\mathbb{R}^{m\times n}$ and $D\in\mathbb{R}^{m\times m}$ . I'm looking for a sufficient condition on the matrices $A,B,C$ and $D$ such that there exists vectors $x\in\mathbb{R}^{n}$ , $y\in\mathbb{R}^{m}$ and $z\in\mathbb{R}^{m}$ such that \begin{align}
 A y &= B x, \\
 A z &= C x + Dy, 
\end{align} and \begin{align}
 y_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\
 z_i &\neq 0 \text{ for each } i = 1,\ldots, m, \\
 y_i - z_i &\neq 0 \text{ for each } i = 1,\ldots, m.
\end{align} Sufficient condition: If $A$ is invertible and if no row of $$M:=\begin{pmatrix}  A^{-1}B\\  C+DA^{-1}B\\  A^{-1}B - C-DA^{-1}B \end{pmatrix}$$ is the null vector. I'm looking for a condition that doesn't assume invertibility. Context: $A,B,C$ and $D$ describe parts of some dynamical system. Exists of such vectors $x,y$ and $z$ guarantees properties of the system.","['matrices', 'linear-algebra']"
4527266,Real Solutions of the expression $\sqrt[2]{x+3-4\sqrt[2]{x-1}}+\sqrt[2]{x+8-6\sqrt[2]{x-1}}-1=0$,"I recently stumbled across the following question: Find all the real solutions $x$ of the expression $\sqrt[2]{x+3-4\sqrt[2]{x-1}}+\sqrt[2]{x+8-6\sqrt[2]{x-1}}-1=0$ It was given by one of my teachers (he was teaching us a few problems involving real and rational numbers). Although I used a few typical algebraic manipulations (such as shifting terms from LHS to RHS and squaring both sides) all of them repeatedly led me towards wrong results. When I plugged the expression $\sqrt[2]{x+3-4\sqrt[2]{x-1}}+\sqrt[2]{x+8-6\sqrt[2]{x-1}}-1$ into Desmos , I came to see that all real numbers from 5 to 10 gave the value of zero when plugged into the expression. Could anyone show (through algebraic manipulations) why this is the case?","['algebra-precalculus', 'functions', 'radicals']"
4527301,How much of an $n$-dimensional manifold can we embed into $\mathbb{R}^n$?,"I observed some naive examples. Spheres, for example, when we cut out one point, can be embedded into $\mathbb{R}^n$. And if we cut out a measure zero set of a projective space, it can be embedded into the Euclidean space of the same dimension.
So I wonder if all manifolds can be embedded into a same dimensional Euclidean space when we cut out a measure zero set? Can anyone prove it or disprove it by giving me some counterexamples?","['manifolds', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
4527325,Almost sure equivalence of regular conditonal probability of a progressive process in a controlled SDE equation.,"Consider the (canonical path) space of continuous functions $\Omega = C([0,T],\mathbb{R})$ with the Borel-sigma algebra generated by the open sets induced by the uniform topology and $\mathbb{P}$ Wiener Measure on it (and canonical process $B_t(\omega) = \omega(t))$ . Working through the paper ""Julien Claisse, Denis Talay, Xiaolu Tan:  A pseudo-Markov property for controlled diffusion processes"" ( https://arxiv.org/abs/1501.03939 ) i am stuck understanding a identity of the regular conditional probability when having a diffusion SDE (to be more precise Page 6, Equation 10): Say we have a solution \begin{equation}
X_T^{t,x,u} = x_t + \int_{t}^{T} \mu(z,{X}_z^{t,x,u}, u_z) dz + \int_{t}^{T} \sigma(z,{X}_z^{t,x,u},u_z) dB_z 
\end{equation} Then it also holds for $ t \leq s    \leq T$ by writing out (and using continuity) \begin{equation}
X_T^{t,x,u} = X_s^{t,x,u} + \int_{s}^{T} \mu(z,{X}_z^{t,x,u}, u_z) dz + \int_{s}^{T} \sigma(z,{X}_z^{t,x,u},u_z) dB_z 
\end{equation} Assume there exists a regular conditional probability $(\mathbb{P}_\omega)_{w \in \Omega}$ for $\mathcal{F}_T:= \bigvee_{0 \leq t\leq T } \mathcal{F}_t$ given $\mathcal{F}_s$ for $ t \leq s \leq T$ . Then it holds for the stopped process $[{X}^{t,x,u}]_s := (X_{s \wedge z}^{t,x,v})_{0 \leq z \leq T} $ \begin{equation}
\mathbb{P}_{\omega} \left( [{X}^{t,x,u}]_s = [{X}^{t,x,u}]_s(\omega) \right) = 1 \qquad  for \quad \mathbb{P} \quad a.all \quad \omega \in \Omega
\end{equation} (see also Lemma 3.2 in the referenced paper). However why is it true that it also holds for the progressive process $u_t$ and the concatenation process $(u_z^{t,\omega})_{0 \leq z \leq T }$ defined by \begin{equation}
u_z^{t,\omega}( \bar{\omega}) := 
\{
\begin{array}{ll}
u_z ( \omega(y) ), \quad \quad \quad \quad \quad \quad \quad if \quad 0 \leq y \leq t  \\
u_z( \omega(t) + \bar{\omega}(y) - \bar{\omega} (t)) , \quad if \quad t \leq y \leq T\\
\end{array}
\end{equation} that \begin{equation}
\mathbb{P}_{\omega} ( u = u^{s,\omega}) = 1 \qquad  for \quad \mathbb{P} \quad a.all \quad \omega \in \Omega \qquad ? 
\end{equation} Especially i am wondering of how to particularly derive the form of the additional concatenation of the above process. Can anyone light this maybe up? Probably i am missing a property of a representation of processes measurable to the sigma algebra generated by the brownian motion in this setting.","['measure-theory', 'stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'probability-theory']"
4527343,Two players roll the dice until the first repetition. Who is more likely to win?,"To give more details about the title question, assume that we have a collection of $n$ distinct elements. On each step, numbering from zero, we choose one of them uniformly at random (with replacement). I am interested in the distribution of the random variable $X_n$ indicating the moment when any element is selected a second time, i.e. the first collision . Specifically, I am interested in whether $X_n$ is more likely to be even or odd. It is easy to see that $$p_k(n) := \mbox{P}(X_n=k) = \Big(1-\frac{1}{n}\Big)\Big(1-\frac{2}{n}\Big) \dots \Big(1-\frac{k-1}{n}\Big)\cdot\frac{k}{n} = \binom{n-1}{k-1} \cdot \frac{k!}{n^k}$$ for all $1\le k \le n$ . I do not know if this distribution has a common name, but it is closely related to the birthday problem . In particular, it is known that the expected value $\mbox{E}(X_n)$ is asymptotically equal to $\sqrt{\frac{\pi}{2}n}$ for large $n$ , see e.g. this Math.SE post , Wikipedia , or the paper ' Bounds on Birthday Attack Times ' by Michael J. Wiener. Let $P_0(n)$ and $P_1(n)$ be the probabilities that $X_n$ is even and odd, respectively. Namely, we have $$ P_0(n) = \sum_{\substack{k=1 \\ k \scriptsize\mbox{ is even}}}^{n} p_k(n), \ \  \ P_1(n) = \sum_{\substack{k=1 \\ k \scriptsize\mbox{ is odd}}}^{n} p_k(n).$$ For $1 \le n \le 6$ , the values of $P_1(n)$ are equal to $1, \frac{1}{2}, \frac{5}{9}, \frac{17}{32}, \frac{329}{625}, \frac{169}{324}$ , respectively. This (and some more numerical experiments that I better not to show explicitly) suggest the following problem. Question Is it true, that for all $n>2$ , we have $P_1(n)>P_0(n)$ , i.e that $X_n$ is more likely to be odd than even? I think that numerical data supports an even stronger statement that $P_1(n) = \frac{1}{2}+\frac{1+o(1)}{8n}$ as $n \to \infty$ , but I don't know how to estimate this sum with such an accuracy. Finally, what happens if there are more than two 'players', i.e. if we look at $X_n \bmod m$ for some fixed $m>2$ (for instance, modulo 3)? Which residue would have an advantage in this case for large values of $n$ ?","['summation', 'binomial-coefficients', 'combinatorics', 'discrete-mathematics', 'probability']"
4527395,Global sections of the sheaf associated to Weil divisor on $\mathbb{P}^{1}$.,"Throughout we will work over $\mathbb{C}$ . Let $P$ be the point $P:=[0:1]$ and $D$ the Weil divisor $D:=nP$ . What are the global sections of the sheaf $\mathcal{O}_{\mathbb{P^{1}}}(D)$ ? I know that $$
\mathcal{O}_{\mathbb{P^{1}}}(D)(U)=\left\{\frac{f(x,y)}{g(x,y)}\ f,g \ \text{are homogenous of degree}\ d :\text{ord}_{p}(f)-\text{ord}_{p}(g)\geq -n\ \text{if}\ p\in U\right\}.
$$ and answer should be homogenous degree $d$ polynomials, but I am unsure how to arrive at this.","['divisors-algebraic-geometry', 'algebraic-geometry', 'sheaf-theory']"
4527418,Automorphism of $\mathbb{Z}^2\rtimes\mathbb{Z}$,"Let $M$ be a matrix in $ \operatorname{GL}(2, \mathbb{Z})$ , are there any books/articles that give a description of $\operatorname{Aut}(\mathbb{Z}^2\rtimes_M\mathbb{Z})$ ? According to this paper , the automorphism group of any polycyclic group is isomorphic to a linear group.  I am trying to understand how the automorphism group is embedded into the matrix group. I was wondering if anybody has invested this particular example $\operatorname{Aut}(\mathbb{Z}^2\rtimes_M\mathbb{Z})$ and gave a description of this group. Thank you!","['automorphism-group', 'finitely-generated', 'semidirect-product', 'geometric-group-theory', 'group-theory']"
4527437,Are maximum-density fixed polyomino packings always isohedral?,"Consider, for a polyomino $P$ made from $n$ unit squares joined at the edges, the arrangements of non-overlapping translations of $P$ . Sometimes we can cover the infinite plane with such translations without any gaps: Other times, we can't, such as with the $T$ pentomino: When we can't do this, one can ask about the maximum density attainable, hereafter $\Delta(P)$ ; by a compactness argument, this will always be less than $1$ if $P$ does not tile the plane. Many times, we can get the maximum packing density by adding $k$ squares to $P$ to produce an $n+k$ -omino $Q$ which does tile the plane by translation, which gives us $\Delta(P) \ge \frac{n}{n+k}$ . For instance, with the $T$ pentomino above and $k=1$ : By a 1984 result of Wijshoff and van Leeuwen, any $Q$ which tiles the plane via translation can do so in an isohedral manner, meaning that the symmetries of the plane are transitive on the tiles. This implies the same for the arrangements of $P\subseteq Q$ as well. I am curious whether the above result generalizes to packing densities less than one, i.e. that maximal packings of $P$ are always possible to do by placing copies of $P$ in a lattice configuration. This is equivalent to the statement that the maximal density is equal to $\frac n{n+k}$ , where $k$ is the smallest number of unit squares needed to add to $P$ to form a translationally-tiling polyomino. I've verified manually that this is the case for all polyominoes with $6$ or fewer cells, but it's not clear to me how to proceed from here. Possibly the statement I'd like to show is equivalent to the $1984$ result, but if so I don't see the proof. It's also quite possible that there are straightforward counterexamples with some larger polyominoes - in particular, any non-tiling $n$ -omino that packs with density larger than $n/(n+1)$ would work. Any suggestions or pointers to relevant literature would be welcome!","['polyomino', 'geometry', 'tiling', 'packing-problem']"
4527449,How many subsequences do you need to look for to infer a word?,"Suppose that a word of length $L \in \mathbf{N}$ is defined to be an element of $A^{L}$ , where $A$ is a finite set of characters and that a subsequence of a word is an ordered list of characters which appear consecutively in the word. If I am going to choose a word of length $L$ and let you choose a set of words, $X$ , of length at most $L$ and I promise to tell you how many times each of your words appears as a subsequence of my word, what is the smallest set, $X$ , that you can choose and be guaranteed of being able to deduce my word? How does it scale with L and the size of A? Thoughts:
You could of course take $X$ to contain every word of length L. One thought was to try to construct words of length $L$ from words of length $L-1$ and words of length $L-1$ from words of length $L-2$ etc. so that you only need $X$ to contain small words but I haven't been able to make that work. Words like ABABABABABAB...ABAB seem quite hard to deduce. I think knowing how many times every word of length 2 or 3 appears might be enough but haven't proved or disproved this yet.","['combinatorics-on-words', 'combinatorics']"
4527466,Division structure of a torsor,"I was looking at torsors from nlab , and I got stuck at the trivialization part. I could prove the fact that the set underlying a $G$ -torsor $\bar T$ is isomorphic to the set underlying $G$ , the so called trivialization of $\bar T$ . But then remark 2.13 stumped me. Before I proceed, some context of the notations, definitions and results. A $G$ -torsor $\bar T$ is a set $T$ and a group action $a:G\times T\to T$ such that the map $a\times \pi_2:G\times T\to T\times T$ is an isomorphism (where $\pi_2:G\times T\to T$ is the natural projection on the second coordinate). Then a theorem says that given $T$ is nonempty, then the set underlying $G$ is isomorphic to $T$ , and such an isomorphism is called a trivialization. Let $\rho:G\to T$ be a trivialization. Now onto the problems. They define a 'division' among the elements of $T$ by the map $$d:=\pi_{1} \circ \left( \rho^{-1} \times id \right): T \times T \rightarrow G \times T \rightarrow G$$ which seems kind of fine, yet iffy to me for the following reason. It seems fine because we can assume that the trivialization $\rho$ comes from the choice of some element $t\in T$ , we can identity $\rho$ with $t$ itself, and call the map $t$ (sloppy language for sure, but helps me visualize what's going on, maybe). Then, $d(t_1,\ t_2)=\pi_1\circ (t^{-1}\times id)(t_1,\ t_2)=\pi_1(t^{-1}(t_1),t_2)=t^{-1}(t_1)$ which looks like division for sure, but we have forgotten the element $t_2$ altogether. Something fishy is going on, or I am being dumb. So question 1 : Am I thinking correctly? If yes, where does the element $t_2$ vanish? If not, then what is the correct interpretation of said map? Moving on, they define $$D:=\rho\circ d:T\times T\to T$$ and call this a 'division structure on $T$ '. Question 2 : What is a division structure on a set? Maybe the answer lies in the answer of question 1, but I am really lost. Finally, the map $D$ looks very odd. Suppose $(t_1,\ t_2)\in T\times T$ , then $$D(t_1,\ t_2)=\rho\circ d(t_1,\ t_2)=\rho(\rho^{-1}(t_1))=t_1$$ and thus whatever value I take in the second coordinate, it vanishes into thin air. Question 3 : How does $D$ make sense? Condensing all the questions into one question, what I really want to ask is : Am I misinterpreting the maps $d$ and $D$ , or am I missing something? Sorry for the long question, but I am really stuck and appreciate any help.","['group-actions', 'group-theory', 'abstract-algebra', 'category-theory']"
4527489,Characterization for quotients of the form $\mathbb{Z}[x]/p(x)$ to be isomorphic,"Question: If $\mathbb{Z}[x]/(p(x)) \cong \mathbb{Z}[x]/(q(x))$ as rings(assuming $1$ goes to $1$ ) then is $p(x)= q(\pm x + a)$ for some $a \in \mathbb{Z}$ ? Certainly the converse is true. For quadratics the above seems to hold for the examples I tried meaning when I take $p(x)$ and $q(x)$ which are not affine translates of each other then the quotients are not isomorphic.
Note that for quadratics the condition is same as having equal discriminants. Also for any isomorphism descending from a surjective map from $\mathbb{Z}[x] \to Z[x]$ answer is yes for such maps $x \to \pm x +a$ (else we won't have surjection). I came to this conjecture from first noticing that $\mathbb{Z}[x]/(p(x)) \cong \mathbb{Z}[x]/(q(x))$ implies that the degrees of $p(x)$ and $q(x)$ have to be the same by looking at the induced $\mathbb{Z}$ module isomorphism. Also if one is monic then so is the other (non monics give a non-finitely generated $\mathbb{Z}$ module). If the answer is no, then are there some other weaker sufficient conditions for the quotients to be isomorphic?","['ring-theory', 'abstract-algebra', 'commutative-algebra']"
4527490,Existence of the moment generating function for a discrete uniform distribution,"Problem: A random variable $X$ is said to have a discrete uniform distribution over $[1, N]$ , with probability mass
function as \begin{split}
    P(X = x) = \begin{cases}
    \frac{1}{N} &\text{for } x =1, 2, \dots, N\\
    0 & \text{otherwise}. 
    \end{cases}
\end{split} Check if mgf exists. If exist, then find the form of mgf. My Approach: Note that $X \sim U(1,N)$ . Thus $E[e^{tX}]$ becomes $$ E[e^{tX}] = \sum_{x} e^{tx} \cdot P(X=x) = \sum_{i=1}^N e^{ti} \cdot \frac{1}{N} = \frac{1}{N} \cdot \Bigg( {\sum_{i=0}^N e^{ti}} \Bigg) - \frac{1}{N} = {\frac{e^{t(N+1)}-e^t}{N(e^t-1)}} $$ Now let's recall the condition for the MGF to exist: The moment generating function (MGF) of a random variable $X$ is a function $M_X(t)$ defined as $$ M_X(t) = E[e^{tX}] $$ We say that MGF of $X$ exists, if there exists a positive constant $a$ such that $M_X(t)$ is finite for all $t\in[−a,a]$ . The doubt: The denominator of the mgf becomes $0$ at $t=0$ . But this shouldn't happen since I know the mgf of the discrete uniform distribution exists (and is exactly of the form which I found). Where did I go wrong? Any sort of help will be highly appreciated! :)","['moment-generating-functions', 'probability-theory']"
4527512,Dense open subset of a topological space with strictly smaller dimension,"An exercise in the first chapter of Hartshorne asks you to give an example of a topological space $X$ and a dense open subset $U$ in $X$ such that $\dim U < \dim X$ . I have an example, but it feels very arbitrary, and I'm wondering if there is a more insightful example. My example : Take $\mathbb{A}^1$ over an infinite field, and adjoin a single point $\infty$ (let $X := \mathbb{A}^1 \cup \{\infty\}$ ). Let the open sets be: $\emptyset, X, \mathbb{A}^1$ , and subsets of $\mathbb{A}^1$ with finite complement. Then $\mathbb{A}^1$ is dense and open, and the topology on $\mathbb{A}^1$ is the usual Zariski topology, so $\dim \mathbb{A}^1 = 1$ , but if $q$ is any point of $\mathbb{A}^1$ , then $\{q\} \subset \{q, \infty\} \subset X$ is a strictly increasing sequence of closed irreducible spaces, so $\dim X = 2$ . This example feels forced. Does anyone know a better example that appears naturally (in any area of math)?","['zariski-topology', 'general-topology', 'algebraic-geometry']"
4527586,Cameron-Martin space for $\ell^2$.,"Let $\{a_n\}$ be a sequence in $\mathcal B=\ell^2$ and let $\xi_n$ be i.i.d. standard normal random variables and consider the sequence $\mathbf{g}= (a_1 \xi_1, a_2 \xi_2,...)$ . By Hairer's notes, exercise 3.5, ( https://www.hairer.org/notes/SPDEs.pdf ) we have that the law of $\mathbf{g}$ is a Gaussian measure on $\mathcal B=\ell^2$ . I am interested in its Cameron-Martin space, which is defined as the closure of the space $$\mathring{\mathcal H}_\mu=\{\{h_n\}\in \mathcal B: \exists \{h_n^\ast\}\in \mathcal B, C_\mu(h^\ast,\ell)=\ell(h) \forall \ell \in \mathcal B\}.$$ I suspect that the space above is the set of all $\mathbf h=\{a_n^2 b_n\}$ for some square summable sequence $\{b_n\}$ . That is because for each $\mathbf h =\{a_n^2 b_n\}$ , we associate the sequence $\mathbf h^\ast=\{b_n\}$ and for arbitrary $\ell=\{y_n\}$ we have $$E[\langle \mathbf h^\ast, \mathbf g\rangle\langle \ell, \mathbf g\rangle]= \sum_{n=1}^\infty a_n^2b_n y_n=\langle \ell, \mathbf h\rangle.$$ Is this correct?","['probability-theory', 'functional-analysis', 'normal-distribution']"
4527608,Error in Falconer's proof of Hausdorff dimension of Cantor set,"In Falconer's Geometry of Fractal Sets, he establishes a lower bound of $s=\log(2)/\log(3)$ on the Hausdorff dimension of the Cantor set $E$ as follows: We show that if $\mathscr{I}$ is any collection of intervals covering $E$ , then $$
1 \leq \sum_{I \in \mathscr{I}}|I|^s \quad(1.21).
$$ By expanding each interval slightly and using the compactness of $E$ , it is enough to prove $(1.21)$ when $\mathscr{I}$ is a finite collection of closed intervals. By a further reduction we may take each $I \in \mathscr{I}$ to be the smallest interval that contains some pair of net intervals, $J$ and $J^{\prime}$ , that occur in the construction of $E$ . ( $J$ and $J^{\prime}$ need not be intervals of the same $E_j$ .) If $J$ and $J^{\prime}$ are the largest such intervals, then $I$ is made up of $J$ , followed by an interval $K$ in the complement of $E$ , followed by $J^{\prime}$ [emphasis mine]. From the construction of the $E_j$ we see that $$
|J|,\left|J^{\prime}\right| \leq|K| .
$$ Then $$
\begin{aligned}
|I|^s &=\left(|J|+|K|+\left|J^{\prime}\right|\right)^s \\
& \geq\left(\frac{3}{2}\left(|J|+\left|J^{\prime}\right|\right)\right)^s=2\left(\frac{1}{2}|J|^s+\frac{1}{2}\left|J^{\prime}\right|^s\right) \geq|J|^s+\left|J^{\prime}\right|^s,
\end{aligned}
$$ using the concavity of the function $t^s$ and the fact that $3^s=2$ . Thus replacing $I$ by the two subintervals $J$ and $J^{\prime}$ does not increase the sum in $(1.21)$ . We proceed in this way until, after a finite number of steps, we reach a covering of $E$ by equal intervals of length $3^{-j}$ , say. These must include all the intervals of $E_j$ , so as $(1.21)$ holds for this covering it holds for the original covering $\mathscr{I} .$ In this proof, $E$ is the Cantor set and $E_j$ are the intervals that occur in its construction (i.e., $E_1=[0,1]$ , $E_2=[0,1/3]\cup [2/3,1]$ , etc.). However, I do not think this proof is correct, as I believe the part in bold is untrue. It is not true that $I=J \cup K \cup J'$ . Here is a counterexample. Consider $E_5$ . Let $I$ be an interval covering the first 11 sub-intervals. Then there is no way to split $I$ up into $J \cup K \cup J'$ . A picture might be helpful here. Is there a way to fix the proof?","['measure-theory', 'hausdorff-measure', 'geometric-measure-theory', 'real-analysis', 'fractals']"
4527654,Abstract Index Notation Inconsistency (Technical - Answer in the Question),"I am reading this great work here and I am trying to make sense of a specific derivation around the middle of the page. In particular, it seems they are claiming that: $(\nabla_v w) (f) = (v^{\alpha} \nabla_{\alpha} w^{\beta})\nabla_{\beta}f$ where we work on a smooth manifold $M$ , $v, w$ are smooth vector fields (i.e. sections of the tangent bundle $\mathcal{T}(M)$ ), $f$ is a scalar function and $\nabla$ is the connection on $\mathcal{T}(M)$ . My understanding is that they are using abstract index notation. But when I consider, for simplicity, a local set of coordinates with a local basis $e_i := \frac{\partial}{\partial x^i}$ I get a seemingly different answer. Specifically, if we write $v = v^i e_i$ and $w = w^j e_j$ one gets: $(\nabla_v w) (f) = \left(\left(v^i \nabla_{e_i} w^k + v^iw^j \Gamma_{ij}^k \right)e_k\right)(f) =  \left(v^i \nabla_{e_i} w^k + v^iw^j \Gamma_{ij}^k \right)\nabla_{e_k} f :=(\nabla_v w)^k \nabla_{e_k} f$ where $(\nabla_v w)^k$ means the $k$ -th component of the vector $\nabla_v w$ in local coordinates. So my answer seems to be: $(\nabla_v w) (f) = \left(v^i \nabla_{e_i} w^k\right) \nabla_{e_k} f + \left(v^iw^j \Gamma_{ij}^k \right)\nabla_{e_k} f$ But if their derivation is correct and I am interpreting abstract notation properly, it seems like I should instead be getting: $ (\nabla_v w) (f) = \left(v^i \nabla_{e_i} w^k\right) \nabla_{e_k} f$ What am I missing? Edit: I wish I could accept multiple answers. Huge Credit to @peek-a-boo and @Jackozee Hakkiuz for adding incredible insight to the problem. If anyone is reading this in the future, I highly recommend going over both answers. Edit2: Also for those future readers, I highly suggest this high level debate here . One thing it illustrates clearly is that quantities like $\nabla_{\alpha} w^{\beta}$ are a priori ill-defined which can lead to understandable confusion. As a result, certain notational conventions are required, ones which are often author dependent. This makes it especially challenging for anyone who approaches the topic using different sources, since finding notational inconsistencies is almost inevitable. Edit3: For my own clarity I would also like to give a formal answer that bridges the gap between the two notations, so here it goes: $\underline{\textbf{(A posteriori) ANSWER:}}$ Let $\nabla_{\alpha} w^{\beta}:= (\nabla w)_{\alpha}^{\beta}$ be the placeholder (abstract) notation for the $(1,1)$ -tensor field $\nabla w$ (said differently, in local coordinates $(\nabla w)_{\alpha}^{\beta}$ corresponds to $(\nabla w)_{k}^{i} (e_i \otimes \epsilon^k)$ or $(\nabla w)_{k}^{i} e_i \epsilon^k$ for short). We will see that this notation is consistent with the numerical one (i.e. the one in local coordinates). The setup is the same as above but we also add a local co-basis $\epsilon^j := dx^j$ . We observe the following: $v = v^k e_k$ (as a $(1,0)$ -tensor field, aka a vector field) $\nabla w = (\nabla w)^i_j e_i \epsilon^j$ (as a $(1,1)$ -tensor field) $e_i (f) = \nabla_{e_i} f$ (by definition) $\nabla_v w = (\nabla w)(v)$ (by definition) One can then perform the following calculations: $$(\nabla_v w)(f) = ((\nabla w)(v))(f) = ((\nabla w)^i_j e_i \epsilon^j v^k e_k)(f) = (v^k(\nabla w)^i_k) e_i(f) = (v^k(\nabla w)^i_k) \nabla_{e_i}f$$ which is ""consistent"" with the abstract index notation of: $$(\nabla_v w)(f) = (v^{\alpha} \nabla_{\alpha} w^{\beta})\nabla_{\beta}f = (v^{\alpha} (\nabla w)_{\alpha}^{\beta}) \nabla_{\beta}f$$ $\underline{\textbf{The Subtleties:}}$ There are some subtleties here, I feel they need to be addressed: The first one is that in some sense we got ""lucky"" that the two expressions (abstract and numerical) seem to match each other one to one (symbol-wise). This is due to the fact that both results above, correspond to scalars so there are no implicit vectors/covectors ""leftover"" as implied from the abstract notation. To see this explicitly, observe the following correspondences between the two notations: $v^{\alpha} \leftrightarrow v^k e_k$ $(\nabla w)_{\alpha}^{\beta} \leftrightarrow (\nabla w)_{l}^{i}e_i \epsilon^l$ $\nabla_{\beta} f \leftrightarrow (\nabla_{e_m} f) \epsilon^m $ This means that when we put everything together we get: $$ (v^{\alpha} (\nabla w)_{\alpha}^{\beta}) \nabla_{\beta}f \leftrightarrow (v^k e_k (\nabla w)_{l}^{i}e_i \epsilon^l) ((\nabla_{e_m} f) \epsilon^m) = (v^k  (\nabla w)_{l}^{i} \nabla_{e_m} f) e_k e_i \epsilon^l \epsilon^m = (v^k  (\nabla w)_{k}^{i}) \nabla_{e_i} f $$ Thus, it is only because we have the perfect amount of contractions (i.e. the final result is a $(0,0)$ -tensor (aka. a scalar)) that the two notations ""match"" each other. In the general case, the two notations will not match one another as (in some sense) the abstract notation ""implies"" the basis vectors while the regular one does not . (For example, even though $v^{\alpha} \leftrightarrow v^k e_k$ , the two expressions are not in a ""perfect"" one to one notational correspondence, symbol-wise). Ambiguity can be eliminated . If one uses correspondences analogous to the ones seen above, ambiguities can be eliminated. This is best seen by performing calculations on different parentheses placements. For example, we know that in local coordinates: $$\nabla_v w = v^i \nabla_{e_i} (w^k e_k) = v^i (\nabla_{e_i} w)^k e_k $$ where $ \nabla_{e_i} w = \left(\frac{\partial w^k}{\partial x^i} + w^j \Gamma_{ij}^k \right) e_k$ so that $ (\nabla_{e_i} w)^k = \frac{\partial w^k}{\partial x^i} + w^j \Gamma_{ij}^k$ . But then, one can calculate $v^{\alpha}\nabla_{\alpha} w^{\beta}$ in three different ways: $$\boxed{v^{\alpha} (\nabla_{\alpha} w)^{\beta} \leftrightarrow v^i (\nabla_{e_i} w)^k e_k = \nabla_v w}$$ $$\boxed{ v^{\alpha} (\nabla w)_{\alpha}^{\beta} \leftrightarrow v^i (\nabla w)^k_i e_k = \nabla_v w }$$ $$\boxed{ v^{\alpha} (\nabla_{\alpha}w^{\beta}) \leftrightarrow v^i (\nabla_{e_i}( w^k e_k))= v^i(\nabla_{e_i} w)^k e_k = \nabla_v w }$$ This means that the notation $v^{\alpha}\nabla_{\alpha} w^{\beta}$ is (a posteriori) unambiguous . Some ambiguity can still arise (if one is not careful). For example, take the notation $\nabla_{\alpha} w^{\beta}$ with one covariant and one contravariant index. In local coordinates, does it correspond to $(\nabla_{e_i} (w^k e_k)) \epsilon^i$ or $(\nabla_{e_i} w^k) \epsilon^i e_k$ ? In other words, where should the ""implied"" basis vectors go? Unless we create some notion of priority of operation, we will have an unbridgeable ambiguity, since the two expressions above are different. However, loosely speaking, the operator $\nabla$ acts on the vector $w$ after it is made ""whole"" (i.e. after we make the correspondence $w^{\beta} \leftrightarrow w^k e_k$ ), so we need to put the implied basis vectors $e_k$ first: $$\nabla_{\alpha} w^{\beta} \leftrightarrow (\nabla_{e_i} (w^k e_k)) \epsilon^i = (\nabla_{e_i} w)^k e_k \epsilon^i$$ Similarly, the hessian of a smooth scalar function has a unique correspondence in local coordinates, given by: $$\color{red}{\nabla_{\alpha} \nabla_{\beta} f \leftrightarrow (\nabla_{e_i} [(\nabla_{e_j} f) \epsilon^j])\epsilon^i} = \dots = [(\partial_i \partial_j f)  - (\partial_k f)\Gamma_{ij}^k]\epsilon^j\epsilon^i  $$ From there, we can derive the expression of the Torsion (defined by $T(f) = (\nabla_{\alpha} \nabla_{\beta} - \nabla_{\beta} \nabla_{\alpha}) (f)$ ) when acting on the scalar : $$T(f) = [(\Gamma_{ij}^k - \Gamma_{ji}^k) - [e_i, e_j]^k](\partial_k f)\epsilon^i \epsilon^j $$ And also the expression for Torsion itself as a (1,2) vector field: $$\boxed{T = [(\Gamma_{ij}^k - \Gamma_{ji}^k) - [e_i, e_j]^k]\epsilon^i \epsilon^j e_k} $$","['differential-geometry', 'connections', 'tangent-bundle', 'mathematical-physics', 'index-notation']"
4527659,Limits for $\sqrt{x^2}-x$ as $x \to \infty$ or $x \to -\infty$,"I know what the final answer should be since $\sqrt{x^2}=|x|$ and so the function becomes $|x|-x=0$ for positive numbers and $-x-x=-2x$ for negative numbers. But I'm just trying to do the limit with multiplying and dividing by the conjugate $\frac{\sqrt{x^2}+x}{\sqrt{x^2}+x}$ and I can't seem to make it work out. I would get something that looks like $\frac{(\sqrt{x^2})^2 - (x)^2}{\sqrt{x^2} + x} $ and then I should simplify something as $x$ based on $x>0$ or $-x$ based on $x<0$ For the positive case, I get $\frac{x^2-x^2}{x+x} = 0$ For the negative case, I get $\frac{(-x)^2-x^2}{-x+x}$ and I not sure if I missed something painfully obvious but I'm not sure how I could simplify this to get $-2x$ The reason I focused on conjugates is because that seems to be how most 1st year questions of this format seem to operate but I wanted to make my own questions and see if conjugates would always work","['limits', 'calculus']"
4527701,"""Complete"" Labelling of Complete Bipartite Graphs","Consider the complete bipartite graph $K_{n,n}$ on $2n$ vertices (e.g. there are vertices $v_1,\dots,v_{2n}$ with an edge between $v_i$ and $v_j$ iff $i\not\equiv j \pmod{2}$ ). A complete labelling is a way of assigning to each edge in $K_{n,n}$ a distinct (unordered) pair of integers $\{p,q\}$ where $1 \leq p,q \leq 2n$ , such that for each vertex the labels of edges incident to it include each of the integers from $1$ to $2n$ . Loosely, each vertex can 'see' every number in its adjoining edges. Question: For which $n$ does a complete labelling exist? Observations: It's vacuously possible for $n=1$ and impossible for $n=2$ as quickly one can see we are required to repeat a label. Since the number of possible labels is ${2n\choose 2}= 2n^2-n$ and there are $n^2$ edges, it's possible there'll always be 'enough' labels to reliably obtain a complete labelling. There are some 'obvious' implicit rules or alternative interpretations: Each integer must appear exactly $n$ times across all labels. Grouping vertices by 'connected to an edge labelled with $k$ ' partitions the vertices into disjoint pairs. After some permutation of the numbers $1,\dots,2n$ , one vertex must have incident edges with labels $\{1,2\},\{3,4\},\dots,\{2n-1,2n\}$ . Investigations: A little pen-and-paper tinkering found me a solution for $n=3$ , and some (not terribly sophisticated) python code generated solutions for $n\leq 7$ using a Wave Function Collapse algorithm. It failed to find a solution for $n=8$ after $50000$ attempts - though I am unsure if this represents a lower likelihood of a 'guess' solution being correct given the greater computation required. A solution for $n=5$ is shown below. Conjecture: A complete labelling exists for all natural numbers $n\neq 2$ . Although, I neither a strong logical or evidential basis for this, so I await disproval excitedly! I attempted an inductive argument, though it seemed fruitless given that while $K_{n+1, n+1}$ contains copies of $K_{n,n}$ as subgraphs, their corresponding sub-labellings clearly cannot be complete. Perhaps with some suitable exchanges it is possible to find a labelling for $n+1$ given $n$ , but I attempted to find a pen-and-paper solutions for $n=4$ from a solution for $n=3$ to no avail. It might also help to consider how many complete labellings a given graph might be expected to have. One other approach I have considered but not investigated thoroughly is potentially establishing the notion of a 'dual' graph (not in the usual sense of planar graphs) whose vertices correspond to edges in $K_{n,n}$ in some way, then potentially results from the theory of vertex-colouring might provide an answer.","['coloring', 'graph-theory', 'combinatorics', 'discrete-mathematics', 'bipartite-graphs']"
4527750,Sigma compactness in Spivak's Differential Geometry Book,"I am reading the first volume of Spivak's Differential Geometry Series, A Comprehensive Introduction to Differential Geometry and I had a question about a topic discussed on page 4. What I am confused about is what he writes on $\sigma$ compactness.  I get what the definition means, as well as his proof related to $\sigma$ compactness.  My question is this: why is this relevant to the discussion of manifolds in the rest of the chapter? I think I have a rough understanding of what is going on, but I'd prefer to not make any brave assumptions. I appreciate any help I can get on this.  I'm trying to stretch myself mathematically, and I want to make sure I understand this.  Thanks.","['manifolds', 'differential-geometry']"
4527836,"Differentiability of the function $ f(x, y) = | e ^x-y | (e^x-1) $","Prove that the function $ f: \mathbb R^2 \to \mathbb R $ defined by
the formula $ f(x, y) = | e ^x-y | (e^x-1) $ is differentiable at $
 (a, b ) \in \mathbb R ^ 2 $ if and only if $ e ^ a \neq b $ or $ a = 0, b = 1 $ . I know that function is differentiable at $(x_0,y_0)$ if exist $Df(x_0,y_0)$ such that: $$\lim_{(h,k) \to (x_0,y_0)} \frac{f(x_0+h, y_0+k)-f(x_0,y_0)-h\cdot \frac{\partial f}{\partial x}(x_0,y_0)-k \cdot \frac{\partial f}{\partial y}(x_0,y_0)}{\sqrt{h^2+k^2}}=0$$ However, in my opinion, the formula of $f$ is too complicated to use this fact and I think that exist better solution.",['derivatives']
4527852,The number of ternary strings with even number of $0$,"Let $a_n$ the number of strings of length $n$ with the alphabet $\{0, 1, 2\}$ such that the number of $0$ in the string is even. It's not difficult, using generating function or recursions, to show that $$
a_n = \frac{3^n + 1}{2}.
$$ But if you look at the formula, there seems to be some sort of bijections between the strings of even and odd numbers of $0$ . In other words, they should be each of about half of all the ternary strings of length $n$ , which is $3^n$ . What is the bijection?",['combinatorics']
4527875,Tensor product of stable bundle and line bundle is stable,"Let $X$ be a smooth projective curve. Let $E$ be a stable vector bundle (i.e $\mu(F)< \mu(E)$ for all subbundles $F$ ). Let $L$ be any line bundle. $G=E \otimes L$ is stable I've computed the slope of $G$ to be $\mu(E)+degL$ . To proceed further I'd have to know what a subbundle of tensor product of vector bundles look like.
How can I settle this? [Here $\mu = deg/rank$ ]","['algebraic-curves', 'vector-bundles', 'algebraic-geometry']"
4527878,"Find all solutions to the equation $\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0$","Find all solutions to the equation $$\frac{dx}{dt}=\sqrt[3]{x}, x(0)=0$$ My solution for $x\neq 0$ : $$\frac{dx}{dt}=\sqrt[3]{x}$$ $$\frac{\frac{dx}{dt}}{\sqrt[3]{x}}dt=\int 1 dt$$ $$\frac{3}{2}x^{\frac 23} = t+C$$ $$x(0)=0 \Rightarrow C=0, \frac{3}{2}x^{\frac 23} = t$$ $$x^{\frac 23}=\frac 23 t$$ $$x=\pm\sqrt{\frac 8{27} t^3}$$ But I don't know what I can do with $x=0$ .",['ordinary-differential-equations']
4527887,Well-Ordering Principle for Non-negative Integers,"Let $\Bbb Z_0$ be the set of all nonnegative integers and $\emptyset \ne S \subseteq \Bbb Z_0$ . Then $S$ has a least element. Definition. A set $A \subseteq \Bbb Z_0$ is called inductive if $0 \in A$ and $x \in A \implies x+1\in A$ . Attempt: Let $A=\{n \in \Bbb Z_0 \mid n\le s, \forall s \in S\}$ . Suppose for the contrary that $S$ has no least element. Since $0$ is the least element in $\Bbb Z_0$ and $S \subseteq \Bbb Z_0$ , we have $0 \le s$ for any $s \in S$ . Hence, $0 \in A$ . Now, let $x \in A$ , then $x \le s$ for any $s \in S$ . Since $S$ has no least element, we have $x<s$ for any $s \in S$ .
Hence, $x+1 \le s$ for all $s \in S$ .
So, $x+1 \in A$ . Thus, $A$ is an inductive set. Hence, $A=\Bbb Z_0$ . Since $S \ne \emptyset$ , we can pick $a \in S \subseteq \Bbb Z_0$ such that $a+1 \in \Bbb Z_0$ . Since $A = \Bbb Z_0$ , then $a+1 \in A$ . Therefore, $a+1 \le a<a+1$ , which is a contradiction. Thus, $S$ has a least element. I already know the Well-Ordering Principle for positive integer $\Bbb N$ . But, I don't know yet that this approach above correct. Any helps would be appreciated. Thanks in advanced.","['solution-verification', 'discrete-mathematics']"
4527908,Convergence of a sequence of suprema of expected values,"Consider a sequence of stochastic processes $$((X_f^{(n)})_{f \in F})_{n \in \mathbb{N}}.$$ All the random variables $X_f^{(n)}$ are defined on the same probability space and assume only non-negative values. In my case $F$ is a unit ball of some separable Banach space. I am looking for a way to express the condition \begin{equation}\label{condition}
\lim_{n \rightarrow \infty} \sup_{f \in F} \mathbb{E}\left[ X_{f}^{(n)}  \right] = 0
\end{equation} in a fashion similar to ``convergence in probability + uniform integrability = convergence in $L^1$ ''. It would be  easy if either there was no supremum over $F$ or the supremum was inside of the expectation, but in the above case I cannot really come up with an if-and-only-if condition. Any help would be greatly appreciated!","['stochastic-processes', 'uniform-integrability', 'probability-theory']"
4527926,Difficulty in proving that the sum of two measurable function is a measurable function,"Let $(\Omega, \Sigma)$ , $(\mathbb{R},\mathcal{B})$ be the two measurable space. Let $f:\Omega\rightarrow \mathbb{R}$ , and $g:\Omega\rightarrow \mathbb{R}$ be the two measurable functions. In order to prove that $f+g$ is a measurable map, I need to show that $$\{\omega\in\Omega: f(\omega)+g(\omega)< x\}\in \Sigma, \forall x\in\mathbb{R}$$ I was reading a proof, which says that: $$\{\omega\in\Omega: f(\omega)+g(\omega)< x\}= \bigcup_{r\in\mathbb{Q}}\Big[\{\omega : f(\omega)<r\}\cap\{\omega: g(\omega)< x-r\}\Big].$$ I have no clue about how the above two quantities are equal. Can somebody simplify it?","['measure-theory', 'lebesgue-measure', 'measurable-functions', 'probability-theory', 'probability']"
4527964,Maximum of two standard normal distribution,"1. Introduction I want to know how the distribution of the maximum of two independent normal distribution is like. The background is that I am making AI of some games, and I am trying to do something similar to Minimax algorithm . However, with assumption that the noise of evaluation function is normally distributed, I realized that the algorithm needs to estimate how the maximum of several normal distribution is like. (There is NO requirement to understand the background like Minimax, for answering this question.) 2. Writing Distribution Mathematically Let $f_1(x) = \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}$ , the probability density function (PDF) of standard normal distribution, and let $\Phi_1(x)$ be the cumulative distribution function (CDF) of it. Let $f_2(x)$ and $\Phi_2(x)$ be the PDF and CDF of the maximum of two independent standard normal distribution. It is not difficult to see that $\Phi_2(x) = \Phi_1(x)^2$ . Therefore, $$f_2(x) = \frac{d}{dx} \Phi_2(x) = 2 \frac{d\Phi_1(x)}{dx} \Phi_1(x) = 2 f_1(x) \Phi_1(x)$$ We drew a graph at desmos.com of the PDF and CDF, and it was like the following figure (PDF is red and CDF is blue). We can see that the PDF is not symmetric, and is a little bit skewed to the right (positive skew). 3. The average and variance Using desmos.com , I could obtain the following result: The average is $\mu = 0.5641895835\dots$ The standard deviation is $\sigma = 0.8256452711\dots$ I could understand that $\sigma = \sqrt{1 - \mu^2}$ , because one can show that the expected square of variable is exactly $1$ . $$E[\max(x_1, x_2)^2] = \int_{-\infty}^{\infty} t^2 f_2(t) dt = \int_{-\infty}^{\infty} t^2 f_1(t) \Phi_1(t) dt$$ $$= \frac{1}{\pi} \int_{-\infty}^{\infty} t^2 e^{-\frac{t^2}{2}} \left(\int_{-\infty}^t e^{-\frac{u^2}{2}} du\right) dt = \frac{1}{\pi} \int_{-\infty}^{\infty} \int_{-\infty}^t t^2 e^{-\frac{t^2 + u^2}{2}} du dt$$ $$= \frac{1}{\pi} \int_0^{\infty} \int_{-\frac{3}{4} \pi}^{\frac{1}{4} \pi} r^3 \cos^2 \theta \cdot e^{-\frac{r^2}{2}} d\theta dr = \frac{1}{\pi} \cdot \left(\int_0^{\infty} r^3 e^{-\frac{r^2}{2}} dr \right)\left(\int_{-\frac{3}{4} \pi}^{\frac{1}{4} \pi} \cos^2 \theta d\theta \right)$$ $$= \frac{1}{\pi} \cdot \left[-(r^2 + 2) e^{-\frac{r^2}{2}}\right]_0^{\infty} \cdot \left[\frac{1}{2} (\sin x \cos x + x)\right]_{-\frac{3}{4} \pi}^{\frac{1}{4} \pi} = 1$$ 4. Question However, I don't have clue how to calculate the exact value of $\mu = 0.5641895835 \dots$ I thought that it is a well-known constant because it is a consequence of a simple problem. However, although I searched on the Internet, there did not seem to be any result about it. So, I would like to know: whether there is a formula of expressing $\mu$ , and the way to calculate $\mu$ precisely. Additionally, I am also thinking about generalizing to arbitrary set of independent normal distributions about the mean/variance of maximum value.","['normal-distribution', 'calculus', 'numerical-methods', 'constants', 'probability']"
4527986,"What elements are in the set $M=\{x \in \mathbb R \mid x^2+5x−6 \in (−1,\infty)\}$?","I want to ask what elements are in this set $M$ : $$M=\{x \in \mathbb R \mid x^2+5x−6 \in (−1,\infty)\}$$ The polynomial can be split into $(x-1)(x+6)$ , but idk how to get an answer. I thought its $M=\{1\}$ because only
the number $1$ is from $(−1,\infty)$ , but that turned out to be wrong.",['elementary-set-theory']
4528047,Using exponential family to find the derivate of the partition function [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 1 year ago . Improve this question Distributions that follow the exponential family have the following form \begin{equation}
p(x) = h(x)\exp \left[\theta^T T(x) - H(\theta)\right]
\end{equation} and $\frac{\partial}{\partial \theta}H(\theta) = \mathbb E_p[T(X)]$ . I have the following function \begin{equation}
H(\theta) = \log \int_z \exp \left[\log P(y|z) - \frac{z^2}{2v} + \frac{mz}{2v}\right]
\end{equation} I want to find $\frac{\partial}{\partial m}H(\theta)$ . For this define $T(z) = \begin{bmatrix} 1 & z/v & z^2 \end{bmatrix}^T$ and $\theta = \begin{bmatrix} \log P(y|z) & m & -1/2v \end{bmatrix}^T$ . Is it correct that $\frac{\partial}{\partial m}H(\theta) = \mathbb E[T_2(z)|\theta] = \mathbb E[\frac{z}{v}|\theta]$ ?","['statistics', 'normal-distribution', 'exponential-distribution', 'maximum-likelihood', 'exponential-function']"
4528079,Showing that the Fourier transform of $f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}$ is $\hat{f_n} = (-i)^nf_n$,"Let $n \in \mathbb{N}_0$ and define $f_n(t)=e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}$ . I am tasked to show that $\hat{f_n} = (-i)^nf_n$ for $\hat{f_n}$ the Fourier transform of $f_n$ . My problem is that I am either not seeing the integral trick I am supposed to use or I fail to see the pattern in the derivatives of $e^{-2\pi t^2}$ as I don't know how to proceed beyond $$\hat{f_n}(y) = \int_{\mathbb{R}}\left(e^{\pi t^2}\frac{d^n}{dt^n}e^{-2\pi t^2}\right)e^{-2\pi i t y}dt$$ I am not looking for a complete solution per se, but rather a better intuition for this problem. Although if you happen to know some elegant and general method for dealing with this problem, feel free to share it! Thanks!","['integration', 'fourier-analysis', 'fourier-transform', 'calculus', 'derivatives']"
4528100,"HMM, reverse engineering the transition matrix","I fitted a 2-states-HMM model last week, and generate a bunch of 1s and 0s, but I forgot to store its parameters (transition matrix). Now, I only got these 1s and 0s, how do I backward/reverse-engineering to estimates these transition matrix? Things that I have : the input data the outputed 0s and 1s from a fitted 2-states-HMM What I want : The transition matrix of the two-states HMM model. Please see the exact input and output data here: https://colab.research.google.com/drive/1N-PKaeHMVI4S1VU7fgcgDqRT5L3h8R0L?usp=sharing And go to the bottom which it says Input_data and Output_data","['statistical-inference', 'statistics', 'parameter-estimation', 'hidden-markov-models']"
4528102,Can we express sum of two sinusoids with different frequencies as a sum of two other sinusoids?,"Can we express the sum of two sinusoids with different frequencies and with different amplitudes as a sum of two other sinusoids with same frequencies as that of the original two sinusoids? for example, $a\cdot \sin(b\cdot x+c)+d\cdot \sin(e\cdot x+f)=p\cdot \sin(b\cdot x+t) +q\cdot \sin(e\cdot x+r)$ and are there infinite possibilities like this? And if so can we fix one one of the two sinusoids like, $a\cdot \sin(b\cdot x+c)+d\cdot \sin (e\cdot x+f)=p\cdot \sin(b\cdot x+t) +4.5\sin(e\cdot x+\frac{\pi}{4})$ or $p\cdot \sin (b\cdot x+t)+p\cdot \sin (e\cdot x+\frac{\pi}{4}).$ I have tried but failed. Can anyone help? I am new to here. Please correct me if I am wrong
Thank you",['trigonometry']
4528129,How different can posterior mean be for two mean zero unit variance priors?,"Suppose $Y \mid \theta \sim N(\theta, \sigma^2)$ . Let $\mu_G(Y)$ be the posterior mean for $\theta$ when $\theta \sim G$ is the prior. How large can the average discrepancy in posterior mean $$
\sup_{G, G_0, \sigma} E_{Y \sim G_0 \star N(0, \sigma^2)} [(\mu_G(Y) - \mu_{G_0}(Y))^2]
$$ be, where $G, G_0$ are constrained to have mean zero and variance one? My guess is that it's bounded by the squared Wasserstein-2 distance between $G, G_0$ , which is in turn bounded by 2, but I'm having trouble proving it.","['statistics', 'bayesian', 'probability']"
4528135,Let $\mu ^{*}$ be a arbitrary outer measure is necessarily satisfied? $\mu ^*(A \cup B) + \mu^{*}(A \cap B) \leq \mu^{*}(A)+\mu ^{*}(B)$,"Let $\mu ^{*}$ be a arbitrary outer measure in a set $X$ and $A,B \subset X$ , the following inequality is necessarily satisfied? $$\mu ^*(A \cup B) + \mu^{*}(A \cap B) \leq \mu^{*}(A)+\mu ^{*}(B)$$ I know that if $A$ (or $B$ ) is $\mu ^{*}-$ measurable then the equality holds but i try to show that supposing that $A,B$ are not $\mu^{*}-$ measurables. I proceded by cases: Case $1$ : $A \subset B$ in this case $A \cup B=B$ and $A \cap B=A$ then $$\mu ^*(A \cup B) + \mu^{*}(A \cap B) =\mu ^*( B) + \mu^{*}(A) $$ Case $2$ : $A \cap B =\emptyset$ in this case $\mu^{*}(A \cap B)=0$ and for subadditivity $$\mu ^{*}(A \cup B) + \mu^{*}(A \cap B)=\mu^{*}(A \cup B)+0\leq \mu^{*}(A)+\mu ^{*}(B)$$ Case $3$ : $A \cap B = \emptyset$ in this case i have problems,
intuitively I think it could be true but the term $\mu^{*}(A \cap B)$ is the problem, i search counterexamples but i not sure. Any hint or help i will be very grateful.","['measure-theory', 'outer-measure']"
4528166,What is the distribution shape of the frequencies of a random uniform distribution?,"Suppose $x$ is randomly sampled uniformly from $[0,N)$ . Suppose we take $M$ samples. In my testing, I have been using $N=50000, M = 1000000$ . The expected number of times each $i \in[0,N)$ was sampled is $M/N$ . The shape of this distrubition is uniform. However, suppose we took this random uniform distribution and counted, not the number of times $x$ was $0$ , but the number of values which were sampled $0$ times. Let's call that the $\mathbf{freq}(0)$ . We can get the $\mathbf{freq}(j)$ values for all $[0,M]$ . This distrubution is not uniform . It looks kind of like a Poisson distrubution centered at $M/N$ , but I'm not sure. In case I wasn't totally clear, here is an example. Suppose we randomly sample uniformly from $[0,10)$ 54 times. The number of times each was sampled is a random uniform distribution looking something like $\{5, 4, 5, 5, 6, 4, 3, 5, 6, 7, 4\}$ , meaning that among the 54 samples, 0 was sampled 5 times, 1 was sampled 4 times, 2 was sampled 5 times, etc. But the distribution of these frequencies is $\{0, 0, 0, 1, 3, 4, 2, 1,0, 0\dots\}$ since there is one value which was sampled three times, 3 values which were sampled four times, 4 values which were sampled five times, etc. What is the shape of this distribution?","['probability-distributions', 'uniform-distribution', 'binomial-distribution', 'probability']"
4528220,How can I construct a basis of $\mathbb{R}^n$ with this property?,"Given $\theta \in (0,\pi/2]$ I would like to find a basis of unit length vectors $\{v_1, \dots, v_n\}$ for $\mathbb{R}^n$ such that each $v_i$ has angle $\theta$ from the subspace spanned by the remaining vectors. Are there any references for something like this? A (recursive) family of examples would be great but even some algorithmic ideas would be helpful. For $n = 2$ it is trivial, take any unit length vector and rotate it by $\theta$ . For $n = 3$ I am already struggling. I have an ugly numeric approach that can find me one solution, however I am having a hard time generalizing it to dimensions $n > 3$ . (I'm happy to share more details but it's not clever at all). The properties of such a basis also means the pairwise angles between the $v_i$ will be the same, but for $n > 2$ it is larger than $\theta$ and (I imagine) continues to increase with the dimension. Knowing this angle in advance would also make constructing examples of these bases by hand much easier.","['linear-algebra', 'vector-spaces']"
4528239,Continuous function with partial derivative that is continuous in one variable but not the other,"Does there exist a function $f: \mathbb R^2 \to \mathbb R$ with the following property $f$ is continuous $\frac{\partial f}{\partial x}$ exists For each fixed $y$ , $\frac{\partial f}{\partial x}(x,y)$ is a continuous function of $x$ $\frac{\partial f}{\partial x}(0,y)$ is not a continuous function of $y$ I suspect the answer is yes, i.e. the first three properties do not imply the fourth since I don't see how to prove that, but I can't think of an easy example.","['partial-derivative', 'continuity', 'multivariable-calculus']"
4528252,Whats the idea behind using Bregman divergence (in particular Bregman proximal method) to minimise a functional?,"Given some functional $E$ on a convex set $\Omega$ , the Bregman divergence $D_E$ (of some convex function $E$ ) is defined at a point $p$ as the difference between its value at that point and its first order Taylor expansion around some other point $q$ : $$D_E(p,q):=E(p)-E(q)-\langle \nabla E(q) , p-q\rangle. $$ For some strictly convex function $\phi$ the proximal map according to this Bregman divergence is $$ \text{Prox}^{D_E}_\phi(p)=\text{argmin}_{q\in \mathcal{D}(\phi)}D_E(q,p)+\phi(q).$$ How can iterative Bregman projections be used for finding the infimum of $\phi$ ?","['convex-optimization', 'proximal-operators', 'numerical-optimization', 'analysis', 'optimization']"
4528254,Two different answers to $\lim_{x \to -\infty} \frac{8x^2-2x^3+1}{6x^2+13x+4}$,"Let us evaluate $$\lim_{x \to -\infty} \frac{8x^2-2x^3+1}{6x^2+13x+4}$$ Dividing the numerator and denominator by $x^3$ we will be left with $$\lim_{x \to -\infty} \frac{\frac{8}{x}-2+\frac{1}{x^3}}{\frac{6}{x}+\frac{13}{x^2}+\frac{4}{x^3}}$$ and since all the terms containg $x$ term will go to $0$ ,our resultant limit becomes $-\frac{2}{0}=-\infty$ . But if we do it in another way substituting $x=-t$ ,then our limit becomes after dividing both numerator and denominator by $t$ , $$\lim_{t \to \infty} \frac{\frac{8}{t}+2+\frac{1}{t^3}}{\frac{6}{t}-\frac{13}{t^2}+\frac{4}{t^3}}$$ Here also since terms containing $t$ go to $0$ ,we are left with $\frac{2}{0}=+\infty$ . Why are we getting two different answers? Surely one of the method is invalid. In books,the first answer was marked correct. But i want to know what's wrong with the second approach.",['limits']
4528258,Papa Rudin theorem $1.39$,"There is the theorem: Suppose $f:X\to [0,\infty ]$ is measurable, $E\in\mathfrak{M}$ , and $\displaystyle \int \limits _Ef\,d\mu =0$ . Then $f=0$ almost everywhere on $E$ . There is the proof: If $A_n=\left \{x\in E:f(x)>\dfrac{1}{n}\right \}$ , $n=1,2,3,\ldots$ , then $$\frac{1}{n}\mu (A_n)\leq \int \limits _{A_n}f\,d\mu \leq \int \limits _Ef\,d\mu =0,$$ so that $\mu (A_n)=0$ . Since $\displaystyle \{x\in E:f(x)>0\}=\bigcup \limits _{n=1}^\infty A_n$ , the theorem follows. I don't understand how  we get this inequality $\displaystyle \frac{1}{n}\mu(A_n)\leq \int \limits _{A_n}f\,d\mu$ . Any help would be appreciated.","['measure-theory', 'lebesgue-integral', 'analysis', 'real-analysis']"
4528284,$\operatorname{Hilb}^8(\mathbb{P}^4_k)$ not irreducible (Ex. in Hartshorne's Deformation Theory book),"Exercise 1.5.8 from Robin Hartshorne's Deformation Theory : 5.8. $\operatorname{Hilb}^8(\mathbb{P}^4_k)$ is not irreducible. Consider the Hilbert scheme of zero-dimensional
closed subschemes of $\mathbb{P}^4_k$ of length $8$ , the ground field $k$ is assumed to be algebraically closed. There is one component of dimension $32$ that
has a nonsingular open subset corresponding to sets of eight distinct points. (I suppose that the author uses it as nontrivial fact) We will
exhibit another component containing a nonsingular open subset of dimension $25$ . The Exercise comprises of four parts and I have problems with the first part: (a) Let $R := k[x, y, z,w]$ , let $\mathfrak{m}$ be a maximal ideal in this ring, and let $I = V + \mathfrak{m}^3$ , where $V$ is a $7$ -dimensional subvector space of $\mathfrak{m}^2/\mathfrak{m}^3$ . Let $B = R/I$ , and let $Z$ be the
associated closed subscheme of $\mathbb{A}^4 \subset \mathbb{P}^4 $ . Show that the set of all such $Z$ , as the
point of its support ranges over $\mathbb{P}^4$ , forms an irreducible $25$ -dimensional subset of
the Hilbert scheme $H = \operatorname{Hilb}^8(\mathbb{P}^4)$ . How to show that the ""set"" of the $Z$ 's as defined in (a) is irreducible? Let call it $S \subset H$ . The Hilbert scheme $H$ is constructed as closed subscheme of the Grassmanian defined by the vanishing of various determinants and is therefore we can endow the ""set"" $S$ as subscheme of $H$ with unique reduced scheme structure. On the set level / on $k$ -valued points $S(k)$ we can define canonically the map $p(k): S(k) \to \mathbb{P}^4(k)$ sending $Z$ the the unique maximal ideal $\mathfrak{m}_Z \subset k[x, y, z,w]$ associated to it as described in the construction above. How can this idea be converted into a 'honest' map $p:S \to \mathbb{P}^4$ ? As soon as it is possible to construct such map $p$ we can use a result (reference ?) that for a proper surjective map $f: X \to Y$ with $Y$ and all fibers irreducible of same dimension, the scheme $X$ is irreducible, too. Therefore the question reduces to 'How to construct $p:S \to \mathbb{P}^4$ from set map $p(k): S(k) \to \mathbb{P}^4(k)$ ?' In addition note that that's just my suggestion how roughly I wanna to tackle this exercise. Maybe there are more effective ways to do it. All suggestions for alternative approaches are of course welcome!","['algebraic-geometry', 'deformation-theory', 'schemes']"
4528286,Is the borel sigma algebra defined on the unit circle same as the borel algebra defined using the subspace topology?,"This is from Sheldon Axler's text on Measure and integration: So we have defined the borel sigma algebra on the unit disk to be the pullback of the sigma algebra on $(-\pi, \pi]$ . However, the unit disk itself has a topology defined as the subspace topology on the complex plane, so if we take those open sets and generate the Borel sigma algebra from that, do we get the same Borel algebra as the pullback? Also, it was not explicitly stated how we define a continuous function on the unit disk, but I assume it is defined using the subspace topology on the unit disk, so if that is the case, then these two different sigma algebras should coincide right?","['measure-theory', 'functional-analysis', 'analysis', 'real-analysis']"
4528354,"How many steps to arrive at $(x,y)$ by adding one number to the other?","I wanted to know if there was a specific formula to solve this problem or can this be solved by any sort of theories like The Birthday Problem . The problem states that you start off with a pair of numbers $(x_1, y_1) = (1, 1)$ . During each round, either $x$ is added to $y$ or $y$ is added to $x$ , to get a new $(x_2, y_2)$ . We have to figure out the minimum number of rounds to get to $(x_2, y_2)$ . Example:
starting point $(x_1, y_1) = (1, 1)$ ;
ending point $(x_2, y_2) = (3,5)$ . $$
(1, 1) \mapsto 
(1, 2) \mapsto 
(3, 2) \mapsto 
(3, 5)
$$ Hence, the shortest possible calculations takes $3$ rounds.","['elementary-number-theory', 'functions', 'combinatorics']"
4528380,"For all $a,b \in \mathbb{R}$ and $k > 0$ : $(a+b)^{2} \geq \dfrac{a^{2}}{1+k} - \dfrac{b^{2}}{k} $","For all $a,b \in \mathbb{R}$ and $k > 0$ we have $$
(a+b)^{2} \geq \dfrac{a^{2}}{1+k} - \dfrac{b^{2}}{k} 
$$ Why? I have shown that the above inequality is valid if and only if $b = \dfrac{k}{1+k}a$ and and which does not hold for $k < 0$ , because $$
\lim_{k \to 0^{-}} \dfrac{a^{2}}{1+k} - \dfrac{b^{2}}{k} = +\infty
$$","['calculus', 'algebra-precalculus']"
4528399,Proof of continuous function of a compact set is uniformly continuous.,"I want to proof: If $X$ is compact, $Y$ is another metric space, and $f:X\to Y$ is a continuous function. Then $f$ is uniformly continuous. I know this is a well-known statement,but the book wants me use the Lebesgue covering lemma to complete the proof: If $X$ is compact and $\mathcal{G}$ is an open cover of $X$ , then there is an $r>0$ such that for each $x\in X$ there is a $G \in\mathcal{G}$ such that $B(x,r)\subset G$ . I have no idea that how the 'open cover' in this lemma helps in the proof of the statement above.How can I get further? Any help and hints will be appreciated! Best regards!","['general-topology', 'real-analysis']"
4528424,"How are Fourier Transforms, Green's Functions and the Poisson Equation related?","So I have a question. It starts, as many good things in life do, with Fourier Transforms. I have been playing around with these things ever since I read somewhere that: $$
\mathcal{F} \left\{ \frac{d u}{dx} \right\}  = \omega \hat u
$$ Where the notation $\hat u$ is the Fourier Transform of $u(\vec r)$ and $\omega = i \hat x$ where $\hat x$ is the reciprocal coordinate such that $\hat u = \hat u(\hat x)$ . I managed to, using the definition of Fourier Transform found in Wikipedia, convince myself that this identity is true. So I ran with it. If that thing is true, than this following expression must also be true: $$
\frac{d u}{dx} = \mathcal{F}^{-1} \{\omega \hat u\}
$$ Which is a pretty weird way of differentiating something when you know how to do that symbolically, but by this point I had used discrete Fourier Transforms in a lot of grid-computations and I knew this to be quite a handy way of doing calculus with discretized functions for which I had no analytical form. So I got excited like a musician. You know what musicians do when they get excited and moved by the beauty in front of them? They compose . So I did, and here is what I got: $$
\frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\}
$$ Which really is just: $$
\frac{d }{dx} \left[ \frac{d u}{dx} \right] = \mathcal{F}^{-1}\{ \omega^2 \hat u\}
$$ This started looking a lot like this: $$
\frac{d^n u}{dx^n} = \mathcal{F}^{-1}\{ \omega^n \hat u\}
$$ Which look like a general expression for the $n$ -th derivative of any function $u$ . Even one that has no analytical form. I googled for about ten minutes and figured out that, indeed. That is true. And not only that but when $n$ is negative , it turns out that it stop being a derivative and starts being an antiderivative , a definite integral. I was mindblown. Fourier, it turns out, had unified calculus under a single computation framework. It is no wonder that this operator on $u$ is called the Fourier Differintegral . Charming. Now, what else can we do with derivatives and integrals? In 1D, it turns out, not much. But things get juicy when you go full planar, adding a whole new dimension to play with. With it, comes a coordinate, $y$ , a reciprocal coordinate $\hat y$ and an imaginary reciprocal coordinate $\sigma = i \hat y$ . Ah, and also a position vector $\vec x = (x,y)$ , a reciprocal position vector $\hat {\vec x} = (\hat x,\hat y)$ and its imaginary counterpart ${\vec \omega} = i \hat {\vec x} = (\omega, \sigma)$ Now we arrive at things I pulled out of my ass. For partial derivatives, I'm doing it like this: $$
\mathcal{F}\left\{ \frac{\partial u}{\partial x} \right\} = \omega \hat u
$$ Since $u$ is now a function of $\vec x$ I understand that $\hat u$ is a multidimensional Fourier Transform in 2D, and a function of $\vec \omega$ . Now, I guess that, equivalently, $$
\mathcal{F}\left\{ \frac{\partial u}{\partial y} \right\} = \sigma \hat u
$$ Notice how the imaginary reciprocal variable that multiplies the transform is actually related to the variable with respect to which the derivative is being taken. I don't know for sure that this is the case. I just assumed this was right and then started computing things. One thing, in particular that I calculated was this: $$
\frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \mathcal{F}\{ \mathcal{F}^{-1} \{\omega \hat u\}\}\}
$$ Which is really: $$
\frac{\partial }{\partial y} \left[ \frac{\partial u}{\partial x} \right] = \mathcal{F}^{-1}\{ \sigma \omega \hat u\}
$$ I was then very confident to say that: $$
\frac{\partial^2 u}{\partial x^2} = \mathcal{F}^{-1}\{ \omega^2 \hat u\}
$$ And likewise for $$
\frac{\partial^2 u}{\partial y^2} = \mathcal{F}^{-1}\{ \sigma^2 \hat u\}
$$ And here I noticed that: $$
\nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2] \hat u\} = \mathcal{F}^{-1} \{ |\vec \omega|^2 \hat u\}
$$ By this point I was really confident, so I decided to go up a notch and add a third dimension with coordinate $z$ , reciprocal $\hat z$ and imaginary reciprocal coordinate $\kappa$ . So in 3D I got: $$
\nabla^2 u = \mathcal{F}^{-1} \{[\omega^2 + \sigma^2 + \kappa^2] \hat u\} = |\vec \omega|^2 \hat u
$$ And here I have grown arrogant (and potentially sloppy) and I thought I could solve the Poisson Equation just using this Fourier magic calculus I had uncovered, so the gods of mathematics punished me for my hubris. It went like this: Here is the Poisson Equation: $$
\nabla^2 \varphi = \rho
$$ Now if I take the Fourier Transform on both sides, here is what I get: $$
|\vec \omega|^2 \hat \varphi = \hat \rho
$$ Which suggests that I can just... $$
\hat \varphi = |\vec \omega|^{-2} \hat \rho
$$ And taking the inverse Fourier Transform on both sides we get that: $$
\varphi = \mathcal{F}^{-1} \{ |\vec \omega|^{-2} \hat \rho \}
$$ But then I found out that this type of equation is actually solved using Green's Functions, like so: $$
\varphi = -\int \int \int \frac{\rho({\vec r}')}{4\pi |\vec r - {\vec r}'|} d^3 \vec r'
$$ Which looks rather... strange. Suddenly there is a 6D function ( $|\vec r - {\vec r}'|^{-1}$ ) multiplying $\rho$ and the entire thing looks like it was solved using volume integrals, which should be like this: $$
\int \int \int u d^3 \vec r = \mathcal{F}^{-1}\left\{\frac{1}{\omega \sigma \kappa}\hat u \right\}
$$ And not this: $$
\varphi = \mathcal{F}^{-1} \left\{ \frac{1}{\omega^2 + \sigma^2 + \kappa^2} \hat \rho \right\}
$$ ... which is the expression I had found using the Fourier Differintegral. So I can't help but ask: Where did I drop the ball? Where in my reasoning did I assume wrong things? I already know I dropped the ball somewhere, since my method did not arrive at something resembling Green's Functions, I just don't know where and I feel the answer will teach me a valuable lesson about the relationship between Fourier Transforms, Green's Functions, convolutions and Poisson's Equations. Maybe that was the lesson the gods were trying to teach me. May wisdom make humble people of us all. Cheers. EDIT 1 This edit is intended as a response to @martin 's answer. Martin, you said in your answer that: $$
\varphi = \mathcal{F}^-1\{|\vec \omega|^{-2}\} \ast \rho
$$ So I went around trying to figure out what \mathcal{F}^-1{|\vec \omega|^{-2}} is and how to solve a convolution in 3D. Here's what I got: So formula 502 in this table shows me that: $$
\mathcal{F} \{ |\vec x|^{-a} \} = \frac{(2 \pi)^a}{c_{n,a}} |\vec \omega|^{-(n-a)} 
$$ if $0 < \Re a < n$ where: $$
c_{n,a} = \pi^\frac{n}{2} 2^a \frac{\Gamma(a/2)}{\Gamma \left( \frac{n-a}{2} \right) }
$$ I noticed that the pair $a=1$ and $n=3$ satisfies the constraint that $0 < \Re a < n$ and gives formula 502 a promising look: $$
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{c_{3,1}} |\vec \omega|^{-2} 
$$ also, for this particular pair,  the complicated $c_{n,a}$ formula acquires a much simpler form, check it out: $$
c_{3,1} = \pi^\frac{3}{2} 2 \frac{\Gamma(1/2)}{\Gamma(1)}
$$ since $\Gamma(1/2) = \sqrt{\pi}$ and $\Gamma(1) = 1$ we have that: $$
c_{3,1} = \pi^\frac{3}{2} 2 \sqrt{\pi}$
$$ And we can use the relationships between exponentiation and square roots to simplify it even further like so: $$
\pi^\frac{3}{2} \sqrt{\pi} = \pi^\frac{3}{2} \pi^\frac{1}{2} = \pi^\frac{3+1}{2} = \pi^\frac{4}{2} = \pi^2
$$ Which means that $c_{3,1}=2\pi^2$ ! How neat is that? -ahem- Anyway, we can plug that back into the formula as: $$
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{2 \pi}{2\pi^2} |\vec \omega|^{-2} 
$$ That warrants some simplification, which gives us $$
\mathcal{F} \{ |\vec x|^{-1} \} = \frac{1}{\pi} |\vec \omega|^{-2} 
$$ Because the Fourier Transform and its inverse are integrals and $\frac{1}{\pi}$ is a constant, I assume I can do this: $$
 \pi |\vec x|^{-1} = \mathcal{F}^{-1} \{|\vec \omega|^{-2} \}  
$$ I can then define a function $f(\vec x) = \pi |\vec x|^{-1}$ and, going back to our Poisson equation $\nabla \varphi = \rho$ , we can state that $$
\varphi = f \ast \rho
$$ Now, I didn't find any compact, understandable formula for a 3D integral, but I found this one: $$(f \ast g)(t)=\int_{-\infty}^{\infty} f(\tau) g(t-\tau) d \tau$$ And since convolutions are commutative then: $$(f \ast g)(t)=(g \ast f)(t)=\int_{-\infty}^{\infty} f(t-\tau)  g(\tau) d \tau$$ so now I'm praying that this thing, when blown to 3D dimensionality, looks like this: $$
(f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty} f({\vec x}-{\vec x}')  g(\vec x ') d^3 \vec x '
$$ When substituting $r=|{\vec x} - {\vec x}'|$ and $\rho(\vec x ') = \rho'$ we get that $$
\varphi = (f \ast \rho)(\vec x)=\int\int\int_{-\infty}^{\infty}\frac{\pi \rho'}{r} d^3 \vec x '
$$ Which looks different from the correct one: $$
\varphi = (f \ast \rho)(\vec x)=-\int\int\int_{-\infty}^{\infty}\frac{\rho'}{4 \pi r} d^3 \vec x '
$$ I'm thinking I dropped the ball somewhere around the transition from 1D convolutions to 3D convolutions, but IDK. Could be an incorrect usage of formula 502. Once again I have to ask Where did I drop the ball? but at least now I know where the 6D pairwise potential comes from.","['multivariable-calculus', 'integral-transforms', 'partial-differential-equations']"
4528455,Proof explanation of Dilworth's theorem.,"I am reading the proof of Dilworth's theorem which uses induction.I have several doubts regarding the proof and I am looking for explanation.I first tried to resolve those doubts on my own but it seems to be too much difficult without any help as my brain is not so combinatorial .I state the theorem below and give the proof which I read.I will mention where I have doubts. Theorem(Dilworth,1950) Let $P$ be a finite poset.Then the maximum size of an antichain in $P$ is equal to the minimum size of a cover of $P$ by disjoint chains. Proof: Let $M$ and $m$ denote respectively the maximum size of an antichain in $P$ and minimum size of a cover of $P$ by disjoint chains. It is trivial that $m\geq M$ . Now we will show that $m\leq M$ .We proceed by induction on $|P|$ . For $|P|=1$ ,it is trivial as both the sizes are $1$ . Now assume that the result is true for $k<|P|$ . Now,we prove the result for $k=|P|$ . Let $C$ be a maximal chain in $P$ . Case-1: Every antichain in $P\setminus C$ contains at most $M-1$ elements. So, $P\setminus C$ is a union of $\leq M-1$ disjoint chains by induction hypothesis. $\implies$ $P$ is a union of $\leq M$ disjoint chains. So, $m\leq M$ . Case-2: Assume that $\{a_1,a_2,...,a_M\}=A$ is an antichain in $P\setminus C$ . Define $S^-=\{x\in P:x\leq a_i \text{ for some } i \}$ $S^+=\{x\in P:x\geq a_i \text{ for some } i \}$ Therefore $A\subset S^-$ Because of maximality,the largest element in $C$ is not in $S^-$ So, $|S^-|<|P|$ By induction hypothesis, $S^-$ is the union of $M$ disjoint chains $S_1^-,S_2^-,...,S_M^-$ where $a_i\in S_i^-$ $\color{red}{\text{(Why?)}}$ If $x\in S_i^-$ and $x>a_i$ then since $x\leq a_j$ for some $j$ we would get $a_i<a_j$ which is a contradiction. Therefore $a_i$ is the maximal element of $S_i^-$ for all $i=1(1)M$ Do,the same for $S^+=\bigcup\limits_{i=1}^M S_i^+$ where $S_i^+\cap S_j^+=\phi$ for all $i\neq j$ and $a_i\in S_i^+$ such that $a_i$ is the minimal element of $S_i^+$ $D_i=S_i^+\cup S_i^-$ is a chain for $i=1(1)M$ observing that $P=S^+\cup S^-$ is the union of disjoint chains $D_i$ where $i=1(1)M$ This is from a note given by our instructor.Can someone explain me the proof so that it becomes crystal clear?Concrete examples at critical points is also welcome. Addendum I have tried to provide as much I have understood below. Clearly, $m\geq M$ because if $\{a_1,a_2,...,a_M\}$ is a maximal antichain and $P=A_1\cup A_2\cup...\cup A_m$ is a cover of $P$ by disjoint chains,then no two $a_i$ 's are in the same $A_k$ so,at least $M$ disjoint chains are required. Now,we have to show, $m\leq M$ . Now to understand Case:1 let us consider the following example: Let, $X=\{1,2,3\}$ and $P=\mathcal P(X)$ .Notice that $M=3$ here. Now, $C=\{\phi,\{1\},\{1,2\},\{1,2,3\}\}$ Then, $P\setminus C=\{\{2\},\{3\},\{2,3\},\{3,1\}\}$ Antichains in $P\setminus C$ are $\{\{2\},\{3\}\}$ and $\{\{2,3\},\{3,1\}\},\{\{2\},\{3,1\}\}$ . So,we observe that all antichains in $P\setminus C$ contains $\leq 3-1=2$ elements and hence we are in Case:1 . Now,clearly we obtain that $P$ is a union of at most $3$ disjoint chains.So, $m\leq 3=M$ . Now let us come to Case:2 which says that there is an antichain in $P\setminus C$ of size $M$ . Assume that $A=\{a_1,...,a_M\}$ is an antichain in $P\setminus C$ . Let, $S^-=\{x\in P:x\leq a_i$ for some $i\}$ Then, $A\subset S^-$ .The explanation is given below: $x\in A\implies x=a_i\leq a_i$ for some $i\implies x \in S^-$ Now, $C$ is a finite chain and hence it has a largest element say $a$ . Due to maximality of size of $C=\{...,a\}$ , $a\notin S^-$ . If $a\in S^-\implies a\leq a_i$ for some $i\implies C\cup \{a_i\}$ is larger chain in size that $C$ . So, $|S^-|<|P|$ . So,we can use the induction hypothesis. $S^-=S_1^-\cup...\cup S_M^-$ disjoint chains $\color{red}{\text{(Why $M$ chains?It could have been any number $k\geq M$,so why $k>M$ is not possible?)}}$ Where $a_i\in S_i$ . Because for $i\neq j$ , $a_i,a_j$ both cannot be in $S_i$ together or in $S_j$ together. Now, $a_i$ is (the) maximal element of $S_i^-$ . Because if $x\in S_i^-$ and $x>a_i$ Then since $x\in S_i^-\subset S^-\implies x\leq a_j$ and so, $a_i<a_j$ which is a contradiction. Doing the same for $S^+$ we can show that $a_i$ is the minimal element of $S_i^+$ . So, $D_i=S_i^-\cup S_i^+=\{....\leq a_i\leq...\}$ is a chain. For $i\neq j$ , $D_i\cap D_j=\phi$ . $\color{red}{\text{(Why?)}}$ So, $P=S^+\cup S^-=D_1\cup...\cup D_M$ which is a union of $M$ disjoint chains. So, $m\leq M$ .","['proof-explanation', 'order-theory', 'combinatorics', 'discrete-mathematics']"
4528459,Does relativization of formulas replace bounded quantifiers or not?,"Let $\phi$ be a formula and $a$ be a set . The relativization $\phi^a$ of $\phi$ to $a$ is defined as the formula obtained by replacing all [ unbounded ?] quantifiers $Qx$ in $\phi$ by $Qx {\in} a$ . I sometimes see the definition that replaces all quantifiers, bounded or unbounded. However, I sometimes see the definition that replaces only unbounded quantifiers. For example: Reference 1 (p. 60): $C^z$ is obtained from $C$ by replacing all unbounded quantifiers $\exists x$ in $C$ by $\exists x \in z$ . Reference 2 (p. 833): For every $\mathrm{L}_\Omega$ formula $A$ we write $A^\sigma$ to
denote the $\mathrm{L}_\Omega$ formula which is obtained by replacing
all unbounded quantifiers $(Q \xi)$ in $A$ by $(Q \xi < \sigma)$ . Reference 3 (p. 29-30): Here $\Phi^M$ is the formula obtained from $\Phi$ by replacing all unbounded quantifiers of the forms $\forall x, \exists x$ by quantifiers $\forall x \in M, \exists x \in M$ . [...] \begin{align}
(\forall x \Phi)_M &= \begin{cases}
\forall x \Phi_M & \text{$\Phi$ is of the form $x \in a \to \Psi$} \\
\forall x {\in} M \Phi_M & \text{otherwise}
\end{cases} \\
(\exists x \Phi)_M &= \begin{cases}
\exists x \Phi_M & \text{$\Phi$ is of the form $x \in a \land \Psi$} \\
\exists x {\in} M \Phi_M & \text{otherwise}
\end{cases}
\end{align} Reference 4 (p. 10): If $A$ is a transitive set and $\phi$ is a formula with parameters in $A$ we denote by $\phi^A$ the formula which arises from $\phi$ by replacing all unbounded quantifiers $\forall u$ and $\exists v$ in $\phi$ by $\forall u \in A$ and $\exists v \in A$ , respectively. Reference 5 (p. 33): Further, we write $A^\sigma$ to denote the $\mathcal{L}_\Omega$ formula which is obtained from $A$ by replacing all unbounded stage quantifiers in $Q \tau$ in $A$ by bounded stage quantifiers $(Q\tau \prec \sigma)$ . Which of the two definitions is correct? Is one more prominent or common than the other? Is there a standard notation and/or terminology for each that distinguishes the two? References: The art of ordinal analysis . Michael Rathjen. 2006. Some theories with positive induction of ordinal strength $\phi \omega 0$ . Gerhard Jäger, Thomas Strahm. The Journal of Symbolic Logic. Volume 61. Issue 3. September 1996. Pages 818-842. Large sets in constructive set theory . Albert Zieger. The University of Leeds School of Mathematics. December 2014. Proof theory of constructive systems: inductive types and univalence . Michael Rathjen. 5 January 2018. On Feferman’s operational set theory OST . Gerhard Jäger. Annals of Pure and Applied Logic. Volume 150. Issues 1-3. December 2007. Pages 19-39.","['predicate-logic', 'quantifiers', 'notation', 'elementary-set-theory', 'terminology']"
4528460,A random walk on $\mathbb{Z}$ is transient iff $\sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty$,"I was asked to prove the said equivalence for a simple random walk on the integers (which may be biased): $X_{i}\in\left\{ \pm1\right\} $ , $S_{n}=S_{0}+\sum_{i=1}^{n}X_{i}$ . The first direction is pretty direct: assume that $\sum_{i=0}^{\infty}\mathbb{P}\left(S_{i}=S_{0}\right)<\infty$ , then according to the Borel–Cantelli lemma, we have $\mathbb{P}\left(S_{i}=S_{0}\quad i.o.\right)=0$ . I am having trouble with the other direction. Ultimately, I would like to use the counterpart of B-C, but $S_i=S_0$ and $S_j=S_0$ might be dependnet (in the case $\mathbb{P}\left(X_{i}=\pm1\right)\neq0.5$ ). If we define $Z=\#\left\{ i\mid S_{i}=S_{0}\right\} $ we know that $\mathbb{E}\left[Z\right]=\sum_{i=0}^{n}\mathbb{P}\left(S_{i}=S_{0}\right)=\infty$ , but I don't know how to proceed from here. I have seen this question which seems very relevant, but it is rather not detailed.","['random-walk', 'probability']"
4528468,Highly polytopic algebras,"( Update: I ended up posting two answers for this, each addressing different things. I feel the second one is more deserving to be accepted if nobody posts a better answer.) Aside from the complex numbers, the triplex numbers , and their isomorphisms, I'd like to know which other unital algebras with dimensions greater than one follow these rules: The algebra must contain non-trivial finite cyclic groups under multiplication aside from $\{-1,1\}$ . This means the algebra has more than two ""roots of unity"". Given any such cyclic group, the elements of the group must form some or all the vertices of a single regular convex polytope instance in the underlying vector space of the algebra. For a cyclic group $G$ that forms vertices of a polytope instance $P$ , let's call $G$ a ""polytopic group"" of $P$ . If $G$ covers all the vertices of $P$ , let's call it a ""full polytopic group"" of $P$ . Since only $n$ -simplexes, $n$ -orthoplexes, and $n$ -orthotopes have regular convex variants in all dimensions $n$ , we can restrict ourselves to those polytope families, but let us add a third rule: If the algebra is $n$ -dimensional, all regular convex simplexes, orthoplexes, and orthotopes of $n$ dimensions or less must have instances in the underlying vector space such that, for any such polytope instance $P$ , there are polytopic groups of $P$ that collectively cover all its vertices. Let us call such an algebra a "" highly polytopic algebra "". For $n=2$ , as implied, $\Bbb{C}$ is a highly polytopic algebra. The imaginary unit $i$ and its powers are a full polytopic group of a square (which is both a 2-orthoplex and a 2-orthotope), and the third roots of unity in the complex plane are a full polytopic group of an equilateral triangle (2-simplex). I don't think split-complex numbers have polytopic groups that cover all three vertices of an equilateral triangle, and dual numbers don't even have that for a square, but please correct me if I'm wrong. For $n=3$ , I believe the three-dimensional triplex numbers (which, I've been told, is isomorphic to $\mathbb R\times \mathbb C$ ) is also highly polytopic, as I'll try to demonstrate below. To lessen confusion, let's denote the two non-real elements of the triplex basis as $j$ and $k$ . Triplex number multiplication is commutative and has $j^3=k^3=jk=1$ , $j^2=k$ , and $k^2=j$ . We find the following polytopic groups in triplex space: $3$ -orthoplex $1$ , $j$ , $k$ and their negatives form a full polytopic group of a regular octahedron , with its generator elements being $-j$ and $-k$ : $$-j\to{k}\to{-1}\to{j}\to{-k}\to{1}$$ $$-k\to{j}\to{-1}\to{k}\to{-j}\to{1}$$ $2$ -orthoplex/ $2$ -orthotope Side note: The non-unit circle that touches $1$ , $j$ , and $k$ and with radius $\sqrt{\frac{2}{3}}$ shares one important property with the unit circle on the complex plane: A full polytopic group of any regular polygon can be generated on the circle using an exponential formula. For the triplex numbers, a generator for an $n$ -sided polygon is equals to $e^{\frac{2\pi}{n\sqrt{3}}(j-k)}$ . Using that formula, we find that $s=\frac{1}{3}-\frac{(\sqrt{3}-1)}{3}j+\frac{\sqrt{3}+1}{3}k$ is a generator of a square on the circle described above. $$\left(s=\frac{1}{3}-\frac{(\sqrt{3}-1)}{3}j+\frac{\sqrt{3}+1}{3}k\right)\to{\left(s^2=-\frac{1}{3}+\frac{2}{3}j+\frac{2}{3}k\right)}\to{\left(s^3=\frac{1}{3}+\frac{\sqrt{3}+1}{3}j-\frac{(\sqrt{3}-1)}{3}k\right)}\to{\left(s^4=1\right)}$$ $3$ -orthotope The square we just described above is a face of a cube centered at the origin in triplex space. The other vertices of the cube are the negatives of $s$ and its powers. Notably, $-s=-\frac{1}{3}+\frac{(\sqrt{3}-1)}{3}j-\frac{(\sqrt{3}+1)}{3}k$ is equivalent to the number $c$ I mentioned in this question about what I call ""cubic numbers"". This cube's vertices are covered by four polytopic groups: $\{s, s^2, s^3, 1\}$ , $\{-s, s^2, -s^3, 1\}$ , $\{-s^2, 1\}$ , and $\{-1, 1\}$ , but perhaps there's a full polytopic group of a cube there somewhere that I didn't find. $3$ -simplex $-s$ is also a generator of a full polytopic group of one of the demicubes of the cube described above: $$\left(-s=-\frac{1}{3}+\frac{(\sqrt{3}-1)}{3}j-\frac{(\sqrt{3}+1)}{3}k\right)\to{\left(s^2\right)}\to{\left(-s^3=-\frac{1}{3}-\frac{(\sqrt{3}+1)}{3}j+\frac{(\sqrt{3}-1)}{3}k\right)}\to{\left(1\right)}$$ A demicube is of course a regular tetrahedron $2$ -simplex This one's the easiest: The third roots of unity $1$ , $j$ , and $k$ form a full polytopic group of an equilateral triangle . This could also be seen by evaluating $e^{\frac{2\pi}{3\sqrt{3}}(j-k)}$ . You could check out the above-mentioned triplex polytopes in this geogebra link . (The point labeled ""A"" is meant to be the multiplicative identity.) Questions: Is there any highly polytopic algebra in four dimensions? I know $\Bbb{H}$ is not highly polytopic because quaternion powers are always co-planar with each other and the origin, so $\Bbb{H}$ doesn't have polytopic groups of regular polyhedra and polychora aside from the 3-orthoplex and the 4-orthoplex. I ultimately would like to know if there's an algorithm to construct highly polytopic algebras in higher dimensions. Also, unfortunately I can't seem to find a full polytopic group of a cube in triplex space. If there is one, perhaps I could make a stronger definition of ""polytopic group"" that is synonymous to full polytopic groups. Is it possible that I just missed one? Motivation: I'm mainly interested in full polytopic groups of $n$ -simplexes because they relate to other ideas I've had in the past, particularly the $n$ -rational numbers , but I figured that adding the other polytope families might potentially be more interesting (and perhaps more useful?) for others.","['cyclic-groups', 'geometry', 'abstract-algebra', 'group-theory', 'hypercomplex-numbers']"
4528483,Other solutions to cubing the cube variation,"It's known that a cube can't be divided into smaller cubes of distinct sizes. So for fun, I defined a ""wannabe cube"" as a cuboid whose dimensions are (not equal, but) consecutive integers and looked to solve the analogous problem. So let's say the $k$ -th wanna be cube is the rectangular prism of size $k\times(k+1)\times(k+2).$ Turns out the fifth wannabe cube can be divided into the first four. Or oppositely, we can build up the fifth wannabe cube from the first four as follows: Join the first and second wannabe cubes along a $2\times 3$ face producing a $2\times 3\times 5$ prism. Join this to the third wannabe cube along a $3\times 5$ face producing a $3\times5\times 6$ prism. Join this to the fourth wanna be cube along a $5\times6$ face produing a $5\times 6\times 7$ prism, which is the fifth wannabe cube. This construction actually works in any dimension. Say if we define a wannabe $n$ -cube as an $n$ -dimensional orthotope of consecutive integral dimensions. Particularly, the $k$ -th wannabe $n$ -cube is the orthotope of size $k\times(k+1)\times...\times(k+n-1)$ . Then the $(n+2)$ -th wannabe $n$ -cube can always be divided into the first $n+1$ such wannabe $n$ -cubes. In, say, 2 dimensions this amounts to fitting together the three rectangles of sizes $1\times 2$ , $2\times 3$ , and $3\times 4$ into a $4\times 5$ rectangle. These constructions can be seen as physical embodiments of an identity on Pascal's triangle: $$\sum_{n=k}^{2k}\binom{n}{k}=\binom{2k+1}{k}$$ Namely, that the first $k+1$ entries of the $k$ -th diagonal sum to the $(k+2)$ -th entry. I'm wondering if there are any divisions of wannabe $n$ -cubes for $n>2$ that aren't given by this family of constructions. For 3 dimensions, I was able to show that the outlined construction is the only one which can be obtained by incrementally joining wannabe cubes to a growing cuboid along matching sides. For clarity, I'm also considering two orthotopes the same regardless of their orientation. E.g. that a division into distinct wannabe cubes could not use both a $1\times2\times 3$ prism AND a $3\times 1\times 2$ prism. Since the volumes of wanna be cubes are just 6 times the tetrahedral numbers, my current plan is to look for tetrahedral numbers which can be expressed as a sum of smaller distinct tetrahedral numbers in many ways, and then try building up a division in 3D space. But this hasn't turned up any new solutions. I know Smith diagrams and Kirchoff laws were used to solve squaring the square and thought maybe those laws could be used here. But I'm not sure how to get started with that approach.","['dissection', 'binomial-coefficients', 'geometry']"
4528498,"Question on gauge fields ""acting on different representations""","- First of all, in the end of this question, unfortunatelly, I'll kind of request the solution of an exercise. But, this isn't for any kind of classroom evaluation. It is just that I kind of grasp the heuristic picture, but I don't know how to perform the technical calculations, due to my poor knowledge on lie algebra and lie group theory. I'm following the text book $[1]$ . The question is written in section VII $)$ , on equation $(8)$ . Now, standard model of particle physics is a (chiral) gauge theory . Therefore, it is a theory which is enconded in the language of Principal fiber bundles and associated fibre bundles. I will write a series of topics that explain better my problem here. I) Connection $1$ - $\mathrm{forms}$ Now, in Salam-Weinberg model, the gauge group of the theory is $G =  SU(2)_{L} \times U(1)_{Y}$ . The connection constructed in the principal bundle, the $(SU(2)_{L} \times U(1)_{Y})$ - $\mathrm{bundle}$ , is the connection $1$ - $\mathrm{form}$ : $$A = W + B.\tag{1}$$ $A$ is not the electromagnetic gauge field, rather, the connection $1$ - $\mathrm{form}$ of the principal bundle. $W$ is the weak gauge field and $B$ the hypercharge gauge field. II) Local Connection $1$ - $\mathrm{forms}$ A local version of $(1)$ (the local gauge field) that ""puts the gauge field on spacetime"" is given by: $$A_{s} = s^{*}A.\tag{2}$$ Where, $s$ is a section on the principal bundle (the local gauge choice), and the $s^{*}$ is the pullback of the section (when we apply this map on $A$ , we bring the information of the gauge field for a region located on the base manifold $\mathcal{M}$ ). Furthermore, since our algebraic landscape deals with groups and lie groups, the action of $A_{s}$ on a vector field $X \in T_{p}\mathcal{M}$ lies on the lie algebra: $A_{s}(X) \in \mathfrak{g}$ . III) Representations, Spinors and Multiplets The map, $\rho: G \to GL(V)$ is the representation of the Lie group (gauge group)  Therefore, using this map we produce matrices that acts on vectors of a vector space $V$ . Furthermore, given a representation $\rho$ , it is possible to define its differential representation: $$\rho_{*}: \mathfrak{g} \to \mathrm{End}(V),$$ where the operation $\cdot_{*}$ is the pushfoward. The necessity of dealing with spinors as multiplets introduces a algebraic structure called: ""Twisted Spinor Bundle"" $[1]$ : $$TS = S \otimes E = S \otimes (P\times_{\rho}V). \tag{3}$$ Where, S is the spinor bundle, and E the associated bundle (the $P$ is the principal bundle and $\rho: G \to GL(V)$ the representation). The tensor structure $(3)$ tells us: ""we have spinor fields in $S$ and the fact that we construct a tensor product with $E$ we construct the well-known multiplets $\psi$ "". Actually, Twisted Spinor Bundles are also called Gauge Multiplet Spinor Bundles . IV) Covariant Derivatives 1 In $(3)$ we can construct the covariant derivative of the theory acting on multiplets (spinors) as: $$D^{A}_{\mu}\psi = \partial_{\mu}\psi + \rho_{*}(A_{s}(X))\psi = \partial_{\mu}\psi - \frac{ig}{2}W^{a}_{\mu}\sigma_{a}\psi - \frac{ig'}{2}B_{\mu}\psi. \tag{4}$$ V) Chirality An important feature of the standard model is its chirality. Following $[1]$ , this means that the whole twisted spinor bundles ""slipts"" in right part $R$ and left part $L$ as: $$ S \otimes E = (S_{L} \otimes E_{L}) \oplus (S_{R} \otimes E_{R}). \tag{5} $$ The spinor field notion is then defined with the one observes that first $S$ splits as $S_R\oplus S_L$ and that we are hence free to define a field $\psi_R$ as a section of some $S_R \otimes E_R$ and a field $\psi_L$ as a section of some $S_L \otimes E_L$ where $E_R$ and $E_L$ have no a priori relation. Then we define $\psi = \psi_R \oplus \psi_L$ The structure $(5)$ is called ""Twisted Chiral Spinor Bundle"". Due to this mathematical structure, and knowing that $E_{L}$ and $E_{R}$ depends on representations. There are two (possibly distinct) representations of $G$ on complex vector spaces $V_{R}$ and $V_{L}$ , i.e., the whole formal bundle that I'm talking about is (with all structures explicitly showed): $$TS_{\mathrm{chiral}} = S \otimes E = (S_{L} \otimes E_{L}) \oplus (S_{R} \otimes E_{R})  = (S_{L} \otimes (P\times_{\rho_{L}}V_{L})) \oplus (S_{R} \otimes (P\times_{\rho_{R}}V_{R})). \tag{6}$$ The map, $\rho_{L}: G \to GL(V_{L})$ is the representation of the Lie group (gauge group), that produce matrices that acts on vectors of a vector space $V_{L}$ . Similarly, the map, $\rho_{R}: G \to GL(V_{R})$ is the representation of the Lie group (gauge group), that produce matrices that acts on vectors of a vector space $V_{R}$ . Therefore, we have two distinct induced representations as well: $\rho_{L*}: \mathfrak{g} \to \mathrm{End}(V_{L})$ and $\rho_{R*}: \mathfrak{g} \to \mathrm{End}(V_{R})$ . VI) Covariant Derivatives 2 In same fashion, we can construct the covariant derivative of the theory acting on chiral multiplets (spinors) as: $$D^{A}_{\mu}\psi= \partial_{\mu}\psi + \rho_{L*}(A_{s}(X))\psi_{L} + \rho_{R*}(A_{s}(X))\psi_{R} \implies$$ $$D^{A}_{\mu}\psi \equiv D^{A}_{\mu}(\psi_{L} + \psi_{R}) =\partial_{\mu}\psi_{L} + \rho_{L*}(A_{s}(X))\psi_{L} + \partial_{\mu}\psi_{R} + \rho_{R*}(A_{s}(X))\psi_{R}\tag{7}$$ VII) My Question Explicitly, I have the following data: $G = SU(2)_{L} \times U(1)_{Y}$ $\mathfrak{g} = \mathfrak{su}(2)_{L} \oplus \mathfrak{u}(1)_{Y}$ $V_{L} = \mathbb{C}_{L}^2 \otimes \mathbb{C}_{Y}$ $V_{R} = \mathbb{C}_{L} \otimes \mathbb{C}_{Y}$ The subscripts are nothing but labels, the $\mathbb{C}$ 's are your standard linear algebra complex vector spaces. Also, for your convenience: $\rho_{L*}:\mathfrak{su}(2)_{L} \oplus \mathfrak{u}(1)_{Y} \to \mathrm{End}(\mathbb{C}_{L}^2 \otimes \mathbb{C}_{Y})$ $\rho_{R*}:\mathfrak{su}(2)_{L} \oplus \mathfrak{u}(1)_{Y} \to \mathrm{End}(\mathbb{C}_{L} \otimes \mathbb{C}_{Y})$ So my question is: how can I show the explicit calculation (i.e. could you please write the calculation) that: $$\rho_{R*}(A_{s}(X)) = \rho_{R*}(W_{s}(X)+B_{s}(X)) = \rho_{R*}(W_{s}(X))+\rho_{R*}(B_{s}(X)) = \rho_{R*}(B_{s}(X))? \tag{8}$$ There are three different, but equivalent, ways to ask question: $1)$ Exercise $8.11.8$ of $[1]$ , page $525$ . $2)$ What happens, explicitly, with $\rho_{R*}(W_{s}(X))$ ? I mean, this term seems to simply ""vanish"". $3)$ In physics (local chart) language: concerning the gauge group of Salam-Weinberg theory, how can I show that: $$\rho_{L*}(A_{s}(X)) = - \frac{ig}{2}W^{a}_{\mu}\sigma_{a} - \frac{ig'}{2}B_{\mu} \tag{9}$$ and $$\rho_{R*}(A_{s}(X))= - \frac{ig'}{2}B_{\mu}?\tag{10}$$ Where $\sigma_{a}$ are the Pauli Matrices . $[1]$ Mark J.D. Hamilton Mathematical Gauge Theory , Springer, 2017.","['lie-algebras', 'connections', 'gauge-theory', 'physics', 'differential-geometry']"
4528531,Can we define a limit of a sequence of groups? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 1 year ago . Improve this question This is just a random thought I had while procrastinating some group theory. Is there a meaningful way to talk about limits of groups? I don't know if this has any use, but here is my thought process so far. We would need a topology on the ""set"" of all groups (but already this might be a problem because this is too big to be a set); the only idea I had would be an order topology, where G is smaller than H if there is an injective homomorphism from G to H.","['limits', 'group-theory']"
4528541,Simplifying the determinant of a matrix.,"Suppose $$A = \begin{pmatrix} 1+a_{1}+a_{1}b_{1}+b_{2} & 1+a_{1} & 1 & 0\\ a_{2}+a_{2}b_{1}+b_{3} & 1+a_{2} & 1 & 1\\ a_{3}+a_{3}b_{1} + b_{4} & a_{3} & 1 & 1\\ a_{4} + a_{4}b_{1} & a_{4} & 0 & 1\end{pmatrix}$$ Show that $$\det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4} & 1+a_{1}-a_{3}+a_{4}\\ -1-b_{2}+b_{3} & a_{1}-a_{2}+a_{4}\end{pmatrix}$$ I am not sure how to show this. I tried to perform some row and column operations but could simplify matrix $A$ . Note that for a general matrix of size $(3k+1)\times (3k+1)$ , the determinant of $A$ can be written in a similar form. Say for a $7 \times 7$ matrix, $$A_{7 \times 7} = \begin{pmatrix} 1+a_1+a_1b_1+b_2 & 1+a_1 & 1& 0 &0 &0 &0\\ a_2 + a_2b_1 + b_3 & 1+a_2 & 1 & 1&0&0&0\\ a_3+a_3b_1+b_4 & a_3 & 1 & 1 & 1& 0 &0\\ a_4+a_4b_1+b_5 & a_4 & 0 & 1 & 1&1 &0\\ a_5+a_5b_1+b_6 & a_5 & 0 & 0 & 1&1 &1\\ a_6+a_6b_1+b_7 & a_6 & 0 & 0 & 0&1 &1\\ a_7 + a_7b_1 & a_7 & 0 & 0 & 0&0 &1
\end{pmatrix}$$ $$\det(A) = \det\begin{pmatrix} b_{1}-b_{2}+b_{4}-b_{5}+b_{7} & 1+a_{1}-a_{3}+a_{4}-a_{6}+a_{7}\\ -1-b_{2}+b_{3}-b_{5}+b_{6} & a_{1}-a_{2}+a_{4}-a_{5}+a_{7}\end{pmatrix}$$ Any thoughts on how to show this in general?","['matrices', 'determinant', 'linear-algebra']"
