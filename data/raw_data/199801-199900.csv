question_id,title,body,tags
3897522,Why is $\sum\limits_{n=1}^{\infty}e^{-(n/10)^2}$ almost equal to $5\sqrt\pi-\frac12$ (agreeing up to $427$ digits)?,"The following I saw as an exercise in a text on modular forms. I am lacking the understanding for why the following is true, but it is nevertheless astonishing: Why is the numerical value of $$x:=\sum_{n=1}^{\infty} \exp(-(n/10)^2)$$ so ridiculously close to the value of $$y:=5\sqrt\pi-\frac12$$ while (and this is the truly surprising aspect) not being equal to it ? And by ""ridiculously close"" I mean that $x$ and $y$ agree up to the 427-th digit after the decimal point: \begin{align}
x=8.
&3622692545275801364908374167057259139877472806\\
&1193564106903894926455642295516090687475328369\\
&2723327081134118121412853331180764328622113012\\
&6254685480139353423101884932655256142496258651\\
&4475413114466047689633981400087319507675739860\\
&2583500950926170092927234872474563201569608877\\
&6295310820270966625045319920380686673873757671\\
&6833994894682925918204397725582580869380029533\\
&6967158956664049274231240924510273274260978066\\
&257808237337\color{red}{62}
\end{align} They first disagree in the red digits. Question: Can someone explain this in ""simple"" words?
It does not have to be elementary, but I want to understand what machinery has to interact to arrive at this.
And how would one even come up with such an example, or similar ones?","['algebraic-number-theory', 'intuition', 'decimal-expansion', 'modular-forms', 'sequences-and-series']"
3897585,The diameter of the set of projections in a C*-algebra is at most 1,"Let $p,q$ be projections in a $C^*$ -algebra $A$ . I am trying to show that $\|p-q\|\leq1$ , but I can't. If the projections $p,q$ commute, then this is easy: we set $C=C^*(1,p,q)$ and this is an abelian $C^*$ -algebra. By the Gelfand representation, we have that $\sigma(x+y)\subset\sigma(x)+\sigma(y)$ in an abelian C*-algebra, thus $\sigma_A(p-q)=\sigma_C(p-q)\subset\sigma_C(p)-\sigma_C(q)\subset\{-1,0,1\}$ and therefore $\|p-q\|\leq1$ . But what about the general case?","['c-star-algebras', 'operator-theory', 'functional-analysis', 'operator-algebras']"
3897597,Techniques for backwards-Leibniz theorem of product of derivative?,"If I have a function like $$u(t)f'g + w(t) fg'$$ and I know that for general $f,g$ : $$(fg)' = f'g + fg'$$ Is there some general way for me to modify the expression so that I can end up with derivative of a product? Own work: $$\frac{\partial }{\partial t}\left\{f(a(t)) \cdot g(b(t))\right\} = a'(t)f'(a(t))\cdot g(b(t)) +f(a(t))\cdot b'(t)g'(b(t))$$ So if I can get $b',a'$ to match $u,w$ then it shall work. A special case when this works should be ""time-flipping"" $a(t) = -t$ $$\sin(t) y(t) + \cos(t) y'(t) =/y(-t) = u(t)/= (u(-t)(-\cos(t)))'$$ But this is just a toy example to see that it works for a simple function. I am curious if it always will be possible or at least for some reasonably large family of functions.","['derivatives', 'calculus', 'soft-question', 'linear-algebra']"
3897612,A geometry question...,"In the given figure, $ABCD$ is a square of side $3$ cm. If $BEMN$ is another
square of side $5$ cm & $BCE$ is a triangle right angled at $C$ . Then the
length of $CN$ will be:- I plotted this on GeoGebra: I can't figure out how to use the smaller square for the question. I did try finding the area of triangle $CBN$ . For that I tried subtracting the area of $CEMN$ , but to no avail. I want to approach this problem mathematically rather than graphically. I highly suspect that there is a way to involve the smaller square somehow. Or, if I somehow prove that $\triangle BCE\sim  \triangle CBN$ , that could solve it.","['euclidean-geometry', 'area', 'geometry', 'geometric-transformation', 'rotations']"
3897617,Expansion of the Frobenius norm,"this might be very elementary question. I was confused by looking at some different sources when expanding the Frobenius norm into trace. Would these two expressions below always be the same? Or only under certain conditions? \begin{aligned}
\left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)\left(X-Y\right)^{\top}\right)
\end{aligned} \begin{aligned}
\left\|X-Y\right\|_{F}^{2}&=\operatorname{tr}\left(\left(X-Y\right)^{\top}\left(X-Y\right)\right)
\end{aligned} Thanks","['matrices', 'transpose', 'linear-algebra', 'matrix-norms']"
3897629,"Find equation of a plane, that contains a point, is perpendicular to plane and parallel to a line.","I have to solve this problem from Analytic Geometry, but I'm pretty much stuck. Find equation of plane π, that contains point A, is perpendicular to plane ρ, and is parallel to line p. Point: A = [1, 1, 1] Plane: ρ:  x - 2y + z - 3 = 0 Line: p:  x + y - 2z - 2 = 0
   2x - y + 3z - 1 = 0 We should get 3 equations in total, to get the plane π. We get the first equation from point A: A = [1, 1, 1] = [x, y, z] -> xa + yb + zc + d= 0
a + b + c + d= 0 The second one is the normal vector of plane ρ: n = (1, -2, 1) -> xa + yb + zc = 0
a - 2b + c = 0 And the third from the vector line p (this is where I get stuck): x + y - 2z - 2 = 0
2x - y + 3z - 1 = 0
Some mind blowing calculation...","['analytic-geometry', 'geometry']"
3897654,"How many different ""Dwayne"" ""The Rock"" ""Johnson"" orderings can be made for $n$ Dwayne The Rock Johnsons?","Let me explain. Whenever I hear Dwayne Johnson's name, I immediately start mixing it up in weird ways by saying his name and nickname multiple times but in random orders. For example ""Dwayne"" ""The Rock"" ""Dwayne"" ""The Rock"" ""Johnson"" ""Johnson"" . But I always follow the same set of rules: If I say a number of ""Dwayne"" 's, $n$ , I have to say the other two items ""The Rock"" and ""Johnson"" $n$ times as well. Basically the full name needs to be said a whole number of times throughout the expression. If I say ""The Rock"" it must be because I said a corresponding ""Dwayne"" sometime before it. For example: ""Dwayne"" ""The Rock"" ""The Rock"" ""Dwayne"" ""Johnson"" ""Johnson"" is not valid. The same is the case for the ""Johnson"" 's after ""The Rock"" . i.e. They have to appear in order. So my question is: For a given number $n$ of ""Dwayne"", ""The Rock"", and ""Johnson"" 's how many different Dwayne The Rock Johnson orderings can I have that follow these rules.","['permutations', 'combinations', 'puzzle', 'combinatorics']"
3897694,"If $f:[0,a] \rightarrow \mathbb{R}$ is continous for all $a>0$ then $f:\mathbb{R}_+ \rightarrow \mathbb{R}$ is continuous?","Let $f:[0,a] \subset \mathbb{R}_+ \longrightarrow \mathbb{R}$ be a continuous function. Question. If $f$ is well defined and continuous for all $a>0$ , then can I assert that $f:\mathbb{R}_+ \longrightarrow  \mathbb{R}$ is also well defined and continous? I think so, since $a>0$ is arbitrary. But I'm not sure.","['continuity', 'functions', 'real-analysis']"
3897726,First Isomorphism Theorem - hole in the proof from my book?,"I have a book that contains a proof for their so-called ""First Isomorphism Theorem"", which esentially states that if $f$ is a surjective group morphism $f:G\to G'$ and $H$ is a normal subgroup of $G$ , then $G/H$ is isomorphic to $G'/f(H)$ . Their proof goes like this: They consider $\pi: G' \to G'/f(H)$ to be the canonical map, $\pi(x) = xf(H)$ , which is surjective. Then they take $f':G\to G'/f(H), \ f'=\pi \circ f$ . Since $f$ and $\pi$ are surjective, $f'$ is also surjective. Thus, by a previously proven theorem in the book (called the 'Fundamental Isomorphism Theorem'), we have that $G/\ker f'$ is isomorphic to $G'/f(H)$ . Now they go on to prove that $\ker f' = H$ in order to finalize the proof. The following sequence of equivalences holds: $x \in \ker f' \Leftrightarrow \pi(f(x)) = 1 \Leftrightarrow f(x) \in \ker \pi = f(H)$ . (the $1$ here is the unity for $G'/f(H)$ , so $f(H)$ ). So now we have $f(\ker f') =f(H)$ and now comes the part where I get stuck. They now call on a previously proven theorem (called the Correspondence Theorem for Normal Subgroups), which states that if we have a surjective group morphism $f:G\to G'$ , then the map $H\to f(H)$ from the set of normal subgroups of $G$ that contain $\ker f$ to the set of normal subgroups of $G'$ is bijective), but this does not look correct as we have no knowledge of whether $H$ actually contains $\ker f$ ( $\ker f'$ does). I also thought of $\pi$ 'ing the equality and instead using $f'(\ker f') = f'(H)$ , but we don't know that $\ker f'$ belongs to $H$ either). **EDIT: ** Source of the faulty theorem: The Romanian book ""Bazele Algebrei, vol. I"" (English would probably be ""Fundamentals of Algebra) by C. Năstăsecu, C. Niță and C. Vraciu, Bucharest, 1986. The page is 54 and the theorem I mentioned is Proposition 3.19.","['normal-subgroups', 'group-theory', 'abstract-algebra', 'group-isomorphism']"
3897734,How to prove collinearity of circumcenters,"Let $A_{1}A_{2}A_{3}$ be a non-isosceles triangle with incenter I. Let $C_{i}$ , $i = 1, 2, 3$ , be the
smaller circle through $I$ tangent to $A_{i}A_{i+1}$ and $A_{i}A_{i+2}$ (the addition of indices being mod 3). Let $B_{i}$ , $i = 1, 2, 3$ , be the second point of intersection of $C_{i+1}$ and $C_{i+2}$ . Prove that the circumcenters
of the triangles $A_{1}B_{1}I, A_{2}B_{2}I, A_{3}B_{3}I$ are collinear. This is an IMO shortlisted question , I have the solution which uses inversion to proof this. However I am not able to visulaize it properly as the diagram, which I have drawn as per my understanding clearly shows that the points are not collinear. Please use a diagram and explain the answer in more elegant manner.
Here is the solution","['euclidean-geometry', 'analytic-geometry', 'inversive-geometry', 'geometry']"
3897756,"Subsequence such that integrals converge over any Borel set in $[0,1]$","I was reading this question: Existence of subsequence such that integration converge The idea is this. I have a sequence of uniformly bounded measurable functions $\{f_{n}\}$ on $[0,1]$ and I want to find a subsequence $f_{n_{j}}$ such that $\lim_{n \to \infty} \int_{A} f_{n_{j}}$ exists for all Borel sets $A$ . I can show the following: (1) If $\{S_{i}\}_{i}$ is a countable collection of Borel sets, then we can find a subsequence so that $\int_{S_{i}} f_{n_{j}}$ has a limit for all $S_{i}$ . (2) That this holds for all half-open half-closed intervals $(a_{i}, b_{i}]$ with rational endpoints. e know that the collection of half-open half-closed intervals with rational endpoints is countable and generates the Borel $\sigma$ -algebra, so the idea is now to approximate every Borel set using sets in this algebra and show that the result holds for them. In particular if $A \subset [0,1]$ is a Borel subset then we can find a sequence $I_{i}$ of half-open half-closed intervals with rational endpoints such that $I_{i} \downarrow A$ , but I'm not able to proceed further. Is it true that if $\int f_{n_{j}}$ has a limit on each $I_{i}$ , and $I_{i}$ is a decreasing sequence of sets, then $\int f_{n_{j}}$ has a limit on $\bigcap_{i} I_{i}$ ?","['borel-sets', 'measure-theory', 'lebesgue-integral', 'sequences-and-series']"
3897760,High-Level math subjects applicable to contest math,"For context, I'm a sophomore high school student. I'll be done with undergrad Abstract Algebra, Topology, and Analysis by the end of the school year, and I'll be burning through the necessary graduate Analysis, (Rudin) and Algebra (Lang) over the summer for Hartshorne, and doing some Differential Geometry from Tu along with Hartshorne. I'm hoping that by the time I make it to Diophantine Geometry, I'll be able to use it to solve Olympiad number theory problems (Diophantine Equations in particular). However, that's something I can't really find out for myself until I actually learn basic Diophantine Geometry, which will take at least two years. So, in order to get around that, I thought ""Why not ask people who know Diophantine Geometry?"" Although this question is mainly about Arithmetic Geometry, answers about other high-level math subjects applicable to Olympiad problems are welcome. And here we are. By the way, I also enjoy the Algebra by itself, it's not like I want to slog through more for the express purpose of solving competition math problems. I just wanted to know if it was an alternative or supplement to studying classical methods.","['contest-math', 'self-learning', 'diophantine-equations', 'algebraic-geometry', 'soft-question']"
3897823,Problem on Hahn-Banach and separation theorem,"A problem on Hahn-Banach theorem and convex separation theorem: Let $X$ be a real normed linear space. $E$ a convex subset of $X$ with nonempty interior, $F$ a linear subspace, with $(\operatorname{Int} E)\cap F=\emptyset$ . Then there exists a bounded linear functional $f\in X^*, f\not\equiv0$ , such that $f(E)\leq0,f(F)=0$ . I think the following corollary of Hahn-Banach theorem may be somewhat useful, but I can't figure out how to combine it with the convex separation theorem. Let $A$ be a linear subspace of normed linear space $X$ , $x_0\in X$ , $d(x_0,A)>0$ , then there exists $f\in X^*$ s.t. $f(A)=0,f(x_0)=d(x_0,A)$ and $||f||=1$ . Thanks for any help or suggestions.","['hahn-banach-theorem', 'convex-analysis', 'functional-analysis', 'real-analysis']"
3897831,How can I prove that any function $f: \mathbb{N} \rightarrow \mathbb{R}$ is continuous?,"I have to find all functions of the type: $$f: \mathbb{N} \rightarrow \mathbb{R}$$ that are continuous. My claim is that all such functions are continuous. If we think about it, $f$ would only have isolated points and we know that a function is always considered continuous at an isolated point. So, by this reasoning, any function $f:\mathbb{N} \rightarrow \mathbb{R}$ is continuous. I hope my reasoning is correct. What I am confused about is how could I prove this formally. We know that continuity is defined like this: A function $f: A \rightarrow \mathbb{R}$ is continuous at a point $c \in A$ if $$\forall V \in \mathcal{V}(f(c)), \exists U \in \mathcal{V}(c) \text{ such that } \forall x \in U \cap A \text{ we have } f(x) \in V$$ How could I possibly prove my point using the definition of continuity? It's really not that difficult to find the answer intuitively, but I don't see how I could make my argument more formal.","['continuity', 'general-topology', 'functions']"
3897855,The definition of the Klein form and its fundamental properties,"In Kubert-Lang's ""Units in the modular function field II"", for a fixed integer $N$ ,
the Klein form is defined as, for $r, s \in \mathbb{Z}$ (at least one of them is not $\equiv 0 \mod N$ ) and for $\omega_i \in \mathbb{C}$ with $\Im \omega_1/\omega_2 \gt 0$ , $$ k_{r,s}(\omega_1, \omega_2)
 = \exp( - \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}),$$ where $\eta$ is the Weierstrass eta function, $\sigma$ is the Weierstrass sigma function, with respect to the latice $ \Lambda = \omega_1 \mathbb{Z} + \omega_2 \mathbb{Z}$ . (i.e., $\eta(\omega) = \zeta(z + \omega) - \zeta(z)$ for any $z \in \mathbb{C}$ , where $\zeta$ is the Weierstrass zeta function with respect to the latice,
and $$ \sigma(z)
 = z \Pi_{(a,b) \in \mathbb{Z}^2 - \{ 0 \}}(1 - \frac{z}{a\omega_1 + b\omega_2}) \exp(\frac{z}{a\omega_1 + b\omega_2} + \frac{1}{2} (\frac{z}{a\omega_1 + b\omega_2})^2).$$ ) The authors says that, for integers $a, b \in \mathbb{Z}$ ,
the Klein form satisfies $$k_{r + aN, s + bN} (\omega_1, \omega_2)
 = (-1)^{ab + a +b } \exp(-2 \pi i \frac {(as - br)}{2N}) k_{r,s}(\omega_1, \omega_2).$$ Using the relation $$ \sigma(z + \omega)/\sigma(z) = (-1)^{ab + a + b}\exp(\eta(\omega)(z + \frac{1}{2} \omega))$$ for $\omega = a\omega_1 + b\omega_2 \in \Lambda$ ,
and using the Legendre relation, in order to show the relation, we must show that $$\exp( - \frac{(r+aN)\eta(\omega_1) + (s + bN)\eta(\omega_2)}{N} \frac{(r+aN)\omega_1 + (s + bN)\omega_2}{N} + \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N} 
+ \eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))) \\
= \exp(-\frac{2 \pi i (as - br)}{2N}). $$ I tried again and again, but I can't get the desired result.
If the term $\eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))$ were replaced by $2\eta(a\omega_1 + b\omega_2)( \frac{r\omega_1 + s\omega_2}{N} + \frac{1}{2} (a\omega_1 + b\omega_2))$ , then I could show the relation. And the authors says that for $\gamma \in \Gamma(N)$ , $k_{0,a}(\gamma \tau)/k_{0,a}(\tau)$ is a $2N$ root of $1$ .
But p108 of the Mazur's paper ""Modular curves and the Eisenstein ideal"", the authors says that $k_{0,a}(\gamma \tau)/k_{0,a}(\tau)$ is not constant. And, in that Mazur's paper, the author also says that
the $q$ expansion of $k_{0,a}(\tau)$ is ( $q = \exp( 2 \pi i \tau)$ ) $$ - \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ (where $\zeta_N = \exp(\frac{2 \pi i}{N}).$ ) But computing it, I've got the wrong result: $$ - \exp(- \frac{1}{2}\eta(1) (\frac{a}{N})^2) \frac{1}{2 \pi i}\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ Now the $q$ expansion of $\sigma(a/N, \tau \mathbb{Z} + \mathbb{Z})$ is $$ -  \frac{1}{2 \pi i} \exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2)\exp(-\frac{2 \pi i}{2N} a)(1 - \zeta_N^a) \Pi_{m \ge 1} (1 - \zeta_N^a q^m)( 1 - \zeta_N^{-a}q^m)(1 - q^m)^{-2}.$$ So if the term $\exp(\frac{1}{2}\eta(1) (\frac{a}{N})^2)$ were replaced by $\exp(\eta(1) (\frac{a}{N})^2)$ , then I could show it. What is wrong in my opinion?
Or are there another definitions of the functions?
(It seems that if the definition of the Klein form is $$ k_{r,s}(\omega_1, \omega_2)
 = \exp( - 2 \frac{r\eta(\omega_1) + s\eta(\omega_2)}{N} \frac{r\omega_1 + s\omega_2}{N}) \sigma(\frac{r\omega_1 + s\omega_2}{N}),$$ then I can show everything, except the ""transformation law"" described in p108 of Mazur's paper.)","['modular-arithmetic', 'complex-analysis', 'elliptic-functions', 'functional-analysis', 'modular-forms']"
3897858,Evaluate $\lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{k}{k^2+n^2}$,"$$\lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{k}{k^2+n^2}$$ I got asked this question in a group, and solved it the following way: $$\lim\limits_{n\to\infty}\left(
\int\limits_1^{n+1}\frac{x}{x^2+n^2}\space dx\leq \sum\limits_{k=1}^{n}\frac{k}{k^2+n^2}\leq \int\limits_1^n \frac{x}{x^2+n^2}\space dx + f(1)\right)
$$ $$\frac{\ln2}{2}\leq \lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{k}{k^2+n^2}\leq\frac{\ln2}{2}$$ Therefore: $$\lim\limits_{n\to\infty}\sum\limits_{k=1}^{n}\frac{k}{k^2+n^2} = \frac{\ln2}{2}$$ The guy who asked me the question said that the answer key said $\frac{\ln\left(\frac{5}{2}\right)}{2}$ but also said that it might be incorrect, so I don't have the answer. What I want to ask is that how come this sum is not equal to $0$ ? We were able to use the squeeze theorem because the limit for $f(1)$ goes to zero. Every other value after $f(1)$ also goes to $0$ (even faster?) and that had me thinking about how this summation can be equal to such a value. Can anyone please explain how this happens? And it would be great if you could also verify or correct my solution.","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
3897918,Hilbert scheme of points of complex surface,"Let $X$ be a smooth algebraic surface over $\mathbb{C}$ i.e a smooth and connected scheme of finite type over $\operatorname{spec}(\mathbb{C})$ of dimension $=2$ . I denote with $X^{[n]}$ the Hilbert scheme of $0$ -dimensional subscheme of length $=n$ . This is known to be a smooth scheme of dim $=2n$ which moreover admits the Hilbert-Chow morphism: $$\chi:X^{[n]} \to X^{(n)} $$ where $X^{(n)}=X \times \cdots X/S_n$ is the symmetric product. Among the properties of $\chi$ it is frequently stated that this morphism restricts to a fiber bundle (in the analytic topology) when considering $$\chi^{-1}(X) \rightarrow X $$ where $X \hookrightarrow X^{(n)} $ via the diagonal embedding. If I got it correctly the idea should be that every fibre $\chi^{-1}(x) \in \chi^{-1}(X)$ is isomorphic to the punctual Hilbert scheme and one would use this to obtain the property. I was not able however to actually describe  explicit trivialization maps neither to get which open subsets to use in the trivializing cover nor why one should use the analytic and not the Zariski topology. Moreover, is there an analogous statement over other algebraically closed field where the world analytic is substituted by etale?","['complex-geometry', 'algebraic-geometry', 'hilbert-polynomial']"
3898000,What's the actual volume of a Great Stellated Dodecahedron? (Im getting seemingly different formulas),"So I was trying to derive the formula for the Great Stellated Dodecahedron starting from a unitary side length Dodecahedron. I managed only to get so far. So when I went to check the actual formula, I found that Mathworld and WolframAlpha were giving me seemingly different formulas, but both assuming unit edge length. The WolframAlpha formula: $V = \frac{1}{4}(7 \sqrt5 - 15) $ The Mathworld formula: $V = \frac{5}{4}(3 + \sqrt5 ) $ I could be misreading the entry on Mathworld (I don't speak english natively)
Now im curious about this difference, both are clearly formulas for something but they can't be both the volume of the Great Stellated Dodecahedron. What am I missing?","['solid-geometry', 'volume', 'geometry', '3d']"
3898041,Limit set of a function is a closed set,"Let $f: M\rightarrow N$ be a continous function between topological spaces, in the case that I am interested we are working with manifolds so we have nice properties. We define the limit set of $L(f)$ as the set of $y\in N$ such that $y=\lim_{n\rightarrow \infty}f(x_n)$ for some sequence $\{x_n\}$ in $M$ that has no convergent subsequence. Is this set going to be closed ? I have tried taking $y\in \bar L(f)$ and take a sequence $y_n\rightarrow y$ such that $y_n=\lim_{k\rightarrow \infty} f(x_n^k)$ . Now I have tried constructing a sequence for $y$ of $f(x_i)$ such that $x_i$ has no convergent subsequence, I have tried a diagonalization argument and so on, but I got nothing.  Does anyone know if this is true or not and if so why ? New edit : As was mentioned in the comments there will be a counterexample if $M$ is not a manifold , so I guess I can change the question to if anyone knows if this is true if we assume that $M$ and $N$ are manifolds? And if this is not true I guess a thing that would help me is that if $f(M)\cap L(f)= \emptyset$ will we have that $f(M)\cap \bar L(f) =\emptyset$ ? This is because I am trying to prove that if we have that $f(M)\cap L(f)=\emptyset $ then we will have that there exists an open set $V$ such that $f(M)\subset V$ and $f:M\rightarrow V$ is proper, and $L(f)$ is exactly the points where properness fails, and this are the obvious first choices to create the open set I think. Thanks in advance.",['general-topology']
3898095,Name for the surface generated by 3D in 4D space and has it has been studied somewhere!!,"I recently come across the equation of the form: $$ 0 = - 5 + 4(x^2+y^2+z^2+w^2) + 16 x y z w$$ As can be seen, it is quartic in four variables $x,y,z,w$ . This yields a three-dimensional surface in a four-dimensional space. To visualize four-dimensional space, just like in four-dimensional spacetime, we can consider $w$ a ""time variable"" and study the three-dimensional surfaces $(x,y,z)$ for fixed $w$ . However, my question is, whether the above surface has a name and whether it has been studied somewhere (i.e. solution of the above polynomial equations)?
For e.g., when $w =1,-1$ , I suspect that it is related to the ""Cayley nodal cubic surface"".","['algebraic-geometry', 'surfaces', 'differential-geometry']"
3898124,Does $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ converge?,"Let's consider the series: $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ . I would suggest that it doesn't converge. One can see it as follows: Let be $S_{2n}:=\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n}$ and $S_{2n+1}:=\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1}$ two subsequences (subseries?) of $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ . Both converge due to alternating series test (Leibniz criterion). However, if I take a look at $|S_{2n}-S_{2n+1}|$ , I notice: $$|S_{2n}-S_{2n+1}|=\Big|\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\sum\limits_{k=1}^{2n+1}\frac{(-1)^k}{k}(-1)^{2n+1}\Big|\\=\Big|2\sum\limits_{k=1}^{2n}\frac{(-1)^k}{k}(-1)^{2n} -\frac{1}{2n+1}\Big|=2\sum\limits_{k=1}^{2n}\frac{(-1)^{k-1}}{k} +\frac{1}{2n+1}>1.
$$ Hence, $\sum\limits_{k=1}^{n}\frac{(-1)^k}{k}(-1)^n$ violates the Cauchy-criterion, or in other words I can always find two partial sums which are not arbitrarily close to each other. Is this correct? Is there a faster or more elegant way two show this result?","['convergence-divergence', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3898146,Summation indices in a proof of Incluson-Exclusion Principle,"I rewrote (for easier time reading in the future) the proof of PIE given in my book as follows below and got tangled up in sum notation. I got questions about the $\color{purple}{\text{purple}}$ and $\color{brown}{\text{brown}}$ bits in the rewritten proof below. The expression $N_{\ge}(\emptyset)$ has no index $j$ so I think $\color{purple}{\displaystyle{\sum_{j=0}^nN_{\ge}(\emptyset)} = N_{\ge}(\emptyset)}$ because there's only one item to sum. Does that make sense? In the case of $\color{brown}{\displaystyle{N_{\ge}(J)\sum_{j=0}^n\left((-1)^m\binom nm\right) = N_{\ge}(J) \cdot 0}}$ , we have $\color{brown}{\displaystyle{\sum_{j=0}^n\left((-1)^m\binom nm\right) = 0}}$ , but the actual theorem says $\displaystyle{\sum_{j=0}^n\left((-1)^j\binom nj\right) = 0}$ and so indices don't match in the $\color{brown}{\text{brown bit}}$ . How can I make the indices match up in the $\color{brown}{\text{brown bit}}$ ? Here below is PIE as given in my book: Definition Statement of PIE Proof Here below is the rewritten proof: Let $U$ be a universe of objects and let $P = \{p_1, p_2, p_3, \ldots, p_n\}$ be a set of properties that the objects may or may not have. Let $N_=(J)$ be a set of all objects all of whose properties are in $J \subseteq P.$ Suppose $|J| = 2$ . Then some of the $N_=(J)$ s are $N_=(\{p_1, p_7\}), \ N_=(\{p_{n-1}, p_n\})$ etc. There are $\binom n2$ such $N_=(J)$ s. Generally, if $|J| = j$ , then there are $\binom njN_=(J)$ objects all of whose properties are in $J$ meaning $\color{red}{\displaystyle{\sum_{|J| = j}(-1)^jN_=(J) = (-1)^j\binom njN_=(J)}}$ . Summing both sides of the expression in $\color{red}{\text{red}}$ above, we have $$\color{blue}{\sum_{j=0}^n\left(\sum_{|J| = j}(-1)^jN_=(J)\right) = \sum_{j=0}^n(-1)^j\binom njN_=(J)}$$ Since removing $N_=(J)$ elements also removes $N_{\ge}(J)$ elements, we can replace $N_=(J)$ s in the expression $\color{blue}{\text{in blue}}$ above with $N_{\ge}(J)$ s as follows: $$\color{green}{\sum_{j=0}^n\left(\sum_{|J| = j}(-1)^jN_{\ge}(J)\right) = \sum_{j=0}^n(-1)^j\binom njN_{\ge}(J)}$$ Note that the number of objects in $U$ with none of the properties is $N_{\ge}(\emptyset)$ meaning the number of times $N_{\ge}(\emptyset)$ includes each object in $U$ is $1$ when the object has none of the properties and $0$ when the object has at least one property. If we can say the same thing about the expression $\color{green}{\text{in green}}$ above, then we can use it instead of $N_{\ge}(\emptyset)$ . Now assume an object in $U$ contains none of the properties in $P$ . Then $|J| = 0$ meaning $\displaystyle{\sum_{j=0}^n\left(\sum_{|J| = 0}(-1)^0N_{\ge}(\emptyset)\right) = \color{\purple}{\sum_{j=0}^nN_{\ge}(\emptyset) = N_{\ge}(\emptyset)}}$ . Since $N_{\ge}\emptyset$ includes each object in $U$ only $1$ time when the object has none of the properties, so does the expression $\color{green}{\text{in green}}$ above. Suppose an object in $U$ contains $m$ properties in $P$ where $1 \le m \le n$ . Then $|J| = m.$ Now $\displaystyle{\sum_{j=0}^n\left(\sum_{|J| = m}(-1)^mN_{\ge}(J)\right) = \sum_{j=0}^n\left((-1)^m\binom nmN_{\ge}(J)\right) = \color{brown}{N_{\ge}(J)\sum_{j=0}^n\left((-1)^m\binom nm\right) = N_{\ge}(J) \cdot 0} = 0}$ Thus the expression $\color{green}{\text{in green}}$ above doesn't even count an object with properties in $J$ . Edit (number of onto functions): We consider functions $\{1, 2, 3, 4, \ldots,k\} \to \{A, B, C, D\}$ . Let $A$ stand for a set of functions whose image does not contain $a$ . Define $B, C, D$ similarly. Let $N_=(\emptyset)$ be the number of elements in a set of functions whose image does not miss any of $a, b, c, d$ and $N_{\ge}(\emptyset) -$ the number of elements in  set of all functions including $\emptyset.$ Let $N_{\ge}(m_a)$ be the number of elements in a set of functions whose image misses $a$ and also possibly $b, c, d$ . In other words, $N_{\ge}(m_a) = |A|$ or $N_{\ge}(m_a) = |A \cap B|$ or $N_{\ge}(m_a) = |A \cap C \cap D|$ etc. Also, let $AB$ stand for $A \cap B.$ Now take a look at the given pics. Our goal is to count the elements in the square outside the union  of $A, B, C, D$ as shown in pic 1 . To that end start by counting all the elements in the square as shown in pic 2 , then remove the elements in the union of $A, B, C, D$ from the total number of elements. When we remove $A$ , we remove $AB$ with it. Look at pic 3 . When we remove $B$ , we remove $AB$ again. Thus removing $A \cup B$ results in us removing $AB$ twice. Generally, removing $A \cup B \cup C \cup D$ results in us removing $AB, AC, AD, BC, BD, CD$ twice each meaning we need to add one copy of each of $AB, AC, AD, BC, BD, CD$ back as we meant to remove them only once. When we remove $A$ , we remove $ABC$ with it as in pic 4 . Similarly, removing $B$ also results in removal of $ABC$ . This holds for $C$ as well meaning as we remove $A \cup B \cup C$ we also remove $ABC$ three times. But earlier we added $AB, BC, AC, AD$ back which resulted in us adding $ABC$ back four times. So, when  we remove $A \cup B \cup C \cup D$ , one copy of $ABC$ is left behind which we need to get rid of. Similarly we need to remove one copy of each of $ACD, BCD, ABD.$ Now removing $A$ results in removal of $ABCD$ with it as in pic 5 . Similarly, removing $B, C, D$ means we remove $ABCD$ three more times. Earlier, we added $AB, AC, AD, BC, BD, CD$ which added $ABCD$ back six times. Then we removed $ABC, BCD, ACD, ABD$ which removed $ABCD$ four times. Thus removing $A \cup B \cup C \cup D$ results in removing $ABCD$ twice meaning we need to add $ABCD$ back once. Algebraically, $$N_=(\emptyset) = \\ N_{\ge}(\emptyset) \\ - (N_{\ge}(m_a) + N_{\ge}(m_b) + N_{\ge}(m_c) + N_{\ge}(m_d)) \\ + (N_{\ge}(m_am_b) + N_{\ge}(m_am_c) + N_{\ge}(m_am_d) + (N_{\ge}(m_bm_c) + N_{\ge}(m_bm_d) + N_{\ge}(m_cm_d)) \\ - (N_{\ge}(m_am_bm_c) + N_{\ge}(m_am_bm_d) + N_{\ge}(m_bm_cm_d) + N_{\ge}(m_am_bm_d)) \\ + N_{\ge}(m_am_bm_cm_d)$$ Now note, $$N_{\ge}(m_a), \ N_{\ge}(m_b), \ N_{\ge}(m_c), \ N_{\ge}(m_d) \ge (n - 1)^k \\ N_{\ge}(m_am_b), \ N_{\ge}(m_am_c), \ N_{\ge}(m_am_d), \ (N_{\ge}(m_bm_c), \ N_{\ge}(m_bm_d), N_{\ge}(m_cm_d \ge (n - 2)^k \\ N_{\ge}(m_am_bm_c), \ N_{\ge}(m_am_bm_d), \ N_{\ge}(m_bm_cm_d), \ N_{\ge}(m_am_bm_d) \ge (n - 3)^k \\ N_{\ge}(m_am_bm_cm_d) \ge (n - 4)^k \\ N_{\ge}(\emptyset) \ge (n - 0)^k$$ Note, $n = 4$ , but we use either $n$ or $4$ depending on whichever is more instructive at the time. Thus, we have $$N_=(\emptyset) \ge \binom 40(n - 0)^k - \binom 41(n - 1)^k + \binom 42(n - 2)^k - \binom 43(n - 3)^k + \binom 44(n - 4)^k = \sum_{j=0}^4\binom 4j(-1)^j(n - j)^k$$ Finally, $N_= (m_a) = (n - 1)^k$ . However, as we saw above (when working with overlapping circles) removing an object with $N_= (m_a)$ properties results in the removal of an object with $N_{\ge} (m_a)$ properties. Thus we can replace $N_{\ge} (m_a)$ with $N_= (m_a)$ meaning $$N_=(\emptyset) = \sum_{j=0}^4\binom 4j(-1)^j(n - j)^k$$","['summation', 'inclusion-exclusion', 'discrete-mathematics']"
3898156,Can any of sum of squares be realized as a sum of this specific form?,"Let $x,y \in \mathbb{Z}$ , and suppose that $x^2+y^2 \ge 4$ .
Do there exist $a,b,c,d \in \mathbb{Z}$ such that $
(a+d)^2+(b-c)^2=x^2+y^2
$ and $ad-bc=1$ ? This question is motivated by an attempt to characterise the norms of matrices in $SL_2(\mathbb{Z})$ : If $A =\begin{pmatrix} a & b \\\ c & d \end{pmatrix} \in SL_2(\mathbb{Z})$ , then $$
(a+d)^2+(b-c)^2=\|A\|^2+2,
$$ so I wonder whether $\|A\|^2+2$ can be any sum of squares.","['sums-of-squares', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'prime-numbers']"
3898249,Proof explanation of an inverse function theorem,"Let $I\neq\emptyset$ be an open interval and $f:I\to\mathbb{R}$ a continuous and injective function. If $f$ is differentiable at $x_0\in I$ and $f'(x_0)\neq 0$ , then $f^{-1}$ is differentiable at $f'(x_0):=y_0$ and we have that $$ (f^{-1})'(y_0)=\frac{1}{f'(x_0)}=\frac{1}{f'(f^{-1}(y_0))}.$$ Proof: Note that $f(I)$ is an interval, $f:I\to\mathbb{R}$ is an homomorphism, and $f^{-1}:J\to I$ is continuous.
Now, $f$ and $f^{-1}$ are strictly monotonic (by a a lemma we previously proved). Let $(y_n)\in J\setminus\{y_0\}$ be any sequence such that $y_n\to y_0$ , and let $x_n=f^{-1}(y_n)$ . The continuity and injectivity of $f^{-1}$ implies that $(x_n)$ is a sequence in $I\setminus\{x_0\}$ such that $x_n\to x_0$ .
Since $f$ is differentiable at $x_0$ and $f^{-1}(x_0)\neq 0$ , then we have $$\displaystyle\lim_{n\to\infty}\frac{f^{-1}(x_n)-f^{-1}(x_0)}{y_n-y_0}=\displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)}=\frac{1}{f'(x_0)}.$$ What I don't understand is how the continuity and injectivity of $f^{-1}$ implies that $(x_n)$ is a sequence in $I\setminus\{x_0\}$ such that $x_n\to x_0$ , and how to go from $\displaystyle\lim_{n\to\infty}\frac{x_n-x_0}{f(x_n)-f(x_0)}$ to $\frac{1}{f'(x_0)}$ . Any help is welcome","['proof-explanation', 'inverse-function', 'real-analysis', 'calculus', 'derivatives']"
3898295,Exponential Sums,"This is my first time on the Mathematics Stack Exchange. I am currently studying for my IIT entrance exam, and while studying and browsing the web, I came across an interesting problem. Let $a,b,c,d \ge 0$ . Find a closed form for the expression: $$\sum_{a+b+c+d = n} 2^{a+2b+3c+4d}$$ in terms of $n$ . At the bottom, it said that a possible hint might be that I should use generating functions. I've been working for the past couple of hours and can't seem to figure out what the generating function of this expression would be. Any help would be greatly appreciated! Thank you,
Rohit","['summation', 'combinatorics', 'generating-functions']"
3898296,How do I define Injective/Surjective functions in terms of sets and not the elements within them?,I am in a Proof writing class and we are currently on functions. I have a good understanding of what it means to be injective or surjective in terms of elements in the set but I am having trouble coming up with a formal definition that just generalizes the definitions in terms of sets using an iff statement. So if we have a function $f: A\rightarrow B$ Would my statement be considered a proper theorem? If $G\subseteq B$ then $f: A\rightarrow B$ is surjective if and only if $G=B$ and $f(f^{-1}( B)) = B$ . My reasoning here being that if our function is surjective everything in B has to have some pre-image in A. I am not sure If I need to say anything about A. For Injective I came up with a similar theorem: If $H\subseteq A$ then $f: A\rightarrow B$ is injective if and only if $A=H$ and $f(f^{-1}(A))=A$ My reasoning again being we need everything in our domain to have some image in B. So our domain subset must take on the whole set A. Your help and knowledge would be extremely appreciated!,"['elementary-set-theory', 'functions']"
3898344,Eigenvalues and eigenvectors of integral operator,"I am trying  to find the eigenvalues and eigenvectors for the following integral operator in $L^{2}[0,1]$ : $T:H \rightarrow H$ is defined as follows: $$ T[f(x)]= \int_{1-x}^{1} f(t) dt $$ . Here is my procedure until now: Let $g(x)$ be an eigenvector associated with the eigenvalue $\lambda$ , then $$T[g(x)]  = \lambda g(x) $$ . Solving this problem is equivalent to solving the following integral equation $$\int_{1-x}^{1} g(t) dt - \lambda g(x) = 0 $$ Differentiating twice the previous equation, we get $$ g(1-x)=\lambda \frac{dg(x) }{dx}  $$ But i couldn't find any way to solve the last differential equation. Is the previous procedure correct? And is there any way to solve the last equation?","['functional-analysis', 'eigenvalues-eigenvectors']"
3898358,How badly are distances distorted by mapping a rectangle to a line?,"Purely out of curiousity, I was thinking about the following question: Let $[n]$ be the set $\{0,1,2,\ldots,n-1\}$ and consider the cartesian product $[n]\times [m]$ . Given any bijection $f:[n]\times[m]\rightarrow [nm]$ , let's define its distortion as the greatest difference $|f(x_1)-f(x_2)|$ in its evaluation between two adjacent elements $x_1$ and $x_2$ of $[n]\times[m]$ where ""adjacency"" is considered that they agree in one coordinate and differ by exactly $1$ in the other coordinate. What is the minimum possible distortion of any bijection? One might consider this as something about minimizing the Lipschitz constant of a bijection between a discrete line segment and a lattice rectangle under the taxicab metric. One can constrain the answer fairly well: for a lower bound to distortion, observe that some element $x_1$ of $[n]\times [m]$ must have $f(x_1)=0$ and some $x_2$ must have $f(x_2)=nm-1$ . There has to be some path of at most $n+m-2$ points in $[n]\times [m]$ , each adjacent to the prior, connecting $x_1$ and $x_2$ . Thus, some difference along this path must be at least $\frac{nm-1}{n+m-2}$ . On the other side, one can consider the explicit bijection $f(a,b)=ma+b$ or $g(a,b)=a+nb$ to get that some bijection has distortion at most $\min(n,m)$ . There are other maps that attain this bound (e.g. you can adapt Cantor's pairing function to this finite domain to get this bound - essentially by ordering the points of the rectangle by diagonals and the filling each diagonal in in a consistent order). My guess is that this bound is optimal, although I lack a good argument to prove it. There's roughly a ratio of $2$ between the two bounds I give here in the worst case. As far as exact results, this the distortion is clearly at least $1$ for any map $[1]\times [n]\rightarrow [n]$ where $n\geq 1$ and at least $2$ for any map $[2]\times [n]\rightarrow [2n]$ where $n\geq 2$ simply due to the pigeonhole principle. One can work out that all maps $[3]\times [3]\rightarrow [9]$ (and by extension $[3]\times [n]\rightarrow [3n]$ for $n\geq 3$ ) have distortion at least $3$ by extending the lower bound argument a touch. I checked with computer that $[4]\times [4]\rightarrow [16]$ maps have distortion at least $4$ and $[5]\times [5]\rightarrow [25]$ maps have distortion at least $5$ . I told my computer to try working on the $[6]\times[6]$ case but I'm not optimistic that it will survive the combinatorial explosion.","['discrete-optimization', 'optimization', 'metric-spaces', 'discrete-mathematics']"
3898364,Integral of a two dimensional function,"A function is defined by $$f(x,y) = \int_{x^2}^{xy} e^{t^2} \,dt$$ We are to decide the partial derivatives of the function. I am quite unsure whether this is a trick question because we could do an approximation of the integrand with the Taylor series, but I can't see how that would simplify the work. Could I just get a direction to as how I should approach this problem? I would very much not like to just get the solution. Note that this is one of the easiest questions on the paper.",['multivariable-calculus']
3898402,"For $X ∼ Pois(λ)$, find $E(2^X)$, if it is finite.","For $X \sim Pois(λ)$ , find $\Bbb E(2^X)$ , if it is finite. Hint: Use these facts, $$\begin{align}\sum_{k=0}^{\infty}\frac{\lambda^k}{k!}&=e^\lambda\\\sum_{k=0}^{\infty}\frac{(k+1)\lambda^k}{k!}&=e^\lambda+\lambda e^\lambda\end{align}$$ The lecture ended before we had time to cover this section and I have no notes to work from. I am having trouble getting to the answer because I am getting confused with my work. Any help would be much appreciated. I have this so far but I am not sure if it is correct, $P(X = x) = \frac{e^{-\lambda}\lambda^x}{x!}$ for $x \in\{ 0, 1, 2, \ldots\}$ $$\begin{align}\mathbb E(2^X) &= \sum_{x=0}^{\infty} 2^x \frac{e^{-\lambda}\lambda^x}{x!}\\&= e^{-\lambda} \mathbb\sum_{x=0}^{\infty} \frac{(2\lambda)^x}{x!}\\&= e^{-\lambda} e^{2\lambda}\end{align}$$ So then $\Bbb E (2^X) = e^\lambda$","['expected-value', 'poisson-distribution', 'probability']"
3898554,"Given a particular matrix $A \in \operatorname{GL}_n(F)$, what is an easy way to determine $A^n$ $\forall n \in \Bbb Z$?","$
\newcommand{\GL}{\operatorname{GL}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\m}[1]{\left( \begin{matrix} #1 \end{matrix} \right)}
\newcommand{\ds}{\displaystyle}
$ Original Problem: Consider the matrix $A \in \GL_3(\R)$ given by, for $x > 0$ , $$A = \m{-1 & x & -x \\ 1 & -1 & 0 \\ 1 & 0 & -1 }$$ What is the general form of $A^n$ for any $n \in \Z$ ? My Work So Far: By doing some particular cases by hand and doing some pattern matching, I was able to observe that $$A^n= (-1)^n \m{ 1 & -nx & nx \\
-n &\ds \frac{n(n-1)}{2} x + 1 &\ds  -\frac{n(n-1)}{2} x \\
-n & \ds \frac{n(n-1)}{2} x &\ds 1 - \frac{n(n-1)}{2} x }$$ Proving this through a sort of ""bidirectional induction"" (i.e. show it with $A$ as the base case for $n \in \Z_{>0}$ , then with $A^{-1}$ as the base case for $\Z_{<0}$ , and then $A^0 = I_3$ trivially) seems more than doable, if a little tedious. However, I don't think that this is the way it was intended to be solved. This comes as a homework problem and was tied to some others I was given. Namely: Find the characteristic polynomial $\mu_A$ of $A$ Deduce $A$ is not diagonalizable Find vectors $U_i \in \R^3$ such that $$AU_1 = -U_1 \qquad AU_2 = U_1 - U_2 \qquad AU_3 = U_1 + U_2 - U_3$$ Show $A$ is similar to the matrix $$B = \m{ -1 & 1 & 1 \\ 0 & -1 & 1 \\ 0 & 0 & -1}$$ (I did so by showing $\mu_A = \mu_B$ .) I've done all of these, so I don't really need help there. It would not be unusual for these to tie into the problem somehow, based on how this professor's homework tends to go, but I'm not sure where the connection lies. So I'm left wondering: based on this information, what is some more elegant way to calculate $A^n$ or prove its general form as given, if any? Be it for this specific matrix tied to the information given, or a more general case of $A \in \GL_n(F)$ .","['matrices', 'exponentiation', 'linear-algebra']"
3898624,Finding $\lim_{\varepsilon\to 0}\int_{\varepsilon}^{\frac{\varepsilon}{1+\varepsilon}}\frac{e^{-x^2}}{x^2}dx$,"$$\lim_{\varepsilon\to 0}\int_{\varepsilon}^{\frac{\varepsilon}{1+\varepsilon}}\frac{e^{-x^2}}{x^2}dx$$ Okay this seems weird to me-- couldn't i just write down the answer 0 because $\int_{0}^{0} f(x)dx=0$ ? (The answer is $-1$ btw)
There was even a hint for the question: $\displaystyle \lim_{x\to0}\frac{e^{-x^2}-1}{x^2}$ may be useful. Am I supposed to go $$\displaystyle \int_{\varepsilon}^{0}\frac{e^{x^2}}{x^2}dx+\int_{0}^{\frac{\varepsilon}{1+\varepsilon}}\frac{e^{x^2}}{x^2}dx\,\,\,?$$ And I couldn't even go L'Hospital with that.....
Any help would be appreciated ! Thanks! Of course, it would be nice if the answer was simply 0!","['limits', 'calculus']"
3898636,"The U.S. Senate consists of $100$ senators, with $2$ from each of the $50$ states","The U.S. Senate consists of $100$ senators, with $2$ from each of the $50$ states. There are $50$ Democrats in the Senate. A committee of size $10$ is formed, by picking a random set of senators such that all sets of size $10$ are equally likely. a) Find the expected number of Democrats on the committee. b) Find the expected number of states represented on the committee (by at least one senator). c) Find the expected number of states such that both of the state’s senators are on the committee. For part a), I defined $D$ as a random variable for democrats, got the Hypergeometric Distribution to be $D$ ~ $(100, c, d),$ and the expectation to be $E(D) = c(\frac{d}{100}) = \frac{cd}{100}$ . Then I plugged in $10$ for $c$ and $50$ for $d$ and got $E(D) = 5.$ For part b), I defined $I_j$ as the random variable for the j-th state represented/ not represented in the committee, got $P(I_j = 1) = 1 - P(I_j = 0) = 1 - \frac{\binom{98}{c}}{\binom{100}{c}} = 1 - \frac{(100 - c)(99 - c)}{(100)(99)}$ , and finally $E(\sum_{j} I_j) = \sum_{j}E(I_j) = \sum_{j}P(I_j = 1) = 50(1 - \frac{(100 - c)(99 - c)}{(100)(99)})$ . Then I plugged in $10$ for $c$ and got $P(I_j = 1) = 1 - \frac{8010}{9900}$ = $\frac{21}{110}$ and $E(\sum_{j} I_j) = 50(1 - \frac{(100 - 10)(99 - 10)}{(100)(99)})$ = $50(\frac{21}{110})$ = $\frac{105}{11} $ For part c), I defined $K_j$ as the random variable for both being from/ not from j-th state in the committee, got $P(K_j = 1) = \frac{\binom{98}{c}}{\binom{100}{c}} = \frac{(100 - c)(99 - c)}{(100)(99)}$ and finally $E(\sum_{j} K_j) = \sum_{j}E(K_j) = \sum_{j}P(K_j = 1) = 50( \frac{(100 - c)(99 - c)}{(100)(99)})$ Then I plugged in $10$ for $c$ and got $P(K_j = 1) = \frac{8010}{9900}$ = $\frac{89}{110}$ and $E(\sum_{j} K_j) = 50(\frac{(100 - 10)(99 - 10)}{(100)(99)})$ = $50(\frac{89}{110})$ = $\frac{445}{11} $ . However, I am not sure if my answers are correct. Any help would be much appreciated!",['probability']
3898663,Find all natural $n$ numbers such that,"Find all natural $n$ numbers such that $15(n!)^2+1$ is divisible by $2n-3$ .
My try: First I assumed $2n-3$ is not prime number. Let $a$ be divisor of $2n-3$ . It's clear that $a<n-1$ , so $15(n!)^2$ is divisible by $a$ . Which means $15(n!)^2+1$ is not divisble by $a$ . But it is given that $15(n!)^2+1$ is divisble by $n$ which means it is divisible by $a$ too. But we have already proved it is not. Contradiction!
So $2n-3$ must be prime number. Now if we change $2n-3$ as $p$ . We can say $15*((p+3)/2)!*((p+3)/2)!$ is congruent to $-1$ by p module. By Wilson's theorem $15*((p+3)/2)!*((p+3)/2)!$ is congruent to $(p-1)!$ by p module. From here I don't know how to continue.","['number-theory', 'divisibility']"
3898670,In $\mathbb{R}$ find the solutions of the equation $x^4-x^3-18x^2+3x+9=0$,"I couldn't solve this question and hence looked at the solution which goes as follows: $0$ is no a root of the equation. Hence: $$x^2-x-18+\frac{3}{x}+\frac{9}{x^2}=0. \text{ So,  } 
x^2+\frac{9}{x^2}-(x-\frac{3}{x})-18=0$$ I state that $y=x-\frac{3}{x}$ , so we have that $y^2=x^2+\frac{9}{x^2}-6$ , in other words $x^2+\frac{9}{x^2}=y^2+6$ . Hence the equation is written as $y^2+6-y-18=0$ so $y^2-y-12=0$ , hence $y=4$ or $y=-3$ so $x=2\pm\sqrt{7}$ , or $x=\frac{-3\pm\sqrt{21}}{2}$ Could you please explain to me why the solution author of the solution thought of originally dividing by the equation by x and after that substituting $x-\frac{3}{x}$ with $y$ ? Also if you can think of a more intuitive approach could you please show it?","['contest-math', 'roots', 'linear-algebra', 'intuition', 'problem-solving']"
3898775,Number of non-intersecting paths for two bodies on a Cartesian grid,"Each body is allowed a jump of $+1 \uparrow$ or $+1 \rightarrow$ in one step. There are $n$ steps available to each body. Find the number of non-intersecting path pairs between a fixed source and a fixed destination within this grid. Assume final destination is $X,Y$ and of course, $X + Y = n$ . My approach Let $\Delta X_1, \Delta Y_1$ and $\Delta X_2, \Delta Y_2$ be the displacement vectors for body 1 and body 2 respectively. Let $Array(\Delta X_1)$ denote the array containing the values of $\Delta X_1$ indexed by step, and so on for other vectors too. Note that there is a bijection between $Array(\Delta X_1)$ and $Array(\Delta Y_1)$ , because a body can jump only along one axis at a time. So we have to consider only $Array(\Delta X_1)$ and $Array(\Delta X_2)$ to find the number of paths. The paths intersect if at any particular step, $\Delta X_1 = \Delta X_2$ , which also implies $\Delta Y_1 = \Delta Y_2$ from the bijection (so ideally, this should happen only at the source and the destination). $Array(\Delta X_1)$ is $n$ sized and monotonic increasing over its length and $Set(Array(\Delta X_1))$ is $\mathbb{N}_{\leq X}$ . An example of a pair of non-intersecting paths for source $(0,0)$ and destination $(4,4)$ is- Body 1 - $Array(\Delta X_1) = \{ 0,0,0,1,1,2,3,4\}$ and $Array(\Delta Y_1) = \{ 1,2,3,3,4,4,4,4\}$ Body 2 - $Array(\Delta X_2) = \{ 1,1,2,3,3,4,4,4\}$ and $Array(\Delta Y_2) = \{ 0,1,1,1,2,2,3,4\}$ This scenario can be condensed to a series of $\rightarrow$ and $0$ (we need to consider only one of the $x$ and $y$ displacement vectors, as the latter can be derived uniquely from the former). If the displacement is $+1$ , place a $\rightarrow$ on the final coordinate, else $0$ . So - Body 1 - $Array(\Delta X_1) = \{ 0,0,0,\rightarrow,0,\rightarrow,\rightarrow,\rightarrow\}$ and Body 2 - $Array(\Delta X_2) = \{ \rightarrow,0,\rightarrow,\rightarrow,0,\rightarrow,0,0\}$ . So basically find the number of pairs of such binary strings where the initial substring of Body 1 $Array(\Delta X_1)$ always has lesser $\rightarrow$ than its counterpart's corresponding inital substring. Is there a better approach? Hints please.","['coordinate-systems', 'combinatorics', 'discrete-mathematics']"
3898882,Find a general solution,"$$(y+e^x)\frac{dy}{dx}=-\frac{y^2}{2}-2ye^x$$ I attempted to turn this inexact eq. to exact: I multiply by IF = $e^t$ and got: $$(ye^x+e^{2x})dy+(\frac{y^2}{2}e^x+2ye^{2x})dx = 0$$ which is exact. Then I integrate, and I get $y^2e^x+2ye^{2x} + c = 0$ . My question is if this is considerate a general solution? or does this expression need to be expressed in terms of y? If so, how can I do this? Thanks",['ordinary-differential-equations']
3898911,Find the limit of the series $6^n/n!$ as $n$ tends to infinity.,"I need to find the limit of the following serie: $\lim_\limits{n\to \infty}$$\frac{6^n}{n!}$ I was thinking of the following solution, but i'm not sure it's correct, please let me know your thoughts :) $0$ $<$ $\frac{6^n}{n!}$ $<$ $\frac{6}{1}\cdot$$\frac{6}{2}\cdot$ $\frac{6}{3}\cdot$$\frac{6}{4}\cdot$$\frac{6}{5}\cdot$$\frac{6}{6}\cdot$ $1\cdot$ $1\cdot$ $1\cdot$ .... $\cdot1\cdot$ $\frac{6}{n}$ $=$ $\frac{1944}{5n}$ $\lim_\limits{n\to \infty}$$0$ $= 0$ $\lim_\limits{n\to \infty}$$\frac{1944}{5n}$ $= 0$ Hence, by using the sandwich theorem, $\lim_\limits{n\to \infty}$$\frac{6^n}{n!}$ $= 0$ .","['limits', 'calculus', 'convergence-divergence', 'sequences-and-series']"
3898912,Working with functions using modulo operator,"I have a function defined in a computer program that I would like to study mathematically: $y = \frac{x}{2} + (\frac{5}{2}x + 1)*(x\%2)$ This function is used on strictly positive integers and also returns integers only. The main issue is that it's using the modulo operator ( $\%$ ) as part of the function calculation, not as in a modular equation, and I have no clue how to translate into a formula that is suitable for mathematical analysis... I have found various topic related to modulo/floor and how to translate them into mathematical form: Solving equations involving modulo operator Solving modular equations with a variable in the modulus divisor How to represent the floor function using mathematical notation? But none of them seems to match this particular situation. Also, to clarify what I mean by ""studiy mathematically"", it would be for example to determine the range of output values if input values are taken in a given range, or to determine limits/periodicity if this function was called recursively.","['algebra-precalculus', 'discrete-mathematics']"
3898922,"Can the Stolz-Cesàro theorem be used to prove L'Hospital's rule, or viceversa?",Stolz-Cesàro theorem is usually stated to be the discrete version of L'Hospital's rule. I was merely wondering whether one of these theorems could be used to prove the other (I couldn't find any proof that does this online).,"['proof-writing', 'alternative-proof', 'sequences-and-series', 'limits', 'derivatives']"
3898925,For any two sets how to prove: (1) $A ⊆ B$ (2) $A ∪ B = B$ (3) $A − B = B − A$,I'm trying to prove these statements are equivalent and I'm stuck at proving the second one. I started by proving that $A \cup B \subseteq B$ . $x \in (A \cup B)  $ then $ x \in B$ $x \in A$ or $x \in B$ then $ x \in B$ $ x \in A $ then $x \in B$ is true from the first assumption $x \in B$ then $x \in B$ is true because B is a subset of itself This was in the first direction. ( $A \cup B \subseteq B$ ) But when I try to do it in the other direction I get this. $B \subseteq A \cup B$ $x \in B$ then $x \in (A \cup B)$ $x \in B$ then $x \in A$ or $x \in B$ I can prove that $x \in B$ then $x \in B$ again but how do I prove $x \in B$ then $x \in A$ ? ( $B \subseteq A$ ),"['elementary-set-theory', 'proof-writing', 'discrete-mathematics']"
3898979,Topologist's sine curve is a simply-connected space,"I am trying to solve the following problem from Hatcher's Algebraic Topology and have written a solution. Could you help me checking my solution, whether I am right? Thanks in advance. $Y$ is simply-connected: Let $$A=\bigg\{\bigg(x,\sin \frac{1}{x}\bigg):0<x\leq 1\bigg\}\text{ and }X=\big\{(0,y):-1\leq y\leq 1\big\}\bigcup A.$$ and $C$ be a simple curve in $\Bbb R^2$ such that $C\cap X=\big\{(0,0),\big(1,\sin 1\big)\big\}$ . Define $Y=X\cup C$ . Now, for each $1\geq \delta>0$ let $$A_\delta=\bigg\{\bigg(x,\sin \frac{1}{x}\bigg):\delta<x\leq 1\bigg\}\text{ and }X_\delta=A_\delta\bigcup\big\{(0,y):-1\leq y\leq 1\big\} ,$$ $$Y_\delta:=X_\delta\cup C.$$ Note that $$Y=\bigcup_{0<\delta\leq 1}Y_\delta.$$ Note that $Y$ is path-connected, we show $\pi_1\big(Y,(0,0)\big)=0$ . Let $\alpha=(\alpha_1,\alpha_2)$ be a loop in $Y$ based at $(0,0)$ . If possible let for each $n\in \Bbb N$ we have $$\text{Image}(\alpha)\cap B_{n}\not=\emptyset\text{ where }B_n:=\bigg\{\bigg(x,\sin \frac{1}{x}\bigg):0<x<\frac{1}{n}\bigg\}.$$ Take, $\alpha(t_n)=(x_n,y_n)\in \text{Image}(\alpha)\cap B_{n}$ for each $n\in\Bbb N$ , so $\alpha_1(t_n)=x_n\to 0$ . Since $[0,1]$ is compact passing to the subsequence we may assume $t_n\to c\in [0,1]$ . So, $\alpha_1(c)=0$ . Also, $-1\leq \alpha_2(c)\leq 1$ as $X$ is closed. Since, $\alpha_1(t_n)=x_n\to 0$ by intermediate value theorem $\alpha_1$ takes all values $\frac{1}{n\pi}$ for all large $n\in\Bbb N$ . Suppose $\frac{1}{k\pi}\in \text{Image}(\alpha_1)$ for all $k\in\Bbb N$ with $k\geq k_0$ , where $k_0\in\Bbb N$ . Let $\alpha_1(t_m)=x_m<\frac{1}{k\pi}<\alpha_1(t_n)=x_n$ for some $m,n\in\Bbb N$ . So, by intermediate value theorem we have $t_m<t_k^*<t_n$ such that $\alpha_1(t_k^*)=\frac{1}{k\pi}$ . Hence, $t_k^*\to c$ as $k\to \infty$ . Now, $\alpha_2(t_k^*)\in\{\pm 1\}$ for all $k\geq k_0$ . So, there is no neighbourhood of $c$ mapped into $\big\{x\in\Bbb R:\alpha_2(c)-\varepsilon<x<\alpha_2(c)+\varepsilon\big\}$ , where $\varepsilon>0$ is so chosen that $\big\{x\in\Bbb R:\alpha_2(c)-\varepsilon<x<\alpha_2(c)+\varepsilon\big\}$ contains exactly one element from $\{\pm 1\}$ , as each nbd of $c$ contains some $t_k^*$ . And this is a contradiction to the continuity of $\alpha_2$ . So, for large $n$ we have $\text{Image}(\alpha)\cap B_n=\emptyset.$ That's, $\text{Image}(\alpha)\subseteq Y_{\delta}$ for some $\delta>0$ . Now, the space $Y_{\delta}$ is homeomorphic to the space $\big\{(x,0):-1< x\leq 0\big\}\cup\big\{(0,y):-1\leq y\leq 1\big\}.$ So, $Y_{\delta}$ is contractible. Hence, $\pi_1\big(Y_{\delta},(0,0)\big)=0$ . In other words, there is a homotopy $H:[0,1]\times [0,1]\to Y_{\delta}$ such that $H:\alpha\simeq_{\text{rel }(0,0)}C_{(0,0)}$ , So, extending the co-domain we have a homotopy $H:[0,1]\times [0,1]\to Y$ such that $H:\alpha\simeq_{\text{rel }(0,0)}C_{(0,0)}$ . So, $\pi_1\big(Y,(0,0)\big)=0$ . Now, $$\Bbb S^1\cong\frac{C}{\big\{(0,0),(1,\sin 1)\big\}}\cong\frac{Y}{X}$$ considering inclusion map $i:C\hookrightarrow Y$ . So, we have a quotient map $f:Y\to \frac{Y}{X}=\Bbb S^1$ . Let $p:\Bbb R\ni t\longmapsto e^{2\pi it}\in \Bbb S^1$ be the universal cover. No Lifting: Now, suppose we have continuous lifting as in the picture below. Now, $p^{-1}(1)=p^{-1}\big([X]\big)$ is a discrete set, so continuity of $\widetilde f$ implies $\widetilde f(0,y)=0$ for $-1\leq y\leq 1$ as $f(0,y)=[X]=1$ for $-1\leq y\leq 1$ . Similarly, $\widetilde f$ is identically $0$ on $\big\{\big(x,\sin \frac{1}{x}\big):0<x\leq 1\big\}\subseteq X$ using continuity. Note that $f$ is surjective, so image of $\widetilde f$ contains either of the two intervals $(-1,0)\subseteq \Bbb R$ or $(0,1)\subseteq \Bbb R$ . Without loss of generality assume, the latter one. Next, for each point $z\in C\backslash \big\{(0,0),(1,\sin 1)\big\}$ the equivalence class $[z]\in \frac{Y}{X}$ is a singleton set and $p^{-1}\big([z]\big)\cap \{x\in\Bbb R:0<x<1\}$ is singleton. So, if $C \backslash \big\{(0,0),(1,\sin 1)\big\}\ni z\longrightarrow (0,0)$ , then $\widetilde f(z)=p^{-1}\big([z]\big)\cap \{x\in\Bbb R:0<x<1\}\longrightarrow 0\in\Bbb R$ and if $C \backslash \big\{(0,0),(1,\sin 1)\big\}\ni z\longrightarrow (1,\sin 1)$ , then $\widetilde f(z)=p^{-1}\big([z]\big)\cap \{x\in\Bbb R:0<x<1\}\longrightarrow 1\in\Bbb R$ , a contradiction as $\widetilde f$ is identically $0$ on $\big\{\big(x,\sin \frac{1}{x}\big):0<x\leq 1\big\}$ .","['fundamental-groups', 'solution-verification', 'covering-spaces', 'general-topology', 'algebraic-topology']"
3898996,Proving covering relation for a product of ordered sets,"Prove that if L and M are two ordered sets, then $(a_2, b_2)$ covers $(a_1, b_1)$ in $L × M$ iff $(a_1 = a_2$ and $b_2$ cover $b_1$ ) or ( $a_2$ cover $a_1$ and $b_1 = b_2$ ).","['order-theory', 'discrete-mathematics', 'covering-spaces']"
3899020,"In $\Delta ABC$ , $\angle A = 20^\circ, \angle C = 90^\circ$. $O$ is a point on $AB$ and $D$ is the midpoint of $OB$ . Find $\angle BCD$ .","In $\Delta ABC$ , $\angle A = 20^\circ, \angle C = 90^\circ$ . $O$ is a  point on $AB$ and $D$ is the midpoint of $OB$ . The circle centered at $O$ with radius $OD$ intersects $AC$ at $T$ . Find $\angle BCD$ . What I Tried : Here is a picture :- You can see what I tried. The first part is to do angle-chasing and find the respective angles in the picture. I also find that $OT \parallel BC$ , since $OBCT$ is a trapezium . Now I think that to find $\angle BCD$ , somehow I have to use the fact that $OD = DB$ (which I have not used) and maybe the parallel lines, but I seem to be stuck here. Can anyone help me figure this out? Thank You.","['problem-solving', 'triangles', 'circles', 'geometry']"
3899036,Conditional Probabilities defective items,"A company sends 30% of its product to Client A and 70% to Client B. Client A reports that 5% of the products it received are defective, whereas Client B reports that 4% of products received are defective. The defective products are returned back to the company. What is the probability of a returned defective product coming from client B? probability of an item being sent to customer A  = $P(C_a) =  0.30$ & $P(D|C_a) = 0.05$ probability of an item being sent to customer B  = $P(C_b) =  0.70$ & $P(D|C_b) = 0.04$ $P$ (Returned) = $P$ (Defective)= $P(D)$ = $0.3 \times 0.05 + 0.7\times 0.04 = 0.043$ My thinking is that I need to find $P$ ( $C_b$ | Defective ) $= P( C_b | D )$ $P( C_b | D ) = \frac{P(D \,\cap \,C_b)}{P(D)} = \frac{P(D \,\cap \,C_b)}{0.043}\;$ so I need to find $P(D \,\cap \,C_b)$ $P(D|C_b) = \frac{P(D \,\cap \,C_b)}{P(C_b)}\;$ so $0.04 = \frac{P(D \,\cap \,C_b)}{0.70}\;$ which implies $P(D \,\cap \,C_b) = 0.028$ $P( C_b | D ) = \frac{0.028}{0.043} = \frac{28}{43}$ however I feel my answer may be wrong. Am i on the right track?","['conditional-probability', 'statistics', 'probability']"
3899042,Is $H$ a subgroup of $\Gamma$?,"The question is in the picture, I understand the requirements for a subset to be a subgroup (closure, associativity under the same operation etc.), however I am struggling with this question. I would be grateful if someone could walk me through this. Thanks in advance. 1.3. Let $\Gamma$ denote the set of all mappings $f:\mathbb R\to\mathbb R$ such that $f(x)\ne 0$ for all $x\in\mathbb R$ . For $f,g\in\Gamma$ , define $fg$ by $$fg(x)=f(x)g(x)$$ for all $x\in\mathbb R$ . You may assume that the set $\Gamma$ forms a group with respect to the above composition. Let $H$ be the subset of $\Gamma$ defined to be the set of all mappings $f:\mathbb R\to\mathbb Z\setminus \{0\}$ . Is $H$ a subgroup of $\Gamma$ ? Remember to justify your answer fully.","['normal-subgroups', 'group-theory', 'abstract-algebra']"
3899064,Homeomorphism between $\mathbb{R}^2$ and the open unit disc,"Given the function $f(x,y)=(\frac{x}{1+\sqrt{x^2+y^2}},\frac{y}{1+\sqrt{x^2+y^2}})$ , I have to prove $f$ is a homemorphism between $\mathbb{R}^2$ and the open unit disc. Proving this function is conitunuous is trivial, proving it's a bijection is a bit harder (I wasn't able to find it's inverse). I did find the same function in a different notation - $f(z)=\frac{z}{1+\|z\|}$ where $z\in\mathbb{R}^2$ , but since it's my first time dealing with two variable functions and homemorphisms, I can't see why $f(z)=f(x,y)$ , other than $\|z\|=\|(x,y)\|$ .
What is the best way to approach this with given notation ( $f(x,y)$ ) - more generally how can I prove this function is a bijection and find its inverse. (I was able to show that $lim_{(x,y)\rightarrow(\infty,\infty)}f(x,y)$ is (1,1) which I guess shows its a surjection but otherwise had no idea).","['multivariable-calculus', 'general-topology']"
3899119,"if $a,b,c,d$ be the first positive solutions of $\sin x=\frac{1}{4}$ then...","if $a,b,c,d$ be the first positive solutions of $\sin x=\frac{1}{4}$ then find $$\sin (d/2)+2\sin (c/2)+3\sin (b/2)+4\sin (a/2)$$ . $(a<b<c<d)$ Attempts $$\sin x=1/4$$ , $$4\sin^2(x/2)(1-\sin^2(x/2))=\frac{1}{16}$$ Let $\sin^2 (x/2)=t$ :it remains to solve $$t-t^2=\frac{1}{64}$$ .The rest is just brute force i.e we calculate  the 4 values $\sin(x/2)$ can take , make sure the angles are arranged in ascending order and substitute the roots.As far as i can see i a getting irrational roots. My question is does anyone spot a trick, that can avoid all this work.!","['trigonometry', 'roots', 'polynomials']"
3899126,Transportation-information inequalities (concentration inequalities),"The following is one example of a transportation-information inequality,  which show connections between optimal transport theory and information theory : $$W_1(\nu,\mu) \leq \left[ 2\sigma^2 D_{KL}(\nu\Vert\mu) \right]^\frac{1}{2} $$ $W_1$ is the Wasserstein distance found in optimal transport theory, and $D_{KL}$ is the Kullback-Leibler (KL) divergence found in information theory. ( Source ) What other transportation-information inequalities are out there?","['statistics', 'concentration-of-measure', 'probability-distributions', 'optimal-transport', 'information-theory']"
3899139,Finding solution of the PDE : An Initial value problem,"I've been given a PDE of the form $$xu_x+(x^2+y)u_y=1-\left(\frac{y}{x}-x\right)u~~; ~~u(1,y)=0$$ Attempt : Firstly it's a first order linear PDE which has the general form $$a(x,y)u_x+b(x,y)u_y=c(x,y)u+d(x,y)$$ Where $a,b,c,d \in \mathcal{C}^1(\Omega)$ , where $\Omega \subseteq \mathbb{R}^2$ (open and connected) and we also have $a^2+b^2 \neq 0$ on $\Omega_2$ . Then first of all the largest $\Omega_2$ on which $F:=a^2+b^2=x^2+(x^2+y)\neq 0$ is : $(x,y) \in \Omega_2\setminus \{(0,0)\}$ right? We have $s \mapsto \Gamma_s:=(f(s_0):=1,g(s_0):=s,h(s_0):=0))$ . Now, from transversality conditions we get : $$J:=\mathrm{det}\, \begin{pmatrix}a&f'\\b&g'\end{pmatrix}\Bigg{|}_{f(s_0),g(s_0),h(s_0)}=\mathrm{det}\begin{pmatrix}1&0\\1+s&1\end{pmatrix}=1$$ So we atleast expect solutions to exist $\textit{locally}$ for all $s \in (-\delta,\delta),\delta>0$ .
Now to find the solutions explicitly i'll employ the method of characteristics: \begin{align}\frac{\mathrm{d}x}{\mathrm{d}t}&=x~~;~x(0,s):=1 \implies x(t,s):=e^t\\\frac{\mathrm{d}y}{\mathrm{d}t}&=x^2+y~~;~ y(0,s):=s \\&=y+e^{2t} \end{align} Solving this we get \begin{align}   y(t,s)&=e^{2t}+c_2e^{t}~~;~y(0,s):=s \implies 
y(t,s)=e^{t}\left(e^t+s-1\right)\end{align} Lastly \begin{align}\frac{\mathrm{d}z}{\mathrm{d}t}&=1-\left(\frac{y}{x}-x\right)z~~;~z(0,s):=0 \\&=1-\left(\frac{e^t(e^t+s-1)}{e^t}-e^t\right)z=1+(1-s)z\end{align} Which gives me $$z(t,s)=\frac{1}{s-1}\left(1-e^{t(1-s)}\right)$$ As the Jacobian was non-zero by inverse function theorem i can expect that i can invert the map $$(t,s)\mapsto \left(x(t,s),y(t,s)\right)$$ and i can solve $x=x(t,s),y=y(t,s)$ uniquely for $t$ and $s$ and the map $$(x,y)\mapsto \left(T(x,y);S(x,y)\right)$$ should be of class $\mathcal{C}^1$ . So i have $x(t,s)=e^t \implies t=\ln\,x=:T(x,y)$ and $y(t,s)=e^{\ln\,x}\left(e^{\ln \,x}+s-1\right)=x(x+s-1)$ i.e $s=(y/x)-x+1=:S(x,y)$ . From here i can write $$u=z(T(x,y),S(x,y))=\frac{x}{y-x^2}\left(1-x^{\frac{x^2-y}{x}}\right)$$ Which is valid when $y>x^2$ . I'm not sure about my workings here, appreciate any hints and help. Thanks.","['partial-differential-equations', 'solution-verification', 'real-analysis']"
3899191,Limits involving nested roots,"Let $x>0$ . Let $a_1=\sqrt{x}, a_{n+1}=\sqrt{x+a_n}$ . So $a_2=\sqrt{x+\sqrt{x}}, a_3=\sqrt{x+\sqrt{x+\sqrt{x}}}$ and so on. Let $t:=\frac{1+\sqrt{1+4x}}2$ . It's easy to show that: $$(1) \lim_{n \rightarrow \infty} a_n = t$$ $$(2) \lim_{n \rightarrow \infty} \frac{t-a_n}{t-a_{n+1}}=2t$$ So one might try calculating: $$f(x):=\lim_{n \rightarrow \infty} (2t)^n (t-a_n)$$ I took interest in this kind of limits since $f(2)=\frac{\pi^2}4$ via Viète's formula. I was able to approximate some other limits, but found nothing meaningful so far. Has this function been considered before? Do you have any hints on how to analyze it? EDIT: It may be helpful to look at $f(x)$ as a result of a single recurrency: $$b_1=(2t)(t-\sqrt{x})$$ $$b_{n+1}=(2t)^{n+1}(t-\sqrt{x+t-\frac{b_n}{(2t)^n}})$$ $$f(x)=\lim_{n \rightarrow \infty } b_n$$ EDIT2: Please note that $(t+\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n+1}=(2t)b_n$ . Since $\sqrt{x+t-\frac{b_n}{(2t)^n}} < t$ , we know that $b_{n+1}>b_n$ . Furthermore, $(t+\sqrt{x+t-\frac{b_n}{(2t)^n}})(b_{n+1}-b_n)=(2t)b_n-(t+\sqrt{x+t-\frac{b_n}{(2t)^n}}) b_n=(t-\sqrt{x+t-\frac{b_n}{(2t)^n}})b_{n}=\frac{b_nb_{n+1}}{(2t)^{n+1}}$ This can be weakened to: $b_{n+1}-b_n \leq \frac{f^2(x)}{(2t)^{n+2}}$ Which means that: $$f(x) \leq b_1 + \sum_{i=1}^n \frac{f^2(x)}{(2t)^{n+2}}=b_1+\frac{f^2(x)}{4t^2-2t}$$ EDIT3: The determinant of this inequality is: $$\Delta:=1-4\frac{b_1}{4t^2-2t}=1-4\frac{2t(t-\sqrt{x})}{4t^2-2t}=1-4\frac{t-\sqrt{x}}{2t-1}$$ $\Delta \leq 0$ for $x \leq \frac{23+8\sqrt{7}}{36}$ . For $\Delta >0$ , this gives us two options: $f(x) \geq (4x+2t)(1+\Delta)$ $f(x) \leq (4x+2t)(1-\Delta)$ We already know that $f(2)=\frac{\pi^2}{4}$ -- this satisfies the second inequality. Since $f(x)$ is quite clearly continuous, we know that: $$f(x) \leq (2x+t)(1-\Delta) \text{  for  } x\geq\frac{23+8\sqrt{7}}{36}$$ $\lim_{x \rightarrow \infty} \frac{b_1}{\sqrt{x}} = \lim_{x \rightarrow \infty} (2x+t)(1-\Delta) = 1$ , so $\lim_{x \rightarrow \infty}\frac{f(x)}{\sqrt{x}}=1$ .","['sequences-and-series', 'real-analysis']"
3899246,A question on the construction of the Lebesgue integral in Rudin,"In Rudin's Real and Complex Analysis (page 19, definition 1.23) we take a measurable function $f:X\rightarrow [0,\infty]$ in which $(X,\mathcal M,\mu)$ is a measure space with $\sigma-$ algebra $\mathcal M$ and measure $\mu$ . We define the integral of this function over $E\in \mathcal M$ to be: $$\int_E f\text{ }d\mu=\sup\int_E s\text{ }d\mu \tag{1},$$ in which $s$ is a simple function, $s:X\rightarrow [0,\infty)$ , defined as: $$s(x)=\sum_{i=1}^n \alpha_i\chi_{A_i}, \tag{2}$$ in which $\chi_{A_i}(x)$ is the characteristic function on $X$ . Rudin states that the supremum in $(1)$ is being taken over all simple measurable functions such that $0 \leq s\leq f$ , I do not know what this means. Is this a pointwise comparison of the functions between $0$ and $f$ ? Standard mandatory disclaimer: I am a physicist not a mathematician.","['measure-theory', 'lebesgue-integral', 'measurable-functions', 'real-analysis']"
3899276,little-oh notation,"Assume I have a function $ f:\mathbb{R}^{m}\to\mathbb{R}^n $ , and let $ h\in \mathbb{R}^m $ . Does the notation $ f\left(x_0+h\right)=o\left(h\right) $ (for fixed $x_0 $ )
mean that $$ \lim_{h\to0}\frac{||f\left(x_{0}+h\right)||_{\mathbb{R}^{n}}}{||h||_{\mathbb{R}^{m}}}=0 ?$$ Because I've seen in a few places that lecturers wrote just $ \lim_{h\to0}\frac{f\left(x_{0}+h\right)}{||h||}=0 $ , which doesn't make any sense to me. I'll be glad for a clarification. What's the acceptable definition worldwide? Thanks in advance","['limits', 'multivariable-calculus', 'definition', 'asymptotics']"
3899310,Is (a version of) the Cayley-Hamilton theorem true for $\mathbb{N}\times\mathbb{N}$ matrices?,"Let $A\in \mathbb{C}^{n\times n}$ . The Cayley-Hamilton theorem states that if $p(x)$ is the characteristic polynomial of $A$ , i.e. $p(\lambda) = \det(\lambda I-A)$ , then $A$ satisfies the corresponding matrix polynomial: $p(A)=0$ . I am wondering if the theorem is still true or can be adapted if instead $A$ is an infinite matrix over $\mathbb{C}$ , i.e. $A\in \mathbb{C}^{\mathbb{N}\times\mathbb{N}}$ . The proofs I have seen involve finite-dimensional vector spaces or the division algorithm, which may not hold in infinite-dimensional spaces. Here we must take $A$ to be trace class; roughly speaking, this asserts that if $(e_k)_k$ is the standard basis, then the sum $\sum_{k=1}^{\infty}\langle |A| e_k,e_k\rangle$ is finite, where $|A|$ is the operator norm . One could define the identity matrix $I$ in an analogous fashion to the finite case and obtain the characteristic function $P$ : $$
P(\lambda) = \det(\lambda I-A)
$$ Here I take $\det$ to be the Fredholm determinant . Apologies if this post is unclear as I haven't studied functional analysis, so I may be misunderstanding the subtleties involved.","['cayley-hamilton', 'matrices', 'linear-algebra', 'functional-analysis', 'characteristic-polynomial']"
3899340,Pair of orthogonal directions is a submanifold,"I would like to ask something i don't understand. In my textbook of manifolds, it says that the subset $M$ of $\mathbb R P^{n} \times\mathbb R P^{n} $ , made from the pairs $(D,D')$ of the orthogonal directions of $\mathbb R^{n+1}-\{0\}$ , is a submanifold of the manifold $\mathbb R P^{n} \times\mathbb R P^{n} $ . So, I thought of something, based on what the inner product function that Nick wrote on his answer, but I'm not sure about it. (thinking of $\mathbb R P^{n} $ as $S^{n}$ by identifying the antipodal points). Let $g:\mathbb R P^{n}\times\mathbb R P^{n}\to\mathbb R$ , with $f=g\circ \pi$ , where $f:S^{n}\times S^{n}\to\mathbb R$ is the inner product function, and $\pi:S^{n}\times S^{n}\to\mathbb R P^{n} \times \mathbb R P^{n} $ is the projection map. Then, $M=g^{−1}(0)$ , and because $f$ is a submersion, so is $g$ . So $M$ will be a submanifold of $\mathbb R P^{n} \times\mathbb R P^{n} $ . What do you think about it?? Thank you in advance!","['submanifold', 'differential-geometry', 'smooth-manifolds', 'manifolds', 'projective-space']"
3899460,Pair of functions with same values and same derivatives at distinct points,"Let $f,g:[0,1]\to \mathbb{R}$ be two functions of class $C^1$ such that $f'(x)>0$ and $g'(x)>0$ for all $x\in [0,1]$ . Assume that $$
f(0) = g(0) \qquad \text{and} \qquad f(1) = g(1).
$$ Show that there exist $x,y\in[0,1]$ such that $$
f(x) = g(y) \qquad \text{and} \qquad f'(x) = g'(y).
$$ I made the following attempts (non of them worked): First, define a function $h:[0,1]\times [0,1]\to \mathbb{R}$ given by $$
h(x,y) = [f(x)-g(y)]^2 + [f'(x)-g'(y)]^2
$$ and try to show that $h(x,y) = 0$ for some $(x,y)$ . Then I defined $k:[0,1]\times [0,1] \to \mathbb{R}$ by $$
k(x,y) = \frac{1}{2}[f(x)-f(y)]^2
$$ and tried to show that at some point $(x,y)$ the divergence of $k$ is zero, because in this case we have that $$
0 = \frac{\partial k}{\partial x}(x,y) + \frac{\partial k}{\partial y}(x,y) = (f(x)-g(y))f'(x)-(f(x)-f(y))g'(y) = (f(x)-f(y))(f'(x)-g'(y))
$$ but it didn't work either. May be I'm not following the right way to solve this exercise.","['calculus', 'derivatives', 'real-analysis']"
3899517,Is every Borel set a countable union of intervals?,"Since the Borel $\sigma$ algebra is generated by all open subsets of $\mathbb{R}$ and all open sets are the countable union of disjoint open intervals, I figured that any Borel set is the countable union of intervals. I also used the fact that if we only use ' $\sigma $ algebra operations' on intervals then we get an interval or countable union of intervals.
But I ask if this is actually true?","['elementary-set-theory', 'borel-sets', 'measure-theory']"
3899532,Solve differential equation by integrating factor,"I have the differential equation $$2\frac{dy}{dx}+3y=e^{-2x}-5$$ I have determined that this needs solving using the integrating factor method. My workings out are in the image provided. Are my workings out correct so far? My main question is, how will I integrate with respect for $x$ ? $$\int e^{3x/2} \times e^{-2x}-\frac{5}{2}e^{3x/2}~~dx+c$$ this again is shown at the bottom of my workings out. after integrating this I will then be able to solve for $y$ . Thank you My workings out for the question","['integration', 'calculus', 'integrating-factor', 'ordinary-differential-equations']"
3899547,Abel criterion proof,"I want to prove the following statement: Let be $\sum\limits_{k=1}^{\infty} a_k$ a convergent series and $\left(b_k\right)_{n\in\mathbb{N}}$ a monotone and bounded sequence. Then $\sum\limits_{k=1}^{\infty} a_kb_k$ is also convergent. I know there already exist a few questions on this problem, however they mostly have additional assumptions (i.e. $\left(b_k\right)_{k\in\mathbb{N}}$ with $b_k\geq 0$ for all $k$ ). My approach: We define $A_n:=\sum\limits_{k=1}^{n} a_k$ . As $A_n$ is convergent there exists a bound $A$ such that $|A_n|\leq A$ for all $n$ . We know that $\left(b_k\right)_{k\in\mathbb{N}}$ is convergent and hence the sequence $\left(A_kb_k\right)_{k\in\mathbb{N}}$ is also convergent (product of two convergent sequences). Let be $n_1$ and $n_2$ two indices such that for all $n,m$ with $n>m>n_1$ it holds $|A_nb_n-A_mb_m|<\frac{\epsilon}{2}$ and for all $n,m$ with $n>m>n_2$ it holds $|b_n-b_m|<\frac{\epsilon}{2A}$ . Now we define $n_0:=\max\{n_1,n_2\}$ . With this in mind we apply Abel's lemma (summation by parts) and it follows for all $n>m>n_0$ : $$
|\sum\limits_{k=m+1}^{n} a_kb_k|=|A_nb_n-A_mb_m+\sum\limits_{k=m}^{n-1} A_k(b_k-b_{k+1})|\leq |A_nb_n-A_mb_m|+\sum\limits_{k=m}^{n-1} |A_k(b_k-b_{k+1})| \cdots
$$ If $\left(b_k\right)_{k\in\mathbb{N}}$ is monotonically decreasing it follows: $$
\cdots<\frac{\epsilon}{2}+ \sum\limits_{k=m}^{n-1} |A_k|(b_k-b_{k+1})\leq \frac{\epsilon}{2}+ \sum\limits_{k=m}^{n-1} A(b_k-b_{k+1})=\frac{\epsilon}{2}+A (b_m-b_n)<\frac{\epsilon}{2}+\frac{\epsilon A}{2A}=\epsilon.
$$ If $\left(b_k\right)_{k\in\mathbb{N}}$ is monotonically increasing it follows: $$
\cdots<\frac{\epsilon}{2}+ \sum\limits_{k=m}^{n-1} |A_k|(b_{k+1}-b_k)\leq \frac{\epsilon}{2}+ \sum\limits_{k=m}^{n-1} A(b_{k+1}-b_k)=\frac{\epsilon}{2}+A (b_n-b_m)<\frac{\epsilon}{2}+\frac{\epsilon A}{2A}=\epsilon.
$$ So in both cases $\sum\limits_{k=1}^{\infty} a_kb_k$ satisfies the Cauchy criterion and hence is convergent. Is this correct or is there a more elegant/faster approach?","['convergence-divergence', 'solution-verification', 'sequences-and-series', 'real-analysis']"
3899594,"Proof that in the family of sets $A_t = (\frac{t}{2} ; \frac{t+1}{2})$ for $t \in (0;1)$, $\bigcup A_{t} = (0;1)$, $\bigcap A_{t} = \{\frac{1}{2}\}$","As in the title, I have to proof that for the family of sets given as: $A_t = (\frac{t}{2} ; \frac{t+1}{2})$ for $t \in (0;1)$ : $\bigcup\limits_{t \in (0;1)} A_{t} = (0;1)$ $\bigcap\limits_{t \in (0;1)} A_{t} = \{\frac{1}{2}\}$ I don't know how to do the proof form right to left. I mean proving that for every $x$ there is $t$ that represents a set that includes $x$ . Usually I would use Archimedes' Axiom but out here I have no natural numbers. How then prove that for x close to 0 there is a set with a boundary that is even closer to 0? I need to do it ""in the way of set theory"" - I mean that I can not use boundary-value analysis nor function analysis.",['elementary-set-theory']
3899604,$\frac{a^{2}-1}{b+1}+\frac{b^{2}-1}{a+1}$ an integer $\Rightarrow \frac{a^{2}-1}{b+1}$ and $\frac{b^{2}-1}{a+1}$ are integers. [duplicate],"This question already has answers here : $\frac{a^{3}+1}{b+1}+\frac{b^{3}+1}{a+1}$ an integer $\Rightarrow \frac{a^{3}+1}{b+1}$ and $\frac{b^{3}+1}{a+1}$ are integers. (2 answers) Closed 3 years ago . If $\frac{a^{2}-1}{b+1}+\frac{b^{2}-1}{a+1}$ is an integer, prove that also $\frac{a^{2}-1}{b+1}$ and $\frac{b^{2}-1}{a+1}$ are integers. By doing the math, I get $\frac{(a-1)(a+1)^{2}+(b-1)(b+1)^{2}}{(a+1)(b+1)}$ is an integer which means that $(a+1)(b+1)$ divides $(a-1)(a+1)^{2}+(b-1)(b+1)^{2}$ . but I don't know how to continue. By the way, this is not homework or anything. I just found it in a book.","['number-theory', 'divisibility']"
3899641,Forced Duffing equation $\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0$ bifurcation analysis,"For the forced Duffing oscillator in the limit where the forcing, detuning, damping, and nonlinearity are all weak: $$\ddot x +x+\varepsilon(bx^3+k \dot x+ax−F\cos(t))=0$$ where $0<\varepsilon<<1$ , $b>0$ is the nonlinearity, $k>0$ is the damping, $a$ is the detuning,
and $F>0$ is the forcing strength. This system deals with saddle-node bifurcations of cycles arise in its analysis. The averaged equations of this system are $$r'=-\frac{1}{2}(kr+\sin(\phi)) \\\phi'=-\frac{1}{8}(4a-br^2+\frac{4F}{r}\cos(\phi))$$ Show that fixed
points for the averaged system correspond to phase-locked periodic solutions for the original forced oscillator. Show further that saddle-node bifurcations of fixed
points for the averaged system correspond to saddle-node bifurcations of cycles
for the oscillator. So far, all I have done is find the fixed points of the averaged system, which occur when $$r=\frac{-F}{k}\sin(\phi)\\\phi=2\pi c-\cos^{-1}(\frac{r(3br^2-4a)}{4F})$$ I don't exactly know how to show that these correspond to phase-locked periodic solutions for the original Duffing oscillator, though. Even more, I struggle to understand how I can classify the fixed points of the averaged equations as saddle-node bifurcations. I found that the Jacobian $J$ of the averaged system is $$J=\begin{bmatrix} -\frac{1}{2}k & -\frac{1}{2}cos(\phi)\\\frac{1}{4}br-\frac{4F}{8}\ln
|r|\cos(\phi) & \frac{F}{2r}\sin(\phi)\end{bmatrix}$$ but this seems way too complicated to be a good way to classify these fixed points. Any help would be extremely appreciated!","['nonlinear-system', 'bifurcation', 'nonlinear-dynamics', 'ordinary-differential-equations']"
3899658,Infinite product of $\sqrt{2}$,"I am struggling to show the following relation $$
\prod_{k=1}^\infty \left(1 - \frac{(-1)^k}{(2k-1)}\right) = \sqrt 2.
$$ I have tried to compute the sum $$
\sum_{k=1}^\infty \log \left(1 - \frac{(-1)^k}{2k-1}\right),
$$ by using the expansion for $\log(1+x)$ , however, I was not able to evaluate the double sum. Furthermore, I tried to square and reorder (although it should not be possible), but haven't quite got the right track. Could someone give me a hint for this problem?","['infinite-product', 'sequences-and-series']"
3899665,"Computing the flows $\theta, \Psi$ of $X$ and $Y,$ and verifying that the flows do not commute.","This is Problem 9-18 from Professor Lee's Intro to Smooth Manifolds (2nd edition). I believe I have most of the proof done, however, I am unsure how to finish it from where I currently am at. Can someone please help me? Thank you so much! Define vector fields $X$ and $Y$ on the plane by $$X = x\frac{\partial}{\partial x} - y\frac{\partial}{\partial y}, \hspace{20 pt} Y = x\frac{\partial}{\partial y} + y \frac{\partial}{\partial x}.$$ Compute the flows $\theta, \Psi$ of $X$ and $Y,$ and verify that the flows do not commute by finding explicit open intervals $J$ and $K$ containing $0$ such that $\theta_s \circ \Psi_t$ and $\Psi_t \circ \theta_s$ are both defined for all $(s,t) \in J\times K,$ but they are unequal for some such $(s,t).$ $\textit{Proof.}$ Recall that the flow is a family of integral curves for a specific vector field. Let $\gamma(t) = (x(t),y(t))$ be the candidate for the flow. We must solve $$X(\gamma(t)) = \gamma'(t).$$ We need to delineate $\gamma'(t).$ It must be an element of the tangent space at $\gamma(t).$ So, now we verify that. $$\gamma'(t) = x'(t)\partial_{x(t)} + y'(t)\partial_{y(t)}.$$ On the other side we have $X(\gamma(t)) = x(t)\partial_{x(t)} -y(t)\partial_{y(t)}.$ Once $X(\gamma(t)) = \gamma'(t)$ it assigns us $$x'(t) = x(t), y'(t) = -y(t).$$ These equations have the solutions $$x(t) = a\cos t = b\sin t, \hspace{20 pt} y(t) = a\sin t + b \cos t,$$ where $a,b$ are arbitrary constants.","['vector-fields', 'multivariable-calculus', 'smooth-manifolds', 'differential-geometry']"
3899679,Derivative of integral with varying domain? Fundamental theorem of calculus?,"Say I want to find the $n^{th}$ derivative of $$\int_{V_x} F{(x_1,...,x_k)}dV$$ where $V_x$ is some $k$ -dimensional volume dependent on $x$ . In the $1$ -dimensional case this is the fundamental theorem of calculus for $n=1$ and we can take higher derivatives after applying the fundamental theorem. To be concrete, say $V_x$ is the cube $[0,x]^k$ . What can I do in general? Is this Stokes theorem?","['integration', 'multivariable-calculus', 'calculus']"
3899689,Proving a certain nonlinear ODE has a solution on $t \geq 0$,"I'm trying to prove that the following ODE admits a solution on the interval $[0, \infty )$ : $\begin{align*}
\begin{cases}
x_1^\prime &= x_2 - x_1^3 \\
x_2^\prime &= \frac{1}{2}x_1 - x_2 + d \sin t
\end{cases}
\end{align*}$ With initial condition $x(0) = x_0$ . The usual approaches seem to fail: the derivative isn't bounded on $\mathbb{R}^2$ and the function does not seem Lipschitz on all of that domain either. Using energy functions doesn't seem to help either. Any hints would be highly appreciated!","['nonlinear-system', 'ordinary-differential-equations', 'real-analysis']"
3899699,(practical) Codes from sphere packings,"I know that good codes allow you to construct good sphere packings in Euclidean space, e.g. the binary Golay code is the key feature of the construction of the Leech lattice. Going in the other direction, I am wondering whether sphere packings in $\mathbf{R}^n$ actually help with designing good codes (for some definition of ""code"" that is useful in a practical setting). I know that codes on $n$ bits are sphere packings in Hamming space, but I'm wondering specifically about Euclidean sphere packings. I know that kissing configurations yield spherical codes (by definition). Are these actually useful in practice? Also, do dense (lattice) packings produce any practically useful codes? From searching on Google I found a small section of a paper of H. Cohn (page 6 of https://arxiv.org/pdf/1003.3053.pdf ) which gives a simplified example of how one can do coding with noise using good sphere packings, but I am wondering whether this is actually done in practice.","['spheres', 'number-theory', 'information-theory', 'reference-request', 'coding-theory']"
3899716,"Maximizing the pairwise point distance in $[-1,1]$","How can we prove that, given $n$ points in $[-1,1]$ , the sum of all their pairwise Euclidean distances (or, equivalently, their average Euclidean distance) is maximized if $\lfloor n/2 \rfloor$ points are placed at $-1$ and the remaining points are placed at $1$ ?","['euclidean-geometry', 'geometry', 'combinatorics', 'optimization', 'average']"
3899727,"If $f_n$ converges uniformly to $f$, then $\int f_n\ d\mu \to \int f\ d\mu$ as $ n \to \infty $","Let ( $\Omega, \mathcal{F}, \mu)$ , with $\mu (\Omega) <\infty$ and suppose $f_n , n \geq 1$ is a sequence of integrable functions that converges uniformly on $\Omega$ to $f$ . Given that $f$ is integrable, show that $$\int f_n\ d\mu  \to  \int f\ d\mu \quad\text{ as }\quad n \to \infty $$ I tried : $\left|\int_\Omega f_n \, d\mu - \int_\Omega f\, d\mu\right| $ : (By linearity of the integral) $= |\int_\Omega (f_n - f)\, d\mu|\leq \int_\Omega |f_n - f|\, d\mu$ How do we prove the inequality above? I tried the following but  I am not sure if it's correct or sufficient mathematical proof. Since $f_n$ integrable, so $\int_\Omega f_n \ d\mu$ is finite which means it exists. $f$ integrable, so $\int_\Omega f\ d\mu$ is finite,   which means it also exists. Thus, since they both exist $(f_n - f)$ exists $= |\int_\Omega (f_n - f)\, d\mu|$ is finite. Hence from basic property of integral since $= |\int_\Omega (f_n - f)\, d\mu|$ exists Finally  how do we apply the Uniform convergence on $  \int_\Omega |f_n - f|\, d\mu$ to conclude that $\int_\Omega f\ d\mu  \to  \int_\Omega f\ d\mu$ as $ n \to \infty $ ?","['integration', 'measure-theory', 'probability-theory', 'uniform-convergence']"
3899733,Algorithm for Determining Maximum Size for Squares in a Rectangle,"I'm trying to create a program to draw a specific number of equally-sized squares within a rectangle. I want to set my squares' sizes to the maximum size possible while still fitting within a rectangle. Empty space is fine so long as the squares can't get any larger by breaking into a new row. Here is an illustration of the desired result of the program with square count values 1-5: So far, I've successfully found the different permutations for the possible number of columns/rows based on square count. Here's an example using a square count of 4 of the permutations that can contain 4 squares: {rows: 1, columns: 4}
{rows: 2, columns: 2}
{rows: 2, columns: 3}
{rows: 3, columns: 2}
{rows: 3, columns: 3}
{rows: 4, columns: 1} Once I have the permutations, how do I determine which one will allow for the maximum square size within the dynamically-sized rectangle? Is there a way to use the area perhaps? Please let me know if you have any questions or if I can make something clearer. Thank you! My apologies, I am not very familiar with mathematical notation, so please know this if posting any. I will do my best to research anything posted, but I may have follow-up questions. Thanks!","['programming', 'area', 'geometry']"
3899784,"Let $X_1,X_2,\dots$ be i.i.d. with cdf $F(x)$ with $\lim_{x\to\infty}x^\alpha\left[1-F(x)\right]=b$.","Problem: Let $X_1,X_2,\dots$ be i.i.d. random variables with distribution function $F(x)$ . Denote the maximum of the first $n$ elements by $M_n$ . Show that if $$\lim_{x\to\infty}x^\alpha\left[1-F(x)\right]=b$$ with fixed positive constants $\alpha,b$ then $n^{-1/\alpha}M_n$ converges in distribution and identify the limiting distribution. What I have so far: Put $Y_n=n^{-1/\alpha}M_n$ . Then we have \begin{align*}
F_{Y_n}(x)
&=P\left(\max_{1\leq k\leq n}X_k\leq n^{1/\alpha}x\right)\\
&=[F(xn^{1/\alpha})]^n\\
&=[1-P(X_1>xn^{1/\alpha})]^n\\
&=\left[1-\frac{x^\alpha n}{x^\alpha n}P(X_1>xn^{1/\alpha})\right]^n.
\end{align*} Now I need to evaluate the limit above. It seems to me that this limit evaluates to $e^{-b/x^\alpha}$ , but upon graphing this function, I note that it is not a valid CDF. Therefore, this cannot be the right answer. Could anyone help with a hint on how to evaluate the limit above rigorously? Thank you for your time and appreciate any feedback.","['probability-distributions', 'probability-theory', 'probability']"
3899888,Function whose derivative is not continuous,"Let $f$ be a differtiable  function on $[a,b]$ and $f'(x)$ is not continous  at some $C$ in $(a,b)$ , then
Either left limit or right limit of $f'(x)$ at $C$ does not exist???? True or false
I know that $f'(x)$ don't have jump Discontinuities.So I think it is true.but not sure","['derivatives', 'real-analysis']"
3899910,Posterior distribution of exponential prior and likelihood?,"I have the prior density function: $$e^{-\theta} \text{ for } \theta > 0$$ and the likelihood function: $e^{\theta -x}$ for $x \geq \theta$ I have gotten the following in my attempt to derive the posterior distribution: $$L(\theta) = \prod e^{\theta - x_i} = e^{n\theta}e^{-\sum x_i} \mathbb{I}_{\min X_i}$$ $$\pi(\theta \mid x) = \frac{ e^{-\sum x_i} e^{\theta(n-1)} \mathbb{I}_{\min X_i}}{e^{-\sum x_i}\int_0^{\min(x)}e^{n\theta}e^{-\theta} \, d\theta}$$ $$= \frac{e^{\theta(n-1)}\mathbb{I_{\min X_i}}}{\frac{1}{n-1}e^{\min(x)(n-1)} - \frac{1}{n-1}}$$ Is this correct? Does this further simplify, or is the posterior simply non-standard?","['statistics', 'bayesian', 'probability']"
3899918,Does there exists a sequence of polynomials and rational functions approximating an analytic function uniformly?,"This question was asked in my complex analysis quiz and I was absolutely confused on which result to use. Consider the function $f(z)=1/z$ on the annulus $A=[{z \in \mathbb{C} : 1/2 < |z|<2}]$ .  Then (a) Does there exists a sequence ${p_n(z)}$ of polynomials that approximate f(z) uniformly on compact subsets of A. (b) DOes there exists a sequence ${r_n(z)}$ of rational functions , whose poles are contained in $\mathbb{C}/A$ and which approximate  f(z) uniformly on compact subsets of A. Attempt : $1/z$ is analytic on $A$ so there will exist a sequence of analytic functions which converge uniformly to $1/z$ on compact subsets of $A$ but why should they be polynomials or rational functions specifically?","['complex-analysis', 'analyticity', 'singularity']"
3899926,Generating functions and a closed form for the Fibonacci sequence - the big picture,"I have spent the last few hours trying to understand one way of deriving a closed form for the Fibonacci sequence. As part of improving my mathematical maturity, I am trying to learn to see the ""big picture"" of what I'm doing as opposed to myopically following the details of a derivation or proof and then forgetting most of what I've done within a few weeks or months. Below I will post my derivation so that it can be checked for errors and general clarity, but what I would like also ask what the proper intuition or high-level perspective is on what I've done. As I look over my work, the key takeaways I see are that we put the Fibonacci sequence into the form of a generating function, and in particular we managed to put the generating function into a compact rational form. Once this was done, the rest of the work essentially involved analyzing the rational form of the generating function and extracting information from it until we were able to write the generating function (in its formal power series form) in two different ways, compare coefficients and thus derive a closed form for the Fibonacci sequence. If these observations are apt, then perhaps the big takeaway is that generating functions can be useful when they make it possible to package a sequence into a form that allows algebraic and/or analytic techniques to be brought to bear on the sequence, when it is represented as a generating function. I would appreciate if anyone can tell me whether my perspective is reasonable or not. My exposition: The Fibonacci numbers are a sequence $1, 1, 2, 3, 5, 8, 13, \dots$ . The first two numbers are 1, and then every subsequent number is the sum of the prior two. Let $(a_n)$ be the sequence of Fibonacci numbers and $f(x) = a_0 + a_1 x + a_2 x^2 + \dots$ be the generating function. Consider that \begin{align*}
x^2 f(x) + x f(x) &= a_0 x^2 + a_1 x^3 + a_2 x^4 + \dots + a_0 x + a_1 x^2 + a_2 x^3 + \dots\\
&= a_0 x + (a_0 + a_1) x^2 + (a_1 + a_2) x^3 + \dots\\
&= a_0 x + a_2 x^2 + a_3 x^3 + \dots\\
&= f(x) - a_0 - a_1 x + a_0 x\\
&= f(x) - 1 - x + x \text{ (using the known values for } a_0 \text{ and } a_1)\\
&= f(x) - 1.
\end{align*} Thus $f(x) = \frac{1}{1 - x - x^2}$ . Now factor $1 - x - x^2$ as $(1 - \alpha x)(1 - \beta x)$ , so that $- \alpha - \beta = -1$ (or more naturally, $\alpha + \beta = 1$ ) and $\alpha \beta = -1$ . If you solve this system of equations you will end up with $\alpha = \frac{1 + \sqrt 5}{2}$ and $\beta = \frac{1 - \sqrt 5}{2}$ , or the reverse. Now consider the partial fraction decomposition \begin{align*}
f(x) &= a_0 + a_1 x + a_2 x^2 + \dots\\
&= \frac{1}{1 - x - x^2}\\
&= \frac{1}{(1 - \alpha x)(1 - \beta x)}\\
&= \frac{a}{1 - \alpha x} + \frac{b}{1 - \beta x}\\
\Rightarrow 1 &= a(1 - \beta x) + b(1 - \alpha x).
\end{align*} This gives us another system of equations such that $a + b = 1$ and $-a \beta - \alpha b = 0$ (or more naturally, $a \beta + \alpha b = 0$ .) If you solve \textit{this} system of equations you get $a = \frac{\sqrt 5 + 1}{2 \sqrt 5}$ and $b = \frac{\sqrt 5 - 1}{2 \sqrt 5}$ .\ Thus, \begin{align*}
f(x) &= a_0 + a_1 x + a_2 x^2 + \dots\\
&= \frac{1}{1 - x - x^2}\\
&= \frac{1}{(1 - \alpha x)(1 - \beta x)}\\
&= \frac{a}{1 - \alpha x} + \frac{b}{1 - \beta x}\\
&= a(1 + \alpha x + \alpha^2 x^2 + \dots) + b(1 + \beta x + \beta ^2 x^2 + \dots)\\
&= (a + b) + (a \alpha + b \beta)x + (a \alpha ^2 + b \beta^2)x^2 + \dots
\end{align*} which explicitly means that $a_k = (\frac{\sqrt 5 + 1}{2 \sqrt 5})(\frac{1 + \sqrt 5}{2})^k + (\frac{\sqrt 5 - 1}{2 \sqrt 5})(\frac{1 - \sqrt 5}{2})^k$ , as desired.","['fibonacci-numbers', 'discrete-mathematics', 'generating-functions']"
3900001,"In $\Delta ABC$ where $\angle A = 60^\circ$, $BP$ and $BE$ trisect $\angle ABC$ and $CP$ and $CE$ trisect $\angle ACB$ . Find $\angle BPE$ .","In $\Delta ABC$ where $\angle A = 60^\circ$ , $BP$ and $BE$ trisect $\angle ABC$ and $CP$ and $CE$ trisect $\angle ACB$ . Find $\angle BPE$ . What I Tried : Here is a picture :- It is not hard to realise that angle-chasing is useful here, and that's exactly the same thing I did. I got the required angles in the picture as shown, but I can't seem to understand how to get $\angle BPE$ once I join the line $PE$ , if there is no suitable idea other than angle-chasing which I can use here, what should I do? Can anyone help me? Thank You. Edit : I become a bit stupid sometimes, hence I didn't realise $E$ is the incenter which makes $PE$ the angle bisector, implying $\angle BPE = 50^\circ$ .","['triangles', 'problem-solving', 'geometry']"
3900006,Why the chain rule does not work for this question?,"$f(x)=x^{14x}$ Find $f'(x)$ I used the chain rule and wrote it as $f(U)=U^{14x},U(x)=x $ , and get an answer : $14x(x)^{14x-1}$ But it is wrong .The right answer should be make $y=x^{14x}$ then $\ln y=\ln x^{14x}$ then $\ln y=14x\ln x$ then differentiate each side with respect to $x$ .
Can anyone explain why my method is wrong ?","['calculus', 'derivatives', 'chain-rule', 'logarithms']"
3900018,Elementary Ways to Solve System of Exponential Equation,"Is there any elementary way (or using Lambert-W maybe) to solve this system of the exponential equation: $$ \begin{cases}
3^{x+y}+2^{y-1}=23, \\
3^{2x-1}+2^{y+1}=43.
\end{cases} $$ I have tried to eliminate the exponent of 2 but it gets me $$ 12 \cdot 3^{x + y} + 3^{2x} = 405 $$ which is more complicated. I have also tried to substitute $ 3^x = u $ and $ 2^y = v $ but there is still $ 3^y $ . Any advice is welcome (it's okay to use non-elementary method). Thanks :)","['exponentiation', 'algebra-precalculus', 'systems-of-equations', 'exponential-function']"
3900109,"Birthday problem with large $n, d$ values","In the Birthday problem , the formulas $${\displaystyle {\begin{aligned}p(n;d)&={\begin{cases}1-\displaystyle \prod _{k=1}^{n-1}\left(1-{\frac {k}{d}}\right)&n\leq d\\1&n>d\end{cases}}&\approx 1-e^{-{\frac {n(n-1)}{2d}}}&\approx 1-\left({\frac {d-1}{d}}\right)^{\frac {n(n-1)}{2}}\end{aligned}}}$$ work well for $d = 365$ and $n=23$ , and gives the usual estimation that if you have 23 people in the same room, the probability to have at least two people born the same day is $\geq 50 \%$ . Question: what formula is available for $p(n; d)$ with more precise error terms? Concrete application: I'm using random 5-alphanumeric-character identifiers for an inventory of objects. Example: V4QH7, WYJ9X, LK6H4, etc. If I have $n = 10,000$ objects, what is the probability that at least 2 objects have the same ID? Note: the last formula (the one after the one with exponential function above) gives Error, numeric exception: overflow in Maple when I take $d=(26+10)^5=60,466,176$ and $n=10,000$ . The formula with exp gives $p \approx 3.8 \%$ but since no error term is given, I don't know if this is accurate. Edit: Mistake: $p \approx 3.8 \%$ was obtained when I took $d=33^6$ (6-alphanumeric characters with a few letters removed for easier identification: I vs 1, etc.). With $d=36^5$ , we get $56.3 \%$ probability of having a collision with the exp formula above, which is in accordance with the accepted answer.","['birthday', 'probability-theory', 'probability']"
3900122,The characteristic polynomial of an ODE is equal to the characteristic polynomial of the companion matrix,"One can write a homogeneous linear differential equation with real constant coefficients $a_k \in \mathbb R$ of degree $n \in \mathbb N$ \begin{equation} \tag{1}
    0 = \sum_{k = 0}^{n} a_{k} \cdot u^{(k)}(t)
    \qquad \text{with } a_n = 1
\end{equation} as a system of $n$ differential equations like so: \begin{align*}
	\begin{pmatrix}
	u \\ \vdots \\ u^{(n - 1)}
	\end{pmatrix}'
	 + \underbrace{\begin{pmatrix}
	 0 & - 1 & 0 & \ldots & 0 \\
	 0 & 0 & - 1 & 0 &  0 \\
	 \vdots & \ddots & \ddots & \ddots & 0 \\
	 0 & \ldots & \ldots & 0 & -1 \\
	 a_0 & a_1 & \ldots & \ldots & a_{n - 1}
	 \end{pmatrix}}_{:= A \in \mathbb R^{n \times n}}
	 \begin{pmatrix}
	 u \\ \vdots \\ u^{(n - 1)}
	 \end{pmatrix}
	 = \begin{pmatrix}
	 0 \\ \vdots \\ 0
	 \end{pmatrix}
\end{align*} The characteristic polynomial of $(1)$ is defined to be $\chi(\lambda) := \sum_{k = 0}^{n} a_{k} \cdot \lambda^{k}$ with $a_n = 1$ .
I want to show that the characterisitic polynomial of $A$ , $\det(\lambda I - A)$ , is equal to the characterisitic polynomial of the differential equation. Here's is what I tried to prove it. If $A = \begin{pmatrix} B & C \\ D & E \end{pmatrix}$ is a block matrix where $E$ is a scalar and thus $C$ and $D$ are vectors, we have $\det(A) = (E - D B^{-1} C) \det(B)$ , if $B$ is invertible. Defining $$B := \begin{pmatrix} \lambda & -1 & 0 & \ldots & 0 \\ 0 & \lambda & -1 & \ddots & \vdots \\ \vdots & \ddots & \ddots & \ddots & 0 \\ 0 & \ldots & 0 & \lambda & - 1 \\ 0 & \ldots & \ldots & 0 & \lambda \end{pmatrix} \in \mathbb R^{(n - 1) \times (n - 1)}, \qquad
C := \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \in \mathbb R^{n - 1}, \qquad D :=\begin{pmatrix} a_0 & a_1 & \ldots & a_{n - 3} & a_{n - 2} \end{pmatrix} \in \mathbb R^{1, n - 1}$$ I obtain for $\lambda \ne 0$ (as $\det(B) = \lambda^{n - 1}$ ) \begin{align*}
\det(\lambda I - A)
& = \det\begin{pmatrix} \lambda & 1 & 0 & \ldots & 0 \\
0 & \lambda & 1 & \ddots & \vdots \\
\vdots & \ddots & \ddots & \ddots & 0 \\
0 & \ldots & 0 & \lambda & 1 \\
a_0 & a_1 & \ldots & a_{n - 2} & a_{n - 1} + \lambda
\end{pmatrix} \\
& = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) - \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix} \lambda^{-1} & \lambda^{-2} & \ldots & \lambda^{-(n - 1)} \\ 0 & \lambda^{-1} & \ddots &  \vdots \\ \vdots & \ddots & \ddots & \vdots \\ 0 & \ldots & 0 & \lambda^{-1} \end{pmatrix} \begin{pmatrix} 0 \\ \vdots \\ 0 \\ - 1 \end{pmatrix} \right] \\
& = \lambda^{n - 1} \left[ (a_{n - 1} + \lambda) + \begin{pmatrix} a_0& \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix}\lambda^{-(n - 1)} \\ \vdots \\  \lambda^{-1} \end{pmatrix} \right] \\
& = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \begin{pmatrix} a_0 & \ldots & a_{n - 2} \end{pmatrix}
\begin{pmatrix} 1 \\ \vdots \\  \lambda^{n-2} \end{pmatrix} \\
& = \lambda^{n} + a_{n - 1} \lambda^{n - 1} + \sum_{k = 0}^{n - 2} a_k \lambda^k. 
\end{align*} Edit I miscalculated $B^{-1}$ , so now the calculation works. Is there a simpler way to show this result?","['determinant', 'ordinary-differential-equations', 'eigenvalues-eigenvectors', 'characteristic-polynomial', 'linear-algebra']"
3900284,A stick of fixed length is broken into 3 pieces. Construct triangle,"A stick of length 1 is broken into 3 pieces in the following way: We chose random interior point on the stick and break it into two pieces After that we choose the longest of the two pieces, choose random point on it and break it again, getting 3 pieces total. The task is to find probability that it would be possible to construct a triangle using that pieces. I've came up with the following idea: Let $X_1, X_2$ be random variable, uniformly distributed across all points on the stick ( $X_1, X_2\in [0,1]$ ). It is clear, that Probability space may be illustrated as follows (marked with yellow): And basing on triangle inequality we may highlight areas on the graph, that would suit us in terms of constructing triangle: So, we get that $|\Omega|=S_{yellow section}=\cfrac{3}{4}$ And probability we are searching for: $Pr(A)=\cfrac{S_{blue section}}{S_{yellow section}}=\cfrac{1}{3}$ . So answer here is $\cfrac{1}{3}$ , however my mate got very different result, including $log$ . Can you help me with this task/find mistake in my solution? Any response is welcome and would be appreciated a lot.","['geometric-probability', 'problem-solving', 'probability']"
3900369,Three player combinatorial game - minimizing communication between Alice and Bob,"Consider the following game between three parties: Alice, Bob and a Referee. The game starts with $n$ closed boxes. For some fixed $k<n$ known to all parties, the Referee freely chooses $k$ distinct boxes and places a prize inside each of them. Alice sees the Referee do this and thus, she knows which boxes have a prize. Bob now enters the room. He can talk to Alice, then choose one box and if this box has a prize, he can keep it. What is the smallest number of bits of information that Alice needs to convey to Bob such that Bob is guaranteed to win a prize? The naive strategy is for Alice to pick a prize-containing box at random and give the ID of this box to Bob. This costs $\log n$ bits of information since she picks one box out of $n$ choices. But given that there are $k$ choices and any of them would do the job, can Alice and Bob win the game with less than $\log n$ bits of communication? If yes, what should their strategy be? EDIT: I am happy to assume the Referee is not adversarial if that admits a better strategy for Alice and Bob.","['game-theory', 'monty-hall', 'combinatorics', 'information-theory']"
3900411,Can two fields that have monomorphism to each other not be isomorphic? [duplicate],"This question already has an answer here : Embedding of fields (1 answer) Closed 3 years ago . More precisely, let $F_1, F_2$ be fields, and we have none zero homomorphisms $f: F_1 \to F_2$ and $g: F_2 \to F_1$ (actually both are monomorphism since $F_1, F_2$ are fields), is there a possibility that $F_1$ is not isomorphic to $F_2$ ?","['field-theory', 'abstract-algebra']"
3900488,Finding the the number of Lattice paths with three steps.,"We define a nice path of length as a path on a lattice from $(0,0)$ to $(n,n)$ such as every step in it is: $r=(0,1)$ , $u=(1,0)$ and $v=(1,2)$ , and it does not go under the line $y=x$ . A. Write a formula for the generating function that counts the number of the nice paths. B.Solve the above formula and find an expression for the number of these paths of length n. C.Find the number of $(1,2)$ steps in all paths of length n. I started with drawing the possible paths for $n=1,2,3,4$ , getting that we have $1,3,9,27$ paths (respectively) if I did really succeed to count all the possible paths for each $n$ . So, then I concluded that each legal path (which is mentioned above) is: Every path = $\epsilon$ or v(every path) u (every path) or r(every path) u (every path). Whhere $\epsilon$ denote the empty path. If $A(x)$ the generating function and x denote the number of steps then I get: $A(x)=1+(xA(x))(xA(x))+(xA(x))(xA(x))$ . $A(x)=1+x^2A(x)^2+x^2A(x)^2$ In B. I can do this after extracting $A(x)$ With finding the coefficient of $x^n$ in $A(x)$ . C. I don't understand if this is related to statistics on the number of (1,2) steps.. I'm not sure if my way of thinking is correct or not, I discovered that my way is not accurate. I'll be glad if you can correct me, and explain it.","['catalan-numbers', 'combinatorics', 'recurrence-relations', 'generating-functions']"
3900551,Splitting of the tangent bundle and Euler characteristic of surfaces,"Let $M$ be a be a closed orientable surface, and suppose that its tangent bundle $TM$ splits into a direct sum of line bundles. How to prove that $M$ 's Euler characteristic is zero? Unfortunately, I am entirely unfamiliar with Characteristic Classes, but I would accept a solution based on properties of these objects; if there is  a reference for a book where I could find this claim or details of a proof, that would be great.","['algebraic-topology', 'vector-bundles', 'differential-topology', 'characteristic-classes', 'differential-geometry']"
3900659,What can I say about a bipartite graph with minimum degree $d$?,"If a bipartite graph is $d$ -regular, then there are $d$ disjoint perfect matchings. In the $d$ -regular case we can iteratively remove a perfect matching, resulting in a $d-1$ -regular graph and repeat. In my graph, all vertices have degree at least $d$ . How can we prove that there are $d$ disjoint edge covers? Is it possible to reduce this to the $d$ -regular case? This is a known result but I am ideally looking for a constructive proof or iterative proof, or one which reduces it to the $d$ -regular case.","['graph-theory', 'combinatorics', 'discrete-mathematics', 'optimization', 'induction']"
3900758,Codomain as part of the definition of a function,"For most practical purposes, the codomain is part of the definition of a function. The codomain is specified at the moment we write $f:A \to B$ , and is exactly what makes the word ""surjective"" meaningful. EDIT: by ""part of the definition"", I mean ""as an item of the ordered pair below"". However, in set theory, the definition of function as a binary relation (i.e. a subset $R$ of $A \times B$ ) does not include its codomain - if we define the function as a set $R$ satisfying certain properties, then we cannot really decide what the codomain is from $R$ . (The domain and image can, however,  be extracted from the set $R$ easily.) Of course, to define the relation $R$ using a formula $\phi$ , we also need to specify a set of objects from which we are choosing to form $R$ . See Axiom schema of specification in ZFC. For example, we may define $R=\{(x,y)\in \mathbb Z^2, x=y\}$ , but the set $\mathbb Z^2$ (the ""domain"" and ""codomain"") is not part of the structure of $R$ . And in fact, sometimes, we need to change the codomain of a function, just like we often need to restrict or extend the domain. For example, when we use the word ""embedding"", we usually mean ""isomorphism onto the image"", so we are implicitly and temporarily ""seeing"" the image as the ""codomain"". So, when we write done a function $f$ , do we actually mean the pair $(R, B)$ , the graph of $f$ and the codomain? Does this depend on the writing style and the convenience of notation for the problem we are solving?","['elementary-set-theory', 'definition', 'functions', 'soft-question']"
3900781,"Algorithm to solve $p(x)f''(x) + q(x)f(x) = 0$ where $p(x), q(x)$ are polynomials","Consider the following ordinary differential equation. $$ p(x)f''(x) + q(x) f(x) = 0$$ Here, $p(x), q(x)$ are given polynomials in $x$ . For example, in my physics education, I am interested in solving the following. $$ 4x^2 f''(x) +(-15-8x^2-4x^4)f(x) = 0 $$ What are the general algorithms to solve these kind of differential equations? Any references are also very appreaciated. One of the ways that I know of would be assuming series solution and solve (recursively) for coefficients, but is there something else?",['ordinary-differential-equations']
3900815,Orthogonality between geodesics and normal spheres in a Riemannian manifold,"In do Carmo's Riemannian Geometry , right after proving Gauss' Lemma and defining normal neighborhoods and normal balls, the author writes: By Gauss' Lemma, the boundary of a normal ball $B_\varepsilon(p)$ is a hypersurface (a submanifold of codimension $1$ ) in $M$ orthogonal to the geodesics leaving $p$ , which is denoted by $S_\varepsilon (p)$ and called the normal (or geodesic sphere). As I understand, the orthogonality refers to ortogonality between the tangent vector of the geodesic and the tangent plane to the said hypersurface. Is it correct? Anyway, how to prove the claim about the sphere being a submanifold and the claim about orthogonality? Thanks in advance.","['geodesic', 'riemannian-geometry', 'differential-geometry']"
3900917,"Is there any official, specific convention that defines whether an expression is considered ""Simplified""?","I see all the time in high school math textbooks problems saying to ""simplify"" an expression. Their explanations of what it means for an expression to be ""simplified"" is a bit vague and does not reflect the rigor mathematics usually provides: "" To simplify an expression means to do all the math possible. "" ( OpenStax Intermediate Algebra ) I have encountered numerous situations in which there is ambiguity as to what looks ""simpler"" to people. For example: $$
\text{Simplify: }\frac{a^2b^{-3}c}{c^2b}
$$ Some people would say the most ""simple"" way to write this is $a^2b^{-4}c^{-1}$ . However, most textbooks would consider $\frac{a^2}{cb^4}$ to be the ""correct"" answer. From my perspective, problems beginning with ""Simplify"" don't really make sense (and are frankly unfair to the students) unless there is a clear definition of what it means for the expression to be ""simplified"" (especially with teachers who are picky about that kind of thing). Is there some sort of convention for simplification that I don't know about that makes this work?","['convention', 'algebra-precalculus']"
3900921,Examples of irreducible holomorphic function in more than one variable.,I'm studying analysis in several complex variables and in particular Weierstrass preparation theorem caught my interest (I'll include the theorem for clarity). In the examples I came up with I only found functions that could be written as the product of an unit and linear Weierstrass polynomials. It's never implied that every Weierstrass irreducible polynomial is of degree 1 or that in general irreducible functions are the degree 1 polynomial so I was looking for some examples of holomoprhic functions that are irreducible (at the origin) but are not polynomials of degree 1.,"['complex-analysis', 'several-complex-variables']"
3900981,"Example of bijective function from $(-1,1)$ to $\Bbb R$","I'm looking for a bijective function $f : (-1,1) \to \mathbb{R}$ . I'm having trouble finding an example.","['elementary-set-theory', 'functions', 'examples-counterexamples']"
3901004,Proof using formal definition of limit that $ \lim_{x\to0} (x^2\sin(x^2+2x)+2)=2 $,"I'm having a hard time trying to understand how to prove the following, using the formal definition of limits: $$
\lim_{x\to0} (x^2\sin(x^2+2x)+2)=2
$$ I already did the following the steps but I'm not sure if my thinking is correct: $$
|x^2\sin(x^2+2x)+2-2|<\epsilon \implies  |x^2\sin(x^2+2x)|<\epsilon \implies |x^2||\sin(x^2+2x)|<\epsilon
$$ and assuming that $x\in{R}$ then $x^2|\sin(x^2+2x)|<\epsilon$ Now I also know that $-1<\sin(x)<1$ however I'm struggling to understand how to best apply this to the proof. I'm not looking for a full solution to the problem, rather I'm interested in confirming if my thinking up until now is correct and maybe some hints to unblock the next steps.","['limits', 'epsilon-delta']"
3901062,Representation of cyclotomic polynomial over finite field with prime characteristic,"If I have a finite field $F_p$ with characteristic $p$ prime (and size $p$ ), the cyclotomic polynomial over $F_p$ is defined as $$\Phi_r(X) = \prod\limits_{i=1}^k (X - \eta_i)$$ where $\eta_i$ is a primitive $r$ th root of unity (maybe not an element of $F_p$ but certainly an element from the cyclotomic field). Is it true that $\Phi(X)$ can always be represented over $F_p[X]$ ? I mean, are the coefficients of $F_p[X]$ always elements of $F_p$ ? I know there are lots of question regarding factorization of cyclotomic polynomials over finite fields, but I couldn't find the answer to this question. I don't really care if it's irreducible or not, just if it can be represented in the field. Thanks!","['finite-fields', 'cyclotomic-polynomials', 'discrete-mathematics']"
3901080,Recurring Sequence with Exponent,"Working with recurring sequences and generating functions, I'm generally lost on solving a general expression of $a_n$ for any $n$ when the next part of the sequence, that is $a_{n+1}$ , is in the form of an exponent, such that $a_n = a_{n-1} +k^{n-1}$ , where k is some constant. I have  no clue on how to approach this problem. I've solved the Fibonacci sequence by subtracting the two pervious terms and shifting the sequence, but it does not seem to work here. I'm particularly working with $a_n = 2a_{n-1} + 5^{n-1}$ , but the sequence expands extremely fast. The base case, $a_{0} = 1$ . Any help would be appreciated!","['recursion', 'recurrence-relations', 'discrete-mathematics', 'generating-functions']"
3901104,"$\mathcal{B}(\mathcal{X},\mathcal{Y})$ is Banach space iff $\mathcal{Y}$ is Banach space.","Taken from Conway's A course in Functional Analysis Chapter 3 Section 2 Problem 1 Problem Statement: Show that for $\mathcal{B}(\mathcal{X}, \mathbb{F})\neq (0)$ , $\mathcal{B}(\mathcal{X},\mathcal{Y})$ is a Banach space if and only if $\mathcal{Y}$ is a Banach space. In the forward direction, we suppose that $\mathcal{B}(\mathcal{X}, \mathbb{F})$ is a Banach space. Then $\{T_n\} \in \mathcal{B}(\mathcal{X}, \mathbb{F})$ is a Cauchy sequence and therefore it converges to $T\in \mathcal{B}(\mathcal{X}, \mathbb{F})$ . This means that $\sup\{||(T_n - T)(x)||: ||x|| = 1\} \rightarrow 0$ as $n \rightarrow \infty$ . Now consider a sequence $y_n \in \mathcal{Y}$ . We hope to show that this sequence converges to $y\in \mathcal{Y}$ . If I can write $y_n = T_n(x)$ then since $T_n$ is Cauchy, it must follow that $T_n(x)$ converges to some $y \in \mathcal{Y}$ . But I'm not sure how to make this clear. Any tips is greatly appreciated!","['banach-spaces', 'functional-analysis', 'real-analysis']"
3901125,"Finding ""redundant"" or ""covered"" vectors in a randomly sampled collection.","Fix integers $k,m,t > 0$ . Say that I sample $w_1, \ldots, w_t$ uniformly and independently from $\{1,2,\ldots,m\}^k$ .  For each $i \in \{1,2,\ldots,t\}$ , let $S_i = \{w_1,\ldots,w_t\}\setminus\{w_i\}$ , i.e., the collection of vectors without $w_i$ .  Say that $w_i = (n^i_1, n^i_2, \dots, n^i_k)$ is ""covered"" by $S_i$ if $(\forall j \in \{1,2,\ldots,k\})(\exists \ell \neq i)$ s.t. $n^i_j = n^\ell_j$ .  Said less formally, $w_i$ is covered if each of its component values appear in at least one other vector (in the same position). In this sense, the covered vector $w_i$ is ""redundant"".  As an example, if my collection is $w_1=(1,2,3,4), w_2=(1,2,5,6), w_3=(1,7,3,4)$ , the vector $w_1$ is covered by $\{w_2,w_3\}$ , but neither of $w_2$ or $w_3$ is covered. My question: Let $\mathsf{Win}$ be the event that among the collection of vectors $w_1,w_2,\ldots,w_t$ there is at least one $w_i$ that is covered by its corresponding $S_i$ . Is there a closed form expression for $\mathrm{Pr}[\mathsf{Win}]$ ?  If not, a tight lower bound will suffice. I've looked at this from various angles, without success.  I can sort it out if there is a single target vector, say $w_1$ , and I want to know the probability that $w_1$ is covered by the corresponding $S_1 = \{w_2,\ldots,w_t\}$ .  But if we let $\mathsf{Win}_i$ be the event that $w_i$ is covered, note that the events $\mathsf{Win}_i$ and $\mathsf{Win}_j$ are not necessarily independent; thus, one cannot (easily) compute $\mathrm{Pr}[\mathsf{Win}]=1-\mathrm{Pr}[\bigwedge_{i=1}^t \neg\mathsf{Win}_i]$ .  Thanks in advance for your ideas.","['coupon-collector', 'discrete-mathematics', 'balls-in-bins', 'probability']"
3901137,Steps to turn on $2^n$ lamps on a roulette wheel,"There are $2^n$ lamps on a roulette wheel. Each time player A decides
on a list of lamps to flip (so on becomes off and off becomes on).
Player B sees player A's list, and rotates the roulette wheel to a
different location. Player A still flip the lights as if they were on
the original location. For example, say there are four lights $1,2,3,4$ . Player A decides to flip the switch of lamp $1$ . Player B
rotates the wheel to $3,4,1,2$ . Then after this round, lamp $3$ is
flipped. At the beginning, some lamps are on and some are off. Prove that regardless of the initial state, for no more than $2^{2n}$ rounds, player A can always turn on all the lights. There is a hint to this problem. Prove by mathematical induction. Suppose there is a way for $2^{k}$ lamps. Consider the case when $2^{k+1}$ . Pair up the lamps that are opposite of each other... I am thinking after we pair up the lamps that are opposite of each other, We define a new state for this $2^{k}$ pairs. If the pair has different states, we treat them as an ""off"" lamp. Otherwise we treat them as an ""on"" light. But there is no way to turn on this ""off"" light in this new, paired up roulette. My idea could be completely off. Certainly, a solution without using this hint is welcome too.",['combinatorics']
3901193,simple calculation with Christoffel symbols on Poincare half-plane,"Equip $H=\{(x,y):y>0, x,y \in \mathbb{R}\}$ with the metric $$ds^2=\frac{dx^2+dy^2}{y^2}.$$ ( https://en.wikipedia.org/wiki/Poincar%C3%A9_half-plane_model ). I want to show that the sectional curvature $$ K(\partial_i,\partial_j) = \frac{\langle R(\partial_i,\partial_j)\partial_j,\partial_i\rangle}{\det(g)}=\frac{R_{ijji}}{\det(g)}$$ is $-1$ . I know this is a simple calculation, but for some reason I'm off by a sign and it's driving me nuts. Here $g$ is the metric in matrix form: $$g=(g_{ij})=
\begin{pmatrix}
\frac{1}{y^2} & 0 \\
0 & \frac{1}{y^2}
\end{pmatrix}.$$ Let $\{ \partial_1=\partial/ \partial x,\partial_2=\partial/ \partial y\}$ be the coordinate basis, and just consider the computation of $K(\partial_1,\partial_2).$ So I just need to compute $R_{1221}=g_{1m}R^m_{\ 221}=g_{11}R^1_{\ 221}$ , where $$R^i_{jkl}=\partial_k \Gamma^i_{jl}-\partial_l \Gamma^i_{jk}+\Gamma^p_{jl}\Gamma^i_{pk}-\Gamma^p_{jk}\Gamma^i_{pl}.$$ I already have the Christoffel symbols, namely \begin{equation*}
\Gamma^1_{12}=\Gamma^1_{21}=-\frac{1}{y},\\
\Gamma^2_{12}=\Gamma^2_{21}=0,\\
\Gamma^1_{11}=\Gamma^1_{22}=0,\\
\Gamma^2_{11}=\frac{1}{y},\\
\Gamma^2_{22}=-\frac{1}{y}. 
\end{equation*} So \begin{align*}
R^1_{\ 221} &= \partial_2 \Gamma^1_{21}-\partial_1 \Gamma^1_{22}+\Gamma^p_{21}\Gamma^1_{p2}-\Gamma^p_{22}\Gamma^1_{p1}\\
&= \partial_2 \left( -\frac{1}{y}\right)+\Gamma^1_{21}\Gamma^1_{12}-\Gamma^2_{22}\Gamma^1_{21}\\
&= \frac{1}{y^2}+\left(-\frac{1}{y}\right)^2-\left(-\frac{1}{y}\right)^2\\
&= \frac{1}{y^2}.
\end{align*} But this gives $R_{1221}=1/y^4$ , and hence $K(\partial_1,\partial_2)=1$ . Please tell me what I'm doing wrong here or what formula is wrong.",['differential-geometry']
3901201,"then there exist $m,n$ such $(m!)\cdot (n!)\equiv r\pmod p$","let $p$ is give odd prime number,show that: for any postive integer $r$ ,then there exists postive ineteger $m,n$ such that $$(m!)\cdot (n!)\equiv r\pmod p$$ I try use this $$(m)!\equiv (p+1)(p+2)\cdots (p+m) \pmod p$$ $$(n)!\equiv (p+1)(p+2)\cdots (p+n) \pmod p$$ so $$(m!)\cdot (n!)\equiv (p+1)(p+2)\cdots (p+m) (p+1)(p+2)\cdots (p+n)\pmod p$$ But I can't it",['number-theory']
