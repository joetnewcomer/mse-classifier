question_id,title,body,tags
2687769,Solve the equation $\cos^2x+\cos^22x+\cos^23x=1$,"Solve the equation: $$\cos^2x+\cos^22x+\cos^23x=1$$
  IMO 1962/4 My first attempt in solving the problem is to simplify the equation and express all terms in terms of $\cos x$. Even without an extensive knowledge about trigonometric identities, the problem is solvable. $$
\begin{align}
\cos^22x&=(\cos^2x-\sin^2x)^2\\
&=\cos^4x+\sin^4x-2\sin^2\cos^2x\\
&=\cos^4+(1-\cos^2x)^2-2(1-\cos^2)\cos^2x\\
&=\cos^4+1-2\cos^2x+\cos^4x-2\cos^2x+2\cos^4x\\
&=4\cos^4x-4\cos^2x+1
\end{align}
$$ Without knowledge of other trigonometric identities, $\cos3x$ can be derived using only Ptolemy's identities. However for the sake of brevity, let $\cos 3x=4\cos^3x-3\cos x$:
$$
\begin{align}
\cos^23x&=(4\cos^3x-3\cos x)^2\\
&=16\cos^6x+4\cos^2x-24\cos^4x
\end{align}
$$ Therefore, the original equation can be written as:
$$\cos^2x+4\cos^4x-4\cos^2x+1+16\cos^6x+4\cos^2x-24\cos^4x-1=0$$
$$-20\cos^4x+6\cos^2x+16\cos^6x=0$$
Letting $y=\cos x$, we now have a polynomial equation:
$$-20y^4+6y^2+16y^6=0$$
$$y^2(-20y^2+6y+16y^4)=0\Rightarrow y^2=0 \Rightarrow x=\cos^{-1}0=\bbox[yellow,10px]{90^\circ}$$
From one of the factors above, we let $z=y^2$, and we have the quadratic equation:
$$16z^2-20z+6=0\Rightarrow 8z^2-10z+3=0$$
$$(8z-6)(z-\frac12)=0\Rightarrow z=\frac34 \& \ z=\frac12$$ 
Since $z=y^2$ and $y=\cos x$ we have:
$$\biggl( y\rightarrow\pm\frac{\sqrt{3}}{2}, y\rightarrow\pm\frac{\sqrt{2}}2 \biggr)\Rightarrow \biggl(x\rightarrow\cos^{-1}\pm\frac{\sqrt{3}}{2},x\rightarrow\cos^{-1}\pm\frac{\sqrt{2}}2\biggr)$$
And thus the complete set of solution is:
$$\bbox[yellow, 5px]{90^\circ, 30^\circ, 150^\circ, 45^\circ, 135^\circ}$$ As I do not have the copy of the answers, I still hope you can verify the accuracy of my solution. But more importantly... Seeing the values of $x$, is there a more intuitive and simpler way of finding $x$ that does away with the lengthy computation?",['trigonometry']
2687791,Can any complex $n\times n$ matrix be in a maximal commuting set of $\lfloor n^2/4\rfloor + 1$ matrices?,"The maximum number of mutually commuting linearly independent complex matrices of order $n$ is equal to $\lfloor n^2/4\rfloor + 1$ ($\lfloor \cdot \rfloor$ denotes the integer part of $\cdot$). This result is attributed to Schur and a short proof can be found here . In an answer to this question , an explicit set $\mathcal{M}$ of linearly independent and mutually commuting matrices is written for $2n\times 2n$ complex matrices:
$$
\begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n} \\ 0_{n\times n} &a\cdot \text{Id}_n\end{pmatrix},
$$
where $a$ is a complex number, $\text{Id}_n$ is the order $n$ identity matrix and $M$ is an arbitrary complex matrix. Now my question: can any nonzero $n\times n$ complex matrix be part of a subspace of ($\lfloor n^2/4\rfloor +1$) linearly independent mutually commuting matrices? If the answer is negative, what would be necessary? To avoid misunderstanding, let me add some redundancy and rephrase my question as: given an arbitrary nonzero $n\times n$ complex matrix $A_0$, is it possible to find $\lfloor n^2/4\rfloor$ linearly independent $A_i$, $i=1,\ldots,\lfloor n^2/4\rfloor$, such that $A_iA_j-A_jA_i=0$ for $i,j=0,\ldots,\lfloor n^2/4\rfloor$? Update The answer to my main question - if any nonzero matrix can be in the maximal subspace of mutually commuting matrices - is negative in general as Ewan Delanoy pointed out with the example of diagonalizable matrices with distinct eigenvalues. The secondary question - what is necessary to be in the maximal commuting set - seems more complicated. A set of mutually commuting matrices can be simultaneously triangularized , then only upper (lower) triangular matrices need be analyzed. For $2n\times 2n$ matrices an explicit set $\mathcal{M}$ of solutions is provided above and for $(2n+1)\times(2n+1)$ matrices the following set, also called $\mathcal{M}$, is a maximal commuting subspace:
$$
\begin{pmatrix}a\cdot\text{Id}_n & M_{n\times n+1} \\ 0_{n+1\times n} &a\cdot \text{Id}_{n+1}\end{pmatrix}.
$$ This set of matrices is also suggested in the paper by M. Mirzakhani, "" A simple proof of a theorem of Schur "", already cited above. Now I would like to change my secondary question for a more specific one: must any maximal commuting subspace have the form of $\mathcal{M}$ ? I think the answer is affirmative and, if that is the case, it will answer this question in MO: "" How many commuting nilpotent matrices are there? "" because the set $\mathcal{M}$ would be the maximal abelian nilpotent subalgebra plus (a multiple of) the identity matrix.","['matrices', 'linear-algebra', 'vector-spaces']"
2687871,Does $P_2P_1 = P_2$ hold when $P_2$ and $P_1$ are orthogonal projections onto $M_2\subseteq M_1$,"Suppose $P_1$ and $P_2$ are the orthogonal projections onto the closed subspaces $M_1$ and $M_2$ of a Hilbert space $X$. If $M_2 \subseteq M_1$, is it always the case that $P_2P_1 = P_2$?","['functional-analysis', 'projection', 'linear-algebra', 'operator-theory']"
2687909,Partial derivatives zero implies function constant,"Let $f:\mathbf{R}^k\to \mathbf{R}$ be a function such that all partial derivatives exists on all of $\mathbf{R}^k$ . If $D_i f(\vec{x})=0$ for all $\vec{x}\in\mathbf{R}^k$ and all $i$ , show that there exists $c\in\mathbf{R}$ such that $f(\vec{x})=c$ for all $\vec{x}$ . Let $\vec{x}=(x_1,\ldots,x_k)\in\mathbf{R}^k$ . Define for all $1\leqslant i\leqslant k$ , $g_i:\mathbf{R}\to\mathbf{R}$ by $g_i(t)=f(x_1,\ldots,t,\ldots,x_k)$ with $t$ on the $i$ -th place. Then $g'(x_i)=D_i f(\vec{x})=0$ . Since $\vec{x}$ and thus $x_i$ is arbitrary, this implies that $g_i \equiv c_i\in\mathbf{R}$ according to some theorem in my textbook. The thing is, how do I now use this information to conclude that $f$ itself is constant?",['multivariable-calculus']
2687931,Closed form $\sum_{n=0}^\infty \frac{\cos(n)}{(2n+1)(2n+2)} $,"Is there a closed form for: $$\sum_{n=0}^\infty \frac{\cos(n)}{(2n+1)(2n+2)} $$ I know it converges through the direct comparison test, and I'm pretty sure there is a closed form, but I'm not entirely certain what it is. Here's what I've done so far: $$\sum_{n=0}^\infty \frac{\cos(n)}{2n+1}-\sum_{n=0}^\infty \frac{\cos(n)}{2n+2} $$
$$\sum_{n=0}^\infty \frac{\cos(n)}{2n+1} = \int_{0}^1\sum_{n=0}^\infty x^{2n}\cos(n)dx = \Re \int_{0}^1 \sum_{n=0}^\infty(x^2e^i)^n dx = \Re \int_{0}^1 \frac{1}{1-e^ix^2}dx = \Re({e^{-\frac{i}{2}}\operatorname{arctanh(e^\frac{i}{2}}))}  $$ $$ \sum_{n=0}^\infty \frac{\cos(n)}{2n+2} = \int_{0}^1\sum_{n=0}^\infty x^{2n+1}\cos(n)dx=\Re\int_{0}^1 x\sum_{n=0}^\infty (x^2e^i)^ndx = \Re \int_{0}^1 \frac{x}{1-e^ix^2}dx=\Re(-\frac{e^{-i}}{2} \ln|1-e^i|)$$ $$\Re({e^{-\frac{i}{2}}\operatorname{arctanh(e^\frac{i}{2}})}+\frac{e^{-i}}{2} \ln|1-e^i|)$$ After plugging all of this in into Wolfram Alpha, I don't get the right answer, however. Where is the mistake? There could be a lot of things wrong. Edit: I more want to see where my mistake in my work is, rather how to do the problem.","['summation', 'integration', 'sequences-and-series', 'calculus']"
2687967,Calculation involving complex numbers.,"We work in the space $\mathbb C^n$ and denote $v$ with $(v_1, \ldots, v_n)$ We also have $p,q > 1$ satisfying $p+q = pq$ Let $v_i = c_i|v_i|$ with every number complex, $|c_i| = 1$ Let $$w_i = \frac{\overline{c_i} |v_i|^{p/q}}{(\sum_j |v_j|^p)^{1/q})}$$ Show that  $\Vert w \Vert_q = 1$ ($q$-norm) and $\sum v_iw_i = \Vert v \Vert_p$ I tried to apply the definitions and just calculate it, but couldn't find the result. I think I'm overlooking something involving the complex conjugate.","['functional-analysis', 'normed-spaces', 'complex-numbers']"
2687984,"How to express $\mathbb{E}(X\,1\{Y>0\})$ in terms of the joint moments $\mathbb{E}(X^iY^j)$","Let $X$,$Y$ be real valued random variables. Assume that for all $i,j\in\mathbb{N}$ we know $\mathbb{E}(X^iY^j)$. Is there a way to express $\mathbb{E}(X\,1\{Y>0\})$ in terms of these joint moments? ($1\{\cdot\}$ is the indicator function) Is there any assumption about the distribution of X and Y that can simplify this problem? EDIT: Assume also that $\mathbb{N}$ includes point zero. EDIT (2nd):
So, I have a half idea which helps me make some progress, but there are multiple points of [extreme?] difficulty which compromise the whole thing. Take the Heaviside function $H(x)$, where $H(x)=\frac{d}{dx}\max(x,1)=\textbf{1}[x\geq 1]$ Therefore: $\mathbb{E}[XH(Y)]=\mathbb{E}[\frac{d}{dY}\max(XY,X)]=\mathbb{E}[\frac{d}{dY}\min(-XY,-X)]$ (1) Now, if we assume $X$ and $XY$ are bivariate normal, we have [1]: $\mathbb{E}[\min(-XY,-X)]=\theta\,\phi\left(\frac{\mathbb{E}[XY]-\mathbb{E}[X]}{\theta}\right)-\mathbb{E}[XY]\Phi\left(\frac{\mathbb{E}[XY]-\mathbb{E}[X]}{\theta}\right)-\mathbb{E}[X]\Phi\left(\frac{\mathbb{E}[X]-\mathbb{E}[XY]}{\theta}\right)$ (2) where $\theta = \sqrt{Var[X]-2Cov[X,XY]+Var[XY]}.$ Even with the above assumption, this is far from complete. I fail to be able to connect formula (1) and (2), since I cannot apply the Leibniz rule on a derivative involving a random variable. [1] M. Cain, The moment-generating function of the minimum of bivariate normal random variables, The American Statistician, 1994","['probability-theory', 'expectation']"
2687992,Mathematical Induction prove that $n^3+5n$ is divisible by $6$,"Sorry, I know this will be a duplicate on the site but the other solution I found confusing and the method look completely different to what I was taught. Prove that $n^3 + 5n$ is divisible by $6$ by using induction The question is Prove by mathematical Induction  $(n^3+5n)$ is divisible by $6$ Here is what I have done 
Assume $n=k$ is true $\sum_{1}^{k} k =\dfrac{(k^3+5k)}{6}$ Now assume $n=k+1$ is true $\sum_{1}^{k+1} k+1 =\dfrac{(k+1)^3+5(k+1)}{6}$ Then now
$\dfrac{(k+1)^3+5(k+1)}{6}=\sum_{1}^{k} k + (k+1)$ $\dfrac{(k+1)^3+5(k+1)}{6}=\dfrac{(k^3+5k)}{6} + (k+1)$ $\dfrac{(k+1)^3+5(k+1)}{6}=\dfrac{(k^3+5k)}{6} + \dfrac{(6k+6)}{6}$ $\dfrac{(k+1)^3+5(k+1)}{6}=\dfrac{(k^3+11k+6)}{6}$ But the other side doesnt equate (LHS) $\dfrac{(k^3+3k^2+3k+1)+5k+5}{6}$ $\dfrac{(k^3+3k^2+8k+6)}{6}\ne\dfrac{(k^3+11k+6)}{6}$ I hope my method was clear enough so you can see where I went wrong. It would be much more use to me if you solved it as I learn better from looking at solutions and then applying them to other questions.","['discrete-mathematics', 'induction', 'proof-verification', 'elementary-number-theory']"
2688009,Extending strongly convex sets,"Let $(M,g)$ be a Riemannian manifold and $S\subset M $ open and strongly convex , i.e. any two points $p,q\in \bar S$ are connected by a unique minimizing geodesic with interior completely lying in $S$. Conjecture: There exists an open neighbourhood $S'$ of $\bar S$ which is also strongly convex. New question (March 20): It seems like the conjecture is false in general and user Dap has suggested a counterexample as an answer. Can you find another counterexample, maybe a simpler one? Some remarks and proof attempts: For sets which are merely convex this is not true. For example an open  hemisphere of $S^2$ cannot be enlarged without losing convexity. The strongly convex sets of $S^2$ on the other hand have their closure strictly contained in a hemisphere and thus can obviously be enlarged to a new strongly convex set. This picture (respectively the ones that I can image on other surfaces) leads me to the assumption that the question has a positive answer. Strongly convex sets are totally normal, (i.e. for every point $p\in S$ there is an open set $V\subset T_pM$ such that $\exp_p\colon V\rightarrow S$ is a diffeomorphism). My first idea was to first find an $S''\supset \bar S$ totally normal and then try to restict it to a strictly convex set. But just working with total normality cannot work: $S^2 \backslash p_0$ is totally normal, but cannot be extended to a larger totally normal set. Unfortunately I don't know how strong convexity can be incorporated from the start. Edit (March 17): Here is a false counterexample, that was posted as an (now deleted) answer. I find it interesting neverletheless: Let $M=\{(x,y)\vert y>-e^x\}\subset \mathbb{R}^2$ and $S$ the open upper half plane (which is only convex and not strongly convex). If $S'$ is a open set containing $\bar S$, then $(0,-\epsilon)\in S'$ for some $\epsilon >0$. Take $x> 0$ so large that $\epsilon> xe^{-x+1}$, then the straight line between $(-x,0)$ and $(0,-\epsilon)$ contains the point $(-x+1,-\epsilon/x)$, which lies outside of $M$. In particular $S'$ cannot be convex.","['riemannian-geometry', 'differential-geometry']"
2688012,Conditions for a neat subgroup to act fixed-point free,"Given a hyperbolic reflection group $G$ acting on hyperbolic space $\mathbb{H}_n$ by, well, reflections in hyperplanes. Does a neat subgroup of $G$ act fixed-point free on $\mathbb{H}_n$? If not, what are sufficient conditions for this? Some thoughts on this: The stabilizer of a point $x\in\mathbb{H}_n$ in some fundamental chamber of the action of $G$ is generated by the reflections in the walls containing $x$. If this is a finite reflection group, its intersection with any neat subgroup of $G$ is empty and this subgroup acts fixed-point free. What if the stabilizer is infinite? When does its intersection with a neat subgroup vanish? (Remember: A subgroup of $G(\mathbb{Q})$ of a linear algebraic group over $\mathbb{Q}$ is called neat if the image of $G$ under some faithful represenation $G\to \textrm{GL}_n(\mathbb{Q})$ is neat, the latter neat here meaning that the subgroup of $\mathbb{C}^\ast$ generated by the eigenvalues of the elements in the subgroup of $\textrm{GL}_n(\mathbb{Q})$ is torsion free. Think of this as ""super-torsion free"": Not only the matrices are torsion free but every entry of their diagonalization.)","['coxeter-groups', 'geometry', 'group-theory', 'lie-algebras', 'lie-groups']"
2688068,Invariant cycle theorem,"Let $f : X \to C$ be a surjective map between projective varieties ($C$ is a curve). Let $C^* = C - \{\text{critical values of $f$}\}$, $X^* = f^{-1}(C^*)$. Fix $t \in C^*$ and let $X_t = f^{-1}(t)$. There are inclusions $i : X_t \to X^*$ and $j : X^* \to X$ which induces map in singular cohomology : $i^* : H^m(X^*) \to H^m(X_t), j : H^m(X) \to H^m(X^*)$. 1) I saw that ""obviously"" $H^m(X_t)^{\pi_1(C_t)} = i^*H^m(X^*)$. Why is this true ? 2) The invariant cycle theorem states that $i^*j^* H^m(X) = H^m(X_t)^{\pi_1(C_t)}$. What is the geometric intepretation of this statement ? What are the interesting corollaries ?","['algebraic-topology', 'algebraic-geometry']"
2688074,Method for approaching combinatorial proofs,I am struggling on how to approach combinatoric proof problems. The identity to prove is $$\binom{10}{5} = \binom{8}{3} + \binom{8}{4} +\binom{8}{4} + \binom{8}{5}$$ I can prove this algebraically but the question says prove combinatorially. I also could use pascals identity to solve the RHS to the LHS. How would I describe this combinatorially? Any guidance would be greatly appreciated.,['combinatorics']
2688123,Dual and bidual spaces of $B(H)$ in norm topology,"I would like to ask whether any description of the dual and double dual spaces exists for the Banach space $B(H)$ - space of bounded linear operators on infinitely dimensional Hilbert space. Can we identify them with some other known Banach spaces, such as we do with the duals in other topologies on $B(H)$?","['functional-analysis', 'operator-theory', 'hilbert-spaces', 'dual-spaces']"
2688169,Identiy matrix with lambda,"I need some help with my homework in a subject called ""Matrices in statistics"".
The task is to show that 
$$ P_{ij}(\lambda)^{-1} = P_{ij}\left(\frac{1}{\lambda}\right), $$ where $P_{ij} $ is an identity matrix, where the value of i-th row and j-th column is $\lambda.$ Here's my initial solution (which wasn't enough for the teacher): --
We need to show, that $$ P_{ij}\left(\frac{1}{\lambda}\right) P_{ij}({\lambda}) = 
P_{ij}({\lambda}) P_{ij}\left(\frac{1}{\lambda}\right) = I_n   $$ So: 1) $$ P_{ij}\left(\frac{1}{\lambda}\right) P_{ij}({\lambda}) = P_{ij}\left(\frac{1}{\lambda} \cdot\lambda\right)   =  P_{ij}(1) = I_n$$ And 2)  $$  P_{ij}({\lambda})P_{ij}\left(\frac{1}{\lambda}\right) = P_{ij}\left(\lambda\cdot\frac{1}{\lambda}\right)   =  P_{ij}(1) = I_n $$ Q.E.D I submitted this solution, but my lecturer gave it back and wrote, that I need to supplement this solution. More precisely, I need to show, that $$ \left(P_{ij}(\lambda)P_{ij}\left(\frac{1}{\lambda}\right)\right)_{kl} = \begin{cases} 1, k=l  \\ 0, k \neq l \end{cases} $$ And that's what gets me confused - I'm not sure, how to do it. 
I would be really thankful, if you could help me with this one!","['matrices', 'statistics', 'linear-algebra']"
2688180,"Calculate this integral $\int_a^b (\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \,ds) \, dt$","How to calculate this kind of integrals? $$\int_a^b \left(\int_a^b \frac{f(t) \overline{f(s)}}{1-ts} \, ds\right) dt$$ $a=0$, $0<b<1$, $t,s \in [a,b]$ are real, and $f$ ""lives"" in $C([a,b], \mathbb{C})$ I have to find that it's equal to $\sum_{n=0}^{+\infty} \left|\int_a^b f(t) t^n \, dt\right|^2.$ I just know that $\sum\limits_n (st)^n = \dfrac{1}{1-ts} \dots$ Could someone help me?","['real-analysis', 'integration']"
2688188,Is every conformal transformation a stereographic projection of an isometry of the inverse projection?,"Here is a really nice video that shows the following: Suppose we have a subset of $\mathbb{C}\cup \{\infty\}$. Then we can project it onto the 2-sphere via the inverse stereographic projection. If we rotate or translate the sphere in the surrounding $\mathbb{R}^3$ and apply the stereographic projection again, then the result amounts to a Möbius transformation of the initial subset. Here is a screenshot to illustrate this: $\hskip2in$ Furthermore, on this wiki-page on conformal maps , the 4th paragraph of the subsection ""Complex analysis"" says that ""A map of the extended complex plane (which is conformally equivalent to a sphere) onto itself is conformal if and only if it is a Möbius transformation."" Now this seems quite interesting to me because it seems to imply that if I take a subset of $\mathbb{R}^2$ and apply a conformal transformation $\Phi$ to it, then I can always write it as
\begin{equation}
\Phi = P\circ \phi\circ P^{-1}
\end{equation}
where $P$ is the stereographic projection and $\phi$ is a rotation and/or translation of the sphere in 3-space. My first question is: Is that right? My second question concerns whether this generalises to higher dimensions. I saw the wiki page on Liouville's theorem which states that every conformal mapping in $\mathbb{R}^n$, $n\ge 3$ is a Möbius transformation.
However, I am not sure if a Möbius transformation in higher dimensions still has the same intuitive meaning as in 2 dimensions.","['differential-topology', 'geometry', 'conformal-geometry', 'differential-geometry', 'symmetric-groups']"
2688237,Is there a quick way to arrive to this partial fraction decomposition?,"I'm reading a book where the author claims without showing the work that after we go through with the algebra of partial fractions we arrive to the formula (note $|z|<1$):
$$\frac{1}{(1-z)(1-z^2)(1-z^3)} = \frac{1}{6}\frac{1}{(1-z)^3}+\frac{1}{4}\frac{1}{(1-z)^2}+\frac{1}{4}\frac{1}{(1-z^2)}+\frac{1}{3}\frac{1}{(1-z^3)}$$ Naturally, I'm trying to reproduce the result, but I'm taking a very naive approach, namely I've expressed the LHS as a product of irreducible factors over $\mathbb{R}$ and I am trying to determine the coefficients:
$$\frac{1}{(1-z)^3}\frac{1}{(1+z)}\frac{1}{(z^2+z+1)} = \frac{A}{(1-z)}+\frac{B}{(1-z)^2}+\frac{C}{(1-z)^3}+\frac{D}{(1+z)}+\frac{Ex+F}{(z^2+z+1)}$$ This seems to have some drawbacks though, because on top of being long winded, once I do obtain all the coefficients I will have to recombine some of the terms to arrive at the author's answer. My question is then: is there some sort of trick which we can use here which I am not aware of, or do we have to suffer through the algebra patiently?","['partial-fractions', 'calculus', 'analysis']"
2688238,Prove $\mathbb{R} ^n$ is contractible for any $n \geq 1.$,"$X$ is contractible if the identity map $i_X : X \to X$ is homotopic to a constant map. Prove $\mathbb{R} ^n$ is contractible for any $n \geq 1.$ Is any contractible space is path connected? I am not sure how to show $\mathbb{R} ^n$ is contractible for any $n \geq 1,$ but I will attempt to show any contractible space is path connected. Suppose $X$ is contractible. Then there exists a homotopy $H$ such that $H(x,0) = x$ and $H(x,1) = c$ for all $x \in X$ and for some fixed $c \in X$. Then every point of $X$ is connected to $c$ by a path, and hence any two points $x_1$ and $x_2$ of $X$ can be joined by a path through $c$, completing the proof.",['general-topology']
2688265,"Intrinsic, coordinate-free interpretation of ""inverse"" of a metric tensor","I am taking a course on general relativity. There is some part where I think they are abusing terminology and I would like to interpret more properly. Given an indefinite metric tensor $g$ on a smooth manifold $M$, write $g$ in component form in local coordinates as $g_{\mu\nu}$. We define the following contravariant tensor in component form in local coordinates as $g^{\mu\nu}$, where $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$, i.e. $g^{\mu\nu}$ is (the components of) the ""inverse"" of the metric tensor. The above is how they defined $g^{\mu\nu}$. I find it problematic to me. Firstly, the definition relies on coordinates. Secondly, metric tensors are type $(0,2)$ tensors instead of type $(1,1)$, i.e. they are not linear transformations. Without reference to coordinates, there is no such thing as ""inverse"" of type $(0,2)$ tensor. I propose two ways to define and interpret $g^{\mu\nu}$ in an as-coordinate-free-as-possible way: While $g$ is nondegenerate, it induces an isomorphism between a tangent space $T_pM$ and its dual $T_p^*M$ by $v\mapsto g_p(v,\cdot)$. This isomorphism transfers the indefinite inner product $g_p$ from $T_pM$ to $T_p^*M$. This inner product on $T_p^*M$ is type $(2,0)$ on the manifold, and $g^{\mu\nu}$ would be the local coordinates expression of this inner product. While $g$ is nondegenerate, hopefully there exists a unique type $(2,0)$ tensor on $M$, denoted $g^{-1}$ (this is NOT an inverse of $g$. It is just a symbol) for which forming the tensor products $g^{-1}\otimes g$ and $g\otimes g^{-1}$ and then do a tensor contraction on each of them would both give the identity linear transformation on each tangent space. Then $g^{\mu\nu}$ would be the local expression of $g^{-1}$. Are the above interpretations of $g^{\mu\nu}$ correct, i.e. can we recover the equation $g^{\mu\nu}g_{\nu\lambda}=g_{\lambda\nu}g^{\nu\mu}=\delta_\lambda^\mu$? I am quite sure interpretation 2 is correct, because multiplying the two components is corresponds to tensor product, and making an upper index and a lower index equal and sum over that index corresponds to tensor contraction. But I am not very sure about interpretation 1.","['tensors', 'differential-geometry']"
2688286,Definition of semimartingale in Protter's book,"In Protter's book, the definition of semimartingale is conducted by a functional approach: Definition: (total semimartingale) A process $X$ is a total semimartingale if $X$ is cadlag, adapted and $I_x:S_u \rightarrow L^0$ is continuous, where $S_u$ is the space of the simple predictable process topologized with uniform convergence and $L^0$ is the finite valued random variable topologized with the convergence in probability. Definition: (semimartingale) A process $X$ is called a semimartingale if, for each $t\in[0,\infty)$, $X^t$  is a total semimartingale. Following this definition, I want to understand the following corollary 2 on p.55. Corollary 2: A local martingale with continuous paths is a semimartingale. The proof in the book claim that this corollary 2 is a immediate consequence of the Corollary 1, which is stated as follows Corollary 1: Each cadlag, locally square integrable local martingale is a semimartingale. However, I don't see the connection between a local martingale with continuous paths and locally square integrability. Further, why does both corollary only induce the process $X$ to be a semi-martingale and not a total semi-martingale? What could go wrong with the total semi-martingale?","['stochastic-processes', 'probability-theory']"
2688306,"Properties of the ""eeny, meeny, miny, moe function""","N.B. : there's another question about ""eeny, meeny, miny, moe"" in mathstackexchange but it is different to what it is presented here. Consider the function $f(p,s)$ defined as follows: There are $p>1$ players. We use a rhyme with $s>1$ steps. We start by player 1 and we apply the rhyme. The player where the rhyme ends is eliminated (cycling around if needed). Step 3 is repeated starting with the next non-eliminated player. The last standing player is $f(p,s)$. So, for example, if $p=5$ and $s=3$ we would eliminate players 3, 1, 5 and 2 in that order and we get $f(5,3)=4$. You can implement this function in Python as: def f(p,s):
    P=range(p)
    while len(P)>1:
        r=s%len(P)
        if r==0:
            r=len(P)
        P=P[r:]+P[:r-1]
    return P[0]+1 Using this, it is easy to get a table with some values (I include up to 16 here): s  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16
  p   --  --  --  --  --  --  --  --  --  --  --  --  --  --  --  --
  1|   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1   1
  2|   2   1   2   1   2   1   2   1   2   1   2   1   2   1   2   1
  3|   3   3   2   2   1   1   3   3   2   2   1   1   3   3   2   2
  4|   4   1   1   2   2   3   2   3   3   4   4   1   4   1   1   2
  5|   5   3   4   1   2   4   4   1   2   4   5   3   2   5   1   3
  6|   6   5   1   5   1   4   5   3   5   2   4   3   3   1   4   1
  7|   7   7   4   2   6   3   5   4   7   5   1   1   2   1   5   3
  8|   8   1   7   6   3   1   4   4   8   7   4   5   7   7   4   3
  9|   9   3   1   1   8   7   2   3   8   8   6   8   2   3   1   1
 10|  10   5   4   5   3   3   9   1   7   8   7  10   5   7   6   7
 11|  11   7   7   9   8   9   5   9   5   7   7  11   7  10  10   1
 12|  12   9  10   1   1   3  12   5   2   5   6  11   8  12   1   5
 13|  13  11  13   5   6   9   6  13  11   2   4  10   8  13   3   8
 14|  14  13   2   9  11   1  13   7   6  12   1   8   7  13   4  10
 15|  15  15   5  13   1   7   5  15  15   7  12   5   5  12   4  11
 16|  16   1   8   1   6  13  12   7   8   1   7   1   2  10   3  11 Apart from the obvious properties $f(1,s)=1$, $f(p,1)=p$ and $f(p,p)=f(p-1,p)$, I was only able to see the cyclic pattern that repeats in rows 2 and 3 (so for 2 or 3 players), but not much more than that. My questions are: Is there any general pattern of the sort $f(p,s+k(p))=f(p,s)$? Is there any other relevant property of the function $f$ that I'm missing? More in general, does this function have any particular name and is used somewhere else in Mathematics? Edit: As for the first question it seems now clear to me that $f(p,s+p!)=f(p,s)$.","['modular-arithmetic', 'discrete-mathematics']"
2688325,Does There exist a bijection between $\mathbb{R}^2$ and $ \mathbb{R}$ such that it is differentiable,"I was thinking recently about Louville's Theorem and the fact that there exist a bijection between $\mathbb{R}^2$ and $ \mathbb{R}$. Since $\mathbb{C}$ is nothing but $\mathbb{R}^2$ via the construction of $\mathbb{C}$ we can use those terms interchangeably. I thought of the following tho transformations. First
$$f_1:\mathbb{R}^2 \to \mathbb{R}$$ which we know exists and then 
$$f_2:\mathbb{R} \to S^1 $$ witch is a well known bijection and is differentiable (as far as I know). the composition of the functions $f_1$ and $f_2$ (let's call it $f_c$) should hence be a bijection between $\mathbb{R}^2$ and $ \mathbb{R}$. Louville's theorem states that any function such that $|f(z)|<M$ and $f\in H(\mathbb{C})$ must be equal to a constant function. aka $f(z)=c, c\in\mathbb{C}$
Our function $f_c$ is not a constant function because it ascribes a unique number in $S^1$ for each number in $\mathbb{C}$ and also $f_c$ is bounded $(f_c(z)<2$ for every $z\in \mathbb{C})$.
The only thing that could make Louville's theorem not work in this case would be the fact that $f_1$ is not a holomorphic function. Which would mean that there is no holomorphic bijection between $\mathbb{R}^2$ and $ \mathbb{R}$. Is this argument true?
If it is, does anyone know any restraints when it comes to differentiability of bijections between $\mathbb{R}^2$ and $ \mathbb{R}$? Thank you in advance.","['multivariable-calculus', 'complex-analysis', 'real-analysis', 'functions']"
2688350,"Find $\lim_\limits{(x,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4}$ where $\alpha > 0$","How can we find the following limit? $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}\qquad \alpha>0$$ By using the polar coordinate, we get
$$\lim_{r\to 0}r^{\alpha+2}\frac{\cos^\alpha\theta \sin^4\theta}{\cos^2\theta+r^2\sin^4{\theta}}=0$$ if $\theta\notin\{\frac{\pi}{2}+\pi k:k\in\Bbb Z\}$.
Now, if $\theta = \frac{\pi}{2}+\pi k$ for some $k\in\Bbb Z$, then we get $$\lim_{(0,y)\to (0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0.$$
Can we conclude that $$\lim_{(x,y)\to(0,0)}\frac{x^\alpha y^4}{x^2+y^4}=0?$$","['multivariable-calculus', 'calculus']"
2688358,Primes congruent to $1$ modulo $6$ are of the form $3a^2+b^2$,"I am asked to prove that primes of the form $6k+1$, where $k$ is an integer can be written as $3a^2 + b^2$, where $a$ and $b$ are integers by using the fact that $\textbf Z[\omega]$ is a UFD, where $\omega^2 + \omega + 1 = 0$. However, I cannot see any connection. How can one connect these two seemingly unrelated facts? Thanks in advance.","['abstract-algebra', 'ring-theory', 'algebraic-number-theory', 'elementary-number-theory']"
2688422,Sobolev approximation lifts to $L^p$ convergence of the exterior powers,"I am reading the book ""Geometric Function Theory and Non-linear Analysis"", where the following claim is used: Let $\Omega \subseteq \mathbb{R}^n$ be a bounded open set. Let $f \in W^{1,s}(\Omega,\mathbb{R}^n)$ , and let $f_n \in C^{\infty}(\Omega,\mathbb{R}^n)$ converge to $f$ in $W^{1,s}$ . Suppose $s \ge k \in \mathbb{N}$ . Then $\bigwedge^k df_n $ converges to $\bigwedge^k df $ in $L^1$ , i.e $$ \int_{\Omega}|\bigwedge^k df_n -\bigwedge^k df | \to 0,$$ where the norm $|\cdot|$ is the standard Euclidean norm on linear maps between exterior powers $\Lambda_k(\mathbb{R}^n) \to \Lambda_k(\mathbb{R}^n)$ . Question: How to prove this claim? My naive approach was to guess that $$ |\bigwedge^k df_n -\bigwedge^k df | \le C |df_n-df|^k, \tag{1}$$ which would imply $$ ||\bigwedge^k df_n -\bigwedge^k df ||_ 1 \le C||df_n-df||_k^k \le \tilde C||df_n-df||_s^k \to 0.$$ However, estimate $(1)$ is false . Edit: I tried to prove this via induction on $k$ . However, I hit an obstacle .","['riemannian-geometry', 'exterior-algebra', 'multivariable-calculus', 'approximation-theory', 'sobolev-spaces']"
2688578,Primes formed by concatenating the mersenne numbers from $2^2-1$ to $2^n-1$,"Concatenate the mersenne numbers $2^2-1$ to $2^n-1$ and define $f(n)$ to be the emerging number , for example $$f(6)=\color\red {3}\color\green {7}\color\red {15}\color\green {31}\color\red {63}$$ We get a prime number for $n=2,3,6,10,23$ and no other such prime numbers with less than $10\ 000$ digits exist. Can we expect infinite many such primes ? What is the next number $n$ such that $f(n)$ is prime ? The following table shows the numbers $n$ upto $n=1\ 000$ , such that $f(n)$ has no prime divisor less than or equal to $611\ 953$ , the $50\ 000$ th prime. The second column shows the number of digits of $f(n)$ ? t=prod(j=1,50*10^3,prime(j));k=2;s=2^k-1;k=k+1;s=c(s,2^k-1);while(k<1000,k=k+1
;s=c(s,2^k-1);if(gcd(s,t)==1,print(k,""  "",length(digits(s)))))
6  8
10  21
18  60
22  87
23  94
45  334
47  363
66  698
99  1538
102  1631
118  2172
150  3484
161  4006
165  4205
183  5159
202  6272
215  7097
221  7495
238  8681
239  8753
258  10187
267  10904
273  11395
282  12152
293  13111
310  14666
317  15331
334  17008
339  17518
341  17724
370  20846
406  25074
425  27463
426  27592
435  28764
453  31182
471  33696
477  34556
497  37501
498  37651
501  38104
509  39326
510  39480
514  40099
526  41986
534  43267
543  44732
551  46054
555  46723
573  49790
575  50137
579  50834
581  51184
582  51360
591  52955
609  56219
615  57328
695  73154
717  77844
723  79149
729  80464
738  82457
742  83351
743  83575
749  84926
767  89044
783  92787
803  97575
846  108276
867  113703
875  115806
891  120070
911  125508
917  127163
934  131910
935  132192
939  133323
946  135313
959  139049
966  141081
?","['number-theory', 'mersenne-numbers', 'prime-numbers', 'elementary-number-theory']"
2688603,Why are Lebesgue measurable sets defined in connection to intersections with other sets?,"$A\subset\mathbb R^n$ is measurable if for all $B\subset \mathbb R^n$ $$m^*(B)=m^*(B\cap A)+m^*(B\setminus A)$$ This is just a definition, and it basically says that the outer measure can be partitioned into the outer measure of the intersection of set $A$ with any other set, and the outer measure of the intersection of that set with the complement of $A.$ But why is it defined as such? One thing it achieves is countable additivity: given two disjoint Lebesgue measurable sets $E$ and $F:$ $$\begin{align}m^*(E\cap F)&= m^*\left((E\cap F) \cap E \right ) + m^*\left((E\cap F) \cap E^c \right )\\[2ex]
&=m^*(E )+ m^*(F)
\end{align}$$ And relatedly, how does it connect with the alternative definition : For all $\epsilon>0$ there exist an open set $G$ and a closed set $F$ such that $F\subset A\subset G$ and $m^*(G\setminus F)<\epsilon$
  ?","['lebesgue-measure', 'measure-theory']"
2688744,"Example: $f(x,y)$ is not Lipschitz in y but still has a unique solution to initial value problem","Consider the differential equation $y^\prime =\sqrt{y}+1$ with $y(0)=0$. My question is how to prove the uniqueness of the solution. I found a hint for it as follows:
Consider $z(x)=(\sqrt{y_1(x)}-\sqrt{y_2(x}))^2$ where $y_1,y_2$ are solutions. I'm trying to use the hint to prove the uniqueness of solution, but I do not know what the hint means. My attempt: $z'(x)=2(\sqrt{y_1(x)}-\sqrt{y_2(x)}[(\sqrt{y_1(x)})'-(\sqrt{y_2(x)})']=(\sqrt{y_1(x)}-\sqrt{y_2(x)})\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)=(y_1'(x)-y_2'(x))\Big(\frac{y_1'(x)}{\sqrt{y_1(x)}}-\frac{y_2'(x)}{\sqrt{y_2(x)}}\Big)$. I couldn't proceed it further. Please give me slightly more comment or hint for the problem. Thanks in advance!",['ordinary-differential-equations']
2688763,Formula for the points in a grid of hexagons.,"Say I fill up the plane with regular hexagons whose side is distance 1, all packed together. Is there a formula or a pattern that gives all points? I let the $(0,0)$ be the center of the first hexagon, and the first six points are easy $(1,0),(-1,0),(1/2,\sqrt{3}/2),(-1/2,\sqrt{3}/2),(1/2,-\sqrt{3}/2),(-1/2,-\sqrt{3}/2)$. Now can each other point be extended from those? It is easy to handle moving sideways since all we have to do is just add one to the $x$ coordinate each time. But what about moving upwards? Thanks in advance.",['geometry']
2688832,Conditional Probability and Borel-Cantelli theorems,"I am given a sequence of events $A_1, A_2, \dots$ that satisfy $$
\sum_{n \geq k} \mathbb{P}\left(A_n \ \middle|\ \bigcap_{i=k}^{n-1} A_i^c \right) = \infty, \forall k \in \mathbb{N}
$$
and asked to conclude that $\mathbb{P}(\limsup_n A_n) = 1$. So far, I've tried: $$
\mathbb{P}(\limsup_n A_n) = \lim_{k\to\infty} \mathbb{P}(\bigcup_{n \geq k}A_n) = \lim_{k\to\infty} \mathbb{P}\left(
   \bigcup_{n \geq k}
   A_n \setminus \bigcup_{i=k}^{n-1}A_i
\right) \\
= \lim_{k\to\infty} \sum_{n \geq k} \mathbb{P}\left(
   A_n \cap \left( \bigcap_{i=k}^{n-1} A_i^c \right)
\right) \qquad \text{(union over disjoint events)}
$$ However, I'm not sure how to link the fact I'm given about the conditional probabilities with the above limit. Any ideas?","['probability-theory', 'limsup-and-liminf', 'measure-theory']"
2688847,Graph embeddable in g-torus (not using Euler),"I would like to know, if there is another way (beside the generalized Euler theorem) to know if a graph is NOT embeddable in a g-torus.","['graph-theory', 'discrete-mathematics']"
2688853,Understanding a result of Serre about zeros of $x^3 - x - 1$ in $\mathbb{F}_p$,"I'm trying to understand a result of Serre which relates the number of zeros in the finite field $\mathbb{F}_p$ of $f(x) = x^3 - x - 1$ to a modular form. The result can be found in the section 5.2 of the paper ""On a Theorem of Jordan"". Let $E = \mathbb{Q}[x]/(f(x))$ be the field obtained by adjoining a single root  of $f$, and let $L$ be the splitting field of $f$ over $\mathbb{Q}$. The Galois group $\textrm{Gal}(L/\mathbb{Q})$ is isomorphic to $S_3$. In the paper, Serre lets $\rho$ be the ""natural"" embedding of $S_3$ into $\textrm{GL}_2(\mathbb{C})$ (which I assume is the representation of $S_3$ as a dihedral group of rotation and reflection matrices) and considers the associated Artin-L function
$$L(\rho,s) = \sum_{n=1}^{\infty}\frac{a_n}{n^s}.$$
He goes on to say that we can characterize the above function by
$$L(\rho,s) = \zeta_E(s)/\zeta(s),$$
where $\zeta$ is the Riemann-zeta function and $\zeta_E$ is the Dedekind zeta function of the number field $E$. He says that this is equivalent to saying that $\rho \oplus 1$ (presumably $1$ denotes the trivial representation of $S_3$) is isomorphic to the 3-dimensional permutation representation of $S_3$. I understand that if $\nu$ is the permutation representation of $S_3$ (i.e. the regular representation), then the Artin $L$-function $L(\nu,s)$ is equal to the Dedekind zeta function $\zeta_L(s)$. In particular, the $L$-function associated to the trivial representation should just be the usual Riemann zeta function. I also know that the Artin $L$-function of a sum of representations is given by the product of the $L$-functions. So this should say that
$$\zeta_L(s) = L(\rho \oplus 1,s) = L(\rho,s)\zeta(s),$$
which would give us $L(\rho,s) = \zeta_L(s)/\zeta(s)$ instead. The other part I am having trouble with is the following: Serre claims that since $S_3$ is a dihedral group, Hecke's theory applies and shows that the power series $F(\tau) = \sum_{n=1}^{\infty}a_nq^n, q = e^{2\pi i\tau}$ is a cusp form of weight $1$ and level $23$. I can't seem to find the specific result of Hecke being used here and would like a reference. Thank you.","['number-theory', 'l-functions', 'modular-forms', 'proof-explanation']"
2688869,"Show that the set of non-$\beta$-Hölder functions is dense in the space of $\alpha$-Hölder function, $0<\alpha<\beta<1$","I want to show that $C^\alpha([0,1]) \setminus C^\beta([0,1])$ is dense in $C^\alpha([0,1])$ where $0<\alpha<\beta<1$. I know that I can show that $C^\beta$ is not dense here, using the function $x^\alpha$. To show that it's complement is dense, given some function $f \in C^\alpha$ and $\epsilon > 0$, I think I can use the function $f+\iota W$, where $\iota < \epsilon$ is chosen so that $f+\iota W \in C^\alpha$ but $f+\iota W \notin C^\beta$, where $W$ is Weierstrass' “sawtooth” function. I'm having a hard time making this idea rigorous however, and I was wondering if there are any easy ways to proceed.","['functional-analysis', 'real-analysis', 'continuity']"
2688870,Exercise (2.16) from Isaacs Character Theory of Finite Groups,"Let $H \le G$ and let $\chi$ be a character of $G$ which vanishes on $G-H$ . Assume that either $H=1$ or $G$ is abelian. Show $|G:H| | \chi(1)$ . Hint: Let $\lambda $ be an irreducible constituent of $\chi_H$ . Under either hypothesis, find $\mu \in Irr(G)$ with $\mu_H = \lambda$ . Compute $[\chi, \mu] $ and conclude $|G:H| |[\chi_H , \lambda ] $ . I am a bit lost at how the hint works: (0)  If $H=1$ then I think I can do it: then an irreducible constituent of $\chi_H$ is $1_H$ . This is the restriction of $1_G$ . We have $[\chi, 1_G] |G| = |H| [ \chi_H, 1_H]$ . Thus, $|G:H| | [\chi_H, 1_H] = \chi(1)$ . (i)  I am lost at how one find $\mu$ when $G$ is abelian. (ii)  I have $|G| [\chi, \mu] = |H|[\chi_H, \lambda]$ , hence $|G:H| | [ \chi_H, \lambda] $ . But I cannot end the proof from here.","['finite-groups', 'representation-theory', 'group-theory', 'proof-verification']"
2688907,Equal standard deviations for maximum probability of positive value?,"An investor wishes to invest $\$700$. There are two independent stocks the investor can choose to invest in, both of which are currently trading at the same share price. The daily returns of the first stock are historically normally distributed with a mean of $3\%$ and a standard deviation of $1.5\%$. The daily returns of the second stock are historically normally distributed with a mean of $4\%$ and a standard deviation of $2\%$. How much should the investor choose to invest (in dollars) in the first stock to maximize his probability of having a positive profit over the course of a day? The answer says when the standard deviations of returns are the same, the probability of profiting is maximized. Suppose he invests \$$x$ into the first stock, then $1.5x = 2(700-x)$, so $x = 400$. But why does this happen when the standard deviations are equal?","['statistics', 'standard-deviation']"
2688918,Proof verification: Why we can find $S\subset S_{\sigma_0}$ such that $\mu(S)>0$?,"Let $(X,\mu)$ be a measure space and $f_1,\cdots,f_d\in L^\infty(X)$. Set
$$g:=\displaystyle\sum_{k=1}^d|f_k|^2\;\;\text{and}\;\;c:=\|g\|_\infty.$$ Let $\sigma:=\{a_1,b_1,\cdots,a_d,b_d\}$ be such that $a_i,b_i\in \mathbb{Q}_+$ for all $i$. Set
$$S_\sigma=\left\{x \in X;\; \left[\Re(f_k(x))\right]^2>a_k,\; \left[\Im(f_k(x))\right]^2>b_k,\;\;k=1,\cdots,d\right\}.$$
and
$$\mathfrak{F}=\left\{\{a_1,b_1,\cdots,a_d,b_d\}\subset \mathbb{Q}_+^{2d};\;\;\sum_{k=1}^d (a_k+b_k) > c-\varepsilon,\;\forall \varepsilon>0\right\}.$$ By ( 1 ), we have
$A_\varepsilon \subset \bigcup_{\sigma \in \mathfrak{F}} S_\sigma,\;\forall \varepsilon>0$ with
$A_\varepsilon=\left\{x\in X;\; g(x)>c-\varepsilon\right\}.$ Since $\mu(A_\varepsilon)>0$ and $\mathfrak{F}$ is countable, then there exists $\sigma_0\in \mathfrak{F}$ such that $\mu(S_{\sigma_0})>0$. Why by subdivising $S_{\sigma_0}$, we can find $S\subset S_{\sigma_0}$ such that $\mu(S)>0$ and $\Re(f_j),\;\Im(f_j)$ keep a constant sign
   in $S$ for all $j\in\{1,\cdots,d\}$? My attempt We can subdivise $S_{\sigma_0}$ as
$$S_{\sigma_0}=B_1\sqcup B_2 \sqcup B_3 \sqcup B_4,$$
with $B_1:=\{x\in X;\;\Re(f_1(x))\ge 0,\;\Im(f_1(x)) \ge 0\}$; $B_2:=\{x\in X;\;\Re(f_1(x))\ge 0,\;\Im(f_1(x)) < 0\}$; $B_3:=\{x\in X;\;\Re(f_1(x))<0,\;\Im(f_1(x)) \ge 0\}$; $B_4:=\{x\in X;\;\Re(f_1(x))<0,\;\Im(f_1(x)) < 0\}$. So there exists $k_0\in \{1,\cdots,4\}$ such that $\mu(B_{k_0})>0$.
But I'm facing difficulties to end the proof.","['real-analysis', 'measure-theory', 'elementary-set-theory']"
2688972,"Finding all positive integers $x,y,z$ that satisfy $2^x=3^y7^z+1$","Find all positive integers $x,y,z$ that satisfy  $$2^x=3^y7^z+1$$. I think that $(x,y,z)=(6,2,1)$ is the only solution, But how can I prove this?",['number-theory']
2688983,Number of binary strings not having even-length runs of $0$ or $1$,"I was solving this algorithmic problem. This problem boils down to finding the finding the number of binary strings of given length $n$ such that those strings don't have any even-length run of $0$ or $1$. The post says that the answer is twice the $n$th term of the Fibonacci series, where the term numbering starts at $0$. I have tried myself to arrive at the same solution but without success. Any help/hint is appreciated.","['permutations', 'combinatorics', 'binary', 'fibonacci-numbers']"
2689028,A characterization of 'orthogonal' matrices,"(All matrices in this post are assumed to be real square matrices). Recently I answered a question on this site where one was asked to show that if $A$ is a symmetric matrix, then $U^{-1}AU$ is symmetric if $U$ is orthogonal. I wondered whether the converse was true as well. Concretely: Suppose that $U$ is an invertible matrix such that for any symmetric matrix $A$, we have that $U^{-1}AU$ is symmetric. Can we conclude that $U$ is orthogonal? So I tried to prove this and quickly realized this is almost true, indeed one can conclude that $UU^t=\lambda Id$ for some real number $\lambda$. (This is what I would call an orthogonal matrix, an orthonormal matrix is one such that $UU^t=Id$). I'll sketch my proof below: Let $A$ be symmetric and assume $U^{-1}AU$ is symmetric. Then $(UU^t)A=A(UU^t)$. For any symmetric matrix, the equality $(UU^t)A=A(UU^t)$ allows us to find some conditions on $(UU^t)$. So I choose the easiest basis of the symmetric matrices I could think of and plugged it in. After some calculations you're able to conclude $(UU^t)_{i,j}=0$ if $i\neq j$ and $(UU^t)_{i,i}=(UU^t)_{j,j}$ for all $i,j$. This is what we needed to show. There is nothing wrong with the above proof, it's just a brute force method. I was wondering whether anyone has a fun conceptual way of proving this. I'd very much like to see such a proof. EDIT: In particular, I'm looking for an argument that a student could find at the beginning of a linear algebra course. (Diagonalization and spectral theorem are a bit too advanced for now).","['matrices', 'orthogonality', 'linear-algebra', 'alternative-proof']"
2689067,Is c isometrically isomorphic $c \times c$?,"Let $c_0$ be the space of all sequences of scalars converging to $0$ with the supremum norm and $c$ be the space of all convergent sequences of scalars with the supremum norm. Is $c$ isometrically isomorphic $c \times c$? Here $c \times c$ is given the max norm: $||(\{x_n\},\{y_n\})||=\max (||(\{x_n\},||(\{y_n\}||)$. This must have appeared on MSE but I couldn't locate it. $c_0 \times c_0$ is obviously isometrically isomorphic to $c_0$ and there cannot be an isometric isomorphism of $c$ onto $c \times c$ that maps $c_0$ onto $c_0 \times c_0$ as seen by a consideration of codimensions. But I am unable to see if c isometrically isomorphic to $c \times c$","['functional-analysis', 'isometry']"
2689087,Is $f(x) = x^2$ surjective on $\mathbb N$? Improving proof notation,"Definition of surjective: Let $A$ and $B$ be sets and let $f: A → B$ be a function.
$f$ is surjective if for each $b ∈ B$ there is some $a ∈ A$ such that $f(a) = b$ Solution attempt: In this case, $f(x) = x^2$,  $A$ and $B$ are both $\mathbb N$ $f: A → B$ $f: ℕ → ℕ$ Let $b = 3$
There is no $a ∈ ℕ$ s.t. $f(a) = a^2 = b $ Therefore $f(x) = x^2$ is not surjective on $ℕ$ Is this proof correct?, and if so, what notation can I use to make the distinction between an element of $ℕ$ as a pre-image and another as image of that element. In this case solved as ""$a$"" being a pre-image of $f: ℕ → ℕ$ and ""$b$"" the image of a on $f$.","['elementary-set-theory', 'proof-verification']"
2689164,What has more potential: Differential algebraic geometry or Diffiety theory?,"Algebraic geometry is said to be useful to study not only specific solutions of polynomial equations but to understand the intrinsic properties of the totality of solutions of a system of equations. However, varieties (which are the central object of study in algebraic geometry) are generalisations of solution sets of polynomial equations and thus do not seem suitable for investigating (intrinsic properties of solutions of a system of) differential equations. But precisely the investigation of differential equations in such an abstract manner would probably enable great applications in physics and computer science. While looking for approaches that generalise the notions of algebraic geometry to the study of differential equations I basically came across the following two (if someone knows more, please let me know as well): Differential algebraic geometry Diffiety Theory However, the information provided on the wiki-sides are not very helpful. Furthermore, I did not find anything on math.SE about diffieties. I found this short introduction to the idea of a diffiety on the ncat-lab but it does not say much about the applications of the idea and does not contrast it with differential algebraic geometry. My questions are: Does anyone know both approaches well enough to compare their methods and applications? If so, please share this knowledge. If I want to learn one of these approaches, which one should I choose? And what would be good books to start? PS: I found a link to a so-called diffiety institute which provides access to a lot of writings on diffiety theory. And here are some more information on differential algebra.","['topos-theory', 'algebraic-geometry', 'reference-request', 'category-theory', 'ordinary-differential-equations']"
2689167,Limit of $\frac{x^c-c^x}{x^x-c^c}$ as $x \rightarrow c$,"My question is: Show that $\lim_{x \rightarrow c} \frac{x^c-c^x}{x^x-c^c}$ exists and find its value. Because the limit is 0/0 I've tried using L'Hopital's rule, but every time I differentiate it I still get 0/0? We're given this hint in the question: use the fact that $a^b=\exp(b\log(a))$ and I've tried doing this to be able to differentiate and use L'Hopital's rule, and also for trying to rearrange it. Can anyone help please?","['real-analysis', 'limits']"
2689178,Stokes' theorem and non-globally defined quantities,"This is a question from a physicist, so please be kind.* Suppose that $M$ is an orientable smooth manifold without boundaries and $\omega$ a form of an appropriate degree such that it can be integrated over $M$,
$$
I=\int_M \omega.
$$
The objective is to compute $I$. According to the Poincaré or Dolbeault-Grothendieck lemma (for real and complex manifolds respectively), locally (in some coordinate neighbourhood, $U_i$, where $M$ looks like an open subset of $\mathbb{R}^n$ or $\mathbb{C}^n$, with $n$ a positive integer, and because i don’t want to restrict the scope i will write $\mathbb{K}$ for either of the two fields) $\omega$ is exact,
$$
\omega_i=dA_i,
$$ 
where $A_i$ is some differential form defined in $U_i$ of the appropriate degree. If $A_i$ were globally defined (in which case I suppose, $A_i=A$, for all $i$) then we could apply Stokes' theorem,
$$
\int_MdA = \oint_{\partial M}A=0,
$$
and learn that $I=0$ (because $\partial M$ is null). I am in the unfortunate (and I believe very common) situation where I only know $\omega$ locally (in particular I have an explicit expression for $A_i$) and I want to compute $I$. So my first question is the following: What precisely does it mean for $A_i$ to be only locally and not globally defined? How can I check whether my $\omega_i$ is globally or only locally exact given only an explicit local expression for $A_i$ and the corresponding transition functions on chart overlaps? For instance, might it be true that in order for $A_i$ to be globally defined it must transform under changes of coordinates as an antisymmetric tensor (on chart overlaps, $U_i\cap U_j$), and might this be sufficient for $A_i$ to be globally defined? Suppose now that I am in the fortunate situation where I know the answer to this question, and I have concluded that $A_i$ is not globally defined and hence that $\omega$ is not globally exact. The next question is: How can I explicitly reconstruct $I$ given the explicit local expression $\omega_i=dA_i$ on $U_i$ and the corresponding transition functions on patch overlaps $U_i\cap U_j$? To be slightly more precise here I am implicitly considering an atlas for $M$, i.e. a family of charts, $\{(U_i,\phi_i)\}$, with $\{U_i\}$ a family of open sets such that $\cup_i U_i=M$, and $\phi_i:U_i\rightarrow \mathbb{K }$ a homeomorphism from $U_i$ to an open subset of $\mathbb{K}$ (in particular, the maps $\phi_i$ are to be considered known and identified with a convenient set of local coordinates). The case of interest is when the transition functions $f_{ij}=\phi_i\circ \phi_j^{-1}$ from $\phi_j(U_i\cap U_j)$ to $\phi_i(U_i\cap U_j)$ are $C^{\infty}$ and also known. For question 2 Stokes' theorem comes to the rescue, given that in a patch $U_i$,
$$
\int_{U_i}dA_i = \oint_{\partial U_i}A_i,
$$
but how exactly does one sum over $i$ to reconstruct the full integral $I$ making use of the transition functions which map $A_j$ to $A_i$ on patch overlaps? *I have rewritten the question completely because following a fairly extensive discussion with @John Hughes (whom I think I annoyed quite a bit, see below) and his correspondingly good comments, it became very clear that my intended question was not clearly formulated.","['riemann-surfaces', 'de-rham-cohomology', 'differential-geometry', 'differential-topology']"
2689184,"If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, is the differential necessarily one-to-one?","If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, $C^1$, and has everyhere non-singular Jacobian, then for any open set $U \subset \mathbb{R}^n$, the image $f(U)$ is open I proved the above theorem using the inverse function theorem. However, I stumbled upon a proof online that removed the necessity of the Jacobian being everywhere non-singular, instead only specifying $f$ to be one-to-one and $C^1$ The proof makes sense to me, except for one statement: ""an injective map has an injective derivative"" If $f: \mathbb{R}^n \to \mathbb{R}^n$ is one-to-one, is the differential necessarily one-to-one? Here is a link to the mentioned proof: https://gist.github.com/pervognsen/11251717","['multivariable-calculus', 'real-analysis', 'proof-verification']"
2689196,Linear differential equation on $\mathbb{R}^2.$,"Let $l_1, l_2, r_1, r_2 $ be negative real numbers such that $r_1r_2 \neq 0$ and consider the two matrices
$$L = \left(\begin{matrix} l_1&0\\ 0& l_2 \end{matrix} \right),\,\, 
R= \left(\begin{matrix} r_1&0\\ 0& r_2 \end{matrix} \right).$$ Let now $g: \mathbb{R}^2 \to \mathbb{R}^2, g(X) = |X|^2 X,$ 
where $|.|$ is the Euclidean norm in $\mathbb{R}^2.$ I'd like to find a solution $f:\mathbb{R}^2 \to \mathbb{R}^2$ to the differential equation $$Df(X) LX + Rf(X) = g(X),$$
where $Df(X)$ denotes the Frechet derivative at the point $X.$ 
The idea in this setting is to introduce $t\mapsto Y(t)$ verifying 
$$\frac{d}{dt}Y = LY, \,\, Y(0) = X.$$ 
The well known candidate is $Y(t) = e^{tL}X.$ 
Hence we can write 
$$\partial_t \left(e^{tR}g(Y)\right) = e^{tR} Dg(Y)LY + Re^{tR}g(Y).$$
Integrating over $(0, +\infty)$ the above equality and denote $f(X) = - \int_0^\infty e^{tR}g(e^{tL}X)dt$, we find that $$g(X) =g(Y(0)) - \lim_{t\to +\infty}e^{tR}g(Y(t)) = Df(X)LX+Rf(X).$$ Remark: The above integral defining $f$ is convergent since 
$$| e^{tR}g(e^{tL}X)| \leq e^{t \max(r_1, r_2)}|X|^3.$$ My question is: Is there a similar method to give a solution to the differential equation 
$$Df(X) \tilde{L}X + \tilde{R}f(X) = g(X),$$ where 
$$ \tilde{L} = \left(\begin{matrix} l_1&0\\ 0& -l_2 \end{matrix} \right),\,\, 
\tilde{R}= \left(\begin{matrix} r_1&0\\ 0& -r_2 \end{matrix} \right).$$ Thank you for any hint.","['ordinary-differential-equations', 'differential-geometry', 'differential-topology']"
2689236,Equivalent Relation,"Given the following problem: Consider the monoid $(\mathbb{Z}, T)$ with the operation $aTb=a+b-a*b$ and the equivalence relation $x \equiv y (\mod{n})$ if and only if $x-y=kn$. Prove that this relation is also compatible with the operation $T$. If I remember correctly, the the equivalent relation between two elements means that they have the same value when operated on by the function. But this one seems to be causing me some confusion and I am not sure where to begin. How do I go about proving this?","['monoid', 'discrete-mathematics']"
2689240,Lagrange spectrum in diophantine approximation theory,"Context. Hurwitz' theorem states that for every irrational $\xi$, there is infinitely many rationals $p/q$ such that $$\left\vert \xi-\frac pq\right\vert<\frac 1{q^2\sqrt 5}.$$ The number $\sqrt 5$ is the best constant possible here, in the sense that if $C>\sqrt 5$, then there exist irrationals $x$ such that $$\left\vert x-\frac pq\right\vert<\frac 1{q^2C}$$ is not satisfied for infinitely many rationals $p/q$. Now, let's define for a real number $x$, $L(x)$ to be the biggest constant such that $$\left\vert x-\frac pq\right\vert<\frac 1{q^2L(x)}$$ for infinitely many $p/q$. As it is said here , an equivalent definition would be $$L(x)=\left(\liminf_{q\to\infty} q^2\vert x-p/q\vert\right)^{—1}.$$ For example, $$L\left(\frac {1+\sqrt 5}2\right)=\sqrt 5,\quad L(\sqrt 2)=2\sqrt 2.$$ The question. I am interested in the set $$L:=\{L(x),\ x\in \mathbb R\}$$ which is called the Lagrange Spectrum . We already know that the part of $L$ lying in $[\sqrt 5,3)$ is discrete, and the final part lying in $[F,+\infty)$ where $$F=\frac {2\,221\,564\,096+283\,748\sqrt{462}}{491\,993\,569}\approx 4.5278$$ is Freiman's constant, is continuous. We also know that the part lying between those two parts as a fractal structure. Since I have found it nowhere, I am asking here: Is there any graphical visualization of the set $L$? If not, would it be possible to make one?","['number-theory', 'visualization', 'diophantine-approximation', 'fractals']"
2689266,Prove that $T$ is bounded and onto.,"I need just a hint, not a whole solution. Problem: Let $$T:C[0,1]\to c$$ where $c$ is the space of the convergent sequences and $$T(f)=\left(f\left(\frac{1}{n}\right)\right)$$ prove that $T$ is bounded surjection. What I have done: The boundedness is easy to check. For the sujectivity lets assume that $a_{n}=f(n)\in c$ so I take $(f\circ g)(n)$ where $g(n)=\frac{1}{n}$ so 
  $$ T((f\circ g)(n))=\left(f\left(\frac{1}{n}\right)\right)$$
  But the problem is that I cant show that $f\circ g$ belongs to $c$. 
  How can I handle this problem? Is my approach true or I should change?","['functional-analysis', 'real-analysis', 'operator-theory', 'analysis']"
2689276,Finding an unbiased estimator of $e^{-2\lambda}$ for Poisson distribution,"If $X_1,X_2,\ldots,X_n\sim \mathrm{Pois}(\lambda)$, find an unbiased estimator of $e^{-2\lambda}$. I am actually supposed to find the UMVUE of $e^{-2\lambda}$ but I first have to find its unbiased estimator. I tried using the MLE of $\lambda$ which is $\hat{\lambda}:= \frac{1}{n}\sum_{i=1}^n X_i$ but I'm not sure where to go from there. I know that by invariance property that $e^{-2\hat{\lambda}}$ will be the MLE of $e^{-2\lambda}$ but I'm not sure if it is also unbiased.","['probability-theory', 'poisson-distribution', 'statistical-inference']"
2689294,How to evaluate $\lim_{x \rightarrow \infty} \left(\left(\frac{x+1}{x-1}\right)^x -e^2\right){x}^2$,"I want to find the limit of the following expression: $$\lim_{x \rightarrow \infty} \left(\left(\frac{x+1}{x-1}\right)^x -e^2\right){x}^2.$$ I got the limit of the first term as $e^2$ so the problem now becomes equivalent to evaluating a $0 \times \infty$ limit, but I'm having difficulty proceeding after this. I tried applying L'Hospital's rule but the derivative gets really complicated after a while. I also tried applying series expansions but that too got me nowhere. Could anyone please tell me how to proceed with this problem?","['real-analysis', 'taylor-expansion', 'exponential-function', 'limits']"
2689357,Jacobian of a quadratic vector field,"I am trying to calculate the partial derivative of a function of several variables, and it is pushing my understanding of matrix calculus. The function is the following $$f(x) = M D(x) R x$$ where $D(x)$ is the diagonal matrix with the vector $x = (x_1,\dots,x_n)$ on the main diagonal, and $M$ and $R$ are $n \times n $ real matrices. What I am looking for is the matrix of partial derivatives $$\frac{\partial f(x)}{\partial x_i}$$ I can derive this by expanding the above into non-matrix notation, but it is quite messy and I can't figure out how to simplify it. Ideally I'd like to have $\partial f(x) / \partial x_i$ in terms of $M$ and $R$. I'm hoping this is a fairly straightforward application of matrix calculus rules, but I can't seem to find any useful way of dealing with this combined function of matrix. Thanks!","['derivatives', 'matrices', 'matrix-calculus', 'partial-derivative', 'jacobian']"
2689374,Derivative of eigenvectors of a symmetric matrix-valued function,"Given a real symmetric $3\times3$ matrix $\mathsf{A}_{ij}$ and its derivative (w.r.t. some parameter, let's call it time ) $\dot{\mathsf{A}}_{ij}$, I want to measure/obtain the rotation (rate and direction) of the eigenvectors (the eigenvectors of a real symmetric matrix form an orthonormal matrix). How can this be done? Edit Since the eigenvectors of a real symmetric matrix are mutually orthogonal, the change of the eigenvectors can only be an overall rotation. An infinitesimal rotation is uniquely determined by the rate $\boldsymbol{\omega}$ such that $\dot{\boldsymbol{x}}=\boldsymbol{\omega}\times\boldsymbol{x}$ for any vector $\boldsymbol{x}$. My question then becomes how to obtain $\boldsymbol{\omega}$.","['derivatives', 'eigenvalues-eigenvectors', 'symmetric-matrices', 'matrix-calculus']"
2689415,For what values of $k \in \mathbb{Z}\setminus\{0\}$ does there exist an unbiased estimator of $e^{k \lambda}$?,"This is inspired by Finding an unbiased estimator of $e^{-2\lambda}$ for Poisson distribution , reminding me of a qualifying exam question that I was frustrated with. Suppose $X_1, \dots, X_n \overset{\text{iid}}{\sim}\text{Poisson}(\lambda)$. For some subset of size $k \leq n$, it can be seen that if $\mathbf{I}$ is the indicator function that
$$\mathbf{I}(X_{i_1}+X_{i_2}+\cdots+X_{i_k}=0)$$
is an unbiased estimator of $e^{-k\lambda}$ for $k \geq 1$. The OP of the linked question above asked an interesting question: does there exist an unbiased estimator of $e^{2\lambda}$? More generally, Is there an unbiased estimator of $e^{k\lambda}$ for $k > 0$? In the comments to the link above, I told the OP I wouldn't know where to begin with this. My first thought was to try finding the distribution of the reciprocal of a Poisson distribution. Say $X \sim \text{Poisson}(\lambda)$, then
$$f_{1/X}(y)=\dfrac{e^{-\lambda}\lambda^{1/y}}{(1/y)!}$$
(what is even the support of this thing?)
and I suppose $(1/y)!$ would have to be extended in cases where $1/y$ isn't an integer... so basically, every case except $y = 1$, and then we'd have to use the Gamma function. But even with this, using indicator functions (as above) will still give $e^{-k \lambda}$ for $k \geq 1$; it doesn't help the problem much.","['parameter-estimation', 'poisson-distribution', 'probability', 'probability-distributions']"
2689420,Surjectivity of an operator on $L^2(R)$ defined as identity minus convolution,"I'm having trouble with the following exercise. Define $T:L^2(\mathbb{R})\to L^2(\mathbb{R})$ by means of the formula $$(Tf)(x)=\int_{0}^{1}f(x+y)dy.$$ I proved that, for all $f\in L^2(\mathbb{R})$, $Tf$ is a continuous function and $||Tf||_2\le||f||_2$, with equality iff $f(x)=0$ almost everywhere. Then there is the following question, on which I'm stuck: is $S:=I-T$ surjective? Here, $I$ is the identity operator. This should be doable at the level of ""Papa Rudin"". Edit: Thanks to the last paragraph in @FTP's great answer, I think I have an idea for a more ""elementary"" (but more complicated) solution. We know that $X:=L^1(\mathbb{R})\cap L^2(\mathbb{R})$ is dense in $L^2(\mathbb{R})$. Suppose $S$ is surjective. Since $S$ is injective and continuous, by the Open Mapping Theorem, it follows that $S$ is invertible. Now, let $g\in X$ and $f=S^{-1}g$. By density, there is $\{f_n\}\subset X$ such that $f_n\to f$ in $L^2(\mathbb{R})$. Write $g_n=Sf_n$. Since each $f_n$ is integrable, it follows by Fubini-Tonelli that each $g_n$ is integrable, and we actually have $$\int_{\mathbb{R}}g_n(x)dx\int_{\mathbb{R}}(Sf_n)(x)dx=0.\quad (*)$$ Also, by the continuity of $S$, we obtain that $g_n\to g$ in $L^2(\mathbb{R})$. If we choose $g$ very nice with positive integral, can we obtain contradiction with equation (*)?","['functional-analysis', 'lp-spaces']"
2689457,Example of an infinite dimensional Hilbert space that is not an RKHS,"I have been studying Reproducing Kernel Hilbert Spaces (RKHS). The definition I am using is as follows: An RKHS is a Hilbert space $\mathcal{H}$ of real-valued functions on a set $X$ such that for all $x \in X$, the evaluation functional $E_x : \mathcal{H} \to \mathbb{R}$ defined by $E_x(f) = f(x)$ is continuous. (Recall that continuity in this context means $\sup_{\|f\|\leq 1}|f(x)| < \infty$ for all $x$.) As one of the first steps in my study I want to see some positive and negative examples of these spaces. So I want to find a non-trivial (meaning infinite dimensional) space of functions which is a Hilbert space but not an RKHS, meaning the evaluation functionals are not continuous. The problem is that in all the references I find, the only example they give is the space of functions $L_{2}$. However, this example is trivial since $L_2$ fails for the simple reason that it is not a space of functions, rather it contains equivalence classes of functions. Can someone please help me find an infinite dimensional Hilbert space of functions that is not an RKHS?","['functional-analysis', 'hilbert-spaces', 'reproducing-kernel-hilbert-spaces']"
2689497,Lyapunov Function for Nonlinear Systems,"Since there is no systematic method to find Lyapunov functions, how should I approach the question shown below to find corresponding Lyapunov function? With $\alpha<0$ $$\begin{align}  x_1'&=-3x_2\\
    x_2'&=x_1-\alpha(2x_2^3-x_2)\\     \end{align}
    $$","['control-theory', 'ordinary-differential-equations', 'nonlinear-system', 'lyapunov-functions']"
2689590,A question regarding dense subsets,Let $M$ be a metric space. Let $A$ and $B$ be dense subsets of $M$ such that $A$ is open. Then prove that $A \cap B$ is dense in $M$. I’m really stuck at this problem. Any hints or solutions will be highly appreciated.,"['general-topology', 'metric-spaces']"
2689620,"Proving Sturm's separation theorem: if $y_1, y_2$ are fundamental solutions of $y''+py+q=0$, their zeros alternate.","This is a problem from Braun's Differential Equations book. Let $p(t)$ and $q(t)$ continuous functions and $y_{1}$, $y_{2}$ a fundamental set of solutions of the ODE
\begin{equation}
y''+p(t)y'+q(t)y=0
\end{equation}
in the interval $t\in(-\infty,\infty)$. Prove that there is only one zero of $y_{1}$ between two consequtive zeros of $y_{2}$.
Hint: Differentiate the expression $y_{2}/y_{1}$ and use Rolle's theorem. This is what I have. Let $a,b$ two consecutive zeros of $y_{2}$ with $a<b$. Since $y_{1}$, $y_{2}$ are a fundamental set of solutions, their Wronskian
\begin{equation}
W(y_{1},y_{2})=y_{1}y_{2}'-y_{1}'y_{2}
\end{equation}
 is not zero for all $t$. This implies that neither $a$ or $b$ are zeros of $y_{1}$. Then, I tried to use the hint computing the derivative of $y_{2}/y_{1}$,
\begin{equation}
\dfrac{d}{dt}(y_{2}/y_{1})=W(y_{1},y_{2})/y_{1}^{2}
\end{equation}
but then I have no idea of what to do, thanks for the help.","['roots', 'ordinary-differential-equations', 'calculus']"
2689631,What are all finite groups such that all isomorphic subgroups are identical?,"There is a simple argument that shows that any two subgroups of a cyclic group that are isomorphic must be identical. This is because they can each be represented in terms of the generator of the cyclic group. This made me wonder, what are all finite groups with the property that any two isomorphic subgroups are identical? My conjecture is that they must all be cyclic. Is this correct?","['finite-groups', 'group-theory']"
2689648,Inequality mod p for all primes implies equality.,"Let $n$, $m$ positive ($n,m>0$) integers such that $n \le m\bmod{p}$ for all primes $p$, prove that $n=m$. To clarify, the above inequality is taken using the representatives $\{0,1...,p-1\}$ of the class of remainders modulo $p$","['number-theory', 'elementary-number-theory']"
2689743,Logic using Permutation,"I have this question in front of me: How many different four letter word can be formed using the letters of “power” such that at least one letter is repeated within the new word. I solved this using this concept: I have four places, so at each place there can be 5 letters placed, assuming 2 places as one (because the letted in both the places is same) and multiplying $5×5×5$ I get 125.
Now because in those 2 places any of the 5 nos. can come so I will multiply 125 by 5 again which gives me 625, but this is wrong... the correct ans is 505.
Kindly help me where I am wrong.","['permutations', 'combinatorics']"
2689746,What is the characteristic function of a p.d.f with $\mathbb{R}^+$ support?,"Considering a generic p.d.f. $p(x)$, we know that the characteristic of such distribution is given by $\varphi(t)=E[e^{i t x}]$. Here $t,x\in (-\infty,\infty)$. Now I would like to ask what is the characteristic function of a p.d.f. where its support is not the whole real numbers, for instance, the support is $\mathbb{R}^+$. I already understand that as follows: Assume a two dimensional distribution $f(x,y)$ in $\mathbb{R}^2$ with characteristic function $E[e^{i(k_x x+k_y y)}]$. By using polar coordinates $x=r \cos \theta$ and $x=r \sin \theta$ and averaging over $\theta$ we find
$$ p(r)=r\int_{0}^{2\pi} d\theta\, f(r\cos\theta,r\sin\theta). $$
Now the support of the distribution $p(r)$ is $\mathbb{R}^+$. In addition, we can average out the characteristic function too,
$$ \frac{1}{2\pi} E\left[ \int_{0}^{2\pi}d\theta\, e^{i r k\cos(\theta-\theta_k)}\right]=E[J_0(k\, r)],$$
where in the above $k_x=k \cos \theta_k$ and $k_y=k \sin \theta_k$. In the above, $J_0(k\, r)$ is the Bessel function of the first kind. In other words, I think the characteristic function for a general distribution $p(r)$ with $\mathbb{R}^+$ is $E[J_0(k\, r)]$. I am not sure that the above conclusion is true. And if it is true I am not sure it is unique. Also what is the systematic way to find the above result?","['probability-theory', 'statistics', 'probability-distributions']"
2689755,Solve pde problem,"For each of the following PDE. (a) solve the characteristic equation (b) define a transformation of the PDE. And obtain the transformed equation. (c) find the general solution of the transformed equation. $$xu_x-yu_y+u =x $$
Let $r=r(r,s) ,s=s (r,s) $
Char. eq. :
$$\frac{dy}{dx}= \frac {-y}{x} $$
$$\frac{dy}{y}= \frac {-dx}{x} $$
$$lny =-lnx + c$$
$$s=c =lnx+lny=lnxy $$
let r=x
$$u_x=u_r +\frac {u_s}{x}$$
$$u_y=0 +\frac {u_s}{y}$$ Substitute in eq.$(r=x,s=lnxy)$
$$ru_r+u_s -u_s+u =r $$
$$ru_r+u=r $$
First one homogeneous  eq
$$u_r+\frac {u}{r}=0$$
$$M= exp ( \int (1/r))=r$$ $$ru_r+u=0$$
$$u = \frac {F (s)}{r}=\frac{F (lnxy) }{x}$$ How to find nonhomogenes  for this problem? 
Thanks","['ordinary-differential-equations', 'characteristics', 'partial-differential-equations']"
2689794,"Probability $\mathbb{P}[X_N = 1 | X_0 = 1]$ for a Markov Chain over $\mathbb{X} = \{1,2,3\}$","Exercise : Let $\{ X_n \}_{n \in \mathbb{N}_0}$ be a Markov Chain over the space $\mathbb{X} = \{ 1,2,3 \}$ with the following transition matrix :
  $$P = \begin{pmatrix} 0 & 1/2 & 1/2 \\ 1/3 & 1/3 & 1/3 \\ p & 2/3 -p & 1/3 \end{pmatrix}$$
  Calculate the probability $\mathbb{P}[X_n=1|X_0=1]$ for all $n \in \mathbb{N}$ for the cases of a) $p=0$, b) $p =1/6$, c) $p=2/3$. How would you calculate $\lim_{n\to \infty} P^n$ without many operations ? Attempt : First of all, I start by finding the eigenvalues and eigenvectors for $P$, which are : $$\det(P-\lambda I) = 0 \Rightarrow \begin{vmatrix} 0 & 1/2 & 1/2 \\ 1/3 & 1/3 & 1/3 \\ p & 2/3 -p& 1/3 \end{vmatrix} = 0 \Leftrightarrow \dots \Leftrightarrow \lambda = \begin{cases} 1 \\ \frac{1}{6}\big(\pm\sqrt{6p-1}-1\big)\end{cases}$$ The problem I have though, is that for example, the case $p=0$ yields complex eigenvalues, which I don't know how to handle in terms of probabilities to continue on. Also, in the case $p=1/6$ we have a Jordan case, which still leaves us with a hard case for $P^n$. Any tips on how to calculate the given probability ?","['stochastic-processes', 'markov-chains', 'probability-theory', 'markov-process']"
2689803,Is a sequence of bounded random variables uniformly integrable?,"It seems to me that a sequence of uniformly bounded random variables $\{X_n\}_{n=1}^\infty$ on a probability space $(\Omega_n, \mathcal{F}_n, P)$ such that $|X_n(\omega)| \leq M$ for all $\omega \in \Omega_n$ is uniform integrable. For if $I$ is the indicator function, suppose $\varepsilon > 0$ is given and let $k = 2M$, then
$$
E(|X_n| I(|X_n| \geq k)) = E(|X_n| I(|X_n| \geq 2M)) = E(|X_n|\cdot 0) = 0 < \varepsilon
$$
Is it true that a sequence of uniformly bounded random variables are uniformly integrable?","['uniform-integrability', 'probability-theory', 'probability', 'random-variables']"
2689897,Cauchy-Riemann equations for a holomorphic function given in polar form,"Suppose $f(re^{i\theta})=R(r,\theta)e^{i\Theta(r,\theta)}$ describes a holomorphic function. Is there a Cauchy-Riemann system that $R(r,\theta)$ and $\Theta(r,\theta)$ must satisfy? I've seen this question answered for the related case $f(re^{i\theta})=U(r,\theta)+iV(r,\theta)$ but not when $f$ is given in polar form. One way to proceed here is to write $$f(re^{i\theta})=\underbrace{R(r,\theta)\cos\big(\Theta(r,\theta)\big)}_{U(r,\theta)}+i\underbrace{R(r,\theta)\sin\big(\Theta(r,\theta)\big)}_{V(r,\theta)}$$ and then use the Cauchy-Riemann equations \begin{equation}\label{eq:1}\frac{\partial U}{\partial r}=\frac{1}{r}\frac{\partial V}{\partial\theta}~\text{ and }~\frac{\partial V}{\partial r}=-\frac{1}{r}\frac{\partial U}{\partial\theta}\end{equation} which were alluded to above. This approach doesn't seem to lead to a nice first order system comparable to the Cauchy-Riemann equations, though. Perhaps there a simplification I'm missing? Has someone seen anything like this in the literature?","['polar-coordinates', 'complex-analysis', 'holomorphic-functions', 'conformal-geometry', 'cauchy-riemann-equations']"
2689907,Holomorphic symplectic manifold,"I have two basic questions regarding hyperkaehler manifolds. A holomorphic symplectic manifold is a complex manifold $X$ endowed with a $(2,0)$ -form $\omega$ .
I know that a Hyperkaehler manifold can be seen as a holomorphic symplectic manifold. But is the converse also true? Is every holomorphic symplectic manifold always Hyperkaehler? If not, can you give a counterexample? Does a Hyperkaehler manifold possess a natural polarization? What is it? Thanks in advance for your answers.","['complex-geometry', 'kahler-manifolds', 'differential-geometry', 'symplectic-geometry']"
2689947,Can we prove $\sin(x)^2 + \cos(x)^2 = 1$ using just their series?,"In particular I mean: $$\sin(x)^2 + \cos(x)^2$$ $$=\left(\sum_{n=0}^{\infty} (-1)^n \frac{x^{2n+1}}{(2n+1)!}\right)^2 + \left(\sum_{n=0}^{\infty} (-1)^n \frac{x^{2n}}{(2n)!}\right)^2$$ However I am not sure how you're supposed to correctly expand and recombine terms when dealing with the sum of two squared series, especially when there are factorials involved. Edit: To be clear, I am asking about manipulating the series I have just stated in order to show that they sum to $1$.","['summation', 'trigonometry', 'sequences-and-series', 'proof-explanation']"
2689952,Einstein's convention and Hamilton's equations in $\Bbb R^3$.,"Consider $F = -\nabla U$ a conservative force field in $\Bbb R^3$ . Assume we describe the motion of a unit mass particle under this force field by a curve $q(t) = (q^1(t), q^2(t), q^3(t))$ . We have Newton's law $$-\frac{\partial U}{\partial q^i}(q(t)) = \ddot{q}^i(t), \quad i=1,2,3,$$ which is a second order differential equation in $\Bbb R^3$ , and we want to convert that into a first order differential equation in $\Bbb R^6$ by calling $p^i \doteq \dot{q}^i$ , so that we have $$\begin{cases} \dot{q}^i(t) = p^i(t) \\[1em] \dot{p}^i(t) = -\dfrac{\partial U}{\partial q^i}(q(t)), \end{cases}$$ great. Question: How can I write indices according to Einstein's summation convention (for psychological reasons, I'll still write summations signs)? The index $i$ in $p^i$ is covariant or contravariant? If $i$ goes ""in the ceiling"" as $p^i$ , then $\dot{p}^i = -\partial U/\partial q^i$ is bad. If $i$ goes ""in the cellar"" as $p_i$ , then $p_i = \dot{q}^i$ is bad. Is there an identification being made? If $M$ is a manifold and $(U, (x^i))$ is a chart, then $(TU, (x^i, \xi_i))$ is a chart in $TM$ , and since we see $\Bbb R^6$ as $T\Bbb R^3$ , this would indicate to use $p_i$ . I'm confused.","['tensors', 'symplectic-geometry', 'differential-geometry', 'hamilton-equations', 'index-notation']"
2689957,Reducing the number of parameters of an ODE system through nondimensionalization,"I am trying to reduce the number of parameters of the ODE system:
  \begin{align}
\label{eq:Iprime}
I'(t)&= \sigma B-\mu I\\
B'_1(t)&=r_1 B_1\left(1-\frac{B_1 }{K_1}\right)-d_1IB_1-m (B_1-B_2) \\
\label{eq:B2prime}
B'_2(t)&=r_2 B_2\left(1-\frac{B_2 }{K_2}\right)-d_2IB_2-m (B_2-B_1) 
\end{align}
  (where $B=B_1+B_2$) by applying nondimensionalization. In all the examples I have seen this reduced the number of parameters. However if I apply the technique to my problem the number of parameters remain the same! Why is this the case? Work thus far We assume the following scaling: 
$$I=\widetilde{I}\widehat{I}  \quad B_1=\widetilde{B}_1\widehat{B}_1 \quad B_2=\widetilde{B}_2\widehat{B}_2  \quad t=\tilde{t} \hat{t} $$ where $\widetilde{I}, \widetilde{B}_1, \widetilde{B}_2, \tilde{t}$ are constants (dimension-carrying), to be chosen, and $\widehat{I}, \widehat{B}_1, \widehat{B}_2, \hat{t}$ are the dimensionless variables. Subbing these into our ODEs leads to 
\begin{align}
\frac{d(\widetilde{I}\widehat{I})}{d(\tilde{t} \hat{t} )}&=\sigma  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\mu \widetilde{I}\widehat{I}\\
\frac{d \widehat{I}}{d \hat{t}}&=\frac{\tilde{t}\sigma}{\widetilde{I}}  ( \widetilde{B}_1\widehat{B}_1 + \widetilde{B}_2\widehat{B}_2)-\frac{\mu \tilde{t}}{\widetilde{I}}  \widehat{I} \\
\frac{d \widehat{I}}{d \hat{t}}&=\left[\frac{\tilde{t}\sigma}{\widetilde{I}}   \widetilde{B}_1\right] \widehat{B}_1 + \left[ \frac{\tilde{t}\sigma}{\widetilde{I}} \widetilde{B}_2 \right] \widehat{B}_2-\left[\frac{\mu \tilde{t}}{\widetilde{I}}\right]  \widehat{I} \\
\end{align} \begin{align}
\frac{d(\widetilde{B}_1\widehat{B}_1)}{d(\tilde{t} \hat{t} )}&=r_1 \widetilde{B}_1\widehat{B}_1\left(1-\frac{\widetilde{B}_1\widehat{B}_1 }{K_1}\right)-d_1\widetilde{I}\widehat{I}\widetilde{B}_1\widehat{B}_1-m (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\
\frac{d \widehat{B}_1}{d \hat{t}}&=r_1 \tilde{t}\widehat{B}_1\left(1-\frac{\widetilde{B}_1 }{K_1}\widehat{B}_1\right)-d_1\tilde{t }\widetilde{I} \widehat{I}\widehat{B}_1-m\frac{\tilde{t}}{\widetilde{B}_1} (\widetilde{B}_1\widehat{B}_1-\widetilde{B}_2\widehat{B}_2) \\
\frac{d \widehat{B}_1}{d \hat{t}}&=\left[ r_1 \tilde{t}\right]\widehat{B}_1\left(1-\left[ \frac{\widetilde{B}_1 }{K_1}\right]\widehat{B}_1\right)-\left[ d_1\tilde{t } \widetilde{I}\right]\widehat{I}\widehat{B}_1-\left[m \tilde{t} \right] \widehat{B}_1+\left[m \frac{\tilde{t} \widetilde{B}_2}{\widetilde{B}_1} \right]\widehat{B}_2
\end{align}
(the procedure for $B_2$ is the same) The square brackets are to indicate the number of independent choices for the scales. In all of the examples I have seen so far the number of independent choices for the scales are the same as the number of the orginal parameters for the system. At maximum I can only choose four of these choices to be $1$. Meaning that I will have $13-4=9$ parameters remaing. The same as what I started with. What am I doing wrong/why can't I reduce the number of parameters in my system?  If you need more clarification on this problem please ask.","['ordinary-differential-equations', 'dimensional-analysis']"
2689972,Range of an integral operator,"I tried to determine the range of the integral operator $T\in\mathcal{L}(L^2(0,1))$ given by
$$(Tx)(t) = \int^1_t x(s)ds \text{ for } x \in L^2(0,1), t\in[0,1]$$
but it doesn't sound very logical.
This is my approach to the solution:
\begin{align*}
y(t) = Tx(t) &= \int^1_t x(s)ds\\
             &= \alpha - \int^t_0 x(s)ds ~~~~~(\alpha = \int^1_0 x(s)ds<\infty) \\
\Rightarrow -y'&=x
\end{align*}
Then
\begin{align*}
(Tx)(0) = \alpha <\infty~~~~~ &, ~~~~~(Tx)(1) = 0\\
(Tx)(t) = -(Ty')(t) &= \int^t_1 y'(s)ds = y(t) - y(1) 
\end{align*}
if the integral exist, i.e. $y'(s)$ exists, $y' \in L^2(0,1)$ and $y(1) = 0$.
Therefore
$$\mathcal{R}(T) = \lbrace y \in H^1(0,1)~ \vert ~y(1) = 0\rbrace$$","['functional-analysis', 'lebesgue-integral', 'operator-theory']"
2690013,Proving a.e surjectivity of suggested factor map in Natural extension of Standard Borel dynamical system,"Let $(X,\mathcal B,\mu, T)$ be a measure preserving dynamical system on a standard Borel space. I'm trying to follow the construction of the natural extension by Michael Hochman on page 53 of his notes . This is very similar to the construction mentioned in this post, which unfortunately fails to answer the one question I have about the construction. For convenience let me repeat the essentials of the construction: We consider $X^\mathbb{Z}$, with the normal product $\sigma$ algebra $\tilde{\mathcal{B}}$, ensuring that each projection $\pi_n$ is measurable. The Kolmogorov extension theorem guarantees the existence of a probability measure $\tilde\mu$ on $\tilde{\mathcal{B}}$, such that the pushforward $\pi_{n*}\tilde\mu=\mu$ for all $n\in\mathbb Z$. The left shift $\tilde T$ is invertible, measurable, and measure preserving on $(X^\mathbb{Z},\tilde{\mathcal{B}},\tilde\mu)$. Now let us define 
$$\tilde X=\{(x_n)\in X^\mathbb{Z}:Tx_n=x_{n+1},~n\in\mathbb Z\},$$
and redefine $\tilde{\mathcal{B}},\tilde\mu,$ and $\tilde T$ to be the relevant restrictions to $\tilde X^~$. Finally let us define $\pi:\tilde X\to X$ by $\pi=\pi_0|_{\tilde X}$. The goal is to show that $\pi$ is a factor map. I am happy with showing that $\pi$ is measurable, measure preserving, and that $\pi\circ \tilde T=T\circ \pi$, but what I have been unable to do is prove that $\mu(\operatorname{im}\pi)=1$, which we need if we want to call $\pi$ a factor map. My idea is to show that the set $A$ consisting of all $x\in X$ for which there is an infinite chain in the graph of all preimages of $x$ has full measure. To be more precise: $A$ consists of all $x\in X$ such there exists a sequence $(x_n)_{n\in\mathbb N_0}$ in $X$ such that $Tx_{n+1}=x_n$ for all $n$, and $x_0=x$. If $A$ does have full measure then we would be done, because for every $x\in A$ it would follow that $(\dots,x_2,x_1,x,Tx,T^2x,\dots)\in \tilde X$. Now I believe that we could construct $A$ as a clever countable intersection of sets of full measure, but I am currently not clever enough to see it. If anyone can see it I'd be much obliged. From scouring the interwebs I know that the existence of a natural extension of a dynamical system is closely related to the notion of the categorical inverse limit. Unfortunately category theory is one of the many need to be familiar with area of mathematics that I am not familiar with. I would still appreciate an answer phrased in terms of inverse limits (if it was explained in a gentle manner), because that can only help my familiarity with category theory. Any help is appreciated.","['dynamical-systems', 'ergodic-theory', 'measure-theory']"
2690026,Cauchy Problem (PDE),"Find the solution to the Cauchy data problem $$\frac{\partial u}{\partial t} − u\frac{\partial u}{\partial x} = −2u$$ where $u(x, 0) = x$ . I know how to solve homogeneous Cauchy problems. However, I am struggling to understand non-homogeneous equations like this one. Can somebody please give a detailed solution?","['derivatives', 'real-analysis', 'ordinary-differential-equations', 'partial-differential-equations']"
2690061,Morphism of vector bundles in algebraic geometric terms,"Hartshorne doesn't give a definition of morphism of vector bundles but only that of isomorphism between vector bundles. After looking at a few online references, I think I understand the following:
Let $V\xrightarrow{f}X$ and $V'\xrightarrow{f'}X$ be two vector bundles of rank $n$ and $m$ respectively on a scheme $X$. Then a morphism of vector bundles is a morphism of $X$-schemes $\phi:V \longrightarrow V'$ such that the induced map $V|_{U_i}\cong\mathbb{A}_{U_i}^n \longrightarrow \mathbb{A}_{U_i}^m \cong V'|_{U_i}$ is linear for all $U_i$  where $\{U_i\}_{i \in I}$ is a cover of $X$ which provides local trivializations for both $V$ and $V'$. I am not sure if my understanding is correct. Please correct me if I am wrong. I do know the differential geometry version of the definition but I want a definition of morphism of vector bundles corresponding to the way Hartshorne has defined vector bundles (II,Ex:5.18). Thanks in advance!",['algebraic-geometry']
2690062,A question about a vector operation,"I was working on a question which defined a new operation between vectors. Let $\vec{u}=(u_i)_{i\in\lbrack n\rbrack},\vec{v}=(v_i)_{i\in\lbrack n\rbrack}\in\Bbb R^n$, define $\star:\Bbb R^n\times\Bbb R^n\to\mathcal{M}_n(\Bbb R)$ where $\vec{u}\star\vec{v}=A_{ij}=(u_iv_j)$ as a matrix. The question is as follows: If $\vec{u},\vec{v}$ are linearly independent, find the rank of the matrix $\vec{u}\star\vec{v}-\vec{v}\star\vec{u}$. What I did was try to conjecture the rank with a couple of common l.i. vectors. I chose $e_1, e_2$. This gave me the matrix with entries $1$ in $(1,2)$, $-1$ in $(2,1)$, and $0$ elsewhere.
Thus if the question is true, the matrix's rank should be $2$. To prove this I tried checking what happened when $\vec{u},\vec{v}$ were linearly dependent. The only possibility is that $\vec{v}$ is a scalar multiple of $\vec{u}$, then $\vec{v}=\lambda\vec{u}$ for a real $\lambda$. Should $\lambda$ be $0$, the matrix's rank would be 1. Else the matrix would be the zero matrix. Now to prove the result, I think that if I prove that the rank is less than or equal to $2$ then I've got the proof. This is because the other cases are covered in what I just wrote. Still, I feel that even if it were true, my proof would be too flimsy and not elegant. I'm looking for any hints and help to approach the solution.","['matrices', 'matrix-rank', 'linear-algebra']"
2690113,Number of divisors of the number $2079000$ which are even and divisible by $15$,"Find the number of divisors of $2079000$ which are even and divisible by $15$? My Attempt: Since they are divisible by $15$ and are even, $2$ and $5$ have to included from the numbers prime factors. $2079000 = 2^3 \cdot 3^3 \cdot 5^3 \cdot 7 \cdot 11$ Therefore, the number of divisors should be $2 \cdot 2 \cdot (3+1) \cdot (1+1) \cdot (1+1)$ But however this answer is wrong. Any help would be appreciated.","['permutations', 'combinatorics']"
2690118,"What is the term for ""merging complementing data streams""","I'm merging 2 data streams together.  One of them is precise, the other accurate.  Excuse if this is obvious, but let me be sure I'm clear: Accurate: Long term average is correct, but otherwise a noisy signal Precise: short term changes are indeed the desired signal, but an overall unknown DC bias exists, and may also drift (i.e, the result of an integration) In essence, these two signals compliment each other (correct me if there's a better word than ""complement"").  I can think of a number of ways to merge these mathematically, but not quite the question.  What I'd like to know is the terminology around this. Is there a term that encapsulates the functionality of merging these signals, and in this case to obtain both precision and accuracy in an output stream?  Linear or non-linear. The specific calculation I'm looking at is below.  It is non-linear if the $K$ term is used: $Output_n = Output_{n-1} + \Delta Precise_n \\
\qquad\qquad + f(Accurate_n -Output_{n-1}-\Delta Precise_n)$ Where $f(\varepsilon) $ is an error transfer function, and can be something like $\qquad\qquad f(\varepsilon) = P*\varepsilon + K *sign(\varepsilon)$ P as a proportional error correction, and/or K as a fixed error correction.  K should be smaller than the noise in the Precise signal, but can be augmented if $sign(\varepsilon)$ is consistent (geometrically augmented with $G$, and/or cumulatively augmented with $C$ $K_n = K_{n-1}    * G^{sign(\varepsilon_n)*sign(\varepsilon_{n-1})} \\
\qquad\qquad\space\space\space + C*sign(\varepsilon_n)*sign(\varepsilon_{n-1})\\
$ C if used is much smaller than the noise in the precise signal G if used is slightly greater than 1, on the order of $\frac{K+C}K$ And bounded to reasonable, positive limits: $Lower <= K_n <= Upper$. Thanks to all for terminology correction suggestions","['signal-processing', 'data-analysis', 'computer-science', 'discrete-mathematics']"
2690156,Show $\mu$($E$) $=$ $\sum_{x_n\in E} a_n$ is a measure on $\mathcal P$($\Omega$).,"Let $\Omega$ be a nonempty set, $\{x_n\}_{n}$ a sequence of distinct elements of $\Omega$, and $\{a_n\}_{n}$ a sequence of nonnegative real numbers.  For E $\subset$ $\Omega$, define $\mu$($E$) $=$ $\sum_{x_n\in E} a_n$. I'm trying to show $\mu$ is a measure on $\mathcal P$($\Omega$). I've already shown that condition (1) holds: 
Since $\{a_n\}_{n}$ is a sequence of nonnegative real numbers, we have  $\mu$($E$) $=$ $\sum_{x_n\in E} a_n$. $\ge$ $0$ for all $E$ $\in$ $\mathcal P$($\Omega$). However, I'm stuck showing that conditions (2) and (3) hold. That is, $\mu$($\emptyset$) $=$ $0$, and for a sequence $\{E_n\}_{n=1}^{\infty}$ of pairwise disjoint sets $\mu$(${\bigcup_{n=1}^{\infty}E_n}$)$=$$\sum_{n=1}^{\infty}$ $\mu$($E_n$) Any suggestions?","['probability-theory', 'measure-theory']"
2690174,An explicit equation for an elliptic curve with CM?,"The elliptic curve
$$y^2=x^3+x$$
has complex multiplication by $i$ (the action of $i$ is $y\to iy$ and $x\to -x$), and any such has equation
$$y^2=x^3+g_2(\Lambda)x+g_2(\Lambda) \ \ \ \text{ where} \ \ \ \Lambda=\alpha\mathbb{Z}[i]$$
i.e. has equation $y^2=x^3+ax$. Is there a way (for a human) to find an explicit equation for an elliptic curve $E$ with complex multiplication by $\mathcal{O}_K$, for
$$K=\mathbb{Q}(\sqrt{-d})$$
any imaginary quadratic number field? (Note that since we know by Dedekind's criterion what the primes of $\mathcal{O}_K$ are, this allows us to, given one equation, find them all. This might be a bit annoying to do in practice, but to me this is the less interesting part of the question.)","['number-theory', 'complex-multiplication', 'elliptic-curves']"
2690215,"If I toss $n$ coins and you toss $n + m$, what is the probability you'll have more heads?","My intuition is that it's the probability that you get at least 1 head in your extra $m$ tosses. Basically on the first $n$ tosses we have equal chance of winning so let's not even count those and only bet on the $m$ extra ones you have. I haven't been able to formalize this. My closest attempt is that this probability is $$P(M + N_2 > N_1) = P(M >N_2-N_1)=E[I(M>N_2-N1)]$$ where $M$ is the random variable for the number of heads in your extra coins, $N_2$ is for the heads in your first $n$ tosses and $N_1$ for mine. I don't know what to do with that indicator variable but I believe the solution lies that way.","['statistics', 'probability']"
2690230,Product rule for gradient,"I have two functions scalar functions of vector $\textbf{x}$, $f(\textbf{x})$ and $g(\textbf{x})$. I know the gradient of $f(\textbf{x})$ (i.e. $\triangledown f(\textbf{x})$) and I want to find the gradient of $f(\textbf{x})g(\textbf{x})$. Can I use the product rule $$\triangledown f(\textbf{x})g(\textbf{x})=g(\textbf{x})\triangledown f(\textbf{x})+f(\textbf{x})\triangledown g(\textbf{x}).$$
I mean does the product rule of differentiation also apply to gradients? Thanks in advance.","['real-analysis', 'calculus', 'vector-analysis']"
2690233,Confusion in finding conditional expecation of indicator function (Lehmann-Scheffe),"I'm trying to find the UMVUE of $e^{-2\lambda}$ with $X_1, X_2,\ldots,X_n \sim \operatorname{Poisson}(\lambda)$ being independent. So since $T(X) := \sum_{i=1}^n X_i$ is a complete sufficient statistic for the Poisson distribution, and also $W(X):= \mathbf{I}(X_1 + X_2 = 0)$ is an unbiased estimator of $e^{-2\lambda}$, then by Lehmann_Scheffe, $$\tau(T)= \mathbb{E}(W(T)\mid T(X))
$$
is the UMVUE of $e^{-2\lambda}$. I'm having trouble computing $\tau$ so I would like to see if this is correct: First, to find the expectation, I need the PDF. $$\mathbb{P}(W(T)=s\mid T(X)=t) = \frac{\mathbb{P}(W(T)=s \ \cap \ T(X) = t)}{\mathbb{P}(T(X) = t)} \\ = \frac{\mathbb{P}(\mathbf{I}(X_1 + X_2 = 0)  = s \ \cap \ X_1 + X_2 + \cdots + X_2 = t)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}\\ = \frac{\mathbb{P}(\mathbf{I}(X_3 + X_4 + \cdots + X_n = t)=s)}{\mathbb{P}(X_1 + X_2 + \cdots + X_n = t)}  
$$
The denominator is just $\frac{e^{-n\lambda}(n\lambda)^t}{t!}$. When $s=1$, we have 
$$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =1) = \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}  
$$
So 
$$\mathbb{P}(\mathrm{I}(X_3+X_4+\cdots+X_n=t) =0) = 1 - \frac{e^{-(n-2)\lambda}((n-2)\lambda)^t}{t!}  
$$
After this, I'm unsure of how to find the expectation. Is my approach correct?","['parameter-estimation', 'probability-theory', 'statistics', 'statistical-inference']"
2690267,$A$ is complex matrix and $A^3=A$. Show that $rk(A)=tr(A^2)$,"$A$ is complex matrix and $A^3=A$. Show that $rk(A)=tr(A^2)$ I'm more concerned with how I can derive the prove of this question. before I ask this question, I fail to prove that whit jordan canonical form","['matrices', 'linear-algebra']"
2690311,Calculating $\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3}$.,"I have a difficulty in calculating this limit: $$\lim_{x \rightarrow 0} \frac{\tan x - \sin x}{x^3},$$ I have tried $\tan x = \frac{\sin x}{\cos x}$, then I unified the denominator of the numerator of the given limit problem finally I got $$\lim_{x \rightarrow 0} \frac{\sin x}{x^{3} \cos x} - \lim_{x \rightarrow 0} \frac{ \sin x}{x^3},$$ Then I got stucked, could anyone help me in solving it?","['limits-without-lhopital', 'calculus', 'limits']"
2690346,Delta distribution is not induced by and tempered function,"Call a function $f: \mathbb{R} \to \mathbb{C}$ a $\textit{tempered}$ function if there exists $N \in \mathbb{N}$ such that $$\int_{\mathbb{R}} |f(x)| \, (1+|x|)^{-N} < \infty.$$ Then it is known that the $\textit{tempered distribution}$ induced by $f$, $$T_f: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$T_f(\phi) = \int_{\mathbb{R}} f(x) \, \phi(x) \, dx$$ is indeed a tempered distribution, where $\mathcal{S}(\mathbb{R})$ is the space of Schwartz functions and a tempered distribution has the property that if $\phi_j \to 0$ in $\mathcal{S}(\mathbb{R})$, then $T_f(\phi_j) \to 0$ . I would like to show that the (a priori) tempered distribution  $$\delta: \mathcal{S}(\mathbb{R}) \to \mathbb{C}$$ $$\delta(\phi) = \phi(0)$$ is not induced by any tempered function $f$. My argument is intuitively clear to me, but the rigor in some steps, I am not sure. On the contrary, suppose that we have some $f$ so that $T_f = \delta$. Let $\varphi$ be some symmetric bump function such that $\mathrm{Supp}(\varphi) \subseteq [-1,1]$ and $\varphi(0) = 1$. Define a sequence $\varphi_j = \varphi(jx)$. Then for all $j \in \mathbb{N}$ we have $$\delta(\varphi_j)=\delta(\varphi(jx)) = \varphi(0) = 1.$$ Thus $\delta(\phi_j) \to 1$. On then other hand, since $\mathrm{Supp}(\varphi_j) = [-1/j, 1/j]$ we have $$T_f(\varphi_j) = \int_{-1/j}^{1/j} f(x) \varphi_j(x) dx$$ so that $T_f(\varphi_j) \to 0$. By uniqueness of limits we must have $T_f \neq \delta$.","['fourier-analysis', 'harmonic-analysis', 'distribution-theory', 'functional-analysis', 'sobolev-spaces']"
2690357,Show that $X_{(1)}=\min(X)$ is independent of ancillary $Y_i = X_{(n)}-X_{(i)}$ for an exponential location family,"Given a random sample $X_1,\ldots,X_n$ distributed according to: $$f(x_i\mid\theta) = e^{-(x_i-\theta)}$$
Show that the minimal sufficient statistic $X_{(1)}$ is independent of the set of ancillary statistics:
$$(Y_1,\ldots,Y_{n-1})=(X_{(n)}-X_{(1)},\ldots,X_{(n)}-X_{(n-1)})$$
where $X_{(i)}$ denotes the $i^\text{th}$ order statistic of a random sample of size $n$. My first thought is to perform a simple transformation using the joint order statistic distribution of $X$: 
$$f_{X_{(1)},\ldots,X_{(n)}}(x_1,\ldots,x_n)=n!f_X(x_1)\cdots f_X(x_n)$$ 
If the order statistic inequalities condition is met $x_1<\cdots<x_n$.
Then confirm independence using factorization of the joint distribution. Using the definitions of $Y_i$ we perform the multivariate transformation, I defined $Y_n=X_{(1)}$ for simplicity. However I quickly run into a difficult jacobian matrix computation for the determinant. Each inverse mapping from $Y\to X$ is of form:
$$X_i=Y_n+Y_1-Y_i$$
The jacobian of this takes on a form I'm not familiar with computing. Not sure how to proceed here or if I should take a look at another approach!","['independence', 'statistics', 'statistical-inference', 'probability-distributions']"
2690375,Calculation of traceless second fundamental form,"The traceless part of second fundamental form is 
$$
\mathring A = A -\frac{H}{n}g
$$
where $A$ is second fundamental form, $H$ is mean curvature, $g$ is metric. The norm square is 
$$
|\mathring A|^2 = |A|^2-\frac{1}{n}H^2.
$$
I want to verify it. What I do:
\begin{align}
|\mathring A|^2  &=g^{ij}g^{kl}\mathring A_{ik}\mathring A_{jl}  \\
&=g^{ij}g^{kl}(A_{ik} -\frac{H}{n}g_{ik})(A_{jl} -\frac{H}{n}g_{jl})  \\
&=g^{ij}g^{kl} (A_{ik}A_{jl}-\frac{H}{n}g_{ik}A_{jl}
-\frac{H}{n}g_{jl}A_{ik}
+(\frac{H}{n})^2g_{ik}g_{jl})  \\
&=|A|^2 -\frac{2H}{n}g^{ij}A_{ij} + (\frac{H}{n})^2\delta_k^j\delta_j^k
\end{align}
Then, I don't know how to deal it. I can't think though why $\delta_k^j\delta_j^k =n$ and what is $g^{ij}A_{ij}$.","['riemannian-geometry', 'differential-geometry']"
2690411,The logarithmic inequality,"$$ \log_{3x+7}{(9+12x+4x^2)}+ \log_{2x+3}{(6x^2+23x+21)} \ge 4$$
The logarithmic inequality is defined for: $ x \in (-3/2, -1) \cup(-1, \infty)$. First, I supposed that my solutions are in the interval $(-3/2, -1)$. Following this interval for which the logarithmic function $t$ are decreasing and multiplying both sides of the inequality by $\log_{2x+3}{(3x+7)}$, I obtained: 
$$ 2+ [1+\log_{2x+3}{(3x+7)}-\log_{2x+3}{6}]\cdot\log_{2x+3}{(3x+7)} \le4\log_{2x+3}{(3x+7)}\\ \log_{2x+3}{(3x+7)}=t<0\\t^2-(3+\log_{2x+3}{6})t+2 \le0$$
And here I got stuck. This inequailty is intended to be solved witouth a calculator, is it possible to continuie doing the inequality withouth it? And what I did wrong?","['logarithms', 'inequality', 'functions']"
2690431,Convergence of a linear recurrence equation,"Let $T \colon \mathbb{C}^n \to \mathbb{C}^n$ be a linear operator. Let $\{u_k\} \subset \mathbb{C}^n$ and $\{v_k\} \subset \mathbb{C}^n$ be two sequences of vectors. Suppose the spectral radius of $T$ is $\rho(T) = q <1$ but we don't know whether or not the spectral norm $\|T\|_2 = \sup_{\|x\|_2=1} \|Tx\|_2$ is smaller than $1$. I have a recurrence relation defined by
\begin{align*}
u_{k+1} = T u_k + \beta v_k,
\end{align*}
where we have the freedom to choose any fixed positive $\beta$.
Furthermore, the $2$-norm of $v_k$ is bounded by
\begin{align*}
\|v_k\|_2 \le  \|u_k\|_2 +  \|u_{k-1}\|_2,
\end{align*} The problem I am considering is to choose $\beta$ as large as possible but still guarantee $u_k \to 0$. I have an argument to show there exists some $\beta > 0$ such that $\|u_k\|_2 \to 0$. This argument is somehow involved and the choice of $\beta$ involves the condition number of $S$ where $S^{-1} J S = T$ is the Jordan decomposition. The argument goes as: 
  We know for any fixed $\varepsilon > 0$ we can define some vector norm $\|\cdot\|_v$ on $\mathbb{C}^n$ such that the induced operator norm of $T$ is $\|T\|_{op,v} = q + \varepsilon$. Then taking $\|\cdot\|_v$ norm on both sides of the recurrence, we get
  \begin{align*}
\|u_{k+1}\|_v \le (q+\varepsilon) \|u_k\|_v + \beta \|v_k\|_v.
\end{align*}
  By norm equivalence, we can bound $\|v_k\|_v \le a \|u_k\|_v + a \|u_{k-1}\|_v$ for some positive constant $a$. Now we have the second order recurrence $\|u_{k+1}\|_v \le (q+ \varepsilon+\beta a) \|u_k\|_v + \beta a \|u_{k-1}\|_v$, which can be solved explicitly. If $\beta$ is sufficiently small, $u_k \to 0$ can be guaranteed. To choose largest $\beta$ with this guarantee, I need the constants $c_1, c_2$ in $c_1\|\cdot\|_2 \le \|\cdot\|_v \le c_2 \|\cdot\|_2$, which is essentially the condition number of $S$. I am wondering whether there are more direct ways to my problem. I also considered expanding the recurrence
\begin{align*}
u_{k+1} = T^k u_1 + \beta( v_k + Tv_{k-1} + \dots + T^{k-1} v_1).
\end{align*}
Then we can apply the fact: for any matrix norm, $\|T^k\| \le c (\rho(T) + \varepsilon)^k$ for every $\varepsilon > 0$ where $c$ is some constant. By taking $2$-norm on both sides, we can avoid what I have done. But I haven't found a way to estimate the sum of $T^{k-j}v_j$ terms. Each sum is defined in a recursive manner with respect to $u_k$ and $u_{k-1}$.","['recurrence-relations', 'linear-algebra', 'convergence-divergence', 'rate-of-convergence']"
2690617,"Determine the values of $x,y$ such that $\cos x \cos z \cos y +\sin x \sin y <0$ for all $z$","To solve a problem I need to determine the range of valus $x,y$ such that $\cos x \cos z \cos y +\sin x \sin y <0 \ \forall z$. I think it is sufficient to let $\cos z=1$ and then let $\cos z = -1$ and solve the system consists of these two inequalities. But I have got odd an odd answer after doing this. Is my method problematic? How to deal with this question?","['inequality', 'trigonometry']"
2690645,Existence of a maximal set (with respect to set inclusion) having a certain property,"I have a set $E$ and a predicate $P$ such that for any subset $S\subseteq E$, $P(S)$ is either true or false. Moreover, I know that there exists an set $X\subseteq E$ such that $P(X)$ is true. My question is: is it always the case that $\{S\mid P(S)\}$ has a maximal element, with respect to the set inclusion relation? My intuition is that it is the case indeed. I am not a mathematician, so I am only weakly confident about it, but I imagine a proof by contradiction that would rely on transfinite induction. We know that the cardinality of $\{S\mid P(S)\}$ cannot be greater than the cardinality of the powerset of $E$. Yet, if there is no maximal element in the set, it means I can always find a strictly bigger set $S\subseteq E$ that satisfies $P$. If I keep growing the $S$ with a sufficiently large transfinite number of steps, I will necessarily reach a set strictly bigger than the powerset of $E$, and thereby prove a contraction. I am not sure whether my proof is valid, and there may be a simpler argument, possibly based on axioms of set theory. Also, it seems to me that my ""proof"" would require something like the axiom of choice to work. Is it the case? Is the conjecture even true? If not, do you have a counter example?",['elementary-set-theory']
2690666,Is a two dimensional rotationally symmetric probability density function equivalent to a radial one dimensional density function??,"Assume that $f(x,y)$ is a two dimensional rotationally symmetric p.d.f.,  $f(x,y)=\tilde{f}(\sqrt{x^2+y^2})$. In the polar coordinates, $x=r \cos\varphi$, $y=r\sin\varphi$, we can average out the azimuthal angle to find a one dimensional radial p.d.f as follows,
$$ p(r)= r\int_{0}^{2\pi}d\varphi\, \tilde{f}(r)=2\pi r\, \tilde{f}(r).\qquad\qquad\qquad \qquad     (1)$$ The generating function of the distribution $f(x,y)$ is written as $\mathbb{E}[e^{i\mathbf{k}\cdot \mathbf{x}}]$ where $\mathbf{x}=(x,y)$ and $\mathbf{k}=(k_x,k_y)$ are two dimensional vectors. Here $\mathbb{E}[\cdots]=\int dx dy \cdots \,f(x,y)$. Then the generating function for a rotationally symmetric distribution is given by
$$  \mathbb{E}[e^{i\mathbf{k}\cdot \mathbf{x}}]=\int_{0}^{\infty}\int_{0}^{2\pi}r dr d\varphi\,e^{i k r \cos(\varphi-\varphi_k)}\tilde{f}(r)=2\pi\int_{0}^{\infty}r dr\, \tilde{f}(r)\,J_0(kr)=\int_0^{\infty} dr\, p(r) J_0(k r), $$
where in the above $k=\sqrt{k_x^2+k_y^2}$ and $\varphi_k=\text{atan2}(k_x,k_y)$ and $J_0(x)$ is the modified Bessel-Function of the first kind. If I define the averaging with respect to $p(r)$ as $\tilde{\mathbb{E}}[\cdots]=\int_0^{\infty} dr\cdots p(r)$ then we have the following:
$$ \mathbb{E}[e^{i\mathbf{k}\cdot \mathbf{x}}] \equiv \tilde{\mathbb{E}}[J_0(k r)]=1-\frac{1}{4}k^2\tilde{\mathbb{E}}[r^2]+\frac{1}{64}k^4\tilde{\mathbb{E}}[r^4]+\cdots\,.$$ Question: Based on the above, the characteristic function of a 2D rotationally symmetric p.d.f. can be obtained only from even moments ( $\tilde{\mathbb{E}}[r^{2k}]$) of the p.d.f. $p(r)$ which is defined in the range $[0,\infty)$. On the other hand, the characteristic function of $p(r)$ can be written as  $\tilde{\mathbb{E}}[e^{i t r}]$ where $t\in(-\infty,\infty)$( What is the characteristic function of a p.d.f with $\mathbb{R}^+$ support? ). It simply means that  $\tilde{\mathbb{E}}[r^{2k+1}]$ are moments of $p(r)$ too. Now I am a bit confused. If I know all $\tilde{\mathbb{E}}[r^{2k}]$ then I know $f(x,y)$. However, in order to know $p(r)$ I should know about $\tilde{\mathbb{E}}[r^{k}]$ for odd $k$  as well as even $k$. On the other hand, $f(x,y)$ and $p(r)$ are uniquelly related to each other from equation (1). My Guess: I think I am missing something obvious! My guess is that $p(r)$ and rotatinaly symmetric $f(x,y)$ are not equivalent, and I can use them interchangeably if I replace the characteristic function  $\tilde{\mathbb{E}}[e^{i t r}]$ to  $\tilde{\mathbb{E}}[J_0(k r)]$ for $p(r)$.","['probability-theory', 'probability', 'statistics', 'probability-distributions']"
2690721,Geometric interpretation of Hölder's inequality,"Is there a geometric intuition for Hölder's inequality? I am referring to $||fg||_1 \le ||f||_p ||f||_q $, when $\frac{1}{p}+\frac{1}{q}=1$. For $p=q=2$ this is just the Cauchy-Schwarz inequality, for which I have geometric intution: The projection of a vector along a direction shortens its length. My question is if there is a similar geometric interpretation for Hölder's inequality. I am aware of the scaling argument , which shows the inequality can only hold when $\frac{1}{p}+\frac{1}{q}=1$; but why should we expect this to be true when the $p,q$ are conjugates? Perhaps there is some physical interpretation? Note that I am looking for intuition, not necessarily a formal proof. Hölder's inequality can be proved using Young's inequality, for which a beautiful intuition is given here . In my perspective, even though this gives intuition to a component in the proof of Hölder's inequality, this does not really give an intuition for the inequality itself . (But maybe I am wrong? does the actual integration have ""true content"" in it, or is Hölder really nothing but Young's inequality in disguise? Part of the confusion is that the intuition for Young's inequality is based on integration , so if we only rely on that, the intuition for Hölder should be some sort of ""double integration""...  ) To see that the geometric intuition of Young's and Hölder's inequalities are somewhat different, we can look at $p=q=2$: In that case, Young's inequality is just the standard AM-GM inequality for two variables. This inequality can be interpreted geometrically . Although here one can also view this as ""projection only shortens"", the scenario is a little different than the one in the Cauchy-Schwarz inequality. (At least the the reasons behind the ""equality cases"" seem slightly different to me).","['intuition', 'inequality', 'integral-inequality', 'geometric-interpretation', 'geometry']"
2690734,"Prove if $f$ and $g$ are integrable, $\int g\,dv=\int fg \, d\mu$","Suppose $(X,A,\mu)$ is a measure space and $f\ge0$ is integrable. 
$$v(A)=\int_Af \, d\mu$$ $v$ is a measure. If $g$ is also integrable with respect to $v$, then $fg$ is integrable with respect to $\mu$ and 
$$\int g\,dv=\int fg \, d\mu$$ I got stuck with proving the sigma additivity for 1) and 2). Anything would be helpful!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
2690736,Weird use of Glasser's Master Theorem,"Consider the following enumeration of the rational numbers in $[\,0,1)$:
$$0, \frac{1}{2},\frac{1}{3}, \frac{2}{3}, \frac{1}{4}, \frac{3}{4}, \frac{1}{5}, \frac{2}{5}, \frac{3}{5}, \frac{4}{5}, \frac{1}{6}, \frac{5}{6}, \cdots$$ Where the list is first group sorted by ascending denominator, then each group in ascending numerator, and deleting all reducible elements. Obviously $0$ is an exception to this sorting. This is an arbitrary choice, but the simplest to read (in my opinion) Also, the list begins at $0$ i.e. $r_0 = 0, r_1 = \frac{1}{2}, \cdots$ Define the following sequence of functions $\phi_n(x)$ $$\phi_0(x) = x- \pi \cot{\pi x}, \phi_1(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} \right) \\\\ \phi_2(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)}\right) \\\\ \phi_3(x) = x - \pi \left(\cot{\pi x} + \cot{\pi\left(x-\frac{1}{2}\right)} + \cot{\pi\left(x-\frac{1}{3}\right)} + \cot{\left(x-\frac{2}{3}\right)}\right) \\\\ \phi_n(x) = x - \pi \left( \sum_{k=0}^n \cot{\pi \left(x-r_k\right)}\right)$$
where $r_k$ is the $k^{\text{th}}$ element in the enumeration described above. Assuming $\displaystyle A = \text{PV} \left( \int_{-\infty}^\infty f(x) \, \text{d}x \right)$ is finite and is Lesbegue measurable, consider the following sequence of integrals $$I_n = \text{PV} \left( \int_{\infty}^\infty f(\phi_n(x)) \, \text{d}x \right)$$ It can be shown with Glasser's Master Theorem, that for any finite $n$, $I_n = A$ What happens in the limit, for $n \to \infty$ ? Is the limit of $I_n$ still equal to $A$? If so, how can I rigorously prove it? Edit: The motivation of this question was out of simple curiosity. I wanted to see what would happen to the integral if I densely filled the function's argument with singularities, and whether any infinite weirdness happens, as is often the case. An internet person argued that if one chooses to accept the Axiom of Countable Choice, then the limit exists and is equal to $A$. Is this the case?","['limits', 'lebesgue-measure', 'improper-integrals', 'integration', 'lebesgue-integral']"
2690749,Wronskian of Airy functions.,"I am trying to show that that the Airy functions defined below satisfy: $W[Ai(x),Bi(x)]=1/\pi$. $$Ai(x)=\frac{1}{\pi} \int_0^\infty \cos(t^3/3+xt)dt$$ $$Bi(x)=\frac{1}{\pi}\int_0^\infty \bigg[ \exp(-t^3/3+xt)+\sin(t^3/3+xt)\bigg]dt $$ I tried to compute it directly but I got stuck, here's the last term I got: $$Ai(x)Bi'(x)-Ai'(x)Bi(x) = \frac{1}{\pi^2}\bigg[ \int_0^\infty \cos(t^3/3+xt)dt \int_0^\infty \bigg( s\exp(-s^3/3+xs)+s\cos(s^3/3+xs)\bigg) ds + \int_0^\infty \sin(t^3/3+xt)tdt\int_0^\infty \bigg(\exp(-s^3/3+xs)+\sin(s^3/3+xs)\bigg)ds \bigg]$$ I don't see how to proceed from here, I guess I need complex integration contour but how exactly? Thanks.
I want also to show that $Bi(x),Bi'(x)>0 \forall x>0$, and to conclude the asymptotic identities:
$$Bi(x) \sim \pi^{-1/2}x^{-1/4}\exp(2/3 x^{3/2})$$ $$Bi'(x)\sim \pi^{-1/2}x^{1/4}\exp(2/3 x^{3/2})$$","['special-functions', 'wronskian', 'ordinary-differential-equations']"
2691015,Converting bounds of double integral to polar coordinates,"I'm trying to convert the following to polar coordinates: $$\int_0^\infty \int_{-\infty}^{-x}\frac{1}{2\pi}e^{-(x^2+y^2)/2}\,dx\,dy$$ After converting to polar coordinates, it should be: $$\int_0^\infty \int_{(3/2)\pi}^{(7/4)\pi}\frac{1}{2\pi}e^{-r^2/2}\,dr\,d\theta$$ My question is how do we arrive at the bounds of $(3/2)\pi$ and $(7/4)\pi$.","['statistics', 'polar-coordinates', 'calculus', 'normal-distribution']"
2691045,Solve $f''(x) = \cos (f(x))$,"I wanted to solve the differential equation :
$$\frac{d^2y}{dx^2} = \cos y$$ I know how to solve it if it was a first derivative ($y'=\cos y $),
But I have no intuition on this one. Either an explicit or an implicit relation is good for me,  but I was more curious about the process than the answer, as I am generally stuck at second derivatives in differential equations.",['ordinary-differential-equations']
2691058,Evaluating the complex summation,"$1,z_1,z_2,z_3,...z_{n-1}$ are the $n^{\text{th}}$ roots of unity, then the value of $\dfrac 1{3-z_1}+ \dfrac{1}{3-z_2}+...+\dfrac 1 {3-z_{n-1}}$ is equal to? I wrote the polar form of the $n$th root of unity $(\cos \dfrac{2k\pi}{n}+i \sin\dfrac{ 2k\pi}{n})$ $\forall ~ k\in\{0,1,2...n-1\} $ But that didn't help at all. How do I go about solving this problem?","['algebra-precalculus', 'complex-numbers']"
