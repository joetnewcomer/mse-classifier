question_id,title,body,tags
50093,Trigonometric equality: $\frac{1 + \sin A - \cos A}{1 + \sin A + \cos A} = \tan \frac{A}{2}$,"Can you guys give me a hint on how to proceed with proving this trigonometric equality? I have a feeling I need to use the half angle identity for $\tan \frac{\theta}{2}$. The stuff I have tried so far(multiplying numerator and denominator by $1 + \sin A - \cos A$) has lead to a dead end. Prove that,
  $$
\dfrac{1 + \sin A - \cos A}{1 + \sin A + \cos A} = \tan \dfrac{A}{2}
$$","['trigonometry', 'algebra-precalculus']"
50112,a split exact sequence of finite groups,Suppose G has a cyclic normal subgroup $\langle a\rangle$ of order $m$ and prime power index $s$ such that $m$ and $s$ are relatively prime. Then the following exact sequence splits: $$1 \longrightarrow  \langle a\rangle \longrightarrow G \longrightarrow G/\langle a\rangle \longrightarrow 1$$ Such group G is called a hyperelementary group. Question: How to define a homomorphism  $G/\langle a\rangle \rightarrow G$ to make the above sequence split ?,"['finite-groups', 'group-theory']"
50125,What is the operation $\boxtimes$?,"Reading papers about $p$-adic analysis and Galois representations, I have found objects like this $D \boxtimes \mathbb{Q}_p$. So my question is what is $\boxtimes$ and how do we read it ?","['notation', 'arithmetic-geometry', 'algebraic-geometry', 'definition']"
50147,Is There Something Called a Weighted Median?,"I was given some data that represents the number of lines in a document as well as the line count per hour (which is the lines in the document divided by the number of hours that the document was worked on).  Considering the following data: lines LCPH
57    133
62    135
33    137
76    139
78    141 Typically, I would express the median LCPH as 137 as it is the 3rd of 5 figures.  However, the median is currently being reported as 139 because it is not until the fourth document that more than half the lines in all documents were seen. This figure is currently being referred to as the Weighted Median LCPH instead of Median LCPH. Is there a different term for this sort of figure?","['statistics', 'median']"
50153,Analysis of Algorithms: Solving Recursion equations-$T(n)=3T(n-2)+9$,"I need your help with solving this recursion equation: $T(n)=3T(n-2)+9$ with the initial condition: $T(1)=T(2)=1.$ I need to find $T(n),$ the complexity of the algorithm which works that way. I tried to do that with induction. Edit: The answer should depend on $n.$ Thank you.","['recurrence-relations', 'discrete-mathematics']"
50154,Reference for the range of possible values in Hahn-Banach Theorem,"This is the usual formulation of Hahn-Banach theorem (some books use sublinear function instead, but it probably does not make much difference): Let $X$ be a vector space and let $p:X\to{\mathbb R}$ be
any convex function. Let $M$ be a vector subspace of $X$ and let
$f:M\to{\mathbb R}$ be a linear functional dominated by $p$ on $M$.
Then there is a linear extension $\hat{f}$ of $f$ to $X$ that is dominated by $p$ on $X$. However if you look into the proof, you find out that it shows that
for a given $v\in X$, there exists an extension with the value $\hat{f}(v)=c$ for any choice of $c$
anywhere between $$\sup_{x\in M,\lambda>0} \frac1\lambda [f(x)-p(x-\lambda{v})] \le c \le
\inf_{y\in M,\mu>0} \frac1\mu [p(y+\mu{v})-f(y)].$$ If $p$ is positive homogeneous, i.e., $p(\lambda x)=\lambda p(x)$ for $\lambda>0$, than the expression for the range is simpler: $$\sup_{x\in M} [f(x)-p(x-{v})] \le c \le
\inf_{y\in M} [p(y+{v})-f(y)].$$ In case the $p$ and $f$ have the additional property that $$(\forall x\in X)(\forall y\in M) p(x+y)=p(x)+f(y)$$ then the above interval can be simplified to $$-p(-v)\le c \le p(v).$$ (This situation happens quite often, take for example $f(x)=\lim x$ on the space $c$ of all convergent sequences and the function $p(x)=\limsup x$ on the
space $\ell_\infty$ of all bounded sequences.) On several occasions I found the above observations useful. (Since sometimes the function $p(x)$ can be chosen in a such way that the set of linear functionals which we want to study is precisely the set of all extensions of some given functional fulfilling $\hat{f}(x)\le p(x)$. Hence this might give some additional information about an interesting set of functionals.) So I wonder why I have not find a book where Hahn-Banach theorem was not formulated with the inclusion of the information about the possible range of those extensions. Question 1 Do you know about a reference giving a formulation of Hahn-Banach theorem which includes the information about possible values of extensions? Question 2 Were the extreme points of the set of Hahn-Banach extensions studied? (By the set of Hahn-Banach extensions I mean the set
of all linear functionals $\hat{f}:X\to{\mathbb R}$ fulfilling $(\forall x\in X)\hat{f}(x)\le p(x)$ and $(\forall x\in M)\hat{f}(x)=f(x)$
for a given linear functional $f:M\to{\mathbb R}$ and a convex function $p:X\to{\mathbb R}$.) EDIT: Perhaps I should mention that the formulation and the proof of Hahn-Banach theorem I followed is from Aliprantis, Border: Infinite-Dimensional Analysis . As far as I remember, all proofs of HBT I've seen have pretty much the same idea. Thus my guess is that the answer to the following question would be probably negative, but I'll ask it anyway. Question 3 Have you seen a formulation (or a proof) of Hahn-Banach theorem that would yield substantially different expression for the bounds I've given above. (I believe its more-or-less obvious from the proof that these bounds are best possible in the sense that extension of $f$ dominated by $p$ cannot have value in the point $v$ outside the given intervals. So only change that could happen would be some different expression of the same value. Alternatively, some books might contain proof of HBT which does not obtain the optimal bounds, but again, I do not see any way how this would simplify the proof, so there's probably no reason to give such a proof.)","['reference-request', 'hahn-banach-theorem', 'functional-analysis']"
50159,Does Permuting the Rows of a Matrix $A$ Change the Absolute Row Sum of $A^{-1}$?,"For $A = (a_{ij})$ an $n \times n$ matrix, the absolute row sum of $A$ is
$$
\|A\|_{\infty} = \max_{1 \leq i \leq n} \sum_{j=1}^{n} |a_{ij}|.
$$ Let $A$ be a given $n \times n$ matrix and let $A_0$ be a matrix obtained by permuting the rows of $A$.  Do we always have
$$
\|A^{-1}\|_{\infty} = \|A_{0}^{-1}\|_{\infty}?
$$","['matrices', 'normed-spaces', 'linear-algebra']"
50164,Understanding of graded algebra,"I am recently learning from Loring W. Tu's An Introduction to Manifolds the concept graded algebra , which is used for introducing exterior algebra. I don't understand the following definition: An algebra $A$ over a field $K$ is said to be graded if it can be written as a direct sum $A=\bigoplus_{k=0}^{\infty}A^k$ of vector spaces over $K$ such that the multiplication map sends $A^k\times A^l$ to $A^{k+l}$. Here are the questions: What does $k$ in $A^k$ mean? Is it a superscript or the power of $A$? (If it is the power, what does $A^0$ mean?) Since I don't know much but some very basic knowledge in abstract algebra, I am trying to understand the concept with some simple examples. (I don't quite understand the wiki article of graded algebra ). Is there any example as simple as the linear space ${\mathbb R}^n$ for the graded algebra? And what does it look like concretely?","['exterior-algebra', 'abstract-algebra']"
50172,Weierstrass approximation does not hold on the entire Real Line,"This is a question from Bergman's companion to Rudin. a) Show that the only polynomials which are bounded as functions $\mathbb{R} \rightarrow \mathbb{R}$ are constant functions. (I can do this) Also done here b)Deduce that if a sequence of polynomials $P_n:\mathbb{R} \rightarrow \mathbb{R}$ converges uniformly on $\mathbb{R}$ to $f$ then  $f$ is a polynomial. I figure that the uniform convergence implies at some point (for large n) the polynomials must have the same highest power because otherwise large values of $\mathbb{R}$ would destroy any hope of uniform convergence. Then eventually the second highest power must be equal as well by a similar argument...Then I guess you could make a similar argument for the co-efficients by plugging in large values of x, the difference in each co-efficient must be quite small in order to maintain the uniform convergence. I would like some help understanding if/why this means that the limit actually is a polynomial.",['real-analysis']
50173,Understanding of exterior algebra,"Consider the following definition from Loring W. Tu's An Introduction to Manifolds : For a finite-dimensional vector space $V$, say of dimension $n$, define
  $$A_*(V)=\oplus_{k=0}^{\infty}A_k(V)=\oplus_{k=0}^{n}A_k(V)$$
  where $A_0(V)={\mathbb R}$, and $A_k(k>0)$ denotes the set of all alternating $k$-linear functions $f$ on $V$, i.e.,
  $$f:V^k\to{\mathbb R},\qquad f(v_{\sigma(1),\cdots,v_{\sigma(k)}})=(\text{sgn}\sigma)f(v_1,\cdots,v_k) \quad\text{for all} \quad\sigma\in S_k.$$
  With the wedge product of multicovectors as multiplication, $A_*(V)$ becomes an anticommutative graded algebra, called the exterior algebra or the Grassmann algebra of multicovectors on the vector space $V$. By definition of graded algebra, $A_*(V)$ has the structure of a vector space. But I don't understand what does the element of $A_*(V)$ look like. For example, if $f\in A_2(V)$ and $g\in A_3(V)$, then what is $f+g$? Since domains of $f$ and $g$ are of different dimensions, how can one ""add"" them? So here are my questions : What does the element of $A_*(V)$ look like? And what's the addition of the vector space? According to the comments, the question above is actually a matter of understanding of the ""direct sum"". In stead of putting another post, I would like to ask a closed related question here: How many different definitions are there of exterior algebra and how are they equivalent to each other? I totally don't understand the one I learn from wikipedia . Any recommendation of a complete treatment of the subject?","['exterior-algebra', 'reference-request', 'abstract-algebra']"
50177,Method of variation of parameters,"In the method of Variation of Parameters of solving differential equations, where do the values used for $y_1$, and $y_2$ come from? Are they roots of the homogenous equation? Also, I assume that I first need to standardize my given equation before I can retrieve my $g(x)$. Is this correct?",['ordinary-differential-equations']
50184,Count of numbers where both $n$ and $n+1$ have all prime factors less than $x$,"If there are $m$ primes less than $x$ then I believe that there are at most $O(\log^m N)$ numbers less than $N$ all of whose prime factors are less than $x$.  This should mean that the number of numbers where both $n$ and $n+1$ have all their prime factors less than $x$ is finite if we consider the tests for $n$ and $n+1$ to be independent random events, and that is optimistic as $n$ and $n+1$ cannot share a prime factor.   I'd like to know, as a function of $m$ and the primes less than $x$ how many such numbers we ""expect"" to find in all of the natural numbers. This came up while playing with this question .",['number-theory']
50185,Notation of Elementary Inverse Functions,"Let $y = f(x) = \sqrt{2x + 1}$ for $x \geq -1/2$. Then, $f$ is injective on its domain and therefore its inverse is well-defined. To find the inverse, we simply invoke the necessary algebraic operations to solve for $x$ and determine that $$
x = \frac{y^2 -1}{2}
$$ and therefore $$
f^{-1}(y) = \frac{y^2 -1}{2}
$$ Now, I realize the name of the indeterminate has no effect on the validity of the expression but in every elementary text I see, the inverse is written instead as
$$
f^{-1}(x) = \frac{x^2 -1}{2}
$$
which is really counterintuitive. If our original function maps from the ""x-axis"" to the ""y-axis"" then it makes sense that the inverse would map from the ""y-axis"" to the ""x-axis"", not conversely. So my question is, Is there a reason why most texts choose the latter representation instead of the former or is it just a convention that is followed without any apparent justification ?","['notation', 'algebra-precalculus']"
50187,Why is the exterior algebra a bi-algebra (and even a Hopf algebra)?,"According to the wikipedia, the exterior algebra of a $\Bbbk$-vector space $V$ is initial with respect to being unital and there existing a $\Bbbk$-linear map $j\colon V\to A$ such that $j(v)^2=0$ for all $v\in V$. This is a reasonable algebra to consider if one is interested in measuring $k$-dimensional volumes, which are specified by $k$ linearly independent vectors, and which are degenerate if the vectors are not linearly independent (equivalently, for $char \Bbbk\neq 2$, measuring $k$-dimensional signed volumes). Then multiplication consists simply of throwing in extra vectors. My question, inspired by the recent question on the meaning of addition in the exterior algebra , is about the meaning of co-multiplication. I happen to know virtually nothing about co-algebras outside of their formalism, so in part I am looking for answers that will help me build some intuition, geometric and otherwise, about what's happening. What is a categorical argument that the exterior algebra satisfying the above universal property has co-multiplication (and is thus a bi-algbera)? Is there a (heurisitc) geometric interpretation of the exterior algebra's co-multiplication similar to the $k$-dimensional volume tracking I sketched above? (Bonus question): Is there any geometric significance to the anti-pode that makes the exterior (bi-)algebra into a Hopf algebra?","['linear-algebra', 'abstract-algebra']"
50188,Evaluate $\int{\frac{4x}{(x^2-1)(x-1)}dx}$,"So I know I need to use the partial fractions method to solve this integral. However when I split it as: $$\frac{4x}{(x^2-1)(x-1)} = \frac{Ax + B}{x^2-1} + \frac{C}{x-1}$$ I find that I can't solve for the values of A, B, C. The question actually hints that first I need to 'factor the denominator completely'. I thought it was already factored, but the solution I have factors it to: $$\frac{4x}{(x^2-1)(x-1)} = \frac{4x}{(x-1)^2(x+1)}$$ Can someone please explain the thought-process behind this transformation, why it is more 'completely factored', and why my initial set-up is incorrect? Question and solution originally from: http://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/exams/prexam4b.pdf http://ocw.mit.edu/courses/mathematics/18-01-single-variable-calculus-fall-2006/exams/prexam4bsol.pdf","['calculus', 'algebra-precalculus', 'partial-fractions', 'integration']"
50198,Acute triangle:  circles with diameters of two sides meet on the third,"Let an acute triangle ABC be given. 
Prove that the circles whose diameters are AB and AC have a point of intersection on BC.
How do I go about this problem?  Can You Please Give Me a Hint?","['geometry', 'euclidean-geometry']"
50217,An expression for $\large \frac{1}{\pi}$,"So I cooked up this expression for $1/\pi$, but I'm not immediately able to prove it, although computation shows it to be true. Perhaps somebody can think of a neat proof! $$\mathrm{Let}~~S_n=\{\omega \in \mathbb{C}: w^n=1\}.\quad \mathrm{Let}~~ a_n=\frac{1}{n}~\sup_{T~ \subseteq S_n}\left|\sum_{\omega \in T}\omega \right|.$$ $$\mathrm{Then}~~ a_n \to \frac{1}{\pi} ~\mathrm{as}~~ n \to \infty.$$ (It seems that the value of $a_n$ can usually be attained by picking all of those $n^{th}$ roots of unity in the upper half plane, for example. The limit would follow from a proof of this observation.) Thanks, and enjoy!","['pi', 'sequences-and-series']"
50222,Covectors and Vectors,"I have a general question about vector/covectors: Background. A vector (for our purposes) is a physical object in each basis of $\mathbb{R}^3$ represented by three numbers such that these numbers obey certain transformation rules when we change the basis. Let $\textbf{x}$ be an arbitrary vector and $\textbf{e}_1, \textbf{e}_2, \textbf{e}_3$ and $\tilde{\textbf{e}}_1, \tilde{\textbf{e}}_2, \tilde{\textbf{e}}_3$ be two bases. These transformation/inverse transformation rules are the following: $$\tilde{x}^{j} = \sum_{j=1}^{3} T_{i}^{j} x^i$$ and $$x^{j} = \sum_{i=1}^{3} S_{i}^{j} \tilde{x}^i$$ Question. Vectors satisfy the above properties. Now if I imagine some other set of objects that satisfy the above properties why do we call them covectors? What are covectors and how are they different from vectors if they satisfy the same properties?","['vector-spaces', 'linear-algebra']"
50224,Finding a pencil of elliptic curves parametrized by a given modular surface,"The following is an attempt to formulate a couple of questions which have been lurking in the back of my mind for a while. I'm sorry if this is long, or if my terminology is not correct, or if my notation is non-standard, or if it turns out that I didn't understand a single thing that I thought I understood. Suppose we are given a genus $0$ congruence subgroup $G$ of $\Gamma=PSL_2(\mathbb{Z})$. The (compactified) modular surface $X_G={\mathbb{H}/G}$ is then a degree $n=|\Gamma:G|$ covering of $X=\mathbb{H}/\Gamma$. Let $K'=\mathbb{C}(w)$ denote the function field of $X_G$ and $K=\mathbb{C}(j)$ denote the function field of $X$. If $G$ is normal in $\Gamma$, the field extension $K'/K$ is a Galois extension with Galois group $\Gamma/G$. Klein's $j$ function can be written as a rational function in $w$, of degree $n$. Now each elliptic curve $E/\mathbb{C}$ corresponds to a point of $X$. We can write such a curve explicitly as the locus of zeroes of a cubic in $\mathbb{CP}^2$; its coefficients are rational in $j$. For almost all choices of $E$, there are $n$ points of $X_G$ lying over $E$. In some cases, these points are known to correspond to some pairs $(E, S)$, where $S$ is some sort of substructure of $E$ (for example a ""level structure""). What's not completely obvious to me is that such pairs $(E,S)$ should also be given by a cubic or quadric equation in normal form, whose coefficients are rational in $w$. Here's an example. The group $\Gamma(2)$ is normal in $\Gamma$; the quotient $\Gamma/\Gamma(2)$ is isomorphic to $S_3$. The projection map $X_{\Gamma(2)} \to X$ is a six-sheeted ramified covering. The generator for $K'$ is then the modular function $\lambda$. The points of $X_{\Gamma(2)}$ correspond to pairs $(E, S)$, where $S$ is a basis (over $\mathbb{F}_2$) for the $2$-torsion subgroup of $E$. Since this group is isomorphic to the Klein four-group, there are indeed $6$ different choices of bases for it over $\mathbb{F}_2$. Now the fascinating thing is that we can consider such a curve as given by an equation in Legendre normal form $y^2=z(z-1)(z-\lambda)$. For most choices of $\lambda$, there are precisely six curves, in this form, which are isomorphic to this one; the other corresponding values of $\lambda$ are related by the action of $\Gamma/\Gamma(2)$. Now my first question is: how would one ""discover"" the normal form $y^2=z(z-1)(z-\lambda)$, given only the group $\Gamma(2)$? Assume, perhaps, that we know (at least) the normal form associated to $j$. Given any subgroup $G$ of $\Gamma$, can we find a normal form associated to $G$ in this manner? If the genus of $X_G$ is bigger than $0$, can we hope, perhaps, to find a similar construction? My second question is a bit more vague: what are other modular surfaces moduli spaces for? This is understood in some specific cases, but is a general answer to be hoped for? Are the examples which are understood well understood? Can we hope to associate to each modular surface a normal form, in which the extra ""structure"" $S$ (whatever it may be) is encoded? Anyways, thanks for reading, and I hope that this made sense!","['modular-forms', 'algebraic-geometry', 'elliptic-curves']"
50238,On the distribution of unimodular matrices generated by the Hermite normal form,"A problem I'm currently considering requires me to generate (pseudo-)random Gaussian integer matrices with Gaussian integer matrix inverses. By analogy with an algorithm I know for generating random orthogonal matrices, I considered an algorithm where a random Gaussian integer matrix whose real and imaginary parts are from a uniform distribution, perform a reduction to the Hermite normal form, and then take the unimodular matrix that reduces the original random matrix to Hermite form as the result. I am aware that this method misses the matrices whose determinants are $\pm i$, but I am curious as to what probability distribution the matrices generated by this method follow. By analogy, I know that the orthogonal matrices obtained from a QR decomposition of a matrix with normally-distributed entries follows the Haar distribution; is there a way to characterize the probability distribution of the unimodular factors of the Hermite decomposition of a uniformly-distributed random Gaussian integer matrix?","['hermite-normal-form', 'matrices', 'linear-algebra', 'probability-distributions', 'unimodular-matrices']"
50240,Problem in understanding set theory,"I am stuck with a small question. It is below, Let $A$ be a set and let $B = \{A,\{A\}\}$. Then, since $A$ and $\{A\}$ are elements of $B$, we have $A \in B$ and $\{A\} \in B$. It follows that $\{A\}\subseteq B$ and $\{\{A\}\} \subseteq B$. However, it is not true that $A\subseteq B$.$\hspace{302pt}\blacksquare$ I have to ask two questions from this text, What is the difference between $A$ and $\{A\}$? As $A$ is contained by $B$ so $A$ belongs to $B$ should be true but it is not. Why is that? Thanks.",['elementary-set-theory']
50253,$\subset$ vs $\subseteq$ when *not* referring to strict inclusion,"Inspired by the confusion in the comments on this question : I always thought that the standard was to read $\subset$ as ""is a strict subset of"", and $\subseteq$ could mean proper or improper inclusion. Was I wrong?","['notation', 'elementary-set-theory']"
50259,Subspaces of Sorgenfrey line and a question about Lindelöf spaces,"1) This is example 5 (page 177 of Dugundji's book). Every subspace of the Sorgenfrey line is separable yet it is not second countable. I know how to prove it is not second countable. My question is: why is every subspace of the Sorgenfrey line separable? the Sorgenfrey line is separable but not metrizable, so I don't see why this follows immediately. 2) Is $\mathbb{R}^{\omega}$ a Lindelöf space? (here $\omega$ means the natural numbers). Can we simply say that $\mathbb{R}^{\omega}$ is metrizable being the countable product of a metrizable space, $\mathbb{R}$, and also separable since the countable product of separable spaces is separable so we have a separable and metrizable space, thus Lindelöf. What is another way of seeing this? Now just for fun, what if we consider $\mathbb{R}^{\mathbb{R}}$. Is this Lindelöf?","['general-topology', 'separable-spaces', 'sorgenfrey-line']"
50269,union of two independent probabilistic event,I have following question: Suppose we have two independent events whose probability are the following: $P(A)=0.4$ and $P(B)=0.7$. We are asked to find $P(A \cap B)$ from probability theory. I know that $P(A \cup B)=P(A)+P(B)-P(A \cap B)$. But surely the last one is equal zero so it means that  result should be $P(A)+P(B)$ but it is more than $1$ (To be exact it is $1.1$). Please help me where i am wrong?,"['independence', 'probability']"
50274,Intuitive interpretation of the Laplacian Operator,"Just as the gradient is ""the direction of steepest ascent"", and the divergence is ""amount of stuff created at a point"", is there a nice interpretation of the Laplacian Operator (a.k.a. divergence of gradient)?","['multivariable-calculus', 'laplacian', 'intuition']"
50284,Is the graph of morphism of projective varieties $X \rightarrow Y$ closed in $X \times Y$?,The graph of a morphism $X \rightarrow Y$ is closed in $X \times Y$ if $X$ and $Y$ are affine varieties. What if $X$ and $Y$ are projective varieties? I am still not quite familiar with projective varieties. So I need some help. Thanks very much.,"['algebraic-geometry', 'projective-schemes', 'projective-geometry']"
50286,Does any moment generating function implies an existence of moments?,"For a real-valued r.v. $\xi$ we suppose and existence of its moment generating function $m(t) = \mathsf E\mathrm e^{t\xi}$ for all $t\in (-h,h)$ where $h>0$. I wonder how many moments $M_n = \mathsf E\xi^n$ are finite. I know that using the differentiation under the Lebesgue integral, local existence of $m(t)$ implies an existence of $M_1 = \mathsf E\xi$. On the other hand for the 2nd moment it seems that the finiteness of $M_2$ is equivalent to the statement that $m''(0)$ exists, hence there should be examples when $m(t)$ in the neighborhood of $0$ while $M_2 = \infty$. For an example when $M_2$ does exist without an existence of $m(t)$ for $t>0$ we can consider a r.v. with a density
$$
f(x) = \frac{2}{\pi(1+x^2)^2}.
$$ Could you provide such an example when $m(t)$ exists in the neighborhood of $t=0$ but $M_2 = \infty$? Maybe you can also refer me to the literature since this question is not covered in my book on the probability.",['probability']
50289,Examples of abelian group with some property,"Somebody told me there exists a torsion-free abelian group $G$, such that $G$ is isomorphic to $G\oplus G\oplus G$, but $G$ is not isomorphic to $G\oplus G$. But he does not tell me such a concrete example, since he dose not know too. I do not know if there is a well-known example about this.
  Could anyone give such an example? Or any references? Thanks.","['group-theory', 'abstract-algebra']"
50296,How to show that $\sqrt{2}+\sqrt{3}$ is algebraic?,"In Abbot's Understanding Analysis I am asked to show that $\sqrt{2}+\sqrt{3}$ is an algebraic number. I have shown that those two are algebraic separately (that was simple), but I can't figure out what to do to show that their sum is algebraic, too. For example, I tried $(\sqrt{2}+\sqrt{3})^{2}=5+\sqrt{24}$ and then I tried to think of a polynomial of form $ax^2-bx^{0}=0$ e.g. $c(\sqrt{2}+\sqrt{3})^{2}-c(5+\sqrt{24})=0$ that would work, but couldn't find the $c$ value that would make $b=c(5+\sqrt{24})$ integer, but couldn't find one. Maybe I can somehow conjecture that the sum of two algebraic numbers must be algebraic, too, but I was wondering if there's a way to find a polynomial to show this. Thanks!","['galois-theory', 'real-analysis', 'polynomials']"
50306,Tightness of a sequence of probability measures and weak convergence of a subsequence,"To establish the relation between weak convergence and characteristic functions, the book I'm studying suggest without proof the following theorem: For any tight sequence of probability measures, there exists a weakly convergent subsequence. For the proof the author refers to ""a rather difficult theorem"", the Helly selection principle. Can anyone suggest me a more simple and direct proof, at least for unidimensional case?",['probability-theory']
50321,Getting a single-value estimation of trust in a computed mean,"Suppose I have a number N of independent ratings of a given item, where each
rating is an integer between 1 and 7 (inclusive).  For simplicity sake,
let us assume the ratings are normally distributed, though the mean and
stddev will be different for each item. I'm looking for a single number which will reflect how much trust I should 
put into the currently calculated mean.  As an example, if I have N=1000 
where every single rating is of 4, this trust should be pretty close to 100%.
If on the other hand I have only N=2 where one rating is 1 and the other is 7,
my trust that the mean is 4 should be very low. I realise that a compute-trust function such as the one I've described 
above will likely need further parameterisation beyond the set of ratings.
If you prefer, I can reformulate compute-trust in terms of ""with 95%
confidence, what is the probability that the population mean will be found
within a given interval around the sample mean?"" Any thoughts on how I can compute this trust in a manner that is statistically 
sound? Edit: changed 'median' to 'mean'.",['statistics']
50342,Concrete Example Illustrating the Interior Product,"Let $V$ be a finite-dimensional vector space, let $v \in V$ and let $\omega$ be an alternating $k$-tensor on $V$, i.e., $\omega \in \Lambda^{k}(V)$. Then, the interior product of $v$ with $w$, denoted by $i_{v}$, is a mapping
$$
i_{v}:\Lambda^{k}(V)\rightarrow \Lambda^{k-1}(V)
$$
determined by $$
(i_v \omega)(v_1, \dots, v_{k-1}) = \omega(v, v_1, \dots, v_{k-1}).
$$ My understanding of this, which is probably far from complete, is that the interior product basically provides a mechanism to produce a $k-1$-tensor from a $k$ tensor relative to some fixed vector $v$. I'm trying to understand however what the interior product actually means and how it is used in practice. Therefore, my question is, Can anyone provide example(s) illustrating computations and/or physical examples that will shed light on its purpose? Also, the interior product seems to be somewhat (inversely?) related to the exterior product in that an exterior product takes a $p$-tensor and a $q$ tensor and makes a $p+q$ tensor and therefore is an ""expansion"". The interior product, on the other hand, is a contraction but always produces a tensor of degree one less than you started out with. So, secondly, What is the precise relation between the interior and exterior products ? Unfortunately, the Wikipedia page is of little help here and I can't find a reference that clearly explains these things.","['multilinear-algebra', 'differential-geometry']"
50351,"Branch points, cuts, branches, and Riemann surface","Can anyone explain branch points, cuts, branches, and Riemann surfaces on this example: 
$$
f(z)=\sqrt {z-4} + \sqrt[4]{z}
$$","['riemann-surfaces', 'complex-analysis']"
50359,Applying analysis to solve a line-of-sight problem,"This was an optional h.w. problem: You are at the origin in $\mathbb{Z}\times\mathbb{Z}$. There are trees of a fixed finite radius at each point in $\mathbb{Z}\times\mathbb{Z}$ (other than the origin). How far can you see? This was asked in (I think), the context of the Stone-Weierstrass Theorem. So I have been thinking there must be some distance where the diameters of the trees visually converge and you get a polynomial approximation of a visual boundary. Thanks for your kindness to reply.",['real-analysis']
50386,The Hessian of the Determinant,"It is well known how to take the derivative of the determinant: let $A(s)$ be a family of square matrices smoothly parametrised by the variable $s$ (in other words, $A:\mathbb{R}\to \mathbb{R}^{N^2}$ is a smooth curve). Then the derivative of the determinant can be given by $$ \frac{d}{ds} \det A(s) = \operatorname{Tr} \left(\tilde{A}(s) A'(s)\right)$$ where $A'(s) = \frac{d}{ds}A(s)$ and $\tilde{A}(s)$ is the adjugate matrix of $A(s)$. The above formula also generalises to the case that $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ is a smooth, $d$-parameter family of square matrices. On the other hand, I have not come across a nice expression for the second derivative (Hessian) of the determinant of such a family. Just by using Leibniz rule, one term is obvious: $\operatorname{Tr}\left(\tilde{A}(s) A''(s)\right)$. However, I don't know of any nice expression of the derivative of the adjugate. Evaluating the adjugate component wise, we see that it should be linear in $A'(s)$. So the question is: does anyone know of a nice form to present the derivative of the Adjugate matrix, or a nice form to present the Hessian of the determinant? For my purpose, an answer to the following weaker form of the problem would also be satisfactory: Let $A:\mathbb{R}^d\to \mathbb{R}^{N^2}$ be a smooth $d$-parameter family of symmetric matrices. Suppose $\det(A(0)) = 0$. Now, if we know that the kernel of $A(0)$ as dimension at least 2, then by a diagonalisation argument, the adjugate matrix $\tilde{A}(0)$ must vanish identically. And therefore the Hessian is determined by the term with the derivative of the adjugate. Then under the assumptions that $d \geq 2$ and $A(0)$ has nullity at least 2: (a) is it possible for the Hessian to be non-degenerate? (b) If so, what are some sufficient conditions to guarantee nondegeneracy? (In the case $d = 1$ it is clear that the ""Hessian"" can be non-degenerate, just by taking $N = 2$ and $A(s) = s I$.)","['linear-algebra', 'determinant']"
50401,Use of parenthesis in lambda calculus,"As a summer project I am trying to learn lambda calculus. I am not that good with math but I have learned myself several programming languages and somehow got the idea that learning lambda calculus would be somewhat like learning another programming language (you are welcome to correct me on that one). I have used lambda expressions in languages like F# and C#. But I am learning lambda calculus with the book ""Lambda calculus and combinators: An introduction"". In this book we are shown several examples of lambda expressions that look really familiar, but they continue the book with parenthesis omitted. The first excersise is to fill in parenthesis and lambda-symbols in lambda expressions that have been omitted. My question is: What are the ""rules"" for use of parentheses in lambda expressions? Example:
$xyz(yx)$ becomes $(((xy)z)(yx))$. I can't seem to figure out what determines where the parenthesis goes...","['calculus', 'lambda-calculus']"
50421,Characterize entire functions $f$ such that $|f(z)| \leq |\sin(z)|$ [duplicate],"This question already has an answer here : Property of Entire Functions (1 answer) Closed 9 years ago . I want to determine all entire functions $f$ such that $|f(z)| \leq |\sin(z)|$. I searched around on MathSEx and I found the following question from which I tried to get inspired but I think it differs substantially from my question: Characterizing nonconstant entire functions with modulus 1 on the unit circle Here's what I tried. It is not complete I am missing some cases.
First if $|f(z)| = |\sin(z)|$ then let $h(z)= \frac{f(z)}{\sin(z)}$. Then $h$ is analytic on $\mathbb{C}\setminus \left\{n \pi,n\in\mathbb Z\right\}$, and we also have that $|h|=1$ on the unit disc $D$. So $\exists c$ such that $h(z)=c$ with $|c|=1$. $h$ is continuous at all $n \pi$, so $h(n\pi)=c.\sin(n\pi)$; so we have that $\exists c$ such that $|c|=1$ and $f(z)=c.\sin(z)$. Now I need to take care of the case $|f(z)| < |\sin(z)|$ and I don't know what to do. Should I try to apply the Schwarz lemma to this case since we would get $|h|<1$? Any help or solution is welcomed.",['complex-analysis']
50428,"$(a_{1}+ a_{2} + ....+a_{k})^{n}$ where $k >2$, what does it generate?",Binomial expansion generates the Pascal triangle but what does it generate when you have different amount of terms there? You can see here the geometric generation with only 2 terms. I am interested to know if you have many terms.,"['sequences-and-series', 'algebra-precalculus', 'discrete-mathematics']"
50430,Closed-form Expression for $\sum_{j=0}^{k-1}(2j+2)\sum_{i=1}^j \frac 1 {i^2}$? (problem with Mathematica),"I need to calculate a closed-form expression for $\sum_{j=0}^{k-1}(2j+2)\sum_{i=1}^j \frac 1 {i^2}$. This isn't particularly difficult, and I do it by hand pretty much routinely. However I found out Mathematica's symbolic computation gives me an answer which is slightly different from the one I'd derived myself: I believe the problem arises when switching the indices of the sums, and I'm sure I'm doing it right, but of course I can't believe Mathematica would give a wrong answer (for such a trivial problem in any case). I'm purposefully omitting giving either answers so as to not bias responses (which, hopefully, there will be, as this is driving me crazy). Any comment or help appreciated, thanks! EDIT: Thanks to all who answered. What I find, by hand, is that my sum (let's call it $S$) is such that $S=k^2H_{k-1}^{(2)}-k$, where $H_{j}^{(2)}=\sum_{i=1}^j 1/j^2$. What Mathematica/Maple finds (when you've done the appropriate simplifications as suggested by Andrew) is $S=k^2H_{k}^{(2)}-k$. For instance, with Maple, I type: assume(k,posint): additionally(k>0):
sum((2*j+1)*harmonic(j,2),j=1..(k-1)); I believe I get this discrepancy when I'm switching the sum on $j$ and the sum on $i$, but I don't understand what I'm doing wrong.","['closed-form', 'discrete-mathematics', 'mathematica']"
50444,Teaching Introductory Real Analysis,"I am currently helping teach an introduction to real analysis course at UC Berkeley. The textbook we are using in Rudin's ""Principles of Mathematical Analysis"" (aka baby rudin). I am trying to find ways to help the students understand the material better. My jobs include Writing solutions to the homework exercises Finding other examples that can supplement Rudin, as Rudin sometimes doesn't present enough examples Suggest Problems for the Midterm and Final Examinations For (2), I have found Kenneth Ross's book (Elementary Analysis: The Theory of Calculus) very helpful. Does anyone suggest any other books, that do a good job in ""holding your hand and walking through"" with various concepts in analysis? Also, this is my first time teaching, so I am wondering how a good solution set should look like. Should it just show the formal solutions, or should it also tell the students a little bit about how to approach the problem and build intution. Or is that too much writing that will distract and be unmotivating for the student to read through? Also, each week we assign about 10-14 problems, but only 3 problems are graded. My solutions are to the 3 graded problems. I believe there is some advantage to this system, but do you I am putting the students at a strong disadvantage if I do not write all of the solutions? Time is limited sometimes, but I plan on writing the solutions to some other problems other than the graded problems. So: How do you decide which problems are worthy of writing solutions to (assuming time is limited and I don't have time to write solutions for all 14 problems)? Does anyone have good ideas to supplement Rudin(which I think is a great book, but my students may disagree) to the 40 student undergraduate class? This is a sort of a broad question. I'm wondering what others did in their previous experiences, if any, while teaching a class with that book?","['education', 'analysis']"
50447,Integration of powers of the $\sin x$,"I have to evalute
$$\int_0^{\frac{\pi}{2}}(\sin x)^z\ dx.$$
I put this integral in Wolfram Alpha, and the result is
$$\frac{\sqrt{\pi}\Gamma\left(\frac{z+1}{2}\right)}{2\Gamma\left(\frac{z}{2}+1\right)},$$
but I don't know why. If $z$ is a positive integer, then one can do integration by parts, many times. Eventually, this yields
$$\int_0^{\frac{\pi}{2}}(\sin x)^{2z}\ dx=\frac{(2z-1)!!}{(2z)!!}\frac{\pi}{2},$$
where $(2n-1)!!=1\cdot 3\cdots (2n-1)$, and $(2n)!!=2\cdot 4\cdots 2n$.
I appreciate your help.","['gamma-function', 'integration']"
50465,Motivating linear algebra for economics students?,"I'm a tutor for the introductory linear algebra course at my school; this course is required for most upper division economics classes, so a lot of my tutees are economics majors. This is a typical linear algebra course that focuses on things like linear dependence, subspaces, eigenvalues, etc. and does not spend time on ""practical applications"".  As a result, a lot of the economics students have no idea why they should be taking the course.  Since I don't know the first thing about economics, I also have no idea why they should be taking the course. Is it possible to convince economics students that linear algebra is important for their field?  More precisely: Are there any motivating examples where linear algebra is used in economics?","['economics', 'linear-algebra', 'education']"
50498,Numb3rs  Challenge,"I am by no means a mathmatician.  I guess you could say that I am a mathematically inclined individual, but never made anything of it until I was in my 30's and became a software engineer .  Although I've never taken the math classes that I should have, the concepts still fascinate me, and I have a question. I was watching an episode of Numb3rs and they stated what seemed like a fairly simple geometry/trigonometry question that I just could not wrap my  brain around entirely. They stated that with a few photos of the same location, by looking at the shadows caused by the sun, and knowing 1 distance in the pics and knowing the ""exact"" time stamp of the images, that they could calculate the exact (within a hundredth of a degree) location of the photos. As I said before, I am no math expert, but wouldn't you also need 1 angle?  Is what they said accurate?  Could you calculate the angle based on the timestamps and the shadows given the height of the object that made the shadow? Edit:
I'm curious, If I posted 4 pictures (15 min apart, with accurate timestamps) with at least 1 item that was a known length, would some one take me up on the challenge of locating where the photos were taken?","['geometry', 'popular-math']"
50500,Prove that minimum of $\lambda \sin \theta + (1 - \lambda) \cos \theta \le -\frac1{\sqrt 2}$,"I need a little nudge to the finish for the last bit of this problem. Express $\lambda \sin \theta + (1 - \lambda) \cos \theta$ in the form $R \sin (\theta + \phi)$, where $R(R>0)$ and $\tan \phi$ are to be given in terms of $\lambda$. Write down an expression in terms of $\lambda$ for the minimum value of $\lambda \sin \theta + (1 - \lambda) \cos \theta$ as $\theta$ varies. Show that, for all $\lambda$, this minimum is less than or equal to $-\dfrac{1}{\sqrt 2}$. The first part needs expansion like the $a\cos x + b\sin x$ formula. Let,
$$
\begin{align}
\lambda \sin \theta + (1 - \lambda) \cos \theta &\equiv R \sin (\theta + \phi) \\
&\equiv R[\sin \theta \cos \phi + \cos \theta \sin \phi] \\
&\equiv (R \cos \phi)\sin \theta + (R \sin \phi) \cos \theta \\
&\equiv a \sin \theta + b \cos \theta
\end{align}
$$ Where, $a = R \cos \phi = \lambda$ $b = R \sin \phi = (1 - \lambda)$ $\tan \phi = \dfrac{b}{a} = \dfrac{1 - \lambda}{\lambda}$ $R = \sqrt {2\lambda^2 - 2\lambda + 1}$ Thus the expression in terms of $\lambda$ is, $$\sqrt {2\lambda^2 - 2\lambda + 1}\Big(\sin (\theta + \phi)\Big)$$ Since $\sin$ has minimum value of $-1$, the minimum value of the expression is $-\sqrt {2\lambda^2 - 2\lambda + 1}$ This is as far I have gotten. I don't understand the -$1/\sqrt 2$ part? I thought may be the root $\ge$ 0 would help, and I tried solving that quadratic, but it has no real roots. How do you go about proving this? Thanks for your help!","['inequality', 'trigonometry', 'algebra-precalculus', 'maxima-minima']"
50502,Proof that $\dim(U \times V) = \dim U + \dim V$.,"The following theorem in Serge Lang's Linear Algebra is left as an exercise, namely, Let $U$ and $V$ be finite dimensional vector spaces over a field $K$, where $\dim U = n$ and $\dim V = m$. Then  $\dim W = \dim U + \dim V$, where $W = U \times V$, the direct product of the two vector spaces $U$ and $V$. Namely, $W$ contains the set of all ordered pairs $(u,w)$ such that $u \in U$ and $v \in V$. The usual axioms for such a direct product are: 1) Addition is defined component wise, namely if $(u_1,v_1),(u_2,v_2) \in W$, then $(u_1,v_1)+(u_2,v_2) = (u_1 + u_2, v_1 + v_2)$; 2)If $c \in K$, then $c(u_1,w_1) = (cu_1,cw_1)$. To prove it, let $(u_1, u_2 \ldots u_n)$ be a basis for $U$ and $(v_1,v_2 , \ldots v_m)$ a basis for $V$. So by definition, every element of $W$ can be written in the form $(a_1u_1 + \ldots a_nu_n, b_1v_1 + \ldots b_mv_m)$, where the $a_i's$ and $b_j's$ belong to the field $K$. Using the above axioms this can be rewritten as: $a_1(u_1,0) + a_2(u_2,0) + \ldots a_n(u_n,0) + b_1(0,v_1) + \ldots b_m(0,v_m)$. Doubt: If we view all the $(u_i,0)$ ordered pairs as being the ""basis"" vectors of $U$ and similarly for the $(0,v_j)$ ordered pairs of $V$, then there are $n+m$ number of them and so proving the linear independence of these objects should suffice. But I'm confused because I know that $u_i's$ by themselves are the basis vectors of $U$, but now we are talking about ordered pairs $(u_i,0)$. How can I get out of such a situation? Perhaps one can define some linear map between say a $u_i$ and the ordered pair $(u_i,0)$. $\textbf{Edit}:$ First it is easy to see that $(U \times \{0\}) \cap (\{0\} \times V)$ is the ordered pair $(0,0)$. The linear independence of the basis vectors as stated above then follows.",['linear-algebra']
50524,What is the connection between the definition of complete intersection variety and complete intersection ring?,"An algebraic variety is called a complete intersection if its defining ideal is generated
by codimension many polynomials. A Noetherian local ring $R$ is called a complete intersection if its completion is the factor ring of a regular local ring by a regular sequence. What is the connection between these two definitions?","['commutative-algebra', 'ring-theory', 'algebraic-geometry']"
50529,is any subset of a manifold a submanifold?,by definition a submanifold is a subset of a manifold which is itself a manifold. consider $A$ a subset of an $n$-manifold $M$. a neighborhood of $x\in A$ is $\mathbb R^n$ since $x$ is an element of $M$ which is a manifold (all its elements have neighborhoods homeomorphic to  $\mathbb R^n$ does this mean that every subset $A$ of $M$ is a submanifold? my guess is no and i think there is some thing wrong with my notion of neighborhood but i can't see how?,"['general-topology', 'manifolds', 'differential-topology']"
50533,Understanding orientability of vector bundles,"I'm having trouble understanding how orientability of vector bundles work. The book I'm reading, Spivak's A comprehensive introduction to differential geometry , is not very clear on this. Edit : Definition If $V$ is a real vector space, and $(b_1 \ldots b_n), (c_1 \ldots c_n)$ are two ordered bases with $b_j=m^i_j c_i$, we say that they are equally oriented if $\det(m^i_j)>0$. An orientation of $V$ is an equivalence class of ordered bases. Let us take a rank $n$ vector bundle $E(B, \mathbb{R}^n, \pi)$ ($E$=total space, $B$=base space, $\mathbb{R}^n$=typical fibre, $\pi$=projection). We say that $E$ is orientable if we can find a trivializing atlas $\mathcal{A}=\{(U_\alpha, t_\alpha)\}_{\alpha}$ such that $t_\alpha \circ t_{\beta}^{-1}$ is orientation-preserving on every fibre of $U_{\alpha}\cap U_{\beta}\times \mathbb{R}^n$. Ok. What I don't get very well is how this allows us to consistently choose an orientation on each fibre of $E$ . I guess we need to import the oriented structure of $U \times \mathbb{R}^n$ into $E$ someway, but I can't seem to focus the details very well. Can you explain this to me? Alternatively, you can give me a good reference too. Thank you. Bonus question ( secondary ) Spivak's book adopts a different definition, that is: a family $\{\mu_p\}_{p \in B}$ of orientations for $\pi^{-1}(p)$ is an orientation of $E$ if the following compatibility condition is satisfied: If $t \colon \pi^{-1}(U)\to U \times \mathbb{R}^n$ is an equivalence (= vector bundle isomorphism ) and the fibres of $U \times \mathbb{R}^n$ are given the standard orientation, then $t$ is either orientation preserving or orientation reversing on all fibres. I cannot understand the link between this definition and the one above.","['reference-request', 'differential-geometry']"
50542,Proof of Euler's Theorem without abstract algebra?,"Every proof I've seen of Euler's Theorem (that $\gcd(a,m) = 1 \implies a^{\phi(m)} \equiv 1 \pmod m$ ) involves the fact that the units of $\mathbb{Z}/m\mathbb{Z}$ form a group of order $\phi(m)$ .  While this is a perfectly good proof, I have to wonder if it was the one that Euler used.  I know that there are fairly old precursors to group theory, but it still seems incongruous. Thus, my question is: Are there proofs of Euler's Theorem that do not use group/ring theory?  In particular, what proof (if any; I don't know whether Euler actually discovered the theorem) did Euler use himself?","['totient-function', 'elementary-number-theory', 'abstract-algebra', 'math-history']"
50543,Indefinite integral $\int^{\infty}_{0}\frac{x}{x^4+1}dx$ via residues,"I want to compute $\displaystyle \int^{\infty}_{0}\frac{x}{x^4+1}dx$ using the residue theorem. The poles in the upper half plane are: location: $\large e^{\frac{\pi i}4}$, order: 1, residue: $\large\frac{1}{4}e^{\frac{3\pi i}2}$ location: $\large e^{\frac{3\pi i}4}$, order: 1, residue: $\large \frac{1}{4}e^{\frac{\pi i}2}$ The problem is that the integral from $-\infty$ to $\infty$ vanishes for symmetry reasons, so I cannot apply the standard approach of putting the half of a 1-sphere on top of the real axis and letting its radius go to infinity. If x was replaced with $x^2$ for instance, I could just divide the result by two. Is there another way of contour integration to evaluate the upper expression?","['integration', 'complex-analysis']"
50549,"Given a real function $g$ satisfying certain conditions, can we construct a convex $h$ with $h \le g$?","The following is Exercise 8 from Chapter 3 of Rudin's Real and Complex Analysis (not a homework problem, just for fun). Let $g$ be a positive function on $(0, 1)$ such that $g(x) \to \infty$ as $x \to 0$. Does there exist a convex function $h$ on $(0, 1)$ such that $h \le g$ and $h(x) \to \infty$ as $x \to 0$? This seems true, but I can't show it. All I've been able to do is to reduce the problem to $g$ being a monotone step function or a strictly monotone piecewise-linear function but this doesn't seem to get me anywhere. I'm not sure how to use the property of $g$ to construct the explicit $h$.",['real-analysis']
50555,Finding a Möbius transformation,"Let $f$ be a holomorphic mapping from {$z:\Re(z)>0$} into itself. Let $1$ be a fixed point of $f$. In addition suppose that $\left|\frac{f(2)-1}{f(2)+1}\right|=\frac13$. I want to show that $f(z)=\frac{az+b}{bz+a}$ where $a=1+e^{i\theta}$ and $b=1-e^{i\theta}$ for some $\theta$. I tried looking at this problem in different ways, I just don't know what to do. Of course if $f(z)=\frac{az+b}{cz+d}$ then I have the obvious $a+b=c+d$. And when I look at  $\left|\frac{f(2)-1}{f(2)+1}\right|=\frac13,$ this actually looks like  $\left|\frac{f(2)-f(1)}{f(2)+f(1)}\right|=\frac13,$ but I don't know what to do with this last expression or if it is useful at all to write things this way and compute this expression. In general I know that a Möbius transformation has at most 2 fixed points and can be written as a composition of inversions, rotations, dilations and translations. Thanks.","['conformal-geometry', 'complex-analysis']"
50558,Unbiased estimator and Variance,"I am having a hard time trying to solve this problem. I don't know how to start it. Any help would be greatly appreciated. Let T be any unbiased estimator of $\tau(\theta),$ and let $W$ be a sufficient statistic for theta. Define $\phi(W)=E[T|W]$. Show that $\phi(W)$ is an unbiased estimator of $\tau(\theta),$ and $\mathrm{Var}(T)=\mathrm{Var}\left(\phi(W)\right) + E\left[\mathrm{Var}(T|W)\right]$. The only thing I know is that this may be a part of the Rao-Blackwell Thm.",['statistics']
50572,Calculate measurements for a diagonal fence beam,"Given the width W and the height H of a rectangle, and the thickness T of a beam extending exactly from the upper left corner to the lower right corner as shown, how do I solve for length X and angle $\alpha$ as shown in the diagram below? The top of the beam touches the top left corner and the bottom of the beam touches the lower right corner. Many thanks to Isaac for the excellent diagram! Update With the help of Isaac's answer and some careful measurement and cutting with a miter saw, my diagonal beam press-fit into the gate frame exactly and stayed by friction for me to fasten it in. Perfection! W $= 39.4375$ in H $= 20.6875$ in T $= 1.53125$ in X $= 43.77$ in $\alpha = 64.29^\circ$ cutting angle $= 25.71^\circ$","['geometry', 'trigonometry', 'euclidean-geometry']"
50574,Does solution of $ x=\sum_{n=0}^\infty e^{-A_n/x}$ exist?,"Usually when working with indefinite sums, I want to work out the sum or whether it convergence. But now I encountered a problem they other way around and I'm clueless... Is there even a general solution of $$ x=\sum_{n=0}^\infty e^{-A_n/x}$$
for $A_n$, where $x$ is given and real, $A_n >0\space\forall n$ and $\frac{dA_n}{dx}=0\space\forall n$? Thank you EDIT: To make my question clearer for the commenters and others, I'm searching for a systematic sequence $A_n$ which, when entered in the equation above, yields $x$ and this should hold for all (real) $x$.",['sequences-and-series']
50592,Finding eigenfunctions and eigenvalues,"Let $K$ be the integral operator defined by $$ (Kf)(x)=\int_0^1 u(x)v(y)f(y) dy $$ for some continuous functions $u,v$ in the Hilbert space with inner product $\langle f,g \rangle = \int_0^1 f(x)^* g(x) dx$ on $(0,1)$. I want to find the eigenfunctions and eigenvalues corresponding to $K$. (this is problem 3.4 in http://www.mat.univie.ac.at/~gerald/ftp/book-fa/ ) The exercise is from a chapter about compact symmetric operators (which this operator is), but it only contains existence theorems. If I could get some helpful hints on how to get started, I'd be thankful. (I have a suspicion this is easier than it looks) Thanks in advance.","['functional-analysis', 'integral-equations']"
50608,particular solution of $4y''-y= \sin(x)\cdot \cos(x/2)$,"So I'm working with a nonhomogeneous second order differential equation: $$4y''-y=\sin(x)\cos(x/2).$$ I know that the general solution, $y$, equals $y_c + y_p$ where $y_c$ is the general solution to the complementary equation and $y_p$ is any particular solution to the nonhomogeneous equation. I'm struggling a little bit with $y_p$ because I'm not sure what form the particular solution should be. I know (at least I think I do) that, for example, the general form of the particular solution for $\cos(x/2)$ is: $$A\sin(x/2) + B\cos(x/2).$$ I also suspect that the general form of the particular solution for $\sin(x) + \cos(x/2)$ is: $$A\sin(x) + B\cos(x) + C\sin(x/2) + D\cos(x/2).$$ However, I'm completely thrown off track with $\sin(x)\cdot\cos(x/2)$. I'd appreciate any insight on the matter, because frankly, the entire concept is still a little loose in my head.",['ordinary-differential-equations']
50612,Continuous representatives in Sobolev Spaces,"My question arise from the study of the possible extensions of Rademacher's Theorem to the Sobolev Space $W^{1,p}(\Omega)$, with $\Omega\subset \mathbb{R}^n$. In specific I'm studying the proof of the fact that any element of $W^{1,p}(\Omega)$ is differentiable almost everywhere if $p>n$. A key result in the proof is that any element in $W^{1,p}(\Omega)$, if $p>n$, has a continuous representative. Unfortunately I did not found any reference for this result. Moreover, looking on Wikipedia's page on Sobolev inequalities , I discovered that in my setup (which is part of the general case $k<\frac{p}{n}$) every element of $W^{1,p}(\Omega)$, if $p>n$, should be an Holder's continuous function. I'm puzzled by that, because I always thought to the element of a Soboloev Space as class of functions, and seems to me unrealistic that any element of any class of those Sobolev spaces is Holder continuous (which, if I'm not wrong, is stated as a property that holds everywhere). So my questions are: 1) Do you have a reference which explain why any class of functions in $W^{1,p}(\Omega)$ (for $p>n$) has a continuous representative? 2) Is it correct the result stated by Wikipedia on Holder continuity? If the answer is yes, where am I wrong in thinking Sobolev functions? Thank you very much for your time!","['sobolev-spaces', 'real-analysis', 'analysis']"
50630,Scoring system for two-party game,"Suppose we have a game that operates with '$a~$' players for one party, and '$b~$' players on the opposing party. Each party has a long-run average win percentage for this game, with $A\%$ and $B\%~$(such that $(A + B) = 100~$) respectively for the first and second party. After $(a+b)$ players are willing to participate in the game, each will be assigned to a party (with equal chance of joining either party) Firstly, I was wondering if the expected long-run average win percentage for a random player joining this game, before his alignment has been assigned, can be calculated using the formula $\frac{aA + bB}{a + b}\% ~$, and if there are any improvements that can be made to this description. More particularly, this game is one of a system of games. The system aims to discourage players exclusively selecting games with a high $\frac{aA + bB}{a + b}\% ~$ percentage, or in other words, games in which a random player joining can more easily win ( before his alignment has been assigned) Furthermore, suppose the first party wins the game, and one were to allocate points to the party based on some scoring system. The maximum number of points allowed to be allocated to a specific party is 100 points. Would the following formula allocate the points in a fair manner, under the assumption that loss rate of party $\propto$ points deserved ? $\displaystyle \frac{(100 - A) + (100 - \frac{aA + bB}{a + b})}{2}~$ noting that $(100 -A) = B$ How could this model possibly be improved? Please comment if this description is too vague or unclear, and I apologize if this question is not suitable for this website. Note: In the formula, I'm averaging functions of two values (long-run average win percentage before and after alignment has been dictated) to determine the points deserved. As an example, suppose a game has 9 players for the first party, and 1 player for the second. The long-run average win probabilities are 60% and 40% respectively for each party. By the first formula, the average win probability for a random player joining this game is $\frac{60\times9 + 40}{9 +1}\%=58\%$ The scoring system punishes joining games with a high win probability, hence, half the score will be determined by $(100 - 58) = 42~$,  as loss rate $\propto$ points deserved. The scoring system also rewards games won by a party with a high win probability less , hence if the first party wins, the second half of the score is determined by $(100 - 60) = 40~$, as loss rate of party $\propto$ points deserved. Therefore, the final score is the average, $\frac{40 + 42}{2} = 41~$.This is modeled by the above formulas. Comment: Specifically, the variation in long-run average win percentages for the parties arises naturally as a result of specific methods, unique to each party , that can be exploited to win the game,  and thus, this may unintentionally favor a specific party (as reflected in win probabilities, if $A\ne B$)","['statistics', 'probability']"
50633,Dominant finite morphism and finite algebraic extension,"I don't know how to prove the following proposition. If two varieties $X$ and $Y$ are irreducible,  a morphism $\phi: X \rightarrow Y$ is dominant and finite, then $K(X)$ is a finite algebraic extension of $\phi^*K(Y)$. Here, $\phi^*: K[Y] \rightarrow K[X]$ sends a function $f \in K[Y]$ to $f \circ \phi \in K[X]$. A morphism $\phi$ of irreducible varieties is called dominant if $\phi(X)$ is dense in $Y$. It is called finite if $K[X]$ is an integral over $\phi^*K[Y]$. Is the following more general statement true? $R_1$ is a domain, which is integral over its subdomain $R_2$. $F_1$ and $F_2$ are respective fields of fractions. Then $F_1/F_2$ is a finite algebraic extension. This extension must be algebraic. But I think the finiteness is not so obvious.",['algebraic-geometry']
50655,Proving the countability of algebraic numbers,"I am trying to prove that algebraic numbers are countably infinite, and I have a hint to use: after fixing the degree of the polynomial, consider summing the absolute values of its integer coefficients, and setting the sum less than or equal to $m$, for each $m \in \mathbb{N}$. I am also allowed to use the fact that every polynomial has a finite number of roots. I am not sure where to begin... Perhaps after fixing the degree of the polynomials, I could try enumerating them, and after proving countability of the set that contains the roots of all polynomials of a certain degree, I could state that the union of infinitely many ($\bigcup_{n=1}^{\infty}$, $n \in \mathbb{N}$) countably infinite sets is countable, and thus the algebraic numbers are countably infinite, too, but I am not sure if this is a good approach/how to enumerate the polynomials. Thanks!","['elementary-set-theory', 'real-analysis']"
50659,Intuition for Little Picard's theorem,"Little Picard's theorem is the following:  Suppose $f:\mathbb{C}\rightarrow \mathbb{C}$ is entire.  Then either 1)  $f$ is constant 2)  $f$ is surjective or 3)  $f$ is onto $\mathbb{C}-\{p\}$ for some point $p\in \mathbb{C}$. Said another way, an entire function which misses 2 points in the codomain is actually constant. The exponential function $f(z) = e^z$, which is never $0$, shows that in general all 3 cases can arise. As someone who knows a bit of differential geometry and algebraic topology, the proof I like (indeed, the only one I remember) goes as follows:  Consider $X= \mathbb{C}-\{p,q\}$ where $p$ and $q$ are distinct complex numbers.  Then an entire map $f:\mathbb{C}\rightarrow X$ lifts to the universal cover $\mathbb{D}(0,1)$ (the unit disc around $0$ in $\mathbb{C}$) of $X$.  The lift of $f$ is an entire bounded function, and hence, by Liouville's theorem, is constant.  This easily implies $f$ itself is constant.  $\square$ The part that's hazy, to me, is the justification that the universal cover of $X$ is (biholomorphic to) the unit disc.  Of course, by uniformization, the universal cover is biholomorphic to either $\mathbb{C}$, $\mathbb{D}(0,1)$, or $S^2$.  Since $S^2$ is compact and $X$ is not, it cannot be $S^2$.  Interestingly, if we remove only a single point from $\mathbb{C}$ (which we assume wlog is $0$), then $e^z$ is a covering map from $\mathbb{C}$ to $\mathbb{C}-\{0\}$.  In other words, the universal cover a once punctured plane is $\mathbb{C}$ but the universal cover of a twice or more punctured plane is the unit disc. Is there good intuition as to why 2 is the crucial number of punctures for which the universal cover is no longer biholomorphic to $\mathbb{C}$? Relatedly (and nicer since it would allow me to avoid using uniformization, which I feel is way overkill for this), Is the universal covering map $\pi:\mathbb{D}(0,1)\rightarrow X$ easy, or even possible, to explicitly write down? By translating, rotating, and scaling, one can obviously assume that $\{p,q\} = \{0,1\}$ A quick google search shows the answer to the second question should be ""yes"", and that $\pi$ can be expressed in terms of modular functions.  Sadly, I don't know anything at all about modular functions, but I couldn't find anyting that tells how to express $\pi$ with modular functions.",['complex-analysis']
50671,Why isn't there a good product formula for antiderivatives?,"Computing antiderivatives is more challenging than computing derivatives, in part due to the lack of a ``product formula''; namely, while $(fg)'$ can be expressed in terms of $f,f',g,g'$, there seems to be no way to express $\int fg$ in terms of $f, \int f, g \int g$ and related quantities. Is there any intuitive reason or heuristic explanation for why no such formula exists? I'm looking for a non-rigorous explanation which can be understood by first-year calculus students.",['calculus']
50678,Trigonometry: Solve $(1-\cos\alpha)^2 + \sin^2\alpha = d^2$ for $\alpha$,"My next step in implementing my algorithm in Java is following. It is quite difficult to explain, but I know what I need. I have this equation: Given: d Asked: $\alpha$ $$(1-\cos\alpha)^2 + \sin^2\alpha = d^2 $$ Which I ""simplified"" to this, using this formula's : $$2+2\cos\alpha+\frac{\cos2\alpha}{2}-\frac{\sin\alpha}{2} = d^2$$ But now, I'm stuck. This is probably pretty easy, but I'm 15 years old at the moment. I didn't see that much trigonometry in school yet. Can you help me?",['trigonometry']
50679,Intuition regarding Chevalley-Warning Theorem,"Three versions of the theorem are stated on pages 1-2 in these notes by Pete L. Clark: http://alpha.math.uga.edu/~pete/4400ChevalleyWarning.pdf Could anyone offer some intuitive way to think about this theorem?
Thanks!","['finite-fields', 'abstract-algebra', 'polynomials', 'algebraic-number-theory', 'intuition']"
50695,Always solving systems of linear equations wrong - what do I do wrong?,"Dear ladies and gentlemen, over time I noticed I (and other) again and again have problems solving ""systems of linear equations"". It seems depending of the steps one chooses, we get different results!! How can that be? Should one not always get the same results no matter which path he goes down? What am I missing? Are there maybe rules I don't know and use even though I shouldn't? I want to give you an example. The set of equations is from a state-price security calculation (see ""State Preference Approach"") which we shall solve using the Gaussian elimination: I: 2P(1) + 2P(2) + 2P(3) =1,6 II: 3P(1) + 0P(2) + 1P(3) =1,0 III: 0P(1) + 2P(2) + 1P(3) =0,8 The solution shall be: P(1) = 0,2 P(2)=0,2 and P(3)=0,4 Not only got I different numbers on the first try that totally went in ""into space"", I want to write down for you my second approach to check for mistakes: II-III = IV = 3P(1) - 2P(2) = 0,2 --> 2P(2) = 3P(1) - 0,2 (this far I'm with the solution)
then I simply plug in the result in the following lines: in III: 4P(1) = 1 --> P(1) = 0,25 in II: 0,75 + P(3) = 1,0 --> P(3) = 0,25 in I: 0,5 + 2P(2) + 0,5 = 1,6 --> P(2) = 0,3 But these results seem to differ. So it seems I clearly miss some important rule! Must I not ""plug in"" results into other rows, as the ""Gaussian"" system seems to avoid?
But how can there be a difference / can I be forced to avoid ""plugging in"" results? This should normally be allowed, shouldn't it? Can you help me find my blind spot? Thanks for your help in advance.",['matrices']
50719,Is this a recurrence for the Mertens function plus 2?,"If we define a symmetric array: $$T(1,1)=3,\; T(1,2)=2,\; T(2,1)=2$$ $$T(1,k)=\frac{-T(n,k-1)-\sum\limits_{i=2}^{k-1}T(i,k)}{k+1}+T(n,k-1)$$ $$ T(n,1)=\frac{-T(n-1,k)-\sum\limits_{i=2}^{n-1}T(n,i)}{n+1}+T(n-1,k)$$ $$ n\geq k>1: T(n,k) = -\sum\limits_{i=1}^{k-1}T(n-i,k)$$ $$ k>n>1: T(n,k) = -\sum\limits_{i=1}^{n-1}T(k-i,n)$$ starting: $$ T(n,k) = \begin{bmatrix} +3&+2&+1&+1&+0&+1&+0 \\ +2&-2&+2&-2&+2&-2&+2 \\ +1&+2&-3&+1&+2&-3&+1 \\ +1&-2&+1&+0&+1&-2&+1 \\ +0&+2&+2&+1&-5&+0&+2 \\ +1&-2&-3&-2&+0&+6&+1 \\ +0&+2&+1&+1&+2&+1&-7 \end{bmatrix}$$ ...and as a Mathematica program: t[n_, k_] := 
t[n, k] = 
   If[And[n == 1, k == 1], 3, 
    If[Or[And[n == 1, k == 2], And[n == 2, k == 1]], 2, 
     If[n == 1,(-t[n,k-1]-Sum[t[i,k],{i,2,k-1}])/(k+1)+t[n,k-1], 
      If[k == 1,(-t[n-1,k]-Sum[t[n,i],{i,2,n-1}])/(n+1)+t[n-1,k], 
       If[n >= k, -Sum[t[n - i, k], {i, 1, k - 1}], -Sum[
          t[k - i, n], {i, 1, n - 1}]]]]]];
nn = 81;
MatrixForm[Table[Table[t[n, k], {k, 1, nn}], {n, 1, nn}]];
Table[t[1, k], {k, 1, nn}] - 2 Then we get in the first row and first column a sequence starting: 3, 2, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, -1, 0, 1, 1, 0, 0, -1, -1,... By subtracting the sequence with 2, do we then get the Mertens function? 1, 0, -1, -1, -2, -1, -2, -2, -2, -1, -2, -2, -3, -2, -1, -1, -2, -2, -3, -3,... The Mertens function is the partial sums of the Möbius function. Edit Nov 6 2011: Excel spreadsheet formulas for the array: European version: =if(and(row()=1; column()=1); 3; if(or(and(row()=1; column()=2); and(row()=2; column()=1)); 2; if(row()=1; (-indirect(address(row(); column()-1))-sum(indirect(address(2; column())&"":""&address(column()-1; column()))))/(column()+1)+indirect(address(row(); column()-1)); if(column()=1; (-indirect(address(row()-1; column()))-sum(indirect(address(row(); 2)&"":""&address(row(); row()-1))))/(row()+1)+indirect(address(row()-1; column())); if(row()>=column(); -sum(indirect(address(row()-column()+1; column())&"":""&address(row()-1; column()))); -sum(indirect(address(column()-row()+1; row())&"":""&address(column()-1; row())))))))) American version: =if(and(row()=1, column()=1), 3, if(or(and(row()=1, column()=2), and(row()=2, column()=1)), 2, if(row()=1, (-indirect(address(row(), column()-1))-sum(indirect(address(2, column())&"":""&address(column()-1, column()))))/(column()+1)+indirect(address(row(), column()-1)), if(column()=1, (-indirect(address(row()-1, column()))-sum(indirect(address(row(), 2)&"":""&address(row(), row()-1))))/(row()+1)+indirect(address(row()-1, column())), if(row()>=column(), -sum(indirect(address(row()-column()+1, column())&"":""&address(row()-1, column()))), -sum(indirect(address(column()-row()+1, row())&"":""&address(column()-1, row()))))))))","['recurrence-relations', 'sequences-and-series', 'number-theory']"
50720,Exotic Manifolds from the inside,"As we know, an exotic $\mathbb{R}^4$ is a manifold which is homeomorphic, but not diffeomorphic to the standard $(\mathbb{R}^4,id)$, and there are even very explicit descriptions of them (Kirby diagrams, etc). Being descriptions from the ""outside"" (forgive the less exact tone) here, I wonder if there are ""inside"" descriptions, i.e.: When you are sitting inside a four-manifold which have the topological properties of being an $\mathbb{R}^4$ (for example the right homology and homotopy properties), can you decide (and if yes, how) that it is not the standard $(\mathbb{R}^4,id)$, but an exotic one ?","['general-topology', 'philosophy', 'low-dimensional-topology', 'differential-topology']"
50735,p-adic norms and products,"I came across the following problems about p-adic norms: Problem. Show that $$\prod_{p} |x|_p  = \frac{1}{|x|}$$ where the product is taken over all primes $p = 2,3,5, \dots$ and $x \in \mathbb{Q}$. We have the following: $|x|_2 = 2^{-\max \{r: 2^{r}|x \}}$, $|x|_3 =  3^{-\max \{r: 3^{r}|x \}}, \ \dots$ so that $$\prod_{p} |x|_p =  \frac{1}{2^{\max \{r: 2^{r}|x \}}} \cdot  \frac{1}{3^{\max \{r: 3^{r}|x \}}} \cdots$$ Then it seems that by the Fundamental Theorem of Arithmetic the result follows. Is this the right idea? Problem. If $x \in \mathbb{Q}$ and $|x|_p \leq 1$ for every prime $p$, show that $x \in \mathbb{Z}$. We know that  $$\prod_{p} |x|_p  = \frac{1}{|x|} \leq 1$$ Then suppose for contradiction that $x \notin \mathbb{Z}$? Or maybe the product is a null sequence? Added. Suppose $x \notin \mathbb{Z}, \ x \in \mathbb{Q}$. Then $x = \frac{r}{s}$ where at least one prime $p$ divides $s$. Then $\text{ord}_{p} x = \text{ord}_{p} r- \text{ord}_{p} s$. So $$|x|_{p} = p^{-\text{ord} _{p} x} = p^{ \text{ord}_{p} s- \text{ord}_{p} r}$$ $$= \frac{p^{\text{ord}_{p} s}}{p^{\text{ord}_{p} r}} > 1$$ which is a contradiction?","['normed-spaces', 'number-theory']"
50746,General term formula of $S_n=\sum\limits_{i=1}^n{\frac{a^i}{i!}}$?,"If $S_n=\sum\limits_{i=1}^n{\frac{a^i}{i!}}$, where a is a positive number, then what's the general term formula of $S_n$?",['sequences-and-series']
50750,"If a sequence of boundaries converges, do the spectrums of the enclosed regions also converge?","A planar region will have associated to it a spectrum consisting of Dirichlet eigenvalues, or parameters $\lambda$ for which it is possible to solve the Dirichlet problem for the Laplacian operator, $$ \begin{cases} \Delta u + \lambda u = 0 \\ u|_{\partial R} = 0 \end{cases}$$ I'm wondering, if we have a sequence of boundaries $\partial R_n$ converging pointwise towards $\partial R$, then will the spectrums also converge? (I make the notion of convergence formal in the following manner: $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\partial R_n)=\partial R$; $\cap_{N=1}^\infty l(\cup_{n=N}^\infty\mathrm{spec}(R_n))=\mathrm{spec}( R)$, where $ l(\cdot)$ denotes the set of accumulation points of a set and $\mathrm{spec}(\cdot)$ denotes the spectrum of a region.) One motivating pathological example is the sequence of boundaries, indexed by $n$, defined by the polar equations $r=1+\frac{1}{n}\sin(n^2\theta)$. The boundaries converge to the unit circle. However, since the gradient of any eigenfunction must be orthogonal to the region boundary (as it is a level set), the eigenfunctions can't possibly converge to anything (under any meaningful notion) and so it makes me question if it's even possible for the eigenvalues to do so. If the answer is ""no, the spectrum doesn't necessarily converge,"" a much broader question arises: what are necessary and sufficient conditions for it to converge? Intuitively, I imagine a necessary condition is that the curvature of the boundaries also converge appropriately, but I have no idea if that's sufficient. EDIT: Another interesting question is if the principal eigenvalue (the smallest nonzero one) can grow arbitrarily large.","['differential-geometry', 'analysis']"
50751,"Taylor Series expansion at z=0, and radius of convergence","I have the following question: Consider the domain
$$
D=B(0,1)\cup B\left(\frac{1}{2}, 1\right)
$$
It is given
that $f:D\rightarrow \mathbb{C}$ is an analytic function in $D$, and $f^{(n)}(0)$ is a positive real number for every positive integer $n$. Let $R$ be the radius of convergence of the Taylor series of $f$ at $z=0$. Is it true that $R>1$?
$$
$$
I have attached my proof to the following problem, although I am not sure if it correct, as I have clearly not used the fact that $f^{(n)}(0)$ is a positive real number for every positive integer $n$. How do I make use of this fact to prove/disprove the statement?
$$
$$
Proof:
Since $f$ is analytic on the ball $B(0,1)$, it follows from the definition of radius of convergence that $R\geq1$.
Suppose on the contrary that $R=1$.
By Taylor's Theorem, we may express $f$ as a Taylor series at $z=0$ as follows:
$$f(z)=\sum_{n=0}^\infty\frac{f^{(n)}(0)}{n!}z^n$$
where the series converges absolutely for all $z\in B(0,1)$, and diverges for all $|z|>1$.
Thus, by differentiating both sides of the above equation $k$ times, we have that for all $z\in B(0,1)$,
$$
f^{(k)}(z)=\sum_{n=k}^\infty\frac{f^{(n)}(0)}{(n-k)!}z^{n-k}.
$$
Also, since $f$ is analytic on the ball $B\left(\frac{1}{2},1\right)$, it follows from Taylor's Theorem that we may also express $f$ as a Taylor series at $z=\frac{1}{2}$ as follows:
$$
f(z)=\sum_{k=0}^{\infty}\frac{f^{(k)}\left(\frac{1}{2}\right)}{k!}\left(z-\frac{1}{2}\right)^k,
$$
where the series converges absolutely for all $z\in B\left(\frac{1}{2},1\right)$.
Now, by setting $z=\frac{1}{2}$, we have that for all $k\geq0$,
$$
f^{(k)}\left(\frac{1}{2}\right)=\sum_{n=k}^{\infty}\frac{f^{(n)}(0)}{(n-k)!}\cdot\frac{1}{2^{n-k}}.
$$
Then for all $z\in B\left(\frac{1}{2},1\right)$, we have the following:
$$
f(z)
=\sum_{k=0}^{\infty}\frac{f^{(k)}\left(\frac{1}{2}\right)}{k!}\left(z-\frac{1}{2}\right)^k
=\sum_{k=0}^{\infty}\sum_{n=k}^{\infty}\frac{f^{(n)}(0)}{(n-k)!k!}\cdot\frac{1}{2^{n-k}}\cdot\left(z-\frac{1}{2}\right)^k
$$
$$
=\sum_{n=0}^{\infty}\sum_{k=0}^n\frac{f^{(n)}(0)}{(n-k)!k!}\cdot\left(\frac{1}{2}\right)^{n-k}\cdot\left(z-\frac{1}{2}\right)^k
=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}\sum_{k=0}^n\frac{n!}{(n-k)!k!}\cdot\left(\frac{1}{2}\right)^{n-k}\cdot\left(z-\frac{1}{2}\right)^k
$$
$$
=\sum_{n=0}^{\infty}\frac{f^{(n)}(0)}{n!}z^n.
$$
Note: The interchanging of the summations is possible as the series $\sum_{k=0}^{\infty}\frac{f^{(k)}\left(\frac{1}{2}\right)}{k!}\left(z-\frac{1}{2}\right)^k$ converges absolutely for all $z\in B\left(\frac{1}{2},1\right)$; this follows from the Rearrangement Theorem, where any rearrangement of an absolutely convergent series converges to the same sum as the original series.
This implies that the Taylor series of $f$ at $z=0$ converges for all $z\in B\left(\frac{1}{2},1\right)$; and in particular for all $z\in\mathbb{R}$, $1<z<\frac{3}{2}$, which contradicts the fact that the series diverges for all $|z|>1$. So we must have $R>1$ as desired.",['complex-analysis']
50757,"Why is ""glide symmetry"" its own type?","Artin's Algebra pages 155 & 156 list the types of symmetry of a plane figure as: Reflective Rotational Translational Glide He then goes on to say ""Figures such as wallpaper patterns may have two talk about other figures having combinations of independent"" symmetries. EDIT: He says ""... having combinations of independent translational symmetries"". See Joriki's answer. Why doesn't glide symmetry count as having two independent (reflection + translation) symmetries? If we're going to count combinations, why not have rotation + translation etc. as their own types too?","['transformational-geometry', 'group-theory', 'abstract-algebra']"
50765,Likelihood Ratio Test,"I am having a hard time understanding what this following question is asking. I do not have any clue pn how to start it. Any help would be great. Thanks in advance. 
Let $X_1,X_2,...,X_n$ be a random sample, $X=(X_1,X_2,...,X_n)^T$, $X=x \in B \subset R^n$ be the observed sample set. Let $f(x|\theta)$ be the joint pdf of the given sample $X$, and let $\Theta$ be an entire parameter set in $R$. For $\Theta \neq \phi$,  define $$\lambda(x)=\frac{\text{sup}_{\Theta_0}L(\theta|x)}{\text{sup}_{\Theta}L(\theta|x)}$$
1) Show that $0\leq \lambda(x) \leq 1$ for all $x\in B$. 2) As $\text{sup}_ \Theta$ $L(\theta|x)$ increases, $\lambda(x)$ decreases. 3) $$\lambda(x)=\frac{L(\hat{\theta}_{0}|x)}{L(\hat{\theta}|x)}$$ 4) $$\lambda(x)=\frac{\text{sup}_{\Theta_0}g(T(x)|\theta)}{\text{sup}_{\Theta}g(T(x)|\theta)}=\lambda^{*}(x)$$ where $T(X)$ is a sufficient statistics and $f(x|\theta)=g(T(x)|\theta)h(x).$ 5) As $\text{sup}_{\Theta}L(\theta|x)$ decreases, $\lambda(x)$ increases. 6) Under what conditions on $\Theta_{0}$ , $\lambda(X)$ is called the likelihood Ratio test statistic? 7) Assuming that $\lambda(X)$ is a Likelihood ratio test statistic, what can you say about $\{x: \lambda(x) \leq c \}$ for any number $c$ satisfying $0 \leq c \leq 1$?",['statistics']
50786,What is a Form Domain of an Operator?,"I tried to look this up on Wikipedia, but I couldn't find anything. I am reading Barry Simon's book ""Schrödinger Operators"", where he brings up the concept of a form domain $Q(A)$ of a self-adjoint operator $A$. I looked in his mathematical physics series, but couldn't pinpoint a definition. In parallel to stating some perturbation theorems about form domains, he also states theorems about operator domains, which are quite self-explanatory. I've never heard of form domains however. What are they? How and where do they come up in operator theory? Why are they important, generally speaking?","['operator-theory', 'functional-analysis']"
50790,Coincidence criterion (measure theory),"The following theorem has been mentioned (and partially proved) in the book Functions of Bounded Variation and Free Discontinuity Problems by Luigi Ambrosio et. al. Let $\mu,\nu$ be positive measures on $(X,\mathcal{E})$. Assume that they are equal on a collection of sets $\mathcal{G}$ which is closed under finite intersections. Also assume that there are sets $X_h \in \mathcal{G}$ such that $\displaystyle{X= \bigcup_{h=1}^\infty X_h}$ and that $\mu(X_h) = \nu(X_h) < \infty$ for all $h$. Then $\mu, \nu$ are equal
on the $\sigma$-algebra generated by $\mathcal{G}$. The authors prove the theorem for the case where $\mu,\nu$ are positive finite measures and $\mu(X) = \nu(X)$ and say that the general case follows easily. 
However, this is not at all straight forward for me. I have tried to prove this
but cannot reach a valid proof. Here is my attempt at a proof: Let $G_h = \{g \cap X_h | g \in G\}$. Then clearly from using the finite case
of the theorem, we have that $\mu,\nu$ coincide on every $\sigma (G_h)$. My 
problem now is to show that this implies that they agree on $\sigma(G)$. All my
attempts in this direction have been futile. I feel that the solution should be relatively easy (as the authors themselves point out). Any help in the proof is greatly appreciated. Thanks,
Phanindra",['measure-theory']
50791,"If $x^m=e$ has at most $m$ solutions for any $m\in \mathbb{N}$, then $G$ is cyclic","Fraleigh(7th ed) Sec10, Ex47. Let $G$ be a finite group. Show that if for each positive integer $m$ the number of solutions $x$ of the equation $x^m=e$ in $G$ is at most $m$, then $G$ is cyclic. I tried it a few hours but I couldn't solve it. So I read the solution. But I narrowly understood the solution, and it's still unclear to me. How can I solve it?","['finite-groups', 'group-theory', 'abstract-algebra']"
50799,How difficult is to find a small clear round spot on a large sheet contaminated by random dirty dots,"The problem arose after a discussion why larger digital camera photo sensors is much more expensive than little bit smaller ones, and the reason was given that it's due to difficulty of finding a larger area spot on a big CCD or CMOS panel. Consider a large clear white sheet (of a given area $S$, and we may consider it of any convenient non-degenerate shape, such as square or circle) with some black dirt dots on it. The average density of the dirt dots is uniform and known to be $p$ dots per unit area. Somebody wants to find a clear round spot of radius $r$ on it. Question 1: how difficult is it to find such a spot (and the term ""difficult"" maybe is defined as ""the probability of a random disc being clear?""). How many such non-overlapping spots there are on the sheet on average? Question 2: how much more difficult is it to find a spot of radius $k\cdot r$ with $k \gt 1$ than a spot of a radius $r$? When $S\gg s=\pi r^2$, this looks easy, but when $S$ is comparable to $s$, the result is not so obvious.",['probability']
50809,Bundle orientability vs manifold orientability,"Given a vector bundle, I am a bit hazy about the difference between the notions of its orientability as a bundle and as a manifold.
I think I know that the following are true, A tangent bundle of a manifold is orientable if and only if the manifold is orientable. The tangent bundle of any manifold thought of as a new manifold is in some sense always ""trivially"" orientable. And also I vaguely remember there being something special about the orientability of the tangent bundle of the tangent bundle of the manifold. I am not aware if there is any generic relationship between the orientability of a vector bundle and its base manifold. It would be helpful if someone can help me tie up these loose ends and help me see the full picture.","['differential-topology', 'differential-forms', 'differential-geometry']"
50834,General solution of $y''+y'=0$,"Plugging the equation $y''+y'=0$ into Wolfram Alpha yields the following solution : $$y(x) = c_2-c_1 e^{-x}.$$ This has me stumped because my textbook states that in the case of $b^2-4ac > 0$. ""If root 1 $r_1$ and root 2 $r_2$ are two real and unequal roots to the auxiliary equation $ar^2 + br + c =0$, then $$y = c_1e^{r_1x} + c_2e^{r_2x}$$ is the general solution."" Based of my book, the solution to the above problem would be $$y=c_2+c_1 e^{-x}$$ so which one's right?",['ordinary-differential-equations']
50855,What does {2x|P(x)} mean?,I am learning sets. I have seen preposition like {2x|P(x)} . I wanted to ask what does 2x mean here? I would be thankful if someone could make it simple to understand. Thanks.,['elementary-set-theory']
50860,"Is an Exterior Product the ""Opposite"" of an Inner Product","One is represented by a dot product, the other by a cross product. The ""inner product collapses two co-ordinate vectors into a scalar, the exterior product seems to expand them in a multilinear (manifold)? The inner product seldom has ""cancellation,"" the exterior product has a lot of cancellation (between the same differential forms). Are they just two opposite sides of the same coin? If not, why do they seem to be so ""parallel""?",['differential-geometry']
50887,Is it possible to solve a separable equation a different way and still arrive at the same answer?,"I have the following equation $$(xy^2 + x)dx + (yx^2 + y)dy=0$$ and I am told it is separable, but not knowing how that is, I went ahead and solved it using the Exact method. Let $M = xy^2 + x $ and $N = yx^2 + y$ $$My = 2xy \text{ and } Nx = 2xy $$ $$ \int M.dx ==> \int xy^2 + x = x^2y^2 + (x^2)/2 + g(y)$$
$$ \text{Partial of } (x^2y^2 + (x^2)/2 + g(y)) => xy^2 + g(y)'$$
$$g(y)' = y$$
$$g(y) = y^2/2$$
the general solution then is 
$$C = x^2y^2/2 + x^2/2 + y^2/2$$ Is this solution the same I would get if I had taken the Separate Equations route?","['ordinary-differential-equations', 'integration']"
50908,Existence of continuous function with a compact support and nontrivial on given compact set in $\sigma$-compact space,"The following fact is trivial to see: Let $X$ be a separable and locally compact metric space, then for each compact set $K\subset X$ there is a continuous function with compact support and such that $f|K=1$. Indeed, $X=\bigcup \limits_{n=1}^{\infty} U_n$, where $\{U_n\}$ is a increasing sequence of open and precompact subset of $X$ (from the Lindelöf theorem). So there is an $m\in \mathbb{N}$, such that $K\subset U_m$. Now, applying Urysohn's theorem to the sets $K$ and $X \setminus U$, we find the suitable function (with support contained in $\operatorname{cl} U_m$, with is compact). If something like that (or similar) would be true, when $X$ was a $\sigma$-compact Polish space?","['general-topology', 'metric-spaces', 'real-analysis']"
50916,particular solution of $y''+y'=xe^{-x}$,"I'm using the method of undetermined coefficients to find a particular solution of: $$y''+y'=xe^{-x}$$ Ostensibly, it seems that $y_p$ should take the form of $(Ax + B)e^{-x}$ At least that's the form that I think I've been taught. Problem is that it just doesn't work out for me. I get a value for A, but not for B... Am I choosing an incorrect yp form?",['ordinary-differential-equations']
50919,Calculate the sum of the infinite series $\sum_{n=0}^{\infty} \frac{n}{4^n}$,A previous problem had us solving $\sum_{n=0}^{\infty} \frac{1}{4^n}$ which I calculated to be $\frac{4}{3}$ using a bit of mathematical manipulation. Wonderful. Thank you for all the prompt responses.  Could anyone suggest an alternate technique that does not involve differentiation?,['sequences-and-series']
50927,The derivative of a product of more than two functions,"I'm trying to generalize the product rule to more than the product of two functions using the fact that I can treat the product of $n$-1 functions as a single one. Here is an example of what I mean: $[f(x)g(x)h(x)]' = [f(x)p(x)]'$ where $p(x) = g(x)h(x)$ $[f(x)p(x)]' = f'(x)p(x) + f(x)p'(x) = f'(x)p(x) + f(x)[g(x)h(x)]'$ $f'(x)p(x) + f(x)[g(x)h(x)]' = f'(x)g(x)h(x) + f(x)[g'(x)h(x) + g(x)h'(x)]'$ which equals $f'(x)g(x)h(x) + f(x)g'(x)h(x) + f(x)g(x)h'(x)$ I generalized this as follows: $$\Big[\prod_{i=1}^{n}f_i(x)\Big]'= f_1'(x)g_1(x) + f_1(x)g'_1(x)$$ where $g_m(x)=$$\prod_{i=1}^{n-m}f_i(x)$, and $g'_{m-1}=[f_m(x)g_m(x)]'=f'_m(x)g_m(x) + f_m(x)g'_m(x)$. Now, I do realize that this is a generalization, and there is  really nothing to prove, but say I wanted to prove that $$\Big[\prod_{i=1}^{n}f_i(x)\Big]'=\sum_{i=1}^{n}f'_i(x)h_i(x)$$ where $h_i(x)=\frac{1}{f_i(x)}\prod_{j=1}^nf_j(x)$, how would I go about doing this (using the generalization above)? I apologize if my notation is hard to understand. Thank you.","['calculus', 'products']"
50937,understanding of the tensor product $V^*\otimes W^*$,"In S.S.Chern's Lectures on Differential Geometry , I don't understand the following text in Chapter 2, which introduces the tensor product: The tensor product $V^*\otimes W^*$ of the vector spaces $V^*$ and $W^*$ refers to the vector space generated by all elements of the form $v^*\otimes w^*$, $v^*\in V^*$, $w^*\in W^*$. It is a subspace of ${\mathcal L}(V,W;{\mathbb F})$. We need to point out that any element in $V^*\otimes W^*$ is a finite linear combination of elements of the form 
  $v^*\otimes  w^*$, but generally cannot be written as a single term $v^*\otimes w^*$ (the reader should construct examples). Here are my questions: What does the first sentence mean? Does it mean
$$V^*\otimes W^*:=\operatorname{span}\{v^*\otimes w^*|v^*\in V^*, w^*\in W^*\} $$
or
$$V^*\otimes W^*:=\{v^*\otimes w^*|v^*\in V^*, w^*\in W^*\} ?$$ In the context, it is only defined that
$$v^*\otimes w^*(v,w)=v^*(v)\cdot w^*(w).$$
What's the ""finite linear combination of elements of the form $v^* \otimes w^* $"" supposed to be defined? And what's the example ""the reader needs to construct""?","['tensor-products', 'multilinear-algebra', 'differential-geometry']"
50944,The inverse variance is proportional to the sample size?,"Interested in meta-analysis I found the lecture notes http://www.uazuay.edu.ec/cibse/lecture_notes.pdf I understand that it makes sense in a meta-analysis to assign different weights
to different studies. Personally I find it natural to weigh by the sample sizes
of the particular studies. However, in the fixed effect model the inverse of the
variances is used. As an explanation on page 17 I read the sentence 
""The inverse variance is roughly proportional to sample size,
but has finer distinctions,( and serves to minimize the variance of the combined effect.)"" I do not understand that and have problems with the first claim. Suppose $X_1,..,X_n$ are random variables iid. Then 
$\bar{X}=\frac{1}{n} \sum_n X_i$ has variance $\frac{Var(X)}{n}$. This is clear.
So one could say that on the level of random variables the inverse variance of the arithmetic mean is proportional to the sample size. However, on the level of samples, we always compute the sample means and the sample
variances, and the sample variance is an estimator for the variance of an individual
measurement and not the variance of the (sample) arithmetic mean. Can somebody clear up my confusion",['statistics']
50947,Product of Combinations is Probability?,"For atomic orbitals: E2 orb: $\binom{N-n_1}{n_2}$ E2 orb: $\binom{N-n_1-n_2}{n_3}$ E2 orb: $\binom{N-n_1-n_2-n_3}{n_4}$ ... En orb: $\binom{n_i}{n_i}$ now probability function is: $P= N! \prod^{n}_{n=1}\frac{1}{n_{i}!}$ Why? In general? [Update] Every combination is greater than 1. So their product is greater than 1. How on earth can such multiplication lead to a probability function? Is the probability function scaled back to range $[0,1]$?",['probability']
50953,Volume of Region in 5D Space,"I need to find the volume of the region defined by
$$\begin{align*}
a^2+b^2+c^2+d^2&\leq1,\\ 
a^2+b^2+c^2+e^2&\leq1,\\ 
a^2+b^2+d^2+e^2&\leq1,\\ 
a^2+c^2+d^2+e^2&\leq1 &\text{ and }\\
b^2+c^2+d^2+e^2&\leq1.
\end{align*}$$
I don't necessarily need a full solution but any starting points would be very useful.","['geometry', 'multivariable-calculus']"
50955,Something connected with Ulam's tightness theorem,"Well known theorem of Ulam says, that each probability measure $\mu$ defined on Borel subsets of polish space $X$ satisfies the following condition: for each $\epsilon>0$ there is a compact subset $K$ of $X$, such that $\mu(K)>1-\epsilon$. I wonder there are any reasonable condition on measure $\mu$ which would guarantee that  for each $\epsilon>0$ there is an open set subset $U$ of $X$, such that $\mbox{cl}\,U$ is compact set and $\mu(\mbox{cl}\,U)>1-\epsilon$. Any idea? It would be very helpful for me.","['probability-theory', 'measure-theory']"
50967,An eigenvalue problem for positive semidefinite matrix,"Let  $  \begin{bmatrix}
A& B   \\ B^*  &C
\end{bmatrix}$ be positive semidefinite, $A,C$ are of size $n\times n$. Is it true that $$\quad \sum\limits_{i=1}^n\lambda_i\begin{bmatrix}
A& B   \\ B^*  &C
\end{bmatrix}\le \sum\limits_{i=1}^n\left(\lambda_i(A)+\lambda_i(C)\right)\quad? $$ Here, $\lambda_i(\cdot)$ means the $i$th largest eigenvalue of $\cdot\quad$","['matrices', 'linear-algebra']"
50973,Isomorphism of modules + tensor product,"Is it true that:
$$M{\otimes}_{A}(A/I) \cong M/IM$$
and
$$IM \cong I {\otimes}_AM$$
where $A$ is a commutative ring, $M$ an $A$-module, and $I \subset A$ an ideal.","['modules', 'tensor-products', 'ideals', 'abstract-algebra']"
50974,Formula for integration bounds of recursively defined polynomial sequence,"We can recursively define a sequence of polynomials by $$P_0(x) := 1$$ and then with the definite integral $$P_n(x) := \int_{c_n}^x P_{n-1}(t) ~\mathrm dt$$ where the $c_n$ are to be chosen so that $$\int_0^1 P_n(t)~\mathrm dt = 0$$ so for $n = 1$ we have $P_1(x) = x - c_1$ so $c_1 = 1/2$. However already for $n=2$ it becomes difficult since $\int_{c_2}^x (t-1/2)~\mathrm dt =  t^2/2 - t/2- c_2^2/2 + c_2/2$ and to have $\int_0^1 P_2(t)~\mathrm dt = 0$ there are two solutions for $c_2$ namely $$c_2 = 1/2 \pm \sqrt 3/6.$$ So $P_2(x) = x^2/2 - x/2 + 1/12$. For $n=3$ we get $c_3 = 0$ and $P_3(x) = x^3/6 - x^2/4 + x/12$ however for $n=4$ we have $c_4=1/2 - 1/2\sqrt (1 - 2/15\sqrt 30)$ (and 3 other solutions) for $P_4(x)=x^4/24 - x^3/12 + x^2/24 - 1/720$. For $n=5$ again $c_5=0$ (however there are another 4 solutions). Is there some general Formula for the $c_n$ so we could have a shortcut for calculating the coefficients of $P_n(x)$? Also with an eye to fractional calculus it would be nice to know if for the Riemann-Liouville integral $$\frac1{\Gamma(\alpha)} \int_c^x P(t) (x-t)^{\alpha-1}~\mathrm dt$$
and $$\int_0^1 P(t)~\mathrm dt = 0$$ if there even is a solution for this - let alone if it would fit in the previously defined sequence. The dream result would of course be not only to have a formula for $c_n$ but a function $c(n)$ that continuously defines such a polynomial sequence. But that seems very remote since even in simple cases with for example $\alpha = 1/2$ and $c>0$ the integral produces complex values...","['complex-analysis', 'ordinary-differential-equations', 'calculus', 'real-analysis']"
50977,Conformal map from the punctured unit disc onto the unit disc?,"I remember seeing this statement, I don't remember where (maybe in Lang's Complex). Is this true or do I have a faulty memory. It was always somewhere in the back of my mind but I never believed it. Is it true that there is a conformal map from the punctured unit disc onto the unit disc?",['complex-analysis']
50989,Factoring $X^{16}+X$ over $\mathbb{F}_2$,"I just asked wolframalpha to factor $X^{16}+X$ over $\mathbb{F}_2$. The normal factorization is 
$$
X(X+1)(X^2-X+1)(X^4-X^3+X^2-X+1)(X^8+X^7-X^5-X^4-X^3+X+1)
$$
and over $GF(2)$ it is
$$
X(X+1)(X^2+X+1)(X^4+X+1)(X^4+X^3+1)(X^4+X^3+X^2+X+1).
$$
Does the second form follow from the first, or is there a different way to factor over $\mathbb{F}_2$? I noticed that simply replacing the $-$ signs with $+$ signs in the first factorization doesn't yield the second one.","['factoring', 'finite-fields', 'abstract-algebra', 'polynomials']"
50992,Sequence with all rationals as limit points,"Is it possible to build the sequence that has all rationals as limit points? A limit point of a sequence $(x_n)$ is a point $x$ such that each neighborhood $(x-\varepsilon,x+\varepsilon)$ contains $x_n$ for infinitely many $n$'s. Equivalently, $x$ is a limit point if and only there is a subsequence $(x_{n_k})$ which converges to $x$. Thank you.","['sequences-and-series', 'examples-counterexamples', 'real-analysis']"
50998,least number of equal fibers,"Here is the generalization of a nice problem. Let $0\not=p,q \in \mathbb{C}[z_1,...,z_n], \; n\geq 1.$ Let $w_1,...,w_k$ be distinct complex numbers s.t. $p,q$ have the same fibers on them, that is, $p^{-1}(w_i)=q^{-1}(w_i)$ for every $1 \leq i \leq k.$ Find the least (if exists) $k \in \mathbb{N}$ s.t. the condition implies $p=q.$ As an example, for $n=1$ it can be verified that $k=2$ is the least number.","['riemann-surfaces', 'algebraic-geometry', 'complex-analysis']"
50999,Fundamental theorem of calculus and pullback bundles,"Let $X$ be a manifold and $\pi:E\rightarrow X$ a vector bundle
over $X$ equipped with a metric $\left\langle \cdot,\cdot\right\rangle $. Let $f:[0,1]\rightarrow M$ be a smooth map, and consider the pullback
bundle $f^{*}E\rightarrow[0,1]$. This is the vector bundle whose
fiber over $t\in[0,1]$ is $$
(f^{*}E)_{t}=\left\{ (t,v)\,:\, v\in E_{f(t)}\right\} .
$$ Let $\phi:f^{*}E\rightarrow E$ denote the map $\phi(t,v)=v$. Let $\nabla$ denote a connection on $E$, and let $D:=f^{*}\nabla$
denote the induced connection on $f^{*}E$. $D$ is defined as follows:
suppose $F\in\Gamma(f^{*}E)$ is a section of $f^{*}E$, so that $\phi(F(t))\in E_{f(t)}$
for all $t\in[0,1]$. Fix $s\in[0,1]$, and let $v$ denote any vector
field on $X$ with $v(f(s))=\phi(F(s))$. Then by definition $$
(D_{\partial_{t}}F)(s)=\left(s,(\nabla_{\dot{f}(s)}v)(f(s))\right)\mbox{ as elements of }(f^{*}E)_{s},
$$ the points being that the right-hand side of the above expression is independent of the choice of $v$. Assume now that $f(0)=f(1)$. Then if $F\in\Gamma(f^{*}E)$ then $\phi(F(1))$
and $\phi(F(0))$ both lie in the same vector space $E_{f(0)}$. My question
is: does the following generalization of the fundamental theorem of
calculus always hold? $$
|\phi(F(1))-\phi(F(0))| \le \int_{0}^{1}|\phi(D_{\partial_{t}}F)(t)|dt.
$$",['differential-geometry']
