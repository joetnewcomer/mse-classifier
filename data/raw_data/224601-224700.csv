question_id,title,body,tags
4623311,Two different ways of solving a combinatorics problem.,"I am working on the following problem: $ \bullet $ How many possibilities exist to assign $n$ people to $k$ rooms, when the rooms are different and there might be empty rooms. My attempt (version $1$ ). The problem becomes quite easy when we look at it in the following way: we want to assign a room to each one of the $n$ people and each room might be assigned to $0,\dots,n$ people. With this in mind, denoting the set of people by $\{ P_1, \dots, P_n \}$ , we have $k$ possibilites for each $P_i$ and then the answer comes given by $k^n.$ My attempt (verson $2$ - using Stirling numbers). Since the rooms are different, we know that the number $$ j! \, {n \brace j }$$ gives us the number of ways to assign $n$ people into $j$ different rooms. Furthermore, if some rooms might be empty, the number $$ \binom{k}{j}j! \, {n \brace j}$$ gives us the number of ways to assign $n$ people into $j$ different rooms, which are picked from $k$ total rooms (this means that $k-j$ rooms are empty, here we just picked the rooms that must be non empty). Then, for example, the number of ways to assing $n$ people into $1$ room (of the total $k$ rooms) is given by $$ \binom{k}{1} 1! \, {n \brace 1} $$ and so on. Then, this leads me to the conclusion that the answer we are looking for is $$ \sum_{j=1}^k \binom{k}{j}j! \, {n \brace j}.  $$ Concerns. With further search, I found out the identity $$ k^n = \sum_{j=0}^\color{red}{n} \binom{k}{j}j! \, {n \brace j} = \sum_{j=1}^\color{red}{n} \binom{k}{j}j! \, {n \brace j}$$ holds for every $k \in \mathbb R.$ BUT my answer isn't exactly this. With the two resolutions above, I have shown that $$ k^n = \sum_{j=1}^\color{red}{k} \binom{k}{j}j! \, {n \brace j} $$ And I really can't see where I am messing this up $(?)$ . Thanks for any help in advance.","['solution-verification', 'combinatorics', 'stirling-numbers', 'discrete-mathematics']"
4623328,Area enclosed by each infinitely repeating pattern in the graph of $\sin (x) + \sin (y) = 1$,"16 year old college math student beginner at calculus. Radians not degrees for the trigonometric functions. This area is exactly the same as the area enclosed by each infinitely repeating pattern in the graph $\cos (x) + \cos (y) = 1$ Graph of $\sin (x) + \sin (y) = 1$ : My working: This area is exactly the same as the area enclosed by each infinitely repeating pattern in the graph $\cos (x) + \cos (y) = 1$ For cosine, the pattern is split symmetrically across all four quadrants of the graph around the origin so integration may be possible to find half of the desired area Solve for y: Subtracting $\cos (x)$ from both sides of the equation: $\cos (y) = 1 - \cos (x)$ Taking inverse cosine of both sides of the equation: $y = \arccos (1 - \cos (x))$ Since the range of the inverse cosine function is $-1 ≤ \arccos (x) ≤ 1$ , if the value of $\cos (x) < 0$ , the value in within the parentheses will be outside of the inverse cosine range and not be valid, so the new function has a minimum of $\arccos(1) = 0$ , likewise, if the maximum y value of $\cos (x)$ would be 1, therefore the other extreme point, the maximum, of the new function would be $\arccos (0) = \frac{\pi}{2}\cdot$ Because any resulting definite integral of this function between any two valid points on the x-axis will be positive, we don't have to consider translating the graph in order to find a more appropriate area-under-the-curve value. We know that, from the graph, provided, that the curve of the function between two valid points will be an arch-shape, similar to the original wave function graphs. So we know that the desired area will be located between two 'paired' roots of the new function, which we can use as parameters for the indefinite integral later. Find two subsequent roots for the function: Set $y = 0$ , so the equation to solve is $\arccos (1 - \cos (x)) = 0$ Take cosine of both sides of the equation: $1 - \cos (x) = \cos (0) = 1$ Replace the subtrahend with the value on the right hand side of the equation: $\cos (x) = 1 - 1 = 0$ Take inverse cosine of both sides: $x = \arccos (0) = \frac{π}{2}$ Since cosine is an even function, all positive roots can be turned negative and still be valid: $\cos (-\frac{π}{2}) = 0$ confirms this in this context. Since the y-intercept is a maximum y-value of the function, we can assume that the shape of the function's curve on both sides of the y-axis are symmetrical and that we have the correct parameters, the diameter of the shape in the graph also seems reasonable and correlates with this conclusion. Integrate this definite integral: $\displaystyle\int_{-\frac{π}{2}}^{\frac{π}{2}} \! \arccos(1 - \cos (x)) \, \mathrm{d}x.$ Take the antiderivative of the function first: $\int \arccos (1 - \cos (x)) dx$ Use u-substitution (only way I know how to integrate) for the contents inside the parentheses: $1 - \cos (x) = u$ Solve for x: Replace the subtrahend with the value on the right hand side of the equation: $\cos (x) = 1 - u$ Take inverse cosine of both sides: $x = \arccos (1 - u)$ Find dx: Differentiate both sides of the equation: $du = \frac{d}{dx} (1 - \cos (x)) = \frac{d}{dx} (1) + \frac{d}{dx} (-\cos(x)) = 0 + \sin (x) = \sin(x)$ --> $du = \sin (x)dx$ Divide both sides by $\sin (x)$ to find $dx$ : $dx = \frac{du}{\sin (x)}$ Substitute the u value into the remaining x from the $dx$ expression: $dx = \frac{du}{\sin (\arccos (1 - u))}$ Simplify the new u-substituted antiderivative expression: $\frac{\int \arccos (u) du}{\sin (\arccos (1-u))}$ Integrate the antiderivative in the numerator: $\frac{u\arccos (u) + \sqrt{1 - u^2}}{\sin (\arccos (1-u))}$ Resubstitute the x-values back into this new expression: $\frac{(1 - \cos (x))(\arccos (1 - \cos (x)) + \sqrt{1 - (1 - \cos (x))^2}}{\sin (\arccos (\cos (x))}\cdot$ Simplify this expression: \begin{eqnarray*}
&&\kern-2em\frac{\arccos (1 - \cos (x)) - \cos (x)\arccos (1 - \cos (x)) + \sqrt{1 - (1 - 2\cos (x) + \cos^{2}(x))}}{\sin ( \arccos (\cos (x))}\\
&=&\frac{\arccos (1 - \cos (x)) - \cos (x)\arccos (1 - \cos (x)) + \sqrt{2\cos (x) - \cos^{2}(x)}}{\sin ( \arccos (\cos (x))}\cdot
\end{eqnarray*} Use the definite integral formula $\displaystyle\int_a^b f(x)\;\mathrm{d}x = F(b) - F(a)$ : \begin{eqnarray*}
&&\frac{\arccos (1 - \cos (\frac{\pi}{2})) - \cos (\frac{\pi}{2})\arccos (1 - \cos (\frac{\pi}{2})) + \sqrt{2\cos (\frac{\pi}{2}) - \cos^{2}(\frac{\pi}{2})}}{\sin ( \arccos (\cos (\frac{\pi}{2}))}\\
&&- \frac{\arccos (1 - \cos (-\frac{\pi}{2})) - \cos (-\frac{\pi}{2})\arccos (1 - \cos (-\frac{\pi}{2})) + \sqrt{2\cos (-\frac{\pi}{2}) - \cos^{2}(-\frac{\pi}{2})}}{\sin ( \arccos (\cos (-\frac{\pi}{2}))}\cdot
\end{eqnarray*} This simplifies to $0 - 0 = 0$ , but it's not zero because I can see that the the area inside the pattern is a finite value, therefore half the area (the definite integral at hand) is a finite value. I don't know where I went wrong or what to do.","['integration', 'calculus', 'area', 'trigonometry']"
4623390,Local homeomorphism between two spaces:,"If $M$ and $N$ are Hausdorff spaces, $f:M→N$ is a local homeomorphism, $M$ is compact and non empty, and $N$ is connected, is $f$ a surjective map? I have an idea: If $f$ is a local homeomorphism then then $f$ is continuous and open map.  Since $M$ is both open and closed in itself $f(M)$ is open in $N$ . And as $M$ is compact,it's image under $f$ is compact, since $f$ is continuous. Now since $N$ is hausdorff every compact set is closed . Therefore $f(M) $ is both open and closed in $N$ . As $N$ is connected therefore only possibility of $f(M)=N$ . Hence $f$ is surjective map. Is my argument okay?","['general-topology', 'solution-verification']"
4623392,"Set notation: Difference between using spot/bullet, vertical bar '|' and or colon ':'","I have seen all three being used, sometimes two of them in the same set definition: mid: ' $\mid$ ' spot/bullet: ' $\bullet$ ' colon: ' $:$ ' Is there a difference between the three and when should each one of these used to specify a set? Thanks.","['elementary-set-theory', 'notation']"
4623458,Help me to get deeper understanding of Euler's proof of his Arithmetical Theorem,"With distinct numbers $a_1, a_2, \ldots, a_n$ , let's denote the products of the differences of each of these numbers with the each of the rest of them by the following principle: \begin{align}
    (a_1-a_2)(a_1-a_3)(a_1-a_4)\dots(a_1-a_n) &= \prod_{1\lt j\leqslant n}(a_1-a_j) = \mu_1, \\
    (a_2-a_1)(a_2-a_3)(a_2-a_4)\dots(a_2-a_n) &= \prod_{\substack{1\leqslant j\leqslant n \\ j\neq 2}}(a_2-a_j) = \mu_2, \\
    (a_3-a_1)(a_3-a_2)(a_3-a_4)\dots(a_3-a_n) &= \prod_{\substack{1\leqslant j\leqslant n \\ j\neq 3}}(a_3-a_j) = \mu_3, \\
    &\ \ \vdots \\
    (a_n-a_1)(a_n-a_2)(a_n-a_3)\dots(a_n-a_{n-1}) &= \prod_{1\leqslant j\lt n}(a_2-a_j) = \mu_n
\end{align} There is a theorem that the sum of reciprocals of that resulting sequence will always be equal to $0$ : \begin{align}
    \sum_{i=1}^n\dfrac{1}{\mu_i}=\dfrac{1}{\mu_1}+\dfrac{1}{\mu_2}+\ldots+\dfrac{1}{\mu_n}=0
\end{align} There also is a more general claim that for any integer $\xi$ , $0\leqslant\xi\lt n-1$ (in other words, $\xi$ is a nonnegative integer which is less than the number of factors in each $\mu$ , which happens to be exactly $n-1$ ), the result is just the same: \begin{align}
    \sum_{i=1}^n\dfrac{a_i^{\xi}}{\mu_i}=\dfrac{a_1^{\xi}}{\mu_1}+\dfrac{a_2^{\xi}}{\mu_2}+\ldots+\dfrac{a_n^{\xi}}{\mu_n}=0
\end{align} Leonhard Euler provides a very similar theorem in his Institutionum Calculi Integralis, Vol. II , §1169 (available in English here , see page 26 of this document), and the way he proves it seems quite confusing to me. I have a couple of questions related to this theorem: Does this theorem have a special name, or is it just a generic fact? In Corollary 1 (§1170), Euler claims that in case if $\xi\geqslant n-1$ the sum no longer vanishes (i.e. is no longer equal to zero because in this case $\xi$ is out of bounds mentioned in the theorem), but is equal to $\dfrac{1}{N}$ instead. But, due to $N$ not being defined directly in the mentioned corollary, how to understand what $N$ is equal to in $\dfrac{1}{N}$ ? It is known to me that $N=1$ in case $\xi = n-1$ , and that $\dfrac{1}{N}=\sum_{i=1}^na_i$ in case $\xi=n$ . What happens to $N$ in case $\xi\gt n$ ? After more thorough reading, I believe that $N$ might be the real polynomial function in the denominator of a proper rational function $\dfrac{1}{N}$ , according to  Euler's Introductione ad Analysin Infinitorum, Ch. II , §40 (available in English here , see page 41 of this document), which Euler himself seems to be referencing. Can you explain Euler's proof of the theorem in question rigorously, so it's clearly seen what goes from where? Edit: in my search for something to improve the question, I've found some useful references that can help. Case $\xi = 0$ had been reviewed in this question . The currently accepted answer makes use of Lagrange polynomial , while Euler seems to have a much more basic proof through partial fraction decomposition , which makes the essence of my question different. Case $\xi = n-1$ had been mentioned in this question . The currently accepted answer is based on a polynomial analysis, thus it's also different from what I'm looking for. Finally, $\xi = n$ is mentioned in my other question . I've got an elegant answer based on Vandermonde determinant and polynomial analysis, and it's not too much of what I am looking for.","['summation', 'proof-explanation', 'partial-fractions', 'products', 'sequences-and-series']"
4623465,Confusion on Ravi Vakil FOAG 15.2.E (b) (09.2022 version),"15.2.E. Suppose $X$ is a normal irreducible Noetherian scheme, and $\mathscr{L}$ is an invertible sheaf, and $s$ is an nonzero rational section of $\mathscr{L}$ . (a) Describe an isomorphism $\mathscr{O} ( \operatorname{div}s) \leftrightarrow \mathscr{L}$ . (You will use the normality hypothesis!) Hint: show that those open subsets for which $\mathscr{O} ( \operatorname{div}s) \cong \mathcal{O}_U$ form a base for the Zariski topology. For each such $U$ , deﬁne $\phi |_U : \mathscr{O} ( \operatorname{div}s )( U )\to \mathscr{L} ( U )$ sending a rational function $t$ (with zeros and poles “constrained by div $s$ ”) to $st$ . Show that $\phi |_U$ is an isomorphism (with the obvious inverse map, division by $s$ ).  Argue that this map induces an isomorphism of sheaves $\phi:\mathscr{O}( \operatorname{div} s ) \stackrel{\sim}{\to} \mathscr{L} $ . (b) Let $\sigma$ be the map from $K(X)$ to the rational sections of $\mathscr{L}$ , where $\sigma( t )$ is the rational section of $\mathscr{O}_X(D)$ deﬁned via (15.2.2.1) (as described in Remark 15.2.3). Show that the isomorphism of (a) can be chosen such that $\sigma( 1 ) = s$ . (Hint: the map in part (a) sends 1 to $s$ .) Here are the portions of the text mentioned above: 15.2.2 Important Definition. Assume now that $X$ is irreducible (purely to avoid making (15.2.2.1) look uglier - but feel free to relax this, see exercise 15.2.B). Assume also that $X$ is normal - this will be a standing assumption for the rest of this section. Let $D$ be a Weil divisor, define $\mathscr{O}_X(D)$ by $$\text{(15.2.2.1)}\qquad \Gamma(U,\mathscr{O}_X(D)):=\{t\in K(X)^{\times}:\operatorname{div}|_Ut+D|_Ut\geq 0\}\cup\{0\}.
$$ 15.2.3. Remark. It will be helpful to note that $ \mathscr{O}_X( D )$ comes along with a canonical “rational section” corresponding to $1 \in K ( X )^{\times}$ . (It is a rational section in the sense that it is a section over a dense open set, namely the complement of Supp $D$ .) I am confused with the map $\sigma$ in (b), since I don't find the explicit definition of it. And what is the $D$ in (b)? This problem is a big obstacle in my way. Thanks for everyone's help.",['algebraic-geometry']
4623472,"If every dense subspace of a normed space $E$ is of finite codimension, is $E$ finite-dimensional?","Let $E$ be a normed $\mathbb{K}$ -vector space ( $\mathbb{K} = \mathbb{R}$ or $\mathbb{C}$ ). I'm interested in the following question: If every dense subspace is of finite codimension in $E$ , then is $E$ finite-dimensional? As a reminder, the codimension of a subspace $D$ is the dimension of the quotient space $E/D$ . It can be shown that it's also the dimension of all algebraic complements of $D$ . This post provides an example of a space in which there exists a dense subspace of infinite codimension, and this post provides a way to construct dense subspaces of any given finite codimension, but I didn't find anything addressing my exact question, though there's probably something I missed here, or maybe it's on MathOverflow I don't know. If possible, if a non-Banach counterexample is found, it'd be nice to then reconsider the question for $E$ Banach, but let's not get ahead of ourselves. I also do not mind at all even just having partial answers, like ""true for $E$ separable Hilbert"", or ""true for $E$ of the form $\mathcal{C}(X,\mathbb{K})$ "", or whatever special cases may come up in your minds. I'm also pro-axiom of choice, in case it makes a difference for this question (like, if you need Hamel bases or that kind of things). Finally, I know that usually question-havers should show what they've tried, but, I'll be honest, I don't really know how to begin... However I thought it was somewhat interesting enough that I'd try asking here anyway, hopefully that is fine? This is not for any homework or project, just my own curiosity. (Feel free to re-tag or edit this post if needed)","['normed-spaces', 'linear-algebra', 'functional-analysis']"
4623477,"Relatively compact subsets in $\mathsf{Top}(X, Y)$ for the product topology (Exercise 2.19 in “Topology: A Categorical Approach”)","I am trying to make sense of Exercise 2.19 in Topology: A Categorical Approach by Tai-Danae Bradley, Tyler Bryson and John Terilla. Exercise 2.19. If $X$ is any set and $Y$ is Hausdorff, then a subset $\newcommand{\Top}{\mathsf{Top}} A ⊆ \Top(X, Y)$ has compact closure in the product topology if and only if for each $x ∈ A$ , the set $A_x = \{ f(x) \mid f ∈ A \}$ has compact closure in $Y$ . My question is essentially the following: Question. What is ‘the correct’ formulation/statement of this exercise? My problems with the given formulation are twofold: The notation $\Top(X, Y)$ stands for the set of continuous maps from $X$ to $Y$ .
This doesn’t make sense if $X$ is only a set.
So either we should require $X$ to be a topological space, or we need to use $\newcommand{\Map}{\mathsf{Map}} \Map(X, Y)$ instead of $\Top(X, Y)$ . When referring to “the product topology”, I assume that we identify $\Map(X, Y)$ with the product $\prod_{x ∈ X} Y$ , so that the projection onto the $x$ -th factor correspond to the evaluation at $x$ .
But it is not clear to me if we’re talking about the closure of $A$ in $\Top(X, Y)$ (endowed with the subspace topology) or its closure in $\Map(X, Y)$ . I have so far not found an interpretation of the exercise which is both correct and also uses that $A$ consists of continuous maps, instead of just arbitrary ones.
More explicitly, I have tried the following so far: If we let $X$ be a topological space and take the closure in $\Top(X, Y)$ , then the equivalence seems to be wrong.¹ If we let $X$ be a topological space and take the closure in $\Map(X, Y)$ , then the equivalence seems to be true;
but not only for subsets of $\Top(X, Y)$ , but for arbitrary subsets of $\Map(X, Y)$ . If we let $X$ be just a set and work with $\Map(X, Y)$ instead of $\Top(X, Y)$ , then the equivalence seems to be true.
This is basically equivalent to the previous point by regarding $X$ as a discrete space. While the last two formulations seem to be correct, they are not really about (continuous) functions $X \to Y$ anymore, but instead about arbitrary products $∏_α Y_α$ of Hausdorff spaces.
This seems to betray the way the exercise is presented, which may or may not be intended. ¹ If we take $Y$ to also be compact, then the second condition is true for every choice of $A$ , in particular for $A = \Top(X, Y)$ .
Thus, $\Top(X, Y)$ would be compact, and hence closed in $\Map(X, Y)$ (which is again Hausdorff).
This would then entail that pointwise limits of continuous maps from $X$ to $Y$ are again continuous, which has the usual counterexamples.","['general-topology', 'category-theory']"
4623510,"How to calculate the first variation of $\,y \rightarrow \int_{a}^{b} y(x)\,\mathrm dx\,$ with $\,V=C^{0}([a,b])$","So I was solving this problem and I wanted to know if I did everything right. The problem is: I have to calculate the first variation of the functional $F:V\rightarrow \mathbb{R}\quad$ defined by $\displaystyle\;y\rightarrow\!\int_{a}^{b} \!y(x)\,\mathrm dx\;$ with $\;V=C^{0}([a,b])\,.$ So the formula I used is this: $\;\delta F[y]=\dfrac{\partial F}{\partial y}\cdot v$ First, we need to calculate the functional derivative $\partial F/\partial y$ $\begin{align}
\dfrac{\partial F}{\partial y}&=\lim\limits_{\varepsilon\rightarrow 0} \dfrac{(F[y+\varepsilon v]-F[y])}{\varepsilon}\\
&=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}[y(x)+\varepsilon v(x)]\,\mathrm dx-\int_{a}^{b}y(x)\,\mathrm dx}{\varepsilon}\\
&=\lim\limits_{\varepsilon\rightarrow 0}\dfrac{\int_{a}^{b}\varepsilon v(x)\,\mathrm dx}{\varepsilon}=\int_{a}^{b}v(x)\,\mathrm dx\,.
\end{align}$ So at the end, with $\;\delta F[y]=\partial F/\partial y\cdot v\,,\,$ I have : $\displaystyle\delta F[y] =\int_{a}^{b}v(x)\,\mathrm dx\cdot v$ Am I right or am I missing something?","['integration', 'calculus-of-variations', 'optimization', 'limits', 'derivatives']"
4623525,Bijection Between $\mathbb{R}$ and $\mathbb{R}\setminus\{1\}$,"I was looking for a way to establish the equinumerosity of $\mathbb{R}$ and $\mathbb{R}\setminus\{1\}.$ This is what I came up with: Consider $f:\mathbb{R}\rightarrow\mathbb{R}\setminus\{1\}$ such that $f(x)=x,$ if $x\in\{0\}\cup\{-\frac{F_{n}}{F_{n+1}}|n\in\mathbb{N}\},$ and $f(x)=1+\frac{1}{x},$ otherwise. Here, $F_{i}$ is the $i^{th}$ Fibonacci number. Is this a valid bijection? If it is, then I'll be really happy with myself because this took me a while to come up with. Perhaps there's a more straightforward bijection I'm missing? Additionally, how would one go about constructing a bijection from $\mathbb{R}$ to $\mathbb{R}\setminus\{1,2,3,\ldots,k\},$ where $k\in\mathbb{N}?$ Note: Just so there is no ambiguity, $F_{0}=0,F_1={1},F_2={1},$ and $F_{n+2}=F_{n+1}+F_{n}\forall n\in\mathbb{N}.$","['elementary-set-theory', 'fibonacci-numbers', 'real-analysis']"
4623537,Form of Q in extended QR decomposition calculated with Householder reflections,"Let $A = QR$ be the extended QR decomposition of matrix $A \in \mathbb{R}^{m \times n}$ which is calculated by using $n$ Householder reflections. Prove by construction that there exist an upper triangular matrix $U \in \mathbb{R}^{n \times n}$ and such a matrix $W \in \mathbb{R}^{m \times n}$ that $Q = I - WUW^T$ . I've tried solving this problem by rearranging equations algebraically but this didn't give me any ideas. I assume that I have to use the fact that Q = $\prod_{i = 1}^n{(I - \frac{2}{w_i^Tw_i}w_iw_i^T)}$ for some vectors $w_i$ but I really don't know how use it. I know $w_i$ aren't just ""some"" vectors but I couldn't think of any property they have which would help me solve the problem. Any help is much appreciated.","['matrices', 'linear-algebra', 'numerical-linear-algebra', 'numerical-methods', 'matrix-decomposition']"
4623545,Topological closure and finitely generated groups and monoids,"I would like to ask your expertise on the following question: Let $G$ be the group of orthogonal matrices of order $n$ over the field $\mathbb{R}$ of reals, equipped with the topology induced by the Euclidean norm of matrices of $G$ . For any subset $S$ of $G$ , denote by $[S]$ the subgroup generated by $S$ , and by $\operatorname{cl}(S)$ the topological closure of $S$ . It is known that, if $S$ is a submonoid of $G$ , then $\operatorname{cl}([S])$ is a group and $\operatorname{cl}([S])= \operatorname{cl}(S)$ . My question is: If $\operatorname{cl}([S])$ is finitely generated (as a group), then may one conclude that $S$ is finitely generated (as a monoid)?","['monoid', 'topological-groups', 'group-theory', 'general-topology', 'lie-groups']"
4623547,Proving the existence and unicity of a stationary point,"Last question for today I promise.
I have the function $f(x) = x^3\ln(x) - x$ . I have to find its absolute or local max and min, if any.
So I studied the derivative but it's not a function I can properly solve, when imposing it equals to zero. So I thought about proving things in different analytical way. The fact is that I am stuck in certan parts. So furst of all I have $$f'(x) = 3x^2\ln(x) + x^2 - 1$$ which is continuous in its domain (the same of $f(x)$ ). I can see ""by hand"" that $x = 1$ is a solution of $f'(x) = 0$ . The fact is that I want indeed to prove that $x = 1$ is a solution, that is there exist a solution. Then I want to prove it's unique.
And then I would like to show that that solution is indeed $x = 1$ . So I thought about considering $g(x) = f'(x)$ and hence studying in terms of $g(x)$ , that is: $g'(x) = 5x + 6x\ln(x)$ , observing that $g'(x) > 0$ for $x > e^{-5/6}$ . This means $g(x)$ is increasing for $x = e^{-5/6}$ and consequently $f(x)$ is convex in that same interval.
Since there is a change in the monotonicity of $g(x) = f'(x)$ , then $f(x)$ has a minimum (due to how the monotonicity changes) and only one, due to having shown there is only one solution for $g'(x)$ , since the solution $x = 0$ is trashed because of Fermat's theorem). Then I showed there exists only one minimum. How can I show that this minima is indeed at $x = 1$ without just saying that ""it's obvious""?","['optimization', 'maxima-minima', 'solution-verification', 'derivatives']"
4623615,Finding angle with geometric approach,"I would like to solve this problem just with an elementary geometric approach. I already solved with trigonometry, e.g. using the Bretschneider formula, finding that the angle $ x = 15° $ . Any idea? I edited showing how I computed the $ x $ value using the Bretschneider formula for the area of the quadrilateral $ ABDE $ and equating to the sum of the triangles' area $ ABE + EFD + BDF $ $$\begin{cases}
BC = a \\
AB = a(1/\tan(2x) - 1) \\
BD = a\sqrt{2} \\
AE = AB/\cos(2x+\pi/6) = a(1/\tan(2x) -1)/\cos(2x+\pi/6) \\
ED = a/\cos(x)
\end{cases}
$$ So I solved this equation with Mathematica, and the only solution that fit the problem is $ x = \pi/12 $ $ a^2/2+(a^2(1/\tan(2x) - 1)(1+\tan(x)))/2 + a^2 \tan(x)/2 =
((a\sqrt{2})^2 + \\
(a(1/\tan(2x)- 1)/\cos(2x+\pi/6))^2 - (a/\cos(x))^2 -(a(1/\tan(2x) - 1))^2)/4 \tan(\pi/2 -2x) $ I guess there is a simpler trigonometric approach, but I just wanted to try with that formula.","['euclidean-geometry', 'triangles', 'geometry']"
4623620,Prove a relation between Theta functions and their derivatives to prove $\sum_{n=-\infty}^\infty \frac{1}{\cosh^2(\pi n )}$,"In an attempt of proving \begin{align*}
\sum_{n=-\infty}^\infty \frac{1}{\cosh^2(\pi n )}=\frac{1}{\pi}+\frac{\Gamma^4\left( \frac14\right)}{8 \pi^3} \tag{1}
     \end{align*} I found the following relationship between theta functions and their derivatives which could help me evaluate $(1)$ , but I don´t know how to prove it. \begin{align*}
\frac{\vartheta_3^\prime(e^{-s \pi})}{\vartheta_3(e^{-s \pi})}-\frac{\vartheta_2^\prime(e^{-s \pi})}{\vartheta_2(e^{-s \pi})}=\frac{\pi}{4}\vartheta_4^4(e^{-s \pi})
     \end{align*} Backgroud: To prove $(1)$ I started by the following equality: \begin{align*}
         \vartheta_2(q)=2 q^{1/4}\prod_{n=1}^\infty (1-q^{2n})(1+q^{2n})^2  \tag{2}
     \end{align*} Proof: Recall Jacobi´s triple product \begin{align*}
\sum_{n=-\infty}^{\infty} q^{n^{2}} z^{n}&=\prod_{n=1}^{\infty}\left(1+z q^{2 n-1}\right)\left(1+z^{-1} q^{2 n-1}\right)\left(1-q^{2 n}\right) 
\end{align*} The letting $z=q$ in the equation above we get \begin{align*}
     \sum_{n=-\infty}^{\infty} q^{n^{2}} q^{n}&=\sum_{n=-\infty}^{\infty} q^{n^{2}+n+\frac{1}{4}-\frac{1}{4}}\\
     &=q^{-1/4}\sum_{n=-\infty}^{\infty} q^{(n+1/2)^2}\\
    &= \prod_{n=1}^{\infty}\left(1+q q^{2 n-1}\right)\left(1+q^{-1} q^{2 n-1}\right)\left(1-q^{2 n}\right)\\
    &= \prod_{n=1}^{\infty}\left(1+ q^{2 n}\right)\left(1+ q^{2 n-2}\right)\left(1-q^{2 n}\right)\\
     &= \prod_{n=1}^{\infty}\left(1+ q^{2 n-2}\right)\prod_{n=1}^{\infty}\left(1+ q^{2 n}\right)\left(1-q^{2 n}\right)\\
     &= 2\left(1+ q^{2}\right)\left(1+ q^{4}\right)\left(1+ q^{6}\right)\cdots\prod_{n=1}^{\infty}\left(1+ q^{2 n}\right)\left(1-q^{2 n}\right)\\
      &=2 \prod_{n=1}^{\infty}\left(1+ q^{2 n}\right)\prod_{n=1}^{\infty}\left(1+ q^{2 n}\right)\left(1-q^{2 n}\right)\\
      &=2 \prod_{n=1}^\infty (1-q^{2n})(1+q^{2n})^2
\end{align*} Now rewrite $(2)$ as \begin{align*}
         \frac{\vartheta_2(q)}{2 q^{1/4}}&=\prod_{n=1}^\infty (1-q^{2n})(1+q^{2n})^2 \\
&=\prod_{n=1}^\infty (1-q^{4n})(1+q^{2n})\\
&=\prod_{n=1}^\infty (1-q^{4n})(1+q^{2n})\cdot \frac{(1-q^{2n})}{(1-q^{2n})}\\
&=\prod_{n=1}^\infty\frac{(1-q^{4n})^2}{(1-q^{2n})} \tag{3}
     \end{align*} Thus taking logarithms on both sides of $(3)$ gives \begin{align*}
         \ln \vartheta_2(q)-\ln 2 ~\frac14\ln q
&=2\sum_{n=1}^\infty\ln (1-q^{4n}) -\sum_{n=1}^\infty\ln(1-q^{2n})\\
&=\sum_{n=1}^\infty \sum_{k=1}^\infty \frac{q^{2nk}}{k}-2 \sum_{n=1}^\infty \sum_{k=1}^\infty \frac{q^{4nk}}{k} \\
&=\sum_{k=1}^\infty \sum_{n=0}^\infty \frac{q^{2k(n+1)}}{k}-2 \sum_{k=1}^\infty \sum_{n=0}^\infty \frac{q^{4k(n+1)}}{k} \\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{q^{2k}}{1-q^{2k}}-2\sum_{k=1}^\infty \frac{1}{k}\frac{q^{4k}}{1-q^{4k}}\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{q^{2k}}{1-q^{2k}}-2\sum_{k=1}^\infty \frac{1}{k}\frac{q^{4k}}{(1-q^{2k})(1+q^{2k})}\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{1}{1-q^{2k}}\left[q^{2k}-\frac{2q^{4k}}{1+q^{2k}} \right]\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{1}{1-q^{2k}}\left[\frac{q^{2k}+q^{4k}-2q^{4k}}{1+q^{2k}} \right]\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{1}{1-q^{2k}}\left[\frac{q^{2k}}{1+q^{2k}(1-q^{2k})} \right]\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{q^{2k}}{1+q^{2k}}\\
&=\sum_{k=1}^\infty \frac{1}{k}\frac{1}{1+q^{-2k}}
     \end{align*} For $q=e^{-s \pi}$ we obtain \begin{align*}
         \ln \vartheta_2\left(e^{-s \pi}\right)-\ln 2 +\frac{\pi s}{4}&=\sum_{n=1}^\infty \frac{1}{n}\frac{1}{e^{2 \pi s n}+1} \nonumber\\
         &=\frac{1}{2}\sum_{n=1}^\infty \frac{1-\tanh(n \pi s)}{n} \tag{4}
\end{align*} Now differentiate $(4)$ w.r.t. to $s$ to obtain \begin{align*}
              \sum_{n=1}^\infty \frac{1}{\cosh^2(n \pi s)} &=-\frac{1}{2}-\frac{2}{\pi}\frac{\vartheta_2^\prime\left(e^{-s \pi}\right)}{\vartheta_2\left(e^{-s \pi}\right)}
     \end{align*} By differentiating both sides of the functional equation  of $\vartheta_3$ w.r.t. $t$ \begin{align*}
   \sqrt{t} \sum_{n=-\infty}^\infty e^{-\pi t n^2}&=\sum_{n=-\infty}^\infty e^{-\frac{\pi n^2}{t} } 
\end{align*} I obtained \begin{align*}
    \frac{1}{2\sqrt{t}}\sum_{n=-\infty}^\infty e^{-\pi t n^2}-\sqrt{t}\sum_{n=-\infty}^\infty n^2 e^{-\pi t n^2}&= \frac{\pi}{t^2}\sum_{n=-\infty}^\infty n^2e^{-\frac{\pi n^2}{t} } 
\end{align*} Letting $t \to 1$ \begin{align*}
  \sum_{n=-\infty}^\infty n^2 e^{-\pi n^2 } &=\frac{1}{4 \pi} \sum_{n=-\infty}^\infty  e^{-\pi n^2 } \tag{5} 
\end{align*} Now recall that \begin{align*}
  \sum_{n=-\infty}^\infty  e^{-\pi n^2 } &=\frac{\sqrt[4]{\pi}}{\Gamma\left(\frac34 \right)} 
\end{align*} Plugging these result in $(5)$ we conclude that \begin{align*}
  \sum_{n=-\infty}^\infty n^2 e^{-\pi n^2 } &=\frac{1}{4\pi^{3/4}\Gamma\left(\frac34 \right)} 
\end{align*} But the same technique fails when trying to evaluate the derivative of $\vartheta_2$ . The functional equation \begin{align*}
   \sqrt{t} \sum_{n=-\infty}^\infty (-1)^n e^{-\pi t n^2}&=\sum_{n=-\infty}^\infty e^{-\frac{\pi }{t}\left(n+\frac12\right)^2 } 
\end{align*} when differentaited w.r.t. to $t$ ends up having a relation between $\vartheta_2$ and the derivatives of $\vartheta_2$ and $\vartheta_4$ , and I dont know how to evaluate neither derivatives. NOTE: Equation $(4)$ can be written in the alternative form \begin{align*}
         \frac{1}{2}\sum_{n=1}^\infty \frac{1-\tanh(n \pi s)}{n}&=\ln \vartheta_2\left(e^{-s \pi}\right)-\ln 2 +\frac{\pi s}{4} \\
&=\ln \frac{\vartheta_2\left(e^{-s \pi}\right)\vartheta_3\left(e^{-s \pi}\right)}{\vartheta_3\left(e^{-s \pi}\right)}-\ln 2 +\frac{\pi s}{4}\\
&=\ln \left(\sqrt{k}\sqrt{\frac{2 \operatorname{K}(k)}{\pi}}\right)-\ln 2 +\frac{\pi s}{4} \\
&=\frac12\ln \left(\frac{2 k \operatorname{K}(k)}{2\pi}\right) +\frac{\pi s}{4} 
\end{align*}","['elliptic-functions', 'sequences-and-series', 'modular-forms', 'theta-functions', 'elliptic-integrals']"
4623653,Solving a riddle with group theory,"In a videogame called ""Heroes of Hammerwatch"", there is an area in which players can try to solve a riddle to obtain loot. The riddle works like this: The area contains a 3 by 3 field of squares, some of them glowing and some of them not. Stepping on any square ""flips"" that square and its neighbors (meaning a glowing square changes to not-glowing and a not-glowing square changes to glowing) except for diagonal neighbors. Example: 1 2 3 4 5 6 7 8 9 Stepping on square 1 would flip the squares 2, 4 and 1 itself. Stepping on square 5 would flip 2, 6, 8, 4 and 5 itself. You start with an arbitrary pattern (I think) and have to step on the squares in such a way as to make all of them glow. I felt like this problem lends itself to group theory. This is what I got so far: -stepping on each square is an element of the group -the group operation is composition, so just stepping on one square after the other -the neutral element is the act of not stepping on any square -the inverse of every element is the element itself, because you just flip all the same squares again -the group is abelian, because the only thing that matters is the amount of times a square gets ""flipped"", not the order From here, I figured that, starting from an ""empty"" board (no glowing squares), every state of the board is the product of either pressing a square or not. Example: 1 0 0 0 0 0 0 0 1 (1 = glowing, 0 = not glowing)
This board is given by pressing square 1 and square 9, then pressing square 5 (the middle one), and not touching any other square. From here, I have been stuck though. I tried to find the combination of squares to press to obtain the board 1 1 1 1 1 1 1 1 1 when starting from the empty board, but didnt really progress. Only found the subgroup generated by {1, 3, 5, 7, 9}. I also had the idea of using linear algebra for this riddle instead, but I really want to solve it with group theory for practice instead. Do you guys have an idea?","['group-theory', 'puzzle']"
4623665,Center of Simple $C^*$-Algebras,"Given a simple $C^*$ -Algebra $A$ , consider its center $C = A \cap A'$ , i.e. the set of elements in $A$ commuting with every other element in $A$ . I have shown that if $A$ is unital, its center is trivial, i.e. $C = \mathbb{C} \cdot I$ . It should be the case that if $A$ is non-unital, we have $C = 0$ , but I don't know how to approach showing this. In the unital case I considered the spectrum of an arbitrary element $a \in C$ , which has to be nonempty, and showed that for some $\lambda \in \sigma(a)$ the set $\overline{(\lambda - c)A}$ is an ideal not containing $1$ , so it must be $\{0\}$ . However, I cannot do that here.","['c-star-algebras', 'operator-algebras', 'operator-theory', 'functional-analysis', 'ideals']"
4623680,"How can I show that $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X)$?","Let $X$ be a uniformly distributed random variable in $[0,1]$ and define $X_n:=\lfloor 2^nX\rfloor 2^{-n}$ for all $n$ . I want to show that $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)=\sigma(X)$ . My idea was the following: Proof : $\subseteq$ Let me remark that for all $n$ , $X_n$ is $\sigma(X)$ measurable since $X_n=f(X)$ where $f(x)=\lfloor 2^nx\rfloor 2^{-n}$ is measurable. Therefore $\sigma(X_n,X_{n+1},...)\subset \sigma(X)$ for all $n$ and hence $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\subseteq\sigma(X)$ . $\supseteq$ Here let me remark that $$\frac{2^nX}{2^n}\leq X_n\leq \frac{2^nX+1}{2^n}$$ where $\frac{2^nX}{2^n}\rightarrow X$ and $\frac{2^nX+1}{2^n}\rightarrow X$ , so in particular $X_n\rightarrow X$ . But since the limit exists we also know that $\limsup_{n\rightarrow \infty}X_n=X$ but we know that $\limsup_n$ is $\sigma(X_n,X_{n+1},...)$ measurable. But then this implies that $X$ is $\sigma(X_n,X_{n+1},...)$ measurable for all $n$ and hence $\bigcap_{n\geq 0} \sigma(X_n,X_{n+1}...)\supseteq\sigma(X)$ . Does this work?","['solution-verification', 'probability-theory', 'probability', 'stochastic-calculus']"
4623764,Analytic continuation of $f(z) = \sum_{n=1}^{\infty} \frac{1}{2^n (z-e^{2\pi i r_n})}$,"Let $\{r_k:k\in\mathbb{N}\}$ be a counting set of rational numbers in $[0,1]$ . Show that $$f(z) = \sum_{k=1}^{\infty} \frac{1}{2^k (z-e^{2\pi i r_k})}$$ is holomorphic on $U_1=\{z\in \mathbb{C} : |z| < 1\}$ and $U_2 = \{z\in \mathbb{C} : |z|>1\}$ . Are both functions analytic continuations from each other? I've already learned that if $(f_n)_{n\in \mathbb{N}}$ is a compactly convergent sequence of holomorphic functions and $f$ its limit, then $f$ is also holomorphic. Taking $$f_n(z) = \sum_{k=1}^{n} \frac{1}{2^k (z-e^{2\pi i r_k})}$$ it would make sense that it's enough to prove that this series is uniformly convergent on $U_1, U_2$ . For $k\in \mathbb{N}$ : $\frac{1}{2^k(z-e^{2\pi i r_k})}$ is holomorphic because we know that $\frac{1}{z-\exp(2\pi ir_k)}$ is holomorphic in $z\in \mathbb{C}\setminus \{\exp(2\pi i r_k)\}$ and $\exp(2\pi i r_k)$ is neither in $U_1$ nor in $U_2$ $\forall k$ . Let $z\in U_1$ and $R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\}$ : \begin{align*}
|f_n(z)-f(z)| = |\sum_{k=n}^{\infty} \frac{1}{2^k(z-e^{2\pi i r_k})}| \leq \sum_{k=n}^{\infty} \frac{1}{2^k|(z-e^{2\pi i r_k})|} \leq \sum_{k=n}^{\infty} \frac{1}{2^k R} \overset{n\rightarrow \infty}{\longrightarrow} 0
\end{align*} Analogous with $z\in U_2$ and $R = \inf\{|z-\exp(2\pi i r_k)|: k\in \mathbb{N}\}$ . With that I should have proven that $f(z)$ is holomorphic on $U_1$ and $U_2$ , right? Now to my main question: What does ""are both functions analytic continuations from each other"" mean in this context? $U_1\cap U_2 = \emptyset$ and they're essentially the same function, so yes?","['complex-analysis', 'holomorph', 'analytic-continuation', 'uniform-convergence']"
4623810,"Is there a ""rational-continuous"" function that satisfies $f(f(x)) = 2x$?","A function $f\colon\Bbb Q\to\Bbb Q$ is rational-continuous at a rational number $\alpha$ iff for any $\varepsilon > 0,$ there exists a corresponding $\delta$ based on $\alpha$ and $\varepsilon$ such that if $|\alpha-x| \leq \delta, |f(\alpha)-f(x)| \leq \varepsilon.$ It is easy to find a continuous function over the reals that satisfies $f(f(x)) = 2x.$ (Just take $f(x) = \sqrt{2}x.$ ) $\sqrt{2}$ is irrational, so we can't use this on the rationals, and I believe that any function continuous over the reals that gives a rational number when evaluated on a rational number cannot satisfy $f(f(x)) = 2x$ . However, as we all know , rational-continuity is weird. A rational-continuous function can ""jump around,"" unlike continuous functions over the reals. Is there a rational-continuous function (jumpy or not) that satisfies $f(f(x)) = 2x$ ?","['general-topology', 'analysis', 'rational-numbers']"
4623892,"What is the largest value of $e_{k}(x_1,\cdots,x_n)$ not obtainable over $(\mathbb{N}^+)^n$?","Let $k,n\in\mathbb{N}^+$ , $\vec{x}=\langle x_1,\cdots,x_n\rangle$ be an $n$ -tuple, and let $e_k(\vec{x})$ be the elementary symmetric polynomial of degree $k$ over $n$ variables (clearly with $k\le n$ ). For fixed $k$ , I'm wondering what can be said about the image of $e_k$ over $(\mathbb{N}^+)^n$ ; that is, the output over all $n$ -tuples of positive integers. Positivity is needed because it's trivial if we allow non-positive arguments: if any of the $x_i$ are zero, then it reduces to a lower degree, and if negative integers are allowed, then we are able to generate $\mathbb{Z}$ by Bezout's Theorem. Clearly $e_k(\vec{x})$ will have $\binom{n}{k}$ terms, so that is the minimum value with each $x_i=1$ . However, playing around in Mathematica suggests there is a threshold $M(k,n)$ such that $e_k(\vec{x})\ne M(k,n)$ for any $\vec{x}$ , but there for every $m>M(k,n)$ there is some $n$ -tuple $\vec{v}$ with $e_k(\vec{v})=m$ . In other words, $e_k$ is surjective except for finitely many values, and I'm wondering what the last of these values is. For example, with $k=2$ and $n=4$ , it seems (although I can't quite prove it) every value except $\{1,2,3,4,5,7,8,10,11,14,16,19,20,26,31,34,40,55\}$ can be reached, implying $M(2,4)=55$ . I tried to play around with several of the forms, in particular looking at reside classes for $e_2(x,a,b,1)$ for $b=1,2$ and values of $a$ .  This is reminiscent of finding the Frobenius number of a set, but perhaps using the symmetry of the polynomials it's possible to say something like, ""eventually you can eliminate all residue classes of a certain modulus, so it remains to check just the first few cases.""","['number-theory', 'additive-combinatorics', 'functions', 'symmetric-polynomials']"
4623989,"Suppose that $X_1$ and $X_2$ are independent. $Y_1=\max\{X_1, X_2\}$ and $Y=\min\{X_1, X_2\}$. Find the joint pdf of $(Y_1, Y_2)$. [duplicate]","This question already has an answer here : Joint distribution of $\min(X_1,\ldots,X_n)$ and $\max(X_1,\ldots,X_n)$. (1 answer) Closed last year . Suppose that $X_1$ and $X_2$ are independent with common pdf $f(x)$ and cdf $F(x)$ . $Y_1=\max\{X_1, X_2\}$ and $Y=\min\{X_1, X_2\}$ . Find the joint pdf of $(Y_1, Y_2)$ . My solution: I have no idea about the pdf of $(Y_1, Y_2)$ but
I can find the cdf and pdf of $Y_1$ , $Y_2$ : $$
P(Y_1\le y_1)=F(y_1)^2
$$ and then $f_{Y_1}(y_1)=2F(y_1)f(y_1)$ for $y_1\in A$ . Similarly, $$
P(Y_2\le y_2)=1-(1-F(y_2))^2
$$ and then $f_{Y_2}(y_2)=2(1-F(y_2))f(y_2)$ for $y_2\in A$ . How to go the next step? Update: $$
P(\max\{X_1, X_2\}\le x_1, \min\{X_1, X_2\}\le x_2)=1-P(\max\{X_1, X_2\}\le x_1, \min\{X_1, X_2\}> x_2)
$$ $$
=1-P(X_1\le x_1, X_2\le x_1, X_1>x_2, X_2> x_2)
$$ When $x_1>x_2$ , we get $$
=1-(F(x_1)-F(x_2))^2
$$","['statistics', 'probability']"
4624011,Taylor series of a matrix exponential,"I am looking to minimize the value of: $$g(t)=\mathrm{Tr}\left[\exp(X+tY)\right]$$ where both $X$ and $Y$ are symmetrical matrices with real coefficients. In general, $X$ and $Y$ do not commute so $\exp(X+tY)\neq\exp(X)\exp(tY)$ . We can further assume that $tY$ is small when compared to $X$ at the minimum. I assume that one of the the simplest approach is to attempt to write $g(t)$ as a Taylor expansion, like: $$g(t)=g_0+tg_1+\frac12t^2g_2+\dots$$ In this case, a good approximation for the minimum is easily obtained with $t\sim-\frac{g_1}{g_2}$ (Newton's method) and the process can be iterated until we meet a convergence criterion. The first two coefficients are quite trivial to find. For instance, $g_0=\mathrm{Tr}\left[\exp(X)\right]$ and $g_1=\mathrm{Tr}\left[Y\exp(X)\right]$ , as explained here . However, I spent some hours on this but I can't find an easy expression for $g_2$ yet. Is there a proper way to express $g_2$ so it can be computed numerically?","['matrix-exponential', 'matrices', 'matrix-calculus', 'linear-algebra', 'taylor-expansion']"
4624067,Why does the sum of eigenvalues equal to trace in terms of linear transformations?,"While studying eigenvectors, I was confronted with two statements: The product of the eigenvalues of some matrix $A$ is equal to the determinant of $A$ The trace of $A$ is equal to the sum of its eigenvalues The thing is I am trying to understand every topic I learn in terms of linear transformations. For example, the first statement made sense to me because since the determinant is how much area scaled after a linear transformation and we are stretching $2$ vectors by their eigenvalues therefore determinant equals the product of eigenvalues. But when I try to understand the second statement I can not relate the trace of a matrix and its eigenvalues because I can't also understand what the trace of a matrix tells us about a linear transformation. I would love it if you can give me an intuitive explanation or let me know if there are topics that I haven't studied that prevent me from understanding this.","['trace', 'linear-algebra', 'linear-transformations', 'eigenvalues-eigenvectors']"
4624101,How to illustrate both of distributions are the same,"Suppose $X_1,X_2,\ldots,X_{n};X^{'}_1,X^{'}_2,\ldots,X^{'}_{n}$ are all independent identically distributed (i.i.d.) random variables defined on the probability space $(\Omega,\mathcal{F},\mathbf{P})$ and having the common distribution $F$ . $\mathbb{I}_{(-\infty,t]}(x)$ be the indicator for
the interval $-\infty<x \le t$ . Let $F_{n}$ denote the emprical distribution function of $X_1,X_2,\ldots,X_{n},$ given by $$F_{n}(t)=F_{n}(t,\omega)=\frac{1}{n}\sum_{k=1}^{n}\mathbb{I}_{(-\infty,t]}(X_{k}(\omega))=\frac{1}{n}\sum_{k=1}^{n}\mathbb{I}_{ [ X_{k}\le t]}, \quad \forall t\in \mathbf{R}.$$ Similarly, $$F^{'}_{n}(t)=F^{'}_{n}(t,\omega)=\frac{1}{n}\sum_{k=1}^{n}\mathbb{I}_{(-\infty,t]}(X^{'}_{k}(\omega))=\frac{1}{n}\sum_{k=1}^{n}\mathbb{I}_{ [ X^{'}_{k}\le t]}, \quad \forall t\in \mathbf{R}.$$ Thus we have $$F_{n}(t)-F^{'}_{n}(t)=\frac{1}{n}\sum_{k=1}^{n}\left(\mathbb{I}_{ [ X_{k}\le t]}-\mathbb{I}_{ [ X^{'}_{k}\le t]}\right), \quad \forall t\in \mathbf{R}.$$ For any fixed $t\in \mathbf{R},$ $$\mathbb{I}_{ [ X_{k}\le t]}-\mathbb{I}_{ [ X^{'}_{k}\le t]}=\begin{cases}
 -1& \text{with probability}\quad F(t)(1-F(t)) ,\\
  0& \text{with probability}\quad 1-2F(t)(1-F(t)), \\
  1& \text{with probability}\quad F(t)(1-F(t)).
\end{cases}$$ also $\mathbb{I}_{ [ X^{'}_{k}\le t]}-\mathbb{I}_{ [ X_{k}\le t]}$ as well. I know that for any fixed $t\in\mathbf{R},$ the distribution of $$D_{n}(t,\omega)=\frac{1}{n}\left |\sum_{k=1}^{n}\left(\mathbb{I}_{[ X_{k}\le t]}-\mathbb{I}_{ [ X^{'}_{k}\le t]}\right)\right |$$ is the same as the distribution of $$D^{'}_{n}(t,\omega)=\frac{1}{n}\left |\sum_{k=1}^{n}\left (\mathbb{I}_{[ X^{'}_{k}\le t]}-\mathbb{I}_{ [ X_{k}\le t]}\right)\right|.$$ But I don't understand that the distribution of $\displaystyle\sup_{t\in\mathbf {R}}D_{n}(t,\omega)$ is the same as the distribution of $\displaystyle\sup_{t\in\mathbf {R}}D^{'}_{n}(t,\omega).$ How to illustrate this rigorously?","['statistics', 'probability-distributions', 'probability-theory']"
4624122,Counting of permutations (possibly) related to Euler number,"I got a question which is somehow related to Euler's number in combinatorics, but I do not know how to make a formal connection. Definition : Given numbers from 1 to $n$ , let $G(n,k)$ denote the number of permutations such that the permutation can be decomposed into $k$ groups, $G_1, \cdots, G_k$ , where in each $G_i$ , $min(G_i)$ is the rightmost one, and $min(G_1)<min(G_2)< \cdots < min(G_k)$ . For instance, for $n=3$ and $k=1$ , it could be $(3, 2,1)$ or $(2,3,1)$ for $n=3$ and $k=2$ , it could be $(3, 1,2)$ or $(2, 1, 3)$ or $(1, 3, 2)$ for $n=3$ and $k=3$ , it could be $(1, 2,3)$ As a result, $G(3,1)=2, G(3,2)=3$ and $G(3,3)=1$ I want to compute $\sum_{k=1}^n k\cdot G(n,k)$ for a given $n$ . My attempt : Interestingly enough, some small example suggests that this is actually $(\sum_{k=1}^n 1/k)\cdot n!$ . My question is whether there is an expression for $G(n,k)$ or $\sum_{k=1}^n k\cdot G(n,k)$ ?","['permutations', 'generating-functions', 'combinatorics', 'sequences-and-series']"
4624148,"How far from home, can my robot roam? It has constant step size, and turns by increasing amounts.","A robot's step size is always $1$ . Between steps it turns right, by increasing amounts: $\frac{1\pi}{2},\frac{2\pi}{3},\frac{3\pi}{4},\frac{4\pi}{5},...$ What is the robot's maximum distance from the origin? Here are the first 15 steps, starting in the lower-left corner (switching color every three steps, for clarity). Superimpose cartesian coordinates, with the first step from $(0,0)$ to $(0,1)$ . After the $n$ th step, the robot's coordinates are: $$x=\sum_{k=1}^n \sin{\left(\sum_{i=1}^k \pi\left(1-\frac{1}{i}\right)\right)}$$ $$y=\sum_{k=1}^n \cos{\left(\sum_{i=1}^k \pi\left(1-\frac{1}{i}\right)\right)}$$ Its distance from the origin is $$d(n)=\sqrt{x^2+y^2}$$ Here is the graph of $d(n)$ against $n$ . It looks like the maximum value of $d(n)$ is $d(2)=\sqrt{2}$ , which corresponds to the red point in the first diagram. But we cannot check the entire graph, because it goes forever. How can we know the maximum value of $d(n)$ ? EDIT It would be enough to prove the following: Lemma: The circle through three consecutive vertices of the robot's path, encloses all subsequent vertices. The circle through the first three vertices $(0,0),(0,1),(1,1)$ is $(x-\frac12)^2+(y-\frac12)^2=\frac12$ . All points on this circle are within $\sqrt{2}$ from the origin. So we would know that $\sqrt{2}$ is the maximum distance form the origin. But I don't know how to prove the lemma. EDIT2 @Intelligentipauca's comment pointed out that the lemma is false. For example, the circle through the points for $n=2,3,4$ does not enclose the point for $n=6$ .","['trigonometry', 'convergence-divergence', 'geometry', 'sequences-and-series']"
4624170,Blowing up of affine space,"I am learning blowing up from An Invitation to Algebraic geometry (Karen E. Smith). In the chapter 7 (103- page) written that blowing up of affine space $\mathbb{A}^n$ along point $p$ is not affine variety. How I can prove it?
I am trying to use the fact that : only global regular functions on a projective variety are constant functions. Could anyone help me to prove it?",['algebraic-geometry']
4624183,Find the value of $\ln\left(\frac{d^2y}{dx^2}\right)_{x=0}$,"If $xy=e-e^y$ then find the value of $\ln\left(\frac{d^2y}{dx^2}\right)_{x=0}$ . I found out that $y+\frac{dy}{dx}x=-e^y\frac{dy}{dx}$ and also $2\frac{dy}{dx}+\frac{d^2y}{dx^2}x=-e^y\frac{dy}{dx}\frac{dy}{dx}-\frac{d^2y}{dx^2}e^y$ ,or $\frac{d^2y}{dx^2}(x+e^y)=\frac{dy}{dx}\left(-e^y\frac{dy}{dx}-2\right).$ What to do next?","['calculus', 'derivatives']"
4624208,Comparing two sequence via their exponential generating function,"I am studying two sequence with their e.g.f.
The first one are the Bell numbers (sequence $A000110$ on OEIS) defined as follows. $B_0=1$ and $$
B_n =\sum_{k=0}^{n-1} {n-1\choose k} B_k
$$ They have a known e.g.f. (via OEIS) which is $$F_B(x)=e^{e^x-1}$$ The second one is the sequence A005046 on OEIS defined as follows $a_0=1$ and $$
a_n =\sum_{k=0}^{n-1} {2n-1\choose 2k} a_k
$$ Clearly $a_n\ge B_n$ for every $n$ . However on OEIS it says that the e.g.f. of $(a_n)_{n\ge0}$ is $$ F_a(x)=e^{\cosh(x) - 1} =e^{\frac{e^x+e^{-x}}{2}-1} $$ which is less than $F_B(x)$ for $x>0$ .","['number-comparison', 'oeis', 'discrete-mathematics', 'generating-functions', 'sequences-and-series']"
4624240,$\lim_{n \to \infty} (a_n - \sqrt{2n})$ if $ a_1 = 1$ and $a_{n+1} = a_n +\frac{1}{a_n}$.,"Put $ a_1 = 1$ and $a_{n+1} = a_n +\frac{1}{a_n}$ . 1) Main question : what can we say about the speed of convergence of $\frac{a_n}{\sqrt{n}}$ to its limit? 2) Additional question: is there a simple way to find the limit of $\frac{a_n}{\sqrt{n}}$ ? More formally: $\lim_{n \to \infty }\frac{a_n}{\sqrt{n}} = \sqrt{2}$ (if there's no mistake below), what can we say about $\frac{a_n}{\sqrt{n}} - \sqrt{2}$ , e.g. does $\lim_{n \to \infty} (a_n - \sqrt{2n})$ exist and can we find it in an explicit form? From direct computation in Python it follows (see above) that $$\text{Hypothesis:} \quad \lim_{n \to \infty} (a_n - \sqrt{2n}) = 0$$ is true. What is obvious? It's easy to see that $a_n$ is strictly increasing and tends to $\infty$ (Indeed, otherwise $a_n$ is bounded and hence $a_n \to a < \infty$ . Thus $a = \lim a_{n+1} = \lim (a_n +\frac{1}{a_n}) = a +\frac{1}{a} \Longrightarrow a = a+\frac{1}{a}$ ). What else do we know? If I'm not mistaken $\big[\sqrt{2n+\frac94} - \frac12\big] \le a_n \le \sqrt{2n}(1+o(1))$ for all $n > 2$ (the proof is above)
and hence $\lim_{n \to \infty} \frac{a_n}{\sqrt{n}} = \sqrt{2}$ . Some numerical experiments are presented in the end of the question. What are the difficulties? The upper bound for $a_n$ was obtained using the asymptotics of the sums $$\sum_{n=1}^{\infty} \frac{1}{[f(n)]}$$ where $f(n) = \sqrt{2n+\frac94} - \frac12$ and $[x]$ is an integer part of $[x]$ . There are good estimates (using intergal approximation) of $\sum_{n=1}^{\infty} \frac{1}{f(n)}$ for good functions $f$ , but it looks like there are no good estimates of $\sum_{n=1}^{\infty} \frac{1}{[f(n)]}$ . Proofs: Lemma1. $a_n \ge \sqrt{2n+\frac94} - \frac12$ if $n = \frac{k(k+1)}2-1 \ge 2$ . Proof. Lets prove that $a_{\frac{k(k+1)}2 - 1} \ge k$ for $k \ge 2$ .  Put $m_k = \frac{k(k+1)}2-1$ . We will use induction and suppose the opposite. We have: $a_{m_k} \ge k$ and $a_{m_{k+1}} < k+1$ . Hence $$ k \le a_{m_k} < a_{m_k + 1 } < a_{m_k + 2 } < \ldots < a_{m_{k+1}} < k+1,$$ $$ a_{m_k+i+1} - a_{m_k+i} = \frac{1}{a_{m_k+i}} > \frac{1}{k+1}$$ Summing l.h.s. or the last inequality we get $$ a_{m_{k+1}} - a_{m_{k}}  > \frac{1}{k+1} + \frac{1}{k+1} + \ldots +  \frac{1}{k+1} = \frac{m_{k+1}-m_k}{k+1}.$$ But $a_{m_{k+1}} - a_{m_{k}} < (k+1)- k = 1$ . Thus $$ 1 > \frac{m_{k+1}-m_k}{k+1} \Longrightarrow m_{k+1}-m_k < k+1 \Longrightarrow \frac{(k+1)(k+2)}2 - \frac{k(k+1)}2 < k+1 \Longrightarrow $$ $$ \Longrightarrow \frac{(k+2)}2 - \frac{k}2 < 1 \Longrightarrow 1 < 1.$$ We got a contradiction. Thus $a_{m_{k+1}} \ge k+1$ . Lemma is proved. Lemma2 : $a_n \ge \big[\sqrt{2n+\frac94} - \frac12\big]$ for all $n \ge 2$ . Proof. Put $u_n = \big[\sqrt{2n+\frac94} - \frac12\big]$ . We have $$ u_n \le \sqrt{2n+\frac94} - \frac12 \Longrightarrow  (u_n + \frac12)^2 \le 2n+\frac94 \Longrightarrow n \ge \frac{u_n(u_n+1)}{2}-1 $$ Thus $$ a_n \ge a_{\frac{u_n(u_n+1)}{2}-1} \ge u_n$$ by lemma 1. Lemma3: $\frac{a_n}{\sqrt{n}} \to \sqrt{2}$ as $n\to \infty$ . Proof. According to lemma 2 it's sufficient to show that $\overline{\lim}_{n \to \infty} \frac{a_n}{\sqrt{n}} \le  \sqrt{2}$ . Put $u_n =  \big[\sqrt{2n+\frac94} - \frac12\big]$ . We have $$ a_{j+1} - a_j = \frac1{a_j} \le \frac{1}{u_j}$$ It follows that $$ a_n = \sum_{j=3}^{n-1} (a_{j+1}-a_j) + a_2 \le 2 + \sum_{j=3}^{n-1} \frac{1}{u_j}$$ As $u_j \sim \frac{1}{\sqrt{2j}}$ hence $$\sum_{j=3}^{n-1} \frac{1}{u_j} \sim \sum_{j=1}^{n} \frac{1}{\sqrt{2j}} \sim \int_{1}^n \frac{dx}{\sqrt{2x}} \sim \sqrt{2n}.$$ Thus $\overline{\lim}_{n \to \infty} \frac{a_n}{\sqrt{n}} \le  \sqrt{2}$ . Numerical experiments. Let's compute $b_n = a_n - \sqrt{2n}$ for first values of $n$ in Python. We get $b_{1000}=0.035513542064180115, 
b_{2000}=0.0278533072346292, 
b_{3000}=0.02405133309876817, 
b_{4000}=0.021633453406735725, 
b_{5000}=0.019907583619357183, 
b_{6000}=0.018589261056305872, 
b_{7000}=0.017536096833907777, 
b_{8000}=0.01666749243514687, 
b_{9000}=0.015933785448964954, 
b_{10000}=0.015302406276020974$ Code: tmp = 1 for i in range(1,11000): if i % 1000 == 0:

    print('i = ', i, 'a_i/sqrt_i - (2*i)^(0.5) ',  tmp - (2*i)**0.5)

tmp = tmp + 1/tmp It looks like $b_{n} > b_{n+1} > 0$ and $b_n \to 0$ as $n \to \infty$ . Addition (an idea that might be helpful) :
there's the next idea which looks like it may be useful but I don't know how to apply it. An equation $a_{n+1}-a_{n} = \frac{1}{a_n}$ looks like an ODE $y' = \frac{1}{y(t)}$ , Solving ODE we get $2y'y = 2 + c \Longrightarrow y^2 = 2t + c\Longrightarrow y = \sqrt{2t + c}$ . Then maybe we may get properties of $a_n$ from the solution $y(t) = \sqrt{2t + c}$ if we estimate the precision of approximation of ODE by our linear recurrence relation. I am not an expert in the field of such approximations. If you think that idea with $y(t)$ is useful please give a hint how to bring this idea to mind.","['alternative-proof', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
4624313,"If $a,b>0$ and $a+b=2$ , prove that $a^{2b}+b^{2a}+(\frac{a-b}2)^2\leqslant2$","If $a,b>0$ and $a+b=2$ , prove that $$
a^{2b}+b^{2a}+(\frac{a-b}2)^2\leqslant2
$$ The equality occurs if and only if $(a,b)\sim(1,1)$ or $(a,b)\sim(2,0)$ or its cyclic permutations. My attempt: By symmetry and the constraint, we may let $x=a-1=1-b$ , the inequality converts to $$
(1+x)^{2(1-x)}+(1-x)^{2(1+x)}+x^2\leqslant2
$$ I tried to let $f(x):=(1+x)^{2(1-x)}+(1-x)^{2(1+x)}+x^2$ , but its derrivative is too complicated. I also tried to write $(1+x)^{1-x}$ as $e^{2(1-x)\ln(1+x)}$ and apply inequalities like $\ln x\leqslant x-1$ , $e^x\leqslant\dfrac{1}{1-x}$ and such, but it would be either too complicated or too crude. How to solve it?","['algebra-precalculus', 'inequality']"
4624399,Show that $E(X) = \sum^{\infty}_{n=0} P(X > n)$,"Show that, if X takes non negtive integet values: $$
E(X) = \sum^{\infty}_{n=0} P(X > n)
$$ The solution is: $$
E(X)
\!
\begin{aligned}[t]
& = \sum^{\infty}_{m=0} m * P(X = m) \\
& = \sum^{\infty}_{m=0} \underbrace{\sum^{m-1}_{n=0}}_{1}  P(X = m) \\
& = \sum^{\infty}_{m=0} \underbrace{\sum^{\infty}_{m=n+1}}_{2} P(X = m) \\
& = \sum^{\infty}_{n=0} P(X > n)
\end{aligned} 
$$ I do not get the mathematical passage that the author is making with the summation that I'm underlying. Edit: In the meanwhile, I propose another solution: $$\begin{align*}
E(X)&=\sum_{n=1}^\infty nP(X=n)\\
&=P(X=1) + 2P(X=2)+3P(X=3)+\dots\\
&=\left(P(X=1)+P(X=2)+P(X=3)+\dots\right)+(P(X=2)+P(X=3)+\dots)+(P(X=3)+\dots)+\dots\\
&=P(X\geq 1) + P(X\geq2)+P(X\geq3)+\dots\\
 &=\sum^{\infty}_{n=1}P(X\geq n)\\
\end{align*}$$ Is that correct?","['expected-value', 'probability-theory']"
4624414,An infinite quantity divided up into infinite number of boxes?,"I'm not sure how to phrase this in a proper set theory setting but let's say that one has a quantity which is infinitely large and then divides the quantity up so that it goes into a collection of boxes, in the limit as the number of boxes goes to infinity, does that mean the amount of the quantity in each box will get infinitely small? So the question is like, what happens with the two infinities when you have an infinite amount of something being divided up into an infinite number of boxes?","['elementary-set-theory', 'set-theory']"
4624439,"A holomorphic function sending integers (and only integers) to $\{0,1,2,3\}$","Does there exist a function $f$ , holomorphic on the whole complex plane $\mathbb{C}$ , such that $f\left(\mathbb{Z}\right)=\{0,1,2,3\}$ and $\forall z\in\mathbb{C}\ (f(z)\in\{0,1,2,3\}\Rightarrow z\in\mathbb{Z})$ ? If yes, is it possible to have an explicit construction? Note that, for example, $h(z)=\frac{1}{6} \left(9-8 \cos \left(\frac{\pi z}{3}\right)-\cos (\pi z)\right)$ is not a valid solution, since, in particular, certain roots of the equation $h(z)=0$ are not integers, but complex numbers. Also note that this question is answered in positive regarding the function $g$ such that $g\left(\mathbb{Z}\right)=\{0,1,2\}$ and $\forall z\in\mathbb{C}\ (g(z)\in\{0,1,2\}\Rightarrow z\in\mathbb{Z})$ . In this case, an example of such function is $g(z)=1-\cos \left(\frac{\pi z}{2}\right)$ .","['complex-analysis', 'galois-theory', 'elliptic-functions', 'riemann-surfaces']"
4624465,Reconstructing a sheaf from its global sections,"Let $\mathcal{F}$ be a sheaf on a smooth manifold $M$ with the property that $\mathcal{F}(U)$ is a $C^{\infty}(U)$ -module for every open subset $U\subseteq M$ .
I wonder if/when you can reconstruct $\mathcal{F}$ from its global sections $\mathcal{F}(M)$ . Maybe you can define something like $\tilde{\mathcal{F}}(U):=C^{\infty}(U)\otimes_{C^{\infty}(M)}\mathcal{F}(M)$ ?","['modules', 'sheaf-theory', 'differential-geometry']"
4624471,Smooth function motivation in Lee,"I was reading a bit in ""Introduction to Smooth Manifolds"" by John M. Lee for some motivations and came across the following section on page $11$ - $12$ . ""Each point in $M$ is in the domain of a coordinate map $\varphi:U \to \hat{U}.$ A plausible definition of a smooth function on $M$ would be to say that $f:M \to \mathbb{R}$ is smooth if and only if the composition $f \circ \varphi^{-1}:\hat{U} \to \mathbb{R}$ is smooth in the sense of ordinary calculus. But this will only make sense if this property is independent of the choice of coordinate chart."" Why is that the case? Wouldn't the definition "" $f$ is continuous iff at every point $p$ there is some chart $(U,\varphi)$ with $p \in U$ such that $f \circ \varphi^{-1}$ is smooth"" be a perfectly fine definition? Maybe I am overlooking something basic here, but I, at the moment, don't see why this wouldn't be well defined.",['differential-geometry']
4624492,"Proof by contradiction that if $f$ and $f\circ g$ are injective, then $g$ is injective","I would like to see if this proof by contradiction is correct. Prove by contradiction that if $f$ and $f\circ g$ are injective, then $g$ is injective. The premise for contradiction is: $f$ and $f\circ g$ are injective, and $g$ is not injective. Proof: Let $g : A\to B$ and $f:B\to C$ .
Suppose $g$ is not injective and $f\circ g$ is injective. Because $g$ is not injective, there exists $m$ and $n$ in $A$ such that $m\neq n$ and $g(m)=g(n)$ . Clearly, $f(g(m)) = f(g(n))$ . Hence $$(f\circ g)(m) = (f\circ g)(n)\tag{1}$$ Because $f\circ g$ is injective. Thus, universal instantiation on the definition, give us: $$(f\circ g)(m) = (f\circ g)(n) \implies m=n.\tag{2}$$ Modus Ponens on $(1)$ and $(2)$ , we conclude that $m=n$ . This contradicts the fact that $m\neq n$ .","['elementary-set-theory', 'functions', 'solution-verification', 'discrete-mathematics']"
4624506,Minimizing the expected time to get a string of heads with biased coins,"Imagine you have $n$ coins. Coin $i$ has a probability $p_i$ to get heads. You can choose these probabilities so long as $\sum_{i=1}^n p_i = \alpha$ for some constant $\alpha \leq n$ . Now that you've chosen the probabilities, you must play a game. Flip coin 1; if it comes up tails, you must restart the game. However, if it comes up heads, flip coin 2; if it comes up tails, you must restart the game. To win the game you must flip $n$ heads in a row. You want to assign the probabilities $p_i$ such that the expected number of coin flips is minimized. Note that if the question were to assign $p_i$ such that the probability of winning is maximized, a standard result is to choose $p_i = \frac{\alpha}{n}$ for all $i$ . Formally, this problem can be stated as: $$\min_{p_i} E_n \text{ where } E_n = \left(n \prod_{i=1}^n p_i + \sum_{i=1}^n (E_n + i) \ p_1 p_2 \ldots p_{i-1} (1 - p_i)\right)$$ With the following constraints: $$\sum_{i=1}^n p_i = \alpha \text{ and } p_i \in [0, 1]$$ In the case of $n=2$ this is easy, we have $p_2 = \alpha - p_1$ and: $$E_2 = 2 p_1 (\alpha - p_1) + (E_2 + 1)(1 - p_1) + (E_2 + 2)p_1(1 - \alpha + p_1)$$ Solving for $E_2$ and setting $\frac{\partial E_2}{\partial p_1} = 0$ , we eventually find: $$p_1 = \sqrt{1 + \alpha} - 1$$ For $\alpha = 1$ , this gives $p_1 \approx 0.414$ . This agrees with intuition - instead of doling out the probabilities equally, you should favor lower probabilities for low indices $i$ and higher probabilities for high indices $i$ . When $\alpha \geq 2$ , we would expect the formula to return $p_1 \geq 1$ , since $p_1 = p_2 = 1$ is optimal, but for some reason it does not do this. However, it already becomes very difficult when $n=3$ . Is there a general way to attack this problem? I'm able to derive the following: $$E_n = \frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{n-1}}{p_1 p_2 \cdots p_n}$$ Using the method of Lagrange multipliters, I can get the following $n+1$ nonlinear equations in $n+1$ variables: $$\frac{\partial E_n}{\partial p_i} = -\frac{1 + p_1 + p_1 p_2 + \ldots + p_1 p_2 \cdots p_{i-1}}{p_1 p_2 \cdots p_{i-1} p_i^2 p_{i+1} \cdots p_n} + \lambda = 0$$ $$\frac{\partial E_n}{\partial \lambda} = p_1 + p_2 + \ldots + p_n - \alpha = 0$$ But I don't see how to solve this set of nonlinear equations.","['optimization', 'statistics', 'probability']"
4624526,Two variables limit.,"Exercise . Discuss in $\alpha\in\mathbb{R}$ the value of following limit $$
\lim_{(x,y)\to(0,0)}f(x,y)=\lim_{(x,y)\to(0,0)}\frac{x^2y}{(x^4+y^2)^\alpha(x^2+y^2)^{\frac{1}{2}}}.
$$ Solution . One has that $|f|\leq|x||y|^{1-2\alpha}$ , which converges to $0$ if $\alpha\leq\frac{1}{2}$ , and hence so does $f$ . On the other hand, looking at the restriction of $f$ on the set $\{x>0, x=y\}$ , one has $$
\lim_{x\to0^+}f(x,x)=\lim_{x\to0^+}\frac{1}{\sqrt{2}}x^{2-2\alpha}
$$ which equals $+\infty$ as soon as $\alpha>1$ , and hence in this case the above limit does not exist (being for example $f$ identically zero on the coordinate axes). I am left with the case $\frac{1}{2}<\alpha\leq1$ , which I tried to work out using polar coordinates but with no success.","['multivariable-calculus', 'limits', 'calculus', 'real-analysis']"
4624593,A Set of $n$ Numbers Where Any $k$ Taken Together Sum to a Prime,"Not too long ago, I saw an interesting question on this site that was asking about the maximum value of $n$ if there exist $n$ numbers such that any $3$ of them taken together sum to a prime. However, before I could answer it, the question was deleted. I am interested in a generalisation of this question. Hence, my question is: Let $k$ be an arbitrary natural number. What is largest natural number $n\ge k$ such that there are $n$ natural numbers from which any arbitrary selection of $k$ numbers add up to a prime? Small values of $k:$ $k=1:$ Here, $n$ can be any natural number. We just have to take a set of $n$ primes. $k=2:$ This is where things start to get interesting. Since if two numbers sum up to an odd prime, they must be of different parity, we get that the maximum value of $n$ is $2.$ $k=3:$ Taking modulo $3$ , we get that in this set of $n$ numbers, we can have reminders $0,1,2.$ Since no $3$ of them taken together must be divisible by $3,$ we get that $n≤6.$ If $n$ were greater than $6$ , then we could find $3$ numbers with the same remainder modulo $3.$ But, we can do better. Suppose $S_3$ is an ordered $6$ -tuple of $6$ numbers taken modulo $3$ . Let $S_3=(0,1,2,0,1,2).$ Note that if $S_3$ were anything else, then too, we could find $3$ numbers such that they have the same remainder modulo $3.$ Hence, if we prove that this set isn't possible, we will be done with the $n=6$ case. In $S_3,$ we can pick the first three numbers. They have remainders $0,1,2.$ Since their sum is $0$ modulo $3,$ this number is either a multiple of $3,$ or $3$ itself. But, this number cannot be $3$ as there is only way to partition $3$ into $3$ natural numbers: $1+1+1.$ Since this number is a multiple of $3,$ and hence cannot be prime, we are done. So, $n≤5.$ With a bit of casework, it's not too tough to show that $n=5$ isn't possible either. So, $n≤4.$ $k≥4:$ This is where I need your help. Following the trend, I took modulo $k.$ I have a sort of upper bound for $n.$ Again, let $S_k$ be an ordered $n$ -tuple of $n$ numbers taken modulo $k.$ Now, $S_k$ cannot have the block $(0,1,2,\ldots,k-1)$ repeating $k$ times. If it did have this block $k$ times, we would again be able to find a number that's divisible by $k.$ In fact, by the same argument, we get that this block can be present at most $k-1$ times. Hence, $n≤k(k-1).$ But, like we saw before, we may be able to do better than $n≤k(k-1).$ Note that I am using the block $(0,1,2,\ldots,k-1)$ to find the upper bound, as if the block were anything else, we would find $k$ numbers with the same remainder modulo $k$ quicker.","['elementary-number-theory', 'combinatorics', 'prime-numbers']"
4624639,"if $f$ and $g$ are solutions of linear homogenous ODE, prove $af+bg$ is also a solution","a) If $f$ and $g$ are solutions of a linear homogenous ODE on some interval, prove that $af+bg$ is also a solution (on that same
interval) for any real $a$ and $b$ . I found a solution online that essentially says: Let, $y''+py'+qy=0$ be the linear homogenous ODE Now check that $af+bg$ is a solution by verifying $(af+bg)''+p(af+bg)'+q(af+bg)=af''+bg''+paf'+pbg'+qaf+qbg$ $=a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0$ (since $f$ and $g$ are solutions which means $f''+pf'+qf=0$ and $g''+pg'+qg=0$ ) My question is , why did the solution use a second order ODE? Can I do this with a first order? i.e.: $y'+py=0$ so $(af+bg)'+p(af+bg)=0$ $af'+bg'+paf+pbg=0$ $a(f'+pf)+b(g'+pg)=0$ Using the same reasoning as the solution above: $a*0+b*0=0$ b) If we drop the ""homogenous"" hypothesis, is this still true? Prove a counterexample or proof. My attempt at a solution $y''+py'+qy=r(x)$ $f''+pf'+qf=0$ and $g''+pg'+qg=0$ Also the general solution to a non-homogenous DE is: $y(x)=g(x)+h(x)$ where $h(x)$ is a particular solution and $g(x)$ is the general solution to the corresponding homogenous DE $(af+bg)''+p(af+bg)'+q(af+bg)=a(f''+pf'+qf)+b(g''+pg'+qg)=a*0+b*0=0$ This is the general solution to the corresponding DE (i.e. the $g(x)$ ) I'm stuck on how to proceed from here.",['ordinary-differential-equations']
4624701,Number of balanced partitions of a set,"Question: A balanced bipartition of set $X$ splits $X$ into two parts $A$ and $B$ so that the sizes
of $A$ and $B$ differ by at most $1$ . In particular, when $X$ is even, $|A| = |B| = \frac{|X|}{2}$ ;
when $|X|$ is odd, $|A| = |B|—1 = \lfloor |X|/2\rfloor$ . We refer to the bipartition as $A \cup B$ . For
example, $\{b, e\} \cup \{a, c,d\}$ and $\{a,c\} \cup \{b, d, e\}$ are two different balanced bipartitions
of {a,b,c,d,e}. On the other hand, we treat $\{a,b\} \cup \{c,d\}$ and $\{c,d\} \cup \{a,b\}$ as
the same bipartition of $\{a, b, c,d\}$ . Solution: For $|X| = 2$ we have $\{1\}\cup\{2\}$ For $|X| = 3$ we have $\{1,2\}\cup\{3\}$ , $\{1,3\}\cup\{2\}$ , $\{2,3\}\cup\{1\}$ For $|X| = 4$ we have $\{1,2\}\cup\{3,4\}$ , $\{1,4\}\cup\{2,3\}$ , $\{1,3\}\cup\{2,4\}$ My thinking is the number of ways are: If $X$ is even then $\frac{\binom{X}{\frac{X}{2}}}{2}$ If $X$ is odd then $\binom{X}{\lfloor \frac{X}{2} \rfloor}$ But I am unsure if this is correct or how to prove it.",['discrete-mathematics']
4624703,What is the simplest solution to this elementary geometry problem?,"The Problem Given a triangle $ABC$ , points $M$ and $N$ are chosen in such way that the midpoint of segment $BM$ is coincident with the midpoint of side $AC$ , and the midpoint of segment $CN$ is coincident with the midpoint of side $AB$ . Prove that points $M$ , $N$ , $A$ are collinear. The drawing is provided. The Solution Below I provide my own solution of the stated problem. Let $P$ be the midpoint of $AB$ , and $Q$ the midpoint of $AC$ . We are going to prove that $A\in MN$ . $\overline{CP}=\overline{PN}$ , $\overline{AP}=\overline{PB}$ (by statement of the problem), $\angle CPB = \angle APN$ (these angles are vertical by definition) $\Rightarrow _\Delta BPC\cong _\Delta APN$ (SAS congruence). $_\Delta BPC\cong _\Delta APN\Rightarrow \angle PCB=\angle PNA\Rightarrow BC\parallel AN$ (from the congruence of alternate angles). $\overline{BQ}=\overline{QM}$ , $\overline{AQ}=\overline{QC}$ (by statement of the problem), $\angle BQC = \angle AQM$ (these angles are vertical by definition) $\Rightarrow _\Delta BQC\cong _\Delta MQA$ (SAS congruence). $_\Delta BQC\cong _\Delta MQA\Rightarrow \angle CBQ=\angle AMQ\Rightarrow BC\parallel AM$ (from the congruence of alternate angles). $BC\parallel AN$ and $BC\parallel AM$ , so $BC$ is parallel to two line segments with a common endpoint. Clearly, as $ABC$ is a triangle, point $A$ is not on line $BC$ . But by Playfair's axiom , at most one line parallel to the given line can be drawn through the point not on this line. Thus, two segments with a common endpoint can be parallel to the same line if and only if they are on the same line. So we conclude that $\overline{AM}$ and $\overline{AN}$ are collinear, and thus, $A$ , $M$ , $N$ are collinear, QED. The Question I'm looking for the simplest solution of this problem you can find. By simplest I mean the most elementary, using only basic facts of geometry, preferrably more elementary than the one I came up with above.","['euclidean-geometry', 'solution-verification', 'geometry']"
4624719,What can be assume when proving an implication in which the hypothesis or conclusion is also an implication?,"The topic is about proving something of the form: $ p \implies q $ in which $p$ is "" $a \implies b$ "" and q is "" $c \implies d$ "". Thus, it can be rewritten as: $(a \implies b) \implies (c \implies d)$ Also, this question focus on direct proof . What I already know: Now, I already know that to prove an implication, we can start by assuming the hypothesis is true and somehow deduct that q is true. Hence, for $p \implies q$ , we can assume that $p$ is true, then try to show that $q$ is logically followed. However, I now have a problem when $p$ and $q$ by themselves are also an implication. I don't think I can show that q follows just by assuming only p is true in this case. What can and can't be assumed? My understanding (let me know if this thought process is wrong): I think we can't prove this by assuming $b, d$ or $q$ to be true because both of them are the conclusion. However, we can assume that $a$ and $c$ are true since they are both a hypothesis, and at the same time also assume p is true since p itself is also a hypothesis. Now, Can our proof be complete by assuming that $a, c $ , and $p$ are true and show that $d$ is logically follows from that 3 assumptions? Also, if we can assume $p$ and $a$ to be true, can we use Modus Ponen on them to deduct that b is also true? Below is just an example of such problem: Proof by contradiction that if $f$ and $f\circ g$ are injective, then $g$ is injective The definition of injective function is $\forall{x}\forall{y}(f(a)=f(b)\implies a=b) $ . It would seem that the question ask us to prove.
""if $f \circ g$ is injective, then $g$ is injective"". hence, $$((f \circ g)(m) = (f \circ g)(n) \implies m=n) \implies (g(m)=g(n) \implies m=n)$$ which is of the form $p \implies q$ and $p$ is "" $a \implies b$ "" and $q$ is "" $c \implies d""$ for which p is "" $(f \circ g)(m) = (f \circ g)(n) \implies m=n$ "" q is "" $g(m)=g(n) \implies m=n$ "" a is "" $(f \circ g)(m) = (f \circ g)(n)$ b is "" $m=n$ "" c is "" $g(m)=g(n)$ "" d is "" $m=n$ The accepted answer starts by assuming $f \circ g$ is injective, which is the same as assuming $p$ is true. Then assume g(m) = g(n), which is the same as assuming $c$ is true, then he is able to show that $d$ is follow. It seems that he only needs to assume $p$ and $c$ to show that $d$ is follow. However, can I also assume "" $a$ "" is true? Also, $p$ and $a$ are sufficient to deduct that m=n. Do I still need to show that m=n is followed by $c$ rather than $p$ and $a$ ? Note: I also want to sorry if my question is very confusing or if I am confused about some very obvious stuff. I have no tutor/school yet, and learn from reading book only and my knowledge may have gaps on obvious stuff.","['proof-writing', 'logic', 'discrete-mathematics']"
4624727,Transformation that maps a rectangular region $D$ in the $uv$-plane onto the region $ R$,"Let $R= (x, y)\in R^2: x≥ 0, y ≥0, 1\le x+y\le 2$ Find a transformation that maps a rectangular region $D$ in the $uv$ -plane onto the region $R$ , and use it to evaluate $\iint_R \frac{y}{x + y} dA $ . This was asked in one of my exams (the exam is over) but I was unable to find a rectangular region $$D in the $uv$-plane which maps to region $R$. I have tried to use the substitution $u=x+y$ and $v=y$ and got a trapezium. Other transformations which I have tried does not work. Does there exist such a transformation? Any help will be appreciated.","['multivariable-calculus', 'calculus', 'jacobian', 'transformation']"
4624779,Is there difference between support of function and support of module?,"Let $A$ be a commutative ring, and $M =A$ can be treated as an $A$ -module. Let $f\in A$ we have the submodule $(f) \subset A$ therefore we can consider the support of the module $(f)$ , which is : $$\text{supp} ((f)) = \{p\in \text{Spec} A\mid (f)_p \ne 0\} \tag{1}$$ where $(f)_p$ means taking the localization of the module $(f)$ at $p$ . There is another definition of support since $f \in A = \mathcal{O}(\text{Spec} A)$ the support of function $f$ can also be defined as ： $$\text{supp}(f) = \{p\in\text{Spec} A \mid f(p) \ne 0\} \tag{2}$$ Where $f(p)$ means the image of $f$ in the residue field via the canonical map $$A\to A_p\to A_p/(pA_p) = \kappa(p)$$ I am not sure, but I found these two different definitions of support are different, in the first one by this post in stack project thereom 10.40.7 which shows if we use definition (1), we have Lemma 10.40.7. Let $A$ be a ring, let $M (= A)$ be an $A$ -module, and let $f\in A$ . Then $p\in V(\text{Ann}(f)) = \text{supp} ( (f))$ if and only if $f$ does not map to zero in $A_p$ . However in definition (2): $f(p) \ne 0$ iff $f$ does not map into the maximal ideal $pA_p$ in the local ring $A_p$ . You see one is equivalent to not mapping to zero another one is not mapped to the maximal ideal. Are these two definitions of support different?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra', 'modules']"
4624886,Change of variables example in an ODE,I do not understand how this was obtained Why the $(y')^2$ becomes $y'$ and the $y'$ becomes $(y')^2$ and why $y^2$ becomes $x^2 (y')^3$ in the new ode? How does $\frac{d y(x)}{d x}$ change under this change of variables?,"['calculus', 'ordinary-differential-equations', 'chain-rule']"
4624890,Asymptotic stability implies the existence of a strong Lyapunov function,"I am having trouble understanding the proof that asymptotic stability implies the existence of a strong Lyapunov function. Taken from the book ""Differential Dynamical Systems"", chapter 4, by James Meiss. The definitions are: and the theorem is: is a strong Lyapunov function. This is the start of the proof: I don't understand how asymptotic stability implies there exists such a ""uniform"" time $T(\rho)$ . I do understand how for all $x\in U$ there exists some time $T(\rho, x)$ but don't understand how there exists a uniform $T(\rho)$ which does not depend on $x$ .","['ordinary-differential-equations', 'lyapunov-functions', 'stability-theory', 'uniform-convergence', 'limits']"
4624896,All natural number solutions for the equation $a^2+b^2=2c^2$,"$a$ , $b$ and $c$ of all Pythagorean triplets can be written in the form $$
\begin{split}
a &= 2mn\\
b &= m^2-n^2 \\
c &= m^2+n^2
\end{split}
$$ where $m$ and $n$ are natural numbers. For any natural number $m$ and $n$ , this set of equations will give a Pythagorean triplet. And all Pythagorean triplets satisfy this set of equations. Can $a$ , $b$ and $c$ of all triplets satisfying the equation $$a^2+b^2=2c^2$$ where $a$ , $b$ and $c$ are natural numbers, be written as a set of equations as for the Pythagorean triplets? So, I need a set of equations that generates triplets that satisfy the equation $a^2+b^2=2c^2$ for any natural numbers I plug into the set of equations. Also, every natural number triplets satisfying the equation $a^2+b^2=2c^2$ must satisfy the set of equations. I tried to derive the set of equations myself, no attempts have been successful yet. I would like to have the proof of the set of equations, (otherwise I won't know if every triple will satisfy the set of equations) Any comments that helps to give an insight into solving the problem are really appreciated.","['sums-of-squares', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'algebra-precalculus']"
4624936,Covering map lifting path uniquely,"Let $p:E\rightarrow B$ be a covering map, let $p(e_0)=b_0$ . Any path $f:[0,1]\rightarrow B$ beginning at $b_0$ has a unique lifting to a path $\bar{f}$ in $E$ beginning at $e_0$ . This is a lemma from the book ""Topology""  by Munkres. I am unable to understand first part of the proof. Proof: Cover $B$ by by open sets $U$ each of which is evenly covered by $p$ . Find subdivision of $[0,1]$ , say $s_0,s_1,...,s_n$ , such that for each $i$ the set $f([s_i,s_{i+1}])$ lies in such an open set . Using Lebesgue number lemma. How to use Lebesgue number lemma here??","['general-topology', 'algebraic-topology']"
4624939,Calculate limits for a function defined by an integral.,"Hello I need help with the second item in the next exercise : The exercise says:
Let the function $F : \mathbb{R} \longrightarrow \mathbb{R}$ given by $F(x) = \int_{x}^{2x}e^{-t^{2}}dt$ . (a) Show that $F(x)$ is a odd function. (b) Calculate $\lim_{x \to \infty} F(x)$ and $\lim_{x \to -\infty} F(x)$ . For the first item, I showed that $F'$ is an even function, but for the second item I have no ideas. Some suggestions?","['integration', 'limits', 'calculus', 'exponential-function']"
4624944,How do I find the function and derivative of an unknown curve?,"I have $x$ and $y$ values to plot the curve and I need to find a tangent line of slope 1 that intersects the curve (and the point at which it intersects). I was trying to do polynomial and exponential regression to model the data and fit the curve (in R) but cannot get it to work. It fits a majority of the data but not the curve and I need the function of the curve to find the derivative and tangent. I don't completely understand the math behind the modeling but all I know is that the fit is inaccurate when I plot it against my data. Is there any way I solve this mathematically? The curve starts slightly downwards, then slopes up gradually before it goes up exponentially If you do a density plot, most of the values are towards the lower end of y. the values of y are right skewed . This could be influencing my model, so would one solution be to remove multiple prior points? UPDATE: I tried to limit the number of points and now my plot looks like this (the line plotted is $8.950433e^{-20}\times1.006621^x$ ). The formula for the line was from taking the exponential regression of the data. From the formula, it kind of makes sense why it's a line but how do I get it to curve?","['mathematical-modeling', 'approximation', 'regression', 'tangent-line', 'functions']"
4624946,"Show that for certain initial conditions the solution curves stay in $[0,\pi]\times[0,\pi]$","$$\dfrac{dx}{dt} = \sin(x)\left(-0.1\cos(x)-\cos(y)\right)$$ $$\dfrac{dy}{dt} = \sin(y)\left(\cos(x)-0.1\cos(y)\right)$$ Show that for the initial condition $(x_0,y_0)\in{[0,\pi]\times[0,\pi]}$ the solution curves are in $[0,\pi]\times[0,\pi]$ for all $t\in{\mathbb{R}}$ . I thought I could show that if the orbit of the solution enters the borders of $[0,\pi]\times[0,\pi]$ . Trying a solution of the form \begin{equation}\begin{bmatrix}
x \\
0
\end{bmatrix}\end{equation} for $x\in{[0,\pi]}$ . This means that $$\dfrac{dx}{dt} = \sin(x)(-0.1\cos(x)-1)$$ But I don't know how to solve this DE. Is there a better method of solving this?",['ordinary-differential-equations']
4624968,"Shading the entire $i$-th row and $j$-th column of an $m\times n$ grid when $\gcd(i,m)>1$ and $\gcd(j,n)>1$, how many grids leave $x$ cells unshaded?","Is there a way of cleverly counting the following scenario? Given an $m\times n$ grid, with $m$ and $n$ relatively prime, imagine shading a subset of the squares of an $m\times n$ grid using this procedure: For each $i \in \{1,\dots,m\}$ such that $\gcd(i,m)>1$ , shade all of the squares in the $i^\text{th}$ row. For each $j\in \{1,\dots,n\}$ such that $\gcd(j,n)>1$ , shade all of the squares in the $j^\text{th}$ column. Let $\sigma (x)$ be the number of ways to choose the ordered pair $(m,n)$ such $m$ and $n$ are relatively prime, and that there are exactly $x$ unshaded squares when you do this procedure. Question : Is there a clever way to compute $\sigma(x)$ ? For example $\sigma (8)$ represents the number of $m\times n$ squares such that it has $8$ holes. I can think of two grids which are in $\sigma(8$ ) and those are $3\times 5$ grids and the $4\times 5$ grids as drawn below (I shaded the third row and $5$ th row for the $3\times 5$ grid as $3$ and $5$ are the only prime factors of the row and column numbers of $3$ and $5$ greater than $1$ ): But there might be more than these two grids which are in $\sigma (8)$ , so is there a formula for counting the total number of grids which fall under $\sigma(x)$ for any $0\le x $ ?","['number-theory', 'combinatorics', 'geometry', 'elementary-number-theory']"
4624969,What exactly is Geometric Analysis?,"I don't really know if here's a good place to ask this question, but I'd appreciate it if someone can guide me here so that I can choose the right path going forward! I recently got my masters degree and now I am trying to figure out what is best for me to do in my PhD and I have bunch of questions about Geometric Analysis and would be happy if professionals can help me through this! What exactly is geometric analysis and what's the difference between this field and Differential Geometry? What are the fundamental courses you need to go through to be called a geometric analyst? What are its applications? Is it mostly applied in physics or does it also have some applications in the industry? If so, what are some examples of the projects in this field and can one apply it to machine learning for instance? Thanks for everyone's help!","['soft-question', 'differential-geometry']"
4624973,Conjecture: The sequence $\frac{2}{n} \sum_{i=1}^n \sqrt{\frac{n}{i-\frac{1}{2}}-1}$ converges to $\pi$,"I found that the series $$s(n) = \frac{2}{n} \cdot \sum_{i=1}^n \sqrt{\frac{n}{i-\frac{1}{2}}-1}$$ converges to $\pi$ as $n \to \infty$ .
To verify this I have computed some values: $n$ $s(n)$ $10^1$ 2.76098 $10^3$ 3.10333 $10^5$ 3.13776 $10^6$ 3.14038 $10^7$ 3.14121 Which seems to support the claim, however, this is no proof of the convergence. I would not know how to begin on a proof of this limit and did not find any similar formula in known approximation formulas . Does anyone have an idea on how such a proof can be constructed?","['pi', 'approximation', 'sequences-and-series']"
4625068,How long does it take for a two-site wait activation?,"Consider two sites, linked as sketched Initially, both sites are off (red). However, each activates (turns green) at a constant rate $f$ . Once activated, a site remains activated. If one activates, it generates an activation wave (green line) which propagates at speed $v$ (given as the number of sites per time unit), as shown Then, one of two things can happen: either the wave activates the neighboring site, provided it has not been activated yet, or the site activates on its own How do we calculate the expected time it takes for one site to activate? In other words, what is the expected waiting time on one site? If there was only one site, the answer would simply be $1/f$ , but I am struggling to frame the two (and possible $n$ -case ) scenario. Any suggestions? My attempt: Given a time interval $dt$ , the probability $P_f$ that one site is activated, within $dt$ , on its own is simply $fdt$ . However, the probability $P$ that a site is activated within $dt$ is given by $$
\begin{align}
P(dt)=&P_1(\text{""site activates autonomously, given the wave didn't reach it within }dt\text{""})\\
&+P_2(\text{""wave activates the site, given it wasn't autonomously activated within }dt\text{""})
\end{align}
$$ However, it seems relatively tricky to write these expressions. Intuitively, I would expect something like $$
P_1(t)=1-e^{-ft/v}
$$ for example, but I have no particular motivation for this choice. It simply satisfies some expected asymptotics. Any ideas? Perhaps an approach based on first-hitting time analysis?","['statistics', 'random-variables', 'stochastic-processes', 'probability', 'dynamical-systems']"
4625071,Given a walk between two vertices can you always find a trail between those two same vertices?,"I want to prove this proposition that intuitively seems true. Let me first start by defining both walk and trail as I'm not studying graph theory in english and I'm not 100% sure if these are the correct terms for what I want to say. Given a graph G, A walk is a succession of vertices and and edges of the following form, $r=x_0a_1x_1\cdots x_{l-1}a_lx_l$ where $l$ (the number of edges) is the length of the walk. A trail is when a walk has no repeated edges. Given this two definitions, let $r=x_0a_1x_1\cdots x_{l-1}a_lx_l$ be a walk with length $l$ from the vertex $k$ denoted by $x_0$ to the vertex $j$ dentoted by $x_l$ . In order to find a trail from $k$ to $j$ with the elements of $r$ , I need to find a subsuccession of $r$ starting in $k$ and finishing in $j$ that has no repeating edges. To do this, as there's no restrictions over $r$ , I first take a subsuccession of the original path that only crosses the initial and final vertices once (starts in $k$ and finishes in $j$ ). I can build this new walk the following way, I define $x_f \in r$ such that $f=\max\{n \in \mathbb N : x_f \in r \textrm{ and } x_f=x_l\}$ and I build the new walk $\hat r =x_0a_1x_1\cdots x_{f-1}a_fx_f$ Now, I define $x_i \in \hat r$ such that $i=\min\{n \in \mathbb N : x_i \in \hat r \textrm{ and } x_i=x_0\}$ and build the new walk $\bar r =x_ia_{i+1}x_{i+1}\cdots x_{f-1}a_fx_f$ I'm not sure if this previous step is necesary for the rest of the proof but thinking about it i thought that having the ending and starting points appear more than once could give some problems and as I only want to prove that there's at least one path with the elements in the original walk, this doesn't restrict the hypothesis. Now that i have the shortened walk, I have to find a way to ""remove"" the extra edges. The way I thought of this by intuition is that if a walk crosses a certain edge and then it crosses it again after som extra steps, those extra steps between the first and last time it crosses the edge can be removed (its a loop, no matter how long it is you end up back going through the same edge). However, if two edges are the same, as the graph is not directional, the walk could be going in same or different directions and I have to treat both cases differently. I propose the following process to build the path from the given walk $\bar r = r_0$ . 1-Let $p_m=i+m$ with $1\leq m \leq f-i$ and let $m=1$ for the first    step 2-While $m\leq f-i$ repeat: 2.1-Given $a_p$ the p-th edge in the walk, define $a_c \in r_{m-1}$ with $c=\max\{n\in \mathbb N : a_n \in r_{m-1} \textrm{ and } a_n=a_p \}$ 2.2-If $c=p$ then do nothing and skip to 2.4 2.3-If $a_p$ appears in $r_{m-1}$ as $xa_py$ and $a_c$ appears in $r_{m-1}$ as $xa_cy$ then define the succession $r_m$ as $r_{m-1}$ with the subsuccession $a_px_{p+1}a_{p+1}\cdots x_{c-1}$ removed 2.3-If $a_p$ appears in $r_{m-1}$ as $xa_py$ and $a_c$ appears in $r_{m-1}$ as $ya_cx$ then define the succession $r_m$ as $r_{m-1}$ with the subsuccession $x_pa_px_{p+1}a_{p+1}\cdots x_{c-1}a_c$ removed 2.4-Add one to $m$ and repeat step 2 3-You are left with $r_m$ a subsuccession built with the verices and edges from $r$ where the starting point is $k$ , the ending point is $j$ and no edges are repeated Therefore the walk built with this method is a trail from the edge $k$ to the edge $j$ I'd like to know wether this constructive proof is right and any sugestions anyone might have to improve it. I apologize in advance for the notation, my discrete maths professor has given us no proves on the topics of graphs and I have no idea on what notation is formaly used. Sorry too for any grammar or spelling mistakes I might have made, english is not my first language. Thanks in advance","['graph-theory', 'solution-verification', 'discrete-mathematics']"
4625076,Combinatorics problem - Counting Colors and Objects,"There are $2n$ colors and $2n$ objects. If we take any two objects, there are exactly $n$ colors that are on one and not on the other. Prove that if we take any two colors, there are exactly $n$ objects that have one, but not the other one. Be $O_m$ the set describing the colors of the object $o_m$ , and $C_m$ the set describing the objects that have the color $c_m$ . So for any i,j, $|(O_i\cup O_j)\setminus(O_i\cap O_j)|=n$ , prove that $|(C_i\cup C_j)\setminus(C_i\cap C_j)|=n$ . I tried Double Counting, but I wasn't able to solve it. I also tried solving it using the Extremal Principle.
I'm pretty sure it has to do with counting the colors from the perspecive of themselves, and from the perspective of the objects, but I've not been able to do it.",['combinatorics']
4625087,When does a semigroup homomorphism preserve identities on monoids?,"Let $X,Y$ be monoids , with identities $e_X,e_Y$ , respectively. Let $f:X\to Y$ be a semigroup homomorphism . That is, any function which satisfies $$f(xy)=f(x)f(y)\quad\forall x,y \in X\tag{1}$$ I know that $f$ is not necessarily a monoid homomorphism , where the definition of a monoid homomorphism is the same as a semigroup homomorphism, but with the additional requirement that it preserve identities: $$f(e_X)=e_Y\tag{2}$$ This second property is required [see, e.g., the discussion in a number of other math.SE threads, such as this one or this one for examples where $(1)$ holds but $(2)$ does not.] The reason I'm interested: In formal language theory, one deals with the set of all finite strings over some alphabet , which forms a monoid under concatenation, with the empty string being the identity (the ""free monoid""). In this area, it is common to state that the identity-preserving property $(2)$ can be derived from the semigroup homomorphism property $(1)$ (see for example Kozen 1997, Lecture 10 , or Kurz's lecture notes, Lecture 7 ).  I know this must be due to something about the structure of this kind of monoid, since it isn't true in general. My question is: what must be true about monoids $X$ and $Y$ such that all semigroup homomorphisms mapping $X$ to $Y$ preserve the identity (and as such are monoid homomorphisms)?","['monoid', 'semigroup-homomorphism', 'abstract-algebra', 'semigroups', 'formal-languages']"
4625145,is the existence of a coarsest topology nontrivial,"I understand what the definition of ""the coarsest topology satisfying some property x"" is, but my question is, shouldn't the existence of such a topology be nontrivial? For example, let $X = \{ a, b, c \}$ , and let our property be ""has four elements"". Then since $\{ \emptyset, \{a\}, \{b, c\}, \{a, b, c\} \}$ and $\{ \emptyset, \{c\}, \{a, b\}, \{a, b, c\} \}$ are both four element topologies on $X$ , there can not be a coarsest topology. I am asking this because many problems from my homework refer to a coarsest topology, and I just want to verify that the existence of one is indeed nontrivial.",['general-topology']
4625164,Does Euclid's bisection of an angle fail for $\frac \pi 3$?,"Does Euclid's bisection of an angle fail for angles of $\frac \pi 3$ (that is, of equilateral triangles)? In Book I Prop. 9 Euclid shows how to bisect an angle by constructing an equilateral triangle across it, and then connecting the angle's vertex to a point on the triangle.  A modern version of this is given on ProofWiki .  But these constructions seem to assume the point on the equilateral triangle will be distinct from the vertex of the angle, which is not necessarily true if the angle is $\frac \pi 3$ . Of course, the repair to the construction is easy: just construct the equilateral triangle on the other side of the line.  But the fact that such a triangle exists is never shown by Euclid (I.1) or by the modernized proofs on ProofWiki . Updates To address the comments: In the diagram, we wish to bisect angle $BAC$ , and we do so by constructing an equilateral triangle $FED$ .  The problem is that if the angle is $\frac \pi 3$ , we may get point $F$ is point $A$ . The construction only works when $F$ is distinct from $A$ . It is well known that there are many omissions in Elements .  However, to my knowledge, these are always cases where Euclid assumes something without (explicitly) proving it; but, in each case, the fact is indeed provable (at least with a sufficient set of axioms).  This case is different, because it is indeed quite possible for $A = F$ , in which case a different construction is needed.  Are there any other known cases where Elements does something like that?","['euclidean-geometry', 'geometry', 'geometric-construction']"
4625186,How many corners does an otherwise intrinsically flat surface need in order to be homeomorphic to a sphere?,"I want to build a virtual world that feels like an unbounded flat plane but actually ""connects back to itself"" with the topology of a sphere. To do this, we can build the world out of polygonal sectors corresponding to the faces of a polyhedron (bordering each other in the same configuration). This works nicely except that each vertex of the polyhedron becomes a weird point, around which a small loop has a total turning angle less than 360°. (For example, if the polyhedron is a cube, each vertex is the meeting point of three square sectors, so you can walk all the way around it while making just three quarter-turns.) We can still render these locations using a portal technique and put something like a tall tree at each one to obscure the visual discontinuity. To make these ""corners"" less conspicuous, I'll use a polyhedron with a large number of vertices. But if we didn't care about that, how few corners could we theoretically get away with? I think the smallest polyhedron that makes sense with my construction is a double-sided triangle, which has 3 corners. But what about a space like a cylinder with each end pinched together (2 corners), or the one-point compactification of a flat open disk (1 corner)? I can't visualize 3D embeddings of these that preserve flatness, but could there still be a corresponding ""flat world"" with just one or two singular points? In order to answer this, I think I need to clarify the idea of a ""manifold with corners"", and that's where I'm having trouble. I've thought about cutting the corners off (leaving a small boundary loop) or rounding them off (leaving a small region with nonzero curvature) to obtain an actual manifold (on which the Gauss-Bonnet theorem might be helpful), but I'm not sure how to express the requirement that the resulting defect is still ""point-sized"", which seems important. Inside the flatworld, I want the singularity to appear as a ""pole"" standing on a point of the plane (as opposed to some larger object), and I think this is what could rule out the one- or two-corner possibilities.","['geometric-topology', 'differential-geometry']"
4625194,Simple question on Hausdorff locally compact subspaces,"I'm reading through Topology by Dugundji and in chapter XI, theorem 6.5(2) the following argument is used in the proof (not a precise quote, I kept the essential parts): Let $A$ be a locally compact subset of a Hausdorff space $Y$ . Each $a\in 
A$ has a nbd $V(a)$ in $Y$ such that $\overline{V(a)}\cap A$ is compact, and therefore closed, in $Y$ . [...] I don't see how this is true. I attempted the following approach: Let $a\in A$ , let $U$ be a relatively compact nbd of $a$ in $A$ , then $U=V\cap A$ for some $V$ open in $Y$ . But I'm not sure if $\overline{V}\cap A$ is compact and I can't think of another way of constructing a nbd that satisfies the required condition. I'm sure the explanation has to be direct and simple (sorry for that) but there might be something that I'm overlooking. Definitions A Hausdorff spaced is said to be locally compact if every point has a nbd with compact closure. A set is said to be relatively compact if it has compact closure. (note that a necessary condition for a space to be locally compact is that it be Hausdorff).","['general-topology', 'compactness']"
4625212,Do differential forms need sheafification?,"Let $M$ be a connected manifold. One defines differential $k$ -forms as sections of the $k^{\text{th}}$ exterior power of the cotangent bundle. This is a sort of sheafification of a more naive approach, which is to let $D$ be the module of differential 1-forms, considered as a $C^{\infty}(M, \mathbb{R})$ -module, and then take the $k^{\text{th}}$ exterior power of $D.$ Question: Is there a connected manifold where the $k^{\text{th}}$ exterior power of $D$ does not coincide with the actual module of differential $k$ -forms, for at least one $k$ ?",['differential-geometry']
4625227,How many ways for a beetle to move from bottom left corner to upper right corner in a 6 x 6 grid if it must be done in 14 steps only?,"Note:  We allow all four directions (up, down, right, left but no diagonal) The 6 x 6 grid is composed of 7 horizontal  lines and 7 vertical lines. We are to calculate how many 14 steps paths are possible where it never returns to the same intersecting point. The answer given is $$C(12, 8)  \times C(8,7) \times (6/8) \times 2 = 5940$$ where $C(n,r)$ is here is the Binomial Coefficient. I cannot seem to figure out how this counting was done.
Can somebody provide me with the explanation regarding how this counting argument was done please? Any help is appreciated, thank you.","['contest-math', 'path-connected', 'binomial-coefficients', 'combinatorics', 'recreational-mathematics']"
4625234,Acute triangle $\triangle ABC$ has $(\sin A - 2 \sin 2B) = 2 - 2 \cos 2B$ Find the range of $\frac{\sin B + \sin C}{\sin A}$,"Angles in acute triangle $\triangle ABC$ has $(\sin A - 2 \sin 2B) \tan A = 2 - 2 \cos 2B$ Find the range of $$\frac{\sin B + \sin C}{\sin A}$$ We can prove that $\frac{a^2}{bc} = 4$ through some tedious trigonometric computations Let $\frac{b+c}{a} = k$ , then $1 > \cos A = \frac{b^2+c^2-a^2}{2bc}$ , and we plug in $\frac{a^2}{bc} = 4$ , and we might be able to find the range of $k$ somehow? But I can't finish and the computation seems daunting. I am also not sure if $\cos A <1$ condition is too loose.","['euclidean-geometry', 'inequality', 'trigonometry', 'triangles', 'algebra-precalculus']"
4625291,Fast way to check linear independence of matrix,"Say we suspect the columns of a matrix are independent and want to verify that fact quickly by hand.  What is the best way to do it? I'm currently studying MITx 18.033 where they recommend checking if the nullspace is $\{\mathbf 0\}$ by reducing into row echelon form, but how can I check quickly (ideally in my head for a small matrix such as below)? $$
\begin{pmatrix}
2 & 1 & 9 \\
3 & 2 & 11 \\
-1 & -3 & 8 \\
4 & 6 & -4
\end{pmatrix}
$$ I suspect columns are independent iff any 3 $2 \times 2$ submatrices with distinct rows and distinct columns are independent.  This allows us to prove a matrix is independent by finding those submatrices: $$
\begin{pmatrix}
2 & 1 \\
3 & 2
\end{pmatrix}, \ 
\begin{pmatrix}
2 & 11 \\
-3 & 8
\end{pmatrix}, \ 
\begin{pmatrix}
2 & 9 \\
-1 & 8
\end{pmatrix}
$$ Is this correct?  If so, why?  If not, how do I prove a matrix's columns are independent quickly?","['matrices', 'linear-independence', 'linear-algebra']"
4625316,Why doesn't Kallenberg include the condition $T_{0} = I$ in his definition of a Feller semigroup?,"I'm a bit confused about Kallenberg's definition of a Feller semigroup in Foundations of Modern Probability, 3rd ed. The setup: $S$ is a locally compact, separable metric space and $C_{0} = C_{0}(S)$ is the (Banach) space of continuous functions on $S$ that vanish at infinity, with the supremum norm. The definition: a semigroup of positive contraction operators $T_{t}$ on $C_{0}$ is a Feller semigroup if $T_{t}C_{0} \subseteq C_{0}$ $T_{t}f(x) \to f(x)$ as $t \to 0$ for all $f \in C_{0}$ , $x \in S.$ My question is: why is the condition $T_{0} = I$ (the identity operator) not included in the definition? I have seen other sources define a Feller semigroup with the condition $T_{0} = I$ included (for instance, Klenke's Probability Theory and Eberle's Markov processes ) and I can't tell what difference it makes. One possibility is that Kallenberg implicitly includes $T_{0} = I$ in his definition of a semigroup along with the usual condition $T_{s}T_{t} = T_{s+t}$ , which some authors in functional analysis seem to do. But this isn't laid out anywhere in the text (that I can find.) The reason this is troubling me is that I'm unsure what the lack of "" $T_{0} = I$ "" means for the transition kernels that correspond to $T_{t}$ . If $S$ is compact and $T_{t}$ is a conservative ( $T_{t}1 = 1$ ) Feller semigroup on $C_{0}$ , then there exists a unique semigroup of transition kernels $\mu_{t}$ on $S$ satisfying $$T_{t}f(x) = \int f(y)\,\mu_{t}(x,dy)\qquad \forall x \in S, f \in C_{0} \qquad (1)$$ If we interpret $\mu_{t}(x,B)$ as ""the probability of transitioning from $x$ to the measurable set $B$ in time $t$ ,"" then it seems like the only reasonable possibility is for $$\mu_{0}(x,B) = \begin{cases} 1 \quad & x \in B \\ 0 \quad & \text{otherwise} \end{cases}$$ i.e., $\mu_{0}(x,\cdot)$ is the point measure on $x$ . In this case, from (1) we have $T_{0}f(x) = f(x)$ for all $x, f$ , i.e., $T_{0} = I$ . But if $T_{0}$ needn't be the identity in general, how can we then make sense of $\mu_{0}$ ? Wouldn't it have to assign some nonzero probability to making a transition in $0$ time?","['markov-process', 'definition', 'functional-analysis', 'semigroup-of-operators', 'probability-theory']"
4625357,Evaluating the infinite series $\sum_{n=0}^{\infty} (1/n!)^m$,"What do we know about the series $$\sum_{n=0}^{\infty} \left(\frac{1}{n!} \right)^m$$ where $m$ is a positive constant integer? We know when $m = 1$ , we get the famous $e$ . But I became curious if the series in general for other exponents has been studied before, and whether the sums would converge to a well-known number. Also, does it have any applications in math other than the well-studied case of $m=1$ .","['number-theory', 'sequences-and-series']"
4625364,Is there an algorithm that can give you a column of a character table that doesn't compute the whole table?,"I've seen a handful of cases of character tables for small groups like S3 , S4 and the quaternion group . I have seen these tables constructed by coming up with some representations and exploiting orthogonality relations and the rule that the sum of $\overline{x}x$ for each element $x$ in the row equals the order of the group (which falls out of the characters of irreducible representations being an orthonormal basis for the characters of all representations). Computing the character table in this way basically proceeds row by row. I'm wondering whether there's a way to efficiently compute a single column at once without computing the rest of the table, i.e. all the traces associated with a particular conjugacy class. For example, the column associated with the conjugacy class containing just the identity element gives the dimensions of all the irreducible representations. I am mostly interested in small finite groups, but I have zero intuition for what kinds of algorithms exist or don't for computing columns of the character table, so I'll accept anything that works on an interesting subclass of groups.","['group-theory', 'abstract-algebra', 'representation-theory', 'characters']"
4625401,Using a Continuous Time Markov Chain for Discrete Times,"As I understand, there some major differences between Discrete Time Markov Chains compared to Continuous Time Markov Chains. For example: Discrete Time Markov Chain: Characterized by a constant transition probability matrix ""P"" Continuous Time Markov Chain: Characterized by a time dependent transition probability matrix ""P(t)"" and a constant infinitesimal generator matrix ""Q"". The Continuous Time Markov Chain is based on the Exponential Distribution and thereby must obey the Memoryless Property. Suppose I collect data at discrete time points (e.g. the health of a patient at the start of every month, e.g. ""healthy"", ""sick"", ""very sick"") - technically speaking, there is nothing stopping me from ""tricking"" my computer and saying that these time measurements are actually continuous , and then estimating the P(t) and Q matrix - even though these concepts (i.e. P(t) and Q) are not defined in a Discrete Time Markov Chain. Based on this set-up (i.e. creating a Continuous Time Markov Chain using fundamentally Discrete data), I anticipate that the P(t) matrix will be likely be ""constant"" between time intervals (i.e. staircase/stepwise function) - e.g. the probability of transitioning between two states remains identical from the start to the end of any given month. My Question: Is what I have described (e.g. piecewise approximation) a somewhat valid approach insofar as treating discrete times as continuous when using Markov Chains? Or is this fundamentally incorrect? The reason I am interested in specifically using a Continuous Time Markov Chain (even if there is discrete data) is that this allows me in theory to obtain time-dependent transition probabilities (even if they are constant between time intervals) as opposed to constant transition probabilities - and these time-dependent transition probabilities can be useful in different real-world applications. Thanks! Note: I understand the risks in using such an approach - in real life, the health of a medical patient might deteriorate or improve within a given month, but the Continuous Time Markov Chain created on discrete data would not be able to pick up on this. However, all things being equal - this kind of information would fundamentally not be captured within the discrete data and the Discrete Time Markov Chain would fundamentally not be able to pick up on this. Therefore, I was interested in learning about whether or not analyzing discrete times using a Continuous Time Markov Chain might provide some benefits over a Discrete Time Markov Chain while likely not incurring any additional disadvantages (i.e. best case scenario - slightly better; worse case scenario - roughly equivalent).","['statistics', 'markov-chains', 'probability']"
4625434,"If everyone in the world chooses a random real number between $0$ and $1$ one by one, how many times will a world record for lowest number be set?","I have a random number generator that generates a random real number between $0$ and $1$ at the press of a button. All 8 billion people in the world line up for a try. Occasionally a world record for lowest number is set. (The first person automatically sets the first world record.) Approximately how many times will a world record be set? Let $n=8\times 10^9$ , the world's population. Approach 1 Let $f(n,k)=$ the number of permutations of the first $n$ positive integers with exactly $k$ numbers that are less than all previous numbers. The first number in any permutation is considered to be less than all previous numbers (in the sense that it sets a new low). The probability that exactly $k$ world records will be set, is $\frac{f(n,k)}{n!}$ . Then the expected number of world records is $\sum\limits_{k=1}^n k\left(\frac{f(n,k)}{n!}\right)$ . But I do not know how to express $f(n,k)$ so that I can calculate the expectation. Maybe there is a smart way to express $f(n,k)$ , like stars and bars . Approach 2 If I roll a six-sided die $m$ times, the expectation for the number of sixes is $m\times \text{(probability that the $k$th throw is a six)}=\frac{m}{6}$ . Similarly, perhaps the expectation for the number of world records can be approximated as $n\times \text{(probability that the $k$th person sets a world record)}=n\times\frac1k$ . But the problem is that $\frac1k$ is not a constant. However, suppose we use the average value of $\frac1k$ , which is $\frac1n \sum\limits_{k=1}^n \frac1k \approx \frac1n \int_0^n \frac1x dx=\frac{\ln{n}}{n}$ . Then the expectation would be $n\times \frac{\ln{n}}{n}=\ln{n}\approx 22.8$ . I tested this approach on excel, and it seems to yield reasonable results. But it seems rather hand-wavy. Context This question was inspired by a question about the sine of integers .","['expected-value', 'combinatorics', 'approximation', 'probability']"
4625438,Is it true the set of Lebesgue-measurable functions which are non-integrable are prevalent in the set of measurable functions?,"Suppose $A$ is an arbitrary set in Caratheodory extension, where $A\subseteq\mathbb{R}^{n}$ and $f$ is an arbitrary, lebesgue measurable function where $f:A\to\mathbb{R}$ According to here and here , “almost all” measurable functions in a function space can be defined without a measure on the set of measurable functions. Such a set of functions are known as prevelant set in a function space. According to my hypothesis, the set of Lebesgue-measurable functions that are non-integrable are prevelant in the set of measurable functions. One statistician, of whom I messaged, stated the following: We follow the argument presented in example 3.6 of this paper ,
take $X:=L^{0}(A)$ (measurable functions over $A$ ), let $P$ denote the
one-dimensional sub-space of $A$ consisting of constant functions
(assuming the Lebesgue measure on $A$ ) and let $F:=L^{0}(A)\setminus
 L^{1}(A)$ (measurable functions over $A$ with no finite integral). Let $\lambda_{P}$ denotes the Lebesgue measure over $P$ , for any fixed $f\in F$ : $$\lambda_{P}\left(\left\{\alpha\in\mathbb{R}\left| \int_{A}\left(f+\alpha\right) d\mu<\infty\right.\right\}\right)=0 $$ Meaning $P$ is a one-dimensional probe of $f$ , so $f$ is a 1-prevalent set. Is this correct? Does this prove my hypothesis? For what other notions of “size” (provided in this answer ) are “almost all” Lebesgue-measurable function non-integrable? As a final note, here is my (informal) attempt to answer this question: Note that almost all functions can be desribed as a set of
pseudo-random points that are non-uniformly distributed in a sub-space
of $\mathbb{R}^{2}$ . (To visualize, see this link ). Now assume we have that same function but it is defined on a lebesgue
measurable set (e.g. defined on $[0,1]$ ). If we partition the
functions’ domain, the subset of points in that function may have
the largest pre-image in a partition that is non-Lebesgue measurable,
making the function non-integrable. The chance that a random set is
Lebesgue measurable is extremely small (see this link ).
Therefore, using the previous paragraph, ""almost all"" functions or
""most"" functions are non-integrable","['measure-theory', 'functions', 'functional-analysis']"
4625477,How many permutations of the first $n$ positive integers have exactly $k$ numbers that are less than all previous numbers?,"Let $f(n,k)=$ the number of permutations of the first $n$ positive integers with exactly $k$ numbers that are less than all previous numbers. The first number in any permutation is considered to be less than all previous numbers (in the sense that it sets a new low). Is there an easy way to calculate $f(n,k)$ in general? For example, we can show that $f(4,3)=6$ by listing the permutations. (In each permutation, the numbers that are less than all previous numbers are in red.) $\color{red}3,\color{red}2,\color{red}1,4$ $\color{red}3,\color{red}2,4,\color{red}1$ $\color{red}3,4,\color{red}2,\color{red}1$ $\color{red}4,\color{red}2,\color{red}1,3$ $\color{red}4,\color{red}2,3,\color{red}1$ $\color{red}4,\color{red}3,\color{red}1,2$ I'm looking for a smart way to calculate $f(n,k)$ , maybe like stars and bars . Context: This is related to ""Approach 1"" in my question about world records . But I find the question here interesting by itself.","['combinatorics', 'stirling-numbers']"
4625507,"Does there exist two linearly independent functions $u$ in $\mathbb{R}^{2}$ satsfying $\bar{\partial}^{2}u(x,y) + A(x,y)u(x,y) = 0$?","Does there exist two linearly independent functions $u$ in $\mathbb{R}^{2}$ satsfying $$\frac{\partial^{2}}{\partial \bar{z}^{2}}u(x,y) + A(x,y)u(x,y) = 0$$ where $A\in C^{\infty}(\mathbb{R}^{2})$ ? We define $$\frac{\partial}{\partial\bar{z}}u(x,y) ={\frac {1}{2}}\left({\frac {\partial }{\partial x}u(x,y)} + i{\frac {\partial }{\partial y}}u(x,y)\right).$$ We know that if $A$ is a antiholomorphic function, then our equation has two independent antiholomorphic solutions $u$ .","['riemann-surfaces', 'ordinary-differential-equations', 'complex-geometry', 'complex-analysis', 'partial-differential-equations']"
4625547,Notation for a partially evaluated $n$ dimensional function,"I can’t seem to find the right phrasing to google this question. Suppose I have function $$f:\mathbb{R}\times\mathbb{R}\to\mathbb{R}$$ you might denote the evaluation of $f$ at point $(x,y)$ as $f(x,y)$ . For any $x_0\in\mathbb{R}$ , you can make a function $g:\mathbb{R}\to\mathbb{R}, y\mapsto f(x_0,y)$ . Is there any standard notation for this function $g$ (which refers to $f$ and $x_0$ in some way)? (Note: I want to use this notation (above) to turn a function with $n$ variables mapping into a ( $n$ - $m$ ) dimensional codomain, the function $f$ above is just an example)","['notation', 'multivariable-calculus']"
4625599,Limit when $E(X^2)=\infty$.,Let $X$ be a random variable taking values in $\mathbb{R}$ such that $E(X^2)=\infty$ . I want to show that $$\lim_{M\to\infty} \frac{\mathbb{E}(X\mathbf{1}_{|X|\leq M})^2}{\mathbb{E}(X^2\mathbf{1}_{|X|\leq M})} = 0$$ The monotone convergence theorem suggests that as $M\to \infty$ the denominator diverges. But this is not enough because the numerator might also diverge. Another idea that came to mind was to write $E(X\mathbf{1}_{|X|\leq M})^2 \leq M^2$ and prove that $$\lim_{M \to \infty} \frac{1}{M^2}\int_{|X|\leq M}X^2dP=\infty$$ which unfortunately doesn't hold. Any help/hint is appreciated!,"['expected-value', 'measure-theory', 'probability-theory']"
4625674,Difficulty solving $4x^2y^3y''=x^2-y^4$,"Solve $4x^2y^3y''=x^2-y^4.$ I was trying to reduce the order of this nonlinear ODE or to transform it into a linear ODE, but I couldn't come up with a suitable substitution. I considered \begin{aligned}(y^4)''&=(4y^3y')'\\&=12y^2(y')^2+4y^3y''\\\implies 4y^3y''&=(y^4)''-12y^2(y')^2\\\implies 4x^2y^3y''&=x^2(y^4)''-12x^2y^2(y')^2\\&= x^2(y^4)''-3(x^2\cdot 4y^2\cdot (y')^2)\\&=x^2(y^4)''-3(x\cdot 2yy')^2\\&=x^2(y^4)''-3(x(y^2)')^2\end{aligned} on the LHS. If I took $u:=y^2,$ I would get $$x^2(u^2)''-3(xu')^2=x^2-u^2,$$ but that situation doesn't seem any better than the initial one. Is it possible to transform the given ODE into a linear one (not necessarily with constant coefficients)? I assume it should be possible as I found it in old materials which don't cover anything beyond linear equations and Riccati's equation, which we transformed into linear. However, I would appreciate any working method suggested.",['ordinary-differential-equations']
4625706,Question in the proof of Lusin's theorem.,"I want to prove Lusin's theorem. Let $E\subset\mathbb R$ be a measurable set and $f:E\to\mathbb R$ be a measurable function. Then, for each $\epsilon$ , there exists $F\subset E$ s.t. $F$ is closed about relative topology from $E$ , $m^\ast(E\setminus F)<\epsilon$ , $f|_F$ is continuous. ( $m^\ast$ is Lebesgue outer measure.) I've already understand the case $m^\ast(E)<\infty$ , and I'm trying general case. Hint is given : Consider $E_k:=E\cap[-k,k]$ . Let $E_k:=E\cap[-k,k]$ . For each $k\in\mathbb N$ , $m^\ast(E_k)<\infty$ and $f|_{E_k}$ is measurable thus there is $F_k\subset E_k$ s.t. $F_k$ is closed about relative topology from $E_k$ , $m^\ast(E_k\setminus F_k)<\epsilon/2^k$ , $f|_{F_k}$ is continuous. $F:=\cup_{k=1}^\infty F_k$ , $E:=\cup_{k=1}^\infty E_k$ . Then $F\subset E$ . Since $E\setminus F\subset\cup_{k=1}^\infty (E_k\setminus F_k)$ , I get $m^\ast(E\setminus F)\leqq\sum_k m^\ast(E_k\setminus F_k)\leqq\epsilon.$ Maybe what I did so far is correct, but this is not sufficient yet : I have to check (i) $F$ is closed about relative topology from $E$ . (ii) $f|_F$ is continuous. For (i), each $F_k$ is closed in $E_k$ but I don't know why $F=\cup_k F_k$ is closed in $E$ . $F$ has to be written as $F=E\cap C$ , where $C$ in closed in $\mathbb R$ . I haven't found such $C$ . For (ii), each $f|_{F_k}$ is continuous but I don't think this guarantees the continuity of $f|_F$ . I think that ""each $f|_{A_k}$ is continuous $\Rightarrow$ $f|_{\cup_{k=1}^\infty}A_k$ is continuous"" doesn't generally hold. (Maybe, I have to show $x_j\to x$ $\Rightarrow$ $f|_F(x_j)\to f|_F(x)$ , but I don't know how.) Postscript Hint says $E_k:=E\cap[-k,k]$ but perhaps $E_k$ has to be defined s.t. $\{E_k\}$ is disjoint : e.g., $E_k=E\cap(k,k+1]$ or, $E_1=E\cap[-1,1]$ , $E_k=E\cap([-k,k]\setminus[1-k,k-1])$ for $k\geqq 2.$","['lebesgue-integral', 'analysis']"
4625732,Solving non-linear fractional differential equation,"I want to solve the non-linear Caputo-type fractional equation of the form ( $0 < \alpha < 1$ ) $$ ^cD^{\alpha}_0 f(t) = af(t)^4 + bf(t) + c$$ I have found, the equation $^cD^{\alpha}_0 f(t) = af(t) + g(t)$ is the Cauchy-type equation and is solved. But, setting $g(t) = af(t)^4 + c$ doesn't work. Just for clarification, I want something like $f(t) =$ something in $t$ . (Numerical solutions / Hints is also fine).Thanks in advance! PS: If Riemann-Liouville or any other fractional derivatives will make it easier, that's also fine. PS2: I am a newbie to fractional calculus, so I have no clue other than this book. Ref: Kilbas, Anatoly A.; Srivastava, Hari M.; Trujillo, Juan J. , Theory and applications of fractional differential equations, North-Holland Mathematics Studies 204. Amsterdam: Elsevier (ISBN 0-444-51832-0/hbk). xv, 523 p. (2006). ZBL1092.45003 .","['fractional-differential-equations', 'fractional-calculus', 'ordinary-differential-equations']"
4625737,Why does $|\hat{\mu}(x)|\leq|x|^{-s/2} \implies \int_{\mathbb{R}^n}|\hat{\mu}(x)|^2|x|^{s-n}dx<\infty$ for the Fourier transform of a measure $\mu$,"( See edit ) I have seen the claim $|\hat{\mu}(x)|\leq|x|^{-s/2} \implies \int_{\mathbb{R}^n}|\hat{\mu}(x)|^2|x|^{s-n}dx<\infty$ for the Fourier transform $\hat{\mu}(\xi)\equiv \int_{\mathbb{R}^n}e^{-2\pi i x\cdot \xi}d\mu(x)$ of a finite Borel measure $\mu$ on $\mathbb{R}^n$ in quite a few places without a proof and I am honestly quite confused by it. Namely, what if the origin belongs to the support of $\hat{\mu}$ ? In that case there is no way that the integral $\int_{\mathbb{R}^n}|\hat{\mu}(x)|^2|x|^{s-n}dx$ can be finite, at least with the crude approximation $$\int_{\mathbb{R}^n}|\hat{\mu}(x)|^2|x|^{s-n}dx \leq \int_{\mathbb{R}^n}|x|^{-n}dx$$ Is there some good-to-know trick that one can use with the assumed inequality to conclude that $\int_{\mathbb{R}^n}|\hat{\mu}(x)|^2|x|^{s-n}dx < \infty$ ? Thanks! Edit: My current understanding is that $s$ can be negative. Moreover $\mu$ is assumed to have a compact support, although I am not sure how that is helpful in this case.","['measure-theory', 'fourier-analysis', 'borel-measures', 'harmonic-analysis', 'real-analysis']"
4625754,"If an event can happen with probability $p$, but I don't see it in $n$ tries, what is $P(p < p_0)$?","How can I calculate the following: Suppose I have an event (in my case, a computer bug) that can happen with unknown probability $p$ . I run $n$ experiments and don't see the bug. What is the probability that $p$ is below some fixed value $p_0$ ? (I am not well versed in this type of statistical analysis. From a bit of research I gather this is a Bernoulli process, and I saw some formulas for determining things like the probability of $k$ successes in $n$ runs, but not exactly what I am interested in. This question occurred to me as I am considering how often one should replicate a bug to make sure it is a consistent bug and not a sporadic bug. Or how sure you can be a bug is fixed if you don't see it anymore.)","['statistics', 'probability', 'bernoulli-distribution']"
4625763,Representation of the C*-algebra of a Kähler manifold,"This question is inspired from mathematics of quantum mechanics. In quantum mechanics, we start with a Kähler manifold $\mathcal{M}$ (which is $\mathbb{CP}^n$ ), with the symplectic form $\omega$ and metric $g$ . We have a correspondence between the geometric data and algebraic data as follows. The symplectic form induces a Poisson algebra on the functions $x,y$ on the manifold $$[x,y] := \omega (dx,dy)\,,$$ and the metric induces a Jordan algebra $$x\cdot y := g(dx,dy)+xy\,.$$ Or, we can treat the two structures at the same time and say that the Kähler structure (metric+symplectic) induces a  C*-algebra (Poisson+Jordan). What seems remarkable to me is the representation of the C*-algebra. The representation is a Hilbert space $\mathcal{H}$ by GNS construction. Magically, its projective space turns out to be the manifold we started with: $$\mathbb{P}\mathcal{H} = \mathcal{M}.$$ Somehow, we go back from algebraic data to geometric data by an application of representation theory. Why is this true for $\mathcal M=\mathbb{CP}^n$ ? Is it true for any Kähler manifold $\mathcal{M}$ ? Answers and references are both appreciated.","['c-star-algebras', 'kahler-manifolds', 'representation-theory', 'functional-analysis', 'mathematical-physics']"
4625799,Integration cannot be replaced by discrete sum [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . The community reviewed whether to reopen this question last year and left it closed: Original close reason(s) were not resolved Improve this question Let $\{x_i\}_{i=1}^\infty$ be a dense subset of $[0,1]$ . Let $(c_n)_n$ be a sequence in $\mathbb{R}$ such that $\sum_{n=1}^\infty c_n= 1$ Question: Is it possible to have $$\int_0^1 f(x)dx= \sum\limits_{i=1}^\infty f(x_i)c_i\ , \ \forall f\in C([0,1]) \quad ?$$ Clearly not, but how to prove it?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
4625802,Maximal domain of unbounded linear differential operator,"Let's consider the following (unbounded) linear Operator. (So called Transport-Operator in some context.) $$ \mathrm{T}: \mathcal{H} \supset \mathcal{D}(\mathrm{T}) \to \mathcal{H} , f \mapsto \mathrm{T}f:= v \cdot \partial_xf(x,v), $$ where $\mathcal{H}$ is a weighted Hilbert-space defined by $ \mathcal{H} := L^2(\mathbb{T}^1 \times \mathbb{R}, M(v)^{-1}\mathrm{d}x\mathrm{d}v),$ $ M(v) := \frac{1}{\sqrt{2 \pi}}\mathrm{e}^{-\frac{1}{2}v^2},$ and the dot product on $\mathcal{H}$ is defined by $$ (f,g)_{\mathcal{H}} := \int_{\mathrm{T}^1 \times \mathrm{R}}\frac{f(x,v)g(x,v)}{M(v)} \, \, \mathrm{d}x\mathrm{d}v,$$ $\mathbb{T}^1$ denotes the one-dimensional Torus. It is easy to verify that $\mathrm{T}$ is formally a skew-symmetric Operator w.r.t. $(\cdot, \cdot)_{\mathcal{H}}$ . $\mathcal{D}(\mathrm{T})$ denotes the maximal domain of $\mathrm{T}$ which should be unique, dense and $\mathrm{T}$ should be a closed operator on $\mathcal{D}(\mathrm{T})$ . $\mathcal{R}(\mathrm{T})$ denotes the range of $\mathrm{T}.$ My question is now how can I define and determine $\mathcal{D}(\mathrm{T})$ and how can I easily describe/characterize $\mathcal{D}(\mathrm{T})$ explicitly? The same question for $\mathcal{R}(\mathrm{T})$ .
The derivative $\partial_x$ is to be understood in the weak-sense, so for all $v \in \mathbb{R}$ we will need $f(x,v) \in H^1(\mathbb{T}^1)$ (Sobolev-space) for example.
My conjecture would be $$ \mathcal{D}(\mathrm{T}) = \left \{f \in \mathcal{H} : \int_{\mathbb{T}^1 \times \mathbb{R}} \frac{v^2(\partial_xf(x,v))^2}{M(v)}\, \, \mathrm{d}x\mathrm{d}v < + \infty \right\} = \{f \in \mathcal{H} : \mathrm{T}f \in \mathcal{H} \},$$ so that $(\mathrm{T}f, g)_{\mathcal{H}}$ for arbitrary $g \in \mathcal{H}$ is at least well-defined.
Would be grateful for any help and ideas! EDIT: My attempt to show closedness of $\mathrm{T}$ on $\mathcal{D}(\mathrm{T}).$ Set $\Omega := \mathbb{T}^1 \times \mathbb{R}$ and $\mathrm{d}\mu := M(v)^{-1}\mathrm{d}x\mathrm{d}v.$ Let $(f_n)_{n \in \mathbb{N}}$ a sequence in $\mathcal{D}(\mathrm{T})$ with $f_n \rightarrow f$ in $\mathcal{H}$ and $\mathrm{T}f_n \rightarrow g$ in $\mathcal{H}$ . We have to show:
1.) $f \in \mathcal{D}(\mathrm{T})$ and
2.) $\mathrm{T}f = g. $ We can proceed as follows. Strong convergence in $\mathcal{H}=L^2(\Omega, \mathrm{d}\mu)$ implies weak convergence so we have: $$ \int_{\Omega} f_n(x,v) \phi(x,v) \, \mathrm{d}\mu \rightarrow \int_{\Omega} f(x,v) \phi(x,v) \, \mathrm{d}\mu$$ for all $ \phi \in C_c^{\infty}(\Omega),$ and $$ -\int_{\Omega} v \cdot f_n(x,v) \partial_x\phi(x,v) \, \mathrm{d}\mu = \int_{\Omega} v \cdot \partial_xf_n(x,v) \phi(x,v) \, \mathrm{d}\mu \rightarrow \int_{\Omega} g(x,v) \phi(x,v) \, \mathrm{d}\mu$$ for all $ \phi \in C_c^{\infty}(\Omega).$ Due to $$  -\int_{\Omega} v \cdot f_n(x,v) \partial_x\phi(x,v) \, \mathrm{d}\mu \rightarrow -\int_{\Omega} v \cdot f(x,v) \partial_x\phi(x,v) \, \mathrm{d}\mu$$ we have $$ -\int_{\Omega} v \cdot f(x,v) \partial_x\phi(x,v) \, \mathrm{d}\mu = \int_{\Omega} g(x,v) \phi(x,v) \, \mathrm{d}\mu,$$ so we can conclude $ g = v \cdot \partial_xf = \mathrm{T}f. $ Would that be correct?","['differential-operators', 'operator-theory', 'function-spaces', 'functional-analysis', 'unbounded-operators']"
4625858,Solving the Double Angle Tangent Formula for $\tan x$,"The double angle formula for $\tan(x)$ is as follows: $$\tan(2x) = \frac{2\tan(x)}{1-\tan^2 (x)}$$ I wanted to see if I could solve this equation for $\tan(x)$ —I figured that I could manipulate this equation to put it in the form of a quadratic equation**. $$\tan(2x)(\tan x)^2 + 2(\tan x) - \tan(2x) = 0 
\\ \implies \tan(x) = \frac{-2 \pm \sqrt{4 - 4(\tan(2x))(-\tan(2x))}}{2\tan(2x)}
$$ Conveniently, the expression for $\tan(x)$ simplifies to $$ \tan(x) = \frac{-1 \pm \sec(2x)}{\tan(2x)}$$ Before calling it a day, I checked to see if any of these branches of the solution were extraneous. As it turns out, the negative branch is extraneous, and is actually equal to $\tan{\left( x - \frac{\pi}{2} \right)} = -\cot(x)$ . This is where I’m confused. Both branches of the expression are valid solutions to both the quadratic equation and the original double angle equation. So why isn’t $\tan (x)$ equal to both of them? I know that would be ridiculous, but I can’t see where this phase shift by $\frac{\pi}{2}$ comes from. **I doubted at first whether the quadratic equation applies in a case where the coefficients (a, b, and c) are functions of the equation's independent variable, in this case $x$ . However, I decided to continue anyway, since the alternative ""safer"" way to solve this would be to complete the square, but that's essentially equivalent to using the quadratic formula anyway.","['algebra-precalculus', 'trigonometry']"
4625860,Show that $\det(A)=2^{p}$,"We have a $(n×n)$ -matrix $A$ with complex entries such that $\,A^{2}=3A-2I$ . $~$ Show that there exists $p\in\{0,1,2,...,n\}$ such that $\det(A)=2^{p}$ .
I don't know if my proof is good. I took the polynomial $G(x)=x^{2}-3x+2=(x-1)(x-2)$ . So $A$ is a solution for $G(x)$ . The $A$ 's minimal polynomial divides $G(x)$ so the minimal polynomial can be $(x-1)$ or $(x-2)$ .
So we will get that $\det(A)=2^{0}$ or $\det(A)=2^{n}$ .","['matrices', 'minimal-polynomials', 'characteristic-polynomial', 'eigenvalues-eigenvectors']"
4625877,Which open sets can be written down as an open ball with respect to some metric?,"Let $(X, \tau)$ be a metrizable topological space. Any metric $d$ on $X$ generates a topology $\tau_d$ on it. Call $d$ admissible if $\tau  = \tau_d$ . Let $\mathcal{D}$ be the set of admissible metrics for $(X, \tau)$ , and denote $B_d(x, r) = \{y : d(x, y) < r\}$ . Consider the set $$\mathcal{U} = \{B_d(x, r) : d\in\mathcal{D}, x\in X, r> 0\}.$$ In other words, $\mathcal{U}$ is the family of those open sets,
which are open balls for some admissible metric on $X$ . Can we characterize elements of $\mathcal{U}$ topologically (i.e. not involving the concept of a metric)? This problem didn't come up in anything I'm working with, I'm asking it out of curiosity.","['general-topology', 'metric-spaces']"
4625903,Prove $(A \cap B) \cap (A \setminus B) = \emptyset$,"I'm asked to prove that $(A \cap B) \cap (A \setminus B) = \emptyset$ .  This is equivalent to showing $(A \cap B) \cap (A \setminus B) \subset \emptyset$ and $\emptyset \subset  (A \cap B) \cap (A \setminus B)$ . Since the empty set is a subset of every set, then all we need to show is $(A \cap B) \cap (A \setminus B) \subset \emptyset$ . Let $x \in (A \cap B) \cap (A \setminus B).$ This is equivalent to $$ \iff x \in A \land x\in B \land x \in A \land x \not\in B $$ $$ \iff x \in A \land x\in B \land x \not\in B $$ Since logical statements associate, then $$ \iff x \in A \land (x\in B \land x \not\in B)$$ which is always false by the law of excluded middle ( $p \land \neg p$ is always false). Since no such element can satisfy this statement, and since $x$ was arbitrary, then no such $x$ exists and hence the set is empty. How is my logic there?  I'm mostly concerned with the bit about law of excluded middle and less about the beginning and middle.","['elementary-set-theory', 'solution-verification']"
4625939,Evaluating $\int_0^{\frac{\pi}{2}}x^2 \cot x\ln(1-\sin x)\mathrm{d}x$,I was able to find $$\int_0^{\frac{\pi}{2}}x^2 \cot x\ln(1-\sin x)\mathrm{d}x=-\frac14\sum_{n=1}^\infty\frac{4^n}{{2n\choose n}}\frac{H_{2n}}{n^3}$$ $$=5\operatorname{Li}_4\left(\frac12\right)-\frac{65}{32}\zeta(4)-2\ln^2(2)\zeta(2)+\frac5{24}\ln^4(2)$$ by converting it to the sum above then evaluating this sum but many integrals and sums were involved in the calculations. Do you have a different idea to find this integral or its sum?,"['integration', 'special-functions', 'alternative-proof', 'harmonic-numbers', 'sequences-and-series']"
4625992,"How many different sets of positive integers $(a, b, c, d)$ are there such that $a \lt b \lt c \lt d$ and $a + b + c + d = 41$?","Is there a general formula which I can use to calculate this and if it's with proof or reasoning would be great as well. Even if you could please solve this $4$ -variable ordered set of positive integers case would be a great help indeed. Sorry I am weak in this topic. Thanks Robert for your pointers in modifying it to $(4\times1) + (3\times2) + (2\times3) + (1\times4) = 41$ , where there are no additional constraints on $x_i$ To solve this must do it manually by cases or is there a short way of calculating it using ""Bars and Stars"" (of which I am aware of the method)? The answer given is $321$ . But when I calculate it like this it doesn't seem to match the answer:
imagine the new given equation as containing $(4+3+2+1) = 10$$x_i$ 's individually so as equivalent to finding the number of positive integer solutions for $b_1 + b_2 + ... + b_{10} = 41$ which is $C(41 - 1, 10 - 1) = C(40, 9)$ . Now since we have $4x_1$ 's that are identical, we need to divide this by $4!$ (for its permutations of ""overcounting"") and similarly divide next by $3!$ (for $3x_2$ 's that are equal), and $2!$ for the $2x_3$ 's that are equal. How is my approach wrong here? anybody, please help me? Thanks again As per Robert's transformation with Anil's suggestion of the generating functions method, here's my work on it: I actually used an online calculator called ""Sage Math"" to do this via this website: https://www.whitman.edu/mathematics/cgt_online/book/section03.01.html With these modifications $f(x)=(1-x^4)^{-1}\times(1-x^3)^{-1}\times(1-x^2)^{-1}\times(1-x)^{-1}$ show(taylor( $f,x,0,33$ )) And have gotten the verification  that $321$ is indeed the answer after some tedious algebra (please note that since we are looking for the coefficient of $x^41$ in the expansion Anil wrote which has $x^(4+3+2+1) = x^10$ factored outside meaning in the remaining Newton's Binomial Expansions with negative exponent, we only need to determine the coefficient of $x^(41-10)= x^31$ in it which is "" $321$ "" as the answer suggested (please see image below):","['elementary-number-theory', 'combinatorics', 'linear-diophantine-equations', 'combinatorial-proofs']"
4625998,Continuity of function of two variable by Epsilon-Delta technique.,"Given a function $f(x,y)= \frac{x^3y^2+x}{x^2+y^2}$ when $(x,y) \neq (0,0)$ . Show that the function $f(x,y)$ is continuous at $(1,1)$ by Epsilon-Delta definition. I can show it easily by the fraction of two continuous functions is continuous. But I need to prove it directly by Epsilon-Delta technique. I'm not able to think, how to do it. Please give some hint! Effort: For any given $\epsilon>0$ , we have $|f(x,y)-1|= |\frac{x^3y^2+x}{x^2+y^2}-1|=|\frac{x^3y^2+x-x^2-y^2}{x^2+y^2}|$ Now, what's next?","['multivariable-calculus', 'epsilon-delta']"
4626049,How can I prove $|A_1 \cup A_2 \cup ... \cup A_n|=|A_1|+|A_2|+...+|A_n|$ using induction?,"For pairwise disjoint sets $A_1,A_2,...,A_n$ how can I prove that: $|A_1 \cup A_2 \cup ... \cup A_n|=|A_1|+|A_2|+...+|A_n|$ using induction and the 2-set addition rule? 2-set addition rule: $|A_i \cup A_j|=|A_i| + |A_j|$ if $A_i$ and $A_j$ are disjoint. I think I should suppose the proposition holds for $P(k)$ for $k\ge 2$ , that is: $$|A_1 \cup A_2 \cup ... \cup A_k|=|A_1|+|A_2|+...+|A_k|$$ Our base case is $P(2)$ : $$P(2)=|A_1 \cup A_2|=|A_1|+|A_2|$$ By the 2-set addition principle ( $A_1$ and $A_2$ are disjoint). Now $$P(k+1) = |A_1 \cup A_2 \cup ... \cup A_k \cup A_{k+1}|$$ And $A_{k+1}$ is disjoint with $(A_1 \cup A_2 \cup ... \cup A_k)$ since $A_{k+1} \cap (A_1 \cup A_2 \cup ... \cup A_k) = (A_{k+1} \cap A_1) \cup (A_{k+1} \cap A_2) \cup...\cup (A_{k+1} \cap A_k) = \emptyset$ So by the 2-set addition rule: $$|A_1 \cup A_2 \cup ... \cup A_k| + |A_{k+1}|=|A_1|+|A_2|+...+|A_k|+|A_{k+1}|$$ Which is what the proposition would conclude for $P(k+1)$ so the original proposition must be true. My question: is there anything wrong with this proof attempt?","['induction', 'cardinals', 'combinatorics']"
4626100,Can these two decreasing and concave functions intersect at more than two points?,"Functions $f(x)$ and $g(x)$ are both continuously differentiable in $x$ for $x\in[0,c]$ . We know both functions are decreasing and concave in $x$ anywhere on $[0,c]$ . I am curious if there is a way to show that if these two functions are to intersect, there can be a maximum of two points of intersection. I did some plotting, and this seems to be true, but only for those functions I plotted, but I would like to show this is true in general or at least have a counter-example. So far, I couldn't come up with two functions as a counter-example. And for the proof, I don't know where to begin! Any suggestions would be very appreciated.","['continuity', 'calculus', 'functions', 'real-analysis']"
4626145,Dice throwing and calculating different probabilities,"Question: In a dice game that is played during the Chinese Moon Festival, participants take turn throwing six dice into a large bowl. If certain combinations show up, the person gets
a prize. Below, we describe the important combinations and the prize assigned to each
combination: six $1$ ’s or six $4$ ’s show up (1st prize) exactly five of any number show up (2nd prize) the numbers $1, 2,3, 4,5,6$ show up (3rd prize) exactly three $4$ ’s show up (4th prize) Compute the probability that each of the combinations described above appears. Suppose there are ten people playing this game and in each round, everyone gets a
turn. By sheer luck, you have the first turn at each round. Now, there is only one first prize
so the first person who throws the right combination wins the prize. What is the probability
that you'll win it in the fifth round? Solution: Probabilities: $\binom{6}{6} \times (\frac{1}{6})^6 \times (1-\frac{1}{6})^{6-6} + \binom{6}{6} \times (\frac{1}{6})^6 \times (1-\frac{1}{6})^{6-6} = \frac{2}{6^6}$ (1st prize) $6 \times \Big(\binom{6}{5} \times (\frac{1}{6})^5 \times (1-\frac{1}{6})^{6-5}\Big)$ (2nd prize) $6! \times  (\frac{1}{6})^6 $ (3rd prize) $\binom{6}{3} \times (\frac{1}{6})^3 \times (1-\frac{1}{6})^{6-3}$ (4th prize) To win at the beginning of the $5$ th round of $10$ people no one should win the 1st prize in $40$ consecutive dice throwing and I should win in the $41$ th dice throwing. $\Big(1- \frac{2}{6^6}\Big)^{40} \times \Big(\frac{2}{6^6}\Big)$ Am I on the right track?","['solution-verification', 'discrete-mathematics', 'probability']"
4626188,How to show that a function is continuous?,"Let $u \in W^{s,p}$ (the space $W^{s,p}$ is the frational sobolev space), $\mu \in \mathbb{R}^{+}$ and $\varepsilon>0$ . The function $f$ is defined by $$H_{\varepsilon}(u-\mu) = \left\{\begin{array}{ll} 1 & \quad \mbox{if }\ u > \mu + \varepsilon, \\[0.1cm]
			\frac{u-\mu}{\varepsilon -\mu} & \quad \mbox{if }\ \mu \leq u\leq \mu + \varepsilon,\\[0.1cm]
			0 & \quad \mbox{if }\ u < \mu,
		\end{array}
		\right.$$ Is this function continuous? and if yes how to prove it?","['continuity', 'functions', 'analysis', 'real-analysis']"
4626190,What does $\det f'(a)=0$ tell us about the function $f$ at $a$?,"Let $f:\mathbb{R}^n\to\mathbb{R}^n$ be a function differentiable at $a\in \mathbb{R}^n$ .  The property that $\det f'(a)\ne 0$ is relevant in multiple theorems e.g. the Inverse Function Theorem, yet I have trouble building an intuition of what such property tells us about $f$ at $a$ . There is, of course, the geometric interpretation of determinants as signed volume, and thus  one could say that "" $\det f'(a)= 0$ tells us that $f'(a)$ i.e. the (best) linear approximation of the function $$\mathbb{R}^n\to\mathbb{R}^n : h \mapsto \Big[f(a+h)-f(a)\Big]$$ 'compresses' any $n$ -dimensional shape into a shape of lower dimension."" By itself, such explanation does not give me much of a geometric intuition of the behavior of $f$ around $a$ , and thus I was wondering if anyone could add something to the story. $$\textbf{What does $\det f'(a)=0$ tell us about the function $f$ at $a$?}$$ $$\textbf{Is there another geometric intuition corresponding to $\det f'(a)=0$?}$$","['determinant', 'real-analysis', 'multivariable-calculus', 'derivatives', 'soft-question']"
4626191,Almost surely equal random variables imply conditional expectations have a common functional form?,"Question : Let $Z$ be a real-valued random variable, $X,Y$ be random elements. Suppose $X=Y$ almost surely. Then does there exist a function $h$ such that $E(Z|X)=h(X)=h(Y)=E(Z|Y)$ almost surely or $E(Z|X)$ is a version of $E(Z|Y)$ ? (note for something to be called a version of $E(Z|X)$ , we not only need almost surely equivalence, but we also need measurable wrt the image sigma algebra of $X$ ) Attempt : I think showing that $E(Z|X)=E(Z|Y)$ almost surely is not a problem, which implies there exists $\sigma(X)$ measurable $f$ and $\sigma(Y)$ measurable $g$ such that $E(Z|X)=f(X)=g(Y)=E(Z|Y)$ almost surely. However, can we deduce further? Or under what conditions can we deduce further?","['conditional-probability', 'measure-theory', 'probability-theory', 'probability']"
