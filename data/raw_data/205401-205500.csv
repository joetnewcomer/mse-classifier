question_id,title,body,tags
4085314,Does an asymptotic vanishing of Jensen gap imply that the variance tend to zero?,"Let $\Omega \subseteq \mathbb{R}^2$ be a nice open bounded, connected domain, having Lebesgue measure $m(\Omega)=1$ . Let $F:[0,\infty) \to [0,\infty)$ be a $C^2$ strictly convex function. Suppose that $F''$ is an everywhere positive strictly decreasing function, and that $\lim_{x \to \infty} F''(x)=0$ . Let $Y_n:\Omega \to \mathbb [0,\infty)$ be continuous , with constant expectations $\int_{\Omega} Y_n=c>0$ , and suppose that $$\lim_{n \to \infty} \int_{\Omega} F(Y_n)-F(\int_{\Omega} Y_n)=0.$$ Is $\lim_{n \to \infty} \int_{\Omega} (Y_n-c)^2=0$ ? If we replace $\Omega$ with an arbitrary probability space $X$ , and only require $Y_n:X \to [0,\infty)$ to be measurable, then the answer can be negative, as the following example shows: Set $F(x) = e^{-x}$ . For $n \in \{1, 2, 3, ...\}$ define $$ Y_n := \left\{\begin{array}{ll}
1 - \frac{1}{\sqrt{n}} & \mbox{ with prob $1-1/n$}\\
1+ \frac{n-1}{\sqrt{n}} & \mbox{ with prob $1/n$} 
\end{array}\right.$$ Then $E[Y_n]=1$ for all $n \in \{1, 2, 3, ...\}$ . $\lim_{n\rightarrow\infty} E[F(Y_n)] = F(1)$ . $\lim_{n\rightarrow\infty} E[(Y_n-1)^2]=1$ . (This example is taken from here .) The question is whether by forcing $Y_n$ to be continuous, the answer changes.","['real-analysis', 'integral-inequality', 'convex-analysis', 'probability-theory', 'random-variables']"
4085404,Can we integrate over a summation index?,"So I am reading this paper https://arxiv.org/pdf/math/0008177.pdf by Jeffrey Lagarias and in the proof of Lemma 3.1 he says \begin{equation}
\int_{1}^{n} \frac{\lfloor t \rfloor}{t^2}dt = \sum_{1 \le r \le n} \int_{r}^{n} \frac{1}{t^2} dt.
\end{equation} In the paper he uses $\infty$ as the upper integration bound, but I think it should be n, and r as the lower end, in order for the next steps in the paper to make sense. The left side is equal to $\int_{1}^{n} \sum_{1 \le r \le t} \frac{1}{t^2} dt$ , but I cannot see how \begin{equation}
\int_{1}^{n} \sum_{1 \le r \le t} \frac{1}{t^2} dt = \sum_{1 \le r \le n} \int_{r}^{n} \frac{1}{t^2} dt
\end{equation} would hold. Yes it is a finite sum, but we are integrating over t, so we shouldn't be able to just move it out of the sum, right? Then I thought it might help that $\lfloor t \rfloor$ is a step function, but we multiply it with $\frac{1}{t^2}$ , so I can't see how I can use that either. I tried using integration by parts but I got a completely different result than whats in the paper. WolframAlpha tells me it exceeds computation time. What am I missing here? I apologise if this is a stupid question, it's been a long time since my Analysis courses!",['integration']
4085463,Intersection number of complex curves in a complex surface,"Suppose $C_1,C_2$ are embedded complex curves in a complex surface $S$ , and $C_1,C_2$ have no common component. Assuming $C_1$ and $C_2$ intersect transversally, the intersection number $C_1\cdot C_2$ is just the cardinality of $C_1\cap C_2$ , because they should intersect positively. Now suppose $C_1$ and $C_2$ intersect (not necessarily transversally) at finitely many points , say $n$ . Then is it true that $C_1\cdot C_2\geq n$ , with equality iff $C_1$ intersects $C_2$ transversally? (Actually I am in a situation that $C_1\cap C_2$ is $n$ points and want to show that $C_1,C_2$ intersect transversally. If this result is true then I may conclude that $C_1,C_2$ intersect transversally, but I'm not sure about this question.)","['complex-geometry', 'algebraic-geometry', 'intersection-theory']"
4085491,Numerical analysis / Linear algebra resource to practice problems,I am studying for a college course on numerical analysis. The current module is numerical solutions to linear equations. I am looking for resources that have lots of practice problems. I have already studied the theory and just want to practice different questions. The topics I wish to emphasise on: matrix norms spectral radius singular values iterated powers of matrices condition numbers iterative methods (and any related topics). I don't mind if the resources are books or online references. I just want many difficult questions to practice,"['book-recommendation', 'reference-request', 'linear-algebra', 'online-resources', 'numerical-linear-algebra']"
4085509,Reproducing kernel Hilbert spaces from Sobolev spaces with weight/density functions,"I would like to understand which of the statements about the Sobolev space $H^1(\mathbb{R})$ remain true if one introduces a density/weight function in the definition. Details The Sobolev space $H^1(\mathbb{R})$ are those square integrable functions whose first weak derivatives exist almost everywhere and are square integrable or briefly $f^2\in L^1$ and $(f')^2\in L^1.$ This space $H^1(\mathbb{R})$ has the following properties It is a reproducing kernel Hilbert space with inner product $$\langle f,g\rangle=\int_{\mathbb{R}} f(x)g(x)\,dx + \int_{\mathbb{R}} f'(x)g'(x)\,dx.$$ The functions in $H^1(\mathbb{R})$ are continuous. Let $w:\mathbb{R}\rightarrow\mathbb{R}$ be a weight function or density, which is a strictly positive function with $\int w(x)\,dx=1.$ Now define the weighted $L^1$ space as $L^1(w):=\left\{ f:\mathbb{R}\rightarrow\mathbb{R} \mid fw\in L^1\right\}$ with norm $\lVert f\rVert_w=\int_{\mathbb{R}} |f(x)| w(x)\,dx$ and the weighted Sobolev space $H^1_w(\mathbb{R})$ by replacing $L^1$ with $L^1(w)$ in the definition above. This means the inner product of $H^1_w(\mathbb{R})$ would be $$\langle f,g\rangle_w=\int_{\mathbb{R}} f(x)g(x)w(x)\,dx + \int_{\mathbb{R}} f'(x)g'(x)w(x)\,dx.$$ Question Does $H^1_w(\mathbb{R})$ still have the two properties? I.e. is it still a reproducing kernel Hilbert space consisting of continuous functions? EDIT: The literature for weighted Sobolev spaces seems to focus on weights which are of ""Muckenhoupt class"" (see this related question ) or "" doubling measures "". But finite measures are never doubling measures. EDIT2 I would like to use the weights in applications to control the asymptotic behaviour of the functions in the RKHS. E.g. I would like to have spaces containing constant functions, polynomials (up to a certain degree) or exponential functions. This means I am quite relaxed about the properties of $w$ . $w$ may be assumed to be continuous or even differentiable, if this helps. Typical examples for $w$ would be functions such as $\exp(-x^2)$ , $\frac{1}{\cosh x}$ or $\frac{1}{(1+x^2)^k}$ .","['hilbert-spaces', 'sobolev-spaces', 'functional-analysis', 'partial-differential-equations', 'reproducing-kernel-hilbert-spaces']"
4085540,Why do the signs of $\sin\theta$ change in 3d rotation matrices about the different coordinate axes?,"Using matrices for 3-dimensional rotations, I'm confused on why the signs change $\sin(\theta)$ to $-\sin(\theta)$ when rotating around the $z$ -axis. What is the cause of this? Rotate X and Y around the Z-axis $$\begin{bmatrix}
\cos\theta & -\sin\theta & 0\\
\sin\theta & \phantom{-}\cos\theta & 0\\
0 & 0 & 1
\end{bmatrix}
$$ Rotate Y and Z around the X-axis $$\begin{bmatrix}
1 & 0 & 0\\
0 & \cos\theta & -\sin\theta\\
0 & \sin\theta & \phantom{-}\cos\theta
\end{bmatrix}
$$ Rotate X and Z around the Y-axis $$\begin{bmatrix}
\phantom{-}\cos\theta & 0 & \color{red}{\sin\theta}\\
0 & 1 & 0\\
\color{red}{-\sin\theta} & 0 & \cos\theta
\end{bmatrix}
$$","['matrices', 'quaternions', 'rotations']"
4085547,Shorthand limits of matrices?,"Is there any relatively easy way to solve limits with matrices? $$
\lim_{h\rightarrow \infty}
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
0 & \frac{1}{hc} & 1
\end{pmatrix}^h
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
\frac{1}{rc} & 0 & 1-\frac{1}{rc}
\end{pmatrix}
$$ I have painstakingly identified the solution for this particular input matrix by identifying numbers with gigantic values of h in matlab. I would prefer to not square a matrix about a hundred times and manually identify the numbers. It's like calculating the derivative of a function with limits, I rather figure out the tricks than always going through the limit. The 4th power of the input matrix looks like this and I suppose you can make certain deductions, such as $\frac{1}{h}$ and $\frac{1}{h^2}=$ 0 and $\frac{3}{h}$ = 1. $$
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
0 & \frac{1}{hc} & 1
\end{pmatrix}^4
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & \frac{-1}{rhc} & \frac{-1}{r}\\
\frac{1}{rhc} & \frac{1}{hc} & 1-\frac{1}{rhc}\\
\end{pmatrix}^2
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r}-\frac{1}{(rhc)^2}-\frac{1}{r^2hc} & \frac{1}{(rhc)^2}-\frac{1}{rhc} & \frac{-1}{r}+\frac{2}{r^2hc}\\
\frac{3}{rhc}-\frac{1}{(rhc)^2} & \frac{1}{hc}-\frac{2}{r(hc)^2} & 1-\frac{3}{rhc}+\frac{1}{(rhc)^2}\\
\end{pmatrix}
$$ I would like to understand how to go from the input matrix straight to the solution , I can reach the solution if I take the 4th power of the input matrix and look for the number 3. But that will become very time consuming for big matrices, on the order of 100 x 100. Edit The $\frac{1}{rc}$ part in the previous limit result had to receive another limit so here's the solution to that. $$
\lim_{h\rightarrow \infty}
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
\frac{1}{rhc} & 0 & 1-\frac{1}{rhc}
\end{pmatrix}^h
=
\begin{pmatrix}
1 & 0 & 0\\
\frac{1}{r} & 0 & \frac{-1}{r}\\
1-e^\frac{-1}{rc} & 0 & e^\frac{-1}{rc}
\end{pmatrix}
$$ Again, I found the solution by squaring many times and seeing error patterns and using wolframalpha to turn error sums to e functions. Maybe there is no simple way to find limits to matrices.","['matrices', 'limits']"
4085593,separating real and imaginary part of complex number,"Question: Separate into real and imaginary parts $(iy)^{iy}$ $(y : real, y \neq0)$ . My attempt $iy^{iy} = e^{iylog(iy)} = e^{iy}*e^{log(iy)}=e^{iy}*e^{ln|iy| + i*arg(iy)}
= e^{iy} * e^{ln(y)+i*(\pi/2 + 2k\pi)}$ real part = $e^{ln(y)}$ imaginary part = $e^{i(y+ (\pi/2) + 2k\pi}$ Can anyone confirm whether it is right or wrong? If wrong, could you please point out which part is incorrect approach? Thanks","['complex-analysis', 'complex-numbers']"
4085657,Matrix function decomposition of the form $A(\theta)=B(\theta)^\top Q B(\theta)$,"Suppose I have a matrix function $A:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ , for which I know the following properties hold: $A(\theta)$ is real, symmetric and bounded for all $\theta$ $A(\theta)$ is a $\mathcal{C}_1$ function, i.e., $A(\theta)$ and $\frac{\partial A(\theta)}{\partial \theta}$ are continuous. There exist two constants $0<c_1\le c_2$ such that for all $\theta$ , $c_1I\preccurlyeq A(\theta)\preccurlyeq c_2 I$ . (so it is positive definite for all $\theta$ ) Is it true that this matrix can always be represented in the following form? $$A(\theta)=B(\theta)^\top Q B(\theta),$$ where $0\prec Q\in\mathbb{R}^{n\times n}$ , $B:\mathbb{R}^n\to\mathbb{R}^{n\times n}$ and $\det(B(\theta))\neq 0$ for all $\theta$ . My intuition says yes, just take power to a half (so $B(\theta) = A(\theta)^{\frac{1}{2}}$ and $Q=I$ . But, I cannot find any reference that 'exploits' this fact. So, my question is, could you give a reference that shows/claims this, or prove this yourself (:P)? Edit: I was also thinking about eigenfunction of the matrix function. However, I'm not sure how this connects exactly to the decomposition above...","['reference-request', 'matrices', 'multivariable-calculus', 'functions', 'matrix-decomposition']"
4085681,"The Homogeneous Space $SO^+(1,3)/ \text{Sim}(2)$","I'm trying to understand the homogeneous space $SO^+(1,3)/\text{Sim}(2)$ . In wikipedia it is said that "" $SO^+(1,3)/\text{Sim} (2)$ is the Kleinian geometry that represents conformal geometry on the sphere $S^2$ "", but I couldn't find much information that explain this statement. I tried to construct the homogeneous space as the space of left cosets of the corresponding subgroup of $SL(2,\mathbb{C})$ (the universal covering group of $SO^+(1,3)$ ). Identifying a point $x = (x_0, x_1, x_2, x_3)$ in $\mathbb{R}^{1,3}$ with the hermitian $2\times 2$ matrix $$ x \leftrightarrow X,\quad X = \begin{pmatrix} x_0 + x_3 & x_1 - i x_2 \\ x_1 + ix_2 & x_0 - x_3 \end{pmatrix}, \quad \mathrm{det} X = x_0^2 - x_1^2 - x_2^2 - x_3^2$$ a matrix $A \in SL(2,\mathbb{C})$ transforms $X$ via $$X' = AXA^\dagger.$$ The subgroup $H$ stabilizing a null line ( $\mathrm{det}X = 0$ ), e.g. $\left\{ \begin{pmatrix} \lambda & 0 \\ 0 & 0 \end{pmatrix}, \lambda \in \mathbb{R} \right\} $ , are matrizes of the form $$A_{H} = \begin{pmatrix} a & b \\ 0 & \frac{1}{a} \end{pmatrix}; a,b \in \mathbb{C}, a \neq 0$$ since $$\begin{pmatrix} a & b \\ 0 & \frac{1}{a} \end{pmatrix} \begin{pmatrix} \lambda & 0 \\ 0 & 0 \end{pmatrix} \begin{pmatrix} a^* & 0 \\ b^* & \frac{1}{a^*} \end{pmatrix} = \begin{pmatrix} \lambda |a|^2 & 0 \\ 0 & 0 \end{pmatrix}$$ The subgroup $H$ is isomorphic to $Sim(2)$ . For a fixed element of $SL(2,\mathbb{C})$ the left coset is $$\begin{pmatrix} e & f \\ g & h \end{pmatrix} H = \left\{ \begin{pmatrix} e & f \\ g & h \end{pmatrix} \begin{pmatrix} a & b \\ 0 & \frac{1}{a} \end{pmatrix}; a,b \in \mathbb{C}, a \neq 0 \right\} \\
\qquad \qquad \quad = \left\{ \begin{pmatrix} ea & eb + \frac{f}{a} \\ ga & gb + \frac{h}{a} \end{pmatrix}; a,b \in \mathbb{C}, a \neq 0 \right\}.$$ For me, this looks like the whole $SL(2,\mathbb{C})$ , but it should result in a partition. Maybe this is not the way to construct this coset space. On dimensional grounds, since $H$ is $4$ dimensional, the homogeneous space is 2D.","['exceptional-isomorphisms', 'group-theory', 'homogeneous-spaces', 'group-actions', 'lie-groups']"
4085695,Proving that the determinant of a real symplectic matrix is $1$ from its eigenvalues,"Let $A$ be a $2n \times 2n$ real symplectic matrix, i.e., $A$ satisfies $$A^TJA = J$$ where $J = \begin{pmatrix} 0 & I_n\\ -I_n & 0 \end{pmatrix}$ . It is a well-known fact that $\det(A) = 1$ . There are several proofs of this, including elementary ones such as outlined here and here . However, I would like to know if there is a direct way of showing that $\det(A) = 1$ from its eigenvalues, and none of the methods I found employed such an argument. If $\lambda$ is an eigenvalue of $A$ , then it can be shown that $\bar{\lambda}$ , $\lambda^{-1}$ , and $\bar{\lambda}^{-1}$ are also eigenvalues of $A$ (they need not be distinct from each other). At first glance, this observation seems to suggest that the eigenvalues cancel each other out when multiplied together, so that the determinant of $A$ is $1$ . Nevertheless, I have yet to find a rigorous formulation of this argument. Besides, this property of eigenvalues alone seems to be insufficient to prove our claim. As an illustration of this inadequacy, suppose $A$ is a $2 \times 2$ real symplectic matrix with eigenvalues $1$ and $-1$ . Then they satisfy the property outlined above, but this implies that $\det(A) = -1$ (of course $A$ is not actually a symplectic matrix in this case). I think there are other properties of $A$ that need to be considered so that we can use the property of eigenvalues to prove the claim that $\det(A) = 1$ . In conclusion, my question is: Can we prove that $\det(A)$ by utilizing the observation that the eigenvalues of $A$ come in ''quadruplets'' of $\lambda$ , $\bar{\lambda}$ , $\lambda^{-1}$ , and $\bar{\lambda}^{-1}$ ? An elementary argument with linear algebra is preferred (without Pfaffians or manifold theory), but I welcome any insight.","['determinant', 'eigenvalues-eigenvectors', 'matrices', 'linear-algebra', 'symplectic-linear-algebra']"
4085795,What is the conceptual idea behind raising and lowering indices?,"I've been watching Eigenchris' playlists on Tensors for beginners and Tensor calculus . His videos really clear up a lot of concepts. In the last video of the Tensor for beginners series, he talks about the motivation behind raising and lowering indices . At minute 7:38, we ""introduces a new notation"" in which the index magically went down: But he doesn't really explain much about that change, and goes on without much comment after that. What I have in mind is that, as we carry the summation, we end up with terms from the metric where i=/=j which turn out to be zero in some cases, but not all of them, right? After he removes the vanishing terms we just adjust the index to be able to carry a sum with epsilon, but that's just a thought I had. Thanks for any feedback you may provide. A bit of context from the video: He uses an incomplete (or dot product with an empty slot) as a one-form to motive the use of the metric as a tool to raise/lower indices.","['special-relativity', 'tensors', 'general-relativity', 'differential-geometry']"
4085802,"Let $X_1, \dots, X_n$ i.i.d. from the $DE(\theta,b)$ Prove the MLE is asymptotically efficient","Let $X_1, \dots, X_n$ be i.i.d. from the $DE(\theta,b)$ distribution (DE is the double exponential).
(a) Show that the MLE $\theta$ is the sample median.
(b) Prove that the MLE $\theta$ is asymptotically efficient. For part (a): $$L(\theta) = \frac{1}{(2b)^n}e^{-\sum_{i=1}^{n}\frac{|X_i-\theta|}{b}}$$ Let $g(\theta) = \sum_{i=1}{n}|X_i-\theta|$ and $L(\theta)$ is the largest when $g(\theta)$ is the smallest. Since $g(\theta)$ is continuous function of $\theta$ , so $$g'(\theta) = \sum_{i=1}^{n}[-1\times 1\{X_i>\theta\}+1\{X_i<\theta\}]= \begin{cases} \text{positive if } \theta > \operatorname{median}(X_i) \\\text{negtive if } \theta < \operatorname{median}(X_i) \end{cases}$$ So, MLE $\hat{\theta}$ is the sample median. For part (b): $lnf_\theta(x_1) = -ln2b-\frac{|x_1-\theta|}{b}$ $\frac{\partial}{\partial\theta}lnf_\theta(x_1)=\frac{1}{b}$ $I(\theta) = E[\frac{\partial}{\partial\theta}lnf_\theta(x_1)]^2=\int_{-\infty}^{\infty}\frac{1}{b^2}\frac{1}{2b}e^{-\frac{|x_1-\theta|}{b}}dx=\frac{1}{2b^3}\int_{0}^{\infty}be^{-y}dy=\frac{1}{2b^2}$ where $y=\frac{|x_1-\theta|}{b}$ , $\frac{dy}{dx}=\frac{1}{b}dx$ , $y \in(0,\infty)$ so, $\mathcal{v(\theta_0)}=\frac{1}{I(\theta_0)}=2b^2$ But, in class, the professor showed us that
We have proved that for the sample median $M_n$ , $\sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2})$ and $f_\theta(\theta)) = \frac{1}{2b}e^{-\frac{|\theta-\theta|}{b}}=\frac{1}{2b}$ So, $\sqrt{n}(M_n-\theta) \xrightarrow{L} N(0, \frac{1}{4(f_\theta(\theta))^2})=N(0, b^2)$ I suppose to get $I(\theta)=\frac{1}{b^2}$ right? Could you help me out?",['statistics']
4085821,Calculus Infinite Limit Proof,"I'm trying to prove: $\lim_{x\rightarrow5^+} \frac{3}{(x-5)(x-2)}=\infty$ Let $M>0$ . Choose $\delta =$ Assume $0<x-5<\delta$ . Show $\frac{3}{(x-5)(x-2)}>M$ $\frac{3}{(x-5)(x-2)}>M$ $\frac{3}{x-5}>M(x-2)$ $x-5<\frac{3}{M(x-2)}$ I'm getting stuck here, I can't find $\delta$ and I've been working for hours on this. Any help would be greatly appreciated! I'm trying to generalize this for a bigger theorem.","['limits', 'calculus', 'real-analysis']"
4085857,Find the control for this system of ODE minimizing the energy.,"Suppose I have the system of ODE $$x'(t)=Ax(t)+Bu(t)$$ $$x(0)=x_0$$ $t$ is defined on an interval $I$ of $\mathbb{R}$ containing $0$ , for $x(t)=(x_1(t),...x_n(t))^T$ , $x_i : I \to \mathbb{R} $ differentible on $I$ , $i=1,2,...,n$ , $A$ is a real valued $n \times n$ matrix, $B \in \mathbb{R}^n$ and $u :I \to \mathbb{R}$ is the control. Find the control $u$ such that $$x(T)=x_T$$ for $T \in I$ and $$\int_0^T u^2(t)dt$$ is minimal. I am asked to solve this by following the steps below, Step 1) find the solution of $$x'(t)=Ax(t)+Bu(t)$$ Step 2) define a linear operator $L$ such that $x(T)=x_T$ can be written as $$Lu=x_T$$ Step 3) find minimal quadratic solution for the above equation. I am asked to first solve $x'=Ax+Bu$ , so by using variation of parameters and the exponential matrix, I get the fundamental solution $$x(t)=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds$$ Then I must define a linear operator, the operator $$Lu=e^{At}x_0+\int_0^t e^{A(t-s)}Bu(s)ds$$ is not linear but the particular solution $$Lu=\int_0^t e^{A(t-s)}Bu(s)ds$$ is,
so I am guessing that I only need the particular solution (is this right? and if yes why?) So I get $$x(T)=Lu=\int_0^T e^{A(T-s)}Bu(s)ds=x_T$$ So now I need to find a minimal quadratic solution to this integral equation, I suppose that means I should find a $u$ such that $$\int_0^T u^2(s)ds$$ is minimal, I am not sure on how to approach this, one idea I had is to use Lagrange multipliers, so I define $$h=\int_0^T u^2(s)ds-\int_0^T \lambda (s) e^{A(T-s)} B u(s)ds$$ where $\lambda (s)$ are the Lagrange multipliers, but I am not sure whether this is right. I am aware of other approaches to such problems like for example, by defining the Hamiltonian $H=u^2+\lambda (Ax+Bu)$ of the problem and solving the equations $$\dot{\lambda }=-H_x$$ $$\dot{x}=H_{\lambda }$$ Such that $$H_u=0$$ but I don't know if this is related to the approach I am asked to take. Can you help?","['integral-equations', 'ordinary-differential-equations', 'optimal-control', 'operator-theory', 'control-theory']"
4085975,Axiom of choice in graph theory,"I have produced the follwoing proof for the theorem that every connected graph has a spanning tree using Zorn's lemma. Let $G$ be the connected graph and consider the set of all tress denoted by $S$ . This set $S$ under subgraph relation $\subset$ is a partially ordered set $(S,\subset)$ . Any chain $C$ of the poset is a chain of tress with respect to inclusion relation. Then, the union of $A=\cup\ T_i$ where $T_i \in C$ $i \in \{1,2,3,....,n\}$ is the upper bound of the chain $C$ . Suppose $A$ is not a tree. Then $A$ is either disconnected or there is cycle present. Suppose $A$ is disconnected then there are two vertices $x, y \in A$ such that there is no path between them. Let $x \in T_x$ and $y \in T_y$ and $T_x , T_y \in A.$ Since $T_x$ and $T_y$ is in chain $C$ (because $A$ is the upper bound of $C$ ) either $T_x \subset T_y$ or $T_y \subset T_x$ and in either cases $x$ must be connected to $y$ in the bigger tree. Hence, $x$ and $y$ is connected in $A$ which is a contradiction. Now suppose that there is a cycle $c \in A$ . Every edge of $c$ must appear in some $T_i$ in $C$ , i,e. $E(c) \subset E(T_i)$ which is a contradiction as $T_i$ is acyclic. Therefore, $A$ is a tree and the upper bound of chain $C$ . Then, by Zorn's lemma
there is a maximal element $T^{\ast}$ in $S$ as every chain of $S$ has an upper bound which is a tree. Suppose $T^{\ast}$ is not a spanning tree suggesting that there is some vertex $u \in G$ that is not in $T^{\ast}$ . Adding the edges between some vertex in $T^{\ast}$ and $x$ creates a new tree $T^{\ast\ast}$ where $T^{\ast} \subset T^{\ast\ast}$ which is a contradiction on maximality of $T^{\ast}$ . I would greatly appreciate on any comments regarding any mistakes or missing parts of the proof.","['elementary-set-theory', 'solution-verification', 'graph-theory']"
4085977,Cycle index for $S_2\times S_4$,"I am trying to determine the cycle index polynomial of $S_2\times S_4$ , for the purpose of finding colourings. This is what I have tried: I computed the polynomials for $S_2$ and $S_4$ : $$Z_{S_4}(t_1,\dots ,t_4)=\frac{1}{4!}\left(6t_4^1+8t_3^1t_1^1+3t_2^2+6t_2^1t_1^2+t_1^4\right)$$ $$Z_{S_2}(s_1,s_2)=\frac{1}{2!}\left(s_1^2+s_2^1\right)$$ where $t_i^k$ indicates the presence of $k$ cycles of length $i$ in some cycle class. From what I can see, 'multiplying' should be done using an operation $\cdot$ likewise: $$t_i^n \cdot s_j^m = w_{\text{lcm}(i,j)}^{mn\cdot\text{gcd(i,j)}}$$ where $w_k$ are variables for $Z_{S_2\times S_4}$ , and if there are several variables as a product (e.g. $t_3^1 t_1^1$ ) then multiplying by something else, we do pairs separately and multiply, for instance: $$t_3^1t_1^1\cdot s_1^2=(t_3^1 \cdot s_1^2)(t_1^1\cdot s_1^2)=w_3^2w_1^2$$ Applying this to the product of both polynomials I get: $$Z_{S_2}(s_1,s_2)\cdot Z_{S_4}(t_1,\dots,t_4)=\boxed{\frac{1}{48}\left(w_1^8 +  6 w_1^4 w_2^2 + 13 w_2^4 + 8 w_1^2 w_3^2 + 8 w_2 w_6 + 12 w_4^2\right)}$$ Correction I miscalculated so have updated. As mentioned in a comment below, I interpreted the technique as, if I set all the $w_i$ to $C$ , then letting $C=$ some number of colors, giving: $$Z_{S_2\times S_4}(C,\dots,C)= \frac{1}{48}\left(C^8 + 6C^6+21C^4+ 20C^2\right) $$ This should evaluate to the number of colorings of $2\times 4$ tiles, up to row and column permutation. It works for $C=2,3$ (we get $22, 267$ ) but fails at $C=4$ where I get $1996$ instead of $1870$ . Would love it if someone could point me in the right direction. Trying to teach myself and I think I've misunderstood how this is meant to work...","['permutations', 'coloring', 'permutation-cycles', 'symmetric-groups', 'group-theory']"
4085978,Relationship of a Simplex to a Symplectic Manifold,"I have another short question regarding terminology. The phonetic similarity of simplex and symplectic manifold has little or nothing to do with any mathematical relationship, correct? I always considered a symplectic manifold to be a smooth manifold with an associated two form and a simplex to be a generalized tetrahedron. In conversation, someone asked me if any relationship existed between them, and I responded that there was none of which I was aware. I wanted to see if anyone could offer a potential connection or validate the absence of one. Thank you all.","['symplectic-geometry', 'simplex', 'differential-geometry']"
4086015,"What does ""The group $G$ acts on $X$ transitively with compact isotropy"" mean?","Let $G$ be group which acts on a space $X$ . The group is said to act transitively on $X$ if it admits exactly one orbit. The subset of points in $G$ which leave a point $x \in X$ fixed form the so-called isotropy group of stabiliser. I am confused by the meaning of the following sentence: The group $G$ acts on $X$ transitively with compact isotropy. Is this to be read as the isotropy group is compact, or the quotient of $X$ by the isotropy group is compact? The question is motivated by a reading of Griffiths-Schmid's paper on locally homogeneous spaces.","['group-actions', 'group-theory', 'algebraic-geometry', 'terminology']"
4086027,Every set in a sigma-algebra not measurable?,"I'm studying Axler's measure theory book and there a set is said to be measurable if it belongs to a $\sigma$ -algebra. A power set is always a $\sigma$ -algebra, and combining these, it would seem that every set in $\mathcal{P}(\mathbb R)$ is measurable, which is not true. What am i not understanding, why don't these definitions contradict each other?","['real-numbers', 'measure-theory', 'real-analysis']"
4086115,"Probability of ""winning a race""?","Suppose there are three people, A, B, and C. Each person starts at $x=0$ and then randomly one person moves forward by 1, with equal probability among all three people. The winner is the first person to reach $x=3$ . By symmetry, the probability of each person winning is 1/3. But now suppose person A wins if they reach $x=2$ , while person B and C still only win at $x=3$ . How can I calculate $P(\text{A wins})$ ? EDIT: Using a simulation in R I obtained these probabilities, but I'm still unsure about the analytical approach. A         B         C 
0.5472412 0.2263920 0.2263668",['probability']
4086161,Aproximation of the Normal Distribution by the Normal Density Function,"In Feller's introduction to probability the next lemma is stated: ""As $x\rightarrow \infty$ $\tag1 \frac{1-R(x)}{x^{-1}n(x)} \rightarrow 1$ Where $R(x)$ is the normal distribution and $n(x)$ is the normal density function. And more precisely: $\tag1 (x^{-1}-x^{-3})n(x) < 1-R(x) < x^{-1}n(x)$ "" This lemma is shown by proving that the left and right hand side of the inequality are the integrals from $x$ to $\infty$ of $(1-3x^{-4})n(x)$ and $((1+x^{-2})n(x))$ and using the inequality: $\tag1(1-3x^{-4})n(x) < n(x) < (1+x^{-2})n(x)$ The problem i have is in a generalization of this that comes as a problem of the book in the same chapter. The statement of the problem is: ""Prove that: $\tag1 \frac{1-R(x)}{\frac{1}{\sqrt{2\pi}}e^{-\frac{1}{2}x^2}(\frac{1}{x}-\frac{1}{x^3}+\cdots+(-1)^k\frac{1\cdot3\cdot7\cdots(2k-1)}{x^{(2k+1)}})}\rightarrow 1$ for $x>0$ "". I've only tried to show that a similar inequality like the second one holds like in the lemma using the derivatives of $x^{-k}n(x)$ and $(x^{-k}-x^{-(k+2)})n(x)$ but i don't see exactly how to get the sum that is involved, so any help would be much appreciated.","['probability-limit-theorems', 'probability-distributions', 'normal-distribution', 'probability-theory', 'density-function']"
4086305,Descent by 2-isogeny on the elliptic curve $y^2 = x^3-226x$?,"Consider the elliptic curve $E: y^2 = x^3-226x$ . I'm trying to follow page 30 (ex. 4) of these notes where they calculate the rank of this elliptic curve. I restate the main definitions: Let $E: y^2 = x^3+ax^2+bx$ and $E': y^2 = x^3+a'x^2+b'x$ where $a' = -2a$ and $b' = a^2-4b$ . Then define the map $$\alpha_{E'}: E'(\mathbb Q) \to \mathbb Q^{\times}/(\mathbb Q^\times)^2$$ where $\alpha_{E'} (\mathcal O) = 1$ , $\alpha_{E'} (0,0) = b'$ and $\alpha_{E'} (\mathcal x,y) = x$ if $x \ne 0$ .  Likewise, we define define the map $$\alpha_{E}: E(\mathbb Q) \to \mathbb Q^{\times}/(\mathbb Q^\times)^2$$ where $\alpha_{E} (\mathcal O) = 1$ , $\alpha_{E} (0,0) = b$ and $\alpha_{E} (\mathcal x,y) = x$ if $x \ne 0$ . Then  the equation $N^2 = b_1M^4 + aM^2e^2 + b_2e^4$ has a solution for some $b_1, b_2, N, M, e \in \mathbb Z, e > 0, b_1b_2 = b,$ if and only if $b_1$ is in $\text{Im}(\alpha_{E}).$ And likewise an analogous statement for $\text{Im}(\alpha_{E'})). $ Then this is applied here: Let $E : y^2 = x^3-226x.$ Then $b = -2 · 113 \implies \text{Im}(\alpha_E) \subset\left\{\pm 1, \pm 2, \pm 113, \pm 2 · 113\right\} $ . We need only consider $b_1 = -1, 2$ . If $b_1 = -1$ then $N^2 = -M^4+226e^4$ so $N = 15, M = e = 1$ . If $b = 2$ then $N^2 = -2M^4+113e^4$ so $N = 9, M = 2, e = 1$ .
Therefore $|\text{Im}(\alpha)| = 8$ . My question is: why does it suffice to consider $b_1 = -1, 2$ ? [I'm thinking automatically $1$ and $b$ are in the image so $b = -2 \cdot 113$ is in the image. So that if $-1$ is in the image, then so is $\pm 2 \cdot 113$ , and likewise $\pm 1$ is in the image. Next, if $2$ is in the image then by multiplication with $-1$ we have $\pm 2$ is in the image. But then how is $\pm 113$ in the image?] Likewise, considering the corresponding isogenous curve Let $E' : y^2 = x^3~+4 \cdot 226x.$ Then $b' = 2^3 · 113 \implies \text{Im}(\alpha_E') \subset\left\{\pm 1, \pm 2, \pm 113, \pm 2 · 113\right\} $ . We need only consider $b_1 = -1, 2$ . [Note $2 \cdot 113 \cong b'\bmod(\mathbb Q ^\times)^2 $ ]. $b_1 = -1: N^2 = -M^4-2^3\cdot 113e^4$ so no solutions by positivity. $b_1 =2: N^2 = 2M^4+4 \cdot 113 e^4$ so $N=22, M=2, e=1$ . So $|\text{Im}(\alpha)| = 4$ . And the same question here again: why does that suffice?","['number-theory', 'elliptic-curves']"
4086331,Where did the + C go - Indefinite integration?,"After performing indefinite integration on both sides, an equation becomes: $$\ln(|v|) + C = \frac{-t}{RC} + C $$ This then simplifies to: $$\ln(|v|) = \frac{-t}{RC} + C $$ Where did the $+ C$ go on the left hand side and why?","['integration', 'logarithms']"
4086368,The $L^p$ limit of characteristic functions is a characteristic function.,"Let $(X,\mathcal{A},\mu)$ a measurable space where $X$ is a set, $\mathcal{A}$ is a $\sigma$ -álgebra and $\mu:\mathcal{A}\rightarrow[0,\infty]$ is a measure. Let $(\chi_{E_{n}})$ be a sequence of characteristic functions; this is $$\chi_{E_{n}}(x)=\begin{cases}
0,&x\notin E_n,\\
1, &x\in E_n. 
\end{cases}$$ where $E_n\in \mathcal{A}$ . Suppose that the sequence $(\chi_{E_{n}})$ is a Cauchy sequence in $L^p(X,\mathcal{A},\mu)$ ; that is given $\epsilon>0$ exists $n_0\in \mathbb{N}$ such that $$\|\chi_{E_n}-\chi_{E_{n}} \|_p=\left(\int|\chi_{E_{n}}-\chi_{E_{m}}|^pd\mu\right)^{1/p}<\epsilon,\,\forall n\geq n_0.$$ I would like to probe that exists a function $f\in L^p$ such that $f=\chi_{E}$ where $E\in \mathcal{A}$ such that the sequence $(\chi_{E_{n}})$ converges to $f$ in $L_p$ ; that is, given $\varepsilon>0$ exists $n_1>0$ such that $$\|\chi_{E_{n}}-f\|_p<\varepsilon,\,\forall n\geq n_1.$$ My attempt: I think the function is $f=\chi_{E}$ where $$E=\bigcup_{m=1}^{\infty}\left(\bigcap_{k=m}^{\infty}E_k\right).$$ The reason I believe it is that the identity $$|\chi_{E_{n}}-\chi_{E_{m}}|=\chi_{E_{n}\triangle E_{m}}$$ where $E_{n}\triangle E_{m}=E_n\backslash E_m\cup E_m\backslash E_n$ together with the Cauchy property implies that $$\mu(E_{n}\triangle E_{m})<\epsilon$$ So my reasoning tells me that the ""substance"" of the functions is in their intersection.","['integration', 'measure-theory', 'characteristic-functions', 'real-analysis']"
4086371,A finite set of coplanar points is such that none lies within the triangle formed by any other three. Are the points vertices of a convex polygon? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question A finite set of points in a plane have a certain property: If we consider any 3 points in the set, and the triangle generated by these points, then NONE of the other points in the set is in the interior of this triangle. Does it follow that we can form a convex polygon having all points as vertices?",['geometry']
4086382,Show that there is a simultaneously tangent line to both the curves $y = e^x$ and $y = \ln x$,I found the derivatives of both of the curves but I'm having trouble on how to move on from here: $y = e^x\implies y' = e^x$ $y = \ln x\implies y' = \frac{1}{x}$,"['tangent-line', 'derivatives']"
4086449,Physical disasters and probability,"An urban area is susceptible to one earthquake per year with probability $30 \%$ and two with probability $5 \%$ , also three earthquakes per year is impossible.
The same area is also susceptible to floods, which may result from heavy rain (a fact which occurs with probability $100 \%$ within a year) or from a Dam failure due to the earthquake (event occurring with probability $25 \%$ in a year). The two events (floods from rain and floods from dam damage) are considered to be independent.
Calculate the probability of a flood in a year (for the specific area). I know it can be solved by using Bayes theorem but am not sure on how to use it. $P(\text{heavy_rain}) = 1$ $P(\text{dam_failure_from_earthquake}) = 0.25$ $P(\text{1_earthquake}) = 0.3$ $P(\text{2_earthquakes}) = 0.05$ $P(\text{3_earthquakes}) = 0.00$ $P(\text{floods}) = P(\text{floods_from_rain})+P(\text{floods_from_dam})$ $P(\text{floods_from_rain}) = 1$ but I don't know how to continue.","['bayes-theorem', 'probability']"
4086467,"Is there some ""types"" of ""discontinuous derivative""? [duplicate]","This question already has answers here : Prove that $f'(a)=\lim_{x\rightarrow a}f'(x)$. (6 answers) Closed 3 years ago . I'm just started learning standard Calculus class in the university (It treats James Stewart's ""Essential Calculus: Early transcendentals"" for 1 semester, just letting you know) and I got some questions about the relationship between 'differentiability of a function' (i.e.the existence of $f'(a)$ regarding $x = a$ ) and 'continuity of a derivative of function'. I made a derivative of a function on my own, which is $$f'(x)=\begin{cases}-x + 2 & x<0, \\ 1 & x=0, \\ -x & x>0. \end{cases}$$ I said the integral constant $C = 0$ arbitrarily for convenience. Because $f'$ is defined at every $R$ , then $f$ is continuous at every $R$ by theorem (don't remember the number of it...) By integrating $f'$ at every interval, then we get $$f(x)=\begin{cases} -x^2/2 + 2x & x<0, \\ 0 & x=0, \\ -x^2/2 & x>0. \end{cases}$$ But by using the definition of $f'(0)$ and given $f(x)$ , we have to say that $f'(0)$ doesn't exist! So this was a contradiction. I asked about this in my country's internet community, but the answers were the following. ""If $f'(a)$ exists, and $f'$ is not continuous at a, then there are 2 types. Both left and right limit of $f'$ at $x = a$ exists but $f'$ is not continuous at $a$ Either left or right limit of $f'$ at $x = a$ doesn't exist, so $f'$ is not continuous at $a$ . $f'$ is only available at type 2. I just accepted with no excuse, but after few days, I just wanted to know some prove about this. I read about this but as I said before, I just started Calculus, so it was too hard for me to understand some comments at there. I am not American or British, so I'm not very good at English and sorry about that. Question is that could somebody prove that ""if $f'(a)$ exists and $f'$ is discontinuous at $x = a$ , then left-hand or right-hand limit of $f'$ at $x = a$ doesn't exist."" Thx for reading this awful long writing.","['continuity', 'calculus', 'derivatives']"
4086633,"$\mathbb{P}^1$ isomorphic to conic in $\operatorname{Proj}(K[x,y,z])$ context","I have recently started to study schemes and I found my self on the follow situation:
I want to prove that $\mathbb{P}^1$ is isomorphic to a conic. I have used the morphisim $\mathbb{P}^1\longrightarrow \mathbb{P}^2 $ that in coordinates is bring by $[s:t]\longrightarrow [s^2-t^2:2st:s^2+t^2]$ , the image of this morphisim is a conic $C$ and it is in fact a isomorphisim on the image so $\mathbb{P}^1\cong C$ as abstract varieties. Now I want to express this situation on $\operatorname{Proj}(K[s,t])\longrightarrow \operatorname{Proj}(K[x,y,z])$ I am absolutly lost on this so I will acept all help, advice or trick.","['projective-geometry', 'projective-schemes', 'algebraic-geometry', 'abstract-algebra', 'schemes']"
4086636,A Difficult Area Problem involving a Circle and a Square,"A few days ago, I encountered the following problem: After a little bit of thinking, I managed to come up with the following solution: Rotate the square $90^\circ$ clockwise and let the new bottom left corner of the square be $(0,0)$ . The circle inscribed in the square is hence centered at $(5,5)$ with a radius of $5$ . The circle equation thus becomes $(x-5)^{2} + (y-5)^{2} = 25 \Rightarrow y = 5 + \sqrt{25 - (x-5)^{2}}$ in the first quadrant. Similarly for the quarter circle, the equation becomes $y = \sqrt{100-x^2}$ . The graph hence looks like this: My intention is to find the shaded area in the above graph. To do so, first I find $X$ by equating $5 + \sqrt{25 - (x-5)^{2}} = \sqrt{100-x^2} \Rightarrow x=\frac{25 - 5\sqrt{7}}{4}$ . From this, I calculate the area of the shaded region as follows: $$\text{Area} = (10 \cdot \frac{25 - 5\sqrt{7}}{4} - \int_0^\frac{25 - 5\sqrt{7}}{4} \sqrt{100-x^2} \,\mathrm{d}x) + (10 \cdot (5 - \frac{25 - 5\sqrt{7}}{4}) - \int_\frac{25 - 5\sqrt{7}}{4}^5 5 + \sqrt{25 - (x-5)^{2}} \,\mathrm{d}x) \approx 0.7285$$ Now, the diagram looks like this: From here, I figured out the shaded area as follows: $$\text{Area} \approx 10^{2} - \frac{\pi(10^{2})}{4} - (\frac{10^{2} - \pi(5^{2})}{4} + 2 \times 0.7285) \approx \boxed{14.6 \:  \text{cm}^{2}}$$ While I did figure out the correct solution, I find my approach to be rather lengthy. I was wondering if there is a quicker, simpler and more concise method (that probably does not require Calculus) that one can use and I would highly appreciate any answers pertaining to the same.","['calculus', 'graphing-functions', 'area', 'algebra-precalculus']"
4086652,Cancellation property for direct products,"I must be misunderstanding something very elementary, because every proof I see of this uses advanced methods (including the one in my course notes). Suppose $G, H, K$ are groups such that $G \times H \cong G \times K$ . We have to prove that $K \cong H$ . Now we know $G \times \{ 1 \} $ is a normal subgroup of the direct product, so we can cancel it out. By the second isomorphism theorem we obtain: $(G \times H)/G \times \{ 1 \} \cong (G \times \{ 1 \})(\{ 1 \} \times H)/G \times \{ 1 \} \cong \{ 1 \} \times H$ and likewise for $K$ . Consequently, $\{ 1 \} \times K \cong \{ 1 \} \times H$ and $K \cong H$ . What is wrong with this proof?","['direct-product', 'group-theory']"
4086672,Lorentzian First Variation Formula?,"Let $(N, g)$ be an $n$ -dimensional Riemannian manifold and $S$ be a submanifold of $N$ of dimension $m$ . For $X \in \Gamma(TS)$ define a family of variations in $M$ to be $$\varphi: S \times  (-\epsilon, \epsilon) \rightarrow M, \hspace{5mm} \varphi(x, s) = \exp_x(sX)$$ so that the surfaces $\varphi_s$ are generated by geodesics eminating from $S$ .
Then $\frac{\partial \varphi}{\partial s}|_{s=0} = X$ is the variation field. So the First Variation Formula for any fixed surface $\varphi_s$ is $${\frac{d}{ds}}\left({Vol}(\varphi_s)\right)_{\{s=0\}} = {\int_{\varphi_s}({div}_{\varphi}X)\Omega_{\varphi_s}}$$ I'm wondering if we let $N$ be Lorentzian with $S$ spacelike if this formula still makes sense? I can't see any mention of this generalisation anywhere but see no reason for it not to be true. The only worry I have is if $X$ is null then the zero inner product could potentially lead to singularity issues. However  I only see hand-wavey potential problems, nothing rigorous. I'm interested in people's thoughts, in particular whether or not $X$ 's being null makes a difference.","['semi-riemannian-geometry', 'riemannian-geometry', 'differential-geometry']"
4086782,Prove $x^n -1 \ge (x-1)^n \ \forall n \in \mathbb{N}\ \land \ \forall x\ \ge 1 $,Does my proof make sense? Problem: prove $x^n -1 \ge (x-1)^n \ \forall n \in \mathbb{N}\  \land \ \forall x\ \ge 1 $ Base step (P(1)): $\  \ \ \ \ \ \ \ x-1 \ge x-1$ Hypotesis (P(n)): $ \ \ \ \ \ x^n -1 \ge (x-1)^n$ Thesis (P(n+1)): $ \ \ \ \ \ \ \ \ x^{n+1}-1 \ge (x-1)^{n+1}$ Induction step: $$x^{n+1}-1 = x^n \cdot x -1 = x \cdot x^n -1 \ge x \cdot (x-1)^n \ge (x-1)^{n+1}$$ where: $x \cdot (\not x\not-\not1\not)^n \ge \not(\not x\not-\not1\not)^n \cdot  (x-1)$ so $x \ge x-1$,"['induction', 'solution-verification', 'discrete-mathematics']"
4086820,"$U \sim \text{unif}(0, 1)$ and $3X^2 - 2X^3 - U = 0$. What is the p.d.f. of $X$?","Let $U \sim \text{unif}(0, 1)$ and let $X$ be the root of the equation $3t^2 − 2t^3 − U = 0$ . Show that $X$ has p.d.f. $$f(x) = \begin{cases}6x(1-x) & 0\le x\le 1 \\ 0 & \text{otherwise} \end{cases}$$ Since $U \sim \text{unif}(0, 1)$ , we have for $0\le a,b\le 1$ , $P(a\le U\le b) = \frac{1}{b-a}$ . We also have $3X^2 - 2X^3 - U = 0$ . To find the p.d.f. of $X$ , I first try to find the c.d.f. of $X$ (and differentiate it later). In fact one can check that $3x^2 - 2x^3$ is an increasing function in $0 \le x \le 1$ . So, $$P(X \le x) = P(3X^2 - 2X^3 \le 3x^2 - 2x^3)\\ = P(U \le 3x^2 - 2x^3) = \int_0^{3x^2 - 2x^3} \,dx = 3x^2 - 2x^3$$ Then $$P(X \le x) = \begin{cases}3x^2 - 2x^3 & 0 \le x < 1 \\
1 &x \ge 1 \\0 &x < 0 \end{cases}$$ which gives the desired result on differentiation. I am wondering if there are any more direct approaches to this problem, i.e. without going through the c.d.f. if possible?","['probability-distributions', 'solution-verification', 'probability']"
4086866,How to prove that the segment $FG= DJ+EH$,"$BE, CD, AF$ are the angle bisectors of the $\triangle ABC$ . A circle passes through the points $D, E, F$ and intersects the triangle again at $J, H, G$ .  How to prove that $FG=DJ+EH$ ? I tried to use  angle bisector theorem with  intersecting secant theorem and got some equations. Am I on the right track? Any hints or answers?","['euclidean-geometry', 'geometry', 'plane-geometry']"
4086876,A game with multiple stones on a board (Combinatorial Game),"We have a game here. The game is played on a $10x10$ board. Here are the rules: Stones are placed on these squares as in the picture below. We have two players with alternating moves. As a legal move, we can move any stone by leftward, downward, and
downward diagonal to any square. (Moves can be of any length greater than or equal to 1). Each stone has a role similar to that of a chess queen In any square, there can be any number of stones. At each turn, only one stone is allowed to move. Once all the stones are placed in the square located at the bottom left corner of the table, the game finishes. The one who plays the last legal move loses the game ( misère convention ). Now we are asked to show the position above is a Normal position. And we are asked to find the all possible winning moves. Here is my attempt: Edit: This initial attempt was WRONG as this is not a Normal game but a Misére one. So, I fixed this: I know that I cannot continue forever by labeling as this.The game is more complex than it looks. I am open to any suggestions. Well, perhaps using Sprague-Grundy function can be useful but I don't know how to make use of it. Thanks in advance. Normal Position (N-position): They are the positions which are winning for the Next player to move. Previous Position (P-position): Positions that are winning for the Previous player (the player who just moved): So, the player aiming to win tries to make a move that puts the next player in a P-position.","['game-theory', 'combinatorics', 'combinatorial-game-theory']"
4086905,How to prove an identity involving a finite sum of binomial coefficients,"I’m struggling to prove this identity $\displaystyle\sum_{m=1}^{n}{\left(\binom nm\frac{{{\left( -1 \right)}^{m-1}}n!}{m} \right)}=\sum_{m=0}^{n-1}{\frac{n!}{n-m}}$ . I do understand that it equals $\begin{bmatrix}
   n+1  \\
   2  \\
\end{bmatrix}$ but if possible, I would like to find a proof without explicitly using Stirling numbers. Any help would be appreciated.","['summation', 'binomial-coefficients', 'combinatorics', 'stirling-numbers']"
4086932,Finding the range of $S$,"Let $f(x)$ and $g(x)$ be a function whose derivatives is on $R$ \ $\{b\}$ and $R$ , respectively. The graph of $f'(x)$ and g'(x) is shown below (on the image below). Let: $$h(x) = f(x) - g(x)$$ and $$S = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2$$ for $a,b,c$ are positive real numbers. Which of the following is true for $x \neq 0$ ? $A. S \in [h(c), h(a+c)] \\ B. S \leq h(c) \\ C. S \in S \in [h(c), h(a+b)] \\ D. S \in [h(a), h(c)]$ The graph: My solution : $$\begin{align}
S & = -[h(b+x^2)]^2 + h(b+x^2) \cdot (1+2h(c)) - [h(c)]^2 \\
& \\
& = -[h(b+x^2)]^2 + 2h(b+x^2)h(c) + h(b+x^2) - [h(c)]^2 \\
& \\
& = - ([h(b+x^2)]^2 - 2h(b+x^2)h(c) + [h(c)]^2) + h(b+x^2) \\
& \\
& = - ([h(b+x^2)] - [h(c)])^2 + h(b+x^2) \\
\end{align}$$ We have: $h'(x) = f'(x) - g'(x)$ When $x \in [b, c)$ : $f'(x) > g'(x)$ so $h(x)$ is increasing. As $x \to b^{+}$ , $f'(x) \to +\infty$ and $g'(x) \to g'(b)$ , therefore $h'(x) \to + \infty$ . This implies $h(x)$ start  from $-\infty$ and going up to $h(c)$ When $x = c$ : $h'(c) = f'(c) - g'(c) = 0$ . $h(x)$ stop increasing. When $x > c$ : $f'(x) < g'(x)$ so $h(x)$ is decreasing. From those indications, we know that $h(x)$ will only have a local maximum at $x = c$ for $x \in [b, +\infty] \,\, (1)$ Now, since $b + x^2 > b$ for $x \neq 0$ and based on $(1)$ , we get: $$h(b + x^2) \leq h(c)$$ Therefore: $$S \leq 0 + h(c) = h(c)$$ So $S$ has a maximum of $h(c)$ For the minimum, since $h(b + x^2) \leq h(c)$ and $h(b + x^2) \to -\infty$ as $x \to b^{+}$ or $x \to +\infty$ , $S$ can approach $-\infty$ and so there is no minimum for $S$ In conclusion, the answer is $\color{red}{B. S \leq h(c)}$ Is my solution correct? If yes, does that mean $a$ has no use in the process of solving? (This is also why I'm uncertainly about my answer) If not, what did I do wrong? If I wrong at the minimum of $S$ , then what is $S_{min}$ ?","['optimization', 'calculus', 'solution-verification', 'derivatives']"
4086970,Is $ \sqrt{2000!+1}$ a rational number?,"Is $
\sqrt{2000!+1}$ a rational number?
This may seem trivial, but as I wrote $2000!+1=n^2$ for $n\in\mathbb{N}$ , I realised that it probably is not a rational number and that I cannot build a constructive proof, because $n^2-1>2^{2000}$ as from here Prove by induction that $n!>2^n$ and also, as $n!<(\frac{n+1}{2})^{n}$ $n! \leq \left( \frac{n+1}{2} \right)^n$ via induction and $n^2-1<(1001+\frac{1}{2})^{2002}$ and these are already extremely hard tot tackle. Any help, please?","['number-theory', 'rational-numbers']"
4086992,Why does the set of all automorphisms on the additive group of integers contain only 2 elements?,"I believe that $f(x)=x$ and $f(x)=-x$ are the only two automorphisms. But the set of automorphisms on $\mathbb{Z}_n$ is isomorphic to $U_n$ .
So as $n$ increases, the number of automorphisms on $\mathbb{Z}_n$ increases.
However, for the entire set of integers, there are only two automorphisms. I want to understand why this changes at infinity.
Why isn't there a limiting property?","['automorphism-group', 'group-theory']"
4087032,Asymptotics of an integral in the coupon collector's problem,"I am interested in asymptotics of the integral $$I(h,n)=\int_0^\infty t(u) e^{-un}du$$ as $h\to \infty$ ( $n$ is a fixed positive integer) where $t(u)$ is the functional inverse of the function $$u(t)=-\ln\left(1-e^{-t}\left(1+t+\frac{t^2}{2}+\dots+\frac{t^{h-1}}{(h-1)!}\right)\right). $$ This integral comes up as an expectation in the coupon collector's problem . Based on numerical evidence, it seems that $I(h,n)$ is nearly linear in $h$ . If someone could provide a quick derivation of the asymptotics of $I(h,n)$ as $h\to\infty$ , that would be great. Note that $1-e^{-t}\left(1+t+\frac{t^2}{2}+\dots+\frac{t^{h-1}}{(h-1)!}\right)$ increases from $0$ to $1$ for $t\in(0,\infty)$ , so this function is invertible.","['integration', 'asymptotics']"
4087051,PDF of area of triangle with normally-distributed coordinates in any dimensions,"Question What is the probability distribution function (PDF) of the absolute area of a triangle with normally-distributed coordinates in $\mathbb{R}^m$ $(m \in \mathbb{N}, m\ge2)$ ? A conjecture is given that can be proved or might help to find the correct solution. The triangle vertices in $\mathbb{R}^m$ are $$ \mathbf{\mathrm{X}_1} =(x_1^1,\ldots, x_1^m),\;\;   \mathbf{\mathrm{X}_2}=(x_2^1,\ldots,x_2^m),\;\;    \mathbf{\mathrm{X}_3}=(x_3^1,\ldots,x_3^m)$$ where $x_i^j$ are independent standard normal distributed variables $$x_i^j\sim\mathcal{N}(0,1)$$ The non-oriented area $A$ of a random triangle instance in $\mathbb{R}^m$ is $$A=\|\mathbf{\mathrm{X}_1}-\mathbf{\mathrm{X}_3}\|\, \|\mathbf{\mathrm{X}_2}-\mathbf{\mathrm{X}_3}\| \frac{\sin\alpha}{2}\tag{1}$$ where $\|\cdot\|$ is the Euclidean norm and $\alpha$ is the angle at $\mathbf{\mathrm{X}_3}$ . The expectation value of $A$ is $$\mathbb{E}[A]=\frac{\sqrt{3}}{2}\left(m-1\right)\tag{2}$$ A proof of eq.(2) can be found in this Cross Validated post . Conjecture for PDF The probability distribution of empirical data of $A$ for any tested $m$ can be fitted quite well with the function $$f(A)=\frac{k^{m-1}} {A \mathrm{e}^k (m-2)! }\, \, \text{
with} \, \ k=\frac{2A}{\sqrt{3}} \tag{3}$$ that fulfills also the conditions of a PDF $$\int_0^\infty f(A) \mathrm{d}A=1\ \,\, \text{and}\ \int_0^\infty A f(A)\mathrm{d}A=\mathbb{E}[A]$$ Experimental data is indistinguishable from eq.(3) but a proof is missing. Related question In a similar case the PDF for the volume of a tetrahedron in $\mathbb{R}^3$ was tried to solve in this Math SE post but no full solution was given so far.","['geometric-probability', 'normal-distribution', 'simplex', 'triangles', 'probability']"
4087095,Subgroups of finitely generated group are not necessarily finitely generated (proof).,"In my abstract algebra course, I learned about finitely generated groups. One of the exercises proves that a subgroup of a finitely generated group is not necessarily finitely generated itself. The exercises looks at the group $S\mathbb{Z}, \circ$ , the group of permutations of $\mathbb{Z}$ . Two permutations are defined: $$\sigma: \mathbb{Z} \to \mathbb{Z}: z \to z +1,$$ (so right-shift) and $\tau_{i,j}$ which fixes all integers, except for $i,j$ which are swapped. We then considered $G = \text{grp}\{\sigma, \tau_{1,2}\}$ (=group generated by...) and need to prove that $\tau_{i,j} \in G$ . I remarked that: $\tau_{n, n+1} = \sigma^{n-1} \circ \tau_{1,2} \circ \sigma^{n-1}$ $\tau_{i,j} = \tau_{j-1, j} \circ \cdot \circ \tau_{i, i+1} \circ \cdots \tau_{j-1, j}$ . and this shows that $\tau_{i,j} \in G$ . Now define $H = \text{grp}(\tau_{i,j} \text{ for all } i,j \in \mathbb{Z} \text{ with } i > j\}$ . This is a subgroup of $G$ which is not finitely generated. questions: is there a more direct way to prove that $\tau_{i,j} \in G$ ? the show that $H$ is not finitely generated, I thought to pick a finite number of $\tau_{i,j}$ . Then there is a largest integer $k$ that is swapped by these generators. This shows that $\tau_{k+1, k+2}$ would not be contained in this finitely generated group, so $H$ can not be finitely generated. Is this correct? Or is it possible that $H$ has other generators than transpositions?","['permutations', 'finitely-generated', 'group-theory', 'combinatorial-group-theory']"
4087100,Folland Exercise 7.18 on Radon measure,"I'm trying to do the exercise below. 18. If $\mu$ is a $\sigma$ -finite Radon measure on $X$ and $\nu \in \mathcal{M}(X)$ , let $\nu=\nu_1+\nu_2$ be the Lebesgue decomposition of $\nu$ with respect to $\mu$ . Then $\nu_1$ and $\nu_2$ are Radon. (Use Exercise 8.) Also, can we say that $\| v\| = \|v_1\| + \| v_2\|$ ? Here $M(X)$ is the space of complex Radon measures on $X$ , and if $\mu \in M(X)$ , we define $\|\mu \| = |\mu|(X)$ , where $|\mu|$ is the total variation of $\mu$ . I know that by Lebesgue decomposition we have $v_1 \ll \mu$ and $v_2 \perp \mu$ . By the Radon- Nikodym theorem,  there exists some $f \in L^+$ such that $v_1(E) = \int_E f d\mu$ for $E \in B_X.$ By exercise 8, we have $v_1$ is Radon. But from here, I'm not sure how to proceed. Any help will be appreciated! Thank you.","['measure-theory', 'radon-nikodym']"
4087133,Is $E[X^2] = E[|X|^2]$?,"Let $X$ be a random variable with mean 0 and variance $\sigma^2$ . Then $E[X^2] = E[|X|^2]$ but when we write them as $$
E[X^2] = \text{Var}[X] + E[X]^2 = \sigma,
$$ and $$
E[|X|^2] = \text{Var}[|X|] + E[|X|]^2,
$$ they are not equal. What is going on here, am I mistaken in thinking $E[X^2] = E[|X|^2]$ ?","['statistics', 'variance', 'expected-value', 'probability-theory', 'probability']"
4087183,Exponentially weighted infinite sum of Bernoulli variables,"Consider the following process.  For each integer $i \geq 0$ , independently sample a Bernoulli distribution with probability $p = 1/2$ , obtaining sample $x_i$ .  Then calculate $x = \sum_{i=0}^\infty x_i \theta^i,$ where $(\theta < 1)$ .  What is the distribution over $x$ ? I see that if θ = 0.5 then this is a uniformly generated binary number between 0 and 2. This post is closely related, but I don't see how the method given there (with θ = 0.5) generalizes to arbitrary $\theta$ .  I am interested in values of $\theta$ close to 1, such as $\theta = 0.95$ . I am ultimately interested in a hypothesis test: $H_0=$ "" $p = 0.5$ "" against the alternative $H_A=$ "" $p \neq 0.5$ "".  The motivation is, I am receiving an infinite sequence of 0s and 1s for which I only retain an exponentially weighted average (not recording the whole sequence).  And based on this weighted average I want to decide whether the 0s and 1s were generated with equal probability or not. Edit: based on an empirical test it appears to be roughly normally distributed.  The following was generated by performing a 100,000 sample run with $\theta = 0.95$ .  It has mean 10 and variance 2.5. 5.9 
 6.0 
 6.1 *
 6.2 *
 6.3 *
 6.4 *
 6.5 **
 6.6 **
 6.7 **
 6.8 ***
 6.9 ***
 7.0 ****
 7.1 *****
 7.2 *****
 7.3 ******
 7.4 ******
 7.5 ********
 7.6 ********
 7.7 *********
 7.8 *********
 7.9 **********
 8.0 ************
 8.1 ************
 8.2 *************
 8.3 **************
 8.4 ***************
 8.5 ****************
 8.6 *****************
 8.7 ******************
 8.8 *******************
 8.9 *******************
 9.0 ********************
 9.1 *********************
 9.2 **********************
 9.3 ***********************
 9.4 **********************
 9.5 ***********************
 9.6 ***********************
 9.7 *************************
 9.8 ************************
 9.9 ************************
10.0 ************************
10.1 ************************
10.2 *************************
10.3 ************************
10.4 ***********************
10.5 ***********************
10.6 **********************
10.7 ***********************
10.8 **********************
10.9 *********************
11.0 *******************
11.1 *******************
11.2 ******************
11.3 ******************
11.4 ****************
11.5 ***************
11.6 ***************
11.7 *************
11.8 ************
11.9 ************
12.0 ***********
12.1 **********
12.2 *********
12.3 ********
12.4 ********
12.5 *******
12.6 ******
12.7 *****
12.8 *****
12.9 *****
13.0 ****
13.1 ***
13.2 ***
13.3 **
13.4 **
13.5 **
13.6 *
13.7 *
13.8 *
13.9 *
14.0 
14.1 Normal quantile plot for $\theta=0.90$ : Normal quantile plot for $\theta=0.95$ : Looks like it has thin tails.","['hypothesis-testing', 'probability']"
4087188,Can we derive this property of the direct sum from the categorical definition of coproduct?,"I know that, for an infinite family $\{B_i\}$ of abelian groups, the direct sum is a subgroup of the direct product $-$ $$
\bigoplus_i B_i \subseteq \prod_i B_i
$$ where an element $(b_i)$ of the direct sum looks just like an element of the direct product, with the condition that cofinitely many $b_i$ are zero. Now, from the group-theoretic perspective this makes perfect sense to me. However, I'm wondering if there's a way to explain why the coproduct exhibits this cofiniteness property, while the product does not, just by looking at the relevant diagrams: Thanks for any help in understanding this! Edit: I have a vague intuition this is related to there being many morphisms going into the direct sum, which in some sense ""obfuscates"" the information and requires us to fall back on what we know about sums from the definition, which imposes the cofiniteness condition. Am I on the right track here?","['group-theory', 'abstract-algebra', 'abelian-groups', 'category-theory']"
4087313,$E(X \mid X > x)$ is increasing in $x$. Why?,"For two points $x < x'$ and a random variable $X$ , we must have $E(X\mid X > x )\leq E(X\mid X > x' )$ . This is ""obviously"" true because the center of the truncated distribution shifts to the right. How do I prove that? I tried working with an iid copy $X^*$ of $X$ to show that the expectation of $X1(X>x)1(X^*>x')$ is smaller than the expectation of $X1(X>x')1(X^*>x)$ but I'm not having any luck with that. All results I can find either focus on normality or assume densities.","['conditional-expectation', 'probability-distributions', 'probability-theory', 'probability']"
4087332,Proving differentiability and calculating the differential of $A \mapsto A^2$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $$\begin{aligned} f : M_n(\Bbb R) &\to M_n(\Bbb R)\\ A  &\mapsto A^2 \end{aligned}$$ Prove that $f$ is differentiable and calculate its differential. Any hints on how to prove that? I don't think $f$ is linear so any theorems I can use?","['matrices', 'matrix-calculus', 'derivatives']"
4087335,"Find the extrema of $f(x,y) = x^3\cdot y^3$ in $\mathbb{R}^2$","I am asked to find the extrema of $$f(x,y) = x^3\cdot y^3$$ in $\mathbb{R}^2$ However, using the Hessian criteria, I get that the determinant of the Hessian matrix is zero for the two possible critical points: $(0,y)$ and $(x,0)$ . Since it is zero, I can't know if it is a $\min$ / $\max$ or a saddle point. I don't know what method or what to do next. What method is there for when this happens? How can I determine wether they are $\min$ or $\max$ ? Thanks!","['extreme-value-theorem', 'maxima-minima', 'multivariable-calculus', 'partial-derivative', 'hessian-matrix']"
4087421,Curious Binomial Coefficient Property,"Let $p = 3m + 1 $ be prime. Let $x$ be the integer closest to $0$ (not necessarily positive) such that: $$ x \equiv {2m \choose m}  \pmod p$$ Then, is it true that: $$ 9 \mid p+1-x$$ For example, when $m=2$ , $p=7$ , then $x=-1$ and $9\mid 7+1-(-1)$ I've programmatically checked it for $p<35000$ , so I'm pretty sure it's true. I don't see any obvious way to simplify the binomial coefficient nor any obvious properties that is has. I'd appreciate any help in understanding why this is the case and if there's some kind of simple underlying relationship here I'm missing.","['number-theory', 'binomial-coefficients', 'divisibility']"
4087484,Shouldn't the chain rule be applied instead of the product rule in linear operators?,"If I have the linear operators $A(x)$ and $B(x)$ , and i want to calculate the derivative of $A(B(x))$ , shouldn't it be: $$A(B(x))'= A'(B(x))B'(x)$$ Instead of $(AB(x))'= A'(x)B(x) + A(x)B'(x)$ ? This may not make much sense, but if you think about this linear operators as matrices, when we have $AB\vec{x} $ , should't we undestand it as "" $A(B(x))$ "", instead of "" $A(x)B(x)$ ""? After all A is acting on $x$ transformed by ""B"", not in $x$ itself. I am making this stupid question because I am trying to study QM through Functional analysis and operator theory, but when I came to this example I thought about something that I hadn't thought before and now I am in doubt, sorry for the silly question. It came from this check @littleO : ""Let $A(x)$ be an operator (It probably means a linear operator,but you know how physicists are!) dependant on a continuous variable x, define its variable by: $$\frac{dA(x)}{dx} \equiv  A'(x) = \lim_{\varepsilon \rightarrow 0} \frac{A(x+\varepsilon)-A(x)}{\varepsilon}$$ If $A$ has an inverse show that $$\frac{dA^{-1}}{dx}= - A^{-1}A'A^{-1}$$ Show also that $$\frac{dAB}{dx}= A'B+ AB'$$ ""","['multivariable-calculus', 'operator-theory', 'linear-algebra']"
4087504,Can the derivative difference be arbitrarily larger than the function difference?,"Background : Let $ f : \mathbb R ^ 2 \to \mathbb R $ be a continuous function such that $ f ( x , x ) = 0 $ for all real numbers $ x $ , and $ f ( x , y ) > 0 $ for all real numbers $ x , y $ with $ x > y $ . Hypothesis : For any such $ f $ , there exists a strictly increasing real function $ g $ such that $ g ' ( x ) - g ' ( y ) \ge f ( x , y ) \big( g ( x ) - g ( y ) \big) $ for all $ x , y $ that $ x > y > 0 $ . Is it possible to prove this hypothesis? Motivation: Since $ f ( x , y ) $ can be arbitrarily large, this question translates to: Can the derivative difference be arbitrarily larger than the function difference? From English the answer seems to be obviously no, but looking at the mathematical formula, the existence seems also obvious. Example 1: Let my try a a small example when $ f ( x , y ) $ has an upper bound, say $ k $ . Then, suppose that $ g ( x ) = e ^ { m x } $ where $ m > k $ . Can we verify that $ g ' ( x ) - g ' ( y ) \ge f ( x , y ) \big( g ( x ) - g ( y ) \big) $ ? Example 2: $ f ( x , y ) = ( x - y ) ^ 2 $ . (my try) Using Taylor expansion, one sufficient condition for the hypothesis is: for any $ n $ , for any $ y \in ( 0 , x ) $ , $$ ( x - y ) ^ n g ^ { ( n + 1 ) } ( x ) \ge ( x - y ) ^ 2 ( x - y ) ^ n g ^ n ( x ) $$ $$ g ^ { ( n + 1 ) } ( x ) \ge ( x - y ) ^ 2 g ^ n ( x ) \text . $$ A little work will show that, for any $ n $ , the solution set for this inequality does exist. However, I am not sure if the solution sets overlap. Related: Is it possible for the derivative of a function to grow arbitrarily faster than the function itself? First answer. Note: I took Martin's suggestion to make everything positive.","['functional-equations', 'functional-inequalities', 'real-analysis', 'functions', 'functional-analysis']"
4087550,Solving trig equation on both sides for finding angle,"I'm reading ""On Packing Squares with Equal Squares"" link . Just for fun/practice I've been trying to find the angle of incline of the rectangles formed by the $\alpha - N+1$ unit squares displayed below. Here's what I've worked out so far: $\beta = \alpha-N$ , small squares are unit squares"" /> It seems I can't find a u value regardless of $\alpha$ and $N$ . In particular, it's not working for the $\alpha = 1000000.01$ and $N=1000000$ proposed in the paper and I'm wondering if I'm making some sort of mistake or if there's something I don't understand.","['trigonometry', 'combinatorics', 'geometry']"
4087598,Spectrum of bilateral shift,"Let $T:l^2(\mathbb{Z})\longrightarrow l^2(\mathbb{Z})$ and define $T(\{x_n\})=\{x_{n-1}\}.$ Some reference tells me that the spectrum of $T$ is $\mathbb{T}=\{\lambda\in\mathbb{C}:|\lambda|=1 \}.$ What I'm sure is that for $\lambda=1$ then $T-\lambda I$ is not invertible. But, I'm not quite sure that for $\lambda\in\mathbb{T}$ other than $\lambda=1$ , $T-\lambda I$ also not invertible. For example, even for $\lambda=-1$ or $\lambda=\frac{\sqrt{2}}{2}+i\frac{\sqrt{2}}{2},$ I can't see why $T-\lambda I$ is not invertible and (for example) if $\lambda=\sqrt{2}+i\sqrt{2}\notin\mathbb{T}$ , then $T-\lambda I$ (perhaps) invertible. Any help would be appreciated. Thank you","['spectral-theory', 'functional-analysis']"
4087608,"Let $B\subset A = \{1,2,3,...,99,100\}$ and $|B|= 48$. Prove that exist $x,y\in B$, $x\ne y$ such that $11\mid x+y$.","Let $B\subset A = \{1,2,3,...,99,100\}$ and $|B|= 48$ .
Prove that exist $x,y\in B$ , $x\ne y$ such that $11\mid x+y$ . Proof: Let $P_0:= \{11,22,...,99\}$ and for $i=
1,2,...49$ and $11\nmid i$ make pairs $P_i:= \{i,99-i\}$ . Now we
have $46$ subsets with sum of each pair in each subset divisible
by $11$ . So if we took at most $1$ element from each set $P_i$ in to $B$ then $B$ would have at most $47$ elements (if $100$ is in $B$ also).
A contradiction. This bound is sharp. Take $$B:=\{i\in A; i\equiv x\pmod{11},\,x\in \{1,2,3,4,5\}\} \cup \{11\}$$ Then $|B| =47$ and there is no $x,y\in B$ such that $11\mid x+y$ . My question here is: Is this problem doable by polynomial method? As to watch in $\mathbb{Z}_{11}[x,y]$ a polynomial $$p(x,y) := \prod_{i=1}^{10}(x+y-i)$$ If the statment would not hold, then $p(x,y)=0$ for all $(x,y)\in \{(a,b)\in B' \times B'; \;a\ne b\}$ where $B' = B\pmod {11}$ . Clearly $|B'|\geq 6$ . Is this good idea?","['contest-math', 'pigeonhole-principle', 'combinatorics', 'algebraic-combinatorics', 'discrete-mathematics']"
4087621,In which of the intervals is $\sqrt{12}$,"In which of the intervals is $\sqrt{12}:$ a) $(2.5;3);$ b) $(3;3.5);$ c) $(3.5;4);$ d) $(4;4.5)$ ? We can use a calculator and find that $\sqrt{12}\approx3.46$ so the correct answer is actually b . How can we think about the problem without a calculator (if on exam for example)? I was able to conclude that $$\sqrt{9}=3<\sqrt{12}<\sqrt{16}=4,\\3<\sqrt{12}<4,\\\sqrt{12}\in\left(3;4\right).$$ How can I further constrict the interval? Thank you in advance!","['inequality', 'irrational-numbers', 'number-comparison', 'radicals', 'algebra-precalculus']"
4087642,Solving $\tan^{-1}\left(\frac{2x+1}{x+1}\right)+\tan^{-1}\left(\frac{2x-1}{x-1}\right)=2\tan^{-1}\left(1+x\right)$,"Find all $x$ that satisfy the equation $$\tan^{-1}\left(\frac{2x+1}{x+1}\right)+\tan^{-1}\left(\frac{2x-1}{x-1}\right)=2\tan^{-1}\left(1+x\right)$$ Attempt: Taking tangent on both sides, and using the identities $\displaystyle \tan\left(\alpha+\beta\right)=\frac{\tan\alpha+\tan\beta}{1-\tan\alpha\tan\beta}$ and $\displaystyle \tan\left(2\alpha\right)=\frac{2\tan\alpha}{1-\tan^{2}\alpha}$ , We obtain $$\frac{\frac{\left(2x+1\right)}{x+1}+\frac{\left(2x-1\right)}{x-1}}{1-\frac{\left(4x^{2}-1\right)}{x^{2}-1}}=\frac{2\left(x+1\right)}{1-\left(x+1\right)^{2}}$$ and after removing the root $x=0$ , we are left with a simple equation $(x^2-2)(2x+1)=0$ . So, the possible roots are $x=0,\pm \sqrt{2}, -\dfrac{1}{2}$ , now my question is, how do I verify which roots satisfy the original equation, because one cannot see directly after plugging, whether it's true or not because it involves arctangents. To avoid this situation, I re-wrote the original equation as $$\tan^{-1}\left(\frac{2x+1}{x+1}\right)-\tan^{-1}\left(1+x\right)=\tan^{-1}\left(1+x\right)-\tan^{-1}\left(\frac{2x-1}{x-1}\right)$$ and because while using the identity $\tan^{-1}\left(\alpha\right)-\tan^{-1}\left(\beta\right)=\tan^{-1}\left(\frac{\alpha-\beta}{1+\alpha\beta}\right)$ , there are no restrictions on $\alpha,\beta$ I went ahead with this, and got the equation $$\frac{\frac{\left(2x+1\right)}{x+1}-\left(x+1\right)}{2\left(x+1\right)}=\frac{\left(x+1\right)-\frac{\left(2x-1\right)}{x-1}}{2x}$$ which simplified to $\left(x+1\right)^{2}\left(x-2\right)+x^{2}\left(x-1\right)=0$ and surprisingly from here, I am not even getting those roots! I used this method to avoid the extraneous roots, what is going wrong here? Edit: The immediately above issue has been resolved, however, the first issue remains unresolved. Note that all calculations have to be performed by hand, since these types of questions are asked in exams in which the allotted time per question averages out to 2-3 minutes.","['algebra-precalculus', 'inverse-function', 'trigonometry']"
4087693,First-order non-linear ordinary differential equation,I know how to solve the Bernoulli differential equation. How to solve the following first-order non-linear ordinary differential equation. $y' + p(x)y + q(x) = y^2 r(x)$ .,['ordinary-differential-equations']
4087731,Solution verification about complex multiplication in polar form,"I am trying to prove the process behind complex multiplication in polar form, which is similar to this: The product of two complex numbers in polar form $r_{1}\,\angle\,\theta_{1}$ and $r_{2}\,\angle\,\theta_{2}$ is $r_{1}r_{2}\,\angle\,(\theta_{1} + \theta_{2})$ . The proof where the polar form is written into rectangular form which will then use sum-to-product formula is already established like this , so I want to try something new. Can someone verify if my attempt is correct? Define two complex numbers $z_{1} = a + bi$ and $z_{2} = c + di$ . Writing it in polar form, we get $z _{1} = \sqrt{a^{2} + b^{2}}\,\angle\,\tan^{-1}\left(\frac{b}{a}\right)$ and $z_{2} = \sqrt{c^{2} + d^{2}}\,\angle\,\tan^{-1}\left(\frac{d}{c}\right)$ , respectively. Also, the product of the two complex numbers $z_{1}z_{2}$ is equal to $(ac - bd) + (ad + bc)i$ . Writing this in polar form, we have $z_{1}z_{2} = \sqrt{(ac - bd)^{2} + (ad + bc)^{2}}\,\angle\,\tan^{-1}\left(\frac{ad + bc}{ac - bd}\right)$ . Let this product be $z_{3}$ . We then check if the radius of the product is equal to the product of the radii of the factors. \begin{align*}r_{1}r_{2} &\overset{?}{=} r_{3} \\ \left(\sqrt{a^{2} + b^{2}}\right)\left(\sqrt{c^{2} + d^{2}}\right) &\overset{?}{=}\sqrt{(ac - bd)^{2} + (ad + bc)^{2}} \\ \sqrt{(a^{2} + b^{2})(c^{2} + d^{2})} &\overset{?}{=} \sqrt{(ac)^{2} - 2abcd + (bd)^{2} + (ad)^{2} + 2abcd + (bc)^{2}} \\ \sqrt{(ac)^{2} + (ad)^{2} + (bc)^{2} + (bd)^{2}} &\overset{?}{=} \sqrt{(ac)^{2} + (bd)^{2} + (ad)^{2} + (bc)^{2}} \\ \sqrt{(ac)^{2} + (ad)^{2} + (bc)^{2} + (bd)^{2}} &= \sqrt{(ac)^{2} + (ad)^{2} + (bc)^{2} + (bd)^{2}}\end{align*} Next, we check if the angle of the product is the sum of the angles of the factors. \begin{align*}\theta_{1} + \theta_{2} &\overset{?}{=} \theta_{3} \\\tan^{-1}\left(\frac{b}{a}\right) + \tan^{-1}\left(\frac{d}{c}\right) &\overset{?}{=} \tan^{-1}\left(\frac{ad + bc}{ac - bd}\right).\end{align*} Before proceeding, we must restrict $\theta_{i}$ on the range of the arctangent function which is $(-\frac{\pi}{2},\frac{\pi}{2})$ as the arctangent function. If, for instance, $\mathrm{Re}(z_{i}) = 0$ , then the sign of $\mathrm{Im}(z_{i})$ will determine the sign of $\frac{\pi}{2}$ . Also, if $\mathrm{Re}(z_{i}) < 0$ and $\mathrm{Im}(z_{i}) > 0$ , we can just take the angle of a complex number $z$ where $\mathrm{Re}(z) = \mathrm{Im}(z_{i})$ and $\mathrm{Im}(z) = \mathrm{Re}(z_{i})$ . Rewrite the relation by letting $b$ , $d$ , $a$ , and $c$ take the value of $\frac{b}{a}$ , $\frac{d}{c}$ , $1$ , and $1$ . Then, by simplifying, \begin{align*}\tan^{-1}\left(\frac{(\frac{b}{a})}{1}\right) + \tan^{-1}\left(\frac{(\frac{d}{c})}{1}\right) &\overset{?}{=} \tan^{-1}\left(\frac{(1)(\frac{d}{c}) + (\frac{b}{a})(1)}{(1)(1) - (\frac{b}{a})(\frac{d}{c})}\right) \\ \tan^{-1}\left(\frac{b}{a}\right) + \tan^{-1}\left(\frac{d}{c}\right) &\overset{?}{=} \tan^{-1}\left(\frac{\frac{b}{a} + \frac{c}{d}}{1 - (\frac{b}{a})(\frac{d}{c})}\right)\end{align*} Then, let $\tan u = \frac{b}{a}$ and $\tan v = \frac{d}{c}$ . By substitution, \begin{align*}\tan^{-1}\left(\tan u\right) + \tan^{-1}\left(\tan v\right) &\overset{?}{=} \tan^{-1}\left(\frac{\tan u + \tan v}{1 - (\tan u)(\tan v)}\right) \\ u + v &\overset{?}{=} \tan^{-1}\left(\frac{\tan u + \tan v}{1 - (\tan u)(\tan v)}\right) \\ \tan(u + v) &\overset{?}{=} \frac{\tan u + \tan v}{1 - (\tan u)(\tan v)}\end{align*} The last relation is known to be true. Thus, $$\tan(u + v) = \frac{\tan u + \tan v}{1 - (\tan u)(\tan v)}$$ Since the product of the radii of the factors is equal to the radius of the product and the angle of the product is equal to the sum of the angles of the factors, therefore, $(r_{1}\,\angle\,\theta_{1})(r_{2}\,\angle\,\theta_{2}) = r_{1}r_{2}\,\angle\,(\theta_{1} + \theta_{2})$ . Edit: Tried to fix the angle problem. ""Before proceeding..."", in particular.","['algebra-precalculus', 'solution-verification', 'trigonometry', 'complex-numbers']"
4087805,Existence of rectifiable/piecewise $C^1$ curves between two points of an open connected domain in $\mathbb{C}$,"The question is the same as the title. Precisely, Suppose $\Omega \subseteq \mathbb{C}$ is an open connected domain. Given two points $z_0, z_1 \in \Omega$ , does there exist a rectifiable curve $\gamma : [0, 1] \to \Omega$ with $\gamma(0) = z_0, \gamma(1) = z_1$ . What about piecewise $C^1$ $\gamma$ ? Since $\mathbb{C}$ is locally path connected and $\Omega$ is connected, we have that $\Omega$ is path connected, so that there is a continuous curve $\gamma$ which starts at $z_0$ and ends at $z_1$ . But can we always choose $\gamma$ to be rectifiable or piecewise $C^1$ (the latter would imply the former though).","['connectedness', 'path-connected', 'curves', 'analysis', 'complex-analysis']"
4087807,"Eberlein-Šmulian theorem and ""Whitley's construction""","The Eberlein-Šmulian theorem states that if $X$ is a Banach space, $\sigma(X,X')$ denotes the weak topology on $X$ and $A\subseteq X$ , then $A$ is (relatively) $\sigma(X,X')$ -compact if and only if $A$ is (relatively) sequentially $\sigma(X,X')$ -compact. Now I've read that by ""Whitley's construction"" (whatever that is) we can show that if $A$ is relatively $\sigma(X,X')$ -compact and $x\in\overline A^{\sigma(X,\:X')}$ , then there is a sequence in $A$ which converges weakly to $x$ . Honestly, I don't get that. Isn't this claim preciesely the definition of $A$ being relatively sequentially $\sigma(X,X')$ -compact? Is there any subtlety I'm missing here?","['weak-convergence', 'weak-topology', 'functional-analysis', 'dual-spaces', 'compactness']"
4087823,Calculating $\cos^{-1}{\frac{3}{\sqrt10}} + \cos^{-1}{\frac{2}{\sqrt5}}$,"$$\cos^{-1}{\frac{3}{\sqrt{10}}} + \cos^{-1}{\frac{2}{\sqrt 5}}= ?$$ Let $\cos^{-1}{\frac{3}{\sqrt{10}}}=\alpha,
 \cos^{-1}{\frac{2}{\sqrt 5}}=\beta$ then, $\cos\alpha=\frac{3}{\sqrt{10}}, \cos\beta=\frac{2}{\sqrt5}$ Therefore $$\cos\alpha=\frac{3\cdot2}{2\sqrt2\sqrt5}= \frac{3}{2\sqrt2}\cdot\cos\beta$$ This is all I did till now. Could you go further with this to answer?","['trigonometry', 'inverse-function']"
4087898,estimate of holder norm of a product,"In a book 'Elliptic Partial Differential Equations of Second Order' by Gilbarg and Trudinger I stumbled upon the following inequality for two functions $f, g$ and their Holder norms in bounded domains $\Omega$ (eq 4.7): $$[f g]_{\gamma} \leq \max(1, d^{\alpha + \beta - 2\gamma}) [f]_{\alpha} [g]_{\beta} $$ where $\gamma = \min(\alpha, \beta)$ and $d = \text{diam}\ \Omega$ , $[f]_{\alpha} = \sup_{x \neq y} \frac{ f(x) - f(y) }{ \lvert x - y \rvert^{\alpha}}$ It's easy to prove that $fg$ is $\gamma$ -Holder (we use Holder embedding in compact spaces): $$[fg]_{\gamma} = \sup_{x \neq y} \frac{ \lvert f(y) g(y) - f(x) g(x) \rvert }{ \lvert x - y \rvert ^{\gamma} }\leq \sup_{x \neq y} \bigg( | f(x) | \frac{ |g(x) - g(y)| }{|x-y|^{\gamma}} + |g(x)|\frac{|f(x) - f(y)|}{|x-y|^{\gamma}} \bigg) \leq |f|_{\infty} [g]_{\gamma} + |g|_{\infty} [f]_{\gamma} $$ so $[fg]_{\gamma}$ is bounded by finite norms of $f,g$ . Then $|f|_{\infty} = \sup_{x \in \Omega} |f(x)| = \sup_{x \in \Omega} | f(x) - f(x_0) + f(x_0) | \leq |f(x_0)| + [f]_{\gamma} |x - x_0|^{\gamma}$ now if $x_0$ is taken such that $f(x_0) = 0$ , then we can bound it by: $$[fg]_{\gamma} \leq 2 [f]_{\gamma} [g]_{\gamma} d^{\gamma}$$ which is far from satisfying - not to mention that such $x_0$ does not necessarily have to exist.","['holder-spaces', 'functional-analysis', 'real-analysis']"
4087914,Is it true that $G^2$ is always hamiltonian?,"Given a connected graph $G$ , define $G^2$ to be a graph with same vertex set as $G$ and edge between two vertices $u$ and $v$ iff the distance between $u$ and $v$ in $G$ is at most $2$ . Is it true that $G^2$ is always hamiltonian? I think that I've proved it, but since I am asked to prove the corresponding result for $G^3$ instead of $G^2$ , I doubt that this result is true. If it is not true, can you please give a counter example?","['graph-theory', 'discrete-mathematics', 'hamiltonian-path']"
4087934,"Given a pair of integers, double one and increment the other until equality","I have a math problem, and I have no idea how to solve it. I first saw it over on the Code Golf StackExchange, here . The poster there hinted at knowing that there is a proof, but no proof was provided. We start with a pair of integers $a$ and $b$. We double one and add one to the other. We have the power to decide which to double and which to increment. We repeat this ""doubling/+1"" process, until the two integers are equal. For example, starting with $(2, 5)$ we can double the 5 and increment the 2 to give $(3, 10)$. Then, we can double the three and increment the 10 for $(6, 11)$. We can double the six and increment the 11 for $(12, 12)$ - and now we have made the numbers equal. Given any pair of integers, it it always possible to make them equal using these steps? Partial Proof All pairs of negative numbers will terminate. This relies on the fact that doubling 0 is 0. If we start with a pair of negative numbers, we can repeatedly add one to one of the numbers until it hits 0. While the other number will have been doubled several times, we can repeatedly decrement it until it too, hits 0. $(-6, -3) \to (-12, -2) \to (-24, -1) \to (-48, 0) \to (-47, 0) \to (-46, 0) \to \cdots \to (0, 0)$","['contest-math', 'puzzle', 'logic']"
4087967,Prove the inequality for the given condition,"Suppose that the boxes $B_1, . . . , B_n ⊂ R^d$ pairwise touch each other, that is, any
two of them are separated and have a common boundary point. Prove that $n ≤ 2^d$ I am trying to use Brunn-Minkowski inequality for brick set and the concept of volume to solve this. Like I am taking a hyperplane H that exactly divide it into two equal parts so that $B_1,. .,B_\frac{n}{2}$ lies in one plane i.e. $H^+$ and $B_\frac{n}{2},..,B_n$ lies in $H^-$ plane. But I am unable to proceed from here. Is there any other method to do so? Any help is appreciated. Thanks in advance.","['discrete-geometry', 'convex-geometry', 'geometry', 'real-analysis', 'inequality']"
4087968,"Expanding $\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x$","I'm having some difficulty expanding the following integral: $$\int_{\Omega}|D^2u|^2(x,t)\;\mathrm{d}x=\int_{\Omega}(D^2u\cdot D^2u)(x,t)\;\mathrm{d}x$$ where $\Omega\subset\mathbb{R}^n$ and $u$ is in $\mathbb{R}^n$ . I want to use integration by parts (which I believe is the way to go here) but haven't been able to start anywhere. I'm very familiar with using integration by parts to solve the related integral $$\int_{\Omega}Du\cdot Dv\;dx=-\int_{\Omega} u\Delta v\;\mathrm{d}x+\int_{\partial\Omega}\frac{\partial v}{\partial\nu}u\;\mathrm{d}s$$ but I haven't been able to achieve the same success in this scenario. Could any of Green's Formulas help? Thanks!","['multivariable-calculus', 'calculus', 'partial-differential-equations']"
4087991,Why are the polygons formed by many extended straight lines in a plane convex?,"I was looking over a chapter in my textbook on linear programming , when I thought of something interesting, but probably unrelated. On a plane, many straight lines - each one extended to infinite length in both directions - are drawn. Let $x$ be a point (the red dot in the diagram below) in the plane so that it is inside (or on the boundary of) a polygon formed by parts of the straight lines. Then the smallest polygon - in terms of area - that contains $x$ (i.e. the green polygon) is convex! Why is this? I don't think we have a convex hull to work with in an attempted proof. Hmmm... if you join up all the intersection points of the lines then you just get lots of triangles. It’s not obvious how this helps though. $$$$ Note that this doesn't work if the lines themselves are strictly convex (or concave), e.g.: The green shape is not convex because the straight line joining the red dots does not lie inside the green shape (this is the definition of convex).","['convex-geometry', 'geometry', 'plane-geometry']"
4088006,False proof that $\frac{13}{6}=0$,"At start, the length of a line segment is $a_0=0$ . When $3$ hours have elapsed since start, its length is $a_3$ . When $1$ hour has elapsed since start, its length increased by $\frac{a_3}{2}$ with respect to $a_0$ (call the new length $a_1$ ). When $2$ hours have elapsed since start, its length increased by $\frac{a_3}{3}$ (call the new length $a_2$ ) with respect to $a_1$ . When $3$ hours have elapsed since start, its length increased by $\frac{a_3}{4}$ with respect to $a_2$ . What is the value of $\frac{a_3}{a_1}$ ? At $t=0$ , the length is $a_0=0$ . At $t=1$ hour, the length is $a_1=\frac{a_3}{2}$ . At $t=2$ hours, the length is $a_2=\frac{a_3}{2}+\frac{a_3}{3}$ . At $t=3$ hours, the length is $\color{red}{a_3=\frac{a_3}{2}+\frac{a_3}{3}+\frac{a_3}{4}=\frac{13}{12}a_3}$ . So $$\frac{a_3}{a_1}=\frac{\frac{13}{12}a_3}{\frac{a_3}{2}}=\frac{13}{6}$$ is the answer. But notice that the red equation enables solving for $a_3$ which gives $a_3=0$ . Therefore $\frac{a_3}{a_1}=0$ and, by transitivity, we have: Conclusion: $\frac{13}{6}=0$ Where is the mistake?","['word-problem', 'algebra-precalculus', 'fake-proofs']"
4088046,What do these alternative formulations of the second Borel-Cantelli lemma (Durrett theorem 4.3.4 and 4.5.5) say?,"I am reading Durrett and I don't understand what do these formulations of the second Borel-Cantelli lemma say: Specifically, what does $P(B_n|F_{n-1})$ mean?","['borel-cantelli-lemmas', 'stochastic-processes', 'probability-theory']"
4088079,Finding the right conditions for a sequence to satisfy convergence in distribution then convergence in probability then convergence almost surely,"I couldn't solve this exercice so I could use some help. It goes like this : Let $(X_n)_{n \in\Bbb N^*}$ be a sequence of real independent random variable defined in some probability space. Let $F_{X_n}$ be the cumulutive distribution function such that $$F_{X_n}=\begin{cases}
1 & \quad   x > 1\\
a_n+(1-a_n)x^n & \quad x \in[0,1] \\ 
0 & \quad  x <0
\end{cases}$$ $a_n\in [0,1] $ Question : Find the right conditions for $a_n$ to have : 1/ Convergence in distribution. 2/ Convergence in probability. 3/Almost sure convergence. ( note that the questions are seperated so it needs to be solved one at a time ).","['probability-distributions', 'sequences-and-series', 'convergence-divergence', 'probability-theory', 'probability']"
4088114,A sufficient condition for the singularity of two measures,"I meet this problem in the proof of the Feldman-Hajek theorem from Stochastic Equations in infinite dimensions written by Da Prato. Two probability measures $\gamma$ and $\mu$ are singular  iff there exists a Borel set $A$ such that $\gamma(A)=1,\mu(A^c)=0$ . Given two probability measures $\gamma$ and $\mu$ on a topological measurable space $(H,\mathcal{B}(H))$ where $\mathcal{B}(H)$ stands for the Borel $\sigma$ -field of $H$ . Assume there exists a sequence of random variables $\{\xi_n\}$ on $(H,\mathcal{B}(H))$ such that $\int_H \xi_n^2\mu(dx)=1$ for every $n\in N$ and $$\lim_{n\rightarrow \infty}\int_H \xi_n^2\gamma(dx)=0.$$ Now it comes the confusing part. The author says that there exists a sequence $\gamma_k \uparrow \infty$ and $n_k\uparrow \infty$ such that $$\mu(\{x\in H:\lim_{n\rightarrow\infty}|\gamma_k\xi_{n_k}(x)|=+\infty\})=1$$ $$\gamma(\{x\in H:\lim_{n\rightarrow\infty}|\gamma_k\xi_{n_k}(x)|=0\})=1$$ And thus $\gamma$ and $\mu$ are singular. I wonder how the series $\gamma_k $ are constructed. Clearly they have to be randomly chosen and measurable! ——————————————————————————————————————————————————— Thanks @LostStatistician18 for reminding me of some conditions I may have missed. I find $\xi_{n}$ should be symmetrically normally distributed both under $\mu$ and $\gamma$ .","['stochastic-analysis', 'measure-theory', 'probability-theory', 'real-analysis']"
4088117,"Prob. 18, Chap. 2, in Royden's REAL ANALYSIS: If $E$ has finite outer measure, then there is an $F_\sigma$-set $F$ and a $G_\delta$-set $G$ with ...","Here is Prob. 18, Chap. 2, in the book Real Analysis by H. L. Royden and P. M. Fitzpatrick, 4th edition: Let $E$ have finite outer measure. Show that there is an $F_\sigma$ set $F$ and a $G_\delta$ set $G$ such that $F \subseteq E \subseteq G$ and $m^*(F) = m^*(E) = m^*(G)$ . My Attempt: Case 1.  If set $E$ is a (Lebesgue) measurable set of real numbers, then by Theorem 11 (iv), Chap. 2, in Royden, there is an $F_\sigma$ set $F$ with $F \subseteq E$ and $m^*(E \setminus F) = 0$ , and by Theorem 11 (ii), there is a $G_\delta$ set $G$ such that $E \subseteq G$ and $m^*(G \setminus E) = 0$ . Note that the sets $F$ and $G$ both are measurable. Now since $E$ is measurable with finite outer measure and since $F \subseteq E$ , therefore $F$ also has finite outer measure. Thus we have $F \subseteq E \subseteq G$ and $m^*(E \setminus F) = 0$ and $m^*(G \setminus E) = 0$ , which by the excision property of measurable sets (i.e. sets $F$ and $E$ ) with finite outer measure yield $m^*(E) - m^*(F) = 0$ and $m^*(G) - m^*(E) = 0$ , and thus $$
m^*(F) = m^*(E) = m^*(G),
$$ as required. Is what I have done correct and accurate in each and every detail? If so, then how to tackle the case when $E$ has finite outer measure but is not measurable? PS: Case 2. Now suppose that set $E$ has finite outer measure but is not measurable. Then by Theorem 11 (ii) and (iv) in Royden, for any $F_\sigma$ set $F$ and any $G_\delta$ set $G$ with $F \subseteq E \subseteq G$ , we must have $m^*(E \setminus F) \neq 0$ and $m^*(G \setminus E) \neq 0$ , that is, $m^*(E \setminus F) >  0$ and $m^*(G \setminus E) > 0$ , which imply $E \setminus F \neq \emptyset$ and $G \setminus E \neq \emptyset$ ; in fact the sets $E \setminus F$ and $G \setminus E$ cannot even be countable. Now as $E = F \cup (E \setminus F)$ , so we have $$
m^*(E) = m^* \big( F \cup (E \setminus F) \big) \leq m^*(F) + m^*( E \setminus F),
$$ which implies $$
m^*(E) - m^*(F) \leq m^*( E \setminus F),
$$ but since $F \subseteq E$ , by the monotonicity of the outer measure we have $$
0 \leq m^*(E) - m^*(F) \leq m^*( E \setminus F). \tag{1} 
$$ And, as $G = E \cup (G \setminus E)$ , so we have $$
m^*(G) = m^* \big( E \cup (G \setminus E) \big) \leq m^*(E) + m^*( G \setminus E ), 
$$ which implies $$
m^*(G) - m^*(E) \leq m^*(G \setminus E),
$$ but since $E \subset G$ , by the monotonicity of the outer measure we have $$
0 \leq m^*(G) - m^*(E) \leq m^*(G \setminus E). \tag{2} 
$$ The relations (1) and (2) hold for any $F_\sigma$ set $F$ and for any $G_\delta$ set $G$ such that $F \subseteq E \subseteq G$ . What next? Where can we get from here? PS (Based on the comments by Tab1e): As set $E$ has finite outer measure, so by the definition of the outer measure, for each positive integer $n$ , we can find a countable collection $\left\{ I_{n, k} \right\}_{k=1}^\infty$ of non-empty bounded open intervals covering set $E$ for which $$
\sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n}, 
$$ Let us put $$
 G_n := \bigcup_{k=1}^\infty I_{n, k}.
$$ Then by our choice of the $I_{n, k}$ , we have $E \subset G_n$ , and also $$ m^*\left( G_n \right) = m^* \left( \bigcup_{k=1}^\infty I_{n, k} \right) \leq \sum_{k=1}^\infty m^* \left( I_{n, k} \right) = \sum_{k=1}^\infty l \left( I_{n, k} \right) < m^*(E) + \frac{1}{n}, $$ and hence we also have $$
m^* \left( G_n \right) < m^*(E) + \frac{1}{n}.
$$ Let us now put $$
G := \bigcap_{n=1}^\infty G_n.
$$ This set $G$ is of course a $G_\delta$ set, and simce for each positive integer $n$ we have $G \subset G_n$ and $E \subset G_n$ , we can conclude that $E \subset G$ also and therefore $$
m^*(E) \leq m^* (G) \leq m^* \left( G_n \right) < m^*(E) + \frac{1}{n},
$$ which in turn implies that $$
m^*(E) \leq m^*(G) < m^*(E) + \frac1n 
$$ for every positive integer $n$ , which upon taking the limit as $n \to \infty$ yields $$
m^*(E) = m^*(G). 
$$ Is this part of my post correct? If so, then how to give the proof for the $F_\sigma$ set?","['measure-theory', 'lebesgue-measure', 'outer-measure', 'analysis', 'real-analysis']"
4088128,"Parenthesized equalities: meaning of parenthesis before equal sign, e.g. $ \frac{n}{q}+\frac1{2q}\left(=\frac{n+1}{q}-\frac{1}{2q}\right)$","What does the parentheses before the equal sign mean? I’ve also never seen an inequality be used in an equation before like this. I’ve searched online but found no help. However, if anyone can be so kind as to send me a link to a text that can help, or take it up themselves to explain I’d be so grateful. Thank you.","['notation', 'algebra-precalculus']"
4088136,"$\sum_{j=1}^n|Z_1\cdots Z_j|$ converges in $L^1$ and a.s. for $Z_i\sim N(0,1)$","I'm trying to prove that $\sum_{j=1}^n|Z_1\cdots Z_j|$ converges in $L^1$ and a.s. as $n\to\infty$ , for $Z_i\sim N(0,1)$ independently. I think I should use that I have a sum of nonnegative r.v.'s with expectation $\sqrt{\frac2{\pi}}^{\ n}$ , which converges to $0$ exponentially. Is there a theorem that gives both $L^1$ and a.s. convergence of a sum of nonnegative r.v.'s with expectations that tend to $0$ exponentially? If not, do you know another approach that can work here?","['statistics', 'probability-theory']"
4088142,Why is this a first integral? - particle near Schwarzschild black hole,"Background I know that the Schwarzschild metric is: $$d s^{2}=c^{2}\left(1-\frac{2 \mu}{r}\right) d t^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} d r^{2}-r^{2} d \Omega^{2}$$ I know that if I divide by $d \lambda^2$ , I obtain the Lagrangian: $$
L=c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\theta}^{2}-r^{2} \sin ^{2} \theta \dot{\phi}^{2}
$$ (where we have also expanded $\Omega^{2}$ into $\theta$ and $\phi$ dependent parts but that's not tha main point). Overdots denote differentiation with respect to affine parameter $\lambda$ . The Euler-Lagrange equations are: $$\frac{\partial L}{\partial x^{\mu}}=\frac{d}{d \lambda}\left(\frac{\partial L}{\partial \dot{x}^{\mu}}\right)$$ Which is, for $x^{\mu}=r$ , $\theta=\pi/2$ , results in: $$\left(1-\frac{2 \mu}{r}\right)^{-1} \ddot{r}+\frac{\mu c^{2}}{r^{2}} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-2} \frac{\mu}{r^{2}} \dot{r}^{2}-r \dot{\phi}^{2}=0$$ Lets set $\theta=\pi/2$ for the remainder of this post. The problem I am happy with everything up to this point. Now my notes say: However, it is often more convenient to use a further first integral
of the motion, which follows directly from $L = c^2$ for a massive
particle, and $L = 0$ for a massless one: $$
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
$$ Why is this called a first integral? Isn't this just the Lagrangian? My notes from another course has this to say on first integrals: When $L\left(y(\lambda), y^{\prime}(\lambda) ; \lambda\right)$ has no explicit dependence on $\lambda$ , i.e. when $\frac{\partial L}{\partial \lambda}=0,$ then we have the
first integral $$
\dot{y} \frac{\partial L}{\partial \dot{y}}-L=\mathrm{const.}
$$ So why does the above quote claim that the Lagrangian itself is the first integral? and why not $\dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.}$ is my first integral? Attempted resolution Let's calculate $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ , in the hope that it might reveal that $\dot{r} \frac{\partial L}{\partial \dot{r}}-L=\mathrm{const.}$ and $
L=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
$ is the same thing put in a different way. $\frac{\partial L}{\partial \dot{r}}=-2\left(1-\frac{2 V}{r}\right)^{-1} \dot{r}$ Then $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ becomes: $$-\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}+r^{2} \dot{\phi}^{2}=\operatorname{const}$$ Flip signs, then, compare the two expressions: $$\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const}$$ $$
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
$$ We can see that some signs differ if I believe that the first integral is $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ and not $L$ itself. I am pretty sure though that the result I get using $\dot{r} \frac{\partial L}{\partial \dot{r}}-L$ is wrong, since we use the other result throughout the lecture notes and it seem to be working. I am mostly happy with the relation: $$
\left(1-\frac{2 \mu}{r}\right) c^{2} \dot{t}^{2}\bbox[5px,border:3px solid red]{-}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=\left\{\begin{array}{lc}
c^{2} & \text { massive } \\
0 & \text { massless }
\end{array}\right.
$$ This is true if the affine parameter is proper time and the particle is massive. (Then $ds^2=c^2d\tau^2$ , so $ds^2/d\tau^2 = c^2$ .) If the affine parameter cannot be proper time, then the particle travels with the $c$ and therefore it is a photon, which has null-like path, making $ds^2$ zero. I can make the leap of faith that if this is true for proper time as affine parameter it is true for non-proper time affine parameters. I am also happy with the relation: $$\left(1-\frac{2 \mu}{r}\right) c^{2}\dot{t}^{2}\bbox[5px,border:3px solid green]{+}\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}=-\operatorname{const}$$ because the derivation seems correct. Question reapproached What I am not happy with is calling the first relation a first integral. It is probably rightly called that, an exam question (PDF page 24, third paragraph from bottom) asking for (I think) that equation saying ""[...] use a simpler expression given by the first integral of the geodesic equations."" So I think there is something here which I don't get. Checking algebra of Othin's answer As suggested, lets calculate $\dot{t}\frac{\partial L}{\partial \dot{t}} - L=\operatorname{const}$ . $$\frac{\partial L}{\partial t}=2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t}$$ Then $$\dot{t}\frac{\partial L}{\partial \dot{t}} - L = \dot{t} 2 c^{2}\left(1-\frac{2 H}{r}\right) \dot{t} - \left(c^{2}\left(1-\frac{2 \mu}{r}\right) \dot{t}^{2}-\left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2}-r^{2} \dot{\phi}^{2}\right)=\operatorname{const}$$ ie $$c^{2}\left(1-\frac{2 H}{r}\right) \dot{t}^2 \bbox[5px,border:3px solid green]{+} \left(1-\frac{2 \mu}{r}\right)^{-1} \dot{r}^{2} \bbox[5px,border:3px solid green]{+} r^{2} \dot{\phi}^{2}=\operatorname{const}$$ Which is not $L$ , but close. (Signs are wrong.)","['mathematical-astronomy', 'euler-lagrange-equation', 'functional-analysis', 'general-relativity', 'mathematical-physics']"
4088165,Finite intersection of nonempty sets is not empty (Poincaré-Miranda theorem),"I am trying to understand a proof of the Poincaré-Miranda theorem given in this article .
So, the theorem states that if we have a continuous map $f = (f_1, \ldots, f_n) : I^n \rightarrow {\rm I\!R}^n$ where $I^n$ is a $n$ -dimensional cube $[-1, 1]^n$ that satisfies the following condition: $$
f_i(x_1, \ldots, x_{i-1}, -1, x_{i+1}, \ldots, n) \leq 0 \\
f_i(x_1, \ldots, x_{i-1}, 1, x_{i+1}, \ldots, n) \geq 0
$$ for each $i = 1, \ldots, n$ , there exists a point $c \in I^n$ such that $f(c) = 0$ . To prove the theorem authors defined sets $$
H_i^- := f_i^{-1}(- \infty , 0] \\
H_i^+ := f_i^{-1}[0, \infty)
$$ Then they say that ""using a compactness argument we infer that the intersection $$
H := \bigcap \{ H_i^- \cap H_i^+ : i=1, \ldots, n\}
$$ is not empty."" I see that each $H_i^-$ and $H_i^+$ is not empty as it follows from the propositions, but why are $H_i^- \cap H_i^+$ nonempty and why is $H$ not empty? Additionally, if $H$ is indeed not empty, it is obvious that for every $x \in H$ it follows $f(x) = 0$ , so we found the zero. Why does not the proof just end here? Thanks to anyone who could help me understand.","['roots', 'analysis', 'continuity', 'functions', 'differential-topology']"
4088170,"Why Doesn't My Proof, That Rational Numbers Are Uncountable, Work? (Using Cantors Argument) [duplicate]","This question already has answers here : Why does Cantor's diagonal argument not work for rational numbers? (2 answers) Why does Cantor's Proof (that R is uncountable) fail for Q? (1 answer) Closed 3 years ago . First I'd like to recognize the shear number of these ""anti-proofs"" for Cantor's Diagonalization Argument, which to me just goes to show how unsatisfying and unintuitive it is to learn at first. It really gives off a ""I couldn't figure it out, so it must not have a mapping"" kind of vibe. I have read these posts [ 1, 2, 3, ] and do recognize many of the mistakes though potentially not all so I apologize if I end up repeating some of them. That said here is my attempted proof which I know must be wrong but don't understand why. Proof by Contradiction: Since all subsets of a countable set are countable, if rational numbers are countable then the subset $(0,1)\in \mathbb{Q}$ is therefore countable. Where $i \in \mathbb{N}$ and $n,d\in \mathbb{N}$ let: $$
q_i=\frac{2^{n_{i1}}\cdot 3^{n_{i2}} \cdot 5^{n_{i3}} \cdot 7^{n_{i4}} \cdot ... }{2^{d_{i1}}\cdot 3^{d_{i2}} \cdot 5^{d_{i3}} \cdot 7^{d_{i4}} \cdot ... },  \text{where} \,n_{ij}\ne0\to d_{ij}=0 \, \text{and} \,d_{ij}\ne0\to n_{ij}=0
$$ By using the fundamental theorem of arithmetic we know that we can represent all integers in both the numerator and denominator of $q_i$ and that $q_i\ne q_k$ unless for all index values of $\mathbf{n}_i$ and $\mathbf{d}_i$ are equal to those of $\mathbf{n}_k$ and $\mathbf{d}_k$ . With this we can represent the set $(0,1)\in\mathbb{Q}$ as $\{q_1, q_2, q_3,...\}$ . However, using Cantor's Diagonalization Argument we can create: $$
q=\dfrac{2^{n_{11}}\cdot 3^{n_{22}} \cdot 5^{n_{33}} \cdot 7^{n_{44}} \cdot ...}{2^{d_{11}\,+1}\cdot 3^{d_{22}\,+1} \cdot 5^{d_{33}\,+1} \cdot 7^{d_{44}\,+1} \cdot ...}
$$ The new value $q$ will not be equal to any elements in $\{q_1, q_2, q_3, ...\}$ and it also isn't necessarily a larger number or smaller number (so I'm not just counting up or down by one). The value $q$ will also need to exist in the subset. Any help pointing out my mistakes will help me finally seal my unease with Cantor's Diagonalization Argument, as I get how it works for real numbers but I can't seem to wrap my mind around it not also being applied to other sets which are countable.","['elementary-set-theory', 'cardinals', 'rational-numbers']"
4088210,Proof that $a^2+b^3+c^4=k$ modulo $m$ always has a solution,"Question Given integers $m\ge1$ and $0\le k<m$ , I want to show that $a^2+b^3+c^4=k \pmod m$ always has a solution. Attempt It’s easy to show that it is enough to prove this when $m$ is a prime or a power of a prime. I think I can show it for $m=p^k, k \ge 2$ , if it is true whenever $m = p$ for some prime $p$ . Using the fact that every prime has a primitive root, it is true whenever $m$ is a prime of the form $p = 6n-1$ because $b^3$ takes all values modulo $p$ . For other $p$ , $a^2$ has $(p-1)/2 + 1$ different values modulo $p$ , $b^3$ has $(p-1)/3 + 1$ different values, and $c^4$ has $\frac{p-1}4 + 1$ different values modulo $p$ if $p-1$ is divisible by $4$ , and $\frac{p-1}2 +1$ otherwise. So there are at least $\frac{p^3}{24}$ sums which need to take $p$ different values. Because of the huge number of combinations, I think the conjecture is true, but I can’t prove it! PS. This is a necessary condition for the conjecture “every integer is the sum of a square, a cube (of a possibly negative integer), and a fourth power.” which may be quite hard to prove.","['number-theory', 'modular-arithmetic']"
4088226,Limit of a real sequence [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Let $$x_{n}=\frac{1}{n+2^{0}}+\frac{1}{n+2^{1}}+...+\frac{1}{n+2^{n}}\quad (n\in\mathbb{N}, \text{ }n\geq 1).$$ What is the limit of $x_n$ ? Attempt: Just the boundedness, $x_n∈(0,2)$ , $\forall n\geq 1$ . Can not even determine the monotonicity.","['limits', 'sequences-and-series']"
4088229,Fourier Sine Series and Fourier Cosine Series,"A regular Fourier Series will turn out to contain only sine terms if the target function is odd, only cosine terms if the target function is even, and both sine terms and cosine terms if the target function is neither even nor odd. By contrast, my understanding is that Fourier Sine Series and Fourier Cosine Series are completely different animals, and that the mathematician can choose to represent their target function using a regular Fourier Series, a Fourier Sine Series, or a Fourier Cosine Series at their discretion by temporarily introducing a regular extension, odd extension, or even extension, respectively, regardless of the target function's actual even or odd status.  It is perhaps natural to use regular Fourier Series to represent functions within intervals centered around $0$ , and Fourier Sine Series or Fourier Cosine Series to represent functions within intervals whose left boundary is $0$ , but it should be possible to simply $u$ -substitute any finite interval function into either of these forms, do the Fourier Series/Fourier Sine Series/Fourier Cosine Series in $u$ , and then revert the substitution. On the other hand, this would mean that any function (continuous or piecewise smooth, a.k.a., any function that could be represented by a regular Fourier Series) could be represented by a single family of sinusoids, either sines or cosines, which seems to be at odds with the Linear Algebra terminology often associated with Fourier Series.  For example, thinking of the sines and cosines in a Fourier Series as a basis leads me to believe that you should expect to require both to represent an arbitrary function, not just one or the other. Is it always possible to represent any function that could be represented by a regular Fourier Series by your choice of Fourier Sine Series or Fourier Cosine Series?  If so, how is this in concord with the Linear Algebraic explanation of Fourier Series?","['fourier-analysis', 'functions', 'linear-algebra', 'fourier-series', 'substitution']"
4088242,How many different 4-digit numbers can be made with the digits from 12333210?,"How many different 4-digit numbers can be made with the digits from $12333210$ ? Attempt. So I've tried splitting into cases: Case 1: Only single letters. 1 2 3 0, except for when 0 is at the first place, so total arrangements minus the arrangements where 0 is in the first place. That is: $$4!-3!$$ (I think) Case 2: One double only. So either (the ""[]"" represent the remaining letters for the other 2 places), 1 1 [2,3,0], or 2 2 [1,3,0], or 3 3 [1,2,0]. $$\frac{4P2}{2!}\cdot 3P2\cdot 3-\frac{3P2}{2!}\cdot 2P1\cdot 3$$ the $4P2$ is for the double to be arranged in the any 2 of 4 places, the $3P2$ is the 3 remaining letters to be arranged in the 2 remaining places, and times 3 because we have 3 cases of this as I wrote above, and finally substracting the other term for the arrangements where 0 is in the first place, $3P2$ to arrange the double onto the remaining 3 places, $2P1$ to arrange the remaining 2 letters onto the last space and 3 times due to the 3 cases. And any of the $/2!$ is to account the repeated digits on the doubles. Case 3: 2 doubles?? I'm unsure whether I should compute this because it may be accounted on the 2nd case already Case 4: 1 triple. The only way to exist a triple is the one such that 3 3 3 [0,1,2], substracting once again the times where 0 is in the first place: $$\frac{4P3}{3!}\cdot 3P1-1$$ $\frac{4P3}{3!}$ on account of the triple to be arranged on the 4 places, $3P1$ for the 3 remaining letters to be arranged at the remaining space and $-1$ on account of the arrangement where 0 is in the first place and followed by 333. Hence my final answer would be to add all these 3 or 4 cases, but I'm unsure if any case arrangement is wrong, if I should account for the 3rd case or not, and if I've managed with the ""0 can't be on the first place"" correctly by accounting it on each case individually. And please, if I have something wrong with my arrangements calculations, please let me know why my thinking is incorrect, I've added the reason to everything I've posted as a result for a reason.","['combinatorics-on-words', 'combinatorics']"
4088274,Sphere falling in viscous fluid boundary condition,"A sphere, radius $a$ , falls at constant velocity $U_0$ through an incompressible
viscous fluid. I am told to assume that \begin{equation}
\mathbf{u} = U_0\mathbf{e}_z + \tilde{\mathbf{u}}(\mathbf{x})
\end{equation} where \begin{equation}
\tilde{\mathbf{u}}(\mathbf{x})=\nabla\times\nabla\times(U_0f(r)\mathbf{e}_z).
\end{equation} From this I have shown that \begin{equation}
\tilde{\mathbf{u}}(\mathbf{x})=-U_0\Delta f \mathbf{e}_z
\end{equation} and by taking the curl of the equations for Stokes flow, \begin{equation}
\Delta^2 f= \text{ constant.}
\end{equation} where $\Delta^2 f=\Delta(\Delta f)$ . I also know that \begin{equation}
\Delta f\rightarrow0 \text{ as } r\rightarrow\infty.
\end{equation} Here is where I am confused I need to show that $f(r)=Ar+\frac{B}{r}$ . I tried to do so by the Ansatz $f=r^a$ and found that $a=-1,0,1,2,4$ are all suitable for $\Delta^2 f=$ constant to hold. So $f(r)=Ar+\frac{B}{r}+C+Dr^2+Er^4.$ To satisfy $\Delta f\rightarrow0 \text{ as } r\rightarrow\infty$ , I set $D=E=0$ . But I cannot explain why $C=0$ . Why is this true? I also don't know how to set $A,B$ I think that $\textbf{u}=U_0\textbf{e}_z \text{ at } r=a$ for the no-slip condition, which sets $\tilde{\textbf{u}}=-U_0\Delta f\textbf{e}_z=0 \text{ at } r=a \implies A=0$ . So now I have $f(r)=\frac{B}{r}$ , and somehow I have to use the fact that $\nabla\times\textbf{e}_\phi=\frac{cot\theta}{r}\textbf{e}_r-\frac{1}{r}\textbf{e}_\theta$ to find $B$ . I am completely lost here.","['multivariable-calculus', 'mathematical-physics', 'fluid-dynamics']"
4088321,Intuition or Geometric model for Dicyclic Groups?,"When studying groups of order 12, I learned about dicyclic groups, of which the quaternion group is the first example, and there is one of every order $4n$ for $n > 1$ . I can do the basic calculations involving them, but I was wondering if there is some mechanical/geometric model I could use as an aid to intuition.  I know every finite group is a subset of the symmetric group on $k$ elements for some $k$ , but I was hoping for something more specific.  For example, the alternating group on four elements can be visualized as the rotations of a tetrahedron.  I haven't been able to find one on my own, and I was wondering whether that is because the ""simple visualization"" is four dimensional, like the quaternions? I tried drawing the elements in two superimposed rings, with a variety of orderings of elements, but I wasn't able to represent the actions of the group elements, the way the symmetries of an equilateral triangle can be thought of as positions or as actions (which I think of as the ""noun and verb problem"".) Is there a common model in use for dicyclic groups?  Or should I just get used to them as an abstraction with no picture?","['visualization', 'finite-groups', 'geometry', 'intuition', 'group-theory']"
4088323,Asymptotic expansion of q-Pochhammer symbol near q = 1,"I'd like to understand the asymptotics of the q-Pochhammer symbol $(a;q)_\infty$ as $q \to 1^-$ with $a$ complex, where $$(a;q)_\infty = \prod_{n = 0}^\infty (1- aq^n).$$ More specifically, I'm actually just interested in the limiting behavior as $a$ approaches an arbitrary point on the unit circle in the complex plane: $(q^x e^{i \theta}; q)_\infty$ as $q \to 1^-$ , with $x$ and $\theta$ real.  I managed to find a partial answer in this paper , which in theorem 3.2 gives the asymptotic expansion $$(q^x; q)_\infty = \frac{\sqrt{2\pi}}{\Gamma(x)}\left(\ln\frac{1}{q}\right)^{\frac{1}{2}-x} \prod_{k = 0, k \neq 1}^\infty \exp\left[\frac{\zeta(2-k)}{k!} B_k(x) (\ln q)^{k-1}\right]$$ for $x > 0$ , where $\zeta(2-k)$ is the Riemann zeta function and $B_k(x)$ are Bernoulli polynomials.  This is just the $\theta = 0$ , $x > 0$ version of what I'm looking for.  Doing some numerical checks, it seems that this expansion is still valid when extended to complex $x$ (at least in some neighborhood of $x = 0$ , which is all I've checked), but this generalization isn't sufficient to get the expansion I want: in the expansion above, the argument $a = q^x$ always approaches 1 as $q \to 1$ even for complex $x$ , whereas I want $a = q^x e^{i\theta}$ to approach an arbitrary point on the unit circle. Is there some other expansion that applies in the regime I'm interested in?  Or perhaps is there a way to modify the above expansion to work for general $\theta$ ?","['special-functions', 'asymptotics', 'complex-analysis', 'pochhammer-symbol', 'infinite-product']"
4088335,"What are some interesting and unusual theorems, identities, notations in Trigonometry?","Context/Motivation When studying a subject I usually like to create a document and make a kind of study guide, notes about the subject. I created the document and write using LaTeX. Also, besides learning about the subject itself (it is math or something related to robotics) I practice LaTeX and English because I write it in English.  Taking trigonometry , the topic of this question, I would write lecturing myself and go through concepts, theorems, formulas detailing as much as possible. Yesterday night I was wondering about making notes about trigonometry and it reminded me of a notation that I did find unusual when I first saw it, but it was interesting regarding the Domain of the tangent function. Working with trigonometry function such that $f:\mathbb{R}\rightarrow\mathbb{R}$ , we have $f(x)=\tan(x)=\dfrac{\sin(x)}{\cos(x)}$ . As $\cos(x)\neq 0$ the domain of the tangent function is usually written as $\operatorname{Dom}(\tan) = \left\{x: x \neq \dfrac{\pi}{2}+k\pi, k \in \mathbb{Z} \right\}$ . We have $\cos(x) = 0$ for $x=\pm\dfrac{\pi}{2}, \pm\dfrac{3\pi}{2}, \pm\dfrac{5\pi}{2}...$ In fact, taking the intervals we have $$... \quad \cup\left(-\dfrac{3\pi}{2},-\dfrac{\pi}{2}\right) \cup \left(-\dfrac{\pi}{2},\dfrac{\pi}{2}\right)\cup \left(\dfrac{\pi}{2},\dfrac{3\pi}{2}\right)\cup \quad ...$$ For $k\in\mathbb{Z}$ . Considering $x_k = \dfrac{\pi}{2}+k\pi = \dfrac{2k\pi+\pi}{2}= \boxed{\dfrac{(2k+1)\pi}{2}}$ , we can note that the intervals are determined by these $x_k$ points, so $$(x_k, x_{k+1})= \left(\dfrac{(2k+1)\pi}{2}, \dfrac{(2k+3)\pi}{2}\right)$$ Once we have an infinity quantity of unions, we can represent it using the big union notation. This is a notation I was not familiar, that is why I found it very interesting. Therefore, we have $$\operatorname{Dom}(\tan) = \left\{x: x \neq \dfrac{\pi}{2}+k\pi, k \in \mathbb{Z} \right\} = \bigcup_{k=-\infty}^{\infty} \left(\dfrac{(2k+1)\pi}{2}, \dfrac{(2k+3)\pi}{2}\right)$$ Question I'm afraid this question might be off-topic but I am asking this here because it seems to be a topic where experience is very important. I am looking identities , theorems , concepts , notations that are not very common, usual in regular books, textbooks. It is just out of my curiosity, but I am really interested in these new things. I'm sure the example I gave above might not be impressive for university students or most people who use MSE, but I was very surprised when I first saw it. More precisely, I am looking for answers of the following kind: Trigonometric Identities that are unusual that might/might not have interesting applications; Notations that are not very common; Concepts, Theorems that are usually not seem is regular high-school but actually presents interesting ideas; University trigonometry concepts with interesting ideas/application in high schools trigonometry; The question is certainly very wide regarding the topic, but any interesting information is very welcomed.","['trigonometry', 'soft-question']"
4088339,Symmetric random walk on a cube [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question A particle is on a symmetric random walk on the vertices of a cube that looks like this: The particle starts in $A$ has to jump to bordering vertex after one second. Every bordering vertex has a $1/3$ probability of being jumped to. What is the probability that the particle reaches $H$ before it reaches $B$ and how long does it take the particle on average to reach one of the two? I am not quite sure where to start, can someone help me?","['random-walk', 'markov-chains', 'stochastic-processes', 'probability-theory', 'probability']"
4088343,Complex functions and real function [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question This is a brief part of an exercise related to differential geometry on the minimal surface Scherk. However I am stuck in an analytical part. Let $$F(x,y)= \arg\left(\frac{z+i}{z-i}\right)$$ where $z = x + iy$ with $z$ different from $\pm i$ , and $\arg$ is the argument of the point with the real axis. Let $$\phi(z) = \frac{\partial F}{\partial x} - i\frac{\partial F}{\partial y}$$ Show that $\phi$ is analytic. Any hints on substitutions or steps to find out what are $\partial_{x}F$ and $\partial_{y}F$ ? ?","['complex-analysis', 'functions', 'partial-derivative', 'derivatives', 'analytic-functions']"
4088382,Analytic solution to a Volterra integral equation with kernel divided by continous differentiable function,"I have been struggling with the following Volterra integral equation for days: $$f(x)=A_{1}-\frac{A_{2}}{h(x)}+\lambda \int_{a}^{x}\frac{h^{\prime
}(s)}{h(x)}f(s)ds$$ , where a, A1 and A2 are positive constants, $\lambda$ is a negative constant, and $h(x)$ is a continuously differentiable function. It is a Volterra integral equation of second kind, and I have tried Adomian Decomposition Method or converting the problem to an ODE of initial value, but I am not able to solve it even if I consider the simplest case: h(x)=x.
Does this type of integral equation have analytic solution? May Laplace transformation help to solve it?
If I have to resort to numerical methods, are there available libraries for Mathematica, Matlab or other software I could use? Any help will be greatly appreciated.","['integration', 'laplace-transform', 'ordinary-differential-equations']"
4088409,What is the expectation of the largest eigenvalue of a Wishart distributed matrix?,"Let $\mathcal{S}_{++}^d$ denote the space of (symmetric) positive definite matrices of size $d \times d$ , where $d\in \mathbb{N}$ is given.
The density function of the $\mathrm{Wishart}_d(\nu,\mathbb{M})$ distribution is defined by \begin{equation}\label{eq:Wishart.density}
    f_{\nu,\mathbb{M}}(\mathbb{X}) = \frac{|\mathbb{X}|^{\nu/2 - (d + 1)/2} \exp\big(-\frac{1}{2}\mathrm{tr}(\mathbb{M}^{-1} \mathbb{X})\big)}{|2 \, \mathbb{M}|^{\nu/2} \pi^{d(d-1)/4} \prod_{i=1}^d \Gamma\Big(\frac{\nu}{2} - \frac{i - 1}{2}\Big)}, \quad \mathbb{X}\in \mathcal{S}_{++}^d,
\end{equation} where $\nu > d - 1$ is the number of degrees of freedom, and $\mathbb{M}\in \mathcal{S}_{++}^d$ is the scale matrix. If $\mathbb{S} \sim \mathrm{Wishart}_d(\nu,\mathbb{M})$ and $\lambda_{\mathrm{max}}(\mathbb{S})$ is the largest eigenvalue of $\mathbb{S}$ , then what is $\mathbb{E}[\lambda_{\mathrm{max}}(\mathbb{S})]$ ? I can't find any reference with a clear answer to this question.","['statistics', 'probability']"
4088473,Distributions of a group scheme as differential operators.,"I'm trying to understand the distributions on an affine group scheme act as differential operators. Let $X$ be a scheme and $G$ a group scheme both affine over some commutative ring $k$ . Suppose $\alpha:G\times X\to X$ is an action on $X$ . Then there is a morphism between the coordinate algebras $\Delta:k[X] \to k[G]\otimes k[X]$ , which makes $k[X]$ a comodule. Let $I$ be the kernel of the augmentation map $\varepsilon:k[G]\to k$ . A distribution of order $\leq n$ on $G$ is a $k$ -linear map $\mu:\frac{k[G]}{I^{n+1}} \to k$ . Each distribution induces a $k$ -linear endomorphism on $k[X]$ via $$D_{\mu}: k[X] \to_\Delta k[G]\otimes k[X] \to _{\mu\otimes \text{id}} k\otimes k[X] \to_\cong k[X] $$ Jantzen claims in chapter 7 of Representation of Algebraic Groups , that this endomorphism $D_{\mu}$ is a differential operator on $k[X]$ . He cites a book by Demazure and Gabriel, which I've also looked at, but they don't provide a proof either. I've been trying to prove it for quite a while now, but haven't made any progress. Does anybody know a proof of this or a resource to find one?","['differential-operators', 'algebraic-groups', 'algebraic-geometry', 'abstract-algebra', 'hopf-algebras']"
4088492,How to solve the following limit similar to factorial type,"Let $\alpha>1$ be a fixed real number and $M(x)=\max\left\{m\in\mathbb{N}:m!\le\alpha^x\right\}$ , prove that $$\displaystyle\lim_{n\to\infty}\displaystyle\frac{\sqrt[n]{M(1)M(2)\cdots M(n)}}{M(n)}=e^{-1}.$$ Use $$n!\sim\sqrt{2\pi n}\left(\frac{n}{e}\right)^n.$$ I got that $$\lim\limits_{n\to\infty}\dfrac{\sqrt[n]{n!}}{n}=e^{-1}.$$ But I don't know how to go on. The properties of this function $M(x)$ should be critical.
Is there any way to parse this function?","['limits', 'real-analysis']"
4088522,Simple way to sample from a Gaussian distribution on SO(3) rotation group,"Given a mean and variance, I want to be able to sample rotations that are ""normally distributed"" according to this mean and variance. I have a metric over rotations, the chordal metric: $\| R_1 - R_2\|$ which tells us how close two rotations are to each other. How do I do this? One way is to consider SO(3) as a sphere $S^3$ in $R^4$ , the space of quaternions, but I don't know how to setup the Gaussian distribution correctly in this case considering a single rotation in SO(3) is an antipodal pair in $S^3$ .","['differential-geometry', 'probability-distributions', 'linear-algebra', 'probability', 'rotations']"
4088531,Functional equations satisfied by both sine and tangent functions.,"The functional equation identity,
(assuming also $\,f(-x)=-f(x)\,$ for all $\,x$ ), $$ f(a)f(b)f(a\!-\!b) + f(b)f(c)f(b\!-\!c)  + f(c)f(a)f(c\!-\!a)
 + f(a\!-\!b)f(b\!-\!c)f(c\!-\!a) = 0  \tag{1}$$ for all $\,a,b,c\,$ has solutions $f(x)=k_1\sin(k_2\,x)$ and $f(x)=k_1\tan(k_2\,x)\,$ with $\,k_1,k_2\,$ complex constants. As a limiting case of both $\,\sin\,$ and $\,\tan,\,$ $\,f(x)=k_1x\,$ is also a solution, and the simplest. I am looking only for non-zero solutions that
have a formal power series expansion. That is, $$ f(x) = a_1 \frac{x^1}{1!} + a_3 \frac{x^3}{3!}
 + a_5 \frac{x^5}{5!} + a_7 \frac{x^7}{7!}+ \cdots \tag{2}$$ is the exponential generating function
for the sequence $\,(0,a_1,0,a_3,0,a_5,0,\dots).\,$ For a solution of the above functional equation,
if $\,a_1=0\,$ then $\,f(x)\equiv 0.\,$ Otherwise, $\,a_1\ne 0\,$ and $\,a_3,a_5\,$ can be arbitrary while
the rest of the coefficients are determined uniquely.
I used Mathematica to compute the first few coefficients.
I found, for example, $$ a_7 \!=\! \frac{11 a_1 a_3 a_5\!-\!10 a_3^3}{a_1^2},
\;\;a_9 \!=\! \frac{21 a_1^2 a_5^2 \!+\!60 a_1 a_3^2 a_5
\!-\!80 a_3^4}{a_1^3},\;\;\dots. \tag{3}$$ I know of $18$ similar identities for $\,\sin\,$ and $\,\tan\,$ (including this one) in three or more variables.
They have some common features as follows. Each is an irreducible homogeneous polynomial equated to zero
where each monomial term in the polynomial is a product of
factors each of which is of the form $\,f(x)\,$ where $\,x\,$ is a variable or an integer linear combination of variables. I also require that $\,f(x) = k_1x\,$ is a solution in which
case the functional equation is a homogeneous algebraic identity. As a non-example , the similar looking non-homogeneous functional equation $$ f(a\!-\!b)\!+\!f(b\!-\!c)\!+\!f(c\!-\!a)\!-\!
f(a\!-\!b)f(b\!-\!c)f(c\!-\!a)\!=\!0 \tag{4}$$ has only the non-zero solutions $\,f(x) = \tan(k_2x),\; k_2\ne0 \,$ and thus, does not qualify. I am interested in those which are
satisfied by both $f=\sin$ and $f=\tan$ . In all but one of the identities of this kind that I know of,
they are also satisfied by the familfy of functions $\,f(x)=k_1\text{sn}(k_2\,x|m),\,$ where sn is a Jacobi elliptic function as well as two other related
elliptic function sc, sd. The one exception is for an
identity with Jacobi Zeta and Epsilon function solutions.
This leads to two natural questions. 1. Do identities exist with solutions aside from the Jacobi functions mentioned? 2. Do identities exist with only sine and tangent solutions? NOTE: Perhaps it would be easier to understand a specialization case.
Suppose there is only one variable $\,a.\,$ Consider the polynomial ring $\,\mathbb{Z}[f_1,f_2,f_3,\dots].\,$ In the first functional equation $(1)$ replace $\,b\,$ with $\,2a,\,$ and $\,c\,$ with $\,-2a\,$ to get the equation $$ f(a)f(3a)f(4a)-f(2a)^2f(4a)+f(a)f(2a)f(3a)-f(a)^2f(2a) = 0.\tag{5} $$ The polynomial equation associated with this equation is $$ f_1f_3f_4-f_2^2f_4+f_1f_2f_3-f_1^2f_2 = 0 \tag{6}$$ where $\,f_n:=f(na).$ This single polynomial equation also has solutions $\,f(x)=k_1\text{sn}(k_2\,x|m)\,$ and
seems to be the simplest such equation for the Jacobi sn function.
There are an infinite number of other equations which come from
specializing the first functional equation $(1)$ . I conjecture
that there is some kind of basis for the ideal of all such
equations. The issues raised here are similar to the ones for my
""Dedekind Eta-function  Identities"" list and studied by Ralf
Hemmecke in his 2018 article ""Construction of all polynomial
relations among Dedekind eta functions of level N"". NOTE: The 18 identities I refer to are in my file Special Algebraic Identities (ident04.txt) along with hundreds of special algebraic identities (also
available via the Wayback Machine).","['formal-power-series', 'functional-equations', 'trigonometry', 'elliptic-functions']"
4088573,"Does any polygon with side number $2n$ with $n \ge 2$ form a torus when all pairs of opposite sides are joined? (works for n=2, 3)","Wikipedia's Eisenstein integer; Quotient of C by the Eisenstein integers says: The quotient of the complex plane C by the lattice containing all Eisenstein integers is a complex torus of real dimension 2. This is one of two tori with maximal symmetry among all such complex tori. [citation needed] This torus can be obtained by identifying each of the three pairs of opposite edges of a regular hexagon. (The other maximally symmetric torus is the quotient of the complex plane by the additive lattice of Gaussian integers, and can be obtained by identifying each of the two pairs of opposite sides of a square fundamental domain, such as [0,1] × [0,1].) This answer explains and shows how identifying opposite sides of a hexagon produces a torus, as do answers to What surface do we get by joining the opposite edges of a hexagon? Question: Bringing opposite sides of a quadrilateral and a hexagon both produce a torus. Does any polygon side number $2n$ with $n \ge 2$ form a torus when all pairs of opposite sides are joined? From the linked answer: Image source: http://www.math.cornell.edu/~mec/Winter2009/Victor/part1.htm",['geometry']
4088583,Generalized Hertzsprung Problem,"The Hertzsprung Problem goes as follows: In how many can we place exactly $n$ non-attacking kings on a $n \times n$ chessboard such that there is exactly $1$ king in each row and column where $n \in \mathbb{N}$ . It turns out that there exists a nice closed for the same (though painful to evaluate for any $n \geqslant 5$ ) which is $$n!+\sum_{k=1}^n {(-1)^k}(n-k)!\sum_{i=1}^k 2^{i} \binom{k-1}{i-1}\binom{n-k+1}{i}$$ which can be obtained using simple Principle of inclusion-exclusion and stars and bars argument. However what if I want to place exactly $2$ kings in each row and column on a $n \times n$ chessboard such that no two kings attack each other on the chessboard? In how many ways can I do this? I tried this for a long time and was able to find a tedious closed form using some restricted permutations but I'm not satisfied with it. Does anyone have any idea if there exists a nice closed form for this? If yes, then which argument did you use? I call this Generalized Hertzsprung problem : In how many ways can we place non-attacking kings on a $n \times n$ chessboard such that there are exactly $m$ kings in each row and column? Note that here $m, n \in \mathbb{N}$ such that $m < n$ . Any advice on how to approach the generalized version? Your help would be highly appreciated. Thanks.","['chessboard', 'inclusion-exclusion', 'combinatorics', 'algebraic-combinatorics']"
4088614,How to determine whether to use complement of event when calculating probability?,"A sample problem in my textbook states: In a recent year, there were 18,187 U.S. allopathic medical school seniors who applied to residency programs and submitted their residency program choices. Of these seniors, 17,057 were matched with residency positions, with about 79.2% getting one of their top three choices. Medical students rank the residency programs in their order of preference, and program directors in the United States rank the students. The term “match” refers to the process whereby a student’s preference list and a program director’s preference list overlap, resulting in the placement of the student in a residency position. Find the probability that a randomly selected senior was matched with a residency position and it was one of the senior’s top three choices. Find the probability that a randomly selected senior who was matched with a residency position did not get matched with one of the senior’s top three choices. 1 is obvious: $(\frac{17057}{18187})(0.792) \approx 0.743$ For 2, the textbook says to take the complement of 0.792: $1 - 0.792 = 0.208$ But my question is: why wouldn't you solve this one the same way as part 1? As in: $$(\frac{17057}{18187})(1 - 0.792) \approx 0.195$$ why wouldn't you do that?","['statistics', 'probability']"
4088618,Relation between Hilbert-Schmidt inner product and tensor products,"Suppose $A$ is a Hilbert-Schmidt operator on a Hilbert space $\mathcal{H}$ , then when is it true that $$\langle Ax, x \rangle_{\mathcal{H}} = \langle A, x \otimes x \rangle_{\text{HS}}, \quad \forall \, x \in \mathcal{H}$$ I know from the definition of Hilbert-Schmidt inner product and the definition of tensor product that $$
\langle A, x \otimes x \rangle_{\text{HS}} = \sum_{j \in J} \langle A e_j, \langle x, e_j \rangle_{\mathcal{H}} x \rangle_{\mathcal{H}}
$$ where $\{e_j\}_{j \in J}$ is an arbitrary ONB of $\mathcal{H}$ . However, I am unable to simplify further. Any help would be appreciated. If the above result is incorrect, in general, I am interested in going from an expression of the form $\langle Ax, x \rangle_{\mathcal{H}}$ to an expression of the form $\langle A, x \otimes x \rangle_{\text{HS}}$ . How could I do that? Also, I would really appreciate some references where I can look into these kind of results in more detail.","['inner-products', 'operator-theory', 'hilbert-spaces', 'tensor-products', 'functional-analysis']"
