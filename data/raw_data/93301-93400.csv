question_id,title,body,tags
1273703,Are there any limits on Standard Deviation of a data set with given $n$ and mean?,"Say a class of 200 students is graded out of 100 marks. The mean of the dataset is 50. Can we put a maximum limit on Standard Deviation for the set ? I thought of putting a number of people onto 100 and the rest to zero and came up with $\frac{(100a + 0*(200-a))}{(200)} = 50$, which gives a = 100, so if a 100 people got 100 and a 100 got zero the average would still be 50. Now Standard Deviation = $\sqrt{\frac{\sum{(x_i - 50)}^2}{N}}$ which would equal $\sqrt{ \frac{100*50*50 + 100 *50 *50}{200}}$ = $50$ Is this the max value of standard deviation of this data set ?
How do I prove it ? EDIT I now tried it out for different mean values and see that all values are less than 50. Thus max value should be 50. But how do I prove it ?","['statistics', 'standard-deviation']"
1273731,Easy example of unit speed plane curve?,"I was trying to find a non-trivial example of a unit speed plane curve. The reason is I want something to work with but if I start with a non-unit speed curve and then do the arc length parameterisation I end up with something impossible. The trivial example is of course the unit circle $(\cos t, \sin t)$ but this is indeed trivial as the curvature is $1$ and also, the circle is too obvious (can determine the curvature just by looking at it). Does anyone know a unit speed curve that is not the circle?",['differential-geometry']
1273736,Let $a \leq x_{n} \leq b$ for all n in N. If $x_{n} \rightarrow x$. Then prove that $a \leq x \leq b$,"Let $a \leq x_{n} \leq b$ for all n in N. If $x_{n} \rightarrow x$. Then prove that $a \leq x \leq b$ Attempt - If I assume that $x$ is greater than both $a$ and $b$. Then since series is given convergent, so after certain stage its elements will lie between $(x-\epsilon , x + \epsilon )$. If I take epsilon to be such that $\epsilon = (b + x) /2$. Then sequence lies to right of $b$, which is contradiction. Same argument for if I take limit to be to left of a. Is this fine? Thanks","['limits', 'proof-verification', 'real-analysis', 'sequences-and-series', 'inequality']"
1273784,Function that satisfies $\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx$,"I was wondering if anyone would be able to help me find a function that satisfies this condition: $$\int_{2^{-n}}^{2^{-(n+1)}} f(x) dx = \int_{2^{-(n+1)}}^{2^{-(n+2)}} f(x) dx$$ It needs to be able to do this on the interval [0, 1]. I've tried a few functions that look similar to what I'm looking for, like $f(x) = \frac{x}{x-1}$, or $f(x) = \frac{x^2}{x-1}$, however none of them have been the solution. I'd appreciate help on this problem, I'm having some trouble figuring out where to start.","['analysis', 'real-analysis', 'integration']"
1273787,Is there a measure theoretic version of Stokes's theorem?,Is there a way to generalize Stokes's theorem on manifolds to general measure spaces? This idea came from trying to generalize the fundamental theorem of calculus to general function/infinite dimensional spaces. Just wondering if anyone can provide a reference or if this notion would even make sense. Thanks,"['analysis', 'reference-request', 'soft-question', 'measure-theory']"
1273902,$\lim_{n\to ∞} \left[\frac{f\left( x +\frac1n\right)}{ f(x)}\right]^n$,"Could anyone solve this problem for me? Let f be a positive differentiable function on the internal $\left[\,0,\infty\right)$. $$\lim_{n\to ∞} \left[\frac{f\left( x +\frac1n\right)}{ f(x)}\right]^n$$ I have been told to take log, but after taking log what to do I am not able to understand.","['limits-without-lhopital', 'limits', 'real-analysis', 'functions']"
1273925,Solve $\int_{0}^{1} \log(x)\log(1-x) dx$ without convolution,"Maybe it's too much to ask for, but is there a way to solve $\int \limits_{0}^{1} \log(x)\log(1-x) dx$ without convolution? Note that $\log x =\log_e x$.","['logarithms', 'calculus', 'definite-integrals', 'integration']"
1273927,"Why does the Residue Theorem still hold, when I let my contour get infinitely large?","The theorem (as I know it) only allows for a finite set of isolated singularities. I integrated, along a square box, a function that has simple poles at all the non-zero integers -- and a triple pole at zero.  Then I let the box get infinitely large to help prove that the sum of $1/n^2$ is $\pi^2 / 6$. But why can the Residue Theorem still be applied, even though the box is getting infinitely large, and the poles will eventually become an infinite set? ...I know that a box is compact, and poles at the integers means this set of poles is a discrete set, hence the set of poles in this compact box ...is finite.  But something about taking the limit is bugging me. Thanks,","['complex-analysis', 'residue-calculus']"
1273941,What is the range of the operator $T$ I mean I want to determine $R(T)$,"Given the normed space $\ell^\infty$ of all bounded sequences of (real or complex) numbers with the norm given by $$||x||:= \sup_{j\in Z^+} |\xi_j|,$$ for each $x:=(\xi_j)_{j=1}^\infty$ in $\ell^\infty$, and given the linear operator $T \colon \ell^\infty \to \ell^\infty$ defined as $$T(\xi_j)_{j=1}^\infty  := (\frac{\xi_j}{j})_{j=1}^\infty,$$ 
What  is the range of the operator $T$ I mean I want to determine $R(T)$","['operator-theory', 'functional-analysis', 'normed-spaces']"
1273960,Amoeba of a line in the plane: An example,"Let $z+w+1=0$ a line in $\mathbb{C}^2$ and let $x=log|z| \ge 0$ and $y=log|w|$. I have to show that 
$$
log(e^x-1) \le y \le 1+e^x
$$
But I can't do it! Can you help me, please?","['algebraic-geometry', 'tropical-geometry']"
1273964,How to determine $\Omega(T)$?,"Let $X=\left\{0,1,2\right\}^{\mathbb{Z}}$ and let $T\colon X\to X$ describe the following dynamics: 1 becomes a 2, 2 becomes a 0 and 0 becomes a 1 if at least one of its two neighbours is 1, otherwise it remains 0. Now define the so-called non-wandering set as follows. For a map $f\colon X\to X$, a point $p$ is called non-wandering provided for every neighborhood $U$ of $p$ there is an integer $n>0$ such that $f^n(U)\cap U\neq\emptyset$. Thus, there is a point $q\in U$ with $f^n(q)\in U$. The set of all non-wandering points for $f$ is called the non-wandering set and is denoted by $\Omega(f)$. Question: What is the non-wandering set $\Omega(T)$ for $T$? Edit Please see the definition of $L$ and $R$ below. I think it is not that difficult to show that $L\cup R\subseteq\Omega(T)$. At least, I think I managed it to show that. But obviously, there are more non-wandering points than the points that are in L or R, since $$
c=...210210210210210210120120120120120...
$$ surely is non-wandering. Thus, there must be at least one more set $C$ with $c\in C$ so that 
$$
\Omega(T)=L\cup R\cup C.
$$
Maybe the set $C$ can be determined. Background The background of this is the following: There is a theorem saying that in order to compute the topological entropy of $T$, it suffices to compute the topological entropy of $T$, restricted to the non-wandering set. I've already computed the topological entropy of $T$ (s. below), and now I would like to verify the theorem. Thus, I need to know what the non-wandering set looks like. Consider the set $L$ consisting of those $x\in X$ that have the following form: Every 1 has a 2 to its right, every 2 has a 0 to its right, every 0 has a 0 or a 1 to its right. On $L$, the map $T$ acts a left-shift. Similarly, let $R$ contain those $x\in X$ where Every 1 has a 2 to its left, every 2 has a 0 to its left, every 0 has a 0 or a 1 to its left. On $R$, the map $T$ acts as the right-shift. The computation of the topological entropy gave $2\ln\rho$, where $\rho$ is the positive root of $x^3-x^3-1$.","['dynamical-systems', 'general-topology']"
1273969,What's the best way to think about the Hessian?,"I've always thought about the Hessian like this: Let $f:\mathbb R^n \to \mathbb R$ be smooth.  Let $g:\mathbb R^n \to \mathbb R^n$ such that $g(x) = \nabla f(x)$.  (I am using the convention that $\nabla f(x)$ is a column vector.)  Then the Hessian of $f$ at $x$ is, by my definition, the $n \times n$ matrix $H(x) = g'(x)$. However, with this way of looking at the Hessian, I'm not thinking of $H(x)$ as being a quadratic form.  I'm worried that there is a ""quadratic form"" viewpoint of the Hessian that I am missing.  There is a rule of thumb I've heard that when a matrix (such as the Hessian) is automatically symmetric, then it's often most natural to think of it as defining a quadratic form.  I realize that you can define a quadratic form at $x_0$ by $x \mapsto \langle x, H(x_0) x \rangle$, and that this quadratic form appears in Taylor's formula.  But I still think I'm missing something, because I don't see why it is fundamentally most natural to think of the Hessian as being a quadratic form. Is it true that there is a quadratic form viewpoint that I'm missing out on?  If so, what is it?  More generally, what do you think is the best way to think about the Hessian?","['hessian-matrix', 'multivariable-calculus']"
1273989,Does $f\Big(x+\frac{1}{n}\Big) \to f(x)$ for a.e. x as $n \to \infty$?,"Let $f$ be a bounded measurable function on the real line, then is it true that $f\Big(x+\frac{1}{n}\Big) \to f(x)$ for a.e. $x$ as $n \to \infty$ I found a result where this is true for $f\in L^1(\Bbb{R})$, and I think in general it may not be true, but I'm not able to find a counter.","['examples-counterexamples', 'measure-theory']"
1274055,Calculate $\int \limits {x^n \over 1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+...+\frac{x^n}{n!}} dx$ where $n$ is a positive integer. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Calculate $$\int \limits {x^n \over 1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+...+\frac{x^n}{n!}} dx$$ where $n$ is a positive integer. Would you give me a hint?","['calculus', 'indefinite-integrals', 'integration']"
1274105,"Picking K counters out of K buckets containing NK counters, N of each different colour, up to N in each","This is a generalisation of a question that recently came up while solving a TopCoder problem . Suppose we have N blue counters, N red counters, N white counters, and so forth, K colours in total. We distribute them into B >= K buckets, such that there are not more than N counters in each. Show that we can always pick K different-coloured counters from a set of K different buckets (one from each). (In the TopCoder problem K was fixed at 3, but here it's arbitrary.) Intuitively this seems simple. I'm looking for a satisfactory simple argument, but I just can't see it. I only have the argument below, which seems more complex than necessary. Using an induction argument, assume we can already pick Q < K different-coloured counters from Q different buckets. Now we'll attempt to show that Q+1 is possible. If there is a colour we haven't yet picked in a bucket we haven't yet picked, pick it and we're finished. Otherwise, counters of this next colour (let's say ""red"" for simplicity) are in a subset of buckets that we have already picked. Divide the set of buckets into 3 sets: (1) currently picked & with red counters, (2) currently picked & no red counters, (3) non-picked & no red counters. Consider the first of these sets. There are Z buckets with at least Z+1 colours (because there is red too). As there are at most NZ counters in total in these, while there are N*(Z+1) counters of these colours altogether, there must be at least N counters of these colours in some of the other two sets. If this is in the last set, then it's easy to see what to do to get Q+1. The remaining case is if it's in the middle set. The rest of the argument refers to the following diagram: The diagram shows that when it's in the middle set, we use the same argument again to show that there have to be some of the left-of-the-line picked counters beyond the boundary. Continuing in the same way, the boundary will be pushed until, eventually, there will be some of our counters in the third set. Every time it's pushed, an arrow is drawn from a picked counter on the left of the line to similarly-coloured counters to the right of the line. When we eventually reach the unpicked set, the third diagram shows that there will be a bunch of arrows all going diagonally left to right. All arrows have unique colour. And the thick red arrow shows how to follow the arrows to update the choice of which colour counter is picked from each bucket to get Q+1. I'm asking if anyone can think of a much simpler proof of the same statement.","['discrete-mathematics', 'balls-in-bins', 'combinatorics', 'contest-math']"
1274117,"Estimation of the order of torsion in $\mathrm{GL}(n,\mathbb Z)$","Let $A \in \mathrm{GL}(n,\mathbb Z)$ be a torsion, I would like to prove that $\mathrm{order}(A)\leq K\exp (cn^{\alpha})$, with $0<\alpha <1$, for $n$ ""large enough"". I know that if $\mathcal{H}(n)$ is the maximal finite order of an element of $\mathrm{GL}(n,\mathbb Z)$ then $$\lim_{n\to \infty} \frac{\ln \mathcal{H}(n)}{\sqrt{n\ln n}}= 1,$$ which is a stronger result than what I asked for, but I'm hoping for a shorter proof for my weaker result.","['estimation', 'torsion-groups', 'group-theory']"
1274119,Is Lipschitz's condition necessary for existence of unique solution of an I.V.P.?,"Is Lipschitz's condition necessary condition or sufficient condition for existence of unique solution of an Initial Value Problem ? I saw in a book that it is sufficient condition. But I want an example to prove it sufficient. That is I want an example of an I.V.P. of the form $$\frac{dy}{dx}=f(x,y)\text{ , with initial condition  } y(x_0)=y_0$$ in which $f(x,y)$ does not satisfy Lipchitz's condition although the I.V.P. has an unique solution. Also , I saw in wikipedia that the I.V.P. $\frac{dy}{dx}=y^{1/3}$ , with initial condition $y(0)=0$ has three solutions. But how we get three solutions ? When I solve the equation with initial condition then I get , $y=\left(\frac{2}{3}x\right)^{3/2}$ According to  uranix's comment , when an I.V.P. has non-unique solution then we can put the solution in the form that given by  uranix. So I think non-unique solution implies infinitely many solutions. So from where the question of existence of $2$ or $3$ or $4$ solutions arise ? I asked for the problem $\frac{dy}{dx}=3y^{2/3}$ with $y(0)=0$ here and the answer of this question says that there are infinitely many solutions. Now, in my mind following three questions arise : $(1)$ Example of an I.V.P. in which $f(x,y)$ does not satisfy Lipschitz's condition but the I.V.P. has unique solution. $(2)$ If an I.V.P. has non-unique solution then can we say that the I.V.P. has infinitely many solutions ? $(3)$ If answer of $(2)$ is negative then how much solutions exist and how we find them ? Can anyone help me to understand these properly ?","['analysis', 'lipschitz-functions', 'ordinary-differential-equations', 'partial-differential-equations']"
1274123,Simplify $\left(ab \sqrt[4]{a^{3}/\sqrt{b\sqrt{b}}}\right)^{2}$,"Question: Simplify $$ \left(ab \sqrt[4]{a^{3}/\sqrt{b\sqrt{b}}}\right)^{2}$$ Attempted solution: Rewriting it to look a bit better: $$\left(ab \sqrt[4]{\frac{a^{3}}{\sqrt{b\sqrt{b}}}}\right)^{2}$$ Rewriting the denominator by replacing square roots with powers of fractions: $$\left(ab \sqrt[4]{\frac{a^{3}}{(b (b^{\frac{1}{2}}))^{\frac{1}{2}}}}\right)^{2}$$ Combining b:s in the denominator: $$\left(ab \sqrt[4]{\frac{a^{3}}{b^{\frac{3}{4}}}}\right)^{2}$$ Distributing the 4th root: $$\left(\frac{ab \cdot a^{\frac{12}{16}}}{b^{\frac{3}{16}}}\right)^{2}$$ Combining a:s with a:s and b:s with b:s: $$\left(b^{\frac{13}{16}} a^{\frac{28}{16}}\right)^{2}$$ Squaring gives the final result: $$b^{26} a^{56}$$ The answer turns out to be: $$a^{\frac{7}{2}} b^{\frac{13}{8}}$$ This is quite far away from the result I reached. Where did I go wrong, and are there are key insights that are useful for solving questions with lots and lots of square, cube and higher roots?",['algebra-precalculus']
1274133,"Given $f(1)=10,f(2)=20,f(3)=30$ find $f(12)+f(-8)$ for a 4-th degree monic polynomial","If $f(x)=x^4+ax^3+bx^2+cx+d$.
Given $f(1)=10,f(2)=20,f(3)=30$ find $f(12)+f(-8)$.
This problem has troubled me a lot.The more I try to solve it,it becomes lengthier. My problem is that there are four unknowns and only three equations. Please help me.","['polynomials', 'functions']"
1274142,How to solve the differential equation $(y^2-1)+2(x-y(1+y)^2)y'=0$?,"I have to find the solution to the differential equation $$(y^2-1)+2(x-y(1+y)^2)y'=0$$ So far I've only learned how to solve equations of the form
$$y'+p(x)y=q(x)$$
And second order equations with constant coefficients. This equation that I have to solve right now does not seem to be of any of these $2$ forms so I don't understand how I can solve this. I know I'm supposed to show some attempted solutions I tried but frankly I just have no idea how to even start here.",['ordinary-differential-equations']
1274145,etale morphism between sheaves,"We knoe that if $f$ and $  f\circ g$ are both etale morphisms between schemes, then so is $g$. Does this statement hold for etale morphisms between sheavs on etale site over a scheme?
More generally, if we do it in a general category and etale is replaced by any other particular property, say  $P$, satifies (1) compositions of $P$  morphisms is a $P$  morphism, (2) pullbacks of a $P$  morphism by a morphism is a $P$  morphism and (3) if $f$ and $  f\circ g$ are both $P$ morphisms. then so is $g$. And we similarly define $P$  morphisms between sheaves on the $P$ site over a object, i.e. $F\to G$ is $P$ if the pull back by any $h_X\to G$ is representable by some $h_Y\to h_X$ with $Y\to X$ a $P$ morphism. Does the statement (3) still hold? For example, the category is the category of affine schemes and $P$ is open immersion, then Sheaves are schemes and $P$ is still open immmersion. And (3) still holds. I'm afraid all of them are wrong, but I need some counterexamples.","['algebraic-geometry', 'category-theory']"
1274156,Intuitive explanation of the potential function of a vector field,"Suppose I have some vector field $$\vec{f}(x,y)=\begin{pmatrix}A(x,y)\\B(x,y)\end{pmatrix}$$ then the potential function (if the field is conservative) can be found by integrating  $A$ with respect to $x$ and $B$ with respect to $y$ and then ""unifying"" the two solutions into a single function. But what does the potential function really mean? Can somebody give me a simple intuitive explanation (maybe graphic) of what this function means?","['vector-analysis', 'vector-fields', 'multivariable-calculus', 'real-analysis']"
1274187,Stirling numbers of the second kind vs. binomial coefficient,"For $n,k$ positive integers, such that $n\geq k$, denote by $\left\{{n\atop      k}\right\} $ the Stirling numbers of the second kind and $\binom{n}{k}$ the binomial coefficient.
It is rather straightforward to prove that $\left\{{n\atop      k}\right\} \geq \binom{n}{k}$. According to some calculation it looks like we also have $n^k\left\{{n\atop      k}\right\} \geq k^n\binom{n}{k}$. I tried to prove this via induction using $\left\{{n+1\atop k}\right\} = k \left\{{ n \atop k }\right\} +
\left\{{n\atop k-1}\right\}$, but no luck. Any idea?","['combinations', 'stirling-numbers', 'binomial-coefficients', 'combinatorics']"
1274198,uncertainty of slope.,"i have a graph that i fitted a line on it using least squares fit.
Now i want to calculate the uncertainty of slope.
i calculated the standard error of slope and now i have this question: the uncertainty of slope is ""slope ± standard error"" or it is the confidence interval???! please help me. i really need the answer. i searched in many books but unfortunately i couldn't find the answer.",['statistics']
1274206,When vectors act on scalars.,"Background. I've been struggling through an introduction to differential geometry this semester. Recently, a tiny part of what we've been learning ""clicked"" for me, and to solidify this, I'd like to get some further information and especially just understand the relevant terminology. Hence this question. Fix a smooth manifold $M$. By a scalarfield on $M$, I mean a smooth function $M \rightarrow \mathbb{R}$. Write $S$ for the ring of scalarfields on $M$. By a vectorfield on $M$, I mean a derivation on $S$; explicitly, this is an $\mathbb{R}$-linear function $v : S \rightarrow S$ satisfying the Leibniz product law: $$v(s_0 s_1) = v(s_0)s_1 + s_0 v(s_1)$$ Write $V$ for the collection of vectorfields on $M$. There's something odd about this situation. In the basic commutative algebra I'm familiar with, ""scalars"" (i.e. the elements of some commutative ring) act on ""vectors"" (i.e. the elements of some abelian group.) That is certainly the case here; we have a multilinear function: $$S, V \rightarrow V$$ that satisfies $s_1(s_0v) = (s_1s_0)v$ and $1_S v = v$, hence $V$ is an $S$-module. However we can also go the other way; vectorfields can act on scalarfields by $v,s \mapsto v(s)$. This gives a multilinear function: $$V,S \rightarrow S$$ It satisfies $v(s_0s_1) = v(s_0)s_1+s_0 v(s_1)$. Furthermore, these two actions are related by: $s_0 (vs_1) = (s_0 v)s_1$ Furthermore, given $s \in S$, we can define the differential $ds : V \rightarrow S$ by writing $ds(v) = v(s)$. Hence $ds$ is an element of the dual space $V^*,$ where $V$ is viewed as an $S$-module. Question. What terminology surrounds this situation? For example: What do we call multilinear maps $V,S \rightarrow S$ that satisfy the Leibniz law? What kind of a structure is formed by the whole data bundle consisting of $S$ and $V$, together with the actions $S,V \rightarrow V$ and $V,S \rightarrow S$? Are other there any basic relationships here that I really need to be aware of? If I understand correctly, $V$ forms a ""Lie algebra"", but I'm not sure of the relevance of this.","['terminology', 'differential-geometry', 'soft-question', 'commutative-algebra']"
1274212,Integral $\int \frac{\mathrm{d}x}{\sqrt{x}+\sqrt{x+1}+\sqrt{x+2}}$,$$\int \frac{\mathrm{d}x}{\sqrt{x}+\sqrt{x+1}+\sqrt{x+2}}$$ I tried substituting $x=z^2$ ... also $x=\tan^2 \theta$ ... but couldn't solve it either ways... if someone can help then it would be good.,"['calculus', 'indefinite-integrals', 'integration']"
1274240,Total derivative for a polynomial,"I refer to Rudin's (Principles of Mathematical analysis, 3rd ed.) definition of differentiability: Suppose E is an open set in $R^n$ and f maps E into $R^m$ and $x \in E$.
If there exists a linear transformation A of $R^n$ into $R^m$ such that 
$$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x) -Ah|  } {|h|} = 0,$$
then we say that f is differentiable at x and we write 
$f'(x) = A$. With this definition and assuming the $l_2$ norm, $f'(x)$ for $(x_1^2 + x_2^2)$ is given by $(2x_1, 2x_2)$. 
since 
$$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1h_1 + x_2 h_2)}{|h|} = \lim_{|h| \rightarrow 0} \frac{(2x_1, 2x_2)'(h_1, h_2)}{|h|}.$$ However I could not find $f'(x)$ for $(x_1 + x_2)^2$ with this definition. 
I get 
$$\lim_{|h| \rightarrow 0} \frac{ |f(x+h) - f(x)|  } {|h|} =  \lim_{|h| \rightarrow 0} \frac{2(x_1 + x_2)(h_1 + h_2) + h_1h_2}{|h|}$$ The cross term $h_1 h_2$ becomes a problem, if it were not present, I would write $f'(x) = (2(x_1+x_2), 2(x_1+x_2))$. I have read elsewhere on this forum that polynomials are differentiable. I would be very thankful I someone were to tell me how do I proceed to find the total derivative, $f'(x)$ for a given polynomial function.","['polynomials', 'derivatives']"
1274248,For a prime $p\ge 17$ is $\dfrac{p^2-1}{24}$ ever a prime?,"It was indicated in the comments of this MO question that if $p\ge5$ is a prime 
then $24|p^2-1$. Indeed $p=6k\pm1$ and $p^2-1=36k^2\pm12k+1-1=12k(3k\pm1)$ and exactly one of $k$ and $3k\pm1$ is even. Let $Q(p)=\dfrac{p^2-1}{24}$ (where $p\ge5$ is a prime). Note that $Q(5)=1$, $Q(7)=2$, $Q(11)=5$, $Q(13)=7$ are all primes (except $1$, which used to be a prime). On the other hand $Q(17)=12$, $Q(19)=15$, $Q(23)=22$, $Q(29)=35$, $Q(31)=40$, $Q(37)=57$, $Q(41)=70$, $Q(43)=77$, $Q(47)=92$, $Q(53)=117$, are all composite numbers, though they do not have a common factor, and I do not see an obvious pattern. Question . Is it true that $Q(p)=\dfrac{p^2-1}{24}$ is composite whenever $p\ge17$ is a prime? If so, how to prove this? Is it known, any references? (Is it related in any way to quadratic residues, am I supposed to know it? :) I ran reduce computer algebra and it tells me the answer is yes for at least all primes $p\le21331777$ (when I interrupted it). Thank you!","['prime-numbers', 'number-theory', 'quadratic-residues', 'elementary-number-theory', 'reference-request']"
1274300,Minimal collection of subsets to reconstruct singletons,"I have come across the following problem in a technical application. For a given integer $n$, what is the minimal collection of subsets of $\{1,\dots,n\}$ such that all ""singleton"" sets $\{1\}, \{2\}, \dots, \{n\}$ can be ""reconstructed"" by set operations (intersection, union, set difference, complement) on those subsets? For example, with $n = 5$ a possible collection is $S_1 = \{1,2,3\}, S_2 = \{2,3,4\}, S_3 = \{3,4,5\}$, because we can write
$$ \{1\} = S_1 \setminus S_2 \\
\{2\} = S_2 \setminus S_3 \\
\{3\} = S_1 \cap S_2 \cap S_3 \\
\{4\} = S_2 \setminus S_1 \\
\{5\} = S_3 \setminus S_2$$ In believe that in general, the sets $S_k = \{k,k+1,\dots,k+c-1\}$, $1\le k \le n-c+1$ and $c = \lceil n / 2 \rceil $ are sufficient, but I am not convinced they are minimal. I am sure this problem has been studied in some context but I am not a professional mathematician and I don't even know where to look ... Any help appreciated!",['combinatorics']
1274303,Motive of a curve and its Jacobian,"Let $C$ be a smooth projective curve with a $k-$rational point $x_0$ and $J$ its Jacobian variety. Let us consider the (almost) canonical embedding $j:C \to J$ that sends $x_0$ to the identity $e \in J(k)$. There is a  decomposition of the Chow Motive of $C$ that we call $h(C)$, depending on $x_0$, namely if we put $p_0(C) = x_0\times C,\, p_2(C)= C\times x_0$ and $p_1(C)= \Delta - p_0(C) - p_2(C)$ then
$$h(C) = h^0(C) \oplus h^1(C) \oplus h^2(C) \quad \text{with }\quad h^i(C) = (C,p_i(C),0)$$ On the other hand, for every abelian variety $A$ of dimension $g$ there is a canonical decomposition 
$$h(A) = \bigoplus_{i=0}^{2g} h^i(A)$$
where $h^i(A) = (A,\pi_i(A),0)$. The projectors $\pi_i$ are unique in such a way  that
$$(id_A\times n)^*(\pi_i(A)) = n^i\pi_i(A)$$ I should prove that the induced map $h(j):h(J)\to h(C)$ is such that restricted to $h^n(J)$ for every $n\in \{0,1,2\}$ it induces a morphism of motives  $h^n(J)\to h^n(C)$, while for $n >2$ is the zero map. Moreover, it should be an isomorphism for $n=0,1$. For $n=0$ it should be true for any variety $X$ with a rational point that $h^0(X)$ is isomorphic to the motive of a point, and since $i$ sends $x_0$ to $e$ it should be trivially true that the induced map is an isomorphism of motives. On the other hand I tried to prove it for $n=1$ but without much results.
I know that it should be an ""easy"" exercise, but I started working on motives quite recently and I am still a bit confused on the techniques I should apply.
I think that the proof should be quite clean since I don't think is necessary to use an explicit description of the $\pi_i$'s but rather the property  of being ""eigenvectors"" for the multiplication by $n$. Any suggestion will be appreciated.",['algebraic-geometry']
1274315,Let $(a_n)$ be any sequence and $(b_n)=n(a_n-a_{n+1})$. If $\sum a_n$ and $\sum b_n$ converges then $\lim_{n\to \infty}na_n=0$,"Let $(a_n)$ be any sequence and $(b_n)$ be a sequence such that $b_n := n(a_n-a_{n+1})$ . Prove that if $\sum a_n$ and $\sum b_n$ converges then $\lim_{n\to \infty}na_n=0$ and $\sum a_n= \sum b_n$ . I've shown the second part assuming the first part. I'm having trouble showing $\lim_{n\to \infty}na_n = 0$ . I know that $\lim a_n=0$ and $\lim n(a_n-a_{n+1})=0$ . How can I use these facts to show the first one? I would greatly appreciate any help. From the answers below I got that $\lim na_n$ exists. So I tried to show that the limit is $0$ by assuming that it is not. First, assume that the limit $l \gt 0$ . Then $\liminf na_n=l$ . Hence for $l/2 \gt 0$ , there is some $N$ such that for $n \ge N$ , we have $na_n \gt l/2$ . This implies that for $n \ge N$ , $a_n \gt l/(2n)$ . But this is a contradiction since we assumed that $\sum a_n$ converges. Finally, assume that $l \lt 0$ . Then $\limsup na_n=l$ . So for $-l/2$ , there is some $N$ such that if $n\ge N$ then $na_n\lt l/2$ . This implies that $-a_n\gt -l/(2n)\gt 0$ . Thus, again by comparison, we get that $-\sum a_n$ diverges, which is a contradiction. Hence the limit must be $0$ .","['analysis', 'sequences-and-series', 'calculus']"
1274325,Reducible polynomials in $\mathbb{Z}[X]$,"Let $(a_n)_{n\geq 1}$ be a strictly increasing sequence of integers and $k$ an integer different from $0$. There exists among the polynomials 
  $$
(X-a_1)(X-a_2)\cdots(X-a_n)+k,\ n\geq 1
$$
  only a finite number of reducible polynomials in $\mathbb{Z}[X]$? I think this must be true, but I have yet no proof. Any idea / solution? Thank you! Nicholas Tatsis","['abstract-algebra', 'polynomials', 'factoring', 'irreducible-polynomials']"
1274352,Is there an exact term for $\sqrt{2+\sqrt{4+\sqrt{8+\dots}}}$,"I'm wondering whether it's possible to find an exact term for the infinite nested radical expression from the title. I got a quite good approximation with my calculator but what I'm looking for is an exact term. $$
f(x)=\sqrt{2^x+\sqrt{2^{x+1}+\sqrt{2^{x+2}...}}}
$$ It is necessary that f satisfies the condition:
$$
f(x)^2=2^x+f(x+1)
$$ EDIT: But there should be infinetely many solutions to this equations - furthermore, I wasn't able to find a single one! Does anyone have an idea how to find an exact finite term - or prove that no such term exists?","['nested-radicals', 'limits']"
1274374,Finding out this combination,"In how many ways three non-empty strings of length less than or equal to $N$ using $k$ different characters can be selected so that in each case, among the three strings, no string is prefix (not necessarily proper prefix) of one of the other two strings. Example :
Let $N=1,k=3$.
Result will be $6$ $$
[a, b, c]\\
[a, c, b]\\
[b, a, c]\\
[b, c, a]\\
[c, a, b]\\
[c, b, a]\\
$$ I am getting stuck for the ""prefix"" part. Please help. I am looking for a way that is computationally cheap as much as possible as $N$ could be as big as $10^{9}$ ($k$ will be much smaller though - upto two digits) . EDIT : $$
1\le N\le 10^9\\
1\le k\le 50\\
$$ More Example : For $N=2,k=2$ answer is $36$","['combinations', 'combinatorics']"
1274380,Increasing sequence of divisors of a quadratic trinomial,"This question is from a Russian contest, the 2011 Tuymaada Olympiad . It's the fourth question on day two for the problems at grade level 2. Let $P(n)$ be a quadratic trinomial with integer coefficients. For each positive integer $n$ , the number $P(n)$ has a proper divisor $d_{n}$ , i.e., $1 < d_{n} < P(n)$ , such that the sequence $d_{1},d_{2},d_{3},\ldots$ is increasing. Prove that either $P(n)$ is the product of two linear polynomials with integer coefficients, or all the values of $P(n)$ , for positive integers $n$ , are divisible by the same integer $m > 1$ . Part (2) of the last sentence says that if $P(n)=an^2+bn+c$ , where $a,b,c$ are integers such that $b^2-4ac$ is not a perfect square, then the sequence $d_{1},d_{2}, \ldots$ is increasing only if there is a positive integer $m>1$ which divides all $P(n)$ . I tried to analyze two different cases: one when $b^2-4ac$ is negative
and one when $b^2-4ac$ is positive and not a perfect square,
but I couldn't go anywhere. Any suggestion would be appreciated.","['contest-math', 'number-theory', 'elementary-number-theory']"
1274411,Is Riesz measure an extension of product measure?,"Suppose $X$ and $Y$ are compact Hausdorff spaces and $(X, \mathcal A, \mu)$ and $(Y, \mathcal B, \nu)$ are finite regular Borel measure spaces.
(By regular I mean that every measurable set can be approximated from above by open measurable sets and from below by compact measurable sets.)
Let $\mu \times \nu$ be product measure on $(X \times Y, \mathcal A \times \mathcal B)$.
For $f \in C(X \times Y)$ define $\psi(f) = \int f \; d(\mu \times \nu)$.
By the Riesz–Markov–Kakutani representation theorem there is a unique regular Borel measure $\lambda$ on $X \times Y$ such that 
$\psi(f) = \int f \; d\lambda$.
I believe I can show that $\lambda$ must be an extension of $\mu \times \nu$, but I am looking for a simpler or more elegant proof.","['real-analysis', 'functional-analysis', 'measure-theory']"
1274420,How to find the exact value of the cosine of 50 degree angle,I want to know the exact value of $\cos 50^\circ$. Actually I have already tried lot of times to solve but I cannot find the exact value of $\cos 50^\circ$.,['trigonometry']
1274449,Orientation at the boundary for manifold with corners: the simplex,"Consider the $n$-simplex $$\Delta[n]:=\{(t_{1},\dots,t_{n})\in \mathbb{R}^{n}\: : \: 0\leq t_{1}\leq t_{2}\leq  \dots \leq t_{n}\leq 1\}.$$ This is a manifold with corners. The cofaces map $d^{i}\: : \: \Delta[n-1]\to \Delta[n]$,  $i=0,\dots,n $, are defined via
$$
d^{0}(s_{1},\dots s_{n-1}):=(0,s_{1},\dots s_{n-1}),\quad d^{n}(s_{1},\dots s_{n-1}):=(s_{1},\dots s_{n-1},1)
$$
and
$$
d^{i}(s_{1},\dots s_{n-1}):=(0,s_{1},\dots, s_{i-1}, s_{i},s_{i}, s_{i+1}\dots , s_{n-1}),\:\text{ for }i=1,\dots n-1.
$$
The union of the image of these maps is the boundary of the $n$-simplex. For which $i$ are these inclusions orientation persevering?
I think that the answer should be: $d^{i}$ for $i$ odd or $i=n$, but i don't know how to prove that.","['differential-geometry', 'orientation']"
1274485,Is every homeomorphism of $\mathbb{Q}$ monotone?,"It is well known that every continuous injective map $\mathbb{R}\rightarrow\mathbb{R}$ is monotone. This statement is false for maps $\mathbb{Q}\rightarrow\mathbb{Q}$. (That is becaus $\mathbb{Q}$ is not complete. You can change from increasing to decreasing and vice versa in each irrational ""hole""). Is it true that every homeomorphism of $\mathbb{Q}$ is monotone?","['rational-numbers', 'ordered-fields', 'general-topology']"
1274526,How many binary strings of length 2n + 1 have more 1's than 0's? Use bijection to prove.,"NOTE: This is a homework question, so I only ask for hints and suggestions to nudge me in the correct direction. Question: Let $n \in \mathbb{N}$. How many binary strings of length 2n + 1 have more 1's than 0's? Use bijection to prove. Progress so far: We need a set of known size, B, along with a bijective function $f: A \rightarrow B$, where A is the set with binary strings of length $2n + 1$ and where each element in A has more 1's than 0's. I tried to find a pattern by setting n = 1: $$ 
\begin{align*}
000 \rightarrow &\lbrace empty set \rbrace \\
001 \rightarrow &\lbrace 1 \rbrace \\
010 \rightarrow &\lbrace 2 \rbrace \\
011 \rightarrow &\lbrace 1,2 \rbrace \\
100 \rightarrow &\lbrace 3 \rbrace \\
101 \rightarrow &\lbrace 1,3 \rbrace \\
110 \rightarrow &\lbrace 1,2 \rbrace \\
111 \rightarrow &\lbrace 1,2,3 \rbrace \\
\end{align*}
$$ I realized that binary strings with more 1's than 0's map to elements in B where the elements are of size $\geq 2n$. Therefore the size of set $S$ where $S = \lbrace x : |x| \geq 2n\rbrace$ is the number of binary strings of length $2n+1$ with more 1's than 0's. This is where I am stuck. I don't know how to fit bijection into this solution. Also I think my mapping from set A to set B is incorrect since set A it should only contain the binary strings that have more 1's than 0's. Any help would be greatly appreciated.",['combinatorics']
1274564,intuition on the fundamental group of $S^1$,"I am familiar with the proof that the fundamental group of the unit circle $S^1$ is $\mathbb Z$, yet I couldn't develop intuition for why it is true. For example, why would I fail if I try to find homotopy from a path that circles $S^1$ once to the path that stays at one point all the time? What brakes the continuity of such a homotopy?","['fundamental-groups', 'algebraic-topology', 'general-topology']"
1274576,Deriving the variance of a binomial distribution,"I know that the variance of a binomial distribution is the number of trials multiplied by the variance of each trial, but I'm not seeing the derivation of this. Here's my logic so far: For each trial ($x$),
$p$ = probability of success (1), and
$1-p$ = probability of failure (0): $$E(x) = 1\cdot p+0\cdot(1-p) = p$$
$$E(x^2) = 1^2\cdot p+0^2\cdot(1-p) = p$$
$$Var(x) = E(x^2)-E(x)^2 = p - p^2 = p(1-p)$$ From here, for any combination of trials ($X$): $$X = x_1 + x_2 + \cdots + x_n$$
$$E(X) = E(x_1) + E(x_2) + \cdots + E(x_n)$$
$$E(X) = np$$
$$E(X^2) = E(x_1^2) + E(x_2^2) + \cdots + E(x_n^2)$$
$$E(X^2) = np$$ By this, the logic indicates the variance would be: $$Var(X) = E(X^2) - E(X)^2 = np - (np)^2 = np(1-np)$$ ...however, this is not correct, since the variance is as follows: $$Var(X) = Var(x_1) + Var(x_2) + \cdots + Var(x_n)$$
$$Var(X) = p(1-p) + p(1-p) + \cdots + p(1-p)$$
$$Var(X) = np(1-p)$$ I'm not seeing in my derivation where I'm missing the mark mathematically, and resulting in the incorrect ""n"" in the parentheses.","['binomial-theorem', 'statistics', 'expectation']"
1274648,How do I know when the limit of a function at a certain point doesn't exist?,$$\lim_{x \to 8} \frac{\sqrt{7+\sqrt[3]{x}}-3}{x-8}$$ I have this limit. I can't use L'Hopital. After rationalizing the numerator I get: $$\lim_{x \to 8} \frac{\sqrt[3]{x}-2}{(x-8)(\sqrt{7+\sqrt[3]{x}}+3)}$$ Isn't there anything left to do after that? How to know if the limit just doesn't exist? EDIT: I typed the limit wrong.,"['limits-without-lhopital', 'calculus', 'limits']"
1274649,Almost sure convergence of the Poisson process,"Let $N = \{N(t) \}_{t\geq 0 }$ be a Poisson process. I already know that $N(t)- \lambda t$ is a martingale where $\mathbb{E} [ N(t) ] = \lambda t$. I want to prove that
$$ \frac{N(t)}{t} \rightarrow \lambda, \quad t \rightarrow \infty \quad \text{ a.s.}. $$ To prove this I received a hint that I should use the martingale law of large numbers which states that: Theorem: (Martingale LLN) 
Let $S= (S_n = \sum_{k=1}^n X_k)_{n=1,2,\ldots}$ be a martingale with respect to the filtration $(\mathcal{F}_n)_{n=1,2,\ldots}$. Then $\sum_{k=1}^n X_k/k$ converges almost surely on the set $\left\{ \sum_{k=1}^\infty k^{-2} \mathbb{E}\left[ X_k^2 \mid \mathcal{F}_{k-1} \right] < \infty \right\}$. Hence $S_n/n \rightarrow 0$ a.s. on the same set. Now to use this theorem I think I need to use Doob-Meyer to decompose $N(t)$ as the sum of a martingale and an increasing process. Which would be the martingale $M(t) = N(t) - \lambda t$ and increasing process $A(t) = \lambda t$. But how is $M(t)$ the sum of a martingale? And I do not know how this exactly relates to the thing I have to prove. Thanks for any help.","['probability-theory', 'poisson-distribution', 'stochastic-processes', 'martingales', 'convergence-divergence']"
1274674,$\sum\limits_{n=1}^{10000000000000000} \frac{1}{n}$,How does wolfram alpha solve $$\sum\limits_{n=1}^{10000000000000000} \frac{1}{n}\approx 37.4186$$so quickly? It solved it in like 3 seconds is there a equation or something,"['summation', 'calculus']"
1274727,Second Variation of Area Functional,"This is a follow up question from this one . I have proved that given a parametrized surface ${\bf x}$, the mean curvature is zero if and only if it is a critical point of the area functional. Then everybody starts handwaving and says that the surface is a minimum point. I want to check that indeed this is the case (mainly because I'll have to redo the calculations in Minkowski space where we also have maximum surfaces, and if I know how to do this here, I can adapt the calculations). That being said, I am not interested in heavy machinery from Riemannian geometry. My notation will be the same as in the other question. For your comfort: Notations: Fix a domain $D$. Here $\bf x$ is a parametrization, ${\bf x}^t={\bf x}+t{\bf V}$ is a variation, with $\bf V$ being zero on $∂D$, $\bf N$ is the normal unit vector and $A(t)$ is the area of ${\bf x}^t$. Assume $H = 0$. I have computed: $$A''(0) = \iint_D \frac{2\langle {\bf x}_u\times{\bf x}_v,{\bf V}_u\times{\bf V}_v\rangle + \|{\bf x}_u\times{\bf V}_v+{\bf V}_u\times{\bf x}_v\|^2}{\|{\bf x}_u\times{\bf x}_v\|^3}\,{\rm d}u\,{\rm d}v$$ I can add the computations I made to get this, if it comes to that. Theory says that we must have this integral being positive . I don't know how to proceed here. Can someone help me please? Thanks. Edit: rephrasing: I want to check that $\bf x$ is a local minimum of the functional. After the comments, I'm not even sure if we'll really have ${\cal A}''(0) > 0 $ then. I need this specific question adressed too. (I'm not sure I even know calculus anymore haha) Edit (05/12): It is of my understanding that if we have a closed contour $\Gamma \subset \Bbb R^3$, then off all surfaces that have $\Gamma$ as boundary, the one that minimizes area has $H = 0$. However, having $H = 0$ does not imply that the surface is area minimizing, as pointed by several people in the comments. Ok. Let's say I am dealing only with a parametrized surface ${\bf x}: D \subset \Bbb R^2 \to {\bf x}(D)\subset \Bbb R^3$. Then the boundary would be just ${\bf x}(\partial D)$. If I consider all the variations ${\bf x}^t$ defined as above, then all of them share the same boundary as $\bf x$. Although ${\bf x}(D)$ may not be the surface having ${\bf x}(\partial D)$ as boundary that minimizes area, I am thinking so far that between all the ${\bf x}^t$, at least , $\bf x$ is the one that gives least area. With this in mind, I wanted to check that ${\cal A}''(0) > 0$.","['differential-geometry', 'multivariable-calculus']"
1274737,Blow-up of derivative of BV function at the jump set,"""Motivation"" Let $u\in BV(\mathbb{R}^n)$ be a function of bounded variation, and let $x\in J_u$ be a point in its jump set.  For $\mathcal{H}^{n-1}$-a.e. such $x$, we can define the unit normal $\nu$ to the boundary, and an upper and lower limit $u^+$ and $u^-$ such that
$$
\lim_{r\to 0} \frac{1}{|B_r(0)|} \int_{B_r(0)\cap H^+_\nu} |u(x+y)-u^+|\,dy
= 0
$$
and
$$
\lim_{r\to 0} \frac{1}{|B_r(0)|} \int_{B_r(0)\cap H^-_\nu} |u(x+y)-u^-|\,dy
= 0
$$
where $H^+_\nu$ and $H^-_\nu$ represent the upper and lower half-spaces with respect to the unit normal $\nu$.  This gives us convergence of the blow-ups of $u$ to a piecewise constant function across a flat jump in $L^1$.  In fact, according to the textbook by Evans and Gariepy we can obtain convergence in $L^{n/n-1}$. My question is about the convergence of the blow-ups of $Du$, the distributional gradient of $u$.  In the case $u=1_E$ that $u$ is the indicator function for a set of finite perimeter $E$, a consequence of De Giorgi's structure theorem is that the blowups $Du$ (and more interestingly, of $|Du|$) converge to the the surface measure of a hyperplane.  I was wondering whether this is still true in the case of a general function of bounded variation. Actual Question More precisely, let $|Du_{x,r}|$ denote the blowup of the measure $|Du|$ at the point $x$ with scale $r$, defined by $|Du_{x,r}|(A) = |Du|(rA+x)$ for Borel sets $A\subset \mathbb{R}^n$.  The question is then: Does the sequence of blow-ups 
  $|Du_{x,r}|$ converge weak-* to the surface measure of the half-plane $H_\nu$ for $\mathcal{H}^{n-1}$-a.e. $x\in J_u$?","['geometric-measure-theory', 'measure-theory']"
1274739,Diagonalize tri-diagonal symmetric matrix,"How to diagonalize the following matrix? \begin{pmatrix}
2  & -1 & 0 & 0 & 0 & \cdots \\
-1 & 2  & -1 & 0 & 0 & \cdots \\
0 & -1 & 2  & -1 & 0 & \cdots \\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
\cdots & 0 & 0 & -1 & 2  & -1 \\
\cdots & 0 & 0 & 0 & -1 & 2  \\
\end{pmatrix} It seems like we need to first compute eigenvalues and eigenvectors for this matrix, like the following: \begin{vmatrix}
2 - \lambda & -1 & 0 & 0 & 0 & \cdots \\
-1 & 2 - \lambda & -1 & 0 & 0 & \cdots \\
0 & -1 & 2 - \lambda & -1 & 0 & \cdots \\
\cdots & \cdots & \cdots & \cdots & \cdots & \cdots \\
\cdots & 0 & 0 & -1 & 2 - \lambda & -1 \\
\cdots & 0 & 0 & 0 & -1 & 2 - \lambda \\
\end{vmatrix} But I just don't know what to do next.","['matrices', 'eigenvalues-eigenvectors', 'tridiagonal-matrices', 'diagonalization', 'linear-algebra']"
1274743,When is $2$ a quadratic residue mod $p$?,"For which prime numbers $p$ is $2$ a quadratic residue modulo $p$ . I know that $2$ is a quadratic residue iff $$2^{\frac{p-1}{2}} =1 \;  \bmod \;(p)
$$ so $$2^{p-1} =1 \;  \mod \; (p).
$$ But I don't know what to do.","['quadratic-residues', 'number-theory', 'quadratic-reciprocity', 'elementary-number-theory']"
1274759,"What is an ""inner isomorphism"" between different groups?","It is well known that if $X$ is a path-connected topological space containing points $x$ and $y$, then the fundamental groups $\pi_1(X,x)$ and $\pi_1(X,y)$ are isomorphic. Wikipedia makes the further claim that the two groups are not only identical up to isomorphism but ""actually even up to inner isomorphism"". What does ""inner isomorphism"" mean here? I know what an inner automorphism is, but here the elements of the groups are different objects (homotopy classes of closed paths that begin and end at $x$, versus ditto at $y$), so automorphisms are definitely not on the table. The obvious isomorphism(s) does arise by fixing a path $\alpha$ from $y$ to $x$ and considering the map
$$[\gamma]\in\pi_1(X,x)\mapsto [\alpha+\gamma-\alpha]\in\pi_1(X,y)$$
which does look at bit like conjugation with $\alpha$ -- but (a) $\alpha$ lives outside either group so I'm not quite prepared to call that ""inner"", and (b) this seems to be very specific to the case of fundamental groups. Is the remark in Wikipedia simply nonsense, or is there a relevant technical meaning of ""inner isomorphism""? Googling didn't seem to uncover one.","['group-theory', 'homotopy-theory', 'fundamental-groups']"
1274782,Invariant Subspace containing linear combination of eigenvectors,"Let
$$T:V\to V$$
be a linear transformation. Suppose that $v_1, v_2, \cdots, v_k \in V$ are eigenvectors of $T$ that correspond to distinct eigenvalues. Assume that $W$ is a $T$-invariant subspace of $V$ that contains the vector $v_1 + v_2 + \cdots + v_k$. Show that $W$ contains each of $v_1, v_2, \cdots, v_k$.","['eigenvalues-eigenvectors', 'vector-spaces', 'linear-algebra']"
1274804,Convergence to the Dirac Delta Function,"Let $h\colon[0,1]\to \mathbb{R}^+$ be any bounded measurable non-negative function with a unique maximum at $a$ and $h$ is continuous at $a$. For $\lambda>0$ define $h_\lambda(x)=C_\lambda h(x)^\lambda$ where $C_\lambda$ normalizes such that $\displaystyle\int_0^1 h_\lambda(x)\,dx=1$. $f$ is any continuous function on $[0,1]$ and $\epsilon>0$. Are the following assertions true? 1)$\displaystyle\lim\limits_{\lambda\to\infty}\int_{h(x)\le h(a)−\epsilon}h_λ(x)f(x)\,dx=0$ and $\displaystyle\lim\limits_{\lambda\to\infty}\int_0^1 h_λ(x)f(x)\,dx=f(a)$. In words, the limit of $h_\lambda$ is the Dirac delta function $\delta(\cdot-a)$. 2) If $h$ is continuous, $h_\lambda(x)$ converges uniformly to $0$ in $\{x: h(x) \le h(a)-\epsilon\}$. It seems correct since the exponentiation suppresses the part somewhere below $h(a)$ while heightens the part of $h$ near $a$. The difficulty seems to be giving a partitions of $[0,1]$ where I can utilize the exponentiation to either suppress or heighten $h$ by the exponentiation of $\lambda$ after $h$ is normalized by $C_\lambda$. This question is a generalization of my more specific case , and was suggested by one commentator whuber. However, he did not supply a proof and I am stumped by the difficulty described the last paragraph.","['probability-theory', 'dirac-delta', 'distribution-theory', 'real-analysis']"
1274816,Numbers that can be expressed as the sum of two cubes in exactly two different ways,"It seems known that there are infinitely many numbers that can be expressed as a sum of two positive cubes in at least two different ways (per the answer to this post: Number Theory Taxicab Number ). We know that $$1729 = 10^3+9^3 = 12^3 + 1^3,$$ and I am wondering if there are infinitely many numbers like this that can be expressed as the sum of two positive cubes in exactly two ways? In fact, are there even any other such numbers? EDIT:
As provided by MJD in the comments section, here are other examples:
$$4104 = 2^3+16^3 = 9^3+15^3,$$
$$13832 = 20^3+18^3=24^3+2^3,$$
$$20683 = 10^3 +27^3 = 19^3 +24^3.$$",['number-theory']
1274824,$f\left( x-1 \right) +f\left( x+1 \right) =\sqrt { 3 } f\left( x \right)$,"Let f be defined from real to real $f\left( x-1 \right) +f\left( x+1 \right) =\sqrt { 3 } f\left( x \right)$ Now how to find the period of this function f(x)?
Can someone provide me a purely algebraic method to solve this problem please? Update:My method An elementary algebraic approach to the problem : $f(x-1)+f(x+1)=\sqrt { 3 } f(x)$ Replace $x$ with $x+1$ and $x-1$ respectively. We get $f(x)+f(x+2)=\sqrt { 3 } f(x+1)$ and 
$f(x-2)+f(x)=\sqrt { 3 } f(x-1)$ From these three equations we get $f(x-2)+f(x+2)=0$ Putting $x=x+2$ and adding with last equation we get $f(x-2)+f(x+4)=0$....(1) Similarly $f(x-4)+f(x+2)=0$.....(2) Put $x=x-6$ in (1) We get $f(x-8)+f(x-2)=0$.....(3) From (1) and (3) we get $f(x-8)=f(x+4)$ So the period of $f(x)$ is 12",['functions']
1274829,General Existence and Uniqueness of ODE,"I am trying to make sure that I understand the following question. As well as I am having a bit of trouble understand the partial solutions given. The question is basically, what can we say about the following IVP ? $$ y'=y^{1/3}$$ with initial value, $y(0)=0$ for $t \ge 0$ I know that the important theorem to be aware of is the theorem about unique solutions depending on continuity of the partial derivative w.r.t to y and existence depending on the continuity of f(t,y) itself. So what I am seeing initially is that the partial derivative (w.r.t to y) is continuous everywhere expect at $y=0$. and  $f(t,y)$ itself is continuous for all values of $t$. So does this Imply that we have a solution for all values of t, but we will only have a unique solution when $y \neq 0 $ ? Solving, using the fact that we have a separable equation gives me, $$y^{-1/3}dy=dt$$ Integrating both sides, $$\frac{3}{2}y^{2/3}=t+c$$ $$y^{2/3}=\frac{2}{3}(t+c)$$ $$y^2=[(2/3)(t+c)]^{3}$$ $$y= \pm [(2/3)(t+c)]^{3/2}$$ From our initial conditions we can solve easily for c, to get that $c=0$ that is $y=\pm (2/3)t^{3/2}$ Now here is my first point of confusion, the solution says for both these answers, $t \ge 0$  I think I am just lost in it and even though I understood what the theorem stated, I don't understand its implications. As well, from initial observation we know $y=\psi(t)=0$ is also a solution. And the final answer is that $y= \lambda (t)= 0$ if $0 \le t \le t_{o}$ and $\pm[(2/3)(t-t_{o})]^{3/2}$ if $t \ge t_{o}$ Is the reason we have that the second part is only for $t \ge t_{0}$ just to avoid taking square root of negatives? I am still confused about the final answer. I understand how we found that we could have $\pm [(2/3)(t+c)]^{3/2}$ and y=0 but what I am not understand is how from that we can obtain y=0 if $0 \le t \lt t_{o}$ and $\pm [(2/3)(t-t_{o})]^{3/2}$  if $t \ge t_{o}$? How was this done? It just directly stated this. Mainly, how is it that it went directly to saying 
Can someone please help tie this together for me? Thanks a lot",['ordinary-differential-equations']
1274834,Absolute Value of Complex Integral,"Let $[a,b]$ be a closed real interval. Let $f:[a,b] \to \mathbb{C}$ be a continuous complex-valued function. Then $$\bigg|\int_{a}^{b} f(t)dt \ \bigg| \leq \int_{a}^{b} \bigg|f(t)\bigg| dt,$$ where the first integral is a complex integral, and the second integral is a definite real integral. There's a neat ""rotational"" proof of this in D'Angelo's An Introduction to Complex Analysis and Geometry . Question: Can this fact also be proven using the Cauchy-Schwarz Inequality? If so, some help would be nice. Thank you...","['complex-analysis', 'inequality']"
1274884,Solve for $y$ explicitly or prove that it is impossible,"Let's say you want to solve for $y$ explicitly (it is by itself on one side of the equation with no $y$'s on the other side) in terms of any sort of function (elementary or non-elementary) in the following equation: $$\sin(y)+e^y-xy=0$$ Is this possible? Or, if it isn't, prove that it is impossible. Edit: After seeing an answer, I am providing another example:
$$\sin(xy)-\frac xye^y=0$$ This is a very ""over the top"" example, but my question is the same. Edit #2: After seeing jgon's answer (thank you), I was wondering: Is there any way to prove that these are impossible to solve explicitly for $y$ without using graphs?",['algebra-precalculus']
1274890,Limits for the integral of a joint distribution,"I'm slightly confused on how to calculate the limits of a joint probabibilty distribution (continous case). For example, the following question I'm unsure of how the limits for the respective integrals were calculated, and would appreciate if someone could talk me through how to find the limits for the integral in general. Thanks. Edit: I understand why the inner integral is from 0 to 1-y, but if I used the same principle to calculate this, shouldn't the outer integral be from 0 to 1-x? I'm simply looking for the highest and lowest values of x and y which satisfy each inequality given in the question.","['probability', 'integration']"
1274897,Expected Value and Variance of transformed Random variable,"I am trying to find the expected value and variance of $Y_i=\ln(X_i)$ for $X$ is uniformly distributed between $1$ and $3$. I believe that $E(Y_i)=(\ln3)/2$ and $\operatorname{Var}(x)=(\ln3)^2/12$. Could someone please confirm this to be true? Isn't $Y$ uniformly distributed on $[0,\ln(3)]$ now?","['probability', 'proof-verification', 'probability-distributions']"
1274914,"Show that if $g \circ f$ is injective, then so is $f$.","The Problem: Let $X, Y, Z$ be sets and $f: X \to Y, g:Y \to Z$ be functions. (a) Show that if $g \circ f$ is injective, then so is $f$. (b) If $g \circ f$ is surjective, must $g$ be surjective? Where I Am: So, I really have trouble with these, for some reason. I can draw pictures and make sense of the problems, but writing down proofs is very difficult for me. Basically, for (a), I ended up with some complicated statement involving an implication implying another implication and then tried to derive a contradiction. It just got so convoluted that I couldn't make sense of it anymore, and I know there's a quick, elegant way to show it. For (b), I know that $g$ need not be surjective. Once again, though, proving it directly from definitions has given me a bit of a headache. Any help here would be appreciated. Thanks in advance. The Proofs! Ok, I did it. Thanks for the help, everyone! Let me know if there's anything wrong with these proofs, or if they could bet any better. (a) Suppose $f$ is not injective. Then
$$ f(x_1)=f(x_2) \implies x_1 \ne x_2 \text{        }(*).$$
Let $f(x_1)=y_0=f(x_2)$ and let $g(y_0)=z_0$. Then $$ (g \circ f)(x_1) = (g \circ f)(x_2) = g(y_0) = z_0. $$ Since $g \circ f$ is injective,
$$ (g \circ f)(x_1) = (g \circ f_2) \implies x_1 = x_2. $$
However, this contradicts $(*)$. Therefore, $f$ must be injective. (b) Suppose $g$ is not surjective. Then $$ \forall y \in Y, \exists z \in Z \text{ such that } g(y) \ne z \text{         }(**).$$ Since, $g \circ f$ is surjective, $$ \forall z \in Z, \exists x \in X \text{ such that } g(f(x)) = z \text{       } (***). $$ Let $f(x) = y$. Then, $$ g(f(x)) = g(y) = z. $$ Because of $(***)$, this is true for all $z \in Z$, which contradicts $(**)$. Therefore, $g$ must be surjective.","['proof-writing', 'discrete-mathematics', 'functions']"
1274924,Eigenvalues of $A^TA$,"Suppose $A$ is a $n\times n$ matrix in $M(\mathbb{R})$ . I'd like to know if the eigenvalues of $A^TA$ have closed forms based on those of $A$ and $A^T$ . Clearly it's false to assume all eigenvalues of $A^TA$ are of the form $\lambda^2$ , where $\lambda$ is an eigenvalue of $A$ , except when $A$ is normal. Can someone provide a proof or more examples?","['linear-algebra', 'matrices']"
1274936,Proving $\sqrt{ab} = \sqrt a\sqrt b$,"I am currently in high school and we are studying radicals. I had asked my math teacher why $\sqrt{ab}=\sqrt{a}\sqrt{b}$ (for all a,b>0) and he tries to prove it by arguing that $a^{1/2}*b^{1/2}=(ab)^{1/2}$ (an exponent law). However, I find this proof problematic since $x^{1/2}$ is simply defined as $\sqrt{x}$, so the reasoning is circular. My view is that $\sqrt{ab}=\sqrt{a}\sqrt{b}$ because once we square both sides we get $ab=ab$. Since we're obviously referring to the positive root, and the function $f(x) = \sqrt{x}$ is injective, it necessarily follows that the original expressions $\sqrt{ab}$ and $\sqrt{a}\sqrt{b}$ are equivalent because for injective functions it is not possible to map distinct elements in the domain to the same element in the range ($ab$). Hence they are equivalent expressions. My question therefore is, is my proof valid and/or rigorous (I find it convincing but maybe it's wrong; I just want to be clear) and secondly was my teacher's proof correct?",['algebra-precalculus']
1274939,Varieties and ideals,"I'm doing the exercises from Fulton of Algebraic Geometry and I'm stuck in the problem 2.44 Let $V$ be a variety in $\mathbb{A}^{n}$, $I=I(V)\subset k[x_{1},\ldots,x_{n}]$, $P\in V$ and let $J$ be and ideal $k[x_1,\ldots,x_n]$ which contains $I$. Let $J'$ be the image of $J$ in $\Gamma(V)$. Prove that exists a natural homomorphism $\varphi$ from $\mathcal{O}_{P}(\mathbb{A}^{n})/J\mathcal{O}(\mathbb{A}^{n})$ to $\mathcal{O}_{p}(\mathbb{V})/J'\mathcal{O}(\mathbb{V})$, and prove that $\varphi$ is an isomorphism. In particular, $\mathcal{O}_{P}(\mathbb{A}^{n})/I\mathcal{O}(\mathbb{A}^{n})$ is isomorphic to $\mathcal{O}_{P}(V)$ If anyone can help I'll really appreciate it :)","['algebraic-geometry', 'ideals']"
1274948,How to determine the expectation of the square of a binomial collection,"I'm trying to find how to express the expectation of the square of a collection of binomial measurements. If we have a collection: $$A = a_1 + a_2 + \cdots + a_n$$ The expectation of $A$ is the sum of the expectations of each term of $A$. However, for the square of $A$, I'm getting stuck on how $np +(n^2-n)p^2$ is derived. Without any background in number theory, I'm trying to find it as follows, assuming a set of three trials in $A$: $$E(A^2) = E((a_1 + a_2 + a_3)(a_1 + a_2 + a_3))$$
$$E(A^2) = E(a_1a_1 + a_1a_2 + a_1a_3 + a_2a_1 + a_2a_2 + a_2a_3 + a_3a_1 + a_3a_2 + a_3a_3)$$
$$E(A^2) = E(a_1a_1 + a_2a_2 + a_3a_3)+ 2 E(a_1a_2 + a_1a_3 + a_2a_3))$$ ...and from here it grinds to a halt. I am not sure how to evaluate these pairings, and what happens to the probabilities in each. For the final product, I get the idea that in squaring we have $n=3$ that are equal, and the remaining being the entire square of values minus these, which would be $n^2-n = 6$, but then how the probabilities are handled is not making sense. I'm not sure why the $n^2-n$ term has a squared probability when the first one has no square. I'm hoping someone can explain this in basic terms.","['binomial-distribution', 'statistics', 'expectation']"
1274971,"Suppose $\{v_1,v_2,v_3\}$ is a basis for some subspace $V$ of $\mathbb R^m$.","Let $b$ be a vector in that subspace. Prove that if $b$ is orthogonal to all three basis vectors, then b has to be a zero vector. Hint: What is $\|b\|$ I do not know how to start this proof. Thanks in advance for any help i get.","['vectors', 'linear-algebra', 'multivariable-calculus', 'proof-writing']"
1274980,Prove that $ND = DN$ where $D$ is a diagonalizable and $N$ is a nilpotent matrix.,"Let $A$ be an $n \times n$ complex matrix. Prove that there exist a diagonalizable matrix $D$ and a nilpotent matrix $N$ such that a. A = D + N b. DN = ND and show that these matrices are uniquely determined. I think I've solved the part a but don't have an idea to continue. Here is what I've tried: Let $D = \begin{bmatrix}
a_{11} & 0 \\
* & a_{nn} \\
\end{bmatrix}$ and $N = \begin{bmatrix} 0 & * \\ 0 & 0 \end{bmatrix}$.
Since D is a lower triangular matrix, its determinant is equal to product of its diagonal entries and hence its characteristic polynomial is $(x-a_{11})...(x-a_{nn}).$ Then D is diagonalizable. Similarly, characteristic polynomial of $N$ is $x^n$ then by Cayley-Hamilton $N^n = 0$. Update: My assumption for D to be diagonalizable was wrong, eigenvalues need not to be distinct. Update after answers : Thank you all for your help. It was just a question asked in the end of the chapter ""Canonical Forms"". So I just know Jordan form, Rational form, etc. I don't know anything about the Lie Algebra, Semi simple matrices, Representation theory, Perfect field mentioned in the answers. Honestly, answers didn't help me to understand the solution but they seem useful so maybe they help other people. Thanks.","['jordan-normal-form', 'linear-algebra', 'diagonalization', 'matrices']"
1274992,What's so special about $p=2$ for the $L^p$ spaces?,"The Banach space dual of $L^p$ is $L^q$, where $q=\frac{p}{p-1}$, but I don't really understand the motivation behind this. In particular, I find it kind of surprising that the only $L^p$ space whose dual is isomorphic $L^p$ is for $p=2$. So I guess I'm wondering what's so special about the number 2 in the context of $L^p$ spaces, or rather, where the formula $\frac 1 p + \frac 1 q = 1$ originally comes from / is motivated from. Edit: I understand that $p=2$ gives the only Hilbert space, but I'm wondering whether there's any sort of deeper reason behind the relationship between $p$ and $q$ --- does $\frac 1 p + \frac 1 q = 1$ arise naturally out of integration theory in a more satisfactory way than ""it just happens to be like that""?","['lp-spaces', 'functional-analysis', 'integration']"
1275019,Summation of a function 2,"Let $n$ is a positive integer. $n = p_1^{e_1}p_2^{e_2}\cdots p_k^{e_k}$ is the complete prime factorization of $n$. Let me define a function $f(n)$ $f(n) = p_1^{c_1}p_2^{c_2}\cdots p_k^{c_k}$ where $c_k = e_k - 1$ Example: $72 = 2^33^2$, so $f(72) = 2^{3-1}3^{2-1} = 2^{2}3^{1}=12$ $144 = 2^43^2$, so $f(144) = 2^{4-1}3^{2-1} = 2^{3}3^{1}=24$ Now let $$F(N) = \sum_{n=2}^N f(n)$$ Example: $F(10) = 1 + 1 + 2 + 1 + 1 + 1 + 4 + 3 + 1 = 15$ Now I want to evaluate $F(N)$ for a fairly large value of $N$, say $10^{12}$. Can I do it without factorizing each number?","['summation', 'number-theory', 'mobius-function', 'arithmetic-functions', 'zeta-functions']"
1275025,Does this reasoning work?,"Consider the following system of ODEs. $$
\theta'=r\\
r'=1-r^2
$$ On the unit circle, $\theta'=1$, and $r'=0$ Now consider the system $$
\theta'=1\\
r'=0
$$ The solution curves to this system are circles. Is this enough to conclude that the unit circle is a solution to the first system? Does this reasoning work in general? In other words, given two systems $x'=f(x)$, and $x'=g(x)$, with $x\in\mathbb{R}^n$, and a curve $h(x)=0$. If $f(x)=g(x)$ on $h(x)=0$, and $h(x)=0$ is an invariant set under the flow of one dynamical system, is it necessarilly an invariant set of both?","['proof-verification', 'ordinary-differential-equations']"
1275070,Does an inseparable extension have a purely inseparable element?,Assume $K/F$ is an inseparable extension. Is it necessary that $K$ contains an element $u \notin F$ that is purely inseparable over $F$ ? I also posted in MO.,"['abstract-algebra', 'field-theory', 'galois-theory']"
1275071,How does $-\frac{1}{x-2} + \frac{1}{x-3}$ become $\frac{1}{2-x} - \frac{1}{3-x}$,"I'm following a solution that is using a partial fraction decomposition, and I get stuck at the point where $-\frac{1}{x-2} + \frac{1}{x-3}$ becomes $\frac{1}{2-x} - \frac{1}{3-x}$ The equations are obviously equal, but some algebraic manipulation is done between the first step and the second step, and I can't figure out what this manipulation could be. The full breakdown comes from this solution
$$
\small\begin{align}
\frac1{x^2-5x+6}
&=\frac1{(x-2)(x-3)}
=\frac1{-3-(-2)}\left(\frac1{x-2}-\frac1{x-3}\right)
=\bbox[4px,border:4px solid #F00000]{-\frac1{x-2}+\frac1{x-3}}\\
&=\bbox[4px,border:4px solid #F00000]{\frac1{2-x}-\frac1{3-x}}
=\sum_{n=0}^\infty\frac1{2^{n+1}}x^n-\sum_{n=0}^\infty\frac1{3^{n+1}}x^n
=\bbox[4px,border:1px solid #000000]{\sum_{n=0}^\infty\left(\frac1{2^{n+1}}-\frac1{3^{n+1}}\right)x^n}
\end{align}
$$ Original image","['fractions', 'algebra-precalculus']"
1275079,Recommendation of multivariable calculus books,"I am looking for some suggestions on a good calculus book I shall keep on hand all the time. I am a graduate student who will be commencing research in the area of theoretical PDE (nonlinear). However I often get stuck on some basic calculus facts where most undergrad knows. My maths background is very applied(financial maths) hence I am lacking the actual preparation to work in theoretical PDE. However, it is too late for me to turn back. Very often as I feel, research in theoretical discipline (especially the analysis of PDE) requires nothing advanced but rather some delicate calculus and real analysis (perhaps at high school level) I found Stewart Calculus: concepts and context helpful helpful since we did not learn how to calculate stuff like surface integral (or any those engineering kind). But the book is too big and quite difficult to find a copy from the library (since first year students have the priority) Spivak is also good. But too little multivariable stuff. If I can find a book contains all that calculus facts allows one to study functional analytics aspects of nonlinear PDE, would be great! Any suggestions appreciated.","['book-recommendation', 'reference-request', 'multivariable-calculus', 'soft-question']"
1275084,Is $\cos(x) \times \cos(2x)$ the same as $\cos(3x)$,When you multiply $\cos(x) \times \cos(2x)$ the same as $\cos(3x)$ or do you have to treat each differently?,['algebra-precalculus']
1275087,"Prove $\int_{-\infty}^{\infty} \frac{x\cos(x)}{(x-2)(x-1)}\,dx = \pi(\sin(1)-2\sin(2))$. Use an indented contour.","To do this I used the Residue Thm but the main issue here is that I cannot get the sine term to appear. Perhaps I'm ignoring something here. We know that the singularity is $x=1,2$ so we should just calculate the residue at these two points as follows: \begin{equation*}
Res(f,2)=-\cos1, \\
Res(f,1)=2\cos2.
\end{equation*} Once we multiply this and sum it we get: $2\pi i(-\cos1+2\cos2)$. No sines appear. Can someone correct me here?","['complex-analysis', 'complex-numbers', 'residue-calculus']"
1275100,Prove $g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$ from definition of limit,"I want to prove $g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$ from the definition of limit, where x is a 1D variable, $o(h)$ is a quantity that depends on scalar h negligible compared to h as h goes to zero i.e. a small quantity Attempt: Recall definition of limit $$\lim_{h\to 0} |\frac{g(x+h) - g(x) - hg'(x)}{h}| = |\frac{o(h)}{h}|$$
which turns into a more familiar form
$$\lim_{h\to 0}  |\frac{g(x+h) - g(x)}{h}| = g'(x)$$ Conclusion 
$$g(x+h) - g(x) = hg'(x) + o(h)$$ Taking a derivative wrt $x$ in above expression ields $$g'(x+h) - g'(x) = hg''(x)$$ Sub in definition of derivative for both LHS terms $$g(x+h^2) - g(x+h) - g(x+h) + g(x) = h^2 g''(x) + ho(h)$$ Therefore $$g(x+h^2) = g(x) + 2hg'(x) + h^2 g''(x) + o(h^2)$$ dividing a 2 across $$\frac{g(x+h^2)}{2}  = \frac{g(x)}{2} + hg'(x) + \frac{1}{2} h^2g''(x) + o(h^2)$$ which is not quite...
$$g(x+h) = g(x) + hg'(x) + \frac{1}{2} h^2 g''(x) + o(h^2)$$ the desired result Can someone show me how to prove this?","['calculus', 'derivatives']"
1275113,Find the particular solution of $y''+y=\cos(t)\cos(2t)$,"Find the particular solution of $L[y]=y''+y=\cos(t)\cos(2t)$ Here my steps: Homogeneous: $y''+y=0$ Roots $+i$ and $-i$ General solution : $y(t)=c_1\cos(t)+c_2\sin(t)+\psi(t)$ Since we are dealing with cosine then we can rewrite the right hand side as: $y''+y=e^{it}e^{2it}=e^{{it}(1+2)}v$ Now to guess: Let: $\psi=e^{{it}(1+2)}v$ $\psi'=e^{{it}(1+2)}v'+3ie^{{it}(1+2)}v$ $\psi''=e^{{it}(1+2)}v''+3ie^{{it}(1+2)}v'+3ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v$ Plug back into $L[y]$: $L[y]=e^{{it}(1+2)}v''+6ie^{{it}(1+2)}v'-9e^{{it}(1+2)}v+e^{{it}(1+2)}v=e^{{it}(1+2)}v$ Dividing each side by $e^{{it}(1+2)}$ We obtain: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ Let : $v(t)=a_0+a_1t+a_2t^2$ $v'(t)=a_1+2ta_2$ $v''(t)=2a_2$ Plug back into: $L[y]=v''+6iv'-9v+v=v''+6iv'-8v=1$ $L[y]=2a_2+6ia_1+12ita_2-8a_0-8a_1t-8a_2t^2=1$ Equating coefficients: $$2a_2+6ia_1-8a_0=1$$
$$ 12ia_2-8a_1=0$$
$$-8a_2=0$$ from there I got: $$ a_2=0$$
$$ a_1=0$$
$$a_0=\frac{-1}{8}$$ Plugging back into $v(t)$ $$v(t)=\frac{-1}{8}$$ Plugging back into $\psi=e^{{it}(1+2)}v$ $$\psi=e^{{it}(1+2)}\frac{-1}{8}$$ Replacing the exponential with sine and cosines. The particular solution must be contain real values since the right hand side has cosine. $$\psi=[\cos(t)+i\sin(t)][\cos(2t)+i\sin(2t)]\frac{-1}{8}$$ I only multiplied the real parts: $$\psi=\frac{-1}{8}[\cos(t)\cos(2t)-\sin(t)\sin(2t)]$$ The inside looks like the sum and angle formula so we have: $$\psi=\frac{-1}{8}\cos(2t+t)$$ but sadly, I went wrong since the answer key says: $\psi(t)=-\frac{1}{16}\cos(3t)+\frac{1}{4}t\sin(t)$ Honestly, I'm not sure where I went wrong",['ordinary-differential-equations']
1275118,Convergence of a sequence 4,"Suppose there is a sequence $\{ x_n \}$. Let us define another sequence $\{ y_n \} $ such that  $$y_n=2x_{n+1} - x_n$$ Please prove that if $\{y_n\}$ converges to $L$, then $\{x_n\}$ is convergent and also converges to $L$. I have approached this problem like: If I consider that $\{x_n\}$ converges to $\alpha$, then taking limit, we can show that:
$$L=\lim y_n = \lim (2x_{n+1}-x_n)=2\lim x_{n+1}-\lim x_n = 2\alpha-\alpha = \alpha$$ However, I am having problem in proving that limit for $\{x_n\}$ exists.","['sequences-and-series', 'convergence-divergence', 'limits']"
1275122,Can we construct a function that has uncountable many jump discontinuities?,"I know that Dirichlet function has uncountable many discontinuities. I think they are removable, because the discontinuities can be removed by redefining the function values of the rational numbers as 0. So Dirichlet function is a function that has uncountable many removable discontinuities, then my question is can we construct a function with uncountable  many jump discontinuities? If not, how do we prove it is impossible? Thank you. An odd but similar question is can we have a function that has uncountable many infinite discontinuities?",['real-analysis']
1275124,Using Cauchy Integral Formula $\int_C \frac2{z^2 -1}dz$,"I want to understand why I can't use Cauchy Integral Formula for the following problem: $$\int_C \frac2{z^2 -1}dz\text{ on the contour } |z-1|=\frac12$$
Now it says that I need $f$ to be analytic everywhere inside and on a simple closed contour. My contour is surely simple and closed. To check that $f$ is analytic, I need to use the Cauchy-Riemann equations I imagine - but let's ignore that since it is known to be analytic(two polynomials are analytic?) So this has one singularity at $z=1$, so then I want to use: $$\int_C\frac{2}{z^2-1}=\int_C \frac{2}{(z^2-1)(z-1)} dz=2\pi i f(1)$$
But $f(1)$ makes this diverge, what's the deal?","['contour-integration', 'complex-analysis']"
1275133,How high a priority does discrete math have for people who want to become machine learning practitioners?,"Machine learning seems to depend on such math fields as probability, statistics, calculus, and linear algebra. @pranav suggested discrete math would be an important prerequisite.
However, someone else a discrete math book would be on a low priority if becoming a machine learning practitioner was my top priority. Although I also want to become a software engineer as well as machine learning practitioner/researcher, I am a professional software engineer already, and I need to learn software engineering, math, and machine learning on my free time. If I was in a 4-year university curriculum, I would definitely start with discrete math. How should I deal with discrete math? Should I learn it after probability, statistics, calculus, and linear algebra? Do I just skip it? Or,  do I need learn it first?","['machine-learning', 'discrete-mathematics']"
1275137,"Limit of $\frac{\sin(x+y)}{x+y}$ as (x,y)→(0,0)","$$
\lim\limits_{(x, y)\to (0, 0)}\frac{\sin(x+y)}{x+y}
$$ I did the following $a)$ along $x$ axis, the limit is one $b)$ along $y$ axis the limit is one $c)$ along $y=x$ the limit is one Since there exists more ways to approach the origin, I know I cannot conclude from the steps given above. $d)$ along $y= -x$ $\frac{\sin(x-x)}{x-x}$ is not defined. Isn't it still possible for the function to have a limit, even though it is not defined at that point? How do I conclude whether a limit exists or doesn't in such a case? EDIT So for,
$$
\lim\limits_{(x, y)\to (0, 0)}\frac{\sin(xy)}{xy}
$$
Can I proceed in a similar manner and perform a substitution and state the limit is 1?","['limits-without-lhopital', 'multivariable-calculus', 'limits']"
1275168,Prove $(8k)^{8k}+(8k+1)^{8k+1}$ and $(8k+1)^{8k+1}+(8k+2)^{8k+2}$ are never perfect squares,"Prove $$(8k)^{8k}+(8k+1)^{8k+1}\ \ \text{ and } \ \ \ (8k+1)^{8k+1}+(8k+2)^{8k+2}$$ are never perfect squares ($k\ge 1$). mod $8$ gives $1$ for both, which is a quadratic residue, so doesn't solve it. Found in AoPS .","['number-theory', 'square-numbers', 'elementary-number-theory']"
1275176,Limit with polylog,"How do you show the following limit? 
$$\lim_{x\to\infty} x\log(-e^x + 1)+\operatorname{Li}_2(e^x)-\frac12x^2=\frac{\pi^2}3$$
Where $\operatorname{Li}_n(x)$ is the polylogarithm. This question is inspired by a thread in the sagemath mailinglist.","['asymptotics', 'calculus', 'limits', 'polylogarithm']"
1275194,What is rational integral function,"The author of the book I am referring assumes two rational integral functions as shown below $p_0\cdot x^n+p_1\cdot x^{(n-1)}+p_2\cdot x^{(n-2)}+...+p_n$ $q_0\cdot x^n+q_1\cdot x^{(n-1)}+q_2\cdot x^{(n-2)}+...+q_n$ Is this term old and he means real functions ?; if not, what is meant by rational integral function ?, rational function, integral function ?","['calculus', 'algebra-precalculus']"
1275217,Probability of Bit Errors Poisson Question,"I'm not quite sure how to get the correct probability for this question. Q: The probability of error in the transmission of a binary digit over a communication channel is 1/10^3. Write an expression for the exact probability of more than 3 errors when transmitting a block of 10^3 bits. What is its approximate value? Assume independence. Since it's approximate, I'm using Poisson's distribution formula. The expected value (for λ) is 1, I believe. K would be 3. However, by plugging these values in and obtaining the result, I'm only getting the answer for exactly 3 errors. I need to get the probability of receiving more than three errors. Initially, I thought it would be: 1 - P(0 errors) - P(1 error) - P(2 errors) - P(3 errors) However, I'm not sure if this is correct or if there's a possibility of overlap. Can anyone help in what the probability should be or if I'm going about this problem in the correct way?","['probability', 'statistics']"
1275237,Scheme whose points over $x\colon\mathrm{spec}(R)\to X$ are the isomorphisms $x^*(F)$ and $x^*(E)$?,"If one has two vector bundles $E\to X$ and $F\to X$ over a scheme $X$, why is there a scheme $S$ over $X$ with points of $S$ over a point $x\colon\mathrm{spec}(R)\to X$ is precisely the set of isomorphisms of $x^\ast(F)$ and $x^\ast(E)$? I think this is a standard result, but going back I can't find it in the literature.","['algebraic-geometry', 'reference-request', 'schemes']"
1275241,How can you alter the Volterra-Lotka system to obtain a model of cooperative species?,"The Volterra-Lotka system for two competitive species is: \begin{equation*}
\frac{dx}{dt} = x(-Ax-By+C) \\
\frac{dy}{dt} = y(-DX-Ey+F)
\end{equation*} where $x,y\geq 0 $ and $ A,B,C,D,E,F$ all positive. The Model system for cooperative species is: \begin{equation*}
\frac{dx}{dt} = x(-Ax+By+C) \\
\frac{dy}{dt} = y(DX-Ey+F)
\end{equation*} but I dont know who to explaining  using words. what happens when $x$ decreases or when $y$ decrease. And if $x$ increases then $y$ increases.",['ordinary-differential-equations']
1275262,"How many $n-$digit number that contain only digits $ 1,2,3,4,5,6$","How many $n- $digit numbers can be formed from the digits $1,2,3,4,5$ and 6, which contains the  numbers $1$ and $2$ as neighbours. Let $p_n$ be the number of n-digit numbers which consist only of the digits 1,2,3,4,5,6, which contains the  numbers $1$ and $2$ as neighbours. $$p_{n+1}=p_{n}+(?)$$
Now How to find  $(?)$",['combinatorics']
1275268,How to know if an integral is well defined regardless of path taken.,"I can calculate \begin{equation*}
\int_0^i ze^{z^2} dz=\frac{1}{2e}-\frac12, 
\end{equation*} but why can I calculate this irrelevant to the path taken? Is this since it is analytic everywhere - if so, how would I go about verifying this? I can't see how to apply the Cauchy Riemann equations here since I don't know how I would break this into the sum of a real and complex component.","['contour-integration', 'complex-analysis']"
1275281,Strongly convex cones,"A polyhedral cone is strongly convex if $\sigma \cap -\sigma =\{0\}$is a face. Then here is the following proposition. Let $\sigma$ be a strongly convex polyhedral cone. Then the following are equivalent. $\sigma$ contains no positive dimensional subspace of $N_\mathbb{R}$ $\sigma \cap (-\sigma)= \{0\}$ $\dim(\sigma^\vee)=n$ This proposition can be found in almost every paper I can find online which talks about strongly convex polyhedral cones, but I cant find a single one which includes the proof. Anyone who knows a rigorous proof who can help me, or who knows where to find one?",['algebraic-geometry']
1275320,Computing two variables limit,"I'm trying to compute the following limit: $$\lim_{(x,y)\to (\infty,\infty)} \frac{x^2+y^2}{x^2+y^4}$$ I think that the limit is actually path dependent, thus does not exist. If we are looking on the path $(x,y)=(t^2,k^2 t)$ for some $k\in \Bbb R$ we get that $$\lim_{(x,y)\to(\infty,\infty)}\frac{x^2+y^2}{x^2+y^4}=\lim_{t\to\infty}\frac{t^4+k^2t^2}{t^4+k^8t^4}=\lim_{t\to\infty}\frac{1+\frac{k^2}{t^2}}{1+k^8}=\frac{1}{1+k^8}$$
Hence, the limit is path dependent, so it does not exist. W|A claims that the limit is 0. What is wrong with my reasoning?",['limits']
1275321,How to obtain the series of the common elementary functions without using derivatives?,"First, I'm a freshman student of physics, not of mathematics, so please excuse my ignorance of mathematics :) Well, I'm reading the book ""Huygens and Barrow, Newton and Hooke"" by Vladimir Arnold, and one excerpt (at the page 43) called my attention greatly: ""he used Taylor's formula for calculating derivatives rather than using the derivatives for the expansion of functions"" (""he"" refers to Sir Isaac Newton). My main question is: How to obtain the series of the common elementary functions (trigonometric, exponential...) without using derivatives? And, for example, could calculus be developed in a way that we would get $\sin x = x - \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + \cdots$ and also the one for cosine and then use this series to find the derivative of sine by seeing that it reduces to the series of the cosine? P.S. I don't know English very well, I just hope the question asked is that I wanted to ask! Thanks in advance to everyone who answer.",['calculus']
1275331,$\int_{-\infty}^\infty\frac{1}{(x^2+a^2)^3}dx=\frac{3\pi}{8a^5}$ for $a>0$,"I've been trying to show that $\int_{-\infty}^\infty\frac{1}{(x^2+a^2)^3}dx=\frac{3\pi}{8a^5}$ for $a>0$ using complex analysis methods.
But for some reason I can't get it to come out. 
Perhaps someone could figure out where I am going wrong. Since there are no poles on the real axis, I know that
$\int_{-\infty}^\infty\frac{1}{(x^2+a^2)^3}dx=2\pi i\cdot\text{Res}\left(\frac{1}{(z^2+a^2)^3},ia\right).$ To calculate $\text{Res}\left(\frac{1}{(z^2+a^2)^3},ia\right)$
I used the fact that on a small enough disk centered at $ai$,
$\frac{1}{(z+ai)^3}=\sum\limits_{k=0}^\infty c_k(z-ai)^k$.
Thus
$\frac{1}{(z^2+a^2)^3}=\frac{\frac{1}{(z+ai)^3}}{(z-ai)^3}=\sum\limits_{k=0}^\infty c_k(z-ai)^{k-3}$.
So $\text{Res}\left(\frac{1}{(z^2+a^2)^3},ia\right)=c_2.$
Where $c_2=\frac{d^2}{dz^2}\frac{1}{(z+ai)^3}\bigg|_{z=ai}=\frac{12}{(2ia)^5}=\frac{3}{8ia^5}.$
But that gives me 
$\int_{-\infty}^\infty\frac{1}{(x^2+a^2)^3}dx=2\pi i\cdot\text{Res}\left(\frac{1}{(z^2+a^2)^3},ia\right)=2\pi i\cdot\frac{3}{8ia^5}=\frac{3\pi}{4a^5}.$ Which is off by $\frac{1}{2}$. I must be making a silly mistake somewhere, but I can't seem to find it. Any help would be appreciated.",['complex-analysis']
1275350,Moments bounds VS Chernoff bounds,"I have to prove that, when bounding tail probabilities of a nonnegative random variable, the moments method is always better than the classical Chernoff method. In mathematical language, I have to prove  that for every $t>0$, 
$$\inf_{n \in \mathbb{N}} \frac{\mathbf{E}[X^n]}{t^n} \leqslant \inf_{s>0} \frac{\mathbf{E}[e^{sX}]}{e^{st}}$$
This result seems to be classical and can apparently be found in a paper by Phillips and Nelsen (1995) available here for the modic sum of 14$. So instead of buying it, I tried to work it out, but the proof I have seems to be rather short for a 5-pages paper... Can somebody check my proof ? Let $s>0$. Then using power series expansion and monotone convergence theorem, we have $\mathbf{E}[e^{sX}] = \mathbf{E}\left[\sum_{n \geqslant 0}\frac{s^n X^n }{n!} \right] = \sum_n \frac{s^n \mathbf{E}[X^n]}{n!} $. Therefore, $$ \frac{\mathbf{E}[e^{sX}]}{e^{st}} = \frac{\sum_n \frac{s^n \mathbf{E}[X^n]}{n!}}{\sum_n \frac{s^n tX^n}{n!}} = \lim_{n \to \infty} \frac{1 + s \mathbf{E}[X] + ... + \frac{s^n \mathbf{E}[X^n]}{n!}}{1 + st + ... + \frac{s^n t^n }{n!}}$$ A classical inequality (often called Cauchy's third inequality ) states that for every nonnegative numbers $a_1,...,a_n,b_1,...,b_n$ one has
$$ \frac{a_1 + ... + a_n}{b_1 + ... + b_n} \geqslant \min_{k \leqslant n} \frac{a_k}{b_k}$$ Therefore, 
$$ \frac{1 + s \mathbf{E}[X] + ... + \frac{s^n \mathbf{E}[X^n]}{n!}}{1 + st + ... + \frac{s^n t^n }{n!}} \geq \min_{k \leq n} \frac{s^k \mathbf{E}[X^k] k!}{k! s^k t^k} = \min_{k \leq n}\frac{\mathbf{E}[X^k]}{t^k}$$ And finally, taking the limit we have, for every $s>0$ : 
$$ \frac{\mathbf{E}[e^{sX}]}{e^{st}} \geq \inf_{n \in \mathbb{N}}\frac{\mathbf{E}[X^n ]}{n!}$$
which is what we wanted to prove.","['probability-theory', 'proof-verification']"
1275357,Smooth points with obstructed deformations,"Let $k$ be an algebraically closed field, e.g. $k=\mathbb C$. Let $Art_k$ be the category of local Artin $k$-algebras with residue field $k$. A deformation functor is a functor $D:Art_k\to Sets$ such that $D(k)$ has one element. Let $D:Art_k\to Sets$ be a prorepresentable deformation functor, i.e. $D\cong h_R=\hom_k(R,-)$ for some local $k$-algebra $(R,m)$ with residue field $k$ and finite-dimensional tangent space $T=(m/m^2)^\vee$.
This is equivalent to the existence of an obstruction theory $(T_1,T_2)$ on $D$ such that for every small extension $\xi:0\to I\to B\to A\to 0$ there is an exact sequence of sets $$0\to T_1\otimes I\to DB\to DA\overset{ob_\xi}{\to}T_2\otimes I.$$
Now, $T_1$ is naturally isomorphic to $T$, which is finite-dimensional, say $\dim T=d$. If $\dim T_2=e$, then it is known that $$d\geq \dim R\geq d-e.$$ The vanishing $T_2=0$ would assure an isomorphism $R\cong k[[t_1,\dots, t_d]]$. Question . In what situations, different from $T_2=0$, do we get $R\cong k[[t_1,\dots, t_d]]$? For example, what about if $ob_\xi=0$
  for all small extensions $\xi$? Basically I am interested in understanding when a point $p$ on a moduli space $X$ is smooth. So for instance if $X$ is a fine moduli space then the deformation functor associated to $p$, i.e. the functor $$D_{X,p}:A\mapsto \{g:\textrm{Spec }A\to X\textrm{ such that }g|_{\textrm{Spec k}}=p\},$$ is isomorphic to $h_R$ where $R=\hat{\mathscr O}_{p}$. So we are in the above situation: we find some obstruction theory $(T,T_2)$ and if $T_2=0$ then $p$ is smooth. Actually if $R=k[[t_1,\dots, t_d]]/J$ then we can take $T_2=(J/\mathfrak nJ)^\vee$, where $\mathfrak n$ is the maximal ideal of $k[[t_1,\dots, t_d]]$. What else than $J=\mathfrak nJ$ can make $p$ a smooth point of $X$? Thank you for any help!","['algebraic-geometry', 'deformation-theory']"
1275358,How to generate $3 \times 3$ integer matrices with integer eigenvalues?,"I am looking for an easy way to generate non-trivial (i.e., not just diagonal) examples of $3 \times 3$ matrices whose entries are integers and whose eigenvalues are also integers. I know how to do this for $2 \times 2$ matrices: you just choose integers so that the discriminant of the characteristic equation is a perfect square. But short of trial and error, with the help of Wolfram Alpha, I don't know a way with larger matrices. Does anyone know how to do this?","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1275365,Approximating non-rational roots by a rational roots for a quadratic equation,"Let $a,b,c$ be integers and suppose the equation $f(x)=ax^2+bx+c=0$ has an irrational root $r$. Let $u=\frac p q$ be any rational number such that $|u-r|<1$. Prove that $\frac 1 {q^2} \leq |f(u)| \leq K|u-r|$ for some constant $K$. Deduce that there is a constant $M$ such that $|r \frac p q| \geq \frac M {q^2}$. (This is useful in approximating the nonrational zeros of a polynomial.) (I wasn't sure what a non-rational root is. And used intermediate value theorem for $x=u-1$ and $x=u$ but couldn't move further.)","['rational-numbers', 'approximation', 'roots', 'algebra-precalculus', 'irrational-numbers']"
1275395,$R^nf_*\mathbb{Z}$ trivial for a morphism with hypersurface fibers.,"I have some questions on local systems. If $f:X\to Y$ is a morphism of projective complex algebraic varieties, $Y$ being a curve, I want to prove that if the fibers of $f$ are smooth hypersurfaces in $\mathbb P^n$ then $R^nf_*\mathbb{Z}$ is a trivial localy system, isomorphic to the constant sheaf $\mathbb Z$. But I have no clue why such an assertion might be true. The only thing I can think of would be to base change to $U$, an open set in the curve, such that U is in the smooth locus of f, there I could use that if I take U sufficiently small, the f is a fibration, and compute the cohomology $H^n(U, \mathbb Z)$. But firstly I dont see how that helps. And in anycase I miss the place where $f$ is not smooth.
Could somebody help me?","['complex-geometry', 'algebraic-geometry', 'homology-cohomology']"
1275420,Is every subgroup of a normal subgroup normal?,"Is every subgroup of a normal subgroup normal ? That is if $H$ is a normal subgroup of a group $G$ and $K$ is a subgroup of $H$, then $K$ is a normal subgroup of $G$. Is it true ? If not what is the example? Progress $a\in G$ and $k\in K$. Then $k\in H$, since $K\subseteq H$. Now, $aka^{-1}=k_1aa^{-1}=k_1\in K$  [since $H$ is normal in $G$, $ak=k_1a$] This implies  that $K$ is normal in $H$. Is my approach correct ?","['group-theory', 'examples-counterexamples', 'normal-subgroups']"
1275436,Is every codimension one subvariety of a projective variety a set-theoretic complete intersection?,"Let $X$ be a projective variety over $\mathbb C$ and $D\subseteq X$ some subvariety which is pure of codimension one. In fact, in my case $D$ is the complement of an open affine subvariety $U\subseteq X$. My question is: Is there a single function $f$ in the projective coordinate ring of $X$ such that $D$ is the vanishing set of $f$? The commutative algebra version of this question would be the following: Given a finitely generated, graded $\mathbb C$-algebra $R$ which is an integral domain and a height one homogeneous prime ideal $\mathfrak p\subseteq R$, is there some $f\in R$ such that $\sqrt{\langle f\rangle}=\mathfrak p$? The Krull Principal Ideal Theorem states in this case that every minimal prime ideal over a (nontrivial) principal ideal is height one, so the question becomes whether each height one prime ideal admits some principal ideal over which it is minimal. My first instinct would be that it is not true, but I do not know a counterexample.","['algebraic-geometry', 'commutative-algebra']"
