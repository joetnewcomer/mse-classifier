question_id,title,body,tags
3586855,"When finding relative minimum/maximum, what is the point of using the second derivative test?",Why use the second derivative test over the first derivative test when finding maxima and minima if it's uncertain what $f''(x) = 0$ is? Why not just always use the first derivative since we need to take the first derivate either way?,['calculus']
3586891,homotopic $C^{2}$ paths are $C^{2}$ homotopic?,"The context is the following : Proposition : Let $E$ be a real Banach space, $\Omega \subseteq E$ open, $\omega \in C^{1}(\Omega,E^{*})$ a closed 1-form. Let be $\gamma_{0},\gamma_{1}$ $C^{2}$ two $\gamma$ -homotopic paths with fixed endpoints. It holds that $\int_{\gamma_{0}} \omega = \int_{\gamma_{1}} \omega$ . I do understand the proof I have of the above statement, which goes on defining $\phi(s) = \int_{\gamma(s,\bullet)} \omega$ and prooving that it's constant; What I don't get is the assumption (without loss of generality) that $\gamma$ , the homotopy between $\gamma_{0},\gamma_{1}$ , a priori $C^{0}$ , can actually be considered $C^{2}$ . Are there any known approximation Thorems for this? I thought about using Stone-Weierstrass or trying to approximate the homotopy with piecewise $C^{2}$ polygonals taken in appropriate dense set, failing. I found some references of what I'm looking for only between manifolds and smooth maps in the following links, but since manifold are above my knowledge i didn't find find any solution. Any help or comment would be appreciated,thanks. Smooth homotopy , Homotopic and Smoothly Homotopic Manifolds","['banach-spaces', 'approximation-theory', 'analysis', 'homotopy-theory', 'closed-form']"
3586908,Proof Verification - Convergence of a Sequence in $L^2(\mathbb{R})$,"Let $f$ be an element in the Sobolev space $H^1(\mathbb{R})=W^{1,2}(\mathbb{R})$ . For every $h>0$ , let $$f_h(x)=\frac{1}{h}(f(x+h)-f(x))$$ Prove that $$\lim_{h\to 0^+}\left\Vert f'-f_h\right\Vert_{L^2(\mathbb{R})}=0$$ My proof seems to be unnecessarily long and anyone who can kindly check if there are any mistakes will be appreciated! Let $\mathcal{D}$ denote the set of all smooth, compactly supported functions $\varphi:\mathbb{R}\to \mathbb{R}$ . It is well-known that $\mathcal{D}$ is dense in $H^1(\mathbb{R})$ . So there is a sequence $(\varphi_n)$ in $\mathcal{D}$ such that $$\lim_{n\to \infty}\left\Vert \varphi_n-f\right\Vert_{H^1(\mathbb{R})}=\lim_{n\to \infty}\left(\left\Vert \varphi_n-f\right\Vert_{L^2(\mathbb{R})}+\left\Vert \varphi_n'-f'\right\Vert_{L^2(\mathbb{R})}\right)=0$$ It follows that $$\lim_{n\to \infty}\left\Vert \varphi_n-f\right\Vert_{L^2(\mathbb{R})}=\lim_{n\to \infty}\left\Vert \varphi_n'-f'\right\Vert_{L^2(\mathbb{R})}=0$$ So both $\varphi_n$ and $(\varphi_n')$ are convergent, hence Cauchy, in $L^2(\mathbb{R})$ . Hence, for any $\epsilon>0$ , there exists an $N_1\in \mathbb{N}$ such that $$i,j>N_1\Rightarrow\left\Vert \varphi_i'-\varphi_j'\right\Vert_{L^2(\mathbb{R})}<\epsilon$$ For every $h>0$ and every $n\in \mathbb{N}$ , define the function $$\varphi_{h,n}(x)=\frac{1}{h}(\varphi_n(x+h)-\varphi_n(x))$$ Now let $n$ be fixed. By the mean value theorem, for every $x$ , there exists some $y\in (x,x+h)$ such that $$\left|\varphi_n'(x)-\varphi_{h,n}(x)\right|^2=\left|\varphi_n'(x)-\varphi_n'(y)\right|^2\leq 2\left\Vert \varphi_n\right\Vert_{C^1(\mathbb{R})}^2=M_n$$ So the sequence $(\left|\varphi_n'-\varphi_{h,n}\right|^2)_h$ is dominated by the Lebesgue integrable function $g_n=M_n\cdot\chi_{\text{support}(\varphi_n)}$ (which is a scalar multiple of a characteristic function). By the dominated convergence theorem, $$
\begin{aligned}
\lim_{h\to 0}\int_{\mathbb{R}}\left|\varphi_n'(x)-\varphi_{h,n}(x)\right|^2dx
&=\int_{\mathbb{R}}\lim_{h\to 0}\left|\varphi_n'(x)-\varphi_{h,n}(x)\right|^2dx \\
&=\int_{\mathbb{R}}\left[\lim_{h\to 0}\left|\varphi_n'(x)-\varphi_{h,n}(x)\right|\right]^2dx \\
&=0
\end{aligned}
$$ So for every $n\in \mathbb{N}$ we have $$\lim_{h\to 0}\left\Vert \varphi_n'-\varphi_{h,n}\right\Vert_{L^2(\mathbb{R})}=0$$ Furthermore, when $h$ is fixed, $(\varphi_{h,n})_n$ converges to $f_h$ in $L^2(\mathbb{R})$ because $\varphi_n$ converges to $f$ in $L^2(\mathbb{R})$ . In particular, $(\varphi_{h,n})_n$ is Cauchy in $L^2(\mathbb{R})$ . Hence, for any $\epsilon>0$ , there exists an $N_2\in \mathbb{N}$ such that $$i,j>N_2\Rightarrow\left\Vert \varphi_{h,i}-\varphi_{h,j}\right\Vert_{L^2(\mathbb{R})}<\epsilon$$ Finally, we know that for arbitrary $m,n,p\in \mathbb{N}$ , $$
\left\Vert f'-f_h\right\Vert_{L^2(\mathbb{R})}
\leq
\left\Vert f'-\varphi_m'\right\Vert_{L^2(\mathbb{R})}+
\left\Vert \varphi_m'-\varphi_n'\right\Vert_{L^2(\mathbb{R})}+
\left\Vert \varphi_n'-\varphi_{h,n}\right\Vert_{L^2(\mathbb{R})}+
\left\Vert \varphi_{h,n}-\varphi_{h,p}\right\Vert_{L^2(\mathbb{R})}+
\left\Vert \varphi_{h,p}-f_h\right\Vert_{L^2(\mathbb{R})}
$$ Fix an $\epsilon>0$ . (i) Choose an $m_0>N_1$ and an $n_0>\max\{N_1,N_2\}$ such that $$\left\Vert f'-\varphi_{m_0}'\right\Vert_{L^2(\mathbb{R})}<\frac{\epsilon}{5};\qquad \left\Vert \varphi_{m_0}'-\varphi_{n_0}'\right\Vert_{L^2(\mathbb{R})}<\frac{\epsilon}{5}$$ (ii) There exists an $h_0>0$ such that whenever $0<h<h_0$ , $$\left\Vert \varphi_{n_0}'-\varphi_{h,n_0}\right\Vert_{L^2(\mathbb{R})}<\frac{\epsilon}{5}$$ (iii) For any fixed $h\in (0,h_0)$ , choose a $p_0>N_2$ such that $$\left\Vert \varphi_{h,n_0}-\varphi_{h,p_0}\right\Vert_{L^2(\mathbb{R})}<\frac{\epsilon}{5};\qquad \left\Vert \varphi_{h,p_0}-f_h\right\Vert_{L^2(\mathbb{R})}<\frac{\epsilon}{5}$$ We conclude that for every $h\in (0,h_0)$ , $$\left\Vert f'-f_h\right\Vert_{L^2(\mathbb{R})}<\epsilon$$ giving the result.","['solution-verification', 'functional-analysis', 'real-analysis']"
3586913,What is the derivative of $\mathbf{a}^T\mathbf{X}^2\mathbf{b}$ wrt the matrix $\mathbf{X}$?,"Given vectors $\mathbf{a}$ and $\mathbf{b}$ , I am looking for the derivative of the following scalar function $$y(\mathbf{X}) = \mathbf{a}^T\mathbf{X}^2\mathbf{b}$$ with respect to matrix $\mathbf{X}$ . I couldn't find a direct answer from Wikipedia .","['matrices', 'matrix-calculus', 'derivatives']"
3586955,Product of a finite family of derived groups,"Let $k\in\mathbb{N}$ and $(G_i)_{1\leq i\leq k}$ a finite sequence of groups. I want to prove $$\prod_{i=1}^k[G_i,G_i]=\left[\prod_{i=1}^k G_i,\prod_{i=1}^kG_i\right].$$ For $k=2$ , I am able to derive the equality $$\prod_{i=1}^2[G_i,G_i]=\left[\prod_{i=1}^2 G_i,\prod_{i=1}^2G_i\right].$$ However, assuming the equality for arbitrary $k\ge 2$ , I am only able to obtain the isomorphism $$\prod_{i=1}^{k+1}[G_i,G_i]\cong\left[\prod_{i=1}^{k+1} G_i,\prod_{i=1}^{k+1}G_i\right].$$ Let's assume the relation for some $k\ge 2$ . Let $I=\big\{(1,[1,k]),(2,\{k+1\})\big\}$ . Then $$\prod_{i=1}^{k+1}[G_i,G_i]\cong\prod_{j=1}^{2}\prod_{i\in I_j}[G_i,G_i].$$ Note that $\prod_{i\in I_j}[G_i,G_i]=\left[\prod_{i\in I_j}G_i,\prod_{i\in I_j}G_i\right]$ . Hence $$\prod_{j=1}^{2}\prod_{i\in I_j}[G_i,G_i]=\prod_{j=1}^{2}\left[\prod_{i\in I_j}G_i,\prod_{i\in I_j}G_i\right]=\left[\prod_{j=1}^{2}\prod_{i\in I_j}G_i,\prod_{j=1}^{2}\prod_{i\in I_j}G_i\right]\cong\left[\prod_{i=1}^{k+1}G_i,\prod_{i=1}^{k+1}G_i\right].$$ Therefore, I only possess a mere isomorphism instead of required equality. Am I doing something wrong? Is this one of those situation where we simply treat the isomorphism as an equality?","['derived-subgroup', 'group-theory', 'abstract-algebra']"
3586957,On the ratio $\frac{F_n}{B_n}$,"One of the interesting limits that I came up with is: $$\lim_{n\to\infty} \frac{F_{n}}{B_{n}}\;\;\;\;\;\;\;\;\;\; \left( n \in \mathbb N^+\right)$$ Where $F_n$ is the nth Fibonacci number and $B_n$ is the nth Bell number . If $n$ is a natural odd number then it can be written as $n=2k-1$ , where $k\in \mathbb N^+$ , Using Stirling's approximation for the double factorial denoted $n!!=\left(2k-1\right)!!$ and the relation $B_{n}\ge n!!$ we have: $$0<\frac{F_{n}}{B_{n}} <\frac{F_{n}}{n!!}\sim  \frac{\left(\frac{1+\sqrt{5}}{2}\right)^{2k-1}-\left(\frac{1-\sqrt{5}}{2}\right)^{2k-1}}{\sqrt{5}}\cdot\frac{2^{k}\sqrt{2\pi k}\left(\frac{k}{e}\right)^{k}}{\sqrt{4\pi k}\left(\frac{2k}{e}\right)^{2k}}<\frac{2\cdot2^{k}}{k^{k}}$$ Taking the limit as $k \to \infty$ and using squeeze theorem follows: $$\lim_{n\to\infty} \frac{F_{n}}{B_{n}}=0$$ Which means as $n$ gets larger,the fraction with the numerator counting  the number of ways to tile a board of size $1Ã—n$ with squares and dominoes of size $1Ã—1$ and $1Ã—2$ respectively and the dinominator counting all possible partitions of a set with cardinality $n+1$ gets smaller. The same can be done for $n$ even.
For more information refer to this link . Note : I've already proved that for all $k \in \mathbb N$ the relation $B_k\ge F_k$ holds, using this we conclude that: $$0<\frac{F_{n}}{B_{n}}\le1$$ The question is that: does there exist a more elegant way to prove this convergence?","['limits', 'bell-numbers', 'fibonacci-numbers']"
3586959,Range of $f(x) = \frac{\sin^2 x + \sin x - 1}{\sin^2 x - \sin x + 2} $,"Here's what I did : $$\text{Let }\quad y = \frac{\sin^2 x + \sin x - 1}{\sin^2 x - \sin x + 2} $$ $$\text{Let } \sin^2x=t$$ $$\Rightarrow\  (y-1)t^2-(y+1)t+2y+1=0 $$ $$\text{Since } t= \sin x\text{ is real,} $$ $$\text{Discriminant} \geqslant 0 $$ $$ \Rightarrow (y+1)^2-4(y-1)(2y+1) \geqslant 0$$ $$ \Rightarrow\ -7y^2+6y+5 \geqslant 0$$ $$\Rightarrow\  7y^2-6y-5 \leqslant 0$$ $$\Rightarrow \ y\in\left[\frac{3-2\sqrt{11}}{7},\frac{3+2\sqrt{11}}{7}\right] $$ But the correct answer (according to wolfram) is $$y \in \left[\frac{3-2\sqrt{11}}{7},\frac{1}{2}\right] $$ Please guide me how I should proceed further .","['trigonometry', 'functions', 'quadratics']"
3586992,Invertibility of the characteristic map,"We are in the context of Hamilton Jacobi equations, in particular I was studying the characteristic method. We want to solve the problem of the special form (Hamiltonian only depending on the "" $p$ "" variables) $$
\begin{cases}
u_t(x,t)+H\big(D_xu(x,t)\big)= 0, \\
u(x,0)=g,
\end{cases}\quad(x,t) \in \mathbb{R}^n \times \mathbb{R}^+ .
$$ In this case it's easy to find that the characteristic line starting from $y$ at time $s$ is $$
X(y,s) = y+sDH\big(Dg(y)\big).
$$ Now set $$\overline{T} = \sup \Big\{ t : \, \det\big[I+tD^2H\big(Dg(y)\big)D^2g(y)\big] >0, \, \forall y \in \mathbb{R}^n \Big\}$$ The problem is that the textbook says that if $D^2H$ and $D^2g$ (Hessian matrices) are bounded, then for all $s < \overline{T}$ the function $y \mapsto X(y,s)$ is invertible (with $C^1$ inverse, but that's clear from the local inverse function theorem since the Jacobian is inverbile by hypothesis). What I can't do is proving surjectivity and injectivity of this map. Can someone help me? Thanks! P.S. The textbook i'm referring to is P.L. Lions, Generalized solutions of Hamilton Jacobi equations , page 14.","['partial-differential-equations', 'analysis', 'real-analysis']"
3586996,Construction of a non-autonomous Hamiltonian diffeomorphism,"Let $(M,\omega)$ be a symplectic manifold. I have read that the autonomous Hamiltonian diffeomorphisms (i.e. a Hamiltonian diffeomorphism generated by a time-independant Hamiltonian) form a proper subset of the Hamiltonian diffeomorphisms of $M$ , but I haven't been able to find a proof of that. Does anyone have a link in which a non-autonomous Hamiltonian diffeomorphism is constructed, and proven that it is indeed non-autonomous.","['symplectic-geometry', 'ordinary-differential-equations', 'differential-geometry']"
3587078,Is a continuous function that satisfies a certain condition uniformly continuous?,"Let $f:[0,+\infty)$ be a continuous function that satisfies: $f(x+q)$ ~ $f(x)$ for $x\to\infty$ (for any $q$ ) Does it follow that $f$ is uniformly continuous? I have managed to show that if there exists $\space$ $\displaystyle\lim_{x\to\infty}\space f(x)=G\in\Bbb{R}$ $\space$ then the function must be uniformly continuos by for given $\epsilon$ picking an $N$ big enough that $\forall_{x>N} |f(x)-G|<\frac{\epsilon}{2}$ and then showing that the function is uniformly continuous on $[0,N]$ and satisfies the definition of unifom continuity for that $\epsilon$ on $[N,+\infty)$ , thus proving it must be uniformly continuous,since we could have chosen any $\epsilon$ . However, that approach fails when we consider the cases where $\displaystyle\lim_{x\to\infty}\space f(x)$ is infinite or non-existent. I've also tried to find a counterexample by experimenting with functions like $\frac{1}{x}\sin(x^{3})$ (which appeared promising since its derivative is unbounded) but so far I haven't found one and my intuition does not steer me to either of the answers. I would appreciate any hints :)","['uniform-continuity', 'real-analysis']"
3587115,Proving $x^TAx \geq 0$ for a positive definite $n \times n$ matrix A.,"I am trying to prove that $x^TAx \geq 0$ for a positive definite $n \times n$ matrix $\mathbf{A}$ and $x \in \mathbb{R}^n$ . I have already proven that for any eigenvalue $y$ of $\mathbf{A}$ , $ y^TAy > 0$ . I have also shown that that $\mathbf{A}$ is diagonalizable (using the fact that positive - definite matrices are symmetric) so we can find a basis of eigenvalues of $\mathbf{A}$ for the vector space $\mathbb{R}^n$ , and therefore $x$ can be written as a linear combination of eigenvalues of $\mathbf{A}$ , i.e: $$x = a_1y_1 + â€¦ + a_ny_n$$ where $y_i, 1 \leq I \leq n$ are eigenvalues and the $a_i$ are scalars. I am just not sure how to complete the final step of showing $x^TAx \geq 0$ . Thank you in advance!","['linear-algebra', 'positive-definite']"
3587158,"A matrix operator on $L^1$-functions has no eigenfunctions, but it has eigendistributions.","Let $A=\begin{bmatrix}2&1\\ 1&1\end{bmatrix}$ and notice it acts on the torus $T=\mathbb{R}^2/\mathbb{Z}^2$ via left-multiplication. It also acts on $C^0(T)$ via $(Af)(x):= f(Ax)$ . We can extend this action to distributions $\mathcal{D}'(T) = (C^\infty(T))^*$ via $(Au)(g) = u(A^{-1}g)$ for any $g\in C^\infty(T)$ ; this agrees with the definition for continuous functions because $$ \int_T f(x)g(A^{-1}x) dx = \int_T f(Ax)g(x) dx $$ by using a linear change of variables and using that $\det(A)=1$ . How can we prove that if $f\in L^1(T)$ satisfies $Af=f$ then $f$ must be almost-everywhere constant, but that there exists an infinite-dimensional space of distributions $u$ satisfying $Au=u$ ? For the latter, I have no idea how to begin. But for the former, I have the following approach: if $f$ were non-constant then for some disjoint open balls $B_1,B_2\subset\mathbb{C}$ we would have that $S_1:=\{ x : f(x)\in B_1\}\subset T$ and $S_2:=\{ x : f(x)\in B_2\}\subset T$ both have positive measure. Then since $f(Ax)=f(x)$ for all $x$ , we obtain that $(A^n(S_1))\cap S_2=\varnothing$ for al $n\ge 0$ . So we want to prove that the action of $A$ on $T$ is ""ergodic"" enough to mix everything. Here is where I get stuck. Thank you for your help!","['operator-theory', 'distribution-theory', 'real-analysis', 'functional-analysis', 'eigenfunctions']"
3587239,$\arctan{x}+\arctan{y}$ from integration,"I was trying to derive the property $$\arctan{x}+\arctan{y}=\arctan{\frac{x+y}{1-xy}}$$ for $x,y>0$ and $xy<1$ from the integral representation $$
\arctan{x}=\int_0^x\frac{dt}{1+t^2}\,.
$$ I am aware of ""more trigonometric"" proofs, for instance using that $\tan{(\alpha+\beta)}=\frac{\tan\alpha+\tan\beta}{1-\tan\alpha\tan\beta}$ , but I was willing to see if there is a proof that uses more directly the properties of the integral representation. For instance, if $x>0$ , one immediately gets $$\begin{aligned}
\arctan{x}+\arctan\frac{1}{x}
&=\int_0^x\frac{dt}{1+t^2} + \int_0^{\frac{1}{x}}\frac{dt}{1+t^2}\\
&=
\int_0^x\frac{dt}{1+t^2}+\int_x^\infty\frac{dt}{1+t^2}\\
&=\int_0^\infty \frac{dt}{1+t^2} = \frac{\pi}{2}
\end{aligned}$$ sending $t\to\frac{1}{t}$ in the second integral.
Similarly I tried considering $$
\int_0^x\frac{dt}{1+t^2} + \int_0^y\frac{dt}{1+t^2}=(x+y)\int_0^1\frac{1+xyt^2}{1+(x^2+y^2)t^2+x^2y^2t^4}\ dt
$$ after rescaling $t\to xt$ and $t\to yt$ . On the other hand, via a similar rescaling $t\to \frac{x+y}{1-xy}t$ , we have $$
\int_0^\frac{x+y}{1-xy}\frac{dt}{1+t^2}
=
(x+y)\int_0^1\frac{1-xy}{(1-xy)^2+(x+y)^2t^2}\ dt\,.
$$ By a clever choice of variable it should (must?) be possible to see that these integrals are actually the same, but I can't figure it out...","['integration', 'trigonometry', 'change-of-variable']"
3587268,Notation for assigning observations to a random variable,"I want to assign some observations to a variable $X$ , but I am unsure what the correct notation (if any) would be. At the moment I am doing it like this: For the following set of observations for X, $$1,8,1,5,8,6,3,3,3,7$$ ... But I was wondering if there was any actual notation where I could write something like this: For the following set of observations $$X = \{1,8,1,5,8,6,3,3,3,7\}$$ ... I dont think the latter is correct, since the random variable $X$ is not actually a set, but I wanted to know if there was any sort of similar notation to indicate the values observed for $X$ .","['notation', 'statistics', 'random-variables']"
3587269,How can I mathematically compare two measurements to improve accuracy?,"I have two different techniques for measuring geolocation accuracy. I am asked to combine two different measurements in any way that might improve accuracy. Say I have an error of n1 using measurements A and an error of n2 using measurements B. Can I do better than both n1 and n2 by using both sets of information? 
Any suggestions or techniques that could be implemented in Python or MATLAB are greatly appreciated.","['statistics', 'mathematical-modeling', 'data-analysis', 'probability', 'random-variables']"
3587275,Does $\gcd(I)=1$ imply the monoid generated by $I$ is $\mathbb{N}$ minus finitely many numbers?,"This is true if $I=\{a_1,\dots,a_n\}$ is a finite set of positive integers. Namely, if $\gcd(a_1,\dots,a_n)=1$ , then for all sufficiently large $N$ there is a non-negative integer solution $(k_1,\dots,k_n)$ to $$k_1a_1+\cdots+k_na_n = N.$$ In other words, the monoid generated by $I$ consists of every natural number except for possibly finitely many exceptions. I want to consider an infinite set $I=\{a_1,a_2,\dots\}$ an infinite set of positive integers with $\gcd(a_1,a_2,\dots)=1$ . Then is it true that for all sufficiently large $N$ there is a non-negative integer solution $(k_1,k_2,\dots)$ to $$k_1a_1+k_2a_2+\cdots = N$$ where $k_i=0$ for all but finitely many $i$ ? My attempt: It's enough to find a finite subset of $I$ with gcd 1, and then we can apply the result of the finite case. To do this, set $b_1=a_1$ . Then $b_1$ has finitely many prime factors, and we can let $p$ be the smallest. Since $\gcd(a_1,a_2,\cdots)=1$ , there exists $a_i$ such that $p \nmid a_i$ . Set $b_2=a_i$ . Now $\gcd(b_1,b_2)$ has strictly fewer prime factors than $b_1$ (since $p$ is not one of them), and we can let $p'$ be the smallest. Again, there must be $a_j$ such that $p' \nmid a_j$ , so set $b_3=a_j$ . Then $\gcd(b_1,b_2,b_3)$ has strictly fewer prime factors than $\gcd(b_1,b_2)$ . Continue in this fashion, and since the number of prime factors of $\gcd(b_1,\dots,b_t)$ is strictly decreasing with $t$ , there must be $T$ such that $\gcd(b_1,\dots,b_T)=1$ . Is this correct? Is there a simpler way to arrive at this result?","['monoid', 'number-theory', 'combinatorics']"
3587283,Alternating power sequence,"I quite randomly stumbled upon the following phenomenon: Let $ f:\mathbb R^+\to\mathbb R^+, x\mapsto x^{-2^{3^{-4^{\cdot^{\cdot^{\cdot}}}}}} $ , then in the interval of $[1,20)$ the plot of $f$ looks like the following: This looks surprisingly similar to the plot of $\frac{1}{x}$ , plotted in red: Why is that? And why is $f$ even well defined (i.e. why does the sequence converge)? It seems as if there exists some $\xi\in[1,2]$ for which $$\forall x\in\mathbb R^+:f(x)=x^{-\xi}\qquad\xi\simeq 1.2$$ $f$ is undefined for negative values and diverges at $0$ : $$\lim\limits_{x\searrow0}f(x)\to\infty $$ Can anyone explain those properties / link to some proof? I'd be quite curious about the exact value of $\xi$ too. Also, if you're interested or don't trust me, I've created the plots with this program. So to clarify this a bit, the problem can be formulated the following way: Show that $$\xi:=-\lim\limits_{n\to\infty}-2^{3^{-4^{\cdot^{\cdot^{\cdot^{\sigma(n)\cdot n}}}}}}\simeq1.1982330602188767$$ $$\sigma:\mathbb N\to\{-1,1\},\ n\mapsto\begin{cases}-1& n\mod2=0\\1&n\mod2=1\end{cases}$$ We can formulate this mathematically precisely: Let $\sigma$ be as defined previously, then define $$e:\mathbb N^2\to\mathbb R,\ (m,n)\mapsto\begin{cases}\sigma(m)\cdot m^{e(m+1,n)}&m<n\\\sigma(m)\cdot m&m=n\end{cases}$$ then $\xi$ is defined as $$\xi:=\lim\limits_{n\to\infty}e(2,n)$$ For this we can proof that $\xi\in(-2,-1)$ by something like the following: $$\forall n\in\mathbb N:\ e(4,n)=-4^{r(n)}\quad r(n)>0\implies e(4,n)<-1$$ $$\implies e(3,n)=3^{e(4,n)}=\left|3^{e(4,n)}\right|<1$$ $$\implies e(2,n)=-2^{e(3,n)}\in(-2,-1)$$ However, this is not a full proof of convergence (but merely of limitedness). I feel like the proof of convergence should contain the value it converges to. But if the value of $\xi$ only appears by this construction, that's rather difficult. It would also suffice to proof the monotony of $e(2,\cdot)$ - however, it clearly isn't monotonous. Maybe, similarly to the proof I presented, the interval for $\xi$ can be minimized bit by bit, but I don't quite know how I'd proceed there. The above proof can easily be generified to the following ( $\forall n\in\mathbb N$ ): $$\forall m\in 2\mathbb N:\ e(m,n)<-1$$ $$\implies\forall o\in\mathbb N\setminus2\mathbb N:\ e(o,n)=|e(o,n)|<1$$ $$\implies\forall p\in2\mathbb N:\ e(p,n)\in(-m,-1)$$ However, a quick computer program indicates that $$\forall m\in2\mathbb N,\ n\gg m:\ e(m,n)\in(-2,-1)$$ I don't know if that's of any use though.","['power-towers', 'sequences-and-series']"
3587312,The Biggest Step Size with Guaranteed Convergence for Constant Step Size Gradient Descent of a Convex Function with Lipschitz Continuous Gradient,"Given a convex function $ f \left( x \right) : \mathbb{R}^{n} \to \mathbb{R} $ with $ L $ - Lipschitz Continuous Gradient. Namely: $$ {\left\| \nabla f \left( x \right) - \nabla f \left( y \right) \right\|}_{2} \leq L {\left\| x - y \right\|}_{2} $$ What is the largest constant step size, $ \alpha $ , one could use in Gradient Descent to minimize the function? In most literature I see $ \alpha = \frac{1}{L} $ yet in some other cases I see $ \alpha = \frac{2}{L} $ . Which one is right? Also, for the case $ f \left( x \right) = \frac{1}{2} {\left\| A x - b \right\|}_{2}^{2} $ what is $ L $ ? Is it the largest Singular Value of $ A $ ?","['convex-optimization', 'convex-analysis', 'gradient-descent', 'linear-algebra']"
3587339,"Given a function, find the sum of the parameters of the function such that the function is continuous.","Consider following function: $$f(x)=\begin{cases} 
      px & x \in [0,1) \\
      m & x = 1 \\
      x^3 + q & x \in (1, 2] 
   \end{cases}
$$ and also the following set: $$A = \{ (p, m, q) \in \mathbb{R^3 | \hspace{0.1cm} \text{f differentiable on} \hspace{0.1cm}(0,2)} \}$$ and the sum $$S = \sum_{(p,m,q) \in A} (p+m+q)$$ I have to find the value of $S$ . This is what I tried. I know that if the function is differentiable, then it is continuous. The above function is continous only if the following relation is true: $$p=m=1+q$$ But if I rewrite $2$ of the parameters in terms of the other one I get the following: $$\sum_{(p,m,q)\in A}(p+m+q) = \sum_{(p, m , q) \in A} (p + p + p -1) = \sum_{(p,m,q) \in A}(3p - 1)$$ And I got stuck. I don't even know if what I did so far is correct.","['calculus', 'functions', 'derivatives']"
3587352,How many times must I apply Lâ€™Hopital?,"I have this limit: $$\lim _{x\to 0}\left(\frac{e^{x^2}+2\cos \left(x\right)-3}{x\sin \left(x^3\right)}\right)=\left(\frac 00\right)=\lim _{x\to 0}\frac{\frac{d}{dx}\left(e^{x^2}+2\cos \left(x\right)-3\right)}{\frac{d}{dx}\left(x\sin \left(x^3\right)\right)}$$ $$\lim_{x\to0}\frac{2e^{x^2}x-2\sin \left(x\right)}{\frac{d}{dx}\left(x\right)\sin \left(x^3\right)+\frac{d}{dx}\left(\sin \left(x^3\right)\right)x}=\lim_{x\to0}\frac{2e^{x^2}x-2\sin \left(x\right)}{\sin \left(x^3\right)+3x^3\cos \left(x^3\right)+\sin \left(x^3\right)}$$ but yet we have $(0/0)$ . If I apply Lâ€™Hopital again, I obtain $$=\lim_{x\to0}\frac{2\left(2e^{x^2}x^2+e^{x^2}\right)-2\cos \left(x\right)}{15x^2\cos \left(x^3\right)-9x^5\sin \left(x^3\right)}$$ again giving $(0/0)$ . But if I apply Lâ€™Hopital a thousand times I'll go on tilt. What is the best solution in these cases? With the main limits or applying upper bonds?","['limits', 'limits-without-lhopital']"
3587363,"For any $k \gt 3$, if $n!+k$ is a perfect power then does there exist any $n\gt k$?","A while ago, I asked a similar question For any $k \gt 1$ , if $n!+k$ is a square then will $n \le k$ always be true? where users mathworker21 and WE Tutorial School proved that for non-square $k$ , $n\le k$ is always true when $n!+k$ is sqaure. Recently I got the idea to check if the property is true for all perfect powers and using PARI, I observed that if, $$n!+k=a^b$$ where $n, k, a, b\in \Bbb{N}$ , $k\gt 3$ and $b\ge 2$ , then $n\le k$ is always true. In search for a counter-example, I covered a range of $k\le 2500$ and $n\le 10^4$ for each $k$ and found none. So my question is, If true, can we prove/partially prove that $n\le k$ always holds? If false, then what is the smallest counter-example where $n\gt k$ ?","['number-theory', 'conjectures', 'elementary-number-theory', 'perfect-powers']"
3587374,problem with the ode $f' = a \delta (x) f$,"The first solution is $$f(0^+) = \exp\left(a \int_{0^-}^{0^+} dx \delta (x)\right) f(0^-) = \exp(a) f(0^-) .$$ The second solution is $$ f(0^+) - f(0^-) = a  \int_{0^-}^{0^+} dx \delta (x) f(x) = \frac{a}{2}(f(0^+ ) +f(0^-)) ,$$ which leads to $$f(0^+) = \frac{1 + a/2}{1 - a/2} f(0^-) .  $$ The two approaches yield different results. Which one is right? I am biased on the second one. But can anyone justify it and point out the flaw in the first one?","['ordinary-differential-equations', 'distribution-theory']"
3587394,"Injectivity of a function $h:[0,+\infty) \rightarrow [0,+\infty)$","Consider $$
h(x) = \dfrac{2x^2}{5ð‘¥+1}.
$$ Is $h$ an injective function? I already tried taking an arbitrary element from the interval $[0,+\infty)$","['functions', 'discrete-mathematics']"
3587399,Utilitarian introduction to commutative algebra,"My goal is to have some good handling over the jujutsu of basic commutative algebra (eg: the depth and difficulty of Atiyah-Mcdonald) so that I can properly read books on complex curves/complex geometry/algebraic geometry. Atiyah-Mcdonald is usually referred to as the canonical choice in this case, but my main problem (difficulty/densenss are secondary problems) with Atiyah-Mcdonald is that it's too much dry, and most (if not all) topics seem to be rather unmotivated Is there any good complex geometry/complex curves/algebraic geometry books out there which develops commutative algebra on an ""utilitarian"" or need based basis ? I'm told that Fulton/Vakil doese that but I'm not sure how good are those books. Thanks !","['complex-geometry', 'algebraic-geometry', 'book-recommendation', 'commutative-algebra']"
3587404,Applying the Mean Value Theorem to a Natural Log Function,"After my prof's lecture I came away unsure when I can and cannot apply the Mean Value Theorem. More specifically when the domain of $f'$ extends beyond the domain of $f$ (like in the case of a function using a natural $\ln$ ). In the case of $f(x)= \ln(x+6)$ , on interval $[3,8]$ the domain would be $x>-6$ or $(-6,âˆž)$ . -This would satisfy the 1st hypothesis since closed interval $[3,8]$ falls within the domain of $f$ , $(-6,âˆž)$ and therefore $f$ would be continuous on $[3,8]$ . -But when I take the derivative, $f'(x)= 1/x+6$ , the domain of $f'$ is $(-âˆž,-6)U(-6,âˆž)$ My question then is: since the domain of $f'$ extends beyond $f$ , does this not satisfy the 2nd hypothesis for MVT on the given interval $[3,8]$ ?","['calculus', 'derivatives']"
3587431,Do all cuspidal automorphic representations of $\operatorname{GL}_2(\mathbb A_{\mathbb Q})$ come from Maass or holomorphic cusp forms?,"A normalized cuspidal newform $f$ (either holomorphic or Maass) can be identified with a function on $\phi: \operatorname{GL}_2(\mathbb Q) \backslash \operatorname{GL}_2(\mathbb A_{\mathbb Q})$ , and it generates an irreducible cuspidal automorphic representation in some $L^2$ -space.  The central character of the corresponding representation depends on the some choice made in the transfer from $f$ to $\phi$ . Do these representations exhaust the cuspidal spectrum of $\operatorname{GL}_2(\mathbb A_{\mathbb Q})$ ?","['number-theory', 'automorphic-forms', 'modular-forms', 'langlands-program']"
3587457,Proving $a^2 + b^2 + c^2 \geqslant ab + bc + ca$,"Let $a, b, c \in \mathbb{R}$ . Show that $a^2 + b^2 + c^2 \geqslant ab + bc + ca$ . My reasoning went as follows and I would like to know if it's correct. $a^2 + b^2 + c^2 \geqslant ab + bc + ca$ $\Leftrightarrow a^2 + b^2 + c^2 -ab - bc - ca \geqslant 0$ $\Leftrightarrow 2a^2 + 2b^2 + 2c^2 -2ab - 2bc - 2ca \geqslant 0$ $\Leftrightarrow a^2 + a^2 + b^2 + b^2 + c^2 + c^2 -2ab - 2bc - 2ca \geqslant 0$ $\Leftrightarrow (a^2 -2ab + b^2) + (b^2 - 2bc + c^2)+ (c^2 - 2ca + a^2) \geqslant 0$ $\Leftrightarrow (a -b)^2 + (b-c)^2 + (c-a)^2 \geqslant 0$ . And the last inequality holds since squares are non-negative. I've seen couple of different ways of doing this and they seem to divide by $2$ and I didn't really understand it so instead can I just use the fact that $2a^2 = a^2+a^2$ ?","['summation', 'inequality', 'symmetric-polynomials', 'sum-of-squares-method', 'algebra-precalculus']"
3587476,Fitting A Semi-Circle Between Three Lines,"I have the general situation illustrated below. I know angles $\Theta_{1}, \Theta_{2}$ and line lengths $A$ , $B$ and $C$ , (where $A = a_{1} + a_{2}$ etc.). I'm trying to calculate the largest semi-circle I can fit on line $B$ (i.e., calculate r), that will (always?) be tangent to line $A$ and $C$ when the angles are acute. So I think: $$
r = \frac{B.sin\Theta_{1}  .sin\Theta_{2} } { sin\Theta_{1} + sin\Theta_{2} }
$$ For the cases where $A > a_{1}$ and $C > c_{1}$ (meaning A is longer than the theoretical tangent point at the angle $\Theta_{1}$ etc.). But what if $A$ was very short? Does $r$ simply become the distance to the end point of $A$ ? And how do I 'know' when this is the case mathematically? EDIT: To add some clarification... This is part of a larger effort to find the largest semi-circle that will fit inside an arbitrary closed polygon. The lines illustrated are thus three consecutive segments of that polygon They may be any angle and length, and don't form a triangle unless my polygon is a triangle. Clearly, if the angle at either or both ends is larger than $\pi$ , the semi-circle can go right to that vertex and isn't impinged upon by the next or previous segment. The part I'm mostly struggling with is incorporating the line length in determining $r$ and the centre point. Obviously, it has an effect at any given angle: My ultimate idea is to draw lines between every polygon vertex and calculate the largest semi-circle that will fit inside the polygon along that line. I hope that will give me a good guess at the largest generally that will fit. EDIT 2: Specifics... I know the blue values, how do I calculate $r$ (and thus $b_{2}$ and $b_{3}$ )? ( $h$ and $b_{1}$ being trivial). It seems like it should be easy...but I just can't seem to get my head around it.","['trigonometry', 'geometry']"
3587489,Geometric representation of all 2x2 matrices,"I am trying to understand the content on pages 4-6 of this paper describing a geometric representation of all 2x2 matrices. So far, I have confirmed that all orthogonal matrices lie on $S^3(\sqrt{2})$ since they can be represented as $\begin{bmatrix}\cos\theta&\sin\theta\\\mp \sin\theta&\pm \cos\theta\end{bmatrix}$ for some $\theta$ .  I have also confirmed that, when representing $A=\begin{bmatrix}a&b\\c&d\end{bmatrix}$ in 4-space by changing coordinates from $x$ , $w$ , $y$ , and $z$ with \begin{align}
a&=(x-y)/\sqrt{2}\\
b&=(z-w)/\sqrt{2}\\
c&=(z+w)/\sqrt{2}\\
d&=(x+y)/\sqrt{2}
\end{align} the set of rotations $SO(2)$ is the great circle on $S^3(\sqrt{2})$ in the $xw$ -plane (with $a^2+b^2+c^2+d^2=2\implies x^2+w^2+y^2+z^2=2$ and $\det A = 1 \implies x^2+w^2-y^2-z^2=2$ , implying $x^2+w^2=2$ , $y=0$ and $z=0$ ) and the set of reflections $O^-(2)$ is the great circle in the $yz$ -plane (with $\det A = -1$ so $y^2+z^2=2$ , $x=0$ and $w=0$ ).  Finally, I have confirmed that all singular matrices on $S^3(\sqrt{2})$ lie on the Clifford torus defined by $x^2+w^2=1$ and $y^2+z^2=1$ (since we have $\det A = ad-bc = x^2+w^2-y^2-z^2=0$ and know that $x^2+w^2+y^2+z^2=2$ ).  So this drawing from the paper makes sense to me: However, I am trying to confirm the following: The complement of this Clifford torus consists of two open solid tori which
are tubular neighborhoods of the great circles $SO(2)$ and $O^-(2)$ .  The cross-sectional disks in these tubular neighborhoods are round disks of radius $(\pi/4)\sqrt{2}$ lying on great 2-spheres which meet the great circles $SO(2)$ and $O^-(2)$ orthogonally. I can't seem to wrap my mind around this.  Any explanations / tips / hints would be greatly appreciated.","['matrices', 'general-topology', 'linear-algebra', 'geometry']"
3587492,Tensor Formula in Hartshorne II.8.20,"Someone in this community should have ask of something similar before, but I found that my question was not answered. The following is an excerpt of Hartshorne II.8.20: I would like to ask why the isomorphism $$\mathscr{I}/\mathscr{I}^2\cong \mathscr{I}_Y\otimes\mathscr{O}_Y$$ is satisfied, where $\mathscr{I}$ is the ideal sheaf used to define the sheaf of module of relative differential through $\Delta^*(\mathscr{I}/\mathscr{I}^2)$ with $\Delta$ being the diagonal morphism and $\mathscr{I}_Y$ is the sheaf of ideal corresponding to the closed subscheme $Y$ . Thanks in advance for answering.","['algebraic-geometry', 'tensor-products', 'schemes']"
3587602,An Indentity of Poisson process,"Question: Suppose buses arrive at a bus stop according to a Poisson process $N_t$ with parameter
. Given a fixed $t > 0$ . The time of the last bus before t is $S_{N_t}$ , and the time of the
next bus after $t$ is $S_{N_{t+1}}$ . Show the following identity: $E(S_{N_{t+1}}-S_{N_t})=(2-e^{-\lambda t})/{\lambda}$ . My attempt: One basic conclusion about Poisson proccess is that:
 the time interval between two arrivals follows $Expo(\lambda)$ . But here,as t is a fixed time point,it can be any point in the interval $S_{N_t}$ and $S_{N_{t+1}}$ .Thus I found it impossible to use the above assumption.So what should I do next?","['statistics', 'renewal-processes', 'stochastic-processes', 'poisson-process', 'probability']"
3587631,Find the limit of $\frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x}$ as $x \to 0$,$$\lim_{x \to 0} \frac{e^{\tan x} - e^x + \ln(\sec x + \tan x) -x }{\tan x - x}$$ I tried to solve this using L'Hopital rule but the resulting differential got too messy $$=\lim_{x \to 0} \frac{e^{\tan x}\sec^2x - e^x + \sec x - 1 }{\tan^2x}$$ $$=\lim_{x \to 0} \frac{e^{\tan x}(\sec^4x+2\sec^2x\tan x) - e^x + \sec x\tan x }{2\tan x \sec^2x}$$ What should I do from here? Differentiate again or use a different strategy?,['limits']
3587642,"Clarification on Furstenberg's topological ""Infinitude of Primes"" proof","I am quite new to topology and I am particularly interested in gaining an intuitive understanding for the following proof: I am wondering if someone would be able to slow down the sequence of thoughts here so that I can put more of the puzzle together. For example: In which sense is the described topology ""metrizable"" How can arithmetic progressions in $(-\infty,\infty)$ be both open and closed (and I don't fully get why this is implied via the complement of the union). By consequence why does this imply finite progressions are closed? How does all this help to build the picture of the final conclusion. I am ""ok"" with the basics of topology / measure theory / diff. geo - just in case you need to gauge how much you need to tailor the answer.","['general-topology', 'prime-numbers', 'sequences-and-series']"
3587710,Evaluating $\int _0^1\frac{\ln ^2\left(x^2+1\right)}{x+1}\:dx$ using real methods,"I've stumbled upon that interesting integral here , the OP managed to transform the integral into something more approachable using contour integration and proved that $$\int _0^1\frac{\ln ^2\left(x^2+1\right)}{x+1}\:dx\:=\:-\pi G+\frac{5}{2}\zeta \left(3\right)+\frac{2}{3}\ln ^3\left(2\right)-\frac{\pi ^2}{24}\ln \left(2\right)$$ I can't come up with a way to attack this one with only real methods, i'd appreciate any help possible.","['integration', 'definite-integrals']"
3587806,Find Îº and Î» $\in R$ for which $\frac {x^2+Îºx+Î»}{x^2+1} \leq 2$ for all $x \in R$.,"By doing the math, we get $x^2-Îºx+2-Î» \geq 0$ . Also $D = Îº^2 - 4(2-Î»)$ ...but I don't know how to continue!","['multivariable-calculus', 'calculus', 'quadratics', 'discriminant']"
3587842,Riemann surfaces exercise,"I am trying to do an exercise from Rick Miranda's Book Let $U$ be the affine plane curve defined by $x^2=3+10t^4+3t^8$ and $V$ defined by $w^2=z^6-1$ , show that the function $F :U \rightarrow V$ , $$F(x,t)=\left(\frac{1+t^2}{1-t^2},\frac{2tx}{(1-t^2)^3}\right)$$ is holomorphic and nowhere ramified when $t \neq  \pm 1$ . Well to prove that this is holomorphic i simply separated in $4$ cases for the coordinate charts that we could have and checked that the local representation of these functions were holomorphic. To prove that part about the unramified points and i was trying to check the derivative of these representations and see where they would be zero, hopefully nowhere in the surface would be the result we wanted, however using this and the fact that we know the derivative of the implicit function i was not able to prove this, so my question is if this is the right way to think abou this problem, or should i be taking another approach, should i try to explicit right with are the implicit functis envolved in this problem? Thanks in advance. New edit:
I was able to prove this was true for two of  the representations , now im trying to prove it for the other two, where in one we suppose that $t$ is given by $h(x)$ and $z$ is given by $k(w)$ , and the other one where $x$ is given by $j(t)$ and $z$ is given by $k(w)$ . 
Calculating the maps and their derivatives i cant seem to get a contradiction for why the derivative cannot be zero. At this point what i cant seem to do is this So suppose now the charts is $t=h(x)$ , so $\frac{df}{dt} \neq 0$ and $z$ depends on $w$ by some holomorphic function k,so $\frac{dk}{dw}\neq 0$ , we have  in local coordinates $\psi_2 \circ f \circ \phi_1(x) = \frac{2h(x)x}{(1-h(x)^2)^3}$ with derivative $\frac{2(5xh(x)^2h'(x)+xh'(x)-h(x)^3+h(x))}{(1-h(x)^2)^4}$ and i cant seem to find a contradition for why this derivative cannot be zero using the fact that those derivatives cannot be zero, so any tips on something i may be forgetting would be helpfull.","['complex-analysis', 'riemann-surfaces']"
3587863,The maximum for $xy \sin \alpha + yz \sin \beta +zx \sin \gamma$.,"Question: Deduce the maximum of $xy \sin \alpha + yz \sin \beta +zx \sin \gamma$ if $x,y,z$ are real numbers that satisfy $x^2+3y^2+4z^2=6$ with $0<\alpha,\beta,\gamma<\pi$ such that $\alpha+\beta+\gamma=2\pi$ . Currently, I am not very sure how to approach the problem. I had an idea to consider the area of a triangle made of 3 smaller triangles with areas $\frac{1}{2}xy \sin \alpha$ , $\frac{1}{2}yz \sin \beta$ and $\frac{1}{2}zx \sin \gamma$ respectively. However, that kinda got me no where as I did not have any good ideas on how to use the condition $x^2+3y^2+4z^2=6$ . Moreover, that would have assumed $x,y,z\geq0$ which might not be the case. So, is there a way to deduce the maximum without a calculus approach?","['maxima-minima', 'trigonometry', 'geometric-inequalities', 'geometry']"
3587942,Finding counterfeit coins,"Suppose I have $N$ rare coins, of which $M \le N$ are counterfeits. I am blind. I ask an oracle who charges me a penny to tell me in yes/no answers whether there is a counterfeit in any group I show her. Is there a strategy to identify all $M$ coins with minimal cost, preferably something better than $O(N)$ ? This sounds like a variant of the counterfeit coin problem but I cannot find a good solution. EDIT: In the case of $M=1$ , one obvious solution is on order of $\log_2(N)$ where you number each coin in order, in base $2$ . Then test each group by digits wherever the coin has a 1 in that digit. Then the counterfeit should be identified by the oracle readout per digit, in base $2$ .","['recreational-mathematics', 'combinatorics', 'discrete-mathematics', 'algorithms']"
3587960,"If a CW Complex captures the topology of manifold, what captures its geometry?","Context: I work on a scientific computing code (mostly fluid mechanics) and I am trying to learn differential geometry, understand it and then mimic its basic structures into a numerical code. My intuition is that the ""mesh"" used in numerical simulations is nothing but the discrete version of the manifold. I understood that a manifold is a topological space and an atlas (at least for continuum mechanics applications). I think about a CW Complex as the topological discretization of the mesh. It is a finite set of cell spaces (i.e., points, lines, surfaces and volumes) related to each other by means of its boundary maps (i.e., the incidence matrices). This can be coded easily. Question: If the CW Complex captures the topological nature of the manifold, which entity captures its ""position""? (may we call this the ""geometric nature"" of the manifold?) I guess I should ""discretize the atlas"" but, is there any structure that already does that?
Is there something I am missing? Thank you! :)","['cw-complexes', 'discrete-mathematics', 'discrete-calculus', 'manifolds', 'differential-geometry']"
3588021,Why is the tangent bundle defined using a disjoint union?,"In textbooks about differential geometry, one finds often the disjoint union in the definition of the tangent bundle (e.g. in ""Lee: Introduction to smooth manifolds"", or ""Amann, Escher: Analysis III""): $$T\mathcal{M}:=\coprod_{p\in\mathcal{M}}T_{p}\mathcal{M}:=\bigcup_{p\in\mathcal{M}}\{p\}\times T_{p}\mathcal{M}$$ I have the following questions: (1) Why is this necessary? Is it necessary for the definition of the smooth structure on $T\mathcal{M}$ ? (2) When we define the tangent space as the set of all derivations (like in Lee's book), aren't $T_{p}\mathcal{M}$ and $T_{q}\mathcal{M}$ for $p\neq q$ allready disjoint? Why is it then necessary to use the disjoint union? Is it just a notation in this cases? How about other definitions of tangent spaces, like the geometric defined tangent space, via equivalence classes of curves? (3) How about other bundles? Also in the definition of tensor fields and differential forms, these authors often use the disjoint union... Thank you all!","['tangent-spaces', 'vector-bundles', 'differential-topology', 'tangent-bundle', 'differential-geometry']"
3588053,Is this a valid proof that the harmonic series diverges?,"Is this a valid proof that the harmonic series diverges? Assume the series converges to a value, S: $$S=1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+...$$ Split the series into two, with alternating even and odd denominators. Since the original series converges, the component series will converge. $$S_{EVEN}=\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\frac{1}{8}+...$$ $$S_{ODD}=1+\frac{1}{3}+\frac{1}{5}+\frac{1}{7}+...$$ $$S=S_{EVEN}+S_{ODD}$$ Show that $S_{EVEN}=\frac{1}{2}S$ $$\frac{1}{2}S=\frac{1}{2}(1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+...)=\frac{1}{2}+\frac{1}{4}+\frac{1}{6}+\frac{1}{8}+...=S_{EVEN}$$ Show $S_{ODD}>S_{EVEN}$ because each odd term is greater than its corresponding even term: $$1>\frac{1}{2}\qquad \frac{1}{3}>\frac{1}{4}\qquad \frac{1}{5}>\frac{1}{6}\qquad ...$$ Show $S_{ODD}=S_{EVEN}$ $$S_{ODD}=S-S_{EVEN}=S-\frac{1}{2}S=\frac{1}{2}S=S_{EVEN}$$ The contradiction implies that the original assumption of convergence is false: $$S_{ODD}>S_{EVEN}$$ $$S_{ODD}=S_{EVEN}$$ $$\therefore S\ne 1+\frac{1}{2}+\frac{1}{3}+\frac{1}{4}+\frac{1}{5}+...$$","['harmonic-numbers', 'solution-verification', 'sequences-and-series']"
3588054,"What is the space, on which a random variable is defined?","Well, I have some simple, maybe silly question about random variables, but there is something that I can not undestand when we define them. Suppese that, we have some random variable $X$ , that is defined in a standard probability space $(\Omega,\mathcal{F}_s,\mathbb{P})$ , where $\mathbb{P}:\mathcal{F}_s\rightarrow [0,1]$ . I struggle to undestand which is the space , that this random variable is defined. Specifically, the random variable $X$ is a function $X:\Omega\rightarrow R$ , where R is some arbitrary space and probabyly the real line.
Can we claim that $X\in L^2(\Omega,\mathcal{F}_s,\mathbb{P})$ or this is wrong? How can we know indeed where this random variable belongs to? Maybe my whole skeptic is wrong, so forgive me in advance, but I am a begginer, who wants to understand, this mathematical theory!","['measure-theory', 'probability-theory', 'random-variables']"
3588073,Difficult limit of expression involving logarithms,"I'm trying to calculate the limit $$ L=\lim_{n \to \infty} \left (\ln^2(n)\ln \left (\frac{\ln(\frac{n}{t})}{\ln(\frac{n}{t+1})}\right )-\ln(n)\ln \left (1+\frac{1}{t} \right) \right),$$ where $t$ is a fixed positive integer, but have so far been unsuccessful. I have reason to suspect that $$L=\frac{1}{2}\ln \left (1+\frac{1}{t}\right )\ln \Big(t(t+1) \Big)=\frac{1}{2} \Big(\ln^2(t+1)-\ln^2(t) \Big) \hspace{15mm} (\ast)$$ because (1). Wolfram alpha sais so (without providing any step by step solution ) and (2). It seems to be correct numerically. Let $$F(n):=\ln^2(n)\ln \left (\frac{\ln(\frac{n}{t})}{\ln(\frac{n}{t+1})}\right )-\ln(n)\ln \left (1+\frac{1}{t} \right).$$ For simplicity, we may take $n=e^m$ , so that $$F^*(m)=m^2\ln \left (\frac{m-\ln(t)}{m-\ln(t+1)}\right )-m\ln \left (1+\frac{1}{t} \right)$$ and \begin{equation}\exp(F^*)=\frac{\left(\frac{m-\ln(t)}{m-\ln(t+1)} \right )^{m^2}}{(1+\frac{1}{t})^{m}}= \left(\frac{t}{t+1} \right )^m\left(\frac{m-\ln(t)}{m-\ln(t+1)} \right )^{m^2}\end{equation} I do not know how to calculate the limit of this as $m \to \infty$ . One could substitute $a=\frac{t}{t+1}$ and then set $b=\ln(a)$ . The result is (any of) these expression: $$a^{m} \left (1+\frac{\ln(a)}{x-\ln(t/a)} \right )^{m^2} =e^{bm} \left(1+\frac{b}{x+b-\ln(t)}\right )^{m^2}=e^{bm} \left(1+\frac{b}{x-\ln(t+1)}\right )^{m^2}.$$ If we pick, say the rightmost expression above and take the logarithm, the problem is reduced to calculating the limit $$\lim \limits_{m \to \infty} \left(bm+m^2\ln \left (1+\frac{b}{m-\ln \left( t+1 \right ) } \right ) \right )$$ $$=\lim \limits_{m \to \infty} m\left(b+\ln \left (\left[1+\frac{b}{m-\ln \left( t+1 \right ) } \right )\right]^{m} \right)$$ $$=\lim \limits_{m \to \infty} m \left( b+ \ln \left ( \left[ 1+\frac{b}{m-\ln(t+1) } \right]^{m-\ln(t+1)} \left[ 1+\frac{b}{m-\ln(t+1) } \right]^{\ln(t+1)} \right) \right)$$ which does at least have the appearance of a simpler problem. Then, setting $m=\ln(t+1)+x$ we must calculate the limit of $$\Big(\ln(t+1)+x\Big) \left(b+\ln \left(\left[1+\frac{b}{x} \right]^{x} \right)+\ln(t+1)\ln \left(1+\frac{b}{x} \right ) \right)$$ as $x$ goes to infinity. (The plan in this calculation is to use $e^b=\lim_{x \to \infty}(1+b/x)^x$ ) as some point. I have also tried to calculate the limit $(\ast)$ using L'Hopitals rule, but to no avail. Does anyone see the golden step i am missing ? : )","['limits', 'substitution', 'logarithms']"
3588089,How to numerically perform analytic continuation?,"I understand that one can in theory analytically continue a function by repeatedly computing new Taylor series. Suppose for example we have an analytic function $f$ defined on some open set $U$ and compute $$T_0(z)=\sum_{n=0}^\infty\frac{f^{(n)}(z_0)}{n!}(z-z_0)^n$$ for some $z_0\in U$ close to the boundary of $U$ . If this converges on $V$ where $U\cap V$ is non-empty, we can then compute another Taylor series to extend further: $$T_1(z)=\sum_{n=0}^\infty\frac{T_0^{(n)}(z_1)}{n!}(z-z_1)^n$$ for some $z_1\in V\setminus U$ etc. However, it is impossible to compute infinitely many terms and higher derivatives quickly become prone to large amounts of cancellation error. Furthermore, one must repeated drop the degree of the next series expansion, as demonstrated here , in order for the result to be useful. Otherwise, with the same degree at the new point $z_1$ , you will end up recovering the original $T_0$ and fail to approximate $f$ further away. So how can one actually numerically compute the analytic continuation of a function? In my specific case, I have a set of data points over a subinterval of $\mathbb R$ and I know some basic behavior about the function $f$ 's derivatives (all derivatives are positive over the subinterval and to the right, which is the area I want to continue to) and that it has no singularities to the right of the given subinterval.","['analytic-continuation', 'complex-analysis', 'sequences-and-series', 'numerical-methods', 'analytic-functions']"
3588175,Understanding of definition in set theory,"I'm a computer scientist with just a basic understanding of set theory. In the book ""Theory of Modeling an Simulation"", a multi component system (MC) is defined as a structure of the following: $$M C =(T, X, \Omega, Y, D, \{M_d\})$$ With $T$ as a time base, $X$ as a set of input values, $\Omega$ as a set of input segments and $Y$ as a set of output values. $M_d$ is defined as: \begin{equation}
    M_d = (Q_d, E_d, I_d, \Delta_d, \Lambda_d)
\end{equation} where $Q_d$ is the set of states of component $d$ . $I_d \subseteq D$ are all components influencing $d$ . $E_d \subseteq D$ are all components which are influenced by $d$ . $\Delta_d : \times_{i \in I_d}Q_i \times \Omega \rightarrow \times_{j\in E_d}Q_j$ is the state transition function of $d$ . $\Lambda_d : \times_{i_ \in I_d}Q_i \times \Omega \rightarrow Y$ is the output function of $d$ . Currently, I'm totally able to understand everything except the definition of $\Delta_d$ and $\Lambda_d$ . 
My issue is at the following: can you tell me what $\times_{i_ \in I_d}Q_i$ in the definitions from $\Delta_d$ and $\Lambda_d$ means? I just can not understand why there is a $\times$ sign in front is..
Even more, through my limited understanding, I do not know where to search for this. Thank you for your help!",['elementary-set-theory']
3588177,Questions about a proof of Stokes' theorem in my calculus 2 lecture notes,"My lecture notes look to prove Stokes' theorem for the special case where a surface can be represented as the graph of some function, so $z=f(x,y)$ . The surface $S$ is parametrized as $r(x,y)=(x,y,f(x,y))$ , where $(x,y)$ is in the region $U$ in the $xy$ plane. 
Now assume $U$ has a boundary curve $C_u$ and $S$ has a boundary curve $C_s$ . My first question is where it is said that the line integral of the vector field $v=(v_1,v_2,v_3)$ over the curve $C_s$ is equal to the line integral of the same vector field over the curve $C_u$ . Why is this the case? Lecture notes for my first question My second question is I believe about a total derivative but I'm not sure. My lecturer has written that since $z=f(x,y)$ , $dz=(f_x)dx+(f_y)dy$ where $f_x$ and $f_y$ denote the partial derivatives of $f$ with respect to $x$ and $y$ respectively and $dx, dy, dz$ denote normal differentials, not partial ones. Can someone also explain this equality to me, please? Lecture notes for my second question","['multivariable-calculus', 'stokes-theorem']"
3588257,Derive the surface of a sphere using integration,"Let's consider this picture: By the Pythagorean theorem, we know that $x^2+z^2=r^2$ so solving for x we have $x=\sqrt{r^{2}-z^{2}}$ Then the circumference of the shadowed circle is $2\pi\sqrt{r^{2}-z^{2}}$ And the surface of the whole sphere is: $\int_{-r}^{r}2\pi\sqrt{r^{2}-z^{2}}\,dz$ However this integral is equal to $\pi^{2}r^{2}$ not $4\pi r^{2}$ . But why ? Why am i wrong ?","['integration', 'definite-integrals', 'surface-integrals', 'geometry', 'calculus']"
3588293,Covariance of a rectified (relu) Gaussian,"Given a normal random vector $$X\sim N(\mu,\Sigma)$$ for spd $\Sigma$ , I'm interested in the covariance matrix $K=\mathrm{cov}(Y)$ of the variable $$Y = \mathrm{relu}(X)$$ where the relu is performed elementwise $Y_i = \mathrm{Max}(0,x_i)$ , so $Y$ is distributed according to the rectified Gaussian distribution . Given I know everything about $\Sigma$ , how can I compute $K$ ? The mean and variance of each $Y_i$ has been covered in other questions on this site, but the off-diagonal elements of $K$ seem pretty challenging to compute, and I haven't found anything on SO or elsewhere online about it. I'm actually after the eigenvectors of $K$ , so if anyone can relate the eigenvectors between $\Sigma$ and $K$ without directly computing $K$ , that would be even more interesting. Thanks! Edit: Just to note there is a similar question asked here and thoroughly answered, but only in the scalar (or diagonal multivariate) case. For multi-dimensional $X$ with correlations, this seems much more challenging.","['integration', 'statistics', 'neural-networks', 'gaussian-integral']"
3588345,Prove that $AD=BC$ if and only if $\measuredangle ADT\equiv \measuredangle TDC$.,"Let $ABC$ be a triangle, $AD$ one of it's heights and $G$ it's centroid. $DS$ is the bisector of $\measuredangle BDA$ with $S\in AB$ , and $SG\cap AC=\{T\}$ . Prove that $AD=BC$ if and only if $\measuredangle ADT\equiv \measuredangle TDC$ . Fist I considerd $AD=BC$ and realised that I had to prove that $DT$ is the bisector of $\measuredangle ADC$ , so proving that $\frac{AD}{DC}=\frac{AT}{TC}$ would be enough. $\frac{AD}{DC}=\frac{AT}{TC}\iff\frac{BC}{DC}=\frac{AT}{TC}$ . From $DS$ is the bisecor of $\measuredangle BDA$ $\implies$ $\frac{AD}{BD}=\frac{AS}{BS}\iff\frac{BC}{BD}=\frac{AS}{BS}$ . I got a little stuck here. What shoud I do next or my abordation is wrong? ( $M$ and $P$ are only to find the position of $G$ ) or maybe a vectorial approach is possible, but I don't see a solution that way.","['contest-math', 'euclidean-geometry', 'analytic-geometry', 'geometry', 'problem-solving']"
3588360,Centralizer of symmetric group,"Let, an element of symmetric group $S_N$ is given by $g=(1)^{N_1}(2)^{N_2}....(s)^{N_s}.$ Here $N_n$ denotes the number of cycles of length $n$ . Its known that the centralizer of this element is given by \begin{equation}
C_g = S_{N_1} \times (S_{N_2} \rtimes \mathbb{Z}_2^{N_2}
) \times \dots\times (S_{N_s} \rtimes \mathbb{Z}_s^{N_s}
).\tag{1}
\end{equation} I have been able to convince myself this formula gives the correct result when $g$ is the identity $(g=(1)^{N_N})$ and when $g$ is given by $g=(N)^1$ . However, lets take a simple case: let's try to find the centralizer of $(1,2)(3,4)$ in $S_4$ . The answer is $C=\{Id, (1, 2)(3, 4), (1, 2), (3, 4), (1, 3)(2, 4), (1, 4)(2, 3), (1, 4, 2, 3), (1, 3, 2, 4)\}$ . I don't how I can construct this set using definition (1). Can anyone walk me through the process please? I tried to construct $\mathbb{Z}_2^2\rtimes S_2 $ . This should be isomorphic to $D(4).$ Then I wrote down the elements of $D(4)$ in cycle notation but that didn't give me the correct answer.","['symmetric-groups', 'wreath-product', 'group-theory', 'semidirect-product']"
3588361,Find maximum value of $\int_{1}^3\frac{f(x)}{x}dx$ if $\int_{1}^{3}f(x)dx=0$ and also $-1\leq f(x)\leq 1$,"Let $f$ be function such that $$\int_{1}^{3}f(x)dx=0$$ and $$-1\leq f(x)\leq 1$$ Then find the maximum value of $$\int_{1}^{3}\frac{f(x)}{x}dx$$ My Attempt: I wonder if this approach is correct or not. Let $$f(x)=\begin{cases} -c, 1\le x\le a \\ 1, a\le x<3 \end{cases}$$ As per the constraint $$c=\frac{3-a}{a-1}$$ Let $$G(a)=\int_{1}^{3}\frac{f(x)}{x}dx=\int_{1}^{a}\frac{-c}{x}dx+\int_{a}^{3}\frac{1}{x}dx=\frac{2}{1-a}\ln a+\ln 3$$ $$G'(a)=\frac{1-a+a\ln a}{a(1-a)^2}$$ After putting $G'(a)=0$ I am not able to solve for $a$","['integration', 'differential', 'real-analysis', 'calculus', 'derivatives']"
3588370,How to prove that 2 sets are equinumerous?,"We suppose that we have 2 sets $\!A$ and $\!B$ , such as that $\!A \smallsetminus \!B$ is infinite and $\!B$ is either finite or countably infinite. 
How do we prove that $\!A \smallsetminus \!B \sim \!A$ ? Intuitionally it does make sense? but it I cant grasp the idea on how to prove it.","['elementary-set-theory', 'discrete-mathematics']"
3588388,How is the Ornstein-Uhlenbeck process stationary in any sense?,"Confused about the definition of a weakly stationary stochastic process. Wikipedia and this Stack question have it that a process is weakly stationary if it has A constant mean $m_X(t)$ Covariance $\operatorname{Cov}(X_t, X_s)$ depending only on $t-s$ Finite (and constant) auto-covariances $\operatorname{Var}(X_t)$ Then on the Wikipedia page for the Ornstein-Uhlenbeck process, it describes the process as ""stationary"" (doesn't say weak or strict, but evidently weak) but then gives the mean and the covariance, and they are not as above. For instance, $$\operatorname{Cov}(X_t, X_s) \propto \left(e^{- \theta |t-s|} - e^{\theta(t + s)} \right)$$ This evidently does not depend only on $t-s$ . What gives?","['stochastic-processes', 'probability-theory']"
3588391,Number of $2$-colorings of edges of the $n$-dimensional cube?,"I'm interested in counting the number of $2$ -colorings of the edges of an $n$ -cube up to rotations and reflections. For $n=1$ there are two coloringsâ€”either color the edge or don't. For $n=2$ there are six colorings: don't color anything, color one edge, color two opposite edges, color two adjacent edges, color all but one edge, and color everything. For $n=3$ there are 144 colorings.
For general $n$ an elementary bound from Burnside's Lemma shows that the number of colorings $C(n)$ satisfies $$
  C(n) \geq \left\lceil\frac{2^{n\cdot2^{n-1}}}{2^nn!}\right\rceil
$$ where $n2^{n-1}$ is the number of edges and $2^nn!$ is the number of symmetries, and which grows astoundingly fast . When I use Burnside's Lemma on the square or the cube, the number of colorings stabilized by some action is ""intuitive"", but on the hypercube and $n$ -cube, this is obviously less intuitive. However, I'm wondering if it's possible to compute $C(4)$ exactly, and find a formula $C(n)$ for all values of $n$ more generally.","['coloring', 'geometry', 'polya-counting-theory', 'combinatorial-geometry', 'combinatorics']"
3588425,Norm of an integral operator in $L^2$,"Let T an operator in $L^2((-1,1))$ such that $Tf(x) = \int_{-1}^{1} (xy + y^2)f(y)dy$ . Calculate the norm of $T$ . I tried calculating $\left\lVert Tf(x) \right\rVert$ first, but I got stucked with the square of the inside integral. Any help?","['integration', 'operator-theory', 'normed-spaces', 'lp-spaces', 'functional-analysis']"
3588454,Random variable corresponding to product of density functions,"Suppose that $X_1, X_2$ are (probably independent) random variables and we have $X_1\sim f(x_1)$ and $X_2\sim g(x_2)$ . Construct a new distribution for the new random variable $X$ as follows: $$X\sim p(x)=\frac{1}{\kappa}f(x)g(x)$$ where $\kappa$ is a normalization factor such that the function is a density function. 
  Is it possible to express $X$ as a function of $X_1$ and $X_2$ in a way that their PDF is $p(x)$ ? More formally, find a function $G$ such that $$X=G(X_1, X_2)\sim p(x)$$ Note: "" $\sim$ "" denotes the probability density function of a random variable. Note 2: If we had $p(x)=f(x)*g(x)$ , where $*$ is the sign for convolution, then I think we may write $G(X_1, X_2) = X_1+X_2$ .","['probability-distributions', 'probability-theory', 'probability', 'random-variables']"
3588455,"Prove that if we decrease by 7 the sum of squares of any three natural numbers, then the result cannot be divisible by 8.","I did like this.... $a^2+b^2+c^2\equiv7(mod 8)$ or, $a^2+b^2+c^2+1\equiv0(mod 8)$ This further implies that 8 divides the sum of the remainders of $a^2$ , $b^2$ , $c^2$ and 1 on dividing by 8.
Now, square of any natural number gives remainders 0,1 or 4 on dividing by 8.
By trial and error, we see that the sum of the remainders is never divisible by 8, for any combination of remainders.
Hence, proved.
But, can please somebody provide me a more elegant proof,by excluding my trial and error part?
Thanks, in advance.","['number-theory', 'elementary-number-theory']"
3588563,A generalization of L'Hospital's Rule,"Let $\mathcal{O}\subset\mathbb{R}^n$ be open, let $f$ , $g:\mathcal{O}\to\mathbb{R}$ be twice continuously differentiable functions, and let $x_0\in\mathcal{O}$ . Suppose that $f(x_0)=g(x_0)=0$ and $\nabla f(x_0) = \nabla g(x_0) = 0$ . Suppose also that $\nabla^2 f(x_0) = \lambda\nabla^2 g(x_0)$ for some number $\lambda$ , and that $\nabla^2 g(x_0)$ is a positive definite Hessian matrix. Prove that $$\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \lambda$$ Proof I'm tempted to use the second-order approximation of $f(x)$ and $g(x)$ , since that is the only place I can think of that the the hessians might appear. However, the approximation is given by $$f(x + h) \approx f(x) + \left<\nabla f(x), h\right> + \frac{1}{2}\left< \nabla^2 f(x)h,h\right>$$ I suppose I could say that $x_0 = x + h$ , so an approximation for $f(x_0)$ could work, since $f(x_0) = 0$ and $\nabla f(x_0) = 0$ . But I doubt that is valid for any $h\neq 0$ . I'll see what happens, anyway. The limit would then become, if I choose $x$ close enough to $x_0$ and $h$ small but nonzero: $$\lim\limits_{x\to x_0} \frac{f(x)}{g(x)} = \frac{\lim\limits_{x\to x_0} f(x)}{\lim\limits_{x\to x_0} g(x)} = \frac{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 f(x_0 + h)h, h\right>}{\lim\limits_{h\to 0} \frac{1}{2} \left< \nabla^2 g(x_0 + h)h, h\right>} = \frac{\nabla^2 f(x_0)}{\nabla^2 g(x_0)} = \frac{\lambda \nabla^2 g(x_0)}{\nabla^2 g(x_0)} = \lambda$$ This feels both extremely sketchy and sort of on the right track.","['multivariable-calculus', 'proof-writing', 'solution-verification', 'real-analysis']"
3588589,Koszul connections and connections on vector bundles,"I'm studying differential geometry, and when I began to study connections, the first definition that I found was the following: Definition : Let M a differentiable manifold. A connection on M is a transformation $\nabla: D(M)\times D(M)\rightarrow D(M)$ , where $D(M)$ is the set of differentiable vector fields on $M$ , which satisfices: a) $\nabla_{fX_1+X_2}Y=f\nabla_{X_1}Y+\nabla_{X_2}Y$ with $f\in C^{\infty}(M)$ and $X_1,X_2, Y\in D(M)$ b) $\nabla_X(\lambda Y_1+Y_2)=\lambda\nabla_X Y_1+\nabla_X Y_2$ , with $\lambda\in\mathbb{R}$ and $X,Y_1,Y_2\in D(M)$ c) $\nabla_X(fY)=f\nabla_X Y+X(f)Y$ , where $f\in C^{\infty}(M)$ and $X,Y\in D(M)$ I had no problem with this definition, but later the book says that we can reinterpret the previous definition, and we can say that a connection is actually a transformation $\nabla:\Gamma(TM)\rightarrow\Gamma(T^*M\otimes TM)$ . Explicitly, if $Y\in\Gamma(TM)$ , then $\nabla Y$ will be the element of $\Gamma(T^*M\otimes TM)$ which satisfices: $$\nabla Y(X,\theta)=\theta(\nabla_X Y)$$ . So, with this, we can generalize the definition of a connection, but this time in a vector bundle, as follows: Definition : Let $\xi=(E,\pi)$ a differentiable vector bundle over a differentiable manifold $M$ . A connection on $\xi$ is a transformation: $$\nabla:\Gamma(E)\rightarrow\Gamma(T^*M\otimes E)$$ with the following properties: a) $\nabla(s)(fX+X',\theta)=f\nabla(s)(X,\theta)+\nabla(s)(X',\theta)$ . b) $\nabla(\lambda s+s')=\lambda\nabla s+\nabla s'$ c) $\nabla(fs)=f\nabla s+df\otimes s$ for all $s,sÂ´\in\Gamma(E)$ , $X,X'\in\Gamma(TM)$ , $\theta\in\Gamma(E^*)$ , $f\in C^{\infty}(M)$ and $\lambda\in\mathbb{R}$ . My problem is that I cannot find the way of joining both definitions. If I take, as particular case, $E=TM$ in the second definition, I don't see why the connection defined in this way is the same (or is connected) with the first one . If the second definition is more general, then it should to reduce to the first one when I take $E=TM$ . My little book doesn't explain more, and begins to construct the connection one-forms $\omega_{ij}$ . If it is important, my book is ""GeometrÃ­a Riemanniana"" of HÃ©ctor SÃ¡nchez Morgado and Oscar Palmas Velasco.","['connections', 'vector-bundles', 'riemannian-geometry', 'differential-geometry']"
3588598,Classify the groups of order $88$ up to isomorphism.,"Classify the groups of order $88$ up to isomorphism. Here is what I have so far (I'm aware that there are $12$ groups, but I don't know which ones I'm missing as well as why the $3$ groups are abelian and the other $9$ are non-abelian.) We are asked to classify the groups of order $88$ up to isomorphism.  To prove that any group of order $88$ is abelian.  So if $|G|=88$ , then we can construct all abelian groups of order $88$ by using the Fundamental Theorem of Finitely Generated Abelian groups.  We can use the Fundamental Theorem of Finitely Generated Abelian Groups wich  states the following: Let $G$ is a finitely generated abelian group. Then \begin{equation}G\cong\Bbb Z^r\times\Bbb Z_{n_1}\times \Bbb Z_{n_2}\times\dots\times\Bbb Z_{n_s},\tag1\end{equation} For some integers $r,n_1,n_2,\dots,n_s$ satisfying the following conditions: $r\ge 0$ and $n_j\ge 2$ for all $j$ , and $n_{i+1}\mid n_i$ for $1\le i\le s-1$ The expression in $(1)$ is unique: if $G\cong\Bbb Z^t\times\Bbb Z_{m_1}\times \Bbb Z_{m_2}\times\dots\times\Bbb Z_{m_u}$ , where $t$ and $m_1,m_2,\dots,m_u$ satisfy 1. and 2. (i.e., $t\ge 0$ , $m_j\ge 2$ for all $j$ and $m_{i+1}\mid m_i$ for $1\le i\le u-1$ ), then $t=r$ , $u=s$ and $m_i=n_i$ for all $i$ . This gives us an effective way of listing all finite abelian groups of a given order.  Namely, to find (up to isomorphism) all abelian groups of a given order $n$ one must find all finite sequences of integers $n_1, n_2,\dots,n_s$ such that $n_j\ge 2$ for all $j\in\{1,2,\dots,s\}$ , $n_{i+1}\mid n_i$ , $1\le i\le s-1$ , and $n_1 n_2\dots n_s=n$ We can also note that every prime divisior of $n$ must divide the first invariant factor $n_1$ .  In particular, if $n$ is the product of distinct primes, which are all to the first power, which is called squarefree, we see that $n|n_1$ , hence $n=n_1$ .  This proves that if $n$ is squarefree, there is only one possible list of invariant factors for an abelian group of order $n$ .  The factorization of $n$ into prime powers is the first step in determining all possible lists of invariant factors for abelian groups of order $n$ . This means that we can break 8 $8$ down into its prime factors which would give us the following: $$88=2\cdot 44=2\cdot 2\cdot 22=2\cdot 2\cdot 2\cdot 11$$ So if we say that $n=88=2^3\cdot 11$ , as we have stated above we must have that $2Ã—11|n_1$ , so possible values of $n_1$ are as follows: $$n_1=2^3\cdot 11~\lor~n_1=2^2\cdot 11~\lor~n_1=2\cdot 11$$ For each of these we need to work out the possible $n_2$ â€™s.  For each resulting pair $n_1,n_2$ we need to then work out the possible $n_3$ â€™s and then continue in this manner until all lists satisfying 1. and 3. are obtained.
Therefore $88$ can be written as $2^3\cdot 11$ .  Which would give us the following: Order $p^\beta$ : Partitions of $\beta$ Abelian Groups $$2^3:~3,~\Bbb Z_8;~~~2,1,~\Bbb Z_4\times\Bbb Z_2;~~~1,1,1,~\Bbb Z_2\times\Bbb Z_2\times\Bbb Z_2$$ $$11^1:~1,~\Bbb Z_{11}$$ We can obtain the abelian groups of order $88$ by taking one abelian group from each of the two lists above and taking their direct product.  Doing this in all possible ways gives all isomorphism types: $$\Bbb Z_{88},~\Bbb Z_8\times\Bbb Z_{11},~\Bbb Z_4\times\Bbb Z_2\times\Bbb Z_{11},~\Bbb Z_2\times\Bbb Z_2\times\Bbb Z_2\times\Bbb Z_{11},~\Bbb Z_2\times\Bbb Z_2\times\Bbb Z_{22},~\Bbb Z_4\times\Bbb Z_{22},~\Bbb Z_2\times\Bbb Z_{44}$$ When we have completed this we will have $12$ groups.  By the Fundamental Theorems above, this is a complete list of all abelian groups of order $88$ , every abelian group of this order is isomorphic to precisely one of the groups above and no two of the groups in this list are isomorphic. We can then define abelian and non-abelian groups.  Abelian groups or commutative groups are groups in which the results of applying the group operation to two group elements does not depend on the order in which they are written, in other words these groups are are groups that following the axiom of commutativity. Abelian groups generalize the arithmetic of addition of integers.  Non-abelian groups, also known as non-commutative groups are groups $(G,*)$ in which there exists at least one pair of elements $a$ and $b$ of $G$ , such that $a*b\ne b*a$ . Of these $12$ groups $3$ of them are abelian and the other $9$ are non-abelian groups. The three abelian groups are $\Bbb Z_{88}$ , $\Bbb Z_4\times\Bbb Z_{22}$ , and $\Bbb Z_2\times\Bbb Z_2\times\Bbb Z_{22}$ .","['group-theory', 'finite-groups', 'groups-enumeration']"
3588660,Weil divisors on normal Noetherian schemes,"I'm looking at Section 7 of Liu's Algebraic Geometry and Arithmetic Curves. In Section 7, Definition 2.4. defines a Weil divisor as a cycle of codimension 1 on a Noetherian integral scheme. But then Liu discusses Weil divisors on normal Noetherian schemes in Definition 2.7., 2.10., Proposition 2.11, etc., as well as the notion of function field $K(X)$ of a normal Noetherian scheme $X$ . I feel lost as the function field is defined over an integral scheme, and up to my understanding, the closest one to the function field of a normal Noetherian scheme is perhaps the direct sum of the function fields of the finite integral components. What I'm more interested is whether Weil divisors are well-defined on normal Noetherian schemes and how close it is to be equivalent to invertible sheaves. Could someone provide an explanation on this? Thank you.",['algebraic-geometry']
3588692,help understanding a weird phenomenon,"I was just thinking that you could closely calculate the square root of $10$ by averaging $10/3$ and $3$ , and I put into desmos that equation with $2$ graphs: $y=\sqrt x$ , $y=x/2a + a/2$ .
I saw that for all positive values of a there's some tangent point where the linear one is tangent to the curve.
would love to know the explanation to that phenomenon, the relation between a and x or anything since I don't know how to figure it out. 
maybe its just the beginning of the Taylor series for $\sqrt{x}$ ?","['calculus', 'functions', 'taylor-expansion']"
3588703,Does it exist a way how to find the ways to get from one point to another when certain points must be avoided in a grid?,"The problem is as follows: The diagram from below shows a grid of $6\times 6$ . How many different ways can you get from $A$ to $B$ without going through any of the highlighted points? The alternatives given are as follows: $\begin{array}{ll}
1.&\textrm{265 ways}\\
2.&\textrm{365 ways}\\
3.&\textrm{395 ways}\\
4.&\textrm{405 ways}\\
\end{array}$ Does it exist a way to simplify this problem?. How exactly can I find the method to solve this?. There isn't any indication of which sort of path should be taken. Hence there could be tons of ways and I'm stuck on it. Can someone help me here?. It would help a lot to me to include some sort of diagram or drawing to justify a resonable method to solve this.","['recreational-mathematics', 'puzzle', 'combinatorics']"
3588788,Generaliztion of an isomorphism theorem from group theory to general categories,"There is a well known theorem from group theory, often called one of the ""isomorphism theorems"". If $S$ is a subgroup of $G$ and $N$ a normal subgroup of $G$ then $SN/N = S/(S \cap N)$ . If we let $\textbf C$ be a suitable category then for subobjects $A$ and $B$ of an object $C$ we can make sense of $A \cap B$ , it is the fibered product of $A$ and $B$ over $C$ . We can also make sense of $AB$ , it is the smallest subobject of $C$ which contains both $A$ and $B$ . I.e the join of $A$ and $B$ in the poset of subobjects of $C$ . If $\textbf C$ has terminal objects and $m: Y \rightarrow X$ is monic we can define $X/Y$ as the pushout of $* \leftarrow Y \xrightarrow m X$ . Then my question is this. In a suitable category $\textbf C$ is it true that $AB/B = A/(A \cap B)$ for subobjects $A$ and $B$ of an object $C$ ?","['group-theory', 'category-theory']"
3588791,If A and B are $n \times n$ matrices where each column sums to p. Then for what values of p will the matrix AB also have all columns that sum to p?,"I have no idea how to approach this question. I've tried working through it with sum notation but it became jumbled. I assume there's another property of matrices that I can use to make this simpler? Since using the basic properties of matrix multiplication seems convoluted. Using generic 2x2 matrices, I was able to find that for p=0 and p=1 AB has columns that add to p. But I'm unsure on how to do this working for a generic nxn matrix.","['matrices', 'linear-algebra']"
3588829,Fourier Transform of compactly supported Distribution is actually a Function,"If $u$ is a compactly-supported distribution on $\mathbb{R}^n$ , how can we prove that its Fourier transform $\mathcal{F}u$ is the tempered distribution given by the function $\xi\mapsto u(e^{-ix\xi})$ ? Here, the Fourier transform is defined on Schwartz functions as $\mathcal{F}\phi(\xi)=\int e^{-ix\xi}\phi(x) dx $ , and on distributions as $\mathcal{F}u(\phi)=u(\mathcal{F}\phi)$ . If $u$ is compactly-supported, then $u=\chi u$ for some compactly-supported smooth $\chi$ , and so $u(e^{-ix\xi}) = u(\chi(x)e^{-ix\xi})$ is well-defined for any $\xi$ . Intuitively, one has $$ \int_{\xi\in\mathbb{R}^n} u(\chi(x)e^{-ix\xi}) \phi(\xi) d\xi = u\left(\int_{\xi\in\mathbb{R}^n}\chi(x)e^{-ix\xi}\phi(\xi) d\xi \right) $$ which is what we want to show (the LHS is $u$ applied to that function on $x$ ), but I'm not sure how rigorous it is to pull the integral sign inside the distribution.","['fourier-analysis', 'fourier-transform', 'distribution-theory', 'real-analysis', 'schwartz-space']"
3588866,Intuition behind spectral radius,"Let $V$ be a normed vector space, and let $T : V \to V$ be a bounded linear operator. Then the spectral radius of $T$ , call it $r(T)$ is defined to be $\lim_{n \geq 1} \|T^n\|^\frac{1}{n}$ , where $\|\cdot\|$ is the canonical operator norm. I would like to know what does this definition tell us intuitively. For finite-dimensional linear operators, we can treat them as matrices and it is simply the largest absolute value of the eigenvalues (as a result of Gelfand's formula). I see it intuitively as the largest extent in which $T$ ""expands"" the vectors in $V$ . However, in the infinite-dimensional case, there may not be any eigenvalues, so I'm not sure how to tweak my intuition for this case. If possible, I would also like to have an explanation of the intuition behind Gelfand's formula. That is, why is the max of $|\lambda_i|$ , the set of eigenvalues, precisely $\lim_{n \geq 1}\|T^n\|^\frac{1}{n}$ ? Any help is appreciated.","['spectral-radius', 'spectral-theory', 'functional-analysis', 'intuition']"
3588971,Does $f'(x)>\frac{f(x)}{x}$ for $x>0$ imply $f$ is convex?,"Let $f\in C^\infty([0,\infty))$ , $f(0)=0$ , $f(x)>0$ for $x>0$ , $f'(x)>\frac{f(x)}{x}$ for $x>0$ . Is $f$ a convex function? If we set $h(x)=xf'(x)-f(x)$ , then $h(0)=0$ , $h(x)>0$ for positive $x$ and $h'(x)=xf''(x)$ . Suppose we can find smooth function $h$ satisfying the conditions above and that $h'$ is not always positive, then $f(x)=x(\int \frac{h(x)}{x^2}dx+C)$ is a counterexample. There are many choices of $h$ , for example, $h(x)=1-\frac{cos x}{1+x}$ . The question is that this integral $\int_a^x \frac{h(t)}{t^2}dt$ may only be meanful for $a>0$ . How can we construct $h$ such that when $a=0$ the improper integral exists? Or is there another way to construct counterexamples? Remark: This is kind of similar to this question .","['improper-integrals', 'convex-analysis', 'functions', 'derivatives']"
3589002,A matrix whose off-diagonal entries are $>0$ has its exponential with all positive entries,"I am taking a course in differential equations and while I was doing some exercises I came across with the following statement: Let $A=(a_{ij})$ be a matrix such that $(a_{ij})>0$ , $i\neq j$ , then all entries of $e^A$ are positive. I've been trying to find some pattern in the exponents of the matrix $A$ but without success.
I also have been thinking of $$.^T:\mathcal{M}_{n\times n}\to \mathbb{R}_{n\times n}$$ as an operator and try to prove that $e^A=e^{\frac{A}{2}}e^{\frac{A}{2}}$ and then proving that $x^TAx>0$ but I'm not sure if this is what I really want. Does $x^TAx>0$ implies that all entries of the matrix $A$ are positive? Or is there a more ""correct"" way of proving the statement? Thank you for your attention!","['ordinary-differential-equations', 'matrix-exponential', 'positive-matrices', 'matrices', 'linear-algebra']"
3589016,"If $f''(x)+f(x)>0$ and $f(x)>0$ $\forall x\in(a,b)$; $f(a)=f(b)=0$; prove that $b-a>\pi$.","Please help me to solve this question: Suppose $f:[a,b] \to \Bbb R$ satisfies: $f''(x)+f(x)>0$ and $f(x)>0$ for all $x\in(a ,b)$ ; $f(a)=f(b)=0$ . Prove that $b-a>\pi$ . Thanks in advance.","['contest-math', 'calculus', 'ordinary-differential-equations', 'analysis']"
3589062,"$2^{\sin(x) + \cos(y)} = 1$ , $16^{\sin^2(x) + \cos^2(y)} = 4$ (system of equations)","My progress so far: $2^{\sin(x) + \cos(y)} = 1$ $16^{\sin^2(x) + \cos^2(y)} = 4$ |||||||||||||||||||| $\sin(x) + \cos(y) = 0$ $\sin^2(x) + \cos^2(y) = \frac12$ I think I'm on the right track, but I'm not sure how to continue. Thanks in advance to anyone who helps.","['trigonometry', 'systems-of-equations']"
3589070,"Properties of bounded, continuous process $(X_t)$ if $(\mathrm{e}^{t/2} X_t)$ is a martingale.","Consider two complex stochastic processes $(X_t)_{t\ge 0}$ and $(Y_t)_{t\ge 0}$ (adapted to a filtration $(\mathcal{F}_t)_{t\ge 0}$ ) with the following properties: $$
\begin{align}
(1)& \ (X_t) \text{ is continuous.}\\
(2)& \ M_t := \mathrm{e}^{t/2} X_t \text{ is a martingale (w.r.t. $(\mathcal{F}_t)$).}\\
(3)& \ |X_t| \le 1 \; \forall t\ge 0.\\
(4)& \ X_0 = 1.
\end{align}
$$ The process $(Y_t)$ has the same properties except that $Y_0 = 0$ . What I wish to find is something about the long-term behaviour of $|X_t|$ and $|Y_t|$ . My conjecture is that $X_t = \mathrm{e}^{\mathrm{i} B_t}$ for a Brownian motion $(B_t)$ and $Y_t = 0 \, \forall t\ge 0$ , but I have not been able to deduce any of the two (Actually, knowing that $|X_t| = 1 \forall t \ge 0$ would be enough for my purposes). One of the corollaries I was able to make is that, since $|M_t|$ and thereby $|M_t|^2$ are sub-martingales, $ \mathbb{E}  |X_t|^2 \ge \mathrm{e}^{-t},$ but that is not very interesting for large $t$ . I would very much appreciate any hint at interesting properties that follow from the above or any further conditions that would, in conjunction with the properties above, be sufficient to imply my conjecture.","['stochastic-processes', 'brownian-motion', 'probability-theory', 'martingales']"
3589074,Rayleigh as exponential family - compute $\mathbb{E}(Y)$ and Var$(Y)$ where Y is a sum of independent squared Rayleigh's distribution,"Prove that X of Rayleigh distribution with pdf $f(x, \sigma) = \frac{x}{\sigma^2}e^{-\frac{x^2}{2\sigma^2}}\mathbb{1}_{(0, \infty)}(x)$ comes from the exponential family and then compute $\mathbb{E}(Y)$ and Var $(Y)$ where $Y = \sum_{k=1}^nX^2_k$ (all $X_k$ are independent). I did the first part: $$
f(x, \sigma) = \frac{x}{\sigma^2}e^{-\frac{x^2}{2\sigma^2}}\mathbb{1}_{(0, \infty)}(x) =
xe^{-\frac{1}{2\sigma^2}x^2 - 2\ln(\sigma)}\mathbb{1}_{(0, \infty)}(x)
$$ so $$
h(x) = x; \eta_1(\theta) = -\frac{1}{2\sigma^2}; T_1(x)=x^2; B(\theta)=2ln(\sigma)
$$ Why is the fact that Rayleigh's comes from an exponential family useful? Are there any equations to quickly solve the second part of the task? I appreciate any help.","['expected-value', 'statistics', 'variance', 'probability']"
3589121,Pattern in Number of Conjugacy Classes of p-groups,"I was playing around with the number of conjugacy classes of $p$ -groups in GAP and made the following conjecture: If there is a group of order $p^{2n}$ with $k$ conjugacy classes then there is a group of order $p^{2n+1}$ with $k+p-1$ conjugacy classes. The only exception is $p=2$ , $n=2$ , $k=7$ . For example, there are groups of order $64$ with $\{13,16,19,22,25,28,34,40,64\}$ many conjugacy classes so there are groups of order $128$ with $\{14,17,20,23,26,29,35,41,65\}$ many conjugacy classes. Can someone prove this or find a counterexample? For reference, here is the data that I was using. [1]
[2]
[4]
[5.8]
[7.10,16]
[11,14,17,20,32]
[13,16,19,22,25,28,34,40,64]
[14,17,20,23,26,29,32,35,38,41,44,50,56,65,68,80,128]
[19,22,25,28,31,34,37,40,43,46,49,52,55,58,61,64,67,70,73,76,82,85,88,100,112,130,136,160,256] [1]
[3]
[9]
[11,27]
[17,33,81]
[19,35,51,83,99,243]
[41,57,73,89,105,121,153,249,297,729]
[43,59,75,91,107,123,139,155,171,187,219,251,267,299,315,363,459,731,747,891,2187] [1]
[5]
[25]
[29,125]
[49,145,625]
[53,149,245,629,725,3125]
[73,169,265,361,649,745,841,1225,3145,3625,15625] [1]
[7]
[49]
[55,343]
[97,385,2401]
[103,391,679,2407,2695,16807] n:=16;
l:=[];
for i in [1..NrSmallGroups(n)] do
    G:=SmallGroup(n,i);
    if not \in(NrConjugacyClasses(G),l) then 
        Add(l,NrConjugacyClasses(G));
        Sort(l);
    fi;
od;
Print(l); One well-known pattern in the above data is a result of Hall that if $n=2k+e$ (with $e=0,1$ ) then $k=p^e+(p^2-1)(n+(p-1)t)$ for some $t\geq0$ .
This explains why the above lists jump by $(2-1)(2^2-1)=3$ when $p=2$ , by $(3-1)(3^2-1)=16$ when $p=3$ , by $(5-1)(5^2-1)=96$ when $p=5$ , and by $(7-1)(7^2-1)=288$ when $p=7$ .","['gap', 'conjectures', 'finite-groups', 'p-groups', 'group-theory']"
3589176,Filling an $8\times 8$ grid with the numbers $1$ to $64$ such that every $3\times 3$ subsquare has a sum less than $256$,Can you help me construct an $8 \times 8$ square filled with numbers from 1 to 64 (each cell has a different number obviously) such that every $3 \times 3$ subsquare has sum of numbers less than $256$ ? I have tried to fill up the corners with the big numbers but I have failed to balance the $3 \times 3$ squares. I have also find some symmetries in the configuration but I still haven't succeed to stay lower than $256$ . The question is a subtask from a problem I invented myself. I don't post the whole problem because I want to protect my work and use it for future projects.,"['matrices', 'combinatorial-designs']"
3589178,How do I evaluate this limit without using L'Hospital or Series expansion?,"I don't know how to do this limit. $$\lim_{x\to\infty}\left(\frac1{x^2\sin^2\frac 1x}\right)^\frac 1{x\sin\frac 1x-1}$$ And here it is as an image, with bigger font: I tried substituting $t=1/x$ and I got something better looking but it still didn't work. What trick do I need to use here?","['limits', 'limits-without-lhopital']"
3589247,Boundedness of one-dimensional Sobolev functions,"Are all Sobolev functions in $W^{1,p}(\mathbb{R})$ essentially bounded? Could any one tell me is this correct or give me an example of a function which belongs to $W^{1,p}(\mathbb{R})$ but is not essentially bounded?","['sobolev-spaces', 'functional-analysis']"
3589340,Show that there exists a number c with a certain condition,"$f:[-1,1]\rightarrow\Bbb{R} $ $f(-1)=f(1)=0$ , both $f$ and $f'$ are differeniable, $a\in(-1,1)$ . Show that $\exists_{c\in(-1,1)}(f''(c)=\dfrac{2f(a)}{a^2-1})$ . By Lagrange's middle point theorem, we have that: $\exists_{x_{1}\in(-1,a)}\space f'(x_1)=\dfrac{f(a)-f(-1)}{a+1}=\dfrac{f(a)}{a+1}$ $\exists_{x_{2}\in(-1,a)}\space f'(x_2)=\dfrac{f(a)-f(1)}{a-1}=\dfrac{f(a)}{a-1}$ . We know that $f'$ is differentiable so we can apply the same theorem again and obtain $\exists_{c\in(x_1,x_2)}\space f''(c)=\dfrac{f'(x_1)-f'(x_2)}{x_1-x_2}=\dfrac{\dfrac{f(a)}{a+1}-\dfrac{f(a)}{a-1}}{x_1-x_2}=\dfrac{2f(a)}{a^2-1}\dfrac{1}{x_2-x_1}$ . I thought this might be a mistake by the author and tried to find a counterexample but the thesis does seem to be correct. It's so close to the answer but I have no clue how to take it from here or if this approach is even salvageable. Why would it be so that we can always find such $x_1$ and $x_2$ that are separated by exactly $1$ ?","['calculus', 'derivatives']"
3589565,Solve $\frac{x+\dots+x^K}{K} = \frac{1}{2}$ for large values of $K$,"I am interested in the unique solution $x$ for the equation : $$
p_K(x)=\frac{x+\dots+x^K}{K}=\frac{1}{2},
$$ for large values of $K$ . When $K$ is small ( $K=1$ and $K=2$ ) we can solve this equation explicitly and find : $$
x=\frac{1}{2}, \frac{\sqrt{5}-1}{2}.
$$ For $K=3$ we still get an explicit solution which is more complicated and from $K=4$ on I do not find an explicit solution. As $K$ tends to infinity, we find that the unique solution $x_K$ of $p_K(x_K)=0$ tends to one, i.e. $\lim_{K \rightarrow \infty} x_K = 1$ . I would like to find an asymptotic approximation of $p_K(x)$ denoted by $\tilde p_K(x)$ for which the solutions $\tilde x_K$ satisfy: $$
\lim_{K\rightarrow \infty} K \cdot \log(x_K)
=
\lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K).
$$ My idea was to use the approximation : $$
\frac{x+\dots+x^K}{K} \approx x^{\frac{\sum_{j=1}^K j }{K}} = x^{\frac{K+1}{2}}.
$$ Using this approximation, we find $\tilde x_K = \left( \frac{1}{2} \right) ^{\frac{2}{K+1}}$ and we find for the limit : $$
\lim_{K\rightarrow \infty} K \cdot \log(\tilde x_K) = -2 \log(2) \approx -1.38
$$ but by numerical approximation,  find : $$
\lim_{K\rightarrow \infty} K \cdot \log(x_K) \approx - 1.592
$$","['real-analysis', 'calculus', 'polynomials', 'limits', 'convex-analysis']"
3589571,Jacobians when integrating over symmetric and antisymmetric matrices,"I'm interested in integrals over matrix elements. For integrals over the elements of a symmetric matrix $S$ or an antisymmetric $A$ , how can I find the Jacobian of the transformations $S \rightarrow B^T S B$ or $A \rightarrow B^T A B$ , where B is some real invertible matrix? Below shares my work and my guesses for what the Jacobians should be. Here's a warmup that shows how I personally would find the Jacobian corresponding to a linear transformation of my matrix of interest. Consider an integral over the $d$ by $d$ dimensional matrix $X$ . That is, consider the integral $$\int_{\Gamma} f(X) dX .$$ $f(X)$ is schematic for a function of all the matrix elements. Here, $dX$ is schematic for $\prod_{i,j}^d dX_{ij}$ . $\Gamma$ is some $d^2$ -dimensional integration region. If I were to consider a real, invertible transformation $U=BX$ , one can quickly see by considering the columns of $X$ that $$\int_{\Gamma} f(X) dX = \int_{\Gamma} f(X) \prod_{j=1}^d \Big(\prod_{i=1}^d dX_{ij}\Big) = \int_{B\Gamma} f(B^{-1}X) \prod_{j=1}^d \frac{\prod_{i=1}^d dU_{ij}}{\det(B)} = \int_{B\Gamma} f(B^{-1}X)  \frac{\ dU}{\det(B)^d}.$$ That is, we have a Jacobian not of $\det(B)$ but of $\det(B)^d$ . One can check this makes sense by considering $B$ a constant $c$ times the identity matrix - then $\det(B) = c^d$ , and $\det(B)^d = c^{d^2}$ , which is exactly what one would get by scaling each of the $d^2$ elements of the matrix $X$ by $c$ . Thus, for $U=BX$ , we have $dX \rightarrow \frac{dU}{\det(B)^d}$ . Similarly, we can see that $$\text{for }U=B^TXB\text{, we have }dX \rightarrow \frac{dU}{\det(B)^{2d}}.$$ This can be seen quickly by performing the transformation in two steps and considering the columns and rows of $X$ . However, what if our integration was over a symmetric matrix $S$ ? That is, consider the integral $\int_{\Delta} f(S)dS.$ Here, $dS$ is schematic for $\prod_{i\geq j}^d dS_{ij}$ . Here, $\Delta$ is the integration region; counting the number of independent components of a symmetric matrix gives us that $\Delta$ must be a $\frac{1}{2}d(d+1)$ -dimensional region. Note that the transformation $U=B^TSB$ preserves the symmetry of $S$ : $U$ is also symmetric. Then, by counting the number of independent components, $\frac{1}{2}d(d+1)$ , I'd guess that $$\text{for } B^TSB\text{, we have } dS \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d+1}}.$$ Similarly, for an antisymmetric matrix $A$ , we have that the transformation $U=B^T A B$ preserves the asymmetry; $U$ is also antisymmetric. Defining $dA = \prod_{i>j}^d dA_{ij}$ , and noting $A$ has $\frac{1}{2}d(d-1)$ independent components, I'd guess that $$\text{for } B^TAB\text{, we have } dA \stackrel{?}{\rightarrow} \frac{dU}{\det(B)^{d-1}}.$$ Are these guesses correct? How can I formally find the Jacobians of the transformations $U = B^T SB$ and $U=B^T AB$ ?","['jacobian', 'multivariable-calculus', 'matrix-calculus', 'linear-algebra']"
3589652,Use Cauchy integral to calculate,"a) $\displaystyle\int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz$ Apply the Cauchy integral with $f(z)=\dfrac{e^z}{(z-3)^2}$ at $ z= -1  $ . Then: $$\int_{\partial B_2(0)}\dfrac{e^z}{(z+1)(z-3)^2}dz = 2\pi if(-1) =2 \pi \dfrac{e^{-1}}{(-4)^2}=2\pi i \dfrac{e^{-1}}{16}=\dfrac{1}{8}\pi i e^{-1} $$ b) $\displaystyle\int_{\partial B_2(0)}\dfrac{\sin z}{(z+i)}dz$ Apply the Cauchy integral with $f(z)=\sin z$ at $z=-i$ . $f(z)=\sin z$ is holomorphic inside $|z|=2$ . Then $$\int_{\partial B_2(0)}\dfrac{\sin z}{z+i}dz= 2\pi i f(-i)=2 \pi i \sin(-i)=2\pi i (-i \sinh(1)) = 2 \pi \sinh(1)$$ $\sin(-i)=-i\sinh(1)$ c) $\displaystyle\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)}$ partial fractions: $\dfrac{1}{z^2+1} = \dfrac{1}{z^2-i^2}= \dfrac{1}{(z+i)(z-i)}=\dfrac{i/2}{z+i}-\dfrac{i/2}{z-i}$ Then $$\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)} = \dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z+i)}-\dfrac{i}{2} \int_{\partial B_2(-2i)}\dfrac{dz}{(z-i)}$$ $C= \partial B_2(-2i)$ Then $f(z)=1$ and $i$ y $-i$ its inside $\mathbb{C}$ , then would be: $$\int_C \dfrac{dz}{z-i} = 2 \pi i f(-i) = 2 \pi i $$ $$\int_C \dfrac{dz}{z-i} = 2 \pi i f(i) = 2 \pi i $$ Then $$\int_{\partial B_2(-2i)}\dfrac{dz}{(z^2+1)}=0$$ d) $\displaystyle\int_{\partial B_1(0)}\dfrac{e^z}{(z-2)^3}dz $ Applying the Cauchy integral with $f(z)=e^z$ at $z=2$ . But 2 does not belong to the circle of radius 1, then the integral is 0. Am I correct? I would like to know if there is any mistake, I am just starting learning about this","['complex-analysis', 'contour-integration', 'cauchy-integral-formula']"
3589684,"If $\operatorname{MSpec}(A)$ with Zariski topology is Hausdorff, is $A$ a pm-ring?","May You help me with the following proof?
Thank You! Let $A$ be a commutative ring with identity $1_A\ne 0_A$ . Let $\operatorname{Spec}(A)$ be the set of all prime ideals of $A$ and let $\operatorname{MSpec}(A)$ be the set of all  maximal ideals of $A$ .\
We know that if $A$ is a pm-ring (i.e. each prime ideal of $A$ is contained in only one maximal ideal of $A$ ) then $\operatorname{MSpec}(A)$ with Zariski topology $\mathscr{Z}_M$ , inherited by  Zariski topology $\mathscr{Z}$ on $\operatorname{Spec}(A)$ , is $T_2$ . We know that $\operatorname{MSpec}(A)$ with Zariski topology $\mathscr{Z}_M$ is $T_2$ in and only if $A\left/\mathscr{J}(A)\right.$ , where $\mathscr{J}(A)=\bigcap \operatorname{MSpec}(A)$ is the Jacobson radical, is a pm-ring. 1) $\forall\ \mathfrak{p}\in \operatorname{Spec}(A)\quad\forall\ \mathfrak{m}_1,\mathfrak{m}_2\in \operatorname{MSpec}(A)\qquad \mathfrak{p}\subseteq\mathfrak{m}_1\cap\mathfrak{m}_2\quad\Rightarrow\quad \mathfrak{m}_1=\mathfrak{m}_2$ ; 2) $\left(\operatorname{MSpec}(A),\mathscr{Z}_M\right)$ is $T_2$ . I tried to prove that $2) \Rightarrow 1)$ , but I think  there is something wrong. We suppose, by absurdum, that: $$\exists\ \mathfrak{p}_0\in \operatorname{Spec}(A)\quad\exists\ \mathfrak{n}_1,\mathfrak{n}_2\in \operatorname{MSpec}(A)\qquad \mathfrak{p}_0\subseteq\mathfrak{n}_1\cap\mathfrak{n}_2\quad\wedge\quad \mathfrak{n}_1\ne\mathfrak{n}_2.$$ Then, there exist $W_1,W_2\in \mathscr{Z}_M$ such that $\mathfrak{n}_1\in W_1$ , $\mathfrak{n}_2\in W_2$ and $W_1\cap W_2=\emptyset$ . $$\mathfrak{p}_0\subseteq\mathfrak{n}_1\:\Rightarrow\:\mathfrak{n}_1\in\overline{\left\{\mathfrak{p}_0\right\}};$$ $$\mathfrak{p}_0\subseteq\mathfrak{n}_2\:\Rightarrow\:\mathfrak{n}_2\in\overline{\left\{\mathfrak{p}_0\right\}}.$$ Let be $$\overline{\left\{\mathfrak{p}_0\right\}}^{\, r}=\operatorname{MSpec}(A)\cap\overline{\left\{\mathfrak{p}_0\right\}}$$ the closure of $\left\{\mathfrak{p}_0\right\}$ in $\left(\operatorname{MSpec}(A),\mathscr{Z}_M\right)$ . So, because $\mathfrak{n}_1\in\overline{\left\{\mathfrak{p}_0\right\}}^{\, r}$ , $W_1\in\mathscr{Z}_M$ and $\mathfrak{n}_1\in W_1$ , for closure's point definition, we have that: $$W_1\cap \left\{\mathfrak{p}_0\right\}\ne\emptyset.$$ So, we deduce that $\mathfrak{p}_0\in W_1$ . In the same way, we have that $\mathfrak{p}_0\in W_2$ . At the end, we obtain that $\mathfrak{p}_0\in W_1\cap W_2\ne\emptyset$ . The absurdum is reached.","['general-topology', 'algebraic-geometry']"
3589693,Complex Analysis vs. $\mathbb{R}^2$ Integrals,"I'm wondering about the following I'm somewhat shocked about, since I wasn't too aware of it, so I'm asking here for clarification or if it is even true... Let $f(z)$ be holomorphic. When calculating the closed loop integral over the contour $C$ and converting to real and imaginary parts, I get the following $$0=\oint_C f(z) \, {\rm d}z = \oint_C (u+iv) \, {\rm d}(x+iy) \\
=\oint_C \left\{ u \, {\rm d}x - v \, {\rm d}y + i\left(u \, {\rm d}y + v \, {\rm d}x\right) \right\} \\
=\oint_C \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} + i \oint_C \begin{pmatrix}v \\ u \\ 0\end{pmatrix} \cdot \begin{pmatrix} {\rm d}x \\ {\rm d}y \\ {\rm d}z \end{pmatrix} \, .
$$ In the last step I interpreted the planar curve $C$ as part of ${\mathbb{R}}^3$ . Now because of the $i$ each term has to vanish separately. Let's consider the first term first: Because $u$ and $v$ are harmonic functions, they are differentiable and thus Stokes theorem can be applied. The first term then becomes $$0=\int_A {\rm d}\vec{A} \cdot \nabla \times \begin{pmatrix}u \\ -v \\ 0\end{pmatrix} = -\int_A {\rm d}\vec{A} \cdot \begin{pmatrix} 0 \\ 0 \\ \partial_x v + \partial_y u \end{pmatrix}$$ where $A$ is any oriented area bounded by the planar curve $C$ . Therefore $\begin{pmatrix}u \\ -v \\ 0\end{pmatrix}$ is integrable and can be expressed as a gradient i.e. $\begin{pmatrix}u \\ -v \\ 0\end{pmatrix}=\nabla \phi(x,y)$ . Plugging this into the CR-equations, it all makes sense $$\frac{\partial u}{\partial x} = \frac{\partial^2 \phi}{\partial x^2} = \frac{\partial v}{\partial y} = - \frac{\partial^2 \phi}{\partial y^2} \quad \Rightarrow \quad \Delta \phi = 0$$ i.e. $\phi$ is harmonic. The second equation reads $$\frac{\partial u}{\partial y} = \frac{\partial^2 \phi}{\partial y \partial x} = -\frac{\partial v}{\partial x} = \frac{\partial^2 \phi}{\partial x \partial y}$$ which is true due to Schwarz. I see that starting from the CR-equations and expressing $u$ and $v$ as a gradient, they are automatically fulfilled. But does this also mean that this is true in all generality? For every holomorphic function, the real and imaginary part is a gradient of some scalar potential $\phi$ that is harmonic i.e. every holomorphic function $f(z)$ can be decomposed as $$f(z) = \partial_x \phi - i\partial_y \phi = 2\,\partial_z \phi \, .$$ Since $\phi$ is harmonic, i.e. $4\partial_\bar{z}\partial_z \phi=0$ , it can be written as $$\phi(x,y) = \tilde{\phi}(z,\bar{z}) = \phi_1(z) + \phi_2(\bar{z}) \, ,$$ since mixed terms can not arise. But now $\phi$ is real $\forall x,y \in \mathbb{R}$ , so $\phi_2=\bar{\phi}_1$ . So essentially it boils down to the fact that the 2d-laplacian nicely factorizes upon the variable transformation $(x,y)\rightarrow (z,\bar{z})$ .","['complex-analysis', 'analysis']"
3589715,"Convergence of $\frac{a_{n+1}}{a_n}$, where $|a_{n+1}a_{n-1} - a_n^2| = 1 $",Let $(a_n)$ a non-decreasing sequence of positive real numbers such that $\lim a_n = \infty$ and $|a_{n+1}a_{n-1} - a_n^2| = 1 $ . Prove that the sequence $(\frac{a_{n+1}}{a_n})$ converges. My little Â´Â´progressÂ´Â´: $|a_{n+1}a_{n-1} - a_n^2| = 1 $ . $\left|\dfrac{a_{n+1}a_{n-1}}{a_n^2} - 1\right| = \dfrac{1}{a_n^2} \to 0$ $\dfrac{a_{n+1}a_{n-1}}{a_n^2} \to 1$ $\dfrac{a_{n+1}}{a_n}\dfrac{a_{n-1}}{a_n} \to 1$,"['sequences-and-series', 'real-analysis']"
3589823,"Solving the PDE $x_{1}\dfrac{\partial f}{\partial x_{1}}+x_{2}\dfrac{\partial f}{\partial x_{2}}=e^{f(x_{1},x_{2})}-\alpha.$","This post is closely related to Dirichlet to Neumann operator in the unit ball with Fourier Analysis . I have transformed the exercise in the post above into a problem of finding solution of PDE: $$x_1\frac{\partial f}{\partial x_1} + x_2 \frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha \text{ for } \alpha>0.$$ This is an exercise in Fourier analysis, so I am not prepared too many knowledge in differential equation. There are two hints: Use the Fourier Expansion; 2. Separate the argument into $\alpha\in\mathbb{N}$ and $\alpha\notin\mathbb{N}$ . However... I don't have any idea about how to solve this.. Any idea? Thank you! Edit: In the link above,  it suggested that if $\alpha\notin\mathbb{N}$ , then $f=\log\alpha$ , and asked the reader to further discover what happens if $\alpha\in\mathbb{N}$ . So if $\alpha\notin\mathbb{N}$ , we have $$x_1\frac{\partial}{\partial x_1}+x_2\frac{\partial f}{\partial x_2}=0 \text{??}$$ Edit 2: Below is how I convert the exercise in the link above to this PDE: Within the context of this exercise, we have the coinciding solution of the Dirichlet problem $$\Delta u=0 \text{ on } B_1$$ $$u=f \text{ on } \partial B_1 =\mathbb{S}^1$$ and of the Neumann problem $$\Delta u=0 \text{ on } B_1$$ $$\frac{\partial u}{\partial\nu}=e^f-\alpha \text{ on } \partial B_1=\mathbb{S}^1,$$ where $\dfrac{\partial u}{\partial\nu}= \nabla u\cdot \nu$ is the normal derivative of $u$ at the boundary with respect to the unit outer normal direction $\nu$ . Now, note that for a point $(x_1,x_2)\in\partial B_1 = \mathbb{S}^1$ , we always have $\nu=(x_1,x_2)$ . Also, by the solution of the Dirichlet problem, we know that $u=f$ on $\partial B_1=\mathbb{S}^1$ , and thus on the boundary we have $$\frac{\partial u}{\partial\nu} = \nabla u\cdot \nu=x_1\frac{\partial u}{\partial x_1} + x_2 \frac{\partial u}{\partial x_2} = x_1 \frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2},$$ but the boundary condition of Neumann problem is $$\dfrac{\partial u}{\partial\nu}=e^{f}-\alpha,$$ and thus we have $$x_1\frac{\partial f}{\partial x_1} + x_2\frac{\partial f}{\partial x_2} = e^{f(x_1,x_2)}-\alpha.$$ Edit 3: (initial value) As ""Ninad"" pointed out, we need to an initial value to decide what is $C(t).$ And I believe perhaps the initial value is related to whether $\alpha$ is natural or not. The exercise does not give what happens if $\theta=0$ . However, I missed one conditiona that $f\in C^\infty(\mathbb{S}^1)$ , a infinity smooth $2\pi-$ periodic function. I don't know if this helps to provide us the initial value.","['fourier-analysis', 'ordinary-differential-equations', 'harmonic-analysis', 'analysis', 'partial-differential-equations']"
3589844,Would duplicates matter in cartesian product of a set?,"For example: \begin{align}
A	&=	\{1, 1, 2\}
\\
B	&=	\{3, 3, 3, 2, 2, 4\}
\end{align} Would $A$ cross $B$ equate to $\{(1,3),(1,2),(1,4),(2,3),(2,2),(2,4)\}$ without the dupes of $(1,3)$ , etc.","['elementary-set-theory', 'discrete-mathematics']"
3589858,"Is it a (minor) typo in the proof of Roman Vershynin's ""High dimensional probability with application to data science"" (linked), Theorem 3.1.1","So I've been currently studying this book on high dimensional probability by Roman Vershynin, which I find pretty awesome! However, I was wondering if in the first line of the proof of Theorem 3.1.1 (P. 43), where he proved a concentration inequality for high dimensional random vectors with independent components, it should be "" it follows that "" instead of "" for simplicity, we can assume that ""? This is because, since he assumed that $EX_i^2=1 \forall i,$ : $$2 \ge E[e^\frac{{X_i^2}}{||X_i||_{\psi_2}^2}] \ge E[1 + \frac{{X_i^2}}{||X_i||_{\psi_2}^2}] = 1 + \frac{{EX_i^2}}{||X_i||_{\psi_2}^2} = 1 + \frac{1}{||X_i||_{\psi_2}^2},$$ implying ${||X_i||_{\psi_2}^2} \ge 1 \forall i$ . Just checking to make sure, thanks!","['statistics', 'probability-distributions', 'random-matrices', 'probability-theory', 'probability']"
3589921,Smullyan Logic Puzzle: Either Tiger behind Door 1 or Gold behind Door 2,"""The prisoner is presented with two doors. In a room behind each door is either gold or a tiger. The sign on the doors are either both true or both are false. Door 1: Either there is a tiger behind this door or gold behind the second door. Door 2: There is gold behind this door. Which door should the prisoner open?"" I have been trying to figure out the answer to this question. If I interpret Sign 1 as an inclusive or, then I end up getting 3 ""no contradiction"" cases, but it doesn't tell me which Door is the best choice (unless we actually care about probability such as 2/3 chance of getting Gold). To mitigate this, the best I could come up with is to interpret Sign 1 as an exclusive or. Solution attempt EDIT: Here are other places on the internet where the same problem appears. http://pi.math.cornell.edu/~araymer/Puzzle/PuzzleNights.html (Page 4) http://www2.gcc.edu/dept/math/faculty/BancroftED/teaching/handouts/MATH213_rules_of_inference.pdf https://www.ibtimes.co.uk/mathematician-puzzle-maker-raymond-smullyan-dead-97-1605912","['puzzle', 'logic', 'discrete-mathematics']"
3590069,How to factorize this determinant?,"The question is to factorize $$\det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix}.$$ I have a hint which is considering the factorization of $\det\begin{pmatrix}1 & a & a^2  \\ 1 & b & b^2 \\ 1 & c & c^2  \end{pmatrix}$ where all entries are real numbers. I don't have any idea how to use the hint. So, I factorize $\det\begin{pmatrix}(x^2+1)^2 & (xy+1)^2 & (xz+1)^2  \\ (xy+1)^2 & (y^2+1)^2 & (yz+1)^2 \\ (xz+1)^2 & (yz+1)^2 & (z^2+1)^2  \end{pmatrix}$ directly and I get the answer which is $2(z-y)^2(z-x)^2(y-x)^2$ . My question is how to use the hint to factorize the given determinant? It is because my method seems very tedious.","['matrices', 'determinant', 'linear-algebra', 'factoring']"
3590072,Applications of Manifolds not embedded in Euclidean Space,"In the first few pages of his book Introduction to Smooth Manifolds , Lee writes: But
  for more sophisticated applications, it is an undue restriction to require
  smooth manifolds to be subsets of some ambient Euclidean space. A way to motivate why we develop tools to do calculus in manifolds is through examples in physics. For instance, one might want to do calculus on the function that maps the surface of the earth to its temeprature on the real line, we might want to study electromagnetic properties of a torus, and so forth. In each of these cases, it is easy to study these surfaces as subsets (or submanifolds) of Euclidean space. What are some ""sophisticated applications"" of manifolds outside of math - as Lee writes - that do not allow us to work with manifolds that are embedded in Euclidean space? Edit: I'm obviously looking for examples other than the one Lee gives himself (and perhaps the most popular here): Looking at space-time as a four dimensional manifold, where it doesn't make sense to embed it in an ambient space.","['smooth-manifolds', 'physics', 'real-analysis', 'manifolds', 'differential-geometry']"
3590145,Exact expression of a trigonometric integral,"Let $a>2$ be a real number and consider the following integral $$
I(a)=\int_0^\pi\int_0^\pi \frac{\sin^2(x)\sin^2(y)}{a+\cos(x)+\cos(y)} \mathrm{d}x\,\mathrm{d}y
$$ My question. Does there exist a closed-form expression of $I(a)$ ? Some comments. Since $a-2<a+\cos(x)+\cos(y)<a+2$ and $\int_0^\pi \int_0^\pi \sin^2(x)\sin^2(y)\ \mathrm{d}x\, \mathrm{d}y=\frac{\pi^2}{4}$ , we have the following bounds $$
\frac{\pi^2}{4(a+2)} < I(a) < \frac{\pi^2}{4(a-2)},
$$ however I didn't manage to find an exact expression for $I(a)$ . Any help is welcome!","['integration', 'trigonometric-integrals', 'closed-form', 'analysis']"
3590196,Example of a symplectic but non-Hamiltonian vector field on $\mathbb{T}^{2n}$,"I want to show that there exists a symplectic vector field on the $2n$ torus $\mathbb{T}^{2n}$ ,  endowed with the unique symplectic form $\omega$ that pullsback to the canonical symplectic form $\omega_0$ on $\mathbb{R}^{2n}$ under the quotient map $\pi:\mathbb{R}^{2n}\to\mathbb{T}^{2n}$ , which is not Hamiltonian. We identify $T_x\mathbb{T}^{2n}\cong\mathbb{R}^{2n}$ for all $x\in\mathbb{R}^{2n}$ and we define the vector field $X\in\mathcal{X}(\mathbb{T}^{2n})$ by $X(x)=v$ for some fixed $0\neq v\in\mathbb{R}^{2n}$ . Then I want to show that $d\iota_X\omega=0$ . I consider the vector field $\tilde{X}$ on $\mathbb{R}^{2n}$ defined by $\tilde{X}(x)=v$ . Then $\tilde{X}$ is symplectic and satisfies $d\pi_x\tilde{X}(x)=X_{\pi(x)}$ , so $d\iota_{\tilde{X}}\omega_0=0$ . But $$
d\iota_{\tilde{X}}\omega_0=d\omega_0(\tilde{X},\cdot)=d(\pi^*\omega(\tilde{X},\cdot))=d\omega_{\pi(\cdot)}(d\pi\tilde X,d\pi\cdot)\underbrace{=}_{?}d\omega(X,\cdot)=d\iota_X\omega,
$$ so $X$ is symplectic. I am not sure about the step with a question mark, as the $d\pi$ in the second argument disappears. Is there any justification for this? Now, I want to show that $X$ is not Hamiltonian. As always, we assume it is, so there exists a smooth map $H:\mathbb{T}^{2n}\to\mathbb{R}$ such that $\iota_X\omega=dH$ . But I don't see how to arrive at some contradiction now.","['symplectic-geometry', 'ordinary-differential-equations', 'hamilton-equations', 'differential-forms', 'differential-geometry']"
3590203,"Calculate $\mathbb{E}(X-Y\mid 2X+Y).$ if $X\sim N(0,a)$ and $Y\sim N(0,b)$","Question: Given that $X$ and $Y$ are two random variables  satisfying $X\sim N(0,a)$ and $Y\sim N(0,b)$ for some $a,b>0$ . Assume that $X$ and $Y$ have correlation $\rho.$ Calculate $$\mathbb{E}(X-Y \mid 2X+Y).$$ I tried to use the fact that if $A$ and $B$ are independent, then $\mathbb{E}(A\mid B) = \mathbb{E}(A)$ and uncorrelated implies independence in jointly normal distribution. So, I attempted to express $X-Y$ as a linear combination of $2X+Y$ and $Z$ where $\operatorname{Cov}(2X+Y,Z) = 0.$ But I am not able to do so. Any hint is appreciated.","['conditional-expectation', 'expected-value', 'normal-distribution', 'probability']"
3590209,Reference request for algebraic geometry,"I'm an undergraduate student; I must read Mumford's Red Book (the first chapter) in order to write the dissertation. However, I find very difficult to understand all the proofs and the propositions of that book; consider that I'm completely new to algebraic geometry, although I know Galois theory and some algebraic topology. I would like a book (or something else) that is more introductory, in order to give a sense to what I read in Mumford's book. I mean, I would like a book that explains the motivation that led to the definitions and the structures of algebraic geometry. Thank you in advance","['algebraic-geometry', 'book-recommendation', 'reference-request']"
3590215,Intuition for sampling random variables?,"I have recently come to understand random variables from the perspective as deterministic measurable functions $X: \Omega \to \mathbb{R}$ . I've been rereading some old statistics text books and realized that in this framework I no longer understand what it means to sample something. For example, in a recent text, I read something along the lines of ""you can sample a geometric random variable with parameter $p$ by flipping a $p$ -weighted coin and counting the number of flips until tails is turned"". I am not sure how to intuitively interpret this as a random variable in this measure theory framework. Somewhat similarly, when texts say ""sample i.i.d $X_1,\ldots, X_n \sim \mathcal{N}(\mu, \sigma^2)$ "" what exactly does this mean? By what process do we actually accomplish this? Does this just mean that we explicitly choose a bunch of functions $X_k: \Omega\to \mathbb{R}$ satisfying equality of distribution functions $F_{X_k}(\alpha) = \Phi(\alpha)$ and independence of laws: $\mathcal{P}_{(X_i, X_j)} = \mathcal{P}_{X_i}\times \mathcal{P}_{X_j}$ ? Any intuitive clarifications would be really helpful! Huge plus if there's a nice way to formalize these methods for sampling and sampling i.i.d etc.","['statistics', 'measure-theory', 'probability-distributions', 'probability-theory', 'probability']"
3590216,Commutativity up to scalar implies commutativity in an algebra,"Let $A$ be a (not necessarily commutative) algebra over a field $k$ . Suppose that for all $a,b\in A$ , we have $kab=kba$ , i.e. commutativity up to scalar. Show that then $A$ is commutative. In the assumption, it is important that it holds for all $a,b\in A$ , otherwise it would be false. This is a step in Exercise 2.4.8 of Radford's book ""Hopf algebras"".","['abstract-algebra', 'noncommutative-algebra', 'hopf-algebras', 'commutative-algebra', 'exterior-algebra']"
3590231,Injective operator on a Banach space,"Let $X$ be a Banach space, $T \in B(X)$ and $||Iâˆ’T||=Î³<1$ . Show that $T$ is injective. My attempt: Suppose that $Tx=Ty$ , for some $x,y\in X$ . Then, by the linearity of $T$ , $$T(x-y)=0\implies ||T(x-y)||=0.$$ By definition of the operator norm, $$||T(x-y)|| \le ||T||\,||x-y||.$$ How can I show that this implies $||x-y||=0$ ?","['operator-theory', 'functional-analysis']"
3590270,Show that a graph with at least half of nodes of degree at least 10 is not planar.,"I am having troubles solving this exercise of a sample test: G=(V, E) is a simple, connected, undirected graph with |V|=n and |E|=m and no bridge. Show that, if G has at least half of its vertices of degree at least 10, G is not planar. I was thinking of using the m <= 3n-6 property, but still do not know where to go from there. Edit: I have thought of something: we know that for every graph of |V|>=11, either itself or its complement is planar. Since in this exercise more than half of the vertices have a degree of at least 10, G is not planar, but its complement is. Can I somehow use this to prove it?","['graph-theory', 'discrete-mathematics', 'planar-graphs']"
3590309,How do I prove this identity for complex numbers?,"I stumbled upon following identity I'm having a hard time proving: if the complex numbers $a,b,c,d$ lie on the unit circle in that order and no half-circle contains all of them, the following identity should hold: $$
|a+d||b+c| + |a+b||c+d| = |a-d||b-c| + |a-b||c-d|
$$ Any idea on how I should approach this / is there some known theorem I'm unaware of?","['geometry', 'complex-numbers']"
3590310,Why derivatives are usually defined for interior points?,"Let $f:D\to\mathbb{R}$ be a function with $D\subseteq\mathbb{R}$ . In calculus, one usually assumes that $D=\mathbb{R}$ or $D\subset\mathbb{R}$ is an interval defined as one of the cases below. \begin{align*}
[a,b]&=\{x\in\mathbb{R}|a\leq x\leq b\}, \quad && [b,+\infty)=\{x\in\mathbb{R}|x\ge b\}, \\
[a,b)&=\{x\in\mathbb{R}|a\leq x< b\}, \quad && (b,+\infty)=\{x\in\mathbb{R}|x> b\}, \\
(a,b]&=\{x\in\mathbb{R}|a< x\leq b\}, \quad && (-\infty,a)=\{x\in\mathbb{R}|x< a\}, \\
(a,b)&=\{x\in\mathbb{R}|a< x< b\}, \quad && (-\infty,a]=\{x\in\mathbb{R}|x\leq a\}.
\end{align*} Definition. Suppose that the following limit exists $$\lim_{t\to x}\frac{f(t)-f(x)}{t-x}=L,\tag{1}$$ where $L\in\mathbb{R}$ . Then, derivative of $f$ at point $x\in D$ is defined to be $Df(x):=L$ . An equivalent definition follows from the change of variables in limits, e.g., Theorem 2 in this post . $$f'(x)=\lim_{\Delta x\to 0}\frac{f(x+\Delta x)-f(x)}{\Delta x},\tag{2}$$ The $\epsilon-\delta$ translation for the definition in $(1)$ is $$\exists L \in \mathbb{R},\,\,\forall\epsilon>0,\,\,\exists\delta>0,\,\forall t\in \big(D\cap B_{\mathbb{R}}(x,\delta)\big)-\{x\} \implies \frac{f(t)-f(x)}{t-x}\in B_\mathbb{R}(L,\epsilon).$$ I usually see in real analysis books that one defines the derivative for interior points of $D$ . However, looking at $(3)$ , I don't understand why such a restriction is usually made. As an example, consider $D=[a,b)$ and let $x=a$ . The above definition then turns into $$\exists L \in \mathbb{R},\,\,\forall\epsilon>0,\,\,\exists\delta>0,\,\forall t\in\big([a,b)\cap B_{\mathbb{R}}(a,\delta)\big)-\{a\}  \implies \frac{f(t)-f(a)}{t-a}\in B_\mathbb{R}(L,\epsilon),$$ which by assuming that $0<\delta <b-a$ is equivalent to $$\exists L \in \mathbb{R},\,\,\forall\epsilon>0,\,\,\exists\delta>0,\,\forall t\in(a,a+\delta) \implies \frac{f(t)-f(a)}{t-a}\in B_\mathbb{R}(L,\epsilon),\tag{3}$$ that I think totally makes sense. Is there any specific reason for confining derivative to interior points? Can some well-known theorems in calculus fail if we don't consider such a restriction?","['calculus', 'derivatives', 'real-analysis']"
