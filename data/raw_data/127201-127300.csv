question_id,title,body,tags
1941689,"If $X_1, \ldots, X_n \sim t_\nu$, a t-distribution with $\nu >1$, how to show $E\left(\max_{1 \leq i \leq n}|X_i|\right) = O\left(n^{1/\nu}\right)$?","If $X_1, \ldots, X_n \sim t_\nu$, a t-distribution with $\nu >1$ degrees of free, with each of them independent, then a  result from probability theory is that: $$
E\left(\max_{1 \leq i \leq n}|X_i|\right) = O\left(n^{1/\nu}\right)
$$ $X_n = O(Y_n)$ means that there exists a constant $a > 0$ such that $|X_n| \leq a|Y_n|$ for all $n$. The common trick is to use moment generating functions, for which it doesnt exist for a t-distribution. Does anyone have any ideas how to approach this?","['probability-theory', 'probability', 'distribution-tails']"
1941701,Relation between the eigenvalues of a matrix A and the eigenvalues of its hermitian and skew-hermitian parts,"It is well know that every matrix $A\in M_n(\mathbb{C})$ can be decomposed in a unique way as a sum of its hermitian part $H$ and its skew-hermitian part $K$:  $$A=\frac{A+A^*}{2}+\frac{A-A^*}{2}$$ However, there is a relationship between the eigenvalues of $A$, $H$ and $K$ that I´m trying to prove without success: If the spectrums of $A, H, K$ are $\sigma(A)= \{ \lambda_1,\ldots,\lambda_n  \}$, $\sigma(H)= \{ \alpha_1,\ldots,\alpha_n \}$, $\sigma(K)= \{ \beta_1, \ldots, \beta_n  \},$ then $$\sum_{i=1}^n\mathopen|\lambda_i\mathclose|^2\leqslant \sum_{i=1}^n\mathopen|\alpha_i\mathclose|^2+ \sum_{i=1}^n\mathopen|\beta_i\mathclose|^2$$ With equality $\text{iff}$ $A$ is normal.","['matrices', 'linear-algebra']"
1941720,What is the name of the theorem that says that ${\frak{g}} \subseteq \{X \in M_n(R) \mid \forall t \in R : e^{tX} \in G\}$?,"For a matrix Lie group $G \subseteq GL_n(\mathbb{R})$, we define the Lie algebra $\frak{g}$ as $T_0 G$. We then proceed to prove the inclusion in the title to give a definition of $\frak{g}$ in terms of the exponential map. I am very interested in learning if this theorem has a name.","['terminology', 'differential-geometry', 'lie-algebras', 'lie-groups']"
1941758,Sum and Product of n-positive integers,"If I have  $ n $-positive integers, and I compute their sum and product, is there any different group of $ n $-positive integers that will have the same sum and product? For example, if $ a,...,z $ denote 26 positive integers, and we define: \begin{align}
a+b+c+d+....+z &= \text{Sum} \\
a \cdot b \cdot c \cdot d \cdot .... \cdot z &= \text{Product}
\end{align} Is there any way I can get the same Sum and Product from a different group of 26 (in this example) positive integers? EDIT: A friend of mine pointed out that knowing that we have a group of 3 that works, we can show that it works for all positive groups of $ n $ integers. For Example:
$ \{3,3,10 \} $ and $ \{2,5,9 \} $ both yield Sum $=16 $ and Product $=90 $. Now we can just continually add a number (let's say 1) as the next integer to get multiple solutions for $ n =4,5,6,... $. Explicitly, $ \{ 3,3,10,1 \} $ and  $ \{ 2,5,9,1 \} $ both give Sum$=17$ and Product$=90$.",['number-theory']
1941765,Are my calculations of a recursive prime-generating function based on logarithms correct?,"I am trying to devise a recursive prime-generating function following an intuition of a possible analogy to Mills and Wright prime-representing functions, in the present case based on logarithms. The proposed prime generating function $f(n)$ will provide not a subset but the complete set of primes being $f(1)=2$, $f(2)=3$, $f(3)=5$... and the prime-generating constant will be called $L$. As in the standard results from Mills and Wright the decimal precision of the constant is important in order to recover the embedded primes and it is not known if 
  $\lim_{n \to \infty} L_n$ is rational or not. This is how it works and the questions are at the end: Start with $n=1$, current prime $p_1=2$, previous accumulated value of the constant will be defined as $L_0=0$ (starting value). Calculate the value for the constant for $n=1$. It will be $$L_1=\frac{Ln(2+L_0)}{Ln(1+1)}$$ (Where $Ln$ is the natural logarithm). Calculate the value for the constant for $n=2$, $p_2=3$, $$L_2=\frac{Ln(3+L_1)}{Ln(2+1)}$$ So if we apply recursively the formula, for $n$: $$L_n=\frac{Ln(p_n+L_{n-1})}{Ln(n+1)}$$ For instance, the following PARI_GP code calculates $L_{500}$ \p2000;
testlimit=500;current_pr=2;L=0;for(n=1,testlimit,L=log(current_pr+L)/log(n+1);current_pr=nextprime(current_pr+1););print(""n is "",testlimit,"" and L is "",L); $$L_{500} = 1.3159864456...$$ Reviewing the results of the tests, it seems that $\lim_{n \to \infty} L_n$ 
oscillates (due to the gaps between primes applied in the formula) but in the long term it is stable and tends to decrease and goes down to a value closer to $1$ and lower than $2$. For example: \p3;
testlimit=500000;current_pr=2;L=0;for(n=2,testlimit,L=log(current_pr+L)/log(n);current_pr=nextprime(current_pr+1););print(""n is "",testlimit,"" and L is "",L); $L_{5000}$ is around $1.23...$ and the above code shows that $L_{500000}$ is around $1.21$. Other similar tests show that it tends to go down to some specific limit near the lower bound of $[1,2]$. As the proccess of recovering the primes is recursive, the way of using the constant is as follows: For instance, assuming that we have $L_{500}$, we need to start obtaining the last prime back: $$f(500)=p_{500}=\lfloor (500+1)^{L_{500}} \rfloor - 1 = 3571$$ Then recover $L_{499}$ and recover $f(499)=p_{499}$ $$L_{499}=((500+1)^{L_{500}})-p_{500}=1.31586811...$$ $$f(499)=p_{499}=\lfloor (499+1)^{L_{499}} \rfloor - 1 = 3559$$ $$...$$ So in general the recursive process to recover $L_{n}$ and $f(n)=p_n$ from $L_{n+1}$ and $f(n+1)=p_{n+1}$ is: $$L_{n}=((n+2)^{L_{n+1}})-p_{n+1}$$ $$f(n)=p_{n}=\lfloor (n+1)^{L_{n}} \rfloor - 1$$ The following code, having $L_{500}$ calculates backwards the complete set of primes $\{p_{500}..p_{1}\}$ curr_L=L;for(n=1,testlimit,curr_n=testlimit-n+2;curr_p=(floor(curr_n^curr_L))-1;print(""n is "",testlimit-n+1,"" ; Current prime is "",curr_p,"" and is_prime check = "",isprime(curr_p));curr_L=(curr_n^curr_L)-curr_p;); There is a little correction required for the calculation of $p_1$. Depending on the $L_n$ calculated, sometimes the recovered value $\lfloor (2)^{L_{1}} \rfloor = 2$ instead of $3$. And for that reason, when the value is $2$, $f(1)=p_{1}=\lfloor (2)^{L_{1}} \rfloor - 1 = 2-1 = 1$ instead of the expected $p_1=2$. To handle this special case, we can express the prime-generating function as: $$f(n)=p_{n}=\lfloor (n+1)^{L_{n}} \rfloor - 1 + \delta_{f(n),2}$$ Where $\delta_{f(n),2}$ is the Kronecker delta function (kindly provided by @MitchellSpector in this question ). Basically it is a little trick that will assure that always $f(1)=2$ independently of the value of $\lfloor (n+1)^{L_{n}} \rfloor$ ($2$ or $3$). It would be possible a definitions-by-case of $f(n)$ as well instead of using one single expression. This is a graph of the evolution of $L_n$ (only four decimals of precision): I would like to ask the following questions: Are the calculations correct or is there a mistake in the assumptions? How could I prove that $L_n$ is decreasing in the long term and 
  there is indeed a limit? The tests show that but I am a little bit lost about
  how to demonstrate that it really is decreasing (because each step depends
  on the gaps between primes). A hint about the starting step would be great! Initially I think this kind of recursive prime-generating function is a little bit different from the original results of Mills and towers of powers of Wright, but it might be possible that a similar idea had been explored before in recent literature. Initially I did not find such references. Are there similar solutions as the one I am devising? Any references to papers would be very appreciated. Thank you!","['proof-verification', 'number-theory', 'constants', 'prime-numbers', 'sequences-and-series']"
1941782,uncountable union of uncountable sets and the usage of the axiom of choice,"I've been facing the next problem: $\alpha$ is an ordinal, show that there is no cardinals collection $\{\beta_i\}$, when $i\in I$. Such as: $\sum_I  \beta_i=\aleph_{\alpha+1}$. When $|I|<\aleph_{\alpha+1}$ and for all $i\in I$, $\beta_i < \aleph_{\alpha+1}$. My first question. Before I got to my current soultion (will be represented immediately) I was trying to solve it using konig inequality , and also tried to find elegant explanation using cardinals definiton, cause $\aleph_{\alpha+1}$ is also an ordinal. I couldn't make it, and I would really like to get any solution using those principles. In my current solution I was trying generalizing the lemma about countable union of countable set, my solution arises a doubt about my usage and understanding of the axiom of choice, I would be grateful for you comments and opinion about it. So, my solution: $|I|<\aleph_{\alpha+1} $ and for for all $i \in I$, $\beta_i <\aleph_{\alpha+1} $, so the largest result of $\sum_I\beta_i $ is when the collection $\{\beta_i\}_I $ cardinality is $\aleph_{\alpha}$ and $I$ carindality is also $\aleph_{\alpha}$. Now I want to prove that the cardinality of $\aleph_{\alpha}$ union of sets with cardinality $\aleph_{\alpha}$ is also $\aleph_{\alpha}$ Let $\{B_i\}_I $ be a collection of set with cardinality $\aleph_{\alpha}$ (representing the $\{\beta_i\}_I $ caridnals collection).
Given set $A$ with cardinality $\aleph_{\alpha}$, there is a bijection $g_I : I \to A$ and a set of bijections $f_i: B_i\to A$.
Now we define new collection of sets composed of $\{i\}\times B_i $ (each and every $i$ with its respective $B_i$). We define a one-to-one function $H:\bigcup(\{i\}\times B_i) \to A\times A$, when $H((i,b))=(g_I(i),f_i(b))$. the function is one to one, cause for $(i_1,b_1)\neq (i_2,b_2)\Rightarrow$ assuming $i_1 \neq i_2\Rightarrow g_I(i_1)\neq g_I(i_2)$ cause $g_i$ is a bijection, while in case $i_1 = i_2$ and $b_1 \neq b_2\Rightarrow f_i(b_1)\neq f_i(b_2)$, also cause $f_i$  is a bijection. So now we know that $|\bigcup (\{i\}\times B_i)|\le |A\times A|$.
Using the axiom of choice I want to show that $|\bigcup B_I|\le|\bigcup (\{i\}\times B_i)| $ The axiom of choice promises existence of a function $C : \bigcup B_i\to I$ such that for each $ b\in \bigcup B_i, C(b) = i$ when $b\in B_i$. Now using $C$, we define $ F: \bigcup B_I \to \bigcup (\{i\}\times B_i), F(b)  = (C(b),b) $. This, function is one-to-one cause for $b_1\neq b_2\Rightarrow (C(b_1),b_1)\neq (C(b_2),b_2)$, no matter how $C$ function operates upon $b_i$ elements. so from transitivity  $|\bigcup B_i|\le|\bigcup (\{i\}\times B_i)|\le |A\times A|$. And a known result about infinite sets which derives from ZL, $|A\times A|= |A|=\aleph_{\alpha}$. While, for a $B_0\in \{B_i\}_I, B_0 \subseteq \bigcup B_I\Rightarrow |B_0|\le|\bigcup B_I|$, so from CSB: $\aleph_{\alpha}=|B_0|\le|\bigcup B_I|\le|A\times A|=\aleph_{\alpha}$. What conclusively means that $\sum_I\{\beta_i\} \le \aleph_{\alpha}<\aleph_{\alpha+1}$","['cardinals', 'ordinals', 'elementary-set-theory', 'proof-verification']"
1941786,"Example of a Distribution that is not symmetric, but has a skewness of zero.","In my probably class we saw that if a distribution is symmetric then the skewness will be zero. Intuitively this makes sense to me since if a data set is symmetric than for each point that is distance 'd' above the mean there will be a point that is distance 'd' below the mean (Although, in practice this is probably just very close to zero and actually zero). However, it was implied in class that the converse is not true. That is, there may exist a distribution which is not symmetric, or skewed, but the skewness is zero. Can anyone give me an actual example of a distribution that is skewed, but the values of the skewness equals zero? Cheers","['statistics', 'probability', 'probability-distributions']"
1941831,"Understanding a proof of ""Every set has a group structure implies Axiom of Choice""","I am following this answer by Asaf to a question, whether every set has a group structure. The main part is the following: To prove that ""every set can be made into a group $\Rightarrow$ Axiom of choice"" Given an infinite set $X$ we define $H(X)$ to be the least ordinal $\alpha$ that there is no injection $g:\alpha\to X$ (this is known as the Hartog number of $X$) If $X$ can be injected into $H(X)$ then $X$ can be well ordered, since being injected into an ordinal means that $X$ inherits a well order. Using the assumption that every set can be given a group structure we give a group structure to $X\cup H(X)$, and from this we deduce that there exists an injection from $X$ into $H(X)$. Therefore if every set can be given a group structure, every set can be well ordered and therefore the axiom of choice holds. With this (part o answer), I have a very simple question, which came to me while considering necessaties of the objects used in the arguments. What is the necessity of taking ""$H(X)$, least ordinal etc"" , in the arguments? I mean, can we take simply $\mathcal{P}(X)$, the power set of $X$, instead of these objects ""$H(X)$, least ordinal "" in the arguments, and carry proof?","['elementary-set-theory', 'proof-explanation']"
1941844,mimimum value of expression $a^2+b^2$,"If $a,b$ are two non zero real numbers and $ab(a^2-b^2) = a^2+b^2,$ Then $\min(a^2+b^2)$ $\bf{My\; Try::}$ We can write it as $$ab=\frac{a^2+b^2}{a^2-b^2}\Rightarrow a^2b^2=\frac{(a^2+b^2)^2}{(a^2+b^2)^2-4a^2b^2}$$ Now Put $a^2+b^2=u$ and $a^2b^2=v,$ Then expression convert into $$v=\frac{u^2}{u^2-4v}\Rightarrow 4v^2-u^2v+u^2=0$$ For real roots, $\bf{Discriminant \geq 0}$ $$u^4-16u^2\geq 0\Rightarrow u^2(u^2-16)\geq 0$$ So we get $$u^2\geq 16\Rightarrow u\geq 4\Rightarrow x^2+y^2\geq 4,$$ My question is can we solve it any other way (without Trigonometric substution), If yes , Then plz explain here, Thanks","['algebra-precalculus', 'inequality', 'optimization']"
1941875,how to find area under normal distribution curve,"Find out the area in percentage under standard normal distribution curve of random variable $Z$ within limits from $-3$ to $3$. my try: probability density function of standard normal distribution is $f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}$ now the area under standard normal distribution curve ($-3\le x\le3$),
$$=\frac{1}{\sqrt{2\pi}}\int_{-3}^3e^{-\frac{x^2}{2}}\ dx$$
$$=2\frac{1}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$ 
$$=-\frac{2}{\sqrt{2\pi}}\int_{0}^3e^{-\frac{x^2}{2}}\ dx$$
$$=-\frac{2}{\sqrt{2\pi}}\left(\int_{0}^{\infty}e^{-\frac{x^2}{2}}\ dx-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$
$$=-\frac{2}{\sqrt{2\pi}}\left(\frac 12-\int_{3}^{\infty}e^{-\frac{x^2}{2}}\ dx\right)$$ i got stuck here, i don't have any clue to solve above integral. please help me to solve it or give some other method to find the area under the curve.","['statistics', 'area', 'normal-distribution']"
1941888,Calculate $\int_0^\infty\frac{\sin(x)\log(x)}{x}\mathrm dx$.,"Calculate $\displaystyle\int_0^\infty\dfrac{\sin(x)\log(x)}{x}\mathrm dx$. I tried to expand $\sin(x)$ at zero, or use SI(SinIntegral) function, but it did not work. Besides, I searched the question on math.stackexchange , nothing found. Mathematica tells me the answer is $-\dfrac{\gamma\pi}{2}$, I have no idea how to get it. Thanks for your help!",['integration']
1941923,Differentiating $\sqrt{\frac{1-x}{1+x}}$,I've differentiated the $\frac{1-x}{1+x}$ which is $\frac{-2}{(1+x)^2}$ I don't know how to proceed to it. Hope someone can show it. Thanks in advance.,"['derivatives', 'calculus']"
1941935,Probability of winning a tennis match with a two point difference?,"Let $A$ and $B$ be two tennis players, playing against each other.
A win of the game is considered when one of the two players is leading by two points (e.g. $A$ scores $5$ points vs. $B$ scoring $3$ points).
The probability of a winning a point when $A$ is serving is $\frac{3}{5}$ the probability of a winning a point when $B$ is serving is $\frac{2}{3}$ $A$ serves first and they take turns after each serve. What is the probability of $A$ winning the game. My progress so far: I realise that A wins the whole game when the following sequence occurs: $WLW$ (where $W$ denotes a win and $L$ denotes a loss). However, also a game win can occur when we have $WLLLW$ or $WLLLLLW$ etc. If $A$ wins a point and $B$ wins a point, the score is essentially re-set. From here on, I am unsure how to continue...","['probability-theory', 'probability']"
1941945,Show that if $p$ is prime then $\Bbb Z_p$ is a field,"Check my proof please. Let $\Bbb Z_p:=\Bbb Z/p\Bbb Z$ be the quotient ring modulo $p$ . I want to prove that if $p$ is prime then $\Bbb Z_p$ is a field. Known facts about the quotient rings of the kind $\Bbb Z_n$ (I dont list all the characteristics of a ring, just some ones): They are commutative rings with unity. $[0]=[n]$ is the identity of addition or zero. Then $[n]+[a]=[a]$ and $[n]\cdot[a]=[n]$ for all $[a]\in\Bbb Z_n$ . $[1]$ is the multiplicative identity or unity. Then $[1]\cdot[a]=[a]$ for all $[a]\in\Bbb Z_n$ . The addition is defined as $[a]+[b]=[a+b]$ , and the multiplication is defined as $[a]\cdot[b]=[ab]$ . $[a]=[b]$ means that exists $z\in\Bbb Z$ such that $a=b+nz$ . $|\Bbb Z_p|=p$ . A ring $\Bbb Z_p$ dont have zero divisors , i.e. doesnt exist $[a],[b]\in\Bbb Z_p$ distinct of $[p]$ such that $[a]\cdot[b]=[p]$ . Proof by contradiction: suppose that exists such $[a],[b]$ distinct of $[p]$ that are zero divisors. Then exist some $z_j\in\Bbb Z$ such that: $$(a+pz_1)(b+pz_2)=pz_3\iff ab+pz_4=pz_3\iff ab=pz_5$$ but this is a contradiction with the assumption that $a,b\neq p$ and $p$ prime, so dont exist divisors of zero on any $\Bbb Z_p$ . $\Box$ For any $[a]\in\Bbb Z_p$ distinct of $[p]$ exist $[b]$ such that $[a][b]=[1]$ . Because $\Bbb Z_p$ dont have zero divisors and is finite for some $[a]\neq[p]$ we have that if $[b]\neq[c]$ then $[a][b]\neq[a][c]$ . Proof by contradiction: if $[a][b]=[a][c]$ in the above conditions then $$[a][c]=[ac]=[ab]=[a][b]\iff ac+pz_1=ab+pz_2\iff a(c-b)=pz_3$$ for some $z_j\in\Bbb Z$ . But $pz_3\in[p]$ , then $[a][c-b]=[p]$ but because there are no zero divisors in $\Bbb Z_p$ then or $[a]=[p]$ or $[c-b]=[p]$ what contradicts the above conditions. $\Box$ Because $[a][b]\neq[a][c]$ for $[a]\neq[p]$ and $[b]\neq[c]$ then for any $[a]\neq[p]$ we have that $$[a]\cdot\Bbb Z_p=\Bbb Z_p\implies \exists [b]\in\Bbb Z_p:[a][b]=[1]$$ In other words: every element of $\Bbb Z_p$ but $[p]$ have a multiplicative inverse as stated above. $\Box$ Because $\Bbb Z_p$ is a commutative ring with $[1]\neq[0]$ and have multiplicative inverse for all their elements but $[p]=[0]$ then $\Bbb Z_p$ is a field. $\Box$","['abstract-algebra', 'ring-theory', 'modular-arithmetic', 'solution-verification']"
1941966,Novikov condition for CIR porcess,"I want to apply the Girsanov theorem for change of measure for geometric Brownian motion under real world measure $\mathbb{P}$ to risk-neutral probability measure $\mathbb{Q}$ where the drift is given by stochastic interest rate modelled by CIR process, i.e. to show $dS(t) = S(t) \cdot (\mu dt + \sigma_S dW_S^{\mathbb{P}}(t)) \iff dS(t) = S(t) \cdot (r(t) dt + \sigma_S dW_S^{\mathbb{Q}}(t))$ where $dr(t) = \kappa_r \cdot (\theta_r - r(t))dt + \sigma_r \cdot \sqrt[]{r(t)}dW_r^{\mathbb{P}} (t) \:$ and  $\: W_S^{\mathbb{P}}(t) = \rho_{S,\,r} W_r^{\mathbb{P}}(t) + \sqrt{1-\rho_{S,\,r}^2}  W_{Z_1}^{\mathbb{P}}(t)$. $W_r^{\mathbb{P}}(t)$ and $W_{Z_1}^{\mathbb{P}}(t)$ are independent Brownian motions. Correlation factor fulfills $|\rho_{S,\,r}|\leq 1$. For this I need to show that the Novikov condition is satisfied for $\gamma_S(t)=\dfrac{\mu_S-r(t)}{\sigma_S} $ With help of Can I apply the Girsanov theorem to an Ornstein-Uhlenbeck process? I already showed that $\mathbb{E} \left[ \exp \left (\dfrac{1}{2}\int_s^{s+\varepsilon} \gamma_S^2(t) dt \right) \right] \leq 
\dfrac{1}{\varepsilon} \int_s^{s+\varepsilon} \mathbb{E} \left[ \exp \left (\dfrac{\varepsilon}{2} \gamma_S^2(t)  \right) \right] dt $ Unfortuneately, I have no idea how to find the upper bound for the expectation under the integral in case of CIR process, especially non-central chi squared distribution. Is it possible at all? Any help appreciated!","['stochastic-processes', 'probability-theory', 'finance', 'brownian-motion']"
1941990,Prove that the function is bounded: $f(x) = \frac{1}{x^{2}+1}$,"Prove that the function is bounded: $$f(x) = \frac{1}{x^{2}+1}$$ I'd like to know several (easy) ways of proofing this and I found 2 ways but actually they are so similar, it might just be the same way.. :p Every convergent function is bounded. If function has infimum and supremum then it's bounded. Is it actually bounded if it just has one of both? 1. $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ $$\lim_{x\rightarrow-\infty}\frac{1}{x^{2}+1}= 0$$ $\Rightarrow 0$ is limit of function so it's convergent and thus bounded. But what if we had 2 different values, for $\pm\infty$? 2.Calculate supremum:$$\lim_{x\rightarrow0}\frac{1}{x^{2}+1}= \frac{1}{0+1}=1$$ Calculate infimum: $$\lim_{x\rightarrow\infty}\frac{1}{x^{2}+1}= 0$$ Thus function is bounded. Are both ways correct? If you answer, please answer to all my questions.","['supremum-and-infimum', 'functions', 'calculus', 'convergence-divergence', 'analysis']"
1941994,Derivative of the binomial $\binom x n$ with respect to $x$,"My background is not mathematics and I need to implement (in C++) the derivative of a binomial, with wxMaxima and wolfram.alpha as a helper. So far, the binomial can be written as:
$$\binom x n = \frac 1 {n!}\prod_{k=1}^n (x-k+1)$$
This reduces to a continued convolution. For my specific needs, the binomial needs to be of the form:
$$\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}$$
But I also need the derivative of it, which wxMaxima solves as
$$-\frac{1}{2}(n+1) \,\left( \psi_0\left( \frac{(n+1) x-n+1} 2 \right) -\psi_0 \left( \frac{(n+1) \,(x+1) }{2}\right) \right) \,\begin{pmatrix}\frac{( n+1) x+n-1}{2}\\ n
\end{pmatrix}$$ while wolfram goes a bit further and, instead of $\psi_0$ gives $H_n$, which they call harmonic number . ( this link ). That $\psi$ seems to have quite an involved formula, but $H_n$,as functions.wolfram has it, is a simple $\sum_{k=1}^n 1/k$, which is a lot simpler in terms of C++. Now, because I have trust issues, I went on to verify the answer given by wolfram, in wxMaxima, for $n=4$. Here's the code: n:4$
g:diff(binomial((n+1)/2*(x+1)-1,n),x),expand,numer$
h:-(n+1)/2*binomial((n+1)/2*(x+1)-1,n)*(sum(1/k,k,1,(n+1)/2*(x-1))-sum(1/k,k,1,1/2*(n*(x+1)+x-1)));
wxplot2d([g,h],[x,0,1]); and here's the output of it: plot As you can see, they don't match; plotting wxMaxima's derivation is a match, but that involves $\psi$ as an infinite sum. So I'm left wondering what's wrong: is the implementation of the harmonic number? Is the derivation formula? Is it the way I transcribed it? TL;DR: I need a derivation formula (not the actual code, that's up to me) for the binomial that is (fairly) simple to implement and doesn't take ages to compute, in C++, as the whole function will be called in a bracketed root-finding algorithm. And I'm also using GMP from gmplib dot org (need 10 rep to post more than 2 links). Following G Cab 's excellent post, and modifying the formulas according to my needs, I managed to come up (with a bit of hammering) to this formula: $$\frac{d}{dx}\binom{\frac{n+1}{2}x+\frac{n-1}{2}}{n}=(-1)^n \frac{n+1}{2} \binom{\frac{n+1}{2}x + \frac{n-1}{2}}{n} \sum_{k=0}^{n-1} \frac{1}{\frac{n+1}{2}x +\frac{n-1}{2}-k}$$ The $(-1)^n$ takes care of odd $n$. Thank you very much everyone that answered.","['derivatives', 'binomial-coefficients']"
1942004,Minkowski inequality $-$ Why should it follow from Holder's?,"$$ ||f+g||_p \le ||f||_p + ||g||_p \quad\text{for $1\le p \le \infty$}$$
I have used Minkowski inequality for a long time without really knowing why it should be true. I kind of take it for grant every time I am dealing with $l^p$ or $L^p$ spaces. Years have passed by and now I am taking a better look at it again. It's not like I don't understand its proofs or anything, the standard proof goes like this: Consider the case $1<p<\infty$, it is not hard to see that 
  $$\left|\frac 12f+\frac 12 g\right|^p\le \left|\frac 12|f|+\frac 12|g|\right|^p$$
  for any $f,g\in L^p$. Since $x \mapsto x^p$ is convex, we have
  $$
\left(\frac 12|f|+\frac 12|g|\right)^p \le \frac 12|f|^p+\frac 12|g|^p.
$$
  Substituting $f,g$ in places of $\frac 12f,\frac12 g$ yields
  $$
\left|f+g\right|^p \le 2^{p-1}\left(|f|^p+|g|^p \right)
$$
  so $f+g\in L^p$. Observe that for $q$ satisfying $\frac 1q + \frac 1p=1$,
  $$\begin{align}
|f+g|^p &\le |f+g|^{p-1}(|f|+|g|) \\ 
\int|f+g|^p  &\le \int|f||f+g|^{p-1} + \int|g||f+g|^{p-1} \\
&= ||f\cdot (f+g)^p ||_1 + ||g\cdot (f+g)^p ||_1 \\
||f+g||^p_p&\le ||f||_p ||(f+g)^{p-1}||_q + ||g||_p ||(f+g)^{p-1}||_q
\end{align}$$
  by Holder inequality. Now, since $(p-1)q=p$ we have
  $$\begin{align}
||(f+g)^{p-1}||_q &= \left(\int |f+g|^{(p-1)q}\right)^{\frac 1q} \\
& =\left(\int |f+g|^p\right)^{\frac 1q} \\
&= ||f+g||_p^{p/q}.
\end{align}$$
  By cancellation on both side of $$||f+g||^p_p\le ||f||_p ||(f+g)^{p-1}||_q + ||g||_p ||(f+g)^{p-1}||_q$$ and noting that $p-p/q=1$, we have
  $$ ||f+g||_p \le ||f||_p + ||g||_p $$
  as required. Yes, each step is clear and not hard to understand. However, it seems magical to me at several steps and I cannot intuitively follow the logical flow of the proof. The proof of Holder inequality via Jensen inequality seems quite natural to me, unlike this one. I'll try to be more specific: My questions Why should I expect that the splitting $|f+g|^p$ into $|f+g|^{p-1}(|f|+|g|)$ should be fruitful? We utilized the fact that $(p-1)q=p$ and $p-p/q=1$. While it is not hard to verify them algebraically, is there an intuitive reason to anticipate them beforehand? What is the intuitive/geometric relation between the conjugate pair $p,q$ beside from their algebraic relation $\frac 1q+\frac 1p =1$? I know that $(l^p)^*=l^q$ but I am not sure how it might help. In general, I just want a better grasp at Minkowski inequality. Any contribution would be really appreciated even if it does not address all of my question.","['functional-analysis', 'real-analysis', 'inequality', 'soft-question']"
1942006,Why is it considered that $(\mathrm d x)^2=0$?,"Why is it okay to consider that $(\mathrm d x)^n=0$ for any n greater than $1$?
I can understand that $\mathrm d x$ is infinitesimally small ( but greater than $0$ ) and hence its square or cube should be approximately equal to $0$ not exactly $0$ . But if this is so then how can we expect the results obtained from calculus to be exact and not just approximate ( like the slope or area under a curve )? I have also noticed some anomalies, like $\sqrt{ (\mathrm d x)^2 + (\mathrm d y)^2 }$ is $0$ but $\mathrm d x\sqrt{1+ (\mathrm d y/\mathrm d x)^2 }$ is not $0$ when these two things are apparently the same . Moreover we can  claim that $$(\mathrm d x)^2=(\mathrm d x)^3=(\mathrm d x)^4 = \cdots = 0$$ which is quite hard to believe. Can you help me figure out the logic behind these things ?","['infinitesimals', 'calculus']"
1942064,$(H^2-K) dA$ is globally invariant under inversions,"I am reading a paper by James H. White which states that the Willmore functional is invariant under conformal mappings. Let $M$ be a smooth compact surface and $f:M\to\mathbb{R}^3$ an immersion. The Willmore functional is given by
$$\mathcal{W}(f)=\int_M\;H^2\;dA,$$
where $H$ is the mean curvature and $dA$ is the induced area form. By Liouville's Theorem , all the conformal mappings can be written as combination of euclidean motions, homotheties and inversions in spheres. The fact that $\mathcal{W}$ is invariant under euclidean motions and homotheties is clear. So we have to see how $\mathcal{W}$ behaves under inversions. Since, by Gauss-Bonnet,
$$\int_M (H^2-K)dA=\int_M H^2 dA - 2\pi\chi(M)=\mathcal{W}(f) - 2\pi\chi(M)$$
and $\chi(M)$ is a geometric invariant, we can focus on the quantity $(H^2-K)dA$ and prove what is left for it. So this is what I want to prove: $(H^2-K)dA$ is invariant under inversions. We can assume that the center of the inversion is $0$. If we denote the radius of the inversion by $r$, the position vector of a point on the inverted surface in terms of a point $x\in M$ is given by $$\tilde{x}=r^2{x\over\|x\|^2}.$$ First we observe that the surface form $dA$ changes under the inversion by $d\tilde{A}=\left({r\over\|x\|}\right)^4 dA$. This is easily done. Now, the next step is to compute the principal curvatures of the inverted surface $\tilde{\kappa}_1, \tilde{\kappa}_2$ in terms of $\kappa_1, \kappa_2$. In the mentioned paper, they get the following: $$\tilde{\kappa}_1=-{\|x\|^2\kappa_1-2h\over r^2},\;\tilde{\kappa}_2=-{\|x\|^2\kappa_2-2h\over r^2},$$ where we have set $h=x\cdot N$ with $N$ the surface normal vector. With this calculation the proof is almost done, but I don't know how to get these values for the inverted curvatures. My guess is that I should get first the inverted normal, say $\tilde{N}$, but my calculations get me to $\tilde{N}=\left({r\over\|x\|}\right)^4 N$, which I'm not sure is right. But even if this was the right value of $\tilde{N}$, how could I derive the principal curvatures? Any help is really appreciated.","['curvature', 'invariance', 'conformal-geometry', 'differential-geometry', 'surfaces']"
1942169,"$X$ is a matrix. Find matrix $A, B$ such that $X=AB-BA$","Let $X \in \mathbb{R}^{3\times 3}$ be a diagonal matrix whose diagonal elements (from left to right) are $1$, $y$ and $-1$. For what values of $y$ will there exist matrices $A,B \in \mathbb{R}^{3\times 3}$ such that $AB-BA=X$? So far using the properties of the trace function, I have deduced that $y$ cannot be non-zero, since $AB-BA=X$ implies $\text{tr}(AB-BA)=\text{tr}(X)$. Since $\text{tr}(AB-BA)=0$, the sum of diagonal elements of $X$ is also $0$. Therefore $1+y+(-1)=0$. How do I show that there exist or does not exist $A,B$ such that $AB-BA=X$, where $y=0$. The usual method of computing the product and difference to find the solution seems to be very tedious. I would be happy if someone can suggest a shorter method. Thanks.","['matrices', 'matrix-equations', 'linear-algebra']"
1942199,Why is ${2n \choose n}/2 $ odd if and only if $n=2^k$?,"If we consider the numbers ${2n \choose n}/2 $ for $n$ small: [ 1, 1 ][ 2, 3 ][ 3, 10 ][ 4, 35 ][ 5, 126 ][ 6, 462 ][ 7, 1716 ][ 8,
  6435 ] [ 9, 24310 ][ 10, 92378 ][ 11, 352716 ][ 12, 1352078 ][ 13,
  5200300 ] [ 14, 20058300 ][ 15, 77558760 ][ 16, 300540195 ] we observe that ${2n \choose n}/2 $ is odd if and only if $n$ is a power of $2$. We can easily check with a computer that it is true for $n \le 2^{10}$, so that we can expect it is true in general. Question: What's the proof?","['combinatorics', 'binomial-coefficients']"
1942211,Does negative transpose sign mean inverse of a transposed matrix or transpose of an inverse matrix?,I want to know meaning of $$H^{-T}$$Is it same with $$(H^{-1})^T$$or $$(H^T)^{-1}$$,"['matrices', 'transpose', 'inverse']"
1942222,A goat tied to a corner of a rectangle,"A goat is tied to an external corner of a rectangular shed measuring 4 m by 6 m.  If the goat’s rope is 8 m long, what is the total area, in square meters, in which the goat can graze? Well, it seems like the goat can turn a full circle of radius 8 m, and a rectangular shed's diagonal is less than 8m (actually √52), and so shouldn't it be just 6 x 4 = 24 sq metre? The answer says it is 53 pi, and I have no clue why it is so or why my way of solving doesn't work. Updated:
Oh, and the only area given is that of the shed's. How can I know the full area in which the goat can actually graze on?","['circles', 'rectangles', 'geometry', 'contest-math', 'area']"
1942223,Regular conditional probability as a limit,"The ""Alternate definition"" section of the current version of the Wikipedia article on Regular conditional probability describes an approach to conditional probability as a limiting process, in a vein similar to the intuitive description often encountered in introductory courses in probability, namely
$$
P(A|X=x) = \lim_{h\downarrow 0} \frac{P(A\cap\{X\in(x-h,x+h)\})}{P(X\in(x-h,x+h))}.
$$ Unfortunately, the article does not cite a reference for this alternate definition. The sole reference attached to the article is [1], however, to my best judgment, the alternate definition does not feature there. I would appreciate a reference to a source, such as a published article, a textbook, or class notes, where this alternate definition is presented and studied. It might be that [1] actually does discusses the alternate definition but that I have missed or didn't realize it, in which case I'd appreciate to be stood corrected: where in the article is it brought up? References [1] D. Leao Jr., M. Fragoso, P. Ruffino, Regular Conditional Probability, Disintegration of Probability and Radon Spaces , Proyecciones, Journal of Mathematics, Vol. 23, No 1, pp. 15-29, May 2004. ( pdf )","['reference-request', 'probability-theory', 'probability']"
1942228,Hyperbolic structure on torus,Is there an quick elementary argument of why the standard torus can not be equipped with a hyperbolic structure?,"['differential-geometry', 'geometry']"
1942231,Prove that determinant of a matrix (with polynomial entries) is non-zero,"For $\mathbf x\in\mathbb (0,1)^n$ with $n>2$ and a positive integer $1\le k<n$, define the mapping $f_{i,j}\colon (0,1)^n\rightarrow (0,1)$ by, $$f_{i,j}(\mathbf x)= \sum\limits_{\substack{\mathcal A \subset \{ 1,\ldots,n\}\backslash \{i,j\}\\  
|\mathcal A|=k-1}}\quad\left( \prod\limits_{l\not\in \mathcal A\cup \{i,j\} }x_l\,\,\cdot\!\!\!\! \prod\limits_{ l\in \mathcal A}(1-x_l)\right).$$ Convention: if the set of indexes is empty, the product is one. I'm trying to prove that the matrix $$M=\begin{cases} m_{i,j}=f_{i,j}(\mathbf{x})\quad &\text{ if } j\neq i,\\ m_{i,j}=0&\text{ if } j=i\end{cases}$$ has full-rank, i.e. $\det(M)\neq 0$ (for $\mathbf{x}$ in $(0,1)^n$). I proved this for two particular cases: $k=1$, $f_{i,j}(\mathbf x)= \prod\limits_{ k\neq i,j} x_k$ and so that case is quite easy. Also if $k=n-1$, $f_{i,j}(\mathbf x)= \prod\limits_{ k\neq i,j} (1-x_k)$, which is also an easy case. However, for $1<k<n-1$, I can only solve by using brute force. For example, for $k=2$ and $n=4$, the matrix is: $$\left(\begin{array}{cccc}0 & x_3(1-x_4)+x_4(1-x_3) & x_2(1-x_4)+x_4(1-x_2) & x_2(1-x_3)+x_3(1-x_2) \\
x_3(1-x_4)+x_4(1-x_3) & 0 & x_1(1-x_4)+x_4(1-x_1) & x_1(1-x_3)+x_3(1-x_1) \\
x_2(1-x_4)+x_4(1-x_2) & x_1(1-x_4)+x_4(1-x_1) & 0 & x_1(1-x_2)+x_2(1-x_1) \\x_2(1-x_3)+x_3(1-x_2)  & x_1(1-x_3)+x_3(1-x_1) & x_1(1-x_2)+x_2(1-x_1) & 0\end{array}\right),$$
and using Mathematica, it can be shown that the determinant of $M$ is not zero for $\mathbf x \in (0,1)^4$. But I'm lost trying to tackle the general case: any suggestions/hints (i.e., they need to be full answers) are very welcome. Update : For several examples $-$ if we set $f_{i,j}(\mathbf x,\mathbf y)= \sum\limits_{\substack{\mathcal A \subset \{ 1,\ldots,n\}\backslash \{i,j\}\\  
|\mathcal A|=k-1}}\quad\left( \prod\limits_{l\not\in \mathcal A\cup \{i,j\} }x_l\,\,\cdot\!\!\!\! \prod\limits_{ l\in \mathcal A}y_l\right)$, and compute the determinante of $M$ above. It is easy to see by inspection that all of terms of $\mathrm{det}(M)$ have the same signal. Perhaps, it would be easier to prove the result for this more general $f_{i,j}$.","['combinatorics', 'polynomials', 'groebner-basis', 'determinant']"
1942255,Is there a special name for a field where each number has a square root?,"For example, not every number in the field of rational numbers with ordinary addition and multiplication has square root. Is there a special name for a field where each number has a square root? Can anyone help provide some reference? Thank you!","['terminology', 'abstract-algebra', 'field-theory']"
1942284,Prove that $f(x) = 2x^{2}-3$ is continuous in all of $\mathbb{R}$,"Prove that $f(x) = 2x^{2}-3$ is continuous in all of $\mathbb{R}$ I'd like to use $\varepsilon$-$\delta$-proof for that because I still got some troubles with it. Let $\varepsilon > 0$, let $\delta =-x_{0}+\sqrt{x_{0}^{2}+\frac{\varepsilon}{2}}$ or $\delta =-x_{0}-\sqrt{x_{0}^{2}+\frac{\varepsilon}{2}}$ . If $|x-x_{0}|< \delta$, then $$|2x^{2}-3-(2x_{0}^{2}-3)|=|2x^{2}-3-2x_{0}^{2}+3|$$ $$=|2x^{2}-2x_{0}^{2}|=|2(x^{2} - x_{0}^{2})|$$ $$=|2\left( (x+x_{0}) (x-x_{0}) \right)| < |2((x+x_{0})\cdot \delta)|$$ $$= |2((x+x_{0}+x_{0}-x_{0}) \cdot \delta)|= |2((2x_{0}+x-x_{0}) \cdot \delta)|$$ $$= |2((2x_{0}+\delta) \cdot \delta)|= |2(2x_{0}\delta+\delta^{2})|$$ $$|4x_{0}\delta + 2\delta^{2}| = \varepsilon$$ $\Rightarrow$ $$2\delta^{2}+4x_{0}\delta - \varepsilon = 0$$ $$\delta^{2}+2x_{0}\delta - \frac{\varepsilon}{2}=0$$ $$\delta_{1,2}= -x_{0}+-\sqrt{x_{0}^{2}+\frac{\varepsilon}{2}}$$ $\delta_{1}= -x_{0}+\sqrt{x_{0}^{2}+\frac{\varepsilon}{2}}$ $\delta_{2}= -x_{0}-\sqrt{x_{0}^{2}+\frac{\varepsilon}{2}}$ I'd like to know if I did it right?","['functions', 'calculus', 'continuity', 'epsilon-delta', 'analysis']"
1942319,Lie derivative on $\mathbb{R}^n$,"This is an exercise left for homework, taken from the textbook which we follow in class, Lee's book ''Introduction to Smooth Manifolds'' Second Edition. Definition: $D_vW(p)=\frac{\mathrm{d} }{\mathrm{d} t}|_{t=0} W_{p+tv}=\lim_{t\rightarrow 0}\frac{1}{t}(W_{p+tv}-W_p)$ (*) Suppose $v\in \mathbb{R}^n$ and $W$ is a smooth vector field on an open subset of $\mathbb{R}^n$. Show that the directional derivative $D_vW(p)$ defined by (*) is equal to $(L_VW)_p$, where $V$ is the vector field $V=v^i\frac{\partial }{\partial x^i}$ with constant coefficients in standard coordinates. Any thoughts? Am i supposed to use the theorem which states $L_VW=[V,W]$, or is there a simpler way to show this equality? Thanks in advance for your answers!","['lie-derivative', 'differential-geometry']"
1942335,Irreducible Plane Curves Over the Algebraic $p$-adic Numbers [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question Let $A=\overline{\mathbb{Q}}\cap \mathbb{Q}_p$ be the relative algebraic closure of $\mathbb{Q}$ in the field of $p$-adics. Is there an example of an irreducible plane curve $C$ given by $f(x,y)=0$ for $f(x,y)\in A[x,y]$ such that $C$ has a smooth $\mathbb{Q}_p$-point, but no smooth $A$-point?","['number-theory', 'arithmetic-geometry', 'field-theory', 'algebraic-geometry']"
1942345,"The diameter of the symmetric group generated by the transposition $(1,2)$ and both left and right rotations by $(1,2,\ldots,n)$","I am trying to understand the following sequence: A186783 -The diameter of the symmetric group generated by the transposition $(1,2)$ and both left and right rotations by $(1,2,\ldots,n)$. The sequence is $0,1,2,6,10,15,21,28,35,45,55,66,\ldots$ If we swap out the $2$ with a $3$ we would have the ""triangular numbers"", ${n+1 \choose 2}$. With that exception what would make us believe this sequence is not the ""triangular numbers"".","['reference-request', 'permutations', 'oeis', 'group-theory', 'symmetric-groups']"
1942364,"Given an $n \times n $ grid, how many squares exist?","How many squares exist in an $n \times n$ grid?  There are obviously $n^2$ small squares, and $4$ squares of size $(n-1) \times (n-1)$. How can I go about counting the number of squares of each size?",['combinatorics']
1942396,Show that the cumulative distribution function is uniformly continuous.,"Let $f:\mathbb R→\mathbb R$ s.t. $\int_{-\infty}^\infty |f(x)| \, dx<\infty$ and $F:\mathbb R→\mathbb R$ defined as $F(x)=\int_{-\infty}^x f(t) \, dt$. Then how to prove that $F(x)$ is uniformly continuous? -I have got no idea how to prove it. However, I know the definition of uniform continuity and the sufficient condition of bounded derivative to prove it. As $f(x)$ may be unbounded too, which suggests that $F'(x)=f(x)$ may not bounded in some case thereby restricting me to use the sufficient condition of uniform continuity. Kindly help.","['functional-analysis', 'uniform-continuity', 'functions']"
1942419,What does the centre dot notation mean? $P(\cdot)$,"I think the notation has something to do with probability. I saw the symbol somewhere and I cannot seem to find anything on it.
If I were to read what it looks like, I would say: probability as a function of dot. It looks like this: $P(\cdot)$ Thanks in advance!","['real-analysis', 'probability', 'notation']"
1942430,Prove $\lim_{x\to p}f(x)=0$ iff $\lim_{x\to p}|f(x)|=0$. [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 7 years ago . Improve this question The hint I was given was to use both the /epsilon - /delta definition and the Cauchy Schwarz Inequality.","['multivariable-calculus', 'functions', 'limits']"
1942438,Using Vieta's Formula,"Given that $p$ is a prime, let all solutions to $$x^{p-1}+x^{p-2}+\dots+x+1=0$$ be $x_1, x_2, x_3, \dots, x_{p-1}$ . Find the value of $$\sum_{i=1}^{p-1} (1-x_i)^{1-p}$$ A friend gave me this, and it proved to be more difficult than initially imagined. Vieta's formula seemed ineffective in solving the problem. So I attempted to use that $$\sum_{i=1}^{p-1} \ln (x-x_i) =\ln (x^{p-1}+x^{p-2}+\dots+x+1)$$ And then try to derive both sides. This proved ineffective. So how would one evalue the above sum?","['algebra-precalculus', 'polynomials']"
1942447,Find maximum of $xy$ subject to $(x+1)^2 + y^2 = 4$ without using Calculus techniques,"Find maximum of $xy$ subject to $(x+1)^2 + y^2 = 4$. It is easy if we apply Calculus techniques (e.g., derivatives, Lagrange multipliers, etc). However, the problem is assigned for students who have not yet learned Calculus. Is there a way to find the maximum without using Calculus techniques? I have a few ideas: Apply geometric means: $xy \leq \frac{x^2+y^2}{2} = -x + 3/2 $. Change object function to $(xy)^2$, then we have $(xy)^2 = x^2(4-(x+1)^2)=x^2(-x^2 - 2x +3)$. That is, maximize  $x^2(-x^2 - 2x +3)$ on $(-3,1)$.","['nonlinear-optimization', 'inequality', 'optimization', 'calculus']"
1942472,Can a diffeomorphism between submanifolds be extended to a self-diffeomorphism of total manifold?,"Let $M^{n}$ be a differential manifold and $S$, $N$ submanifolds of $M$ with same dimension $\lt n$, $\phi$ is a diffeomorphism from $S$ to $N$, can $\phi$ be extended to a self-diffeomorphism of $M$? Or does there exist such $\phi$, $\phi$ can be extended? Special case is $M=\mathbb{R}^{n}$, $S$ is an $s$-dimensional submanifold $(s\le n)$,and $\phi$ is a diffeomorphism from $S$ to an open set in $R^{s}$, the question is can $\phi$ be extended to a self-diffeomorphism of $\mathbb{R}^{n}$? If not, does there exist such $\phi$, $\phi$ can be extended?","['differential-topology', 'geometry']"
1942491,Integral where upper limit is a dot - meaning?,"Here's what I'm looking at (from this book , page 14): I get that the dot on top of $B$ is the time derivative - but what's that dot near the top of the integral?","['integration', 'notation']"
1942575,Determining whether a multivariable function is onto,"A problem is asking me to identify whether the following function is onto: $$ R^2 -> R^2$$    $$f(x,y)=(x+y,2x+2y) $$ I understand the definition of onto (each real number solution in a certain domain being corresponded to) and it makes sense enough, when given ordinary linear equations like $$y = x^2 $$ which is not onto, considering no negative values of y can be reached. This can easily be seen graphically.  I'm more confused when given a function of more than one variable, such as the one above or 
$$R^3->R^3 $$
$$f(x,y,z)=(x+y,y+z,x+z) $$ Can I get some help putting these functions in similar terms to the y = 3x case?","['linear-algebra', 'functions']"
1942576,Trivality of tangent bundle and complex structure,"Let $M$ be a smooth manifold of dimensions $2n$, and suppose 
$$TM \equiv M \times \mathbb R^{2n}. $$ Does it follow that $M$ admits a complex structure? It seems like the almost complex structure defined by taking the standard (after choosing a global frame $\{ v_1, \dots, v_n\}$) one at each point on $TM$ $p \in M$: $$J_p: T_pM=\mathbb R^{2n} \rightarrow T_pM=\mathbb R^{2n},$$
given by
$$J_p = \begin{bmatrix}
    0 & I \\
    -I & 0
\end{bmatrix}$$
is a good candidate for a complex structure. But how can we know if this is integrable? I am aware of the Newlander–Nirenberg theorem, but am wondering if there are other ways to know if I can't compute that. Thanks!","['complex-geometry', 'differential-geometry']"
1942580,$K[X]$-modules are $K$-vector spaces with a linear transformation,"In the Atiyah's book there is this example about $A$ -modules. Let $A=\mathbb{K}[x]$ , where $\mathbb{K}$ is a field. An $A$ -module is a $\mathbb{K}$ -vector space with a linear transformation. Can someone explain this claim to me?","['abstract-algebra', 'modules']"
1942586,Existence of measurable set $A\subseteq{\mathbb R}$ which is locally uncountable and so is its complement,Is there a measurable set $A\subseteq{\mathbb R}$ such that $|A\cap I|$ and $|A^\complement\cap I|$ are both uncountable for any open interval $I$?,"['lebesgue-measure', 'measure-theory']"
1942597,How to prove the existence of this $A$ such that $\mu(A)=0$ under these conditions?,"Let $(X, \sum, \mu)$ be a measurable space. Supose that $f_{n}:X \rightarrow \mathbb{R}\cup\{\infty\}\cup \{-\infty\}$ are non-negatives and measurables functions for all $n \in \mathbb{N}$ . If $$\lim_{n \to \infty} \int_{X} f_{n} d\mu = 0,$$ prove that exist a set $A \in \sum$ such that $\mu(A) = 0$ and $$\lim_{n \to \infty} f_n(x) =0$$ for all $x \in A^{c}$ . I was trying to use $$\int_{X} f_{n} d\mu = \sup \{\int_{X} \phi(x) d\mu, \phi \in F_{n}\}$$ where $F_{n}$ its a set where $\phi$ are simples, measurables, non-negatives and $\phi(x) \leq f_{n}(x)$ for all $x \in X$ , but I get nowhere.","['measure-theory', 'functions']"
1942599,Calculating double sum $\sum_{i=-1}^{5}\sum_{j = -5}^{1}(1 + 2i + 3j + 4ij)$,"How can I calculate sums of the form:
$$\sum_{i=-1}^{5}\sum_{j = -5}^{1}(1 + 2i + 3j + 4ij)?$$ Shall I calculate it like first for $i = -1, j = \{-5,\dots, 1\}$, then for $i = 2, j = \{-5, \dots, 1\}$? Is there some shortcut for calculating those kinds of sums, i.e. something akin to summation of arithmetic series?","['algebra-precalculus', 'summation']"
1942602,Does $S^1 \times S^3$ admit a complex structure? [duplicate],"This question already has answers here : Complex structure on $\mathbb S^1 \times \mathbb S^3$ (1 answer) Why $S^1\times S^{2m-1}$ carries a complex structure. (2 answers) Closed 7 years ago . Does $S^1 \times S^3$ admit a complex structure? It is parallelizable, so it admits an almost complex structure...is there a nice way to see that this is or is not integrable?","['complex-geometry', 'differential-geometry', 'differential-topology']"
1942615,Divergence of $\sum_n e^{n(\sin n -1)}$,"Does $\sum_n e^{n(\sin n -1)}$ converge ? I'm pretty sure this series diverges because it seems $n(\sin n -1)$ is ""close"" to $0$ infinitely often. Nevertheless I lack skills in diophantine approximation to prove that... Plot of $- n(\sin n -1)$ for $1\leq n\leq 500$. Dents in the blue background are small values. Can someone point me in the right direction ? Context : some textbook question asks for the radius of $\sum_{n} e^{n\sin n}z^n$. It's easy to prove that the answer is $\frac 1e$, and I'd like to know what happens at $z=\frac 1e$.","['diophantine-approximation', 'sequences-and-series']"
1942618,Balls and Bins Variation (Multiple Rounds),"Consider the following balls and bins experiment: we repeatedly throw a fixed number of balls randomly into a shrinking set of bins. The experiment starts with $n$ balls and $n$ bins. In each round $k,$ we throw $n$ balls into the remaining bins, and then discard any non-empty
bins; thus, only bins that are empty at the end of round $k$ survive to round $k + 1$. What is the expected number of rounds before every bin among the initial $n$ is nonempty? In general, assuming we are conducting the standard balls-and-bins experiment, then given $m$ balls and $n$ bins, we know that the expected number of bins that are empty is $n(1 - \frac{1}{n})^m$. With this in mind, I've attempted to consider each round in the above experiment as an instance of the standard balls into bins problem, and have reformulated the question into an equivalent one, thus: Suppose that for each round, exactly the expected number of bins are empty. Then after how many rounds does the experiment end? However, the expected number formula quickly gets unwieldy. But consider the function defined for any integer $i \geq 0$: $f(i+1) = 2^{f(i)}$ for $i \geq 1$ and $f(0) = 1$ Could I use an induction argument to show that, after round $i$ and before round $i + 1$ (assuming that exactly the expected number of bins are empty for each round), we have that $\frac{n}{f(i+1)} \leq \textit{number of empty bins} < \frac{n}{f(i)}$ and try to tightly bound the solution from above?","['balls-in-bins', 'expectation', 'probability']"
1942621,"What is the odd that a data set is distributed according to a Uniform 0,1?","I have a data set consisting of values in between 0 and 1 and I would like to have a statistic that help me understand if the data is really uniformly (evenly) distributed. I can't find anything so i'm actually using some custom tests - 
Given a dataset of n points, 
I'm calculating the 'expected distance' between 2 subsequent point (100/n) and I compute the square of the difference between the effective distance of two subsequent ordered point and the expected distance. it's calibrated using my own perception of what's uniformly distributed and what's not. Anybody has something better?","['statistics', 'probability']"
1942634,Prove the subdifferential of $f(x)=\max_i f_i(x)$ is $\partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)\right) $,"How to prove that the subdifferential of $f(x) = \max_{i=1,\dots, n} f_i(x)$ satisfies
        \begin{align}
        \partial f(x) = \operatorname{conv}\left( \bigcup \partial f_i(x)  \right)
    \end{align} How am I suppose to utilize the property of convex hull here?","['subgradient', 'real-analysis', 'convex-analysis']"
1942683,Pacman on a Mobius Strip,"Pacman lives on the 2D cylinder, $S^1 \times I$, where $I \subset \mathbb{R}^1$ is an interval of the real line. So that we can play the game on a flat surface, we portray Pacman's world on a fundamental domain, a rectangle, and identify two boundary edges in the appropriate way. Let's say the width of the screen is $2\pi$. Suppose instead, Pacman lived on the Mobius strip. Would we need to double the screen width to $4\pi$? Or would that introduce a 2-1 redundancy in configurations on the Mobius band? In other words, is a reversed orientation at the same place considered a different configuration? Has his world gotten twice as big, or is it the same size with peculiar properties? Would we see Pacman reverse or would we see his world be reversed? For example, if Pacman had a mole on his right cheek while traveling rightward, would we now only see the mole on his left cheek while traveling leftward in the section of the screen that is $2\pi < \theta < 4\pi$? Help me understand Pacman on the Mobius strip. Thank you.",['general-topology']
1942716,Show that $\sum^{+\infty}_{k=1} \sqrt[k]k-1$ diverges,"Show that $\sum^{+\infty}_{k=1} \sqrt[k]k-1$ diverges. I was advised to investigate the proof of $\sqrt[k]k \rightarrow 1$ to get inspiration. If this is true, why does the sum not converge to $0$?","['real-analysis', 'sequences-and-series']"
1942790,What is the set of triangular numbers mod $n$?,"My question is motivated by the following fact (for a couple of proofs see my answer at MSE 1926967 ; note that this fact also answers the $\ell = 1$ case of MO 265513 ): Fact. The set of triangular numbers modulo $n$ yields $\mathbb{Z}/n\mathbb{Z}$ iff $n$ is a power of $2$. My specific question is whether there is a straightforward way to describe this set given, for example, the prime factorization of $n$. But I also have the broader question: What else can be said about the set of triangular numbers modulo $n$ as a function of $n$? For example, when $n$ is odd, we might expect the set to have cardinality around $(n+1)/2$. To illustrate this by way of example, consider $n = 11$, and the first $11$ counting numbers each reduced modulo $11$: $$1, 2, 3, 4, 5, -5, -4, -3, -2, -1, 0$$ However many distinct elements we have after the first $(n+1)/2$ triangular numbers will be the final total, since (as suggested by the negative notation above) the elements reverse course thereafter, and wend their way back to $0$ at which point they reset as they have formed a cycle. But I am not only interested in an estimate of the cardinality based on $n$'s parity; I am hoping for the whole shebang! In that spirit: Q. How can we describe the set of triangular numbers modulo $n$ given $n$'s prime factorization?","['reference-request', 'number-theory', 'group-theory', 'modular-arithmetic', 'elementary-number-theory']"
1942806,Confusion: line bundles over $\mathbb{P}^1$ and sheaves,"I understand the notion of a line bundle $L \to X$ where $X$ is the base. Locally at some $U \subset X$ we have that $L = U \times \mathbb{C}$ (if we work over the complex numbers). Over the projective space it is clear that at every point we have an additional structure, that of a line bundle, where each point of $\mathbb{P}(X)$ has as a fiber the line that it represents. On the other hand I also understand the notion of the structure sheaf $\mathcal{O}_X$ and Serre's twisting sheaf $\mathcal{O}_X(d)$ in terms of regular functions over $U$ and in terms of rational functions of degree $d$ over $U$ respectively. What I struggle to understand is how to understand the connection between the sections of the line bundle (for $\mathbb{P}^1$ what are those section?) with the sections of sheaves. In specific how are polynomials of degree $d$ of $\mathcal{O}(d)$ related to the line over a point of the projective space, say $\mathbb{P}^1$?","['fiber-bundles', 'sheaf-theory', 'complex-geometry', 'algebraic-geometry', 'vector-bundles']"
1942810,Does every even-dimensional Lie group admit a complex structure?,"Let $M$ be an even-dimensional Lie group. Is it true that $M$ admits a complex structure? I can see that there exists an almost complex structure, since the tangent bundle is trivial. But this is not sufficient Trivality of tangent bundle and complex structure . So I'm wondering if adding the Lie group structure is enough to guarantee integrable. Maybe the Nijenhuis tensor simplifies in some way?","['complex-geometry', 'differential-geometry', 'lie-groups']"
1942814,Stratonovich integral and chain rule,"From Wiki : if $f:\mathbb{R}\times\mathbb{R} \rightarrow \mathbb{R}$ is a smooth function, then
  $$\int\limits_0^T \frac{\partial f}{\partial W}(W_t,t)\circ \mathrm{d}W_t + \int\limits_0^T \frac{\partial f}{\partial t}(W_t,t)\mathrm{d}t = f(W_T,T) - f(W_0,0)$$
  which is akin to the chain rule of ordinary calculus. How is that akin to the chain rule? I can't see how that relates to $(f\circ g)' = (f'\circ g)\cdot g'$.","['integration', 'chain-rule', 'stochastic-integrals']"
1942823,Is $x * d/dx$ closed on $L^2(\mathbb{R})$?,"More formally, sticking to the one-dimensional case for this problem, let $M_x$ be the multiplication-by-$x$ operator, $$\begin{split} \mathfrak{D}(M_x) &= \lbrace f(x) \in L^2(\mathbb{R}): x f(x) \in L^2(\mathbb{R}) \rbrace \\
M_x [f](x) &= x f(x) \end{split},$$ and let $D$ be the differentiation-by-$x$ operator
$$ \begin{split} \mathfrak{D}(D) & = \lbrace f(x) \in L^2(\mathbb{R}): f^{\prime}(x) \in L^2(\mathbb{R}) \rbrace ( = \mathcal{H}^1(\mathbb{R}))\\
D[f](x) &= f^{\prime}(x) \end{split},$$
where we allow $f^{\prime}(x)$ to be a distributional derivative.  (Of course, thanks to the Fourier Transform $\mathcal{F}$, $$Df(x)  = \mathcal{F}^{-1} \left[i \xi \mathcal{F}[f](\xi) \right](x),$$
perhaps multiplied by a constant depending on the convention on the Fourier Transform.)  Of course, $M_x$ is closed and self-adjoint (e.g., Reed/Simon, Functional Analysis, Chap. VIII, Section 3, Prop. 1), and $D$ is closed by the same reasoning (being a constant multiple of the self-adjoint $i \frac{d}{dx}$). Question: Is $M_x \circ D$ closed on the natural domain, i.e., $ \mathfrak{D}(M_X \circ D) = \lbrace f(x) \in L^2(\mathbb{R}): f^{\prime}(x) \in L^2(\mathbb{R}), xf^{\prime}(x) \in L^2(\mathbb{R}) \rbrace$ ? Note: Of course, replacing $\frac{d}{dx}$ by $i \frac{d}{dx}$ may be simpler to allow self-adjointness, but I prefer to state it in the more elementary way.  Of course, if the proof is easier with $i \frac{d}{dx}$, then please just use that. Work so far: Let us set up a standard closure proof, so suppose that $f_n(x) \in \mathfrak{D}(M_X \circ D)$ and $f(x) \in L^2(\mathbb{R})$ satisfy \begin{align} f_n(x) &\overset{L^2}{\rightarrow} f(x) \tag{1}\\
x f_n^{\prime}(x) &\overset{L^2}{\rightarrow} g(x);  \tag{2} \end{align}
we wish to show that $f(x) \in \mathfrak{D}(M_X \circ D)$ and $x f_n^{\prime}(x) \overset{L^2}{\rightarrow} x f^{\prime}(x)$. Claim: It suffices to show that 
$$f_n^{\prime}(x) \overset{L^2}{\rightarrow} h(x) \tag{3} $$ for some $L^2$-function $h$. Proof: Suppose that (1), (2), and (3) hold (for some $L^2$ functions $\left(f_n\right)_{n = 1}^{\infty}$, $f$, $g$, and $h$).  By (1), (3), and the closure of $D$, it follows that $f(x) \in \mathfrak{D}(D)$ and $f_n^{\prime}(x) \overset{L^2}{\rightarrow} f^{\prime}(x)$, i.e., $h(x) = f^{\prime}(x)$.  Letting $h_n(x) = f_n^{\prime}(x)$, (3) now reads
$$h_n(x) \overset{L^2}{\rightarrow} h(x), \tag{3*}$$
and (2) now reads
$$x h_n(x) \overset{L^2}{\rightarrow} g(x), \tag{2*}$$
so by the closure of $M_x$, $h \in \mathfrak{D}(M_x)$ and $x h_n(x) \overset{L^2}{\rightarrow} x h(x)$, i.e., $g(x) = x h(x)$.  Remembering that $h_n(x) = f_n^{\prime}(x)$ and $h(x) = f^{\prime}(x)$, we have that $f^{\prime}(x) \in \mathfrak{D}(M_x)$, i.e., $f(x) \in \mathfrak{D}(M_x \circ D)$, and $x f_n^{\prime}(x) \overset{L^2}{\rightarrow} x f^{\prime}(x)$.  The closure proof is complete. $\square$ Thus, the problem reduces to proving the claim.  Certainly, for any $\delta > 0$, we can define the bounded measurable function $$\varphi_{\delta}(x) = \begin{cases} \frac{1}{x}, & |x| \geq \delta \\ 0, & |x| < \delta \end{cases},$$
which as a multiplier of course defines a bounded (hence continuous) linear operator on $L^2(\mathbb{R})$, so applying to (2),
$$\varphi_{\delta}(x) \cdot x f_n^{\prime}(x) \overset{L^2}{\rightarrow} \varphi_{\delta}(x) g(x). $$ Of course, $\varphi_{\delta}(x) \cdot x $ is the characteristic function $\mathbb{1}_{|x| \geq \delta}(x)$, so $$ \mathbb{1}_{|x| \geq \delta}(x) \cdot f_n^{\prime}(x) \overset{L^2}{\rightarrow} \varphi_{\delta}(x) g(x). \tag{4} $$ Thus, we seem to have two options: (A) show for some small $\delta$ that $\mathbb{1}_{|x| < \delta}(x) f_n^{\prime}(x)$ also converges to some $L^2$ function, (B) carefully take the limit in (4) as $\delta \to 0$. [I regard (B) as nonobvious: clearly as $\delta \to 0$, for each $n$ separately, the left-hand side tends to $f_n^{\prime}(x)$ in $L^2$-norm, but without any uniformity in $n$, it would be difficult.  Certainly, the right-hand-side will get worse as $\delta \to 0$, without an additional observation.]  Here is where I am stuck.","['functional-analysis', 'differential-operators', 'operator-theory']"
1942857,How can this integral expression for the difference between two $\zeta(s)$s be explained?,"With $z=\sigma + x \, i;\,\, \sigma,x \in \mathbb{R}$, numerical evidence strongly suggests that: $$\displaystyle \int_{0}^{\infty} \,\zeta(z)\,\zeta(\overline{z})\,\Gamma(z)\,\Gamma(\overline{z})  \,dx =\pi\,\Gamma(2\,\sigma)\,\big(\zeta(2\,\sigma-1)-\zeta(2\,\sigma)\big)$$ for all $\sigma > 1$. Could this be proven? Just to share that the equation can be extended towards $0<\sigma<1$ by starting from: $$\Gamma(s)\, \zeta(s) = \int_0^\infty x^{s-1}\left(\frac{1}{e^{x}-1}-\frac{1}{x}\right)\,dx$$ and following the same logic as in the answer below. This gives the closed form: $$\displaystyle \int_{0}^{\infty} \,\zeta(z)\,\zeta(\overline{z})\,\Gamma(z)\,\Gamma(\overline{z})  \,dx =\pi\,\Gamma(2\,\sigma)\,\left(\frac{(2\,\sigma-3)}{(2\,\sigma-1)} \,\zeta(2\,\sigma-1)-\zeta(2\,\sigma)\right)$$ Note that since $\sigma$ could also be $\in \mathbb{C}$, this implies that when $\rho=2\,\sigma$ or $\rho=2\,\sigma-1$ ($\rho$ = a non-trivial zero of $\zeta(s)$), then the RHS becomes fully multiplicative. The process can be extended indefinitely by for instance starting from $$\Gamma(s)\, \zeta(s) = \int_0^\infty x^{s-1}\left(\frac{1}{e^{x}-1}-\frac{1}{x} +\frac12\right)\,dx$$
which gives for the domain $-1<\sigma<0$ the following closed form: $$\displaystyle \int_{0}^{\infty} \,\zeta(z)\,\zeta(\overline{z})\,\Gamma(z)\,\Gamma(\overline{z})  \,dx =\pi\,\left(2\,\sigma-3)\,\Gamma(2\,\sigma-1)\,\zeta(2\,\sigma-1\right)$$ And subsequently for the domain $-2<\sigma<-1$ we start from:
$$\Gamma(s)\, \zeta(s) = \int_0^\infty x^{s-1}\left(\frac{1}{e^{x}-1}-\frac{1}{x} +\frac12-\frac{x}{12}\right)\,dx$$ which gives the closed form: $$\displaystyle \int_{0}^{\infty} \,\zeta(z)\,\zeta(\overline{z})\,\Gamma(z)\,\Gamma(\overline{z})  \,dx =\pi\,\Gamma(2\,\sigma)\,\left(\frac{(2\,\sigma-3)}{(2\,\sigma-1)} \,\zeta(2\,\sigma-1)-\frac{\sigma}{3}\,\zeta(2\,\sigma+1)\right)$$ Don't think there is an easy pattern or generic formula since Bernoulli numbers are involved.","['number-theory', 'mellin-transform', 'riemann-zeta', 'gamma-function']"
1942859,Proving $\phi$ is convex if and only if $\phi'(x) \leq \phi'(y)$ for all $a < x < y < b$,"I'm an undergraduate student that find a problem for proofing theorem below. First it defined that: A function f defined on an interval (a,b), $-∞ ≤ a < b≤ ∞$, is said to be a convex function if for all x,y in (a,b) and for all $0 < γ < 1$, $$f[γx + (1 - γ)y] ≤ γf(x) + (1 - γ)f(y)$$ 
We say f is strictly convex if the above inequality is strict. The theorem said that, if f is differentiable on (a,b), then 1) f is convex if and only if $f' (x) ≤ f'(y)$, for all $a < x < y < b$, 2) f is strictly convex if and only if $f'(x) < f'(y)$, for all $a < x < y < b$. I try to proof the first part from the left to the right, in this way: a) Let f is differentiable on (a,b), and f is convex. Let $γ=\frac{(k-j)}{(k-x)}$. The value of γ is arbitrary value such that $0<γ<1$. Choosing x,j,k and y, such that $a < x <j<k< y < b$,
and $j=\frac{(k-j)}{(k-x)} x+\frac{(j-x)}{(k-x)}k$, 
so, by the  definiton of convex 
$$f(j)=f\left(\frac{(k-j)}{(k-x)} x+\frac{(j-x)}{(k-x)} k\right)≤\frac{(k-j)}{(k-x)} f(x)+\frac{(j-x)}{(k-x)} f(k)$$
Then $(k-x)f(j)≤(k-j)f(x)+(j-x)f(k)$ $((k-j)+(j-x) )f(j)≤(k-j)f(x)+(j-x)f(k)$
$(k-j)f(j)+(j-x)f(j)≤(k-j)f(x)+(j-x)f(k)$
$(k-j)f(j)-(k-j)f(x)≤(j-x)f(k)-(j-x)f(j)$
$(k-j)[f(j)-f(x) ]≤(j-x)[f(k)-f(j) ]$ $$\frac{[f(j)-f(x) ]}{(j-x)}≤\frac{[f(k)-f(j) ]}{(k-j)}$$
Doing the same step for j,k and  y, yields
$$\frac{[f(k)-f(j) ]}{(k-j)}≤\frac{[f(y)-f(k) ]}{(y-k)}$$
So, we get
$$\frac{[f(j)-f(x) ]}{(j-x)}≤\frac{[f(k)-f(j) ]}{(k-j)}≤\frac{[f(y)-f(k) ]}{(y-k)}$$
$$\frac{[f(j)-f(x) ]}{(j-x)}≤\frac{[f(y)-f(k) ]}{(y-k)}$$ and the last step i was taking the limit j→x for the left side and k→y for the right one, yields $ f' (x)=lim_{j→x}⁡\left(\frac{[f(j)-f(x) ]}{(j-x)}\right)≤lim_{k→y}⁡\left(\frac{[f(y)-f(k) ]}{(y-k)}\right)=f' (y) $ is it reasonable to doing this? i have searching for the proof in some reference but i haven't find a clear explanation yet. ones said that the last step 
 you should take the limit $j→x^+$ and $k→y^-$, but i didn't understand what the reason. I find this problem in Robert V. Hogg math statistic book. The right reference should be analysis book from Hewitt-Stromberg but i didn't find it in my library.","['statistics', 'convex-analysis']"
1942874,Consider infinity in codomain of rational function if field is algebraically closed,"In Example 2.3.4 on pg. 43 of Algebraic Geometry and Arithmetic Curves by Qing Liu, it says If $k$ is algebraically closed, we can consider a rational function as a function $k \rightarrow k \cup \{+\infty\}$. Why is it important that the field be algebraically closed?",['algebraic-geometry']
1942943,Algebra transitive action (Isaacs 4.1),"I'm working on the following questions and I'm unable to make any progress on it. Can anyone offer any insight? Let $G$ act transitively on a set $\Omega$ and suppose $G$ is finite.  Define an action of $G$ on $\Omega \times \Omega$ by putting $(\alpha, \beta) \cdot g =(\alpha\cdot g, \beta\cdot g)$. Let $\alpha \in \Omega$. Show that $G$ has the same number of orbits on $\Omega \times \Omega$ as $G_{\alpha}$ does on $\Omega$.","['finite-groups', 'abstract-algebra', 'group-theory', 'group-actions']"
1942946,Calculating this limit: $\lim_{n\to\infty}\;n\cdot\sqrt{\frac{1}{2}\left(1-\cos\frac{360^\circ}{n}\right)}$,"$$\lim_{n\to\infty}\;n\cdot\sqrt{\frac{1}{2}\left(1-\cos\frac{360^\circ}{n}\right)}$$ I need help with this as I cannot figure out how to calculate the limit value for this function as $n$ approaches infinity. I know this is an irrational function, but I'm not able to come to an answer. If I graph this function, there are 2 horizontal asymptotes (positive and negative) but both are irrational.","['trigonometry', 'calculus', 'limits']"
1942960,Prove or disprove that there is a rational number $x$ and an irrational number $y$ such that $x^y$ is irrational,"If $x$ is a rational number and $y$ is an irrational number then $x^y$ is irrational. I have tried to prove it, let $x= 2$ and $y=\sqrt{2}$ then $x^y = 2^{\sqrt{2}}$, if it is an rational number then let $x=2^{\sqrt{2}}$ and $y=\sqrt{2}$, then, $x^y= 2^{\sqrt{2} *\sqrt{2}}=4$ which is rational, therefore $x^y$ is rational. Is it correct?","['irrational-numbers', 'discrete-mathematics']"
1942965,Different proof for $W \le V \to \dim(W) \le \dim(V)$,"Larson Edwards Falvo - Elementary Linear Algebra Different approach: Let $\dim(V) = n$. If $\dim(W) > n$, then a basis for $W$ has more than $n$ vectors, contradicting either of the theorems below. QED Is that wrong? Am I doing some kind of catch-22 here?","['vector-spaces', 'multivariable-calculus', 'linear-algebra', 'vectors', 'vector-analysis']"
1942980,"Two definitions of Lebesgue integral, are they equivalent?","So far, I have encountered two main definitions for the Lebesgue integral of a non-negative measurable function $f$. 1) $\int_A f d \mu = \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ 2) $\int_A f d \mu = \sup \{ \inf _{x_i \in E_i} \sum_{i=1}^N  f(x_i)
\mu(E_i) \} = \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i)
\}$ where in 2), the $E_i$'s form a partition of A and the sup is taken
  over all such partitions. I want to prove that they are equivalent. Here is what I did. 1) $\leq$ 2) : take $h$ a simple function such that $h \leq f$. Write $h$ in its canonical form : $h = \sum_i c_i \mathcal{1}_{A_i}$. Since $h \leq f$, we have $c_i \leq f(x_i)$ for all $x_i \in A_i$. Using the definition of the Lebesgue integral for simple functions, we then have $\int_A h d \mu = \sum_i c_i \mu (A_i) \leq \sum_{i=1}^N (\inf _{x_i \in A_i} f(x_i)) \mu(A_i) \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$, from which we have $\sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} \leq \sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \}$. 2) $\leq$ 1) : let $E_i$'s be a partition of A, and $\alpha = \sum_i \inf_{x_i \in E_i} \{f(x_i) \} \mu (E_i)$. Fix $\epsilon > 0$. We can choose ($x_i^*)_i$ such that $\inf f(x_i) \leq f(x_i^*) \leq \inf f(x_i) + \epsilon$. We have $\alpha \leq \sum _i f(x_i^*) \mu (E_i)$. We set $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i}$. Then $h^* = \sum _i (f(x_i^*) - \epsilon) \mathcal{1}_{E_i} \leq \sum _i (\inf_{x_i \in E_i} f(x_i)) \mathcal{1}_{E_i} \leq f$. We also have $\int_A h^* d \mu = \sum_i (f(x_i^*) - \epsilon) \mu (E_i) = \sum_i (f(x_i^*) \mu (E_i) - \epsilon \mu (A) $. 
So $\alpha \leq \int_A h^* d \mu + \epsilon \mu(A) \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \} + \epsilon \mu (A)$.
Take $\epsilon \to 0$ so that $\alpha \leq \sup _{h \leq f, \space h \space simple} \{ \int_A h d \mu \}$ and finally $\sup \{ \sum_{i=1}^N (\inf _{x_i \in E_i} f(x_i)) \mu(E_i) \} \leq  \sup _{h \leq f, h simple} \{ \int_A h d \mu \}$. Is it correct ? Can ce generalize the second part to the case $\mu(A) = \infty$ ? If this is correct, in which case the definition 2) is preferably used ?
Thanks.","['real-analysis', 'integration', 'lebesgue-integral', 'measure-theory', 'analysis']"
1942983,Find $\int_0^{2\pi}\frac1{5-4\cos x}\ dx$,"$$\int_0^{2\pi}\frac1{5-4\cos x}\ dx$$
How do I compute this integral? An online integral calculator gives an antiderivative as
$$\frac{2\arctan(3\tan\frac x2)}3$$
but then gives the definite integral as $\frac{2\pi}3$. Obviously this doesn't make sense as the antiderivative vanishes at $x=0$ and $x=2\pi$.","['integration', 'definite-integrals', 'trigonometric-integrals', 'calculus']"
1942994,Confusion about Positive Curvature in Holomorphic Bundles.,"I'm trying to understand the principle that curvature decreases in holomorphic subbundles and increases in quotient bundles as shown in G-H (Griffiths Harris) page 78-79. Setup: Let $E\rightarrow M$ be a Hermitian holomorphic bundle, $S\subset E$ be a holomorphic subbundle and $Q=E/S$ be the quotient bundle. Q can be seen in  unitary frames to be diffeomorphic to $S^{\perp}$. Hence $S$ and $Q$ are both Hermitian. Let $D_E$, $D_S$ and $D_Q$ be the connections compatible with the complex structures (Compatibility with the metric and vanishing on holomorphic sections). By uniqueness of such connections, $D_S=\pi_S\circ (D_E|_{A^0(S)})$ where $A^0(S)$ are sections of $S$. Now $A:=D_E|_{A^0(S)} -D_S$ is $C^\infty (M)$ linear and maps to sections in $S^{\perp}=Q$ with coefficients in $A^{(1,0)}(M)$ (Because of the definition of a connection and the compatibility conditions on $D_E$ and $D_S$). In short, $A\in A^{(1,0)}(Hom(S,Q))$. Now, as in G-H, choosing a unitary frame ${e_i}$ for E (the first few vectors of which form a frame for $S$) we see that the matrix of forms for $D_E$ is $\theta_E = \begin{bmatrix} \theta_S & A^* \\ A & \theta_Q \end{bmatrix}$. Remark 1: The above formula is incorrect since in a unitary frame $\theta_E = -\theta_E^*$. So we should have a $-$ sign along with the $A^*$. Question 1: Is this correct? Assuming this, the correct formulas are: $\theta_E = \begin{bmatrix} \theta_S & -A^* \\ A & \theta_Q \end{bmatrix}$, $\Theta_S=\Theta_E|_{S}-A^*\wedge A$, $\Theta_Q=\Theta_E|_Q-A\wedge A^*$. These are derived from $\Theta = d\theta-\theta\wedge\theta$. Definition: G-H defines $\Theta$ to be positive if for all $v$ - holomorphic tangent vectors, the matrix $-i(\Theta(x);v,\bar v)$ is positive definite, in particular hermitian. Here, I assume we are fixing a local unitary frame, evaluating the coefficients of (1,1)-forms of $\Theta(x)$ on $(v,\bar v)$ and then looking at the resulting matrix in $Hom(E_x,E_x)$. Claim 1: is that $\Theta_Q \geq \Theta_E|_Q$, i.e. curvature increases in quotient bundles. Using the corrected formula, we want to show $-A\wedge A^*$ is positive. Note, a computation shows (by definition?) $-A\wedge A^*$ is in $A^{(1,1)}(Hom(Q_x,Q_x))$ and the $(p,q)$ entry is $-\sum_{\alpha,\beta} \sum_k a_{pk}^\alpha \bar{a}_{qk}^\beta dz_\alpha\wedge d\bar{z}_\beta$. And following G-H, contracting with $(\partial{z_\alpha},\partial{\bar z_\alpha})$ gives the matrix $-A^\alpha {A^*}^\alpha$ in $Hom(Q_x,Q_x)$ and they conclude that this proves positivity. Question 2: Since I am Un Idiota and the most I can do is follow instructions, I am assuming they are checking (as required by the definition) that $-i(-A^\alpha {A^*}^\alpha)$ is hermitian positive definite. But clearly the $i$ is spoiling things. Can I drop it from the definition? What am I missing? Positivity in other sources (e.g. https://www-fourier.ujf-grenoble.fr/~demailly/manuscripts/agbook.pdf defn 6.3 on page 338) have neither the $i$ nor the $-$ sign. But I need the $-$ sign so I can't drop both! Is there some standard viewpoint on the definition of positivity (Hopefully one that would fit consistently into this argument)? Question 3: Why is it sufficient to check the positivity condition on vector pairs of the form $(\partial z_k,\partial \bar z_k)$? If I try to contract with $(\sum_{l} c_l \partial z_l, \sum_k \bar c_k \partial \bar z_k)$, I get a matrix in $Hom(Q_x,Q_x)$ I get something far uglier which doesn't seem to be of the form $MM^*$ Thank you for reading. I have 80 Rep now! Will put Bounty when it's eligible.","['complex-geometry', 'algebraic-geometry', 'holomorphic-bundles', 'vector-bundles', 'differential-geometry']"
1943008,Find the points on the ellipsoid $x^2 + 2y^2 + 3z^2 = 1$ where the tangent plane is parallel to the plane $3x - y + 3z = 1$.,"Find the points on the ellipsoid $x^2 + 2y^2 + 3z^2 = 1$, where the tangent plane is parallel to the plane $3x - y + 3z = 1$. I'm not sure how to go about solving this. I'd appreciate some help.",['multivariable-calculus']
1943050,Manifold without point homotopy equivalent to wedge of $2$-spheres.,"Let $F$ be a closed, simply connected four-manifold. Let $F'$ be obtained from $F$ by removing a point. How do I see that $F'$ is homotopy equivalent to a wedge of $S^2$'s?","['homology-cohomology', 'homotopy-theory', 'manifolds', 'algebraic-topology', 'general-topology']"
1943071,How to solve the multiple integral $\int\sin^{p-2}(\theta_1)\sin^{p-3}(\theta_2) \cdots \sin(\theta_{p-2})\ d\theta_1d\theta_2\cdots d\theta_{p-1}$?,"I am wondering how to solve the multiple integrals of this form: $$
\int_{\theta_1 \in [0,\pi]}\ldots\int_{\theta_{p-2} \in [0,\pi]}\int_{\theta_{p-1} \in [0,2\pi]} \sin^{p-2}(\theta_1)\sin^{p-3}(\theta_2) \cdots \sin^{2}(\theta_{p-3}) \sin(\theta_{p-2})\ \ \  d\theta_1d\theta_2\cdots d\theta_{p-1}
$$ Here, the first $p-2$ integrals go from $[0,\pi]$ while the last goes from $[0,2\pi]$. I was wondering if there was a simple way to solve the integral. Thanks. How to solve the multiple integral $\int\sin^{p-2}(\theta_1)\sin^{p-3}(\theta_2) \cdots \sin(\theta_{p-2})\ \ \  d\theta_1d\theta_2\cdots d\theta_{p-1}$","['multivariable-calculus', 'integration']"
1943126,Prove that $\{ \sum_{n=0}^k z^n \}_{k=0}^\infty$ does not converge uniformly?,"So as the title says, can anyone prove that $\{ \sum_{n=0}^k z^n \}_{k=0}^\infty$ does not converge uniformly 0n the disk $D(0,1)$? I think it would converge uniformly to $1/(1-z)$ since it is a geometric series, but professor posed the problem so I'm thinking that must not be correct. Thoughts?","['complex-analysis', 'sequences-and-series', 'uniform-convergence', 'limits']"
1943135,"Number of one-to-one function such that $f(f(x))=x$ and $\left\lvert f(x)-x \right\rvert>2$ for all $x\in\{1,2,...,2n\}$","One of my friends asked this combinatorics problem, and I am completely lost... The problem is to find the number of one-to-one function $f:\{1,2,...,2n\}\mapsto\{1,2,...,2n\}$ such that
$$f(f(x))=x$$and$$\left\lvert f(x)-x \right\rvert>2$$for all $x\in\{1,2,...,2n\}$ I tried to make a recurrence relation, directly count it...etc. No methods seems to work for me. P.S. If this can be solved, can it be extended to the case where $\left\lvert f(x)-x \right\rvert>k$?","['combinatorics', 'discrete-mathematics']"
1943157,How to show this almost sure convergence,"I need to show following almost sure convergence: $X_1$, $X_2$, ..., be identically distributed non-negative random variables with $E[X_1]<\infty$. Prove that: $$\frac{X_n}{n}\to 0$$ with probability $1$. I want to use the first Borel Cantelli lemma:
For some $\epsilon>0$:
$$A_k = \{\frac{X_k}{k}>\epsilon\}$$
$$A = \cup_{n\geq 1}\cap_{k\geq n}A_k$$
and show $P(A) = 0$. However, after apply Markov's inequality to $P(A_k)=P(\frac{X_k}{k}>\epsilon)$, I can't show $\sum_{k=1}^{\infty}P(A_k)<\infty$ ... Thank you for your help.",['probability-theory']
1943179,Solving the equation $(x^2-3x+1)^2=4x^2-12x+9$,I need to solve the following equation: $$(x^2-3x+1)^2=4x^2-12x+9.$$ I think I need to bring everything to one side but I don't know anything else.,"['algebra-precalculus', 'polynomials']"
1943214,Show $\Bbb{R}^2/C \cong \Bbb{T} \times \Bbb{R}$,"Let $\Bbb{T}$ be the multiplicative subgroup of $\Bbb{C}\setminus\{0\}$ with unit modulus and $C$ the additive subgroup of $\Bbb{R}^2$ generated by a non-zero vector. Show that $\Bbb{R}^2/C \cong \Bbb{T} \times \Bbb{R}$. My initial thought is to in some way encorporate the first isomorphism theorem, much like the case of $\Bbb{R}/\Bbb{Z} \cong \Bbb{T}$, but I cannot quite figure it out. So another thought is to come up with some chain of isomorphisms in which each constituent isomorphism would be easier to handle. However, I have been told that there is a 'natural' isomorphism which, of course, would be preferable. Hints and directions are appreciated.","['abstract-algebra', 'group-theory', 'group-isomorphism']"
1943224,How would I prove that this function is affine if $f(x+h)-f(x)=hf'(x)$?,"Let $f$ be a differentiable function such that for every $x$ and $h$ it holds that $f(x+h)-f(x)=hf'(x)$ . Prove that $f(x)=kx+n$ where $k$ and $n$ are constants. I get it why this is true, and I tried to prove it somehow, but I can't seem to prove it rigorously with analysis. Any ideas?","['real-analysis', 'functional-equations']"
1943243,"Why do we treat differentials as infinitesimals, even when it's not rigorous","From single-variable calculus where we first encounter differentials we are told fairly often that differentials are not to be treated as infinitesimal quantities/objects (but we are never really told why) , and we kind of hand-wave things and manipulate them as fractions (e.g $\frac{dy}{dx}$) in things like $u$-Substitution. $$\int f(x) \ dx$$ In the above integral, the differential, $dx$, seems to be doing absolutely nothing other than signifying where the expression for the integrand finishes. But the differential, seems to have more of a part to play in multivariable calculus. In multivariable calculus we often encounter integrals like $$\oint \vec{f(x)}\bullet\vec{dX} = \oint \|\vec{f(x)}\| \cdot \|\vec{dX}\| \cos(\theta)$$ where we can treat vector differentials as if they were infinitesimal vectors, and perform normal vector operations (such as taking the dot product) Questions and comments It doesn't seem all that clear to me why we can't treat either scalar or vector differentials as infinitesimals. Furthermore why do we even treat differentials as infinitesimals, even when it's not technically rigorous? I'm sure there must be conditions that need to be met that allow us to treat differentials as infinitesimals. It seems like in single-variable calculus the differentials we encountered just a special case of the differentials we encounter in multivariable calculus, which we can do more with. I've heard of integration on differential forms (which I'm about to start learning), and I'm assuming that all the differentials we've encountered thus far in single and multivariable calculus must be special cases of this general form . Am I correct? If not then where do we rigorously define differentials and the operations we can perform with them?","['multivariable-calculus', 'real-analysis', 'differential-topology', 'vectors']"
1943244,On the general form of the family $\sum_{n=1}^\infty \frac{n^{k}}{e^{2n\pi}-1} $,"I. $k=4n+3.\;$ From this post , one knows that $$\sum_{n=1}^\infty \frac{n^{3}}{e^{2n\pi}-1} = \frac{\Gamma\big(\tfrac{1}{4}\big)^8}{2^{10}\cdot5\,\pi^6}-\frac{1}{240}$$ and a Mathematica session reveals $$\sum_{n=1}^\infty  \frac{n^{7}}{e^{2n\pi}-1} =\frac{3\,\Gamma\big(\tfrac{1}{4}\big)^{16}}{2^{17}\cdot5\,\pi^{12}}-\frac{1}{480}$$ $$\sum_{n=1}^\infty  \frac{n^{11}}{e^{2n\pi}-1} =\frac{189\,\Gamma\big(\tfrac{1}{4}\big)^{24}}{2^{22}\cdot5\cdot13\,\pi^{18}}-\frac{691}{65520}$$ and so on. The $691$ is a clue that Bernoulli numbers are involved. II. $k=4n+1.\;$ It evaluates to a rational number, $$\sum_{n=1}^\infty  \frac{n^{5}}{e^{2n\pi}-1} =\frac{1}{504}$$ $$\sum_{n=1}^\infty  \frac{n^{9}}{e^{2n\pi}-1} =\frac{1}{264}$$ $$\sum_{n=1}^\infty  \frac{n^{13}}{e^{2n\pi}-1} =\frac{1}{24}$$ etc, with the last mentioned in this post . Q: What are the general forms of I and II in terms of the Bernoulli numbers? (And a reference to Ramanujan's Notebooks, if possible.) (Note: 2019 edit to 2016 post) Since the $\Gamma(n)$ /pi ratio involved has a simple form in terms of the elliptic integral singular value $K(k_n)$ , turns out it was just the case $\tau=\sqrt{-1}$ , $$\beta_1=\frac{\Gamma\big(\tfrac14\big)^8}{2^8\pi^6}=\left(\frac{K(k_1)}{\pi}\right)^4$$ By analogy, the case $\tau=\sqrt{-3}$ , $$\color{red}{\beta_3}=\frac{3\Gamma\big(\tfrac13\big)^{12}}{2^9\cdot2^{1/3}\,\pi^8}=\left(\frac{K(k_3)}{\pi}\right)^4$$ So alternatively, $$\sum_{n=1}^\infty \frac{n^{3}}{e^{2\pi\,n}-1} = \frac1{20}\beta_1-\frac{1}{240}$$ $$\sum_{n=1}^\infty  \frac{n^{7}}{e^{2\pi\,n}-1} =\frac{3}{10}{\beta_1}^2-\frac{1}{480}$$ and a quick test showed, $$\sum_{n=1}^\infty \frac{n^{3}}{e^{2\pi\sqrt3\,n}-1} = \frac{1}{16}\color{red}{\beta_3}-\frac{1}{240}$$ $$\sum_{n=1}^\infty \frac{n^{7}}{e^{2\pi\sqrt3\,n}-1} = \frac{17}{32}\color{red}{{\beta_3}^2}-\frac{1}{480}$$ and so on.","['bernoulli-numbers', 'sequences-and-series', 'gamma-function', 'closed-form']"
1943248,Are maps $\operatorname{Spec}k[x]\times \operatorname{Spec}k[x]\to \operatorname{Spec}k[x]$ somehow bivariate polynomials?,"Let $R$ be a (commutative unitary) ring object in a topos $\mathcal E$. Say $R$ is a Fermat ring if it satisfies $$\forall f:R\to R\;\exists !g:R^2\to R\;: \forall x,x^\prime \in R\;[fx^\prime -fx=g(x,x^\prime )\cdot (x^\prime -x)].$$
As Kock writes in his SDG book, this is an alternative synthetic foundation for calculus. Suppose now $\mathcal E$ is a topos of $k$-algebras for $k$ a commutative unitary ring. $\mathcal E$ has the ring object $R=\operatorname{Spec}k[x]$. I want to prove that $$\mathcal E\models R\text{ is Fermat}.$$ A chain of isomorphisms quickly reminds me that $f:R\to R$ is just a univariate polynomial with coefficients in $k$. If an arrow $g:R^2\to R$ were just a bivariate polynomial, then it seems we can get a desired $g$ by looking at $f(Y)-f(X)$. Indeed, since $Y-X\mid Y^k-X^k$, then $Y-X\mid f(X)-f(Y)$ and any bivariate polynomial which gives $f(Y)-f(X)$ when multiplied by $Y-X$ should work. My problem is that I can't seem to get a chain of isomorphisms leading to $k[x,y]$; all I get is $$\mathsf{Hom}(\operatorname{Spec}k[x]\times \operatorname{Spec}k[x],\operatorname{Spec}k[x])\cong \mathsf{Hom}(k[x],k[x]\times k[x])\cong \mathsf{Hom}(\mathbf{2},Uk[x])$$i.e two univariate polynomials...","['synthetic-differential-geometry', 'topos-theory', 'algebraic-geometry', 'commutative-algebra', 'category-theory']"
1943298,Product of digits of a 7 digit number is $2^43^4$,"Find the number of seven digit numbers whose product of digits is $2^43^4$. One method is to list out all possible sets of seven digits that give this product, and then find number of permutations for each case. But there are too many cases that way. Is there a shorter method?","['combinatorics', 'elementary-number-theory']"
1943305,How to show that the Weierstrass function is meromorphic in whole of $\mathbb{C}$,"The Weierstrass function is defined as:
$$\wp(z) = \frac{1}{z^2}+ {\sum\limits_{\omega\in\Gamma'}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right).$$ Fix $r>0$. For $\left|z\right| \leq r,$ we split the sum in two parts:
\begin{align}
\wp(z) = \frac{1}{z^2}+{\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|\leq 2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right) + {\sum_{\substack{\omega\in\Gamma', \\ \left|\omega \right|>2r}}} \left (\frac{1}{(z-\omega)^2}-\frac{1}{\omega^2}\right).
\end{align}
Clearly $z=0$ is a pole of $\wp$. Also, the first part of the summation meromorphic function. Now we will show that the second part of the summation is bounded above and therefore cannot have any pole. Note that $$ \frac{1}{(z-\omega)^2}-\frac{1}{\omega^2} = \frac{z(2\omega - z)}{\omega^2 \left(z-w\right)^2}. $$
Assuming $\left|\omega\right| > 2\left|z\right|$,
$$\left| 2\omega - z \right| \leq \left| 2\omega \right| + \left| z \right| < \frac{5}{2}\left| \omega \right|,$$
$$ \left| z-\omega \right| \geq \left| \omega \right| - \left| z \right| > \frac{\left| \omega \right|}{2}.$$
This implies $$ \left| \dfrac{1}{(z-\omega)^2}-\dfrac{1}{\omega^2} \right| \leq \dfrac{\left| z \right| \times \dfrac{5}{2} \left| \omega \right|}{\left| \omega \right|^2 \times \dfrac{\left| \omega \right|^2}{4}} = \dfrac{10\left|z\right|}{\left|\omega\right|^3} \leq \dfrac{10r}{\left|\omega\right|^3}$$ We know that $\sum \frac{1}{\omega^3} < \infty$. Hence $\wp$ is a meromorphic function in $|z|\leq r$. Now how to show that $\wp$ is meromorphic in $\mathbb{C}$? I have seen some proofs and I am not able to understand how do they proceed after this step.","['riemann-surfaces', 'complex-analysis', 'meromorphic-functions']"
1943306,"Is the set of exponents for which a monotone function is Hölder-continuous closed in $(0,\infty)$?","Fix an a bounded (non-degenerate) interval $I\subset\mathbb R$. For $F:I\to\mathbb R$ and $\alpha>0$, a function is said to be Hölder-continuous of exponent $\mathbf{\alpha}$ if $$\sup_{\substack{x,y\in I\\x\neq y}}\frac{|F(x)-F(y)|}{|x-y|^{\alpha}}<\infty.$$ It is well-known (and quite easy to prove) that if $0<\alpha<\beta$ and $F$ is Hölder-continuous of order $\beta$, then it is also Hölder-continuous of order $\alpha$. This implies that the set of exponents for which a given function is Hölder-continuous is one of the following forms: empty, $(0,\infty)$, $(0, \alpha^*)$ for some $\alpha^*>0$, or $(0,\alpha^*]$. The question is whether this set of exponents is closed or not. To exclude pathologically oscillating functions, suppose that $F$ is non-decreasing and is Hölder-continuous for $\alpha\in(0,\alpha^*)$. Is it also Hölder-continuous of order $\alpha^*$?","['general-topology', 'real-analysis', 'holder-spaces']"
1943328,How many kinds of non abelian groups are there?,"I know about $S_n$, $D_n$ and $A_n$. And from my limited understanding there seem to be many more. I would like to know whether there is some kind of relation that links a small set of non Abelian groups to create the other ones. Something like with the Abelian groups and the Fundamental Theorem of Abelian Groups.","['group-theory', 'soft-question']"
1943343,Cartesian Product of (Lebesgue) Non-measurable Sets (without referring to Product Measure),"Let $A\subset\mathbb{R}$ be a nonmeasurable set, and let $E\subseteq\mathbb{R}$ be a (Lebesgue) measurable set. How do we show that the Cartesian product $A\times E$ is nonmeasurable in $\mathbb{R}^2$? Is there any way to show it without referring to the concept of product measure ? I can understand ( How to show product of two nonmeasurable sets is nonmeasurable? ) but would like a more concrete method. Perhaps using Caratheodory criterion or something like that? Thanks for any help. Hope the question makes sense..","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1943351,"Why is the random variable $X-Y$ almost sure constant if it is independent of $X$ and $Y$? Is this valid for a general random variable $f(X,Y)$?","Good day, In class we said that if a random variable $X-Y$ is independent of random variables $X$ and $Y$ then $X-Y$ is almost sure constant, i.e. there exists a $c \in \mathbb{R}$ such that $P(X-Y=c)=1$ . First, I don't exactly how to prove this. I know that $X$ is constant if it is independent of itself. Therefore I could prove that $X-Y$ is independent of itself (But the other directions doesn't hold I suppose). Do I know that $X-Y$ is independent of itself? Is it correct to say: If $Z$ is independent of $X$ and $Y$ then it is independent of $g(X,Y)$ where $g$ is a measurable function. I don't think so. The definition of independence doesn't give this property. Then how do I prove that $X-Y$ is almost sure constant? Another approach through expectations: $$E(X-Y|X)=E(X-Y|Y)=E(X-Y)=EX-EY $$ But is seems not leading me to a goal. So: Why is the random variable $X-Y$ almost sure constant if it is independent of $X$ and $Y$ ? Is this valid for a general random variable $f(X,Y)$ (where $f$ is measurable for example)? i.e. $f(X,Y)$ is almost sure constant it it is independent of $X$ and $Y$ ? If not I would ask for a counterexample. Thanks a lot for your help,
Marvin","['probability-theory', 'probability']"
1943364,Bounded differentiation operator,"Is there an example of a normed space $X$, $\dim X=\infty$ such that the differentiation operator $T=\frac{d}{dx}$ is bounded, meaning that $T \in B(X)$ ?","['functional-analysis', 'operator-theory']"
1943458,Are those two numbers transcendental?,"Suppose, $u$ solves the equation $$u^u=\pi$$ and $v$ solves the equation $$v\cdot e^v=\pi$$ So, we have $u=e^{W(\ln(\pi))}$ and $v=W(\pi)$. $u$ and $v$ should be the real solutions (in this case, they are unique). If someone can prove that $u$ and $v$ are transcendental, the next step would be to show that every solution is transcendental, but for the moment, the real case is sufficient. Are $u$ and $v$ transcendental ? The Lindemann-Weierstrass-theorem does not help here because $\pi$ is transcendental and $\ln(\pi)$ and $W(\ln(\pi))$ are probably (does anyone know a proof ?) transcendental. The algdep-command of PARI/GP does neither indicate the algebraicy of $u$ nor the algebraicy of $v$. So, if $u$ or $v$ is algebraic, the minimal polynomial must have high degree or coefficients with a large absolute value.","['number-theory', 'lambert-w', 'transcendental-numbers']"
1943536,The functional equation $f(x+x) = f(x)f(x)$,"Consider the exponential functional equation on the diagonal
\begin{align}
f(x+x) = f(x)f(x)\,, \quad f(0) = 1\,.
\end{align} What are the solutions for this problem? Either for all $x $ or for non-negative $x $. EDIT: Apparently the following is wrong:  I guess the exponential functions $x \mapsto e^{cx}$ are the only solutions if we assume measurability, continuity or some other regularity property.","['real-analysis', 'analysis', 'functional-equations']"
1943544,Problem getting a diffeomorphism work on $\mathbb{R}^{3}$,"I am trying to come up with a diffeomorphism f the cube onto itself. The reason I am trying to do this is because for a study I am doing I need a way of deforming the cube (onto itself) and be able to perfectly ""bring back"" the deformed domain to the original, in order to test some numerical algorithm I am working. Clarification just in case I have the XY problem . This is a follow up question from Example of a diffeomorphism on $\mathbb{R}^{3}$ onto itself (or cube onto itself) . 
I am trying to make a real example out of it. I start from defining ""a smooth (at least continuously differentiable) vector field $V(x,y,z)$ on the cube $Q$, which vanishes on the boundary of the cube. If I don't understand wrong this is  a valid $V$:
\begin{align*}
V(x,y,z) = \bigl(
  &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\
  &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L),\\
  &\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L)\bigr),
\end{align*}
being $L$ the length of the cube. Then I solve the ODE 
$$
\frac{d}{dt} h(x,y,z,t)= V(x,y,z)
$$ 
with initial conditions  $h(x,y,z,0) = (x,y,z)$. If I am not wrong (I may be) the solution is:
$$
\frac{d}{dt}[h_x(x,y,z,t), h_y(x,y,z,t), h_z(x,y,z,t)] = [V_x, V_y, V_z].
$$ And by integrating the right hand side from $t=0$ to $t=t_0$ $dt$ I get:
$$
h_x(x,y,z,t) = x_0 + t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L)
$$
(same with the other two). This equation does indeed describe a map of the cube onto itself (automorphism?), that actually looks similar to the image in the original post (especially if you modify $L$ by e.g. $L/2$). However, I am supposed to find the inverse map if I find the solution to $h_x(x,y,z,-t)$, which, if I am not wrong, simply translates to 
$$
h_x(x,y,z,-t)=x_0 - t\cdot\sin(x \cdot \pi/L)\sin(y \cdot \pi/L)\sin(z \cdot \pi/L)
$$
in my case. However, when numerically testing this, if I ""warp"" a grid with the first map, and then use this last one on those ""warped"" points, I do not get the original grid. So questions: What is wrong with my maths? What did I miss? What is the solution for this map? Is there one? If this is a bad way of doing the thing, what should I do? How can I get a map of the cube onto itself that I can invert ""perfectly"" (meaning I don't want a numerical approximation)? PD: MATLAB code that I am using for this: [x,y,z]=meshgrid(0:30,0:30,0:30);

L=30;
t=1;
x2=x - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L);
y2=y - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L);
z2=z - t.*sin(x.*pi/L).*sin(y.*pi/L).*sin(z.*pi/L);

x3=x2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L);
y3=y2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L);
z3=z2 + t.*sin(x2.*pi/L).*sin(y2.*pi/L).*sin(z2.*pi/L);

for indz=1:31
    clf
%     plot(x(:,:,indz),y(:,:,indz),'r.');
    mesh(x(:,:,indz),y(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','g','FaceColor','none');

    hold on
    mesh(x2(:,:,indz),y2(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','k','FaceColor','none');
    mesh(x3(:,:,indz),y3(:,:,indz),zeros(size(x,1),size(x,2),1),'EdgeColor','b','FaceColor','none');

    view(2)
    axis tight
    drawnow
    pause(0.2)
end","['numerical-methods', 'definite-integrals', 'ordinary-differential-equations', 'lie-groups']"
1943597,Showing that the exponential map is a smooth diffeomorphism,"I'm trying to show that the exponential map from the symmetric matrices to the positive definite symmetric matrices is a smooth diffeomorphism. To show that the map is smooth, I'm just relying on the fact that it's the sum of smooth maps. I'm trying to use the inverse function theorem to show that the differential of the exponential map at every $s \in S_n(\mathbb{R})$ is invertible, and therefore that the map has a smooth inverse at every $s \in S_n(\mathbb{R})$, and therefore, that its inverse is smooth globally. I'm not quite sure how to show this. One idea was to use: $$ de^{A} = d\sum_{p=0}^{\infty} \frac{1}{p!}A^p = 0 $$
$$ \sum_{p=0}^{\infty} d(\frac{1}{p!}A^p) = 0$$ Because $A$ is a symmetric matrix, we can diagonalize.
$$ \sum_{p=0}^{\infty} d(\frac{1}{p!}qA^pq^{-1}) = 0 $$
Then the entries on the diagonal look like $D^p = qAqq^{-1}...qq^{-1}Aq^{-1}$ $$ \sum_{p=0}^{\infty} d(\frac{1}{p!}D^p) = 0 $$
Where $D_{ij} = 0 $ unless $i = j$, and $(\frac{1}{p!}D^p)_{ii} = \frac{1}{p!}(D_{ii})^p $. I'm not sure how best to show that the differential is injective though.","['multivariable-calculus', 'differential-geometry', 'linear-algebra', 'lie-algebras']"
1943604,Prove q-Vandermonde Identity using a lattice path counting argument,"I have this problem, from Aigner's A Course in Enumeration : Vandermonde's formula for the $q$-binomial coefficients is
      $${{n+m}\brack {p}}_{q} = \sum_{k=0}^p 
q^{(m-k)(p-k)}{{m}\brack {k}}_{q}{{n}\brack {p-k}}_{q}$$
      Prove this formula by a lattice path counting argument.
      (Hint: Count lattice paths from $(0,0)$ to $(p,m+n-p)$, and consider the intersection between such paths with the line $x+y=n$.) I've previously solved this for the standard Vandermonde identity (not the q-analog), but I'm having trouble generalizing this to the q-analog. I understand this proof , could I perhaps using a similar argument to prove this?","['combinatorics', 'q-analogs']"
1943620,Find a non-linear function that when composed with a distance function is also a distance function.,"Is there a non-linear function from $\mathbb{R}^+_0 \rightarrow \mathbb{R}^+_0$ so that for each distance function $d$ (on any set), the composition $f◦d$ is also a distance function? I know that for $f◦d$ to be a distance function it must satisfy: Let $X$ be a set and let $d : X \times X \rightarrow \mathbb{R}^+_0$. For $d$ to be a distance function it must satisfy the following conditions $\forall x, y, z \in X$ $d(x, y) \geq 0$ $d(x, y) = 0 \iff x = y$ $d(x, y) = d(y, x)$ $d(x, z) \leq d(x, y) + d(y, z)$ I have tried using $f = \sqrt x$. I am sure my proof for the first 3 conditions is correct however i am not sure about condition 4. My Attempt: To prove: $\sqrt{d(x,z)} \leq \sqrt{d(x,y)} + \sqrt{d(y,z)}$. We already know that $d$ satisfies condition 4, so square rooting both side gives me $\sqrt{d(x,z)} \leq \sqrt{d(x,y) + d(y, z)}$. From the triangle inequality we know that $\sqrt{d(x,y)} + \sqrt{d(y,z)} \geq \sqrt{d(x,y) + d(y, z)}$ $\implies \sqrt{d(x,y)} + \sqrt{d(y,z)} \geq \sqrt{d(x,y) + d(y, z)} \geq \sqrt{d(x, z)}$. Q.E.D","['inequality', 'metric-spaces', 'functions']"
1943637,Is there a definition of closed sets in terms of closed balls,"In my general topology lectures, closed sets are defined in terms of open sets but I wonder whether closed sets can be defined in terms of closed balls in metrisable topological spaces.","['general-topology', 'metric-spaces']"
1943663,Is $E[\frac{\alpha}{b+x}]$ decreasing in $\alpha$ when $x$ is Gamma distributed with parameters $\alpha$ and $\beta$,"My question is whether $E[\frac{\alpha}{b+x}]$ is decreasing in $\alpha$ when $x$ is Gamma distributed with parameters $\alpha$ and $\beta$ (under the shape/rate parameterization). It seems like it would hold intuitively, and numerically, but the actual derivative of the expectation is incredibly complicated, so I don't even know where to start.","['statistics', 'probability', 'probability-distributions']"
1943685,Reason(s) Why the Set of Odd Permutations is Not a Subgroup of $S_n$,"Today in class, the professor went over some reasons why the set of odd permutations is not a subgroup of $S_n$. There are a couple reasons that I don't understand; those are: 1) The product of two odd permutations is even, so the group is not closed. 2) It does not contain the identity element (id = even). These may be stupid questions, but why is the product of two odd permutations even? I thought odd * odd was always odd? Also, why is it that the identity element must be even? Thank you for the help!","['permutations', 'abstract-algebra', 'group-theory']"
1943695,Prove $\int_0^{\infty}\frac{\ln (x)}{(x^2+1)(x^3+1)}\ dx=-\frac{37}{432}\pi^2$ with real method,"I came across the following integral:
$$\large{\int_0^\infty \frac{\ln (x)}{(x^2+1)(x^3+1)}\ dx=-\frac{37}{432}\pi^2}$$
I know it could be solved with resuide method, and I want to know if there are some real methods can sove it?
Meanwhile,I remember a similar integral:
$$\large{\int_0^\infty \frac{1}{(x^2+1)(x^a+1)}\ dx=\frac{\pi}{4}}$$
And I want to know the following one:
$${\color{red}{\large{\int_0^\infty \frac{\ln x}{(x^2+1)(x^a+1)}\ dx = \huge{?}}}}$$
Using the Mathematica I got the follow result. Could you suggest some ideas how to prove this?  Any hints will be appreciated.","['residue-calculus', 'calculus', 'complex-analysis', 'improper-integrals', 'integration']"
1943726,Evaluating a Rather Complicated Limit with a Simple Solution: $\lim_{x \to 0} {\cos(3x)-\cos(x) \over x^2}$,"SE, I've encountered an interesting question while practicing evaluation of limits. The following is the expression to be evaluated: $$\lim_{x \to 0} {\cos(3x)-\cos(x) \over x^2}$$ I've tried applying the double angle formula to $\cos(3x)$ by taking it as $\cos(2x+x)$ but the expression just gets messier as I go forward. I would thus greatly appreciate a push in the right direction! The given solution is ""$-4$""","['algebra-precalculus', 'trigonometry', 'calculus', 'limits-without-lhopital']"
1943751,The law of a continuous stochastic process and its canonical realization,"Let $T>0$ $d\in\mathbb N$ $(\Omega,\mathcal A,\operatorname P)$ be a probability space $X:\Omega\times[0,T]\to\mathbb R^d$ be a continuous stochastic process on $(\Omega,\mathcal A,\operatorname P)$ and $$\tilde X:\Omega\to C^0([0,T],\mathbb R^d)\;,\;\;\;\omega\mapsto X(\omega,\;\cdot\;)$$ We can Show that $\tilde X$ is a random variable on $(\Omega,\mathcal A,\operatorname P)$, i.e. $\tilde X$ is measurable with respect to $\mathcal A$-$\mathcal B(C^0([0,T],\mathbb R^d))$. Moreover, the distribution $\operatorname P_{\tilde X}$ of $\tilde X$ is called distribution of $X$. Now, let $$Y:C^0([0,T],\mathbb R^d)\times[0,T]\to\mathbb R^d\;,\;\;\;(\omega,t)\mapsto\omega(t)\;.\tag 1$$ If we could show that $Y$ is a stochastic process on $\left(C^0([0,T],\mathbb R^d),\mathcal B(C^0([0,T],\mathbb R^d)),\operatorname P_{\tilde X}\right)$, then it would make sense to call $Y$ canonical realization of $X$. All we need to show is that $Y_t$ is measurable with respect to $\mathcal B(C^0([0,T],\mathbb R^d))$-$\mathcal B(\mathbb R^d)$, for all $t\in[0,T]$, isn't it? However, in a book that I'm reading, the author writes that $Y$ is a stochastic process on $\left(C^0([0,T],\mathbb R^d),\mathcal B(C^0([0,T],\mathbb R^d)),\operatorname P_{\tilde X}\right)$, cause $(1)$ is (jointly) continuous and hence $Y$ is measurable with respect to $\mathcal B(C^0([0,T],\mathbb R^d))\otimes\mathcal B([0,T])$-$\mathcal B(\mathbb R^d)$. The former measurability is often called product measurability . I don't see why we need it here. A stochastic process doesn't need to be product measurable. Moreover, I don't think that the stated argumentation is correct. As far as I know, the only sufficient condition of this kind is that if $Y$ is a stochastic process (i.e. each $Y_t$ is measurable) with one-side continuous paths, then $Y$ is product measurable. Is there anything I'm missing?","['stochastic-processes', 'probability-theory', 'probability-distributions']"
1943762,A discrete maths problem on $2$ sets,"We have $2$ sets, $A=\{a_1,a_2,a_3\}$ and $B=\{b_1,b_2,b_3\}$ with $a_i, b_j\in\mathbb{Z}$. Is there any condition on elements of $A$ and $B$ that yield $A+B=\{a_i+b_j| i,j \in\{1,2,3\}\}$ or $A.B=\{a_i.b_j| i,j \in\{1,2,3\}\}$ have $3$ distinct elements with any multiplicities?",['discrete-mathematics']
