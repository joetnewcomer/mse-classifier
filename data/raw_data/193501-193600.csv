question_id,title,body,tags
3707569,Alternative proofs that the multiplicative group $(\mathbb{Z}/p\mathbb{Z})^*$ is cyclic,"Let $p$ be prime. Every proof of the fact that $(\mathbb{Z}/p\mathbb{Z})^*$ is cyclic uses at some point the fact that $x^a = 1 \pmod{p}$ has at most $a$ solutions. Gauss' original two proofs (showing the existence of a primitive root) use this fact and this source actually gives seven proofs, of which only the last one does not use this fact, but that one instead uses heavier machinery with cyclotomic polynomials anyway. It annoys me that this fact is so necessary and I was wondering if there are any proofs without it. The closest to an alternative proof, that I know of, works when the prime decomposition of $p-1$ contains primes only once, i.e. $p-1 = q_1 q_2\cdots q_n$ for distinct primes $q_i$ . Primes below $100$ of this form are $3$ , $7$ , $11$ , $23$ , $31$ , $43$ , $47$ , $59$ , $67$ , $71$ , $79$ , and $83$ . This alternative proof is due to McKay's proof of Cauchy's Theorem. It suffices to show for each prime $q_i$ that there exists an $a_i$ with order $q_i$ , since then the product $a_1 a_2 \cdots a_n$ has order $q_1 q_2 \cdots q_n = p-1$ and so $(\mathbb{Z}/p\mathbb{Z})^*$ is cyclic. So let's fix $q = q_i$ for some $q_i$ . Instead of finding an $a \neq 1$ with $a^q \equiv_p 1$ , let's solve the simpler problem of finding $(a_1, \ldots ,a_q)$ with $a_1 a_2 \cdots a_q \equiv_p 1$ . There are $(p-1)^{q-1}$ such tuples: Fix the first $q-1$ elements and solve for $a_q$ . Furthermore, if $(a_1, a_2, \ldots ,a_q)$ is a solution, then the cyclic permutation $(a_q, a_1, \ldots,a_{q-1})$ is one. That means solutions that are permutations of each other come in sets of size $q$ , except for the solutions that are their own permutation, i.e. $a_1 = \cdots = a_q$ . Let $N$ be the set of such solutions. We have that $|N| = (p-1)^{q-1} - \textit{sets of size }q$ . As $p-1$ and $q$ are obviously multiples of $q$ , the right side is a multiple of $q$ and therefore $|N|$ is too. But since $N$ contains the solution $(1, \ldots, 1)$ , $|N| \geq q$ and therefore there exists a non-trivial solution $(a, \ldots, a)$ with $a^q = 1$ . $\tag*{$\Box$}$ I like this alternative proof, because it has a more combinatorial flavour to it and is less algebraic. Again, I'd be thankful for any proof that does not use the fact that $x^a \equiv 1 \pmod p$ has at most $a$ solutions directly. Any deviance from the ""standard"" would help immensely.","['modular-arithmetic', 'alternative-proof', 'combinatorics', 'discrete-mathematics', 'group-theory']"
3707570,Cardinality of the Minimum Feedback Vertex Set of a directed graph,"Are nontrivial bounds known for the size of the minimum feedback vertex set of a directed graph in terms of the cardinalities of the edge and/or vertex sets? I've found quite a number of references for undirected graphs subject to various constraints (connected, planar, girth-bounded, etc), see for example this arxiv paper: https://arxiv.org/pdf/1603.04559.pdf . However, my searches don't seem to be turning much up for directed graphs, which seem to be to be different enough that the undirected results may not naturally apply. In particular I'm curious if a relationship is known between the number of edges in a digraph and the maximum possible size of a minimum feedback vertex set.","['directed-graphs', 'graph-theory', 'extremal-combinatorics', 'combinatorics', 'extremal-graph-theory']"
3707605,"In the described game, can the rubies be divided into 105 piles of one?","Problem In a pirate ship, there is a chest with 3 sacks containing 5, 49, and 51 rubies respectively. The treasurer of the pirate ship is bored and decides to play a game with the following rules: He can merge any two piles together into one pile, and he can divide a pile with an even number of rubies into two piles of equal size. He makes one move every day, and he will finish the game when he has divided the rubies into 105 piles of one. Is it possible for him to finish the game? Solution attempt It seems to me that it's not possible to finish the game, and here is the argument that I could come up with: Assume, for the sake of reaching a contradiction, that a state with 105 piles of 1 can be reached. The start state has no piles of 1, so the piles of 1 must have been obtained from other piles. From the rules, the only way of obtaining a pile of 1 from other piles is by splitting a pile of 2 into two piles of 1. So, each two piles of 1 must have originated from splitting a pile of 2. The resulting number of piles of 1 generated this way is even, because every pile of 2 generates 2 piles of 1. However, there is an odd number (105) of piles of 1, so this state is impossible to from the given start state using the defined rules. Is this correct, or at least on the right track?","['solution-verification', 'discrete-mathematics']"
3707612,Prove that the smallest positive integers for which the Euclidean algorithm takes $n$ steps are $F(n+1)$ and $F(n)$,"Problem The Euclidean state machine is defined by the rule $$(x,y) \rightarrow (y,\mathrm{rem}(x,y)),$$ for $y > 0$ . Prove that the smallest positive integers $a\geq b$ for which, starting in state $(a,b)$ , the state machine will make $n$ transitions are $F(n+1)$ and $F(n)$ , where $F(n)$ is the $n$ -th Fibonacci number. Note : $\mathrm{rem}(x,y)$ denotes the remainder of the division of $x$ by $y$ . Can someone please verify this solution? Solution Proof by induction. Induction hypothesis : $P(n)$ := the smallest positive integers $a \geq b$ for which, starting in state $(a,b)$ , the state machine will make $n$ transitions are $F(n+1)$ and $F(n)$ . Base case ( $n = 1$ ): $P(1)$ is the proposition: the smallest positive integers $a \geq b$ for which, starting in state $(a,b)$ , the state machine will make $1$ transition are $a = F(2) = 1$ and $b = F(1) = 1$ . $P(1)$ is true, because, starting at state $(F(2), F(1)) = (1,1)$ , there's one transition to $(1,0)$ , after which there are no more transitions. Inductive step : Assume that $P(n)$ is true for some $n \geq 1$ . Then, the smallest positive integers $a \geq b$ for which, starting in state $(a,b)$ , the state machine will make $n$ transitions are $F(n+1)$ and $F(n)$ . We want to show $P(n + 1)$ . Suppose that there are integers $a$ and $b$ , with $a \geq b$ , for which, starting in state $(a,b)$ , the state machine makes $n + 1$ transitions. From state $(a,b)$ , there is one transition to $(b, \mathrm{rem}(a,b))$ . So, from $(b, \mathrm{rem}(a,b))$ , the state machine makes $n$ transitions. Therefore, by the induction hypothesis, $b \geq F(n + 1)$ and $\mathrm{rem}(a,b) \geq F(n)$ . By the division theorem, $\mathrm{rem}(a,b) = a - qb$ , where $q$ is an integer. So, $a - qb >= F(n)$ . Also, since $a \geq b$ , then $q \geq 1$ . Therefore: $$a \geq F(n) + qb \geq F(n) + b \geq F(n) + F(n + 1) \geq F(n + 2)$$ Therefore, $a \geq F(n + 2)$ and $b \geq F(n + 1)$ . Now, I will show that, starting from $(F(n+2), F(n+1))$ , the state machine will make $n + 1$ transitions. Consider the state $T = (F(n + 2), F(n + 1))$ . It has a transition to $T' = (F(n + 1), \mathrm{rem}(F(n+2), F(n+1)))$ . But $\mathrm{rem}(F(n+2), F(n+1)) = \mathrm{rem}(F(n) + F(n+1), F(n+1)) = \mathrm{rem}(F(n), F(n+1)) = F(n)$ . So, $T' = (F(n + 1), F(n))$ . By the induction hypothesis, starting from $T'$ , the state machine makes $n$ transitions. Therefore, starting from $T$ , the state machine makes $n + 1$ transitions. It follows that the smallest positive integers $a \geq b$ for which, starting in state $(a,b)$ , the state machine will make $n+1$ transitions are $a=F(n+2)$ and $b=F(n+1)$ . This proves $P(n + 1)$ . Therefore, by induction, $P(n)$ is true for all $n\geq 1$ .","['elementary-number-theory', 'euclidean-algorithm', 'solution-verification', 'discrete-mathematics']"
3707624,Why is variance defined as $\sum\limits_{n} |\mu -x_i|^2$ and not $\sum\limits_{n} |\mu -x_i|$?,"If we wanted to measure how much the values $x_1, \ldots ,x_n$ of a sample differ from the mean $\mu$ , it seems more intuitive to me to use the formula $$\frac{\sum\limits_{n}  |\mu -x_i|}{n}$$ instead of the formula for variance. I've read about some geometric interpretations of variance as well as standard deviation, yet this just seems to push the questions further back, as we could ask what reason do we have to care more about the distance between the vectors $(x_1,\ldots x_n)$ and $(\mu ,\ldots ,\mu)$ as opposed to just the average distance between a possible value $x_0$ and $\mu$ . Some explanations of the variance formula point to the fact that variance pays more attention to values further apart from the mean, but two immediate questions come to mind: Why should we give more importance to values farther apart from the mean? And why should we do so by squaring the respective distances instead of, say, cubing them?","['statistics', 'variance', 'definition', 'soft-question', 'probability']"
3707653,"If an element of a group $G$ has infinite order, prove that powers of that element are distinct","Let $G$ be a group and $x \in G$ such that $x$ has infinite order.
Prove carefully that the elements $x^n $ where $n \in \mathbb{Z}$ are
all ${\bf distinct}$ Proof. (attempt) We are given $|x| = \infty$ . Now, say $x^n = x^m$ for some $n \neq m $ integers. So that $x^{n-m} = e$ but this means that $|x| \leq n - m $ which is a contradiction. Is this correct? Now, what if $|x| = N$ ? instead of $\infty$ as in problem above. I think we can use similar argument to argue that $e, x, x^2,..., x^{N-1}$ are distinct. For instance, say we have $m,n \in Z$ less than $N$ so obviously $n-m < N$ . Therefore, if $x^n = x^m $ , then $x^{n-m} = e$ but this is contradiction since $N$ is the least with $x^N = e$ . QED Is this correct?","['group-theory', 'abstract-algebra', 'solution-verification', 'infinite-groups']"
3707744,Are there any explicit solutions of $yy' = 5x$ that pass through the origin?,First part: Use the fact that $5x^2 − y^2 = c$ is a one-parameter family of solutions of the differential equation: $y y'= 5x$ to find an implicit solution of the initial-value problem: $$y \dfrac {dy}{dx} = 5x \\ y(2) = −6$$ I obtained the answer as $y^2=5x^2+16$ . Then it asks if there are any explicit solutions of $yy' = 5x$ that pass through the origin? I'm not sure how to go about this part.,['ordinary-differential-equations']
3707805,Find all functions $f$ :- $\mathbb{N}$ $\to$ $\mathbb{N}$ such that :- $xf(y) + yf(x) = (x + y)f(x^2 + y^2)$,"So here is the Question :- Find all functions $f$ :- $\mathbb{N}$ $\to$ $\mathbb{N}$ such that :- $$xf(y) + yf(x) = (x + y)f(x^2 + y^2)$$ I tried substituting values for $x$ and $y$ , but I couldn't reach to a possible clue to the solution. Any hints or suggestions will be greatly appreciated!","['functional-equations', 'functions', 'analysis']"
3707830,Elementary Relations/Functions and the Solvability of their Inverses,"Background I've been interested lately in the idea of solving and inverting equations, and a question came to mind. Feel free to correct my notation, I could benefit from cleaner structuring. Going to stick primarily to the realm of real numbers here; just finished freshman year as a Math-Physics major, so I'm ill-equipped to deal with anything other than algebra and introductory calculus, but any and all answers are appreciated nonetheless. Here's some context: in secondary education (high school), we typically learn about equation types of the simplest forms, starting off with equations such as - $$\begin{align}
y &= ax + b\tag{1}\label{1}\\
y &= a^x\tag{2}\label{2}\\
y &= \sin(x)\tag{3}\label{3}\\
y &= ax^2 + bx + c\tag{4}\label{4}
\end{align}$$ Where $a$ , $b$ , $c$ $\in{\mathbb{R}}$ , $x$ is your input, and $y$ your output. In all of these cases, one can set $y = 0$ and proceed to solve for $x$ , or better yet, invert the functions altogether, setting $y$ in terms of $x$ . Here are those same examples, inverted - $$\begin{align}
x &= \frac{y - b}{a}\tag{5}\label{5}\\
x &= log_{a}(y)\tag{6}\label{6}\\
x &= \sin^{-1}(y)\tag{7}\label{7}\\
x &= \frac{-b\pm{\sqrt{b^2-4a(c-y)}}}{2a}\tag{8}\label{8}
\end{align}$$ Functions over Relations The first aspect I'm struggling with understanding is why differentiate between a function and a relation? In other words - what good does that bring?
In the case of \eqref{7} and \eqref{8}, their domains are restricted to allow them to continue being functions. Why do we do this? Why not just treat the two as relations instead of different inverse function branches? Complexity of Inverting In addition to the aforementioned, I was also curious about, with functions as simple as these, why does combining them seems to make inversion that much harder? For instance, let's suppose you combined \eqref{2} and \eqref{4}, resulting in something of this form: $$y = a^x + bx^2 + cx + d$$ Right off the bat this looks incredibly hard to solve for $x$ when $y=0$ , much less manipulating the equation to have it be in terms of $x$ . Why is this the case, and what methods would you use to tackle problems like these? (Note: I'm sure one could always define the inverse as the inverse of the function, just as how the square root is by definition the inverse function/relation of $ y = x^2 $ , but I would prefer solutions in terms of elementary functions.) Any intuitive and/or rigorous explanations help, thanks!","['elementary-functions', 'algebra-precalculus', 'functions', 'real-analysis']"
3707881,To prove that there are infinitely many prime numbers using topology,"Proof Let $\tau$ denote that collection of $S(a,b)$ . We show $\tau$ is topology. $\varnothing \in \tau$ is automatic. Next, since $\mathbb{Z} = \bigcup \{ n \} $ and $\{ n \} = S(1,0)$ , then it is in $\tau$ . Now, take a collection of $\{ S(a,b) \}_{a,b \in \mathbb{Z}}$ . we need prove $\bigcup_{a,b} S(a,b) \in \tau $ . Isnt this automatic by definition? Finally, if $S(a_1,b_1)$ and $S(a_2,b_2)$ are two arithmetic progressions, then $$ S(a_1,b_1 ) \cap S(a_2, b_2) = \{ a_1 n + b_1 \} \cap \{ a_2 n + b_2 \}$$ By choosing $n$ , I think it is possible to write this intersection as union of elements of the form $\{ a_3 k + b_3 \}$ but, I am unable to do this rigorously. But I know it is possible by choosing $n$ appropriately.. (b) If $x \in \bigcup_p S(p,0) $ then $x $ lies in some $S(p,0)$ , that is $x = pn $ for some $p$ . Since $p \neq 1,-1$ , then $x \in \mathbb{Z} \setminus \{-1,1\}$ . Im stuck on the other inclusion. I mean it seems intutitively obvious, Im having hard time writing it rigorously. finally, assume we have only finite number of primes. Notice that $\mathbb{Z} \setminus S(p,0) = \bigcup_{q \neq p} S(q,0) $ which is open so $S(p,0)$ is closed. The complement of $\mathbb{Z} \setminus \{-1,1\} $ is $\{-1,1\}$ which is not open since set if finite... I havent used the fact that there are finitely many primes... where did I make a mistake?","['general-topology', 'solution-verification', 'prime-numbers', 'arithmetic']"
3707887,Prove vector-valued function is differentiable,"Give $D$ is an open set in $\mathbb{R}^n$ , $f: D\to \mathbb{R}^p$ is differentiable on $D$ . Supposing that $f$ has second derivative at $x_0\in D$ . For every $u\in \mathbb{R}^n$ , give $g: D\to \mathbb{R}^p$ define by $$g(x)=f'(x)(u), \forall x\in D.$$ Prove that $g$ is differentiable at $x_0$ and $g'(x_0)(v)=f^{(2)}(x_0)(u,v)$ . My attempt: Give $h\in \mathbb{R}^n$ that $x+h \in D$ . Firstly, we have $$g(x_0+h)-g(x_0)=f'(x_0+h)(u)-f'(x_0)(u)=\left[f'(x_0+h)-f'(x_0)\right](u) \quad (1)$$ Secondly, since $f$ have second dervative at $x_0$ that exist linear mapping $A:\mathbb{R}^n\to L(\mathbb{R}^n,\mathbb{R}^p)$ that $$f'(x_0+h)-f'(x_0)=A(h)+\vert h \vert_2\varphi(h) \quad (2)$$ with $\lim\limits_{h \to 0_{\mathbb{R}^n}}\varphi(h)=0_{L(\mathbb{R}^n,\mathbb{R}^p)}$ . From (1) and (2), we infer that $$g(x_0+h)-g(x_0)=A(h)(u)+\vert h \vert_2\varphi(h)(u)$$ I stuck here now. I wonder that there exists linear mapping $A_1: \mathbb{R}^n \to \mathbb{R}^p$ and function $\varphi_1:\mathbb{R}^n \to \mathbb{R}^p$ that $A_1(u)(h)=A(h)(u)$ and $\varphi_1(u)(h)=\varphi(h)(u)$ ?","['derivatives', 'vector-analysis']"
3707961,Definition of Equivalence Relation [duplicate],"This question already has answers here : Are ""if"" and ""iff"" interchangeable in definitions? (15 answers) Closed 4 years ago . I was going through the text ""Discrete Mathematics and its Application"" by Kenneth Rosen (5th Edition) where I am across the definition of equivalence relation and felt that it is one sided. Definition: A relation on a set A is called an equivalence relation if it is reflexive, symmetric, and transitive. Now let us analyze the situation of what equivalence is meant to us intuitively. Let there be a binary relation $R$ defined on a set $A$ . Now we suppose that $R$ be reflexive, symmetric and transitive. So we have for $a,b,c \in A$ $a R a$ (by the reflexive property of R) if $a R b$ then $b R a$ (by the symmetric property of R) if $a R b$ and $b R c$ then $aRc$ (by the transitive property of R) Intuitively we can satisfy ourselves with the fact that the above are the necessary conditions for $R$ to be equivalent. So ""if $R$ is reflexive, symmetric and transitive, then $R$ is an equivalence relation"" Now working our intuition for equivalence relation $\sim$ we note the following. Let $\sim$ be an equivalence relation on a set A, then for $a,b,c \in A$ we have, $a \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ then $b \sim a$ (by the intuitive knowledge of what $\sim$ means) if $a\sim b$ and $b \sim c$ then $a\sim c$ (by the intuitive knowledge of what $\sim$ means) Now we see that (1) implies $\sim$ is reflexive, (2) implies that $\sim$ is symmetric and (3) implies that $\sim$ is transitive. So we have ""if $\sim$ is an equivalent relation then $\sim$ is reflexive, symmetric and transitive"" From the two intuitive implications we can conclude that A relation on a set A is called an equivalence relation if and only if it is reflexive, symmetric, and transitive. and not what the book says. This definition makes quite sense unlike the book definition which says that if $R$ fails to be either reflexive or symmetric or transitive then $R$ may or may not be an equivalence relation, which after all gives a weird feeling. Correct me if my logic is wrong.","['proof-writing', 'relations', 'definition', 'discrete-mathematics', 'elementary-set-theory']"
3708005,Understanding the Poisson bracket for this system in $so(4)$,"I have the following system of ODEs: \begin{align*}
    \dot{x}_1 &= x_2x_3A_{32} + x_5 x_6 A_{65}\\
    \dot{x}_2 &= x_1 x_3 A_{13} + x_4 x_6 A_{46}\\
    \dot{x}_3 &= x_1 x_2 A_{21} + x_4 x_5 A_{54}\\
    \dot{x}_4 &= x_3x_5 A_{35} + x_2 x_6 A_{62}\\
    \dot{x}_5 &= x_3 x_4 A_{43} + x_1 x_6 A_{16}\\
    \dot{x}_6 &= x_2 x_4 A_{24} + x_1 x_5 A_{51}
\end{align*} where $A_{ij} = \frac{1}{I_i} - \frac{1}{I_j}$ . Questions: What is the Poisson bracket associated with the above system? If it is as I describe, then why is $\{H,H\} \neq 0$ ? Am I doing something wrong? My Work: For a smaller system: $$
\begin{cases}
\dot{x}_{1} &= \alpha_{1} x_2 x_3,\\
\dot{x}_{2} &= \alpha_{2} x_3 x_1,\\
\dot{x}_{3} &= \alpha_{3} x_1 x_2
\end{cases}
$$ where $\alpha_i = \frac{1}{I_k} - \frac{1}{I_j}$ s are some physical constants. I had been using the following definition $$
\{\mathbf{x},\mathbf{y\}} = (\nabla f(\mathbf{x}))^{T} \omega(\mathbf{x}) \nabla \mathbf{y}
$$ where $\omega$ is a $n \times n$ matrix called the Poisson matrix. For this small system $$\omega(x) = \begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix} \in so(3)$$ Using this definition we can confirm that the function $H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3}$ is the Hamiltonian of this system. This is done by recovering the system from the Poisson bracket, e.g, $$\dot{x}_1 = \{x_1, H\}$$ . There are two ways I done this computation. The first way keeping everything as matrices and taking the trace at the end: $(-,-) = tr(x^{T}\omega y)$ BUT I DONT KNOW WHY THIS WORKS. Why the trace? The basis for $so(3)$ are the three matrices $e_1 = \begin{pmatrix}
0 & 0 & 0\\
0 & 0 & -1\\
0 & 1 & 0
\end{pmatrix}$ $e_{2} = \begin{pmatrix}
    0 & 0 & 1 \\ 0 & 0 & 0 \\ -1 & 0 & 0
    \end{pmatrix}$ $e_{3} =  \begin{pmatrix}
    0 & -1 & 0 \\ 1 & 0 & 0 \\ 0 & 0 & 0
    \end{pmatrix}$ Then by definition $\nabla x_1 = \sum_{i=1}^{3} \frac{\partial x_1}{x_i} \cdot e_i = e_1 = \begin{pmatrix}
    0 & 0  & 0 \\ 0 & 0 & -1 \\ 0 & 1 & 0
    \end{pmatrix}$ . Similarly, $\nabla H = \begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix}$ And so I then do the computation \begin{align*}
    \{x_1, H\} &= \underbrace{\begin{pmatrix}
   0 & 0 & 0\\
   0 & 0 & 1\\
   0 & -1 & 0
   \end{pmatrix}}_{(\Delta_x f)^T} \underbrace{\begin{pmatrix}0 & -x_{3} & x_{2} \\ x_{3} & 0 & -x_{1} \\ -x_{2} & x_{1} & 0  \end{pmatrix}}_{\omega(x)} \underbrace{\begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2}\\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix}}_{\Delta_x H}\\
    &= \begin{pmatrix}
    0 & 0 & 0 \\
    -x_2 & x_2 & 0\\
    -x_3 & 0 & x_1
    \end{pmatrix}\begin{pmatrix}
    0 & -\frac{x_3}{I_3} & \frac{x_2}{I_2} \\
    \frac{x_3}{I_3} & 0 & -\frac{x_1}{I_1}\\
    -\frac{x_2}{I_2} & \frac{x_1}{I_1} & 0
    \end{pmatrix}\\
    &= \underbrace{\begin{pmatrix}
    0 & 0 & 0 \\
    \frac{x_1 x_3}{I_3} & \frac{x_2 x_3}{I_3} & -\frac{x_2^2}{I_2} - \frac{x_1^2}{I_1} \\
    -\frac{x_1x_2}{I_2} & \frac{x_3^2}{I_3} + \frac{x_1^2}{I_1} & -\frac{x_2 x_3}{I_2}
    \end{pmatrix}}_{A}
\end{align*} Taking the trace of the matrix above yields: \begin{align*}
    tr(A) &= \frac{x_2x_3}{I_3} - \frac{x_2x_3}{I_2}\\
    &= (\frac{1}{I_3} - \frac{1}{I_2})x_2 x_3
\end{align*} which is exactly the first equation of our little system! We can also recover $\dot{x}_2$ and $\dot{x}_3$ . But, again, I don't really know why we have to take the trace other than it works. The function $f = x_1^2 + x_2^2 + x_3^2$ is also a conserved quantity and is verified by showing $\{f,H\} = 0$ . So $f$ and $H$ are in involution. The second way to do the computation is a little different. I compute with: $(-,-) = (\nabla x)^T B (\nabla y)$ where $(x), (y)$ are written in terms of the basis and $B$ is made up of $b_{ij} = \{e_i, e_j\}$ . Turns out that $B = \omega$ here. Then $(\nabla x) = \begin{pmatrix}1 \\ 0 \\ 0\end{pmatrix}$ and $(\nabla y) = \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix}$ . So then doing the computation $$\begin{pmatrix}1; 0; 0\end{pmatrix} B \begin{pmatrix}\frac{x_1}{I_1} \\ \frac{x_2}{I_2} \\ \frac{x_3}{I_3} \end{pmatrix}$$ yields what we desire: $x_2x_3\alpha_1$ . So we see these two computations are equivalent. But why are they equivalent? The trace seems kinda arbitrary to me. Now, for the big system I have found three suspected conserved quantities. One of them being the Hamiltonian $H = \frac{1}{2}\frac{x_1^2}{I_1} + \frac{1}{2}\frac{x_2^2}{I_2} + \frac{1}{2}\frac{x_3^2}{I_3} + \frac{1}{2}\frac{x_4^2}{I_4} + \frac{1}{2}\frac{x_5^2}{I_5} + \frac{1}{2}\frac{x_6^2}{I_6}
$ . I won't write the computations here, but I confirmed that we recover the big system. That is, I get $$\dot{x}_1 = \{x_1, H\}$$ where $\omega \in so(4)$ . HOWEVER , I have seen that $$\{H,H\} \neq 0$$ This is bad since it should be zero. I do not understand why it is not zero since I did the calculation showing we recover the system so that means the system is Hamiltonian doesn't it? Questions: What is the Poisson bracket associated with the big system? If it is as I describe, then why is $\{H,H\} \neq 0$ ? Am I doing something wrong?","['classical-mechanics', 'ordinary-differential-equations']"
3708028,"Criteria for a distribution function to uniquely determine a measure on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$","I know that for a function $F:\mathbb{R}\to[0,1]$ with the below criteria, $F$ is a distribution function for a probability measure $\mu_F$ . Indeed, we can define $\mu_F=F(b)-F(a)$ for all $a,b\in\mathbb{R},a<b$ the Lebesgue-Stieltje measure. Criteria: $F$ has to be monotone increasing, right-continuous and $F(-\infty)=0$ , $F(\infty)=1$ . But when looking at $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$ , are there similar criteria to create a measure from a function $F:\mathbb{R}^2\to[0,1]$ ?
That is, $F$ is the distribution function of a probability measure $\mu$ on $(\mathbb{R}^2,\mathcal{B}(\mathbb{R}^2))$ ? My thoughts so far: If $Z=(X,Y)$ is a random variable with distribution $\mu$ , then we want $F(z)=F(x,y)=P_{\mu}(X\leqslant x,Y\leqslant y)$ for all $x,y\in\mathbb{R}$ . This would mean that $$F(-z)\to0\text{ and }F(z)\to1\text{ when }\vert z\vert\to\infty$$ that is $$F(-\infty,-\infty)=0\text{ and } F(+\infty,+\infty)=1$$ This is our first hypothesis.
We have to keep the monotone increasing hypothesis, thus I suggest $$F(x_1,y)\leqslant F(x_2,y)\text{ for }x_1\leqslant x_2$$ and $$F(x,y_1)\leqslant F(x,y_2)\text{ for }y_1\leqslant y_2$$ but I am not sure.
For $F$ being right-continuous, we can relax it to be continuous, or maybe it we can only rely on $F$ being continuous from the right and from the top (same kind of idea in 1D but here in 2D). Now if $z_1=(x_1,y_1)$ and $z_2=(x_2,y_2)$ are such that $x_1< x_2$ and $y_1< y_2$ , then the rectangle $(x_1,y_1), (x_2,y_1), (x_2,y_2), (x_1,y_2)$ has a positive volume, thus the function $F$ has to increase in that domain. This would mean that another hypothesis is $$
F((x_2,y_2))-F((x_2,y_1))-F((x_1,y_2))+F((x_1,y_1))\geqslant 0
$$ for all $x_1\leqslant x_2, y_1\leqslant y_2$ . Thus I suggest the following: There exists a measure $\mu$ such that $F$ is its distribution function if and only if: (1) $F$ is right-continuous and top-continuous (right-continous on each of its variable) (2) $F$ is increasing: $$F(x_1,y)\leqslant F(x_2,y)\text{ for }x_1\leqslant x_2$$ and $$F(x,y_1)\leqslant F(x,y_2)\text{ for }y_1\leqslant y_2$$ (3) $F$ gives a positive value for a rectangle: $$
F((x_2,y_2))-F((x_2,y_1))-F((x_1,y_2))+F((x_1,y_1))\geqslant 0
$$ for all $x_1\leqslant x_2, y_1\leqslant y_2$ . In that case, $\mu$ is defined to be $$\mu([x_1,y_1]\times[x_2,y_2])=F((x_2,y_2))-F((x_2,y_1))-F((x_1,y_2))+F((x_1,y_1))
$$ on all rectangles of $\mathbb{R}^2$ , and the sets of all rectangles is a generator of $\mathcal{B}(\mathbb{R}^2)$ so we may extend the value of $\mu$ for all Borel sets of $\mathbb{R}^2$ . I'll try to prove this and see if this works, still any help is appreciated !
Thanks !","['measure-theory', 'lebesgue-measure', 'borel-measures', 'borel-sets', 'probability-theory']"
3708034,Chern class in Bott-Tu,"I've met some trouble in understanding Chern class. I first touch the Chern class in classyfying space of characteristic class. For $\pi:E\rightarrow X$ and we have such relationship: $$Vect^n_{\Bbb C}(X) \cong [X,Gr_n(C^{\infty})] \cong [X,BU(n)]$$ so for $f:M\rightarrow Gr_n(C^{\infty})\cong BU(n)$ , we define the Chern class via pullbuck $f^*:H^*(BU(n),\Bbb Z)\rightarrow H^*(M,\Bbb Z)$ . Intuitively speaking, this apprroach classfies the complex vector bundle. It seems that Bott&Tu uses Grothendieck's approach by constructing line bundle and the first Chern class We then define the projectivization of vector bundle $\pi:P(E)\rightarrow M$ , more precisely we have the taotological exact sequence $$0\rightarrow S \rightarrow \pi^{-1}E\rightarrow Q\rightarrow 0$$ here $\pi^{-1}E$ is the vector bundle over $P(E)$ ; $S$ is the universal bundle over $P(E)$ and $Q$ is the quotient bundle. We then set $x=c_1(S^*)$ where $S^*$ is called hyperplane bundle which is dual to $S$ and Bott&Tu says the cohomology ring of $H^*(P(E))=H^*(M)[x]/(x^n+c_1(E)x^{n-1}+...+c_n(E))$ Here the Chern clss is defined via projective bundle and I can't undertstand it and its relationship between the way given by characteristic class? More precisely, the relationship between Grassmannian manifold and projective bundle?","['characteristic-classes', 'algebraic-geometry', 'algebraic-topology']"
3708126,"LR Test in Beta($\theta$, 1) with $H_0 = {\theta_0}$","I'm trying to obtain $\alpha$ -level LR Test where $(X_1, ... X_n)$ are from Beta( $\theta$ , 1) with $H_0 = {\theta_0}$ and $H_0 \neq \theta_0$ . I'm looking for $$
\lambda(X) = \frac{\sup_{\theta \in \Theta_0}l(\theta)}{\sup_{\theta \in \Theta}l(\theta)}
$$ Suppose $T := \sum^n_{i=1}\ln{X_i}$ and we know that MLE of Beta $(\theta, 1)$ equals $$
\hat{\theta}=\frac{-n}{T}
$$ Our $\lambda(X)$ is then: $$
\lambda(X) = \frac{\theta_{0}^{n} (X_1 \cdot ... \cdot X_n)^{\theta_0 - 1}}{(\frac{-n}{T})^n(X_1 \cdot ... \cdot X_n)^{\frac{-n}{T} - 1}} = \left(\frac{- \theta_0 T}{n}\right)^n(X_1 \cdot ... \cdot X_n)^{\theta_0 + \frac{n}{T}}
$$ We want to find $\lambda(X) < c$ but we may as well look for $\ln\lambda(X) < \ln c = \hat c $ . Taking the logarithm: $$
\ln\lambda(X) = n \ln{\frac{- \theta_0 T}{n}}+\left({\theta_0 + \frac{n}{T}}\right)T =n \ln{\frac{- \theta_0 T}{n}} + \theta_0T + n
$$ This has to be lesser than some $\hat c$ $$
n \ln{\frac{- \theta_0 T}{n}} + \theta_0T + n < \hat c $$ We define $f(x) = n \ln{\frac{- \theta_0 x}{n}} + \theta_0x + n$ and see how it behaves: $$
f'(x) = \frac{n}{\frac{\theta_0x}{n}} + \theta_0 \\
f'(x) = \frac{n^2}{\theta_0x} + \theta_0
$$ $f'(x) > 0$ iff: $$
\frac{n^2}{\theta_0x} + \theta_0 > 0 \\
\frac{n^2}{\theta_0x} > - \theta_0 \\
\theta_0x < -\frac{n^2}{\theta_0} \\
x < -\left(\frac{n}{\theta_0}\right)^2
$$ This makes x negative, but it does not raise my concern because $x=\sum \ln X_i$ where $X_i \in (0,1)$ so $\ln X_i < 0$ it can be indeed negative. I'd then get the LR test looking like: $$
\varphi(X) = \begin{cases}
1 & T < d_1 \text{ or } T > d_2  \\
\gamma_1 & T = d_1 \\
\gamma_2 & T = d_2 \\
0 & T \in (d_1, d_2)
\end{cases}
$$ where $d_1 < -\frac{n^2}{\theta_0^2} < d_2$ for $d_1, d_2$ calculated to meet $\alpha$ . Is my solution correct so far? I have noticed the question has been answered here , but I could not fully get the grasp of it and wanted to go step by step.","['statistics', 'log-likelihood', 'maximum-likelihood', 'hypothesis-testing', 'probability']"
3708188,Increasing function $ f : \mathbb R ^ + \to \mathbb R $ with $ x f ( x ) + 2 > 0 $ and $ f ( x ) f \left( f ( x ) + \frac 4 x \right) = 1 $,"Let $ f : \mathbb R ^ + \to \mathbb R $ be an increasing function, such that $ x f ( x ) + 2 > 0 $ and $ f ( x ) f \left( \frac { x f ( x ) + 4 } x \right) = 1 $ , then find the value of $ \lfloor f ' ( 1 ) \rfloor $ (where $ \lfloor \cdot \rfloor $ represents greatest integer function). My approach is as follow $ f ( x ) f ( y ) = 1 $ as $ y = \frac { x f ( x ) + 4 } x $ , I am trying to use the function given as $ x f ( x ) + 2 > 0 $ in $ y $ but I'm not able to proceed.","['functional-equations', 'functions']"
3708253,Basis of meromorphic $1$-forms on compact Riemann surface.,"I am trying to solve the following exercise but I do not really know how to proceed. For an integral divisor $D$ and any compact Riemann surface $M$ , describe a basis of the space $\Omega(-D)$ . Where $\Omega(-D) = \{\omega\in\mathcal{M}\Omega^1(M)\mid (\omega)\geq -D\}$ . Since $D = \sum_v s_vp_v$ is integral i.e. $s_v>0$ for at least one $v$ , the elements of $\Omega(-D)$ are meromorphic $1$ -forms which have a pole of order at most $s_v$ at $p_v$ . This clearly contains all holomorphic differentials and I know how to generate those, but I have no idea about any classification of the strictly meromorphic differentials in terms of their dimension. I have only seen proven the existence of such differentials with a single pole of higher order, or of differentials with two simple poles in the book of Wilhelm Schlag on A course in Complex Analysis and Riemann surfaces . I also tried finding the dimension using the Riemann-Roch theorem, but for that I would need to find the dimension of $L(-D)$ the meromorphic functions on $M$ which only have poles at $p_v$ of order at most $s_v$ . But I am not sure how to formally do this aswell. Does anyone know how to find this basis $\Omega(-D)$ ?","['complex-analysis', 'riemann-surfaces', 'divisors-algebraic-geometry', 'linear-algebra']"
3708256,Surface-Curl integral questions,"Good morning, I have some questions about a surface integral with curl. The exercise is the following: Be $(\Sigma, \omega)$ an oriented surface with boundary where $$\Sigma = \{(x, y, z): x^2 + y^2 = z^2+1 ,\ -1 \leq z \leq 3\}$$ Calculate $$ \int\int_{\Sigma}\langle \text{rot}F, \omega\rangle \text{d}\sigma$$ where $$ F(x, y, z) = -\dfrac{1}{3}(y, x, z)$$ and $$\omega(0, 1, 0) = (0, 1, 0)$$ Then he asks to verify the result also by applying Stokes (later). Some details on the procedure Well first of all it's not a big deal to find out that $$\text{rot}F = \dfrac{1}{3}(1, 1, 1)$$ Then we have $$\int\int_{\Sigma} \langle \text{rot}F, \omega\rangle \text{d}\sigma = \dfrac{1}{3}\int\int_{\Sigma} \sum_{i = 1}^3 \omega_i \text{d}\sigma$$ Where $\omega = (\omega_1, \omega_2, \omega_3)$ . A parametrisation for $\Sigma$ is given by $$\phi:[0, 2\pi) \times [-1, 3] \to \Sigma$$ where $$\phi(\theta, z) = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, z)$$ In particular we find that the Jacobian is $$
\begin{pmatrix}
-\sqrt{z^2+1}\sin\theta & \dfrac{z}{\sqrt{z^2+1}}\cos\theta \\
\sqrt{z^2+1}\cos\theta & \dfrac{z}{\sqrt{z^2+1}}\sin\theta \\
0 & 1
\end{pmatrix}
$$ And its rank is two. Thence: $$\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi}{\partial z} = (\sqrt{z^2+1}\cos\theta, \sqrt{z^2+1}\sin\theta, -z)$$ Now: $$\omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = \dfrac{\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}}{||\dfrac{\partial \phi}{\partial \theta} \wedge \dfrac{\partial \phi(\pi/2, 0)}{\partial z}||} = (0, 1, 0)$$ He then now says that $\phi$ is compatible with $\omega$ hence the integral is $$\dfrac{1}{3}\int \int_{[0, 2\pi]\times [-1, 3]}\left( \sqrt{z^2+1}\cos\theta + \sqrt{z^2+1}\sin\theta - z)\right) \text{d}\theta\text{d}z = -\dfrac{1}{3}\int\int_{\ldots}z \text{d}\theta\text{d}z = -\dfrac{8\pi}{3}$$ Now my questions It's all clear until we need to calculate the norm of the cult of $\phi$ , then its blackout. 1) I found NOWHERE that the compatibility between $\omega$ and $\omega(\phi)$ has to be verified through the ration between the cult of $\phi$ and its norm. So why do we have to do this? 2) Once we verified the compatibility... then what? I mean I do not need to know that $\omega(0, 1, 0) = \omega(\phi(\pi/2, 0)) = (0, 1, 0)$ do I? So why do I have to do this? Thank you very much for your time, those are really critical points for me to understand... Updates I understood that the proof of the compatibility is irrelevant for the exercise. So it remains the first question: why does that method tell me that they are compatible?","['integration', 'surface-integrals', 'curl', 'multivariable-calculus', 'parametrization']"
3708296,Indefinite integral:$\int \cos(2018x)\sin^{2016}(x)dx$,"Evaluate $\int \cos(2018x)\sin^{2016}(x)dx$ I could solve this using IBP, $$I=\int \cos(2017x+x)\sin^{2016}(x)dx$$ $$=\int \cos(2017x)\sin^{2016}(x) \cos(x)dx -\int \sin^{2017}(x)\sin(2017x)$$ $$=\frac{\cos(2017x)\sin^{2017}(x)}{2017} +\int \frac{2017\sin^{2017}(x)\sin(2017x)}{2017}dx - \int \sin^{2017}(x)\sin(2017x)$$ $$=\frac{\cos(2017x)\sin^{2017}(x)}{2017}+c$$ However I while trying to solve this question using complex numbers, I didn't obtain the final result. Here's what I did: The give integral is $\int e^{2018ix} (\frac{e^{ix}-e^{-ix}}{2i})^{2016} dx$ (considering real of this and in subsequent steps) $$=\frac{1}{2^{2016}} \int e^{2ix}(e^{2ix}-1)^{2016} dx$$ . $e^{2ix}-1=t$ , $e^{2ix}2idx=dt$ $$=\frac{1}{2^{2016}} \int t^{2017} dt/2i$$ . $$=\frac{t^{2017}}{2^{2017}i \cdot 2017}+c$$ So answer is $$-Im(\frac{t^{2017}}{2^{2017} \cdot 2017})$$ I'm unable to evaluate $t^{2017}=(e^{2ix}-1)^{2017}$ and get it to the form as obtained by IBP.
I did the binomial expansion however, I wasn't able to get it to a nice form. Also is there a generalisation to this problem? Can $\int \cos(mx) \sin^{n}(x) dx$ also be evaluated like this?(not by using reduction formula)","['integration', 'complex-analysis', 'calculus', 'indefinite-integrals', 'complex-integration']"
3708343,Prove that quadratic form is differentiable from the definition,"I am trying to show that a quadratic form $Q: \mathbb{R}^n \to \mathbb{R}, Q(x)=x^T A x$ is differentiable from the definition of the differential. I started by considering $Q(x+h)=(x+h)^T A (x+h)=x^T A x + x^T A h + h^T A x + h^T A h$ . We need to find a linear map $L_Q: \mathbb{R}^n \to \mathbb{R}$ s.t. $\lim \limits_{h \to 0} \frac{Q(x+h)-Q(x)-L_Q(h)}{\|h\|}$ . Note that $x^T (A + A^T) h = \langle x, (A + A^T) h \rangle$ is a linear map, so this is my candidate for the differential of $Q(x)$ , but I am struggling to show that the error term $r_Q(x)=h^T A h$ decays sublinearly. What I tried was to use the CS inequality to show that $\frac{|r_Q(x)|}{\|h\|} = \frac{|\langle h, Ah\rangle|}{\|h\|} \leq \frac{\|h\| \|Ah\|}{\|h\|} = \|Ah\|$ , but I don't see how I can show that the right-hand side has a limit of 0. Can someone please tell me if I am on the right track and give me some guidance? This is a problem in my lecture notes right after the definiton of the differential, so it should be able to solve this without much more knowdledge. Thanks a lot!","['derivatives', 'linear-transformations', 'real-analysis']"
3708375,Can the cross section of parallelepiped be a regular pentagon,"Came across this question in a children's recreational mathematics book. Apparently, the cross section of a cube cannot be a regular pentagon. It could be a irregular pentagon though. But if we generalize this problem, can the cross section of parallelepiped be a regular pentagon? How do we prove that?","['euclidean-geometry', 'cross-sections', 'geometry']"
3708408,Use the Chinese Remainder Theorem to determine the value of $x$. [duplicate],"This question already has answers here : Smallest integer which leaves remainder 3, 2, 1 when divided by 17, 15, 13 (3 answers) Closed 5 days ago . I'm trying to solve the following modular arithmetic question using the Chinese Remainder Theorem, using this link . (We learned a different method in our class, but I found this easier to grasp). $$x \equiv 1 (\text{mod} \ 5)$$ $$x \equiv 2 (\text{mod} \ 7)$$ $$x \equiv 3 (\text{mod} \ 9)$$ $$x \equiv 4 (\text{mod} \ 11)$$ I then represented $x$ as a sum of $4$ boxes, such that the first term is ""related"" to $\text{mod} \ 5$ (i.e. the $1^{st}$ term will not be made $0$ due to the $\text{mod} \ 5$ ), the second term is related to $\text{mod} \ 7$ and so on. Here's what I mean by ""related"": If we only consider $\text{mod} \ 5$ , the value of box $1$ is $693$ , the value of box $2$ is $495$ , then $693 \  \text{mod} \ 5 = 3$ but $495 \  \text{mod} \ 5 = 0$ . Likewise, if we only consider $\text{mod} \ 7$ , then the value of box $1$ is $693 \  \text{mod} \ 7 = 0$ but $495 \  \text{mod} \ 7=5$ . And so on... After doing all that, I have $$x = (7 \times 9 \times 11) + (5 \times 9 \times 11) + (5 \times 7 \times 11) + (5 \times 7 \times 9)$$ The next step is applying the $\text{mod} \ 5$ to $x$ : $$\begin{align}
x \ \text{mod} \ 3 &\equiv 691 \ \text{mod} \ 5 + 495 \ \text{mod} \ 5 + 385 \ \text{mod} \ 5 + 315 \  \text{mod} \ 5 \\ &\equiv 693 \ \text{mod} \ 5 + 0 + 0 + 0 \\ &\equiv 693 \ \text{mod} \ 5 \\ &\equiv 3 \ (\text{mod} \ 5)
\end{align}$$ This is where I get stuck. In the video, and the video doesn't explain how to deal with such a scenario. PS - If there is a more ""intuitive"" or more efficient version of the Chinese Remainder Theorem, I'd be grateful if you could share it. PPS - Sorry if the question is a bit awkwardly formulated. As you can guess this is my first doing this.","['elementary-number-theory', 'chinese-remainder-theorem', 'modular-arithmetic']"
3708469,Operation that returns a unique result for each unordered set of numbers,"What operation $f$ can I apply to any two numbers $a$ & $b$ , such that $$f(a,b) = f(b,a)$$ where $f(a,b)$ is unique for any combination of a & b in the set of whole numbers? P.S. I'm really not sure what tag to use here, I'd appreciate someone adding the correct one. A word on why : I have a database table with two columns $a$ and $b$ .
I'd like to ensure that there are no duplicate rows in my table.
However, I don't care about the order of $a$ and $b$ . a | b
--+--
1 | 2
2 | 1 Is effectively a duplicate.",['elementary-set-theory']
3708545,A conjectured upper bound for $\left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n$ and $x\geq 1$,Hi I have (related https://mathoverflow.net/questions/337457/prove-that-left-fracxn1xn-11-rightn-left-fracx12-rightn ): Let $x\geq 1$ a real number and $n\geq 2$ a natural number then we have : $$\Big(\frac{x^{n+1}+1}{x+1}\Big)\Bigg(\frac{\Big(\frac{x+1}{2}\Big)^n}{\Big(\frac{x^2+1}{x+1}\Big)^n}+\frac{(x^n+1)^2}{(x^{n-1}+1)(x^{n+1}+1)}\Bigg)\geq \left(\frac{x^n+1}{x^{n-1}+1}\right)^n+\left(\frac{x+1}{2}\right)^n$$ It's a conjecture so I have tested for $n\leq 20$ . Maybe it works also for $n$ a positive real number . If it's true I think we can make something similar to the Peter Mueller's answer. For the case $n=3$ here we have the derivative . Clearly the numerator (of the derivative) is positive for $x\geq 1$ .But in some way it's not so easy with the general case . So how to solve it ? Any helps is greatly appreciated. Thanks in advance for all your contribution .,"['conjectures', 'examples-counterexamples', 'real-analysis', 'inequality', 'derivatives']"
3708567,Determine unknown probability by observing results,"Disclaimer: Sorry, i'm a self-taught non-native, apart from basics, i don't know the proper terms. I'm pretty sure there has to be a part of statistics that would help me deal with my problem, but somehow i'm unable to word my question well enough for google to be helpful. What if i'm observing fake coin being tossed. Say the coin's heads-chance is some random number (could be anything from 0% to 100%, all numbers equal...i hope uniform distribution is the correct term). Now i saw the coin being flipped X times (could be 10, 100 or millions) and get heads Y times.
Now obviously i could say ""This is most probably Y/X head-chance coin"", but what if i wanted something like a histogram? Something that would let me say ""I'm 90% sure that coins head-chance is between 60% and 80%, 4% sure it's between 80% and 90%, 4% sure it's between 50% and 60% and so on..."" I'd appreciate even non-complete answers or pointers, ""Go study X-E-omega-effect"" or such would be helpful as well.","['statistics', 'parameter-estimation', 'probability']"
3708582,Toothpaste tube parametrization,"A flexible cylindrical tube is distorted by flattening/pinching at one end along radius and leaving the other end circular. So it is isometric to a cylinder. Eccentricity is zero at the circular section end and 1 at the pinched end if at all the transitioning sections would be ellipses. A pde describing a class of developable surfaces is: $$rt-s^2=0,\quad r= \dfrac{\partial ^2 z}{\partial x^2},\;t= \dfrac{\partial ^2 z}{\partial y^2},\; s= \dfrac{\partial ^2 z}{\partial x\partial y};$$ It is a second degree Monge-Ampère elliptic partial differential equation. What is an orthogonal mapping between the cylinder and the Toothpaste tube surface meshes and what is its parametrization ?","['partial-differential-equations', 'differential-geometry']"
3708585,Why don't we just take the root of the numerator instead of taking the root of the whole thing in the expression for standard deviation?,The equation of the standard deviation of a dataset is given by $\sqrt{\frac{\sum{(x_{i} - \bar{x}})^2}{N}}$ . Why is that the case and why can't we use $\frac{\sqrt{\sum{(x_{i} - \bar{x}})^2}}{N}$ instead? The units line up and we don't have to worry about negatives in this case too. Thanks a million in advance!,"['statistics', 'variance', 'standard-deviation']"
3708633,Type of singularity in $\frac{1}{1-e^z}$,"My complex analysis textbook uses the following definitions for removable singularity, essential singularity and for poles of a complex function: If $a$ is an isolated singularity of $f$ and the numbers $c_n$ for $n \in \mathbb{Z}$ are the coefficients in the Laurent series of the function, $\sum_{n\in\mathbb{Z}} c_n(z-a)^n$ , then: $a$ is a removable singularity if $c_n = 0$ for $n < 0$ $a$ is a pole of order $m \in \mathbb{N}$ if $c_{-m} \neq 0$ and $c_n=0$ for $n < -m$ $a$ is an essential singularity if $c_n \neq 0$ for an infinite number of negative values of $n$ Then, based on this definition I am supposed to classifies the singularity of the function $f(z)= \frac{1}{1-e^z}$ at the point $z = 0$ So, first I began by finding the Laurent series of this funcion: $$\begin{align}
\frac{1}{1-e^z}&=\sum_{n \geq0}e^{zn}
\end{align}$$ Because we have that $e^z=\sum_{k \geq 0}\frac{1}{k!}z^k$ . So if we let $z=zn$ then we get: $$\begin{align}
\sum_{n \geq0}e^{zn} &= \sum_{n \geq0} \sum_{k \geq 0}\frac{n^k}{k!}z^k
\\
\\
&= \sum_{k \geq0}\  \underbrace{ \sum_{n \geq 0} \frac{n^k}{k!}}_{:=a_k}\ z^k
\\
\\
&= \sum_{k \geq0} a_k z^k
\end{align}$$ So, according to the definition my book gave, this is a removable singularity. But when I checked the solutions it said that $z=0$ is a pole of order $1$ . So did I do something wrong and if so what did I do wrong or is the did the book author make a mistake?","['complex-analysis', 'singularity']"
3708667,Proving that the boundary of all compact $1-$ manifolds have an even number of points.,"It is well-known that if $M$ is a connected $1-$ manifold, then it is diffeomorphic to either $[0,1]$ , $[0,1)$ , $(0,1)$ , or $\mathcal{S}^{1}$ . It is often stated as a trivial corollary that if, in addition, $M$ is compact, then the boundary $\partial{M}$ has an even number of points. My question is how this follows from the above statement. It is clear that if $M$ is compact, then that limits the choices which $M$ can be diffeomorphic to to either $[0,1]$ or $\mathcal{S}^{1}$ . So let's suppose that it is the case where $M$ is diffeomorphic to $[0,1]$ through the map $\phi:[0,1] \rightarrow M$ . Now, it's clear that $(\phi, [0,1])$ is not sufficient to be a chart for every point in $M$ , because $[0,1]$ is not open in either $\mathbb{R}$ nor the halfspace $H^{1}$ . Regardless, $M$ is a $1-$ manifold by way of some charts by assumption, and hence we can talk about $int(M)$ and $\partial{M}$ . We know from standard results that $int(M)$ is open in $M$ and $\partial{M}$ is closed in $M$ , and that both sets are disjoint from each other, so we can say that $$\phi: [0,1] \rightarrow int(M) \cup \partial{M}$$ is a diffeomorphism. But at this point I am a bit stuck. I feel like we should be able to prove that $\partial{M}=\phi(\{0,1\})$ , and hence $\partial{M}$ consists of $2$ points, an even number, but I can't seem to make any headway here. Can anybody point me in the right direction? Thanks!","['general-topology', 'differential-topology', 'differential-geometry', 'real-analysis']"
3708674,What does it mean for an ODE to be conservative?,"What does it mean for an ODE to be conservative? For example, I already read somewhere that the equation $$w\cdot y''-y+y^{2k+1}=0,$$ with $w>0$ and $k\in \mathbb{N}$ constants fixeds, is conservative. In practice, what does this mean?","['calculus', 'ordinary-differential-equations']"
3708692,"Suppose $b$ is the smallest element of $B$. Then $b$ is also a minimal element of $B$, and it is the only minimal element.","Working on the book: Daniel J. Velleman. ""HOW TO PROVE IT: A Structured Approach, Second Edition"" (p. 206) Suppose $R$ is a partial order on a set $A$ , and $B \subseteq A$ .
  Suppose $b$ is the smallest element of $B$ . Then $b$ is also a minimal element
  of $B$ , and it is the only minimal element. I proved that $b$ is a minimal element of $B$ using the property of antisymmetry ( $R$ is a partial order ). Now, I will prove uniqueness using Fitch-style Natural Deduction system. $
\def\fitch#1#2{\quad\begin{array}{|l}#1\\\hline#2\end{array}}
\def\Ae#1{\qquad\mathbf{\forall E} \: #1 \\}
\def\Ai#1{\qquad\mathbf{\forall I} \: #1 \\}
\def\Ee#1{\qquad\mathbf{\exists E} \: #1 \\}
\def\Ei#1{\qquad\mathbf{\exists I} \: #1 \\}
\def\R#1{\qquad\mathbf{R} \: #1 \\}
\def\ci#1{\qquad\mathbf{\land I} \: #1 \\}
\def\ce#1{\qquad\mathbf{\land E} \: #1 \\}
\def\oi#1{\qquad\mathbf{\lor I} \: #1 \\}
\def\oe#1{\qquad\mathbf{\lor E} \: #1 \\}
\def\ii#1{\qquad\mathbf{\to I} \: #1 \\}
\def\ie#1{\qquad\mathbf{\to E} \: #1 \\}
\def\be#1{\qquad\mathbf{\leftrightarrow E} \: #1 \\}
\def\bi#1{\qquad\mathbf{\leftrightarrow I} \: #1 \\}
\def\qi#1{\qquad\mathbf{=I}\\}
\def\qe#1{\qquad\mathbf{=E} \: #1 \\}
\def\ne#1{\qquad\mathbf{\neg E} \: #1 \\}
\def\ni#1{\qquad\mathbf{\neg I} \: #1 \\}
\def\IP#1{\qquad\mathbf{IP} \: #1 \\}
\def\x#1{\qquad\mathbf{X} \: #1 \\}
\def\DNE#1{\qquad\mathbf{DNE} \: #1 \\}
$ $
\fitch{
1.\, b \in B \land \forall x(x \in B \to bRx) \qquad \textit{b is the smallest element of B}\\
2.\,b \in B \land \neg \exists x(xRb \land x \neq b) \qquad \textit{b is a minimal element of B}
}{
  3.\,\neg \exists x(xRb \land x \neq b) \ce{2}
  \fitch{4.\, c \in B \land \neg \exists x(xRc \land x \neq c)}{
	  5.\,c \in B \ce{4}
	  6.\,\neg \exists x(xRc \land x \neq c) \ce{4}
	  \fitch{7.\, b \neq c}{
	    8.\,\forall x(x \in B \to bRx) \ce{1}
	    9.\,c \in B \to bRc \Ae{8}
	    10.\,bRc \ie{9,5}
	    11.\,bRc \land b \neq c \ci{10,7}
	    12.\,\exists x(xRc \land x \neq c) \Ei{11}
	    13.\,\bot \ne{6,12}
}\\
14.\,b = c \IP{7-13}
  }\\
  15.\, \forall z((z \in B \land \neg \exists x(xRz \land x \neq z)) \to b = z) \Ai{14}
  16.\,b \in B \land \neg \exists x(xRb \land x \neq b) \land \forall z(z \in B \land \neg \exists x(xRz \land x \neq z)) \to b = z) \ci{2,15}
}
$ Is my uniqueness proof correct ? EDIT: This rules of inference can be found in Appendix C of this book: forallx: An Introduction to Formal Logic","['logic', 'relations', 'solution-verification', 'discrete-mathematics', 'natural-deduction']"
3708702,Solving $f(f(f(x))) = 17$ when $f(x)=x\{x\}$,"Can anybody help me in how to approach this problem.
I expanded the fractional part of $x$ and tried to simplify but nothing is happening it is not coming in any format. For real number $x$ let $\lfloor x \rfloor$ be the greatest integer less than or equal to $x$ , and define $\{x\}=x-\lfloor x \rfloor$ to be the fractional part of $x$ . For example, $\{3\} = 0$ and $\{4.56\}=0.56$ . Define $f(x)=x\{x\}$ , and let $N$ be the number of real-valued solutions to the equation $f(f(f(x))) = 17$ for $0\le x\le 2020$ . Find the remainder when $N$ is divided by $1000$ .","['fractional-part', 'ceiling-and-floor-functions', 'functions']"
3708742,Proving $\lim_{n\to\infty}\sum_{k=qn}^{pn}\frac1k=\log\frac{p}{q}$,"This is a question from Apostol Vol 1, it states: If $p$ and $q$ are fixed integers, $p≥q≥1$ , show that $$\lim_{n\to\infty}\sum_{k=qn}^{pn}\frac1k=\log\frac{p}{q}$$ It seems likely to me that this sum must be somehow connected to the integral $\int_q^p\frac1xdx$ . I have tried to convert the sum so that it adopts the typical form of a Riemann sum, but I have been unable to. Another possibility that I thought of was using the integral estimate for infinite series, but again, this hasn't yielded anything. How should I approach the problem?","['limits', 'calculus', 'sequences-and-series', 'real-analysis']"
3708863,Linear transformation bounded iff its kernel is closed in infinite-dimensional Banach spaces,"I am working on Problem 8, Chapter 6, in Luenberger's Optimization by Vector Space Methods . It states: ""Show that a linear transformation mapping one Banach space into another is bounded if and only if its nullspace is closed."" I am having a bit of trouble with the converse. In particular, if we let $f:X \rightarrow Y$ be a linear transformation, Luenberger doesn't assume that either $X$ or $Y$ be finite-dimensional. Do you have any idea on how to proceed? I have thought (without success) of considering the quotient space $\hat{X}/\ker f$","['general-topology', 'linear-algebra', 'functional-analysis']"
3708866,Is the generalization $\sum_{n=1}^\infty\frac{H_{\frac np}}{n^q}$ known in the literature?,"I managed to derive the following generalization $$\sum_{n=1}^\infty\frac{H_{\frac np}}{n^q}=(-1)^qp \sum_{n=1}^\infty\frac{H_{pn}}{(pn)^q}-\sum_{j=1}^{q-2}(-p)^{-j}\zeta(q-j)\zeta(j+1)\tag1$$ and wondering if its known in the literature or not. One of the nice applications is $$\sum_{n=1}^\infty\frac{H_{\frac n2}}{n^{2q}}=\left[1+2^{-2q-1}(2q+1)\right]\zeta(2q+1)-\frac12\sum_{j=1}^{2q-2}\left[1-(-2)^{1-j}\right]\zeta(j+1)\zeta(2q-j)$$ $$+\sum_{j=1}^{q-1}\left[1-2^{1-2j}\right]\zeta(2j)\zeta(2q-2j+1)\tag2$$ which follows from setting $p=2$ and replacing $q$ by $2q$ then substituting the two generalizations: $\sum_{n=1}^\infty\frac{(-1)^{n}H_n}{n^{2q}}$ and $\sum_{n=1}^\infty \frac{H_n}{n^q}$ . Another application is $$\sum_{n=1}^\infty\frac{H_n}{n^{q}}=-\frac12\sum_{i=1}^{q-2}(-1)^i\zeta(q-i)\zeta(i+1),\quad q=3,5,7,...$$ which follows from setting $p=1.$ Thanks Here is the proof of $(1)$ . $$\sum_{n=1}^\infty\frac{H_{\frac np}}{n^q}=\frac1p\sum_{n=1}^\infty\frac{1}{n^{q-1}}\left(\frac{H_{\frac np}}{\frac np}\right)$$ use that $\int_0^1 x^{n-1}\ln(1-x)dx=-\frac{H_n}{n}$ $$=\frac1p\sum_{n=1}^\infty\frac{1}{n^{q-1}}\left(-\int_0^1 x^{\frac np-1}\ln(1-x)dx\right)$$ $$=-\frac1p\int_0^1\frac{\ln(1-x)}{x}\sum_{n=1}^\infty\frac{x^{\frac np}}{n^{q-1}}dx$$ $$=-\frac1p\int_0^1\frac{\ln(1-x)}{x}\text{Li}_{q-1}(x^{\frac 1p})dx$$ set $x^{\frac 1p}=y$ $$=-\int_0^1 \frac{\ln(1-y^p)\text{Li}_{q-1}(y)}{y}dy$$ expand $\ln(1-y^p)$ in series $$=\sum_{n=1}^\infty\frac{1}{n}\int_0^1 y^{pn-1}\text{Li}_{q-1}(y)dy$$ integrate by parts repeatedly $$=\sum_{n=1}^\infty\frac 1n\left((-1)^q\frac{H_{pn}}{(pn)^{q-1}}-\sum_{j=1}^{q-2}(-1)^j\frac{\zeta(q-j)}{(pn)^j}\right)$$ $$=(-1)^qp\sum_{n=1}^\infty\frac{H_{pn}}{(pn)^q}-\sum_{j=1}^{q-2}(-p)^j\zeta(q-j)\left(\sum_{n=1}^\infty\frac{1}{n^{j+1}}\right)$$ $$=(-1)^qp\sum_{n=1}^\infty\frac{H_{pn}}{(pn)^q}-\sum_{j=1}^{q-2}(-p)^j\zeta(q-j)\zeta(j+1).$$ By the way, using $(2)$ , we can also find the two useful generalizations: $$\sum_{n=1}^\infty\frac{(-1)^nH_{\frac n2}}{n^{2q}}=\left[2^{-2q-1}(2q+3)-1\right]\zeta(2q+1)$$ $$-\frac12\sum_{j=1}^{2q-2}\left[2^{1-2q}-1+(-2)^{1-j}\right]\zeta(2q-j)\zeta(j+1)-\sum_{j=1}^{q-1}\left[1-2^{1-2j}\right]\zeta(2j)\zeta(2q-2j+1)$$ and $$\sum_{n=1}^\infty\frac{(-1)^n \overline{H}_n}{n^{2q}}=\left[q-2^{-2q-1}(2q+1)\right]\zeta(2q+1)+\left[2^{1-2q}-2\right]\ln(2)\zeta(2q)$$ $$-\frac12\sum_{j=1}^{2q-2}(-2)^{1-j}\zeta(j+1)\zeta(2q-j)-\sum_{j=1}^{q-1}\left[1-2^{1-2j}\right]\zeta(2j)\zeta(2q-2j+1)$$","['integration', 'reference-request', 'harmonic-numbers', 'closed-form', 'sequences-and-series']"
3708939,$\sigma$ algebra question,"So I'm learning a little bit of measure theory currently, and I am a little in the dark as to the motivation for the definition of the $\sigma$ algebra. My biggest question is why the $\sigma$ algebra must be used rather than the power set (since a $\sigma$ algebra is a proper subset of the power set, right?). I had read somewhere that it has something to do with the Banach-Tarski paradox, but I am unsure why if that is the case. Can someone help shed some light on this for me? Thanks!","['measure-theory', 'set-theory']"
3708953,Definition of product of two random variables,"Suppose you have two random variables $X:\Omega \to \mathbb{R}$ and $Y:\Omega \to \mathbb{R}$ which are not necessarily independent. How is the product $XY$ defined and how do I calculate the image of the new random variable $Z:=XY$ ? In the context of covariance of finite random variables we are told to simply multiply all elements of each image, $X(\Omega)$ and $Y(\Omega)$ , with each other. However, we are not provided with any formal definition. So I have no idea what to do when we have non-finite random variables. Which makes it even more confusing is that the professor uses the following notation when he states a Lemma about the expected value of $XY$ when both random variables are independent: Lemma: The expected value, $\mathbb{E}(XY)$ exists iff $\sum\limits_{\omega\in \Omega}|X(\omega)Y(\omega)|p(\omega)<\infty$ . Proof $\sum\limits_{\omega\in \Omega}|X(\omega)Y(\omega)|p(\omega)=\sum\limits_{x\in X(\Omega)}\sum\limits_{y\in Y(\Omega)}|xy|\sum\limits_{\omega: X(\omega)=x, Y(\omega)=y}p(\omega)= $ ... So here it seems that the expression $XY$ is treated like the multiplication of two real-valued functions. Can someone give me an explanation of $XY$ ?","['covariance', 'definition', 'probability-theory', 'probability', 'random-variables']"
3708961,Poincare Bendixson theorem,"Poincare-Bendixson states that if the $\omega$ -limit set of the trajectory doesn't contain critical point, then the $\omega$ -limit set is a periodic orbit, I am just wondering if there is an example showing that $\omega$ -limit set contains both critical point and periodic orbit",['ordinary-differential-equations']
3708973,Does $\left|A\right|=\left|\mathbb{N}\right|$ and $\left|\mathbb{N}\right|=\left|\mathbb{Z}^+\right|$ imply $\left|\mathbb{Z}^+\right|=\left|A\right|$,"Suppose $A$ is a countably infinite set. Is it true that if $\left|A\right|=\left|\mathbb{N}\right|$ and $\left|\mathbb{N}\right|=\left|\mathbb{Z}^+\right|$ , then $\left|\mathbb{Z}^+\right|=\left|A\right|$ ? It can be show by the Cantor-Bernstein theorem that $|\mathbb{N}|=|\mathbb{Z}^+|$ , as \begin{align}
f&:\mathbb{N}\hookrightarrow\mathbb{Z}^+, \ \text{as} \ f(x)=x+1\implies \left|\mathbb{N}\right|\leq\left|\mathbb{Z}^+\right| \\
g&:\mathbb{Z}^+\hookrightarrow\mathbb{N}, \ \text{as} \ g(x)=x-1\implies \left|\mathbb{Z}^+\right|\leq\left|\mathbb{N}\right|.
\end{align} If we know $\left|A\right|=\left|\mathbb{N}\right|$ and $\left|\mathbb{N}\right|=\left|\mathbb{Z}^+\right|$ , can we deduce that $\left|\mathbb{Z}^+\right|=\left|A\right|$ ?","['elementary-set-theory', 'real-analysis']"
3709072,"""Partition"" without the disjoint condition","A partition of a set $A$ is defined as a set of pairwise disjoint sets whose union is $A$ . I'm interested in a related concept, where for a set $A$ you have $Q = \{A_1 \ldots A_n\}$ such that union of all $A_i$ is $A$ but $A_i$ needn't be pairwise disjoint. I'm looking for standard term for this concept so I can look up further literature on it. The term will read as "" $Q$ is the superpartition for $A$ "", with superpartition being replaced by the actual term.","['elementary-set-theory', 'terminology']"
3709099,"Is $B^s_{p,p} = W^{s,p}$?","In Adams & Fournier's book ""Sobolev Spaces"" page 255, there are assertions: $$ W^{s,p} = F^{s}_{p,2}
\\
B^s_{p,p} = F^s_{p,p}$$ Note: Here $s>0$ , $1\leq p<\infty$ , and the fractional order Sobolev spaces are defined as complex interpolation spaces $W^{s,p} = [L^p , W^{m,p} ] _{s/m}$ where $m$ is the smallest integer greater than $s$ . AFAIK, the above definition is equivalent to the following definition (at least for $0<s<1$ ), employing Gagliardo seminorms: $f \in W^{s,p}$ iff $f \in W^{\left \lfloor{s}\right \rfloor ,p}$ and $[D^\alpha f] := (\int \frac{|D^{\alpha}f(x)-D^{\alpha}f(y)|^p } {|x-y|^{(s-\left \lfloor{s}\right \rfloor)p + n}} dxdy)^{1/p} < \infty$ for all $|\alpha|=\left \lfloor{s}\right \rfloor$ . The Triebel-Lizorkin spaces and the Besov spaces are defined by Paley-Littlewood decompositions. In Triebel's book ""Interpolation theory, function spaces, differential operators(1978)"", page 169,
the fractional order Sobolev spaces are defined as $W^{s,p} = B^s_{p,p}$ for $s>0$ non-integer. Therefore, the two monographs are coherent only when $p=2$ . Could someone explain about this situation? My background: I have little experience and knowledge of function spaces. To get some knowledge, I am starting to have a brief overview of the famous monographs. However, the problem described above makes me very confused. Any bits of help are welcome. Thank you!!","['sobolev-spaces', 'functional-analysis', 'partial-differential-equations']"
3709105,epsilon-delta proofs: can delta be defined in terms of both epsilon and x?,"I'm trying to construct a proof for the following limit: $\lim_{x \to -2} \frac{2x+5}{x+4} = \frac{1}{2} $ I know I need to show that for $ \epsilon > 0$ , $\exists \delta > 0$ such that if $0 < |x-(-2)| < \delta$ , $\left|\frac{2x+5}{x+4} - \frac{1}{2}\right| < \epsilon$ Here's the steps I took that led me to my question: $\left|\frac{2x+5 - \frac{1}{2}(x+4)}{x+4}\right| < \epsilon$ $\left|\frac{2x+5 - \frac{1}{2}x - 2}{x+4}\right| < \epsilon$ $\left|\frac{\frac{3}{2}x +3}{x+4}\right| < \epsilon$ $\frac{3}{2} * \left|\frac{x+2}{x+4}\right| < \epsilon$ I know I want to get this value in terms of just $x+2$ so that I can pick my delta value in terms of epsilon, since the limit is defined as $-2$ . Since I have that $x+2$ sitting right in the denominator (and this professor is not sadistic so I don't think he would try to make this overly difficult), I'm wondering if it's ""legal"" for me to pull the whole $|x+4|$ out of the denominator, and do something like: $\frac{3}{2(x+4)} * \left|x+2\right| < \epsilon$ and then make $\delta = \frac{\epsilon}{\frac{3}{2x+8}} = \frac{\epsilon(2x+8)}{3} $ If I'm not allowed to do this, does anyone have suggestions about where I might go with it next? I'm really not sure how else to get rid of the denominator. Thanks very much!","['limits', 'proof-explanation', 'epsilon-delta', 'real-analysis']"
3709113,A nonempty set is countable iff it is the image of a function whose domain is a nonempty countable set,"A nonempty set is countable iff it is the image of a function whose domain is a nonempty countable set Attempt: Let $A$ be non empty and countable. If $A$ is countably infinite, we can find a bijection $f: \mathbb{N} \to A$ . If I understand the problem correctly, I ${\bf need}$ to build a function $g: B \to A $ where $B$ is countable and $g(B) = A$ . But, I have no idea how to proceed ? Can someone lead into the right direction? The converse I believe is more manageable. Suppose there is some map whose domain is countable set then the image is also countable either finite or infinite... qed I am still stuck on $\implies$ direction.","['elementary-set-theory', 'solution-verification', 'real-analysis']"
3709133,Unable to explain flow of steps in this basic modular expression?,"Consider the expresion $(13 + 11)· 18 (\mod 7)$ : $(13+11)· 18 ≡ (6+4)· 4 (\mod 7)$ Note the transition from $(13+11)·
> 18$$\implies$ $(6+4)· 4$ $≡ 10 · 4 (\mod 7)$ $≡ 3 · 4 (\mod 7)$ Note the transition from $10 · 4$$\implies$ $3 · 4$ $≡ 12 (\mod 7)$ $≡ 5 (\mod 7)$ $≡ 5$ These 2 transitions involve subtracting 7, but in each case they were either the factor( the $10$ going to $7$ in the 2nd transition) or a component of a factor( the $13$ and $11$ going to $6$ and $4$ in first transition). I would have understood if they subtracted 7 from the product itself( like the $12$ going to $7$ in the final step) because I can intutively understand that the equivalence that both sides have the same remainder when divided by 7 still holds. I didn't get how this was possible( there wasn't any law/theorem stating you could do that). Few pages down , I saw this corollary: $ab ≡ [(a \mod n)(b \mod n)](\mod n)$ Is the transitions some result of the corollary or is there some knowledge I'm lacking entirely to explain those transitions?","['modular-arithmetic', 'discrete-mathematics']"
3709190,"Calculate $\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv$","$$\iiiint_{x^2+y^2+u^2+v^2\leq 1}e^{x^2+y^2-u^2-v^2}\,dx\,dy\,du\,dv$$ So we just learned substitution and i thought maybe for this integral doing 2 polar subs for x,y and for u,v but i'm not sure this is the right way for this. Any hints will be welcome","['integration', 'multivariable-calculus', 'multiple-integral', 'fubini-tonelli-theorems']"
3709277,Question about Zariski density and polynomials with full Galois group,"Let $A_3\subseteq {\mathbb Q}^4$ be the sets of all $q=(q_3,q_2,q_1,q_0)$ such that $P_q=X^4+q_3X^3+q_2X^2+q_1X+q_0$ has no rational root. Let $A_2 \subseteq A_3$ be the subset of all $q$ 's such that $P$ is irreducible over $\mathbb Q$ . Let $A_1 \subseteq A_2$ be the sub-subset of all $q$ 's such that $P$ has Galois group $S_4$ over $\mathbb Q$ . My questions : Is $A_1$ Zariski-dense in $A_2$ ? Is $A_2$ Zariski-dense in $A_3$ ? My thoughts : $B=\cap_{r\in {\mathbb Q}} \lbrace q \ | \ P_q(r)\neq 0 \rbrace$ is a countable intersection of open sets, so it is probably not open or closed.","['galois-theory', 'algebraic-geometry', 'polynomials']"
3709300,How to solve without using trig?,"In the diagram, five identical squares have been placed together. What is $\angle ABC$ ? It's easy with trig but can't find an answer without using it. Thanks!","['euclidean-geometry', 'trigonometry', 'angle', 'geometry']"
3709310,Calculate $\iint\frac{dxdy}{(1+x^2+y^2)^2}$ over a triangle,"Calculate $$\iint\frac{dxdy}{(1+x^2+y^2)^2}$$ over the triangle $(0,0)$ , $(2,0)$ , $(1,\sqrt{3})$ . So I tried changing to polar coordinates and I know that the angle is between $0$ and $\frac{\pi}{3}$ but I couldn't figure how to set the radius because it depends on the angle.","['integration', 'multivariable-calculus', 'polar-coordinates', 'multiple-integral', 'fubini-tonelli-theorems']"
3709348,"Integral of $\int^{\infty}_0 \frac{e^{-x}}{x^s+1}\,dx$","Related information Integral of $\int^{\infty}_0 \frac{x^n}{x^s+1}dx$ This is an integral very similar to the gamma function integral: $$R(s)=\int^{\infty}_0 (1+x^s)^{-1} e^{-x}\,dx$$ i want to find the function $R$ . I do know some values of $R$ : $$R(0)=1$$ $$R\left(\frac{1}{2}\right)=\frac{-\pi \text{erfi}(1)+\text{Ei}(1)+e \sqrt \pi}{e}$$ $$R(1)= -e\text{Ei}(-1)$$ $$R(2) = \text{Ci}(1)\sin(1)-\text{Si}(1)\cos(1)+\frac{1}{2}\pi\cos(1)$$ Can any of you provide hints or solutions? 
Also, thanks to an answer by Sewer we know that: $$\lim_{s \to \infty}R(s)=1$$","['integration', 'calculus', 'gamma-function']"
3709356,How many language families are there?,"Let’s define a finite transducer as a $5$ -tuple $(Q, A, B, \phi, \psi)$ , where $Q$ is a finite collection of states , $A$ is a finite input alphabet , $\phi: Q\times A \to Q$ is the transition function and $\psi: Q \times A \to B^*$ is the output function . Any transducer defines a transducer function $\overline{\psi}: Q\times A^* \to B^*$ described by the following recurrence: $\overline{\psi}(q, \Lambda) = \Lambda$ , where $\Lambda$ is the empty word. $\overline{\psi}(q, a \alpha) = \psi(q, a) \overline{\psi}(\phi(q, a), \alpha)$ , where $a \in A$ , $\alpha \in A^*$ . Let’s call a function $f: A^* \to B^*$ a regular transduction iff $\exists$ a finite transducer $(Q, A, B, \phi, \psi)$ and an initial state $q \in Q$ , such that $\forall \alpha \in A^*$ we have $f(\alpha) = \overline{\psi}(q, \alpha)$ . Now, let’s call a set of languages $\mathfrak{F}$ over a finite alphabet $A$ ( $|A| > 2$ ) a family iff it satisfies two properties. 1) $\forall L_1, L_2 \in \mathfrak{F} L_1 \cup L_2 \in \mathfrak{F}$ 2) $\forall L \in \mathfrak{F}$ and $\forall$ regular transductions $f$ $f(L) \in \mathfrak{F}$ . My question is: How many language families are there? As $|A^*| = \aleph_0$ , then there are $2^{\aleph_0}$ languages total, and thus the number of language families is $\leq 2^{2^{\aleph_0}}$ . On the other hand, it is $\geq 2^{\aleph_0}$ as every single language generates a countable family, and thus the union of all countable families (which is a proper subset of the set of all families) has size $2^{\aleph_0}$ , which is only possible, when the number of countable language families is $2^{\aleph_0}$ itself. However, I do not know how to determine, whether it is $2^{\aleph_0}$ or $2^{2^{\aleph_0}}$ (it can not be something in between because otherwise it would have been a constructive counterexample to the continuum hypothesis, which is known to be independent from ZFC).","['elementary-set-theory', 'formal-languages', 'automata', 'discrete-mathematics']"
3709438,How to create a non-square 2D grid with spherical topology.,"When programming Conway's Game of life on my computer . A problem arises; how to deal with the borders on the board? Do the cells at the border have to take into consideration less neighbours than the cells on the inside of the board? That creates unwanted effects at the edge of the board (as if the rules of the game changed there). Another possibility is to make an infinite board, but programming this seems complicated in many ways. The solution? Sticking toghether the opposite sides of the board so that things seem to teleport to the other side when faced with the problem of interacting with one edge (a typicall approach for many similar games). This is topologically equivalent to a Torus as far as I understand. Whose fundamental polygon is Then I played a little with different topologies. I've been able to create Conway's Game of Life with Klein Bottle topology and then with the Real Proyective plane topology . But there's one final topology I want to examine and is the Sphere. Surprisingly the Sphere is becoming more and more difficult to wrap my head around so I ask you for help. According to the Fundamental polygon of a Sphere I should glue together the top side of the board with the left side, and the bottom side with the right side. But I want my board on Game of Life to be non-square (the grid of cells not having the same number of rows and columns). This wasn't a problem in the other topologies because I was gluing together sides with the same lenght, but now things are more complicated. One solution would be to stretch/contract one of the sides to accomodate the other before gluing them. But how could I do that with a discrete subdivision of space (like in a board of cells)? If I make on cell bigger/smaller then how I define its neighbours now? I could create/destroy some cells so that the number of cells in one side matched the other I have to glue together, but then there would be cells without any role in the game or cells that interact to much with the neighbours, and what rules do I implement for that decission exactly? So I'm stuck here, I want to see a glider move across the board regularly without been destroyed or transformed just at the edges because I didn't implemented a sphere topology correctly, without imposing a square grid in particular. What should I do?","['cellular-automata', 'general-topology', 'discrete-geometry', 'spheres']"
3709489,Lebesgue - Radon - Nikodym Theorem: Question about $\sigma$-finite case,"Lebesgue Radon Nikodym Theorem Let $\nu$ be a $\sigma$ -finite signed measure on $(X,\mathcal{A})$ and $\mu$ a $\sigma$ -finite positive measure on $(X,\mathcal{A})$ There exist unique $\sigma$ -finite signed measures $\rho,\lambda$ on $(X,\mathcal{A})$ such that $$\nu=\rho+\lambda\qquad \rho \ll\mu,\qquad\lambda \perp \mu.$$ There exists an extended $\mu$ -integrable function $f$ such that $d\rho=f\,d\mu$ i.e. $$\nu=f\,d\mu+\lambda$$ If we also have $\nu=\tilde{f}\,d\mu+\lambda$ where $\tilde{f}$ is an extended $\mu$ -integrable function, then $$\tilde{f}=f\quad\mu\text{-a.e}$$ Proof. Case 1 Suppose first that $\mu$ and $\nu$ are both finite, positive measures. On this first step I have no problems Case 2 Suppose that $\mu$ , $\nu$ are both $\sigma$ -finite positive measure. We can write $$X=\bigcup_l E_j\quad\text{and}\quad X=\bigcup_k F_k,$$ with $\mu(E_j)<\infty$ , $\nu(F_k)<\infty.$ Then $$X=\bigcup_{j,k}(E_j\cap F_k)=\bigcup_l A_l$$ disjointly with $\mu(A_l), \nu(A_l)<\infty.$ Define $$\mu_k(E)=\mu(E\cap A_k)\quad \nu_k(E)=\nu(E\cap A_k),$$ so by case 1. we can write $\nu_k=\rho_k+\lambda_k$ for some unique measures with $\rho_k \ll \mu_k$ and $\lambda_k\perp \mu_k.$ Note that $$\mu_k(A_k^c)=\mu(A_k^c\cap A_k)=0,$$ so $A_k^c$ is a $\mu_k-$ null set. Therefore $$f^{'}_k=f_k\chi_{A_k}$$ equals $f_k$ $\mu_k-$ a.e, so we can replace $f_k$ with $f^{'}_k$ without changing $\lambda_k$ or $\rho_k.$ In other words, we  can assume that $f_k(x)=0$ $\forall x\notin A_k.$ Since the $A_k$ are disjoint, we can therefore define $$f=\sum_{k=1}^\infty f_k.$$ Since $f\ge 0$ , $$d\rho=f\,d\mu$$ defines a positive measure. Also, $$\lambda=\sum_{k=1}^\infty \lambda_k$$ is a positive measure, since each $\lambda_k\ge 0$ Question I'm trying and trying again to show the following but I can't: $\lambda, \rho$ are $\sigma$ -finite; $\nu=\rho+\lambda$ ; $\rho \ll \mu$ ; $\lambda\perp \mu$ ; The uniqueness statements hold. Could you please give me some suggestions on the basis of what I have already shown?","['measure-theory', 'proof-writing', 'solution-verification', 'real-analysis']"
3709519,Function on $\mathbb{C}$ with all primes as zeros?,"According to the Weierstraß factorization theorem , an entire function with all primes as zeros would be (if I didn't mess up): $$\tilde P(z) = \prod_{p \text{ is prime}} \left(1-\frac{z}{p}\right) \cdot e^{z/p} $$ for $z\in\mathbb{C}$ . Formally I can define the slightly different function $$P(z) = \prod_{p \text{ is prime}} \left(1-\frac{z}{p}\right)$$ for $z\in\mathbb{C}$ . Question : For which $z\in\mathbb{C}$ does the latter converge apart from the obvious $z=0$ or $z$ is prime? Thoughts: Taking the $\log$ this translates to the question for which $z$ $$\sum_{p \text{ is prime}}\log(1-z/p)$$ converges. I am guessing that this is not the case because thinning the harmonic series out to just primes, $\sum 1/p$ , does not make it convergent and since $\sum \log(1-1/n)$ is not convergent, thinning out to primes likely does not help either. Yet for $\tilde P$ the $\log$ yields $$\sum_{p \text{ is prime}}(z/p+\log(1-z/p)),$$ which is, in absolute value, even larger but should converge because $\tilde P(z)$ converges. Hmm???","['complex-analysis', 'infinite-product', 'prime-numbers', 'weierstrass-factorization']"
3709537,Proving a version of a local limit theorem,"Let $\{X_n \}$ be a sequence of integer valued i.i.d random variables that are symmetric around $0$ , and $\mathbb{E}|X_1|^3<\infty, P[X_1 = 1]>0, P[X_1=0]>0$ . Let $S_n = X_1+\dots+X_n$ . Show that $$\lim_{n\rightarrow\infty}\sqrt{2\pi\sigma^2n}P[S_n=0]=1.$$ I know that I can write $$P[S_n = 0] =\frac{1}{2\pi} \int_{-\pi}^{\pi}\phi^n(t)dt=\frac{1}{2\pi\sqrt{n}}\int_{-\pi\sqrt{n}}^{\pi\sqrt{n}}\phi(t/\sqrt{n})^ndt.$$ Where $\phi$ is a characteristic function of $X_1$ . But apart from that I am stuck.","['probability-limit-theorems', 'probability-theory']"
3709564,Minimal Surface has constant Gaussian Curvature After Conformal Change $\tilde{g}=-Kg$,"Question: suppose $M\subset \mathbb{R}^3$ is a minimal surface (mean curvature $H \equiv 0$ ), show that after conformal change $$\tilde{g}=-Kg$$ the Gaussian curvature $\tilde{K}\equiv 1$ . Since $H \equiv 0$ implies $k_1=-k_2$ , where $k_1,k_2$ are principal curvature and we assume $k_1\geq k_2$ , we get $K=-k_1^2<0$ , so $\tilde{g}=-Kg$ is a conformal change indeed. I try to verify above statement by direct computation via moving frame: Let $\{\omega^i\},\{\tilde{\omega}^i\}$ be orthogonal coframes, and suppose $g = (\omega^1)^2 + (\omega^2)^2$ , and $\tilde{g} = (\tilde{\omega}^1)^2 + (\tilde{\omega}^2)^2$ , then $$\tilde{\omega}^i=\sqrt{-K}\omega^i$$ Also, we suppose connection 1-form $\omega_1^2 = p\omega^1+q\omega^2$ . \begin{align*}
d \tilde{\omega}^1 &= d(\sqrt{-K}\omega^1)=-(\sqrt{-K})_2 \omega^1 \wedge \omega^2+\sqrt{-K}d\omega^1\\
&=-(\sqrt{-K})_2 \omega^1 \wedge \omega^2+\sqrt{-K}\omega^2\wedge\omega_2^1\\
&=-(\sqrt{-K})_2 \omega^1 \wedge \omega^2+p\sqrt{-K} \omega^1\wedge \omega^2
\end{align*} \begin{align*}
d \tilde{\omega}^2 &= d(\sqrt{-K}\omega^2)=(\sqrt{-K})_1 \omega^1 \wedge \omega^2+\sqrt{-K}d\omega^2\\
&=(\sqrt{-K})_1 \omega^1 \wedge \omega^2+\sqrt{-K}\omega^1\wedge\omega_1^2\\
&=(\sqrt{-K})_1 \omega^1 \wedge \omega^2+q\sqrt{-K}\omega^1\wedge\omega^2
\end{align*} where $d(\sqrt{-K})=(\sqrt{-K})_1 \omega^1 + (\sqrt{-K})_2 \omega^2 $ . Then according to structure equation: $d\omega^i=\omega^j\wedge\omega_j^i$ and $d\tilde{\omega}^i=\tilde{\omega}^j\wedge\tilde{\omega}_j^i$ , we get $$
\tilde{\omega}_2^1 = \omega_2^1 + \frac{(\sqrt{-K})_2}{\sqrt{-K}}\omega^1 - \frac{(\sqrt{-K})_1}{\sqrt{-K}}\omega^2 
$$ Take exterior differential \begin{align*}
d\tilde{\omega}_2^1 &= d \omega_2^1-\left[\left(\frac{(\sqrt{-K})_1}{\sqrt{-K}}\right)_1+ \left(\frac{(\sqrt{-K})_2}{\sqrt{-K}}\right)_2 \right]\omega^1\wedge \omega^2 \quad \quad \quad \quad (1) 
\end{align*} By Cartan structure equation: $$\Omega_2^1 = d \omega_2^1 = R_{1212}\omega^1 \wedge \omega^2 = K\omega^1 \wedge \omega^2 \quad \quad \quad \quad (2) $$ Similarly, $$\tilde{\Omega}_2^1 = d \tilde{\omega_2^1 }= \tilde{K}\tilde{\omega}^1 \wedge \tilde{\omega}^2 = -K \tilde{K}\omega^1 \wedge \omega^2 \quad \quad \quad \quad (3) $$ Then combine $(1),(2),(3)$ : $$\tilde{K}=-1 + \frac{1}{K}\left[\left(\frac{(\sqrt{-K})_1}{\sqrt{-K}}\right)_1+ \left(\frac{(\sqrt{-K})_2}{\sqrt{-K}}\right)_2 \right]$$ So the problem converts to verify the following PDE of Gaussian curvature $K$ is true: \begin{equation}
\frac{1}{K}\left[\left(\frac{(\sqrt{-K})_1}{\sqrt{-K}}\right)_1+ \left(\frac{(\sqrt{-K})_2}{\sqrt{-K}}\right)_2 \right]=2 \quad  \quad  \quad (4)
\end{equation} Since we haven't really use the fact that $M$ is a minimal surface (except for $K<0$ ), I guess the above equation follows from $M$ is a minimal surface. But, frankly, I have no idea how to preceed. So I try to compute a concrete example of minimal surface to get some hints, for example, the helicoid $$
x(u,v)=(a \sinh(v) \cos(u),a \sinh(v) \sin(u),au)
$$ then my computation shows that $K=-\frac{1}{\cosh^2(v)}$ , plug into $\frac{1}{K}\left[\left(\frac{(\sqrt{-K})_1}{\sqrt{-K}}\right)_1+ \left(\frac{(\sqrt{-K})_2}{\sqrt{-K}}\right)_2 \right] = 1$ instead of $2$ , so I got puzzled. Is my computation for $\tilde{K}$ having something wrong or the question itself having something wrong. Could you please help me with that? Thank you in advance!","['riemannian-geometry', 'minimal-surfaces', 'conformal-geometry', 'differential-forms', 'differential-geometry']"
3709587,Definition of weak-$*$ topology,"Though I learned some basic theory about topological vector spaces, I always confused about the definition of weak- $*$ topology. Given $x\in X$ , let $\phi_x: X^*\to \Bbb R$ denote the evaluation map $u\to u(x)$ at $x$ . The weak- $*$ topology on $X^*$ is the initial topology associated with the family of all evaluation mpas $\phi_x: X^*\to \Bbb R$ . Thus, the weak- $*$ topology is the smallest topology on $X^*$ for which all evaluation maps $\phi_x$ are continuous. What is the definition of initial topology, how to understand the first statement? Why the weak- $*$ topology is the smallest topology on $X^*$ for which all evaluation maps $\phi_x$ are continuous? 3.There is also a conclusion: every subset of $X^*$ which is open for the weak- $*$ topology is also open for the strong topology. (1) I want to show that $(X^*,SOT)\rightarrow (X^*, \|\cdot \|)$ is continuous, but the book I refered mention that ""since all evaluation maps are continuous for the strong topology"", why can we prove (1) by the reason $""\cdots""$ .","['general-topology', 'operator-theory', 'topological-vector-spaces', 'functional-analysis']"
3709636,Polynomial with roots modulo all primes $p \equiv 3 \pmod 4$,"Does there exist an irreducible polynomial $f \in \mathbb{Z}[X]$ of degree $n \geq 2$ with a zero modulo all primes $p \equiv 3 \pmod 4$ ? For example, there is such a polynomial $X^2+1$ if we choose primes $p \equiv 1 \pmod 4$ . Mostly using quadratic residues, we can come up with many such polynomials for various collections of primes, but I wasn't able to find one for primes $p \equiv 3 \pmod 4$ .","['number-theory', 'finite-fields', 'polynomials']"
3709671,"Is there a real-analytic monotone function $f:(0,\infty) \to \mathbb{R}$ which vanishes at infinity, but whose derivative admits no limit?","A function $f:\mathbb{R} \to \mathbb{R}$ is called real-analytic if for each $x_0 \in \mathbb{R}$ there exists a neighbourhood of $x_0$ where $f$ is given by a convergent power series centred at $x_0$ . Problem: Is there a real-analytic monotone function $f:(0,\infty) \to \mathbb{R}$ which vanishes at infinity, but whose derivative admits no limit as $x \to \infty$ ? We can note some weaker, but related, results. The (non-monotone) function $f(x)=x^{-1} \sin x^2$ is a real-analytic function on $(0, +\infty)$ and has the property that $\lim_{x \to +\infty} f(x) = 0$ but $\lim_{x \to + \infty} f'(x)$ fails to exist. It's not difficult to construct monotone examples if real-analyticity is weakened to merely being infinitely differentiable. The basic construction is straightforward. For each integer $n \geq 2$ , and on each interval $[n, n+1-1/n^3]$ , set $f(x)=1/n$ , and on intervals $[ n+1-1/n^3, n+1]$ the function is linear, and decreasing from $\frac{1}{n}$ to $\frac{1}{n+1}$ . This function is piecewise linear, and not smooth at the transition points, but it's trivial to smoothen this construction by utilizing appropriate variants of $\exp(1/x)$ , rather than a linear interpolation. By the mean value theorem, we have that $\sup_{x \in [n+1-1/n^3, n+1]} |f'(x)| \geq \left|\frac{\frac{1}{n+1} - \frac{1}{n}}{\frac{1}{n^3}}\right|=\frac{n^3}{n(n+1)} \xrightarrow{n \to + \infty} + \infty$ hence $\lim f'(x)$ fails to exist. However, I don't think one can use these ideas to obtain a real-analytic monotone function with the desired properties, since there's no real-analytic ""transition"" functions.","['limits', 'derivatives', 'monotone-functions', 'real-analysis']"
3709776,How big is $\{n\in\Bbb N\mid 1\leq n\leq 2000\text{ and the digital sum of }n^2=21\}$?,"well,the title of the question makes it clear, The question is :find the number of natural numbers between 1 to 2000 such that the sum of digits of their squares is equal to 21. my approach:
just to make the question more clear I want to illustrate with help of an example . $$89^2 = 7921$$ and sum of digits of $7921$ is $7+9+2+1=19$ now since the squares of numbers from $1$ to $2000$ can be from $1$ to $7$ digits ,I could not find any way except calculating the squares by hand and adding their digits .so,is there a better method for solving this question? any help is greatly appreciated.","['recreational-mathematics', 'combinatorics']"
3709801,Let $a$ and $b$ be elements of odd order in a finite group. Show that $a^2$ and $b^2$ commute if and only if $a$ and $b$ commute.,"I really don't know how to solve this problem. I just know that if $|a|=2k_1+1$ and $|b|=2k_2+1$ , then, $a^{2k_1+1}=e=a^0$ and $b^{2k_2+1}=e=b^0$ . Also, if $|G|=n$ , then, $2k_1+1,2k_2+1\equiv 0 \pmod n$ .","['group-theory', 'finite-groups']"
3709829,Connections between numerical calculation of eigenvalues and other topics,"In the preface to his book Linear Algebra and Its Applications (2013), Peter Lax says It is with genuine regret that I omit a chapter on the numerical calculation of eigenvalues of self-adjoint matrices. Astonishing connections have been discovered recently between this important subject
  and other seemingly unrelated topics. Can anyone comment on Lax's ""astonishing connections"", or suggest papers or course notes?","['eigenvalues-eigenvectors', 'reference-request', 'matrices', 'numerical-linear-algebra', 'soft-question']"
3709836,Combinations series: $\frac{{n \choose 1}(n-1)^3+{n \choose 3}(n-3)^3+\ldots}{n^2(n+3)\cdot 2^n}$,"Evaluate $\frac{{n \choose 1}(n-1)^3+{n \choose 3}(n-3)^3+\ldots}{n^2(n+3)\cdot 2^n}$ for $n=10$ . Attempt: I'll deal with the case n being even, as we need to evaluate for n=10. the numerator is $${n \choose 1}(n-1)^3+{n \choose 3}(n-3)^3+\ldots$$ $$=\sum_{r=odd} {n \choose r}(n-r)^3$$ (not sure if this is a correct notation). $$=\sum_{r=odd}{n \choose n-r}r^3=\sum_{r=odd} {n \choose r}r^3$$ (parity being same as n is even and r is odd, although I don't think this matters much). Using the identity ${n \choose r}=\frac{n}{r} {n-1 \choose r-1}$ repeatedly in following steps, $$=n\sum_{r=even} {n-1 \choose r-1}r^2$$ $$=[n(n-1)](1+\sum_{r=odd} {n-2 \choose r-2}[(r-2+3)+\frac{1}{r-1}]$$ $$=[n(n-1)](1+(n-2)\sum_{r=even}{n-3 \choose r-3}+3\sum_{r=odd}{n-2 \choose r-2}+\frac{1}{n-1} \sum_{r=even}{n-1 \choose r-1} -1)$$ $$=[n(n-1)]((n-2)\cdot 2^{n-4} +3\cdot 2^{n-4}+\frac{2^{n-2}}{n-1}$$ This simplfies to $n \cdot 2^{n-4} (n^2+7n-4)$ . Which is incorrect. The answer for $n=10$ (numerator/denominator is given as $\frac{1}{16}$ ). Where am I going wrong? Also the hint given for this problem was ""expand $\frac{(e^x+1)^n - (e^x-1)^n}{2}$ in two different ways"". I didn't quite understand this approach? Could someone please explain this approach and any other approach also?","['summation', 'binomial-coefficients', 'combinatorics', 'sequences-and-series', 'binomial-theorem']"
3709863,Does every inner product space have Hilbert completion?,"Does every inner product space have Hilbert completion which is a Hilbert space? If so, how can we define the inner product in the Hilbert space?","['hilbert-spaces', 'inner-products', 'complete-spaces', 'functional-analysis']"
3709869,"Find the largest possible order of an element of $\frac{\Bbb{Z}_{12}\times\Bbb{Z}_3\times\Bbb{Z}_6}{\left<(8,2,4)\right>}.$","I find the following problem tough: Find the largest possible order of an element of the quotient group $$
Q = \frac{\mathbb{Z}_{12} \times \mathbb{Z}_{3} \times \mathbb{Z}_{6}}{\left<(8,2,4)\right>}.
$$ So far I've got that the order of $\left<(8,2,4)\right>$ is $3$ , so $|Q| = 72$ . But I think I am lacking the basic intuition behind quotient groups involving direct product of groups. So the question has this second, added personlly, part: what is the intuition behind these quotient groups? Thanks in advance!","['group-theory', 'quotient-group', 'abelian-groups']"
3709873,Another definition of normal operator,Another definition of a normal operator is that $T$ is normal if and only if $\| \textsf{T}v \| = \| \textsf{T}^∗ v \|$ for all $v$ . Can someone please explain to me the definition and possibly give me some intuition for why this definition makes sense? And how it connects to the usual definition of a normal operator?,"['inner-products', 'operator-theory', 'linear-algebra', 'functional-analysis', 'normal-operator']"
3709915,Intuition behind gradient being linear combination of constraint gradients in Lagrange multipliers,"I already understand the intuition behind why the gradient of a function $f$ at its maximum $(x,y)$ subject to some constraint $g$ satisfies: $\nabla f(x,y) = \lambda\nabla g(x,y)$ For some constant $\lambda$ . There are a lot of depictions online of the single constraint case in 2D, where you see that gradients of a function at a point are always perpendicular to the level set of the function at that point. You then conclude that the gradient of $f$ and the gradient of $g$ must be parallel (just a verbal way of expressing the equation above), because $\nabla f$ points in the direction of steepest ascent, and if $f$ is differentiable then it's continuous and the tangent plane is a good local approximation, and if you could move in some direction that increased $f$ but that was also parallel to $\nabla g$ , you would be able to move along the level set of $g$ at $(x,y)$ and increase $f$ a little more without violating the constraint. My problem is that this intuition falls apart with two or more constraints. Somehow this ends up being true for arbitrarily high dimension: $\nabla f(x_1,\ldots,x_D) = \sum_{i=1}^n \lambda_i\nabla g_i(x_1,\ldots,x_D)$ I can see that if we stay in two dimensions and have two constraints, any two non-perpendicular vectors end up spanning the whole space so it must be the case that they can sum to $\nabla f$ . But if the number of dimensions is high, and the number of constraints is smaller than the number of dimensions, it's not obvious to me why $\nabla f$ must be a linear combination of $\nabla g_i$ . What I can accept, is that it must be the case that at the maximum moving in the direction of $\nabla f$ must require moving in a direction that has a non-zero projection onto at least one $\nabla g_i$ . In other words if we consider one pair of $(\nabla f, \nabla g_i)$ , two vectors always lie in some plane, and we can consider $\nabla f$ to be the sum of two vectors: one that is parallel to $\nabla g_i$ and one that is perpendicular to $\nabla g_i$ . Since it must be the case at the maximum that going further in the direction of $\nabla f$ would cause us to violate at least one constraint, there must be at least one $\nabla g_i$ where in a plane that only contains the two of them its part that is parallel to $\nabla f$ is non-zero. But I have no idea how we get from that to a linear combination of all constraints. How do I get an intuition for this? Maybe there is an intuitive visualization for the multiple constraints case? I haven't been able to find one.","['multivariable-calculus', 'calculus', 'vector-spaces', 'lagrange-multiplier']"
3709929,Proving $\tan\frac{4\pi}{11} + 4\sin\frac{\pi}{11} = \sqrt{11}$,In a similar vein as $\tan\frac{3\pi}{11} + 4\sin\frac{2\pi}{11} = \sqrt{11}$ discussed in this question is this identity: $$\tan\frac{4\pi}{11} + 4\sin\frac{\pi}{11} = \sqrt{11}$$ Trying to adopt a method on the same line however lamentably fails. I wonder if the arguement of $11$ th roots of unity can still be effectively employed in this case. Is there a way to adapt it or there could be possibly an easier way out to prove the result?,['trigonometry']
3709990,Canonical lifting of vector fields,"Consider a compact Kahler manifold $M$ of complex dimension $n$ and a holomorphic vector field $X$ defined on it. Let $L$ be the line bundle $\Lambda^nT^{1,0}M$ . Then is there a canonical way of lifting $X$ to another vector field $X^* $ on $L$ ? In general, is there such a procedure by which we can canonically lift vector fields on a manifold to other vector bundles defined on the same manifold?","['kahler-manifolds', 'complex-geometry', 'vector-fields', 'vector-bundles', 'differential-geometry']"
3710031,Which spaces have uncountable perfect sets?,"I have been thinking about the following question: for which topological spaces $X$ are all perfect subspaces of $X$ uncountable, where perfect means closed with no isolated points. As long as $X$ is $T_1$ , we know that perfect sets are at least infinite. One sufficient condition is for a space to be completely Baire, meaning that every closed subspace is Baire . This works since countable sets are meager inside perfect sets, but in a completely Baire space closed sets are nonmeager in themselves. By the Baire Category Theorem this covers complete metric spaces like $\mathbb{R}^n$ (or any Polish space) and locally compact Hausdorff spaces. In these last classes of topological spaces it can actually proved that perfect sets have size continuum. The counterexample to keep in mind is $\mathbb{Q}$ as a subspace of the reals. This is a metric space which is perfect but countable. So, I was wondering if anybody knew more about this question, or had some interesting examples. I would be especially curious to know of weaker conditions that show perfect sets have size $\mathfrak{c}$ .","['general-topology', 'baire-category', 'descriptive-set-theory']"
3710032,Conjecture: All but 21 non-square integers are the sum of a square and a prime,"Update on 6/19/2020. This discussion led to deeper and deeper results on the topic. The last findings are described in my new post (including my two answers), here . I came up with the following conjecture. All non-square integers $z$ can be represented as $z=x^2 + y$ where $x$ is an integer and $y$ is a prime. The exceptions are z = 10, 34, 58, 85, 91, 130, 214, 226, 370, 526, 706, 730, 771, 1255, 1351, 1414, 1906, 2986, 3676, 9634, 21679. Note that this is deeper than Goldbach conjecture (all even numbers are the sum of two primes) because squares are far rarer than primes. Also, few numbers are the sum of two squares, such numbers (sums of two squares) are far more abundant than primes, but their natural density is also zero. But all numbers are the sum of four squares. Surprisingly, all integers can be represented as $z = \lfloor x^c \rfloor +
\lfloor y^c \rfloor$ where $x, y$ are positive integers and $c < \log_{22} 63$ is a positive constant; but this fails at $c = \log_{22} 63$ as $z=73$ becomes an exception. See section 1 in this article for details; this is also a conjecture. Question : Can you verify if my conjecture is true up to some very large $z$ ? I tested it only for $0\leq z < 750000$ . Heuristics behind this conjecture This is by no mean a proof, but rather, I explain here why I think it could be true. Let denote as $r(z)$ the number of solutions to $x^2 +y \leq z$ where $x, y$ are integers and $y$ is prime. For a fixed large $z$ , we want to count the number of integer couples $(x, w)$ below the curve $z=x^2+ w\log w$ , with $x, w\geq 0$ , in order to approximate $r(z)$ . The choice of $w \log w$ is a direct consequence of the prime number theorem, replacing primes by their approximation, for large primes.  That count $r(z)$ grows faster than $O(z)$ . The derivative $dr(z)/dz$ thus grows faster than $O(1)$ , and it shows how the number of solutions to $z=x^2+y$ grows on average, faster than $O(1)$ as $z$ increases. More details about the heuristic approach Essentially, we are trying to count the number of blue points under the red curve in the plot below (in this example, $z=100$ ). The equation for the curve is $w \log w = z-x^2$ , and $z$ is assumed to be fixed. The equation can be re-written as $w = (z-x^2)/W(z-x^2)$ where $W$ is the Lambert function , which behaves asymptotically like the $\log$ function. Thus the number of points below the red curve is asymptotically (for large values of $z$ ) equal to $$r(z) \sim \int_0^\sqrt{z} \frac{z-x^2}{W(z-x^2)}dz \sim 
 \int_0^\sqrt{z} \frac{z-x^2}{\log(z-x^2)}dz = \frac{1}{2}\int_0^z \frac{u}{\sqrt{z-u}\cdot\log u}du.$$ Let us denote as $\phi(z)$ the function defined by the rightmost integral. We have $r(z) \sim \phi(z)$ . I computed the exact values of $r(z)$ and $\phi(z)$ for various small and large $z$ , and clearly, $r(z) \rightarrow C \cdot \phi(z)$ , but I am not sure if $C=1$ . See WolframAlpha computations here . The number of solutions to $z=x^2+y$ (with $y$ prime) is thus, on average, as $z$ gets larger and larger, asymptotically equivalent to $d\phi(z) / dz$ . Below is a table featuring $r(z)$ and $\phi(z)$ . Good asymptotic approximations for very large $z$ are $$\phi(z)\approx\frac{2}{3}\cdot \frac{z^{3/2}}{\log z} \mbox{ and } 
\frac{d\phi(z)}{dz}\approx \frac{\sqrt{z}}{\log z}.$$ The last result is compatible with the one posted in the answer by Dietrich Burde, confirming that the approach I used here is sound. Note that the same methodology could be applied to sums of squares or sums of primes or any sums of integers. It is pretty generic. Final comment The number of solutions to $z = x^2 + y$ (with $y$ prime, $x$ an integer) is equal to $r(z)-r(z-1)$ . In all cases, $r(z)$ grows slowly (polynomial at most) and thus $r(z)-r(z-1) \sim dr(z)/dz$ . We could get deeper results with second- and third-order approximations in all the asymptotic results used in this article, rather than just first-order approximations. Below is a chart featuring the distribution for the number of solutions to $z=x^2+y$ [that is, the distribution of $r(z)-r(z-1)$ ]  for $700000\leq z < 740000$ . For instance, there are $441$ different $z$ 's between $z = 700000$ and $z = 740000$ for which $z=x^2 + y$ has exactly $50$ solutions. Below is the same chart, but for $100000\leq z < 140000$ . The two distributions are strikingly similar in shap2. Finally, among the first 750,000 $z$ 's, we have: $z = 78754$ is the last one to admit only one decomposition as $z =
   x^2+y$ $z = 101794$ is the last one to admit exactly two decompositions $z = 339634$ is the last one to admit exactly three decompositions $z = 438166$ is the last one to admit exactly four decompositions $z = 383839$ is the last one to admit exactly five decompositions The $z$ 's that admit only one decomposition are listed below. I searched for this sequence to see if it had been discovered, but could not find any reference. z = 2, 5, 8, 13, 15, 22, 24, 26, 31, 37, 40, 46, 50, 55, 61, 70, 74, 76, 82, 94, 99, 106, 115, 120, 127, 133, 136, 142, 145, 154, 159, 166, 170, 178, 184, 202, 205, 219, 221, 235, 246, 250, 253, 265, 268, 274, 295, 298, 301, 310, 316, 319, 325, 328, 334, 340, 346, 379, 391, 394, 399, 412, 424, 436, 439, 442, 445, 469, 490, 505, 511, 559, 562, 571, 574, 586, 589, 610, 616, 646, 694, 781, 793, 799, 829, 834, 835, 874, 914, 922, 946, 949, 970, 979, 991, 994, 1030, 1045, 1066, 1090, 1105, 1164, 1204, 1219, 1243, 1324, 1354, 1366, 1384, 1411, 1450, 1501, 1549, 1555, 1642, 1717, 1726, 1765, 1786, 1810, 1885, 1981, 1990, 2041, 2059, 2074, 2146, 2167, 2245, 2266, 2284, 2344, 2410, 2416, 2479, 2650, 2806, 2821, 2854, 2899, 2926, 3004, 3094, 3151, 3166, 3184, 3319, 3418, 3502, 3811, 3859, 3865, 3964, 3991, 4216, 4222, 4279, 4330, 4414, 4504, 4510, 4645, 4654, 4711, 4930, 5482, 5506, 5545, 5986, 6031, 6049, 6274, 6439, 7009, 7081, 7441, 7549, 7954, 8086, 8584, 8824, 9214, 9571, 10165, 10774, 11509, 11806, 13834, 15106, 15334, 15565, 16081, 16186, 23851, 31879, 33205, 44536, 78754","['sums-of-squares', 'number-theory', 'goldbachs-conjecture', 'asymptotics', 'prime-numbers']"
3710037,Where is the factor of 2 missing in my calculation?,"I am trying to follow a computation done in Chern's Complex Manifolds without Potential Theory. I find an extra factor of 2 in my computation which differs from the book and the thing I knew. Let $\theta=C^m/\Gamma$ where $C^m$ is m-dimensional complex plane and $\Gamma$ is a rank 2m lattice. The goal is to see existence of restricted type Kahler structure on $\theta$ under some condition of $\Gamma$ . Assume $\Gamma=\langle\pi_1,\dots,\pi_{2m}\rangle$ Restricted type Kahler structure is demanding that Kahler form $\hat{H}\in H^2(\theta,Z)$ . Let $\tau_{ij}$ be the torus generated by $C^2/\langle\pi_i,\pi_j\rangle\subset\theta$ . Set $\pi_i=(\pi^1_i,\dots,\pi^m_i)$ . Then $\tau_{ij}$ is a set of basis for $H^2(\theta,Z)$ . Consider Poincare duality $H^2(\theta,Z)\times H_2(\theta,Z)\to Z$ by integration. So to test a closed form's integrality, it suffices to test by Poincare duality. Assume there restricted Kahler structure $H=\sum_{ij}h_{ij}dz_i\otimes d\bar{z}_j$ . Then corresponding Kahler form is given by $\hat{H}=\frac{i}{2}\sum_{ij}h_{ij}dz_i\wedge d\bar{z}_j$ . Thus $\int_{\tau_{ij}}\hat{H}\in Z$ for all $i,j$ iff $\hat{H}$ is restricted Kahler structure. WLOG, we can integrate $H$ over $\theta$ to obtain invariant restricted Kahler structure as it is a group. Thus we can assume $h_{ij}$ are all constants. Then I got, $\frac{i}{2}\int_{\tau_{ij}}\sum_{lm}h_{lm}dz_l\wedge d\bar{z}_m=\frac{i}{2}\sum_{lm}h_{lm}\int_{\tau_{ij}}dz_l\wedge d\bar{z}_m$ $=\frac{i}{2}\sum_{lm}h_{lm}(\pi^i_l\bar{\pi}_m^j-\pi_m^i\bar{\pi}_l^j)\in Z$ . However, the book does not have $\frac{1}{2}$ in computation above.(i.e. $i\sum_{lm}h_{lm}(\pi^i_l\bar{\pi}_m^j-\pi_m^i\bar{\pi}_l^j)\in Z$ ) $\textbf{Q:}$ Where did I miss a factor of $2$ ? Ref. Chern, Complex Manifolds Without Potential Theory, Sec 7, equation 7.30 pg 65","['kahler-manifolds', 'complex-geometry', 'algebraic-topology', 'differential-geometry']"
3710069,Is the rank of a matrix equal to the number of non-zero eigenvalues?,"I have studied before that the rank of a matrix = number of non zero Eigen values. But recently i came across a problem and i dont think it is valid there. I know i am going wrong somewhere. $$A=
\begin{bmatrix} 
0 & 4 & 0 \\
0 & 0 & 4\\
0 & 0 & 0 \\
\end{bmatrix}
\quad
$$ The Rank of this matrix is 2. So there should be 2 non zero eigen values. But I only get 0 as the eigen value(λ) using $$[A-λI]=0$$ Can anybody explain? Thanks","['matrices', 'matrix-rank', 'eigenvalues-eigenvectors']"
3710082,"Question from Mac Lane and Birkoff (Chapter II, section 3, problem 9) -- $\operatorname{Aut} (\mathbb{Z}_6) \cong \mathbb{Z}_2$","I'm working my way through Mac Lane and Birkhoff Algebra and I have a question about one of their exercises. We are asked to show that certain automorphism groups are isomorphic (as groups) to a given group. For example, one part is to show that $\operatorname{Aut} (\mathbb{Z}_6) \cong \mathbb{Z}_2$ . I'm fairly confident that I can write out all of the automorphisms and show that they are the only ones and show that they form a cyclic group of order (which we know from the reading is isomorphic to $\mathbb{Z}_2$ ). But, I'd like to argue this in a different way, that might save some time for the other parts of the problem. I'm curious if my solution is valid, or if I am missing something(s). Here it goes. From a prior problem (coincidentally that I asked about a few days ago), we know that if $\phi: G \rightarrow H$ is an morphism between groups, then the image of $\phi$ forms a subgroup, $\operatorname{Im}(\phi) \subseteq H$ . Next, since subgroups of cyclic groups are cyclic, we know that $\operatorname{Im}(\phi) = \langle a \rangle$ , for some $a\in H$ . Finally, if we are talking about automorphisms then, in particular, $\phi$ is an epimorphism. So, $\langle a \rangle =\operatorname{Im}(\phi) = H$ . ( Does this imply that generators of $G$ must be mapped to generator(s) of $H$ ? ) Thus, this problem reduces to finding the number of distinct generators of each group in question. For example, $\mathbb{Z}_6$ has two generators, $1$ and $5$ . Hence, there are two distinct automorphisms, implying that the order of $\operatorname{Aut} (\mathbb{Z}_6) $ is two. The first is the identity automorphism, call it $\phi_{1}$ and the second, call it $\phi_2$ sends: $$
\begin{align}
1 &\mapsto 5\\
2 &\mapsto 2\\
3 &\mapsto 3\\
4 &\mapsto 4\\
5 &\mapsto 1\\
\end{align}
$$ From this, $\phi_2 \circ \phi_2 = \phi_1$ . So, $\operatorname{Aut} (\mathbb{Z}_6) = \left\{ \phi_1, \phi_2 \, | \, \phi_2^2 = \phi_1\right\} \cong \mathbb{Z}_2$ . ( Could we not get another automrphism by taking $\phi_2$ and instead of sending $2 \mapsto 2$ and $4 \mapsto 4$ , send $2 \mapsto 4$ and $4 \mapsto 2$ ? ) As you can see, this solution/these ideas are not fully baked, so any assistance would be greatly appreciated. Thanks in advance.","['automorphism-group', 'group-theory', 'abstract-algebra', 'cyclic-groups']"
3710094,"Intuition behind Isomorphic spaces ""Being the Same""","I know that isomorphic spaces are treated as the same.  But why is it so.... Like $R^2$ and the set of all ${(x, y, 0) }$ are isomorphic but the ""same"" vectors in the two spaces are actually different vectors. Some isomorphic spaces might be having even different rules of vector addition and scalar multiplication, then why the corresponding vectors in both will be the same. Also any N dimensional vector space $V$ is isomorphic to $F^n$ . Buth that n dimensional vector space can be a space of matrices or of polynomials or of any other abstract vectors. How does saying corresponding vectors in each such n dimensional vector spaces are ""the same"" as the the n tuple in $F^n$ . All these vectors have different rules for multiplication and addition, then what is the intuitive reasoning behind them being treated as same. Will it not defeat the purpose of treating abstract objects as vectors. Edit: Precisely this An n dimensional polynomial space is isomorphic to $F^n$ . An n dimensional space of matrices ( n= ab) is isomorphic to $F^n$ . Now how is Differentiation in n-dimensional polynomial space mirrored in $F^n$ ( n- tuple are constants) and How is a transpose operation in n dimensional matrix space mirrored to $F^n$ . Also since the n dimensional space and n dimensional matrix space are isomorphic to $F^n$ , then they should be isomorphic to each other too ( is this correct). But then how is differentiation in n dimensional polynomial space mirrored to an n dimensional matrix space.","['vector-space-isomorphism', 'intuition', 'linear-algebra', 'linear-transformations']"
3710136,"Integrate $\int_0^{2\pi}\frac{\ln(a + b\cos x)}{c + d\cos x} dx$, Residue theorem","I have recently been given a challenge problem in my Complex Analysis class. Suppose $a > b > 0$ and $c > d > 0$ . Evaluate $$\int_0^{2\pi} \frac{\ln(a + b\cos x)}{c + d\cos x} dx$$ using the Residue Theorem. Unfortunately, I don't even know where to begin with this one. I have managed to solve the integral where the integrand is $$\frac{a + b\cos x}{c + d\cos x}$$ where the contour I used was the usual a square but I'm not sure whether that can be applied here. If anyone could provide any assistance, that would be greatly appreciated!","['integration', 'complex-analysis', 'residue-calculus']"
3710142,When does almost sure pointwise convergnce of a sequence of stochastic processes imply uniform almost sure convergence,"Suppose $\{X_n(t) \; : \; t \in K\}$ $n=1,2,...$ is a sequence of stochastic processes indexed by a compact subset of a metric space $K$ , and $\{X_\infty(t) \; : \; t\in K\}$ is a limiting process satisfying $X_n(t) \stackrel{a.s.}{\to} X_\infty(t)$ for all $t \in K$ . My question is: under what conditions is this enough to imply that $\sup_{t \in K} |X_n(t) -X_\infty(t) | \stackrel{a.s.}{\to} 0$ ? I assume this must depend on the continuity of each process, and perhaps on the complexity of $K$ , but I cannot find a suitable general result of this type.","['empirical-processes', 'stochastic-processes', 'probability-theory', 'real-analysis']"
3710197,Evaluating $\int_{0}^{\infty} (\frac{\sin x}{x})^2 dx$ using complex analysis,"I need to calculate $\displaystyle\int_{0}^{\infty} \left(\frac{\sin x}{x}\right)^2dx$ . I have started with defining: $$f(z) = \frac{1-e^{2iz}}{z^2},\quad z\in\mathbb{C}\;.
$$ Then divided it into four contour integrals, just standard stuff: $\int_{-R}^{-r} \frac{1-e^{2ix}}{x^2} dx + \int_{r}^{R} \frac{1-e^{2ix}}{x^2} dx + \int_{C_r }^{} \frac{1-e^{2iz}}{z^2} dz+\int_{C_R }^{} \frac{1-e^{2iz}}{z^2} dz$ First two integrals add up to $2\int_{r}^{R} \frac{1-\cos(2x)}{x^2} dx$ Third integral, Laurent series for $\frac{1-e^{2iz}}{z^2} = \frac{-2i}{z} +2 + \frac{4iz}{3} - \frac{2z^2}{3} +... = \frac{-2i}{z} + P(z)$ and $\int_{C_r} P(z) = 0$ as $r \to 0$ , so: $\int \frac{-2i}{z}dz=-\int_0^\pi \frac{2i^2re^({ti})}{re^{ti}}dt = 2\pi$ for $z = re^{ti}$ $\int_{C_R }^{} \frac{1-e^{2iz}}{z^2} dz \leq \frac{2}{R} +\frac{|e^{2xi}|e^{-2y}}{|z^2|} \leq \frac{2+\pi}{R}= 0$ as $R \to \infty$ $2\int_{0}^{\infty} \frac{1-\cos(2x)}{x^2} dx + 2\pi = 4\pi$ so my integral is equal to $\pi$ , whereas it should be $\frac{\pi}{2}$ . Where did I go wrong?","['complex-analysis', 'calculus', 'improper-integrals']"
3710205,Prove that $-X$ is measurable with respect to some sigma field.,"I am reading A Second Course in Probability by Ross and Peköz. I came across the following question: 1.10.4. Show that if $X$ and $Y$ are real-valued random variables measurable with respect to some given sigma field, then so is $XY$ with respect to the same sigma field. My attempt was to first show that: (i) $X+Y$ is measurable (ii) $cX$ is measurable for any $c\in\mathbb R$ (iii) $X^2$ is measurable. Then, since $XY = \frac14[(X+Y)^2-(X-Y)^2]$ , we just use the above properties. Property (i) was proven in Chapter 1. 
However, I am having a bit of trouble showing property (ii) in the case that $c < 0$ . From the problem setup, we know that for any $x\in\mathbb R$ , $\{X\leq x\}\equiv\{\omega\in\Omega:X(\omega)\leq x\}\in \mathcal F$ . So $\{-X\leq x\}=\{X\geq -x\} = \{X<-x\}^c$ , but I don't know if $\{X<-x\}\in\mathcal F$ . If I expand a little more, it seems I essentially need to show that $\{X=x\}\in\mathcal F$ , but I don't see how that is necessarily true. I have seen the question Product of two random variables , though in the answers, they assume knowledge of the fact that $cX$ is measurable, and they also seem to be working with the Borel $\sigma$ -algebra. I have also seen these notes , but they omit the case where $c < 0$ . My Question: How do we show that $\{-X\leq x\}\in\mathcal F$ if we already know that $\{X\leq x\}\in\mathcal F$ ? Do we need to assume something about the sigma field in order to show this for real-valued random variables (e.g. Borel sigma field)?","['self-learning', 'measure-theory', 'probability-theory', 'random-variables']"
3710237,polynomial equation $ A(x+y_1)(x+y_2)...(x+y_n) + B(x+z_1)(x+z_2)...(x+z_k) = f(x) $ ??,"Consider given integers $A,B$ such that $AB \neq 0$ . Consider a given polynomial $f(x) = a_0 + a_1 x + a_2 x^2 + ... $ of degree $n > 1$ with rational coefficients $a_i$ . Now I wonder about solving the diophantine equations of type : $$ A(x+y_1)(x+y_2)...(x+y_n) + B(x+z_1)(x+z_2)...(x+z_k) = f(x) $$ where $y_i,z_i$ are fractions and $0<k<n$ is an integer. In particular the cases when $k = 1,2,n$ . Also the similar equation $$ A(x+w_1)(x+w_2)...(x+w_n) + C = f(x) $$ where $w_n$ are fractions and $C$ is a nonzero integer. Already for small $n$ and small $A,B$ i got stuck trying to solve those. Does vieta jumping help here ? Is this a hard problem or an easy one ? When do we have solutions ? Can we classify when we have solutions ? Is this rather algebra or rather number theory ? Can we parametrise the solutions for $n<5$ ? EDIT: For the case of the second type equation notice that if $C$ is an integer there are only finitely many possibilities for it. There exist upper bounds and lower bounds for $C$ due to the shape of the polynomial.
We can combine that idea by considering all potential $C$ with the rational root test and factoring to make a list of all candidates $w_i$ . Therefore we have made a method to solve it because we made it computable, however this is very inefficient ofcourse. The harder question of rational $C$ comes to mind. Maybe it reduces too. Probably. The main question (equation 1) does not seem aided by this though. Just some comments...","['number-theory', 'vieta-jumping', 'diophantine-equations', 'factoring', 'polynomials']"
3710270,Blowing up the whitney umbrella over the z-axis,"My professor gave us the example of the Whitney Umbrella as an example of a non-trivial resolution of singularities. I'm aware that to resolve the singularities of the Whitney Umbrella, I need to first, blow up the Whitney Umbrella over the $Z$ -axis. As per my understanding, since the ideal generating the $z$ -axis in $\mathbb A[x,y,z]$ is $(x,y)$ , the blowup of $W =\mathbb V(x^2-zy^2)$ is a subset of $\mathbb A^3\times\mathbb P^1$ , and is defined as ( $Cl$ is the closure) $$Cl\{((x,y,z),[x:y])\mid (x,y)\neq(0,0),\;x^2=zy^2\}$$ But, I'm just not making any progress in figuring out what the blow up actually is. It's easy to see that in the blow-up, $y\neq0$ and $z\geq0$ . Letting $t=\sqrt z$ , we can partition the blowup as follows: $$Cl\left(\big\{\left((0,y,0),[0:1]\right)\mid y\in\mathbb A\big\}\;\cup\;\big\{\left((yt,y,t^2),\left[1:t\right]\right)\mid t>0\big\}\cup\big\{\left((-yt,y,t^2),\left[1:-t\right]\right)\mid t>0\big\}\right)$$","['algebraic-geometry', 'blowup', 'singularity']"
3710304,Odds of an Unwinnable Game of Rummikub,"I was playing the game Rummikub with my family the other day and the tiles were drawn in such a way that the game could not end. Here are the rules: There are 106 tiles in the game which at the game's start are face down; there are two full ""decks"" of cards (represented by colors instead of suit and 11,12,13 instead of face cards) and two jokers, which act as wild cards. Tiles placed on the board remain there, face up, for the rest of the game, and can be manipulated in the ways described in rule #4. This is a ""rummy"" style game, meaning that legal groupings of tiles are either sets of a number or straights of a single color. Members of sets are unique, meaning that groupings of this kind are at most size 4. The minimum size for any grouping is 3. Groupings are created from the tiles in a player's hand, either (A) entirely from the player's hand, (B) adding one or more tiles to an existing grouping, or (C) breaking up and reforming groupings already present on the board such that method (B) may be used. Note that method (C) is legal even if an illegal grouping is created before the player adds their tile, such that after their turn all groupings on the board are legal. Jokers may be used by their initial player as a stand-in for any tile on the board, and any player on the board may use them afterward by replacing them with the tile they stand in for. A joker which exists on the board must be played by the end of a player's turn and may not be returned to that player's hand. A player wins when they have no tiles in their hand. There are maximum four players to the game, and minimum two. Each player begins the game with 14 tiles. There is no limit as to the number of tiles a player can play during their turn. If a player cannot play a tile during their turn, they draw a tile. Before they begin regular play, each player must ""go down"", meaning create groupings entirely from their hand such that the sum of the tiles of all their groupings is greater than or equal to 30. Each player ""goes down"" separately, meaning that some players may be in the midst of regular play while others are stuck with usable tiles in their hand that do not exceed 30. Jokers cannot be used to ""go down"". There are additional irrelevant rules which deal with scoring. My question is: what are the odds that such a game will end without anyone winning, i.e. all players have no available moves and there are no more face down tiles on the board.? My suspicion is that the odds of this are astronomically low; in fact, I would have bet against it being possible until it happened to me the other night. I do not have extensive mathematical experience but I am towards the end of an undergraduate degree of mathematics, so if you can manage, please form your answers with this in mind. Please let me know if you need clarification on the rules.","['combinatorics', 'card-games', 'probability']"
3710315,"(Proof verification) For each $s \in S$, show that $\sum_{t \in Gs} \frac{1}{|Gt|}=1$","This is an exercise from Lang's Algebra, Chapter 1. I got my solution but I'm not sure that it's correct. Feel free to point out what's wrong with me. Let $G$ be a finite group operating on a finite set $S$ . For each $s \in S$ , show that $$\sum_{t \in Gs} \frac{1}{|Gt|}=1$$ ( $Gs$ means the orbit) My efforts Notice that for $t \in Gs$ , we have some $t = g \cdot s$ where $g \in G$ , which is equivalent to $s = g^{-1} \cdot t$ . Hence $s \in Gt$ iff $t \in Gs$ . Therefore we have $$\sum_{t \in Gs} \frac{1}{|Gt|}=\sum_{s \in Gt}\frac{1}{|Gt|}=\frac{1}{|Gt|}\sum_{s \in Gt}1=1$$ Does this approach work? I found some solution claimed that $|Gs|=|Gt|$ , but I found no way to prove it, so I tried this one. But is it possible to for me to apply the equivalence relationship under the sum operator? Did I miss something? Appreciated in advance! Update 1: I realized how to prove that $|Gs|=|Gt|$ . In fact, for $t \in Gs$ , we have $Gt=G(g\cdot s)=Gs$ . Generally, the two orbits of $G$ are either disjoint or are equal. I forgot this.","['finite-groups', 'abstract-algebra', 'solution-verification', 'group-theory', 'group-actions']"
3710382,What does $\sin x \cdot \sin 2x \cdot \sin 3x \cdot ... \cdot \sin nx$ equal to?,"Problem: Find a general formula for: $$A = \sin x \cdot \sin 2x \cdot \sin 3x \cdot ... \cdot \sin nx$$ How do I come across with this? Or is there a way to simplify this? I tried to at least guess the formula but it seems complicated. Edit: From the comments, and some thoughts from me, I suspect that the answer to the above could be $$A = 2^{-n} \cdot i^n \cdot \prod_{k=1}^{n} \, \left(e^{-kix} - e^{kix}\right)$$ Is this formula correct? (Can it be further simplified?) If yes, how do I prove it?","['algebra-precalculus', 'trigonometry']"
3710402,Generate random points on perimeter of ellipse,"Sampling only from the uniform distribution $U(0,1)$ , I am hoping to use transformations to create random values distributed uniformly around the perimeter of an ellipse. Eventually, I'd like to do the same on the surfaces of ellipsoids and other problematic objects. My first idea was as follows. We can easily get $\Theta \sim U(0,2\pi)$ . Then, from the parametric form of the ellipse, $$X \equiv a \cos \Theta \\
 Y \equiv b \sin \Theta $$ is a random point on the ellipse's perimeter. Similarly, if we independently sample another angle $\Phi \sim U(0,\pi)$ , we could use $$X \equiv a \sin \Theta \cos\Phi \\
 Y \equiv b \sin \Theta \sin\Phi\\
Z \equiv c \cos \Theta $$ The problem with these approaches is that they are uniformly distributed with respect to theta, not along the surface. They are equivalent to taking a uniform distribution on a circle and then projecting about the radius to the perimeter of the ellipse, so the density of points is higher near the major axis, as you can see here: (This is itself counterintuitive to me: One would expect the points to be denser about the minor axis since they are being ""sprayed"" over a more concentrated region, right?) How can I generate points distributed uniformly about the perimeter of the ellipse? Here is an example of what I am trying to do but using a circle instead. The transformation used there doesn't work for the ellipse because it creates the same bunching behavior.","['statistics', 'transformation', 'random-variables']"
3710440,$\int\limits_{-1}^1 f' ^ 2\leq \frac{1}{2} \left( \int\limits_{-1} ^ 1 f^2 + \int\limits_{-1}^ 1 (f'')^2 \right) $,Assume $f$ is a twice differentiable function on $\Bbb R $ and $ f'' $ is continuous. Assume further that $ f(-1) = f(1) = 0$ then prove that $$\int\limits_{-1}^1 f' ^ 2\leq \frac{1}{2} \left( \int\limits_{-1} ^ 1 f^2 + \int _{-1}^ 1 (f'')^2 \right) $$ I thought of applying Rolle's theorem on $f$ but I'm not sure how to use it to prove this inequality. I also thought of using A.M. - G.M. inequality but it does not seem fruitful. I am stuck and unable to start this problem. Thank you.,"['integration', 'inequality', 'functions', 'real-analysis']"
3710452,If $\beta^{11}=(12893)$ in $S_{20}$.Find $\beta$,"Order of $\beta^{11}$ is 5. hence, $\frac{n}{(n, 11)}=5$ . If $11|n \implies n=55$ .So $\beta$ is a combination of 2 cycles 5 and 11. Let $\beta =(a_1, a_2,a_3,a_4,a_5)(a_6,a_7,a_8,a_9,a_{10},a_{11},a_{12},a_{13},a_{14},a_{15},a_{16})$ .Now the first 5 cycle will be equal to $(12893)$ and for the remaining elements I have a choice of $11$ elements out of $15$ .So predicting the element is not possible in this case If $n=5$ ,then $\beta$ is a 5 cycle so $\beta^{10} \beta =(12893) \implies \beta=(12893)$ This has been my attempt. Is this OK?","['symmetric-groups', 'group-theory', 'permutation-cycles']"
3710453,"For ordinals $\delta$ and $\beta$, if $\delta\in$ or $=\beta$, then there exists $\gamma\in$ or $=\beta$ such that $\beta=\delta+\gamma$","Prove that for ordinals $\delta$ and $\beta$ , if $\delta \in$ or $= \beta$ then there exists an ordinal $\gamma \in$ or $= \beta$ such that $\beta = \delta + \gamma$ . I tried using transfinite induction on $\delta$ Base case seems simple, since you can let $\gamma = \beta$ , but I'm stuck on the proof for both the successor ordinal and limit ordinal cases. My class has not yet defined ordinal subtraction, so I can't use that in the proof. Any help or hints would be appreciated!","['elementary-set-theory', 'proof-explanation', 'ordinals']"
3710492,"How many 3-letter words without repeating them can be made up of alphabet {a, b, c, d, e, f} in which the letter e or the letter f or both are used?","How many 3-letter words without repeating them can be made up of
alphabet {a, b, c, d, e, f} in which the letter e or the letter f or both are used? 
with permutations","['combinatorics', 'discrete-mathematics']"
3710640,How to show that the space of Kähler metrics is simply connected?,I wanted to find the proof showing that the space of Kähler metrics on a given Kähler manifold $M$ is simply connected. Any reference containing such topological results on the space of Kähler metrics would be highly appreciated.,"['differential-topology', 'kahler-manifolds', 'differential-geometry']"
3710641,Using GAP to find the left coset of a subgroup. What am I doing wrong?,"If $g=(1,2,3)\in A_4$ then to find the right coset $Hg$ of the subgroup $H=\langle(1, 2)(3, 4)\rangle=\{(), (1,2)(3,4)\}$ in $A_4$ , we just need to run the code: gap> a4:=AlternatingGroup(4);
Alt( [ 1 .. 4 ] )
gap> H:=Subgroup(a4, [(1,2)(3,4)]);
Group([ (1,2)(3,4) ])
gap> g:=(1,2,3);
(1,2,3)
gap> R:=RightCoset(H,g);
RightCoset(Group([ (1,2)(3,4) ]),(1,2,3))
gap> Elements(R);
[ (1,2,3), (1,3,4) ] However, repeating the same procedure for the left coset $gH$ returns the following error: gap> L:=LeftCoset(H,g);
Error, Variable: 'LeftCoset' must have a value Can someone please help me in understanding the following two things: 1) What does the error mean when finding the left coset $gH$ ? 2) Why is GAP calling the set $\{(1,2,3), (1,3,4)\}$ a right coset, when this is in fact the left coset $gH$ if you do the calculations by hand? Thank you for your help. EDIT: I am running GAP version 4.11.0, which is the latest version I think. EDIT: @DerekHolt solved the problem for me: gap> g1:=(1,3,2);
(1,3,2)
gap> L:=RightCoset(H,g1);
RightCoset(Group([ (1,2)(3,4) ]),(1,3,2))
gap> Elements(L);
[ (2,3,4), (1,3,2) ] So that the left coset would be $\{(2,4,3), (1,2,3)\}$ (I wonder if this last step can be automated?).","['gap', 'group-theory', 'abstract-algebra']"
3710656,Easy example of a bijective continuous self mapping whose inverse is discontinuous,"Let $f : X \to X$ be a continuous bijective mapping from a metric space onto itself. Is $f^{-1}$ continuous too? I don't think so, but I'm struggling to find a counterexample. I've read that if $X=\mathbb{R}^n$ or $X$ a compact Hausdorff space, $f^{-1}$ is always continuous. The Open Mapping Theorem gives us another restriction to find a counterexample, namely all linear operators from a Banach space to itself. Also, I know there's an easy example if we allow to use two different topologies. Here I'm only considering the topology induced by the one metric we have.","['inverse-function', 'metric-spaces', 'continuity', 'functional-analysis', 'general-topology']"
3710667,Need a book on Graduate Complex Analysis.,"Is there a book on complex analysis that deals with the same topic as the 3rd chapter of Nevanlinna Paatero's  ""Introduction to Complex Analysis"" but more rigorously?","['complex-analysis', 'reference-request']"
3710888,Embedding a manifold with connection in $\mathbb{R}^3$ (also need help with diff. eqs.),"Say a universe we want to study has two finite spatial dimensions, which we will imagine to describe a square of side length two. For the diff. manifold underlying the Newtonian spacetime with which we choose to study this universe, say we equip the set $M:=\mathbb{R}^{+}_0\times[-1,1]\times[-1,1]$ with $\mathcal{O}_{\text{St.}}|_M$ and $$\mathcal{A}:=\{(M,\text{id}_M)\}$$ Our universe has, at the points corresponding to $(t,0,0)\in M$ for any $t\in\mathbb{R}^{+}_0$ , a point mass of 1 unit mass. This means that on a point mass at coordinates $p=(t,x,y)$ acts a gravitational force of $$F_p:=\frac{G}{x^2+y^2}$$ Newtons. We set $G$ to $1$ . The force vector on the point mass at $p$ may be described with $$\widehat{\text{F}}_p:=(\widehat{\text{F}}^0_p,\widehat{\text{F}}^1_p,\widehat{\text{F}}^2_p)\equiv\left(0,\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2},\frac{\frac{y}{x}}{x^2+y^2}\right)$$ This determines the connection $\nabla$ (I define a connection as a map taking a vector field and a $(p,q)$ -tensor field to another $(p,q)$ -tensor field that satisfies the Leibnitz rule, agrees with partial differentiation for $(0,0)$ -tensor fields, is $C^\infty$ -linear w.r.t. the vector field input and respects addition of the tensor field input) on our manifold, as we set all the connection coefficient functions $\Gamma$ to zero except for $$\Gamma^i_{00}(p):=\widehat{\text{F}}^i_p$$ (A result of the autoparallel equation: Objects in the spacetime $M$ on which no force acts besides gravitation move along straight lines of $M$ as defined by the connection $\nabla$ if and only if we define the $\Gamma$ s this way.) I want to find an injective embedding $\phi:M\rightarrow\mathbb{R}^3$ of this spacetime manifold into three-dimensional Euclidean space such that the pullback connection $\nabla^*$ , coming from the Euclidean connection $\nabla^E$ , is equal to $\nabla$ . The idea behind this is that we would like to see the spacetime $M$ ""unstretched"" - with the curvature encoded into its embedding and resulting ""weird"" shape - in $\mathbb{R}^3$ . What is the best way to calculate what such an embedding of a spacetime would look like? And for which kind of connection carrying manifolds do such $\mathbb{R}^3$ visualizations actually exist? My approach of finding a differential equation for an embedding $\phi$ went as follows (I would be very glad for pointing out blunders in the calculation, as otherwise solving the diff. eqs. underneath is meaningless): Per definition, we have $$\nabla^*_X\ \phi^*(s)=\phi^*\left(\nabla^E_{\phi_*(X)}\ s\right)$$ for any covectors $s\in T^1_0\mathbb{R}^3$ . We will define $\phi^{-1}$ as a map from $\phi(M)$ to $M$ by $\phi^{-1}(p)=q$ iff $\phi(q)=p$ . This leads to: $$\nabla^*_{e_i}\ \phi^*((\phi^{-1})^*(\in^j))=\nabla^*_{e_i}\ \in^j=\sum_q-\Gamma^{*q}_{ij}\cdot\in^q=\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)$$ where $e_i$ and $\in^j$ are the natural basis and induced dual basis vector fields of our chart $(M,\text{id}_M)$ respectively. We want for $\Gamma^{*q}_{ij}=\Gamma^q_{ij}$ to hold and hence get: $$\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)=\left(\phi^*\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)\right)(e_k)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))$$ We start to run into a notational issue here: $\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)$ is an element of $C^\infty(M)$ , but $\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))$ is an element of $C^\infty(\phi(M))$ . The above equality is therefore, technically, not correct. As the latter term stems from the definition of the covector-field-pullback $\phi^*$ , the problem is that we forgot an input transformation via $(\phi(\cdot))$ at the end of it, i.e., we should write $$\left(\sum_q-\Gamma^q_{ij}\cdot\in^q\right)(e_k)(\cdot)=\left(\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)\right)(\phi_*(e_k))(\phi(\cdot))$$ We will keep this in mind and drop $(\phi(\cdot))$ again. We can calculate the left-hand side of the last result and use the Leibnitz rule for connections on the right-hand side: $$\sum_q\delta^q_k\cdot(-\Gamma^q_{ij})=\nabla^E_{\phi_*(e_i)}\ (\phi^{-1})^*(\in^j)(\phi_*(e_k))-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right)$$ This leads to: $$-\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \in^j\bigg((\phi^{-1})^*(\phi_*(e_k))\bigg)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi_*(e_i)}\ \phi_*(e_k)\right)$$ $$=\nabla^E_{\phi_*(e_i)}\ \in^j(e_k)-((\phi^{-1})^*(\in^j))\left(\nabla^E_{\phi^m_{*i}e^E_m}\ \phi^n_{*k}e^E_n\right)$$ where we use the Einstein summation convention at the lower index and input of $\nabla^E$ , define $e^E_i$ as the basis vector fields of $\mathbb{R}^3$ and $\phi^a_{*b}$ as the coefficient functions of the linear maps between the tangent spaces $T_pM$ and $T_p\mathbb{R}^3$ provided by $\phi_*(p)$ for each $p\in M$ . Further, we get: $$-\Gamma^k_{ij}=\nabla^E_{\phi_*(e_i)}\ \delta^j_k-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\nabla^E_{e^E_m}\ \phi^n_{*k}e^E_n\right)$$ $$\stackrel{Leibnitz}{=}0-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\nabla^E_{e^E_m}\ e^E_n+(\nabla^E_{e^E_m}\ \phi^n_{*k})\cdot e^E_n\bigg)\right)$$ $$=-((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot\bigg(\phi^n_{*k}\cdot\Gamma^{E\ a}_{mn}\cdot e^E_a+e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right)$$ where $\Gamma^{E\ q}_{ij}$ are the connection coefficient functions of three-dimensional Euclidean space. Per definition, these functions equal zero everywhere. Hence: $$\Gamma^k_{ij}=((\phi^{-1})^*(\in^j))\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\right)=\in^j\left((\phi^{-1})_*\bigg(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot e^E_n\bigg)\right)$$ Due to the linearity of $(\phi^{-1})_*$ we have: $$\Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})_*(e^E_n))\right)$$ By $(\phi^{-1})_{*a}^b$ we will denote the coefficient functions of the linear maps from $T_p\mathbb{R}^3$ to $T_pM$ given by $(\phi^{-1})_*$ at every point $p$ of the image $\phi(M)$ . As before, instead of viewing $(\phi^{-1})_{*a}^b$ as a $C^\infty(\phi(M))$ function, as the symbol itself suggests, we will treat it as a $C^\infty(M)$ function via the transformation $(\phi(\cdot))$ . Going on, we get: $$\Gamma^k_{ij}=\in^j\left(\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^a_{*n}\cdot e_a\right)=\delta^j_a\cdot\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}$$ Which lets us conclude with: $$\Gamma^k_{ij}=\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}\ \stackrel{Einstein}{:\iff}\ \Gamma^k_{ij}=\sum_m\sum_n\phi^m_{*i}\cdot e^E_m(\phi^n_{*k})\cdot (\phi^{-1})^j_{*n}$$ The map $\phi$ we search takes values from $\mathbb{R}^3\supset(\mathbb{R}^{+}_0\times[-1,1]\times[-1,1])$ to $\phi(M)\subset\mathbb{R}^3$ , which lets us exercise multivariable calculus on it: $$\phi^a_{*b}(p)=\frac{\partial\phi^a}{\partial e_b}(p)$$ $$(\phi^{-1})^a_{*b}(\phi(p))=\frac{\partial(\phi^{-1})^a(\phi(\cdot))}{\partial e_b}(p)$$ where $\phi^a$ and $(\phi^{-1})^a$ are the $a$ th component functions of $\phi$ and $\phi^{-1}$ respectively. This leads us to the following differential equations for $\phi$ : $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_1}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}$$ $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_0}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right)\cdot \frac{\partial(\phi^{-1})^0}{\partial e_n}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2}$$ and $$\left(\sum_m\sum_n\frac{\partial\phi^m}{\partial e_i}\cdot \frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_k}\right)\cdot \frac{\partial(\phi^{-1})^j}{\partial e_n}\right)(t,x,y)=0$$ for any $(i,j,k)\in\{0,1,2\}^3$ where either $i\neq0$ or $j\neq0$ . Is this set of equations solvable? Can we find the solution? How to approach this? Are there conventions one should respect in their notation? To me it seems that one big problem with this is that we don't know the codomain of our unknown $\phi$ - which is of course what I actually want to know. I am very unexperienced in diff. eqs. and have no approach to this. Is it acceptable that both $\phi$ and its inverse appear in the formula? I would also be interested to know if we can feed such complex systems of diff. equations into computational systems like Wolfram Alpha! Edit: I noticed that, for any invertable differentiable function $f:\mathbb{R}\rightarrow\mathbb{R}$ , $$f^{-1}(f(x))=x\implies\left(\frac{\partial}{\partial x}(f^{-1}\circ f)\right)(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(x))\cdot \frac{\partial f}{\partial x}(x)=1\implies\frac{\partial f^{-1}}{\partial x}(f(\cdot))=\frac{1}{\frac{\partial f}{\partial x}}(\cdot)$$ Hence, we can replace the partial deriviatives of $\phi^{-1}$ above with $$\frac{\partial(\phi^{-1})^0}{\partial e_n}(\phi(\cdot))=\frac{1}{\frac{\partial\phi^0}{\partial e_n}}(\cdot)$$ Edit 2: Unfortunately unsuccessful attempt: Assume we are really lucky and for all $(m,n)\in\{0,1,2\}^2\backslash\{(0,0)\}$ at least one of the two functions $\frac{\partial\phi^m}{\partial e_0}$ , $\frac{\partial}{\partial e_m}\left(\frac{\partial\phi^n}{\partial e_2}\right)$ vanish. Then we have: $$\frac{\frac{y}{x}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_2}\right)(t,x,y)$$ In more conventional notation: $$\frac{\partial}{\partial t}\left(\frac{\partial\phi^0}{\partial y}\right)(t,x,y)=\frac{\frac{y}{x}}{x^2+y^2}$$ $$\implies\frac{\partial\phi^0}{\partial y}(t,x,y)=\int\frac{\frac{y}{x}}{x^2+y^2}\text{d}t=t\cdot\frac{\frac{y}{x}}{x^2+y^2}$$ $$\implies\phi^0(t,x,y)=\int t\cdot\frac{\frac{y}{x}}{x^2+y^2}\text{d}y$$ Freely available math software (like Wolfram Alpha) can solve this integral, and so we get: $$\phi^0(t,x,y)=t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x}$$ If this were true, we would have one coordinate component of our embedding function $\phi$ nailed down. But it fails: If the assumption starting our current approach off holds, we have $$\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}=\left(\frac{\partial\phi^0}{\partial e_0}\cdot \frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)\cdot \frac{1}{\frac{\partial\phi^0}{\partial e_0}}\right)(t,x,y)=\frac{\partial}{\partial e_0}\left(\frac{\partial\phi^0}{\partial e_1}\right)(t,x,y)$$ Which puts the following condition on our calculated component of $\phi$ : $$\frac{\partial}{\partial t}\left(\frac{\partial}{\partial x}\left(t\cdot\frac{\text{ln}(y)}{x}-t\cdot\frac{\text{ln}(x^2+y^2)}{2x}\right)\right)=\frac{\sqrt{1-\frac{y^2}{x^2}}}{x^2+y^2}$$ But, as simple differentiation shows, this equality is not satisfied.","['connections', 'smooth-manifolds', 'multivariable-calculus', 'partial-differential-equations', 'differential-geometry']"
3710993,Exponent Laws [confused?],"I was just reading about exponent laws and came across the image below. On the first line, if I solve the equation commutativity, I get the answer $a^5$ , but if I solve it with the distributive method I get $6a^2$ . Why am I getting two different answers for equally valid methods?",['algebra-precalculus']
3711024,What is meaning of $X/P$? ($X$ is a set and $P$ is a partition),"The definition of $x/E$ when $E$ is an equivalence relation is : $$x/E = \{y\in X \mid (y,x)\in E \},$$ and the definition of $X/E$ : $$X/E = \{x/E\ \mid x\in X\}.$$ Now, what is $X/P$ when $P$ is a non-empty partition of X?","['elementary-set-theory', 'equivalence-relations', 'set-partition', 'relations']"
3711039,Questions on a proof in do Carmo's Riemannian Geometry - Computations with Lie groups and Lie algebras,"I am a little confused with making computations with Lie groups and Lie algebras, and would appreciate very much any help with the following questions. In page 44 of my Brazilian edition of do Carmo's Riemannian Geometry, he states that if a Lie group $G$ has a bi-invariant metric, the inner product that the metric induces on the Lie algebra $\mathcal G$ satisfies $$
\langle [U, X], V \rangle = - \langle U, [V, X] \rangle.
$$ He argues in the following way: For every $a \in G$ , the automorphism $R_{a^{-1}}L_a: G \longrightarrow G$ is a diffeomorphism that leaves $e$ fixed.  Therefore, the differential $d(R_{a^{-1}}L_a) = Ad(a): \mathcal G \longrightarrow \mathcal G$ is a linear map. First question: the differential above is at $e$ , right? He proceeds:
Explicitly, $$
Ad(a)Y = dR_{a^{-1}} dL_a Y = dR_{a^{-1}}Y \quad \forall Y \in \mathcal G
$$ Second question: Here, the differential $dR_{a^{-1}}$ is computed at $a$ , right? So it should be $$
d(R_{a^{-1}})_a (dL_a)_e Y(e) = d(R_{a^{-1}})_aY(a).
$$ He now argues that if $x_t$ is the flow of $X \in \mathcal G$ then $$
[Y, X] = \lim_{t \to 0} \frac1t (dx_t(Y) - Y).
$$ Third question: Shouldn't it be $$
[Y, X] = \lim_{t \to 0} \frac1t (dx_t(Y) - Y)x_t ?
$$ Following, he claims that since $X$ is left-invariant, then $L_y \circ x_t = x_t \circ L_y$ Why does it hold? He then concludes the proof, but the remaining of it I think I can understand Thanks in advance and kind regards.","['riemannian-geometry', 'lie-algebras', 'lie-groups', 'differential-geometry']"
3711046,Why no flat metric on a sphere?,"What is a simple proof that there is no flat metric on a sphere, but there is a flat metric on a torus? Ideally such a proof would clearly distinguish between the sphere and the torus. Naively one might think neither can be
endowed with a flat metric. I'm hoping for some insight that
reveals this naive view to be incorrect.","['metric-spaces', 'riemannian-geometry', 'differential-geometry']"
3711149,$p$-Norm of Block Diagonal Matrix,"Let $A\in \mathbb{K}^{r\times r}$ , $B\in \mathbb{K}^{(n-r)\times (n-r)}$ and $C = \mathbb{K}^{n\times n}$ such that \begin{equation}
C =
\begin{pmatrix}
A & 0_{r\times (n-r)}\\
0_{(n-r)\times r} & B\\
\end{pmatrix}.
\end{equation} I want to prove $||C||_p = \max\{||A||_p, ||B||_p\}$ for all induced $p$ -matrix norms. I already showed the direction "" $\ge$ "" by writing \begin{equation}
||C||_p = \max_{||x||_p = 1} ||Cx||_p \ge \max_{\substack{||x||_p = 1\\ x_j=0\ \forall\ j>r}} ||Cx||_p = \max_{||x||_p = 1} ||Ax||_p = ||A||_p
\end{equation} and the same for $B$ , thus $||C||_p \ge \max\{||A||_p, ||B||_p\}$ . However, I do not see an easy approach to prove the direction "" $\le$ "". Any help is appreciated.","['matrices', 'normed-spaces', 'linear-algebra', 'block-matrices']"
3711152,Can transitive graphs have non-integer growth dimension?,"Say we have some transitive (edge and vertex) graph $G$ s.t $|B(v,n)| \leq c n^\alpha$ for some $\alpha\geq 0$ . Polynomial growth/dimension in this setting meaning that for some $d\geq 0$ we can pick two constants $a,b$ s.t $an^d\leq |B(v,n)| \leq bn^d$ for every integer $n$ . Is it always the case that $d$ will be an integer when $G$ is transitive?","['graph-theory', 'combinatorics', 'probability-theory']"
