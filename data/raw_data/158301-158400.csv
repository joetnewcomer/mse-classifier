question_id,title,body,tags
2714085,Maximizing expectation integral given PDF inequality,"I came across a paper that in the context of one of its proofs takes an upper bound on a random variable's probability distribution and turns it into a bound on its expected value. In particular, let $X$ be a random variable with PDF $f$. The result is that if $$P(X \geq x) = \int_x^\infty f(\gamma) d\gamma \leq 2 e^{-2mx^2}$$ then $$E\left[e^{2(m-1)X^2}\right] = \int_0^\infty f(\gamma)e^{2(m-1)\gamma^2} d\gamma \leq 4m.$$ The paper proposes a proof saying that the first constraint is met with equality for $g(\gamma) = 8 m \gamma e^{-2m\gamma^2}$ and that this distribution will maximize the expected value expression. For this distribution one can compute the expected value to be $E\left[e^{2(m-1)X^2}\right] = \int_0^\infty 8 m \gamma e^{-2m\gamma^2}e^{2(m-1)\gamma^2} d\gamma = 4m$. I believe that the result is true (I came up with a different proof) but I am confused about the validity of this argument. Why is it true that a distribution, $g$, that achieves equality in the first constraint will also maximize the expected value expression? If it was true that for such a $g$ we knew $g(\gamma) \geq f(\gamma)$ for any $f$ satisfying the first inequality then I could see why this argument is valid. However, I don't think that's necessarily true. Consider the following counterexample. Suppose that $\int_x^1 f(\gamma) d\gamma \leq 1 - x$. Equality can be achieved for $g(\gamma) = 1$. However, $f(\gamma) =  3/2 e^{-\gamma}$ also meets the inequality and yet it is not true that $ 3/2 e^{-\gamma}  \leq 1$ for all $\gamma$. So I am wondering if this argument is valid and if so how?","['expectation', 'integral-inequality', 'probability']"
2714092,How to understand the coidea of a colagebra?,Let $C$ be a coalgebra with coproduct $\Delta$ and counit $\epsilon$. Then a subset $I\subseteq C$ is a coideal if $\Delta(I)\subseteq I\otimes C+C\otimes I$ and $\epsilon(I)=0$. My question is why the coideal should be defined by this way? Does it show the  absorptivity ?,"['abstract-algebra', 'hopf-algebras', 'coalgebras']"
2714132,Use of a Laplace Transform to solve Abel's Integral Equation,"I am following an exercise where I have to solve Abel's integral equation: $$f(t)=\int_{0}^{t} \frac {\phi(\tau)}{(t-\tau)^{\alpha}} d\tau$$ I have taken the first step and shown that: $$\bar{\phi }(s) = \frac{1}{\Gamma(1-\alpha)}s^{1-\alpha}\bar{f}(s)$$ However, I now have to assume a function $\psi$ such that $\phi=d\psi/dt$ and $\psi(0)=0$.  Using my above result I have find the Laplace transform of $\psi(t)$ and then show that $\phi(t)=\frac{\sin \pi \alpha}{\pi}\frac{d}{dt}\int^t_0\frac{f(\tau)}{(t-\tau)^{1-\alpha}}d\tau$ Could anyone provide some pointers on how I would do this?  I am guessing that the sine term will appear due to the identity $1/\Gamma(z)\Gamma(1-z)=\frac{\sin \pi z}{\pi}$ but not sure how exactly to carry it all out.","['integral-equations', 'complex-analysis', 'laplace-transform']"
2714134,Levy measure of borel sets away from $0$,"The following is  of Philip Protter at page 26 of the book Stochastic integration and Differential equations that I have not been able to proved yet. Let $X$ a Levy process, and $\Lambda$ a borel set in $\mathbb{R}$ away from $0$ (that is, $0 \notin \bar{\Lambda}$), then $\nu(\Lambda) = E[N_{1}^{\Lambda}] < \infty$. Where $N_{t}^{\Lambda} = \sum_{0 < s \leq t} 1_{\Lambda}(\Delta X_{s})$.
Protter says that it follows from Theorem 34, but for me it means I have to prove  first that the jumping times have independent and stationary increments something that I am not sure. My attempt I assume that the jumping times $T_{\Lambda}^{1} := \lbrace t >0 : \Delta X_t \in \Lambda \rbrace, \cdots, T_{\Lambda}^{n}:= \lbrace t > T_{\Lambda}^{n-1}: \Delta X_{t} \in \Lambda \rbrace,...$ have independent and stationary increments. Therefore, taking into account that $ T_{\Lambda}^{n} = T_{\Lambda}^{1}  + \sum_{k=1}^{n} T_{\Lambda}^{k} - T_{\Lambda}^{k-1} $ we get \begin{align}
\nu(\Lambda) &= E[N_{1}^{\Lambda}] \\
&=\sum_{n=1}^{\infty}P[T_{\Lambda}^{n} \leq 1] \\
&\leq \sum_{n=1}^{\infty}P[T_{\Lambda}^{1} \leq 1, ..., T_{\Lambda}^{n} - T_{\Lambda}^{n-1} \leq 1 ] \\
&\leq \sum_{n=1}^{\infty}(P[T_{\Lambda}^{1} \leq 1])^{n} < \infty
\end{align}
and the last part depends on the fact $P[T_{\Lambda}^{1} \leq 1] < 1$ something that I am not quite sure. Apparently, this is not at all an intuitive fact. Any hint will be welcome.","['stochastic-processes', 'probability-theory', 'stopping-times', 'levy-processes']"
2714146,Evaluate $\int_{0}^{\frac{\pi}{2}}\frac{x^2}{ \sin x}dx$,"I want to evaluate $$\int_{0}^{\frac{\pi}{2}}\frac{x^2}{ \sin x}dx$$
First,I tried to evaluate like this:
$$\int_{0}^{\frac{\pi}{2}}\frac{x^2}{ \sin x}dx=\int_{0}^{\frac{\pi}{2}}x^2\left(\frac{1+\cos x}{\sin x}\right)\frac{dx}{1+\cos x}=\int_{0}^{\frac{\pi}{2}}x^2\left(\frac{1+\cos x}{\sin x}\right)d\left(\frac{\sin x}{1+\cos x}\right)$$ $$=\int_{0}^{\frac{\pi}{2}}x^2d\log\left(\frac{\sin x}{1+\cos x}\right)=x^2\log\left(\frac{\sin x}{1+\cos x}\right)|_{0}^{\frac{\pi}{2}}-2\int_{0}^{\frac{\pi}{2}}x\log\left(\frac{\sin x}{1+\cos x}\right)dx$$ $$=0+2\int_{0}^{\frac{\pi}{2}}x\log\left(\frac{1+\cos x}{\sin x}\right)dx=2\int_{0}^{\frac{\pi}{2}}x\log\left(1+\cos x\right)dx-2\int_{0}^{\frac{\pi}{2}}x\log\left(\sin x\right)dx$$
$$=2\int_{0}^{\frac{\pi}{2}}x\log\cot \left(\frac{x}{2}\right)dx=8\int_{0}^{\frac{\pi}{4}}x\log\cot xdx$$
but I can't proceed next step,help me,thanks.","['integration', 'calculus', 'analysis']"
2714237,The sum of continuous almost everywhere function is also continuous almost everywhere?,"My textbook is asking me to solve this exercise.
I thought this is trivial, but The author put ‘prove or disprove’. So I’m confusing. Let f and g be a real-valued function on [a,b]. And assume that f and g are continuous almost everywhere on [a,b]. Prove or disprove : f+g is continuous almost everywhere. I think this is true. If f is continuous at x and g is continuous at x, f+g is continuous at x. So if A is the set of points of discontinuity of f, and B is of g, then AUB contains the set C of discontinuous points of f+g, i.e. C is contained in AUB By definition, A and B is of measure zero. And the sum of countably many measure zero sets is also of measure zero. So, AUB is of measure zero. Therefore C is of measure zero. So f+g is continuous almost everywhere. Am I right? Thanks","['almost-everywhere', 'real-analysis', 'measure-theory']"
2714262,How can we prove that $\pi > 3$ using this definition,"I've been trying to prove that $\pi > 3$ by using the following definition: $$\pi = 2\int_{-1}^1{ {\sqrt{1-t^2}}}\, dt$$ Which comes from finding what the area of the unit circle is. (This path can be found in Spivak's Calculus, in case someone wants to read about this topic) I've done it already using sums and geometry, but I'm having a really bad time trying to find a good starting point, let alone the entire path for this proof. Any help would be greatly appreciated.","['inequality', 'integral-inequality', 'calculus', 'integration', 'pi']"
2714275,Prerequisites for Stanley's Enumerative Combinatorics,"I want to learn combinatorics, and have read people's messages around here recommending Enumerative Combinatorics by Stanley; most of these suggestions also state that it's a dense book. If by dense they mean that it is really detailed, I have no problem with it, but it also makes me wonder if I need any previous knowledge before reading it. Thanks in advance.","['combinatorics', 'book-recommendation', 'reference-request', 'soft-question']"
2714450,"Does $\exp(At)=\exp(Bt)$ for infinitely many $t$ imply $A=B$ where $A,B$ are square matrices? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Suppose $A$ and $B$ are two square matrices so that $e^{At}=e^{Bt}$ for infinite (countable or uncountable) values of $t$ where $t$ is positive. Do you think that $A$ has to be equal to $B$? Thanks,
Trung Dung. Maybe I do not state clearly or correctly. I mean that the equality holds for all $t\in (0, T)$ where $T>0$ or $T=+\infty$, i.e. for uncountable $t$. In this case I think some of the counter-examples above do not work because it is correct for countable $t$.","['matrices', 'matrix-equations', 'matrix-exponential', 'linear-algebra']"
2714470,recurrence relation function definition,"From my textbook: ""A recurrence relation of degree k is defined by a function $p: N \times Z^k \rightarrow Z $ and an initial sequence of integers $a_0,a_1,...,a_k$ such that for all $n\ge k$ the number $a_n$ is equal to $p(n,a_{n-1},...,a_{n-k})$.
The problem with this definition is that it describes any function on the natural number if there is no restriction on the properties of the function p. But usually some form of restriction on p is implicit in this definition."" My questions: So the function $p$ has the domain of paired natural numbers and k-dimensional integers?and it  maps to the set of integers? Does it mean when the inputs are $n,a_{n-1},...,a_{n-k}$ then it maps to $a_n$? If the sequence is a function from $N$ to $R$, so in the definition $Z$ should be replaced by $R$? Why does p need to have some form of restriction and what is the problem that it describes any function on the natural numbers?","['recurrence-relations', 'discrete-mathematics']"
2714503,Number of 7 lettered words each consisting of 3 vowels and 4 consonants which can be formed by using the letters of the word DIFFERENTIATION,"Find the number of 7 lettered words each consisting of 3 vowels and 4 consonants which can be formed by using the letters of the word DIFFERENTIATION. Vowels are IIIEEAO. Consonants are DFFNNTTR.Vowels can be selected in $\binom{4}{3}$ ways and consonants can be selected in $\binom{5}{4}$ ways and total ways are $\binom{4}{3}\binom{5}{4}7!=100800$ ways, but the answer is 532770. Where did I go wrong?",['combinatorics']
2714517,Usage of Schwarz Reflection Principle to Study Conformal Equivalence of Annuli,"Let $A(1,r) = \{z \in \mathbb{C} : 1 < |z| < r\}$. I would like to prove the standard result that $A(1,r)$ and $A(1,r')$ are conformally equivalent iff $r = r'$. To prove the nontrivial direction, suppose that we have an analytic isomorphism $f \colon A(1,r) \to A(1,r')$. I would like to iteratively use the Schwarz reflection principle to extend this isomorphism to an automorphism of the punctured plane with a removable discontinuity at the origin (I've seen this idea suggested in various sources). Then, I can use the fact that the only automorphisms of the plane are of the form $z \mapsto az + b$ to deduce the desired result. However, to use Schwarz, I seem to require that $f$ have a continuous extension to the boundary of the annulus $A(1,r)$. Is there any way to show that $f$ has such a continuous extension, or is there a version of Schwarz that doesn't put this requirement on $f$?","['continuity', 'complex-analysis', 'conformal-geometry']"
2714550,"Theorem 3.4.11 in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Limit superior and limit inferior of a bounded sequence of real numbers","Here is Theorem 3.4.11 in the book Introduction to Real Analysis by Robert G. Bartle and Donald R. Sherbert, 4th edition: If $\left( x_n \right)$ is a bounded sequence of real numbers, then the following statements for a real number $x^*$ are equivalent. (a) $x^* = \limsup \left( x_n \right)$. (b) If $\varepsilon > 0$, there are at most a finite number of $n \in \mathbb{N}$ such that $x^* + \varepsilon < x_n$, but an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (c) If $u_m = \sup \left\{ \ x_n \ \colon \ n \geq m \ \right\}$, then $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} = \lim \left( u_m \right)$. (d) If $S$ is the set of subsequential limits of $\left( x_n \right)$, then $x^* = \sup S$. And, here is Definition 3.4.10: Let $X = \left( x_n \right)$ be a bounded sequence of real numbers. (a) The limit superior of $\left( x_n \right)$ is the infimum of the set $V$ of $v \in \mathbb{R}$ such that $v < x_n$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by 
  $$ \limsup \left( x_n \right) \qquad \mbox{ or } \qquad \limsup X \qquad \mbox{ or } \qquad \overline{\lim} \left( x_n \right). $$ (b) The limit inferior of $\left( x_n \right)$ is the supremum of the set of $w \in \mathbb{R}$ such that $x_n < w$ for at most a finite number of $n \in \mathbb{N}$. It is denoted by 
  $$ \liminf  \left( x_n \right) \qquad \mbox{ or } \qquad \liminf X \qquad \mbox{ or } \qquad \underline{\lim} \left( x_n \right). $$ Finally, here is the proof of Theorem 3.4.11: [In what follows, I have included my questions and explanations within square brackets.] (a) implies (b). If $\varepsilon > 0$, then the fact that $x^*$ is an infimum implies that there exists a $v$ in $V$ such that $x^* \leq v < x^* + \varepsilon$. Therefore $x^*$ also belongs to $V$, [I think here it should be $x^*+\varepsilon$ instead of $x^*$. Am I right?] so there can be at most a finite number of $n \in \mathbb{N}$ such that $x^*+\varepsilon < x_n$. On the other hand, $x^*-\varepsilon$ is not in $V$ so there are an infinite number $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$. (b) implies (c). If (b) holds, given $\varepsilon > 0$, then for all sufficiently large $m$ we have $u_m < x+\varepsilon$. [I think this should be $u_m < x^* + \varepsilon$. Am I right?] Therefore, $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} \leq x^* + \varepsilon$. [I think we can even write this as $\inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\} < x^* + \varepsilon $. Am I right?] Also, since there are an infinite number of $n \in \mathbb{N}$ such that $x^* - \varepsilon < x_n$, then $x^* - \varepsilon < u_m$ for all $m \in \mathbb{N}$ and hence $x^* - \varepsilon \leq \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Since $\varepsilon > 0$ is arbitrary, we conclude that $x^* = \inf \left\{ \ u_m \ \colon \ m \in \mathbb{N} \ \right\}$. Moreover, since the sequence $\left( u_m \right)$ is monotone decreasing, we have $\inf \left( u_m \right) = \lim \left( u_m \right)$. (c) implies (d). Suppose that $X^\prime = \left( x_{n_k} \right)$ is a convergent subsequence of $X = \left( x_n \right)$. Since $n_k \geq k$, we have $x_{n_k} \leq u_k$ and hence $\lim X^\prime \leq \lim \left( u_k \right) = x^*$. Conversely, there exists $n_1$ such that $u_1 - 1 \leq x_{n_1} \leq u_1$. [I think here we can even write $u_1 - 1 < x_{n_1} \leq u_1$. Am I right?] [Suppose that $n_k$ has been chosen.] Inductively choose $n_{k+1} > n_k$ such that 
  $$ u_k - \frac{1}{k+1} < x_{n_{k+1}} \leq u_k. $$
  [But I think we should write this as $u_{k+1} - \frac{1}{k+1} < x_{n_{k+1}} \leq u_{k+1}$ or as $u_{k+1} - \frac{1}{k+1} \leq x_{n_{k+1}} \leq u_{k+1}$. Am I right? What is an explicit procedure and justification of how $x_{n_2}$ can be chosen such that $n_2 > n_1$ and such that $u_2 - \frac{1}{2} \leq x_{n_2} \leq u_2$? I would appreciate a detailed account of this. ] Since $\lim \left( u_k \right) = x^*$, it follows that $x^* = \lim \left( x_{n_k} \right)$, and hence $x^* \in S$. [In fact, here we have even shown that $x^*$ is the largest element of $S$. Am I right?] (d) implies (a). Let $w = \sup S$. If $\varepsilon > 0$ is given, then [because there can be no subsequential limit of $\left( x_n \right)$ exceeding $w$ and, as there is always a convergent subsequence of every bounded sequence of real numbers, so there can be no subsequence of $\left( x_n \right)$ having infinitely many terms in the interval $(w, +\infty)$] there are at most finitely many $n$ with $w + \varepsilon < x_n$. Therefore $w+\varepsilon$ belongs to $V$ and $\limsup \left( x_n \right) \leq w+\varepsilon$. On the other hand, there exists a subsequence of $\left( x_n \right)$ converging to some number larger than $w - \varepsilon$, [We call that number $x$.] so that $w - \varepsilon$ is not in $V$,  [Is this because there are infinitely many terms of that convergent subsequence that are in any neighborhood of $x$ and hence are greater than $w + \varepsilon$?] and hence $w - \varepsilon \leq \limsup \left( x_n \right)$. [How is this true? Is it because of the fact that if $\limsup \left(x_n \right)$ were less than $w - \varepsilon$, then there would be some element $v \in V$ such that  $\limsup \left(x_n \right) \leq v < w - \varepsilon < x$ and by definition of set $V$ in that case there would be only finitely many terms of the sequence exceeding $v$ and hence only finitely many terms of the sequence in the neighborhood $( w - \varepsilon , 2x + w-\varepsilon )$, for example, of $x$?] Since $\varepsilon > 0$ is arbitrary, we conclude that $w = \limsup \left( x_n \right)$. In the above proof, I have included my questions and points of confusion within square brackets. Is there any occasion where (in any of my explanations within square brackets) I've gone wrong with my reasoning? In (c) implies (d), once we have shown that $x^*$ is an upper bound for the set $S$ of subsequential limits of $\left( x_n \right)$, is there an alternative approach to showing that $x^*$ is in fact the supremum of $S$ (or even an element of $S$)?","['real-analysis', 'limsup-and-liminf', 'calculus', 'proof-explanation', 'sequences-and-series']"
2714603,Divergence of Petersson inner product,"Consider a complex number $z = x + i y$ and functions $f,g : \mathbb{H} \to \mathbb{C}$ (where $\mathbb{H}$ is the upper half-plane i.e. complex numbers whose imaginary part is greater equal to zero). Actually we can ask that $f,g$ are modular forms for $SL_2(\mathbb{Z})$. Then one can define the Petersson inner product as
$$
\langle f, g \rangle := \int_{\mathbb{H}/SL_2(\mathbb{Z})} f(z) \overline{g(z)} y^{2k}\frac{dx dy}{y^2}
$$
as is defined in equation (2.3) in this paper . Now, we read in section 3 of the same paper that if $f,g$ are weakly holomorphic modular forms (that is meromorphic modular forms all of whose poles are contained at cusps) then the inner Petersson inner product will generally diverge. I understand that this is the case since we end up integrating (as far the interesting imaginary part of $z$ is concerned) all the way up to infinity. I would like to ask for a simple, down to earth explicit example where this divergence is apparent and what a simple way to regularize such a divergence would be. I just need to get some intuition really.","['number-theory', 'analytic-number-theory', 'modular-forms', 'inner-products']"
2714607,Nonexistence of a homeomorphism between a open set and the unit sphere,"Let $U\subset\mathbb{R^n}$ be a open set and $\mathbb{S^n}$ the unit sphere of $\mathbb{R^{n+1}}$(i.e. $\mathbb{S^n}=\{x\in\mathbb{R^{n+1}}:||x||=1\}$).
How can I show that there's no homeomorphism between $U$ and $\mathbb{S^n}$? My progress: As image of connected set over a continuous function is connected and $\mathbb{S^n}$ is connected, if there was such homeomorphism, then $U$ would be connected as well. Now, using the fact that all connected open sets of $\mathbb{R^n}$ is homeomorphic to $\mathbb{R^n}$, it suffice to show that there's no homeomorphism between $\mathbb{R^n}$ and $\mathbb{S^n}$. I know that for all $p\in \mathbb{S^n}$, $\mathbb{S^n}- \{p\}$ is homeomorphic to $\mathbb{R^n}$. Any help would be much appreciated!","['general-topology', 'spheres', 'real-analysis']"
2714630,"Is every group (Z/nZ, +) isomorphic to some subgroup of $(Z/mZ)^\times$? If not, find all n for which this doesn't hold","My progress on this so far : ($\mathbb{Z}$/$n\mathbb{Z}$, $+$) is basically the cyclic group $C_n$. So the question boils down to if there is an element $x$ of order $n$ for any $n$ in the group  $(\mathbb{Z}$/$m\mathbb{Z})^\times $, because then $\langle x\rangle = (x, x^2, x^3,...,x^{m-1}, e)$ forms a cyclic group of order $m$. The question can then also be formulated as the problem of finding $S$ where $$S = \{ n : \not\exists a, m \in \mathbb{Z}^+ s.t. a^n \equiv 1\ (mod\ m)\}$$ Or as $$ S = \{ n : n\  \text{is not the order of any number a mod any number m }\}$$ From this I can definitely conclude that $x \in S\ if\ \forall  n\ x \nmid \phi(n) $, because of Euler's Theorem , but I guess that doesn't really give us anything. I'm not sure if this is the way I should be going about the problem, and if it is then I am not sure on how I should proceed further.","['group-theory', 'cyclic-groups']"
2714657,A sequence $p_n(x)$ that converges for infinitely many values of $x$,"Let $(a_n)_{n \geq 1}, (b_n)_{n \geq 1}, (c_n)_{n \geq 1}$ be sequences of real numbers. Knowing that the sequence $$p_n(x)=(x-a_n)(x-b_n)(x-c_n)$$
  converges for infinitely many values of $x$, prove that it converges for every $x \in \mathbb{R}$. This is very similar to what happens to a polynomial when it is involved in something which happens ""for infinitely many values"": it actually happens for all values. Starting from this, I tried to write $$p_n(x)=x^3-(a_n+b_n+c_n)x^2+(a_nb_n+b_nc_n+c_na_n)x-a_nb_nc_n$$
But from here, I don't know anything about these $3$ sequences and I couldn't proceed further.","['real-analysis', 'limits', 'sequences-and-series', 'calculus', 'convergence-divergence']"
2714690,Proof for this identity: $\sum\limits_{k=0}^\infty(\zeta(2k+3)-1) = \frac{1}{4}$,"$$
\sum_{k=0}^\infty (\zeta(2k+3) 
-1) = \frac{1}{4}
$$
I found this nice identity and want to prove it, but I don't know how.
My first thought was that I can create a geometric series with the same limit like:
$$\sum_{k=0}^\infty \frac{1}{8} \cdot \frac{1}{2^k} = \frac{1}{4}$$
But I have no idea, how to write the first one in a way, that these are the same. They are obviously only equal, if we sum infinitely many terms together.
Do you have ideas, how to proof this identity or do you know, where the proof can be found?
Thank you for your answers!","['riemann-zeta', 'sequences-and-series', 'geometric-series']"
2714711,"how do I show this :$\sum_{k=0}^{2n }\binom{2n}{k} \sin ((n-k)x)=0$ , for every real $x$ and for every integer $n$?","My attempt fails to show this formula $\sum_{k=0}^{2n }\binom{2n}{k} \sin ((n-k)x)=0$ which I have accrossed in my textbook, using induction proof, but I think by induction seems very hard, I want to know if there is any simple way to show the titled identity that is true for every for every real $x$ and for every integer $n$ ? Thanks in advance.","['induction', 'summation', 'trigonometry', 'convergence-divergence']"
2714748,Open subsets of a irreducible topological space,"I have this problem: ""Show that a topological space X$\neq\emptyset$ is irreducible $\Leftrightarrow$ $\forall$ U open subset of X, U is connected. "" I can easily prove the $\Rightarrow$ part, using that every open subset of an irreducible space is itself irreducible (therefore connected). But what about the $\Leftarrow$ part? I have no idea how to prove it, and to be fair I'm not even sure it's true. Thanks everybody!","['general-topology', 'algebraic-geometry']"
2714774,Rigorous proof to show that the $15$-Puzzle problem is unsolvable,"So this is supposedly a very popular puzzle by Sam Loyd. (I don't want answerers to provide solutions directly from some website etc. I mean, an ingenious solution is more welcome please.) Now, as you see all numbers are arranged in ascending order except the last two. The rule of the game is to move a numbered block which is adjacent to the blank space and create another blank place in its original position (I mean, you just shift the position of blank space around by sliding the numbers around) . Now the question goes as, can you arrange all the numbers in the correct ascending order? (The final outcome would be just interchange the positions of $15$ and $14$). I know of a solution using a very clever trick of invariance (the invariant seemed quite non-trivial to me ;-P). Can others here come up with some interesting solutions?","['invariance', 'proof-writing', 'puzzle', 'recreational-mathematics', 'discrete-mathematics']"
2714780,Alternating Series Test Proof,"I am somewhat stuck on this proof of the alternating series test, could you please point me to the right direction ? Let $(a_n)$ be a decreasing sequence that converges to $0$. Prove that the series
$$\sum_{n=1}^{\infty}(-1)^{n+1}a_n$$
converges by showing that the sequnce of partial sums is a cauchy sequence. Proof. Let $s_m$ denote the mth partial sum, that is,
$$s_m = a_1-a_2+a_3\dots\pm a_m$$
Observe that
$$|s_n - s_m| = |(-1)^{(m+1)+1}a_{m+1}+(-1)^{(m+2)+1}a_{m+2}+\dots +(-1)^{(n)+1}a_{n}|$$
$$\leq |a_{m+1}-a_{m+2}| + |a_{m+3}-a_{m+4}|+\dots +|a_{n-1}-a_n|$$
$$\leq (n-m)|a_{m+1}-a_{m+2}|$$
I think the next step is to use the fact that $(a_n)$ is a Cauchy sequence (because it is convergent.) to show that the above expression can be made as small as possible, however, $(n-m)$ is a variable quantity so I am not really sure how to proceed.","['sequences-and-series', 'proof-verification']"
2714829,"Let $f\in L^1(\mathbb R)$ and let $F,G:\mathbb R\to\mathbb R$ be the functions defined by: ...","I have applied for a Ph.D. in Trieste and am preparing for the exams. I am having a problem with Problem 8 here . Here is the text. Let $f\in L^1(\mathbb R)$ and let $F,G:\mathbb R\to\mathbb R$ be the functions defined by: $$F(x)=\int_x^{x+1}f(t)dt,\qquad\text{and}\qquad G(x)=\left|\int_x^{x+1}f(t)dt\right|.$$ (a) Prove that $G$ has a maximum point on $\mathbb R$ . (b) Give an example of $f\in L^1(\mathbb R)$ such that $F$ has no maximum point on $\mathbb R$ . Now, unless I'm much mistaken (proof at question end), we have: $F,G$ continuous on $\mathbb R$ ; $F,G$ tend to 0 as $|x|\to\infty$ . With that, by 2., both $F$ and $G$ are less than their sup-norms on $\mathbb R$ whenever $|x|>M$ for $M$ big enough, and by 1. and the compactness of $[-M,M]$ they must have a maximum on $[-M,M]$ , which is then a global maximum on $\mathbb R$ . So (a) is done, and (b)… is asking me to disprove the maximum of $F$ which I just proved, so it is impossible! Is my reasoning above correct? Are the proofs below correct? Or is there anything I am missing that is wrong in them? Proofs $F,G\to0$ as $|x|\to\infty$ I write $|x|\to\infty$ to say $x\to\infty$ or $x\to-\infty$ , so let's do $x\to\infty$ , and $x\to-\infty$ is proved the same way, more or less. Now: $$|F(x)|\leq\int_x^{x+1}|f(t)|dt\leq\int_x^{+\infty}|f(t)|dt,$$ which tends to zero for $x\to+\infty$ since $f\in L^1(\mathbb R)$ . And of course $G$ is already handled this way. For $x\to-\infty$ : $$|F(x)|\leq\int_{-\infty}^{x+1}|f(t)|dt.$$ Continuity We rewrite: $$F(x)=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt,$$ $1_A$ being the indicator of $A$ . Suppose $x\to x_0$ . If $t<x_0$ , then $t<x$ eventually, so that $f(t)1_{[x,x+1]}(t)=0$ eventually, hence $f(t)1_{[x,x+1]}\to0$ . The same occurs if $t>x_0+1$ , whereas of $t\in[x_0,x_0+1]$ then $t\in[x,x+1]$ eventually so that $f(t)1_{[x,x+1]}(t)=f(t)$ eventually. So for $t\neq x_0$ we have $f(t)1_{[x,x+1]}(t)\to f(t)1_{[x_0,x_0+1]}(t)$ . Since this leaves out only two points, $x_0$ and $x_0+1$ , the convergence is pointwise almost everywhere. All of these functions have absolute values that is at most $|f(t)|$ , so by dominated convergence we have: $$F(x)=\int_x^{x+1}f(t)dt=\int_{-\infty}^{+\infty}f(t)1_{[x,x+1]}(t)dt\to\int_{-\infty}^{+\infty}f(t)1_{[x_0,x_0+1]}(t)dt=\int_{x_0}^{x_0+1}f(t)dt=F(x_0),$$ as $x\to x_0$ , proving $F$ is continuous. $G=|F|$ is the composition of $h(x)=|x|$ and $F$ , and since $h,F$ are both continuous we have $G=h\circ F$ is also continuous.",['real-analysis']
2714870,An elementary proof for a series involving logarithm,"I want to prove that $$ \sum_{x=1}^{\infty} \frac{\ln x}{x^2} \leq 1$$ in an elementary way, but haven't found such a proof yet. Would you please share with us if you find any elementary proof of the inequality? Also, is there any elementary method to find upper bounds for series involving logarithms?","['sequences-and-series', 'calculus']"
2714871,"Evaluate integral $\int_{0}^{\frac\pi2}\frac{x}{\sin(x)+6x} \, dx$","Show that
$$\int_0^{\pi/2}\frac x {\sin(x)+6x} \, dx =\frac{\pi}{2\sqrt2}\ln \left( \cot \frac \pi 8 \right)$$ I tried to apply a related theory for the integral using substitution for $x,$ since there is 0 as the lower interval,  $x=\frac\pi2-x $ $$\int_0^{\pi/2}\frac{\frac\pi2-x}{\sin(\frac\pi2-x)+6(\frac\pi2-x)} \, dx$$ But what to do next? Is there a specific substitution when there is a denominator with trigonometric and algebraic function together?","['integration', 'trigonometric-integrals']"
2714877,Variation on Harmonic Series,"I'm trying to establish the convergence or divergence of the following variant of the harmonic series:
$$\frac{1}{1}+\frac{1}{2}-\frac{1}{3}-\frac{1}{4}-\frac{1}{5}+\frac{1}{6}+\frac{1}{7}-\frac{1}{8}-\frac{1}{9}-\frac{1}{10}+\frac{1}{11}\cdots$$ Where the sign pattern has period 5, ie, it looks like this: ++---++---++---.... My thought has been to find a regrouping that diverges, since I only need to find one in order to show the series is divergent. I tried to bound this series below by increasing the denominator on the positive terms, and decreasing it on the negative terms to yield a series like $$\left(\frac{1}{2}+\frac{1}{2}\right)-\left(\frac{1}{3}+\frac{1}{3}+\frac{1}{3}\right)+\left(\frac{1}{7}+\frac{1}{7}\right)+ \cdots$$ but I'm pretty sure this will converge. I don't really know what approach to take next. A hint would be greatly appreciated! Thanks!","['convergence-divergence', 'sequences-and-series', 'analysis']"
2714910,"Find all pairs of $(x, y ,z)$ such that $x + y =\sqrt{z^{2} + 2018}, .... $","Find all pairs of $(x,y,z)$, of real numbers, such that $$ x + y = \sqrt{z^{2} + 2018} $$
$$ x + z = \sqrt{y^{2} + 2018} $$
$$ y + z = \sqrt{x^{2} + 2018} $$ An attempt : 
Squaring we get
$$ (x + y)^{2} = z^{2} + 2018 \implies  (x + y)^{2} - z^{2} = 2018 $$
$$ (x + z)^{2} = y^{2} + 2018 \implies  (x + z)^{2} - y^{2} = 2018 $$
$$ (y + z)^{2} = x^{2} + 2018 \implies  (z + y)^{2} - x^{2} = 2018 $$
which also means
$$ (x + y - z)(x + y + z) = 2018 $$
$$ (x + z - y)(x + y + z) = 2018 $$
$$ (z + y - x)(x + y + z) = 2018 $$
so
$$ \frac{2018}{x+y-z} =  \frac{2018}{x+z-y} =  \frac{2018}{y+z-x} $$ $$(x+y-z) = (x+z-y) = (y+z-x)$$
$$y-z = z-y \implies z = y $$
$$x-y= y-x \implies x=y $$
so my answer is
$$(x, y, z), \:\:\: x=y=z $$
but with the 3 initial equations, we must also have
$$ (x+y) = \sqrt{z^{2} + 2018} \implies 4x^{2} =  x^{2} + 2018 $$
or
$$ x^{2} = 2018/3 $$
so the solution is
$$(x, y, z), \:\:\: x=y=z = \sqrt{2018/3}  $$ Is this sufficient already? are there better techniques?","['algebra-precalculus', 'contest-math', 'proof-verification']"
2714922,"Differentiate $f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}}$, $-2<x<2$","Differentiate $f(x)=\frac{\cos^{-1}\frac{x}{2}}{\sqrt{2x+7}}$, $-2<x<2$ My Attempt $$
\begin{align}
f'(x)&=\frac{\sqrt{2x+7}\frac{d}{dx}\cos^{-1}\big(\frac{x}{2}\big)-\cos^{-1}\big(\frac{x}{2}\big)\frac{d}{dx}\sqrt{2x+7}}{(\sqrt{2x+7})^2}\\
&=\frac{\sqrt{2x+7}\frac{\frac{-1}{2}}{\sqrt{1-\frac{x^2}{4}}}-\cos^{-1}\big(\frac{x}{2}\big)\frac{2}{2\sqrt{2x+7}}}{(\sqrt{2x+7})^2}\\
&=\frac{\frac{-\sqrt{2x+7}.\sqrt{4}}{2\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{\sqrt{2x+7}}}{2x+7}\\
&=\frac{\pm2}{2.\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\
&\color{blue}{=\frac{\pm1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\}
\end{align}
$$ Result by Mathematica D[(ArcCos[x/2]/Sqrt[2 x + 7]),x] $$
f'(x)={\frac{-1}{\sqrt{2-x}\sqrt{2+x}\sqrt{7+2x}}-\frac{\arccos\big(\frac{x}{2}\big)}{(7+2x)^{3/2}}}\\
\color{blue}{=\frac{-1}{\sqrt{2x+7}\sqrt{4-x^2}}-\frac{\cos^{-1}\big(\frac{x}{2}\big)}{(2x+7)^{3/2}}\\}
$$ Mathematica seems to use only the principal root, ie. $\sqrt{4}=+2$ but in that case can I say the given solution is incomplete ? Or is there any other domain or range considerations which eliminate the other case ?","['derivatives', 'calculus']"
2714990,What does equivalence of categories really tell us?,"Apology for the vague question, but I hope I can make it clearer by considering some examples. Also, I added algebraic geometry to the tags because I'll borrow my examples from there, but feel free to change the tags. From a formal point of view, an equivalence between two categories tells us that the two categories are 'essentially the same'. Nevertheless it may still happen that two equivalent categories come from different fields of mathematics, both with their own sets of definitions or theorems, which do not seem to translate to each other quite easily. Here are three examples which exhibit the just-mentioned phenomenon in different degrees. The category of prevarieties over an algebraically closed field $k$ is equivalent to the category of integral schemes of finite type over $k$. (The definitions are as in Mumford's Red Book.) In both categories, objects are represented by topological spaces with a sheaf. These spaces are constructed very similarly (the former from maximal ideals, and the latter from prime ideals) and look very much alike (the latter being like the former but with some hidden points, some reducedness). Many definitions, constructions, and proofs in the former category just carry over to the latter. The category of smooth projective curves over $\mathbb{C}$ is equivalent to the category of compact Riemann surfaces. There are strong similarities between the two categories. Both have meromorphic functions and divisors, both have differential forms, both have a Riemann--Roch theorem, and so on. It seems to me ( but I may well be wrong here ) that these similarities are more of a heuristic observation than something formal. For instance you can't apply the analytification functor to transform Riemann--Roch for curves into Riemann--Roch for Riemann surfaces, let alone transform a proof of one into a proof of the other. For many suitable ringed spaces $(X,\mathcal{O}_X)$, there's an equivalence between the category of finite-rank locally free sheaves on $X$, and the category of finitely generated projective $\mathcal{O}_X(X)$-modules, including for affine schemes and smooth manifolds. The objects representing both categories are very different in nature, and it seems to me that concepts from one category don't have a nice translation to the other category. (Stupid examples: How do I say that a smooth vector bundle $E$ on $M$ has vanishing curvature in terms of $C^\infty(E)$? How does saying a projective $A$-module is the natural module over a matrix ring translate to coherent sheaves?) There is an equivalence between the category of commutative $C^*$-algebras and the opposite category of compact Hausdorff topological spaces. Yet the objects representing the two categories are entirely different. There are concepts in one category that do not translate well to the other. (Stupid examples: To which $C^*$-algebras do manifolds correspond? What is the Spectral Theorem in the context of compact Hausdorff topological spaces?) It is often said that the distinction between classical and modern algebraic geometry is artificial, and the equivalence between classical varieties and finite-type integral schemes should back up this statement. But no-one would say that the distinction between functional analysis and topology is artificial because of Gelfand--Naimark. So how much should I still think of the equivalence between varieties and integral schemes? Exactly how happy should I be when I prove that two seemingly different categories are equivalent? In short: What does an equivalence of categories really tell us?","['category-theory', 'algebraic-geometry']"
2715004,Let p be a prime number,"can we find all $n$ such that $p$ divides $n! - 1$?
I ran into this problem while trying to solve a functional equation. I need to prove that $n=1$ and $n=p-2$ are the only solutions (for sufficiently large p)","['number-theory', 'factorial', 'congruences']"
2715013,Prove that there exists only one number $x \in \mathbb{R}$ such that $\sin(x)=x-1$.,"Prove that there exists only one number $x \in \mathbb{R}$ such that $\sin(x)=x-1$. I have been looking after this problem now on Holidays, but I can't prove the uniqueness. Let $f(x)=\sin(x)$ and $g(x)=x-1$. Both functions are continuous everywhere, particularly in the interval $[1,2]$. We also may say that both functions are bounded in that interval by $0\leq f(x) \leq 1$ and $0\leq g(x) \leq 1$ (computing), so that we can say that there exists a number $x$ such that $\sin(x)=x-1$. But now I can't prove that $x$ is unique.","['trigonometry', 'calculus']"
2715039,Why is $y_{n+1}=\frac{1}{2}(y_n+\sqrt{\frac{1}{2^{2n}}+y_n^2})$ giving the inverse of $\pi$?,"A simple and interesting recursion: $$y_{n+1}=\frac{1}{2}(y_n+\sqrt{\frac{1}{2^{2n}}+y_n^2})$$ has these curious solutions $$y_1=-\infty,y_{\infty}=\frac{1}{2\pi}$$
$$y_1=-\frac{1}{2},y_{\infty}=\frac{2}{3\pi}$$
$$y_1=0,y_{\infty}=\frac{1}{\pi} $$
$$y_1=\frac{1}{2},y_{\infty}=\frac{2}{\pi}$$ Cannot find it in the literature as such and it does not look like coming from AGM, but I suspect elliptic integrals. Still cannot start from anywhere for some time. Any ideas?","['recursion', 'pi', 'limits']"
2715098,Does the zero product property hold in vector spaces? [duplicate],This question already has answers here : Does $\lambda$ $\mathbf u$ $= 0$ imply $\lambda = 0$ or $\mathbf u$ $= 0$? (4 answers) Closed 6 years ago . Suppose $V$ is a vector space over a field $F$.  Let $v \in V\setminus \{0\}$ and $\lambda \in F$.  Does $\lambda v= 0$ imply $\lambda = 0$?,['linear-algebra']
2715121,"Gradient of largest eigenvalue of matrix, with respect to individual elements of the matrix","If $M$ is a $n\times n$ matrix, let $f(M)$ denote the largest eigenvalue (in absolute value) of $M$.  In other words, if $\lambda_1,\dots,\lambda_n$ are the eigenvalues of $M$, define $$f(M) = \max(|\lambda_1|,\dots,|\lambda_n|).$$ This can be viewed as a function $f:\mathbb{R}^{n^2} \to \mathbb{R}$ on a $n^2$-dimensional input. Now given a matrix $M$, I'd like to compute the gradient $\nabla f(M)$.  How do I do that? Equivalently, for each $i,j$, I want to compute the derivative ${\partial  \over \partial M_{i,j}} f(M)$ of $f(M)$ with respect to the $i,j$-th entry of the matrix.  I can't figure out a clean way to compute this, as computing the eigenvalues involves Gaussian elimination, and it's not clear how to differentiate through that process. (This is based on application where I want to do gradient descent on a function with a term of the form $f(M)$, so I need to be able to compute the gradient to do that.)","['eigenvalues-eigenvectors', 'matrix-calculus', 'multivariable-calculus', 'spectral-radius', 'vector-analysis']"
2715168,Domain of complex function / singularity,"The definition of singularity in Wolfram MathWorld says ""Complex singularities are points $z_0$ in the domain of a function $f$ where $f$ fails to be analytic."" http://mathworld.wolfram.com/Singularity.html But in the function $f(z)=\frac{1}{z}$, zero is a pole, thus a singularity. So according to the above definition, $z=0$ is in the domain of $f$?","['complex-analysis', 'complex-numbers']"
2715253,Differential equations -- exact solution to nonlinear second order ODE,"I get a problem with the following nonlinear second order ODE: $y^2\dfrac{d^2u(y)}{dy^2}+(\Sigma-\rho y^\lambda-\Omega y)y\dfrac{du(y)}{dy}-(\alpha\rho y^\lambda+\beta \Omega y) u(y)=0$, where $\Sigma, \rho,\Omega,\alpha,\beta,\lambda$ are constants, and $\lambda$ is positive real number. I want to ask for the exact solution to it. For example, for the special case that $\rho=0$, one of the exact solutions is $M(\beta,\Sigma,\Omega y)$ in the form of Kummer function.","['hypergeometric-function', 'ordinary-differential-equations']"
2715283,Return of 2D Brownian motion to the origin,"In what follows $B$ denotes two-dimensional Brownian motion and $\mathbb{B}(0,z)$ denotes the disk with radius $z$ centered at the origin. I also define the hitting time $\tau_C = \inf\{t \geq 0: B_t \in C\}$. It then follows that $$P^x\{\tau_{\mathbb{B}(0,r)} < \tau_{\mathbb{B}(0,R)^{\mathsf{c}}}\} = \frac{\log{R} - \log{\lvert x\rvert}}{\log{R} - \log{r}}$$ for $0 < r < \lvert x\rvert < R$. In other words, the expression above is the probability that the two-dimensional Brownian motion started at $x$ enters the ball with radius $r$ before it exits the ball with radius $R$. Using this expression I want to compute $P^x\{\tau_{\{0\}} < \infty\}$, namely the probability that the two-dimensional Brownian motion started at $x$ hits the origin in finite time. The double limit 
$$\lim_{R \to \infty}\lim_{r\to 0}\frac{\log{R} - \log{\lvert x\rvert}}{\log{R} - \log{r}} = 0$$
gives the right answer. But the double limit 
$$\lim_{r\to 0}\lim_{R \to \infty}\frac{\log{R} - \log{\lvert x\rvert}}{\log{R} - \log{r}} = 1$$
gives the wrong answer. I am trying to understand what is going wrong in this latter approach. First, 
$$\lim_{R \to \infty}P^x\{\tau_{\mathbb{B}(0,r)} < \tau_{\mathbb{B}(0,R)^{\mathsf{c}}}\} = P^x\{\tau_{\mathbb{B}(0,r)} < \infty\} = 1$$ But somehow it is not true that
$$1_{\tau_{\mathbb{B}(0,r)}(\omega) < \infty}(\omega) \to 1_{\tau_{\{0\}}(\omega) < \infty}(\omega) \label{a}\tag{1}$$
as $r \to 0$. If it were true, then $P^x\{\tau_{\{0\}} < \infty\}$ would be $1$, which is wrong. But I don't see why $\ref{a}$ does not hold. Can someone explain the error in my reasoning?","['stochastic-processes', 'probability-theory', 'brownian-motion', 'stopping-times']"
2715300,Equality of irrational numbers and the need for a proof,"This seems obvious to me, but do we need a formal proof to establish this is always true? Let $x$ and $y$ be irrational numbers with decimal representations $x = 0.x_1x_2x_3 \dots$ $y = 0.y_1y_2y_3 \dots$ If $x_n = y_n$ for $n \in \mathbb{N}$ Can we then infer that $x = y$ ?",['elementary-set-theory']
2715359,Why are these determinants $0$?,"These $3$ matrices below have determinants of $0$. Increasing each element by $1$ still results in a determinant of $0$: \begin{bmatrix}1&2&3\\4&5&6\\7&8&9\end{bmatrix}
\begin{bmatrix}2&3&4\\5&6&7\\8&9&10\end{bmatrix}
\begin{bmatrix}3&4&5\\6&7&8\\9&10&11\end{bmatrix} Also, if I square each element of the first matrix the determinant isn't $0$, it's $-216$, but if I continue this pattern to a $4 \times 4$ matrix the determinant then is $0$: \begin{bmatrix}1&4&9\\16&25&36\\49&64&81\end{bmatrix}
\begin{bmatrix}1&4&9&16\\25&36&49&64\\81&100&121&144\\169&196&225&256\end{bmatrix} This pattern seems to work for matrices with dimensions $2$ higher than what the elements are raised to: \begin{bmatrix}1&8&27&64&125\\216&343&512&729&1000\\1331&1728&2197&2744&3375\\4096&4913&5832&6859&8000\\9261&10648&12167&13824&15625\end{bmatrix}
\begin{bmatrix}1&16&81&256&625&1296\\2401&4096&6561&10000&14641&20736\\28561&38416&50625&65536&83521&104976\\130321&160000&194481&234256&279841&331776\\390625&456976&531441&614656&707281&810000\\923521&1048576&1185921&1336336&1500625&1679616\end{bmatrix} Can someone please explain why this is?","['matrices', 'determinant']"
2715382,The definition of finite group scheme.,"I am reading some materials about group schemes.I am very confused about some definitions.I know what is a group scheme and also know what is an affine group scheme over $k$, $k$ is an commutative ring with identity.But I can not find definition about the finite group scheme over $k$. I guess that a finite group scheme over $k$ is like $Spec A$, $A$ is $k$-algebra of finite type. Is it right? Could you give me a strict definition? And does finite group scheme have relation with group?","['arithmetic-geometry', 'algebraic-geometry']"
2715442,Linear Differential Equation for Performance Level,"I'm solving a question from my Calculus textbook that asks the following: Let $P(t)$ be the performance level of someone learning a skill as a function of the training time $t.$ The graph of $P$ is called a learning curve. In Exercise 9.1.15 we proposed the differential equation $\frac{dP}{dt} = k[M-P(t)]$ as a reasonable model for learning, where $k$ is a positive constant. Solve it as a linear differential equation and use your solution to graph the learning curve. My solution was to use the fact that a linear differential equation can be written in the following form: $\frac{dy}{dx} + yP(x) = Q(x)$ where $t$ is the independent variable, and $P$ is the dependent variable (so $P(t)$ is the like 'y' in this case.) I rewrote the given differential equation for the performance level as such: $\frac{dP}{dt} + kP(t) = kM$ Thus, my $k$ is like my ""function of x"", and my $P(t)$ is my ""y."" By setting $e$ to the power of the integral of $k$ , I got $e^{kt}$ as my integration factor. I multiplied both sides of my differential equation by $e^{kt}$ , and then rewrote the left side as $(e^{kt} · P(t))'$ . I integrated both sides, and factored out $k$ and $M$ since they're both constants. The answer I got for $P(t)$ is $M + ce^{-kt}.$ So for my actual question: Is this correct? I'm not very sure about my solution to this question. Also, is my logic for each of my steps sound and reasonable? Thank you so much for reading!","['applications', 'ordinary-differential-equations']"
2715466,Explicit Formula for Cabling of Braids,"Given the Artin braid groups on $n$ and $m$ strands $Br_n$ and $Br_m$, there are cabling operations $\circ_k:Br_n\times Br_m\to Br_{n+m-1}$ that take a braid $\beta\in Br_m$ and replace the $k$th strand of a braid in $Br_n$ with $\beta$. See the following picture for the operation $\circ_2\colon Br_4\times Br_2\to  Br_5$: Does anyone know of an explicit description of these operations in terms of generators? Writing them down in low degrees, I seem to be able to work out a rough idea of how they should operate, but it seems likely to me that someone has already figured this out?","['reference-request', 'abstract-algebra', 'braid-groups', 'group-theory']"
2715525,"How to evaluate $\int_0^1 \frac{1-x}{\ln x}(x+x^2+x^{2^2}+x^{2^3}+x^{2^4}+\ldots) \, dx$?","Evaluate the definite integral:
$$\int_0^1 \frac{1-x}{\ln x}(x+x^2+x^{2^2}+x^{2^3}+x^{2^4}+\ldots) \, dx$$ I think the series involving $x$ converges because $x\in[0,1]$, but I cannot form an expression for the series. If I let
$$
u_n=x^{2^{n-1}} \\ \frac{\ln u_n}{\ln x}=2^{n-1}
$$
but then this series does not converge. Even WolframAlpha cannot evaluate a definite integral together with an infinite series, so I am stuck on this.","['definite-integrals', 'sequences-and-series', 'calculus']"
2715542,"If a triangle is not equilateral, must its orthocenter and circumcenter be distinct?","According to Proof Wiki , if a triangle is not equilateral, then its orthocenter and circumcenter must be distinct. The exact quote is Let △ABC be a triangle. Let O be the circumcenter of △ABC. Let G be the centroid of △ABC. Let H be the orthocenter of △ABC. Then O, G and H are the same points if and only if △ABC is equilateral. If $\triangle ABC$ is not equilateral, then $O, G$ and $H$ are all distinct. Well, it looks like I've found a counterexample: $$A=(0,0),\quad B=\left(1-\frac{\sqrt 3}{2},\frac12\right),\quad C=\left(1-\frac{\sqrt 3}{2},-\frac12\right)$$ and the orthocenter and circumcenter both coincide at $(1,0)$ , right? So is my counterexample valid, or did I screw something up?","['examples-counterexamples', 'triangles', 'proof-verification', 'geometry']"
2715574,$X$ standard normal. $Y=X^2$. Find the pdf of $Y$ and covariance between $X$&$Y$,"Let $X$ be standard normal random variable, i.e., $X ∼ N(0, 1)$. Consider transformed random
variable: $Y = X^2$. (a) Find the probability density function of $Y$. (b) Find the covariance between $X$ and $Y$. I'm new to probability and don't really get yet transformation of random variables. If anyone could help me with this one I'd much appreciate. My work: $F_Y(y)=P(Y\leq y)=P(X^2\leq y)=P(X\leq\sqrt{y})=F_X(\sqrt{y})=\int_{-\infty}^\sqrt{y}\frac{1}{\sqrt{2\pi}}e^{-t^2/2}\,dt$ that's where I'm stuck. Edit: Is the correlation 0 because of the odd-moments popping in the equation?","['statistics', 'probability']"
2715622,"Differentiate $y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2}$, $-1\leq x\leq1$","Find $\frac{dy}{dx}$ if $y=\sin^{-1}x+\sin^{-1}\sqrt{1-x^2}$, $-1\leq x\leq1$ The solution is given as $y'=0$ in my reference. But that doesn't seem to be a complete solution as the graph of the function is: My Attempt Let $x=\sin\alpha\implies \alpha=\sin^{-1}x$
$$
y=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{1-\sin^2\alpha}=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\cos^2\alpha}\\
=\sin^{-1}(\sin\alpha)+\sin^{-1}\sqrt{\sin^2(\tfrac{\pi}{2}-\alpha)}=\sin^{-1}(\sin\alpha)+\sin^{-1}|\sin(\tfrac{\pi}{2}-\alpha)|\\
=n\pi+(-1)^n(\alpha)+
$$ How do I proceed further and find the derivative ?","['derivatives', 'inverse-function', 'calculus']"
2715634,Basic example of $\sigma$-algebra generated by a random variable,"I need some practice finding the $\sigma$-algebra generted by a radnom variable, but I can't find any worked examples online. I was wondering if anyone could work through the solution to the following example I've made up: Let $X : ([0,1],\mathbb{P}) \to (\mathbb{R}, \mathscr{B})$, where $\mathbb{P} = \lambda$ is the lebesgue measure  ($\lambda[a,b] = b-a$) and $\mathscr{B}$ is the Borel sigma algebra. 
Define: 
$$X(\omega) = 
\begin{cases}
0 &x <1/3\\
1 & x \in [1/3, 2/3]\\
2x & x \in (2/3, 1]\\
\end{cases}
$$ Intuitively, I think it might be something like 
$$\sigma(X) = \{X^{-1}(B): B \in \mathscr{B}\} = \sigma(\{[0,1/3),[1/3,2/3],\mathscr{B}((2/3,1]),[0,1]\})$$
Am I right, and how do I make this rigorous? If anyone could point me to more worked examples that would also be good. 
Thanks!","['probability-theory', 'measure-theory', 'random-variables']"
2715665,"If $f(x)^2=x+(x+1)f(x+2)$, what is $f(1)$?","Suppose $f$: $\mathbb{R}_{\geq 0} \rightarrow \mathbb{R}$ and $f(x)^2 = x + (x+1)f(x+2)$, what is $f(1)$? Or more in general, what is $f(x)$? The motivation behind this problem is that I want to find what the number of this nested radical $\sqrt{1+2\sqrt{3+4\sqrt{5+6\sqrt{7+8...}}}}$. This can be written more generally as $f(x)=\sqrt{x+(x+1)f(x+2)}$ where $x=1$. This is where the problem arises from. If anybody can find an expression for the nested radical or find $f(x)$ I would be very happy!","['nested-radicals', 'functions', 'functional-equations']"
2715666,Posterior predictive distribution in Gelman's book,"I don't see how Gelman gets the equality $\int p(\tilde{y},\theta\mid y)d\theta=\int p(\tilde{y}|\theta,y)p(\theta\mid y)d\theta$. Can someone clarify this for me.",['probability-theory']
2715672,quotient topology doesn't preserve separation axioms,"According to Wikipedia Quotient topology is ill-behaved with respect to Separation Axioms , locally compactness and simply connectedness . I have examples to support this argument for locally compactness [$\mathbb{R}/\mathbb{N}$ will do the job] and for simply connectedness [identifying $0$ and $1$ in $[0,1]$ gives a circle which is not simply connected] but cannot think of any suitable example for separation axioms. Need help!","['general-topology', 'examples-counterexamples', 'separation-axioms', 'quotient-spaces']"
2715694,"Prob. 3, Sec. 3.4, in Bartle & Sherbert's INTRO TO REAL ANALYSIS, 4th ed: Does this sequence converge?","Let the sequence $\left( f_n \right)_{n \in \mathbb{N} }$ be given by 
$$ f_1 \colon= 1, \qquad f_2 \colon= 1, \qquad \mbox{ and } \qquad f_n \colon= f_{n-1} + f_{n-2} \ \mbox{ for all } n \in \mathbb{N} \mbox{ such that } n > 2. $$
That is, $\left( f_n \right)_{n \in \mathbb{N} }$ is the famous Fibonacci sequence. Now let the sequence $\left( x_n \right)_{n \in \mathbb{N} }$ be given by 
$$ x_n \colon= \frac{f_{n+1} }{f_n} \ \mbox{ for all } n \in  \mathbb{N}. $$
Thus we have 
$$ x_1 = \frac{f_2}{f_1} = 1, $$
and 
$$ x_n = \frac{ f_n + f_{n-1} }{f_n} = 1 + \frac{f_{n-1}}{f_n} \ \mbox{ for all } n \in  \mathbb{N} \mbox{ such that } n \geq 2. $$ Does the sequence $\left( x_n \right)_{n \in \mathbb{N} }$ converge in $\mathbb{R}$? If so, then what is $\lim_{n \to \infty} x_n$? My Attempt: First, we note that $$ f_1 = f_2 = 1, \ f_3 = f_2+ f_1 = 2, \ f_4 = f_3 + f_2 = 3, \ f_5 = f_4 + f_3 = 5, \ f_6 = f_5 + f_4 = 8, \\ f_7 = f_6 + f_5 = 13, \ f_8 = f_7 + f_6 = 21, \ f_9 = f_8 + f_7 = 34, \ f_{10} = f_9 + f_8 = 55. \\ f_{11} = f_{10} + f_9 = 89, \ f_{12} = f_{11} + f_{10} = 144, \ f_{13} = f_{12} + f_{11} = 233, \\ f_{14} = f_{13} + f_{12} = 377. $$
  So 
  $$ x_1 = \frac{f_2}{f_1} = 1, \ x_2 = \frac{f_3}{f_2} = 2, \ x_3 = \frac{f_4}{f_3} = \frac{3}{2}, \ x_4 = \frac{f_5}{f_4} = \frac{5}{3}, \ x_5 = \frac{f_6}{f_5} = \frac{8}{5}, \\ x_6 = \frac{f_7}{f_6} = \frac{13}{8}, \ x_7 =  \frac{f_8}{f_7} = \frac{21}{13}, \ x_8 = \frac{f_9}{f_8} = \frac{34}{21}, \ x_9 = \frac{f_{10}}{f_9} = \frac{55}{34}, \ x_{10} = \frac{f_{11}}{f_{10}} = \frac{89}{55}, \\ x_{11} = \frac{f_{12}}{f_{11}} = \frac{144}{89}, \ x_{12} = \frac{f_{13}}{f_{12}} = \frac{233}{144}, \ x_{13} = \frac{f_{14}}{f_{13}} = \frac{377}{233}.     $$ From these calculations we notice that 
  $$ x_1 < x_3 < x_5 < x_7 < x_9 < x_{11} < x_{13} < 2, $$ 
  and 
  $$ x_2 > x_4 > x_6 > x_8 > x_{10} > x_{12} > 0.  $$ How to proceed from here? How to show that 
$$ x_{2n-1} < x_{2n+1} \ \mbox{ and } \ x_{2n} > x_{2n+2} \ \mbox{ for all } n \in \mathbb{N}?$$ And, how to show that the subsequence $\left( x_{2n-1} \right)_{n \in \mathbb{N} }$ of $\left(x_n \right)_{n \in \mathbb{N}}$ is bounded above? Can we treat it as obvious that the subsequence $\left( x_{2n} \right)_{n \in \mathbb{N} }$ of $\left( x_n \right)_{n \in \mathbb{N} }$ is bounded from below by $0$? Once we have shown that the subsequence $\left( x_{2n-1} \right)_{n \in \mathbb{N} }$ is increasing and bounded from above, then let us put 
$$ x^\prime \colon= \lim_{n \to \infty} x_{2n-1}. $$ And, once we have shown that the subsequence $\left( x_{2n} \right)_{n \in \mathbb{N} }$ is decreasing and bounded from below, then let us put 
$$ x^* \colon= \lim_{n \to \infty} x_{2n}. $$ How to proceed from here? Can we show that $x^{\prime} = x^*$? And, once we have shown this, then how to find $\lim_{n \to \infty} x_n$?","['real-analysis', 'limits', 'sequences-and-series', 'convergence-divergence', 'analysis']"
2715697,"$U$ be the set of all $2×2$ matrices with real entries such that all their eigenvalues belong to $C - R $, and $X = M_2(R)$. Is $U$ open?","Let $U$ be the set of all $2×2$ matrices with real entries such that all their eigenvalues belong to $C - R  $, and $X = M_2(R)$. How can I prove $U$ is open in $X$? Can anyone please help me by giving any hint? I know this is not a closed set.","['matrices', 'eigenvalues-eigenvectors', 'general-topology', 'metric-spaces']"
2715707,"Proving if $\sum\limits_{n=1}^∞\frac1{a_n}<∞$, then $\sum\limits_{n=1}^∞(\frac{na_n}{S_n})^α\frac1{a_n}<∞$","Well, I was confronted with this wonderful problem: Given that $\{a_n\}$ is a positive sequence such that $\sum\limits_{n=1}^\infty \dfrac1{a_n}<\infty$. Let $S_n=\sum\limits_{k=1}^na_k$, then prove for any $0\leqslant\alpha\leqslant2$,
  $$
\sum_{n=1}^{\infty}{\left( \frac{na_n}{S_n} \right) ^{\alpha}\frac{1}{a_n}}<\infty.
$$ Moreover, my friend guesses that the following inequality may be correct: $$
\sum_{n=1}^{\infty}{\left( \frac{na_n}{S_n} \right) ^{\alpha}\frac{1}{a_n}}\leqslant2^{\alpha}\sum_{n=1}^{\infty}{\frac{1}{a_n}}.
$$ The constant $2^\alpha$ is the best choice. For $\alpha=0$ and $\alpha=1$, see A version of Hardy's inequality involving reciprocals. , which is not hard to prove. But for $\alpha\in\mathbb R_+$, I do not know how to deal with it. Any advice will be appreciated. Thank you! P.S. The equivalent inequality of integral is this: Given $f(x)\geqslant0$. Let $F(x)=\int_0^xf(t)\,\mathrm dt$, prove that
  $$
\int_0^{\infty}{\left( \frac{xf\left( x \right)}{F\left( x \right)} \right) ^{\alpha}\frac{\text{d}x}{f\left( x \right)}}\leqslant 2^{\alpha}\int_0^{\infty}{\frac{\text{d}x}{f\left( x \right)}}. \quad 0\leqslant α\leqslant 2
$$","['inequality', 'sequences-and-series']"
2715729,Probability such that the average of those 22 numbers is the smallest possible average?,"A person divides a square to 100 small squares with equal sizes. Each small square is randomly selected and given a number from 1-100, such that each square is 'unique'. Then the person counts the sum of the numbers at each row, column, and each diagonal. In total, 22 numbers (22 sum results). What is the probability such that the average of those 22 numbers is the smallest possible average ..? Attempt: From my understanding, the diagonals are the two diagonals, and they certainly do not overlap each other at the center. Thisis because the size is 100 (even number). The placing of numbers on the rows and columns doesnt affect the total values. Any permutation will have a total $5050$. So the average will be affected only by the permutations in the two diagonals. The smallest average is when the numbers $1-20$ are in the diagonals. Hence the number of outcomes (the permutation) is 
$$ (20 \times 19 \times ..... \times 2 \times 1) = 20! $$ And it should be multiplied by
$$ 80! $$
Which is the number of placing for the non-diagonals. While the total possible outcome should be $$ 100! $$ So the probability should be : $$ \frac{80! 20!}{100!} $$ Is this accurate? Are there better methods? Thanks.","['permutations', 'contest-math', 'probability']"
2715752,How was the area formula for a circle ($A = \pi r^2$) derived before the introduction of calculus?,"How did mathematicians prior to the coming of calculus derive the area of the circle from scratch, without the use of calculus? The area, $A$, of a circle is $\pi r^2$. Given radius $r$, diameter $d$ and circumference $c$, by definition, $\pi := \frac cd$.","['circles', 'area', 'geometry']"
2715810,Studying uniform continuity on a multivariable function.,"Prove that the function $f(x,y)=\frac{1}{x^2+y^2}$ is not uniformly continuous over the domain: $D$={$(x,y):x^2+(y-2)^2 <2^2$} Using the definition: $\forall \delta>0 , \exists \epsilon(\delta) > 0 / \forall (x_1,y_1) , (x_2,y_2) 
\in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ Let : $(x_1,y_1)=(0,\frac{\delta}{2})$ $(x_2,y_2)=(\frac{\delta}{4},0)$ Conditions on $\delta$: $x_1^2+(y_1-2)^2<2^2 \implies 0<\delta<8$ $x_2^2+(y_2-2)^2<2^2 \implies 0<\delta<8$ $|f(x_1,y_1)-f(x_2,y_2)|=\frac{12}{\delta^2}>\epsilon$ $\forall \delta>0 , \exists \epsilon(\delta) \in ]0,\frac{12}{\delta^2}[  / \forall (x_1,y_1) , (x_2,y_2) 
\in D  : 0< ||(x_1,y_1),(x_2,y_2)||<\delta \implies |f(x_1,y_1)-f(x_2,y_2)|>\epsilon$ However I'm not sure if this is correct nor I am sure if It's possible to put conditions on $\delta$ while proving uniform continuity. I would be grateful to whoever can point out my mistakes or whoever has a much cleaner solution to this question. Thanks in advance.","['multivariable-calculus', 'uniform-continuity', 'epsilon-delta', 'proof-verification']"
2715821,"Injectiveness and surjectiveness of $f$ and of $g$, respectively, of the composition $g\circ f$.","a) If $f: E \to F$ and $g: F \to E \ $ are functions such that $g \circ f$ is injective, then $f$ is injective b) If $f: E \to F$ and $g: F \to E \ $ are functions such that $g \circ f$ is surjective, then $g$ is surjective. Proof: a) Assume $g \circ f$ is injective but $f$ is not injective. Then there exist $x_1, x_2 \in E$ such that $x_1 \neq x_2$ but $f(x_1) = f(x_2)$ . Then $g(f(x_1)) = g(f(x_2))$ . Since $g\circ f$ is injective, $x_1 = x_2$ , a contradiction! Therefore $f$ is injective. b) Assume $g \circ f$ is surjective but $g$ is not surjective. Then there exists $e_1 \in E$ such that $e_1 \neq g(z)$ for all $z \in f$ . Since $g\circ f$ is surjective, there exists $x_1 \in E$ such that $g(f(x_1)) = e_1$ . But this is a contradiction to the hypothesis that $g$ is not surjective, since $f(x_1) = y_1 \in F$ ! Therefore $g$ is surjective. Is everything here correct or did I make a mistake somewhere? If anyone has a cleaner proof I'd appreciate it as well, since I have still not convinced myself of the validity of the one I wrote above.","['algebra-precalculus', 'functions', 'proof-verification']"
2715836,A determinant involving a polynomial is $0$,"Let $n \geq 2$ and $f:\mathbb{R} \to \mathbb{R}, \: f(x)=(x-x_1)(x-x_2)\dots(x-x_n)$ where $x_1,\dots, x_n$ are distinct real numbers. The matrix $A=(a_{ij})_{1 \leq i,j \leq n}$ is defined as follows:
  $$a_{ij}=\begin{cases}
\dfrac{f'(x_i)-f'(x_j)}{x_i-x_j}, & \text{if } i \neq j \\[6px]
f''(x_i), & \text{if } i = j
\end{cases}$$
  Prove that $\det A = 0$ This is related to this problem, so I know that $$\frac{1}{f(x)}=\sum_{k=1}^n \frac{1}{f'(x_k)(x-x_k)}, \quad \forall x \neq x_i$$ I tried to write the terms of $A$, but getting the second derivative of $f$ in terms of those $x_i$ is pretty complicated. So I tried to simplify it by writing $$f(x)=a_nx^n+a_{n-1}x^{n-1}+\dots+a_0$$
and so $f''(x)=n(n-1)x^{n-2}+(n-1)(n-2)x^{n-3}+\dots+2\cdot 1\cdot x^0$. However, I am not sure this will lead to something helpful.","['derivatives', 'polynomials', 'partial-fractions', 'matrices', 'determinant']"
2715849,Find the maximum likelihood estimator of b (Regression coefficient),"Consider the regression model:
$y_i = bx_i + e_i,\quad     1 ≤ i ≤ n$, Suppose that $x_i$’s take values −1 or +1 and $e_i$’s have density
$f(t) ={\frac{1}{2}}e^{−|t|}, t \in \mathbb{R}$. Find the maximum likelihood estimator of $b$. Therefore  $\; y_i-bx_i \sim \epsilon \quad \text{,which follows}\quad f(t) ={\frac{1}{2}}e^{−|t|}\\
\therefore f(y,b,x_i)= {\frac{1}{2}}e^{−|y_i-bx_i|}\\
\Rightarrow L(y,x_i,b) = {\frac{1}{2}}^n e^{−\sum|y_i-bx_i|}\\
\Rightarrow \frac{\partial\log L(y,x_i,b)}{\partial b} = -\frac{\partial{\sum |y_i-bx_i|}}{\partial b} $ Any ideas about how to proceed??","['maximum-likelihood', 'linear-regression', 'statistics', 'probability']"
2715873,Proving limits for fractions using epsilon-delta definition,"Using the $\epsilon - \delta $ definition of the limit, prove that: $$\lim_{x\to 0} \frac{(2x+1)(x-2)}{3x+1} = -2$$ I firstly notice that my delta can never be greater than $\frac{1}{3}$ because there is a discontinuity at $x=-\frac{1}{3}$. I applied the standard steps as follows: $\vert \frac{(2x+1)(x-2)}{3x+1}  +2 \vert = \vert\frac{2x+3}{3x+1}\vert \vert x\vert$ Right now I need to restrict $x$ to some number, but I am not sure which value should I choose in order to easily bound my fraction, any help on choosing the correct delta is appreciated!","['epsilon-delta', 'calculus', 'limits']"
2715881,Minimize modulus of three-variable function,"Let $$
\begin{array}{l}
f: [0,\frac{\pi}{2}]^3 \to {\mathbb R}^+, \\
(\theta_1,\theta_2,\theta_3) \mapsto
|2+e^{i\theta_1}+e^{i\theta_2}+e^{i\theta_3}|
\end{array}
$$ Numerical values suggest that the minimum of $f$ is $\sqrt{13}$, and is attained at the following four points : the three permutations of $(0,\frac{\pi}{2},\frac{\pi}{2})$, and $(\frac{\pi}{2},\frac{\pi}{2},\frac{\pi}{2})$. But I was unable so far to show it rigorously, any help appreciated.","['multivariable-calculus', 'complex-numbers']"
2716043,Is every linear transformation with at least two eigenvalues decomposable?,"(A linear transformation $T : V \rightarrow V$ is decomposable if $V$ can be written as the direct sum of two proper $T$-invariant subspaces) Some background for this question: I have been going back over linear algebra recently, with the intention of pushing standard theorems and proofs to their bare essentials, so I can get an idea of when a property is ""necessary"" and when it is only ""sufficient"". For example the theorem which states ""Every finite-dimensional inner product space $V$ induces a natural isomorphism $V \rightarrow V^*$, given by $v \mapsto <v,->$"" has nothing to do with symmetry or the full power of positive definiteness. So this can be generalised from an inner product to a non-degenerate bilinear form. However this theorem does depend on the finite dimensionality of $V$, and so any proof must make use of this property. Now I have been trying to prove the following theorem (or find a counter example to it). ""Given that $V$ is a vector space, and $T : V \rightarrow V$ is a linear transformation with at least two distinct eigenvalues. Then $T$ is decomposable"". This theorem can be proven for the finite dimensional case by considering the Frobenius normal form of some matrix of $T$, but this proof explicitly uses the property of finite dimensionality. I have tried hard to produce either a more general proof that doesn't use finiteness, or a counter example in the infinite case, but I feel like I have hit a brick wall. Any help would be greatly appreciated, so thank you in advance.","['eigenvalues-eigenvectors', 'matrices', 'matrix-decomposition', 'linear-transformations', 'linear-algebra']"
2716052,Showing that the derivative operator is not bounded on $L^2 (\mathbb{R})$,"If we define $D$ as the set of functions $f \in L^2 (\mathbb{R})$ such that $f' \in L^2 (\mathbb{R})$ too, then it can be shown (I think) that the linear operator $\frac{d}{dt}: D \rightarrow L^2 (\mathbb{R})$ is not bounded with respect to the norm on $L^2 (\mathbb{R})$, which is $||f|| = \sqrt{\int_{-\infty}^{\infty}|f(t)|^2 dt}$. But I'm having trouble proving this. What I'd like to do is find a sequence $(f_n)$ in $D$ with $||f_n||=1$ $\forall n \in \mathbb{N}$ but with $||f_n '||$ increasing and becoming arbitrarily large with $n$. I understand that if we were working with the space of continuous functions on $\mathbb{R}$ instead of $L^2 (\mathbb{R})$, with the nice supremum norm, we could just use $f_n (t) = \sin(nt)$, but that won't work here. I tried the sequence $f_n(t) = \sqrt{\frac{\sin(nt)}{t}}$ because that's got a handy $L^2$-norm of $\sqrt{\pi}$ for all $n\in\mathbb{N}$, but its derivative isn't in $L^2 (\mathbb{R})$ so I feel stuck. I think I'm approaching the problem correctly, but the norm in question here isn't easy to work with -- is there a simple sequence I could use?","['functional-analysis', 'lp-spaces']"
2716062,"Let $X$ be a random variable, $\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}$ for $s \approx t$. As","Question Let $X$ be a random variable for which we only have the value of its Moment Generating Function $M_X$ on a discrete set of points, I am looking for a stable method to compute:
$$\frac{M_X(s) - M_X(t)}{s-t}=\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}, \qquad s \approx t \approx 0.$$ Thoughts/ Attempts The problem is, clearly, that both the numerator as the denumerator becomes small as $|s-t| \rightarrow 0$. We can however use the Taylor expansion of $e^{sX}$ to overcome this problem, indeed we have (with $
\gamma_n(x,y)
=
\sum_{m=0}^{n-1} x^m y^{n-1-m}
$):
\begin{align*}
\frac{\mathbb{E}[e^{sX}-e^{tX}]}{s-t}
&= - \sum_{n=1}^\infty \frac{\gamma_{n-1}(s, t)}{n!} \left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}.
\end{align*}
This resolves our original problem. However it introduces another problem, namely the computation of: $\left(  \frac{d^n M_X}{ds^n} \right)\bigg|_{s=0}$, which again causes numerical instability. I have tried to stabilize this differentiation. I see no method to do this (except by applying Cauchy's integral formula , which can not be applied here as we only know the value of $M_X(s)$ for real values $s$). Maybe we can rewrite this formula again in function of $M_X(s)$ but I am not sure how. I have also implemented the suggestions found here but high order derivatives unfortunately can not be computed this way. Background/Reason For Question I am using this quantity in a recursion and numerical errors introduced by it make the recurrence fail. This happens after $10-20$ steps, some precision is lost in each subsequent step and this inaccuracy explodes..","['derivatives', 'real-analysis', 'recursive-algorithms', 'probability', 'numerical-methods']"
2716133,Number of ways in which $4$ people can be selected out of $10$ people sitting in a row such that exactly two are consecutive,"Number of ways in which $4$ people can be selected out of $10$ people sitting in a row such that exactly two are consecutive is? My attempt: I tried two approaches: PIE: required ways = total ways - ways in which three are consecutive + 
ways in which four are consecutive. But I do not know how to calculate ""ways in which three are consecutive"", so I am stuck. Direct: the problem is that: If I select two people at the ends: - - 3 4 5 6 7 8 9 10 or 1 2 3 4 5 6 7 8 - - , then I have six more ways to select the third person, but am unsure about the fourth way. If I select two people in the middle, like so 1 2 3 - - 6 7 8 9 10 , then selecting the next two people is troublesome, as the seats 1 and 2 cannot be occupied. Both these approaches seem cumbersome. I hope there's a simpler way. What is that?","['combinations', 'combinatorics']"
2716182,Preimage of Cartier divisor under finite morphism,"Let $f: C' \to C$ a finite morphism between two integral curves, so $1$-dimensional, proper $k$-schemes for a fixed field $k$. Since $f$ is finite and therefore by definition affine, we conclude that $C' = Spec(\mathcal{A}')$ is the relative spectrum for a coherent $\mathcal{O}_C$-algebra $\mathcal{A}' = f_*(\mathcal{O}_{C'})$. It's obvious that $\mathcal{A}'$ is locally free on regular locus $Reg(C)$. I suppose that follows from the structure theorem about PID's over Dedekind rings, therefore for small enough open enviroments $U \subset C$ we get $\mathcal{A} \vert _U \cong \mathcal{O}_U ^{\oplus r}$. Futhermore let $D \subset C$ be an effective Cartier Divisor of $C$ with corresponding ideal sheaf (=invertible sheaf) $\mathcal{L} := \mathcal{O}_{C}(D)$. Via $f^{-1}(D) := D'$ we get a Cartier divisor on $C'$. My question is: Why holds $f_*(\mathcal{O}_{D'}) = \mathcal{O}_D ^{\oplus r}$ ?","['algebraic-curves', 'divisors-algebraic-geometry', 'algebraic-geometry']"
2716192,"How to convert a detailed mathematical, statistical compuational process into mathematical and statistical equation?","I am working on a problem that allows haplotype phasing. I have developed this computation method (expressed in detail below) and have developed a python code (not shown here) to solve the issue. Now, I am having a hard time trying to translate this computation in mathematical and/or statistical language. I have revisited maths and I am able to comprehend the process at several places, but I cannot translate those ideas onto my computation (or algorithm). Here is my computation (step by step) with a very workable example. I have tried to make it as comprehensive as possible, let me know if there is any confustion: Fig 01: Example of the haplotype file contig   pos     ref   all-alleles   ms01e_PI   ms01e_PG_al   ms02g_PI   ms02g_PG_al   ms03g_PI   ms03g_PG_al   ms04h_PI   ms04h_PG_al   ms05h_PI   ms05h_PG_al   ms06h_PI   ms06h_PG_al
2      15881764   .      .           4           C|T           6           C|T           7           T|T           7           T|T           7           C|T           7           C|T
2      15881767   .      .           4           C|C           6           T|C           7           C|C           7           C|C           7           T|C           7           C|C
2      15881989   .      .           4           C|C           6           A|C           7           C|C           7           C|C           7           A|T           7           A|C
2      15882091   .      .           4           G|T           6           G|T           7           T|A           7           A|A           7           A|T           7           A|C
2      15882451   .      .           4           C|T           4           T|C           7           T|T           7           T|T           7           C|T           7           C|A
2      15882454   .      .           4           C|T           4           T|C           7           T|T           7           T|T           7           C|T           7           C|T
2      15882493   .      .           4           C|T           4           T|C           7           T|T           7           T|T           7           C|T           7           C|T
2      15882505   .      .           4           A|T           4           T|A           7           T|T           7           T|T           7           A|C           7           A|T This (fig 01) is my main data file. The main idea is to take a pool of several samples which have short phased haplotype blocks represented as PI (phased index) and PG_al  (phased genotype) . For any sample, the site where two consecutive haplotype blocks are not joined represents the break point (and has different PI values). In the above example only sample ms02g has break point and needs to be phased. All other samples have haplotype block that bridges these two consecutive haplotype blocks and contains the data required to extend the phase state of sample ms02g . Fig 02: Representing a break-point in the sample: ms02g contig    pos    ms02g_PI    ms02g_PG_al
2     15881764     6         C|T
2     15881767     6         T|C
2     15881989     6         A|C
2     15882091     6         G|T
                   ×——————————×—————> Break Point
2     15882451     4         T|C
2     15882454     4         T|C 
2     15882493     4         T|C
2     15882505     4         T|A So, in the above haplotype file there is a breakpoint in sample ms02g at the position 15882091-15882451 . For sample ms02g(PI-6) the haplotypes are C-T-A-G and T-C-C-T . Similarly, at PI-4 the haplotypes are T-T-T-T and C-C-C-A . Since the haplotype is broken in two levels, we don’t know which phase from level-6 goes with which phase of level-4. But, all other samples have full haplotype intact that bridges this position. We can therefore use this information from other samples to join the two consecutive haplotype in sample ms02g. Using markov-chain transition to extend the haplotype block: Since, all other samples are completely phased bridging that breakpoint, I can run a markov-chain transition probabilities to solve the phase state in the sample ms02g . To the human eye/mind you can clearly see and say that left part of ms02g (PI-6, i.e C-T-A-G is more likely to go with right block of PI-4 C-C-C-A), thereby creating the extended haplotype block as C-T-A-G-C-C-C-A and T-C-C-T-T-T-T-T . Below, I show how I can apply the first order markov transition matrix  to compute the likelyhood estimates, calculate the log2Odds and then assign and extend the haplotype in proper configuration. And, I feed this logic to the computer using python. Calculation of likelyhood estimates using markov transition: Step 01:   prepare required haplotype configuration The top PI-6 is Block 01 and the bottom PI-4 is Block 02 . The phased haplotype in the left within each block is Hap-A and on the right is Hap-B . Fig: representation of the haplotype breakpoint and block assignment ms02g_PI    ms02g_PG_al
  6         C|T \
  6         T|C | Block 1
  6         A|C |
  6         G|T /
  ×——————————×—————> Break Point
  4         T|C \
  4         T|C |
  4         T|C | Block 2
  4         T|A /
           ↓   ↓
       Hap-A   Hap-B So, the two consecutive blocks can be extended in one of the two possible haplotype configurations: Parallel Configuration:
 Block01-HapA with Block02-HapA, so B01-HapB with B02-HapB 
Vs. Alternate Configuration:
 Block01-HapA with Block02-HapB, so B01-HapB with B02-HapA Step 02: Compute transition matrix and estimate likelihood for each configuration. Fig 03: Representation of the allele transition matrix (from alleles of former block-01 to alleles of later block-02). Possible
transitions     ms02g_PG_al
 │              ┌┬┬┬  C|T \
 │              ││││  T|C | Block 1
 │              ││││  A|C |
 │              ││││  G|T /
 └────────────> ││││   ×—————> Break Point
                │││└> T|C \
                ││└─> T|C | Block 2
                │└──> T|C |
                └───> T|A /
                     ↓   ↓ 
                 Hap-A   Hap-B I count the number of transitions from each nucleotide of PI-6 to each nucleotide of PI-4 for each haplotype configuration across all the samples and convert them to transition probabilities. And multiply the transition probabilities from the first nucleotide in PI-6 to all nucleotides of PI-4. Then similarly multiply the transition probability from 2nd nucleotide of PI-6 to all nucleotides in PI-4, and so on. When transition probabilities are calculated for all possible combination (from each position of PI-6 to each position of PI-4), then I compute the cumulative transition probabilities for each possible haplotype configuration. Fig 04 : Representation of nucleotide counts (emission counts) at positions 15881764 and 15882451. pos\allele     A    T    G    C
15881764       0    8    0    4
15882451       1    7    0    4 Fig 05 : Representation of transition matrix counts (from pos 15881764) to (pos 15882451). This transition matrix is computed from nucleotides (A,T,G,C) at block01 to nucleotides (A,T,G,C) at block02 for all the positions. Transition counts are then converted to transition probabilities. from     to
          A    T    G    C
  A       0    0    0    0
  T       1    6.5  0    0.5
  G       0    0    0    0
  C       0    0.5  0    3.5
**Note: if the PI matches between two blocks the transition are counted 
  as 1, else 0.5. Sample ms02g itself is also taken as an observation. Step 03: Compute the maximul likelihood for each configuration Fig 06 : Likelihood estimate for parallel configuration using transition counts (probabilities) Parallel configuration:
Block-1-Hap-A (C-T-A-G) with Block-2-Hap-A (T-T-T-T)
  CtT × CtT × CtT × CtT = (0.5/4)*(0.5/4)*(0.5/4)*(0.5/4) = 0.000244
+ TtT × TtT × TtT × TtT = (0.5/2)*(0.5/2)*(0.5/2)*(0.5/2) = 0.003906
+ AtT × AtT × AtT × AtT = (0.5/3)*(0.5/3)*(0.5/3)*(0.5/3) = 0.0007716
+ GtT × GtT × GtT × GtT = (0.5/2)*(0.5/2)*(0.5/2)*(0.5/2) = 0.003906
——————— ————————— ——————— ————————— Max Sum (likelihoods) = 0.008828
                                    Average (likelihoods) = 0.002207

Block-1-Hap-B (T-C-C-T) with Block-2-Hap-B (C-C-C-A)
  TtC × TtC × TtC × TtA = (0.5/8)*(0.5/8)*(0.5/8)*(0.5/8) = 0.00001526
+ CtC × CtC × CtC × CtA = (2.5/10)*(2.5/10)*(2.5/10)*(2.5/10) = 0.003906
+ CtC × CtC × CtC × CtA = (1.5/8)*(1.5/8)*(1.5/8)*(1.5/8) = 0.001236
+ TtC × TtC × TtC × TtA = (0.5/4)*(0.5/4)*(0.5/4)*(0.5/4) = 0.000244
——————— ————————— ——————— ————————— Max Sum (likelihoods) = 0.0054016 
                                    Average (likelihoods) = 0.0013504
**note: 
  - ""AtC"" -> represent ""A"" to ""C"" transition
  - ""+"" represents the summation of the likelyhoods Fig 07 : Likelihood estimate for alternate configuration using transition counts (probabilities) Alternate configuration:
Block-1-Hap-A (C-T-A-G) with Block-2-Hap-B (C-C-C-A)
  CtC × CtC × CtC × CtA = (3.5/4)*(3.5/4)*(3.5/4)*(3.5/4) = 0.5861 
+ TtC × TtC × TtC × TtA = (1.5/2)*(1.5/2)*(1.5/2)*(1.5/2) = 0.3164
+ AtC × AtC × AtC × AtA = (2.5/3)*(2.5/3)*(2.5/3)*(2.5/3) = 0.4823
+ GtC × GtC × GtC × GtA = (1.5/2)*(1.5/2)*(1.5/2)*(1.5/2) = 0.3164
——————— ————————— ——————— ————————— Max Sum (likelyhoods) = 1.7012 
                                    Average (likelihoods) = 0.425311

Block-1-Hap-B (T-C-C-T) with Block-2-Hap-A (T-T-T-T) 
  TtC × TtC × TtC × TtA = (6.5/8)*(7.5/8)*(7.5/8)*(6.5/8) = 0.5802
+ CtC × CtC × CtC × CtA = (6.5/10)*(7.5/10)*(7.5/10)*(6.5/10) = 0.237
+ CtC × CtC × CtC × CtA = (5.5/8)*(6.5/8)*(6.5/8)*(6.5/8) = 0.36875
+ TtC × TtC × TtC × TtA = (3.5/4)*(3.5/4)*(3.5/4)*(2.5/4) = 0.4187
——————— ————————— ——————— ————————— Max Sum (likelyhoods) = 1.60465
                                    Average (likelihoods) = 0.4011625

**note: 
  - the sum of the likelihoods > 1, in the above example.
  - So, we can rather use the product of the likelyhoods. Fig 08 : Likelhood estimate of Parallel vs. Alternate configuration. Likelihood of Parallel vs. Alternate configuration

= likelihood of Parallel config / likelihood of Alternate config

= (0.002207 + 0.0013504)/ (0.425311 + 0.4011625)
= 0.0043043  (i.e 1/232)
Therefore, haplotype is 232 times more likely to be phased 
in ""alternate configuration"".

log2Odds = log2(0.0043043) = -7.860006
So, with our default cutoff threshold of ""|5|"", we will be extending 
the haplotype in ""alternate configuration"" Final Output data: contig   pos   ref   all-alleles   ms02g_PI   ms02g_PG_al
2   15881764   .    .                  6      C|T
2   15881767   .    .                  6      T|C
2   15881989   .    .                  6      A|C
2   15882091   .    .                  6      G|T
2   15882451   .    .                  6      C|T
2   15882454   .    .                  6      C|T
2   15882493   .    .                  6      C|T
2   15882505   .    .                  6      A|T So, how do I translate my overall computation process in mathematical and statistical language. I am looking to represent computation of: emission counts (and probabilities) transition counts (and probabilities) summation of the transition counts for each configuration computation of likelihood ratio and it's conversion to log2Odds I am trying to represent my computation (algorithm) in the way it's shown in these links, but have a hard time doing it. https://www.probabilitycourse.com/chapter11/11_2_3_probability_distributions.php https://stats.stackexchange.com/questions/25540/summation-of-conditional-probability-in-bayesian-network How to write an algorithm as an equation? FYI - I am biologist but have managed to learn python and write a python program to work it out ( https://codereview.stackexchange.com/questions/186396/solve-the-phase-state-between-two-haplotype-blocks-using-markov-transition-proba ). The code isn't relevant in this question though. Now, it's my time to work out some maths and represent my algorithm. Any help, suggestion is appreciated. Thanks,","['biology', 'markov-chains', 'statistics', 'markov-process', 'probability']"
2716201,"What is the difference between lemma, axiom, definition, corollary, etc?","In learning basic analysis I am encountering a lot of language that seems somewhat tough to define: Axiom, proposition, definition, lemma, theorem, law, corollary Are there clear separations in definitions between all these things or is there a lot of overlap? Or is it sometimes a judgment call? How do most people use these words?","['real-analysis', 'proof-writing', 'soft-question', 'definition']"
2716210,Lagrangian intersection problem,"I am trying to prove the following: Let $(M,\omega)$ be a symplectic manifold and suppose that $X$ is a compact lagrangian submanifold of $M$ with $H^{1}_{\text{dR}}(X)=0$. Then, every lagrangian submanifold $Y$ of $M$ which is $C^1$-close to $X$ intersects $X$ in at least two points. (A submanifold $Y$ of $M$ is $C^1$-close to $X$ if there is a diffeomorphism $h\colon X\longrightarrow Y$ such that $\iota \circ h$ is $C^1$ close to the inclusion $X\longrightarrow M$. In order to prove it, due to the similarity, I think that it can be used the following theorem which I have already proven: If $(M,\omega)$ is a compact symplectic manifold such that $H^{1}_{\text{dR}}(M)=0$, then any symplectomorphism $\varphi \colon M \longrightarrow M$ $C^1$-close to $\text{id}$ has at least two fixed points. However, I have no idea how to define $\varphi$ in order to apply the theorem because I need a symplectomorphism from $X$ to $X$. Can anyone give me a clue? Or due you believe it is not possible to use the theorem? Is there any better idea to prove it? Thanks in advance. (Both of the theorems are applications of Lagrange Neighborhood Theorem)","['symplectic-geometry', 'differential-geometry']"
2716229,Implicit Differentiation Coordinates at dy/dx = 0,"With the equation $x^2+2xy-3y^2+16=0$, I need to find the coordinates of the points on the curve where $\frac{dy}{dx} = 0$. I think I have correctly used implicit differentiation to get $\frac{dy}{dx} = \frac{-x-y}{x-3y}$. And the multiplied by the denominator and subsituted zero to get, $0 = -x-y$, however, I'm not sure where to go from here. If anyone could help that would be great. Thanks.","['derivatives', 'implicit-differentiation', 'calculus']"
2716247,Evaluating $\lim_{x\to \infty } (x +\sqrt[3]{1-{x^3}} ) $,"$$\lim_{x\to \infty } (x +\sqrt[3]{1-{x^3}} ) $$ 
What method should I use to evaluate it. I can't use the  ${a^3}$-${b^3}$ formula because it is positive. I also tried to separate limits and tried  multiplying  with  $\frac {\sqrt[3]{(1-x^3)^2}}{\sqrt[3]{(1-x^3)^2}}$ , but still didn't get an answer. I got -$\infty$, and everytime I am getting $\infty -\infty$ .","['calculus', 'limits']"
2716268,What is the intuition for the multiplicity of a root of a polynomial equation?,"The fundamental theorem of algebra states that: Every non-zero, single-variable, degree $n\,$ polynomial with complex coefficients has, counted with multiplicity , exactly $n\,$ complex roots. The term I want to understand is multiplicity. Now, I already know the following: The multiplicity of a root $x_0$ of a polynomial equation $p(x) = 0\,$ tells us how many times the factor $(x - x_0)\,$ divides the polynomial $p(x)$. That's fine. But what I want to have, is an intuition. The definition of a root is: $x_0\,$ is a root of $p(x) = 0\,$ if and only if $p(x_0) = 0$. So, I don't understand how a root could have a property called multiplicity . For example, the multiplicity of a root $x_0$ cannot be the number of times $x_0\,$ solves the equation $p(x) = 0\,$ because a root cannot really solve an equation more than once . Nor does it make sense as the number of times the graph of the function $y = p(x)\,$ meets the $x$-axis at the $x$-coordinate that is $x_0$ , because they meet at a given point only once [because of the definition of a function]. But I do know of the geometric intuition: If multiplicity is odd, then the axis is crossed , otherwise it touches and comes back , and the higher the multiplicity, the closer $p(x)\,$ stays near the $x$-axis in the neighborhood of $x = x _0$, and so on. What I want to have is an intuition, an answer to the question: What is the multiplicity of a root of a polynomial equation the multiplicity of? Can somebody plz help me on this one? NOTE: Here, I am using multiplicity the same way I would use the term frequency . Plz correct me if this usage is wrong.","['algebra-precalculus', 'intuition', 'roots', 'polynomials']"
2716277,The relation of $O$- and $\Omega$-symbols and an unexpected absolute sign in the definiton of $O$.,"In the article Big Omicron and the big omega and big theta D. Knuth defines on p.19
\begin{align*}
 O(f(n)) & := \{ g(n) \mid \exists C > 0 \exists n_0 > 0 \forall n \ge n_0 : |g(n)| \le Cf(n) \} \\ 
 \Omega(f(n) & := \{ g(n) \mid \exists C  0 \exists n_0 > 0 \forall n \ge n_0 : g(n) \ge Cf(n) \}.
\end{align*}
And the whole article centers around the idea to define $\Omega(f(n))$ as a lower bound notation contrary to $O(f(n))$. On wikipedia it is written
$$
 f(n) \in \Omega(g(n)) \Leftrightarrow g(n) \in O(f(n)).
$$ But what puzzles me is the absolute sign in the definition of $O(f(n))$. By this the above symmetry does not hold? So why is there an absolute sign? And then shouldn't there be an absolute sign around $f(n)$ too? The author itself mentions on page 21: [...] Note that there is a slight lack of symmetry in the above definitions of $O$, $\Omega$, and $\Theta$, since absolute value signs are used in $g(n)$ only in the case of $O$. This is not really an anomaly, since $O$ refers to a neighborhood of zero while $\Omega$ refers to a neighborhood of infinity. [...] Guess it is related to my question, but I totally do not understand this paragraph, both symbols say something about the behaviour for very large arguments, hence both refer to a neighborhood of infinity?? So what does the author has in mind if he says $O$ refers to a neighborhood of zero? So I hope someone could clarify the relations. Why is there an absolute sign written in one definition, and not in the other? And why is there no absolute sign around both $g(n)$ and $f(n)$? And how does the equivalence from wikipedia holds? As I see it, it does not holds in general, just for positive functions? And what does the author tries to say with the above cited paragraph?","['real-analysis', 'asymptotics', 'number-theory', 'computational-complexity', 'analysis']"
2716280,Does every bounded operator on a complex Hilbert space have an eigenvalue?,"Is the following statment true? Let $\mathscr{H}$ be a complex Hilbert space and let $\varphi: \mathscr{H} \to \mathscr{H}$ be a bounded operator. Does $\varphi$ have an eigenvalue in general? If yes, how to prove this? If not, what is a counterexample and what property does one need in order to ensure the existence of an eigenvalue. In particular, I am interested in the case where $\mathscr{H}$ has a unitary representation $\pi: G \to U(\mathscr{H})$ such that $\varphi \circ \pi(g) = \pi(g) \circ \varphi$ for all $g \in G$.","['functional-analysis', 'eigenvalues-eigenvectors']"
2716285,Prove $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})$ is divergent. [duplicate],"This question already has answers here : Ways of showing $\sum_\limits{n=1}^{\infty}\ln(1+1/n)$ to be divergent (8 answers) Closed 6 years ago . Evaluate if the following series is convergent or divergent: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})$. I tried to evaluate the divergence, applying the Weierstrass comparison test: $\sum\limits_{n=1}^\infty \ln({1+\frac 1 {{n}}})>\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$. Since the function is decreasing then as $1+\frac{1}{n}$ is closer to $0$ then the inequality follows. Obviously $\sum\limits_{n=1}^\infty \ln({\frac 1 {{n}}})$ diverges since $\lim_{n\to\infty}\ln({\frac 1 {{n}}})=-\infty$. Questions : 1) Is my answer right? If not why? 2) What other kind of approach do you propose? Thanks in advance!","['real-analysis', 'sequences-and-series', 'calculus']"
2716308,Is there any way to determine $\mathrm{Arg}[ \Gamma(ix) ]$?,"Fixing some real $x>0$, there is a result which says that:
$$
|\Gamma(ix)| \ = \ \frac{\sqrt{\pi}}{\sqrt{x\sinh(\pi x)}}
$$ So this means the following:
$$
\Gamma(ix) \ = \ \frac{\sqrt{\pi}}{\sqrt{x\sinh(\pi x)}} e^{i \mathrm{Arg}\left[ \Gamma(ix) \right]}
$$ Is there any way to determine $\mathrm{Arg}\left[ \Gamma(ix) \right]$? What properties does this function have? I've included a plot of the function, and there are a bunch of discontinuities for this function.","['complex-analysis', 'gamma-function', 'complex-numbers', 'functions']"
2716380,Using the Saddle point method (or Laplace method) for a multiple integral over a large number of variables,"I am trying to understand the saddle point method used in the large N limit of matrix models. First, for the case of the integral of a single variable I found this notes There they say that you can approximate the integral $$I(A)=\int_{-\infty}^\infty e^{Ag(x)} dx$$
by expanding $A g(x)$ in powers of $y=(x-x_0)\sqrt{A}$ $$Ag(x)=Ag(x_0)+\frac{1}{2}g''(x_0)y^2+\frac{g'''(x_0)}{6\sqrt{A}}y^3+\dots$$ where $x_0$ is a maximum of $g(x)$. Then we expand the exponential $e^{Ag(x)}$ $$e^{Ag(x)}=e^{Ag(x_0)}e^{\frac{1}{2}g''(x_0)y^2}(1+\frac{g'''(x_0)}{6\sqrt{A}}y^3+\frac{g''''(x_0)}{72A}y^4+\dots)$$ so now $$I(A)=e^{Ag(x_0)}\int_{-\infty}^\infty \frac{dy}{\sqrt{A}}e^{\frac{1}{2}g''(x_0)y^2}(1+\frac{g'''(x_0)}{6\sqrt{A}}y^3+\frac{g''''(x_0)}{72A}y^4+\dots) $$ The first integral is a gaussian, the others are known, so this gives an asymptotic series $$I(A)=e^{Ag(x_0)}\sqrt{\frac{2\pi}{-Ag''(x_0)}}(1+\frac{C_2}{A}+\frac{C_4}{A^2}+\dots) $$ Now, I want to do the same for the large N limit of partition function of a matrix model. The partition function is a multiple integral over a large number of variables $$Z=\int_{-\infty}^\infty [dm] e^{-N^2 S(m)}$$ $$[dm]=\prod_{j=1}^{N}dm_j$$ Here, the action is a function of $N$ variables. For the gaussian model for example $$S(m)=\frac{2}{\lambda N}\sum_{j'=1}^N m_{j'}^2-\frac{2}{N^2}\sum_{j'=2}^N\sum_{i=1}^{j'-1}\ln|m_i-m_{j'} |$$ Let's say $m_{(0)}$ is the minimun of $S(m)$. Using Taylor expansion of several variables we have $$e^{-N^2S(m)}=e^{-N^2S(m_{(0)})} \ e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})} \ e^{-\frac{1}{3!N}(u\cdot\nabla)^3S(m_{(0)})-\frac{1}{4!N^2}(u\cdot\nabla)^4S(m_{(0)})+\dots}$$ where the vector $u$ is $$u=N(m-m_{(0)})$$ $$e^{-N^2S(m)}=e^{-N^2S(m_{(0)})} \ e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})} \left(1 -\frac{1}{3!N}(u\cdot\nabla)^3S(m_{(0)})-\frac{1}{4!N^2}(u\cdot\nabla)^4S(m_{(0)})+\dots\right)$$ so now $$Z=N^{-N}e^{-N^2S(m_{(0)})}\times\int [du]e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})} \left(1 -\frac{1}{3!N}(u\cdot\nabla)^3S(m_{(0)})-\frac{1}{4!N^2}(u\cdot\nabla)^4S(m_{(0)})+\dots\right)$$ Now, the first integral is a gaussian. The second integral $$\int [du]e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})}\frac{1}{3!N}(u\cdot\nabla)^3S(m_{(0)})=0$$ The problem is in the third integral $$C=\int [du]e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})}\frac{1}{4!N^2}(u\cdot\nabla)^4S(m_{(0)})$$ Since $$(u\cdot\nabla)^4S(m_{(0)})=\sum u_{i_1}u_{i_2}u_{i_3}u_{i_4}\partial_{i_1}\partial_{i_2}\partial_{i_3}\partial_{i_4}S(m_{(0)})$$ we have $$C=\frac{1}{4!N^2}\sum \partial_{i_1}\partial_{i_2}\partial_{i_3}\partial_{i_4}S(m_{(0)})\int [du]u_{i_1}u_{i_2}u_{i_3}u_{i_4}e^{-\frac{1}{2}(u\cdot\nabla)^2S(m_{(0)})}$$ The sum over indices will give an order $N^4$ so it seems that $C$ is bigger than the gaussian integral, the opposite of what you expect. What is the problem here? Have I done the power counting properly? I want to get an asymptotic series where the terms get smaller and smaller.","['approximation-theory', 'integration', 'asymptotics', 'laplace-method']"
2716548,Changing the length of an interval for the Fourier coefficients,"Suppose the trigonometric Fourier coefficients are defined in the usual way as $$a_n=\frac{1}{\pi}\int_{- \pi}^\pi f(x)\cos(nx)dx \qquad \text {&} \qquad b_n=\frac{1}{\pi}\int_{- \pi}^\pi f(x)\sin(nx)dx\tag{a}$$ on the interval $[-\pi,\pi ]$ and the length of the interval ($2\pi$) changes to $2\ell$ where $\ell \gt 0$. All the textbooks I have checked just state the answer is $$a_n=\frac{1}{\ell}\int_{- \ell}^\ell f(x)\cos\left(\frac{n\pi x}{\ell}\right)dx \qquad \text {&} \qquad b_n=\frac{1}{\ell}\int_{- \ell}^\ell f(x)\sin\left(\frac{n\pi x}{\ell}\right)dx\tag{b}$$ What transformation took place to convert the integrals in $(\mathrm{a})$ to the integrals in $(\mathrm{b})$? More specifically; in the argument of the sines and cosines why does $nx \to \dfrac{n\pi x}{\ell}$? Since the interval is now of length $2 \ell$ why is the factor out front not $\dfrac{1}{2\ell}$? I realise that this is a lot of questions, so if anyone could answer even one of them it will be greatly appreciated. I can't show my efforts at trying to answer the question (the prerequisite) since I am stuck at the very start.","['fourier-series', 'fourier-analysis', 'trigonometry', 'integration', 'trigonometric-integrals']"
2716627,Embedded Elliptic Curve is Projectively Normal,"This comes from Ex IV.4.2 in Hartshorne. Namely we want to show that if $D$ is a divisor of degree at least three on an elliptic curve $X$, and we embed $X$ into $\mathbb{P}^n$ via this very ample divisor, then the image is projectively normal. I want to use the trace condition, namely that for every $d>0$, the natural map $$\Gamma(\mathbb{P}^n,\mathcal{O}(d)) \to \Gamma(X,i^*\mathcal{O}(d))$$ is surjective (this is essentially Ex II.5.14(d)). I can show this for $d=1$, but I'm not sure how to show this more generally. I figure something cohomological or the $d$-Uple embedding might help, but I'm just not sure. Any ideas?","['abstract-algebra', 'elliptic-curves', 'algebraic-geometry']"
2716630,A multiplier to make a couple of bounded functions continuous wrt one variable?,"Let $Q=(0,1)^2$ be a square, functions $f_1,f_2\in L_\infty(Q)$ and $f_1,f_2\ge c>0$ on $Q$. Does there exists for every pair of such $f_1,f_2$ a function $g>0$ on $Q$ s.t. function $f_1g$ is continuous wrt $x$ on $(0,1)$ for every $y\in(0,1)$ and $f_2g$ is continuous wrt $y$ for every $x\in(0,1)$?","['multivariable-calculus', 'real-analysis', 'continuity']"
2716634,Prove the relation is an equivalence relation.,"Problem Define the relation $R$ on the set of natural numbers as $(a,b) \in R
> \iff 2 \vert(a^2 + b) $. Prove that $R$ is an equivalence relation. This is what I have so far. Claim: Define the relation $R$ on the set of natural numbers as $(a,b) \in R
> \iff 2 \mid(a^2 + b) $. The relation $R$ is an equivalence relation. Proof: Part 1 (Reflixivity): Let $R = \{(a,b) \in \Bbb{N} \times \Bbb{N} \mid 2 \mid (a^2+b)\}$ be given and suppose that $b \in \Bbb{N}$. Then, for some integer $k$: $$\require{enclose}
\enclose{downdiagonalstrike}{\begin{align}
2 \mid b^2 +b \iff 2k &= b^2 +b \\
 & = b^2 + b - (b^2 + b) + (b^2 + b) \\ 
 & = 2b^2 + 2b - (b^2 + b) \\ 
 & = -2b^2 - 2b + (b^2 + b)
\end{align}}$$ Therefore, 
$$\enclose{downdiagonalstrike}{\begin{align}
2 \mid b^2 +b \iff b^2 + b &= 2k -2b^2 - 2b \\
 & = 2(k - b^2 - b)\\  
 & 
\end{align}}$$ $\enclose{horizontalstrike}{\text{Thus, $2 \mid b^2 +b$ for some integer $(k - b^2 - b)$. Which implies that $R$ is Reflexive.}} $ EDIT: Thanks to some positive feed back I have been let known that this is not showing Reflexivity. Part 2 (Symmetry) Let $R = \{(a,b) \in \Bbb{N} \times \Bbb{N} \mid 2 \mid (a^2+b)\}$ be given and suppose that, for any $a,b \in \Bbb{N}$, $ a\mathbf{R}b \leftrightarrow b\mathbf{R}a.$ 
Then, for some integer $k$: $$\enclose{updiagonalstrike}{\begin{align}
2 \mid a^2 + b  & \iff 2k = a^2 + b \\
 & \iff 2k + (a + b^2) = (a^2 + b) + (a + b^2)\\ 
 & \iff (a + b^2) = (a + b) + (a^2 + b^2) - 2k \\ 
 & 
\end{align}}$$ $\enclose{horizontalstrike}{\text{Since, the relation $R$ is proven to be Reflexive, let the integer $ m = a = b $ and let the integer $ n = a^2 = b^2 $. Then, }}$ EDIT: This is not a valid way to show Symmetry, since Part 1 (Reflexivity) has not been proven. $$\enclose{updiagonalstrike}{\begin{align}
a + b^2 = (a + b) + (a^2 + b^2) - 2k & \iff (a + b^2) = 2m + 2n - 2k \\
 & \iff (a + b^2) = 2(m + n -k)\\  
 & 
\end{align}}$$ $\enclose{horizontalstrike}{\text{Thus, $2 \mid b^2 + a$ which implies that $R$ is Symmetric since $ a\mathbf{R}b \leftrightarrow b\mathbf{R}a $.}}$ Part 3 (Transitivity) Let $R = \{(a,b) \in \Bbb{N} \times \Bbb{N} \mid 2 \mid (a^2+b)\}$ be given and suppose that, for any $a,b,c \in \Bbb{N}$, $ a\mathbf{R}b \text{ and } b\mathbf{R}c.$ Then, let $k$ and $h$ be some integers: \begin{align}
2k = a^2 + b \text{ and } 2h = b^2 + c & \implies 2(k + h) = (a^2 + b) + (b^2 + c) \\
 & \\ 
 & \\ 
 & 
\end{align} Comment: And, this is where I get stuck. I am struggling to find a way to show that $ (2 \mid a^2 + b) \land (2 \mid b^2 + c) \implies 2 \mid a^2 + c$. I've also tried eliminating $b$ like so, $2h = (2k - a^2)^2 + c$, but this doesn't seem to get me any where. I feel like I'm running in circles here. My Question Can you argue that $R$ is transitive since it has already been shown that $2 \mid b^2 + b$? This would imply something like ""$2(k - b^2 - b) + 2h = (a^2 + c) +(b^2 + b)$ is logically equivalent to $(2 \mid b^2 + b) \land (2 \mid a^2 + c)$."" And, this simplifies to just $2 \mid a^2 + c$ by the inference rule of simplification [$(p \land q) \to p$]. Which ultimatily I believe gets me to my goal, but I'm not sure if it is two far of a leep to go from $2(k - b^2 - b) + 2h = (a^2 + c) +(b^2 + b) \implies (2 \mid b^2 + b) \land (2 \mid a^2 + c)$. I hope my question was specific enough. Otherwise, I would much appreciate some guidance on showing how this relation is transitive if anyone is feeling generous. Thanks!","['relations', 'proof-writing', 'elementary-set-theory', 'proof-verification']"
2716696,Variation of constants on a second order DE,"Given is the DE $$\frac{d^2z}{dx^2}+z=\frac{\mu}{x^2}z$$
I have to prove that for $0<a\leqslant x$, $$z(x)=z(a)\cos(x-a)+z'(a)\sin(x-a) + \int_a^x \sin(x-\xi)\frac{\mu}{\xi^2}z(\xi) \, d\xi$$
I think it is a good idea to use variation of constants. So first I solved $\frac{d^2z}{dx^2}+z=0$, giving me $z(x)=c_1\sin(x)+c_2\cos(x)$, with $c_1, c_2$ constants. Then, I wrote $z(x)=u_1(x)\sin(x)+u_2(x)\cos(x)$ and calculated $z'(x)$ and $z''(x)$ accordingly. But here I got lost. Can anybody help me?",['ordinary-differential-equations']
2716698,Rational plane curves as images of $\mathbb{P}^1$,"A rational projective plane curve is a curve $C\subseteq \mathbb{P}^2$ such that there exists a birational map $\mathbb{P}^1\dashrightarrow C$. My question is whether the following statement is true: A curve $C\subseteq \mathbb{P}^2$ is rational if and only if there exists a morphism of varieties $f: \mathbb{P}^1\rightarrow \mathbb{P}^2$ with $C = f(\mathbb{P}^1)$. I've been studying rational curves recently and it seems completely natural to expect this to be the case, but I haven't been able to come up with a proof or counterexamples. The image of $\mathbb{P}^1$ by a nonconstant morphism $\mathbb{P}^1\rightarrow \mathbb{P}^2$ is an irreducible curve, and I was hoping that I could describe every rational plane curve in this way. The only if direction seems to be closely related to the question of whether a birational map $\mathbb{P}^1\dashrightarrow C$ can be extended to a morphism $\mathbb{P}^1\rightarrow C$, but I cannot seem to find a reference for the truth of this statement either. My primary interest is for when the base field is $\mathbb{C}$, if this can affect the answer. Thanks very much for any help!","['plane-curves', 'algebraic-geometry']"
2716730,Eisenstein series twisted by a Dirichlet character,"On page 17 of https://people.mpim-bonn.mpg.de/zagier/files/doi/10.1007/978-3-540-74119-0_1/fulltext.pdf , we see a remark where the author mentioned that If $\chi$ is a non-trivial Dirichlet character then there is an Eisenstein series having the Fourier expansion for 
$$\mathbb{G}_{k,\chi}(z)=c_{k}(\chi)+\sum_{n=1}^{\infty}\Big(\sum_{d|n}\chi(d)d^{k-1}\Big)q^n$$ 
which satisfies the modularity 
$$\mathbb{G}_{k,\chi}\Big(\frac{az+b}{cd+d}\Big)=\chi(a)(cz+d)^{k}\mathbb{G}_{k,\chi}(z).$$
When the character $\chi$ is primitive but not-trivial satisfying $\chi(-1)=(-1)^k$, I found no problem with it except that I think one of the ""${\chi}$"" has lost (typo) a ""bar"". For example I found it more correct if we write 
$$\mathbb{G}_{k,\chi}(z)=c_{k}(\chi)+\sum_{n=1}^{\infty}\Big(\sum_{d|n}\overline{\chi}(d)d^{k-1}\Big)q^n.$$
Maybe the book is correct and I am wrong or maybe vice versa. When $\chi$ is non-primitive satisfying $\chi(-1)=(-1)^k$, my problem is that I have no clue about how to confirm why the same formula for the Eisenstein series can hold (suppose that the book is correct). I know that a combination of  ""old forms"" like 
$$\sum_{d|\frac{m}{m^\ast}}\frac{a_d}{\sum_{d|\frac{m}{m^\ast}}a_d}f_d(z)$$ satisfies the same modularity condition, where $m$ is the modulus of $\chi$, $\chi^{\ast}$ the primitive character which induces $\chi$ and $m^{\ast}$ its modulus. And $a_d=d^k\mu(m/dm^{\ast})\chi^{\ast}(m/dm^{*})$, $f_d=\mathbb{G}_{k,\chi^{\ast}}(dz)$. But it is for sure not exactly what is written in the book as $\mathbb{G}_{k,\chi}(z)$, hence the book gives a ""new"" family of Eisenstein series (which might not be a linear combination of ""old forms""), about which I would like to know how to compute and also a little bit more details of the its background. Also I would like to know how to prove that $f_d(z)$ in the above formula are linearly independent over $\mathbb{C}$.","['number-theory', 'dirichlet-series', 'modular-forms']"
2716737,Problem evaluating a contour integral using parametrization,"I tried to solve the following contour integral:
$$
\oint_\gamma  {\frac{{dz}}{{z - c}}}
$$
Where $\gamma$ is a disk centered at the origin. In order to do so, I used the following parametrization: 
$$
\begin{array}{l}
 z &= Re^{i\varphi }, \qquad 0 < \left| R \right| \ne \left| c \right|  \\ 
 dz &= iRe^{i\varphi } d\varphi 
 \end{array}
$$
Replacing in the contour integral:
$$
\begin{array}{l}
 \oint_\gamma  {\frac{{dz}}{{z - c}}}  &= \int\limits_0^{2\pi } {\frac{{iRe^{i\varphi } }}{{Re^{i\varphi }  - c}}} d\varphi  \\ 
  &= \left. {\ln \left( {Re^{i\varphi }  - c} \right)} \right|_0^{2\pi }  \\ 
  &= \ln \left( {Re^{i2\pi }  - c} \right) - \ln \left( {Re^{i0}  - c} \right) \\ 
  &= \ln \left( {R - c} \right) - \ln \left( {R - c} \right) \\ 
  &= 0
 \end{array}
$$
However, by the residue theorem the contour integral must be equal to $2\pi i$ if $\left| R \right| > \left| c \right|$, whereas in the answer obtained by parametriztion the value is always $0$. My question is: What am I missing here? Where is my mistake? Thank you in advance.","['logarithms', 'complex-analysis', 'contour-integration', 'complex-integration']"
2716792,Showing that an operator between Hölder spaces is a contraction,"I'm having trouble with the following problem: Consider $\beta\in(\frac{1}{2},1]$, $\xi\in\mathbb{R}$, $f:\mathbb{R}\longrightarrow \mathbb{R}$ bounded with first and second derivatives bounded, $X\in \mathcal{C}^{\beta}[0,T]$ (Space of Hölder continuous functions of order $\beta$). Let $\alpha\in (\frac{1}{2},\beta)$ and define 
  $$\mathcal{M}_T:\mathcal{C}^{\alpha}[0,T]\longrightarrow \mathcal{C}^{\alpha}[0,T]$$
  $$Y\mapsto \xi + \int_{0}^{t}f(Y(r))dX(r), \quad t\in[0,T]$$
  (Usual Riemann-Stieltjes integral).
  Show that there exist $T_0\in[0,T]$ such that for every $Y,\tilde{Y}\in\mathcal{C}^{\alpha}[0,T_0]$
  $$\|\mathcal{M}_{T_0}(Y)-\mathcal{M}_{T_0}(\tilde{Y})\|_{\alpha, [0,T_0]}\leq \frac{1}{2}\|Y-\tilde{Y}\|_{\alpha,[0,T_0]},$$
  where $$\|X\|_{\alpha,[0,T]}:=\sup_{s\neq t \in [0,T]}\dfrac{|X(t)-X(s)|}{|t-s|^{\alpha}}.$$ I don't see how to use the boundedness of the second derivative and to obtain the inequality. What is more, I don't even know if the function is well-defined. (I just know that the integral exists, and here I use the boundedness of the first derivative).","['real-analysis', 'ordinary-differential-equations', 'holder-spaces', 'stieltjes-integral']"
2716803,"Is $[0,1]$ the Stone-Čech compactification of $(0,1)$?","In this note by G. Eric Moorhouse, which appears to be some course notes handout, it is stated on page 3: The [two-point] above is the Stone-Čech compactification of $(0,1)≃\mathbb{R}$;that is, $\beta\mathbb{R} ≃ [0, 1]$. This is the most general compactification of $\mathbb{R}$ in a sense that we proceed to describe. Then he argues, complete with diagrams, that for every embedding of $(0,1)$, there is a unique extension On the other hand, it was my understand, as similar to how it is set out by Henno Brandsma in this answer , that the Stone-Čech compactification does not generally admit an explicit description, only existence proofs using the axiom of choice. And moreover we can say that it has cardinality $2^{2^\mathfrak{C}}$ (or at least $2^\mathfrak{c}$ I'm not sure) so there's no way that $[0,1]$ could be homeomorphic to $\beta\mathbb{R}.$ Also, as stated in this question , no sequence in $X$ will have its limit in $\beta X\setminus X$, whereas the endpoints of $[0,1]$ are sequential limit points of $(0,1)$. So what's going on here? Probably just a mistake by Moorhouse, right?","['general-topology', 'compactification']"
2716899,How do you notate the edges connected to a node in a graph?,"So given a directed multigraph $G=(V,E)$ I am notating the nodes of any edge like so $(e_s,e_t) = e \in E$ where $e_s$ is the source and $e_t$ is the target. However I am unsure how to notate the edges of a node that finish or start there. So far I have written $v_\text{in} = \{e_1,...,e_k\} \subseteq E$ where $v_\text{in}$ is all edges finishing at node $v$. However it's not explicit that the edges in $v_\text{in}$ actually finish at node $v$. Is there a standard convention or more accepted way of expressing this? Also I'm in the computer sciences so it would be great to know if I have made any mistakes in expressing the maths involved in this question. Edit: Might have answered my own question, can I just write $v_\text{in} = \{e \mid e_t = v \}$?","['graph-theory', 'network', 'notation', 'elementary-set-theory']"
2716901,Expected value of the absolute value of the difference between two independent uniform random variables? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I am trying to calculate the expected value of the absolute value of the difference between two independent uniform random variables. Let $X_1\sim\operatorname{Uniform}(0, 2)$ and $X_2\sim\operatorname{Uniform}(0, 2)$ and $X_1$ and $X_2$ are independent. I want to calculate $\operatorname E \left[|X_1 - X_2|\right]$. Can anyone please help?","['uniform-distribution', 'probability', 'expected-value']"
2716919,"Series with $\sum a_n$ converges but $\sum n a_n^2$ diverges, and $a_n$ is decreasing",Is it possible to have a sequence $a_n \geq 0$ which is decreasing and such that $\sum a_n < \infty$ but $\sum n a_n^2 = \infty$? I have seen Find a sequence $a_n$ so that $\sum |a_n|$ converges but $\sum n |a_n|^2$ diverges where the condition holds but the $a_n$ given is not decreasing.,['sequences-and-series']
2716928,Does the tangent bundle on a 2-sphere span $\mathbb R^3$ and how are the operations defined?,"As a follow-up to this question I would like to clarify whether the tangent bundle on a sphere in $\mathbb R^3$ spans $\mathbb R^3$ to make sure I get the concept. The tangent bundle is the set of the tangent planes at every single point on the surface of the 2-sphere $S$ and would be defined as $$TS := \{(p, v):\ p\in S, v\in T_pS\}$$ If I get the idea correctly, there would be tangent planes through each point on the surface like the following ones in the drawing representing 3 single points ($P, S, Q)$: Each plane would be translated to go through the origin to construct a vector space of tangent planes: If the above is correct, the intuition is clear: the tangent bundle on the sphere would enable us to find a plane in any possible orientation, and hence, the disjoint union of these tangent planes would span $\mathbb R^3.$ Or is the disjoint piece a game changer? QUESTIONS: Does this ""fan"" of translated tangent planes span $\mathbb R^3$? And how are the addition and scalar multiplication of a vector space defined on this tangent bundle?",['differential-geometry']
2716938,Confusion about notation in do Carmo's Riemannian Geometry book,"This comes from page 125 in the chapter on isometric immersions. Setup: $f:M \rightarrow \bar{M}$ is an immersion. For each $p \in M$ there exists a neibhborhood $U \subset M$ of $p$ such that $f(U) \subset \bar{M}$ is a submanifold of $\bar{M}$ . He says to simplify the notation we will identify $U$ with $f(U)$ and each $v \in T_q M$ , $q \in U$ with $df_q v \in T_{f(q)}\bar{M}$ . Here is where I get confused: He says, for each $p \in M$ , the inner product on $T_p \bar{M}$ splits $T_p\bar{M}$ into the direct sum $T_p\bar{M}=T_pM \oplus (T_pM)^{\perp}$ . Further, if $v \in T_p \bar{M}$ , $p \in M$ , we can write $v=v^{T}+v^{N}$ where $v^T \in T_p M$ and $v^N \in (T_p M)^\perp$ . He says such a splitting is clearly differentiable, in the sense that the mappings $(p,v) \mapsto (p,v^T)$ and $(p,v)\mapsto (p,v^N)$ of $T\bar{M}$ into $T\bar{M}$ are clearly differentiable. My issue with this: I think every instance of $T_p \bar{M}$ should be replaced by $T_{f(p)}\bar{M}$ ,(i.e. we identify $T_p M$ with $df_p(T_p M) \subset T_f(p)\bar{M})$ , unless if $p \in U$ then we could make the identifications that he stated which would make more sense, however, he says this for $p \in M$ , not just $p \in U$ , and He made the ""mistake"" of using $p$ several times which makes me believe that he meant to. So what is going on here? Is he just being incredibly sloppy or am I missing something here?","['smooth-manifolds', 'riemannian-geometry', 'differential-geometry']"
2716955,noncompact difference,The Taylor series can be used to approximate a derivative to a desired amount of error. For instance $\displaystyle \frac{du}{dx} = \frac{u_{i+1}-u_{i-1}}{2\Delta x} + O((\Delta x)^2)$ I have a problem where it asks for a 4th order noncompact difference. Can someone please explain what noncompact means in this context and how it differs from a compact scheme? Thanks,"['derivatives', 'taylor-expansion', 'finite-differences', 'ordinary-differential-equations', 'approximation']"
2716960,Proving Inequalities With Mathematical Induction,"I'm currently working on this problem: $$ 1 + 2^n + ≤ 3^n \text{ for all } 𝑛 ≥ 1 $$ So far, I have: Basis Step:
$ 1 + 2^1 ≤3^1 $
$ P(1) \text{ is true} $ Inductive Step: Assume P(k) holds, prove P(k+1) $P(k) = 1 + 2^k ≤ 3k$ $ P(k+1) = 1 + 2^{k+1} ≤ 3^{k+1} \text{ (I.H.)}$ $ 1 + 2^{k+1} = 1 + 2 * 2^k$ $ \quad \quad \, \, \quad  = 2 * 2^k + 1 ≤ 2 * 3^k$ But now, I'm unsure what to do next. Any help would be aprreciated!
Thank you.","['induction', 'inequality', 'discrete-mathematics']"
2717025,Precise definition of Dihedral Group,"I am studying "" Abstract Algebra "" written by dummit/foote.
In page 23, This book defines the concept of dihedral group as follows: For each n = 3, 4, 5, etc... , 
The set of symmetries of a regular n-gon, where a symmetry is any rigid motion of the n-gon which can be effected by taking a copy back on the original n-gon so it exaclty covers it. I can understand what it means but I have some curious about this definition
because this definition is not rigorous for me.
What is the definition of rigid motion? what is copy back?
Can we admit this kind of vague terminology in math? I tried to find other book which describing the concept symmetry.
In the book "" A first course in Abstract Algebra, this book defines a symmetry of geometrical figure as a rearrangement of the figure preserving the arrangement of its sides and vertices as well as its distance and angles.
I thought that this definition is also depending on our intuition... and not rigourous if we compare this with set theory, real analysis, or other mathematical definition... So, In summary...
Q1. I understood that symmetry is a transformation which preserves shape, angle, distances... in intuitive meaning Did I understand well?
Q2. If I understood well, what is a precise and rigorous definition of symmetry? as we did in set theory, real analysis, etc...
Q3. Can we deal geometry by using a precise and rigorous method only using axiom, set, logic ??","['abstract-algebra', 'symmetry', 'group-theory', 'geometry']"
2717045,Is the P-adic limit $\lim_{n \to \infty} p^{p^n}$ 0 or 1?,"I was playing around making up some limits of things I thought would be interesting to check out and came to something I thought was quite simple on the surface but found it wasn't very consistent with other things in my mind. $$\lim_{n \to \infty} p^{p^n} = 0$$ To me this seemed very obviously 0, since more and more times you multiply p by itself you really get smaller and smaller with respect to the p-adic metric. But something about the concept of exponents and ""counting"" just doesn't seem to make sense in the context of p-adic numbers since there is no ordering. In talking about a similar limit for the p-adic logarithm I've heard it mentioned that the exponent would be getting smaller and smaller, so perhaps it's just as fine to interpret this as, $$\lim_{n \to \infty} p^{p^n} = p^0 = 1$$ Which is perhaps a bit dubious at first, but if I were to expand this as the Mahler series, $$p^x = \sum_{n=0}^\infty (p-1)^n \binom{x}{n} = 1 + (p-1)x + (p-1)^2\frac{x(x-1)}{2} + \cdots$$ Then I run into this issue of taking the limit from this definition a bit more seriously, $$\lim_{n \to \infty} 1 + (p-1)p^{n} + (p-1)^2\frac{p^{n}(p^{n}-1)}{2} + \cdots$$ $$=1$$ However maybe I'm doing some sort of circular reasoning or even running into simply some kind of choice of convention here, I'm not quite so sure.","['p-adic-number-theory', 'limits']"
2717069,Directional derivative for a piece-wise function,"Consider the function $f:\mathbb{R}^2 \rightarrow \mathbb{R}$ given by $$f(x,y)=\begin{cases} 
\frac{xy}{x^2+y^2} &\text{if} (x,y)\neq (0,0)\\
0 &\text{if} (x,y)=(0,0)
\end{cases}$$
For which vectors $v=(v_1,v_2)\neq(0,0)\in\mathbb{R}^2$ does the directional derivative $D_vf(0,0)$ exist? Evaluate the directional derivative wherever it exists. I've managed to get this down to $$D_vf(0,0)=\lim_{h\to 0}\frac{v_1v_2}{h(v_1^2+v_2^2)}$$ and as far as I'm aware this can only exist if one of $v_1$ or $v_2$ is $0$, but not both by the original condition. If this is the case does that mean the directional derivative only exits for $v=(v_1,0) \; \text{or}  \; v=(0,v_2)$? If that is the case, how would I find the value of the directional derivative as it comes down to $D_vf(0,0)=\frac{0}{0}$?","['multivariable-calculus', 'derivatives']"
2717112,Limit involving binomial coefficients without Stirling's formula,"I have this question from a friend who is taking college admission exam, evaluate: $$\lim_{n\to\infty} \frac{\binom{4n}{2n}}{4^n\binom{2n}{n}}$$ The only way I could do this is by using Stirling's formula:$$ n! \sim \sqrt{2 \pi n} (\frac{n}{e})^n$$ after rewriting as $$\lim_{n\to\infty} \frac{(4n)!(n!)^2}{4^n(2n)!^3}$$ and it simplifies really satisfying to $\frac{1}{\sqrt2}$. However Stirling's formula is not in the syllabus nor taught in high school, is there an elementary approach to this limit?","['binomial-coefficients', 'limits']"
2717119,Prove by induction that $4n^2 + 1 < 3\cdot 2^n$ for every $n \ge 6$,"My question is about solving for $k+1$ I did the base case and tried to solve the induction step. this is what I tried my hypothesis is $4k^2 + 1 < 3\cdot 2^k$ is true then I need to show that it is true for $k+1$ I did right hand side by doing this $3\cdot2\cdot2^k = 3\cdot 2^{k+1}$ but I am not able to do left hand side
please help","['algebra-precalculus', 'induction', 'inequality']"
2717141,"integrals, is my answer correct? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Calculate this: $$\int_{0}^{\pi }\frac{\sin2017x}{\sin x}\mathrm dx$$ I have calculated this in the answers, but I'm not sure whether it's right or wrong. I'll be glad if you check it. Thank you guys!","['integration', 'trigonometric-integrals']"
