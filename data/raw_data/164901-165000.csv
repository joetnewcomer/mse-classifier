question_id,title,body,tags
2865846,finding the order of an element in a group presentation,"Let $G$ be a group given by
$$G \ =\ \langle \ x, \  y \mid x^4 =\ y^4=1, \ yx =\ x^2\ y^2 \rangle$$
I found  $\ G/G'$ to be isomorphic to $C_4$.
What can we say about the order of $\ x$ in $G$?
The final answer is $4$.
But $\ x^4 = 1$ in $G$ means that $x^4$ belong to the normal closure of $R$ where $R$ is the relations in $G$. I can't see the connection. Can any one help?","['combinatorial-group-theory', 'group-theory']"
2865860,How many group homomorphisms we can get from ${\mathbb Z}_{20}$ to ${\mathbb Z}_{10}$?,"How many group homomorphisms we can get from ${\mathbb Z}_{20}$  to   $ {\mathbb Z}_{10}$? My Try: I think $4$. Because $1 , 2 , 5 , 10$ are the only possible order of image of ${\mathbb Z}_{20} $. Can anyone please help me?","['group-theory', 'cyclic-groups', 'finite-groups']"
2865862,Flaw in my classification of groups of order 2015,"In an attempt to classify groups of order $2015 = 5 \cdot 13 \cdot 31$, I deduced that only $\mathbb{Z}/2015 \mathbb{Z}$ was the only such group. I then checked with some sources that informed me that there are two groups of order 2015 (up to isomorphism) so I was wondering what the flaw in my proof is. I approached the problem as follows: Let $G$ be a group of order 2015. It is easy to apply Sylow's theorems to show that the Sylow-13 and the Sylow-31 subgroups are normal in $G$; so let $P_{13}$ and $P_{31}$ denote the Sylow-13 and Sylow-31 subgroups, respectively. Also let $P_{5}$ denote a Sylow-5 subgroup. Since $P_{31}$ is normal, $P_{31}P_{5}$ is a subgroup of order $5 \cdot 31$ in $G$. Now since $P_{13}$ is normal, we have that $P_{13} (P_{31} P_{5})$ is a subgroup of order $13 \cdot 31 \cdot 5$ in $G$; hence $G = P_{13} (P_{31} P_{5})$. Thus since $P_{13} \cap (P_{31} P_{5}) = \{ e\}$, $P_{13}$ is normal, and $G = P_{13} (P_{31} P_{5})$, we have that $G = P_{13} \rtimes_{\theta} P_{31}P_{5}$. Now since $\operatorname{Aut}(P_{13}) \cong \mathbb{Z} /12 \mathbb{Z} $ and $|P_{31} P_{5}|$ is relatively prime to 12, $\theta : P_{31} P_{5} \rightarrow \operatorname{Aut}(P_{13})$ must be trivial. Hence $G = P_{13} \times P_{31} P_{5}$. Now I assumed since $P_{5}$ and $P_{31}$ are both cyclic there product would be as well, but I am guessing this is where the flaw in my proof is. This is further supported by the fact that there is two groups of order 155. How can I see that there are two possibilities for $P_{5}P_{31}$ and conclude that there are only two groups of order 2015?","['group-theory', 'sylow-theory', 'finite-groups', 'groups-enumeration']"
2865952,Doubt in quotient rule for derivatives.,"I'm reading Rudin, and in theorem 5.3 he states that: Suppose $f,g$ are real functions on $[a,b]$ that are differentiable at $x \in [a,b]$. Then $f/g$ is differentiable at $x$, provided that $g(x) \neq 0$ and $$\left(\frac{f}{g}\right)'(x) = \frac{g(x)f'(x) - g'(x)f(x)}{g^2(x)}$$ Following Rudin's convention, the domain of $f/g$ are those point $x$ of $[a,b]$ where $g(x) \neq 0$. However, Rudin's definition of differentiability only mentions the cases where $f$ is differentiable on an open or closed interval. How do I interpret/solve this? For completeness, here is the relevant definition: Let $f$ be defined (and real-valued) on $[a,b]$. For any $x \in [a,b]$, form the quotient $$\phi(t) = \frac{f(t) - f(x)}{t-x}\quad (a < t <b, t \neq x)$$ and define $$f'(x) = \lim_{t \to x} \phi(t)$$ Maybe I'm just worrying too much..","['derivatives', 'real-analysis']"
2865975,$E(X)>E(Y)$ always imply that $P[X>Y]>0$,"Prove or disprove- ""If $E(X)>E(Y)$ then $P[X>Y]>0$ What I attempted:- I am using contradiction. Suppose $E(X)>E(Y)$. We assume that $P[X>Y]=0$ Now, we have 
\begin{equation}
\begin{aligned}
&P[X>Y]=0\\
\Rightarrow & P[(X-Y)>0]=0 \\
\Rightarrow & P[Z>0]=0   \qquad \mbox{where}\quad Z=X-Y
\end{aligned}
\end{equation} 
The last equation imply that \begin{equation}
\begin{aligned}
& Z\le 0 \\
\Rightarrow & E(Z)\le 0 \\
\Rightarrow & E(X-Y)\le 0 \\
\Rightarrow & E(X)\le E(Y)\\
\end{aligned}
\end{equation}
which is a contradiction. Therefore we must have $P[X>Y]>0$","['probability', 'random-variables']"
2865976,Correspondence between flat connections and fundamental group representations,"Let $M$ be a manifold.
Two stackexchange posts state that there is a correspondence
$$
\{
(P,A):
P \text{ a $G$-bundle}, A \text{ flat connection}
\}
\leftrightarrow
\{
\text{morphisms }
f:\pi_1(M) \rightarrow G
\},
$$
namely Recovering a principal connection from its monodromy and Are vector bundles given by their monodromy? . I believe that the correspondence is given by
$$
(P,A)
\mapsto f,
$$
where $f([\gamma])=g$, where $g$ denotes the element such that $\gamma^*(0)g=\gamma^*(1)$.
Here $\gamma^*$ denotes a horizontal lift of $\gamma$.
This is well defined by flatness of $A$. If $\pi_1(M)$ is a free group, I understand why the correspondence is 1-to-1.
That is because to define a $G$-bundle on $M$ I need to specify precisely an element in $G$ for each loop in $\pi_1(M)$, that is the idea mentioned in the accepted answer of the question https://mathoverflow.net/questions/4138/why-are-local-systems-on-a-complex-analytic-space-equivalent-to-vector-bundles-w?noredirect=1&lq=1 . Why is this correspondence 1-to-1 in general?
  Can you point me to a reference which contains the proof?","['principal-bundles', 'fundamental-groups', 'holonomy', 'differential-geometry']"
2865979,Is there a scalar product s.t. the following list is orthogonal?,"Let $A_1=\begin{pmatrix}1&0\\0&0\end{pmatrix}\quad A_2=\begin{pmatrix}1&1\\0&0\end{pmatrix},\quad A_3=\begin{pmatrix}1&1\\1&0\end{pmatrix},\quad A_4=\begin{pmatrix}1&1\\1&1\end{pmatrix}$. Is there a scalar product s.t. $\|A_k\|=k$ for $k=1,2,3,4$ and $A_i\perp A_j$ for $i\neq j$ ? With $\langle A, B \rangle = \mbox{Tr}(A^\top B)$ we have that $\|A_i\|=i$, but unfortunately it's not orthogonal. So, how can we conclude?",['linear-algebra']
2866006,Reduction of Order Leads to Non-Elementary Integral,"If  $u_1=x+1$ is a solution of $$xu''-(x+1)u'+u=0$$ find another linearly independent solution using reduction of order. I let $u_2=(x+1)v(x)$ be the second solution. Hence
$$v''(x^2+x)-v'(x^2+1)=0$$ Let $w=v'$, so 
\begin{align}
\frac{dw}{dx}(x^2+x)-w(x^2+1)&=0 \\
\frac{dw}{dx}&=\frac{(x^2+x)^{-1}(x^2+1)}{w^{-1}} \\
\text{ln}(w)&=\int \frac{x^2+1}{x^2+x} \ dx \\
\text{ln}(w)&=\int 1-\frac{1}{x}+\frac{2}{x+1} \ dx \\
\text{ln}(w)&=x+\text{ln}\left(\frac{(x+1)^2}{x}\right)+C \\
w&=C_1\frac{e^x(x+1)^2}{x} \\
v&=C_1\int \frac{e^x(x+1)^2}{x} \ dx
\end{align}
Where $$C_1\int \frac{e^x}{x} \ dx$$ cannot be solved. How do I find $v$?","['integration', 'calculus', 'reduction-of-order-ode', 'ordinary-differential-equations']"
2866028,Taking partial derivatives of related variables,"As the title states, say I have a function $f(x,y)$ and I am given $y$ as a function $y(x)$. Say I take the partial derivative of $f$ with respect to $x$: apparently, I am just supposed to let $y$ be a constant as if it were unrelated to $x$, as I can see by googling or from other questions on this site. (For example, this question has an answer which says so, but with no explanation.) My question then, is why? The way I've learnt partial derivatives is always via a sort of geometric intuition: draw the surface described by $z=f(x,y)$ in $\mathbb{R}^3$, and to take $\partial f/\partial x$ is to draw a plane parallel to the $x$ and $z$ axes and orthogonal to the $y$ axis, and look at the slope of the curve along intersection of the surface of the plot and the plane. It seems then, that we can't treat $y$ as constant in the event that they are non-independant, since doing so would imply that some points on the $xy$-plane as in the previous visualisation would not be achievable. (For example, if $y=x^2$ and $f(x,y)=x+y$, it doesn't make sense to consider the point $(1,2,3)$ since it's not achievable.) I would think then that to take $\partial f/\partial x$ would require finding explicitly a function $g(x)$ so that $g(x)=f(x,y)$, then taking $\partial g/\partial x$, but apparently that is not the case. Taken to the extreme, say $x=y$ and we have, say, $f(x,y)=x^2+y^2$. According to what's supposed to be the case, 
$$ \frac{\partial f}{\partial x}(x,y) = 2x. $$
If we however first substitute $x=y$ we will get $4x$. Why is my reasoning wrong? And why is the correct reasoning, well, correct? (To be clear: I have no problems with taking partial derivatives if the variables are unrelated. I don't think this will be very useful, but for context, I encountered this in trying to apply the Euler-Lagrange equation in an applied math problem. I came into this issue as the equation required me to take the partial derivative of a function $\mathcal{L}(q,q',t)$ with respect to $q'$, where $q'=dq/dt$.)","['multivariable-calculus', 'calculus', 'functions']"
2866034,How can a non-mathematician intuitively understand the importance of algebraic varieties?,"In my splintered readings, I have come to understand that algebraic varieties/ideals and their investigations/extensions/unifications dominated a large part of 20th century mathematics. Many profound results were achieved and prized awards given out for discoveries in this field. But as an engineer it still escapes me why they are the quantities of central interest and how they fit into the big picture (i.e. how they provide connections between different fields of mathematics, which I think is why they are so widely investigated?). In my case I think the main issue is the vast terminology that one has to internalize before one can begin to understand even basic results. A Wikipedia reading inevitably turns to multi-hour link-fest. To be more precise, my interest as an engineer arises specifically in their connection to dynamical systems theory. An algebraic approach to dynamical systems has been sporadically attempted since the 60s; and I think was initiated by Kalman in his study of dynamical systems over rings. Recently, far more general approaches have also been adopted incorporating category theory, sheaves etc.  For example here and here . So I am trying to find a coherent picture of their importance to (1) modern mathematics, (2) ODE's and differential geometry (3) systems theory. Understandably, the question might be too wide to answer in a single answer, so multiple answers are invited. As for an idea of what I am looking for see this Quora answer. The answer beautifully breaks down what is probably a one line rejoinder for a mathematician into something even a sufficiently advanced high school student can understand (but the answer doesn't have to be that simple or long, intuition is more important). I am not opposed to taking courses in Abstract Algebra to fully understand them, but from an engineering stand-point, a motivation for that type of commitment is hard to bring about unless I first get a big-picture idea of why/if it will be useful.","['abstract-algebra', 'soft-question', 'dynamical-systems']"
2866038,Given a matrix $A$ find $A^n$.,"$A=$$
    \begin{bmatrix}
    1 & 2\\ 
    0 & 1  
    \end{bmatrix} $ Find $A^n$. My input: $A^2= \begin{bmatrix}
    1 & 2\\ 
    0 & 1  
    \end{bmatrix} 
\begin{bmatrix}
    1 & 2\\ 
    0 & 1  
    \end{bmatrix}  = \begin{bmatrix}
    1 & 4\\ 
    0 & 1  
    \end{bmatrix} $ $A^3 = \begin{bmatrix}
    1 & 6\\ 
    0 & 1  
    \end{bmatrix} $
...... $A^n = \begin{bmatrix}
    1 & 2n\\ 
    0 & 1  
    \end{bmatrix} $ This was very basic approach. I want to know if there is any other way a smart trick or something to solve this problem ?","['matrices', 'linear-algebra']"
2866062,Is codomain whatever we make? [duplicate],"This question already has answers here : Codomain of a function (5 answers) Closed 5 years ago . For example, if I say $f(x) = \ln \left\{ x \right\}$ where $ \{ \cdot \}$ denotes the fractional part function. Is there any way to know the codomain of this function? And Now if I define $f : \mathbb{R} \to \mathbb{R}$, now the codomain is $\mathbb{R} $. So is it safe to say, codomain could be anything we want so long as it contains range, if there isn't a codomain already given? So, $ \sin : \mathbb{R} \to [-1,1]$ is as correct as writing $\sin : \mathbb{R} \to \mathbb{R}$? So I take it that if domain and codomain aren't given, then I could also say Codomain $\equiv$ Range? EDIT : What I'm trying to ask is, if it's only a matter of codomain, then every function can be called surjective and conversely every function can be called into function? Which makes it all ambiguous. I have so many confusions with co-domain, but can anyone just explain me these for the time being? Help is appreciated :)",['functions']
2866072,Discussing $\frac{d}{d\theta}e^{i\theta}$ aka cis before complex derivatives and complex exponential,"A First Course in Complex Analysis by Matthias Beck, Gerald Marchesi, Dennis Pixton, and Lucas Sabalka Definition of $e^{i \theta}$ (or cis in other texts) About Prop 1.3f, how is it possible to discuss derivative of $e^{i \theta}$ before both defining derivatives of complex functions (Ch2) (including functions of a real variable I think!) and defining the complex exponential (Ch3)? In particular, the proof of Prop 1.3f seems to assume linearity of the derivatives of complex functions. There's even this exercise later on: Exer 1.6b I know how to do this with Ch3's definition of the complex exponential. I don't believe this is possible to do with only Ch1 even if we write $e^{\phi + i\phi} = e^{(i+1)\phi}$ .","['complex-analysis', 'derivatives', 'exponential-function', 'complex-numbers']"
2866077,Degrees of Freedom in Affine Transformation and Homogeneous Transformation,"I understand that a 2D Affine Transformation has 6 DOF and a 2D Homogeneous Transformation has 8 DOF. However, how can I identify what those independent paramters are? If we consider Euclidean Transformation, it has 3 DOF: rotation, translation in x and translation in y. 
\begin{bmatrix}C_\theta&-S_\theta&t_x\\S_\theta&C_\theta&t_y\\0&0&1\end{bmatrix}
If we consider Similarity transform, it has 4 DOF: rotation, translation in x, translation in y and scaling.
\begin{bmatrix}sC_\theta&-sS_\theta&t_x\\sS_\theta&sC_\theta&t_y\\0&0&1\end{bmatrix} 1) Similarily, what makes up the 6 DOF of Affine matrix and 8 DOF of Homogeneous matrix? 2) Unlike the Euclidean and Similarity Transformation, is there no fixed set of DOF? 3) Can they be any six (if we take Affine as example) of rotation, translation (in x, y), scaling (in x, y), shearing, reflection etc. depending on the application? 4) If so, given an Affine matrix, can we know what the DOF are without knowledge of application? Link1 says Affine transformation is a combination of  translation,  rotation, scale, aspect ratio and shear. Link2 says it consists of 2 rotations, 2 scaling and traslations (in x, y). Link3 indicates that it can be a combination of various different transformations. I am a little confused about the whole idea. Thanks in advance.","['projective-geometry', 'geometry', 'linear-transformations', 'transformation', 'affine-geometry']"
2866088,How to solve $\sqrt{49-x^2}-\sqrt{25-x^2}=3$?,"I recognize the two difference of squares: $49-x^2$ and $25-x^2$. I squared the equation to get:
${49-x^2}-2(\sqrt{(49-x^2)(25-x^2)})+{25-x^2}=9$ However, I can't quite figure out how to remove the root in the middle. Any help is appreciated.",['algebra-precalculus']
2866098,Query on converting a differential equation to Sturm-Liouville Form,"I have a query on Sturm-Liouville operators written in a textbook that I am currently using for my course on Mathematical Methods in Physics. In the book, I do agree that Sturm-Liouville equations take the form of $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ whereby  $r(x)=\frac{dp(x)}{dx}$ However, in the portion written in the book, it says that any $2^{\text{nd}}$ order differential equations in the form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ can be converted into Sturm-Liouville form by multiplying through by a suitable integrating factor, which is given by $F(x)=\exp\int^{x}{\frac{r(u)-p'(u)}{p(u)}du}$ to give the Sturm-Liouville (S-L) form $[F(x)p(x)y]'+F(x)q(x)y+\lambda F(x)\rho(x)y=0$ with a different but still non-negative weight function $F(x)\rho(x)$. My question now here is, why a different weight factor? If so, what is the point of writing $\rho(x)$ in the non-(S-L) form anyway? To my knowledge, I think that when we are solving an eigenvalue equation associated with a differential operator, it is always in the form of $Ly=\lambda y$, where $L$ refers to the differential operator. So I believe that the starting equation here should not be $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ but rather, $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda y = 0$ that is equivalent to solving an eigenvalue equation $Ly=\lambda y$ (apart from the negative sign in $\lambda$) Hence, the integrating factor $F(x)$ to convert a non-(S-L) differential operator $L$ in $Ly=\lambda y$ is the weight function as I feel that $F(x)Ly=\lambda F(x)y$ => $L'y=\lambda F(x)y$, where $L'$ is the new differential operator in S-L form. Indeed, if we compare with the initial form $p(x) \frac{d^2y}{dx^2} + r(x) \frac{dy}{dx} + q(x)y + \lambda \rho(x) y = 0$ equivalent to $Ly=\lambda \rho(x) y$, we should deduce that $\rho(x) = F(x)$, i.e. $F(x)$ is the weight function in this case right? Or am I misinterpreting the proposed form from the book that I am referring to? Should there always be a $\rho(x)$ in a general $2^{\text{nd}}$ order ODE when we are trying to solve the eigenvalues in the initial step?  Thank you. PS: Sorry for the long question! FYI: The book I am referring to is ""Mathematical Methods for Physics and Engineering"" by Riley, Hobson and Bence.","['hilbert-spaces', 'mathematical-physics', 'sturm-liouville', 'ordinary-differential-equations']"
2866168,"Playing 4 games, then playing until a loss - expected number of losses","Consider a game where you win with probability $p$. We play 5 such games, and if we win game number 5, we keep playing until we lose. a) Find the expected number of games played b) Find the expected number of games lost Considering the a) part of problem, I defined X to be a random variable that measures number of games, starting at 5 (since we are sure the game is played at least 5 times) and came up with $$E(X) = \frac{5-4p}{1-p}$$
which appears to be correct. I have no idea how to approach the second part of the problem. The official solution states that it is the expected number of games played times the probability of loss, i.e. $E(X)\cdot(1-p) = 5-4p$ with no explanation whatsoever. So any sort of intuition or explanation (or an alternative solution) would be greatly appreciated. $\bf{edit}$ I have arrived at E(X) by defining a discreet random variable with the following distribution
 $$X\sim \left( \begin{array}{ c c c }
 5 & 6 & 7 & ... & k \\ 
 1 - p & p(1-p) & p^2(1-p) & ...  & p^{k-5}(1-p)  
\end{array} \right)
$$
and applying the definition of expectation. $\bf{edit 2}$ I tried defining a random variable $Y$ which measures the number of losses in first four games. So for 0 losses we must win every game, so the probability is p^4, etc. I arrived at the following distribution:
$$X\sim \left( \begin{array}{ c c c }
 0 & 1 & 2 & 3 & 4 \\ 
 p^4 & p^3(1-p) & p^2(1-p)^2 & p(1-p)^3  & (1-p)^4  
\end{array} \right)
$$ However the expectation of this variable doesn't match the solution $5-4p$.","['expectation', 'probability']"
2866173,Number of surjective functions from a set with $m$ elements onto a set with $n$ elements,"I was trying to calculate the number of surjective (onto) functions from A to B. Let a set $A$ contain $m$ elements and another set $B$ contain $n$ element i.e. $$|A|=m, \quad |B|=n.$$
Now, if $n>m$, no. of onto functions is $0$. When $m \ge n$, since there should be no unrelated element in B, let us relate first n elements of a A to B,so that all elements of B gets related. Hence total possibility for first n elements of A(  which actually contain m elements ) is
$$n!$$
Now the remaining $m-n$ elements in $A$ can be related to any of the $n$ elements of $B$. Hence the total possibility of the remaining $m-n$ elements of $B$ is
$$n^{m-n}$$ Therefore total number of surjective function is$$n!*n^{m-n}$$ Is anything wrong in this calculation ?If its wrong ,can anyone suggest correct method with proof.","['permutations', 'combinations', 'functions', 'combinatorics']"
2866184,Intuition about turning a polynomial ring into a field,"Let's consider the polynomial ring $R=F[x]$ over a field $F$. Then by taking the quotient by a principal ideal $I=(f(x))$ generated by an irreducible polynomial $f(x)$, we obtain a field $R'=F[x]/(f(x))$. It's easy to see that $R'$ is indeed a field. Since the ideals of $R$ which contain $I$ are in bijective correspondence with the ideals of $R'$, we can conclude that $R'$ has only two ideals and is therefore a field (as $I$ is maximal in $R$ since $f(x)$ is irreducible). I wanted to ask, is there an intuitive way of understanding why taking the quotient by some ideal makes $F[x]$ into a field? I would ideally like some way of demonstrating that the existence of a nonzero polynomial equivalent to zero in $R'$ somehow allows us to describe an algorithm to calculate multiplicative inverses...","['abstract-algebra', 'polynomials']"
2866240,Give a Bijection between $\mathbb{R}\setminus\mathbb{Z}$ and $\mathbb{R}\setminus\mathbb{N}$ [duplicate],"This question already has an answer here : Proof there is a 1-1 correspondence between an uncountable set and itself minus a countable part of it (1 answer) Closed 5 years ago . Give a Bijection between $\mathbb{R}\setminus\mathbb{Z}$ and $\mathbb{R}\setminus\mathbb{N}$ I got a bijection between $\mathbb{N}$ and  $\mathbb{Z}$
Given by $\phi(1)=0$ $\phi(2)=-1$ $\phi(3)=1$ $\phi(4)=-2$ $\phi(5)=2$ $\phi(6)=-3$ But I cannot figure how to find a bijection between $\mathbb{R}\setminus\mathbb{Z}$ and $\mathbb{R}\setminus\mathbb{N}$","['elementary-set-theory', 'discrete-mathematics']"
2866274,Drawing balls without replacement until three of a color are drawn,"This problem came up in a game I was playing. It's surely a standard combinatorics problem, but I'm having trouble searching for an exact match. Suppose an urn contains $B$ blue balls, $R$ red balls, and $G$ green balls. Balls are drawn without replacement until you've drawn three balls of any one color (not necessarily consecutively). What's the probability of ending with three blue balls? My reasoning is as follows: the number of ways to stop after drawing three blue balls is equal to the number of ways of drawing exactly two blue and at most two red and green balls (in any order), times the number of blue balls ending the sequence. This suggests that the number of ways of ending with blue is $$E_B = \sum_{i=0}^2 \sum_{j=0}^2 (B-2)(2+i+j)!\binom{B}{2}\binom{R}{i}\binom{G}{j}$$ where the binomial terms select which balls are drawn and the factorial enumerates all permutations, for the sequence of balls excluding the last ball. The probability of ending with three blue balls is then
$$\frac{E_B}{E_B+E_R+E_G}$$
for $E_R$ and $E_G$ computed analogously. Is this approach correct? EDIT: Proposed recursive solutions to the problem (e.g. Blatter's answer below) are practical if one just cares about computing a number, but I'm particularly interested in an answer to this question that 1) explains why the above formula is incorrect, and 2) salvages the formula.",['combinatorics']
2866280,Definition of a primitive representation,"In this paper , a representation $\pi: G \to \operatorname{GL}(V)$  is said to be primitive if $\pi$ is irreducible and there exists no decomposition of $V$ as the direct sum of proper non-zero subspaces permuted by G. Could you please describe what 2. technically means? It would be helpful if you could write the condition as a formula. Right now, this just sounds like 1. where there is no proper non-zero subspace which is invariant under $G$. I thought being permuted by $G$ and being invariant under $G$ is the same. Thank you!","['abstract-algebra', 'representation-theory', 'terminology']"
2866313,Equivalent definitions of Lie bracket,"Wikipedia gives two definitions of the Lie bracket $[X, Y]$ of vector fields $X$ and $Y$ on a manifold $M$, which it claims are equivalent: Let $\Phi_t^Z$ be the flow associated with the vector field $Z$. At a point $x \in M$, $[X, Y]_x :=  \left.\frac{\mathrm{d}}{\mathrm{d} t}\right|_{t=0} (\mathrm{d}\Phi^X_{-t}) Y_{\Phi^X_t(x)}$. This is the definition that appears in the texts that I have seen. However, the article claims that the Lie bracket can also be defined as follows: $[X, Y]_x := \left.\frac12\frac{\mathrm{d}^2}{\mathrm{dt}^2}\right|_{t=0} (\Phi^Y_{-t} \circ \Phi^X_{-t} \circ \Phi^Y_{t} \circ \Phi^X_{t})(x) = \left.\frac{\mathrm{d}}{\mathrm{d} t}\right|_{t=0} (\Phi^Y_{-\sqrt{t}} \circ \Phi^X_{-\sqrt{t}} \circ \Phi^Y_{\sqrt{t}} \circ \Phi^X_{\sqrt{t}})(x)$. I find the latter definition quite intuitive in geometric terms. But I don't see how to show that the two are equivalent. Can someone point me in the right direction, or provide a reference? Thanks","['lie-algebras', 'vector-analysis', 'differential-geometry']"
2866321,Why is the isomorphism of Artin-Wedderburn a ring isomorphism?,"I'm somewhat confused on the following calculation in the proof of Artin-Wedderburn: let $R$ be a semisimple ring such that (since $R$ is f.g. over itself) we have $R \simeq \oplus_{i = 1}^r S_i^{n_i}$. Now, $$
R^{op} \stackrel{(1)}\simeq End_R(R) \simeq End_R(\oplus_{i = 1}^r S_i^{n_i}) \stackrel{(2)}\simeq \bigoplus_{i = i}^rEnd_R(S_i)^{n_i \times n_i} 
$$ Now, I know that $(1)$ is a ring isomorphism between $R^{op}$ and the left $R$-module morphisms from $R$ to istelf, but why is $(2)$ a ring isomorphism? I know we have $Z(R)$-module isomorphism between $Hom_R(M \oplus N,P)$ and $Hom_R(M,P) \oplus Hom_R(N,P)$, so $(2)$ could be stated as a module isomorphism, but however $(1)$ is a ring homomorphism so it would not make sense in this context.","['ring-theory', 'abstract-algebra', 'modules']"
2866331,A Classical Introduction to Modern Number Theory (by Ireland and Rosen) Vs Algebraic Number Theory (by Jarvis): Which should I use?,"I’m a freshman math major student and was looking for some recommendations on number theory books. Just to clarify, I was reading Abstract Algebra: An Introduction by Thomas Hungerford. Although it was my first abstract algebra book, it gave me some background in group and ring theory, modular arithmetic, fields, etc. I was undecided about two books: A Classical Introduction to Modern Number Theory by Ireland and Rosen, and Algebraic Number Theory by Jarvis, but I would like to use only one of them. At first glance, it seemed that the first one is way more rigorous than the second — specially because it says graduate textbooks, not undergraduate textbooks on it — but it turns out that a lot of people said that this is actually not true, and that the algebraic number theory book by Jarvis is more rigorous. Because of that, I would like to hear from you opinions on these two books, and I would really appreciate if you could list the prerequisites for each of them. Do I need to know anything besides the basics of abstract algebra to attack these books? Do I need to know some linear algebra, or some analysis or even calculus to handle it? Moreover, which would be more appropriate — in terms of material covered, clarity and rigor — considering that I have studied abstract algebra using Hungerford's book and that I have a basic (I mean, BASIC) background in elementary number theory? (*) Is there one of them that is more algebraic than the other? Does one of them presents analytic number theory and requires some background in analysis? I also accept recommendations regarding other books, though I'm really looking for opinions on those two books I mentioned. (*) I have never used any number theory book before, but Hungerford's book gave me some insights on elementary number theory, and I have attended some college classes in elementary number theory while on my high school senior year.","['number-theory', 'algebraic-number-theory', 'book-recommendation', 'elementary-number-theory']"
2866337,$\mathbb CP^n$ with a quadric removed is homeomorphic to $T(\mathbb RP^n)$.,"Let $V_f$ be the zero set of a quadratic $z_1^2+\dots +z_{n}^2$ in $\mathbb CP^{n}$. I would like to show that $P^{n}(\mathbb C) \setminus V_f$ is diffeomorphic to the total space of the tangent bundle $T (\mathbb RP^{n})$. Some of my observations: Let $\tilde{V_f}$ be the preimage of $V_f$ under the projection $S^{2n+1} \to \mathbb CP^n$. Rewriting the condition that letting $\mathbf z \in \mathbb C^{n+1}$ be a vector $z_n^2+\dots +z_0^2=0$ in real terms, we get that $(|x|^2-|y|^2)+2i \langle x, y \rangle=0$, so $|x|^2=|y|^2$ and the inner product is zero. However, since $|x|^2+|y|^2=1$ if it is to be on $S^{2n+1}$, we get that $|x|$ and $|y|$ are both $\frac{1}{\sqrt{2}}$. Hence, a diffeomorphism $\tilde{V_f} \to V_2(\mathbb R^{n+1})$ (where the codomain is the set of orthonormal $2$-frames in $\mathbb R^{n+1}$) is given by $(x,y) \mapsto (\sqrt{2}x,\sqrt{2}y)$. From this, we can conclude that $V_f \cong G_2(\mathbb R^{n+1})$. Is there a way to conclude from here?","['diffeomorphism', 'geometry', 'algebraic-topology']"
2866360,Selfish Trolley Car Debate,"So, I'm in disagreement with my boyfriend over the following scenario: given the trolley car problem (1 person on one track and 5 people on a second track, an out of control trolley car will kill the 5 unless you pull the lever to kill the 1), he knows that one of the people is me and the other 5 are strangers. Since I'm amazing, he selfishly wants to maximize the chance that I survive. His solution: he says he would pull the lever to save the five people. Since he doesn't know which one I am, he should save the most people to maximize the chance of saving me. My solution: I say I wish he would put more thought into saving me. Instead of assuming a uniform distribution on my position, he should assume a uniform distribution on all distributions. So, he should pull the lever with 5/6 probability and let it kill the 5 with 1/6 probability. So, what's the right answer? Is there even a right answer? Update People keep downvoting the correct answer, so I'll put it here. The correct answer is neither one of the suggestions. The assumption of no information about my placement means there is no prior information from which to draw a probability of survival. That means there is only one strategy where this probability is even defined: to pull the lever with probability $1/2$ . This trivially maximizes the survival probability ( $1/2$ ), since it is the only probability.",['probability']
2866397,"If $k\sin x\cos x=\sin(kx)$ is an identity for all $x$, then what is the smallest value of $k$?","If $k\sin x\cos x=\sin(kx)$ is an identity for all $x$, then what is the smallest value of $k$? I tried analyzing the domain and range of the sine and cosine functions, and how this identity came to be but I could not conceptualize this problem. Any help to understand this would be appreciated","['algebra-precalculus', 'trigonometry']"
2866420,Identity map from sup norm to $L^2$,"I'm struggling with the apparently simple task to show that the identity map $Id:(C[a,b],||\cdot||_\infty)\rightarrow (C[a,b],||\cdot||_2)$ is continuous. Taking $x\in C[a,b]$, I'm trying to find a $\delta$ so that: 
$$||x-y||_\infty = \max|x(t)-y(t)|<\delta \implies$$
$$||x-y||_2<\epsilon \iff \sqrt{\int_a^b(x(t)-y(t))^2dt}<\epsilon$$
Expanding: 
$$\sqrt{\int_a^b x^2(t)dt-2\int_a^b x(t)y(t)dt+\int_a^b y^2(t)dt}<\epsilon$$
But then I'm stuck... My idea: $\max|x(t)-y(t)|<\delta \implies \int |x-y|<\delta(b-a)\implies\int x<\delta(b-a)+\int y$ But how does that relate to $\int x^2$?","['normed-spaces', 'functional-analysis', 'real-analysis']"
2866431,Proof of Total Probability Theorem for Conditional Probability,"The law of total probability states: Let $\left({\Omega, \Sigma, \Pr}\right)$ be a probability space. Let $\left\{{B_1, B_2, \ldots}\right\}$ be a partition of $\Omega$ such that $\forall i: \Pr \left({B_i}\right) > 0$ . Then: $\displaystyle \forall A \in \Sigma: \Pr \left({A}\right) = \sum_i \Pr \left({A \mid B_i}\right) \Pr \left({B_i}\right)$ I want to prove that this is true also for conditional probabilities. So basically I want to prove the following: Let $\left({\Omega, \Sigma, \Pr}\right)$ be a probability space. Let $\left\{{B_1, B_2, \ldots}\right\}$ be a partition of $\Omega$ such that $\forall i: \Pr \left({B_i}\right) > 0$ . Then: $\displaystyle \forall A, C \in \Sigma: \Pr \left({A \mid C}\right) = \sum_i \Pr \left({A \mid C \cap B_i}\right) \Pr \left({B_i}\right)$ This is how I attempted it: $$Pr(A\mid C) = Pr(A|C\cap \Omega) = Pr(A\mid C\cap\left(\bigcup_iB_i\right))$$ because it is a partition. Then, using the fact that intersection distributes over union I got: $$Pr(A\mid C\cap\left(\bigcup_iB_i\right)) = Pr(A \mid \bigcup_i\left(C\cap B_i\right))$$ I can't go any further. I know that in a probability space we have that the probability measure $Pr$ is countably additive. I know that if we have a probability space $(\Omega, \Sigma, \Pr)$ then the triplet $(\Omega, \Sigma, Qr)$ with $$Qr: Qr(A) := Pr(A | C)$$ is a probability space as well. But I have no idea how to use these two information to finish the proof.","['probability-theory', 'probability']"
2866446,How can schemes see two points associated with $y = x^2$,I have read somewhere that one motivation of scheme theory is that using methods developed in scheme theory we can detect that the point associated to the intersection of $y = 0 \cap y = x^2$ has multiplicity two. Can someone explain this in details to me. That would be very helpful.,"['algebraic-geometry', 'schemes']"
2866473,"Whilst There Are Three Characteristic Equations, Only Two of Them Are Linearly Independent?","Take the general quasi-linear equation $$a(x, y, u)u_x + b(x, y, u)u_y - c(x, y, u) = 0. \tag{1}$$ We assume that there exists a solution of the form $u = u(x, y)$ . We can define a solution surface in $(x, y, u)$ space via the implicit form of the solution $$f(x, y, u) \equiv u(x, y) - u = 0.\tag{2} $$ The normal vector to the solution surface is $$\nabla f = (f_x, f_y, f_u) = (u_x, u_y, -1).\tag{3}$$ We can therefore write the PDE as the dot product of two vectors $$au_x + bu_y - c = (a, b, c) \cdot (u_x, u_y, -1) = 0.\tag{4}$$ This shows that the vector $(a(x, y, u), b(x, y, u), c(x, y, u))$ is a tangent vector to the solution surface at the point $(x, y, u)$ . We can construct a curve in $(x, y, u)$ space such that the tangent of the curve is equal to the vector $(a, b, c)$ at every point. Such a curve is called a characteristic curve . A parameterisation of this curve is given by the equations $x = x(t), y = y(t), u = u(t)$ : $$\left( \frac{dx}{dt}, \frac{dy}{dt}, \frac{du}{dt} \right) = (a, b, c)\tag{5} $$ So we are left with a system of ODEs called the characteristic equations : $$\frac{dx}{dt} = a(x, y, u), \frac{dy}{dt} = b(x, y, u), \frac{du}{dt} = c(x, y, u). \tag{6}$$ I am told that, whilst there are three characteristic equations, only two of them are linearly independent. This implies that their solution consists of a two-parameter family of curves in $(x, y, u)$ space. Why are only two of them linearly independent? And why does this imply that their solution consists of a two-parameter family of curves in $(x, y, u)$ space? I'm trying to understand this for my upcoming lecture on method of characteristics. Help is much appreciated. 8-) EDIT: I found the following information in chapter 4.1 of the textbook Essential Partial Differential Equations , by Griffiths, Dold, and Silvester: We first consider the very special case of (4.1) with $a = b = c = r = 0$ , that is $$pu_x + qu_y = f \ \ \ \text{(4.2)}$$ In many physical applications, see (pde.1), one of the independent variables might represent a time-like variable. In stationary applications both variables might be spatial variables. We begin by considering a curve defined by the height of the surface $z = u(x, y)$ in three dimensions above a path $(x(t), y(t))$ in the $x-y$ plane that is parameterised by $t$ . This curve has slope $$\frac{du}{dt}(x(t), y(t))$$ which, by the chain rule, is given by $$\frac{du}{dt}(x(t), y(t)) = u_x \frac{dx}{dt} + u_y \frac{dy}{dt} \ \ \ (4.3)$$ Thus, by choosing the parameterization such that $$\frac{dx}{dt} = p, \frac{dy}{dt} = q, \ \ \ (4.4)$$ the PDE (4.2) reduces to the ODE $$\frac{du}{dt} = f \ \ \ (4.5)$$ The parameter $t$ is not an intrinsic part of the system and can be avoided by writing the three ODEs in (4.4) and (4.5) in the generic form $$\frac{dx}{p} = \frac{dy}{q} = \frac{du}{f} \ \ \ (4.6)$$ Paths in the $x-y$ plane described by (4.4) are known as characteristic curves or, simply, as characteristics, and equations (4.6) are known as the characteristic equations fo (4.2). The relations (4.6) define three equations, of which any two are independent. P.S.: Bolding in textbook quotation is my own. EDIT 2: Ok, I just realised something. I think this might have something to do with my question: (From the chapter and textbook described in the first edit.) Example 4.1 Find the general solution of the PDE $pu_x + u_y = u$ , where p is constant. Show that the problem is well posed when solved in the infinite strip { (x, y) : x \in \mathbb{R}, 0 \le y \le Y } and an initial condition $u(x, 0) = g(x), x \in \mathbb{R}$ , is applied, where $g$ is a continuous bounded function. The characteristic equations are $$\frac{dx}{p} = \frac{dy}{1} = \frac{du}{u}$$ which we may write as $$\frac{dx}{dy} = p, \frac{du}{dy} = u$$ [...] See how we got the two ODEs $\frac{dx}{dy} = p, \frac{du}{dy} = u$ from the equation $\frac{dx}{p} = \frac{dy}{1} = \frac{du}{u}$ , which has three terms? See how it seems that we cannot get another ODE? If I do the necessary manipulations of $\frac{dx}{dy} = p, \frac{du}{dy} = u$ to get $\frac{dx}{dy} = p$ and $\frac{du}{dy} = u$ , if we then attempt to do further manipulations to get a third ODE, then there must be some dependency in the third ODE we get and one of the other two ODEs. I'm not sure if I'm onto something here, but it's something that I just thought of.","['surfaces', 'linear-algebra', 'vector-analysis', 'partial-differential-equations', 'characteristics']"
2866485,How does linear algebra over the octonions and other division algebras work?,"An interesting question, which has been discussed in many forms on this site, is how many results from the study of linear algebra over vector spaces carries over when we allow the scalars to form an algebraic structure more general than a field. For example, if you stop requiring multiplicative commutativity and allow the scalars to form an arbitrary division ring, then a surprising amount of structure carries over unchanged, as discussed here : you lose the notion of the determinant and eigenvalues become more subtle, but you still have unique basis cardinality, Gaussian elimination, the Rouché-Capelli theorem, and matrix representations of arbitrarily linear maps between finite-dimensional modules. A similar story applies if you allow the scalars to form an arbitrary commutative ring. However, all hell can break loose if you drop commutativity and division: a module over an arbitrary ring does not necessary have an invariant basis number, so much of the structure of linear algebra over fields immediately falls apart. What happens if you further generalize the scalars' division ring to be a division algebra that is not associative? How much structure do you lose if you give up associativity but preserve division? To be concrete, consider generalized modules with the octonions as their ""scalars"". (A precise definition of what I mean is given in this question , together with the requirement $(rs)\cdot x = r \cdot (s \cdot x)$.) Do these modules have a notion of invariant basis number? If so, can arbitrary linear maps between finitely generated such generalized modules be represented by matrices? Are the matrix elements given by the usual formula $A(\hat{e}_j) = \sum_i A_{ij} \hat{e}_i$?","['octonions', 'modules', 'abstract-algebra', 'linear-algebra', 'division-algebras']"
2866558,Under what conditions does $H(X\mid f(Y))=H(X\mid Y)$?,"I have the problem that I cannot solve:
Under what conditions does $H(X∣f(Y))=H(X∣Y)$?
I would like to draw a result about the relation between $p_X(\cdot | g(Y))$ and $p_X(\cdot | Y)$. Are they equal? This is an exercise in the textbook. There is a solution, but I don't think it's correct (more precisely, it is not satisfactory enough). The provided solution is as below. Suggested Solution (not satisfactory). If $H(X|g(Y )) = H(X|Y )$, then $H(X)−H(X|g(Y )) = H(X) − H(X|Y )$, i.e., $I(X; g(Y )) = I(X; Y )$. This is the condition for equality in the data processing inequality. From the derivation of the inequality, we have equality iff $X → g(Y ) → Y$ forms a Markov chain. Hence $H(X|g(Y )) = H(X|Y )$ iff $X → g(Y ) → Y$ . This condition includes many special cases, such as $g$ being one-to-one, and $X$ and $Y$ being independent. However, these two special cases do not exhaust all the possibilities.","['statistics', 'markov-chains', 'entropy', 'probability']"
2866567,Finding the $n$-th derivative of $a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$,"Let $f(x)= a_nx^n+a_{n-1}x^{n-1}+\cdots+a_1x+a_0$.
  I am trying to find $f^n(x)$. By applying the power rule $n$ times, I get this
$$f^n(x)=a_{n}(n\cdot n)x^{n-n}+\cdots+ a_1$$
which I think can be simplified to
$$f^n(x)=a_{n}n^2+\cdots+ a_1$$ However, I don't think I have the correct answer, as my exercise book is telling me the answer is.
$$a_n\,n\cdot(n-1)\cdot\,\cdots\,\cdot 1$$ What did I do wrong? I'm under the impression I have done multiple mistakes.",['derivatives']
2866602,Possible mistake finding the maximum volume of a box with the AM-GM inequality?,"I have found the following problem: What is the box (without a top) of largest volume which can be constructed from
  a square piece of tin of edge length $2a$ by cutting a square from each corner
  and folding up the edges I have tried to solve it with the AM-GM inequality: I have the sides $(2a-2h), (2a-2h),h$ and then: $$\frac{(2a-2h)+ (2a-2h) + h}{3}\geq \sqrt[3]{(2a-2h)^2h}$$ Equality holds when $(2a-2h)=(2a-2h)=h$, when I try to solve, I find: $$h=\frac{2a}{3}$$ When I try to do the same with derivatives, I find that the roots of the derivative of $(2a-2h)^2h$ are $a$ and $\frac{a}{3}$. I may be doing something extremely silly but I can't figure out what is wrong.","['optimization', 'inequality', 'derivatives']"
2866629,Show that $\int_0^1 4 \space\operatorname{li}(x)^3 \space (x-1) \space x^{-3} dx = \zeta(3) $,"My mentor tommy1729 wrote $\int_0^1 4 \space \operatorname{li}(x)^3 \space (x-1) \space x^{-3} dx = \zeta(3) $ I wanted to prove it thus I looked at some methods for computing integrals and also representations of $\zeta(3)$ that might be useful. But nothing was very helpful to me. In particular the fact that the RHS is so short - Just Apery’s constant - was surprising.
I expected it longer and more complicated. 
So I tend to believe that either the integral computation requires many steps and Then Finally we Get a long expression but alot of cancellation until we are left with Apery’s constant only.
Or There is a simple way to get Apery’s constant directly with a trick I missed. In either case it is amazing I would say. So How to show that $$\int_0^1 4 \space \operatorname{li}(x)^3 \space (x-1) \space x^{-3} dx = \zeta(3) $$ I would like to see different ways to show it.
I assume real analysis methods are simpler than complex analysis methods ( on the complex plane like contour integration ). I Also wondered If not knowing the RHS in advance would change the difficulty of this question. Also I wonder about $$ \int_0^1 5 \space \operatorname{li}(x)^4 \space (x-1) \space x^{-4} dx = ?? $$","['conjectures', 'definite-integrals', 'special-functions', 'calculus', 'riemann-zeta']"
2866642,Proving $\lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} = \frac{1}{2}$,"Prove that $$ \lim_{x \to 0+} \sum_{n=0}^\infty \frac{(-1)^n}{n!^x} =
 \frac{1}{2}. $$ We know that $$ \sum_{n=0}^\infty \frac{(-1)^n}{n!^x}$$ converges for any $x>0$. So I try to evaluate the limit as $x$ approaches $0$ numerically. It seems that the limit approaches $\displaystyle \frac{1}{2}$. I know that $$\sum_{n=0}^\infty \frac{(-1)^n}{n!} = \frac{1}{e}.$$ Does it help to solve this problem?","['limits', 'calculus', 'summation']"
2866649,"Expected area ""sweeped"" on an infinite Minesweeper board","Given an average density (x/y) of x mines in y squares, is it possible to calculate the expected number of squares you can ""sweep"" (i.e. identify whether there is a mine or not) on an infinitely sized minesweeper game, before you have to guess- i.e. no more squares can be deduced.","['recreational-mathematics', 'probability']"
2866663,"$[1+(\frac{1+i}{2})][1+(\frac{1+i}{2})^2][1+(\frac{1+i}{2})^{2^2}]...[1+(\frac{1+i}{2})^{2^n}]=(1-\frac{1}{2^{2^n}})(1+i)$,where $n\ge 2$",Show that $$\!\!\!\left[1+\left(\frac{1+i}{2}\right)\right]\!\!\!\left[1+\left(\frac{1+i}{2}\right)^2\right]\!\!\!\left[1+\left(\frac{1+i}{2}\right)^{2^2}\right]\cdots\left[1+\left(\frac{1+i}{2}\right)^{2^n}\right]\!\!\!=\left(1-\frac{1}{2^{2^n}}\right)(1+i)$$ for $n\ge 2$. I took $\frac{1+i}{2}=\frac{1}{\sqrt2}e^{i\frac{\pi}{4}}$ and tried solving but i could not reach the RHS.Please help.,"['algebra-precalculus', 'complex-numbers']"
2866671,About the kernel of the structure map of a morphism of schemes,"Let $f:X\to Y$ be a morphism of schemes, let $\mathcal{K}$ be the kernel of the structure map $\mathcal{O}_Y\to f_*\mathcal{O}_X$. Do we have $$\mathrm{Supp}(\mathcal{O}_Y/\mathcal{K})=\overline{f(X)}?$$ Recall that the support of a sheaf (of abelian groups) is the set of points at which the stalk of that sheaf is nonzero. I can prove $\subset$ as follows: If $y\in Y$ is in the support, then $(f_*\mathcal{O}_X)_y\neq 0$ (since the image of $1$ is $1$), so for any open neighborhood $V$ of $y$, we have $\mathcal{O}_X(f^{-1}V)\neq 0$, so $f^{-1}V\neq \emptyset$, thus $y\in \overline{f(X)}$. How about the convese? As $\mathcal{O}_Y/\mathcal{K}$ is of finite type, the support is closed, so one possible way is to show $f(X)\subset\mathrm{Supp}(\mathcal{O}_Y/\mathcal{K})$. Somebody told me it's always true but you can add some mild conditions if you need.","['algebraic-geometry', 'schemes', 'sheaf-theory']"
2866672,Riemann integral on a single point,"Let $f$ be a continuous function over $\mathbb{R}$. Let $c\in(-\infty,\infty)$. Then, it is believed that $\int_{c}^{c}f(\xi)d\xi=0$ in the sense of Riemann integration. Is this just a definition? But the definition of the Riemann integral on wikipedia or on text book always start with a closed interval [a,b] with $b>a$ and then partition the interval. So, the definition of Riemann integral does not cover the case $a=b$. So, $\int_{c}^{c}f(\xi)d\xi=0$ seems to be a property of a Riemann integral inferred by its definition. Can anyone tell me if $\int_{c}^{c}f(\xi)d\xi=0$ is a definition or not and show me some proof?","['integration', 'calculus', 'riemann-integration']"
2866731,Integration of $\frac{1}{x^2-a^2}$ by trigonometric substitution?,"$$\int \frac{1}{x^2-a^2}dx$$ Now, I know this can be done by splitting the function into two integrable functions, $\displaystyle\dfrac{1}{2a}\int \bigg(\dfrac{1}{x-a} - \dfrac{1}{x+a}\bigg)dx$ And then doing the usual stuff. My question is, how can we do this by using trigonometric substitution? The only thing that gets in my mind is $x=a\sec\theta$, but then got stuck on proceeding further. Any help would be appreciated.","['integration', 'indefinite-integrals', 'trigonometry']"
2866764,Random variable with exponential distribution.,"Let $X$ be random variable with exponential distribution $\mathcal{E}(2)$ and let $Y$ be another random variable such that $$Y=\max\left(X^2, \frac{X+1}{2} \right).$$
Find the distribution for random variable $Y$. Distribution for $X$ is $f_X(x)= 2e^{-2x}, x>0$ and zero otherwise. Now, for variable $Y$ we have that it's distribution is zero whenever $y \leq \frac{1}{4}$ For $y=t> \frac{1}{2}$ we have the following: $F_Y(t)=\int_0^{2t-1} f_X(x)dx= 1- e^{2-4t}$ Similarly, for $y=t>1$ we have $F_Y(t)=\int_0^{\sqrt{t}} f_X(x)dx= 1- e^{-2\sqrt{t}}$ But, i cannot understand what happens in case that $y$ takes random value on interval $(\frac{1}{4}, \frac{1}{2})$. It's the black line on the graph. How can i handle situations like this? Any help appreciated!","['probability-distributions', 'exponential-distribution', 'probability']"
2866769,Why are asymptotic expansions not common for solving partial and algebraic differential equations?,"There are many different numerical methods for solving partial and algebraic differential equations. The common commercial and open source packages (e.g. Elmer and OpenFOAM) use discretization/meshing based methods such as finite difference or finite volume. When I was in undergrad we used to use power series for this matter all the time. They have several advantages over other methods: The result is easily differentiable/integratable They seem to be computationally less expensive (the solution can be found analytically/symbolically) Complicated boundary and initial conditions can be applied easily (like moving objects) No meshing is required solutions are reusable, only the boundary/initial conditions need to be applied However, It is a surprise that there are not many proprietary or open source software implementing this algorithms. At least I haven't seen much. The only thing I have seen so far is the Mathematica's AsymptoticDSolveValue function, which is only for ODEs. Now my question is why asymptotic methods are not as common for solving nonlinear differential equations. Are there any scientific studies showing they are less efficient than common methods? Maybe there are some products and I am not aware of them. If that's the case I would appreciate if you could let me know. P.S.1. There are other series too: Chebyshev polynomials Padé rational functions approximant Lagrange interpolation Laurent series Fourier series to mention some. P.S.2. Other relevant topics: Frobenius method , WKB approximation , Spectral method , Finite element method P.S.3. I asked a follow-up question here and here .","['numerical-methods', 'asymptotics', 'ordinary-differential-equations', 'partial-differential-equations']"
2866804,"Mathematical bases for which $q$ and $\dot{q}$ could be treated as independent variable in $L(q,\dot{q},t)$","Mathematical bases for which $q$ and $\dot{q}$ could be treated as independent variable in $L(q,\dot{q},t)$. In Lagrangian mechanics with single degree of freedome $q(t)$ and it's first degree deritative $\dot{q}(t)$ were considered as independent variable. $\displaystyle \frac{\partial \dot{q}}{\partial q}=\frac{\partial q}{\partial t\partial q}=\frac{\partial q}{\partial q\partial t}=\frac{\partial C}{\partial t}=0$ and its general form $\displaystyle \frac{\partial q^{(n)}}{\partial q}=0$ could be easily proven. However, $\displaystyle \frac{\partial q}{\partial \dot{q}}=0$ was not so straight forward. My question was that: Prove the general form $\displaystyle \frac{\partial q}{\partial q^{(n)}}=0$ in Lagrangian equation $L(q,\dot{q},t)$ where $\displaystyle\frac{\partial L}{\partial q}=\frac{d}{dt}(\frac{\partial L}{\partial \dot{q}})$ and $q^{(n)}$ was the $n$ th deritative of $q$.","['calculus', 'analysis', 'euler-lagrange-equation']"
2866812,Is the composition of a Sobolev function and a smooth function Sobolev?,"Let $\Omega \subseteq \mathbb{R}^n$ be an open bounded domain, and let $1<p<n$.  Suppose that $f \in W^{1,p}(\Omega)$ is continuous*, and $g \in C^{\infty}(\mathbb{R})$. Is it true that $g \circ f \in W^{1,p}_{loc}(\Omega)$? My guess was that the answer is positive, and that $\partial_i (g \circ f)(x)=g'(f(x)) \partial_i f(x)$ but a naive calculation to prove it failed. *Note that the continuity of $f$ does not follow from $f \in W^{1,p}(\Omega)$, since $p<n$; this is an additional assumption I am adding.","['regularity-theory-of-pdes', 'sobolev-spaces', 'weak-derivatives', 'real-analysis']"
2866872,Probabilities maximizing products,"Given is an expression of the form $P=P_1\times P_2\times\dots\times P_n$, where each $P_i$ is a sum of some distinct elements from $\{x_1,x_2,\dots,x_k\}$. (For example, $P=x_1(x_1+x_2)(x_1+x_3)$.) We want to maximize this expression subject to the constraints $x_i\geq 0$ for all $i$, and $\sum_{i=1}^k x_i=1$. Let $A$ be the value of $P_1$ at the maximum. Let $B$ be the value of $P_1$ at the maximum if we instead maximize the expression $P'=P_2\times P_3\times\dots\times P_n$, subject to the same constraints. Is it true that $A\geq \frac{n-1}{n}B+\frac{1}{n}$?","['optimization', 'inequality', 'probability']"
2866894,Natural isomorphism to dual space of an inner product space in complex case.,"$k$ is a field and $V$ is a finite dimensional $k$ vector space that has an inner product $\langle - , - \rangle$. If $k = \mathbb{R}$, there is a natural isomorphism $\phi \colon V \rightarrow V^*$, $v \mapsto \langle v, - \rangle$. However, if $k = \mathbb{C}$, $\langle v, - \rangle$ is not linear because $\langle v, cx \rangle = \overline{c} \langle v, x \rangle$. $\langle -, v \rangle$ is linear but in this case  $V \rightarrow V^*$, $v \mapsto \langle -, v \rangle$ is not linear. Are there no natural isomorphism between $V$ and $V^*$ when $k=\mathbb{C}$?",['linear-algebra']
2866897,"Why is $\sin : \mathbb{R} \to [-5,5] $ different from $\sin : \mathbb{R} \to \mathbb{R}$? [duplicate]","This question already has answers here : How is the codomain for a function defined? (6 answers) Functions with different codomain the same according to my book? (4 answers) Closed 5 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved My teacher says these two functions are different, why though? $$\sin : \mathbb{R} \to [-5,5] \tag{1}$$ $$ \sin : \mathbb{R} \to \mathbb{R} \tag{2} $$ Both have the same domain and range. What difference does changing the codomain make here, so long as I keep the codomain as a superset of the range? More generally speaking, $f : A \to B $ and $f: A \to C$ where $B$ and $C$ are the codomain of the same function $f$ and are supersets of range of $ f$ What difference would that make? How would changing the codomain (in this case) mean the functions are different? Isn't the function $f$ the same?",['functions']
2866916,Average time waiting for bus,"There are two buses: first arrives to the bus top every 8 minutes, and second arrives every 12 minutes. What is the average waiting time on a bus stop if we take whatever came first?
It is expected that buses arrive at regular intervals, so first arrives every 8 minutes and second arrives every 12 minutes, but it is unknown when did they started. I was thinking in the following way: there is 1/3 probability that second bus arrives at 8-12 minutes, so it is after the first one. In this case probability is 1/3 * 4(average waiting time for the first bus). Then we have 2/3 probability that second bus came at 0-7 minutes, and basically we have two buses that arrive every 8 minutes. In this case I estimated average waiting time as 2 minutes(4 min average waiting time and 2 minutes because there are two 8 minutes buses now) and in this case answer is 2 * 2/3 + 4 * 1/3 = 8/3 = 2 + 2/3.
But I'm not sure that in case of two buses there is actually 2 minutes waiting time and not some other number, and also I think that this is a ""standard"" problem that should have standard way of solving. Please guide me to the correct answer.",['probability']
2866973,Real-valued function $f : \mathbb{R} \to \overline{\mathbb{R}}$ is continuous iff its graph is closed,"I'm considering functions  $f : \mathbb{R} \to \mathbb{R}$ with the codomain extended to the extended reals $\mathbb{R} \cup \{\pm\infty\}.$ Now if $f$ is continuous then the function $g : \mathbb{R} \times \overline{\mathbb{R}} \to \mathbb{R} : (x, y) \mapsto y - f(x)$ is also continuous. Then the graph of $f$ is the set $g^{-1}(\{0\})$ and is therefore closed. It seems plausible that the converse is true - that is, that if the graph of $f : \mathbb{R} \to \overline{\mathbb{R}}$ is closed then $f$ is continuous - but I wouldn't be too surprised if there's an annoying counterexample. Is the converse true?","['continuity', 'general-topology']"
2867022,Derivation of derivative of multivariate Gaussian w.r.t. covariance matrix,"I'm reading a paper, probabilistic CCA , in which the authors state derivatives without showing derivations. I would like step-by-step derivations to convince myself. Consider a $d$-dimensional multivariate Gaussian random variable: $$
\textbf{x} \sim \mathcal{N}(\boldsymbol{\mu}, \Sigma)
$$ In probabilistic CCA, we define $\Sigma = W W^{\top} + \Psi$, where $W \in \mathbb{R}^{d \times q}$ and $\Psi \in \mathbb{R}^{d \times d}$. I'd like to compute the derivative w.r.t. $\boldsymbol{\mu}$, $W$, and $\Psi$ for the negative log-likelihood. The stationary point for $\boldsymbol{\mu}$ is just the empirical mean (shown below*) or $\hat{\boldsymbol{\mu}}$. Plugging in the minimum for the parameter $\boldsymbol{\mu}$ into the negative log-likelihood, we get: $$
\frac{\partial \mathcal{L}}{\partial W}
=
\frac{\partial}{\partial W} \Big\{
\overbrace{
    \frac{1}{2} \sum_{i=1}^{n}(\textbf{x}_i - \hat{\boldsymbol{\mu}})^{\top} \Sigma^{-1} (\textbf{x}_i - \hat{\boldsymbol{\mu}})
}^{A}
+
\overbrace{\frac{n}{2} \ln |\Sigma|}^{B} + \overbrace{\text{const}}^{C}
\Big\}
$$ Clearly, $C = 0$. But I'm not sure how to handle $A$ and $B$, particularly since $\Sigma = W W^{\top} + \Psi$. *Derivative w.r.t. $\boldsymbol{\mu}$ The negative log-likelihood is: $$
\mathcal{L}
=
\frac{1}{2} \sum_{i=1}^{n}(\textbf{x}_i - \boldsymbol{\mu})^{\top} \Sigma^{-1} (\textbf{x}_i - \boldsymbol{\mu}) + \frac{n}{2} \ln |\Sigma| + \text{const}
$$ The derivative of the two rightmost terms with respect to $\boldsymbol{\mu}$ is $0$, meaning we just need to compute: $$
\frac{\partial}{\partial \boldsymbol{\mu}}
\Big\{
\frac{1}{2} \sum_{i=1}^{n}(\textbf{x}_i - \boldsymbol{\mu})^{\top} \Sigma^{-1} (\textbf{x}_i - \boldsymbol{\mu})
\Big\}
=
0
$$ By the linearity of differentiation, we have: $$
\frac{1}{2}
\sum_{i=1}^{n}
\frac{\partial}{\partial \boldsymbol{\mu}}
\Big\{
(\textbf{x}_i - \boldsymbol{\mu})^{\top} \Sigma^{-1} (\textbf{x}_i - \boldsymbol{\mu})
\Big\}
=
0
$$ Using Equation ($86$) from the Matrix Cookbox , we get: $$
\frac{1}{2}
\sum_{i=1}^{n}
\Big\{
-2 \Sigma^{-1} (\textbf{x}_i - \boldsymbol{\mu})
\Big\}
=
0
$$ Finally, solve for $\boldsymbol{\mu}$, we get: $$
\begin{align}
0
&= \frac{1}{2} \sum_{i=1}^{n} \Big\{ -2 \Sigma^{-1} (\textbf{x}_i - \boldsymbol{\mu}) \Big\}
\\
&= - \sum_{i=1}^{n} \Big\{ \Sigma^{-1} \textbf{x}_i - \Sigma^{-1} \boldsymbol{\mu} \Big\}
\\
&= - \sum_{i=1}^{n} \Big\{ \Sigma^{-1} \textbf{x}_i \Big\} + n \Sigma^{-1} \boldsymbol{\mu}
\\
- n \Sigma^{-1} \boldsymbol{\mu} &= - \Sigma^{-1} \sum_{i=1}^{n} \textbf{x}_i
\\
\boldsymbol{\mu} &= \frac{1}{n} \sum_{i=1}^{n} \textbf{x}_i
\end{align}
$$ And we're done.","['partial-derivative', 'statistics', 'matrix-calculus', 'derivatives']"
2867066,Is this $\sigma$-algebra generated by the canonical projections?,"I came across the following question to which I could not figure out a straightforward answer: Let $H$ be a separable, (in general infinite-dimensional) real Hilbert space and $V$ a separable Banach space such that $V \subseteq H$ continuously and densely. $\mathbb{B}$ is defined as the following space of paths: $$\mathbb{B} := \{y \in C(\mathbb{R}_+,H)|\int_{0}^{T}||y(s)||_V\text{d}s < \infty \,\,\forall \,\,T >0\}.$$ One defines a metric $\rho$ on $\mathbb{B}$ through
$$\rho(\omega_1,\omega_2) := \sum_{k=1}^{\infty}2^{-k}\bigg[\bigg(\int_{0}^{k}||\omega_2(s)-\omega_1(s)||_V \text{d}s + \underset{t \in [0,k]}{\text{sup}}||\omega_2(t)-\omega_1(t)||_{H}\bigg)\wedge 1\bigg].$$ Finally let $\pi_t:\mathbb{B} \to H$ denote the canonical projection at time $t \geq 0$, i.e. $\pi_t(y) := y_t$. Now I want to consider the Borel $\sigma$-algebra of the topology on $\mathbb{B}$, which is induced through $\rho$, denoted by $\mathcal{B}(\mathbb{B})$ as usual. **My question: Is it true - and if yes: how does one show this - that **
$$\mathcal{B}(\mathbb{B}) = \sigma(\pi_t|t \geq 0)?$$ Clearly each $\pi_t$ is continuous and hence ""$\supseteq$"" is trivial. But the other inclusion does not seem to be clear to me. I am especially concerned, because the definition of the metric involves the $V$-norm, which is not equivalent to the norm on $H$, but might be much bigger. Hence - in general - the metric $\rho$ could be much bigger than just the weighted sup-norm (i.e. the metric we'd obtain by dropping the first summand for every $k$). I would be very grateful for any hint on this! Thanks a lot!","['measure-theory', 'functional-analysis']"
2867075,"what is ranks do in Singular value decomposition, if rank = k, others than k first singular values set to 0 or gone?","hi lets assume we have matrix A with 4 rows x 3 columns, when we input to svd it become: U = 4x4 , S = 4x3 and VT = 3x3. after that i specify that rank k=2 so what exactly happen to the other than the first k ranks? is it set to zero or completely gone like U = 4x2 S=2x2 and VT = 2x3? both cases resulting the same rows and columns, right? which is 4x3, but does it affect the value of the reduced matrix?","['matrices', 'linear-algebra', 'svd', 'matrix-decomposition']"
2867095,"Is there any measurable function $f:[0,1]\to\mathbb R$ such that every modification is nowhere continuous?","It is a well-known fact, that by Lusin's theorem, given any measurable function $f:[0,1]\to\mathbb R$, and any $\varepsilon>0$ there exists a compact set $E\subset[0,1]$ such that $|[0,1]\setminus E|<\varepsilon$ and $f|_E$ is continuous. However, this does not seem to exclude the possibility that $f$ and all of its modification on null sets are nowhere continuous (since $E$ could simply be a fat Cantor set, i.e. have empty interior). So is it indeed possible that $f$ and all its modifications are nowhere continuous, and yet measurable?","['measure-theory', 'lebesgue-measure', 'real-analysis']"
2867110,Catalan numbers and triangulations,"The number of ways to parenthesize an $n$ fold product is a Catalan number in the list $1,1,2,5,14,\cdots$ where these are in order of the number of terms in the product. The $n$th such number is also the numbr of ways to triangulate an $n+1$-gon. I'm wondering whether there is a simple translation between a specific parenthesized $n$ fold product to a specific triangulation of an $n+1$-gon. For example one parenthesized 5-fold product is $(12)(3(45))$ This then would (hopefully) be translatable to one of the $14$ triangulations of a $6$-gon. I have tried various ways to label the vertices of the $n+1$ gon to see which parenthesized $n$ fold product a given triangulation goes with, but no luck.","['catalan-numbers', 'triangulation', 'combinatorics']"
2867122,"Justify: if $x\gt 0$, $\;\lim_{n\to\infty} \sqrt{n}\cdot{\overbrace{\sin\sin\cdots\sin}^{n\space\text{sines}}(x)}=\sqrt{3}$","I believe that I have managed to show that (if $x\gt 0$)
$$\lim_{n\to\infty} \sqrt{n}\cdot{\overbrace{\sin\sin\cdots\sin}^{n\space\text{sines}}(x)}=\sqrt{3}$$
I did this by defining a sequence as $a_0=x$ and the recursion
$$a_{n+1}=\sin a_n$$
I then approximated the recursion with the first two nonzero terms of the Maclaurin series for sine, giving me
$$\Delta a_n=-\frac{x^3}{6}$$
I then approximated this with a differential equation
$$y'=-\frac{y^3}{6}$$
Which I then easily solved... the answer follows from here. Question: How can this be made more rigorous? I don't know how to justify that my approximations are good enough for th error to vanish under the limit. What theorems are generally used to justify approximations of discrete recursions with differential equations? I think I know how to justify the approximation of some with its Maclaurin series using the Lagrange error bound.","['limits', 'approximation', 'recurrence-relations', 'reference-request']"
2867140,Proving that 1/3 has no finite decimal representation,"There is a problem where i need to prove that 1/3 has no finite decimal representation Here's my proof, can someone tell me if its valid? Proof Lets assume there is a decimal representation for $\frac{1}{3}$, Therefore: $ \exists n,b \in \mathbb{N} $ : $ (\frac{b}{10^n}=\frac{1}{3}$) $ \land (\sum_{k=1}^n \frac{a_k}{10^k}=\frac{1}{3})$ By the theorem: $\frac{1}{3} = \frac{b}{10^n} $ b = $\frac{10^n}{3}$ = $\frac{(2 \times 5)^n}{3}$ Thats a contradiction ($b \notin \mathbb{N}$), Since that fraction is irreducible (Both 2,5,3 are prime numbers). Is my proof valid? If not, Can someone explain what's wrong with it? Thanks.","['algebra-precalculus', 'proof-verification', 'decimal-expansion']"
2867221,A graph $G$ of radius at most $k$ and maximum degree at most $d$ has no more than $1 + kd^k$ vertices,"Proposition: A graph $G$ of radius at most $k$ and maximum degree at most $d$ has no more than $1 + kd^k$ vertices. Assuming that $d > 2$ and $k > 3$, improve the bound in above proposition to $d^k$. The above question is from exercise problem in Diestel's book. How can we formally prove this?","['graph-theory', 'combinatorics', 'discrete-mathematics']"
2867227,In how many ways a student can get $2m $ Marks,An examination contains four Question papers each paper carrying maximum marks as $m$. Find number of ways a student appearing for all the four papers gets a total of $2m$ Marks. I used generating Polynomial method that is to find coefficient of $x^{2m}$ in $$(1+x+x^2+\cdots+x^m)^4$$ which is $$(1-x^{m+1})^4(1-x)^{-4}$$ which gives the coefficient of $x^{2m}$ as $$\binom{2m+3}{3}-4 \times \binom{m+2}{m-1}$$ But the answer is just $\binom{2m+3}{3}$. What went wrong?,"['binomial-coefficients', 'combinatorics', 'polynomials', 'binomial-theorem']"
2867259,A finitely generated group that contains a subgroup which is not finitely generated.,Question: Give a finitely generated group that contains a subgroup which is not finitely generated. What I know: I learned that the free group with two generators $F_{2}$ (with 2 by 2 matrix generators)  is finitely generated. But I do not know what the subgroup of it which is not finitely generated. Can some one help me find such a subgroup? I would appreciate!,"['combinatorial-group-theory', 'finitely-generated', 'group-theory']"
2867261,Measure of set where holomorphic function is large,"Suppose that $f:\mathbb{C}\rightarrow \mathbb{C}$ is a non-constant entire function.  By Liouville's theorem, we know that $f$ must take on arbitrarily large values.  However Liouville doesn't say anything about what this large set must look like.  In particular is it possible that the large values of $f$ are concentrated on a set of small measure? More precisely, does there exist a non-constant entire function $f$ such that $\lambda(\{x: |f(x)|>1 \})<\infty$?  Here $\lambda$ denotes the $2$-dimensional Lebesgue measure.","['complex-analysis', 'entire-functions', 'lebesgue-measure', 'analysis']"
2867314,"Particle moves in square, what is the expected distance before first return to edge?","There is a unit square with a particle moving in it. After the particle collides with an edge, the angle of reflection is random and is drawn from the uniform distribution on $[-\frac{\pi}{2}, \frac{\pi}{2}].$ The question is to find the average distance the particle covers before it returns to the same edge next time (I guess that after a large number of collusions the starting point is not important). To me, this sounds like a question about the stationary distribution of a Markov chain with a continuum of states. However, the problem actually is taken from a physics olympiad for high school students. It is claimed that the answer is $2\sqrt{2}$. If it is not a mistake, there probably is an intuitive non-rigorous argument why the answer is $2\sqrt{2}$. Update 1: I was asked to post the original text of the problem here. It is somewhat different from what I wrote above, but I believe that this is basically the same question: Problem: In a computer model, movement of a particle inside of a square is
  simulated. Square has sides of length L, the speed of the point is V.
  After a collusion with an edge point bounces at a random angle
  (equiprobable from -90 to 90 degrees) with the same speed. Estimate the
  number of collusions with one of the sides after a large period of
  time T. Answer: $\frac{TV}{2\sqrt{2}L}$. Update 2: There were attempts to do a simulation (see comments below), and the results tend to be somewhat smaller than $2\sqrt{2}$. Also, in my simulation the distribution of collision points is not uniform (points close to angles are more frequent) and distribution of distance from bounce to bounce is asymmetric and bimodal.","['markov-chains', 'probability']"
2867320,Is it always necessary to prove the 'iff' in both directions?,"I have an exercise in my course, which asks to prove $A \cup B = B \iff A \subseteq B$. My proof is: Let $A \nsubseteq B$, that is, $\exists a \in A : a \notin B$. Then from the definition follows $a \in A \cup B = B$, in contradiction to the initial assertion. $\square$ Usually I see that it's much more rigorous to prove $\implies$, then $\impliedby$, but I'm not sure, if that's only an option or a strict rule — and specifically if my proof does the job in both directions or there are some gaps that I don't recognize. My script suggests a really long 10+ lines proof using the 'both directions style', but I myself don't really see this necessity at least here. This being said, is it always a must to prove the 'iff' in both directions?","['elementary-set-theory', 'proof-writing']"
2867327,Estimate the volume of Voronoi cell,"Let given a ball of radius $\alpha$ centered in point $u$ in $d$-dimensional space. Let given a sample of $n$ uniformly distributed vectors $x_i$ ($i = 1,\dots,n$) inside the ball. For each vector $x_i$ we connect points $u$ and $x_i$ by a segment and build a hyperplane $P_i$ through the middle of the segment and orthogonal to it. In the general case, the constructed hyperplanes bound a polyhedron in $\mathbb{R}^d$. I need to estimate from above the probability that uniformly distributed vector $q$ fall into this polyhedron. In some cases the constructed polyhedron can be unbounded. More precisely, as far as I know it can be if and only if $u$ is not contained in the convex hull of $x_i$. This probability can be estimated, see for example here . Then I can write down $\mathbb{P}( q$ in polyhedron $) \le \mathbb{P}(u$ in convex hull$) \mathbb{P}(q$ in polyhedron$|u$ in convex hull$) + \mathbb{P}(u$ not in convex hull$) \mathbb{P}(q$ in polyhedron$|u$ not in convex hull$) \le \mathbb{P}(q$ in polyhedron$|u$ in convex hull$) + \mathbb{P}(u$ not in convex hull$)$. In such a way the problem of an unbounded polyhedron is overcome and we only need to estimate $\mathbb{P}(q$ in polyhedron$|u$ in convex hull$)$. One more idea is that w.l.o.g. we can assume that $x_i$ is uniformly distributed on the sphere. Then we know the radius of sphere inscribed in our polyhedron, which is exactly equal to $\alpha / 2$. There is another interpretation of this problem, maybe it can help. It is easy to see that our polyhedron is exactly the Voronoi cell of point $u$, so we need to estimate the volume of Voronoi cell. It is obvious that when $n$ tends to infinity, then polyhedron volume tends to zero, so there has to be some rate of converges, but I do not know how to estimate it. Thank you very much for any ideas, proofs, estimates, papers and so on!","['geometric-probability', 'convex-hulls', 'convex-geometry', 'geometry', 'probability']"
2867344,Are there sequence $(u_n)\subset \mathbb R^+$ s.t. $\sum_{n=1}^\infty u_n<\infty $ but $nu_n\not\to 0$?,"I have an exercise that ask me to prove that if $(u_n)$ is decreasing, $u_n\geq 0$ for all $n$ and $\sum_{n=1}^\infty u_n$ converge, then $nu_n\to 0$. I proved it, but for me this result in fact correct even if we remove $(u_n)$ decreasing. Because I have in mind that $1/n$ is the quickest speed of non convergence, i.e. if $\sum_{n}v_n$ converge, it will have $\alpha $ s.t. $v_n\leq \frac{1}{n^\alpha }<\frac{1}{n}$ for all $n$ for a certain $n$. But if we impose $u_n$ decreasing, maybe my imagination is wrong. So is there a positive sequence s.t. $\sum_{n=1}^\infty u_n$ converge but $nu_n\not\to 0$ ?","['sequences-and-series', 'real-analysis']"
2867347,A conjecture about a circle bound to any triangle,"Given an equilateral triangle $ABC$, we choose a point $D$ inside it, determining a new triangle $ADB$. We draw the circles with centers in $A$ and in $B$ passing by $D$, determining the new points $E$ and $F$ on the side $AB$. If now we draw the two circles with center in $A$ and in $B$ and passing by $E$ and by $F$, respectively, we determine two new points $G$ and  $H$ on the sides $AD$ and $DB$. This post A conjecture related to a circle intrinsically bound to any triangle shows that the points $EGDHF$ determines always a circle. Now we focus on the segments $DG$ and $CD$, and we draw their perpendicular bisectors. They intersect in the point $I$. The circle with center in $I$ and passing by $C$, pass also through $G$ and $D$, for any $D$. Moreover, it always determines a point $J$ on the side $AC$ of the equilateral triangle. A similar construction can be done starting from the perpendicular bisectors of $CD$ and $DH$, obtaining the center $K$ and the point $L$ on the side $CB$ of the equilateral triangle. My conjecture is that the points $CJEFL$ always determine a circle. Please, can you help me to find an elementary proof of such conjecture? Thanks for your suggestions!","['euclidean-geometry', 'triangles', 'geometry']"
2867375,Evaluating $\lim_{x\to0} \frac{\cos x - \cos 3x}{\sin 3x^2 - \sin x^2}$,"$$ \lim_{x\to0}\frac{\cos x-\cos (3x)}{\sin (3x^2)-\sin (x^2)} $$ Is there a simple way of finding the limit? I know the long one: rewrite it as 
$$ -\lim_{x\to 0}\frac{\cos x-\cos(3x)}{\sin(3x^2)}\cdot\frac{1}{1-\dfrac{\sin(3x^2)}{\sin(x^2)}} $$
and then find both limits in separately applying L'Hospital's rule several times. The answer is $2$.","['limits', 'calculus', 'trigonometry']"
2867376,Are context free languages closed under taking substring?,"Let $L$ be context free and $\Sigma $ an alphabet Define $s(L):=\{y \in \Sigma ^*\mid \exists x,z \in \Sigma ^*: xyz \in L\}$ Is $s(L)$ context free ? I haven't been able to find a counterexample so im thinking i have to prove it. I was trying to use the closure properties. $s(L)$ obviously contains $L$ so maybe i can write $s(L)$ as the union of several context free languages. Could i please get some help, im stuck.","['context-free-grammar', 'logic', 'discrete-mathematics', 'formal-languages', 'computer-science']"
2867402,Differentiation under the integral sign for the Itō integral,"Let $(\Omega,\mathcal A,\operatorname P)$ be a complete probability space $T>0$ $I:=(0,T]$ $(\mathcal F_t)_{t\in\overline I}$ be a complate and right-continuous filtration on $(\Omega,\mathcal A,\operatorname P)$ $M$ be a real-valued continuous square-integrable $\mathcal F$-martingale on $(\Omega,\mathcal A,\operatorname P)$ $\mu_M$ denote the Doléans measure corresponding to $M$ $d\in\mathbb N$ $\Lambda\subseteq\mathbb R^d$ be open Now, let $F:\Omega\times\overline I\times\Lambda\to\mathbb R$ with $$F_t\in C^1(\Lambda)\;\;\;\text{almost surely for all }t\in\overline I\tag1$$ and $$F(x)\in\mathcal L^2(\mu_M)\;\;\;\text{for all }x\in\Lambda\tag2.$$ Moreover, let $i\in\left\{1,\ldots,d\right\}$ and assume that $$\int\sup_{x\in K}\left|\frac{\partial F}{\partial x_i}(x)\right|^2\:{\rm d}\mu_M<\infty\;\;\;\text{for all compact }K\subseteq\Lambda\tag3.$$ Let $$N(x):=F(x)\cdot M$$ denote the Itō integral process of $F(x)$ with respect to $M$ for $x\in M$. Are we able to conclude that $N$ is partially differentiable with respect to the $i$th-variable? For the sake of simplicity, assume $\Lambda=\mathbb R^d$. Let $x\in\Lambda$ and $$G(h):=\frac{N(x+he_i)-N(x)}h\;\;\;\text{for }h\in\mathbb R\setminus\left\{0\right\}.$$ Clearly, $$G(h)=\frac{F(x+he_i)-F(x)}h\cdot M\;\;\;\text{almost surely for all }h\in\mathbb R\setminus\left\{0\right\}\tag4$$ and $$\frac{F_t(x+he_i)-F_t(x)}h\xrightarrow{h\to0}\frac{\partial F_t}{\partial x_i}(x)\;\;\;\text{almost surely for all }t\in\overline I\tag5$$ From $(3)$ and $(5)$, we should be able to conclude that the convergence in $(5)$ holds in $L^2(\mu_M)$ by applying Lebesgue's dominated convergence theorem. This should yield that the corresponding Itō integral processes converge in the space of square-integrable martingales. Am I missing something?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
2867458,Reciprocity of different prime numbers can approximate $1$?,"I want to see if there exist $p_1<p_2<p_3<\cdots<p_{1000}$ different prime numbers such that
$|1/p_1+\cdots+1/p_{1000}-1|\le ({1\over p_{1000}})^2.$ a) what is my point with this? Nothing. But what is the point of the twin number conjecture? Maybe I can prove that such problem has always a solution, even for more terms. b) what is my progress? We cannot have $|1/p_1+\cdots+1/p_{1000}-1|=0$ (which I proved), so there must be an error. Also, $|1/2+1/5+1/7+1/11+1/13-1|=0.0107<1/13$, so for five terms there is a solution with power 1, but 1/13^2 cannot be used here. In fact, I proved that the problem has a solution, if the expression on the right is simply $({1\over p_{1000}}),$ that is, the second power is just first power. (We can have $k$ terms, in general, too, not just 1000 terms.) I wish to have second power (or anything bigger than 1 is great as long as we tend to infinity with the number of terms), because of Hurwitz theorem https://en.wikipedia.org/wiki/Hurwitz%27s_theorem_(number_theory) Remark: the sum of repciprocal of all primes is infinity, that looks good/promising in order not to get a contradiction.","['number-theory', 'harmonic-numbers', 'prime-numbers']"
2867471,"How to show $\int g \, d\nu = \int g \cdot f \, d\mu$, where $\nu(K) = \int_K f \, d\mu$?","Let $\mu$ be a measure on $X$, and let $f:X \rightarrow [0,+\infty]$ be $\mu$-measurable. Define the measure:
$$
\nu(K) = \int_K f \, d\mu
$$
I know that any $\mu$-measurable function $g:X \rightarrow [0,+\infty]$ is also $\nu$-measurable. I want to show that for any such function, we have:
$$
\int g \, d\nu = \int g \cdot f \, d\mu
$$ I have already proven the case where $g$ is simple. I have also proven that if $h$ is simple, then $h \leq g \;$ $\nu$-a.e. if and only if $hf \leq gf \; \mu$-a.e. Using these facts and the definition of the lower integral, we can write:
\begin{align*}
\int g \, d\nu 
&= \sup
\left\{
\int h \, d\nu :
h \text{ is } \nu \text{-integrable, simple, }
h \leq g \; \nu \text{-a.e.}
\right\}
\\
&= \sup
\left\{
\int h \cdot f \, d\mu :
h \text{ is } \nu \text{-integrable, simple, }
hf \leq gf \; \mu \text{-a.e.}
\right\}
\end{align*}
But at this point I'm lost as to how to transform this into the lower integral of $\int gf \, d\mu$, i.e.:
$$
\sup
\left\{
\int h \, d\mu :
h \text{ is } \nu \text{-integrable, simple, }
h \leq gf \; \mu \text{-a.e.}
\right\}
$$","['integration', 'measure-theory', 'lebesgue-integral', 'real-analysis']"
2867479,GRE combinatorics question about counting the no. of sets questions satisfying a certain requirement.,"From ETS Major Field Test in Mathematics A student is given an exam consisting of
  8 essay questions divided into 4 groups of
  2 questions each. The student is required to
  select a set of 6 questions to answer,
  including at least 1 question from each of
  the 4 groups. How many sets of questions
  satisfy this requirement? I'm thinking $$\binom{2}{1}^4 \binom{4}{2}$$ because we have to pick 1 from each of the 4 groups of 2 and then from the remaining 4 questions we pick 2.","['permutations', 'combinations', 'combinatorics']"
2867502,A question on the limit $\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$,"I stumbled upon this question from a Calculus exam: Let $f \in C^1(\mathbb{R})$ be monotonically decreasing such that $\lim \limits_{x \rightarrow \infty} f(x) = 0$. Prove that the limit
$$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$$
exists and is finite. Now obviously the solution the writers of this problem had intended was to look at the integral
$$\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx = \int \limits_{1}^{\infty} f(x)\cos x \, dx$$
which converges by the Dirichlet criterion and then the Riemann sums attributed to the partitions $\Pi_n = \{0, \frac{1}{n}, \frac{2}{n}, ..., 1
 \}$ are $S_n=n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}$ which if would imply 
$$\lim \limits_{n \rightarrow \infty} n \sum \limits_{j=1}^{n} \frac{\cos(\frac{n}{j})f(\frac{n}{j})}{j^2}=\int \limits_{0}^{1} \frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx < \infty$$
if the function $\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}$ was Riemann-integrable in $[0,1]$. The problem with that approach however, is that the integral 
$$\int \limits_{0}^{1}\frac{cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$
is not a Riemann-integral but an improper-integral so one cannot conclude that
$$\lim \limits_{n \rightarrow \infty} S_n = \int \limits_{0}^{1}\frac{\cos(\frac{1}{x})f(\frac{1}{x})}{x^2}dx$$ After many hours of thinking, trying to resolve this issue I realized that the question may be false as it makes no sense that the limit exists because that would sort of mean that the integral is a proper Riemann-integral which would imply the function is bounded (which is not necessarily true depending on the choice of $f$). To test this, I used Wolfram Alpha to calculate numerical estimates with the function $f(x)=\frac{1}{x}$, which confirmed my speculations, though I have not been able to rigorously prove it. My question is can you find an example that disproves the claim the question makes? Or was I wrong and the claim can be proven?",['calculus']
2867522,"Putnam 2007 A5: Finite group $n$ elements order $p$, prove either $n=0$ or $p$ divides $n+1$","Putnam 2007 Question A5: ""Suppose that a finite group has exactly $n$ elements of order $p$, where $p$ is a prime. Prove that either $n=0$ or $p$ divides $n+1$."" I split this problem into two cases: where $p$ divides $|G|=m$, and where $p$ does not divide $m$. The latter case is trivial - by Lagrange's Theorem, $n=0$ as the order of an element must divide the order of the group. The first case appears more complicated and my idea is to use Sylow Theory, and I came across an interesting solution using this on https://blogs.haverford.edu/mathproblemsolving/files/2010/05/Putnam-2007-Solutions.pdf : ""There are 1 + kp Sylow p-subgroups and, because they are all conjugate and every element of order p is contained in some Sylow p-subroup, they partition the n elements of order p into 1 + kp equal-size collections. The number of elements of order p in any p-group is always one less than a power of p, implying n + 1 ≡ (1 + kp)(-1) + 1 ≡ 0 modulo p."" The places I am stuck are: 1) Why the fact that all Sylow p-subgroups being conjugate and every element of order $p$ contained in some Sylow p-subgroup (I understand why both these facts are true), implies that the Sylow p-subgroups partition the $n$ elements of order $p$ into $1+kp$ equal-size collections - heck I don't even understand what is meant by this... 2) Why the number of elements of order $p$ in any p-group is always one less than a power of p. If anyone has other ways to show that $p$ dividing $m$ implies that $p$ divides $n+1$, that would also be greatly appreciated :) Thanks","['contest-math', 'finite-groups', 'abstract-algebra', 'sylow-theory', 'group-theory']"
2867523,"Rotman's proof of $H_1(X,x_0) \cong H_1(X)$","Let $X$ be a topological space, and $x_0\in X$ . Then $H_n(X)\cong H_n(X,x_0)$ whenever $n\ge 1$ . For $n\ge 2,$ it's easy: the exact sequence $$
  \cdots
  \rightarrow
  H_n(\left \{ x_0 \right \})
  \rightarrow
  H_n(X)
  \rightarrow
  H_n(X,x_0)
  \rightarrow
  H_{n-1}(\left \{ x_0 \right \})
  \rightarrow
  \cdots
$$ and the dimension axiom implies the result whenever $n\ge 2$ . My problem is with Rotman's proof of the case $n=1$ , specifically the proof that $\ker k\neq 0$ . \begin{align*}
  \cdots
  \to
  H_1(\{x_0\})
  \to
  H_1(X)
  \xrightarrow{g}
  H_1(X,x_0)
  \to
  H_0(\{x_0\})
  \xrightarrow{h}
  H_0(X)
  &\xrightarrow{k}
  H_0(X,x_0) \\
  &\to
  0.
\end{align*} since $H_1(\{x_0\}) = 0$ , the map $g$ is injective;
  by Exercise 5.2, $g$ is surjective (hence is an isomorphism) if and only if $h$ is injective.
  The map $h$ has domain $H_0(\{x_0\}) \cong \mathbb{Z}$ and target the free abelian groups $H_0(X)$ .
  If $h \neq 0$ , then $h$ must be injective (if $\ker h \neq 0$ , then $H_0(X)$ would contain a nontrivial finite subgroup isomorphic to $\mathbb{Z}/{\ker h}$ ).
  Now $\operatorname{im} h = \ker k$ , so that $\ker k \neq 0$ implies that $\operatorname{im} h \neq 0$ , hence $h \neq 0$ , as desired.
  But $k$ , being induced by inclusion, is the map $S_0(X)/B_0(X) \to S_0(X)/B_0(X) + S_0(x_0)$ [ $S_0(X) = Z_0(X) = Z_0(X,x_0)$ ] given by $\gamma + B_0(X) \mapsto \gamma + B_0(X) + S_0(x_0)$ , and so $\ker k = (B_0(X) + S_0(x_0))/B_0(X)$ .
  The proof of Theorem 4.14 describes $B_0(X)$ as all $\sum m_x x$ with $\sum m_x = 0$ ;
  hence $\ker k \neq 0$ , and the proof is complete. (Original scanned images here and here .) Is he making the identification \begin{align*}
       H(X,x_0)
&=     \ker \overline{\partial} / {\operatorname{im} \overline{\partial}} \\
&\cong Z_0(X,x_0)) / B_0(X,x_0) \\
&=     Z_0(X) / B_0(X,x_0) \\
&=     Z_0(X) / ( B_0(X) + S_0(x_0) )
\end{align*} when he claims that $\gamma+B_0(X)\mapsto\gamma +B_0(X)+S_0(x_0)?$ Is there another more intuitive way to treat the case $n=1$ ? Edit: or maybe we can just note that if $r \colon X \to \{ x_0 \}$ is the constant map, then $r\circ h = 1_{\{ x_0 \}}$ and therefore $$
  1_{H_0( \{ x_0 \})}
= H_0(1_{ \{ x_0 \}})
= H_0(r \circ h)
= H_0(r)\circ H_0(h)
= r_* \circ h_*,
$$ so $h_*$ is injective. From this we can even get the result that $\tilde H_0 \cong H_0(X,x_0)$ , for we know that $$
  H_0(X) \cong \mathbb Z \oplus \tilde H_0(X)
$$ and, since $h_*$ is injective, $$
  0
  \rightarrow
  H_0(\{ x_0 \})
  \xrightarrow{h_*}
  H_0(X)
  \xrightarrow{k_*}
  H_0(X,x_0)
  \rightarrow
  0
$$ is a short exact sequence, and now since $r_*\circ h_*=1$ , it splits, so that $$
      H_0(X)
\cong H_0(\{ x_0\}) \oplus H_0(X,x_0)
\cong \mathbb Z\oplus H_0(X,x_0).
$$","['homological-algebra', 'proof-explanation', 'category-theory', 'abstract-algebra', 'algebraic-topology']"
2867527,What is the action of $\mathrm{Gal}(\overline{\mathbb{Q}}/\mathbb{Q})$ on the Teichmuller tower?,"The basis of Grothendieck's esquisse d'un programme is that there exists an action of the absolute galois group of the rationals on the Teichmuller tower, the collection of all etale fundamental groups of the moduli spaces of algebraic curves. Not only this, but apparently Belyi's theorem can be used to prove that it is already faithful on $\pi_1^{et}(\mathcal{M}_{0,4})$. I've looked at quite a few resources discussing this and none of them actually explicitly define what this action is (or what it is induced by). Are there any resources that explicitly define the action and give a proof using Belyi's theorem that this action is faithful? Thanks for any help.","['galois-theory', 'algebraic-geometry']"
2867603,How to apply the principle of inclusion and exclusion in this problem,8 new employees are assigned to 3 different departments. How many differents ways of assigning the employees exist if each department has to receive at least one employee? At first I thought to work with the departments as sets but I think that is not possible beacuse the problem does not say that the employees can work in one or more departmets. Is working with the departments as sets the right option?,"['inclusion-exclusion', 'combinatorics']"
2867673,Is there a non-elementary function with an elementary derivative and an elementary inverse?,"Elementary functions are combinations of powers, exponentials and logarithms, using composition and arithmetic operations. The inverse of an elementary function may not be elementary, and the integral of an elementary function may not be elementary. There are a couple of equivalent ways to ask my question: Is there a function $F(x)$ that is non-elementary, but its derivative $F'(x)$ and inverse $F^{-1}(x)$ are both elementary? Is there an elementary function $f(x)$ whose integral $F(x)$ is non-elementary, but can be expressed as the inverse of some elementary function? (i.e. $F^{-1}(x)$ is elementary) Here are some non-examples: The Lambert W function is the inverse of $x e^x$. It is not elementary, but its derivative is not elementary either:
$$W'(x) = \frac{W(x)}{x(1 + W(x))}$$ The ""exponential integral"" $Ei(x)$ is the integral of $\int \frac{e^x}{x} dx$, which is non-elementary. Its inverse $Ei^{-1}(x)$ is not elementary either, so this is not what I'm looking for.",['integration']
2867682,Are there interesting example of pseudo-Riemannian manifold other than spacetime manifold?,The 4 dimensional spacetime manifold is a typical example of pseudo-Riemannian manifold. Are there other mathematically or physically interesting example of it?,"['riemannian-geometry', 'motivation', 'physics', 'soft-question', 'differential-geometry']"
2867693,Example of a non-nilpotent finite group $G$ so that every non-trivial normal subgroup of $G$ intersects $Z(G)$ non-trivially?,"It is well-known that if $G$ is a nilpotent group, then every non-trivial normal subgroup of $G$ has non-trivial intersection with $Z(G)$.  I would like to find examples of non-nilpotent finite groups $G$ so that every non-trivial normal subgroup of $G$ intersects $Z(G)$ non-trivially.  In particular, I am interested in examples having small order.  I have checked some of the well-known groups of small order to see whether they satisfy the condition in question, and those that I checked do not.","['group-theory', 'abstract-algebra', 'finite-groups']"
2867694,Show there is no group $G$ of order $240$ with $5$ conjugacy classes,"Original problem: suppose a group $G$ has irreducible complex representations of dimensions $1,1,2,3$, and $d$. Find $d$. Using some basic dimension counting, we immediately get $d=3$ or $d=15$. The goal is to eliminate $d=15$. If $d=15$, then $|G|=1^2+1^2+2^2+3^2+15^2=240$. Since $G$ has two $1$-dimensional irreducible representations, $[G:G']=2$, and $G'$ is normal. Thus $G'$ is a union of conjugacy classes. But $G'$ has elements of order $1$, $2$, $3$, and $5$, so it contains at least $4$ conjugacy classes. The only possibility is that $G\setminus G'$ is a single conjugacy class of order $120$. At this point, it is driving me crazy that I can't find an easier contradiction. It seems absurd that $G$ can have $240$ elements with half of them in a single conjugacy class. I've included a solution I found which I am comfortable with, but which seems a bit convoluted to me. Solution: For any $x\in G\setminus G'$, the centralizer $C_G(x)$ has order $2$, so $x^2=1$. Then $C_G(x)$ extends to a Sylow $2$ subgroup $P$. The center of $P$, $Z(P)$, is nontrivial, and $Z(P)\subseteq C_G(x)$, so $C_G(x)=Z(P)$. But if $x$ is central in $P$, then $|C_G(x)|\ge |P|=16$.","['group-theory', 'representation-theory']"
2867879,Is the Birkhoff–von Neumann theorem true for infinite matrices?,"The Birkhoff–von Neumann theorem states that every $n \times n$ doubly stochastic matrix is a convex combination of permutation matrices. Is this true for $\mathbb{N} \times \mathbb{N}$ matrices as well? If so, can you provide a reference?","['infinite-matrices', 'reference-request', 'linear-algebra', 'stochastic-matrices', 'discrete-mathematics']"
2867911,If $\sum a_n$ converges and every $a_n$ is positive then $\sum a_n^{(n-1)/n}$ converges? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 5 years ago . Improve this question Let $(a_n)$ be a sequence of real positive numbers such that $\sum a_n$ is a convergent series. What can we say about the series $\sum a_n^{\frac{n-1}{n}}$? Show that it is convergent or find a counterexample. I'm trying to find a counterexample but maybe is true that the series is convergent.",['sequences-and-series']
2867933,Definition of limit: $\forall n>N$ or $\forall n \geq N$?,"My question is about the definition of limit. Definition: The number $a$ is said to be the limit of the sequence $\{x_n\}$ if $\forall \epsilon > 0$, $\exists N \in \mathbb{N}$ such that $\forall n > N$, we have
  $$|x_n - a| < \epsilon.$$ This definition is in the book ""The fundamentals of Mathematical Analysis - Fikhtengol'ts"". But in ""Principles of Mathematical Analysis - Walter Rudin"", he uses the condition $n \geq N$. Is there any difference in the definition of limit when we use the conditions $n>N$ and $n \geq N$?","['real-analysis', 'calculus', 'definition', 'sequences-and-series', 'limits']"
2867979,Hat 'trick': Can one of them guess right?,"There are $n$ boys and $n$ girls. Each of them is given a hat of only 4 possible (known) colors and doesn't know its color. Now each can only see all the colors of hats of those of the other gender and no contact is allowed, then each is asked to guess the color of their own hat at the same time . Determine whether such $n$ exists that there is a strategy that at least one child can guess right under any circumstances. When there are only 3 possible colors, the problem has been solved ($n=2$ is OK) through simple algebra. But when it comes to 4 colors, the problem seems much harder. Please help. Solution for 3 colors ($n=2$): we use 0, 1, 2 to represent the colors, the boys are $a, b$ and girls are $c, d$, respectively. Each boy knows the value of $c, d$, while each girl knows the value of $a, b$.
Now consider the four number:$$a+b+c,$$ $$d+a-b,$$ $$d+b-c,$$ $$d+c-a.$$ It's easy to show at least one of them is divisible by 3. As a result, the strategy is: $A, B, C, D$ guess $c+d$, $c-d$, $-a-b$, $b-a$$\pmod 3$ respectively, and one of them must be right.","['puzzle', 'combinatorics', 'combinatorial-game-theory', 'game-theory', 'recreational-mathematics']"
2867994,Distinguishing between the square roots of a quadratic residue,"For a prime $p$, given $g$, $x = g^{2r}\pmod p$, and $y$ such that $y^2 \equiv x \pmod p$, is it possible to determine if $y \equiv g^r \pmod p$ without calculating the discrete logarithm? Comparing numbers by size doesn't seem to help. For example, with $p = 107$, $g = 2$, and $x = 2^{92} \equiv 33 \pmod {107}$, the square roots are $56$ and $51 \mod p$. Is there a way to determine that $56 \equiv 2^{46}$ just from the values of $p,g,x$ without computing/knowing r? EDIT: Could this be possible if r is restricted to some range? like $0\leq r < \frac p2$ or $\frac p2 \leq r < p$? Thanks!","['number-theory', 'modular-arithmetic']"
2868013,Show that a semigroup with $aS \cup \{a\} = bS \cup \{b\}$ and $Sa \cup \{a\} = Sb \cup \{b\}$ is a group,"Let $S$ be a semigroup such that for all $a,b \in S$
$$
 aS \cup \{a\} = bS \cup \{b\} \quad \text{and} \quad
 Sa \cup \{a\} = Sb \cup \{b\}.
$$
where $aS = \{as : s \in S \}$ and similarly $Sa$. I want to show that $S$ is a group. The above conditions imply
$$
 a \in bS
$$
for $a \ne b$, hence $S \setminus\{b\} \subseteq bS$ for all $b \in S$. If $S$ contains an idempotent $e$, then as $e \in eS$ we have $eS = S = Se$ and $e$ is the identity. With an identity element it is easy to see that $aS = S$ and $S = Sa$ for every $a \in S$, hence that $S$ is a group. So, if I could show that $S$ contains an idempotent then it follows that $S$ is a group. But here I am stuck, so could anyone provide a hint how to proceed? By the way, this is an exercise from J. Howie Fundamentals of Semigroup Theory (page 61).","['group-theory', 'abstract-algebra', 'semigroups']"
2868050,Who derived $\int_0^\infty e^{-nx}x^{s-1}dx = \Pi(s-1)/n^s$?,"Does anyone know the name of the paper in which this equation first appeared? Thank you!
$$\int_0^\infty e^{-nx}x^{s-1}dx = \Pi(s-1)/n^s$$","['complex-analysis', 'number-theory', 'math-history', 'reference-request']"
2868069,Find limit $\lim_{n\to \infty}\frac{\sum_{k=1}^n k^n}{n^n}$. [duplicate],"This question already has answers here : How to evaluate $ \lim \limits_{n\to \infty} \sum \limits_ {k=1}^n \frac{k^n}{n^n}$? (6 answers) Closed 1 year ago . Now I want to find the limit 
 $$\lim_{n\to\infty}\frac{\sum\limits_{k=1}^n k^n}{n^n}.$$
I try to use the Stolz theorem as follows:
$$\lim_{n\to \infty}\frac{\sum_{k=1}^n k^n}{n^n}=
  \lim_{n\to \infty}\frac{\sum_{k=1}^{n+1} k^{n+1}-\sum_{k=1}^n k^n}{(n+1)^{n+1}-n^n}$$
$$=\lim_{n\to\infty}\frac{(n+1)^{n+1}+\sum_{k=1}^n(k^{n+1}-k^n)}{(n+1)^{n+1}-n^n}$$
$$=1+\lim_{n\to\infty}\frac{\sum_{k=1}^n(k^{n+1}-k^n)}{(n+1)^{n+1}}.$$
It seems to deal with the summation like this form:
$$\sum_{k=1}^{n}k^p,\text{with}\ p=n,n+1.$$
I have no way to deal this summation, any help and hint will welcome!","['number-theory', 'limits', 'sequences-and-series']"
2868093,Inverse Laplace and Fourier Transform in Statistics,"I am currently exploring the use of inverse laplace transform and inverse fourier transform in statistics. From what I have read, for a random variable $ X $ with $ f(x) $ and $ F(x) $ as its PDF and CDF, we can use inverse fourier transform to recover its PDF, which is $$ f(x) = \frac{1}{2\pi} \int_{-\infty}^{\infty} e^{-itx} \phi(t) dt $$ If I want to calculate the CDF of $ X $ at point a, I can integrate above formula $$ F(a) = \frac{1}{2\pi} \int_{-\infty}^{a} \int_{-\infty}^{\infty} e^{-itx} \phi(t) dt dx $$ with $ \phi(t) = E[e^{itX}] $ is a characteristic function of $ X $. From what I have read from Wikipedia, we can use inverse laplace transform to calculate a CDF of a random variable directly by using this formula $$ F(a) = \mathcal{L}^{-1} \bigg{\{} \frac{1}{s} E \Big{[}e^{-sX} \Big{]} \bigg{\}} $$
$$ F(a) = \frac{1}{2 \pi i} \int_{\gamma - i \infty}^{\gamma + i \infty} \frac{e^{sa}}{s} E \Big{[}e^{-sX} \Big{]} ds $$ I know inverse fourier transform is equal to inverse laplace transform if we take $ s = \gamma + it $. I am trying to prove the formula for $ F(a) $ is equal if we use inverse fourier transform and inverse laplace transform, but somehow, it is obvious that they are different, one with one integral and the other with two integrals. Or is there something wrong in the formula that I have stated so both of the are not the same? If it is already correct, kindly need your help to prove it. Thank you.","['fourier-transform', 'statistics', 'laplace-transform']"
2868131,Cumulative distribution function for geometric random variable,"For geometric random variable $f(k)=(1-p)^{k-1}p$ . \begin{align}
F(k) = P(X\leq x) &= 1-P(x>k) \\
&= 1 - \sum_{i=k+1}p(1-p)^{i-1} \\
&= 1 - (1-p)^k \sum_{i=1}^ \infty p(1-p)^{i-1} \\
&= 1 - (1-p)^k
\end{align} I would appreciate a breakdown of these steps. Especially why do we take $1-P(x>k)$ and what operations are preformed on the summation sign? 
Stating the obvious is very welcone, my mathematical background is quite limited.","['statistics', 'probability-distributions']"
2868137,Is the maximal set of injectivity of a measurable map a measurable set?,"Consider a measurable function $f \colon X \to Y$, where $X$ and $Y$ are measurable spaces. The following definition makes sense even with no measurable structure, but the main question does not. Consider the sets $A$ that satisfy the following condition. The sets those where the function $f$ is injective with respect to the entire domain $X$:
$$
\forall x_a \in A \; \forall x \in X \text{ we have } f(x_a) = f(x) \implies x= x_a.
$$
In particular, this is a stronger condition than only requiring $f|_A$ to be injective, as $f$ may not take the values it takes in $A$ outside of $A$, either. Call such a set a good set. Based on blackboard sketches it seems that an equivalent definition for a set to be good is that, for all subsets $S \subseteq A$, we have $f^{-1}(f(S)) = S$. Unions of good sets are still good sets, so there is a maximal good set. Question: Is the maximal good set measurable? What about if $X = Y = \mathbb{R}$ (with Borel or Lebesgue measure, for example)? Blackboard sketching suggests that there is some hope for continuous real-valued functions. Also, if this kind of set of injectivity has a standard name or work done on it, then I would be happy to know of it.","['measure-theory', 'functions', 'measurable-functions']"
2868146,"In an arithmetic sequence, the third term is 10 and the fifth term is 16.","I'm just working on some summer problems so that I can be more prepared when I go into my class in the fall. I found a website full of problems of the content we will be learning but it doesn't have the answers. I need a little guidance on how to do this problem. In an arithmetic sequence, the third term is $10$ and the fifth term is $16$. Find the common difference. Find the first term. Find the sum of the first $20$ terms in the sequence. So, the arithmetic formula is $a_n = a_1 + (n – 1)d$ right? The common difference is the difference between the terms I think. So $16 - 10 = 6$, but there is a term between that so divided by $2$ it is $3$. How do I find the first term and the sum of the first 20 terms?","['algebra-precalculus', 'sequences-and-series']"
2868149,Arithmetic series problem,"Given $\left\{a_n\right\}$ arithmetic progression, $a_1=2$, $a_{n+1}=a_n+2n$ $\left(n\:\ge \:1\right)$. $a_{50}=?$ What i did:
$$a_n+d=a_n+2n$$
$$d=2n$$
$$a_{50}=2+d\left(n-1\right)$$
$$a_{50}=2+2\left(n^2-n\right)$$
$$a_{50}=2+2\cdot 2450$$
$$a_{50}=4902$$ But this is wrong. Answers: $$A=2452,\:B=2450,\:C=2552,\:D=2500$$",['sequences-and-series']
2868165,Infinite graph is planar iff it can be embedded in sphere,"My question is about the following statement about planar graphs: A graph is planar (i.e. can be embedded in the plane) if and only if it can be embedded in the sphere $S^2$. By an embedding we mean the following: To every vertex $v \in V$ we associate a unique point in $\mathbb{R}^2$(or $S^2$). To every edge $e \in G$ we associate a unique simple arc, which is a homeomorphic image of $[0,1]$, connecting the points associated to its end vertices such that no two arcs intersect other than in a common vertex point. The ""only if"" part of the statement above follows directly by using stereographic projection, which gives an embedding $\mathbb{R}^2 \hookrightarrow S^2$. 
For the ""if"" part, in every proof I find one simply says that in an embedding of a graph in $S^2$, one can always avoid a point and can therefore use stereographic projection again to get the embedding in $\mathbb{R}^2$. I can see this fact being true for finite graphs, as the embedded graph is just the bijective continuous image of finitely many intervals glued together in some way (which is a compact space), so if the image was the whole $S^2$, we would get a homeomorphism between $S^2$ and something which isn't $S^2$. But what about infinite graphs? As far as I know, a graph is just defined to be a tuple $(V,E)$, where $V$ is a set (of vertices) and $E$ is a set consisting of some 2-element-subsets of $V$.
So $V$ can be basically anything, so I could for example just take $V = S^2$ (as a set), or any other (uncountable) infinite set. 
In this case, how can I ensure that in a drawing of the graph on $S^2$ I can still avoid one point? Or is this statement not even true for infinite graphs?","['graph-theory', 'general-topology', 'topological-graph-theory', 'planar-graphs']"
2868172,Can you prove the power rule for irrational exponents without invoking $e$?,"The power rule states that for any real number $r$, $$\frac{d}{dx}x^r=rx^{r-1}$$ Now one common way to prove this is to use the definition $x^r=e^{r\ln x}$, where $e^x$ is defined as the inverse function of $\ln x$, which is in turn defined as $\int_1^x\frac{dt}{t}$. But this puts the cart before the horse, because students typically learn differential calculus before integral calculus.  And there is a perfectly good definition of exponentiation of real numbers that does not rely on integral calculus: $$x^r=\lim_{q\rightarrow r} x^q$$ where $q$ is a variable that ranges over the rational numbers. So my question is, if we use this definition, and we take it for granted that $\frac{d}{dx}x^q=qx^{q-1}$ holds true for rational numbers (which can be easily proven without invoking $e$), then can we prove the power rule for real exponents without invoking $e$? EDIT: Here’s a more precise formulation of the definition above.  If $r$ is a real number, we say that $x^r = L$ if for any $\epsilon>0$ there exists a $\delta>0$ such that for any rational number $q$ such that $|q-x| < \delta$, we have $|x^q-L|<\epsilon$.","['exponentiation', 'real-analysis', 'calculus', 'limits', 'derivatives']"
2868182,Minimizing RSS by taking partial derivative,"I am learning about linear regression, and the goal is to find parameters $\beta$, that minimize the RSS. My textbook accomplishes this by finding $\partial \text{ RSS} /\partial \beta = 0$ However, I am slightly stuck on the following step: They define: $RSS(\beta) = (\mathbf{y} - \mathbf{X}\beta)^{T} (\mathbf{y}-\mathbf{X}\beta$, where $\beta$ are scalars, $y$ is a column vector, and $X$ is a matrix. They find that $\frac{\partial RSS}{\partial \beta} = -2\mathbf{X}^T(\mathbf{y}-\mathbf{X}\beta)$ I tried deriving this result. I first wrote:
$(\mathbf{y} - \mathbf{X}\beta)^{T} (\mathbf{y}-\mathbf{X}\beta) = (\mathbf{y}^{T} - \mathbf{X}^{T}\beta)(\mathbf{y} - \mathbf{X}\beta)$ I then expanded the two terms in brackets:
$\mathbf{y}^{T}\mathbf{y} - \mathbf{y}^{T}\mathbf{X}\beta - \mathbf{y}\mathbf{X}^{T}\beta + \mathbf{X}^{T}\mathbf{X}\beta^2$ Now, I differentiate this with respect to $\beta$:
$-\mathbf{y}^{T}\mathbf{X} - \mathbf{y}\mathbf{X}^{T} + 2\beta \mathbf{X}^{T}\mathbf{X}$ This is where I get stuck, comparing my result with the derived result, we both have the $2\beta \mathbf{X}^{T}\mathbf{X}$ term, but I don't know how my first 2 terms should simplify to give $-2\mathbf{X}^{T}\mathbf{y}$.","['optimization', 'calculus', 'maxima-minima', 'statistics']"
2868199,Probability that a sum of uniformly distributed random variables is large,"Problem Let $\ell_1 \le \ell_2 \le \dots \ell_n$ be nonnegative real numbers, and $S$ a nonnegative real number that is smaller than the sum of the $\ell_i$. Suppose that for $i = 1, 2, \dots, n$, a number $a_i$ is picked from the interval $[0, \ell_i]$ uniformly at random. What is the probability that $$a_1 + a_2 + \dots + a_n \ge S\text{ ?}$$ Progress If $S > \ell_2 + \ell_3 + \dots + \ell_n$, it seems that the answer is just $$\frac{\left(\ell_1 + \ell_2 + \dots + \ell_n - S \right)^n}{n!\cdot \ell_1\ell_2\cdots \ell_n}.$$
I got this by computing the volume of the associated region, which in this case forms a simplex. I'm not sure what the answer is in the general case however. If there isn't a nice closed form, I'd still like to find an algorithmic approach that could determine the answer quickly.","['volume', 'probability-distributions', 'geometry', 'probability-theory', 'probability']"
