question_id,title,body,tags
2684240,Roulette probability based on last 10 rolls,"Let me preface by saying I'm not good at math. This question is purely from a mathematical standpoint. Thanks very much in advance! Ok, so lets say I want to calculate the odds of a roulette wheel landing on red or black based on the last $10$ rolls. I realize that every roll is independent of each other and each roll has a $50\%$ chance of landing on red or black (assuming no $0$). But how could I calculate the probability of rolling a red or black on the $11$th roll if I know the previous $10$ rolls? For example if the last 10 rolls had $5$ reds and $5$ blacks, I'd assume the $11$th roll would be a $50-50$ chance since there were an equal number of reds and blacks for the previous $10$ rolls (please correct me if this isn't correct). What if there were only $1$ red and $9$ blacks in the last $10$ rolls? How would I calculate the probability of getting a red or black on the $11$th roll? I'm assuming there's some kind of formula I could use to calculate this right?","['statistics', 'probability']"
2684247,Find a linear transformation such that $S^2 = T$ (general case),"Suppose a linear transformation $T:\mathbb{R}^3\rightarrow \mathbb{R}^3$, $T\begin{pmatrix}
x\\ 
y\\ 
z
\end{pmatrix} = \begin{pmatrix}
x+y\\ 
y+z\\ 
z+x
\end{pmatrix}$ (just an example). How do i find a linear transformation S such that $S^2=S\circ S=T$ ? it's part of a basic linear algebra course. Would appreciate a general answer (not specific to the example above). Iv'e already tried several ways, didn't manage to get somewhere.. Thanks. EDIT: The answer is supposed to be based only on basic matrices and linear transformations material, no Diagonalization and eigenvalues","['matrices', 'linear-algebra', 'linear-transformations']"
2684304,Proof that the area of a region in the plane is unchanged under shearing transformations,"My Geometry book (Lang's Geometry 2nd edition) presents the following two theorems concerning shearing transformations without proof: The area of a region in the plane is unchanged under shearing
  transformations. The volume of a region in 3-space is unchanged under shearing
  transformations. I would like to know how these results can be proved. The book defines area and volume to be the amount of space enclosed by a 2/3-dimensional figure, expressed in unit squares and cubes respectively. As to what a ""shearing transformation"" is, it simply states that it's a ""stretching in some direction"", so you are free to pick the definition you find best suited to the proof. Thanks.","['linear-algebra', 'linear-transformations', 'geometry']"
2684415,How to tell whether the ranges of two matrices intersect,"Assume that we have two matrices over the complex field i.e. $A\in\mathbb{C}^{m\times n_1}$ and $B\in\mathbb{C}^{m\times n_2}$. Let their range spaces be the sets
$$R(A)=\{A\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_1}\}$$ 
and 
$$R(B)=\{B\mathbf{x}|\mathbf{x}\in\mathbb{C}^{n_2}\}$$
respectively. I want to know if there is a systematic way to tell whether the range spaces have common elements. I am trying to check this in Matlab thus I am trying to find an algorithm. In other words if the columns are s.t. $A=\begin{bmatrix}\mathbf{a}_1&\dots&\mathbf{a}_{n_1}\end{bmatrix}$ and $B=\begin{bmatrix}\mathbf{b}_1&\dots&\mathbf{b}_{n_2}\end{bmatrix}$ how to tell if the following is true
$$\sum_{i=1}^{n_1}x_i\mathbf{a}_i=\sum_{i=1}^{n_2}y_i\mathbf{b}_i\Leftrightarrow x_i=y_i=0 \text{ for all }i$$ The matrices are fixed, so I am not looking for a way to construct them.","['matrices', 'linear-algebra', 'vector-spaces']"
2684442,Projectivization of direct sum of line bundles,Is there a way to understand the projectivization of direct sum ( finite if needed ) of line bundles in terms of $\mathbb{P}^1$- bundles under appropriate conditions? Thanks in advance!,['algebraic-geometry']
2684448,Munkres Example 16.3,"The example says: Let $I=[0,1]$. The dictionary order on $ I \times I$ is just the restriction to $I\times I$ of the dictionary order on the plane $\mathbb{R} \times \mathbb{R}$. However, the dictionary order topology on $I \times I$ is not the same as the subspace topology on $ I\times I$ obtained from the dictionary order topology on $\mathbb{R} \times \mathbb{R}$! For example, the set $\{1/2\} \times (1/2,1]$ is open in $I \times I$ in the subspace topology, but not in the order topology, as you can check. I have been reading responses about this same problem for a while and they have been very helpful. However, I want to check if my own explanation is correct. The set $\{1/2\} \times (1/2,1]$ is open in $I \times I$ in the subspace topology. This is because we can get it as a result of $I \times I \cap 
 \{ 1/2 \} \times (1/2,3/2)$  where $\{ 1/2 \} \times (1/2,3/2)$ is open in the dictionary order on the plane $\mathbb{R} \times \mathbb{R}$. (NOTE: Corrected notation in the end) Now for the order topology on $I \times I$, the sets are of the form: $(a,b) \times (c,d)$ where $a < c$ and $b < d$ $[0,b) \times [0,d)$ where $b < d$ $(a,1] \times (c,1]$ where $a < c\\$ The set $\{1/2\} \times (1/2,1]$ does not correspond to any of the forms shown for a basis in the order topology on $I \times I$. Then, the set $\{1/2\} \times (1/2,1]$ will be closed in the order topology on $I \times I$. To make sure I am understanding, the set he set $\{1/2\} \times (1/2,1)$ should be open in both topologies, right? Edit: As @Berci and @Henno Brandsma noted, I had problems using Munkres' notation. I will not delete my previous notation as it might be helpful for other people to identify the same mistake ( Corrected ) Now for the order topology on $I \times I$, the sets are of the form: $((a,b),(c,d))$ where $(a,b) < (c,d)$ on the lexicographic order $[(0,0),(a,b))$ where $(0,0) \leq (a,b)$ on the lexicographic order $((a,b),(1,1)]$ where $(a,b) \leq (1,1)$ on the lexicographic order Then the set $\{1/2\} \times (1/2,1]$ on Munkres' notation will be $((1/2,1/2), (1/2,1)]$. So this set will be closed on the order topology on $I \times I$.","['general-topology', 'order-topology']"
2684474,Phragmén-Lindelöf Theorem,"Let $f : G \to C$ be analytic and suppose that $G$ is bounded. Fix $z_0\in \partial G$ and suppose that $\limsup_{z→w}
| f(z)| ≤ M$ for $w \in\partial G$, $w\neq z_0$. Show that if $\lim_{z→z_0}
|z − z_0|^\alpha | f(z)| = 0$ for every $\alpha > 0$, then
  $| f(z)| ≤ M$ for every $z \in \partial G$. (Hint: If $a < G$, consider $\varphi(z) = (z − z_0)(z − a)^{−1}.$) Firstly, let's remind Phragmén-Lindelöf Theorem: Let $G$ be a simply connected region and let $f$ be an analytic function on $G$. Suppose there is an analytic function $\varphi: G \to \mathbb{C}$ which never vanishes and is bounded on $G$. If $M$ is a constant and $\partial_\infty G=A\cup B$ such that: 
  $$a) \text{ For every }a\in A, \limsup_{z\to a}|f(z)|\leq M; $$
  $$b) \text{ For every }b\in B \text{ and }\eta>0, \limsup_{z\to b}|f(z)||\varphi(z)|^\eta \leq M; $$
  then $|f(z)|\leq M$ for all $z\in G$. In the hypothesis of Phragmén-Lindelöf's Theorem, $G$ is simply connected, so we can use that, as $\varphi \neq 0$ in $G$, it exists an holomorphic determination of the logarithm. In this case, we can suppose that $G$ is open because if $f$ is analytic on $G$ is analytic in an open set $\Omega$ such that $G\subset \Omega$ too. As $G$ is bounded, let's fix $a\in \mathbb{C}\backslash \overline{G}$. The Möbius transformation 
$$\varphi(z) = \frac{z − z_0}{z − a}$$
verifies that $\varphi(z_0)=0$, $\varphi(a)=\infty$ and $\varphi(\infty)=1$. This transforms the line that pass through $z_0$ and $a$ into another line. That's my progress at the moment, because I don't know how to define a logarithm with that. I think, once I've done that, I could follow the same proof as the Phragmén-Lindelöf Theorem to finish my exercise. Any help would be appreciate.","['complex-analysis', 'problem-solving']"
2684500,My Simple Combinatorial Method to Enumerate All Sudoku Solution Grids,"How may possible Sudoku Solution Grids are there? The correct answer is: $6,670,903,752,021,072,936,960$ or $6,671E21$ as was proved $12$ years ago! Or was it? Their proof never really enumerated the solutions and involved a lot of mainframe time to compute, which involved a lot of reducing test cases to make the computations complete in human time. When they said and I read that no simple combinatorial answer was possible, I immediately though, ""Well I already know one."" So I ran the numbers on a calculator but did NOT get their solution and I can't see a flaw in mine. I can not follow there solution to the end since I have to rely on their computer results.  I sent this to the author but did not hear back, which is not surprising since it is not there place to prove me wrong. I have waited and looked over my method long enough that I stand by my solution. Since this has to be expressed in the form of a question, ""What is wrong with my solution?"" So, here is, what we have all been waiting for: My Simple Combinatorial Way to Enumerate Sudoku Solution Grids Note: The problem we are trying to solve here is for N0 which is the maximum number of correct Sudoku answer grids.  So, no discussion of swapping to get equivalent solutions. There are three Bands, rows $1$-$3$, $4$-$6$, and $7$-$9$, and three Stacks columns $1$-$3$, $4$-$6$, and $7$-$9$ in a Sudoku Grid. We will start with a discussion of the first Band which is the top three rows. The three bands and stacks divide the Sudoku Grid in to $9$ Blocks, labeled $B_1$, $B_2$, and $B_3$ in Band$1$, $B_4$, $B_5$, and $B_6$ in Band$2$, and $B_7$, $B_8$, and $B_9$ in Band$3$, which also means $B_1$, $B_4$, and $B_7$ in Stack$1$, $B_2$, $B_5$, and $B_8$ in Stack$2$, and $B_3$, $B_6$, and $B_9$ in Stack$3$. Substituting Numbers for Symbol Positions We will substitute the $9$ numbers used in the $81$ squares to leave $B_1$ with the following Grid: |1 2 3|
|4 5 6|     Note: 9! cases
|7 8 9| With this we can now think of the numbers not as number but as symbol positions, where $9!$ different solutions will use the same symbol positions for all the $81$ squares. Describing Row Constraints for a Band Say we want to fill in Band1 and we start in what I call an All-Normal Pattern for $B_1$ Row$1$, as follows: Row 1: |1 2 3|     |     |    Note: The symbol positions in B2 and
Row 2: |4 5 6|1 2 3|     |    B3 only describe the row they are in,
Row 3: |7 8 9|     |1 2 3|    not the column positions. Given this as a starting point, how do we fill in the symbol positions from the $B_1$ Row2, in $B_2$? If we tried to put them in Row1 we have a problem when we get to B3 because we would need to use positions already occupied.  So, we have to follow the same All-Normal pattern as $B_1$ Row1, resulting in the following: Row 1: |1 2 3|     |4 5 6|    Note: The symbol positions in B2 and
Row 2: |4 5 6|1 2 3|     |    B3 only describe the row they are in,
Row 3: |7 8 9|4 5 6|1 2 3|    not the column positions. Then $B_1$ Row$3$ follows the All-Normal pattern to fill in the blanks. A Normal Symbol is defined as a symbol that goes down one to the second block and down one to the third block, where if you go below the bottom you wrap to the top of the band. Abnormal Symbol is defined as a symbol that goes down two to the second block and down two to the third block, with wrapping. An All-Normal Pattern is where all nine symbols are normal. Now lets look at a case with one abnormal symbol from $B_1$ Row$1$, we will pick $3$: Row 1: |1 2 3|     |     |    Note: The symbol positions in B2 and
Row 2: |4 5 6|1 2  |    3|    B3 only describe the row they are in,
Row 3: |7 8 9|    3|1 2  |    not the column positions. Here we have the symbol $3$ going down $2$, and down $2$ with a wrap, instead of down $1$ and down $1$. Now if we try to fill in $B_1$ Row$2$ and Row$3$ in $B_2$ and $B_3$ we find that we have a similar problem, unless we do the same thing as Row$1$ and pick one of the three symbols to be abnormal. The same thing can be said about having two abnormals from Row$1$, we need to repeat this for the other two Rows.  And finally we have the three abnormals case. For brevity I will leave it to the reader to validate this or they can just look at any Sudoku solution grid. Abnormal Pattern is defined as having one abnormal symbol per row in $B_1$. Normal Pattern is defined as having one normal symbol per row in $B_1$. All-Abnormal Pattern is defined as all $27$ symbols being abnormal. The normal and abnormal patterns need further clarification. For normal pattern we need to know, which of the three positions in each row in B1 contains the normal symbol. For abnormal pattern we need the same for the abnormal symbol.  For each row there are $3$ positions for a total of $3\times3\times3 = 27$. So the total number of permutations of symbols into rows in $B_2$ and $B_3$ for Band$1$ is $1$ for the All-Normal, $3\times3\times3$ for the Abnormal, $3\times3\times3$ for the Normal, and $1$ for the All-Abnormal patterns. Let us call this $R$: R = 1 + 3*3*3 + 3*3*3 + 1 = 56 permutations of 9 symbols in 3 rows. It should be noted that $R$ describes all three blocks even if you were to swap them, it is a natural constraint on any block/stack and we can use it later to describe all $3$ blocks and $3$ stacks. The rest is trivial, but I will highlight the important parts. Describing Column Constraints for $B_2$ and $B_3$ We still need to describe the column positions for the symbols in $B_2$ and $B_3$, which is just the permutations of the three number in each sub-row, which is $6$.  Let us call this $P$: P = 6*6*6 * 6*6*6 = 6*6 * 6*6 * 6*6 = 46656 permutations of 3 symbols in 6 sub-rows. Implementation Note: Because in the Normal and Abnormal cases the one value will end up in a different row then the other two, When doing the permutations only permute two symbols, $A$ and $B$, through three positions: |A B x|, |A x B|, |x A B|, |B A x|, |B x A|, |x B A| Where $A$ and $B$ are: |A B x|    For each row in B1 in the All-Normal and All-Abnormal patterns.

|x A B|    For each row in B1 where,
|A x B|    in the Abnormal pattern, x is the position of abnormal symbol and,
|A B x|    in the Normal pattern, x is the position normal symbol. The remaining character x will find its position as the open position in its assigned row. So for Band$1$ the total number of solutions using symbol positions is: R * P = 56 * 46656 = 2612736 Note: I can use a number between $1$ through $2612736$ to calculate a specific permutation of these solutions or I can use a solution and use the above discussion to assign a specific number to this permutation. The Constraining the Rest of the Bands and Stacks If I want to describe the starting positions for $B_4$ and $B_7$ I can use $R$ and $P$ for Stack$1$ like I did for Band1 and know all the permutations of $B_4$ and $B_7$.  Unimportant Note: Later I could do some renumbering for $B_1$ when describing Stack$1$ to gain symmetry for the final row and column descriptions. $R$ can be used on Band$2$ and Band$3$ to describe the row positions for $B_5$, $B_6$, $B_8$, and $B_9$. $R$ can be used on Stack$2$ and Stack$3$ to describe the column positions for $B_5$, $B_6$, $B_8$, and $B_9$. If I know the row and column positions for each symbol for $B_5$, $B_6$, $B_8$, and $B_9$ then the completion of each permutation just involves matching the row and column for each symbol for each block. Conclusion I can describe Band$1$ and the row positions in Band$2$ and Band$3$ as: Row Contribution = R * P * R * R = 8,193,540,096 I can describe Stack$1$ and the column positions in Stack$2$ and Stack$3$ as: Column Contribution = R * P * R * R = 8,193,540,096 The total is just multiplying these two numbers and $9!$ for substituting numbers for symbol positions Total = 9! * 8,193,540,096 * 8,193,540,096 = 2.436162195571x10^25 Since it is just multiplying digits I could list all the digits but my math package does not have that may significant digits. Unimportant Note: It might be good to swap the $P$s between the two contributions since the $P$ in row contribution fixes the column positions and vice versa, or not. So, now I have a total of numbers solutions and a design for a function that, given a number in this range, I can derive a specific solution or given a solution, I can derive its ordinal number and I know how to count through all solutions. Humorous note: $8,193,540,096 \times 8,193,540,096 = 6.7134E19$ So again, where is my mistake?","['puzzle', 'combinatorics', 'sudoku']"
2684568,How to prove a fixed point is stable?,"\begin{align*}
\dot{x} &= 2 x - \frac{8}{5} x^2 - xy\\
\dot{y} &= \frac{5}{2} y - y^2 - 2 xy
\end{align*} So I have this dynamical system. I linearized it and found that the fixed point (at $x = 1.25$, $y = 0$) is the boundary case ( https://en.wikipedia.org/wiki/Linear_dynamical_system#/media/File:LinDynSysTraceDet.jpg ). $\tau$ = -2 and $\bigtriangleup$ = 0, which means the linearization says that it is a line of fixed point, which doesn't make sense since there is only 1 fixed point. My professor says that for non-linear system there is disturbance for the boundary case, so 3 possible outcomes are possible. It can be either saddle node, line of fixed points, or stable point. I tried pplane software (you can download it online for free too), and when I zoomed it for x = [1.2, 1.3], y = [-.05, .05] . It does look like there is a line of fixed point slightly above the fixed point (1.25, 0). I'm kind of stuck here. How do I prove the stability of this fixed point now? I also found the Lyapunov function for the system: 
$$V = \ln(x) - \frac{4}{5}\ln(y).$$ The two eigenvalues are -2 & 0 with eigenvectors (1,0) & (5, -8) respectively for fixed point (1.25, 0). This problem is just very weird. I have no idea what eigenvalue of 0 means. I also graphed out all the eigenvectors of the other fixed points too. Basically, I can't tell if the fixed point (1.25, 0) is stable or not. Please help!! SOME IDEAS: Let's say I have a region D bounded by four points: (1.24, .01), (1.26, .01), (1.24, 0), (1.26, 0). It's easy to see that the Lyapunov inside this region is always positive, and $\dot{V}$ is always negative when x, y > 0. This proves that the fixed point (1.25, 0) is stable at least around this neighborhood. And that means it has to be stable! Does this approach mathematically sound? Thank you!","['chaos-theory', 'ordinary-differential-equations']"
2684588,"Let $G$ be a locally compact group with Haar measure $\mu$. Is right-translation continuous on $L^1(G,\mu)$?","Let $G$ be a locally compact group with right-invariant Haar measure $\mu$. I know that the space of compactly supported continuous functions $C_c(G)$ is dense in $L^1(G,\mu)$. If $G$ is 1st-countable, I can show that whenever $y_n\to y$ in $G$, we have that for any $\varphi\in C_c(G)$, if we let $\varphi_{y_n}(x)=\varphi(x y_n)$, then $$\varphi_{y_n}\xrightarrow[L^1(G,\mu)]{n\to\infty}\varphi_y$$ by dominated convergence, so that the map $$G\to L^1(G,\mu),\quad \phi\to\phi_y$$ is continuous. But is this still true if $G$ is not 1st countable?","['functional-analysis', 'harmonic-analysis', 'real-analysis', 'group-theory']"
2684659,Are multiple kinds of attractors (chaotic and otherwise) possible within a single system of differential equations?,"I’m looking for any $n$-dimensional system of first order differential equations where depending on the initial conditions you can end up in a number of attractors, for example multiple chaotic orbits, fixed points, periodic etc. More specifically, I’m looking for a system that has two or more different chaotic attractors with optionally period orbits or fixed points. I have only ever seen one system that happened to be three-dimensional which had two chaotic attractors and they had the property of being identical – reflected across the origin. I can't remember what it was. One thing I’m currently exploring is random matrices, as they appear to have some peculiar properties (complex uniformly distributed eigenvectors).
If you use them as a linkage/weighting matrix in a dynamical system, chaotic behavior is trivial to achieve, but I’m not certain that they have multiple distinct orbits.","['chaos-theory', 'ordinary-differential-equations', 'dynamical-systems']"
2684662,Compute $\limsup |\sin^n (n)|$,"In the Scottish Book, there is a question posed by Stanislaw Hartman, which goes as follows: It is easy to see that $\liminf |\cos^n n| = \liminf |\sin^n n| = 0$. A bit harder is to prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$. Find $\liminf(\cos n)^n$, $\liminf(\sin n)^n$, and $\limsup(\sin n)^n$. For the time being, I can see easily why the first two equalities hold. But how does one prove $\limsup|\cos n|^n = 1$ and $\limsup|\sin n|^n = 1$? My attempt for $\sin$: Try to find $|\sin n|$ arbitrarily close to $1$. Let $d$ sufficiently small be given and suppose we want to find $n$ so that $|\sin n| > 1-d$. Since $\sin(\frac{\pi}{2} - x) = \cos x \geq 1-\frac{x^2}{2}$, we proceed with finding $$n\in \left(\frac{\pi}{2} - \sqrt{2d}, \frac{\pi}{2} + \sqrt{2d}\right) \mod 2\pi.$$
This reduces to finding $n,k\in\mathbb{Z}$ such that
$$\left|n - k(2\pi) - \frac{\pi}{2}\right| \leq \sqrt{2d}.$$
It looks like a Dirichlet approximation theorem at this point, but what I need is a bound on $n$ in terms of $d$. I tried using the Pigeonhole Principle (which would elucidate a bound like the below), but I got stuck. The idea is (naively, I think it's possible) to obtain a bound that looks something like
\begin{align*}
|n| \leq \frac{M}{\sqrt{d}},
\end{align*}
from which the proof is easy to complete. Any ideas on how to bridge this gap to obtain such a bound? Thanks in advance! Edits: Typographical errors.","['real-analysis', 'limsup-and-liminf']"
2684676,A quicker way to do a Lagrange multiplier problem,"I was working on the problem: minimize $x + 4z$ subject to $x^2 + y^2 + z^2 \le 2 $.  I have it solved, I want a faster method for use in standardized exams. My work: I tackled this using Lagrange Multipliers, considering the interior by looking for points where all individual partial derivatives of $x+ 4z$ are zero, (of which there are none).  Then considering the boundary $$ (x + 4z) - \lambda ( x^2 + y^2 +z^2 - 2) $$ From here I differentiated w.r.t x,y,z, $\lambda$ and set equal to 0 to yield $$ 1 - 2\lambda x = 0 \rightarrow 1 = 2\lambda x $$ 
$$ - 2 \lambda y = 0  \rightarrow 0 = 2 \lambda y \rightarrow y=0$$
$$ 4 - 2 \lambda z = 0 \rightarrow 4 = 2\lambda z$$
$$  - (x^2 + y^2 +z^2  -2 ) = 0  \rightarrow x^2 +z^2 = 2$$ Looking at equations 1, 3 we have $$ \frac{1}{2} = \lambda x, 2 = \lambda z $$ And therefore $$ \frac{1}{4} + 4 = \lambda^2 (x^2 +z^2 ) = 2 \lambda ^2 $$ $$ \frac{17}{8} = \lambda ^2 $$ And thus $$ \lambda = \pm \sqrt{ \frac{17}{8} } $$ $x = \frac{1}{2\lambda}, z = \frac{2}{\lambda} $ Yields $$ x + 4z = \frac{1}{2\lambda} + 4 \frac{2}{\lambda} = \frac{17}{2 \lambda} 
 = \pm \sqrt{17} \sqrt{2} = \pm \sqrt{34}$$ Clearly $-\sqrt{34}$ is smaller, so we opt for that as our solution. Now while this works, and makes sense, its not satisfactory as it TAKES SO LONG. And on a Math GRE where the expectation is to do this under 30 seconds a problem, I was hoping there was a faster method. Any suggestions? [Also open to ways to speed up the process, since even the same method with a different angle might be superior]","['multivariable-calculus', 'lagrange-multiplier']"
2684793,Weak derivative of the Weierstrass function?,"The Weierstrass function is a function that is continuous everywhere but nowhere differentiable. I'm wondering if it has 1-th weak derivative. According to a book I'm reading, $u(x_1,x_2) = f(x_1) + f(x_2)$ defined in $\Omega = (0,1) \times (0,1)$, where $f$ is the Weierstrass function, actually has 2-th weak derivative. We have
$$0 = \int_\Omega(f(x_1)+f(x_2))\frac{\partial^2 \phi}{\partial x_1 \partial x_2}dx
_1dx_2$$ for all $\phi \in C_c^\infty (\Omega)$. Hence weak derivative $D_{x_1}D_{x_2}u = 0.$ So does weak derivate $D_{x_1}u$ exist?","['functional-analysis', 'partial-differential-equations']"
2684801,Derivative product rule $\frac{d}{dx}\sqrt{xe^{-3x}}$,"I have this question in one of my textbooks and no matter what approach I take to solving it the answer is always wrong $\frac{d}{dx}\sqrt{xe^{-3x}}$ It would be greatly appreciated if someone could explain it to me. ps, I'm new to math stack exchange, so please tell me if am asking the right kind of questions. Thanks :) The textbook says the answer is: $\frac{e^{-3x}}{2{\sqrt{x}}}-3{\sqrt{xe^{-3x}}}$","['derivatives', 'calculus']"
2684867,Prove that $AB=BA$,"Let $E$ be a complex Hilbert space Let $A,B\in \mathcal{L}(E)^+$. Assume that there exists $z\in \mathbb{C}^*$ such that $AB=zBA$. Why
  $$AB=BA\;?$$",['functional-analysis']
2684877,Differentiation under the Integration Sign,"I have difficulty with the following question: My understanding is that I can partially differentiate the integral with respect to any parameter (in this case, a or b) to obtain a derivate of the initial function. My problem is that when I partially differentiate the integral with respect to a or b, I obtain the following: I am unsure how I could obtain anything useful by evaluating either integrals (and then integrating that result to obtain $g(x)$). Any help would be much appreciated.","['derivatives', 'integration']"
2684925,$\exists g\in G: H\cap gPg^{-1}$ is a Sylow $p-$subgroup of $H$,"Question: If $P$ is a Sylow $p-$subgroup of $G$ and $H\leq G$ with $p||H|$ then $\exists g\in G: H\cap gPg^{-1}$ is a Sylow $p-$subgroup of $H$. Attempt: We consider the action $H\times G/P\to G/P$ with $(h,xP)\to hxP$ and we know that 
$$|G/P|=\sum_{x\in S}|[xP]_{H}|=\sum_{x\in S}|H:H\cap xPx^{-1}|$$
since $|[xP]_{H}|=|H:Stab_H(xP)|=|H\cap xPx^{-1}|$. Now, $P$ is a Sylow $p-$subgroup of $G$ so $p\nmid |G/P|=m$ so from above $\exists g\in G:p\nmid |H:H\cap gPg^{-1}|$ If $|H:H\cap gPg^{-1}|\not=1$: we have that $p\nmid |H:H\cap gPg^{-1}|$ and $H\cap gPg^{-1}$is a $p-$subgroup of $H$( please explain this statement ) so $H\cap gPg^{-1}$ is a Sylow $p-$subgroup of $H$. If $|H:H\cap gPg^{-1}|=1$ then $H=H\cap gPg^{-1}\Rightarrow H\leq gPg^{-1}(= $$p-$group) and therefore $H\cap gPg^{-1}$ is a Sylow $p-$subgroup of $H$. Is this proof correct? Is there a quicker way to prove the initial statement?","['abstract-algebra', 'proof-verification', 'group-actions', 'proof-explanation', 'group-theory']"
2684992,Finding an element $x$ of a Hilbert space with $\|Tx\|=\|T\|$,"Let $T:X\to Y$ be a bounded linear operator between Hilbert spaces $X$ and $Y$. Is there always some $x \in X$ with $\|x\|_X=1$, such that $\|Tx\|_Y = \|T\|$? I am guessing this is not always the case, but can't think of an example where this does not happen.","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2685006,"Let $f,g$ be functions such that $(g\circ f)(x)=x$ for all $x \in D(f)$ and $(f\circ g)(y)=y$ for all $y \in D(g)$, prove that $g=f^{-1}$","Attempt: I need to basically show that:
$$D(f)=R(g)\\
D(g)=R(f)$$ We can clearly infer the following from the information given above:
$$\begin{align}
D(f)=R(g\circ f) \subseteq R(g) \tag{1}\\
D(g)=R(f\circ g) \subseteq R(f) \tag{2}
\end{align}$$ And we know trivially from the definition of function composition,
$$\begin{align}
R(f) \subseteq D(g) \tag{3}\\
R(g) \subseteq D(f) \tag{4}
\end{align}$$ From $(1),(2),(3)$ and $(4)$, we get out intended result. Is this proof correct? I'd like to know if there is any other way of proving this.","['elementary-set-theory', 'functions']"
2685022,"Applying a formula like $\mathbb{E}[ X \vert \sigma(\mathcal{F},\mathcal{G}) ] = \mathbb{E}[X \vert \mathcal{G}]$","Let $(Y_1, \ldots, Y_n)$ be a $[0,1]^n$-valued random vector and $U_1, \ldots, U_n$ independent random variables, uniformly distributed on $[0,1]$, and independent of $(Y_1, \ldots, Y_n)$. For some fixed $j \in \{1, \ldots, n\}$ consider the random variable $X := \mathbb{1}_{\{U_j \leq Y_j\}}$. Then I want to show that
$$
\mathbb{E}[X  \vert \sigma(Y_1, \ldots, Y_n)] =
\mathbb{E}[X  \vert  \sigma(Y_j)].
$$
To prove this, I thought I can use the result, that if 
$\mathcal{F}$ is independent of $\sigma(\mathcal{G},\sigma(X))$, then
$$
\mathbb{E}[ X   \vert  \sigma(\mathcal{F},\mathcal{G}) ] = \mathbb{E}[X  \vert  \mathcal{G}].
$$
Hence we set $\mathcal{G} : = \sigma(Y_j)$ and $\mathcal{F} : = \sigma(Y_i \colon i \in \{1, \ldots, n\} \setminus \{j\} )$. Then $\sigma(\mathcal{F},\mathcal{G}) = \sigma(Y_1, \ldots, Y_n)$. Now it remains to show that $\mathcal{F}$ is independent of 
$$
\sigma(\mathcal{G},\sigma(X)) = \sigma(Y_j,\mathbb{1}_{\{ U_j \leq Y_j \}}).
$$
But I only know that $U_j$ is independent of $\mathcal{F}$. I don't know anything about $Y_j$ and $Y_i$ for $i \neq j$. How can I safe this?","['probability-theory', 'conditional-expectation', 'probability', 'random-variables']"
2685104,Diagonalizability of elements of finite subgroups of general linear group over an algebraically closed field,"How to show that every element of $G$, where $G$ is a finite subgroup of $GL_n(\mathbb{k})$, the general linear group of square matrices of order $n$ over some algebraic closed field $\mathbb{k}$, is diagnonalizable if $\mathbb{k}$ is an algebraic closure of $\mathbb{Q}$? I know that a matrix is diagonalizable if its minimal polynomial is seperable in the field on which the matrix is defined. Now, what if the minimal polynomial has repeated roots? How do we ensure the diginalizability? Any hints. Thanks beforehand.","['linear-algebra', 'linear-groups']"
2685111,Ways to determine how fast a sequence diverge/converge [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question I have recently read about divergent/convergent sequences, and I wonder what methods are normally used to show how fast a sequence grow. Can I simply find the difference between two adjacent terms? And how can I do this for sequences like harmonic series? 
Thank you! Here is an example, for a sequence that can be written as an iterated function $$ x_{n+1}=\frac{1}{\sqrt{x_n^2+1}} $$ How can I determine how fast this series grow?","['convergence-divergence', 'divergent-series', 'sequences-and-series', 'analysis']"
2685169,Curvature tensor in terms of the metric tensor,"I have been asked to express the curvature tensor in terms of the metric tensor, with $\mathcal{R}$ being the Riemann curvature tensor, given in terms of the Christoffel symbols of the second kind
$${\mathcal R}{^\rho_{\sigma\mu\nu}}=\partial_\mu\Gamma^\rho_{\nu\sigma}-\partial_\nu\Gamma^\rho_{\mu\sigma}+\Gamma^\rho_{\mu\lambda}\Gamma^\lambda_{\nu\sigma}-\Gamma^\rho_{\nu\lambda}\Gamma^{\lambda}_{\mu\sigma}$$
and this at the same time given as
$$\Gamma^\mu_{\nu\lambda}=\frac{1}{2}g^{\mu\kappa}\left[ \partial_\nu g_{\lambda\kappa} +\partial_\lambda g_{\nu\kappa} -\partial_\kappa g_{\nu\lambda}\right]$$
I have found this series of formulas in wikipedia, it starts from the top form of the curvature tensor I provided and then goes on to lower the top index with $g_{\gamma\rho}$ and treats it as if it were straighforward. However,  I have, per example, expanded the two terms with first partial derivatives, like:
\begin{aligned}
\partial_\mu\Gamma^\rho_{\nu\sigma}&=\frac{1}{2}\partial_\mu\left[ g^{\mu\kappa}\partial_\nu g_{\lambda\kappa}+g^{\mu\kappa}\partial_\lambda g_{\nu\kappa}-g^{\mu\kappa}\partial_\kappa g_{\nu\lambda}  \right]\\
& \begin{split}=\frac{1}{2}&\left[ \partial_\mu g^{\mu\kappa}\partial_\nu g_{\lambda\kappa}+g^{\mu\kappa}\partial_\mu\partial_\nu g_{\lambda\kappa} +\partial_\mu g^{\mu\kappa}\partial_\lambda g_{\nu\kappa}\right.\\
&\left.+g^{\mu\kappa}\partial_\mu\partial_\lambda g_{\nu\kappa}-\partial_\mu g^{\mu\kappa}\partial_\kappa g_{\nu\lambda}-g^{\mu\kappa}\partial_\mu\partial_\kappa g_{\nu\lambda} \right] \end{split}
\end{aligned}
yet from taking together the two expansions, only 6 terms with second partial derivatives show up (no second partials come from the Christoffel symbols), as opposed to the 8 that should show up. 
How can I prove the Wikipedia identity? Can I get the lowering metric tensor inside the derivative (as in $\partial_\mu g_{\lambda\rho}\Gamma^{\rho}_{\nu\sigma}$), if so, what would I get? I've never seen a Christoffel symbol contracted with a metric tensor, since once getting the new tensor inside the symbol expansion, I have the same question, can I get the metric tensor inside the partial? Any help will be greatly appreciated.","['riemannian-geometry', 'differential-geometry', 'general-relativity']"
2685182,Finding general formula for $\cos^{-1}({\cos{x})}$,"My teacher teacher told me that for a general angle $x$, $\cos^{-1}({\cos{x})}$ does't represent $x$ but different straight lines depending upon the intervals in which it lies. For ex: $$\cos^{-1}{\cos{x}}=$$
$$x,0\leq x \leq \pi \\ 2\pi-x,\pi\leq x \leq 2\pi\\…$$
making the graph look like :- From wolfram alpha He told us that if we have to find the value of $\cos^{-1}({\cos{x})}$ for a particular $x$ we will have to first find the range in which $x$ lies and then judge with the help of graph but I wondered if there is a direct formula for that. I tried with $\tan^-1({\tan{x}})$ and got it as :- from wolfram alpha I even verified this with wolfram alpha and got it right but the problem with $\cos$ is that when I try to solve it similarly like I did with the $\tan$ one, and get the interval in which $n$ lies, the extremities of the interval differ by $0.5$ because of which for some values their floor and ceiling match but for some values there isn't an integer value lying in that interval like this :- from wolfram alpha so what to do in that case and what does no value of $n$ lying in the interval signify? Thanks for help :)","['trigonometry', 'inverse-function']"
2685199,"The separability of $c, c_0$ and $c_{00}$","There are three spaces of real sequences given: 1) $c$ - a space of convergent sequences, 2) $c_0$ - a space of sequences such that $\forall x_n\in c_0,\ \lim_{n \to\infty} x_n = 0$, 3) $c_{00}$ - a space of sequences with finite amout of numbers different than $0$. With the supremum norm:
$$\sup_{k\in \mathbb{N}}|x_k^n - x_k^m|$$ I am to show whethere they are separable spaces or not. I think that it will be enough to show that $c_0$ is separable, won't it? I think so because $c_{00} \subset c_0 \subset c$. However I don't really know how to show the separability of $c_{00}$. I think it must be something related to rational numbers but I don't know where to go from there.","['functional-analysis', 'sequences-and-series', 'separable-spaces']"
2685217,"""relative frequency distribution"" of function values","Let $\ f(x):U\longrightarrow\mathbb{R}$ be a continuous real-valued function over a closed interval $U\subseteq\mathbb{R}$. I would like to define a ""relative frequency distribution"" function, $\ \mathcal{F}:\mathbb{R}\longrightarrow[0,1]$ which measures in some sense ""how often"" the function $f$ takes a value $y$. The idea is made more precise as follows. Let $\{J_k\}$ be a family of disjoint intervals that covers the reals:
\begin{equation}
\bigcup_{k\in\mathbb{Z}}J_k=\mathbb{R}\qquad J_i\ \cap J_j = \emptyset\quad\text{if}\quad i\neq j
\end{equation}
Let also $y_k\in J_k$ be a value in each interval. Define
\begin{equation}
F(y_k):=\frac{\lambda[f^\leftarrow(J_k)]}{\lambda[U]}
\end{equation}
where $\lambda$ is the standard (Lebesgue) measure. Finally, I'd like to ""define"", with a lot of handwaving, \begin{equation}
\mathcal{F}(y) :=\lim_{\lambda[J_k]\rightarrow 0}F(y_k)
\end{equation} Of course, this definition makes no sense because $y$ is not well defined (how to pick $y$ as the interval $J_k$ becomes smaller?) and also because it all collapses to zero...but I hope that the sense of it is clear. My question is: how could I formally define the function $\mathcal{F}$? The discrete version of the idea works because it's all about counting how many points lie in the preimage of each $y$, but I don't know how to extend it to the continuous case, or even if it's possible.","['statistics', 'measure-theory', 'functions']"
2685220,Divide a finite measure set into two part of equal measure using one hyperplane in R^n.,"The term ""measure"" here are all refer to Lebesgue measure. A is a measurable set in $\mathbb R^n$ with finite measure, the goal is to prove that there exists a real number $r$ with $m(A \cap\{x:x_n<r\})=m(A\cap\{x:x_n>r\})$, where $x=(x_1,...,x_n)$ in $\mathbb R^n$. I've tried to prove that measure of finite measure sets is a continuous function and use intermediate value theorem. I've done this for closed cubes, but encountered some problem when trying to prove this for general measurable sets. Also, I was wondering if there's a simpler way to do this. Any help or hint are very appreciated.","['lebesgue-measure', 'measure-theory']"
2685246,Proof of the functional equation $\xi(s)=\xi(1-s)$,"We know that 
$$\xi(s)=\frac{s(s-1)\pi^{\frac{{-s}}2}}{2}\Gamma\left(\frac{s}{2}\right)\zeta(s)$$ I want to prove the functional equation
$$\xi(s)=\xi(1-s)$$
I could not find it anywhere. My approach is to show that this equality holds:
$$\frac{s(s-1)\pi^{\frac{{-s}}2}}{2}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\frac{(1-s)(-s)\pi^{\frac{{s-1}}2}}{2}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)$$
We can get rid of the polynomials straight away because they are clearly the same:
$$\pi^{\frac{{-s}}2}\Gamma\left(\frac{s}{2}\right)\zeta(s)=\pi^{\frac{{s-1}}2}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)$$ Now, since $\zeta(s)=2^s\pi^{s-1}\text{sin}\left(\frac{\pi s}{2}\right)\Gamma(1-s)\zeta(1-s)$
we have $$\pi^{\frac{{-s}}2}\Gamma\left(\frac{s}{2}\right)2^s\pi^{s-1}\text{sin}\left(\frac{\pi s}{2}\right)\Gamma(1-s)\zeta(1-s)=\pi^{\frac{{s-1}}2}\Gamma\left(\frac{1-s}{2}\right)\zeta(1-s)$$
Dividing through $\zeta(1-s)$
$$\pi^{\frac{{s-2}}2}\Gamma\left(\frac{s}{2}\right)2^s\text{sin}\left(\frac{\pi s}{2}\right)\Gamma(1-s)=\pi^{\frac{{s-1}}2}\Gamma\left(\frac{1-s}{2}\right)$$
Using Euler's reflection formula $\Gamma(z)\Gamma(1-z)=\frac{\pi}{\texttt{sin}(\pi z)}$ we have$$\pi^{\frac{{s}}2}2^s\frac{\Gamma(1-s)}{\Gamma\left(1-\frac{s}{2}\right)}=\pi^{\frac{{s-1}}2}\Gamma\left(\frac{1-s}{2}\right)$$ At this point I am not sure what to do next. Wolfram seems to agree with me hence perhaps maybe this is the way forward. Could anyone help me out how to finish this? or maybe give me completely new proof, I do not mind either way.","['complex-analysis', 'riemann-zeta', 'gamma-function']"
2685311,Transformation of a sufficient statistic,"I want to prove that if $T$ is a sufficient statistic for $\theta$, then any transformation one-to-one, lets say $\phi=g(T)$ is also sufficient for $\theta$, I suppose is easier to do with the factorization theorem.","['probability-theory', 'statistics', 'statistical-inference']"
2685320,Axiom of Dependent Choice implies Axiom of Countable Choice [Proof Verification],"I have searched over the forum but not found any similar question, so I post it here. Axiom of Dependent Choice Let $T \neq\varnothing$ and $\mathcal{R} \subseteq T^2$ such that $\forall a \in T, \exists b \in T: a\mathcal{R}b$. Then there exists $(x_n \mid n \in \mathbb N)$ such that $x_n \mathcal{R} x_{n+1}$. Axiom of Countable Choice Let $(A_n \mid n \in \mathbb N)$ be a sequence of non-empty sets and $X=\bigcup_{n \in \mathbb N} A_n$. Then there exists a mapping $f: \mathbb N \to X$ such that $f(n) \in A_n$. My proof that Axiom of Dependent Choice implies Axiom of Countable Choice: Let $T=\{s: n \to X \mid n \in \mathbb N \text{ and } \forall k < n:s(k) \in A(k)\}$ and $\mathcal{R}=\{(u,v) \in T^2 \mid u \subsetneq v\}$. Assume $s: n\to X$ such that $s\in T$. Let $s': n+1\to X$ such that $s'(x)=s(x)$ for all $x<n$ and $s'(n)\in A(n)$. Then $s'\in T$ and $s\subsetneq s'$. That is for all $s \in T$, there exists $s' \in T$ such that $s \mathcal{R} s'$. As a result, $\mathcal{R}$  satisfies the requirement of DC. Hence there is a sequence $(s_n \mid n \in \mathbb N)$ through $T$ such that for all $m \le n \in \mathbb N \colon s_m \subseteq s_n$. Let $f=\bigcup_{n \in \mathbb N} s_n$. Now we prove $f$ is the desired function. 1. f is a function Let $(k,a),(k,b) \in f$. Then $\exists u, v \in (s_n \mid n \in \mathbb N)$ such that $ (k,a) \in u, (k,b) \in v$. Assume $u \subsetneq v$. This implies $(k,a) \in v$. $(k,a),(k,b) \in v \implies a=b$. Thus $f$ is a function. 2. $\mathrm{dom}(f)=\mathbb N$ $\forall s \in T: 0 \in \mathrm{dom}(s) \implies 0 \in \mathrm{dom}(f)$. Assume $n \in \mathrm{dom}(f)$. This implies $\exists s_{t} \in (s_n \mid n \in \mathbb N): n \in \mathrm{dom}(s_{t})$. On the other hand, $s_{t} \subsetneq s_{t+1} \implies n+1 \in \mathrm{dom}(s_{t+1}) \implies n+1 \in \mathrm{dom}(f)$. Thus $\mathrm{dom}(f)=\mathbb N$. 3. Condition $f(n) \in A_n$ Let $(n,f(n)) \in f$. Then $\exists s_{t} \in (s_n \mid n \in \mathbb N): (n,f(n)) \in s_{t}$. $s_{t}(n) \in A_n \implies f(n) \in A_n$. Please check if my above proof is correct. Many thanks for your help!","['axiom-of-choice', 'elementary-set-theory', 'proof-verification']"
2685441,Cayley-Hamilton theorem to compute this.,"$ A = \pmatrix{0&-3&0\\3&0&0\\0&0&-1}$ Compute the $e^{At}$.
Well, the first problem of this is to calculate the inverse of $A$ using Cayley-Hamilton theorem. But for this second problem, I don't know how to solve it, should I use the Cayley-Hamilton theorem?","['cayley-hamilton', 'linear-algebra']"
2685455,Must a surjective continuous map on a compact space into itself be injective?,"Let $X$ be a compact space and $f:X\to X$ a self map on that space.
Suppose that $f$ is continuous and surjective. Is it then also injective? Without the compactness condition not necessarily, but is compactness sufficient? If not, what else needs to be demanded?",['general-topology']
2685496,Proving the Lebesgue measure is $\sigma$-finite for any dimension,"I understand how the Lebesgue measure on the real line is $\sigma$-finite, but I don't understand how to prove that the lebesgue measure is $\sigma$-finite for any dimension. Here I'm using the definition of $\sigma$-finite as follows; A measure $\mu$ on a measurable space ($X$,$\Sigma$) is $\sigma$-finite if there exists a sequence of measurable sets $E_1$,$E_2$,... $\in$ $\Sigma$ such that $X$ = $\cup^{\infty}_{k=1}$$E_k$ and $\mu (E_k)$ < $\infty$ for every $k \geq 1$. Thank you to anyone who could explain this to me!","['lebesgue-measure', 'measure-theory']"
2685516,"In mathematics, is there a theory that deals with the distribution of numbers?","Firstly, I have a hard time expressing my question. (English is my second language and I have no math education. If you know what I mean, please edit the question.) Suppose, $a_{1,n} ; a_{2,n} ; a_{3,n} ;a_{4,n}; ...$ series are given. All the elements included in these series are Natural Numbers: $\left\{a_{1,n} ; a_{2,n} ; a_{3,n} ;a_{4,n}; ....\right\}\in \mathbb{Z^{+}}.$ I would like to give an example before asking my question. $a_{1,n}= \left\{1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,... \right\}$ $a_{2,n}=\left\{1,3,4,5,6,7,8,9,13,15,17,19,20...\right\}$ $a_{3,n}=\left\{1,5,7,9,11,14,19,20...\right\}$ We see that, for sequence $a_{1,n}$ the distribution of numbers is more ""orderly""  than for sequence $a_{2,n}.$ For sequence $a_{2,n}$ the distribution of numbers is more ""orderly"" for sequence $a_{3,n}.$ By ""orderly"", I mean the denser settlement of the positive integer numbers. We can define the sequence of $a_{1,n}$ as the ""most orderly"" sequence. For example: The numbers in sequence $a_{4,n}$ are more ""orderly distributed"" than from sequence $a_{5,n}.$ $a_{4,n}=\left\{1,2,3,4,5,6,7,8,9,11,13,15,17,19,21,23,25 ...\right\}$ $a_{5,n}=\left\{1,7,9,13,17,19,21,23,25...\right\}$ How can I say that the sequence of $a_{3,n}$ is more ""orderly"" than the  sequence of $a_{5,n}?$ I can not say which sequences/series are more ""orderly"" , because I have no mathematical criterion to do this. Is there a theory that deals with the distribution of positive integer numbers? For example, which sequence of numbers are more ""orderly distributed?""","['number-theory', 'natural-numbers', 'sequences-and-series']"
2685566,Non linear differential equation $\sin(xy) =\frac {dy}{dx}$,A friend of mine set the question $\sin(xy) =\dfrac {dy}{dx}$ but after quite a long time I have made no headway (so far I have tried $v=xy$ and the solving it as a $1$ st order differential equations and also differentiating and trying to solve as a second order ( $3$ rd order makes things even messier). If anyone is able to tell me whether this is doable that would be fantastic! Edit: he said he had a closed form solution but if there was an infinite sum solution that would also be fine Edit a lot lot later. It turned out he was trolling me and the equation is not solvable,"['ordinary-differential-equations', 'calculus']"
2685611,"In group theory, what is meant by the structure of a group?","In group theory, what is meant by the structure of a group?
I have an intuitive idea of this (well, I think). e.g.: the structure relates to the operation of the group and its set. But is there a formal definition of the idea of structure? or it's really just a qualitative term to describe a group?  (and, in fact, many other mathematical objects, like Vector spaces, etc...)",['group-theory']
2685656,Quotient of the torus being homeomorphic to Mobius strip?,"I'm considering X to be the space $[-1,1] \times [-1,1]$, with left glued to right and top glued to bottom, and points symmetric with respect to the diagonal glued to each other. But I still can't visualize the final shape of X. Is that a cone? And I don't know how this shape could be mapped to a Mobius strip?",['general-topology']
2685671,"Can one define a uniform measure on $(\Omega = \{0,1\}^\mathbb{N}, P(\Omega))$","Can one define on the measurable space $(\Omega = \{0,1\}^\mathbb{N}, P(\Omega))$
a measure $\mathbb{P}$ uniform in the sense that sets of sequences with $k$ fixed values should have measure $\dfrac{1}{2^k}$. Note that I require that the measure be defined on all of $P(\Omega)$. I also assume the axiom of choice to be true. My guess would be it's not possible as my intuition was that since $\Omega$ is in bijection with $\mathbb{R}$, the measurable space looks a bit like $(\mathbb{R}, P(\mathbb{R}))$ which I think cannot have a translation invariant measure supporting Vitali sets for example. I read this question: Uniform probability measure on $\{0,1\}^\omega$ but the link seems dead. Does Kolmogorov theorem indeed provides a positive answer to my question? I saw the other simple construction from Lebesgue measure but it supports only Lebesgue measurable sets, not all of $P(\Omega)$ as far as I understand.","['probability-theory', 'measure-theory']"
2685689,Does taking a fractional derivative remove a fractional amount of Holder regularity?,"We define the space $C^{n+\alpha}$ as functions who are $n$ times differentiable whose $n$th derivative is $\alpha$ Holder. That is, each time we take a derivative we remove one number of regularity. When functions have no more regularity to give to take a derivative we get to negative $C$ spaces. I.e. it can be shown that a function that is $C^{-.5}$ is the weak/distributional derivative of a $C^{.5}$ function. How far can this be generalized? Can this be generalized to fractional derivatives? Meaning if we take a $.5$ derivative of a $C^{.9}$ function do we get a $C^{.4}$ function? Recall that we define $D^\alpha f(x)=\frac{1}{\Gamma(1-\alpha)}\frac{d}{dx}\int_0^x\frac{f(t)}{(x-t)^\alpha}dt$. This is true for $f(x)=x^{\alpha}$, which is $C^{\alpha}$. Fractional derivatives give $cx^{\alpha-\beta}$ which is $C^{\alpha-\beta}$.","['fractional-calculus', 'real-analysis', 'holder-spaces']"
2685713,"If $\sin(x)=3\cos(x)$, compute $\sin(x)*\cos(x)$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I drew a triangle and was instructed to use a property unknown to me. Apparently the answer is $\frac{3}{\sqrt{10}}*\frac{1}{\sqrt{10}}=\frac{3}{10}$. Is this answer correct, and what theorem/formula is this?",['trigonometry']
2685744,Where does the Jordan canonical form show up in more advanced mathematics?,"My bounty for this question expires soon :) Edit: in regards to the bounty offered, what current research trends use the Jordan canonical form ? If one takes a second course in Linear Algebra — or a graduate level Linear Algebra course — one typically learns about non-diagonalizable operators and the Jordan canonical form. However, where does the Jordan canonical form show up again in later, more advanced mathematics?  All I hear from applied mathematicians is that the Jordan canonical form is useless in practice (in academic research).  If it's not useful in applied mathematics, is it an important tool in pure mathematics?  If so, in which areas of pure mathematics?","['jordan-normal-form', 'matrices', 'abstract-algebra', 'research', 'linear-algebra']"
2685773,How to find the probabilities that maximize the standard deviation?,"Consider the problem: ""one dice has three faces $1,2,3$ with probabilities $p_1,p_2,p_3$ and it is thrown many times with a mean value $a$. What are the values for $p_1,p_2,p_3$ which maximizes the uncertainty consistent with the mean value $a$?"" Here I believe uncertainty should be understood as standard deviation. So, the experiment for a single such dice being thrown has outcomes described by the random variable $X$ taking values on $\{1,2,3\}$ with probabilites $p_1,p_2,p_3$ respectively. I believe that it all boils down to maximizing $$\sigma^2(p_1,p_2,p_3)=E[X^2](p_1,p_2,p_3)-E[X]^2(p_1,p_2,p_3)$$ subject to the constraints $$g(p_1,p_2,p_3)=p_1+p_2+p_3=1\quad h(p_1,p_2,p_3)=E[X](p_1,p_2,p_3)=p_1+2p_2+3p_3=a.$$ Thus it seems we should use Lagrange multipliers and solve $$\nabla \sigma^2=\lambda \nabla g+\mu\nabla h,\\ g(p_1,p_2,p_3)=1, \\ h(p_1,p_2,p_3)=a.$$ Furthermore, it is clear that we have $$\dfrac{\partial}{\partial p_i}E[f(X)]=\dfrac{\partial}{\partial p_i}\sum_j f(x_j)p_j=f(x_i).$$ so that we have $$\nabla \sigma^2(p_1,p_2,p_3)=(1-2E[X],4-4E[X],9-6E[X]).$$ Thus we have the system of equations $$\begin{cases}1-2E[X]&=\lambda+\mu \\ 4-4E[X]&=\lambda+2\mu\\ 9-6E[X]&=\lambda+3\mu\\ p_1+2p_2+3p_3&= a \\ p_1+p_2+p_3&= 1.\end{cases}$$ The fourth equation imposes $E[X]=a$. Hence substituting it on the first and second we can find $\mu = 3-2a$ and $\lambda = -2$. But now this ought to be wrong. If I substitute this on the third equation I get $$9-6E[X]=-2+9-6a\Longleftrightarrow 6E[X]=6a+2$$ which is incompatible with the fourth equation which tells that $E[X]=a$. So this method doesn't work. What am I doing wrong here? What is the right way to solve this problem?","['probability-theory', 'probability', 'statistics', 'problem-solving']"
2685842,Golden Rectangle into Golden Rectangles,"Can these golden rectangles be rearranged to exactly cover the underlying cyan  golden rectangle? That's the entire question.  All that follows is related discussion. I want to make a more elegant proof without words. It comes from the series: $$\sum_{n=1}^{\infty} \phi^{-n} = \phi$$ The golden rectangle has a well known dissection into squares to make the golden spiral. I'm wondering if the golden rectangle can be divided into smaller golden rectangles with areas $(\phi^{-1}, \phi^{-2},\phi^{-3}, \phi^{-4}, \phi^{-5})$ , ... . Half the rectangles have edges of the form $\phi^{-n}(\sqrt{\phi} \times \sqrt{1/\phi})$ , the other half have edges of the form $\phi^{-n}(1 \times \phi)$ . These two series make squares with areas $1/\phi$ and $1$ , as shown in the opening graphic. Note that the golden triangle can be divided into golden gnomons with those exact areas. Also, the golden gnomon can be divided into golden triangles with those exact areas, as shown below. If we solve the following instead, $$\sum_{n=2}^{\infty} x^{-n} = x$$ we get $x = 1.46557123187...$ , the real root of $x^3-x^2-1=0$ , which is the Narayana cow sequence constant.  In 1356, Narayana asked ""A cow gives birth to a calf every year. In turn, the calf gives birth to another calf when it is three years old. What is the number of progeny produced during twenty years by one cow?"". It turns out I could make a proof without words with the infinite cow fractal . But it wasn't until today that I thought to  add the summation to the picture. So then I tried making the golden version, and found it had apparently never been done, and seemed to be hard to solve. Is there a similar picture for the golden rectangles with a rearrangement of the rectangles in the first image above? Basing the rectangle dissection on Ammann's Chair doesn't seem to help, but their are nice ideas there. This is related to New Substitution Tilings Using 2, φ, ψ, χ, ρ .","['golden-ratio', 'polygons', 'tiling', 'recreational-mathematics', 'sequences-and-series']"
2685860,Mistake in proof of $\sin(a+b)=\sin(a)\cos(b)+\cos(a)\sin(b)$,"I'm trying to prove the identity $$\sin(a+b)=\sin(a)\cos(b)+\cos(a)\sin(b)$$ by using a specific vector and scalar method. However, there has to be a mistake somewhere, since the equation at the end isn't correct. Such mistake I can't find. I would really appreciate any help/thoughts.","['proof-writing', 'trigonometry', 'vectors', 'geometry']"
2685873,Solving Laplace Equation with two dielectrics in cylindrical coordinates,"Suppose a concentric cylinder of height $c$ and radius $b$ with it's top cap held at $V=V_0$ and all other surfaces held at $V=0$. For calculating the potential inside the cylinder I use the Laplace equation because there is no free charge. For solving the Laplace equation in cylindrical coordinates I use a product approach:
\begin{align*}
\varphi(r,z,\phi)=\mathcal{R}(r) \cdot \mathcal{Z}(z) \cdot \Psi(\phi)
\end{align*}
With the followings solutions:
\begin{align*}
\mathcal{Z} &\propto e^{\pm \, k z} \\
\Psi &\propto e^{\pm \, i \nu \phi} \\
\mathcal{R} &\propto J_{\nu}(k r) \ \mathrm{or} \ N_{\nu}(k r)
\end{align*}
Where $J_{\nu}$ are the Bessel functions and $N_{\nu}$ are the Neumann functions. Our boundary conditions are:
\begin{align*}
\varphi(r,z=c)&=V_0 \\
\varphi(r,z=0)&=0 \\
\varphi(r=b,z)&=0
\end{align*}
Because of rotational symmetry we can find $\nu=0$ and therefore we can neglect the $\Psi$ component. Now following the calculation from the following lecture on page 4 we end up with the solution:
\begin{align*}
\varphi(r,z) = \sum_{n=0}^{\infty} A_n \, J_0(k_n r) \, \sinh(k_n z) \\
A_n = \frac{V_0}{\frac{b^2}{2} [J_1(k_n b)]^2 \, \sinh(k_n c)} \int_0^b r \, J_0(k_n r) \, dr
\end{align*}
($A_n$ is obtained by making use of the orthogonality of the Bessel functions, see page 3.) I have already checked this solution numerically and it's correct. So far so good. But now suppose the same cylinder, filled with a dielectric material of $\epsilon=\epsilon_1$ from $r=0$ to $r=a$ and another dielectric material with $\epsilon=\epsilon_2$ from $r=a$ to $r=b$. Now the problem becomes more complicated and we get an additional boundary condition:
\begin{align*}
\varphi(r,z=c)&=V_0 \quad \quad (1) \\
\varphi(r,z=0)&=0 \quad \quad (2)  \\
\varphi(r=b,z)&=0 \quad \quad (3)  \\
\varphi(r=a,z)&=f(z) \quad \quad (4)  \\
\end{align*}
To solve this problem I have tried two different approaches with yet no success. In both approaches I divided the solution into two solution. $\varphi_1$ in the area of $\epsilon_1$ and $\varphi_2$ in the area of $\epsilon_2$. First approach goes as follows: Approach 1 $\Psi_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}_{1,2}$: Due to boundary condition (1) and (2) $\mathcal{Z}=\sinh(k_{1n} z)$ $\mathcal{R}_{1 \ }$: Because $N_0(0) = \infty$ , $\mathcal{R}_{1}=J_0(k_{1n} r)$ $\mathcal{R}_{2 \ }$: We combine Bessel and Neumann functions so that that they hold boundary condition (3) $\rightarrow$  $\mathcal{R}_{2} = G_0 = \frac{J_0(k_{1n} r)}{J_0(k_{1n} b)} - \frac{N_0(k_{1n} r)}{N_0(k_{1n} b)}$ The resulting solutions are:
\begin{align*}
\varphi_1  &= \sum_{n=0}^{\infty} A_{1n} \, \sinh(k_{1n} z) \, J_0(k_{1n} r) \\
\varphi_2  &= \sum_{n=0}^{\infty} A_{2n} \, \sinh(k_{2n} z) \, G_0(k_{2n} r) \\
\end{align*}
with
\begin{align*}
A_{1n} &= \frac{V_0}{\sinh(k_{1n} c) \frac{a^2}{2} \left( [J_0(k_{1n} c)]^2 + [J_1(k_{1n} c)]^2 \right)} \int_0^a r \, J_0(k_n r) \, dr \\
A_{2n} &= \frac{1}{\sinh(k_{2n} c) \int_a^b r \, G_0(k_{2n} r) \, dr} \int_a^b r \, G_0(k_{2n} r) \, V_0 \, dr = \frac{V_0}{\sinh(k_{2n} c)}
\end{align*}
Now the only two parameters undetermined are $k_{1n}$ and $k_{2n}$. To solve for the two unknowns we have the two continuity conditions: \begin{align*}
\varphi_1(r=a,z) &= \varphi_2(r=a,z) = f(z) \\
\epsilon_1 \frac{\partial \varphi_1(r,z)}{\partial r} \biggr\rvert_{r=a} &= \epsilon_2 \frac{\partial \varphi_2(r,z)}{\partial r} \biggr\rvert_{r=a}
\end{align*}
But it turns out, that the equations are just too complicated to solve for $k_{1n}$ and $k_{2n}$. So I got stuck at this point here. Approach 2 So for me the more promising approach is to use the superposition principle and devide the problem in one problem A with the boundary conditions:
\begin{align*}
\varphi(r,z=c)&=V_0 \quad \ (1A) \\
\varphi(r,z=0)&=0 \quad \quad (2A)  \\
\varphi(r=b,z)&=0 \quad \quad (3A)  \\
\varphi(r=a,z)&=0 \quad \quad (4A)  \\
\end{align*}
and another problem B with the boundary conditions:
\begin{align*}
\varphi(r,z=c)&=0 \quad \quad (1B) \\
\varphi(r,z=0)&=0 \quad \quad (2B)  \\
\varphi(r=b,z)&=0 \quad \quad (3B)  \\
\varphi(r=a,z)&=f(z) \ \  (4B)  \\
\end{align*}
If I solve both of these problems and add up their solutions, due to the superposition principle I get the solution I desire:
\begin{align*}
\varphi = \varphi_A + \varphi_B
\end{align*}
The solution to $\varphi_A$ is already given in the lecture I have posted above, see link on page 6-7. For $\varphi_B$ it goes as follows: $\Psi^B_{1,2}$: Rotational symmetry $\rightarrow$ $\nu=0$ and $\Psi=1$ $\mathcal{Z}^B_{1,2}$: Because neither $\sinh$ nor $\cosh$ can satisfy $\varphi(r,z=c)=0$ and $\varphi(r,z=0)=0$, $k$ has to become complex $k \rightarrow ik$ so that the $\sinh$ will become a $\sin$ with $k_n= i \frac{n \pi}{c}$: $\mathcal{Z}^B_{1,2}= \sin(\frac{n \pi}{c} z)$ $\mathcal{R}^B_{1 \ }$: Similar to the first approch we get:   $\mathcal{R}^B_{1} = J_0(i \frac{n \pi}{c} r)$ $\mathcal{R}^B_{2 \ }$: And $\mathcal{R}^B_{1} = G_0 = \frac{J_0(i \frac{n \pi}{c} r)}{J_0(i \frac{n \pi}{c} b)} - \frac{N_0(i \frac{n \pi}{c} r)}{N_0(i \frac{n \pi}{c} b)}$ The resulting solutions are:
\begin{align*}
\varphi_A = \sum_{n=0}^{\infty} A_{n} \, \sinh ( k_n z) \, g_0 (k_n r)
\end{align*}
with
\begin{align*}
A_n = \frac{V_0}{\sinh(k_n c)} \quad \quad \mathrm{and} \quad \quad g_0 = \frac{J_0(k_n r)}{J_0(k_n a)} - \frac{N_0(k_n r)}{N_0(k_n a)}
\end{align*}
and
\begin{align*}
\varphi_{B,1} &= \sum_{n=0}^{\infty} B_{1n} \, \sin \left( \frac{n \pi}{b} z \right) \, J_0 \left(i \frac{n \pi}{c} r \right) \\
\varphi_{B,2}  &= \sum_{n=0}^{\infty} B_{2n} \, \sin \left( \frac{n \pi}{b} z \right) \, G_0 \left(i \frac{n \pi}{c} r \right)
\end{align*}
with
\begin{align*}
B_{1n} &= \frac{1}{\frac{c}{2} J_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz \\
B_{2n} &= \frac{1}{\frac{c}{2} G_0 \left(i \frac{n \pi}{c} a \right)} \int_0^c f(z) \sin \left(\frac{n \pi}{b} z \right)  \, dz
\end{align*}
The big advantage to this approach is that $k_n$ is easily determined by finding the $k_n$ for which $g_0(b)=0$. But the big disadvantage is that there is no way, to get the $f(z)$ out of the definite integral in $B_{1n}$ and $B_{2n}$. Here $f(z)$ is the only unknown left. But by using the continuity condition I see no way to find out $f(x)$. From my numerical solution and also in approach 1, one can see, that $f(x)$ has to be a $sinh(...)$. Maybe this is a hint for you. I think there is no way to solve the equation by using the definite integral of $f(x)$ to express $B_{1n}$ and $B_{2n}$. In Germany we have a saying ""...den Wald vor lauter Bäumen nicht sehen."" -- ""...overlooking the forest due to too many trees."". And I think, and I hope that it is something obvious that I'm overlooking and I hope you can help me to find it. Also I am open for new proposals to solve this equation. Big thanks in advance! Pearli","['harmonic-functions', 'poissons-equation', 'cylindrical-coordinates', 'boundary-value-problem', 'ordinary-differential-equations']"
2685921,How to show $\sum_{n=1}^{\infty}\frac{H_{n+1}}{n(n+1)}=2$?,"I am interested in the proof of
$$\sum_{n=1}^{\infty}\frac{H_{n+1}}{n(n+1)}=2, \quad 
H_{n}=1+\frac{1}{2}+\cdots+\frac{1}{n}$$ This result can be verified by Mathematica or by WolframAlpha This series can be found as Problem 3.59 (a) in the book Ovidiu Furdui: Limits, Series, and Fractional Part Integrals . Problems in Mathematical Analysis, Springer, 2013, Problem Books in Mathematics; where it is stated in the form $$\sum_{k=1}^\infty\left(1+\frac12+\frac13+\dots+\frac1{n+1}\right)\frac1{n(n+1)}=2.$$ Some related thoughts: It is relatively easy to show that the series converges, the $n$-th term is approximately $\frac{\ln n}{n(n+1)}$. So we could use limit comparison test with the series $\frac1{n^\alpha}$ for any $\alpha\in(1,2)$. If the numerator is one, the series sums to $1$: How can I prove that $\sum_{n=1}^\infty \frac{1}{n(n+1)} = 1$? Here we expect larger result.","['harmonic-numbers', 'sequences-and-series']"
2685932,Prove $P(\bigcup_{i=1}^nA_i) \ge \sum_{i=1}^nP(A_i)-\sum_{1\le i \lt j \le n}P(A_i \cap A_j)$,"Let $A_1, A_2,...A_n$ be events. Prove $$P(\bigcup_{i=1}^nA_i) \ge \sum_{i=1}^nP(A_i)-\sum_{1\le i \lt j \le n}P(A_i \cap A_j)$$ I think this statement is intuitively obvious but I don''t know how to prove it.
I already tried to use induction and here is my work: Base case: $n=2$ $$P(A_1 \cup A_2)=P(A_1)+P(A_2)-P(A_1 \cap A_2)$$
Assume this is true for $n$.
I want to show $$P(\bigcup_{i=1}^{n+1}A_i) \ge \sum_{i=1}^{n+1}P(A_i)-\sum_{1\le i \lt j \le {n+1}}P(A_i \cap A_j)$$ and I'm stuck here.","['probability-theory', 'probability']"
2685950,When is $2^{2n+2}+2^{m+2}+1$ a perfect square?,"Let $m,n$ be natural numbers satisfying $m\leq 2n$. Is it true that $$2^{2n+2}+2^{m+2}+1$$ is a perfect square if and only if $m=n$? What I have tried: Under the assumption $m<n$, I've tried to 'squeeze' the above number between two consecutive squares, implying it cannot be a perfect square. This works fine because (writing $P(m,n)=2^{2n+2}+2^{m+2}+1$), we have $(2\cdot2^n)^2<P(m,n)<(2\cdot2^n+1)^2$. But this method doesn't work when $\frac{m}{2}\leq n<m$, and I wonder if there is any pair $(m,n)$ with $m\ne n$ making $P(m,n)$ a perfect square. Any advice is welcome.","['number-theory', 'square-numbers', 'elementary-number-theory']"
2685984,Probability Density Function for Gamma Distributions,"Shouldn't probability density functions be in the form of $$P(X\in dx) = \cdots$$ Why does the one for gamma distributions divide by $dt$? $T_r =$ time of $r^\text{th}$ arrival after time $0$ in a poisson arrival process with rate $\lambda$. And if I multiply both sides by $dt$, how am I supposed to calculate $dt$ on the right side?",['statistics']
2685990,"Proof verification that $\mu_f$($E$) $=$ $\int_E f\,d\lambda$ is a measure on $\mathcal M$.","I was wondering if I had correctly proved this problem:  Let $f$ be a nonnegative $\mathcal M$-measurable function.  Define $\mu_f$ on $\mathcal M$ by $\mu_f$($E$) $=$ $\int_E f\,d\lambda$.  Prove that $\mu_f$ is a measure on $\mathcal M$. Here is my solution: Condition (1): Since $f$ is nonnegative, $f$ $\ge$ $0$.  Also since $f$ is $\mathcal M$-measurable function, we get for each $E$ $\in$ $\mathcal M$ that $\int_E f\,d\lambda$$\ge$ $0$.  Hence, $\mu_f$($E$) $\ge$ $0$ for each $E$ $\in$ $\mathcal M$. Condition(2):  Let $E$ $=$ $\emptyset$.  Since $\lambda$($E$) $=$ $0$, we have that $\int_E f\,d\lambda$ $=$ $0$.  So, $\mu_f$($\emptyset$) = $0$. Condition (3):  Let $\{E_n\}_{n=1}^{\infty}$ be a sequence of pairwise disjoint sets in $\mathcal M$.   Now because we also have that $E_n$ $\in$ $\mathcal M$ for each $n$ $\in$ $\Bbb N$, and $f$ is a nonnegative $\mathcal M$-measurable function, we have that $\int_{\bigcup_{n=1}^{\infty}E_n}f{\rm d}\lambda$ $=$ $\sum_{n=1}^{\infty}\int_{E_n}f$.  Hence, $\mu_f$(${\bigcup_{n=1}^{\infty}E_n}$) $=$  $\sum_{n=1}^{\infty}$($\mu_f$($E_n$))",['measure-theory']
2686004,"Proving $1+\cos a + \cos 2 a+ \cdots + \cos(n-1)a = 0$, when $a=2\pi/n$ and $n$ is odd","I am trying to prove that:
$1+\cos a+\cos 2a+\cos3a+\cos4a=0$
where $a=\frac{2\pi}5$ (pentagon arrangement). Actually this is true for any $n>1$:
$1+\cos a+\cos2a+\dots+\cos(n-1)a=0$ (polygon) where $a={2 \pi\over n}$. Easy to show for even $n$ since the $\cos$ cancel themselves 2 at a time
but but for odd $n$ (say 5)?
This comes from the fact that if you have $n$ same objects equally space around a unit circle, the center of gravity has to be at the origin, so sum of sines equals zero (easy) and sum of cosine also, not so easy for odd $n$.",['trigonometry']
2686050,Expected duration of Gossip spread,"This is a repost of a question by me earlier(which I deleted as it had less details), Consider the following algorithm (phone call model) of spreading a gossip/rumor - On day $1$, only $1$ person knows the rumor, he calls randomly independently a person in the city of total $n$ persons and tells the rumor to that person. The process is repeated by all those that know the rumor on a given day (each knower calls a single person daily,who may or may not know the rumor already). This is repeated till all know the rumor after some days. I want to show the expected duration till all people in the city know, is $O(\log n)$. I wish to know if the following approach is correct : At any day, suppose I have $k$ number of people who know the rumor, and $T_{k+1}$ be the time(day number) when a new $(k+1)^{th}$ member comes to know. Then since, at this instant, for any person who knows the rumor, probability of selecting new person to inform rumor is $\frac{n-k}{n-1}$, and by linearity of expectation, number of new person increased in a day $= \frac {k(n-k)}{n-1} = d$(say). So time to get one additional member who knows is $= 1/d = E(T_{k+1} - T_{k}) $, as it is a geometric variable, then I can apply $E(T_n) = \sum_{k=0}^{k=n-1}E(T_{k+1}- T_k) $to get $E(T_n) =$ harmonic number $\approx \ln n$","['expectation', 'probability']"
2686053,"Can the total derivative, as a linear map, have a different domain from the domain of the function?","I'm reading the total derivative as a linear map definition on Wikipedia . The function in the definition is $f:U \mapsto \mathbb{R}^m$, where $U$ is an open set in $\mathbb{R}^n$. However, the derivative is defined as a linear map that maps from $\mathbb{R}^n$ to $\mathbb{R}^m$. Does that mean the linear map has a different domain (i.e. $\mathbb{R}^n$) than the function itself (the function has domain $U$)? How is that possible? In my understanding, the total derivative, when given a point from the domain of the function itself (i.e. $U$), tells us the speed and direction of the function at that point. If so, it doesn't make sense to put a point that's in $\mathbb{R}^n$ but not in $U$ into the total derivative, does it?","['derivatives', 'real-analysis', 'linear-transformations', 'calculus']"
2686063,"Standard Deviation for sums of fair dice given the number of dice, and the number of sides on each die","$n$ fair die are rolled, and each dice has $x$ sides, with the numbers on the sides going from $1$ to $x$, and with each side having a different number from the other sides. how do I figure out the standard deviation for the probability of the sums of the numbers that the die land on?","['statistics', 'probability']"
2686125,About bounded below operators,"In Murphy's book, ''bounded below'' is defined on a linear map $u\colon X\to Y$ between Banach spaces, not on a bounded linear operator.But continuity of $u$ is used when proving closedness of $u(X)$. Do I misunderstand the definition of ''bounded below''? Or does continuity of $u$ follow from ''bounded below''?","['functional-analysis', 'operator-theory']"
2686129,Is $\sqrt n = \omega (log^{300}n)$?,"When I plug in some really big values of $n$, I find $\sqrt n$ to be much smaller than $(log^{300}n)$. For instance if I plug in $10^{100}$, $\sqrt n = 10^{50}$ and $(log^{300}n) = 4.6 \times 10^{708}$. But the text I was reading states $\sqrt n = \omega (log^{300}n)$. Can someone help me verify this (without taking the limits)? As I understand if I take $\lim_{x\to\infty}\frac {\sqrt n}{(log^{300}n)}$, we will have to differentiate around $300$ times and finally we will get $\lim_{x\to\infty}{K\times\sqrt n}$ which will give us $\infty$. However I am not sure if I did this correctly. Can someone verify this too?","['asymptotics', 'discrete-mathematics']"
2686150,Inverse of a modular matrix,"I have the matrix$$
\begin{pmatrix}
1 & 5\\
3 & 4
\end{pmatrix} \pmod{26}
$$
and I need to find its inverse. I do it according to this website . I find the modular multiplicative inverse (of the matrix determinant, which is $1×4-3×5=-11$) with the extended Euclid algorithm (it is $-7 \equiv  19 \pmod{26}$). Then I have $$\frac{1}{19}×\begin{pmatrix}4 & -5\\-3 & 1\end{pmatrix}.$$ I calculate that$$-5 \equiv 21 \pmod{26},\ -3 \equiv 23 \pmod{26}.$$ No matter what I do I am not able to get the solution they have on the website, which is$$\begin{pmatrix}2 & 17\\5 & 7\end{pmatrix}.$$ Can someone help me with this? What am I doing wrong?","['matrices', 'modular-arithmetic']"
2686159,Why Can't $\delta$ depend on $x$ in $\delta-\epsilon$ Proofs,When proving $\lim_{x\to a} x^2 = a^2$ we need to show that for all $\epsilon > 0$ there is some $\delta > 0$ so that if $0<|x-a|<\delta$ then $|x^2-a^2|< \epsilon$. Now writing $|x^2-a^2|$ as $|x-a||x+a|$ why can't we just pick $\delta = \frac{\epsilon}{|x+a|}$. Why is the reason that $\delta$ must be independent of $x$?,"['epsilon-delta', 'proof-writing', 'limits']"
2686161,Yet another matrix equation [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Solve the following matrix equation for $X$. $$\left[\begin{array}{cc}
5 &-8\cr
8 &1
\end{array}\right] X + \left[\begin{array}{cc}
6 &6\cr
3 &5
\end{array}\right] = \left[\begin{array}{cc}
-1 &4\cr
-3 &-1
\end{array}\right] X$$ Please give me some hint to do this question. Thanks.","['matrices', 'matrix-equations', 'linear-algebra', 'systems-of-equations']"
2686181,Find the equation of median without finding its vertices,"There is a triangle ABC. Equation of AB is $x + y = 2$, Equation of AC is $2x + 3y = 5$ and Equation of BC is $5x - y = 7$. Given above, how do I find the equation of median $AD$ without finding any vertices of triangle ABC ?","['trigonometry', 'triangles', 'geometry']"
2686198,Find $\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x}$,"The question is to evaluate $$\lim_{x\to\infty}\left(\sqrt{\ln(e^x+1)}-\sqrt{x}\right)^{1/x}$$ This is an indeterminate form of type $0^0$, so I've tried using the identity $a^b=e^{b\ln a}$ and somehow apply l'Hospital's, which leads to pretty complex derivatives and I'm getting nowhere. I've also tried multiplying by the conjugate and perhaps factorize, without success.",['limits']
2686241,Change of canonical sheaf in blowing-up,"Suppose $Y$ is a singular projective variety defined over (for simplicity) $\mathbb{Q}$, then from Prop 7.5 of Hartshorne, the dualizing sheaf of $Y$ is defined and let's denote it by $\omega_Y$. Let $Z$ be the singular locus of $Y$ and for simplicity, let's assume $Z$ is irreducible. Now let $X$ be the blow-up of $Y$ at $Z$ with exceptional divisor $D$
\begin{equation}
\pi:X \rightarrow Y
\end{equation}
What is the relation between the dualizing sheaf $\omega_X$ (which is a line bundle as $X$ is smooth) and the pull-back of $\omega_Y$? e.g is it still true that 
\begin{equation}
\omega_X=\pi^* \omega_Y+D
\end{equation}
just like the smooth case?","['complex-geometry', 'algebraic-geometry']"
2686244,Clarifying the homomorphism's definition?,"So, I've heard a lot that homomorphism is a structure-preserving mapping between somewhat algebraic objects. Rigorous definition is: 
$f(x \circ y) = (f x) \circ (f y)$.
And it's not clear enough for me why operation from the left (denoted as $\circ$) remains unchanged on the right ? Does it mean that homomorphism suppose to work only with algebraic objects sharing the same operation? Or this definition is just not enough explicit, shadowing the fact that $\circ$ gets mapped as well to a, let say, $\bullet$ and those are different in general? Intuitively it feels like $f(x \circ y) = (f x) \bullet (f y)$ would be more correct.","['abstract-algebra', 'definition']"
2686304,Example of a continuous function s.t. $f(\overline{A}) \subsetneq \overline{f(A)}$,"This question is a subproblem of the A map is continuous if and only if for every set, the image of closure is contained in the closure of image . I am not able to come up with any example of a continuous function s.t. $f(\overline{A}) \subsetneq \overline{f(A)}$. Can anyone give such an example?","['general-topology', 'real-analysis', 'examples-counterexamples', 'continuity']"
2686305,Determine whether a polynomial is reducible over a finite field,"I'm trying to determine whether a $\mathbb F_5[x] / (x^2 + x + 2)$ is a field. This is equivalent to determining whether $x^2 + x + 2$ is reducible over $\mathbb F_5[x]$. I can prove reducibility by showing that there are two nonzero $a(x), b(x) \in \mathbb F_5[x]$ such that $a(x)b(x) = x^2 + x + 2$. There are 25 elements in $F_5[x]$ of degree ≤ 1, representable as $Ax + B$ for $0 \leq A, B < 5$. I have tried several such $a(x), b(x)$'s of this form and can't find a pair that satisfies this equation, so I'm inclined to believe $x^2 + x + 2$ is irreducible. Is there a way to prove that this is true without exhaustively multiplying all pairs of 25 elements?","['irreducible-polynomials', 'finite-fields', 'abstract-algebra', 'factoring', 'ring-theory']"
2686330,Lang undergraduate algebra about Galois theory,"I can't solve this problem. 
Please help me . Let $F$ be a field of characteristic $0$ , contained in its algebraic closure $A$. Let $a∈A$ and suppose $a∉F$, but every finite extension $E$ of $F$ with $E\neq F$ contains $a$. In other words, $F$ is a maximal subfield of $A$ not containing $a$. Prove that every finite extension of $F$ is cyclic. I tried to solve it like below. Let $f(x)$ be a minimal polynomial of $a$ over $F$. Let $deg(f)=n$. Let $E/F$ be finite $m$ dimension such that $(m,n)=1$. Because $E/F$ is finite dimension, $a∈E$. So $F⊂F(a)⊂E$. Because $(m,n)=1$, $F=F(a)$ so $a∈F$. Contradict to $a∉F$. I had a contradiction. What is wrong? 
Does it mean no finite extension of $F$ such that $(m,n)=1$? if so I suppose any finite extension of $F$ is not $(m,n)=1$. Let $σ$ be Automorphism of $F(a)/F$. then $σ(a)$ be a root of $f(x)$. Then $F(σ(a))$ is finite extension of $F$ and $F(σ(a))⊂A$.So,$a∈F(σ(a))$.So,$F(a)⊂F(σ(a))$. Because $[F(a):F]=deg(f)=n$. So ,$F(a)=F(σ(a))$. So $F(a)$ is minimal field of $f(x)$. Because $F$ is a field of characteristic $0$, $F(a)$ is separable over $F$. So $F(a)/F$ is a Galois extension. If n is not prime , there exist nutural number $r$ such  that $1<r<n, r|n$. Let G be Galois group of $F(a)/F$. Let H be a subgroup of $G$ with order $r$. Let K be fix field of $H$ over $F$. Then $a∉K$and $K/F$ is finite extention. So contradiction .So n need to be a prime number.  Because of n is prime number, Galois group of $F(a)/F$ is cyclic. After that what should I do. I tried to solve it again. Let $N$ be a Galois group of $E/F(a)$. Since order of $G/H$ is $p$, there exist $g∈G$ such  that  $g∉H$.
$g$ generates a cyclic group $<g>$. Let $K$ be the fix field of $<g>$, then $E/K$ is finite dimention, so $E/F$ is finite dimention . If not $K=F$, then $F(a)⊂K$. So, $g∈H$. Contradiction. Hence, $K=F$. So $G=<g>$. Hence  $E/F$ is cyclic. Q.E.D. I could solve it. 
Is it correct?","['number-theory', 'abstract-algebra', 'galois-theory', 'field-theory']"
2686378,Modulus Operation with Negative Numbers,"This topic has been asked many times and I have seen all the answers but none of them was helpful in clearing my doubt which is why this question is here. First of all, let's get the definitions out of the way. Division Algorithm: Let $a$ and $b\ (\neq0)$ be any integers. Then there exist unique integers $q$ and $r$ with the property that $a=bq+r$ , where $0 \leq r \lt |b|$ $\bmod{}$ Operator: For the same $a,b,q$ and $r$ as stated in the definition above, we say $$a \bmod b=r$$ Note that $a,b$ and $q$ can either be positive, negative or even $0$ (if we exclude $b$ ) whereas $r$ can only be non-negative. Case $1$ : Positive Divisor and Positive Dividend: $$68 \bmod 12=8$$ Reasoning: $68=12(5)+8$ Case $2$ : Positive Divisor and Negative Dividend: $$-68 \bmod 12=4$$ Reasoning: $-68=12(-6)+4$ Case $3$ : Negative Divisor and Positive Dividend: $$68 \bmod -12=8$$ Reasoning: $68=-12(-5)+8$ Case $4$ : Negative Divisor and Negative Dividend: $$-68 \bmod -12=4$$ Reasoning: $-68=-12(6)+4$ Doubt: Are all the cases that I listed sound along with their reasoning? If not, please show me how to evaluate $\bmod{}$ correctly. Also, would you like to add something which would enhance my knowledge of modulo arithmetic or anything in general?","['abstract-algebra', 'modular-arithmetic']"
2686391,Non-unital ring $(2\mathbb{Z})[X]$ is not Noetherian,"Let $R = 2\mathbb{Z}$ . Then $R[x]$ is not a noetherian ring. I do not understand why this is so, because Hilbert's basis theorem says: If R Noetherian ring, then R[X] a is Noetherian ring (from wiki ). I suppose that $2\mathbb{Z}$ is principal ideal ring: Let (2), (4), (6), ... are the ideals, therefore all elements are generated by one ideal, so $2\mathbb{Z}$ is principal ideal ring. And we conclude that $2\mathbb{Z}$ is a noetherian ring. Why can't use Hilbert's basis theorem for $R[x]$ ?","['abstract-algebra', 'ring-theory', 'rngs', 'noetherian']"
2686471,How to check continuous function $f$ has extension?,"Question : given, $S= \{(x,y)∈\mathbb{R^2}: -1≤x≤1, -1≤y≤1\}$ and, $T=S-\{(0,0)\}$ and let $f$ be continuous function from $T$ to $\mathbb{R}$ , then choose the correct, (a) Image of $f$ must be connected. (b)Image of $f$ must be compact. (C) any such continuous function $f$ can be extended to a continuous function from $S$ to $\mathbb{R}$ (d) if $f$ can be extended to continuous function from $S$ to $\mathbb{R}$ then image of $f$ must be bounded! My attempt : I had done with (a) and (d) and they are correct ! But I am not able to discard (b) and (c). For (b) I know, continuous image of compact set is compact, but i am not able to discard/ prove (b). Please help me :-(, in (b) and (c) only...","['real-analysis', 'connectedness', 'functions', 'compactness', 'general-topology']"
2686484,How to calculate the automorphisms group of nonzero rational number multiplicative group?,"For the nonzero rational number multiplicative group $Q^\times$, how to calculate the automorphisms group $Aut(Q^\times)$ ? First,suppose $\phi :Q^\times \to Q^\times$ is an automorphism,it must send 1 to 1, and -1 to -1, I think the question is to determine the primes to be sent what? But I have trouble in determining this thing. I guess this group is $ Z_2\oplus  \oplus_{p \ primes} Z $.
Any help will be greatly appreciated, thanks!","['group-theory', 'group-isomorphism']"
2686497,How to solve the SDE $dB_t = r_tB_tdt$ when $r_t$ is stochastic?,"Suppose we have $$dB_t = r_tB_tdt$$
and we have further that $r_t$ is a deterministic function of time $t$.
I know that this is simply an ODE with separable variables that has standard solution in $[t,T]$:
$$B_T = B_te^{\int_{t}^{T}r_sds} $$ but what if $r_t$ is stochastic? For example it could be $$dr_t = \mu r_tdt + \sigma r_tdW_t$$ where $W_t$ is the Brownian motion. Then $B_t$ itself is stochastic and how can I integrate something like $$\int_{t}^{T}\frac{dB_t}{B_t}$$
Isn't this a stochastic integral? I am having hard times figuring out why in this case the solution is the same as when $r_t$ is deterministic. Thank you very much for your help","['stochastic-processes', 'ordinary-differential-equations', 'probability', 'stochastic-calculus']"
2686606,Equation of a plane passing through 3 points,"It should be simple, but I'm having trouble. The three points are
$$A(1,-2,1)\qquad B(4,-2,-2)\qquad C(4,1,4)$$
The plane I get is
$$x+2y+z+6=0$$
but it obviously does not pass through the three points $A,B,C$.","['3d', 'linear-algebra', 'vectors', 'analytic-geometry']"
2686638,"Finding three unknowns from three equations. Solvable? If so, how?","I have the following three equations:
\begin{cases}
v_{1f}\cos(37^\circ)+v_{2f}\cos(\theta) & = 3.5 \times 10^5 \\
v_{1f}\sin(37^\circ)-v_{2f}\sin(\theta) & = 0\\
v_{1f}^2+v_{2f}^2 & =(3.5 \times 10^5)^2
\end{cases}
And I want to solve for $v_{1f}$, $v_{2f}$, and $\theta$. This is a system of three equations but it doesn't seem solvable and I've tried everything I know to solve it. For example, nothing can cancel with each other like you would in an easy system, and I've tried using the 3rd equation to solve for $v_{1f}$ or $v_{2f}$ but it still does not come out correctly. I do know the answers, just not how to get them. Here they are:
\begin{cases}
v_{1f}=2.8 \times 10^5 \\
v_{2f}=2.11 \times 10^3  \\
\theta=53^\circ \\
\end{cases}
Am I missing some information needed for solving this? I really appreciate any help with this question. Sorry that I could not show more of my work but I'm stuck and showed what I know so far. Thank you. Also, if someone sees that it isn't solvable that would help as well. EDIT: I messed up typing the 1st equation, fixed now.","['problem-solving', 'trigonometry', 'systems-of-equations']"
2686687,"On bernoulli percolation, increasing events and Russo's formula","I am very new to this particular branch of probability theory, I try to be as formal as possible. In this question I consider bernoulli percolation as it is usually introduced as a first model (see for instance Geoffrey Grimmett). Problem: Let $x,y \in \mathbb{Z}^d$ . Prove that $f(p):= \mathbb{P}_p( x \leftrightarrow y)$ is strictly increasing in $p \in[0,1]$ . My approach: First of, let me just state that it is clear that $f(p)$ is increasing in $p$ . Both, intuitively and rigorously. The event $ x \leftrightarrow y$ (there exists an open path from $x$ to $y$ ) is an increasing event,  i.e. opening up edges is beneficial for the event $x \leftrightarrow y$ and it is a straightforward result from percolation theory that if $A$ is an increasing event, then $p \in [0,1] \mapsto \mathbb{P}_p(A)$ is increasing. The issue of course being that I want to establish that $\mathbb{P}_p(A)< \mathbb{P}_q(A)$ for $p<q$ . Here I consider it to be a good idea to use Russo's formula: Theorem (Russo's formula): Let $A \in \mathcal{F}_E$ be an increasing event depending on the edges in a finite subset $F \subset E$ only. Then $p \mapsto \mathbb{P}_p(A)$ is differentiable, and \begin{align} \frac{d}{dp} \mathbb{P}_p(A) = \sum_{e \in F} \mathbb{P}_p(e \text{ is pivotal for }A)  \tag{*}\end{align} If I can use this formula to prove that $f'(p)>0$ , then indeed $f$ is strictly increasing. Of course the event $A = \{x \leftrightarrow y \}$ is increasing and depends only on finitely many edges. Consider dimension $d=2$ , then I identified pivotal edges $e \in E$ as follows: An edge $e$ is pivotal (essential) for the event $x \leftrightarrow y$ if and only if there exists an open path from $x$ to $y$ going through the open edge $e$ (say $\gamma$ ) and there exists a dual open path (say $\gamma^*$ ) that connects the two endpoints of $e^*$ and is a circuit containing the point $x$ . The picture below (taken from P. Nolin Percolation) is related to said situation, it depicts the event $0 \leftrightarrow \partial B_n$ and shows the paths $\gamma$ and $\gamma^*$ . My Question: How do I complete the proof? I would be happy to understand it even just in the case of $d=2$ . So far I have: $$ f'(p) = \sum_{e \in F} \mathbb{P}_p( \exists \gamma, \exists \gamma^*) \overset{FKG}\geq\sum_{e \in F} \mathbb{P}_p( \exists \gamma) \mathbb{P}_p( \exists \gamma^*) \overset{?}>0 $$","['probability-theory', 'percolation']"
2686724,How to prove the sum of sample is the complete statistics for gamma distribution?,"For a random sample $x_1, x_2, \cdots, x_n$ coming from the Gamma distribution with $\varGamma(1,\theta).$ How to prove that the $ \sum_i^nx_i $ is the complete statistics? What I have done is that, the sum of sample follows $ \varGamma(n,\theta) $, and let $ t=\sum_1^n x_i $,then, $$\operatorname E(g(t))=\int_0^\infty g(t)\frac{\theta^n}{\varGamma(n)}t^{n-1}e^{-\theta t} \, dt = 0$$ The problem is that I don't know how to show that the expectation equals to $0$ can imply that $P(g(t)=0)=1$. Could anyone help to prove it?","['gamma-distribution', 'probability-theory', 'statistics', 'statistical-inference']"
2686729,Relation between uncountably infinite probability space and continuous random variables,"I have a doubt on how to relate uncountably infinite probability space and continuous random variables. Take the following random experiment ""choose a number from $[0,1]$"". I construct the associated probability space. the sample space $\Omega\equiv[0,1]$ the $\sigma$-algebra equal to the Borel $\sigma$-algebra on $[0,1]$, $\mathcal{B}([0,1])$ the measure $\mathbb{P}:\mathcal{B}([0,1])\rightarrow [0,1]$ (this is not necessarily equal to the Lebesgue measure; there may be some numbers ""more attractive"" than others; also, it can't be the ratio of counting measures because $\Omega$ is uncountably infinite) Consider now the random variable 
$$
X:\Omega\rightarrow \mathbb{R}
$$ ($\star$) I assume that this random variable is continuous ($\star \star$) (EDITED following a useful comment below) I assume that this random variable has cdf continuous on $\mathbb{R}$ and strictly monotone. Clearly, ($\star \star$) $\rightarrow$ ($\star$). I want to understand the different implications of these two assumptions on $(\Omega, \mathcal{F}, \mathbb{P})$. With this objective in mind, I have separated my question into 4 sub-questions. 1) Could you help me to formally understand the relation between $(\star)$ and $\Omega$ above? Is $\Omega$ uncountably infinite a necessary condition for $X$ being a continuous random variable? 2)  Could you help me to formally understand the relation between $(\star)$ and $\mathbb{P}$ above? 3) Could you help me to formally understand the relation between $(\star \star)$ and $\Omega$ above? I think that all that matters here should be thorugh condition ($\star$), correct? 4) Could you help me to formally understand the relation between $(\star \star)$ and $\mathbb{P}$ above? Is this simply the relation among probability measure, pdf, cdf?","['probability-theory', 'random-variables', 'probability-distributions']"
2686860,Finding the Weingarten Map of a simple surface,"I'm trying to find the Weingarten map of the surface parametrized by $\sigma(u, v) = (u, v, u^2 + v^2)$. Now, one can easily find that the Gauss map (which is just the surface normal $\bf N$) is given by
$$
{\bf N}(\sigma(u, v)) = -\frac{(u,v,1/2)}{\sqrt{u^2+v^2+1/4}}
$$
The Weingarten map is the negative derivative of this, $\mathcal{W} = -D_{\vec p}{\bf N}$. To calculate the derivative of a smooth function $f$ from a surface $S$ parametrized by $\sigma$, onto a surface $\tilde{S}$ parametrized by $\tilde{\sigma}$, one first needs to find functions $\alpha$ and $\beta$ such that
$$
f(\sigma(u, v)) = \tilde{\sigma}(\alpha(u, v), \beta(u, v))
$$
so that, for a curve on $S$, $\gamma(t) = \sigma(u(t), v(t))$, the tangent vector $(f\circ\gamma)'$ is given by
$$
\tilde{\sigma}_\alpha(\alpha_uu'+\alpha_vv') + \tilde{\sigma}_\beta(\beta_uu'+\beta_vv')
$$ In this case, since the Gauss map maps from the tangent space to the surface, $T_{\vec p}S$, onto itself, we need to find $\alpha$ and $\beta$ such that
$$
{\bf N}(\sigma(u,v)) = \sigma(\alpha(u,v),\beta(u,v))
$$
but
$$
\sigma(\alpha(u,v),\beta(u,v)) = (\alpha(u,v),\beta(u,v),\alpha^2(u,v)+\beta^2(u,v))
$$
and so clearly, we must have
$$
\alpha(u,v) = -\frac{u}{\sqrt{u^2+v^2+1/4}} \\
\beta(u,v) = -\frac{v}{\sqrt{u^2+v^2+1/4}}
$$
but then $\alpha^2(u,v) + \beta^2(u,v)$ doesn't equal the last component, so there aren't functions $\alpha$ and $\beta$ that satifsy the above equality. What is happening? I can't figure this out. From this, it seems like the Weingarten map shouldn't exist , but clearly it does. Any help here would be greatly appreciated!","['derivatives', 'differential-geometry']"
2686879,Beginner probability question: Bimodal distribution (ie like some Yelp reviews),"Background Let's say a Yelp reviewer either gives 1 stars or 5 stars because when her experience is average she doesn't feel as motivated to write a review. Sometimes she will give 2 or 4 stars, and extremely rarely will give 3 stars. Let's say $X =$ number of stars she will give Question 1 You work for Yelp and your boss asks you what kind of RV is $X$ and what is the associated probability mass function? What is the formula for $P(X=k)$ ? For example, if $X =$ average male height in feet, then $X$ is like a normal RV and we can use the pdf to find the $P(X>12)$ My attempt Maybe you say to the boss, $X$ is a Beta RV with parameters like $(\frac{1}{2},\frac{1}{2})$ although Beta is continuous In general I can't find an explicit pmf for the bimodal. When I go on the wikipedia for the Multimodal Distribution, it is the first distribution I've seen that doesn't have a pmf/pdf, cdf, mean, etc. It seems like the Bimodal should have its own thing like the Poisson or Gamma... Thanks for your help and putting up with my ignorance.","['statistics', 'probability']"
2686881,Roots of random polynomials.,"Assume $P(x)$ is a random polynomial of degree $d$, where its coefficients are picked uniformly at random from $\mathbb{F}_p$, and $p$ is a large prime number. So the polynomial is defined over $\mathbb{F}_p$. Question 1: What is the probability that polynomial $P(x)$ has at least one root? Question 2: Are roots of $P(x)$ random values in $\mathbb{F}_p$?","['finite-fields', 'polynomials', 'linear-algebra', 'factoring']"
2686903,Question on evaluating $\int_{C}\frac{e^{iz}}{z(z-\pi)}dz$ without the residue theorem,"I am trying to figure out how to evaluate the integral $\int_{C}\frac{e^{iz}}{z(z-\pi)}dz$ where $C$ is any circle centered at the origin with radius greater than $\pi$.  I can see that $\frac{e^{iz}}{z(z-\pi)}$ is analytic everywhere except where $z=0$ and $z=\pi$, both of which are in the region bounded by $C$. I can also see that by using the Taylor expansion of $e^{iz}$ that
$$\int_{C}\frac{e^{iz}}{z(z-\pi)}dz = \sum_{n=0}^{\infty}\frac{i^{n}}{n!}\int_{C}\frac{z^{n-1}}{z-\pi}dz$$ I'm supposed to apply Cauchy's Theorem or Cauchy's Integral Theorem to evaluate the integral along this curve but I am not sure how to do so.  I do not yet have the residue theorem in my tool box.","['cauchy-integral-formula', 'complex-analysis', 'complex-integration']"
2686908,Find the image of unit disk under $f(z)= \frac{1}{z}\prod_{k=1}^{n}(z-a_k)^{\lambda_k}$,"Find the image of $f$ defined on $|z|<1$ complex unit disk, given by $$f(z)= \frac{1}{z}\prod_{k=1}^{n}(z-a_k)^{\lambda_k}$$where $|a_k|=1$, $0<\lambda_k<1$, $\sum_{k=1}^n\lambda_k=2$. The first thing came to my mind is Schwarz-Christoffel integral $$S(z)=\int_0^z\frac{1}{\prod_{i=1}^n(\zeta-A_i)^{\beta_i}}d\zeta$$ which maps the real line onto a polygon. The image of $\infty$ under $S$ is not one of the vertices if $\sum_{k=1}^n\beta_k=2$. I'm not sure how to relate the Schwarz-Christoffel integral to the $f$. Any suggestion is appreiciated.","['complex-analysis', 'conformal-geometry']"
2686925,Some questions on the details of an integration of Brownian Motion Against itself,"I'm going through a computation that calculates the integral of a Brownian motion with respect to another Brownian Motion. Apparently I can do this with martingales (after searching here), but I haven't learned these yet. The solution method I'm looking at uses $L^2$ convergence directly. I've indicated the places I get lost below: Question: Show:$$\int_0^tB_sdB_s = \frac{1}{2} B_t - \frac{1}{2} t$$ Create a process that approximates $B_t$. Consider a simple process $$f_n(s)=\sum_{\text{over partitions labeled by  }t_j}B_{t_j}\mathbb{1}_{[t_j,t_{j+1})}(s)$$ Then: $$\mathbf{E}\bigg[\int_0^t(B_s-f_n(s))^2ds\bigg] \\ = \mathbf{E}\bigg[\sum_j\int_{t_j}^{t_{j+1}}(B_s-B_{t_j})^2ds\bigg] \\ = \sum_j \frac{1}{2}(t_{j+1}-t_j)^2 \rightarrow 0$$ In the second line, I think ito isometry is used. When I plug in the definition of $f_n$ and work through it, I get lost. Using $f_n$ so constructed in part 1, we approximate the integral of interest then take limits. That is $$\int_0^tB_sdB_s = \lim_{n\rightarrow \infty} \int_0^t f_n dB_s = \lim_{n\rightarrow \infty} \sum_{j=1}^n B_{t_j}(B_{t_{j+1}}-B_{t_j})$$ Noting then that $B_{t_{j+1}}^2-B_{t_j}^2=(B_{t_{j+1}}-B_{t_j})^2 + 2B_{t_j}(B_{t_{j+1}}-B_{t_j})$ we obtain:
$$\sum_j B_{t_j}(B_{t_{j+1}}-B_{t_j}) = \frac{1}{2}B_t^2 - \sum_j \frac{1}{2}(B_{t_{j+1}}-B_{t_j})^2$$
The result follows by taking $n$ to the limit. Where does the first term on the right hand side come from ($\frac{1}{2}B_t^2$)? I see a telescoping series, but after all the cancellation, I'm left with 2 terms. Does $\lim_{n \rightarrow \infty}B_{t_n}^2$ go to zero?","['stochastic-processes', 'probability-theory', 'stochastic-integrals', 'brownian-motion', 'stochastic-calculus']"
2686933,"References for ""Multidimensional Sturm-Liouville Theory""","I am interested in what I'll call (perhaps erroneously) multivariate Sturm–Liouville theory , i.e., solutions to equations of the form
$$\nabla\cdot(P(x)\nabla Y)+Q(x)Y=-\lambda W(x)Y\tag{1}$$
for $\lambda\in\mathbb R$, $Y:\mathbb R^d\to\mathbb R$, and $P,Q,W:\mathbb R^d\to\mathbb R$. If $d=1$, then, as shown on the Wikipedia page, there is a very well developed theory for solutions of $(1)$ on an interval $[a,b]$ with fixed boundary conditions. This is called Sturm-Liouville theory. I suspect that such a theory has been generalized to higher dimensions. However, after googling for a while using keywords such as ""multivariate Sturm-Liouville theory"", I'm starting to suspect that the study of $(1)$ in higher dimensions has a different name, since I've completely failed to find good comprehensive resources on such problems. Question: Are there textbooks that treat multivariate problems such as $(1)$ in detail? I'm especially interested in fixed point-type arguments for existence and uniqueness, as well as continuity results for the solution $(Y,\lambda)$ with respect to the ""data"" $P,Q,W$.","['reference-request', 'ordinary-differential-equations', 'analysis', 'partial-differential-equations']"
2686977,Continuity of eigenvectors of self-adjoint matrices,"What I have here is a matrix function $A(x,y)$, defined on $[0,1]\times [0,1]$, such that each $A(x,y)$ is a self-adjoint positive semi-definite $n\times n$-matrix (with possibly complex entries). Moreover, $(x,y)\mapsto A(x,y)$ is supposed to be continuous on $\mathbb R^2$ and it has constant rank, i.e., the multiplicity of the zero eigenvalue is constant and all the other eigenvalues remain in an interval $[a,b]$, where $a > 0$. Can I then find continuous matrix functions $U$ and $B$ such that
$$
A(x,y) = U(x,y)B(x,y)U(x,y)^*,
$$
$U(x,y)$ is unitary, and $B(x,y)$ is block diagonal of the form
$$
B = \begin{pmatrix}0&0\\0&B_2\end{pmatrix},
$$
where $B_2(x,y)$ is a square matrix function of the size equal to the rank of $A(x,y)$?","['matrices', 'eigenvalues-eigenvectors', 'continuity', 'linear-algebra']"
2687013,Determining if a function described by a Taylor series has a relative extremum at a point,"I have a very easy question from the 2004 BC2 (Form B) AP Calculus exam. The question is: $f$ is a function with derivatives of all orders for all real numbers. The third-degree Taylor polynomial for $f$ about $x=2$ is given by 
  $$T(x)=7-9(x-2)^2-3(x-2)^3$$
  b) [...] Determine whether $f(2)$ is a relative maximum, minimum, or neither, and justify your answer. To solve this, I used the second derivative test for relative extremum: Since $f'(2)=T'(2)=2$ and $f''(0)=T''(2)=-18<0$, $f$ has a relative maximum at $x=2$. I was thinking, though, what if I used the first derivative test for relative extremum instead? x         2
     <–––––––|–––––––>
T'(x)    +   0   – Since the first 3 derivatives of $f(x)$ equal those of $T(x)$ around $x=2$, and $T'(x)$ changes from positive to negative at $x=2$, $f$ has a relative maximum there. Now first of all, of course the second method is longer. However, I think it may also be incorrect, or at least require further justification. I do not think, though, that further justification is needed since $f$ does indeed equal $T$ for those first three derivatives at $x=2$ by definition. Of course, that last statement of mine may very well be wrong. So, my question is, what is wrong with the second method for determining that $f$ has a relative maximum at $x=2$?","['derivatives', 'taylor-expansion', 'calculus']"
2687242,Finding First Variation,"Given the functional $ I(U) = \int_0^1 F(u'(x),u(x), x) dx$ and: $F(p,u,x)= \sqrt{p^2+u^2}$ how would you find the first variation using say the definition on Wikipedia (Gateaux Derivative) and what's the purpose of it? I know the Euler-Lagrange tell you the functions which make the given functional stationary, but I'm not quite certain how to relate these two ideas. Thanks","['multivariable-calculus', 'calculus-of-variations', 'euler-lagrange-equation']"
2687252,Axiom of Dependent Choice implies Axiom of Countable Choice,"I have formalized Noah Schweber's idea to prove that Axiom of Dependent Choice implies Axiom of Countable Choice, but I'm not sure if my below proof is correct or not. Please have a look and check! Thank you for your help! Axiom of Dependent Choice Let $T \neq\varnothing$ and $\mathcal{R} \subseteq T^2$ such that $\forall a \in T, \exists b \in T: a\mathcal{R}b$. Then there exists $(x_n \mid n \in \mathbb N)$ such that $x_n \mathcal{R} x_{n+1}$. Axiom of Countable Choice Let $(A_n \mid n \in \mathbb N)$ be a sequence of non-empty sets and $X=\bigcup_{n \in \mathbb N} A_n$. Then there exists a mapping $f: \mathbb N \to X$ such that $f(n) \in A_n$. Here is my take: Let $\mathcal{R}=\{(x,y) \in X^2 \mid \exists n \in \mathbb N \text{ such that } x \in A_n \text{ and } y \in A_{n+1} \}$. Clearly, $\mathcal{R}$ satisfies the requirement of DC. Hence there is a sequence $(x_i \mid i \in \mathbb N)$ where for some $n,x_i \in A_{i+n}$ $\forall i \in \mathbb N$. Let $f: \mathbb N \to X$ in which $f(i) \in A_i$ $\forall i<n$ and $f(i)=x_{i−n}$ $ \forall i \geqslant n$. Defining $f$ in this way, we have the desired function.","['axiom-of-choice', 'elementary-set-theory', 'proof-verification']"
2687352,"If $G$ has exactly one subgroup $H$ of order $k$, prove that $H$ is normal in $G$.","I am doing a presentation in my class, and I have to write the proof to this theorem. Could someone please verify whether it is correct or not? It is based on ( https://math.stackexchange.com/q/156008 )'s proof. If $G$ has exactly one subgroup $H$ of order $k$, prove that $H$ is normal in $G$. Proof: Let $H=\left \{ e,h_{1},h_{2},...,h_{k-1} \right \}\leq G$. Then $|H|=k$. Claim: The set $gHg^{-1}=\left \{ghg^{-1}:g\in G  \right \}$ is a subgroup of $G$. Proof: (one-step subgroup test) Non-empty: Since $H\leq G$, $e\in H$. Then $geg^{-1}=gg^{-1}=e\in gHg^{-1}$. Therefore, $gHg^{-1}\neq \varnothing$. Closure Inverse: Let $gh_{1}g^{-1},gh_{2}g^{-1}\in gHg^{-1}$. Then $(gh_{1}g^{-1})(gh_{2}g^{-1})^{-1}=gh_{1}g^{-1}(g^{-1})^{-1}h_{2}^{-1}g^{-1}$ (since $G$ and $H$ have inverses) $=g(h_{1}h_{2})g^{-1}\in gHg^{-1}$ (since $H$ is a subgroup hence a group) $\square$ Then $gHg^{-1}=\left \{ e,gh_{1}g^{-1},gh_{2}g^{-1},...,gh_{k-1}g^{-1} \right \}$ and so $|gHg^{-1}|=k$. Since $G$ has exactly one subgroup of order $k$, $H=gHg^{-1}$. Therefore, $H$ is normal in $G$. $\square$","['abstract-algebra', 'normal-subgroups', 'group-theory', 'proof-verification']"
2687373,It is possible to prove that there are infinitely many points in space in Hilber'ts axiomatization of geometry? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Dumb question, i know, there's no explicit axiom about it, but it's somehow possible? Thanks in advance.","['axiomatic-geometry', 'proof-theory', 'geometry']"
2687375,"How to find UMVUE of $\theta^k$ when $x_1, \ldots, x_n$ is a sample from Bernoulli$(\theta)$?","Let $x_1, x_2, \ldots, x_n$ be a random sample from the Bernoulli ( $\theta$ ). The question is to find the UMVUE of $\theta^k$ . I know the $\sum_1^nx_i$ is the complete sufficient statistics for $\theta$ . Is $\left(\frac{\sum_1^nx_i}{n}\right)^k$ the estimator or any other possible estimator? Could someone just help me?","['statistical-inference', 'probability-distributions', 'statistics', 'probability', 'parameter-estimation']"
2687397,"What is this group $G=\langle a,b,c\mid a^2=1, b^2=1, c^2=ab\rangle$","Consider the group presentation $$G=\langle a,b,c\mid a^2=1, b^2=1, c^2=ab\rangle.$$ Is this a known group? What is $G$ isomorphic to? Thanks a lot.","['combinatorial-group-theory', 'abstract-algebra', 'group-theory', 'group-presentation']"
2687516,When is the preimage of a hypersurface a hypersurface,"Let $f:X \to Y$ be a proper, dominant morphism of projective varieties and $H_Y$ a hypersurface in $Y$. Is the fiber $f^{-1}(H_Y)$ a hypersurface in $X$?","['algebraic-geometry', 'commutative-algebra']"
2687591,Alternative proof for continuity of matrix inversion,"I am to show that $\mathrm{inv}: \mathrm{GL}_{n \times n}(\mathbb{R}) \to \mathrm{GL}_{n \times n}(\mathbb{R}); A \mapsto A^{-1}$ is a continuous function. I have shown this by showing that the deteriminant is continuous, and that forming the adjunct is continuous, in which case one can apply the formula $A^{-1} = \frac{1}{\mathrm{det}(A)}\mathrm{adj}(A)$. This is all fine, but I would love to see a more intuitive proof that doesn’t simply rely on showing that some formula is composed of continuous functions. I tried finding some sort of relation between the operator norms of $A$ and $A^{-1}$ but couldn’t find anything that holds in the general case. For example, if something along the lines of $\min_{x \in \mathbb{S}^{n-1}}{(Ax)} \times \max_{x \in \mathbb{S}^{n-1}}{(A^{-1}x)} = 1$ were true, one could construct an epsilon-delta proof via that. However, the above expression while inutive, appears to be false (it seems to hold only if the chosen x happen to be eigenvectors unless my attempts to verify it in mathematica were somehow wrong). Does anyone know of a more intuitive proof of the statement than the one I outlined above?","['matrices', 'continuity', 'alternative-proof']"
2687657,Hamiltonfunction under a Duffing Oscillator,"Given is the following oscillator $$\ddot{x} + \lambda\dot{x}=x-x^3$$
I've already rewritten this as a system of first order equations $$\begin{cases}
\dot{x} = y \\ \dot{y}=x-x^3-\lambda y
\end{cases}$$
Now the question is how the Hamiltonfunction $H(x,y)=\frac{1}{2}y^2+U(x)$ changes in time under our given oscillator with $\lambda \in \mathbb{R}$. We know that for $\lambda=0$, $\dot{y}=-U'(x)$. If I calculate $\dot{H}$ I find $$\dot{H}=\frac{\partial H}{\partial x}\frac{\partial x}{\partial t} + \frac{\partial H}{\partial y}\frac{\partial y}{\partial t}=U'(x)\cdot y + y(x-x^3-\lambda y)$$
The problem I face is that I don't know if $U(x)$ depends on $\lambda$. Does anyone have an idea to get me going?",['ordinary-differential-equations']
2687660,"If $1!+2!+\dots+x!$ is a perfect square, then the number of possible values of $x$ is?","If $1!+2!+\dots+x!$ is a perfect square, then the number of possible values of $x$ is? I looked for a general way of expanding such a factorial series but I was not able to find one, without that I don't know any other way to approach this problem. All help is appreciated.","['factorial', 'square-numbers', 'sequences-and-series']"
2687673,Quadratic forms and real quadratic fields,"When we have a squarefree, negative integer $d$, class number of $\mathbb{Q}\sqrt{d}$ is equal to the number of positive definite reduced binary quadratic forms of discriminant $d$ or $4d$ depending on whether $d \equiv_{4} 2,3$ or $d \equiv_4 1$. Do we have a such relation when we have a positive $d$ and real quadratic number field $\mathbb{Q}\sqrt{d}$? Can we say something like 'the class number of the number field is equal to the negative definite quadratic forms of fixed discriminant' ?","['number-theory', 'quadratic-forms', 'field-theory']"
2687684,Find the number of ways of arranging the letters,"Find the number of ways of arranging the letters $\text{AAAAA, BBB, CCC, D, EE & F}$ in a row if no two $\text{C's}$ are together? My Attempt: Well, I should I arrive at the answer if I subtract the cases where 3 $\text{C's}$ and 2 $\text{C's}$ appear together from the total. Total possibilities $= \frac{15!}{5!3!3!2!}$ Total possibilities where 3 $\text{C's}$ appear $=\frac{13!}{5!3!2!}$ However, I am not able to find the possibilities for 2 $\text{C's}$ being together and get to the answer. Any help would be appreciated.","['permutations', 'combinatorics']"
2687741,product of two analytic functions is 0,"Let $U$ be a subset of $\mathbb{C}$, which is not connected. How can I find two analytic functions $f$ and $g$ from $U$ to $\mathbb{C}$ such that $f\neq0$ and $g\neq0$. But $f\cdot g = 0$. thanks for any hint.","['complex-analysis', 'analysis']"
2687765,Confusion about Axiom of Choice,"Well first I state my understanding about AC.If we have a family of nonempty sets,for every set we could use Existential Instantiation to fix an element in it.And if the family has finite sets,we could fix finite times then we get a choice function.But if the family has infinite sets,we couldn't use Existential Instantiation to fix elements infinite times. Is my understanding right? And if it is right,what really confused me is that if there is a requirement on every set such that it there is one and only one element satisfying the requirement in every set,why don't we have to fix elements infinite times?Is there something justifying it rigorously? Thanks for your help.And my English is poor,you could point out anything that I did not state clearly.","['foundations', 'logic', 'elementary-set-theory', 'axiom-of-choice']"
