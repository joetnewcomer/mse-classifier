question_id,title,body,tags
1836069,"Prove that $\overline{f(A)}\subseteq f(\overline{A})$ where $f: X \rightarrow Y$ is continuous, $X$ is compact and $A \subseteq X$","Suppose that $X$ and $Y$ are topological spaces, $f: X \rightarrow Y$ is a continuous map and $A \subseteq X$ . It's not very hard to prove that $f(\overline{A})\subseteq \overline{f(A)}$ , where $\overline{A}$ denotes the closure of $A$ . Now assume that $X$ is compact. I'm trying to prove the inclusion $\overline{f(A)}\subseteq f(\overline{A})$ . We know that $\overline{A}$ is compact so $f(\overline{A})$ is compact too. If $Y$ were a Hausdorff space, $f(\overline{A})$ would be closed, and $\overline{f(\overline{A})}=f(\overline{A})$ , so $\overline{f(A)}\subseteq \overline{f(\overline{A})}=f(\overline{A})$ , as required. Nevertheless, I don't have the hypothesis that $Y$ is Hausdorff. So I'm struggling either trying to prove the inclusion, or to find out a counterexample. Thank you.","['continuity', 'general-topology', 'examples-counterexamples', 'compactness']"
1836077,A certain unique rotation matrix,"One can find that the matrix $A=\begin{bmatrix}
    -\dfrac{1}{3} &     \dfrac{2}{3}      &     \dfrac{2}{3}   \\
     \dfrac{2}{3}  &    -\dfrac{1}{3}   &   \dfrac{2}{3}    \\
    \dfrac{2}{3} &    \dfrac{2}{3}  &    -\dfrac{1}{3}  \\
        \end{bmatrix}  $ is at the same time $3D$ rotation matrix and for it the sum of  entries in  every  column (row)  is constant (here $-\dfrac{1}{3}+   \dfrac{2}{3}     +     \dfrac{2}{3} = 1)$. The same is true if we change  the order of columns in it. For example: $A_1=\begin{bmatrix}
    \dfrac{2}{3} &     \dfrac{2}{3}      &     -\dfrac{1}{3}   \\
     -\dfrac{1}{3}  &    \dfrac{2}{3}   &   \dfrac{2}{3}    \\
    \dfrac{2}{3} &    -\dfrac{1}{3}  &    \dfrac{2}{3}  \\
        \end{bmatrix}  $       $A_2=\begin{bmatrix}
     \dfrac{2}{3} &     -\dfrac{1}{3}      &     \dfrac{2}{3}   \\
     \dfrac{2}{3}  &     \dfrac{2}{3}   &   -\dfrac{1}{3}    \\
    -\dfrac{1}{3} &    \dfrac{2}{3}  &     \dfrac{2}{3}  \\
        \end{bmatrix}  $ Questions: Is it any systematic way to find other non-trivial (without $0$ and $1$) rotation matrices with this property? Especially it is interesting whether the above rotation matrices are the only ones with rational entries ?- maybe someone knows other rotation matrices exist where the sum of entries is constant.. and... Can it be proved in some way that if the sum of entries in columns for a  rotation matrix is constant then it should be equal to the length of  column vectors?","['matrices', 'rotations', 'number-theory', 'linear-algebra']"
1836132,Method of Annihilators Tedium...,"One of the exam preparation questions for MIT's online Honors Differential Equations course asks for a general solution of 
\begin{align}
(D^2 - 1)^4(D^3 + 1)^5y = 3e^t
\end{align}
The fact that the equation is written in terms of differential operators makes me think that I'm to use the method of annihilators; indeed, it's easy enough to see that $D-1$ annihilates $3e^t$.  So we have the homogeneous ODE
\begin{align}
(D^2 - 1)^4(D^3 + 1)^5(D - 1) & = 0\Rightarrow\\
(D + 1)^9(D-1)^5\left(D - \frac{1}{2} + i\frac{\sqrt{3}}{2}\right)^5\left(D + \frac{1}{2} - i\frac{\sqrt{3}}{2}\right)^5 & = 0
\end{align}
which means we have (the absurdly unwieldy, at least to me) basis of solutions
\begin{align}
	\lbrace e^{-t},te^{-t},\ldots, t^8e^{-t}, e^t, te^t, \ldots, t^4e^t, e^{\frac{t}{2}}\cos\left(\frac{x\sqrt{3}}{2}\right), te^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right), \ldots, t^4e^{\frac{t}{2}}\cos\left(\frac{t\sqrt{3}}{2}\right),\\ e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), te^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right), t^4e^{\frac{t}{2}}\sin\left(\frac{t\sqrt{3}}{2}\right)\rbrace
\end{align} My question is this:  do I now really need to go through what appears to be an insane amount of differentiation/algebra to get the particular solution?  Or is there some trick I'm not seeing/fundamental fact I'm missing?",['ordinary-differential-equations']
1836175,Example $3.3.1$ in Hartshorne,"Let $k$ be an algebraically closed field, and let $$X = \operatorname{Spec} k[x,y,t]/(ty-x^2)$$ $$Y = \operatorname{Spec} k[t]$$ Hartshorne comments that both schemes $X$ and $Y$ are of finite type over $k$. This means that the natural morphisms $X \to k$ and $Y \to k$ are of finite type. I know that if $k$ is algebraically closed then the closed points of $A_k^1$ is in one-to-one correspondence with elements of $k$. Is $k$ a scheme? How do I show that $X \to k$ a morphism of finite type?",['algebraic-geometry']
1836189,Skew-symmetric parts of stochastic matrices,"It's easy to see that the set $\{W - W^T : W \in \mathbb{R}^{n \times n}\}$ is precisely the set of real skew-symmetric matrices. This continues to be the case if we restrict to (entry-wise) non-negative matrices (i.e., $$\{W - W^T : W \in \mathbb{R}^{n \times n}\} = \{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}\}).$$ Is there a simple condition if we restrict to non-negative right-stochastic matrices? (i.e., those whose rows sum to $1$)? That is, is there a simple condition $C$ on $A$ such that $$\{W - W^T : W \in \mathbb{R}^{n \times n}, \text{ each } W_{i,j} \geq 0, W \text{ right-stochastic}\} = \{A \in \mathbb{R}^{n \times n} : A \text{ is skew-symmetric}, C \text{ holds}\})?$$ For context, I reduced a problem to the optimization problem $$\min_{W \in \mathbb{R}^{n \times n}} x^T (W - W^T) y$$ where $x, y \in \mathbb{R}^n$ are fixed vectors, subject to $W$ being non-negative and right stochastic, and I'm wondering whether there's a simple equivalent problem of the form $$\min_{A \in \mathbb{R}^{n \times n} \text{skew-symmetric}} x^T A y.$$","['matrices', 'stochastic-matrices', 'optimization']"
1836190,Evaluate an increasing sum of binomial coefficients: $\sum_{k=1}^nk\binom{m+k}{m+1}$,I've been working on a problem and got to a point where I need the closed form of $$\sum_{k=1}^nk\binom{m+k}{m+1}.$$ I wasn't making any headway so I figured I would see what Wolfram Alpha could do. It gave me this: $$\sum_{k=1}^nk\binom{m+k}{m+1} = \frac{n((m+2)n+1)}{(m+2) (m+3)}\binom{m+n+1}{ m+1}.$$ That's quite the nasty formula. Can anyone provide some insight or justification for that answer?,"['combinatorics', 'summation', 'binomial-coefficients']"
1836195,Exciting applications of the Riemann-Roch-theorem for Riemann-surfaces,"This semester I took a lecture on Riemann surfaces. The professor proved the Riemann-Roch theorem (stated below). As an application of it, he proved elementary results, we did earlier in the course anyway, e.g. that the Riemann-sphere has genus $0$. My question is: What are the (less elementary) applications and implications of the Riemann-Roch theorem in the form below? Theorem: Let $X$ be a compact Riemann-surface, $D$ a divisor of $X$. Then the Cech-cohomology groups $H^0 (X, \mathcal{O}_D)$ and $H^1(X, \mathcal{O}_D)$ are finite-dimensional vector-spaces and $$\dim H^0 (X, \mathcal{O}_D) - \dim H^1(X, \mathcal{O}_D) = 1 - g + \deg D$$
where $\mathcal{O}_D$ is the sheaf with $\mathcal{O}_D(U) := \{ f \text{ meromorphic on } X : ord_x(f) \geq -D(x), \forall x \in U \}$ and the natural restriction maps, and $g$ is the genus of $X$.","['homology-cohomology', 'complex-analysis', 'riemann-surfaces', 'complex-geometry']"
1836203,Doob Meyer decomposition in an exercize,I have to find the Doob Meyer decomposition for the following process: $Y_t=e^{(1+B_t^2)}$ I think that the method is to derive with the Ito's formula the process and I've obtained: $dY_t=2B_te^{(1+B_t^2)}dB_t+e^{1+B_t^2}(1+2B^2_t)dt$ I want the process in an explicit mode (not with the integral). Is there a general method to do this with other process?,"['stochastic-processes', 'probability-theory', 'stochastic-calculus', 'stochastic-analysis']"
1836217,Differentiability of piecewise functions,"Check whether the function is differentiable:
$$f:\mathbb{R}^2\rightarrow \mathbb{R}$$
$$f= \begin{cases} 
      \frac{x^3-y^3}{x^2+y^2} & (x,y)\neq (0,0) \\
      0 & (x,y) = (0,0) \\
   \end{cases}
$$ So what I did is I calculated the partial derivatives of the function in point $(0,0)$. I got:
$$\frac{∂f}{∂x}\left(0,0\right)=lim_{t\rightarrow 0}\left(\frac{f\left(t,0\right)-f\left(0,0\right)}{t}\right)=lim_{t\rightarrow 0}\left(\frac{t^3}{t^3}\right)=1$$and
$$\frac{∂f}{∂y}\left(0,0\right)=lim_{t\rightarrow 0}\left(\frac{f\left(0,t\right)-f\left(0,0\right)}{t}\right)=lim_{t\rightarrow 0}\left(\frac{-t^3}{t^3}\right)=-1$$ And since the answers I got are not equal, that means the function isn't partially derivable in point $(0,0)$ so it isn't differentiable either? I'm not sure whether what I did was right, differentiability is still a little unclear to me, for multivariable functions. I also asked about it here Differentiability of function definition but have yet to get an answer. Can someone tell me if I'm on the right track at least?","['derivatives', 'real-analysis']"
1836236,For which $a$ and $b$ is this matrix diagonalizable?,"For which $a$ and $b$ is this matrix diagonalizable? $$A=\begin{pmatrix} a & 0 & b \\ 0 & b & 0 \\ b & 0 & a  \end{pmatrix}$$ How to get those $a$ and $b$? I calculated eigenvalues and eigenvectors, but don't know what to do next?","['matrices', 'linear-algebra']"
1836278,Dividing Two Objects in Half Using One Line,"Imagine having a piece of paper with two different shapes on it, each at a random location.  Can we always draw a straight line through the piece of paper, in a manner that divides both objects in half? Keep in mind that I am an 8th grader, and most probably unfamiliar with technical terms.  This just came to my mind.  But if it's necessary to use technical terms, I guess that is my problem and I'll have to look them up.",['geometry']
1836284,"$gh = hg, \ \gcd(|g|, |h|) = 1\Rightarrow|gh| = |g||h|\ \ (|a|$ = order of element $a)$","Let $G$ be a group and $g,h \in G$. I need to prove that if $g$ and $h$ commute and their orders are coprime, then $|gh| = |g||h|$, that is, the order of their product is the multiple of their orders. Since $gh = hg$, then $(gh)^{ lcm(|g|, |h|)} = e$, so, $|gh|$ divides $lcm(|g|, |h|)$. Since $\gcd(|g|, |h|) = 1$, then $lcm(|g|, |h|) = |g||h|$. So, $|gh|$ divides $|g||h|$.","['abstract-algebra', 'group-theory']"
1836306,I want to show that $\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3$,"I want to show that $$\int_{-\infty}^{\infty}{\left(x^2-x+\pi\over x^4-x^2+1\right)^2}dx=\pi+\pi^2+\pi^3$$ Expand $(x^4-x+\pi)^2=x^4-2x^3+2x^2-2x\pi+\pi{x^2}+\pi^2$ Let see (substitution of $y=x^2$) $$\int_{-\infty}^{\infty}{x\over (x^4-x^2+1)^2}dx={1\over 2}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ Substituion of $y=x^3$ $$\int_{-\infty}^{\infty}{x^3\over (x^4-x^2+1)^2}dx={1\over 4}\int_{-\infty}^{\infty}{1\over (y^2-y+1)^2}dy$$ As for $\int_{-\infty}^{\infty}{x^2\over (x^4-x^2+1)^2}dx$ and $\int_{-\infty}^{\infty}{x^4\over (x^4-x^2+1)^2}dx$ are difficult to find a suitable substitution. This is the point where I am shrugged with to find a suitable substitution To lead me to a particular standard integral. Need some help, thank. standard integral of the form $$\int{1\over (ax^2+bx+c)^2}dx={2ax+b\over (4ac-b^2)(ax^2+bx+c)}+{2a\over 4ac-b^2}\int{1\over ax^2+bx+c}dx$$
And $$\int{1\over ax^2+bx+c}dx={2\over \sqrt{4ac-b^2}}\tan^{-1}{2ax+b\over \sqrt{4ac-b^2}}$$","['integration', 'definite-integrals', 'calculus', 'proof-verification']"
1836312,Integral involving the von Mises-Fisher distribution,"I'm going quickly through the VonMises-Fisher distribution $M$ on $\mathbb S^{d-1}$ and its properties. Its probability density function is: $$f(x; \kappa,\mu)= c(\kappa)\exp(\kappa x^T\mu)$$ where $\kappa \times \mu \in[0,\infty)\times \mathbb S^{d-1}$ are fixed parameters and $c(\kappa)$ is a normalizing constant so the whole thing will integrate $1$. Specifically, $$c(\kappa)=\left(\frac\kappa2\right)^{d/2-1}\frac1{\Gamma(d/2)I_{d/2-1}(\kappa)}$$
where $\Gamma$ is the gamma function and $I_\nu$ is the modified Bessel function of the first kind and order $\nu$ is
$$I_\nu(\kappa) = \frac{(\kappa/2)^\nu}{\Gamma(\nu +\frac12)\Gamma(\frac12)}\int_{-1}^1e^{\kappa t}(1-t^2)^{\nu -\frac12}dt.$$ The paper I'm reading mentions the following equality without further explanation $$\int_{\mathbb{S}^{d-1}_{\geq\frac1r}}f_\kappa(\xi) \, d\xi = c(\kappa) \frac{s_{d-2}}{s_{d-1}} \int_0^{\arccos(-\log(rc(\kappa))/\kappa)} e^{\kappa\cos\theta} \sin^{d-2} \theta \, d\theta$$
  where $s_{d-1}=\displaystyle \frac{2\pi^{\frac d2}}{\Gamma(\frac d2)}$ and 
  $$\mathbb S_{\geq\frac1r}^{d-1} =\{x\in\mathbb S^{d-1}:f(x;\kappa,\mu)\geq r\}$$ for a fixed $r>0$ I'm a bit rusty on multivariable integration. I thought on spherical coordinates, but I'm not sure how does it work when I'm working on $d-1$ dimensions. How can I get the highlighted equality?","['multivariable-calculus', 'integration', 'definite-integrals', 'probability-distributions']"
1836343,"Q27 from AMC 2012(Senior): Five consecutive integers that sum to a perfect square, and the three middle terms sum to a perfect cube.","Five consecutive integers $p,q,r,s,t$ ,each less than $10000$ , produce a sum which is a perfect square, while the sum of $q,r,s$ is a perfect cube.What is the value of $ \sqrt{p+q+r+s+t}$ ? What I have tried so far: Let $p=r-2$ $p+q+r+s+t =5r $ $5r=x^2 $ $q+r+s =3r =y^3 $ $x^2 -y^3 =2r $ So,the only perfect squares which are divisible by $5$ are the multiples of $5$ : $25,100,225,400...$ I also observed that $100-25=75,225-100=125$ ,where $125-75=50$ .Trying that for $225-100=125,400-225=175$ where $175-125=50$ Then,for the perfect cube which are divisible by 3 and must be less than the perfect squares. $30^3,60^3,90^3,120^3....$ And here is were I got stuck at... Is the concept I'm using correct?","['algebra-precalculus', 'contest-math', 'elementary-number-theory']"
1836350,Antiderrivative of ${d^2 y \over dx^2} = 1-x^2$,"At any point $(x,y)$ on a curve, ${d^2 y \over dx^2} = 1-x^2$, and an equation of the tangent line to the curve at the point $(1,1)$ is $y=2-x$. Find an equation of the curve. This is what I've done $${d^2 y \over dx^2} = 1-x^2
\\ \int dy' = \int (1-x^2)dx
\\y' = x- {x^3 \over 3} +C
\\ {dy\over dx}=x- {x^3 \over 3} +C
\\\int dy =\int (x- {x^3 \over 3} +C)dx
\\y= {x^2 \over 2}-{1\over 3} \cdot {x^4 \over 4}+c_1x + c_2
\\ y= {x^2 \over 2}-{x^4 \over 12}+c_1x +c_2$$ Do I now substitute (1,1) in this? I don't this is right. Someone help me. Thank you!","['derivatives', 'integration', 'calculus', 'proof-verification']"
1836351,Are convex combinations of projection operators still projection operators?,"If $P_1, P_2: V \to V$ are linear projection operators on the vector space $V$ with $R := P_1(V) = P_2(V)$, is it true that any convex combination of $P_1$ and $P_2$ is again a projection operator $P_3$ with $P_3(V) = R$? I'm trying to figure out whether or not the convex combination of two Ehresmann connections on a fiber bundle is again an Ehresmann connection, as is seemingly implied by the first paragraph of $\S$2 of this paper .","['differential-geometry', 'linear-algebra', 'differential-topology']"
1836389,Finding Limit of Nested/Continued Logarithm,"For a sequence $a_n$ defined by: $$a_1 = \ln(1)$$ $$a_2 = \ln\left(\frac{1}{\ln(2)}+1\right)$$ $$\dots a_n = \ln\left(\frac{1}{\ln(\frac{1}{\ln(\dots 1/\ln(n
))}+1)}+1      \right)$$ with $n$ logarithms. Could someone offer a solution/hint for the limit: $$\lim_{n\to\infty}a_n$$ Thank you kindly! Note: My initial attempts involved simplifying to $a_n = ln\left( 1/a_n + 1 \right)$ but I wasn't sure as to where to go from there (not to mention, I noticed this is sort of cheating if we wanted to find the limit purely analytically, since verifying that the limit converges was done through graphing). Another note: As Mc Cheng already pointed out, the limit does converge to some number, but I also noticed that if we were to subsitute the $n$ in the sequence with another variable $x$, then the limit of increasing values of $a_n$ as $x \to \infty$ seem to flip-flop between being an under-approximation and over-approximation (e.g., $\lim_{x\to\infty} a_4 < \lim_{n\to\infty}a_n,$ but $\lim_{x\to\infty} a_5 > \lim_{n\to\infty} a_n$), and the approximations (obviously) get closer to the actual value of $\lim_{n\to\infty} a_n$, so I wonder if the sequence can be turned into an alternating series?","['logarithms', 'limits']"
1836390,Ref. Requst: Space of bounded Lipschitz functions is separable if the domain is separable.,"I have been scouring the internet for answers for some time and would therefore appreciate a reference or a proof since i'm not able to produce one myself. Let $(\mathcal{X},d)$ be a metric space, and let $$
BL(\mathcal{X})=\{f:\mathcal{X}\to \mathbb{R}\, \, | \, \, f \text{ is Lipschitz and bounded}\}
$$
denote the bounded real-valued Lipschitz countinuous functions on $\mathcal{X}$. Does anyone know a reference for the statement: If $(\mathcal{X},d)$ is separable then $(BL(\mathcal{X}),||\cdot||_\infty)$ is separable?","['functional-analysis', 'reference-request', 'general-topology', 'lipschitz-functions']"
1836416,Sine identity involving (3/p) for prime p greater than 3.,"I am working through Ireland and Rosen's ""Classical Introduction to Modern Number Theory"" and am very stuck on this problem (#34 in Chp 5, 2nd edition): Note that $(a/b)$ is the Legendre symbol (or Jacobi symbol, if you prefer). ""If $p$ is an odd prime distinct from $3$ show that $(3/p)=\prod\limits_{j=1}^{\frac{p-1}{2}}(3-4\sin^2\frac{2\pi j}{p}).$"" I have verified the claim computationally for $p=5$ and $p=7$, and drawn some nice little plots of $5-$ and $7-$ gons in the complex plane, but this has given me no real insight that I can generalize to higher primes. I suspect I am missing some simple but esoteric trigonometric fact. If it is not asking too much, I would strongly prefer some hints instead of complete answers so that I can solve it mostly on my own, and I promise to post my complete answer later when (if?) I solve it. Many thanks. UPDATE After many days of nonsense in which I used a double angle formula incorrectly because it was 5am, I have the following: For a given prime $p$, let $z_k=e^{(2\pi ki)/p}$. Then, $|z_k^2-1|=2\sin\left(\frac{2\pi k}{p}\right)$. I will post the full proof of this in my answer when I eventually get it. I now have the original RHS as $\prod\limits_{j=1}^{\frac{p-1}{2}}(3-4|z_j^2-1|^2)$. I am trying not to use advanced techniques such as those advocated in the comments. Hints on where to go from here are greatly appreciated.","['abstract-algebra', 'trigonometry', 'complex-numbers', 'elementary-number-theory']"
1836440,Identity relation vs Reflexive Relation,"So we're starting relations in my discrete structures class this week, and I've probably read this over 10 times by now...I believe I have a good understanding of Identity Relations, but Reflexive Relations seem to have me slightly confused. From my understanding, an example of Identity relation using set $A = \{1,2,3,4\}$ $R_1 = \{(1,1), (2,2), (3,3), (4,4)\}$ because each element is equal to itself. $R_2 =\{ (1,1), (2,2), (3,3), (4,4), (1, 4)\}$ would not be an identity relation, as $1 \neq 4$. What I don't understand is why The relation $R_2$ defined by $R_2 = \{(1, 1), (3, 3), (2, 1), (3, 2)\}$ is not a reflexive relation on $A$, since $(2, 2) \notin R_2$. is not a Reflexive Relation Could someone give me an example of what a simple reflexive relation is, and isn't? Thanks all for the input, see below for a good example of a Reflexive Relation Here's what the book describes both as: Identity relation. Let $A$ be any set. Then the relation $R = \{(x, x) : x \in A\}$ on $A$ is called the identity relation on $A$. Thus, in an identity relation, every element is related to itself only. For example, consider $A = \{a, b, c\}$ and define relations $R_1$ and $R_2$ as follows.
  $R_1 = \{(a, a) ,(b, b), (c, c)\}$ $R_2 = \{(a, a), (b, b), (c, c), (a, c)\}$ Then $R_1$ is an identity relation on $A$, but $R_2$ is not an identity relation on $A$ as the element $a$ is related to $a$ and $c$. Reflexive relation. A relation $R$ on a set $A$ is said to be a reflexive relation if every element of $A$ is related to itself. Thus, $R$ is reflexive iff $(x, x) \in R$ for all $x \in A$.
  A relation $R$ on a set $A$ is not reflexive if there is an element $x \in A$ such that $(x, x) \notin R$.
  For example, consider $A = (1, 2, 3)$. Then the relation $R_1$ defined by $R_1 = \{(1, 1), (2, 2), (3, 3), (1, 3), (2, 1)\}$ is a reflexive relation on $A$.
  The relation $R_2$ defined by $R_2 = \{(1, 1), (3, 3), (2, 1), (3, 2)\}$ is not a reflexive relation on $A$, since ($2, 2) \notin R_2$.
  Remark Every identity relation on a non-empty set $A$ is a reflexive relation, but not conversely. Consider $A = \{a, b, c\}$ and define a relation $R$ by $R = \{(a, a), (b, b), (c, c), (a, b)\}$. Then $R$ is a reflexive relation on $A$ but not an identity relation on $A$ due to the element $(a, b)$ in $R$.",['discrete-mathematics']
1836491,Additivity of trace,"Let $A$ be a finitely generated abelian group and $\alpha:A\to A$ be an endomorphism. Since $A=A_{free}\oplus A_{torsion}$, we can induce $\bar \alpha:A_{free}\to A_{free}$, i.e. $\bar\alpha$ is a map from $\oplus\mathbb Z$ to itself. Write $\bar\alpha$ as  a matrix form and define $tr(\alpha)=tr(\bar\alpha)$ as the trace of the matrix. Assume we have short exact sequence of finitely generated abelian groups $A,B,C$
and endomorphisms $\alpha,\beta,\gamma$ where the following diagram commutes.
$$\begin{array}{c} 0 & \to & A & \to & B & \to & C & \to & 0 \\ & & \!\downarrow \alpha && \!\downarrow\beta && \!\downarrow\gamma & \\ 0 & \to & A & \to & B & \to & C & \to & 0\end{array}$$ Prove $tr(\beta)=tr(\alpha)+tr(\gamma)$.","['abelian-groups', 'abstract-algebra', 'exact-sequence', 'group-theory']"
1836496,"Prove that $\exp(A+B)=\exp(A)\exp(B)$ iff $[A,B] = 0$","I have searched throughout the forum and online as well, and I got that with condition of $[A,B]=0$, $e^{(A+B)t}=e^{At}e^{Bt}$. Now the question is, to show for any matrices $A$ and $B$, it is true that $e^{(A+B)t}=e^{At}e^{Bt}$ for all t if and only if $[A,B]=0$ Well, I asked my professor and he told me to show what if $AB \neq BA$, and I really have no idea how to condense those massive equations.... The reason why is that $[A,B]=0$ is not a condition given in this question and I'm confused.","['ordinary-differential-equations', 'linear-algebra']"
1836497,Find all functions $f\colon \mathbb{R}^* \to \mathbb{R}^* $ satisfying $f(xyz)=f(xy+yz+xz)\big(f(x)+f(y)+f(z)\big)$ when $xy+yz+xz\ne 0$,"Find all functions $f\colon \mathbb{R}^* \to \mathbb{R}^* $ from the non-zero reals to the non-zero reals, such that $$f(xyz)=f(xy+yz+xz)\big(f(x)+f(y)+f(z)\big)$$ for all non-zero reals $x, y, z$ such that $xy+yz+xz\ne 0$ . I think that only two solutions are: $f(x)=\frac{1}{3}$ and $f(x)=\frac{1}{x}$ . I would appreciate any suggestions.","['algebra-precalculus', 'functional-equations']"
1836504,The projective space as a homogeneous space,I want to understand why the projective space $\mathbb RP^n$ is diffeomorophic to $SO(n+1)/O(n)$? and why we can write the latter as $O(n+1)/O(n)\times O(1)$?,"['group-actions', 'homogeneous-spaces', 'differential-geometry']"
1836506,"Calculate $\lim_{x\to0}{F(x)\over g(x)}$, where $ g(x)=x$ and $F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt$.","Calculate $\displaystyle\lim_{x\to0}{F(x)\over g(x)}$, where $ g(x)=x$ and 
$\displaystyle F(x)=\int_0^x {e^{2t}-2e^t+1\over 2\cos3t-2\cos2t+\cos t} \, dt$. i'd love for someone to explain not only the technical procedure here but also what are the theorems that allow it to take place. To my understanding, the Fundamental theorem of calculus is key here and Newton-Leibniz theorem also contributes. Even though I do feel I understand them as well as the subtle connection they made between antiderivatives and definite integrals I can't seem to apply any of that when it comes to limits and proofs.","['definite-integrals', 'limits']"
1836537,Eigenvalues and eigenvectors of $I \otimes A \ + \ B^T \otimes I$ (used in Sylvester's equation),"Let $A$ and $B$ be $n \times n$ square matrices, with resp. eigenpairs $(\lambda_i,U_i)$ and $(\mu_j,V_j)$. Let $I_n$ be the order $n$ identity matrix. I have seen a result that says that the $n^2$ eigenvalues of $$M_{AB}:=I_n \otimes A \ + \ B^T \otimes I_n$$ are all the $\lambda_i+\mu_j$. My question is: how is it possible to prove it, and under which conditions is it true ? Another issue: can something be said about the eigenvectors of $M_{AB}$ ? Appendix: The context is that of Sylvester equation : Let notation $M^{vec}$ be associated with the ""vectorialization"" of a matrix $M$, obtained by ""stacking"" its columns like in the following example:
\begin{equation}
M = \begin{pmatrix}a&c&e\\
b&d&f\end{pmatrix} \ \rightarrow \ M^{vec} = \begin{pmatrix}a\\b\\c\\
d\\e\\f\end{pmatrix} 
\end{equation} We will need the fundamental technical property of Kronecker product: $$\underbrace{(ABC)^{vec}}_{vector}=\underbrace{(C^T \otimes A)}_{matrix}.\underbrace{(B)^{vec}}_{vector} \ \ \ (*)$$ Sylvester equation is $$\text{find} \ X \ \ \ \text{such that} \ \ \ AX+XB=C$$ Being evidently equivalent to $$(AX)^{vec} + (XB)^{vec} = C^{vec}$$ it can can be transformed, using property (*) and linearity of Kronecker's product, into: $$(I \otimes A \ + \ B^T \otimes I)X^{vec}=C^{vec}$$ explaining the usefulness of matrix $M_{AB}=I \otimes A \ + \ B^T \otimes I$, its inversibility being a condition for a unique solution to Sylvester's equation, this inversibility being determined by the fact that none of its eigenvalues is zero.","['eigenvalues-eigenvectors', 'matrix-equations', 'kronecker-product', 'matrices', 'linear-algebra']"
1836578,Find the sides of the triangle.,"The triangle with sides $8-15-13$ has a $60^{\circ}$ angle. The triangle with sides $11-35-31$ also has a $60^{\circ}$ angle. Find a triangle $x-y-403$ where $x$ and $y$ are relatively prime positive integers and the angle opposite the side of length $403$ is $60^{\circ}$ angle. I have been trying this problem but doesn't get the solution. I am unable to find how I can use the given info to solve this. Is the given information sufficient to solve the problem? If yes, can anybody please tell me how I can use them. Thanks in advance.","['triangles', 'geometry']"
1836682,Calculating the gcd of complex numbers,"I need help in calculating the gcd of complex numbers For Example: $\gcd(3+i,1-i)$. The problem is,I don't even know what's the algorithm for complex numbers...","['number-theory', 'complex-numbers', 'gcd-and-lcm', 'elementary-number-theory']"
1836693,What is the use of hyperreal numbers?,"For sometime I have been trying to come to terms with the concept of hyperreal numbers. It appears that they were invented as an alternative to the $\epsilon-\delta$ definitions to put the processes of calculus on a sound footing. From what I have read about hyperreal numbers I understand that they are an extension of real number system and include all real numbers and infinitesimals and infinities. I am wondering if hyperreal numbers are used only as a justification for the use of infinitesimals in calculus or do they serve to have some other applications also (of which I am not aware of)? Like when we extend our number system from $\mathbb{N}$ to $\mathbb{C}$ at each step there is some deficiency in the existing system which is removed in the next larger system. Thus $\mathbb{Z}$ enables subtraction which is not always possible in $\mathbb{N}$ and $\mathbb{Q}$ enables division which is not always possible in $\mathbb{Z}$. The reasons to go from $\mathbb{Q}$ to $\mathbb{R}$ are non-algebraic in nature. The next step from $\mathbb{R}$ to $\mathbb{C}$ is trivial and is based on need to enable square roots, but since the existing $\mathbb{R}$ is so powerful, the new system of complex numbers exploits this power to create rich field of complex analysis. Does the system of hyperreal numbers use the existing power of $\mathbb{R}$ to lead to a richer theory (something like the complex analysis I mentioned earlier)? Or does it serve only as an alternative to $\epsilon, \delta$ definitions? In other words what role do the non-real hyperreal numbers play in mathematics? Since I am novice in this subject of hyperreal numbers, I would want answers which avoid too much symbolism and technicalities and focus on the essence.","['nonstandard-analysis', 'epsilon-delta', 'applications', 'infinitesimals', 'analysis']"
1836761,"How to prove $(\{2^n3^m\alpha\})_{m,n\in\mathbb{N}}$ is dense in [0,1]?","$\forall \alpha\in [0,1]\setminus\mathbb{Q}$, how to prove $(\{2^n3^m\alpha\})_{m,n\in\mathbb{N}}$ is dense in [0,1]? $\{x\}$ is the fractional part of x. Any hint would be appreciated!","['number-theory', 'equidistribution', 'dynamical-systems', 'elementary-number-theory']"
1836813,Find real parts of the complex roots of this $9^{th}$ order polynomial in explicit form,"I have a following polynomial. (See WolframAlpha ): $$x^9-6x^8+14x^7-16x^6+36x^5-56x^4+ 24x^3-320x+\frac{640}{9}=0 \tag{1}$$ Wolfram says that $(1)$ has three real roots and three pairs of complex conjugate roots. I know one of the real roots of this polynomial is equal to: $$x_1=\tan \left(\frac{1}{3} \arctan \frac{1}{3} \right)+\tanh \left(\frac{1}{3} \text{arctanh} \frac{1}{3} \right) =0.22267663636945$$ I know this is a root, because I derived the polynomial from this expression. Edit By @IvanNeretin's comment we also have two other real roots: $$x_2=\tan \left(\frac{\pi}{3} +\frac{1}{3} \arctan \frac{1}{3} \right)+\tanh \left(\frac{1}{3} \text{arctanh} \frac{1}{3} \right)$$ $$x_3=\tan \left(\frac{2\pi}{3} +\frac{1}{3} \arctan \frac{1}{3} \right)+\tanh \left(\frac{1}{3} \text{arctanh} \frac{1}{3} \right)$$ A very surprising (for me) discovery: all six complex roots have the same absolute value of the imaginary part: $$b=Im(x)= \pm \frac{\sqrt[3]{2}}{\sqrt{3}} (1+\sqrt[3]{2})= \pm 1.643902181980216,~~~~\text{for all } x \notin \mathbb{R}$$ I found it using ISC, but hadn't found closed forms for the real parts. How can we find explicitly the real parts of the complex roots? My thoughts - if we substitute in $(1)$: $$x=a \pm i b$$ then we obtain two equations for one real varibale $a$ (for each three pairs of complex roots of course), and by adding and subtracting these equations we can lower their order. Since there should be three distinct real parts $a$, we could probably bring it down to a cubic equation. But I need a CAS for that and I'm not sure it will lead to a solution.","['polynomials', 'hyperbolic-functions', 'roots', 'trigonometry']"
1836819,Integer Solutions to an Ellipse,"I'm trying to find positive integer solutions to the ellipse
$$x^2 - xy + y^2 - k^2 = 0$$
where $k$ is a constant. Specifically, I already have two solutions for a given $k$, and I'm trying to find a third, possibly by using the two known solutions. (I'm trying to copy the technique of producing rational solutions to a conic from one known rational point on the conic.) I have searched a lot on the Internet, but most resources either suggest checking all values within the range of the ellipse's 'box', or give methods for a particular type of ellipse. Edit: I found some questions which are similar to mine, but I cannot apply the technique used in them to my question mainly because I do not understand the technique. They all mention work by Fricke and Klein (1897). My questions are: How many [positive] integer solutions does a general ellipse have? How can we find them? (From scratch, or knowing a few solutions beforehand?)","['diophantine-equations', 'conic-sections', 'elementary-number-theory', 'geometry']"
1836858,"How is ""expressing"" a differential operator ""in cylindrical coordinates"" rigorously defined?","I'm a mathematician (with little knowledge of differential geometry) trying to study physics. One of the greatest problems is the language regarding coordinate transformations. I tend to think of such transformations as functions (diffeomorphisms), whereas physicists just rename the arguments, e.g. $f(x,y,z)=f(\rho,\varphi,z)$. I've gotten used to that and for some things (e.g. integration) it works just fine. But I can't get my head around the transformation (?) of differential operators. For example: Let $f:\mathbb{R}^3\backslash\big(\{0\}\times\{0\}\times\mathbb{R}\big)\rightarrow\mathbb{R}$ be smooth. I have before me the statement that ""in cyclindrical coordinates""
$$\nabla f=\vec{\mathbf{e}}_\rho\frac{\partial f}{\partial\rho}+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\frac{\partial f}{\partial\varphi}+\vec{\mathbf{e}}_z\frac{\partial f}{\partial z}.$$ Now you probably consider me a pedant, but I can only try to understand that by introducing the mapping $\theta:]0,\infty[\times\mathbb{R}\times\mathbb{R}\rightarrow\mathbb{R}^3$,
$$\theta(\rho,\varphi,z)=(\rho\cos\varphi,\rho\sin\varphi,z).$$ My understanding is that the above equation is actually
$$\nabla(f\circ\theta)=\vec{\mathbf{e}}_\rho\partial_1(f\circ\theta)+\vec{\mathbf{e}}_\varphi\frac{1}{\rho}\partial_2(f\circ\theta)+\vec{\mathbf{e}}_z\partial_3(f\circ\theta).$$
Or is it $\left(\nabla f\right)\circ\theta$ instead of $\nabla(f\circ\theta)$? And what are these vectors $\vec{\mathbf{e}}_\rho,\vec{\mathbf{e}}_\varphi,\vec{\mathbf{e}}_z$? I read that $\vec{\mathbf{e}}_\varphi$ is the ""unit vector in $\varphi$-direction"". But what does that even mean? How can one express these ""unit vectors"" using $\theta$? There is a lot in the literature about how to derive such equations, but I can't really use it because I don't understand the meaning behind the symbols and I don't really understand the point of it all.","['multivariable-calculus', 'differential-geometry', 'vector-analysis']"
1836862,Importance of the homogeneity assumption in definition of linear map,"Let $V$ and $W$ be vector spaces over field $F$.  A function $f: V \rightarrow W$ is said to be linear if for any two vectors $x$ and $y$ in $V$ and any scalar $\alpha\in F$, the following two conditions are satisfied: $f(x + y) = f(x) + f(y)$ $f(\alpha x) = \alpha f(x)$ Let $F$ be a field of real numbers. Is it possible to construct $f$ such that the first condition is satified but not the second one?",['linear-algebra']
1836875,More than 99% of groups of order less than 2000 are of order 1024?,"In Algebra: Chapter 0 , the author made a remark (footnote on page 82), saying that more than 99% of groups of order less than 2000 are of order 1024. Is this for real? How can one deduce this result? Is there a nice way or do we just check all finite groups up to isomorphism? Thanks!","['p-groups', 'finite-groups', 'abstract-algebra', 'groups-enumeration', 'group-theory']"
1836895,How are varieties related polynomials?,My teacher says that varieties and ideals are related to each other while I tend to mix polynomials and varieties in my terminology. Could some explain how varieties are related to polynomials? And why it is important not to mix varieties and polynomials together?,"['polynomials', 'algebraic-geometry', 'terminology', 'soft-question', 'ideals']"
1836898,Minimal generating set of Rubik's Cube group,"The Rubik's Cube group is generated by the six moves $\{F,B,U,D,L,R\}$. However, is this the minimal generating set for the group? In other words, can I simulate the move $F$ just by making the moves $B,U,D,L,R$? If I try this out on an actual Rubik's cube, it doesn't seem quite simple (starting from a solved state twist the front face and try to solve the cube without turning it again), but I don't see any reason why it would be impossible either.","['permutations', 'rubiks-cube', 'recreational-mathematics', 'group-theory']"
1836928,Can the coefficients of a Dirichlet series be recovered? [duplicate],"This question already has an answer here : For all Dirichlet series, is $a_n$ unique to $f(s)$? (1 answer) Closed 6 years ago . Specifically if I have a known function $F(s)$ is there some way I can find a function $f(n)$ that satisfies this equation? $$F(s) = \sum_{n=1}^\infty \frac{f(n)}{n^s}$$ I'm imagining something similar to finding the coefficients of a Fourier series.","['complex-analysis', 'dirichlet-series']"
1836931,Solve in integers the equation $\sqrt{x^3-3xy^2+2y^3}=\sqrt[3]{13x+8}$,"Solve in integers the equation 
  $$\sqrt{x^3-3xy^2+2y^3}=\sqrt[3]{13x+8}$$ My work so far: I used www.wolframalpha.com . Then $x=9,y=8 -$ solution. My attempt: 1) Let $\sqrt{x^3-3xy^2+2y^3}=a, \sqrt[3]{13x+8}=b$. Then 
$$\begin{cases}
a=b\\
a^2-b^3=x^3-3xy^2+2y^3-13x-8 \in \mathbb Z
\end{cases}$$ 2) $$(x^3-3xy^2+2y^3)^3=(13x+8)^2$$ Addition: We have $$A^6\cdot B^6=(13x+8)^2$$
and $$n^6 \equiv 0, \pm 1 (\bmod 13)$$","['number-theory', 'integers', 'diophantine-equations']"
1836945,What shape is the Sage logo,Just curious what is this shape used by Sage as its logo?,"['polyhedra', 'geometry']"
1836980,"Fake proof that $\frac{e^x-1}{e^x+1}=e^x$, via integrating $\operatorname{sech} x$ in two ways","We start with the integral: $$\int \text{sech}(x)dx$$ Method 1 \begin{align}
\int \text{sech}(x)dx & = \int\frac{2}{e^x+e^{-x}}dx \\
&= \int\frac{2e^x}{e^{2x}+1}dx
\end{align}
Using the substitution $u=e^x$, \begin{align}
\int \text{sech}(x)dx & = \int\frac{2}{u^2+1}du \\
&= 2\text{ arctan}(u)+c \\
&= 2\text{ arctan}(e^x)+c
\end{align} Method 2 \begin{align}
\int \text{sech}(x)dx & = \int\frac{\text{cosh}(x)}{\text{cosh}^2(x)}dx \\
& = \int\frac{\text{cosh}(x)}{\text{sinh}^2(x)+1}dx
\end{align}
Using the substitution $u=\text{sinh}(x)$,
\begin{align}
\int \text{sech}(x)dx & = \int\frac{1}{u^2+1}du \\
&= \text{arctan}(\text{sinh}(x))+c \\
&= 2\text{ arctan}(\text{tanh}(\frac{x}{2}))+c \\
&= 2\text{ arctan}(\frac{e^x-1}{e^x+1})+c
\end{align} Thus, we obtain: $$\frac{e^x-1}{e^x+1}=e^x$$ However, I see no reason why they are equal. Did I do something wrong in the calculation?","['fake-proofs', 'exponential-function', 'integration', 'trigonometry']"
1836984,For what integral value of $n$ is $3\pi$ the period of the function $\cos(nx)\sin(5x/n)$?,"For what integral value of $n$ is $3\pi$ the period of the function $\cos(nx)\sin(5x/n)$ ? What should be the correct approach to this problem?Will taking the LCM of the periods of the two functions in product be equal to the period of the whole function? I tried like period of $\cos(nx)$ is $2\pi$/n and $\sin(5x/n)$ is $2\pi n/5$ 
So the period should be L.C.M of $2\pi$/n and $2\pi n/5$.Which is equal to $2\pi n/\gcd(n,5)$. However after this I'm not being able to proceed. Help please!","['algebra-precalculus', 'functions']"
1836988,"If deg$(v) \geq k$ for all $v \in V(G)$, then G contains a matching of cardinality $\min \{k,\lfloor{\frac{|V|}{2}}\rfloor\}$","Let$G = (V; E)$ be an undirected graph. Show that if deg$(v) \geq k$
  for all $v \in V$, then G contains a matching of cardinality $\min \{k,\lfloor{\frac{|V|}{2}}\rfloor\}$. I have no idea how to solve this problem. Can anyone give me a hint? Thank you in advance!","['graph-theory', 'discrete-mathematics']"
1836998,Dilemma about value of limit,"$$
\lim_{x\to 0^{+}}\left[\left(1+\frac{1}{x}\right)^x+\left(\frac{1}{x}\right)^x+\left(\tan(x)\right)^{\frac{1}{x}}\right]$$ Attempt: I used $\tan(x)\approx x$ also $(1+n)^{1/n}=e$ so I let $x=0+h$ and lim changes to lim h tending to $0$. But that gives me $e+\infty+h^{1/h}$. While the answer is an integer between $0-9$ . Where is my mistake?.Thanks","['limits-without-lhopital', 'limits']"
1837007,Can one factor matrices?,I know that one can factor integers as a product of prime numbers. Is there an analog of it to matrices? Can we define prime matrices such that every matrix is a product of prime matrices? Is there any applications of factorization of matrices?,"['matrices', 'prime-factorization', 'linear-algebra']"
1837011,Subgroups of finite abelian groups.,"For every subgroup $H$ of a finite abelian group $G,$ there exists a subgroup $N$ of $G$ such that $G/N \cong H.$ I need to prove this or give a counter example. I am aware of isomorphism theorems and classification of abelian groups, direct products etc.","['finite-groups', 'abelian-groups', 'group-theory']"
1837012,Polynomials generating the same $p$-adic fields,"I wonder if the following fact is true: Pick $l\in \mathbb N$ a number and let $f,g\in \mathbb Z_p[x]$ be monic polynomials with coefficients in the ring of $p$-adic integers such that $f\equiv g \pmod{p^l}$ and they are irreducible mod $p^l$.
Then the roots of $f$ generate the same field as the roots of $g$. Can someone help me proving this or finding a counterexample?","['abstract-algebra', 'polynomials', 'algebraic-number-theory', 'p-adic-number-theory']"
1837032,How many arrangements of the letters in the word CALIFORNIA have no consecutive letter the same?,"First off, the correct answer is $$584,640 = {10!\over 2!2!}- \left[{9! \over 2!}+{9! \over 2!}\right] + 8!$$ which can be found using the inclusion-exclusion principle. My own approach is different from the above: In the word CALIFORNIA, we have 2 repeating A's and 2 I's, and 6 remaining unrepeated letters. We first place the 6 unrepeated letters, a total of 6! arrangements. Then, to avoid the A's and B's in consecutive positions, we place the 2 A's and 2 B's between the 6 letters, including the beginning and ending positions, which gives us 8 possible positions. The number of possible arrangements is the permutation of 7 out of 4, but we have to divide out the repeating A's and B's, which is $${7!\over 2!2!3!}$$ So in total, we have $${6!7!\over 2!2!3!} = 151,200$$ which is obviously different from the correct answer. Why is this wrong, and if possible, how I can fix this using the same approach?",['combinatorics']
1837062,A System of Infinite Linear Equations,"Suppose that $\{a_{i}\}_{i=-\infty}^{\infty}$ with $\sum_{i=-\infty}^\infty a_{i} \lt \infty$ is known and that $\{b_i\}_{i=-\infty}^{\infty}$ is such that $$\sum_{i=-\infty}^\infty a_{i}b_{-i} =1,$$ and that, for all $k \in \mathbb Z/\{0\}$, $$\sum_{i=-\infty}^\infty a_{i}b_{-i+k} =0.$$ Is it possible to solve for $\{b_{i}\}_{i=-\infty}^{\infty}$ as a function of $\{a_{i}\}_{i=-\infty}^{\infty}$? I just can't figure out where to start from.","['orthogonality', 'sequences-and-series', 'systems-of-equations']"
1837070,Is boundedness required in equivalence between $\frac1n\sum_{k=1}^na_k\to0$ and $\frac1n\sum_{k=1}^na_k^2\to0$?,"Suppose $a_n$ is a sequence of non-negative real numbers. If $a_n$ are un-bounded, then I want to know if $\dfrac{1}{n}\sum_{k=1}^na_k\to0$ as $n\to\infty$ is equivalent to $\dfrac{1}{n}\sum_{k=1}^na_k^2\to0$ as $n\to\infty$. I believe it is not true, but cannot figure out an argument. Some sort of $a_n$ taking very large values after very long distances...","['cesaro-summable', 'real-analysis', 'examples-counterexamples', 'sequences-and-series']"
1837079,Simplifying Trace of a Matrix Expression ${\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)$,"Let $B$ be a symmetric invertible matrix  and let $A$ be a diagonal matrix with non-zero entries on the main diagonal. I am trying to simplify the following expression 
\begin{align}
{\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)
\end{align}
where $D=I+ ABA$ Case that I did: In the one dimensional case we get 
\begin{align}
\frac{B}{(1+BA^2)^2}.
\end{align} Also in the case that $A = a I$ (all diagonal elements are the same) we get \begin{align}
(I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T = (I+ BA^2)^{-1} B (I+ BA^2)^{-1}
\end{align} 
and therefore the trace becomes
\begin{align}
{\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right).
\end{align} My question is what can we say about a more general diagonal $A$? Can we show also that 
\begin{align}
{\rm Tr} \left( (I- D^{-1} BA^2) B (I- D^{-1} BA^2)^T \right)= {\rm Tr} \left( (1+ABA)^{-1} B(1+ABA)^{-1} \right)= {\rm Tr} \left(  B(1+ABA)^{-2} \right)
\end{align} Note that another way to look at this problem is to look through the norm operator
\begin{align}
\| I- D^{-1} BA^2 \|
\end{align} I wounder if there is a software that can take care of this simplification.","['matrices', 'trace', 'matrix-equations', 'matrix-decomposition']"
1837094,Gauss measure is not a pushforward of product measure,"Let $ N = \{1,2,3, \ldots \}$. We define $\varphi : N^N \mapsto [0,1]$ as 
$$ \varphi \left( (a_n)_{n \in_N} \right) = [0;a_1, a_2, \ldots ]$$
Where the expression on the right is a infinite continued fraction:
$$[0;a_1,a_2,a_3,\dotsc] = \cfrac{1}{a_1+\cfrac{1}{a_2+\cfrac{1}{a_3+\cdots}}}$$
 It is standard exercise that this is borel mapping (continuous in fact, see here ). We define the Gauss measure on $[0,1]$ as:
$$ d\nu = \frac{1}{\ln(2)}\,\frac{dx}{1+x} $$ Let $\mu$ be a probability measure on $N$. By a product measure on $N^N$ i mean probabilistic measure $\widetilde { \mu }$ such that for any set of the form $F = F_1 \times \cdots \times F_n \times N \times N \times \cdots$ holds equality 
$$ \widetilde {\mu } (F) = \prod_{n=1}^n \mu (F_k) $$
Such a measure exists and is unique due to teorem about extending funtion on ring to a measure. I am to prove that the Gauss measure $\nu$ is not a pushforward of any such measure $\mu$, meaning that for any $\mu$ the equality 
$$ \nu( A ) = \mu ( \varphi^{-1} [A])$$
cannot hold for every borel $A \subset [0,1]$. I am looking for any help. I have no experience with infinite measure products. This is an exercise for my ergodic theory course. I had obtained a tip that i don't understand: Product measure has the property that basis clopen sets with disjoint supports are stochastically independent. Gauss measure does not have this property.","['ergodic-theory', 'measure-theory']"
1837101,Function measurable iff the components are?,"If $\boldsymbol{f}:X\to\mathbb{R}^n$ is a $\mu$-measurable function, I think it is quite easy to see that its components $f_i$ also are. In fact, the projection $\pi_i:\mathbb{R}^n\to\mathbb{R}$, $\boldsymbol{x}\mapsto x_i$ is continuous and therefore Borel measurable, and $f_i=\pi_i\circ\boldsymbol{f} $. Therefore the counterimage through $\pi_i$ of a Borel subset of $\mathbb{R}$ is a Borel subset of $\mathbb{R}^n$, whose counterimage through $\boldsymbol{f}$ is a $\mu$-measurable subset of $X$. I cannot prove the converse, i.e. that, if $f_1,\ldots,f_n$ are $\mu$-measurable functions, then $\boldsymbol{f}=(f_1,\ldots,f_n)$ is, but I would not be amazed if it were true. Is it and, if it is, how can it be proved? I thank any answerer very much.","['real-analysis', 'measure-theory']"
1837104,When is $2^m3^n +1$ the square of some integer?,"Find all pairs of natural numbers $(m, n)$ for which $2^m3^n +1$ is the square of some integer. The powers of $2$ modulo $10$ cycle as $2,4,8,6,\ldots$ and the powers of $3$ as $3,9,7,1,\ldots$. The square of am integer needs to end in $0,1,4,5,6,$ or $9$. Thus, $2^m3^n$ ends in $4$ or $8$. Therefore, if $m \equiv 1 \pmod{4}$, then $n \equiv 2 \pmod{4}$; if $m \equiv 2 \pmod{4}$ then $n \equiv 3,0 \pmod{4}$; if $m \equiv 3 \pmod{4}$, then $n \equiv 1,0 \pmod{4}$; if $m \equiv 0 \pmod{4}$, then $n \equiv 1,2 \pmod{4}$. Do I preform casework from here or is there an easier way?",['number-theory']
1837106,Prove this using counting techniques: $\sum_{k=0}^{n}{\binom{2n+1}k} = 2^{2n}$,"I recently came across a question while studying for an exam. I haven't been able to solve it. We had to prove:
$$\sum_{k=0}^{n}{2n+1\choose k} = 2^{2n}$$ We had to use counting techniques. This was my attempt Let S be the set of all subsets of [1....2n]. We know that the size of S is $2^{2n}$
Another way of counting the subsets of [1....2n] is ????? ... Therefore, since we've used two different methods to count the same thing, then 
$$\sum_{k=0}^{n}{2n+1\choose k} = 2^{2n}$$ My problem is, I can't think of a second way to count the subsets such that it equals the summation. Am I on the right track here, or is there another set of objects I can count to make the proof easier? Thanks for the help.","['binomial-coefficients', 'combinatorial-proofs', 'permutations', 'combinatorics', 'summation']"
1837116,"Find $\lim_{(x,y)\to(0,0)} g \left(\frac{x^4 + y^4}{x^2 + y^2}\right)$ where $\lim_{z\to 0}\frac{g(z)}{z}=2.$","This limit seems different to me than all the other multi variable limits already asked on this site. Let $g \colon \mathbb R \to \mathbb R $ be such that $$ \lim_{z\to
 0}\frac{g(z)}{z}=2. $$
  Evaluate if the limit $$\lim_{(x,y)\to(0,0)} g \left(\frac{x^4 +
 y^4}{x^2 + y^2}\right)$$ exists, and if it does, determine it. I tried to approach the limit with for example $x=0$, $y=0$ etcetera, but I'm not even sure where for example $g(x^2)$ so $g(z)$ goes to. 
Also, polar coordinates does not seem the way to go here.","['multivariable-calculus', 'real-analysis', 'limits']"
1837181,"How to show this formula to get a square root of a number in ""just few seconds"" is true?","I don't remember in which topic I found it but I know it was there. And I still have not find a proof of this nice approximation. Let $x$ be a non perfect square number. If $y$ is the closer perfect square to $x$ such that $y < x$ then $$\sqrt{x}\approx \sqrt{y}+\frac{x-y}{2\cdot \sqrt{y}}$$ And it gives at the maximum two correct decimals after the decimal point. My first reaction to try to find from where this formula goes, was to expand it, I found $$2\sqrt{xy}\approx y +\sqrt{2y}\cdot(x-y)$$ and I tried to find a remarkable identity but I failed and I'm still stuck there. Also I don't know how should I prove the maximum of two correct digits after the comma. If we look for example to the sqare root time of $1000$ , $961$ is the closer perfect square which verifies the condition. Then we have $\sqrt{1000} \approx 31 + \frac{39}{2*31} = 31.62903...$ and with a calculator we have $\sqrt{1000} = 31.6227766017...$ which is quite good. Any hints would be appreciate, thank you in advance.","['radicals', 'calculus', 'approximation']"
1837188,Example of non-noetherian ring whose spectrum is noetherian and infinite,"A topological space is noetherian if it satisfies the descending chain condition for its closed subsets. Let be $R$ a commutative ring and let $\mathrm{Spec}(R)$ its spectrum with Zariski topology. I already know some examples of non-noetherian rings whose spectrum is noetherian, but in all these cases the spectrum is noetherian as it is finite. Can someone give me an example of a non-noetherian ring whose spectrum is noetherian and with infinite points ? Thanks!","['zariski-topology', 'noetherian', 'algebraic-geometry', 'commutative-algebra']"
1837213,Fallacy limit problem - Where is the mistake?,"This problem comes from our text book. Evaluate 
$$
\lim\limits_{x \to 0}\frac{2^x-1-x}{x^2}
$$
without using either L'Hopital's rule or Taylor series. The picture below shows the solution given by the text book. Many of the people who responded to this question say that the limit diverges. But according to the solution (see the attached image) given by the text book the limit is  $ \frac{(\ln2)^2}{2} $. Is there an error in the solution given in the attached image? Note : This problem ( and the solution) comes from Cengage, a long time favourite of the students preparing for IIT JEE  Exam (IITs are India's most prestigious engineering institutes.)  It is suprising that no student/teacher has found this mistake all these days. However, once posted on Stackexchange, the fallacy was resolved in a very short time. Thanks to all the people who responded to this problem. Now the mistake in the book's solution is found.","['limits-without-lhopital', 'calculus', 'limits']"
1837255,What is the meaning of $\Bbb{Q}[x]/f(x)$?,"I am very confused with the meaning of $\Bbb{Q}(x)/f(x)$. Does it mean the set of all polynomials modulo $f(x)$?
If it does then how can we say that $\Bbb{R}[x]/(x^2+1)$ is isomorphic to set of complex numbers?","['number-theory', 'abstract-algebra', 'finite-fields', 'quotient-spaces']"
1837259,Functions invariant under scaling,Which functions are invariant under the transformation $$f(x)=af(bx)$$ for constants $a$ and $b$? Are functions of the form $cx^n$ and $de^x$ the only analytic ones (as in having a power series expansion) that satisfy this? What if the additional requirement $a=b$ was imposed? What about pathological functions? Are there an infinite number of pathological functions (disregarding a multiplicative constant) that satisfy this?,['functions']
1837273,Stone-Weierstrass theorem of $\mathbb{S}^2$,Someone told me that every continuous function on $\mathbb{S}^2$ could be expressed as a uniform limit of restrictions to $\mathbb{S}^2$ of polynomials. Does this result come from the Stone-Weierstrass theorem? Could anyone be able to explain to me what it means formally? Thanks for your help!,['functional-analysis']
1837290,$\mathbb{P}(B) = 1 \implies \mathbb{P}(A \mid B) = \mathbb{P}(A)$,"Suppose I have two events $A$, $B$ from the same sample space with $\mathbb{P}(B) = 1$ and $\mathbb{P}(A) > 0$. How can I show that $$\mathbb{P}(B) = 1 \implies \mathbb{P}(A \mid B) = \mathbb{P}(A)\text{?}$$ The definition of conditional probability doesn't help me, as it isn't clear what can be done with $\mathbb{P}(A \cap B)$. If I use Bayes' Theorem, I get $\mathbb{P}(B \mid A)\mathbb{P}(A)$, so the only way this would work is that $\mathbb{P}(B \mid A) = 1$. This makes intuitive sense, but I can't prove that $\mathbb{P}(B \mid A) = 1$. This should be a trivial question, but I'm not seeing how to do it. HINTS , not full solutions, are appreciated.",['probability']
1837327,How do I find a closed form of ${\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx$?,"How do I evaluate the closed form for $g(n)$? Where n is an integer, $n\ge 1$ $${\pi^{2n}\over \zeta(2n)}\int_{-1}^{1}{x^{2n-2}\over \pi^2+(2\tanh^{-1}{x})^2}dx=g(n)$$ Make  a subsititution $u=\tanh^{-1}{x}\rightarrow dx=sech^2{u}du$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2+4u^2}du=g(n)$$ $${\pi^{2n}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over \pi^2[1+\left({2u\over \pi}\right)^2]}du=g(n)$$ $${\pi^{2n-2}\over \zeta(2n)}\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{\tanh^{2n}{u}\over 1+\left({2u\over \pi}\right)^2}du=g(n)$$ Apply geometric series ${1\over 1+x}=\sum_{k=0}^{\infty}(-1)^kx^k$ $${\pi^{2n-2}\over \zeta(2n)}\cdot{\left({2\over \pi}\right)^{2k}}\sum_{k=0}^{\infty}(-1)^k\int_{-\infty}^{\infty}{1\over sinh^2{u}}\cdot{u^{2k}\tanh^{2n}{u}}du=g(n)$$ I am shrugged here how to evaluate this integral at this point, I would some help please. The first few values for $n=1,2,3,4,...$ are $1,4,22,{428\over 3},...$ From the look at its trend, $g(n)$ only seem to  yield rational values? As for the odd powers, we get zero as a result.[checked through wolfram integrator] $$\int_{-1}^{1}{x^{2n-1}\over \pi^2+(2\tanh^{-1}{x})^2}dx=0$$","['integration', 'definite-integrals', 'sequences-and-series', 'calculus']"
1837333,function is not differentiable on $\mathbb R\setminus\{0\}$,"I need to prove that the given function $f$ is not differentiable on $\mathbb R \setminus\{0\}$. $$
f(x) =  \begin{cases} x^2, \ x \in \mathbb{Q}\\
0,  \ x \in \mathbb{R}-\mathbb{Q}
\end{cases}
$$
Can anyone tell me for $c\in\mathbb R \setminus \{0\}$ my work is correct or not and also if you have any different way to this question comment them please thanks Suppose $c\in\mathbb R \setminus \{0\}$ be arbitrary CASE 1  $c\in\mathbb R \setminus \mathbb Q$.
Let $\varepsilon = c^2>0$ Let $δ>0$ be arbitrary Suppose $|x-c|<δ$ If $c > 0$ choose $x'\in(c,c+\delta) \cap \mathbb R \setminus\mathbb Q$. If $c < 0$ choose $x'\in(c-\delta,c) \cap \mathbb R \setminus\mathbb Q$. Since $|f(x')-f(c)|=|x'^2-0^2 |=|x'^2-0^2 |>c^2=ε$.
$f$ is not continuous at $0$ hence $f$ is not differentiable at $\mathbb R \setminus\mathbb Q$.","['derivatives', 'real-analysis', 'calculus']"
1837337,Is there such a thing as a matrix of functions?,"Do we ever put functions as entries of a matrix? If so, are these matrices used in linear algebra or do they have some other special use? There have been minor not neccessarily conflicts per se, but disagreements on the nature of this question and so I am adding a little statement below to clear this up. I have noticed that ""function"" has been interpreted two ways within the answers. An actual raw function such as merely writing ""f"". Such a concept is beyond my current understanding (unless I am being stupid somehow), but it is interesting nonetheless. A function call returning a value. This is primarily what I meant in my post. Either one of these is valid. In fact, I think the broadness of this question dictates the fact that people will interpret it differently. In essence, your mileage will vary, and both are so similar from my standpoint that they are all good answers (or good examples if not standalone answers).","['matrices', 'terminology']"
1837339,Existence of an inverse relation for $R \subseteq A \times A$.,"I'm stuck with the following problem: Given the set $A = \{1,2,3,4,5\}$, construct a relation $R \subseteq A \times A$ such $$ R \circ R^{-1} = \triangle_A = \{(a,a) \hspace{5pt} | \hspace{5pt} a \in A\}$$ and $$R^{-1} \circ R = \{(a,a)\};$$ that is, the set that consists of all the ordered pairs in $A \times A$ where both elements are equal, and a set that consists of only one point. Now, here's where I've got so far: Let $R = \{(1,1), (1,2) , (1,3) , (1,4) , (1,5) \}$, so that $R^{-1} = \{(1,1), (2,1) , (3,1) , (4,1) , (5,1) \}$. Then, $$R^{-1} \circ R = \{ (1,1)\} $$ But, as you can see, taking $R \circ R^{-1}$, gives back all of $R$. I've taken similar choices of $R$ and none of them work. Now, for example, for the set that contains all ordered pairs with equal entries let $R = \{ (1,2),(2,3),(3,4),(4,5), (5,1) \}$, then $ R^{-1} = \{ (2,1) , (3,2), (4,3) , (5,4) , (5,1)\}$. Taking the composition from either side yields $\triangle_A$. My guess is that there is no such $R$: in order to get $\triangle_A$, our relation must be in a way that $R \circ R^{-1} = R^{-1} \circ R$, which would imply that the other condition cannot be met. So, provided I'm not wrong, lets say $X= \{ R \subseteq A \times A \hspace{5pt} | \hspace{5pt} R \circ R^{-1} = \triangle_A\}$, that is, the set of all appropriate choices of $R$ such that $R \circ R^{-1} = \triangle_A$, and in a similar way, let $Y = \{ R \subseteq A \times A \hspace{5pt} | \hspace{5pt} R \circ R^{-1} = \{(a,a)\} \hspace{5pt} \text{for some} \hspace{5pt} a \in A\}$. How would I prove that $X \cap Y = \varnothing$? Edit If we arrange all the members of $A \times A$ in the following matrix/array, we can make the following conclusions: $$ \left( \begin{array}{[ccccc}
(1,1) &(1,2)&(1,3)&(1,4)&(1,5)\\
(2,1)&(2,2)&(2,3)&(2,4)&(2,5)\\
(3,1)&(3,2)&(3,3)&(3,4)&(3,5)\\
(4,1)&(4,2)&(4,3)&(4,4)&(4,5)\\
(5,1)&(5,2)&(5,3)&(5,4)&(5,5)\\
\end{array} \right) $$ Let $R_{(n,n)} = \{ (n,n)\}$. Define $R_{(n,a_i)} = \{ (n,a_i) \hspace{5pt} | \hspace{5pt} a_i \in A \hspace{5pt} \text{for} \hspace{5pt} i = 1, ... ,5 \}$, this is represented as the $n$-th row in the matrix. Similarly, $R_{(a_i,n)} = (R_{(n,a_i)})^{-1}$, is the $n$-th column in the matrix. Now, if we want to choose a relation on $A$ such that $ R \circ R^{-1} = \triangle_A $ contains all the ordered pairs with equal entries in $A \times A$, and $R_{(n,n)}$ is possible, then we have only five options, namely, the five rows of the matrix. Since none of those rows is an appropriate choice of $R$, then we can conclude that there does not exist any relation on $A$ that has the properties requested. Does this count as a proof or does it lack something?","['relations', 'elementary-set-theory']"
1837353,Is it possible to convert the polar equation $\ r = k \cos (\theta n) + 2$ into cartesian form?,"Is it possible to convert the polaer equation 
  $$\ r =  k \cos (\theta n) + 2$$ 
  into cartesian form? Here, $k$ is some constant and $n$ is any positive whole number greater than $2$. The farthest that I managed to get was: $$r^2 = kr \cos(\theta n) +2r$$ $$\to\qquad r^2 - 2r - kr\cos(\theta n) = 0$$ $$\to \qquad x^2 +y^2 - 2 \sqrt{x^2 +y^2} -kr \cos (\theta n) = 0$$ I noticed that $\cos(\theta n)$ is reminiscent of the Chebychev polynomials and figured that $r\cos(\theta n)$ could also be generalized into polyomials in terms of $x$, but that turns out to be impossible.","['algebra-precalculus', 'polynomials', 'trigonometry', 'polar-coordinates']"
1837379,Tips for integrating $\int \frac{dx}{1+\cos(x)}$,"I tried the following
$$ 
\int \frac{dx}{1+\cos(x)}=\int \frac{1-\cos(x)}{1-\cos^2(x)}\,dx=
\int \frac{1-\cos(x)}{\sin^2(x)}\,dx\\
=\int \frac{1}{\sin^2(x)}\,dx-\int \frac{\cos(x)}{\sin^2(x)}\,dx=\int \csc^2x\,dx-\int \cot(x)\csc(x)\,dx
$$
Which I looked up and found to be equal to 
$$
\csc(x)-\cot(x)+c
$$ Do I need to memorize the final identity, or is there a more elegant way to evaluate this integral?","['integration', 'trigonometry', 'trigonometric-integrals', 'calculus']"
1837410,"Inverse trigonometric function identity doubt: $\tan^{-1}x+\tan^{-1}y =-\pi+\tan^{-1}\left(\frac{x+y}{1-xy}\right)$, when $x<0$, $y<0$, and $xy>1$","According to my book $$\tan^{-1}x+\tan^{-1}y =-\pi+\tan^{-1}\left(\frac{x+y}{1-xy}\right)$$
  when $x<0$, $y<0$, and $xy>1$. I can't understand one thing out here that when the above mentioned conditions on $x$ and $y$ are followed then the denominator of the argument of $\tan^{-1}(1-xy)$ become negative while the numerator too becomes negative and $x$ and $y$ both are less than zero. Now as both the numerator and denominator are negative the arguments i.e  $\left(\frac{x+y}{1-xy}\right)$becomes positive overall. Now why do we add $\pi$ to the expression when we are already having a positive argument which can be found in the first quadrant which is found in principal range. Now is it because we can also find the postive tangent function in third quadrant also? If this is so, why has this been mentioned up as a separate identity rather than another solution?",['trigonometry']
1837421,Divergence of Material Derivative,"Let $u : \Bbb{R}^n\times \Bbb{R}  \to \Bbb{R}^n\times \Bbb{R} $ be a divergence free vector field. Then the material derivative $D $  is given by: $$ \frac { \partial u_j}{\partial t}+\sum_{i=1}^{n} u_i \frac { \partial u_j}{\partial x_i} $$ Taking the divergence of $Du_j $ one obtains : $$\sum_{j=1}^{n}\sum_{i=1}^{n} \frac {\partial}{\partial x_j}u_i \frac { \partial u_j}{\partial x_i}=\sum_{j=1}^{n}\sum_{i=1}^{n}\left ( \frac {\partial u_i}{\partial x_j} \frac { \partial u_j}{\partial x_i}+u_i \frac { \partial^2 u_j}{\partial x_i \partial x_j}\right)$$ and rearranging terms, $$=\left(\sum_{j=1}^{n} \frac {\partial u_j}{\partial x_j}\right)^2 -\sum_{i\neq j} \left (\frac {\partial u_i}{\partial x_i}\frac {\partial u_j}{\partial x_j}-\frac {\partial u_i}{\partial x_j}\frac {\partial u_j}{\partial x_i}\right)+ \sum_{j=1}^{n}u_j \frac {\partial}{\partial x_j}\sum_{i=1}^{n}\frac {\partial u_i}{\partial x_i}$$ from which it follows that for divergence free vector fields $$div (Du_j)=-\sum_{i\neq j} \left (\frac {\partial u_i}{\partial x_i}\frac {\partial u_j}{\partial x_j}-\frac {\partial u_i}{\partial x_j}\frac {\partial u_j}{\partial x_i}\right)$$ which corresponds to a sum of Jacobian determinants. Is this correct?","['multivariable-calculus', 'partial-derivative', 'differential-operators', 'fluid-dynamics']"
1837436,Cardinality of subsets with finite intersections,"Let $\ F_0 $ be a family of disjoint subsets of $ C$. $\ |C|= \aleph_0$. Prove that $\ (*)   |F_0|\leq\aleph_0 $. This part was relatively simple, in the presence of choice an injection can be defined from each set to one of its elements in C. By Cantor the conclusion is simple. I'd like to show that $\ |F_1|\leq\aleph_0$ where $\ F_1 $ is a family of subsets of $C$ in which for any two distinct $\ A,B\in F_1$, $A\cap B \leq 1 $. From here I'd like to inductivly prove $\ |F_n|\leq\aleph_0$ where $\ F_n  $ is a family of subsets of $C$ in which for any two distinct $\ A,B\in F_n$, $A\cap B \leq n $. I'd like to isolate the intersections and show each family as a disjoint family and use $ (*) $ from above. My problem is properly defining an injection.","['cardinals', 'elementary-set-theory']"
1837463,Bell numbers and the Moments of expected number of fixed points,"Let $X_N$ be the random variable corresponding to the number of fixed points (1-cycles) in a permutation chosen uniformly at random from $S_N$ . Then, the $m^{\text{th}}$ moment, when $m < N$ , is equal to $B_m$ where $B_k$ is the $k^{\text{th}}$ Bell number (equal to the number of partitions of the set $[k]$ ). This is a really interesting combinatorial fact, not least because all the moments are integers (moments past the $N^{\text{th}}$ are as well) and deserves a combinatorial solution. Now, I have two proofs that are fairly combinatorial - however, both involve sums and avoid the combinatorial meaning of $X_{N}^m$ , whatever that may be. I reproduce the first below: Let $X_N$ be the number of fixed points in a random permutation on $N$ elements. It is well known that $E[X_N]=1$ . For $0\leq M \leq N$ , $E\left[{X_N \choose M}\right]$ counts the expected number of sets of $M$ fixed points. We can count this using indicator random variables. Consider all subsets of $[N]$ containing $M$ elements. The probability that a subset of size $M$ is pointwise fixed by a random permutation $\pi$ is $\frac{(N-M)!}{N!}$ because there are $(N-M)!$ permutations on the remaining elements that keep the subset of $M$ elemnts fixed (the number of automorphisms on $[N]-[M]$ ). This factor can also come from realizing that a permutation on $[N]$ induces an injection from a subset of $M$ elements to $[N]$ of which only $1$ sends each element to itself. Because the injections are uniformly chosen, we have a factor of $\frac{1}{N^{\underline{M}}} = \frac{(N-M)!}{N!}$ . Hence the expected value is ${N \choose M} \times \frac{(N-M)!}{N!} = \frac{1}{M!}$ .
Hence the expected value of $E[(X_N)^{\underline{M}}]$ is $1$ . Hence the expected value of $E[(X_N)^M]$ is $B_M$ , the $M^{th}$ Bell Number for $0 \leq M \leq N$ . However, this involves at the end an ugly summation and (in my mind) fails to capture why the expectation ought to be an integer outside of coincidence. It is also the case that as $N \rightarrow \infty$ , $X_N \rightarrow \text{Poi}(1)$ where Poi( $\lambda$ ) is the Poisson distribution with mean $\lambda$ . Hence, it may be related to properties of the Poisson random distribution when the mean is $1$ . I believe that the number of $k$ -cycles is in generally asymptotically Poi( $1/k$ ) but I don't remember where I read this and have not proven it myself - it would be interesting to see if they too had integer moments though I'd doubt it. I'm really just looking for combinatorial intuition on this - a proof that doesn't involve summation or less intuitive things would be nice (summation in the context of a union is fine, but the random M! in the above summation leaves me unhappy). Thanks for your help!","['permutations', 'combinatorics', 'poisson-distribution', 'probability-distributions']"
1837466,Is there a name for matrices that are symmetric along the cross diagonal? [duplicate],"This question already has an answer here : Name of a special matrix (1 answer) Closed 8 years ago . Something like $$
A=
\begin{bmatrix}
    a & b & c\\
    b & d & e\\
    c & e & f
\end{bmatrix}
$$ would be a symmetric matrix because the values are reflected along the diagonal, and $A=A^\intercal$ Is there a name for a matrix that's symmetric along the cross diagonal? Something like $$
B=
\begin{bmatrix}
    c & b & a\\
    e & d & b\\
    f & e & c
\end{bmatrix}
$$","['matrices', 'symmetric-matrices', 'terminology']"
1837482,Show that every group of order $35$ is abelian. [duplicate],"This question already has an answer here : Let $G$ be a group of order 35. Show that $G \cong Z_{35}$ [duplicate] (1 answer) Closed 7 years ago . How can I show that every group of order $35$ is abelian ? I know that if such a group is abelian the it's isomorphic to $\mathbb Z_{35}$ or $\mathbb Z_7\times \mathbb Z_5$. But, how can I show that any group of order 35 is either isomorphic to $\mathbb Z_{35}$ or to $\mathbb Z_7\times \mathbb Z_5$ ?",['group-theory']
1837494,What kind of $n^{th}$ order polynomials are solvable by a square matrix with integer entries?,"Consider a polynomial (monic for simplicity): $$x^n+a_1x^{n-1}+\dots+a_{n-1}x+a_n=0$$ Here we assume the roots are complex numbers. $a_k$ are integers. Now consider the corresponding matrix polynomial: $$X^n+a_1X^{n-1}+\dots+a_{n-1}X+a_nI_m=0$$ Here $X$ is an $m \times m$ matrix, and $I_m$ - $m \times m$ identity matrix. $a_k$ are still integers. What kind of polynomials can be solved in matrices with integer entries? Obviously, any $X$ should satisfy at least its characteristic polynomial equation by Cayley–Hamilton theorem, as well as its minimal polynomial. But I don't know if every polynomial with integer coefficients is also a characteristic or minimal polynomial of some matrix with integer entries.","['matrices', 'abstract-algebra', 'polynomials', 'linear-algebra']"
1837503,Is quotient of open invariant subset open?,"I am reading GIT book by Mumford. He needs special cases of the following conjecture several times. Conjecture Let $G$ be a reductive algebraic group acting on an irreducible affine scheme $X=Spec A$. Let $Y = Spec A^G$ and $\phi: X \rightarrow Y$ is canonical map. Let $U$ be an open invariant subset in $X$. Then $\phi (U)$ is open. Question Is it true? Remark 1. It is a theorem from Mumford’s book that $\phi$ is a categorical quotient . Remark 2 As far as Mumford do not formulate the conjecture as a proposition in his book, I expect it to be false. Edited My question is closely related to this one . user31480 has asked about invariant subsets, I have asked about any. I have just realized, that it is not essential at all. For any open subset $U$ one can consider union of $gU$ for all $g \in G$ (still open, has the same image by quotient map, but invariant). Also I noticed that assumption of $X$ being irreducible is crucial. You can find a counterexample as an answer for aforementioned question.","['algebraic-groups', 'algebraic-geometry', 'commutative-algebra']"
1837532,Understanding notation for the sequence definition,"Looking for assistance in translating this definition into more laymen terms?  In other words, can someone explain it to me like I'm a 5 year old? Definition . A sequence ($s_n$) is said to diverge to $+\infty$ and we write $\lim (s_n) = +\infty$ provided that
   for every $M$ in $\mathbb R$ there exists a number $N$ such that $n > N$ implies that $s_n > M$.
  Similarly, $(s_n)$ is said to diverge to $-\infty$ and we write $\lim (s_n) = -\infty$, provided that
   for every $M$ in $\mathbb R$ there exists a number $N$ such that $n > N$ implies that $s_n < M$. I start to get confused with the variables as I don't know what some of them mean.  For example, why are some capitalized and other not?  I know that $\mathbb R$ represents the real number line, and I'm comfortable with its notation; so it seems simple to me that $M$ is just some number on $\mathbb R$. As I progress through the definition, though, new variables are thrown out with no context (so it seems).  For example, it says ""$N$ such $n > N$"", what are these variables and why is one capitalized one not?  Why couldn't two completely different variables be used? Generally speaking, are there a set a rules to reference when one is reading math definitions (e.g. capital letters typically represent ___,etc) Any help is greatly appreciated.","['real-analysis', 'notation', 'elementary-set-theory', 'sequences-and-series', 'definition']"
1837559,Commonplace sets,"I recently started reading about sets of numbers, set builder notation, and operations on sets of numbers. To practice using different symbols (e.g., $\wedge$ ) and different set ""layouts,"" I decided to delineate some common categories/sets of numbers. Some of them are named with single symbols while others are not. In addition, some are defined using more than one set or a union of sets. However, most equivalent sets that can be achieved through basic algebraic manipulation (e.g., substituting $\sqrt{-k}$ for $k\sqrt{-1}$ ) have been excluded. Natural: $\Bbb{N}=\{ 1,2,3,4,\ldots\}$ Whole: $\Bbb{W}=\Bbb{N}\cup\{0\}=\{ 0,1,2,3,4,\ldots\}$ Integers: $\Bbb{Z}=\{ z\mid\left| z\right|\in\Bbb{W}\}=\{\ldots,-4,-3,-2,-1,0,1,2,3,4,\ldots\}$ Integers excluding zero: $\Bbb{Z_{0}}=\Bbb{Z}\setminus\{0\}$ Even: $\{n\mid n/2\in\Bbb{Z}\}=\{\ldots,-4,-2,0,2,4,\ldots\}$ Odd: $\{n\mid (n+1)/2\in\Bbb{Z}\}=\{\ldots,-3,-1,1,3,\ldots\}$ Prime: $\{p\mid p/k\notin\Bbb{Z}\wedge p\in\Bbb{Z}\wedge k\neq 0\wedge k\neq 1\wedge k\neq p\}=\{2,3,5,7,11,\ldots\}$ Composite: $\{p\mid p=hk\wedge h\in\Bbb{Z}\setminus\{p\}\wedge k\in\Bbb{Z}\setminus\{p\}\}$ Units: $\{u\mid 1/u\in\Bbb{W}\}=\{1\}$ Zero divisors: $\{d\mid dh=0\wedge h\neq 0\}=\{0\}$ Rational: $\Bbb{Q}=\{ q\mid q=a/b\wedge a\in\Bbb{Z}\wedge b\in\Bbb{Z_{0}}\}=\{a/b\mid a\in\Bbb{Z}\wedge b\in\Bbb{Z_{0}}\}$ Irrational: $\Bbb{R}\setminus\Bbb{Q}$ Real: $\Bbb{R}=\{r\mid -\infty <r<\infty\}=\{r\mid r\in (-\infty,\infty)\}$ Real excluding zero: $\Bbb{R}_{0}=\Bbb{R}\setminus\{0\}$ Non-real: $\Bbb{I}\cup\Bbb{C}$ Imaginary: $\Bbb{I}=\{j\mid j^{2}<0\}=\{j\mid j=k\sqrt{-1}\wedge k\in\Bbb{R_0}\}=\{k\sqrt{-1}\mid k\in\Bbb{R_0}\}$ Complex: $\Bbb{C}=\{c\mid c=a+b\sqrt{-1}\wedge a\in\Bbb{R_{0}}\wedge b\in\Bbb{R_{0}}\}=\{a+b\sqrt{-1}\mid a\in\Bbb{R_{0}}\wedge b\in\Bbb{R_{0}}\}=\{a+b\mid a\in\Bbb{R_0}\wedge b\in\Bbb{I}\}$ In the above image, the use of question marks indicates that the sub-categories are not mutually exclusive (e.g., zero is both whole and even). Unlike some of the other sets, $\Bbb{N_0}$ and $\Bbb{R_0}$ have been equated to a single symbol not because those symbols are conventional but rather because doing so made the delineation of the sets of rational, irrational, composite, imaginary, and complex numbers easier and clearer. Also, I was not completely sure if $\Bbb{R}=(-\infty,\infty)$ , and I was very unsure if it is correct to say $\Bbb{Z}=\pm\Bbb{W}$ . So, here's my question: Have a made any 'grammatical' or 'punctuational' errors? Did I use $\wedge$ too much? Other than taking away one set from the setof real numbers (e.g., real take away composite equals prime), are there any quick or easy ways of defining the sets that I missed? Sorry for the novice question; I just finished pre-calculus. Much thanks in advance! —C.T.",['elementary-set-theory']
1837560,How to prove: Orthogonal complement of kernel = Row space?,"I'm really confused when trying to prove the following: Suppose $T:\mathbb{R}^n \to \mathbb{R}^m$ is a linear transformation represented by the matrix $A$ whose rows are given by $\{z_1^T,...,z_m^T\}$. Denote by $\mathrm{Ker}(A)$ the kernel of the transformation. I am trying to prove the following. $$\mathrm{Ker}(A)^\perp = \mathrm{span}\{z_1^T,...,z_m^T\} \tag{1}$$ It should be easy to prove but I'm completely confused at the moment. Thanks in advance!!","['matrices', 'linear-algebra']"
1837575,"Real Analysis, 2.18 (Fatou's Lemma) Integration of Nonnegative functions","2.18 Fatou's Lemma - If $\{f_n\}$ is any sequence in $L^+$, then $$\int \left(\lim_{n\rightarrow \infty}\inf f_n\right) \leq \lim_{n\rightarrow \infty}\inf\int f_n$$ Attempted proof - We know that $$\int \left(\lim_{n\rightarrow \infty}\inf f_n\right) = \int \sup_{k\geq 1}\left(\inf_{n\geq k}f_n\right) = \int \lim_{k\rightarrow \infty}\inf_{n\geq k}f_n$$ Then by the Monotone Convergence theorem $$\int \lim_{k\rightarrow \infty}\inf_{n\geq k}f_n = \lim_{k\rightarrow \infty}\int \inf_{n\geq k}f_n$$ Note that the Monotone Convergence theorem can be applied because $$\inf_{n\geq k} f_n \leq \inf_{n\geq k+1} f_n$$ We see that $\inf_{n\geq k}f_n \leq f_k$ for all $n\geq k$. So,
\begin{align*}
\int \inf_{n\geq k}f_n &\leq \int f_k \ \forall n\geq k\\
&\leq\inf_{n\geq k}\int f_n\\
&\leq \lim_{k\rightarrow \infty}\inf_{n\geq k}\int f_n
\end{align*} I may have some indexing mistakes but I think this is a sufficient proof. Any suggestions is greatly appreciated.","['real-analysis', 'measure-theory', 'proof-verification']"
1837585,Prove that a function is contractive,"I'm stuck with the following. I need to prove that in $D:=[0,1]\times[0,1]$ the function $F$ is contractive, where $F:\mathbb{R}^2\rightarrow\mathbb{R}^2$ is defined as: \begin{align} F(x,y):=(\frac{1}{2} e^{-x}+\frac{y}{2},\frac{1}{3} e^{-y}+\frac{x}{3})  \end{align} Actually the problem is to prove: $\exists !_{(x^*,y^*)\in D}: F(x^*,y^*)=(x^*,y^*)$ using Banach fixed point theorem. First of all $D$ must be closed and $F(D)\subset D$, and both are true in this case. But I have to prove $F$ is contractive to use Banach fixed point theorem. I tried Mean Value theorem in both components $F_1(x,y)$ and $F_2(x,y)$ but that didn't help. I would love to see methods for proving contraction in general. Thanks!","['multivariable-calculus', 'fixed-point-theorems', 'real-analysis']"
1837586,Reductive homogeneous spaces,"If $G$ is a connected Lie group and $K$ is a closed subgroup of $G$ then $G/K$ is a homogeneous space. If $\frak g,k$ are the lie subalgebras of $G,K$ resp. Then under the projection $\pi:G\rightarrow G/K$ we get $\mathfrak g/\mathfrak k\cong T_o(G/K)$ . A homogeneous space is called reductive if there exists a subspace $\frak m$ of $\frak g$ such that $\frak g= k\oplus m$ and $Ad(k)\frak m\subset m$ for all $k\in K$ . That is, $\frak  m$ is $Ad(K)$ -invariant. My question is: How would the condition $Ad(k)\frak m\subset m$ imply that $\frak [k,m]\subset m$ ? And why the converse is true in case $K$ is connected? My Attempt: Let $X\in \frak k$ and $Y\in \frak m$ . Assume $[X,Y]\in \frak k$ , then $exp\ t[X,Y]\subset K$ . Hence, $exp\ t(ad_XY)=exp\ t(\frac d {ds}\{Ad(exp\ sX)Y\}|_{s=0})$ . Since $Ad(exp\ sX)Y\in \frak m$ for all $s\in \mathbb R$ then, $\frac d {ds}\{Ad(exp\ sX)Y\}|_{s=0}\in \frak m$ since it is a subspace. Therefore, $exp\ t(ad_XY)=exp\ t Y'\subset K$ for some $Y'\in \frak m$ . But this is a contradiction since there is a one-to-one correspondence between the elements in $\frak k$ and the one parameter subgroups in $K$ . Is my proof okay? Any comments would be appreciated!","['homogeneous-spaces', 'differential-geometry', 'lie-groups']"
1837588,How can I compute a presentation of the tangent bundle for a smooth manifold defined by a family of polynomials?,"Consider a smooth manifold $M$ given by a system of smooth functions
$$
\begin{align*}
f_1 = 0 \\
\cdots \\
f_k = 0
\end{align*}
$$
in $n$ variables. This has the algebraic description as the $\mathbb{R}$-algebra
$$
C^\infty(M) = \frac{C^\infty(\mathbb{R}^n)}{(f_1,\ldots,f_k)}
$$
How can I compute the tangent and cotangent bundle in terms of a similar presentation of algebras? Here is a running example from the answers: Consider the circle
$$
C^\infty(S^1) = \frac{C^\infty(\mathbb{R}^2)}{x^2 + y^2 - 1}
$$
Then, the normal vector fields to the manifold should be spanned by the vectors
$$
y\frac{\partial}{\partial x} - x \frac{\partial}{\partial y}
$$
since
$$
\left(y\frac{\partial}{\partial x} - x \frac{\partial}{\partial y}\right)(x^2 + y^2 - 1) = 2xy - 2xy = 0
$$
But, this confuses me since the tangent vector evaluated at (1,0) should be
$$
-\frac{\partial}{\partial y}|_{(1,0)}
$$
which looks like a tangent vector if you draw the picture. For reference, this can be done by looking at the embedding of $M$ inside $\mathbb{R}^n$, restricting the euclidean tangent bundle to $M$ and cutting out the vector fields which vanish on $M$. This can be explicitly presented as the vanishing of $f_1,\ldots, f_k$ and $\nabla f_1, \ldots, \nabla f_k$ where
$$
\nabla f_i = \frac{\partial f_i}{\partial x_1} \partial_{x_1} + \cdots + \frac{\partial f_i}{\partial x_n} \partial_{x_n}
$$
where the $\partial_{x_j}$'s are the vertical coordinates of $T\mathbb{R}^n \to \mathbb{R}^n$","['real-algebraic-geometry', 'differential-geometry', 'algebraic-geometry']"
1837595,Rewriting $\mathscr P(\bigcup_{i \in I} A_i)\not\subset\bigcup_{i \in I} \mathscr P(A_i)$ in more fundamental terms.,"Working through Velleman's ""How to Prove It"" and currently having a bit of difficulty with a problem where I'm asked to rewrite this: $$\mathscr P\left(\bigcup_{i\in I} A_i\right)\not\subset\bigcup_{i \in  I} \mathscr P(A_i)$$ . . . using only . . . $$\land \lor \exists \forall \in \notin \to \iff = \neq$$ Here's what I've come up with so far: $$ \mathscr P\left( \bigcup_{i\in I} A_i \right) \not\subset \bigcup_{i\in I} \mathscr P(A_i) $$ The first element I look at here is the 'not a subset' element. $$
\exists x\,\left( x\in \mathscr P\left( \bigcup_{i\in I} A_i \right) \wedge x\notin \bigcup_{i\in I} \mathscr P(A_i) \right)
$$ I know being a member of the power set of something is basically being a subset of that thing. $$\exists x \left( \left(\forall y(y \in x \to \exists i \in I(y \in A_i)\right) \land x \notin \bigcup_{i\in I} \mathscr P(A_i)\right)$$ This next part is where I feel like I'm making some kind of mistake. I feel like I've got something wrong about the union of a power set here. $$\exists x(\forall y(y \in x \to \exists i \in I(y \in A_i)) \land \lnot \exists i \in I(\forall y(y \in x \to y \in A_i)))$$ If $x$ is the member of a union of some indexed family of sets, then there exists some some index $i$ in the index set $I$ such that $x$ is a member of the set $A_i$. So if the set it's a member of is the $\mathscr P(A_i)$ . . . I just substitute the definition of membership in a power set in where I'd otherwise state the set? I feel like I'm making a really silly mistake in my reasoning, but I'm not sure what.","['predicate-logic', 'logic', 'elementary-set-theory', 'quantifiers']"
1837613,Prove that $\text{Dom } (S\circ R) ⊆ \text{Dom }R $,"Let $R$ be a relation from $A$ to $B$ and $S$ be a relation from $B$ to $C$. Suppose, $x \in \text{Dom }(S\circ R)$. Then, it follows that there $\exists y \in C$ such that $(x,y) \in S\circ R $. Since $S\circ R$ is a subset of $A\times C$ and by the definition of $\text{Dom }(S\circ R)$. Thus there $\exists y \in C$ such that $(\exists r \in B)[(x,r)\in R$ and $(r,y)\in S ]$. Therefore, it follows that $x\in \text{Dom }R$, since the statement $(\exists r \in B)(x,r)\in R$ satisfies the requirement for membership in $\text{Dom }R$. Is this proof correct? Thank you!","['elementary-set-theory', 'proof-verification']"
1837628,Roots of $y=x^3+x^2-6x-7$,"I'm wondering if there is a mathematical way of finding the roots of $y=x^3+x^2-6x-7$? Supposedly, the roots are $2\cos\left(\frac {4\pi}{19}\right)+2\cos\left(\frac {6\pi}{19}\right)+2\cos\left(\frac {10\pi}{19}\right)$, $2\cos\left(\frac {2\pi}{19}\right)+2\cos\left(\frac {14\pi}{19}\right)+2\cos\left(\frac {16\pi}{19}\right)$ and $2\cos\left(\frac {8\pi}{19}\right)+2\cos\left(\frac {12\pi}{19}\right)+2\cos\left(\frac {18\pi}{19}\right)$. Anything helps. I don't think substituting $x$ with something like $t+t^{-1}$ will work.","['polynomials', 'roots', 'trigonometry']"
1837665,Is the equality $1^2+\cdots + 24^2 = 70^2$ just a coincidence?,"I have read a question (written in Korean. Added on September 9, 2020: the link is broken, unfortunately. ) that the equality $$1^2+2^2+\cdots + 24^2 = 70^2$$ is just a coincidence or not. It is related to the integral points of the following elliptic curve (?): $$y^2 = \frac{1}{3}x^3 + \frac{1}{2}x^2+\frac{1}{6}x.$$ (To note, its determinant is $1/1296$ .) I have heard that finding the integral points of an elliptic curve is very hard in general, so my first goal is to find rational solutions to it. However, I have not learned of the number theory so I have no idea how to find rational points of that. Is there any idea to solve this? Furthermore, is there an idea to find integral points of it? I would be appreciated for your help!","['number-theory', 'elliptic-curves']"
1837681,"Examples of (not) uniformly continuous, non-differentiable, non-periodic functions","Let $I\subseteq\mathbb{R}$ and $f:I\to\mathbb{R}.$ $(0)$ If $f$ is discontinuous on $I$, then it is not uniformly continuous. $(1)$ Suppose $I$ is open and bounded. If $f$ is unbounded on $I$, then it is not uniformly continuous. Otherwise, let $I=(a,b)$. If we can extend $f$ to $[a,b]$ by continuity, i.e. define $f^*(x):=\begin{cases}\lim\limits_{x\to a^+} f(x) & x=a \\ f(x) & x\in(a,b)\\ \lim\limits_{x\to b^-}f(x) & x=b\end{cases},$ $f^*$ is continuous on its domain, which is compact, and thus uniformly continuous by Heine-Cantor's theorem. Hence, so it is on $(a,b)$. If, instead, at least one of the limits above does not exist, say as $x\to a^+$, then there exist real sequences $x_n, y_n\to a^+$ such that $\lvert f(x_n)-f(y_n)\rvert\not\to0,$ implying that $f$ is not uniformly continuous. $(2)$ If $I$ is closed and bounded, then it is compact: a trivial adaptation of $(1)$ applies. $(3)$ Suppose $I$ is open and unbounded, say $I=(a,\infty)$; in view of $(1)$, let $f(x)\underset{x\to a^+}\longrightarrow l\in\mathbb{R}.$ 
  If $f(x)\underset{x\to\infty}\longrightarrow L\in\mathbb{R}$ then $f$ is u.c., because, by Heine-Cantor, so is its continuous extension $\overline{f}$ to $[a,\infty]\subset\overline{\mathbb{R}}$ (similarly as in $(1)$). Otherwise, let $f$ be differentiable. If $\limsup\limits_{x\to\infty}\lvert f'(x)\rvert= \infty$ then $f$ is not u.c. by the mean value theorem, whereas if $f'$ is bounded, $f$ is lipschitzian and thus the same theorem implies u.c. Finally, assume $f'$ is unbounded on $(a,b]$ (doesn't matter if not defined on some $(\alpha,\beta]\subseteq(a,b]$) and defined and bounded elsewhere. $f$ is u.c. on $(a,b]$ because so is $f^*$ on $[a,b]$ , as well as on $(b,\infty)$, due to being lipschitzian there. By the continuity of $f$ at $b$, it suffices to make $x\in(a,b]$ 
  and $y\in(b,\infty)$ sufficiently close to $b$ in order to have them satisfy the condition of u.c. Therefore, we conclude $f$ is u.c. on $I$. $(4)$ If $I$ is closed and unbounded, a trivial adaptation of $(3)$ applies. So one may note that the only functions whose uniform continuity is really interesting to investigate, are the ones defined on an unbounded interval, being globally continuous, non-periodic and non-differentiable on an unbounded subinterval of their domain. Do we know such functions, both uniformly and not uniformly continuous?","['derivatives', 'real-analysis', 'uniform-continuity']"
1837707,$x$ is odd if and only if $3x+6$ is odd,Prove the following proposition. Let $x\in\Bbb Z$. Then $x$ is odd if and only if $3x+6$ is odd. I'm currently not seeing a way to transform $3x+6$ into the format of $2k+1$ in order to prove odd. This is my first time dealing with discrete mathematics and proofs and I'm trying to get a feel for it.,['discrete-mathematics']
1837772,Testing for symmetry about a curve/line,"In High School Algebra , after studying how to plot a graph of $f(x)$ (rather called $y$) against $x$ in Cartesian coordinates, we studied several tests to determine the symmetry of the plotted graphs about the $x$ and $y$ axes, and around the origin point . The tests seemed pretty simple and intuitive, you can Google them if you need to. However, when I  thought of investigating the symmetry of a certain function (say $f(x)$) about a given equation of a line or a curve (say $g(x)$), the solution did not seem to be that intuitive. The case seemed trivial and easy when $g$ was simply $x=0$ or $y=0$, but what about other cases when $g(x)$ is in fact a function of $x$? That is, How to investigate the symmetry of $f(x)$ around $g(x)$? Another question that crossed my mind when thinking of the first one was the case when  $f(x)$ is not simply a single function, as in discussing whether two functions (say $v(x)$ and $s(x)$) formed a mirror image of each other around a third function (say $g(x)$) or not. The case again seems simple enough when $v(x) = inverse(s(x))$ , and $g(x)=x$. Although I know no test for that, but it is always the case that a function and its inverse are symmetric around $g(x)=x$. And, again, I can't think of a possible way to check for the symmetry of two separate curves about a third curve , or even generate a mirror image of  a given curve about another given curve (you may consider that last sentence a third part of the question).","['algebra-precalculus', 'symmetry', 'curves', 'graphing-functions']"
1837775,Zero integral implies zero function almost everywhere [duplicate],"This question already has answers here : If $\int_0^x f \ dm$ is zero everywhere then $f$ is zero almost everywhere (6 answers) Closed 6 years ago . Assume $f$ is Riemann integrable and further assume that $\int_a^x f=0$ for all $x$. How would I go about showing that $f$ itself is $0$ almost everywhere? I am new to Lebesgue's measure theory so I am hoping for a somewhat elementary proof if possible? I know that almost everywhere means all except a set of measure zero. I was wondering if I could get a point to start on? We are told $f$ is Riemann integrable, so that means by Lebesgue's criterion for Riemann integration that there are at most countably infinitely many discontinuities. Thank you!","['real-analysis', 'lebesgue-measure', 'measure-theory']"
1837784,help verifying equation $\int_0^ x \frac{1}{1+t^n} dt$,"As a follow up to a previous posting addressing the integral of $1/ (t^n+1)$ for $n\in \Bbb{N}$ I found the following $$\int_0^ x \frac{1}{1+t^n}\, dt=\sum_{i=0}^{\infty}\frac{(i!)(n^i)x^{in+1}} {(x^n+1)^{i+1}\prod_{k=0}^i (kn+1)}$$ My son programmed the equation in CC++, and showed me that it works for $n=1,2$, 3... But as n gets large the computer cannot calculate. Many thanks to the management of this webside. I would like assistance verifying if this equation is correct or wrong. My request is also for others to demonstrate how the equation may be developed. The relevance of the equation is that it generates multiple series/functions. Think of it as a function generator n. E.g. At n= 1 the equation gives the series for $$LN(x+1)$$ At n= 2 the equation gives the series for $$ATAN(x)$$ The equation should be useful to other.","['sequences-and-series', 'calculus', 'indefinite-integrals', 'integration', 'power-series']"
1837814,"Show ""countable complement topology"" is a topology","I'm teaching my self topology with the aid of a book. I'm trying to prove the following is a topology: Let X be an infinite set. Show that $\mathscr{T}_2=\{U \subseteq X : U = \emptyset $ or $ X\setminus U $ is countable$ \} $ is a topology. My book calls this set the "" countable complement topology "" Please let me know if my proof is valid. corollary [A]: The union of countable sets are countable. proof: I'm going to take as a given that that the union of finite set are finite, and just prove the case of countable infinite sets . A set is countable infinite if there is a bijection between the set and the natural numbers. Let $A$ and $B$ be a countable infinite sets. Now let $f$ be the bijection between $A$ and $\mathbb{N}$ and $g$ be $B$'s bijection. $f:\mathbb{N}  \rightarrow  A$ $g:  \mathbb{N}  \rightarrow B$ There can always exist a bijection $h$ that maps $A \cup B $ to $\mathbb{N}$.
To account for non disjoint sets, let $B' = B \setminus A$; (note: $A \cup B = A \cup B'$) $h:  \mathbb{N}   \rightarrow  A \cup B'$ $
h(x) = \left\{
        \begin{array}{ll}
            f(\frac{1}{2}x); & \quad x \in 2\mathbb{N} \\
            g(\frac{x+1}{2}); & \quad x \in 2\mathbb{N}+1
        \end{array}
    \right.
$ $h^{-1}(x)$ would map $A$ to the even numbers, and $B'$ to the odd numbers.
 Therefore, The union of countable sets are countable. (QED) Now to verify the definition. (i) X and Ø are elements of $\mathscr{T}$. Ø is given in the definition of $\mathscr{T}_2$. Also, the empty set is considered finite, with cardinality zero. Thus $X$ is in $\mathscr{T}_1$ because its complement is finite, and finite numbers are countable. (ii) $\mathscr{T}$ is closed under finite intersections. Let:$V_k \subseteq X$ be any countable set. (ie: $V_1, V_2...$ are all countable sets) So Then, $U_k = X \setminus V_k$, would be the subsets of $\mathscr{T}_2$. $U_k \cap U_j = X \setminus (V_k \cup V_j) $, due to De Morgan's law. Thus, $U_k \cap U_j \in  \mathscr{T}_2$, since the Union of two countable sets is also countable[A]. And This reasoning can be extended to any finite amount of intersections. (iii) $\mathscr{T}$ is closed under arbitrary unions. Let $A=\bigcup_{i \in I} U_i$ be an arbitrary union of sets whose complements are are countable.Then by De Morgan's law, $A=X \setminus (\bigcap_{i \in I}V_i)$, for some countable sets $V_i$. By the definition of intersection, $\bigcap_{i \in I}V_i \subseteq V_i$ for all $i \in I$. Since $V_i$ is countable so is any of its subsets.  Hence the complement of $A$ is countable.","['general-topology', 'proof-verification']"
1837817,Are complex numbers complete in every way?,"I was told many times a story. Indeed a fascinating one to me as a student learning mathematics. First there were natural numbers. People started adding things and finding solutions to finding the unknowns when the results of the addition are known. The intriguing ""non-existent"" solutions to certain additive equations involving natural numbers, lead to finding negative numbers and zero. Complementing the set such that there is a solution to every problem of simple addition. Then came the extensive use of multiplication to ease the laborious addition operations. Leading to problems asking to find the unknowns when the results of multiplication are known. Extending the story, what lead to the discovery of rationals is to solve any equations involving simple multiplication. And what lead to the discovery of irrationals is the solutions to equations involving simple exponents, and even more. (such as?) Finally, the exciting polynomials gave birth to complex numbers in a way that every polynomial equation has all solutions within complex numbers. My question is simply this. Is it the end of the story? Can we expect anything more? Is there a set of numbers that is sufficient for every operation that we can imagine? Or, is it a never ending story?","['number-theory', 'analytic-number-theory', 'complex-numbers', 'elementary-number-theory']"
1837850,Galois group of a palindromic polynomial is not $S_n$?,"Let $f(x) = a_nx^n+\cdots+a_0 \in \mathbb{Q}[x]$ be a palindromic polynomial; that is, the coefficients of $f$ satisfy $a_n = a_0$, $a_{n-1} = a_1$, and more generally $a_{n-i} = a_i$ for all $0\leq i\leq n$. Is it true that if $n > 2$, then the Galois group of $f$ (i.e. the group $Gal(K/Q)$ where $K$ is a splitting field of $f$) is not $S_n$ ?","['abstract-algebra', 'galois-theory']"
1837865,Value of $\left(\frac{a}{b-c}+\frac{b}{c-a}+\frac{c}{a-b}\right)\cdot \left(\frac{b-c}{a}+\frac{c-a}{b}+\frac{a-b}{c}\right)$,"If $a+b+c=0,$ Then value of $\displaystyle \left(\frac{a}{b-c}+\frac{b}{c-a}+\frac{c}{a-b}\right)\cdot \left(\frac{b-c}{a}+\frac{c-a}{b}+\frac{a-b}{c}\right)$ $\bf{My\; Try::}$ Given $$\displaystyle \left(\frac{a}{b-c}+\frac{b}{c-a}+\frac{c}{a-b}\right)\cdot \left(\frac{b-c}{a}+\frac{c-a}{b}+\frac{a-b}{c}\right) = 1+1+1+\frac{b-c}{a}\left(\frac{b}{c-a}+\frac{c}{a-b}\right)+\frac{c-a}{b}\left(\frac{a}{b-c}+\frac{c}{a-b}\right)+\frac{a-b}{c}\left(\frac{a}{b-c}+\frac{b}{c-a}\right)$$ Now How can I Solve after that, Is there is any less complex method  plz explain here , Thanks",['algebra-precalculus']
1837891,Algebraic multiplicity = geometric multiplicity?,"I was wondering if algebraic multiplicity was equal to the geometric multiplicity. If the matrix (of size $n\times n$) is diagonalisable, i.e. the characteristic polynomial is of the form $$p(x)=(x-\lambda_1)^{m_1}\cdot ...\cdot (x-\lambda_k)^{m_k}$$
with $m_1+...+m_k=n$, I think that indeed algebraic multiplicity of $\lambda_i$ (i.e. $m_i$) and geometric multiplicity are the same. But is there cases where it doesn't hold ?",['linear-algebra']
1837906,Inequality : $\sum_{k=1}^n x_k\cdot \sum_{k=1}^n \frac{1}{x_k} \geq n^2$,"I have to show the inequality of $$\left(\sum_{i=1}^n x_i\right)*\left(\sum_{i=1}^n \frac{1}{x_i}\right) \geq n^2.$$For $x_1, ... x_n \in \mathbb{R_{>0}}$ and $ n \geq 1$. I wanted to show this inequality by induction. The basis is clear, but I am not sure how to do the inductive step. Can someone help me with that?","['induction', 'inequality', 'analysis']"
