question_id,title,body,tags
3576942,Prove that if $q\mid x$ and $q^k\parallel x^p$ then $p\mid k$.,"Suppose $p\neq q$ are distinct primes, and $x\in\mathbb{Z}$ . Suppose further, that $q\mid x$ , and that there is an integer $k\in\mathbb{Z}$ such that $q^{k}\mid x^{p}$ and $q^{k+1}\nmid x^{p}$ . Is it always true that $k$ is a multiple of $p$ ? Firstly, it is clear that $q^{p}\mid x^{p}$ , so that $p\le k$ . However from here I cannot conclude anything...","['number-theory', 'elementary-number-theory', 'prime-numbers']"
3576943,Prove that $\left\lfloor{\frac{n}{2}}\right\rfloor+\left\lfloor\frac{\left\lceil\frac{n}{2}\right\rceil}{2}\right\rfloor+\cdots=n-1$.,"Prove that, for $n\in \Bbb{Z}^+$ , $$\left\lfloor{\frac{n}{2}}\right\rfloor+\left\lfloor\frac{\left\lceil\frac{n}{2}\right\rceil}{2}\right\rfloor+\left\lfloor\frac{\left\lceil\frac{\left\lceil\frac{n}{2}\right\rceil}{2}\right\rceil}{2}\right\rfloor+\cdots = n - 1\,,$$ where there are $\lceil{\log_2n}\rceil$ addends on the left-hand side. I don't know how I could prove this. Any ideas? There is an intimate relationship here with a binary tree where each addend is the number of nodes on that layer, and $n$ is the number of leaves.","['binary', 'ceiling-and-floor-functions', 'trees', 'elementary-number-theory', 'discrete-mathematics']"
3576945,Definition of a Frenet curve,"Recently I followed a course in Differential Geometry. The book we use is Differential Geometry: Curves - Surfaces - Manifolds by Wolfgang KÃ¼hnel. My question is about the definition of a Frenet curve. 2.4. Definition (Frenet curve). Let $c(s)$ be a regular curve in $\mathbb{R}^n$ which is parametrized by arc length and $n$ -times continuously differentiable. Then $c$ is called a Frenet curve , if at every point the vectors $c',c'',\dots,c^{(n-1)}$ are linearly independent. The Frenet $n$ -frame $e_1,e_2,\dots,e_n$ is then uniquely determined by the following conditions: (i) $e_1,\dots,e_n$ are orthonormal and positively oriented. (ii) For every $k=1,\dots,n-1$ one has $\text{Span}(e_1,\dots,e_k)=\text{Span}(c',\dots,c^{(k)})$ . (iii) $\langle c^{(k)},e_k\rangle>0$ for $k=1,\dots,n-1$ . My thought: In this definition, I think (i) and (ii) are enough to uniquely determined $e_1,\dots,e_n$ . Indeed, one obtains $e_1,\dots,e_{n-1}$ from $c',\dots,c^{(n-1)}$ by means of the Gram-Schmidt orthogonalization procedure, and the missing $e_n$ is then uniquely determined by condition (i). Why condition (iii) is necessary? Any help would be appreciated.","['linear-algebra', 'differential-geometry']"
3576979,Is my proof of the Fibonacci sequence correct?,"Been working on this for some time now but have no idea if it's correct! Any hints are appreciated. Recall the Fibonacci sequence: $f_1 = 1$ , $f_2 = 1$ , and for $n \geq 1$ , $f_{n+2} = f_{n+1} + f_n$ . Prove
that $f_n > (\frac{5}{4})^n\  \forall \ n \geq 3$ . My answer: ""base case"" $[f_3 = 2 > (\frac{5}{4})^3\ = \frac{125}{64}\ correct$ $[f_4 = 3 > (\frac{5}{4})^4\ = \frac{625}{256}\ correct$ $assume\ f_k > (\frac{5}{4})^k\ for\ some\ k \geq 3$ $[and\ f_{k-1} > (\frac{5}{4})^{k-1}$ $then \ f_{k+1} = f_k + f_{k+1} > (\frac{5}{4})^k + (\frac{5}{4})^{k-1}$ $so \ f_{k+1} > (\frac{5}{4})^k + (\frac{5}{4})^{k-1}\ > (\frac{5}{4})^k (\frac{5}{4})^k = (\frac{5}{4})((\frac{5}{4})^k) = (\frac{5}{4})^{k+1}$ $f_{k+1} > (\frac{5}{4})^{k+1}$ $so \ f_n > (\frac{5}{4})^n \ \forall \ n \geq 3$ QED","['fibonacci-numbers', 'solution-verification', 'discrete-mathematics']"
3576992,"Proving $\int_0^1 \sqrt{x \left(\sqrt{-3 x^2+2 x+1}-x+1\right)} \, dx=\frac{7 \pi }{12 \sqrt{6}}$","I was given that $$\int_0^1 \sqrt{x \left(\sqrt{-3 x^2+2 x+1}-x+1\right)} \, dx=\frac{7 \pi }{12 \sqrt{6}}.$$ Which is numerically correct. I tried a few substitutions, none of which lead to anything valuable. I'd like you to help me on the problem. Thanks in advance!","['integration', 'definite-integrals']"
3577024,Proving $\int _{-\pi }^{\pi }\int _{-\pi }^{\pi }\int _{-\pi }^{\pi }\log \left| 1+e^{i x}+e^{i y}+e^{i z}\right| dxdydz=28 \pi \zeta (3)$,"It is known that: \begin{align}
&\int _{-\pi }^{\pi }\log\left(\,{\left\vert
\,{ 1 + \mathrm{e}^{\mathrm{i}x}}\,\right\vert}
\,\right)\,\mathrm{d}x = 0
\\[5mm] &\
\int _{-\pi }^{\pi }\int_{-\pi }^{\pi }
\log\left(\,{\left\vert\,{1 + \mathrm{e}^{\mathrm{i} x} + \mathrm{e}^{\mathrm{i}y}}
\,\right\vert}\,\right)
\,\mathrm{d}x\,\mathrm{d}y
\\[2mm] = &\
\frac{\pi\left[%
\psi ^{\left(1\right)}\left(1/3\right) -
\psi^{\left(1\right)}\left(2/3\right)\right]}{\,\sqrt{\,3\,}\,}
\end{align} which are direct consequences of Cauchy integral, Poisson integral formula $\,+\,$ Fourier expansion, respectively. However, I have no idea for the $3$ -dimensional case: \begin{align}
&\int_{-\pi}^{\pi}\int_{-\pi}^{\pi}
\int _{-\pi}^{\pi}
\log\left(\,{\left\vert\,{1 + \mathrm{e}^{\mathrm{i}x} + \mathrm{e}^{\mathrm{i}y} + \mathrm{e}^{\mathrm{i} z}}
\,\right\vert}\,\right)
\,\mathrm{d}x\,\mathrm{d} y\,\mathrm{d}z
\\[2mm] = &\
28\pi\,\zeta\left(\,{3}\,\right)
\end{align} Any kind of help is appreciated.","['integration', 'definite-integrals']"
3577075,"Bayes estimate for loss function $\ell(t,\alpha)=\frac{1}{\alpha^2}(t-\alpha^2)^2$?","I am given the following info for $\{X_i\}_{i=1}^{n}$ : $$X\sim f(X|\alpha)=\alpha X^{-(\alpha+1)}I(X>1).$$ Propose a convenient family of priors and find the Bayes estimate for the loss function $\ell(t,\alpha)=\frac{1}{\alpha^2}(t-\alpha^2)^2$ . The joint distribution is of the form $$f(\underline{X}|\alpha)=\alpha^n\prod_{i=1}^{n}X_i^{-(\alpha+1)}I(X_i>1)=\alpha^n e^{-(\alpha+1)\sum_{i=1}^{n}\log(X_i)}I(X_{(1)}>1).$$ Then, $\text{Gamma}(a,b)$ seem like good priors with a posterior $\text{Gamma}(a+n,\sum_{i = 1}^{n}\log(X_i)+b)$ . To find the posterior: $$P(\alpha|X)\propto P(X|\alpha)P(\alpha)\propto\alpha^n e^{-\alpha\sum_{i=1}^{n}\log(X_i)}e^{-b\alpha}\alpha^{a-1}$$ . Adding the like terms gives us the posterior.
To find the Bayes estimate we minimize with respect to $t$ , we consider the following Bayes risk function $$\int \ell(t,\alpha)f(\alpha|\underline{X})d\alpha=\int \frac{1}{\alpha^2}(t-\alpha^2)^2f(\alpha|\underline{X})d\alpha.$$ Since $\frac{\partial}{\partial t}\frac{1}{\alpha^2}(t-\alpha^2)^2=\frac{2t}{\alpha^2}-2$ and $\int (\frac{2t}{\alpha^2}-2)f(\alpha|\underline{X})d\alpha<\infty$ , we have $$\frac{\partial}{\partial t}\int \ell(t,\alpha)f(\alpha|\underline{X})d\alpha=2\int \left(\frac{t}{\alpha^2}-1\right)f(\alpha|\underline{X})d\alpha
= 2tE\bigg[\frac{1}{\alpha^2}\bigg|\underline{X}\bigg]-2.$$ Setting this equal to $0$ implies the Bayes estimate is $$\hat{t}=\frac{1}{E\bigg[\frac{1}{\alpha^2}\bigg|\underline{X}\bigg]}.$$ Define $R=\sum_{i = 1}^{n}\log(X_i)$ .
Then \begin{align}
E\bigg[\frac{1}{\alpha^2}\bigg|\underline{X}\bigg]
& =\int \alpha^{-2}\frac{(R+b)^{n+a}}{\Gamma(n+a)}\alpha^{n+a-1}e^{-\alpha(R+b)}d\alpha \\
& = \frac{(R+b)^{n+a}}{\Gamma(n+a)}\frac{\Gamma(n+a-2)}{(R+b)^{n+a-2}}\int \frac{(R+b)^{n+a-2}}{\Gamma(n+a-2)}\alpha^{n+a-2-1}e^{-\alpha(R+b)}d\alpha \\
& =\frac{(R+b)^{n+a}}{\Gamma(n+a)}\frac{\Gamma(n+a-2)}{(R+b)^{n+a-2}} \\
& =\frac{(R+b)^2}{(n+a)(n+a-1)}
\end{align} Then $\hat{t}=\frac{(n+a)(n+a-1)}{(\sum_{i = 1}^{n}\log(X_i)+b)^2}$ . I am asked to find the asymptotic distribution as well. We have $\frac{(n+a)(n+a-1)}{(\sum_{i = 1}^{n}\log(X_i)+b)^2}\stackrel{p}\to\alpha^2$ so it is a consistent estimator. I am trying to use the Central Limit Theorem, but need to find the variance, is my work up to now correct? If so, how do I find the asymptotic distribution?","['statistical-inference', 'statistics', 'parameter-estimation', 'bayesian', 'bayes-theorem']"
3577078,"When $\langle \sigma\rangle$ and $\langle\tau\rangle$ intersect trivially, where both $\sigma$ and $\tau$ are $n$-cycles in $S_n$","Let $\sigma,\tau\in S_n$ be two $n$ -cycles. When does $\langle\sigma\rangle\cap\langle\tau\rangle=1$ ? Note that $\sigma$ and $\tau$ are conjugate in $S_n$ and WLOG we may assume $\sigma = (1,2,\dots,n)$ and $\tau = \sigma^g$ for some $g\in S_n$ . I need to find the number of $g$ in $S_n$ such that $\langle\sigma\rangle\cap\langle\sigma^g\rangle=1$ . In particular, I'm now focusing the case when $n+1$ is a Fermat prime, and hence $n$ is a power of $2$ . Is there any methods to deal with this problem for $n+1$ to be a Fermat prime or even for general $n$ ?","['permutations', 'permutation-cycles', 'finite-groups', 'combinatorics', 'group-theory']"
3577135,"Unexpected result, does $\Big\lfloor\frac{n-1}{2}\Big\rfloor=\sum_{i=1}^\infty\bigg\lfloor\frac{n+2^i-1}{2^{i+1}}\bigg\rfloor $","While trying to prove something else, I arrived at the result that for $n\in\Bbb{Z}^+$ $$\Big\lfloor\frac{n-1}{2}\Big\rfloor=\Big\lfloor\frac{n+1}{4}\Big\rfloor+\Big\lfloor\frac{n+3}{8}\Big\rfloor+\Big\lfloor\frac{n+7}{16}\Big\rfloor+\cdots=\sum_{i=1}^\infty\bigg\lfloor\frac{n+2^i-1}{2^{i+1}}\bigg\rfloor$$ This result was quite spectacular to me and I want to know if it is true and if it is how I can prove it. For all cases I've tried it seems to hold true.","['ceiling-and-floor-functions', 'combinatorics', 'infinitary-combinatorics', 'sequences-and-series']"
3577193,Closed form for $\sum_{k=0}^{l}\binom{k}{n}\binom{k}{m}$,"Does there exist any closed form for the following sum? $$\sum_{k=0}^{l}\binom{k}{n}\binom{k}{m}$$ Where $l \in \mathbb N$ and $m,n \in \mathbb Z$ My try: $$ \sum_{k=\max\left(m,n\right)}^{l}\binom{k}{n}\binom{k}{m}=\sum_{k=0}^{l}\binom{k}{k-n}\binom{k}{k-m}$$ $$=\left(-1\right)^{\left(-n-m\right)}\sum_{k=0}^{l}\binom{-n-1}{k-n}\binom{-m-1}{k-m}$$ $$=\left(-1\right)^{\left(-n-m\right)}\sum_{k=0}^{l}\binom{-n-1}{-1-k}\binom{-m-1}{k-m}$$ $$=\left(-1\right)^{\left(-n-m\right)}\binom{-n-m-2}{-m-1}$$ $$=\left(-1\right)^{\left(-n-m\right)}\binom{-n-m-2}{-n-1}=\left(-1\right)^{\left(-m-1\right)}\binom{m}{-n-1}$$ $$=\left(-1\right)^{\left(-m-1\right)}\binom{m}{m+n+1}=\left(-1\right)^{n}\binom{n}{m+n+1}$$ I'm not sure whether it's right, so can someone verify the solution, and if it's not right then please provide a closed form (of course if that's exist).","['summation', 'binomial-coefficients', 'closed-form', 'discrete-mathematics']"
3577196,Find the general solution to $\csc \theta + \sec \theta = 1$,"Find the general solution to $$\csc\theta + \sec\theta =1$$ This is how I solved.
We have, \begin{align}
\csc\theta + \sec\theta &=1\\
\frac1{\sin\theta} + \frac1{\cos\theta}& =1\\
\frac{\sin\theta+\cos\theta}{\sin\theta\cos\theta} &=1\\
(\sin\theta + \cos\theta)^2 &= (\sin\theta\cos\theta)^2 \\
1 + 2\sin\theta\cos\theta &= \frac{4\sin^2\theta\cos^2\theta}4\\
1 + \sin2\theta &= \frac{(2\sin\theta\cos\theta)^2 }4\\
4 + 4\sin2\theta &= \sin^2 2\theta\\
\sin^2 2\theta - 4\sin2\theta - 4 &= 0\\
\sin2\theta &= 2 - 2\sqrt2\end{align} Now here I am stuck. Can someone please help me proceed further?","['trigonometry', 'functions']"
3577198,Finding a primitive element in a field with 27 elements. [duplicate],"This question already has answers here : Find generator of multiplicative group of $\mathbb{F}_{27}$ (2 answers) Closed 4 years ago . I am trying to construct a field with 27 elements, and find a primitive element in that field. I considered the irreducible polynomial $f(x)=x^3+2x+1$ over $\mathbb{Z}_3[x]$ . Then I considered $$\mathbb{Z}_3[x]/\langle f\rangle.$$ This is a field with $3^{\deg f}=3^3=27$ elements. I know that the unique elements of this field are given by $$\{a_0+a_1t+a_2t^2:a_i\in\mathbb{Z}_3\}$$ where $t=x+\langle f\rangle$ . Now my question is, what is an efficient way (for beginners) to find a primitive element of this field? One could argue that it suffices to find an element $u\in\mathbb{Z}_3[x]/\langle f\rangle$ with $\text{ord}(u)\neq 1,2,13$ , but finding such a $u$ is computationally tedious (at least to me). Any comments or advice appreciated.","['field-theory', 'finite-fields', 'abstract-algebra']"
3577229,Combinatorics: Distributing coins among three persons,"Seven different coins are to be divided among three persons. If no two of the persons receive the same number of coins but each receive at least one coin & none is left over, then the number of ways in which the division may be made is: (A) 420 (B) 630 (C) 710 (D) None of these The answer given is option (B) This is how I solved it Using inclusion-exclusion principle, distribution of 7 different coins into 3 different groups (of unknown size), such that no group is empty. $$3^7-^3C_1(3-1)^7+^3C_2(3-2)^7=1806$$ But this also includes the counting of groups with any two same number of coins, thus we need to subtract that from the above term, let it be $K$ . The only possible case with ""If no two of the persons receive the same number of coins"" is 2 same, 2 same, 1 different, these are $$(3,3,1), (2,2,3),(1,1,5)$$ Thus the corresponding arrangements are:- $$\frac{7!}{3!3!1!} , \frac{7!}{2!2!3!}, \frac{7!}{1!1!5!}$$ Sum of these three $K=392$ Thus answer $=1806-392=1414$ But the answer given is option (B). Where am I wrong? How would you solve the problem?","['permutations', 'combinatorics']"
3577232,How to show convolution is associative?,"Consider a semigroup $\Gamma$ and the space $$l^1(\Gamma) := \left\{f: \Gamma \to \mathbb{C}: \sum_{x \in \Gamma} |f(x)| < \infty\right\}$$ where the summation is understood as in the following definition: Let $S$ be any set. Let $f: S \to \mathbb{C}$ be a function. We say $\sum_{n \in S}f(n)$ converges to $F\in \mathbb{C}$ if the following
  condition is satisfied: For all $\epsilon > 0$ , there is a finite subset $T_0$ of $S$ such
  that if $T\supseteq T_0$ and $T$ is a finite subset of $S$ , then $$\left|\sum_{n \in T} f(n)-F\right| < \epsilon$$ I know the basic properties of this summation, i.e. Fubini etc. Define the convolution $f * g$ by $$(f*g)(x) = \sum_{\{(y,z)\in \Gamma^2: yz = x\}} f(y)g(z)$$ I'm trying to prove that $$((f*g)*h)(x)=(f*(g*h))(x)$$ or equivalently $$\sum_{ab=x}\sum_{st = a}f(s)g(t)h(b) = \sum_{ab=x}\sum_{st=b}f(a)g(s)h(t)$$ but I can't formally justify why these two sums must coincide. Any help is appreciated!","['measure-theory', 'summation', 'convolution', 'semigroups', 'functional-analysis']"
3577271,Asymptotic expansion of $\sum _{k=1}^n \left(\frac{k}{n}\right)^k$,"How to calculate the asymptotic expansion of these two sums w.r.t $n$ to arbitrary precision? $$\sum _{k=1}^n \left(\frac{k}{n}\right)^k,\sum _{k=1}^n \Gamma\left(\frac{k}{n}\right)^{-k}$$ According to O.Furdui's problem book Limits, Series and fractional part integrals it might be a open problem. I have no idea other than knowing the first sum tends to $\frac{e}{e-1}$ as $n\rightarrow \infty$ (Tannery theorem, for instance) and would like you to help. Note that similar sums $\small\sum\limits _{k=1}^n \left(\frac{k}{n}\right)^n$ , $\small\sum\limits _{k=1}^n \Gamma\left(\frac{k}{n}\right)^{-n}$ can be approximated using Taylor expansion but not so helpful for this case. Thanks in advance!","['gamma-function', 'limits', 'summation', 'asymptotics']"
3577310,Pushforward as integral functor for Azumaya varieties,"Consider $(X,\mathcal{A}_X)$ and $(Y,\mathcal{A}_Y)$ two Azumaya varieties over a field $k$ . Recall that an Azumaya variety is the data of a variety and a sheaf of semisimple $\mathcal{O}_X$ -algebra $\mathcal{A}_X$ . For my purpose, we can assume $k=\mathbb{C}$ and both $X$ and $Y$ are smooth and proper. Assume that $f : (X,\mathcal{A}_X) \to (Y,\mathcal{A}_Y)$ is a strict morphism (that is, the data of a morphism $f_0$ between $X$ and $Y$ such that $f_0^*\mathcal{A}_Y \simeq \mathcal{A}_X$ ). In [Kuznetsov, ""Hyperplane sections and derived categories"" https://arxiv.org/pdf/math/0503700.pdf ], Kuznetsov defines (appendix D) the pushforward by $f$ of a coherent $\mathcal{A}_X$ -module $F$ as the sheaf $(f_0)_*F$ with an $\mathcal{A}_Y$ -module structure induced by $$(f_0)_*F \otimes_{\mathcal{O}_Y}\mathcal{A}_Y \simeq (f_0)_*(F\otimes_{\mathcal{O}_X} \mathcal{A}_X) \to (f_0)_*F$$ Denote by $Rf_*$ the derived functor of $f_*$ . In the same paper, Kuznetsov gives the following definition (after lemma $2.1$ ): Denote $p_X,p_Y$ the projection from $X\times Y$ to $X$ and $Y$ respectively. Given an object $K\in D^b(X\times Y, \mathcal{A}_X^{opp} \boxtimes \mathcal{A}_Y)$ , we define the integral functor $\phi_K : D^b(X,\mathcal{A}_X) \to D^b(Y,\mathcal{A}_Y)$ with kernel $K$ as the functor $$F \mapsto (Rp_Y)_*\left((p_X)^*_0F\otimes_{\mathcal{O}_X\boxtimes \mathcal{A}_Y}K \right)$$ Question 1 : Is the functor $Rf_*$ isomorphic to an integral functor ? If it is, what is the kernel ? To me the best candidate is $\mathcal{O}_{\Gamma_f}$ but I'm not sure how to endow it with a $\mathcal{A}_X^{opp}\boxtimes \mathcal{A}_Y$ -module structure. My guess is to consider the strict closed immersion morphism $j : (\Gamma_f,j_0^*(\mathcal{A}_X^{opp}\boxtimes \mathcal{A}_Y)) \to (X\times Y,\mathcal{A}_X^{opp}\boxtimes \mathcal{A}_Y)$ and consider $Rj_*\mathcal{O}_{\Gamma_f}$ . Question 2 : Given an integral functor $\phi_K : D^b(X,\mathcal{A}_X) \to D^b(Y,\mathcal{A}_Y)$ and a morphism $g :(Z,\mathcal{A}_Z) \to (X,\mathcal{A}_X)$ , do we have the formula (which holds true for usual derived categories of coherent sheaves, [Huybrechts, Fourier-Mukai transform in algebraic geometry , ex. 5.12]) $$\phi_K \circ Rg_* \simeq \phi_\mathcal{R}$$ with $\mathcal{R}\simeq (g\times id_Y)^*K$ . I think the same proof as for usual $\mathcal{O}$ -sheaves work if we have the expected description of $g_*$ in Question 1 .","['derived-categories', 'coherent-sheaves', 'algebraic-geometry', 'derived-functors']"
3577320,Candies drawn from both bowls,"Bowl $1$ contains $N_1$ candies, all different from each other. Bowl $2$ contains $N_2$ candies, all different from each other. However there are $p$ candies identical across the two bowls. In other words, the first bowl contains $p$ candies identical to as many candies in the second bowl and $N_1 - p$ candies that have no equivalent in the second bowl. Similarly, the second bowl contains $p$ candies that are identical to as many candies in the first bowl, and $N_2 - p$ candies that have no equivalent in the first bowl. Now suppose I draw $m_1$ random candies from the first bowl and $m_2$ from the second one. What is the mean number of candy pairs that I will be drawing? In other words, how many of those $m_1$ candies I draw will be identical to some candies I'm drawing from the second bowl? Or equivalently, how many of the $m_2$ candies I have drawn from the second bowl will be identical to some of the $m_1$ candies I have drawn from the first bowl?","['expected-value', 'combinatorics', 'probability']"
3577343,"Suppose $-3$ is a quadratic residue mod $p>3$, where $p$ is prime. Show that $p$ is of form $x^2+3y^2$.","Suppose $-3$ is a quadratic residue $\mod p>3$ , where $p$ is prime. Show that $p$ is of form $x^2+3y^2$ . I have shown that the standard norm on $\mathbb{Z}[\sqrt{-3}]$ and the norm on $\mathbb{Z}[\zeta_3]$ given by $N(a+b\zeta_3)=a^2-ab+b^2$ have the same image. (Here, $\zeta_3$ is a primitive cubic root of unity, so $\mathbb{Z}[\zeta_3]$ is the Eisenstein integers.) I believe I am supposed to use this fact to prove my claim. In other words, given by $p\mid n^2+3$ for some $n$ , I need to show that there is some polynomial in $\zeta_3$ with integer coefficients such that taking norm will yield $p$ . Any and all help would be appreciated. I know the usual argument in this kind of situation is to appeal to unique factorization and proceed without this business of showing that the norm image coincides with the norm image of the Eisenstein integers, but unfortunately $\mathbb{Z}[\sqrt{-3}]$ is not a UFD.","['number-theory', 'algebraic-number-theory']"
3577345,A problem from the Shortlist of the Romanian Mathematics Olympiad,"Prove that $$\int_0^x \left(1+\frac{t}{1!}+\frac{t^2}{2!}+...+\frac{t^{2n+1}}{(2n+1)!} \right)\cdot \frac{1}{1+t^2} dt <\frac{\arctan x}{x} \int_0^x e^t dt$$ for all $x>0$ . (it is not stated in the question, but I suppose that $n$ is just a positive integer) I tried to use the fact that $e^t= \sum\limits_{k=0}^\infty \frac{t^k}{k !}$ . This gave me that $$1+\frac{t}{1!}+\frac{t^2}{2!}+...+\frac{t^{2n+1}}{(2n+1)!}< e^t, \forall t\in [0,x]$$ for some arbitrarily fixed $x$ . This lead to $$\int_0^x \left(1+\frac{t}{1!}+\frac{t^2}{2!}+...+\frac{t^{2n+1}}{(2n+1)!} \right)\cdot \frac{1}{1+t^2} dt < \int_0^x \frac{e^t}{1+t^2} dt.$$ From here I tried to apply IBP, but it doesn't seem to get closer to the required inequality.","['contest-math', 'definite-integrals', 'real-analysis', 'integral-inequality', 'inequality']"
3577369,Is every diffeomorphism conformally equivalent to a volume preserving diffeomorphism?,"Let $D \subseteq \mathbb{R}^2$ be the closed unit disk. Let $f:D \to D$ be a diffeomorphism. Does there exist a smooth $h \in C^{\infty}(D)$ such that $h\cdot f$ is an area-preserving diffeomorphism of $D$ ? Clearly, such an $h$ must send the entire boundary $\partial D$ either to $1$ or to $-1$ . Another necessary condition is $\det\big(d (h\cdot f)\big) = 1$ . Since $$
d (h\cdot f)=h df+dh \otimes f=h df+f \cdot (\nabla h)^T,
$$ by the matrix determinant lemma , at all points where $h \neq 0$ , we have $$
\det\big(d (h\cdot f)\big)=h^2\det(df) \big( 1+h^{-1}(\nabla h)^T ((df)^{-1} \cdot f)\big),
$$ so we need to solve the following PDE for $h$ $$\det(df) \cdot \big( h^2+h(\nabla h)^T ((df)^{-1} \cdot f)\big)=1.$$","['real-analysis', 'partial-differential-equations', 'differential-topology', 'algebraic-topology', 'differential-geometry']"
3577383,Probability of choosing envelopes,"Suppose that you have 20 different letters and 10 distinctly addressed envelopes. The 20 letters consists of 10 pairs, where each pair belongs inside one of the 10 envelopes. Suppose that you place the 20 letters inside the 10 envelopes, two per envelope, but at random. What is the probability that exactly 3 of the 10 envelopes will contain both of the letters which they should contain? I have seen similar questions to this one but they always assign only one letter to one envelope. Also, the scenario is usually how to choose AT LEAST one right envelope. I am not too clear as to how to adapt to this new scenario.",['probability']
3577384,"Show that this is an integral basis of the ring of integers of $\mathbb{Q}(\sqrt{2},\sqrt{3})$.","Show that $1,\sqrt{3},\sqrt{2},\frac{\sqrt{2}+\sqrt{6}}{2}$ is an integral basis for $\mathcal{O}_K$ where $K=\mathbb{Q}(\sqrt{2},\sqrt{3})=\mathbb{Q}(\sqrt{2},\sqrt{3},\sqrt{6})$ . Clearly, the rank of $\mathcal{O}_K$ is indeed $4$ , so it suffices to show that this set spans. My objective is to take an arbitrary $\alpha\in \mathcal{O}_K$ and show that it can be written in terms of my claimed basis with integer coefficients, but I am not sure how one would achieve this. I know there is the exercise treating the general case in Marcus's $\textit{Number Fields}$ , but I am interested in keeping the computations straightforward and limited to this concrete example. (In any case, I do not quite see how the exercise in Marcus is done either.) Any and all help would be appreciated.","['number-theory', 'algebraic-number-theory']"
3577406,Affine conics over an algebraically closed field of char 2,"The following argument classifies affine conics over an algebraically closed field $k$ with char $(k)\neq 2.$ But I don't see where it uses the hypothesis that char $(k)\neq 2.$ Is it an unnecessary hypothesis? Let $k$ be an algebraically closed field with char $(k)\neq2$ and let $Q(x,y)$ be an irreducible polynomial of degree 2 in $k[x,y].$ We write $Q(x,y)=ax^2+bxy+cy^2+dx+ey+f$ for some $a,\ldots,f \in k.$ Since $k$ is algebraically closed, we have $$Q(x,y)=L_1L_2+L_3+f$$ where $L_3=dx+ef$ and $L_1,L_2$ are linear forms in $k[x,y]$ such that $L_1L_2=ax^2+bxy+cy^2.$ We split into two cases: (1) $L_1\sim L_2$ and  (2) $L_1\not\sim L_2.$ (1) If $L_1\sim L_2,$ then $L_2=\lambda L_1$ for some $\lambda \in k^\times$ and we have $Q(x,y)=\lambda L_1^2 + L_3 + f.$ Since $k$ is algebraically closed and $Q(x,y)$ is irreducible in $k[x,y],$ it follows that $L_1\not\sim L_3.$ Therefore $(u,v)=(\sqrt{-\lambda}L_1,\,L_3+f)$ is an affine change of coordinates such that $$Q(x,y)=v-u^2.$$ (2) If $L_1\not\sim L_2,$ then there exist $\lambda,\mu \in k$ such that $L_3=\lambda L_1+\mu L_2.$ As such, we have $Q(x,y)=(L_1+\mu)(L_2+\lambda)-(\lambda\mu-f).$ Since $Q(x,y)$ is irreducible in $k[x,y],$ it follows that $\lambda\mu-f\neq 0.$ Therefore $(u,v)=\left(\dfrac{L_1+\mu}{\sqrt{\lambda\mu-f}}\,,\dfrac{L_2+\lambda}{\sqrt{\lambda\mu-f}}\right)$ is an affine change of coordinates such that $$Q(x,y)\sim uv-1.$$ If $L_1=\alpha x+ \beta y$ and $L_2=\gamma x + \delta y,$ then $L_1\sim L_2$ if and only if $\alpha\delta-\beta\gamma=0.$ Moreover, since $L_1L_2=ax^2+bxy+cy^2,$ we have $$(\alpha\delta-\beta\gamma)^2=(\alpha\delta+\beta\gamma)^2-4(\alpha\gamma)(\beta\delta)=b^2-4ac.$$ It thus follows that $$k[x,y]/(Q(x,y))\cong \left\{\begin{array}{ll}
k[t] & \text{if } b^2=4ac,\\
k[t,t^{-1}] & \text{if } b^2\neq 4ac.
\end{array}\right.$$",['algebraic-geometry']
3577428,Differences Between Studying Scheme Theory with and Without Cohomology,I am curious about the main ways the geometric intuition is different in studying schemes with cohomology (ie Hartshorne) vs. studying schemes without cohomology (ie Eisenbud-Harris). It would be very useful to know how the types of geometric questions one is able to solve differ between the cohomological vs. non-cohomological approach. Thank you.,['algebraic-geometry']
3577452,What is the difference between min and argmin in the context of random variables,"Consider the indepdently distributed expenontial variables $T_1,T_2,T_3,...T_N$ and let $M=\min_{i=1,...,N} T_i$ and $i_s=arg min_{i=1,...,N} T_i$ . Now I understand the difference between min f(x) and argmin f(x) in the context of deterministic function, but in the context of random variables I'm lost as to the difference. Isn't the argument the variables themselves? So let's say that the outcome of $T_3$ is the smallest of the set, so that the outcome of $M$ is $T_3$ , then isn't the outcome of $i_s$ also $T_3$ ?","['statistics', 'probability']"
3577457,Character induced by the trivial character of a subgroup,"We know that if $\{ e\}<G$ is the trivial subgroup and $\chi_0$ is the (necessarily) trivial character of $\{e\}$ , then the induced character in $G$ can be written neatly as $$
\textrm{Ind}_{\{ e \}}^{\ G}(\chi_0) = \sum_{\chi}\chi(1)\chi,
$$ where the sum is taken over the irreducible characters of $G$ . But if $H < G$ is any subgroup and $\chi_0$ is the trivial character of $H$ , can we find a similarly neat expression for $$
\textrm{Ind}_{H}^{G}(\chi_0)?
$$","['characters', 'representation-theory', 'abstract-algebra', 'linear-algebra', 'group-theory']"
3577523,$\lim_{n \to \infty} \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}}$,"Let $a_n = \frac{1}{n} \sum_{r=1}^{\infty} e^{-\frac{r^2}{2n^2}}$ . The sequence is well-defined by considering the ratio test. What then is $\lim_{n \to \infty} a_n$ ? I suspect it is $\sqrt{\pi/2}$ , by converting the ""riemann sum"" into a gaussian integral, but there were some slight details that I'm unable to justify. I've tried using another sequence, $b_{L,n} = \frac{1}{n} \sum_{r=1}^{nL} e^{-\frac{r^2}{2n^2}}$ , to help me in the process. $\lim_{n \to \infty} b_{L,n} = \int_0^L e^{-\frac{x^2}{2}} dx$ and $\lim_{L \to \infty} b_{L,n} = a_n$ . But to show that $\lim_{n \to \infty} \lim_{L \to \infty} b_{L,n} = \sqrt{\pi/2}$ , I can't just swap the order of the limits, can I?","['integration', 'riemann-sum', 'improper-integrals', 'real-analysis']"
3577554,Hessian in second-order Taylor approximation,"The second order Taylor approximation for a function $f(\mathbf{x}^*)$ is presented to me as $$f(\mathbf{x}^* + h\mathbf{y}) = f(\mathbf{x}^*) + h \nabla f(\mathbf{x}^*)^T \mathbf{y} + \dfrac{1}{2} h^2 \mathbf{y}^T \nabla^2 f(\mathbf{x}^*) \mathbf{y} + O(h^3)$$ But from what I understand, shouldn't there be a Hessian matrix in the third term? Or is the Laplacian in the third term somehow the Hessian? I would greatly appreciate it if people would please take the time to clarify this.","['laplacian', 'multivariable-calculus', 'taylor-expansion', 'optimization', 'hessian-matrix']"
3577558,Show that $\sqrt{x^2+y^2}$ is not differentiable at the origin.,"This is very simple but I am having some trouble. Please indicate whether my proof is correct. We want to show that $f(x, y) = \sqrt{x^2 + y^2}$ . Is not differentiable at the origin. If it was differentiable, the partial derivatives at the origin would exist. However, for example, $$
\lim_{h \to 0} \frac{f(h,0) - f(0, 0)}{h} = \frac{|h|}{h} = 
\begin{cases}
1, \quad \text{ if } h \to 0^+ \\
-1, \quad \text{ if } h \to 0^-
\end{cases}
$$ and hence the partial derivatives fail to exist. Thanks in advance.","['multivariable-calculus', 'solution-verification', 'real-analysis']"
3577636,Invertibility in the Complex Group Ring,"Let $\Gamma$ be a discrete group, and let $\Bbb{C}[\Gamma]$ be the associated complex group ring. If $\sum_{g \in \Gamma} a_g g$ represents some element in the group ring, where all but finitely many of the $a_g \in \Bbb{C}$ are nonzero, then we can equip $\Bbb{C}[\Gamma]$ with the follow natural involution: $$\left(\sum_{g \in \Gamma} a_g g \right)^* := \sum_{g \in \Gamma} \overline{a_g} g^{-1}$$ This gives us a notion of self-adjoint elemnts ( $f^* = f$ for $f \in \Bbb{C}[\Gamma]$ ) and positive elements (those of the form $\sum h_i^* h_i$ for $h_i \in \Bbb{C}[\Gamma]$ ). From here we can get an ordering on the self-adjoint elements: $f \le g$ if and only if $g - f$ is positive. I am wondering whether the following is true: Let $f \in \Bbb{C}[\Gamma]$ be a positive element. Then $f$ is invertible in $\Bbb{C}[\Gamma]$ if and only if there exists $\epsilon > 0$ such that $f \ge \epsilon 1$ . I know this is true for $C^*$ -algebras, and I suspect that the proof just amounts to $C^*$ -algebraic tricks. For example, the forward direction is trivial, because if $f$ is invertible in $\Bbb{C}[\Gamma]$ , then it is invertible in the full group $C^*$ -algebra $C^*(\Gamma)$ , which is just the completion of $\Bbb{C}[\Gamma]$ with respect to a particular norm (see this ). However, the other direction is not obvious. If $f \ge \epsilon 1$ for some $\epsilon > 0$ , then $f$ is invertible in $C^*(\Gamma)$ . But why must its inverse live in $\Bbb{C}[\Gamma]$ ? My gut tells me this is true, but I don't see it at the moment.","['group-rings', 'c-star-algebras', 'functional-analysis', 'operator-algebras']"
3577650,Are there solutions to this coupled system of ODE's?,"While working on a variational problem, I have reached to the following question. Let $0<\lambda < \frac{1}{2}$ be a parameter. Are there smooth strictly increasing surjective maps $\phi:[0,1] \to [0,\lambda]$ satisfying $\phi(0)=0$ and the coupled system of ODE's given by $$\phi'=\frac{1}{2}\big(1 + \sqrt{1-4\frac{\phi \phi'}{r}}),\frac{\phi}{r}=\frac{1}{2}\big(1 - \sqrt{1-4\frac{\phi \phi'}{r}})\,\,\,\,\,?$$ Note that taking the product of two these equations result in a consistent tautology. In particular, I want the ODE to be defined everywhere, i.e. $\frac{\phi \phi'}{r} \le \frac{1}{4}$ for every $r$ . Are there such solutions $\phi$ that also satisfy $\phi^{2k}(0)=0$ for every natural $k$ ? Note that for the crucial value $\lambda=\frac{1}{2}$ (which is out of scope here), $\phi(r)=\lambda r$ does the job. However, $r \mapsto \lambda r$ does not satisfy the ODE for $\lambda < \frac{1}{2}$ .","['calculus', 'symmetry', 'ordinary-differential-equations', 'real-analysis']"
3577711,How many number of permutations,"Hereâs a question Iâm struggling with: There are 10 book consisting of 4 biographies and 6 novels.  A person stacks four of the books together.  In the stack of four books, at least 2 books must be biographies.  How many possible permutations are there for stacking the four books? I thought of two ways to do this problem: (# permutation for biographies with r=2) * (# permutations for 8 remaining books with r=2) * (# of possible positions where the two biographies can occupy) $$ ^4 P_2 * ^8 P_2 * ^4 C_2= 
\\\dfrac{4!}{2!} * \dfrac{8!}{6!} *\dfrac{4!}{2!*2!}=
\\4*3*8*7*6=
4032$$ The rationale behind this method is as follow: The other method I used is as follow: to satisfy the requirement, you first pick two biography books out at random, which has 4*3 permutations.  Then, out of the 8 remaining books, you pick 2 random books, which has 8*7 permutations.  In terms of order, there are 6 total combinations for ordering of biography B and other books X (BBXX, BXBX, BXXB, XBBX, XBXB, XXBB).  Thus the solution should be 4*3*8*7*6 The other method I used is as follow: (# of total permutations) - (# of permutations with no biography) - (# of permutations with exactly 1 biography) $$
^{10} P_4 - ^6 P_4 - ^6 P_3 * ^4 P_1 * ^4 C_1
\\
\dfrac{10!}{6!}-\dfrac{6!}{2!}-\dfrac{6!}{3!}*4*4=
\\ 5040-360-1920=2760
$$ The rationale behind this is simpler: from total amount of permutations, I subtract away permutations where no biography books exist and permutations where only one biography book exist, leaving only permutations with 2 or more biography books. The two methods both make logical sense to me, so Iâm lost as to why they give different results. Iâm struggling to see what went wrong that would cause the two to have differing solutions","['permutations', 'combinatorics', 'discrete-mathematics']"
3577829,Understanding old notation from set theory / game theory,"I am reading an old paper from 1949 by Shapely, Karlin and Bohnenblust titled ""Solutions of Discrete Two-persons Games"" They introduce the following definitions However I am unsure how these definitions should be interpreted. It doesn't seem like $\Sigma$ represents a sum operation here as the sets $I_{1}(x)$ and $J_{2}(y)$ seem to be sets of indices, so I don't see a reason why you would want to add elements of the sets together. Should I interpret $\Sigma$ here as a union of the sets of indices instead? If so how should I interpret $\Pi$ ? Once again it doesn't seem like a standard product operator?","['elementary-set-theory', 'game-theory', 'notation']"
3577880,Properties of three terms of a geometric series,"Iâm [still!] working on the equation in this question , namely $$(b^2+2)^2=(a^2+2c^2)(bc-a).  \tag{$\star$}$$ where $a,b,c$ are integers. Evidently, $(\star)$ implies $$\frac{b^2+2}{bc-a} = \frac{a^2+2c^2}{b^2+2},  \tag{1}$$ which is to say that $\{bc-a,b^2+2,a^2+2c^2\}$ are three consecutive terms of a geometric series. QUESTION: Does that fact provide any information that would help in solving $(\star)$ ? i.e. , are there properties of geometric series that can be brought to bear on the problem? Each fraction in $(1)$ is actually an integer, in case that provides more leverage/structure. EDIT: The reason I know this is that I derived this equation from the equation $x^3=y^2+2$ , where $x=(b^2+2)/(bc-a)$ is a positive integer by assumption.","['divisibility', 'elementary-number-theory', 'diophantine-equations', 'sequences-and-series', 'geometric-series']"
3577888,Continuum crosses over plane,"The task is to check whether it is possible to fit continuum number of geometric figures called crosses (which is basically square diagonals (on the picture there are 2 of them)) in a way that they have no intersections at all. Cross sizes are not necessary equal, it may vary from cross to cross. P.S: I was thinking about drawing some imaginary circle around each cross joint point, then selecting there set of 4 points with rational coordinates, each one for section inbeetween cross lines (in a upper-left ""corner"", upper-right ""corner"", lower-left and lower right ""corners"").  Than we have that unique set of 4 points with rational coordinates, but don't know what to do next with them. Maybe there is a way to prove, that number of that sets of 4 rational coordinates is lower that continuum or somethingg similar to that..","['elementary-set-theory', 'geometry', 'discrete-mathematics']"
3577895,What is the simplest example of a non-exchangeable sequence of random variables?,"I read the wiki article on exchangeability which contained examples, but not negative examples. Can you please provide the simplest negative example of exchangeable random variables? Also, I know that: $P(x_1, x_2) = P(x_1 | x_2) P(x_2) = P(x_2 | x_1) P(x_1)$ I thought this is always true, but now I think it is invalid for cases of non-exchangeable random variables, but I cannot see examples.","['statistics', 'probability-distributions', 'random-variables']"
3578018,Use epsilon-delta definition of limit to establish the following: $\displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3}$,"I understand that my solution here is probably not the most efficient (My professor's solution is ""cleaner"") but it is how my mind attacked the problem.  I have been losing lots of points for minor details that I've been unable to see.  Does the following proof hold?  Am I making any major (or minor) errors? \begin{align*}
\left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|&\leq\left|\frac{1}{2+\sqrt{x}}\right|+\left|\frac{1}{3}\right|<\epsilon~~~\mbox{(by triangle inequality)}\\
&\implies\left|\frac{1}{2+\sqrt{x}}\right|+\frac{1}{3}<\epsilon\\
&\implies\left|\frac{1}{2+\sqrt{x}}\right| < \epsilon-\frac{1}{3}\\
&\implies \frac{1}{2} < \epsilon-\frac{1}{3}~~~~\mbox{(Because, }\sqrt{x}~\mbox{only a real number when } x\geq 0.)\\
&\implies 1<2(\epsilon-\frac{1}{3})\\
&\implies \left|x-1\right|<2\epsilon-\frac{2}{3}=\delta~~~~\mbox{(Because, choosing }x~s.t.~0<x<2\implies~-1<x-1<1)\\
\end{align*} $\therefore \left|x-1\right|<\delta\implies\left| \frac{1}{2+\sqrt{x}}-\frac{1}{3}\right|<\epsilon$ and $\displaystyle\lim_{x\to 1}\frac{1}{2+\sqrt{x}}=\frac{1}{3}$","['limits', 'solution-verification', 'epsilon-delta']"
3578053,"GRE math question: $ \lim_{x \to 0} \left[ \frac{1}{x^2} \int_0^x \frac{t + t^2}{1 + \sin t}\, \mathrm{d} t \right] $","This question is from the Princeton Review book Cracking the GRE Mathematics Subject Test , chapter 2, question 7. The question asks to find the following limit: $$ \lim_{x \to 0} \left[ \dfrac{1}{x^2} \int_0^x \dfrac{t + t^2}{1 + \sin t}\, \mathrm{d} t \right] $$ My solution was as follows: let $F(t)$ be some antiderivative of $(t + t^2)/(1 + \sin t)$ . Then, the limit can be written $\begin{align} \lim_{x \to 0} \left[ \dfrac{1}{x^2} (F(x) - F(0)) \right] &= \lim_{x \to 0} \left[ \dfrac{1}{x} \cdot \dfrac{F(x) - F(0)}{x} \right] \\
&= \lim_{x \to 0} \left[ \dfrac{1}{x} \cdot F'(0) \right] = 0 \end{align}$ However, the correct answer is $\dfrac{1}{2}$ , as given here: Since the integral equals $0$ when $x = 0$ , the limit is of the indeterminate form $\dfrac{0}{0}$ , so we apply L'HÃ´pital's rule $$\lim_{x \to 0}\frac{\int_0^x \dfrac{t + t^2}{1 + \sin t} \, dt}{x^2} = \lim_{x \to 0}\frac{\dfrac{x + x^2}{1 + \sin x}}{2x}$$ $$ = \lim_{x \to 0}\frac{x(1 + x)}{2x(1 + \sin x)} = \lim_{x \to 0}\frac{1 + x}{2(1 + \sin x)} = \frac{1}{2}$$ I understand the provided solution, but cannot see why my solution is incorrect?","['integration', 'limits', 'calculus', 'limits-without-lhopital']"
3578055,Does it make sense to take the standard deviation of a uniform distribution of values?,"You can get the mean and standard deviation of any set of numbers and come up with a Gaussian fitting them. What does it mean if you do this with another distribution, such as a uniform distribution? For instance, if i generate 1,000,000 uniform random numbers from 0 to 100, i get a mean of about 50 and a standard deviation of about 28.8. Gaussians have the 68-95-99.7 rule ( https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule ) which say that 68% of the values are going to be +/- 1 standard deviation from the mean. Using that logic, that says that 68% of the values in the uniform distribution are going to be between 50-28.8 and 50+28.8 aka between 21.2 and 78.8.  those values have a range of 57.6 which is definitely not accurate for a uniform distribution.  68% of the samples should be +/-34 from the mean of 50. When i plot the normalized histogram (pdf) of these two distributions, this is what I get. So, does taking the standard deviation of a uniform distribution make sense?  Can it tell me anything in particular other than a very general sense of how much dispersion the values have?  It doesn't feel very useful when the standard deviation itself has to be interpreted differently based on what sort of distribution the data itself has.  This doesn't seem to let you compare two pieces of data that may have different distributions. Am i missing something? Thanks!","['statistics', 'uniform-distribution', 'gaussian']"
3578109,How many ways can we divide the Space with $N$ lines?,"We know that the maximum number of pieces that can be created with a given number of cuts (lines) $n$ , is given by the formula: $$P_{\max} = \frac{n^2 + n + 2}{2}$$ This is the problem of dividing a pancake or pizza by $n$ cuts, also known as Lazy Caterer's Sequence. A simple proof can be found here in this Wikipedia article: A Pancake Division . But my question is a little different: How many ways can we divide the space with $n$ lines? So it's not just about getting the most out of formed regions! The figure below shows examples of the problem for better understanding: Now let $P$ be the number of possible ways, how to determine $P$ as a function of $n$ ? Note that it doesn't matter the size or the shape of the regions, but how they are separated in space by the lines. I think what should be considered is the behavior between the lines, for example, if they are parallel, if they are concurrent, or if they are parallel and concurrent! The simplest form would be all parallel lines and the most complex form would all be concurrents at different points.","['combinations', 'geometry', 'combinatorial-geometry', 'combinatorics', 'sequences-and-series']"
3578136,"Number of times for which a random process reaches a ""trough""","Let $W_t$ be the Wiener process (Brownian motion). I am interested in the following distributions/probabilities What is the expected number of intervals on $[0,1]$ on which $W_t<a$ , where $a<0$ is a given number? More precisely: since $W_t$ is continuous in $t$ , the set $U=\{t\in[0,1]:W_t<a\}$ is open. Therefore, $U$ can be expressed as at most countably many open intervals. Write $U=\bigcup_{k=1}^n B_k$ , where $B_k$ are disjoint open intervals. We may have $n=\infty$ . My question is: what is the expected value of $n$ ? What distribution does $n$ follow? Let $m$ be number of intervals in $\{B_k\}$ with length at least $b$ . $m$ is always finite. What is the expected value of $m$ ? I do not find these in my random process book, but I think they are quite natural questions to ask. I would really appreciate it, if anyone could provide me with some reference about those problems, or write an answer to them.","['stochastic-processes', 'statistics', 'brownian-motion']"
3578151,Simplifying $\frac{\sin x+\sin x\tan^2x}{\tan x}$ to $\sec x$,"I have to simplify (the answer is $\sec(x)$ ): $$\frac{\sin(x)+\sin(x)\cdot\tan^2(x)}{\tan(x)}$$ I have looked at images for all trig identities but nothing shows $\sin(x)+\sin(x)$ or $\frac{\sin\left(x\right)+\sin\left(x\right)}{\tan\left(x\right)}$ In short: I tried brute tests on the calculator, but different values give differing answers. Longer explanation: I tried to test random values (but the same value for each function), and keep getting differing results. Such as $\tan(45)^2 = 1$ in degree mode, but $\tan(5)^2 != 1$ . Tried in Radian mode and the results are $2.62$ and $11.43$ respectively. So couldn't pick out a pattern. **EDIT Thank you for those who have provided answers. I really can not figure out what happened to my original $sin(x) + $ in all the answers provided.",['trigonometry']
3578161,"Basic probability question, confusion with basic concept","There is a bag with $95$ green and $5$ red balls. If $3$ balls are drawn, what is the probability that $2$ of them are red and $1$ green? Solution 1: Total number of ways is $100\choose3$ Favorable number of ways = $\binom{5} {2} * \binom{95}{1}$ So required probability = $\frac{\binom{5}{2} \binom{95}{1}}{\binom{100}{3}}$ Solution 2:
   However since the favorable number of ways is obtained by multiplying $5\choose2$ and $95\choose1$ , it takes the order between two red and one green in to account while the total number of ways does not take order in to account. 
Should I therefore divide by $2$ to get the accurate favorable number of ways?
so required probability will be $\frac{\binom{5}{2} \binom{95}{1}}{\binom{100}{3}*2}$ Solution 3: Total number of ways = $\binom{100}{3} * 3!$ (counting in way to account for order) Favorable number of ways = $\left(\binom{5}{2} * 2!\right) * \left(\binom{95}{1} * 1!\right)$ (to also account for order like in denominator) So required probability = $\frac{\binom{5}{2} * \binom{95}{1}}{\binom{100}{3} *3}$ ) I understand that as long as we are consistent in taking in to account the order when counting favorable and total ways, we should get the same probability.
However I am finding it hard to reconcile solution 1 vs solution 2 vs solution 3.","['discrete-mathematics', 'combinatorics', 'probability']"
3578213,Estimating the value of $\sigma$ for Brownian motion,"Let $X_t=\sigma W_t$ be a stochastic process, where $W_t$ is the Wiener process and $\sigma$ is an unknown parameter. I want a formula to estimate the value of $\sigma$ (which could not be found in my book) from $n$ successive measurements $(x_{t_k})_{k=1}^n$ . Now, I understand that $(X_{t_k})_{k=1}^n$ is a normally distributed vector in $\mathbb R^n$ , but so I could NOT estimate its co-variance matrix because I only have the sample of ONE vector in $\mathbb R^n$ , and co-variance could NOT be estimated from ONE sample. That stops me from estimating $\sigma$ . How could I estimate $\sigma$ ?","['stochastic-processes', 'statistics', 'brownian-motion', 'parameter-estimation']"
3578247,For how many integers $n$ is $n^6+n^4+1$ a perfect square? [duplicate],This question already has answers here : Square valued integer polynomial (3 answers) Closed 4 years ago . QUESTION For how many integers $n$ is $n^6+n^4+1$ a perfect square? I am completely blank on how to start. Could anyone please provide tricks on how to get a start on such questions? Thanks for any answers!,"['algebra-precalculus', 'diophantine-equations']"
3578397,Having difficulty understanding what happens when reversing order of quaternion multiplication.,"The textbook I am reading claims that quaternion multiplication works like so: $
q_1q_2 = V_1 \times V_2 + s_1V_2+s_2V_1+s_1s_2-V_1 \cdot V_2
$ Which is a simplified view of $
q_1q_2 = (x_1w_2+y_1z_2-z_1y_2+w_1x_2)i\\
+(y_1w_2+z_1x_2+w_1y_2-x_1z_2)j\\
+(z_1w_2+w_1z_2+x_1y_2-y_1x_2)k\\
+(w_1w_2-x_1x_2-y_1y_2-z_1z_2)
$ where $q$ is defined as: $q=xi+yj+zk+w$ Which makes sense to me if you take into consideration the multiplication rule for quaternions: $i^2=j^2=k^2=ijk=-1$ The thing that I don't understand is how reversing the order of quaternion multiplication works. The textbook defines it as: $q_2q_1=q_1q_2-2(V_1 \times V_2)$ However I have no idea how to go about obtaining this result.","['linear-algebra', 'quaternions']"
3578433,Difference between a Reduced Residue Class and a Reduced Residue System,"Currently studying Dirichlets proof of Primes in arithmetic progressions.
Is there a difference between a Reduced Residue Class and a Reduced Residue System? Any subset $R$ of the integers is called a reduced residue system modulo $n$ if: $\gcd(r, n) = 1$ for each $r$ contained in $R$ ; $R$ contains $\phi(n)$ elements;
no two elements of $R$ are congruent modulo $n$ . For example, if a reduced residue system for $12$ is $a = \{1,5,7,11\}$ , and we have also $\{13,17,19,23\}$ , would the reduced residue class be the general description of this? e.g all the terms that are equal to the terms in $a \mod 12$ ? Is the reduced residue class the same as the set of all congruence classes?","['number-theory', 'abstract-algebra']"
3578439,Proof of Klingenberg's lemma in do Carmo's Riemannian Geometry,"The following is Exercise 10.1 in Riemannian Geometry by M. do Carmo. (Klingenberg's Lemma). Let $M$ be a complete Riemannian manifold with sectional curvature $K<K_0$ , where $K_0$ is a positive constant. Let $p,q\in M$ and let $\gamma_0$ and $\gamma_1$ be two distinct geodesics joining $p$ to $q$ with $\ell(\gamma_0)<\ell(\gamma_1)$ . Assume that $\gamma_0$ is homotopic to $\gamma_1$ , that is, there exists a continuous family of curves $\alpha_t$ , $t\in[0,1]$ such that $\alpha_0=\gamma_0$ and $\alpha_1=\gamma_1$ . Prove that there exists $t_0\in(0,1]$ such that $$\ell(\gamma_0)+\ell(\alpha_{t_0})\geq\frac{2\pi}{\sqrt{K_0}}.$$ The hint goes: Hint: Assume $\ell(\gamma_0)<\pi/\sqrt{K_0}$ (otherwise, we have nothing to prove). From Ranch's Theorem, $\exp_p:TpM\to M$ has no critical point in the open ball $B$ of radius $\pi/\sqrt{K_0}$ , centered at $p$ . For $t$ small, it is possible to lift the curve at to the tangent space $T_pM$ , i.e., there exists a curve $\widetilde{\alpha}_t$ in $T_pM$ , joining $\exp_p^{-1}(0)=0$ to $\exp_p^{-1}(q)=\widetilde{q}$ , such that $\exp_p\circ\widetilde{\alpha}_t=\alpha_t$ . It is clear that it is not possible to do the same for every $t\in[0,1]$ , since $\gamma_1$ cannot be lifted keeping the endpoints fixed. We conclude that for all $\varepsilon>0$ there exists a $t(\varepsilon)$ such that $\alpha_{t(\varepsilon)}$ can be lifted to $\tilde{\alpha}_{t(\varepsilon)}$ and $\tilde{\alpha}_{t(\varepsilon)}$ contains points with distance $<\varepsilon$ from the boundary $\partial B$ of $B$ . In the contrary case, for some $\varepsilon>0$ , all lifts $\tilde{\alpha}_t$ are at the distance $\geq\varepsilon$ from $\partial B$ ; the set of $t$ 's for which it is possible to lift $\alpha_t$ will then be open and closed and $\alpha_1$ could be lifted, which is a contradiction. Therefore, for all $\varepsilon>0$ , we have $$\ell(\gamma_0)+\ell(\alpha_{t(\varepsilon)})\geq\frac{2\pi}{\sqrt{K_0}}-\varepsilon.$$ Now choose a sequence $\{\varepsilon_n\}\to0$ , and consider a convergent subsequence of $\{t(\varepsilon_n)\}\to t_0$ . Then there exists a curve $\alpha_{t_0}$ with $$\ell(\gamma_0)+\ell(\alpha_{t_0})\geq\frac{2\pi}{\sqrt{K_0}}.$$ Why do such liftings exist? We only know that $\exp_p$ is nonsingular on $B(0,R):=\{v\in T_pM:|v|<R\}$ , not that $\exp_p|_{B(0,R)}$ is a covering map or anything. Local diffeomorphisms can behave badly when it comes to lifting curves! So here is my question: Let $(M,g)$ be a complete Riemannian manifold and $p\in M$ . Suppose $\exp_p$ is nonsingular everywhere on $B(0,R)\subset T_pM$ . Does any curve on $M$ starting from $p$ with length $<R$ lift to a curve on $T_pM$ starting at $0$ ? What about homotopies of such curves? In particular, why does the hint works? Another question: While in do Carmo's book this result is called Klingenberg's lemma, I cannot find it in any other resource. When and in which paper did Klingenberg prove this?","['riemannian-geometry', 'differential-geometry']"
3578453,Asymptotic behaviour of $\int_0^{\infty } x^{-x} \exp (n x) dx$,"I was given that $$\underset{n\to \infty }{\text{lim}}\frac{\int_0^{\infty } x^{-x} \exp (n x) \, dx}{\exp \left(\frac{n-1}{2}+\exp (n-1)\right)}=\sqrt{2 \pi }$$ This is accessible via Laplace method. But how can we compute its full asymptotic expansion w.r.t $n$ ? Any help will be appreciated.","['integration', 'limits', 'definite-integrals', 'asymptotics']"
3578480,Make sense of norm notations and why is $\int_\Omega \nabla\theta\cdot\nabla\theta_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|\theta|_1^2$?,"I have the heat equation \begin{align}
\dot{u}(x,t) -\Delta u(x,t) = f(x,t),& \quad x\in\Omega\subset\mathbb{R}^2 \\
u(x,t) = 0, & \quad x\in\Gamma = \partial\Omega, 0<t\leq T \\
u(x,0) = u_0(x),& \quad x\in\Omega
\end{align} In my book there is a proof about a stability estimate. In one of the lines they state that $$a(u,u_t)=\int_\Omega \nabla u\cdot\nabla u_t \ d\mathbf{x} = \frac{1}{2}\frac{d}{dt}|u|_1^2.$$ How does the last equality follow? Need help getting through the arithmetic there. What does the sub-index $1$ mean in the absolute value? EDIT: Adding an attempt. Attempt: $$\int_\Omega \nabla\theta\cdot\nabla\theta_t \ dx =  \int_\Omega\nabla\theta\cdot \nabla\left(\frac{d}{dt}\theta\right) \ dx$$ $$=\int_\Omega \nabla\theta\cdot\frac{d}{dt}\nabla\theta \ dx =\frac{1}{2}\frac{d}{dt}\int_\Omega(\nabla\theta)^2 \ dx =\frac{1}{2}\frac{d}{dt}||\nabla\theta||^2.$$ But why does the book have $\frac{1}{2}\frac{d}{dt}|\theta|_1^2$ instead?","['integration', 'normed-spaces', 'derivatives']"
3578512,A question about the conjugation,"I'm studying something about the Conjugation and my reference propose to prove that the property of being topologically mixing is preserved by conjugation.
Obviously the definition of topologically mixing is clear while about the conjugation I have that: Give two metric spaces $X,Y$ and $f:X\rightarrow X$ , $h:Y\rightarrow Y$ we can say that $f$ and $h$ are conjugate if there exists a homeomorphism $\theta:X\rightarrow Y$ such that $\theta\circ f=h\circ \theta$ .( $f\sim h$ ) Shortly which I have to prove is: given X and Y two metric spaces and the two applications $f:X\rightarrow X$ and $h:Y\rightarrow Y$ , $f$ is conjugated with $h$ by $\theta$ then $f$ is topologically mixing $\iff$ $h$ is too. TOPOLOGICALLY MIXING: A function $f$ is topologically mixing if $\forall A,B$ open and not empty set $\exists n\in \mathbf{N}$ such that: $f^{a}(A)\cap B\not=\emptyset,\ \forall a\ge n.$","['continuity', 'general-topology', 'functions', 'mixing']"
3578586,Why doing MÃ¶bius transformation behaves like operating a 2X2 matrix of coefficients of MÃ¶bius transformation?,"So, I was doing proof that inverse of MÃ¶bius transformation is again a MÃ¶bius transformation. I started with $w= f(z)=\frac{az+b}{cz+d}$ . To find the inverse, I found z in terms of w by doing simple algebra and I got $z=\frac{dw-b}{-cw+a}$ =g(w) (say). Alternatively, I can just look at the inverse of the matrix of coefficients of f(z) i.e. \begin{pmatrix}a&b\\c&d\end{pmatrix} and find its inverse to get $\frac{1}{ad-bc} \begin{pmatrix}d&-b\\-c&a\end{pmatrix}$ . This inverse matrix has elements corresponding to the inverse transformation. I just can't see why. Please explain without using any group theory if possible.","['complex-analysis', 'linear-algebra', 'geometry']"
3578617,Subgroup generated by commensuration class of an element of a virtually free group,"Suppose $G$ is a group. Letâs call $a, b \in G$ commensurate , iff $\exists m, n \in \mathbb{Z} \setminus \{0\}$ , such that $a^n = b^m$ . One can see, that commensuration is an equivalence relationship: $$a = a$$ $$(a^n = b^m) \to (b^m = a^n)$$ $$((a^n = b^m) \cap (b^p = c^q)) \to (a^{np} = c^{mq})$$ Thus we can divide the elements of a group into commensuration classes (letâs denote a commensuration class of an element $a$ as $CC(a)$ . My question is: Suppose $G$ is a finitely generated virtually free group, $a \in G$ is an infinite order element. Is it always true that $\langle CC(a) \rangle$ is virtually cyclic? Note, that we can not omit virtual freedom of $G$ , because of the existence of Baumslag-Solitar groups $BS(n, m) = \langle a, t| ta^nt^{-1} = a^m \rangle$ . $a$ is required to be of infinite order because of the existence of the following two facts: All finite-order elements of a group form a commensurability class Proof : If $a$ and $b$ are finite-order elements, then we can take $n$ and $m$ as their respective orders. If $a^k = e$ and $a^n = b^m$ , $b^{mk} = e$ . Q.E.D. $(C_2 \ast C_3)â$ is non-cyclic free Proof: follows from Ping-Pong lemma.","['infinite-groups', 'finitely-generated', 'abstract-algebra', 'free-groups', 'group-theory']"
3578688,"Szamuely's ""Galois groups and fundamental groups"" exercise I.7","I am working through the exercises in TamÃ¡s Szamuely's book ""Galois group and fundamental groups"". Exercise 7 from the first chapter is the following. Let $k$ be a field and $\bar{k}$ be a fixed algebraic closure of $k$ . Given a finite etale $k$ -algebra $A$ with a finite group $G$ acting via $k$ -algebra automorphisms on it, say it is ""Galois"" if $A^G = k$ , and $\mathrm{dim}_k(A) = |G|$ . Show that such an algebra $A$ is Galois if and only if $A \otimes_k \bar{k} \simeq \bar{k}[G]$ as a $G$ -module. I'm currently stuck at showing that if $A$ is Galois, then $A \otimes_k \bar{k} \simeq \bar{k}[G]$ as a $G$ -module. I get that they both have dimension $|G|$ as $\bar{k}$ -algebra, that the $G$ -invariants are $\bar{k}$ in both case, and that this boils down to finding a basis of the form $(x, g_1.x,\ldots,g_{n-1}.x)$ of $A \otimes_k \bar{k}$ . I know that in the case that $A$ is a finite separable extension of $k$ , then being Galois and being a normal extension is the same, and $G$ is then the Galois group of $A$ . In this case the normal basis theorem gives a slightly stronger result, i.e $A \simeq k[G]$ as a $G$ -module. I tried using this to get the general case where $A$ is a product of finite separable extensions of $k$ , but this is getting nowhere. I also tried to see if I could adapt the proof of the normal basis theorem to get this, but I couldn't as well. Any hint would be appreciated.","['galois-theory', 'group-rings', 'abstract-algebra', 'commutative-algebra']"
3578723,Tensor product of two $n\times n$ orthogonal matrices with determinant $+1$,"If two square matrices A and B are two $n\times n$ orthogonal matrices with determinant unity i.e., $\det A=\det B=+1$ and $A^TA=B^TB=I$ , will the tensor product $C\equiv A\otimes B$ also be orthogonal and have determinant $+1$ ? Can we understand this without restricting to special choices for $n$ such as $n=2,3$ etc","['matrices', 'orthogonal-matrices', 'tensor-products', 'rotations']"
3578739,"Exercise 9 Chapter II, Kunen's Set Theory (1983)","The exercise is the following: Let $\mathcal{B}\subseteq \mathcal{P}(\omega)$ be an almost disjoint family of size $\kappa$ , where $\omega\le\kappa<2^\omega$ . Let $\mathcal{A} \subseteq \mathcal{B}$ , with $|\mathcal{A}|\le\omega$ . Assuming MA ( $\kappa$ ), show that there is a $d\subseteq \omega$ such that $\forall x \in \mathcal{A}(|d\cap x|<\omega)$ and $\forall x \in \mathcal{B}\setminus\mathcal{A}(|x\setminus d| < \omega)$ A more or less direct consequence of MA ( $\kappa$ ) is a similar statement, where we drop the hypothesis $|\mathcal{A}|\le\omega$ and require just that $\forall x \in \mathcal{B}\setminus\mathcal{A}(|x\cap d| = \omega)$ . I tried to use this similar statement, but I don't get much further. In particular I have a problem in using in a smart way both the hypotheses on the countability of $\mathcal{A}$ and MA . Being more specific, the coutability of $\mathcal{A}$ induces me to first use MA and then ""adjust"" what I have found by adding\subtracting elements to\from $d$ by iterating over $\mathcal{A}$ (for example by a diagonal argument). Could you give me a hint? Thanks EDIT: I'm beginning to wonder whether it is solvable..","['combinatorics', 'set-theory']"
3578769,Definition of adjoint operator (asking for intuition),"Definition of the adjoint operator: A linear operator T on an inner product space V is said to have an adjoint operator $T^{*}$ on V if $\langle T(u),v \rangle= \langle u,T^{*}(v) \rangle$ . Question: Why people come up with that definition? It does not sound intuitive to me. $T^{*}$ is the transpose conjugate of T right, and does that definition follow from definition of inner product space?","['inner-products', 'linear-algebra']"
3578804,Expected length of longest common substring,"For two strings, a longest common substring is a substring of maximal length which is common to both strings. For example for $babba$ and $cdddabbd$ a longest common substring is $abb$ which has length $3$ . Consider two uniformly sampled binary strings, each of length $n$ .
  What is the expected length of a longest common substring? If we wanted to know the expected longest common prefix length would be $1$ .  The length follows the geometric distribution . So we could look at all $1 \leq i, j \leq n$ and try to compute the maximum expected value of all longest common prefixes starting at index $i$ in the first string and index $j$ in the second. I don't know how to complete this analysis however.","['combinatorics', 'probability']"
3578808,Labeled tree generation given degree sequence,"I'm looking for some algorithm implementations for generating all labelled trees having the degree sequence as an input. I have found the Nakano's article: http://www.kurims.kyoto-u.ac.jp/~kyodo/kokyuroku/contents/pdf/1644-06.pdf , however the implementation is nowhere to be found and implementing it on my own just from the text would be at least difficult. Are there any ready implementations? I use Sagemath on a daily basis, however I believe its implementation in 'graphs' is based on generating all graphs and then checking whether the degree sequence fits, which is quite unaccaptable. If anyone were to recommend any implementation of the unlabelled trees with given degree sequence that would be highly appreciated as well, however it is not the main question.","['graph-theory', 'discrete-mathematics', 'algorithms']"
3578846,"Is there a well-defined `uniform' distribution on $C([0, 1])$?","I'm wondering whether we can define a uniform distribution on the space of continuous functions over a compact set, e.g. $C([0, 1])$ . If so, then how should I rigorously describe it? And how can I numerically ` draw' a random function from this distribution? If not, then what additional assumption/constraints should I impose to have a well-defined uniform or any kind of distribution that I can easily draw samples from?","['statistics', 'probability-distributions', 'probability-theory']"
3578861,Using Functional Analysis for Differential Equations,"In the functional analysis books that I have read, they do not explain how the ideas and theorems of functional analysis (in the sense of operators on Banach spaces) help to deal with differential equations, such as proving existence or uniqueness of solutions. Could someone give me an example of using the ideas and theorems of functional analysis to actually say something about a (partial) differential equation?","['numerical-methods', 'functional-analysis', 'analysis', 'partial-differential-equations']"
3578865,A uniform tail bound on Poisson random variable.,"Let $N$ be a Poisson random variable with mean $\lambda>0$ . Prove that there exists uniform constants $M,c>0$ (do not depend on $\lambda$ ) such that $\mathbb{P}(N\geq n) \leq \lambda M\, e^{-cn}$ . I tried to prove this with Chernoff style tail bound on $N$ . But most of them give a bound on $\mathbb{P}(N \geq \lambda + t)$ where $t\geq 0$ . Any help is appreciated.","['poisson-distribution', 'probability-distributions', 'probability-theory', 'probability']"
3578867,Show $\sum_{k=1}^{2n-1}\frac{\left(-1\right)^{k-1}k}{\binom{2n}{k}}=\frac{n}{n+1}$,"How it can be shown that: $$\sum_{k=1}^{2n-1}\frac{\left(-1\right)^{k-1}k}{\binom{2n}{k}}=\frac{n}{n+1}$$ for $1 \le n$ I tried to use this method , but that was not helpful, Also I tried to use the following identity: $$\frac{1}{\binom{2n+1}{k}}+\frac{1}{\binom{2n+1}{k+1}}=\frac{2n+2}{2n+1}\ \frac{1}{ \binom{2n}{k}}$$ and use some telescoping property , but again that did not help me. Please if it's possible,then do the proof using elementary ways.","['summation', 'binomial-coefficients', 'discrete-mathematics']"
3578886,Confusion about the proof of PoincarÃ© lemma for Currents,"In Demailly's ""Complex Analytic and Differential Geometry"" page 20 2.D.4, he proves the PoincarÃ© lemma for Currents. The theorem states as follows: Let $\Omega\subset\mathbb R^m$ be a starshaped open subset,and $T$ is a closed current of degree $q$ and order $s$ ,then there exists a current $S$ of degree $q-1$ and of order $\leq s$ ,satisfying $dS=T$ on $\Omega$ . The most useful approach to prove this theorem is first show the fact that every closed current is cohomologous to a smooth form. (A) In the proof of (A) ,he claims if $\Theta$ and all its derivatives are currents of order $0$ , then $\Theta$ is smooth. I know currents of order $0$ on manifold can be considered as differential forms with measure coefficients, but how can we get this claim? Any suggestion and references are appreciated, thanks a lot!","['complex-geometry', 'geometric-measure-theory', 'complex-analysis', 'algebraic-topology', 'differential-geometry']"
3578964,Four consecutive terms of an arithmetic progression,The product of four consecutive terms of an arithmetic progression of integers plus the fourth power of the common difference is always a perfect square. Verify this identity by incorporating symmetry into the notation. I worked on this problem to solidify my heuristic for symmetry. My solution is as follows: $$(a_n-2d)(a_n-d)(a_n)(a_n+d)=a_n^4-2da_n^3-d^2a_n^2+2d^3a_n$$ Adding $d^4$ we have: $$a_n^4-2da_n^3-d^2a_n^2+2d^3a_n+d^4$$ I substituted $x=a_n$ and I obtained $$x^4-2dx^3-d^2x^2+2d^3x+d^4=(x^2-dx^2-d^2)^2$$ But I feel like I didn't use symmetry and I missed the point of the problem. What do you think?,"['arithmetic-progressions', 'sequences-and-series']"
3578973,Monotonicity with expectation,"I think the following is true but I cannot prove it. Let $Z_1, Z_2$ are two random variables defined on the same sample space $\Omega$ . Suppose that $Z_1(\omega) < Z_2(\omega)$ for all $\omega\in \Omega_0$ and $Z_1(\omega) = Z_2(\omega)$ for all $\omega\in \Omega\setminus\Omega_0$ . We have: If $P(\Omega_0)=0$ , i.e. $Z_1=Z_2$ almost surely, then $E(Z_1)=E(Z_2)$ . If $P(\Omega_0)>0$ , then $E(Z_1)<E(Z_2)$ . Could you show if it holds or not?","['conditional-expectation', 'measure-theory', 'expected-value', 'probability-theory']"
3578978,Exponential equation word problem,"The doubling time for a certain skin bacteria is 1.8 hours. There are 1000 bacteria per 1 square centimeter on your phone, and 100 square centimeters in contact with your face. Write the general equation. Find the amount of bacteria on your face after 2 hours after taking a call. My solution: $1.$ $doubling time={ln 2}/{k}$ $1.8={ln2}/{k}$ $k=0.39$ $N={N_0}e^{0.39t}$ $N={1000*bacteria}*{cm^{-1}}{100cm}^{2}*{e}^{0.39t}$ $N=100000*e^{0.39t}$ $2.$ $N=100000*bacteria*e^{0.39*2}=218147=220000*bacteria$ Is this okay for the ""general"" equation?","['algebra-precalculus', 'solution-verification', 'exponential-function']"
3578982,"The probability of choosing a coin out of two different coins, one with tail on both faces","Sue has two coins. One is fair, with a head on one face and a tail on the other. The second is a trick coin and has a tail on both faces. Sue picks up one of the coins at random and flips it. a) Find the probability that it lands heads up. b) Given that it lands tails up , find the probability that she picked up the fair coin. My turn: a) We have one head out of three tails and one head, so the answer is $\frac{1}{4}$ . b) I do not understand how can i start with this?!","['bayes-theorem', 'probability']"
3579012,What is the distribution of $N-X_N$ if $X_i$'s are i.i.d $\operatorname{Exp}(1)$ and $N=\min\{n\ge1:X_n>1\}$?,"Suppose $(X_n)_{n\in\mathbb N}$ is an i.i.d sequence of random variables where $X_1$ has an exponential distribution with mean $1$ . Let $N=\min\{n\ge1:X_n>1\}$ . I am asked to find the distribution of $N-X_N$ . Now $N$ has a geometric distribution, with $$P(N>n)=P(X_1\le 1,\ldots,X_n\le 1)=(1-e^{-1})^n\quad,\,n\in\mathbb N$$ And it can be shown that distribution function of $X_N$ is just $$P(X_N\le x)=P(X_1\le x\mid X_1>1)\,,$$ so that $X_N$ has a shifted exponential distribution with density $$f_{X_N}(x)=e^{-(x-1)}1_{x>1}$$ Since a joint distribution of $N$ and $X_N$ is not given, the question would have made sense if $N$ and $X_N$ were independent. In any case, I tried to find the distribution function as \begin{align}
P(N-X_N\le x)&=\sum_{n=1}^\infty P(n-X_n\le x,N=n)
\\&=\sum_{n=1}^\infty P(X_n\ge n-x,X_1\le 1,\ldots,X_{n-1}\le 1,X_n>1)
\\&=\sum_{n=1}^\infty P(X_1 > \max(n-x,1))(P(X_1\le 1))^{n-1}
\\&=\sum_{n=1}^\infty e^{-\max(n-x,1)}(1-e^{-1})^{n-1}
\\&\stackrel{?}=\sum_{n=1}^{x+1}e^{-1}(1-e^{-1})^{n-1}+\sum_{n=x+2}^\infty e^{-(n-x)}(1-e^{-1})^{n-1}
\end{align} The last two sums can be evaluated, but I am not sure if I arrive at a valid answer. Is there a simpler way to solve this, perhaps using some independence argument? Is it guaranteed that $N-X_N$ will be absolutely continuous? I could say that if $N$ and $X_N$ were independent, but don't think that is true here. @Henry has pointed out that $N$ and $X_N$ are indeed independent. I think I understand the logic but would like to see a formal proof of the independence. Assuming independence, I get \begin{align}
P(N-X_N\le y)&=\int P(N\le y+x)f_{X_N}(x)\,dx
\\&=\int_{1-y}^\infty \left(1-(1-e^{-1})^{y+x}\right)e^{-(x-1)}dx\,1_{y<0}+\int_1^\infty \left(1-(1-e^{-1})^{y+x}\right)e^{-(x-1)}dx\,1_{y>0}
\end{align} Does this look right?","['independence', 'probability-distributions', 'exponential-distribution', 'stopping-times', 'probability']"
3579028,How to impose orthonormality constraints by method of Lagrange multipliers,"I want to find the matrix $\Phi: \mathbb{R}^n \to \mathbb{R}^m$ , $m<n$ that minimizes $$V={\rm tr}(\Phi R \Phi^T)$$ subject to the orthonormality constraint $$\Phi\Phi^T=I$$ where $R: \mathbb{R}^n \to \mathbb{R}^n$ is a given symmetric positive definite matrix. How do I apply the Lagrange multiplier method to this constrained optimization problem? I tried: $$\tilde{V}={\rm tr}(\Phi R \Phi^T) - {\rm tr}\left(\Lambda(\Phi\Phi^T-I)\right)$$ where $\Lambda:\mathbb{R}^m \to \mathbb{R}^m$ is a general matrix of Lagrange multipliers, and this yields the necessary conditions for minimality: $$\frac{\partial \tilde{V}}{\partial \Lambda}=\Phi\Phi^T-I=0$$ $$\frac{\partial \tilde{V}}{\partial \Phi}=R\Phi^T-\Phi^T\Lambda=0$$ The first equation is the orthonormality condition, so far so good. But how is the second equation supposed to find minimum candidates/critical points? Intuitively, I think I know that the global minimum argument $\Phi$ must be the basis of the eigenspace of the lowest $m$ eigenvalues of $R$ . But $\Lambda$ is a general $m\times m$ matrix and nothing seems to constrain it to a diagonal matrix with $m$ eigenvalues (let alone the lowest) on the diagonal . Or am I applying the Lagrange multiplier method the wrong way? By the way: it seems easy for $m=1$ . Then $\Phi^T$ simply becomes a vector and $\Lambda=\lambda$ becomes a scalar. This yields the eigenvalue problem for one eigenvector, which is 'diagonal' in the trivial sense: $$R\Phi^T-\lambda\Phi^T=0$$","['orthonormal', 'linear-algebra', 'lagrange-multiplier', 'eigenvalues-eigenvectors']"
3579044,"Prove that $\int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}$","Prove that if $f:[0,1] \to \mathbb{R}$ is a continuously differentiable function with $\int_0^1 f(x)\,dx=0$ , then $$\int_0^1 \big(1-x^2\big) \big(f'(x)\big)^2\,dx \ge 24 \left(\int_0^1 xf(x)\,dx\right)^{\!2}.$$ I think that I should somehow use the Cauchy-Schwarz inequality, but I wasn't succesful in doing this. I know that $$\left(\int_0^1 xf(x)\,dx\right)^2\le \int_0^1 x^2\,dx \cdot \int _0^1 f^2(x)\,dx=\frac{1}{3}\int _0^1 f^2(x)\,dx,$$ but this is clearly not enough.","['inequality', 'integral-inequality', 'real-analysis']"
3579075,Relating fraction to convex combinations of numerators and denominators of two other fractions,"Consider the following functions $$h(\alpha,x,x',y,y') = \frac{\alpha x + (1-\alpha)x'}{\alpha(x+y) + (1-\alpha)(x'+y')}$$ $$g(x,y) = \frac{x}{x+y}$$ where all values $x,x',y,y',\alpha$ lie in $[0,1]$ . Notice that $h(0,x,x',y,y') = g(x',y')$ and $h(1,x,x',y,y') = g(x,y)$ . Question: Is it true that $h(\alpha,x,x',y,y')$ always lies between $g(x,y)$ and $g(x',y')$ for all $x,x',y,y',\alpha$ in $[0,1]$ ?","['inequality', 'functions']"
3579091,How to determine if the line between two points in 3D is intercepted by a sphere,"This is a simple question, but my geometry is a little rusty. If I have two points that lie outside of a sphere in 3D space, and I am given the X, Y, and Z coordinates for them, how do I determine if the line that intercepts those points is intercepted by a sphere with a center at the origin and a given radius r? Thanks!",['geometry']
3579135,Why is it enough for a Tangent Plane to only agree with the slope of an $m$-input function in $m$ directions?,"Thanks for reading. Short Version Why is it enough for the tangent plane to only agree with the slope of a differentiable 2-input function in the $x$ and $y$ directions at a certain point for it to agree in every direction at that point? Isnât it still possible for the function to cross the tangent plane, even if their slopes agree in the $x$ an $y$ directions, when we move in some other direction? Long Version Say we have some two-input differentiable function $f(x,y)$ defining some surface in 3D space. The input into the function is $(x,y)$ coordinates, and the output of the function is the height of the surface (or hill) at each input point. Consider some point $(x_0,y_0)$ on this surface. At this point, the surface will have an infinite number of slopes, since there are an infinite number of directions we can move in. However, for two of those directions, the slopes are easy to find: The slope is $\frac{\partial f}{\partial x}$ in the $x$ direction. The slope is $\frac{\partial f}{\partial y}$ in the $y$ direction. For a plane to be tangent to a function at $(x_0,y_0)$ , it means it only touches the surface at that point, and doesn't cross the surface anywhere. When we find a tangent plane to the surface at $(x_0,y_0)$ , its enough to make the slope of the plane agree with the slope of the surface in 2 directions. Usually the $x$ and $y$ directions. I'm aware that two slopes, one in the $x$ and one in the $y$ , uniquely define a single plane at that point. However, there are an infinite number of functions that may have those slopes at that point. Tangency implies that the slope of the tangent plane agrees with the slope of the function at that point in every direction, not just the $x$ and $y$ directions. If it didn't agree with the slope of the function at that point in some arbitrary direction, then were we to draw lines in that direction on both the ""tangent"" (not actually tangent) plane and the function, those lines would intersect, and thus the plane wouldn't be brushing the surface at only a single point, and it wouldn't be a tangent plane. Why is it enough for the tangent plane to only agree with the slope of a differentiable 2-input function in the $x$ and $y$ directions at a certain point for it to agree in every direction at that point? Isnât it still possible for the function to cross the tangent plane, even if their slopes agree in the $x$ an $y$ directions, when we move in some other direction? I'd like to be able to see the answer intuitively...but I can't, and I need help. Thank you! Edit: I added this as a comment to David's answer below, but I'll add it as part of the question too, as I think it may help communicate what I'm trying to ""see"". Take the function $f(x)=x^2$ . That's a parabola in 2D space. If a tangent line agrees with the slope $2x$ at a certain point...well, there's only one slope to agree with, so I KNOW that the line will be tangent. But, now take the surface $f(x,y)=x^2$ . That's a surface in 3D space. A tangent plane at a certain point must have a slope of $2x$ in the $x$ direction and $0$ in the $y$ direction Lines drawn on a certain ""tangent"" plane in those two directions will be tangent to the surface at that point, since the slopes agree in those two directions. We used those directions (the $x$ and $y$ directions) to define the plane. But, how can I intuitively ""see"" that lines drawn on the plane in ANY direction will be tangent to the function at that point? Edit 2 As someone pointed out in the comments, a tangent line doesn't not crossing the function only applies if the function is convex (or concave) like $x^2$ . Here, let me draw out the function, and draw a tangent line to $f(x)=x^2$ . The tangent line (blue) agrees with the slope of the function (red) in the one direction it has to agree with the slope in (the $x$ direction). Therefor, I know that the tangent line won't cross the parabola. If it didn't agree with the slope, then it would cross it either when we moved a little bit towards the right from the point of tangency, or a little bit towards the left. However, lets say we draw a tangent plane the graph of $f(x,y)=x^2$ , with a tangent plane drawn to it. Although I know lines on the plane won't cross the surface when moving in the $x$ direction or in the $y$ direction (the two white tangent lines won't cross the $x^2$ surface) since the plane agrees with the slope of the surface in those two directions, how can I know that if I draw lines on the plane in some other arbitrary direction, those lines also won't cross the surface? Why was the slope agreeing in the $x$ and $y$ directions enough to define the tangent plane? Thanks! Final Thoughts: After watching some of Ted's (in the comments) lectures on YouTube, I've realized that when I asked this question, although I had an intuitive ""feel"" for differentiability in one dimension, I hadn't really thought enough about what it meant in higher dimensions. Differentiability (for a two dimensional surface) means that the function is locally flat. Say that $T(x â )$ is the tangent plane to a function $f(x â )$ at a point $a â$ . Therefor, it must agree with the slopes of $f(x â )$ in every direction at $a â$ , not just the $m$ Cartesian directions, for which we have the partial derivatives of $f(x â )$ . However, when we define $T(x â )$ , we only make it agree with the slopes of $f(x â )$ in the $m$ Cartesian directions. $$T(x â )=\frac{âf}{âx_0} (x_0-a_0 )+\frac{âf}{âx_1}(x_1-a_1 )+â¯+\frac{âf}{âx_m}(x_m-a_m )$$ (Each of the $x_i$ 's are orthogonal Cartesian coordinates, they're the components of the input vector $\vec{x}$ , and each of the partial derivatives above are evaluated at $\vec{a}$ ). This is because given the $m$ partial derivatives of $f$ at $\vec{a}$ , thereâs only one unique hyper-plane approximation for $f(x â )$ which can be defined!! That is, the $m$ partial derivatives of $f(x â )$ at $a â$ define a unique hyper-plane. There is no other hyper-plane that has $m$ slopes and passes through $(\vec{a}, f(\vec{a}))$ in $(m+1)$ dimensional space ( $m+1$ because we're adding one more dimension for the function's output itself) . My original question was why this implied that the tangent plane agreed with the slopes in every direction. The answer? It doesn't! It's entirely possible for this ""tangent"" (in quotations because in this case, it's not actually tangent) to agree with the slopes in the $m$ Cartesian directions, but not in every direction (Ted has some good examples in the lectures referenced in the comments, and here's a picture from ""Math-Insight"") . However, if that unique âtangentâ plane doesnât agree with the functionâs slopes in every direction (not only the Cartesian directions) that just means that the function isnât differentiable at $a â$ !!! In other words, at $\vec{a}$ the function isn't locally flat. To prove that a function is differentiable at $\vec{a}$ , we attempt to build a ""tangent"" plane at that point by making it agree with the slope of our function in the $m$ Cartesian directions, and then show that the ""tangent plane"" indeed is a tangent-plane by showing that... $$\mathrm{lim_{(|dx â |â0)}}(\frac{â¡[(f(a â+dx â )-f(a â )]-[T(a â+dx â )-T(a â )])}{|dx â |})=0$$ ...regardless of the direction in which we move from $\vec{a}$ (regardless of the direction of $\vec{dx}$ ) . In his videos, Ted shows an example of this (with the difference that in his example, $T(\vec{x})$ is not a tangent plane but a linear approximation passing through the origin, although the idea is the same.) In a nutshell, to answer my original question, if the function is differentiable, then itâs enough for the tangent plane to agree with the slope of the function in the $m$ Cartesian directions for it to agree in every direction, because $m$ slopes define a unique tangent-plane in $m+1$ dimensional space, and that the tangent plane agrees with the slopes of the function in every direction is just the definition of differentiability - that the function is locally flat. And to show that a function indeed is differentiable, we have to show that a potential unique ""tangent"" (in quotes because it may not be a tangent plane if the function isn't differentiable) plane to the function at a certain point that agrees with the slope of the function in the $m$ Cartesian directions agrees with the slope of the function in every direction, so that the function indeed is locally flat. Thank you!","['geometry', 'multivariable-calculus', 'calculus', 'vector-analysis', 'differential-geometry']"
3579136,How is KolmogorovâSmirnov statistic non-constant?,I'm reading about KolmogorovâSmirnov test . The KolmogorovâSmirnov statistic is given by $$D_{n}=\sup _{x}\left|F_{n}(x)-F(x)\right|$$ It seems to me that $D_n$ is a constant for each $n$ . Could you please elaborate on how $D_n$ is non constant?,"['probability-limit-theorems', 'statistics', 'probability-distributions', 'hypothesis-testing']"
3579241,Mayer-Vietoris Sequence for Cohomology with Supports,"I am working on problem III.2.4 in Hartshorne, and I have been quite stuck in showing the existence of a Mayer-Vietoris sequence for cohomology with supports. To be more precise, I have $Y_1,Y_2\subseteq X$ closed subsets and I want to show a long exact sequence $$ \cdots \to H^i_{Y_1\cap Y_2}(X,\mathscr{F})\to H^i_{Y_1}(X,\mathscr{F})\oplus H^i_{Y_2}(X,\mathscr{F})\to  H^i_{Y_1\cup Y_2}(X,\mathscr{F})\to\cdots.$$ I intend to do this by showing that there is an exact sequence $$ 0\to \Gamma_{Y_1\cap Y_2}(X,\mathscr{F})\to \Gamma_{Y_1}(X,\mathscr{F})\oplus\Gamma_{Y_2}(X,\mathscr{F})\to \Gamma_{Y_1\cup Y_2}(X,\mathscr{F})\to 0$$ and then using it to extract a long exact sequence on relative cohomology. Showing exactness at the first and second positions is not very hard. I am stuck trying to show surjectivity of $\Gamma_{Y_1}(X,\mathscr{F})\oplus\Gamma_{Y_2}(X,\mathscr{F})\to \Gamma_{Y_1\cup Y_2}(X,\mathscr{F})$ . I have tried a lot of acrobatics to construct for a given $s\in \Gamma_{Y_1\cup Y_2}(X,\mathscr{F})$ a pair $(s_1,s_2)\in \Gamma_{Y_1}(X,\mathscr{F})\oplus\Gamma_{Y_2}(X,\mathscr{F})$ so that $s_1-s_2=s$ , but to no avail. It occurred to me that I might want to solve the flasque case first, but even the flasque assumption hasn't helped. I'd really appreciate a nudge in the right direction.","['algebraic-geometry', 'homology-cohomology', 'sheaf-cohomology', 'sheaf-theory']"
3579245,Proofs of divisibility [duplicate],"This question already has answers here : Proof of $\gcd(a,b)=ax+by\ $ [Bezout's identity] (2 answers) Closed 3 years ago . I have the following question for homework. Fix positive integers $a$ and $b$ .  Hereâs an inductive definition of
a set $S$ : Foundation rule: $a,b â S$ . Constructor rule: If $m,n â S$ , then $m â n â S$ . (a)  Suppose $h$ is a common factor of $a$ and $b$ .  Use the exclusion
rule to prove that for every $n â S$ , $h$ divides $n$ . (b)  Suppose $k â S$ is  a  positive  integer  which  is  not  a  factor  of $a$ . Prove that
there is some $l â S $ such that $0 < l < k$ . (Hint:  Consider the
sequence $a,a â k,a â 2 k,...$ and use the fact that $\mathbb{N}$ is well-ordered.) (c)  In  the  same  way  that  you  proved  (b),  we  may  also  prove
the following fact:  if $k â S$ is a positive integer which is not a
factor of $b$ , then there is some $l â S$ such that $0 < l < k$ . Use (b)
and the above fact to prove that there is some positive integer in $S$ which is a common factor of $a$ and $b$ . (Hint:  Use the fact that $\mathbb{N}$ is
well-ordered.) (d)  Use (a) and (c) to conclude that S contains gcd( $a,b$ ). I'm unsure about how to even start (a) and (b). For (a) I thought something along the lines of ""because $h$ is a common factor of $a$ and $b$ , as $n â S$ , $h$ must be a divisor of $n$ "" yet apparently this is quite far off what we are supposed to do. I have literally no idea how to even start (b). Any help would be appreciated.","['elementary-number-theory', 'divisibility', 'discrete-mathematics']"
3579265,Find the maximum value of a sum of cosines given certain condition,"In my calculus class, I've come across this problem when we were on the topic of Jensen's Inequality: \begin{multline}A=\{\cos(x_1)\cos(x_2)\dots\cos(x_n)\in\Bbb{R}:\\n\in\Bbb{N},x_1^2+...+x_n^2=1\}.\end{multline} We are tasked with finding $\sup A$ .
I have tried the obvious approach of writing $$\cos(x_1)\cos(x_2)\dots\cos(x_n)=e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)}$$ and then trying to find an achievable upper bound for the value of $$\ln(\cos(x_1))+\dots+\ln(\cos(x_n)$$ by using Jensen's inequality for the concave function $\ln(\cos(x))$ in hope that those pesky squares
could be dealt with using $$\sqrt{\dfrac{a_1^2+...+a_n^2}{n}}\geqslant\dfrac{a_1+...+a_n}{n}$$ but so far that didn't help.
I am quite convinced that the answer will be $\dfrac{1}{\sqrt{e}}$ since that is what $e^{\ln(\cos(x_1))+\dots+\ln(\cos(x_n)}$ approaches from below when $x_1=x_2=...=x_n$ and $n$ goes to $\infty$ , but can't prove it.
Using different methods I've managed to show that the answer is smaller than $e^{\cos(1)-1}$ which is just barely larger than $\dfrac{1}{\sqrt{e}}$ , but in proving so I have used inequalities with different equality conditions. Any help would be appreciated as I feel I am missing something I should definitely find out by now. By the way, no integrals allowed :) Yay early calc","['jensen-inequality', 'maxima-minima', 'multivariable-calculus', 'products', 'optimization']"
3579311,On a certain discrepancy measure between probability distributions on the symmetric group of permutation $\mathfrak S_n$,"Let $\mathfrak S_n$ be the symmetric group of permutations on $n$ objects and let $P$ and $Q$ be a probability distributions on $\mathfrak S_n$ (i.e $P$ and $Q$ are points on the $n!$ -simplex). For $1 \le i < j \le n$ , let $p_{ij}$ be the probability that a random permutation $\sigma$ drawn from $P$ ranks $j$ ahead of $i$ , i.e satisfies $\sigma(i) < \sigma(j)$ . Consider the quantity $\Delta(P,Q) := \sum_{1 \le i < j \le n}|p_{ij}-q_{ij}|$ . Question. Is it possible to reasonably upper-bound $\Delta(P,Q)$ in terms of some distance (e.g total variation) between $P$ and $Q$ ?","['discrete-geometry', 'geometric-probability', 'symmetric-groups', 'probability-theory', 'probability']"
3579341,Value of a and b in a function if its local minimum is at a certain point?,I have a function as written below: $$ g(x) = x^3 + ax^2 + bx $$ It is known to have a local minimum at $ x = -\frac{1}{\sqrt{3}} $ whose value is $ y = -\frac{2\sqrt{3}}{9} $ I have tried using the second derivative test and I got stuck at $ -\frac{6}{\sqrt{3}} + 2a > 0 $ (second derivative at c is > 0) and $ 1 -\frac{2a}{\sqrt{3}} + b = 0 $ (first derivative equals to 0) Any ideas?,"['systems-of-equations', 'applications', 'analysis', 'maxima-minima', 'derivatives']"
3579346,Abstract definition of tangent space,"I've been learning some introductory analysis on manifolds and have had a small issue ever since the notion of tangent spaces at points on a differentiable manifold was introduced. In our lectures, we began with the definition using equivalence classes of curves. But it is also possible to define tangent spaces using derivations of smooth functions (and apparently several other ways too, but for now I'm only familiar with these two). It seems intuitively sensible to call both these pictures (the curve and derivative ones) ""equivalent"": let the point of interest be $p$ and pick a local chart $\phi$ . Then we form a quotient of the set of curves through $p$ (parametrized so that $p=\gamma(0)$ ), declaring $\gamma_1\sim\gamma_2$ iff $(\phi\,\circ\,\gamma_1)'(0)=(\phi\,\circ\,\gamma_2)'(0)$ . This is one particular version of a tangent space at $p$ . But we could also define it as the space of derivations, i.e. linear maps from $C^\infty(M)$ to $\mathbb{R}$ satisfying the Leibnitz rule $$D(fg)=D(f)g(p)+f(p)D(g)$$ For any equivalence class of curves $[\gamma]$ at $p$ , the operator defined on $C^\infty(M)$ by $$
D_{[\gamma]}(f)=(f\circ\gamma)'(0)
$$ is a derivation; conversely, it is true that every derivation is such a directional derivative (proof: Equivalence of definitions of tangent space ). Most of this a recap of a part of Wikipedia . At any rate, both of these notions seem to give in some sense ""the same"" tangent spaces. Here is my problem: I don't actually understand what precisely it is we are checking for when trying to decide if some two definitions are equivalent; right now, all I would personally try to do is show isomorphism of vector spaces and then try to convince myself that this isomorphism respects some vague notion of direction. But then $\mathbb{R}^{\mathrm{dim}(M)}$ is certainly isomorphic to any tangent space of the manifold $M$ , at least as a vector space. Nevertheless, just declaring $T_pM=\mathbb{R}^{\mathrm{dim}(M)}$ doesn't strike me as a successful construction of a tangent space. Now, there are two levels to my question, ordered by ""degree of abstraction"", so to speak (presumably they also get harder to answer). I do, however, believe they are connected. First, is there some precise notion of vector space isomorphisms respecting direction on a manifold? Specifically, is $\mathbb{R}^{\mathrm{dim}(M)}$ a valid tangent space or is it not, or do I perhaps have to specify some additional structure on it and then check that the additional structure relates to, say, the curve definition in a correct way? (I suppose this last case would require taking one definition of the tangent space as the absolute foundation and comparing all others to it, which I find somewhat unsatisfying.) Second, is there perhaps an abstract, ""external"" definition of a tangent space? What I'm talking about could be something like, ""Given a smooth manifold $M$ , a point $p\in M$ and a vector space $V$ , this vector space is called a tangent space at $p$ if it satisfies some properties $X,Y,Z...$ "" where these $X,Y,Z$ don't depend on the type of objects in $V$ or other particular details specific to $V$ . The motivation behind asking this is related to the situation with ordered pairs of objects (yes, this is quite a leap): I can use the Kuratowski definition or infinitely many others, and in each case, I will be able to eventually convince myself that, indeed, this thing before me works just as well to encode ""ordered-ness"" of objects as any other. But I don't have to keep referring to one of these specific cases, I just need to describe how pairs should arise and behave in general: there is a two-place function $f$ that sends two objects $x$ and $y$ to $(x,y)$ and there are two projections $\pi_1,\pi_2$ that pull $x$ and $y$ back out. (For a precise definition see this PDF , I summarised the discussion from there. It goes on to define products also within category theory.) Furthermore, I would find it highly suspect if some theorem about ordered pairs referred to the particulars of the Kuratowski definition - all the relevant information about $(x,y)$ should be recoverable from just the abstract setup described above (or better yet, in the linked PDF). Is there some way of treating tangent spaces in this same spirit? I know this question is vague, but I honestly don't know how better to phrase it, I hope I've at least gotten the mindset across if nothing else.","['smooth-manifolds', 'differential-geometry']"
3579445,Computational issues with the Runge Katta method for ODE first order,"Since the corona virus there are no office hours hence I can only come here for help and the internet got hacked so school email is down. Plus its even harder to get feedback through an email. I am not sure why my answer is off. I am not sure how many times I have to do the method.This took an hour for me to type up because I caught a mistake in my first steps so I had to type this up while doing it on a calculator $\frac{dy}{dx}=y+1$ where $y(0)=1$ $h=.25$ on the interval $[0,.5]$ round to 5 decimal places The formulas are: $k_1=f(x_n,y_n)$ $k_2=f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1)$ $k_3 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_2)$ $k_4 = f(x_{n+1},y_nhk_3)$ $y_{n+1} = y_n+\frac{h}{6}[k_1+k_2+2k_3+k_4]$ Okay here we go: $k_1 = f(x_n,y_n)=f(0,1)=1+1=2$ $k_2 = f(x_n+\frac{1}{2}h,y_n+\frac{1}{2}hk_1)=f(0+\frac{1}{4}\frac{1}{2},1+\frac{1}{2}\frac{1}{4}2)=f(0 + \frac{1}{8}, 1+\frac{1}{8}2)=f(\frac{1}{8},1+.25) = 1+.25=1.25$ $k_3 = f(x_n + \frac{1}{2}h, + y_n + \frac{1}{2}hk_2)=f(0+\frac{1}{2}\frac{1}{4},1+\frac{1}{2}\frac{1}{4}1.25)=f(0+\frac{1}{8},1+\frac{1}{8}1.25)=f(\frac{1}{8},1+.15625)=1+1.15625=2.15626$ $k_4=f(x_n+1,y_nhk_3)= f(.25,1+\frac{1}{4}2.15626)=f(.25,1+.53907)=1+1.53907=2.53907$ $y_{n+1}= y_n + \frac{h}{6}[k_1+2k_2+2k_3+k_4]=1 +\frac{.25}{6}[2+[2(1.25)]+[2(2.15626)]+2.93507]=1+.04167[2+2.5+4.31252+2.93507]=1+.41607[11.74759]=1+.48952=1.48952$ so we have $y_1=1.48952$ Now I assume I do the whole thing over with $y(.25)=1.48952$ $k_1= f(.25,1.48952)=1+1.48952=2.48952$ However I run into the problem that the answer is supposed to be $2.29740$ ...I am already over the answer on my first iteration.....I assume that I am supposed to do the two iterations stop at $y_2$ I don't understand what I am doing wrong...","['runge-kutta-methods', 'solution-verification', 'ordinary-differential-equations']"
3579450,Finite Rings and Product of Finite Fields,"Here is the problem which I got inspired from, which is from one of our school's prelim exam: Prove or disprove: If $R$ is a finite, commutative ring with unit, then it is a product of fields. It is obviously true if $R$ has prime order (because in this case $R$ itself is a field). Now it remains the case with non-prime order. Since $R$ is finite order, if it can be written as a product of fields, then all the fields must be finite. By considering the total number of elements, it is natural to consider the prime factorization of the order of $R$ . In order to see the statement is true or false, I consider first as an example the ring $R=\mathbb{Z}/6\mathbb{Z}$ . If it is a product of finite fields, then it can only be $F=\mathbb{F}_2\times\mathbb{F}_3$ . It turns out the function $\phi:R\to F$ given by $\phi(0)=(0,0)$ , $\phi(1)=(1,1)$ , $\phi(2)=(0,2)$ , $\phi(3)=(1,0)$ , $\phi(4)=(0,1)$ , $\phi(5)=(1,2)$ gives an isomorphism. In fact, it is THE isomorphism. Given the example above, this makes me believe that the statement is true. However, problems continue to arise as I attempted to prove it: (1) Given any finite commutative unital ring with order $n$ , is it true that $R\cong \mathbb{Z}/n\mathbb{Z}$ ? (2) We can consider two ""types"" of factorization and the proofs will go differently: If we let $n=p_1\cdots p_k$ be the prime factorization of $n$ , then we consider the field product $\mathbb{F}_{p_1}\times\cdots\times \mathbb{F}_{p_k}$ . If we let $n=p_1^{n_1}\cdots p_k^{n_k}$ to be the prime factorization and requiring all $p_i$ are distinct, then we can consider the field product $\mathbb{F}_{p_1^{n_1}}\times\cdots\times \mathbb{F}_{p_k^{n_k}}$ (3) How do one even construct an isomorphism? The one shown in our example seems to have no pattern at all. If the statement is actually false, I would like to see some counterexamples. Further than that, what extra condition is necessary (and sufficient) for the statement to be true?","['ring-theory', 'finite-fields', 'abstract-algebra', 'commutative-algebra']"
3579462,Alternating sum of positive integers,"Suppose $A = (a_n) = (a_1, a_2, a_3, . . .)$ is an positive, increasing sequence of integers. Define an $A$ - expressible number $c$ if $c$ is the alternating sum of a finite subsequence of $A.$ To form such a sum, choose a finite subset of the sequence $A,$ list those numbers in increasing order (no repetitions allowed), and combine them with alternating plus and minus signs. We allow the trivial case of one-element subsequences, so that each an is $A-$ expressible. Definition. Sequence $A = (a_n)$ is an âalt-basisâ if every positive integer is uniquely $A-$ expressible. That is, for every integer $m > 0,$ there is exactly one way to express $m$ as an alternating sum of a finite subsequence of $A.$ Examples. Sequence $B = (2^{nâ1}) = (1, 2, 4, 8, 16, . . .)$ is not an alt-basis because some numbers are B-expressible in more than one way. For instance $3 = â1 + 4 = 1 â 2 + 4.$ Sequence $C = (3^{nâ1}) = (1, 3, 9, 27, 81, . . .)$ is not an alt-basis because some numbers (like 4 and 5) are not C-expressible. Can some sequence $\{E\}$ with first term $1$ and second term $4$ be an alt-basis? What terms would this sequence include? What about another sequence $\{F\}$ with first term $2$ and second term $3$ ? What terms would this sequence include?","['number-theory', 'elementary-number-theory', 'discrete-mathematics', 'sequences-and-series', 'algebra-precalculus']"
3579474,On descending chain of ideals with zero intersection in a complete semi-local ring,"Let $R$ be a Noetherian semi-local ring, let $\mathfrak m_1,...,\mathfrak m_n$ be the finitely many maximal ideals. Let $J=\mathfrak m_1\cap ...\cap \mathfrak m_n$ denote the Jacobson radical of $R$ . Also assume $R$ is $J$ -adically complete. If $\{I_n\}_{n\ge 0}$ is a descending chain of ideals in $R$ such that $\cap_{n\ge 0} I_n=(0)$ , then how to prove that there exists a function $f: \mathbb N \to \mathbb N$ such that $\lim_{n\to \infty} f(n)=\infty$ and $I_n \subseteq J^{f(n)}, \forall n\ge 1$ ? Here $\mathbb N$ denotes the set of non-negative integers.","['formal-completions', 'algebraic-geometry', 'commutative-algebra']"
3579478,Ãtale fundamental group acts by similitude on Tate modules of abelian variety,"Note : the below question doesn't make sense. I have mistakenly mixed two different $\pi_1$ and came up with a false claim. See my last comment. Let $X$ be an abelian variety over an algebraically closed field $k$ of characteristic $p>0$ . We fix a polarization $\lambda$ of $X$ , of degree prime to $p$ . This polarization induces the Weyl pairings $X[N]\times X[N]\rightarrow \mu_N$ on the $N$ -torsion subgroups of $X$ , for any integer $N$ . These pairings are alternate (symplectic when $N$ is prime to the degree of $\lambda$ ). Taking for $N$ the successive powers $l^m$ of a prime number $l\not = p$ , these pairings are compatible with eachother and taking limit, tensoring with $\mathbb Q_l$ , we obtain an alternate pairing $E_l:V_l(X)\times V_l(X)\rightarrow \mathbb Q_l(1)$ where $V_l(X)=T_l(X)\otimes_{\mathbb Z_l} \mathbb Q_l$ is the rational Tate-module. Now, we have natural identifications of $T_l(X)$ (resp. $V_l(X)$ ) with the Ã©tale homology group $H_1(X,\mathbb Z_l)$ (resp. $H_1(X,\mathbb Q_l)$ ). Taking the restricted product of these groups for $l\not = p$ , we obtain the group $H_1(X,\mathbb A^p_f)$ where $\mathbb A^p_f$ is the ring of finite adÃ¨les away from $p$ , which is equipped with an alternate pairing to $\mathbb A_f^p(1)$ . Eventually, the Ã©tale fundamental group of $X$ is given by $\pi_1(X)=\prod_{l\text{ any prime}}T_l(X)$ . This group acts on $H_1(X,\mathbb A^p_f)$ by addition componentwise, forgetting about the $p$ -component. In Kottwitz paper ""Points on some Shimura varieties over finite field"" in chapter $5$ , it is claimed that $\pi_1(X)$ acts by similitudes on $H_1(X,\mathbb A^p_f)$ . I fail to see why it is true, and I need help for this purpose. If I interpret the statement componentwise, I believe it should mean that for any $l\not = p$ , there exists some scalar $c\in \mathbb Q_l^{\times}$ such that for any $x,y\in V_l(X)$ and $a\in T_l(X)$ , $E_l(x+a,y+a)=cE_l(x,y)$ in $\mathbb Q_l(1)$ . I can't not see why this would hold... Edit: I believe that my interpretation in the last paragraph must be totally wrong. If I consider $l$ prime to the degree of $\lambda$ , then the pairing $E_l$ is symplectic. Assuming that my last statement were true, first I see that $c\not = 1$ : otherwise I would have $E_l(x,a)+E_l(a,y)=E_l(x-y,a)=0$ for every $x,y$ and $a$ as above, which for a fixed $a$ implies $a=0$ , that is absurd.  But if $c\not = 1$ , simply taking $a=0$ gives an absurdity. This adds up to my confusion, I would gladly appreciate any explanation about how I ought to understand ""by similitudes"" here.","['fundamental-groups', 'algebraic-geometry', 'arithmetic-geometry', 'abelian-varieties']"
3579480,Looking for a book that picks up where Understanding Analysis by Abbott left off?,"I'm currently going through the Understanding Analysis text by Abbott and was interested in what typically comes after once I finish going through this book. Would multivariable analysis of some sort come next? If so, what book would you recommend? Or is there more ""single-variable"" analysis left to be done? Edit: I noticed the downvote, would appreciate any advice on how I can improve my answer.","['book-recommendation', 'reference-request', 'analysis', 'real-analysis']"
3579498,"Closed form of $\int_0^1\frac{W_0(-t/e)}{W_{-1}(-t/e)} \,dt$","$\require{begingroup} \begingroup$ $\def\e{\mathrm{e}}\def\W{\operatorname{W}}\def\Wp{\operatorname{W_0}}\def\Wm{\operatorname{W_{-1}}}\def\Ei{\operatorname{Ei}}$ Is there a known closed form for the integral \begin{align}	
I&=\int_0^1
\frac{\Wp(-\tfrac t\e)}{\Wm(-\tfrac t\e)} 
\,dt
\approx 0.151216902884937
\tag{1}\label{1}
,
\end{align} where $\Wp,\Wm$ are two real branches of the Lambert $\W$ function? An alternative form of \eqref{1} is \begin{align}	
I&=\e\cdot\!\!\int_0^1
\frac{\sqrt[1-t]{t}(1-t+t\,\ln t)(t-1-\ln t)}{(1-t)^3}
\, dt
\tag{2}\label{2}
.
\end{align} Using series expansion of $\Wp$ it can be expressed in terms of the infinite sum: \begin{align}
I&=\e-2-
\e\cdot\sum_{n=1}^\infty
\frac{\Gamma(n+2,n+1)}{\Gamma(n+2)\,n^3\,(1+\tfrac1n)^{n+1}}
\tag{3}\label{3}
.
\end{align} Also, the closed form of \eqref{1} 
can be found, using closed form of either \begin{align}
I_2&=\int_0^1 \left(-\Wp(-\tfrac t\e)-\frac1{\Wm(-\tfrac t\e)}\right)^2\, dt
\approx 0.62200121658
\\
\text{or }\quad 
I_3&=\int_0^1 \left(-\Wp(-\tfrac t\e)+\frac1{\Wm(-\tfrac t\e)}\right)^2\, dt
\approx 0.01713360504
,
\end{align} or both, since \begin{align}
I_2+I_3&=
20+4\,\e\,(\Ei(1,1)-2)
\approx 0.639134821620414414482
,
\end{align} where \begin{align}
\Ei(1,1)&=\int_1^\infty \frac{\exp(-t)}t \, dt
\approx 0.21938393439552
.
\end{align} Any ideas? $\endgroup$","['integration', 'definite-integrals', 'closed-form', 'lambert-w']"
3579558,How do I establish the differentiability of this function by definition?,"Given this function $f(x) = (x^2 + 1)^{\sin(x)} $ How would I establish its differentiability over the entire function? I understand how to establish differentiability at a point $c$ , by assessing the limit of the different quotient at $x = c$ . But how would I establish the differentiability of the entire function?","['continuity', 'calculus', 'derivatives']"
3579564,"If $a \ge 0$ satisfies $\sin(\sqrt{x+a})=\sin(\sqrt{x})$ for all $x\ge 0$, what can we say about $a$?","Question: Let $a\ge 0$ be a constant such that $\sin(\sqrt{x+a})=\sin(\sqrt{x})$ for all $x\ge 0$ . What can you say about a? Justify your answer. Solution: Obviously $a=0$ is a solution to the identity $$\sin(\sqrt{x+a})=\sin(\sqrt{x}), \forall x\ge 0.$$ Now let us assume that $\exists a>0$ , such that $$\sin(\sqrt{x+a})=\sin(\sqrt{x}), \forall x\ge 0.$$ Since we have $\sin(\sqrt{x+a})=\sin(\sqrt{x})$ for all $x\ge 0$ and for some constant $a>0$ , this implies that if we fix $x$ , then we can conclude that $$\sqrt{x+a}=n\pi+(-1)^n\sqrt{x}\text{, where $n$ is an integer such that }\sqrt{x+a}>0.$$ Squaring both sides we have $$x+a=n^2\pi^2+(-1)^n2n\pi\sqrt{x}+x\\ \implies a=n^2\pi^2+(-1)^n2n\pi\sqrt{x}\hspace{0.5 cm}...(*)$$ $(*)$ helps us in concluding that $a$ is a function of $n$ and $x$ and it is not a constant, and more specifically we have $$a(n,x)=n^2\pi^2+(-1)^n2n\pi\sqrt{x},$$ which is a contradiction to our assumption that $a>0$ is a constant. Thus $a=0$ is the only possible solution to the identity stated in the question. Is my solution correct and is there any alternative method to solve the problem?","['trigonometry', 'solution-verification', 'real-analysis']"
3579587,"For non-negative reals such that $a+b+c\geq x+y+z$, $ab+bc+ca\geq xy+yz+zx$, and $abc\geq xyz$, show $a^k+b^k+c^k\geq x^k+y^k+z^k$ for $0<k<1$","Let $a$ , $b$ , $c$ , $x$ , $y$ , $z$ be non-negative real numbers such that $$a+b+c \geq x+y+z,$$ $$ab+bc+ca \geq xy+yz+zx,$$ $$ abc \geq xyz$$ Show that $$a^k+b^k+c^k \geq x^k+y^k+z^k, \quad 0 < k < 1$$ This is case $n=3$ of a problem posted by Ji Chen in the Art of Problem Solving forums, 2008. I have posted a partial result in an answer below. I have a proof when $r = \frac12,$ for weaker conditons $$a+b+c = x+y+z,$$ $$\min(x, y, z) \leqslant \min(a, b, c),$$ $$\max(a, b, c) \leqslant \max(x, y, z).$$ Indeed, if $u, v > 0$ it's easy check $$\sqrt{u} - \sqrt{v} \leqslant \frac{u-v}{2\sqrt{v}}.$$ Assume $x \geqslant y \geqslant z$ and $a \geqslant b \geqslant c$ then $x \geqslant a, \; c \geqslant z.$ Therefore $$\begin{aligned}\sqrt{x}+\sqrt{y}+\sqrt{z} - \sqrt{a} - \sqrt{b} - \sqrt{c} & \leqslant \frac{x-a}{2\sqrt{a}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& \leqslant \frac{x-a}{2\sqrt{b}}+ \frac{y-b}{2\sqrt{b}}+ \frac{z-c}{2\sqrt{c}} \\& =\frac{x+y+z-a-b-c}{2\sqrt{b}}-\frac{(c-z)(\sqrt{b}-\sqrt{c})}{2\sqrt{bc}} \leqslant 0.\end{aligned}$$","['multivariable-calculus', 'systems-of-equations', 'inequality']"
3579630,"Showing there is a symplectomorphism of $(T^{*} M, \omega_{T^{*}M})$ satisfying this property","Problem: Let $M$ be a manifold, and let $\alpha$ be a closed $1$ -form on $M$ . Show that there exists a symplectomorphism of $(T^{*} M, \omega_{T^{*} M})$ that maps the zero section to the submanifold $\text{Im}(\alpha) := \left\{ \alpha_x: x \in M \right\} $ of $T^{*} M$ . Attempt: The zero section of $T^{*} M$ is $$M_{0} = \left\{ (x, \xi) \in T^{*} M \mid \xi = 0 \ \text{in} \ T_x^{*} M \right\}. $$ My idea was to define the symplectomorphism $\phi$ as $\phi := \alpha \circ \pi$ , where $\pi : T^{*} M \rightarrow M$ is the projection. I have to show that $\phi$ is a diffeomorphism and that $\phi^{*} (\omega_{T^{*} M}) = \omega_{T^{*} M}$ ? However, this is where I am stuck. Also, I'm not sure where I need the closedness of $\alpha$ . Help is appreciated.","['differential-topology', 'symplectic-geometry', 'differential-geometry']"
3579670,Show $\sum_{k=0}^{n}2^{n-k}\binom{a+k}{k}\frac{a-k}{a+k}=\binom{a+n}{n}$,"How it can be shown that: $$\sum_{k=0}^{n}2^{n-k}\binom{a+k}{k}\frac{a-k}{a+k}=\binom{a+n}{n}$$ Where $a \ne 0$ My try: $$\sum_{k=0}^{n}2^{n-k}\binom{a+k}{k}\frac{a-k}{a+k}=\sum_{k=0}^{n}2^{n-k}\binom{a+k}{a}\frac{a-k}{a+k}$$ $$=\frac{1}{a}\sum_{k=0}^{n}2^{n-k}\binom{a+k-1}{a-1}\left(a-k \right)$$ $$=\sum_{k=0}^{n}2^{n-k}\binom{a+k-1}{a-1}-\frac{1}{a}\color{red}{\sum_{k=0}^{n}2^{n-k}\binom{a+k-1}{a-1}k }$$ On the other hand: $$\color{red}{\sum_{k=0}^{n}2^{n-k}\binom{a+k-1}{a-1}k}=\sum_{k=0}^{n}\left(a+k-1\right)2^{n-k}\binom{a+k-2}{k-1}  $$ But computing these expressions takes much time and I think there should be a better way, but I cannot find that way. Please if it's possible, then do the proof using elementary ways.","['summation', 'binomial-coefficients', 'discrete-mathematics']"
3579706,Probability regarding die faces,"Problem: Six faces of a cube are numbered randomly 1,2,3,4,5,6. The probability that faces 1 & 6, 6 & 3 and 3 & 1 will share an edge is: I guess the other way of asking this problem is ""probability that faces 1, 6 and 3 will share a corner"" and then it will be simply: $$ \frac{^3P_3}{^6P_3} = \frac{1}{20} $$ (Here, $^nP_r$ denotes permutations of $r$ things from $n$ total things) But, I was wrong. Any hints? The correct answer was: $\dfrac{2}{5}$","['combinatorics', 'probability']"
3579736,Representations of simple nonabelian groups,"In this post all groups are finite, and all representations are complex linear finite dimensional representations. If a group $G$ is abelian, then all of its irreducible representations are of one dimension. The converse is also true: if all of its irreducible representations are of one dimension, then that group is abelian. What interests me is to what extent can we detect nonabelianity from this information. To be more precise, I'd like to argue that nonabelian groups do not have $1$ -dimensional representations other than the trivial one. However, this is a false statement because the permutation group $S_3$ over $3$ elements has a nontrivial yet $1$ -dimensional representations. But that representation comes from induction from its normal subgroup $A_3$ ! Therefore my guess should be modify so that the group is simple. The simplest nonabelian simple group is $A_5$ . I also check $A_6$ and $A_7$ on Groupprop . Neither of them has nontrivial $1$ -dimensional representations. So is my guess true: Any nonabelian simple group has no nontrivial $1$ -dimensional representation?","['representation-theory', 'finite-groups', 'simple-groups', 'group-theory', 'abelian-groups']"
3579760,Value of $\arg(\sin \theta +i\cos \theta)$,"If $\displaystyle \frac{3+i\sin \theta}{4-i\cos \theta}$ is purely real number , where $\theta \in [0,2\pi].$ Then what is $\arg(\sin \theta +i\cos \theta)$ ? What I tried: \begin{align*}
\frac{3+i\sin \theta}{4-i\cos \theta} & =\frac{(3+i\sin \theta)(4+i\cos \theta)}{(4-i\cos \theta)(4+i\cos \theta)}\\
&=\frac{12-(\sin \theta\cos \theta)+i(4\sin \theta+3\cos \theta)}{16+\cos^2 \theta}\in \mathbb{R}
\end{align*} means $(4\sin \theta+3\cos \theta)=0$ , namely $\displaystyle \tan \theta = -3/4$ . So either $\theta\in (\pi/2,\pi)$ or $\theta\in(3\pi/2,2\pi)$ . Now $\arg(\sin \theta+i\cos\theta)=\arctan\left(\frac{\cos \theta}{\sin \theta}\right)=\arctan(\cot\theta)=-4/3$ , but the answer given as $\displaystyle \pi-\tan^{-1}(4/3)$ . How do I solve this? Help me please.","['trigonometry', 'complex-numbers']"
3579784,Finding solution of $\frac{e^x}{2} = 1 + x + \frac{x^2}{2!} + \dots + \frac{x^k}{k!}$,"I am looking to find a numerical solution to the equation: $$\frac{e^x}{2} = 1 + x + \frac{x^2}{2!} + \dots + \frac{x^{k-1}}{(k-1)!}$$ where $k$ is of the order of $10^7$ , and the solution is accurate to at least 2-3 decimals. It is not hard to show that the equation has exactly one positive solution (just differentiate and use induction). Roughly, it measures how good of an approximation is the Taylor expansion of $e^x$ upto $k$ terms. Thus, as $k$ increases, we can expect $x$ to increase as well, as the accuracy of the expansion increases farther from the origin. We can get an lower bound on $x$ by the Taylor's theorem on $e^x$ , as the remainder here is $\frac{e^x}{2}$ , so $\frac{e^x}{2} \leq \frac{e^x}{k!}x^k$ which implies $\sqrt[k]{\frac{k!}{2}} \leq x$ . Thus by Stirling's formula, we see that the lower bound is almost $\frac{k}{e}$ . My first idea was to use binary search, by simply computing the entire function and checking its sign. I coded it up in Python, but unfortunately, it only works for values of $k$ upto about $1000$ . Beyond that, the values get too big to fit in its numeric data type. I tried many other ways, like Newton's method which did not work out, amongst other things. The main problem here is I can't find a way to avoid computing the entire function (which will overflow). I have tried hard to solve it, but couldn't do it so hopefully you all can help.","['algebra-precalculus', 'approximation', 'numerical-methods']"
3579795,$k$-algebra morphisms from formal power series ring,"Background/Motivation: I was playing around with a certain construction that I am trying to generalize and therefore needed to compute some examples to get a feel for the situation. I realized that I don't really feel comfortable with $\text{Hom}_k(k[[t]],-)$ for some field $k$ , i.e. with $k$ -algebra morphisms from a formal power series ring. One of the first examples I considered led to the following question: The question: Let $k$ be an arbitrary field and consider the set of $k$ -algebra homomorphisms $\text{Hom}_k(k[[t]],k)$ . How do the elements look like? My thoughts: At first it feels like such a morphism is determined by the value of $t$ which cannot be since 1) The $k$ -algebra morphism doesnt allow us to commute with our infinite (formal) sums. 2) Then we would have $k[[t]] \cong k[t]$ as this is the universal property of the polynomial ring as free $k$ -algebra. So that is not what we are looking for.
We certainly have the $t \mapsto 0$ morphism, but if $t$ is not sent to $0$ , I am confused. Somehow it feels like the set of these morphisms is given (rather: can be identified) by all elements $a \in k$ , such that all these formals sums ""converge"" if I plug in $a$ . But now $k$ is not necessarily a topological field and hence talking about convergence does not seem to be the correct way of thinking. Thus ""converge"" should mean define an element in $k$ here, but I don't really know.","['formal-power-series', 'ring-theory', 'abstract-algebra', 'commutative-algebra']"
3579866,Identity related to $\sum_{k=0}^{n}\frac{x^k}{\binom{n}{k}}$,"How it can be shown that: $$\sum_{k=0}^{n}\frac{x^k}{\binom{n}{k}}=\left(n+1\right)\left(\frac{x}{x+1}\right)^{n+1}\sum_{k=1}^{n+1}\frac{1+x^k}{\left(1+x\right)k}\left(\frac{1+x}{x}\right)^{k}$$ for $x \ne-1$ I tried Additive Forms of Reciprocal Pascalâs Identity , but could not derive that.","['summation', 'binomial-coefficients', 'discrete-mathematics']"
3579939,"What does it mean that the ""resolvent mapping is analytical""?","This is taken directly from our lecture notes, where $H$ is a complex Hilbert space. Defintion. Let $T: H \supset \text{dom}(T) \subset H$ be densely defined. The set $\varrho(T) := \{\lambda \in \mathbb{C}: \lambda I - T: \text{dom}(T) \to H$ has a bounded inverse in $H$ } is the resolvent set of $T$ . The mapping $R: \varrho(T) \to L(H)$ , $R_{\lambda} = R(\lambda) := (\lambda I - T)^{-1}$ is called the resolvent mapping. Theorem. Let $T$ be as above. $\varrho(T)$ is open. The resolvent mapping is analytic and the resolvent identity $$
R_{\lambda} - R_{\mu} = (\mu - \lambda) R_{\lambda} R_{\mu}
$$ holds. Proof. 1. Let $\lambda_0 \in \varrho(T)$ and $| \lambda - \lambda_0 | < \| R_{\lambda_0} \|^{-1}$ . 
Then $$ \tag{1}
\lambda I - T
= (\lambda_0 I - T) + (\lambda - \lambda_0) I
= (\lambda_0 I - T) \underbrace{\left[ I - (\lambda - \lambda_0)(\lambda_0 I - T)^{-1}\right]}_{[\ldots]^{-1} = \sum_{n = 0}^{\infty} ((\lambda - \lambda_0)(\lambda_0 I - T)^{-1})^n}.
$$ Hence $\lambda I - T$ is invertible. A formal calculations yields $$
(\lambda I - T)(\mu I - T) \left[ ( \lambda I - T)^{-1} - (\mu I - T)^{-1}\right] = (\mu - \lambda) I
$$ and a similar identity can be derived by multiplying from the right side. So, only the (easy) inspection of domains is missing. $\square$ Questions What does it mean for the resolvent mapping to be analytic? Is it that we can write it as $R_{\lambda} = \sum_{n = 0}^{\infty} a_n T^n$ ? Does this follow from $(1)$ since $(1)$ implies $$
(\lambda I - T)^{-1}
= \sum_{n = 0}^{\infty} ((\lambda_0 - \lambda) (\lambda_0 I - T))^n \cdot (\lambda_0 I - T)^{-1}
= \sum_{n = 0}^{\infty} (\lambda_0 - \lambda)^n (\lambda_0 I - T)^{n - 1} \ ?  
$$ The wording at the beginning of the proof of 1. troubles me a little. We assume that $\varrho(T) \ne \emptyset$ , right? So would the exact wording of the beginning be: Let $\lambda_0 \in \rho(T)$ . Then there exists a $\lambda \in \mathbb C$ such that $| \lambda - \lambda_0 | < \ldots$ ?","['proof-explanation', 'spectral-theory', 'functional-analysis']"
