question_id,title,body,tags
1412732,An example of why $f(f^{-1}(B))\neq B$,"Let $f:X\rightarrow Y$ be a function and $B\subseteq Y$ a subset of $Y$ . I know (and have proven) that $f(f^{-1}(B))\subseteq B$ . I've also found an example where $f(f^{-1}(B))\neq B$ for $B= \mathbb{R}$ . I want to find another example, because I find my example very silly. Any help?","['elementary-set-theory', 'examples-counterexamples', 'functions']"
1412747,Do approximate identities remain approximate identities if one adjoins 1 to a C* Algebra?,"If we have a C* Algebra $\mathscr{U}$ without an identity we can adjoin an identity $\mathbb{1}$ in the following way: We take $\mathscr{\tilde U}$ to be the set $\{(\alpha,A); \alpha \in \mathbb{C}, A \in \mathscr{U}  \}$ On $\mathscr{\tilde U}$ we define addition in the obvious way and define multiplication via $(\alpha,A) \cdot (\beta,B) := (\alpha \beta,\alpha B + \beta A +AB)$. Also we can introduce a shorthand notation $(\alpha, A) := \alpha \mathbb{1} + A$ The norm of the new states is defined via:
$$\|\alpha\mathbb{1}+A\| := \sup\{\|\alpha B + AB\|\ ; BÂ \in \mathscr{U},\, \|B\|=1\}$$ The result will again be a C* algebra and $\mathscr{U}$ can be identified as the subset of elements in the form $(0,A)$ of $\mathscr{\tilde U}$. My question is whether an approximate identity on $\mathscr{U}$ will still have the property of being an approximate identity on the new C* Algebra. Is the proof easy?","['c-star-algebras', 'linear-algebra', 'functional-analysis']"
1412748,"Can we find uncountably many disjoint dense measurable uncountable subsets of $[0,1]$?","Can we find uncountably many disjoint dense measurable uncountable subsets of $[0,1]$? Obviously we may as well assume all the subsets have measure $0$. If I didn't specify the subsets were uncountable, we could easily find uncountably many dense disjoint measurable subsets $X_\alpha$ (all countable) by taking a Hamel basis $H$ of $\mathbb{R}$ as a vector space over $\mathbb{Q}$, where $H$ contains a non-zero rational number, and then set $X_\alpha = (\mathbb{Q} + \alpha) \cap [0,1]$ for every $\alpha \in H$. But what about if we want uncountable measurable dense subsets? I think it should still be that there are uncountably many that can be found that are disjoint, according to my intuition, but I don't know an ""easy"" way to show it like I can in the case of countable dense subsets.","['lebesgue-measure', 'real-analysis']"
1412761,Another integral related to Fresnel integrals,"How would we prove this result by real methods ? $$\int_0^{\infty } \frac{\sin \left(\pi  x^2\right)}{x+2} \, dx=\frac{1}{4} \left(\pi-2 \pi  C\left(2 \sqrt{2}\right)-2 \pi  S\left(2 \sqrt{2}\right)+2 \text{Si}(4 \pi ) \right)$$ As you can easily see, Fresnel integrals are involved. What are your ideas on it?","['calculus', 'special-functions', 'real-analysis', 'definite-integrals', 'integration']"
1412766,"$G$ be a finite group of order $n$ , $H$ be a proper subgroup of order $m$ such that $(n/m)!<2n$ ; $G$ is not simple","Let $G$ be a finite group of order $n$ , $H$ be a proper subgroup of order $m$ such that $(n/m)!<2n$ ; then how to show that $G$ is not simple ? I have proceeded by Cayley's theorem , $\ker f$ is normal and contained in $H$ ; so it is a proper normal subgroup of $G$ , we need moreover a non-singleton normal subgroup , which I am not able to get . Please help . Thanks in advance .","['group-theory', 'finite-groups']"
1412767,Number of polyhedron diagonals,"Suppose that I have a polyhedron with given number of faces, edges and vertices are given. Is there a formula that gives me the number of polyhedron diagonals, http://mathworld.wolfram.com/PolyhedronDiagonal.html ?",['geometry']
1412775,Does the limit $\lim\limits_{x\to0}\left(\frac{1}{x\tan^{-1}x}-\frac{1}{x^2}\right)$ exist?,Does the limit: $$\lim\limits_{x\to0}\frac{1}{x\tan^{-1}x}-\frac{1}{x^2}$$ exist?,"['limits', 'calculus', 'real-analysis', 'algebra-precalculus']"
1412776,Why does $\lim_{x\to 0} \frac {\sin (xy)}{x} \to y $?,"Let $f(x,y) = \frac{\sin (xy)}{x}$ for $x\neq 0$. How should you define $f(0,y)$ for $y\in \mathbb{R}$ so as to make $f$ a continuous function on all of $\mathbb{R}^2$? So in order for a function to be continuous the limit of the function approaching the point has to equal the value of the function at the point. In this case the trouble point is $(0,\,y)$. Now i looked at some other posts and took the suggestion of using L'Hopital's rule. Using it I get the right solution, but I don't understand why doing L'Hopital does provide the right solution? If i am in a multivariate environment, how am i allowed to use something like L'Hopital?","['continuity', 'multivariable-calculus']"
1412817,Find all pair of cubic equations,"Find all pair of cubic equations $x^3+ax^2+bx+c=0$ and $x^3+bx^2+ax+c=0$, where $a,b$ are positive integers and $c$ not equal to $0$ is an integer, such that both the equations have three integer roots and exactly one of those three roots is common to both the equations. I tried the sum and product relationships with the coefficients, but I have more variables than the number of equations.All I got was that the common root has to be $x=1$. What do I next?Thanks.","['cubics', 'quadratics', 'algebra-precalculus']"
1412827,When can a set have an upper bound but no least upper bound?,"So I'm taking real analysis and have noted that one of the benefits of the Dedekind cut is that 'if one of the sets made has an upper bound it also has a least upper bound'. I don't understand how a set can have an upper bound and no least upper bound, though. Is what can lead to this declaring a set in the rationals that is bounded above by an irrational number? I don't see any other way for this to be true (and I don't know why you'd ever make that set, or similarly why it's a 'special' property of the Dedekind cut rather than the general case). Thank you for your time,","['rational-numbers', 'analysis', 'real-numbers', 'real-analysis']"
1412848,$|G:H|=p^n$ means $O_p(H)\leq O_p(G)$?,"Let $H\leq G$ (finite group) and $|G:H|=p^n$, ($p$ is a prime number) prove that: $$O_p(H)\leq O_p(G)$$ note: $O_p(G)$ defined as the intersection of all Sylow-$p$ groups in $G$ I try to prove $G_p\cap H\in Sylow_p H$, however it's not obvious and maybe wrong. And the proposition is definitely true if $H$ is normal( if $H$ is normal, we don't need the condition: $|G:H|=p^n$).","['abstract-algebra', 'group-theory', 'finite-groups']"
1412867,Is there a way to visualize a group?,"Is there a way to picture a group in ones head? I want to ""see"" the difference between abelian and non-abelian group. And if $f$ is a group homomorphism, is there a way to see that $\ker(f)=1\Leftrightarrow f$ is injective? What about topological groups? All I see is a map between two spaces. Are there any easy to visualize examples to have in mind?","['visualization', 'group-theory', 'intuition']"
1412896,Is there a natural Riemannian structure on the total space of a vector bundle?,"Suppose $B$ is a Riemannian manifold and $\pi: E \to B$ is a smooth vector bundle equipped with a metric. Is there a natural Riemannian metric on $E$, i.e. a bundle metric on $TE\to E$? It seems like there should at least be metrics on $TE$ which restrict to the original Riemannian metric on $TB \subset TE$. I tried to show that the metrics on $E \to B$ and $TB \to B$ give a splitting of the short exact sequence $VE \to TE \to \pi^* TB$ but ended up swimming in symbols.","['differential-geometry', 'vector-bundles', 'riemannian-geometry']"
1412899,Is every axiom in the definition of a vector space necessary?,"Definition: A vector space over a field $K$ consists of a set $V$ and two binary operations $+: V \times V \to V$ and $\cdot: K \times V \to V$ satisfying the following axioms: Commutativity of $+$ . Associativity of $+$ . Existence of an identity element $\mathbf{0}$ for $+$ . Existence of inverses for $+$ . Compatibility of $\cdot$ with multiplication in $K$ . Distributivity of $\cdot$ over $+$ . Distributivity of $\cdot$ over addition in $K$ . $1_K$ is a left identity of $\cdot$ . Question: Are all seven of the previous axioms necessary (in the sense that weakening any one of them permits a structure which is not a vector space)? If not, which can be weakened (or removed)? EDIT: user7530 has quite cleverly shown that the commutativity of $+$ can be derived from axioms 2-8. Supposing we throw this out, can the remaining axioms all be proven necessary? EDIT 2: It was pointed out that axiom 3 cannot simply be thrown out, as the definition of an inverse in axiom 4 depends on the existence of $\mathbf{0}$ . What if we tweak the statement of axiom 4 to axiom 4': ""For every $x \in V$ , there exists $y \in V$ such that $(x+y)+x = x$ and $(y+x)+y = y$ ""? Is this weakened version equivalent to the original, and if so, does it allow the removal of axiom 3?","['abstract-algebra', 'vector-spaces', 'definition', 'axioms']"
1412921,Why do polynomial regressions have larger variance at the end?,"In reading the book ""An Introduction to Statistical Learning with Applications in R"", I came across this graph: It shows that the point-wise variance is larger at the ends of the regression curve. Why is that? I thought that the variance may be larger because there seem to be fewer data points near the end, but the variance is calculated on the coefficients so the number of data points used in the estimate has no impact. Then I thought that the variance is larger because the X values are larger, but the graph on the left side shows slight increases in variance on both sides (even when X is small). In general, I have read that polynomials have notorious end behaviours - what causes this? Thanks.","['covariance', 'polynomials', 'statistics', 'regression', 'linear-algebra']"
1412932,What's the sample space for a conditional expectation?,"Define a probability space $(\Omega,\cal F,\Bbb P)$ and a $\cal F$ measurable random variable $X$, the conditional expectation given a sub $\sigma$-algebra $\cal F_0 \subseteq \cal F$ is a random variable $X_0=\Bbb E(X|\cal F_0)$ satisfying the following two conditions: $X_0$ is $\cal F_0$ measurable. For any $E\in \cal F_0$, it holds that $\int_E {X} d\Bbb P =\int_E {X_0} d\Bbb P$. My confusion is that, since $X_0$ is a random variable, it must be a function defined on a sample space, then what is the sample space for $X_0$? Further the sample space must be equipped with a $\sigma$-algebra to form a measurable space, so what is the $\sigma$-algebra? Since $X_0$ is $\cal F_0$ measurable, so I suppose the $\sigma$-algebra associated with the sample space is $\cal F_0$? Thank you!","['probability-theory', 'conditional-expectation']"
1412951,Examples of statements that are true for real analytic functions but false for smooth functions,"I'm writing because I don't know the usefulness of real analytic functions. I mean, I know that analyticity is something more respect differentiable ($C^\infty$ function), but I don't have in mind a result, which is true only for real analytic functions, and then become false for $C^\infty$ functions which are not analytic.","['analysis', 'real-analysis', 'functional-analysis']"
1412986,Name for kind of big O notation with leading coefficient,"Context: As known the big O notation $O(f(n))$ describes a function $g(n)$ such that there is a constant $C \ge 0$ with $\limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le C$ (I assume that $f(n)$ is never zero). Thus the big O notation can be used to characterize the speed of convergence (for example for an algorithm). So I can write $$a(n) = a + O\left(\frac 1n\right)$$ which means that the function / algorithm $a(n)$ calculates in almost all calculation steps $n$ the desired number $a$ with an error less than $\frac Cn$ for a constant $C\ge 0$. Problem: Whereby I can compare two algorithms by their speed of convergence with the big O notation, the above notation $a(n) = a + O\left(\frac 1n\right)$ does not say anything about the actual error in the n-th step because I do not know the constant $C\ge 0$. My solution: One may introduce a new notation, lets say the big Psi notation $\Psi(f(n))$ which is defined as $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| < 1$$ For example one may write $$a(n) = a + \Psi\left(\frac{42}{n}\right)$$ so that $a(n)\in O\left(\frac 1n\right)$ with the constant $C=42$. There are also arithmetic rules for the big Psi notation similar to the rules for the big notation, for example: $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b+ab}{n}\right)$$ My question: Because the proposed solution is simple and somehow straightforward I guess there was already a mathematician who wrote about it. Can you point me to a textbook/paper where this notation is discussed, please? How is this notation called in mathematics? So far I have only found the big Omega notation and the big Theta notation ... Update: After rethinking the notation I would now define $$g(n)\in \Psi(f(n)) \iff \limsup_{n\to\infty} \left|\frac{g(n)}{f(n)}\right| \le 1$$ such that $$\left(1+\Psi\left(\frac an\right)\right)\left(1+\Psi\left(\frac bn\right)\right)\subseteq 1+\Psi\left(\frac{a+b}{n}\right)$$ because $$\limsup_{n\to\infty} \frac{\frac{a+b}{n}}{\frac{a+b}{n}+\frac{ab}{n^2}} = 1$$","['calculus', 'definition', 'sequences-and-series', 'asymptotics', 'numerical-methods']"
1413019,Question about vector spaces with the discrete topology,"Is it true that every vector space with the discrete topology is a topological vector space? (That is, a topological space with continuous addition and scalar multiplication whose singletons are closed?) Obviously singletons are closed in such a space by discreteness. I would also guess that addition/scalar multiplication are continuous since I thought every map from $X$ $\times$ $X$ to $X$ would be continuous, again by discreteness. The only issue is that I'm not sure I'm allowed to say that every set in $X$ $\times$ $X$ is open. I'm questioning even very basic things I thought I knew since my professor asked us to do this problem as an ""outside the box"" problem. By the way, it's not homework, but he wants to discuss it next class.","['functional-analysis', 'general-topology', 'topological-vector-spaces']"
1413094,Iterated circumcenters - proving collinearity and establishing distance ratios,"Let $P_0, P_1, P_2$ be three points on the circumference of a circle with radius $1$, where $P_1P_2 = t < 2$. For each $i \ge 3$, define $P_i$ to be the centre of the circumcircle of $\triangle P_{iâ1}P_{iâ2}P_{iâ3}$. Prove that the points $P_1, P_5, P_9, P_13, \ldots$ are collinear. Let $x$ be the distance from $P_1$ to $P_{1001}$, and let $y$ be the distance from $P_{1001}$ to $P_{2001}$. Determine all values of $t$ for which $\sqrt[500]{\frac{x}{y}}$ is an integer. This question is from the 2001 Canada National Olympiad. Is there a cleaner way of solving the problem, using e.g. vectors, complex numbers, or some theorem, so that one does not need to take cues from such an accurate diagram? The diagram below (from GeoGebra) shows an example of a possible (converging) sequence of points. (It is possible for the points to diverge if $t$ is nearly zero or nearly diametral.) Note that every triangle other than possibly the initial one is isosceles, i.e. $$P_{i-2}P_{i}=P_{i-1}P_{i},\qquad i\ge 3 \tag{1}$$ Since $P_i$ (for $i\ge3$) is equidistant from all of $P_{iâ1},P_{iâ2},P_{iâ3}$, then $P_i$ must be on the perpendicular bisector of $P_{iâ1}P_{iâ2}$ and $P_{iâ2}P_{iâ3}$. So, if $\angle{P_{i-2}P_{i-1}P_{i}}=\alpha,\angle{P_{i-1}P_{i}P_{i+1}}=\beta$ (as labeled for the $i=3$ example), then 
$$\beta=\frac{\pi}{2}-\alpha \tag{2}$$
This relationship can be applied inductively to all larger $i$ to show that:
$$\triangle{P_{i-2}P_{i-1}P_{i}} \sim \triangle{P_{i}P_{i+1}P_{i+2}},\qquad i\ge 3 \tag{3}$$
and that 
$$\triangle{P_{i-2}P_{i-1}P_{i}} \perp \triangle{P_{i}P_{i+1}P_{i+2}},\qquad i\ge 3 \tag{4}$$
Also because triangles turn through $90^\circ$ every two point, then side $P_{i-2}P_{i-1}$ is anti-parallel to side $P_{i+2}P_{i+3}$, i.e.
$$P_{i-2}P_{i-1} \parallel P_{i+3}P_{i+2},\qquad \tag{5}$$ As before, let $\angle{P_{i-2}P_{i-1}P_{i}}=\alpha,\angle{P_{i-1}P_{i}P_{i+1}}=\beta$, with $i\ge3$ odd, and let distance $P_{i-2}P_{i-1}=x$. Then 
$$\begin{align}
P_{i-1}P_{i}&=\frac{1}{2}x\sec\alpha \\[1em]
P_{i}P_{i+1}=\frac{1}{2}\left(\frac{1}{2}x\sec\alpha\right)\sec{\beta}&=\frac{1}{4}x\,\sec\alpha\,\mathrm{cosec}\,\alpha \\
\end{align}$$
so
$$\frac{P_{i-2}P_{i-1}}{P_{i}P_{i+1}}=4\sin\alpha\cos\alpha=2\sin{2\alpha}=2\sin{2\beta}=4\sin\beta\cos\beta$$ From the problem definition, assuming WLOG that $\beta<\frac{\pi}{2}$, we have $\sin\beta=\frac{t}{2}$ and $\cos\beta=\frac{\sqrt{4-t^2}}{2}$ so that $$\frac{P_{i-2}P_{i-1}}{P_{i}P_{i+1}}=t\sqrt{4-t^2},\qquad i\ge3 \tag{6}$$ Applying (6) repeatedly: $$\frac{P_{i}P_{i+1}}{P_{i+2k}P_{i+2k+1}}=\left(t\sqrt{4-t^2}\right)^k,\qquad i\ge1,k\ge0 \tag{7}$$ Part 1 I'm not quite sure how to prove this rigorously. It's obvious that the next point in the sequence depends only on the previous three and nothing beyond this. So if triangles turn through $90^\circ$ every two points (and hence $180^\circ$ every four points) and are similar, then any four consecutive segments in the path are a translated, mirrored and scaled image of the next four segments in the path. Then $P_1,P_5,P_9,\ldots$ must be collinear. And $P_2,P_6,P_{10},\ldots$, etc. Should I formalise this e.g. by using vectors? Part 2 The path is a regular spiral (claimed), so by similarity arguments $$\frac{x}{y}=\frac{P_{1}P_{2}}{P_{1001}P_{1002}}=\left(t\sqrt{4-t^2}\right)^{500}$$
Therefore, for the ratio as a function of $t$:
$$f(t)=\sqrt[500]{\frac{x}{y}}=t\sqrt{4-t^2}$$
Since $f(t)$ has a maximum when $t=\sqrt2$ and $f(\sqrt2)=\sqrt2\sqrt{4-2}=2$, we immediately have one integer value at:
$$\boxed{t=\sqrt2}$$
So it remains to find $t$ at which $f(t)=1$. 
$$\begin{align}
t\sqrt{4-t^2}=1 \implies t^2(4-t^2)=1 &\implies t^4-4t^2+1=0 \implies t^2=2\pm\sqrt3
\end{align}$$ From $t^2=2+\sqrt3$ we get $\boxed{t=\dfrac{1+\sqrt3}{\sqrt2}}$ From $t^2=2-\sqrt3$ we get $\boxed{t=\dfrac{-1+\sqrt3}{\sqrt2}}$ Remark: If $\frac{1+\sqrt3}{\sqrt2}<t<2$ or $t<\frac{-1+\sqrt3}{\sqrt2}$, the sequence of points diverges ($f(t)<1$).","['contest-math', 'geometry', 'proof-verification', 'trigonometry']"
1413108,How to draw the 5 dimensional hypercube graph with 56 edge crossings?,I'm probably doing something stupid but I can't seem to think of a way to draw $Q_5$ with $cr(Q_5) = 56 $. In this paper the author says drawing a hypercube graph with $\leq56$ edge crossings is easy (look under section 3 topological invariants) Could anyone give me an idea of how to get 56 edge crossings. I always end up getting 60 when I draw it.,"['graph-theory', 'combinatorics']"
1413111,"How many ways to arrange m chosen objects when there are n total objects, and some are indistinguishable?","I have $n$ different types of objects, where each member of a type is indistinguishable from every other member. There are $k_1$ of the first type, $k_2$ of the second type, and so on, up to $k_n$ of the $n$th type. I want to choose only a few of these objects, call it $m$, where $m \le \sum\limits_{i=1}^{n}k_i$. In the case where $m = \sum\limits_{i=1}^{n}k_i$, the formula is $\frac{m!}{k_1!k_2!...k_n!}$ What is the formula if I don't take all the objects? Rather, $m$ is strictly less than $\sum\limits_{i=1}^{n}k_i$ EDIT: I've tried to think of it in a straightforward way, like the derivation of the formula I gave. There's a nice explanation on another question here: Combination and permutation of indistinguishable objects . But I always run into problems with the fact that the amount of each type is fixed and could not be used in its entirety. Most recently, I've been trying to think of it like putting $m$ objects in $n$ boxes, where the boxes have different maximum capacities. I keep getting stuck there too.",['combinatorics']
1413132,"Intersections of Planes, Points...","I'm in sixth grade and learning geometry. Can someone tell me if I'm correct? The intersection of a point and a point is a point. The intersection of a point and a line is a point. The intersection of a point and a plane is a point. The intersection of a line and a point is a point. The intersection of a line and a line is a point. The intersection of a line and a plane is a point. The intersection of a plane and a point is a point. The intersection of a plane and a line is a point. The intersection of a plane and a plane is a line. Something seems wrong to me here. Can someone check this.
Also is it the same when asking for the intersection of a plane vs point, and point vs plane or something along those terms as seen in the statements above?",['geometry']
1413140,Ramanujan Infinity sum functional equations,"i was reading about the mellin transform ans i found the following
$$\sum _{k=1}^{\infty } \left(\frac{e^{-k x}}{e^{-2 k x}+1}-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$
but i do not how to show? it seem interesting i try firt ti get the mellin transform it is equivalent to
$$\sum _{k=1}^{\infty } \left(\frac{1}{2} \text{sech}(k x)-\frac{\pi  \text{sech}\left(\frac{\pi ^2 k}{x}\right)}{2 x}\right)=\frac{\pi }{4 x}-\frac{1}{4}$$ get by ramanujan
I found a similar formula
$$\sum _{k=1}^{\infty } \frac{\pi  (-1)^{k+1} \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)}{2 x}-\frac{1}{4}=\sum _{k=1}^{\infty } \frac{1}{2} \text{sech}(k x)$$ could you show it?
Also i like to include
$$\sum _{k=1}^{\infty } -\frac{16 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^5 (2 k-1)^5}-\frac{1}{180} \left(x \left(\pi ^2-6 x^2\right) \left(x^2+\pi ^2\right)-90 \zeta (5)\right)=\sum _{k=1}^{\infty } \frac{1}{k^5 \left(e^{2 k x}+1\right)}$$
and
$$\sum _{k=1}^{\infty } \frac{4 x^2 \left(\pi  \coth \left(\frac{\pi ^2 (2 k-1)}{2 x}\right)-\pi \right)}{\pi ^3 (2 k-1)^3}-\frac{1}{12} \left(x^3+\pi ^2 x-6 \zeta (3)\right)=\sum _{k=1}^{\infty } \frac{1}{k^3 \left(e^{2 k x}+1\right)}$$",['analysis']
1413145,Show that $(1+\frac{1}{n})^n+\frac{1}{n}$ is eventually increasing,"I would like to find a  way to show that the sequence $a_n=\big(1+\frac{1}{n}\big)^n+\frac{1}{n}$ is eventually increasing. $\hspace{.3 in}$(Numerical evidence suggests that $a_n<a_{n+1}$ for $n\ge6$.) I was led to this problem by trying to prove by induction that $\big(1+\frac{1}{n}\big)^n\le3-\frac{1}{n}$, as in $\hspace{.4 in}$ A simple proof that $\bigl(1+\frac1n\bigr)^n\leq3-\frac1n$?","['sequences-and-series', 'calculus']"
1413157,Law of Large Numbers - utility/difficulty of various versions.,"This may or may not be an answer to Is there an easy proof that the set of $x \in [0,1]$ whose limit of proportion of 1's in binary expansion of $x$ does not exist has measure zero? , depending on how easy a proof has to be to count as easy. The law of large numbers comes in various versions: LLN${}_p$ Suppose $X_1,\dots$ are iid with $\Bbb EX_1=0$ and $\Bbb E|X_1|^p<\infty$. Then $\frac1n(X_1+\dots+X_n)\to0$ almost surely. Proving LLN${}_1$ is considerably harder than proving LLN${}_2$. My impression is that this gives LLN${}_2$ a higher utility/difficulty ratio, since it suffices for many applications. Seems to me that LLN${}_4$ has yet a much higher utility/difficulty ratio. There's an awesomely easy proof of LLN${}_4$. The obligatory question then is this: Question Are there a lot of standard probability distributions out there, that actually come up, that give $\Bbb E X^4=\infty$ but $\Bbb E X^2<\infty$? Hoping for an answer to that question is why I'm posting this. The idea that people might be amused by the proof of LLN${}_4$ is absolutely no part of my motivation, I swear; that would not be a legal reason for this post's existence. Of course you need to see the proof I have in mind in order to evaluate that utility/difficulty ratio: Say $S_n = X_1+\dots+X_n$. Multiply out the product $S_n^4$:
$$S_n^4=\sum X_{j_1}X_{j_2}X_{j_3}X_{j_4}.$$Independence shows that most of those terms have mean $0$; the only terms with non-zero mean are of the form $X_j^4$ or a ""permutation"" of $X_j^2X_k^2$. A trivial bit of combinatorics shows then that $$\Bbb ES_n^4\le cn^2.$$ Monotone convergence shows that $$\Bbb E\left[\sum\left(\frac 1n S_n\right)^4\right]<\infty.$$Hence $\sum\left(\frac 1n S_n\right)^4<\infty$ almost surely, so $\frac1nS_n\to0$ almost surely. (Note I'm not claiming to be smart here; I saw this somewhere many years ago.)","['probability', 'law-of-large-numbers']"
1413163,Probability that two sets do not intersect,"I'm trying to understand this simpler problem so I can apply the process to a more difficult homework problem. Let $U$ be a set with $n$ elements. Select $2$ independent random subsets $A_1, A_2 \subset U.$ Both $A_i$ are chosen so that all $2^n$ choices are equally likely. I would like to compute the probability that $A_1, A_2$ are disjoint. I am looking for a calculation based on counting. I have no idea how to compute this, and I would appreciate any help given.","['probability-theory', 'probability']"
1413222,What are super-translations?,"There's been a lot of news lately about a possible solution to the black hole information paradox from a presentation given by Stephen Hawking to the KTH Royal Institute of Technology in Stockholm . Many of them mention the term super translation . This article , for example, states (with emphasis added): His flash of inspiration came when listening to a lecture in April
  about what are called super-translations , a bit of the heady
  branch of mathematics known as group theory. Dr Hawking thinks that
  incoming particles shed their information like a coat as they pass
  into a black hole, leaving it draped on the event horizon itself. Super-translations mathematically describe how that information influx can slightly jiggle the fabric of space at the horizon, in turn
  shifting around when and how the black hole radiates. What exactly are super-translations, mathematically? I have been trying to search for this term online, but the results are flooded with news articles. Where can I find some sources that describe super-translations in greater detail? EDIT: Another description of super-translations is provided in a recent article : The horizon of a black hole has the weird feature that itâs a sphere
  and itâs expanding outward at the speed of light. For every point on
  the sphere, thereâs a light ray. So itâs composed of light rays. But
  it doesnât get any bigger and thatâs because of the force of gravity
  and the curvature of space. And, by the way, thatâs why nothing that
  is inside a black hole can get outâbecause the boundary of the black
  hole itself is already moving at the speed of light. Thereâs this symmetry of a black hole that we all knew about in which
  you move uniformly forward and backward in time along all of the light
  rays. But thereâs another symmetry, which is the new thing in this
  paper (though various forms of it have been discussed elsewhere). Itâs
  a symmetry in which the individual light rays are moved up and down.
  See, individual light rays canât talk to each otherâif youâre riding
  on a light ray, causality prevents you from talking to somebody riding
  on an adjacent light ray. So these light rays are not tethered
  together. You can slide them up and down relative to one another. That
  sliding is called a super-translation. And in a way, it looks like you're not doing anything. Think of a
  bundle of infinitely long straws and you move one up and down relative
  to the other. Are you doing anything, or not? What we showed is that
  you are doing something. It turns out that adding a soft graviton has
  an alternate description as a super-translation in which you move some
  of these light rays back and forth relative to one another. Thatâs super-translations on black holes. Super-translations were
  introduced in the 1960s, and they were talking not about the light
  rays that comprise the boundary of spacetime at the horizon of a black
  hole but the light rays that comprise the boundary of spacetime out at
  infinity. The story started by analyzing those supertranslations.","['abstract-algebra', 'group-theory', 'physics', 'mathematical-physics']"
1413266,Exact Differential Equations $(axy^2+by)dx+(bx^2y+ax)dy=0$.,"$M(x,y)dx + N(x,y)dy=0$ is said to be a perfect differential when $$\frac{\partial (M(x,y))}{\partial y}=\frac{\partial (N(x,y))}{\partial x}$$ Let $M_y=\frac{\partial (M(x,y))}{\partial y}$ and $N_x=\frac{\partial (N(x,y))}{\partial x}$. In case if: $\frac{M_y-N_x}{N(x,y)}$ is only a function of x only (say $f(x)$) then it has a integrating factor $e^{\int f(x)dx}$. But if $\frac{M_y-N_x}{N(x,y)}$ is a constant then what will be the integrating factor? Say for this problem:
  $(axy^2+by)dx+(bx^2y+ax)dy=0$ Is there any other method to solve this differential equation?",['ordinary-differential-equations']
1413324,Finite partition of a group by left cosets of subgroups,"Let $G$ be a group.
Suppose there exist a finite sequence of elements $a_1, \cdots, a_n$ and a finite sequence of subgroups $H_1, \cdots, H_n$ such that $G = \bigcup_{i=1}^n a_iH_i$ is a disjoint union.
If $(G: H_1) = \cdots = (G : H_n) \lt \infty$, then $H_1= \cdots = H_n?$ I came up with this problem when trying to solve this question. Explicit construction of Haar measure on a profinite group",['group-theory']
1413326,Minimum of $f(x)=\sum_{i=1}^n\frac{a_n}{x-b_n}$ occurs at extreme point?,"Let $a_1,\ldots,a_n$ be real numbers and $b_1,\ldots,b_n>1$. Define $$f(x)=\sum_{i=1}^n\frac{a_i}{x-b_i}.$$ Is it always true that $f(x)\geq\min\{f(0),f(1)\}$ for all $x\in[0,1]$?","['algebra-precalculus', 'functions']"
1413345,All continuous functions are analytic,"This might be very silly to ask, but somehow this sequence of results are leading me to this wrong result. I am dealing with complex analysis and the mistake I am making might be because I am using some results from real analysis. If a function, $f(z)$, is continuous in simply connected domain, then it will be Riemann integrable and hence its antiderivitive, $F(z)$, will exist and moreover the antiderivative will be differentiable in the domain. This implies that $F(Z)$ is analytic since it is differentiable in the neighborhood of all points. Which also means that it is infinitely times differentiable. And hence even $f(z)$ is infinitely times differentiable and hence, $f(z)$ is also analytic.",['complex-analysis']
1413358,Is the spectral radius of a Hermitian matrix a non-decreasing function of the magnitude of its entries?,"I strongly suspect the answer is yes. By the min-max theorem , the largest eigenvalue of a hermitian matrix $M$ is
$$
\lambda_{max}=\text{max} \left( \frac{x^*Mx}{x^*x} \right)
$$
This is also its spectral radius. It seems intuitively that $\lambda_{max}$ could not decrease if some entries of $M$ increased in magnitude (in such a way that $M$ remains hermitian of course), but I cannot prove it.",['matrices']
1413403,Monty Hall problem again (from Grimmett and Stirzaker),"Grimmett and Stirzaker Exercise 1.4.5.2 In a game show you have to choose one of three doors. One conceals a car, 2 conceal goats. You choose a door but the door is not opened immediately. Instead the presenter opens another door, which reveals a goat. He offers you the opportunity to change your choice to the third door (unopened and so far unchosen ). Let $p$ be the conditional probability that the third door conceals the car. The presenter's protocol is: (i) he is determined to show you a goat; with a choice of two, he picks one at random. Show that $p=2/3$ (ii)he is determined to show you a goat; with a choice of two goats (Billy and Nan), he shows Billy with probability b. Show that $p=\frac{1}{1+b}$ (iii) he opens a door at random irrespective of what is behind. Show that $p=1/2$ I understand (i) but not (ii). For (i) my answer is: Label the doors D1,D2,D3, the car C, the goats G1 and G2, a goat G then $P(D3=C|D2=G)=\frac {P(D3=C \  \cap\ D2=G)} {P(D2=G)}=\frac{P(D3=C\ \cap D2=G | D1=C)  P(D1=C) + P(D3=C\ \cap\ D2=G | D1 \neq C) P(D1 \neq C) }{P(D2=G |D1=C)P(D1=C)+P(D2=G|D1 \neq C)P(D1\neq C)}=\frac{0*{1\over3}+1 * {2\over3}}{1*{1\over3}+1*{2\over3}}={2\over3}$ however similarly for (ii) my answer would be (calling Billy G1): $P(D3=C|D2=G1)=\frac {P(D3=C \  \cap \ D2=G1)} {P(D2=G1)}=\frac{P(D3=C\ \cap D2=G1 | D1=C)  P(D1=C) + P(D3=C \ \cap D2=G1 | D1 \neq C) P(D1 \neq C) }{P(D2=G1 |D1=C)P(D1=C)+P(D2=G1|D1 \neq C)P(D1\neq C)}=\frac{0*{1\over3}+{1\over2}*{2\over3}}{b*{1\over3}+1*{2\over3}}=\frac{1}{b+2}$ where is my mistake ? [Note: this question has undergone some changes in wording over successive editions of the book, in an attempt at clarifying the problem statement, as seen in the comments below. I attempted to answer what I believe was the problem intended by the authors.]","['probability-theory', 'monty-hall', 'probability', 'recreational-mathematics']"
1413444,Pointfree probability theory,"I must confess I hardly know anything about probability theory. Still, I'm interested in the following: Much like pointfree topology, where one basically replaces topological spaces by their locales of open sets, I figured there is a way to do something similar with $\sigma$-algebras and with probability spaces. Any thoughts on that? Does somebody know, whether this has been studied
  before? Here are some more thoughts: I suppose a problem is how to recover the sample space $\Omega$ from a pointfree probability space, as there is a no guarantee that there is an injection $\Omega \to \sigma$ from the sample space to the $\sigma$-algebra of a probability space. I wonder, how important it is to have a sample space at all. I (think I) know, that probability theory is actually about random variables, but do we really need a sample space to talk about those? Also, considering that there is no obvious notion of a morphism between probability spaces, maybe there are other objects we should look at?","['probability-theory', 'category-theory']"
1413453,"Calculating $\iint_{D} \left(x-y\right)dxdy$ where $D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}$","$$\iint_{D} \left(x-y\right)dxdy$$ where $D=\left\{0\le x-y\le 1,\:1\le xy\le 2\right\}$ So the substitution is pretty obvious, but j is: $J\:=\frac{1}{x+y}$ $$$$
I dont see how I get rid of the denominator and what is the new integral without x,y. Can somebody show me what is the trick here and how Do i get a new integral with the substitution $u=x+y, v=xy$?","['multivariable-calculus', 'definite-integrals', 'integration']"
1413504,"$\sum_{m=1}^{\infty }\frac{m}{2^n} \mu (E_{n,m}) \uparrow \int f d\mu$","Let $f \geq 0$ and $E_{m,n}=\{x :m/2^n \leq f(x) < (m+1)/2^n \}$ I need to show that as $n \uparrow \infty$ $\sum_{m=1}^{\infty }\frac{m}{2^n} \mu (E_{n,m}) \uparrow \int f d\mu$ My attempt:
I know that by fixing $n$, we have that on the set 
$E_{n,m}$ that $m/2^n \leq f(x)$ If we consider m to to finite , then $F=\mathbb{1}_{E_{n,m}}m/2^n$ is a simple function such that $F \leq f \implies \int F d \mu \leq \int f d \mu \Leftrightarrow \sum_{m=1}^{K }\frac{m}{2^n} \mu(E_{m,n}) \leq \int f d \mu $ Now taking K to the limit we get that $\sum_{m=1}^{\infty }\frac{m}{2^n} \mu (E_{n,m}) \leq \int f d \mu$ . It is clear that for a fixed n the sets $E_{n,m}$ are disjoint First I need to show that the L.H.S is increasing in $n$ (I have already shown it is always less than equal to $\int f d \mu$. Then I also need to show that the lim  of the L.H.S is indeed $\int f d \mu$. I cannot show that L.H.S is increasing in $n$ and that the limit is indeed the R.H.S Can you give me any hints on how to proceed. Please note that I am not looking for any complete solution Thank you",['probability-theory']
1413507,Solve this integral:$\int_0^\infty\frac{\arctan x}{x(x^2+1)}\mathrm dx$,"I occasionally found that $\displaystyle\int_0^{\Large\frac{\pi}{2}}\dfrac{x}{\tan x}=\dfrac{\pi}{2}\ln 2$. I tried that $$\int_0^{\Large\frac{\pi}{2}}\dfrac{x}{\tan x}=\int_0^{\Large\frac{\pi}{2}}x   \ \mathrm d(\ln \sin x)=-\int_0^{\Large\frac{\pi}{2}}\ln (\sin x)=\dfrac{\pi}{2}\ln 2$$
Then I tried another method 
$$\int_0^{\Large\frac{\pi}{2}}\dfrac{x}{\tan x}=\int_0^\infty\dfrac{\arctan x}{x(x^2+1)}\mathrm dx$$
I tried to expand $\arctan x$ and $\dfrac{1}{1+x^2}$, but got nothing, also I was confused that whether $\displaystyle\int_0^\infty$ and $\displaystyle\sum_{i=0}^\infty$ can exchange or not? If yes, on what condition? Sincerely thanks your help!","['calculus', 'closed-form', 'definite-integrals', 'sequences-and-series', 'integration']"
1413513,decomposing a function into embedding and projection,"I have a simple question. If $f:\mathbb{S}^{2}\rightarrow\mathbb{R}$ is a non-constant continuous
function, can we represent it as a composition $f=p\varphi$, where
$\varphi:\mathbb{S}^{2}\rightarrow\mathbb{R}^{3}$ is an embedding and
$p:\mathbb{R}^{3}\rightarrow\mathbb{R}$ is the projection onto the $x$-axis?
If the answer is negative, what about the case of a Morse function $f$? Remark. For the torus $\mathbb{T}^{2}$ this fails to be true - it is
enough to take $f$ constant in the complement of a small open disk.","['geometric-topology', 'algebraic-topology', 'general-topology', 'functions']"
1413569,Double integral $\int\int_A y dx dy$,"Calculate Double integral $$\iint_A y dxdy$$ 
where:
$$A=\{(x,y)\in\mathbb{R}^2 : x^2+y^2\le4, y \ge 0 \}$$
I do not know what would be the limit of integration if i change this to polar coordinates. I will manage to do the latter part of the question.","['analysis', 'calculus', 'integration']"
1413641,Is the graph of $xy=1$ in $\mathbb C^{2}$ connected? [duplicate],"This question already has answers here : The graph of xy = 1 is connected or not (4 answers) Closed 8 years ago . The graph  of  $xy=1$  in  $\mathbb C^{2}$ is  set  of  points $(x+iy,u+iv)$ that satisfies  $$xu-yv=1$$ and $$uy+xv=0$$ How  to  find  if  this  set  is  connected  or  not . I  also  have another  doubt. Can  this  set  mean  the collection  of  points $\{x+iy \in \mathbb C | xy=1\}$=$\{x+{{i}\over {x}}|x\in \mathbb R\}$ $?$ Some  lead  please ,  I  am  totally  clueless.","['complex-analysis', 'general-topology']"
1413650,Solve $y''=y^2$,"Are there any 'basic' solutions to this differential equation (ie using polynomials, exponetials, trigonometric functions and logarithms)? I cannot figure it out at all using the techniques I know for solving differential equations.",['ordinary-differential-equations']
1413671,"The ring $\mathbb{C}[x,y]/\langle xy \rangle$","What can be said about the ring $\mathbb{C}[x,y]/\langle xy \rangle$? I was very certain that
$$\mathbb{C}[x,y]/\langle xy \rangle \cong\mathbb C[x] \oplus\mathbb C[y]$$
since the elements in $\mathbb{C}[x,y]/\langle xy \rangle$ are of the form
$$a_0+\sum_{i=1}^{n}{b_ix^i}+\sum_{i=1}^{m}{c_iy^i}$$
and from that I deduced the above isomorphism. While I am still pretty sure that it is true, I cannot formulate a proof. So I am asking, is this true? If not, can someone show why? and maybe give another interpretation for this ring?","['abstract-algebra', 'ring-theory']"
1413678,How to find $\lim_{n\to\infty}\left(\frac{\pi^2}{6}-\sum_{k=1}^n\frac{1}{k^2}\right)n$?,"How to find $\lim_{n\to\infty}\left(\frac{\pi^2}{6}-\sum_{k=1}^n\frac{1}{k^2}\right)n$? It is well-known that $\lim_{n\to\infty}\sum_{k=1}^n\frac{1}{k^2}=\frac{\pi^2}{6}$, so $$\lim_{n\to\infty}\left(\frac{\pi^2}{6}-\sum_{k=1}^n\frac{1}{k^2}\right)n=\lim_{n\to\infty}\frac{\frac{\pi^2}{6}-\sum_{k=1}^n\frac{1}{k^2}}{\frac{1}{n}}$$ has indeterminate form $\frac{0}{0}$. But it can't use L'HÃ´pital's Rule.","['sequences-and-series', 'calculus', 'limits']"
1413686,Notation for the ith row and column of a matrix,"When noting the $i^{th}$ scalar of a vector $\mathbf{x}$ one usually does it as $x_i$, since it is a scalar When doing this for matrices that are being denoted in bold, let's say $\mathbf{A}$, how should I write the $i^{th}$ row or $j^{th}$ column? This question provides some insight as to how to distinguish  from rows and columns but does not address any possible standards for doing it as $\mathbf{A}_{i*}$, $A_{i*}$ or maybe even $\mathbf{a}_{i*}$ Is there a preferable form?","['vectors', 'notation', 'matrices']"
1413702,Is there anything known about the zeros of $ \sum_{n=1}^{\infty} (\frac{1}{\rho_n^s} +\frac{1}{\overline{\rho_n}^s})$?,"Assuming the RH and $\rho_n =\frac12 + \gamma_n i$ being the n-th non-trivial zero of $\zeta(s)$, then numerical evidence suggests that: $$f(s) :=\displaystyle \sum_{n=1}^{\infty} \left(\frac{1}{\rho_n^s} +\frac{1}{\overline{\rho_n}^s}\right)$$ converges for all $s \ge 1$ and has an infinite number of zeros 'just above' the odd integer values of $s$. I know that $f(1) = 1 + \frac{\gamma}{2} -\frac12 \ln(4\,\pi)$, however wondered whether there is anything known about: 1) closed forms for other positive integer values of $s$ and/or, 2) the (quite regular) pattern of the zeros of $\,f(s)$? I attach a graph to illustrate the point around the zeros ($y=f(s)$, magnified by $10^{12}$ and $n=99$):","['sequences-and-series', 'number-theory', 'riemann-zeta']"
1413708,Are all operations functions?,"I have looked at Wikipedia(I know it's not completely reliable) but on it an operation is formally defined as: ""A function Ï is a function of the form $Ï : V â Y$, where $V â X_1 Ã â¦ Ã X_k$."" and I have also heard one of my professors mention in passing that an operation is a ""special"" kind of function. Here's the thing, functions have only one output and therefore operations should too, however the indefinite integration operation has infinitely many outputs if the antiderivative exists. Can you please clear this up for me, perhaps the definition of an operation is wrong? And if so what is exactly an operation then? Thank you very much.","['calculus', 'definition', 'functions']"
1413763,The Intution Behind Real Symmetric Matrices and Their Real Eigenvectors,"I am wondering about the geometric intuition behind real symmetric matrices and their corresponding linear transformations. Is it possible to understand geometrically why real symmetric matrices have only real eigenvalues? That is, what do symmetric linear transformations have in common geometrically that make this true? I am NOT after a proof of this fact; what I am curious about is whether there is a geometric argument for it. I am after the sort of intuition one gets from looking at linear transformations in $\mathbb{R}^2$ and envisioning their eigenvalues and eigenvectors. For example, it is very intuitively clear why nontrivial rotation matrices cannot have real eigenvectors. It is also geometrically clear why diagonal matrices have their eigenvalues equal to their diagonal elements. EDIT: Upon further thought, I have realized that a simple consequence of symmetry is that in the singular value decomposition of a matrix $M = U \Sigma V^*$, we have $U = V$. Thus requiring symmetry means that the linear transformation must be accomplished by performing a rotation, performing a scaling along the axes, and then reversing the original rotation . The missing piece for me is why defining a transformation via a symmetric matrix means that the transformation can be decomposed in this simple way.","['linear-algebra', 'intuition']"
1413778,A singular Gronwall inequality,"Let $f : [0,T] \to \Bbb{R}^+$ be a continuous function such that $f(0)=0 $ and : $$
f(t)\le C\int_0^t s^{-1}f(s) ds,\; \forall t\in [0,T]
$$ for some constant $C>0.$ Is it true that $f(t)=0,\; \forall t\in [0,T]?$","['inequality', 'real-analysis', 'ordinary-differential-equations', 'integration']"
1413794,How $\sigma$-algebra determines random variable?,"In my probability textbook there is a statement saying that Knowing the $\sigma$-algebra $\sigma(X)$ generated by a random variable $X$ is equivalent to knowing $X$ itself. We equate $\sigma(X)$ to our everyday term ""information"". Here, sigma-algebra generated by a random variable is defined as the following. Suppose $X:(\Omega,\cal{F}) \to$ $(E,\cal E)$, then the sigma-algebra generated by $X$ is $\sigma(X)=\sigma\{X^{-1}(A):A\in \cal E \}$, i.e. the sigma-algebra generated by $\{X^{-1}(A):A\in \cal E \}$. The following is my understanding with confusion. If we know the random variable $X$, which is a function mapping elements in $\Omega$ to $E$, then clearly we can determine $\sigma(X)$ by its definition. This direction is OK to me. For the other direction, if $\sigma(X)$ is known, then how $X$ is determined? For example, suppose $\Omega=\{1,2,3,4,5,6\}, E=\{0,1\}$ and $\sigma(X)=\{\emptyset,\Omega, A, A^C\}$ where $A=\{1,3,5\}$. Now it seems that we have two possibilities, $X(x) = \left\{ {\begin{array}{*{20}{c}}
0&{x \in A}\\
1&{x \in {A^C}}
\end{array}} \right.$ and $X(x) = \left\{ {\begin{array}{*{20}{c}}
1&{x \in A}\\
0&{x \in {A^C}}
\end{array}} \right.$. It looks to me that knowing only the $\sigma(X)$ can not help distinguish between the two. The above seems to be very basic in probability theory and it affects my understanding of some later claims. For example, my textbook states the following theorem for conditional expectation. I don't understand why the following argument is true . Given two random variable $X,Y$ where $Y$ is $\cal F_0$ measurable and $\cal F_0$ is a sub $\sigma$-algebra of $\cal F$, then $\Bbb{E}(XY|\cal F_0)$$=Y\Bbb{E}(X|\cal F_0)$. The argument is that $Y$ is determined given $\cal F_0$, so it can be moved outside of the expectation. I have to understand this clearly. Thank you!","['probability-theory', 'measure-theory']"
1413820,A continuous bounded function from $\mathbb R$ to $\mathbb R$ can be increasing or not?,"Let $f:\mathbb R \rightarrow \mathbb R$  be  a continuous  and  bounded  function , then $a$) $f$ has a fixed point. $b$) $f$ cannot be increasing $c$) $\lim_{x\rightarrow \infty} f(x)$  exists. Now  I  think  $a$)  is  correct.  For $f$   being  continuous  and  bounded  there  is  a  positive  integer  say $M$  such  that  $$|f(x)| < M$$ i.e. $$-M < f(x) < M$$ i.e. $$f(-M) > -M \text{ and } f(M) < M$$ i.e. if  we  take $$g(x)=f(x)-x$$  then  it  is  continuous  and $$g(-M) < 0$$ and  $$g(M) > 0$$
and  hence  there  is  a  point $x_0$  such  that  $$g(x_0)=0$$ i.e. $$f(x_0)=x_0$$ For  option $c$) I  drew  this  graph thinking  it  would  be  possible  for   the  function $f$  to  be  increasing  with  $y=M$  being  its  asymptote  but  I  am  not  sure  since  could  not  get  the  analytic  definition  of  this . So  if  $c$)  is  wrong  then  it  can  be  increasing  then $f(x)$  being increasing  and  bounded will not  the  limit  in  $c$)  exist? But  may  be  not  always .  Need  little  help  on  proving  $b$)  and  $c$)  wrong . Thanks..","['real-analysis', 'functions']"
1413845,Proving that and how $ \frac{1}{n}\sum\limits_{p\le n}\lfloor n/p \rfloor - \sum\limits_{p\le n} 1/p $ approaches $0$,"Let $p$ denote a generic prime number. By Mertens' second theorem, the sequence $$\sum\limits_{\ p \le n} \frac1p - \log\log n$$ converges to the Meissel-Mertens constant $M\approx 0.2614972$. Now let $\omega(n)$ be the number of distinct prime factors of $n$. Analogously, we have $$\lim_{n\to\infty} \frac{1}{n}\sum\limits_{k\le n} \omega(k) - \log\log n = M,$$ hence combining this, the first result and $$\sum\limits_{k\le n} \omega(k)=\sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor, $$ we find $$\bbox[5px,border:2px solid #B2B550]{\lim_{n\to\infty} \frac{1}{n} \sum\limits_{p\le n} \left\lfloor \frac{n}{p}\right\rfloor - \sum_{p\le n} \frac{1}{p}=0. }\tag{$\star$}$$ How could we prove $(\star)$ directly? Is it known if the sequence is negative for all $n>2$, or at least if it changes sign finitely often?","['alternative-proof', 'prime-numbers', 'number-theory', 'real-analysis', 'sequences-and-series']"
1413854,Characterize in terms of fibre,"I am not familiar with the notion ""characterize"" in the following context. Does this mean to redefine or?.... Any help would be appreciated. Thank you. For a function $f:X\to Y$, and y an element of Y, let $f^{-1}(y) := \{ x \in X| f(x) = y\}$. This is sometimes called the ""fibre"" of f over y. Problem: characterize 1 to 1 and onto functions in terrm of the fibres","['notation', 'definition', 'functions']"
1413859,Does the Riemann-Hypothesis imply the Twin-Prime-Conjecture?,"The Riemann hypothesis ( https://en.wikipedia.org/wiki/Riemann_hypothesis )
is one of the most important conjectures in number theory. I read that the
Riemann hypothesis implies the Goldbach Conjecture and would allow much
better estimates for the prime-counting function. What about the Twin-Prime-Conjecture ? Would it follow from the Riemann-hypothesis ?","['riemann-hypothesis', 'number-theory', 'twin-primes']"
1413874,"Can we add an uncountable number of positive elements, and can this sum be finite?","Can we add an uncountable number of positive elements, and can this sum be finite? I always have trouble understanding mathematical operations when dealing with an uncountable number of elements. Any help would be great.","['summation', 'sequences-and-series', 'real-analysis']"
1413887,sufficient statistics of a sequence of normal random variable,"If $X_1, X_2\ldots,X_n$ are independent variables with $X_i \sim \mathcal N(i\theta,1)$, $\theta$ is an unknown parameter. What is a one dimensional sufficient statistic $T$ of this sample? I have a intuition guess that the answer is $\frac{1}{n}\sum_{i=1}^n \frac{X_i}{i}$, but I don't know how to prove it through definition or get it using factorization. Can anyone give me a hint on how to derive it? Thanks!","['statistics', 'sufficient-statistics']"
1413891,Solve: $x''(t)-2x'(t) + x(t) = 2 \sin(3t)$,"It is asked to solve the ODE $x''(t)-2x'(t) + x(t) = 2 \sin(3t)$ for $x(0)=10, \; x'(0)=0$ It is equivalent to the first order system in two variables $$\begin{bmatrix} x' \\ y' \end{bmatrix} = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} \begin{bmatrix} x \\ y \end{bmatrix} + \begin{bmatrix}0 \\ 1 \end{bmatrix} 2 \sin(3t), \; \begin{bmatrix}x_0 \\ y_0 \end{bmatrix} = \begin{bmatrix} 10 \\ 0 \end{bmatrix}$$ If $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} , B = \begin{bmatrix}0 \\ 1 \end{bmatrix}$$ the solution for this ODE of first order is $$ e^{tA} \begin{bmatrix} x_0 \\ y_0 \end{bmatrix} + \int_0^t e^{(t-s)A}2B\sin(3s)ds $$ I know we could look for the characteristic polynomial of the equation and try to find a solution which combines sines and cosines terms, but since I am studying resolution of second order systems using first order ones, I would like to check if this is a good way of solving it. The exponential of the matrix $tA$, for example, doesn't seems to have a good form (except if I wrote something wrong). So, what is the better way of solving it? Thanks in advance! Edit: $$A = \begin{bmatrix} 0 & 1 \\ -1 & 2 \end{bmatrix} = \begin{bmatrix} 1&-1 \\ 1&0 \end{bmatrix} \begin{bmatrix} 1&1 \\0 &1 \end{bmatrix} \begin{bmatrix}0 &1 \\ -1&1 \end{bmatrix}$$ $$\Rightarrow e^{t\begin{bmatrix} 1& 1\\0 &1 \end{bmatrix}} = e^{t\begin{bmatrix} 1&0 \\ 0&1 \end{bmatrix} + \begin{bmatrix} 0&t \\ 0&0 \end{bmatrix}}=\begin{bmatrix}e^t &0 \\0 &e^t \end{bmatrix} \begin{bmatrix} 1&t \\0 &1 \end{bmatrix}$$","['linear-algebra', 'ordinary-differential-equations', 'exponential-function']"
1413904,Show that the equation $\cos(\sin x)=\sin(\cos x)$ has no real solutions.,"The following problem was on a math competition that I participated in at my school about a month ago: Prove that the equation $\cos(\sin x)=\sin(\cos x)$ has no real solutions. I will outline my proof below.  I think it has some holes.  My approach to the problem was to say that the following equations must have real solution(s) if the above equation has solution(s): $$
\cos^2(\sin x)=\sin^2(\cos x)\\
1-\cos^2(\sin x)=1-\sin^2(\cos x)\\
\sin^2(\sin x)=\cos^2(\cos x)\\
\sin(\sin x)=\pm\cos(\cos x)\\
$$ I then proceeded to split into cases and use the identity $\cos t = \sin(\frac{\pi}{2} \pm t\pm y2\pi)$ to get $$
\sin x=\frac{\pi}{2} \pm \cos x\pm y2\pi\\
$$ and the identity $-\cos t = \sin(-\frac{\pi}{2}\pm t\pm y2\pi) $ to get $$
\sin x=- \frac{\pi}{2}\pm \cos x \pm y2\pi.\\
$$ where $y$ is any integer.  I argued that $y=0$ was the only value of $y$ that made any sense (since the values of sine and cosine remain between $-1$ and $1$).  Therefore, the above equations become $$
\sin x=\frac{\pi}{2} \pm \cos x\implies \sin x \pm \cos x=\frac{\pi}{2}\\
$$ and $$
\sin x=- \frac{\pi}{2}\pm \cos x\implies \cos x\pm\sin x= \frac{\pi}{2}.\\
$$ Then, by a short optimization argument, I showed that these last two equations have no real solutions. First, does this proof make sense?  Second, if my proof makes sense, then I feel that it was not very elegant nor simple.  Is my approach the best, or is there a better (i.e., more elegant, shorter, simpler) proof?","['problem-solving', 'trigonometry']"
1413909,Stellating the Octahedron,"I have a few related questions and I'd be happy to get some help with any one of them. Is the stellation of a polyhedron generally a 'messy' affair that involves trimming away portions of the enlarged faces? (See below for my implicit assumption, $\text{stellation} \cong \text{dilation},$ that is probably wrong, I see now) Are there any polyhedral stellations that come from simply dilating the faces , no trimming necessary? Is there a good reference that explicitly defines stellation? I use truncation often but have gotten by with intuition, but this is not the case with stellation. Context: I am trying to create a very primitive animation/demonstration that shows the stellation of an octahedron to yield the stella octangula. Unfortunately, it seems that the mental image I have for stellation isn't living up to the real thing. I've always thought that one simply dilates each face, since this is what happens to the star polygons. Quoting Coxeter ( Regular Polytopes , section 6.2 on Polyhedra Stellation), it appears this is not necessarily the case: In order to stellate a Polyhedron, we have to extend its faces symmetrically until they again form a polyhedron. At any rate, I started with the stately octahedron, shown here using the software Grapher for OSX (I prefer the faces colored uniformly, but it defaults to checkerboard every time I scale). Now I start to scale each face about its center; this image shows each face scaled by a factor of $2$ . I'm noticing the triangular 'hole' starting to close up in the NorthEast region, for example. Here it is scaled by a factor of $4$ , where the three planes bounding the previous triangular 'hole' have finally met at a point. And here's a closeup near the three planes meeting at that point, with the 'hole' finally closed. In the last image I think I can see the stella octangula hiding underneath, but the there are many portions of the enlarged faces that need to be thrown away. I only have hands-on experience stellating regular polygons, by extending the edges, to produce star polygons . In that case, it's just a matter of finding the right scale factors for the edges with no trimming required. My goal was for an expository lecture to non math-majors, but that animation is shaping up to be far too complicated.","['polyhedra', 'visualization', 'solid-geometry', 'geometry']"
1413937,Probability or Set,"I'm really good at probability, but this time I seems like I'm not. My friends asked me a very tricky question, and I want to see if there's anyone who can find out the answer. Here's the question: In a survey of 1,000 randomly selected people: 756 believed that the price of the particular product of is reasonable compared to the last year increase, 487 believed that the quality of that product is worse than the last year, and 363 believed both From here we know: Product is reasonable + Product is worse = 363 But how about Product is reasonable + Product is not worse Product is not reasonable + Product is worse Product is not reasonable + Product is not worse I'm trying to figure it out using set like $P(A\cup B\cup C\cup D)$ but seems like hard for me.","['elementary-set-theory', 'algebra-precalculus', 'combinatorics']"
1413946,How to prove convergence in $L^p$ imply convergence in $L^r$ when $p>r$?,$X_n$ converges to $X$ in $p$th mean. Show that $X_n$ also converges to $X$ in $r$th mean when $p\ge r$. I have tried conditioning on $|X_n-X|\ge1$ and $|X_n-X|<1$  but no luck.,['probability-theory']
1413951,Indicator Function Distributive Property Proof,"This is my first post(: I'm trying to understand how to prove the distributive property using the indicator function.  I have made the truth tables and understand how this is proved using set notation as in this question: Set Distributive Property Proof But I cant seem to understand how to write this using indicator function notation. $\mathbb{A}$ is a proposition about elements $x \in X$ and we put the corresponding set $A = \{x \ \in X: \mathbb{A}(x)\}$. For each $x \in X$ and $A \subset X$ define the indicator function of the set A by \[1_A (x) := \begin{cases} 
1 & if \; x \in A \\ 
0 & if \; x \notin A 
\end{cases}
\]
Further, the ""and"" and ""or"" of this are given by: $ (1_A \wedge 1_B)(x) = 1_A(x) \cdot 1_B(x) = 1_A \cdot 1_B $ $ (1_A \vee 1_B)(x) = 1_A(x) + 1_B(x) - 1_A(x) \cdot 1_B(x) = 1_A + 1_B - 1_A \cdot 1_B$ Prove: $ (\mathbb{A} \vee (\mathbb{B} \wedge \mathbb{C})) \iff ((\mathbb{A} \vee \mathbb{B}) \wedge (\mathbb{A} \vee \mathbb{C})) $ Writing the left side in indicator function notation I think it should be:
$ 1_A \vee (1_B \wedge 1_C)(x) = 1_A \vee ( 1_B \cdot 1_C ) = 1_A + (1_B \cdot 1_C) - 1_A \cdot (1_B \cdot 1_C) $ However, the textbook says it should be: $ 1_A \cdot (1_B + 1_C - 1_B \cdot 1_C) $
Which looks like the reverse order of what I think, so I'm stuck here. Thank you!","['elementary-set-theory', 'proof-writing']"
1413955,"Why is $(\mathbb{R}, \mathcal{P}(\mathbb{R}))$ called a measurable space when actually is not?","I get confused when I put the following three notes together: Power set of any set is a $\sigma$-algebra. If $X$ is a set and $\Sigma$ is a $\sigma$-algebra over $X$, then the pair $(X, \Sigma)$ is a measurable space. Vitali set is known as a counterexample that there is no measure on all the subsets of $\mathbb{R}$. By (1) and (2), one may think that $(\mathbb{R}, \mathcal{P}(\mathbb{R}))$ is a measurable space, which intuitively concludes that there must be a measure on all the subsets of $\mathbb{R}$. However, (3) says the opposite. Can anyone help me understand what is going on?","['real-analysis', 'measure-theory']"
1413961,Prove that : $n \mid \varphi (a^{n}-1)$ $a>1$,"Prove that : $n \mid \varphi (a^{n}-1)$ $a,n$ positive integers wih $a>1$ I know that $a$ has multiplicative order $n$ in the ring of integers modulo $a^{n}â1$ and the order of the  group of units modulo $a^{n}â1$ is $\varphi (a^{n}-1)$. How can I prove that $a^{\varphi (a^{n}-1)}= 1 $mod$(a^{n}-1)$? Thaks for your help.","['abstract-algebra', 'group-theory']"
1413964,The Rhombohedron,"I am trying to model a rhombohedron (using Blender) as a first pass to building DÃ¼rer's solid so I am trying to calculate the (x,y,z) values for a given side length 'a' and angle 'theta' (starting with 72).  I want to express the (x,y,z) coordinates in terms of 'a' and 'Theta' so I can use Excel to calculate them and see the effect of changing 'theta'.  I think I have these relationships correct. I am also trying to characterize the four interior diagonals and find the center of the Rhombohedron to determine where the cicumsphere would be for Durer's solid. I can determine the diagonals of the flat sides, no problem, in terms of 'a' and 'theta', but I am completely stuck in trying to determine the lengths of the interior diagonals of the Rhombohedron and in finding the center, in terms of 'a' and 'theta'.  Any help would be greatly appreciated.","['polyhedra', 'solid-geometry', 'geometry', 'trigonometry']"
1414013,How to simplify $ \int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx $ using Green's indentity?,"Let $\varphi\in C_c^\infty(\Bbb{R^2})$ (infinitely differentiable functions with compact support) and consider
  $$
I=\int_{\Bbb{R}^2}\Delta\varphi(x)\log|x|^2\ dx,
$$
  the existence of which is given by the fact that $\log |x|^2$ is locally integrable. How far can one simplify this integral using the Green's identity? Let $R>0$ be large enough so that $\{|x|<R\}$ contains the support of $\varphi$. Denote $\Omega_{\varepsilon}=\{\varepsilon<|x|<R\}$ and $g(x)=\log|x|^2$ Then 
$$
I=\lim_{\varepsilon\to 0+}\int_{\Omega_\varepsilon}\Delta\varphi(x)\log|x|^2\ dx
=\lim_{\varepsilon\to 0+}\left(\int_{\Omega_\varepsilon}\Delta(\log|x|^2)\cdot 
\varphi(x) dx
+\int_{\partial\Omega_{\varepsilon}}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds\right)
$$
where $\frac{\partial\varphi}{\partial\nu}$ is the normal derivative. 
Note that the first term is zero since $\Delta g=0$ on $\Omega_\varepsilon$. Let $\Gamma_\varepsilon=\{|x|=\varepsilon\}$ and $\Gamma_2=\{|x|=R\}$. Then by straightforward calculations, one has
$$
\lim_{\varepsilon\to 0}\int_{\Gamma_\varepsilon}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds=0. 
$$
Everything now boils down to the calculation of 
$$
\lim_{\varepsilon\to 0}\int_{\Gamma_2}g\frac{\partial\varphi}{\partial\nu}-\varphi\frac{\partial g}{\partial\nu}ds
=\int_{\Gamma_2}\log R^2\frac{\partial\varphi}{\partial\nu}-\varphi\frac{2}{R}ds.
$$
Could anyone simplify it further?","['multivariable-calculus', 'real-analysis', 'functional-analysis', 'partial-differential-equations']"
1414019,"Difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model?","I posted this question on Cross Validated - but I think it applies here too. Also, it increases the chances of the question being answered. Link here If this is not acceptable - administrators please delete, and anybody else please do not take points away from me for this reason. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ What is the difference between the ""Hazard Rate"" and the ""Killing Function"" of a diffusion model? Some Definitions: The Killing Function The function k(t,x) is interpreted as the killing rate. Informally, this means that if, at time t , the particle is alive and is situated at the point x , then the probability that it dies in the next h units of time is approximately k(t,x)h when h is small. \begin{gather*}
Pr(\rho  \leq t+h \mid \rho  > t, X(t) = x)\approx k(t, x)h & (1)
\end{gather*} And, \begin{gather*}
dX(t) = \mu dt+\sigma dW\ & (2)
\end{gather*} Hazard Rate \begin{gather*}
Pr(t \leq T \leq t+h \mid T > t) \approx \lambda (t)h & (3)
\end{gather*} That is, Î»(t)h represents the instantaneous chance that an individual will die in the interval (t, t + h) given that this individual is alive at age t . Lastly, to put it in perspective here is a picture of a diffusion with arbitrary Killing Function k(x) = a + Sqrt(t/b) , where a, and b are some constants. I added the lines for later reference. So, these results raise a lot more question. How do I interpret ""rho"" in Equation (1) for example - if I am modeling a type of bird population for with X(t) ? How do I relate the Killing Function with the Hazard Rate? Is it OK to say that if the f(t) is the density distribution of the First-Passage-Times (Refer to Fig-2), then the Hazard rate for the diffusion (2) is: \begin{gather*}
\lambda (t) = \frac{f(t)}{1-F(t)} & (4)
\end{gather*} If I do not know the killing function - but I observe the first passage time distribution as in Fig-2: Is it possible to solve for the Killing Function? Lastly, in the definition k(t,x) is a function of both variables { x,t }. In the literature, most of the time is referred as k(x) , which is really k(X(t)) since X() is a function of t . But if one was to actually apply it - as I did in Fig-1, say: \begin{gather*}
k(x) = b[(x(t)-a)^{2}]\ & (5)
\end{gather*} I would have to express it in terms of X(t) : \begin{gather*}
X(t) = a+\sqrt{\frac{t}{b}} & (6)
\end{gather*} But X(t) is reserved for the diffusion model (2) so it makes it extra confusing. Note: Assume OP (original poster) is very unintelligent; hence, be very specific, use simple words, do not leave any algebra out, and do not hesitate to curse me out if I wrote something stupid above. Thank you so much in advance! ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~","['wiener-measure', 'probability', 'statistics', 'stochastic-processes']"
1414022,"A reason for the value of $\int_{0}^{1}\log{(x)}\log{(1-x)}\,\mathrm{d}x$","In this .pdf document, which is just a list of Putnam-style undergraduate-level problems from various sources, the third question is as I have stated it below (up to a change of notation). Evaluate $$I=\int_{0}^{1}\log{(x)}\log{(1-x)}\,\mathrm{d}x.$$ Feel free to take a moment to try to solve this yourself, if you have never seen it before. My answer is as follows. In the interval $(0,1)$, we may expand $\log{(1-x)}$ as a power series: 
\begin{eqnarray*}
\log{(1-x)} & = & -\left(x+\frac{x^{2}}{2}+\frac{x^{3}}{3}+\ldots+\frac{x^{r}}{r}+\ldots\right)\\
& = & -\sum_{k=0}^{\infty}\frac{x^{k}}{k}
\end{eqnarray*} Now for any $p\in\mathbb{N}$, using integration by parts we see that $$\int_{0}^{1}x^{p}\log{x}\,\mathrm{d}x = -\frac{1}{(p+1)^{2}}.$$ Combining the above results, we have:
\begin{eqnarray*}
\int_{0}^{1}\log{(x)}\log{(1-x)}\,\mathrm{d}x & = & \int_{0}^{1}\log{(x)}\left[-\sum_{k=1}^{\infty}\frac{x^{k}}{k}\right]\,\mathrm{d}x\\
& = & -\sum_{k=1}^{\infty}\frac{1}{k}\left[\int_{0}^{1}x^{k}\log{x}\,\mathrm{d}x\right]\\
& = & \sum_{k=1}^{\infty}\frac{1}{k(k+1)^{2}}\\
& = & \sum_{k=1}^{\infty}\left[\frac{1}{k(k+1)}-\frac{1}{(k+1)^{2}}\right]\\
& = & \sum_{k=1}^{\infty}\frac{1}{k(k+1)}-\sum_{k=1}^{\infty}\frac{1}{(k+1)^{2}}\\
& = & 1-(\frac{\pi^{2}}{6}-1)\\
& = & 2-\frac{\pi^{2}}{6},
\end{eqnarray*}
where we have evaluated one of the two series at the end via a telescoping sum, the details of which I have left out, and the value of the other series is well-known. I found the result surprising. I performed a small sanity check by attempting to sketch the graph within the interval; Wolfram|Alpha agrees with my sketch and agrees with my answer, but, to me at least, this information is uninformative about why the result is true. To be more precise, I don't understand how or why the answer relates to the original elements of the question. My question : Is there any reason why one would expect $\pi$ to appear in this answer? Is there some tricky change of variables or some unbeknownst-to-me complex analysis way of evaluating this integral which sheds more light on the relation between it and $\zeta(2)$? With this in mind, I should also be precise about what I am accepting as an answer: T's & C's : If no such ""deeper relation"" between question and answer is apparent to anyone at all , then I will accept ""it's just a coincidence"" as an answer. If a connection is apparent to somebody, but it involves mathematics that you fear may be beyond me, feel free to post it anyway if you wish, and I'll do my best to understand what you've said. By a ""deeper relation"", I mean any interpretation or rephrasing of the question into terms beyond elementary calculus; other areas of mathematics, or even physical interpretations, will do. Other integrals which are surprising in a similar way may also be helpful.","['calculus', 'soft-question', 'definite-integrals', 'logarithms', 'integration']"
1414028,Limiting value of $\frac{x^n e^x}{n!}$ as $n\to\infty$,"For the Taylor Series the remainder is of the form $$R_n = \frac{(x-a)^n}{n!} f^{(n)}(\xi) $$ with $a \leq \xi \leq x$ For the series of $e^x$ about $0$ (that is, the Maclaurin series) the remainder is 
$R_n = \frac{x^n}{n!}e^{\xi}$ with $0 \leq \xi \leq x$. Now, I have to prove that as $n \to \infty$ this $R_n \to 0$. How do I go about it? My problem is that I know nothing about $x$ and whether it is bigger or smaller than $n$. In short, how do I prove  $$\lim_{n\to\infty}\frac{x^n}{n!}e^{\xi}=0$$","['taylor-expansion', 'sequences-and-series', 'limits', 'exponential-function']"
1414037,Why does $e^{-x}$ approach $0$ as $x$ gets large? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Why is it that $$\lim_{x \to -\infty} e^x = 0?$$ Context:
College has started back up again and I like to understand the reasons why things do the things they do, rather than just memorizing. I'm sure i've seen it before - but I am coming up with a blank today.","['limits', 'algebra-precalculus', 'exponential-function']"
1414038,Can different tetrations have the same value?,"Suppose, we have two numbers $a\uparrow \uparrow b$ and $c\uparrow \uparrow d$. To avoid trivial cases, suppose $a,b,c,d>2$ and $(a,b)\ne (c,d)$. Is there a quartupel $(a,b,c,d)$ with $a\uparrow\uparrow b=c\uparrow \uparrow d$ under the given conditions ? In general, $b<d$ implies $a\uparrow \uparrow b<c\uparrow\uparrow d$, but if $a$
is large enough, $a\uparrow\uparrow b$ will exceed $c\uparrow \uparrow d$. So, this argument does not rule out the possibility of the equality.","['tetration', 'big-numbers', 'number-theory']"
1414059,Topological idea of orientability of manifold,"While reading Poincare Duality a new idea of orientability of manifold came in my mind.I dont know wheather this idea is new or not, or even true or false. My idea is following... A $n$-dim manifold $X$ is called non-orientable if there exists an embedding $i: M\times \mathbb{D^{n-2}} \to X$, where $M$ is a mobius strip. Otherwise $X$ is called an orientable manifold. For an example if $n=2$, then by using of classification theorem of compact  $2$-manifold we can see that my definition works for 2-dim compact manifold...But I got stuck while doing this generalization...Till now I've knowladge of algebraic topology only upto Poincare Duality of Hatcher's book... I want to know whether my this general idea is true or not, and in case if it is true then how do I prove it?? I need some HINTS for that...and if it is a well known result then please give me some reference where I can study this properly.","['orientation', 'algebraic-topology', 'general-topology', 'geometric-topology']"
1414076,Finding an explicit eigenvector,"Let $A$ be an $n\times n$ matrix over a field and let $\operatorname{adj}(A)$
denote its classical adjoint. Suppose all column sums of $A$ are zero so that $A$ is singular. If $\operatorname{rank}(A) = n-1$, the adjoint is nonzero. Hence any nonzero column of $\operatorname{adj}(A)$ provides an explicit (right) eigenvector of $A$ in the null space because $A\operatorname{adj}(A)=0$. Unfortunately, if $\operatorname{rank}(A)\le n-2$, the adjoint is identically zero so this fails to yield an eigenvector. Here is the question: Is there a way to modify the above to get an explicit eigenvector in the null space when $\operatorname{rank}(A)\le n-2$? (What is desired is a formula rather than an algorithm.) Thank you","['linear-algebra', 'matrices']"
1414112,5-Card Poker Two-Pair Probability Calculation,Question: What is the probability that 5 cards dealt from a deck of 52 (without replacement) contain exactly two distinct pairs (meaning no full house)? Solution: $$\frac{\binom{13}{2}\binom{4}{2}\binom{4}{2}\binom{11}{1}\binom{4}{1}}{\binom{52}{5}} = 0.047539$$ Why doesn't ${13\choose 1}{4\choose 2}{12\choose 1}{4\choose 2}{11\choose 1}{4\choose 1}\over{52\choose 5}$ OR ${13\choose 3}{4\choose 2}{4\choose 2}{4\choose 1}\over{52\choose 5}$ work?,"['probability', 'poker', 'discrete-mathematics']"
1414129,Is every $\sigma$-algebra generated by a partition?,"I know that every finite $\sigma$-algebra is generated by a finite partition, but is every infinite $\sigma$-algebra also generated by ""kind of"" partition? Can anyone help provide a explanation or reference? Thank you!","['probability-theory', 'measure-theory']"
1414134,Prove $\sin^2 \theta +\cos^4 \theta =\cos^2 \theta +\sin^4 \theta $,"Prove $$\sin^2(\theta)+\cos^4(\theta)=\cos^2(\theta)+\sin^4(\theta)$$ I only know how to solve using factoring and the basic trig identities, I do not know reduction or anything of the sort, please prove using the basic trigonometric identities and factoring. After some help I found that you move the identity around, so: $\sin^2(\theta)-\cos^2(\theta)=\sin^4(\theta)-\cos^4(\theta)$ Then, $\sin^2(\theta)-\cos^2(\theta)=(\sin^2(\theta)+\cos^2(\theta))(\sin^2(\theta)-\cos^2(\theta))$ the positive sum of squares defaults to 1 and then the right side equals the left, but how does that prove the original identity?",['trigonometry']
1414138,A criteria for Jordan decomposition of a matrix over a general ring,"When I look at different proofs of the Jordan-Chevalley decomposition of a matrix, the minimal hypothesis I usually found is about the perfection of the field over which such decomposition occurs (e.g. Wikipedia article). But it seems to me that the gist of the proofs is not about the field but about the type of characteristic polynomial of the matrix. More precisely, by using the two following definitions for a general ring $A$ Definition 1 A $n\times n$ -matrix $M$ with coefficients in the ring A is semisimple if $A^n$ is a semi-simple $A[M]$ -module (here I am using the definition provided in the answer of @à² _à²  below) Definition 2 A separable polynomial $P$ of $A[X]$ is a polynomial whose discriminant is invertible in $A$ Then is the following statement true? A matrix $M$ over $A$ have a unique Jordan-Chevalley decomposition if its characteristic polynomial divides a power of a separable polynomial. Edit By Jordan decomposition I mean here the additive one, i.e. there exist $M_s$ and $M_n$ , two $n\times n$ $A$ -matrices respectively semi-simple and nilpotent, such that $$M=M_s+M_n$$ If it is not, what kind of general property should the ring $A$ have to make it true (like $A$ is a domain, etc.) Edit 2 I believe my definition 1 is equivalent in a domain to the fact that the matrix is diagonalizable in an algebraic extension of the fraction field which is equivalent to the fact that the matrix is cancelled by a separable polynomial (definition 2) in $A[X]$","['linear-algebra', 'matrix-decomposition', 'commutative-algebra']"
1414144,"Find all integers $m,n$ for which $m^2+n^2$ is a square and $\sqrt{\frac{2m^2+2}{n^2+1}}$ is rational","This is a repost of my old question here . The question is as follows: Find all integers m and n, such that $m^2 + n^2$ is a square and  $\sqrt{\frac{2(m^2+1)}{n^2+1}}$ is rational. I have made no progress on this problem since I last asked it. Tito gave a very nice answer for if $m,n$ were rational numbers, but it would be awesome if some people could provide some insight into this problem and solve it once and for all. It would also be really cool if we replaced that 2 with another number and investigated that as well. (For your reference, the original question was 2010 USAJMO problem 6. I tried to bash the question out and somehow I arrived at this problem. I suspect I made a mistake somewhere because the problem is no longer homogeneous.)","['number-theory', 'diophantine-equations']"
1414155,Is $\{\}$ equal to $\{ \{\} \}$? [duplicate],This question already has answers here : In naive set theory â = {â} = {{â}}? (2 answers) Closed 8 years ago . Is $\emptyset$ equal to $\{\emptyset\}$? I know an emptyset contains no elements.  So I feel like they would be equal. Can someone explain how they wouldn't be?,['discrete-mathematics']
1414204,Solving general linear ODE $\sum_{k=0}^n y^{(k)}=0$,"Is there a way to solve this general linear ODE:
$$\sum_{k=0}^n y^{(k)}=0$$ For the first few $n$ here are the solutions: $$\begin{array}{c|c}
n & y \\
\hline 0 & 0 \\ 
1 & c_1 e^x \\ 
2 & c_1 e^{x/2} \sin \left( \frac{\sqrt 3}2 x\right)+c_2 e^{x/2} \cos \left( \frac{\sqrt 3}2 x\right)\\ 
3 & c_1e^{-x}+c_2 \sin x + c_3 \cos x\\ 
4 &  c_1 e^{- \left(1+\sqrt5\right) x/4} \sin\left(\sqrt{\frac{5-\sqrt5}8} x\right)+c_2e^{- \left(1-\sqrt5\right) x/4} \cos\left(\sqrt{\frac{5+\sqrt5}8} x\right)+c_3 e^{- \left(1-\sqrt5\right) x/4} \sin\left(\sqrt{\frac{5+\sqrt5}8} x\right)+c_4 e^{- \left(1+\sqrt5\right) x/4} \cos\left(\sqrt{\frac{5-\sqrt5}8} x\right) \\
\vdots & \vdots
\end{array}$$ My attempt: We have $$\mathscr{L}\left\{\sum_{k=0}^ny^{(k)}\right\}=0\\
\sum_{k=0}^n \left( s^k \bar y-\sum_{i=0}^{k-1} s^i c_{i+1}\right)=0\\
\bar y = \frac{\displaystyle \sum_{k=0}^n \sum_{i=0}^{k-1}c_{i+1} s^i}{\displaystyle\sum_{k=0}^ns^k}\\
y = \mathscr L^{-1}\left\{\frac{\displaystyle \sum_{k=0}^n \sum_{i=0}^{k-1}c_{i+1} s^i}{\displaystyle\sum_{k=0}^ns^k}\right\}$$ I do not know how to evaluate that Laplace expression. Changing the summation index of the top sum and realising that the bottom is a geometric series we get that $$y=\mathscr L ^{-1}\left \{ \sum^n_{k=1}(n+1-k)\frac{(1-s)s^{k-1}}{1-s^{n+1}}\right \}$$
So I am just left with the evaluation of $$\mathscr L ^{-1}\left \{ \frac{(1-s)s^{k-1}}{1-s^{n+1}}\right \}$$
Mathematica cannot seems to evaluate this Laplace expression. Let $*$ define the convolution operator such that: $\mathscr L \left \{ f*g\right \}=\bar f \bar g$. Therefore $$\mathscr L ^{-1}\left \{ \frac{(1-s)s^{k-1}}{1-s^{n+1}}\right \}=\left[x^{-k} \left ( \frac1{\Gamma(1-k)}-\frac1{x\Gamma(-k)}\right )\right]*\mathscr L^{-1}\left\{\frac1{1-s^{n+1}}\right\}$$
Now I am really stuck.","['laplace-transform', 'ordinary-differential-equations']"
1414210,How do I solve $(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0$,"$(x\cos y-y\sin y)dy+(x\sin y+y\cos y)dx=0$ ATTEMPT: Rearranging the terms: $(x\cos ydy+y\cos ydx) -y\sin ydy+x\sin ydx=0$ Dividing by $\cos x$ we get: $(xdy+ydx)-y\tan ydy+x\tan ydx=0$ $ d(xy)-y\tan ydy+x\tan ydx=0$ But i am not able to simplify it further. I also tried partial derivatives: Let $(x\sin y+y\cos y)=M(x,y)$ and $(x\cos y-y\sin y)=N(x,y)$ This is not an exact differential equation as $\frac{\partial M(x,y)}{\partial y} \ne \frac{\partial N(x,y)}{\partial x}$ But $\frac{\frac{\partial M(x,y)}{\partial y} - \frac{\partial N(x,y)}{\partial x}}{N(x,y)}=1$ which is independent of $ x, y$. Can i use this result to solve my problem?",['ordinary-differential-equations']
1414223,Differentiation under the integral sign when derivative exists only almost everywhere,"Regarding the Theorem 3 from here (or pdf ver. ). Let $X$ be an open subset of $\mathbb{R}$ , and $\Omega$ be a measure space. Suppose that a function $f\colon X\times\Omega\to \mathbb{R}$ satisfies the following conditions: $f(x,\omega)$ is a measurable function of $x$ and $\omega$ jointly, and is integrable over $\omega$ ,  for almost all $x\in X$ held fixed. For almost all $\omega\in\Omega$ , $f(x,\omega)$ is an absolutely continuous function of $x$ . (This guarantees that $\frac{\partial f}{\partial x}(x,\omega)$ exists almost everywhere.) $\frac{\partial f}{\partial x}(x,\omega)$ is ""locally integrable"" --- that is, for all compact intervals $[a,b]\subset X$ : $$
\int_a^b\!\!\int_{\Omega}\left| \frac{\partial f}{\partial x}(x,\omega) \right| d\omega dx<\infty.
$$ Then, $\int_\Omega f(x,\omega)d\omega$ is an absolutely continuous function of $x$ , and for almost every $x\in X$ , its derivatives exists and is given by $$
\frac{d}{dx}\int_{\Omega}f(x,\omega)d\omega
=
\int_{\Omega}\frac{d}{dx}f(x,\omega)d\omega.
$$ Question 1: I do not see how to show $\int_\Omega f(x,\omega)d\omega$ is absolutely continuous in $x$ . Question 2: I also wonder where I can find this result preferably in a book or a paper etc. ==================================================== Here is my thought:
Notations basically follow the plametmath note by Steven Cheng (the link above). Let $(\Omega,\mu)$ be the measure space and $(a,b):=X\subset\mathbb{R}$ . For $\mu$ -almost all $\omega\in \Omega$ , we assumed that $f(\cdot,\omega)$ is an absolutely continuous function in the first variable. Hence for a.e.- $x$ we have \begin{align}
f(x,\omega)&=f(a,\omega)+\int_{a}^x\frac{\partial}{\partial y}f(y,\omega)\,dy
\end{align} Letting $G(x):=\int_{a}^x\int_{\Omega}\frac{\partial}{\partial y}f(y,\omega)\,
d\mu(\omega ){d}y$ and $g(x):=\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)\,
d\mu(\omega )$ by virtue of Lebesgue differentiation theorem for the indefinite integral, for a.e. $x$ we have \begin{align}
\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega )
&=g(x)=
\frac{\partial}{\partial x}G(x)
=
\frac{\partial}{\partial x}\int_{\Omega}\int_{a}^x
\frac{\partial}{\partial y}f(y,\omega)\,
dyd\mu(\omega ).
\end{align} where the last equality is ok from the assumption $
\int_{\Omega}\int_{a}^x\left|\frac{\partial}{\partial y}
	f(y,\omega)\right|
\,dyd\mu(\omega )
	<\infty
$ . Adding $0=\frac{d}{d  t}\int_{\Omega}f(a,\omega)d\mu(\omega )$ to the above yields \begin{align}
\int_{\Omega}\frac{\partial}{\partial x}f(x,\omega)d\mu(\omega )
&=\frac{\partial}{\partial x}\left(\int_{\Omega}\left\{
\int_{a}^x
\frac{\partial}{\partial y}f(y,\omega)\,
dy+f(a,\omega)\right\}d\mu(\omega )
\right)\\
&=\frac{\partial}{\partial x}\int_{\Omega}
f(x,\omega)d\mu(\omega ).
\end{align} I think the RHS exists for almost every $x$ because LHS does, which is the second claim of Theorem 3. I am not sure if I can immediately say $\int_\Omega f(x,\omega)d\omega$ is AC in $x$ , which is the first claim of 3., Theorem 3.","['lebesgue-measure', 'measure-theory', 'solution-verification', 'real-analysis', 'reference-request']"
1414224,Product of non-normal subgroups equals the whole group,"Is there any example of two subgroups $H, K \le G$, none of which normal in $G$, such that $G = HK$?","['abstract-algebra', 'group-theory']"
1414236,Lie Derivative Equals to Lie Bracket,"I am reading the book Introduction to Smooth Manifold written by M.Lee. I am confusing with the concept of Lie derivative. We have $\mathcal{L}_XY=[X,Y]$. However we have $D_XY=X(Y^i)\frac{\partial}{\partial x^i}$ in Euclidean space, and $\mathcal{L}_XY=D_XY$. It is obvious that $$\mathcal{L}_XY-\mathcal{L}_YX=[X,Y]$$ Where do I make mistakes?",['differential-geometry']
1414237,What's wrong with this proof of SchrÃ¶der-Bernstein theorem?,"In V. A. Zorich's Mathematical Analysis I there is an exercise to Analyze the following proof of the SchrÃ¶der-Bernstein theorem: $(\operatorname{card} X \leq \operatorname{card} Y) \land (\operatorname{card} Y \leq \operatorname{card} X) \Rightarrow (\operatorname{card} X=\operatorname{card} Y).$ proof. It suffices to prove that if the sets $X$, $Y$, and $Z$ are such that $X \supset Y \supset Z$ and $\operatorname{card} X=\operatorname{card} Z$, then $\operatorname{card} X=\operatorname{card} Y$. Let $f : X \rightarrow Z$ be a bijection. A bijection $g : X \rightarrow Y$ can be defined, for example, as follows: $$
g(x) =
\begin{cases}
\ f(x)      & \text {if $x \in f^n(X) \setminus f^n(Y)$  for some $n \in \mathbb{N}$} \\
 x   & \text {otherwise} \\
\end{cases}
$$ Here $f^n=f \circ \cdots \circ f$ is the $n$th iteration of the mapping $f$ and $\mathbb N$ is the set of natural numbers. Though it's fancy, I didn't find there is any grate mistake in this proof. But the word ""analyze"" strongly implies there is something wrong in this proof. Is that because I have to apply AC (axiom of choice) for finding such $x \in f^n(X) \setminus f^n(Y)$ for some $n \in \mathbb{N}$? Or there's other thing going wrong here?","['elementary-set-theory', 'proof-writing']"
1414246,Every ordered field that has the least upper bound property is isomorphic to the real number system.,"Okay, so here's a theorem from Rudin: ""Every ordered field that has the least upper bound property is
isomorphic to the real number system."" Here's a definition: ""Ordered fields are isomorphic if there is a bijection between the underlying sets that preserves the field operations and the orders."" i.e. Ordered fields (F, +, Â·, â¤) and (K, â, â, â¼) are isomorphic if there exists a bijection h : F â K such that
 (a) for all x,y â F, h(x+y)=h(x)âh(y), 
 (b) for all x,y â F, h(xÂ·y)=h(x)âh(y), and 
 (c) for all x,y â F, if xâ¤y,then h(x)â¼h(y). Here are some things that I think are true: 1.) The field T with two elements {0, 1} is an ordered field that has the least upper bound property. Because T is finite, every nonempty subset of T has a maximum. Max(T) = LUB(T) for all subsets of T that have a maximum element. Therefore all subsets of T have a least upper bound in T. Thus T has the least upper bound property. 2.) Since T has the LUB property, it is isomorphic to the real # system and therefore there exists a bijection between the real numbers and {0, 1}. 3.) ...but since {0, 1} is finite and the real numbers are infinite, there can be no surjection from {0,1} -> real numbers. Therefore there's no bijection between the reals and {0, 1}. So... Which thing that I think is true is not actually true? Thanks!","['ordered-fields', 'analysis', 'real-analysis']"
1414279,Proving an inequality between $\frac 1{n+1}$ and $\frac 1n$ and a definite integral,"For all natural numbers $n$, prove that $$\frac 1{n+1} < \int_n^{n+1} \frac 1t \, dt < \frac 1n$$ I have tried working with $\frac 1{t+1} < \frac 1t < \frac 1{t-1}$ but this doesn't help. The result I obtained was $$\log (\frac{k+2}{k+1}) < \int_n^{n+1} \frac 1t \, dt < \log (\frac k{k-1})$$","['inequality', 'integration']"
1414281,"Show $k[x]\cap(y-x^2,z-x^3)={0}$ in $k[x,y,z]$. $ k$ field.","Let k be a field. How could I show that 
$k[x]\cap(y-x^2,z-x^3)={0}$  in $k[x,y,z]$. I understand that there's a whole algorithm I could go through with Grobner basis, elimination theorem etc. but is there a simple argument? This problem arises from finding the dimension of a twisted cubic in Algebraic Geometry. In fact, $k[x,y,z]/(y-x^2,z-x^3)\simeq k[x] $ just by the isomorphism theorem. But to show the isomorphism directly using the obvious map gets me to my original question. So this suggests my original question ought not be too hard yet it seems to me to be non-obvious. Any comments on this point would be appreciated. Thanks! Update : Thanks for your answers which I've upvoted, but my question is really how to solve the problem directly (e.g. perhaps by considering degrees of various variables) instead of appealing to isomorphism results which, as I mentioned above, I am aware of. And the second question is more philosophically why using isomorphism theorems would make things easier.","['ring-theory', 'algebraic-geometry']"
1414292,How do we call a pair of sets between which there is a bijection that need not have additional property?,"Let $A,B$ be sets and let $f: A \to B$. Then we say that $A,B$ are isomorphic under $f$ if $f$ is a linear function that maps $A$ onto $B$ in a one-to-one manner; that $A,B$ are homeomorphic under $f$ if $f$ is a continuous function that maps $A$ onto $B$ in a one-to-one manner; that $A,B$ are ($C^{r}$)-diffeomorphic under $f$ if $f$ is a $C^{r}$ function with $C^{r}$ inverse that maps $A$ onto $B$ in a one-to-one manner; and so on. I think it may be convenient to have a name for a pair of sets between which there is a ""simple"", need-not-have-additional-property bijection, for if not then one may have to use many words to articulate. I am not aware if there is already one such name, so I would like to know it if any exists and, if no such name exists, I would like to solicit some ideas for coining a new terminology, say simply calling such a pair of sets morphic under the bijection.","['elementary-set-theory', 'soft-question', 'math-history', 'terminology', 'functions']"
1414298,Range of a Rational Function,"How to find the Range of function  $$f(x)= \frac{x^2-3x-4}{x^2 - 3x +4}$$
I tried to equate the expression to $y$, then cross multiplied 
$$ y= \frac{x^2-3x-4}{x^2 - 3x +4}$$
$$ y(x^2 - 3x +4)= x^2-3x-4 $$
bought the terms to one side so it becomes a quadratic and made Discriminant to zero , but i cant seem to reach anywhere.. Any suggestions?","['rational-functions', 'quadratics', 'functions']"
1414365,"Exist basis, simultaneously upper-triangular?","Let $A, B \in M_n(\mathbb{C})$ be such that $\text{rank}(AB - BA) \le 1$. Does there exist a basis of $\mathbb{C}^n$ with respect to which $A$ and $B$ are simultaneously upper-triangular?","['linear-algebra', 'matrices']"
1414376,Minimum value function,"It's just a very simple question, is there a function defined and that tells you the minumum and maximum value of a list of variables, like:
min(4, 3) = 3
min(2, 19) = 2
max(1, 10, 3) = 10
Is that the way to write it? or is there a accepted way of doing it?
I'm just a high-schooler and I like playing with math sometimes and I wanted to know if there's a more formal way of writing min() and max(), so don't get mad if it's a very obvious thing to you. Thanks.","['absolute-value', 'functions']"
1414386,Tangent to dual curve.,"Let $C$ be a smooth projective plane curve, let $P \in C(k)$, and let $\ell$ denote the tangent line to $C$ at $P$. Let $C^*$ denote the dual curve to $C$, in the dual plane $(\mathbb{P}^2)^*$ (the plane that parameterizes lines in $\mathbb{P}^2$).Let $P^*$ denote the line in $(\mathbb{P}^2)^*$ that parameterizes the lines passing through the point $P$. Note that (by definition of $C^*$), the line $\ell$ corresponds to ta point of $C^*$. My question is, is $P^*$ the tangent line to $C^*$ at its point $\ell$? I am able to check that the answer is yes in the case of a conic, but I do not know how to show this in general.","['abstract-algebra', 'algebraic-geometry']"
1414393,Showing a subset of $S_n$ is a subgroup,"Let $P$ be the set of all the elements of $S_n$ which can be written as $\sigma\mu\sigma^{-1}\mu^{-1}$ for $\sigma, \mu \in S_n$. Show this is a subgroup. This doesnt seem to be as simple as rearranging elements and manipulating terms. I'm quite stuck. Can anyone help? I've tried rearranging $klk^{-1}l^{-1}mnm^{-1}n^{-1}$ to get in the form $\sigma\mu\sigma^{-1}\mu^{-1}$","['group-theory', 'finite-groups', 'permutations']"
1414394,Trying to Understand a Remark about Zariski Topology,"I'm reading some notes in which following remark is given: The Zariski topology is quite different from the usual ones. For example, on affine space $ \mathbb A^n$ a closed subset that is not equal to $ \mathbb A^n$ satisfies at least one non trivial polynomial equation and has therefore necessarily dimension less than $n$, so the closed subsets in Zariski topology are in a sense ""very small"". My questions are the following: What is the meaning of dimension here? What is the meaning of 'so the closed subsets in Zariski topology are in a sense ""very small""'? What are some other ""weird"" properties of the Zariski topology?","['zariski-topology', 'algebraic-geometry', 'general-topology', 'commutative-algebra']"
1414454,$\mathbb{C}$-algebra automorphism of $M_n(\mathbb{C})$ has form $X \mapsto AXA^{-1}$.,"As the title suggests, what is the easiest way to see that any $\mathbb{C}$-algebra automorphism of $M_n(\mathbb{C})$ has the form $X \mapsto AXA^{-1}$ for some fixed $A \in GL_n(\mathbb{C})$?","['abstract-algebra', 'linear-algebra', 'matrices']"
1414461,"Generalizing limits of sums, products, and quotients of sequences to abstract topological spaces?","Introductory real analysis books usually prove a list of properties about limits of sequences of real and complex numbers. Suppose $\lim x_n=x$ and $\lim y_n=y$. Then: $\lim (x_n+y_n) = x+y$ $\lim c x_n = cx$ (where $c \in F$ for some field $F$) $\lim x_n y_n = xy$ $\lim x_n^p=x^p$ $\lim \frac{1}{x_n} = \frac{1}{x}$ (provided for all $n$, $x_n \ne 0$ and $x \ne 0$) $\lim \frac{x_n}{y_n}=\frac{x}{y}$ (provided for all $n$, $x_n \ne 0$, $y_n \ne 0$, $x \ne 0$ and $y \ne 0$) However, these results can be generalized beyond sequences of real & complex numbers to more abstract sequences in more general metric and topological spaces. I would like to prove these results for an arbitrary metric space. However, the problem is, as far as I know, these operations (addition, multiplication and division) are only defined for numbers . So in an abstract metric or topological space, does $x_n+y_n$ have meaning? How would one generalize these properties to abstract metric and topological spaces? Should you start with topological field extensions, topological algebras over a topological field, normed algebras, or something else? And just how general can you make these results? What is the most general topological space in which the operations $x_n+y_n$, $x_n \times y_n$, $\frac{x_n}{y_n}$, etc. make sense and all 6 properties of limits hold? Thanks for your thoughts!","['limits', 'metric-spaces', 'abstract-algebra', 'sequences-and-series', 'general-topology']"
1414480,What is the probability that a natural number is a sum of two squares?,"Some natural numbers can be expressed as a sum of two squares: $$2=1^2+1^2$$
$$25=3^2+4^2$$
$$50=7^2+1^2$$ If one chooses a random natural number, what would be the probability that that number is a sum of two squares? Is it zero? I read about LagrangeÂ´s theorem on squares, but it looks it canÂ´t be useful here. NOTE 1: ""Square"" means ""square of a natural number"". NOTE 2: I am aware that the expression ""random natural number"" is not a strict math notion. However, as I said in a comment, one can adopt a reasonable strict definition, which is not difficult to devise at all. It is mentioned also in an answer below. NOTE 3: A related question on SE: How to determine whether a number can be written as a sum of two squares?","['number-theory', 'probability']"
1414540,If $N$ is nilpotent then there exists $A$ such that $A^2=I+N$,"Suppose $N\in M_{3\times 3}^{\mathbb{C}}$ is a nilpotent matrix. Prove that there exists $A\in M_{3\times 3}^{\mathbb{C}}$ such that $A^2=I+N$. Hint: find $A$ in the form $A=P(N)$ where $P$ is a polynomial in $\mathbb{C}[x]$. I really can't think of a way to prove this. I know that $N^3=0$, so I tried combining this fact with $A^2=I+N$. I deduced that $(A^2-I)^3=0$, and thus the minimal polynomial of $A$ must include the terms $(x-1)(x+1)$. Any suggestions?",['linear-algebra']
1414563,Divergence based robust inference,"The term 'divergence' means a function $D$ which takes two probability distributions $g,f$ as input and puts out a non-negative real number $D(g,f)$. I have learnt that the inference based on minimizing the following divergence is robust against those data points which are not compatible with the assumed model (called outliers ), for some specific range of $\alpha \in \mathbb{R}^n$. $$D_{\alpha}(g,f) = \frac{1}{\alpha-1}\log \int g^{\alpha}f^{1-\alpha}~dx,$$ where $g$ stands for the data driven density and $f$ stands for the model density (see, for example here ). There are several names to this $D_\alpha$ in the literature viz., Renyi divergence, power divergence, $\alpha$-divergence etc. Can someone give a justification of how this method is robust? Update: (as suggested by  @Bruce Trumbo) Some Preliminaries: Suppose that $x_1,\dots,x_n$ i.i.d. samples drawn according to a particular member of a parametric family of probability distributions $\mathcal{F} = \{f_\theta\}$ ( model ). (Let us assume, for simplicity, that the $f_{\theta}$'s have a common support set $\mathbb{X}$ which is finite and also that $\mathbb{X}$ is the undelying sample space.) Our objective is to choose a special member $f_{\theta^*}$ of $\mathcal{F}$ which ""best"" explains the observed samples. The Maximum Liklihood Estimate (MLE) is a widely used inference method which asks us to choose the $f_\theta$ for which $\Pi_{i=1}^n f_\theta(x_i)$ is maximum. Let $\hat{f}$ be the empirical measure of the $x_i$'s. Then observe that
\begin{eqnarray}
 \frac{\prod_{i=1}^n f_\theta(x_i)}{\prod_{i=1}^n \hat{f}(x_i)} & = & \prod_{x\in\mathbb{X}} \left(\frac{f_\theta(x)}{\hat{f}(x)}\right)^{n\hat{f}(x)}\\
& = &\exp\{-nD(\hat{f}\|f_\theta)\},
\end{eqnarray}
where $D(\hat{f}\|f_\theta)=\sum_x \hat{f}(x)\log(\hat{f}(x)/f_\theta(x))$ is the well-known Kullback-Leibler divergence. Hence, MLE is a minimizer of $D(\hat{f}\|f_\theta)$ over $\theta$. For minimization, $(\partial/\partial_\theta) D(\hat{f}\|f_\theta) = 0$. This implies $$\sum_{x\in \mathbb{X}} \hat{f}(x) \frac{\partial}{\partial\theta}\log f_\theta(x) = 0.$$
That is, $$\sum_{i=1}^n \frac{\partial}{\partial\theta}\log f_\theta(x_i) = 0,$$ called the score equation . Thus, one needs to solve the score equation for finding the MLE. However, MLE is not robust when few of the $x_i$ are outliers. The inference based on minimizing the power divergence is known to be robust against the outliers. Note that the power divergence $D_\alpha(\cdot\|\cdot)\to D(\cdot\|\cdot)$ as $\alpha\to 1$. I roughly remember that the ""improved"" (or generalized) score equation corresponding to the power divergence to be something like
$$\sum_{i=1}^n f_{\theta}(x_i)^c\frac{\partial}{\partial\theta}\log f_\theta(x_i) = 0,$$ for some $c>0$, potentially a function of $\alpha$. The thing I would like to know is whether there is any way by which this generalized score equation can be shown to be derived from the power divergence as in the case of MLE as the score equation is derived from the Kullback-Leibler divergence. PS: I asked a question along the same lines in stats.stackexchange which did not get much attention. The link is here .","['robust-statistics', 'information-theory', 'statistics', 'statistical-inference']"
