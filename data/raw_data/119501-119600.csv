question_id,title,body,tags
1782871,To Show Closedness of a Graph in an Application of Closed Graph Theorem,"Here's an old exam question I am struggling with: Let E be a Banach space and $ (x_n)_{n \in N} \subset E $ such that $
 \sum_{n=1} ^{\infty} | \langle x_n , x^* \rangle |  < \infty $ for all
  continuous linear functionals $ x^* \in E^* $. Show that then there exists a constant $ C < \infty $ such that $$
 \sum_{n=1} ^{\infty} | \langle x_n , x^* \rangle |  \leq C||x^*|| .$$ What I know is that I should show that the graph of a linear map $ T: E^* \rightarrow l^1 $ is closed and then the continuity of $ T $ would follow from the closed graph theorem. But the problem here is that I don't have any idea where to start showing closedness of the graph.",['functional-analysis']
1782872,Marginally Gaussian not Bivariate Gaussian - Ito Integral,"Let $(W_t)_{0\leq t\leq 1}$ be a Wiener process defined up to time $1$ on some probability space. Consider the random vector $$\left(W_{1},\int_0^1 \operatorname{sgn}(W_s) \, dW_s\right)=:(W_1,X_1)$$
where the integral expression is an Ito integral. Using properties of the properties of the Ito integral and the P. Levy characterization of Brownian motion, it is not hard to show that both marginals are $\mathcal{N}(0,1)$. Furthermore, it is claimed that this random vector is not bivariate Gaussian. I'm having trouble seeing this last bit. My attempt so far is as follows: Assume the contrary. Then since \begin{align*}
\operatorname{E}\left[W_1\int_0^1\operatorname{sgn}(W_s)\,dW_s\right] & = \operatorname{E}\left[\left(\int_0^1 \, dW_s\right)\cdot\left(\int_0^1 \operatorname{sgn}(W_s) \, dW_s \right)\right]\\[6pt]
& = \operatorname{E}\left[\int_0^1 \operatorname{sgn}(W_s) \, ds\right] \\[6pt]
&=\int_0^1 \operatorname{E} \left[\operatorname{sgn}(W_s)\right] \, ds\\[6pt]
&=0,
\end{align*}
where we use the Ito isometry, Fubini-Tonelli, and the symmetry of the normal distribution, we would then have that $W_1$ and $X_1$ are uncorrelated hence independent $\mathcal{N}(0,1)$ random variables. I am not sure where to rigorously go from here. One thing that's clear to me from John Dawkins comment is that the process $(W_{t},X_{t})_{0\leq t\leq 1}$ is not Gaussian, since
$$aW_{t}+X_{t}=\int_{0}^{t}(a+\text{sgn}(W_{s}))dW_{s}$$
and therefore
$$\langle{aW+X}\rangle_{t}=\int_{0}^{t}(a+\text{sgn}(W_{s}))^{2}ds,$$
which is not deterministic. Since Gaussian martingales have deterministic quadratic variation, the process is not Gaussian. So there is some collection of times $t_{1}<\cdots<t_{n}$ such that the finite-dimensional distribution is not multivariate Gaussian. But why does it follow that $(W_{1},X_{1})$ is not Gaussian?","['stochastic-processes', 'probability-theory', 'probability', 'stochastic-integrals', 'brownian-motion']"
1782926,Multiplying A Coefficient by an Indexed Multiplier using Generating Functions,"If I have a particular exponential generating function, $$G(x)=\sum_{n=0}^\infty a_n\frac{x^n}{n!}$$ then what would be the generating function for $$H(x)=\sum_{n=0}^\infty (n+1)a_n\frac{x^n}{n!}$$ in terms of $G(x)$?  I know that I can change it to $$H(x)=\sum_{n=0}^\infty na_n\frac{x^n}{n!}+\sum_{n=0}^\infty a_n\frac{x^n}{n!}=J(x)+G(x)$$ Now I know that 
$$G'(x)=\sum_{n=0}^\infty na_n\frac{x^{n-1}}{n!}$$ and so $$H(x)=xG'(x)+G(x)$$ Is there a way to write $H$ without the derivative of $G$? I would like $H$ as a function in terms of strictly $G$, and without a $G'$ term.","['derivatives', 'real-analysis', 'generating-functions', 'sequences-and-series']"
1782941,$e^z-P(z)$ has infinitely many zeros,"If $P\in\mathbb{C}[z]$ is a non-zero polynomial, prove $f(z):=e^{z}-P(z)$ has infinitely many zeros. I've made some progress so far, but I still have a step missing. Here is where I'm at: Suppose $f$ has finitely many zeros, namely $z=a_1,\,...\,,a_n \neq 0$ and possibly $z=0$ . Since $e^z$ and $P(z)$ are entire functions with orders of growth $1$ and $0$ respectively, then $f$ has order of growth $1$ . From Hadamard's factorization: $$f(z)=e^{\alpha z+\beta}z^m\prod_{k=1}^{n}\left(1-\frac{z}{a_k}\right) e^{\left(\frac{z}{a_k}\right)} \tag{$*$}$$ (where $\alpha$ , $\beta$ are complex constants and $m:=$ multiplicity of $z=0$ ). Rearranging $(*)$ , we get: $$f(z)=e^{\gamma z+\beta}Q(z)\tag{$**$}$$ (where $\gamma = \alpha +\sum_{k=1}^{n}\frac{1}{a_k}$ is a constant and $Q(z)=z^m\prod_{k=1}^{n}\left(1-\frac{z}{a_k}\right)$ is a polynomial of degree $m+n$ ) Now, for $d:=\text{deg}(P)$ , notice $f^{(d+1)}(z)=e^{z}$ . Taking the $(d+1)$ -th derivative in $(**)$ , we get $e^{z}=e^{\gamma z+\beta}R(z)$ , where $R$ is a polynomial of degree $m+n$ . If $m+n\geq 1$ , $R$ has at least one root, while $e^{z}$ has none (absurd), so $m+n=0\Rightarrow m=n=0\Rightarrow f$ has no zeros. Conclusion: either $f$ has infinitely many zeros or none at all. Now how can I prove it has at least $1$ zero?",['complex-analysis']
1782950,"The notion of ""infinitely differentiable""","Wiki takes me to the section ""smoothness"" which I don't entirely get, it's just too much stuff for me. My question is, what exactly is it? An infinitely differentiable function is one that is infinitely differentiable. Literally, that's what my lecture note says. So I thought the notion is very obvious, so what I had in mind was Not infinitely differentiable $\to$ those that reaches a ""dead end"" $f(x)=a_0+a_1x+a_2x^2+...+a_nx^n$ since $n+1$ differetiations will make it $0$ and I thought this can be considered a ""dead end"" for instance. Then, Infinitely differentiable $\to$ doesn't have a dead end $f(x)=\sin{x}$ or $f(x)=e^{nx}$ say. These can be differentiated as many times as one wishes, but it doesn't have a dead end. It keeps changing to another function, coming back, or staying like itself. However, it seems like my understanding has been wrong. In a probelem's solution in class, I have been told, $f(x)=xy$ is a polynomial so it is infinitely differentiable. In my way of thinking, polynomials meet a dead end so they are not. But my class has claimed otherwise. So what exactly are functions that are infinitely differentiable, and what are not? Sure, $0$ differentiated then it is $0$ so we can differentiate it a million or gazzilion times in a sense. But what functions aren't? Functions that cannot be defined at a certain point then?","['derivatives', 'calculus']"
1782952,Monty Hall Problem extended,"After seeing the popularity of the standard $3$ door problem, Monty thought to put a twist in the story. There are $N$ doors, $1$ car, $N-1$ goats. We need to choose any one of the doors. After we have chosen the door, Monty deliberately reveals one of the doors that has a goat and asks us if we wish to change our choice. After we decide our choice, Monty then again reveals one more door that has a goat and asks us if we wish to change our choice (both 1st and 2nd). This goes on. What strategy should we follow? Keep switching? And if we keep switching, is it okay to switch to some of the previous choices (provided they are still not revealed!!)","['probability-theory', 'monty-hall', 'probability', 'puzzle', 'recreational-mathematics']"
1782953,Pollard Rho - DLP Algorithm Implementation,"I am working with Pollard Rho Algorithm DLP. I have developed in Java and Python the way to calculate the table to find the collisions, and then using congruences and some others tricks  I am getting a solution for $g^x  = h \bmod{p} $ (DLP) So with normal numbers I can find solutions of $x$ for example $19^x = 24717 \bmod{48611}$, (we can see that $48611$ is prime number) in this case $x = 37869$ (this is the example of the book ""An Introduction to Mathematical
  Cryptography""of Hoffstein, Pipher and Silverman ) but for example if I try to do: $2^x = 2063614956960 \bmod{2199023255561}$ In Python is no working and in Java is taking long time (with some silly calculus calculus it will take 15 days to check all the options). However I have realised that $2199023255561$ is no prime, is generated by $11$, that is, $2199023255561 \bmod{11} = 0$ and $2063614956960 \bmod{11} = 7$ so I am remembering some basic concepts of discrete maths, but I am not sure which theorem is, maybe Euler o Lagrant (I am guessing), I would like to reduce the equation to smallest possible like : $2^x = 7 \mod{11}$ this equation I can process quickly and the result is $ x = 7$ my question is, if my logic is ok and can I do it ? some mathematic rules is supporting me? and If I am wrong (is more possible), Someone can help how can I reduce the equation?","['logarithms', 'algorithms', 'cryptography', 'discrete-mathematics']"
1782989,Differentiating the pull-back of a one-form,"Let $\Omega$ be an open subset of a vector space $V$ and let $\alpha\colon \Omega\to V^*$ be a one-form on $\Omega$. Assuming that $\alpha$ is differentiable, then for any $x\in \Omega$, $D\alpha (x)\colon V\to V^*$ is a linear map. Hence we can think of this derivative as a bilinear map $D\alpha(x)\colon V\times V\to \mathbb{R}$. We say that $\alpha$ is symmetric if the bilinear form $D\alpha$ is symmetric for all $x$. I'm trying to understand the following fact from a book that I'm reading: Let $f\colon \Omega'\to \Omega$ be a smooth map from an open subset $\Omega'$ of a vector space $V'$ to $\Omega$. Then If $\alpha$ is symmetric, then so is the pull-back $f^*\alpha$. Here the pull-back is defined by $(f^*\alpha)(x)(v)=\alpha(f(x))Df(x) v$.   $\quad$ (I) My book proves this as follows: $$D(f^*\alpha)(x)(v_1, v_2)=D\alpha(f(x))(Df(x) v_1, Df(x) v_2)$$ and
  clearly this is symmetric in $v_1$ and $v_2$ if $D\alpha(f(x))$ is
  symmetric. I don't quite see where the displayed equality comes from. (Using coordinates shouldn't be a good idea because symmetry seems to be coordinate dependent.) I believe the source of my difficulty lies in not feeling comfortable with differentiating something like $S(x)\circ T(x)$. Here $S(x)$ and $T(x)$ are both functions (after evaluation) and $S(x)\circ T(x)$ is the composition of two functions. This is perhaps too elementary, but any help with clarification is appreciated. Thanks!","['differential-forms', 'differential-geometry', 'vector-analysis']"
1783003,Characteristic Polynomial and Minimal Polynomial,"(True or false): Suppose $A$ is an $n \times n$ matrix and $A^{k} = 0$ for some k. Then the characteristic polynomial is $x^n$ I am inclined to believe this is true since if $x^{k} = 0$ then the minimal polynomial divides $x^k$ thus implying that the minimal polynomial has the form of $x^a$ for some $a$. Which implies that the characteristic polynomial has the form of $x^b$ for some $b$ since the minimal polynomial divides the characteristic polynomial.
I know that the characteristic polynomial for an $n \times n$ matrix will have the form of $x^n -trace(A)x^{n-1} + ....+(-1^n)det(A)$ So if the characteristic polynomial has the form $x^b$ then for an $n \times n$ matrix it will have the form $x^n$ Another question is  (True or false) Suppose $A^k = 0$ for some $k$. Then the minimal polynomial of A is $x^k$ I am inclined to believe this is false since the question doesnt really state weather $k < n$ or $k > n$ or if $k$ is the first number which sets $A$ to zero. Thus there can be an $a < k$ where $A^a = 0$ Is my reasoning sound for both examples?","['matrices', 'linear-algebra']"
1783028,unfolding of a recurrence,"I've been reading the book ""Concrete Mathematics"" from Graham et. al.
And there is a relation (on pg. 27) $s_n = s_{n-1}a_{n-1}/b_n$, and authors point that this relation can be unfolded, resulting with $$s_n = \frac{a_{n-1} \ldots a_1}{b_n \ldots b_2}$$ I don't understand why authors stops unfolding at n = 2 ? Shouldn't it be 
$$s_n = \frac{a_{n-1} \ldots a_0}{b_n \ldots b_1}$$ Thanks in advance","['recurrence-relations', 'discrete-mathematics']"
1783034,Why is $f(4)$ the area under $f'(x)$ specifically from $0$ to $4$ and not for ex from $1$ to $4$ or $2$ to $4$?,"I've seen the geometric argument for why any differentiable function $f(x)$ gives the rate of change of the area under its own curve to $x$ for a specific input $x$, and it makes sense to me. It also makes sense that if $f(x)$ gives you $\frac{dA}{dx}$, then $F(x)$, its antiderivative, should give you the actual area under the curve of $f(x)$. What I don't understand however, is why does $F(x)$ give you the area from $\textbf{0}$ specifically to $x$ given a single input $x$? In other words, where does that $0$ come from? Because $f(x)$ gives you $\frac{dA}{dx}$ at $x$ regardless of how much area comes ""before"" (for smaller values of $x$). How do we decide that $F(x)$ starts computing the area under the curve of $f(x)$ from $0$ and not some other starting point? Obviously it's very convenient but it seems pretty arbitrary to me.","['integration', 'calculus', 'area']"
1783064,"Is there a rigorous way of saying, ""if $G$ and $H$ are isomorphic then $G$ and $H$ share all the same properties""?","So, most of us have been in an introductory algebra course and proved basic facts about isomorphic groups (or rings, modules, etc., we'll use groups as the example and call them $G$ and $H$), such as ""$G$ and $H$ have the same number of elements of order $n$ for any $n\in\mathbb{N}$"", or ""$G$ is abelian if and only if $H$ is abelian"", and so on. After a while, we stop bothering to prove that properties we care about are always preserved by isomorphism. For instance, we'd find it very natural to say that if $G$ is isomorphic to a nilpotent group then $G$ is nilpotent, and use it without hesitation in a proof. And indeed, we're never failed because isomorphisms do indeed encode the ""sameness"" of two groups. But I'm wondering if there is still some rigorous way of saying that these types of properties will always be shared? I'm not doubting that this is true, but it's just something I was curious about.","['abstract-algebra', 'group-theory']"
1783083,Example of a Symbol: Connection.,"I'm trying to get more intuition for the symbol of a differential operator. In particular, I've tried the example of a connection. What is the most efficient way to compute the symbol for, say, a linear connection $\nabla$ on a vector bundle $E\to X$, given locally by
$$\nabla s^{\alpha} =\sum_\beta(\partial_\beta-A_\beta) s^\alpha?$$","['ordinary-differential-equations', 'differential-geometry']"
1783098,When is a balance assumption consistent?,"From Asymptotic analysis and perturbation theory by Paulsen: Find the behavior of the function defined implicitly by $$x^2+xy-y^3=0$$ as $x\to\infty$ . [...] The ﬁnal case to try is to assume that $xy$ is the smallest term. Then $x^2 ∼ y^3$ , which tells us that $y ∼ x^{2/3}$ . To check to see if this is consistent, we need to check that $xy \ll x^2$ . Indeed, $xy ∼ x^{5/3}$ which is smaller than $x^2$ as $x → ∞$ . Note that in this book $f\ll g \iff f\in o(g)$ . I don't understand why it says that the assumption is consistent if $xy \ll x^2$ . Say, $$f(x)+g(x)+h(x)=0$$ and we assume $h \sim g$ . Then we immediately have $f\in o(g)$ and $f\in o(h)$ . So, if I'm not mistaken, as soon as we have $x^2 ∼ y^3$ , it follows $xy \in o(x^2)$ . Moreover what does it mean to assume ""that $xy$ is the smallest term"" if not $xy \in o(x^2)$ ? The whole reasoning seems circular. What am I missing here?","['perturbation-theory', 'asymptotics', 'functions', 'limits']"
1783100,What would an infinite dimensional projective space look like as a scheme?,"In topology, we can construct $\mathbb{CP}^\infty$ as the direct limit of $\cdots\rightarrow \mathbb{CP}^n \rightarrow \mathbb{CP}^{n+1}\rightarrow \cdots$ with the embedding given by $[x_0: x_1: x_2: x_3: \cdots: x_n] \mapsto [x_0: x_1, x_2: x_3: \cdots: x_n: 0]$. Is there a similar construction of an infinite-dimensional version of projective space as a scheme? One natural choice would be $\text{Proj } \mathbb{C}[x_0, x_1, \ldots]$, but this doesn't quite work. This ring is the direct limit of $\mathbb{C}[x_0, x_1, \ldots, x_n] \rightarrow \mathbb{C}[x_0, x_1, \ldots, x_{n+1}]$ with the natural inclusions, so the corresponding projective scheme should be the inverse limit of the $\mathbb{CP}^n$. (This isn't quite true because the maps aren't actually maps of graded rings, and only induce rational maps on the projective spaces, but it at least works in the affine category: $\text{Spec } \mathbb{C}[x_0, x_1, \ldots]$ is a variety over $\mathbb{C}$ with points given by arbitrary infinite sequences of numbers in $\mathbb{C}$, which is the inverse rather than direct limit of the affine spaces). What happens if we take $\text{Proj } R$ (or if it's easier, $\text{Spec } R$), where $R$ is the inverse limit of $\mathbb{C}[x_1, \ldots, x_{n+1}] \rightarrow \mathbb{C}[x_1, \ldots, x_n]$ given by $x_{n+1} \mapsto 0$? The inverse system corresponds to the direct system of $\mathbb{CP}^n$ given above, but perhaps the limit does not give quite the right thing. The MO post here discusses this problem, mentioning why $\text{Proj } \mathbb{C}[x_0, \ldots]$ doesn't work, but it does not say what goes wrong when you take $\text{Proj } R$ EDIT I believe $R$ can be described as follows: elements of $R$ are infinite sums of monomials of finite degree in the $x_i$, such that for any $n$, the set of non-zero terms involving $x_1, \ldots, x_n$ is finite.","['algebraic-topology', 'category-theory', 'projective-space', 'algebraic-geometry']"
1783140,Proving $\frac{d}{dx}x^2=2x$ by definition,"I did the following proof earlier and just wanted conformation as to whether it works. The question was to show
$$\frac{d}{dx}x^2=2x$$
by the difference-quotient definition of a derivative, and then prove that limit with the $\epsilon$, $\delta$ definition for limits. I said that
$$\frac{d}{dx}x^2=\lim_{h\to0}\frac{(x+h)^2-x^2}{h}=\lim_{h\to0}\frac{h^2+2xh}{h}$$
Then we assert that this is equal to $2x$ which we show is true using the $\epsilon$, $\delta$ definition. We must show that for all $\epsilon > 0$, there exists $\delta>0$ such that for all $|h-0|<\delta$ we have
$$\left|2x-\frac{h^2+2xh}{h}\right|<\epsilon$$
Assuming $h\neq 0$ this simplifies to
$$\left|2x-h-2x\right|=|-h|=|h|<\epsilon$$
So we're left with that for all $|h|<\delta$ we must have $|h|<\epsilon$, which is true for all $\epsilon$ if $\delta=\epsilon$. Hence we can conclude the limit does equal $2x$ and
$$\frac{d}{dx}x^2=2x$$ I know its kind of a dull question, so if you have any cool insights or tips to share that'd be appreciated!","['derivatives', 'limits', 'proof-verification', 'calculus', 'epsilon-delta']"
1783200,Continuous for each variables does not implies continuous,"Prove or disprove the following statement: Statement. Continuous for each variables, when other variables are fixed, implies continuous? More clearly, prove or disprove the following problem: Let $\displaystyle f:\left[ a,b \right]\times \left[ c,d \right]\to \mathbb{R}$ for which: For every $\displaystyle {{x}_{0}}\in \left[ a,b \right]$, $\displaystyle f\left( {{x}_{0}},y \right)$ is continuous on $\displaystyle \left[ c,d \right]$ respect to variable $ \displaystyle y$. For every $ \displaystyle {{y}_{0}}\in \left[ c,d \right]$, $ \displaystyle f\left( x,{{y}_{0}} \right)$ is continuous on $ \displaystyle \left[ a,b \right]$ respect to variable $\displaystyle x$. Then $\displaystyle f\left( x,y \right)$ is continuous on $ \displaystyle \left[ a,b \right]\times \left[ c,d \right]$. ? https://hongnguyenquanba.wordpress.com/2016/05/12/problem-6/","['multivariable-calculus', 'examples-counterexamples', 'continuity', 'functions']"
1783225,Example of Pairwise Independent but not Jointly Independent Random Variables?,"I am asked to: Find a joint probability distribution $P(X_1,\dots, X_n)$ such that $X_i
  , \, X_j$ are independent  for all $i \neq j$ , but $(X_1, \dots , X_n)$ are
  not jointly independent. I have no idea where to start, please help.","['independence', 'probability']"
1783271,Conjecture function $g(x)$ is even function?,"Let $f,g:R\to R\setminus\{0\}$ and $\forall x,y\in R$,such
$$\color{crimson}{f(x-y)=f(x)g(y)-f(y)g(x)}$$ I have  prove  the function $\color{crimson}f$ odd function. because let $y=0$ we have
$$f(x)=f(x)g(0)-f(0)g(x)\tag{1}$$
Let $x=0,y=x$ we have
$$f(-x)=f(0)g(x)-f(x)g(0)\tag{2}$$
by $(1),(2)$ ,then $$f(x)=-f(-x)$$ Conjecture： $\color{crimson}{g(x)}$ is even function For this function $g(x)$ is even problem ,I don't have any idea to prove it.But I think is right,because $$\color{blue}{\sin{(x-y)}=\sin{x}\cos{y}-\sin{y}\cos{x}}$$
$$\color{crimson}{\sinh{(x-y)}=\sinh{x}\cosh{y}-\sinh{y}\cosh{x}}$$",['functions']
1783283,"Function that satisfies the given (x,y) values","I am trying to come up with a function that (approximately) satisfies these $(x,y)$ values. $(2, 2), (3, 3), (4, 4), (5, 5), (6, 6), (7, 7), (8, 6), (9, 5), (10, 4), (11, 3), (12, 2), (13, 1), (14, 2), (15, 3), (16, 4), (17, 5), (18, 6), (19, 7), (20, 8), (21, 7), (22, 6), (23, 5), (24, 4), (25, 3), (26, 2), (27, 3), (28, 4), (29, 5), (30, 6), (31, 7), (32, 8), \text {(other, anything)}$ I want to create a continuous function $f(x)$ which takes $x$ as input and outputs the corresponding $y$ value as given above. If possible, I don't want a piecewise function. Is there a technique to do this?","['functions', 'interpolation']"
1783298,Surface area of Convex bodies contained in one another,"Suppose we have two compact convex bodies one contained in the other in $\mathbb{R}^n$, $C\subset D\subset \mathbb{R}^n$. Does it follow that the ($n-1$ dimensional) surface area of $C$ is less than $D$? If so is there a natural sequence of $n-k$  dimensional quantities ($g_0=$Volume, $g_1=$Surface area,...) such that whenever $C\subset D\subset \mathbb{R}^n$ (where $C$ and $D$ are again compact) and $n>k$ we have $g_k(C)>g_k(D)$?","['real-analysis', 'integral-geometry', 'inequality', 'convex-geometry', 'differential-geometry']"
1783299,What happens when $X$ equals its own expected value?,"Let $X$ denote a random variable with “moments” $M_1:=E(X)$, $M_2:=E(X^2)$, . . .
(the first four of which, at least, are assumed to be finite). Show that $M_4+6M_2(M_1)^2\ge 4M_3(M_1)+3(M_1)^4$ Under what circumstances would you get equality? I figured out the first part which is just basically $E((X-E(X))^4)\ge 0$.  But the second part when $E((X-E(X))^4)=0$ I suppose this makes $X-E(X)=0$ and I got $X=E(X)$ which I'm not sure what this means. Does that mean all possible $X$ are the same with the same probability? The answer might be simple but somehow I am confused.",['probability']
1783301,Associativity of $x*y := \frac{xy}{x+y+1}$,"In order to show something is associative one must show that $(x*y)*z$ = $x*(y*z)$. I want to show that $x * y = \frac{xy}{x+y+1}$ is associative. This is for self-study (I'm learning algebra over the summer) and need help finishing the proof. Below are my (hopefully not incorrect) steps, 1) $x*(y*z)$ = $x * \left(\frac{yz}{y+z+1}\right)$ Applying the binary operation * I obtain 2) $\frac{x\left(\frac{yz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ Here is where my first question comes: Question 1: Do I multiple x (in numerator) using the definition of *, or do I multiply x the ""normal"" way? If I multiply x the ""normal"" way, then why do I not use the * definition recursively for each application of *? Assuming I do multiplication the ""normal"" way I obtain, 3) $\frac{\left(\frac{xyz}{y+z+1}\right)}{x+\frac{yz}{y+z+1}+ 1}$ I know in order to add fractions they must have the same denominators. This means the denominator for each term needs to be y+z+1. So this brings me to 4) $\frac{\left(\frac{xyz}{y+z+1}\right)}{{y+z+1}(x+\frac{yz}{y+z+1}+ 1)}$ Here is where I become and more uncertain... Question 2: Again, do I use the * definition of multiplication or do I use the ""normal"" definition of multiplication for multiplying the y+z+1 across and why? Again, assuming I use ""normal"" multiplication I obtain 5)  $\frac{\left(\frac{xyz}{y+z+1}\right)}{\frac{xy+xz+x+yz+y+z+1)}{y+z+1}}$ From here I'm suck and not sure how to reduce this any further. Question 3: How do I proceed from step 5) to the conclusion? I believe once I understand this, I will be able to show $(x*y)*z$,I would like to see the rest of the proof from where I am stuck. Thank you very much in advance for the help. Disclaimer: A similar question has been asked & answered in this thread , but I have different questions than the ones they posed.","['abstract-algebra', 'fractions', 'binary-operations']"
1783343,Prove that identity element is unique,During an exam I tried to prove that the identity element of group (G.•) is unique. I approached this way: Suppose there are two identity elements $e_1$ and $e_2$. Then: $a^{-1}•a=e_1$ $a^{-1}•a=e_2$ $e_1=e_2$ What are the potential flaws in my proof?,"['group-theory', 'discrete-mathematics']"
1783370,Question about the rational normal curve and different representations of it.,"I know the rational normal curve as the image of a polynomial map \begin{gather}
\phi:K\rightarrow K^n\\
\phi(t)=(t,t^2,\dots,t^n)
\end{gather} My question is proving the variety defined by the set of homogeneous quadratics obtained by taking all possible $2 \times 2$ sub-determinants of the matrix 
\begin{bmatrix}
    x_0  & x_1 & x_2& \dots &x_{n-1}\\
    x_1 & x_2& x_3& \dots &x_{n}
\end{bmatrix} 
is equivalent to the parametric representation. What I have done so far was look at a special case 
\begin{bmatrix}
    x_0  & x_1 & x_2\\
    x_1 & x_2& x_3
\end{bmatrix}
and the map $$\phi(t)=(t,t^2,t^3,t^4).$$ Clearly when I take the sub-determinants the ideal of $\varphi(K)$ is 
$$I =\langle x_0x_2-x_1^2,x_0x_3-x_1x_2,x_1x_3-x_2^2\rangle.$$ 
We can see that the image of the map is contained in the ideal generated by the sub determinants, but I am not sure how to argue that the image contains the ideal. Maybe helpful: this is in I.V.A. by Cox, Little and O'Shea as a part of a problem in 8.4, problem 11, part d.","['algebraic-curves', 'abstract-algebra', 'algebraic-geometry', 'commutative-algebra']"
1783380,$f$ is an entire function such that $f(0)=0$,"Let $f$ be a non-constant entire function satisfying the following conditions:
$f(0)=0$ and for each $N \gt 0$ the set $\{z \mid \left| f(z)\right| < N\}$ is connected. Prove that $f(z)=cz^n$ for some constant $c$ and positive integer $n$. $f(0)=0$ implies there is $r>0$ such that $f(z)\neq 0$ for any $z\in \{z: 0<|z|\leq r\}$ All I can see it $0$ is the only root of $f(z)$. Suppose not, for small $M$, 
the set $\{z: |f(z)|<M\}$ contains disjoint open sets which contradicts connectedness of the set. So, there is only one root. So, $0$ is the only root of $f(z)$. Can we now say that we are forced to have $f(z)=cz^n$ for some $n$? I could not see more than this.. please give some hints.","['complex-analysis', 'entire-functions', 'connectedness']"
1783390,Why are the four fundamental subspaces fundamental?,"The four fundamental subspaces in linear algebra, as discussed by Gilbert Strang [ 1 ], are the kernel, image, dual space kernel, and dual space image (nullspace, column space, left nullspace, row space). He calls the relationship between these ""the fundamental theorem of linear algebra"". Of course the kernel and image are needed for the first isomorphism theorem, which is fundamental to understanding what a homomorphism is. But why are the dual space image and kernel important and ""fundamental""? A similar question was asked [ 2 ] with one answer stating In short, these four spaces (really just two spaces, with a left and a
  right version of the pair) carry all the information about the image
  and kernel of the linear transformation that A is affecting, whether
  you are using it on the right or on the left. but this entirely avoids the issue of why the image and kernel of the adjoint of a transformation are important to understanding the transformation. Strang's paper [ 1 ] seems to suggest that these subspaces may provide intuition for the singular value decomposition, but I haven't studied the singular value decomposition (SVD) yet. I'm also not sure if the SVD is fundamental enough to justify classifying the dual space kernel and image as ""fundamental"". edit: Thanks for the replies :) To the men and women of the future that take the path I have trodden on, I'll leave some links that I found particularly helpful [ 3 , 4 , 5 ].","['functional-analysis', 'abstract-algebra', 'linear-algebra']"
1783392,number of integer solutions to $2x_1 + x_2 + x_3 = n$,"I'm working on a problem for which I need to efficiently compute the number of integer solutions to equations of the form $x_1 + \cdots + x_k = n$ with some subset of $\{x_1, \dots, x_n\}$ equivalent. For example, the problem stated in the title: $x_1 + x_2 + x_3 + x_4 = n$ with $x_1 = x_2$ I know how to find the number of integer solutions to such an equation without the added restraint of equivalent variables. I'm also familiar with adding restraints of the form $x_i \geq m$. But I don't see how to adapt that method here. In my combinatorics book, similar problems are tackled using generating functions. I believe that the generating function for my equation would be: $$(1+x^2 + x^4 + \cdots)(1+x + x^2 + \cdots)^2 = \dfrac{1}{1-x^2} \dfrac{1}{(1-x)^2}$$ But unless there is a way to put this into the form $\sum_{n=0}^{\infty} a_n x^n$ and thus recover $a_n$ I don't see how to make use of it. Ideally, I would like to find a method that works for $2x_1 + x_2 + x_3 = n$ and which can also be extended to cases with more variables and different subsets of equivalent variables.","['diophantine-equations', 'combinatorics']"
1783397,Complex ($\mathbb C$) least squares derivation,"I know how to derive the least squares in the real domain. If a tall matrix $A$ and a column vector $b$ are real,
then the solution of the least squares problem $Ax = b$ can be derived as: $$\begin{align}
\{E(x)\}^2 &= ||Ax - b||^2 \\
&= (Ax-b)^T (Ax-b) \\
&= x^T A^T Ax - x^T A^T b - b^T Ax + b^T b \\
&= x^T A^T Ax - 2 x^T A^T b + b^T b \qquad (\because (Ax)^T b = b^T (Ax))
\end{align}$$ Differentiating both sides with respect to $x$, $$\begin{align}
\frac{d \{E(x)\}^2}{dx} &= 2A^T Ax - 2 A^T b
\end{align}$$ Setting $\frac{d \{E(x)\}^2}{dx} = 0$ to find when we get the minimum $E(x)$, $$
2A^T Ax - 2 A^T b = 0 \\
A^T Ax = A^T b \\
x = (A^T A)^{-1} A^T b
$$ Now, we turn to the complex-valued situation. Assume $A$ and $b$ are complex, $$\begin{align}
\{E(x)\}^2 &= ||Ax - b||^2 \\
&= (Ax-b)^H (Ax-b) \\
&= x^H A^H Ax - x^H A^H b - b^H Ax + b^H b \\
\end{align}$$ Here, I have some problems. First, $x^H A^H b \neq b^H Ax$ unless $(Ax)^H b$ is real. Most of all, I don't know how to differentiate the complex matrices above. How to proceed the derivation? There are plenty of derivations in the real domain in Google, but I couldn't find detailed explanation of the general complex case.","['matrix-calculus', 'least-squares', 'linear-algebra']"
1783418,All subgroups normal $\implies$ abelian group,"This is , I think an easy problem just that I am not getting the catch of it. How to show whether or not the statement is true? All subgroups of a group are normal$\implies$ the group is an abelian group? I have been able to show the other way round.","['abelian-groups', 'normal-subgroups', 'group-theory']"
1783549,Expanding a norm over a given space,"Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ Prove that $\|x\|_q \leq \|x\|_p$ for all $x \in \Phi$ and there is no constant $C$ such that $\|x\|_p \leq C\|x\|_q$ for all $x \in \Phi$ How can I show that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$? this proves both inequations but I cant manage to epand and solve the sup function. What is the system here? is it at all a good direction?","['functional-analysis', 'supremum-and-infimum']"
1783555,Resolving $\sec{x}\left(\sin^3x + \sin x \cos^2x\right)=\tan{x}$,"Going steadily through my book , I found this exercise to resolve $$ \sec{x}\left(\sin^3x + \sin x \cos^2x\right)=\tan{x}$$ Here's how I resolve it ($LHS$) and again bear with me as I truly reverting to a feeling of vulnerability, like a child actually As $\sec x$ is equal to $\frac{1}{\cos x}$ That leads us to this $$\frac{(\sin^3x+\sin x\cos^2x)}{\cos x}$$ I'm factorizing one $\sin x$ $$\frac{\sin x(\sin^2x+\cos^2x)}{\cos x} = \frac{\sin x(1)}{\cos x} = \tan x$$ That seems to work otherwise I completly messed this up Reading the book's solution, I have something different... $$\begin{align*}
LHS&=\frac{\sin^3x}{\cos x}+ \sin x \cos x \\[4pt]
&=\frac{\sin x}{\cos x}-\frac{\sin x\cos^2x}{\cos x}+\sin x\cos x\\[4pt]
&= \tan x\end{align*}$$ What did I miss?",['trigonometry']
1783574,discretized Brownian motion,"These are the definitions I'm working with: A (standard) Brownian motion in $\mathbb{R}$ is a stochastic process $W(t)$ $(t \geq 0)$ such that the following properties hold: $W(0) = 0$ almost certain $W(t) - W(s) \sim\mathcal{N}(0, t - s)$ (for 0 $\leq s \leq t$) For every $0 = t_0 < t_1 < ... < t_N$, $W(t_j) - W(t_{j-1})$ $(1 \leq j \leq N)$ are independent. For a certain natural number $N \geq 1$ and $j \in \left\{0, 1, 2, ..., N\right\}$, we define $\Delta t = T/N$ en $t_j = j \cdot \Delta t$. Further, let $W_j = \tilde{W(t_j)}$, the numerical approximation of $W(t_j)$. Now let $Z_j \sim \mathcal{N}(0, 1)$ independent vaiables for $j \in \left\{1, 2, ..., N\right\}$ and let $W_0 = 0$ and $W_j = W_{j-1} + \sqrt{\Delta t} \cdot Z_j$ for $j \in \left\{1, 2, ..., N\right\}$. We call $W_j$ $(j = 0, 1, 2, ..., N)$ a discretized Brownian motion. A discretized Brownian motion is considered a special case of the standard Brownian motion. It is clear that properties (1) (by definition of the discretized Brownian motion) and (3) hold since $W_j$ can be written as $\sqrt{\Delta t} \cdot \sum_{n = 1}^{j} Z_n$ and all $Z_n$'s are independent. However, it is not clear to me that $W_t - W_s$ is normally distributed with mean 0 and variance $t-s$. Since $W_t - W_s = \sqrt{\Delta t} \cdot \sum_{n = s + 1}^{t} Z_n$, shouldn't $W_t - W_s$ be normally distributed with mean 0 and variance $(t - s) \cdot \Delta t$?","['stochastic-processes', 'monte-carlo', 'statistics', 'stochastic-calculus']"
1783575,Is there a rearrangement theorem for conditionally convergent improper integrals?,"The famous Riemann rearrangement theorem states that for a conditionally convergent real number series, we can rearrange the order of summation to make it converge to any prescribed number in the extended real line. In particular, this result sheds much light on the significance of absolute convergence , without which it would be quite dangerous to manipulate a convergent series. For improper (Riemann) integrals, we can also distinguish conditionally convergent integrals from absolutely convergent ones using analogous definitions. I'm wondering, however, if there also exists an analogous ""rearrangement theorem"" for improper integrals which reveals the essential difference (like eligibility for rearrangement, in the case of numerical series) between the two kinds of convergent integrals? Indeed, can we even define an integral version of ""rearrangement""? Of course, one distinction I'm already aware of is that absolutely convergent integrals are also integrable in the Lebesgue sense while conditionally convergent ones fail to be. But this is not what I want, since it doesn't appear nearly as striking as what is exhibited in the series version. PS: as far as I can tell, one probable way to see why eligibility for rearrangement matters is how we define a valid expectation for a numerical random variable. We require expectations (numerical series for discrete  random variables, and integrals for continuous ones) to be absolutely convergent, for if they were not, then they would undesirably depend on the ""chronological order"" in which we observe events, violating our basic principle that expectations should be stable and inherent in the random variable itself, rather than affected by how each event chronologically arises.","['riemann-integration', 'calculus', 'integration', 'soft-question', 'convergence-divergence']"
1783586,Evaluate the volume of the solid defined by $x^2+y^2+z^2 \leq 9$ and $x^2+y^2 \leq 3y$,"I am asked to solve the following problem: Evaluate the volume of the solid defined by $x^2+y^2+z^2 \leq 9$ and
  $x^2+y^2 \leq 3y$. I thought about using spherical coordinates: $$
0 \leq \rho \leq 3\\
0 \leq \theta \leq 2\pi\\
0 \leq \phi \leq \frac{\pi}{2}
$$ with $\rho^2sin(\phi)$ on the integral, but that didn't work. $$
\int_{0}^{\pi/2} \int_{0}^{2\pi} \int_{0}^{3} \rho^2sin(\phi) \ d\rho d\theta d\phi
$$ Where did I go wrong? TEXTBOOK ANSWER: $18 \pi$","['multivariable-calculus', 'multiple-integral', 'coordinate-systems', 'calculus']"
1783593,Evaluation of $\int_{0}^{1}\frac{1}{\sqrt{1+x}+\sqrt{1-x}+2}dx$,"Evaluation of $$\int_{0}^{1}\frac{1}{\sqrt{1+x}+\sqrt{1-x}+2}dx$$ $\bf{My\; Try::}$ Let $$I = \int_{0}^{1}\frac{1}{\sqrt{1+x}+\sqrt{1-x}+2}dx$$ Put $x=\cos 2 \theta\;,$ Then $dx = -2\sin 2 \theta d\theta$ and Changing Limit, We get $$I = \int_{0}^{\frac{\pi}{4}}\frac{2\sin 2 \theta}{\sqrt{2}\cos \theta+\sqrt{2}\sin \theta+2}d\theta$$ So $$I = \int_{0}^{\frac{\pi}{4}}\frac{\sin 2 \theta}{\cos\left(\theta-\frac{\pi}{4}\right)+1}d\theta$$ Now Put $\displaystyle \theta-\frac{\pi}{4}=t\;,$ Then $d\theta = dt$ and changing limits So $$I = \int_{-\frac{\pi}{4}}^{0}\frac{\cos 2t}{\cos t+1}dt=2\int_{-\frac{\pi}{4}}^{0}\frac{2\cos^2 t-2+1}{\cos t+1}dt$$ after that we can solve easily, My question is can we solve it without Using Trig substution, Plz explain me Thanks",['integration']
1783630,Showing the power set is equinumerous to ${}^X2$,"I'm trying to prove that the power set of $X$ and the set of functions from $X$ to $2$ are equinumerous. I think the best way to do so is to define a bijection between the two, but I'm not sure how to construct such a function. How do I do this?",['elementary-set-theory']
1783640,Weird differential equation - Jacobi?,"In class we had differential equations of the type
$$y'=\frac{\left(Ax+By \right)y+ \alpha x + \beta y}{\left(Ax+By \right) x+ax+by},$$
where $A,\alpha,a,B,\beta,b$ are constants. The names of the constants were chosen in a way, so that the 3 different constants for each variable look a bit an a and b. For example $$y'=\frac{(x-y)y-x-y}{(x-y)x+x+y}$$ I did not fully understand the method of solution. In class the professor called them ""Jacobi Differtial calculus"" (translated). I did not find anything suitable on the web, except Wolfram Mathworld http://mathworld.wolfram.com/JacobiDifferentialEquation.html There is at least a differential equation named Jacobi. But this does not seem to be the same as in my question. Any help? My question : Q1 What are DE's like those called? Q2 How to solve them?",['ordinary-differential-equations']
1783642,Operator norm of an identity map over $l_p$ space,Let $1 \leq p < q \leq \infty$ ($p$ and $q$ are not related) conclude that the identity map I : $ l^n_p → l^n_q$ has operator norm exactly 1. I figured I need to show that given $\|Ix\| \leq c\|x\|$ from the wiki definition the infimum over c is 1 so that $\|Ix\| = \|x\|$. is this a correct definition for the operator norm? and is it a good approach? Can i apply Riesz–Thorin theorem here? if so how?,"['functional-analysis', 'lp-spaces']"
1783679,"If $f'(a)=f''(a)=0$ but $a$ is not an inflection point, then must $a$ be a maximum or minimum?","Let $f:\mathbb{R}\rightarrow\mathbb{R}$ be twice-differentiable and $a\in\mathbb{R}$. If $f'(a)=f''(a)=0$ but $a$ is not an inflection point, then must $a$ be a maximum point or a minimum point?","['derivatives', 'calculus']"
1783732,Is a hypersphere of non-integer dimension a fractal?,"Thanks to the gamma function the formula for the surface of a unit http://mathworld.wolfram.com/Hypersphere.html $$
S(n) = \frac{2 \pi^{n/2}}{\Gamma(n/2)}
$$
allows to calculate the surface of a hypersphere of non-integer dimensions. I wanted to know, what is the number of dimensions I need, so that the surface of a n-sphere (with radius 1) equals the area of a square (with ""radius"" 1), which means solving the equation
$$
4 = \frac{2 \pi^{n/2}}{\Gamma(n/2)}
$$
for $n$. Since $S(n)$ has a maximum at $n=7.256...$ one get't two positive solutions:
$$
n=1.534...\\
n=15.86...
$$
(see https://www.wolframalpha.com/input/?i=2Pi%5E(n%2F2)%2FGamma(n%2F2)+%3D%3D+4 ). Now my questions is: Since the number of dimensions of the n-sphere from this equation is a non-integer, does that mean such a sphere would be a fractal ? If so, is it possible to construct a n-sphere with 1.534 dimensions somehow and draw it?","['fractals', 'spheres', 'dimension-theory-analysis', 'geometry', 'gamma-function']"
1783765,Slope of axes of a General Conic Section,"A General Conic Section is given by the equation $ax^2 + by^2 + 2hxy +2gx +2fy + c =0 $ . Let the $\theta$ be the slope of one of its axes. Prove that : $$\tan 2\theta = \dfrac{2h}{a-b}$$ I tried to use the definition of the conic section by taking cases for various conics such as parabola, pair of straight lines, ellipse, hyperbola etc. but it became too tedious to calculate. Are there some properties with which this question can be simplified ? Any help will be appreciated. Thanks.","['analytic-geometry', 'conic-sections', 'geometry']"
1783769,Norms inequality in a sequence space,"Let  $1 \leq p<q \leq \infty$ (p an q are not related) Let $\Phi$ be the vector space of all sequences with at most finitely many nonzero elements, meaning $\Phi=\{\{x_n\}_{n=1}^\infty|$ there is $n_o$ such that $x_n=0$ whenever $n\leq n_0\}$ i want to show that $\|x\|_q\leq \|x\|_p$ and there exist no C such that $\|x\|_p \leq C\|x\|_q$ by showing that $\sup_{0\neq x\in \Psi} \frac{\|x\|_p}{\|x\|_q}=\infty$ If $\|x\|_p=1$ then $|x_i|\leq 1 \forall x_i$ and then $|x_i|^q<|x_i|^p$ and $\displaystyle\sum_{i=1}^{n_0} |x_i|^q \leq \displaystyle\sum_{i=1}^{n_0} |x_i|^p$. I am not sure if i can simply apply the roots hereon both sides. is this at all a correct direction? Is there another way to go about it?","['functional-analysis', 'normed-spaces', 'inequality', 'convergence-divergence']"
1783782,Analyzing limits problem Calculus (tell me where I'm wrong).,"I came accross: $$\lim_{x \to\ 0}\frac{x\cos x - \log (1 + x)}{x^{2}}$$ I tried the following please tell me where I 'm wrong: $$\lim_{x \to\ 0}\frac{x\cos x - \log (1 + x)}{x^{2}} $$ $$\text{(Dividing by }x)$$ $$=\displaystyle\lim_{x \to\ 0}\dfrac{ \cos x - \dfrac{\log (1 + x)}{x}}{x} $$ $$=\lim_{x \to\ 0}\frac{\cos x - 1}{x} $$ $$=\lim_{x \to\ 0}\frac{-2\sin^{2} \dfrac{x}{2}}{x} $$ $$=\lim_{x \to\ 0}\frac{-2x\sin^{2}\dfrac{x}{2}}{(\dfrac{x}{2})^{2}\times 4} $$
$$\lim_{x \to\ 0}\dfrac{-x}{2}=0$$
But Answer given is $\dfrac{1}{2}$ Please Help.","['calculus', 'limits']"
1783787,Determine convergence of harmonic series with a minus every third term,"I want to evaluate the following sum:
$$
1 + \frac{1}{2} - \frac{1}{3} + \frac{1}{4} + \frac{1}{5} - \frac{1}{6}+\ldots
$$
call the sequence $a_k$. That is, the harmonic series, with the sign flipped every third term. I tried to approach this in two ways. One way is by defining a sequence $b_k = \left(\frac{1}{3k-2} + \frac{1}{3k-1} -\frac{1}{3k}\right)$. Then, we can see that $\sum b_k$ is the series $\sum a_k$ after we gather every three terms. Then, $b_k = \frac{9k^2-2}{3k(3k-2)(3k-1)}$, and $b_k\sim \frac{1}{3}\frac{1}{k}$, and therefore $\sum b_k$ diverges. Can I conclude that $\sum a_k$ diverge? The other way is by partial sums. I concluded the following inequality
$$
S_{3n-2} < S_{3n} < S_{3n-1}
$$
But also, I discovered that $S_{3n-2},S_{3n-1},S_{3n}$ are increasing, and I was unable to find a bound for them. Now, for the first way, it is kind of implying that the series diverges, since we learned at our calculus class that if $\sum a_n$ converges, then any way of putting brackets will result in a convergence. On the other way, if a series is created by putting brackets in another series with bounded bracket length, then if it is convergent then the original series converges. Does this series diverge?","['sequences-and-series', 'calculus']"
1783790,Is there a property that is not preserved by isomorphism? [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 8 years ago . Improve this question Isomorphism preserves the operation of domain, so codomain inherits some properties of domain related to operation. So isomorphism is somewhat like ""Equivalence"". But, if the property of domain is not ""related to operation"", maybe that property is not preserved. So is there a property that is not preserved by isomorphism? Or Isomorphism is a ""generalized equivalence""? Thank you for your answer in advance, and I apologize for my terrible English.",['abstract-algebra']
1783886,$X$ be a non-empty subset of irrational numbers such that sum of any two elements of $X$ is rational ; then is there any upper bound for $|X|$?,Let $X$ be a non-empty subset of irrational numbers such that sum of any two elements of $X$ is rational ; then is there any possible upper bound for the cardinality of $X$ ? Can $X$ be infinite ?( I know that $|X|$ can be $2$ ) . Please help . Thanks in advance,"['real-analysis', 'abstract-algebra', 'soft-question', 'elementary-set-theory', 'combinatorics']"
1783894,"Are the ""weights"" inside a neural network actually ""terms"" for a polynomial?","This just hit me today. I am not too experienced with math or neural networks, but I am trying to find out about them in my own way so I can some day understand them well. So I was thinking about how neural networks are connected to more familiar things that I know. This is just speculation, but I would like to know if I am on the right track at all. Currently I think that neural networks are in fact ""function adapters"" (if that is the correct term in english) so that when they are learning, they are trying to adapt some invisible function so that the inputs match the wanted outputs. If I wanted to do something like this by hand, I would of course adjust the terms of some function that I am adapting. Like if I had a simple polynomial: 5x + 1 I would adjust the x until the function outputs what I want with the given input. I think the x in this example might actually represent a weight in a neural network. It would make a lot of sense if this was the case! And then there is the ""back propagation"", which I have not studied that much at all, but I think it has to do with correcting the other weights when adjusting one, because if the weights are the unknowns of a polynomial and I adjust some unknown in a polynomial - all the previous calculations would be off to account for this name input, because the old inputs use the same network / polynomial as the new input that the network / polynomial was just adjusted for. So this ""back propagation"" takes this into account and tries to minimize the error for the old inputs? Am I on the right track here? Simply put: Are weights = unknowns in a polynomial Is ""back propagation"" = Making sure that the polynomial gives same outputs for the old inputs, after the polynomial has been adjusted to work with a new input Thanks!","['neural-networks', 'polynomials', 'functions', 'error-propagation']"
1783903,Trig Formula for distance,"I found a formula for calculating the distance to the sun for any given day of the year.  The formula uses the cosine function and I am not able to calculate the distances correctly.  I fear it is my misunderstanding of the formula or cosine.  I need help determining where my math is incorrect. Formula: r = a(1-e*e)/(1+e cos(θ)) a = Semi Major Axis of Orbit = 149.6 kilometers e = eccentricity of Earth orbit is .01672 θ = Theta = # of days since last perihelion * 365.25/360 Solved for May 22, 2002.  Perihelion was on Jan 2, 2002. (140 days.) I know the answer should be around 151 million kilometers for that day. And that is about 1.3% farther than the mean of 149.6. r = 149.6(1-.01672 * .01672) /    (1 + e * cos(140 * (365.25/360)) r = 149.6 * (.98328 * .01672) /   (1.01672 * cos(140 * (1.01583)) r = 149.6 * (.01644)          /   (1.01672 * cos(142.2162)) r = 2.459424                  /   (1.01672 * .79033) r = 2.459424                  /   .8035443176 r = 3.0607 ????  Where is my misunderstanding? Edit/Added 05/16/2016: Great Answer AugSB!  I was overlooking the radians conversion.  It works great now.  Thank you so much.  For anyone else needing similar calculations here is the Excel formula developed by AugSB: =149.6*(1-(0.01672*0.01672))/(1+(0.01672*COS(B3*(365.25/360)*(PI()/180)))) where column ""B"" holds the number of days since last perihelion. it should also be noted that this formula assumes the Earth's orbit is a perfect circle in which the Earth travels the same amount each day.  The orbit is actually oblong and travels slightly faster in orbit when nearer to the Sun.  So these numbers are not exact.  Another Method to account for Kepler's theory can be used, but it is way over my head.  NASA has an applet that can get exact distances at the following link. http://ssd.jpl.nasa.gov/horizons.cgi","['spherical-trigonometry', 'trigonometry']"
1783914,An example of a regular function over an open set,"Look at the example 2.1.7 at page 19 of these notes (which is the same example present in Mumford's red book at page 21). The author shows that a regular function isn't a ratio of two polynomial, and in particular in the above example inside the open set $U$ we have the following regular function:
$$\phi(p)=\left\{\begin{array} {lll}
\frac{X_1}{X_2} & \textrm{if $p\in X_2\neq 0$}\\\\
\frac{X_3}{X_4} & \textrm{if $p\in X_4\neq 0$}
\end{array}\right.$$ I agree with the fact that $\frac{X_1}{X_2}=\frac{X_3}{X_4}$ in $K(X)$ when $X_2\neq0$ and $X_4\neq0$, but the point is that such two functions shoud be the same in the whole $V$ to make sensible the example. Infact in in the intersection $\{X_2\neq0\}\cap\{X_4\neq0\}$ we have the regular function described by $\frac{X_1}{X_2}$ or $\frac{X_3}{X_4}$, but what about the rest of $U$?",['algebraic-geometry']
1783935,"Indefinite integral of $\int \frac { dx}{x^{2m}+1}, m \in \mathbb R $","I spent a few hours trying to solve this indefinite integral: $$\int \frac { dx}{x^{2m}+1}, m \in \mathbb R $$ I tried to transform the fraction to partial fractions getting $\int \frac{\imath}{2(x^m+\imath)} {dx} - \int \frac{\imath}{2(x^m-\imath)} {dx}$, but this doesn't help me because we enter in complex analisys, a field that I've no knowledgement and searching for methods that can help me with this make me nuts. I think the solution should involve some sort of induction, so I tried to solve for $ m= 0, 1, 2...$, but this doesn't help me neither. I've no luck looking for some books in the university library that can help me. Putting the integral in wolframalpha gives me the result:$$ x \space_2F_1(1,\frac{1}{2m};1 + \frac{1}{2m}; -x^{2m}) \color{silver} {+constant}$$where $_2F_1(1,\frac{1}{2m};1 + \frac{1}{2m}; -x^{2m})$ is the hypergeometric function (I'd never hear about this before) . This reveals to me that this problem is beyond my capabilities. Can some kind soul make a detailed resolution for me? If possible, not going into deep complex analysis.","['indefinite-integrals', 'complex-analysis']"
1783944,"Showing that, at an elliptic point, a surface lies on one side of the tangent plane.","Let $p\in S$ be an elliptic point of a surface $S$. I want to show that there exists a neighbourhhod $V$ of $p$ in $S$ such that all points in $V$ belong to the same side of the tangent plane $T_p(S)$. The book I'm reading (Do Carmo), proceeds as follows: Let $\mathbf{x}$ be a parametrization at $p$, with $\mathbf{x}(0,0)=p$. We consider the distance function: $$d(u,v)=\langle \mathbf{x}(u,v)-p,N(p) \rangle$$ Applying Taylor's Formula we obtain: $$\mathbf{x}(u,v)=\mathbf{x}(0,0)+\mathbf{x}_u u +\mathbf{x}_v v+\frac{1}{2}(\mathbf{x}_{uu}u^2+2\mathbf{x}_{uv} uv + \mathbf{x}_{vv}v^2)+\overline{R}$$ where all derivatives are evaluated at $(0,0)$ and $\displaystyle\lim_{(u,v)\to (0,0)}\frac{\overline{R}(u,v)}{u^2+v^2}=0$. Rearranging the terms: $$d(u,v)=\frac{1}{2}II_p(w)+R(u,v)$$ where $II_p(\cdot)$ is the ssecond fundamental form at $p$, $w=u\mathbf{x}_u+v\mathbf{x}_v$ and $R(u,v)=\langle \overline{R}(u,v),N(p)\rangle$. The author claims that $\lim_{w\to \mathbf{0}}\frac{R}{|w|^2}=0$. My question is Why is this? The norm $|w|$ is given by the first fundamental form $|w|^2=I_p(w)=Eu^2+2Fuv+Gv^2$, with $EG-F^2>0$. Hence: $$\frac{\overline{R}}{|w|^2}=\frac{u^2+v^2}{Eu^2+2Fuv+Gv^2}\frac{\overline{R}}{u^2+v^2}$$ If I could show that the quotient $$\frac{u^2+v^2}{Eu^2+2Fuv+Gv^2}$$ is bounded in a neighbourhood of $(0,0)$, then I would be done, but I don't see why this should be the case. Any ideas? (or alternative proofs of the initial claim?).","['differential-geometry', 'surfaces']"
1783945,Question about Gauss divergence theorem,"The Divergence Theorem says that for $\Omega \subset  \mathbb{R}^n$
$$\int_\Omega \nabla \cdot F = \int_{\partial\Omega} F \cdot n $$
where $n$ is the outward normal. Assuming that $F$ is a smooth vector field, does the Divergence Theorem require $\partial \Omega$ to have $n$ dimensional Lebesgue measure zero?","['multivariable-calculus', 'real-analysis']"
1784018,Functions and Derivatives,Generaly curious: Let there be a set of functions: Will the sum of the derivatives of the functions be equal to the derivative of the sums?,"['derivatives', 'functions']"
1784030,The quotient of a manifold by a submanifold is never a manifold?,"Let $M$ be a connected smooth manifold. Let $S$ be a connected embedded submanifold of positive dimension and co-dimension ,  which is also a closed subset of $M$ . Is it true that the quotient space $M /S$ (i.e the space obtained by identifying all $S$ to a single point) is never a manifold? Here is an attempt to show this: If we denote the equivalence relation by $E \subseteq M \times M$ , then it is proven in Bourbaki (see here ), that the quotient $M /S$ inherits from $M$ a smooths structure if and only if: $E$ is a closed submanifold of $M \times M$ , and The first projection $\pi_1: E \to M$ is a submersion In our case $E=\{(x,x)|x \notin S \}\cup S \times S=\{(x,x)|x \in M \}\cup S \times S$ Now assume $E$ is a submanifold of $M \times M$ . Then, since $(S\times S)^c$ is open in $M \times M$ , $E \cap (S\times S)^c=\{(x,x)|x \in S^c \}$ is open in $E$ . Since $S$ is closed in $M$ , $S^c$ is open in $M$ , so $\dim E =\dim \big( E \cap (S\times S)^c \big)=\dim \big( \operatorname{diag}(S^c) \big)=\dim(S^c)=\dim M:=n$ If I am not mistaken, then for every point $s \in S$ , $\dim_{(s,s)}E=(n-s)+2s > n$ which is a contradiction. The part of $(n-s)$ comes from paths spanning the complementary directions to $T_sS$ in the diagonal, i.e $\alpha(t)=(\beta(t),\beta(t))$ starting from $(s,s)$ , such that $\dot \beta(0)=v \in T_sM\setminus T_sS$ . The part of $2s$ comes from the freedom of movement inside the manifold $S \times S$ . Is this true? Is there a shorter (direct) argument? (perhaps one which does not use Bourbaki's result)","['smooth-manifolds', 'differential-geometry', 'differential-topology']"
1784034,Derivative of the stress tensor,"Let $\partial u_i/\partial x_i=0$ then given that $$\sigma_{ij} = -p\delta_{ij}+\mu\left(\frac{\partial u_i}{\partial x_j}+\frac{\partial u_j}{\partial x_i}\right)$$ Show that $$\frac{\partial \sigma_{ij}}{\partial x_j} = -(\nabla p)_i+\mu\nabla^2u_i$$ So the solution is: \begin{align}
\frac{\partial \sigma_{ij}}{\partial x_j} &= \frac{\partial}{\partial x_j} \left(-p\delta_{ij}+\mu\left(\frac{\partial u_i}{\partial x_j}+\frac{\partial u_j}{\partial x_i}\right)\right) \\
&= - \frac{\partial p}{\partial x_j} + \mu\left(\frac{\partial^2 u_i}{\partial x_j\partial x_j}+\frac{\partial^2 u_j}{\partial x_j\partial x_i}\right) \\
&= - \frac{\partial p}{\partial x_j} + \mu\frac{\partial^2 u_i}{\partial x_j\partial x_j} \tag{since $\partial u_i/\partial x_i=0$} \\
&= - (\nabla p)_i + \mu\nabla^2 u_i
\end{align} But why on the first to the second line have we suddenly removed $\delta_{ij}$ (which is the kronecker delta)?","['multivariable-calculus', 'fluid-dynamics']"
1784062,In what conditions a weak solution is a classical solution?,"I'm studying elliptic equations in divergence form $$-D_{j}(a_{ij}(x)D_{i}u) + c(x)u = f(x) \text { in a domain } \Omega \subset \mathbb{R}^{n}$$
I call a function $u \in H^{1}(\Omega)$ a weak solution if for every $\phi \in H^1_0(\Omega)$,
$$\int_\Omega (a_{ij}D_iuD_j\phi + cU\phi) = \int_\Omega f\phi$$
where The coefficients $a_{ij} \in L^{\infty}$ are uniformly elliptic; that is, there exists a positive $\lambda$ such that for all $x \in \Omega$, $\xi \in \mathbb{R}^n$,
$$\sum_{i,j} a_{ij}(x) \xi_i\xi_j \ge \lambda |\xi|^2$$ The coefficient $c \in L^{\frac{n}{2}}(\Omega)$ and the inhomogeneous term $f \in L^{\frac{2n}{n+2}}(\Omega)$. I have two questions: In what conditions a weak solution is a classical solution? How to find an example of elliptic pde with weak solution but not classical solution? Thanks very much!","['real-analysis', 'partial-differential-equations', 'regularity-theory-of-pdes', 'elliptic-equations', 'sobolev-spaces']"
1784122,A limit of the p-th power of a function integral [duplicate],"This question already has answers here : ""Scaled $L^p$ norm"" and geometric mean (3 answers) Closed 8 years ago . What is $$\lim_{p\to0}\left(\int_0^1\left(f(x)^p\right)dx\right)^{1/p},$$ where $f$ is continuous? It was an exam question and I don't know how to even get started with it, could you help? (It is not $f(x^p) $ but $\left(f(x)^p\right)$ so the $p$ -th power of $f(x)$ )","['integration', 'limits']"
1784129,Prove that if $n$ and $m$ are positive integers such that $n^n|m^m$ then $n|m$.,"This is how I did it, but not sure if it is a correct proof. Assume that $n^n | m^m$.  And we write $m= p_1^{a_1}p_2^{a_2}...p_k^{a_k}$ and $n=q_1^{b_1}q_2^{b_2}...q_l^{b_l}$.
So, 
$$m^m = (p_1^{a_1}p_2^{a_2}...p_k^{a_k})^m = c \ . \ (q_1^{b_1}q_2^{b_2}...q_l^{q_l})^n$$ for some positive integer $c$ which requires that for any $q_i$ there exist some $p_j$ such that $q_i=p_j$ and of course $ nb_i \le ma_j$. But this doesn't not imply $b_i \le a_j$ meaning I failed to prove $n|m$. On the other hand whatever example I try I can't come to a counterexample. Is it true if $n^n|m^m$ then $n|m$ for $n$ and $m$ are positive integers?","['abstract-algebra', 'discrete-mathematics']"
1784132,Prove $\exp(\mathrm{Tr}(X))=\det(\exp(X))$,Show that $\exp(\mathrm{Tr}(X))=\det(\exp(X))$ where $X$ is a matrix using the concept of the Jordan normal form I realised this formula by considering that: $\det(\exp(X))=\exp(\lambda_1) \times\cdots \times \exp(\lambda_n)=\exp(\lambda_1 + \cdots + \lambda_n)=\exp(\mathrm{Tr}(X))$ $\lambda_i$ are eigenvalues of $X$ I was unsure how to prove it using the Jordan normal form - could you help me please?,"['matrix-equations', 'jordan-normal-form', 'abstract-algebra', 'exponential-function', 'representation-theory']"
1784137,"In mathematics, what is an $N \times N \times N$ matrix?","In mathematics, what is an $N \times N \times N$ matrix? I think this is a tensor but definitions of tensors that I have read are so overly complicated and verbose that I have trouble understanding them.","['matrices', 'tensors']"
1784149,Easier way to induce an orientation to the border of a manifold,"I'm working in the following exercise: Let $M=\{(x, y, z): x²+y²=1 \,\text{and}\, 0\leq z \leq 1\}$. Let $\alpha:(0,1)²\rightarrow \mathbb R³$ be given by $\alpha(u, v)=(\cos u, \sin u, v)$. Consider the orientation of $M$ such that $\alpha$ belongs to it. Describe the normal unitary vector corresponding to this orientation. Describe the tangent vector of $\partial M$ corresponding to the orientation induced by this orientation of $M$. My attempt: Consider $\alpha_1, \alpha_2, \alpha_3, \alpha_4$ given by: $\alpha_1: (0, 1)\times [0, 1)$, $\alpha_2: (\frac{1}{2}, \frac{3}{2})\times [0, 1)$ given by $\alpha_i(u, v)=(\cos u, \sin u, v)$ $\alpha_3: (-1, 0)\times [0, 1)$, $\alpha_4: (-\frac{3}{2}, -\frac{1}{2})\times [0, 1)$ given by the formula $\alpha_i(u, v)=(\cos -u, \sin -u, 1-v)$. It's easy, but long and boring to verify that the determinants of the differential of all the following functions are all positive where they are defined: $$\alpha\circ\alpha_1^{-1}, \alpha\circ\alpha_2^{-1},\alpha\circ\alpha_3^{-1},\alpha\circ\alpha_4^{-1},\alpha_1\circ\alpha_2^{-1},\alpha_1\circ\alpha_3^{-1}, \alpha_1\circ\alpha_4^{-1}, \alpha_2\circ\alpha_3^{-1}, \alpha_2\circ\alpha_4^{-1}, \alpha_3\circ\alpha_4^{-1}$$ Now all we have to do is to calculate the cross products, normalize them, to restrict these functions to the border adjusting the sign and finally, to calculate the tangent vectors and normalize them. As you can see, this is a very long solution if you need to write all the details. Therefore, I'm not satisfied with my solution. Is there a way to make a shorter solution?","['smooth-manifolds', 'differential-geometry']"
1784166,"Why are There No ""Triernions"" (3-dimensional analogue of complex numbers / quaternions)? [duplicate]","This question already has answers here : Proving that $\mathbb R^3$ cannot be made into a real division algebra (and that extending complex multiplication would not work) (4 answers) Can there be a set of numbers, which have properties like those of quaternions, but of dimension 3? [duplicate] (1 answer) Closed 8 years ago . The community reviewed whether to reopen this question 2 years ago and left it closed: Original close reason(s) were not resolved Since there are  complex numbers (2 dimensions) and quaternions (4 dimensions), it follows intuitively that there ought to be something in between for 3 dimensions (""triernions""). Yet no one uses these. Why is this?","['abstract-algebra', 'quaternions', 'complex-numbers']"
1784173,Show that $(A \cup B)-B=A$ is false. Why is my method wrong?,So the textbook uses a counter example to show this which is pretty simple. I tried playing around with the algebra. Ie. $(A \cup B)-B$ is equal to $(A \cup B)\cap \bar{B}$ and associative law says this is equal to $A \cup (B\cap \bar{B})$. This results in $A \cup \emptyset$ which equals to $A$. What am I doing wrong here?,['elementary-set-theory']
1784202,Show that a k-form can be expressed as wedge product,"I'm trying to show that given a $1$-form $\omega$ and a $k$-form $\alpha$ such that $\alpha \wedge \omega = 0$ then there exists a $(k-1)$-form $\beta$ such that $\alpha = \omega \wedge \beta$. I'm struggling to show this even in the case of multi-linear algebra and forgetting forms. I've been trying to use the fact that if $v_i$ are a basis for $V$ then $\{v_{i_1}\wedge...\wedge v_{i_k} : 1 \le i_1 < ... < i_k \le n\}$ is a basis for $\bigwedge ^k V$. I thought perhaps we could extend $\omega$ (I'm talking just in the multilinear algebra case here and not thinking about manifolds - I'll worry about that after!) to a basis of $V$ and then $\alpha \wedge \omega = 0$ kills any terms containing $\omega$ in the expansion of $\alpha$ but I'm struggling to make any further progress, Thanks for any help","['differential-forms', 'smooth-manifolds', 'differential-geometry', 'multilinear-algebra']"
1784220,Rigor in proving continuity of $f$ over a closed interval $I$,"Given a function $f$ on a closed interval $I \subset \mathbb{R}$, where $I = [a,b]$, to prove continuity of $f$ over the interval $I$, what is generally done is the following. 1. We prove that $f$ is continuous at endpoint $a$ $$ \lim_{x \ \to \  a^{+}} f(x) = f(a)$$ 2. We prove that $f$ is continuous $\forall c \in (a, b)$ $$ \lim_{x \to c} f(x) = f(c) \ \ \ \ \text{,}\ \ \ \ \forall c \in (a, b)$$ 3. We prove that $f$ is continuousat endpoint $b$ $$ \lim_{x \ \to \  b^{-}} f(x) = f(b)$$ My question revolves around step 2 of this process. Usually all that is done to prove the continuity of $f$ over the open interval $(a, b)$, is (using the Limit Laws) a simple direct substitution of a variable $c$ denoting an arbitrary number in the open interval, which is basically substituting $c \in (a, b)$ into $f(x)$. But I don't see why this is mathematically rigorous? Initially I assumed something along the lines of Mathematical Induction would have been used to prove $f$ to be continuous $\forall c \in (a, b)$, but that doesn't seem to be the case. Why does just choosing an arbitrary $c$ to represent a number in the interval $(a, b)$, and showing that the chosen $c$ satisfies the given definition of continuity, prove that $f$ is continuous over $(a, b)$ ? I will give an example below to highlight what I'm asking. Example : Prove $f(x) = e^{x}$ to be continuous over the open interval $(0, 2)$ Proof : Let $c \in (0, 2)$ $$\begin{equation}
\begin{aligned}
\lim_{x \to c} f(x) &= \lim_{x \to c} e^{x} \\
&= e^{c} \\
&= f(c)\\
\end{aligned}
\end{equation}$$
$$Q.E.D$$ (This is how most proofs for continuity of functions over intervals are treated (at least in first-year Calculus courses) . But this way of proving continuity doesn't seem rigorous to me at all. It does nothing to explicitly state or show that $c$ is an element of the open interval $(0, 2)$ other than the fact that we assume it to be true. I could say 'Let $c \in \mathbb{R}$', and the proof would still hold, even though $f(x)$ is only defined for $c \in \mathbb{R^{+}}, \ \text{where} \ c \neq 0$. Just stating that $c \in (a, b)$ and showing that $lim_{x \to c} f(x) = f(c)$, doesn't seem like it proves anything to me, because $c$ doesn't necessarily need to be bounded within that interval, as we have done nothing to explicitly show $c \in (a, b)$ other than the fact that we stated it or assumed it. I mean using this sort of proof process, I could say let $c$ be an element of any arbitrary interval $R$, and by simply using direct substitution to show that $lim_{x \to c} f(x) = f(c)$, I would show that $f$ would be continuous over $R$, even if $f$ contained discontinuities over $R$, which this sort of proof process doesn't take into account at all. To re-emphazize, what I'm getting at, what I'm trying to say is I could always pick an arbitrary $c$ in any arbitrary open interval $I$, and using the Limit Laws (as used in this way in these sorts of proofs) always prove $f$ to be continuous on $I$ even if  $I \not\subset D$, i.e. the open interval lies outside of the domain of the function. As a last sub-question, in Real Analysis is there a more rigorous way in which continuity of a function $f$ over a closed interval $I$ is proven? Is there a reason in Real Analysis why continuity of functions over closed intervals is proved in this sort of way? Essentially I'm looking for a deeper understanding of proving continuity of functions over intervals, and I'm sure Real Analysis must have the answers to the questions I'm asking.","['continuity', 'real-analysis', 'calculus', 'limits']"
1784232,Applications of $Ext^n$ in algebraic geometry,"I have been doing a project about $\operatorname{Ext}^n$ functors for my commutative algebra class. I used the approach via extensions of degree n. Basically I have shown the long exact sequence associated to a short exact sequence for $\operatorname{Hom}_{\mathcal{A}}$ where $\mathcal{A}$ is an abelian category. To apply this construction I have proven the Chinese remainder theorem as a ""toy"" example. I would like to find applications in algebraic geometry that are not very technical. I feel confident with the language of sheaves but my general knowledge of algebraic geometry is very poor. Could someone please point to some references where I can look?","['sheaf-theory', 'algebraic-geometry', 'reference-request', 'homological-algebra', 'abelian-categories']"
1784245,Blowing up a model for a plane curve,"Let $R = \mathbb{C}[[t]]$ and let $\mathcal{X} \hookrightarrow \mathbb{P}_R^2$ be the arithmetic surface defined by the equation
     $$(X^2 - 2Y^2 + Z^2)(X^2 - Z^2) + tY^3Z = 0.$$
The generic fiber of $\mathcal{X}$ is a smooth quartic plane curve $C$ - so $\mathcal{X}$ is a model for $C$, but it is not regular. The special fiber of this surface is 
     $$\mathcal{X}_{\mathbb{C}} : (X^2 - 2Y^2 + Z^2)(X^2 - Z^2) = 0.$$
I am trying to blow the surface up at the point $[0,1,0]$ in the special fiber to get a regular model for $C$. Blowing this point up in the special fiber can be achieved by looking in the affine patch $Y\neq0$ and then blowing up the origin $(X,Z) = (0,0)$. First we can blow up the ambient space at this point to get 
    $$\{(X,Z,[a_0,a_1])\mid a_0Z = a_1X\}\hookrightarrow \mathbb{A}_R^2\times\mathbb{P}_R^1$$
and then look at the patch of $\mathbb{P}^1$ where $a_0\neq0$ which will give $Z = sX$ where we are now using coordinates $[1,s]$ for the affine patch. Restricting to $\mathcal{X}$, we end up with the equation
    $$X\left[(X^2-2+s^2X^2)(X-s^2X) + ts\right] = 0$$
If we look in the other affine path where $a_1\neq 0$ with coordinates $[r,1]$ we get a similar equation
    $$Z\left[(r^2Z^2 - 2 + Z^2)(r^2Z - Z) + t\right] = 0$$
These two affine equations should glue to give an arithmetic surface having generic fiber $C$ and special fiber a conic meeting two lines, each at two points. But the special fiber when $a_0\neq 0$ has equation
     $$X^2(X^2-2+s^2X^2)(1 - s^2) = 0,$$
and when $a_1\neq 0$, the equation is
     $$Z^2(Z^2 - 2 + r^2Z^2)(r^2 - 1) = 0.$$ I guess the conic is given affine locally by the equation $Z^2-2 + r^2Z^2$ and the two lines are given by $1-s$ and $1+s$? Something still seems wrong to me. Any suggestions are welcome.","['plane-curves', 'algebraic-geometry', 'blowup']"
1784262,"How is the derivative truly, literally the ""best linear approximation"" near a point?","I've read many times that the derivative of a function $f(x)$ for a certain $x$ is the best linear approximation of the function for values near $x$. I always thought it was meant in a hand-waving approximate way, but I've recently read that: "" Some people call the derivative the “best linear approximator” because of how accurate this approximation is for $x$ near $0$ (as seen in the picture below). In fact, the derivative actually is the “best” in this sense – you can’t do better ."" (from http://davidlowryduda.com/?p=1520 , where $0$ is a special case in the context of Taylor Series). This seems to make it clear that the idea of ""best linear approximation"" is meant in a literal, mathematically rigorous way. I'm confused because I believe that for a differentiable function, no matter how small you make the interval $\epsilon$ around $x$, there will always be for any $a$ near $x$ in that interval a line going through $x$ that is either as good an approximation of $f(a)$ as the one given by $f'(x)$ (in case the function is actually linear over that interval), or a better approximation (the case in which the line going through $(x, f(x))$ also goes through (a, f(a)) and any line between this line and the tangent at $x$). What am I missing?","['derivatives', 'calculus', 'approximation']"
1784307,This linear operator has no eigenvalues,"Let $T : L^2(\mathbb R) \to L^2(\mathbb R)$ be a linear operator defined by 
$$(Tf)(x)=f(x+1).$$
Show that $T$ has no eigenvalues, i.e., there exists no $f \not= 0$ in $L^2(\mathbb R)$ such that $(Tf)(x)=\lambda f(x)$ for any $\lambda \in \mathbb C$. My work: Okay, so the $\lambda=0$ case was straightforward for me: If $\lambda=0$, then $Tf(x)=0$, which means $f(x)=0$ because $T(0)=0$ since $T$ is linear. But I am stuck on the case $\lambda \in \mathbb C \setminus \{0\}$. How can I work with $f(x+1)=\lambda f(x)$ and show that $f(x)=0$?","['eigenvalues-eigenvectors', 'operator-theory', 'functional-analysis', 'eigenfunctions', 'linear-transformations']"
1784312,Deciding whether a form in the exterior power $\bigwedge^k V$ is decomposable,"Let $V$ be a vector space and $\bigwedge^kV$ be the $k$th exterior power. I'm trying to find a condition that characterizes when an element $\omega \in \bigwedge^kV$ is decomposable in the sense that $\omega = v_1 \wedge ... \wedge v_k$ for some $v_i \in V$. Now if $\omega$ is decomposable, then $\omega^2 = 0$, and I wondered whether the converse holds in the general case? (Or perhaps for some restrictions on the dimension of $V$ or k?). This is trivially true for $k=1$ but I'm not sure about other cases.","['differential-forms', 'exterior-algebra', 'differential-geometry', 'multilinear-algebra']"
1784332,Explanation of the Sum of an Infinite Series Equation,"I've been presented with the following infinite sum (where $P$ is the probability of an event, and $1-P$ is therefore the probability of it not occurring. I was given the following equation as fact:
$$\sum_{i=1}^\infty(iP^{i-1}(1-P))=\frac{1}{1-p}$$ Now I know from my basic education the sum of an infinite series for probability $P$ is $$ \sum_{i = 0}^\infty P^i = \frac{1}{1-P}$$ Assuming I haven't made a very basic mistake, these are therefore equivalent but I don't really see how. Normally, I would ask the person who proposed the first equation to justify their working, however, in this instance I can't (they have an obscure transform named after them and would take it very badly!). My question is, how is the first equation evaluated to such a simple result, and therefore equivalent to the second. Thanks!","['power-series', 'summation', 'probability', 'sequences-and-series']"
1784333,Generalized Jacobian Conjecture,"Is there any known generalization of Jacobian conjecture which gives condition for $k[f_1, \ldots, f_m] = k[g_1, \ldots, g_m]$ where all $f_i$ and $g_i$ are functions over $x_1, \ldots, x_n$? Note that possibly, $n \neq m$. Is there anything known even if we assume $n = m$? One possibility is that the determinant of Jacobian for $f$ and $g$ (as in Jacobian conjecture) is same w.r.t. all $m$-tuples of $x_i$'s. This is stronger than Jacobian conjecture, but I cannot prove the other way round.","['abstract-algebra', 'ring-theory', 'algebraic-geometry', 'commutative-algebra']"
1784337,Second order non linear differential equation: Central force question,"The problem is as below: I have derived that the particle satisfies the motion equation
$$ \frac{d^2u}{d \theta ^2 } + u = \frac{F(1/u)}{mh^2u^2} $$ 
by Newton's Law, $u= 1/r$ and $h = r^2 \frac{d \theta }{dt}$ is constant. Hence, by substitution, we obtain from initial conditions that 
$h=av$, and that from the motion equation,
$$ \frac{d^2 u }{ d \theta ^2 } = 6au^2$$ 
And this as far I could get. I could not really solve this equation - I tried multiplying by $u'$ both sides but nothing useful turns out. Also I could not find a place to use the information on radial velocity. (Probably used for solving the differential equation). Any ideas? Thank you!","['classical-mechanics', 'physics', 'ordinary-differential-equations']"
1784381,Trying to compute a Riemann-sum as an integral where the,"$\lim_{n\to\infty} a\cdot n +\sum_{n^2}^{4n^2} \dfrac{1}{k+n}$
I have the following problem: I want to figure out , for what value of a does this limit converge and what value does it then converge to. I'm assuming that I'm supposed to rewrite the sum as an integral where I can evaluate the integral in the form of ""n's"". Given the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx = [2\sqrt{x}]_{1}^{4}$, where $x= k+n$. I think I can finish the problem. However, I am unsure of how to get the integral $\int_{1}^{4}\dfrac{1}{\sqrt{x}}dx$ Any help will be grateful. Thanks for your time.","['riemann-sum', 'riemann-integration', 'limits']"
1784402,"What does ""the average continuous function is nowhere monotonic"" mean?","I plan on asking my professor what he meant by ""average continuous function,"" but as it is possible that this is a concept as vague as the statement, I was hoping to get some interesting answers/interpretations from stack exchange first. How do you think of the average of some infinite group of things? Or does this just mean that the real line is so dense/big that it is somehow likely that a function would bounce around everywhere except on some countable number of points? I'm sorry this is vague, I will be sure to post his response if I get a good one. I would also appreciate any resources or reading; googling around hasn't been fruitful.","['real-analysis', 'reference-request', 'probability-theory', 'measure-theory', 'soft-question']"
1784437,O'Neill's differential geometry: typo in formula for partial derivative?,"I am working through Barrett O'Neill's Elementary Differential Geometry and I'm mildly confused. Exercise 3 in section 4.3 ask you to verify that $$\mathbf{y}_{u}=\mathbf{x}_{u}\frac{\partial \bar{u}}{\partial u}+\mathbf{x}_{v}\frac{\partial \bar{v}}{\partial u} $$ with a similar equation for $\mathbf{y}_{v}$. I have that $\mathbf{y}(u,v)=\mathbf{x}(\bar{u},\bar{v})$. I thought this was a typo, however this same notation appears in the next section as is. Any and all help will be appreciated.","['derivatives', 'differential-geometry', 'calculus']"
1784444,Integral of Wiener Squared process,"I don't have a background of stochastic calculus. It is known fact that definite integral of standard Wiener process from $0$ to $t$ results in another Gaussian process with slice distribution that is normal distributed with mean equal to $0$ and variance $\frac{T^3}{3}$ i-e $$ \int_0^{t} W_s ds \sim \mathcal{N}(0,\frac{t^3}{3})    $$ Question: What if we square the standard Wiener process and then integrate i-e $$ \int_0^{t} W_s^2 ds \sim ?    $$ Would that be scaled Chi-square distributed ?","['stochastic-processes', 'brownian-motion', 'probability', 'stochastic-integrals']"
1784476,About transitive subgroups of symmetric group $S_n$,"When I am studying Galois theory I came across some problems:
Let $S_n $ be the symmetric group on $n$ letters($|S_n|=n!$).How to determine all the transitive group     $G$ of  $S_n $ ( A subgroup $G$ of  $S_n $ is called transitive if for each $i,j\in \{1,2,\dots,n\}$,there is a $\tau \in G $ with $\tau(i)=j$).With some knowledge about group action, I can proof that if $G$ is a transitive group of  $S_n $ then $n$ divide the order of $G$.For $n=3$,I can easily find out all the transitive subgroups of $S_3$ because we can check each of its subgroup.But when $n$ is bigger ,for instance $n=4,5$ do we have to first find out all the subgroups of $S_n$ and then check which one is transitive.I want to know if we have a better method.\
Another question is given a transitive group $G$ of $S_n $ ,how to find a field $F$,and an irreducible polynomial $f(x)$ over $F$,so that if $K$ is the splitting of $f(x)$,then we have :
$$ G\cong Gal(K/F)$$
I even don't know how to get down to this question.Hope someone can help me.Thank you very much!","['abstract-algebra', 'galois-theory', 'field-theory', 'symmetric-groups']"
1784481,Find $\frac{1}{7}+\frac{1\cdot3}{7\cdot9}+\frac{1\cdot3\cdot5}{7\cdot9\cdot11}+\cdots$ upto 20 terms,"Find $S=\frac{1}{7}+\frac{1\cdot3}{7\cdot9}+\frac{1\cdot3\cdot5}{7\cdot9\cdot11}+\cdots$ upto 20 terms I first multiplied and divided $S$ with $1\cdot3\cdot5$
$$\frac{S}{15}=\frac{1}{1\cdot3\cdot5\cdot7}+\frac{1\cdot3}{1\cdot3\cdot5\cdot7\cdot9}+\frac{1\cdot3\cdot5}{1\cdot3\cdot5\cdot7\cdot9\cdot11}+\cdots$$
Using the expansion of $(2n)!$
$$1\cdot3\cdot5\cdots(2n-1)=\frac{(2n)!}{2^nn!}$$
$$S=15\left[\sum_{r=1}^{20}\frac{\frac{(2r)!}{2^rr!}}{\frac{(2(r+3))!}{2^{r+3}(r+3)!}}\right]$$
$$S=15\cdot8\cdot\left[\sum_{r=1}^{20}\frac{(2r)!}{r!}\cdot\frac{(r+3)!}{(2r+6)!}\right]$$
$$S=15\sum_{r=1}^{20}\frac{1}{(2r+5)(2r+3)(2r+1)}$$ How can I solve the above expression? Or is there an simpler/faster method?","['summation', 'sequences-and-series']"
1784485,Is the function differentiable at $0$?,"Let $$f(x) = \begin{cases}\begin{align*}&\cos{\dfrac{1}{x}}, &x \neq0 \\ &0, &x=0. \end{align*}\end{cases}$$ Is the function $F(x) = \displaystyle \int_{0}^x f dx$ differentiable at $0$ ? We can see that the function $f(x)$ is continuous everywhere except $x=0$ . In order to show differentiability we will need to show the derivatives from the left and right are equal. So we need to show that $$\lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h}.$$ The derivative from the right is $$\lim_{h \to 0^+} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^+} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h}$$ and the derivative from the left is $$\lim_{h \to 0^-} \dfrac{F(x+h)-F(x)}{h} = \lim_{h \to 0^-} \dfrac{\displaystyle \int_{0}^x\cos{\dfrac{1}{x+h}}-\int_{0}^x\cos{\dfrac{1}{x}}}{h}.$$ I am not sure how to proceed.",['calculus']
1784486,Solution to $\sqrt{x-2} = 3- 2\sqrt{ x}$,"The above question is from Serge Lang's basic mathematics. The question asks if there are any values of x which satisfy the above equation. Serge Lang's answer key states that there is no solution. From the equation, I squared both sides, then used the quadratic formula to find $\sqrt{ x} = 2-\sqrt{1/3}$ or $2+\sqrt{1/3}$.
I substituted the former into the equation, and found that it would equate $\sqrt{x-2}$ to a negative answer, and hence is wrong since $\sqrt{x-2}$ refers to the positive root of $x-2$.
For the second answer, I substituted and found that both sides of the equation equaled out. I drew $y=\sqrt{x-2}$ and $y=\sqrt{x}$ in a graphing calculator, and found they intersected correctly at a point. Yet the book states there are no solutions. What went wrong?",['algebra-precalculus']
1784489,The GCD of a Univariate Integer-Valued Polynomial over a Set,"Let $\mathcal{I}[X]$ denote the subring of $\mathbb{Q}[X]$ consisting of all integer-valued polynomials (i.e., $f(X)\in \mathbb{Q}[X]$ such that $f(k)\in\mathbb{Z}$ for all $k\in\mathbb{Z}$).  For $f(X)\in\mathcal{I}[X]$ and a nonempty subset $S$ of $\mathbb{Z}$, write $\gamma_S(f)$ for the gcd of all $f(k)$ for $k\in S$.  It is well known that each $f(X)\in\mathcal{I}[X]$ takes the form $f(X)=\sum_{i=0}^n\,A_i\,\binom{X}{i}$ for some $n\in\mathbb{N}_0$ and $A_0,A_1,\ldots,A_n\in\mathbb{Z}$ (that is, $\mathcal{I}[X]$ is a free abelian group with a $\mathbb{Z}$-basis consisting of $\binom{X}{i}$ for $i=0,1,2,\ldots$), so we set the modified content $\tilde{C}(f)$  of $f(X)$ to be $\gcd\left(A_0,A_1,\ldots,A_n\right)$. Here are some examples (also stated in the link below) with $S=\mathbb{Z}$.  For the polynomial $X^2+X=0\binom{X}{0}+2\binom{X}{1}+2\binom{X}{2}$, the modified content is $\gcd(0,2,2)=2$, which equals the gcd of all $k^2+k$ with $k\in\mathbb{Z}$.  For the second one, $X^3-X=0\binom{X}{0}+0\binom{X}{1}+6\binom{x}{2}+6\binom{X}{3}$ so that the modified content is $\gcd(0,0,6,6)=6$, which is also the gcd of all $k^3-k$ with $k\in\mathbb{Z}$. The question is: what are all nonempty subsets $S$ of $\mathbb{Z}$ such that $\gamma_S(f)=\tilde{C}(f)$ for all $f(X)\in\mathcal{I}[X]$?  I know that $S$ must satisfy this condition: for every prime $p\in\mathbb{N}$, integer $r>0$, and $j\in\left\{0,1,2,\ldots,p^r-1\right\}$, there exists $s\in S$ such that $s\equiv j\pmod{p^r}$.  Do we have anything similar if we replace $\mathbb{Z}$ by a GCD domain $D$ and $\mathbb{Q}$ by the field of fractions $F$ of $D$ (whence $\mathcal{I}[X]$ by the subring $\mathcal{I}_D[X]$ of $F[X]$ of D-valued polynomials , namely, $f(X)\in F[X]$ such that $f(k)\in D$ for all $k\in D$)?  What if we increase the number of variables, that is, asking the same question about the gcd of a $D$-valued polynomial in $F\left[X_1,X_2,\ldots,X_l\right]$ over a subset $S\subseteq D^l$ and its modified content (if it is possible to define the modified content over $D$ ), where $l>1$ is an integer?  See also here . For example, we take $D:=\mathbb{R}[T]$ (so that $F=\mathbb{R}(T)$).  Then, $\mathcal{I}_D[X]$ is precisely $D[X]=\mathbb{R}[T,X]$.  For $f(X)\in D[X]$, we have $f(X)=\sum_{i=0}^n\,A_i(T)\,X^i$ for some $n\in\mathbb{N}_0$ and $A_i(T)\in \mathbb{R}[T]$, with $i=0,1,2,\ldots,n$, from which we define $\tilde{C}(f)$ to be $\gcd\left(A_i(T)\right)_{i=0}^n$.  Already when $S$ is an infinite subset of $\mathbb{R}\subseteq \mathbb{R}[T]=D$ do we have $\gamma_S=\tilde{C}$. When $D:=\mathbb{Z}$ and $f(X)\in\mathcal{I}[X]$, then write $f(X)=\sum_{i=0}^n\,A_i\,\binom{X}{i}$ with $A_0,A_1,\ldots,A_n\in\mathbb{Z}$.  Then, it can be easily seen that $\gamma_S(f)=\tilde{C}(f)$ for $S:=\big\{i\in\{0,1,2,\ldots,n\}\,|\,A_i\neq 0\big\}$. P.S. (1)  In fact, we can also define $\gamma_\emptyset(f)$ to be $0$ for all $f(X)\in\mathcal{I}[X]$ (or for a $D$-valued polynomial $f(X)\in \mathcal{I}_D[X]\subseteq F[X]$).  For the zero polynomial $0$, we can set $\gamma_S(0)=0$ and $\tilde{C}(0)=0$. (2)  I may also need to assume that $D$ is a unique factorization domain, or even a principal ideal domain.  I'm not sure if an infinite subset of a GCD domain necessarily has a gcd.","['polynomials', 'gcd-and-lcm', 'number-theory', 'ring-theory', 'prime-numbers']"
1784515,Does every $C^1$ closed differential form differ from some $C^\infty$ closed form by an exact form?,"Question: On a $C^\infty$ manifold, does every $C^1$ closed differential form differ from some $C^\infty$ closed form by an exact form? Motivation: This result holds for $C^1$ closed 1-forms on a $C^\infty$ manifold $M$ for which the first singular homology is finitely generated (e.g., all compact manifolds). Proof: Let $\omega$ be any $C^1$ closed form on $M$. Let $\sigma_1,\ldots,\sigma_k$ be a $C^\infty$ basis for $H_1(M;\mathbb{R})$ and $[\theta_1],\ldots,[\theta_k] \in H_{dR}^1(M)$ be the dual basis (identifying $H_{dR}^1(M)$ with $Hom(H_1(M;\mathbb{R}),\mathbb{R})$ via the de Rham theorem), where $\theta_1,\ldots,\theta_k$ are $C^\infty$ forms. For $1 \leq i \leq k$, define $c_i := \int_{\sigma_i} \omega$. Then the one-form $\alpha := \omega - c_1 \theta_1 \ldots - c_k \theta_k$ satisfies $\oint_\gamma \alpha = 0$ for any closed loop $\gamma$, since $\gamma$ is a cycle and is thus a finite $\mathbb{R}$-linear combination of the $\sigma_i$ and the integral of $\alpha$ over all $\sigma_i$ is zero. It follows that given any two points $p,q \in M$, the integral of $\alpha$ over a path from $p$ to $q$ is independent of the path. For any points $p$ and $q$, denote this value by $\int_p^q \alpha$. Fix a basepoint $x_0 \in M$ and define $f:M\to \mathbb{R}$ by $f(p):= \int_{x_0}^p \alpha$. It is easy to show that $df = \alpha$. Hence $\omega - c_1 \theta_1 \ldots,c_k \theta_k = \alpha = df$, showing that $\omega = c_1 \theta_1 \ldots + c_k \theta_k + df$, as desired. This completes the proof. Note that the proof works if $\omega$ is only differentiable, and in general $f$ is only as smooth as $\omega$. Observation: The result would follow for $k$-forms on compact manifolds if it were true that ""If the integral of a $C^1$ closed $k$-form $\omega$ over every $k$-cycle was zero, then $\omega$ is exact."" If $\omega$ was a $C^\infty$ closed form, then this property could be shown using the de Rham theorem. However, I haven't yet figured out from the de Rham theorem proof whether it would work with $C^1$ forms rather than $C^\infty$ forms.","['algebraic-topology', 'differential-geometry', 'differential-topology']"
1784563,"I don't understand why $\oint_\gamma f\, dz=0$ holds for holomorphic functions.","I've recently learned a proof of Cauchy's Integral Theorem, i.e, If $U\subseteq \Bbb C$ is  open and simply connected, $f:U\to\Bbb C$ is holomorphic and $\gamma$ is a closed curve, $\gamma\subseteq U$, then
  $$
\oint_\gamma f(z)\,\mathrm dz=0
$$ But I don't understand why this is true, I'm looking for intuition, as just from reading a proof, this still seems a bit magical. I'm not searching for proofs, intuitive arguments are sufficient.","['intuition', 'complex-analysis']"
1784564,Geometric meaning of the contact condition?,"I am trying to understand contact structures. The definition of a contact manifold is this: Let $M$  be a $2n + 1$-manifold and let $\omega$ be a differential $1$-form such that $\omega \wedge (d\omega)^n \neq 0$ pointwise. Then $M$ is a contact manifold and $\omega$ defines a contact structure on $M$. Without using differential forms I think it means something like this: A contact structure on $M$ is a smooth distribution of hyperplanes. This means that there is a smooth map $D$, the distribution, with the property that $D(m)$ is a $2n$-dimensional subspace of $T_m M$ for all $m \in M$. At least, this is my understanding so far. Finding uninteresting examples seems easy: For example, take the sphere $S^2 \subset \mathbb R^3$ and consider tangent lines that vary smoothly with $m \in S^2$. Writing down an explicit expression for these smooth tangent lines poses a slight challenge to me I admit and I am wondering whether if I could write down a formula that defines a tangent line to $S^2$ in $\mathbb R^3$ if it would help me gain intuitive understanding of this mysterious condition $\omega \wedge (d\omega)^n \neq 0$. So my question is: What are we trying to achieve by requiring $\omega \wedge (d\omega)^n
 \neq 0$? What does it achieve in terms of the geometric properties of
  the distribution of hyperplanes?","['intuition', 'contact-topology', 'differential-geometry']"
1784571,What is the Difference Between a Version and a Modification of a Stochastic Process?,"Under what circumstances would one say that: The stochastic process $X$ is a version of the stochastic
  process $Y$? Background: See here for a related but slightly different question on Math.SE. Usually the word version is used most often in connection with conditional expectations, or general random variables, to mean that: The random variable $X$ is a version of the random variable $Y$ iff: $$\mathbb{P}[X=Y]=1,$$ i.e $X=Y$ almost surely. I have also heard the term used in reference to stochastic processes, but in this case I am not sure how it should be used, and how it relates to the terms modification and indistinguishable . Let $X,Y$ be random functions (i.e. stochastic processes) mapping from the index set $T$ to a measurable space $\Omega$. $X$ is a modification of $Y$ iff $$\forall\ t\in T,\ \mathbb{P}[X(t)=Y(t)]=1$$ and $X$ is indistinguishable from $Y$ iff $$\mathbb{P}[X(t)=Y(t),\ \forall\ t \in T]=1.$$ Note here the different placements of the logical quantifier $$\forall\ t\in T$$ outside vs. inside the definition of the set whose probability is in question between the two definitions. However, under what circumstances would we say that: The stochastic process $X$ is a version of the stochastic
  process $Y$?","['stochastic-processes', 'probability-theory', 'measure-theory', 'soft-question', 'stochastic-analysis']"
1784573,A ring isomorphic to a proper subring of itself,"Give an example of a ring which is isomorphic to a proper sub-ring of itself. HINT : Consider $\Bbb R^\Bbb N$ . My try :As given in the hint I considered $\Bbb R^\Bbb N$ i.e the set of all sequences from $\Bbb N\rightarrow \Bbb R$ with pointwise multiplication and addition. Consider the set $X=\{(x_n):x_0=0\}$ i.e. those sequences from $\Bbb R^\Bbb N$ whose first term is zero.
This is a proper subset of $\Bbb R^\Bbb N$ and also a subring  but the isomorphism given by $a_0+a_1x+\ldots +a_nx^n+\ldots\mapsto a_1x+a_2x^2+\ldots a_nx^n\ldots$ is not working .How to find the correct one?","['abstract-algebra', 'ring-theory']"
1784613,Another Hockey Stick Identity,"I know this question has been asked before and has been answered here and here . I have a slightly different formulation of the Hockey Stick Identity and would like some help with a combinatorial argument to prove it. First I have this statement to prove:
$$
\sum_{i=0}^r\binom{n+i-1}{i}=\binom{n+r}{r}.
$$
I already have an algebraic solution here using the Pascal Identity:
$$
\begin{align*}
\binom{n+r}{r}&=\binom{n+r-1}{r}+\binom{n+r-1}{r-1}\\
&=\binom{n+r-1}{r}+\left[\binom{n+(r-1)-1}{(r-1)}+\binom{n+(r-1)-1}{r-2}\right]\\
&=\binom{n+r-1}{r}+\binom{n+(r-1)-1}{(r-1)}+\left[\binom{n+(r-2)-1}{r-2}+\binom{n+(r-2)-1}{(r-2)-1}\right]\\
&\,\,\,\vdots\\
&=\binom{n+r-1}{r}+\binom{n+(r-1)-1}{(r-1)}+\binom{n+(r-2)-1}{(r-2)-1}+\binom{n+(r-3)-1}{r-3}+\cdots+\left[\binom{n+1-1}{1}+\binom{n+1-1}{0}\right]\\
&=\binom{n+r-1}{r}+\binom{n+(r-1)-1}{(r-1)}+\binom{n+(r-2)-1}{(r-2)-1}+\binom{n+(r-3)-1}{r-3}+\cdots+\binom{n+1-1}{1}+\binom{n-1}{0}\\
&=\sum_{i=0}^r\binom{n+i-1}{i}.
\end{align*}
$$ I have read both combinatorial proofs in the referenced answers above, but I cannot figure out how to alter the combinatorial arguments to suit my formulation of the Hockey Stick Identity. Basically, this formulation gives the ""other"" hockey stick. Any ideas out there?","['combinatorics', 'summation', 'binomial-coefficients']"
1784642,"If $\int_{A} fdxdy=0 $ for every unit-area rectangle $A$, does it follow that $f=0$ a.e? [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 8 years ago . Improve this question Is it true that for any locally integrable function $f \in L_{\mathrm{loc}}^1(\mathbb{R}^2)$, if 
$$ \int_a^b \int_c^d f(x,y) \,\textrm{d}x \,\textrm{d}y=0 \hspace{6mm} \textrm{for all $a,b,c,d$ with $|b-a||d-c|=1$,} $$
then $f=0\,$ a.e.?",['measure-theory']
1784646,Whats the difference between a series and sequence?,"I was looking at a question earlier that involved sequences and found out that the sequence converged to 0 but the series diverged to infinity. How is that possible? for example the sequence was $a_n$ $=$ $\frac{1}{x_n}$ 
The question was here: Convergence of $x_n=f\left(\frac{1}{n}\right), n\geq 1$","['terminology', 'sequences-and-series']"
1784647,Coordinate charts vs. coordinates on manifolds,"I just realised that I'm confused what coordinates really means in the context of manifolds. For example, say $M=S^2$. Then we can define smooth charts as follows: Let the open sets be $U = S^2$ minus the north pole and $V=S^2$ minus the south pole and then $\varphi : U \to \mathbb R^2$ the stereographic projection $(X,Y)\mapsto ({x\over 1-z}, {y \over 1-z})$. I have no trouble visualising these maps. But there are several things that bother me. For example, $S^2$ is two dimensional. This is supported but the fact that there are two component coordinates $(X,Y)$. But: in order to define these one assumes a point on $S^2$ is given by $(x,y,z)$. Cartesian coordinates in $\mathbb R^3$. I used to think that if a manifold has $n$-dimensions a point on it is given as an $n$-tuple $(x_1, x_2, \dots, x_n)$ which is used to define a map into $\mathbb R^n$. Please could someone tell me what I misunderstand about coordinate
  charts? The second thing that I am confused about is this: When people write something like local coordinates $x_1, \dots, x_n$ for
  the manifold $M$ does one really mean charts $\varphi_1, \dots,
 \varphi_n$? The coordinate charts define what each point $(x_1, \dots, x_n)$ is so I assume one cannot speak meaningfully of coordinates of a manifold unless one is talking about charts. Or am I missing something? When I am writing about coordinates I use $(x_1, \dots, x_n)$ to denote a bunch of numbers, like $(1,2,3,4)$, basically the coordinates of a vector as in linear algebra and coordinate charts are the maps that transport a similar bunch of numbers in $\mathbb R^4$ to $(1,2,3,4)$. I want to define a contact structure on a manifold. Some easy examples in  order to better understand. But: The standard contact structure on $S^3$ is given by the contact form $xdy - y dx$. If we'd have the stereographic projection charts for $3$-dimensions,
  as above, would this contact form now be $XdY - Y dX$ or really just
  $xdy - y dx$?","['coordinate-systems', 'differential-geometry', 'definition']"
1784652,Discrete Math: Multiplying a set by ∅,"How would you multiply any set by  $\varnothing$? Lets say $A \times \varnothing$. Would that simply be equal to  $\varnothing$? or Would I write out $(a,  \varnothing ), (a_1,  \varnothing), (a_2,  \varnothing)$ etc. If possible could you explain your answer in words or short proof?","['elementary-set-theory', 'discrete-mathematics']"
1784671,When can we not treat differentials as fractions? And when is it perfectly OK?,"Background I am a first year calculus student so I would prefer if answers remained in Layman's terms. It is common knowledge and seems to me a mantra that I keep hearing over and over again to ""not treat differentials/derivatives as fractions"". I am of course, in particular, referring to Leibniz notation . However, aside from a quick response such as ""oh, it's because its not a fraction but rather a type of operator"", I never really got a full answer as to why we can't treat it as such. It just kind of sits at the edge of taboo in my mind where it sometimes gets used and sometimes doesn't. Confusion is further compounded when a lot of things seem to just work out if we treat them just as fractions (e.g. u-substitution/related-rates) Example Air is being pumped into a balloon at a rate of $100cm^3/s$ . We want the rate of change of radius when the radius is at $25cm$ . $$\text{we are given}\ \frac{dv}{dt}=100cm^3/s$$ $$\text{we want}\ \frac{dr}{dt}\ \text{when}\ r=25cm$$ Thus we will solve this by using the relation $v=\frac{4}{3}\pi r^3$ $$\frac{dv}{dt}=\frac{dv}{dr}\frac{dr}{dt}$$ $$\frac{dv}{dt}\frac{dr}{dv}=\frac{dr}{dt}$$ $$100\frac{1}{4\pi r^2}=\frac{1}{25\pi}$$ So the answer is $\frac{dr}{dt}=\frac{1}{25\pi}$ when $r=25cm$ *Note the manipulation of derivatives just as if they were common fractions using algebra. Question When exactly can I treat differentials/derivatives as fractions and when can I not? Please keep in mind that at the end of the day, I am a first year college student. An answer that is easy to understand is preferred over one that is more mathematically rigorous but less friendly to a beginner such as me.","['derivatives', 'notation', 'calculus', 'terminology', 'integration']"
1784679,To Prove the relation between HCF and LCM of three numbers,"if $p,q,r$ are three positive integers prove that $$LCM(p,q,r)=\frac{pqr \times HCF(p,q,r)}{HCF(p,q) \times HCF(q,r) \times HCF(r,p)}$$ I tried in this way; Let $HCF(p,q)=x$ hence $p=xm$ and $q=xn$ where $m$ and $n$ are relatively prime. similarly let $HCF(q,r)=y$ hence $q=ym_1$ and $r=yn_1$ where $m_1$ and $n_1$ are Relatively prime. Alo let $HCF(r,p)=z$ hence $r=zm_2$ and $p=zn_2$ we have $$p=xm=zn_2$$ $$q=xn=ym_1$$and $$r=yn_1=zm_2$$ can i have any hint to proceed?","['number-theory', 'elementary-number-theory']"
1784712,Evaluation of $\sin \frac{\pi}{7}\cdot \sin \frac{2\pi}{7}\cdot \sin \frac{3\pi}{7}$,"Evaluation of $$\sin \frac{\pi}{7}\cdot \sin \frac{2\pi}{7}\cdot \sin \frac{3\pi}{7} = $$ $\bf{My\; Try::}$ I have solved Using Direct formula:: $$\sin \frac{\pi}{n}\cdot \sin \frac{2\pi}{n}\cdot......\sin \frac{(n-1)\pi}{n} = \frac{n}{2^{n-1}}$$ Now Put $n=7\;,$ We get 
 $$\sin \frac{\pi}{7}\cdot \sin \frac{2\pi}{7}\cdot \sin \frac{3\pi}{7}\cdot \sin \frac{4\pi}{7}\cdot \sin \frac{5\pi}{7}\cdot \sin \frac{6\pi}{7}=\frac{7}{2^{7-1}}$$ So $$\sin \frac{\pi}{7}\cdot \sin \frac{2\pi}{7}\cdot \sin \frac{3\pi}{7} =\frac{\sqrt{7}}{8}$$ Now my question is how can we solve it without using Direct Formula, Help me Thanks",['trigonometry']
1784770,How does ~ distribute over parentheses?,"In my recent Discrete Math final exam, we had a question where I thought the answer was false but apparently it is true. It is the following: $$((\forall x)P(x)) \rightarrow  ((\forall y) Q(y))) \equiv (((\exists y) \sim Q(y) \rightarrow (\sim(\forall x)P(x)))$$ Clearly, this is an application of the definition of implication and I knew this, but I thought that the solution was false since the ~ in 
$$(\sim(\forall x)P(x)))$$ 
should not distribute into the P(x) since I thought that 
$$(\sim(\forall x)P(x)))$$
and 
$$(\sim(\forall x) \sim P(x)))$$ are different. I'm thinking its missing an extra pair of parens: $$((\forall x)P(x)) \rightarrow  ((\forall y) Q(y))) \equiv (((\exists y) \sim Q(y) \rightarrow (\sim((\forall x)P(x))))$$ Am I right here?","['logic', 'discrete-mathematics']"
1784807,$A$ abelian variety. Is the multiplication by $n_A$ surjective?,"According with Mumford the answer is yes, but there are some obscure points in the proof. We know that there exists a very ample line bundle on $A$ since every abelian variety is projective. Hence, via some corollaries of the Theorem of the Cube, we get an ample line bundle whose restriction to the kernel of $n_A$ is both the trivial bundle and very ample. This should imply that the dimension of $\ker n_A$ is $0$. Furthermore, Mumford says that the fact that the dimension of the kernel is $0$ implies the surjectivity of $n_A$. Is anybody able to clarify these two points to me? Thanks in advance!","['abelian-varieties', 'algebraic-geometry']"
1784836,Put the numbers $\cos a=a; \sin \cos b=b; \cos \sin c=c$ in ascending order,"Let $a,b,c \in \left[0;\frac{\pi}2\right]$ such that 
  $$\cos a=a; \sin \cos b=b; \cos \sin c=c.$$
  Put the numbers $a, b, c$ in ascending order. My work so far: If $x>0$, than $\sin x<x$. If $x \in \left[0;\frac{\pi}2\right]$ then $\cos x$ decreasing function. Then $$\sin \cos x<\cos x< \cos \sin x.$$
$$b=\sin \cos b<\cos b;$$
$$\cos c<\cos \sin c=c.$$ I need help here.",['trigonometry']
1784869,Examples of sequences of positive terms $\{a_n\}$ such that $a_n^{1/n}\rightarrow l ~~\text{does not imply}~~ \frac{a_{n+1}}{a_n}\rightarrow l$,"Give some examples of sequences of positive terms $\{a_n\}$ such that $$a_n^{1/n}\rightarrow l ~~\text{does not imply}~~  \frac{a_{n+1}}{a_n}\rightarrow l$$ If $a_n>0$ for all $n\in \mathbb{N}$, it can be shown that  $\frac{a_{n+1}}{a_n}\rightarrow l $ implies $a_n^{1/n}\rightarrow l$ by using log, a continuous function. This is Cauchy's second limit theorem. But I don't know how to show that its converse (that is the question I have written above) is not true. Please help.","['real-analysis', 'convergence-divergence', 'sequences-and-series', 'analysis']"
