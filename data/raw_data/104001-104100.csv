question_id,title,body,tags
1459723,What is the exact meaning of self consistent?,"I've heard the term self consistent being used when referring to differential equations, etc. but I have to admit that I'm unsure as to what is exactly meant by this. Is it simply that there is at least one solution that solves the equation? For example, suppose I have a 2nd order differential equation of the form $$a(x)y''(x)+b(x)y'(x)+c(x)y(x)=f(x)$$ Would one say that this differential equation is self consistent if there is at least one solution $\tilde{y}(x)$ such that $$a(x)\tilde{y}''(x)+b(x)\tilde{y}'(x)+c(x)\tilde{y}(x)=f(x)$$ or does the term self-consistent mean something completely different?","['terminology', 'ordinary-differential-equations']"
1459727,Deriving the Center of Mass of a semi-circular disk with cylindrical coordinates,"Problem: Derive the Center of Mass of a semi-circular  disk of mass $M$ and radius $R$. My attempt: $$Y_{CM}=\int ydm$$ Now, $$dm=\sigma dA$$ where $\sigma$ is mass per  unit area. Converting into Cylindrical Coordinates,$$dA=rdrd\theta$$. Also, $$y=r\sin\theta$$ Hence the integral  can be rewrittenn as $$\int_0^R\int_0^{\pi}r^2\sin\theta d\theta dr$$ However this Integral gives me the wrong value  of the Y coordinate of  the Center of Mass. I would be truly grateful for any help  with  this problem.","['calculus', 'integration']"
1459734,The chance of the word to be of all different letters?,"A four letter word is written down by taking letters from the word KANYAKUMARI.What is the chance of the word to be of all different letters? Since KANYAKUMARI has K,K,A,A,A,N,Y,U,M,R,I letters in which there are 2 K's and 3 A's.The chance that the four letter word to be of all different letters$=\frac{m}{n}$,where $m=\binom{8}{4}$,because there are 8 different letters K,A,N,Y,U,M,R,I.and $n=\binom{11}{4}$.So probability$=\frac{7}{33}$,but the correct answer is $\frac{840}{1109}$.I dont know where i have made the mistake.Please help me.","['probability', 'combinatorics']"
1459801,Can we recover a space from its continuous functions?,"Let $X$ be a topological space and let $\mathcal{F}(X,\Bbb R)$ be the set of continuous function from $X$ to $\Bbb R$ . Can we recover the topology of $X$ by only the knowledge of $\mathcal{F}(X,\Bbb R)$ ? That is, can we determinate whether or not a subset $U$ of $X$ is open only by using $\mathcal{F}(X,\Bbb R)$ ? If not, is there an example of two distinct topologies $\mathcal{T_1}$ and $\mathcal{T}_2$ on a set $X$ such that $$\mathcal{F}_{\mathcal{T}_1}(X,\Bbb R)=\mathcal{F}_{\mathcal{T}_2}(X,\Bbb R)\,?$$ I know from this question that we can indeed recover the topology of $X$ if we consider instead continuous functions from $X$ to $\{0,1\}$ with topology $\big\{\emptyset,\{1\},\{0,1\}\big\}$ . But what about the case $X=\Bbb R$ ?","['continuity', 'general-topology', 'inverse-problems']"
1459877,the convergence in probability of the stochastic integral,"In Jacod's Limit Theorems for Stochastic Processes :page 47,thm 4.31 (iii) $X$ is a semimartingale , $H_n$ are predictable process converge pointwise to $H$ , and $|H^n|\le K$ , where $K$ is a locally bounded predictable process, then $\forall t:H^n\cdot X_t\to H\cdot X_t$ in probability. First consider when $X\in\mathcal V$ (cadlag , adapted , $X_0=0$ , finite variation on $[0,t],\forall t$ ) I want to prove the conclusion above by markov inequality:
$$\mathbb P\left(|(H^n-H)\cdot X_t|\ge \varepsilon\right)\le\varepsilon^{-1}\mathbb E\left[|(H^n-H)\cdot X_t|\right]$$ since $H^n-H\to 0 $ pointwise and $|H^n-H|\le 2K$  , so I want to use DCT,but I can't prove that $\mathbb E\left[|K\cdot X_t|\right]<\infty$ $$\mathbb E\left[|K\cdot X_t|\right]\le\mathbb E\left[\int_0^t|K_s|d| X_s|\right]\le C\mathbb E| X_t| $$ I can't get $\mathbb E| X_t|<\infty$ from $X\in\mathcal V$. Thanks a lot！","['probability-theory', 'stochastic-calculus', 'convergence-divergence', 'stochastic-analysis']"
1459888,Curvature of a regular curve is a smooth function of parameter if it does not vanish,"We have that the curvature of a curve $\gamma (t)$ is given by $K(t)=\|\gamma ''(t)\|$ iff $\|\gamma '(t)\|=1$. If $\|\gamma '(t)\| \neq 1$, then we find the arclength $s(t)=\int_0^t \|\gamma '(u)\|du=g(t)$, then we solve for $t=g^{-1}(s)$. Then we have that $\gamma (s)=\gamma (g^{-1}(s)) \Rightarrow \|\gamma '(s)\|=1$. So we find the curvature by the formula $K(s)=\|\gamma ''(s)\|$. When the curvature of a regular curve $\gamma (t)$ is everywhere $>0$, then show that the curvature is a smooth function of $t$. Could you give me some hints how we could show this?","['differential-geometry', 'curves', 'curvature']"
1459894,Maintain the variance in a stream,"I am trying to understand how to compute the variance online.  If I have the value 23 ten times, the value 10 five times and the value 4 three times, what is the variance? We know the mean is $(230+50+12)/18 \approx 16.22$  and the variance is approx $61.17$ . It is easy to maintain the mean online by just storing the sum and the number  of numbers we have seen. However, if the numbers arrived one at a time, how can we maintain the variance as the numbers arrive? That is without just storing all the numbers  and recalculating each time a new number arrives.","['means', 'statistics']"
1459899,The Green's function of the beam deflection equation,"This is a problem in a textbook used in my class: Suppose we have an infinite elastic beam, where the deflection $u(x)$
  satisfies the differential equation $$\frac{d^4 u}{dx^4}+k^4 u =
> f(x),$$ where $k^4$ is a positive constant regarded as known, and
  $f(x)$ is a load. For part (a) of the question, we assume that the load is a unit
  concentrated load at $x = \xi$, so that it satisfies we equation
  $$\frac{d^4 u}{dx^4}+k^4 u = \delta(x - \xi),$$ where $\delta$ is the
  Dirac-delta equation. So, for this problem I would like to find the deflection (which is equal to the free space Green's function). I've looked in several mechanics textbooks to find a detailed approach as to how to calculate this, but only found solutions for different problems.","['greens-function', 'distribution-theory', 'ordinary-differential-equations', 'fundamental-solution']"
1459951,Compute the distance between two points with standard normal distribution.,"Suppose the position of points $x_1$, $x_2$, $x_3,\ldots, x_{10}$ on real line satisfies standard normal distribution. Then for a new point $x$, set $d_i(x):=|x-x_i|$, $i=1,\ldots, 10$. Compute the density function $$p(x):=\min_{i}\{d_i(x)\}$$ I, apparently, get confused on how to write done $d_i$ and how to have $min$ function. Please advise!","['probability', 'statistics']"
1459952,When are convex polygon tilings Voronoi?,"A square is divided into convex polygons.  Is this always a Voronoi diagram ? If not, what are some simple examples of non-Voronoi tilings? Which of the pentagon tilings are Voronoi? I took a look at Recognizing Dirichlet tessellations . There are three rules right off: At a Voronoi vertex, all angles $a < \pi$. Let segment $p q$ be between two trivalent vertices, and let angles $p$ and $q$ be those angles that don't include $p q$. Then $p+q> \pi$. If a Voronoi tessellation has more than one vertex, the generating points are uniquely determined. There are other rules listed, but those three are a powerful start.","['tiling', 'geometry']"
1460026,Algebra (Matrix Theory) Linear Maps,"I had a problem in my booked i tried to prove. Here is the problem ""Let $x_1,\ldots,x_n$ be different real numbers and $y_1,\ldots,y_n,s_1,\ldots,s_n$ some real numbers. Prove that there exists a polynomial $p(x)$ of a degree less than $2n$ such that $p(x_i)=y_i$ and $p'(x_i)=s_i$ for every $i=1,2,\ldots,n.$"" Here is my attempt: Let $V= \{ p(x)\in \mathbb{R}[x] \mid \deg(p(x))<2n\}$ $\Rightarrow$
$\exists$ $h(x)\in V$ s.t $\deg(h(x))<n<2n$ from here we now can apply Lagrange Interpolation theorem. $\Rightarrow$ $h(x_i)=y_i$. Let $Q= \{ g(x)\in \mathbb{R}[x] \mid g(x)=p'(x),\ p(x) \in V, \deg(p(x)) < n \}$ where $\dim(Q)=n-1$. Now let $g(x)\in Q \Rightarrow \deg(g(x))<n-1$ Let $A: Q\rightarrow \mathbb{R}^n$ $g(x) \rightarrow (g(x_1),\ldots,g(x_n))$ Now assume $g(x)\in \ker(A) \Rightarrow g(x_i) = 0$, $n$ zeros $\Rightarrow \ g(x)=0 \ \Rightarrow \ker(A) = \{0\} \Rightarrow A$ injective $+$surjective $\Rightarrow g(x_i)=s_i$ But since $\deg(h(x))<n \Rightarrow h'(x) \in Q \Rightarrow g(x)=h'(x) \Rightarrow g(x_i)=h'(x_i)=s_i$ My Professor had i quick look and stated that this was a good attempt but it was not correct. He said that my proof does not with certainty show that this $h'(x_i)=s_i$ ill get in the end will fullfill $h(x_i)=y_i$ I didnt understand him, because in my opinion im sure that the derivitive of $h(x)$ which fullfill $h(x_i)=y_i$ lies in $Q$ and therefor i can let my $g(x)$ be equal to $h'(x)$ I would be grateful if someone can explain this?","['polynomials', 'linear-algebra', 'numerical-methods']"
1460037,"Why is $\displaystyle\int_{x=-\infty}^{x=\infty} f(x) \delta(x) \, \mathrm{d}x = f(0)$?","I understand that $\delta(x)=0$ whenever $x \ne 0$ and that $\displaystyle\int_{x=-a}^{x=b}  \delta(x) \, \mathrm{d}x = 1 \space$ $\forall\, a,b \gt 0$ and also that $\displaystyle\int_{x=-\infty}^{x=\infty}  \delta(x-a) \, \mathrm{d}x = 1$. But I see no justification that $\color{blue}{\displaystyle\int_{x=-\infty}^{x=\infty} f(x) \delta(x) \, \mathrm{d}x = f(0)}$ for any arbitrary function $f(x)$. I'm asking this question because the formula marked blue was given to me in response to this previous related question asked by me. But every-time I search the internet for an explanation of its derivation all I get is the same formula stated without proof. Hence, could someone please prove and/or explain the origin of the formula marked blue? Thank you.","['dirac-delta', 'calculus', 'integration']"
1460038,Compute sum of binomial coefficients,"I am trying to solve a combinatorics problems where I need to compute a sum based on binomial coffecients: $${\sum_k}{\frac{1}{k+1}}{99\choose k}{200\choose 120 - k} $$ The only pattern that I can see here is : $${\sum_k}{99\choose k}{200\choose 120 - k} = {99 + 200\choose 120}$$ My guess has been that I need to incorporate the $${\frac{1}{k+1}}$$ in order to match the form: $${\sum_k}{n\choose k}{m\choose l - k} = {m + n\choose l}$$ I have tried decomposing the binomial coefficients into their factorial form: $${\sum_k}{\frac{1}{k+1}}{99\choose k}{200\choose 120 - k} $$ = 
$${\sum_k}{\frac{1}{k+1}}{\frac{99!}{k! * (99 - k)!}}{\frac{200!}{(120 - k)! * (200 - (120 - k))!}} $$ = $${\sum_k}{\frac{99!}{(k + 1)! * (99 - k)!}}{\frac{200!}{(120 - k)! * (200 - (120 - k))!}} $$ I do not see any patterns here that can lead me to the correct answer. Any advice or direction would be appreciated! Please do not post a solution!","['discrete-mathematics', 'summation', 'combinatorics']"
1460048,"How to investigate the $\limsup$, the $\liminf$, the $\sup$, and especially the $\inf$ of the sequence $(\sqrt[n]{|\sin{n}|})_{n=1}^{\infty}$?","How to investigate the $\limsup$, the $\liminf$, the $\sup$, and especially the $\inf$ of the sequence $(\sqrt[n]{|\sin{n}|})_{n=1}^{\infty}$? Edit: The limit of this sequence is already investigated years ago in this post: Calculate $\lim_{n \to \infty} \sqrt[n]{|\sin n|}$ . So the $\limsup$, the $\liminf$, and the $\sup$ are clearly 1. Sorry for did not search wisely.","['real-analysis', 'supremum-and-infimum', 'trigonometry', 'analysis', 'radicals']"
1460108,For what subset of the reals is the difference of any two elements always unique such that every real can be so represented?,"I am looking for a subset $A$ of the real numbers such that given a real number $z$ not equal to $0$ there exists a unique $x,y \in A$ such that $x-y=z$. Or for any real number $z$ not equal to $0$, $x-y=z$ for $x,y \in A$ has one and only one solution. Some examples of what can't happen: If $A=\{0,1,2\}$ then if $z=1$ we have two solutions, $2-1=1$ and $1-0=1$. If $A=\mathbb{Z} $ then $z=.5$ has no solutions. Another way I have been looking at this problem is as points on a number line where there is always exactly one set of points any given distance apart. EDIT: dropped largest/maximal from the question given Patrick's lemma.","['real-numbers', 'elementary-set-theory', 'number-theory', 'additive-combinatorics']"
1460125,"Is every function $f$ on $ \mathbb R^2$ such that $f(x,y) \le g(x) + g(y)$ for every $(x,y)$, for some function $g$ on $\mathbb R$?","Is the following statement true? For every $f: \mathbb R^2 \to \mathbb R$, there exists $g:\mathbb R \to \mathbb R$, such that $f(x,y) \le g(x) + g(y)$ for all $x,y \in \mathbb R$. I do not think so. However, I couldn't find a counterexample. In case that doesn't hold, what conditions could we impose on $f$ so that the statement becomes true?",['real-analysis']
1460132,"The thinking behind ""4 times 5 is 12, and 4 times 6 is 13, and 4 times 7 is-oh dear! I shall never get to 20 at that rate!"" from Alice in Wonderland?","Excerpt from Lewis Carroll's Alice in Wonderland : ""Let me see: four times five is twelve, and four times six is thirteen, and four times seven is-oh dear! I shall never get to twenty at that rate!"" My questions: What mathematical machinery might the writer have had in mind when writing down the sentences above? Could one reengineer Carroll's thoughts behind? Is there any explanatory hint/link/allusion in Carroll's work or somewhere else? I know that Carroll was a mathematician. So, there must be something out there...","['number-theory', 'puzzle']"
1460159,When to type a function in bold?,"Is it convention to bold any function with more than one output? For example, $\textbf{f}:\textbf{R}^2 \mapsto \textbf{R}^3$ or $f: \textbf{R}^2 \mapsto \textbf{R}^3$ $\textbf{f}(\textbf{x})$ or $f(\textbf{x})$",['multivariable-calculus']
1460229,Every vector bundle has a metric connection?,"Let $(E,g)$ be a vector bundle with a metric over a manifold $M$. Does $(E,g)$ always admit a compatible (metric) connection? If so, are there examples where there exists only one such metric connection?","['differential-geometry', 'connections', 'vector-bundles']"
1460256,How do we know that $x^2 + \frac{1}{x^2}$ is greater or equal to $2$?,"For one problem, we were supposed to know that: $$x^2 + \frac{1}{x^2}\geq 2.$$ How do you deduce this instantly when looking at the expression above?","['algebra-precalculus', 'inequality']"
1460300,Equivalent condition to differentiability of a function in a general set.,If $A\subset \mathbb{R}^k$ is an arbitrary set one said that $f:A \rightarrow \mathbb{R}^n$ is differentiable if for each point $x\in A$ exists an open set $U_x$ and a function $\tilde{f}:U_x \rightarrow \mathbb{R}^n$ such that $\tilde{f}$ is differentiable and $\tilde{f}|_{A \cap U_x}=f|_{A \cap U_x}$. Is this equivalent to the existence of a single open set $A\subset U$ and a differentiable function $\tilde{f}:U \rightarrow \mathbb{R}^n$ such that $\tilde{f}|_A=f|_A$? I try to use partitions of unity but I couldn't do it.,"['multivariable-calculus', 'real-analysis', 'general-topology', 'derivatives']"
1460357,"From $\prod_{d\mid n}d=n^{\sigma_0(n)/2}$ to $n!=\operatorname{lcm}(1,\ldots,n)^{e(n)}$, where $\sigma_0(n)$ is the number of divisors","We know that $$\prod_{d\mid n}d=n^{\sigma_{0}(n)/2}$$ for every integer $n\geq 1$, where $\sigma_{0}(n)$ is the number of positive divisors of $n$, see for example [1] (exercise 10, page 47). And for this sequence of divisors $1=d_{1}<d_{2}<\cdots<d_{\sigma_{0}(n)}=n$, we have that $$\operatorname{lcm}(d_{1},d_{2},\cdots,d_{\sigma_{0}(n)})=n.$$ It is know that the average order of $\sigma_0(n)$ is $\log n$  for example by this formula $$\frac{1}{x}\sum_{n\leq x}\sigma_{0}(n)=\log x +O(1)$$ (for another, see for example Theorem 3.3 in page 57 of [1] and more about the growth in Theorem 13.12). Now we consider $e(n)$ defined for every integer $n>1$ by
$$n!=\operatorname{lcm}(1,2,\cdots,n)^{e(n)}$$ Question What about the growth of $e(n)$? Can you improve my computations or give more details? My attempt was from the relationship between the second Chebyshev function, defined for $x>0$, as (really in terms of von Mangoldt function) $\psi(n)=\sum_{p^a\leq x}\log p$, where the sum is extended over all prime powers least or equal than $x$ (see this site Math Stack Exchange or the page of Wikipedia corresponding Chebyshev function), satisfies
$$\operatorname{lcm}(1,2,\cdots,n)=e^{\psi(n)}$$
and too satisfies an equivalence with Prime Number Theorem, as this form $\psi(x)\sim x$ (Theorem 4.4, page 79 of [1]). I write from this $\psi(n)\sim n$ for large values of integers $n$, and will use Stirling equivalence. Question (Solved by an user in comments) Can I use $\psi(x)\sim x$, as I said, this is I made a substitution from a real $x$ to the variable in integers $n$ (and compute the limit as I show)? I believe that is 'yes' since both sequences reals and integeres are distributed as same manner, mod 1, is this? We take logarithms in the equation that defines the exponent $e(n)$, for $n>1$ and we take limits $$\lim_{n\to\infty}e(n)=\lim_{n\to\infty}\frac{\log \sqrt{2\pi n}+n(\log n-\log e)}{n}$$ Thus dividing by $n$, we compute $e(n)\sim \log n$, or in this form $\lim_{n\to\infty}\frac{e(n)}{\log n}=1$. I don't know if this problem is in the literature, my only goal is learn and edit the best post. Thanks in advance. References: [1] Apostol, Introduction to Analytic Number Theory, Springer. [2] This Mathematics Stack Exchange, second Chebyshev's function, Stirling equivalence, Prime Number Theorem.","['least-common-multiple', 'number-theory', 'proof-verification', 'divisor-counting-function', 'asymptotics']"
1460368,Non measurable function but measurable pre-image,"I am having trouble with a problem. It gives a hint to use the Vitali construction, but I honestly do not understand it. The question is: Show that there is a function $f:\mathbb{R} \to \mathbb{R}$ is not Lebesgue measurable, but $\forall$ $ a \in \mathbb{R}$ $f^{-1}(a)$ is measurable. My try: 
Let $E$ be a non-measurable subset of $\mathbb{R}$ , and $f(x)= \begin{cases} 
      x & x\in E \\
      -x & x\notin E
     \end{cases}
$ I am not certain that this is correct. Any help would be greatly appreciated, and all apologies for the simplistic inquiries.","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1460383,Proof that an $n \times n$ real symmetric matrix has $n$ real eigenvalues [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 8 years ago . Improve this question How to show that an $n \times n$ real symmetric matrix has $n$ real eigenvalues
$$
\lambda_n \geqslant \dots \geqslant  \lambda_1
$$ 
with corresponding eigenvectors $\mathbf v_n, \dots,\mathbf v_1$ such that
$$
\mathbf v_i^T \mathbf v_j = \delta_{ij}.
$$
I am struggling a bit on how to start this proof. I tried looking online but could not find anything. Perhaps I am looking in the wrong direction, would it be the same to prove that such a matrix would have $n$ real orthogonal eigenvectors and thus $n$ real eigenvalues? Hints on where to start this proof would be appreciated.","['eigenvalues-eigenvectors', 'linear-algebra', 'matrices']"
1460396,Is the Completeness of $X$ really necessary here?,"From an exercise in Kreyszig's Functional Analysis, it is stated that Let $X$ be a Banach Space and $(x_n)$ be a sequence in $X$ such that the sequence $(f(x_n))$ is bounded $\forall f\in X'$, show that $(\|x_n\|)$ is bounded. I attempted to prove it by letting $g_n\in X''$ be the canonical image of $x_n$ and invoked the Uniform Boundedness Theorem to show that the sequence $(\|g_n\|)$ is bounded. Since $\|g_n\|=\|x_n\|$, the sequence $(\|x_n\|)$ is bounded so the theorem is proved. However, I noticed that nowhere in the proof did I once use the fact that $X$ is complete. The closest thing I used is the completeness of $X'$ but $X'$ is always a Banach Space regardless of $X$ so I am at lose. So, is my proof correct? If it's not, then how could I use the completeness of $X$ to fix it? Thank you in advance. Edit : It appears that $X$ being a Banach Space is really not necessary after all. Anyway, can anyone think of a way to prove it using the fact that $X$ is complete?","['sequences-and-series', 'banach-spaces', 'functional-analysis', 'metric-spaces']"
1460397,Prove that $\lim(x_n)=0$ if and only if $\lim(|x_n|)=0$.,"Prove that $\lim(x_n)=0$ if and only if $\lim(|x_n|)=0$. Definition:
Let $X = (x_n)$ be a sequence in $\mathbb{R}$ and let $x\in\mathbb{R}$. Then $\lim(x_n) =x$ iff for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-x|<\varepsilon$ for all $n\geq k$. I am wondering if this is sufficient: If we know that $\lim(x_n)=0$, we know for all $\varepsilon>0$, $\exists k\in\mathbb{N}$ such that $|x_n-0|<\varepsilon$ for all $n\geq k$. We can rewrite this as the following:
$$|x_n-0|<\varepsilon \Leftrightarrow ||x_n|-0|<\varepsilon\Leftrightarrow \lim(|x_n|)=0$$ Then the conclusion seems to follow logically. Is the proof really that simple or are there some things I am missing?",['real-analysis']
1460412,concentration of maximum of gaussians,"Let $X=(X_1,\ldots,X_n)$, where $X_i \sim N(0,1)$ are iid. I'm looking for a result (and a proof outline) on the concentration of the max abs value of these Gaussians, $\|X\|_\infty$. That is, some result of the form $P(\bigl | \|X\|_\infty -\sqrt{2\log (2n)}\bigr |>t)<o(t)$, where $o(t)$ is any reasonable function that goes to $0$ as $t$ gets large. I know these results: $E \|X\|_\infty \leq \sqrt{2 \log (2n)}$, $P(\|X\|_\infty \geq \sqrt{2 \log (2n)}+t)\leq 2\exp(-t^2 /2)$, which seems to be the ""right tail"" of the result I'm looking for.","['normal-distribution', 'probability', 'statistics']"
1460439,Commutator Identity and Commutator Subgroup,"It is well known that if $G$ is any group in which $[G,G]$ is abelian, then for any $a,b,c\in G$, $$\bigl[[a,b],c\bigr]\cdot\bigl[[b,c],a\bigr]\cdot\bigl[[c,a],b\bigr]=1$$ Conversely, if this identity holds for all $a,b,c\in G$ then is it necessary that $[G,G]$ is abelian?",['group-theory']
1460442,How to show this function is increasing? Related to Normal distribution.,"Numerically, it seems the following function $F(x)$ is increasing in $x$. How can I show it analytically? $$F(x)=G(x)L'(x)$$ where $L(x)=\frac{(1-G(x))^3}{G'(x)}$ and $G(x)=\int_{-\infty}^x \frac{e^{- \frac{ 1}{2} y^2}}{\sqrt{2\pi}} dy$, and $L'(x)$ is derivative of $L(x)$. Note that $G(\cdot)$ can be interpreted as the CDF of standard normal distribution and $L(\cdot)$ is proportional to inverse hazard rate of standard normal distribution. We can show that $L(\cdot)$ is decreasing.","['normal-distribution', 'derivatives']"
1460481,"Gambler's Ruin variant: each bet is for 1/k dollars, what happens to probability of winning as k approaches infinity?","I am trying to a solve a variant on the Gambler's Ruin problem, in which two gamblers $A$ and $B$ make a series of bets until one of the gamblers goes bankrupt. $A$ starts out with $i$ dollars, B with $N-i$ dollars. The probability of A winning a bet is given by $p$, with $0 < p < 1$. Each bet is for $\frac{1}{k}$ dollars, with $k$ a positive integer. The problem asks us to find the probability that $A$ wins the game, and to determine what happens to this as $k \rightarrow \infty$. I know that the probability of $A$ winning in the normal gambler's ruin problem (i.e. when $k=1$) if $A$ starts out with $i$ dollars is $\frac{1-(\frac{q}{p})^i}{1-(\frac{q}{p})^n}$. My intuition is that the probability that $A$ wins the game approaches $0$ as $k \rightarrow \infty$ in this particular problem, but I am unsure of how to show this algebraically.","['probability', 'statistics', 'gambling']"
1460485,Existence of independent uniform random variables,"Let $\Omega = [0,1]$ equipped with the Borel $\sigma$-algebra and Lebesgue measure. Let $X:\Omega\rightarrow\mathbb{R}$ be defined by $X(\omega) = \omega$. Clearly $X$ is uniform $(0,1)$ distributed. Does there exist a random variable $Y:\Omega\rightarrow\mathbb{R}$ which is uniform $(0,1)$ distributed such that $X$ and $Y$ are independent? And if so, can we find a formula for $Y$? I'm convinced that $Y$ should exist, but I don't know how I would construct it.","['probability', 'probability-distributions']"
1460533,Compare $(5/6)^4$ and $(35/36)^{24}$ without calculation,Can anyone provide some hint how to compare $(5/6)^4$ and $(35/36)^{24}$ without calculation? Thanks! After some transformation this question is equivalent to check if $(5/6)^{20}(7/6)^{24}$ is larger than one or smaller than one.,"['algebra-precalculus', 'inequality']"
1460561,Higher order of Jordan form,"Let $\lambda =1$ is the eigenvalue corresponding to the single Jordan block $J$. Prove $J^m \sim J$ with an arbitrary positive integer $m$. My try: Because $\lambda = 1$ is eigenvalue, $(J-I)^m =0$. After that $(J-I)^{m-1} J = (J-I)^{m-1}$. At this point, I do not know how to continue.","['linear-algebra', 'matrices']"
1460568,Trigonometric identities from differential equation,"If one knows that the solutions of $y''+ y=0$ are two functions $s(x)$ and $c(x)$, and we know that $s(0)=0$, $s'(0)=1$, $c(0)=1$, $c'(0)=0$, then how can one start to prove that $s(x+a)=s(x)c(a)+c(x)s(a)$? What is it that has to be observed in order for one to understand how to initiate the proof? One is not supposed to use the actual trigonometric functions in this case. Here we assume that we know nothing about sin and cos.","['ordinary-differential-equations', 'trigonometry']"
1460574,Everyone knows that there is only one nonsingular conic tangent to five general lines in $\mathbb{P}^2$,"This is a statement in Fulton's Introduction to intersection theory:
''there is only one nonsingular conic tangent to five general lines in $\mathbb{P}^2$ ''
He implied that this is clear by duality, for example.
Anyone could Please give me a more lucid explanation?","['algebraic-geometry', 'intersection-theory']"
1460577,Cartesian product $\mathbb R^m \times E$ is measurable for $E \subset \mathbb R^n$ measurable,"I am trying to show that given $E \subset \mathbb R^n$ measurable, then $\mathbb R^m \times E \subset \mathbb R^{m+n}$ is measurable but I don't have a clue how to show this. I could think of the following: If $E$ is a measurable set, then $E=H \setminus N$, with $H$ a $G_{\delta}$ set and $N$ a null set, so $H= \bigcap_{n \in \mathbb N} G_n$, with $G_n$ is open. So $$\mathbb R^m \times E= \mathbb R^m \times (H \setminus N)$$$$=(\mathbb R^m \times \bigcap_{n \in \mathbb N} G_n) \setminus (\mathbb R^m \times N)$$$$=\bigcap_{n \in \mathbb N} (\mathbb R^m \times G_n) \setminus (\mathbb R^m \times N)$$ So it is sufficient to prove the proposition for open and null sets. I would appreciate help to show the proposition for these two types of sets. Thanks in advance","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1460590,"How to prove function $f(x,y)=\frac{1}{xy}$ is not uniformly continuous?","Here I consider uniform continuity of functions in $\mathbb{R}^n$. Take a function of two variables for example. We said that $f(x,y)$ is uniformly continuous if for any $\epsilon>0$, we can find a $\delta>0$ [depends on $\epsilon$ only] such that $|f(x_1,y_1)-f(x_2,y_2)|<\epsilon$ whenever $|x_1-x_2|+|y_1-y_2|<\delta$. 
Hence to find a counter example. We need is to show
\begin{align*}
	\exists\epsilon_0>0\,\,\text{such that}\,\,\forall\delta>0,\,\,\exists(x_1,x_2)\in \mathbb{R}^2\,\,\text{with}\,\,|x_1-x_2|<\delta\,\,\text{and}\,\,|f(x_1)-f(x_2)|\ge\epsilon_0
\end{align*} Now consider $f(x,y)=\frac{1}{xy}$, I was told that it is not uniformly continuous and I have trouble show this.
Could anyone help for a solution using the definition I wrote?","['analysis', 'calculus', 'real-analysis', 'multivariable-calculus']"
1460611,"Using Lagrange's theorem, prove that a non-abelian group of order $10$ must have a subgroup of order $5$.","Using Lagrange's theorem, prove that a non-abelian group of order $10$ must have a subgroup of order $5$. Attempt: Let $G$ be a group of order $10$. By Lagrange's theorem, if there exist a subgroup $H$ of $G$, the $o(H)=1,2,5~ or~ 10$. Assume that there is no subgroup of order $5$. Let, $e\neq a\in G$. Then we have $o(a)|o(G)$. If every element of $G$ is of order $2$, then $a^2=e$ i.e $a=a^{-1}$ for all $a\in G$. Then I can show that $G$ is abelian. This is a contradiction. Therefore, every element of $G$ is not of order $2$. Thus, $o(a)=5 ~or~ 10$.
If $o(a)=5$, then $H= \left< a \right>$ is a cyclic subgroup of order $5$. I don't know the approach is correct or not.","['abstract-algebra', 'group-theory', 'abelian-groups', 'cyclic-groups']"
1460642,All distances are rational prove the set is countable,"Question: Assume $E$ is a subset of $R^n$ , and $x,y\in E\Rightarrow d(x,y)\in \mathbb Q$ , show that $E$ is at most countable. ( $d$ is the Euclidean distance) I believe I've solved the $n=2$ case, which was 2013 real analysis qualifying exam problem at UCI. For general $n$ it was stated on the exam that the statement is true, without asking a proof. I'm wondering if anyone can give a more general solution because my solution for $n=2$ is difficult to extend to any $n$ . My solution for $n=2$ : If $E=\emptyset$ we are done. If not, take $x\in E$ , by the assumption $E\subset \underset{q\geq0,q\in Q}{\cup}\partial B(x,q)$ where $\partial B(x,q)=\{y\in R^2\mid d(x,y)=\sqrt{x^2+y^2}=q\}$ . Because countable union of countable sets is countable, and $Q$ is countable, it's sufficient to prove $C_q=E\cap \partial B(x,q)$ is countable for each $q$ . For any given $q$ , if $C_q=\emptyset$ we are done. If not, let $p_1\in C_q$ , if $p_2\in C_q$ , we can define $\theta=\angle p_2xp_1$ , $d(p_1,p_2)=2q\sin{\frac{\theta}{2}}\in Q\Rightarrow \sin{\frac{\theta}{2}}\in Q$ . Therefore it's sufficient to prove at most countably many $\theta$ satisfies $\sin{\frac{\theta}{2}}\in Q$ . To prove the last statement, for any given $s\in Q$ The solution set of $\sin{\frac{\theta}{2}}= s$ is $\emptyset$ (when $|s|>1$ ) or $\{\theta\mid \theta=2\arcsin{s}+4n\pi\}\cup \{\theta\mid \theta=2\pi-2\arcsin{s}+4n\pi\}$ where $n\in Z$ . The solution set is clearly countable, because $Q$ is countable, the solution set of $\sin{\frac{\theta}{2}}\in Q$ is countable union of countable set, therefore countable.","['real-analysis', 'general-topology']"
1460661,"If $f$ is continuously differentiable in $[a,b]$, $f(a)=f(b)$, and $f'(a)=f'(b)$, then there exist $a<x_1<x_2<b$ such that $f'(x_1) = f'(x_2)$.","This problem is from Apostol's Mathematical Analysis : Let $f$ be continuously differentiable in $[a, b]$. If $ f(a) = f(b)$ and if
  $f^{'}(a) = f^{'}(b)$, then prove that there exists $x_1$ and $x_2$ in $(a, b)$ such that $x_1\neq x_2$ but $f^{'}(x_1) = f^{'}(x_2)$. My try: By Rolle's Theorem $\exists x_0\in (a,b) $ such that $f^{'}(x_0)=0$.
How to guarantee existence of $x_1,x_2 $ from here? Can it be solved from a geometrical point of view?","['calculus', 'real-analysis', 'derivatives']"
1460697,The difference between replacement and substitution,"Is there any difference replacement and substitution? For example, Let $f(x)=x^2$ be a function defined on real values. If we replace $x$ by $x+1$, we get $$f(x+1)=(x+1)^2$$
In this case, can we say that we substitute $x$ with $x+1$? Normally, when we substitute $i$ with $j+1$, that means $i=j+1$. But as the former case, when we replace $x$ by $x+1$, that does not mean $x=x+1$. When we get $f(x+1)=(x+1)^2$ by replacement from $f(x)=x^2$, how to explain clearly that does not mean $x=x+1$?","['algebra-precalculus', 'functions']"
1460704,How do I symbolically prove that $\lim_{n \to \infty } (n-n^2)=- \infty $?,"Intuitively we know that $n^2$ grows faster than $n$, thus the difference tends to negative infinity. But I have trouble proving it symbolically because of the indeterminate form $\infty - \infty$. Is there anyway to do this without resorting to the Epsilon-Delta definition ?",['limits']
1460712,Li Shanlan's combinatorial identities,"I am struggling to prove the following combinatorial identities:
$$(1)\quad\sum_{r=0}^m \binom{m}{r}\binom{n}{r}\binom{p+r}{m+n} = \binom{p}{m}\binom{p}{n},\quad \forall n\in\mathbb N,p\ge m,n$$
$$(2)\quad\sum_{r=0}^m \binom{m}{r}\binom{n}{r}\binom{p+m+n-r}{m+n} = \binom{p+m}{m}\binom{p+n}{n},\quad \forall p\in\mathbb N$$
The book I found them in says they were discovered and proved by Chinese mathematician Li Shanlan in the 19th century but I have failed to find any of his works translated into English in the Internet. I am looking for an either combinatorial or algebraic solution.","['binomial-coefficients', 'combinatorics']"
1460736,P. Halmos Naive Set Theory Exercise 1 section 3,"There was this question asked a little while ago, and it was not closed: Consider the sets $∅,\lbrace ∅ \rbrace,\lbrace\lbrace ∅ \rbrace\rbrace $ etc.; consider the pairs such as $ \lbrace ∅ ,\lbrace ∅ \rbrace \rbrace$ formed by any two of them; Consider the pairs formed any two such pairs, or else the mixed pairs formed by any singleton and any pair; and proceed so on ad infinitum.
  Are all sets obtained in this way unique from each other? The question was never answered, so I am asking it again, but in a slightly different way. I am pretty stuck on this question because I don't quite get what it is asking. I was wondering, what does it mean by all sets; does this include the singletons  $∅,\lbrace ∅ \rbrace,\lbrace\lbrace ∅ \rbrace\rbrace $ ? And also, do sets being ""distinct"" mean that they are not equal? And finally, if someone could answer the question, but give hints, with proof, I would appreciate it. We have only looked at the Axiom of Specification, Extension and Pairing. EDIT: Here is the best proof that I could think of, please let me know where I went wrong, or could improve: It is obvious that by the remarks of Section 3, there exists sets $A = \lbrace a_1, a_2 \rbrace $ and $B = \lbrace b_1, b_2 \rbrace$ created in the specified way. And, by the equivalent formulation of the Axiom of Pairing and remarks of Section 3, $A$ and $B$ are the only such sets which contain $a_1$, $a_2$ and $b_1$, $b_2$, respectively. Now suppose, to the contrary, that $A$ and $B$ are not distinct; i.e. $A = B$. However, by the Axiom of Extension , $A = B$ implies $a_1, a_2 \in B$, and $b_1, b_2 \in A$; this is a contradiction, since by the previous remarks, $A$ and $B$ were the only sets which contained $a_1$, $a_2$ and $b_1$, $b_2$, respectively.",['elementary-set-theory']
1460787,"Let $Y$ be a finite-dimensional normed space, $X$ a normed space, and $T: X \to Y$ a surjective linear operator. Show that $T$ is an open mapping.","Let $Y$ be a finite-dimensional normed space, $X$ a normed space, and $T: X \to Y$ a surjective linear operator. Show that $T$ is an open mapping. I think if I can show that $T(B_X)$ contains an open ball then I am done where $B_X$ is the unit ball in $X$ . But I am unable to show that. Need some help...","['open-map', 'operator-theory', 'functional-analysis', 'normed-spaces']"
1460806,What is the general definition of time-scale for a differential equation?,"For a differential equation like $\dot{x}=ax$ where $x$ is a function on an independent variable $t$, and $\dot{x}=\frac{dx}{dt}$, and $a$ is a constant, we define the time-scale $\frac{1}{a}$, which if $a$ be a negative real number we can look at this concept as the half-life time of $x$ over $\ln 2$. And you know half-life time means the time you need to loose half of your initial amount of $x$. As you can see it at the following calculation. Let $a=-k$ where $k\in\mathbb{R}^{>0}$. From our differential equation we have $x=x_0e^{-kt}$ so if I show the half-life time by $\tau_{\frac{1}{2}}$ then
$$\frac{x_0}{2}=x_0e^{-k\tau_{\frac{1}{2}}}\Longrightarrow \ln 2=k\tau_{\frac{1}{2}}\Longrightarrow \tau_{\frac{1}{2}}=\frac{\ln 2}{k}$$
With this concept time-scale we do many things, one is choosing the step size for numerical simulation of our differential equation using Euler method for example. Or if we have a system of differential equations in the form above, we say which one has a faster effect on our populations by comparing time-scales of each present differential equation. But what should I define time-scale for a general form of a differential equation which at least do those two works for us that I mentioned?","['dynamical-systems', 'numerical-methods', 'ordinary-differential-equations']"
1460820,Why $\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A)$?,"I know that $A\cdot\mathrm{adj}(A) = \det(A) \cdot I$, but why $\mathrm{adj}(A)\cdot A = A\cdot\mathrm{adj}(A)$?","['determinant', 'linear-algebra', 'matrices']"
1460834,Show that $x^{n-1}+\cdots +x+1$ is irreducible over $\mathbb Z$ if and only if $n$ is a prime.,"I proved that if $n$ is a prime, then $p(x)=x^{n-1}+\cdots+x+1$ is irreducible over $\mathbb Z$. But, I don't know how to prove that if $p(x)$ is irreducible over $\mathbb Z$, then $n$ is prime. Can you give me a hints?","['ring-theory', 'irreducible-polynomials', 'abstract-algebra', 'polynomials', 'factoring']"
1460853,Infinite intersection of open sets,"I need to prove that the infinite intersection of open sets may [must] not be open.
I can show through examples that this is true, but this is not sufficient for a proof.
- Can somebody give a formal proof ?
Thanks.",['real-analysis']
1460855,A basic question on local cohomology,"Let $X$ be a smooth, projective variety, $i:X \hookrightarrow \mathbb{P}^n$ a closed immersion for some $n>0$, $U \subset X$ an open subset and $Z \subset X$ a local complete intersection subscheme. Denote by  $j:U \to \mathbb{P}^n$ the natural immersion. Let $\mathcal{F}$ be a locally free sheaf on $X$. Is $H^i_{Z \cap U}(j_*(\mathcal{F}|_U)) \cong H^i_{Z \cap U}(\mathcal{F}|_U)$?","['algebraic-geometry', 'sheaf-cohomology', 'local-cohomology', 'sheaf-theory']"
1460861,"Problems with Leibniz rule in calculating the covariant derivative of a $(1,1)-$ tensor. Where is my mistake?","Let be $$R=\sum _{\alpha, \beta} R^\alpha_\beta \frac{\partial}{\partial x^\alpha} \otimes dx^\beta. $$ I want to calculate $\nabla_\gamma(R)=\nabla_{\frac{\partial}{\partial x^\gamma}}(R).$ My book gives me this first result:
$$\nabla_\gamma(R)=\sum_{\beta}\big(\nabla_\gamma\sum_\alpha\big[R^\alpha_\beta\frac{\partial}{\partial x^\alpha}\big]\big)\otimes dx^\beta+\sum_\alpha \frac{\partial}{\partial x^\alpha}\otimes\big(\nabla_\gamma\sum_\beta\big[R^\alpha_\beta dx^\beta\big]\big).$$ I have tried to obtain this expression using the Leibniz rule for tensors but I have found something different for the second addendum:
$$\nabla_\gamma(R)=\sum_\beta\big( \nabla_\gamma\big[\sum_\alpha R^\alpha_\beta\frac{\partial}{\partial x^\alpha}\otimes dx^\beta\big]\big)=\\\sum_\beta\big(\nabla_\gamma\sum_\alpha\big[R^\alpha_\beta\frac{\partial}{\partial x^\alpha}\big]\big)\otimes dx^\beta+\sum_\beta\big(\sum_\alpha R^\alpha_\beta\frac{\partial}{\partial x^\alpha}\big)\otimes \nabla_\gamma dx^\beta=\\\\\sum_\beta\big(\nabla_\gamma\sum_\alpha\big[R^\alpha_\beta\frac{\partial}{\partial x^\alpha}\big]\big)\otimes dx^\beta+\sum_\alpha\big(\frac{\partial}{\partial x^\alpha}\otimes\sum_\beta\big(R^\alpha_\beta\nabla_\gamma dx^\beta \big) \big). $$ Where is my mistake? Why is this last expression  not correct?I have only used the rules of the covariant derivative but my result is different. How can I obtain the first formula? Thanks in advance for the help!","['differential-geometry', 'riemannian-geometry', 'tensors']"
1460864,Each of the two persons makes a single throw with a pair of unbiased dice.What is the probability that the throws are equal?,Each of the two persons makes a single throw with a pair of unbiased dice.What is the probability that the throws are equal? Since the same throws can result if either both of them get 1 or both of them get 2 or both of them get 3 or both of them get 4 or both of them 5 or both of them get 6. So I calculated probability as $\frac{1}{6}\times\frac{1}{6}+\frac{1}{6}\times\frac{1}{6}+\frac{1}{6}\times\frac{1}{6}+\frac{1}{6}\times\frac{1}{6}+\frac{1}{6}\times\frac{1}{6}+\frac{1}{6}\times\frac{1}{6}=\frac{1}{6}$ But my answer is wrong and the correct answer is $\frac{73}{648}$.Please help me with the correct approach to solve it.Thanks.,"['probability', 'combinatorics']"
1460881,How to calculate radius of the surface of water at different points in time when it is poured into a spherical container?,"So I have a sphere with radius $1\ cm$, and I'm pouring in water at $0.5\ cm^3/s$. How would I find a the radius of the surface of the water at any given time? So in the end this would look like a quadratic equation relating time and radius, where the maxima would be where radius $= 1\ cm$. How would I go about using differentiation in solving for this equation?","['volume', 'calculus', 'quadratics', 'derivatives']"
1460891,Prove that f is surjective,"The problem: Prove or refute the following: If $f,g,h: \mathbb{R} \to \mathbb{R}$ and $f \circ g \circ h$ is surjective then $f$ is surjective. My solution:
(The definition of surjective: iff $∀y ∈ T ,∃x ∈ S \implies f(x) = y$) Let $f\colon A \to B$, $g\colon B \to C$ and $h: C \to D$. Lets say $b ∈ A$, $a ∈  B$. We know by definition that $f(g(h(a)) = b$ Therefore $f(a) = b$, $g(a) = b$, $h(a)=b$, so $f$ is surjective. I am kind of confused (as you can see from my solution)
Please help, am I at least on the right track or completely wrong.","['elementary-set-theory', 'relations', 'functions']"
1460909,"If $τ_x^k$ is the time of the $k$-th entrance of a Markov chain into $x$, then $\text P_x[τ_y^k<∞]=\text P_x[τ_y^1<∞](\text P_y[τ_y^1<∞])^{k-1}$","Let $E$ be at most countable and equipped with the discrete topology and $\mathcal E$ be the Borel $\sigma$-algebra on $E$ $X=(X_n)_{n\in\mathbb N_0}$ be a discrete Markov chain with values in $(E,\mathcal E$) and distributions $(\operatorname P_x)_{x\in E}$ $\tau_x^0:=0$ and $$\tau_x^k:=\inf\left\{n>\tau_x^{k-1}:X_n=x\right\}$$ for $x\in E$ and $k\in\mathbb N$ Let $$\varrho(x,y):=\operatorname P_x\left[\tau_y^1<\infty\right]\color{blue}{=\operatorname P_x\left[\exists n\in\mathbb N:X_n=y\right]}\;.$$
I want to prove, that $$\operatorname P_x\left[\tau_y^k<\infty\right]=\varrho(x,y)\varrho(y,y)^{k-1}\;\;\;\text{for all }k\in\mathbb N\tag 1$$ using the strong Markov property: $$\operatorname E_x\left[f\circ (X_{\tau+t})_{t\in \mathbb N_0}\mid\mathcal F_\tau\right]=\operatorname E_{X_\tau}\left[f\circ X\right]\tag 2$$ for all $x\in E$, $\sigma(X)$-stopping times $\tau$ and bounded, $\mathcal E^{\otimes\mathbb N_0}$-measurable $f:E^{\mathbb N_0}\to\mathbb R$. I want to prove $(1)$ by induction over $k\in\mathbb N$. Since, $k=1$ is trivial, we only need to care about $k-1\to k$. Since $$\left\{\tau_y^{k-1}<\infty\right\}\cap\left\{\tau_y^k<\infty\right\}=\left\{\tau_y^k<\infty\right\}$$ and $$\left\{\tau_y^{k-1}<\infty\right\}\in\mathcal F_{\tau_y^{k-1}}\;,$$ we've got $$\operatorname P_x\left[\tau_y^k<\infty\right]=\operatorname E_x\left[1_{\left\{\tau_y^{k-1}<\infty\right\}}\color{red}{\operatorname P_x\left[\tau_y^k<\infty\mid\mathcal F_{\tau_y^{k-1}}\right]}\right]\;,\tag 4$$ by definition of the conditional expectation. Now, I think, that we somehow need to apply $(2)$ with $\tau=\tau_y^{k-1}$ to the $\color{red}{\text{red}}$ term in order to obtain $$\operatorname E_x\left[1_{\left\{\tau_y^{k-1}<\infty\right\}}\color{red}{\operatorname P_x\left[\tau_y^k<\infty\mid\mathcal F_{\tau_y^{k-1}}\right]}\right]=\operatorname E_x\left[1_{\left\{\tau_y^{k-1}<\infty\right\}}\varrho(y,y)\right]\;,$$ but I can't figure out how I need to choose $f$.","['probability-theory', 'measure-theory', 'markov-process', 'markov-chains', 'stochastic-processes']"
1460918,Is the formula $\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)}$ a standard result?,"Is the formula for the form $1^{\infty}$ that is  $$\lim\limits_{x\to a} (1+f(x))^{g(x)}=e^{\lim\limits_{x\to a}f(x)g(x)}$$ a standard formula? Does it have any special name?
Note:In the above formula $f(x)\to 0$ as $x\to a$ and $g(x)\to\infty$ as $x\to a$ I'm asking because I'm not sure whether it will be allowed in a subjective maths exam(it sometimes greatly simplifies calculations).Thanks.","['limits', 'soft-question']"
1460955,Prove derivative with summation by induction,"I have this math question. That I am stuck on. If $f$ is a function, let $Df$ be its derivative.  For $n\in
 \mathbb{Z}^+$ let $$ f^{(n)} = \underbrace{D \cdots D }_{n\mathrm{\
 times}} f $$ be the $n^\mathrm{th}$ derivative of $f$.  In this
   notation the usual product rule from calculus  says that   $$
 (fg)^{(1)} = fg^{(1)} + f^{(1)} g. $$ Using the product rule, prove
   the formula for the $n^\mathrm{th}$ derivative of a product  $$
 (fg)^{(n)} = \sum_{k=0}^n \binom{n}{k} f^{(n-k)} g^{(k)}. $$ (Hint: 
   The proof in here is similar to the proof of the Binomial Theorem.) Here's my work for it so far: $$(fg)^{(1)} = \sum_{k=0}^{1}\binom{1}{k}f^{(1-k)}g^k=f^{(1)}g^{(0)}+f^{(0)}g^{(1)}$$ We assume $P(m)$ is true (induction assumption): $$(fg)^{(m)}=\sum_{k=0}^{m}\binom{m}{k}f^{(m-k)}g^k$$ We want to show that $P(m+1)$ is also true: $$(fg)^{(m+1)}=\sum_{k=0}^{m+1}\binom{m+1}{k}f^{(m+1-k)}g^k$$ I'm not sure how to connect the induction assumption with $P(m+1)$ thanks.","['binomial-theorem', 'summation', 'proof-writing', 'derivatives']"
1460977,"Show that $\{x \in \mathbb{Q}:x \geq 0, x^2 \leq 2\}$ has no rational least upper bound.",Lets denote the least upper bound by  $\alpha \in \mathbb{Q}$ and $\delta > 0$ be a small number. Now $\alpha^2 \neq 2$ because there is no such rational $\alpha$. If $\alpha^2 > 2$ then $(\alpha +\delta)^2 >2$ and so $\alpha$ is not a least upper bound. I can't obtain a valid reason why $\alpha^2 < 2$ can not be the case.,['analysis']
1461016,Is this real-valued function differentiable at $0$?,"Define $$y(x)=\begin{cases} x^2 & x\ge 0 \\ -x^2 & x \lt 0\end{cases}$$ I find that this function continuous at $x=0$. However, when I try to find this function is differentiable at $x=0$: $$ \lim_{h\to 0+}\frac{f(x+h)^2-f(x^2)}{h}  = \lim_{h \to 0} \frac{2xh+h^2}{h}$$ therefore $f'(x) = 2x$ implies $f'(0)= 2\cdot 0 = 0$ $$ \lim_{h\to 0-}\frac{f-(x+h)^2-f(-x^2)}{h}  = \lim_{h \to 0} \frac{-2xh-h^2}{h}$$ therefore $f'(x)=-2x , f'(0) = 0$ The function is differentiable at $x=0$ Correct?","['derivatives', 'calculus', 'functions']"
1461038,How exactly does the sign of the dot product determine the angle between two vectors?,"I am told that $v\cdot w=0$ means that the angle between the vectors $v$ and $w$ is $90$ degrees. Then I am told that the sign of $v\cdot w$ (when it isn't equal to zero) determines whether the angle between vectors $v$ and $w$ is above or below $90$
degrees; The angle is above $90$ degrees when $v\cdot w<0$ and below $90$ degrees when $v\cdot w>0$ I have a picture from the book Introduction to Linear Algebra by Gilbert Strang, but it's quite confusing for me. I don't understand how the above information is reflected in this diagram. I would like an explanation of this for me as it will help me answer a question from the section's accompanying problem set.",['linear-algebra']
1461066,A Proof of the Hausdorffness of the Grassmannian Using the Basics,"$\DeclareMathOperator{\Span}{span} \newcommand{\R}{\mathbf R} \newcommand{\mc}{\mathcal} \DeclareMathOperator{\GL}{GL} \DeclareMathOperator{\grassman}{GR} \newcommand{\set}[1]{\{#1\}} \DeclareMathOperator{\id}{Id}$
I had previously asked here if there is a neat way to prove that the Grassmannian manifold is Hausdorff (without using matrices). Olivier  Begassat gave a nice answer which uses a clever idea endowing the vector space with an inner product. There were other interesting answers too. I now have a more basic proof of the same statement which I would like to share. I hope this will be useful to the reader. Definition. Let $V$ be an $n$-dimensional vector space. A $k$- frame in $V$ is an injective linear transformation $T:\R^k\to V$. The set of all the $k$-frames of $V$ is written as $F_k(V)$. It is clear that $F_k(V)$ is an open subset of $\mc L(\R^k, V)$ and is therefore a $kn$-dimensional manifold. We define an equivalence relation $\sim$ on $F_k(V)$ as follows:
Given two $k$-frames $S$ and $T$ in $V$, we write $S\sim T$ if and only if $T(\R^k)=S(\R^k)$.
It is clear that $\sim$ is an equivalence relation on $F_k(V)$.
Note that $T\sim S$ for $T, S\in F_k(V)$ if and only if there exists $\tau\in \GL_k(\R)$ such that $S=T\circ \tau$. Definition. We define the Grassmannian manifold $\grassman_k(V)$ as the quotient space $F_k(V)/\sim$. Remark. Note that $\R^k$ is in no way special here. We could have chosen any $k$ dimensional linear space. It can be easily seen that the Grassmannian remains undisturbed either as a set or a topological space under this change. We will make use of this flexibility shortly. We now set out to prove that the Grassmannian manifold is Hausdorff. We require some ground work to do that. Define $\pi:F_k(V)\to\grassman_k(V)$ as the projection map, that is, $\pi$ carries $T$ to the equivalence class of $T$ Theorem 1. The projection map $\pi:F_k(V)\to \grassman_k(V)$ as defined above is an open map. Proof. Let $U$ be open in $F_k(V)$. We need to show that $\pi(U)$ is open in $\grassman_k(V)$, for which we need to show that $\pi^{-1}(\pi(U))$ is open.
Let $T\in \pi^{-1}(\pi(U))$.
Thus $\pi(T)\in \pi(U)$, which is equivalent to saying that $T=S\circ \tau$ for some $S\in U$ and $\tau\in \GL_k(\R)$. Define a map $f_\tau:F_k(V)\to F_k(V)$ as $f_\tau(L)=L\circ \tau$ for all $L \in F_k(V)$.
learly $f_\tau$ is continuous with $f_{\tau^{-1}}$ being a continuous inverse to it.
Therefore, $f_\tau$ is a homeomorphism, and we deduce that $f_\tau(U)$ is open in $F_k(V)$.
Now we have $T\in f_\tau(U)$ and $f_\tau(U)\subseteq \pi^{-1}(\pi(U))$, showing that $\pi^{-1}(\pi(U))$ is open, and consequently $\pi$ is an open map. For any given $(n-k)$-dimensional subspace $A$ of $V$, let $\mc U_A$ denote the set of all the $k$-dimensional subspaces of $V$ which intersect $A$ trivially.
Thus for any member $B\in \mc U_A$, we have $V=A\oplus B$. Lemma 2. Given an $(n-k)$-dimensional subspace of $V$, the set $\mc U_A$ is open in $\grassman_k(V)$. Proof. Write $m=n-k$ and fix a basis $u_1 , \ldots, u_m$ of $A$.
For a member $T\in F_k(V)$, $\pi(T)$ intersects $A$ trivially if and only if $(u_1 \wedge \cdots \wedge u_m)\wedge(T\mathbf e_1 \wedge \cdots \wedge T\mathbf e_k)\neq 0$.
So we define a function $f:F_k(V)\to \Lambda^k V$ as
\begin{equation*}
f(T) = (u_1 \wedge \cdots \wedge u_m)\wedge(T\mathbf e_1 \wedge \cdots \wedge T\mathbf e_k)\neq 0
\end{equation*}
Thus $\pi(T)\in \mc U_A$ if and only if $f(T)\neq 0$ if and only if $T\in f^{-1}(\Lambda^k V-\set{0})$.
herefore, 
\begin{equation*}
\mc U_A=\textstyle\pi(f^{-1}(\Lambda^k V-\set{0}))
\end{equation*}
Now $f$ is continuous (smooth, even) because it is a restriction of a linear map on $\mc L(\R^k, V)$ to the open subset $F_k(V)$ of $\mc L(\R^k, V)$
Using the fact that $\pi:F_k(V)\to \grassman_k(V)$ is open, we see that $\mc U_A$ is open in $\grassman_k(V)$. Definition. Let $P$ and $Q$ be $k$ and $n-k$ dimensional subspaces of $V$ respectively such that $V=P\oplus Q$.
Given any linear map $T:P\to Q$, the graph $\Gamma(T)$ of $T$ is defined as 
$$
	\Gamma(T)=\set{x+Tx:\ x\in P}
$$ Lemma 3. Let $A$ be an $(n-k)$-dimensional subspace of $V$ and $B$ be a $k$-dimensional subspace of $V$ which intersects $A$ trivially.
Then $\mc U_A$ is homeomorphic to $\mc L(B, A)$. Proof. To prove this, we think of frames as injective linear maps from $B$ to $V$.
Let $p_{A, B}:V\to A$ be the projection onto $A$ with respect to $B$ and similarly we have $p_{B, A}$.
Note that for any $T\in \pi^{-1}(\mc U_A)$, we have $p_{B, A}\circ T:B\to B$ is a linear isomorphism.
Define a function $\Phi:\pi^{-1}(\mc U_A)\to \mc L(B, A)$ as $\Phi(T)=\Phi_T$, where the latter is defined as
\begin{equation*}
\Phi_T(x) = (p_{A, B}\circ T)\circ (p_{B, A}\circ T)^{-1}(x)
\end{equation*}
for all $x\in B$.
It can be shown that $\Phi_T=\Phi_S$ if and only if $\pi(T)=\pi(S)$ and that $\Phi$ is surjective.
Since $\pi$ is a quotient map, we get a bijective continuous map $\bar\Phi:\mc U_A\to \mc L(B, A)$ such that the following diagram commutes We now show that $\bar \Phi$ is a homeomorphism by constructing an inverse.
Define $g:\mc L(B, A)\to \mc U_A$ as $g(S)=\Gamma(S)$, that is, $g$ takes $S$ to its graph in $V$.
Let $B'\in \mc U_A$ be arbitrary and let $T:B\to V$ be defined as the restriction of $p_{B', A}:V\to B'$ to $B$.
Then it is easy to check that $\pi(T)=B'$ and thus $T\in \pi^{-1}(\mc U_A)$.
Therefore $\bar \Phi(B')=\Phi(T)$ and it can be easily checked that $\Gamma(\Phi(T))=B'$, showing that $g\circ \bar\Phi=\id$ and we are done. Theorem 4. The Grassmannian $\grassman_k(V)$ is Hausdorff. Proof. Let $B$ and $B'$ be any two distinct members of $\grassman_k(V)$.
  We can find an $(n-k)$ -dimensional subspace $A$ of $V$ which intersects both $B$ and $B'$ trivially.
  Now we use Lemma 3 and the Hausdorffness of $\mc L(B, A)$ to finish.","['differential-geometry', 'smooth-manifolds', 'grassmannian']"
1461105,strictly increasing functions and bijection,"$f $  is strictly increasing 
   \begin{align} 
& \rightarrow f^{-1} \text{is strictly increasing}\tag {i} \\
& \rightarrow f^{-1} \text{is strictly decreasing}\tag {ii}  \\
& \rightarrow f \quad \text{is injective}\tag {iii}\\
& \rightarrow f \quad \text{is surjective}\tag {iv}\\
& \rightarrow f^{-1} \text{is bijective}\tag {v}  
\end{align} which statements are true? (iii)  $f$ is injective since $x<y\rightarrow f(x)<f(y)$ (iv) I believe it doesn't need to be surjective.   $f:\Bbb N\to \Bbb N$ with $f(x)=x+1$ (v) since $f$ isn't bijective we cannot talk about $f^{-1} $ function","['analysis', 'monotone-functions', 'functions']"
1461184,Partial derivative of double summation mixing rule,"I am trying to derive the expression for fugacity for a Van der Waals gas using the fundamental thermodynamic equations. I have encountered a problem in trying to calculate the partial derivative of a double summation mixing rule as follows: $$
an^2 = \sum_i\sum_j n_in_j(1-k_{ij})\sqrt{a_ia_j}
$$
I know from the Prausnitz molecular thermodynamics text that the derivative for a simpler expression is as follows:
$$
an^2 = \sum_i\sum_j n_in_j\sqrt{a_ia_j}
$$
$$
 \frac{\partial{(an^2)}}{\partial{n_i}} = 2\sqrt a_i\sum_jn_j\sqrt a_j
$$ However, when I take the derivative of the expression involving the constant $k_{ij}$ I get the following:
$$
\frac{\partial{an^2}}{\partial{n_i}} = 2an_i(1-k_{ii})\sum_jn_j(1-k_{ij})\sqrt{a_j}
$$ Wondering if anyone has insight into a more clear, systematic way to take a partial derivative of a double sum. NOTE: $a_i,a_j,k_{ij}$ are all considered to be constants and $n$ is the total number of moles and therefore also a constant.","['summation', 'derivatives']"
1461189,LCM of binomial coefficients and related functions,"I know about the following identity: $$\displaystyle \text{lcm} \left( {n \choose 0}, {n \choose 1}, ... {n \choose n} \right) = \frac{\text{lcm}(1, 2, ... n+1)}{n+1}$$
1) Is there any method to find $$\displaystyle \text{lcm} \left( {n \choose 0}, {n \choose 1}, ... {n \choose k} \right)$$ for any $k \le n$ without evaluating all the binomial coefficients? 2)Let me define $$g(n,k)=\displaystyle \text k{n \choose k}$$
$$\displaystyle \text{lcm} \left( g(n,0), g(n,1), ... g(n,n) \right)=\text{lcm}(1, 2, ... n)$$
Can we find $$\displaystyle \text{lcm} \left( g(n,0), g(n,1), ... g(n,k) \right)$$ for any $k \le n$ ? EDIT:
$$\displaystyle \text{lcm} \left( {n \choose 0}, {n \choose 1}, ... {n \choose k} \right)=\frac{\text{lcm}(n-k+1,n-k+2, ... n+1)}{n+1}$$
$$\displaystyle \text{lcm} \left( g(n,0), g(n,1), ... g(n,k) \right)=\text{lcm}(n-k,n-k+1, ... n)$$","['least-common-multiple', 'number-theory', 'binomial-coefficients', 'arithmetic-functions']"
1461202,"Given a positive integer, how can we determine if it is the hypotenuse of a right triangle with integer side lengths?","Given a positive integer, how can we determine if it is the hypotenuse of a right triangle with integer side lengths? For example: $5$ it can be hypotenuse as its other sides $3$ & $4$ are integers. $13$ it can also hypotenuse as its other sides $12$ & $5$ are integers. $12$ can't be hypotenuse because other two sides can't be integers.","['geometry', 'triangles']"
1461203,Do there exist two primes $p<q$ such that $p^n-1\mid q^n-1$ for infinitely many $n$? [duplicate],"This question already has answers here : If $a\geq 2$, $a\nmid b$, and $a^n-1\mid b^n-1$ for all $n\in\mathbb{N}$, then $b=1$ (4 answers) Closed 2 months ago . We can prove that there is no integer $n>1$ such that $2^n-1\mid 3^n-1$. This leads to the following question: Is it true that for every pair of primes $p<q$ there are only finitely many integers $n$ such that $p^n-1\mid q^n-1$? Are there two primes $p<q$ and an integer $n>p+q$ such that $p^n-1\mid q^n-1$?Is it true that if $n>6$ then $2^n-1\nmid 5^n-1$? Edit: Here are some examples:
\begin{array}{ll}
2^{36}-1\mid 41^{36}-1,
&3^{12}-1\mid 97^{12}-1,\\
5^{6}-1\mid 37^{6}-1,
&7^{4}-1\mid 151^{4}-1.
\end{array} Now I prove there is no integer $n>1$ such that $2^n-1\mid 3^n-1.$ Proof: Denote $A=2^n-1$ and $B=3^n-1$. If $n$ is even then $3$ divides $A$ but not $B$, a contradiction. If $n$ is odd then $A\equiv -5\pmod {12}$. Since every prime greater than $3$ is $\equiv \pm1,\pm5 \pmod {12}$, some prime factor $p$ of $A$ must be congruent to $\pm5 \pmod {12}$. As $p \mid B$, we have $3^{n+1}\equiv 3 \pmod p$. Since $n+1$ is even, we get $(\frac{3}{p})=1$, a contradiction again.","['number-theory', 'divisibility', 'elementary-number-theory']"
1461217,If the Fourier transform of a measure is zero then the measure is zero,"If $\mu$ is a complex finite Borel measure on a separable real Hilbert space $H$ be such that $$\hat \mu (x) = \int \limits _H \Bbb e ^{\Bbb i \langle x, y \rangle} \Bbb d \mu _{(y)} = 0, \ \forall x \in H ,$$ then $\mu = 0$. I have found a similar problem on $\Bbb R$ but the hints given there do not help me much in this case. The authors of the book where I have found this suggest the following: 1) show that $\mu \big( \{ y \in H \mid \langle x, y \rangle \ge \alpha \} \big) = 0$ 2) it follows that the measure of every closed convex set is $0$ 2') in particular, the measure of closed balls is $0$ 3) then the measure of every strongly measurable set is $0$ 4) so $\mu = 0$. I do not understand almost anything of the above, my knowledge of measure theory being at an undergraduate level. In particular, I do not know what a strongly measurable set is. Could anyone please at least sketch a proof, following the above lines or an alternate idea? (The context is to show that the Banach algebra of complex finite measures embeds topologically in the Banach algebra of complex continuous bounded functions.)","['fourier-analysis', 'hilbert-spaces', 'measure-theory']"
1461225,Why does the notion of vanishing at infinity require local compactness|,"The definition I know of 'vanishing at infinity' for locally compact topological space is the following: A function $f:X\to (Y,||\cdot||)$ on a locally compact space $X$ is said to vanish at infinity if for every $\epsilon>0$ the set $\{x\in X|\ ||f(x)||\geq\epsilon\}$ is compact. However, I don't understand the necessity of the local compactness of $X$. To me it seems that if we skip that requirement, then on a non-locally-compact space, there will probably be only the null-function which vanishes at infinity, so the whole notion becomes a bit useless, but is still well-defined I think? So can someone elaborate on why this is requirement is necessary? Edit: I don't think it's true that only the null-function would vanish at infinity, but the function would have to be $0$ at non-locally-compact points.",['general-topology']
1461229,How to prove $\frac{n^n}{3n!}<\frac{e^n}{2}-\sum_{k=0}^{n-1}\frac{n^k}{k!}<\frac{n^n}{2n!}$,I met this problem: prove: $\displaystyle \frac{n^n}{3n!}<\frac{e^n}{2}-\sum_{k=0}^{n-1}\frac{n^k}{k!}<\frac{n^n}{2n!}$ I tried expand $e^n$ at $x=0$ then: $\displaystyle e^n=\sum_{k=0}^{n}\frac{n^k}{k!}+\frac1{n!}\int_0^ne^t(n-t)^ndt$ but I have no idea next... And another way of thinking:both side divided $e^n$ the LHS became $\displaystyle \frac{n^n}{3n!e^n}$ then can we use the stirling's fomula to find sth.? Could someone help me? Thanks!,"['power-series', 'limits', 'integration', 'analysis', 'inequality']"
1461231,Continuity of the Fourier transform of a measure,"If $\mu$ is a complex finite Borel measure on a separable real Hilbert space $H$ then $$x \mapsto \hat \mu (x) = \int \limits _H \Bbb e ^{\Bbb i \langle x, y \rangle } \Bbb d \mu _{(y)}$$ is continuous. This slightly reminds me of showing that the convolution of a function in $L^p$ and another one from $L^{\frac {p+1} p}$ is continuous. In this latter case, the proof was done in steps, showing things for step functions, then for linear combinations of them and finally taking a limit, but I do not know whether this approach can be mimicked here. Edit: An application of the Lebesgue dominated convergence theorem quickly proves the above. Question closed.","['fourier-analysis', 'continuity', 'hilbert-spaces', 'measure-theory']"
1461238,Solving a system of differential equations of order 3,"We have the set of differential equations $$w_{1}+\frac{d^{2}}{dt^{2}}w_{1}-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ $$w_{1}-\frac{d}{dt}w_{1}-w_{2}+\frac{d}{dt}w_{2}=0$$ The question is to show that every strong solution of the above can be written as $$w(t)=\begin{bmatrix}\alpha_{1}-3\alpha_{2}\\\alpha_{1}\end{bmatrix}e^{t}+\begin{bmatrix}\alpha_{2}\\\alpha_{2}\end{bmatrix}te^{t}+\begin{bmatrix}\beta\\\beta\end{bmatrix}e^{-2t}+\begin{bmatrix}\gamma\\\gamma\end{bmatrix}e^{-t}$$ In other words, I want to find a solution to $(P(\frac{d}{dt})w)(t)=0$ for all $t\in\mathbb{R}$. That is $$P(\frac{d}{dt})w=\begin{bmatrix}1+\frac{d^{2}}{dt^{2}} & -3-\frac{d}{dt}+\frac{d^{2}}{dt^{2}}+\frac{d^{3}}{dt^{3}}\\1-\frac{d}{dt} & -1+\frac{d}{dt}\end{bmatrix}\begin{bmatrix}w_{1}\\w_{2}\end{bmatrix}(t)=0$$ I calculated $\det P(\xi)=(\xi-1)^{2}(\xi+1)(\xi+2)$, the roots of which are $\lambda_{1,2}=\pm 1$ and $\lambda_{3}=-2$, with $\lambda_{1}=1$ having multiplicity 2. I don't know where to go from here though. I think I have to calculate the eigenvectors, but given the nature of my matrix (which is composed of $\frac{d}{dt}$ rather than constant values) it seems a bit tricky. Edit: I tried taking the Fourier transform $$\begin{cases}F(w_{1}(t))+F(\frac{d^{2}}{dt^{2}}w_{1}(t))-F(\frac{d}{dt}w_{2}(t))+F(\frac{d^{2}}{dt^{2}}w_{2}(t))+F(\frac{d^{3}}{dt^{3}}w_{2}(t))=0\\F(w_{1}(t))-F(\frac{d}{dt}w_{1}(t))-F(w_{2}(t))+F(\frac{d}{dt}w_{2}(t))=0\end{cases}$$ To get $$\begin{cases}\hat{w}_{1}(f)+(2\pi if)^{2}\hat{w}_{1}(f)-2\pi if\hat{w}_{2}(f)+(2\pi if)^{2}\hat{w}_{2}(f)+(2\pi if)^{3}\hat{w}_{2}(f)=0 \\ \hat{w}_{1}(f)-2\pi if\hat{w}_{1}(f)-\hat{w}_{2}(f)+2\pi if\hat{w}_{2}(f)=0\end{cases}$$ But when I simplify, I get $$\begin{cases}(1-4\pi^{2}f^{2})\hat{w}_{1}(f)=(2\pi if+4\pi^{2}f^{2}+8\pi^{3}if^{3})\hat{w}_{2}(f) \\ (1-2\pi if)\hat{w}_{1}(f)=(1-2\pi if)\hat{w}_{2}(f)\end{cases}$$ Which cannot be right because then we would have that $\hat{w}_{1}(f)=\hat{w}_{2}(f)$. Edit 2: (Correction of Edit 2) Edit 3: For the second equation, we have $$\frac{d}{dt}(e^{-t}(w_{1}(t)-w_{2}(t)))=0$$ Now, plugging in $w_{1}(t)=w_{2}(t)+ce^{t}$ into the first equation yields $$w_{2}(t)+ce^{t}+\frac{d^{2}}{dt^{2}}(w_{2}(t)+ce^{t})-3w_{2}-\frac{d}{dt}w_{2}+\frac{d^{2}}{dt^{2}}w_{2}+\frac{d^{3}}{dt^{3}}w_{2}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=-2ce^{t}$$ First we want to calculate the homogeneous part of the equation. That is $$\frac{d^{3}}{dt^{3}}w_{2}(t)+2\frac{d^{2}}{dt^{2}}w_{2}(t)-\frac{d}{dt}w_{2}(t)-2w_{2}(t)=0$$ The characteristic equation is $$r^{3}+2r^{2}-r-2=0$$ i.e. $$(r-1)(r+2)(r+1)=0$$ Hence we get the general solution: $$w_{2}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Now I want to find the particular solution, $w_{2}(t)_{p}$. The inhomogeneous part is $f(t)=-2ce^{t}$ Take $w_{2}(t)_{p}=-kte^{t}$. Then $\frac{d}{dt}w_{2}(t)_{p}=-kte^{t}-ke^{t}$; $\frac{d^{2}}{dt^{2}}w_{2}(t)_{p}=-kte^{t}-2ke^{t}$; $\frac{d^{3}}{dt^{3}}w_{2}(t)_{p}=-kte^{t}-3ke^{t}$. $$-kte^{t}-3ke^{t}-2kte^{t}-4ke^{t}+kte^{t}+ke^{t}+2kte^{t}=-2ce^{t}$$ i.e. $$-6ke^{t}=2ce^{t}$$ So $k=\frac{c}{3}$. That gives us the general solution $w_{2}(t)_{h}+w_{2}(t)_{p}$: $$w_{2}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{3}te^{t}$$ Now we plug $w_{2}(t)=w_{1}(t)-ce^{t}$ into the first equation to get $$w_{1}(t)+\frac{d^{2}}{dt^{2}}w_{1}(t)-3w_{1}(t)+3ce^{t}-\frac{d}{dt}w_{1}(t)-ce^{t}+\frac{d^{2}}{dt^{2}}w_{1}(t)-ce^{t}+\frac{d^{3}}{dt^{3}}w_{1}(t)-ce^{t}=0$$ i.e. $$\frac{d^{3}}{dt^{3}}w_{1}(t)+2\frac{d^{2}}{dt^{2}}w_{1}(t)-\frac{d}{dt}w_{1}(t)-2w_{1}(t)=-ce^{t}$$ The characteristic equation of the homogeneous part of the equation is $$r^{3}+2r^{2}-r-2=0$$ Again, we get $$w_{1}(t)_{h}=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}$$ Take $w_{1}(t)_{p}=-kte^{t}$. Then substituting gives $-6ke^{t}=-ce^{t}$, thus $k=\frac{c}{6}$. So $w_{1}(t)_{p}=-\frac{c}{6}te^{t}$. Hence $$w_{1}(t)=C_{1}e^{-2t}+C_{2}e^{-t}+C_{3}e^{t}-\frac{c}{6}te^{t}$$ So we can write $$w(t)=\begin{bmatrix}
w_{1}(t) \\ w_{2}(t)
\end{bmatrix}=\begin{bmatrix}
C_{3} \\ C_{3}
\end{bmatrix}e^{t}+\begin{bmatrix}
-\frac{c}{6} \\ -\frac{c}{3}
\end{bmatrix}te^{t}+\begin{bmatrix}
C_{1} \\ C_{1}
\end{bmatrix}e^{-2t}+\begin{bmatrix}
C_{2} \\ C_{2}
\end{bmatrix}e^{-t}$$ However, this is not the solution I was after.","['analysis', 'eigenvalues-eigenvectors', 'ordinary-differential-equations']"
1461269,Why do differentiation rules work? What's the intuition behind them? (Not asking for proofs),"Differentiation rules have been bugging me ever since I took Basic Calculus. I thought I'd develop some intuitive understanding of them eventually, but so far all my other math courses (including Multivariable Calculus) take the rules for granted. I know how to prove some of the rules. The problem is that algebra manipulation alone isn't quite convincing to me. Is there any possibility of understanding why the algebra happens to work that way? For example, why do the slopes of the tangent line to the parabola x^2 happen to be determined by 2x? Looking at it graphically, there's no way I could've told that. Any sources covering this issue (books; internet sites; etc) would be very greatly appreciated. Thanks in advance.","['calculus', 'real-analysis', 'intuition', 'derivatives']"
1461320,Struggling to understand real projective space,"My ultimate goal is to show that the real projective space $\mathbb{P}^n_{\mathbb{R}}$ is an $n$-manifold. But first I'd like to understand the topological structure of $\mathbb{P}^n_{\mathbb{R}}$. The quotient topology on $\mathbb{P}^n_{\mathbb{R}}$ is defined via the quotient map $q \colon \mathbb{R}^{n+1} \backslash \{0\} \to \mathbb{P}^n_{\mathbb{R}}$ sending $x \mapsto [x]$, its linear span. Thus open subsets of $\mathbb{P}^n_{\mathbb{R}}$ are collections of lines through the origin in $\mathbb{R}^{n+1}$ (correct)? Then $\beta := \{U_i\}_{0 \le i \le n}$ is a basis for the topology on $\mathbb{P}^n_{\mathbb{R}}$ (correct?), where $U_i = \{[(x_0, \ldots, x_n)]: x_i = 1 \}$ since each $U_i$ contains every line through the origin for a fixed $i$; so $\mathbb{P}^n_{\mathbb{R}}$ is second countable. Proving $\mathbb{P}^n_{\mathbb{R}}$ is Hausdorff is where I am having a bit of trouble. I understand that this should be true since distinct points in $\mathbb{P}^n_{\mathbb{R}}$ are just distinct lines in $\mathbb{R}^{n+1}$, but I cannot think of a way to say this precisely. What is an open neighborhood of a point in $\mathbb{P}^n_{\mathbb{R}}$? I think it is the line itself with a ""bundle"" of the lines contained in a circle around it. But again, I am unsure of how to make this precise. This is my attempt: let $[v]$ and $[w]$ be distinct points in $\mathbb{P}^n_{\mathbb{R}}$. Let $d(v,w) = \delta$ (the Euclidean metric on $\mathbb{R}^{n+1}$), then $U = \{[u]: d(u,v) < \frac{\delta}{2} \}$ and $Z = \{[z] : d(z,w) < \frac{\delta}{2}\}$ are disjoint open neighborhoods of $[v]$ and $[w]$, respectively. I think I am okay with the locally Euclidean property. Suggestions/corrections would be appreciated.","['projective-space', 'quotient-spaces', 'manifolds', 'general-topology']"
1461346,Intersection of two non-linear equations?,"Graphically it is clear that $$1 + 2e^{{(x-y)}^2}(x-y) = 0$$ $$e^{{(x-y)}^2} - y = 0$$ has a unique solution, but how do I solve this analytically? If this is not possible, then what would be an appropriate technique to try and estimate the answer.","['nonlinear-system', 'multivariable-calculus', 'algebra-precalculus']"
1461363,Closed unit ball of Hilbert space: sequentially compact in weak topology?,"Following the proof of the existence theorem in chapter 1 of Hofer-Zehnder, Symplectic Invariants and Hamiltonian Dynamics , I find: We have used the well-known fact that the closed unit ball of a Hilbert space is weakly sequentially compact. I googled a bit, and found this . I have also managed to put together a fully understandable proof of the same result from Wikipedia, where I specify fully understandable (to me) because I know it is, whereas that pdf is something I haven't read through yet. But the point is the pdf assumes the Hilbert space is separable . Now, HZ uses this to deduce a bounded sequence in $H_1(S^1)$, the space of absolutely continuous $2\pi$-periodic functions $\mathbb{R}\to\mathbb{R}^{2n}$ with square-integrable derivatives (i.e. derivatives in $L_2(S^1)$). I can justify this usage of the result only in two ways: The result holds for all Hilbert spaces, not only separable ones, which is what the form of the statement suggests; That space is separable. Which is true? And how would I prove it?","['hilbert-spaces', 'functional-analysis', 'compactness']"
1461368,Which sheaves on a projective bundle are flat over the base scheme?,"Assume $X$ is a noetherian scheme over $\mathbb{C}$ and $E$ a locally free sheaf of finite rank on $X$. Denote the the associated projective bundle by $f: \mathbb{P}(E)\rightarrow X$. Are there any coherent sheaves on $\mathbb{P}(E)$ that are flat over $X$ except locally free ones? I'm especially interested in such $G\in Coh(\mathbb{P}(E))$ with the property that the canonical morphism $f^{*}f_{*}G\rightarrow G$ is an isomorphism. For these $G$ we have $G_{|\mathbb{P}(E(x))}=\mathcal{O}_{\mathbb{P}(E(x))}^{r_x}$ for all closed points $x\in X$ and some $r_x\geq 1$. So we have $H^i(\mathbb{P}(E(x)),G_{|\mathbb{P}(E(x))})=\{0\}$ for all $i\geq 1$. I'm trying to see that this implies $R^if_{*}G=0$ for all $i\geq 1$, for which flatness of $G$ over $X$ is needed.","['algebraic-geometry', 'sheaf-theory', 'commutative-algebra']"
1461379,How do I show that $\overline{\mathbb{Q}} := \{\alpha \in \mathbb{C}\mid \alpha\text{ is algebraic over }\mathbb{Q} \}$ is algebraically closed?,"How do I show that $\overline{\mathbb{Q}} := \{\alpha \in \mathbb{C}\mid \alpha\text{ is algebraic over }\mathbb{Q} \}$ is algebraically closed?
I am thinking about solving this using that $\mathbb{C}$ is algebraically closed but I don't know hot to proceed from here.",['abstract-algebra']
1461392,"Is $G/N$ cyclic if $N$ is a subgroup generated by $(2,3)$?","I am given $G=\mathbb{Z}_6 \oplus\mathbb{Z}_6$ and I am told to let $N$ be its cyclic subgroup generated by $(2,3)$. Is the group $G/N$ cyclic? Why? And if it is, what is its generator? So: Since the order of $\langle 2\rangle$ in $\mathbb{Z}_6$ is $3$ and the order of $\langle 3\rangle$ in  $\mathbb{Z}_6$ is $2$, the order of $(2,3)$ in $\mathbb{Z}_6 \oplus\mathbb{Z}_6$ is $6$. This is where im stuck. I'm not sure how to find out if $\mathbb{Z}_6 \oplus\mathbb{Z}_6/ \langle (2,3)\rangle$ is cyclic. I know a group is cyclic if $G=\langle g\rangle=\{g^{n}\mid n \in \mathbb(Z)\}$. I also know it has to do with relatively prime numbers, but I can't see how to put it together. Thanks in advance!","['abelian-groups', 'group-theory', 'finite-groups']"
1461429,Is norm $E[|X|^p]^{1/p}$ a continous function of $p$,"Suppose $X$ is a random variable such that $E[|X|^p]<\infty$ is a function
\begin{align}
f(p)=E[|X|^p]^{1/p}
\end{align}
continuos function of $p$ for $1 \le p < \infty$. Here is my attempt to show that the function is continuos as $c$.
I tried to use $\epsilon-\delta$ definitions. So, I want to show that for every $\epsilon>0$ there exists $\delta>0$ such that $|c-p| \le \delta$ implies
\begin{align}
|f(p)-f(c)| \le \epsilon.
\end{align} But I could'n get the above in the form \begin{align}
|f(p)-f(c)|=|E[|X|^p]^{1/p}-E[|X|^c]^{1/c}| \le K |c-p|
\end{align}
for some constant $K$.  If I could do the above the rest would follow by picking $\delta=\frac{\epsilon}{K}$. Thanks for any help Edit Based on the suggestion by  @Did Let $q$ be a conjugate exponent of $p$ and $r$ be conjugate exponent of $c$ then \begin{align}
\left|||X||_p-||X||_c \right|&=  \left| \sup_{Y: ||Y||_q \le 1}||XY||_1- \sup_{Y: ||Y||_r \le 1}||XZ||_1 \right|\\
& \le \sup_{Y: ||Y||_q \le 1}||XY||_1+ \sup_{Y: ||Y||_r \le 1}||XY||_1 \text{ by Triangle Inequality}\\
& \le 2\sup_{Y: ||Y||_{\max(q,r)} \le 1}||XY||_1 \text{ taking sup over the largest domain }
\end{align}","['probability-theory', 'expectation', 'random-variables', 'functional-analysis', 'normed-spaces']"
1461447,Algebarically solving $-2x^2 =\ln(x)$,Is it possible to solve $-2x^2 = \ln(x)$ without using a calculator?,"['algebra-precalculus', 'logarithms']"
1461448,Solve $\cos{(7x)}+\sin{(3x)}=0$,"Solve $\cos(7x)+\sin(3x)=0$ I'm stuck. Help me, please. I did $\cos(7x)=\cos(4x+3x)=\cos(4x)\cos(3x)-\sin(4x)\sin(3x)$ So the original equation becomes $\cos(4x)\cos(3x)-\sin(4x)\sin(3x)+\sin(3x)=0$ But that became long and ugly. What can I do now?",['trigonometry']
1461450,There is a smart way to solve this 2x2 system of linear equations?,"I was helping a high school student to study system of equations and he showed me this problem: \begin{align}\frac{x-a}{b} + \frac{y-b}{a} &= x-a \\
\frac{x+y-b}{a}+\frac{y+b}{b} &= 3 .
\end{align} I suggested to him to solve for one of the variables (e.g.. $x$) and then substitute, but the problem becomes painfully hard, because of all the literals, and fractions. So, there is a intelligent way to solve this system of equations?","['systems-of-equations', 'linear-algebra', 'algebra-precalculus']"
1461484,"Why do mathematicians say that ""let an operator be represented by a matrix"" instead of operator is the matrix?","For example, look at this sentence from Perko's text on dynamical system ""It follows from Cauchy Schwarz inequality that if $T \in L(R^n)$ is represented by the matrix $A$ with respect to the standard basis for $R^n$ $_\cdots$"" pg 11 What does it mean for a $T: R^n \to R^n$ to be represented by a matrix? Isn't it by definition that $T: R^n \to R^n$ is equivalent to an n by n matrix? Can someone translate exactly what it means by ""represented"" versus ""not represented""?","['operator-theory', 'linear-algebra']"
1461537,Question About Definition of Almost Everywhere,"I suppose I'm a bit confused about the definition in the following regard: A property holds a.e. if it holds everywhere except for a set of measure $0$. Now, if the particular property is only defined for a set of measure $0$, is it a.e. by default? Say I have two 'continuous' (standard topology) sequences $f,g: \mathbb{N} \to \mathbb{R}.$ Are we then allowed to say that $f = g$ a.e.? Or instead do the functions have to be defined on a set of non-zero measure and a.e. refers to some measure $0$ subset? I ask because a homework exercise asks me if two real functions are continuous and agree a.e. on a subset of $\mathbb{R}$, are necessarily identically equal. Clearly this is true if the points are not isolated, since if continuous functions disagree at some point, they must disagree on a non-zero measure set since open sets have non-zero measure. Though it need not be if I just use sequences. So, what are the requirements to use the phrase a.e.? Wolfram definition: A property of $X$ is said to hold almost everywhere if the set of points in $X$ where this property fails is contained in a set that has measure zero. This would seem to imply that it is a.e. by default.","['definition', 'measure-theory', 'real-analysis', 'functional-analysis', 'analysis']"
1461557,How to calculate the derivative of the pseudo-inverse matrix,"I have the following equation: $$
\begin{equation} 
 x=(A^TA)^{-1}A^Tb
\end{equation}
$$ and I need to calculate the derivative of k-th entry of x with respect to A: $$
\begin{equation} 
\frac{\partial x(k)}{\partial A}
\end{equation}
$$ I really don't know how to solve it! Can someone help me? Thank you!","['derivatives', 'pseudoinverse', 'matrices']"
1461576,Lebesgue integral is finite but functions are unbounded in every interval,"The following is a question in my midterm exam. Let $f_n: \mathbb{R} \rightarrow \mathbb{R}$ be a seq. of Lebesgue integrable functions such that 
$$
\int_{-\infty}^{\infty} f_n(x) dx =1
$$
and $\sup_n f_n$ is unbounded in every open interval. Find such a sequence of $f_n$'s. I've tried to think along the lines that for every irrational number my functions are $0$. But I'm unable to proceed along this direction since I don't know how to manipulate the functions at rational numbers to meet the above criteria.","['lebesgue-integral', 'measure-theory']"
1461620,Show that $R \cap R^{−1}$ is an equivalence relation on $X$?,"Let $R$ be a reflexive and transitive relation on $X$. Show that $R \cap R^{−1}$ is an equivalence relation on $X$. I am stuck and feel like I am doing something wrong, any help would be much appreciated! Here is what I have so far: $R = \{1,2,3\}$ $X = \{2,3,4,5,6\}$ $R$ on $X = \{(1,2), (1,3),(1,4),(1,5),(1,6),(2,4),(2,6),(3,6)\}$ $R^{-1} = \{1,1/2,1/3\}$ $R \cap R^{-1} = \{1\}$ $R^{-1}$ on $X = \{(1,2),(1,3),(1,4),(1,5)(1,6)\}$ TRUE Am I completely off-base or am I close? Thanks!","['elementary-set-theory', 'proof-verification', 'relations', 'discrete-mathematics']"
1461626,"find the p.d.f. first by determining their d.f.’s, and secondly directly..","If the r.v. $X$ is distributed as Negative Exponential with parameter $λ$, find the p.d.f. of each one of the r.v.’s $Y, Z$, where $Y = e^X,\, Z = \log{X}$, first by determining their d.f.’s, and secondly directly.. Could any one help me with this question where $f(x)=λ e ^{− λ x}$ in Negative Exponential I got $fY(y)=\dfrac{1}{y\ fx(\log{y})}$,  $fZ(z)=e^z fx(e^z)$ in the direct way but I am not sure of my answer","['statistics', 'proof-verification']"
1461662,What is the probability that the number of zeros of a binary sequence with length $m$ is at least $q$?,"For a practical problem for a project (not educational) for a friend I need your help. We have a sequence $b$ of $m$ binary values: $$b_1,...,b_m $$ Update: We have sequence b'of m' hexadecimal ($b_i$=0,1,...15) values: $$b_1,...b'_m$$ $n<m$ of them have value 1, the other $m-n$ elements have value 0 $n'<m'$ of them don't have value 0 (but 1-15), the other $(m'-n')$ elements have value 0 We choose $x<m$ elements and randomly change their values (independenty). So if $b_j$ is one of those $x$ elements, then the probability that its value is 0 is $1/2$ and the probability that its value if 1 is $1/2$ after modifying. We choose $x'<m'$ elements and randomly change their values (independenty). So if $b_j$ is one of those $x'$ elements, the probability that its value  is 0 is $1/16$ and the probability that its value is not 0 is $15/16$ after modifying. After that I want to know what the probability is for having at least $q<m$ zeros in the modified elements. After that I want to know what the probability is for having at least $q'<m'$ zeros in the modified elements. I need a method to calculate this probability $P(m,n,x,q)$. I need a method to calculate this probability $P'(m',n',x',q')$. $m > 1000$ $0<n<0.3m$ $0<x<0.6m$ $0.5m <q < m$ Any idea? Thanks a lot ! In addition: if a mathematical approach is not available, I will also accept any answer that contains a complete java or matlab code where I just can plug in $m,n,x$ and $q$ with output $P$.","['calculus', 'limits', 'algebra-precalculus', 'combinatorics', 'probability']"
1461675,Solving IVP with differential equation $y'' - 2y' + 2y = x + 1$,"I need to solve the following IVP. I've just recently started to learn about differential equations, so I would appreciate if you could check my work, and point out any mistakes. Also, should problems of this type always be solved by the next method? Problem: Solve the following initial value problem: \begin{align*} y'' - 2 y' + 2y = x + 1, \qquad y(0) = a, \quad y'(0) = 0. \end{align*} Attempt: The associated homogeneous differential equation $$ y'' - 2 y' + 2y = 0 $$ has characteristic equation $$ r^2 - 2r + 2 = 0. $$ The complex roots of this equation are $$ r_{1,2} = 1 \pm i. $$ Hence the homogeneous differential equation has the following general solution: \begin{align*} y_h (x) = e^x (c_1 \cos x + c_2 \sin x), \end{align*} with $c_1, c_2$ constants. The RHS $f(x) = x + 1$ of the original ODE can be solved by the method of undetermined coefficients, and we guess a solution has the form $$ y_p(x) = Ax + B $$ with $y_p'(x) = A$ and $y_p''(x) = 0$. Substituting this in the original differential equation gives $$ 0 - 2A + 2(Ax + B) = x + 1. $$ Equating coefficients of like powers gives \begin{align*} \begin{cases} 2A &= 1  \\ -2A + 2B &= 1 \end{cases} \end{align*} so that $A = 1/2$ and $B = 1$. So a particular solution of the ODE is $$ y_p(x) = \frac{1}{2} x + 1. $$ But this does not have the required initial conditions. So we write the general solution of the ODE as \begin{align*} y(x) &= y_h(x) + y_p(x) \\ &= e^x (c_1 \cos x + c_2 \sin x) + \frac{1}{2} x + 1 \end{align*} so that \begin{align*} y'(x) = c_1 e^x (\cos x - \sin x) + c_2 e^x (\sin x + \cos x) + \frac{1}{2}. \end{align*} Then, on applying the initial conditions: \begin{align*} y(0) = a = c_1 + 1 \qquad \text{and} \qquad y'(0) = 0 = c_1 + c_2 + \frac{1}{2} \end{align*} Together this gives $c_1 = a - 1$ and $c_2 = \frac{1}{2} - a$. Hence the solution of the given IVP is $$ y(x) = (a-1) e^x \cos x + (\frac{1}{2} - a) e^x \sin x + \frac{1}{2}x + 1. $$",['ordinary-differential-equations']
1461676,How to evaluate the limit of $\frac {2\cos({x-1})-2}{({x²}-2\sqrt{x}+1 )}$ when $x\to1$ without using L'Hospital's rule?,How do I evaluate this without using L'Hospital's rule:$$\lim_{x \to 1}\frac {2\cos({x-1})-2}{({x²}-2\sqrt{x}+1 )}\ ?$$ Note : I used L'Hospital's Rule I find $\frac{-4}{3}$ but in wolfram alpha is $0$,"['limits-without-lhopital', 'limits', 'real-analysis', 'functions']"
1461693,Are we allowed to choose infinite number of elements from an infinite set?,"$f$ is surjective $\implies f$ has right inverse. Suppose $f$ is surjective. Then for any $b \in B$ there's at least one $a \in A$ such that $f(a) = b$. Choose one such $a$ for each $b$ and define $g: B \implies A$ by letting $g(b)$ the chosen $a$. Then $f(g(b)) = b$, so $f \circ g= 1_B$. Since surjective functions allow any number of arrows each from different points in the domain onto a single point in the codomain, the proof of the given statement depends on making possibly infinite number of choices (one $a \in A$ with $f(a) = b$ for each $b \in B$). Apparently selecting an infinite number of elements is a big deal and a problem. Why is that? Thanks.",['elementary-set-theory']
1461696,"For a set of symmetric matrices $A_i$ of order p, show that if the sum of their ranks is p, $A_iA_j=0$","Here's what I know. Matrices $A_i$ for $i=1,...,k$ are all symmetric p by p matrices. $\sum\limits_{i=1}^k A_i = I_p$ where $I_p$ is the p by p identity matrix $\sum\limits_{i=1}^k rank(A_i) = p$ With this, I have to find a way to show that for all $i \neq j$, $A_iA_j=0$. I assume this is solved by showing that $rank(A_iA_j)=0$ but every attempt that I've made to do that goes nowhere. Can anyone point me in the right direction?","['vector-spaces', 'matrices', 'equivalence-relations', 'matrix-rank', 'linear-algebra']"
1461731,Sum of independent symmetric random variables is symmetric,"This is exercise 3.2.5 from Probability and Random Processes by Grimmett and Stirzaker: Let $X_r$, $1 \leq r \leq n$, be independent random variables which are discrete and symmetric about $0$, that is, $X_r$ and $-X_r$ have the same distributions. Show that for all $x$, $P(S_n \geq x) = P(S_n \leq -x)$ where $S_n = \sum_{r = 1}^n X_r$. Is the following proof correct? $\textbf{Proof:}$ Let $x \in \mathbb{R}$ be arbitrary. Then, \begin{eqnarray}
P(S_n \geq x) & = & P\left(\sum_{r = 1}^n X_r \geq x\right) \\
& = & P\left(\underset{x_1 + \cdots + x_n \geq x}{\bigcup_{x_1, \ldots, x_n}} \{X_1 = x_1\} \cap \cdots \cap \{X_n = x_n\} \right) \\
& = & \underset{x_1 + \cdots + x_n \geq x}{\sum_{x_1, \ldots, x_n}} P\left(\{X_1 = x_1\} \cap \cdots \cap \{X_n = x_n\}\right) \\
& = & \underset{x_1 + \cdots + x_n \geq x}{\sum_{x_1, \ldots, x_n}} P\left(\{X_1 = x_1\}) \cdots P(\{X_n = x_n\}\right) \textrm{(from independence of variables)}\\
& = & \underset{x_1 + \cdots + x_n \leq -x}{\sum_{x_1, \ldots, x_n}} P\left(\{X_1 = -x_1\}) \cdots P(\{X_n = -x_n\}\right) \textrm{(from symmetry of variables)} \\
& = & \underset{x_1 + \cdots + x_n \leq -x}{\sum_{x_1, \ldots, x_n}} P\left(\{X_1 = -x_1\} \cap \cdots \cap \{X_n = -x_n\}\right) \textrm{(from independence of variables)}\\
& = & P\left(\underset{x_1 + \cdots + x_n \leq -x}{\bigcup_{x_1, \ldots, x_n}} \{X_1 = -x_1\} \cap \cdots \cap \{X_n = -x_n\} \right) \\
& = & P\left(\sum_{r = 1}^n X_r \leq -x\right) \\
& = & P(S_n \leq -x)
\end{eqnarray} as required. $\square$ Is there a quicker/better way of proving the result?","['probability-theory', 'probability', 'proof-writing']"
1461758,Question from Folland Chapter 1 Exercise 14,"The problem is the following: If $\mu$ is a semifinite measure and $\mu(E)=\infty$, for any $C>0$ there exists $F \subset E$ with $C < \mu(F) < \infty$. We were told as a hint to consider the set $F = \left\lbrace F\subset E : 0 < \mu(F)<\infty \right\rbrace$. I know that because $\mu$ is semifinite, we must have that $F$ is nonempty by definition (and that's pretty much all $\mu$ being semifinite tells me). However, I don't see any reason why for any $C>0$ I can find an element in this set that satisfies the problem.","['real-analysis', 'measure-theory']"
1461776,What is the difference between Linear Least Squares and Ordinary Least Squares?,"My understanding is that Ordinary Least Squares (Usually taught in Statistics classes) uses the vertical distance only when minimizing error/residuals (see Wikipedia for Ordinary Least Squares ) with a modeled line. On the other hand, Linear Least Squares (Usually taught in Linear Algebra classes) uses vertical and horizontal distance components when minimizing the error/residuals (See Wikipedia for Linear Least Squares ) with the modeled line, in effect minimizing the ""closest"" distance. Is this correct? Normally would one expect to get the same estimation of parameters for a linear model?","['statistics', 'linear-algebra', 'least-squares', 'regression']"
1461793,Integral of the Laplace-Beltrami Operator multiplied by a function,"I have the following problem: Let $\mathcal{M}$ be a $2D$-manifold in $\mathbb{R}^3$ and let $g$ denote its metric. Furthermore it is known that $\mathcal{M}$ is a closed manifold (i.e. it has no boundary like a sphere, torus, etc.) I believe that above information is sufficient to prove
\begin{align}
\int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U \rangle_g \, \mathrm{d}\mu,
\end{align}
for $W \in H^1(\mathcal{M})$ and $U \in C^2(\mathcal{M})$ (I need this for a finite element simulation). Here $\langle , \rangle_g$ denotes the inner product with respect to $g$. According to Wikipedia, above equality holds for all compactly supported functions $W$ and $U$ (source: https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator ). I am not quite sure whether I understand Wiki correctly. Suppose $\mathcal{M}$ is just a closed subset of the $xy$-plane, then the Beltrami operator would simply reduce to the ordinary $2D$-Laplacian and the right hand side would certainly contain an additional integral over $\partial \mathcal{M}$ (if $W$ and $U$ are nonzero there). So to me the compactness of the supports seems insufficient for above equation to hold (could somebody tell me what I am getting wrong here ?).
I believe, however, that if $\mathcal{M}$ has no boundary, the boundary integral should vanish and above equation should follow. What I have done: let $S:\Omega \rightarrow \mathcal{M}$ parametrize $\mathcal{M}$, I carried out above integral in $\Omega$ utilizing $S$. Using standard calculus identities, after some steps I arrive at
\begin{align}
\int_{\mathcal{M}} W \nabla_\mathcal{M}^2 U \, \mathrm{d} \mu & = \int_{\partial \Omega} \left(w \sqrt{|g|} \nabla_\mathcal{M} u \right) \cdot \mathbf{n} \, \mathrm{d}s -\int_{\mathcal{M}} \langle \nabla_\mathcal{M} W,\nabla_\mathcal{M} U\rangle_g \, \mathrm{d}\mu
\end{align}
where $w \equiv W \circ S$ and $\nabla_\mathcal{M} u \equiv \nabla_\mathcal{M} U \circ S$. I believe that the boundary integral on the right hand side should vanish for closed $\mathcal{M}$. The reason is that the local counterparts $w$ and $u$ of $W$ and $U$ satisfy certain continuity constraints across $\partial \Omega$ which translates to $w$ and $\nabla_\mathcal{M} u$ having the same value on two segments of $\partial \Omega$ but with $\mathbf{n}$ pointing in the opposite direction so that the boundary integral vanishes. Of course my explanation lacks mathematical rigor and I was wondering what the best way to prove this is.
I believe that there exist proves that avoid integrating over $\Omega$ all together but I don't know where to start looking, also I don't want to read an entire book to understand the concept of wedge-products etc so if the formal proof is complicated could someone refer me to a source that I can cite that proofs exactly above statement ?
Thank you in advance.","['differential-operators', 'manifolds', 'differential-geometry', 'calculus']"
1461806,Modifying definition of $\sigma$-algebra,"Let's start with basic definitions. Def. Given a set $X$  we say that $\Sigma\subset 2^X$ is a $\sigma$-algebra , if $X\in\Sigma,$ $\Sigma$ is closed under complementation and countable unions. Def. Given a family $\mathcal A$ of subsets of $X,$  we say that $\sigma$-algebra $\sigma(\mathcal A)$ is the $\sigma$-algebra generated by $\mathcal A$ , if it is the smallest $\sigma$-algebra containing $\mathcal A.$ Asaf Karagila said in this answer , that $\sigma(\mathcal A)$ has descriptive, but transfinite form. It is $\Sigma^0_0=\Pi^0_0=$ finite intersections from $\mathcal{A}$ For countable ordinals $\alpha$ let: $$\Sigma^0_\alpha=\{\bigcup_{i\in\mathbb N} A_i\mid A_i\in\bigcup_{\beta<\alpha}\Pi^0_\beta\},\quad 
\Pi^0_\alpha = \{X\setminus A\mid A\in\Sigma^0_\alpha\},\quad 
\Delta^0_\alpha=\Sigma^0_\alpha\cap\Pi^0_\alpha$$
and then we define $\Delta=\bigcup_{\alpha<\omega_1} \Delta^0_\alpha.$ I heard of slightly different approach (I colored differences in $\color{green}{\text{green}}$), i.e. $\color{green}{\Delta_0=\mathcal{A}\cup\{\emptyset\}}$ For countable ordinals $\alpha$ let: $$\Sigma_\alpha=\{\bigcup_{i\in\mathbb N} A_i\mid A_i\in\bigcup_{\beta<\alpha}\color{green}{\Delta}_\beta\},\quad 
\Pi_\alpha = \{X\setminus A\mid A\in\Sigma_\alpha\},\quad 
\Delta_\alpha=\Sigma_\alpha\color{green}{\cup}\Pi_\alpha$$ and then we define $\Delta=\bigcup_{\alpha<\omega_1} \Delta_\alpha.$ In both cases the claim is that $\Delta=\sigma(\mathcal A).$ I have no clue how someone is able to proof that, particularly the inclusion $\Delta\subset\sigma(\mathcal A),$ but I believe it can be done and I am willing to assume that $\Delta=\sigma(\mathcal A)$ in both cases above. From now on, if I say transfinte induction, I mean the second one, i.e. the one with $\color{green}{\text{green}}$ elements. I would like to create a new object $\Gamma,$ which I get from transfinite induction, but restricted to the finite ordinals, i.e. I just simply do normal induction and define $\Gamma=\bigcup_{n<\infty}\Delta_n.$ To be even more explicite, let's actually define it using good old normal induction. So we set $\Delta_0=\mathcal{A}\cup\{\emptyset\}$ For every $n\in\mathbb{N}:$ $$\Sigma_n=\{\bigcup_{i\in\mathbb N} A_i\mid A_i\in\bigcup_{k<n}\Delta_k\},\quad \Pi_n = \{X\setminus A\mid A\in\Sigma_n\},\quad \Delta_n=\Sigma_n\cup\Pi_n$$
and then we define $\Gamma=\bigcup_{n<\infty}\Delta_n.$ For me $\Gamma$ is big enought. But since $\Delta=\sigma(\mathcal A)$ and $\Gamma$ looks smaller than $\Delta$ it is likely that $\Gamma\neq\sigma(\mathcal A).$ Unfortunetely $\Gamma$ seems not to be even a $\sigma$-algebra, for the same reason that $\Delta$ is in fact $\sigma$-algebra. (See Arturo Magidin's answer) It fails to be closed under countable unions. But it fails, due to considering diagonal-like elements (and actually this is the reason why I define $\Gamma$ in such way). What is the idea! (don't be scared in first reading) I would like to define two new objects called $\gamma$-algebra and $\gamma$-algebra generated by $\mathcal A.$ $\gamma$-algebra is defined similar to $\sigma$-algebra, but with $\color{blue}{\text{different (weaker) conditions}}$. $\gamma$-algebra generated by $\mathcal A$ is smallest such $\gamma$-algebra and it has descriptive form, which equals $\Gamma.$ In other words, I would like to end up with the following situation: Def. Given a set $X$  we say that $\Omega\subset 2^X$ is a $\gamma$-algebra , if $X\in\Omega,$ $\Omega$ is closed under complementation and $\color{blue}{\text{some conditions I ask you to invent}}$. Def. Given a family $\mathcal A$ of subsets of $X,$  we say that $\gamma$-algebra $\gamma(\mathcal A)$ is the $\gamma$-algebra generated by $\mathcal A$ , if it is the smallest $\gamma$-algebra containing $\mathcal A.$ Thm. Let $\mathcal A$ be a family of subsets of $X.$ If we define $\Gamma$ as above, then
  $$\Gamma=\gamma(\mathcal A).$$ The question What conditions should be in $\color{blue}{\text{blue}}?$","['real-analysis', 'descriptive-set-theory', 'measure-theory']"
1461818,Various definitions of Moduli space of Riemann surfaces and Uniformization theorem,"I'm sorry for the quantity of questions I'm asking, but I would like to solve once and for all many doubts I have on equivalent definitions of the moduli space of Riemann surfaces. Definition 1 : The Moduli space of Riemann surfaces of genus $g$ is $\mathcal{M}_g:=\{$ Riemann surfaces of genus $g$ }/isomorfisms $\qquad$    ($\mathcal{M}_g$ here is just as a set) I know that, for $g\ge 2$, we have this equivalent definition of $\mathcal{M}_g$: Definition 2 : $\mathcal{M}_g:=\{$hyperbolic metrics on $S_g$}/isometry $\qquad S_g$ is the oriented topological surface of genus $g$. I've been told that, for $g\ge 2$, definition 1 and 2 are equivalent because of the Uniformization theorem , which says that a simply connected Riemann surface has conformal class of curvature 1 or 0 or -1. 
For what I've understood, for any Riemann surface $X$ of genus $g\ge 2$ we consider the universal cover $\widetilde{X}$ which is of conformal type -1 (is the hyperbolic plane $\mathbb{H}$) and the hyperbolic metric descends on $X$. Question 1: Why is the conformal type of $\widetilde{X}$ -1? Why can't it be spheric or flat? Question 2: Given an isomorphism class of Riemann surfaces, (i.e. a complex structure, up to isomorphism) how do I get (or compute, if you prefer) the hyperbolic metric? Given the hyperbolic metric, how do I get the complex structure? I've also found this other definition for $g\ge 2$: Definition 3 : $\mathcal{M}_g:=\{$hyperbolic metrics on $S_g$}/orientation preserving diffeomorphisms which brings me directly to the next question: Question 3 : Why are definition 2 and 3 equivalent? I.e why is it the same to consider hyperbolic metrics up to isometries or up to just orientation preserving diffeomorphisms? Now I want to consider the case $g=1$. I've found this definition: Definition 4 : $\mathcal{M}_1:=\{$flat metrics on $S_1\}/$action of $\mathbb{R}^+\times Diffeo^+(S_1)$ I guest this too must be a conseguence of the Uniformization theorem, but I can't really get it: Question 4: Why can't the universal cover of a Riemann surface of genus 1 be spheric or hyperbolic? I know that the Riemann surface $X_\tau:=\mathbb{C}/(\mathbb{Z}+\tau\mathbb{Z})$, $\tau\in \mathbb{H}$, is isomorphic to $X_\eta$, $\eta\in\mathbb{H}$ if and only if there are integer numbers $a,b,c,d$ with $ad-bc=1$ such that $\eta=\frac{a\tau +b}{c\tau +d}$ Question 5: Given an isomorphism class of Riemann surfaces of genus 1, (and so by the above observation, an orbit of $\mathbb{H}$ by the action of $PSL(2,\mathbb{Z})$) how do I get (or compute, if you prefer) the flat metric? Given the flat metric, how do I get the complex structure? Thank you very much!","['moduli-space', 'algebraic-geometry', 'riemann-surfaces', 'differential-geometry', 'hyperbolic-geometry']"
1461839,"Is there such a thing as ""hypertopology"" (analogous to the hyperreals)?","The hyperreal number system adds infinities and infinitesimals, allowing Calculus to be done using these things instead of limits (sort of like when calculus was originally invented, but with rigor).This got me thinking, could this be done with topology to create hypertopology (an equivalent but perhaps more intuitive way of doing topology)? It should be able to translate, since hyperreals can be represented as ultrafilters of reals. Hypertopology would involve ultrafilters of the points of the topology presumably. To define the topology, I presume you would define when two points of the hypertopology are infinitesimally close. To convert a metric space into a topology, you would simply define two hypertopology points as close whenever their distance is infinitesimal (again, ultrafilters can be applied point wise.) Has this ever been studied? What axioms would hypertopology ""closeness"" need to follow to be equivalent to regular topology? Is there an axiomatic approach (not requiring the ultrafilters (the hyperreals have an axiomatic basis))?","['nonstandard-analysis', 'reference-request', 'infinitesimals', 'general-topology']"
