question_id,title,body,tags
4869946,Function Investigation proof verification,"https://pasteboard.co/WMejlmetcznl.png This is the question We have, \begin{gather}
f( m+n) \ =\ f( m) \ +\ f( n) \ +\ mn( m\ +\ n) \notag\\
\end{gather} And we have to prove, \begin{gather*}
f( n) \ =\ \sum _{i\ =\ 0}^{d} c_{i} n^{i}\\
f( n) \ =\ c_{0} \ +\ c_{1} n\ +\ c_{2} n^{2} \ +\ ...\ +\ c_{d} n^{d}\\
f( m) \ =\ c_{0} \ +\ c_{1} m\ +\ c_{2} m^{2} \ +\ ...\ +\ c_{d} m^{d}
\end{gather*} So we have from the assumptions, \begin{gather*}
f( m\ +\ n) \ =\ c_{0} \ +\ c_{1}( m\ +\ n) \ +\ c_{2}( m\ +\ n)^{2} \ +\ ...\ +\ c_{d}( m\ +\ n)^{d}\\
=\ f( n) \ +\ f( m) \ +\ 2c_{2} mn\ +\ 3c_{3} mn( m\ +\ n) \ +\ ...\\
\end{gather*} We conclude from the above expression that $d$ has a maximum value of 3 and from (1) we get the relations, \begin{gather*}
2c_{2} \ =\ 0,\ 3c_{3\ } \ =\ 1\\
\Longrightarrow \ f( n) \ =\ c_{0} \ +\ c_{1} n\ +\ \frac{1}{3} n^{3} ;\ c_{0} ,\ c_{1} \ \in \ \mathbb{R}
\end{gather*}","['functions', 'solution-verification']"
4869975,A weird probability question,"This is the problem in question: You have two identical bowls: the first one contains 3 white balls and 4 black balls, and the second one contains 4 white balls and 5 black balls. If you choose randomly a ball from the two bowls, what is the probability it is white ? Let's define our events as such: A1 = choosing a ball from the first bowl A2 = choosing a ball from the second bowl B  = choosing a white ball One approach would be using the theorem of total probability: $$\text{We know that }P(B|A_1) = \frac34\text{ and }P(B|A_2) = \frac45\text{, and that:}$$ $$P(A_1) = P(A_2) = \frac12\text{, because the bowls are identical}$$ $$P(B) = P(B |A_1)\times P(A_1) + P(B|A_2)\times P(A_2) = \frac37 \times \frac12 + \frac49 \times \frac12 = 55/126$$ The second approach would be simplifying the problem: Because the two bowls are identical, we could just say we don't even choose between two bowls, but just between the set of all balls. Then we could calculate the probability directly: $$P(B) = \frac {\text{number of white balls}} {\text{total number of balls}} = \frac7 {16}$$ Now, which one is correct? (and why?) Both solutions seem reasonable, and they have approximately the same value $$\frac{55}{126}\approx0.4365$$ $$\frac{7}{16}\approx0.4375$$ However, mathematically speaking, they are different results. Which one is correct?","['solution-verification', 'combinatorics', 'probability']"
4870020,Probability of strings making a complete loop,"We got 6 pieces of strings. The top ends of the ropes are randomly paired up
and tied together. The same procedure is done with the bottom ends of the
strings. What is the probability that, as a result of the process, the 6 pieces of strings will be  connected in a single closed loop of string? My initial attempt is to just multiply the probabilities in each step: Probability of tying the top of the string in initial procedure is 1. Because any head we tie will not effect the result we are getting. Second, step if we choose a random string and random bottom end of it, out of our new 3 full strings; only one of the outcomes will not be favorable out of all 5 bottom ends. So favorable probability $\frac{4}{5}$ Third, if we consider one of our new tied strings's bottom ends we have 2 favorable ends out of 3 ends so $\frac{2}{3}$ and finally 1 way to tie the last ends. Therefore in total; $1 \times \left(\frac{4}{5}\right) \times \left(\frac{2}{3}\right) \times 1 = \frac{8}{15}
$ This was my attempt am I missing something?","['puzzle', 'probability-theory', 'probability']"
4870024,A Conjecture in Low-Dimensional Topology.,"Context I looked through a book called ""Problems in Low-Dimensional Topology,"" where Rob Kirby lists a set of problems. He provides a list of problems, states their conjectures, and describes any progress on the problems. The book was published in 1995, so I don't know if some of the problems that Kirby stated are still open or proven. 
There is one problem that stood out for me. Problem 1.8 (Stallings) Suppose $\beta$ is a word in the generators $\sigma_1 , . . . , \sigma_{n−1}$ and their inverses in the braid group $B_n$ . If the length of $\beta$ is minimal over all words representing the same element of $B_n$ , call β minimal. Conjecture: If the last letter of a minimal word $\beta$ is $\sigma_i^\epsilon$ , then the word $\beta\sigma_i$ is again minimal $(\epsilon = \pm1)$ . Question My first question is a matter of understanding the statement. What does it mean to be minimal? Is it saying that for a word $\beta$ with a last letter of $\sigma_i^\epsilon$ and an inverse in the braid group, if it's in its reduced form representing the same element of $B_n$ (which doesn't make sense because how can a reduced word represent the same element of a group?) then $\sigma_i$ is again minimal? If $\epsilon=-1$ , then how can $\beta\sigma_i$ represent the same element of the group as well? My misunderstanding is with the term minimal . My second question is about the status of the conjecture. Kirby states that this is still an open problem. Has this conjecture been proven since then? I tried to look for stalling conjectures on the internet, but I couldn't find a webpage that talks about this problem. Is there a different name for this conjecture?","['abstract-algebra', 'free-groups', 'geometric-group-theory', 'group-theory', 'algebraic-topology']"
4870028,Existence of special transversal on foliation,"This is a somewhat technical question about a line in Sharpe's book Differential Geometry: Cartan's Generalization of Klein's Erlangen Program in the proof of the structure theorem, Theorem 8.3. We have assumed that the leaf space of a foliation $M/\sim$ is Hausdorff. Because of this, each leaf $\mathcal{L}$ is closed and is in particular an embedded submanifold. Using the hypotheses that a foliation has trivial holonomy (w.r.t. transversals) and each leaf has finitely generated fundamental group, he concludes that a given leaf has a transversal that meets each leaf at most once. How can I justify this? Because each leaf is embedded, we can certainly choose a transveral intersecting a given leaf exactly once. But the tricky part is doing this uniformly for all the leaves meeting the transversal. If we replaced the assumption that leaves $\mathcal{L}$ have finitely generated fundamental group $\pi_1(\mathcal{L})$ with the stronger assumption that each leaf is compact, then Theorem 7.8 of Sharpe guarantees the existence of a tubular neighborhood $U$ of $\mathcal{L}$ with a leaf-preserving diffeomorphism $U\cong \mathcal{L}\times T$ for some transversal $T$ , which would give a transversal meeting each leaf at most once. The main application of finitely generated fundamental group and trivial holonomy I've seen is that, given a transveral of a leaf, we can pick a single open subset of the transversal where the entire fundamental group acts as the identity. Thanks!","['cartan-geometry', 'foliations', 'fundamental-groups', 'holonomy', 'differential-geometry']"
4870070,Weak convergence of Hilbert-space valued stochastic process,"Suppose I have a sequence of stochastic processes $(X_n(t))_{t\in [0,1]}$ such that $X_n(t) \in H$ , where $H$ is some separable Hilbert space and $X_n \in C([0,1,], H)$ . Goal. I want to show that $X_n$ converges weakly to a stochastic process $X$ , i.e. $\mathrm{Law}(X_n) \rightharpoonup \mathrm{Law}(X)$ as probability measures on $C([0,1],H)$ . Question. Suppose I know that for any dual elements $\phi_1,\ldots,\phi_k \in H'$ , where $k$ is finite but arbitrary, it holds that the sequence of processes $(\langle{\phi_1,X_n(t)}\rangle,\ldots,\langle{\phi_k,X_n(t)}\rangle)_{t\in [0,1]} \in C([0,1], \mathbb{R}^k)$ converges weakly to $(\langle{\phi_1,X(t)}\rangle,\ldots,\langle{\phi_k,X(t)}\rangle)_{t\in [0,1]}$ . Does this imply that $X_n$ weakly converges to $X$ ? If it makes a difference, the desired limit $X$ is a Gaussian process. I suspect the answer is yes by some sort of approximation argument, but I couldn't find a reference in the usual suspects as far as texts on infinite-dimensional stochastic processes. Any help would (in particular, a specific reference if it's true) is greatly appreciated!","['probability-limit-theorems', 'stochastic-analysis', 'stochastic-processes', 'functional-analysis', 'probability-theory']"
4870073,Inverse of a special triangular matrix,"Let $A$ be an invertible upper triangular matrix with $A_{i,j}=A_{i+1,j+1}$ for all $i,j$ . How can I show that $A^{-1}$ has the same property? That, it is (an upper triangular) matrix with $A^{-1}_{i,j}=A^{-1}_{i+1,j+1}$ for all $i,j$ . I have verified this using computations, but is there a simple proof? Thanks.","['matrices', 'linear-algebra']"
4870084,Calculating Expected Throws for All Die Faces to Appear Twice [duplicate],"This question already has answers here : Expectation time for at least 2 balls in each bin (1 answer) Coupon collector problem for collecting set k times. (2 answers) Closed 4 months ago . I'm working on a problem where I need to find the expected number of throws required for each face of a six-sided die to appear at least twice. I've conceptualized the problem using a triplet $(a, b, c)$ to represent the state of the system: $a$ : numbers drawn twice, $b$ : numbers drawn once, $c$ : numbers not drawn, with the initial state being $(0, 0, 6)$ . The transitions are: From $c$ to $b$ with probability $\frac{c}{6}$ , From $b$ to $a$ with probability $\frac{b}{6}$ . And then you can use brute force and write an algorithm to solve it using DP. But I’m wondering if there is any clever method. I know a lot of theorems have been studied on coupon collections issues. But this is a particular simple example so I’m curious about some simpler but clever strategy free of those existing theories that are intended for a more general scenario.","['expected-value', 'dice', 'combinatorics', 'coupon-collector', 'probability']"
4870134,Solved - Finding the function of $(1+x)(1+x^4)(1+x^{16})(1+x^{64})....$,"The question states:
For $0 < x < 1$ , let $f(x) = (1+x)(1+x^4)(1+x^{16})(1+x^{64})(1+x^{256}).... $ Find $f(x).$ $f(x)=
\prod_{n=0}^{\infty}(1+x^{(4^n)})$ I've tried noticing that it looks similar to the telescoping series $(1+x)(1+x^2)(1+x^4)...$ but applying the same trick of multiplying by $(1-x)$ doesn't work here. Can someone please guide me in the right direction to solving this problem?
Thanks very much. Edit: The original problem was to find $f^{-1} \left ( \frac{8}{5f(\tfrac{3}{8})} \right)$ , as can be seen in this thread. The solution can be found by noticing that $f(x)f(x^2)=(1+x)(1+x^2)(1+x^4)(1+x^8)... = \frac{1}{1-x}$ and by plugging in $x = 3/8$ (credit to John Omelian) we get $f(\frac{3}{8})f(\frac{9}{64}) = \frac{1}{1-\frac{3}{8}} = \frac{8}{5}$ , in which then we can divide by $f(\frac{3}{8})$ and take the inverse of both sides to get $f^{-1} \left ( \frac{8}{5f(\tfrac{3}{8})} \right) = \boxed{\frac{9}{64}\:}$ . I apologize for any confusion that I've caused, but thanks to everyone for their support and feedback!","['telescopic-series', 'sequences-and-series']"
4870163,$(A+B)^2\|B^{-1}\|-A$ is positive definite?,"If $A$ and $B$ are positive definite matrix, $(A+B)^2\|B^{-1}\|-A$ is also positive definite? When $A$ and $B$ are real numbers, then $(A+B)^2\|B^{-1}\|-A$ is obviously positive definite. When $A$ and $B$ are matrix, I can use orthonormal matrix $O$ to transform $B$ an diagonal matrix $\operatorname{diag}\{\lambda_1,\cdots,\lambda_n\}$ . Then I can transform the original problem to $(A+B)^2-\lambda_n A$ is it also positive definite?","['matrices', 'linear-algebra', 'analysis', 'matrix-decomposition']"
4870204,"Is it always true that if $x_n\to0$, $y_n\to0$ there exist $\epsilon_n\in\{-1,1\}$ such that both $\sum\epsilon_nx_n$and $\sum\epsilon_ny_n$ converge? [duplicate]","This question already has answers here : Must there be a sequence $(\epsilon_n)$ of signs such that $\sum\epsilon_nx_n$ and $\sum\epsilon_ny_n$ are both convergent? (3 answers) Closed 4 months ago . I saw this interesting problem: Let $x_n$ and $y_n$ be real sequences with $x_n \to 0$ and $y_n \to 0$ as $n \to \infty$ . Show that there is a sequence $\varepsilon_n $ of signs (i.e. $\varepsilon_n \in \{−1, +1\}$ for all $n$ ) such
that $\sum \varepsilon_n x_n$ is convergent. Must there be a sequence $\varepsilon_n$ of signs such that $\sum \varepsilon_n x_n$ and $\sum \varepsilon_n y_n$ are both
convergent? I solved the first question as follows: either $\sum  |x_n|$ converge or diverge if it is convergent we are done and if it is divergent the way to prove that there is a sequence $\varepsilon_n $ of signs (i.e. $\varepsilon_n \in \{−1, +1\}$ for all $n$ ) such
that $\sum \varepsilon_n x_n$ is convergent is the same way the Riemann's rearrangement theorem is proved I mean choosing some limit say $0$ and adding a  negative term then positive terms until the partial sum exceeds $0$ then adding the next negative and so on the limit will be $0$ this way is analogous to the proof of Riemann's rearrangement theorem. Riemann's rearrangement theorem: if $\sum {x_n}$ converges, but not
absolutely. Suppose $-\infty \le a \le b \le \infty $ Then there exists a rearrangement $\sum {x_n'}$ with partial sums $s_n'$ such that $\lim \inf s_n' = a$ , $\lim \sup s_n' =b$ . (I don't know if this proof is right or wrong btw. if it is wrong I ask for other proof) As the way the second question is written it seems that the answer is no but I  couldn't find any counterexamples or proof of this claim.","['summation', 'real-analysis', 'solution-verification', 'sequences-and-series', 'convergence-divergence']"
4870252,Prove ${2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n}$,"I am trying to prove ${2n \choose n+i} \geq e^{-8 i^2/n} {2n \choose n}$ for $0\leq i \leq n$ . My attempt: I rewrote ${2n \choose n+i}$ to $${2n \choose n+i} = {2n \choose n} \prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) $$ So all I need is to prove $$\prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{-8i^2/n}$$ I tries using $1-x \geq e^{-x/(1-x)}$ inequality to get $$\prod_{1\leq j \leq i} \left( 1- \frac{2j-1}{n+j} \right) \geq e^{- \sum_{j=1}^i \frac{2j-1}{n+1-j}}$$ However, I think this inequality is too loose. For example, for $n=400$ and $i=399$ , it violates what we are trying to prove. $$8 i^2/n < \sum_{j=1}^i \frac{2j-1}{n+1-j}$$","['inequality', 'real-analysis']"
4870284,"Number of ways to form a sequence of 10 letters from four 'a's, four 'b's, four 'c's, and four 'd's if each letter must appear at least twice","Question: How many ways are there to form a sequence of 10 letters from four 'a's, four 'b's, four 'c's, and four 'd's if each letter must appear at least twice? My Approach: I started out with the exponential generating function where: XA represents the number of 'A's, XB represents the number of 'B's, XC represents the number of 'C's, XD represents the number of 'D's,
and $XA + XB + XC + XD = 10$ . Here, $A(XA) = \frac{x^2}{2!} + \frac{x^3}{3!} + \frac{x^4}{4!}$ , and $A(X) = (A(XA))^4$ . My Problem: I am unable to solve or reduce it to a standard formula of $e^x$ from which I can easily calculate the coefficient of $ x^{10} $ . Can I do it directly using permutation with limited repetition?",['combinatorics']
4870285,What structure do smooth maps and diffeomorphisms preserve?,"Context: Continuous maps between topological spaces are structure preserving in the following sense: Given two topological spaces $(X,\tau_X),(Y,\tau_Y)$ (where $(X,\tau_X)$ is the topological space of set $X$ endowed with a topology $\tau_X$ ; similarly $(Y,\tau_Y)$ ), a map $f:(X,\tau_X)\to (Y,\tau_Y)$ is continuous iff $f^{-1}(V)\subset X$ is open whenever $V\subset Y$ open. A smooth manifold is a Hausdorff second countable locally Euclidean space with a smooth structure. The smooth structure is one where you take a smooth atlas and consider the unique maximal smooth atlas generated by it. Problem: Question 1: What is the structure on a smooth manifold? Is it this maximal smooth atlas itself or merely the requirement that the co-ordinate charts (which the underlying topological manifold already has) need to be smoothly compatible? Question 2: I want to show or think of smooth maps as the maps that preserve this structure of smooth manifolds. How can I do that? Here is my (WRONG) guess: given a map $f:M\to N$ ( $M,N$ smooth manifolds), $f$ is smooth iff for every smooth chart $(V,\psi)$ in the smooth structure of $N$ , we get a unique smooth chart $(U,\phi)$ (via some kind of taking pre-images) in the smooth structure of $M$ .","['smooth-functions', 'smooth-manifolds', 'differential-geometry']"
4870293,Finding general term for a sequence satisyfing provided conditions (with goniometric functions),"I want to inquire about how would one find the formula for $a_n$ (or reccurently) if we know: $$ \alpha_k = arctan\left(\frac{a_{(n+1)}}{a_n} \right) $$ $$ \sin{\alpha_k} = \sin{\alpha_{(k-1)}} \cdot \frac{ a_n }{a_{(n+1)}}  $$ This is also true for the first elements, the sequence entry condition is $$ \alpha_1 = \arctan{ \left( \frac{a_2}{a_1} \right)}$$ It does not matter what exact values $a_1 $ and $a_2$ hold as long as they satisfy this condition When I try to plug in those conditions together, I get $ a_{(n+1)} = a_{(n+1)} $ which is obviously true but also not helpful at all.","['functions', 'sequences-and-series']"
4870340,"Proof of Triangle Inequality for $d(g; x, y) = \left(|x-y|^4 + g\,| x \times y |^2\right)^{\frac{1}{4}}$","I am seeking assistance in proving that a function, denoted as $d(g; x, y)$ , defined on $\mathbb{R}^2 \times \mathbb{R}^2$ and parameterized by the non-negative real number $g$ , may satisfy the triangle inequality. The function is defined as follows: \begin{align}
d(g; x, y) &:= \left(|x-y|^4 + g\,| x \times y |^2\right)^{\frac{1}{4}} \\
&= \left( \left((x_1 - y_1)^2 + (x_2 - y_2)^2\right)^2 + g\,(x_1\,y_2 - x_2\,y_1)^2 \right)^{\frac{1}{4}}
\end{align} where $x=(x_1,x_2),\,y=(y_1, y_2)$ . It is noteworthy that when $g=0$ , $d(g;x,y)$ coincides with the Euclidean distance. My ultimate goal is to prove that $d(g; x, y)$ is a distance function on $\mathbb{R}^2$ for a certain range of $g$ . While it is trivial that $d(g; x, y)$ is non-degenerate and symmetric with respect to $x$ and $y$ , the proof of the triangle inequality is not straightforward. Through numerical calculation, I observed that the triangle inequality seemed to hold in the range $0\leq g \leq 6$ . In other words, the conjecture I wish to prove is as follows: for $0 \leq g \leq 6$ , and only in that range, $$
d(g;x,y)+d(g;y,z)≥d(g;x,z)
$$ holds for all $x,y,z \in \mathbb{R}^2$ . I am looking for guidance and information related to proving this statement. Thank you for your assistance!","['inequality', 'geometry']"
4870356,how many natural numbers require at least 6 terms to express as the sum of distinct squares?,"I wrote a computer program as an exercise in dynamic programming. It finds the minimum number of distinct squares which sum to some positive target integer n. I found something interesting and would like to know if there's any relevant work on it. My question is basically the following. Are 124 and 188 the only two positive integers that require a minimum of 6 distinct terms to express as a sum of squares? 1^2 + 2^2 + 3^2 + 5^2 + 6^2 + 7^2 = 124
1^2 + 2^2 + 3^2 + 5^2 + 7^2 + 10^2 = 188 There doesn't appear to be any shortage of numbers that require a minimum of 5 distinct squares. It seems conceivable to me that there are no more because for large n there are so many possible combinations, but I can't prove it. My approach was to try to disprove it with a counterexample. The time and space complexity of my program is currently O(n^1.5) so it gets quite greedy for RAM very quickly. I think I could rewrite it to search for a counterexample greater than my current limit of 5,000,000, but if it is something that is already known, then please let me know. Thanks. EDIT: I found this http://matwbn.icm.edu.pl/ksiazki/aa/aa67/aa6745.pdf in which '2. An initial upper bound' claims to prove that  every integer n greater than or equal to 4^5 = 1024 can be expressed as the sum of 5 distinct squares. I guess that does it if the proof is correct and my program works correctly.","['number-theory', 'square-numbers', 'combinatorics']"
4870361,When does A and exp(B) commuting imply A commutes with B?,"Let $A,B$ $\in GL_{n}(\mathbb{C})$ and $[A,B] = AB-BA = 0$ . My question is about the existence of a $b \in M_{n}(\mathbb{C})$ such that $B = \exp(b) $ and $[A,b] =0 $ . Note that in general $[A,\exp(b)]=0$ does not imply $[A,b]=0$ . As an example consider the matrices $A = \begin{bmatrix}1&i\\i&2\end{bmatrix}$ $B= I$ and $b = \begin{bmatrix}2 \pi i&0\\0&0\end{bmatrix} $ . We have $[A, \exp(b)] = 0$ but $[A,b] \neq 0$ . However, we can replace $b$ with $\widetilde{b}=0$ . I believe in general this could have something to do with the locus in $M_{n}(\mathbb{C})$ where $\exp$ is a local isomorphism. This is the locus of matrices $X$ which have $\lambda_{i} - \lambda_{j} \neq 2 \pi i k $ for $k \in \mathbb{Z} - 0 $ for any two eigenvalues $\lambda_{i} , \lambda_{j}$ of $X$ . Note that $\exp$ is still surjective when restricted to this locus. Namely, perhaps if we restrict to preimages of $B$ under the exponential in this locus then perhaps the matrices commute. In particular, in the example above $b$ is not in this locus.","['matrices', 'matrix-exponential', 'lie-groups']"
4870389,Reference for the lemma in Nakagawa's article,"In K.Nakagawa's article ""On the orders of automorphisms of a closed Riemann surface"" there is Lemma 3, that doesn't have any proof or reference. Specifically, let $S$ be a closed Riemann surface, $G$ be the cyclic group of order $N$ , acting on $S$ by automorphisms and $\tilde{S} = S / G$ be the closed Riemann surface obtained by identifying those point on $S$ which are equivalent under the action of $G$ . Then, the lemma claims that if the number of branch points of covering map $\varphi : S \rightarrow \tilde{S}$ is $0$ , then the Riemann surface $S$ is conformally equivalent to $y^N = f(x)$ , where $f(x)$ is a polynomial in $x$ . Where can I find the reference for it or is there an obvious proof that I'm missing?","['riemann-surfaces', 'algebraic-geometry', 'reference-request']"
4870462,Why is none trying to find the value of $\displaystyle\sum_{n=1}^\infty\frac1{n^n}$?,"I wish to know the exact value of $$\sum_{n=1}^\infty\frac1{n^n}.$$ I've found on the internet some mentions of the equality (the Sophomore's Dream , as I've learned) $$\sum_{n=1}^\infty\frac1{n^n} = \int_0^1\frac1{x^x}dx.$$ Wolfram Alpha's computations returns the value of $1.29128599706266\dots$ to the RHS integral. However, it says no result found in terms of standard mathematical functions for $$\int\frac1{x^x}dx$$ so the equality does not yield a definitive answer. I am aware there might be no better way to describe this constant value, and it could just be given its own name, such as the Euler–Mascheroni constant. Wondering if that was the case I've searched for its first few digits and was led to the Sophomore's Dream and then to this and this related questions, none of which relieved my curiosity. All I've learned was that it was not even known if this constant is a rational number. I apologize for the nature of my question, which is utterly meta, but I just cannot help myself to wonder: Is this problem uninteresting? Why were so many mathematicians mobilized to solve the Basel Problem but so little is talked about this (surely just as pretty) series? Is this somehow a much harder question? Why? Also, I'm guessing if this value had a name someone would have mentioned it by now, so I'm calling it Bernoulli's Constant (not Sophomore's Constant since there are two equations in the Sophomore's Dream). The digit sequence is A073009 on OEIS, as pointed out in a comment.","['integration', 'closed-form', 'sequences-and-series', 'math-history', 'soft-question']"
4870556,Probability of infinite coin toss where tossing stops only if the number of heads is twice the number of tails,"The coin is tossed until the number of heads is exactly equal to twice the number of tails. The coin lands on heads with probability 𝑝. What is the probability that a coin will be tossed forever?
A similar problem (A coin lands heads with probability 𝑝. The coin is tossed until the first heads will come up. What is the probability that an even number of tosses will be made?) was solved by reducing the result to an infinite sum, which was an infinite sum of a geometric progression: $$(1-p)*p + (1-p)^3*p +\ldots = (1-p)*p*(1+(1-p)^2+(1-p)^4 + \ldots) = (1-p)*p/(1-(1-p)^2)=(1-p)/(2-p)$$ I assume, if I’m not mistaken, that there should be a similar approach here, but I’m a little confused about how exactly this should be decided: I first thought to subtract from 1 the probability of events when the coin toss stops, that is, when the number of heads becomes equal to twice the number of tails (when you get 1 tail and 2 heads, 2 tails and 4 heads, and so on), but the problem is that there are several sequences in which, for example, 2 tails and 4 heads, because tails and heads can occur in different orders: $$1 - (\binom{3}{1}*(1-p)*p^2+\binom{6}{2}*(1-p)^2*p^4+\ldots)$$ I can’t apply the formula of an infinite geometric progression here and I don’t really understand how to apply the formula of a power series here, although maybe this formula is needed here. Maybe I'm deciding in the wrong way altogether? Please tell me what to do here, because unfortunately I have no more ideas...",['probability']
4870589,A self-proof of Vapnik - Chervonenkis theorem,"Theorem : For every $\varepsilon >0$ , with the probability greater than $1-\varepsilon$ \begin{align*}
    R_p(\hat{g}_{n,\mathcal{G}}) - R_{p}(g^*_{p,\mathcal{G}}) \le 2 \sqrt{\dfrac{2V_{\mathcal{G}}(2n) \ln(4(2n+1)\varepsilon^{-1}) }{n}}
\end{align*} where $V_{\mathcal{G}}(N) = \sup_{\mathbb{X_N} \in \mathcal{X}^N} V_{\mathcal{G}} (\mathbb{X_N})$ with $\mathbb{X_N} = \left\{x_1,x_2,\ldots x_N\right\}$ . The notation $V_{\mathcal{G}}$ denotes for the VC-dimension of $\mathcal{G}$ . My proof : (This proof was confirmed to be wrong by my lecturer. He pointed out the mistake was at the union bound after (1.1). I wonder if you could help me fix this point by using Rademacher complexity) Before going to the proof, let us recall some definitions, and lemmas we are going to use. Lemma (Sauer).
For all $S = \left\{x_1,\ldots,x_N\right\} = \mathbb{X_N}$ , we have \begin{align*}
    \vert T_{\mathcal{G}}(S)\vert &\le \sum_{k=0}^{V_{\mathcal{G}}(\mathbb{X_N})} C^k_N\\
    & \le \begin{cases}
        (N+1)^{V_{\mathcal{G}}(\mathbb{X_N})} & \text{, if } N> V_{\mathcal{G}}(\mathbb{X_N})\\
        2^N & \text{, if } N \le V_{\mathcal{G}}(\mathbb{X}_N)
    \end{cases}
\end{align*} Lemma: (Symmetrization).
Let $l$ be the loss function that \begin{align*}
    a \le l(y,y') \le a+1,\ \forall y,y' \in \mathcal{Y}.
\end{align*} Let $\mathcal{D'}_n = (x'_i,y'_i),\ i=\overline{1,n}$ be a sample independent of $\mathcal{D}_n$ with the same law. Then for all $t \ge \sqrt{\dfrac{2}{n}}$ \begin{align*}
    \mathbb{P}\left(\sup_{g \in \mathcal{G}}\vert \hat{R}_n(g) - R_p(g)\vert > t\right) \le 2\mathbb{P}\left(\sup_{g \in \mathcal{G}}\left\{\hat{R}_n(g) - \hat{R}'_n(g)\right\} > \dfrac{t}{2}\right),
\end{align*} where $\hat{R}_n(g):=\dfrac{1}{n}\sum_{i=1}^n l(y_i,g(x_i))$ , $\hat{R}'_n(g):=\dfrac{1}{n}\sum_{i=1}^n l(y'_i,g(x'_i))$ . Back to the main proof. Let $nt^2 = 2V_{\mathcal{G}}(2n)\ln\left(4(2n+1)\varepsilon^{-1}\right)$ . Then, we would like to prove that \begin{align*}
\mathbb{P}\left(R_p(\hat{g}_{n,\mathcal{G}}) - R_p(g^*_{p,\mathcal{G}}) > 2t \right) \le \varepsilon. \tag{*}
\end{align*} Firstly, we use the following fact \begin{align*}
R_p(\hat{g}_{n,\mathcal{G}}) - R_p(g^*_{p,\mathcal{G}}) \le 2\sup_{g \in \mathcal{G}}\vert \hat{R}_n(g) - R_p(g)\vert,
\end{align*} and obtain that \begin{align*}
\mathbb{P}\left(R_p(\hat{g}_{n,\mathcal{G}}) - R_p(g^*_{p,\mathcal{G}}) > 2t \right) \le \mathbb{P}\left(\sup_{g \in \mathcal{G}}\vert \hat{R}_n(g) - R_p(g)\vert > t \right).
\end{align*} Next, by using the symmetrization lemma, we have \begin{align*}
\mathbb{P}\left(\sup_{g \in \mathcal{G}}\vert \hat{R}_n(g) - R_p(g)\vert > t \right) \le 2 \mathbb{P}\left(\sup_{g \in \mathcal{G}}\left\vert \hat{R}_n(g) - \hat{R}'_n(g)\right\vert > \dfrac{t}{2} \right). \tag{1.1}
\end{align*} Let us consider the trace of $\mathcal{G}$ on $(\mathbb{X}_n,\mathbb{X}'_n)$ , denotes $T_{\mathcal{G}}(\mathbb{X}_n,\mathbb{X}'_n)$ . Each element of $T_{\mathcal{G}}(\mathbb{X}_n,\mathbb{X}'_n)$ is a sample $\left\{x_{j_1},x_{j_2},\ldots,x_{j_K}\right\}$ with $K \le 2n$ . To each of these elements, we can associate a prediction $g \in \mathcal{G}$ such that $g(x_{j_k})$ = 1, $\forall k \in [K]$ . We just built a bijection between $T_{\mathcal{G}}(\mathbb{X}_n,\mathbb{X}'_n)$ and a subset of $\mathcal{G}$ , denotes $\hat{\mathcal{G}}_n$ . With this notation, we have $$\vert \hat{\mathcal{G}}_n\vert = T_{\mathcal{G}}(\mathbb{X}_n,\mathbb{X}'_n) \le (2n+1)^{V_{\mathcal{G}}(2n) 
 }.$$ Therefore, we can derive from (1.1) that \begin{align*}
\mathbb{P}\left(\sup_{g \in \mathcal{G}}\vert \hat{R}_n(g) - R_p(g)\vert > t \right) &\le 2 \mathbb{P}\left(\sup_{g \in \mathcal{G}}\left\vert \hat{R}_n(g) - \hat{R}'_n(g)\right\vert > \dfrac{t}{2} \right)\\
&=2\mathbb{P}\left(\sup_{g \in \hat{\mathcal{G}}_n}\left\{\hat{R}_n(g) - \hat{R}'_n(g)\right\}> t/2\right)\\
&\le 2 \sum_{g \in \hat{\mathcal{G}}_n} \mathbb{P}\left(\hat{R}_n(g) - \hat{R}'_n(g)> t/2   \right) \tag{union bound}\\
&\le 2\vert \hat{\mathcal{G}_n}\vert \max_{g \in \mathcal{G}} \mathbb{P}\left(\hat{R}_n(g) - \hat{R}'_n(g) > t/2 \right)\\
& \le 2(2n+1)^{V_\mathcal{G}(2n)} \max_{g \in \mathcal{G}} \mathbb{P}\left(\hat{R}_n(g) - \hat{R}'_n(g) > t/2 \right)
\end{align*} By substituting $\hat{R}_n(g) = \dfrac{1}{n}\mathbb{1}_{g(X_i)\neq Y_i}$ and using the Hoeffding inequality, we obtain the result in theorem.","['statistics', 'probability-distributions', 'machine-learning', 'inequality', 'probability']"
4870599,What is the intuition behind the single-pass algorithm (Welford's method) for the corrected sum of squares?,"The corrected sum of squares is the sum of squares of the deviations of a set of values about its mean. $$
S = \sum_{i=1}^k\space\space(x_i - \bar x)^2
$$ We can calculate the mean in a streaming fashion, as follows: $$
m_n = \frac{n-1}{n}m_{(n-1)} + \frac{1}{n}x_n
$$ I understand the intuition behind this: the sum of the previous $n-1$ values was divided by $n-1$ , so by multiplying those values by $\frac{n-1}{n}$ , we down-weight the sum properly. However, we can also calculate the full corrected sum of squares as follows: $$
S_n = S_{n-1} + \frac{n-1}{n}\left( x_n - m_{n-1}\right)^2
$$ However, I don't have a good intuition for why this works. It looks like we use the previous corrected sum of squares value, and then add the square of the current value's deviation from the mean of all the previous values. But, this algorithm doesn't make sense to me, even if it was derived logically. These formulas are from ""Note on a Method for Calculating Corrected Sums of Squares and Products"" by B.P Welford.","['statistics', 'variance', 'sampling-theory', 'means', 'average']"
4870610,If $f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x)$ how to find $f^{(2000)}(0)$ without Taylor series?,"I saw this challenging problem: If $f(x) =\sin^{1200}(x) \ln(1+x)^{500}\arctan^{300}(x)$ how to find $f^{(2000)}(0)$ . Applying Leibniz rule seems to be the only way to solve this, but  it quickly became a nightmare finding a pattern: $$f'(x)=1200 \sin^{1199}(x) \ln(1+x)^{500}\arctan^{300}(x)\cos(x)+\sin^{1200}(x) 500\frac{\ln(1+x)^{499}}{1+x}\arctan^{300}(x)+300\sin^{1200}(x) \ln(1+x)^{500}\frac{\arctan^{299}(x)}{1+x^2}$$ This function becomes very messy and ugly in the first derivative so there is no way Leibniz rule is applied here. There must be some trick since $1200+500+300=2000$ . My friend (who gave me this problem) gave a very strange proof to this problem with  the answer $2000!$ but I didn't understand his proof. There is an easy way to solve this problem with Taylor's series like user170231's answer, But I want to ask is other way to solve it without Taylor series ?","['calculus', 'derivatives', 'real-analysis']"
4870626,How to estimate the best variance-proxy of a sub-Gaussian distribution from data?,"Suppose we have $N$ independently identically distributed (i.i.d.) samples $X_1,\cdots,X_N$ generated from a sub-Gaussian random variable $X \sim \mathbb{P}$ . Then by definition there exists the smallest $\sigma >0$ such that $$\mathbb{E}[\exp(\theta (X - \mathbb{E}[X]))] \le \exp(\theta^2 \sigma^2/2) ~~ \forall \theta \in \mathbb{R}.$$ My question is that how can we estimate such $\sigma$ from the $N$ i.i.d. samples? What statistical properties can we expect for the estimator? My attempt (a natural idea) is to use the definition with the empirical distribution. Let $\bar X$ be the sample mean and solve the following optimization problems $$\hat \sigma_1 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(X_i - \bar X)})}}{\theta}$$ and $$\hat \sigma_2 := \sup_{\theta >0} \frac{\sqrt{2\ln (\frac{1}{N} \sum_{i=1}^N e^{\theta(\bar X - X_i)})}}{\theta}$$ and then take the maximum of the two, i.e., let $\hat \sigma := \max(\hat \sigma_1, \hat \sigma_2)$ . In this way, we obtain the smallest variance-proxy for the empirical distribution. However, this optimization problems have no closed-form solution. Hence, it is difficult to prove unbiasedness, etc. Is there any other method for estimating the variance-proxy from the data with nice statistical properties? It would be appreciated if there is any reference paper or note. Thank you!","['statistical-inference', 'statistics', 'descriptive-statistics', 'parameter-estimation']"
4870713,Every matrix is a product of two symmetric matrices,"Let $\mathbb{F}$ be a field with char $\mathbb{F} \neq 2$ .
Let $A \in M_n(\mathbb{F})$ .
Does there exist symmetric matrices $B,C \in M_n(\mathbb{F})$ such that $A=BC$ ? The answer is yes when $\mathbb{F} = \mathbb{C}$ or $\mathbb{R}$ . But how about other fields?","['matrices', 'linear-algebra', 'symmetric-matrices', 'products']"
4870721,Understanding the proof of Theorem 10.1 in Montgomery & Vaughan's Multiplicative Number Theory,"In the last step of the proof of Theorem 10.1 in the book Multiplicative number theory I: Classical theory by Hugh L. Montgomery, Robert C. Vaughan I couldn't understand what exactly ""turn the path through an angle"" means. I reparametrize $x$ to $xe^{-\frac12 \arg z}$ and I got $$\int_{-\infty}^{\infty} e^{- \pi x^2 e^{-\arg z}z} e^{-\frac12 \arg z} dx$$ which is not equal to the result that the authors claim, i.e. $$z^{-1/2} \int_{-\infty}^{\infty} e^{- \pi x^2} dx = |z|^{-1/2} e^{-\frac12 \arg z} \int_{-\infty}^{\infty} e^{- \pi x^2} dx.$$ My question is how to show that $$\int_{-\infty}^{\infty} e^{- \pi x^2 z} dx = z^{-1/2} \int_{-\infty}^{\infty} e^{- \pi x^2} dx$$ is true?","['fourier-analysis', 'cauchy-integral-formula', 'proof-explanation', 'complex-analysis', 'theta-functions']"
4870743,How to prove the existence of this progression? Struggling with a BDMO problem.,"This problem is from Bangladesh Mathematical Olympiad $2023$ , The problem statement is as follows- Prove that there is sequence of $2023$ distinct positive integers such that the sum of the squares of two consecutive integer is a perfect square itself I can come up with such a sequence but I can't prove it. Such as- $(2^k-1)2^0 , (2^{k-1}-1)2^k , (2^{k-2}-1)2^{k+(k+1)} , (2^{k-3}-1)2^{k+(k+1)+(k+2)}\cdots\cdots\cdots$ Where $k$ represents the number of elements in the sequence I want to create. Say, I want such a sequence with 5 numbers. Plugging in $k=5$ , $(2^5-1)2^0=31$ $(2^{5-1}-1)2^5=480$ $(2^{5-2}-1)2^9=3584$ $(2^{5-3}-1)2^{12}=12288$ $(2^{5-4}-1)2^{14}=16384$ We get the sequence $31,480,3584,12288,16384$ which obviously meet up the conditions. I also tried to find out the $n^{th}$ term of the sequence like this: $n^{th}$ term = ${2^{k-(n-1)}-1}[2^{(n-1)(k+1-\frac{n}{2})}]$ (based on the observed property of the sequence , some simplifications will lead to this) Then what is left to prove is : $\sqrt{({2^{k-(n-1)}-1}[2^{(n-1)(k+1-\frac{n}{2})}])^2+({2^{k-(n)}-1}[2^{(n)(k+1-\frac{n+1}{2})}])^2}$ AND $\sqrt{({2^{k-(n-1)}-1}[2^{(n-1)(k+1-\frac{n}{2})}])^2+({2^{k-(n-2)}-1}[2^{(n-2)(k+1-\frac{n-2}{2})}])^2}$ Are integers, that's where I am stuck, I can't conclude because huge calculations arise with seemingly no positive deduction. Thanks, at least for reading this:)","['contest-math', 'square-numbers', 'discrete-mathematics', 'sequences-and-series']"
4870808,Function $y=e^{1/\ln(x)}$ has a singularity at $x=1$ which breaks it into two continuous parts. What is the minimum distance between these parts?,"The function $y = e^{1/\ln(x)}$ has a singularity at $x = 1$ that partitions it into two separate continuous sections. Notice that it has reflective symmetry about a $y=x$ axis, because it can be re-written as $\ln(x)\ln(y) = 1$ . When plotted, the 'left curve', (i.e. where $x < 1$ ), has a discreet length, is continuous between the points $(0,1)$ and $(1,0).$ The 'right curve', (i.e. where $x > 1$ ), is of infinite length. It has asymptotes at $x = 1$ , and $y=1$ . I am trying to find the minimum distance between the left and right curves, so slopes must be calculated. The derivative of the function is $$-\frac{y}{x\ln^2(x)}$$ The left curve has a centre point at $(\frac{1}{e}, \frac{1}{e})$ , and the tangent at this point is $-1$ . The normal there has slope $1$ , and intersects the right curve at $(e, e)$ .
The tangent at that intersection is also $-1$ , so the normal there has a slope of $1$ . The straight line between $\left(\frac{1}{e}, \frac{1}{e}\right)$ and $(e, e)$ therefore seems to represent the minimum distance between the left and right curves. It equates to $\sqrt{2}\left(e - \frac{1}{e}\right)$ .
But could there be another pair of points, (one in each section) where the distance between them is shorter than this? Clearly they would have to have identical slopes. I think the answer is no but I have not been able to prove it. It would be nice if a method could be found that could be applied to piecewise-continuous functions in general. Maybe the Lagrangian method could be used, but I've only seen that applied to two distinct functions, and this problem is about a single function that has two separate subdomains.","['functions', 'real-analysis']"
4870903,"List all topologies of $X = \{1,2,3\}$ up to homeomorphism.","Problem 2-2 in Lee's Introduction to Topological Manifolds reads: Let $X = \{1, 2, 3\}$ . Give a list of topologies on $X$ such that
every topology on $X$ is  homeomorphic to exactly one on your list. Below is my attempted solution. Is there a more efficient way to solve a problem like this one? My method felt a bit tedious. The topologies can be partitioned into classes according to how many 1- and 2-element sets they contain. Let us write $(m, n) \in \{0, \dotsc, 3\}^2$ to represent each such class, with $m$ being the number of 1-element sets (singletons) and $n$ the number of 2-element sets in the topology, and go through each of them. Note that homeomorphisms preserve class membership. (0,0) This class contains only the trivial topology $\{X, \varnothing\}$ . (1,0) If we add a single singleton to the trivial topology we obtain a topology homeomorphic to $\{X, \varnothing, \{1\}\}$ . (2,0) This class is empty, since if we have two singletons we must also have the 2-element set given by their union. (3,0) Similarly empty. (0,1) Adding a 2-element set to the trivial topology we obtain a topology homeomorphic to $\{X, \varnothing, \{1, 2\}\}$ . (1,1) There are two distinct ways we can add a singleton and a 2-element set to the trivial topology; either they overlap or they do not. Hence we either get a topology homeomorphic to $\{X, \varnothing, \{1\}, \{1, 2\}\}$ , or one homeomorphic to $\{X, \varnothing, \{1\}, \{2, 3\}\}$ . Note that these are not homeomorphic to each other. (A homeomorphism would have to map $\{1\}$ to $\{1\}$ , but simultaneously map $\{1,2\}$ to $\{2,3\}$ , which is impossible.) (2,1) All topologies in this class are hoemorphic to $\{X, \varnothing, \{1\}, \{2\}, \{1, 2\}\}$ , since a topology with two singletons must also contain their union. (3,1) All three singletons together generate the discrete topology, so this class is empty. (0,2) If we have two 2-element sets they must overlap on a single element, thus giving us also a singleton via closure under finite intersection. Hence this class is empty. (1,2) By the logic above, the topologies in this class are all homeomorphic to $\{X, \varnothing, \{1\}, \{1, 2\}, \{1, 3\}\}$ . (2,2) These topologies must have a (1,2)-topology as a subset, by the logic
above. Hence they are homeomorphic to $\{X, \varnothing, \{1\}, \{2\}, \{1, 2\}, \{1, 3\}\}$ . (3,2) Again, only the discrete topology contains all three singletons. (0,3) With all three 2-element sets we get all the singletons via intersections, hence this class is empty. (1,3) Similarly empty. (2,3) Similarly empty. (3,3) Here we have only the discrete topology, $\mathcal P(X)$ . Hence every topology on $X$ is homeomorphic to exactly one of the following topologies: $$\{X, \varnothing\},$$ $$\{X, \varnothing, \{1\}\},$$ $$\{X, \varnothing, \{1, 2\}\},$$ $$\{X, \varnothing, \{1\}, \{1, 2\}\},$$ $$\{X, \varnothing, \{1\}, \{2, 3\}\},$$ $$\{X, \varnothing, \{1\}, \{2\}, \{1, 2\}\},$$ $$\{X, \varnothing, \{1\}, \{1, 2\}, \{1, 3\}\},$$ $$\{X, \varnothing, \{1\}, \{2\}, \{1, 2\}, \{1, 3\}\},$$ $$\mathcal P(X).$$","['general-topology', 'problem-solving']"
4870971,"How to prove continuity of $g(y)=\max_{x\in K} f(x,y)$ when $K$ is compact?","I have a function $f$ that is continuous over the set of variables, $f: \ K \times M \to \mathbb{R}$ , where $K$ is a compact domain of a metric space and $M$ is a metric space. And I want to prove that the function $g(y)=\sup_{x\in K} f(x,y)$ is continuous. As $K$ is a compact, $f$ is uniformly continuous with respect to its first argument. Also, for the same reason, $g(y)=\sup_{x\in K} f(x,y)=\max_{x\in K} f(x,y)$ . For now, I know that for the case of $f$ is continuous with respect of each variable separately, this doesn't work (see an example here Is supremum over a compact domain of separately continuous function continuous? ) and there is a proof for the case when $M$ is also a compact, or $f$ is just uniformly continuous on $M$ , too ( How prove this $g(x)=\sup{\{f(x,y)|0\le y\le 1\}}$ is continuous on $[0,1]$ ), which I personally doubt as there is no guatantee that y-s will be close there (in my notations they are x-s). All my tries fail when I come to the point where I need to say something about argmaxes of even close y-s. For example: I consider a sequence $y_n \to y_0$ with $n\to\infty$ . For each $y_n$ there exists $x_n$ such that $g(y_n)=\max_{x\in K} f(x,y_n)=f(x_n,y_n).$ Then, I try to estimate the difference $$|g(y_{n+k})-g(y_n)|=|f(x_{n+k},y_{n+k})-f(x_n,y_n)|=|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})+f(x_{n+k},y_{n})-f(x_n,y_n)|\le|f(x_{n+k},y_{n+k})-f(x_{n+k},y_{n})|+|f(x_{n+k},y_{n})-f(x_n,y_n)|.$$ One can easily see that the first component tends to zero with $k\to\infty$ , but I have no idea what to do with the second one... Any help would be very appreciated!","['analysis', 'continuity', 'multivariable-calculus', 'calculus', 'multivalued-functions']"
4870982,How to combine two probabilities for the same event? Context: error correction codes / decoding,"I'm learning the maths behind error correction codes. For this purpose I made this question for myself: Assume there are two random bits $x_0$ , $x_1$ , which are both i.i.d. and have a 50% chance of being 0 or 1 (the information bits) and an additional check bit $x_2 = x_0 \mathbin{\mathsf{XOR}} x_1$ ). You now transfer all 3 bits through an additive white gaussian noise channel (AWGNC), one by one. The AWGNC adds noise to each bit independently and on the receiver side, you can only restore each bit with some probability, depending on what you received. You do this for each bit individually and independently of the other bits and conclude that the 3 probabilities of the bits to be 1 are $p = (0.2, 0.9, 0.7)$ , I.e. $P(x_0 = 1 \;|\; \text{given the noisy version of $x_0$ that you received}) = 0.2$ $P(x_1 = 1 \;|\; \text{given the noisy version of $x_1$ that you 
received}) = 0.9$ $P(x_2 = 1 \;|\; \text{given the noisy version of $x_2$ that you received}) = 0.7$ Obviously, the best guess $y$ for the sent bits $x$ is $y = (0, 1, 1)$ . How to calculate the combined probability of this guess to be correct? Or of any other guess? I.e. how to calculate $P(x = (0, 1, 1) \;|\; p = (0.2, 0.9, 0.7))$ or other guesses? Using the fact that the originally sent $x_2$ is the XOR of the other two originally sent bits, we can use the probabilities $p_0$ and $p_1$ to infer another probability about $x_2$ : $$
\begin{align}
p_2' := {} &P(x_2 = 1 \mid p_0 = 0.2, p_1 = 0.9) \\[4pt]
= {} & P(x_0 = 0 \mid p_0 = 0.2) \cdot P(x_1 = 1 \mid p_1 = 0.9) \\
 {} & + P(x_0 = 1 \mid p_0 = 0.2) \cdot P(x_1 = 0 \mid p_1 = 0.9)\\[4pt]
= {} & (1 - 0.2) \cdot 0.9 + 0.2 \cdot (1 - 0.9) \\[4pt]
= {} & 0.8 \cdot 0.9 + 0.2 \cdot 0.1\\[4pt]
= {} & 0.72 + 0.02\\[4pt]
= {} & 0.74.
\end{align}
$$ How to combine the probabilities $p_2' = 0.74$ and $p_2 = 0.7$ into the combined probability $p_2'' := P(x_2 = 1  \;|\; p = (0.2, 0.9, 0.7))$ ? Does the question make sense in this form? Do I need to know the distribution of the probability after the channel, given a bit's value before the channel? To give an illustration and overview of the relations for transferring a single bit b:","['conditional-probability', 'statistical-inference', 'statistics']"
4871021,"If $x,y,z>0$ and $x+y+z=1$,then find the maximum value of $(1-x)(2-y)(3-z)$.","If $x,y,z>0$ and $x+y+z=1$ ,then find the maximum value of $(1-x)(2-y)(3-z)$ . My Attempt: We have $0<1-x<1,1<2-y<2,2<3-z<3$ so A.M-G.M inequality cannot be used since we can never have $1-x=2-y=3-z$ . Is there any other way out","['maxima-minima', 'calculus', 'algebra-precalculus', 'inequality']"
4871053,Discontinuous process with same marginals as Brownian Motion,"I understand that there are many discontinuous modifications of Brownian Motion, one of which seen in my course was: $$X_t = \begin{cases} 
      B_t & t\neq u \\
      0 & t=u \\ 
   \end{cases}
$$ where $(B(t):t\in[0,\infty))$ is BM and $u$ is $\text{Unif}([0,1])$ . However, I don't understand the claim that $(X(t))$ has the same finite dimensional marginals as $(B(t))$ . Recall that same finite-dimensional marginals means that with probability 1, for any finite sets $\{t_j\}_1^k\subseteq T$ , the collection of marginals of both stochastic processes equate in law with each other. Any help is appreciated. Edit: I'm adding ""with probability 1"" to the beginning the definition of same finite-dimensional marginals for congruence with the answer below. Also, in the same light as the answer below, it's clear that if the BM was $(B(t): t\in [0,n] \cap \mathbb{N})$ for some $n$ , then equality in f.d. marginals would no longer hold.","['stochastic-processes', 'measure-theory', 'brownian-motion', 'probability-theory']"
4871057,Algebraic points on circles/solutions to $x^2+y^2=a$ over $\overline{\mathbb Q}$: is there a point with odd degree field extension over $\mathbb Q$?,"It is well known how to parameterize rational solutions to $x^2+y^2=a$ for $a\in \mathbb N^+$ : first we know which $a$ admit integer solutions ; our characterization is strong enough to tell us that theses are exactly the $a$ admitting rational solutions; then if we have one rational solution we can get all of them by drawing rational-slope lines through that one point . I am interested in knowing if people understand well the algebraic solutions to $x^2+y^2=a$ , i.e. the solutions $x,y\in \overline{\mathbb Q}$ to $x^2+y^2=a$ . In particular, I am interested in knowing if there exists an algebraic solution $(x_0,y_0)\in \overline{\mathbb Q} \times \overline{\mathbb Q}$ to $x^2+y^2=3$ such that the degree of the field extension $[\mathbb Q(x_0,y_0): \mathbb Q]$ is odd. (Indeed it has no solutions over $\mathbb Q$ ) EDIT: a ""famous"" (in the sense of large number of upvotes/views) question on MSE in fact shows that $[\mathbb Q(x_0+iy_0):\mathbb Q]$ must be even. If $z_0:= x_0+iy_0$ was a root of unity (of say order $n$ ), then $z_0^{n-1} = \overline{z_0}$ , and so $\mathbb Q(z_0) = \mathbb Q(z_0, \overline{z_0})= \mathbb Q(x_0,iy_0)$ . Unfortunately, this does not give us what we want, and moreover, $z_0$ does not have to be a root of unity .","['field-theory', 'number-theory', 'algebraic-number-theory', 'extension-field']"
4871060,How many positive integers up to 100 can be written as a sum of 4 or more consecutive integers?,"I was trying to solve the question: how many positive integers $n \leq 100$ can be written as the sum of 4 or more consecutive integers. Here was my approach: Let $m$ represent the number of consecutive integers considered, and let $k$ be the starting integer. For $m=4$ : $$ n = (k) + (k + 1) + (k + 2) + (k + 3) = 4k+6$$ So the inequality becomes: $$4k+6 \leq 100$$ $$k \leq 23$$ So there are 23 solutions for $m=4$ . For $m=5$ , using a similar approach there are 18 solutions. In a similar vein, this can be done until $m=13$ , when there is only 1 solution.
| $m$ | Number of solutions |
| --- | -------------------- |
| 4   | 23                   |
| 5   | 18                   |
| 6   | 14                   |
| 7   | 11                   |
| 8   | 9                    |
| 9   | 7                    |
| 10  | 5                    |
| 11  | 4                    |
| 12  | 2                    |
| 13  | 1                    |
In total, there are 94 solutions. However, there is double counting involved. This means that the real solution must be less than 94. So my question is how many positive integers are there less than 100 that cannot be written as a sum of 4 or more consecutive integers?",['number-theory']
4871091,Proof for Particular Fair Shuffle Algorithm,"I ran multiple simulations of the following function, and it seems to be fair shuffling, given that all permutations were roughly equal, but I don't understand why it works. It's just inserting at random positions within the current shuffled deck isn't it? I know about Fisher-Yates shuffling, for reference. def shuffle_deck(deck):
    shuffled_deck = []
    for card in deck:
        r = random.randint(0, len(shuffled_deck))
        shuffled_deck.insert(r, card)
    return shuffled_deck I was expecting that it would be uneven probability permutation.","['python', 'combinatorics', 'probability', 'algorithms']"
4871225,Number of possible relations with following restrictions | Discrete Mathematics [closed],"Closed . This question needs details or clarity . It is not currently accepting answers. Want to improve this question? Add details and clarify the problem by editing this post . Closed 4 months ago . Improve this question I am new to math. stack exchange, I am really not sure how I am supposed to ask this but I need a logical explanation and a way to logical way to approach questions like these. I tried doing it myself. Thank you. The question is as follows: $$\text{Let } A = \{a, b, c, d, f, g, h, k, u, x, z,y\}. \\$$ $(1)$ How many relations are there on $A$ that are reflexive, symmetric, and contain all the elements $(a,z), (u,u)$ and $(z,a)$ ? (2) How many relations are there on $A$ that are antisymmetric and contain all the elements $(c,c), (x,a)$ and $(b,c)$ , but do not contain the element $(c,b)$ ? (3) ) ) How many relations are there on A that are reflexive, antisymmetric, and contain all the elements (x,a),(a,a),(b,d), and (a,z), but do not contain any of the elements (a,x),(a,b),(d,z),(z,d) or (c,d)? My Solution Let $A$ = {a,b,c,d,e,f,g,h,k,w,x,y,z}. (1) . But I can use the formulas outlined for reflective and systematic. the formula is $$2^{\frac{n^2-n}{2}}$$ since the last condition says, the relation should also include (a,z),(u,u), and (z, a). We will subtract 3 from the power of the whole expression as these elements are already contained. (2) not sure about the other 2.","['relations', 'discrete-mathematics']"
4871263,unbiased estimator of sample mean,"The question: Given a random sample $X_1,...,X_n$ show that $\frac{1}{n}\sum_{i=1}^n X_i$ is an unbiased estimator for $E(X_1)$ . My confusion: Given a statistical model $(\Omega,\Sigma,p_{\theta})$ , where $p_{\theta}$ is a parameterized collection of probability measures, We define $E_{\theta}(X)=\int_{\Omega}X(\omega)p_{\theta}(d\omega)$ , where $X:\Omega\to\mathbb{R}$ . If $\frac{1}{n}\sum_{i=1}^nX_i$ is unbiased, we need $E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=\theta$ , when $\theta=E(X_1)$ . But what is $E(X_1)$ ? With respect to what measure are we integrating, if we're considering all these different measures on the space? For any $\theta$ , $E_{\theta}(\frac{1}{n}\sum_{i=1}^nX_i)=E_{\theta}(X_1)$ , because each $X_i$ is identically distributed. My guess is, you would say something like, ""for a fixed probability measure $p_{\varphi}$ on $(\Omega,\Sigma)$ , $\frac{1}{n}\sum_{i=1}^nX_i$ is an unbiased estimator for $E_{\varphi}(X_1)$ ."" Is this right?","['statistics', 'definition']"
4871298,"Given $f(x)=x+\sin(x+\sin(x+\sin(x+\cdots)))$, how would I go about finding the value of $f(1)$ to $5$ decimal places?","Consider a function $f(x)$ that is defined as $$f(x)=x+\sin(x+\sin(x+\sin(x+\cdots)))$$ Now, say we wanted to find the value of $f(1)$ to $5$ decimal places. My question is, how would I go about doing this? Here is what I have tried: Note that we have $$f(1)=1+\sin(1+\sin(1+\sin(1+\cdots)))=1+\sin(f(1))$$ So we need to find a number such that $$y=1+\sin(y)$$ or $$\sin(y)-y+1=0$$ however I do not know how to solve it from here. So my question is: How should I go about evaluating $f(1)$ to $5$ decimal places? For context I have been finding the exact values of functions at certain points such as $$g(e)\text{ for }g(x)=\ln(x+\ln(x+\ln(x+\ln(x+\cdots))))$$ and $$h(1)\text{ for }h(x)=\ln(x+1/(x+\ln(x+1/(x+\ln(\cdots)))))$$ and decided to be evil and gave myself this to evaluate to only $5$ decimal places.","['calculus', 'trigonometry', 'recreational-mathematics']"
4871308,"Solving the system $A\cos x +B\cos y=C$, $A\sin x + B\sin y=D$","It is not a school-related problem, and pretty much the title. I am looking for a way to solve the system of trigonometric equations: $$
A\cos(x) + B\cos(y) = C \tag{1}
$$ $$
A\sin(x) + B\sin(y) = D \tag{2}
$$ I have already solved the system for the specific case of $A=B$ and am looking to generalize this to $A \neq B$ . For reference, my solution to the specific case $A=B$ is: $$x = \cos^{-1}(\frac{C}{A} - \cos(\cot^{-1}(\frac{C}{D})-\cos^{-1}(\frac{D}{2A\sin(\cot(\frac{C}{D})})) \tag{3}$$ $$y = \cot(\frac{C}{D}) - \cos^{-1}(\frac{D}{2A\sin(\cot(\frac{C}{D})})) \tag{4}$$ I have been using computer software such as WolframAlpha to solve it, and they do solve it, but I've been unable to come up with a solution by hand after a couple months of working on it pretty regularly. Can it be solved by hand?","['algebra-precalculus', 'systems-of-equations', 'trigonometry', 'recreational-mathematics']"
4871500,Method to produce polyharmonic functions from harmonic ones,"Let $F:\mathbb{R}^2\to\mathbb{R}$ be a harmonic function (it can be thought as one component of a holomorphic function), take $m$ odd and let $f:\mathbb{R}^{m+1}\to\mathbb{R}$ , $$f(a,x_1,\dots,x_m)=\frac{1}{x_1^2+\dots x_m^2}F\left(a,\sqrt{x_1^2+\dots x_m^2}\right).$$ Prove that for any $k\in\mathbb{N}$ it holds $$
\Delta_{m+1}^kf(a,x_1,\dots,x_m)=(m-3)\cdot...\cdot(m-2k-1)\sum_{j=1}^{k+1}a_j^{(k+1)}b^{j-2k-2}\partial_b^{(j-1)}F(a,b(x_1,\dots,x_m)),
$$ where $\Delta_{m+1}=\partial^2_a+\sum_{j=1}^m\partial^2_{x_j}$ is the Laplacian of $\mathbb{R}^{m+1}$ and $a_j^{(k)}:=\frac{(2k-j-1)!}{(j-1)!\,(k-j)!\,(-2)^{k-j}}$ satisfy $a_j^{(k+1)}=a_{j-1}^{(k)}+(j-2k)a_j^{(k)}$ for any $j=1,\dots,k+1$ . In particular, $f$ is polyharmonic of degree $\frac{m-1}{2}$ . A proof by induction is what I am trying to get.","['harmonic-analysis', 'real-analysis', 'complex-analysis', 'laplacian', 'combinatorics']"
4871535,Matricial differential equation and eigenvalues,"I have the matricial differential equation: $$\frac{\text{d}}{\text{d}t}\vec{x}(t)=A \cdot \vec{x}(t)$$ where: $$\vec{x}(t) \in \mathbb{R}^3, \quad A=\begin{pmatrix} a & 0 & b \newline 0 & c & 0 \newline d & 0 & a \end{pmatrix}, \quad a,b,c,d \in \mathbb{R}$$ I need to find $a,b,c,d \in \mathbb{R}$ such that: $$\lim_{t \to +\infty} \vec{x}(t)=\vec{0}$$ for every initial condition $\vec{x}(0) \in \mathbb{R}^3$ . The very short solution provided by the text is that the eigenvalues of the matrix $A$ must have negative real part. I can't undetstand why. What I have done so far: I know that the solution can be written in the form: $$\vec{x}(t)=e^{tA} \cdot \vec{x}(0)$$ and I can easily find the eigenvalues of $A$ : $$c, \quad a-\sqrt{bd}, \quad a+\sqrt{bd}$$ If the real part must be negative then we must have: $c<0$ ; $a<0 \quad \text{if} \quad bd<0$ ; $a<-\sqrt{bd} \quad \text{if} \quad bd>0$ .","['linear-algebra', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4871552,Maximization of Linear Least Squares with a Triangular Matrix over The ${L}_{2}$ Unit Ball,"I have the following optimization \begin{align}
\max_{\|x\|^2\le1} \|Lx - y\|
\end{align} where $L$ is a lower triangular and $y$ is a given vector.
Does it admit a closed-form solution? I am interested in the general case where $L$ may be not invertible and even not square, but any progress under some assumptions will be muchnappreciated. The motivation is to understand the standard least squares method but as a game from the ""adversarial"" point of view: $y$ is a measurement sequence and $x$ is the set of possible states from $y = Lx + n$ .","['nonlinear-optimization', 'least-squares', 'linear-algebra', 'non-convex-optimization', 'optimization']"
4871567,Topology - Is infinite product of quotient maps a quotient map?,"Product of two quotient maps need not be a quotient map.
However, given two quotient maps $$p : A → B\\ q : C → D$$ if  A, B, C, D are locally compact Housdorff then $$p × q : A × C → B × D$$ is a quotient map. Does this still hold for product of arbitrary number of maps? Given collection of quotient maps $\{p_i : X_i → Y_i | i ∈ I\}$ , where $X_i, Y_i$ are locally compact Housdorff spaces for all $i∈I$ ,
is $$\prod p_i : \prod X_i → \prod Y_i$$ a quotient map in the usual product topology? I first attempted to find a counterexample in $\Bbb R^\omega$ , trying to figure out whether $p × id^\omega : \Bbb R × \Bbb R^\omega → p(\Bbb R)\Bbb × R^\omega$ is quotient if p is any quotient map with such condition. Any help or hint would be greatly appreciated.",['general-topology']
4871588,"Find two coins of weight a, among n coins, where n-2 coins are coins of weight b","Task: Among the n coins there are exactly 2 coins of weight a, and exactly $n − 2$ coins of weight $b, a < b$ .
It is allowed to compare the weight of any two coins in one turn (there are three comparison results: lighter, heavier, of the same
weight). Purpose: to determine the weight of each coin. Prove that there is such a constant $C$ that
at least $\frac{2n}{3} − C$ comparisons are necessary. Some thinkings: Not sure where to begin. As far as I understand, in this task it is necessary to prove that in less than $\frac{2n}{3}$ comparisons it is impossible to determine the weight of each coin. UPD: In the comments, people left really good advice on solving this task, but it is unclear how to link this to the constant C. It is clear that at best it is possible to determine the weight of all coins in 2 comparisons, and in $\frac{2n}{3}$ comparisons in the worst case. But how exactly to relate this to the constant C is not entirely clear to me","['algorithmic-game-theory', 'game-theory', 'discrete-mathematics', 'algorithms']"
4871602,How to show perpendicularity in a square,"$ABCD$ is a square so $AB=BC=CD=DA=20$ and $AE=BF=15$ . Since $DAE \sphericalangle =90^0$ we can use the Pythagorean theorem so $AD^2+AE^2=DE^2$ and we get that $DE=25$ . We know that $DAE\sphericalangle=ABF\sphericalangle=90^o$ , $AD=AB=20$ and $AE=BF=15$ so from (SAS) we get that triangles $DAE\equiv ABF$ . How to show that $AF \perp DE$ , so that I can calculate $AM=\frac{AD\cdot AE}{DE}=12$",['geometry']
4871604,Ruling out finite time explosions to infinity of Riccati differential equations,"Let $\xi^*,\varsigma,\eta:\mathbb{R}\rightarrow (0,\infty)$ . You may assume any degree of smoothness required of these functions. Assume that $\xi^{*}(t)$ and $\varsigma(t)$ have finite positive limits as $t\rightarrow\infty$ and as $t\rightarrow -\infty$ . Define $\delta:\mathbb{R}\rightarrow\mathbb{R}$ by: $$\delta(t)=\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}-\varsigma(t),$$ for all $t\in\mathbb{R}$ . Finally, assume that for all $\tau\in\mathbb{R}$ , $\xi_{\tau}:[\tau,\infty)\rightarrow [0,\infty)$ satisfies: $$\xi_{\tau}(\tau)=\varsigma(\tau),$$ and: $$\dot{\xi}_{\tau}(t)={\xi_{\tau}(t)}^2+\left(\delta(t)-\eta(t) {\delta(t)}^2 \right) \xi_{\tau}(t)+\eta(t) {\delta(t)}^2 \xi^{*}(t).$$ In particular, I need that $\xi_{\tau}(t)<\infty$ for all $t\ge\tau$ , $\tau\in\mathbb{R}$ , so $\xi_{\tau}$ does not explode to infinity in finite time for any $\tau$ . This will hold for example, if $\eta(t)=0$ and $\varsigma(t)=\xi^{*}(t)$ for all $t\in\mathbb{R}$ , in which case a solution is $\xi_{\tau}(t)=\xi^{*}(t)$ . I.e., if we restrict BOTH $\varsigma$ and $\eta$ then we can definitely ensure that $\xi_{\tau}$ does not explode to infinity in finite time. My question is the following: Can you ensure that $\xi_{\tau}$ does not explode to infinity in finite time (for all $\tau$ ) by only restricting ONE of $\varsigma$ and $\eta$ , not both? (You may nonetheless impose restrictions on the limits of $\varsigma(t)$ as $t\rightarrow\pm\infty$ if necessary, even if you are restricting $\eta$ not $\varsigma$ .) Constant coefficient case In the case in which $\xi^*$ , $\varsigma$ and $\eta$ are all constant over time, Maple gives the solution as: $$2\xi_{\tau}(t)=\varsigma(\eta\varsigma+1)-\omega\tanh{\left[(t-\tau)\frac{\omega}{2}+\operatorname{arctanh}{\left(\frac{\varsigma(\eta\varsigma-1)}{\omega}\right)}\right]},$$ where $\omega = \varsigma\sqrt{\eta^2\varsigma^2+2\eta(\varsigma-2\xi^{*})+1}$ . So, in this case, for the solution to be bounded, we need $\varsigma>\max{\left\{\frac{1}{\eta},\frac{2\sqrt{\eta\xi^{*}}-1}{\eta}\right\}}$ OR $\varsigma\in\left(\xi^{*},\frac{1}{\eta}\right)$ so at least in this case, we only need to restrict $\varsigma$ , not $\eta$ . Another special case Suppose: $$\eta(t)=\frac{\delta(t)+2\xi^{*}(t)+2\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}}{{\delta(t)}^2},$$ then the two roots of the right hand side of the ODE are identical, meaning that $\dot{\xi}_{\tau}(t)\ge 0$ , and making the algebra much neater. This is positive and real, as required, as long as $\xi^{*}(t)+\delta(t)\ge 0$ , i.e. $\varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}$ . With this particular $\eta(t)$ , the root of the right hand side of the ODE at $t$ is given by: $$r(t)=\xi^{*}(t)+\sqrt{\xi^{*}(t){\left[\xi^{*}(t)+\delta(t)\right]}}.$$ Using the results from Kilicaslan and Banks (2010) , we have that the system does not explode to infinity if and only if for all $t\in\mathbb{R}$ , $r(t)\ge\varsigma(t)$ , for which it is sufficient that $\varsigma(t)\le\xi^{*}(t)$ and $\varsigma(t)\le \xi^{*}(t)+\frac{\dot{\xi}^{*}(t)}{\xi^{*}(t)}$ (as already assumed). The formulas given in Kilicaslan and Banks (2010) imply that in this case, the solution is given by: $$\xi_{\tau}(t)=r(\infty)-\frac{r(\tau)-\varsigma(\tau)}{1+(t-\tau)\left[r(\tau)-\varsigma(\tau)\right]},$$ where $r(\infty):=\lim_{t\rightarrow\infty}{r(t)}$ , however, this does not appear to solve the original ODE, so something must be wrong (perhaps in the paper??). Yet another special case Suppose: $$\eta(t)=\frac{\bar{\xi}\left[\bar{\xi}+\delta(t)\right]}{{\delta(t)}^2\left[\bar{\xi}-\xi^{*}(t)\right]},$$ where $\bar{\xi}$ is a constant with $\bar{\xi}+\delta(t)>0$ and $\bar{\xi}-\xi^{*}(t)>0$ for all $t\in\mathbb{R}$ . Define: $$r(t)=\frac{\xi^{*}(t)\left[\bar{\xi}+\delta(t)\right]}{\bar{\xi}-\xi^{*}(t)}>0,$$ then the original ODE is: $$\dot{\xi}_{\tau}(t)=\left[\xi_{\tau}(t)-\bar{\xi}\right]\left[\xi_{\tau}(t)-r(t)\right].$$ For this not to explode to infinity in finite time, it is sufficient that $\xi_{\tau}(\tau)=\varsigma(\tau)<\bar{\xi}$ , so we need $\varsigma(t)<\bar{\xi}$ for all $t\in\mathbb{R}$ .","['stability-theory', 'quadratics', 'ordinary-differential-equations']"
4871655,Finding the solution of a Differential Equation $xy''=-yy'$,"I have the following initial value problem, $$
\begin{cases}
    xy''=-yy', \; \forall x \in (0,1] \\
    y(0)=0,\\
    y'(0)=2
\end{cases}
$$ If it was linear, I would have no problems. But here I don't know what method should I use.
I have used the Euler method to find an approximation to the answer in some points. Can someone give me a hint of how to find the analytic soluction.",['ordinary-differential-equations']
4871702,"In how many ways it is possible to take out balls from the basket , such that will take out at least one from each color?","The question: In a basket there are $20$ black balls, $15$ white balls and $18$ red balls. All balls with the same color are identical to each other. In how many ways it is possible to take out balls from the basket (when order doesn't matter), such that will take out at least one black ball, at least one white ball and at least one red ball (without using pascal triangle principles) ? I though that the min number of balls we have to choose is $3$ (one from each color),and the max numbers of balls is $20 + 15 + 18 = 53$ So, for the min number of balls we'll have $1$ option (black, white and red), and from now we'll ""glue"" the subset {black, white and red} to all the other options. For $4$ balls we have ${3 \choose 1}$ options in order to choose the ball which appears twice. For the 5 balls we have ${3 \choose 1}$ options to choose the color of the forth ball, and ${3 \choose 1}$ and to choose the color of the fifth ball. it means ${3 \choose 1} ^ 2$ Hence, I thought to write that for every $0≤k≤49$ we have the same pattern of options, thus we I would do a sigma of this pattern. But it is not correct because there are limitations on the number of balls from a certain color (for example, imagine that we choose $15$ white and 1 from each other color, the number of options for $18$ balls rely on only $2$ colors. so, I would like to have a direction of how to approach to this question correctly. By the way, firstly I thought to use Inclusion–exclusion principle but I saw that it became complicated because if we want to use the complement principle, we need to separate to three different cases. (because the complement of at one least red, at least one black and at least one white by De-Morgan is no black balls or no white balls or no red balls.) I know that there is a similar question - How many ways can you select $5$ marbles if you select at least one from each color? but here the upper bounds don't exceed to their sum (50), so I am not sure this approach will help to solve the exercise.","['binomial-coefficients', 'combinatorics', 'balls-in-bins', 'discrete-mathematics']"
4871791,Why does Im($(-1/10^n)^{-1/10^n}$) turn into the digits of pi as integer n gets larger?,"$(-.001)^{-.001} \approx 1.007 - .0031634i$ , $(-.000001)^{-.000001} \approx 1.000014 - .00000314164$ , and $(-.000000001)^{-.000000001} \approx 1.0000000207 - .00000000314159271i$ . Notice that as we approach 0 from the negative direction for $x^x$ by  magnitudes of ten, the imaginary component more closely approximates pi, divided by an increasing power of ten. Why does this happen?","['limits', 'calculus', 'pi', 'complex-numbers']"
4871793,Find the angle x in the regular octagon below,"In the plane figure below, we have a regular octagon $ABCDEFGH$ , and I belongs to the diagonal $CG$ , so that $\angle GIH = 30°$ . Knowing this, determine in degrees the value of the angle indicated by x.(S: $75^o$ ) Itry: $a_i=\frac{180.(8-2)}{8} = 135^o $ $\angle IGH = \frac{135} {2}=67,5^o  \implies\angle IHG = 82,5^o$ $\angle AHB = \frac{180-135}{2}=22,5^o \implies \angle BHI  =135 - 82,5-22,5 = 30^o \cong \angle GIH$ I'm missing a relationship to finish that I didn't find","['euclidean-geometry', 'geometry', 'plane-geometry']"
4871802,game coloring points rioplatense olympiad 1999,"This problem is from the Rioplatense MO 1999/3 L3. The question is the following: Two players $A$ and $B$ play the following game: $A$ chooses a point, with integer coordinates, on the plane and colors it green, then $B$ chooses $10$ points of integer coordinates, not yet colored, and colors them yellow. The game always continues with the same rules; $A$ and $B$ choose one and ten uncolored points and color them green and yellow, respectively. a) The goal of $A$ is to achieve $111^2$ green points that are the intersections of $111$ horizontal lines and $111$ vertical lines (parallel to the coordinate axes). $B$ 's goal is to stop him. Determine which of the two players has a strategy that ensures they achieve their goal. b) The goal of $A$ is to achieve $4$ green points that are the vertices of a square with sides parallel to the coordinate axes. $B$ 's goal is to stop him. Determine which of the two players has a strategy that ensures they achieve their goal. I'm having trouble with part b).
For part a), A has the winning strategy. The winning strategy is to place at least $111 \times 11 ^{110}$ green points in a horizontal line. The strategy is basically to form row by row (horizontally), and put a sufficient number of points so that it can form another horizontal row. $111 \times 11 ^{110}$ is the minimum because if we need to place $f_i$ points in the $ith$ row, we need to place $10f_{i} + f_{i} = 11f_{i}$ points in the $i-1th$ row. Then it's easy too see that it's the minimum. I am aware that it is not formally explained, if necessary I can write it formally and better explained, although I think this idea is quite intuitive. For part b) I have no idea how to continue with the problem. I spent a long time with this part and I'm not even sure who has the winning strategy.
I found this problem in a Pigeonhole principle resource, so that may be useful.","['contest-math', 'game-theory', 'combinatorics', 'combinatorial-game-theory']"
4871803,"A curious result from Mathematica on $\int\sin^5x\cos^7x \,dx$","While creating an answer key for my Calc 2 students and trying to save a bit of time, I plugged a trigonometric integral into Mathematica and was a little confused about how it approached the problem. The integral was $$
\int\sin^5x\cos^7x \,dx
$$ The typical way to evaluate this integral would be to use the Pythagorean identity and a substitution. Something like this: \begin{align*}
\int\sin^5x\cos^7x \,dx
&= \int (\sin x)\big(\sin^4 x\big)\cos^7x\,dx \\
&= \int(\sin x)\big(1-\cos^2 x\big)^2\cos^7 x\, dx \\
&= -\int \big(1-u^2\big)^2u^7\, du \\
&= -\int u^7-2u^9+u^{11}\,du \\
&= -\frac{u^8}{8}+\frac{u^{10}}{5}-\frac{u^{12}}{12}+C \\
&= -\frac{\cos^8 x}{8}+\frac{\cos^{10}x}{5}-\frac{\cos^{12}x}{12}+C
\end{align*} Where I've used the substitution $u=\cos x$ . We also could have used $u=\sin x$ to get $$
\int\sin^5x\cos^7x \,dx = \frac{\sin^6 x}{6} -\frac{3\sin^8 x}{8}+\frac{3\sin^{10}}{10}-\frac{\sin^{12} x}{12}+C
$$ which is equivalent up to an added constant. When I plugged the integral into Mathematica, it spat out $$
-\frac{5\cos 2x}{1024}
-\frac{5\cos 4x}{8192}
+\frac{5\cos 6x}{6144}
+\frac{\cos 8x}{4096}
-\frac{\cos 10x}{10240}
-\frac{\cos 12x}{24576}
+C
$$ I know this is equivalent to the other two answers. It's a bit tedious to do, but playing around with identities will show this. My question is how could we arrive at this result in a natural way? From the arguments of $\cos$ being even multiples of $x$ and the powers of 2 showing up in the denominators, it's clear that the power reducing identities are being used. (That is, $\cos^2 x = \frac{1+\cos 2x}2$ and/or $\sin^2 x = \frac {1-\cos 2x}2$ .) Typically we would use these if we were evaluating $\int \sin^mx\cos^nx\,dx$ , where $m$ and $n$ are both even. But with $m$ and $n$ both odd, I'm perplexed. I haven't been able to find an approach that doesn't require applying these identities after integrating. I've attempted to rewrite the integrand in terms of even powers of $\sin$ and $\cos$ , but that was a dead end. I also tried rewriting the integral as \begin{align*}
\int\sin^5x\cos^7x \,dx
&= \frac 1{32} \int \big(\sin^5 2x\big) \big(\cos^2 x\big)\,dx \\
&= \frac 1{64} \int \big(\sin^5 2x\big) \big(1+\cos 2x\big)\,dx 
\end{align*} but this didn't quite get me there. I'd appreciate any help figuring this out, thanks!","['calculus', 'trigonometric-integrals']"
4871848,Is every regular Alexandrov topology a partition topology?,"An Alexandrov topology on a set $X$ is a topology in which arbitrary intersections of open sets are open.  Equivalently, every point has a smallest open neighborhood. Given a partition on a set $X$ , one can form the corresponding partition topology by taking the partition as a base for the topology. It is clear that a partition topology is an Alexandrov topology (the smallest nbhd of a point is the partition block containing the point), and is regular . The converse should be true: Every regular Alexandrov topology is a partition topology. In particular, every finite regular space has a partition topology. Can anybody provides a proof?",['general-topology']
4871850,A very odd resolution to an integral equation,"Here is something I've found on the internet $$\begin{aligned}
f-\int f&=1\\
\left(1-\int\right)f&=1\\
f&=\left(\frac1{1-\int}\right)1\\
&=\left(1+\int+\int\int+\dots\right)1\\
&=1+\int1+\int\int1+\dots\\
&= 1+x+\frac{x^2}2+\dots\\
&= e^x
\end{aligned}$$ At first I interpreted this as a joke, but on closer inspection I'm not so sure... The first thing I checked was the solution, and fairly enough, $e^x$ satisfies the initial equation. It is not the only solution, though: $\lambda e^x$ works jut as well for any $\lambda\in\mathbb R$ . I'm not really familiar with integral equations; I don't know if the solution space is a vector space. Maybe it is and our goal is to find a basis for it. If that is the case, then $e^x$ may actually be the expected result. That being settled, I started looking at the development. Writing $\displaystyle f-\int f$ as $\displaystyle \left(1-\int\right)f$ is completely fine, its just operational calculus. The ""division of both sides"" by $\displaystyle\left(1-\int\right)$ must actually mean applying its inverse $\displaystyle\left(\frac1{1-\int}\right)$ on both sides, right? This seems alright, but its not obvious at all to me why should this operator be invertible. In fact, the existence of multiple solutions suggests otherwise... The most ridicule step surely is writing $\displaystyle \frac1{1-\int}=1+\int+\int\int+\dots$ . If this is true, it settles my problem with the previous step. Is it, though? I can see how the infinite integral summation may be well-defined, but no further than that. I know it is referencing the formal power series equality $\displaystyle\frac1{1-X} = 1+X+X^2+\dots$ , but I don't know if the same proof applies since it requires distributiveness. Finally, there is an infinite amount of problematic constants being vanished into nonexistence at $\displaystyle 1+\int1+\int\int1+\dots$ , a single of which would be enough to destroy the solution: $e^x+1$ does not satisfy the equation. My guess is that this is really a joke, but I want to know just how much of it is true. Where does the problems arise and how big are they? Is there anything salvageable here?","['calculus', 'solution-verification', 'integral-equations', 'functional-analysis']"
4871851,Kummer's Lemma and $1+\zeta$,"In lecture we were told to think about the following: Kummer's Lemma: Let $p$ be an odd prime and let $\zeta := e^{2\pi i / p}$ . Every unit of $\mathbb{Z}[\zeta]$ is of the form $r\zeta^g$ , where $r$ is real and $g$ is an integer. This theorem says that the units of $\mathbb{Z}[\zeta]$ can be thougth as the points of the lines that pass through the vertices of an equally spaced $p$ -gon centered at the origin. (Said vertices are of the form $\zeta^g$ .) However, we know that $1+\zeta$ is a unit . But this number does not lie on one of those lines. What is wrong here? Edit: This question seems to originate from Stewart & Tall's ""Algebraic Number Theory and Fermat's Last Theorem"" (4th edition), where it appears as Ex. 11.6 (p. 199).","['number-theory', 'cyclotomic-fields', 'roots-of-unity']"
4871877,Is there a standard name for the boundary of a cube?,"A distinction is commonly made between a ball (solid) and a sphere (the boundary of a ball). This distinction is made in other dimensions as well (e.g. circle versus disc, in 2D). From what I've seen trying to research this online, the word ""cube"" is usually used to refer to the soild. Is there any standard name for just the boundary of a cube? The only thing that came to mind was ""box"" but that both doesn't seem to be a standard term, and might intuitively refer to the boundary of any rectangular prism. (My question extends to squares as well, i.e. is there a standard name for just the boundary of a square? And a similar question could be asked in reverse about the torus, where that name is usual used to refer to only the boundary.) If there is no standard name, my follow-up question would be: why do we have distinct names for spheres and balls, but not other common solids? Just historical accident? (Is the distinction made in languages other than English?)","['convention', 'geometry', 'terminology']"
4871914,Integration by parts does not work for this complex integral. Why?,"There is a longer integral for which integration by parts $\displaystyle\int udv=uv-\int vdu$ was attempted as it came across in research: $$
\frac i{2\pi}\int_0^{2\pi}\underbrace{\ln\left(1+\frac{e^{-i t}+1}a\ln\left(1-\frac1be^{\frac{e^{it}+1}a}\right)\right)}_u
\;d\bigg[\underbrace{\ln\left(e^\frac{e^{it}+1}c-b\right)}_v\bigg] \tag{0}
$$ as it has the below problem, but here is a simpler one to better get the point across $$
\frac1{2\pi}\int_0^{2\pi}
\ln(e^{it}+a)\;
d\left[\ln\left(e^{e^{it}}-2\right)\right] \tag{1}
$$ Integrating $(1)$ by parts should give: $$\frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a)
\;d\left[\ln\left(e^{e^{it}}-2\right)\right]
\\
 = \underbrace{\left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi}}_\text{should equal zero when plugging in directly}-\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right) 
\;d\left[\ln(e^{it}+a)\right] 
\ .
\tag2
$$ Directly substituting, we should get $\left[\ln(e^{it}+a)\ln\left(e^{e^{it}}-2\right)\middle]\right|_0^{2\pi} =0$ . Checking numerically shows $(2)$ to be false. However, instead, we numerically get $\ln(a-1)i$ instead of $0$ : $$
\frac1{2\pi}\int_0^{2\pi}\ln(e^{it}+a)
\;d\left[\ln\left(e^{e^{it}}-2\right)\right]
\\
=\ln(a-1)i -\frac1{2\pi}\int_0^{2\pi}\ln\left(e^{e^{it}}-2\right) d\left[\ln(e^{it}+a)\right] \tag{3}
\ .
$$ What is the correct way to find the extra $[uv]|_a^b$ term, like $\ln(a-1)i$ here?","['integration', 'complex-integration', 'solution-verification', 'constants', 'complex-numbers']"
4871940,Derivative of Discrete Mutual Information,"Given a fixed channel $p(y|x)$ ,  and denoting the discrete input pmf $p(x)$ ,  and the corresponding output pmf by $p(y)$ ,  prove that: $$
 \frac{\partial I(X;Y)}{\partial p(x)} = I(X=x;Y) - \log{e}
$$ where, $$
  I(X=x;Y) =  \sum_{y} p(y|x) \log{\left(\frac{p(y|x)}{p(y)}\right)}
 $$ Note $^*$ : the $\log$ is base 2 here. Solution (thanks to @stochasticboy321): We can write $$
I(X;Y) = \sum_{i}p(x_i)I(X=x_i;Y) \Longrightarrow  \frac{\partial I(X;Y)}{\partial p(x_j)} = I(X=x_j;Y) + \sum_{i} p(x_i)\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}.
$$ But, $$ 
\begin{aligned}
\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}} &= \frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i) \log{\left(\frac{p(y_k|x_i)}{p(y_k)}\right)}\\
&= \frac{\partial }{\partial p(x_j)}\left(\sum_{k} p(y_k|x_i) \log{(p(y_k|x_i)) - \sum_{k} p(y_k|x_i)\log [p(y_k)]}\right) \\
&= -\frac{\partial }{\partial p(x_j)} \sum_{k} p(y_k|x_i)\log [p(y_k)] \\
&= - \sum_{k} p(y_k|x_i)\color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}}
\end{aligned}
$$ Where $p(y_k) =\sum_i p(y_k|x_i)p(x_i)$ . Let's continue, $$
\begin{aligned}
\color{blue}{\frac{\partial \log [p(y_k)]}{\partial p(x_j)}} &= \frac{\log e}{p(y_k)} \frac{\partial }{\partial p(x_j)} \sum_i p(y_k|x_i)p(x_i)\\
&= \frac{\log e}{p(y_k)} p(y_k|x_j) 
\end{aligned}
$$ Hence, $$
\begin{aligned}
\color{red}{\frac{\partial I(X=x_i;Y)}{\partial p(x_j)}}  &= -\log e \left[\sum_k \color{purple}{\frac{p(y_k|x_i)p(y_k|x_j)}{p(y_k)}} \right]\\
&= -\log e \left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right]
\end{aligned}
$$ Thus, $$
\begin{aligned}
\frac{\partial I(X;Y)}{\partial p(x_j)} &= I(X=x_j;Y)  -\log e \sum_{i} p(x_i)\color{red}{\left[\sum_k \color{purple}{\frac{p(x_i|y_k)p(y_k|x_j)}{p(x_i)}} \right]}\\
&= I(X=x_j;Y)  -\log e  \sum_i p(x_i|y_k) \sum_k p(y_k|x_j) \\
&\stackrel{(a)}{=} I(X=x_j;Y)  -\log e  
\end{aligned}
$$ (a) The two sums add up to one since they are summations over legitimate probability space ( $i, \ k$ ).","['multivariable-calculus', 'mutual-information', 'information-theory']"
4871968,"How do you find the multivariable limit $\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x +\sqrt y }$","$\lim_{(x,y)\to(0,0)}\frac{xy}{\sqrt x+\sqrt y}$ considering domain $\{(x,y) \in \mathbb{R}^2 : x,y \ge 0, (x,y) \ne 0\}$ I tried using polar coordinates, but the theta function is unbounded. I also tried using the sandwich theorem, but could not find appropriate bounds. How do I approach this question","['limits', 'multivariable-calculus']"
4871976,Notational inconsistency for subsets and strict subsets,"I am reading ""Introduction to Topology"" by Gamelin and Greene (2nd edition) and I am confused by what appears to be an inconsistency in notation for subsets and strict subsets. The image below shows page 61 from this book. From the solid red highlighted expressions, it appears that $A \subset B$ means $A$ is a subset (not strict) of $B$ . However, from the blue dashed highlighted expression, it appears that $A \subseteq B$ means $A$ is a subset (not strict) of $B$ . Is this a typo in the book, or have I misunderstood something? Frustratingly, they don't define the symbols $\subset$ and $\subseteq$ anywhere. UPDATE: Thanks to the helpful comments and answer, it's clear that there is a notational inconsistency and the authors mean ""subset"" (not ""strict subset"") with both $\subset$ and $\subseteq$ on pg 61. But, is it safe to assume they always mean ""subset"" (and never have a use for ""strict subset"") through the entire book? I'm new to topology, and there are other places where I cannot figure out whether they mean ""strict subset"". Is it likely that the entire book would never have a use for ""strict subset""?","['elementary-set-theory', 'notation']"
4871983,"Evaluating $\int_0^{\pi } \frac{\sin (n \sigma )}{(a-\cos \sigma )^2} \, d\sigma$","What is the formula for $$\int_0^{\pi } \frac{\sin (n \sigma )}{(a-\cos \sigma )^2} \, d\sigma$$ where $ a>1 $ and $n$ is a positive integer? To evaluate, I tried to replace $a$ with $\cosh\xi $ in the integral $$\int_0^{\pi } \frac{\sin (n \sigma )}{(\cosh\xi-\cos \sigma )^2} \, d\sigma$$ Please note that I already obtained the formula for  a related integral $$\int_0^{\pi } \frac{\cos (n \sigma )}{(a-\cos \sigma )^2} \, d\sigma
= \frac{\pi(n \sqrt{a^2-1}+a)}{(a^2-1)^{3/2} (\sqrt{a^2-1}+a)^n}
$$","['integration', 'trigonometric-integrals']"
4871984,How do we know that common rearrangement proofs of the Pythagorean theorem work for any right triangle?,"I’m a little bit puzzled by geometrical proofs, like the common algebraic proof for the Pythagorean theorem listed Wikipedia's ""Pythagorean theorem"" entry . I understand the idea of arranging the right triangles and the area $c^2$ in a neat way to form another square and writing the area of the new square in different terms and going from there. But I’m a little confused on how you actually know that the right triangle you draw isn’t just the one special right triangle with which you can actually form such a square. How do we know this way of rearranging the pieces works for any other right triangle?",['geometry']
4872009,sufficient condition for a triangle to exist (and fibonacci numbers),"Let $n\in\mathbb{N}\setminus\{1,2\}.$ Let $a_1,a_2,\ldots,a_n$ be $n$ (not necessarily distinct) real numbers each in the interval $(1,F_n),$ where $F_n$ is the $n^{\text{th}}$ Fibonacci number. Show that we can pick three numbers $a_i,a_j,a_k$ from the $n$ real numbers such that there exists a triangle with lengths $a_i,a_j$ and $a_k.$ I initially thought of using some sort of pigeonhole principle, but that doesn't seem to work. I have no idea how to proceed. Of course, the triangle inequality is key here, but like I said, I don't know how to use it. For $n=3$ it is trivial. We have $a_1,a_2,a_3\in(1,2).$ WLOG, assume $1<a_1\leq a_2\leq a_3<2.$ It is obvious that $a_2+a_3>a_1.$ It is also obvious that $a_3+a_1>a_2.$ We need to show that $a_1+a_2>a_3.$ Suppose not. Then, $a_1+a_2\leq a_3.$ But then, the LHS is greater than $2$ while the RHS is less than $2.$ A contradiction. Maybe this approach can be generalised? I don't know.","['contest-math', 'fibonacci-numbers', 'combinatorics', 'triangles']"
4872022,Can a smooth curve have a segment of straight line?,"Setting: we are given a smooth curve $\gamma: \mathbb{R} \rightarrow \mathbb{R}^n$ Informal Question: Is it possible that $\gamma$ is a straight line on $[a,b]$ , but not a straight line on $[a,b]^c$ ? Formal Question: It is possible that $\gamma''(t)=0$ for all $t\in [a,b]$ , while $\gamma''(t)\neq 0$ for some $t\not\in [a,b]$ ? The motivation for me to ask this question is that the textbook we use in our geometry class discusses only smooth curves on bounded open intervals. While I know that the curvature $\gamma''$ can be zero on a point (for instance: $\gamma(t)=(t,\sin t)$ has zero curvature on $\{n\pi:n\in\mathbb{Z}\}$ ), I can not come up with an example of a smooth curve $\gamma$ such that $\gamma''$ is $0$ on some interval $(a,b)$ . I think such an example if exists will be interesting to see in GGB, but I failed to come up with one due to my inexperience. Thanks for any help.","['curves', 'smooth-functions', 'geometry', 'differential-geometry']"
4872139,Logarithmic Equation Involving Trigonometric Functions,Solve the following equation in real numbers: $\log_2(\sin x) + \log_3(\tan x) = \log_4(\cos^2 x) + \log_5(\cot x)$ My approach: $\log_2(\sin x) + \log_3\left(\frac{\sin x}{\cos x}\right) = \log_2(\cos x) + \log_5\left(\frac{\cos x}{\sin x}\right)$ Now the equation can be written as: $\log_2(\sin x) + \log_5(\sin x) + \log_3(\sin x) = \log_2(\cos x) + \log_5(\cos x) + \log_3(\cos x)$ How can I continue my proof?,"['trigonometry', 'functions', 'logarithms']"
4872154,Can GAP compute this 16-dimensional representation of AlternatingGroup(6)?,"I am interested in a particular 16-dimensional representation of $A6$ , the alternating group on 6 things. I first construct an amalgam, gamma, of two copies of SymmetricGroup(4): F:=FreeGroup([""s1"",""s2"",""s3"",""t1"",""t2"",""t3""]); 
AssignGeneratorVariables(F); 
rel:=Union(
[s1^2,s2^2,s3^2,s1*s3*s1^-1*s3^-1,(s1*s2)^3,(s2*s3)^3],
[t1^2,t2^2,t3^2,t1*t3*t1^-1*t3^-1,(t1*t2)^3,(t2*t3)^3],
[s1*s3*(t1*t3)^-1, s1*(t2*t3*t1*t2)^-1, s2*s3*s1*s2*t1^-1]);
gamma:=F/rel;  #an amalgam of two SymmetricGroup(4) Next, I find an epimorphism from gamma onto $A6$ and take the kernel, $K$ : A6:=AlternatingGroup(6);
QA:=GQuotients( gamma, A6);
f:=QA[1]; #an epimorhism from gamma onto A6
K:=Kernel(f);
GeneratorsOfGroup(K);
AbelianInvariants(K);  # the abelianization of the kernel K of f is free abelian of rank 16
Size(AbelianInvariants(K)); Note that the abelian quotient, $K_{ab}$ , of $K$ is free abelian of rank 16. Now A6 acts (up to inner automorphisms) on $K$ and $K_{ab}$ . For instance, the generators of A6 act on K.1 as follows pre:= List(GeneratorsOfGroup(A6),  a -> PreImagesRepresentative(f,a) );
act:=List(pre, u -> MappingByFunction(K,K, x -> x^u));
List(act, a -> a(K.1)); I would like to know the 16-dimensional representations of $A6$ given by the action of $A6$ on $K_{ab}$ or $K_{ab} \otimes Z/2$ . My question is: Can GAP compute the $(16 \times 16)$ -matrices of this representation of $A6$ (over the integers or the integers mod 2)?","['gap', 'group-theory', 'representation-theory']"
4872167,Torsion-free covariant derivative,"Let $\nabla$ be an affine connection on a smooth manifold $\mathcal M$ . We say $\nabla$ is torsion-free if for any vector fields $X, Y$ on $\mathcal M$ , we have \begin{equation}
\nabla_X Y - \nabla_Y X = [X, Y],
\end{equation} where $[X, Y] = X \circ Y - Y \circ X$ is the Lie bracket of $X$ and $Y$ . Is it equivalent to state the torsion-free property as \begin{equation}
\nabla_X Y = X \circ Y
\end{equation} for all vector fields $X, Y$ ? Why or why not?","['vector-fields', 'derivatives', 'smooth-manifolds', 'differential-geometry']"
4872171,When is cdf $F_{X_1+\dots+X_n}(c)$ of sum of iid zero mean random variables decreasing in sample size $n$?,"Let $X_1, X_2, \dots$ be a sequence of i.i.d. random variables with mean zero (e.g., $N(0,1)$ ). Let $n > m$ and $c \geq 0$ . I want to show that $$ P\left(\sum_{i=1}^n X_i \leq c \right) \leq P \left( \sum_{i=1}^m X_i \leq c \right).$$ In view of existing concentration bounds like Hoeffding's inequlity which scale with the the length of the sequence, i.e. $n$ and $m$ , I would think that the above statement should hold. Edit: Since it was pointed out that this doesn't hold for specific cases where $n$ and $m$ are small and the distribution of $X_i$ is discrete, assume that $X_1, X_2, \dots$ are Gaussian and $n$ sufficiently large.","['concentration-of-measure', 'probability-theory', 'probability']"
4872180,how to evaluate $ \int_0^1 \frac{\arctan \left(x \pm \sqrt{x^2+1}\right)}{x+1} d x $,"how to evaluate $$
\int_0^1 \frac{\arctan \left(x \pm \sqrt{x^2+1}\right)}{x+1} d x
$$ Denote: \begin{align*}
I &= \int_0^1 \frac{\arctan \left(x+\sqrt{x^2+1}\right)}{x+1} \,dx \\
J &= \int_0^1 \frac{\arctan \left(x-\sqrt{x^2+1}\right)}{x+1} \,dx \\
\left(x+\sqrt{x^2+1}\right)\left(x-\sqrt{x^2+1}\right) &=-1 \Rightarrow\left(x-\sqrt{x^2+1}\right)=\frac{-1}{\left(x+\sqrt{x^2+1}\right)} \\
\Rightarrow \arctan \left(x-\sqrt{x^2+1}\right) &=-\arctan \left(\frac{1}{x+\sqrt{x^2+1}}\right) \\
\end{align*} \begin{align*}
&I+\int_0^1 \frac{\arctan \left(x+\sqrt{x^2+1}\right)+\arctan \left(x-\sqrt{x^2+1}\right)}{x+1} \,dx \\
= &\int_0^1 \frac{\arctan \left(\frac{x+\sqrt{x^2+1}+x-\sqrt{x^2+1}}{1-\left(x+\sqrt{x^2+1}\right)\left(x-\sqrt{x^2+1}\right)}\right)}{x+1} \,dx \\
=& \int_0^1 \frac{\arctan \left(\frac{2x}{1-\left(x^2-\left(x^2-1\right)\right)}\right)}{x+1} \,dx=\int_0^1 \frac{\arctan (x)}{x+1} \,dx
\end{align*}","['integration', 'calculus', 'definite-integrals']"
4872208,"prove a trigonometric polynomial has no more than $2n$ disctinct roots in $[0,2\pi)$","Prove the nonzero polynomial $$T(\theta)=a_0+\sum\limits_{k=1}^{n}(a_k\cos
k\theta+b_k\sin k\theta)$$ has no more than $2n$ distinct roots on $[0,2\pi).$ My attempt: From $\cos k\theta=\dfrac{e^{ik\theta}+e^{-ik\theta}}{2}$ and $\sin k\theta=\dfrac{e^{ik\theta}-e^{-ik\theta}}{2i}$ , $T$ can be rewritten as $T(\theta)=e^{-ni\theta}p(e^{i\theta})$ , with $p$ is a polynomial, $degp\leq 2n$ . $\Rightarrow T(\theta)=0\iff e^{-ni\theta}p(e^{i\theta})=0\iff p(e^{i\theta})=0$ I know that $p(z)=0$ has no more than $2n$ roots in $\mathbb{C}$ , but how can I argue that $p(e^{i\theta})=0$ also has no more than $2n$ roots in $[0,2\pi)$ ?","['complex-analysis', 'trigonometry']"
4872250,MGF dominated by an exponential function,"Let $X$ be a mean-zero random variable whose moment generating function is bounded by a symmetric exponential function, e.g: $$
\mathbb{E}[e^{\lambda X}] \leq \exp(c|\lambda|)
$$ with $c > 0$ . Question :
Is it true that $X$ must be bounded ? I asked one of my professors this and he told me it was the case, but I am not sure if it is true and if so how it can be proved. For proofs like this it seems like one would taylor expand both sides and take $\lambda$ to $0$ to get some sort of bounds, but I don't think this method works here because $e^{c|\lambda|}$ decays slower than $e^{c\lambda^2}$ , and the variables for which the second holds are the sub-gaussians, which of course include some unbounded variables (e.g. the usual Gaussian). I would imagine instead one should look at $\lambda$ large, but I am not sure how to carry out the argument.","['moment-generating-functions', 'probability']"
4872409,Confusion about atan vs atan2,"I have the following function: $$f(\omega) = \arctan\left(\frac{-\omega\cdot R / L}{-w^2 + 1/(C\cdot L)}\right)$$ When I try to plot it ( atan(f(w)) ), I get the following: This is clearly not an atan plot. I investigated a bit and found atan2 . When I use it instead ( atan2(-w*R/L, -w**2 + 1/(C*L)) ), I obtain: That does look like a atan plot. My question is : is it correct to provide numerator and denominator as first and second arguments of atan2 as I did? Also, about atan2 , my understanding is that the tan function doesn't have information about the sign of cos and sin , so it can't know if the angle comes from the first or third quadrant if the result is positive, or from the second or fourth quadrant if the result was negative. I don't understand very well why is this; how can not knowing the angle quadrants lead to the 'wrong' plot above? SageMath code to test the above R = 100
L = 1.5e-3
C = 1.6e-9

abs_H(w) = 1/(C*L) / sqrt( (-w**2 + 1/(C*L))**2 + (R/L*w)**2 )
# phase_H(w) = atan( -(w*R/L) / (-w**2 + 1/(C*L)) ) * 180/pi 
phase_H(w) = atan2( -(w*R/L) , (-w**2 + 1/(C*L)) ) * 180/pi      # Notice atan2; otherwise it doesn't work

p = plot(abs_H, w, (10, 1e7), scale='loglog')
q = plot(phase_H, w, (10, 1e7), ymin=-180, scale='semilogx')
show(p)
print('')
show(q)
print('')","['python', 'sagemath', 'computational-mathematics', 'limits', 'trigonometry']"
4872412,Expected number of picks from normal distribution such that the sum exceeds 𝑟? Does the value converge?,"I have found the similar question for Uniform Distribution i.e, U(0,1) but not for Normal distribution, So basically the problem is, using a standard normal distribution i.e., 𝑋∼𝑁(𝜇, $𝜎^2$ )
random number generator, keep generating and summing until the sum is greater than 𝑟. How to find the expected number of moves you will take. I am not even sure the value is defined. I was modelling the equation like here - Link , and now the equation also has Normal distribution term in the right hand side while picking a number, this converts it to a form where it appears, convolution theorem might be used, tho I am not able to proceed ahead or gain any useful breakthrough. Welcome to any other methods as well","['laplace-transform', 'normal-distribution', 'expected-value', 'stopping-times', 'probability-theory']"
4872420,Solving equation involving shifts of the unknown function.,"Let's consider the following equation: $$
a_2.f(t+2) + a_1.f(t+1) + a_0.f(t+0) = g(t)
$$ where $a_0,a_1,a_2$ are non zero reals and $g(t)$ is a known function. Although it looks like an ordinary differential equation of order 2, the ""changing"" parts on the left part of this equation doesn't involve derivatives but rather a shift on the main variable $t$ . Question(s): what is the name of such an equation and what kind of methods do we have to find the unknown function $f(t)$ ?","['functional-equations', 'recurrence-relations', 'ordinary-differential-equations']"
4872427,Background for Gross-Zagier paper,"I have been reading the paper ""Heegner points and derivatives of $L$ -series"" by Gross and Zagier. Link to the paper. In section III of the paper, they use intersection theory to express a local height at a non-archimedean place in terms of an intersection product. I have been having trouble understanding the algebraic geometry in this section. So far I have been reading chapters 7 and 9 of Qing Liu's book ""Algebraic Geometry and Arithmetic Curves"", but it seems as though I will need to learn scheme theory in more technical detail in order to get anything out of it. I have read through the non-archimedean argument in Gross-Zagier's paper ""On singular moduli"", which can be regarded as a special case of the ""Heegner points and derivatives of $L$ -series"". This has helped motivate the use of Hom-sets between points of $X_0(N)$ , but doesn't seem to require the intersection theory arguments in the latter paper (maybe this is present in the form of Deuring's lifting theorem). Finally I will state my question. Where should I look in order to get the necessary background to understand these calculations?","['elliptic-curves', 'reference-request', 'algebraic-geometry', 'modular-forms', 'intersection-theory']"
4872522,Inequality of entropies for Bernoulli plus Gaussian,"Question: Given $X\sim\text{Bernoulli}(\alpha)$ , $Y\sim\mathcal{N}(0,1)$ , and two non-random constants $C_1$ and $C_2$ such that $C_1>C_2$ . What can we say about the inequality between the two differential entropies $H(C_1X+Y)$ and $H(C_2X+Y)$ ? I.e., is it true that $$
H(C_1X+Y)>H(C_2X+Y).
$$ Fact: We know from here that differential entropy is shift invariant but is affected by scaling. Attempt: I am aware that for a single random variable $X$ alone, the entropy $H(X)$ is shift invariant. Hence we can rewrite $$
H(C_1X+Y)=H(\epsilon X+C_1X+Y),
$$ for some $\epsilon$ . However, this is not a simple shift because we are shifting by a fraction of a random variable.","['statistics', 'entropy', 'information-theory', 'inequality', 'probability']"
4872532,Orthant probability,"Let $(X_i)_{i \in \mathbb{N}} \sim \mathcal{N}(0,1)$ , where all variables are i.i.d. For the random vector $$\mathbf{X} = \left [ X_1, \ \ \ \ X_1 + X_2, \ \ \ \  X_2 + X_3, \ \ \ \  \cdots \ \ \ \   X_{n-2} + X_{n-1}, \ \ \ \ X_{n-1} \right ]^T,$$ how can I find $\mathbb{P}(\mathbf{X} \geq 0)?$ In other words, this is the probability that all components are non-negative. For small $n$ , the probabilities (although tedious) are doable. I was wondering if there is a general form, or bounds, for this probability. I know that orthant probabilities for $n >5$ do not have a closed form, but in this specific case I believe something can be done, since this is not a classic orthant problem. The tricky part about this is that the covariance matrix is non-invertible, so I cannot integrate joint densities. I would appreciate a detailed answer (if one exists). Thanks!","['probability-theory', 'probability']"
4872602,For any $f:\mathbb{Z_+}\to \mathbb{Z_+}$ there exist $f(a)\le f(a+b)\le f(a+2\cdot b)$,"I am preparing for the USAJMO, however I got stuck on the following question. I would really appreciate any help/hint. For any function ( not necessarily having Bijectivity ) $f:\mathbb{Z^+} \to \mathbb{Z^+}$ : Prove that there exist positive integers $a$ and $b$ so that $f(a) \le f(a+b) \le f(a+2 \cdot b)$ . I can prove it pretty easily if $f$ is a monotonically increasing/decreasing function. However I have no clue how to prove the general case where $f$ is not necessarily monotonic. Any help is appreciated. OP thinking : A point I already got : monotonically decreasing function must have a lower bound, .... so if it is eventually from some value $f(x)$ must be the lowest value onwards .... But I am not so sure when/if function $f$ values change up/down all the time ....","['contest-math', 'elementary-number-theory', 'functions', 'abstract-algebra', 'natural-numbers']"
4872619,The relation between the eigenvalue of a Hermitian matrix and the block matrix that composed by it real and imaginary part,"Recently I am reading a paper . In their ""Proof of Lemma 1"" on page 24, they have: $$\lambda_+(\mathbf{Q})=2\lambda_+(\tilde{\mathbf{Q}})$$ where $\mathbf{Q}$ is a Hermtian matrix, $\tilde{\mathbf{Q}}=\frac{1}{2}\left[\begin{array}{cc}
\operatorname{Re}\{\mathbf{Q}\} & -\operatorname{Im}\{\mathbf{Q}\} \\
\operatorname{Im}\{\mathbf{Q}\} & \operatorname{Re}\{\mathbf{Q}\}
\end{array}\right]$ is a real symmetric matrix, $\lambda_+ = \max\{\lambda_{\max}(\mathbf{-Q},0)\}$ , and $\lambda_{\max}$ denotes the maximum eigenvalue. I haven't figured out why it's that.","['matrices', 'hermitian-matrices', 'skew-symmetric-matrices', 'matrix-decomposition']"
4872621,ch. 8.3 Exercise 1 in Cohomology of Number Fields,"Let $k$ be a number field, $S$ a set of places of $k$ , $k_S$ the maximal extension of $k$ unramified outside $S$ , $\mathcal{O}_S$ the subring of $k_S$ with $\nu_{\mathfrak{p}}(\alpha)\geq 0$ for all $\mathfrak{p}\not\in S$ , and $G_{S}$ the galois group $\text{Gal}(k_S/k)$ . I am considering the following exercise at the end of chapter 8.3 in Cohomology of Number Fields. Show the isomorphism $H^2(G_S,\mathcal{O}_S^\times)\cong Br(k_S/k)$ My attempt was to consider the following exact sequence $$0\longrightarrow k_S^\times \longrightarrow I_{k_S}\longrightarrow C_{k_S}\longrightarrow 0$$ Where $I_{k_S}$ and $C_{k_S}$ are the Ideles and Idele class group of $k_S$ . (Note that if $k_S$ is an infinite extension, these are simply the Galois invariance of the absolute Ideles and Idele class group.) We take the cohomology long exact sequence of this exact sequence and obtain at $H^2$ : $$0\simeq H^1(G_S, C_{k_S})\longrightarrow H^2(G_S,k_S^\times)\simeq Br(k_S/k)\longrightarrow H^2(G_S,I_{k_S})\longrightarrow H^2(G_S, C_{k_S})$$ This gives an isomorphism $Br(k_S/k)\simeq \ker(H^2(G_S,I_{k_S})\to H^2(G_S, C_{k_S}))$ . Now suppose $k=\mathbb{Q}$ and $S=\{3,\infty\}$ , so $3^\infty|\#G_S$ . Since $C$ is a formation module (defn. 3.1.8 in Cohomology of Number Fields ) we have $H^2(G_S,C_{k_S})\simeq \frac{1}{\#G_S}\mathbb{Z}/\mathbb{Z}$ . On the other hand, for $H^2(G_S,I_{k_S})$ we have $$H^2(G_S,I_{k_S})(3)\simeq H^2(k,I)(3)\simeq \bigoplus_{\mathfrak p} H^2(k_{\mathfrak p})(3)\simeq \bigoplus_{\mathfrak p} \mathbb{Q}_3/\mathbb{Z}_3$$ by propositions $8.1.8$ and $8.1.7$ of the same book. (Note that we are using $A(p)$ to denote the $p$ -power torsion of $A$ .) Altogether, this yields $$Br(k_S/k)(3)=H^2(G_S,k_S^\times)(3)\simeq \ker \left(\bigoplus_{\mathfrak p} \mathbb{Q}_3/\mathbb{Z}_3\longrightarrow \mathbb{Q}_3/\mathbb{Z}_3\right)$$ where the map is given by summation. Now by 8.3.11 of the same book, we have $$H^2(G_S,\mathcal{O}_S^\times)(p)\simeq \ker\left(\bigoplus_{\mathfrak p\in S\setminus S_\infty} \mathbb{Q}_3/\mathbb{Z}_3\longrightarrow \mathbb{Q}_3/\mathbb{Z}_3\right)=\ker(\mathbb{Q}_3/\mathbb{Z}_3\to \mathbb{Q}_3/\mathbb{Z}_3)\simeq 0.$$ So these are two formulas, one for the $3$ -part of $Br(k_S/k)$ and another for the $3$ -part of $H^2(G_S,\mathcal{O}_S^\times)$ . One is trivial and the other is decidedly not trivial, so they cannot be isomorphic. Here is another approach I took, which similarly suggested that the groups are not isomorphic, or at least not canonically isomorphic. We have a canonical embedding $\mathcal{O}_S^\times\to k_S^\times$ , and we would hope the isomorphism of their Brauer groups is induced by this embedding. To that end, suppose for simplicity that $k=\mathbb{Q}$ and $S=\{2,5,7,\dots,\infty\}$ . we consider the following exact sequence $$0\longrightarrow \mathcal{O}_S^\times \longrightarrow  k_S^\times \longrightarrow  \prod_{\mathfrak{p}|3} \mathfrak{p}^{\mathbb{Z}}=A$$ where the product is over the primes $\mathfrak{p}$ lying over $3$ and the last map need not be surjective. To turn this into a short exact sequence, we would like to determine the image of the last map. To do this, fix a prime $\mathfrak{p}^*$ lying over $3$ , and let $D\subseteq G_S$ be its decomposition group. The $A$ can be described as the set of functions $f:G_S\to \mathbb{Z}$ such that $f(\sigma\tau)=f(\tau)$ for all $\sigma \in D$ . We identify each prime $\mathfrak{p}$ in the product with a coset of $G/H$ , and then for $a\in A$ we set $f_a(H\sigma)=\nu_{(\mathfrak{p}^*)^\sigma}(a)$ . To determine the image of $k_S$ in $A$ , note that anything in the image must be a continuous function. To see this, observe that any $\alpha \in k_S$ is in a Galois subextension $k_S/K/k$ , and $f_\alpha$ then factors through the finite quotient $\text{Gal}(K/k)$ and is therefore continuous by properties of the Krull topology. Furthermore, all such continuous functions are possible: if $f\in A$ is continuous, then it factors through some finite quotient of $G_S$ , and we can pass to the corresponding extension of $k$ to find a suitable $\alpha$ . With this description of the image of $k_S^\times$ in $A$ in mind, it becomes clear that we have constructed an isomorphism $$k_S^\times/\mathcal{O}_S^\times\simeq \text{Ind}^D_{G_S}(\mathbb{Z})$$ since the collection of such continuous functions is precisely the definition of the induced module. Taking the long exact sequence in cohomology, we obtain $$H^1(G_S,\text{Ind}^D_{G_S}(\mathbb{Z}))\longrightarrow H^2(G_S,\mathcal{O}_S^\times)\longrightarrow H^2(G_S,k_S^\times)\longrightarrow  H^2(G_S,\text{Ind}^D_{G_S}(\mathbb{Z}))\longrightarrow H^3(G_S,\mathcal{O}_{S}^\times) $$ Computing the first and fourth term in this sequence, we obtain $$H^1(G_S,\text{Ind}^D_{G_S}(\mathbb{Z}))\simeq H^1(D,\mathbb{Z})\simeq H^1(\hat{\mathbb{Z}},\mathbb{Z})\simeq \varinjlim H^1(\mathbb{Z}/n\mathbb{Z},\mathbb{Z})\simeq 0$$ $$H^2(G_S,\text{Ind}^D_{G_S}(\mathbb{Z}))\simeq H^2(D,\mathbb{Z})\simeq H^2(\hat{ \mathbb{Z}},\mathbb{Z})\simeq H^1(\hat{\mathbb{Z}},\mathbb{Q}/\mathbb{Z})\simeq \mathbb{Q}/\mathbb{Z}.$$ Taking the $p$ -part of this exact sequence for $p\neq 3$ and using 8.3.11, we obtain $$0\longrightarrow H^2(G_S,\mathcal{O}_S^\times)(p)\longrightarrow H^2(G_S,k_S^\times)(p)\longrightarrow \mathbb{Q}_p/\mathbb{Z}_p\longrightarrow 0.$$ Once again, we find that $H^2(G_S,\mathcal{O}_S^\times)$ is only a subobject of $H^2(G_S,k_S^\times)$ , and similarly the failure for it to be an isomorphism comes from the primes not in $S$ . The ideas here apply more generally for other $S$ and $k$ and you still get a nontrivial cokernel of the inclusion in those cases. I do not immediately see a flaw with either of these arguments, and the fact that they both show the exercise being false makes me think it might just be a mistake in the book, but I am far too aware of my shortcomings to believe this, so I am hoping someone can point out a mistake in these arguments.","['algebraic-number-theory', 'number-theory', 'galois-cohomology', 'group-cohomology', 'class-field-theory']"
4872654,How to find the time period of the sum of two waves with different frequency,"I'm dealing with a few sin waves that represent music. An example is the following f(x)=sin( $\frac{1.47x}{440}$ )+sin(x) How would I represent the time period of this superposed wave? Also if the frequency is possible to find (to my knowledge it isn't), can it be represented with a formula?","['functions', 'wave-equation']"
4872707,Which is the correct definition of covectors?,"Some says covectors are linear map that maps $ V \mapsto R $ (which means it's just a row vector considering vectors are $ n $ x $ 1 $ matrix and mapping is matrix multiplication), while some say it's a certain value whose components follow $ v'_i = \frac{\partial x^j}{\partial x'^i} v_j $ under coordinate transformation from $ x $ coordinate to $ x' $ coordinate. Let's see gradient for example. $ \nabla $ itself is not a map $ \nabla : V \mapsto R $ , but divergence, $ \nabla \cdot $ is. So $ \nabla $ is not a covector of first definition. However, considering $ \nabla $ 's components are $ \frac{\partial}{\partial x^i} $ , they actually follow the transformation rule by $ \frac{\partial x^i}{\partial x'^j} \frac{\partial}{\partial x^i} = \frac{\partial}{\partial x'^j} $ . So it's a covector of the second example. Which one's right? Are they actually the same definition and I'm misunderstanding something or are they just different things I'm talking about?","['covariance', 'vectors', 'tensors', 'definition', 'differential-geometry']"
4872784,"Are there general solutions to quadratic, 2D, continuous, time-invariant dynamical systems?","I am a bit new to dynamical systems and don't know my way around terminology, so have had a hard time answering this for myself. I know the basics of theory for 2D linear, time-invariant systems, i.e., $$
\dot{x}=a_1x+a_2y \\
\dot{y}=b_1x+b_2y
$$ I know that there are explicit exponential solutions, and a ton of theory about fixed points and stability. I'm wondering if there is any equivalent theory out there for the next higher-degree systems, i.e., $$
\dot{x}=a_1x+a_2y+a_3x^2+a_4y^2+a_5xy \\
\dot{y}=b_1x+b_2y+b_3x^2+b_4y^2+b_5xy
$$ I get that this is non-linear, so not nearly as simple. But are there any general solutions or time-scales/eigenvalue-equivalents? Is there a standard approach to looking at this as pieced-together linear approximations? Is there a name for this kind of system or its study that I can look up? Thanks very much!","['quadratics', 'ordinary-differential-equations', 'nonlinear-system', 'nonlinear-dynamics', 'dynamical-systems']"
4872800,Showing $ \frac{2^j(2k-j)!}{(k-j)!}=\sum_{l=j}^k\frac{2^ll(2k-l-1)!}{(k-l)!} $,"For any given $j,k\in\mathbb{N}$ , with $j\leq k$ , how is it possible to prove the following equivalence? $$
\frac{2^j(2k-j)!}{(k-j)!}=\sum_{l=j}^k\frac{2^ll(2k-l-1)!}{(k-l)!}
$$ My attempt is: $$
\frac{2^j(2k-j)!}{(k-j)!}=\sum_{l=j}^k\frac{2^ll(2k-l-1)!}{(k-l)!}\\
2^j\binom{2k-j}{k-j}k!=\sum_{l=j}^k\binom{2k-l-1}{k-l}l2^l(k-1)!\\\binom{2k-j}{k-j}k=\sum_{l=j}^kl2^{l-j} \binom{2k-l-1}{k-l}
$$ and then I don't know how to continue. Maybe there is some binomial formula for this sum, but I don't know.","['summation', 'factorial', 'binomial-coefficients', 'combinatorics', 'arithmetic']"
4872806,$f = X^{3} + 2$ irreducible in $\mathbb{F}_{49}[X]$.,"Question : Prove that $f = X^{3} + 2$ is irreducible in $\mathbb{F}_{49}[X]$ . Is $f$ irreducible in $\mathbb{F}_{7^{n}}$ for all even $n$ ? My attempt : Notice that $\deg(f) = 3$ , therefore $f$ is irreducible in $\mathbb{F}_{49}[X]$ if and only if $f$ has no roots. We will first look at $f$ in $\mathbb{F}_{7}[X]$ , since if $f$ would have roots in $\mathbb{F}_{7}$ then it would also have roots in $\mathbb{F}_{49}$ because it is a subfield. With checking we find that $f$ has no roots in $\mathbb{F}_{7}$ . Now let $\alpha \in \overline{\mathbb{F}_{7}}$ be a root of $f$ . Then, it follows that $\mathbb{F}_{7}(\alpha)$ is a finite field extension of $\mathbb{F}_{7}$ of degree 3. Then it follows that $\# \mathbb{F}_{7}(\alpha) = 7^{3}$ and by uniqueness of this field we have $\mathbb{F}_{7}(\alpha) \cong \mathbb{F}_{7^{3}}$ . Now, we clearly see that a field containing a root of $f$ must contain $\mathbb{F}_{7^3}$ , it follows directly that this root is not contained in $\mathbb{F}_{49}$ , and therefore $f$ is irreducible in $\mathbb{F}_{49}[X]$ . The polynomial $f$ is not irreducible over $\mathbb{F}_{7^{n}}$ for all even $n$ . Consider for example $n = 6$ , then since $3$ is a divisor of $6$ we have that $\mathbb{F}_{7^{3}} \subset \mathbb{F}_{7^{6}}$ is a subfield, and therefore the root of $f$ is contained in $\mathbb{F}_{7^{6}}$ . My concern : My main concern with my proof is if my approach of assuming that $\mathbb{F}_{7}(\alpha) \cong \mathbb{F}_{7^{3}}$ is correct, and whether I can directly say that such a root of $f$ can never be in $\mathbb{F}_{49}$ . If anybody can help me out or has suggestions I would appreciate that.","['field-theory', 'galois-theory', 'finite-fields', 'abstract-algebra']"
4872864,On asymptotics of certain sums of multinomial coefficients,"Given positive integers $n$ and $k$ , set $$ S_{n,k}=\sum_{\substack{a_1+a_2+\dots+a_k=2n\\ a_i \in 2\mathbb{N},\,i=1,\ldots,k}}\frac{(2n)!}{a_1!a_2!\dots a_k!},
$$ where $2\mathbb{N}=\{0,2,4,\ldots\}$ . According to the answers of Special sum of multinomial coefficients! there is no ""nice"" closed form expression for $S_{n,k}$ . My question is: How can one find the asymptotics of $S_{n,k}$ for fixed $n$ when $k \rightarrow \infty$ ? My thoughts so far: It is mentioned in the link above that the expression for $S_{n,k}$ resembles Sterling numbers of the second kind, so perhaps some approximation results for those numbers may be relevant.
Also, I did some numerical experimentation that seems to suggest that the naive guess $S_{n,k}\sim C_n \cdot k^n$ (where $C_n>0$ depends on $n$ only) is plausible.","['asymptotics', 'combinatorics', 'multinomial-coefficients', 'multinomial-distribution', 'probability']"
4872931,Showing that for every acute angle $x$ in a right triangle $\frac{1}{\sin x}+\frac{1}{\cos x}\ge 2\sqrt 2$ is always true,"The problem is to show that in a right-angle triangle with hypothenuse K and sides M and N, the inequality $\frac{K}{M}+\frac{K}{N}\ge 2\sqrt 2$ is always true. My approach: I tried to simplify the expression $\frac{K}{M}+\frac{K}{N}$ = $\frac{K(M+N)}{MN}$ , and since we have a triangle, this means that $K\ge M+N$ which makes us conclude that $$\frac{K(M+N)}{MN}\ge\frac{K^2}{MN}$$ , and since K is the hypothenuse, we get: $\frac{K(M+N)}{MN}\ge \frac{M^2+N^2}{MN}\ge 2$ by AM-GM. So this means that I got: $$\frac{K}{M}+\frac{K}{N}\ge 2$$ but couldn't reach the desired result which is $2\sqrt 2$ , so any help or another approach is much appreciated.","['inequality', 'trigonometry']"
4872936,Are these two function spaces identical?,"Let the function space $A$ denote all functions $f : [0, 1) \to [0, 1)$ such that, for some set $Z$ of Lebesgue measure zero, the derivative $f'$ exists on $[0, 1) \setminus Z$ and $|f'| = 1$ there. Let the function space $B$ denote all functions $f : [0, 1) \to [0, 1)$ such that for all $a < b$ in $[0, 1)$ , the portion $G_f(a,b) = \{(x,f(x)) ∈ \mathbb R^2 \mid a \le x < b\}$ of the graph of $f$ that lies above $[a, b)$ has Hausdorff $1$ -measure $H_1(G_f(a,b))$ equal to $\sqrt 2(b - a)$ . (Note that we do not assume functions to be continuous.) Does $A = B$ ? Edit: The following is false, as was shown to me by Christian Remling by an easy application of the Cantor function:
""It is clear to me that $A$ is a subset of $B$ ."" (Wrong!) So the real question is whether the reverse inclusion holds.","['measure-theory', 'geometric-measure-theory', 'real-analysis']"
4872976,Complex Differentiability Limit Definition,"I have $f(x+iy)=y^2$ . This satisfies the Cauchy-Riemann equations for all $z_0=(x,0)$ . Now I want to check whether $f$ is differentiable for all $(x,0)$ . So I use the definition, taking the limit as $h$ tends to $0$ , and letting $h=a+bi$ : $$\frac{f(z_0+h)-f(z_0)}{h}=\frac{f(x+a+bi)-f(x)}{a+ib}=\frac{b^2}{a+ib}$$ But how am I meant to compute the limit of $\frac{b^2}{a+ib}$ as $h$ tends to $0$ , which I suppose means $(a,b)$ tends to $(0,0)$ . Can someone help me do this using the definition please? Thanks.","['analysis', 'real-analysis', 'complex-analysis', 'calculus', 'derivatives']"
4872980,Is there a rigorous probability theoretic formulation of linear regression,"Let $(\Omega,\Sigma,P)$ be some probability space and let $Y: \Omega \to \mathbb{R} $ be a random variable. Let $x, \beta \in \mathbb{R}^n$ . In a linear model we assume something like this: $E(Y|x)=\beta^Tx$ . I know conditional expectation, where you condition on either an event $E \in \Sigma$ or on a sub- $\sigma$ -algebra $\Sigma' \subseteq \Sigma$ . As a special case of the latter, we can condition on another random variable. But the variable $x$ is not random, so how would you rigorously define $E(Y|x)$ ?","['linear-regression', 'statistics', 'probability-theory', 'probability']"
4873001,Find value of this sum,"Let $$\lim_{x\rightarrow 0}\frac{f^{}(x)}{x}=1$$ and for every $x,y \in \mathbb{R} $ we have: $$f(x+y)=f(x)-f(y)+ xy(x+y)$$ Now Find : $$\sum_{i=11}^{17}f^{\prime} (i)$$ I think this question is false because we have: Let $x=y$ then : $$f(2x) =2x^3$$ and $f(x)=\frac{x^3}{4}$ But $$\lim_{x\rightarrow 0}\frac{f^{}(x)}{x}\neq1$$","['limits', 'calculus', 'derivatives', 'summation']"
4873006,Are Stonean spaces a reflective subcategory of topological spaces with continuous open maps?,"I'm not talking about Stone spaces! A Stonean space is a compact Hausdorff extremally disconnected space i.e. a compact Hausdorff space such that for all open $U$ , its closure $\overline{U}$ is also open. I've been thinking about applications of regular open sets in topology, and I came up with the following: If $X$ is a topological space then the set $\text{RO}(X)$ of regular open sets of $X$ forms a complete Boolean algebra. This Boolean algebra induces a Stonean space $S(X)$ by Stone duality . Thus to any topological space we can assign a Stonean space $S(X)$ . If $X$ is a Stonean space, then $\text{RO}(X)$ is a Boolean algebra of clopen sets of $X$ , so that Stone duality tells us $S(X)$ gives us $X$ back. If $f:X\to Y$ is a continuous map, then $f^{-1}(\text{RO}(Y))\subseteq \text{RO}(X)$ is not true in general (see comments below). Thus I want to restrict to continuous open maps. In this case, if $U\in\text{RO}(Y)$ then $$\text{int }\overline{f^{-1}(U)} = \text{int }f^{-1}(\overline{U}) = f^{-1}(\text{int }\overline{U})$$ see also this post by Paul Frost. So $f^{-1}$ induces a map from $\text{RO}(Y)$ to $\text{RO}(X)$ . Then the Stone duality induces a map $S(f):S(X)\to S(Y)$ . Now, using this, does that mean that Stonean spaces are a reflective subcategory of topological spaces with continuous open maps?","['boolean-algebra', 'general-topology', 'category-theory']"
4873035,Is it possible to go over all lines of a grid with a pencil without lifting it or going over a drawn line?,"Is it possible to go over all lines of an infinite grid with a pencil without lifting it or going over a drawn line ? The pencil can cross through a segment already drawn but cannot go over an already drawn line. After doodling around, I have the feeling it is not possible.
If indeed it is not possible what demonstration exist of this result ?
If it is possible then can you show a way of drawing the grid ? Easily one sees that drawing a sub-grid of $m\times n$ is possible for all values of m and n, by drawing vertical lines and then horizontal lines, without removing the pencil. Here is an example of a sub-grid of $3\times 6$ .","['graph-theory', 'combinatorics', 'geometry', 'eulerian-path']"
4873119,Elementary proof that $(1+1/x)^x$ is increasing for $x \in \mathbb{R}_{>0}$,"All similar proofs I could find show that $(1+1/n)^n$ is increasing for positive integers values of $n$ only, or show that the derivative of $(1+1/x)^x$ is positive for all $x \in \mathbb{R}_{>0}$ . I'm looking for a proof that $x<y \implies (1+1/x)^x < (1+1/y)^y$ for all $x,y \in \mathbb{R}_{>0}$ that doesn't use the derivative of $a^x$ nor the derivative of $\log_a (x)$ My attempt so far: Let $x,y \in \mathbb{R}_{>0}$ such that $x<y$ . Let $$
\begin{align}
J&= \frac {\left(1+\frac{1}{y}\right)^y}{\left( 1+ \frac{1}{x} \right)^x}\\
&= \frac {(\frac{y+1}{y})^x(1+\frac{1}{y})^{y-x}}{(\frac{x+1}{x} )^x}\\ 
&= {(\frac{xy+x}{xy+y})^x(1+\frac{1}{y})^{y-x}} \\
&= {\left(1 - \frac{y-x}{xy+y}\right)^x\left(1+\frac{1}{y}\right)^{y-x}}
\end{align}
$$ Showing that $\left(1 - \frac{y-x}{xy+y}\right)^x > \frac{1}{\left(1+\frac{1}{y}\right)^{y-x}}$ is what I'm missing","['calculus', 'algebra-precalculus', 'inequality']"
4873132,The projective tensor norm on tensor product of Banach spaces implies the inner product on tensor product of Hilbert spaces?,"As presented in the answer of this post , the projective tensor norm on the algebraic tensor product of two Banach spaces $X$ and $Y$ is given by
\[
\Vert \omega\Vert_{\pi} = \inf\left\{\sum \lVert x_{i}\rVert_X \,\lVert y_{i}\rVert_ Y\,:\,\omega = \sum_{i=1}^{n} x_{i} \otimes y_{i}\right\}
\]
with respect to which we complete the algebraic tensor product to obtain the tensor product Banach space $X \otimes Y$ For Hilbert spaces $H_1$ and $H_2$ , as shown in this link , the inner product on the algebraic tensor product is given by linear extension of the formula \begin{equation}
\langle \phi_1 \otimes \phi_2, \psi_1 \otimes \psi_2 \rangle = \langle \phi_1, \psi_1 \rangle_{H_1} \langle \phi_2, \psi_2 \rangle_{H_2}
\end{equation} for $\phi_1, \psi_1 \in H_1$ and $\phi_2, \psi_2 \in H_2$ . Next, the tensor product Hilbert space $H_1 \otimes H_2$ is defined to be completion under this inner product. Now, my question is Does the projective tensor norm on general Banach spaces, as defined above for $X$ and $Y$ , imply the inner product $\langle \phi_1 \otimes \phi_2, \psi_1 \otimes \psi_2 \rangle$ when $X$ and $Y$ are both Hilbert spaces? I think this must be the case, but it does not seem obvious at all. Could anyone please provide any clarification?","['banach-spaces', 'inner-products', 'hilbert-spaces', 'tensor-products', 'functional-analysis']"
4873213,Can convolution fails to be commutative and distributive in the sense of (generalized) Riemann integral?,"For two (say continuous) function $f,g\in C(\mathbb R^n)$ , let us define \begin{equation}
f\ast g(x):=\lim_{R\to+\infty}\int_{B^n(0,R)}f(x-y)g(y)dy,
\end{equation} whenever the integral converges. It is partially clear that under such definition the convolution can be non-associative (e.g. this post ). Indeed by taking Fourier transform convolutions become multiplications, where the multiplications of distributions can be non-associative without certain restrictions. My question is, can it fails to be commutative and distributive as well? More precisely, Can we find $f,g\in C(\mathbb R^n)$ such that $f\ast g$ and $g\ast f$ both converge, but not equal? Can we find $f,g,h\in C(\mathbb R^n)$ such that $f\ast h$ , $g\ast h$ and $(f+g)\ast h$ all converge, but $f\ast h+g\ast h\neq(f+g)\ast h$ ? Note that both questions don't have analogy on the products of distributions. For a bonus part from Proposition 8.6 in Folland's Real Analysis , set $\tau_hf(x)=f(x-h)$ , Can we find $f,g\in C(\mathbb R^n)$ and $h\in\mathbb R^n$ such that $f\ast g$ and $(\tau_hf)\ast g$ are both converge, but $\tau_h(f\ast g)\neq(\tau_hf)\ast g$ ? Can we do the similar thing to $f\ast(\tau_hg)$ as well?","['riemann-integration', 'convolution', 'analysis', 'real-analysis']"
4873317,Prove that $\sum_{k=0}^n(-1)^k\binom{n}{k}^2 \omega_k=0 .$,"I am reading a paper in which there is a part that I don't understand. Set $f(z)=\sum_{n=0}^{\infty}(-1)^n \frac{z^n}{n ! n !} \text { and } \frac{1}{f(z)}=\sum_{n=0}^{\infty} \omega_n \frac{z^n}{n ! n !} \text {. }$ Then we have $\sum_{k=0}^n(-1)^k\binom{n}{k}^2 \omega_k=0$ I tried to choose $z=1$ , and then I multiplied both sides of two equations, but I don't know how to transform an infinite sum into a finite sum. My attempt by using Cauchy product: $$1=\left(\displaystyle \sum_{i=0}^{+\infty}(-1)^i \frac{z^i}{i ! i !}\right)\left(\displaystyle \sum_{j=0}^{+\infty} \omega_j \frac{z^i}{j ! j !}\right)=\displaystyle \sum_{n=0}^{+\infty} c_n z^n$$ where $$c_n=\displaystyle \sum_{k=0}^n(-1)^k \frac{1}{k ! k !} \omega_{n-k} \frac{1}{(n-k) !(n-k) !}=\frac{1}{(n !)^2} \sum_{k=0}^n(-1)^k\binom{n}{k}^2 \omega_{n-k}$$","['combinatorics', 'analysis', 'real-analysis']"
4873325,Connections on a complex line bundle,"Let $L \to M$ be a complex line bundle over a smooth manifold $M$ . Let $\{U_\alpha\}$ be a trivializing open cover with transition maps $z_{\alpha\beta}:U_\alpha \cap U_\beta \to \Bbb C^* = \text{GL}(1,\Bbb C)$ . The bundle of endomorphisms of $L$ , $\text{End}(L) \cong L^* \otimes L$ is trivial since it can be defined by transition maps $(z_{\alpha\beta})^{-1} \otimes z_{\alpha\beta} = 1$ . This, the space of connections on $L$ , $\mathcal{A}(L)$ is an affine space modeled by the linear space of complex valued $1$ -forms. A connection on $L$ is simply a collection of $\Bbb C$ -valued $1$ -forms $\omega^\alpha$ on $U_\alpha$ related on overlaps by $$\omega^\beta = \frac{dz_{\alpha\beta}}{z_{\alpha\beta}} + \omega^\alpha = d\log z_{\alpha\beta} + \omega^\alpha.$$ Could someone explain to me what is going on here? I'm familiar with line bundles, transition maps, connections and bunch of other stuff, but this seems very weird to me. I have never seen for example this affine space $\mathcal{A}(L)$ which I believe is defined to be somehow realted to $\Omega^1(\text{End}(L))$ . This expression involving the logarithm is also pulled out of thin air, how does one go about deriving such a thing?","['connections', 'vector-bundles', 'differential-geometry']"
