question_id,title,body,tags
1191715,Is there any difference between $\subset$ and $\subseteq$ [duplicate],"This question already has answers here : $\subset$ vs $\subseteq$ when *not* referring to strict inclusion (4 answers) Closed 2 days ago . If we write $A \subset B$ and $B \subset A$ then we can assert that $A = B$ and the same goes for $A \subseteq B$ , $B \subseteq A$ ... So then what is the essential difference between these two notations?",['elementary-set-theory']
1191727,Do any of these sequences have infinitely-many distinct iterates under run-length substitution?,"Let 
$$S = \{x \in \{1,2\}^\mathbb{N}: \ \text{every run in }x\text{ has finite length}\}$$
and define
$$T: S\to \mathbb{N}^\mathbb{N}
$$
such that for any $x\in S$, ${T}x$ is the sequence of run-lengths in $x$; that is, $Tx$ is the result of replacing each (maximal) run by its length. The $T$-iterates of $x$ (when they exist) are then $x, Tx, T^2x, T^3x,...$ Terminology :  Given an infinite sequence $x$, a run in $x$ is any subsequence of $x$ comprising one or more contiguous equal elements. A run is called maximal if it is not adjacent to an element equal to those in the run. If a run has finitely many elements, then their number is called the length of the run; otherwise, the length of the run is said to be infinite. Question : Is there any $x \in S$ having infinitely-many distinct $T$-iterates? If ""no"", how to prove? If ""yes"", how to construct an example? I suspect that no $x\in S$ has infinitely-many distinct iterates, and that every $T$-trajectory either terminates at some iterate not in $S$, or eventually enters a cycle (as in the following examples). Each point in $S$ has exactly two immediate $T$-predecessors, and these are mutual ""complements""; i.e., for any $x\in S$, there exist exactly two points in $S$, say $w$ and $\overline{w}$, such that $Tw = T \ \overline{w} = x$, where $\overline{w}$ is the result of replacing (in  $w$) each $1$ by $2$ and vice versa. (Consequently, in the above picture, it must be the case that $y=\overline{z}$ and $q=\overline{t}$. This applies to the numerical examples given below in $1b$.) The question can be formally restated by partitioning $S$ as follows: $$S = (A_{1a} \cup A_{1b}) \cup A_2$$
where (with $i,j,k$ restricted to nonnegative integers)
$$\begin{align}
A_{1a} & = \{x \in S : \exists i (T^i x \notin S)  \}\\
A_{1b} & = \{x \in S : \forall i (T^i x \in S) \ \mathrm{and} \  \exists\ j\ne k \ (T^jx = T^kx)  \}\\
A_2 & = \{x \in S : \forall i (T^i x \in S) \ \mathrm{and} \  \forall\ j\ne k \ (T^jx \ne T^kx)  \}.
\end{align}
$$ In other words, for any infinite sequence $x\in S$, iterating $T$ repeatedly must result in exactly one of the following cases: $x$ has only finitely-many distinct iterates in $S$. 1a. The iterations terminate due to an iterate that's not in $S$ (i.e., it has some element that's neither $1$ nor $2$, and/or it has a run of infinite length). E.g., $1112...\to 3...$, or $121212...\to 1^\infty$. 1b. All iterates remain in $S$, but there are only finitely many of them (i.e., they eventually enter a finite cycle). E.g., the two Kolakoski sequences are the only 1-cycles (fixed points) of $T$: 
$$12211212212211211221211212211... \to 12211212212211211221211212211...\\
 2211212212211211221211212211... \to 2211212212211211221211212211... 
$$
Using the first of these two, here's an example corresponding to the above picture: 
$$\begin{align}
     &.\\
     &.\\
\to\ v =\ &22122112112212112112212212112...\\
\to\ w =\ &21221221121221211211221221211...\\
\to\ x =\ &11212211211212212112212211212...\\
\to\ y =\ &21122121121122122112122121122... \ (= \overline{z}) \\
\to\ z =\ &\mathbf{12211212212211211221211212211}...\\
\to\ z \to\ &z\ \to \ ...
\end{align}
$$ 
By starting with the first element of each sequence of a cycle, then working ""backwards"", it's straightforward to construct cycles of arbitrary finite size; e.g., here's a 3-cycle construction (with labels corresponding to the above picture): 
$$\begin{align}
     &.\\
     &.\\
\to\ p =\ &211221221211211221211212211211212212112212211...\\
\to\ q =\ &122121121221121122121121122122121121221121121...\ (= \overline{t})\\
\to\ r =\ &\mathbf{121121122122112122121121122121121221121121221...}\\
\to\ s =\ &\mathbf{112122122112112122112112212112...}\\
\to\ t =\ &\mathbf{2112122121122122112\dots}\\
\to\ r =\ &\underline{121121122122}112122121121122121121221121121221...\\
\to\ s\ \to &\ t\ \to\ r\ \to\ s\ \to t\ \to\ r\ \to\ ...
\end{align}$$ $x$ has infinitely-many distinct iterates in $S$. Question : How to prove whether $A_2$ is empty? If $A_2$ is not empty, how to construct an example element? Since, for any $x \in \{1,2\}^\mathbb{N}$, an infinite run can occur only as a suffix, $S$ seems to have the same cardinality as the set of irrationals in the real interval $[0,1]$ (i.e. uncountably infinite); cf. all the reals in that interval except those with ""terminating"" binary expansions. On the other hand, although it seems that $A_{1b}$ is countable, I'm unsure of the cardinality of $A_{1a}$ (uncountable?). So I'm unable to deduce even the cardinality of $A_2$.","['sequences-and-series', 'dynamical-systems', 'recreational-mathematics', 'discrete-mathematics']"
1191728,Why don't we write $\nabla_{X}(fY) = f\nabla_{X}Y$ instead of $\nabla_{X}(fY) = f\nabla_{X}Y+ X(f)Y$ for affine connections?,"According to do Carmo, in Riemannian Geometry pages 49-50, he says let $\mathcal{X}(M)$ denote the set of all vector fields of class $C^{\infty}$ on $M$. Let $\mathcal{D}(M)$ denote the ring of all real-valued functions of class $C^{\infty}$ defined on $M$. An affine connection $\nabla$ on differential manifold $M$ is a mapping $\nabla : \mathcal{X}(M) \times \mathcal{X}(M) \rightarrow \mathcal{X}(M)$ which is denoted by $(X,Y) \xrightarrow{\nabla} \nabla_{X}Y$ and which satisfies the following properties: $\nabla_{fX+gY}Z = f\nabla_{X} Z+ g\nabla_{Y}Z$ $\nabla_{X}(Y+Z) = \nabla_{X}Y + \nabla_{X}Z$ $\nabla_{X}(fY) = f\nabla_{X}Y+ X(f)Y$ in which $X,Y,Z \in \mathcal{X}(M)$ and $f,g \in \mathcal{D}(M)$. The first property simply is linear in the first argument $X$ right? In that case, $X$ happened to be defined as $fX + gY$ So why is the second argument different? By argument, I mean if I write the covariant derivative like $\nabla (X,Y)$, I can clearly see the linearity stated in property 1. Then property 2 looks like property 1 in this regard in that it satisfies the addition property. But the multiplication part is different. Instead of yielding just $\nabla_{X}(fY) = f\nabla_{X}Y$ it yields $\nabla_{X}(fY) = f\nabla_{X}Y+ X(f)Y$. My Question: Why don't we write  $\nabla_{X}(fY) = f\nabla_{X}Y$? Why do we specify the product rule? Why is this important for affine connections? Remark: Consider the first property. We have $\nabla_{fX}Z=f\nabla_X Z$. That doesn't involve the product rule, does it? So why do we need it when we are applying it $\nabla_{X}(fZ)$?","['differential-geometry', 'riemannian-geometry']"
1191730,How to solve $\int\sqrt{1+x\sqrt{x^2+2}}dx$,"I need to solve $$\int\sqrt{1+x\sqrt{x^2+2}}dx$$ I've chosen the substitution variables $$u=\sqrt{x^2+2}$$
$$du=\frac{x}{\sqrt{x^2+2}}$$ However, I am completly stuck at $$\int\sqrt{1+xu} dx$$ Which let me believe I've chosen wrong substitution variables. I've then tried letting $u=x^2+2$ or simply $u=x$, but it does not help me at all solving it. Would someone please give me an hint on this ? Thanks.","['improper-integrals', 'integration']"
1191741,How can I find $\lim_{x \to 0}\left(\frac{e^{2\sin x}-1}{x}\right)$ without l'Hopital's Rule?,"How do I evaluate
$$\lim_{x \to 0}\left(\frac{e^{2\sin x}-1}{x}\right)$$ I know it's the indeterminate form since the numerator and denominator both approach 0, but I can't use l'Hopital's rule so I'm not sure how to go about finding the limit.","['limits-without-lhopital', 'calculus', 'limits']"
1191753,"Is there an isometry between $L^2([0,1])$ and $L^2(\mathbb{R})$?","I was wondering whether there can exist an isometric operator from a bounded $L^2([0,1])$ space to one with an unbounded interval, let's say $L^2(\mathbb{R})$? Both shall be equipped with the standard Lebesgue measure and the Borel sigma algebra.","['real-analysis', 'functional-analysis', 'measure-theory']"
1191759,Finding the roots of $(1 + i)^{\frac{1}{4}}$,"The professor says that the $n = 4$ roots of this are in the form: $\cos(\frac{\theta + 2k\pi}{n}) + i\sin(\frac{\theta + 2k\pi}{n})$, where $k = 0, 1, 2, 3$. So to find $\theta$, we find the $r = \sqrt{(1)^2 + (1)^2} = \sqrt{2}$ since $Re(1+i) = 1$ and $Im(1+i) = 1$. So $\sqrt{2}\cos\theta = 1$ and $\sqrt{2}\sin\theta = 1$, so the angle $\theta$ is $\frac{\pi}{4}$. However, if we do $k=0$, then we get that one of the roots is $1$, which is obviously not true since $1^4 \neq 1 + i$. The professor says that the solutions are: $k=1: \cos(\frac{9\pi}{16}) + i\sin(\frac{9\pi}{16})$, $k=2: \cos(\frac{17\pi}{16}) + i\sin(\frac{17\pi}{16})$, and $k=3: \cos(\frac{25\pi}{16}) + i\sin(\frac{25\pi}{16})$. I plugged these into WolfRamAlpha and rose them to the $4$th power, but none of them return the form $1+i$. What is incorrect about these steps?","['complex-analysis', 'complex-numbers']"
1191768,"If $f$ maps sets of measure zero to sets of measure zero, then so does $g(x)=x+f(x)$.","I want to prove the following. Let $f:[a,b]\to\mathbb{R}$ be continuous and non-decreasing, and suppose that $f$ maps sets of (Lebesgue) measure zero to sets of measure zero. Then, so does
  $$g(x)=x+f(x).$$ This is used in a proof in Rudin Real and Complex Analysis, but I can't understand the argument. He simply say that this ""follows easily"" from the fact that: ""If the $f$-image of some segment of length $\eta$ has length $\eta'$, then the $g$-image of that same segment has length $\eta+\eta'$."" I am able to prove that this statement is true, but how does that imply that $g$ maps null sets to null sets?","['real-analysis', 'measure-theory']"
1191776,Give an argument for $\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx$,"For any $n$ and $p\geq 0$ give an argument that the following is true: $$\int_{0}^{n} x^p dx \leq 1 +2^{p} + 3^{p} + \cdots+ n^{p}\leq \int_{0}^{n+1} x^p dx$$ I'm having trouble even beginning this question. My first thought it to somehow meld this with the squeeze theorem, but, again, am not sure how to begin and show any real work. Any insight is very much appreciated.",['multivariable-calculus']
1191798,What is the proof that the rank of a matroid is sub-modular?,"Recall the definition of the rank of a matroid $(V, \mathcal{I})$: $$ r(A) = \operatorname{rank}(A) = \max_{I \in \mathcal{I}}\{ | A \cap I | \} = \max\{ |I| : I \subseteq A, I \in \mathcal{I} \}$$ I was trying to prove that the rank of a matroid is a sub-modular function, i.e. that the following inequality holds for all subsets of the ground set (i.e. $\forall A, B \subseteq V$): $$ r(A) + r(B) \geq r(A \cup B) + r(A \cap B)$$ I tried a ""picture proof"" by drawing a couple of sets and seeing how their intersection with independent elements behaved and I can only conclude that its in fact an equality. I am sure there is something wrong with that method and its not a real proof but wasn't sure how else to approach it. Does someone have a proof or a suggestion on good direction I might try to actually prove this result? Also, is this suppose to be ""intuitively obvious""? Because its not completely obvious for me.","['matroids', 'functions']"
1191800,"Principal bundles, connection forms and fundamental vector fields","Suppose $\pi:P\rightarrow M$ is a principal bundle, $\omega\in \Omega^1(P;\mathfrak{g})$ is the connection one form and $\sigma(\cdot)$ is the fundamental vector field associated to some vector field in $\mathfrak{g}$. That is, for $X\in \mathfrak{g}$, the value of $\sigma(X)$ at $p\in P$ is given by $
\begin{align*}
\sigma(X)_p=\frac{d}{dt}\bigg|_{t=0}p\exp(tX).
\end{align*}$ I am reading through some lecture notes at the moment and the author decomposes a vector field $u\in \mathfrak{X}(P)$ in to its horizontal and vertical components. So $u=u_v + u_h$. The author then takes the derivative $u_v f$ of some function $f$ defined on $P$. They make the argument that since this derivative at a point $p\in P$ depends only on the vector $u_v$ at $p$, they may assume $u_v = \sigma(\omega(u))$. I understand the idea of the derivative only depending on the vector at $p$. However, I am stuck on how they can assume that $u_v=\sigma(\omega(u))$. I don't understand how $u_v$ at $p$ is equal to $\sigma(\omega(u))$ at $p$. If anyone could help that would be greatly appreciated. Thanks! EDIT: Thanks very much for your detailed response. I have a few questions 1) Why have you written that map as $\rho_p$ instead of $L_p$? Isn't it just left multiplication? On that note, is it kosher to define left multiplication on $P$? 2)Is the reason $\sigma(X)_p=(\rho_p)_{*,e}X$ because $X=\frac{d}{dt}\big|_{t=0}\exp(tX)$, so $(\rho_p)_{*,e}X=(\rho_p)_{*,e}\frac{d}{dt}\big|_{t=0}\exp(tX)=\frac{d}{dt}\big|_{t=0}p\exp(tX)=\sigma(X)_p$?? 3) Why does $R_g$ and $\delta_g$ being diffeomorphisms imply  $\rho_p$ has constant rank? Why did you go to the effort of adding $g$ and $g^{-1}$? 4) Why does dim$V_p$=dim$\mathfrak{g}$ imply the map is surjective? I'm thinking that's just something from linear algebra I'm forgetting? Thank you again!","['principal-bundles', 'lie-groups', 'manifolds', 'differential-geometry', 'lie-algebras']"
1191809,A nonempty class of isomorphic groups defines a group,"The context of this question is from the definition of the sporadic Mathieu group $M_{23}$, which (in one possible definition) is the stabilizer of a point in $M_{24}$, which is a certain subgroup of $S_{24}$ (a permutation group on $24$ points). When I read this I was a bit surprised since they haven't specified which point is to be stabilized, but I assume that the content here is that it doesn't matter, since different stabilizers yield isomorphic subgroups. Which brings me to the question: is it possible to define a single group given a set of isomorphic groups? Of course in this case we could define $M_{23}$ as the stabilizer of the first point, assuming that the $24$ original points are distinguishable to begin with, for example if they were the numbers $1,\dots,24$ and I could pick out $1$ specifically. But in more general contexts this can be problematic, especially for sets which have no distinguished members, such as the set of all free ultrafilters of $\Bbb N$. If I have a family $(G_i)_{i\in I}$ of groups for some nonempty index set $I$ such that $G_i\cong G_j$ for all $i,j\in I$, is it possible to define a group $G$ such that $G\cong G_i$ for all $i$? By the observation above, if there is some term $x$ for which $x\in I$ is provable, then it would suffice to take $G=G_x$, but what if there is no such $x$?","['foundations', 'elementary-set-theory', 'group-theory', 'logic']"
1191818,why does e raised to the power of negative infinity equal 0?,"Why is it that e raised to the power of negative infinity would equal 0 instead of negative infinity? I am working on problems with regards to limits of integration, specifically improper integrals and a little confused as to what things approach infinity our negative infinity versus approaching zero","['infinity', 'calculus', 'limits', 'exponentiation']"
1191826,Can anyone tell me if this is correct?,"Suppose that the temperature of a metal plate is given by $T(x; y) = x^2 +2x+y^2$, for points $(x, y)$ on the elliptical plate defined by $x^2 + 4y^2 <= 24$.
Find the maximum and minimum temperatures on the plate. This is what i have done so far. Finding critical point:
$T(x)=2x+2$, $T(y)=2y$. Equating to $0$, $x=-1$, $y=0$.
Critical point is $(-1,0)$ and is a minimum. On the boundary,   $  x^2 + 4y^2 = 24$ $g(x,y)=x^2 + 4y^2$ $g(x)=2x, g(y)=8y$ $2x+2=A2x$---------(1) $2y  =A8y$---------(2) $x^2+4y^2=24$------(3) When solving from equation 1 and 3 im getting $ x=-1,y=|(23/4)^{0.5}|,$ and $x=|24^{0.5}|, y=0$ and when from eqn 2 and 3 im getting $x=|24^{0.5}|,y=0, A= 0.25 , x=-4/3, y=|(50/9)^{0.5}|$. Is this correct? am getting different values when using equation $1$ and $2$.","['lagrange-multiplier', 'multivariable-calculus']"
1191847,Tangent space of tangent vector,"Let $M$ be a smooth manifold. There's a (split) short exact sequence $$0\to T_aM\to T_v(TM)\stackrel {D_v}{\to} T_aM \to 0,$$
where $v\in TM$ and $a=\pi(v)\in M$. I'm trying to understand what this exact sequence means, but I don't even have an intuition about $T_v(TM)$. Can someone explain an example of tangent space of tangent vector (e.g. tangent vector of $S^1$)? How to take a tangent vector of a tangent vector geometrically?","['differential-topology', 'differential-geometry', 'manifolds']"
1191886,Why are the countable ordinals a set?,"The countable ordinals are themselves either countable or uncountable.  They cannot be countable since that would involve a set with itself as an element, so they are uncountable. If they are uncountable, then either they form a consistent totality or they are a proper class.  If they form a consistent totality, then a least uncountable ordinal exists, otherwise not. So how is it determined that the sequence of countable ordinals is or is not a consistent totality?  My problem is that it cannot be said they form a consistent totality because there is an uncountable limit ordinal since that would be circular.  Therefore, how exactly is this resolved? Edit, as requested: My question is not equivalent to the question, ""What axioms are you using?  ZFC?""  Nor is it answered by (I haven't figured out how to format the symbols on this site so I'll just spell it out): ""The set of all countable ordinals is the set of those elements x of the cardinal number two to the aleph-null such that x is a countable ordinal"", which seems to be a way of saying x is a countable ordinal if it is a member of the set of countable ordinals, except two to the aleph-null is a number: it is not a set but the cardinality of a set, so it doesn't actually have elements.  I suppose the intention of that answer could be taken as that all countable sets are elements of the power set of N, except the assertion isn't true.  Nor is my question addressed by comments to that answer.  So far as I can tell, the answerer did not define omega plus one, nor have I ever seen two to the aleph-null described as an ordinal.  Besides which, the relation of the cardinal number two to the aleph-one to the limit ordinal omega-one is outside ZFC.  So I honestly don't see what any of this has to do with my question or why I was asked to edit it accordingly.","['ordinals', 'elementary-set-theory']"
1191889,Residue of two functions,"Let be $f,g$ functions analytic in $z_0$, with $z_0$ a zero of order one of $g$ and $f(z_{0})\neq 0$. Show that 
$$
\operatorname{Res}\Bigl(\frac{f}{g},z_0\Bigr)=\frac{f(z_{0})}{g'(z_{0})}
$$
My attemp... If $z_{0}$ is a zero of order one of $g$, then $g(z)=(z-z_{0})h(z)$, with $h(z)$ analytic and $h(z_{0})\neq 0$, then by analyticity of $g$ and $h$: $g'(z)=h(z)+(z-z_{0})h'(z)$. So, 
$$
\frac{f(z_{0})}{g'(z_{0})}=\frac{f(z_{0})}{h(z_{0})+0}=\lim_{z\to z_{0}}{(z-z_{0})\frac{f(z)}{g(z)}}
$$ 
But in the ultimate point I'm not sure",['complex-analysis']
1191899,Homotopy equivalence between O-O and $\theta$,"Show that the dumbbell O-O (where there's no space between the ""O"" and ""-"") and the letter $\theta$ are homotopy equivalent, using the definition. So, let $X$ be the set of points in the dumbbell, and $Y$ the set of points in $\theta$. We should give continuous maps $f:X\rightarrow Y$ and $g:Y\rightarrow X$ such that $f\circ g$ is homotopic to $\text{id}_Y$ and $g\circ f$ is homotopic to $\text{id}_X$. I'm thinking about mapping the ""-"" in the dumbbell to the $-$ in $\theta$, and the two ""O""s in the dumbbell to the two halves of $\theta$. But each end of the ""-"" in the dumbbell is connected to only one half, while each end of the $-$ in theta is connected to both halves. So I'm not sure what to do.","['homotopy-theory', 'general-topology']"
1191901,Floor function of a factorial,"Compute $$\left\lfloor \frac{1000!}{1!+2!+\cdots+999!} \right\rfloor.$$ How can I start with the problem? I thought of dividing by some number, but then I thought that some small numbers when added could also give an integer. Thanks.","['factorial', 'ceiling-and-floor-functions', 'algebra-precalculus']"
1191923,"Second Countable, First Countable, and Separable Spaces","Upon further studying Topological Spaces, I understand: If a space $X$ has a countable dense subset, then $X$ is a separable space. A space $X$ is first countable provided that there is a countable local basis at each point of $X$. A space $X$ is second countable if and only if its topology of $X$ has a countable basis. My question is: Why would a space $X$ that is second countable also be first countable and separable? Why would a space $X$ that is first countable not necessarily be considered a separable space and vice versa? I do have a rough idea as to why a second countable space is also a first countable space. A second countable space has a countable basis $\mathcal{B}$ $-$ which consist of a countable family of open sets $-$ then the members of $\mathcal{B}$ which contain a particular point $a$ form a countable local basis at $a$. Thus each second countable space is first countable. Now if the space $X$ is second-countable, to also be separable, there needs to exists a countable dense subset of $X$. It has been established that if $X$ is a second countable space, it has a countable basis $B$. This is where I get stuck. I am not sure as to why spaces that are first countable do not imply they are separable and vice verse. Does it have to do with the fact first countable spaces deal with countable local basis that may or may not be dense? Am I on the right track? Sorry for the rather long question. If is it rather confusing, let me know so I can clarify. I want to thank you in advance for taking the time to read this question. I greatly appreciate any assistance you provide.","['second-countable', 'separable-spaces', 'general-topology', 'first-countable']"
1191924,Is it true that every element of $V \otimes W$ is a simple tensor $v \otimes w$?,"I know that every vector in a tensor product $V \otimes W$ is a sum of simple tensors $v \otimes w$ with $v \in V$ and $w \in W$. In other words, any $u \in V \otimes W$ can be expressed in the form$$u = \sum_{i=1}^r v_i \otimes w_i$$for some vectors $v_i \in V$ and $w_i \in W$. This follows from the proof of the existence of $V \otimes W$, where one shows that $V \otimes W$ is spanned by the simple tensors $v \otimes w$; the assertion now follows from the fact that, in forming linear combinations, the scales can be absorbed in the vectors: $c(v \otimes w) = (cv) \otimes w = v\otimes (cw)$. My question is, is it true in general that every element of $V \otimes W$ is a simple tensor $v \otimes w$?","['exterior-algebra', 'multilinear-algebra', 'abstract-algebra', 'tensor-products', 'linear-algebra']"
1191931,Length from tangent circles,"A circle $Γ_1$ of radius $25$ is externally tangent to a circle $Γ_2$ of radius $16$ at $C$. Let $AB$ be a common direct tangent, so that $A$ lies on $Γ_1$ and $B$ lies on $Γ_2$. Draw the tangent to $Γ_1$ that is parallel to $AB$; let this tangent intersect $Γ_1$ at $T$, and the common transverse tangent through C at $U$. Then find the  length of $TU $. From where do I start the problem. I tried drawing some perpendiculars, but I don't think it helps.","['contest-math', 'geometry', 'circles']"
1191941,Exterior derivatives involving representations,"I have two questions regarding the exterior derivative of vector valued forms when representations are involved: Suppose $V$ is a vector space, $M$ a smooth manifold and $\omega$ is a $V$ valued $k$-form on $M$. Ie, $\omega \in \Omega^k(M;V)$. Suppose furthermore that $\rho_1:G\rightarrow GL(V)$ is a representation for some Lie Group $G$ and $\rho_2:\mathfrak{g}\rightarrow GL(V)$ is the induced Lie algebra representation. The function $\rho_1(g)\circ \omega$ could be considered as a $V$ valued $k$-form on $M$. If $d:\Omega^k(M;V)\rightarrow \Omega^{k+1}(M;V)$ is the exterior derivative for $V$ valued $k$-forms, then $d(\rho(g)\circ \omega)=\rho(g)\circ d\omega$. I am just wondering why this is true? Furthermore, in some lecture notes I'm reading the author also writes for $\eta\in \Omega^1(M;\mathfrak{g})$ that $d(\rho_2(\eta)\circ \omega)=\rho_2(d\eta)\circ \omega-\rho_2(\eta)\wedge d\omega$ I am completely in the dark as to how this equation makes sense. I don't understand how the RHS is a 2-form or where the RHS even comes from. How would you make sense of this equation?","['differential-forms', 'lie-groups', 'manifolds', 'differential-geometry', 'lie-algebras']"
1191945,Problem with definite integral $\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$,$$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ The question says 'evaluate the integral using the suggested substitution. It gives $u=\cos x$. But I think Let $u=1-2\sin x$ is better. $$\int_{0}^{\frac{\pi}{6}}\cos x\sqrt{1-2\sin x} dx$$ $$u=1-2\sin x$$ $$du=-2\cos x dx$$ $$=-\frac{1}{2}\int_{1}^{0}\sqrt{u}du$$ $$=\frac{1}{2}\int_{0}^{1}\sqrt{u}du$$ $$=\left | \frac{u^{\frac{3}{2}}}{3} \right |_{0}^{1}$$ $$=\frac{1}{3}$$ My question is how to solve it by using $u=\cos x$. Can anyone show the solution for it? Thanks a lot!,"['trigonometry', 'calculus', 'definite-integrals', 'integration']"
1191982,Limit for sequence $a_{m+n}\leq a_m+a_n$,"Let $a_1,a_2,\ldots$ be a sequence of positive real numbers. Suppose that $a_{m+n}\leq a_m+a_n$ for all $n\geq 1$. Does $\lim_{n\rightarrow\infty}\dfrac{a_n}{n}$ always exist? From $a_{m+n}\leq a_m+a_n$ we know that $a_n\leq na_1$, so that $\dfrac{a_n}{n}\leq a_1$, which means the sequence $\dfrac{a_n}{n}$ is bounded both from above and below. But this is not enough to conclude that the limit exists.","['limits', 'real-analysis']"
1192008,Evan's Proof to Converse of Mean Value Property.,"The theorem state: If $u \in C^2(U)$ satisfies $$ u(x) = \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ for each ball $B(x,r) \in U$, then u is harmonic. The issue that I have is with the proof of the theorem. He asserts to show it by contradiction, to assume that $\Delta u > 0$. Then for $$\phi(r) =  \frac{1}{|\partial B(x,r)|}\int_{\partial B(x,r)} u(y) dS(y)$$ $$ 0 = \phi ' (r) = \frac{r}{n}\frac{1}{|B(x,r)|} \int_{B(x,r)} \Delta u(y) dy >0$$ a contradiction. My issue is that I'm pretty sure that we show $\phi ' (r) = 0$ in the opposite direction by using the fact that $u$ is harmonic in the first place, so I don't see how we can use that fact here, especially when we're assuming the opposite. I feel like there is something very sly going on here. Can someone explain the proof?","['real-analysis', 'partial-differential-equations']"
1192013,$\sum_{i=1}^{89} \sin^{2n} (\frac{\pi}{180}i)$ is a dyadic rational,"Last year's Euclid contest had a problem asking for the rational value of $\sum_{i=1}^{89} \sin^{6} (\frac{\pi}{180}i)$.  I tested this sum for different even powers, and the result was always a dyadic rational (meaning it is of the form $\frac{n}{2^m}$ for positive integers $n$ and $m$).  Can someone prove that $\sum_{i=1}^{89} \sin^{2n} (\frac{\pi}{180}i)$ is a dyadic rational for all positive integers $n$, or find a counterexample? More generally, is $\sum_{i=1}^{k} \sin^{2n} (\frac{\pi}{k}i)$ always a dyadic rational for positive integers $n,k$?","['sequences-and-series', 'trigonometry']"
1192045,Bisecting the area and perimeter,"In triangle $ABC$, $AB=16$, $AC=15$, and $BC=13$. Point $D$ is on $AB$, and point $E$ is on $AC$ so that $DE$ bisects both the area and perimeter of triangle $ABC$. (In other words, both $DA+AE$ and $DB+BC+CE$ are equal to half the perimeter.) Find $DE^2$.","['geometry', 'triangles']"
1192088,Rationalizing the fraction $\frac{1}{1-\sqrt2 -\sqrt3}$,"I'm having problem in rationalizing the following root with the fraction $$\frac{1}{1-\sqrt2 -\sqrt3}$$
Eventually after many tries, I found the solution which was : $$\frac{-\sqrt2 (1-\sqrt2 +\sqrt3 )}{4}$$
But I want to know if there's a specific method to use in a case like this.
Thanks","['rationalising-denominator', 'radicals', 'algebra-precalculus']"
1192121,Is a square of a prime ideal in a UFD always primary?,"More concretely, Let $R$ be a UFD and $\mathfrak{p}$ a prime ideal ideal of $R$. Does it always hold that $\mathfrak{p}^2$ is a primary ideal? I know that it always holds if $\mathfrak{p}$ is a principal ideal or a maximal ideal, so one needs only consider rings of Krull dimension $\geqslant 3$. I proposed this question mainly because I'd like to know how well-behaved a prime ideal of UFD could be.","['abstract-algebra', 'unique-factorization-domains', 'ideals', 'commutative-algebra']"
1192134,Locus of a midpoint,"Let $Γ_1$ be a circle of radius $4$, and let $Γ_2$ be a circle of radius $14$. The distance between the centers of $Γ_1$ and $Γ_2$ is $25$. Let $A$ be a variable point on $Γ_1$, let $B$ be a variable point on $Γ_2$, and let $M$ be the midpoint of $AB$. Let $S$ be the set of all possible locations of $M$. Then find the area of $S$. I am getting $81\pi$. See the figure With one of my friends, I got this.Let C1 be centered at $(0,0)$ and let C2 be $(25, 0)$. The points that will lie on the boundary of S are
1. Midpoint of (-4, 0) and (11, 0) i.e. (3.5, 0)
2. Midpoint of (4, 0) and (39, 0) i.e. (21.5, 0)
3. Midpoint of the tangents joining C1 and C2 Let the angle that the point of intersection of the tanget to circle C1 and C2 makes with the x axis be $\theta $ then
The point of intersection on C1 and C2 are
  $$(x_1, y_1) = (4\cos{\theta}, 4\sin{\theta})$$
  $$(x_2, y_2) = (25+14\cos{\theta}, 14\sin{\theta})$$ The equation of the tangent is
$$ y = -\frac{x}{\tan{\theta}} + c$$ Putting the above two points in the line equation and eliminating c gives
  $$ 10\sin{\theta} = -\frac{25 + 10\cos{\theta}}{\tan{\theta}}$$
  $$ 10\sin{\theta}\tan{\theta} = -25 - 10\cos{\theta}
   \frac{2}{\cos{\theta}} = -5
   \cos{\theta} = -\frac{2}{5}$$ This gives
  $$ \sin{\theta} = \pm \frac{\sqrt{21}}{5}$$ The two pairs of points of intersection in C1 and C2 are
  $$ (x_1, y_1) = (-\frac{8}{5}, \frac{4\sqrt{21}}{5})$$
  $$ (x_2, y_2) = (\frac{97}{5}, \frac{14\sqrt{21}}{5})$$
and
   $$(x_1, y_1) = (-\frac{8}{5}, -\frac{4\sqrt{21}}{5})$$
  $$ (x_2, y_2) = (\frac{97}{5}, -\frac{14\sqrt{21}}{5})$$ This gives the other two mid points as $(\frac{89}{10}, \frac{9\sqrt{21}}{5})$, $(\frac{89}{10}, -\frac{9\sqrt{21}}{5})$ Using all the mid points obtained and putting them into the ellipse equation
  $$ \frac{(x - x_1)^2}{a^2} + \frac{(y - y_1)^2}{b^2}$$
following are obtained
  $$ x_1 = \frac{25}{2}$$ and $$ y_1 = 0$$
 $ a = 9$ and $b=9$.
 Where is it wrong.($81\pi$ is wrong!) Please help, thanks.","['geometry', 'circles', 'locus']"
1192151,Prove complements of independent events are independent.,"Given a finite set of events $\{A_i\}$ which are mutually independent, i.e., for every subset $\{A_n\}$, 
$$\mathrm{P}\left(\bigcap_{i=1}^n A_i\right)=\prod_{i=1}^n \mathrm{P}(A_i).$$ show that the set $\{A_i^c\}$, that is the set of complements of the original events, is also mutually independent. I can prove this, but my proof relies on the Inclusion-Exclusion principle (as does the proof given in this question ). I'm hoping there is a more concise proof. Can this statement be proved without the use of the Inclusion-Exclusion principle?",['probability']
1192173,Probability - marbles without replacement,"Math is my weakest subject and I'm having a hard time trying to figure out what equation to use in this problem: A jar contains 5 purple balls, 10 pink balls, and 7 blue balls. If 3 balls are to be drawn successively without replacement. What is the probability of getting 2 purple balls and 1 pink ball? Ans: I tried doing (5/22)(4/21) & (10/22). But I don't think it's right... Any help is appreciated.","['probability', 'statistics']"
1192178,Curve in a product of tori,"Consider the curve $\gamma:\mathbb R\to (\mathbb R/\mathbb Z)^n$ given by $$\gamma(t)=(a_1t,\ldots,a_nt)$$ for generic real numbers $a_1,\ldots,a_n$. Is the image of $\gamma$ dense in $(\mathbb R/\mathbb Z)^n$?","['calculus', 'real-analysis', 'general-topology', 'analysis', 'differential-geometry']"
1192189,Maximum value of the absolute value of a holomorphic function,"Consider the holomorphic function $f(z) := \frac{1}{z}(e^z - 1) = \sum_{k=0}^\infty \frac{z^k}{(k+1)!}$ with $\text{Re}(z) \leq 0$ and let $g(z) := |f(z)|$. Show that the maximum of $g$ is attained at $z = 0$. I have solved this by the bruteforce method by setting $z = a + b i$, $a \leq 0$, $b \in \mathbb{R}$ and considering $g$ as a real-valued function $g(a,b)$.
By the maximum modulus principle I only considered the case $a = 0$ which drastically simplifies the task. However, this principle is only applicable for bounded domains. 
Can I also apply this here? Or is there another ""simple"" method that directly gives the answer?",['complex-analysis']
1192199,What is wrong with ${13 \choose 1}{4 \choose 2} \cdot {12 \choose 1}{4 \choose 2}$ as combinations for two pair in poker?,Let's consider two pairs in a 52 cards deck of poker where every person gets five cards. My idea to approach this problem is to take following steps: First pair There are ${4 \choose 2}$ combinations getting two cards of the same rank There are ${13 \choose 1}$ combinations of having a specific rank out of a suit Second pair There are still ${4 \choose 2}$ combinations to get two cards of the same rank However as one card per suit is gone we only have ${12 \choose 1}$ for each combination out of a suit Any card There are ${4 \choose 1}$ combinations getting one card out of the same rank There are ${11 \choose 1}$ combinations to getting one card out of a suit This would yield in: $$P(TP) = \frac{{4 \choose 2}{13 \choose 1} \cdot {4 \choose 2}{12 \choose 1} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ According to wikipedia the correct probability would be calculated as: $$P(TP) = \frac{{13 \choose 2}{4 \choose 2}{4 \choose 2} \cdot {4 \choose 1}{11 \choose 1}}{{52 \choose 5}}$$ What is the mistake in my model and how could I think of the one provided in wikipedia?,"['combinations', 'probability', 'binomial-coefficients']"
1192241,How to row reduce a matrix with complex entries?,"I have been doing some practice questions for university, and one of them is regarding row reducing a complex matrix.
From what I can work out, I think (i could very well be wrong) that the first unknown (row 1) should be (1/32)(41i - 82)
And as such, the second unknown should be (-3-2i) - (2 + 2i)((1/32)(41i - 82)) However this looks messy, and is making me think i may have done it wrong. If someone could please let me know if i am correct or not, and if not, where i went wrong, i would be hugely grateful. Thanks heaps in advance
Corey","['gaussian-elimination', 'linear-algebra', 'complex-numbers', 'matrices']"
1192281,"Show that if $U \subseteq C$ and $V \subseteq C$ are both open and convex sets, then the set $U \cap V \subseteq C$ is open and convex as well.",I think you have to prove that as it is the intersection then both are in open and convex sets seeing as they are on their own. Don't really know how to put this down in notation though.,['analysis']
1192306,Define the Riemann integral via trapezoids instead of rectangles,"Let $I$ be an interval and $f\colon I \to \mathbb{R}$. Recall that $f$ is called Riemann-integrable with integral $s$ if the following is true: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any tagged partition $x_0,\ldots,x_n$ of $I$ and $t_0,\ldots,t_{n-1}$ whose mesh is less than $\delta$, we have $$\left|\sum_{i=0}^{n-1} f(t_i) (x_{i+1}-x_i) - s\right| < \epsilon$$ The intuitive idea which leads to the Riemann integral is that you approximate the ""area under the curve"" by rectangles. However one could also start with the idea to approximate it via trapezoids. So one could try to define the ""trapezoid integral"" via: For all $\epsilon > 0$, there exists $\delta > 0$ such that for any  partition $x_0,\ldots,x_n$ of $I$ whose mesh is less than $\delta$, we have $$\left |\frac{1}{2} \sum_{k=0}^{n-1} \left( x_{k+1} - x_{k} \right) \left( f(x_{k+1}) + f(x_{k})\right) -s \right | < \epsilon$$ Would this ""trapezoid integral"" be equivalent to the Riemann integral in the sense that a function is trapezoid integrable iff it is Riemann integrable and the integral s are equal in this case? If not, is one more general than the other? If not: Is it possible to make a slightly different definition of the integral starting from the trapezoid idea such that one can state such a theorem? Is it also possible to generalize the idea to a Newton-Cotes approach and also get a clear connection to the Riemann integral Is this type of ""trapezoid"" integral (or a generalization) known in the literature? If so, do you have a reference which states and proves theorems about the relation to the Riemann integral? Note that I know the trapezoid Rule for approximating the Riemann integral but this is only for numerical approximations.","['real-analysis', 'riemann-sum', 'integration', 'analysis', 'reference-request']"
1192308,Putnam 2006 B1 Problem,"Show that the curve $x^{3}+3xy+y^{3}=1$ contains only one set of three distinct points, $A,B,$ and $C,$ which are the vertices of an equilateral triangle, and find its area. Yikes. Without knowing that this is the Folium of Descartes, it says the equation is reducible somehow... $$x^3 + 3xy + y^3 - 1 = 0$$ Is factorable somehow. I tried the cubic way, but it is still insane... $$(x - 1)(x^2 + x + 1) + y(y^2 + 3x) = 0$$ But that doesnt help any bit, I could change $3xy$ I suppose: $$\implies x^3 + xy + xy + xy + y^3 - 1 = 0$$ But that doesnt help either!","['calculus', 'contest-math', 'real-analysis', 'algebra-precalculus', 'elementary-number-theory']"
1192320,"If $A_i$ is a compact subset of a metric space $(X_i,d)$ where $i = 1,2$ to show that $A_1 \times A_2$ is compact in $X_1 \times X_2$.","If $A_i$ is a compact subset of a metric space $(X_i,d)$ where $i = 1,2$ to show that $A_1 \times A_2$ is compact in $X_1 \times X_2$. Proof: Let $\{(a_n ,b_n)\}$ be any sequence in $A_1 \times A_2$ . Then $a_n $ in $A_1$ has a convergent subsequence $a_{n_i} \to a$ and taking the sequence $b_{n_i}$ in $A_2$ we have a convergent subsequence $b_{n_{i_{j}}} \to b$ and also $a_{n_{i_{j}}} \to a$. Thus the sequence $\{(a_n ,b_n)\}$ have a convergent subsequence $(a_{n_{i_{j}}},b_{n_{i_{j}}} ) \to (a,b)$. Thus  $A_1 \times A_2$ is compact in $X_1 \times X_2$. Is my working correct??","['metric-spaces', 'general-topology']"
1192322,Strong Markov property of Bessel processes,"I am thinking about the following: If $(B_t)_{t \geq 0}$ is a  Brownian motion in $\mathbb{R}^3$, how can we show that the Bessel process (of order $3$) $(|B_t|)_{t \geq 0}$ has the strong Markov property? Any hints?","['probability-theory', 'brownian-motion', 'markov-process', 'stochastic-processes']"
1192339,How to prove whether the equation set has a unique solution?,"\begin{eqnarray}
\begin{cases}
\sin A \sin C-(\sin B)^2=0
\cr AC-B^2=0
\cr A+B+C-\pi=0
\cr A>0,B>0,C>0
\end{cases} \end{eqnarray} How to prove whether the equation set has a unique solution or not ?","['contest-math', 'geometry', 'systems-of-equations', 'education']"
1192347,Mapping vector spaces over two different fields?,"I was having linear algebra class and we have been discussing about a possible group homomorphism that might allow mapping between two vector spaces over two different fields This is also an extension of this question Suppose we have vector spaces $V$ and $W$ over some general field $\mathbb{F}_1$ and $\mathbb{F}_2$ and $T$ is a (linear) map from $V$ to $W$ In order to get around the issue of this vector space axiom becoming undefined because of c being in different fields $$T(c\mathbf{x})=cT(\mathbf{x})$$ What's the issue in doing this (adapting the definition of group homomorphism, where there are two groups $(G,@)$ and $(H,*)$ )? $$\phi (a @b)=\phi(a)*\phi(b)$$ to the context of vector space (where the fields are defined as $(\mathbb{F}_1,+,*)$ and $(\mathbb{F}_2,"",@)$ ) $$T(c_\mathbb{F_1}*\mathbf{x})=T(c_\mathbb{F_1})@T(\mathbf{x})=c_\mathbb{F_2}@T(\mathbf{x})$$ (The two cs are different because they are elements of different fields) It seems valid as long every element in $\mathbb{F}_2$ can be mapped from at least one in $\mathbb{F}_1$ . What subtleties have we overlooked? If this is valid is this still a linear algebra?","['group-theory', 'linear-algebra', 'vector-spaces']"
1192399,"Let $(X,\mathcal M, \mu)$ be a measure space, and $\mu_0$ the semifinite part of $\mu$. Show that there is a measure $\nu$ such that $\mu=\mu_0+\nu$.","This is Exercise 15c Chapter 1 from Folland. I know that if there is an $F\subseteq E$ with $0<\mu(F)<\infty$, then $\mu_0(E)=\mu(E)$. If $\mu(E)=\infty$, and $F\subseteq E$,$\mu(F)<\infty$ imply $\mu(F)=0$, then $\mu_0(E)=0$. I tried
$$\nu(E)=
\begin{cases}
0, & \text{if }\mu(E)=\mu_0(E)\\
\infty, & \text{otherwise}.
\end{cases}$$ This satisfies $\mu=\mu_0+\nu$, but it is not a measure.  If $\{E_n\}\subseteq\mathcal M$ is a collection of disjoint sets, and $\nu(E_n)=\infty$ for some $n$, and $\nu(E_m)=0$ for $m\ne n$ then
$$\sum_{n=1}^\infty \nu(E)=\infty,$$ but there are $F\subset\bigcup E_n$ such that $0<\mu(F)<\infty$, since such sets are contained in any $E_m$ distinct from $E_n$; so,
$$\nu\left(\bigcup_{n=1}^\infty E_n\right)=0.$$ Letting $\nu(E)=\infty$ if and only if $\mu(E)=\infty$ doesn't work either.  We only need a countable collection of sets of finite measure, such that the union has infinite measure to see that $\nu$ is not a measure in this case. So, I'm stuck.","['real-analysis', 'measure-theory']"
1192433,Infinite sequence series. Limit,"If $0<x<1$ and
$$A_n=\frac{x}{1-x^2}+\frac{x^2}{1-x^4}+\ldots+\frac{x^{2^n}}{1-x^{2^{n+1}}},$$
then $\lim_{n\to\infty} A_n$ is $$\text{a) }\ \dfrac{x}{1-x} \qquad\qquad \text{b) }\ \frac{1}{1-x} \qquad\qquad \text{c) }\ \frac{1}{1+x} \qquad\qquad \text{d) }\ \frac{x}{1+x}$$ How to do this? Not able to convert in any standard form.","['sequences-and-series', 'calculus', 'limits']"
1192438,$(1-\zeta_m)$ is a unit in $\mathbb{Z}[\zeta_m]$ if m contains at least two prime factors,"We know that for $m=p^r, 1-\zeta_m$ is a prime.Now suppose that m has at least 2 distinct primes appearing in its prime factorization,we need to show that $1-\zeta_m$ is a unit in its ring of integers $\mathcal{O}_{\mathbb{Q}(\zeta_m)}=\mathbb{Z}[\zeta_m]$. 
I tried proving that $N_{\mathbb{Q}}^{\mathbb{Q}(\zeta_m)} (1-\zeta_m)=\pm 1$ but got stuck in finding norm of $\zeta_m$. Some hint would be nice.","['number-theory', 'algebraic-number-theory', 'commutative-algebra']"
1192458,"Prove that if $a^p-b^p$ is divisible by $p$, then it is also divisible by $p^2$","$a$ and $b$ are natural numbers and $p$ is a prime number. Prove that if $a^{p}-b^{p}$ is divisible by $p$, then it is also divisible by $p^{2}$. My attempt: Based on Fermat's theorem $(a^{p}-a)$ and $(b^{p}-b)$ are divisible by p, therefore their difference is divisible by $p$. I.e.
$(a^{p}-a)-(b^{p}-b)=(a^{p}-b^{p})-(a-b)$ is divisible by $p$. Since the left hand side of this equation is divisible by $p$, the right hand side also should be divisible by $p$. Based on the problem's assumption $(a^{p}-b^{p})$ is divisible by $p$ therefore we conclude $(a-b)$ must be divisible by $p$. Now We can factorize $(a^{p}-b^{p})$
$$(a^{p}-b^{p})=(a-b)(a^{p-1}+a^{p-2}b+....+b^{p-1})$$
we have to prove $(a^{p}-b^{p})$ is multiple of $p^2$ which means we have to show the right hand side should be multiple of $p^{2}$. But $(a-b)$ is multiple of p therefore we have to show 
$(a^{p-1}+a^{p-2}b+\cdots+b^{p-1})$ is multiple of $p$ and I am stuck here. Any help would be appreciated.","['lifting-the-exponent', 'elementary-number-theory', 'algebra-precalculus']"
1192459,Help with the Basis Step of a strong induction proof,"I have to determine whether the following definition is valid and, if so, find a formula for $f(n)$. $f(0)=1$, $f(1)=0$, $f(2)=2$, $f(n)=2f(n-3)$ for $n \geq 3$ I know it is valid because I successfully get a result for $f(3)$, $f(4)$, $f(5)$ and so on from such a definition. I have also devised a formula which includes $2$ cases: $f(n) = 0$ when $(n \bmod 3) = 1$ and $f(n) = 2^{\lceil n/3 \rceil}$ otherwise. Now, the exercise asks to prove the formula and I see that I have to do it considering the $2$ separate cases and I guess that I can use strong induction. In fact, I am trying to use strong induction to prove the  first case but I am not sure about how to do it. So, the proposition $P(n)$ can be: $f(n) = 0$ when $(n \bmod 3) = 1$ for $n = 1, 4, 7, 10,...$ The Basis Step can be $f(1) = 0$, $f(4) = 0$, $f(7) = 0$ and then would come the Inductive Step. My question is about the Basis Step, I am not sure if it is defined correctly or if I should use a kind of recursive expression which is the subject in the book. I will very much appreciate your advice.",['discrete-mathematics']
1192493,Understanding the proof of Jordan-Hölder Theorem.,"I need some help to understand the proof of this theorem which can be found in the book Introduction to Representation Theory by Pavel Etingof, Oleg Golberg, Sebastian Hensel, Tiankai Liu, Alex Schwendner, Dmitry Vaintrob, and Elena Yudovina. Let $V$ be a finite dimensional representation of $A$, and $0=V_0\subset V_1 \subset  ...\subset V_n=V$, $0=V_0'\subset V_1' \subset  ...\subset V_m'=V$ be filtrations of $V$, such that the representations $W_i:=V_i/V_{i-1}$ and $W_i':=V_i'/V_{i-1}'$ are irreducible for all i. Then $n=m $, and there exists a permutation $\sigma$ of 1, ..., n such that $W_{\sigma (i)}$ is isomorphic to $W_i'$. First proof: (for k of characteristic zero). The character of V obviously equals the sum of characters of $W_i$, and also the sum of characters of $W_i'$ But by Theorem 2.17, the characters of irreducible representations are linearly independent, so the multiplicity of every irreducible representation $W$ of $A$ among $W_i$ and among $W_i'$ are the same. This implies the theorem. Second proof: (general) The proof is by induction on $\text{dim}V$ . The base of induction is clear, so let us prove the induction step. If $W_1=W_1'$ (as subspaces), we are done, since by the induction assumption the theorem holds for $V/W_1$. So asume $W_1\neq W_1'$. In this case $W_1 \cap W_1' = 0$ (as $W_1, W_1'$ are irreducible), so we have an embedding $f: W_1 \oplus W_1' \rightarrow V$. Let $U=V/(W_1 \oplus W_1')$ and $0=U_0 \subset U_1 \subset ... \subset U_p=U$ be a filtration of $U$ with simple quotients $Z_i=U_1/U_{i-1}$ (it exists by Lemma 2.8 in the book). Then we see that: 1) $W/W_1$ has a filtration with successive quotients $W_1', Z_1, ...,Z_p$ and another filtration with successive quotients $W_2. ..., W_n$. 2) $W/W_1'$ has a filtration with successive quotients $W_1, Z_1, ...,Z_p$ and another filtration with successive quotients $W_2'. ..., W_n'$. By the induction assumption, this means that the collection of irreducible representations with multiplicities $W_1. W_1', Z_1, ..., Z_p$ coincides on one hand with $W_1, ..., W_n$, and on the other hand, with $W_1', ..., W_n'$. We are done. I'm trying to complete some details and understand the proof. With respect to the proof 1. In this proof they claim that the character of $V$ is equals to the sum of the characters of $W_i$, and also the characters of $W_i'$. This means that each $W_i$ is not isomorphic to $W_j$ with $i \neq j $ as representation of $V$. The same happen with the $W_i'$ representations. Now, in the book we have the following definition: Let $A$ be an algebra and $V$ a finite-dimensional representation of $A$ with action $\rho$. Then the character of $V$ is the linear function $\chi_{V}: A \rightarrow k$ given by $$\chi_{V}(a)=tr|_{V}(\rho(a))$$ On this point I understand that the characters of irreducible representations are linearly independent, but I have a problem with the conclusion. Why this is possible? There is a note that says the following This proof does not work in characteristic $p$ because it only implies that the multiplicities of $W_i$ and $W_i'$ are the same modulo $p$, which is not sufficient. In fact, the character of the representation $pV$, where $V$ is any representation, is zero. With respect to the second proof. The base induction. If $\text{dim}V=1$, then we have $V$ has the filtration $0=V_0 \subset V_1=V$ (the reason is $V=\langle v\rangle$ where $v \in V$) then obviously any representation has the same length because any element $v \in V$ generates $V$ and $V/0$ is the same $V$. Right? Finally, I want to understand how works the embedding $f$ in the case when $W_1\neq W_1'$ Thanks.","['ring-theory', 'representation-theory', 'abstract-algebra', 'group-theory', 'linear-algebra']"
1192512,Upper bound on the distance of orthogonal matrices,"Dear math stackexchange users, I have a question on orthogonal matrices: suppose I have a matrix $X\in\mathbb{R}^{n\times n}$ and I consider the orbit of the orthogonal group $O(n)$ acting from the left on $X$: 
$$Orb= \{Y\in\mathbb{R}^{n\times n}\ |\ Y=OX, O\in O(n)\}.$$
Now let $\epsilon>0$ and suppose I have $OX\in Orb$ satisfying $\|X-OX\|_F\leq \epsilon$. If $I$ is the identity matrix, is there any way to give an upper bound on the distance $\|I-O\|_F$ in terms of $\|X\|_F$ and $\epsilon$? Of course, if I have an upper bound on $\|I-O\|_F$, then by submultiplicativity of the Frobenius norm this gives an upper bound on $\|X-OX\|$. However, is there anything for the other direction? The orthogonal group is a compact set and this gives a trivial upper bound. This one however, is not applicable in my problem. Thank you very much!","['analytic-geometry', 'matrices', 'riemannian-geometry', 'matrix-calculus', 'differential-geometry']"
1192515,Bijection on Preordered Sets Implies Homeomorphism,"Prove that if $X$ and $Y$ are finite, then the ""converse"" of one of my other questions Homeomorphism on a Preordered Set is true: if $h: X \to Y$ is bijective and satisfies  $\forall a,b \in X, \quad  (a \trianglelefteq_{\mathscr{S}}b \iff h(a) \trianglelefteq_{\mathscr{T}}h(b))$, then $h$ is a homeomorphism. We are assuming that $h$ is bijective, i.e., both one-to-one and onto. Our goal is to arrive at the conclusion that $h$ is continuous and has a continuous inverse in order to label it as a homeomorphism. I read somewhere that because $X$ and $Y$ are finite, then one-to-one and onto are equivalent, but I do not want to claim this and call it a day. Instead, I would like a more detail-oriented approach: Since $h$ satisfies $a \trianglelefteq_{\mathscr{S}} b \iff h(a) \trianglelefteq_{\mathscr{T}} h(b)$, then if $a \in \overline{\{b\}}$ in $\mathscr{S}$, then $h(a) \in \overline{h(b)}$ in $\mathscr{T}$. So a closed set in one topology gets mapped to another closed set in a separate topology. Wouldn't this show that $h$ is continuous? As for the pre-image, could I apply $h^{-1}$ to the condition that is satisfied to get $h^{-1}(a) \trianglelefteq_{\mathscr{S}} h^{-1}(b) \iff a \trianglelefteq_{\mathscr{T}} b$ since $h$ is bijective, i.e., it has an inverse? Thank you in advance for reading this post and for any assistance provided, it is greatly appreciated.","['proof-writing', 'order-theory', 'general-topology', 'functions']"
1192524,Does free bimodule exist?,"Let $R,S$ be rings and $A$ be a set. Does there exist a free $(R,S)$-bimodule $F(A)$ on $A$? How do I construct it? Is it just $\oplus_{i\in A} (R\times S)$?","['abstract-algebra', 'modules']"
1192583,Why we cannot simplify $\partial x$?,"First consider the formula: $$\frac{df}{dt}=\frac{df}{dx}\frac{dx}{dt}$$ As we can see, $dx$ can be simplified from the RHS to get the LHS. This can be explained like this: define $y=x'(c)(t-c)+x(c)$ the tangent of $x(t)$ at $c$. Then $dx$ in $df/dx$ is simply $\Delta x$, while $dx$ in $\frac{dx}{dt}$ is $\Delta y$. And since $\Delta x\approx \Delta y$ when $\Delta t$ is very small, we can simplify $dx$ in the above formula. Now consider the formula: $$\frac{\partial f}{\partial u}=\frac{\partial f}{\partial x}\frac{\partial x}{\partial u}+\frac{\partial f}{\partial y}\frac{\partial y}{\partial u}$$ where $x,y$ are functions with variables $u,v$ and $f$ is function with variables $x,y$. Of course we cannot simplify $\partial x$. I tried to use the same argument for $dx$, but it is more difficult to imagine. Maybe I need some exact definition of $\partial f$ and $\partial x$ (just a guess, maybe the definition is related to a tangent plane?) Thanks so much.","['partial-derivative', 'calculus', 'multivariable-calculus', 'derivatives']"
1192587,"Counterexamples for the Converse of ""Topological Conjugacy Implies Equal Topological Entropy""","Question: I would like to find two topological dynamical systems that are not topologically conjugate but nevertheless have the same topological entropy. Two topological dynamical systems $f:X\to X,g:Y\to Y$ are topologically conjugate if there is a homeomorphism $H:X\to Y$ such that $f\circ H=H\circ g$. If $X$ is a compact metric space and $f:X\to X$ is continuous, then $d_n(x,y):=\max\{d(f^k(x),f^k(y))|0\leq k\leq n-1\}$; $\forall \varepsilon>0, N(n,\varepsilon):=\max\{|\{p_1,...,p_m\}|=m|i\neq j \implies d_n(p_i,p_j)\geq\varepsilon\}$ (i.e., $N(n,\varepsilon)$ is the largest number of points $p_1,...,p_m\in X$ such that $i\neq j\implies d_n(p_i,p_j)\geq\varepsilon$); and $h(f):=\lim_{\varepsilon\to0}\limsup_{n\to\infty}\dfrac{1}{n}N(n,\varepsilon)$ is the topological entropy of $f$ (This is the definition of Bowen and Dinaburg, and it requires a metric. For a definition that works for the general setting see wikipedia ). Motivation: We know that topological conjugacy preserves topological entropy. But the converse is not true: first example that comes to mind is that $h(\sigma^+_k)=\log k=h(\sigma_k)$ but $\sigma^+_k\not\sim \sigma_k$, where $\sigma^+_k:\Sigma^+_k\to \Sigma^+_k$ is the topological Bernoulli shift and $\sigma_k:\Sigma_k\to \Sigma_k$ is the full shift. But it is immediate that these two systems are not conjugate (full shift is bijective, the other is not). So I am more interested in finding a counterexample where both of the systems are invertible, though any other counterexamples, either that use metric spaces or topological spaces, are welcomed.","['metric-spaces', 'dynamical-systems', 'general-topology']"
1192660,Riemann-Roch theorem without heavy tools,"I have read two proofs of Riemann-Roch : one very quick in Forster, Lecture on Riemann Surfaces which use cohomology of sheaf, and results from functional analysis. Another one is in the book of Miranda about Riemann surfaces, which is more elementary, but use lot of intermediate results and especially snake lemma. Each time I'm reading one of these proofs, I just can't convince myself that is true because I have to believe these results in functionnal analysis, or the snake lemma. To me it looks really like powerful and a bit mysterious results (even it's probably a basic result for most of mathematician). So my question is Can we find a reasonably short proof of Riemann-Roch which not use homological algebra or functional analysis and which is ""almost"" elementary ? I'm aware that this theorem is quite powerful so we probably need a bit of work or powerful theorem. But, what is the most ""effective"" proof which use not too much big results ?",['algebraic-geometry']
1192662,How can I prove the following set theory question? (Intersection left distributes over difference),"I have $$(A-B)∩C = (A∩C)-(B∩C)$$ and $A-B = \{x ∈ A ∧ x ∉ B\}$                    definition of difference, $(A-B)∩C = \{x ∈ A ∧ x ∉ B ∧ x ∈ C\}$      definition of intersection And now I'm stuck on trying to make it equal to the left side or vice versa? Could someone lead me into the right direction using the $""\{x ∈ \ldots \text{and }x∉ \ldots\}""$ notation? Thank you very much.",['elementary-set-theory']
1192684,Sum of factorial fractions,Find the sum $$\sum\limits_{a=0}^{\infty}\sum\limits_{b=0}^{\infty}\sum\limits_{c=0}^{\infty}\frac{1}{(a+b+c)!}$$ I tried making something like a geometric series but couldn't. Then I couldn't think of anything.,"['factorial', 'summation', 'calculus']"
1192688,Reducing eigenvalues of symmetric PSD matrix towards 0: effect on ratios of original matrix elements?,"Let $\boldsymbol{S}$ be $k \times k$ positive semi-definite real symmetric matrix with eigen decomposition $\boldsymbol{S} = \boldsymbol{X} \boldsymbol{\Lambda} \boldsymbol{X}'$ ($\boldsymbol{\Lambda}$ diagonal, $\boldsymbol{X}$ orthonormal matrix of eigenvectors). Assume that we reduce each eigenvalue $\lambda_i$ by $\psi_i \in [0, \lambda_i]$, for $i = 1, \ldots, k$ with $\lambda_i$ sorted so that $\lambda_i \ge \lambda_{i+1}$. Define our ratio of interest $r_{ij}^\psi$ in terms of elements of the new matrix $\boldsymbol{S}^{\psi}$: $$ r_{ij}^\psi = \frac{s_{ij}^\psi}{\sqrt{s_{ii}^\psi}\sqrt{s_{jj}^\psi}} = \frac{\sum_{l=1}^k x_{il} x_{jl} (\lambda_l - \psi_l)}{\sqrt{\sum_{l=1}^k x_{il}^2 (\lambda_l - \psi_l)} \sqrt{\sum_{l=1}^k x_{jl}^2 (\lambda_l - \psi_l)}}. $$ How does $r_{ij}^\psi$ change as $\psi_i$ grows? In particular, I am interested in the case when $\psi_i \ge \psi_{i+1}$ for $i = 1, \ldots, k$. Initially my numerical experiments delivered an increase in absolute value, but now I have found the cases that yield a decrease instead. I can formulate a counterexample for some subset of parameter values using the fact that columns of $\boldsymbol{X}$ are length one and orthogonal to each other (e.g., when $\lambda_k \approx 0$), so strictly speaking I'm done; but I wonder if it can be shown more generally and elegantly. (If $r_{ij}^\psi$ is thought as a correlation coefficient, then this problem has direct relation to statistics.)","['eigenvalues-eigenvectors', 'statistics', 'linear-algebra']"
1192701,Is it true that $(f\cdot g)\circ \phi=(f\circ \phi)\cdot(g\circ \phi)$?,"Let be $f,g,\phi: \mathbb{R} \longrightarrow \mathbb{R}$ Is it true that $(f\cdot g)\circ \phi=(f\circ \phi)\cdot(g\circ \phi)$? ($\cdot$ is the product of functions) Thanks!",['functions']
1192729,Express $\sqrt{3}\sin\theta - \cos\theta$ as: $a\cos (\theta + \alpha) $,Express $\sqrt{3}\sin\theta - \cos\theta$ as: $a\cos (\theta + \alpha) $ Can someone please explain to me how to go about doing this?,['trigonometry']
1192771,Galois representations and isogenies of elliptic curves,"Let $E$ be an elliptic curve over $\mathbb{Q}$. For each prime $\ell$, the action of $\mathrm{Gal}(\bar{\mathbb{Q}}/\mathbb{Q})$ on $E[\ell]$ (the group of $\ell$-division points of $E$) defines a representation
$$\rho=\rho_\ell:\mathrm{Gal}(\bar{\mathbb{Q}}/\mathbb{Q}) \longrightarrow \mathrm{GL}(2,\mathbb{F}_\ell). $$ Then how to prove that $\rho$ is reducible if and only if $E$ admits an isogeny of degree $\ell$?","['elliptic-curves', 'algebraic-geometry', 'number-theory', 'galois-representations']"
1192775,Crossing of strings,There are two strings of color red and blue. They are made to cross each other odd number of times (greater than one) without any self crossing. Is it always possible that there will be pair of crossings which are adjacent on both the strings ?,"['planar-graphs', 'graph-theory', 'geometry', 'curves']"
1192798,Large numbers and CLT: confusion over the behavior of the sum of iid random variable,"In a nutshell I am confused about the fact that the fluctuations of the sum behave as $ \sqrt n $ but the empirical mean converges (fluctuations here behave as $  \frac {1}{\sqrt n} $). Below my reasons for the confusion and a few examples that puzzle me. In the simplest version of the law of large numbers the empirical mean of n independent and identical (iid) random variables converges in probability to the mean u of the underlying true distribution as n tends to infinity. Now, I am tempted to say that as the sampling mean converges we can also say that the sum of n iid random variables converges to $n \cdot u$ or if we don't want to use the word 'converge' because there is still n in the limit we can say that the sum behaves as $n \cdot u$ . Is this right? Does this mean, for 1-D RV, that for $ n \rightarrow \infty $ only the sequences $ \{x_1, x_2, .., x_n\} $ whose terms sum to $n \cdot u$  survive? In fact, using the central limit theorem, say that the RV X has finite variance, the probability distribution of the sum approximates a normal distribution with mean $ n \cdot u $ and variance $ \sigma^2 \cdot n$, which really means that fluctuations keep growing with n. This means that for example if I toss n times a fair coin it's not actually true that for n going to infinity I'll surely get n/2 times tails and n/2 time heads (because that would mean that the value of the sum was fixed). Is this right? This feels slighty strange to me, because the empirical mean will almost surely (almost surely in an informal sense here) be u and yet the sum won't be $n \cdot u$ . In the same spirit, I can express the law of large numbers saying that the empirical probability $ \sum \delta(x - x_i)/n $ converges to $ P(x_i)$. Then consider a 1-d random walk of a particle on the x axis with step either -1 and 1 with equal probability starting from the origin. The mean squared distance will behave as n in the limit. If I say,  though, that in the limit $ N \rightarrow  \infty \Rightarrow n_-1 \rightarrow n/2 $ and $ n_+1 \rightarrow n/2 $, respectively the number of occurrences of step -1 and step +1, it seems obvious that for $ n \rightarrow \infty\ $ the particle will be in the origin and it's hard to understand how the mean distance could possible behave as $ \sqrt n $. This is wrong, isn't? In general, given X RV and P(X) with X well-behaved enough for some version of the the law of large numbers and CLT, can I say that the number of occurrences $ N_x = P(x)\cdot N $ as $ N \rightarrow  \infty $? I notice that people say this in many situations, I myself have done so without really thinking about it in a fair amount of occasions, and yet I have doubts now because the fluctuations in the number of occurrences grow with N. I study physics and I'm studying a bit of serious statistics and probability as part of a statistical mechanics course (just so you know my background). In many situations in statistical mechanics the energy of the system is a sum of many independent terms and it would be a disaster if the fluctuations grew with the numbers of terms. This adds to my confusion. Thanks in advance for any responses.","['probability-theory', 'central-limit-theorem', 'probability-limit-theorems', 'law-of-large-numbers']"
1192808,"Confusion with a function being ""onto"" and 1-1 correspondence.","If we are given an onto function $f : A \to B$, then this ensures that every element of $B$ corresponds to something in $A$. But does this necessarily mean that the number of elements in set $B$ equates to that of the number of elements in set $A$? My understanding is the answer to this is ""no"". If my understanding is correct, then given a 1-1 correspondence between two sets $A$ and $B$ (i.e 1-1 and onto) does THIS then imply that the number of elements in sets $A$ and $B$ are equal? Also, assume that $A$ and $B$ are finite sets. I do not want to necessarily talk about cardinality of infinite domains.","['elementary-set-theory', 'functions']"
1192815,Functions in $L^p$ spaces,"If I have a function $f:\mathbb{R} \rightarrow \mathbb{R}$ that belongs to $L^p(\mathbb{R})$ for all $p\geq 2$ including $p=\infty$, that is
$$f \in \bigcap_{p\in [2,\infty]} L^p(\mathbb{R})$$
and all the norms have the same bound, let us say that for all $p\geq 2$
$$\|f\|_p \leq C$$
and
$$\|f\|_\infty \leq C$$ Can we conclude that $f$ is also in $L^p(\mathbb{R})$ for $1< p <2$ and that 
$$\|f\|_p \leq C?$$","['calculus', 'real-analysis', 'functional-analysis', 'integration', 'inequality']"
1192839,Find the fraction that creates a repeating decimal that repeats certain digits,"Is there any way to find the fraction $x/y$ that, when converted to a decimal, repeats a series of digits $z$? For example: ${x}/{y} = z.zzzzzzzz...$ or with actual numbers, $x/y = 234.234234234...$ (z is 234) If this is impossible, is there a way that does the same but the value to the left of the decimal is not $z$?","['arithmetic', 'fractions', 'decimal-expansion', 'algebra-precalculus']"
1192853,Combinatorics and Probability- where am I wrong?,"Let there be a cube with $n$ sides denoted $1,...,n$ each. The cube is tossed $n+1$ times. For $1\le k\le n$, what is the probability that exactly $k$ first tosses give different number (i.e, the $(k+1)$-st toss give a number that was already gotten.) I really need to know why I got a slightly different answer from the official one. My attempt: Let us build a uniform sample space. $\Omega=\{a_i=(i_1,...,i_k)|1\le i_j\le n\}$. $|\Omega|=(n+1)^n$, $\forall \omega\in \Omega, P(\omega)={1\over |\Omega|}$.
We seek for the event $A=\{(i_1,...,i_k,i_{k+1},...,i_{n+1})|i_t\ne i_s, \forall 1\le t\ne s\le k, k\in \{i_1,...,i_k\}\}$. This is the problematic part: $|A|={n\choose k}\cdot k!\cdot k \cdot n^{n-k-1} $. (Then I and the answer use the formula for probability of an even it a uniform sample space.) The point is, the answer says: $|A|={n\choose k}\cdot k!\cdot k \cdot n^{n-k} $. I don't understand why; First I pick $k$ numbers, count all their permutations, then pick one of them for the $(k+1)$-th toss, and then I have $n-k-1$ tosses left, each of which has $n$ possibilities. I would appreciate your help.","['probability', 'combinatorics']"
1192876,"Is there an algebraic non-rational extension of the integers, whose set of prime elements contains the prime integers?",Let the ring $\mathbb{Z}[\alpha]$ with $\alpha$ an algebraic number. Let $P(\mathbb{Z}[\alpha])$ be the set of all the prime elements of $\mathbb{Z}[\alpha]$. Question : Is there $\alpha$ algebraic and non-rational such that $P(\mathbb{Z}) \subset P(\mathbb{Z}[\alpha])$?,"['abstract-algebra', 'commutative-algebra', 'prime-numbers', 'ring-theory']"
1192892,Evaluating $\int\frac{1}{5\cos x+\sin x+7}~dx$,Evaluating  $$\int\frac{1}{5\cos x+\sin x+7}~dx.$$ This can be done by substituting $$\sin x = \frac{2t}{1 + t^2}$$ and $$\cos x = \frac{1 - t^2}{1 + t^2}.$$ However after I substitute it I cannot simplify it to get anything easier to integrate. After substituting I got: integral $$\frac{1 + t^2}{2(t + 2)(t + 3)}$$ or $$\frac{1 + t^2}{12 + 2t^2 + 10t}.$$ Could someone give me a hint? Many thanks.,['integration']
1192922,$S_{n+1}$ not isomorphic to subgroup of $S_n \times S_n$,"I've been asked to prove that there is no injective homomorphism from $S_{n+1}$ to $S_n \times S_n$ for $n\ge4$. This seems to me to follow from the fact that $S_{n+1}$ cannot be recognized as a direct product of two of its subgroups, essentially because it has only one normal subgroup.  Are there any ways to do this via order considerations, as this was my first impulse upon seeing the problem.","['abstract-algebra', 'group-theory', 'finite-groups']"
1192924,Calculating the intersection of two spaces of polynomials,"This problem is driving me nuts. I feel like there should be an elementary argument, yet I have failed to find one. Consider the vector space $V_n=\mathbb Q[x]/{x^{2n+1}}=\mathbb Q\{1,x,x^2,\ldots, x^{2n}\}$. Define polynomials $p_m=x^m+(-1-x)^m+(-1-x)^{2n-m}$. Consider the subspaces of $V_n$ given by $$I=\mathbb Q\{p_m\,:\, 0\leq m\leq 2n\},$$ $$J=\mathbb Q\{x^{2i}\,:\,0\leq i\leq n\}$$ and $$K=\mathbb Q\{x^{2i}-x^{2n-2i}\,:\,0\leq i\leq n\}.$$ I want to prove that $$I\cap J= K.$$ Inclusion in the reverse direction is clear: $p_m-p_{2n-m}=x^m-x^{2n-m}$. The hard part is showing the inclusion $I\cap J\subset K$. Computer calculations show this is true for all $n$ I have checked. Update: I have asked this question at mathoverflow. Update 2: This question now has an answer at mathoverflow.","['abstract-algebra', 'polynomials', 'vector-spaces', 'linear-algebra']"
1192932,What do we know if we know the determinant and trace of a matrix?,"Can we reconstruct an $n\times n$ matrix if all we know is the determinant and trace of that matrix (and its size: i.e. what $n$ is)?  I would think not, because two scalars wouldn't be enough to tell us about all $n^2$ entries.  But then this brings up three questions for me. What else do we need to specify a matrix exactly?  Obviously, I'm not talking about something for which we can immediately get back the matrix, like for instance, the transpose.  But what if we knew all of the eigenvectors and eigenvalues?  Or what if we know its Jordan form? If all we know is the trace and the determinant, what else can we figure out about the matrix?  Obviously we know whether it is invertible or not, but what about say the eigenvalues -- can we figure them out in the $n\ge 3$ case?  Can we figure something else out about the matrix? Are the trace and the determinant the only invariants of a matrix under change of basis? Basically, I'm just trying to find out exactly what information is encoded in the trace and determinant of a matrix. I know that the trace is the sum of the eigenvalues, the determinant is the product of the eigenvalues, and that the determinant of a matrix is the factor by which areas change under the linear transformation $x \mapsto Ax$, where $A$ is the matrix.  Is there anything thing else that knowing both of these invariants tell us?","['linear-algebra', 'matrices']"
1192949,What does this notation mean: $\mathbb{Z}_2$,"$\mathbb Z$ (Our usual notation for the integers) with a little subscript at the bottom. This is the question being asked: what are the subgroups of order $4$ of $\mathbb Z_2 \times\mathbb Z_4$ ($\mathbb Z_2$ cross $\mathbb Z_4$) Give them as sets and
identity the group of order 4 that each of the subgroup is isomorphic to I was thinking that it meant the set of integers modulo $4$ and modulo $2$, but I'm not too sure Give them as sets and
identity the group of order $4$ that each of the subgroup is isomorphic to What is the definition of ""order"". I couldn't really find that anywhere either.","['abstract-algebra', 'notation']"
1192984,Proving that every set in the ring generated by all rectangles can be covered by a finite disjoint union of rectangles,"Let $\mathcal{J}^n$ by the collection of all ""rectangles"" in $\mathbb{R}^n$, that is: $[[a,b))\in\mathcal{J}^n\iff [[a,b))=[a_1,b_1)\times[a_2,b_2)\times\cdots\times[a_n,b_n)$ where $a,b\in\mathbb{R}^n$ and $a_i\le b_i\ \forall i$ I will call such $[[a,b))$ ""rectangles"". $R(\mathcal{J}^n)$ denotes the ring generated by the collection of n dimensional rectangles. A ring is http://www.maths.kisogo.com/index.php?title=Ring_of_sets - simply put, a class of sets closed under set-subtraction and union. Ring generated by can be found at http://www.maths.kisogo.com/index.php?title=Ring_generated_by along with the proof that given $S\in R(\mathcal{J}^n)$ that there is a finite covering using sets in $\mathcal{J}^n$, that is: $$S=\bigcup^n_{i=1}[[a_i,b_i))$$ Question: I need to show that given a finite covering, there is a finite DISJOINT covering. this is obvious, but is difficult to prove. You can ignore everything below this line, it is just my proof of work Example Take the rectangle $[[0,5))\subset\mathbb{R}^2$ that is the ""square"" $\{(x,y)|0\le x< 5,\ 0\le y< 5\}$ It is easy to see $[[0,5))-[[1,6))=[[0,1))\cup [0,1)\times[1,5)\cup [1,5)\times [0,1)$ for example. Yet $[[0,5))-[[1,2))$ has yet more cases (8 infact), a cube less something inside of it has 26 chunks. What I think I must do I think I need to do something by induction, and consider $\mathcal{J}^n=\mathcal{J}^n\times\mathcal{J}^1$. What I have done I have shown this to be true for $\mathcal{J}^1$ (and could do it for any any specific n). How? Given a covering $\cup^m_{i=1}B_i$ where $B_i\in\mathcal{J}^1$ we may generate a disjoint covering as follows: Define $A_1=B_1$ and $A_n=B_n-\cup^{n-1}_{i=1}A_i$ (notice $\cup^n_{i=1}A_i=\cup^n_{i=1}B_i$) Let us proceed by induction: $A_1$ can be expressed directly as an interval in $\mathcal{J}^1$ Assume $A_n$ can be expressed as the union of disjoint members of $\mathcal{J}^1$, then $A_{n+1}=S_{n+1}-\bigcup^n_{i=1}A_i=S_{n+1}\cap[\cup_{i=1}^nA_i]^c$ Then WLOG you can order the (disjoint) intervals present in $\cup_{i=1}^nA_i$, then the complement takes the form $(-\infty,a_1)\cup[b_1,a_2)\cup\cdots\cup[b_{n-1},a_n)\cup[b_n,\infty)$ and as $S_{n+1}=[x,y)$ You don't include all the intervals whos endpoints are below $x$ you cut the one whos lower bound is $\le x$, you keep including while the upper bound is $< y$ then you cut the one which contains $y$, then you ignore the remainder. A subset of a finite set is finite, so we have (by induction) shown we can cover a finite covering by a finite collection of disjoint members from $\mathcal{J}^1$","['lebesgue-measure', 'real-analysis', 'measure-theory']"
1192992,How do you find the sine of the angle between two vectors?,I do not know what the sine of the angle between two vectors is. I think it may be the vector created by connecting the tips of the two vectors but I am not sure. How do you find the sine of the angle between two vectors?,"['vectors', 'angle', 'trigonometry']"
1193000,Transfer function for double cart system,"System: Define X2 = Y2; I've described the system with the following diff equation: 
$$f_{tot} = m_1\ddot{x_1} + k(x_2-x_1)+m_2\ddot{x_2}+B(\dot{x_2}-\dot{x_1})$$
where m1, m2, k and B are Cart mass 1, Cart mass 2, Spring constant and dampner constant respectively. All derivatives are in respect to time, $t$. I get the following after Laplace: 
$$F(s) = m_1s^2X_1(s)+k(X_2(s)-X_1(s)) + m2s^2X_2(s)+B(sX_2(s)+sX_1(s))$$ $$G(s) = \frac{Output}{Input} = \frac{X_2(s)}{F(s)} = ???$$ But the problem is that I can't define $X_1(s)$. Should $X_1(s)$ be interpreted as $1$? Is the above formula correct?","['control-theory', 'laplace-transform', 'ordinary-differential-equations']"
1193008,On diagonizability of commutating matrices,Let $A$ and $B$ be $n\times n$ matrices over the Galois Field of order $p$ ($p$ is a prime). Suppose that $A$ and $B$ are diagonizable matrices and that they commutate. Is it possible to make them simultaneously diagonizable in $GF(p)$? I know that when we have an algebraically closed field the things go fine. What in this case? I find somethings that lead me to think that this is true.,"['abstract-algebra', 'field-theory', 'finite-fields', 'matrices']"
1193018,How can I understand cohomology theories in the context of basic homology theory?,"Please pardon the ignorance in advance -- I'm doing research, trying to solve a specific problem, so naturally I'm led down paths in mathematics I never had the opportunity to study in depth. I understand the basic ideas of topology, and algebraic topology. For example, I understand the intuitive interpretation of $\pi_0(X)$ as the free group on $k$ letters, where $X$ has $k$ ""holes"" in it... and so on in higher dimensions to get all the homotopy groups. I then think of homology as the abelianization of those homotopy groups. This all makes sense intuitively. The reason I think I need to understand cohomology is because it sounds like it provides a means of assigning a notion of ""quantity"" or ""value"" to the elements of a given space. In particular, I'm going to be looking at the clique complex of a weighted graph, so there are many interesting ""quantities"" to look at. What I don't get is how this cohomology business actually works to get me there, and what I need to learn to finish off this problem. Now, it's my understanding that cohomology is the ""algebraic dualization"" of the concept of homology. And I sorta get that, and I've read how one can literally turn a chain complex into a cochain complex trivially. I also have noticed that, while there are an abundance of ""cohomology theories,"" there is only one notion of ""homology"" I've come across. If someone could help me put all this together in my head I would be super appreciative! And given knowledge of my use case, any hints on whether I'm off on the wrong tangent would be really helpful as well. TIA!!","['abstract-algebra', 'homology-cohomology', 'algebraic-topology']"
1193021,A function that is both open and closed but not continuous,"This does not have to be a very extravagant example just something that I can wrap my head around to have a concrete idea. I was thinking that this could be satisfied by the function $f: [0, 2\pi) \rightarrow B_1 = \{(x,y): x^2 + y^2 = 1\}$ $$
f(x) = (\cos(x),\sin(x))
$$ The point of discontinuity is obviously at $(1,0)$. I was hoping to get a little help convincing myself that this is both an open and closed mapping. (Still getting use to the definition). So, my definition as stated follows. Let $f:X \rightarrow Y$ be a function on the indicated spaces. Then $f$ is an $\bf{open}$ $\bf{function}$ or $\bf{open}$ $\bf{mapping}$ if for each open set $O$ in $X$, $f(O)$ is open in $Y$. The function $f$ is a $\bf{closed}$ $\bf{function}$ or $\bf{closed}$ $\bf{mapping}$ if for each closed set $C$ in $X$, $f(C)$ is closed in $Y$. My idea was along the lines of the following. Let $O$ be an open set in the interval $[0, 2\pi)$. Then $O = (a,b)$, $a>0, b<2\pi$.  Suppose $U = (0, 2\pi) = \cup O$. Where $U$ is the collection of all open sets $O$. Then $f(U)$ is the unit circle excluding the point $(1,0)$ Therefore an open set. Let $C$ be a closed set in the interval $[0, 2\pi)$. Let $\epsilon > 0$ be given. Then $C = [a, b]$ such that $a \geq 0$, $b \leq 2\pi- \epsilon$. Suppose $H = [0, 2\pi- \epsilon]= \cup C$. $H$ is the collection of all closed sets $C$. Then $f(H)$ is the approximately the unit circle radius 1 starting at the point $(1,0)$ ending and including a point before the point $(\cos(2\pi- \epsilon), \sin(2\pi- \epsilon))$. Therefore a closed set. Thus we have found a discontinuous, closed and open map.","['functional-analysis', 'general-topology']"
1193057,Showing that $\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)}$.,"If $f$ and $g$ are differentiable functions with $f(0) = g(0) = 0$ and $g'(0) \neq 0$, show that $\lim_{x \to 0}\frac{f(x)}{g(x)} = \frac{f'(0)}{g'(0)}$. I consider that perhaps: $$
\begin{align}
\\ \lim_{x \to 0}\frac{f(x)}{g(x)} &= \lim_{x \to 0}\frac{f(0+x) - f(0)}{x} \cdot \frac{1}{ \lim_{x \to 0}\frac{g(0+x) - g(0)}{x}} = f'(0) \cdot \frac{1}{g'(0)} = \frac{f'(0)}{g'(0)}
\end{align}
$$ But, it seems like that's maybe not quite right. I'm not certain. Insight?","['calculus', 'limits', 'proof-verification', 'real-analysis', 'derivatives']"
1193076,Product of character values,"From Isaac's character theory book; $3.12$ Let $x\in Irr(G)$ and $g,h\in G$ . Show that $$\chi(g)\chi(h)=\dfrac{\chi(1)}{|G|}\sum_{z\in G}\chi(gh^z)$$ I had thought that it was related to $3.9)$ ; $$K_iK_j=\sum_{v}a_{ijv}K_v$$ where $K_i$ is the sum of the $i$ th conjugacy classes and $$a_{ijv}=\dfrac{|\mathfrak{K_i}||\mathfrak{K_j}|}{|G|}\sum_{\chi\in Irr(G)}\dfrac{\chi(g_i)\chi(g_j)\overline{\chi(g_v)}}{\chi(1)}$$ But by using above equalites, I found that $$\chi(g)\chi(h)=\sum_{\chi\in Irr(G)}\dfrac{\chi(g)\chi(h)}{\chi(1)}$$ . I could not proceed more. Any help would be appreciated.","['characters', 'group-theory', 'representation-theory']"
1193084,Proof of squeeze theorem for functions,"Suppose for all $x$ we know $g(x)\le f(x)\le h(x)$ and $\lim_{x\to c} g(x)=L=\lim_{x\to c} h(x)$. Does the following argument work to conclude that $\lim_{x\to c} f(x)=L$? Let $\epsilon\gt 0$ be given. Then we can find a $\delta_1$ such that if $|x-c|\lt\delta_1$, then $|g(x)-L|\lt\epsilon$ and a $\delta_2$ such that if $|x-c|\lt\delta_2$ then $|h(x)-L|\lt\epsilon$. Let $\delta = min\{\delta_1,\delta_2\}$. Then for all x such that $|x-c|\lt\delta$, it follows that $|g(x)-L|\lt\epsilon$ and $|h(x)-L|\lt\epsilon$. This means that $L-\epsilon\lt g(x)\le h(x)\lt L+\epsilon$. But since $g(x)\le f(x)\le h(x)$,  $L-\epsilon\lt f(x) \lt L+\epsilon$. Hence $\lim_{x\to c}f(x)=L$. This proof is slightly different from others that I've seen, but it doesn't seem to be wrong. Is there anything that I'm missing?",['real-analysis']
1193108,"min(a + b,c) $\leq$ min(a,c) + min(b,c)?","Is the following always true ?  if we have min(a + b,c) $\leq$ min(a,c) + min(b,c) according to the cases I have analyzed it seems to be true but I want to double check. For a $\geq$ 0,b $\geq$ 0, c $\geq$ 0",['algebra-precalculus']
1193120,"Second order partial of $f(x,y)=\frac{xy(x^2-y^2)}{x^2+y^2}$ [duplicate]","This question already has an answer here : partial derivation 2nd order for a function defined by parts (1 answer) Closed 2 years ago . Consider the function $f(x,y)=\dfrac{xy(x^2-y^2)}{x^2+y^2}$ for $(x,y) \neq (0,0)$, $f=0$ otherwise. I have to compute $\dfrac{d^2f}{dydx}(0,0)$. I know that I have to calculate $\frac{df}{dx}$ first.
But that is , $\frac{df}{dx} = \frac{\partial f}{\partial y}\frac{dy}{dx}+\frac{\partial f}{\partial x}$.
When I put it in wolframalpha, it gives me this calculation, and also just $\frac{\partial f}{\partial x}$ as an alternative form. wolframalpha calculation link Why do I just ignore the y'? I don't know what y(x) is, as a function of x. 
y and x are independent functions, no?","['partial-derivative', 'multivariable-calculus']"
1193121,Bernstein's Theorem of Analytic Function Proof,"I'm studying from a textbook and came across an exercise to prove the following, which it calls the Bernstein's Theorem: If $f$ is infinitely differentiable on an interval $I$, and $f^n(x)\ge0$ for all $n\in\mathbb N$ and $x\in I$, then $f$ is analytic on $I$. I'm trying to find more information and/or a proof of it, though most of my search comes up with the Cantor-Bernstein or Schröder-Bernstein theorems, which don't seem to be the same as this. Is anyone more knowledgable of this proof?","['real-analysis', 'differential']"
1193223,Extension Lemma for Smooth maps (Lee vs. Lee),"I've been reading Jeffrey Lee's, Manifolds and Differential Geometry and John Lee's, Introduction to smooth manifolds . In the first book ( here , in page 31), after introducing partition of unity, there's an exercise that says: Exercise 1.74. Show that if a function is smooth on an arbitrary set $S\subset M$ as defined earlier, then it has a smooth extension to an open set that contains $S$. Where he says "" as defined earlier "", I assumed he meant that $M$ was paracompact, but maybe I missed something else. On the sencond book (check here , in page 45) there's the Extension Lemma for Smooth Functions, and it says: Lemma 2.26 (Extension Lemma for Smooth functions). Suppose $M$ is a smooth manifold with or without boundary, $A \subset M$ is a closed subset, and $f:A\to\Bbb R^k$ is a smooth function. For any open subset $U$ containing $A$, there exists a smooth function $\hat f:M\to\Bbb R^k$ such that $\hat f|_A=f$ and $supp(f) \subset U$. And after proving the lemma there is this exercise that made me very confused. Exercise 2.27. Give a counterexample to show that the conclusion of the extension
  lemma can be false if A is not closed. Doesn't exercise 2.27 imply that exercise 1.74 is incorrect? Are those trick questions? Like when you are asked to prove something right but it turns out is not? Or, more likely, did I missed something in Jeffrey Lee's book?","['differential-geometry', 'manifolds']"
1193262,"For a random variable $X$ such that $P(a<X<b)=1$, showing $E(X)E\left(\frac{1}{X}\right) \le\frac{(a+b)^2}{4ab}$","I've worked on the following problem and have a solution (included below), but I would like to know if there are any other solutions to this problem, particularly more elegant solutions that apply well known inequalities that I've overlooked. QUESTION : Suppose we have a random variable s.t. $P(a<X<b) =1$  where $0 < a < X < b$ , $a$ and $b$ both positive constants. Show that $$E(X)E\left(\frac{1}{X}\right) \le \frac{(a+b)^2}{4ab}$$ Hint :  find constant c and d s.t. $\frac{1}{x} \le cx+d$ when $a<x<b$, and argue that then we shall have $E(\frac{1}{X}) \le cE(X)+d$ MY SOLUTION : For a line $cx+d$ that cuts through $\frac{1}{X}$ at the points $x=a$ and $x = b$, it's easy to show that $ c = - \frac{1}{ab} $  and $d = \frac{a+b}{ab} $, $$ E\left(\frac{1}{X}\right) \le - \frac{1}{ab} E(X) + \frac{a+b}{ab} $$ $$ abE\left(\frac{1}{X}\right) + E(X) \le (a+b) $$ and because both sides of the inequality are positive, it follows that: $$ \left(abE\left(\frac{1}{X}\right) + E(X)\right)^2 \le (a+b)^2 $$ $$ (ab)^2E\left(\frac{1}{X}\right)^2 + 2abE\left(\frac{1}{X}\right)E(X) + E(X)^2  \le (a+b)^2 $$ Now, for the LHS, we can see that 
$2abE\left(\frac{1}{X}\right)E(X) \le (ab)^2E\left(\frac{1}{X}\right)^2 + E(X)^2$ because $0 \le (ab)^2E\left(\frac{1}{X}\right)^2 - 2ab\,E\left(\frac{1}{X}\right)E(X) + E(X)^2 = \left(ab\,E\left(\frac{1}{X}\right) - E(X)\right)^2 $ So, $$ 4ab\,E\left(\frac{1}{X}\right)E(X) \le (ab)^2E\left(\frac{1}{X}\right)^2 + 2ab\,E\left(\frac{1}{X}\right)E(X) + E(X)^2  \le (a+b)^2 $$ and therefore: $$ E\left(\frac{1}{X}\right)E(X) \le \frac{(a+b)^2}{4ab} $$  Q.E.D. Thanks for any additional solutions you might be able to provide.  Cheers!","['probability', 'functional-inequalities', 'expectation', 'inequality']"
1193304,Showing a complex analytic function is unbounded,"This was one of the problems on a previous year's Complex Analysis final exam. Assume $f\in \mathcal O (\mathbb H )$, non-constant, and $f(\frac {i}{\sqrt n})=0, \forall n\in \mathbb N$. Prove that $f$ takes unbounded values. What I tried so far: I tried to argue that the point $z=0$ had to be an essential singularity since the function cannot be continued to be holomorphic there (taking the value $0$), for then it would be forced to be identically the $0$ function (which it is assumed not to be). Then i squared the input domain to argue that the essential singularity must take on unbounded values in the upper-half plane somewhere near $z=0$. But, I think my reasoning is wrong because this may not be an isolated singularity at all and may be a point in the branch cut of a holomorphic function or something. I'm wondering if someone can write me up a nice proof and/or explanation about how to tackle this problem, thanks.","['singularity-theory', 'complex-analysis']"
1193331,Graphs of interesting integrals of the form: $\int \sin^a(x^a)\cos^a(x^a)$,"Here are a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$
Where $a$ is an even, positive integer. $a = 2$ $a = 4$ $a = 6$ Now, a few graphs of the form:- $$\int \sin^a(x^a)\cos^a(x^a)dx$$
Where $a$ is an odd, positive integer. $a = 1$ (Common) $a = 3$ $a = 5$ $a = 7$ The integrals themselves are hideous.. But the graphs are fun to observe (At least for me), especially for the cases where a is odd. I know that the difference in these graphs (Between odd and even $a$ ) is caused mainly by the powers the sin and cos functions are raised to, rather than the powers of the arguments of these functions. I have 4 questions :- Why does $\int \sin^a(x^a)\cos^a(x^a)dx$ , where $a$ is even, have fewer and fewer oscillations (i.e. the line becomes less wavy) as the value of $a$ increases? (Note: I know that the lines come closer and closer to the x-axis as evidenced by the outputs of the integral on the graph) What is going on when $a$ is odd? Can you explain why that beaker-like structure is formed; specifically, why does it seems to dip around a certain value ,then raise again to oscillate so much and the die out? Some questions regarding an observation of the case $a = 5$: Why does there seem to be a tiny bump just after the '4' on the x-axis? What value could be causing this, and why? And when $a = 7$, why do there seem to be tiny successive bumps beyond 2 and -2? I realize that answers may not exist for all of these questions, because these maybe the intrinsic qualities of the graph of the integral in discussion, but I'm hoping that some people may have insights and explanations for the features of the graphs I've questioned about.... Especially questions 3 and 4. I hope you don't consider the question silly; I'm genuinely interested in knowing the reasons (If they exist) for the above graphs.","['trigonometry', 'graphing-functions', 'calculus', 'integration']"
1193356,Number of unique binary strings containing at least m sequential 1s,"Let $Z\left(n,m\right)$ be the number of unique binary strings of length $m$ containing at least one instance of $n$ consecutive 1's. I am trying to come up with an expression for $Z$, preferably directly calculable though I will accept a recursive solution as well. I have attempted a formulation based on [1] ,
$$ \hat{Z}\left(n,m\right) = \sum_{q=m}^{n}\sum_{i=1}^{\lfloor \frac{q}{m}\rfloor}(-1)^{i+1}\binom{n-q+1}{i}\binom{n-mi}{n-q}\text{,}$$
however I am getting some discrepencies against test cases I worked out by hand. For example, it works for $Z\left(7,6\right)=3$ and $Z\left(7,5\right)=8$, but it does not work for $\left(7,4\right)=16$ (the formulation above gives $20$). N.B. : my definitions of $n$ and $m$ are opposite those of [1]; $q$ is the same. I believe it has something to do with double-counting some string permutations, but I haven't been able to work out what else I have to take out. Update : I found a recursive formulation [2] that gives me the same result as my $\hat{Z}$ above:
$$ \tilde{Z}\left(n,m\right) = 2\tilde{Z}\left(n-1,m\right) + 2^{n-m-1}-\tilde{Z}\left(n-m-1,m\right) $$
Having found this independent formulation, I will have to revisit my counting and see if I've made a mistake somewhere. Bonus points for an answer that works for arbitrary dictionaries, i.e. $W\left(a,n,m\right)$ where $a$ is the number of possible symbols in each position of the string. The original question would be equivalent to $Z\left(n,m\right) = W\left(2,n,m\right)$. [1] G.L., Number of binary strings containing at least n consecutive 1 [2] Gerry Myerson, response to Number of bit strings with 3 consecutive zeros or 4 consecutive 1s",['combinatorics']
1193380,What constitutes an outcome in probability?,"In probability, I often have trouble determining which situations to take as distinct outcomes for calculation. For instance, if we have a die with its faces numbered $1, 2, 2, 3, 3, 6$ and we roll it twice. We get $2$ on the first roll and $3$ on the second. Again rolling it twice we get $2$ and $3$. But the $2$ we got the second time is not the same $2$ as the first one. Its the other $2$ inscribed on the face of the die (the die has two $2$'s). So, for the purpose of calculating the probability that the sum of the two rolls in a die will be a certain number $4$, say, will these two situations constitute distinct outcomes? This is just a simplified, distilled example of a persistent problem I face in probability. Is there any way to think about outcomes that can make this clearer?",['probability']
1193398,Residue Theorem and Homologous to zero,"This is a very basic question and I couldn't find it posted yet but here it goes;
The Residue Theorem states that if $f:G\to \mathbb{C}$ is analytic on $G$- a region and $f$ has isolated singularities $b_1,...,b_k$ and $\gamma$ is homologous to 0, then $$\displaystyle\int_{\gamma}f=2\pi i\sum_1^kn(\gamma;b_s)\mathrm{Res}(f;b_s).$$ 
To my understanding, if $\gamma$ ""wraps"" around an isolated singularity, $b_i$, (say 1 time), then $\gamma$ would not be homologous to 0 since $n(\gamma;b_i)=1$ in this case ($b_i\in \mathbb{C}-G$). I think I need a better picture of the situation in this case. Thank you for your help!","['complex-analysis', 'homotopy-theory']"
1193422,Why hyperreal numbers are built so complicatedly?,I have seen approaches at building hyperreal systems by using complicated notions like ultrafilters and the like. Why not just postulate the existence of infinitesimal element $\varepsilon$ and infinite $\omega=1/\varepsilon$ like we do with complex numbers and build a field system around them?,"['abstract-algebra', 'number-systems', 'nonstandard-analysis']"
1193424,$ \int_{0}^1\frac{1+x^6}{1-x^2+x^4-x^6+x^8}dx$,"How do we compute this integral ?
$$ \int_{0}^1\frac{1+x^6}{1-x^2+x^4-x^6+x^8}dx$$
I have tried partial fraction but it is quite hard to factorize the denominator. Any help is appreciated.","['calculus', 'integration']"
1193432,"Prove that $\{\sin x, \sin 2x, ... , \sin nx\}$ is a linearly independent set","Prove that $\{\sin x, \sin 2x, ... , \sin nx\}$ is linearly independent. The short solution that I do not understand is as follow:
For p and q are positive integer, we have 
$$
\int\limits_{0}^{\pi}\sin{px}\sin{qx} dx=\left\{\begin{array}{l}0\qquad  \textrm{if } \, p\ne q\\
\dfrac{\pi}{2}\qquad \textrm{if }\, p=q\ne0\\
\end{array}
\right.
$$
Applying this result to show that if $\sum\limits_{k=1}^{n}\alpha_k\sin kx=0$ then $\alpha_k=0,\, k=1,\dots,n$ The solution is too short for me too understand. I would be grateful if you could explain this problem more in detail for me.","['lp-spaces', 'fourier-series', 'linear-algebra']"
1193442,Relation between residual spectrum and point spectrum.,"Suppose T is a bounded operator on a Hilbert space. Show that if λ is in the
residual spectrum of T, then $\bar{λ}$ is in the point spectrum of the adjoint. Here is what I think needs to be done.  We know that $\langle Tu,v\rangle = \langle u,T^*v\rangle = \overline{\langle T^*v,u\rangle}$.  Does that help with connecting $\lambda$ with $\overline{\lambda}$?","['spectral-theory', 'hilbert-spaces', 'functional-analysis']"
1193466,Tan inverse summation,$$S=\sum\limits_{i=1}^{4}\tan^{-1} x_i$$ How to simplify this ? I think I will have to use this : but it looks too long a method . Is there a method or symmetrical way which yields the answer quickly ? note : $x_i$ are the roots of a fourth degree polynomial so I know the sum and product of the roots,"['summation', 'inverse', 'trigonometry']"
1193492,If $\sin^{-1}\frac{2a}{1+a^2}-\cos^{-1}\frac{1-b^2}{1+b^2}=\tan^{-1}\frac{2x}{1-x^2}$ then what is value of x?,If $\sin^{-1}\frac{2a}{1+a^2}-\cos^{-1}\frac{1-b^2}{1+b^2}=\tan^{-1}\frac{2x}{1-x^2}$ then what is value of x? Solution $\tan^{-1}x=\tan^{-1}a-\tan^{-1}b=\tan^{-1}\frac{a-b}{1+ab}$ x=$\frac{a-b}{1+ab}$ i have a doubt what is $\tan^{-1}a$ and $\tan^{-1}b$ and how come $\tan^{-1}x=\tan^{-1}a-\tan^{-1}b$?,"['inverse', 'trigonometry']"
1193531,$\sum_{n=1}^{\infty} \frac{n^2}{ n!}$ equals [duplicate],This question already has answers here : Calculate sum of series $\sum \frac{n^2}{n!}$ [duplicate] (3 answers) Closed 9 years ago . $ \sum_{n=1}^{\infty} \frac{n^2}{ n!} $ equals I'm not able to convert in any standard series? Any hints?,"['sequences-and-series', 'calculus', 'limits', 'exponential-function']"
