question_id,title,body,tags
2781573,What should I use in order to get the required function $g \in L^1$?,"Let $f \in L^1$ be such that $f$ is not equivalent to any bounded function. Prove there exists a function $g \in L^1$ such that $fg \notin L^1$. I know that $m( \{x:|f(x)|>M\} )>0$, $\forall M>0$, but I can't see to come up with anything else useful to use for this problem in order to get the required function. Thank you for any hints as to how may I go about approaching this problem.","['lebesgue-measure', 'lebesgue-integral', 'measure-theory']"
2781594,Bijective Function from N to N x N [duplicate],"This question already has answers here : bijection between $\mathbb{N}$ and $\mathbb{N}\times\mathbb{N}$ [duplicate] (3 answers) Bijection between natural numbers $\mathbb{N}$ and natural plane $\mathbb{N} \times \mathbb{N}$ [duplicate] (1 answer) Closed 6 years ago . I have an idea but I don't know how to formalize my idea in a function. That's what n should give me back as (x,y): n = 0 -> (0,0)
n = 1 -> (1,0)
n = 2 -> (0,1)
n = 3 -> (2,0)
n = 4 -> (1,1)
n = 5 -> (0,2)
n = 6 -> (3,0)
n = 7 -> (2,1)
n = 8 -> (1,2)
n = 9 -> (0,3)
n = 10 -> (4,0) How do I formulate a function for this one?
I also need one for N -> N x {1,...,n) but I think it's the same function, is it?","['natural-numbers', 'functions']"
2781621,Interesting integral of $\frac {\log x}{\sqrt{1-x^2}}$,"Question: Evaluate$$I=\int\limits_0^{1/2}dx\,\frac {\log x}{\sqrt{1-x^2}}$$ I've given it the best I could. The first thing I did was use differentiation under the integral sign, but I wasn't sure where to continue after that. The next thing I did was try substituting $x\mapsto 2x$ to get the limits in terms of zero and one and transform $I$ into the beta function (no luck because the denominator is $1-4x^2$). Then I tried integration by parts. It seemed promising at first because I got$$I=-\frac {\pi}6-\int\limits_0^{1/2}dx\,\frac {\arcsin x}{x}$$But I'm not sure how to evaluate the second integral. I even tried using a Laplace Transform, but I'm again not sure how to evaluate the integral$$\mathcal{L}(f(t))=\int\limits_0^{\infty}dt\, e^{-st}\arcsin t$$ Can anybody give me a few hints as to where to begin and what to do? I'm out of ideas.","['integration', 'definite-integrals']"
2781624,How far can two symmetric matrices be from being non-commuting (in a suitable norm)?,"In some work I've been doing, I've come across a problem which involves a family of symmetric matrices $A_k\in\mathbb R^{n\times n}$ for $1\le k\le m$. The problem is very well-behaved when $A_k$ pair-wise commute, but things become harder if the $A_k$ are very far from commuting. What I would like to do, then, is to maximize
$$
 \| [A,B]\|_{HS}^2,\quad\hbox{ subject to } \|A\|_{HS},\|B\|_{HS}\le1
$$
where $\|\cdot\|_{HS}$ denotes the Hilbert-Schmidt norm, $\|A\|_{HS}^2=\sum_{i,j}A_{ij}^2=\operatorname{tr}(A^TA)$. I have found that for any maximal pair $(A,B)$, we must have that 
$$
\operatorname{tr}ACBB-\operatorname{tr}CBAB=\lambda\operatorname{tr}CA,
\quad
\operatorname{tr}AACB-\operatorname{tr}ABCA=\eta\operatorname{tr}CB
$$
for all $C\in\mathbb R^{n\times n}$, and $\lambda,\eta\ge0$ (this is equivalent to $(A,B)$ being a local maximum). This is equivalent to the condition that
$$
ABB-BAB=\lambda A,\quad
BAA-ABA=\eta B.
$$
But can this condition be reduced further? I'm really more interested in finding the maximum, than finding which $A,B$ attain it.","['matrices', 'convex-optimization', 'linear-algebra']"
2781664,Sylow subgroups/subgroup lattices in algebraic topology?,"One of my professors this semester very graciously offered to give me some projects to work on over the summer, and I happily accepted his offer. The only (minor) drawback is that my interests lie in topology, while his lie in Sylow subgroups and subgroup lattices (among other group theoretic topics). Algebra is my weakest subject, so it will be great to work more in it. However, I was wondering if anyone knew of any connections between Sylow subgroups and techniques in algebraic topology so that I could have some extra motivation when working through the various problems he gives me this summer! I truly appreciate any insight you may be able to offer!","['algebraic-topology', 'sylow-theory', 'group-theory']"
2781716,"Different ""eigenspaces"" of a module automorphism with non-trivial intersection","I'm messing around in the following setting: let $$\Bbb C' = \{a+bh \mid a,b \in \Bbb R, h \not\in \Bbb R, h^2=1\} \cong \frac{\Bbb R[x]}{(x^2-1)}$$be the ring of split-complex numbers. Define a linear operator $J: \Bbb R^2 \to \Bbb R^2$ by $J(e_1) = e_2$ and $J(e_2) =e_1$, where $(e_1,e_2)$ is the standard basis of $\Bbb R^2$. The split-complexification $\Bbb C' \otimes_{\Bbb R} \Bbb R^2 \cong (\Bbb C')^2$ has a $\Bbb C'$-module structure, and $(e_1,e_2)$ is a $\Bbb C'$-basis, great. We consider the unique $\Bbb C'$-linear extension of $J$ to $(\Bbb C')^2$, also denoted by $J$. The characteristic polynomial of $J$ is $x^2-1$, which has four roots in $\Bbb C'$: $1$, $-1$, $h$ and $-h$. Since $\Bbb C'$ is commutative, each ""eigenspace"" $E_\lambda$ is s submodule of $(\Bbb C')^2$. I have computed $$\begin{array}{|c|c|}
\hline {\boldsymbol \lambda} & \textbf{basis} \\  \hline  1 & (1,1) \\ \hline  -1 & (1,-1) \\ \hline h & (1,h) \\ \hline -h & (1,-h) \\ \hline
\end{array}$$ Every basis of $(\Bbb C')^2$ over $\Bbb C'$ must have the same cardinality, so clearly $(\Bbb C')^2$ cannot be the direct sum of these $E_\lambda$. Also: $$(1+h)(1,1) + (-1-h)(1,h) = (0,0).$$This is very disturbing to me. I do realize that $1+h$ and $1-h$ are the only zero divisors (up to real scaling) in $\Bbb C'$. If $$E_1 = \{(a+bh,a+bh) \mid a,b \in \Bbb R \}\quad\mbox{and}\quad E_h = \{(a+bh, b+ah) \mid a,b \in \Bbb R\},$$the above says that these ""eigenspaces"" intersect along a ""line"" $a=b$ (corresponding to the zero divisor). What is really going on here? Is there a ""better"" choice of decomposition for $(\Bbb C')^2$ here? Better yet, is there a way to develop ""spectral"" theory in this context of modules? I feel like I'm walking on eggs here. Thanks for any help!","['eigenvalues-eigenvectors', 'modules', 'abstract-algebra', 'free-modules', 'linear-algebra']"
2781728,Intuition for a Basis in Vector Spaces,"Some confusion about the basis vectors. These questions came from the definition of a basis at the bottom. I'm wondering: What the relation is between basis and non-basis vectors. Or put another way, if every single element in the vector space is linearly dependent on the linearly independent basis vectors (they are independent among themselves, but dependent amongst the other vectors). This is based on my understanding of the definition of span and the idea that every vector in the space is a linear combination of the basis vectors (except the basis vectors). I'm visualizing the span as: the basis vectors are all linearly independent, but then they ""connect"" to every other vector outside of the basis via linear combination. In that sense they reach every other vector, and so span the vector space. Wondering if that is correct. If the number of basis vectors is the dimension. Just want to make sure I'm understanding. What the relation between a basis and a generating set is. A brief intuition on how to think of basis vectors. A set of elements in a vector space is called a basis if they are: Linearly independent . And every vector in the vector space is a linear combination of the set (the basis vectors?). In another perspective, a basis is a linearly independent spanning set . Formally, a basis $B$ of a vector space $V$ over a field $F$ is a linearly independent subset of $V$ that spans $V$. A span of a set of vectors in $V$ is the intersection of all subspaces containing that set. In the definition of free module they compare a basis to a generating set .","['linear-algebra', 'vector-spaces']"
2781784,norm of a vector and linear transformation,"Let $A \in M_{n}(\mathbb{C})$ be nonsingular matrix. Then I m trying to find the dual norm of $\|x\|_{A} \triangleq\|Ax\|_{2}$. My try: \begin{align}
\|y\|_{A}^{D} &= \max_{\|x\|_{A}=1} |y^*x| \\
              &= \max _{\|Ax\|_{2}=1} |y^*x| \\
              &= \max _{\|Ax\|=1} |y^*A^{-1}Ax| \\
              &\leq \max _{\|Ax\|=1} \|y^*A^{-1}\|_{2} \|Ax\|_{2} \\
              &= \|y^*A^{-1}\|_{2}
\end{align} but now I don't know how to continue! Any idea, comment? Thanks","['matrices', 'linear-algebra']"
2781878,Proof Verification: Derivative theorem,"Suppose that $f: \mathbb{R} \to \mathbb{R}$ is differentiable at $c$ and that $f(c)=0$. Show that $g(x)=|f(x)|$is differentiable at $c$ iff $f'(c)=0$ $\implies$ Suppose $g'(c)$ exists Then we have that  for $x<c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \le 0$ and for $x>c$, $\lim_{x \to c}\frac{g(x)-g(c)}{x-c}$ =$\lim_{x \to c}\frac{|f(x)|}{x-c} \ge 0$ Then, by squeeze theorem, we have that $g'(c)=0$ Can anyone please tell me how to use the aforementioned result to get to the point where I can show that $f'(c)=0$? $\impliedby$ $f'(c) = 0 \implies \lim_{x \to c}\frac{f(x)}{x-c} =0$
$\implies |f'(c)|=0 = g'(c)$ Can anyone please verify this proof and also help arrive at the result that is emboldened? I'd also appreciate if you could lend some tips/hints to approach such questions and if there are easier ways to proof this. Thank you.","['derivatives', 'real-analysis', 'calculus', 'proof-verification', 'proof-explanation']"
2781904,Integrate $\sin^{-1}\frac{2x}{1+x^2}$,"Integrate $\sin^{-1}\frac{2x}{1+x^2}$ The solution is given in my reference as: $2x\tan^{-1}x-\log(1+x^2)+C$. But, is it a complete solution ? My Attempt $$
\int 2\tan^{-1}x \, dx=\int \tan^{-1}x \cdot 2\,dx=\tan^{-1}x\int2\,dx-\int\frac{1}{1+x^2}\int2\,dx\cdot dx\\
=\tan^{-1}x \cdot2x-\int\frac{2x}{1+x^2}\,dx=2x\tan^{-1}x-\log(1+x^2)+C
$$
$$
2\tan^{-1}x=\begin{cases}\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|\leq{1}\\
\pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x>0\\
-\pi-\sin^{-1}\frac{2x}{1+x^2}\text{ if }|x|>{1}\text{ and }x<0
\end{cases}\\
\sin^{-1}\frac{2x}{1+x^2}=\begin{cases}2\tan^{-1}x\text{ if }|x|\leq{1}\\
\pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x>0\\
-\pi-2\tan^{-1}x\text{ if }|x|>{1}\text{ and }x<0
\end{cases}
$$
$$
\int\sin^{-1}\frac{2x}{1+x^2}\,dx=\begin{cases}\int2\tan^{-1}x\,dx&\text{ if } |x|\leq{1}\\\int\pi\, dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x>0\\-\int\pi \,dx-\int2\tan^{-1}x\,dx&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}=\begin{cases}\color{red}{2x\tan^{-1}x-\log(1+x^2)+C\text{ if } |x|\leq{1}}\\\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x>0\\-\pi x-2x\tan^{-1}x+\log(1+x^2)+C&\text{ if }|x|>{1}&\text{ and } x<0\end{cases}$$ So don't we have two more cases for our solution rather than that is given in my reference, right ?","['trigonometry', 'inverse-function', 'calculus', 'indefinite-integrals', 'integration']"
2781913,Let $P(x)=x^2 -1$. Find out number of distinct real roots of $P(P(\cdots P(x))\cdots)=0$ where there are $2018$ $P$.,Let $P(x)=x^2 -1$. Find out number of distinct real roots of $P(P(\cdots P(x))\cdots)=0$ where there are $2018$ $P$. My Attempt Let $Q(x)=P(P(\cdots P(x))\cdots)=0$ where there are $2017$ $P$ is a root of $P(x)=0$. Thus $Q(x)=\pm1$. But from this point I am lost how to proceed. Can anyone help. Thanks in advance.,"['algebra-precalculus', 'polynomials']"
2781922,Solving a non-zero quadratic equation of two variables,"I have a problem where I am to find the intersection between the two curves
$$
\begin{cases}
x^2 - y^2 = 3 \\
xy = 2,
\end{cases}
$$
which I can easily see that the two points $\pm(2,1)$ are the real solutions to this problem, but I don't know how to solve for this systematically. I tried two approaches to solve for it as a quadratic equation: approach 1 (insert the second equation in the first): $$
4x^2 - 4y^2 + 4xy = 20 \Leftrightarrow (2x + y)^2 - 5y^2 = 20 \Leftrightarrow x=\dfrac{\pm\sqrt{5y^2 + 20} - y}{2}
$$ approach 2 (substitute $y$ for $x$): $$
x=\frac{2}{y} \Rightarrow x^2 - \frac{4}{x^2} = 3 \Leftrightarrow x^4 - 3x^2 - 4 = 0
$$ But I don't know how to continue from here. I found this math.stackexchange question where they solve a similar equation using these two approaches, but they don't end up with a constant under the root since they have $0$ on the right-hand side. Also, since that equation don't have any real solutions, I don't get how to apply it to this equation. How do I solve this kind of equation systematically? (not 100% sure what type of the equation it is) Edits: Corrected substitution from incorrectly substituting $y$ for $y$. I had arrived at this step earlier, but I still don't know how to solve the equation.","['algebra-precalculus', 'quadratics', 'quadratic-forms', 'systems-of-equations']"
2781988,Characterization of tail sigma algebra (and its complement),"I am not able to extract the concept of a tail $\sigma$-algebra from its definition. So, given a probability space $(\Omega,\mathcal{A},P)$ and sub-$\sigma$-algebrae $\mathcal{A}_1,\mathcal{A}_2,\dots$ (e.g. $\mathcal{A}_j:=\sigma(X_j)$ for r.v. $X_1,X_2,\dots$ on $\Omega$) I call $$\mathcal{C}_\infty:=\bigcap_{j=1}^\infty\sigma\left(\bigcup_{k=j}^\infty \mathcal{A}_k\right)$$ the tail $\sigma$-algebra. So how would I extract from this defintion the property that allows people  say ""this event is not part of $\mathcal{C}_\infty$ because it depends on $X_1$"" as I often read? There must be some assumptions for this statement to hold, e.g. $(X_j=X_1,\,j\in\mathbb{N})$ would disallow the statement (right?); I feel the $\mathcal{A}_1,\mathcal{A}_2,\dots$ must somehow ""separate"" the space? Can someone give me a criterion with proof that allows me talk conceptually about tail $\sigma$-algebrae? Thank you very much. (I don't have trouble to see why e.g. $\limsup$'s are in the tail $\sigma$-algebra, I rather have trouble with giving negative answers)","['probability-theory', 'probability', 'measure-theory', 'elementary-set-theory']"
2782010,Proving that a functional equation is constant,"A functional equation over ℝ is defined as 
$$
f(x+1/y) + f(x-1/y) = 2f(x)f(1/y) 
$$ and it's given that $f(0) = 0$. We have to prove that $f(x)=0$ for all real x. Substituting $y$ as $1/x$, we get that  $f(2x)= 2 f^2(x)$ but I'm stuck as to any further progress.","['functions', 'functional-equations']"
2782021,Quasi-Cauchy sequences,"Sequence $(x_n)$ is called quasi-Cauchy if $\lim_{n\rightarrow\infty}|x_{n+1}-x_n|=0.$
I need help proving the following theorems: Quasi-Cauchy sequence of real numbers is Cauchy if and only if it has exactly one cluster point. Sequence of real numbers is Cauchy if and only if every subsequence is quasi-Cauchy. I understand the implications to the right (they are trivial), but have trouble proving the opposite way. Any help would be appreciated :)","['real-analysis', 'cauchy-sequences', 'sequences-and-series']"
2782069,Abstract properties of the absolute Galois group over $\mathbb{Q}$,"Let $L := \mathrm{Gal}\Big(\overline{\mathbb{Q}}/\mathbb{Q}\Big)$ be the absolute group over the rationals. What are the most important and/or interesting abstract group theoretical properties of $L$ and $L^{ab} := L/[L,L]$, where $[L,L]$ is the first derived subgroup of $L$ (that is, the commutator subgroup of $L$) ?  For instance, what is known about torsion in both $L$ and $L^{ab}$ ? Are $L$ or $L^{ab}$ (related to) free groups ? Just to be clear: I am not asking about information about the various interesting actions of $L$ or $L^{ab}$ on geometric objects; rather, I am wondering about abstract group theoretical properties.","['abelian-groups', 'galois-theory', 'group-theory', 'galois-extensions']"
2782079,Probability of flipping a coin an exact number of times,"A biased coin flips heads with probability 4/9 and tails with probability 5/9. The coin is flipped 90 times. What is the probability that heads is flipped exactly 40 times? I believe this question requires the utilisation of the binomial distribution in which: $Pr(H) = \frac{4}{9}$ $Pr(T) = \frac{5}{9}$ $n = 90$ $k = 40$ $Pr(X = 40) = {90\choose 40}(\frac{4}{9})^{40}  (1 - \frac{4}{9}) ^{90-40} =
{90\choose 40}  (\frac{4}{9})^{40} (\frac{5}{9}) ^{50}$ is approximately 0.084 Am I going abouts this correctly? Not feeling too confident","['binomial-distribution', 'probability', 'discrete-mathematics']"
2782125,Probability distribution of faulty buses made during one week,"I've been trying to figure out the following question: I modelled the discrete random variable X with a binomial distribution such that: X ~ Bin(3, 3/5) I took the number of trials to be 3 because there can be between zero and three faulty buses ( successful trials ) on the Monday in question. I took the probability of having one faulty bus on a given day to be 3/5. I calculated this by taking the probability of one bus on one day to be 1/5 (because there are five days) and multiplying by three (because there are three faulty buses made that week). This gave me Pr(success) = 3/5 When I used the equation V(X) = npq (n=trials, p=success probability, q=failure probability) I found the variance of X to be equal to 18/25. ( npq = 3 x 3/5 x 2/5 = 18/25 ) In the answers for this question though, the variance is listed as being 12/25: I don't know what I've done wrong here. Have I missed out some fundamental bit of information? Is the answer sheet wrong? Any help is massively appreciated. Thank you for taking the time to read all of this :)","['statistics', 'binomial-distribution', 'probability', 'probability-distributions']"
2782139,How to find the indefinite integral of sin(sin(x))dx?,"(I got this function by mistake, when I miswrote other function. Now I'm curious how to find the antiderivative of what I miswrote) I have no a clue how to calculate it and neither does Wolfram Alpha or any other site that I tried. Trig formulas from school course don't seem to be useful too.","['indefinite-integrals', 'calculus']"
2782166,Formal Definition of an Algebraic Torus,"From what I can gather, an algebraic torus is an algebraic group defined over a field, which is isomorphic to ~something~ . I can't quite tell based on the definitions below what that something is exactly. Wondering if one could write a formal definition of an algebraic torus that explains some of the components in a little more detail. Wondering what the $\times$ is in $\mathbf {T} ({\overline {F}})\cong ({\overline {F}}^{\times })^{r}$. The Wikipedia one (4) has a few notation symbols I haven't seen before. (3) sort of makes sense but has a few notational symbols new as well. (2) provides some additional aspects. (1) is short but I also don't follow the notation. (5) is the closest to making sense. (1) Let $k$ be a field. An algebraic $k$-torus $T$ is an algebraic $k$-group such that over a (fixed) separable closure $\bar{k}$ of $k$ it becomes isomorphic to a direct product of $d$ copies of the multiplicative group: $$T \times_k \bar{k} \approx \mathbb{G}^d_{m,\bar{k}}$$ (here $d$ is the dimension of $T$). (2) Fix a field $k$, and let $k_s$ be a fixed separable closure of $k$. Let $\mathbb{A}^d$ denote $d$-dimensional
  affine space and let $\mathbb{G}_m$ denote the multiplicative group. If $V$ is a variety and $D$ is a finite set, write $$V^D := \bigoplus_{\delta\in D} V \approx V^{|D|}$$ If $D$ is a group, then $D$ acts on $V^D$ by permuting the summands. Write $$\mathbb{A}^D := (A^1)^D = \bigoplus_{\delta\in D}\mathbb{A}^1$$ If $G$ is a group and $H$ is a subgroup, define a norm map $\mathbf{N}_H : \mathbb{G}^G_m \to \mathbb{G}^{G/H}_m$ by $$(\alpha g)_{g\in G} \mapsto (\prod_{\gamma \in gH} \alpha_\gamma)_{gH\in G/H}$$ and let $$\mathbb{T}_G := \ker[\mathbb{G}^G_m \overset{\oplus\mathbf{N}_H}{\longrightarrow} \bigoplus_{1\neq H \subseteq G} \mathbb{G}^{G/H}_m]$$ An algebraic torus $T$ (over $k$) is an algebraic group defined over $k$ that is isomorphic over $k_s$ to $\mathbb{G}^d_m$, where $d$ is necessarily the dimension of $T$. If $k \subseteq L \subseteq k_s$ and $T$ is isomorphic to $\mathbb{G}^d_m$ over $L$, then one says that $L$ splits $T$. (3) We denote by $k^\ast$ the multiplicative group of non-zero elements of $k$ considered as an algebraic group over $k$. It is usually denoted by $G_m$ and is the affine algebraic group $Spec(k[t, t^{−1}])$ endowed with the comultiplication $t \to t \otimes t$ on the coordinate ring... An algebraic torus over $k$ is an algebraic group $T$ isomorphic to a finite direct product $k^\ast \times \dots \times k^\ast$. (4) An algebraic torus is a type of commutative affine algebraic group. Let $F$ be a field with algebraic closure ${\overline {F}}$. Then a $F$-torus is an algebraic group defined over $F$ which is isomorphic over ${\overline {F}}$ to a finite product of copies of the multiplicative group. In other words, if $\mathbf {T}$ is an $F$-group it is a torus if and only if $\mathbf {T} ({\overline {F}})\cong ({\overline {F}}^{\times })^{r}$ for some $r\geq 1$. (5) Let $G$ be a group. We say that $G$ is an algebraic group if $G$ is a quasi-projective variety and the two maps $m: G\times G \to G$ and $i: G \to G$, where $m$ is multiplication and $i$ is the inverse map, are both morphisms. The group $\mathbb{G}_m$ is $GL_1(K)$. Note that as a group $\mathbb{G}_m$ is the set of units in $K$ under multiplication. Let $G$ be an algebraic group. If $G$ is affine then we say that $G$ is a linear algebraic group . If $G$ is projective and connected then we say that $G$ is an abelian variety . The algebaic group $\mathbb{G}^k_m$ is called a torus. So a torus in algebraic geometry is just a product of copies of $\mathbb{G}_m$.",['algebraic-geometry']
2782167,Sequence of functions converge uniformly on every closed bounded interval,"Suppose $\{f_n\}$ is a sequence of functions on $\mathbb R$ and that for any $a,b\in\mathbb R$, the sequence of restricted functions $f_n |_{[a,b]}\rightarrow f_{[a,b]}$ uniformly. (My notation here is not great). Is it possible to use the $f_{[a,b]}$ and piece them together to find a function $f$ such that $f_n \rightarrow f$ uniformly on $\mathbb R$? I think it's true that $f_{[a,b]}$ and $f_{[c,d]}$ have to agree on the intersection of $[a,b]$ and $[c,d]$. Not sure whether this is enough though. It's reminding me a lot of presheaves. Is this possible? How does one (dis)prove it?","['functional-analysis', 'uniform-convergence', 'analysis']"
2782176,Eigenvalues of a complex $4 \times 4$ Matrix,"Let $A\in \mathbb{C}^{4×4}$. We are given the equation $$A^{4}+12A^{2}=6A^{3}+8A$$ and $$\textrm{rank}(A)=2\cdot\textrm{rank}(A-2I_{4})=4$$ I already found out that $2$ is an Eigenvalue, but how do I determine the other ones ? I'm mainly looking for hints.","['matrices', 'eigenvalues-eigenvectors', 'linear-algebra']"
2782203,How does the metric on the Poincaré half plane model work?,"Let $\mathbb{H} = \{ z = x+iy \in \mathbb{C} | \Im(z)=y>0\}$ be the upper half plane. I often see that $\mathbb{H}$ is endowed with a metric which is written as
$$ ds^2 = \frac{dx^2+dy^2}{y^2}. $$ I do not understand what $s, d(\cdot)$ mean and how one can measure the distance of the points on $\mathbb{H}$ with that formula. And why do we have to take squares? It seems like there is some background to that which I do not have. And I also do not know where to start. Could someone please explain me what all these things mean? Thank you!","['complex-analysis', 'metric-spaces', 'hyperbolic-geometry', 'geometry']"
2782224,"Show that if $Y$ is $\sigma (X)-$ measurable, there is a measurable function $f:\mathbb R\to \mathbb R$ s.t. $Y=f(X)$.","Show that if $Y$ is $\sigma (X)-$measurable, there is a measurable function $f:\mathbb R\to \mathbb R$ s.t. $Y=f(X)$. I really have problem to show that. It's clear that if $Y=f(X)$, then $Y$ is $\sigma (X)$ measurable. But the converse looks complicate. Attempt Step 1 : Let $Y=\boldsymbol 1_A$ for $A\in \sigma (X)$. In particular, there is a $B\in \mathcal B(\mathbb R):=\{borel\ set\ of\ \mathbb R\}$ s.t. $$X^{-1}(B)=A. $$ If I set, $$f(x)=\boldsymbol 1_B(x),$$
then $$Y(\omega )=\boldsymbol 1_A(\omega )=\boldsymbol 1_{X^{-1}(B)}(\omega )=\boldsymbol 1_{B}(X(\omega ))=f(X(\omega )).$$ Step 2 : If $Y$ is simple, the same argument work. Step 3 : If $Y$ is positive, there is a sequence $Y_n$ of simple function s.t. $Y_n\nearrow Y$. Let $(f_n)_n$ s.t. $$Y_n=f_n(X).$$
Since $Y_n$ are a.s. increasing, $(f_n(X(\omega ))_n$ is a.s. increasing. But how can I prove that is converge to $f(X(\omega ))$ for some measurable $f$ ? I just have that $(f_n(X(\omega )))_n$ converge to $f(X(\omega ))=Y(\omega )$, but it doesn't tell me that the function $f:\mathbb R\longrightarrow \mathbb R$ obtained is Lebesgue measurable.","['probability', 'measure-theory']"
2782228,Find the work done by a force field,"I am new to vector calculus and found this problem in the textbook which I am not sure how to work with, but I want to learn to do so: Show details to find the work done by the force field $\mathbf{F} =x^3\,\mathbf{i}+y^3\,\mathbf{j}$
  in moving an object from $P(1, 0)$ to $Q(2, 2).$","['multivariable-calculus', 'vectors', 'calculus']"
2782235,Is there a notion of minimal polynomial if the ring is not a field?,"If $\alpha\notin R$ but $\alpha$ is the root of a polynomial in $R[x]$ do we study the “minimal polynomial” over $R$ for that root? (Would we still want it to be with leading coefficient 1? irreducibility uniqueness and minimal degree would be kept I guess) For example if you consider $2/3\notin\Bbb Z$, the only polynomial over that comes to mind is $3x-2$","['abstract-algebra', 'minimal-polynomials', 'ring-theory', 'extension-field', 'field-theory']"
2782248,UMVUE and Cramér-Rao lower bound,"I'm trying to solve the following problem: Let $X_1 , \dots , X_n \sim$ Bernoulli$(\theta)$, iid. Find $T_n$, the UMVUE of $\theta(1-\theta)$, and show that Var($T_n$) does not attain the Cramér-Rao lower bound. My attempt to solve the problem was first find the sufficient statistic $\sum_{i=1}^{n}X_i \sim$ Binomial$(n,p)$. Then, I found the UMVUE to be $T_n = \dfrac{n}{n-1}\bar{X}(1-\bar{X})$, where $\bar{X} = \dfrac{\sum_{i=1}^{n}X_i}{n}$. The Cramér-Rao lower bound is: $\dfrac{(1-2\theta)^2}{n\left[\theta(1-\theta) \right]^{-1}}$ I got stuck on finding the Variance of $T_n$, since: $Var(T_n) = \left(\dfrac{n}{n-1}\right)^2 Var(\bar{X}(1-\bar{X})) = \left(\dfrac{n}{n-1}\right)^2 \left[\dfrac{\theta(1-\theta)}{n} + \dfrac{Var((\sum_{i=1}^{n}X_i)^2)}{n^2}\right]$ Is there a simpler way to find $Var((\sum_{i=1}^{n}X_i)^2)$ than taking the 4th derivative of m.g.f. to get $E(\sum_{i=1}^{n}X_i)^4$? Obs: this was a test question with limited time to solve.","['statistical-inference', 'density-function', 'probability-theory', 'probability', 'random-variables']"
2782300,Finding An Entire Function If There Is,"Find if there is an entire function such that  $|f(x+iy)|=5e^{xy}$ and $f(i)=5$ I have tried to build system of equations to find $x,y$ from   $|f(x+iy)|=5e^{xy}$ we get $x^2+y^2=25e^{2xy}$ but this has led me to nowhere",['complex-analysis']
2782326,A conference uses $4$ main languages. Prove that there is a language that at least $\dfrac{3}{5}$ of the delegates know.,"A conference uses $4$ main languages. Any two delegates always have a common language that they both know. Prove that there is a language that at least $\dfrac{3}{5}$ of the delegates know. Source: Romania TST 2002 My attempt: Suppose we have $n$ people and that $l_i$ people speak language $L_i$ . 
We want to prove that $l_i$ is at least $3n/5$ for some $i\in\{1,2,3,4\}$ . Say it is not true. Then we have $l_i<3n/5=:m$ for all $i$ .
Because of condition we have $$ {n\choose 2} \leq \sum_{i=1}^4{l_i\choose 2} < 4\times {m\choose
2}$$ From here we get $11n>35$ and there is no contradiction if $n>3$ . Update (1. 20. 2019) : I am now trying to develop the idea that was mentioned by Servaes in the commentary. Lemma: Among any $4$ delegates there are $3$ that speaks the same languague. Proof: If somebody speaks only one languague we are done. Say there is somenone that speaks exactly 2 languagues $L_1$ and $L_2$ . Then by pigeonhole out of 3 there are two in the same ""hole"" and we are done again. Suppose that everyone speaks at least 3 languague. Then by double counting between languagues and 4 people we have $$4\cdot 3\leq 4\cdot l\implies l\geq 3$$ where $l$ is maximum cardinality of $L_i$ in this double counting and we are done. $\square $ Any idea how to finish now?","['extremal-combinatorics', 'combinatorics', 'contest-math', 'algebraic-combinatorics', 'discrete-mathematics']"
2782346,What is the intuitive interpretation of the transpose compared to the inverse?,"I've been thinking about this question already for a long time and I've just encountered it again in the following lemma: $$f(x) = g(Ax + b) \implies \nabla f = A^T \nabla g(Ax + b) $$ This lemma makes intuitive sense if you think of it as taking the $x$ to the space $Ax$, calculating the gradient and then taking the result back to the original space. But why is ""taking the result back"" realised as $A^T$ and not $A^{-1}$? By doing the calculations you get $A^T$, no doubt, but I always expect an inverse. In general, when should I expect a transpose and when an inverse? Where are they similar and where do they differ?","['intuition', 'matrices', 'transpose', 'inverse', 'linear-algebra']"
2782393,Sum of $1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017}$,"I have this problem from a college exam: Let $\epsilon$ be a non-real root of unity of order 2018, find the sum$$S=1+4\epsilon +9\epsilon^2 +16\epsilon^3+...+2018^2 \epsilon^{2017}$$Here is my try. First I considered $$S_1=\sum_{k=0}^{2018} x^k=\frac{1-x^{2019}}{1-x}$$ Now I derivate one time then I multiply again by x to get: $$\sum_{k=0}^{2018} kx^k=\frac{x-x^{2020}-2019x^{2019}}{(1-x)^2}$$ And now I must derivate one more time and set $x=\epsilon$ in order to get the desired sum: $$S=\sum_{k=0}^{2018} k^2\epsilon^{k-1}=\frac{2019^2\epsilon^{2018}-2020\epsilon^{2020}-(2019^2-3\cdot2019-1)\epsilon^{2019}-\epsilon-1}{(1-\epsilon)^3}$$ And my final simplification to the numerator was:$$\epsilon(2019^2-2021)-2018\epsilon^2-2019^2-1$$ Now there were 5 answers given, and not a single one  was even close to this one. Out of luck because only 2 answer had $1-\epsilon$ in the denominator I have choosen the correct one, which was: $$S=\frac{2018(2018\epsilon-2020)}{(1-\epsilon)^2}$$ Can you help me to get that  answer?","['derivatives', 'summation', 'roots-of-unity']"
2782411,NURBS: Advice for specifying the first derivative at the endpoints of a C2 cubic spline interpolation.,"I am working through The NURBS Book by Piegl and Tiller.  In the book they give an efficient algorithm for computing the traditional $C^2$ cubic spline for global interpolation of arbitrary data points.  However, this algorithm requires that the user specify the first derivative (unit direction and magnitude) at each endpoint. Obviously this choice can significantly affect the way the curve turns out looking.  Is there any ""recommended"" choice for these first derivatives if we have no prior knowledge of what the data looks like that we are trying to interpolate?  I.e. is there a best choice for a very general interpolating algorithm?","['derivatives', 'spline', 'interpolation']"
2782446,Why is this integral equal to zero?,"The other day, I was helping some friend to study maths for an exam when we came across with this exercise: Let $\Omega\subset\mathbb R^3$ be a connected bounded subset with differentiable boundary $\partial\Omega$ . If the divergence of $F:\mathbb R^3\rightarrow \mathbb R^3$ is zero in $\Omega$ , then $$ \int_\Omega F^TJ^T F dx = 0 , $$ where $J$ means the jacobian of $F$ and $J^T$ the transpose matrix of $J$ . The exercise also suggests use integration by parts. To be honest, I have never seen some identity like such a one. I have looked at the classical divergence theorem, but I don't know how to apply it here. Also it seems that the identities for the curl and the divergence  can't work here. We have develop the integral expression, which can be written compactly as follows: $$F^TJ^TF=  \sum_i F_i^2\frac{\partial F_i}{\partial x_i} + \sum_{i,j\\i<j} F_iF_j\left(\frac{\partial F_i}{\partial x_j} + \frac{\partial F_j}{\partial x_i}\right) $$ The first sum is ( $1$ over $3$ times) the divergence of the vector field $F^3=(F_1^3,F_2^3,F_3^3)$ , so in this case we can use the divergence theorem. But I can't see how to use the fact that the divergence of $F$ is zero (clearly it doesn't imply that the divergence of $F^3$ is also $0$ ). May you give us some clue please? To be honest I'm very lost with that (and it is supposed I'm the savant of both...) PD: There is no tag for homework or something similar :(","['multivariable-calculus', 'definite-integrals']"
2782495,Show that $\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1}$ using the Mean Value Theorem,"Show that $\lim_{x\to\infty}x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{1}{m+1}$ using the Mean Value Theorem with m and x being whole numbers, for x greater or equal to 1 and m greater or equal to 0. My work so far: We define $f(x) := x^\frac{1}{m+1}$. From the MVT it follows that there exists an $x_0$ between a and b (with b > a) such that: $$\frac{f(b)-f(a)}{b-a} =  f'(x_0)$$ From choosing $b = x$ and $a = x - 1$ it follows that: $$x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1} = \frac{1}{(m+1)x_0^\frac{m}{m+1}}$$ Multiplication with $x^\frac{m}{m+1}$ results in: $$x^\frac{m}{m+1}(x^\frac{1}{m+1} - (x-1)^\frac{1}{m+1}) = \frac{x^\frac{m}{m+1}}{(m+1)x_0^\frac{m}{m+1}}$$ I intuitively want to take the limit to infinity of the left-hand side, but I can't justify why the $x$ and $x_0$ terms will cancel-out. I'd also to be happy to hear of any methods that don't use the MVT.","['derivatives', 'real-analysis', 'limits']"
2782555,Nested convex curves: inside curve strictly shorter,"Note: This question is very similar but different to a previous question [1] of mine. I decided to ask a new question for the sake of clarity. In order to complete a proof, I need to show the following (explained using figure attached). However, as this seems very intuitive to us, we seek a reference for the below statement (we already have a proof) : Given: We are given two points A and B in the two-dimensional plane connected by the direct black line. Points A and B are also connected by the green vectors (the corresponding green curve is convex). Additionally, points A and B are also connected by the red vectors. The corresponding red curve is convex goes from A to B ""on the same side"" as the green curve (with respect to the black line) goes ""outside"" the green curve (with respect to the black line), i.e. the area between the green and the red curve is strictly positive . Task: Prove that the length of the green line is strictly shorter than the red line. By looking at the picture this is intuitive. This can of course be proven by a lengthy algebraic calculation. However, we want to avoid that and want to refer to a reference for something that intuitive instead. Question: Can someone think about a reference (not a proof) of this statement? Some notes: We need to show that the green curve is strictly shorter than (not just shorter or equal to) the red line Based on our literature research, we find references [2] that only prove the ""shorter or equal""-version, but not the ""strictly shorter""-version. As found in [1], this question is related to the Crofton formula . The wikipedia article [3] states that it can be used to show that Between two nested, convex, closed curves, the inner one is shorter. However, no further citation to that statement is given and we can't find one either. Furthermore, I am not sure whether ""shorter"" means indeed ""strictly shorter"" or just ""shorter or equal to"". Many thanks [1] Result needed: outside curve longer than convex inside curve [2] Proposition 5.6 here: https://books.google.co.uk/books?id=KMZl_iQR5IUC&pg=PA32&lpg=PA32&dq=%22nested+convex+bodies%22+perimeter&source=bl&ots=Ue4_krxSwz&sig=QZDDEASxt3_I4kSSlVbyplHpzmM&hl=en&sa=X&ved=0ahUKEwj9vpTY_cjaAhVGVSwKHWTDBEMQ6AEIPDAD#v=onepage&q=%22nested%20convex%20bodies%22%20perimeter&f=false [3] Application section in: https://en.wikipedia.org/wiki/Crofton_formula","['curves', 'real-analysis', 'linear-algebra', 'algebraic-geometry']"
2782603,Total derivative vanishes implies function is constant,"I came across this problem: Let $f: \mathbb{R}^n \to \mathbb{R}^m$ be a totally differentiable function, whose derivative $Df$ vanishes for all $x\in \mathbb{R}^n$. Show that $f$ is constant. Hint: Consider the line segment $L$ connecting two arbitrary points $x_{0},y_{0}\in\mathbb{R}^n$ and show that for suitable points $x_{i},y_{i}\in L$, $|f(x_{i})-f(y_{i})|$ is sufficiently small. Use compactness to proof the statement. We aren't supposed to use partial derivatives like this: Functions where the total derivative is zero .
Also, we don't have a mean value theorem for multi variable functions and we didn't introduce connectedness yet (as used in the second answer above). For the first part of the hint, one could use continuity of $f$ to force $|f(x_{i})-f(y_{i})|$ small enough. But this would require $\delta$ to be (very) small aswell.
Regarding compactness, my first guess was to define a sequence of points $\in L$ and use sequential compactness in some sense.
None of my thoughts worked out so far. Regards
cerocius","['derivatives', 'calculus', 'multivariable-calculus', 'compactness', 'analysis']"
2782637,"Helly's Theorem Doesn't Hold for $L^\infty[0,1]$","Problem: Show that the conclusion is not true for $X=L^\infty[0,1]$. Here is my attempt, which I now realize is incorrect: Consider $f_n \in L^1[0,1] = Y$ defined by $f_n = n \cdot 1_{[0,1/n]}$. I already showed that no subsequence of $\{f_n\}$ converges weakly in $Y$. Let $T_n : X \to \Bbb{R}$ be defined by $T_n(f) = \int_X f \cdot f_n$. It's not hard to show $\{T_n\}$ is a bounded sequence. Hence, if Helly's theorem were true, there would exist a subsequence $\{T_{n_k}\}$ and $T \in X^*$ s.t. $T(f) = \lim_{k \to \infty} T_{n_k}(f)$. But $T(f) = \int_X f g$ for some $g \in Y$ and therefore $$\lim_{k \to \infty} \int_X f \cdot f_{n_k} = \int_X fg$$ for every $f \in X$. This means that $f_{n_k} \to g$ weakly in $Y$, which is a contradiction. As you may have noticed, I wrongly thought that the Riesz Representation theorem holds for $L^\infty[0,1]$...such a nice, quick solution, too...Is there anyway of fixing it? I found this question asked elsewhere on MSE, but the proposed solution uses the Hahn-Banach theorem, which is currently not at my disposal.","['weak-convergence', 'real-analysis', 'measure-theory']"
2782669,the de Rham poincare duals of the unit circle in the punctured plane.,"I'm reading from Bott-Tu's Differential forms in algebraic topology , and came across the following exercise: (p52 Exercise 5.16(b)) Let $S$ be the unit circle in the plane, and $M := \mathbb{R}^2-\{0\}$. Show that the closed Poincare dual of the unit circle $i : S\hookrightarrow M$ in $H^1(M)$ is 0, but the compact Poincare dual is the nontrivial generator $\rho(r)dr$ in $H^1_c(M)$, where $\rho(r)$ is a bump function with total integral 1 (By a bump function we mean a smooth function whose support is contained in some disc and whose graph looks like a ""bump""). (My question is near the bottom) To compute the closed dual, we must check that there is a closed form $\eta = \eta_r dr + \eta_\theta d\theta$ on $M$ such that for every closed compactly supported form $\omega = \omega_rdr + \omega_\theta d\theta$ on $M$, we have
$$\int_S i^*\omega = \int_M\omega\wedge\eta$$
However, since $\omega$ is compactly supported and closed, realizing $S$ as the boundary of the punctured unit disk, Stoke's theorem tells us that $\int_S i^*\omega = 0$, so we may pick $\eta = 0$. To compute the compact dual, we must now find a closed compactly supported form $\eta' = \eta_r' dr + \eta_\theta' d\theta$ on $M$ such that for every closed (not necessarily compactly supported) form $\omega = \omega_rdr + \omega_\theta d\theta$, we have again: $$\int_S i^*\omega = \int_M\omega\wedge\eta$$ If we assume $\eta' = \rho(r)dr$ as in the exercise, we wish to check:
$$\int_S i^*\omega = \int_0^{2\pi}\omega_\theta(1,\theta)d\theta = \int_\epsilon^{\epsilon'}\rho(r)\left(\int_0^{2\pi}\omega_\theta(r,\theta)d\theta \right)dr$$
where the support of $\rho(r)$ is contained in the annuli of radii $\epsilon,\epsilon'$.
Again, applying Stoke's theorem to the closed annulus with radii $R_1,R_2$, we find that the integral $\int_{0}^{2\pi}\omega_\theta(r,\theta)d\theta$ is independent of the choice of $r$. Thus, $\rho(r)dr$ has total integral 1, the equality is checked as desired. My question is: Suppose we didn't know to guess $\eta' = \rho(r)dr$. How would we have deduced it? Ie, let us just assume that $\eta' = \eta'_rdr + \eta'_\theta d\theta$, with support on the annulus of radii $\epsilon,\epsilon'$. Thus we want to find $\eta'_r,\eta'_\theta$ such that:
$$\int_0^{2\pi}\omega_\theta(1,\theta)d\theta =  \int_0^{2\pi}\int_\epsilon^{\epsilon'}\omega_r\eta'_\theta drd\theta - \int_0^{2\pi}\int_\epsilon^{\epsilon'}\omega_\theta\eta'_r drd\theta$$
The fact that $\omega,\eta$ are closed is the same as saying that $\frac{\partial\omega_r}{\partial\theta} = \frac{\partial\omega_\theta}{\partial r}$ and $\frac{\partial\eta'_r}{\partial\theta} = \frac{\partial\eta'_\theta}{\partial r}$. How can we deduce that we may pick $\eta'$ to be of the form $\eta' = \rho(r)dr$?",['differential-geometry']
2782717,What exactly is a matrix?,"I know how basic operations are performed on matrices, I can do transformations, find inverses, etc. But now that I think about it, I actually don't ""understand"" or know what I've been doing all this time. Our teacher made us memorise some rules and I've been following it like a machine. So what exactly is a matrix? And what is a determinant? What do they represent? Is there a geometrical interpretation? How are they used? Or, rather, what for are they used? How do I understand the ""properties"" of matrix? I just don't wanna mindlessly cram all those properties, I want to understand them better. Any links, which would improve my understanding towards determinants and matrices? Please use simpler words. Thanks :)","['matrices', 'linear-algebra', 'soft-question']"
2782737,Does the orientation on a product of manifolds depend on the order of the product?,"I'm learning differential geometry for the first time. I'm sure this question is answered in a number of books, but it doesn't seem to be addressed in any book I have. Let $M,N$ be orientable manifolds of dimension $m,n$. I'd like to view an orientation on $M$ as given by a choice of a nowhere vanishing top differential form. The product maps
$$N\leftarrow M\times N\rightarrow M$$
give maps $\Omega^n(N)\rightarrow \Omega^n(M\times N)$ and $\Omega^m(M)\rightarrow\Omega^m(M\times N)$. Thus given an orientation $n$-form $\omega_N$ on $N$ and an orientation $m$-form $\omega_M$ on $M$, we get two natural choices of orientation forms on $M\times N$, namely $\omega_N\wedge \omega_M$ and $\omega_M\wedge\omega_N$. They are the same if and only if $M,N$ are both even-dimensional. (Here I don't want to distinguish $M\times N$ from $N\times M$). In particular, if either $M$ or $N$ are odd-dimensional, then this doesn't seem to give a canonical choice of an orientation on the product $M\times N$. Would it be correct to say that the ""product orientation"" on the product of two oriented manifolds only exists if both manifolds are even dimensional? Ie, if one of the manifolds is odd-dimensional, then there is no canonical choice of an orientation on the product?",['differential-geometry']
2782767,A case contains 5 red-wrapper chocolate and 10 white-wrapper chocolate. We randomly choose 8 chocolates at random without replacement.,"How likely is it that (a) 4 chocolates are red-wrapped? $\binom{5}{4}\binom{10}{4}\over\binom{15}{5}$ (b) all chocolates are white-wrapped? $\frac{10\times9\times8\times7\times6\times5\times4\times3}{15\times14\times13\times12\times11\times10\times9\times8}$ (c) at least one chocolate is red-wrapped? $\frac{C(10,8)}{C(15,8)}= \frac{45}{6435}$ $P=1-\frac{45}{6435}$ Have I done it correctly? Not sure how to go about it so I just followed the examples from the book. Would appreciate it if anyone could point out my mistakes.","['combinations', 'probability']"
2782784,Is $\vec{F}\cdot \hat{n}$ zero on the boundary?,"I'm trying to do the following exercise Let $\Omega\subset\mathbb R^3$ be a connected bounded subset with differentiable boundary $\partial\Omega$ . If the divergence of $F:\mathbb R^3 \rightarrow \mathbb R^3$ is zero in $\Omega$ , then $$ \int_\Omega F^TJ^TFdx=0, $$ where $J$ means the jacobian of $F$ and $J^T$ the transpose matrix of $J$ . I have asked a solution for it here and @MarkViola has proved that the integral is equal to $$\int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS , $$ where $n$ is the normal vector to the surface defined by the boundary. Now, why is this integral also zero??? I can see that if $\operatorname{div}F=0$ on $\Omega$ then $$\int_{\partial \Omega} F\cdot n \, dS = 0. $$ But, does it imply that $F\cdot n =0$ on the boundary? If not, why the integral $$\int_{\partial\Omega} (F\cdot F)(F\cdot n) \, dS $$ is equal to zero? Remark I should remark that @Mark also gave an example for which $F\cdot n$ is not zero on the boundary and the integral is. The only problem I see in that case is that the boundary is $C^1$ a.e. but not actually $C^1$ .","['multivariable-calculus', 'definite-integrals']"
2782814,Why are skew-symmetric matrices of interest?,"I am currently following a course on nonlinear algebra (topics include varieties, elimination, linear spaces, grassmannians etc.). Especially in the exercises we work a lot with skew-symmetric matrices, however, I do not yet understand why they are of such importance. So my question is: How do skew-symmetric matrices tie in with the topics mentioned above, and also, where else in mathematics would we be interested in them and why?","['algebraic-geometry', 'matrices', 'reference-request', 'skew-symmetric-matrices', 'soft-question']"
2782858,Schwarz's Theorem and Discontinuous Second Derivatives,Let $f:\mathbb{R}^2 \to \mathbb{R}$ be twice differentiable in $a$. Suppose that $\frac{\partial^2f}{\partial x \partial y}$ is continuous in $a$. Is it possible that $\frac{\partial^2 f}{\partial y \partial x}$ is discontinuous in $a$? The question is motivated by Schwarz' Theorem - as this observation would show that it would not help at merely computing the derivatives because mostly we would not know whether both second derivates are continuous. And that's needed to apply Schwarz (at least in the way I know the theorem).,"['multivariable-calculus', 'real-analysis']"
2782860,UFD iff PID in Dedekind domain.,Let $A$ be a Dedekind domain. PID implies UFD. So for the other direction assume $A$ is an UFD. In this proof the author only considers prime ideals instead of any proper ideal. Why is this sufficient?,"['abstract-algebra', 'algebraic-number-theory']"
2782868,Finding closed form of $\int_{-\infty}^{\infty}\frac{dx}{x^{2n}+1}$,"So, I would like to find a closed form (if it exists?) for the following integral:
$$\int_{-\infty}^{\infty}\frac{dx}{x^{2n}+1}$$ for $n\in\Bbb{N}$. I tried complex analysis approach. Define function $$f(z)=\frac{1}{z^{2n}+1}$$
$f$ has simple poles at $z_k=e^{i\frac{\pi+2k\pi}{2n}}$. Define contour $\gamma$ to be a half circle in the upper part of the complex plane: $\gamma:(0,\pi)\rightarrow\Bbb{C},z=Re^{i\psi}$, where we will let $R\to\infty$ later on. By residue theorem we have:
$$\int_{-R}^{R}f(x)dx+\int_\gamma f(z)dz=2 \pi i\sum_{k=0}^{n-1}\operatorname{Res}(f,z_k)$$ Now I can factor the polynomial denominator of the function $f(z)$:
$$f(z)=\prod_{l=0}^{2n-1}\frac{1}{z-z_l}$$
So $$\sum_{k=0}^{n-1}\operatorname{Res}(f,z_k)=\sum_{k=0}^{n-1}\prod_{l=0,l\neq k}^{2n-1}\frac{1}{z-z_l}$$
But from this point, I have no idea how to proceed, not even if this is the correct approach. Any useful hints, tips, links are appreciated, thanks in advance.","['complex-analysis', 'improper-integrals', 'definite-integrals', 'closed-form']"
2783000,"Recreational math: If $f(f(x))=e^x$, bound the integral $\int_0^1 f(x)dx$","I've been studying functions $f:\mathbb R\to\mathbb R$ that satisfy $f(f(x))=e^x$ (or, half-iterates of the exponential function). I know that there's only one such analytic function, but it's really hard to study since it is almost certainly non-elementary and I only know how to find finitely many terms of its Maclaurin Series. Instead, I'm studying all continuous and increasing functions $f$ satisfying $f(f(x))=e^x$, and I've alighted on the following problem (which I came up with out of curiosity). I propose this question to all interested residents of MSE: Given that $f$ is continuous and increasing and $f(f(x))=e^x$, find some bounds for the integral
  $$\int_0^1 f(x)dx$$ I've managed to come up with some pretty sweet bounds (in fact, they are the best possible bounds ), and I'll post them after this question gets some answers. I'll accept whatever answer has the tightest bounds, with a proof. NOTE: Most people probably wouldn't think of this as recreational ""fun"" math, but hey, I did it for fun, and I'm proposing it as a problem to be done just for fun. So please try to enjoy it, and please don't try to close it.","['functional-equations', 'integration', 'definite-integrals', 'recreational-mathematics', 'upper-lower-bounds']"
2783004,"Integration using ""method of judicious guessing""","My calculus-book gives an example of integration using the method of judicious guessing. But I do not intuit the method very well. QUESTION: How does the derivative of $f_{mn}(x)$ ""suggest that we try"" $I=Px^4\left(\log {x}\right)^2 +Qx^4\log{x}+Rx^4+C$? Where does this trial formula come from? The example goes as follows: Find the derivative of $f_{mn}(x)=x^m\left(\log {x}\right)^n$ and use the result to suggest a trial formula for $I=\int x^3\left(\log {x}\right)^2dx$. Thus evaluate this integral. Solution: We have
  $$f'_{mn}(x)=mx^{m-1}\left(\log {x}\right)^n+nx^{m-1}\left(\log {x}\right)^{n-1}.$$
  This suggests that we try
  $$I=Px^4\left(\log {x}\right)^2+Qx^4\log{x}+Rx^4+C$$
  for constants $P$, $Q$, $R$ and $C$. Differentiating we get
  $$\frac{dI}{dx} = 4Px^3\left(\log {x}\right)^2 + 2Px^3\log{x} + 4Qx^3\log{x} + Qx^3 + 4Rx^3 = x^3\left(\log {x}\right)^2,$$
  solving for $P$, $Q$ and $R$ we arrive at the right answer:
  $$\int x^3\left(\log {x}\right)^2dx=\frac{1}{4}x^4\left(\log {x}\right)^2-\frac{1}{8}x^4\log{x}+\frac{1}{32}x^4+C.$$ Please note my level of mathematics is still ""in development"": I am learning without a teacher. BACKGROUND: In my efforts I did notice the following, which also results in the right answer:
$$\frac{d}{dx}x^m\left(\log {x}\right)^n=mx^{m-1}\left(\log {x}\right)^n+nx^{m-1}\left(\log {x}\right)^{n-1}.$$
Integrating both sides we get:
$$x^m\left(\log {x}\right)^n=m\int x^{m-1}\left(\log {x}\right)^n dx+n\int x^{m-1}\left(\log {x}\right)^{n-1}dx.$$
Now we can define $g_{mn}(x)$ as follows:
$$g_{mn}\left(x\right)=\int x^{m-1}\left(\log {x}\right)^n dx=\frac{1}{m}x^m\left(\log {x}\right)^n-\frac{n}{m}\int x^{m-1}\left(\log {x}\right)^{n-1}dx.$$
Taking $m=4$ and $n=2$ we get:
\begin{align}
I&=\int x^3\left(\log {x}\right)^2dx=g_{42}(x)=\frac{1}{4}x^4\left(\log {x}\right)^2-\frac{1}{2}\int x^3\log{x}\,dx\\
&=\frac{1}{4}x^4\left(\log {x}\right)^2-\frac{1}{2}g_{41}(x)=\frac{1}{4}x^4\left(\log {x}\right)^2-\frac{1}{8}x^4\log{x}+\frac{1}{32}x^4+C.
\end{align}","['integration', 'calculus']"
2783011,Integrating integer powers of $\cos(\theta)$,"Consider the following integral for $n\in\mathbb{N}$: $$I_n = \int_0^\pi\cos^n\theta\,d\theta \tag1$$ which, using integration by parts, one can show to be $I_n = 0$ for $n$ odd and to be equal to $$I_{2m} = \frac{(2m-1)!!}{(2m)!!}\pi \tag2$$ for $n = 2m$ even. However, I've tried to evaluate $(1)$ using the binomial theorem to write powers of cosine as a sum over powers of exponential functions: \begin{align}
I_n &= \frac{1}{2^n}\int_0^\pi\left(e^{i\theta}+e^{-i\theta}\right)^n\,d\theta\\
&=\frac{1}{2^n}\sum_{k=0}^n{n\choose k}\int_0^\pi e^{ik\theta}e^{-i(n-k)\theta}\,d\theta\\
&=\frac{1}{2^n}\sum_{k=0}^n{n\choose k}\int_0^\pi e^{i(2k-n)\theta}\,d\theta\\
&=\frac{1}{2^n}\sum_{k=0}^n{n\choose k}\frac{e^{i\pi(2k-n)}-1}{i(2k-n)}
\end{align} However, this result seems to contradict $(2)$ - note that if $n = 2m$ is even, then $2k-2m$ is an even integer and $e^{2\pi i(k-m)} = 0$ for any value of $k$, which would imply that $I_n$ is nonzero only for odd values of $n$. Furthermore, if $n = 2m -1$, then we have $e^{i\pi(2k-2m+1)} = -1$ and we thus have \begin{align}
I_{2m-1} &= \frac{1}{2^{2m-1}}\sum_{k=0}^{2m-1}{{2m-1}\choose k}\frac{-2}{i(2k-2m-1)}\\
&=\frac{i}{2^{2m}}\sum_{k=0}^{2m-1}{{2m-1}\choose k}\frac{1}{2(k-m)-1} \tag3
\end{align} which is purely imaginary, which is obviously wrong. So what's the problem here? Am I somehow wrong in using the binomial theorem? Have I made a computational error that explains this odd result? Can this approach to computing $(1)$ be salvaged? EDIT. Taking into consideration that the sum is actually non-zero if the argument of the exponential function is itself zero, which occurs if, for $n$ = $2m$, we have $k = m$, this gives us: \begin{align}
I_{2m} &= \frac{1}{2^{2m}}{{2m}\choose m}\pi = \frac{(2m)!}{2^{2m+1}m!}\pi
\end{align} However, I don't see how this is equal to $(2)$. I have also tried to show that $(3)$ is $0$, by symmetry, but I have no managed to show this yet either.","['definite-integrals', 'integration', 'trigonometry', 'binomial-theorem']"
2783018,Uniform convergence of Lipschitz functions and convergence of their derivatives,"Suppose the following: $f$ and $f_m$ ( $m\in\mathbb N$ ) are real-valued functions on $[0,1]$ ; $f(0)=f_m(0)=0=f(1)=f_m(1)$ for each $m\in\mathbb N$ ; $f$ and $f_m$ ( $m\in\mathbb N$ ) all share the same Lipschitz constant $\kappa>0$ , meaning that $|f(y)-f(x)|\leq\kappa|y-x|$ and $|f_m(y)-f_m(x)|\leq\kappa|y-x|$ for each $x,y\in[0,1]$ and $m\in\mathbb N$ ; and $\lim_{m\to\infty}\sup_{x\in[0,1]}|f_m(x)-f(x)|=0$ (that is, we have uniform convergence). I am trying to either prove or disprove the following (the derivatives exist almost everywhere by Lipschitz and hence absolute continuity): $$\int_0^1|f'(t)|\,\mathrm dt\leq\liminf_{m\to\infty}\int_0^1|f'_m(t)|\,\mathrm dt.$$ It’s quite easy to come up with examples showing that the inequality can actually be strict, but I am wondering whether it always holds in the first place, or whether there is a neat counterexample. Any feedback is greatly appreciated.","['continuity', 'real-analysis', 'measure-theory']"
2783026,Prove that the origin is Liapunov Stable for the given system,"Consider the system $$\dot{x} = y \\ \dot{y} = -4x  $$
($\dot{x}$ means $\displaystyle \frac{dx}{dt}$ and $\dot{y}$ means $\displaystyle \frac{dy}{dt})$ I need to prove that the fixed point $\mathbf{x^{*} = 0}$ is Liapunov stable. For reference, according to my textbook (Strogatz), a fixed point $\mathbf{x^{*}}$ is said to be Liapunov stable if $\forall \epsilon > 0$, $\exists \delta >0$ such that $|\mathbf{x(t)} - \mathbf{x^{*}}| < \epsilon$ whenever $t \geq 0$ and $|\mathbf{x(0)}- \mathbf{x^{*}}|< \delta$ Now, if we let $x(0)=x_{0}$, $y(0)=y_{0}$, I was able to solve this system using eigenvalues and eigenvectors to have the following general solution:
$$ x(t) = x_{0} \cos (2t) + \frac{y_{0}}{2}\sin(2t) \\ y(t) = -2x_{0}\sin(2t) + y_{0}\cos(2t)$$ According to my solutions manual, ""it can be observed from the equations in the solution that $\delta < |x_{0}|$. This implies that $|x(t),y(t)|<2|x_{0}|$, where $2|x_{0}|$ is the radius $\epsilon$. Thus, it can be concluded that $|x(t),y(t)|<\epsilon$."" However, I don't understand how they were able to show this. We never really learned how to do these kinds of $\epsilon-\delta$ proofs for systems in my class, and this solution doesn't give a very good (meaning ""detailed"") explanation as to how they went about doing it. Could someone please explain to me step-by-step how they were able to surmise all of this? I would like to be able to apply this to other problems going forward, but unless I see one done out in detail, I am afraid I will not be able to. Thank you for your time and patience.","['dynamical-systems', 'stability-theory', 'epsilon-delta', 'stability-in-odes', 'ordinary-differential-equations']"
2783087,Why is there no $B$ component of acceleration in my Multivariable Calculus class?,"In our class, we're learning that you can split up the acceleration, $\mathbf{a}$, of a particle into two convenient components, like so: $$\mathbf{a} = a_T\mathbf{T} + a_N\mathbf{N}$$ Where $a_T$ is the ""tangential component"" of acceleration, $a_N$ is the ""normal component"", and $\mathbf{T}$ and $\mathbf{N}$ are the unit tangent and unit normal vectors to the curve $\mathbf{r}(t)$, respectively. But we also learned earlier about a third kind of vector, $\mathbf{B}$ - the ""binormal vector"" - which is orthogonal to both $\mathbf{N}$ and $\mathbf{T}$. Why isn't the formula thus $$\mathbf{a} = a_T\mathbf{T} + a_N\mathbf{N} + a_B\mathbf{B}?$$ Note: I know that the binormal vector $\mathbf{B}$ is not generally defined as a unit vector for the purposes of Multivariable Calculus classes. But in this instance, just assume $\mathbf{B}$ represents the unit binormal vector, and that $a_B$ represents the ""binormal component"" of acceleration. I have a sneaking suspicion that the jounce, $\mathbf{j} = \mathbf{r}^{(3)}(t)$, of the particle moving along $\mathbf{r}(t)$ is, in fact, defined by $$\mathbf{j} = j_T\mathbf{T} + j_N\mathbf{N} + j_B\mathbf{B}.$$ ...since, well, $$\mathbf{v} = \Vert\mathbf{v}\Vert\mathbf{T} = v_T\mathbf{T}$$ and $$\mathbf{a} = a_T\mathbf{T} + a_N\mathbf{N};$$ It just seems like each new order of derivative taken of $\mathbf{r}(t)$ adds to the equation a new, orthogonal component of motion. If that's the case, why??","['vector-fields', 'physics', 'differential-geometry', 'vector-analysis']"
2783108,"Continuous,Discontinuous ,Differential and non Differentiable function Graph properties","I am quite familiar with how to prove differentiability and continuity of functions(by equations).This   doubt is  to get some meaningful information which I might have missed and it is related  to visual inspection of  graph plots. Continuous v/s discontinuous  function Graph With out solving  equations,  sometimes I can easily identify the non continuity of the graph because  there will be    breaks in the graph path. There should be non breaking path between any two points in a continuous function Doubt An example of graph which is continuous but not differentiable   is given below . Can we identify the non differentiable functions from graph plot as we did earlier in continuity? What I have found is given below Is this visual inspection  method cover all non differentiable cases? If it misses some cases what are those cases? NB :: I am not looking for a visual inspection method to substitute entirely   equation solving method. But I believe if I get such information , some times  it may save time","['derivatives', 'limits', 'functions', 'calculus', 'continuity']"
2783118,"Finding the intersection of two lines, in polar coordinates","The sticking point is figuring out the substitutions for a ratio of cosines of differences. I have a pair of lines in polar coords: $$r = \frac{s_1}{\cos(\theta - \alpha_1)} \qquad r = \frac{s_2}{\cos(\theta - \alpha_2)}$$ where $$\begin{align}
\alpha_1 &= 6^\circ \\
s_1 &= 0.9945218953682733 \\
\alpha_2 &= 74^\circ \\
s_2 &= 0.27563735581699916
\end{align}$$ I then need to do trigonometric substitution to solve for $\theta$. $$\begin{align}
\frac{s_1}{\cos(\theta - \alpha_1)} &= \frac{s_2}{\cos(\theta - \alpha_2)} \\[4pt]
\frac{s_1}{s_2} &= \frac{\cos(\theta - \alpha_1)}{\cos(\theta - \alpha_2)} 
\end{align}$$ I am stumped after that point.",['trigonometry']
2783122,Geometric and topological ways to define intersection number,"Resently I'm reading Bott-Tu's differential forms in algebraic topology , and comparing it with some differential topology textbook. While proving the Poincare-Hopf theorem, it defines the intersection number $I(M,N)$, where M, N are submanifold of another smooth manifold K with condition $\dim M+\dim N=\dim K$, to be $I(M,N)=\int_K \eta_M \wedge\eta_N.$ Here $\eta_M$ is the Poincare dual of M, so $\eta_M \wedge\eta_N$ is a top form in K, assuming the dimension condition. My question is why this definition is coincide with the definition appeared in differential topology, which using the orientable intersection number that ""sum up"" $\pm 1$  locally according to the orientation. If it's possible, I perfer an answer without using much algebraic geometry. Any links to webpage or other reference are welcomed as well. Great Thanks!","['algebraic-topology', 'differential-forms', 'differential-geometry']"
2783131,"$X=\sum_{i=1}^{N}X_i$,estimator for $N$","Let $X_i$, $i\geq 1$, be independent and identically distributed random variables having the uniform distribution over $(0,1)$. Let $X$ be defined as $X=\sum_{i=1}^{N}X_i$, where $N$ is an unknown integer. (a) Find an unbiased estimator $T(X)$ of $N$. (b) Decide with adequate reasons, if $\dfrac{T(X)}{N}$ converges to $1$ almost surely, as $N$ goes to infinity. At first I thought that since $E(X)=N\cdot \dfrac{1}{2}$, then the unbiased estimator, $T(X)=2X$. But one of my friend said that since $X$ contains $N$, $2X$ can not be an estimator here which is true. So, I am stuck. Any help appreciated. Thanks.","['law-of-large-numbers', 'statistics', 'statistical-inference']"
2783190,Finding Eigen Values of ODE,"I am asked to find non-trivial eigen values of the BVP $$y''(x)+\lambda y(x) =0, \quad y \left(\frac{\pi}{2} \right) = 0, \quad y(0)= -3y'(0).$$
However, after few routine calculations, I obtained $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$. It is also given that the eigen values may be given by $\lambda_n = 4n^2 \pi ^2$. I am unable to show the given solutions satisfy the equation  $\tan ^2 (\sqrt{\lambda} \pi/2) = 9\lambda$.",['ordinary-differential-equations']
2783276,How many pairwise non-similar rectangles are there in an 8x8 chessboard?,"How many pairwise non-similar rectangles are there in an $8 \times 8$ chessboard? My answer is $22$ wherein I do trial and error.
I really cannot think of an equation or formula for this.","['recreational-mathematics', 'geometry']"
2783345,Proof or reference to the Weyl inequalities?,"Does anyone know a proof or reference to the following result? Suppose that $A, B$ are both $m \times n$ real matrices. Then for all $1 \leq k \leq \min\{m, n\}$, 
  $$|\sigma_k(A) - \sigma_k(B)| \leq \|A - B\|.$$ I think these are called the Weyl inequalities, and I remember learning a proof of this result using the minimax characterization of these singular values but I can't reconstruct the proof. Anyone know it or have a reference to it?","['reference-request', 'singular-values', 'linear-algebra']"
2783367,"In 1990 people, each one is connected to at least 1327 others, then there is a fully-connected 4-group of people","We have 1990 people and each one is ""connected"" to at least 1327
  others. Show that there is a group of 4 people where each one is
  ""connected"" to every other person of that group. We could model the relation ""connected to"" as a graph, where each node is a person (so $n=1990$ in $G(V,E)$) and two people $u,v$ are connected iff $u,v\in E$. Then from the problem statement, $δ(G)\geq 1327$ and thus $m\geq 1327$. We need to show that $G(V,E)$ has the complete graph $K_4$ as a subgraph, i.e. $K_4\subset G$. But I am stuck here. Any help to advance my solution?","['inclusion-exclusion', 'combinatorics', 'graph-theory', 'contest-math']"
2783399,Example of a series!,"Give an example of a convergent series $$\sum_{n=1}^{\infty}a_n$$ such that the series $$\sum_{n=1}^{\infty}a_{3n}$$ is divergent.. 
I cannot find find any kind of series...
I am also be very thankful if you find a divergent series (changing the term convergent)...",['sequences-and-series']
2783500,Understanding the Core of an Operator or Map.,"I first saw a reference to the core of an unbounded linear operator in the context of Hilbert space and subsequently in the more general context of Banach space. The definition here https://math.stackexchange.com/q/716486 ..... ""For $D \subseteq D(T)$ to be a core for $T$, what you want is $\{(x,Tx): x \in D\}$ to be dense in the graph $\{(x,Tx): x \in D(T)\}$ of $T$ (as subsets of ${\mathscr H} \times {\mathscr H}$, with the norm topology)."" .....
 is in the context of Hilbert space, but only refers to the norm, so presumably applies equally in Banach space. Question 1 : In some references I can find it is required that $D$ is a subspace while others just identify it as a subset ? Question 2 :  I think I could adapt the definition given to a purely topological context and say ""For first-countable Hausdorff topological spaces $(X, \mathscr T)$ and $(Y, \mathscr S)$ and a partially defined function $f: D(f) \subset X \to Y$ then $D \subset D(f)$ is a core for $f$ if $G(f|_D) = \{x, f(x): x \in D\}$ is dense in $G(f) =  \{x, f(x): x \in D(f)\}$ with the product topology on $(X \times Y)$"" I think this reduces to the Banach space definition since the norm topology on ${\mathscr H} \times {\mathscr H}$ is just the product space topology, but I   can't find any reference to confirm or contradict this and would appreciate feedback. Question 3 : taking the Banach space definition, it seems that if $D$ is a core for $T$ then $D$ is a dense in $D(T)$ but the converse doesn't necessarily hold  - correct ? Question 4 : I think I get the same result $D$ is a core for $f \implies D$ is a dense in $D(f)$ with my topological definition - correct ? Proof: Let $x \in D(T)$ so that $(x, f(x)) \in G(f)$ If $O_X$ is any open set containing $x$ and $O_Y$ is any open set containing $f(x)$ then $O_X \times O_Y $ is an open set in the product topology and contains some point $(x_D, f(x_D))$ with $x_D \in O_X \implies D$ is dense in $D(f)$. Question 5 : I think with the Banach space definition it follows that a closed operator is uniquely defined by its values on a core. Question 6 : I think with my topological space definition it follows that a closed operator is uniquely defined by its values on a core provided that the spaces are first countable and Hausdorff. Proof: Relies on the following results A convergent sequence in Hausdorff space has a unique limit. If a space is first countable, $C$ is closed and $D$ is dense in $C$ then any point  in $C$ is the limit of a convergent sequence of points in $D$. The product topology of two first-countable spaces is first-countable Firstly assert that if $f :D(f) \subset X \to Y$ and $G $ is a dense subset of $G(f) =  \{(x, f(x)): x \in D(f)\}$ in the product topology then $\overline G = G(f)$. This follows since $X \times Y$ is first countable so any $(x, f(x))$ is the limit of a sequence $(x_n, f(x_n))$ in $G$ and being Hausdorff this limit is unique. Then let $f,g$ be two closed functions $f:D(f) \subset X \to Y$ and $g:D(g) \subset X \to Y$ with $D$ a core for both and $f = g$ on $D$. Then $G =  \{x, f(x): x \in D\} =  \{x, g(x): x \in D\}$ is dense in $G(f)$ and $G(g)$ So  $G(f) = G(g) = \overline G \implies f = g$ on $D(f) = D(g)$.","['general-topology', 'banach-spaces', 'unbounded-operators', 'hilbert-spaces']"
2783558,Is the product of Jacobson rings a Jacobson ring?,"Is the product 
$$
\prod_{i\in I}R_i
$$ 
of a family $(R_i)_{i\in I}$ of Jacobson rings a Jacobson ring? (Here ""ring"" means ""commutative ring with one"".)","['category-theory', 'abstract-algebra', 'commutative-algebra']"
2783562,Laplace PDE with impulse function,"I want to solve the PDE: $$u_{xx}+u_{yy}=-\delta(x-x_0)\delta(y-y_0)$$ 
in the rectangle: $\quad R=\{(x,y):0\leq x\leq a\quad0\leq y\leq b\},\quad (x_0,y_0)\in R$ with the boundary conditions: $$u_x(0,y)=u_x(a,y)=0$$
$$u(x,0)=u(x,b)=0$$ Which method should I use when there is an impulse function (in this case Dirac's delta)?","['dirac-delta', 'harmonic-functions', 'ordinary-differential-equations', 'partial-differential-equations']"
2783630,Euler method and bisection method,"I'd like to solve the equation 
$$ \phi''(x) = \lambda \sin (\phi(x)) $$
where $x \in (0,L)$, $\phi'(0) = 0$, $\phi'(L) = 0$. Let $ \psi = \phi'$ and
$$ \phi'(x) - \psi(x) = 0$$
$$ \psi'(x) - \lambda \sin (\phi(x)) = 0$$ for $x \in (0,L)$ and $\psi(0) = 0, \phi(0) = \phi_0.$ Can anybody help me to find $\phi_0 $ numerically such that $\phi'(L) = 0$ holds? I received the advice to compute the solution of $(\phi, \psi)$ with the explicit Euler method for $\phi_0 = 1.5$ and $\phi_0 = 3$ and to use the method of bisection to compute $\phi_0$. In addition, I received the following values: Number of steps (bisection): $2^6$ Length of steps (Euler): L/100 L = 5 $\lambda$ = 2 Thanks for any help! EDIT: In the meantime, I coded a bit. I added your functions as well as an implementation of Euler and bisection. See what I did so far. Now my problem is to connect your functions with my functions. Can you please help a bit? (For example, it's not clear to me where to define the function, and it's not clear to me when calling your functions ""model"" and ""omegaL""...) funtion x = eubisect()
    u = bisection(f, a, b, N, eps_step, eps_abs)

function dotu = model(t,u)
    lambda = 2;
    dotu = [ u(2); lambda*sin(u(1)) ]
end

function omegaL= f(phi0)
    L = 5;
    N = 100;
    t,u = Euler(model, 0, L, N, [phi0,0])
    omegaL = u(end,2)
end

function [t, y] = Euler(f, a, b, N, y0)
    clear t % Clears old time steps and
    clear y % y values from previous runs
    %a=0; % Initial time
    %b=1; % Final time
    %N=10; % Number of time steps
    %y0=0; % Initial value y(a)
    h=(b-a)/N; % Time step
    t(1)=a;
    y(1)=y0;
    for n=1:N % For loop, sets next t,y values
        t(n+1)=t(n)+h;
        y(n+1)=y(n)+h*f(t(n),y(n)); % Calls the function f(t,y)=dy/dt
    end
    %plot(t,y)
    %title(['Euler Method using N=',num2str(N),' steps'])
end


function [ r ] = bisection( f, a, b, N, eps_step, eps_abs )
    % Check that that neither end-point is a root
    % and if f(a) and f(b) have the same sign, throw an exception.

    if ( abs(f(a)) < eps_abs )
    r = a;
    return;
    elseif ( abs(f(b)) < eps_abs )
    r = b;
    return;
    elseif ( f(a) * f(b) > 0 )
        error( 'f(a) and f(b) do not have opposite signs' );
    end

    % We will iterate N times and if a root was not
    % found after N iterations, an exception will be thrown.

    for k = 1:N
        % Find the mid-point
        c = (a + b)/2;

        % Check if we found a root or whether or not
        % we should continue with:
        %          [a, c] if f(a) and f(c) have opposite signs, or
        %          [c, b] if f(c) and f(b) have opposite signs.

        if ( abs(f(c)) < eps_abs )
            r = c;
            return;
        elseif ( f(c)*f(a) < 0 )
            b = c;
        else
            a = c;
        end

        % If |b - a| < eps_step, check whether or not
        %       |f(a)| < |f(b)| and |f(a)| < eps_abs and return 'a', or
        %       |f(b)| < eps_abs and return 'b'.

        if ( b - a < eps_step )
            if ( abs( f(a) ) < abs( f(b) ) && abs( f(a) ) < eps_abs )
                r = a;
                return;
            elseif ( abs( f(b) ) < eps_abs )
                r = b;
                return;
            end
        end
    end

    error( 'the method did not converge' );
end","['real-analysis', 'eulers-method', 'matlab', 'bisection', 'ordinary-differential-equations']"
2783657,"solve the system of equation for $a,b,c,d,e$ [closed]","Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question Solve the system of equation for $a,b,c,d,e$ $$3a=(b+c+d)^3$$  $$3b=(c+d+e)^3$$  $$3c=(d+e+a)^3$$  $$3d=(e+a+b)^3$$ $$3e=(a+b+c)^3$$
I try this problem I find this system of equation is symmetric i.e, if (a,b,c,d,e) satisfy this system , then it's permutation is also satisfy this equation I want a problem solving tip to start this problem","['algebra-precalculus', 'symmetric-polynomials']"
2783689,Outward/Inward flux of a conservative vector field over a closed loop,"Due to path independence the line integral of a conservative vector field for a closed loop is 0--assuming this is over some simply connected region. In my calculus text book it says that ""conservative"" implies that it follows the principles of conservation of energy, hence does this imply the inward and outward flux over the same closed loop is also 0?","['multivariable-calculus', 'calculus']"
2783705,Number of primes within a number of given length,"My question is most easily explained via an example: Consider the number $n=1967$. It contains the prime numbers $7$, $19$, $67$ and $967$ as all those are primes and you can find them in the decimal expansion of $1967$. In contrast, the part $196$ for example is not prime. Given a fixed number of digits, $k$ say, what can be said about the maximal number of primes you can find in the decimal expansion of a $k$-digit number? I will call this number $m(k)$. The above example then teaches us that $m(4)\ge 4$. Clearly, $m(k) \ge k$ e.g. by considering the number with digit $3$ $k$-times. The same argument shows that $m(k+1)\ge m(k)+1$. The maximal number of subnumbers for a $k$-digit number is $\frac{k(k+1)}{2}$, so trivially $m(k)\le \frac{k(k+1)}{2}$. The first cases are:
$$m(1)=1\ (e.g. n=3)\\
m(2)=3\ (e.g. n=23)\\
m(3)=6\ (e.g. n=373).$$","['number-theory', 'prime-numbers']"
2783741,find ratio of area of triangle $\Delta{AFG}$ to area of $\Delta {ABC}$,"In the figure below, $BD=DE=EC$, $F$ divides $AD$ so that $FA:FD=1:2$ and $G$ divides $AE$ so that $GA:GE=2:1$. Find ratio of area of triangle $\Delta{AFG}$ to area of $\Delta {ABC}$ My Try: I noticed that $G$ is centroid of $\Delta{ADC}$ and $$Ar(ABD)=Ar(ADE)=Ar(AEC)$$ any clue?","['plane-geometry', 'euclidean-geometry', 'triangles', 'geometry', 'centroid']"
2783746,Find an estimate for $p=P(X\geqq 20)$,"I have this problem I'm stuck with. I would really appreciate some help. ""The following is a sample from a normal distribution. $7.6 , 9.6, 10.4, 10.7, 11.9, 14.1, 14.6, 18.5$ a) Let $X$ have this normal distribution and let $p=P(X\geqq 20)$. If
  we estimate $p$ by the relative frequency, we just get 0. Suggest
  another estimate. b)Let $x$ be the 95th percentil, i.e. a value such that $P(X \leqq x)
> = 0.95$. Suggest an estimate for $x$."" This seems fairly straight forward. I dont get the same answer as my text book however. I'm thinking since $X\sim N(\mu,\sigma ^2)$ we can estimate $\mu = \bar{X} = 12.175$, and $\sigma ^2 = s^2 =\frac{1}{n-1}\sum_{k=1}^{n}(X_k-\bar{X})^2 \approx 11.79 \implies s \approx 3.43$. So far so good. Now $p = P(X\geqq 20) = 1- P(\frac{X-\bar{X}}{s}\leqq\frac{20-\bar{X}}{s})$ where $\frac{X-\bar{X}}{s} \sim F_{t_6}(0,1)$, i.e. a T-distribution of $n-2 = 8-2 = 6$ degrees of freedom. (This is where I'm uncertain whether this is correct). This leads to $p = 1-F_{t_6}(2.28)$ which gives the wrong answer according to the book. In the same way does the (b)-part go wrong as well. The main question is how to use the T-distribution correctly. Is the T-distribution even the right choice in this problem? I would very much appreciate some help because my text book dont explain this types of problems very well. Cheers!","['statistics', 'estimation', 'probability', 'parameter-estimation']"
2783748,Spin structure on Whitney sum,Suppose $E_1$ and $E_2$ are oriented vector bundles with $w_2(E_i)=0$ over a compact manifold $M$. Then $E=E_1  \oplus E_2$ admits a spin structure too. Can we choose a spin structure of $E$ such that the associated spin vector bundle $S(E)$ is given as the Whitney sum of $S(E_1)\oplus S(E_2)$?,['differential-geometry']
2783757,Can divergence be thought of as a Radon-Nikodym derivative?,"The divergence theorem states roughly $$\int_\Omega \operatorname{div}U\ dV=\int_{\partial \Omega}U\cdot n\ dS$$ where $U$ is a vector field, $\Omega$ is a region of space with a smooth boundary, $n$ is a smooth normal vector field on its boundary. This looks alot like a Radon-Nikodym derivative: a certain function on sets can be obtained by integrating something on that set. In this interpretation, we would think of the right hand side of the equation, the flux of $U$ out of $\Omega$, as a (signed) measure. It seems like it should be additive like a measure should be, and perhaps it can even be extended to the Borel sigma algebra via something like Carathéodory's extension theorem. The other question is if this ""flux measure"" is absolutely continuous with respect to the Lebesgue measure. This is plausible since the flux out of a flat surface should be zero: remember that we're thinking of closed surfaces here, so we should be integrating on ""both sides"" of the flat surface, which will cancel out. Can this interpretation in any way be made rigorous?","['multivariable-calculus', 'radon-nikodym', 'real-analysis', 'measure-theory']"
2783761,Can the operator $\operatorname{id}_V-(AB-BA)$ be nilpotent in the infinite-dimensional case.,"Let $V$ be a infinite-dimensional vector space over the field of characteristic $0$ and $A,B$ be linear operators of $V$. Let $\operatorname{id}_V$ be an identical operator. Using trace function it is not easy to show that the operator
$$
\phi = \operatorname{id}_V-(AB-BA)
$$
can not be nilpotent in the case, where $V$ is finite-dimensional . My question. Can the above operator $\phi$ be nilpotent in the case, where $V$ is infinite-dimensional?","['nilpotence', 'matrices', 'abstract-algebra', 'trace', 'linear-algebra']"
2783789,Inertia of the matrix $\frac{1}{(i+j)!}$,"Let $A$ be a matrix defined as $a_{ij} = \frac{1}{(i+j)!}$ of order $n\times n$, then find the inertia of $A$ for every $n\geq 1$. Can you find the sign of determinant of $A$ for every $n$? We define inertia of a square matrix $A$ as $(\mu(A),\delta(A),\nu(A))$, where $\mu(A),\delta(A),\nu(A)$ denote number of positive, zero and negative eigenvalues of $A$.
By using Mathematica we have a conjecture for it as follows : 
  Inertia of $A_{n\times n}$ = $(\frac{n}{2},0,\frac{n}{2})$, if n is even and $(\frac{n+1}{2}$,0,$\frac{n-1}{2}$) \,  if n is is odd. I tried to solve this and reached a conclusion that if I could find sign of $det(A_{n\times n})$ for every n then I am done. So, Can anyone help me for finding sign of $detA$ or inertia of $A$ directly? Thanks in advance.","['matrices', 'matrix-calculus', 'matrix-decomposition', 'functional-analysis', 'linear-algebra']"
2783790,Borel $\sigma$-algebra of initial topology,"Let $X$ be a set and $h_i : X \to \mathbb{R}$, $i \in I$ a collection of maps. We can consider on $X$ the initial topology $\tau$ generated by all the $h_i$ and the induced Borel $\sigma$-algebra $\mathcal{B}(\tau)$. We can also consider the $\sigma$-algebra $\Sigma$ on $X$ generated by the maps $h_i$. Under which topological conditions on the space $(X, \tau)$ can we deduce that $\mathcal{B}(\tau) = \Sigma$? I am searching for rather general conditions that can be used for applications. In my situation, I know that $(X,\tau)$ is at least separable and Hausdorff.","['general-topology', 'measure-theory']"
2783805,"Find the following limit : $\lim_{(x,y)\rightarrow(0,0)}xy \log\left|y\right|$","I have been trying to solve this limit: $$\lim_{(x,y)\rightarrow(0,0)}xy\log\left|y\right|$$ I tried using polar coordinantes but i could not find the answer. I tried using iterate limits but i dont get any information . Any hint on how to solve this?","['multivariable-calculus', 'limits']"
2783905,Number of simultaneously solvable linear equations with nonnegative variables,"Let $N$ variables $x_i \ge 0$ but not all of them be zero. One may fix $\sum_{i=1}^N x_i = 1$. There are $P$ equations which need to be solved, with coefficients $a^k_i$ indexed with superscripts $k = 1 ... P$ and subscripts $i = 1 ... N$: $$
a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\
a^2_1 x_1 + a^2_2 x_2 + \cdots + a^2_N x_N  = 0\\
\vdots \\
a^P_1 x_1 + a^P_2 x_2 + \cdots + a^P_N x_N  = 0\\
$$ Let $P$ and $N$ be sufficiently large to come up with probabilistic statements (see below). Allow only binary choices $a_i^k = \pm 1$. If all $P \cdot N$ many coefficients are chosen randomly with probability 0.5 for $\pm 1$, and if all coefficient vectors are in general position , then it is known [Thomas Cover, [Cover65]] that variables $x_i$ can be found which solve the equations for at least 50 % of all such random choices of coefficients, as long as $2 P <  N$. [Note: this is half as many equations than can be solved if there were no restrictions on the $x_i$. ] Now define a different generative procedure for the binary coefficients as follows.  Select randomly the coefficients in the first equation $a^1_i$. Let the other coefficients be $a^k_i = \prod_{j=1}^k  a^1_{(1+[(j+i-1) \! \! \! \mod \! \! N])}$ (which are then also binary). To illustrate, the first three equations are 
$$
a^1_1 x_1 + a^1_2 x_2 + \cdots + a^1_N x_N  = 0\\
a^1_1 a^1_2 x_1 + a^1_2 a^1_3 x_2 + \cdots + a^1_N a^1_1 x_N  = 0\\
a^1_1 a^1_2 a^1_3 x_1 + a^1_2 a^1_3a^1_4 x_2 + \cdots + a^1_N a^1_1 a^1_2  x_N  = 0\\
\vdots
$$ Then numerical simulations indicate that variables $x_i$ can be found in the same sense as above as long as $1.7 P <  N$. Already for small values, like $N =10$, the effect can be seen in simulations. I.e. with this choice of coefficients, the equations are ""easier"" in the sense that more of them can be simultaneously solved. Can anybody give a hint why this is so? (and possibly also how the ratio 1.7 can be calculated?)  I was reading about equations with nonnegative coefficients but didn't get a handle on  this problem. Also, a link might exist to elementary symmetric polynomials but I couldn't find material which elucidates this question. [Cover65] Thomas M. Cover: ""Geometrical and Statistical Properties of Systems of
Linear Inequalities with Applications in Pattern
Recognition"". IEEE TRANSACTIONS ON ELECTRONIC COMPUTERS, Volume: EC-14, Issue: 3, June 1965, pp. 326 - 334.","['inequality', 'optimization', 'probability', 'combinatorics', 'linear-algebra']"
2783911,Convergent sub-martingale?,"I have this problem. Suppose $(X_t)_{t\geq 1}$ is a sequence of random variables with values in $[0,\infty)$. I know that $E(X_t)\rightarrow 1$ as $t\rightarrow \infty$ and $(\exp(X_t))_{t\geq 1}$ is a sub-martingale. I would like to find whether $\sup_t E(\exp(X_t))<\infty$ in order to apply the martingale convergence theorem. Do you have any idea if this is possible/not feasible? Thanks!","['probability-theory', 'martingales']"
2783915,How to prove the closed form of the integral $\int \frac {dx}{\prod_{r=0}^n (x+r)}$,"I want to derive a closed formula for the integral $$I_n= \int \frac {dx}{\prod_{r=0}^n (x+r)}$$ On writing out first few terms we get For $n=0$, $$I_0=\ln \vert x\vert+C$$
For $n=1$, $$I_1=\ln \vert x\vert-\ln \vert x+1\vert+C$$
For $n=2$ $$I_2=\frac {1}{2!}\left(\sum_{r=0}^2 (-1)^r\binom {2}{r} \ln \vert x+r\vert\right)+C$$
For $n=3$ $$I_3=\frac {1}{3!}\left(\sum_{r=0}^3 (-1)^r\binom {3}{r} \ln \vert x+r\vert\right)+C$$ Hence for generalized $n$ we have $$I_n=\frac {1}{n!}\left(\sum_{r=0}^n (-1)^r\binom {n}{r} (\ln \vert x+r\vert)\right)+C$$ Now this is just an observation but I want to prove that it is correct. I have tried lot of methods but not useful. Partial fractions would have been most useful but would go out on tedious task which is nearly impossible.  Also integration by parts won't help nor any trig substitution. So any ideas are welcome.  And ya,  this is not a homework question,  it's a question which I just saw in a integral challenge paper.","['indefinite-integrals', 'integration', 'calculus']"
2783980,Inclusion–exclusion principle [Proof verification],"Please check if my proof contains any error! Thank you so much! Lemma: Let $A=\{J\subseteq\{1,2,\cdots,k\}\mid J\neq\emptyset\}$ and $B=\{J\subseteq\{1,2,\cdots,k+1\}\mid J\neq\emptyset\}$, then $$B-A=\{\{k+1\}\}\cup\{\{k+1\}\cup x\mid x\in A\}$$ Proof: $y\in B-A\Leftrightarrow\begin{cases}\ y\in B\\y\notin A\\\end{cases}\Leftrightarrow\begin{cases}\ y\subseteq\{1,2,\cdots,k+1\}\text{, and }y\neq\emptyset\\y\not\subseteq\{1,2,\cdots,k\}\text{, or }y=\emptyset\\\end{cases}\Leftrightarrow\begin{cases}\ y\subseteq\{1,2,\cdots,k+1\}\text{, and }y\neq\emptyset\\\exists m\in y\text{ such that } m\notin\{1,2,\cdots,k\}\text{, or }y=\emptyset\\\end{cases}\Leftrightarrow\begin{cases}\ y\subseteq\{1,2,\cdots,k+1\}\\m=k+1\in y\\\end{cases}\Leftrightarrow y\in\{\{k+1\}\}\cup\{\{k+1\}\cup x\mid x\in A\}$. Equivalently, $B=A\cup\{\{k+1\}\}\cup\{\{k+1\}\cup x\mid x\in A\}.$ $$\tag*{$\blacksquare$}$$ Inclusion–exclusion Principle: Let $A_1,A_2,\cdots,A_n$ be finite subsets of a set $X$ and $A=\{J\subseteq\{1,2,\cdots,n\}\mid J\neq\emptyset\}$, then $$\left |\bigcup_{i=1}^nA_i\right|=\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$$ Proof of Inclusion–exclusion Principle: It's trivial that the theorem is true for $n=2$. Assume that it is true for $n=k$, then $$\left |\bigcup_{i=1}^kA_i\right|=\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$$ First, notice that $-\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}(A_j\cap A_{k+1})\right|$ $=-\sum_{J\in \{\{k+1\}\cup x\mid x\in A\}}(-1)^{(\left|J\right|-1)-1}\left|\bigcap_{j\in J}A_j\right|$ $=\sum_{J\in \{\{k+1\}\cup x\mid x\in A\}}(-1)^{(\left|J\right|-1)-1+1}\left|\bigcap_{j\in J}A_j\right|$ $=\sum_{J\in \{\{k+1\}\cup x\mid x\in A\}}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$ For $n=k+1$, we have that $\left |\bigcup_{i=1}^{k+1}A_i\right|$ $=\left |(\bigcup_{i=1}^{k}A_i)\cup A_{k+1}\right|$ $=\left |\bigcup_{i=1}^{k}A_i\right|+\left |A_{k+1}\right|-\left |(\bigcup_{i=1}^{k}A_i)\cap A_{k+1}\right|$ [It's clear the theorem for $n=2$] $=\left |\bigcup_{i=1}^{k}A_i\right|+\left |A_{k+1}\right|-\left |\bigcup_{i=1}^{k}(A_i\cap A_{k+1})\right|$ $=\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|+\sum_{J\in \{\{k+1\}\}}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|-\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}(A_j\cap A_{k+1})\right|$ $\text{ [We apply inductive hypothesis in which the theorem is true for $n=k$]}$ $=\sum_{J\in A}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|+\sum_{J\in \{\{k+1\}\}}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|+\sum_{J\in \{\{k+1\}\cup x\mid x\in A\}}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$ $=\sum_{J\in A\cup\{\{k+1\}\}\cup\{\{k+1\}\cup x\mid x\in A\}}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$ $=\sum_{J\in B}(-1)^{\left|J\right|-1}\left|\bigcap_{j\in J}A_j\right|$ Thus, the theorem is true for $n=k+1$. By principle of induction, Inclusion–exclusion Principle is proved. $$\tag*{$\blacksquare$}$$","['inclusion-exclusion', 'proof-writing', 'elementary-set-theory', 'proof-verification']"
2784001,Probability that at least $2$ of a group of $4$ people were born on the same day of the week,"What is the probability that at least $2$ of a group of $4$ people were born on the same day of the week? My attempt: Probability that at least $2$ of a group of $4$ people were born on the same day of the week=1-probability that at most $1$ of a group of $4$ people were born on the same day of the week (NONE ARE BORN ON SAME DAY). =$1-\frac{\text{Possibility none born on same day}}{\text{No. Of total possibilites}}$
=$1-\frac{4.5.6.7}{7.7.7.7}
=$0.659$",['probability']
2784057,Uniform convergence of Fourier series proof,"I want to prove that: If $f(x)$ is continuous with a period of $2\pi$ and its derivative $f^\prime(x)$ is piecewise continuous, then the Fourier series of $f(x)$ converges uniformly to $f(x)$. I'm familiar with the exact same proof (presented below) except for the fact that $f^\prime(x)$ has jump discontinuities. Don't these discontinuities affect the uniform convergence of $f$ ? What changes in the proof if $f^\prime$ is piecewise continuous?","['fourier-series', 'ordinary-differential-equations', 'uniform-convergence', 'partial-differential-equations']"
2784092,What distinguishes 'family' vs. 'set' of functions?,"Source: Stewart, James. Calculus: Early Transcendentals (6 edn 2007) . [p. 50 Top:]   To understand how the expression for a function relates to its graph, it’s helpful to graph a family of functions , that is, a collection of functions whose equations are related. In the
  next example we graph members of a family of cubic polynomials. [p. 391 Middle:]   You should distinguish carefully between definite and indefinite integrals. A definite
  integral $\int^b_a f(x) \,dx$ is a number, whereas an indefinite integral $\int f(x) \,dx$ is a function (or family [format mine] of functions). Of functions: how does 'family' differ from 'set'? Why did James Stewart write 'family' instead of 'set'? I read this that feels too advanced for univariate calculus.",['functions']
2784095,What is the probability that you chose the coin B,"Question Suppose you have two coins A and B the probability of head in A is $\frac{1}{4}$ and the probability of head in B is $\frac{3}{4}$ .
Now, suppose you have chosen a coin and tossed it two times.
The output was head and head. What is the probability that you chose the coin B. My Approach I used Bayes' theorem , Let Event, $\text{output to be Head Head}=E$ Req'd probability= $P({B}\mid{E})$ $$P({B}\mid{E})=\frac{P({E}\mid{B})\times P(B)}{P({E}\mid{B})\times P(B)+P({E}\mid{A})\times P(A)}$$ $$=\frac{\frac{3}{4} \times \frac{3}{4}  \times \frac{1}{2}} {{\frac{3}{4} \times \frac{3}{4}  \times \frac{1}{2}}+{\frac{1}{4} \times \frac{1}{4}  \times \frac{1}{2}}}$$ $$=\frac{9}{10}$$ Am i correct?",['probability']
2784126,Hartshorne Exercise 1.1.10: Give an example of a noetherian topological space of infinite dimensions,"Hartshorne Exercise 1.1.10: Give an example of a noetherian topological space of infinite dimensions. I'm baffled by why such space can exist. Instincts told me that I shouldn't take the Spec of any Noetherian ring because then prime ideals have finite height, which makes the dimension of the Spec to be finite as well. 
But this still doesn't make sense for the following reason: by part $(a)$, we know that $X$ is a topological space which is covered by a family of open subsets $\{U_i\}$, then $\dim X = \sup \dim U_i$. $X$ by being a Noetherian topological space, is campact and thus we can have a finite sub-cover of the cover, and $\dim X = \max U_j$, $j \in J$ where $J$ is a finite index set. Since each $U_j$ is then finite dimensional space, we thus have $\dim X$ in finite. Am I missing something? What would be an example?",['algebraic-geometry']
2784150,Generalisation of Index of a curve to higher dimensions,"Im studying Non Linear Dynamics and Chaos from Strogatz's textbook. In the sixth chapter, while talking about non linear flows in 2 dimensions he introduces the index of a curve in a vector field and shows some beautiful properties that the index has. I've understood how to use the index to classify and or deduce properties about fixed points in two dimensions but what about three dimensions? In 3D, is there any generalisation of the index that is useful in analysing fixed points? I'm assuming it would have to be a property of surfaces rather than curves but I couldn't conclude anything further on my own. I'm asking this because the (Lorenz-like) system of equations I'm dealing with for my project is a 3 dimensional system and the fixed points are tedious functions of the parameters and it would be really difficult to use linearisation or such other methods for them. Thanks in advance. Note: There is an answer to a similar question on math overflow, but I understood practically nothing of the answers there and it didn't seem too useful anyway.","['general-topology', 'dynamical-systems', 'nonlinear-dynamics']"
2784153,Limit of Probability and Probability of Limit,"Let $\{x_k\}$ and $x^*$ be a sequence and a point in $\mathbb{R}^n$, respectively. Can we conclude that
$$\lim_{k\to\infty} \mathrm{Prob}(x_k=x^*)=1$$
and
$$\mathrm{Prob}(\lim_{k\to\infty} x_k=x^*)=1$$
are equivalent or that one implies the other? I think the first one implies the second, but not vice versa since $x^*$ might not be part of the sequence.","['real-analysis', 'probability', 'limits']"
2784179,Consider homotopy of closed curves. Show equality of winding numbers.,"a) Let  $H: [0,K] \times [0,1] \rightarrow \mathbb{R}^2 $ be a homotopy of closed curves, so $H$ is continuous and for every $\sigma \in [0,1]$
 it holds that  $ c_{\sigma}:[0,K] \rightarrow \mathbb{R}^2$, $c_{\sigma}(t) := c(t,\sigma) $ is a $C^0$-closed curve. Consider $ p \in \mathbb{R}^2$ with $p \notin H([0,K] \times [0,1]) $. Show: $$ W_{c_0}(p) = W_{c_1}(p) $$ (winding number around $p$) b) Consider now a $C^0$-closed curve  $s : [0,K] \rightarrow \mathbb{R}^2$. Let $p,q \in \mathbb{R}^2$ be in the same connected component of $\mathbb{R}^2$\ $s([0,K])$ Show: $$ W_s(p) = W_s(q) $$ Attempt: So for a) :First I tried to use the definition of the winding number around a point. $W_{c_o}(p)$:= $\frac{1}{2\pi}$ $( \Theta(K) - \Theta(0))$, where $\Theta$ is the angle function of $\frac{ c_o - p}{||c_o -p||}$, but I failed. So I've started to consider $H$. We know that $H$ is continuous and $[0,K] \times [0,1] $ is compact. So $H$ is uniformly continuous, which means that for $\varepsilon = 1$ there is a $\delta > 0$ such that $| H_{s1}(t) - H_{s2}(t) | < 1$ $\forall t \in [0,K]$ and $|s_1 - s_2 | < \delta $, where $s_1, s_2 \in [0,1]$. This implies (""hopefully"") that $| \frac{H_{s1}(t)}{H_{s2}(t)} - 1 | < \frac{1}{|H_{s2}(t)|}$. Like you see I'm really lost in this exercise. I really need help here. b) For this part I have only an little idea. So I think that we somehow can use a) for this. We could try to get the claim of a) to solve b) , but to be honest I don't know how the "" transform"" b) to a).","['curves', 'winding-number', 'differential-geometry', 'homotopy-theory']"
2784186,Doubts about a question I asked a long time ago (eigenvalues),"Here I posted a question about the eigenvalues of the matrix $A:=vv^t$ (where $v\in\mathbb{R}^n$). The question was answered but I think (after some time) that I am not satisfied. Can someone please expand the answer? I don't understand why $A$ has rank at most $1$ and why this fact implies that $\lambda=\sum x_i^2$ is the unique eigenvalue. In addition, can I conclude that $A$ is diagonalizable?","['eigenvalues-eigenvectors', 'diagonalization', 'linear-algebra']"
2784192,How many visions does Dr. Strange need? [closed],"Closed. This question is off-topic . It is not currently accepting answers. Closed 6 years ago . This question is not about mathematics, within the scope defined in the help center . This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Improve this question In Avengers: Infinity War... Dr. Strange sees 14.5 million futures, in which they only defeat Thanos in one. Later, when Thanos has the Infinity Gauntlet and all the gems, he kills off half the population of the universe, randomly. He makes quite a deal about how important and fair the randomness of it is. However, if you removed half the population randomly, there are a great deal more potential outcomes than 14.5 million. If Thanos only removed half the population of Earth (which we can round to 8 billion), how many outcomes could there be that Dr. Strange would have to see?",['combinatorics']
2784205,Recommended Books for differential equations?,"I am planning to take Differential equations next semester, but due to a timetable issue I want to study most of it this summer in my spare time to make it easier. These are the Topics that will be included, which I think represent about half of the Differential equations in other universities: Ordinary differential equations. Explicitly solvable equations, exact and linear equations. Well-posedness of the initial value problem, existence, uniqueness, continuous dependence on initial values. Approximate solution methods. Linear systems of equations, variational system.  Elements of stability theory, stability, asymptotic stability, Lyapunov functions, stability by the linear approximation. Phase portraits of planar autonomous equations. Laplace transform, application to solve differential equations. Discrete-time dynamical systems. I am not only looking for textbooks with rigorous exercise sets, any books are welcome, heavy in theory ones as well.","['book-recommendation', 'ordinary-differential-equations', 'advice', 'analysis']"
2784221,Do the local sections of $\text{Spec}(R)$ have open supports (generalization of Hartshorne ex2.1.14)?,"For any sheaf and any local section, we have a general fact that the support of the local section (not to be confused with the support of the sheaf) is closed. (c.f. Hartshorne p.67 ex-2.1.14) Recall: $\text{supp}(s) := \{x\in \text{dom}(s): s_x\neq 0 \}$ . When $R$ is an integral domain, then the support of the structure sheaf of $\text{Spec}(R)$ is even open (not hard to prove: pass to local the section is represented by some $a/b$ , while $a\neq 0$ because $R$ is an domain. This presentation is local, and nonvanishing by definition of "" $0$ "" in a ring of fractions). Question If $R$ is just a commutative ring, is there any criterion for that any local section of $\text{Spec}(R)$ has open support?","['sheaf-theory', 'algebraic-geometry', 'commutative-algebra']"
2784255,Differentiation of a function defined over $\mathbb{R}^{m^{2}}$ (matrix spaces).,"For $1\leq i,j\leq m$, let $f_{ij}:\mathbb{R}^{m^{2}} \to \mathbb{R}$ defined by $f_{ij}(X)V = (X^{2})_{ij}$. For any $X,V \in \mathbb{R}^{m^{2}}$, show that $df_{ij}(X)V$ is the $ij$-th element of the matrix $XV + VX$. Get a similar result with $X^{3}$. Generalize. I don't want an answer. Actually, I didn't understand how this function works. I would like an explanation to understand this function and a small hint to start solving.","['derivatives', 'real-analysis']"
2784312,Completeness of Holder Space,"How do I show that the space of complex-valued functions on $[0,1]$ such that $$|f(x) -f(y)| < C|x-y|^\frac{1}{2}$$ with norm $$\|f\| = \sup_{x \in [0,1]} |f(x)| + \inf C$$ where inf is over the constants such that the holder bound holds, is a Banach space? $C$ depends on the function.","['functional-analysis', 'real-analysis', 'banach-spaces', 'analysis']"
2784322,Problem related to Fundamental Theorem of Integral Calculus [duplicate],"This question already has an answer here : Differentiability of antiderivative and continuity of derivative (1 answer) Closed 6 years ago . This is a problem from the book of Terence Tao, Analysis I and it looks like- Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a monotone increasing function . Let $F:[a,b]\to\Bbb{R}$ be the function
  $F(x)=\int_{[a,x]} f$. Let $x_0\in[a,b]$. Show that $F$ is
  differentiable at $x_0$ if and only if $f$ is continuous at $x_0$. Observe that, the ""if"" part can be easily proved by using Fundamental Theorem of Calculus. Just to recall ... Statement of Fundamental Theorem of Calculus: Let $a<b$ be two real numbers, and let $f:[a,b]\to\Bbb{R}$ be a
  Riemann Integrable function. Let $F:[a,b]\to\Bbb{R}$ be the function
  $F(x)=\int_{[a,x]} f$. Then $F$ is continuous on $[a,b]$. Furthermore,
  if $x_0\in[a,b]$ and $f$ is continuous at $x_0$, then $F$ is
  differentiable at $x_0$, and $F'(x_0)=f(x_0)$. Here, $f$ is monotone increasing on $[a,b]$, hence $f$ is Riemann Integrable on $[a,b]$ and $f$ is continuous at $x_0\implies F$ is differentiable at $x_0$ I can't understand how to tackle the ""only if"" part. There is a hint given in the book: Consider left and right limits of $f$ and argue by contradiction . Can anybody solve the part in a rigorous and analytical way? Thanks for your help in advance!","['derivatives', 'real-analysis', 'riemann-integration', 'calculus', 'continuity']"
2784365,Lemma which is used on Open Mapping Theorem,"Let $X$ and $Y$ Banach spaces wrt $\|.\|_X$ and $\|.\|_Y$ norms respectively and $S \subseteq X$ . $T:X \to Y$ is a linear and bounded (continuous) map Show that $\theta_X \in int(S) \Rightarrow \theta_Y \in int(T(S)) $ (where “int” is interior of a set and $\theta$ s are zeros of spaces) I have written if $\theta_X \in int(S) \Rightarrow \exists r \gt 0 , B(\theta_X,r) \subseteq S$ ( $B(\theta_X ,r)$ is open ball which has $\theta_X$ as center and $r$ as radius) I have been trying to show that : $ \exists \varepsilon \gt 0 , B(\theta_Y,\varepsilon) \subseteq T(S)$ i.e. $\theta_Y $ is an interior point of $T(S)$ but I stuck. I know it is very basic but I cannot guess anything about it. Is there any mistake in my writtens? How can I use being Banach space in here? I need a proof or guidance as simple as possible. Thanks in advance for helps","['real-analysis', 'banach-spaces', 'calculus', 'functional-analysis', 'analysis']"
2784373,Periodic sequences resulting from a summation over the Thue–Morse sequence,"Let $s_2(n)$ denote the sum of digits of $n$ in base-2 (OEIS sequence A000120 ), and $t_n=(-1)^{s_2(n)}$. Note that $t_n$ is the signed Thue–Morse sequence (OEIS sequence A106400 ), satisfying the recurrence
$$t_0=1,\quad\,t_n=(-1)^n\,t_{\lfloor n/2\rfloor}.\tag1$$
Also,
$$t_n=\operatorname{mod}\left(\!2n+\sum_{k=1}^n(-1)^{\binom n k},\,3\!\right).\tag2$$
Now, consider the family of sequences
$$u^{(m)}_n=\left|\sum_{k=0}^n\binom{m+n-k}m\,t_k\right|,\tag3$$
where $m$ is the index of a sequence within the family, and $n$ is the index of an element within the sequence. These sequences can be seen as iterated partial summations of the original sequence $t_n,$ with their signs dropped. It appears that each sequence $u^{(m)}_n$ is periodic with period $2^{m+1}$, i.e.
$$u^{(m)}_{n+2^{m+1}}=u^{(m)}_n,\tag4$$
and the sum of its elements in each period
$$\sum_{n=0}^{2^{m+1}-1}u^{(m)}_n=2^{\binom{m+1}2},\tag5$$
the largest element(s) being
$$\max_{n\ge0}\,u^{(m)}_n=2^{\binom m2}.\tag6$$
How can we prove that? Update : It also appears that, for $m>0,$ each period has $2^m-m+1$ distinct elements, has $m+1$ elements equal to $0$, $m+1$ elements equal to $2^{\binom m2}$; all other values, if present at all, occur exactly twice, positioned symmetrically around the largest elements; for each value $k$ present, the value $2^{\binom m2}-k$ is also present.","['binomial-coefficients', 'number-theory', 'combinatorics', 'sequences-and-series', 'discrete-mathematics']"
2784440,Solve the ODE using complex variables,"$$y''+4y=1+\cos2x$$
The homogeneous solution is 
$$y_H=A\cos 2x+B\sin 2x$$
I'm strugling to find the correct trial solution for the inhomogeneous part of the ODE. $y_P=Ce^{2ix}$ does not work because the constant cancels out after inputing the solution into the ODE (and the real part is already a fundamental solution of the ODE). $y_P=Cxe^{2ix}$ leaves me with $Ce^{2ix}(4iC-1)=1$. I could, of course, splitthe ODE into two inhomogeneous ODEs, solve for them, and sum their solution for the particular solution of the initial ODE, but that's not the point of this exercise.",['ordinary-differential-equations']
2784455,"Why not define a ""Dolbeaut complex"" with $\partial$ instead of $\overline{\partial}$? [duplicate]","This question already has answers here : Why only consider Dolbeault cohomology? (3 answers) Closed 6 years ago . Yesterday I saw the definition of Dolbeaut cohomology: let $(M,J)$ be a complex manifold, write the exterior derivative as ${\rm d} = \partial +\overline{\partial}$, where $\partial: \Omega^{\ell,m}(M) \to \Omega^{\ell+1,m}(M)$ and $\overline{\partial}: \Omega^{\ell,m}(M) \to \Omega^{\ell,m+1}(M)$, look at the complex $$\cdots \xrightarrow{\hspace{.4cm}\overline{\partial}\hspace{.4cm}} \Omega^{\ell,m-1}(M)\xrightarrow{\hspace{.4cm}\overline{\partial}\hspace{.4cm}} \Omega^{\ell,m}(M) \xrightarrow{\hspace{.4cm}\overline{\partial}\hspace{.4cm}} \Omega^{\ell,m+1}(M)\xrightarrow{\hspace{.4cm}\overline{\partial}\hspace{.4cm}} \cdots$$and put $$H^{\ell,m}_{\rm Dolbeaut}(M) \doteq \frac{\ker(\overline{\partial}: \Omega^{\ell,m}(M) \to \Omega^{\ell,m+1}(M))}{{\rm Im}(\overline{\partial}: \Omega^{\ell,m-1}(M) \to \Omega^{\ell,m}(M))}.$$Great. Question: Why did we took the above complex instead of $$\cdots \xrightarrow{\hspace{.4cm}\partial\hspace{.4cm}} \Omega^{\ell-1,m}(M)\xrightarrow{\hspace{.4cm}\partial\hspace{.4cm}} \Omega^{\ell,m}(M) \xrightarrow{\hspace{.4cm}\partial\hspace{.4cm}} \Omega^{\ell+1,m}(M)\xrightarrow{\hspace{.4cm}\partial\hspace{.4cm}} \cdots?$$ I get that $f: M \to \Bbb C$ is $J$-holomorphic if and only if $\overline{\partial}f = 0$ so that $\overline{\partial}$ sort of measures how stuff in general would be away from being holomorphic. But I'd like a more solid explanation.","['homology-cohomology', 'differential-forms', 'differential-geometry', 'complex-geometry']"
2784467,Estimating the volume of a spherical cap,"If $d$ is very large, there is a nice way to estimate the surface area of a ""spherical cap,"" that is, the part of a unit $(d-1)$-sphere on one side of a hyperplane: each coordinate of a random point on the surface of the sphere is distributed almost like a Gaussian with variance $1/d$, and we can use this to determine the probability that a random point lies in the cap. This trick is useful for other questions related to the spherical cap, and so it would be nice to know how good this Gaussian approximation is. Is there a nice bound one can prove in terms of $d$? The particular quantity I'm trying to estimate is the following: given a spherical cap whose surface area takes up some fraction $\alpha$ of the total surface area of the sphere, what is the average Euclidean distance from a random point on the sphere to the closest point on the cap?","['probability', 'geometry']"
