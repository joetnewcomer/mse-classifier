question_id,title,body,tags
4671844,"If 12 dice are rolled, what is the probablity that the minimum value of the 12 dice is 2?","Here is how I tried solving it: First we need to make sure that we do not roll any 1s in the 12 rolls: $(5/6)^{12} ≈ 0.1121$ We also need to make sure that at least one of the rolls results in a 2 or higher. The probability of rolling a 2 or higher on a single roll is $5/6$ . The probability of rolling at least one 2 or higher in 12 rolls is $1 - (1/6)^{12} ≈ 0.9999$ . Therefore, the probability is $P = (5/6)^{12} \cdot 0.9999 ≈ 0.1121$ , which is the wrong answer. I looked at this question previously asked, however all the answers seems to just use brute force to get all the values of the numerator because it only dealt with 2 dice. Any ideas on how to attempt to solve this problem would greatly be appreciated. Thanks!","['statistics', 'probability', 'sequences-and-series']"
4671918,Locally compact 2nd countable Hausdorff space and complete metrizability,"I was recently trying to verify certain things in the setting of Locally compact 2nd countable Hausdorff spaces. I thought that this is a natural collection of spaces more general than metric spaces, but I think that following the metrization theorems that such spaces must be metrizable with a complete metric. I didn't find such an explicit statement, so I thought to check whether this is indeed correct. My reasoning\intuition is as follows. A locally compact Hausdorff space is completely regular. By Uryshonn's metrization theorem, a completely regular 2nd countable space is metrizable. Every 2nd countable locally compact Hausdorff space is also $\sigma$ -compact. For such a space, closure of balls should be compact. The last point is imprecise, but I have the strong suspicion that such  a statement must hold. I assumed if I am wrong someone can generate a well known counter-example. Also if this statement is correct, this should be known under some terminology which I don't know how to look for. I would appreciate any directions on this.","['complete-spaces', 'second-countable', 'metric-spaces', 'general-topology', 'compactness']"
4671933,Proof of Bertrand test of convergence,"How to prove the Bertrand convergence test that states: If there exists such limit that $$\lim_{n\to\infty}((n(\frac{a_n}{a_{n+1}}-1)-1)\ln{n})=q$$ then the series $\sum^{+\infty}_{n=1}=a_n$ , where $a_n>0, \forall n\in\mathbb{N}$ , converges if $q>1$ and diverges if $q<1$ . The problem is from the book Drugi, D., Collection of Problems in Mathematical Analysis (in Serbian),  Naša Knjiga D.O,O., 2003, pp. 38, problem 16.","['limits', 'convergence-divergence', 'sequences-and-series', 'real-analysis']"
4671950,"Showing if the group $\frac {F[a,b]}{(a^2 = b^2)}$ is torsion free","For background, i've noticed that its easy to show that an equation of form $x^n = a$ in an abelian torsion free group always has either an unique solution for $x$ or none. So i decided to try seeing what happens for non-abelian torsion free groups. The most obvious and simple example to try would be the quotient $G = \frac {F[a,b]}{N}$ where $F[a,b]$ is the free group on two generators, $N$ is the normal subgroup generated by the relation $a^2=b^2$ . It is simple to show that $a\neq b$ in $G$ therefore it remains to show whether $G$ is torsion free. It is easy to show that there is no $x$ such that $x^n = a^2b^{-2}$ in the free group because $x$ would have to have reduced form $ayb^{-1}$ and therefore any non-trivial power would have $b^{-1}a$ in its reduced form. Of course this is not enough as $N$ has elements other than $a^2b^{-2}$ . I'm not sure what else to do. I could try to find an invariant that classifies whenever an element is in $N$ and then try showing that if $x^n$ is in $N$ then $x$ is in $N$ , but that doesn't seem easy. Is there an easier way? What are some  more systematic techniques to solve this kind of problems?",['group-theory']
4671956,Are there methods to convert a delay differential equation (DDE) into a (second-order) ODE? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I have a system of three non-linear differential equations of which one has a delay. Is it possible to convert such a system into (second-order) ODEs?","['calculus', 'ordinary-differential-equations']"
4672009,How to prove Picard's existence and uniqueness theorem by Tonelli sequence instead of Picard sequence? For O.D.E./ODE.,"$\qquad$ First of all, this question for O.D.E. comes from an end-of-book exercise with no answer. $\qquad$ Secondly, allow me to give the definitions of the relevant contents in the question to avoid ambiguity. You can also skip or skim this section until you get to the dividing line if you are familiar with it. Lipschitz condition $\qquad$ Say the function $f(x,y)$ satisfies the Lipschitz condition for $y$ on the region $G$ , if there is a constant $L$ , such that for any $(x,y_1),(x,y_2)\in G$ , there goes $|f(x,y_1)-f(x,y_2)|\le L|y_1-y_2|$ . Say the constant $L$ Lipschitz constant . Cauchy problem $\qquad$ The initial value problem $$\frac{\mathrm{d} y}{\mathrm{d} x} =f(x,y),\qquad y(x_0)=y_0,$$ $\qquad$ where the function $f(x,y)$ is continuous on the rectangle closed region $D:|x-x_0|\le a,|y-y_0|\le b$ . Picard's existence and uniqueness theorem $\qquad$ Call it Picard theorem for short. Assuming that the function $f(x,y)$ is continuous on the closed region $D$ and satisfies the Lipschitz condition for $y$ , then the solution of the Cauchy problem exists and is unique on the interval $|x-x_0|\le h$ , where $$h=\min{\left\{ a,\frac{b}{M}\right\} },\qquad M=\max_{(x,y)\in D}{|f(x,y)|}.$$ Picard sequence $$y_n(x)=y_0+\int_{x_0}^{x}f\left(s,y_{n-1}(s)\right)\mathrm{d}s,\qquad |x-x_0|\le h,$$ where $h$ is defined the same as in Picard theorem . Tonelli sequence Note: Only the right side of the initial condition is discussed. $\qquad$ On the interval $I=[x_0,x_0+h]$ , where $h$ is defined the same as in Picard theorem , construct the sequence $\{y_n(x)\}$ as follows: For each positive integer $n$ , divide the interval $I$ into $n$ parts, taking the points as $x_k=x_0+kd_n$ , where $d_n=\frac{h}{n},k=1,2,...,n$ , and then defining $$\begin{matrix}
 y_n(x)=\begin{cases}
 y_0, & x\in\left[x_0,x_1\right], \\
 y_0+\displaystyle \int_{x_0}^{x-d_n}f\left(s,y_n(s)\right)\mathrm{d}s,  & x\in\left(x_1,x_n\right],
\end{cases} & n=1,2,\cdots
\end{matrix}.$$ We call the sequence $\{y_n(x)\}$ the Tonelli sequence . $\qquad$ In preparation for the exchange of integrals and limits in the later steps, one step in the proof of Picard theorem is to prove the uniform convergence of Picard sequences , using the difference $$|y_n(x)-y_{n-1}(x)|\le L\left|\int_{x_0}^{x}|y_{n-1}(s)-y_{n-2}(s)|\mathrm{d}s\right|,$$ treating the function column as a series of function terms, and applying the Weierstrass M discriminance. $\qquad$ In fact, the core of my problem is how to prove that the Tonelli sequence is uniformly convergent under the conditions of Picard theorem , which is that $f$ continuous on the closed region $D$ and has Lipschitz condition on $y$ . $\qquad$ I had a hard time doing the similar thing on the Tonelli sequence . When I take the difference and use the Lipschitz condition , I don't get a recursive inequality, but I get an inequality involving integration between B and B itself. $$\begin{align}|y_n(x)-y_{n-1}(x)| & = \left | \int_{x_0}^{x-d_n}f\left ( s,y_n(s) \right )\mathrm{d}s - \int_{x_0}^{x-d_{n-1}}f\left ( s,y_{n-1}(s) \right )\mathrm{d}s  \right | \\&=\left | \int_{x_0}^{x-d_{n-1}}\Big ( f\big ( s,y_n(s) \big )-f\big ( s,y_{n-1}(s) \big ) \Big )\mathrm{d}s + \int_{x-d_{n-1}}^{x-d_{n}}f\left ( s,y_n(s) \right )\mathrm{d}s  \right | \\&\le \left |\int_{x_0}^{x-d_{n-1}}L\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+M|d_{n-1}-d_n|\\&= L \left |\int_{x_0}^{x-d_{n-1}}\left|y_n(x)-y_{n-1}(x)\right |\mathrm{d}s\right |+\frac{Mh}{n(n-1)}\end{align}$$ $\qquad$ In addition, I know the Arzela-Ascoli theorem, and I also learned that Tonelli sequence have uniform boundedness and  equicontinuity, and it seems that equicontinuity plus point-by-point convergence can lead to uniform convergence (sorry but idk if it is right), but in the process I still face problems like the above. It seems that any attempt to apply the Lipschitz condition would create such problems making the proof could not continue. $\qquad$ I don't know if I'm thinking in the right direction. If you know how to prove it or anything related to it, please discuss below, tks!","['cauchy-problem', 'lipschitz-functions', 'ordinary-differential-equations', 'real-analysis']"
4672026,"If $X$ and $Y$ have the same distribution and $E[Y|X]=X$ almost surely, does that mean $X=Y$ almost surely?","Let $X$ and $Y$ be 2 random variables such that $X\sim Y$ and $E[Y|X]=X$ almost surely. Is it true that $X=Y$ almost surely? This is true in the Gaussian case and I was wondering if it was true in general. EDIT: I think I got it if $X$ and $Y$ are $L^2$ , can someone confirm? \begin{align*}
E[(Y-X)^2]
&= E[Y^2]-2E[YX]+E[X^2]\\
&= E[Y^2]-2E[E[YX|X]]+E[X^2]\\
&=E[Y^2]-E[X^2]\\
&=0
\end{align*} Hence $X=Y$ a.s. What about $L^1$ ?","['conditional-expectation', 'probability-distributions', 'probability-theory', 'probability']"
4672086,Number of flips on a binary string to get m consecutive 1s,"I am thinking about a deceptively simple problem, at least for my admittedly poor statistics standard. A $k$ long binary string of all $0$ s is given.
A random element is chosen, and flipped.
What is the expected number of flips until $m$ consecutive $1$ s appear? I would also be content with a ""toroidal"" string, such that for example $110001$ counts as a string with $3$ consecutive $1$ s. I could not make any progress at all. The best I have got so far is considering the string as a vertex of a $k$ -dimensional hypercube. Then the ""flipping"" turns the problem to a random walk on the hypercube. But how to characterise vertices that have $m$ consecutive coordinates equal to $1$ ?
Any hint welcome, such as ""read chapter X of textbook Y"", or ""have a look at this technique"". Thanks a lot EDIT Following the remarks from @lulu, I attach below a chart documenting the mean time for reaching $m$ consecutive $1$ s on a string long $k$ , starting from all $0$ s (averages over one thousands trials). The numerical results are (rows for string length, columns for mean flipping time) \begin{equation}
  \left(
    \begin{array}{*5{c}}
     0 & 0 & 0 & 0 & 0 & 0\\
     0 & 1 & 0 & 0 & 0 & 0\\
     0 &  1 & 3.986 & 0 & 0 & 0\\
     0 &  1 &  5.112 & 13.87 & 0 & 0\\
     0 &  1 &  6.571 &  18.165 & 43.81 & 96.468\\
  \end{array}\right)
\end{equation} For completeness's sake I paste below the snippet used to get the numbers and the plot import numpy as np
import random
import matplotlib.pyplot as plt

def flipping (string_length,m):

    s = np.zeros(string_length,dtype=bool)
    counter = 0
    max_count = 0
    count_one = 0
    while max_count < m: 
        count_one = 0
        max_count = 0
## Choose random element to flip, and flip
        randelem = np.random.randint(string_length)
        s[randelem] = np.logical_not(s[randelem])
## Count consecutive 1s
        for i in range(string_length):
            if s[i] == 0:
                count_one = 0
            else:
                count_one += 1
                max_count = max(max_count, count_one)
                counter +=  1

    return counter,s , max_count



number_of_m = 10
number_of_l = 6
number_of_realis = 1000
## Initialise array to hold results: 
## rows to string lengths, columns to number of required consecutive ##1s
res = np.zeros((number_of_l, number_of_m))
## choose number of trials, initialise array to hold individual ##results
realis = np.zeros(number_of_realis)
## loop using the above defined ""flipping"" function
for len in range ( number_of_l):
    for m in range (len+1):
        for j in range(number_of_realis):
            realis[j] = flipping(len, m)[0]
        res[len,m] = np.average (realis)

## plot

for i in range (1, 6):
    plt.plot(np.arange(1,i+1),  res[i,1:i+1],label=f""string lenght = {i}"")
plt.xlabel(""Number of consecutive 1s"")
plt.ylabel(""Mean number of flips"")
plt.legend()
plt.savefig(""math_stack.png"") I also add a plot for larger $m$ , it is actually quite interesting, it actually reminded me that I would already be quite content getting some result in the limit of string length $\to \infty$ , at fixed $m$ , maybe this is more tractable.","['binary', 'combinatorics', 'random-walk']"
4672149,Prove limit of non linear recursive sequence,"Let $p_0=1$ and $p_2=0$ . For $n\in\mathbb{N}$ , $n\geq 4$ , $n$ even the series is defined by \begin{align*}
p_{n} =\frac{n-2}{n-1}\left(p_{n-2} + \frac{1}{n-3}p_{n-4}  \right).
\end{align*} I played around with that and I am pretty sure that \begin{align*}
\lim\limits_{n\rightarrow \infty} p_n = \frac{1}{\sqrt{e}},
\end{align*} but I did not manage to prove that.
The series itself is a solution to a nice riddle that I won't reveal for now so as not to spoil anything. But $p_n$ are probabilities, so $0\leq p_n\leq 1$ for all $n$ . Maybe someone here has an idea how to go about it? EDIT If it is easier we can also define the series for $t\in\mathbb{N}$ as $p_0=1$ , $p_1=0$ and for $t\geq 2$ \begin{align*}
p_{t} =\frac{2t-2}{2t-1}\left(p_{t-1} + \frac{1}{2t-3}p_{t-2}  \right).
\end{align*} EDIT-2 @dxiv: Your answer is completley sufficient. I was't aware oeis covered that so well and I have no idea how you could spot the transformation so quickly...amazing! Now that the cat's out of the bag, the original riddle (which is basically the dancing problem) goes like this:
You have $n=2t$ students sitting pairwise at $t$ tables in a classroom and you randomly create a new seating order. What is the probability that no old partner sits together again? And this is how I came up with the sequence:
Randomly drawing names from a bowl: With probability $\frac{n-2}{n-1}$ we have no old couple at the first table. Given this situation, the old partners of the first couple can either sit together or not. They sit together: This has probability $\frac{1}{n-3}$ . After that we have the original situation minus two couples. They do not sit together: We can treat them as if they had been an old couple and we immediatly have the original situation minus one couple. This leads directly to $p_{n} =\frac{n-2}{n-1}\left(p_{n-2} + \frac{1}{n-3}p_{n-4}  \right)$ .","['limits', 'recurrence-relations', 'sequences-and-series']"
4672154,Stuck while solving $\frac{d}{dx} \int_{x^2}^{x^3} e^{t^2} dt$,"I have to solve $\frac{d}{dx} \int_{x^2}^{x^3} e^{t^2} dt$ , and below is what I've done. $$\frac{d}{dx} \int_{x^2}^{x^3} e^{t^2} dt$$ $$= \lim_{h \to 0} \frac{\int_{(x + h)^2}^{(x + h)^3} e^{t^2} dt - \int_{x^2}^{x^3} e^{t^2} dt}{h}$$ $$= \lim_{h \to 0} \frac{\int_{x^3}^{(x + h)^3} e^{t^2} dt - \int_{x^2}^{(x + h)^2} e^{t^2} dt}{h}$$ Due to the mean value theorem for definite integrals , $$\int_{x^3}^{(x + h)^3} e^{t^2} dt = ((x + h)^3 - x^3) e^{p^2} = (3 h x^2 + 3 h^2 x + h^3) e^{p^2}$$ $$\int_{x^2}^{(x + h)^2} e^{t^2} dt = ((x + h)^2 - x^2) e^{q^2} = (2 h x + h^2) e^{q^2}$$ Substituting, $$\cdots = \lim_{h \to 0} \frac{(3 h x^2 + 3 h^2 x + h^3) e^{p^2} - (2 h x + h^2) e^{q^2}}{h}$$ $$= 3 e^{p^2} x^2 - 2 e^{q^2} x$$ How do I clean up $p$ and $q$ ?","['integration', 'real-analysis', 'calculus', 'limits', 'derivatives']"
4672223,Kneser-like graphs,"Given three integers $p,q,r$ with $p \leq q \leq r/2$ , consider a graph $G(p,q,r)$ whose vertices are $q$ -element subsets of $\{1,...,r\}$ , and such that two vertices are adjacent if and only if their intersection has size $p$ . For $p=0$ we get Kneser graphs ; for $p=q-1$ we get Johnson graphs . Does this class of graph have a name, for general $p$ ?","['graph-theory', 'combinatorics', 'discrete-mathematics', 'terminology']"
4672237,How does one prove that partial functions of measurable functions of 2 var. are nearly all measurable?,"How does one prove that if $F: X \times Y \to Z$ is measurable for a product measure, then the partial function $F(x,.) : Y \to Z$ (defined by $y \mapsto F(x,y) )$ is measurable for almost all $x$ in $X$ ? I avoid being more precise for a start so that I have the best chances of getting a usable answer for my purpose. (Some not mentioned hypotheses are probably needed.) Motivation: In the book Operator Theory in Function Spaces by Kehe Zhu integral operators are introduced with $Tf(x) = \int_X K(x,y)f(y)d\mu(y)$ where $K$ and $f$ are in $L^2$ (and therefore measurable) and to make sure that the integral exists it is necessary to have the partial functions also in $L^2$ which will be easy once they are measurable (for almost all $x$ )","['measure-theory', 'lp-spaces', 'measurable-functions']"
4672251,"Finding a formula for the $n^{\text{th}}$ term in a series solution to $y''+y'+e^{-x}y=0,y(0)=3,y'(0)=5$","I am tasked to find a formula for the $n^{\text{th}}$ term in a series solution to $y''+y'+e^{-x}y=0$ subject to initial conditions $y(0)=3,y'(0)=5$ . If $y$ is a solution to this DE and $D=\frac{d}{dx}$ it isn't too hard to see by taking successive derivatives that $$\Big[D^{n+2}+D^{n+1}+e^{-x}(D-1)^n\Big]y=0 \text{ for all } n\geq 0$$ Since $(D-1)^n=\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k$ can write this as $$\Bigg[D^{n+2}+D^{n+1}+e^{-x}\sum_{k=0}^n(-1)^{n-k}{n\choose k}D^k\Bigg]y=0$$ If we evaluate this as $x=0$ and rearrange terms we can say $$y^{(n+2)}(0)=-\Bigg[y^{(n+1)}(0)+\sum_{k=0}^n(-1)^{n-k}{n \choose k}y^{(k)}(0)\Bigg]$$ Is there any chance of finding a general formula for $y^{(n)}(0)$ in terms of $n$ ? I also considered writing $y$ as $\sum_{n=0}^{\infty}a_nt^n$ and $e^{-t}$ as $\sum_{n=0}^{\infty}\frac{(-t)^n}{n!}$ and solving for the coefficients this way, but I wasn't about to spend a majority of my day collecting coefficients. Thoughts? Thanks.","['ordinary-differential-equations', 'sequences-and-series']"
4672283,Polynomials that are zero as functions on finite fields,"I want to find a description for the ideal $Z \subseteq k[\underline{x}]$ where $Z = \{f \in k[\underline{x}] | f(\mathbb{A}^n_k) = \{ 0 \}\}$ . I can show that for an infinite field $k$ , the polynomials in $k[\underline{x}]$ inject into the set of functions from $\mathbb{A}^n_k$ to $k$ , but I'm struggling to find all polynomials which are zero functions when $k$ is finite. For $\mathbb{A}^1_k$ , we have that all such polynomials lie in the ideal $(x^q - x)$ since being zero at a point gives a linear factor at that point and $x^q - x$ is the product of these linear factors where $q = |k|$ . Now I think $Z = (x_1^q - x_1, ... , x_n^q - x_n)$ since I haven't found any polynomials not of this form (even with an extensive computer search!), but I'm having trouble proving it. Any reference requests are greatly appreciated!","['field-theory', 'finite-fields', 'algebraic-geometry', 'commutative-algebra']"
4672358,What is the sum of the series $\sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}}$?,"We have Euler's well known result $$
\sum_{r = 1}^n\frac{1}{r} = \log n + \gamma + O\Big(\frac{1}{n}\Big)
$$ where $\gamma$ is the Euler-mascheroni constant. I experimentally observed a generalisation of the above. We have for $a > 1$ , $$
\sum_{r = 1}^{n-1}{1 \over r \sqrt[a]{\log \big(\frac{\log n}{\log r}\big)}} =   \Gamma\Big(1 - \frac{1}{a}\Big)\log n + C_a + O\Big(\frac{1}{n}\Big)
$$ where $C_a$ is a constant that depends only on $a$ . If this is true than Euler's result will correspond to the special case $a \to \infty$ . Question : Can this be proved or disproved?","['elementary-number-theory', 'number-theory', 'gamma-function', 'sequences-and-series', 'limits']"
4672364,Pullback and wedge product,"I am essentially asking the same question as this one . My goal is to prove $$f^*(\omega\wedge\theta)=f^*\omega\wedge f^*\theta.$$ The step I'm missing is precisely the one in the linked question, that is, why is $$(\omega\wedge \theta)(f_*(v_1),\cdots ,f_*(v_p),f_*(w_1),\cdots ,f_*(w_q))$$ the same as $$\omega(f_∗(v_1),⋯,f_∗(v_p))\wedge \theta(f_*(w_1),⋯,f_∗(w_q))?$$ The problem is already that I do not really know how to evaluate something of the form $$(\omega\wedge\theta)(v_1,\dots,v_p,\,w_1,\dots,w_q)$$ in general. I assume that, what I need for that, is the ""summation formula for wedge products"", as mentioned in the linked question. What formula is this? Or maybe, to get started, how would you even compute $$(\mathrm dx\wedge \mathrm d y)(\partial/\partial y,\partial/\partial x),$$ as is mentioned in the linked post's comments?","['exterior-algebra', 'differential-forms', 'differential-geometry']"
4672390,Divide a circle into n segments of equal area where all segments share a common point on the circumference of the circle,"Divide a circle into n segments of equal area where all segments share a common point on the circumference of the circle. The segments must be created by drawing straight lines from a common point on the circumference to the point at which they intersect the circumference again. Each segment must have an equal area. I am adding the simplest case here in the event that my description is unclear (wanting A, B, C to have equal areas in the attached image). The use case here is agricultural - a single gate opening into several fields of equal area so it doesn't need to be mathematically perfect, just close enough - I would be happy with an approach which minimized the difference in the area of n segments inside the circle.","['calculus', 'trigonometry']"
4672432,"Difference betwwen $(\sqrt{2}, \pi)\cap \mathbb Q$ and $[\sqrt{2}, \pi]\cap \mathbb Q$ in $\mathbb Q$","Consider two metric spaces $X,Y$ such as $Y\subseteq X$ . Let $A\subseteq Y$ . $A$ is open in $Y$ $\iff$ $\exists B\subseteq X$ open in $X$ such that $ A=B\cap Y$ According to this theorem, the set $(\sqrt{2} , \pi) \cap \mathbb Q$ must be open in $\mathbb Q$ . However, as $\sqrt{2}$ and $\pi$ are irrationals, we are excluding boundaries that do not exist in $\mathbb Q$ , so I think the set $(\sqrt{2} , \pi) \cap \mathbb Q$ is closed. The problem is that I see no differences (in terms of openness / closedness) between $(\sqrt{2} , \pi) \cap \mathbb Q$ and $[\sqrt{2} , \pi] \cap \mathbb Q$ . Could it be that they are both open and closed ?","['general-topology', 'irrational-numbers', 'rational-numbers']"
4672436,"If $\{x_n\}$ is positive, decreasing and $\sum x_n=\infty,$ then $\sum x_ne^{-\frac{x_{n}}{x_{n+1}}}=\infty$","The problem is Suppose that $\{x_n\}$ is positive, monotonic decreasing and $\sum_{n=1}^\infty x_n=+\infty$ . Prove that $$\sum_{n=1}^\infty x_ne^{-\frac{x_{n}}{x_{n+1}}}=+\infty.$$ This is a past examination problem of Analysis . I tried many ways, but failed. One trival idea is to prove $\frac{x_{n}}{x_{n+1}}$ is bounded above, but I failed to show it, maybe it is not ture.  Hope to find some hints here, thanks in advance.","['divergent-series', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4672498,Local ring homomorphism induced by $\varphi$,"At page $59$ in Görtz & Wedhorn 's ""Algebraic Geometry 1"" book, there is the following text: ""Let $\varphi : A \rightarrow B$ be a homomorphism of rings and set $X = \operatorname{Spec} B$ and $Y = \operatorname{Spec} A$ . Let $^a\varphi : \operatorname{Spec} B \rightarrow \operatorname{Spec} A$ be the associated continuous map. We will now define a morphism $(f,f^b):X \rightarrow Y$ of locally ringed space and such that $f = ^a\varphi$ and $f^b_Y:A=\mathscr O_Y(Y) \rightarrow (f_*\mathscr O_X(Y))=B$ equals $\varphi$ . Set $f = ^a\varphi$ , for $s \in A$ , We have $f^{-1}(D(s))=A_s \rightarrow B_{\varphi(s)}= (f_*\mathscr O_X)(D(s))$ is the homomorphism induced by $\varphi$ . This ring homomorphism is compatible with restrictions to principal open subsets $D(t) \subset D(s)$ . As the principal open subsets form a basis of the topology, this defines a homomorphism $f^b:\mathscr O_Y \rightarrow f_*\mathscr O_X$ of sheaves of rings. For $x \in X$ , the homomorphism $f^\#_x: \mathscr O_{Y,f(x)} = A_{\varphi^{-1}(p_x)} \rightarrow B_{p_x} = \mathscr O_{X,x}$ is the homomorphism induced by $\varphi$ and in particular it is a local ring homomorphism. This finishes the definition of $(f,f^b)$ ."" I understand the first two paragraphs. So we have a special homomorphism $(f,f^b)$ of locally ringed spaces $(X,\mathscr O_X)$ and $(Y,\mathscr O_Y)$ . This $(f,f^b)$ is special enough such that for $s \in A$ , $f^b_{D(s)}:f^{-1}(D(s))=A_s \rightarrow B_{\varphi(s)}= (f_*\mathscr O_X)(D(s))$ is the homomorphism induced by $\varphi$ , that is $f^b_{D(s)}(\frac {a} {s^n}) = \frac {\varphi(a)} {\varphi(s)^n}$ , and $f=a^\varphi$ . Now we can say that $(f,f^b)$ is fixed because for $s\in A$ , $D(s)$ is the principal open subsets so that $f^b$ is fixed. And $f^b$ is fixed so is $f^\#$ since $f^b$ and $f^\#$ have an one-to-one correspondence. Now we have $f^\#$ being fixed, so for $x\in X$ , the local ring homomorphism defined as $f^\#_x$ is fixed. Then I have trouble understanding the third paragraph which says ""For $x \in X$ , the homomorphism $f^\#_x: \mathscr O_{Y,f(x)} = A_{\varphi^{-1}(p_x)} \rightarrow B_{p_x} = \mathscr O_{X,x}$ is the homomorphism induced by $\varphi$ "". That means $f^\#_x(\frac {a} {q^n}) = \frac {\varphi(a)} {\varphi(q)^n}$ . But it is not at all obvious, right? So why is $f^\#_x$ being the homomorphism such that $f^\#_x(\frac {a} {q^n}) = \frac {\varphi(a)} {\varphi(q)^n}$ ?","['algebraic-geometry', 'sheaf-theory']"
4672523,Challenging probability exercise,"I'm dealing with the following exercise (don't be scared about the lenght of the exercise, it has been almost solved): Mr Smith has 3 boxes of chocolates. The first box contains 20 bars of milk chocolate, 12 bars of dark chocolate and 8 bars of white chocolate. The second box contains 10 bars of milk chocolate, 10 dark chocolates and 10 milk chocolates.
The third box has 10 milk and 10 dark chocolates. Mr Smith tosses a fair coin twice for choosing from which box he takes a bar of chocolate. If, by tossing a coin twice, he gets ""Head"" twice in a row, Mr Smith randomly chooses a bar of chocolate from the first box; if he gets tails twice in a row,Mr Smith randomly chooses a bar of chocolate from the second box; otherwise he randomly chooses from the third box. It is clear,that once the box has been chosen, the flavour of the chocolate bar is randomly chosen. (a) What's the probability that the first bar of chocolate is a milk chocolate? My answer: I call $A$ the event of choosing the first box. $P(A)=1/4$ ; I call $B$ the event of choosing the second box. $P(B)=1/4$ ; I call $C$ the event of choosing the second box. $P(B)=1/2$ ; And $P(M|A)=1/2,P(M|B)=1/3,P(M|C)=1/2$ Let P(M) the probability of picking milk as first chocolate flavour, and it is equal to: $P(M)=P(A \cap M)+P(B \cap M)+P(C \cap M)$ And this also reads as: $P(M)=P(M|A)P(A)+P(M|B)P(B)+P(M|C)P(C)=11/24$ Is that correct? (b) Mr Smith has drawn a milk chocolate. What's the probability that it comes from the first box? Applying the Bayes theorem: $P(A|M)=\frac{P(M|A)P(A)}{P(M)}=\frac{(1/2)(1/4)}{11/24}=3/11$ Is that correct? (c) Mr Smith has just eaten the first chocolate bar and it was a milk chocolate. What's the probability that the second chocolate bar taken from the same box is again a milk chocolate? Answer: I define as $P(M_2)$ the probability that the second chocolate bar is milk chocolate. Then, $P(M_2|M)=P(M_2 \cap A|M)+P(M_2 \cap B|M)+P(M_2 \cap C|M)$ which should read as: $P(M_2|M)=P(M_{2} | A \cap M)P(A|M) + P(M_{2} | B \cap M)P(B|M) + P(M_{2} | C \cap M)P(C|M) $ Here I'm stuck because the professor's slides provide an hint, saying that I should end up with computing the following probability: $P(M_2|M)=P(M_2|A)P(A|M) + P(M_2|B)P(B|M) + P(M_2|C)P(C|M)$ And this seems to occur iff $P(M_{2} | A \cap M) = P(M_2|A)$ , and the same for B,C. Why is that possible? I guess, e.g., $P(M_2|A \cap M)=19/39$ . right? Maybe, I can have the equality between $P(M_2|A \cap M)=P(M_2|A)$ , if I interpret the event $M_2$ as the second milk chocolate drawn, and conditioning on $M$ is just redundant. Is that correct?","['logic', 'probability-theory', 'probability']"
4672579,Prove a relation based on angles $\frac{\pi}{11}$ and $\frac{\pi}{22}$,"$\textbf{Question:}$ Prove that $$\frac{1}{2}+\cos\frac{2\pi}{11}+\cos\frac{4\pi}{11}=\frac{1}{4\sin\frac{\pi}{22}}$$ $\textbf{My Attempt:}$ Let $z= e^{\frac{i\pi}{22}}$ then the given relation will simplify out to $$z^{18}-z^{16}+z^{14}-z^{12}+z^{10}-z^9-z^{8}+z^{6}-z^4=0$$ $$z^{14}-z^{12}+z^{10}-z^{8}+z^{6}-z^5-z^{4}+z^{2}-1=0$$ Now I am not getting how to proceed further by this approach so I switched to another approach i.e. multiply out the whole expression and then open it using product of trigonometric ratio rules as follows: $$2\sin\frac{\pi}{22}+2\left(\sin\frac{5\pi}{22}-\sin\frac{3\pi}{22}\right)+2\left(\sin\frac{9\pi}{22}-\sin\frac{7\pi}{22}\right)=1$$ which also seems like a dead end to me, so if anyone has insights on how to proceed further please help me out...","['trigonometric-series', 'algebra-precalculus', 'trigonometry']"
4672596,"Prove the following from $L = \int_0^1 x^2 \sin^ {-1 }(\frac{x^2}{1+\sqrt{1+x^4}}) \,dx$","Prove that $$L=\int_0^1 x^2 \sin^
{-1
}\left(\frac{x^2}{1+\sqrt{1+x^4}}\right) \,dx= \frac{1}{3 \sqrt2} \left(\sqrt2\sin^
{-1
}(\tan{\frac{π}{8}})+\frac12 \log_e\frac{(1+\sqrt{\sqrt 2-1})}{(1-\sqrt{\sqrt 2-1)}}-\sqrt{\sqrt 2+1}\right)$$ I started by substituting $x = \tan(y)$ after which I spent quite some time and figured out; $$L_1=\frac{1}{3}\int_0^\frac{π}{4} x^2 \sin^
{-1
}(\tan\frac{y}{2}) \ d(\tan y)^\frac{3}{2}$$ after which I chose integration by parts as the approach and finally ended on; $$ L_2= \int_0^\frac{π}{4} \frac{2\tan\frac{y}{2}}{(1-\tan^2\frac{y}{2}) \sqrt{1-\tan^2\frac{y}{2}}} \ d(\tan\frac{y}{2})$$ I am stuck on the step mentioned above, I am unable to think of a substitution that can work here. It would be really great to see any alternate or a faster appproach to proving the above relation. Edit -- To get $L_1$ , after substitution i converted to siny and cosy and simplified it, To get $L_2$ ,  i used $\sin^
{-1
}(\tan\frac{y}{2})$ as 'U' and $d(tan y)^\frac{3}{2}$ as the 'V' $\sin^
{-1
}(\tan\frac{y}{2})$ in the integration by parts (UV) method","['integration', 'calculus', 'algebra-precalculus']"
4672604,Differentiation with respect to a ratio,"I have a function: $y= 35(α/β)^4-84(α/β)^5+70(α/β)^6-20(α/β)^7$ Is it legitimate to say $x = α/β$ , and therefore the derivative with respect to $x$ is: $140(α/β)^3-420(α/β)^4+420(α/β)^5-140(α/β)^6$ Noting that alpha is a variable and beta is a constant value, and their ratio (and therefore the value of x) in this specific problem is a value between 0 and 1, where 0 is the start of a rotation, and 1 is the end of the rotation. Sorry that this is super basic, I did this work well over a decade ago, its a long time since I last did any calculus and I feel like I am losing my mind looking back at this. Slapping it into Wolfram Alpha suggests I was correct, but I'm not sure I trust it or myself! :D","['derivatives', 'ordinary-differential-equations']"
4672635,Proof of the Riemann curvature endomorphism as parallel transport around an infinitesimal loop(Thm 7.11 of Lee's Introduction to Riemannian Manifolds),"$\def\bbR{\mathbb{R}}$ I have looked for a while about this in MSE and I couldn't find anything. Here it goes: In J. M. Lee, Introduction to Riemannian Manifolds , 2nd ed., there is this result: I wanted to ask about Lee's proof of Theorem 7.11. For each $\delta,\varepsilon\in\bbR\setminus\{0\}$ and $z\in T_pM$ , denote the limiting term in the RHS of (7.10) as $$
F_{\delta,\varepsilon}(z)=\frac{P_{\delta, 0}^{0,0} \circ P_{\delta, \varepsilon}^{\delta, 0} \circ P_{0, \varepsilon}^{\delta, \varepsilon} \circ P_{0,0}^{0, \varepsilon}(z)-z}{\delta \varepsilon}.
$$ What Lee's proof actually does is showing that the following iterated limit formula holds: $$
R(x,y)z= \lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z).
$$ My question is: how does one show then that $\displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ ? Looking ""iterated limits"" in google brought me to this : I am trying to check the hypotheses for our situation. On the one hand, from Lee's proof one has that $\displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ exists. But on the other hand, I don't know how to show existence of $\displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z)$ , nor how to argue that of among the limits $\displaystyle\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ and $\displaystyle\lim_{\delta\to 0}F_{\delta,\varepsilon}(z)$ , at least one converges uniformly. My questions are: How one would verify the other hypothesis of the theorem on interchange of two limits? Is there any different approach to this auxiliar theorem to show that $\displaystyle\lim_{\delta\to 0}\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)=\lim_{\delta,\varepsilon\to 0}F_{\delta,\varepsilon}(z)$ ? In case it helps, here is the expression from Lee's proof for the limit in $\varepsilon$ : $$
\lim_{\varepsilon\to 0}F_{\delta,\varepsilon}(z)
=
\frac{P_{\delta, 0}^{0,0}\left(D_t Z(\delta, 0)\right)-D_t Z(0,0)}{\delta},
$$ where $D_t$ is the covariant derivative along the curve $t\mapsto\Gamma(s,t)$ (where $s=\delta$ on the first case and $s=0$ in the second case) and $Z$ is the vector field along $\Gamma$ (i.e., a lifting to $TM\to M$ of the map $\Gamma$ ) explained at the beggining of Lee's proof: Define a vector field $Z$ along $\Gamma$ by first parallel transporting $z$ along the curve $t\mapsto\Gamma(0,t)$ , and then for each $t$ , parallel transporting $Z(0,t)$ along the curve $s\mapsto \Gamma(s,t)$ . The resulting vector field along $\Gamma$ is smooth by another application of Theorem A.42 (the fundamental theorem on flows) as in the proof of Lemma 7.8. In the quote I have incorporated the correction to p. 201.","['riemannian-geometry', 'curvature', 'holonomy', 'limits', 'differential-geometry']"
4672652,Can't get the coefficient of a particular solution for a Differential Equation,"I am trying to solve a differential equation that goes as follows: $$y'' - \left( a + 1 \right) \cdot y' + a \cdot y = 2 \cdot \cosh(a \cdot x),\, a \in \mathbb{R}$$ I have progressed enough and now trying to get the particular solution for $e^{ax}$ , using the form $y_p = Ae^{ax}$ . The problem is that putting it in the equation gives $0 = e^{ax}$ , making me unable to get $A$ to begin with. I suspect that I've done something wrong along the road but can't determine it. I appreciate any help I can get.",['ordinary-differential-equations']
4672689,Find the product of $5x^2+5xy-9y^2$ and $5x^2-5xy-9y^2$,"Find the product of $5x^2+5xy-9y^2$ and $5x^2-5xy-9y^2$ I did a straight-up multiplication of every term and got the correct answer, $25x^4-115x^2y^2+81y^4$ However given the identical expressions given (differing only in sign), I can't help but feel there is a pattern/simplification I'm not seeing here and a direct multiplication is in fact the wrong (or inefficient) method. I thought it could be of the form $(a+b)(a-b)$ , but the signs don't match. Thank you.",['algebra-precalculus']
4672714,Brezis' exercise 4.14: Egorov theorem,"I'm trying to solve below exercise in Brezis' Functional Analysis , i.e., Exercise 4.14 Let $(\Omega, \mathcal F, \mu)$ be a measure space such that $\mu(\Omega)<\infty$ . Let $f,f_n$ be real-valued measurable functions such that $f_n \to f$ $\mu$ -a.e. For $\alpha>0$ and $n \in \mathbb N^*$ , let $$
S_n (\alpha) := \bigcup_{k \ge n} \{ |f_k-f| > \alpha\}.
$$ Prove that $\mu(S_n (\alpha)) \xrightarrow{n \to \infty} 0$ for all $\alpha>0$ . Prove that for every $\delta>0$ there is $A \in \mathcal F$ such that $\mu(A) < \delta$ and $f_n \to f$ uniformly on $A^c :=\Omega \setminus A$ . Could you confirm if my below attempt is correct? Proof 1. Let $$
C := \{\omega \in \Omega : f_n (\omega) \xrightarrow{n \to \infty} f (\omega)\}.
$$ By $\varepsilon$ - $\delta$ argument (please see here for more details), we have $$
C=\bigcap_{m \in \mathbb N^*} \bigcup_{N \in \mathbb N} \bigcap_{n \ge N} \{|f_n - f| \le 1/m\} \in \mathcal F.
$$ We have $\mu(C^c)=0$ , so $$
\mu \bigg ( \bigcup_{m \in \mathbb N^*} \bigcap_{N \in \mathbb N} S_N (1/m) \bigg ) = 0.
$$ Then $$
\mu \bigg (\bigcap_{N \in \mathbb N} S_N (1/m) \bigg ) = 0
\quad \forall m \in \mathbb N^*.
$$ Notice that $S_{N+1} (1/m) \subset S_N (1/m)$ . By the continuity of finite measure from above, $$
\mu (S_{N} (1/m)) \xrightarrow{N \to \infty}  0
\quad \forall m \in \mathbb N^*.
$$ Fix $\delta > 0$ . For each $n \in \mathbb N$ , there is $\varphi (n) \in \mathbb N$ such that $\mu(S_{\varphi (n)} (1/n)) < \delta 2^{-n}$ . Let $A := \bigcup_n S_{\varphi (n)} (1/n)$ . Then $\mu(A) \le \sum_n \delta 2^{-n} \le \delta$ and $$
A^c = \bigcap_n \bigcap_{k \ge \varphi (n)} \{ |f_k-f| \le 1/n\}.
$$ Fix $\varepsilon >0$ . There is $N \in \mathbb N$ such that $1/N < \varepsilon$ . If $k \ge \varphi(N)$ then $|f_k-f| \le 1/N < \varepsilon$ on $A^c$ . This completes the proof.","['measure-theory', 'solution-verification', 'uniform-convergence']"
4672770,What does the difference of constants in equations of parallel straight lines mean?,"I was trying to prove the formula for distance of a point in the cartesian plane from a line. And there are many easy proofs.
I was looking for something “tastier”. For equations of planes in 3d, the coefficients $A$ , $B$ and $C$ in $Ax + By + Cz + D = 0$ represent a vector perpendicular to the plane! My goal was to find such a connection for 2d lines. Indeed, for $Ax + By = 0$ , the dot product of any vector $(x,y)$ with $(A,B)$ is $0$ . And the constant term can arise when we shift our origin! My goal then is to see how this shift of the origin relates to the constant term that’s added. Notice that the intercepts of the line $Ax + By + C = 0$ are $|\frac{-C}{A}|$ and $|\frac{-C}{B}|$ and the distance between this line and another line $Ax + By + D = 0$ is $\frac{\left|C-D\right|}{\sqrt{A^{2\ }+B^{2}}}$ . The pattern is clearly visible if we set $D = 0$ . I wanted to see how I could relate this to vectors. Firstly, the distance between the origin and the line can be written as $k\sqrt{A^2 + B^2}$ where $k \in \Bbb{R^+}$ . This distance is also the dot product $(kA,kB)\cdot(|\frac{-C}{A}|,0)$ . Hence, $|kC|$ is the same distance. Notice that the line equation $Ax + By + C = 0$ can be written as $A(x-kA) + B(y-kB) = 0$ . It is clear that $-C = kA^2 + kB^2$ .
Evaluating all previous equations, we get $$\frac{|C|}{A^2 + B^2}\sqrt{A^2 + B^2} = |kC|$$ I want to expand this further, can this be done along more vectors other than $(1,0)$ or $(0,1)$ or $(A,B)$ ? My understanding is of a highschool student but dont dumb your answers down, but don’t assume my knowledge of linear algebra or algebraic geometry to be like a math major’s. (I will probably join a BMath course soon though!)","['vectors', 'orthogonality', 'geometry', 'linear-algebra', 'algebra-precalculus']"
4672780,Is my method of approximation correct to solve this question?,"(I hope this one to be an appropriate question over StackExchange platform.) Given $y = \dfrac{\sqrt{1 + 2x} \sqrt[4]{1 + 4x}}{\sqrt[3]{1 + 3x} \sqrt[5]{1 + 5x} \sqrt[7]{1 + 7x}}$ , to find $y'(0)$ . $$y = (1 + 2x)^{\frac{1}{2}}(1 + 4x)^{\frac{1}{4}}(1 + 3x)^{-\frac{1}{3}}(1 + 5x)^{-\frac{1}{5}}(1 + 7x)^{-\frac{1}{7}}$$ Before differentiating $y$ , $lim_{x \to 0} y$ is obtained using the method of binomial expansion and the powers of $x$ greater than $1$ have been neglected. Reason to neglect: as $x$ tends to $0$ , the denominator of $x$ is on the increasing side, hence raising $x$ to higher power (say $a$ ) lowers its value significantly so that the expression ( $x^a$ ) tends more towards $\texttt{zero}$ . Having said that, $$\lim_{x \to 0} y = (1 + x + ...x^2 + ...)(1 + x + 0)(1 - x + ...x^2 + ...)(1 - x + 0)(1 - x + 0) = (1 + x)^2(1 - x)^3$$ Writing more casually: $$\lim_{x \to 0} y = (1 + 2x + ...)(1 - 3x + 0) \approxeq 1 + (2 - 3)x + 0 = 1 - x$$ So now on differentiating $y$ at $x = 0$ , it is expected to get: $y'(0) = 0 - 1$ , i.e., $y'(0) = \mathbf{- 1}$ . I duly acknowledge that this method is not an ideal method but I want to enquire whether this type of inference will yield correct answers in most cases or not and whether (all) the inferences used/mentioned here are appropriate. Also tell me how do you feel about this method of solving. Thank you!","['ordinary-differential-equations', 'calculus', 'functions', 'limits', 'derivatives']"
4672836,"Exact sequence of sheaves in Beauville's ""Complex Algebraic Surfaces"" Fact I.17","Here $S$ is a complex surface and $C$ is an irreducible curve on $S$ . I can't see why ""considering the exterior powers"" for the sequence $0\to{{\Omega^1}_{S}(-C)}|_C\to {\Omega^1}_{S|C}\to \Omega_C\to 0$ would give $\mathcal O|_S(K+C))|_C=\omega_C$ . I think we need to show the sequence is split, but why is that true by considering the exterior powers? Also, I am a little confused about what the sections of ${\mathcal O}_S(-C)|_C$ look like. I know they are the sections of the line bundle associated with the divisor $-C$ though. At last, aren't ${\Omega^1}_{S|C}$ and $\omega_C$ just the sheaf of 1-forms on $C$ ? What's the difference?","['algebraic-geometry', 'sheaf-cohomology']"
4672852,Why don't these hexagons tile seamlessly-- without space in between? Where in my math have I gone silly?,"Intro to the Problem Hi there! I've been trying to seamlessly tile hexagons on an XY plane-- without spaces in between. I've currently accomplished this , however: which as you can tell, isn't seamless or uniform in spacing. What I'd like answered here is, where in my math have I veered off track, for my result not to match what I desire? Why does the distance between the hexagons vary instead of being constantly zero? If you can't answer that because I've failed to communicate my process effectively, how would you go about tiling hexagons on an XY plane to achieve the desired result? The Math Behind the Tiling In order to tile regular hexagons as in the desired seamless result, I took the approach of stacking hexagons in radial layers. Starting with a center hexagon ( L0 ), I'd surround it with a layer ( L1 ) of hexagons, then I'd surround L1 with L2 of hexagons, and so on. My ultimate question regarding said radial stacking approach was: what's the distance and the angle from the center of the starting hexagon ( L0 ) to the center of any other hexagon in the tessellation? Knowing such an angle and distance, I could radially plot each hexagon from the center of the tessellation. By analyzing the case of circles, finding that the centers of two similar bitangent circles are a distance of twice their radius apart, I figured that the centers of two similar regular hexagons are a distance of twice their radius apart-- only, the radius of a hexagon changes; it isn't constant. Here's a GeoGebra diagram demonstrating how I found the radius of a hexagon-- or the distance from its center to its perimeter, given an angle: Distance from the center of a hexagon to its perimeter, at angle theta . Because this radius repeats every PI/3, or TAU/6 radians, the input angle for which to calculate a radius can be modulated by PI/3, such that it becomes ( angle MOD PI/3 ). For a regular polygon with n sides, the input angle with which to find a radius can be modulated by TAU/n radians. I also graphed my derived equations in Desmos: Desmos graph of n-sided regular polygon . They seem to check out. Once I'd found the radius of a hexagon at an angle, I postulated that to find the coordinates of the center of another hexagon, say from origin L0 to a hexagon in set L1 , I simply had to find the angle between the L0 and the next hexagon, find the radius of that angle as a vector, and multiply that radius vector by the layer index + 1 (layer 1 + 1 = 2). For L0 to L2 , I'd multiply said vector by 3, because layer 2 + 1 = 3. Here's a diagram of said vector multiplication in GeoGebra: Distances between hexagon centers . Coding the Tessellation Coding the tessellation of the hexagons in both Python (visualized in Blender) and JavaScript (visualized in P5.js), I achieved these undesirable results: Blender/Python result , P5.js/JavaScript result . Here's my code in JavaScript: var m = -sin(TAU/n)/(1 - cos(TAU/n)); //slope between p[0] (1,0) and p[1] (sin(TAU/n), cos(TAU/n)
var n = 6; //number of sides
var spacing = 8; //multiplies radii by an arbitrary constant because this is on a small pixel scale
var numLayers = 1; //number of radial layers to draw

function getRadius(angle) //returns the radius of a polygon at an angle from the x axis
{
  let x1 = -1/(tan(angle % (TAU/n)) * (1/m) - 1) ;
  let y1 = m * (x1 - 1);
  let r1 = sqrt(x1*x1 + y1*y1);
  return r1;
}

function drawShapes() //For each layer, 
{
  for (let layer = 1; layer < numLayers + 1; layer++) //start at L1 because L0 can't be drawn
  {
    let shapes = n*layer; //There are 6 * current-layer # of hexagons per layer
    for(let shape = 0; shape < shapes; shape++)
    {
      let theta = shape * TAU/shapes; //360 degrees are evenly divided into the # of shapes
      let dist = layer * 2*getRadius(theta); //distance from L0 to L[i] is 2r*i
      
      let x = dist * cos(theta); //get the coordinates of the current shape
      let y = dist * sin(theta);
      
      polygon(spacing*x, spacing*y, 8, n) //draw a regular polygon (x, y, radius, # of sides)
    }
  }
} Where I Think the Issue is Because the hexagons in my current hexagonal tiling are especially distant around every 60 degrees, I suspect that the modulus of PI/3, which I utilize to evaluate X1 (the x coordinate of a point along a hexagon's perimeter) in my Desmos project is closely linked to my issue of non-uniform spacing. That's all the information I have to give-- I once again ask, why does the distance between the hexagons vary instead of being constantly zero? What would you do differently to achieve a seamless or uniformly spaced hexagonal tesselation?","['tessellations', 'geometry', 'computational-mathematics', 'trigonometry', 'symmetry']"
4672907,Can a group have $\aleph_1$ many subgroups?,"I know that any countable group has either countably many or continuum many subgroups ( source ), but I'm curious about uncountable groups. It seems like the proof for countable groups $G$ crucially uses the fact that $2^G$ is a polish space, which I think isn't true anymore when $G$ is uncountable. So then: Is it possible for an uncountable group to have exactly $\aleph_1$ -many subgroups? (Assuming $\lnot \mathsf{CH}$ , of course) More generally, for uncountable cardinals $\kappa$ and $\lambda$ , is it possible to have a group of size $\kappa$ with exactly $\lambda$ many subgroups? There are definitely some easy combinations of $\kappa$ and $\lambda$ that we can rule out, but this problem seems like it is probably hard. Thanks!","['group-theory', 'cardinals', 'descriptive-set-theory', 'set-theory']"
4672913,"Sufficiency part For the existence of $\bar p(\cdot)$ $\in$ $W(p)$ for which $C([0,1]$) is closed subspace in $L^{\bar p(\cdot)}([0;1])$.","Handwriting this would be impossible, so I apologize. These are the definitions and theorems  which we need for the proof of the theorem : BFS is defined as the Bannach Function Space. Let $W(p)$ denote set of all functions equimeasurable with $p(\cdot)$ . There is the theorem: My question is all about the sufficiency part of the theorem. There is the proof of it: I want to apply this theorem for 2 dimensions. This will be the statement applied to 2 dimensions: For the existence of $\bar p(\cdot)$ $\in$ $W(p)$ for which $C([0,1]^2$ ) is closed subspace in $L^{\bar p(\cdot)}([0;1]^2)$ it is necessary and sufficient that $\lim_{t\to 0+}$ $ sup \frac {p^{*}(t)}{ln(e/t)}$ $\gt$ $0$ . I've already shown that the theorem $2.2$ and $2.3$ work for two dimensions. I have  a trouble in reconstructing this proof for two dimensions. I've added an important definition About the BFS and  subspaces of BFS X. Any help would be appreciated.","['banach-spaces', 'measure-theory', 'analysis', 'real-analysis', 'functional-analysis']"
4672965,Distributions with 'Gaussian Tails',"In a paper I was reading, the following seemingly artificial assumption is used: suppose $f$ is some probability density function on $\mathbb{R}^d$ , and let $\phi$ denote the density of a $N(0,I_d)$ random variable. Then they assume either: for any $x \in \mathbb{R}^d$ , there exists a finite positive constant (possibly depending on $f$ such that $f(x) \le C_f \phi(x)$ . for any $x \in \mathbb{R}^d$ , there exists finite positive constants $c_1,c_2$ (not depending on $f$ ) such that $c_1 \phi(x) \le f(x) \le c_2 \phi(x)$ . I am trying to understand what kind of densities belong to either class. For example, the standard sub-gaussian assumption does not belong to the class of densities in 1. since any unbounded density does not satisfy 1, even though there are sub-gaussian distributions that are unbounded. Questions: what kind of distributions belong to either class? Do gaussian mixtures belong to either class? What does the dependence of the constants on $f$ change ? or how does the class of densities that satisfy either of the conditions change if the constants were/were not allowed to depend on $f$ ?","['statistics', 'probability-distributions', 'probability-theory', 'probability']"
4672966,Verifying formulas and process for surface area and volume of a spindle torus,"While working on this geometry problem I reasoned that the surface area of the spindle torus is the surface area of the apple (outer surface) plus the surface area of the lemon (inner surface) while the volume of the spindle torus is the volume of the apple minus the volume of the lemon, since the lemon would not fill with liquid even though it contains space, and because that volume would otherwise be double-counted.  Is this correct, or should the lemon's volume be either added or not subtracted for some reason? Second, while less important, I found the surface area and volume formulas, posted below, but always appreciate reaching a deeper understanding of mathematics when I learn how complicated formulas are derived step-by-step.  If anyone likes taking time to do that I would love to know how the integrals are evaluated analytically to derive (1) and (2), along with how the volume formula for the spindle torus (last line) is derived from the volumes of the apple and lemon (2).  If not, don't worry about this part.","['derivation-of-formulae', 'geometry', 'solid-of-revolution']"
4672968,What is the area of the shaded region if $AB=120$?,"What is the area of the blue region if $AB=120$ . I can only think in $\triangle OAB$ (O b being the low left point and the triangle is right) I can use Pythagoras which involves $120$ , radius $r$ of the arc and diameter $d$ of the circle but I'm stuck there. $$r^2=d^2+120^2$$ I know the area we are looking for is $$\frac{1}{4}\pi r^2 - \pi \left(\frac{d}{2}\right)^2$$","['pythagorean-triples', 'area', 'geometry']"
4672973,Independence among order statistics,"Given i.i.d uniform variables $X_1, ..., X_n \sim U[0,1]$ , is it true that the event $X_i \leq \min_{k \in [i-1]} X_k$ is independent of $X_j \leq \min_{k \in [j-1]} X_k$ for any $i > j$ ? The answer seems to be yes through symmetry: $\Pr(X_i \leq \min_{k \in [i-1]} X_k | X_j \leq \min_{k \in [j-1]} X_k) = \Pr(X_i \leq \min_{k \in [i-1]} X_k | X_l \leq \min_{k \in [j-1]} X_k)$ for any $l \in [j-1]$ , but I don't have a rigorous proof. Also, if the above is true, it seems then that this statement is true regardless of the distribution of $X_i$ ?","['order-statistics', 'probability-distributions', 'uniform-distribution', 'probability']"
4672977,normal vector stokes theorem calculation,"$$\int\int_S (\nabla \times F) \cdot n \, dS$$ Solving Stokes' theorem using Curl form, I'm having an issue with the calculation of the normal vector when using parameterization of the surface and a cross product vs. $f(x,y,z)=z-g(x,y)$ form. Curl is given as $\langle-2z, -2x, -2y\rangle$ and C as the intersection of $z=\sqrt{5-x^2 -y^2}$ and $z=1$ Parameterizing results in $r(x,y)=\langle x, y, 1\rangle$ . The cross product of the partial with respect to x and y gives a normal vector of $\langle 0,0,1 \rangle$ However if I want to use the form $z-g(x,y)$ with $z=\sqrt{5-x^2 -y^2} $ it is not evident to me how I get to the same normal vector after taking the gradient. I get $\left\langle \frac{x}{\sqrt{5-x^2 -y^2}}，\frac{y}{\sqrt{5-x^2 -y^2}}, 1 \right\rangle$ Can someone point me in the right direction?","['multivariable-calculus', 'stokes-theorem']"
4672987,An inequality considering linear ODE,"Let $p(t),q(t)$ , and $r(t)$ be continuous functions on an open interval $I$ and let $t_0\in I$ . Assume that there exists a positive constant $M$ such that $$|p(t)|+|q(t)|+|r(t)|<M$$ for all $t\in I$ . Let $f$ be a three times differentiable function which is a solution of a differential equation $$f'''(t) + p(t)f''(t) + q(t)f'(t) + r(t)f(t) = 0$$ on $I$ . Finally, define $$\psi(t) := |f(t)|^2 + |f'(t)|^2 + |f''(t)|^2$$ I want to show that (1) $|\psi'(t)|\leq 2(1+M)\psi(t)$ (2) $\psi(t_0)e^{-2(1+M)|t-t_0|} \leq \psi(t) \leq \psi(t_0)e^{2(1+M)|t-t_0|}$ If the inequality in (1) is shown, (2) can be proven using an integrating factor. But I cannot figure out how to prove (1). $$\psi'(t) = 2f(t)f'(t) + 2f'(t)f''(t) + 2f''(t)f'''(t)$$ The form of $\psi'(t)$ seems to require the Cauchy-Schwarz inequality to prove the inquality, but where to use the bounds of $p,q,r$ ? Another try was to substitute $-pf'' - qf' - rf$ in the place of $f'''$ , but I have no idea how to relate its upper bound with the function $\psi$ itself. Thanks in advance for any form of help, hint, or solution!","['inequality', 'functional-inequalities', 'ordinary-differential-equations', 'real-analysis']"
4673021,What is the relation between a triangle’s centroid and its pedal triangle?,The pedal triangle of a point inside a triangle is the triangle formed by connecting the three feet of the perpendiculars drawn from that point to each side of the triangle. What is the relation between the centroid of a triangle and its pedal triangle? Is it a special triangle center of the pedal triangle(one that is in the triangle center database)?,"['euclidean-geometry', 'triangle-centres', 'centroid', 'geometry']"
4673040,A little unknown test for convergence of series with positive terms,"While helping a highschooler studying for her Calculus class, she showed me a convergence test for series which is kind of a hybrid between the ratio and root test, and of which  I was not aware: Theorem: Suppose $a_n>0$ for all $n$ . If $\limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac1e$ , then the  series $\sum_na_n$ converges. If there is $N$ such that for all $n\geq N$ , $\Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e$ , then the series $\sum_na_n$ diverges. I put this test to the test with simple examples: $\sum_n\frac1n$ : $\Big(\frac{n+1}{n}\Big)^n=\big(1+\tfrac1n\big)^n\nearrow e$ and so, with $a_n=\frac1n$ , $\Big(\frac{a_{n+1}}{a_n}\Big)^n\geq\frac1e$ which means that $\sum_n\frac1n$ diverges, as it should. $\sum_n\frac1{n^p}$ : $\Big(\frac{n}{n+1}\Big)^{pn}=\frac{1}{\big(1+\frac1n\big)^{np}}\xrightarrow{n\rightarrow\infty}\frac{1}{e^p}$ . For $p>1$ we get convergence and for $p<1$ divergence, as it should $\sum_n\frac{(a)_n(b_n)}{(c)_nn!}$ , where $(z)_0:=1$ and $(z)_n=z\cdot\ldots\cdot(z+n-1)$ for $n\geq1$ , $z\in \mathbb{C}$ . For simplicity, assume that $a,b,c\in\mathbb{R}\setminus\mathbb{Z}_-$ . Let $u_n=\Big|\frac{(a)_n(b_n)}{(c)_nn!}\Big|$ . Then, for all $n$ large enough $$\Big(\frac{u_{n+1}}{u_n}\big)^n=\Big(\frac{(a+n)(b+n)}{(n+1)(n+c)}\Big)^n=
\Big(1+\frac{a-1}{n+1}\Big)^n\Big(1+\frac{b-c}{n+1}\Big)^n\xrightarrow{n\rightarrow\infty}e^{a+b-c-1}$$ The series $\sum_nu_n$ converges if $a+b-c<0$ and diverges if $a+b-c>0$ . This can also be obtained by Raabe's test for example. In fact, the  Theorem above seems to be at the same level as Raabe's test  in the de Morgan hierarchy . Question: Does anybody know of the provenance of the Theorem above (a reference)? Thanks! Edit: Just to present a  short proof of the theorem above: (1) Let $p>1$ be such that $\limsup_n\Big(\frac{a_{n+1}}{a_n}\Big)^n<\frac{1}{e^p}<\frac{1}{e}$ Then, there is $N$ such that for all $n\geq N$ $$a_{n+1}<e^{-p/n}a_n$$ Let $H_n:=\sum^n_{k=1}\frac1k$ the $n$ -th sum of the harmonic series. It follows that $$a_{n+1}<\exp(-pH_n)e^{H_N}A_N<c_Nn^{-p},\qquad n\geq N$$ for some constant $C_N>0$ . (2) The assumption here  implies that for all $n\geq N$ $$a_{n+1}\geq e^{-1/n}a_n$$ and so, $$a_{n+1}\geq e^{-\big(\tfrac{1}{n}+\ldots+\tfrac{1}{N}\big)}a_N=e^{-H_n}e^{H_N}a_N\geq \frac{C_N}{n}$$ for some constant $c_N>0$ . Hence $\sum_na_n$ diverges.","['calculus', 'sequences-and-series', 'reference-request', 'real-analysis']"
4673085,"Is this measure the Lebesgue measure on $[0,1]^2$?","I'm struggling with the following question from a measure theory past paper: Suppose $\lambda_d$ is the Lebesgue measure on $\mathbb{R}^d$ . Let $\mu$ be a measure on Borel subsets of $[0,1]^2$ such that $\mu \ll \lambda_2$ and $$\mu(B\times [0,1])=\mu([0,1]\times B) = \lambda_1(B)$$ for any $B$ a Borel subset of $[0,1]$ . Does it hold that $\mu(D)=\lambda_2(D)$ for any $D$ a Borel subset of $[0,1]^2$ ? $\huge \textbf{My Attempt}$ I think the claim does not hold since I can't find a good angle of attack to prove it does hold. I initially thought about trying to show the Radon-Nikodym derivative of $\mu$ with respect to $\lambda_2$ is the identity function, but I can't think of a way to prove this. Then I tried to show that the set $$\mathscr{A}:=\{D\in \mathscr{B}([0,1]^2):\mu(D)=\lambda_2(D)\}$$ contains the product sets and is a $\sigma$ -algebra but again I am struggling to think of a way to show either of these. The only other way I can think of would be by defining some new finite signed measure as $$\nu_0(A)=\mu(A)-\lambda_2(A)$$ and showing it is always 0, but I don't think this approach will work. For a counter example, I don't really know where to start. The question before this concerned creating a closed set, $\tilde{C}\subset [0,1]$ , of positive Lebesgue measure which contains no non-empty open sets using a Cantor type construction so I'm wondering if I can construct a $\mu$ which assigns $\mu(\tilde{C}\times \tilde{C})=0\neq \lambda_2(\tilde{C}\times \tilde{C})$ as $\lambda_2(\tilde{C}\times \tilde{C})=(\lambda_1(\tilde{C}))^2>0$ . However, I'm not sure how to take advantage of the fact $\tilde{C}$ is closed and contains no non-empty open sets. Any hints/nudges would be appreciated!",['measure-theory']
4673094,Solve the following differential equation : $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}.$,"Solve the following differential equation : $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}.$ My solution goes like this: Given, $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$ . We write this, as $(D^3+1)y=3+e^{-x}+5e^{2x}.$ We first, calculate the value of the complementary function, i.e general solution of the differential equation, $(D^3+1)y=0.$ The roots of $f(x)=x^3+1$ are : $-1,a_1=\frac12+\frac{\sqrt 3i}{2},a_2=\frac12-\frac{\sqrt 3i}{2}.$ Thus, the complementary function $CF=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x},$ where $c_1,c_2,c_3$ are arbitary constants. Now, we evaluate the particular integral $y$ of the original equation $$\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$$ i.e \begin{align}
&y=\frac{1}{D^3+1}(3+e^{-x}+5e^{2x}) \\
&\implies y=\frac{1}{D^3+1}(3)+\frac{1}{D^3+1}e^{-x}+5\frac{1}{D^3+1}(e^{2x}) \\
&\implies y = 3+\frac{1}{D+1}(\frac{1}{D^2-D+1}e^{-x})+5\frac{e^{2x}}{9} \\
&\implies y = 3+\frac{1}{D+1}\frac{e^{-x}}{3}+5\frac{e^{2x}}{9} \\
&\implies y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.
\end{align} Thus, the particular integral is, $$y= 3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.$$ Now, the complete solution of the given equation $\frac{d^3y}{dx^3}+y=3+e^{-x}+5e^{2x}$ is : $$y_1=CF+y=c_1e^{-x}+c_2e^{a_1x}+c_3e^{a_2x}+3+\frac{xe^{-x}}{3}+5\frac{e^{2x}}{9}.$$ Is the above solution correct? If not, where is it going wrong ? Is the method, valid?","['solution-verification', 'ordinary-differential-equations']"
4673106,Integral $\int_{0}^{1}\int_{0}^{1}\frac{(xy)^k}{1+(\frac{y}{x})^p}dxdy$,"Determine the Integral: $$I=\int_{0}^{1}\int_{0}^{1}\frac{(xy)^k}{1+(\frac{y}{x})^p} \mathrm{d} x \mathrm{d} y$$ The answer seems to be $$\frac{1}{2}\frac{1}{(k+1)^2}$$ that has no dependence on $p$ , although I am not sure as I donot have the answer and this was verified using desmos for smaller values. The integral is simplified to $$I=\int_{0}^{1}y^k(\int_{0}^{1}\frac{(x)^k}{1+(\frac{y}{x})^p} \mathrm{d} x) \mathrm{d} y$$ $$I=\int_{0}^{1}y^k(\int_{0}^{1}\ x^k\left(1+(\frac{y}{x})^p  \right)^{-1} \mathrm{d} x) \mathrm{d} y$$ Now using $(1+x)^{-1}=\sum_{r=0}^{\infty }(-1)^rx^r$ $$(1+(\frac{y}{x})^p)^{-1} = \sum_{r=0}^{\infty }(-1)^r(\frac{y}{x})^{pr}$$ $$=\sum_{r=0}^{\infty }(-1)^ry^{pr}x^{-pr}$$ The inner integral can then be evaluated as: $$\int_{0}^{1}\ x^k\left(1+(\frac{y}{x})^p  \right)^{-1} \mathrm{d} x = \sum_{r=0}^{\infty }(-1)^ry^{pr}\int_{0}^{1}x^{k-pr} \mathrm{d} x$$ $$\int_{0}^{1}x^{k-pr} \mathrm{d} x = \frac{1}{k-pr+1}, r\neq \frac{k+1}{p}
$$ I am not sure what to do when $r=\frac{k+1}{p}$ as we will be integrating $\frac{1}{x}$ from $0$ to $1$ which comes out to be undefined.
Leaving that term out: $$\int_{0}^{1}\ x^k\left(1+(\frac{y}{x})^p  \right)^{-1} \mathrm{d} x =\sum_{r=0}^{\infty }\frac{(-1)^ry^{pr}}{k-pr+1}$$ Now $$\int_{0}^{1}\int_{0}^{1}\frac{(xy)^k}{1+(\frac{y}{x})^p} \mathrm{d} x \mathrm{d} y=\sum_{r=0}^{\infty }\frac{(-1)^r}{k-pr+1}\int_{0}^{1}y^{k+pr} \mathrm dy$$ The integral $$\int_{0}^{1}y^{k+pr} \mathrm{d} y = \frac{1}{k+pr+1}, r\neq -(\frac{k+1}{p})$$ Again the same restrictions on $r$ but $r$ cannot be negative so this won't happen. Finally the original integral becomes: $$I=\sum_{r=0}^{\infty }\frac{(-1)^r}{(k-pr+1)(k+pr+1)}$$ $$I=\frac{1}{2(k+1)}\sum_{r=0}^{\infty } (\frac{(-1)^{r}}{k-pr+1}+\frac{(-1)^{r}}{k+pr+1})$$ $$I=\frac{1}{2(k+1)}(\sum_{r=0}^{\infty } (\frac{(-1)^{r}}{k-pr+1})+\sum_{r=0}^{\infty }(\frac{(-1)^{r}}{k+pr+1}))$$ I don't know how to evaluate those sums, individually putting them into wolfram gives an answer with Hurwitz Lerch Transcendent. Although together they seem to give: $$\frac{1}{k+1} + \frac{\pi}{2p}(\tan(\frac{\pi k}{2p}+\frac{\pi}{2p})+\cot(\frac{\pi k}{2p}+\frac{\pi}{2p}))$$ Hence, $$\frac{1}{2}\frac{1}{(k+1)^2} + \frac{\pi}{4p(k+1)}(\tan(\frac{\pi k}{2p}+\frac{\pi}{2p})+\cot(\frac{\pi k}{2p}+\frac{\pi}{2p}))$$ Which is close to the answer guessed at the beginning (except the trigonometric terms ofcourse). Though it seems I have made a mistake somewhere as the trig terms are affecting the answer making it wrong.
Well this was my way of attempt, I am guessing there must be a better way than using the binomial to expand.","['integration', 'multivariable-calculus', 'calculus', 'definite-integrals']"
4673126,Is the convolution of the Heaviside distribution with itself well defined?,"During a lecture, we were told that for two distributions $S,T \in \mathcal{D'}(\mathbb{R})$ the convolution $S* T$ is defined if either $S$ or $T$ had a compact support, or, if both of them were tempered distributions. With that said, the Heaviside distribution does not appear to have a compact support, nor is it a tempered distribution. Meaning that $H * H$ , where $H$ denotes the Heaviside functional, is not well defined. Yet, our professor asked us to prove that the second derivative of $H * H$ is equal to Dirac's delta. $$
(H * H)'' = \delta_0
$$ How is this possible?","['convolution', 'dirac-delta', 'functional-analysis', 'distribution-theory']"
4673146,A hint in Brezis' exercise 4.16.1,"Let $(\Omega, \mathcal F, \mu)$ be a $\sigma$ -finite measure space. I'm trying to prove below hint in Brezis' Functional Analysis , i.e., Let $p \in [1, \infty)$ and $p'$ its Hölder conjugate. Let $f_n,f,g:\Omega \to \mathbb R$ be measurable functions such that $f_n,f \in L^p(\Omega)$ for all $n$ . Assume $f_n \to f$ in the weak topology $\sigma(L^p, L^{p'})$ and $f_n \to g$ $\mu$ -a.e. Then $f=g$ $\mu$ -a.e. Could you confirm if my attempt is fine? Is there a way to bypass the use of exercise 3.4.1? Proof First, we need the following result, i.e., Brezis' exercise 3.4.1 Let $E$ be a Banach space and $(x_n) \subset E$ such that $x_n \to x$ in $\sigma(E, E^*)$ . Then there exists a sequence $(y_n) \subset E$ such that $y_n \in \operatorname{conv} (\{x_n, x_{n+1}, \ldots\})$ for all $n$ and $y_n \rightarrow x$ in norm. Let $(g_n)$ be a sequence given by above result for the pair $((f_n)_n, f)$ . Then $g_n \in \operatorname{conv} (\{f_n, f_{n+1}, \ldots\})$ and $g_n \to f$ in $L^p$ . There is a sub-sequence $\varphi$ of $\mathbb N$ such that $g_{\varphi (n)} \to f$ $\mu$ -a.e. Fix $\omega \in \Omega$ such that $f_n (\omega) \to g (\omega)$ and $g_{\varphi (n)} (\omega) \to f (\omega)$ . Fix $\varepsilon >0$ . There is $N$ such that for every $n >N$ $$
\begin{align}
|f_n(\omega) - g (\omega)| &< \varepsilon.
\end{align}
$$ Fix $n>N$ . There is $\psi (n) > \varphi (n)$ and a probability vector $(t_i)_{i=\varphi (n)}^{\psi (n)}$ such that $$
g_{\varphi (n)} = \sum_{i=\varphi (n)}^{\psi (n)} t_i f_i
\quad \mu \text{-a.e.}
$$ Then $$
|g_{\varphi (n)} (\omega) - g (\omega)| \le \sum_{i=\varphi (n)}^{\psi (n)} t_i |f_i (\omega) - g(\omega)| < \varepsilon.
$$ Hence $g_{\varphi (n)} (\omega) \to g (\omega)$ . It follows that $g(\omega) = f(\omega)$ . This completes the proof.","['banach-spaces', 'lebesgue-integral', 'real-analysis', 'lp-spaces', 'functional-analysis']"
4673191,Maximum interval for differential equation,"The exercise says as follows: Find the maximum interval of solution of $$y'=\frac{y^3}{1+y^2}e^x+x^2 \cos y, y(0)=y_0$$ I don't even know how to approach this problem. I'm aware of the maximize $a'$ method but I'm not able to reach anything. Any help is appreciated.","['calculus', 'ordinary-differential-equations']"
4673217,Integral of $\sin(x^n)$ in terms of sinus and Gamma function. [duplicate],"This question already has answers here : Evaluation of a Fresnel type integral. (2 answers) Closed last year . Prove that (for $n>1$ ) \begin{equation}
    \int_{0}^{+\infty} \sin(x^n)dx=\sin\left(\frac{\pi}{2n}\right)\Gamma\left(\frac{n+1}{n}\right).
\end{equation} I tried to prove this using a similar argument for Fresnel's integral, but I don't see what I'm doing wrong.
Define $\gamma_1:[0,R]\rightarrow\mathbb{C}, \gamma_1(t)=t$ , $\gamma_2:[0,\pi/(2n)]\rightarrow\mathbb{C}, \gamma_2(t)=Re^{it}$ and $\gamma_3:[R, 0]\rightarrow\mathbb{C}, \gamma_3(t)=te^{i\pi/(2n)}.$ For Local Theorem of Cauchy, if $f(z)=e^{iz^n}$ , then $$\int_{\gamma_1+\gamma_2+\gamma_3} f=0.$$ Now, studying each integral alone, we have for $\gamma_1$ , $$\int_{\gamma_1}f=\int_{0}^{R}e^{it^n}dt\rightarrow\int_{0}^{+\infty}\cos(t^n)dt+i\int_{0}^{+\infty}\sin(t^n)dt.$$ For $\gamma_2$ , $$\int_{\gamma_2}f=\int_{0}^{\pi/(2n)}e^{i(Re^{it})^n} Rie^{it}dt=Ri\int_{0}^{\pi/(2n)}e^{iR^n(cos(nt)+isin(nt))}e^{it}dt=Ri\int_{0}^{\pi/(2n)}e^{-tR^ncos(nt)}e^{-tiR^nsin(nt))}dt,$$ second exponencial, inside integral, have modulus 1, so if R goes to infinity, integral over $\gamma_2$ is zero because $Re^{-tR^ncos(nt)}\rightarrow0$ . For $\gamma_3$ , $$\int_{\gamma_3}f=\int_{R}^{0}e^{i(te^{i\pi/(2n)})^n}e^{i\pi/(2n)}dt=-e^{i\pi/(2n)}\int_{0}^{R}e^{it^ne^{i\pi/2}}dt=-e^{i\pi/(2n)}\int_{0}^{R}e^{-t^n}dt.$$ Then taking imaginary part and R going to infinity, $$\int_{0}^{+\infty} \sin(t^n)dt=\sin\left(\frac{\pi}{2n}\right)\int_{0}^{+\infty} e^{-t^n}dt.$$ Where is my mistake? Any help? Thanks.","['complex-analysis', 'improper-integrals']"
4673247,An inequality for the derivative of Japanese bracket,"I'm reading a book that uses some properties of Japanese bracket. The author claims that, for any real number $m$ and multi-index $\beta$ , it holds $$|\partial_x^\beta\langle x\rangle^m|\leq C^{|\beta|}\beta!\langle x\rangle^{m-|\beta|},$$ for some constant $C>0$ which doesn't depend on $\beta$ . I did some computations with some fix $m$ , for example $m=1$ or $m=2$ , etc., and assuming that $x\in\Bbb{R}$ (the property should be true for $\Bbb{R}^n$ ). But, by taking large $\beta\in\Bbb{N}$ , the derivatives starts to get complicated (a lot of computations). Does anybody know about a simple way to prove this inequality? Of course, we define $\langle x\rangle:=(1+|x|^2)^{1/2}$ ,","['derivatives', 'real-analysis']"
4673271,Hoffman - Definition of Blaschke product,"I am self-studying Banach Spaces of Analytic Functions by Hoffman. I have a question regarding the following theorem: According to the author, the set $K$ formed as in the theorem is compact. I do not see how. If we consider the $\alpha_n = 1/n$ for each $n \in \mathbb N$ then the Blaschke product makes sense because the $\prod_{n=1}^{\infty} 1/n = 0$ . However, the set $K$ formed out is $K=\mathbb N$ which is not compact. Is the author mistaken or am I doing something wrong?","['complex-analysis', 'hardy-spaces']"
4673291,Determining if a process is a martingale,"Let $Z(t)$ be a standard brownian motion, given $W(t) = t^{2} Z(t) -2\int_0^t sZ(s) ds$ , determine if this is a martingale process! We know that a process is martingale if $E[W(t+1)|F(t)] = W(t)$ , with $F(t)$ as a filtration process. I think we should start by writing down $E[W(t+1)|F(t)]$ first, which is $E[(t+1)^{2} Z(t+1) -2\int_0^{t+1} sZ(s) ds|F(t)]$ . This can be split into two terms: First term: $E[(t+1)^{2} Z(t+1)|F(t)]$ which I believe would be $(t+1)^2Z(t)$ , since a standard brownian motion is a martingale. Second term: $-2E[\int_0^{t+1} sZ(s) ds|F(t)]$ for this part, i think we need to use integration by parts, but I ended up with $\int Z(s) ds$ and I'm not sure how to proceed with that result. Am I doing the right steps? Thanks in advance","['random-walk', 'stochastic-processes', 'martingales', 'brownian-motion', 'probability']"
4673297,Show that $X$ is finite and $E(|X|)<\infty$,"Let $\left(X_{n}\right)_{n \geq 0}$ is a martingale with $X_{0}=0$ . Assume $$\sum_{n=1}^{\infty} E\left(\left(X_{n}-X_{n-1}\right)^{2}\right)<\infty .$$ Show $X=\lim _{n \rightarrow \infty} X_{n}$ (and it is finite) and $E(|X|)<\infty$ I have shown that the martingale is uniformly integrable and thus $X=\lim _{n \rightarrow \infty} X_{n}$ with the convergence happening in $L^1$ (even though I also have shown it is bounded in $L^2$ ). Now, I do not know how to argue for that $X$ is finite a.s and that $E|X|<\infty$ . For the first one: I am thinking since the convergence happens in $L^1$ , then $X$ must be finite? For the second one: I have a result saying that if $L$ is a vector space and expectation is a linear map
on it then $|X|\in L \Leftrightarrow X \in L$ but I am not sure if that helps me in any way. I think a finite random variable implies a finite expectation, but the reverse implication is not necessarily true but if I could show that the absolut value of it is finite then part 2 is solved.","['probability-limit-theorems', 'uniform-integrability', 'lp-spaces', 'probability-theory', 'probability']"
4673329,Hypergeometric distribution question steps,"A firm that refurbishes personal computers (PCs) recently received 24 faulty computers
of a similar make and model. 21 PCs have been fixed but an intern mistakenly packaged all the PCs to storage without labeling. The repair engineer will need to test the PCs one after another until he identifies the 3 PCs that are not yet fixed. Find the probability that he tested 15 computers to complete the task. I think hypergeometric distribution works over here. Steps I have done : The number of ways to choose 3 faulty computers out of the 24 is given by the binomial coefficient: ${24 \choose 3} = \frac{24!}{3!(24-3)!} = 2024$ The total number of ways to choose 15 computers from the 24 is: ${24 \choose 15} = \frac{24!}{15!(24-15)!} = 2,704,156$ The probability that the repair engineer tested exactly 15 computers to complete the task is the probability that he tested exactly 12 good computers and 3 faulty computers in a specific order, followed by 12 more good computers in any order. The probability of this specific sequence of events is: Don't know further how to proceed ?","['combinatorics', 'discrete-mathematics']"
4673334,Why do equilateral triangles relate to cubics,"I found this question talking about the relation between an equilateral triangle and cubics with three distinct real roots. Here's an image from the original post with an example: What this post says about cubic curves is: You can always construct an eq. triangle such that: the triangle's inscribed circle's center lines up with the point of inflection on the curve the function's min. and max. points line up with the inscribed circle's left- and rightmost points (not necessarily tangent to the sides) and the triangle's vertices line up with the roots. (as seen on the image) You can use these facts to solve a cubic of this kind using with the triple angle cosine identity, but my question is not about that. My question would be about how you can prove this is construction is always possible. (1.) I started using the fact that cubic curves have point symmetry on the inflection point, thus the min. and max. points are equally distanced from the poi. on the x-axis.
This defines a circle. (2.) Then, if we draw another circle with the same center but twice the radius, we can construct a specific eq. triangle in between the two circles such that the two are the inscribed and circumscribed circles of that triangle. (3. ??) We have infinitely many ways of rotating this, but for some reason, one of them will line up such that the vertices are in line with the roots. I got stuck here. Thanks for any help and please explain in a very simple manner.","['cubics', 'roots', 'geometry', 'triangles', 'polynomials']"
4673376,"I'm searching for a monotonically increasing function, defined for all reals, that is concave down and a ""gentle curve"" (no aymptotes).","I'm searching for a monotonically increasing function, defined for all reals, that is concave down and a ""gentle curve"" (no aymptotes). The link below provides an image: example of gentle concave down curve This post has some examples of near misses: Is there a bijective, monotonically increasing, strictly concave function from the reals, to the reals? For instance: y = -e^(-x) However, this is not by any means a gentle curve, outside of the domain (-3,3). The rest of the graph looks essentially like a right angle.",['functions']
4673421,Lyapunov analysis of marginally stable linear systems,"Before I start, I want to emphasize I'm dealing with marginally stable linear systems, so many theorems about stable systems simply do not apply. Let $\dot{x} = Ax$ be a marginally stable. That is, for any $x(0) \in \mathbb{R}^n$ , there exists $M > 0$ such that $\|x(t)\|_2 \leq M$ for all $t \geq 0$ . This is equivalent to saying that all eigenvalues of $A$ have non-positive real part, and the eigenvalues with zero real part have $1 \times 1$ Jordan blocks. (I am particularly dealing with a matrix $A$ with real eigenvalues, and all but one eigenvalues are negative, and the the remaining (simple) eigenvalue is $0$ .) This manuscript states $A$ is marginally stable if and only if there exists $Q \geq 0, P >0$ such that $$A^TP + PA = -Q.$$ My question is, is there a procedure to find such $Q, P$ ? Edit: The linked manuscript has incorrect statements. @user1551 cleared everything up in their answer.","['ordinary-differential-equations', 'lyapunov-functions', 'matrices', 'stability-in-odes', 'linear-algebra']"
4673465,Integral $\int_0^\infty\frac{\cos(\pi x^2)}{\cosh (\pi x)(\cosh (4\pi)-\cos(4\pi x))}dx$,"I encountered the integral $$
\int_0^\infty\frac{\cos(\pi x^2)}{\cosh (\pi x)(\cosh (4\pi)-\cos(4\pi x))}dx=\frac{1}{\sinh (4\pi)}\left(\frac{\coth(\pi)}{\sqrt{2}}-\frac{1+\sqrt{2}}{16\,\pi\sqrt{\pi }}\Gamma^2\left(\frac{1}{4}\right)\right)
$$ The value is checked in high precision via CAS. Some posts on the site considered integrals of similar forms, e.g. Does $\int_{0}^{\infty}{\sin{(\pi{x^2})}\over \sinh{(\pi{x}})\tanh(x\pi)}\mathrm{d}x$ have a simple closed from? tough integral involving $\sin(x^2)$ and $\sinh^2 (x)$ integral $\int_{0}^{\infty}\frac{\cos(\pi x^{2})}{1+2\cosh(\frac{2\pi}{\sqrt{3}}x)}dx=\frac{\sqrt{2}-\sqrt{6}+2}{8}$ I thought of integrating on contours similar to those above, so I tried functions like $$
\frac{e^{i \pi  z^2}}{\cosh (\pi  z) (1-e^{i\pi z})}
$$ and integrated them on rectangular contours. However, all attempts failed. I was more convinced that contour integration cannot directly find the answer because of the $\Gamma^2\left(\dfrac{1}{4}\right)$ term. That made me think that the integral somehow relates to elliptic functions. After all my effort, I could not work out the result on my own. How can we evaluate the integral? Maybe some really clever contour integration, or applying special functions formulas? Any help would be appreciated.","['integration', 'calculus', 'contour-integration', 'definite-integrals']"
4673488,How to prove that $\log_5(6)>\log_6(7)$?,I know that $\log_5(6)>\log_6(7)$ but I wanted to prove it without calculating the values. After generalizing it it turned this way (for $x>1$ ): $$\frac{\ln(x)}{\ln(x-1)}>\frac{\ln(x+1)}{\ln(x)}$$ $$\ln(x)^2>\ln(x+1)\ln(x-1)$$ and based on the fact that $\dfrac{\mathrm{d}}{\mathrm{d}x}\ln(x)=\dfrac{1}{x}$ I conclude that since $\dfrac{1}{x-1}$ is bigger than $\dfrac{1}{x+1}$ then it must have changed more so for example in the first part of question $\dfrac{1}{5}>\dfrac{1}{7}$ so $\ln(5)$ to $\ln(6)$ rate of change is bigger than $\ln(6)$ to $\ln(7)$ and it convinced me. But I would like a more formal proof if there is and preferably one that doesn't use derivatives.,"['inequality', 'logarithms', 'number-comparison', 'algebra-precalculus', 'derivatives']"
4673490,Show that $\sum_{k=0}^m \frac{(k(m-k))^p}{k! (m-k)!} (-1)^k$ is an integer,"A user posted this question the other day, but it was voluntary removed and it was of low quality, and I believe it's a useful question for those interested in combinatorics like myself Show $$\sum_{k=0}^m \frac{(k(m-k))^p}{k! (m-k)!} (-1)^k$$ is an integer where $m$ and $p$ are non-negative integers. This series is inherently interesting, because while it is a sum of fractional terms, they appear to always magically cancel add up to an integer. For example, when $m=8$ and $p=4$ , the series is $$
-\frac{2401}{5040}+\frac{20736}{1440}-\frac{50625}{720}+\frac{65536}{576}-\frac{50625}{720}+\frac{20736}{1440}-\frac{2401}{5040}=1
$$ This looks like an extraordinary coincidence. Furthermore, I wrote a Python script to check the hypothesis for small values of $m$ and $p$ , and the pattern persisted; the result is indeed always an integer for all $m$ and $p$ in the range $\{1,2,\dots,10\}$ . When I first saw this question, Stirling numbers of the second kind came to mind. Note the similarity to the Stirling numbers of the second kind, which are integers: $${p\brace m} = \frac{1}{m!}\sum_{k=0}^m (-1)^k \binom{m}{k} (m-k)^p$$ Similarly, it is not obvious that the right hand side of the above expression always returns an integer, but it can be proven via a combinatorial argument using the principle of inclusion exclusion. Since the series in this question is a slight modification of the series defining ${p\brace m}$ , the only difference being an additional factor of $k^p$ in each term, one might expect that there is also an underlying combinatorial interpretation for this series.",['combinatorics']
4673493,Remarkable property of equilateral triangles,"In a discussion with a math friend and colleague, I was asked to find the angle $APB$ in an equilateral triangle if $AP=3$ , $BP=4$ and $CP=5$ . In solving this question I rediscovered that the same angle is always found when $a$ , $b$ and $c$ satisfy the Pythagorean theorem. But I rediscovered another, very surprising theorem. In an equilateral triangle, the following is true: $$L^2=\frac{1}{2}(a^2+b^2+c^2)+2{\sqrt{3}}Area_{\triangle _{(PA,PB,PC)}}$$ I was able to prove this by using two times the cosine rule and expressing $Area_{\triangle _{(PA,PB,PC)}}$ by applying Héron's rule. (And using this theorem, the property mentioned first is easy to prove.) My question is: is this a known theorem? Does it have a name?","['triangles', 'geometry']"
4673572,Solution to $\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx$,"This was a question that I thought about as I was playing around with the cardinal sine function: Find $$\lim_{n\rightarrow\infty}\int_{-\infty}^\infty\frac{\sin^{\circ n}x}{x}dx$$ with ${}^\circ$ denoting functional composition. I don't really know how to approach this question, but intuition tells me that this approaches $\pi$ . Maybe it doesn't even converge. I have no idea. WA says that for $n=3$ we get something around $-5$ .","['integration', 'definite-integrals', 'calculus', 'trigonometric-integrals', 'limits']"
4673602,Showing that it is possible to Differentiate under the Integral Sign - Proof Trouble,"Suppose $f: [a,b] \times [c,d] \rightarrow \mathbb{R}$ is continuous and $\frac{\partial f}{\partial x}$ is continuous. Define $F(x) = \int_c^d f(x,y)dy$ . The purpose of this question is to arrive at the conclusion of being able to differentiate under the integral sign. The first part of the question asked me to show that $F$ was continuous. I've done that, but I'm stuck on this second part which is stated as so: Prove that $F$ is differentiable and that $F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y) dy$ . Hint: Let $\phi (t) = \int_c^d \frac{\partial f}{\partial x}(t,y) dy$ and let $\Phi (x) = \int_a^x \phi(t) dt$ . Show that $\phi$ is continuous and that $F(x) = \Phi (x) + \text{constant}$ . Proving that $F$ is differentiable: Going back to the definition, and since this is a function in one dimension, a function $f$ will be differentiable at a point $a$ if there is a unique number $m$ such that $$  \lim_{h \to 0} \frac{f(a+h) - f(a) - mh}{h} = 0.$$ So specifically to this question I can write: $$\lim_{h \to 0} \frac{F(a+h) - F(a) - mh}{h} = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) dy - \int_c^df(a,y)dy - mh}{h} \\ = \lim_{h \to 0}\frac{\int_c^d f(a+h,y) - f(a,y) dy}{h}  - \lim_{h \to 0} \frac{mh}{h} = \lim_{h \to 0}\int_c^d \frac{f(a+h,y) - f(a,y)}{h}dy - m.$$ I want to bring the limit into the integral, but if I recall that is something that is not straightforward and needs to be proven, which we haven't done yet. But assuming that it is permitted I would be left with: $$\int_c^d \frac{\partial f}{\partial x}(a,y)dy - m = 0$$ But now I'm left in a problem because I'm trying to show $F$ is differentiable, but I'm claiming already that what I arrived at is the needed value. This comes up because I have to show that $F'(x) = \int_c^d \frac{\partial f}{\partial x}(x,y)dy$ . I see where the hint is going to lead me: Showing $\phi$ is continuous and $F(x) =  \Phi(x) + \text{const}$ , I would be able to use $\phi (t)$ being continuous to get $F'(x)$ . To show $\phi (t)$ is continuous the argument would be similar to what was done for $F$ : $$|\phi(t_1) - \phi(t_2)| = \bigg |\int_c^d \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y) dy\bigg| \leq \int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy \tag{$**$}$$ By uniform continuity for all $\frac{\epsilon}{d-c}$ there is a $\delta > 0$ such that for all $t_1,t_2 \in [a,b]$ , when $|t_1 - t_2| < \delta$ then $\bigg| \frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| < \frac{\epsilon}{d-c}$ . Therefore by $(**)$ we will have: $$\int_c^d \bigg|\frac{\partial f}{\partial x}(t_1,y) - \frac{\partial f}{\partial x}(t_2,y)\bigg| dy < \int_c^d \frac{\epsilon}{d-c} dy = \epsilon$$ Therefore $\phi(t)$ is continuous. I know from discussion and reading another solution, there is suppose to be some application of a chain rule, but I don't understand how it comes about. I know $F(x)$ is suppose to be some composition of functions but I don't see clearly what that composition is. I also don't understand how/why we use a parameterization in $t$ .","['multivariable-calculus', 'derivatives', 'real-analysis']"
4673634,Verify Complex Solutions of Riemann Zeta Function?,"I've recently been trying to visualize the Riemann Zeta complex function by graphing it on a 3D plot. I wrote some code to compute the value of the following integral representation of the Riemann Zeta function, which should be defined in Re (Input) $\neq$ 1: $$\zeta(s) = \frac{2^{s-1}}{s-1} -2^{s} \int_{0}^{\infty}\frac{\sin(s\arctan(x))}{(1+x^{2})^{\frac{s}{2}}(e^{\pi x}+1)}dx$$ Where s is the complex Input. Through the use of GNUPlot's code libraries, I graphed the following: Re (Input) - X-axis Im (Input) - Y-axis Re (Output) - Z-axis Im (Output) - Color As far as I understand, this is one of the standard ways to visualize & graph complex functions. With this setup I'm able to generate some plots: Plots above depicts the function on -5 < Re (Input) < 5 and -5 < Im (Input) < 5. (forgive the messed up colorbar label on the right, it appears that C++'s GNUPlot library is quite old...) Plots above depicts the function on -30 < Re (Input) < 30 and -30 < Im (Input) < 30. So far, so good. However, I want to be able to verify whether my code produces the correct answers for this representation of the RZ function. I'm able to compare some of my answers to some of the known answers out there, such as any answer with Re (Input) > 1, real negative integers and discovered nontrivial zeroes , and those all appear to check out. However, apart from those, I haven't really been able to find a good way of reliably verifying the rest of my answers here. Most popular online function graphing calculators either fail to compute that integral function (such as Desmos ), are unable to accept complex inputs (such as GeoGebra ), or only have static plots of it (such as Wolfram ). Since I'm not a mathematician, and since I'm not aware of all the possible resources out there or methods for verifying solutions for complex functions, I wanted to ask that here. What would be some good ways of verifying solutions to complex functions, such as the Riemann Zeta function? Is there a tool that is used when computing complex functions such as these, for verifying solutions? Thanks for reading my post, any guidance is appreciated.","['complex-analysis', 'riemann-zeta']"
4673705,A series in cosine : $\sum_{j=1}^{n} (-1)^{j-1}\cos\left(\frac{j\pi}{2n+1}\right)$,"$\textbf{Question :}$ Prove that $$\sum_{j=1}^{n} (-1)^{j-1}\cos\left(\frac{j\pi}{2n+1}\right)=\frac{1}{2}$$ $\textbf{My Attempt :}$ Let $n$ be odd (will do even case later) then the series can be written as $$S = \sum_{j=1}^{(n+1)/2}\cos((2j-1)x) - \sum_{j=1}^{(n-1)/2} \cos(2jx)$$ where $x = \frac{\pi}{2n+1}$ Now as both the cosine series are just arithmetic progressions in their angles with the common difference of $2x$ , we can simplify $S$ as follows : $$S = \frac{1}{\sin x}(\sin((n+3)x)\cos((n+2)x)-\sin((n+1)x)\cos(nx))$$ As $2\sin a\cos b = \sin(a+b)+\sin(a-b)$ $$S = \frac{1}{2\sin x}(\sin(2n+5)x)-\sin((2n+1)x)$$ Again as $\sin c -\sin d = 2\sin((c+d)/2)\cos((c-d)/2)$ $$S = \frac{1}{2 \sin x} (2\sin 2x \cos((2n+3)x))$$ $$S = \cos x \cos ((2n+3)x)$$ Now this is where I am getting stuck, I can't get it equal to any constant...","['trigonometry', 'trigonometric-series', 'calculus', 'algebra-precalculus']"
4673751,"Closed form for the hypergeometic function $\,_{4}F_{3}\left(1,-k,k+\frac{3}{2},\frac{1}{2};\frac{1}{2}-k,k+2,\frac{3}{2};1\right)$","Let $K(x)$ be the complete elliptic integral of the first kind with the following convention $$K(x):=\int_{0}^{\pi/2}\frac{\mathrm{d}\theta}{\sqrt{1-x\sin^{2}(\theta)}}.$$ For a research work, I would like to compute the Fourier-Legendre coefficients of $K(x)^{2}$ , that is, integrals of the type $$\int_{0}^{1}K(x)^{2}P_{n}\left(2x-1\right)\mathrm{d}x$$ where $P_{n}(x)$ are the Legendre polynomials. For now, I'm only able to prove the following representation for odd $n$ $$\int_{0}^{1}K(x)^{2}P_{2k+1}(2x-1)\mathrm{d}x=\frac{1}{(2k+1)(k+1)}\,_{4}F_{3}\left(\left.{1,-k,k+\frac{3}{2},\frac{1}{2}\atop\frac{1}{2}-k,k+2,\frac{3}{2}}\right|1\right).$$ I tried to search some identities and to apply some classical results to this $_{4}F_{3}$ but I'm not able to find a closed form of such function (for closed form I intend some representation in terms of ratio of Gamma functions or in terms to functions strictly related to Gamma function). So my question is: does anyone know if this hypergeometric function admits a closed form? Bonus question : for even $n$ , for now, I have no idea to attack this problem, so any suggestions are welcome.","['integration', 'special-functions', 'real-analysis', 'closed-form', 'hypergeometric-function']"
4673759,Definition of Lebesgue Space,"I came across two definitions of Lebesgue space (also known as standard probability space ) and I would like to know if they are equivalent. It seems to me so, but I think the second one can allow atoms. ( Sinai ): $(M,\mathcal{A},\mu)$ is called Lebesgue space if $M$ is isomorphic mod 0
with the segment $[0, 1]$ carrying the standard Lebesgue measure. From Sinai, Ya. G. (1994), Topics in ergodic theory. (Aaronson) : A Lebesgue space is a complete measure space wich is isomorphic to the completion of a standard measure space. From Aaronson, J. (1997), An introduction to Infinite Ergodic Theory. Furthermore, Sinai requires that $(M,\mathcal{A},\mu)$ be a measure space without points of positive measure. $(X,\mathcal{B},\nu)$ is standard measure space if $(X,\mathcal{B})$ is a polish space equipped whith its collection of Borel sets. It seems to me that when Aaronson requires the space to be complete and isomorphic to a Polish space, he ends up avoiding atoms, but I'm not sure. Are these definitions equivalent? And the role of the measure in the Aaronson's definition was also not very clear.","['measure-theory', 'ergodic-theory', 'probability-theory']"
4673787,Prove $ \sum_{i=0}^{n}(-1)^{i}q^{(i+1)i/2}{n\choose i}_{q}{n+m-i\choose n}_{q}=1 $,"Show that for any non-negative integers $n, m$ such that $n\le m$ , we have $$ \sum_{i=0}^{n}(-1)^{i}q^{(i+1)i/2}{n\choose i}_{q}{n+m-i\choose n}_{q}=1 $$ where ${n\choose i}_{q}$ is the Gaussian binomial coefficient . I have noticed that the expression ${n\choose i}_{q}{n+m-i\choose n}_{q}$ is symmetric with respect to $n$ and $m$ . Also, $(-1)^{i}q^{(i+1)i/2}{n\choose i}_{q}$ is the coefficient of $t^{i}$ in $\prod_{j=0}^{n-1}(1-q^{j+1}t)$ and ${n+m-i\choose n}_{q}$ is the coefficient of $t^{n}$ in $\prod_{j=0}^{m-i}(1-q^{j}t)^{-1}$ . I have been working on this problem for days yet still have no solutions though this should not be very hard.","['combinatorics', 'q-analogs']"
4673837,Leibniz integral rule for differentiation under the integral sign - is this condition needed?,See the general rule (the theorem) here: Leibniz rule - general form My question is: do we need to require the derivatives $a'(x)$ and $b'(x)$ to be continuous? The Wikipedia page requires that condition. But I have a calculus book which requires only that these derivatives exist i.e. it requires only that the functions $a(x)$ and $b(x)$ are differentiable. Which one is correct?,"['integration', 'multivariable-calculus', 'calculus', 'real-analysis']"
4673840,Composition of pushout remains a pushout,"$\require{AMScd}$ Define groups and homomorphismes as below. I am supposed to show, that given the left and the right rectangle are pushouts/amalgames, then the outer/big one is a pushout/amalgame aswell. $$
\begin{CD}
G @>f>> G' @>f'>> G''\\
@VgVV @VVg'V @VVg''V \\
H @>h>> H' @>h'>> H''
\end{CD}
$$ Does that mean that I have to show that the following diagram is a pushout? $$
\begin{CD}
G @>f' \circ f>> G''\\
@VgVV @VVg''V \\
H @>h' \circ h>> H''
\end{CD}
$$ If so, do I have to show that the different pushouts $H''$ of the right rectangle and the big rectangle coincide? I.e. Given $H'$ and $H''$ are pushouts, they are defined as $H':= G'*_{G}~H$ and $H'':= G'' *_{G'}~H'$ . Given my previous thought process do I now have to show that (where $\newcommand{\ngenl}{\mathopen{\vartriangleleft}}
\newcommand{\ngenr}{\mathopen{\vartriangleright}}\ngenl S \ngenr$ denotes the normal subgroup generated by $S$ ): $$
\begin{align*}G''*H/ \ngenl (f'\circ f)(x)g(x)^{-1} \forall x \in G\ngenr & =: G'' *_{G}~H \\
& \stackrel{!}{=}G'' *_{G'}~H'\\
&= G'' *_{G'}~(G' *_{G}~H)\\ &= 
\frac{G''*(G'*H/\ngenl f(x)g(x)^{-1} \forall x\in G\ngenr)}{\ngenl f'(\bar{x})g'(\bar{x})^{-1} \forall \bar{x}\in G'\ngenr}
\end{align*}
$$ And if so how do I continue ?","['group-theory', 'category-theory']"
4673851,Ratios of higher order derivatives with exponentially growing bound.,"Let $f: \mathbb R \rightarrow \mathbb R$ be a smooth function. Is there a characterization of the space of functions $f$ whose ratios of higher order derivatives are bounded by at most an exponentially growing constant at infinity? That is, for some $K$ large enough, there is a constant $C > 0$ for which $$ \Bigg\lvert\frac{f^{(n)}{(x)}}{f(x)}\Bigg\rvert \leq C^n, \quad \forall |x| \geq K. $$ (It might be that the correct form to avoid issues with zeroes is $|f^{(n)}(x)| \leq C^n(L + |f(x)|)$ instead, where $L$ is large enough.) I have an intuition the answer is functions which are bounded by an exponential but I do not know how to characterize the space to allow for bad behaviour like bounded oscillations with unbounded derivatives, e.g. $f(x) = e^x + \sin^2(e^x)$ , say. I also expect this space to contain functions of the form $f(x) = P(x)e^{cx}$ where $P$ is a polynomial and $c$ is a constant. For instance, if $f(x) = xe^x$ then $f^{(n)}(x) = (x + n)e^x$ and $|n/x| \leq n/K$ is growing linearly. More generally the $n$ -th derivative of $P(x)e^x$ is $Q_n(x)e^x$ where $Q_n$ is a binomial sum of derivatives of $P$ up to order $n$ , and so there at most $2^n$ terms in $Q_n$ and each of the terms can be bounded uniformly since $|P^{(n)}(x)/P(x)|$ is the ratio of a lower order polynomial to a higher order one for $n \geq 1$ . The motivation for this question is to have a class of functions which are ""essentially exponential"" in growth with respect to all derivatives. I suppose I could always claim this definition above is the characterization but I was wondering if there is a cleaner characterization given the examples seem relatively well behaved and almost commonplace. Edit 1: after some reading, I have a feeling this might be related to Fourier transforms and that it might be possible to prevent bad behaviour by asking for a condition on the Fourier transform (e.g. that it has compact support, say). However I don't really know enough about the Fourier transform to make a tractable characterization. Edit 2: I would be okay if someone can give a characterization of the case $n = 1$ , although I expect this to lead to a characterization for all $n \geq 1$ . There was a deleted answer that showed this is equivalent to boundedness of the log derivative which implies at most exponential growth. But this is not sufficient since one can have exponential growth with a bad derivative coming from a bounded oscillatory term, e.g. $f(x) = e^x + \sin^2(e^{e^x})$ .","['real-analysis', 'complex-analysis', 'entire-functions', 'calculus', 'functional-analysis']"
4673865,Where's my misstep in this trigonometric problem?,"$$\begin{align}
\tan\alpha+\cot\alpha=\sqrt{ 6 } \\
\tan^6\alpha+\cot^6\alpha= \ ?
\end{align}$$ The given answer is $52$ , but I got $214$ instead: \begin{align} \tan\alpha+\cot\alpha=\frac{1}{\sin\alpha \cos\alpha}&=\sqrt{ 6 } \\ \\
\tan^6\alpha+\cot^6\alpha=\frac{\sin^6\alpha}{\cos^6\alpha}+\frac{\cos^6\alpha}{\sin^6\alpha}&= \\
 \\
\frac{\sin^{12}\alpha+\cos^{12}\alpha}{\sin^6\alpha \cos^6\alpha}&= \\
\\
\frac{(\sin^3\alpha)^4+(\cos^3\alpha)^4}{\sin^6\alpha \cos^6\alpha}&= \\
 \\
\frac{1-2\sin^6\alpha \cos^6\alpha}{\sin^6\alpha \cos^6\alpha}&= \\ \\
\frac{1}{\sin^6\alpha \cos^6\alpha}-2&= \\ \\
(\sqrt{ 6 })^6-2=216-2&=214
\end{align} I know my method is not the most efficient, but my question is: what is wrong with my solution?",['trigonometry']
4673891,First order differential equations - issue,"I understand the existence and uniqueness theorem as follows: Let $\dot{x}=F(t,x)$ and $F'_x(t,x)$ be continuous on a rectangle $\Gamma$ , and let $(t_0,x_0)$ be points (of our initial value problem) inside $\Gamma$ . Then, there exists exactly one local solution passing through $(t_0,x_0)$ .
The rectangle $\Gamma$ is defined as: $\Gamma=\{(t,x):|t-t_0| \le a, |x-x_0| \le b\}$ , where $a,b>0$ are constants. If these conditions are met, there exists a unique solution for our initial value problem, that might only exist for an interval smaller than $(t_0-a,t_0+a)$ . Indeed, defining $M= max_{(t,x) \in \Gamma} |F(t,x)|$ , and $r=min \{ a,b/M \}$ , our initial value problem has a unique solution on $(t_0 - r, t_0 +r)$ Now, by considering the following initial value problem: $\dot{x}=x^2$ , and $x(0)=1$ , there exists a unique solution because $x^2$ is continuous everywhere (existence), and $2x$ is still continuous everywhere(uniqueness). Thus, these two functions are trivially continuous around $(0,1)$ . The particular solution yields: $x(t)= \frac{1}{1-t}$ , which exists for all $t \in(- \infty,1)$ . However, if I look at the theorem, I always expect a solution symmetric around $t_0$ , i.e. $(t_0 - r, t_0 +r)$ . Why is this not the case? Update: Source:
Knut Sydsaeter, Peter Hammond,Atle Seierstad, - Further Mathematics for Economic Analysis (2009)","['derivatives', 'ordinary-differential-equations']"
4673932,"If $u,v\in \dot H^{1/4}(\mathbb R)$, where does the product $uv$ lie?","Let $u,v\in\dot H^{1/4}(\mathbb R)$ . Since by Sobolev embedding $\dot H^{1/4}(\mathbb R)\hookrightarrow L^4(\mathbb R)$ , I know that the product $uv$ lies in $L^2(\mathbb R)$ , that is, it holds the estimate $$ \|uv\|_{L^2}\leq C\|u\|_{\dot H^{1/4}}\|v\|_{\dot H^{1/4}}. $$ My question is, is it possible to find a better estimate on $uv$ ? Does the product lie in some Besov space of positive regularity, or any normed space that is strictly contained in $L^2$ ? The Sobolev space $\dot H^s(\mathbb R)$ is the space of tempered distributions such that the Fourier transform is locally integrable and the following norm is finite: $$ \|u\|^2_{\dot H^s}=\left|\int_{\mathbb R}|\xi|^{2s}|\widehat u(\xi)|^2\,d\xi\right|, $$ where $\widehat u$ is the Fourier transform of $u$ .","['sobolev-spaces', 'fourier-analysis', 'functional-analysis', 'real-analysis']"
4673955,Understanding supremum of ordinals. Differences between limit ordinal and successor ordinal.,"I apologize for this naive question, but I'm new to set theory and I'm having a hard time figuring out properties of supremum of ordinals. From here on out I will refer to $\alpha$ as a nonzero ordinal. If I understand correctly the following are equivalent. $\alpha$ is a limit ordinal; $\bigcup \alpha = \alpha$ . Moreover, in general we have that $\bigcup \alpha$ = $\text{sup }\alpha$ . Now I don't understand one thing, if $\bigcup \alpha$ = $\text{sup }\alpha$ , this means that whenever $x\in\alpha$ we have that $x\in\bigcup\alpha$ (because $\bigcup\alpha$ is the supremum of $\alpha$ ). However by transitivity $x\in\bigcup\alpha$ implies $x\in\alpha$ . So it seems to me that property 2. hold for all ordinals, limit and successor. Now, I know that there is some problem in my understanding, but where? Also, consider $6= \{ 0,1,2,3,4,5 \}$ , is it correct to say that $\bigcup 6 = \{ 0,1,2,3,4 \} $ ? If so, the previous point $x\in\alpha$ implies $x\in\bigcup\alpha$ is not true (because $5\in 6$ , but $5\not\in\bigcup 6$ ), but then I must be missing something about the supremum properties. I thought the supremum was the least of the upperbounds, so as an upperbound, whenever $x\in\alpha$ we should have $x\in \text{sup } \alpha$ .","['elementary-set-theory', 'ordinals', 'supremum-and-infimum']"
4673971,Does an algebra automorphism of $M_n(R)$ preserves the characteristic polynomial?,"These days, I am playing around Skolem Noether for matrix algebras, and a question arises. If $F$ is a field , a by-product of Skolem Noether implies that any $F$ -algebra automorphism $\rho$ of $M_n(F)$ preserves the characteric polynomial, that is $\chi_{\rho(M)}=\chi_M$ for all $M\in M_n(F)$ . Question 1. Can we prove that any $F$ -algebra automorphism $\rho$ of $M_n(F)$ preserves the characteric polynomial without using the fact that $\rho$ is inner ? (I would like to use this fact to give a constructive proof of Skolem Noether) Question 2. More generally, let $R$ be a commutative ring with $1$ , and let $\rho$ be an $R$ -algebra automorphism. Is is true that $\rho$ preserves the characteristic polynomial ?? It seems to be true for integral domains: if $R$ is a domain, let $K$ be its quotient field.
Then $\rho\otimes Id_K$ is an automorphism of $M_n(K)$ and hence is inner (even if $\rho$ isn't), so the result follows easily, since $M$ may be viewed as a matrix with values in $K$ . If the answer for Question 2 is positive in full generality, I would be grateful to have an elementary argument (or at least, as much as possible).","['abstract-algebra', 'linear-algebra', 'commutative-algebra', 'characteristic-polynomial']"
4673987,Determining the branch of the complex argument,"I am working on a complex analysis problem and I am stuck at a particular section. Basically, we are given a region in which the argument of a complex number, $\arg(z)$ , is defined, and its value at a given point. We are then asked to find the argument of other complex numbers based on this information. The point in which I am stuck is when working in $\mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\}$ and I am given $\arg(1)=0$ , and I need to find $\arg(i)$ . I thought that the ""origin line"" for measuring the angle of a complex number in this set was the reflection through the origin of the removed line: $\{re^{i(\frac{\pi}{4} + \pi)}: r \geq 0\}$ . From there, I calculated the argument of 1 from this line: $$
\arg(1) = \frac{3\pi}{4} + 2\pi n \quad , n\in \mathbb{Z}
$$ and imposed $\arg(1) = 0$ to find $n$ , but I get nonsense: $n = -3/8$ . Where is the flaw in this argument? Perhaps I am wrong when defining the argument in $\mathbb{C} \setminus \{re^{i\frac{\pi}{4}}: r \geq 0\}$ this way? By the way, the answer turns out to be $\arg(i) = -\frac{3\pi}{2}$ . Any help will be appreciated.","['complex-analysis', 'complex-numbers']"
4674003,There exists $j>i\geq 1$ such that $\mu(A\cap f^{-(k_j-k_i)}(A))>0$ when $\mu(A\cap f^{-n_j}(A))>0$ for a probability measure $\mu$ and sequence $n_j$,"This exercise is from the book Foundations of Ergodic Theory by Viana, p. 9, exercise 1.2.5. part a.) Let $(X,\mathcal{F},\mu)$ be a probability space and $f:X\to X$ be a $\mu$ -preserving mapping in the sense that $\forall A\in\mathcal{F}:\mu(A) = \mu(f^{-1}(A))$ . Assume that $A\in\mathcal{F}:\mu(A) > 0$ and that $\left(n_j\right)_{j=1}^\infty$ is the sequence of non-negative integers such that $\mu(A\cap f^{-n_j}(A)) > 0$ for all $j=1,2,\dots$ . The task is the show that for any increasing sequence $k_1<k_2<\cdots$ there exists $j>i\geq 1$ such that $\mu(A\cap f^{-(k_j - k_i)}(A)) > 0$ . A given hint is to first show that for every $N\in\mathbb{N}$ such that $\frac{1}{N} < \mu(A)$ there exists $j\in\mathbb{N}$ such that $0\leq n_j\leq N$ and $\mu(A\cap f^{-n_j}(A)) > 0$ . A minor detail which has occurred to me is that instead of "" any "" sequence $k_1<k_2<\cdots$ we should probably only look at subsequences of $(n_j)_{j\in\mathbb{N}}$ . Since it is not true that if $(a_n)_{n=1}^\infty, (b_n)_{n=1}^\infty$ are any two sequences of non-negative integers, then $\exists n\in\mathbb{N}:\exists j>i: a_n = b_j - b_i$ . But I digress. As of now I have not made any progress in the main task or the given hint. Namely, ( Progress on hint: ) I have not managed to deduce any relationship with connects increasing power of $f$ to decreasing lower bound of $\mu(A\cap f^{-n_j}(A))$ . Poincarés recurrence theorem gives us that $\mu$ almost every point of $A$ visits $A$ for infinitely many powers of $f$ . Therefore $B := \limsup_{n\to\infty} A\cap f^{-n}(A)$ has a non-zero measure and by Borel-Cantelli lemma $\sum_{n=1}^\infty \mu(A\cap f^{-n}(A)) = +\infty$ . ( Progress on the main task: ) While we can add as many pre-images of $f$ to $\mu$ 's argument, $\mu(A) = \mu(f^{-M}(A)),\forall M\geq 0$ , I have not managed to connect this to the difference between $n_j,n_i$ . Something tells me that we would like to use a proof by contradiction by bounding $A\cap f^{-n_j}(A)$ above by some $A\cap f^{-(n_{j_1} - n_{j_2})}(A)$ , but so far this has not lead to anything. The biggest problem that I think I am having is that I have no way of knowing with which indices the points of $A\cap f^{-n_j}(A)$ may visit $A$ again, and hence it is difficult to say anything about $\mu(A\cap f^{-(n_j - n_i)}(A))$ .","['measure-theory', 'ergodic-theory', 'probability', 'real-analysis']"
4674024,Alternate methods for finding the shortest distance between these two curves,"There are two curves $2y^2=2x-1$ and $2x^2=2y-1$ Find the shortest distance between these two curves. I'm well aware of the traditional method of solving these type of questions. The concept is that the minimum distance is along the common normal. And then we find $y'$ and the points and then we proceed accordingly. I want to know if there is some other method of solving this particular question. A method that uses minimum calculus is appreciated. That being said, it doesn't mean that calculus based methods are not welcomed. I'm posting my attempt below as answer.","['analytic-geometry', 'coordinate-systems', 'curves', 'calculus', 'algebra-precalculus']"
4674052,Integral form of a telescoping series,"I want to know if the following relation is true $$ \frac{1}{2\pi i}  \int_{-\infty}^{\infty} dz  \tanh{\pi z} \left[\frac{1}{\sqrt{1-z^2}} - \frac{1}{\sqrt{2-z^2}} \right] = \sum_{n>0} \left[\frac{1}{\sqrt{1+(n+1/2)^2}} - \frac{1}{\sqrt{2+(n+1/2)^2}} \right]. $$ I will name the last series $S$ .
I consider a complex contour that begins in the negative part of the real axis, goes around the points $\pm 1, \pm \sqrt{2}$ with half circle, then closes with a half circle in the upper half plane. The small half circles on the real axis won't give any contribution to the full integral because the residues at $z=\pm 1, \pm \sqrt{2}$ will be zero. Same for the half circle in the upper half plane when we took its radius to be large (the integral for the $\frac{1}{\sqrt{1-z^2}}$ term will be canceled by the $\frac{1}{\sqrt{2-z^2}}$ term for radius that tends to infinity).
Now the contour enclosed poles of $\tanh{\pi z}$ which are at the points $i(n+1/2)$ , $n>0$ . By the residue theorem I get that the integral written above $I= 2\pi i S$ . I tried to numerically evaluate $I$ in Mathematica to compare it to the numerical value of $S$ . I found significative differences between the two. So I would like to know if my calculation is flawed.","['complex-analysis', 'complex-integration']"
4674077,regularity condition for a solution of a parabolic PDE on a manifold,"Given a Riemannian manifold $(M,g)$ (can be taken parallelizable if needed) a second order differential operator $\mathcal{L}$ can be expressed in a local chart as $$
\mathcal{L}f(x)= \left(a^{ij}(x) \partial_i \partial_j+ b^j(x) \partial_j +c(x)\right)f(x)
$$ The operator is said uniformly elliptic (w.r. to $g$ ) if, given an atlas $\{(U_\alpha,(x^i)_\alpha)\}$ exists a constant $C>0$ such that in any chart $$
a^{ij}(x) \xi_i\xi_j \ge C g^{ij}(x) \xi_i\xi_j
$$ It is immediate that a uniformly elliptic operator is elliptic. Let now $u(t,x)$ a classical solution of the Cauchy problem \begin{align} 
\label{parabolic}(\partial_t -\mathcal{L}) u(t,x)&=0\\
u(0,x)&=f(x)
\end{align} Where $f: M \rightarrow \mathbb{R}$ Is a smooth function.
If we assume the coefficients to be smooth functions  it is true that $\mathcal{L}u(t,x)$ is a solution for the Cauchy problem? i.e has $\mathcal{L}u(t,x)$ the same regularity of $u(t,x)$ ? EDIT: If the manifold is compact the result should follow. Indeed it is possible to prove that for compact manifolds any elliptic operator is the generator of a Feller semi-group.  For a dense classe of functions the semigroup can be written as $e^{tA}$ . By the Kolmogorov backward formula.  for any such function $e^{tA}f$ is a solution of the PDE. If the function is taken to be smooth of course $u(t,x):=e^{tA}f(x)$ is smooth in $t$ and in $x$ . I am not abke to characterize in any useful way the set of functions for which this works.","['elliptic-equations', 'operator-theory', 'parabolic-pde', 'partial-differential-equations', 'differential-geometry']"
4674084,"Building a puzzle game - how to calculate total of combination sets once ""invalid"" results are excluded?","Please bear with me on terminology; I am learning! Also, please correct me if I've gotten anything wrong so far. Context: I am  creating a single-player tile-based puzzle game where: Once tiles (200 in total) have been played in a slot (5 slots/tiles per turn), three or more of them cannot be played again together. This means any three or more of the initial five tiles, in any combination. If { 1 2 3 4 5 } has been played, then { 1 3 5 10 15} is now an illegal move for the rest of the game ; while { 1 3 6 10 15} is legal. Five tiles are played every turn until there are no more legal plays (only five tiles can ever be played; more or fewer than five is not a legal play) Question :  What is the shortest possible length of a game with all legal plays? My thoughts: So far I have discovered combinatorics seems the place to start, with a simple combination. Letting $n=200$ and $r=5$ . Order is not important (tiles can be played in any slot), repetition is not allowed (tiles are unique). The number of sets of five tiles is $$
C(n,r) = \binom{n}{r} = \frac{n!}{(r!(n-r)!)}.
$$ This results in 2535650040 subsets, so as I understand it there are 2535650040 possible opening plays. Each turn concludes and removes more subsequent options for sets, and this is the part I'm having deep trouble getting my head around turning into a formula. Edit as requested to show an example for a smaller set: With a set of 7 tiles instead of 200, and 3 slots instead of 5, there are 35 subsets: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Instead of 3-tile combinations unable to be played in future moves, this uses 2-tile combinations unable to be played in future moves: Move 01: {1,2,3} Result: {1, 2, * }, {1, 3, * }, and {2, 3, * } are now illegal.
13/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 02: {1,4,5} Result: {1, 2, * }, {1, 3, * }, {2, 3, * }, {1, 4, * }, {1, 5 * }, and {4, 5, * } are now illegal.
22/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 03: {1,6,7} Result: {1, 2, * }, {1, 3, * }, {2, 3, * }, {1, 4, * }, {1, 5 * }, {4, 5, * }, {1, 6, * }, {1, 7, * }, and {6, 7, * } are now illegal.
27/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 04: {2,4,6} Result: {1, 2, * }, {1, 3, * }, {2, 3, * }, {1, 4, * }, {1, 5 * }, {4, 5, * }, {1, 6, * }, {1, 7, * }, {6, 7, * }, {2, 4, * }, {2, 6, * }, and {4, 6, * } are now illegal.
31/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 05: {2, 5, 7} Result: {1, 2, * }, {1, 3, * }, {2, 3, * }, {1, 4, * }, {1, 5 * }, {4, 5, * }, {1, 6, * }, {1, 7, * }, {6, 7, * }, {2, 4, * }, {2, 6, * }, {4, 6, * }, {2, 5, * }, {2, 7, * }, and {5, 7, * } are now illegal.
33/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 06: {3, 4, 7} Result: {1, 2, * }, {1, 3, * }, {2, 3, * }, {1, 4, * }, {1, 5 * }, {4, 5, * }, {1, 6, * }, {1, 7, * }, {6, 7, * }, {2, 4, * }, {2, 6, * }, {4, 6, * }, {2, 5, * }, {2, 7, * }, {5, 7, * }, {3, 4, * }, {3, 7, * } and {4, 7 * } are now illegal.
34/35 sets cannot be played: {1,2,3} {1,2,4} {1,2,5} {1,2,6} {1,2,7} {1,3,4} {1,3,5} {1,3,6} {1,3,7} {1,4,5} {1,4,6} {1,4,7} {1,5,6} {1,5,7} {1,6,7} {2,3,4} {2,3,5} {2,3,6} {2,3,7} {2,4,5} {2,4,6} {2,4,7} {2,5,6} {2,5,7} {2,6,7} {3,4,5} {3,4,6} {3,4,7} {3,5,6} {3,5,7} {3,6,7} {4,5,6} {4,5,7} {4,6,7} {5,6,7} Move 07: {3, 5, 6} Final move - Result: 35/35 sets cannot be played Outcome: 7 tiles, 3 slots, 2 matches illegal = 7 moves 200 tiles, 5 slots, 3 matches illegal = ? moves I'm probably just not using the right terminology to look for what I need but as I've found several similar pages here (although mostly around excluding specific pairs or combinations from the start, while this is more self-referential), it seems like the right place to ask. The closest I've found was this Combinations with limited overlap which is a coding question but did have a similar problem to solve. Again, I am fairly new to this and - trying to be - self-taught for personal projects (not knowledgeable on real maths at all; in my 40s and have mostly been working in the arts), so any help or direction would be much appreciated. Suggestions on where to research next or naming the formulas/theorems you would use would also be fantastic (as while I'd obviously love a direct answer, it's how to get there that will help me improve on solving this kind of problem in the future). Thank you!","['combinatorics', 'extremal-combinatorics']"
4674103,Is it necessary to consider the case $p=1$ separately?,"Let $p \in [1, \infty]$ . Let $f \in L^p_{\text{loc}} (\mathbb R)$ be $T$ -periodic, i.e., $f(x+T) = f(x)$ a.e. $x \in \mathbb R$ . Let $$
\bar f := \frac{1}{T} \int_0^T f (t) \, dt.
$$ We define a sequence $(u_n) \subset L^p(0, 1)$ by $u_n (x) := f(nx)$ for all $x \in (0, 1)$ . Theorem $u_n \to \bar f$ in the weak topology $\sigma(L^p, L^{p'})$ where $p'$ is the Hölder conjugate of $p$ . I'm reading the proof of above theorem, i.e., Proof First, it is easy to check that $\int_a^b u_n(t) d t \rightarrow(b-a) \bar{f}$ (for every $\left.a, b \in(0,1)\right)$ . This implies that $u_n \rightarrow \bar{f}$ weakly $\sigma(L^p, L^{p^{\prime}})$ whenever $1<p \leq \infty$ (since $p^{\prime}<\infty$ , step functions are dense in $L^{p^{\prime}})$ . When $p=1$ , i.e., $f \in L_{\mathrm{loc}}^1(\mathbb{R})$ , there is a $T$ -periodic function $g \in L^{\infty}(\mathbb{R})$ such that $\frac{1}{T} \int_0^T|f-g|<\varepsilon$ (where $\varepsilon>0$ is fixed arbitrarily).
Set $v_n(x)=g(n x), x \in(0,1)$ and let $\varphi \in L^{\infty}(0,1)$ . We have $$
\left|\int u_n \varphi-\bar{f} \int \varphi\right| \leq 3 \varepsilon\|\varphi\|_{\infty}+\left|\int v_n \varphi-\bar{g} \int \varphi\right|
$$ and thus $\lim \sup _{n \rightarrow \infty}\left|\int u_n \varphi-\bar{f} \int \varphi\right| \leq 3 \varepsilon\|\varphi\|_{\infty} \forall \varepsilon>0$ . It follows that $u_n \rightarrow \bar{f}$ weakly $\sigma\left(L^1, L^{\infty}\right)$ . My question Clearly, $(0, 1)$ has finite Lebesgue measure, so $L^\infty (0, 1) \subset L^1(0, 1)$ and thus step functions are still dense in $L^\infty (0, 1)$ . Could you explain why the author still considers the case $p=1$ separately? $\lim \sup _{n \rightarrow \infty} \left|\int v_n \varphi-\bar{g} \int \varphi\right| =0$ in the proof? Update: I have added below steps for more clarity. We have $$
\begin{align}
& \left|\int_0^1 u_n \varphi-\bar{f} \int_0^1 \varphi\right| \\
\le{} & \int_0^1 |u_n - v_n| \varphi + \bigg | \int_0^1 v_n \varphi - \bar g \int_0^1 \varphi \bigg | + \int |\bar g - \bar f| \varphi \\
\le{} & \|\varphi\|_\infty \int_0^1 |u_n - v_n| + \bigg | \int_0^1 v_n \varphi - \bar g \int_0^1 \varphi \bigg | + \|\varphi\|_\infty |\bar g - \bar f|.
\end{align}
$$ First, $|\bar g - \bar f| = \frac{1}{T} \int_0^T|f-g|< \varepsilon$ . Let $m := \lfloor n/T \rfloor$ . Then $$
\begin{align}
\int_0^1 |u_n - v_n|  &= \int_0^1 |f(nx)-g(nx)| \, dx = \frac{1}{n} \int_0^n |f-g| \\
&= \frac{1}{n} \int_0^{mT} |f-g| + \frac{1}{n} \int_{mT}^n |f-g| \\
&= \frac{m}{n} \int_0^{T} |f-g| + \frac{1}{n} \int_{mT}^n |f-g| \\
&\le \frac{(m+1)\varepsilon T}{n} = \big ( \big \lfloor \frac{n}{T} \big \rfloor +1 \big )\frac{\varepsilon T}{n} \\
&\le 2 \frac{n}{T} \frac{\varepsilon T}{n} = 2 \varepsilon.
\end{align}
$$","['banach-spaces', 'weak-convergence', 'lp-spaces', 'functional-analysis', 'weak-topology']"
4674120,"How to find all solutions of the ODE $x'=3x^{\frac{2}{3}}, x(0)=0$","Problem: Find all the solutions of the IVP $$x'=3x^{\frac{2}{3}}, x(0)=0$$ for $t\geq 0$. Here $3x^{\frac{2}{3}}$ is not $C^1$, so the existence and uniqueness theorem does not apply here. My guess the solutions is $$x=\left\{\begin{matrix}
0 & \text{if }0\leq t< t_{0} \\ 
 (t-t_{0})^3 &  \text{ if }t\geq t_{0}  
\end{matrix}\right.$$
$t_{0}\in\mathbb{R^+}$ or $t_{0}\rightarrow+\infty$. But my professor told me that there are a lot more! My main question is: How can I find all the solutions, and then prove that they are all the solutions, rigorously?","['analysis', 'ordinary-differential-equations', 'real-analysis']"
4674132,How do I input a vector form of ODE's on Runge-Kutta-4 for generality to solve in matlab,"I have come across a system of ODE's that are written on vector/Matrix format such that; $Ax'=b$ For simplicity, say the system of ODE's has a vector $x'$ containing first order derivatives of 2-variables: $x$ and $y$ with respect to $t$ such that $x' = [x1'  x2']^T $ with given initial conditions on $x1(0)=a0$ and $x2(0)=b0$ . Vector $b$ and Matrix $A$ contain dependent variables of $x1, x2, t$ . How would you generally input such a system in Runge-Kutta 4 to solve for $x1$ and $x2$ .  A general method that will be applicable for a larger ODE system as well. I am using matlab. For an actual example; suppose $$\pmatrix{1+x1^2& t-x2\\2x1x2&1}\pmatrix{x1'\\x2'}=\pmatrix{3x1^2x2^2+7x2-4\\x1-x2t+x2}$$ For the given initial conditions $x1(0)=10$ and $x2(0)=20$ How do I input such a format of ODE's into Runge-Kutta-4th order to solve in Matlab, a way that will be general for any larger bigger size of ODEs in a format $Ax'=b$ Thank you very much for your help in advance","['nonlinear-analysis', 'ordinary-differential-equations', 'computational-mathematics', 'runge-kutta-methods', 'numerical-methods']"
4674165,"What is the boundary of $ A=\{u : |u^{\prime}|\le 2 \text{ and } u(x)\neq k\pi,\forall k\in\mathbb{Z}, \forall x\in [0, 1] \}$?","Let $u:[0, 1]\to\mathbb R$ be of class $C^1$ over $[0, T]$ .
As an exercise I have to determine the boundary of the set $$ A=\{u : |u^{\prime}|\le 2 \text{ and } u(x)\neq k\pi,\forall k\in\mathbb{Z}, \forall x\in [0, 1] \}.$$ Here is considered on $C^1([0, 1])$ the topology coming from $\| u\|_\infty +\| u'\|_\infty$ . I am in trouble in determining the boundary of $A$ . If $k=0$ , I note that $A$ reduce to $$ A=\{u : |u^{\prime}|\le 2 \text{ and } u(x)\neq 0, \ \forall x\in [0, 1] \}$$ and its boundary should be $$ \partial A=\{u : |u^{\prime}|\le 2 \text{ and } u(x)=0, \ \forall x\in [0, 1] \}.$$ I don't know how to proceed in the case $k\neq 0$ . Could someone please help? Thank you in advance.","['function-spaces', 'calculus', 'general-topology', 'real-analysis']"
4674211,Graded measure theory?,"I've just had a thought that somebody else has definitely had before but I don't know what they might have called it. I have a $\sigma$ -algebra $(X=\mathbb{R}^n, \Sigma)$ and I want to measure it, but instead of using a map $\Sigma \rightarrow \mathbb{R}$ , I want a map $\Sigma \rightarrow (\mathbb{N} \rightarrow \mathbb{R})$ that tells me how much measure a subset has in each dimension, measuring the topological interior separately from the topological boundary. This should give back the regular measure when restricted to the dimension of the space. I imagine an open ball in 3-space would have measure $(0, 0, 0,  \frac{4}{3} \pi r^3, 0...)$ , a closed ball would have measure $(0, 0, 4 \pi r^2, \frac{4}{3} \pi r^3, 0...)$ , a set of n isolated points would have measure $(n, 0...)$ , and so on. I would also hope this can extend to more than 2 dimensions in a single set, so that for example we can have a closed line segment sticking out of a 2-ball and get measure in dimensions 0, 1, and 2. Can anyone provide pointers to constructions like this in the literature? e: A less ambiguous way to ask this might be to ask about measures of embeddings of cell complexes in $\mathbb{R}^n$ , keeping the measures of embeddings of different dimensional cells in different grades.","['general-topology', 'graded-rings', 'measure-theory']"
4674219,Find tetrahedron triangles' angles from dihedral angles.,"Finding dihedral angles between tetrahedron faces from triangles' angles at the tip has been answered in: Dihedral angles between tetrahedron faces from triangles' angles at the tip My questions is the converse: at the tip of a tetrahedron, if I know the three dihedral angles, can I find the three triangles' angles?","['triangles', 'trigonometry', 'geometry', '3d']"
4674221,"What is Kock's ""comprehensive axiom"" in synthetic differential geometry?","Anders Kock mentions many axioms in his book about synthetic differential geometry.
It seems that, what nlab calls: ""the Kock-Lawvere axiom"", is called axiom $1^W_k$ in the book.
This axiom states that for a Weil algebra $W$ over $k$ we have that $R \otimes W$ is isomorphic to $R^{\text{Spec}_R(W)}$ . This makes a decent amount of sense to me, however in this book axiom $1^W_k$ is actually a consequence of axiom $2^k$ , which he calls: ""The comprehensive axiom"".
This axiom states that for finitely presented $k$ -algebras $B$ , and any $R$ -algebra $C$ , the canonical map $$\text{hom}_{R\text{-Alg}}(R^{\text{Spec}_R(B)},C) \to \text{Spec}_C(B)$$ is an isomorphism. I don't quite understand this axiom and I especially don't get why one would assume this, as it is not necessarily about infinitesimals. So why is this axiom assumed?
Is this axiom maybe just a standard result in algebraic geometry, and if so which?","['synthetic-differential-geometry', 'algebraic-geometry']"
4674258,"Find the global maximum of $\frac{\sqrt {xy}}{(x + y + 2)^2},$ where $x,y\geq 0 $ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question I tried AM-GM inequality and got some conclusions then came to the max value = 1/16 where $x = y = 1$ (still not sure 100%). But it was more likely ""guessing"" method.
We can analyse the function and etc. But  if you can then, please, show me more than 1 method.
Cause it's school level olympiad problem which doesn't actually consider high school knowledge, so no calculus. I would like to see how to solve it like a student from a middle school, but other methods are also approved. Thanks","['contest-math', 'inequality', 'maxima-minima', 'optimization', 'algebra-precalculus']"
4674291,"HMMT 2014 #9, how many times has Lucky performed the procedure when there are 20 tails-up coins?","There is a heads up coin on every integer of the number line. Lucky is initially standing on the zero
point of the number line facing in the positive direction. Lucky performs the following procedure: he
looks at the coin (or lack thereof) underneath him, and then, If the coin is heads up, Lucky flips it to tails up, turns around, and steps forward a distance of
one unit. If the coin is tails up, Lucky picks up the coin and steps forward a distance of one unit facing the
same direction. If there is no coin, Lucky places a coin heads up underneath him and steps forward a distance of
one unit facing the same direction. He repeats this procedure until there are 20 coins anywhere that are tails up. How many times has
Lucky performed the procedure when the process stops? Source: HMMT 2014 Problem 9 (with an official solution) In the official solution, 4 claims are made: We keep track of the following quantities: Let $N$ be the sum of $2^k$ , where $k$ ranges over all nonnegative integers such that position $−1 − k$ on the number line contains a tails-up coin.
Let $M$ be the sum of $2^k$ , where $k$ ranges over all nonnegative integers such that position $k$ contains a tails-up coin. We also make the following definitions: A ""right event"" is the event that Lucky crosses from the
negative integers on the number line to the non-negative integers. A ""left event"" is the event that
Lucky crosses from the non-negative integers on the number line to the negative integers. We now make the following claims: Claim (a): Every time a right event or left event occurs, every point on the number line contains a coin. Claim (b): Suppose that $n$ is a positive integer. When the $n$ th left event occurs, the value of $M$ is equal to $n$ . When the $n$ th right event occurs, the value of $N$ is equal to $n$ . Claim (c): For a nonzero integer $n$ , denote by $\nu_2(n)$ the largest integer $k$ such that $2^k$ divides $n$ . The number
of steps that elapse between the $(n−1)$ st right event and the $n$ th left event is equal to $2\nu_2(n)+1$ .
The number of steps that elapse between the $n$ th left event and the $n$ th right event is also equal
to $2\nu_2(n)+1$ . (If $n − 1 = 0$ , then the “ $(n − 1)$ st right event” refers to the beginning of the
simulation.) Claim (d): The man stops as soon as the $1023$ rd right event occurs. (Note that $1023 = 2^{10} − 1$ .) Below is my attempt of a proof of claims (a)-(d) except (b), which was proved in the official solution. I'd appreciate any simpler solutions and an answer to my question at the end. We prove claims (a)-(d) except claim (b) below. Proof of claim (a): We use induction on the total number of right and left events to prove the following stronger claim: right after each event, there exist some nonnegative integers $l$ and $m$ so that points $0$ to $l-1$ contain tails up coins, points $-m$ to $-1$ contain tails up coins, and points $l$ and $-(m+1)$ contain heads up coins and  all other points contain coins. The claim holds trivially in the base case where no events have occurred. Suppose the some (possibly zero) events have occurred. Suppose first that a right event just occurred (we consider the starting point to be the end of the 0th right event) and Lucky is currently at point $0$ and facing in the positive direction. Then Lucky picks up the coins at points $0$ to $l-1$ , turns over the coin at point $l$ , puts heads up coins at points $l-1$ down to $0$ and then moves to point $-1,$ causing a left event to occur, and the claim holds. If a left event just occurred, then Lucky is at point $-1$ . Since Lucky only performed a finite number of moves, only a finite number of tails up coins are on the number line. Lucky then proceeds to pick up all tails up coins from $-1$ down to $-m$ for some nonnegative integer m where m is maximal so that the coins at points $-1$ down to $-m$ are tails up, flips the heads up coin at $-(m+1)$ , and places heads up coins at $-m$ to $-1$ . Lucky then moves to point $0$ and a right event occurs. At this point, the claim of the inductive hypothesis still holds, since coins at $0$ to $l-1$ are unchanged. Proof of claim (c): For convenience, say a point $x$ is tails up if there is a tails up coin at it and define a heads up point similarly. Also say that for a positive integer $n$ , bit $k$ in $n$ 's binary representation is the bit with value $2^k.$ Note that since right and left events alternate, with a left event occurring first, then the $n$ th left event always precedes the $n$ th right event and the $(n+1)$ th left event follows the $n$ th right event (which follows by induction). Thus, by claim (b) and the above observation,  after the $(n-1)$ th right event, $M=n-1, N=n-1$ . In particular, the point $k\ge 0$ is tails up iff $2^k$ has a nonzero coefficient in the binary representation of $n-1$ . Now if $k=v_2(n)$ , we know that points $0,\cdots, k-1$ are all tails up, since the bits with values $2^0,\cdots, 2^{k-1}$ in the binary representation of $n-1$ must all be $1$ 's and bit $k$ is $0$ because otherwise we'd get a contradiction to the maximality of $k$ . Hence $k$ steps are taken to pick up the coins at points $0$ to $k-1,$ and k+1 more steps to place heads up coins at $k-1,\cdots, -1$ . After this, the $n$ th left event occurs. This proves the first part of the claim. Now to prove the second part, suppose the $n$ th left event has just finished, where $n\ge 1.$ Then Lucky is at point $-1$ . Let $k=v_2(n).$ Then since $N$ equals $n-1$ at this point, point $-1-k$ is tails up iff bit $k$ in the binary representation of $n-1$ is a $1$ . By the definition of $k$ , bits $0$ to $k-1$ in the binary representation of $n-1$ are $1$ while bit $k$ is a zero. Hence Lucky picks up the $k$ coins at $-1,-2,\cdots, -k$ , flips over the heads up coin at $-k-1$ , and puts heads up coins at $-k,\cdots, -1$ and then moves to point $0$ . At this point, the $n$ th right event occurs and exactly $2k+1$ steps have occurred, as required. Finally, Proof of claim (d): We just need to consider the end of a left or right event. (Why?) The man stops precisely when $N$ has $x$ ones and $M$ has $y$ ones in their binary representation, where $x+y = 20$ . If the man stopped at the nth left event, then $M=n, N =n-1$ . If the man stopped at the nth right event, then $N=n,M=n$ . Note that the number of ones in the binary representation of $n-1$ is the number of ones in the binary representation of $n$ plus $v_2(n)-1.$ So we need to find the smallest positive integer $n$ so that $2B(n) + v_2(n)-1 = 20$ or $B(n)=10,$ where $B(n)$ is the number of ones in the binary representation of $n$ . The smallest $n$ with $B(n)=10$ is $1023.$ If $B(n)<10,$ then $v_2(n) = 21 - 2B(n)\ge 3,$ so $n$ is divisible by $8$ . If $B(n)\leq 5,$ then $n$ is divisible by $2^{10}$ and thus would be larger than $1023.$ $n$ is at least $2^{v_2(n)} \cdot (2^{B(n)}-1),$ since $n/2^{v_2(n)}$ has $B(n)$ ones in its binary representation.  Thus if $B(n)= 6,7,8,9$ then $n$ is at least the minimum of $2^9\cdot 63, 2^7\cdot 127, 2^5\cdot 255, 2^3\cdot 511,$ all of which exceed $1023$ . If the man has yet to complete the nth right event, then $N$ is strictly less than $n$ and $M$ is at most $n$ (by the proof of claim (b) in the official solution, in between a left event and the subsequent right event or vice versa, $M$ or $N$ respectively decrease, and only increase by $1$ when Lucky turns over a heads up coin before turning around and placing tails up coins. But doesn't the man stop just before he completes the $n$ th right event?","['contest-math', 'binary', 'combinatorics', 'discrete-mathematics', 'algorithms']"
4674334,Does this mean that Euler's constant diverges?,"I was playing around with the Euler-Mascheroni constant and the constant $\gamma$ is defined as the limit below. $$
\gamma = \lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log{n}
$$ It can also be expressed as this other sum which is basically the same thing but rewritten. It sums up the individual differences rather than the entire difference. $$
\gamma = \lim_{n\to\infty} \sum_{k=1}^n \left( \frac{1}{k} - \log\left(\frac{k+1}{k} \right) \right)
$$ This was where I was playing around and my work is right below. $$
\gamma = \lim_{n\to\infty} \sum_{k=1}^n \left( \frac{1}{k} - \log\left(\frac{k+1}{k} \right) \right) = \lim_{n\to\infty} \sum_{k=1}^n \left( \frac{1}{k} - \log\left(1+\frac{1}{k} \right) \right) \\
=\lim_{n\to\infty} \left[ \sum_{k=1}^n \left( \frac{1}{k} \right) -\sum_{k=1}^n \left( \log\left( 1+\frac{1}{k} \right) \right) \right]
$$ And using the properties of logarithms (specifically $\log{ab}=\log{a}+\log{b}$ ) we can rewrite the left sum as a product. $$
\lim_{n\to\infty} \left[ \sum_{k=1}^n \left( \frac{1}{k} \right) -  \log\left( \prod_{k=1}^n 
 \left(1+\frac{1}{k} \right) \right)  \right]
$$ Actually, while I'm writing this I don't even know why I rewrote the fraction that way because it does nothing but it turns out that that product is equal to $n+1$ (this is easily provable using factorials). So it follows that the limit is equal to $$
\lim_{n\to\infty} \left[ \sum_{k=1}^n \left( \frac{1}{k} \right) -  \log(n+1)  \right] 
$$ And in the beginning, $\gamma$ was defined as $$
\gamma = \lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log{n}
$$ So $$
\lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log{n} = \lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log(n+1)
$$ This can be tested by plugging in some numbers on a calculator like desmos . And this is where things start to break, assuming that my proof is correct. Since this formula holds true for any $n$ at infinity, surely this must be true for $n+2$ . And for $n+3$ , and $n+l$ , where $l$ is a positive integer. And the same thing backward must be true, that is, it's also true for $n-1$ and $n-l$ . So what if $l=n-1$ ? Then the formula yields $$
\lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log({n-l}) = \lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log({n-(n-1)}) = \lim_{n\to\infty} \sum_{k=1}^n \left(\frac{1}{k}\right) - \log({1}) \\
= \sum_{k=1}^n \frac{1}{k} = \infty
$$ Does this mean that $\gamma$ isn't even a constant and goes to infinity? I don't think so but I can't really find a mistake in my logic. I don't even know enough calculus to do a test (but I've heard of it) to check if the series converges.","['limits', 'calculus', 'euler-mascheroni-constant', 'logarithms']"
4674351,Embedding manifolds with boundary into Euclidean space,"Whitney proved that any n-dimensional manifold $M^{n}$ can be embedded into $\mathbb{R}^{2n}$ (as a codimension $n$ submanifold). A similar argument can be used to show that any $n$ -dimensional CW complex also embeds into $\mathbb{R}^{2n}$ . I am interested in the following related question. Suppose $M^{2n}$ is a $2n$ -dimensional manifold with boundary. When can it embed into $\mathbb{R}^{2n}$ ? There are two obvious necessary conditions. First, $M^{2n}$ must be parallelizable (since $\mathbb{R}^{2n}$ is). Second, the intersection form on $M$ must vanish.
I will further assume that $M^{2n}$ has the homotopy type of an $(n-1)$ -dimensional CW complex, so that Whitney's theorem allows this CW complex to embed into $\mathbb{R}^{2n}$ . I believe that these conditions are sufficient: any parallelizable $M^{2n}$ with boundary and homotopy type of an $n-1$ -dimensional CW complex embeds into $\mathbb{R}^{2n}$ . Are there any existing results in this direction? Clarification: I mean that the manifolds are smooth and compact with boundary, and embeddings are smooth.","['differential-topology', 'homotopy-theory', 'smooth-manifolds', 'differential-geometry']"
4674355,Is it a double coincidence $\sum_{n\ge 2}\frac{1}{n^2}\frac{\log(n!)^{\frac{1}{n}}}{\log n} \approx\lim_{n\to\infty}\frac{(n!)^{\frac{1}{n}}}{n}$?,"Is this a double co-incidence or is there a reason why $$
\sum_{n = 2}^{\infty}\frac{\log n!}{n^3 \log n} 
= \sum_{n = 2}^{\infty}\frac{1}{n^2}\frac{\log (n!)^{\frac{1}{n}}}{\log n} \approx \lim_{n \to  \infty}\frac{(n!)^{\frac{1}{n}}}{n} = \frac{1}{e}
$$ This this could be a coincidence because the difference between the sum and the limit is $< 0.0000523$ which is not earth shatering-ly small lets say as in case of the famous $e^{\pi \sqrt{163}}$ . However there is some similarity in the form of the sum and the limit so this could be a double co-incidence i.e. not just in value but also on the form of the expression. Or is there an explanation why?","['summation', 'number-theory', 'analysis', 'sequences-and-series', 'limits']"
4674420,"Norm of the adjoint operator of an operator of the form $\int_{0}^{1} k(t,s)f(s)ds$","I am asked to compute the adjoint of the operator $A: L^2[0,1] \longrightarrow L^2[0,1] $ defined by $$
Af(t)=\int_{t}^{1} t (s-1/2) f(s) ds
$$ and then to compute its norm, that is $\lVert A^* \rVert$ . My attempt: For the first part, we have that for all $f,g \in L^2[0,1]$ : \begin{align*}
\langle Af,g \rangle&= \int_{0}^{1} Af(t)g(t) dt= \int_{0}^{1} \left(\int_t^1  t (s-1/2) f(s) ds \right) g(t) dt\\
&= \int_{0}^{1} \int_{0}^{s} t (s-1/2) f(s) g(t) dt\, ds= \int_{0}^{1} f(s) \left( \int_{0}^{s} t(s-1/2)g(t)dt \right) ds
\end{align*} and therefore $$
A^{*}g(s)= \int_{0}^{s} t(s-1/2)g(t)dt
$$ Now, since $A$ is bounded (by $\lVert k \rVert_{2}$ , where $k(t,s)=1_{[t,1]}(s)t(s-1/2) \in L^2([0,1]\times[0,1])$ for example) I know that $\lVert A^* \rVert=\lVert A \rVert$ which led me to trying to compute $\lVert A \rVert$ . One natural candidate for $\lVert A \rVert$ is precisely $\lVert k \rVert_{2}=\frac{\sqrt{7}}{ \sqrt{720} }$ . I tried to find $f \in L^2[0,1]$ such that $\lVert  f \rVert_2=1$ and $\lVert  Af \rVert_2=\lVert  k \rVert_2=1$ but it wasn't such a good idea and consequently my approach is probably wrong. Any help or suggestion? In advance thank you.","['operator-theory', 'adjoint-operators', 'functional-analysis']"
4674443,"Alternative proofs of $L^q[0,1]$ is a (Baire) first category subset in $L^p[0,1]$","Let $1\leq p<q<\infty$ , deduce from each one of the following statements that $L^q[0,1]$ is a (Baire) first category subset in $L^p[0,1]$ : Let $p<r<s<q$ and $\beta=s/(s-1)$ , we define $g_n=n\cdot\chi_{[0,n^{-\beta}]}\; (n\in\mathbb{N})$ and $$
f(t)= \left\{
\begin{array}{ll}
t^{-1/r} & 0<t\leq 1 \\
0 &  t=0.
\end{array}
\right.\quad(*)
$$ Then: $$
\lim_{n\rightarrow\infty}\int_0^1{f(t)\,g_n(t)\;dt}=0
$$ For all $f\in L^q[0,1]$ , but not for $f\in L^p[0,1]$ defined above $(*)$ . The inclusion $L^q[0,1]\hookrightarrow L^p[0,1]$ is s non surjective continuous map. My work: I have proved that both statements are true, which is not trivial. But I have real struggles to deduce from each of them that $L^q[0,1]$ is a (Baire) first category subset in $L^p[0,1]$ . I think that both of them are related with some dense subset, but I am really...","['measure-theory', 'lebesgue-integral', 'lp-spaces', 'functional-analysis', 'baire-category']"
4674509,Can all matrices be written as the sum of commuting matrices?,"Can all matrices be written as the sum of commuting matrices? All invertible matrices $A$ can be written as a sum of matrices $A=B+C$ such that $B$ and $C$ commute, $BC=CB$ . One example might be $$B=\frac{A+A^{-1}}{2}, \quad C=\frac{A-A^{-1}}{2},$$ but I'm not certain if this is the only solution. My question is: Can non-invertible matrices be written as the sum of commuting matrices? Edit: In response to Lulu's comment, I need to add another condition that neither of $B$ or $C$ commute with all matrices.","['matrices', 'linear-algebra', 'linear-transformations']"
4674524,A question about Folland's definition of parametrization,"Here is Folland's definition of parametrization in his book Real analysis: modern techniques and their applications Chapter 11.2 Hausdorff measure. We now consider lower-dimensional sets in $\mathbb{R}^n$ . If $1 \le k \le n$ , a k-dimensional $C^1$ submanifold of $\mathbb{R}^n$ is a set $M\subset\mathbb{R}^n$ with the following property: For each $x \in M$ there exist a neighborhood $U$ of $x$ in $\mathbb{R}^n$ , an open set $V\subset\mathbb{R}^k$ , and an injective map $f:V\to U$ of class $C^1$ such that $f(V) = M\cap U$ and the differential $D_xf$ - i.e., the linear map from $\mathbb{R}^k$ to $\mathbb{R}^n$ whose matrix is $\left[\left(\frac{\partial f_i}{\partial x_j}\right)(x)\right]$ - is
injective for each $x\in V$ . Such an $f$ is called a parametrization of $M \cap U$ . Compare this definition with the definition of smooth manifold, which requires the coordinate map is a local homeomorphism. I want to show in Folland's definition, the local inverse $f^{-1}:M\cap U\to V$ is continuous with respect to the subspace topology of $M\subset\mathbb{R}^n$ . But in Do Carmo's book Differential geometry of curves and surfaces , in the definition of regular surface, local homeomorphism is required. Thanks in advance if you can give me any hint (or counterexample)! Thanks to @Kenny Wong's answer (figure 8 space) below. We can conclude that, ""figure 8 space"" is a $1$ -dimensional $C^1$ manifold in Folland's definition, but in the mean time, it is not a topological manifold if it is equipped with subspace topology.","['manifolds', 'differential-geometry']"
4674563,Number of squares that can be formed by picking points in a cubic equation graph,"I am a Vietnamese grade-12 student, and we take the Graduation test Mock test in Mathematics today. While the test itself is really difficult, here is one question that stands out for being so far away. Given the graph $f(x) = x^{3} - 3x$ , how many squares can be formed by picking 4 distinct points on the graph of this function? Due to the limited time I had for this question, I wasn't able to solve it in time. However, I DO managed to figure out these properties of such a square, if it exists: I marked the 2 extremums of the function with $A (-1, 2)$ and $B(1, -2)$ . If we call the vertices of this square $M$ , $N$ , $P$ , and $Q$ in clockwise order respectively, then $M$ , $P$ must lie in the graph region between $A$ and $B$ , $N$ lies in the region to the right of $B$ , and $Q$ lies in the region to the left of $A$ The square must have center in $O(0, 0)$ I have no idea what else to do. Please help! The picture is the Geogebra graph of $f(x) = x^{3} - 3x$ for reference.","['algebra-precalculus', 'geometry']"
4674582,Global existence of solution to first order complex-valued differential equation,"Consider a trivial fiber bundle $\mathbb{R} \times M \rightarrow M$ with a fundamental vector field $\xi$ running up the fibers. I would like to study the pair of differential equations $$\mathcal{L}_{\xi} f_1 = F(x,f_1,f_2) $$ $$\mathcal{L}_{\xi} f_2 = G(x,f_1,f_2) $$ Here, $x$ are coordinates on the total space $\mathbb{R} \times M$ , and assume that everything here is real. Note that $F,G$ are not in general homogeneous functions of $f_1$ or $f_2$ . Under what conditions do global solutions $f_1,f_2$ exist? My first instinct was Picard's theorem, but as far as I know, that doesn't necessarily apply unless I can decouple $f_1$ and $f_2$ . After some googling I came upon the Cauchy–Kowalevski theorem, however that does not seem to imply global existence, which is necessary for my application. Is this problem too general to say anything about, or are there known results for such differential equations?","['ordinary-differential-equations', 'differential-geometry']"
