question_id,title,body,tags
3047309,Catalan numbers: why is there a division by $(n+1)$,"There are many interpretations of the Catalan numbers. The one I relate to most readily is the number of paths from the bottom-left to the top-right of an $n \times n$ grid which don't cross the main diagonal. The number of ways to do this is: $$C_n = \frac{2n \choose n}{n+1} = {2n \choose n} - {2n \choose n-1}$$ The second expression in the equation above has a very neat, intuitive explanation which is described in the answer by @Marcus M here which involves mapping the undesirable paths (that do cross the main diagonal) to a smaller grid. That explanation, I understand. However, there must also be a way to directly interpret the first expression. The total paths in the grid are $2n \choose n$ . and the number of paths that don't cross the main diagonal are a very specific fraction of the total paths: $\frac{2n \choose n}{n+1}$ . What is the intuitive reason that for every $n+1$ paths in the grid, one of them doesn't cross the main diagonal? When I ask this question to people who haven't heard of it before, their first instinct is to say half the paths should not cross the main diagonal. I know one of the things that makes them way off is that towards the end of the path, the probability that you'll cross the main diagonal becomes quite high. Is there a more concrete line of attack?","['combinations', 'catalan-numbers', 'combinatorics', 'sequences-and-series']"
3047323,weak* convergence of convolution between mollifiers and Radon measure,"I've got a question concerning mollifiers. If $\Omega \subset \mathbb{R}^N$ is open and $\mu = (\mu_1,..., \mu_m)$ is a Radon measure in $\Omega$ . Let $(\rho_{\epsilon})_{\epsilon > 0}$ be a family of mollifiers. Why does $\mu_{\epsilon} := \mu * \rho_{\epsilon} \mathcal{L}^N$ locally weakly* converge in $\Omega$ to $\mu$ as $\epsilon$ goes to zero? I tried it using Fubini, but couldn't really see how it works out? Whereas a sequence of Radon measures $(\mu_h)_{h \in \mathbb{N}}$ is called weak* convergent if there exists a Radon measure $\mu$ such that for all $u \in C_0(\Omega)$ \begin{equation}
 \lim\limits_{n \rightarrow \infty}{\int_{\Omega}} u d\mu_h = \int_{\Omega} u d\mu
\end{equation} holds. The convolution of a measure $\mu$ and a continiuos function $f$ is defined as \begin{equation}
\mu * f (x) := \int_{\Omega} f(x-y) d\mu(y)
\end{equation} Thanks in advance!","['measure-theory', 'weak-convergence', 'convolution', 'geometric-measure-theory', 'real-analysis']"
3047338,Combinatorics problem (deck of cards),"I'd be extremely thankful if someone could help me with this. A company makes decks of $50$ cards. There are $40$ ""regular"" cards, that always come in a certain order, and $10$ ""special"" cards that don't have a particular order and also they are interspersed between the deck. (Note the $40$ regular cards don't always come one after the other, they just always follow the same order) In how many different combinations could the deck come? My thinking (which apparently is wrong) was to say well, I'll choose $10$ out of the $50$ possible places and multiply that by $10!$ which is the number of ways I can shuffle the special cards. So $$
\binom{50}{10} \cdot 10! .
$$ And that would be it since the regular cards will always be positioned in a certain order. 
But the answer key says $$\binom{n+k-1}{n} = \binom{49}{10}\cdot 10! = \frac{49!}{39!}.$$ Any clue why instead of picking $10$ out of $50$ it picks $10$ out of $49$ ? Thanks.",['combinatorics']
3047373,Line Of Best Fit With Perpendicular Error,"The standard statistical formula for the least squares error gives us a line that minimises the sum of the vertical distances of the sample points to the line. Suppose that I wanted to find the equation of a line that minimises the sum of the perpendicular distance of the points to the line, is there a way of analytically solving this problem?","['regression', 'statistics', 'geometry']"
3047386,How long does it take for this sequence to obtain this loop?,"For positive integers $m,n$ , define a sequence $S_m(n)$ so that $S_m(1)=m$ , $S_m(n+1)=S_m(n)^2-1$ if $S_m(n)$ is prime, and $S_m(n+1)$ is the greatest prime factor of $S_m(n)$ otherwise. It is clear that, regardless of $m$ , this sequence always gets caught in the infinite loop $2,3,8,2,3,\dots$ , because for any prime $p$ , $p^2-1=(p+1)(p-1)$ and if $p\neq 2$ then the greatest prime factor of this term is at most $\lceil p/2\rceil<p$ . What is less clear, is the rate at which the sequence becomes stuck into that loop. If we define $t(m)$ to be the smallest integer $n$ so that $S_m(n)=2$ , then how does $t(m)$ grow? By some numerical testing I've found that $t(m)<20$ for all $m<10000$ , which seems to suggest a logarithmic growth speed. In particular, for any positive integer $N$ , is it always true that there exists an $m$ so that $t(m)>N$ ? Any thoughts are appreciated!","['elementary-number-theory', 'sequences-and-series']"
3047388,Divisor of $x^2+x+1$ can be square number?,"$$1^2+1+1=3$$ $$2^2+2+1=7$$ $$8^2+8+1=73$$ $$10^2+10+1=111=3\cdot37$$ There is no divisor which is square number.
Is it just coincidence? Or can be proved? *I'm not english user, so my grammer might be wrong","['divisibility', 'number-theory', 'elementary-number-theory', 'diophantine-equations', 'square-numbers']"
3047389,Can exact square roots not be found?,"I'm brushing up on some higher level maths for a programming project. I was going along and then I realized that I have absolutely no idea how square roots can be computed. I mean sure, I've memorized a lot of perfect squares. But I wouldn't be able to get an exact answer from some arbitrary number like 432,434, would I? I Googled how to calculate square roots and everything always points to it basically being based on algorithms which have a degree of error because they're more or less guesses. I can't seem to find out why it's impossible to get an exact square root though. Like why can't you plug in $x$ to a function $f(x)$ and get the exact square root? Very curious about this.","['programming', 'calculus', 'algebra-precalculus', 'square-numbers']"
3047391,Can a function be neither convex nor concave everywhere?,"For a simpliest example, define continuous $f:\mathbb R\to\mathbb R$ to be locally convex in neighborhood $U\subset\mathbb R$ if $\{y>f(x)|x\in U\}$ is a convex set. $f:\mathbb R\to\mathbb R$ to be locally concave in $U\subset\mathbb R$ if $\{y<f(x)|x\in U\}$ is a convex set. Say $f$ is neither convex nor concave everywhere if $f$ is neither locally concave nor convex in all neighborhood set $U\subset\mathbb R$ and $|U|>3$ . Is it possible? (I guess yes) Say $f$ is neither convex nor concave everywhere if $f$ is neither locally concave nor convex in all measure non zero  neighborhood set $U\subset\mathbb R$ . Is it possible? (I guess no)","['continuity', 'functions', 'convex-analysis', 'real-analysis']"
3047396,"Find $p>1$ that ${\int\limits^p_1}\frac{1}{x}\,\mathrm{d}x={\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x$","Find $p>1$ that $${\displaystyle\int\limits^p_1}\dfrac{1}{x}\,\mathrm{d}x={\displaystyle\int\limits^p_1}\ln\left(x\right)\,\mathrm{d}x$$ \begin{align*}
    &{\displaystyle\int}\dfrac{1}{x}\,\mathrm{d}x=\ln\left(\mid x \mid \right) && \vert \ \text{general integral}
\end{align*} $F_1(x)=\ln\left({\mid x \mid} \right)+C$ \begin{align*}
    &{\displaystyle\int}1\cdot\ln\left(x\right )\,\mathrm{d}x && \vert \ 2. \text{ with } f'=1, g=\ln(x)\\
    &=x\ln\left(x\right)-{\displaystyle\int}1\,\mathrm{d}x\\
    &=x\ln\left(x\right)-x
\end{align*} $F_2(x)=x\ln\left(x\right)-x+C$ \begin{align*}
    \left[\ln\left(\mid{x}\mid\right)+C\right]^p_1&=\left[\ln({\mid p \mid})+C\right]-\left[\ln({\mid 1 \mid})+C\right]\\
    &=\ln\left(p\right)
\end{align*} \begin{align*}
    &\left[x\ln\left(x \right)-x+C\right]^p_1=\left[p\ln\left(p \right)-p+C\right]-\left[1\ln\left(1\right)-1+C\right]
\end{align*} It was already mentioned, that I had a typo. I corrected everything and contributed my own solution.","['calculus', 'definite-integrals', 'analysis']"
3047416,Example of a maximum likelihood estimator that is not a sufficient statistic,"I am currently researching on providing some bounds on estimation using some information theoretic tools (I won't expend on that here for now, I may make a post about it later) and turns out that given a phenomenon $X$ , an observation $Y$ , then $\hat{x}(Y)$ , the maximum likelihood estimator of $X$ based on $Y$ , may apparently not be a sufficient statistic and this is a something I would like to study, the answer to this post states that such an example exists when $Y$ consists of samples that are not i.i.d but fails to provide such an example and I haven't found anything about it. Have anyone seen something of the sort ?","['statistics', 'sufficient-statistics', 'maximum-likelihood', 'estimation']"
3047436,Reason why addition and multiplication are both required - unlike boolean algebra where NOR is enough?,"Apologies for the simplicity of this question. In Boolean Algebra the introduction of the function {NOR} is sufficient to create, with suitable finite combinations of this function, all possible sets of states for a set of n two state variables (so all n placed functions of two state variables can be expressed as finite combinations of NOR). There are similar combinations of two or one place boolean functions that achieve this objective e.g. it could also be any of {OR, NOT}, { $\Rightarrow$ , NOT}, {AND, NOT}, {NAND}. The question is : Is there a similar 'high level' objective behind providing both '+' and ' $\times$ ' two place functions in arithmetic, and is there any reasoning which shows why '+' and ' $\times$ ' are both required to meet the objective and whether there are any other function combinations that meet the objective ?","['boolean-algebra', 'elementary-number-theory', 'logic', 'abstract-algebra', 'elementary-set-theory']"
3047513,Spanning subgraph with all degrees odd,"In a country, there are 100 towns. Some pairs of towns are joined
by roads. The roads do not intersect one another except meeting
at towns. It is possible to go from any town to any other town by
road. Prove that it is possible to pave some of the roads so that
the number of paved roads at each town is odd. Clearly we have here a connected simple graph with no loops and with $100$ nodes.
Say we have $2n$ nodes instead of $100$ nodes and prove the statement with induction. It is enough to observe only (spanned) trees. Clearly this work if $n=1$ . Both nodes have a degree $1$ (since it is connected). Suppose the statement is true for all $k\leq n$ and we prove the statement for $n+1$ . So we have a node $A$ with degree $1$ and let $B$ be it neighbour. Color this edge $AB$ red and delete all other edges from $B$ and observe the rest of the graph (without $A$ and $B$ also). This graph is a forest so in each component we can color edges red so that each node has odd red edges (by induction hypothetis). So we are done, just pave all red edges. Edit: As bof mentioned in comment this is not correct. Any idea how to solve it?","['contest-math', 'trees', 'graph-theory', 'solution-verification', 'combinatorics']"
3047540,"What is the order-type of the set of natural numbers, when written in alphabetical order?","We are all familiar with the standard nomenclature for the smallish
natural numbers, such as one, two, three, ..., one hundred, one hundred one, ..., fifteen
   thousand two hundred forty-nine. I have in mind the simple American number naming
conventions ,
together with the names for large
numbers . ( Update Names of large numbers seems to be more thorough. Note to Wikipedians: should probably merge those two pages somehow.) Preliminary question. Is there a sensible naming system that
provides a canonical name for every natural number? That is, I want a naming system that extends the current naming
system sensibly in such a way that every number gets a unique name. Please provide a system and explain why it is sensible. For example, if there were some natural way to extend the Latin naming convention indefinitely, that would be great. Let me assume that some of you will be able to provide such a
naming system. Main Question. What is the order-type of the set of natural
numbers, when written in alphabetical order? For example, the order will not be the same as the order $\omega$ of the natural number themselves, since presumably there will be
infinitely many numbers starting with ""o"", as in one hundred, one
million, one thousand, and so on, and these will all be
alphabetically preceding two hundred, two million, two thousand and
so on. So the order type will probably be related naturally $L\times 26$ for some order $L$ , or actually, less than $26$ , since probably not
every letter will be a legitimate first letter of a number name. It is conceivable that the order type will depend on syntactic features of the naming convention. Here is a part of the order, for numbers up to 100: (from hervé
graumann
1988 ) 1) eight

2) eighteen

3) eighty

4) eighty-eight

5) eighty-five

6) eighty-four

7) eighty-nine

8) eighty-one

9) eighty-seven

10) eighty-six

11) eighty-three

12) eighty-two

13) eleven

14) fifteen

15) fifty

16) fifty-eight

17) fifty-five

18) fifty-four

19) fifty-nine

20) fifty-one

21) fifty-seven

22) fifty-six

23) fifty-three

24) fifty-two

25) five

26) forty

27) forty-eight

28) forty-five

29) forty-four

30) forty-nine

31) forty-one

32) forty-seven

33) forty-six

34) forty-three

35) forty-two

36) four

37) fourteen

38) hundred

39) nine

40) nineteen

41) ninety

42) ninety-eight

43) ninety-five

44) ninety-four

45) ninety-nine

46) ninety-one

47) ninety-seven

48) ninety-six

49) ninety-three

50) ninety-two

51) one

52) seven

53) seventeen

54) seventy

55) seventy-eight

56) seventy-five

57) seventy-four

58) seventy-nine

59) seventy-one

60) seventy-seven

61) seventy-six

62) seventy-three

63) seventy-two

64) six

65) sixteen

66) sixty

67) sixty-eight

68) sixty-five

69) sixty-four

70) sixty-nine

71) sixty-one

72) sixty-seven

73) sixty-six

74) sixty-three

75) sixty-two

76) ten

77) thirteen

78) thirty

79) thirty-eight

80) thirty-five

81) thirty-four

82) thirty-nine

83) thirty-one

84) thirty-seven

85) thirty-six

86) thirty-three

87) thirty-two

88) three

89) twelve

90) twenty

91) twenty-eight

92) twenty-five

93) twenty-four

94) twenty-nine

95) twenty-one

96) twenty-seven

97) twenty-six

98) twenty-three

99) twenty-two

100) two

101) zero Let me add that I don't necessarily expect that the order is a well-order. For example, if we have a naming convention whereby $10^k$ is represented for large $k$ simply by repeating ""penpenpenpen $\cdots$ pen"", then we could make a descending sequence via penpenpenpen $\cdots$ pen twelve, which would descend as the number of pen's increased, since we would be replacing t with p.","['puzzle', 'elementary-number-theory', 'logic', 'order-theory', 'elementary-set-theory']"
3047554,Set that doesn't shatter certain subsets,"Let $A\subset \mathcal{P}(\{1, \dots, n\}) $ and $ B \subset \{1, \dots, n \}$ We say $A$ shatters $B$ if $\forall y \subset B, \exists x \in A$ such that $x \cap B = y$ . I am asked to show that if $A$ does not shatter the sets: $\{1,2,3\},\{2,3,4\},\ \dots, \{n-2,n-1,n,\}, \{n-1,n,1\}, \{n,1,2\}$ and $n$ is a multiple of $3$ then $|A| \leq 7^{\frac{n}{3}}$ My current thinking is that, for each of these $3$ -sets, we have to miss at least one of their subsets. Specifically, for each $a \subset \{x,y,z\}$ there are $2^{n-3}$ subsets of $\{1,\dots,n\}$ that intersect with $\{x,y,z\}$ to give $a$ . (Call the set of there $2^{n-3}$ subsets $C_{\{x,y,z\}}(a)$ ) Hence if $A$ does not shatter $\{1,2,3\}$ because we are missing $a$ , then $A$ cannot contain these $2^{n-3}$ subsets. I want to say that there is some subset $B \subset \mathcal{P}(\{1,\dots,n\}$ of size $8* 7^{\frac{n}{3}}$ such that we must have $A \subset B$ , and we may only have at most $\frac{7}{8}$ of the elements of $B$ . I suspect we have something like: $B = \bigcup_{\{x,y,z\} \text{ mentioned earlier}}\bigcup_{a \subset \{x,y,z\}} C_{\{x,y,z\}}(a)$ However, at this point I am stuck and I'm not sure how to proceed. I can't think of a nice way to count the size of $B$ and show it is what I want because I can't think of an easy way to account for all of the overlaps occurring.","['elementary-set-theory', 'combinatorics']"
3047567,Prove $\sum_{k=m}^n{k\choose k-m}{2n\choose 2k}=4^{n-m}\frac{n(2n-m-1)!}{(2n-2m)!m!}.$,"For $n\in\mathbb{Z}_{\ge 1}$ and $m\in\{0,1,\ldots,n\}$ I'd like to prove $$\sum_{k=m}^n{k\choose k-m}{2n\choose 2k}=4^{n-m}\frac{n(2n-m-1)!}{(2n-2m)!m!}.$$ Though I've confirmed the identity for all $n\le 20$ , sadly I don't have a proof.","['summation', 'binomial-coefficients', 'combinatorics']"
3047584,Drawing balls with a finite number of replacement,"I have to solve this problem: ""Suppose a box contains $18$ balls numbered $1–6$ , three balls with each number. When $4$ balls are drawn without replacement, how many outcomes are possible?"". (The order does not matter). I can't find a simple formula for it.
I've tried in this way and I don't know if it is right way: A random outcome could or could not have the number $1$ .
If it has it, the outcome could be $111$ plus a number $2\le n \le 6$ , or $11$ plus two numbers or $1$ plus three numbers. In the first case we have a total of ${{5}\choose{1}} = 5 $ outcomes. In the second case we have a total of ${{5}\choose{2}} + 5 = 15$ outcomes. In the last case we have a total of ${{5}\choose{3}} + 5 +5\times 4 = 35 $ outcomes. Finally, if the outcomes does not have the number 1 we have a total of $ {{5}\choose{4}} + 5\times(4\times 3 + 4) + 5\times 4 + 5 = 110$ . So there are 165 possible outcomes. Is it right? If yes, there is a simpler and much more elegant way to prove it? Thanks","['permutations', 'combinations', 'combinatorics', 'probability']"
3047598,Bochner integrability of mappings of Bochner integrable functions,"Suppose I have a Bochner integrable function, $t\mapsto u(t)\in X$ , with $X$ a separable Banach space, and $0\leq t\leq T<\infty$ .  If I introduce a mapping $f:[0,T]\times X\to X$ , under what assumptions will $t\mapsto f(t,u(t))$ be Bochner integrable?  Is joint measurabiity of $f$ sufficient?  Continuity?","['measure-theory', 'lebesgue-measure', 'functional-analysis']"
3047636,Prove the inequality $\ln {(1+\frac{1}{x})}> \frac{2}{2x+1}$,"Prove the inequality $$\ln {(1+\frac{1}{x})}> \frac{2}{2x+1}$$ for $x>0$ . My attempt: Let $$f(x)=\ln {(1+\frac{1}{x})}-\frac{2}{2x+1}$$ Then $$f'(x)=-\frac{1}{x(x+1)}+\frac{4}{(2x+1)^2}$$ $$f''(x)=\frac{1}{x^2}-\frac{1}{(x+1)^2}-\frac{8}{(2x+1)^3}>0$$ Then the function $f$ is convex. There exists a minimal point $x_0$ such that $f(x)\geq f(x_0)$ . However, there's no critical point $x_0$ such that $f'(x_0)=0$ , and $\lim_{x \rightarrow \infty} \sup {f'(x)}=0$ . Then I want to show that $f(x)>0$ , how do I continue my proof? I have been trying another approach using Cauchy's MVT by letting $$f(x)=\ln {x}$$ $$g(x)=\frac{1}{2x+1}$$ such that $$\frac{f(x+1)-f(x)}{g(x+1)-g(x)}=\frac{f'(c)}{g'(c)}$$ where $c \in (x,x+1)$ but failed. 
As what I did is $$\ln {(1+\frac{1}{x})}=\frac{1}{c} \cdot \frac{(2c+1)^2}{2} \cdot \frac {2}{(2x+1)(2x+3)}$$ I can't simply do the inequality $$\frac{1}{c} \cdot \frac{(2c+1)^2}{2}>\frac{1}{x} \cdot \frac{(2x+1)^2}{2}$$ as $c>x$ because $\frac{1}{c} < \frac{1}{x}$ but $\frac{(2c+1)^2}{2} > \frac{(2x+1)^2}{2}$ . Edited: Of course, I know that $$\ln {(1+ \frac{1}{x})}>\frac{x}{1+x}$$ for $x>-1$ . I just need to prove that $$\frac{x}{x+1}>\frac{2}{2x+1}$$ But I hope to find out another approach using calculus method.","['proof-verification', 'derivatives']"
3047646,$p$-adic values of rational points on elliptic curves,"The following question came up naturally whilst studying diophantine equations: given an elliptic curve $E$ of the form $Y^2 + aY = X^3 +bX^2 + cX + d$ defined over $\mathbb{Q}$ , consider the subset $C \subseteq \mathbb{Q}$ of numbers which appear as either the first or the second coordinate of a rational point on $E$ . If we assume that $E$ has infinitely many points then $C$ is infinite. I would like to understand how 'large' $C$ can get, in particular: can we choose $E$ such that $C$ has unbounded $p$ -adic value for all prime numbers $p$ ? Maybe we can at least choose $E$ such that $C$ has unbounded $p$ -adic value for all primes in a given finite set of prime numbers? I know almost nothing about the topic, so any pointers you might have to articles or books studying the set $C$ would be very helpful.","['algebraic-curves', 'number-theory', 'elliptic-curves']"
3047650,Support of a quasi-coherent sheaf on an affine scheme,"Let $A$ be a commutative ring, $X = \text{Spec }A$ , and let $M$ be a $A$ -module. The $\mathcal{F} = \tilde M$ is a sheaf on $X$ . Exercise II.5.6 in Hartshorne's Algebraic Geometry states that if $A$ is noetherian and $M$ is finitely generated, then $\text{Supp } \mathcal{F} = V(\text{Ann} M)$ . I proved this, but it feels suspicious that I did not use that $A$ should be noetherian. This is my proof: Let $m_1,\dotsc,m_n$ be generators of $M$ . Then $\text{Ann }M = \bigcap _i\text{Ann } m_i$ . Also the $m_i$ generate the stalks of $\mathcal{F}$ , so $\mathcal{F}_x = 0$ if and only if $(m_i)_x = 0$ for all $i = 1,\dotsc, n$ . Hence $\text{Supp }\mathcal{F} = \bigcup_i \text{Supp }m_i = \bigcup_i V(\text{Ann } m_i) = V(\bigcap_i \text{Ann }m_i) = V(\text{Ann }M).$ Did I make any mistake, or can the noetherian hypothesis be omitted here?","['algebraic-geometry', 'commutative-algebra', 'sheaf-theory']"
3047706,Vector space of a Function (Example for understanding the concept),"In my textbook is stated: 
Let G be a finite-dimensional vector space of real functions in $R^D$ .
What is meant by "" vector space of real functions ""? I know what a vector space is, by I don't get how can real functions form vector space (The only vector spaces that I might see regarding a function are the vector space of the domain and codomain) Please, if you are aware, provide me a tangible and intuitive example with the explanation, as I find examples extremely useful for understanding.","['functions', 'vector-spaces']"
3047789,Test the convergence of series $\sum \frac{a^n}{a^n+x^n}$ when $x\neq0$,"Let $ t_n=\frac{a^n}{a^n+x^n}$ and using root test $$ \displaystyle \lim_{n \to \infty} {t_n}^{\frac{1}{n}}=  \lim_{n \to \infty} \left(\frac{a^n}{a^n+x^n}\right)^{\frac{1}{n}}= \lim_{n \to \infty} \frac{1}{\big(1+(\frac{x}{a})^n\big)^{\frac{1}{n}}}$$ Now I am stuck, I don't know how to evaluate this limit","['calculus', 'sequences-and-series', 'real-analysis']"
3047830,Number of homomorphisms between two arbitrary groups [duplicate],"This question already has an answer here : Strategy for determining the number of homomorphisms between two Groups (1 answer) Closed last year . How many homomorphisms are there from $A_5$ to $S_4$ ? This is how I tried to solve it. If there is a homomorphism from $A_5$ to $S_4$ , then order of element of $S_4$ should divide the order of its preimage. Now what are the possible order of elements in $S_4$ .1,2,3 and 4. Since $A_5$ contains (12345), which is of order 5.. what could be image of (12345). Definitely Identity element which is of order 1. Similarly all 5 cycles must be mapped to identity.  There are 24 elements of 5 cycles. 24 elements out of 60 are mapped to identity .. now only two types of homomorphisms are possible. either 30:1mapping  or 60:1 mapping.  Consider (12)(34) which belongs to $A_5$ . It's image can be element of order 2 or identity.there are 15 elements of order 2 . suppose these 15 elements are mapped to some element 'g' of order 2 of $S_4$ , you need another 15 elements to get mapped to 'g' to have 30 :1 mapping. Other type of elements left in $A_5$ is of order 3. None of them can be mapped to g. hence 15 elements of order 2 should be mapped to identity .. so ,  (24+15=39) elements mapped to identity.As mentioned earlier it should be 30 :1 or 60:1 mapping. So it must be 60:1 mapping.Hence a trivial homomorphism. Answer is 1. I wanted to know is there any other technique which can be used to find number of homomorphism in the above question ?
In general, how to find number of homomorphism  between any two arbitrary groups ?","['group-homomorphism', 'permutation-cycles', 'finite-groups', 'abstract-algebra', 'group-theory']"
3047845,"If $f: \mathbb{R} \to \mathbb{R}$ is a continuous surjection, must it be open?","If $f: \mathbb{R} \to \mathbb{R}$ is a continuous surjection, must it be open? I think not. I proved if $f: \mathbb{R} \to \mathbb{R}$ is an open continuous surjection, then $f$ is a homeomorphism. So, if the question is true, every continuous surjection must be a homeomorphism. But, I didn't find a counterexample. Can someone help me?","['general-topology', 'metric-spaces', 'real-analysis']"
3047854,On the logarithm of the fractional part Integral,"Let $\{\}$ denote the fractional part function, then does the following integral admit a closed-form ? $$\int_{0}^{1}x\ln\bigg(\bigg\{\frac{1}{x}\bigg\}\bigg)dx$$","['integration', 'fractional-part', 'calculus', 'zeta-functions']"
3047932,Which matrices commute with $\operatorname{SO}_n$?,"$\newcommand{\GLp}{\operatorname{GL}_n^+}$ $\newcommand{\SO}{\operatorname{SO}_n}$ Let $n>2$ , and Let $A \in \GLp$ be an invertible real $n \times n$ matrix, which commutes with $\SO$ . Is it true that $A= \lambda Id$ for some $\lambda \in \mathbb{R}$ ? An equivalent requirement is that $A$ commutes with every skew-symmetric matrix. One direction is obtained by differentiating a path of orthogonal matrices starting at the identity. The converse implication comes from the fact that every element of $\SO$ equals to $\exp(M)$ for some skew-symmetric $M$ . Note that if we assume that $A \in \SO$ , then the answer is positive : we must have $A=\pm Id$ .","['orthogonal-matrices', 'group-theory', 'symmetry', 'lie-groups']"
3047957,Two Variable Function with Specific Properties (Challenging),"I am looking for a two variable function (a surface), $g(x,b)$ , with the following properties: $$g:\ [0,\pi] \times (0,\pi) \to[0,\pi] $$ $$g(0,b)=0$$ $$g(\pi,b)=\pi$$ $$g(b,b)=\frac{\pi}{2}$$ $$g\left( x,\frac{\pi}{2}\right)=x$$ Where $x\in[0,\pi]$ and $b\in(0,\pi)$ . Can you define such a $g$ ?  I have been looking for a while and am unable to find one.  If you cannot define one, does such a $g$ even exist?","['multivariable-calculus', 'calculus', 'functions', 'surfaces']"
3047976,Intuition Wanted: Why Define Integrals Component-Wise,"In our analysis course, we just defined the following: Let $g := (g_1, \ldots, g_n) \colon [a, b] \to \mathbb{R}^n$ , where $g_1, \ldots, g_n \colon [a,b] \to \mathbb{R}$ are integrable.
Then we call the integral of $g$ over $[a,b]$ \begin{equation*}
\int_{a}^{b} g(t) \ \text{dt}
:= \begin{pmatrix}
\int_{a}^{b} g_1(t) \ \text{dt} \\
\vdots \\
\int_{a}^{b} g_n(t) \ \text{dt}
\end{pmatrix}.
\end{equation*} I came across this definition again at the beginning of measure theory, when we stated: We ultimately want to integration functions $\mathbb{R}^n \to \mathbb{R}^m$ , but because of the above definition we can, without loss of generality, restrict ourselves to the case $m = 1$ . My Question What is the intuition behind this definition, why does it make sense, if you will, ''on a deeper level''?","['integration', 'measure-theory', 'big-picture', 'intuition']"
3047992,Existence of subgroup of order power of prime in a finite abelian group?,Say I have a finite abelian group $G$ such that $\left | G \right |=p_1^{n_1}...p_m^{n_m}$ where the $p_i$ 's are distinct. Can I say that $G$ must have a subgroup of order $p_1^{n_1}$ ? I'm thinking that I can use the structure theorem to write $G$ as $\mathbb{Z}/p_1^{n_{1_{1}}}\times ...\times \mathbb{Z}/p_1^{n_{1_{s_{1}}}}\times...\times\mathbb{Z}/p_m^{n_{m_{1}}}\times ...\times \mathbb{Z}/p_m^{n_{m_{s_{m}}}}$ then just take $\mathbb{Z}/p_1^{n_{1_{1}}}\times ...\times \mathbb{Z}/p_1^{n_{1_{s_{1}}}}\times\left \{ 1 \right \}...\times\left \{ 1 \right \}$ where $\sum_i n_{1_{i}}=n_1$ . Is my reasoning correct? I would like to avoid using Sylow theorems. Thanks in advance.,"['abelian-groups', 'group-theory', 'finite-groups']"
3047998,Entropy Lower Bound in Terms of $\ell_2$ norm,"Define $$
\begin{align}
H(p_1, \dots, p_n) &= \sum_{i=1}^n p_i\log1/p_i\\
&=\log n+\sum_{i=1}^n\sum_{k=2}^\infty (-1)^{k + 1} n^{k - 1} \frac{(p_i - 1/n)^k}{k (k - 1)},
\end{align}
$$ where $p_1,\dots,p_n\ge0$ sum to $1$ . Then we have the classic inequality $H(p_1,p_2)\ge(\log2)(1-2((p_1-1/2)^2+2(p_2-1/2)^2))=(\log 2)(1-2\|p-1/2\|^2)$ , and we might wonder if that could be extended for $n>2$ .
In particular with something like $$\begin{align}
H(p_1,\dots,p_n)&\ge(\log n)(1-c_n\|p-1/n\|^2_2).
\end{align}$$ From experiments with $n=3$ , it seems like $c_n\ge\frac{2 n (\log n/2)}{(n-2) \log n}=2(1-O(1/\log n))$ suffices, but I don't have a proof of this. It is also slightly inconvenient that it can go below $0$ , something that wasn't the case with the $n=2$ case. Bounding the terms individually, we can get $H(p_1,\dots,p_n)\ge-2+4\sum_{i=1}^n\frac{p_i}{1+p_i}$ , which is non-negative, but not as relatable to the $\ell_2$ norm. We can also bound $H\ge n/4-\|p-1/2\|_2^2$ , but somehow bounding centered in $1/n$ seems more natural. Is there a well known lower bound like this, relating $H(p)$ with $\|p\|_2$ ? Ideally, one that is asymptotically tight at $p_1=\dots=p_n=1/n$ and is always positive.","['summation', 'entropy', 'information-theory', 'real-analysis', 'inequality']"
3048052,Prove that $(1-p^n)^m + (1 - q^m)^n \geq 1$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 5 years ago . Improve this question Prove that $(1-p^n)^m + (1 - q^m)^n \geq 1$ for positive integers $m,n$ and $p,q \in (0,1)$ such that $p+q=1$ . The idea is that $1-p^n$ may be interpreted as the probability of $n$ failures in $n$ Bernoulli trials with the probability of success $p$ . Similarly, $1-q^m$ is the probability of $m$ failures in $m$ Bernoulli trials with the probability of success $q$ . Essentially, since $p+q = 1$ you may think as having one coin with a bias $p$ . But the crucial trick is contained in the answer below, pointed out by @william122","['probability-theory', 'probability']"
3048063,What is the next step in the prove? (Mathematical Induction) $\left(x^{n}+1\right)<\left(x+1\right)^{n}$,"I have to prove this preposition by mathematical induction: $$\left(x^n+1\right)<\left(x+1\right)^n \quad \forall n\geq 2 \quad \text{and}\quad x>0,\,\, n \in \mathbb{N}$$ I started the prove with $n=2$ : $\left(x^{2}+1\right)<\left(x+1\right)^{2}$ $x^{2}+1<x^{2}+2x+1$ We see that; $x^{2}+1-x^{2}-1<2x$ $0<2x$ Then $x>0$ And this one carries out for $n=2$ Now for $\quad n=k \quad$ (Hypothesis) $\left(x^{k}+1\right)<\left(x+1\right)^{k}$ We have $\displaystyle x^{k}<\left(x+1\right)^{k}-1\ldots \quad (1)$ Then, we must prove for $\quad n= k+1 \quad$ (Thesis): $x^{k+1}+1<\left(x+1\right)^{k+1}$ We develop before expression as: $x^{k+1}<\left(x+1\right)^{k+1}-1\ldots \quad (2)$ According to the steps of mathematical induction, the next stpe would be  use the hypothesis $(1)$ to prove thesis $(2)$ . It's in here when I hesitate if the next one that I am going to write is correct: First way: We multiply hypothesis $(1)$ by $\left(x+1\right)$ and we have: $x^{k}\left(x+1\right)<\left[\left(x+1\right)^{k}-1\right]\left(x+1\right)$ $x^{k}\left(x+1\right)<\left(x+1\right)^{k+1}-\left(x+1\right)$ Last expression divided by $\left(x+1\right)$ we have again the expression $(1)$ : $\displaystyle \frac{x^{k}\left(x+1\right)<\left(x+1\right)^{k+1}-\left(x+1\right)}{\left(x+1\right)}$ $x^{k}<\left(x+1\right)^{k}-1$ Second way: If we multiply $(2)$ by $x$ we have: $xx^{k}<x\left[\left(x+1\right)^{k}-1\right]$ $x^{k+1}<x\left(x+1\right)^{k}-x$ And if we again divided last expression by $x$ , we arrive at the same result $\displaystyle \frac{x^{k+1}<x\left(x+1\right)^{k}-x}{x}$ $x^{k}<\left(x+1\right)^{k}-1$ I do not find another way to prove this demonstration, another way to solve the problem is using Newton's theorem binomial coeficients, but the prove lies in the technical using of mathematical induction. If someone can help me, I will be very grateful with him/her!
Thanks
-Víctor Hugo-","['algebra-precalculus', 'induction']"
3048088,Associativty of smash product on compact spaces,"Let $Y,Z$ be compact or $X,Z$ locally compact. Then the canonical bijection 
  $$ (X \wedge Y) \wedge Z \rightarrow X \wedge (Y \wedge Z) $$ 
  is a homeomoprhism. I can prove the case when $X,Z$ are locally compact using exponential law. But I don't see how one approaches the case when $Y,Z$ compact. Hints?","['general-topology', 'homotopy-theory', 'algebraic-topology']"
3048112,"Difficult Definite Integral $\int_{0}^{\frac{\pi}{2}} \sqrt{1+2\cos^2\left(\frac{\pi}{2} - x\right)} + \sin x\, dx$","I have spent several days trying to solve this integral, but to no avail. This isn't from a textbook, but a challenge problem given to me by a professor. I am not looking for anyone to give me the solution, but just to lead me in the right direction. The problem is to compute the following integral: \begin{equation}
\int_{0}^{\frac{\pi}{2}} \sqrt{1+2\cos^2\left(\frac{\pi}{2} - x\right)} + \sin x\, dx
\end{equation} When first approaching this problem I tried to utilize the cofunction identity: \begin{equation}
\cos\left(\frac{\pi}{2}-x\right) = \sin x
\end{equation} The integral then became: \begin{equation}
\int_{0}^{\frac{\pi}{2}} \sqrt{1+2\sin^2x} + \sin x\, dx
\end{equation} I have tried several things from this point such as using the formulas \begin{equation}
\sin^2x = \frac{1}{2}[1-\cos(2x)]
\end{equation} The integral then became: \begin{equation}
\int_{0}^{\frac{\pi}{2}} \sqrt{2-\cos(2x)} + \sin x\, dx
\end{equation} The issue is I have tried several run arounds(of which I will not post each) with identities and other methods, but I seem to be hitting dead ends. Also, I want to mention that I'm trying to solve this using elementary methods only. I only have experience up to calculus II. Any constructive criticism or comments would be greatly appreciated! Thank you.","['integration', 'calculus']"
3048119,If $xR=I$ we can say that $x\in I$?,"Probably this is a dumb question, I lack knowledge on abstract algebra. Suppose that $R$ is a ring without unity and $I\subset R$ is a non-trivial ideal. If $xR=Rx=I$ for some $x\in R$ , we can says that $x\in I$ ? If not, there is some other conditions that ensures that $x\in I$ ?","['ring-theory', 'abstract-algebra', 'ideals']"
3048126,Show $f$ : integrable $\Leftrightarrow$ $ \sum_{n=1}^{\infty} \mu\left( \{ x \in E : |f(x)| \ge n \} \right) < \infty $,"Let $f : E \to [0, \infty]$ measurable, where $E$ is a finite measure space. Show : $f$ is integrable if and only if $$
\sum_{n=1}^{\infty} \mu\left( \{ x \in E : |f(x)| \ge n \} \right) < \infty
$$ Try ( $\Rightarrow$ ) Let $E_n = \{ x \in E : |f(x)| \ge n \}$ . Since $E_1 \supset E_2 \supset \cdots $ , we have $$
E = (E\setminus E_1) \sqcup (E_1 \setminus E_2) \sqcup  \cdots
$$ Therefore, $\int_E f d\mu = \int_{E\setminus E_1} f d\mu + \int_{E_1\setminus E_2} f d\mu +  \cdots$ But I cannot proceed from here. ( $\Leftarrow$ ) We have $\sum_{n=1}^{\infty} \mu\left( E_n \right) = \sum_{n=1}^{\infty} \int \chi_{E_n} d\mu$ , By Levi's convergence theorem, $\sum_{n=1}^{\infty} \chi_{E_n}$ converges a.e. in $E$ . But I'm stuck at how I can relate this to the integrability of $f$ .","['measure-theory', 'real-analysis']"
3048132,Evaluate $\int_{-\pi/4}^{\pi/4}\frac{x}{\sin x}\mathrm{d}x$,"I am working on the integral $$I=\int_{-\pi/4}^{\pi/4}\frac{x}{\sin x}\mathrm{d}x=2\int_0^{\pi/4}\frac{x}{\sin x}\mathrm{d}x$$ Which I am fairly confident has a closed form, as $$\int_{0}^{\pi/2}\frac{x}{\sin x}\mathrm{d}x=2G$$ Where $G$ is Catalan's constant. Preforming a tangent half angle substitution, we have that $$I=4\int_0^{\sqrt{2}-1}\frac{\arctan x}{x}\mathrm{d}x$$ Then using $$\arctan x=\sum_{n\geq0}(-1)^n\frac{x^{2n+1}}{2n+1}$$ We have $$I=4\sum_{n\geq0}\frac{(-1)^n}{(2n+1)^2}(\sqrt{2}-1)^{2n+1}$$ Which is painfully similar to $G$ . I do not know how to deal with that extra $(\sqrt{2}-1)^{2n+1}$ bit though... In another post of mine I showed that $$I=\pi\sum_{n\geq1} n\log\bigg(\frac{4n+1}{4n-1}\bigg)\prod_{k\geq1\\k\neq n}\frac{k^2}{k^2-n^2}$$ And similarly I showed that $$\sum_{n\geq1}n\log\bigg(\frac{2n+1}{2n-1}\bigg)\prod_{k\geq1\\k\neq n}\frac{k^2}{k^2-n^2}=\frac{4G}\pi$$ So I have to questions. How do I find an exact value for $I$ ? And are the last two series representations correct? Thanks. Major Edit: Okay so I found a closed form for the integral. Wolfy gave me $$\int\frac{x}{\sin x}\mathrm{d}x=i\bigg(\text{Li}_2(-e^{ix})-\text{Li}_2(e^{ix})\bigg)+x\log\frac{1-e^{ix}}{1+e^{ix}}$$ I guess that Wolfy didn't want to do the algebra, so I did it by hand. It took me like $10$ minutes, but I am pretty sure that $$I=-\frac34G+\frac{\pi^2}4\bigg(\frac{13}{24}-i\bigg)-\frac{i\pi}4\log(1+\sqrt{2})+\frac{i-1}{32\sqrt{2}}\bigg[\psi^{(1)}\bigg(\frac{5}{8}\bigg)-\psi^{(1)}\bigg(\frac{1}{8}\bigg)\bigg]+\frac{i+1}{32\sqrt{2}}\bigg[\psi^{(1)}\bigg(\frac{3}{8}\bigg)-\psi^{(1)}\bigg(\frac{7}{8}\bigg)\bigg]$$ Where $\psi^{(1)}$ is the first derivative of the di-gamma function.","['integration', 'definite-integrals', 'closed-form', 'sequences-and-series', 'catalans-constant']"
3048185,Expected value and projection of a normal random variable onto a linear span...?,"I just wanted to clarify a part of a proof which used the fact a random variable has zero mean. Suppose $X, Z_{s_1},\dots,Z_{s_n}$ are all jointly normal random variables for all $s_i \leq t$ and $n \geq 1$ . Define $\mathcal{L}(Z,t)$ as set of random variables of the form $$c_1 Z_{s_1}+ \dots + c_n Z_{s_n}$$ for $c_i \in \mathbb{R}$ and $s_i \leq t$ , $n \geq 1$ , along with all their limit points (in $L^2(P)$ ). Define $\tilde{X} = X - \mathcal{P}_L (X)$ where $\mathcal{P}_L$ is the projection onto the space $\mathcal{L}(Z,t)$ . Then the proof use $\mathbb{E}[\tilde{X}] = 0$ . My understanding of this, is that the projection $\mathcal{P}_L (X)$ coincides with the conditional expectation $$\mathcal{P}_L (X) = E[X|\mathcal{F}_L]$$ where $\mathcal{F}_L$ is the $\sigma$ -field generated by the random variables in $\mathcal{L}(Z,t)$ . And hence by law of total expectation we have $$\mathbb{E}[\tilde{X}] = \mathbb{E}[X] - \mathbb{E}[E[X|\mathcal{F}_L]] = \mathbb{E}[X]-\mathbb{E}[X] =0$$ Is this correct? The proof doesn't really explain, and the relationship between projection and probability spaces is still a bit confusing to me.","['probability-theory', 'projective-geometry', 'geometry']"
3048203,"Invent Binary Operator $*$ on Reals that Can Create $+$, $-$, $\times$, $\div$ [duplicate]","This question already has answers here : Is there an numeric arithmetic with a single operator? (3 answers) Closed 5 years ago . Exact Question: Invent a single binary operator $*$ such that for every real numbers $a$ and $b$ , the operations $a + b$ , $a - b$ , $a \times b$ , $a \div b$ can be created by applying $*$ (multiple times), starting with only $a$ 's and $b$ 's From my interpretation, you can apply $*$ recursively some number of times with carefully selected parameters to produce the desired outcome. I thought the operator should be a combination of $a - b$ and $a \cdot b^{-1}$ Since $-$ and $\div$ can produce $+$ and $\times$ respectively Please do not tell me the full answer. Give me a hint to point me towards the right path","['functions', 'abstract-algebra', 'contest-math', 'recursion']"
3048209,Solve the differential equation $\frac{dy}{dx}=\frac{y+2y^5}{4x+y^4}$,Solve the differential equation $$\frac{dy}{dx}=\frac{y+2y^5}{4x+y^4}$$ My try: we can write the equation as: $$\frac{dy}{dx}=\frac{1}{y^3}\frac{\left(1+2y^4\right)}{1+\frac{4x}{y^4}}$$ Multiplying both sides with $\frac{1}{y^5}$ we get: $$\frac{1}{y^5}\frac{dy}{dx}=\frac{1}{y^8}\frac{y^4(2+\frac{1}{y^4})}{1+\frac{4x}{y^4}}=\frac{1}{y^4}\frac{(2+\frac{1}{y^4})}{1+\frac{4x}{y^4}}$$ Now letting $$\frac{1}{y^4}=t$$ we get $$\frac{-1}{4}\frac{dt}{dx}=\frac{t^2+2t}{4tx+1}$$ Any way further to convert in to variable separable?,"['algebra-precalculus', 'homogeneous-equation', 'ordinary-differential-equations']"
3048221,How to place 14 dots on the plane,"A friend asked me a question to ponder over: You got $14$ dots which you need to place on a plane in such a way so that you get the maximum amount of similar distances between each $2$ points. I managed to get $31$ ( $12$ first hexagon $+ 12$ second hexagon $ + 7$ distance between each point of $2$ hexagons) by drawing $2$ hexagons one below the other with the distance between the centers equal to the side length The answer is incorrect though according to him. Any insight towards the solution would be helpful. PS: don't mind the black-red, I made such distinction just for the sake of pointing out the position of $14$ dots ( $2$ hexagons + $2$ centers)",['geometry']
3048226,A measure zero uncountable set for the Lebesgue-Stieltjes measure $\mu_F$.,"Let $\mu_F$ be the Lebesgue-Stieltjes measure on $\Bbb R$ associated with the increasing function $F:\Bbb R\to \Bbb R$ . Construct an uncountable set of measure 0 for $\mu_F$ . When $F(x)=x$ and we get the Lebesgue measure on $\Bbb R$ and the Cantor set in the interval $[0,1]$ is an example. My guess is that the same argument can be repeated for $\mu_F$ , if there is an interval $[a,b]$ where $F$ is continuous (hence making sure that any singleton set has measure 0), and construct a ""Cantor set"" there. But an increasing function can have a dense set of discontinuities as shown in here . What can one do in this case?",['measure-theory']
3048275,Expressing Riemann sums as integrals,"$$L_2=\lim_{n→∞}\sum_{k=1}^n\frac{(k-\cos^2(k))^4}{n^5}.$$ My teacher said that when brackets at the numarator is expanded the limit of sum except $\dfrac{k^4}{n^5}$ equals $0$ , so the Riemann sum becomes $\lim\limits_{n→∞}\sum\limits_{k=1}^n\dfrac{k^4}{n^5}$ . I don't understand this point. Please explain this point. Sorry for my bad English.","['limits', 'definite-integrals']"
3048309,Are numbers in which all substrings are primes finite regardless of base?,"For no particular reason I stumbled upon the sequence/set of all numbers where all substrings of the decimal representation is prime (A085823). It's quite easy to see that this must be a finite set. My approach was that the digits must alter between 3 and 7 eventually (since a prime can't end with 2 or 5 for larger than that and a prime can't consist of just two non-1 digits, those would be dividable by 2, 5 or 11) and we can see where it ends. But if we change the base that approach fails. For example in base 8 a prime can end in 3, 5 or 7 AFAICS so such numbers can end in a altering sequence of 3, 5 and 7s but having tree digits to alter between there's always a way to vary the sequence. I tried to write a python snippet to produce such numbers in different bases and for those I've tested the program seem to hang (ie don't seem to find any more numbers). Is it true that there are only a finite number of such numbers for every base?","['number-theory', 'prime-numbers']"
3048354,Why in physics the elementary work is written as $\delta W$ instead of $dW$?,"Why an elementary work is written $\delta W$ instead of $dW$ ? For example, it's often written $$\delta W=F\cdot dr$$ if $dr$ is the elementary displacement. Why don't we write as usual $dW=F\cdot dr$ ? I saw an answer here but it doesn't really answer to the question (at my opinion). By the way, since at the end $W_{AB}=\int_A^B \delta W$ , I really don't understand this $\delta W$ . Is there mathematically a sense ?","['derivatives', 'differential-forms']"
3048378,Combinatorial Proof for the equation $\sum_{i=0}^j {j \choose i} 2^{j-i} = 3^j$,$$\sum_{i=0}^j {j \choose i}2^{j-i} = 3^j$$ My approach: I know the binomial way to do this is to think of the RHS as $(1+2)^j$ and then expand using binomial like so: $$(1+2)^j = \sum_{i=0}^j {j \choose i} \cdot 2^{j-i} \cdot 1^i$$ $$ = (1+2)^j = \sum_{i=0}^j {j \choose i} \cdot 2^{j-i}$$ But I am not sure how to do the combinatorial proof.,"['binomial-theorem', 'combinatorics', 'discrete-mathematics']"
3048427,Splitting a string into 2 parts such that the number of 1's in part A are equal to the number of 0's in part B.,"The full question is as follows. Prove that every binary string of length $n$ can be split down into 2 substrings where string $S = A.B$ such that the number of $0's$ in A is equal to the number of $1's$ in B. Example: a) String $010010$ can be split as $01.0010$ . The number of $0's$ in A is $1$ and the number of $1's$ in B is $1$ too. b) String $11101000$ can be split as $1110.1000$ I am stumped, I don't know how to apply any of the classic proof techniques I usually use.","['pigeonhole-principle', 'combinatorics', 'discrete-mathematics']"
3048483,Inverse image of a structure sheaf under the inclusion of a closed subset and why it isn't quasi-coherent?,"This question is motivated by Example 5.2.4 in Algebraic Geometry by Hartshorne (page 112). As far as I understand, it claims that if $Y$ is a closed subscheme of $X$ , and one considers the inverse image of $\mathcal{O}_X$ under the inclusion of $Y$ (labeled in the book by $\mathcal{O}_{X|Y}$ ), then the resulting sheaf on $Y$ might not be a sheaf of modules, or even if it is, then it won't be quasi-coherent. As there is no counterexample given, I am wondering what the reason for this might be. Is it because $\mathcal{O}_{X|Y}$ is not an $\mathcal{O}_Y$ module for the particular choice of the structure on $Y$ , or would it be independent of what the chosen structure sheaf on $Y$ would be, as long as it gives a closed immersion? To elaborate: If I restrict my consideration to affine schemes. Then $X = Spec(A)$ and $Y = Spec(A/\alpha)$ . Does $\mathcal{O}_{X|Y}$ not being an $\mathcal{O}_Y$ -module correspond to choosing an incorrect $\alpha$ ? An additional question related to this would be: When does $\mathcal{O}_{X|Y}$ give a structure sheaf of a scheme on $Y$ ? If it were possible always, then it would mean for the previous question that we can always choose a structure sheaf on $Y$ such that $\mathcal{O}_{X|Y}$ is a sheaf of modules.","['algebraic-geometry', 'schemes']"
3048527,Improved sieve for primes and prime twins?,"Suppose we want to estimate the number of primes between $x$ and its square root, say for example between $10$ and $100$ with a sieve. There are $90 $ numbers so we estimate : $\pi(10,100) = 90(1-1/2)(1-1/3)(1-1/5)(1-1/7) \\
= 90 * 2 * 4 * 6 /2 / 3 / 5 / 7 = 90 * 24 / 105 = 20,57\ldots$ This is very good. The true value is $21$ . However we know from The prime number theorem and from Mertens’ theorem that this is only a good estimate up to a multiplicative constant. The problem is easily identified. On one hand we have that divisions have remainders leading to increasing error terms. On the other hand we have this : $(1-1/2)(1-1/3)\cdots(1-1/p_{n-1})(1-1/p_n)$ where $p_n$ is close to The square root of $x$ , leading to meaningless things ( terms ) like $x/(2*3*p_{n-1}*p_n) << 1$ . Truncating is thus the idea. Let $\omega(n)$ count the number of distinct prime factors of the integer $ n \geq 2$ . This $\omega(n)$ is called the prime omega function. Consider the truncated version of $(1-1/2)(1-1/3)\ldots$ : $$ \pi(t,t^2 + t) = \sum_{1<i<t} \frac{(-1)^{\omega(i)} t^2}{i} $$ Where $i$ are the squarefree integers. How much better is this? More precisely: is $\sum_{1<i<t^2} \frac{(-1)^{\omega(i)} t^2}{i} $ asymptotic to $\frac{t^2}{2 \ln(t)} $ or are we still off by a constant factor ? And if we are still off by a constant is it the same as Mertens’ or did we improve it - closer to $1$ - ?
How about a closed form then? The analogue question for prime twins: is $\sum_{2<j<t^2} \frac{(-2)^{\omega(j)} t^2}{j} $ where $j $ are squarefree odd integers, asymptotic to $\frac{t^2}{2 \ln^2(t)}$ or are we still off by a constant factor? And if we are still off by a constant is it the same as Mertens’ squared or did we improve it - closer to $1$ - ?
How about a closed form then? I was unable to find this online or in libraries.","['twin-primes', 'sieve-theory', 'number-theory', 'truncation-error', 'prime-numbers']"
3048558,Explanation - prove that SupB=InfA,"Why is it stated in the solution that $SupB\in{B}$ ? 
By the completeness axiom the set B has a supremum, but that does not imply that it's the maximal element of the set. My intuition is telling me whats written is correct, but I don't know how to state it.","['elementary-set-theory', 'proof-explanation', 'intuition']"
3048570,Solving an integral equation with inverse Laplace transform,"Let $\alpha,\beta,\mu>0$ .  I am looking for a solution, i.e. a function $g(x)$ , that satisfies $$
\frac{\beta^{\alpha}}{\Gamma(\alpha)}\int_0^\infty g(x)x^{\alpha-1}e^{-\beta x}\,\mathrm dx=\left(\frac{\alpha}{\beta}-\mu\right)^{-1},
$$ where $\alpha/\beta>\mu$ .  Note that such a solution would yield an unbiased estimator for $\left(\frac{\alpha}{\beta}-\mu\right)^{-1}$ , i.e. if $X\sim\operatorname{Gamma}(\alpha,\beta)$ then $\operatorname Eg(X)=\left(\frac{\alpha}{\beta}-\mu\right)^{-1}$ . I tried solving this with an inverse Laplace transform by writing $$
\mathcal L\left\{x^{\alpha-1}g(x)\right\}(\beta)=\frac{\Gamma(\alpha)}{\beta^{\alpha}}\left(\frac{\alpha}{\beta}-\mu\right)^{-1}.
$$ I recovered $g(x)$ by taking the inverse transform of both sides and then multiplying by $x^{1-\alpha}$ . $$
\begin{aligned}
g(x)%
&=x^{1-\alpha}\mathcal L^{-1}\left\{\Gamma(\alpha)s^{-\alpha}\left(\frac{\alpha}{s}-\mu\right)^{-1}\right\}(x)\\
&=-\frac{x^{1-\alpha}}{\mu}\mathcal L^{-1}\left\{\Gamma(\alpha)s^{-\alpha}\left(1-\frac{\alpha/\mu}{s}\right)^{-1}\right\}(x).
\end{aligned}
$$ Using Bateman's Tables of Integral transforms, volume 1, $5.4.(9)$ , this evaluates to $$
g(x)%
=-\frac{1}{\mu}\Phi_2\left(1;\alpha;\frac{\alpha}{\mu}x\right),
$$ where $$
\Phi_2(b_1,\dots,b_n;\gamma;z_1,\dots,z_n)=\sum_{m_1=0}^\infty \cdots\sum_{m_n=0}^\infty \frac{(b_1)_{m_1}\cdots (b_n)_{m_n}}{(\gamma)_{m_1+\cdots +m_n}m_1!\cdots m_n!}z_1^{m_1}\cdots z_n^{m_n}
$$ is the hypergeometric function of $n$ variables.  In this case we have a hypergeomatric function of a single variable; thus, $$
g(x)%
=-\frac{1}{\mu}{_1}F_1\left(1;\alpha;\frac{\alpha}{\mu}x\right).
$$ Unfortunately, this solution only yields sensible results if $\alpha/\beta<\mu$ (I have tried using some example parameters in MATLAB which demonstrates this). That said, what I am interested in is the case where $\alpha/\beta>\mu$ . The formula in my table of integral transforms only has the restriction $\alpha>0$ .  Maybe there is an error?  How can I get the solution to work for positive $\mu$ ? We can check the solution which does seem to be correct.  Using G&R formula $7.522.9$ we find $$
\begin{aligned}
\operatorname Eg(X)%
&=-\frac{\beta^{\alpha}}{\mu\Gamma(\alpha)}\int_0^\infty x^{\alpha-1}e^{-\beta x}{_1}F_1\left(1;\alpha;\frac{\alpha}{\mu}x\right).\,\mathrm dx\\
&=-\frac{1}{\mu}{_2}F_1\left({1,\alpha\atop\alpha};\frac{\alpha}{\beta\mu}\right)\\
&=-\frac{1}{\mu}{_1}F_0\left({1\atop -};\frac{\alpha}{\beta\mu}\right)\\
&=-\frac{1}{\mu}\left(1-\frac{\alpha}{\beta\mu}\right)^{-1}\\
&=\left(\frac{\alpha}{\beta}-\mu\right)^{-1}.
\end{aligned}
$$ So I am puzzled as to why this solution does not work for $\alpha/\beta>\mu$ .  One thing worth noticing is that when $\alpha/\beta<\mu$ , the argument of the ${_1}F_0(1;-;\alpha/(\beta\mu))$ above is less than one and so the series defining it converges to $\left(1-\frac{\alpha}{\beta\mu}\right)^{-1}$ in the usual sense.  For $\alpha/\beta\geq\mu$ the aruguement is greater than or equal to unity and the series defining the ${_1}F_0$ diverges; thus analytic continutation is used.  Maybe this plays into the issue?  Here is a test in MATLAB showing disagreement when $\alpha/\beta<\mu$ : alpha = sym(10);
beta = alpha/8;
mu = sym(5);

syms x g(x)
g(x) = -hypergeom(1,alpha,alpha*x/mu)/mu;
for i = 1:512
    X = gamrnd(double(alpha),double(1/beta));
    est(i) = vpa(g(X));
end

mean(est) = -11979.51
(alpha/beta-mu)^(-1) = 1/3","['integration', 'laplace-transform', 'gamma-function', 'gamma-distribution', 'hypergeometric-function']"
3048573,What does it mean for a complex differential form on a complex manifold to be real?,"I am trying to read Kobayashi's ""Differential geometry of complex vector bundles"". There are many places where a complex differential form is referred to as being real . e.g Chapter I, Proposition 7.24 p. 28 "" A closed real $(p,p)$ form $\omega$ on a compact Kähler manifold M is cohomologous to zero if and only if $ \omega = id' d'' \phi $ for some real $(p-1,p-1)$ -form $\phi. $ "" Similarly, p. 41 Chapter II, Proposition 2.23 "" Given any closed real $(1,1)$ -form $\phi$ representing $c_{1}(E),$ there is an Hermitian structure $h$ in $E$ such that $\phi = c_{1}(E)$ provided $M$ is compact Kähler."" What does it mean? Does it mean that the coefficients are real valued function? Or does it mean that it is the same under complex conjugation ?
i.e. $\bar{\phi} = \phi ? $ To be more explicit, Let $\alpha, \beta : \mathbb{C}^{2} \rightarrow \mathbb{R}$ be real valued smooth functions and let $$ \phi = [\alpha(z_{1}, z_{2}) + i \beta(z_{1}, z_{2})] dz_{1}\wedge d\bar{z_{2}} -  [\alpha(z_{1}, z_{2}) - i \beta(z_{1}, z_{2})] dz_{2}\wedge d\bar{z_{1}} . $$ This is a $(1,1)$ -form on $\mathbb{C}^{2}$ with the property that $\bar{\phi} = \phi, $ since $ \overline{dz_{1}\wedge d\bar{z_{2}}} = - dz_{2}\wedge d\bar{z_{1}},$ but this is a real valued form only if $\beta \equiv 0$ on $\mathbb{C}^{2}.$ Or does $\phi$ being real mean something else? Thanks in advance for any help.","['complex-geometry', 'differential-forms', 'differential-geometry']"
3048616,Show that a function $f: A \to B$ is surjective if and only if it has a right inverse.,"The textbook I'm reading from defines a right-inverse as follows: Let $f: A \to B$ be a function. A right inverse of $f$ is a function $g: B \to A$ with the property that $f \circ g = id_B$ where $id_B$ is the identity function on $B$ . Now, I'm asked to show that $f: A \to B$ has a right inverse iff it is surjective. The author notes that I may use the axiom of choice in this proof. In other words, they say to assume that given a family of disjoint nonempty subsets of a set, there is a way to choose one element in each member of the family. Proof Attempt: Suppose $f$ has a right inverse. So, there is a function $g: B \to A$ such that $f \circ g = id_B$ . Now, choose an element $b \in B$ . We observe that $g(b) \in A$ and that, by hypothesis, $f(g(b)) = id_B(b) = b$ . So, we have found an element of $A$ (namely $g(b)$ ) whose image under $f$ is $b$ . Since the choice of $b$ was arbitrary, we conclude that $f$ is surjective. For the converse, suppose that $f$ is surjective. We need to construct a function $g: B \to A$ such that $f \circ g = id_B$ for all $x \in B$ . In order to do this, fix $b \in B$ . Since $f$ is surjective, there exists an element $g(b) \in A$ such that $f(g(b)) = b$ . However, we note that there may be more than one element of $A$ whose image is $b$ under $f$ (i.e. there may be more than one possible candidate for $g(b)$ ). In order to isolate a single candidate for $g(b)$ , we proceed by cases. Case 1: The function $f$ sends all elements of $A$ to $b \in B$ . In this case, we may conclude by the surjectivity of $f$ that $b$ is the only element of $B$ . Therefore if we pick an element $s \in A$ , we may define a function $g: B \to A$ where $g(b) = s$ . Then, we simply observe that $f(g(b)) = f(s) = b = id_B(b)$ . Since $b$ is the only element of $B$ , we may immediately conclude that $g$ is a right inverse of $f$ . Case 2: Only the elements in a proper subset of $A$ are mapped to $b$ by the function $f$ . In this case, we can divide the set $A$ into two disjoint subsets $R$ and $S$ where $R$ is the set of all elements in $A$ that get mapped to $b$ by $f$ and $S$ is the set of all elements of $A$ that are not mapped to $b$ by $f$ . By the axiom of choice, I can select a single element $p \in R$ and simply let $g(b) = p$ . This shows that for any element $x$ of $B$ , I can always find a single candidate for $g(x) \in A$ such that $f(g(x)) =x$ . Then, we can properly define a function $g: B \to A$ that sends any $x \in B$ to the single candidate $g(x)$ that we have chosen, and, by construction, $g$ will be a right inverse of $f$ . This completes the proof. Is my proof correct? This is the first time I've done a proof requiring the axiom of choice, so I'm wondering if I've invoked it correctly. Are there any ways that I could improve the proof or details that perhaps I'm missing?","['elementary-set-theory', 'proof-writing', 'functions', 'proof-verification']"
3048620,Adaptedness of Solution to SPDE,"I'm trying to understand existence of solutions to SDPEs of the form $$
dX = AXdt + F(t,X)dt + B(t,X)dW
$$ from the Hilbert space point of view, following Da Prato & Zabcyzk.  They rely on the standard fixe point method, showing that if the interval $[0,T]$ , is sufficiently small, the mapping $$
\mathcal{K}(Y)(t)= S(t)\xi + \int_0^t S(t-s)F(t,Y)dt + S(t-s)B(t,Y)dW
$$ has a fixed point in the space $$
C([0,T];L^2(\Omega, \mathbb{P};H))
$$ equipped with the norm $$
\|X\|_T^2= \sup_{0\leq t\leq T}\mathbb{E}\|X(t)\|_H^2.
$$ The existence argument is standard, using the sequence $X_0 =\xi$ , $X_{n+1} = \mathcal{K}(X_n)$ . What I am curious/unsure about is the adaptedness, both of the sequence and the limiting solution.  Assuming $\xi$ is $\mathcal{F}_0$ , how can I see that: Each $X_n$ is adapted The limit, $X$ , is also adapted","['stochastic-processes', 'measure-theory', 'probability']"
3048647,Nilpotent ring and Nilpotent groups.,"Let $R$ be a ring (associative and with unity) and $B$ be a subring with the property that $B^n = 0$ i.e. $$ \forall\; x_1, x_2, \dots, x_n \in B: \; x_1 \cdot x_2 \cdots x_n = 0$$ My aim is to prove that the set $G = \{1+x \mid x\in B\}$ is a nilpotent group. It is easy to see that $G$ is a group. For $n=3$ , I found $$[1+x,1+y]_G = 1+[x,y]_R,$$ where $$[1+x,1+y]_G = (1-x+x^2)(1-y+y^2)(1+x)(1+y)$$ is the group commutator
and $[x,y]_R = xy-yx$ is the ring commutator. And $G$ is a nilpotent group of degree no more than $3$ . But if $n=4$ , I got $$[1+x, 1+y]_G = 1 + x^2y+xy+xyx+y^2x-yx-yxy,$$ and I don't know what to do.","['nilpotent-groups', 'nilpotence', 'ring-theory', 'abstract-algebra', 'group-theory']"
3048659,What is the difference between a weakly stationary process and strictly stationary process?,In some lecture slides I read that the definition of a weakly stationary process is that The mean value is constant The covariance function is time-invariant The variance is constant and I read that the definition of a strictly stationary process is a process whose probability distribution does not change over time. What concrete properties of a strictly stationary process is not included in the definition of a weakly stationary process?,"['stochastic-processes', 'probability-distributions', 'stationary-processes', 'probability']"
3048683,Balanced subset sum problem,"Suppose I have a set of $2N$ items with weights $w_0, w_1, \ldots, w_{2N-1}$ . I want to identify the two most ""balanced"" sets of $k$ items each, where $k$ is not given but limited to the range $k_\min \leq k \leq N$ . More specifically: Identify the two disjunct subsets $A = \{a_0, a_1, \ldots a_{k-1}\}$ and $B = \{b_0, b_1, \ldots a_{k-1}\}$ subject to the following conditions: all the $a_i,  b_i \in [0,2N-1]$ $A \cap B = \varnothing$ both sets have  equal cardinality $k$ where $k_\min \leq k \leq N$ the following quantity $\epsilon$ is minimized: $$ \begin{align}
\epsilon &= \frac{|W_A-W_B|}{W_A+W_B} \cr
W_A &= \sum\limits_{i=0}^{k-1}w_{a_i} \cr
W_B &= \sum\limits_{i=0}^{k-1}w_{b_i} \cr
\end{align}$$ Is this a known problem? (I found Is there a ""balanced knapsacks"" problem with a known result? which is similar) In my case the value of $N$ is not very large (under 50) so I don't really care about proving NP-hard, I just want to know a good strategy.",['combinatorics']
3048689,"Peano Axiom Proofs: Proving $a < b$, if and only if $a + + \leq b$","As for where I am getting my Peano Axioms, its from Terrance Tao's Analysis I text ( math.unm.edu/~crisp/courses/math401/tao.pdf ). I am unsure whether my proof is correct for proving the forward implication. Here is the work I have done so far: Assure $a<b$ . Suppose for the sake of contradiction we have that $a + 1 \not\leq b$ . In that case $a+1 > b$ . From the definition of $<$ we have that $a \leq b$ and $a \neq b$ . From the definition of $a \leq b$ we get that $b = a +d$ for some positive $d$ . Also from the definition of $<$ , we get that $b \leq a +1$ and $a +1 \neq b$ . From the definition of $a \leq b$ we get that $a + 1 = b + f$ for some positive $f$ . We now have that $a + 1 = a + d + f$ . By cancellation law we have that $1 = d+ f$ . Since $d$ and $f$ were both positive we now have a contradiction. Thus $a+1 \leq b$ . We didn't prove $1 = d+ f$ is a contradiction for positive numbers, so I am feeling hesitant towards this proof. Please suggest some ideas!","['elementary-set-theory', 'proof-writing', 'proof-verification', 'peano-axioms']"
3048700,"Equivalent statements about a metric space (homeomorphism, continuity, cluster points, clopen sets and compacts)","Problem. Let $M$ a metric space with metric $d$ . Prove that the following conditions are equivalent. (a) $M$ is homeomorphic to $M$ with discrete metric. (b) Every function $f:M \to M$ is continuous (c) Every bijection $g: M \to M$ is a homeomorphism (d) $M$ has no cluster points (e) Every subset of $M$ is clopen (f) Every compact subset of $M$ is finite [(a) $\Longrightarrow$ (b)] Given $\epsilon > 0$ take $\delta < 1$ so, $d_{M}(x,y) < 1$ implies $x = y$ and so, $d(f(x),f(y)) = 0 < \epsilon$ . [(b) $\Longrightarrow$ (c)] $g: M \to M$ is continuous and, since $g^{-1}:M \to M$ is a bijection too, $g^{-1}$ is continuous. [(d) $\Longrightarrow$ (e)] Let $S$ be a subset of $M$ . Since $M$ has no cluster points, $S$ is closed. Moreover, if $p \in S$ , we can take ball $B_{r}(p)$ with $r$ small such that $B_{r}(p) \cap S = \{p\}$ . Then $S$ is open too. [(f) $\Longrightarrow$ (a)] If the metric is not discrete, take $p$ a cluster point of $M$ . Thus, for every $n \in \mathbb{N}$ , considering $B_{1/n}(p)$ , we obtain a convergente sequence, that is, $p_{n} \to p$ . But, every convergent sequence is bounded and every subsequence converges to $p$ , therefore, $\{p_{n}\}_{n}\cup \{p\}$ is compact, a contradiction. Is there a mistake? For (c) $\Longrightarrow$ (d) and (e) $\Longrightarrow$ (f) I have no idea. Can someone help me?","['general-topology', 'metric-spaces', 'real-analysis']"
3048703,"Show that any non-trivial ideal of $(L_1,*)$ is dense","This is related to this other question , I mean, the linked question comes to my mind trying to solve the following exercise: Show that any non-trivial ideal of $(L_1,*)$ is dense. Here $(L_1,*)$ is the space $L_1(\Bbb R^n,\Bbb R)$ and $*$ stay for convolution. I know that $(L_1,+,*)$ is a Banach algebra without unity. I dont know the concept of ""ideal"" related to a Banach algebra so I assumed that it is asking for a set $I\subset L_1$ such that $f*\varphi\in I$ for any pair $(f,\varphi)\in L_1\times I$ . Correct me if my interpretation was wrong. FIRST APPROACH: I know that if $g\in L_1$ and $\|g\|_1=1$ then $g_\epsilon(x):=\epsilon^{-n}g(x/\epsilon)$ defines a kernel $\{g_\epsilon:\epsilon>0\}$ that approximates the unity, that is, I know that $$\lim_{\epsilon\to 0} g_\epsilon* f=\|g\|_1 f=f\tag1$$ in $L_1$ for any chosen $f\in L_1$ . Then my first idea was construct some approximation to the unity from a function $f*\varphi\in I$ (where $(f,\varphi)\in L_1\times I$ ), that is, I want to show that if $f*\varphi\in I$ then $(f*\varphi)_\epsilon\in I$ for any $\epsilon>0$ , from here it would be easy to see that $$h_\epsilon:=\frac{(f*\varphi)_\epsilon}{\|f*\varphi\|_1}\in I\tag2$$ and $\lim_{\epsilon\to 0}h_\epsilon*r=r$ for any chosen $r\in L_1$ , what would imply that $I$ is dense in $L_1$ . Then note that $$(f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int f(x-y+K)\varphi(y)\, dy\tag3$$ for $K:= x/\epsilon-x$ . Then also note that $\epsilon^{-n} f(\,\cdot+K)\in L_1$ for any chosen $K\in\Bbb R^n$ , so an heuristic argument make me think from $(3)$ that $(f*\varphi)_\epsilon\in I$ . However, this heuristic argument seems wrong, because we cannot fix $K$ as a constant and say that $g(x-y):=f(x/\epsilon-y)$ belongs to $L_1$ because $g$ is not well-defined. SECOND APPROACH: Note that $$(f*\varphi)_\epsilon(x)=\epsilon^{-n}\int f(x/\epsilon-y)\varphi(y)\, dy=\epsilon^{-n}\int \tau_{x/\epsilon}\check f(y)\varphi(y)\, dy\\
=\epsilon^{-n}\int(\tau_{x/\epsilon}\check f\cdot \varphi)(y)\, dy
=\epsilon^{-2n}\int\tau_{x/\epsilon}\check f(y/\epsilon)\varphi(y/\epsilon)\, dy
\\=\epsilon^{-2n}\int f\left(\frac{x-y}\epsilon\right)\varphi(y/\epsilon)=(f_\epsilon*\varphi_\epsilon)(x)\tag4$$ where $\tau_a f(x)=f(x+a)$ and $\check f(x)= f(-x)$ . Then it would be enough to show that if $\varphi\in I$ then $\varphi_\epsilon\in I$ , but this doesn't seems feasible. THIRD APPROACH: Let any smoothing kernel $\{\varphi_\epsilon:\epsilon>0\}\subset L_1$ (by example the Gaussian kernel) and some $\psi\in I\setminus\{0\}$ . Then, by the definition of ideal, we knows that $\varphi_\epsilon*\psi\in I$ for all $\epsilon>0$ , thus by $(1)$ we can see that $\psi$ is a limit point of $I$ , and because this holds for every function on the ideal then we conclude that $I$ is closed and perfect. Then, if $I$ is dense, it must be the case that $I=L_1$ . However Im again stuck here, that is, I dont know how to show that $I=L_1$ . Moreover: if it would be true that $I=L_1$ then $I$ is, indeed, a trivial ideal of $L_1$ , contradicting the existence of non-trivial ideals, so the statement to be proved will be a vacuous truth. Some help will be appreciated, thank you.","['lp-spaces', 'abstract-algebra', 'convolution', 'real-analysis']"
3048709,Restricting divisors to closed fiber of relative curve over henselian DVR,"Setup: $k$ is an algebraically closed field. $\mathcal{O} = k\{t\}$ is the henselization of $k[t]_{(t)}$ . $V \rightarrow \text{Spec}(\mathcal{O})$ is proper and has a section. $V$ is irreducible, nonsingular, and of dimension $2$ . $X$ is the closed fiber of $V$ . Question: Why is $\text{ker}(\text{Pic}(V) \rightarrow \text{Pic}(X))$ uniquely divisible by $n$ when $n$ is prime to $\text{char}(k)$ ? This is claimed (no proof) in Artin, Grothendieck Topologies, Prop. 4.4.2.",['algebraic-geometry']
3048736,Existence of maps on $\mathbb{N} \cup \{0\}$ satisfying $\phi(ab)=\phi(a)+\phi(b)$ [duplicate],"This question already exists : There are finitely many maps on nonnegative integers satisfying $\phi(ab)=\phi(a)+\phi(b)$ [duplicate] Closed 3 years ago . How many maps $\phi : \mathbb{N} \cup \{0\} \to  \mathbb{N} \cup \{0\} $ are there with the property that $\phi(ab)=\phi(a)+\phi(b)$ , for all $a,b \in \mathbb{N} \cup \{0\} $ ? My Attempt is $$\phi(0)+\phi(m)=\phi(0) \implies \phi(m)=0\quad \text{ for all } m \in  \mathbb{N} \cup \{0\}$$ Hence there is only one such map. Is it correct?",['algebra-precalculus']
3048753,Odds in a “reverse” raffle,"If in a raffle style drawing, but where you draw until one card or number is left, are the odds any different than when the first card drawn is the winner/loser?","['statistics', 'combinatorics', 'probability']"
3048778,Calculating integral over unit ball using co area formula,"Calculate $\int_V\frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}}dxdydz$ , where $V=\{(x,y,z)|x^2+y^2+z^2\lt 1\}$ using the co area formula. So I know that the formula is: $\int_V f dx$ = $\int_a^b\int_{M_c}\frac{f(x)}{|\nabla \phi(x)|}dS dc$ , where $\phi:V\rightarrow\mathbb{R}\in C^1,\forall x: \nabla \phi(x)\neq 0,\phi(V)=(a,b)$ and $M_c=\{x=(x_1,...,x_n)\in V|\phi(x)=c,c\in(a,b)\}$ . My attempt - So the difficult part here (for me) is to find the correct $\phi$ . I thought about $\phi = x^2+y^2+z^2$ which is $\phi \in(0,1)$ but $\nabla \phi(0,0,0) =0$ so I cannot use it. I also thought about $\phi = (x^2+y^2+z^2)^{\frac{3}{2}}$ , but also here $\nabla \phi(0,0,0) =0$ so I cannot use it. Other attempt was to use the function that we integrate - $\phi = \frac{1}{2+(x^2+y^2+z^2)^{\frac{3}{2}}}$ and here I know that $\nabla \phi \neq 0 \forall x$ , but when I lower the dimension of the integral, which variable should be gone?(i.e., instead of $3$ dimensional, $dxdydz$ , what should it be?) Or perhaps, is there any better $\phi$ to use in this case?","['integration', 'area', 'multivariable-calculus', 'calculus', 'multiple-integral']"
3048816,If $T$ is compact such as $T(e_n)=\lambda_n e_n$ then $\lim_n\lambda_n=0$,"Let $H$ be a Hilbert separable space. with the orthonormal bais $(e_n)_{n\in \mathbb{N}}$ , and $(\lambda_n)_{n\in \mathbb{N}}$ a bounded sequence in $ \mathbb{C}$ . We define the operator $T : H \rightarrow H$ , as $T(e_n)=\lambda_n e_n$ We need to prove that $T$ is compact if and only if $\lim_n\lambda_n=0.$ I only did the reverse : Suppose that $(\lambda_n)_n $ converge to $0$ . We define the finite rank operators $T_k$ defined as : $T_k(x)=(\lambda_1x_1,..,\lambda_kx_k,0,0,...)$ . And now we prove that $||T_k-T||\rightarrow 0.$ We have : $T(x)-T_k(x)=(0,0,...,\lambda_{k+1}x_{k+1},...)$ then : $||T_k-T||\leq sup_{n>k+1}|\lambda_n|\rightarrow0.$ then $T$ is compact as limit of finite rank operators. [But I'm always stuck with the direct implication]","['hilbert-spaces', 'operator-theory', 'functional-analysis']"
3048884,What exactly is a 'dummy variable'?,"I was watching an online course ( The Calculus You Need - MIT OpenCourseWare ), when (around 03:08), the lecturer (Gilbert Strang) says that he doesn't ""care what that dummy variable is"" (the variable x associated with the function y ). He made the following change in the video: $$\frac{d}{dx}\int_{0}^{x}y(x) = y(x)\Rightarrow \text{ change } \Rightarrow\frac{d}{dx}\int_{0}^{x}y(t)dt = y(x)$$ I don't know why the notation of that variable doesn't cause any ambiguity here (I'm assuming Strang wanted to mean that), once x is clearly creating a dependency relation with the bound and the variable associated with the function y . Ultimately... why it doesn't make a difference to call the variable associated with y by x (in this case)? What exactly is a 'dummy variable'?","['integration', 'calculus', 'riemann-integration', 'real-analysis']"
3048914,determinant differentiable at identity,"I would like to prove that the determinant $det:\mathbb{M_n}\rightarrow\mathbb{R}$ is differentiable at the identity matrix with $(Ddet(I))(H)=tr(H)$ . Using the definition of differentiability this boils down to showing that: $$\displaystyle{\lim_{H \to 0}}\frac{\det(I+H)-det(I)-tr(H)}{||H||}=0 \space \space (\bigstar)$$ Now I'm aware of the formula: $\det(I+H)=1+tr(H)+\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}\dots$ but I don't see any way to show that: $$\displaystyle{\lim_{H \to 0}}\frac{\frac{(tr(H)^2-tr(H^2))}{2!}+\frac{(tr(H)^3-3tr(H)tr(H^2)+2tr(H^3))}{3!}+\dots}{||H||}=0$$ $1)$ I'm aware of the directional derivative aproach (see here for example link ) but that can be used only after we know that the derivative exists, to show that $(Ddet(I))(H)=tr(H)$ . $2)$ I'm also aware of the proof treating the determinant as a polynomial function of $n^2$ variables. But I would like to know if there exists a proof using just the definition of derivative $(\bigstar)$ . Thank you!","['linear-algebra', 'real-analysis']"
3048915,Primitive Trinomial for $82589933$?,"At Twelve new primitive binary trinomials , $x^{74207281}+x^{9999621}+1$ is shown to be a primitive trinomial in $GF(2)[x]$ . Note that $2^{74207281}-1$ was the largest known (Mersenne) prime before 2018, that is the prime in the exponent of the Mersenne prime gave rise to a trinomial. In the interim, new Mersenne primes have been found. Just announced (Dec 2018), $2^{82589933}-1$ is prime, and in Jan 2018, $2^{77232917}-1$ was found to be prime. Do these also give rise to trinomials? To be explicit, are there $j$ or $k$ such that $x^{82589933}+x^{j}+1$ or $x^{77232917}+x^{k}+1$ are primitive trinomials in $GF(2)[x]$ ?","['number-theory', 'finite-fields', 'polynomials', 'prime-numbers']"
3048939,Eigenvalues of a matrix given its derivative.,"As noted in the comments, the following notations will be used: $\lambda_i$ is the $i^{th}$ eigenvalue (presumably listed by descending magnitude) $|v_i\rangle$ is the $i^{th}$ column eigenvector $\langle v_i|$ is the $i^{th}$ row eigenvector Suppose I have a large matrix $A(x,t)$ depending on two free parameters (so each input of $A$ is a continuous function of $x$ and $t$ ).  Let $$
B(x,t)=\frac \partial {\partial t}A(x,t)\\
C(x,t)=\frac \partial {\partial  x}B(x,t)
$$ If I know all the eigenvalues and eigenvectors of $A$ , and I know all the eigenvalues and eigenvectors of $C$ , then can I use that information to find the complete eigenvalues and eigenvectors of $B$ ? I know that if such a relationship exists that it is non-trivial, since, for example, if $$
A=\sum \lambda_i |v_i\rangle\langle v_i|,
$$ then $B$ is given by $$
B=\sum (\frac \partial {\partial t}\lambda_i) |v_i\rangle\langle v_i|+ \lambda_i  |\frac \partial {\partial t}v_i\rangle\langle v_i|+ \lambda_i  |v_i\rangle\langle \frac \partial {\partial t}v_i|,
$$ which shows that $B$ is not diagonalized the same as $A$ , but surely a relationship must exist between them.","['matrices', 'matrix-calculus', 'linear-algebra', 'eigenvalues-eigenvectors']"
3048959,Independent random variables and function of them,"If X and Y are two independent random variables and g is some function, is there some theorem saying that g(X) and g(Y) are also independent. Maybe under certain conditions for g? Monotonic? I have the feeling this should be true... under certain conditions maybe. Also any reference or idea for a proof?","['statistics', 'probability-theory', 'probability']"
3048961,Does $\text{SO}_2(\mathbb{Q}_5)$ contain non-trivial elements?,"I was trying to find an element of $\text{SO}_2(\mathbb{Q}_5)$ for the $5$ -adic numbers.  By analogy with $\text{SO}_2(\mathbb{R})$ $$ \left[ \begin{array}{rr} a & -b \\ b & a \end{array} \right] \text{ with } a^2 + b^2 = 1$$ When we solve this equation modulo $5$ , the perfect squares are $\square = \{ 0,1,4\}$ and the only solutions are $0^2 + (\pm 1)^2 = 1$ up to permutations. Momentarily, I considered $4^2 + (\sqrt{-15})^2 = 1$ but we have $\sqrt{-15} \notin \mathbb{Q}_5$ or else we'd have the valuation $|\sqrt{-15}|_5 = \frac{1}{\sqrt{5}}$ . Certainly there are trivial elements such as the identity element and the $90^\circ$ rotation: $$ \left[ \begin{array}{rr} 0 & \mp 1 \\ \pm 1 & 0 \end{array} \right] , 
\left[ \begin{array}{rr} \pm 1 & 0 \\ 0 & \pm 1 \end{array} \right] \in \text{SO}_2(\mathbb{Q}_5) $$ By the looks of it $\text{SO}_2(\mathbb{Q}_7)$ only has trivial elements as well as the perfect squares are $\square_7 = \{0,1,2,4\}$ . EDIT As the answer points out $\text{SO}(\mathbb{Q}) \subset \text{SO}(\mathbb{Q}_5)$ .  Can we find elements of $\text{SO}(\mathbb{Q}_5) \backslash \text{SO}(\mathbb{Q})$ ? I think the answer work because $(5,13)=1$ and so $\frac{1}{13} \in \mathbb{Z}_5$ .","['number-theory', 'p-adic-number-theory', 'lie-groups']"
3048974,product of quadratic forms of random vectors uniform on the sphere,"Let $g = (g_1, ..., g_n)$ be a random vector distributed uniformly on the sphere $\{ x \in \mathbb{R}^n : \| x \|_2 = 1 \}$ . Let $A, B$ be two symmetric $n \times n$ matrices. I am interested in a simple formula for: $$\mathbb{E}[ g^T A g g^T B g ]$$ In particular, I know that if $g$ is a standard normal $N(0, I)$ , then $$
  \mathbb{E}[ g^T A g g^T B g ] = 2 \mathrm{Tr}(AB) + \mathrm{Tr}(A)\mathrm{Tr}(B) \:.
$$ Does something similar hold in the uniform on a sphere case?","['probability-distributions', 'uniform-distribution', 'probability-theory']"
3048978,Geodesic on the catenoid,"Consider the catenoid $\{(x,y,z) \in \mathbb{R}^3: \cosh z = \sqrt{x^2+y^2}\}$ , given with the parameterization $$
r(u,v) = (\cosh u \cos v, \cosh u \sin v, u) \ \ .
$$ I'm trying to show that if $\gamma(t) = r(u(t),v(t))$ is a geodisic curve, such that $\gamma$ is bounded (for every $t\in\mathbb{R}$ ), then the image $\gamma$ is the circle $\{x^2 + y^2 =1, z=0\}$ . By Clairaut's relation , it seems that that $\gamma$ is geodesic if and on if the following are true: $$
\cosh(u)^2 \dot v \equiv c\\
\cosh(u)^2 (\dot u^2 + \dot v^2) \equiv 1 \ \ ,
$$ where $c$ is constant, and $u,v$ are understood as functions of $t$ . The first equation comes from Clairaut's relation, and the second comes from the first fundamental form. However, trying all sorts of algebric manipulations, I couldn't find a way to show that if $\gamma$ is not the said circle, then it is unbounded.  I'd be glad for any hint.","['geodesic', 'differential-geometry']"
3048990,How to prove that $\sum_{k=1}^{\infty}\frac{k^{n+1}}{k!}=eB_{n+1}=1+\cfrac{2^n+\cfrac{3^n+\cfrac{4^n+\cfrac{\vdots}{4}}{3}}{2}}{1}$,"Through some calculation, it can be shown that $$e = 1+\cfrac{1+\cfrac{1+\cfrac{1+\cfrac{\vdots}{4}}{3}}{2}}{1}\tag{1}$$ $$2e = 1+\cfrac{2+\cfrac{3+\cfrac{4+\cfrac{\vdots}{4}}{3}}{2}}{1}\tag{2}$$ $$5e = 1+\cfrac{2^2+\cfrac{3^2+\cfrac{4^2+\cfrac{\vdots}{4}}{3}}{2}}{1}\tag{3}$$ In general, how can I show that $$\sum_{k=1}^{\infty}\frac{k^{n+1}}{k!}=eB_{n+1}=1+\cfrac{2^n+\cfrac{3^n+\cfrac{4^n+\cfrac{\vdots}{4}}{3}}{2}}{1}$$ , where $B_n$ is the $n^{th}$ Bell number. I saw a similar question on Brilliant.org, but I did not pay close attention to the proof, and I ended up forgetting how to prove this kind of problem.
I remember that the proof involves in simplifying the denominator and moving from top to bottom so that the final denominator is in the form of $n!$ , which is the criteria for Maclaurin series. Here is the background information Infinite Series $\sum\limits_{k=1}^{\infty}\frac{k^n}{k!}$","['continued-fractions', 'sequences-and-series']"
3049005,"Solve similar right triangles, given one's hypotenuse, the other's base, and the sum of their heights.","I encountered this problem while trying to determine a generic equation for entasis , but this question is not about entasis. $\theta$ is wanted—given this lovely figure given that the two triangles are similar, and given $a$ , $b$ , and $h$ . I recognize that the sum of the heights of the triangles equals $h$ , and that their ratio equals the scale factor, which seems like a likely avenue, but my trigonometry and geometry are weak and I can’t figure this one out.","['triangles', 'euclidean-geometry', 'trigonometry', 'geometry']"
3049024,"How to prove $\int_0^\infty \ln(1+\frac{z}{\cosh(x)})dx=\frac{\pi^2}{8}+\frac{(\cosh^{-1}(z))^2}{2},z\ge1$ and a closed form for $-1<z<1$?","I observed graphically that $$f(z)=\int_0^\infty \ln\left(1+\frac{z}{\cosh(x)}\right)dx=\frac{\pi^2}{8}+\frac{(\cosh^{-1}(z))^2}{2},z\ge1$$ Can anyone explain why this holds? I tried differentiating with respect to $z$ but I didn't really know how to continue further. Also, if anyone knows how to obtain a closed form of $f(z)$ for $-1<z<1$ it would be greatly appreciated as I'm curious about specific values such as $f(-\frac{1}{2})=-\frac{7\pi^2}{72}$ and $f(\frac{1}{2})=\frac{5\pi^2}{72}.$","['improper-integrals', 'calculus', 'definite-integrals', 'closed-form']"
3049036,Maximal inequality for the Itō integral,"Let $W$ be a Brownian motion and $X$ be a predictable process with $$\operatorname E\left[\int_0^t|X_s|^2\:{\rm d}s\right]<\infty\;\;\;\text{for all }t\ge0.$$ Now, let $p\ge2$ . How can we show that $$\operatorname E\left[\sup_{s\in[0,\:t]}\left|\int_0^sX_r\:{\rm d}W_r\right|^p\right]\le C\operatorname E\left[\left|\int_0^t\left|X_s\right|^2\:{\rm d}s\right|^{\frac p2}\right]\tag1$$ for some $C\ge0$ . Let $$q:=\frac p{p-1}.$$ Clearly, we somehow need to apply Doob's inequality and the Itō formula. Doob's inequality yields $$\operatorname E\left[\sup_{s\in[0,\:t]}\left|\int_0^sX_r\:{\rm d}W_r\right|^p\right]\le q^p\operatorname E\left[\left(\left|\int_0^tX_s\:{\rm d}W_s\right|^2\right)^{\frac p2}\right]\tag2.$$ How do we need to proceed from here?","['stochastic-integrals', 'stochastic-analysis', 'stochastic-processes', 'probability-theory', 'stochastic-calculus']"
3049068,How to solve linear differential-difference equation?,"Given a linear differential-difference equation: $$A_{n+2}+\partial A_{n+1}+\partial^2 A_n=0,$$ where $A$ is a function of $n$ and $x$ , and $\partial$ represents the derivative about $x$ . How to solve this equation? The general case is this form $$A_{n+2}+P_1 A_{n+1}+P_2 A_n=0,$$ where $P_1,P_2$ are differential operator depending on function of $x$ . I have tried to set $A_n=B^nA_0$ , where $B$ is a pseudo differential operator and $A_0$ is a function of $x$ , then I obtain $$B^2+P_1B+P_2=0.$$ After solving $B$ , we have the solution of $A_n$ . I don't know whether this is correct and how to do next.","['differential-operators', 'recurrence-relations', 'ordinary-differential-equations']"
3049093,$\mathbb{R}\cong X\times Y$ (homeomorphic) implies $X$ or $Y$ is a point.,"The following is Problem 2.1.1 from Tammo tom Dieck's Algebraic Topology : Suppose $\mathbb{R}\cong X\times Y$ (homeomorphic). Then $X$ or $Y$ is a point. FWIW, the section 2.1 is about (path) connected components (i.e., $\pi_0$ ) and the notion of homotopy. I tried to use the usual trick by removing one point and consider the number of components, but failed. The spaces $X,Y$ are arbitrary, and so I don't know how to handle this. Any hints will be appreciated!","['general-topology', 'algebraic-topology']"
3049117,Evaluating $\lim\limits_{x \to \infty}\frac{4^{x+2}+3^x}{4^{x-2}}$,"$$\lim_{x→∞}\frac{4^{x+2}+3^x}{4^{x-2}}.$$ I have solved it like below: $$\lim_{x→∞}\left(\frac{4^{x+2}}{4^{x-2}}+\frac{3^x}{4^{x-2}}\right)=\lim_{x→∞}\left(4^4+\frac{3^x}{4^x}·4^2\right).$$ Since, as $x → ∞$ , $3^x → ∞$ , $\dfrac{3^x}{4^x} → 0$ , the limit is equal to $4^4=256$ . Have I solved it correctly? This was a practice test question and the given solution was wrong. So, I solved it and I am preparing alone, no friend to discuss, so I posted it here.","['limits', 'calculus']"
3049126,Every positive power of $5$ appears in the last digits of bigger power of $5$,"Problem. Show that for every positive integer $n$ , there is an integer $N > n$ such that the number $5^n$ appears as the last few digits $5^N$ . For example, if $n = 3$ , we have $5^3 = 125$ and $5^5 = 3125$ , so $N = 5$ would satisfy. This is a problem from the worksheet of the class Putnam Seminar at CMU. Please give hints towards the right direction and not the full solutions. Thanks!!","['contest-math', 'number-theory']"
3049136,$L^q$ contains $L^p$ but $l^p$ contains $l^q$?,"Suppose $1\leq q < p < \infty$ . For finite Lebesgue measure sets $X$ ( $\mu(X) < \infty$ ), we have $$
L^p(X) \subset L^q(X)
$$ On the other side, for the infinite counting measure $\mu$ ( $\mu(\mathbb{N})=\infty$ ), we have $$
L^q(\mathbb{N}) = l^q \subset l^p = L^p(\mathbb{N})
$$ Why does the containment reverse directions? Is is because they use different measures, because one measure is finite and the other infinite, or something else? I'd like to have an intuitive understanding why this reversal occurs.","['lp-spaces', 'functional-analysis']"
3049164,Can we obtain an explicit and efficient analytic interpolation of tetration by this method?,"I am curious about this. It has been a very long time since I have ever toyed with this topic but it was an old interest of mine quite some time ago - maybe 8 years (250 megaseconds) ago at least, and I never really got to a conclusion, nor do I think anyone did entirely satisfactorily, and some comments on a Youtube video I was watching inspired me to dust it off and try once more to give it another thwack. And the question is, basically, how can one construct a ""reasonable"" interpolation of the ""tetration"" operation, also called a ""power tower"", which for those who have not heard of it is defined for natural $n > 1$ and real $a > 1$ by $$^{n} a = a \uparrow \uparrow n := \underbrace{a^{a^{a^{...^a}}}}_{\mbox{$n$ copies of $a$}}$$ where the nested exponentiations on the left are evaluated in rightassociative fashion, so the deepest (""highest"") layer is done first, e.g. $$^{3} 3 = 3^{3^3} = 3^{27} = 7\ 625\ 597\ 484\ 987$$ and not left-associative, i.e. $$^{3} 3 \ne (3^3)^3 = 3^{3 \cdot 3} = 3^9 = 19\ 683$$ What the ""interpolation"" part means is basically, given that this definition clearly only works for values of the second argument, $n$ (called as such by analogy with exponentiation even though it is written first in the ""left-superscript"" notation just introduced), usually called the ""height"" of the tetration or power tower for obvious reasons, that are natural numbers at least 1, since we have to have a whole number of ""copies of $a$ "" - half a copy, say, wouldn't make much sense, as while you can literally write a half-written "" $a$ "", that is formally nonsense and has no mathematical meaning, although it may have other forms of meaning from other angles of human understanding and analysis, e.g. perhaps as a form of artsey commentary. Mathematical meaning is, though, of course, what we're interested in. We can, of course, naturally extend this in a way similar to extending exponentiation to the integers by noting that $$^{n+1} a = a^{^n a}$$ and thus $$^{n-1} a = \log_a(^n a)$$ and if we do this we can at least extend that $^0 a = 1$ , similar to exponentiation, and $^{(-1)} a = 0$ , a rather interesting result when viewed in contrast to exponentiation given that the first negative exponentiation of a number is not a constant but instead its reciprocal. Of course, we cannot extend now to $^{(-2)} a$ , as then we get $\log_a(0)$ which is undefined (though of course if you want to stretch the maths a bit and expand the codomain to the extended reals, you can say $^{(-2)}a = -\infty$ . In any case though, $^{(-3)} a$ and further are definitely, really undefined, since no real exponential can be negative, much less negative infinity!). So this peters out. Of course, the most interesting bit - as hinted at with the ""half a copy"" business above - is trying to extend the height $n$ to real values, presumably in $(-2, \infty)$ at least. And there have been a number of methods fielded in those past epochs which attempt to do this as well as some interesting ones regarding the conditions which are required to produce a suitably ""natural"" extension, given that it is trivially obvious that one can, of course, ""interpolate"" a given sparse sequence of points in any way that one desires and, moreover, even with the identities $^{n+1} a = a^{^n a}$ , they only suffice to make it unique insofar as whole-number increments of the tower are concerned - fill any unit interval with anything you like, and the identity will extend to provide an interpolant that will satisfy it. For exponentiation, this non-uniqueness is much less of a problem because we also have the additional identity $a^{n+m} = a^n a^m$ , which lets us extend to rational values, however no such identity exists for tetration. In this regard, extension of tetration is similar to the question of extension of the factorial, which is similarly impoverished of identities, with new ones interestingly only coming about after the extension was done by Euler in the form of the gamma function, to meet a challenge originally proposed by Bernoulli to do exactly this. The gamma function, however, is still ostensibly more ""natural"" simply because a) it often crops up and b) it has some very cute integral representations, esp. the darlin' $$\Gamma(x) = \int_{0}^{1} [-\log(u)]^{n-1}\ du$$ (Though with regard to objection a), one could say this may be simply because we have not yet found such an expression, and thus places where it might be useful, could be currently written off as ""unsolvable"".) Yet clearly, that doesn't seem to have been the case for tetration, either. Moreover, in all these past discussions, many of the extension methods proposed are in fact extremely cumbersome and computationally intensive to approximate, involving elaborate constructs like Riemann mappings, infinite limits of integral equations, and so forth - all things that are, while mathematically valid, both inelegant and also not something you're going be able to program into a software pack like Mathematica and have it spit out 2500 digits of $^{1/2} 2$ in the blink of an eye. But nonetheless, one particular method out of these proposed methods seems be both fairly simple and like that it might possible be amenable to more detailed analysis, and that is the ""Carleman matrix"" operator method. This method is most succinctly expressed for the specific case $a = e$ , to construct the ""natural tetrational"" $\mathrm{tet}(x) :=\ ^x e$ with real height $x$ , so we'll just focus on that for now. But basically it is based on the following two observations. The first is that one can consider the result of the height- $n$ power tower of $e$ as the iterated exponential evaluated at $1$ , namely $$^n e = \exp^n(1)$$ or perhaps more nicely for what we're about to do, $$^{n-1} e = \exp^n(0)$$ which has some interesting gamma-function like quality about it with the offset. And the second one is the following. If we let $\exp$ be given by its power series, $$\exp(x) = \sum_{n=0}^{\infty} \frac{x^n}{n!}$$ then we can actually represent such iterated exponentials using what is called its Carleman matrix , basically the infinite-order ""matrix"" with entries $$C[\exp]_{ij} = \frac{i^j}{j!}$$ such that if we have the infinite vector of exponential coefficients $\mathbf{a} = \left[ \frac{1}{1!}\ \frac{1}{2!}\ \frac{1}{3!}\ \cdots \right]^T$ then the vector $\mathbf{b}_n = (\mathbf{C}[\exp])^n \mathbf{a}$ is the coefficients of $\exp^n$ . In particular, if we sum the top row, we get exactly what we want: $^{n-1} e$ . Now the question is, though, how can we compute this matrix power for fractional $n$ in some explicit form? It seems one possible way to do this, and the way that I saw when this method was suggested (by Gottfried Helms, who was here a long time ago, not sure if they're still so) was to try to diagonalize the matrix, so that you can use the fact that if a matrix $\mathbf{A} = \mathbf{P}\mathbf{D}\mathbf{P}^{-1}$ for some diagonal matrix $\mathbf{D}$ (which happens to be the matrix of eigenvalues) then $\mathbf{A}^t = \mathbf{P}\mathbf{D}^t\mathbf{P}^{-1}$ where that the inner power is easy to compute as you just take exponents of the diagonal terms. And numerically this seems to work for at least finite truncations of the matrix $\mathbf{C}[\exp]$ , but analytically it may be on shaky ground, as I see with this thread here that I just saw when trying to dig into this once more: Diagonalization of an infinite matrix and moreover it's still not super efficient as we'd like - we're not computing 2500 digits of $^{1/2} e$ and I want them computed, dammit! However, it seems that in this case some of the objections raised in that thread do not perhaps apply here. In particular, it is mentioned how that an infinite matrix diagonalization is ambiguous (seems to mirror the situation with the tetration interpolation generally where there is great freedom) due to choice of suitable normed vector space over which to make sense of it, and moreover in that question it was pointed that the usual most ""natural"" space, $\ell^2$ , did not work for that questioner's particular matrix, because in particular it would ""map"" most ""vectors"" of $\ell^2$ to effectively outside of the space. However , this Carleman matrix seems better behaved - in particular, due to the fact that any vector $[\ a_0\ a_1\ a_2\ \cdots\ ]^T \in \ell^2$ by definition must have terms that converge absolutely as an infinite sum, then that owing to the factorials in the matrix when we multiply by it we should also get an (even more) convergent series, as is illustrated by considering the bounding ""vector"" $[\ 1\ 1\ 1\ \cdots\ ]^T$ as ""formally"" acted upon by $\mathbf{C}[\exp]$ . So it seems that in that regard, we are in better shape against at least the objections raised in that thread in this case than for that poster's scenario. Thus the question I have is, if we take the relevant target space as $\ell^2$ , can we find a ""natural"" infinite diagonalization and thus matrix-power for this case, and moreover express it somehow in terms of at least some sort of infinite combinatorial sums or otherwise easily-manipulated expressions for its coefficients? Moreover, another interesting and seemingly natural question provoked by this is, exactly how sensitive is the method to the choice of at least norm used to interpret the matrix power, i.e. can we have absolute freedom to interpolate $^n e$ in any way we please, and if not, then just how much do we get? I suspect ""a lot"", but there are some ""lots"" that are more than others in mathematics, even when infinities are concerned, thanks to Cantor. And can we do the inverse - i.e. if I fill in $^{n} e$ with some freely-chosen interpolant in the interval $[0, 1]$ (for the purpose of making things easy I'll assume it's continuous and moreover equals $1$ at $x = 0$ and $e$ at $x = 1$ ) - can we find a norm such that the associated Carleman matrix power will produce that interpolant? Does it have to be analytic (seems right, but keep in mind that we are summing the top row , not necessarily creating a power series valid for all or even any inputs, though again it also ""seems"" right that if not analytic, it'll diverge)? If so, what's the proof? ADD (epoch time 1545.46 Ms): In the quest for an at least summatory-formula shot at the diagonalization, I note this matrix has the interesting relations among rows and columns given by $$C[\exp]_{(i+1)j} = \sum_{k=0}^{j} \frac{1}{(j - k)!} C_{ik}$$ and $$C[\exp]_{i(j+1)} = \frac{i}{j+1} C_{ij}$$ Not sure if this helps anything, though. But at least it shows there is structure and thus we're not just dealing with effectively purely random matrices and thus in theory it might somehow be exploited in some fashion to simplify things. ADD 2 (same time): You should actually sum the second row of the matrix power to get the tetration $^n e$ , not the first to get $^{n-1} e$ . The first row always sums to 1. ADD 3 (ue+1545.47 Ms): The first row formula above allows us to derive the interesting property that if $\mathbf{C}[\exp]$ is right-multiplied by the factorial matrix $$\mathbf{D} := \begin{bmatrix} 1 & \frac{1}{1!} & \frac{1}{2!} & \frac{1}{3!} & \cdots \\
0 & 1 & \frac{1}{1!} & \frac{1}{2!} & \cdots \\
0 & 0 & 1 & \frac{1}{1!} & \cdots \\
0 & 0 & 0 & 1 & \cdots \\
& & \cdots & & \end{bmatrix}$$ where $D_{kj} = \frac{1}{(j-k)!}$ (with negative-argument factorials ""naturally"" extended as $\infty$ so these $D$ terms are $0$ ), it shifts up by one row. ADD 4 (sim. time): It looks like we can move both up and down, in particular the matrix $\mathbf{D}$ with entries $D_{kj} = (-1)^{k-j} \frac{1}{(k-j)!}$ will shift down, while the other matrix above, perhaps better called $\mathbf{U}$ instead will shift up. Shifting left and right is possible as well but via the Hadamard product and not the ordinary product, as the second set of relations between columns indicates. In particular, the Hadamard product with the matrix $\mathbf{L}$ with entries $L_{ij} = \frac{i}{j+1}$ will shift left, and the matrix $\mathbf{R}$ with entries $R_{ij} = \frac{j}{i}$ will shift right. Thus we have some interesting system for ""moving"" the matrix about like some kind of tableau or grid - not sure what these symmetry properties do though for easing the analytic solution/explicit formula of the matrixpower as a summation.","['real-analysis', 'linear-algebra', 'functional-analysis', 'combinatorics', 'tetration']"
3049188,Question about Isomorphism of Aut(G),"In material supplied by my instructor there is a question which asks to pick the incorrect statement among the following: If $\text{Aut}(G_1)\cong \text{Aut}(G_2)$ and $G_1$ is infinite group then $G_2$ is also infinite If $\text{Aut}(G_1) \cong \text{Aut}(G_2)$ and $G_1$ is finite group then $G_2$ is also finite If $G_1$ not isomorphic to $G_2$ then Aut( $G_1$ ) not isomorphic to Aut( $G_2$ ). $G_1$ and $G_2$ are two groups, Aut(G) is group of their automorphisms and "" $\cong$ "" means isomorphism. I know all three above are incorrect as Aut( $\Bbb Z_3$ )= $U_3$ (that is, $\Bbb Z_2$ ) and I also found this statement on groupprops : ""Of the three endomorphisms, two are automorphisms: the identity map and the square map. These form a cyclic group of order two: the square map, applied twice, gives the identity map"" and Aut( $\Bbb Z$ ) is isomorphic to $\Bbb Z_2$ so $G_1=\Bbb Z$ and $G_2=\Bbb Z_3$ $G_2=\Bbb Z$ and $G_1=\Bbb Z_3$ We can easily see $\Bbb Z$ and $\Bbb Z_3$ are not isomorphic but Aut( $\Bbb Z$ ) and Aut( $\Bbb Z_3$ ) are. But the given answer is that (1.) is only incorrect statement. Is there any problem in my reasoning?","['automorphism-group', 'group-theory', 'abstract-algebra']"
3049210,Indefinite integral of $\frac{\tan{(1+x^2)}}{1+x^2}$,"Let $f(x) = \frac{\tan{(1+x^2)}}{1+x^2}$ , find $\int f(x)dx$ . I've tried many substitutions (including trigonometric substitutions like $x=\tan \theta$ ) and also integration by parts but didn't work . We can apply power series but it doesn't solve problem .","['integration', 'indefinite-integrals', 'calculus', 'real-analysis']"
3049214,How to deal with sets of measure zero?,"When I learned measure theory, sets of measure zero begin to perplex me somewhat, especially the Fubini Theorem. My method to deal with them is to consider equivalent classes of measurable functions instead, where the equivalent relation refers to being equal almost everywhere. However, when I learn dynamical systems, sets of measure zero perplex me more. For instance, in the book ergodic theory of Petersen, in section 4.1.A, the following proof: Firstly, $G_x$ may not always be in $L^2(X)$ . But we can modify $G$ by letting $G_x=0$ if $G_x \notin L^2(X)$ . But a new problem appears. To make $T$ be an isometry, we need $G$ be genuinely $T \times T$ -invariant, i.e., $(T \times T)G=G$ . I know that for an almost everywhere $T \times T$ -invariant $G$ , we can modify $G$ such that $G$ is genuinely $T \times T$ -invariant. But for above $G$ , if we modify $G$ again, can the new $G$ satisfy that $G_x$ are always in $L^2(X)$ ? Maybe we can modify $G$ such that it satisfy the two conditions, belonging to $L^2(X)$ and $T \times T$ -invariant. However, the emphasis is not it. In fact, I often be troubled with these problems. I know that these are trivial details and many powerful results in measure theory benefit from the introduction of sets of measure zero. But, to be rigorous and to read fluently, are there some methods to deal with sets of measure zero? I don't want to be trapped in problems of sets of measure zero and I want to appreciate the beauty of mathematics directly without bother of sets of measure zero. Thank you very much!","['measure-theory', 'ergodic-theory', 'dynamical-systems']"
3049217,If $f$ has no non trivial fixed points and $f\circ f$ is the identity then $f(x)=x^{-1}$ and $G$ is abelian for $f$ an automorphism of $G$ [duplicate],This question already has an answer here : Finite $G$ with involutory automorphism $\alpha$ with no nontrivial fixed points. Proving properties of $\alpha$. (1 answer) Closed 5 years ago . Let $f$ be an automorphism of the finite group $G$ such that $f\circ f=id$ and $f(x)=x\implies x=e$ Prove that $f(x)=x^{-1}~\forall x\in G$ If we can prove that $f(x)$ and $x$ commute for any given $x\in G$ then we're done with the proof because it would imply $$xf(x)=f(x)x$$ $$f(f(x))f(x)=f(x)x$$ Because $f\circ f=id$ . And since $f$ is a homomorphism: $$f(f(x)x)=f(x)x\implies f(x)x=e\implies f(x)=x^{-1}$$ I don't know how to proceed The fact that $G$ is abelian can be deduced once we have $f(x)=x^{-1}$ because that function is a homomorphism if and only if $G$ is abelian,"['group-homomorphism', 'involutions', 'automorphism-group', 'finite-groups', 'group-theory']"
3049315,How to factor this quadratic expression?,"A bit confused on how to factor $2x^2 + 5x − 3 = 0$ . Firstly, I multiplied $a \cdot c$ , so $2(-3)=-6$ , however couldn't find two numbers that will add up to $5$ . Then, I thought of the factors could be $6(-1)=-6$ , and $6+-1=5$ . However, shouldn't the $a$ and $c$ values always be multiplied together, (the $(2)(-3)$ , rather than $6(-1)$ ? When following the factors $-1$ and $6$ .  I have $(2x^2-1x)(6x-3)$ $x(2x-1)+3(2x-1)$ Is this correct, if not; what is the best way to solve a leading coefficient when factoring?",['algebra-precalculus']
3049384,How do I calculate $\lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x}$,"I just started studying limits a week ago and today I got this question on my YouTube feed. I am having a hard time with it. The question is: Find the value of: $\lim_{x\to \infty}(\frac{x!}{x^x})^\frac{1}{x}$ The answer to this question is $\frac{1}{e}$ There in the comment section, someone suggested to use the Stirling's approximation.  It states $n! \approx   e^{-n}*n^n*\sqrt{2\pi n}$ After putting this in the question it reduces down to:  proving that $\lim_{x\to \infty} \sqrt{(2\pi x)}^\frac{1}{x}=1$ I just don’t know how to eliminate that $\pi$ in the expression. Wolfram Alpha suggested to use Puiseux series. Apparently, the wiki page is just too advanced for me to comprehend. I would be really thankful to anyone who could shed some light on this problem! ( The video link is: https://www.youtube.com/watch?v=89d5f8WUf1Y&t=0s )","['limits', 'calculus']"
3049410,Let $G$ be a polycyclic group and assume that every finite quotient of $G$ is nilpotent. Then $G$ is nilpotent,"Let $G$ be a polycyclic group and assume that every finite quotient of $G$ is nilpotent. Then $G$ is nilpotent. First some preliminaries: Every infinite polycyclic group contains a free abelian normal subgroup. On a polycyclic group one can use 'Noetherian' induction: If $G$ is polycyclic and doesn't have a property P, we can assume every proper quotient of $G$ has the property P, while $G$ doesn't.  (see the book Polycyclic groups by Daniel Segal) If $G$ is finitely generated and nilpotent of class $k$ , we can define the Hirsch length of $G$ as the sum $\sum_{i =1}^{k}m_i = h(G)$ where $m_i$ is the rank of the free part of $C^i(G)/C^{i+1}(G)$ . If $G$ is polycyclic, then we can define the Hirsch number as the number of infinite factors in a witness series of $G$ 's polycyclicity. This number is the same for all such series, and it equals the Hirsch length from above when $G$ is also nilpotent (and hence finitely generated). Here's where I got to: If $G$ is finite, by assumption it is nilpotent. So we can assume $G$ is infinite. Let $A \triangleleft G$ be a free abelian subgroup. Let $p$ denote a prime number; if $G/A^p$ is trivial, $G$ is abelian and hence nilpotent. So we can assume that for every $p$ , $G/A^p$ has the property that if every finite quotient is nilpotent, then so is $G/A^p$ . But if $T/A^p \triangleleft G/A^p$ , and $G/A^p / T/A^p \cong G/T$ is a finite quotient, by assumption $G/T$ is nilpotent. Therefore for every $p$ , $G/A^p$ is nilpotent. Thus, $\forall p$ prime $\exists n_p \in \mathbb{N}$ with $C^{n_p}(G) \subset A^p$ . Now, each such $G/A^p$ is polycyclic, and nilpotent, and hence finitely generated. Thus $h(G/A^p) \leq h(G)$ . From here I suspect we can bound the sequence $n_p$ to get that there is $m \in \mathbb{N}$ s.t $C^{m}(G) \subset \cap_{p} A^p$ . Questions: Can I bound this sequence $n_p$ ? Why is it true that $\cap_p A^p = \{e\}$ ?","['nilpotent-groups', 'group-theory', 'infinite-groups']"
3049412,Question about Proof of Merten's theorem (Cauchy-Product formula),"I have a question about the proof on the german wikepedia page: The proof is stated as follow: Let $A= \sum_{k=0}^{\infty}a_k$ and $B=\sum_{k=0}^{\infty}b_k$ , if at least one of them is absolutely convergent, then their Cauchy-Product converges to $AB$ . Definition of the Cauchy-Product: $C=\sum_{k=0}^{\infty}c_k,c_k=\sum_{j=0}^{k}a_jb_{k-j}$ Without loss of generality let A be the absolutely convergent series and $S_n=\sum_{k=0}^{n}c_k$ 1: $AB=(A-A_n)B+\sum_{k=0}^{n}a_kB$ 2: $S_n=\sum_{k=0}^{n}a_kB_ {n-k}$ 1-2= $AB-S_n=(A-A_n)B+\sum_{k=0}^{n}a_k(B-B_{n-k})$ $(A-A_n)B \rightarrow 0$ and with $N:=[\frac{n}{2}]$ the other series can be splitted into two parts with: $\sum_{k=0}^{N}a_k(B-B_{n-k})+\sum_{k=N+1}^{n}a_k(B-B_{n-k})$ Then $|\sum_{k=0}^{N}a_k(B-B_{n-k})|\leq \sum_{k=0}^{N}|a_k(B-B_{n-k})|=\sum_{k=0}^{N}|a_k||(B-B_{n-k})|\leq\max\limits_{N \leq k \leq n}|B-B_k|\sum_{k=0}^{N}|a_k|\rightarrow 0$ Because the last expression of the above inequalities is a product with a zero-convergent sequence with a bounded sequence. Because the zero-convergent sequence $(B-B_k)$ is bounded there is a $C > 0$ with $|B-B_k|<C\forall k \in \mathbb{N}$ Hence $|\sum_{k=N+1}{n}a_k(B-B_{n-k})|\leq \sum_{k=N+1}{n}|a_k||(B-B_{n-k})|\leq C\sum_{k=N+1}{n}|a_k|\rightarrow 0 \square$ I don't understand why the sum is splitted in two parts, can also somebody tell me what's with the $max$ estimate. Thank you for your time, I would appreciate your help very much.","['analysis', 'sequences-and-series']"
3049434,why is $\frac{k\cdot n!}{k!(n-k)!} = \frac{n(n-1)!}{(k-1)!((n-1)-(k-1))!}$?,"This is an equation from my textbook that I am trying to understand: $$ \frac{k\cdot n!}{k!(n-k)!} = \frac{n(n-1)!}{(k-1)!((n-1)-(k-1))!}$$ What I got so far, is that $\frac{k\cdot n!}{k!} = \frac{n!}{(k-1)!}$ however, why does the same principle apply for (n-k)! in the denominator? Isn't there only one k in the numerator that I can cancel out in the denominator? In other words, shouldn't it be $\frac{k\cdot n!}{k!(n-k)!} = \frac{n(n-1)!}{(k-1)!((n-k)!}$ ?","['binomial-coefficients', 'factorial', 'discrete-mathematics']"
3049455,"If ideal $I$ of domain $R$ is free $R$-module, then $I$ is principal ideal.","If ideal $I$ of domain $R$ is free $R$ -module, how to prove $I$ is principal ideal? Is this right if $R$ is just a commutative ring? My thought: $(1)$ for non-zero ideal $I$ and $J$ , let $0\not=a \in I, 0\not=b \in J$ , since $R$ is  domain, $0\not=ab\in I \cap J$ , so $I \cap J\not =\ \varnothing$ . $(2)$ Suppose $\{e_i\}_{i\in \Lambda}$ is basis of free $R$ -module $I$ , then $I=\bigoplus_{i\in \Lambda}Re_i$ , and every $R{e_i}$ is principal ideal generated by $e_i$ . Inaccurately speaking, $I$ is generated by elements in $\bigcap_{i\in \Lambda}Re_i$ . If $\Lambda$ is a finite set, then that's right. What if $\Lambda$ is an infinite set? And what if $R$ is just a commutative ring istead of a domain? Thanks in advance!","['free-modules', 'modules', 'ring-theory', 'abstract-algebra', 'ideals']"
3049464,"In the Physicists' definition of the path integral, does the result depend on the choice of partitions?","The standard definition of the path integral in Quantum Mechanics usually goes as follows: Let $[a,b]$ be one interval. Let $(P_n)$ be the sequence of partitions of $[a,b]$ given by $$P_n=\{t_0,\dots,t_n\}$$ with $t_k = t_0 + k\epsilon$ where $\epsilon = (b-a)/n$ , and $t_0 = a$ , $t_n=b$ . Let $\mathfrak{F}: C_0([a,b];\mathbb{R}^d)\to \mathbb{C}$ be a functional defined on the space of continuous paths on $[a,b]$ . One defines its discretization as the set of functions $\mathfrak{F}_n : \mathbb{R}^{(n+1)d}\to \mathbb{C}$ given by $$\mathfrak{F}_n(x_0,\dots,x_n)=\mathfrak{F}[\xi_n(t)]$$ where $\xi_n(t)$ is the curve defined by taking the partition $P_n$ , defining $\xi(t_i)=x_i$ and linearly interpolating between the points - in other words $\xi(t)$ is for $t\in [t_i,t_{i+1}]$ the straight line joining $x_i$ and $x_{i+1}$ . One defines the functional integral as the limit $$\int_{C_0([a,b];\mathbb{R}^d)}\mathfrak{F}[x(t)]\mathcal{D}x(t)=\lim_{n\to \infty}\int_{\mathbb{R}^{(n+1)d}} \mathfrak{F}_n(x_0,\dots, x_n) d^dx_0\dots d^dx_n$$ if it exists. In the case of interest for physics one has $\mathfrak{F}[x(t)]=e^{iS[x(t)]}$ or rather $\mathfrak{F}_E[x(t)]=e^{-S_E[x(t)]}$ the euclidean version. So by slicing the time axis into equal subintervals, one converts the functional to a sequence of functions, integrates those and takes the limit. This is the construction outlined for instance in Peskin's book or Sakurai's book, just rewritten in a more ""mathematical"" form. Now, if on the very first step we choose another sequence of partitions $(P_n)$ such that the sequence of partition's norms $|P_n|\to 0$ as $n\to \infty$ but which is not the sequence of equal subintervals, would the resulting path integral be different? I don't see reason why it should be equal. The intervals endpoints are distinct, the interpolations are distinct, hence the maps $\mathfrak{F}_n$ are distinct. If it is I think this is a big problem. After all, the way we are slicing the time axis is arbitrary and just one trick to make the problem easier to deal with.","['integration', 'functional-calculus', 'quantum-field-theory', 'quantum-mechanics', 'mathematical-physics']"
3049488,simple looking but hard to prove geometrical problem: prove that 4 points on the same circle.,"Pardon my bad drawing. ABCD is a square. E is any point on CD. F,G,H are the incenters of triangles BCE, ABE and ADE. Prove that EFGH are on the same circle.","['contest-math', 'euclidean-geometry', 'geometry']"
3049498,Equivalent definition of Schwartz space,"Please tell me about the equivalent definition of schwartz space. Definition of Schwartz space is the following. $$ f(x) \in \mathcal{S} \overset {\mathrm{def}} {\Leftrightarrow} \displaystyle \sup_{x \in \mathbb{R^d} } \left|x^\alpha\partial^\beta_x f(x)\right| < \infty $$ $\forall$$\alpha,\forall$$\beta$ $\in$ $\mathbb{Z^d_+} $ ( $\alpha,\beta$ is multi-index notation) My textbook is written the following statement. $$ \displaystyle \sup_{x \in \mathbb{R^d} } \left|x^\alpha\partial^\beta_x f(x)\right| < \infty\Leftrightarrow \displaystyle \sup_{x \in \mathbb{R^d} } \left|\partial^\alpha_x (x^\beta f(x))\right| < \infty $$ I have proved $\Rightarrow$ by using Leibniz's rule. But I haven't proved $\Leftarrow$ . Please tell me proof $\Leftarrow$ .","['fourier-analysis', 'fourier-transform', 'distribution-theory', 'functional-analysis', 'schwartz-space']"
3049535,"A finite abelian group ""take-away"" game","Fix a finite abelian group $G=\bigoplus_{i=1}^{n} \mathbb{Z}/n_i\mathbb{Z}$ . Suppose we play the following game. For any $1 \leq k < |G| $ , we choose $k$ elements from the group at random without replacement. If the $k$ elements sum to $0$ , we win. Else we lose. Which $k$ should we pick? Of course, $k$ depends on $G$ (which is more or less characterized by the $n_i$ 's). We can ask questions like this for general non-abelian groups, where the order we choose elements matters, and that is precisely the order we perform the group operation. But I suspect it gets hard in this case. Note I $\textit{expressly exclude}$ the case that $k=|G|$ , for the simple fact that it's generally going to be quite likely that the elements sum to $0$ in that case (for instance, if the group has odd order, or has even order and more than one element of order $2$ ). Note also that partial answers are acceptable since this seems to be a not-so-easy problem. For instance, if you have an answer for the specific case where $G$ is cyclic, feel free to mention it.","['group-theory', 'combinatorics']"
3049574,Evaluating $\frac1{3^k}+\frac1{6^k}+\frac1{10^k}+\cdots$,"The series $\frac1{3^k}+\frac1{6^k}+\frac1{10^k}+\cdots$ for integers $k>1$ at the triangular numbers can be written as $$\sum_{i=2}^\infty\frac1{\left(\frac{i(i+1)}2\right)^k}=2^k\sum_{i=2}^\infty\frac1{(i(i+1))^k}$$ At first glance, the zeta function may be of use. Partial fractions can be used to evaluate the sum, but only for small values of $k$ since the general formula is $$\frac1{i^k(i+1)^k}=\frac{a_1}i+\frac{a_2}{i^2}+\cdots+\frac{a_k}{i^k}+\frac{b_1}{i+1}+\frac{b_2}{(i+1)^2}+\cdots+\frac{b_k}{(i+1)^k}.$$ Of course, once the expression in the sum is presented as such, $\zeta(\cdot)$ can replace each individual term. However, apart from this tedious method, I do not see a way to get rid of the summation. Is there another way?","['riemann-zeta', 'sequences-and-series']"
3049581,Self adjoint operators and trace class property,"This is a variation of the problem questioned some time ago. For a complex Hilbert space $H$ let $T: H \rightarrow H$ be a bounded operator. We call that $T$ is a trace-class operator if the following sum $$\sum_{i}{\langle |T|e_{i}, e_{i} \rangle} < \infty$$ converges, where $|T| = (T T^{*})^{\frac{1}{2}}$ is the absolute value of the operator. Assume that $$\sum_{i}{\langle Te_{i}, e_{i} \rangle}$$ converges for any basis in the space $H$ . How to prove that if the aformentioned property holds then the operator is a trace class operator? The progress on the problem is the following:
Given an arbitrary bounded operator $T: H \rightarrow H$ , one can use the following decomposition $$T = \big( \frac{T + T^{*}}{2} \big) ^{*} + i \big( \frac{T^{*} - T}{2i} \big) ^{*}$$ The latter line gives the decomposition $$T = A + i B$$ where $A, B$ are normal operators. For the normal operators we can apply the spectral theorem that proposes that $T$ is unitary equivalent to $$(UT U^{-1})(f(x)) = g(x) f(x)$$ where $$U: H \rightarrow L^{2}(X, \mu)$$ $$g \in L^{\infty}(X, \mu)$$ Though this decomposition classify the operator in a broad sence, i see no direct way to conclude the statement. Are the any hints that may extend the previous argument? If not, are there any ways to conclude the statement?","['operator-theory', 'functional-analysis']"
3049586,Bounded and Self-adjoint Linear Operator and Its Inverse,"Let $H$ be a Hilbert space and suppose that $A:H \rightarrow H$ is a bounded, self-adjoint linear operator such that there is a constant $c>0$ with $c\|x\| \leq \|Ax\|$ for all $x\in H$ . Prove that $A^{-1}:H \rightarrow H$ exist and it is bounded. Hint: For ontoness of $A$ it is enough to show that range of $A$ is closed. Why ? Please help me, if you have any good answer in this question.","['self-adjoint-operators', 'functional-analysis']"
