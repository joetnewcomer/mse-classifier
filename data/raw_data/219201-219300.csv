question_id,title,body,tags
4482096,Image of the map $f : M_n \to M_n$ given by $f(X) = X + \|X-I\|I$.,"Let $M_n = M_n(\Bbb{C})$ be the algebra of $n\times n$ complex matrices and let $\|\cdot\|$ be the operator norm. Consider the map $$f : M_n \to M_n, \qquad f(X) = X + \|X-I\|I.$$ I'm interested in the image of this map. First I noticed that the image seems to be contained in the group of invertible matrices $GL(n)$ . Namely, for all $X \in M_n$ we need to show that $X + \|X-I\|I$ is invertible. By plugging in $X+I$ it is equivalent to show that $X + (1+\|X\|)I$ is invertible for all $X \in M_n$ . But this is true since the spectrum $\sigma(X)$ of $X$ is contained in the closed ball of radius $\|X\|$ around the origin so clearly $-(1+\|X\|) \notin \sigma(X)$ . For $Y \in GL(n)$ we can try to find $X \in M_n$ such that $f(X) = Y$ . Clearly it has to be of the form $X = Y - \alpha I$ for some scalar $\alpha \ge 0$ . Plugging in we have $$Y-\alpha I + \|(Y-\alpha I) - I\| = f(Y-\alpha I) = Y \implies \alpha = \|Y - (\alpha + 1)I\|.$$ The question can be reformulated to: for which $Y \in GL(n)$ exists $\alpha \ge 0$ such that $\alpha = \|Y - (\alpha + 1)I\|$ ? It is easy to see that there is no such $\alpha$ for $Y = \beta I$ when $\beta \in \langle -\infty, 1\rangle$ . So maybe the image is $$GL(n)\setminus \{\beta I : \beta \in \langle -\infty, 1\rangle\}?$$ Any partial information about the image is welcome.","['real-analysis', 'matrices', 'linear-algebra', 'functional-analysis', 'matrix-equations']"
4482101,Prove that there exists no integer $x > 99$ s.t. $x = \text{product of digits of }x + \text{sum of digits of }x$,"I define a number $x$ to be nice if $x = \text{product of digits of }x + \text{sum of digits of }x$ . For example, $$69 = (6 \times 9) + (6 + 9)$$ Motivated by the niceness of $69$ , I wish to characterise all nice numbers. I thought it would be best to slowly increase the number of digits and try to see a pattern. $2$ Digits $$x = \overline{ab} = 10a + b = ab + a + b \implies 9a = ab \implies 9 = b \; \text{as} \; a \neq 0$$ $$\therefore x = \{19, 29, 39, ..., 99\}$$ $3$ Digits $$x = \overline{abc} = 100a + 10b + c = abc + a + b + c \implies 99a + 9b = abc \implies 11a + b = \frac{abc}{9}$$ Testing $0 \le a, b, c \in \mathbb{Z} \le 9$ and $a \neq0$ s.t. $\frac{abc}{9} \in \mathbb{Z}$ , I find no solutions exist. It seems that as we increase the number of digits, no more nice
numbers exist. How do I prove this? Other interesting questions I thought of: nice numbers in different base systems? considering clusters of digits instead of individual digits? You don’t have to answer these questions, but if you want to give any insights on them, feel free to do so.",['number-theory']
4482144,Do linearly independent vector fields with vanishing Lie bracket always have integral manifolds which are level sets?,"Suppose I have a smooth distribution on an open subset of $M = R^n_{++}$ consisting of $n-1$ vector fields $X^i$ . I know three things about this distribution: At each point $p \in M$ the set $(X^1_p,\ldots,X^{n-1}_p)$ is linearly independent and so spans an $(n-1)$ -dimensional subspace of $R^n$ . For each pair $i,j, \; 1 \leq i < j \leq n-1, \; [X^i,X^j] = 0$ (so the system is in involution). At each point of $M$ the normal to the subspace spanned by this system is non-negative, with the last coordinate strictly positive. By 2, Frobenius tells us this system is integrable with its integral manifolds forming a foliation of $M$ . I understand this also means that there are local coordinates $x^i$ such that $X^i = \partial/\partial x^i$ . I have two questions: In this particular case does there always exist a single function $f: M \rightarrow R$ (smooth or not) such that the integral manifolds (hypersurfaces) are level sets of $f$ ; i.e. each leaf $L$ of the foliation is of the form $f^{-1}(x)$ for some $x \in R$ ? Is each leaf $L$ of the foliation globally the graph of some function $h:D \rightarrow R$ (i.e. $L = (x_1,\ldots,x_{n-1},h(x_1,\ldots,x_{n-1}))$ where $D$ is some domain in $R^{n-1}$ ? My sense is that the first statement is not true but (because of the third assumption) that the second is. As to 1., it has been suggested to me that if I looked at the distribution from the perspective of the normal field (i.e. 1-form approach) that my assumption that the Lie bracket vanishes is equivalent to the ""well-known"" integrability conditions insuring there exists a function f with the required property, but I'm not sure this is right. I would appreciate any insight into this (surely) simple issue that I'm just not seeing. Edit : In case it helps I can give a concrete example of the vector fields I'm working with when $n=3$ . $X^1 = (-m(x_1,x_2,x_3),1, 0), \; X^2 = (-n(x_1,x_2,x_3),0,1)$ . Here the smooth functions $m$ and $n$ are strictly positive everywhere on $M$ and bounded away from zero and $\infty$ on every compact subset of $M$ . I also simply impose the condition $[X^1,X^2] = mn_1 - nm_1 -n_2 +m_3 = 0$ . Note that the normal field is $N_{(x_1,x_2,x_3)} = (1,m(x_1,x_2,x_3),n(x_1,x_2,x_3)) \gg 0$ . Further clarification : Assumptions on m are, for each fixed $x_2,x_3$ that $\lim_{x \rightarrow \infty}m(x,x_2,x_3) = 0, \lim_{x \rightarrow 0}m(x,x_2,x_3) = \infty$ . A similar assumption holds for $n$ ; i.e. for each fixed $x_1,x_3$ $\lim_{x \rightarrow \infty}n(x_1,x,x_3) = 0, \lim_{x \rightarrow 0}n(x_1,x,x_3) = \infty$ .","['foliations', 'partial-differential-equations', 'differential-geometry']"
4482190,A Sturm–Liouville problem on the whole space,"Let $q \in C^\infty(\mathbb{R}) \cap L^\infty(\mathbb{R})$ be an even function, with $q(0)<0$ , and assume $q$ converges exponentially fast to $\bar{q}>0$ for $x \to \pm\infty$ . Let $f \in L^2(\mathbb{R})$ be an odd function. Then, I am looking for a solution of $$
-u'' + q(x)u = f(x)
$$ such that $u \in L^2(\mathbb{R})$ . The problem looks very innocent, reminiscent of a Sturm–Liouville problem (but on the whole of $\mathbb{R}$ ). However, the non-positivity of $q$ around $0$ jeopardizes many of my attempts to approach it. I also thought of going to the Fourier domain, but then I get stuck on an integral equation, as I am stuck on the above. Am I missing a reference where this problem is already extensively treated? Or otherwise, any suggestion on how to approach the existence of an $L^2$ solution?","['sturm-liouville', 'ordinary-differential-equations']"
4482201,Does the following condition make a function $L^2$?,"Suppose $u_n\to u$ point-wise, and $\|u_n\|_2<C$ for all $n$ . Does this imply $u\in L^2$ ? This is my attempt. By boundedness of $u_n$ , there's a subsequence that converges weakly to some function $f\in L^2$ . But how do I prove $f= u$ ?","['functional-analysis', 'real-analysis']"
4482202,"Should the vertical ""at bar"" go before or after the function being differentiated?","Suppose I want to calculate the derivative of a long function at a particular point $a$ . Is it more common to write $$
\frac{d}{dx} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x)\right) \Bigg|_{x=a}
$$ or $$
\frac{d}{dx}\Bigg|_{x=a} \left( x^2 \sin(x)^{(3x-1)^2} + \frac{e^x}{2 x^3 -2 x^2 +1} -3 \log(x) \right)?
$$ Does either notational convention have any advantages for clarity?","['notation', 'derivatives']"
4482219,Convergence to locally stable equilibria,"I am working with a system whose trajectories converge to the set of equilibria. I can characterize all the equilibria in a nice way and easily compute their stability and whether they are isolated or not. The dynamical system is described by a Lipschitz continuous vector field. I conjecture that the system is ""well behaved"" so that the trajectories converge (almost always, depending on their initial condition) to the set of locally stable equilibria. I know that this is not always the case as there are plenty of counterexamples, e.g. this classic system with a homoclinic trajectory ( example ). I am just wondering if there is any approach/result available in the literature to show that this is true, given certain conditions on the vector field.","['vector-fields', 'nonlinear-dynamics', 'ordinary-differential-equations', 'dynamical-systems']"
4482301,"For a Brownian motion on a Riemannian manifold, is the log of the transition probability proportional to the squared geodesic distance?","I am trying to gather some intuition on the connection on diffusion processes and Riemannian geometry, with only very limited knowledge of the latter. First, let us consider a Brownian Motion in euclidean space. For $\Sigma$ a positive definite matrix, $W_t$ standard $n-$ dimensional brownian motion, $X_t$ solving the stochastic differential equation $$ d X_t = \Sigma^{\frac{1}{2}} d W_t$$ we have that the density $p(x,y,t)$ for $X_t$ starting at $y$ solves the Fokker-Planck equation $$ \frac{d}{dt} p = \frac{1}{2} \Sigma_{ij} \frac{\partial}{\partial x_i} \frac{\partial}{\partial x_j} p$$ where we use Einstein summing convention. The density is given by $p(x,y,t) \propto \exp \left(\frac{-(x-y)^T \Sigma^{-1}(x-y)^T} {2t}\right)$ (where the proportionality is since we still require normalization). Now some geometry:
Let $(M,g)$ be a Riemannian manifold (we assume compactness and everything else we might need for the definitions to work). The gradient $\operatorname{grad}$ is defined implicitly by obeying the relation $\langle \operatorname{grad} f, X \rangle_g = d f(X)$ for any vector vield $X$ . The adjoint of the gradient is the divergence $\operatorname{div}$ . One then has the Laplace-Beltrami-operator on the manifold given by $$\Delta_M = \operatorname{div}\operatorname{grad}.$$ One can explicitly write the Laplace-Beltrami operator as $$\Delta_M f = \frac{1}{\sqrt{\det g}} \frac{\partial}{\partial x_j} (\sqrt{\det g} g^{ij} \frac{\partial}{\partial x_i} f)$$ where $g^{ij}$ refers to the inverse of the metric tensor. We now see that our Fokker-Planck equation includes a special case of the Laplace-Beltrami-Operator, where the manifold is just euclidean space and the metric is the constant $\Sigma^{-1}$ . Indeed, in these notes the definition of a Brownian motion on a manifold is taken to be the process which has density $p(x,y,t)$ , where $p$ is the minimal, positive solution to $$\frac{d}{dt}p =  \frac{1}{2} \Delta_M  p, \  p(x,y,0) = \delta_y(x).$$ Now on our manifold, we have the notion of a geodesic distance, which is given by $$ d_M(x,y) = \inf \{ L(\gamma) | \gamma \text{ a piecewise continuos curve from } x \text{ to } y\}.$$ In our standard euclidean case this is just a straight line. In particular, the geodesic distance on $E = (\mathbb{R}^n,\Sigma^{-1})$ should be given by $$d_E(x,y) = \sqrt{(x-y)^T \Sigma^{-1} (x-y)}.$$ Now by comparison with our density for the Brownian motion, we see that $$\ln p(x,y,t) \propto - d_E(x,y)^2.$$ This makes of course intuitively sense to be the generalization of the isotropic case: Diffusion is characterized by equiprobable motion in all directions, so by introducing a different covariane matrix of diffusion tensor we assume that we are in a space with some different metric, but still the transition probability only depends on this metric. My question is now, whether this property generalizes to the definition of Brownian motion on arbitrary Riemannian manifolds above. That is, do we have that $$\ln p(x,y,t) \propto - d_M(x,y)^2$$ on a general Riemannian manifold $M$ ? Intuitively, this should hold. Unfortunately, Im not familiar enough with differential geometry to prove this. What might be useful is that geodesics should fulfill the 'geodesic equation' in local coordinates $$\frac{\partial x_k} {\partial t^2} = -\Gamma^{k}_{ij} \frac{\partial x_i}{\partial t}\frac{\partial x_j}{\partial t} $$ with $\Gamma^{k}_{ij}$ the Christoffel symbol of the second kind. This also somewhat appears in the equation for the Laplace-Beltrami operator since $$ \Gamma^i_{ij} = \frac{1}{\sqrt{\det g}} \frac{\partial}{\partial x_j} (\sqrt{\det g})$$ but I don't know if this helps. I would appreciate any comments on the question, also if you know some good references to further study these connections.","['riemannian-geometry', 'stochastic-processes', 'stochastic-differential-equations', 'brownian-motion', 'differential-geometry']"
4482318,Real-world set theory/combinatorics problem,"I have a real-world problem, and, unfortunately, being an engineer, I feel compelled to solve it. I went to Walmart the other day and attempted to purchase eleven over-the-counter (OTC) health-related products totaling $104.07. At checkout, $$61.54  of the total was approved for payment by my insurance OTC benefit card, while $42.53 of the total was not approved (which I paid for in cash.) The receipt does NOT indicate which items were approved and which were not. The problem I'm trying to solve is: Which items were approved and which were not? The eleven items and their costs are as follows: Gauze pads       2.34 Alcohol          3.48 Antibiotic cream 4.12 Lidocaine cream  4.94 Bandages         4.97 Hydrocortisone   7.12 Peptobismol      8.56 Toothpaste      10.59 Melatonin       13.76 Ibuprofan 19.71 Loratadine 24.48 In summary:
There are eleven elements in total divided into two sets. Set ""A"" contains ""n"" elements totaling $61.54. Set ""B"" contains ""11-n"" elements totaling $42.53. What are the ""n"" elements contained in set ""A""? I don't have a clue as to how to go about solving this. I do realize, however, that there may be more than one solution to this problem. Edit: Taxes have been removed from the totals.","['elementary-set-theory', 'combinatorics']"
4482344,"Let $\mu, \nu$ both have finite second moments and $\nu \in \Pi(\mu, \nu)$. Then the inner product $\langle \cdot, \cdot\rangle$ is $\pi$-integrable","In proving Knott-Smith optimality , I come across this result. Could you have a check if my attempt is fine, or it contains some logical mistakes? Theorem: Let $\mu, \nu$ be Borel probability measures on $X:=\mathbb R^d$ such that $\mu, \nu$ both have finite second moments. Let $\pi$ be a Borel probability measure on $X^2$ with marginals $\mu, \nu$ . Then the inner product $\langle \cdot, \cdot\rangle$ is $\pi$ -integrable. My attempt: Clearly, $\langle \cdot, \cdot\rangle$ is continuous and thus Borel measurable. Because $X^2$ is separable, $\langle \cdot, \cdot\rangle$ is $\pi$ -measurable. We have $$
2\langle x, y\rangle = |x|^2 + |y|^2 - |x-y|^2,
$$ so $$
\begin{align}
2\int_{X^2} |\langle x, y\rangle| d \pi(x, y) &= \int_{X^2} \big | |x|^2 + |y|^2 - |x-y|^2 \big | d \pi(x, y) \\
& \le \int_{X^2} \big [ |x|^2 + |y|^2 + |x-y|^2 \big ] d \pi(x, y) \\
& \le 3 \int_{X^2} \big [ |x|^2 + |y|^2 \big ] d \pi(x, y) \\
& = 3 \int_{X} |x|^2 d \mu(x) + 2\int_{X} |y|^2 d \nu(y) \\
&< +\infty.
\end{align}
$$ This completes the proof.","['measure-theory', 'probability-theory', 'real-analysis']"
4482378,Gâteaux differentiable function (with linear continuous Gateaux derivative) that is not Fréchet differentiable,"Currently, I am following a portion of the text Methods of Nonlinear Analysis . For normed real vector spaces $(X,\|\cdot\|_X),(Y,\|\cdot\|_Y)$ and $a \in X$ , consider the following definitions from this text. Directional Derivative: Let $h \in X$ . If the limit $$\lim_{t \to 0;\ t \in \mathbb{R}} \frac{f(a+th)-f(a)}{t}$$ exists in $Y$ , then the value of the limit is called the derivative of $f$ at $a$ in the direction $h$ and is denoted $\delta f(a;h)$ . Gâteaux Derivative: Suppose $\delta f(a;h)$ exists for all $h \in X$ . If the function $Df(a) : X \to Y$ where $h \mapsto \delta f(a;h)$ is continuous and linear, then $Df(a)$ is called the Gâteaux derivative of $f$ at $a$ . Fréchet Derivative: If there exists a continuous linear transformation $A : X \to Y$ such that $$\lim_{\|h\|_X \to 0} \frac{\|f(a+h)-f(a)-Ah\|_Y}{\|h\|_X}$$ then we call $A$ the Fréchet derivative of $f$ at $a$ and denote it $f'(a)$ . Question: I am struggling to find an example of $X,Y,a,f$ such that the Gâteaux derivative of $f$ exists at $a$ but the Fréchet derivative of $f$ does not exist at $a$ . Can anyone help provide such an example? Please note that the Gâteaux derivative must be linear and continuous. Since $f$ having a Fréchet derivative at $a$ implies the continuity of $f$ at $a$ , I imagine there exists an example with $X = \mathbb{R^2}, Y = \mathbb{R}, a = (0,0)$ such that $Df(0,0)$ exists but $f$ is not continuous at $(0,0)$ . Among many other online sources, I have read the following stack exchange posts but I believe that each asks a slightly different question or uses a slightly different definition of the Gâteaux derivative: Example of a continuous and Gâteaux differentiable function that is not Fréchet differentiable. What is an example of Gâteaux differentiable but not Fréchet differentiable at a point in a finite-dimensional space? Question about discontinuous function with directional derivatives at a points","['frechet-derivative', 'gateaux-derivative', 'multivariable-calculus', 'examples-counterexamples']"
4482382,Differentiability of a piecewise defined discontinuous function,"Let $ f(x) = \begin{cases}
x-4 & \text{if } x \lt 1; \\
x+1 & \text{if } x > 1; \\
0 & \text{if } x = 1.
\end{cases}$ Why isn’t this function differentiable at 1? Why isn’t its derivative=1 at x=1? Both left and right sides have their derivatives equal to $1$ as $x$ approaches $1$ . I can see that the left hand and right hand derivatives do not agree at $x=1$ . However, could someone explain that to me intuitively, that is, without using the definition of the derivative?","['calculus', 'derivatives', 'slope']"
4482388,"""Universal family"" used despite moduli space not being fine?","This is something that has nagged me for a while, and I think I basically know the issue, but thought I would see what people had to say. In the study of moduli of curves, one will sometimes see the following morphism referred to as the ""universal family"" or the ""universal genus $g$ curve"": $$\pi : M_{g,1} \to M_{g}.$$ Here $M_{g}$ is the coarse moduli space of smooth genus $g$ curves, and $M_{g,1}$ is the moduli space of smooth genus $g$ curves with one marked point. It's pretty clear intuitively why $\pi$ above would be a candidate for the universal family, but we know the moduli space of smooth curves is not fine! So equivalently, there is no universal family. I am under the impression that this is basically an abuse of terminology. In other words, $\pi: M_{g,1} \to M_{g}$ is not THE universal family---there are non-trivial families of curves with automorphisms that do not pull back from $\pi$ . But it is a family where every curve appears once, so people via an abusive of terminology call it the universal family. Is my idea here correct? If so, is this something to be wary of in moduli theory--people speaking about universal families despite the moduli problem not being representable?","['algebraic-curves', 'algebraic-geometry', 'moduli-space']"
4482468,(complex)Let $f: D \subset \mathbb R^2 \to \mathbb R^2$ be a function defined on an open subset $D \subset \mathbb R^2$...,"Let $f: D \subset \mathbb R^2 \to \mathbb R^2$ be a function defined on an open subset $D \subset \mathbb R^2$ . Remember that $f$ is said to be differentiable (in the Frechét sense) at (x_0, y_0) \in D if there is a linear map $Df(x_0, y_0) : \mathbb R^2 \to \mathbb R^2$ such what $$\lim_{(h,k) \to (0,0)} \frac{f((x_0,y_0) + (h,k)) – f(x_0, y_0) – Df(x_0, y_0)( h,k)}{\vert\vert(h,k)\vert\vert} = 0$$ Show that $f$ is differentiable in the complex sense at $z_0 = x_0 + iy_0$ if and only if $f$ is differentiable in the Frechét sense and $Df(x_0, y_0)$ defines a linear $\mathbb C$ mapping of $\mathbb C$ into $\mathbb C$ i.e. $Df(x_0, y_0)(z + \lambda z' ) = Df(x_0, y_0)z + \lambda Df(x_0,y_0) z', \forall z, z', \lambda \in \mathbb C$ . $f$ is differentiable in $z_0$ if there is $f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}$ . $\lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} - f'(z_0) = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0) - f'(z_0)h}{h}$ . $f'(z_0)(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h} (z + \lambda z')=  \lim_{h \to 0} \frac{f(z_0 + h) - f(z_0)}{h}(z + \lambda z') = \lim_{h \to 0} \frac{f(z_0 + h)z +f(z_0 + h)\lambda Z'}{h} - \frac{f(z_0)z + f(z_0)\lambda z'}{h} = \lim_{h \to 0}\frac{f(z_0 + h)z - f(z_0)z}{h} + \frac{f(z_0 + h)\lambda Z'-\lambda z'}{h} = f'(z_0)(z) + f'(z_0)(\lambda z')$ That's what I managed to come up with, is it acceptable? Grateful for the attention.","['solution-verification', 'limits', 'derivatives', 'problem-solving', 'complex-numbers']"
4482480,Moebius transformations preserving unit circle,"Find all Moebius Transformations preserving unit circle Note : I am more interested if I got these computations right than the answer. Approach-1 From page-124 of Needham, a general moebius transformation of form $f(z) = \frac{az+b}{cz+d}$ can be decomposed as: $z \to z + \frac{d}{c}$ (translate) $z \to \frac{1}{z}$ (inversion) $z \to -\frac{ad-bc}{c^2}z$ (rotation and dilation) $z \to z + \frac{a}{c}$ (translate) It is clear to me that if it is to preserve the unit circle than 1. and 4. must be opposite , so I have: $$ \frac{d}{c} = - \frac{a}{c} \implies d = -a \tag{1}$$ We also need that in 3. the magnitude should be preserved ( no other dilations), we have: $$ c^2 = e^{it} ( a^2+bc)  \implies c^2 e^{-it} -bc = a^2 \implies \pm \sqrt{c^2 e^{-it} -bc }=a\tag{2}$$ Putting the $d=-a$ in our function and then using (2), we have: $$ f(z) = \frac{\pm \sqrt{c^2 e^{-it} -bc }z+b}{cz-\pm \sqrt{c^2 e^{-it} -bc }}$$ Is this all the simplification possible or is it possible to kill any more variables? Approach-2 This is based on a discussion with a friend. The idea is to begin with the symmetric principle i.e: symmetric points are mapped to symmetric point under moebius transformations, we have: $$ w= f(0) = \frac{b}{d}$$ $$ \frac{1}{w} = \frac{a}{c}$$ This leads to the following constraint: $$ w=  \frac{b}{d} = \frac{c}{a}$$ Or, $$ b = dw$$ and, $$ c=aw$$ Leading to: $$ f(z) = \frac{az+dw}{awz+d}= \frac {\frac{az}{d} + w}{\frac{a}{d} wz + 1}$$ Letting $\frac{a}{w} = \lambda$ , we have: $$ f(z) = \frac{\lambda z+w}{\lambda w z +1} \tag{3}$$ Now, without loss of generality let's suppose that point $1$ gets sent to some point $e^{it}$ on the unit circle, we have: $$ \frac{\lambda + w}{ \lambda w +1} = e^{it}$$ $$ \lambda ( 1-e^{it} w)= e^{it} -w$$ $$ \lambda = \frac{e^{it} - w} {1- e^{it} w} \tag{4}$$ Can the expression received after plugging (3) into (4) be simplified any more? Also, how exactly could I know how many independent variables there would be at them end of this problem?","['complex-analysis', 'solution-verification', 'complex-numbers', 'mobius-transformation']"
4482504,Showing that the Lie-algebra $\mathfrak{g}$ of $G \subset SL(V)$ is semisimple and irreducible on $V$.,"Let $G \subset SL(V)$ be a connected algebraic group, acting irreducible on $V$ , where $V$ is a complex vectorspace of dimension $n$ I want to show that the Lie algebra $\mathfrak{g}$ of $G$ is semisimple and acts irreducible on $V$ . Below is the full answer to this problem. Thanks goes to Callum and Bart Michels for their hints, that greatly helped to getting this solution. Irreducibility: We have $\mathfrak{g} \subset \mathfrak{sl}(V)$ a Lie subalgebra. Now note, that by assumption $G$ acts irreducible on $V$ via an representation say $\Phi$ . Let $\phi$ be the associated representation for $\mathfrak{g}$ . To show: $\Phi$ irreducible $\Leftrightarrow$ $\phi$ irreducible: So if we have a subspace of $V$ , invariant
under the action of every $X \in \mathfrak{g}$ , it is invariant under $\phi(X)^n$ for all $n$ , since $V$ is finite-dimensional. Hence it must be invariant under $exp(\phi(X))$ . Using that, if $G$ is a connected Lie group, it is generated by exponentials of elements of $\mathfrak{g}$ , we get one implication.  Conversely, if a subspace is invariant under $G$ , it must be invariant under each $\Phi(exp(hX))$ and hence also under the limit $$lim_{h \to 0}\left(
\frac{\Phi(exp (hX)) − \Phi(I)}{h}\right)
= \phi(X).$$ Semi-simplicity: To show, that $\mathfrak{g}$ is semisimple, we need to prove that the center of $\mathfrak{g}$ is trivial. That is $$\mathfrak{z}(\mathfrak{g})=\{x \in \mathfrak{g}\mid[x,s]=0 ,\forall s \in \mathfrak{g}\}.$$ Since we consider a Lie-subalgebra of $\mathfrak{sl}(V)$ , we have $[x,s]=xs-sx$ . Now, by the irreducibility of the representation together with Schur´s lemma, we have that all elements in the center are multiples of the identity matrix. But since $\mathfrak{g} \subset \mathfrak{sl}(V)$ , and hence the traces need to be zero, we conclude that the center is trivial. Edit: Let $\mathfrak{h}$ be an abelian ideal and $V = \oplus_{\lambda \in\Lambda} V_{\lambda}$ the decomposition of $V$ into joint eigenspaces $$V_{\lambda} = \{v \in V : \forall X \in \mathfrak{h}, (X - \lambda(X))v=0\}.$$ First we show, that forall $\lambda$ we have that $V_{\lambda}$ is $\mathfrak{g}$ -invariant. Let $v \in V_{\lambda}$ , $X \in \mathfrak{h}$ then for any $Y \in \mathfrak{g}$ we note by the comment below $$(X - \lambda(X))Yv = [X,Y]v -\lambda(X)Yv + YXv = [X,Y]v$$ as $v \in V_{\lambda}$ . Observe that $[X,Y] \in \mathfrak{h}$ and so $[X,Y]v \in V_{\lambda}$ , so we have $$(X - \lambda(X))^2Yv = (X - \lambda(X))[X,Y]v=0.$$ Recall that $X \in End(V)$ is said to be diagonalizable, if $V$ has a basis of eigenvectors for $X$ . But this is equivalent to our decomposition $V = \oplus_{\lambda \in\Lambda} V_{\lambda}$ .
So $X$ is diagonalizeable. Then, since the generalized eigenspaces coincide with the normal eigenspaces, this imples $Yv \in V_{\lambda}$ . Since by the first part (hopefully), $\mathfrak{g}$ acts irreducible on $V$ , there can only be trivial $\mathfrak{g}$ -invariant subspaces. So with the decomposition above we conclude $V = V_{\lambda}$ for some $\lambda \in \Lambda$ . Hence $\forall v \in V, \forall X \in \mathfrak{h}: (X - \lambda(X))v=0$ , i.e. $\forall v \in V, h \in \mathfrak{h}: Xv=\lambda(X) v$ . So $\mathfrak{h}$ acts by scalars on $V$ . But then again, the elements of $\mathfrak{h}$ being scalar multiples of the identity map, contradicts $\mathfrak{h} \subset \mathfrak{g} \subset \mathfrak{sl}(V)$ as the traces need to be $0$ . So $\mathfrak{h}=0$ .","['group-theory', 'lie-algebras', 'lie-groups']"
4482508,Imaginary Numbers in the Ring of $p$ Adic Integers?,"I am currently reading the book ""ultrametric calculus: an introduction to $p$ adic analysis"" by Schikhof, and I came across this problem: ""Prove that $x^2+1=0$ has no solutions in $\mathbb{Z}_3$ but has two solutions in $\mathbb{Z}_5$ ."" Here, $\mathbb{Z}_p$ denotes the ring of $p$ adic integers.  The issue that I am having is that the book has never given any theorems for irreducibility in $\mathbb{Z}_p$ or any helpful theorems related to finding solutions to polynomials $f(x)\in\mathbb{Z}_p[x]$ up until this point, so I am assuming that there must be some way to do this solely on the construction of $\mathbb{Z}_p$ ? I am unsure on how I could do this, and could use some help as I have completely run out of ideas.  Any help is appreciated.","['number-theory', 'p-adic-number-theory', 'analysis']"
4482532,Find all entire functions $f$ such that $|f(z)| \leq |\sin(z)|$,"I am trying to solve the following exercise: Find all entire functions $f$ such that $|f(z)| \leq |\sin(z)|$ , $\forall z \in \mathbb{C}$ I think Liouville's Theorem is the way to go. Liouville's Theorem states that: Every bounded entire function must be constant. Since $\cos(z)=0$ for $z=\frac{2k+1}{2} \pi$ , my answer would be that the only entire function is the zero function $g\equiv 0$ . Am I correct? Edit: I got a little bit confused, because in $\mathbb{R}$ , sin is bounded with $|\sin(x)|<1$ .
Because of this I thought that I only need to search constant functions f, such that $|f(z)| \leq |\sin(z)|$ . This is why I thought that the Zero-Function is the only option. Considering the comments, $f(z):= a \sin(z)$ with $|a| \leq 1$ also fullfill the condition wanted. How can I proof that these are all function?",['complex-analysis']
4482538,Random walk with a killing probability,"Consider a simple symmetric random walk $(S_n)_{n \geq 0}$ with $S_0 = 0$ on $\mathbb{Z}.$ We add the killing probability: At each step, with probability $\alpha$ , the random walk is killed and the process stops. With probability $1-\alpha$ , the random walk continues. Let $$\tau_1 = \min \{n : S_n = 1\}.$$ Set $\tau_1 = \infty$ if the process is killed before the first time it reaches $1.$ I want to find $\mathbb{P}(\tau_1 < \infty).$ To me, this is the probability that the random walk reaches $1$ before it is killed. I am struggling with this problem (I am doing practice qualifying exams and this was a question on an earlier exam, so obviously, no solution is provided). My first thought was to calculate $$\sum_{n=0}^{\infty}\mathbb{P}(S_n = 1) (1- \alpha)^{2n+1}.$$ However, I now know this is completely wrong. I know that you can only get to $1$ in an odd number of steps but $\mathbb{P}(S_n=1)$ includes the times that the random walk goes above $1$ and then gets back to $1$ in $n$ steps, which I do not want. So, I am completely stuck. Does anyone have any ideas? Thanks!","['stopping-times', 'probability-theory', 'random-walk']"
4482571,Find all polynomials $p(x) \in \mathbb{C}[x]$ such that $p(\mathbb{R}) \subset \mathbb R $ and $p(\mathbb{C - R}) \subset \mathbb{C - R }$.,"Find all polynomials $p(x) \in \mathbb{C}[x]$ such that $p(\mathbb{R}) \subset \mathbb R $ and $p(\mathbb{C - R}) \subset \mathbb{C - R }$ . Note that $ \mathbb C =\{a+bi\mid a,b \in\mathbb R \}$ . For example let $0 \neq a_0,c_0 \in\mathbb R $ then $p(x)=a_0x +c_0$ is one answer.","['abstract-algebra', 'polynomials', 'complex-numbers']"
4482593,Wedge Product and Cross Product,"I am trying to understand the wedge product. I know that: It acts only on p-forms (and hence will act on the dual vector space for my question ahead), It is different from cross-product in general (henceforth, I will restrict my question to $\textbf{R}^3$ only). I am following the notes by David Tong . On page 77, he says: As a more specific example, consider $M = \textbf{R}^3$ and $ω = ω_µdx^µ$ and $η = η_µdx^µ$ . We then have $$ω ∧ η = (ω_1dx^1 + ω_2dx^2 + ω_3dx^3
) ∧ (η_1dx^1 + η_2dx^2 + η_3dx^3)
= (ω_1η_2 − ω_2η_1)dx^1 ∧ dx^2 + (ω_2η_3 − ω_3η_2)dx^2 ∧ dx^3 + (ω_3η_1 − ω_1η_3)dx^3 ∧ dx^1$$ Notice that the components that arise are precisely those of the cross-product acting
on vectors in $\textbf{R}^3$ . This is no coincidence: what we usually think of as the cross-product between vectors is really a wedge product between forms I don't understand the last line. Is it because the dual space and normal vector space are the same? So, working with duals will fetch us the result. Because clearly, vector and dual vector have different components and bases in general.","['manifolds', 'differential-geometry']"
4482607,Number of lattice points with the same parity inside a triangle,"Given a point $(a,b)$ with positive coordinates, I'd like to count the number of lattice points $(x,y)$ with the same parity (i.e., $x \equiv y \ (mod \ 2)$ ) inside the triangle $(0,0)(a,0)(a,b)$ . How to compute it fast? Note that without the parity requirement, it can be done using Pick's theorem directly.","['elementary-number-theory', 'combinatorics', 'geometry']"
4482615,Prove $f$ is a polynomial,"Let $f$ be defined on $\mathbb{C}$ be holomorphic. Suppose there exists and $n \in \mathbb{Z}^+$ such that $$\int_{\partial B_1(0)}\frac{f(z)}{(z-a)^n}=0$$ for all $a \in B_1(0)$ . prove $f$ is a polynomial. Here is what I have, by corollary of Cauchy's theorem we have that $$f^{(n-1)}(a)(2 \pi  i)/(n-1)!=0$$ Which implies $f^{(n-1)}(a)=0$ and this implies $f$ is a polynomial since its derivative at some point eventually is zero, specially on some finite radius ball.","['complex-analysis', 'solution-verification']"
4482632,What is wrong with my proof that $\int 2x dx= 2x^2$ by writing $2x=\underbrace{2+2+\cdots+2}_{x\;\text{times}}$?,"I know $\int 2x \,dx = x^2 + C$ (by the power rule) but why does the following proof not give the same answer? \begin{align*}
\int 2x \,dx &= \int \underbrace{(2 + 2 + 2 + \dots + 2)}_{x \text{ times}} \, dx \\
             &= \underbrace{\int{2} \, dx + \int{2} \, dx + \dots \ + \int{2}_ \, dx}_{x \text{ times}}\\
             &= 2x + 2x + \dots + 2x + C \\
&= 2x \times x + C \\
&= 2x^2 + C
\end{align*} (And I have the same question for this false proof that $\int{2^x} \, dx = 2^{x}x+ C$ ) \begin{align*}
\int{2^x} \,dx &= \int \underbrace{(2 \cdot 2 \cdot 2 \cdot \dots \cdot 2)}_{x \text{ times}} \cdot 1 \, dx \\
             &= 2 \cdot \int \underbrace{(2 \cdot 2 \cdot 2 \cdot \dots \cdot 2)}_{(x-1) \text{ times}} \cdot 1 \, dx && (\text{Constant Multipule Rule})\\
             &= 2^2 \cdot \int \underbrace{(2 \cdot 2 \cdot 2 \cdot \dots \cdot 2)}_{(x-2) \text{ times}} \cdot 1 \, dx && (\text{Constant Multipule Rule})\\
             &= 2^x \cdot \int{1} \, dx \\
&= 2^{x}x+ C \\
\end{align*} I suspect that it has something to do with not being able to: Change integral of sums to sums of integrals for an arbitrary $x$ , and Remove a constant out of an integral if there are variable numbers of those constants. But I'm not sure why these do not hold. If this is the reason, is there a theorem stating it? Thanks in advance!","['integration', 'calculus', 'solution-verification', 'fake-proofs']"
4482644,"For an infinite sequence of real numbers that monotonically grows, is infinity considered a limit?","My question is on the topic of ""infinity"" as a limit. Does a sequence like $1,2,3,4,5,6,7,\ldots$ have a limit of infinity, or is it considered as not having a limit. If infinity is considered a limit, is this sequence considered convergent then? My reason for this question is to further distinct these two sequences: $1,2,1,3,1,4,1,5,1,6,1,\ldots$ and $1,2,4,3,7,4,10,5,\ldots$ As per my last question from a previous post My professor stated that the first sequence is considered as ""not going to infinity"", while the second one does. He hasn't given a formal definition of what that means, but I'm going to assume that means $\lim a_n = \infty$ and $\lim a_n$ does not exist. If infinity is considered a limit point, would that make the second sequence convergent, converging to infinity, while the first sequence as being divergent, with the limit not existing? In that case, wouldn't this clash with some definitions? In a wikipedia article, it says ""Zaporedje $2, 4, 6, 8, 10,\cdots$ je divergentno in ima nepravo stekališče neskončno"" which in translation means ""The sequence $2, 4, 6, 8,\cdots$ is divergent and it has the false limit point of infinity"" But as previously stated, this form of sequence would have indeed a limit, of infinity, and thus would be convergent?","['metric-spaces', 'real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence']"
4482646,Sheldon Axler Real Analysis Exercise 16 Section 3A,"This is an exercise I've been working a whole day. I wrote a solution involving Zorn's lemma, but I'm not really satisfied using this non-standard result. I'm wondering if there is an easier solution that gets around this lemma. Suppose $\mathcal{S}\subset\mathcal{T}$ are $\sigma$ -algebras on $X$ , $\mu_1$ and $\mu_2$ are measures on $(X,\mathcal{S})$ and $(X,\mathcal{T})$ , respectively, and $\mu_1(E)=\mu_2(E)$ $\forall \,E\in\mathcal{S}$ . Prove that if $f:X\to[0,\infty]$ is $\mathcal{S}$ -measurable, then $\int fd\mu_1=\int fd\mu_2$ . Edit: $(X,\mathcal{S})$ and $(X,\mathcal{T})$ are generic measurable spaces, and my definition of integral is $\int fd\mu=\sup\{\sum_{A_i\in P}\mu(A_i)\inf_{A_i}(f)|P\text{ is a partition of } X\}=\sup\{\sum_i c_i\mu(A_i)|\sum_i c_i\chi_{A_i}\leq f\}$ .
Here the $c_i$ can be chosen arbitrarily from $\mathbb{R}$ , so I didn't expect to partition the range of $f$ . My reasoning: Cleary $\int fd\mu_1\leq\int fd\mu_2$ since $\mathcal{S}\subset\mathcal{T}$ , so we need the inequality on the other direction. No matter how our strategy is, we need to say that for any partition of $\{F_i\}\subset2^\mathcal{T}$ , we can find another partition $\{E_i\}\subset2^\mathcal{T}$ that approximates the integral better. The choice of $\{E_i\}$ clearly will depend on $\{F_i\}$ , so we kind of need to find a refinement using only sets from $\mathcal{S}$ . The problem is that if some $F_i\in2^\mathcal{T}-2^\mathcal{S}$ then there will be some elements that ""goes out of"" $F_i$ and the refinement will not work. However we can go around this if we can find a $F_i\subset E_i$ such that $f$ has the same range on them, and here comes Zorn's lemma. Proof: By Zorn's lemma, there exists a minimal set $E_i\in\mathcal{S}$ such that $F_i\subset E_i$ . Then $$f(F_i)\subset f(E_i)$$ and $$F_i\subset f^{-1}(f(F_i))\cap E_i\subset f^{-1}(f(E_i))\cap E_i=E_i$$ . But $f$ is $\mathcal{S}$ -measurable, so $f^{-1}(f(F_i))\cap E_i\in\mathcal{S}$ , and by minimality $f^{-1}(f(F_i))\cap E_i=E_i\Longleftrightarrow E_i\subset f^{-1}(f(F_i))$ . Finally $$f(E_i)\subset f(f^{-1}(f(F_i)))\subset f(F_i)\subset f(E_i)\Longrightarrow f(F_i)=f(E_i)$$ . Now we can consider the $m$ subsets $\tilde{E_j}$ generated by intersection of the $n$ $E_i$ s: These form a partition. Then $$\sum_{F_i}\mu_2(F_i)\inf_{F_i}(f)=\sum_{F_i}\sum_{\tilde{E_j}}\mu_2(F_i\cap\tilde{E_j})\inf_{E_i}(f)\leq\sum_{\tilde{E_j}}\sum_{F_i}\mu_2(F_i\cap\tilde{E_j})\inf_{\tilde{E_i}}(f)=\sum_{\tilde{E_j}}\mu_1(\tilde{E_j})\inf_{\tilde{E_i}}(f)$$ as desired.","['integration', 'measure-theory', 'real-analysis']"
4482654,Is there a well-defined exponential of an integral operator?,"I’ve seen $e^{\frac{d}{dx}}$ yield the shift operator, but would $e^{I}$ where $If(x)\equiv \int f(x)dx$ yield something interesting as well (or even something at all)? I’d imagine that it’s defined as the integral operator is better behaved than the differentiation operator, but I it interesting and does this operator have a name? $e^I$ is just shorthand for $\sum_{n=0}^{\infty} \frac{I^n}{n!}$ as is oftentimes done in operator theory and related fields.","['integration', 'operator-theory', 'exponential-function', 'functional-analysis']"
4482659,"Can a ""squashed"" sequence of consecutive integers be unambiguously separated?","Cross-posted to MO . I have thought of an interesting math problem while looking through the code golf website. It is concerning the challenge Decipher a squashed sequence . Here is a quick rundown of what a squashed sequence is: Take a sequence of ascending consecutive positive integers (of which there are at least two numbers), and concatenate them into one string. The concatenated result is a squashed sequence. The challenge asks you to reverse the process. Essentially, given a squashed sequence, you have to return the list of integers that formed the squashed sequence. For example, given the squashed sequence "" $1011121314$ "", the output would be $[10,11,12,13,14]$ . My question is: Does there exist a squashed sequence which can be formed by more than one set of numbers (at least two numbers in each set)? If so, give an example of such a squashed sequence. Otherwise, why not?","['number-theory', 'recreational-mathematics', 'integers', 'elementary-number-theory']"
4482764,Couldn't understand decomposition inside derivation of option valuation using Martingale method,"In the derivation of option valuation using Martingale method in continuous time framework of my book named ""Brownian Motion Calculus by Ubbo F Wiersema"" I have faced some issue. I add the derivation below: In the continuous time setting, start from the standard SDE for the stock price, $$\frac{dS_t}{S_t}=\mu dt+\sigma dB_t$$ Introduce the discounted stock price, $$S^{\star}_t\stackrel{\text{def}}{=}\frac{S_t}{e^{rt}}$$ Ito's formula for $S^{\star}_t$ as a function of $t$ and $S$ gives, $$\frac{dS^{\star}_t}{S^{\star}_t}=(\mu-r)dt+\sigma dB_t=\sigma\left[ \frac{\mu-r}{\sigma}dt+dB_t \right]=\sigma\left[ \phi dt+dB_t \right]$$ The probability density of $B_t$ at $B_t=x$ is, $$\frac{1}{\sqrt t \sqrt{2\pi}}\exp\left[-\frac12\left(\frac{x}{\sqrt t}\right)^2\right]$$ This can be decomposed into the product of two terms, $$\frac{1}{\sqrt t \sqrt{2\pi}}\exp\left[-\frac12\left(\frac{\phi t+x}{\sqrt t}\right)^2\right]\exp\left[\frac12 \phi^2 t+\phi x\right]$$ With $y \stackrel{\text { def }}{=} \varphi t+x$ the first term can be written as $$
\frac{1}{\sqrt{t} \sqrt{2 \pi}} \exp \left[-\frac{1}{2}\left(\frac{y}{\sqrt{t}}\right)^{2}\right]
$$ which is the probability density of another Brownian motion, say $\widehat{B}(t)$ , at $\widehat{B}(t)=y$ . It defines $\widehat{B}(t) \stackrel{\text { def }}{=} \varphi t+B(t)$ , so $\widehat{d} \widehat{B}(t)=\varphi d t+d B(t)$ . Substituting the latter into the SDE for $S^{\star}$ gives $$
\frac{d S^{\star}(t)}{S^{\star}(t)}=\sigma d \widehat{B}(t) \quad \text { and } \quad \frac{d S(t)}{S(t)}=r d t+\sigma d \widehat{B}(t)
$$ This says that under the probability distribution of Brownian motion $\widehat{B}(t)$ , $S^{\star}$ is a martingale. I couldn't understand how they bring the decomposition like this? Another thing is, ""Why we need to sustain martingale?"". Does it mean we couldn't predict the future regardless of all prior knowledge, as it introduce arbitrage? @Ali, give an answer to show the decomposition. $$
\exp\left[-\frac{1}{2}\left(
\frac{\phi t + x}{\sqrt{t}}\right)^{2}\right]=
\exp\left[-\frac{1}{2}\left(
\phi^{2}t+2\phi x + x^{2}/t\right)\right]=
\exp\left[-\frac{1}{2}
\phi^{2}t-\phi x\right]
\exp\left[-\frac{1}{2t}x^{2}\right]
$$ But, still, I couldn't get any intuition of that decomposition. Like, I want to know why this decomposition was introduced, and its meaning in the context of option valuation. Does this related with Girsanov transformation ?","['martingales', 'measure-theory', 'stochastic-calculus']"
4482903,Understanding the use of compactness in getting a variation of an admissible curve from a vector field,"I came across the following result in the book ""Riemannian Manifolds: An Introduction to Curvature"" by John Lee. Theorem: Let $\gamma: \left[ a, b \right] \rightarrow M$ be an admissible curve and $V$ be a vector field along $\gamma$ . Then, $V$ is the variation field of some variation of $\gamma$ . Moreover, if $V$ is proper then the variation can be chosen to be proper. The proof of this result uses the fact that $\left[ a, b \right]$ is compact. Particularly, this is used to define the variation of $\gamma$ as $\Gamma \left( s, t \right) = \exp \left( s V(t) \right)$ . I am  not sure if I understand this properly. Here are my thoughts on how it is used. We know that $\exp$ is defined on an open set containing the zero section. That is, there is an open set $\mathscr{E} \subseteq TM$ (the tangent bundle) such that for all $p \in M$ , $\left( p, 0 \right) \in \mathscr{E}$ . Therefore, for each $p \in M$ , there are balls $B^{\left( p \right)} \left( 0, \delta_p \right) \subseteq \mathscr{E} \cap T_pM$ . Now, we see that $V(t) \in \frac{2 \| V(t) \|}{\delta_p} B \left( 0, \delta_p \right)$ so that these (scaled) balls form an open cover for $V \left( \left[ a, b \right] \right)$ (am I right here?). Once this is done, we get a finite subcover and hence there is some $\epsilon > 0$ such that for $\left| s \right| < \epsilon$ , we have $s V(t) \in \mathscr{E} \cap T_{\gamma \left( t \right)} M$ . Is this reasoning correct? It seems to me that there will be some disjoint unions involved and we may not be able to use the scaled balls to conclude what we want. Any thoughts about this are appreciated!","['riemannian-geometry', 'differential-geometry']"
4482917,Closed form for $\int_ 0^{1/2} {\frac {x} {3 +4 x^2}\ln\frac {\ln\left (1/2 +x \right)} {\ln\left (1/2 - x \right)}\mathrm {d} x} $ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Find the integral $$\int_ 0^{1/2} {\frac {x} {3 + 4 x^2}\ln\frac {\ln\left (1/2 + x \right)} {\ln\left (1/2 - x \right)}\mathrm {d} x} $$ Wolfram | Alpha tells me that the answer is $ - 0.0951875 ... $ ,
and its possible closed form is $ - \frac18\ln2\ln3$ . But I don't know how to do it. Thanks in advance.","['integration', 'indefinite-integrals', 'closed-form']"
4482975,Approximating $\sum_{p\in\Bbb P} \frac1{p\ln p}$,"It's known that the sum $$\sum_{p\in\Bbb P} \frac1{p\ln p} \approx 1.6366$$ converges approximately to the indicated value, see here for example. How is this approximation calculated? The series converges painfully slow.  For example, when naively summing up to primes below $10^7$ , the sum is $0.06$ off the real value (assuming all digits of "" $1.6366$ "" are correct), whereas the summands are of size around $10^{-8}$ : sum(10000000) = 1.5745747442808875 , delta = 6.20e-09 So what's the technique / transformation used to speed up the computation and to keep rounding errors at bay? FYI, here is the ad-hoc Sage script which produced the line above: #!/usr/bin/env sage

def f(x):
    return float(1 / (p * log(p)))

bound = 1e7

s = 0
for p in Primes():
    if p > bound:
        break
    s += f(p)

print (""sum(%.0f) ="" % bound, s, "", delta = %.2e"" % f(bound))","['numerical-methods', 'prime-numbers', 'sequences-and-series']"
4482990,"Follow up question to question involving $\lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}$","I tried to answer Q 4482921 by elementary calculus but got stuck : The first step in my solution is to replace $x$ by $2u$ so that $dx=2du$ . The integral becomes $$\lim_{n\to \infty}\frac{\int_0^1 \left(x^2-x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}= \lim_{n\to \infty}\frac{\int_0^\frac{1}{2}\left(4u^2-2u-2\right)^n \ 2du}{\int_0^1 \left(4 x^2-2 x-2\right)^n \, dx}$$ $$=2\left(1-\lim_{n\to \infty}\frac{\int_
\frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n \,
dx}\right)$$ Now I need to prove that $\displaystyle\lim_{n\to \infty}\frac{\int_
\frac{1}{2}^1 \left(4x^2-2x-2\right)^n \, dx}{\int_0^1 \left(4 x^2-2 x-2\right)^n\, dx} =0.$ It certainly looks like it in the graph, and I also found that $$\int_{\frac 12}^1 \left(4 x^2-2 x-2\right)^n\, dx= \int_{-1}^{0} \left(4 x^2-2 x-2\right)^n\, dx.$$","['limits', 'calculus', 'definite-integrals']"
4483004,What is the intuition for defining the measurable sets in this manner?,"I am following a construction of a measure on subsets of $\mathbb{R}$ . The idea is to first define the semi-algebra of intervals and a ""measure"" $\mu$ on this semi-algebra which acts as we expect a length function should. i.e. $\mu(I)=b-a$ for intervals $[a,b)$ (note: the intervals can be open or closed on either side), $\mu(I+x)=\mu(I)$ (length is translation invariant), and $\mu(E)=\sum\mu(E_i)$ where $E_i$ are the disjoint union of $E$ . Given this, I can quite easily show that we can extend this to a $\sigma$ -additive measure $\nu$ , on the algebra generated by this semi-algebra of intervals. My question is regarding the extension of this $\nu$ to $\pi$ on the $\sigma$ -algebra containing this algebra. The idea as I understand is to first define an outer measure $\pi^*$ on all subsets of $\mathbb{R}$ . An outer measure satisfies the following properties: $\mu(\emptyset)=0$ . For $E\subseteq F$ , $\mu(E)\leq\mu(F)$ . For $E\subseteq\bigcup E_i$ , $\mu(E)\leq\sum\mu(E_i)$ We define $\pi^*(E)$ as $\inf\limits_{\{E_i\}}\nu(E_i)$ , where the $E_i$ are elements of the algebra and cover $E$ . It is easy to check this is an outer measure. Here is where I am confused about intutition: The next step is to define a collection of measurable sets $\mathcal{U}$ using this outer measure by stipulating that a set $S\subseteq\mathbb{R}$ is measurable if for every $E\subseteq\mathbb{R}$ $\pi^*(E)=\pi^*(A\cap E)+\pi^*(A^c\cap E)$ . I am struggling to understand intuitively why these sets should be the measurable ones? Can anyone elucidate this condition for me? I understand that from here we can show that this measurable space is a $\sigma$ -algebra containing the algebra and that $\pi^*$ on this space extends $\nu$ . But I'm struggling with the definition of $\mathcal{U}$ .","['measure-theory', 'outer-measure', 'real-analysis']"
4483006,A complicated integral that Mathematica can't compute,"I have a very complicated distribution function of which I want to find the expected value. The distribution I got for a function having the cosine of the samples taken from a Gaussian distribution. $$ y = \cos(x) $$ Where $x$ values are drawn from a Gaussian distribution and we assume $x \in [-\pi, \pi]$ . $$ p(x) = \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(x - \mu_x)^2}{2\sigma_x^2}} $$ The distribution of $y$ hence can be written as below: So, now the distribution of $y$ becomes: $$ p(y) = \frac{2}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}} $$ I referred this article for the formulation. They do it when $x$ follows a uniform distribution. However, here I have a Gaussian distribution. The main question that I want to ask is to sum the random numbers one would generate using this distribution. $$ \sum_i y_i$$ Where $y_i$ are the samples drawn from $p(y)$ . I thought of solving this by the following integral. I thought like a Frequentist and found that the sum of a number of variables is the sum of the independent variables (samples) I have multiplied with the probability of those samples occurring out of some number of draws from the distribution. Please let me know if this thinking is not adequate enough. So, at the end it becomes: $$ \sum_i y_i = \sum_{i = 1}^{N_s} y_i p(y_i) $$ Where $N_s$ is the number of independent samples. This can be written in integral form as: $$ \int_{-1}^{+1} \frac{2y}{\sqrt{1 - y^2}} \frac{1}{\sqrt{2\pi\sigma_x^2}}e^{-\frac{(\cos^{-1}(y) - \mu_x)^2}{2\sigma_x^2}} dy$$ I say $-1$ to $+1$ instead of a finite length here because I assume $N_s$ in the previous expression to be large enough. This looks like the expected value of the distribution. Is there a way to find a closed form solution to this equation (Or approximate closed form)? When it comes to Gaussian distribution, this quite nicely explained before and I have seen it here . When I tried this integral with the complicated distribution, it said that it can't converge. Is there way to do this? EDIT: I implemented a wrong function on mathematica. Now, I do the correct one. It doesn't say it can't converge anymore, however, it doesn't compute it now. It just returns with the integral expression. EDIT Version 2 ===================================================== I took $y = \cos(\theta)$ to solve the integral and I have now, $$  \int_{-\pi}^{\pi} \frac{2 \cos(\theta) e^{-\frac{\theta - \mu_x^2}{2\sigma_x^2}} }{\sqrt{2\pi \sigma_x^2}} d\theta$$ The solution I have from Mathematica looks like the following. $$ \frac{e^{-i \mu_x - \frac{1}{2\sigma_x^2}}}{2\sigma_x^2} \Bigg[ -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} - \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) + e^{i 2 \mu_x} \Bigg( -\operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} - \frac{\pi}{\sigma}}\Big) + \operatorname{erf}\Big( {\frac{\mu_x}{\sigma_x} + \frac{i \sigma_x}{2} + \frac{\pi}{\sigma}}\Big) \Bigg) \Bigg)  \Bigg] \Bigg[ \operatorname{erf}[\frac{i + \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
 \operatorname{erf}[\frac{i - \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
  i e^{i2 \mu_x} (\operatorname{erfi}[\frac{1 + i \sigma_x^2 (-\mu_x + \pi)}{\sqrt{2} \sigma_x}] - 
     \operatorname{erfi}[\frac{1 - i \sigma_x^2 (\mu_x + \pi)}{\sqrt{2} \sigma_x}] \Bigg] $$ Is there a relation between this and the nice solution given by @snoop ?","['integration', 'probability-distributions', 'closed-form', 'gaussian', 'probability']"
4483103,"Two points in $z,w \in \mathbb C$ with same distance to three other points [duplicate]","This question already has an answer here : proof that triangle have only one particular circumscribed circle (1 answer) Closed 2 years ago . Suppose that $a,b,c \in \mathbb C$ are three complex numbers which do not lie on a line. Further, let $z,w \in \mathbb C$ are such that $$
|z-a|=|w-a|, \ \ |z-b|=|w-b|, \ \ |z-c|=|w-c| .
$$ Is it true that $z=w$ ? Intuitively I would say yes due to geometric considerations but I would like to have a formal proof of it.","['complex-analysis', 'euclidean-geometry', 'geometry']"
4483154,"Resultants - if $a,b$ have degrees $m,n$, why are there polynomials $u,v$ of degrees $m-1,n-1$ so that $Res(a,b)=av-bu$?","Let $R$ be a commutative ring. Suppose I have polynomials $a\left(x\right),b\left(x\right) \in R\left[x\right]$ of degrees $m,n$ , respectively. My reading states that there then exist polynomials $u\left(x\right),v\left(x\right) \in R\left[x\right]$ of degrees $m-1,n-1$ , respectively, such that $a(x)v(x)-b(x)u(x) = \operatorname{Res}_{m,n}(a(x), b(x))$ . My question is why must this be true and how can it be proven? I have tried small examples by hand and it seems to be true, but I have no idea how to approach a proof. I think there might be a way via the division algorithm, but that isn't obvious to me.","['determinant', 'functions', 'algebraic-geometry', 'abstract-algebra', 'resultant']"
4483244,Martingale property and conditional expectation,"We consider a cadlag stochastic process $(X_r)_r$ with non-decreasing sample paths and $X_0=0.$ Let $Y_r:=X_r-r.$ We suppose that $(Y_r)_r,(Y^2_r-r)_r,$ and $(Y_r^3-3rY_r-r)_r$ are martingales relative to the canonical filtration of $(\mathcal{F}_r)_r$ of $(X_r)_r.$ Let $f:\mathbb{R} \to \mathbb{R}$ be a function of class $C^{\infty}$ with bounded derivatives. Find, for $r \leq u,$ the expression of $E[f(X_u)|\mathcal{F}_r]$ in term of the derivatives of $f.$ So how could we find an expression for $E[f(X_u)|\mathcal{F}_r]$ ?","['stochastic-analysis', 'stochastic-processes', 'martingales', 'probability-theory', 'stochastic-calculus']"
4483248,Function where second derivative is equal to reciprocal squared,"I was recently interested in whether there exists a closed-form solution for the position of an object (say a spacecraft) in freefall as a function of time. To complicate things, I wanted to take into account the increase in acceleration as you get nearer to the surface. The differential equation is straightforward: $ x''(t) = \frac{\mu}{x(t)^2} $ where $\mu = G \cdot m_{planet}$ How, if at all, would I go about solving this? I've tried to use the common functions that return themselves when differentiated (trig, hyperbolic trig, exponential, etc.) but none of them obviously equal the reciprocal of themselves when differentiated twice. The closest I've gotten was $B\cosh(D \cdot t)$ , but it doesn't work in the differential equation and doesn't quite match the analytical curve.",['ordinary-differential-equations']
4483264,Is there an analog of Maxwell's theorem on the unit ball?,"Maxwell's theorem says that any product measure on $\mathbb{R}^n$ which is invariant to orthogonal matrices (meaning that the measure of $A$ and the image of $A$ under any orthogonal map $Q$ is the same) is a Normal distribution of the form $N(0, \kappa I_n)$ for $\kappa > 0$ . Is there an analogous statement when instead of $\mathbb{R}^n$ we make the space the unit ball in $\mathbb{R}^n$ ?",['probability-theory']
4483266,Prove that $2\sum\limits_{cyc}{} \frac{1}{a}+9k^2\sum\limits_{cyc}{} \frac{1}{a+b}+ t^4\sum\limits_{cyc }^{}\frac{1}{a^2+b^2}$,"prove that :if $a,b,c,t,k,u>0$ and $a^2+b^2+c^2=3u^2$ then : $2\sum_{cyc}{} \frac{1}{a}+9k^2\sum_{cyc}{} \frac{1}{a+b}+ t^4\sum_{cyc }^{}\frac{1}{a^2+b^2}\geq \frac{3(2+3k+t^2)^2}{2u(u+2)}.$ my attempt: we know this: $\frac{3}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}\leq \sqrt{\frac{a^2+b^2+c^2}{3}}=u$ so $2\sum_{cyc}{} \frac{1}{a}\geq \frac{6}{u}= \frac{3.4}{2u}$ and after  using CBC inequality we will get : $9k^2\sum_{cyc}{} \frac{1}{a+b}\geq \frac{81k^2}{2(a+b+c)}\geq\frac{81k^2}{2\sqrt{(a^2}+b^2+c^2)3}=\frac{81k^2}{2.3u} $ and $ t^4\sum_{cyc }^{}\frac{1}{a^2+b^2}\geq \frac{9t^4}{2(a^2+b^2+c^2)}=\frac{3t^4}{2.u^2}$ so $2\sum_{cyc}{} \frac{1}{a}+9k^2\sum_{cyc}{} \frac{1}{a+b}+ t^4\sum_{cyc }^{}\frac{1}{a^2+b^2}\geq \frac{3.4}{2u} +\frac{81k^2}{2.3u} +\frac{3t^4}{2.u^2}=3(\frac{4}{2u} +\frac{9k^2}{2u} +\frac{t^4}{2.u^2})\geq \frac{3(2+3k+t^2)^2}{4u+2.u^2}=\frac{3(2+3k+t^2)^2}{2u(u+2)}.$ so finally: $2\sum_{cyc}{} \frac{1}{a}+9k^2\sum_{cyc}{} \frac{1}{a+b}+ t^4\sum_{cyc }^{}\frac{1}{a^2+b^2}\geq \frac{3(2+3k+t^2)^2}{2u(u+2)}.$ does my attempt is true?","['inequality', 'analysis', 'cauchy-schwarz-inequality', 'solution-verification', 'algebra-precalculus']"
4483294,When will $AB$ and $B$ have the same column space?,"Suppose $A\in \mathbb{R}^{n\times n}$ and $B\in \mathbb{R}^{n\times m}$ with $n>m$ and $rank(B)=m$ . Now given $B$ , I would like to find all $A$ 's such that $AB$ and $B$ have the same column space. A trivial solution would be $A=I_n$ , but I would like to find all solutions (explicitly if possible? or any necessary and sufficient condition that $A$ need to satisify?)","['matrices', 'matrix-calculus', 'linear-algebra', 'vector-spaces']"
4483300,Statistic distribution of the sum of $n$ random variables,"Assume we have $n$ real variables $x_i$ ,  where ${i=1,2,\cdots, n}$ . They are independent random variables uniformly distributed in $[0,1]$ . We define the variable $y=\frac{\sum_{j=1}^n x_j\exp(i 2\pi j/n)}{\sum_{j=1}^n x_j}$ . The question is to find the p.d.f. of $|y|$ , when $n$ is large enough. I tried to think about the central limit theorem. But it looks this is not a standard form of that. I also tried to derive the pdf by regarding $y$ as the sum of many random variables, which is also hard given the complicated formula. This question also has a clear geometric picture. But I am not sure how to find the statistics from it. I am basically stuck here.","['statistics', 'probability-distributions', 'random-variables']"
4483323,Alternate forms of $ \sum\limits_{n=2}^\infty \text P(n)=\sum\limits_{p\text{ prime}}\frac1{p(p-1)} $ with the prime zeta function.,"We know that: $$\sum_{n=2}^\infty (\zeta(n)-1)=1$$ but what about with the Prime Zeta function $\text P(s)$ ?: $$\sum_{n=2}^\infty \text P(n)=0.77315666904979…$$ Now interchange the sum with the prime numbers $ p_k$ $$\sum_{n=2}^\infty \sum_{m=1}^\infty \frac1{p_m^n}= \sum_{m=1}^\infty\sum_{n=2}^\infty \frac1{p_m^n}=\sum_{n=1}^\infty \frac1{p_n(p_n-1)}$$ which gives the same decimal. There are also many prime-related constants . Another form given by @reuns is with the Euler Phi and Mobius functions : $$\sum_{n=2}^\infty \text P(n)=\sum_{n=2}^\infty \frac{\phi(n)\ln(\zeta(n))}n-\sum_{n=2}^\infty \frac{\mu(n)\ln(\zeta(n))}n= \sum_{n=2}^\infty \frac{\phi(n)\ln(\zeta(n))}n-\text C= \gamma-\text B_1+ \sum_{n=2}^\infty \frac{\phi(n)\ln(\zeta(n))}n $$ where $\text P(s)=-\ln(\epsilon)+\text C+O(\epsilon),\epsilon>0$ as seen in Formulas 3 throught 5 here on MathWorld and $\text C=\text B_1-\gamma$ with the Merten’s constant $\text B_1$ and Euler Mascheroni constant . It also gives the same decimal. The Euler phi sum is similar to @Steven Clark’s formula ( $14$ ) here . If one read further in the Merten’s constant article , then the amount of prime factors average deviation constant $\text B_2$ appears giving: $$\sum_{n=2}^\infty \text P(n)=\sum_{p\text{ prime}}\frac1{p(p-1)}=\text B_2-\text B_1= 0.77315666904979… $$ Now that a “closed form” has been found, are there any alternate forms of the $0.77315666904979…\,$ constant in terms of special functions, integrals, manipulated sums et cetera?","['closed-form', 'sequences-and-series', 'constants', 'zeta-functions', 'prime-numbers']"
4483325,A question about $\pi$-system.,"If $\mathcal{P}$ is a $\pi$ -system and use $\mathcal{S}$ to denote the intersection of all $\lambda$ -systems containing $\mathcal{P}$ . I want to show $\mathcal{S}$ is also a $\pi$ -system. By definition, take $A,B\in \mathcal{S}$ and show $A\cap B\in \mathcal{S}$ . Let $\mathcal{A}:=\{C\subseteq X\mid A\cap C\in S\}$ , it suffices to show $B\in\mathcal{A}$ . If I can show $\mathcal{A}$ is a $\lambda$ -system containing $\mathcal{P}$ , this completes the proof. I can already show $\mathcal{A}$ is a $\lambda$ -system, but I can't show $\mathcal{P}\subseteq\mathcal{A}$ . Could anyone give me some help? Thanks in advance!","['measure-theory', 'probability-theory']"
4483348,Are matrices more easier to work with than linear maps? [closed],"Closed . This question is opinion-based . It is not currently accepting answers. Want to improve this question? Update the question so it can be answered with facts and citations by editing this post . Closed 1 year ago . Improve this question I never liked doing computations with matrices. This was before I learnt that matrices are used to represent linear maps. Also that the definition of matrix multiplication stems from trying to find the matrix of the composition of two linear maps. Another property of a matrix of some linear map is that a matrix multiplied by a vector is the same as evaluating the linear map for that specific vector. What I don't understand is that why is the use of matrices in applications so emphasised. I could be wrong, but wouldn't it be easier to evaluate a function than to multiply a matrix with a vector. The only useful way I see matrices making computations easier is when one wants to find the inverse of a linear map that is hard to find just by the expression of the map. Even while learning about matrices for the first time, a lot of problems phrased in terms of matrices looked like they would be phrased better in terms of functions or just equations. My question is that, how is it more useful to use matrices than it is to use linear maps?","['matrices', 'soft-question', 'linear-algebra']"
4483357,Milnor's appendix on classification of 1-manifolds,"I'm trying to understand Milnor's proof of the classification of 1-manifolds, given in the appendix of his boook ""Topology from a differentiable point of view"". Any smooth, connected one dimensional manifold is diffeomorphic to either to the circle $S^1$ or to some interval of real numbers. Remark: I thought doing in many posts as part 1, and so on, but I think it will lose the idea. However, if some moderator thinks that will be better I will do it :D. To proof that, he gives a definition and a lemma, Definition A map $f: I\to M$ is a parametrization by arc-length if $f$ maps $I$ diffeomorphically onto an open subset $\dagger$ of $M$ , and if the ""velocity vector"" $d f_s(1) \quad\varepsilon\quad T M_{f(s)}$ has unit length, for each $s \varepsilon I$ .
Any given local parametrization $I^\prime\to M$ can be transformed into a parametrization by arc-length by a straightforward change of variables. Lemma Let $f: I\to M$ and $g: J\to M$ be parametrizations by arc-length. Then $f(I) \cap g(J)$ has at most two components. If it has only one component, then $f$ can be extended to a parametrization by arc-length of the union $f(I) \cup g(J)$ . If it has two components, then M must be diffeomorphic to $S^1$ . Here is the beginning of the proof Proof Clearly $g^{-1}$ of maps some relatively open subset of $I$ diffeomorphically onto a relatively open subset of $J$ . Furthermore the derivative of $g^{-1}$ of is equal to $\pm 1$ everywhere. My doubts are:
To prove that let $U=f(I)\cap g(J)$ , so $$h:=g^{-1}\circ f:f^{-1}(U)\to g^{-1}(U)$$ that ""open subset of $I$ "" is $f^{-1}(U)$ so $h$ is a diffeormophism. From there $h(s)=g^{-1}\circ f(s)$ , then $dh_s=dg^{-1}_{f(s)}(df_s)$ . I know that $\|df_s(1)\|=\|dg_t(1)\|=1$ , and $dg^{-1}_t=(dg_{g^{-1}(t)})^{-1}$ , so $$\begin{aligned}\|dh_s(1)\|&=\|dg^{-1}_{f(s)}(df_s(1))\|\\&=\|(dg_{g^{-1}(f(s))})^{-1}(df_s(1))\|\\&=\|(dg_{h(s)})^{-1}(df_s(1))\|\end{aligned}$$ (1) But I don't know what to do from that. Another way I'm trying is: From $h(s)=g^{-1}\circ f(s)$ , we get $g(h(s))=f(s)$ , then $dg_{h(s)}(dh_s)=df_s$ . Applying in $1$ , we get $dg_{h(s)}(dh_s(1))=df_s(1)$ , so $dg_{h(s)}(h'(s))=df_s(1)$ ,but I get stuck again. Then he continues: So $\Gamma:=\{(s,t)\in f^{-1}(U)\times g{-1}(U)|f(s)=g(t)\}\subset I\times J$ , then $\Gamma=\operatorname{graph}(h)$ and it is a closed subset. I get this, my second doubt is: (2) What information about the intersection $U$ will $\Gamma$ give? (3) Why does he say that $h$ is ""locally"" a diffeomorphism? As $h'(s)=\pm 1$ , then $h(s)=s+b$ or $h(s)=-s+b$ , where $b$ is a constant. In the proof says that $\Gamma$ consists of line segments of slope $\pm 1$ . (4) The word ""segments"" means that $\Gamma$ can be made of many pieces, but if $h$ is a diffeomorphism, and its domain $f^{-1}(U)$ is connected(or I'm wrong?), it is not $\Gamma$ made of one part? Then... (5) I do not understand why there can be at most one of these segments ending on each of the 4 edges of $I\times J$ .
In order to this post not being huge I will do a second part. Thank you.","['manifolds', 'differential-topology', 'smooth-manifolds', 'differential-geometry']"
4483369,"$\forall f\in Y^X$ with $\text{Gr}(f) $ is closed implies $f\in C(X, Y) $. Does this implies $(Y, \tau_Y) $ is compact?","$(X, \tau_X) $ and $(Y, \tau_Y) $ be two topological spaces. $\forall f\in Y^X$ with $\text{Gr}(f) $ is closed implies $f\in C(X, Y) $ . Question : Does this implies $(Y, \tau_Y) $ is compact? Notation: $Y^X$ : Set of all functions from $X$ to $Y$ . $C(X, Y) =\{f\in Y^X: f \text{ is continuous }\}$ $\text{Gr}(f) =\{(x,f(x)):x\in X\}\subset X×Y$ I want to show if $Y$ is not compact, then $\exists f\in Y^X$ with $\text{Gr}(f) $ is closed but $f\notin C(X, Y) $ If we choose $X=Y=\Bbb{R}$ and endowed with euclidean topology, then $f(x)= \begin{cases}\frac{1}{x} & x\neq 0 \\ 0 & elsewhere
\end{cases}$ is one such example of required functions. But how to prove for any two topological spaces $X$ and $Y$ ? A variant of this interesting question is on my MO post .","['compactness', 'continuity', 'general-topology', 'soft-question', 'closed-graph']"
4483393,Detecting homotopy by precomposing with paths.,"Let X and Y be topological spaces, and denote $\mathbb{S}^n$ the $n$ -sphere. i) Suppose that $f,g:X \to Y$ are maps such that for every path $\alpha: [0,1] \to X,$ we have that $f \circ \alpha$ is left homotopic to $g \circ \alpha$ .
Does this imply that $f$ is left homotopic to $g$ ? ii) Suppose instead that for every map $\alpha: \mathbb{S}^n \to X$ and every $n \geq 0, $ we have that $f \circ \alpha$ is left homotopic to $g \circ \alpha$ . Is it true that $f$ and $g$ must be left homotopic? iiia) If not, does this become true if we assume $X$ and $Y$ to be nice? iiib) What if $X,Y$ are CW-complexes? Note: these are not homework questions. They are questions which I am asking to myself but I don’t know how to answer because I don’t have time to think about them now.","['general-topology', 'homotopy-theory', 'cw-complexes', 'algebraic-topology']"
4483510,An Exercise from Liebeck's 'A Concise Introduction to Pure Mathematics',"The exercise reads: 'Find an integer $n$ and a rational $t$ such that $n^t=2^{1/2}3^{1/3}$ .
Here is my attempt of solution: $$n^t=2^{1/2}3^{1/3}\Rightarrow n^{2t}=2\times 3^{2/3}\Rightarrow n^{6t}=2^33^2=72.$$ Therefore, a suitable choise is $t=1/6$ and $n=72$ . Is my solution a proper one? Is there any quicker solution? Thank you very much.","['algebra-precalculus', 'solution-verification']"
4483574,Derivative of $ \frac{1}{x}$ geometrically,"Context: ""Essence of calculus"" series called ""Derivative formulas through geometry""- 3rd episode of chapter 3 , 3Blue1Brown : From 10:08 - 12:35 he discusses the derivative of $\frac{1}{x}$ geometrically.  After the discussion of the basic idea, it is given as an exercise to solve. I tried solving it but I always come to conclusion that $\frac{d}{dx} \frac{1}{x} = -\frac{1}{x(x+dx)}$ , or $ -\frac{1}{(x^2 +xdx)}$ . Is it allowed to neglect "" $x  dx$ ""? Or is there another correct method? My try: $$ \begin{align}A_{(lost)} &= A_{(gained)} \\ A_{(lost)} &= x * -d(\frac{1}{x})  \\ A_{(gained)}&= dx \frac{1}{(x+dx)} \end{align}$$ note:- $d(\frac{1}{x})$ is negative because the change was negative (between $x$ and $x+dx$ ) That means that $$x * -d(\frac{1}{x}) = dx  \frac{ 1}{(x+dx)}$$ Isolating the $d(\frac{1}{x})$ yields: $$d(\frac{1}{x}) = -\frac{dx}{x(x+dx)}$$ Dividing the dx from both sides give you the derivative: $$\frac{d(\frac{1}{x})}{dx} = -\frac{1}{x(x+dx)} = -\frac{1}{(x^2 + x  dx)}$$","['calculus', 'derivatives']"
4483589,Interpolation of a 2D segment using its projection,"Consider the following diagram: The blue segment is projected on the orange projection screen from a specific point of view. The projection is shown at the bottom of the image. About the blue segment is known the following: the orthogonal distance between the point of view and each of the segment's edges, d1 and d2 . Now we select a point on the projected segment, such that it splits the projected segment into 2 segments of length p1 and p2 . Is there a way, with that information, to calculate the orthogonal distance d from the point of view to the actual point represented by the point on the projected segment? If so, how? If not, what info would be required? Notes: The complexity comes from the fact that the proportions of the projected segment do no respect the proportions of the original segment (notice how the original segment is roughly split in half, while the projected segment is not split in equal halves, i.e. p1 is not equal to p2 ). Since the segment is straight, I wonder if there is a formula that would give the ratio on the segment based on the ratio on the projected segment. I've done some empirical tests to find the p1/(p1+p2) ratio that represents the exact middle of the original segment. x1 and x2 are the other coordinate of the left and right segment, respectively. x1 d1 x2 d2 ratio p1/(p1+p2) -0.5 1 0.5 2 0.5 2/3 -0.5 1 0.5 3 0.5 6/8 -0.5 1 0.5 4 0.5 8/10","['trigonometry', 'projective-geometry', 'geometry']"
4483593,"How to interpret FTC1 that assumes integrability on $[a,b]$ and continuity at $c \in [a,b]$, as opposed to continuity on $[a,b]$?","Consider the statement of the first fundamental theorem of calculus in Chapter 14 of Spivak's Calculus Let $f$ be integrable on $[a,b]$ , and define $F$ on $[a,b]$ by $$F(x)=\int\limits_a^x f\tag{1}$$ If $f$ is continuous at $c$ in $[a,b]$ , then $F$ is differentiable at $c$ , and $F'(c)=f(c)$ (If $c=a$ or $b$ , then $F'(c)$ is understood to mean the right- or
left-hand derivative of $F$ ) In Chapter 13, entitled ""Integrals"", there is a theorem which says $f$ continuous on $[a,b]$ $\implies$ $f$ integrable on $[a,b]$ Why didn't the statement of the fundamental theorem simply say Define $F$ on $[a,b]$ by $$F(x)=\int\limits_a^x f$$ If $f$ is continuous at $c$ in $[a,b]$ , then $F$ is differentiable at $c$ , and $F'(c)=f(c)$ . Here is my attempt at explaining this Consider the function $$f(x)=\begin{cases} x, \text{ if } x \text{ rational } \\ 0, \text{
 if } x \text{ irrational }\end{cases}$$ and consider the point $x=0$ . $f$ is continuous at $0$ (and nowhere
else). Let $[a,b]$ be any interval containing $0$ , and let $P$ be a partition
on $[a,b]$ . Then $$L(f,P)=\sum\limits_{i=1}^n m_i \Delta t_i = 0$$ $$U(f,P)=\sum\limits_{i=1}^n M_i \Delta t_i>0$$ Hence $L(f,P) \neq U(f,P)$ for all partitions, and hence $f$ is not
integrable on $[a,b]$ . Thus the right-hand side of $(1)$ isn't even defined. So is the reason we can't use the second proposed version of the theorem because of cases such this? It seems that most of the statements of the first fundamental theorem of calculus use continuity of $f$ on an interval $[a,b]$ . This rules out the example I gave above. The first version above seems to be more general, because it accommodates continuity on an interval. If $f$ is continuous on $[a,b]$ then it is integrable on $[a,b]$ , and so we can apply the first theorem to each point in this interval to conclude that for all $x \in [a,b]$ , $F'(x)=f(x)$ . Is this analysis correct?","['integration', 'calculus', 'derivatives']"
4483628,"the angle formed by incenter, vertex and circumcenter","In the triangle $\triangle ABC$ , $I$ is the incenter, $O$ is the circumcenter. Prove that $\angle ICO=\frac{|\angle A-\angle B|}{2}$ . I found this conclusion in steps of some proof. But I spent a day on it without answer. I used GeoGebra to test it. It was correct. Can anyone help me? Thanks. I tried to connect $AI,BI$ . Assuming $\angle B >\angle A$ . I can draw $\angle IBE=\frac{\angle B-\angle A}{2}$ . But I do not see how to relate it to the question angle. @peterwhy solved this question in the comment part and reminded that I need to check the case when the circumcenter is outside of the triangle. So, I used GeoGebra to check that case. During this process, I fixed $B$ and $C$ and let $A$ moving around the circle. I found that the trace of the incenter $F$ is a union of two arcs. By GeoGebra, they are truly two arcs from two circles. I want to ask where are the centers of them and what are radii of these two circles. Now I know the centers of these two arcs and the radii of them. I post the GeoGebra graph first here: Because I move the point $A$ along the circumcircle. So its angle is fixed. The angle $\angle CDB=90^\circ+\frac{A}{2}$ after computation. Hence it is also fixed. So its trace will be an arc. This arc has the central angle $180^\circ -A$ and the radius will be the perpendicular bisector $BC$ . Therefore the center is, the midpoint of the arc $BC$ , $E$ on the circumcircle. The radius is $BE$ . With the similar reason, the other arc has the center $F$ , the midpoint of the greater arc $CAB$ . The radius is $BF$ . DONE.","['triangles', 'geometry']"
4483731,Interpreting completion of a module as a Taylor expansion,"An exercise in Atiyah-Macdonald asks the reader to show that if $A$ is a Noetherian ring, $\mathfrak{a}$ is an ideal of $A$ , and $M$ is a finitely-generated $A$ -mdoule, then $$\hat{M} = 0 \iff \operatorname{Supp}(M) \cap \operatorname{V}(\mathfrak{a}) = \emptyset.$$ The authors then state: The reader should think of $\hat{M}$ as the ""Taylor expansion"" of $M$ transversal to the subscheme $\operatorname{V}(\mathfrak{a})$ : the above result then shows that $M$ is determined in a neighborhood of $\operatorname{V}(\mathfrak{a})$ by its Taylor expansion. I am completely befuddled by the block quote. Specifically: What does it mean to have a Taylor expansion ""transversal to [a] subscheme""? How is this result related to $M$ being determined in a neighborhood of $\operatorname{V}(\mathfrak{a})$ by its Taylor expansion?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4483759,"$\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12$ for all $t\in \Bbb R$","This question stems from the identity listed as Equation $(6.6)$ in this paper on Pg. $11$ . We want to show $$\color{blue}{\lim_{r\to\infty} \int \cos^2(rz + t)\, d\pi_{\#}\mu(z) = \frac12} \tag{6.6}$$ for all $t\in \Bbb R$ . Here, $\mu$ is a Borel probability measure on the cone $$\mathcal C^d = \{(x_1, \ldots, x_{d+1}): |(x_1, \ldots, x_d)| = |x_{d+1}|\} \subset \Bbb R^{d+1}$$ satisfying $$|\hat\mu(\xi)| \le |\xi|^{-\alpha/2} \quad\quad  \forall \xi\in \Bbb R^{d+1} \tag{6.1}$$ for some $\alpha > d-1$ . Further, we assume $\operatorname{supp} \mu \subset \{(\xi, |\xi|) \in \Bbb R^d\times \Bbb R: \epsilon \le |\xi| \le 1/\epsilon\}$ for some $\epsilon > 0$ . Let $\pi:\Bbb R^{d+1} \to \Bbb R$ be the projection map onto the last coordinate; i.e. $\pi: (x_1, \ldots, x_{d+1}) \mapsto x_{d+1}$ . The paper suggests: Since $d\ge 2$ and $\alpha/2 > (d-1)/2 \ge 1/2$ , $(6.1)$ gives $\pi_{\#}\mu\in L^2(\Bbb R)$ . Hence, $\pi_{\#}\mu \in L^1(\Bbb R)$ with $\|\pi_{\#}\mu\|_1 = 1$ , and $(6.6)$ then follows by approximating $\pi_{\#}\mu$ in $L^1$ with a finite linear combination
of characteristic functions of disjoint intervals. I tried to follow the line of thought in the paper to first show that $\pi_{\#}\mu \in L^2(\Bbb R)$ . If we are to use Theorem $3.3$ of Mattila's Fourier Analysis and Hausdorff Dimension , we shall have to show that $\pi_{\#}\mu\in \mathcal M(\Bbb R)$ and $\widehat{\pi_{\#}\mu}\in L^2(\Bbb R)$ . I have \begin{align*}
\widehat{\pi_{\#}\mu}(\xi) &= \int_{\Bbb R} e^{-2\pi i\xi z} \, d\pi_{\#} \mu(z) \\ &= \int_{\Bbb R^d\times\Bbb R} e^{-2\pi i\xi z}\, d\mu(x,z)\\
&= \widehat{\mu}(0, \ldots, 0, \xi)
\end{align*} So, $$\int_{\Bbb R} \left| \widehat{\pi_{\#}\mu}(\xi) \right|^2 \, d\xi = \int_{\Bbb R} |\widehat{\mu}(0, \ldots, 0, \xi)|^2\, d\xi \lesssim \int_{\Bbb R} |\xi|^{-\alpha}\, d\xi = 2\int_0^\infty \xi^{-\alpha}\, d\xi$$ Is $\int_0^\infty |\xi|^{-\alpha}\, d\xi < \infty$ ? Note that $\alpha \ge 1$ . As suggested by @J.G. in the comments, it's enough to show $$\lim_{r\to\infty} \int e^{2irz}\, d\pi_{\#}\mu(z) = \lim_{r\to\infty}\widehat{\pi_{\#}\mu}\left(-\frac{r}{\pi}\right) = 0$$ Notation. $\mathcal M(\Bbb R^n)$ is the set of all Borel measures $\nu$ satisfying $0 < \nu(\Bbb R^n) < \infty$ with compact support $\operatorname{supp} \nu \subset \Bbb R^n$ .","['measure-theory', 'fourier-analysis', 'proof-explanation', 'analysis', 'limits']"
4483804,Generalizations of Stein's identity to product of functions of gaussian vector,"Given a $d$ -dimensional Gaussian $X \sim N(\mu, \Sigma)$ and two real-valued differentiable functions $f,g$ with bounded first derivatives, I am wondering if there is a simple expression for the covariance: $$
\text{Cov} (f(X),g(X)).
$$ For example, when $f(X) = X_k$ for some $k \in \{1,\dots, d\}$ , Stein's lemma gives $$
\text{Cov} (X_1,g(X)) = \sum_{i=1} \Sigma_{1i} \mathbb{E} \left( \frac{\partial g(x)}{\partial x_i} \bigg |_{x=X}\right).
$$ Are there similar expressions for more general $f$ though? reference: Lemma 1 here https://reader.elsevier.com/reader/sd/pii/016771529490121X?token=62B2E8B7BA323BC1AF5B699E9A3115C156EC43A959F764C79D0C044730A2C8F605B5CC1F9E5807764BCC2D37793AC619&originRegion=us-east-1&originCreation=20220630190410","['statistical-inference', 'statistics', 'gaussian', 'probability']"
4483809,How to integrate $\int x'(t)\ dx(t)$ and similar?,"I just wanted to ask how to integrate the one in the title and also another one which is very similar. Also, I don't know why but WolframAlpha just treats the derivatives as contants. Integrals: $$\int x'(t) dx(t)\ ;$$ $$\int x'(t)^2 dx(t)\ .$$ I don't really know but using $x'(t)=\frac{dx}{dt}(t)$ and isolating differentials may help.","['integration', 'calculus', 'derivatives']"
4483861,Evaluating $\lim_{n\to\infty} \left(\prod_{r=0}^n \binom{n}{r}\right)^\frac{1}{n(n+1)}$,"Problem statement: If $$ A = \lim_{n\to\infty} \left(\prod_{r=0}^n \binom{n}{r}\right)^\frac{1}{n(n+1)}, $$ Find $A^2$ . Solution: 1) \begin{align*}
\prod_{k=0}^{n} \binom{n}{k}
&= \prod_{k=1}^{n} \frac{n!}{k!(n-k)!}
= \frac{(n!)^{n+1}}{(1! \cdot 2!\cdots n!)^2} \\
&= \prod_{k=1}^{n} (n+1-k)^{n+1-2k} \\
&= \prod_{k=1}^{n} \left(\frac{n+1-k}{n+1}\right)^{n+1-2k},
\end{align*} since $\sum_{k=1}^{n} (n+1-2k) = 0$ . Taking log and limit, we get \begin{align*}
& \frac{1}{n}\sum_{k=1}^{n}\left( 1 - \frac{2k}{n+1} \right) \ln\left( 1 - \frac{k}{n+1} \right) \\
&\to \int_{0}^{1} (1 - 2x)\log(1-x) \, \mathrm{d}x
= \frac{1}{2}.
\end{align*} Source : FITJEE AITS 2020 FT-8 Paper 1 of JEE Advanced question 53 Could anybody explain the solution? I tried taking log both sides to bring the power down but got stuck on evaluating $$\sum_{r=0}^n \ln\binom{n}{r}$$ further I think the answer should be $e$ instead of $0.5$ which is given.","['limits', 'calculus', 'binomial-coefficients', 'combinatorics']"
4483865,"If $G$ is nonabelian & solvable s.t. the centralizer of each nontrivial element is abelian, then $G$ is Frobenius with kernel its Fitting subgroup","I'm dealing with the following problem in Isaacs Finite Group Theory [6A.5], I would appreciate if you could help: Let $G$ be a nonabelian solvable group in which the centralizer of every nonidentity element is abelian. Show that $G$ is a Frobenius group where $F(G)$ is the Frobenius kernel. [Here $F(G)$ is the Fitting subgroup of $G$ ] My attempt: I think if we can show that centralizer $C_G(n)$ of any nonidentity element $n$ of $F(G)$ is contained in $F(G)$ , the result will immediately follow. But I could not show that.","['solvable-groups', 'finite-groups', 'abstract-algebra', 'group-theory', 'frobenius-groups']"
4483871,"$y''+y=x^2+1, y(\pi)=\pi^2, y'(\pi)=2\pi$ - By Laplace Transform","I am solving the following IVP by Laplace Transform: $$y''+y=x^2+1,\qquad y(\pi)=\pi^2, \qquad y'(\pi)=2\pi$$ Let $f(x)=u_{\pi}(x)y(x-\pi).$ Then, $$f''(x)+f'(x)=u_{\pi}(x)(x-\pi)^2+u_{\pi}(x), \qquad f(0)=\pi ^2, \qquad f'(0)=2\pi.$$ Using the Laplace Transform and writing $F:=\mathcal{L}\{f\}(s)$ , we have $$s^2F-sf(0)-f'(0)+F=e^{-\pi s}\dfrac{s+1}{s^2},$$ $$(s^2+1)F-(s+1)\pi^2-2\pi=e^{-\pi s}\dfrac{s+1}{s^2},$$ $$F=e^{-\pi s}\dfrac{s+1}{s^2(s^2+1)}+\dfrac{(s+1)\pi^2}{(s^2 +1)}+\dfrac{2\pi}{(s^2+1)}.$$ Be $$\dfrac{s+1}{s^2(s^2+1)}=\dfrac{As+B}{s^2}+\dfrac{Cs+D}{s^2+1}. $$ We have $$As^3+As+Bs^2+B+Cs^3+Ds^2=s+1,$$ $$A+C=0, B+D=0, A=1, B=1,$$ $$C=-1, D=-1, A=1, B=1.$$ So, $$\dfrac{s+1}{s^2(s^2+1)}=\dfrac{s+1}{s^2}-\dfrac{s+1}{s^2+1}= \dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac{1}{s^2+1}.$$ Then, $$F=e^{-\pi s}\left(\dfrac{1}{s}+\dfrac{1}{s^2}-\dfrac{s}{s^2+1}-\dfrac {1}{s^2+1}\right)+\pi^2\left(\dfrac{s}{s^2+1}+\dfrac{1}{s^2+1}\right)+ 2\pi\dfrac{1}{s^2+1}.$$ By the inverse transform, $$f(x)=u_\pi(x)\left(1+(x-\pi)-\cos(x-\pi)-\sin(x-\pi)\right)+\pi^2 \left(\cos(x)+\sin(x)\right)+2\pi\sin(x).$$ Returning to the variable $y$ and remembering that $\cos(x+\pi)=-\cos(x)$ and $\sin(x+\pi)=-\sin(x)$ , we have $$y(x)=1+x-\cos(x)-\sin(x)+\pi^2(-\cos(x)-\sin(x))-2\pi \sin (x) .$$ However, the Wolfram's solution is $y(x)=-1 + x^2 - \cos(x)$ . Thank you in advance!","['initial-value-problems', 'laplace-transform', 'ordinary-differential-equations']"
4483878,Define $g(x) = \frac{ f(x)}{f(f(x))}$. Prove or. disprove: If $g$ is unbounded then $\sum 1/f(n)$ diverges.,"Let $f$ be positive non decreasing function s.t. $\lim f(x)$ is $\infty$ as $x$ grows. Define: $g(x) = \frac{ f(x)}{f(f(x))}$ . Prove or disprove: If $g$ is unbounded at $[a, \infty]$ and bounded at $[-\infty, a]$ for every $a>0$ then $\sum 1/f(n)$ diverges. In my feeling the statement must be true. My intuition tells me $f(n) = O(n)$ but im not able to prove the statement, so im asking your advice. How can i continue?","['calculus', 'sequences-and-series', 'real-analysis']"
4483928,A specific binomial summation identity [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question Let $m$ and $n$ be positive integers with $m < n$ . Prove \begin{equation}
   \left(\sum_{k=0}^m \binom{m}{k}\frac{(-1)^k}{n-k} \right)\left(\sum_{k=0}^m \binom{n}{k}\frac{(-1)^k}{k+1} \right) =\sum_{k=0}^m \binom{m}{k}\frac{(-1)^k}{(n-k)(k+1)} 
\end{equation} I'm not sure how to approach this problem. A Hint would be greatly appreciated!","['summation', 'binomial-coefficients', 'combinatorics']"
4483950,Density of bounded linear operators space,"Assume that $D$ is a dense subspace of a Banach space $X$ . Can we conclude that the space of bounded linear operators $\mathcal{B}(D,D)$ is dense in $\mathcal{B}(D,X)$ ? Thank you in advance","['general-topology', 'operator-theory', 'functional-analysis']"
4483987,"Uniform convergence of $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2})$ on $[0,a],a>0$.","I want to show the uniform convergence of $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2})$ on $[0,a],a>0$ . I tried it as follows: $f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\sim n\left(\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\right)\to \frac{x^2}{4\pi}$ . It follows that $f_n$ converges pointwise to $f:x\mapsto \frac{x^2}{4\pi}$ . To show uniform convergence, I want to show that the sequence $y_n:=\sup_{x\in [0,a]}|f_n(x)-f(x)|$ converges to $0$ . $\begin{align}
f_n(x)&=n\left((\sqrt{4\pi^2n^2+x^2}-2n\pi)-(\sqrt{4\pi^2n^2+x^2}-2n\pi)^3 \frac 1{3!}\right)+o(1/n)\\
&=\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n)
\end{align}$ $\begin{align}|f_n(x)-f(x)|&\le \left|\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)-\frac {x^2}{4\pi}\right|+\left(\frac{x^2}{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)^3\frac{1}{n^23!}+o(1/n)\\
&\le a^2\left(\frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(x/n)^2}+2\pi}\right)+o(1/n)\\
&\le a^2\left( \frac 1{4\pi}-\frac 1{\sqrt{4\pi^2+(a/n)^2}+2\pi}\right)+o(1/n)\end{align}$ It follows that $y_n\to 0$ . Is my proof correct? Thanks. Note: $(1): f_n(x)=n\sin (\sqrt{4\pi^2n^2+x^2}-2n\pi)=n\sin t= n(t-\frac {t^3}{3!}) +nR(t),$ where $t=\sqrt{4\pi^2n^2+x^2}-2n\pi=\frac{x^2}{\sqrt{4\pi^2n^2+x^2}+2n\pi}\le \frac {a^2}{4n\pi}$ and $R(t)$ is the remainder term. $(2): |R(t)|\le t^5\sum_{k=1}^\infty \frac{t^{2k-2}}{(2k+3)!}\le \frac{a^{10}}{{4\pi}^{5}n^5}\color{blue}{\sum_{k=1}^\infty \frac{a^{4k-4}}{(4n\pi)^{2k-2}(2k+3)!}}=M\frac{a^{10}}{{4\pi}^{5}n^5}$ . The blue colored series converges and is hence bounded by some $M>0$ . It follows that $|n R(t)|\le M\frac{a^{10}}{{4\pi}^{5}n^4}$ , whence $R(t)=o(1/n)$ .","['real-analysis', 'solution-verification', 'uniform-convergence', 'sequences-and-series', 'limits']"
4484005,Weighted AM-GM Inequality,"I was tried to learn some tools on inequalities, and I learn the ""Weighted AM-GM Inequality"" in a small book: in this book there is an exercise and a solution, I did my attempt and it was look right, but after some time I remembered one thing that make it wrong, but the problem is that the solution in the book  used the same idea that I did (and I think this idea is wrong), so I need to verify if I am right or not! Example 1.2.1. Let $a, b, c$ be positive real numbers such that $a + b + c = 3.$ Show that $a^bb^cc^a\leq1 $ Solution. Notice that $:1=\frac{a+b+c}{3} \geq \frac{ab+bc+ac}{a+b+c}\geq \color{red}{\textrm{$(a^b.b^c.c^a)^{\frac{1}{a+b+c}}$}}$ . I have two notes: -the red expression must be $\color{red}{\textrm{$(b^a.c^b.a^c)^{\frac{1}{a+b+c}}$}}$ . -the both red expressions are wrong because we can't  do this unless $a,b,c$ are positive integers . this is the definition of Weighted AM-GM Inequality in this book :","['algebra-precalculus', 'solution-verification', 'a.m.-g.m.-inequality', 'inequality']"
4484060,Sum with binomial coefficient [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 1 year ago . Improve this question How can I prove that $$\sum_{y=1}^{k \choose s} (-1)^{y+1} \sum_{r=0}^{k} (-1)^{r} {k \choose r} {{k-r \choose s} \choose y} = (-1)^{k-s} {{k-1} \choose s-1} ?$$ I know this is correct from simulations.","['combinatorics', 'combinatorial-proofs', 'discrete-mathematics']"
4484099,"Stokes theorem and parameterization of an ""ellipse""","Calculate the work done by the field $F$ along the curve $L$ with anticlockwise orientation where $F(x,y,z)=(\sin (x^2)-y,2x+y+z,\arctan (z)-x)$ and $L=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2=1\}$ . My thoughts: $$\nabla \times F=(-1,1,3),$$ which motivates the use of Stokes' Theorem, $$\int \limits _LF\cdot dr=\iint \limits _S(\nabla \times F)\cdot n\,dS,$$ where $S$ is a surface such that $\partial S=L$ . I decided to work with $$S=\{(x,y,z)\mid 2x+4y+3z=0,x^2+y^2+z^2\leq 1\},$$ with an upward facing normal. My question is: How can I parameterize $S$ ? My attempt:
Taking $z=\frac{-2x-4y}{3}$ implies $x^2+y^2+\left (\frac{-2x-4y}{3}\right )^2\leq 1$ , which is equivalent to $13x^2+16xy+25y^2\leq 9$ . This implies that a possible parameterization is $$r(x,y)=\left (x,y,\frac{-2x-4y}{3} \right),\,\,13x^2+16xy+25y^2\leq 9.$$ My main issue is with the parameterization of $13x^2+16xy+25y^2\leq 9$ in the $xy$ plane. This is a rotated ellipse, and I am not sure what might be the best way to approach this. Thanks!","['multivariable-calculus', 'stokes-theorem']"
4484109,curves on abelian surface $E\times E$,"It is Exercise 4.16.4 in Kollár , and we are working on $\mathbb{C}$ . Let $E$ be a very general smooth elliptic curve and consider $E\times E$ . Let $m,n>0$ be coprime integers and define the closed subspace $E_{m,n}\subset X$ be the image of $$E\rightarrow E\times E,\quad x\mapsto(mx,nx)$$ We know that $E_{1,0},E_{0,1},E_{1,1}$ is a basis of the group of one-cycles on $E\times E$ . Hence we can write $$[E_{m,n}]=a_1[E_{1,0}]+a_2[E_{0,1}]+a_3[E_{1,1}]$$ To compute the coefficients, I need to know the intersection number $E_{m,n}.E_{1,0}$ and $E_{m,n}.E_{1,1}$ .","['complex-geometry', 'algebraic-geometry', 'elliptic-curves']"
4484116,Related to order statistics?,"say there are $N$ independent and identically distributed random channels i.e., $Z_i \in \{ Z_1, Z_2,..., Z_i\}$ with PDFs $f_{Z_i}(z)$ ordered in ascending order. Then using order statistics the PDF of $Z_1 = \text{min}_i(Z_i)$ is given by $f_{Z_1}(z) = N (1-F_{Z_i}(z))^{N-1}f_{Z_i}(z)$ ---(1) My query is will equation (1) will be valid if the random variables are not independent and not identically distributed. Any help in this regard will be highly appreciated.","['cumulative-distribution-functions', 'statistics', 'probability-distributions', 'density-function']"
4484142,"Calculating $\mathbb{E}[N]$ for $N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}}$, using Wald's lemma","Suppose I have a sequence of $i.i.d$ random variables: $X_1,X_2,... \sim Geom(p)$ .
That means that each of the $X_i$ 's is holding an unknown random number of trials until a 'success'. Since $0<p<1$ , we know that $X_i$ has a finite expectation $\mathbb{E}[X_i] = \frac{1}{p}$ , which also means that there exists an integer $N \in \mathbb{N}$ such that: $$N = \displaystyle \min_{n\in \mathbb{N}}\Big\{X_1+X_2+...+X_n= \sum_{i=1}^{n}{X_i\geq5000}\Big\}$$ We would like to calculate the expectation of this finite integer 'stopping time' $N$ . From Wald's lemma: If $X_i$ are i.i.d. with finite $\mathbb{E}[X_i] = \mu$ , and N is a finite stopping time then: $\mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = \mu\mathbb{E}[N]$ . My problem is how to deal with the 'greater-equal' ( $\geq$ ) sign. Since we define $N = \displaystyle \min_{n\in \mathbb{N}}\Big\{\sum_{i=1}^{n}{X_i\geq5000\Big\}}$ , this means that $X_N$ contibutes a number of trials with which the sum exceeds $5000$ , but we dont know the exact sum. My intuition is something like: if we take the the sum as the bare minimum, then $$\mathbb{E}\Big[\sum_{i=1}^{N}{X_i}\Big] = 5000= \mathbb{E}[N]\times \frac{1}{p}  \to  \mathbb{E}[N] = 5000p$$ But even if that's the case I'm having trouble justifying taking the sum as exactly 5000. Another possible approach is to condition on $\sum_{i=1}^{N}{X_i}=k$ and take the expectation, but $k = 5000, 5001,...$ and I'm not sure how to formulate this, since $k$ is potentially unbounded ( $k\in [5000,\infty)$ ), if that's even a valid approach. I'd love some guidance please.","['expected-value', 'stopping-times', 'probability']"
4484150,Solution verification of a Double Integral,"This question was left as an exercise in class of multivariable calculus of my brother and I am not sure about my solution of it when he asked me the same. Question: Compute the integral $\displaystyle I=\iint \limits _R\frac{y}{x+1}\,dA$ , $R=[0,2]\times [0,4]$ . Attempt: $dA=dx\times dy$ and I thought that $[0,2]$ is limit of $x$ and $[0,4]$ is the limit of $y$ . \begin{align*}I & =\int \limits _0^2\int \limits _0^4\frac{y}{x+1}\,dy\,dx \\
& =\int \limits _0^28\frac{1}{x+1}\,dx \\
& =8\log (x+1)|_0^2 \\
& =8\log 3.
\end{align*} I was confused in what $R=[0,2]\times [0,4]$ means here and that's why I thought on asking a second opinion. Can you please confirm it to me? Also, as I was looking at this question another time, I found that as $A=xy$ , so $dA=d(xy)=y\,dx+x\,dy$ and so my attempt is not right. Can you please confirm it too? So, a new solution will be $\displaystyle \int \limits _0^2\int \limits _0^4\frac{y}{x+1}(y\,dx+x\,dy)$ but now I am getting $\displaystyle \int \limits _0^2\int \limits _0^4\frac{y^2}{x+1}\,dx+\frac{xy}{x+1}\,dy$ . Now, I am confused that in the first integral there is only $dx$ and there are $2$ limits (i.e. of both $x$ and $y$ ) and so is the case for second integral. This seems to be mistake and so I think I am doing something wrong. So, can you please point it out? Thanks!","['integration', 'definite-integrals', 'analysis', 'real-analysis', 'multivariable-calculus']"
4484192,How to find the radius of convergence of series solution of an ODE?,"Recently, I watched a video https://www.youtube.com/watch?v=8lHAMZWDQHI which taught me how to find the radius of convergence of power series solution of an ODE. For example, if we want to find the radius of convergence of power series solution $\displaystyle y=\sum \limits _{i=0}^\infty c_n(x-1)^n$ of ODE $(x^2+2x+5)y''+xy'+y=0$ , just finding the roots of $x^2+2x+5$ then computing the minimum distance between $1$ and one of the roots. Generally, we have an ODE as follows: $$y^{(n)}+p_{n-1}(x)y^{(n-1)}+\cdots +p_1(x)y'+p_0(x)y=0,$$ where $p_i(x)$ is analytic in the complex plane except for some poles, and $\rho _1,\rho _2,\ldots ,\rho _k$ are regular singular points. Assume $\displaystyle y(x)=\sum c_kx^n$ is a nonzero formal power series solution. Is it true that the radius of convergence of $y(x)$ equals $\displaystyle d=\min \limits _{1\leq i\leq k}\rho _i$ ? Is it possible that there exists $|z_0|>d$ making $y(z_0)$ converge? Thanks very much for your precious help.","['complex-analysis', 'power-series', 'ordinary-differential-equations']"
4484201,An Inverse Jensen Inequality,"Given $\sigma>0$ , let $X\sim N(0,\sigma^2I_d)$ be a normal random variable in $\mathbb R^d$ . Prove or disprove: there exists a constant $C$ such that for any 1-Lipschitz function $f:\mathbb R^d\rightarrow\mathbb R$ , $$
\log\big(\mathbb E[e^{f(X)}]\big) - \mathbb E[f(X)] \leqslant C\sigma^2.
$$ Here, $f$ is 1-Lipschitz means that $f$ is continuous in $\mathbb R^d$ and $|f(x)-f(y)|\leqslant |x-y|$ for any $x,y\in\mathbb R^d$ . This problem comes from the proof of Proposition 3 in this paper , where the authors claimed that the log-Sobolev inequality can be used to prove eq. 25. The problem above can be viewed as a simplified version of eq. 25 of the paper. However, I found it non-trivial to obtain eq. 25 using the log-Sobolev inequality, the log-Harnack inequality, or other functional inequalities I know. It looks more like an inverse type of the Jensen's inequality, which is the reason this problem is so named. Here are some direct observations of this problem. First of all, by Jensen's inequality, one has $$
\log\big(\mathbb E[e^{f(X)}]\big) - \mathbb E[f(X)] \geqslant 0,
$$ which seems no help. Another idea is to consider the random variable $Y = f(X)$ rather than $X$ itself. Then it may be reasonable to consider $$
\log\big(
\mathbb E[e^Y]\big) - \mathbb E[Y] \leqslant \mathrm{Var}(Y).
$$ Unfortunately, this inequality does not hold for arbitrary $Y$ . Also, the Lipschitz property of $f$ becomes implicit here. In the case $d=1$ , if one chooses $f(x) = x$ directly, it can be verified that $$
\log\big(\mathbb E[e^{X}]\big) - \mathbb E[X] = \frac{\sigma^2}2.
$$ This is why the RHS has a square term of $\sigma$ . Any suggestions are appreciated. Even the solution in the case $d=1$ is fine.","['inequality', 'jensen-inequality', 'probability']"
4484205,why selecting objects one by one without replacement and selecting objects at the same time give same probability?,"I am trying to understand why the probability of selecting objects one by one without replacement is the same as selecting them at once. I see it algebraticaly thanks to the hypergeometric distribution. However ,i cannot  intuitively convince myself . I always feel that they must be different ,because when we draw objects one by one without replacement , it cause dependency , but when we draw them at once ,i.e selecting them at the same time , i feel that the dependency will be negated. Briefly , i want a clear explanation (no algebraic ) about why selecting objects one by one without replacement and selecting objects at the same time give same probability ? Intuative and clear clarifications will be appreciated. Thanks in advance ! A simple example : We selects two employees for a job from a group of six people, of which one is female and five are male. Find the probability that the female is selected ? First answer : $$\frac{\binom{1}{1}\binom{5}{1}}{\binom{6}{2}}=1/3$$ Second answer: $$(1/6)(5/6)+(5/6)(1/6)=1/3$$","['intuition', 'probability-distributions', 'discrete-mathematics', 'probability']"
4484217,Evaluating $\int_{0}^{t}\frac{e^{-x}\sin(x)}{x}dx$,"In my assignment of Laplace transforms, I have been given the task to evaluate the integral $$\int_{0}^{t}\frac{e^{-x}\sin(x)}{x}dx$$ The case when $t\to \infty$ can be easily tackled. But for any $t>0$ , it seems rather absurd and tedious, and not at all related to Laplace transform. Here's what I have done. $$\begin{aligned}
I=\int_{0}^{t}\frac{e^{-x}\sin(x)}{x}dx&=\int_{0}^{t}\sum_{r=0}^{\infty}\frac{(-1)^{r}x^{r}}{r!}\frac{\sin(x)}{x}dx \\ &=\sum_{r=0}^{\infty}\frac{(-1)^{r}}{r!}\int_{0}^{t}x^{r-1}\sin(x)dx
\end{aligned}$$ Now, using Integration by Parts, I arrived at the following result. $$\int_{0}^{t}x^{r-1}\sin(x)dx=\sum_{i=0}^{r-1}(-1)^{i}i!{r-1\choose i}t^{r-1-i}\sin\left(t+\frac{\pi}{2}(1+i)\right)$$ $$I=\sum_{r=0}^{\infty}\frac{(-1)^{r}}{r!}\sum_{i=0}^{r-1}(-1)^{i}i!{r-1\choose i}t^{r-1-i}\sin\left(t+\frac{\pi}{2}(1+i)\right)$$ Is there some way to solve this, and is the above evaluation correct. Thanks in advance.","['integration', 'definite-integrals', 'laplace-transform', 'calculus', 'power-series']"
4484225,Sylow $2$-subgroup Mathieu Group $M_{24}$,"I need to compute the Sylow $2$ -subgroup of the Mathieu Group $M_{24}$ . Unfortunately, this is hard to identify with a machine as it is of order $2^{10}$ and therefore not on the GAP library. I have also looked at a few books and papers on the Mathieu group, but could not find the information. Any other suggestions?","['computational-algebra', 'sporadic-groups', 'sylow-theory', 'group-theory', 'mathieu-groups']"
4484252,A silly question on range of a quadratic function.,"Let $f:\mathbb{R}\to\mathbb{R}\space|\space f(x) = x^2 + 3x + 2 \space\forall\space x 
\in\mathbb{R}$ We are asked to find the range of the function $f$ I begin as follows: Assume $y=x^2 + 3x + 2$ for some $ x\in\mathbb{R}$ Collecting terms to one side, $x^2 + 3x + 2-y=0$ Now as x is real, the discriminant of the above quadratic expression must be greater than or equal to zero. Hence, $3^2 - 4(1)(2-y)\geqslant0$ Solving, we get, $y\geqslant-1/4$ Hence the range of the function $f$ is the set $A=\{x|x\geq-1/4\}$ This answer is correct. The range of the function $f$ is indeed the set $A=\{x|x\geq-1/4\}$ . However, I have a problem with my last step. The final equality only tells me that $y$ is greater than OR equal to $-1/4$ . It doesn't exactly say that $y$ WILL take all values greater than AND equal to $-1/4$ . Is there a way to prove that $y$ will take all such values? Any hint or help regarding this will be appreciated. Thank you","['algebra-precalculus', 'quadratics', 'inequality']"
4484261,"Two different recurrence relations, same solution","Say, I want to solve two recurrence relations in 2D given by $$\begin{cases}\alpha_{i,j}=\frac{1}{4}\left(\alpha_{i+1,j}+\alpha_{i-1,j}+\alpha_{i,j+1}+\alpha_{i,j-1} \right)\\\alpha_{i,j}=\frac{1}{4}\left(  \alpha_{i+1,j+1} + \alpha_{i-1,j+1} + \alpha_{i-1,j-1} +\alpha_{i+1,j-1} \right)\end{cases}$$ where $\alpha_{i,j}$ is the solution to both equations simultaneously for the same choice of boundary values $\alpha_{i,j}^B$ for each equation. Now I'm wonderng, if there can exist such solutions. My intuition is that there shouldn't, especially as the equations are not linear superpositions of each other. I'm not sure if my question is already ruled out by uniqueness of the solution.","['boundary-value-problem', 'recurrence-relations', 'harmonic-functions', 'discrete-mathematics']"
4484281,Calculate $\lim \sqrt[n] \frac{(2n)!}{(n !)^2}.$ [duplicate],"This question already has answers here : Show that that $\lim_{n\to\infty}\sqrt[n]{\binom{2n}{n}} = 4$ (7 answers) Closed 1 year ago . I want to calculate $$\lim_{n\to \infty} \sqrt[n] \frac{(2n)!}{(n !)^2}$$ According to Wolfram alpha https://www.wolframalpha.com/input?i=lim+%5B%282n%29%21%2F%7Bn%21%5E2%7D%5D%5E%7B1%2Fn%7D , this value is $4$ , but I don't know why. I have $\sqrt[n]{\dfrac{(2n)!}{(n !)^2}}=\sqrt[n]{\dfrac{2n\cdot (2n-1)\cdot \cdots \cdot (n+2)\cdot (n+1)}{n!}}$ but I have no idea from here. Another idea is taking $\log.$ $\log  \sqrt[n] \frac{(2n)!}{(n !)^2}=\dfrac{\log \frac{(2n)!}{(n !)^2}}{n}
=\dfrac{\log \dfrac{2n\cdot (2n-1)\cdot \cdots \cdot (n+2)\cdot (n+1)}{n!}}{n}
=\dfrac{\log [2n\cdot (2n-1)\cdot \cdots \cdot (n+2)\cdot (n+1)]-\log n!}{n}
$ . This doesn't seem to work. Do you have any idea or hint ?","['limits', 'radicals', 'factorial']"
4484351,Are groups always isomorphic to their image under power maps?,"Let $(G, \cdot)$ be a finite group of order $n$ . Consider the map: $$G \to G, g \mapsto g^k$$ for $k$ , $n$ coprime. This is injective, but generally not a homomorphism. Define a new group $G_k = (G,\circ)$ by: $$g \circ h = (g^k h^k)^{\frac{1}{k}}$$ where naturally $g \mapsto g^{\frac{1}{k}}$ is the (well-defined) inverse of $g \mapsto g^k$ . It is easily verified that this gives a valid group operation. Question : Is it true that $G \cong G_k$ for all $k$ coprime to $n$ ? The map is clearly an isomorphism if $G$ is abelian (or more generally, $k$ -abelian). Also, note that the order of any $g \in G_k$ is the same as the order of $g \in G$ . Thus, $G, G_k$ have the same order sequence, so according to this answer , any counterexample must have $n \ge 16$ , and I am not very familiar with these groups. (But I do not expect the statement to be true.) I also noticed that if $G \sim H$ when $H \cong G_k$ for some $k$ (coprime to $|G|$ ), then $\sim$ is an equivalence relation.","['group-theory', 'abstract-algebra', 'finite-groups']"
4484375,Prove that $f(x)=0$ for all $x\in\mathbb R$ when $f(xy)=xf(x)+yf(y)$.,"My attempt: We let $y=0$ , so $f(0)=xf(x)$ . When both $x$ and $y$ are $0$ , we get $f(0)=0\implies xf(x)=0 \implies f(x)=0.$ Two questions: Is this rigorous enough? When we divide both sides by $x$ to get $f(x)=0$ , should we account for the case when $x=0$ ?","['algebra-precalculus', 'functions', 'solution-verification']"
4484425,Let $f$ be proper convex such that $f \in L_1 (\mu)$ and $\mu$ not give mass to small sets. Then $f$ is differentiable $\mu$-a.e.,"I'm reading Section 2.1 of Chapter 2 in Villani's textbook Topics in Optimal Transportation. Now, let us assume that $\mu$ does not give mass to small sets, and let $\varphi$ be as above. Since $\varphi$ lies in $L^{1}(d \mu)$ , it is $d \mu$ -almost everywhere finite: $\mu[\operatorname{Dom}(\varphi)]=1$ . On the other hand, the border $\partial \operatorname{Dom}(\varphi)$ of the convex set $\operatorname{Dom}(\varphi)$ is a small set; so, $\mu[\operatorname{Int}(\operatorname{Dom}(\varphi))]=1$ . Now, on $\operatorname{Int}(\operatorname{Dom}(\varphi))$ , the set of nondifferentiability of $\varphi$ is a small set. On the whole, $d \mu$ -almost every point of $X$ is a differentiability point for $\varphi$ . So, for $d \mu$ -almost all $x$ , the subdifferential of $\varphi$ at the point $x$ is $\{\nabla \varphi(x)\}$ . Recalling that a statement true for $d \mu$ -almost all $x$ is also true for $d \pi$ -almost all $(x, y)$ , we obtain that $y=\nabla \varphi(x)$ for $d \pi$ -almost all $(x, y)$ . I'm trying to fill in the detail by proving below theorem, i.e., Let $X =\mathbb R^d$ and $\mu$ be a Borel probability measures on $X$ . A subset $B$ of $X$ is a small set if its Hausdorff dimension $\dim_H (B)$ is at most $d-1$ . Assume that $\mu$ does not give mass to small sets, i.e., $\mu (B) =0$ for every Borel set $B$ which is also a small set. Let $f:X \to \mathbb R \cup \{+\infty\}$ be proper convex such that $f \in L_1 (\mu)$ . Theorem: The map $f$ is differentiable $\mu$ -a.e. If $N$ be a $\mu$ -null subset of $X$ such that $f$ is differentiable on $N^c := X \setminus N$ , then the gradient $\nabla f :N^c \to X$ of $f$ is measurable. I'm not sure if my below attempt is fine, or it contains some logical mistakes. Could you please have a check on it? My attempt: Let $D_1 := \operatorname{dom} f := \{x \in X \mid f(x) < +\infty\}$ and $D_2 := \operatorname{int} D_1$ . Because $f$ is proper convex, $D_1 \neq \emptyset$ is convex. Because $f \in L_1 (\mu)$ , $D_1$ is Borel measurable and $\mu(D_1) = 1$ . Because $D_1$ is convex, its boundary $\partial D_1$ is a small set , i.e., $\dim_H (\partial D_1) \le d-1$ . Then $\mu(\partial D_1)=0$ . Let $$
E := \{x \in D_2 \mid  f \text{ is not differentiable at }x\}.
$$ Then $E$ is Borel measurable and is a small set , i.e., $\dim_H (E) \le d-1$ . So $\mu (E) = 0$ . Let $F := D_2 \setminus E$ . Then $$
F = \{x \in D_2 \mid  f \text{ is differentiable at }x\}
$$ is Borel measurable and $\mu (F) =1$ . Clearly, $f$ is differentiable on $F$ and thus differentiable $\mu$ -a.e. Let $N$ be a $\mu$ -null subset of $X$ such that $f$ is differentiable on $N^c := X \setminus N$ . Then the gradient $\nabla f : N^c \to X$ of $f$ is well-defined. Let $$
\frac{\partial f}{ \partial x_i} : N^c \to \mathbb R \quad i = 1, \ldots,d
$$ be the partial derivatives of $f$ on $N^c$ . We have $\nabla f$ is Borel measurable if and only if $\frac{\partial f}{ \partial x_i}$ is measurable for all $i = 1, \ldots,d$ . Let $\{e_1, \ldots, e_d\}$ be an orthonormal basis of $X$ . Notice that $$
\frac{\partial f}{ \partial x_i} (x) = \lim_{n \to \infty} \frac{f(x+e_i/n)-f(x)}{1/n} = \lim_{n \to \infty} n \left [ f \left (x + \frac{e_i}{n} \right) - f (x) \right ].
$$ Because $f$ is continuous, the map $$
g_{n,i}: N^c \to \mathbb R, x \mapsto n \left [ f \left (x + \frac{e_i}{n} \right) - f (x) \right ]
$$ is measurable for each $n \in \mathbb N^*$ and each $i \in \{1, \ldots, d\}$ . It follows that $$
\frac{\partial f}{ \partial x_i} = \lim_n g_{n, i}
$$ is measurable. This completes the proof.","['measure-theory', 'calculus-of-variations', 'multivariable-calculus', 'derivatives', 'convex-analysis']"
4484433,Non-diagonal n-th roots of the identity matrix,"Q . Are there $n$ -th root analogs of this non-diagonal cube-root of the $3 \times 3$ identity matrix? \begin{align*}
\left(
\begin{array}{ccc}
 0 & 0 & -i \\
 i & 0 & 0 \\
 0 & 1 & 0 \\
\end{array}
\right)^3
= \left(
\begin{array}{ccc}
 1 & 0 & 0 \\
 0 & 1 & 0 \\
 0 & 0 & 1 \\
\end{array}
\right)
\end{align*} I am looking for $A^n=I$ , where $I$ is any dimension $\le n$ . (A naive question: I am not an expert in this area.)","['matrices', 'linear-algebra', 'roots-of-unity']"
4484476,Better upper bound for $ \sum_{k=2}^{\infty} \frac{1}{2^{k-1}} \sum_{n=1}^{\infty} (n! \: \text{mod} \: k) $,"Since the sum $\sum_{n=1}^{\infty} (n! \: \text{mod} \: k)$ will be zero beyond $k-1$ , the series could be interpreted as an finite sum of length $k-1$ . Also, the max value of $n! \: \text{mod} \: k$ is naturally $k-1$ . Taken together one has a max value of: $$ \sum_{k=2}^{\infty} \frac{(k-1)^2}{2^{k-1}} = 6 $$ Mathematica gives the value of $$ \sum_{k=2}^{\infty} \frac{1}{2^{k-1}} \sum_{n=1}^{\infty} (n! \: \text{mod} \: k) \approx 3.005674093 $$ so my upper bound is clearly quite crude. What would be a better upper bound? Edit: To be clear my upper bound is $\frac{x (x+1)}{(1-x)^3}$ for $ \sum_{k=2}^{\infty} x^{k-1} \sum_{n=1}^{\infty} (n! \: \text{mod} \: k)$ .",['sequences-and-series']
4484541,Proving irreducibility for $\frac{a^2+b^2}{ac+bd}$,"For positive integers $a,b,c,d$ satisfying $ad-bc=1,$ prove that $\frac{a^2+b^2}{ac+bd}$ is irreducible. That is, $\gcd(a^2+b^2, ac+bd)=1.$ I know how to prove $\frac{a+b}{c+d}$ is irreducible if $ad-bc$ with bezouts theorem, but am not sure how to prove $\frac{a^2+b^2}{ac+bd}$ is irreducible. I tried using proof by contradiction, writing $km=a^2+b^2$ and $kn=ac+bd$ and $\gcd(m,n)=1$ for $k>1.$ Then from the first equation I got $b=\sqrt{km-a^2}.$ Then plugging it into the second equation I got $kn=ac+(\sqrt{km-a^2})d.$ Then, $c=\tfrac{kn-d\sqrt{km-a^2}}{a}.$ So then $$ad-bc=ad-(\sqrt{km-a^2})\left(\frac{kn-d\sqrt{km-a^2}}{a}\right)=ad-\left(\frac{kn\sqrt{km-a^2}}{a}-\frac{dkm}{a}+ad\right).$$ Thus, $$ad-bc=\frac{dkm}{a}-\frac{kn\sqrt{km-a^2}}{a}=1.$$ This doesn't seem to lead anywhere useful, though. There are two many variables and there seems like no good way to prove that $k=1.$ Is there a better way then this? Possibly a way to prove irreducibility like proving $\frac{a+b}{c+d}$ is irreducible using bezouts theorem? Thanks in advance.","['number-theory', 'elementary-number-theory']"
4484553,Can differentiability classes be extended to negative values?,"A function $f$ is said to be of differentiability class $C^k$ if its first $k$ derivatives are continuous. It has the property that $m>n \implies C^m \subset C^n$ and that all $C^k$ s are algebras. My question is this: Seeing as many continuous functions are the integrals of discontinuous functions, can we define $C^k; k<0$ to be the set of functions whose $k^{th}$ integral is continuous? As the integral of a continuous function is continuous, $C^0 \subset C^k;k<0$ and it’s pretty easy to check that this is a vector space, but is it an algebra? Another question is if there exists a $C^{-\infty}$ as the intersection of all $C^k;k<0$ analogously to how $C^{\infty}$ is defined. If this set exists, is it still an algebra and what would it look like? Seeing as $C^k$ gets broader as $k$ gets smaller, $C^{-\infty}$ is actually just the limit as $k$ approaches $-\infty$ . Another thing to note would be what functions aren’t in $C^{-\infty}$ and the other $C^k$ s.","['integration', 'analysis', 'distribution-theory', 'functions', 'indefinite-integrals']"
4484607,Number of ways to divide $10$ children into two teams of $5$?,"Consider the following two examples from A First Course in Probability by Sheldon Ross, EXAMPLE 5b Ten children are to be divided into an A team and a B team of 5 each. The A team will play in one league and the B team in another. How many different divisions are possible? Solution by the Book itself There are $\frac{10!}{5!5!} = 252$ possible divisions. My Solution and Reasoning We first choose five children which can be done in $C(10, 5)$ possible ways. Then we decide which team(either A or B) to assign these children to, which can be done in $2!$ ways. Finally we assign the remaining five children to the remaining team, depending on the second level(if the first five children were assigned to A, then the remaining five should be assigned to B, and if the first five were assigned to B, the remaining to A). Therefore this level has only $1$ possible way, and hence the final answer is $C(10, 5) \times 2! \times 1 = 252 \times 2 \times 1 = 504$ . EXAMPLE 5c In order to play a game of basketball, 10 children at a playground divide themselves into two teams of 5 each. How many different divisions are possible? Solution by the Book itself Note that this example is different from Example 5b because now the order of the two teams is irrelevant. That is, there is no A and B team, but just a division consisting of 2 groups of 5 each. Hence, the desired answer is $\frac{10!/(5!5!)}{2!} = 126$ My Solution and Reasoning First we choose 5 children for one team which can be done is $C(10, 5)$ ways and then we put the remaining children in the other team, which is done in $1$ way. Thus the answer is $C(10, 5) \times 1 = 252$ , which can also be obtained by dividing the answer to Example 5b by 2, or in other words, removing the order of the groups in Example 5b. There seems to be a conceptual misunderstanding in my way of thinking, but what is it?",['combinatorics']
4484670,Why doesn't the condition $a/s=b/t$ iff $at=bs$ suffice when defining localization of commutative rings? [duplicate],"This question already has answers here : Equivalence of elements in a ring of fractions (2 answers) Closed last year . Let $A$ be a commutative ring with identity. Let $S \subseteq A$ be a multiplicatively closed set. Then the localization of $A$ by $S$ is defined as $S^{-1}A = \frac{A \times S}{\sim}$ where $(a,s) \sim (b,t) \iff \exists u \in S,$ such that $u(at-bs)=0$ . My question is, why do we have a condition $\exists u\in S$ and not $\forall u \in S$ ? Since in case of construction of $\mathbb{Q}$ from $\mathbb{Z}$ , my intuitions says , if we want $\frac{a}{s}=\frac{b}{t}$ then we better make sure $\frac{a}{s}=\frac{b}{t}=\frac{ua}{us} $ for all $u\in S:= \mathbb{Z}-\{0\}$ and hence $u(at-bs)=0$ for every $u \in S$ . Why don't we translate this to general rings? I can see that this has something to do with $\mathbb{Z}$ not allowing any zero divisors. But I am unable to see clearly.","['localization', 'abstract-algebra', 'commutative-algebra']"
4484672,Simplifying $\textrm{sinc}( \textrm{acos}(\cdot) )$,"Geometry is not my strong point - I'm trying to clean up some equations from a paper and ended up with this mess. $$ \textrm{sinc} \left[ \textrm{acos} \left( \frac{p_{x}}  {\sqrt{p_x^2 + p_y^2 + p_z^2}} \right) \right] \\ $$ I've never really came across sinc before. $$ \textrm{sinc}(\theta) = \frac{ \sin(\theta)} {\theta} $$ It's come from substituting the angle with vectors to generalise the equation. where theta is the angle between point vector p and the origin $$ p=(p_x, p_y, p_z) $$ giving $$ \theta = \textrm{acos} \left( \frac{p_{x}}  {\sqrt{p_x^2 + p_y^2 + p_z^2}} \right)  \\ $$ Can this be simplified without ending up with the same mess in the denominator?","['trigonometry', 'geometry']"
4484674,Limit question involving the the greatest integer function and fractional part,"The value of $$\mathop {\lim }\limits_{n \to \infty } {\left( {\frac{{\sin \left\{ {\frac{2}{n}} \right\}}}{{\left[ {2n\tan \frac{1}{n}} \right]\left( {\tan \frac{1}{n}} \right)}} + \frac{1}{{{n^2} + \cos n}}} \right)^{{n^2}}},$$ where $[.]$ denotes greatest integer function and $\{.\}$ denotes fractional part function, is _________. My approach is as follow $\mathop {\lim }\limits_{n \to \infty } \sin \left\{ {\frac{2}{n}} \right\} = {0^ + };\mathop {\lim }\limits_{n \to \infty } \left[ {2n\tan \frac{1}{n}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {2\frac{{\tan \frac{1}{n}}}{{\frac{1}{n}}}} \right] = \mathop {\lim }\limits_{n \to \infty } \left[ {{2^ + }} \right] = 2$ $\mathop {\lim }\limits_{n \to \infty } \tan \frac{1}{n} = 0;\mathop {\lim }\limits_{n \to \infty } \frac{1}{{{n^2} + \cos n}} = \mathop {\lim }\limits_{n \to \infty } \frac{1}{{n\left( {1 + \frac{{\cos n}}{n}} \right)}} = 0.$ Cannot approach from here.","['limits', 'calculus', 'real-analysis']"
4484706,Give an example of a set which has exactly 3 limit points and the set is closed.,"So the question is, Give an example of a set of real numbers which has exactly 3 limit points and the set is closed. I have been trying to solve it by myself, and I know one potential set can be $A$ , such that, $A = \{k + \frac{1}{n}\ : n \in \mathbb{N} \}$ Where $K$ is the limit point of $A$ . The problem with this set is, this set is not closed, by the definition of a closed set, the set also need to contain it's limit points, but $k \notin A$","['limits', 'set-theory', 'real-analysis']"
4484726,How to represent $\mathbb{R}^n$ as a matrix?,"Is there a basis in which Hadamard product in $\mathbb R^n$ is isomorphic to the matrix product, and on the matrix main diagonal all elements be equal to the mean value of the elements of the vector in $\mathbb R^n$ ? For instance, in $2$ -dimensional case, we have $$ (a,b) \to \begin{pmatrix} \dfrac{a+b}{2} & \dfrac{b-a}{2} \\ \dfrac{b-a}{2} & \dfrac{a+b}{2} \end{pmatrix} $$ What would be in the $n$ -dimensional case?","['matrices', 'linear-algebra']"
4484728,Prove $ \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} } \det(A) \right]^T $,"I have a bit of trouble proving an adjugate matrix equality for $A_{n\times n} = [a_{ij}]$ , $$ \mbox{adj} (A) = \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T, \quad i, j = 1,\dots,n $$ I tried by definition $$ \mbox{adj} (A) = \left[
(-1)^{i+j} \sum_{\sigma \in C_n } \left( \mbox{sgn} (\sigma)  \prod_{k=1\\k\neq i   }^n  a_{k,\sigma(k)} \right)
\right]^T
$$ where $C_n = \{\sigma \in S_n : \sigma(i) = j \} $ and $S_n$ is the set of all $n$ -permutation of the set $\{1,\dots,n\}$ . $sgn(\sigma)$ is $-1$ if permuatation $\sigma$ is odd , it's $1$ otherwise . $\sigma(k)$ returns the $k$ th element of $\sigma$ . The thing within the square bracket is the $(i,j)-$ cofactor . I pause here and look at the RHS of the statement $$ \left[ \frac{\partial }{\partial a_{ij} }  \det(A)  \right]^T 
= \left[\sum_{\sigma\in C_n } \left( \mbox{sgn} (\sigma)   \prod_{k=1\\ k \neq i}^n  a_{k,\sigma(k)} \right)
\right]^T $$ But I'm unable to deal with the signs ...","['determinant', 'proof-writing', 'matrices', 'abstract-algebra', 'matrix-calculus']"
4484736,How to compare scores with different possible point totals,"Is there way to fairly compare multiple scores with varying possible point totals such as 26/90, 54/70,67/80, etc...? This is based on the same test but not all questions apply to everyone taking it, so people don't lose or gain points if they are not able to answer. We also can not only compare questions relating to all participants. 90 is the maximum amount possible to score if all questions answered.",['statistics']
4484765,Having trouble understanding the generalized chain rule for multivariable functions,"Determine the derivative of $F\circ G$ where $F:\mathbb{R^3}\rightarrow \mathbb{R^2}$ and $G:\mathbb{R^2}\rightarrow \mathbb{R^3}$ are given by $$F(x,y,z)=\begin {pmatrix} \cos(x)-z \\xe^{y}\end {pmatrix}, G(u,v)=\begin {pmatrix} v^2\\uv+v^3 \\ u^2+v\end {pmatrix} $$ I'm having trouble applying the generalized chain rule for this question. $F\circ G=F(G(u,v)) = \begin {pmatrix} \cos(v^2)-(u^2+v)\\v^2e^{uv+v^3}\end {pmatrix}.$ Unless I'm mistaken we can treat the composition of the functions as a different function such that $F\circ G =h:\mathbb{R^2}\rightarrow \mathbb{R^2}$ and then differentiating this function would simply be taking the partial derivatives of the two expressions: $$D=\begin {pmatrix}\frac{\partial}{\partial u} \big(\cos(v^2)-u^2-v \big)& \frac{\partial}{\partial v} (\cos(v^2)-u^2-v) \\ \frac{\partial}{\partial u} (v^2\exp({uv+v^3})) & \frac{\partial}{\partial v}(v^2\exp(uv+v^3))\end {pmatrix}$$ But I honestly don't know if any of this is correct and would appreciate if someone could help.","['partial-derivative', 'multivariable-calculus', 'derivatives', 'real-analysis']"
4484810,"Find all continuous functions $f$ satisfying $\int\limits_0^x f=(f(x))^2+C$, for some constant $C\neq 0$. Why is the assumption $C\neq 0$ necessary?","The following is a problem from Chapter 14 ""The Fundamental Theorem of Calculus"" from Spivak's Calculus (a) Find all continuous functions $f$ satisfying $$\int\limits_0^x f=(f(x))^2+C, \text{ for some constant } C \neq 0\tag{1}$$ assuming that $f$ has at most one $0$ . (b) Also find a solution that is $0$ on an interval $(-\infty, b]$ with $0<b$ , but non-zero for $x>b$ (c) Finally, for $C=0$ and any interval $[a,b]$ with $a<0<b$ , find a solution that is $0$ on $[a,b]$ , but non-zero elsewhere. My question is: why the assumption that $C \neq 0$ ? The solution to this problem doesn't seem to require that assumption. Here is my solution to part $(a)$ First, note that $$\int_0^0 f = 0 = (f(0))^2+C \implies f(0)=\pm \sqrt{-C}$$ Therefore, if $f$ satisfies $(1)$ then $C<0$ . Assume $f$ is continuous. Then we can apply the FTC1 to $(1)$ $$f(x)=2f(x)f'(x)$$ $$f(x)(1-2f'(x))=0\tag{2}$$ By assumption, $f$ is zero at most at one point. Assume $f(x_0)=0$ . Then at all $x\neq x_0$ we have $$f'(x)=\frac{1}{2}\tag{3}$$ But we know that a function of the form $f(x)=\frac{x}{2}+b$ satisfies $(3)$ . Therefore, we can use the FTC2 to obtain $$\int_0^x \frac{1}{2}=f(x)-f(0)=\frac{x}{2}$$ $$f(x)=\frac{x}{2}+f(0)$$ $$f(x)=\frac{x}{2}\pm \sqrt{-C}=\frac{x}{2}\pm k, k\in\mathbb{R}$$ This represents the set of all lines with slope $1/2$ . If we choose $C=0$ then we have $$f(x)=\frac{x}{2}$$ $$\int_0^x \frac{x}{2}dx = \frac{x^2}{4}=\left ( \frac{x}{2} \right )^2=(f(x))^2$$ So the question remains: why the assumption that $C\neq 0$ ? Here is the solution to part $(b)$ For this item, we are dropping the assumption that $f$ is zero at only one point. As before, we have eq. $(2)$ . If $f(x)=0$ for all $x \in (-\infty,b]$ , then for $x>b$ we have $f'(x)=\frac{1}{2}$ . This means that $$f(x)=\begin{cases} 0, \text{ if } x\leq b \\ \frac{x}{2}+k, \text{ if } x>b, \text{ with } k \in \mathbb{R} \end{cases}$$ Part (c) is similar, but now $f$ is zero on an interval $[a,b]$ with $a<0<b$ . By the same steps as before, we have that $f'(x)=\frac{1}{2}$ for $x \in (-\infty,a) \bigcup (b,\infty)$ .","['integration', 'proof-explanation', 'calculus', 'derivatives']"
4484848,Sequence of $0$’s and $1$’s without six consecutive identical blocks,"Let $S_n$ be the number of sequences of $n$ zeroes and ones such that the sequence does not contain six consecutive identical blocks of numbers. Show that $S_n$ tends to infinity as $n\to\infty$ . Since we just need to show $S_n$ tends to infinity, I think a rough estimate should be enough. I think I should consider arbitrarily long sequences of zeroes and ones that follow a particular form and that don't contain six consecutive identical blocks of numbers. Or maybe some recurrence relation involving $S_n$ with some base cases might do the trick. Let $T_n$ denote the set of binary strings counted by $S_n$ . Then $T_i$ is the set of length i binary strings for $i < 6.$ $T_6$ only excludes the all zero and all one strings. I think $T_7$ just excludes $\{10^{(6)}, 0^{(6)}1, 0 1^{(6)}, 1^{(6)}0\}$ since we can only have a block of $1$ repeating $6$ times consecutively. Obviously for a string of length n, the maximum length of an identical block that repeats at least 6 times consecutively is $\lfloor \frac{n}6\rfloor$ .","['contest-math', 'combinatorics-on-words', 'combinatorics', 'discrete-mathematics']"
4484866,"Convergence for ""general"" version of Riemann zeta function?","Let $\{u_k\}_{k\in\mathbb{N}}$ be a non-decreasing sequence of positive reals limiting to infinity. We define the ""general"" Riemann zeta sum for $\{u_k\}_k$ by $
\zeta(s) = \sum_{k=1}^{\infty} (u_k)^{-s}
$ . We denote by $S \subset \mathbb{C}$ the set of $s \in \mathbb{C}$ for which the sum above converges. My question is, in general what does $S$ look like? We know for $u_k = k$ that $S$ is a right half-plane $\{\mathrm{Re}s>1\}$ . In general can we say $S$ is a right half-plane, to the right of the imaginary axis? Note: a result like this seems to be used just after equation (1.20) in this paper of Dan Freed. Of course, there are probably more constraints on $\lambda_k = u_k$ in the situation given in the paper than I've put here, but I was wondering how general the result used is. I tried taking a look at the paper which is cited as [Se] after equation (1.20) in Dan Freed's paper, but I couldn't really understand it enough to extract the above statement.","['complex-analysis', 'convergence-divergence', 'riemann-zeta', 'real-analysis']"
4484889,Sophomore's Eternal Dream $\int_0^1\cdots\int_0^1 (t_1t_2\cdots t_n)^{t_1t_2\cdots t_n} dt_1 dt_2\cdots dt_n$,"Given $$
{\tt I}_{n} =
\int_{0}^{1}\int_{0}^{1}\cdots\int_{0}^{1} \left(\,{t_{1}t_{2}\ldots t_{n}}\,\right)^{t_{1}\ t_{2}\ \ldots t_{n}}\,\,\,{\rm d}t_{1}\,{\rm d}t_{2}\ldots{\rm d}t_{n}
$$ Prove: $$I_1=I_2<I_3<\cdots <\lim_{n\rightarrow\infty}I_n=1$$ For $n=1$ , it is Sophomore's dream: https://en.wikipedia.org/wiki/Sophomore%27s_dream $$ I_1=\int_0^1 t_1^{t_1} dt_1=\sum_{n=1}^\infty \frac{(-1)^{n-1}}{n^n}$$ For $n=2$ , $$I_2=\int_0^1\int_0^1 (t_1t_2)^{t_1t_2}dt_1 dt_2$$ let $x=t_1, y=t_1 t_2$ , where the integral region is mapping to the triangle region $0<x<1, 0<y<x$ , and Jacobian $J=\frac{1}{x}$ $$I_2=\int_0^1\int_0^x y^y \frac{1}{x}dy dx=\int_0^1\int_y^1 y^y \frac{1}{x} dx dy=-\int_0^1 y^y\ln(y) dy$$ Series expansion: $$y^y=e^{y\ln(y)}=\sum_{n=0}^\infty \frac{1}{n!}y^n \ln^n(y)$$ $$I_2=-\sum_{n=0}^\infty \frac{1}{n!}\int_0^1 y^n \ln^{n+1}(y) dy$$ $$\int_0^1 y^n \ln^{n+1}(y) dy=\frac{(-1)^{n+1}n!}{(n+1)^{n+1}}$$ After simplify the result we got: $$I_2=I_1$$ Next, for $I_3$ , let $x=t_1, y=t_1t_2, z=t_1t_2t_3$ , Jacobian is $J=\frac{1}{z}$ $$I_3=\int_0^1\int_0^x\int_0^y z^z\frac{1}{z} dzdydx$$ Question.1: Is there an analytic result for $I_3$ ? Question.2: How to generalize the calculation to $I_n$ , as well as the limit of $I_n$ ?","['integration', 'improper-integrals', 'definite-integrals']"
