question_id,title,body,tags
4157016,"trigonometrical inequality $\tan{A} + \tan{B} + \tan{C} = \tan{A}\tan{B}\tan{C}$, Hint to prove $S2\neq 1$? [duplicate]","This question already has answers here : $\tan A + \tan B + \tan C = \tan A\tan B\tan C\,$ in a triangle (12 answers) Closed 3 years ago . I was studying conditional identities for triangle in trigonometry where I had to prove that $\tan{A} + \tan{B} + \tan{C} = \tan{A}\tan{B}\tan{C}$ So I started with $\tan({A+B+C)}=\frac{S1-S3}{1-S2}$ where $S1=\sum_{cyc}\tan A$ , $S2=\sum_{cyc}\tan A.\tan B$ and $S3=\prod_{cyc}\tan A$ Now since $A+B+C=\pi$ and since $\tan\pi=0$ , therefore $\frac{S1-S3}{1-S2}=0$ and hence $S1=S3$ but I got stuck when I realized that another condition, $1-S2\neq0$ should also be true. Now I can't prove this. I'm not having any idea how to start to prove $S2\neq1$ for a triangle. please give me a hint.",['trigonometry']
4157020,"$\text{ If }\sin{x} + \cos{x} = m\text{, prove:- }\sin^{6}{x} + \cos^{6}{x} = \frac{4 - 3(m^2 -1)^2}{4}\text{, where }m^2 \leq 2$","My question is not exactly what's in the title. The proof is pretty straight forward:- From $\sin{x} + \cos{x} = m$ , we get $3\sin^2{x}\cos^2{x} = \frac{3(m^2 - 1)^2}{4}$ . Consequently, $\sin^{6}{x} + \cos^{6}{x} = 1 - 3\sin^2{x}\cos^2{x} = 1 - \frac{3(m^2 -1)^2}{4}$ . My question is what's the purpose of $m^2 \leq 2$ in the question? I always get extremely scared whenever I see some form of inequality included in a question. It took me about 15 minutes to solve the above problem just because I got scared of the inequality. But as it turns out I did not need it at all. So Why include it in the question at all? Or does my solution have some critical flaw? Any insight will be very helpful. Additionally, please let me know of any tips for working with inequalities if possible. For example, how to find a possible path to just start because that's the biggest issue with me. I don't seem to be able to start working a problem if there's an inequality, I start panicking for some reason. Not sure if I'm making this last part much clear or not.","['trigonometry', 'inequality']"
4157074,Using the Residue theorem to solve contour integral $\oint_{|z|=R}\frac{z^2}{1-e^{2\pi i z^3}}dz$ with $n<R^3<n+1$ for positive integer $n$.,"I'm a second-year undergraduate in a non-major complex analysis course. We just started looking at using residues to compute the values of contour integrals, and I've run into a homework problem that's giving me some trouble. I spent a lot of time on this one and I think I got it, but my answer is off by a negative compared to the one in a student-created answer bank that on rare occasion does have incorrect answers, but is usually right. I'm looking for help confirming my thought process and making sure that I didn't make any mistakes here. The integral is $\oint_{|z|=R}\frac{z^2}{1-e^{2\pi i z^3}}dz$ with $n<R^3<n+1$ for positive integer $n$ . I started by identifying all singularities as those $z$ where $1-e^{2\pi i z^3}=0$ , and discovered that this is true exactly when $z^3=k$ for some integer $k$ . I then let $z=re^{i\theta}$ , which gives $z^3=k\Rightarrow r^3e^{3i\theta}=ke^{i(0)}\Rightarrow r=k^{1/3},\theta=\frac{2\pi l}{3}$ , where $l$ is an integer. Since $z$ is like a ""3rd root of $k$ "" (like a 3rd root of unity), there are 3 of them with unique positions in the complex plane, these are those $z$ with $l=0,1,2$ , or $\theta=0,\frac{2\pi}{3},\frac{4\pi}{3}$ , and all with $r=k^{1/3}$ . Then, for a particular integer $k$ , we have 3 distinct roots that solve $1-e^{1\pi i z^3}=0$ . However, the integers $k$ that we are concerned with are restricted by those that fall within the interior of the contour $|z|=R$ . These are those $k$ where $|k|\leq n$ , or $k=0,\pm 1,\pm 2,\dots , \pm n$ . This is because of the fact that if a number $z$ is to fall within the interior of $|z|=R$ , then its modulus $|z|$ should surely be less than $R$ , or $|z|<R$ , which implies that $|z|^3<R^3$ , and since we have $n<R^3<n+1$ , this implies that $|z|^3\leq n<R^3<n+1$ (All of this was difficult for me to visualize at first. I can't really think of a better way to explain this fact, and to be honest, I only feel like I've half convinced myself of it). Given that we have our general singularity $z=re^{i\theta}$ from above, and the constraint that $|z|^3\leq n$ for a point to be in the contour, we can now put a restriction on the values of $k$ that allow $z=re^{i\theta}=k^{1/3}e^{i\theta}$ to fall within the interior of the contour. We have $|z|=|k^{1/3}|\Rightarrow |z|^3=|k|\leq n$ , so we select $k=0,\pm 1,\pm 2,\dots , \pm n$ as previously stated. Now that we have a finite number of singularities that fall within the interior of the contour, we can count them and see that there are $2n$ values of $k$ (not counting $k=0$ ) and 3 values of $l$ , which makes for $6n$ distinct singularities. When $k=0$ , we only count 1 singularity, since the length of $z$ just becomes 0 and changing to different angles makes no difference in its distinct position on the complex plane. We let all of the $6n$ distinct non-zero singularities be defined as $z_{k,l}=k^{1/3}e^{i\frac{2\pi}{3}l}$ for ranges of $k$ and $l$ previously defined. We now compute the integral using the Residue theorem. Let $f(z)=\frac{z^2}{1-e^{2\pi i z^3}}$ . This is $\oint_{|z|=R}\frac{z^2}{1-e^{2\pi i z^3}}dz
=\oint_{|z|=R}f(z)dz=2\pi i \left[
\sum_{k=1}^{n}\sum_{l=0}^{2}(Res(f(z),z_{k,l}))
+\sum_{k=1}^{n}\sum_{l=0}^{2}(Res(f(z),z_{-k,l}))
+Res(f(z),0)\right]$ . Now to compute the residue for each non-zero singularity $z_{k,l}$ . I claim that each $z_{k,l}$ is a finite pole of order 1, or a simple pole. This can be shown by taking \begin{eqnarray}
\lim_{z\to z_{k,l}}(z-z_{k,l})f(z)\\
=\lim_{z\to z_{k,l}}(z-z_{k,l})\frac{z^2}{1-e^{2\pi i z^3}}\\
=\lim_{z\to z_{k,l}}\frac{z^3-z_{k,l}z^2}{1-e^{2\pi i z^3}}\\
=\lim_{z\to z_{k,l}}\frac{3z^2-2z_{k,l}z}{-e^{2\pi i z^3}(6\pi i z^2)}\\
=-\frac{1}{6\pi i}\lim_{z\to z_{k,l}}\frac{3-2z_{k,l}z^{-1}}{e^{2\pi i z^3}}\\
=-\frac{1}{6\pi i}\frac{3-2z_{k,l}z_{k,l}^{-1}}{e^{2\pi i z_{k,l}^3}}\\
=-\frac{1}{6\pi i}\frac{1}{e^{2\pi i z_{k,l}^3}}
\end{eqnarray} Since this value is certainly not zero, each non-zero $z_{k,l}$ is indeed a simple pole, and it just happens that the limit we just computed is the same limit that computes exactly the residue at the point $z_{k,l}$ , since it is a simple pole. Since $z_{k,l}^3=k$ as we have previously defined we then have $-\frac{1}{6\pi i}\frac{1}{e^{2\pi i k}}$ , where $e^{2\pi i k}=1$ since the coefficient $k$ is an integer. This leaves us with a residue of $-\frac{1}{6\pi i}$ for all non-zero $z_{k,l}$ . Now we look at the residue at $z=0$ . We similarly use a limit to simultaneously show that $f(z)$ has a simple pole at $z=0$ and compute the residue at that pole. \begin{eqnarray}
\lim_{z\to 0}zf(z)\\
=\lim_{z\to 0}z\frac{z^2}{1-e^{2\pi i z^3}}\\
=\lim_{z\to 0}\frac{z^3}{1-e^{2\pi i z^3}}\\
=\lim_{z\to 0}\frac{3z^2}{-e^{2\pi i z^3}(6\pi i z^2)}\\
=-\frac{1}{2\pi i}\lim_{z\to 0}\frac{1}{e^{2\pi i z^3}}\\
=-\frac{1}{2\pi i}
\end{eqnarray} This gives the residue $Res(f(z),0)=-\frac{1}{2\pi i}$ . With the residues $Res(f(z),z_{k,l})=-\frac{1}{6\pi i}$ for all valid $k,l$ from before, this gives $$
\oint_{|z|=R}\frac{z^2}{1-e^{2\pi i z^3}}dz
=\oint_{|z|=R}f(z)dz
=2\pi i \left[
\sum_{k=1}^{n}\sum_{l=0}^{2}(Res(f(z),z_{k,l}))
+\sum_{k=1}^{n}\sum_{l=0}^{2}(Res(f(z),z_{-k,l}))
+Res(f(z),0)\right]
=
2\pi i \left[
\sum_{k=1}^{n}\sum_{l=0}^{2}(-\frac{1}{6\pi i})
+\sum_{k=1}^{n}\sum_{l=0}^{2}(-\frac{1}{6\pi i})
-\frac{1}{2\pi i}\right]
=
2\pi i \left[
3n(-\frac{1}{6\pi i})
+3n(-\frac{1}{6\pi i})
-\frac{1}{2\pi i}\right]
=
2\pi i \left[
6n(-\frac{1}{6\pi i})
-\frac{1}{2\pi i}\right]
=
6n(-\frac{1}{3})
-1
=
-2n-1
$$ So my end answer is $-2n-1$ , but the answer bank says $2n+1$ , without much additional explanation. Is there any place I messed up? Thanks.","['complex-analysis', 'contour-integration', 'solution-verification', 'residue-calculus']"
4157083,"Find the equations of circles passing through $(1, -1)$ touching the lines $4x+3y+5=0$ and $3x-4y-10=0$","Find the equations of circles passing through $(1,-1)$ touching the lines $4x+3y+5=0$ and $3x-4y-10=0$ The point of intersection of the lines is $(\frac25,-\frac{11}5)$ If we want this point of intersection to be $(0,0)$ (because the given lines are perpendicular) then define the new $X=x-\frac25,Y=y+\frac{11}5$ , where $x,y$ are the original coordinates. So, the point $(1,-1)$ in the new system becomes $(\frac35,\frac65)$ . Since it's in the first quadrant, so, the center of the circle will be $(r,r)$ where $r$ is the radius of the circle. So, $(\frac35-r)^2+(\frac65-r)^2=r^2\implies r=\frac35,3$ So, the equations of circles are $$(X-\frac35)^2+(Y-\frac35)^2=\frac9{25}\implies(x-1)^2+(y+\frac85)^2=\frac9{25}$$ and $$(X-3)^2+(Y-3)^2=9\implies(x-\frac{17}5)^2+(y-\frac45)^2=9$$ Is this correct?","['circles', 'geometry', 'transformation']"
4157126,"Show that there exists a unique bounded linear operator T such that $\langle T_n x,y\rangle \to \langle Tx,y\rangle$","Let H be a Hilbert space. Assuming $\{Tn\}_n$ is a sequence of bounded linear operators from
H to H such that for all x, y $\in$ H we have $\langle T_n x, y\rangle$ converges as $n \to \infty$ .Show that there exists a unique bounded linear operator T such that $\langle T_n x,y\rangle \to \langle Tx,y\rangle$ . I tried to use a result of extension which states that we can extend in a unique way a bounded linear operator over a dense subset to an operator over the whole space. But I don't see how to use it and if it's a good idea.","['hilbert-spaces', 'functional-analysis']"
4157127,"Is there an explicit example of an uncountable collection of pairwise disjoint, dense, countable subsets whose union equals $[0,1]$?","Question: Is there an explicit example of an uncountable collection of pairwise
disjoint, dense (in $[0,1]$ ), countable subsets (i.e. each subset has countable many elements) of $[0,1]$ whose union equals $[0,1]$ ? I know that if we change the question to, ""Is there an explicit example of an uncountable collection of pairwise disjoint, dense, uncountable subsets of $[0,1]$ ? "", then there is a good answer here . But I can't seem to answer the question if countable subsets are required. The obvious attempts are something like $$\huge{\cup}_{\normalsize{\text{p prime}}} \ \normalsize{ \left(\sqrt{p}+\mathbb{Q}\right)}$$ but I think this is the wrong route to go down, because we're not going to get an uncountable number of disjoint sets whose union is $\ [0,1]$ . Alternatively if we start with, $$\huge{\cup}_{\normalsize{ i\text{ is Irrational}}} \ \normalsize{ \left(i+\mathbb{Q}\right)}$$ then I don't see an easy way to deal with repeat sets, other than invoking the axiom of choice. But I don't want to invoke AOC, I want an explicit example. And repeat sets are a problem because I require them to be disjoint. So I don't see a way round this, although maybe there is one? But probably a different approach altogether is necessary. Something tells me the answer is easy and I have forgotten the solution, but I can't think of what it is. Also, I require an explicit example: invoking the axiom of choice is not an acceptable answer imo. And I'm fairly sure the answer is ""yes"", although maybe the answer is no because of something like the Baire Category Theorem, which I don't really understand? It's also possible this question is not easy (for me) and is out of my depth... I don't know.","['general-topology', 'rational-numbers', 'irrational-numbers', 'real-analysis']"
4157159,What values of $n$ is $A=\frac{a_1}{a_2}+\frac{a_2}{a_3}+\cdots +\frac{a_{n-1}}{a_n}+\frac{a_n}{a_1}$ an integer?,"Suppose $a_i\in \mathbb{N} $ such that $a_i\ne a_j$ and $\displaystyle\frac{a_i}{a_{i+1}}\ne \frac{a_j}{a_{j+1}}~$ for $i \ne j$ $($ take $a_{n+1}=a_1)$ and $\displaystyle A=\frac{a_1}{a_2}+\frac{a_2}{a_3}+\cdots +\frac{a_{n-1}}{a_n}+\frac{a_n}{a_1}$ . Then for what values of $n\ge  2$ can $A$ be an integer? I  found that if $a_i=(n-1)^i$ then $A$ is a integer but I realised that this  violates the condition $\displaystyle\frac{a_i}{a_{i+1}}\ne \frac{a_j}{a_{j+1}}~$ . It was easy to prove that $A$ is not an integer  for $n=2$ , but I was stuck for the cases where for $n>2$ . I then I plugged in some values for $a_i$ but for $n>2$ but I found no solution. Intuitively I think there is no solution but I am unable to prove it. Any help would be appreciated. Edit-1: As pointed out by @WillJagy it is possible for $n=3,4,5$ . So can we actually prove it is true for $n>6$ or are there some $n>6$ for which it isn't true? Edit-2: I was able to find a solution for $n=5$ from a solution for $n=4$ . $$~~~~~~~~~~\frac{6}{4} + \frac{4}{3} + \frac{3}{1} + \frac{1}{6}  = 6$$ $$\implies \frac{2}{4} + \frac{4}{3} + \frac{3}{1} + \frac{1}{6}  = 5$$ $$\implies \frac{6}{2} +\frac{2}{4} + \frac{4}{3} + \frac{3}{1} + \frac{1}{6}  = 8$$ But I could not find a solution for $n=6$ from a solution for $n=5$ because when I rewrite $\frac{4}{3} =1+\frac{1}{3}$ , the $1$ in the numerator of $\frac{1}{3}$ is used by another $a_i$ . I don't know whether we can prove by induction. Any suggestions is welcome.","['number-theory', 'divisibility', 'elementary-number-theory']"
4157232,Evaluating $\lim_{n\to\infty}\left(1 + \frac{\sin n}{5n + 1}\right)^{2n + 3}$ (a $1^\infty$ indeterminate form),"My professor gave us this limit as part of homework: $$
\lim_{n\to\infty}\left(1 + \frac{\sin n}{5n + 1}\right)^{2n + 3}
$$ I can see it's in the indeterminate form $1^{\infty}$ , so my first thought was to write it in the form $\lim_{n\to0}\left(1 + n\right)^{\frac{1}{n}}$ since I know that's equal to $e$ , so I tried that: $$
\begin{align*}
\lim_{n\to\infty}\left(1 + \frac{\sin n}{5n + 1}\right)^{2n + 3} &= \\
\lim_{n\to\infty}\left(1 + \frac{\sin n}{5n + 1}\right)^{\left(2n+3\right)\cdot\left(\frac{1}{\frac{\sin n}{5n + 1}}\right)\cdot\left(\frac{\sin n}{5n + 1}\right)} &= \\
\lim_{n\to\infty}\left(\left(1 + \frac{\sin n}{5n + 1}\right)^{\frac{1}{\frac{\sin n}{5n + 1}}}\right)^{\frac{2n + 3}{5n + 1}\cdot\sin n} &=
\end{align*}
$$ Up to this point I was happy, 'cause I successfully wrote it in that form, but I got stuck when I tried to solve the limit of the outermost exponent, since it doesn't have a limit.
I apologize in advance for any grammatical or sentence structure errors, English is not my first language. Anyways, any feedback about this problem, like hints, where my error was, or literally anything, is extremely accepted!","['limits-without-lhopital', 'analysis', 'indeterminate-forms', 'limits', 'trigonometry']"
4157267,Question on a proof involving linear algebra,"I've encountered the following proof, but I'm stuck at the last step. Let $X \subset \mathbb{R}^s$ be a vector space. Let $p(x): X \to \mathbb{R}$ be a linear transformation such that $p(x)$ must be positive if every element of $x \in X$ is positive. Create $$M = \{ (-p(x), x); x \in X \}$$ Then $M$ is a vector space given the linearity of $p(x)$ . In fact, $M$ cannot consist entirely of positive elements, so $M$ is a hyperplane that only intersects the positive orthant of $\mathbb{R}_+^{s+1}$ at the point $0$ . I understand everything above, but at the end, it says: We can then create a linear function $F : \mathbb{R}^{s+1} \to \mathbb{R}$ such that $F(-p,x) = 0$ for $(-p,x) \in M$ , and $F(-p,x) > 0$ for $(-p,x) \in \mathbb{R}_+^{s+1}$ except the origin. I don't understand how this follows. How can you be sure that you can create such a function? Is there a theorem that I'm missing?","['linear-algebra', 'functional-analysis']"
4157295,Can we equivalently write $x\in y$ as $x\subseteq \cup y$?,I was thinking about if these two are equivalent? I am sure that this is very obvious and I am just not very sure whether or not is this true? Here is a few things I thought of: $x\in y\Longrightarrow\{x\}\subseteq y\Longrightarrow \cup \{x\}\subseteq\cup y\Longrightarrow x\subseteq \cup y.$ I think these arguments also go the other way if we note $\cup a\subseteq \cup b$ then $a\subseteq b$ ? Any suggestion is very much welcomed!,['elementary-set-theory']
4157325,"What functions have the property $f(AB)=f(A)f(B)$ where $A,B$ are matrices?","There is the classic example $\det(AB)=\det(A)\det(B)$ . I am looking for other examples of this property, it is best if f maps matrices to scalars but any example would be great. Note: No need for trivial $f(A)=cA$ type functions; I am looking for something more exotic.","['matrices', 'functions', 'linear-algebra']"
4157329,"If $X$ is Gaussian, prove that $X-\lfloor X \rfloor \sim U(0,1)$ as its variance becomes large","I have a normal distributed random variable, $X$ with mean $\mu$ and standard deviation, $\sigma$ . I don't believe it matters, but this distribution was obtained as a result of summing a large number of independent, identically distributed random numbers with finite variance (hence invoking the central limit theorem). It seems intuitive that $X - \lfloor X \rfloor$ should become closer and closer to a uniform random number between $(0,1)$ as the variance of $X$ increases. And in the limit, it should become a uniform random number. Is there a proof for this claim or a refutation of it? Context: this is going to help ""complete"" the accepted answer here: As the variance of a random variable grows, the conditional distribution of it residing in an interval of length $1$ becomes uniform . Larger picture, I'm trying to prove Blackwell's theorem from renewal theory. See here for details: Going ""well into the lifetime"" of a renewal process means the time until the next event will be uniform conditional on inter-arrival?","['central-limit-theorem', 'uniform-distribution', 'probability']"
4157341,Express $-X$ and $X-Y$ with characteristic functions,"Let $X$ and $Y$ be independent identical distributions with characteristic function $\varphi$ . Express the characteristic functions of $-X$ and $X-Y$ with $\varphi$ My attempt: I know that $\varphi_X(t)=\mathbb{E}(e^{itX})$ and that $\mathbb{E}(X_1X_2)=\mathbb{E}(X_1)\mathbb{E}(X_2)$ if $X_1, X_2$ are independent. Knowing that, I can compute $$\varphi_{X-Y}(t) = \mathbb{E}(e^{it(X-Y)})=\mathbb{E}(e^{itX}e^{-itY})\overset{?}{=}\mathbb{E}(e^{itX})\mathbb{E}(e^{-itY})=\varphi_X(t)\varphi_{-Y}(t)$$ I am not 100% sure about the third step (marked with a ""?""). I know that measurable functions of independent variables are also independent and that $exp$ is borel-measurable, but maybe the $i$ makes a difference? Moving on to $\varphi_{-X}$ $$\varphi_{-X}(t)=\mathbb{E}(e^{it(-X)})=\mathbb{E}(e^{i(-t)X})=\varphi_{X}(-t)$$ Same thing with $\varphi_{-Y}$ , so I can simplify $\varphi_{X-Y}$ even further so that $\varphi_{X-Y}(t)=\varphi_X(t)\varphi_{Y}(-t)$ Now I've expressed both characteristic functions with $\varphi_X$ and $\varphi_Y$ Did I do it correctly?","['characteristic-functions', 'probability-theory']"
4157348,Divergence of complex series,"Let be $\sum\limits_{k=1}^{n} z_k $ a complex series which converges (only) conditionally. Show that there exists a bijection $\varphi:\mathbb{N}\to \mathbb{N}$ such that $\lim\limits_{n\to\infty}\left|\sum\limits_{k=1}^{n} z_{\varphi(k)}\right| =\infty$ . My ( edited ) approach: For each $k\in\mathbb{N}$ let be $z_k=a_k i + b_k$ . If $\lim\limits_{n\to\infty}\left|\sum\limits_{k=1}^{n} b_k\right|=\infty$ or $\lim\limits_{n\to\infty}\left|\sum\limits_{k=1}^{n} a_k\right|=\infty$ then $$
\left|\sum\limits_{k=1}^{n} a_k i + b_k\right|=\sqrt{\left(\sum\limits_{k=1}^{n} a_k\right)^2+\left(\sum\limits_{k=1}^{n} b_k\right)^2}\geq  \left|\sum\limits_{k=1}^{n} a_k\right|\left|\sum\limits_{k=1}^{n} b_k \right|$$ which would imply that $\sum\limits_{k=1}^{n} z_k$ doesn't converge (Cauchy-Schwarz inequality). If $\sum\limits_{k=1}^{n} a_k$ oscillates and $\sum\limits_{k=1}^{n} b_k $ converges and then I will always find a small enough $\epsilon >0$ such that for each index $n_0$ I find a $n$ with $n>n_0$ and $\sum\limits_{k=n_0+1}^{n} a_k>\epsilon$ . Hence, $$
\left|\sum\limits_{k=n_0+1}^{n} a_k i + b_k\right|=\sqrt{\left(\sum\limits_{k=n_0+1}^{n} a_k\right)^2+\left(\sum\limits_{k=n_0+1}^{n} b_k\right)^2}\geq  \left|\sum\limits_{k=n_0+1}^{n} a_k\right|>\epsilon,
$$ which again would imply that $\sum\limits_{k=1}^{n} z_k$ doesn't converge. In both cases we get a contradiction, hence both series must converge. Further, if both limits $\sum\limits_{k=1}^{\infty} |b_k|$ and $\sum\limits_{k=1}^{\infty} |a_k|$ exist at the same time then  it follows: $$
\sum\limits_{k=1}^{n} \left|a_k i + b_k\right|\leq\sum\limits_{k=1}^{n} |a_k i |+\sum\limits_{k=1}^{n} |b_k |= \sum\limits_{k=1}^{n} |a_k|+\sum\limits_{k=1}^{n} |b_k|.
$$ This would mean that $\sum\limits_{k=1}^{n} z_k $ converges unconditionally which again is a contradiction. So at least one series must diverge (towards $\infty$ ). WLOG let be $\sum\limits_{k=1}^{\infty} |b_k|$ the divergent (towards $\infty$ ) series. We define two sets: $I_-$ which contains all the indices of terms $b_k<0$ and $I_+$ which contains all indices of the terms $b_k\geq 0$ . Now  we consider two real series which are made by indices of $I_+$ and indices of $I_-$ respectively, $\sum\limits_{k=1}^{n} b_k^+$ and $\sum\limits_{k=1}^{n} b_k^-$ . If only one of the series diverges then $\sum\limits_{k=1}^{n} b_k$ diverges which is a contradiction. If both converges then it follows that $\sum\limits_{k=1}^{n} |b_k|$ converges which is also a contradiction. Hence, both series $\sum\limits_{k=1}^{n} b_k^+$ and $\sum\limits_{k=1}^{n} b_k^-$ must diverge (towards $\infty$ ). Let be $\epsilon >0$ arbitrarily chosen. We define a bijection $\varphi:\mathbb{N}\to\mathbb{N}= I_+\cup I_-$ (note that $I_+$ and $I_-$ are clearly a disjoint decomposition of $\mathbb{N}$ ): $$\begin{align*}
&\varphi(1)=\min\{k\in I_-\}\\
&\varphi(2)=\min\{k\in I_+\}\\
&\varphi(3)=\min\{k\in I_+\setminus \{\varphi(2)\}\}\\
&\vdots\\
&\varphi(n_0)=\min\{k\in I_+\setminus \{\varphi(2), \varphi(3), \cdots , \varphi(n_0-1)\}\},\\
\end{align*}
$$ where we increment $n_0$ until reach $$
\left|\sum\limits_{k=1}^{n_0}z_{\varphi(k)}\right|= \left|\sum\limits_{k=1}^{n_0} a_{\varphi(k)} i + b_{\varphi(k)}\right|=\sqrt{\left(\sum\limits_{k=1}^{n_0} a_{\varphi(k)}\right)^2+\left(\sum\limits_{k=1}^{n_0} b_{\varphi(k)}\right)^2}\geq  \left|\sum\limits_{k=1}^{n_0} b_{\varphi(k)} \right|>\epsilon.
$$ Then we start the procedure again: $$\begin{align*}
&\varphi(n_0+1)=\min\{k\in I_-\setminus \{\varphi(1)\}\}\\
&\varphi(n_0+2)=\min\{k\in I_+\setminus \{\varphi(2), \varphi(3), \cdots , \varphi(n_0)\}\}\\
&\varphi(n_0+3)=\min\{k\in I_+\setminus \{\varphi(2), \varphi(3), \cdots , \varphi(n_0)\}, \varphi(n_0+2)\}\\
&\vdots\\
&\varphi(n_1)=\min\{k\in I_+\setminus \{\varphi(2), \varphi(3), \cdots , \varphi(n_0),\varphi(n_0+2),\cdots , \varphi(n_1-1)\}\},\\
\end{align*}
$$ until we reach $$
\begin{split}
\left|\sum\limits_{k=n_0+1}^{n_1}z_{\varphi(k)}\right| &= \left|\sum\limits_{k=n_0+1}^{n_1} a_{\varphi(k)} i + b_{\varphi(k)}\right|\\
& =\sqrt{\left(\sum\limits_{k=n_0+1}^{n_1} a_{\varphi(k)}\right)^2+\left(\sum\limits_{k=n_0+1}^{n_1} b_{\varphi(k)}\right)^2}\geq  \left|\sum\limits_{k=n_0+1}^{n_1} b_{\varphi(k)} \right|>\epsilon.
\end{split}
$$ The function $\varphi$ is injective because every element of each both sets $I_+$ and $I_-$ is chosen once. If there existed an element $i_-\in I_-$ which would have no preimage under $\varphi$ then by construction of $\varphi$ it must be possible to add infinitely many $i_+\in I_+$ such that $\left|\sum\limits_{k=1}^{n} b_{\varphi(k)} \right|$ will always remain below $\epsilon$ . However, this means that $\sum\limits_{k=1}^{n} b_k^+$ converges which is a contradiction. (The same argument can be used if we assume that there existed an $i_+\in I_+$ which has no preimage). To summarize, we have found a bijection $\varphi$ which rearranges the series $\sum\limits_{k=1}^{n} z_{\varphi(k)}$ such that we can push $\left|\sum\limits_{k=1}^{n} z_{\varphi(k)}\right|$ above any value $M$ . Is this correct so far?","['convergence-divergence', 'solution-verification', 'sequences-and-series', 'real-analysis']"
4157381,Determining the type of a differential form on an elliptic curve,"As we know, there are three types of differential forms on an elliptic curve: those of first kind  -- which have no poles or zeroes, those of the second kind -- which have poles with residue 0, and third kind -- which have simple poles with non-zero residue. Now, if the elliptic curve is given in Weierstrass form I know that $\frac{dx}{y}$ is a differential of the first kind, and that $\frac{xdx}{y}$ is a differential of the second kind, and one could similarly write a differential of the third kind. However, I was wondering if there is a way of determining the type of a meromorphic differential form, say $\frac{u(x_1,x_2)dx_1}{f(x_1,x_2)}$ , where $f$ is a cubic polynomial in the variables $x_1,x_2$ , and $u(x_1,x_2)$ is any polynomial, without changing variables to get $f$ into Weirestrass form?","['complex-geometry', 'elliptic-curves', 'algebraic-geometry', 'differential-forms']"
4157390,Normal derivative of a partial derivative,"I am reading lecture some lecture notes where the professor defines a function as such: $$\frac{d}{dc}\bigg[\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}\bigg]_{c=c^*} < 0$$ This I can make sense of as: the slope of the function $\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}$ being less than 0 at $c=c^*$ The professor goes on to say that by ""carefully differentiating"" we can rewrite this as: $$ \bigg[\frac{\partial^2W(c',c)}{\partial c'^2} + \frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg]\Bigg|_{c'=c=c^*} < 0 $$ This I do not understand. My initial thought would be that by taking the derivative of $\frac{\partial W(c',c)}{\partial c'}\bigg|_{c'=c}$ with respect to $c$ only should simply yield: $\frac{\partial^2 W(c',c)}{\partial c' \partial c}\bigg|_{c'=c=c^*}$ . Where does this extra term of $\frac{\partial^2W(c',c)}{\partial c'^2}$ come from? Can someone point out where I have misunderstood?","['partial-derivative', 'multivariable-calculus', 'calculus']"
4157436,Confusion regarding the axiom schema of replacement,"I've seen in various sources such as this one describe the axiom schema of replacement involving a function: If $F$ is a function, then for any $X$ there exists a set $Y=F[X]=\{ F(x):x \in X \}$ . My questions are really elementary, so please bare with me. What exactly is the domain and codomain of $F$ ? Shouldn't that be specified? In the axiom schema, we talk about any set $X$ . Does this mean the domain of $F$ is the entire universe of ZFC? How can any function possibly be defined on every possible element in ZFC? Is there an example that can help me grasp this?","['functions', 'set-theory']"
4157466,Surjective operator to separable space,"Let $X,Y$ be Banach spaces and $T:X\to Y$ be a surjective bounded linear operator. If $Y$ is separable then there exists a separable subspace $Z$ of $X$ such that $T(Z)=Y$ . I tried the following: $Y = \overline{\{y_1, \dots, y_n, \dots\}}$ . Since $T$ is surjective $y_i = Tx_i$ for some $x_i \in X$ . Let $Z = \overline{\text{span}(x_1,\dots,x_n,\dots)}$ . I know that $Z$ is separable so if I could prove that $T(Z)=Y$ the proof would be complete. However I could only prove that $\overline{T(Z)}=Y$ . Any help would be appreciated. Thanks.","['operator-theory', 'functional-analysis', 'real-analysis']"
4157473,Why is the graph of $sec^2(x)/tan^2(x)$ continuous at $x=\pi /2$?,"$f(x)=\frac{sec^2(x)}{tan^2(x)}$ Domain of $sec^2(x)$ and $tan^2(x)$ is $\mathbb{R}-(2n+1)\frac{\pi}{2}$ , for $n \in \mathbb{Z}$ , hence $f(x)$ also has the same domain. I expected a discontinuity at at $x=\frac{\pi}{2}$ . But, the graph does not say so. Graph from Wolfram Alpha: What am I missing in my concept? Also, even if the software did the manipulation $\frac{sec^2(x)}{tan^2(x)}$ = $\frac{1}{sin^2(x)}$ it should have been valid only for $x \in \mathbb{R} -(2n+1)\frac{\pi}{2}$ .","['trigonometry', 'functions', 'functional-analysis', 'graphing-functions']"
4157490,Finding Taylor's series of the function: $\frac{e^{a \sin^{-1}x}}{\sqrt{1-x^2}}$,"Show that $$\frac{e^{a \sin^{-1}x}}{\sqrt{1-x^2}}=1+\frac{ax}{1!}+\frac{(a^2+1^2)x^2}{2!}+\frac{a(a^2+2^2)x^3}{3!}+\frac{(a^2+1^2)(a^2+3^2)x^4}{4!}+\cdots$$ My attempt:
I integrated the function and got $\frac{e^{a \sin^{-1}x}}{a}$ then I wrote the series of $e^{a \sin^{-1}x}$ but it contained terms like $(\sin^{-1}x)^2$ , $(\sin^{-1}x)^3$ and so on so I could not find the series. My idea was to find the series of the anti derivative of the function and then to derivate the obtained series. Any other way to do it?","['power-series', 'sequences-and-series', 'taylor-expansion', 'real-analysis']"
4157504,Gaussian integral with a sine in the exponential,"This Gaussian integral came up while working on a likelihood analysis for pulsar timing arrays: $$
\int_{-\pi}^{\pi} \exp\left( -(x-y \sin{\gamma} )^2 \right) \mathrm{d}\gamma
$$ I've tried everything I can think of, but I can't get an analytic solution. If you expand the square, the cross term can be written in terms of a cosh, but I don't know where to go from there. That gives the integrand as $\exp{(-x^2)} \exp{(-y^2 \sin^2\gamma)} \cosh{(2 x y \sin{\gamma})}$ . The substitution $\beta\equiv\sin{\gamma}$ leads to $$
\oint_0^0 \frac{\exp\left( -(x-y \beta )^2 \right)}{\sqrt{1-\beta^2}} \mathrm{d}\beta
$$ I also tried $\arctan \alpha \equiv \gamma$ , which gives $$
\int \frac{\exp\left[-\left( x- \frac{\alpha y}{\sqrt{1+\alpha^2}} \right)^2\right]}{1+\alpha^2} \mathrm{d}\alpha
$$ Neither Mathematica nor Rubi are able to evaluate any of these integrals. I was hoping the residue theorem might be applicable, or one of these substitutions might get into a form that Mathematica knows. Any help will be greatly appreciated and certainly land you in the acknowledgements of the paper my collaborators and I are working on:) Any approximate results (saddle point approximation?) would also be appreciated.","['integration', 'definite-integrals', 'contour-integration', 'gaussian-integral', 'residue-calculus']"
4157569,In what capacity is $S^{3}$ locally the same as $S^{2}\times S^{1}$?,"Clearly they're not the same topologically as the former is simply connected and the latter not so. One can consider the three-sphere as diffeomorphic to $SU(2)$ $$S^{3}\simeq SU(2)$$ Now, we also know that there is a relationship for the two-sphere: $$S^{2}\simeq\frac{SU(2)}{U(1)}$$ And of course there is the isomorphism: $$S^{1}=U(1)$$ So we can write: $$S^{2}\times S^{1}\simeq\frac{SU(2)}{U(1)}\times U(1)$$ I'm guessing I can't just cancel out these factors to say that $S^{2}\times S^{1}\simeq SU(2)$ , (As these $U(1)$ groups may be different ones in fact) but I'm thinking that maybe locally they're somehow equivalent, perhaps a lie bracket of vector fields on both adhere to the $su(2)$ lie algebra??? Since both manifolds are parallelizable this appears feasible. Could someone please enlighten me?","['general-topology', 'riemannian-geometry', 'lie-groups', 'differential-geometry']"
4157619,"Trace of product of commutators in $\operatorname{SL}(2,\mathbb{C})$","Let $A,B\in\operatorname{SL}(2,\mathbb{C})$ and define $x=\operatorname{tr}(A)$ , $y=\operatorname{tr}(B)$ , $z=\operatorname{tr}(AB)$ . It is know that these satisfy the equation $$\operatorname{tr}([A,B])=x^2+y^2+z^2-xyz-2,$$ where the commutator $[A,B]=ABA^{-1}B^{-1}$ is defined in the group setting. For $A,B,C,D\in\operatorname{SL}(2,\mathbb{C})$ can one obtain a similar formula for $\operatorname{tr}([A,B][C,D])$ in terms of $x=\operatorname{tr}(A)$ , $y=\operatorname{tr}(B)$ , $z=\operatorname{tr}(AB)$ , $r=\operatorname{tr}(C)$ , $s=\operatorname{tr}(D)$ , $t=\operatorname{tr}(CD)$ ? Edit:
I think such an equation will also need to include the variables $u=\operatorname{tr}(AC)$ , $v=\operatorname{tr}(BD)$ , $w=\operatorname{tr}(AD)$ , $\omega=\operatorname{tr}(BC)$ (so that its level sets are $8$ dimensional surfaces).","['matrices', 'trace', 'matrix-equations', 'linear-algebra']"
4157637,How to multiply certain analytic matrices so they behave like the identity near $0$?,"I have the following parametrized matrix: $$
A(u,t) = \begin{bmatrix} \cos(t(u+s)) & \frac{\sin(t(u+s))}{u+s} \\ -(u+s)\sin(t(u+s)) & \cos(t(u+s))\end{bmatrix} \quad u \in \Bbb R\setminus \{0\} \text{ and } t > 0
$$ which is clearly entire in $s$ . For any given $N$ , my goal is to find, or at least prove the existence of, a sequence $\{(u_i,t_i)\}_{i=1}^M$ s.t. $$\prod_{i=1}^M A(u_i,t_i) =I+O(s^N)$$ I have been playing around with this problem and I have found numerical solutions up to $N=8$ . My hope is that there is some pattern I can recursively exploit to generate new solutions from old ones. It is easy to get rid of odd powers of $s$ because of the parity of their coefficients, but the even powers are more troublesome. By Taylor expanding, I can tell that this problem would be equivalent to solving a system of equations involving powers and trig functions, but I do not know enough about such systems to find my required sequence of $(u_i,t_i)$ . I looked online and it seems there is a link to the Matrix Membership problem, which is in general undecidable, but perhaps in this case, with such an explicit form for $A$ , something more can be said. Are there any references that I might find useful? Edit For example, for $N=3$ , note that, to second order, $$A(-1,t_1)A(1,4\pi)A(-1,4\pi-t_1) = I + \begin{bmatrix}-8\pi\sin\left(2t_1\right)&-8\pi\cos\left(2t_1\right)\\-8\pi\cos\left(2t_1\right)&8\pi\sin\left(2t_1\right) \end{bmatrix}s^2=I+B(t_1)s^2$$ and $B(t_1+\pi/2)=-B(t_1)$ , so by choosing $t_1=\pi$ , we have $$A(-1,\pi)A(1,4\pi)A(-1,3\pi)A(-1,3\pi/2)A(1,4\pi)A(-1,5\pi/2)=I+O(s^3)$$ I believe that in general, using $u_i$ 's with fixed absolute value should be possible, but I cannot prove it. By choosing $|u_i|=1$ and looking at the zeroth order term of the general product of $A(u_i,t_i)$ 's, it is easy to see that $\sum_i t_i \in 2\pi \Bbb N$ . My guess is that similar constraints apply to higher order terms, but getting explicit expressions for those is harder.","['matrices', 'trigonometry', 'matrix-equations']"
4157735,Using Bayesian inference to optimize my food stand - Is this feasible?,"During the last days I was learning about bayesian inference, and am now wondering if this could be of any practical use for a food stand I own. Here is the problem: I own a food stand and sell sausages before football matches. Before every match, I have to decide how many buns and sausages I have to buy / how many sells I expect. This depends mostly on the condition, of how many tickets have been sold for the match. This condition can change quite dynamically from match to match. So although I can make quite an educated guess out of the years of experience, I still end up throwing away many buns and sausages after each match. So the problem I am thinking about can be summarized as: Given the number of sells I've made in the past for number of tickets T, how many buns and sausages shall I buy for todays match, when the sold tickets are T_today. Even if the following approach is of no practical use, I am still curious in terms of it being an academic exercise. So my idea is the following: Describe the guessed number of sells for the next match using a normal distribution, with mean and variance being an educated guess. Measure the number of tickets that have been sold, and the number of sells I have made. Before the next matchday, look at the numbers of tickets that have been sold and update the normal distribution, given this number of tickets and the experience of the last match. Now, its quite straightforward how you update a normal distribution using bayesian inference. You measure a dataset, compute its variance and mean and you are then able to compute the updated normal distributions mean, variance etc. However, in this case, the measurement I do is the number of sells I've made and tickets sold. This I can measure with 100% accuracy, so there is no variance. How can I perform bayesian inference in this case?","['statistics', 'bayesian', 'normal-distribution', 'gaussian', 'bayes-theorem']"
4157795,Is 'auxiliary equation' or 'characteristic equation' more common?,"In my school textbook, when solving second order homogeneous differential equations it talks about using the equation's ' auxiliary equation '. However, I've seen in many places, such as Wikipedia, that the term ' characteristic equation ' is used instead. Which one is more standard? Are they both alright? If I write an article is it ok to talk about either of them? Thanks for your help.","['calculus', 'ordinary-differential-equations', 'terminology']"
4157800,Infinite number of subsequences converging to the same limit,"Let $(u_n)_n$ be a real sequence. We suppose there exists a strictly increasing function $\phi:\mathbb{N}\to\mathbb{N}$ such that for all $k\in\mathbb{N}$ , $(u_{\phi(n)+k})_n$ converges to the same limit $l$ . Does this necessarily mean that $(u_n)_n$ converges to $l$ ? The issue with this problem is that the number of subsequences is infinite, which creates a problem when trying to do the proof using the $\epsilon$ -definition, so I wonder if this result is actually false which would surprise because it seems so intuitively true.","['convergence-divergence', 'sequences-and-series']"
4157859,Understanding the proof of the Full SVD from the Economy SVD,"$\textbf{The Economy SVD}$ : Let $r:=rank(A)\leq \min(m,n)$ where $A\in\mathbb{R^{m\times n}}$ . We know that the number of singular values of $A$ or the cardinality of the set $\Sigma(A):=\{\sigma_{i}|i=1,2,\cdots,m\}$ is $r$ . Considering $AA^{T}$ , we know that it is symmetric and thus by the Jordan-Schur's decomposition, we may write: $$
AA^{T}=QDQ^{T}
$$ Here $D$ which is a diagonal matrix, houses the eigenvalues of $AA^{T}$ which are $r$ in total since $rank(A)=rank(AA^{T})$ . Now this means that if we consider $D_{r}$ which is an $r\times r$ diagonal matrix that removes the $0$ 's from the diagonal entries and only contains that $r$ eigenvalues $\Sigma_{r}^{2}$ , as well as $Q_{r}$ excluding the zero orthonormal vectors we can write : $$
AA^{T}Q_{r}\Sigma_{r}^{-1}=Q_{r}\Sigma_{r}
$$ Now let $U_{r}:=Q_{r}$ , $S_{r}:=\Sigma_{r}$ , and $V_{r}:=A^{T}Q_{r}\Sigma_{r}^{-1}=A^{T}U_{r}S_{r}^{-1}$ , we may write: $$
AV_{r}=U_{r}S_{r}
$$ where $U_{r}^{T}U_{r}=I_{r}$ and it can also be shown that $V_{r}^{T}V_{r}=I_{r}$ which lets us to rewrite the equation above as: $$
A=U_{r}S_{r}V_{r}^{T}
$$ Request : I hope someone can explain the beginning of the proof as I didn't understand it. The proof below essentially derives the full scale SVD from the economy SVD $\textbf{Theorem (Full Scale SVD)}$ : Consider the matrix $A\in\mathbb{R}^{m\times n}$ with $\operatorname{rank}(A)=r\leq p=\min(m,n)$ . We claim that there exist two orthogonal matrices $U\in\mathbb{R}^{m\times m}$ and $V\in\mathbb{R}^{n\times n}$ and a diagonal matrix $S\in\mathbb{R}^{m\times n}$ , such that : $$
AV=US\iff A=USV^{T}\iff S=U^{T}AV
$$ where $S$ has the singular values of $A$ on its diagonal $\operatorname{diag}(S):=\begin{bmatrix}\sigma_{1}&\sigma_{2}&\cdots&\sigma_{p}\end{bmatrix}$ . Furthermore, $\sigma_{1},\sigma_{2},\cdots,\sigma_{r}>0$ and the remaining $p-r$ singular values $\sigma_{r+1}=\sigma_{r+2}=\cdots=\sigma_{p}=0$ . The columns of $U$ and the columns of $V$ are called the left-singular vectors and right-singular vectors of $A$ , respectively . $\textbf{Proof}$ . Construct $V:=[V_{r}\;\;V_{n-r}]$ and $U:=[U_{r}\;\;U_{n-r}]$ , by adding orthogonal vectors so that $V$ forms a basis for $\mathbb{R}^{n}$ and $U$ forms a basis for $\mathbb{R}^{m}$ . Recall that $Av_i \in \operatorname{Im}(A) = \{y \in \Bbb C^n \mid y = Ax; x \in \Bbb C^n\}$ with $Av_i = \sigma_i u_i$ for $i = 1,\dots,r$ and $\sigma_i > 0$ . Moreover, $u_i = \frac 1{\sigma_i } Av_i$ form a basis for $\operatorname{Im}(A)$ for $i = 1,\dots, r$ . Thus, $v_{r+1},\dots,v_n \in \ker(A)$ i.e. $AV_{n-r} = 0$ , since otherwise $\operatorname{rank}(A) > r$ (proof by contradiction) The proof then goes on...","['matrices', 'proof-explanation', 'linear-algebra', 'svd']"
4157876,Proving $f_n(x)$ not uniformly convergent,"Let $f_n(x)$ be a sequence of functions that converge for $f(x)$ on $x\in[a,b]$ but it's not uniformly convergent on the same range of $x$ . Prove that $f(x)$ is not uniformly convergent on $x\in(a,b)$ . Here is my attempt : using contrapositive I assumed $f(x)$ is  uniformly convergent on $x\in(a,b)$ then $\sup_{x\in(a,b)}|f_n(x)-f(x)|\to 0$ . I'm assuming that from this point  I should conclude that $\sup_{x\in[a,b]}|f_n(x)-f(x)| \to 0$ which makes the statement true but I'm not sure what should I do next and is it possible that ( $\limsup_{x\in(a,b)}|f_n(x)-f(x)|=\limsup_{x\in[a,b]}|f_n(x)-f(x)|$ ). If it was the other way around it would be much easier I suppose.","['sequence-of-function', 'calculus', 'functions']"
4157880,Approximation of absolute value function using polynomial,"By Stone-Weierstrass theorem, the set of polynomial is dense in $C[a,b]$ , I am wondering what is the sequence of polynomial which can approximate absolute value function $|x|$ ? I know using $\sqrt{x^{2}+1/n}$ can approximate it but it is not a polynomial.","['functional-analysis', 'real-analysis']"
4157884,Generating functions in discrete mathematics - How to find coefficient from polynomial with multiplication,"So I encountered a certain problem when studying generating functions that I seem to be unable to fully solve. The problem is as follows: Carol is collecting money from her cousins to have a party for her aunt. If eight of the cousins promise to give 2, 3, 4, or 5 dollars each, and two others each give 5 or 10 dollars, what is the total number of ways Carol can collect exactly 40 dollars? The solution is the coefficient of the $x^{40}$ term in the generating function $(x^2 + x^3 + x^4 + x^5)^8(x^5+x^{10})^2$ which may be simplified to finding the coefficient of the $x^{14}$ term in $(1+x+x^2+x^3)^8(1+x^5) = (\frac{1-x^4}{1-x})^8(1+2x^5+x^{10}) = (1-x^4)^8(1-x)^{-8}(1+2x^5+x^{10})$ This is where I am stuck. To find the coefficient I know that I am suppose to find all possible combinations of how $x^{14}$ can be created by evaluating the polynomial. This would be done quite easily if only the first term $(1-x^4)^8(1-x)^{-8}$ would exist but how I am suppose to do this with the additional term $(1+2x^5+x^{10})$ present? I.e how do I evaluate the different combinations $x^{14}$ may be created and the find the coefficient with the second term present? There is a solution given in the book, but I do not understand the principle behind it.","['binomial-coefficients', 'discrete-mathematics', 'generating-functions']"
4157889,Can we compose two empty mappings?,"I am reading ""Set Theory and General Topology"" by Takeshi SAITO (in Japanese). This book contains the following problem: Let $X$ be a set. Let $F, G : \text{Map}(X, X)\times\text{Map}(X, X)\to\text{Map}(X, X)$ be mappings such that $F(f, g) = f\circ g$ and $G(f, g) = g\circ f$ . What is the necessary and sufficient condition on $X$ that ensures $F=G$ ? The author's answer for this problem is here: The necessary and sufficient condition on $X$ that ensures $F=G$ is $|X|\leq 1$ . If the number of elements of $X$ is less than or equal to $1$ , then $\text{Map}(X, X)=\{\text{id}_X\}$ . So, $F=G$ . Let $a, b\in X$ and $a\neq b$ . Let $a, b : X\to X$ be constant mappings such that $a(x) = a$ and $b(x) = b$ for any $x\in X$ . Then, $F(a, b) = a\circ b = a \neq b=b\circ a=G(a,b)$ . So, $F\neq G$ . Let $X=\emptyset$ . In this case, I think we cannot compose two empty mappings, so we cannot define $F$ and $G$ . So I think the necessary and sufficient condition on $X$ that ensures $F=G$ is $|X|= 1$ . Am I wrong?","['elementary-set-theory', 'function-and-relation-composition']"
4157904,Line tangent to conic section,"Let $l_1$ and $l_2$ be two lines in the real projective plane $\mathbb{R}P^2$ and let $\phi: l_1 \rightarrow l_2 $ be a projective transformation. Show that every line constructed by connecting a point $P \in l_1$ and its image $\phi(P)$ is tangent to the same conic section. I tried this exercise as follows: Let $Q$ be the intersection of the two lines $l_1$ and $l_2$ . We can assume (after a projective transformation) that $Q = E_0 = [(1,0,0)]$ . Since we have to come op with a conic section, I thought that the asked conic section was then the cone with top $E_0$ , call it $V(f)$ . The function $f$ is then equal to $$ f(x_1,x_2) = a x_1^2 + b x_2^2 + c x_1 x_2, \quad a,b,c \in \mathbb{R}, $$ since $V(f)$ is a cone with top $E_0$ and therefore $f$ is independent of $x_0$ . Now I am kinda stuck; I do not know how to proceed. I would like to give conditions to the variables $a,b$ and $c$ but I do not see how. Thanks for helping me!","['projective-geometry', 'geometry']"
4157907,"Weak convergence in $L^2(\mathbb{R}^3,L_{loc}^2(\mathbb{R}^3))$ thanks to diagonal extraction.","Let $f_n$ be a sequence bounded in $L^2(\mathbb{R}^3,L_{loc}^2(\mathbb{R}^3))$ which means that, for any bounded set in $\mathbb{R}^3$ , one has for any $n>0$ $$\int_{\mathbb{R}^3} \int_{B} |f_n(x,y)|^2 \ dx dy \leq M$$ with a constant $M$ independent of $n$ . I would like to prove that, up a to an extraction $\sigma$ , $(f_{\sigma(n)})_n$ weakly converges toward $f$ in $L^2(\mathbb{R}^3,L_{loc}^2(\mathbb{R}^3))$ . Using Banach-Alaoglu, I know that there exists an extraction $\sigma_B$ , depending on the set $B$ , such that $(f_{\sigma_B(n)})_n$ weakly converges toward $f_B$ in $L^2(\mathbb{R}^3,L^2(B))$ . I know the classic procedure is to use diagonal extraction to get the result, but as I am unfamiliar with the method I don't know how to prove the result. Does anyone know how to justify properly this property or know a reference that explains how to deal with it ? Any help is welcomed.","['lp-spaces', 'functional-analysis', 'reference-request']"
4157927,Representing a matrix as a scalar,"Is there a way to represent a matrix as a scalar without losing information? I know that I can use something as the Frobenius Norm, but that does not account for change in element positions. For example: The F norm of the (1x3) matrix (2, 1, 0) is equal to the F norm of a matrix like (1, 2, 0). But the matrices are different. So is there a way to represent a matrix by a scalar and also detect these position changes? UPDATE:
My practical problem here is I have a measurement process that outputs two matrices for a specific measurement. Both are square, symmetrical and integer matrices. First one looks like: [,1] [,2] [,3] [,4]
[1,]    0    1    1    0
[2,]    1    0    0    1
[3,]    1    0    0    0
[4,]    0    1    0    0 Second one: [,1] [,2] [,3] [,4]
[1,]    0    0    1    0
[2,]    0    0    0    1
[3,]    1    0    0    0
[4,]    0    1    0    0 Now, suppose I have a chain of measurements and need to detect an ""anomalous"" measurement (say, the matrix elements change radically in position and/or value). What I need to face this problem is to somehow convert a matrix to a single value and then perform anomaly detection on this chain of scalars. I thought about using the norm, but the posiotining information is lost, like I described above.","['functions', 'linear-algebra']"
4158025,Does $L^1(\mathbb R)$ implies $\limsup_{n\to\infty} f(nx)<\infty$ for almost every $x\in\mathbb R$?,"Question: Let $f\in L^1(\mathbb R)$ be non-negative. Is the proposition $$
\limsup_{n\to\infty} f(nx)<\infty
$$ true for almost every $x$ in $\mathbb R$ ? What I have been trying to prove is $$
\limsup_{n\to\infty} a_n f(nx)<\infty,\quad \text{a.e. }x\in\mathbb R
$$ for each real-values sequence $\{a_n\}$ that decreases to $0$ . I think Fubini's theorem might help, but I have no way of doing that.","['lebesgue-measure', 'lebesgue-integral', 'analysis', 'real-analysis']"
4158104,Prove that $\Bbb{R}^2/\Bbb{Z}^2\approx S^1\times S^1$,"Here $\Bbb{R}^2/\Bbb{Z}^2$ is the quotient space obatined from $\Bbb{R}^2$ by identifying points of $\Bbb{Z}^2$ i.e. $(x,y)\sim (x',y')\iff (x,y),(x',y')\in\Bbb{Z}^2$ . $S^1\times S^1:=\{(z,w)|\ z,w\in S^1\}$ I define $f:\Bbb{R}^2\to {S^1}\times S^1$ by $f(x,y)=(e^{2\pi i x},e^{2\pi i y})$ . $f$ is continuous and onto. As $f(n,m)=(1,1)\ \forall (n,m)\in \Bbb{Z}^2$ i.e. $f$ agrees on $\Bbb{Z}^2$ . By the property of quotient space, $f$ induces a continuous map $\tilde{f}:\mathbb{R}^2/\Bbb{Z}^2\to S^1\times S^1$ such that $\tilde{f}([x,y])=f(x,y)$ . This map is onto as well. But this map is not injective. I couldn't move forward from here. Although I have observed one thing- instead of only identifying the points of $\Bbb{Z}^2$ if we identify the points as follows- $$(x,y)\sim (x',y')\iff (x-x',y-y')\in\Bbb{Z}^2$$ Then we would have $\Bbb{R}^2/\sim\approx S^1\times S^1$ , the same $f$ will give rise to this homomorphism. Can anyone help me to solve the problem? Thanks for help in advance.","['general-topology', 'algebraic-topology', 'quotient-spaces']"
4158128,How to find a Lyapunov function in this case?,"We have the system of differential equations $$
\begin{aligned}
\frac{dx}{dt} &= y + \sin{x}\\
\frac{dy}{dt} &= -5x-2y.
\end{aligned}
$$ It's necessary to prove that the system is stable using a Lyapunov function or else show that it's not, following Lyapunov's/Chetaev's theorem. The first thing I want to ask about is whether it's appropriate to solve the problem for using the fact that $\sin{x} \approx x$ around $x=0$ : $$
\begin{aligned}
\frac{dx}{dt} &= y + x\\
\frac{dy}{dt} &= -5x-2y.
\end{aligned}
$$ If that is possible, I would usually check a few functions such as $V(x, y) = ax^2 + by^2$ or $V(x, y) = ax^4 + by^2$ , or $V(x, y) = ax^4 + by^4$ or even $V(x,y) = ax^2 + by^2 + cxy$ . The problem is that, unlike simpler problems I haven't yet managed to find such a function that the total derivative of $V$ is strictly positive/negative and the function itself is strictly negative/positive respectively for all pairs of $(x,y)$ except for $(0, 0)$ . Probably, I am trying to prove something that is not true and this is obvious from the beginning. I have tried a few simulation in Python to iterate over different suitable functions and values of $a, b$ to match the criterion, but there was no match.","['stability-theory', 'ordinary-differential-equations', 'lyapunov-functions']"
4158138,"If $|x_1-x_0|<R$, then the series $\sum_{n=1}^\infty n \cdot a_n(x-x_0)^{n-1}$ converges absolutely at $x_1$.","I have a long question regarding the power series $\sum_{n=1}^\infty a_n(x-x_0)^n$ . Let $0 \le R \le \infty$ be the convergence radius of the power series $\sum_{n=1}^\infty a_n(x-x_0)^n$ . In the first question I have successfully proved that for $|q|<1$ the limit $\lim_{n \to \infty}[n \cdot q^n]=0$ , while using the power series $\sum_{n=1}^\infty n \cdot x^n$ . Now I have this question: If $|x_1-x_0|<R$ , then the series $\sum_{n=1}^\infty n \cdot a_n(x-x_0)^{n-1}$ converges absolutely at $x_1$ .
Consider $q=\frac{|x_1-x_0|}{|x_2-x_0|}$ where $|x_1-x_0|<|x_2-x_0|<R$ I have tried to use the things I have proved above, but I don't have any clue on what to do. I will appreciate some hints on how to prove this. Thanks a lot!","['power-series', 'calculus', 'functions', 'sequences-and-series']"
4158139,Showing the remainder of a Taylor series is bounded by some sequence,"I have the function $$f(z)=\frac{z^{2}+1}{z^{10}-2}$$ and I wish to show $$|R_{n}(z)|<\frac{1}{2^{n}}$$ for its Taylor series around $0$ , for all $|z|<\frac{1}{2}$ . I thought I should use $$R_{n}(z)=\frac{1}{2\pi i}(z-z_{0})^{n+1}\oint_{\gamma}\frac{f(w)}{(w-z_{0})^{n+1}(w-z)}\,\mathrm dw$$ considering $\gamma$ , a circle of radius $r_1$ around $z_0=0$ . Also, if we denote $r_0=\frac12$ , it is clear that $$|w-z|=|(w-z_{0})-(z-z_{0})|\geq r_{1}-r$$ from the inverse triangle inequality. We can also notice that $$|w-z_0| = |w| = r_1$$ and get after the integral evaluation: $$|R_{n}(z)|\leq\frac{\max\limits_{w\in\gamma}|f(w)|}{1-r/r_1}\left(\frac{r}{r_{1}}\right)^{n+1}$$ But I can't seem to see a way to proceed from here. It obviously has something to do with finding the maximum of the function on the circle, but that doesn't seem to lead me to the answer. I will be glad for any help.","['complex-analysis', 'taylor-expansion']"
4158140,$K= \overline{conv(E)}$ iff for every continuous linear functional $f:X\rightarrow \mathbb{R} $ we have $ \sup_{x \in K}f(x)= \sup_{x \in E}f(x)$,"We consider $X$ a normed space, $K$ a compact and convex subset of $X$ and $E \subseteq K$ . We need to show that the following are equivalent: $K= \overline{conv(E)}$ for every continuous linear functional $f:X\rightarrow \mathbb{R} $ we have $ \sup_{x \in K}f(x)= \sup_{x \in E}f(x)$ Also, by $conv(E)$ , we denote the convex hull of $E$ , meaning the set of all convex combinations of points in $E$ . I know that $K = conv(Ext(K))$ , by the Krein-Milman theorem, where $Ext(K)$ is the set of the extreme points of $K$ . I don't know where to go from there. Any help would be greatly appreciated.","['functional-analysis', 'analysis', 'convex-hulls']"
4158142,Prove that $ t_{n}=1+\frac{\ln (2 n)}{2 n}+o\left(\frac{1}{n}\right) $ as $n$ tends to infinity.,"For all $n \geqslant 1$ , we define $$
f_{n}(t):=t^{2 n}-2 n t+1
$$ (i) Prove that there exists a unique solution of $f_{n}(t)=0$ in $[1,+\infty)$ . This solution will be denoted by $t_{n}$ . (ii) Prove that $\lim _{n \rightarrow+\infty} t_{n}=1$ . (iii) Prove that $$
t_{n}=1+\frac{\ln (2 n)}{2 n}+o\left(\frac{1}{n}\right)
$$ as $n$ tends to infinity. For the question (I)
Since $\lim_{t\to 1^{+}} f_n(t)=-2n<0$ And $\lim_{t\to +\infty}f_n(t)=+\infty<0$ By intermediate value theorem there exist $t_n \in[1, +\infty)$ which $f_n(t_n)=0$ For other question, I think for a lot but I cant do it. please kindly give me a hint or something that I can do this. THANK in advance !","['taylor-expansion', 'calculus', 'polynomials']"
4158170,"Weak solution of $u_t+(1-2u)u_x=0$ with $u(x,0)=\frac{1}{4}$ for $x<0$ and $u(x,0)=1$ for $x\geq 0$?","We want to find the weak solution  of $\frac{\partial{u}}{\partial{t}}+(1-2u)\frac{\partial{u}}{\partial{x}}=0$ with $u(x,0)=\begin{cases} \mbox{$\frac{1}{4}$} & \mbox{if } x<0 \\ \mbox{1} & \mbox{if $x\geq 0$} \end{cases} $ We have that $ x\in [-L,L]$ and $t\geq 0$ . By using the method of characteristics, I get: $$u(x,t)=\begin{cases} \mbox{$\frac{1}{4}$} & \mbox{if } x-\frac{1}{2}t<0 \\ \mbox{1} & \mbox{if $x+t\geq 0$} \end{cases} $$ I know that it can be shown that $\int_{-L}^Lu(x,t)dx=\frac{L}{4}+L=\frac{5}{4}L$ for all $ t>0$ . I'm not sure how I would proceed from here? If we let $x_D(t)$ be the position of the discontinuity, we require that: $\frac{5}{4}L=\int_{-L}^{x_D}u_{-}(x,t)dx+\int_{x_D}^{L}u_{+}(x,t)dx$ , however I'm unsure how to determine $u_{-}$ and $u_{+}$ ? I think this would help to determine the weak solution.","['continuity', 'multivariable-calculus', 'partial-differential-equations', 'real-analysis']"
4158196,"Let $f(x)$ be a thrice differentiable function in $[a,b]$, and $a<k_1<k_2<k_3<k_4<k_5<b$ and $f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1$","Let $f(x)$ be a thrice differentiable function in $[a,b]$ , and $a<k_1<k_2<k_3<k_4<k_5<b$ and $f(a)=f(b)=-2,f(k_1)=f(k_3)=3,f(k_2)=f(k_4)=-3,f(k_5)=-1$ , then min number of root of the equation $f(x)f'''(x)+f'(x)f''(x)=0$ in $[a,b]$ Graphing the function $f(x)$ (rough sketch) gives a minimum of 4 roots. Also the given expression can be written as $$g(x)=\frac {d(f(x)f''(x))}{dx}$$ Now to satisfy the minimum root condition, let us assume that the roots of $f(x)$ and $f''(x)$ coincide. so $f(x)f''(x)$ will have a minimum of four roots. Now since the derivative will have one root less than the original function, $g(x)$ will have at least 3 roots. However the correct answer is 4. I suspect this assumption of mine was wrong let us assume that the roots of $f(x)$ and $f''(x)$ coincide But I am not able to prove that it is wrong. Is there a proper solution for this?","['calculus', 'functions', 'derivatives']"
4158197,Expected norm of matrix product with random unit vector,"Let $S_d = \{x \in \mathbb{R}^d : \|x\| = 1\}$ be the unit sphere in $\mathbb{R}^d$ . Let $A \in \mathbb{R}^{n \times d}$ . Note that \begin{align}
    \max_{x \in S_d} \|A x\| &= \max \sigma(A) & \text{(spectral norm)} \\
    \min_{x \in S_d} \|A x\| &= \min \sigma(A) \\
    \operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|^2 &= \frac{\|\sigma(A)\|^2}{d}
\end{align} where $\sigma$ yields the singular values of a matrix (note that $\|\sigma(\cdot)\| = \|\cdot\|_\text{F}$ is the Frobenius norm). Is there a similar expression for $\operatorname*{\mathbb{E}}_{x \sim \text{uniform}(S_d)} \|A x\|$ ?","['operator-theory', 'normed-spaces', 'matrices', 'linear-algebra', 'singular-values']"
4158265,"Injective, surjective and bijective functions","I'm attempting the following proof, I need help in the last part and any recommendation is important for me, I appreciate the help: $2.$ Suppose that $f\colon A\to B$ and $g\colon B\to C$ are functions. Prove that if $g\circ f$ is injective, then $f$ is injective. Prove that if $g\circ f$ is surjective, then $g$ is surjective. Conclude that if $g\circ f$ is bijective, then $f$ is injective and $g$ is surjective \begin{align*}
\forall x,y\in A 
&\implies g(f(x)=g(f(y)))
\\
&\implies (g\circ f)(x)=(g\circ f)(y)
\\
&\implies x=y
\end{align*} $\therefore$ $f$ is injective Suppose that $c\in C$ $\exists b\in B$ $g(b)=c$ . $(g\circ f)(b)=c$ hence $g(f(b))=c$ . So $g$ is injective.","['elementary-set-theory', 'functions']"
4158283,Is $\pi$! irrational? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question The factorial value of $\pi$ can be found using the gamma function like so: $$\Gamma \left( \pi +1 \right) = \int\limits_0^\infty {s^{\pi} e^{ - s}\,\mathrm{d}s}  7.1880827$$ This, however, raises the question of if this number is irrational. It is well-known that $\pi$ is irrational, but is $\pi!$ irrational too?","['gamma-function', 'calculus', 'pi', 'irrational-numbers']"
4158299,Where is the function $f(x) = |x| + |x+1| + e^x$ not differentiable,"While plotting the function mentioned below using a graphing software quickly gives the answer, I am unable to understand how one must solve it.
I have studied some properties of modulus functions and would have been able to draw the graph if the $e^x$ was missing. Find where the function defined on the real line given by $f(x) = |x| + |x+1| + e^x$ is not differentiable. Could you please let me know how this question must be approached.
Moreover, a hint regarding how a question such as the one mentioned above can be solved if $e^x$ was replaced by some other function like $\log(x)$ or $\sin(x)$","['calculus', 'functions']"
4158321,How many times does a term appear in this sum?,"Let $F(a, b) = x^{a + b} + F(a, a + b) + F(b, a + b), x \in (0,\infty) -\{ 1 \}$ and $a, b \in \mathbb{N}$ . So $F(1, 2) = x^3 + F(1, 3) + F(2, 3) = x^3 + x^4 + x^5 + F(1, 4) + F(3, 4) + F(2, 5) + F(3, 5)$ . And the recursion continues infinitely. How many times does the general term $x^k, k \geq 3$ appear in the infinite sum that emerges from the above stated recursive function? for the call $F(1, 2)$ So basically I just want the multiplier of the general term. That is, if the sum is of form $S = \sum^{\infty}_{k = 3}a_kx^k$ then I want to find out the value of $a_k$ in terms of $k$ and constants. It is easy to observe that each term appears at least once because the call $F(1, i)$ will add $x^i$ to the sum and also call $F(1, i + 1)$ .",['functions']
4158361,Lie algebras role in algebraic group theory,"I know nothing about algebraic groups, but I know somewhat about Lie groups. I'm wondering what their exact analogue is in algebraic groups. Given $G$ , we still have the tangent space at $e$ , call it $T$ . We have $Ad:G \to GL(T)$ , and so we have a map $T \to End(T)$ , so lie bracket is still defined. Suppose $G, H$ are algebraic groups (connectedish?) is a map $G \to H$ determined by the lie algebra map? Is there a baker formula? The reason I am concerned this might not be true is I saw a proof in math overflow that a projective  irreducible algebraic group is abelian; the proof noted that by properness, $Ad$ is trivial (Being the image of a map from proper to affine), but while in regular Lie groups this would suffice to finish, he needed to note that it must also fix either order derivatives; (i.e the stalk mod a higher power of the maximal ideal).","['algebraic-geometry', 'lie-algebras', 'algebraic-groups']"
4158375,Understanding mathematical concept behind phase space and phase portait,"I'd like to expose the problem through an example, which was what made me think about it. It's a rational mechanics problem. Consider the one dimensional Cauchy problem $\begin{cases}m\ddot{x} = F(x,\dot{x},t) \\ x(0) = x_0 \\ \dot{x}(0) = v_0 \\\end{cases}$ where $F : \mathbb{R} \times \mathbb{R} \times \mathbb{R} \longrightarrow \mathbb{R}, x,x_0,v_0 \in \mathbb{R}$ , and suppose exists a potential $V$ , i.e a function such that $-V'(x) = F(x)$ . With the last sentence we made sure to consider only purely positional forces and as a consequence, ""purely positional potentials"". From this we can introduce thec concept of phase space and phase portrait. What I don't get about this last one, is the following : We started from the differential equation above, which has solutions that are scalar function that depends on time, i.e $x = x(t)$ , but when we use $V$ (which is a function of $x$ ) to deal with phase portrait, seems like $x$ has become now an idependant variable, forgetting about time. So my question is what mathematical concept is behind this type of study, what allows to do so, and why. Any help or reference would be appreciated, phase space seems a little bit misterious to me yet.","['ordinary-differential-equations', 'differential', 'classical-mechanics', 'physics', 'mathematical-physics']"
4158400,Finding ACF of MA Process,"trying to solve a general problem found in my textbook. I have the following MA Process, $X_t = \epsilon_t +0.8\epsilon_{t-1} + 0.4\epsilon_{t-12}+0.32\epsilon_{t-13} $ I simply want to calculate the autocorrelations of $X_t$ for lags k = 0 and k = 12 Here's what i've done so far: For $K = 0$ $E(X_t\epsilon_t) = \epsilon_t^2=1$ and for $K = 12$ $E(X_t\epsilon_{t-12}) = ? $ I think I am on the right path but would appreciate anyone who can guide me to the finish line! Thank you :D","['statistics', 'probability']"
4158402,Geometric condition number for uniform sampling a real algebraic set,"A problem I'm on a hunt to figure out. I'm not sure whether it's better placed here or on Overflow- it has the feeling of being very standard for the right person but I haven't done enough geometric measure theory to spot it. Happy to be prompted to migrate if it seems beyond scope. Setup: Let $f \in \mathbb{R}[x_1,\dots,x_N]$ and assume the real part of the corresponding algebraic set $S$ defined as $\mathcal{V}(f)\cap\mathbb{R}^N$ is compact. If it helps, assume that it's smooth. Let $\epsilon > 0$ and suppose $x\in S$ , i.e. $f(x) = 0$ . Question: Let $B(x,\epsilon)$ be the Euclidean ball of radius $\epsilon$ around $x$ in $\mathbb{R}^N$ . Can we associate some geometric condition number $c(S)$ based on a well-known geometric quantity (e.g. extrinsic curvature) to $S$ and give a lower bound on the (N-1)-dimensional Hausdorff measure (in $\mathbb{R}^N$ ) of the set $ S \cap B(x,\epsilon)$ in terms of $c(S)$ ? This is related to a probability question: Given $x$ and $\epsilon$ , is there a somewhat satisfying lower bound on the probability you get a point within $\epsilon$ of $x$ when you sample from the uniform distribution on $S$ given by Hausdorff measure? It seems visually intuitive that this should depend on extrinsic curvature, but I can't quite get it there. An upper bound would also be nice, but that's going to be trickier and rely on the algebraic properties more (you need Bezout's Theorem at least to limit spirals, I think). The background: Diaconis et al. have a type of tutorial on these questions https://statweb.stanford.edu/~cgates/PERSI/papers/sampling11.pdf . In particular, Section 2.2 recalls the following area formula recorded in Federer's Geometric Measure Theory . It gives a formula for the Hausdorff measure of a set in terms of a parameterization. The following is a slightly weaker form of the result for conciseness. Theorem Let $\lambda^m$ denote the Lebesgue measure on $\mathbb{R}^m$ . If $u:\mathbb{R}^m\to\mathbb{R}^n$ is Lipschitz, $A\subseteq\mathbb{R}^m$ is $\lambda^m$ measurable, $u(A) = P$ , and $u$ is injective: $$
H^m(P) = \int_{\mathbb{R}^n} 1_P(y) H^m(dy) = \int_A J_m u(z) \lambda^m(dz)
$$ where $J_m u(z)$ is the Jacobian $\sqrt{\det((Du(z)^T Du(z))}$ . In our context $P$ is our set $S\cap B(x,\epsilon)$ and we a priori don't know a parameterization $u:\mathbb{R}^{N-1}\to\mathbb{R}^N$ . The Implicit Function Theorem is standard for this, with the caveat that $\epsilon$ must be small enough. Assume for the sake of convenience that we can use the IFT on the last variable. We at least know that if $\epsilon$ is small enough we have a parameterization $u:U\to\mathbb{R}^N$ with $U\subseteq\mathbb{R}^{N-1}$ of the form $u(z) = (z,g(z))$ for some $g:\mathbb{R}^{N-1}\to\mathbb{R}$ . Furthermore, let $B=\frac{\partial f}{\partial x_N}(u(z)))^{-1}$ . Then $Du(z)$ is a matrix of the form $(I_{n-1} | (-B\hat\nabla_f(u(z)))^T$ where $\hat\nabla_f(u(z))$ is the first $n-1$ coordinates of $\nabla_f(u(z))$ . So $Du(z)^T Du(z) = I_{n-1} + B^2 \hat\nabla_f(u(z))\hat\nabla_f(u(z))^T$ . That's the best I have for now: There's probably some better way to write that matrix for the determinant, or even a better approach to the question. It still seems like there's more say after that if it's related to e.g. extrinsic curvature or some other familiar quantity.","['geometric-measure-theory', 'differential-geometry']"
4158415,Topos Theory as a link between logic and geometry,"I was reading about topos theory and in many ways some people said that TT can be used to unify logic and geometry. What does that mean? I have an OK background in category theory (at least up to adjunction theorems) and topology (general, differential and a little bit of algebraic topology) so feel free to those concepts in your explanation.","['topos-theory', 'logic', 'geometry', 'category-theory']"
4158429,Covering $6 \times 6$ chessboard with $1 \times 2$ tiles [duplicate],"This question already has an answer here : Minimum number of dominoes on an $n \times n$ chessboard to prevent placement of another domino. (1 answer) Closed 3 years ago . Problem : prove that after putting in 11 1 by 2 tiles in the 6 by 6 chessboard, there is definitely room to put in another tile. Tiles cannot overlap with each other. So I found this similar problem: https://www.ocf.berkeley.edu/~wwu/cgi-bin/yabb/YaBB.cgi?board=riddles_easy;action=display;num=1134555169 Apparently if you cover the whole chessboard with 18 tiles, you can find a line without crossing each tile. But is this a related problem to the original problem?","['contest-math', 'puzzle', 'chessboard', 'combinatorics', 'tiling']"
4158449,Prove that all the eigenvalues of the following SturmLiouville problem are positive: $u'' + (\lambda -x^2)u = 0$ $\hspace{0.5cm}$ $0< x < \infty$,"Prove that all the eigenvalues of the following SturmLiouville problem are
positive $u'' + (\lambda -x^2)u = 0$ $\hspace{0.5cm}$ $0< x < \infty$ $u'(0) = \lim_{x \to \infty}{u(x)} =0$ I'm trying to solve this Sturm-Liouville problem. But I haven't gotten a solution yet. I tried to solve it analogously to what I did in the following problem: $v'' + \lambda v= 0$ $\hspace{0.5cm} 0<x<L$ $v(0) = v(\pi) = 0$ In this case you can show that if $\lambda = 0$ , then $v(x) = 0$ . Then $\lambda = 0$ is not an eigenvalue of the problem. If $\lambda \neq 0$ , then the general solution to the problem is $v(x) = Ae^{i\sqrt{\lambda}x} + Be^{-i\sqrt{\lambda}x}$ . This solution satisfies the conditions of the problem if and only if \begin{cases} A +B =0\\ Ae^{i\sqrt{\lambda}\pi} + Be^{-i\sqrt{\lambda}\pi}=0 \end{cases} This system has a non-trivial solution if and only if $e^{-i\sqrt{\lambda}\pi} - e^{i\sqrt{\lambda}\pi}=0$ . From this it is obtained that the eigenvalues are $\lambda_n = n^2$ and since $B=-A$ , the eigenfunctions are $v_n{(x)} = 2iA\sin(nx)$ . However, my problem here is different in the sense that the coefficient $\lambda -x^2$ is variable by $x$ . I think that if $\lambda<0$ , let's say $\lambda = -k^2$ , then $u(x) = C_{1} e^{(\sqrt{k^2 + x^2})x} + C_{2}e^{-(\sqrt{k^2 + x^2})x}$ If $\lambda = 0$ , then $u'' -x^2 u = 0 \Rightarrow  u(x) =  C_{1} e^{(x)x} + C_{2}e^{-(x)x}$ If $\lambda> 0$ , let's say $\lambda = k^2$ , then $u(x) = C_{1} e^{(\sqrt{x^2 - k^2})x} + C_{2}e^{-(\sqrt{x^2 - k^2})x}$ Now I'm going to consider these cases and see which of them is possible, however I do not know if this form is correct.
How can I solve this problem?
I need some help or a way to solve this problem.
I would like to know a solution.
Any help is appreciated","['eigenfunctions', 'sturm-liouville', 'ordinary-differential-equations', 'eigenvalues-eigenvectors']"
4158453,How does this algebraically simplify to reach this result?,A problem I saw in my textbook uses the following logic to simplify an equation. $$\frac{m}{2}(m+1)+(m+1)\cdot\frac{2}{2}$$ $$=\frac{m+1}{2}(m+2)$$ $$=\frac{m+1}{2}[(m+1)+1]$$ The problem that I'm having is that I do not understand how the first line simplifies to the second line. Whenever I attempt to simplify the problem I am left with $$\frac{m(m+1)}{2}+m+1$$ which is pretty much the same as if the 2/2 was never included in the problem. Any insight on how this works would be greatly appreciated.,['algebra-precalculus']
4158455,What is the probability that 83% or more of this sample...?,"This equation comes from Edgenuity's course of Statistics, and I am taking the course as a high school senior. I understand how to find the z-score and, in this case, not the standard deviation. A cell phone provider has 85% of its customers rank their service as ""satisfactory. Nico takes a random sample of 75 customers from this cell phone provider. What is the probability that 83% or more of this sample ranks the providers service as ""satisfactory? A. 0.314 B. 0.485 C. 0.562 D. 0.686 My guess with the observed value and mean value for the $z$ -score equation would be to substitute $0.83$ for the former and 0.85 for the latter. For the standard deviation, I believe I use the equation $\sqrt{np (1-p)}$ , substituting $0.85$ for $p$ and $75$ for $n$ . However, when I found the calculation to be $\approx 3.0923$ , I thought it was higher than what I'm normally used to. And sure enough, when it was substituted into the $z$ -score equation, the z-score was far too small. How do I find the correct answer? What methods were incorrect?","['statistics', 'probability']"
4158457,Proving chain rule for Caratheodory derivative,"I am trying to follow the argument given in the following papers: S. Kuhn, ""The Derivative a la Caratheodory"" (1991); E. Acosta G., C. Delgado G., ""Frechet vs. Caratheodory"" (1994); S. Arora, H. Browne, D. Daners, ""An alternative approach to Frechet derivatives"" (2019). Definition of differentiability. Function $f : \mathbb{R}^n \to \mathbb{R}^m $ is said to be differentiable (in a sense of Caratheodory) at point $x \in \mathbb{R}^n$ if there is a one-parameter family of linear functions $\Phi_{x, \cdot} : \mathbb{R}^n \to L(\mathbb{R}^n, \mathbb{R}^m)$ that for all $y$ satisfies the following condition $$ \Phi_{x,y}(y-x) = f(y)-f(x) $$ and is continuous at $x$ , in other words, $$ \lim \limits_{y \to x} \Phi_{x,y} = \Phi_{x,x}. $$ Here, limit is to be understood in the usual sense where there is some metric on the space of all linear functions from $\mathbb{R}^n$ to $\mathbb{R}^m$ (usually taken to be operator norm). Derivative of $f$ at $x$ is then said to be linear function $\Phi_{x,x}$ . Proof of the chain rule given in the papers. Let us assume that we are given functions $f : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^m \to \mathbb{R}^p$ which are differentiable at $x \in \mathbb{R}^n$ and $f(x) \in \mathbb{R}^m$ respectively. In that case, let us start by thinking of family of linear functions that might be reasonable for $g \circ f$ . Here, we use that $f$ and $g$ are differentiable and we use some one-parameter family of linear functions $\Phi^{f}_{x, \cdot}$ and $\Phi^{g}_{f(x), \cdot}$ . $$ (g \circ f)(y) - (g \circ f)(x) = \Phi^{g}_{f(x), f(y)}(f(y)-f(x)) = (\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}) (y-x)$$ This motivates to consider one-parameter family of linear functions given as follows. $$ \Phi^{g \circ f}_{x,\cdot} = \Phi^{g}_{f(x), f(\cdot)} \circ \Phi^{f}_{x,\cdot} $$ Now, the only remaining thing is to show that it is continuous at $x$ . Part of the proof I do not understand: The claim is that this one-parameter family of linear function $\Phi^{g \circ f}_{x,\cdot} $ is continuous because $\Phi^{g}_{f(x), \cdot}$ , $\Phi^{f}_{x, \cdot}$ and $f$ are all continuous functions and composition of continuous functions is again continuous. I have trouble understanding this as I think that the composition of one-parameter families of linear functions is not with respect to variable $y$ . My approach: Instead, I would propose the following proof where the key idea is not that the composition of continuous functions is continuous but rather that if we take operator norm then it has submultiplicativity property, so the proof is similar to the proof of limit of product where the product is now the composition. Use that functions are linear and triangle inequality to have the following. $$|\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}| = $$ $$ = |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x})
+ (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} + \Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) | $$ $$ \leq |(\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ (\Phi^{f}_{x,y}-\Phi^{f}_{x,x})| + 
| (\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}) \circ \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)} \circ (\Phi^{f}_{x,y} - \Phi^{f}_{x,x}) |$$ Now, use submultiplicative property of operator norm to bound norm of composition by a multiplication of individual norms. $$|\Phi^{g}_{f(x), f(y)} \circ \Phi^{f}_{x,y}-\Phi^{g}_{f(x), f(x)} \circ \Phi^{f}_{x,x}| $$ $$  \leq |\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| + 
|\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}| | \Phi^{f}_{x,x} | + |\Phi^{g}_{f(x), f(x)}||\Phi^{f}_{x,y} - \Phi^{f}_{x,x}| $$ Now, from continuity of one-parameter family of linear functions and from continuity of $f$ (as it is differentiable), we get that there is $\delta_1 > 0$ such that if $0 < |y-x| < \delta_1$ then $|f(y) - f(x)| < \delta_2$ and $|\Phi^{g}_{f(x), f(y)}-\Phi^{g}_{f(x), f(x)}|< \textrm{min}(1,\varepsilon)/(3+| \Phi^{f}_{x,x} |)$ . Also, there is $\delta_2 > 0$ such that if $0 < |y-x| < \delta_2$ such that $|\Phi^{f}_{x,y}-\Phi^{f}_{x,x}| < \textrm{min}(1,\varepsilon)/(3+|\Phi^{g}_{f(x), f(x)}|)$ . Then, taking $\delta = \textrm{min}(\delta_1, \delta_2)$ gives the desired results. Problems with my approach: even if we assume that my proof is correct, this is somewhat in contradiction with what authors claim about why Caratheodory derivative might be useful. The usual claim is that all proofs become easier as one just needs to consider continuity of functions and does not rely on many computations with $\varepsilon-\delta$ . On the other hand, my proof shows that Caratheodory derivative approach requires many details to be proved, for example, it requires that out of all norms we use norm which is submultiplicative (not obvious to me why such choice of norm would be natural or obvious), and proof required to work with $\varepsilon - \delta$ . Also, it is not clear to me how (and if possible) to rewrite my approach in a more concise way using only that composition of continuous functions is continuous (or a similar result).","['derivatives', 'chain-rule', 'real-analysis']"
4158557,An integral : $\int_{-a}^a \frac{\cos(\pi n x/a)}{x^2+a^2}\mathrm dx$,"How can I evaluate $$\int_{-a}^a \frac{\cos(\pi n x/a)}{x^2+a^2}\,\mathrm dx$$ ? My attempt : $$\begin{align}\mathcal{I}& =\displaystyle\int_{-a}^a \dfrac{\cos(\pi n x/a)}{x^2+a^2}\mathrm dx \\ & = 2\displaystyle\int_0^a \dfrac{\cos(\pi n x/a)}{x^2+a^2}\, \text{ ,even integrand}\\ &=2\displaystyle\int_0^{\pi n} \dfrac{\cos u}{\frac{a^2u^2}{n^2\pi^2}+1}\left(\dfrac{a\mathrm du}{n\pi}\right)\, \text{ ,via substituting $u=\dfrac{\pi nx}{a}$}\\ &=\dfrac{2\pi n}{a}\displaystyle\int_0^{\pi n} \dfrac{\cos u}{u^2+n^2\pi^2}\, \mathrm du\\ & =\dfrac{2\pi n}{a}\displaystyle\int_0^{n\pi} \dfrac{\cos u}{(u-i\pi n)(u+i\pi n)}\,\mathrm du\\ & =\dfrac{i}{a}\underbrace{\displaystyle\int_0^{n\pi} \dfrac{\cos u}{u+i\pi n}\mathrm du}_{\mathcal{I_1}}-\dfrac{i}{a}\overbrace{\displaystyle\int_0^{n\pi}\dfrac{\cos u}{u-in\pi}\mathrm du}^{\mathcal{I_2}}\, \text{ ,via partial fraction decomposition}\end{align}$$ $$\begin{align}\mathcal{I_1}&=\displaystyle\int_0^{n\pi}\dfrac{\cos u}{u+i\pi n}\mathrm du \\&=\displaystyle\int_0^{(i+1)n\pi}\dfrac{\cos(v-i\pi n)}{v}\mathrm dv\, \text{ ,via substituting $v=u+i\pi n$}\\ & = \displaystyle\sum_{k=0}^\infty \dfrac{(-1)^k}{(2k)!}\displaystyle\int_0^{(i+1)n\pi} \dfrac{(v-i\pi n)^{2k}}{v}\mathrm dv\, \text{ ,via Maclaurin series}\end{align}$$ from where I can't figure out further. Any help is appreciated.","['integration', 'calculus']"
4158603,(combinatorial and algebraic) proof that $\sum_{q=1}^n \binom{n}{q}\binom{n-q}{2k-2q}2^{2(k-q)} = \binom{2n}{2k} - \binom{n}{2k}2^{2k}$,"I arrived at the following identity throught an attempt at a homework problem: the LHS is my solution; the RHS, the book's one. And as far I have tested , they are equivalent. $$\sum_{q=1}^n \binom{n}{q}\binom{n-q}{2k-2q}2^{2(k-q)} = \binom{2n}{2k} - \binom{n}{2k}2^{2k}$$ The combinatorial meaning of this identity is rather intuitive as per the homework's verbatim: ""Find the number of ways of forming a group of $2k$ people from $n$ couples where $k,n \in \Bbb N$ and $2k \leq n$ so that at least one couple is included in such group"". That being said, the combinatorial meaning of the LHS (and even the RHS) is rather clear: it iterates over $q = \{1,2,...,n\}$ couples, selects $2k-2q$ couples from the remaining $n-q$ and then picks one person from each of these $2k-2q$ couples,  which can be done in $2^{2k-2q}$ ways, to form the group. The RHS does this by composition: removing those groups that do not contain one couple. However, while I know what the LHS is supposed to do, I don't know how it works. I know that the groups that do not contain 1 couple is given by $\binom{n}{2k}2^{2k}$ (after all, this is the case where $q=0$ ), but I have little idea as to how the $\binom{2n}{2k}$ term counts the ""rest"". And I have way fewer ideas about how would I prove this thing algebraically. Later edit: I realized that the $\binom{2n}{2k}$ term simply counts the number of ways of grouping $2k$ people from the overall $2n$ people of the $n$ couples. So ofc all you'd need to do after that is remove those groups that do not contain 1 couple.","['summation', 'binomial-coefficients', 'combinatorics', 'combinatorial-proofs']"
4158712,$\widehat{A}$ is $\widehat{m}$-formally tale over $A$,"In example 28.1 from the book Commutative ring theory of Matsumura, it is told that the completion $\widehat{A}$ of the local ring $(A,\mathfrak{m})$ is $\hat{m}$ -formally smooth over $A$ . Recall of $\hat{m}$ -formally-smooth: if $C$ is an $A$ -algebra with a squared zero ideal $I$ , given $u:\widehat{A}\to C/I$ continuous for the discret topology of $C/I$ (that is $u(\hat{m}^k)=0$ for some $k$ ) there exists a lifting of $u$ to $C$ . $\require{AMScd}$ \begin{CD}
\widehat{A} @>{u}>> C/I\\
@AAA @AAA\\
A @>{\lambda}>> C
\end{CD} There is a clue: use that $\widehat{A}/\hat{m}^k\simeq A/\mathfrak{m}^k$ . If one has a $A$ -algebra $C$ and with squared zero ideal $I$ , a morphisme $u:\widehat{A}\to C/I$ , if $u(\hat{m}^k)=0$ then $u$ induces $$ \widehat{A}\to \widehat{A}/\hat{m}^k=A/\mathfrak{m}^k\to C/I $$ so that it is enough to have a lifting of $A/\mathfrak{m}^k\to C/I$ to $C$ : $\require{AMScd}$ \begin{CD}
A/\mathfrak{m}^k @>{u}>> C/I\\
@A{s}AA @A{\sigma}AA\\
A @>{\lambda}>> C
\end{CD} but is seems it should require that $\lambda(\mathfrak{m}^k)=0$ which is not in the hypothesis. Any idea?","['algebraic-geometry', 'abstract-algebra', 'commutative-algebra']"
4158729,"Assume $y(t)$ is a solution of the problem $y''+e^{-t}y'+y=0,y(0)=1,y'(0)=2$ then $(y'(t))^2+(y(t))^2\le 5,\quad \forall t\in[0,\infty)$","Assume $y(t)$ is a solution of the problem $$y''+e^{-t}y'+y=0,y(0)=1,y'(0)=2$$ then $$(y'(t))^2+(y(t))^2\le 5,\quad \forall t\in[0,\infty)$$ I have considered to use mean value theorem but did not end up so well. Then I comment on the $e^{-t}y$ term since as t goes infinity this term vanishes so we end up with $y''+y=0$ which has periodic solution $$c_1 \cos(t)+c_2 \sin(t)$$ and applying initial values we have $$y=\cos(t)+2\sin(t)$$ and taking derivative we see that $y^2+(y')^2\le \sqrt5^2+\sqrt5^2$ If we tried to start with equation as $$y''+y'+y=0$$ assuming $e^{-t}$ is max at $t=0$ so start with maximum (But it doesnot imply maximality of the desired equality so again failed) I think there should be more simple way to analyse the question, any hint answer would be appreciated!",['ordinary-differential-equations']
4158735,Complementary function / particular integral,"The function $y(x)$ satisfies $$
x^2 \frac{d^2 y}{dx^2} + (1-2a)x \frac{dy}{dx} + a^2 y = 3x^b,
$$ where a and b are real parameters. Can somebody explain how to find the complementary function for this and how I would find what the particular integral would be where it is $b \neq - a$ . I have been grappling with the concept for the complementary function that $ycf(x)$ is equal to $A(x) + B(x)$ where they are constants but it is just going entirely over my head right now, especially the particular integral to the ODE.",['ordinary-differential-equations']
4158759,"Solving a trigonometric equation, can someone check please?","We are asked to solve the equation $$-\sin x + 1 = \cos x + 2$$ where $x \in [0,\frac{3\pi}{2}]$ What I did was this: \begin{align*}
   \sin x + \cos x &=-1 \\
   1 + \sin2x &= 1 \\
   \sin 2x &=0 \\
   2x &\in \{0, \pi, 2\pi\} \\
   x &\in \left\{0,\frac{\pi}{2}, \pi\right\}
\end{align*} Now, by checking, we can see that only $\frac{\pi}{2}$ is the solution. But is it correct to square the equation? Edit: I just realized that $\frac{\pi}{2}$ is not a solution and $\pi$ and $\frac{3\pi}{2}$ should are. I am sorry that I didn't include $\frac{3\pi}{2}$ , this is because in the actual context that I am working with, I already know that it is a solution.","['algebra-precalculus', 'solution-verification', 'trigonometry']"
4158791,How to prove whether $i\in\mathbb{Q}(\cos\frac{2\pi}{5}+i\sin\frac{2\pi}{5})$ or not,"I've been solving problems from my Galois Theory course, and I need help with some detail in this one. It says: Calculate how many subfields has the splitting field of $P=X^7+4X^5-X^2-4$ over $\mathbb Q$ . What I first noticed is that $P$ can be expressed as product of irreducibles as: $$P=(X-1)(X^4+X^3+X^2+X+1)(X^2+4),$$ so the roots of $P$ are $\{1,\omega,\omega^2,\omega^3,\omega^4,2i,-2i\}$ , where $\omega=\cos\frac{2\pi}{5}+i\sin\frac{2\pi}{5}$ . Hence, the splitting field of $P$ over $\mathbb{Q}$ is $$\mathbb{Q}(1,\omega,\omega^2,\omega^3,\omega^4,2i,-2i)=\mathbb{Q}(i,\omega).$$ Now, I want to find $\operatorname{Gal}(\mathbb{Q}(i,\omega)/\mathbb Q)$ , but to correctly find the automorphisms I first need to know the degree of the extension $\mathbb Q(i,\omega)/\mathbb Q$ (then I would know that degree equals the number of automorphisms, since it's a Galois extension because $\mathbb{Q}(i,\omega)$ is splitting field over $\mathbb Q$ of a separable polynomial). My problem is finding this degree. I know $$[\mathbb Q(i):\mathbb Q]=2, \quad [\mathbb Q(\omega): \mathbb Q]=4,$$ but I'm not sure if the degree of $\mathbb Q(i,\omega)/\mathbb Q$ is $4$ or $8$ . My intuition tells me the easiest path will be checking if $i\in\mathbb Q(\omega)$ (implying degree $4$ ) or if $i\notin\mathbb{Q}(\omega)$ (implying degree $8$ ). However, I don't figure out an easy way to prove this. How can I find is $i$ is or not inside $\mathbb Q(\omega)$ ? Is there an easier way to find this degree? Any help or hint will be appreciated, thanks in advance.","['galois-theory', 'abstract-algebra', 'splitting-field', 'extension-field', 'galois-extensions']"
4158818,Prove that this map isn't invertible on all of $\mathbb R^2$.,"Let $F(x,y)=(e^x\cos y, e^x\sin y)$ , show that $F$ isn't invertible on all of $\mathbb R^2$ , although it's locally invertible everywhere. It's obvious that $F$ is locally invertible everywhere, because $$J_F(x,y)=\pmatrix{e^x\cos y & -e^x\sin y\\ e^x\sin y & e^x\cos y}$$ so the determinant of the jacobian matrix is $$\Delta_F(x,y)=e^{2x}\cos^2x+e^{2x}\sin^2x=e^{2x}\ne0$$ Hence $F$ is locally invertible everywhere, but i can't prove that $F$ isn't invertible.","['matrices', 'jacobian', 'multivariable-calculus', 'inverse']"
4158821,Property of a closed path,"I have the feeling this property holds true but I cannot find a way to prove it: Let $\gamma: [0,1] \to \mathbb{R}^n$ be a differentiable closed path, i.e. $\gamma(0) = \gamma(1)$ . Then there exist two values $\lambda_1 \neq \lambda_2 \in [0,1[$ such that $$ \left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right> \geq 0, $$ and $$ \left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right> \geq 0. $$ Graphically this means that I can find two points on my path such that the projection of the tangent vector at these points on the line linking these two points face each other. I would be happy to discuss how this problem could be approached :) EDIT: I did not want to influence the reflexion in a way that would lead to a dead-end but here is the way I tried to approach the problem. Let define the function $f: [0,1] \to \mathbb{R}$ as follows: $$f(\lambda)
=
\left<\gamma'(\lambda),  \gamma(\lambda^{+}) - \gamma(\lambda) \right>
+
\left<\gamma'(\lambda^{+}),  \gamma(\lambda^{+}) - \gamma(\lambda) \right>,
$$ with $\lambda^{+}(\lambda) = mod(\lambda + 0.5, 1)$ . Hence, for instance $\lambda^{+}(0.25) = 0.75$ and $\lambda^{+}(0.9) = 0.4$ . Obviously $\lambda^{+}(\lambda^{+}(\lambda)) = \lambda$ . We have that $f$ is continuous and $f(\lambda^{+}) = - f(\lambda)$ .
Therefore, one can find with the Intermediate Value Theorem a value $\lambda_1$ such that $f(\lambda_1) = 0$ . If I define and $\lambda_2 = \lambda^+(\lambda_1)$ , this implies that $$
\left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
+
\left<\gamma'(\lambda_2),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
= 0 
$$ $$
\Leftrightarrow
\left<\gamma'(\lambda_1),  \gamma(\lambda_2) - \gamma(\lambda_1) \right>
=
\left<\gamma'(\lambda_2),  \gamma(\lambda_1) - \gamma(\lambda_2) \right>.
$$ This means that I can find two points on my curve $\gamma(\lambda_1) $ and $\gamma(\lambda_2)$ such that the projection of the tangent vector at these points on the line between these points either point towards each other, either away from each other. I am however stuck on how to prove that the former should hold (I believe it should based on intuition and a lot of examples I tried).","['vector-analysis', 'real-analysis']"
4158888,To prove continuity of $\varphi(p) = \int_X |f|^p\ d\mu$ on $E = \{p: \varphi(p) < \infty\}$ where $0 < p < \infty$,"Suppose $f$ is a complex measurable function on $X$ , $\mu$ is a positive measure on $X$ , and $$\varphi(p) ~=~ \int_X |f|^p \; d\mu \quad (0 <p <\infty)$$ Let $E :=\{ p : \varphi(p) <\infty\}$ . Assume $\|f\|_\infty >0$ . Prove that $\log\varphi$ is convex in the interior of $E$ and that $\varphi$ is continuous on $E$ . I have proved that $\log\varphi$ is convex on $\operatorname{int}E$ , using Holder's inequality. We know that log-convexity implies convexity, so $\varphi$ is also convex on $\operatorname{int}E$ , and hence continuous on $\operatorname{int}E$ . How do we establish the continuity of $\varphi$ on $E$ , though? I am missing exactly the points in $E\setminus\operatorname{int}E$ . I know this question has been previously answered here , but I'm unable to understand the proof. Moreover, I am trying to take a different approach to prove continuity. Let $\{p_n\}_{n\in\mathbb N}$ be a sequence in $E$ , such that $p_n\to p$ (may or may not be in $E$ - we don't know if $E$ is closed, right?). I wish to show $\varphi(p_n)\to \varphi(p)$ , i.e. $$\lim_{n\to\infty} \int_X |f|^{p_n}\ d\mu = \int_X |f|^p \ d\mu$$ This looks like a possible application of Lebesgue's Dominated Convergence Theorem, but I could not find a dominating function yet. I would appreciate any help, thanks a lot! I have already seen this answer - it did not help much. The proof was too convoluted, and I needed more details. @OliverDiaz and @robjohn were able to provide convincing detailed arguments, hence this post.","['measure-theory', 'lp-spaces', 'functional-analysis']"
4158928,Why $\mathbb{Z}_{p}[p^{1/p^{\infty}}]/p$ $\cong$ $\mathbb{F}_{p}[t^{1/p^{\infty}}]/t$?,"I'm reading Peter Scholze's Perfectoid Space , and I'm confused with the isomorphism $\mathbb{Z}_{p}[p^{1/p^{\infty}}]/p$ $\cong$ $\mathbb{F}_{p}[t^{1/p^{\infty}}]/t$ . What is the meaning of $\mathbb{Z}_{p}[p^{1/p^{\infty}}]/p$ and $\mathbb{F}_{p}[t^{1/p^{\infty}}]/t$ ? What's more, how to define the completions of $\mathbb{Q}_{p}(p^{1/p^{\infty}})$ and $\mathbb{F}_{p}((t))(t^{1/p^{\infty}})$ ?","['number-theory', 'abstract-algebra', 'algebraic-geometry', 'arithmetic-geometry']"
4158959,Finite magmas representing all unary functions by terms,"Say that a magma $\mathcal{M}=(M;*)$ is unary-rich iff for every function $f:M\rightarrow M$ there is a (one-variable, parameter-free) term $t_f$ such that $t_f^\mathcal{M}=f$ . For example: The one-element magma (and the empty magma if we permit such) is trivially unary-rich. A bit less trivially, there is a two-element unary-rich magma (in fact exactly 0ne up to isomorphism): namely $M=\{a,b\}$ and $*$ given by $$a*a=b, a*b=a, b*a=a, b*b=b$$ via the terms, $x, xx, (xx)x,$ and $(xx)(xx)$ . My question is: Are there unary-rich magmas of every (or at least arbitrarily large) finite cardinality? Of course every unary-rich magma is finite since there are only countably many terms. Note that if $\vert M\vert=n<\omega$ then $M$ is unary-rich iff $M$ has $n^n$ -many distinct one-variable terms up to equality-as-functions. Already the case of $3$ -element magmas seems interesting and difficult to brute-force .","['universal-algebra', 'model-theory', 'abstract-algebra', 'magma']"
4158972,Can there be a triangle ABC if $\frac{\cos A}{1}=\frac{\cos B}{2}=\frac{\cos C}{3}$?,"Can there be a triangle ABC if $$\frac{\cos A}{1}=\frac{\cos B}{2}=\frac{\cos C}{3}\;?$$ Equating the ratios to $k$ we get $\cos A=k$ , $\cos B=2k$ , $\cos C=3k$ .
Then the identity $$\cos^2A+\cos^2B+\cos^2C+2\cos A \cos B \cos C=1 \implies 12k^3+14k^2-1=0$$ $f(k)=12k^3+14k^2-1$ being monotonic for $k>0$ can have at most one real positive root. Further, $f(0)=-1, f(1/3)=1>0$ , so there will be one real root in $(0,1/3)$ . Hence all three cosines will be positive and less that 1, for a unque triangle to be possible. What can be other ways to solve  this question?","['calculus', 'trigonometry']"
4158986,Is the series $\sum_{n=1}^\infty \frac{3^n n^2}{n!}$ convergent,Is the following series convergent or divergent? $$\sum_{n=1}^\infty \frac{3^n n^2}{n!}$$ I used root test to check but I am stuck at $$\lim\limits_{n \to \infty} \sqrt[n]\frac{3^n n^2}{n!} = \lim\limits_{n \to \infty} \frac{3n^\frac{2}{n}}{n!^\frac{1}{n}}$$ I don't know how to calculate $\lim\limits_{n \to \infty} n!^\frac{1}{n}$ . So I'm guessing maybe I am going at it in the wrong way.,"['real-analysis', 'calculus', 'sequences-and-series', 'limits', 'convergence-divergence']"
4159011,Rings with primal term reducts,"This question is a follow-up to this one . Say that a term reduct of a ring $\mathcal{R}=(R; +,\times,0,1)$ is a magma $\mathcal{M}$ whose domain is $R$ and whose magma operation is $(x,y)\mapsto t(x,y)$ for some (parameter-free, two-variable) term in the language of rings (denote this magma by "" $\mathcal{R}_t$ ""). For example, if $\mathcal{S}=\mathbb{Z}/2\mathbb{Z}$ and $u(x,y)=(x+1)(y+1)$ then $\mathcal{S}_u$ is isomorphic the magma with domain $\{0,1\}$ and operation $(x,y)\mapsto \max\{1-x,1-y\}$ . I'm interested in the number of terms, or at least unary terms, of term reducts of rings. Specifically: An algebra is primal iff every function on the algebra is represented by a term. An algebra is unary-rich iff every unary function on the algebra is represented by a term. For example, the $\mathcal{S}_u$ above is unary-rich (and I think primal, but I'm not certain about that). My question is: Which rings have primal (or at least unary-rich) term reducts? Even the $\mathbb{Z}/p\mathbb{Z}$ s seem nontrivial here. Note that no magma with more than one element which has an idempotent can be unary-rich, so we need to look at terms more complicated than just $+$ or $\times$ .","['universal-algebra', 'model-theory', 'abstract-algebra', 'magma']"
4159052,very basic set theory ? about the Pairing Axiom over multiple sets,"I'm going through a text 1 right now that's laying out basic set axioms. It describes/defines the Pairing Axiom, and then gives the following example: ""Given three sets u, v, and w"". The pairing axiom yields the sets {u, v}, {{u, v}, w}, {{u, v}, {w}}. I think that this list is meant to be exhaustive. But aren't there other sets also generated on u,v,w by the Pairing axiom? Specifically - what about $\{v,w\}$ , $\{u, \{v, w\}\}$ , or $\{\{u\}, \{v,w\}\}$ for example? I feel like I'm missing something obvious and found no answer elsewhere, thanking you in advance for any thoughts 1 Daniel W. Cunningham: Set Theory: A First Course , page 32",['elementary-set-theory']
4159054,$x^{x\sqrt{x}} = (x\sqrt{x})^x$: How can I correctly explain the existence of the root $x=1$?,"This is a fairly well-known equation: $x^{x\sqrt{x}} = (x\sqrt{x})^x$ $$x^{x\sqrt{x}} = (x\sqrt{x})^x \iff x^{x^{3/2}}=(x^{3/2})^x \iff x^{x^{3/2}}=(x^{3x/2})$$ Take logarithms on both sides: $$x^{3/2}\ln{x}=\frac{3x}{2}\ln{x}$$ From here it is quite easy to get one of the roots $x=\frac{9}{4}$ and $x=0$ , if we divide both sides by $\ln{x}$ . However, why is $0$ not suitable, but $1$ is suitable?",['algebra-precalculus']
4159069,why $\Lambda_N g = \Lambda_n g $?,"In Rudin RCA theorem $2.20 $ Page no: $51$ Rudin say that if $n >N$ , then we have $$\Lambda_N g =\Lambda_n g \leq \Lambda_n f \leq \Lambda_n h = \Lambda_N h$$ Here I'm confused that why $\Lambda_N g =\Lambda_n g $ ? My attempt: It is given that $g$ is constant on each box in $\{\Omega_n\}$ Now if $n > N$ , then $$\Lambda_n g :=  2^{-nk} \sum\limits_{x \in P_n} g(x)$$ Now  using the Property :
For $\{\Omega_n\}$ , if $Q\in \Omega_r$ , then vol $(Q)=2^{-rk}$ ; and if $n>r$ , the set $P_n$ has exactly $2^{(n-r)k}$ points in $Q$ we  have $$\Lambda_n g :=  2^{-nk} g(x)\sum\limits_{x \in P_n}.1=2^{-nk} g(x)2^{(n-N)k}=2^{-Nk}g(x)$$ $$\implies 2^{-Nk}g(x) \neq \Lambda_N g :=  2^{-Nk} \sum\limits_{x \in P_n} g(x)$$ Therefore $$\Lambda_N g \neq
 \Lambda_n g$$",['measure-theory']
4159078,Strong law of large numbers for transformations of non-iid random variables,"Let $X_1, X_2, \ldots$ be a sequence of $E$ -valued random variables (not necessarily iid), for $(E,\mathcal{E})$ some measurable space. For any $f : E\rightarrow\mathbb{R}$ integrable, define $\hat{\mathbb{E}}_n(f):=\frac{1}{n}\sum_{j=1}^n f(X_j)$ and $\mathbb{E}(f):=\mathbb{E}[f(X_1)]$ . Suppose that there is an integrable $f_0 : E\rightarrow\mathbb{R}$ such that $$\tag{1} \hat{\mathbb{E}}_n(f_0)\longrightarrow \mathbb{E}(f_0) \quad \text{almost surely} \quad \text{as } \ n\rightarrow\infty.$$ Given regularity conditions of your choice on $f_0$ , are you aware of 'minimal' dependence restrictions on the sequence $(X_j)$ that are sufficient to from (1) infer that $$\tag{2}\lim_{n\rightarrow\infty}\hat{\mathbb{E}}_n(f)=\mathbb{E}(f) \quad \text{a.s.} \quad \text{for $\textbf{any}$ integrable } \ f:E\rightarrow\mathbb{R} \, ?$$ Edit: If $f_0=\mathrm{id}_E$ , then $(X_j)$ iid guarantees '' $(1)\Rightarrow (2)$ '' as desired. I'm interested in a choice of $f_0$ such that'' $(1)\Rightarrow (2)$ '' holds for a sequence $(X_j)$ with a less (least?) restrictive dependence structure.","['measure-theory', 'probability-limit-theorems', 'real-analysis', 'law-of-large-numbers', 'probability-theory']"
4159106,Rudin Ch.11 Exc 11: proof check,"The exercise. Denote the space of all Lebesgue integrable functions on $X$ as $L(\mu)$ i.e. finite Lebesgue integral. Suppose $f,g \in L(\mu).$ Define the distance function $$d(f,g) = \int_X |f-g| d\mu.$$ Show that $L(\mu)$ is a complete metric space i.e. all Cauchy sequences are convergent. My proof. (Skip the proof of $d$ is a metric). Suppose $\{f_n\}$ is a Cauchy sequence. Let $\epsilon > 0$ . There exists $N$ such that $m,n \geq N$ implies that $\int_X |f_n - f_m| d\mu < \epsilon$ . Fix $m \geq N$ . Define $g(x) = \liminf_{n \to \infty} f_n(x)$ pointwise. We first show that $\liminf_{n\to \infty} |f_m(x) - f_n(x)| = |f_m(x) - g(x)|$ . By definition of limit inferior, there exists $T$ such that if $n\geq T$ , then $ 0\leq g(x) - f_n(x) < \epsilon$ , and there exists a subsequence such that $\lim_{k \to \infty} f_{n_k}(x) = g(x)$ . Hence, if $n\geq T$ , then $$|f_m(x) - g(x)| - |f_m(x) - f_n(x)| \leq |g(x) - f_n(x)| < \epsilon.$$ Furthermore, for large enough $k$ , $$||f_m(x) - f_{n_k}(x)| - |f_m(x) - g(x)|| \leq |g(x) - f_{n_k}(x)| < \epsilon, \text{ so } \lim_{k\to \infty} |f_m(x) - f_{n_k}(x)| = |f_m(x) - g(x)|.$$ By Fatou's lemma, we have $$\int_{X} \liminf_{n\to\infty} |f_m(x) - f_n(x)| = \int_X |f_m(x) - g(x)| \leq \liminf_{n \to\infty} \int_X |f_m(x) - f_n(x)| < \epsilon.$$ Hence, if $m \geq N$ , then $d(f_m, g) < \epsilon$ . Also, $g \in L(\mu)$ because $g-f_m \in L(\mu)$ and $f_m \in L(\mu)$ , so $g = g-f_m + f_m \in L(\mu)$ . From this follows completeness. I checked other proofs of this, but I can't figure out what exactly is wrong with my proof. Could someone help me in checking my proof? It seems alright to me... thank you!","['measure-theory', 'solution-verification', 'lebesgue-integral', 'real-analysis']"
4159142,If a vector space and one subspace are both infinite-dimensional. Are they equal?,"From linear algebra I know this theorem: If V is a finite-dimensional vector space and W is a subspace of V, then $$
i)~ dim~ W \leq dim ~V\\
ii) ~dim~ W = dim~ V \longrightarrow W=V
$$ As a lemma: If $dim ~W = \infty$ $\longrightarrow$ $dim~V = \infty$ My question is: Being W a subspace of V, Is this valid? $$
dim~W=dim~V = \infty \longrightarrow W = V
$$ My tentative answer is no. If V is the vector space of continuous functions, and W is the vector space of polynomials, then W is a subspace of V and is infinite-dimensional, as well as V (by the last lemma). Here $dim~W=dim~V=\infty$ but $W\neq V$ . I would like to ensure this proof is correct, thanks.",['linear-algebra']
4159157,Is there a solution $y(t)$ for $y''+e^ty=0$ satisfying $\lim\limits_{t\to \infty} y(t)=1$,Is there a solution $y(t)$ for $y''+e^ty=0$ satisfying $\lim\limits_{t\to \infty} y(t)=2021$ If we write the eq as $\dfrac{y''}{e^{-t}}+y=0$ then $y$ cannot be equal to non zero constant because of uniqueness? How to approach that kind of problem?,['ordinary-differential-equations']
4159174,Let $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a linear map such that $T^3 = T$. Show that $T$ is diagonalizable (elementarily),"While revising for my linear algebra exam I came across this exam problem: Let $T: \mathbb{R}^3 \rightarrow \mathbb{R}^3$ be a linear mapping such that $T^3 = T$ and $T^2 \neq T,T^2 \neq \text{id}_V$ . If $ \text{dim(ker(} T )) = 2$ , show that the matrix of $T$ is equal to diag $(0,0,-1)$ with respect to a suitable basis. My idea Assume for the moment that $T$ is diagonalizable. If $\lambda$ is an eigenvalue of $T$ then $\lambda^3 = \lambda$ since $T^3 = T$ , so $\lambda \in \{ 0,-1,1 \}$ . Since $ \text{dim(ker(} T )) = 2$ , clearly $0$ is an eigenvalue. If all three values above would be eigenvalues then the geometric multiplicity of $0$ would be $1$ and not $2$ , so $-1$ and $1$ can't both be eigenvalues. If $1$ is the other eigenvalue then since $T$ is diagonalizable we'd have that the matrix of $T$ is $M = $ diag $(0,0,1)$ with respect to some basis. But $M^2 = M$ so we'd have $T^2 = T$ which is not the case. So $-1$ must be the other eigenvalue and the result follows. Question How can I prove that $T$ is diagonalizable? For some context, the only definition of diagonalizability we have is that $T: V \rightarrow V$ diagonalizable if there is a basis of $V$ made up of eigenvectors of $V$ and that's how we usually prove diagonalizability (by explicitely providing a basis). We also have the spectral theorem for symmetric matrices but I doubt it helps here. (I also know that the minimal polynomial of $T$ must divide $x(x-1)(x+1)$ via the relation given, so the minimal polynomial is a product of distinct linear factors, so $T$ is definitely diagonalizable. But we haven't done this in the course and this method wouldn't be accepted in the exam without a full proof, so I'm looking for a more elementary way.)","['diagonalization', 'linear-algebra']"
4159209,Proof of Mergesort $N$ elements with $N \log N + O(N)$ comparisons,"In the book ""An introduction to the Analysis of Algorithms"", written by Robert Sedgewick and Philippe Flajolet during the proof of the Theorem 1.1: (Mergesort) To sort an array of N elements, Mergesort uses $N \log N + O(N)$ comparions. It contains the following part: To get an indication for the nature of the solution to this recurrence, we consider the case when N is a power of 2: $C_{2^n} = 2C_{2^n-1} + 2^n$ for $n \geq 1$ with $C_1 = 0$ . Dividing both sides of this equation by $2^n$ , we find that $$
\frac{C_{2^n}}{2^n} = \frac{C_{2^n-1}}{2^{n-1}} + 1 = \frac{C_{2^n-2}}{2^{n-2}} + 2 = \frac{C_{2^n-3}}{2^{n-3}} + 3 = ... = \frac{C_{2^0}}{2^0} + n = n.
$$ I can't understand the trick behind incrementing the right side of the term (+1, then +2, then +3).
Why does the change of the left side lead to the increment on the right side? Any help is kindly appreciated.","['sorting', 'analysis', 'algorithms']"
4159227,Proving the Laplace transform of $t^n e^{at}$ using mathematical induction [duplicate],"This question already has an answer here : Proof of Laplace transform of real number powers: $\mathcal{L} \{ t^n \} = \frac{n!}{s^{n + 1}}$ (1 answer) Closed 3 years ago . Given the problem of finding the Laplace transform of the function $$f(t)=t^ne^{at}$$ with $n\in\mathbb{N}$ and $a\in\mathbb{R}$ , I realize it can be shown the transform is $$\frac{n!}{(s-a)^{n+1}}$$ by more than one direct method. However, I'd like to show this using strong induction. I began the problem by showing that \begin{align*}
\mathcal{L}\{te^{at}\}=\int_{0}^{\infty}te^{-(s-a)t}\,dt&=\frac{1}{(s-a)^2},\\
\mathcal{L}\{t^2e^{at}\}=\int_{0}^{\infty}t^2e^{-(s-a)t}\,dt&=\frac{2}{(s-a)^3},\text{ and}\\
\mathcal{L}\{t^3e^{at}\}=\int_{0}^{\infty}t^3e^{-(s-a)t}\,dt&=\frac{6}{(s-a)^4}.
\end{align*} I originally did this so I could personally see the pattern. Then, I began the proof as follows. $\textbf{Proof:}$ Let $P(n)$ be the statement $$P(n): \mathcal{L}\{t^ne^{at}\}=\frac{n!}{(s-a)^{n+1}},$$ for all $n\in\mathbb{N}.$ We have already shown $P(1), P(2), P(3)$ are true, so the base case has been proven. Inductive Step: Assume $P(k)$ is true for all $k$ . We must then show that $P(k)\implies P(k+1).$ We see \begin{align*}
\mathcal{L}\{t^{k+1}e^{at}\}&=\frac{(k+1)!}{(s-a)^{k+2}}\\
&=\frac{k+1}{s-a}\frac{k!}{(s-a)^{k+1}}\\
&=\frac{k+1}{s-a}\mathcal{L}\{t^{k}e^{at}\} \quad\text{(by the inductive step)}
\end{align*} At this point, I'm not really sure where to go. I'm pretty awful at proofs, so I assume I'm probably not going in the right direction after the inductive hypothesis.","['induction', 'laplace-transform', 'ordinary-differential-equations']"
4159319,The solutions of the equation $|p^3-q^2|=2^n$,"Let's consider the following equation $$|p^3-q^2|=2^n$$ where $p$ , $q$ are distinct primes and $n$ a non-negative integer. The equation admits some trivial solutions: $$|2^3-3^2|=2^0$$ $$|3^3-5^2|=2^1$$ $$|5^3-11^2|=2^2$$ $$|17^3-71^2|=2^7$$ Clearly, not all values of $n$ admit a solution (for example, if $3|n$ the equation has no solution). Are there other solutions besides those mentioned above? Many thanks.","['number-theory', 'elliptic-curves', 'prime-numbers', 'diophantine-equations']"
4159321,Can every polyomino of even size be tiled by $L$-trominoes when scaled up by a factor of $3$?,"The $L$ -tromino does not tile a $3\times 3$ square. However, it can tile $3\times3$ squares glued together in various ways: I am interested in the polyominoes $P$ for which the $L$ -tromino can tile a copy of $P$ scaled up by a factor of $3$ along both axes. It is easy to see that some $P$ will fail, such as any $1\times k$ rectangle for odd $k$ . However, it appears that every polyomino $P$ with an even number of cells can be tiled by $L$ shapes when it is scaled up by $3$ . This holds for all polyominoes $P$ with $2, 4, 6, \ldots, 12$ cells. One might be tempted to argue from induction: given a polyomino $P$ of size $2k$ , subtract a $3\times 6$ domino from $P$ (which can be tiled), and tile the remaining size- $(2k-2)$ polyomino with $L$ s by the inductive hypothesis. The problem with this is that there are arbitrarily large polyominoes for which it is not possible to remove a domino and leave a single connected polyomino behind; in fact, there are polyominoes that cannot be split into any collection of smaller polyominoes of even size. ( This is one example.) A variant of this inductive argument that might work is to show that from any polyomino $P$ , one can remove either a domino, a $T$ -tetromino, or two $L$ trominoes, to leave a smaller connected polyomino. If this were true, it would suffice for the induction, but I don't see a great way to prove it. (It may well have a counterexample!) Can every polyomino with an even number of squares be tiled by L shapes of one-third the side length?","['polyomino', 'geometry', 'tiling']"
4159327,Can someone explain how to find the t-distribution for estimating the difference in means of two normal populations when variance is not known?,See the question referenced. How do I find the t-part? If my confidence level is $95\%$ what do I do with that number? I know what to do to find the inverse distribution in other estimations but this does not make sense to me. It is the last part of the answers section. I have googled and found out this is the t-distribution: $\frac{x-\mu}{s / \sqrt{n}}$ I insert the numbers from the answers but I get the wrong answer. $$\frac{-10}{4.64/ \sqrt{100}}=-21.5517241379$$ This is not close at all to the answer.,"['statistics', 'parameter-estimation', 'maximum-likelihood']"
4159365,Natural inner product on the Hom space.,"Let $P$ and $V$ be vector spaces of dimension $k$ and $n$ , respectively. I want to know if it's possible to endow $\text{Hom}(P,V)$ with a natural inner product, but without using the (non-canonical) isomorphism between $V$ and $V^*$ and then considering something like $\langle{\phi},{\psi}\rangle = \text{tr}(\phi^{\top} \circ \psi)$ . What comes to my mind and what is more suitable to the setup I'm working on is something like this: assume that $P$ and $V$ have inner products $\langle \cdot, \cdot \rangle_P$ and $\langle \cdot, \cdot \rangle_V$ , respectively. Then, we can identify $P \otimes V$ with $\text{Hom}(P,V)$ using the map $p \otimes v \mapsto \langle \cdot, p \rangle_P v$ and we can consider the canonical inner product on $P \otimes V$ given by $\langle p \otimes v, q \otimes w \rangle_{P \otimes V} := \langle p, q \rangle_P \langle v, w \rangle_P$ . But how this translates to the Hom space? I've tried this: choosing basis $\{p_1,\ldots,p_k\}$ and $\{v_1,\ldots,v_n\}$ (and I can't assume those basis are orthogonal) and defining $G$ as the Gram matrix of $V$ , ie, $G = (\langle v_i, v_j \rangle_V)_{i,j}$ , I've identified $\text{Hom}(P,V)$ as the space of matrices $n \times k$ written in those basis and then defined the inner product as $\text{tr}(A^{\top}GB)$ . But this formula is wrong because on $P \otimes V$ we have $\langle{p_i \otimes v_r},{p_j \otimes v_s}\rangle_{P \otimes V} = \langle{p_i},{p_j}\rangle_P \langle{v_r},{v_s}\rangle_V$ , but the inner product $\text{tr}(A^{\top}GB)$ on $\text{Hom}(P,V)$ computed on the corresponding matrices of the linear transformations $A := \langle \cdot,p_i \rangle_P v_r$ and $B := \langle \cdot,p_j \rangle_P v_s$ gives me $$\sum_{\lambda=1}^k \langle p_{\lambda},p_i \rangle_P \langle v_r,v_s \rangle_V \langle
p_{\lambda},p_j \rangle_V$$ and this is not what I expected. Any idea how should I proceed?",['linear-algebra']
4159385,Show that $f(x) = e^x \cos(x)$ on $\mathbb{R}$ is a tempered distribution,"As shown in the title. I know that the anti-derivative of $f(x) = e^x \cos(x)$ is $ \frac{\sin(x)+\cos(x)}{2} e^x$ , whose anti-derivative is $\frac{\sin(x)}{2}e^x$ ...but not sure how to prove it.","['schwartz-space', 'distribution-theory', 'analysis', 'real-analysis']"
4159414,Difficulty understanding a proposition in Fulton and Harris regarding $\mathrm{Sp}_{2n}(\mathbb{C})$,"In Fulton & Harris, Proposition 23.1 (computing the fundamental groups of classical complex Lie groups) and the Exercises after it, specifically in dealing with $\mathrm{Sp}_{2n}(\mathbb{C})$ , the book seems to suggest that the following two submanifolds are diffeomorphic to each other: $$
M = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n}
: x_1^Ty_2 - x_2^Ty_1 = 1 \right\} ,
\\
M' = \left\{ ((x_1,x_2),(y_1,y_2)) \in \mathbb{C}^{2n}\times\mathbb{C}^{2n}
: x_1^Tx_1 + x_2^Tx_2 + y_1^Ty_1 + y_2^Ty_2 = 1 \right\} .
$$ In the book these are written as $$
M = \{ (v,w) \in \mathbb{C}^{2n}\times \mathbb{C}^{2n} : Q(v,w) = 1\},
\\
M' = \{ z \in \mathbb{C}^{4n} : z^T z = 1\} ,
$$ where $Q$ is an alternating non-degenerate quadratic form on $\mathbb{C}^{2n} \times \mathbb{C}^{2n}$ , which I've put in ""standard form"" by picking a symplectic basis. My guess is, $M$ and $M'$ are shown diffeomorphic by showing there is a change-of-basis on $\mathbb{C}^{4n}$ which takes the $M$ expression to the $M'$ one. However, I'm not sure how to find such a change-of-basis; how do I complete the argument here that $M$ and $M'$ are diffeomorphic? Perhaps I'm missing something? Any help would be much appreciated! Edit: as Stephen has pointed out in the comments, $(v,w) \mapsto v^Tv + w^Tw$ is not at all a bilinear form (as I'd incorrectly written earlier).","['representation-theory', 'linear-algebra', 'symplectic-linear-algebra', 'lie-groups']"
4159453,Dense subspace of dual of reflexive Banach space,"Let $X$ be a reflexive Banach space and $Y$ be a subspace of $X^*$ with the property that for all $x\in X$ with $x\neq 0$ there exists an $f\in Y$ such that $f(x) \neq 0$ . Prove that $\overline{Y} = X^*$ . Attempt: Basically we need to prove that if $f \in X^*$ then there exists some $y\in Y$ such that $\lVert f-y\rVert_{X^*} < \varepsilon$ . To do so we write $$\lVert f - y \rVert_{X^*} = \max_{\lVert x^{**} \rVert = 1} |x^{**}(f-y)| = \max_{\lVert x \rVert = 1}|f(x) - y(x)|$$ From there I got stuck. Note that the first equality is a result of Hahn-Banach theorem while the second comes from the fact that $X$ is reflexive. Alternatively, I thought about proving that $(\overline{Y})^\circ \neq \emptyset$ which would imply that $\overline{Y}=X^*$ .","['banach-spaces', 'operator-theory', 'functional-analysis', 'real-analysis']"
4159510,Why do we write the rule for integration by substitution in a way that is different from what we're actually doing?,"The statement of the rule is the following: For a continuous function $f:I\rightarrow\mathbb{R}$ on a real interval $I$ , and a continuously differentiable function $\phi:[a,b]\rightarrow I$ , it holds that \begin{equation}\int_a^bf(\varphi(t))\cdot\varphi'(t)dt=\int_{\varphi(a)}^{\varphi(b)}f(x)dx\tag{1}\end{equation} My question is how one is supposed to actually use this equation, or how to make it intuitively agree with what one is actually doing, mechanically, when integrating by substitution. This has always bugged me, and I give the precise difficulties in my understanding below. I first learned integration by substitution 'procedurally', and was confronted with the actual theorem only later. To demonstrate what I mean by procedurally, consider this example integral, disregarding whether integration by substitution is useful for solving it, as that is not the purpose of my question. $$\int_1^2 t\exp(\frac{t}{2})dt$$ Naively, I could try to do the following: $$x:=\varphi(t)=\frac{t}{2} \implies t=2x$$ $$\frac{dx}{dt}=\frac{1}{2} \implies \text{'}dt=2dx\text{'}$$ and since I 'replace $x$ by $\frac{t}{2}$ ' the limits of integration become $\frac{1}{2}=0.5$ and $\frac{2}{2}=1$ .
Substituting $x$ and $dx$ in my integral accordingly, I arrive at $$\int_{0.5}^1 2x \exp(x) 2dx$$ Now I see that the function $f$ which 'fits' my chosen $\varphi$ 'in hindsight' is $f(x)=4x \exp(x)$ , and indeed $f(\varphi(t))=2t\exp(\frac{t}{2})$ and $\varphi'(t)=\frac{1}{2}$ , which allows me to write my original integral in the form $$\int_1^2 2t\exp(\frac{t}{2})\frac{1}{2}dt$$ which agrees with the left-hand-side of (1) and thus justifies what I did after the fact, since $f$ is continuous and $\varphi$ is continuously differentiable on $[1,2]$ . However, assume I do not know the above procedure, and instead only equation (1). How could I apply it, not yet knowing $f$ ? If I were to just chose $\varphi(t)=\frac{t}{2}$ as above, I would be left with this partially substituted integral: $$\int_1^2t\exp(\varphi(t))dt$$ and proceed how? To go strictly by the formula, I would still need to find $f$ , whose form isn't immediate from the integral (or at least it isn't obvious to me that it would always be reasonably apparent, in particular for more complicated integrals). You might object that I could start on the other side of equation (1), defining $f(x)=x\exp(\frac{x}{2})$ , choose $\varphi(t)=2t$ , and immediately obtain $$f(\varphi(t))\varphi'(t)=2t\exp(t)2$$ However , this leads to a problem with the limits of the integral: The rule for integration by substitution does not require $\varphi$ to have an inverse. And even if it did, then to make the equation agree with what is actually done it would be more sensible to write $$\int_a^b f(x)dx = \int_{\varphi^{-1}(a)}^{\varphi^{-1}(b)}f(\varphi(t))\varphi'(t)dt$$ because this is what I'm actually doing. How to amend one's mental model so equation (1) and the actual procedure agree with each other?","['integration', 'substitution', 'real-analysis']"
4159521,Fundamental Difference between the Real and the Complex,"I am an undergraduate student studying Complex Analysis. Since holomorphic functions have properties that differentiable functions on a real line do not have in general (i.e. Taylors Thoerem, Open Mapping Property, etc.), I come into question about what property of complex plane results in this nice results on holomorphic functions. In my textbook, written by Silverman, most of the properties come from the Taylors Thoerem. Thus my question might converge to, what difference between real line and complex plane makes every holomorphic function has their power series expansion? In my first sight, I thought about the difference on an integral (on real line) and the line integral (on complex plane), but this seems not very fundamental, since real multivariable functions also might not have the power series expansion. I think the difference is related to their algebraic, or topological properties, but it is hard to reveal it for me. It will be glad if someone give me an insight.","['complex-analysis', 'analysis']"
4159545,Necessity of the use of reals in the metric definition,"A metric $d$ is a function $d:X \times X \to \mathbb{R}$ such that $d(x,y)\geq 0$ and equals $0$ iff $x=y$ . $d(x,y)=d(y,x)$ and the triangle inequality holds. From these requirements, the only things that are used is that the codomain has a $0$ , a $+$ operation and a linear order. So, it seems, we could in principle define a metric by a function $d: X \times X \to G$ such the same expressions hold, and where $G$ is an ordered group. My question is, what usual theorems do we lose by picking that definition? In particular, some key questions come to mind If there is a metric on a space $X$ , when allowing for other groups in the codomain, does that imply there is a metric with codomain $\mathbb{R}$ ? That is, does the collection of metrizable spaces expand with the new definition? A kind of converse to the previous one, for any infinite ordered group $G$ , if there is metric with codomain $\mathbb R$ , is there necessarily one with codomain $G$ that generates the same topology? (it being infinite is necessary as the trivial group satisfies all the metric properties but always generates the discrete topology).","['general-topology', 'metric-spaces']"
4159615,Betti number of a complex manifold which is a flat family,"In practice, we wish to know if the topology of the base manifold and that of general fibres can somewhat ""control"" the topology of the total space. Precisely, let $\pi: X\to B$ be a flat morphism between projective complex manifolds where the base space $B$ and fibres are supposed to be connected. By Sard's theorem, one knows that general fibres of $\pi$ are smooth, and diffeomorphic to each other by Ehressmann's theorem. On knowing the Betti numbers of the base space $B$ and those of a general fibre $F$ , does one have something controlling the Betti numbers of the total space $X$ ? P.S. In the case where $\pi: X\to B$ is furthermore supposed to be smooth (submersive), by Ehressmann's theorem, $\pi$ is in fact a topological fibration. So one may utilise things like Leray-Serre's spectral sequence to control the Betti numbers of $X$ . For example, in the case when $B$ is simply-connected, one has $$b_k(X)\leq \sum_{p+q=k}b_p(F)b_q(B),$$ where $b_i$ signifies the Betti numbers.","['complex-geometry', 'algebraic-geometry', 'fibration', 'betti-numbers']"
4159647,For what orders does Cauchy's theorem hold?,"If a prime number $p$ divides the order of a finite group, then there is an element in $G$ of order $p$  this is Cauchy's Theorem. For what numbers $n$ does this hold? In other words: what is the set $C$ of positive integers $n$ such that every group with order divisible by $n$ has an element of order $n$ ? If $n\in C$ and $G$ is a group of order $n$ then clearly $G$ is cyclic: it follows that $n$ is what is called a cyclic number, and this tells us that it is square-free and if $p_1\cdots p_r$ is its factorization into primes we have $p_i\not\equiv1\mod p_j$ for all choices of $i$ and $j$ in $\{1,\dots,n\}$ . On the other hand, it is not difficult to show that $C$ is closed under divisors: if $n\in C$ and $m\mid n$ , then $m\in C$ . Finally, Cauchy's theorem tells us that $C$ contains all primes. Is there anything else in there?","['group-theory', 'finite-groups']"
4159655,Solving $a + \frac1a = 12$,"I recently came across this problem in a textbook and I have no idea how to solve it : $$a + \frac1a = 12$$ Here is what I tried: $$\begin{align}
a + \frac1a &= 12 \tag1\\[4pt]
a^2 + 1 &= 12a \tag2 \\[4pt]
a + 1 &= 3.4641a \tag3 \\[4pt]
1 &= 2.4641a \tag4 \\[4pt]
a &= 0.4058 \tag5
\end{align}$$ Obviously this cant be correct because $$\begin{align}
a + \frac1a &= 12 \tag6\\
0.4058 + \frac{1}{0.4058} &= 12 \tag7\\[4pt]
0.4058 + 2.4642 &= 12 \tag8 \\[4pt]
2.87 &\neq 12 \tag9
\end{align}$$ I assume this is very simple and I am just ignorant. I would appreciate it if someone could explain how this problem and problems like it are solved.",['algebra-precalculus']
4159715,"$\log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k(\mathbf{z} \mid \theta_{0}, \mathbf{x}) d \mathbf{z} $","I am trying to peace tighter the EM algorithm but I am not able to follow certain steps: What I know: Marginal pdf: $$f_{X_{1}}\left(x_{1}\right)=\int_{-\infty}^{\infty} f_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right) d x_{2}$$ for all $x_1$ element of domain of $X_1$ Conditional Distributions: $$p_{X_{2} \mid X_{1}}\left(x_{2} \mid x_{1}\right)=\frac{p_{X_{1}, X_{2}}\left(x_{1}, x_{2}\right)}{p_{X_{1}}\left(x_{1}\right)}$$ MLE: $$L(\theta ; \mathbf{x})=\prod_{i=1}^{n} f\left(x_{i} ; \theta\right), \quad \theta \in \Omega$$ $$l(\theta)=\log L(\theta)=\sum_{i=1}^{n} \log f\left(x_{i} ; \theta\right), \quad \theta \in \Omega$$ $$
k(\mathbf{z} \mid \theta, \mathbf{x})=\frac{h(\mathbf{x}, \mathbf{z} \mid \theta)}{g(\mathbf{x} \mid \theta)}         (6.6.1)
$$ What I am trying to follow: The observed likelihood function is $L(\theta \mid \mathbf{x})=g(\mathbf{x} \mid \theta) .$ The complete likelihood function is defined by $$
L^{c}(\theta \mid \mathbf{x}, \mathbf{z})=h(\mathbf{x}, \mathbf{z} \mid \theta)
$$ Our goal is maximize the likelihood function $L(\theta \mid \mathbf{x})$ by using the complete likelihood $L^{c}(\theta \mid \mathbf{x}, \mathbf{z})$ in this process.
Using (6.6.1), we derive the following basic identity for an arbitrary but fixed $\theta_{0} \in \Omega:$ $$
\begin{aligned}
\log L(\theta \mid \mathbf{x}) &=\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int \log g(\mathbf{x} \mid \theta) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int[\log h(\mathbf{x}, \mathbf{z} \mid \theta)-\log k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=\int \log [h(\mathbf{x}, \mathbf{z} \mid \theta)] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z}-\int \log [k(\mathbf{z} \mid \theta, \mathbf{x})] k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\
&=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right]-E_{\theta_{0}}\left[\log k(\mathbf{Z} \mid \theta, \mathbf{x}) \mid \theta_{0}, \mathbf{x}\right]
\end{aligned}
$$ where the expectations are taken under the conditional pdf $k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right)$ . Define the first term on the right side of $(6.6 .3)$ to be the function $$
Q\left(\theta \mid \theta_{0}, \mathbf{x}\right)=E_{\theta_{0}}\left[\log L^{c}(\theta \mid \mathbf{x}, \mathbf{Z}) \mid \theta_{0}, \mathbf{x}\right]
$$ Q1: from the definition of observed likelihood and complete likelihood above I don't see how this $$\log L(\theta \mid \mathbf{x}) =\int \log L(\theta \mid \mathbf{x}) k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right) d \mathbf{z} \\$$ equations has come about. Q2: If I understand Q1 I might be able to understand why Log is only taken on $g(x|omega)$ And if this is correct, then I do given(6.6.1) understand the steps log(a/b)=log(a)-log(b) so I can follow down to line 4. Q3: I don't understand what happening from line 4 to the last line, what do they mean by: where the expectations are taken under the conditional pdf $k\left(\mathbf{z} \mid \theta_{0}, \mathbf{x}\right)$ . and how k(...) term disappear and we get a capital Z and the conditional part of k is now the condition inside the Expectation.","['statistics', 'probability']"
4159823,How can I find the relative minimum and maximum point as $f(x)$ is equal to zero?,"How can I find the relative minimum and maximum points of $f(x)=x^3-\frac{3}{2}x^2$ . This is what I found so far: \begin{align*}
 f(x)&=x^3-\frac{3}{2}x^2 
\\ f'(x)&= 3x^2-3x 
\\ f''(x)&=6x-3
\end{align*} As the critical points are $0$ and $1$ .","['calculus', 'derivatives']"
4159840,Find a sequence satisfying a specific limit,"Find a positive sequence $\{a_n\}$ , such that $$\lim_{n\to\infty}n\left({1+a_{n+1}\over a_n}-1\right)=1.$$ This problem arises when proving that Given a positive sequence $\{a_n\}$ , the RHS 1 in $$\varlimsup_{n\to\infty}n\left({1+a_{n+1}\over a_n}-1\right)\geq1$$ cannot be replaced by any greater number. So, I think that we can construct such a sequence such that this limit (and thus upper limit) is exactly 1, to prove that we cannot reach a stronger result. The solution directly gives the sequence $a_n=n\ln n$ without any rough work, and I can verify the alleged limit is indeed 1. But I wonder how to come up with such an example?","['limits', 'calculus', 'limsup-and-liminf', 'sequences-and-series']"
