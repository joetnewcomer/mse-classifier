question_id,title,body,tags
2500022,"Do primes, expressed in binary, have more ""random"" bits on average than natural numbers?","In my programming projects I sometimes pick large primes when I want somewhat ""random"" bits, e.g. for hashing or trivial obfuscation via XOR or modular mutiplication. My intuitive sense is that primes in binary are a little more ""random"", but I'm not sure if reality bears that out. So a couple of questions I was curious about: After stripping the leading and trailing $1$'s, is the probability of a $1$ in the remaining binary digits of a prime much different than the $0.5$ that one would assume for $\Bbb N$ at large? Is the ""diffusion"" or distribution of bits after the leading $1$ any more ""random"" in the primes than in $\Bbb N$? I'm not sure the statistical metric to ask about here, so apologies if the question is vague; intuitively I'm wondering if ""randomly distributed"" bits like $10010111$, as opposed to $11110000$, are more common in the primes. If it helps, the question could be confined to common integer representations on computers, e.g. unsigned 64-bit: $0 \le x \lt 2^{64}$.","['binary', 'statistics', 'prime-numbers', 'integers']"
2500035,What's the difference between quasi-concavity and concavity?,What's the intuitive difference between quasi-concavity and concavity? Can you give an example of a quasi-concave function that is not concave?,"['functional-analysis', 'real-analysis', 'functions']"
2500065,"Show that $f+\epsilon g\in{\rm Diff^1}(\Bbb R^m)$ for all $\epsilon\in(-\epsilon_0,\epsilon_0)$","This is an exercise on page 220 of Analysis II of Amann and Escher Here $\rm Diff^k$ means the set of diffeomorphisms where the $k$-th derivative is also an homeomorphism. My work below. The exercise have a hint that Ive not used because I dont see the relation to the exercise. The hint says ""consider the function ${\rm id_{\Bbb R^m}}+f^{-1}\circ(\epsilon g)$ and apply exercise 7 "". We want to show for each case that exists some $\epsilon_0>0$ such that $f+\epsilon g\in{\rm Diff^1}(\Bbb R^m)$ for all $\epsilon\in(-\epsilon_0,\epsilon_0)$. (a) We have that $\exists \alpha>0: |f(x)-f(y)|\ge \alpha |x-y|,\, x,y\in\Bbb R^m$. $\exists\beta>0:|g(x)-g(y)|\le\beta|x-y|,\, x,y\in\Bbb R^m$. If $f+\epsilon g$ is not injective then exists $x,y\in\Bbb R^m$ such that $f(x)-f(y)=-\epsilon(g(x)-g(y))$. Clearly $f+\epsilon g\in C^1(\Bbb R^m)$ for any $\epsilon\in\Bbb R$. From the third statement we have that $|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|$, and from the first two statements we have that
$$
\alpha|x-y|\le|f(x)-f(y)|=|\epsilon|\,|g(x)-g(y)|\le\beta|\epsilon|\,|x-y|\tag1
$$
Hence if we take $\epsilon_0=\alpha/\beta$ then we get a contradiction in $(1)$ so the function is injective. And
$$
|f(x)-f(y)+\epsilon(g(x)-g(y))|\ge\big||f(x)-f(y)|-|\epsilon||g(x)-g(y)|\big|\\\ge\big|\alpha|x-y|-|\epsilon|\beta|x-y|\big|=\big|\alpha-|\epsilon|\beta\big||x-y|\tag2
$$
so $(f+\epsilon g)^{-1}​$ is Lipschitz for each $\epsilon\in(-\alpha/\beta,\alpha/\beta)​$, so it is continuous. From it continuity we find that $f+\epsilon g​$ is surjective because the image of it inverse is $\Bbb R^m​$. It remains to show that $\partial(f+\epsilon g)(x)\in\mathcal L{\rm is}(\Bbb R^m)$ for each $x\in\Bbb R^m$. From $(2)$ we have that
$$
|(f+\epsilon g)(x)-(f+\epsilon g)(y)|\ge K |x-y|,\quad\forall x,y\in\Bbb R^m,\, K:=|\alpha-\beta|\epsilon||>0
$$
Thus, from the definition of directional derivative, it follow that $\partial (f+\epsilon g)(x)h\neq 0$ for all $x,h\in\Bbb R^m\setminus\{0\}$. (b) Without lose of generality suppose that this bounded set, namely $X$, is open and convex. Then $\overline X$ is compact, so its easy to see that $g$ is bounded. Also $\partial g$ is zero outside of $\overline X$ so $\partial g$ is also bounded and, by the MVT, $g$ is Lipschitz. By the same reasons $f^{-1}|_{f(X)}$ is also Lipschitz so we knows that $f+\epsilon g\in{\rm Diff^1}(X,Y)$ for some $Y\subset\Bbb R^m$. It remains to see that $f+\epsilon g$ is bijective in $\Bbb R^m$. To show injectivity we need to show that $f(x)-f(y)\neq\epsilon g(y)$ for all $x\in\Bbb R^m\setminus X$ and all $y\in X$, what is equivalent to show that $f(X)=(f+\epsilon g)(X)$. My questions: Is the part a) correctly done? Im stuck in the part b), Im unable to prove the injectivity of $f+\epsilon g$ in $\Bbb R^m$ knowing that this function is injective, separately, in the domains $\Bbb R^m\setminus X$ and $X$. I need some help here. UPDATE: I think I finally solved it. Observe that 
$$
(f+\epsilon g)(x)=({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(f(x))\tag3
$$ 
I set $h:=g\circ f^{-1}$. Then by the mean value theorem we knows that $$
|h(y)-h(x)|\le\sup_{t\in[0,1]}|\partial h(xt+y(1-t))||x-y|\tag4
$$ and because $\partial h(f(x))=\partial g(x)[\partial f(x)]^{-1}$ is continuous and vanishes if $x\notin X$ then we can conclude that $\partial h$ is bounded, so $h$ is Lipschitz with some Lipschitz constant $K>0$. Thus using $(3)$ we can see that $$
\frac{|x'+\epsilon h(x')-y-\epsilon h(y')|}{|x'-y'|}\ge\left|1-|\epsilon|\frac{|h(x')-h(y')|}{|x'-y'|}\right|\ge |1-|\epsilon|K|\tag5
$$ whenever $|\epsilon| K<1$, and where $x':=f(x)$ and $y':=f(y)$. This shows that for $\epsilon\in(-1/K,1/K)$ we have that $$
|({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(x')-({\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1})(y')|\ge |1-|\epsilon|K||x'-y'|\tag6
$$ From a previous result I know that a function $r\in C^1(\Bbb R^m,\Bbb R^m)$ such that $|r(x)-r(y)|\ge\alpha|x-y|$ for all $x,y\in\Bbb R^m$ and some $\alpha>0$ is a diffeomorphism in the whole $\Bbb R^m$, then ${\rm id}_{\Bbb R^m}+\epsilon g\circ f^{-1}$ is a diffeomorphism, and by $(3)$ we find that $f+\epsilon g$ it is also.$\Box$ Can someone confirm if the above is correct? Thank you.","['multivariable-calculus', 'inverse-function', 'analysis', 'proof-verification']"
2500079,Identity involving pullback in symplectic geometry,"I'm following Ana Cannas' book , trying to prove Proposition 2.5. If you don't want to open the book, here's a sketch of the problem. Let $X_\mu = \{ (x,\mu_x); x \in X, \, \mu_x \in T_x^*X\}$, where $X$ is a manifold. $X_\mu$ defines, thus, a submanifold in the cotangent bundle. The de Rham 1-form $\mu: X \to T^*X$ is said to depend smoothly on $x$. Define $s_\mu: X \to T^*X: x \mapsto (x,\mu_x)$ as ""the 1-form $\mu$ regarded exclusively as a map"", which obviously has $X_\mu$ as its image, and let $\alpha$ be the tautological 1-form on $T^*X$. Proposition 2.5 wants to prove that $s_\mu^* \alpha = \mu$. The proof starts by noticing that one possible definition of the tautological form, $\alpha_p = (d\pi_p)^*\xi$, where $p=(x,\xi)$ and $\pi$ is the projection $\pi:T^*X \to X: (x,\xi) \mapsto x$, says that if $p=(x,\mu_x)$, then $\alpha_p = (d\pi_p)^*\mu_x$ (pretty obvious). She then writes that $(s_\mu^* \alpha)_x = (ds_\mu)_x^* \alpha_p = \dots$. I can't understand how $s_\mu^*$ became $(ds_\mu)^*$. If $s_\mu: X \to T^*X:x \mapsto (x,\mu_x)$, then $s_\mu^*: T^* X \to T^*(T^*X): (x,\gamma) \mapsto (?,?,?)$, and I also can't get what $(ds_\mu)^*$ is/does. I think I'm having problems visualising the actions of all those mappings.","['symplectic-geometry', 'differential-geometry']"
2500089,"Since the Curvature tensor depends on a connection (not metric), is it the relevant quantity to characterize the curvature of Riemannian manifolds?","The definition of the Riemann curvature tensor does not include a metric. So, if we have a smooth manifold(not a Riemannian manifold), we can define the Riemannian curvature tensor for it by just giving it a connection (not the Levi-Civita connection). No metric is needed. Now, if we also assign a metric to the smooth manifold, we can take traces of the Riemann curvature tensor and get the Ricci scalar. Does this imply that, for a Riemannian manifold (not just smooth), the relevant quantity that measures the curvature is the Ricci scalar and not the full Riemannian curvature tensor? Because to define the Riemann curvature tensor we need a connection (not a metric) and to define the Ricci scalar we also need a metric. Lastly, since the Riemann curvature tensor depends on the connection and not the metric and the connection gives the way to parallel transport vectors, does it mean that parallel transporting the same vector along the same closed curve on two different Riemannian manifolds that correspond to the same smooth manifold (but we assign the same connection but different metric to each one), we will get the same angle of rotation for that vector at the end-point (which is also the starting-point) of the curve? Note: When talking about a connection, I do not mean the Levi-Civita connection which comes from a metric. The question is about the difference in the role that the connection and metric play (independently) in the Riemann curvature tensor.","['riemannian-geometry', 'smooth-manifolds', 'curvature', 'affine-geometry', 'differential-geometry']"
2500094,"Showing $a^n + b^n \ge (a+b)^n$ for $a,b > 0$ and $0 \le n \le 1$ [duplicate]","This question already has answers here : Prove $(|x| + |y|)^p \le |x|^p + |y|^p$ for $x,y \in \mathbb R$ and $p \in (0,1]$. (2 answers) Closed 6 years ago . Initially I had to show that for $0\le n \le 1$ and $a,b > 0$: $$a^n + b^n \ge (a+b)^n$$ I made $u(x) = a^x + b^x - (a+b)^x$ $$u'(x) = a^x \log a + b^x \log b - (a+b)^x \log (a+b)$$ Now I get stuck. Cannot find where to go. please help me.","['algebra-precalculus', 'inequality', 'exponential-function', 'calculus']"
2500099,Prove that $1^2 + 2^2 + \cdots + n^2 = \frac{n(n+1)(2n+1)}{6}$ without using induction.,"I have to deduce the following formula $$1^2 + 2^2 + \cdots + n^2 = \frac{n(n+1)(2n+1)}{6},$$ while using the given formula $$\binom{k}{0}+\binom{k+1}{1}+\cdots+\binom{k+r}{r}=\binom{k+r+1}{r}$$ I tried to find values for $k$, such that $\binom{k}{0}=1^2$ etc. but that didn't work. Does anybody have a push in the right direction? Thanks!","['binomial-coefficients', 'discrete-mathematics']"
2500171,Does L-Hopital's rule fail for $\lim _{x \to \infty} \frac{x+\sin x}{x+2 \sin x}$?,For $$\lim _{x \to \infty} \frac{x+\sin x}{x+2 \sin x}$$ if I solve it by dividing by $x$ and I get the correct answer which is $1$ but when I apply L-Hopital rule for $\infty /\infty$ form then I get answer as limit does not exist because we cannot say anything about $$\lim _{x \to \infty} \frac{1+\cos x}{1+2 \cos x}$$ Why does L-Hopital rule fail for this case? Or is something incorrect in my approach?,"['calculus', 'limits']"
2500208,Is the essential derivative of a convex function continuous?,"Suppose that $f:\mathbb{R}\rightarrow\mathbb{R}$ is convex on $\mathbb{R}$, but not necessarily differentiable everywhere. We know, though, that such a function will be differentiable almost everywhere, and that the set of points where $f$ is nondifferentiable, call this $\cal{A}$, is at most countable, and of Lebesgue measure zero. Then, the derivative of $f$, call it $f'$, exists on $\mathbb{R}\setminus\cal{A}$. Question #1: Is $f'$ continuous on $\mathbb{R}\setminus\cal{A}$? Question #2: Does the result change if $f$ is nondecreasing? I ask these questions because we know that a convex, differentiable function on $\mathbb{R}$ has a continuous derivative; I am just wondering if this proposition can be extended as described above. Thanks!","['real-analysis', 'convex-optimization', 'measure-theory', 'convex-analysis']"
2500222,Is it possible to win $2048$ in a $3 \times 3$ grid?,"I was playing the famous $2048$ game with the non-standard $3 \times 3$ grid. I quickly realized it was awfully difficult to even get up to $256$, with the best possible algorithm(of course in my head, not using a computer). So my question is - Is it possible to win $2048$ in $3 \times 3$ grid? If yes, can it be shown mathematically?","['induction', 'puzzle', 'combinatorics', 'recreational-mathematics', 'discrete-mathematics']"
2500226,A finite collection of natural numbers has a largest element.,"Let $A$ be a nonempty finite collection of natural numbers. I want to prove that $A$ has a largest element, that is $A$ contains a number $m'$ such that $m'\ge a$ for every element $a$ in $A$. I defined the set $$M:=\{n\in\mathbb N:n+1>a,\forall a\in A\}$$ so by Well-ordering principle of $\mathbb N$, there is $m'\in M$ such that $m'\le m$ for all $m\in M$ which implies that $m'+1>a$ or $m'\ge a$ for all $a\in A$. Now while it seems easy that $m'\in A$, I can't show that. Can you help me, please?","['elementary-set-theory', 'real-analysis', 'analysis', 'elementary-number-theory']"
2500229,Compactness argument for type I singularities of the mean curvature flow,"My question is about the compactness argument to find a limiting self-similar solution to the mean curvature flow near a singularity. Consider a mean curvature flow $(M_t)_{t \in I}$ of $n$-dimensional immersed manifolds in $\mathbb{R}^{n+1}$ developing a type I singulartiy at the oringin at time $T$. We can rescale these surfaces as follows
$$ \tilde{F}(p,s) = \frac{1}{\sqrt{2(T-t)}} F(p,t), \: \: \: s(t) = -\frac{1}{2} \text{ln}(T-t). $$
It is well known that all the covariant derivatives of the second fundamental form of $\tilde{F}$ are bounded for $s \in [-\frac12 \text{ln}(T), \infty)$. Then I would like to ask why Huisken, in his famous paper about singularities, used an unknown (at least to me) compactness theorem to show that $\tilde{F}$ tends, locally, to a smooth immersion for some subsequence $(s_j)$. Since to me, it doesn't seem to hard to show the boundedness of the Christoffel symbols and of the metric for $\tilde{F}$ for all points $p$ in a compact set where $\tilde{F}(p,s)$ remains bounded for $s \rightarrow \infty$. Then one can use the well-known Ascoli-Arzela theorem together with a diagonal subsequence argument to find such a smooth limiting surface. Edit: While thinking about this I found that proving the boundedness of the Christoffel symbols could in fact be not so easy. This is since the symbols of the original imersion evolves as $$ \frac{\partial \Gamma_{ij}^k}{\partial t} = A \ast \nabla A, $$
where the $\ast$ denotes a metric contraction of the two tensors. Also we know that $A$ blows up somewhere near a singularity and together with the fact that the christoffel symbols of the rescaled immersion are the same ones as the original immersion we may indeed get stuck. But still I'de like to have some explanation or confirmation why Huisken didn't use the Ascoli-Arzela compactness theorem.","['riemannian-geometry', 'mean-curvature-flows', 'partial-differential-equations', 'parabolic-pde', 'differential-geometry']"
2500256,"If a function $f$ is analytic in an open set $U$, then $\int_{\partial T}f(z)dz=0$ for every closed triangle $T$ in $U$","If a function $f$ is analytic in an open set $U$, then $\int_{\partial T}f(z)dz=0$ for every closed triangle $T$ in $U$. I already have the result for a rectangle but how can I prove that for a triangle you also have this? What other geometric figures do you have this result for? Could anyone help me, please? Thank you very much.","['complex-analysis', 'integration', 'analysis']"
2500282,Expected value and standard deviation of $\sqrt{X}$,"Let X be a poisson-distributed stochastic variable, where 
$$E(X) = m$$ 
$$ V(X) = m.$$ 
Let $Y=\sqrt{X}.$ Calculate (approximatively) $$E(Y),$$ $$V(Y).$$ Now, the answer is 
$$E(Y) \approx \sqrt{m},$$
$$V(Y) \approx \left( \frac{1}{2\sqrt{m}} \right)^2$$ I've looked in my textbook but I don't know where this is coming from. Why is this the case? What (simple) theorem can I use to verify that it's true?.","['probability-theory', 'probability', 'expectation', 'statistics']"
2500367,Non-metric topological continua,"What important results hold for non-metric continua, or where can I find a survey of such results? There are three definitions of a continuum around: a non-empty topological space that is (1) connected compact metric, or (2) connected compact Hausdorff [e.g., General Topology by Willard], or (3) connected compact [ ProofWiki ]. I am interested in non-trivial properties commonly known for definition (1) that have been found to also hold for definitions (2) or even (3).","['general-topology', 'metric-spaces', 'continuum-theory']"
2500371,"Function with extrema 1, 1/4, 1/9, ...","I am looking for a function $F(x)$ which has an infinite number of extrema, and whose values at those extrema are of the form $1/n^2$ for $n \in \mathbb{Z}$. To clarify, I don't want a function whose extrema are at $x=1,1/4,1/9,\ldots$. Instead, I want the function itself to take these values at the extrema. Edit: To clarify, there shouldn't be multiple extrema with value F(x)=0! Thanks for your help!","['special-functions', 'functions']"
2500396,Why do we need an upper bound for $|x+5|$ when $x$ is close to $3$?,"Show that $\lim_{x\to 3}$ $x^2 + 2x + 6 = 21$ using the $\epsilon-\delta$ definition. The following is part of the proof to the above statement. proof . We have to show that given any $\epsilon>0$, there exists some $\delta>0$ such that $|f(x)-21|<\epsilon$ whenever $0<|x-3|<\delta$. Working backwards: $|f(x)-21|=|x^2+2x-15|=|(x+5)(x-3)|=|x+5||x-3|<|x+5|\delta$, if $|x-3|<\delta.$ We need an upper bound for $|x+5|$ when $x$ is close to $3$. If $|x-3|<1$, then $2<x<4$, so that $7<x+5<9$, so $|f(x)-21|=|x+5||x-3|<9|x-3|<9\delta$. My question is why do we need an upper bound for $|x+5|$ when $x$ is close to $3$?","['continuity', 'analysis']"
2500459,Generalization of the dominated convergence theorem,"There exist theorems, such as dominated convergence theorem, monotone convergence theorem, for a sequence of integrals in which both integrand and measure change in $n$? In other words, there is a theory about the convergence of sequences of integrals like $\int f_n d\mu_n\xrightarrow{n\to+\infty}$ If yes, could someone give me some reference?","['real-analysis', 'integration', 'measure-theory', 'convergence-divergence']"
2500480,The longest list of analogies between vector spaces and categories ever made,"I suspect this question exists in different forms, elsewhere. I would like to know what's going on with this table, how to fill the missing items and how to continue the list, and what is the analogy that underlies it. ╔═══════════════════════╦══════════════════════╗
║ vector spaces         ║ categories           ║
╠═══════════════════════╬══════════════════════╣
║ tensor product        ║ product              ║
╠═══════════════════════╬══════════════════════╣
║ linear map            ║ functor              ║
╠═══════════════════════╬══════════════════════╣
║ dual space            ║ opposite category    ║
╠═══════════════════════╬══════════════════════╣
║ canonical pairing     ║ hom functor          ║
╠═══════════════════════╬══════════════════════╣
║ ground field          ║ category of sets     ║
╠═══════════════════════╬══════════════════════╣
║ bidual injection      ║ Yoneda embedding     ║
╠═══════════════════════╬══════════════════════╣
║ ev(v) -> f = f(v)     ║ Yoneda lemma         ║
╠═══════════════════════╬══════════════════════╣
║ V~V** in finite dim   ║  ???                 ║
╠═══════════════════════╬══════════════════════╣
║ bilinear map          ║ profunctor           ║
╠═══════════════════════╬══════════════════════╣
║ ???                   ║ co/complete category ║
╠═══════════════════════╬══════════════════════╣
║ linear representation ║ ???                  ║
╠═══════════════════════╬══════════════════════╣
║ ???                   ║ adjoint functors     ║
╠═══════════════════════╬══════════════════════╣
║ ???                   ║ Kan extensions       ║
╠═══════════════════════╬══════════════════════╣
║ ???                   ║        coend         ║
╚═══════════════════════╩══════════════════════╝","['category-theory', 'linear-algebra']"
2500481,Showing Green's function on a Riemann surface can be pulled back,"I want to prove the following: Let $R,S$ be two Riemann surfaces, and suppose Green's function $g_S$ exists for $S$. Let $f: R \to S$ be a nonconstant analytic function. Prove that Green's function $g_R$ exists for $R$, and $g_R(p,q) \leq g_S(f(p),f(q))$ for all $p,q \in R$. This is an exercise in Gamelin's book, XVI.3.2. The exercise does not forbid $f$ being constant, but it seems evidently necessary. Some thoughts Let $(U, z)$ be a coordinate patch of $R$ containing $q$ with $z(q) = 0$, let $v$ be a subharmonic function in $R \setminus\{q\}$ with compact support such that $$\limsup_{x \to q} v(x)+\log |z(x)|< \infty,$$ then by definition $g_R(\cdot,q)$ is the supremum of all such $v$, so it is enough to prove that $v(p) \leq g_S(f(p), f(q))$. Letting $(V,w)$ be a coordinate chart around $f(q)$ in $S$ with $w(f(q)) = 0$, we know that 
$$\lim \limits_{x \to q} g_S(f(x),f(q))+\log |w(f(x))|$$
 exists, as the function is even harmonic there. Subtracting, we get that 
$$\limsup \limits_{x \to q} \;v(x)- g_S(f(x),f(q))+\log \lvert \frac{z(x)}{w(f(x))}\rvert < \infty.$$ If $w \circ f$ is a coordinate function, which is not always true, then  $\limsup _{x \to q} \;v(x)- g_S(f(x),f(q)) < \infty$ so for every $\epsilon>0$, the function $\varphi_\epsilon(x) := v(x)- (1+\epsilon)g_S(f(x),f(q))$ extends to be subharmonic at all $R$. It is $\leq 0$ off a compact set, the support of $v$, so the maximum principle forces $\varphi_\epsilon \leq 0$. Letting $\epsilon \to 0$ we are done. This question is a slightly more quantitative version of the fact, of which I do not know a proof, that an analytic map from a parabolic Riemann surface to a hyperbolic Riemann surface is constant. The main tool in proofs on this topic seems to be the maximum principle; how can it be leveraged here without adding an assumption on $f$?","['riemann-surfaces', 'maximum-principle', 'complex-analysis', 'greens-function', 'analysis']"
2500512,Is a principal submatrix of a diagonalizable matrix diagonalizable?,"Let $A$ be diagonalizable, i.e., $A=X \Lambda X^{-1}$ for some diagonal matrix $\Lambda$. Consider $B$ which is a principal submatrix of $A$. Does there exist an invertible matrix $Y$ and a diagonal matrix $D$ such that 
$B=Y D Y^{-1}$ ? $D$ and $\Lambda$ can be related through the interlacing property. Can $X$ and $Y$ also be related to each other. Specifically, if $X$ has small condition number, does $Y$ also have a small condition number? Any thoughts/pointers are appreciated!","['matrices', 'diagonalization', 'matrix-decomposition', 'linear-algebra']"
2500534,Solving a first order differential equation in terms of Lambert W-function Continued:,"I have had some excellent answers in response to a previous question here: Solving a first order differential equation in terms of Lambert W-function However I would like to show my work some more since I am still stuck, and terribly lost, in hopes for some further guidance. Thank you all. So I was left with an equation of the following form: $$ t + K = \frac{bx^2}{2a} + \frac{c\ln(x)}{a}   $$ Multiplying through by $$ \frac{2a}{c}$$
Gives $$ \frac{2a(t + K)}{c} = \frac{b}{c} x^2 + 2\ln(x)   $$ Following by: $$ exp(\frac{2a(t + K)}{c}) = x^2 exp(bx^2/c)   $$ It is at this point here I am stuck. I actually am not 100 percent sure the final step I made is valid. However I belive here that the function is now in the form:  $$ y = x exp(x) $$ Which is the form needed to use the Lambert function $$ W(y) = x $$ Any help is appreciated. Thanks all.","['lambert-w', 'ordinary-differential-equations']"
2500552,Proving a sequence converges uniformly,"The problem is: Let $f ∈ C(\mathbb R)$. Prove that the sequence $\{f_n\}^∞_{n=1}$ defined by $f_n(x)=\frac{1}{n}\sum^{n-1}_{k=0}f(x+\frac{k}{n})$ converges uniformly on each finite interval $[a, b]$. How would you prove this? I'm not that good at analysis so any hints or advice would be great. I was thinking maybe comparing sequences and using triangle inequality? However, the summation is really throwing me off.","['real-analysis', 'uniform-convergence', 'sequences-and-series', 'convergence-divergence', 'analysis']"
2500596,Is left shift operator compact?,"Let $T:l^2 \to l^2$ be $T(a_1,a_2,...)=(a_2,a_3,...)$. Is this linear operator compact. If yes, how to prove it? If no, please give an example. I want to show that if $(a_2,a_3,...)$ has a cluster point or not. I think it is sufficient to show that if $(a_1,a_2,...)\in l^2$ , does it have a cluster point?","['hilbert-spaces', 'linear-algebra', 'linear-transformations']"
2500597,Subgroup of $D_n$ isomorphic to $Q_8$.,"Is there an $n$ such that $D_n$ contains a subgroup isomorphic to $Q_8$ ? My immediate thought is no, but I'm not sure how to prove it. I know that there are only $2$ non-Abelian groups of order $8$ (up to isomorphism): $D_4$ and $Q_8$ . I feel like the answer should fall out from here but I'm stuck.","['finite-groups', 'abstract-algebra', 'group-isomorphism', 'dihedral-groups', 'group-theory']"
2500640,Derivative of an expansion of a function in spherical harmonics,"Recall that, for fixed $k \in Z_+\cup \{0\}$, a spherical harmonic of degree $k$ is the restriction to $S^{n-1}$ of a harmonic polynomial on $R^n$ that is homogeneous of degree $k$. Let $Y_{k,1}\, ,Y_{k,2}\,\ldots, Y_{k,d_{n-1}(k)}$, with $\displaystyle{d_{n-1}(k):={n+k-1\choose k}}$, be an orthonormal basis of the subspace in $L^{2}(S^{n-1})$ of degree $k$ of spherical harmonics. Then, $\{Y_{k,m}\}$ is an orthonormal basis for $L^2(S^{n-1})$, i.e. for any $g\in L^2(S^{n-1})$, one has:
$$
g=\sum_{k,m}g_{k,m}Y_{k,m},
$$
with
$$
g_{k,m}:=\int_{S^{n-1}}g(\omega)Y_{k,m}(\omega) d\omega, \quad k=0,1, \ldots; m=1,2, \ldots, d_{n-1}(k),
$$
This so-called spherical harmonic expansion of $g$ converging to it in $L^2(S^{n-1})$. QUESTION: How to find high order derivative $g^{(\ell)}$.","['derivatives', 'harmonic-analysis', 'partial-derivative', 'spherical-geometry', 'spherical-harmonics']"
2500685,Vector Fields given a set of differential equations,"I'm having some trouble figuring out how to draw (by hand) the vector field given a set of differential equations. Consider the following: $\frac{dx}{dt} = x+y$ $\frac{dy}{dt} = -x + y$ Normally, when I am given just one differential equation, like $\frac{dy}{dt} = y$, I can easily compute the values by hand and can plot this out - think of this as picking coordinates of $(t,y)$. Would this approach be the same for this given system of differential equations? I'm more interested in the process, but a graph of how this should look like would be greatly appreciated as well.",['ordinary-differential-equations']
2500742,Fourier series of $f(x) = \frac{1-a^2}{1+a^2-2a \cos x}$,"Consider the following function $f(x) = \frac{1-a^2}{1+a^2-2a \cos x}$. I'm looking for its represention of this kind:
$\sum_{\mathbb{Z}} a_k e^{ikx}$ So we can rewrite function as $f(x) = \frac{1}{1+a^2}\frac{1-a^2}{1-\frac{2a \cos x}{1+a^2}}$ And consider it is as a geometric sequence, with $q=\frac{2a \cos x}{1+a^2} = \frac{a (e^{ix}+e^{-ix})}{1+a^2}$ So the series probally look like:
$\sum_{\mathbb{Z}} \left(\frac{a}{1+a^2}\right)^{|k|} e^{ikx}$.
Is it correct? Does it work for complex a?","['fourier-series', 'analysis']"
2500778,Listing the elements of a covering for a closed disk by open disks,"This problem is somewhat related to some homework I had recently. However, as stated, I don't know if a solution yet exists. I asked some friends and some of my professors, but none of them know how to make significant progress on this problem either, so I figured I'd come here. Any help is appreciated. The Problem : Let $\mathcal{U}$ be a minimal finite covering of the closed unit disk in $\mathbb{R}^2$ by open disks. (""Minimal"" in the sense that if we removed a ball from $\mathcal{U}$, then it would no longer be a covering.) Prove or disprove that we can index $\mathcal{U}$ as $\mathcal{U}=\{U_i\}_{i\in\{1,2,\ldots n\}}$ so that $U_i\cap U_{i+1}\neq\varnothing$ for all $i\in\{1,2,\ldots n-1\}$. As an example, see the covering below and the arrows which indicate a way to list them in the desired fashion: Some observations : This isn't true for minimal finite coverings of all sets, or even of convex sets. For example, a triangle can be covered by four open balls so that they can't be listed as desired: Given a $\mathcal{U}$, we can construct a graph $G$ from it by letting disks in the covering correspond to vertices in $G$ and saying that two vertices are adjacent if their corresponding disks intersect. We call $G$ the graph induced by $\mathcal{U}$. In this form, the given problem is equivalent to finding a Hamilton path in the graph induced by the covering. It's worth noting that the graph induced by $\mathcal{U}$ might not be planar. In fact, given any $n\in\mathbb{N}$, we can construct coverings whose induced graphs have $K_n$ as a subgraph. If $\mathcal{U}$ has at least 3 disks, its induced graph is non-separable. Furthermore, every vertex in its induced graph is included in some 3-cycle.","['circles', 'graph-theory', 'combinatorial-geometry', 'hamiltonian-path', 'geometry']"
2500784,Proving $\tan\left( \frac {a+b}{2}\right) = \frac{\sin a+\sin b} {\cos a + \cos b } $,"Prove $$\tan\left( \frac {a+b}{2}\right) = \frac{\sin a+\sin b} {\cos a + \cos b } $$ Can someone help? I separated $\tan$ and did double-angle, but I just went into a circle and couldn't get the trig functions without the halves.","['proof-writing', 'trigonometry']"
2500809,Probability of picking a the first $n$ digits from a number in the Vitali set,"Consider a Vitali set , $V$, that only contains numbers with a unique decimal notation. We define $P_k(V)$ as the probability that, when picking a random sequence of $k$ digits, there is a $v \in V$ whose decimal notation begins with those $k$ digits. Since their are only finitely many $k$ digit sequences, this is well defined for any $k \ge 0$. Note that $\lim_{k \to \infty} P_k(S)$ is equal to the measure of $S$ if $S$ is measurable, I think. My question is, what is the behavior of the function $f(k)=P_k(V)$. Also, does this behavior change is use a base other than base ten?","['asymptotics', 'probability', 'measure-theory']"
2500831,General solution of $\dot{x} = A(t) x$,"This is a very basic question, but I'm having trouble solving this. Let $A(t): \mathbb{R} \rightarrow \mathcal{L}(\mathbb{R}^n,\mathbb{R}^n)$ be a $\mathcal{C}^{\infty}$ function, and consider the linear diferential equation $$ \dot{x} = A(t) x$$
$$x(0) = x_0,$$ where $\mathcal{L}(\mathbb{R}^n,\mathbb{R}^n)$ are the linear transformations of $\mathbb{R}^n$ to $\mathbb{R}^n.$ I'm wondering if $ \exp \left (\int_ {0}^{t} A (s) ds \right) x_0 $ is a solution of the above differential equation. As far as I know this is only valid when $A(t) = A_0$, and I could not find any book that says this result is true. NB: If we pretend that life is beautiful and things work as we would like
\begin{align*}
\frac{d}{dt}\left(\exp \left (\int_ {0}^{t} A (s) ds \right) x_0 \right) &= \frac{d}{dt}\left(\int_{0}^{t} A(s) ds\right) \left(\exp \left(\int_ {0}^{t} A (s) ds \right)\right) x_0\\
&= A(t) \left(\exp \left(\int_ {0}^{t} A (s) ds \right)\right) x_0.
\end{align*} But I am not very sure of such manipulations.","['ordinary-differential-equations', 'dynamical-systems']"
2500870,Proving a function of the empirical distribution is a Martingale,"Let $X_1, \dots, X_n$ be a sequence of i.i.d. random variables with distribution function $G$, let $$ G_t = \frac{\# \{ k : X_k \leq t \}}{n} $$ define the empirical distribution relative to the random variables. Set $A_t  = \sqrt{n} (G_t - G(t))$, $$ M_t = \frac{A_t}{1 - G(t)}\ \ \ \ \ B_t = A_t + \int_{-\infty}^t M_s\ dG(s)\ \ \ \ \ V_t = B_t^2 - G_t $$ It is rather simpler to verify that $M_t$ and $B_t$ are martingales with respect to the filtration $\Sigma_t = \sigma(G_s: s \leq t)$, but however I try to compute that $V_t$ is a martingale, I end up with an incredibly difficult calculation. Is there any easy way to see that $V_t$ is a martingale?","['stochastic-processes', 'empirical-processes', 'statistics', 'martingales']"
2500881,Nearest semi-orthogonal matrix using the entry-wise $ {\ell}_{1} $ norm,"Given an $m \times n$ matrix $M$ ( $m \geq n$ ), the nearest semi-orthogonal matrix problem in $m \times n$ matrix $R$ is $$\begin{array}{ll} \text{minimize} & \| M - R \|_F\\ \text{subject to} & R^T R = I_n\end{array}$$ A solution can be found by using Lagrangian or polar decomposition , and is known to be $$\hat{R} := M(M^TM)^{-1/2}$$ If $\|\cdot\|_F$ is replaced by the entry-wise $1$ -norm $$\|A\|_1 := \|\operatorname{vec}(A)\|_1 = \sum_{i,j} |A_{i,j}|$$ the problem becomes $$\boxed{\begin{array}{ll} \text{minimize} & \| M - R \|_1\\ \text{subject to} & R^T R = I_n\end{array}}$$ What do we know about the solutions in this case? Is $\hat{R}$ still a solution? If the solution is something else, do analytic forms or approximations exist? Any insight or direction to literature is appreciated.","['optimization', 'matrices', 'orthogonal-matrices', 'stiefel-manifolds', 'non-convex-optimization']"
2500910,Proving the statement $A\setminus(A\setminus B)=B\setminus(B\setminus A)$,"I'm trying to prove/disprove the following statement: $$A \setminus (A \setminus B) = B \setminus (B \setminus A)$$ From what I gather, it is true, since a simple venn diagram check suggests the two are the same. 
However, I am stuck on how to prove it. Note : I am also given the following equalities to use in my proof: $$A \setminus B = A \cap B^\complement$$
$$(A \setminus B)^\complement = A^\complement \cup B$$ So far, I have tried converting the first part of the original statement using the given equalities into the following: $$ A \cap (A^\complement \cup B) $$ but I have no idea if I'm on the right path. I can't seem to find out how to show that the two are subsets of one another. Any help would be greatly appreciated!","['proof-writing', 'elementary-set-theory', 'discrete-mathematics']"
2500914,Feller continuous Markov kernels on a compact metric space has an invariant distribution?,"I am trying to prove that a Feller continuous Markov kernel on a compact metric space has an invariant distribution. Let $S\ $be a compact metric space. Let $\mathcal{B}(S)$ be the Borel $\sigma $-algebra on $S$.  A Markov kernel $P$ on the measurable space $\left (S ,\mathcal{B}(S)\right )$ is a function  $P :S \times \mathcal{B}(S) \rightarrow [0 ,1]$ such that for all $s \in S$, $P\left (s , \cdot \right )$ is a probability measure on $S$ and for each $B \in \mathcal{B}(S)$, $P\left ( \cdot  ,B\right )$ is a $\mathcal{B}(S)$-measurable function. The Markov kernel $P$ is Feller continuous if for any bounded, continuous function $f :S \rightarrow \mathbb{R}$, the function $\int _{S}f\left (s^{ \prime }\right )P\left (s ,ds^{ \prime }\right )$ is continuous. Define the operator $T :\mathcal{P}(S) \rightarrow \mathcal{P}(S)$ where $\mathcal{P}(S)$ is the set of all probability measures on $S$ by  \begin{equation}T\lambda (B) =\int _{S}P(s ,B)\lambda (ds) .
\end{equation}Say that $\mu $ is an invariant distribution if $T\mu  =\mu $. Theorem: There exist an invariant distribution. Proof: Since $S$ is compact $\mathcal{P}(S)$ is compact in the weak topology. $\mathcal{P}(S)$ is clearly convex. Furthermore, if $\lambda _{n} \rightarrow \lambda $ (weakly) then for any continuous and bounded function $f$, $\int f(s)T\lambda _{n}(ds) =\int f(s^{ \prime })P(s ,ds^{ \prime })\lambda _{n}(ds) \rightarrow \int f(s^{ \prime })P(s ,ds)\lambda (ds) =\int f(s)T\lambda (ds)$ since $\int _{S}f\left (s^{ \prime }\right )P\left (s ,ds^{ \prime }\right )$is continuous and $\lambda _{n} \rightarrow \lambda $. So $T$ is continuous. Schauder fixed-point theorem: https://en.wikipedia.org/wiki/Schauder_fixed-point_theorem imply that $T$ has a fixed point. Is my proof correct? Do you know about other proofs / counter example?","['probability-limit-theorems', 'probability-theory', 'alternative-proof', 'proof-verification']"
2500933,Causal inference calculus (Bayesian Probability),"Here is my problem : There is a causal Markovian model as follows. By the definition of interventional probability , since $\text{do}(x)$ makes no edges between $X$ and $Z_1, Z_2$ , we have $$
 P(y\mid \text{do}(x)) =\sum_{z_1,z_2,z_3}P(z_1)P(z_2)P(z_3\mid z_1, z_2)P(y \mid z_2, z_3, x),
> $$ where $\text{do}(\cdot)$ is so-called do-calculus (Judea pearl, 2010). Then show that the summation over $z_2$ is $$
 P(y\mid \text{do}(x)) =\sum_{z_1, z_3} P(z_1)  P(z_3\mid z_1)P(y \mid z_1, z_3, x).
$$ Here is what is tried . To hold the equality of the above problem, we must have $$
  \sum_{z_2} P(z_3 \mid z_1,z_2)P(y,z_2,z_3,x) 
$$ should be same with $$
  P(y,z_1,z_3,x),
$$ but I don't know how this can happen. Thanks, Reference: Judea Pearl, 2010, An Introduction to Causal Inference, The International Journal of Biostatistics, pp15-16","['causality', 'bayesian', 'statistics', 'probability', 'causal-diagrams']"
2500951,Standard criterion for essential self-adjointness,"Suppose $T:H\rightarrow H$ is a symmetric operator on a Hilbert space $H$. I want to show that $\text{Ran}(T\pm i)$ are dense in $H$ implies that $T$ is essentially self-adjoint, that is the closure of $T$ is self-adjoint. I already know that if $\text{Ran}(T\pm i)=H$ then $T$ is self-adjoint, and I feel that the above should follow from this in a simple way, but I don’t quite see it. Thanks.","['functional-analysis', 'operator-theory', 'hilbert-spaces']"
2500980,Induction on n allowed?,To prove that ${{n}\choose{r_1}}{{n-r_1}\choose{r_2}}{{n-r_1-r_2}\choose{r_3}}...{{n-r_1-r_2-...-r_{m-1}}\choose{r_m}}=\frac{n!}{r_1!r_2!...r_m!}$ where $r_1+r_2+...r_m=n$. I have proved this using induction on $n$. But my professor says that induction can not be done by $n$ because there is some problem for well-defined-ness of which $m$ is taken. Can someone explain why the problem occurs?,"['combinatorics', 'induction']"
2501002,A problem in integral,"Let $f: [0,1]\to \mathbb R$,  $f \in C^2[0,1]$ with $f(0) = f(1) = 0$. For all $x\in (0,1)$, $f(x) > 0$. Indicate with $M = \max \limits_{x\in [0,1]} f(x)$. Prove that $\forall a,b \in [0,1]$,  $a<b$, $$
\int_a^b \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq \frac{1}{M} \vert f'(b) - f'(a)\vert.
$$ Prove that
$$
\int_0^1 \left| \frac{f''(x)}{f(x)} \right| \mathrm{d}x \geq 4.
$$ The first half of the problem is easy to solve with Newton-Leibniz formula and basic properties of Riemann integral. I wonder the proof of 2. Thanks in advance!","['integration', 'calculus', 'analysis']"
2501010,"Show that there exists $x \in H$ with $\|x\|=1$ and $|\langle Tx,x\rangle |=\|T\|$","Let $H$ be a Hilbert space and let $T:H\to H$ be a bounded self-adjoint linear operator. Show that there exists $x \in H$ with $\|x\|=1$ and $|\langle Tx,x\rangle |=\|T\|$. I know that $\|T\|=\sup\{|\langle Tx,x\rangle| : \|x\|=1\}$. I think the completeness can produce such $x$, but I don't know how to prove this.","['general-topology', 'linear-algebra', 'hilbert-spaces']"
2501022,Is it always okay to drop higher order terms out in Taylor approximation?,"In many books that one comes across with regarding statistics, function approximation, optimization etc. it is almost certain that you run into Taylor expansion, be it multidimensional or one-dimensional : $$f(x)=f(a)+\frac{f'(a)}{1!}(x-a)+\frac{f''(a)}{2!}(x-a)^2+\frac{f'''(a)}{3!}(x-
a)^3+\cdots,$$ and almost always one can see the authors drop out the higher order terms in the Taylor approximations, i.e. terms with derivatives $\geq 3$. I came to realize that I've never seen a well justified explanation on why the terms with derivatives $\geq 3$ are always dropped out. My question is: What is the justification for this? Why the magic number of $3$? Why not $4$? or $900$? UPDATE: An example illustrating what partly motivated my question: Let us consider a situation where we approximate a probability distribution with the normal distribution using Fisher information. If I remember correctly, in normal approximation using Fisher information we also ""drop out"" higher orders than 3 out. Lets also say that we are dealing with an application where we need ""a very high precision"", higher than we can obtain with the ""2nd order"" normal approximation. In order to use the normal approximation, would we now need to derive a more higher order normal approximation?","['functional-analysis', 'statistics', 'taylor-expansion', 'approximation']"
2501024,Kolmogorow-Smirnow-Test with estimated parameters,"I think the most popular alternative to the ""normal"" Kolmogorow-Smirnow-Test for normal distribution (if the parameters are unknown and therefore have to be estimated) is the Lilliefors-Test. I am thinking about this (maybe too simple) alternative: first, you calculate the empirical mean and empirical variance of the given data (or of a subset of it). Afterwards, you generate a sample of normally distributed random variables with these estimated parameters. Then, you can test, whether the original sample and the generated sample have the same distribution using the 2-sample-kolmogorow-smirnow-test. This would be a simple method to check, whether a given sample follows a normal distribution. I don't see problems but it seems to be too simple. Does anybody see problems/mistakes in this idea? Thank you very much!","['statistics', 'hypothesis-testing', 'normal-distribution']"
2501062,Where am I wrong in the solution for this combinatorics question?,"A combinatorics question goes like this :- There are 10 couples who wish to play mixed doubles tennis, without there being a couple in the court, i.e., there cannot be a husband of a wife, or vice versa either in the same team or opposite team. How many ways are there to play the match? I did this like:- We can select the first wife in 10 ways, the second wife in 9 ways. Now there cannot be 2 husbands of these wives. So for selecting the husbands there are 8 X 7 ways. Total no.of ways = 10 X 9 X 8 X 7 = 5040. But the answer is 2520 = 1/2 (5040). Where am I wrong? Thank You.","['combinations', 'combinatorics']"
2501105,"Find the equation of the circle circumscribing the triangle formed by the three points $(a,0,0)$, $(0,b,0)$ and $(0,0,c)$.","Assuming the equation of the circle circumscribing the triangle formed by the three given points is given by the sphere through the three points and the plane through the three points. Plane is given by $\frac{x}{a} + \frac{y}{b} + \frac{z}{c} = 1$
and assuming the equation of the sphere is $x^2+y^2+z^2+2ux+2vy+2wz+d=0$. I've not been able to find the value for $d$, putting the three points in the equation of sphere I got $3$ equations with $4$ variables.","['analytic-geometry', 'spheres', '3d', 'geometry']"
2501158,Prove that there is only a single point of minimum distance for $N>3$ points,"We want to mimimize the sum of distances from $n$ distinct points. Prove that there exists only one such point for $n>3$ if all the $n$ points lie on a single plane (and not on a single line) The problem seems quite tough, but might possess an elegant solution. I tried creating two PDE's (partial differential equations) for $x$ and $y$ coordinates and couldn't find anything fruitful. Might be something towards vectors and their sum.","['vectors', 'calculus', 'geometry']"
2501189,Eigenvalues of a matrix multiplied by its transpose,I recall being told that the eigenvalues of the matrix formed by multiplying a matrix by its transpose are the squares of the eigenvalues of the original matrix. Is that true for any matrix? Surely not.,"['eigenvalues-eigenvectors', 'matrices', 'transpose', 'symmetric-matrices', 'linear-algebra']"
2501235,Measurability of a set in the definition of almost sure convergence,"Many books define almost sure convergence as follows: The sequence of random variables ${(X_n)}_{n \in \mathbb{N}}$ defined on the probability space $(\Omega, \mathcal{F}, P)$ converges almost surely to a random variable $X$ defined on the same probability space, if 
  $$
P(\{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}) = 1.
$$ In connection to this question , I wonder if the set $A := \{ \omega \in \Omega: \lim_{n \rightarrow \infty} X_n(\omega) = X(\omega)\}$ is implicitly assumed to be measurable or whether it is actually a priori measurable. If the latter is true, how can one show this?","['real-analysis', 'measure-theory', 'convergence-divergence', 'probability-theory']"
2501244,Make all cells of the square be the same color,"$200 \times 200$ square is colored in chess order. In each move we can take any $2 \times 3$  rectangle and change the color of all its cells. Can we make all cells of the square be the same color ? I still do not have an idea. Is this construction useful, square with every $2 \times 3$  rectangle contains 2 black cells ?",['combinatorics']
2501287,Proof of finite abelian groups structure theorem using representation theory,"While looking at the french wikipedia page for the finite abelian groups structure theorem (that is, every finite abelian group $G$ is isomorphic to a group of the form $\mathbb{Z}/a_1\mathbb{Z} \times \dotsb \times \mathbb{Z}/a_n\mathbb{Z}$, with $a_1 \mid \dotsb \mid a_n$), which can be found here for french-speaking people, I found written that there are short proofs of the theorem relying on representation theory. But I have not found a proof of this kind anywhere; all the classical books prove it with the usual generalization to finitely generated $A$-modules with $A$ a principal ring. Would someone know anything about such a proof ? If so, could someone give me a reference where it would be done, or write the proof here ? Thanks in advance.","['representation-theory', 'reference-request', 'abelian-groups', 'group-theory']"
2501294,Show that for a scalar field the following relation is true,"The FLRW line element is $ds^2=dt^2 -a^2(t)[(dx^1)^2+(dx^2)^2+(dx^3)^2]$. For a homogeneous scalar field $\psi$ show that
$$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = \ddot{\psi} + 3\frac{\dot{a}}{a}\dot{\psi}$$ The dot denotes a time derivative, the Einstein summation convention applies. I think I get close but I'm going wrong somewhere! Here's what I've tried: Expand the second covariant derivative
$$g^{\mu\nu}\nabla_{\mu}\nabla_{\nu} \psi = g^{\mu\nu}\nabla_{\mu}(\partial_{\nu}\psi)$$
$$=g^{\mu\nu}(\partial_{\mu}\partial_{\nu}\psi-\partial_{\sigma}\psi\Gamma^{\sigma}_{\mu\nu})$$
$$=g^{\mu\nu} \partial_{\mu}\partial_{\nu}\psi - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$
Homogeneous means $\psi$ is a function of $t$ only, and so for the first term to be non-zero sub in $\mu=\nu=0$. Because they're dummy indices that doesn't mean I also have to set them to zero in the second term. 
$$=g^{00}\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi$$
From the line element, $g^{00}=1$. Now looking at the second term, I can write out the Christoffel symbol:
$$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu} = \frac{1}{2}g^{\sigma\epsilon}g^{\mu\nu}\left(\frac{\partial g_{\mu\epsilon}}{\partial x^{\nu}}+\frac{\partial g_{\nu\epsilon}}{\partial x^{\mu}} - \frac{\partial g_{\mu\nu}}{\partial x^{\epsilon}}\right)$$
Contract with the metric:
$$\color{red}{\frac{1}{2}g^{\sigma\epsilon}\left(\frac{\partial g^{\nu}_{\epsilon}}{\partial x^{\nu}}+\frac{\partial g^{\mu}_{\epsilon}}{\partial x^{\mu}} - 0\right)}$$ 
Because the last term is either one or zero so the derivative is always zero. Contract again: 
$$\frac{1}{2}\left(\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}+\frac{\partial g^{\sigma\mu}}{\partial x^{\mu}}\right)$$
Relabel the dummy index and get 
$$g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}=\frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}$$
If I substitute that back in to the original equation, 
$$\frac{\partial ^2\psi}{\partial t^2} - g^{\mu\nu}\Gamma^{\sigma}_{\mu\nu}\partial_{\sigma}\psi = \frac{\partial ^2\psi}{\partial t^2} - \frac{\partial g^{\sigma\nu}}{\partial x^{\nu}}\partial_{\sigma}\psi$$
Looking at that I would conclude that only $\sigma = 0$ in the second term is non-zero, because I'd have $\partial_{\sigma}\psi$ which is zero unless the derivative is with respect to $t$. But the metric is diagonal so having $g^{0\nu}$ would mean I can't ever have the $a^2$ term and I won't get the $\dot{a}/a$ that I need. Where have I gone wrong?","['tensors', 'differential-geometry']"
2501295,Pythagorean Triple where $a=b$?,"I cannot find even a single webpage mentioning this topic. I'm a programmer and I'm looking for a 45-45-90 triangle where all of the sides are whole numbers . In the video I am watching, they say to use $ a = 10 $ , $ a = 10 $ , $ c = 14 $ because $ 10 \sqrt{2}  $ is close enough to $ 14 $ . In my program I am worried this could have serious consequences because it's not accurate. Does there exist a case where $ 2 a^2 = c^2 $ where a and c are whole numbers ? If it does not exist, why? Does it revolve around the fact that $ \sqrt{2} $ is irrational?","['elementary-number-theory', 'pythagorean-triples', 'geometry']"
2501297,How to check if $\sum_{\ell\in\mathbb{Z^2}} e^{-\vert x-\ell \vert^2}=\sum_{\ell\in\mathbb{Z^2}} e^{-\vert y-\ell \vert^2}$?,"suppose $x, y\in\mathbb{R}^2$ are given such that $\vert x \vert=\vert y \vert$ (where $\vert x \vert$ denote the Euclidean norm). I want to investigate, if the following limits are equal or not: $$\sum_{\ell\in\mathbb{Z^2}} e^{-\vert x-\ell \vert^2}=\sum_{\ell\in\mathbb{Z^2}} e^{-\vert y-\ell \vert^2}$$ If $y=-x$ then both limits are equal. But what happens, if the angle between $x,y$ is between $0$ and $\pi$? I tried to use $\vert x-\ell\vert^2=\vert x \vert^2 - 2\vert x \vert\vert y \vert\cos(\angle(x,\ell))+ \vert \ell\vert ^2$, but it did not help me. Can someone tell me how to approach this problem? Best regards","['integer-lattices', 'exponential-function', 'sequences-and-series']"
2501314,Integrate $\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx$,"I have tried solving the following integral using Calc I&II methods and couldn't find an answer.
I would like to know the techniques for solving such integral. $$\int\sqrt{25\sin^2(5x)+49\cos^2(7x)}dx$$","['integration', 'calculus']"
2501333,"Proof of "" The set of all self-adjoint operators are closed "".","The set of all self-adjoint operators are closed in Hilbert space. Let $T \in B(H)$ where $H$ is the Hilbert space and $B(H)$ denotes the set of linear operators from $H \mapsto H$. We are working with the operator norm. Adjoint of an operator $T$ is denoted by $T^*$. An operator $T$ is  said to self-adjoint if $T = T^*$ where $T^*$ is the adjoint of operator $T$. To prove the above statement let us consider a sequence of self - adjoint operators $\{T_{n}\}$ converging to a operator $T$ , now if we prove that the operator $T$ is self-adjoint then we would prove that the set of all self-adjoint operators is closed. So $T_{n} \rightarrow T$ To prove that $T$ is self-adjoint Proof - Let us consider $\|T -T^*\| = \|T - T_{n} + T_{n} - T_{n}^* + T_{n}^* - T^*\|$ By triangle inequality we have  $\|T -T^*\| \leq \|T -T_{n}\| + \|T_{n} - T_{n}^*\| + \|T_{n}^* - T^*\|$ So we have now that $T_{n} = T_{n}^*$ as $T_{n}$ are self-adjoint operators so the middle term become zero. so we are left with $\|T -T^*\|\leq \|T -T_{n}\| + \|T_{n}^* - T^*\|$. Now the term $\|T - T_{n}\| \rightarrow 0$ but how to deal with the second term $\|T_{n}^* - T^*\| = \|T_{n} - T^*\|$? As if we showed the RHS to be zero then $T= T^*$ implying $T$ is self-adjoint and hence the set of all self-adjoint operators are closed!","['functional-analysis', 'adjoint-operators', 'proof-verification', 'proof-explanation']"
2501336,Finding condition if a quadratic equation have at least one real root given interval of x,"Question: Find value of $a$ if equation $ax^2 + 2x -1 = 0$ has at least one real root for  $3 \leq x < 4$ I know if the restriction of $x$ is not given, I can use $D \geq 0$ to get condition for at least one real root, but if $x$ is restricted then how do I do it?","['algebra-precalculus', 'roots', 'quadratics']"
2501362,Is rolling 10% 10 times just as good as 1 100% or better?,"How do you determine the value of a percentage based on how many rolls? For example would 6 rolls at 10% be better than 1 roll at 60%? 1 roll at 100% to 100 rolls at 1%? I know each probability is independent of each other, but I don't know how to determine which is a better odd.","['probability-theory', 'probability']"
2501370,Definition of Open set,"Why cannot mathematicians agree on a definition!!! Open sets can be defined in two ways: Either using metric space or using topological spaces. I came across with the definition that is defined in a third way i.e A set $S$ is said to be open if, for each $x \ \epsilon \ S$, there exists an open interval $I_{x}$ such that $x \ \epsilon \ I_{x} \subseteq S. $ Are all these definitions equivalent? It is very difficult for me to understand this one. I am an undergraduate student and having a course in Real Analysis. Can anybody help me which definition I should follow. (I mean which is easier to understand and digest.)","['general-topology', 'real-analysis']"
2501378,Is $x_1^2-x_2^3$ irreducible?,"Is the polynomial $f=x_1^2-x_2^3 \in K[x_1,x_2]$, with $K$ field, irreducible? I think yes, because: $f=x_1^2-x_2^3$ has degree $3$. If suppose that $f=hk$ is a non trivial factorisation, than deg$h=1$ and deg$k=2$. Then i write $h=ax_1+bx_2+c$ and $k=a'x_1^2+b'x_2^2+c'$ (i don't write $k=a'x_1^2+b'x_2^2+c'+d'x_1x_2$ because in $f$ there isn't the mixed product $x_1x_2$). Then multiplyng and imposing the equality between $f$ and $hk$ i find that $aa'=0; \quad  ab'=0; \quad ac'=0$, so i can conclude $a=0$ otherwise $k=0$ and that is absurd. Continuing, i have also $ba'=0; \quad bb'=-1; \quad bc'=0$, so i conclude that $b\neq0$ and $a'=c'=0$. But now in $hk$ doesn't compare the monomial $x_1^2$. Absurd. Is correct my reasoning? Are there other ways to prove this irreducibility? In genereal which are the ""tricks"" or observations one can use to prove irreducibility of polynomial in such cases?","['irreducible-polynomials', 'algebraic-geometry', 'factoring', 'ring-theory', 'commutative-algebra']"
2501401,Compute characteristic classes of principal bundle over surfaces,"Let $G$ be a connected Lie group and $\Sigma$ a closed surface. We know that principal $G$-bundles $P$ can be topologically classified by a characteristic class $c(P)\in H^2(\Sigma,\pi_1G)\cong\pi_1G$. (One of the argument for this is here . The following is my question: Let $G$ be a semisimple connected (or even compact) Lie group, $\beta\colon\pi_1(\Sigma)\to G$ a group homomorphism, and $\Sigma$ a closed oriented surface. Consider the universal cover $\tilde{\Sigma}\to\Sigma$ which is a principal $\pi_1(\Sigma)$-bundle. Form the associated bundle $\tilde{\Sigma}\times_\beta G$ which is necessarily a principal $G$-bundle over $\Sigma$. Hence, it should have a characteristic class $c(\tilde{\Sigma}\times_\beta G)\in H^2(\Sigma,\pi_1G)\cong\pi_1G$. Is there a way to compute this characteristic class in terms of $\beta$? The difficulty here is that $\pi_1(\Sigma)$ is not connected, and hence I cannot use the functoriality of the characteristic class. Another one is that I am not sure how to compute the induced map $B\beta\colon B\pi_1(\Sigma)\to BG$ concretely. Any idea or reference is greatly appreciated!","['algebraic-topology', 'principal-bundles', 'differential-geometry']"
2501405,Counting vectors in $\mathbb{Z}_n^n$ with $0$ as a most common coordinate value,"How can we count the number of vectors in $\mathbb{Z}_n^n$ that have $0$  as their strictly most common coordinate value appearing exactly $k$ times ? More precisely, if we denote by 
$$\alpha(\mathbf{x},m)=\sum_{k=1}^n \mathbb{1}\{ x_k=m \} $$ the number of times the value $m$ appears as a coordinate in a vector $\mathbf{x}=(x_1,x_2,...,x_n)$, then the numbers I am looking for are $$ \beta(n,k) = \big| \big\{ (x_1, x_2,...,x_n) \in \mathbb{Z}_n^n: \alpha(0) = k, \alpha(\mathbf{x},0)>\alpha(\mathbf{x},m) \quad \mbox{for} \quad m=1,2,...,n-1  \big \}\big|$$ I would also like to count the vectors that have $0$ as the most common coordinate value appearing exactly $k$ times which is the same number of times another coordinate value appears. In this latter case, I would also like to know how many values are tied and the formal definition would be $$ \hat{\beta}(n,k) = \big| \big\{ (x_1, x_2,...,x_n) \in \mathbb{Z}_n^n: \alpha(\mathbf{x},0) = k, \exists m \in \{1,2,..., n-1\} \quad \mbox{s.t.} \quad \alpha(\mathbf{x},0)=\alpha(\mathbf{x},m)  \big \}\big|$$ For example, if $n=3$, then all $27$ vectors are $(0,0,0), (0,0,1),..., (2,2,2)$, of which there is $1$ that has $0$ as strictly most common coordinate value appearing $3$ times (the vector $(0,0,0)$), there are $6$ that have  $0$ as strictly most common coordinate value appearing $2$ times (those are the vectors $(0,0,1), (0,1,0), (1,0,0), (0,0,2), (0,2,0)$ and $(2,0,0)$) and 6 which have $0$ as the most common value appearing once, but it is not the only most common coordinate value (those are the $6$ permutation and $3$ values are tied). So $\beta(3,3)=1, \beta(3,2) = 6$ and $\hat{\beta}(3,1) = 6$) EDIT When $k>n/2$, calculations are easy as we only need to place the $k$ zeros and fill the remaining places with arbitrary numbers, so $\beta(n,k) = C^k_n \times (n-1)^{(n-k)}$. The problem lies in the case when $k\leq n/2$. EDIT 2 Here are some precomputed values : $\beta(3,k) = [0, 0, 6, 1]$ for $k=0,1,2,3$ $\beta(4,k) = [0, 0, 36, 12, 1]$ for $k=0,1,...,4$ $\beta(5,k) = [0, 0, 240, 160, 20, 1]$ for $k=0,1,...,5$ $\beta(6,k) = [0, 0, 1800, 2400, 375, 30, 1]$ for $k=0,1,...,6$ $\beta(7,k) = [0, 0, 15120, 40950, 7560, 756, 42, 1]$ for $k=0,1,...,7$ and for $n=6$ the $A[i][j]$ entry in the matrix below stores the number of times that $0$ appears a maximal number of times equal to $i$ together with $j$ other values. [[    0.     0.     0.     0.   720.     0.] [    5400.   900.     0.     0.     0.     0.] [    100.     0.     0.     0.     0.     0.] [    0.     0.     0.     0.     0.     0.] [    0.     0.     0.     0.     0.     0.] [    0.     0.     0.     0.     0.     0.]] The same matrix for $n=5$: [[   0.    0.    0.  120.    0.] [   360.    0.    0.    0.    0.] [   0.    0.    0.    0.    0.] [   0.    0.    0.    0.    0.] [   0.    0.    0.    0.    0.]]","['combinations', 'combinatorics', 'vectors']"
2501417,$\sum\limits_{k=0}^{\frac{p-1}2}3^k\binom{p}{k}\equiv 2^p-1\pmod{p^2}$,"Let $p$ be prime and $p\ge5$. My friend askes me the following $\sum\limits_{k=0}^{\frac{p-1}2}3^k\binom{p}{k}\equiv 2^p-1\pmod{p^2}$. Here $\binom{p}{k}=\frac{p!}{k!(p-k)!}$ for any $k=0,\ldots,p$. We define $0!=1$. I think this is true, but I have no idea to attempt it.","['contest-math', 'binomial-coefficients', 'elementary-number-theory']"
2501476,"Mysterious factor of ""$5$"" appearing in differential equation solution","Please forgive my formatting, this is my first post here. I am trying to solve the differential equation of the form: $x' = A*x$ Where A is the $(n \times n)$ matrix: $
        \begin{pmatrix}
        2 & -5 \\
        1 & -2 \\
        \end{pmatrix}
$ I found the characteristic equation to be given by: $\lambda^2 + 1 = 0$  which has roots $\lambda = {i, -i}$ Solving the augmented matrix $(A-i*I\ |\ 0)$ gave me the corresponding eigenvector $\phi_1$: $
        \begin{pmatrix}
        2 + i \\
        1 \\
        \end{pmatrix}
$ This gives the complex solution $x = \phi_1*\exp(it)$ I know that if a solution can be written as $x = u(t) + i*v(t)$, then $u(t)$ and $v(t)$ are also solutions. By Euler's formula, I got: $ u(t) = 
        \begin{pmatrix}
        2 \cos (t) - \sin (t) \\
        \cos (t) \\
        \end{pmatrix}
$ $ v(t) = 
        \begin{pmatrix}
        2 \sin (t) + \cos (t) \\
        \sin (t) \\
        \end{pmatrix}
$ However, the solution in textbook is: $ u(t) = 
        \begin{pmatrix}
        5 \cos (t) \\
        2 \cos (t) + \sin (t) \\
        \end{pmatrix}
$ $ v(t) = 
        \begin{pmatrix}
        5 \sin (t) \\
        - \cos (t) + 2 \sin (t) \\
        \end{pmatrix}
$ What it looks like is they may have used a different eigenvalue / eigenvector, but I can't figure out what they used. I believe my solutions are correct, I checked the first one $\big(u(t)\big)$ by plugging it back in to the original equation. Wolfram alpha also gives the solution with the ""$5$"".  I wonder why? Thanks.","['ordinary-differential-equations', 'linear-algebra']"
2501484,Convolution of l-adic sheaves is commutative,"I am trying to figure out how to prove a very basic statement about convolution of $\ell$-adic/perverse sheaves in Katz's ""Rigid local systems"" (section 2.5.3, (1) ).
The fact is fairly obvious and I guess the proof is purely formal, but I'm not sure about it since my knowledge of the schemes formalism is limited. Settings : 
$G/k$ is a (smooth, separated) group scheme over a field $k$, of pure relative dimension $d$. Denote $\mu: G\times_k G\rightarrow G$ the multiplication map and $e: k\rightarrow G$ the identity section.
For $K$, $L\in D^b_c(G,\overline{\mathbb{Q}_l})$ (the ""derived category"" of $\ell$-adic sheaves over $G$, $l\neq \text{char}(p)$) define the product $K\times L\in D^b_c(G\times_k G,\overline{\mathbb{Q}_l})$ as
$$K\times L:=pr_1^{*}K\otimes^{\mathbf{L}}pr_2^{*}L$$
with $pr_1$, $pr_2$ the canonical projections $G\times_k G\rightarrow G$, and $\otimes^{\mathbf{L}}$ the derived tensor product, which I'll just denote by $\otimes$ in the following. Now define their $\star_{*}$ convolution as
$$K\star_{*}L:=R\mu_{*}(K\times L)\in D^b_c(G,\overline{\mathbb{Q}_l})$$ The claim is that, if $G$ is commutative, then the $\star_{*}$ convolution is commutative. What I did: We want to show that if $K$, $L\in D^b_c(G,\overline{\mathbb{Q}_l})$, then
$R\mu_{*}(K\times L)=R\mu_{*}(L\times K)$, i.e., as $\otimes$ is commutative, that 
$R\mu_{*}(pr_1^* K\otimes pr_2^*L)=R\mu_{*}(pr_2^* K\otimes pr_1^*L)$ First, I think that
$$(pr_2,pr_1)^* (pr_2^*K\otimes pr_1^*L)\simeq pr_1^*K\otimes pr_2^*L$$
i.e., $(pr_2,pr_1)^*(L\times K)=K\times L$.
This sounds reasonable that, if we switch both factors in $G\times_k G$, then we should replace $K\times L$ by $L\times K$. But I'm not sure this is obvious. Moreover, we have the following commutative diagram (as $G$ is commutative)
\begin{array}{ccc}
G\times G &\xrightarrow{(pr_2, pr_1)}& G\times G\\
|& &| \\
\mu & &\mu \\
\downarrow & &\downarrow\\
G &\xrightarrow{id}&G
\end{array} and I think it's also cartesian. I would like to use some kind of ""proper base change"" to say that $R\mu_*(pr_2,pr_1)^*=R\mu_*$, and to apply $R\mu_*$ to my (claimed) equality $(pr_2,pr_1)^*(L\times K)=K\times L$ to conclude. But the problem is that I'm not sure that I can do it directly with my diagram, as $\mu$ is maybe not proper. Another option could just be to use the commutativity of my diagram to write
$R\mu_*R(pr_2,pr_1)_*=R\mu_*$,
and invoke that $R(pr_2,pr_1)_*=(pr_2,pr_1)^*$ by proper base change applied to the following diagram
\begin{array}{ccc}
G\times G &\xrightarrow{(pr_2, pr_1)}& G\times G\\
|& &| \\
id & &(pr_2,pr_1) \\
\downarrow & &\downarrow\\
G\times G &\xrightarrow{id}&G\times G
\end{array} Is there anything right in what I wrote ? Is there some easier proof ?
Thanks for your help !","['derived-functors', 'group-schemes', 'convolution', 'algebraic-geometry']"
2501558,Prove: $\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}}$ for $0\le m<1$,"How to prove the following identity? For $0\le m<1$,
$$\int_{0}^{\pi/2}\frac{dx}{\sqrt{(1-m \cos^2 x)(1+ m \sin^2x)}}=\int_{0}^{\pi/2}\frac{dx}{\sqrt{1-m^2 \sin^2 x}}$$
Then it will be an elliptic integral. Firstly, the objects in the squre root are not equal, so it cann't be solved just by Trigonometric Identities.","['definite-integrals', 'elliptic-integrals', 'calculus']"
2501564,Reducing to reduced echelon form in linear algebra seems to do so many useful things - what gives? Why?,"What's the underlying thing about reducing a matrix to reduced echelon form that solves so many things for us? Some determinations that can be from reducing to reduced echelon form: Testing linear dependency Seeing if a matrix has an inverse Solving a system of linear equations Finding the number of free parameters in a system of linear equations Finding row and column rank I know why it can determine each individual thing, but there seems to be some underlying essence behind it all. It seems like a very convenient algorithm, so there must be something about it I'm missing that explains why it can explain so many things about a matrix. My guess is that it reduces the matrix down to its basis which may determine the rest by analyzing the basis, but other than that, not sure. What gives? ADDENDUM: I've been told in a comment that ""reducing to RREF reveals the basis of the row-space"". How can a matrix have a different row space and column space simultaneously? What does that mean about matrices, exactly?","['matrices', 'matrix-rank', 'linear-algebra', 'systems-of-equations']"
2501573,How to make sense of complex coordinates on manifolds?,"I'm used to the following definition of a smooth manifold: An $n$ -dimensional chart for a topological space is a pair $(U,x)$ where $U$ is an open set of said space and $x : U\to \mathbb{R}^n$ is a homeomorphism. Two $n$ -dimensional charts $(U,x)$ , $(V,y)$ for a topological space are said $C^\infty$ compatible if $U\cap V=\emptyset$ or $U\cap V\neq\emptyset$ and $x\circ y^{-1} : y(U\cap V)\to x(U\cap V)$ and $y\circ x^{-1}: x(U\cap V)\to y(U\cap V)$ are $C^\infty$ functions on $\mathbb{R}^n$ . One $n$ -dimensional $C^\infty$ atlas for a topological space is a collection of $n$ -dimensional $C^\infty$ compatible charts whose domains cover the topological space. A smooth manifold is a Hausdorff, second-countable topological space together with one maximal $n$ -dimensional $C^\infty$ atlas. That's basicaly it. Now, it is quite common to see people doing something on the case of $S^2$ and manifolds related to it: like spacetimes with spherical symmetry. What is done is: people use stereographic projection to map $S^2\setminus \{N\}$ onto $\mathbb{R}^2$ , building the chart $$x(a,b,c)=\left(\dfrac{a}{1-c},\dfrac{b}{1-c}\right)$$ with two coordinate functions $x^1$ and $x^2$ . Now obviously since $\mathbb{C}$ is just $\mathbb{R}^2$ with a multiplication law, we can write $$x=(x^1,x^2)=x^1(1,0)+x^2(0,1)=x^1+ix^2.$$ That much is quite fine. Again, as a vector space over $\mathbb{R}$ , $\mathbb{C}$ is just $\mathbb{R}^2$ and so the chart can be seen as a mapping $x : S^2 \setminus \{N\}\to \mathbb{C}$ . Now here comes what troubles me. Instead of using coordinate functions $x^1=I^1\circ x$ and $x^2=I^2\circ x$ one switches to $z,\overline{z}$ . So people try to use the ""coordinate functions"" as $$z(a,b,c)=\dfrac{a}{1-c}+i\dfrac{b}{1-c},\quad \bar{z}(a,b,c)=\dfrac{a}{1-c}-i\dfrac{b}{1-c}$$ I can't get the point on how to rigorously make sense of this. I mean, the pair $(z,\bar{z})$ is a map $(z,\bar{z}) : S^2\setminus\{N\}\to \mathbb{C}^2$ , so it isn't a chart. Furthermore, as a real vector space $\mathbb{C}^2$ is $\mathbb{R}^4$ which is quite strange here. Finaly, all the information about the coordinates is already contained in $z$ above. Actualy, $\bar{z}$ is just one operation performed on $z$ . So what is the point with this? How does one rigorously in the context of the definitions I've mentioned, make sense of this approach? How is this a valid and rigorous thing to do, and what is the idea behind it?","['real-analysis', 'coordinate-systems', 'smooth-manifolds', 'complex-analysis', 'differential-geometry']"
2501579,Why is a unique Sylow p-subgroup normal?,"I need to prove that a group $G$ with $|G| = pq$, where $p$, $q$ are primes, cannot be simple. I have already reduced this problem to showing that a unique Sylow $p$-subgroup is normal. The answers i have found so far are something along the line of ""the Sylow $p$-subgroup is normal because all $p$-Sylow subgroups are conjugate to each other"" which means diddly-squat to me. I need help understanding that last part.","['abstract-algebra', 'normal-subgroups', 'sylow-theory']"
2501599,An Infinite Limit?,"The following question has a finite limit $\left( {{{11e} \over {24}}} \right)$, which can be easily obtained from the method of expansions . But I get an infinite limit, and I am not exactly sure where I have gone wrong. Please let me know also the reason why a certain step cannot be undertaken. Question - $$Evaluate\,\mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$}
\kern-0.1em/\kern-0.15em
\lower0.25ex\hbox{$\scriptstyle x$}}}} - e + {\textstyle{1 \over 2}}ex} \over {{x^2}}}$$ My Try - $$\eqalign{
  & Let\,\,L = \mathop {\lim }\limits_{x \to 0} {{{{(1 + x)}^{{\raise0.5ex\hbox{$\scriptstyle 1$}
\kern-0.1em/\kern-0.15em
\lower0.25ex\hbox{$\scriptstyle x$}}}}} \over {{x^2}}} - \mathop {\lim }\limits_{x \to 0} {{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}  \cr 
  & Let\,\,{L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{{{(1 + x)}^{{\textstyle{1 \over x}}}}} \over {{x^2}}}} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {{{e - {\textstyle{1 \over 2}}ex} \over {{x^2}}}} \right)}}  \cr 
  & {L_1} = {e^{\mathop {\lim }\limits_{x \to 0} \left( {{{\log (1 + x)} \over x} - \log ({x^2})} \right)}}\,\,and\,{L_2} = {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {e\left( {1 - {\textstyle{x \over 2}}} \right)} \right) - \log {x^2}}}  \cr 
  & {L_1} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}\,and\,\,{L_2}\, = \,{e^{1 + }}^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right) - \mathop {\lim }\limits_{x \to 0} \log {x^2}}  \cr 
  & L = {L_1} - {L_2} = {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}} - {e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2} + \mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}  \cr 
  & \,\,\,\,\, = \,\,{e^{1 - \mathop {\lim }\limits_{x \to 0} \log {x^2}}}.\left( {1 - {e^{\mathop {\lim }\limits_{x \to 0} \log \left( {1 - {\textstyle{x \over 2}}} \right)}}} \right)  \cr 
  & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} {e \over {{x^2}}}\left( {1 - \left( {1 - {\textstyle{x \over 2}}} \right)} \right)  \cr 
  & \,\,\,\,\, = \,\,\mathop {\lim }\limits_{x \to 0} \,{e \over {2x}} = \infty  \cr} $$","['calculus', 'limits']"
2501619,Prove that an elementary function is non-decreasing,"For $p > 0$, define : $f(p) := \int_0^1{p (1-x)^{p^2 + 2p}e^{\frac{(1+p)^2}{2} x(x+2)}}dx$ The question is : how to prove that $f$ is a non-decreasing function of p? It seems to be the case numerically. Note that one can show that $p \mapsto f(p)/p$ is a non-increasing function because $f(p)/p = \int_0^1{\frac{1}{1-x}e^{(1+p)^2[\ln{(1-x)} + \frac{x(x+2)}{2}]}dx}$ and $\ln{(1-x)} + \frac{x(x+2)}{2}$ is always negative. So the difficulty here is to understand why does the factor $p$ help! Thanks a lot for any clue!","['real-analysis', 'integration']"
2501635,"Show that $\zeta_K(2) = \frac{\pi^4}{48 \sqrt{2}} $, with $K = \mathbb{Q}(\sqrt{2})$","For the real number field $K = \mathbb{Q}(\sqrt{2})$ the ring of integers is $\mathcal{O}_K = \mathbb{Z}[\sqrt{2}] $.  We can solve Pell's equation and so there are units $(1 - \sqrt{2})^k$ with $k \in \mathbb{Z}$.  One can show that $K$ is a: Euclidean domain principal ideal domain unique factorization domain Taking their word for it, there a norm $N_K: a + b \sqrt{2} \mapsto |a^2 - 2b^2 | $ and we can write the Dedekind Zeta function :
$$ \zeta_K(s) = \sum_{(a,b) \in \mathbb{Z}^2} \frac{1}{(a^2 - 2b^2)^k} = \frac{1}{1 - 2^{-s}} \prod_{p = \pm 1 (8)} \frac{1}{(1 - p^{-s})^2} \prod_{p = \pm 1 (8)} \frac{1}{(1 - p^{-2s})} $$ If we plug in $s = 2$ I found the numerical result stated without proof.  And I'm starting to migrate the existing proofs over $\mathbb{Z}$ to the ring $\mathbb{Z}[\sqrt{2}]$: $$ \zeta_K(2) = \sum_{x \in \mathbb{Z}[\sqrt{2}]} \frac{1}{N(x)^2} = \sum_{(a,b) \in \mathbb{Z}^2} \frac{1}{\big(a^2 - 2b^2\big)^2} = \frac{\pi^4}{48 \sqrt{2}} $$ The definition does not quite make sense over numbers, which explains the shift to ideals. We have $ \mathcal{O}(K)= \mathbb{Z}[\sqrt{2}$ and $$ \zeta_K(2) = \sum_{I \subseteq \mathcal{O}(K)} \frac{1}{N_{K/\mathbb{Q}}(I)^2} = \sum_{(a,b) \in \mathbb{Z}^2} \frac{1}{\big(a^2 - 2b^2\big)^2} = \frac{\pi^4}{48 \sqrt{2}} $$ The Euler product is the product over prime ideals : $$ \zeta_K(2) = \prod_{P \subseteq \mathcal{O}(K)} \frac{1}{1-    N_{K/\mathbb{Q}}(P)^{-2} } = \frac{1}{1 - 2^{-2}} \prod_{p = \pm 1 (8)} \frac{1}{(1 - p^{-2})^2} \prod_{p = \pm 1 (8)} \frac{1}{(1 - p^{-4})}
\stackrel{?}{=}
\sum_{(a,b) \in \mathbb{Z}^2} \frac{1}{(a^2 - 2b^2)^2} $$","['number-theory', 'zeta-functions', 'algebraic-number-theory']"
2501636,"Show that $\mu (E \setminus A)>0$ if $A$ is a nonmeasurable subset of $X$ and $A \subseteq E$, $E$ is measurable.","This question was asked here , but the question wasn't asking for an answer, just a clarification on the proof by contradiction they already had, so I'm reposting it to ask for help arriving at the answer. Because the asker above said they proved it by contradiction, that is what I'm attempting. But I'm having issues wrapping my brain around the concept of set measurability. So let's give it a good college try with what I can understand. For clarity, $\mu$ is an outer measure. A is nonmeasurable, so $\exists B \subseteq X$ such that $\mu (B) \ne \mu (B \bigcap A) + (B \bigcap A^c)$. E is measurable, so $\mu (B) = \mu (B \bigcap E) + (B \bigcap E^c)$. This essentially means $\mu (B) \ge \mu (B \bigcap E) + (B \bigcap E^c)$ (Since monotonicity is alredy implied by the definition of outer measure). So we can attempt a proof (or the start of one) by contradiction. Assume $\mu (E \setminus A)=0$. So $E \setminus A$ is a null set. So $\mu (B) \ge \mu (B \bigcap E) + (B \bigcap E^c)$. I assume I'm supposed to show a contradiction using that A is nonmeasurable. I'm unsure how. Another way I think could work would be to use: $\mu (E \setminus A)= \mu (E \bigcap A^c) = 0$ and arrive at a contradiction given that A is nonmeasurable, but again, I'm unsure how. I just need some direction from here. Thank you in advance for any advice.","['real-analysis', 'measure-theory']"
2501654,How does one go about $\lim_{y\to x} \frac{x^n-y^n}{x-y}$?,$$\lim_{y\to x} \frac{x^n-y^n}{x-y}$$ I think it would not be differentiable because if we used L'Hôpital the denominator would become $0$. I would like to learn which approach would be better and why: the long-division or by differentiation?,"['limits-without-lhopital', 'calculus', 'limits']"
2501699,Intersection of $\left(A_i\cup B_i\right)$ for two disjoint sets of events,"Let $\{A_1,A_2\dots A_n\}$ be a mutually disjoint set of events. Let $\{B_1,B_2\dots B_n\}$ be another mutually disjoint set of events. I am trying to transform the expression
$$C=\bigcap\limits_{i\in\{1,2,\dots,n\}}\left(A_i\cup B_i\right)$$ 
to something equivalent by somehow taking the union symbol out. I think that for $n=2$ it's
$$C=(A_1\cap B_2)\cup(B_1\cap A_2)$$
but I cannot generalize it for any $n$.","['combinatorics', 'elementary-set-theory']"
2501718,Joint Distribution from marginal distributions and the distribution of the sum,Let's assume I know the marginal distributions of two random variables. Does knowing the marginal distribution of their sum or their linear combination in general gives me any information on the possible joint distribution? Can it improve the Frechet bounds for the joint? Is it possible to have general results without imposing restrictions on the type of the marginal distributions?,"['multivariable-calculus', 'probability']"
2501736,Function that Returns a Set,"I was having a talk with one of my computer science teachers and he claimed that there are no functions that can return a variable number of parameters in mathematics (outside of extremely abstract fields).  I haven't had much experience with these topics, so instead I took this as a challenge and decided to try the following: f:Z->{Z}
f(x)={yϵZ|0<y<x} Where 'f' should return the set of all integers in between the 0 and 'x' that aren't negative. I understand that this isn't STRICTLY defining a function with multiple return values (such as a potential function that solves the quadratic equation), but is it still a series of valid mathematical statements that would provide a workaround counterexample? (also, any information about functions that return variable-size tuples would be helpful) EDIT: I might've overstated my professor's position on this, so please don't take that so far into consideration...","['elementary-set-theory', 'functions']"
2501763,Distribution with largest mean absolute deviation,"Let $X$ be a random variable such that $E(X^2)<\infty$ . Let $m$ denote a median of $X$ (that is to say any number $m$ such that $P(X\leq m)=P(X>m)=\frac 12$ ) and $\sigma$ its standard deviation. It's known that $|E(X)-m|\leq \sigma$ (see this for example). Out of curiosity, I'm looking for an example of a distribution such that $E(X)-m = \sigma$ (that is to say it has maximal mean absolute deviation) If $E(X)-m = \sigma$ , it's not hard to prove (see my post for example) that we must have $$P(X-E(X)\leq -\sigma)=\frac12$$ The last equality is quite intuitive: in a discrete setting, if there's one outlier much larger than the rest of the values, $E(X)$ will be much higher than $m$ , hence $X-E(X)$ will be mostly negative. I can't come up with an example of such distribution off the top of my head.","['inequality', 'median', 'probability-theory', 'standard-deviation', 'probability-distributions']"
2501815,$\| U+\|V\|_r\|_m =\| U+V \|_m$ if and only if $m=r=2$.,"Let $\|U \|_p = \left( E[|U|^p] \right)^{\frac{1}{p}}$. Is the following result statement true? Let $U$ and $V$ bet two independent, symmetric, non-degenerate random
  variables.  Then, \begin{align*} \| U+\|V\|_r\|_m =\| U+V \|_m
 \end{align*}  if and only if $r=m=2$. Note that the ""if""  directions is trivial since
\begin{align}
E[(U + \|V\|_2)^2]= E[U^2]+E[V^2]=E[(U+V)^2].
\end{align} The question is how to show the ""only if"" direction. I also feel that this should have come up somewhere. For example, is it related somehow to the fact that $L^2$ norm is the only $L_p$ norm induced by the inner product?  Anyway, this is just a thought and the question is not about inner products.","['normed-spaces', 'probability-theory', 'expectation']"
2501823,"A differentiable and unbounded function with infinitely many critical points, all of them being local maxima.","I'm searching for a function $f: \Bbb R^2 \rightarrow \Bbb R$ which has following properties: Both $\frac{\partial f}{\partial x}$ and $\frac{\partial f}{\partial y}$ exist and are continuous in $\Bbb R^2$. There are infinitely many critical points* , in all of which, $f$ has local maximum. $f$ is unbounded both from above and from below. Could you give me some hints, or even better, show me explicitly the desired function? *$(x,y)$ such that $\frac{\partial f}{\partial x}(x,y)=\frac{\partial f}{\partial y}(x,y)=0$.","['multivariable-calculus', 'real-analysis']"
2501862,Covariance and contravariance,"Given a vector space $V$, a vector $v \in V$ can be written in components with respect to different bases, say $X$ and $Y$. Now when i make a transformation from $X$ to $Y$, the components of the vector are transforming contravariantly. Now the dual space$V^*$ of $V$ is also a vector space, but the components of a vector there transform differently in a change of dual basis, i.e. covariantly. My question is, if we see the dual space $V^*$ as a vector space $W$, having no relation with the vector space $V$, will we then say that the components of a vector $w \in W$ will transform contravariantly?",['differential-geometry']
2501870,Proof for an identity (from Ramanujan written),"I saw an identity by Ramanujan
$$\forall n \in \mathbb{N} ,n>1 :\lfloor \sqrt n+\sqrt {n+2}+\sqrt{n+4} \rfloor=\lfloor \sqrt {9n
+17}\rfloor$$ I tried to prove it by limit definition .  I post my trial below . If possible check my prove (right , wrong) ? Then Is there more Idea to proof ?","['algebra-precalculus', 'alternative-proof', 'proof-verification', 'ceiling-and-floor-functions']"
2501908,Induction of the summation equation,"I have been on this problem and my professor has given me anything useful to work with.
$$\forall n\geq 1 :\quad \sum_{i=1}^{2n} i^3 = n^2(2n+1)^2$$ This is all the work I have currently on this problem. Base case n = 1
$$\sum_{i=1}^{2n} i^3 = 1^3+2^3 = 9$$
$$n^2(2n+1)^2 = (1)^2(2(1)+1)^2 = 9$$
Suppose $$\sum_{i=1}^{2k} i^3 = k^2(2k+1)^2$$
Need to show $$\sum_{i=1}^{2k+2} i^3 = (k+1)^2(2(k+1)+1)^2$$
observe $$\sum_{i=1}^{2k+2} i^3 =\sum_{i=1}^{2k} i^3+(2k+1)^3+(2k+2)^3$$
Inductive Hypothesis says
$$=k^2(2k+1)^2+(2k+1)^3+(2k+2)^3$$
$$=(2k+1)^2(k^2+2k+1)+(2k+2)^3$$
$$=(2k+1)^2((k+1)^2)+2^3(k+1)^3$$
$$=(k+1)^2((2k+1)^2+(k+1)2^3)$$
$$=(k+1^2(2k+1)^2+(8k+8))$$
$$=(k+1)^2(4k^2+4k+8k+9)$$
$$=(k+1)^2(4k^2+12k+9)$$
$$=(k+1)^2(2k+3)^2$$
Been stuck here for a while and have no idea where to go...","['induction', 'discrete-mathematics']"
2501938,Solutions of $0 = A¥sin^2(¥alpha) + B¥sin(2¥alpha) - C$?,"I am facing a problem from physics class involving a projectile motion which can be described with such an given equation:
$$
h = -¥frac{1}{2} ¥frac{g}{v_{0}^2 ¥cos^2¥alpha} d^2 + ¥frac{¥sin¥alpha}{¥cos¥alpha} d + y.
$$
The goal is to find the minimum value of (rearranging above equation)
$$
v_0(¥alpha) = ¥frac{d}{¥cos¥alpha} ¥cdot ¥sqrt{¥frac{1}{2} ¥cdot ¥frac{g}{¥tan¥alpha ¥cdot d + y - h}}.
$$
This involves finding solutions to $v_0'(¥alpha) = 0$. I was able to find the derivative ($t := d¥cdot ¥tan¥alpha +y-h$):
$$
v_0'(¥alpha) = ¥frac{¥tan ¥alpha}{¥cos¥alpha ¥cdot ¥sqrt{t}} - ¥frac{d}{2¥cos^3¥alpha ¥cdot (¥sqrt{t})^3}
$$
Because of the condition $v_0'(¥alpha) = 0$ this simplifies to (it is know that the solution is around $50^¥circ$)
$$
0 = ¥sin¥alpha - ¥frac{d}{2¥sin¥alpha ¥cos¥alpha ¥cdot (d¥cdot ¥tan¥alpha + y - h)}
$$
or
$$
0 = d¥sin(2¥alpha)¥tan¥alpha + (y-h)¥sin(2¥alpha) - d.
$$
or
$$
0 = 2d¥sin^2(¥alpha) + (y-h)¥sin(2¥alpha) - d
$$
or with some constants $$ 0 = 2d¥sin^2(¥alpha) + B¥sin(2¥alpha) - d. $$ How can I find the
  solutions from here?","['derivatives', 'trigonometry', 'mathematical-physics']"
2501986,Calculate risk (under squared error loss) of MSE estimator of normal variance,"I am given that $X_i\stackrel{\text{iid}}{\sim}N(\mu,\sigma^2)$. As part of a larger problem, I am to calculate the risk function of $\widehat{\sigma^2}_\text{MSE} = \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2$, using the loss function $L(\sigma^2,\widehat{\sigma^2}) = (\sigma^2-\widehat{\sigma^2})^2$. I get stuck just trying to simplify down the horrible expression one gets when applying the definitions of risk and loss functions. Maybe I'm missing some shortcut or something. Any tips would be appreciated. So far I have: \begin{align*}
R(\sigma^2,\widehat{\sigma^2}_\text{MSE}) = {} & \mathbb E[(\sigma^2 - \frac1{n-1}\sum_{i=1}^n(X_i-\bar X)^2)^2]\\
= {} & \cdots\\
= {} & \sigma^4 - 2\sigma^2\frac1{n-1} \left( \sum_{i=1}^n \mathbb E(X_i^2-\bar X^2) \right) \\ & {} + \frac1{(n-1)^2}\mathbb E\left[ \sum_{i=1}^n(X_i-\bar X)^4 + 2\sum_{i<j} (X_i-\bar X)^2(X_j-\bar X)^2 \right]
\end{align*} Any attempts to expand out that last big term seem to end in disaster.","['decision-theory', 'statistics', 'estimation']"
2501990,matrix wise tangent inverse (arctan),"Given a matrix $X$, an expression for the matrix cosine and sine are given by
$$
\textrm{cos}(X) = \frac{e^{iX} + e^{-iX}}{2}\\
\textrm{sin}(X) = \frac{e^{iX} - e^{-iX}}{2i}
$$
I have been trying to find a convenient expression for the arctan with no luck. Is there any expression for the arctan?
$$
\textrm{tan}^{-1}(X) = \text{?}
$$","['matrices', 'grassmannian', 'trigonometry', 'linear-algebra']"
2502009,Cauchy distribution maximum likelihood estimator: previous sources?,"On Wikipedia's page for the wrapped Cauchy distribution , there's a fixed-point algorithm to calculate the maximum-likelihood estimator using a Möbius transformation in the Poincaré disc that the circular Cauchy distribution can be considered to lie in. By using the parameterization $\theta=x_0+i\gamma$ on the regular Cauchy distribution and treating $\theta,\overline{\theta}$ as independent variables, the maximum likelihood equation becomes
$$\sum_{j=1}^n\frac{1}{\theta-x_j}=\frac{n}{\theta-\overline{\theta}} $$
and there's a similar (likely the same after Cayley transform, even) estimator
$$\theta_{k+1}=\overline{\theta_k-n\left(\sum_{j=1}^n \frac{1}{\theta_k-x_j} \right)^{-1}} $$
which can also be shown to be a contraction in the Poincaré upper half-plane metric by Schwarz-Pick (and that $z \mapsto -\overline{z}$ is an isometry).  However, the Wikipedia page's sources do not mention these and I've been unable to find a reference showing either algorithm, aside from Wikipedia itself.  But, the calculation is simple enough that I'd be legitimately surprised if neither had been done before.  Since I'm not hugely familiar with the statistics literature, I figure this should be aired out.","['reference-request', 'probability-theory', 'statistics']"
2502031,proof of which x for which sin(x) can be written in terms of real radicals,"Is there a constructive proof of the following statement? Expressions in real radicals exist for a trigonometric function of a
  rational multiple of $\pi$ if and only if the denominator of the fully
  reduced rational multiple is a power of $2$ by itself or the product of
  a power of $2$ with the product of distinct Fermat primes. I'm writing a computer algebra system, and it would be nice if the algorithm for rewriting trigonometric expressions in terms of real radicals wouldn't have to consider every denominator case separately. Ideally, it would work for any value for which the above statement says such conversion is possible, even for Fermat primes that haven't been discovered yet, memory limitations aside. I figure if a constructive proof of the statement exists, it could be used as a model for the algorithm.","['trigonometry', 'prime-numbers']"
2502099,"If $A \subset Y \subset X$ with $A'\subset X$ and $A = A' \cap Y$, then $A = A'$","If $A \subset Y \subset X$ with $A'\subset X$ and $A = A' \cap Y$, then $A = A'$ Is the statement in the title correct?  I don't think so, but my attempt seems convincing it is: $$\begin{align}A = A' \cap Y &\implies Y-A ~{= (Y-A') \cup (Y -Y)  \\= Y -A'} \\ & \implies Y-(Y-A) = Y-(Y-A') \\ & \implies A = A'\end{align}$$ Where is my mistake?",['elementary-set-theory']
2502100,A triple integral involving $\text{Li}_2$,"I am grateful to Cornel Ioan Valean for sharing this nice problem. Find a closed form in terms of the values of the $\zeta$ function for the following triple integral:
  $$ \iiint_{(0,1)^3}\frac{\text{Li}_2(1-xyz)}{1-xyz}\,dx\,dy\,dz.$$ My approach has been the following one By exploiting the dilogarithm reflection formula, it is enough to compute the above integral where $\text{Li}_2(1-xyz)$ is replaced by $1,\log(xyz)$ and $\text{Li}_2(xyz)$; By Fubini's theorem these integrals boil down to Euler sums of weight $5$, which can be computed in terms of values of the $\zeta$ function through Theorems $2.2$ and $3.1$ of Flajolet-Salvy . I am looking for an alternative/more elementary approach.","['special-functions', 'sequences-and-series', 'alternative-proof']"
2502141,help me verify my proof ($\lfloor x-1 \rfloor = \lfloor x \rfloor - 1$),"prove the following statement: $\forall x \in \mathbb{R}, \lfloor x-1 \rfloor = \lfloor x \rfloor - 1$ suppose $x \in \mathbb{Z}$, then $\lfloor x-1 \rfloor = x-1 $ and $ \lfloor x \rfloor -1 = x-1 $ since the floor of any integer is itself. suppose $x \in \mathbb{R} $, then $\lfloor x-1 \rfloor$ will give an integer that is also given when taking $\lfloor x \rfloor -1$. eg. $\lfloor 1.5-1 \rfloor = \lfloor .5\rfloor = 0 = \lfloor 1.5 \rfloor - 1$ i think ive almost got this proof correct but something about it just doesnt seem quite right. Can someone please help me verify? -thanks","['proof-writing', 'proof-verification', 'discrete-mathematics']"
2502144,"a consequence of a linear transformations $\mathcal{L} (X, Y) $ between two real Banach spaces $X$ and $Y$","I read a following statement in an academic paper from Journal of Mathematical Analysis and Applications . Please refer to Lemma 3 in https://ac.els-cdn.com/S0022247X05001897/1-s2.0-S0022247X05001897-main.pdf?_tid=75c416d8-c0c8-11e7-9e29-00000aacb362&acdnat=1509735466_ce1f132e2c3285a21b65e184e2630ecd . Let $E$ be a real Banach space endowed with complete norm $\| \cdot \|$ and $P$ be a total cone of $E$ . Suppose $B \colon P \to P$ is a bounded linear operator. Therefore this operator $B$ can be uniquely extended to a bounded linear operator on $\overline{B} \colon \overline{P-P} = E \to E$ such that $\| \overline{B} \| = \| B\|$ . Since there is no proof or any comments regarding this statement in that paper, I did not get why it is true. I was thinking that this statement might be a consequence of the Hahn Banach theorem for linear transformations $\mathcal{L} (X, Y) $ between two real Banach spaces $X$ and $Y$ . In fact, the precondition for such consequences may require the space $Y$ having the extensible property, please refer to Section 10 in this note http://www-personal.umich.edu/~romanv/teaching/2009-10/602/short-history-of-analysis.pdf . However, regarding the statement I wrote here, they only assumed that $E$ is a real Banach space with a total positive cone $P$ . I did not get why is that. So, could anyone please help me out and explain it? or could anyone please prove the statement I wrote above? Any idea or suggestion would be much appreciated! Thanks in advance!","['real-analysis', 'functional-analysis', 'linear-transformations', 'linear-algebra', 'analysis']"
2502155,"Counter example of the dominated convergence theorem, if the dominating function was not integrable.","Let $(\Omega, \mathcal{A}, \mu)$ be a measure space and $f_1, f_2, f_3,\dots:\Omega \to \overline{\mathbb{R}}$ be measurable functions such that
$$f_n \to f\quad f_n\geq f_{n+1}\geq 0$$
Show that if $f_1$ is integrable then
$$\lim_{n\to\infty}\int_\Omega f_n \;d\mu = \int_\Omega f \;d\mu$$
Show also that if $f_1$ is not integrable, this equality need not be true. The result follows immediately from the Dominated Convergence Theorem, if we use $f_1$ as the dominating function. The counterexample is a bit harder. Consider $\Omega = \overline{\mathbb{R}}$ such that $f:\overline{\mathbb{R}}\to\overline{\mathbb{R}}$ and we have a measure space equipped with the Borel sigma-algebra and Lebesgue measure. Let $$f_n(x) = \frac{x}{n}$$ Then $$f_n(x)\to f(x)=0\quad as \quad n\to\infty$$ But $$\int_{\overline{\mathbb{R}}}f_1 \;dm = \int_{\overline{\mathbb{R}}}x \;dm = \infty\neq 0 = \int_{\overline{\mathbb{R}}}f \;dm$$ Does this solution make sense to you?","['proof-verification', 'improper-integrals', 'lebesgue-integral', 'measure-theory', 'analysis']"
2502156,Differential equation: $y''\cdot y'\cdot y=1$,"I've been playing around with differential equations. I can easily solve the differential equation
$$y'\cdot y=1$$
for $y:\mathbb R\mapsto\mathbb R$,
and I can also solve
$$y''\cdot y=1$$
using substitution into the previous example. However, I cannot figure out this differential equation:
$$y''\cdot y'\cdot y=1$$
Does anybody have any ideas about how to solve this? So far, the only technique that I know that seems valid for this differential equation is the use of Taylor Series, but that gets too messy for me to get anything useful out of it.","['derivatives', 'ordinary-differential-equations']"
2502177,"Show that function $f(\mathbf{x})= (|x_1-x_2|,|x_2-x_3|,...,|x_n-x_1|)$is continuous","How to show that the function $\space f: \Bbb{R^n} $ $\rightarrow$ $\Bbb{R^n}$ such that it takes a vector $\mathbf{x} = (x_1,...,x_n)$ and returns a vector consisting of absolute values of the differences: $f(\mathbf{x})= (|x_1-x_2|,|x_2-x_3|,...,|x_n-x_1|)$ is continuous? If I can show that function that takes $1 \rightarrow 2, 2 \rightarrow 3, etc.$ is a bijection will that be enough? Thank you.","['continuity', 'real-analysis', 'functions']"
2502215,Tangent cone to a set at a given point and first-order necessary optimality conditions,"Consider the problem of minimizing a continuously differentiable function $f: \mathbb{R}^{n} \to \mathbb{R}$ with respect to $x$ in the set $$X = \{x: l_{j} \leq x_{j} \leq u_{j}, \, j=1,\cdots , n\}. $$ I need to do the following two things: For $n =3$, I need to describe the tangent cone to $X$ at the point $\overline{x}=(l_{1},u_{2},a)^{T}$, where $l_{3}<a<u_{3}$. Derive first-order necessary conditions of optimality for this problem. For #1 : My notes/text defines the tangent cone as the set $T_{X}(x)$ of all tangent directions for $X \subset \mathbb{R}^{n}$ at $x\in X$. And, a direction $d$ is called tangent to a set $X \subset \mathbb{R}^{n}$ at the point $x \in X$ and scalars $\tau_{k}>0$, $k = 1,2, \cdots $ such that $\tau_{k} \downarrow 0$ and $$d = \lim_{k \to \infty}\frac{x^{k}-x}{\tau_{k}}.$$ It also defines the tangent cone as the closure of the cone of feasible directions at $x \in X$: $$ T_{X}(x) = \overline{K_{X}(x)}= \overline{cone(X-x)}, $$ where $cone(X-x)$ is the cone generated by the convex set $X$. The set $X$ given in this problem seems to be convex by construction, but I do not know how to find the cone of feasible directions, and then determine its closure. In fact, below is a graph of the feasible region in the specific case when $0 \leq x_{1} \leq 2$, $-1 \leq x_{2} \leq 4$, and $\frac{1}{2} \leq x_{3} \leq 1$: For #2 : I am not really sure what to do. I don't know if the system  $$ \text{minimize}_{x \in X} f(x) \\ \text{subject to} \\ x_{1} \geq l_{1} \\ x_{2} \geq l_{2} \\ x_{3} \geq l_{3} \\ x_{1} \leq u_{1} \\ x_{2} \leq u_{2} \\ x_{3} \leq u_{3} $$has metric regularity, so I don't know if there are any results that I can apply or even generally how to find the first-order necessary conditions. There are some results that I have seen for functions that are twice differentiable, but we are not guaranteed of that here - only that $f$ is continuously first differentiable. Could somebody please help me? I am extremely lost, and not really understanding what to do. Thank you.","['nonlinear-optimization', 'convex-analysis', 'operations-research', 'convex-optimization', 'differential-geometry']"
2502259,What is this polynomial series? [closed],"Closed. This question is off-topic . It is not currently accepting answers. This question is missing context or other details : Please improve the question by providing additional context, which ideally includes your thoughts on the problem and any attempts you have made to solve it. This information helps others identify where you have difficulties and helps them write answers appropriate to your experience level. Closed 6 years ago . Improve this question I've come across this simple, yet peculiar series of polynomials wich I can't quite find a general formula for. It goes as such: $$1$$
$$x+1$$
$$x^2+x+2$$
$$x^3+x^2+2x+6$$
$$x^4+x^3+2x^2+6x+24$$
$$\vdots$$ I'm finding these polynomials by looking at a term in the $n^{th}$ integral of Ei(x). For example, the 5th integral of Ei(x) is; $$\frac{1}{120}(x^5Ei(x)-e^x(x^4+x^3+2x^2+6x+24))$$","['polynomials', 'sequences-and-series']"
2502263,Why does $3^{16} \times 7^{-6}$ become $\frac{3^{16}} {7^{6}}$?,"I was doing an exercise on exponents: $$\begin{align}
\left(3^{-8} \times 7^3\right)^{-2} &= \left(3^{-8}\right)^{-2}\times \left(7^3\right)^{-2} \\ 
&= 3^{16} \times 7^{-6} \\
&= \frac{3^{16}} {7^{6}} \\
\end{align}$$ Why did $7^{-6}$ turn to $7^{6}$? More generally, why does a negative exponent turn positive when moved to the denominator?  Would appreciate kindergarten language ;-D","['algebra-precalculus', 'exponentiation']"
2502272,Give an example of a $f$ such that $\mathcal{L}^p(\mathbb{R})\not\ni f\in\mathcal{L}^q(\mathbb{R})$ $(1\leq p\leq q)$,"(a) Give an example of a $f:\mathbb{R}\to\mathbb{R}$ such that $\mathcal{L}^p(\mathbb{R})\not\ni f\in\mathcal{L}^q(\mathbb{R})$ for $1\leq p\leq q$ (b) Give an example of a $f:\mathbb{R}\to\mathbb{R}$ such that $\mathcal{L}^\infty(\mathbb{R})\not\ni f\in\mathcal{L}^p(\mathbb{R})$ for $ p \in[1,\infty)$ $f\in\mathcal{L}^q(\mathbb{R})$ if $$\int_\mathbb{R} |f|^q \;d\mu < \infty$$ (a) $$f(x) = e^{{-x}^{1/q}}  \textbf{1}_{[0, \infty)}$$ Then $$\int_\mathbb{R} \left| e^{{-x}^{1/q}}  \textbf{1}_{[0, \infty)}\right|^q \;d\mu =
\int_{[0,\infty)} e^{-x}   \;d\mu = 1$$ $$\int_\mathbb{R} \left| e^{{-x}^{1/q}}  \textbf{1}_{[0, \infty)}\right|^p \;d\mu =
\int_{[0,\infty)} e^{{-x}^{p/q}}   \;d\mu = \infty$$ Seems like a complicated example (you need complex analysis to prove the last integral does not converge), but I cannot think of anything simpler. I believe, there should be a much simpler example. (b) $f\in\mathcal{L}^\infty$ if $f$ is bounded almost everywhere i.e. $|f
|\leq c \quad a.e$ for some $c\geq0$ $$f(x)=\begin{cases}
x&x\in\mathbb{Q} \\
0&otherwise
\end{cases}$$ $f(x)$ is not bounded and $$\int_\mathbb{R} |f|^p \;d\mu = 0$$ for all $p\in[0,\infty)$ I need help verifying these examples and suggestions on other maybe easier/more intuitive examples.","['functional-analysis', 'lp-spaces', 'measure-theory', 'proof-verification']"
2502287,Rate of change of the radius of a balloon,"""Helium is pumped into a spherical balloon at a rate of $4$ cubic feet per second. How fast is the radius increasing after $3$ minutes?"" So this is what I did: $$V = \frac{4}3\pi r^3$$ $$\frac{dv}{dt} = 4\pi r^2\frac{dr}{dt}$$ $$4 = 4\pi r^2\frac{dr}{dt}$$ $$\frac{dr}{dt} = \frac{4}{4\pi r^2}$$ I did not know how to proceed after this. How am I supposed to find the value of $r$? Do I have to somehow relate $3$ minutes ($180$ seconds) in somehow? Any help?","['derivatives', 'implicit-differentiation', 'calculus']"
2502301,How to find $x$ given $\log_{9}\left(\frac{1}{\sqrt3}\right) =x$ without a calculator?,"I was asked to find $x$ when: $$\log_{9}\left(\frac{1}{\sqrt3}\right) =x$$ 
Step two may resemble: 
$${3}^{2x}=\frac{1}{\sqrt3}$$ I was not allowed a calculator and was told that it was possible. I put it into my calculator and found out that $x$=-0.25 but how do you get that?","['algebra-precalculus', 'radicals', 'logarithms', 'calculator']"
