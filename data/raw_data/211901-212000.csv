question_id,title,body,tags
4268942,What exactly are the differences between real multivariable limits and complex limits?,"What I understand is a definition of $\lim_{z \to z_0}f(z) = L$ is as follows. Is this correct? 1.1. Let $f=u+iv$ be a complex function defined on an open deleted neighbourhood of some point $z_0=(x_0,y_0) \in \mathbb C$ . 1.2. I understand the above translates to Let $G \subseteq \mathbb C$ . Let $z_0=(x_0,y_0) \in \mathbb C$ . Let $f: G \to \mathbb C$ . Let $U$ be an open neighbourhood of $z_0$ with $U \ \setminus \{z_0\} \subseteq G$ . We have $f=u+iv$ , for $u,v: G \to \mathbb R$ . 1.3. Then $\lim_{z \to z_0}f(z) = L$ is defined as the both of the ff are true $$\lim_{(x,y) \to (x_0,y_0)}u(x,y)=\Re(L)$$ $$\lim_{(x,y) \to (x_0,y_0)}v(x,y)=\Im(L),$$ where the real limit definition of real multivariable applies. Apparently the above is an equivalent definition of the original definition of $\lim_{z \to z_0}f(z) = L$ which is given as something like... For all $\varepsilon > 0$ , there exists $\delta > 0$ s.t. $|f(z)-L| < \varepsilon$ whenever $0 < |z-z_0| =$ $\sqrt{(x-x_0)^2+(y-y_0)^2}$ $< \delta$ BUT apparently ... ... we don't restrict only to $z=(x,y) \in $ ( either $G$ or $U$ or $U \ \setminus \ \{(z_0)\}$ . I think $U$ . ) or something unlike in the case for real multivariable limits ? What's going on? How does ' $f(z)$ ' make sense if you don't have $z \in Domain(f)=G$ ? And then maybe this even applies to (1) above. 2.1. UPDATE 1 : While I've found a textbook outside our syllabus that talks about $z \in G$ (by $G$ i mean domain of $f$ ), I've just checked out a textbook in our syllabus (Brown Churchill) that doesn't seem to have $z \in G$ . What's going on please? UPDATE 2 : the closest i think i've seen to $z \in G$ is that $z$ 'has an image w' but i don't think it necessarily means $z \in G$ ...or idk...i think 'has an image w' is like assuming rather than implying $z \in G$ In particular, what do we get as the difference in the case of $v=0$ , i.e. $f=u$ , i.e. $f$ is real-valued (I think in this case (3) implies $\Im(L)=0$ )? I mean to ask the difference between $$\lim_{(x,y) \to (x_0,y_0)}u(x,y)=\Re(L)$$ $$\lim_{z \to z_0}f(z)=L$$ If there's such a difference then I think the 2 $\lim$ 's are actually different like we ask for the 'complex limit' vs the 'real limit'. I guess both the complex and real limits are equal if they both exist, but apparently the complex limit has stricter requirements or something. So the only difference in output is whether they exist or not. Note : This is the correct version of the incorrect question I asked previously .","['analysis', 'real-analysis', 'complex-analysis', 'multivariable-calculus', 'limits']"
4268949,Is $\Vert A^n \Vert = \Vert A \Vert^n$ for normal operator $A$ on inner product space?,"Exercise 7(a) after $\S$ 87. Norm from Paul R. Halmos's ""Finite-Dimensional Vector Spaces"" (second edition) invites a comment on the following assertion. If (linear operator) $A$ is normal, then $\Vert A^n \Vert = \Vert A \Vert^n$ for every positive integer $n$ . For reference, $\S \ 87.$ Norm (from the book) has the following definition for the norm $\Vert \cdot \Vert$ of a linear operator: $\Vert A \Vert = \inf \big\{K: \Vert Ax \Vert \leq K \Vert x \Vert \text{ for all vectors } x \big\}.$ Going by the discussion preceding this exercise (in the book), I assume that the assertion concerns inner product spaces, not the more general normed vector spaces. However, the inner product space, say $\mathcal V$ , of the assertion is not specified to be over the complex (or real) field, and is not said to be finite-dimensional. Also, $\mathcal V$ is not given to be complete. I am able to see why the assertion holds if $\mathcal V$ is finite-dimensional over the complex field. The reason is the spectral theorem for normal operators on such vector spaces. Similarly, I also understand that the assertion would hold in another case: if $A$ is self-adjoint and if $\mathcal V$ is finite-dimensional over the real field. I am not able to imagine what happens in the general case however, and would appreciate a pointer. Thanks.","['operator-theory', 'vector-spaces', 'linear-algebra', 'functional-analysis', 'linear-transformations']"
4268962,Checking if $y=\ln(xy)$ is a solution of $(xy-x)y''+xy'^2+yy'-2y'=0$,"Check whether $y=\ln (xy)$ is an answer of the following differential equation or not $$(xy-x)y''+xy'^2+yy'-2y'=0$$ First I tried to solve the equation, $$x(yy''-y''+y'^2)+yy'-2y'=0$$ $$x((yy')'-y'')+(yy')-2y'=0$$ Since I have $-y''$ in the parenthesis , the substitution $z=yy'$ doesn't work here but if it was $-2y''$ instead, I could use the substitution $u=yy'-2y'$ but it is not the case. My second try was taking derivative of the answer (i.e $y=\ln(xy)$ ) and plugging it in the D.E, $$y'=\frac1x+\frac{y'}y\quad\Rightarrow y'(1-\frac1y)=\frac1x\quad\Rightarrow y'=\frac y{y-1}\times \frac1x$$ $$y''=\frac{-1}{x^2}+\frac{yy''-y'^2}{y^2}\quad\Rightarrow y''=\frac{y}{y-1}\times(\frac{-1}{x^2}-\frac{y^2}{y'^2})$$ But it is getting really ugly when I plug $y,y',y''$ in the original equation.","['derivatives', 'ordinary-differential-equations']"
4268995,"Is $y’=x^y\implies xy’’-x\ln(x)y’^2-yy’=0,y’=y^x\implies xy'(x)^4-y(x)y'(x)^2 \left(xy''(x)+2y'(x)\right)+y(x)^2\left(y'''(x)y'(x)-y''(x)^2\right)=0$?","Here is a partial inspiration for this question: ODE: $$y''y+ax+by+c=0,y=k\pm\sqrt2\int\sqrt{a\int\ln(y)dx-(ax+c)\,\ln(y)-by+K}dx,\int\frac{dy}{\sqrt{K-(ax+c)\,\ln(y)+a\int\ln(y)dx-by}}=k\pm\sqrt2x$$ and Is there an algebraic solution for the differential equation $y'= x^y$ , and if not, can we prove this? this is the same question, but has no current answer and only asks for a proof for a non-algebraic solution whereas I ask about a true solution. I know it is not traditional to ask 2 questions, but I cannot choose which equation to solve, so I will solve very simple first order nonlinear differential equations: $$y’=x^y\text{ and }y’=y^x$$ Here are the solutions: $$y+c=\int y^x dx,y+c=\int x^y dx$$ There is also no way I can use the Frobenius method as the serie solution method. Let’s try to separate the variables ignoring restrictions for now: $$y’=x^y\implies \ln(y’)=y\ln(x)\implies \log_x(y’)=y\implies y’^\frac1y=x\mathop\implies^{\frac d{dx}}1=\frac{y’(x)^{\frac1y-1}(yy’’-y’^2\ln(y’))}{y^2} $$ $$y’=y^x\implies y’^\frac1 x=y\implies \log_y(y’)=\frac{\ln(y’)}{\ln(y)} \mathop\implies^{\frac d{dx}} 1=\frac{y’’}{\ln(y)y’}-\frac{\ln(y’)y’}{\ln^2(y)y}$$ Here is the solution set family and differential curves for $y’=x^y$ : Here is the solution set family for $y’=y^x$ : How do I solve for $y’=x^y$ and $y’=y^x$ for $y$ explicitly? Any series solution is appreciated. If there is a closed form, then please also write it even though it seems unlikely. Here is some progress on $y’=x^y$ using the fact that $$x^{\sum\limits_{n\ge0} A_n}=\prod_{n\ge0} x^{A_n}:$$ $$y=\sum_{n\ge 0} a_n x^n\implies \sum_{n\ge1} a_n nx^{n-1}-\prod_{n\ge0} x^{a_n x^n} =0\implies a_1 +2 a_2 x+3 a_3x^2+…-x^{a_0}x^{a_1x}x^{a_2 x^2}\cdot …=0$$ If we were to assume that $\{a_n\}=0$ , then this would not work as we have the coefficients in the exponent. For the other differential equation, one can find the following power series using an experimental infinite Multinomial Theorem . I will also use the aforementioned Pi Product notation and factorial definition of the multinational for a representation in terms of more common functions. $n_m$ is just the mth index with the condition that the sum of all infinite number of indices is $x$ . There may be a typo: $$y’=y^x,y=\sum_{n\ge0} a_n x^n\implies\sum_{n\ge1} a_n nx^{n-1}=\left(\sum_{n\ge0}a_n x^n\right)^x\implies \sum_{n\ge1} a_n nx^{n-1}-\sum_{\sum\limits_{m\ge1} n_m=x}\frac{x!\prod\limits_{t\ge0} a_t^{n_m}x^{tn_m}}{\prod\limits_{k\ge 1}n_m!}=0$$ Here is an attempt to put $y’=x^y$ into a more manageable form using a similar method as @Cesareo’s keeping restrictions in mind. This is the corrected answer: $$y’=x^y\implies y’’=x^{y-1}(x\ln(x)y’+y)\implies y’=x^y=\frac{xy’’}{x\ln(x)y’+y}\implies xy’’=x\ln(x)y’^2+yy’\implies\boxed{ xy’’-x\ln(x)y’^2-yy’=0}$$ which has no apparent solution so ho would I solve $$xy’’-x\ln(x)y’^2-yy’=0$$ I can think of a power series which may or may not need $2$ different indices: $$\sum_{n=2}^\infty a_n n(n-1)x^{n-1}-x\ln(x)\sum_{n=1}^\infty a_n nx^{n-1}\sum_{n=1}^\infty a_n nx^{n-1}-\sum_{n=0}^\infty a_n x^n \sum_{n=1}^\infty a_n nx^{n-1}=0$$ We are so close to solving, so do you have any ideas? Please correct me and give me feedback!","['nonlinear-analysis', 'exponentiation', 'ordinary-differential-equations', 'power-series', 'fourier-series']"
4269062,"Are there ""tight"" (as possible) upper bounds for $\max_t{\left\{ \left|\frac{d f(t)}{dt}\right|\right\}}$ for time-limited functions???","The main objective is to find some upper bound for the maximum rate of change as ""tight"" as possible, hopefully related to characteristics of the functions as its signal energy, or its higher Fourier coefficient, or something easy to obtain from the function itself. Also, figure out what is needed for a finite-energy time-limited signal (so, with infinite bandwidth), to have a bounded rate of change $\max_t |\frac{df(t)}{dt}|<\infty$ . For some partial answers you can go directly to my 2nd answer here Following the notation of exercise 4.49 of the book ""Signals and Systems, 2nd Edition"" (Alan V. Oppenheim, Alan S. Willsky, with S. Hamid) [ 1 ], the Fourier Transform is defined as $F(j \omega) = \int_{-\infty}^{\infty} f(t) e^{-j \omega t} dt$ , so the function can be described as $f(t) = \frac{1}{2 \pi} \int_{-\infty}^{\infty} F(j \omega) e^{j \omega t} d\omega$ , where $j = \sqrt{-1}$ . Let $f(t)$ being a function which fulfill the conditions to have a Fourier Transform $F(j \omega)$ . Then using the composition of a complex number in their amplitude and phase, and the “triangle inequality”, I can establish the following (here $|\cdot |$ is the absolute value): $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left| \left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t}\right| d\omega \right\} \\ & \leq \max_{t}\left\{ {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j \right|\left|\omega F(j\omega)\right|\left| e^{j\sphericalangle\left(j\omega F(j\omega )\right)}\right| \left| e^{j \omega t}\right| d\omega \right\} \\ & = {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega \texttt{     (Eq. 2)}\end{split}\end{equation}$$ since $1 = |e^{j\phi}|, \forall \phi \in \mathbb{R}$ (note that $j = e^{j\frac{\pi}{2}}, \omega t \in \mathbb{R},$ and angle $ \sphericalangle\left(j\omega F(j\omega )\right) \in \mathbb{R}$ ), and the remaining integral is independent of $t$ . In the book ""Fourier Series: A Modern Introduction - Volume 1 (2nd Edition)"" (R. E. Edwards) [ 2 ], on Chapter 2 point 2.3.6 (point (3) of the ""Remarks"") is proved that if $f(t)$ is of bounded variation, and $\hat{f}(n)$ is its Fourier transform (defined differently from here), then: $$ \left|{n \hat{f}(n)}\right| \leq V(f), \texttt{     (Eq. 5)}$$ with $V(f)$ the total variation of $f(t)$ . Since the total variation for a Riemann integrable function $f(t)$ with $M < \infty$ can be defined as: $$ V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt $$ I believe that the bound $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq V(f)$ is going to be ""too loose"", because is the same situation as considering the sum of a series of positive coefficients $a_n \geq 0$ and asking to compare $\max_{n}\{a_n\} \leq \sum_{-\infty}^{\infty}a_n$ (!) . (!): Caution is needed, because the sum is under an integral, so the "" $dt$ "" could change the intuition. As example, consider the ramp function that starts at the origin and ends at the point $(1/2,\pi)$ , its max value is $\pi$ but the sum of the area under the curve is $\sqrt{\pi/2}/2 < \pi$ , but on converse, if the edge is on $(2,\pi)$ it’s area under the curve will be $2 \sqrt{\pi} > \pi$ ). So I want to know: A. Is the bound ${\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega$ tight ""enough""? or It will be a ""loose one"" as $V(f)$ ? Here I know that: $$ \frac{df(t)}{dt}\Big\vert_{t=0} = {\frac{1}{2\pi}} \int_{-\infty}^{\infty} j\omega F(j\omega) d\omega $$ by making $t=0$ in the exponent $e^{j \omega t} = e^0 = 1$ of the inverse Fourier transform definition, so I have hope is not ""too loose"". For avoiding any kind of ""strange behavior"" like continuous functions nowhere differentiable or differentiable functions nowhere continuous, and the full zoo of functions in between, please consider that the functions $f(t)$ is as following: $$f(t) = x(t) \cdot (\theta(t-t_0) - \theta(t-t_F))$$ is a non-constant one-variable real-valued function defined for every $t \in (-\infty; \infty)$ with $\theta(t)$ the unitary step function and $t_0<t_F$ , so $f(t)$ haves a beginning at $t_0$ and an end at $t_F$ , being $f(t) = 0 \text{ if } t\leq t_0 \text{ or } t_F \geq t$ , letting the property $\mathbb{F}\left\{\frac{df(t)}{dt}\right\} = j\omega F(j \omega)$ being true. Let $f(t)$ be a Lebesgue integrable function: $\int_{-\infty}^{\infty}|f(t)|dt < \infty$ , and also a finite energy function: $\int_{-\infty}^{\infty}|f(t)|^2dt < \infty$ . If needed, also Riemann integrable. Consider that function $x(t)$ is continuous, also smooth so all derivatives exists and are bounded (or at least, one time differentiable), so using $\max$ or $\sup$ or else is equivalent (same for $f(t)$ except at the points $f(t_0)$ and $f(t_F)$ ), and also that $f(t)$ is of bounded variation. In the same mentioned point of [2], at ""Remarks"" section, is said that Wiener have proved that for a bounded function to be continuous if and only if it behave as: $$ \lim_{N \to \infty} \frac{1}{N} \sum_{|n| \leq N} \left|n \hat{f}(n) \right| = 0 \texttt{     (Eq. 6)}$$ Also assume that the function $f(t)$ follows the Rieman-Lebesgue Lema [ 3 ], and the conditions needed to have a Fourier transform $F(jw)$ described by a function -not by a distribution as Dirac's delta $\delta(t)$ or others- so that the Paley–Wiener theorem is fulfilled [ 4 ]. I would like to represent ""naively"" physically possible time-limited phenomena with $f(t)$ , so I don't want in principle, to put restrictions to $f(t_0)$ or $f(t_F)$ , but if needed, first start with $f(t_F)=0$ , and as last resource, add $f(t_0)=0$ , making $f(t)$ compact-supported but not necessarily $f(t) \in C_c^{\infty}$ , since to be a Bump function it also requires that $\lim_{t->\partial t} \frac{d^n f(t)}{dt^n} = 0$ so every derivative is continuous at the boundaries - and just if nothing else is possible, let $f(t)$ be a Bump function $f(t) \in C_c^{\infty}$ . I don't know if there exists a space of non-analytic $C_c^{\infty}$ -alike functions that can have $f(t_0)\neq 0$ and/or $f(t_F)\neq 0$ or both, if exists, please let me know how is called and any reference to search for them (I left it as a separated question in here ). Since $\frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F)) + x(t)\cdot\delta(t-t_0) -  x(t)\cdot\delta(t-t_F)$ , when looking for $\max_t |\cdot|$ , it will be ""infinite"" because $\delta(t) = \infty$ at $t=0$ . Because of this, I am explicitly avoiding the discontinuity at the edges, so $t_0 < t < t_F$ could let me work with $\frac{df(t)}{dt} = \frac{dx(t)}{dt}\cdot (\theta(t-t_0) - \theta(t-t_F))$ , so $\max_{t_0 < t < t_F} |\frac{df(t)}{dt}| = \max_{t_0 < t < t_F} |\frac{dx(t)}{dt}|$ . If the bound is applicable for more general functions, please let me know which constraints you have removed. B) What other tight bounds for $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\}$ are known?? Since using the same argument of the main equations I will have that $\max_{t}\left\{\left|{f(t)}\right|\right\} \leq {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|F(j\omega)\right| d\omega \leq \int_{-\infty}^{\infty}\left|f(t)\right| dt \textit{  ¿}\leq\textit{?} \sqrt{\int_{-\infty}^{\infty}\left|f(t)\right|^2 dt} = \sqrt{E_0}$ , so I am trying to find something proportional somehow to the energy $E_0 = \int_{-\infty}^{\infty}\left|f(t)\right|^2 dt$ of the function $f(t)$ , even tried to multiply by $\frac{E_0}{E_0}$ to form things of the fashion of $\int_{-\infty}^{\infty}\frac{\left|F(j \omega)\right|^2}{E_0} d\omega = 1$ so $g(\omega)= \frac{\left|F(j \omega)\right|^2}{E_0}$ could be think as a probability distribution and use bounds for the expected value $E_g[\omega]$ and $E_g[\omega^2]$ with unsuccessful results. I have found on internet some bounds as the Kalman-Rota or the Landau-Kolmogorov-Hadamard inequalities that states that $||f'||_2 \leq \sqrt{2} ||f||_2^{1/2}||f''||_2^{1/2}$ [ 5 ], but my intuition says that commonly $\max_{t}|f'| \ll \max_{t}|f''|$ .
I also found other inequalities like Poincare's, Sobolev's, Friedrichs's, or Uncertainty Principle relations, but the inequality goes on the other direction $\textit{something}(f) \leq \sup |f'|$ . On the comments were mentioned some bounds applicable to band-limited functions (Bernstein inequality [ 11 ], Markov brothers' inequality [ 12 ], Others [ 13 ], follow the main article [ 14 ]), but since here I am asking about time-limited functions , which are going to have unbounded domain on the frequencies [ 10 ], I believe they are not applicable. C) What restrictions have to fulfill $f(t)$ so it happen to be true that $\max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} \leq \max_{\omega}\left\{\left|\omega \cdot F(j \omega)\right|\right\}$ ???? I have tried with a few functions and it happen to be true, so if you know of any demonstration related please share any reference. I also found counterexamples for finite-energy time-limited, so I want to know which conditions must happen to make it “useful”. This bound ""conjecture"" come from the following mistake: $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & = \max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}j\omega F(j\omega)e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 1)} \\ & =\max_{t}\left\{ \left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|j\omega F(j\omega)\right| e^{j\sphericalangle\left(j\omega F(j\omega )\right)} e^{j \omega t} d\omega \right| \right\} \end{split}\end{equation}$$ Where, if I let $M_\omega^* = \max_\omega |j\omega F(j \omega)|$ which happens at $\omega^* = \arg \max |j\omega F(j \omega)|$ , then at this I will have that $e^{j\sphericalangle\left(j\omega F(j\omega )\right)} = e^{j \phi^*}$ for some $\phi^* \in \mathbb{R}$ , so: $$ \begin{equation}\begin{split} M = \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} & \leq? \max_{t}\left\{ \left|{\frac{M_\omega^* \cdot e^{j \phi^*}}{2\pi}} \int_{-\infty}^{\infty} e^{j \omega t} d\omega \right|\right\} \texttt{     (Eq. 3)} \\ & \leq \max_{t}\left\{ M_\omega^* |e^{j \phi^*}|\left|{\frac{1}{2\pi}} \int_{-\infty}^{\infty}e^{j \omega t} d\omega \right| \right\} \\ & = M_\omega^* \max_{t}\left\{\left|\delta(t) \right| \right\} \\ &  = \max_\omega |j\omega F(j \omega)| \max_{t}\left\{\left|\delta(t) \right| \right\} \texttt{     (Eq. 4)}\end{split}\end{equation}$$ In Eq. 4 the result will be $0$ for $t \neq 0$ , and "" $\infty$ "" if $t=0$ , so certainly is not a rightfully obtained bound, but ignoring somehow the delta function makes me wonder about the value of $$M_\omega^* = \max_\omega |j\omega F(j \omega)| \texttt{     (Eq. 7)}$$ . Added later: Following a suggestion, I am going to review some examples. (!!) I solved these examples using Wolfram-Alpha website [ 7 ], which works by default with a different definition of the Fourier transform, so be careful about it. I don't review if the results are ""theoretically"" right, and I have already found examples where Wolfram-Alpha gives numerically wrong results. Gaussian function example_______________________ First one, the case of the Gaussian function: Let $f(t)$ a less restricted function that is not time-limited, but vanishes at infinity, $f(t)=\frac{1}{\sqrt{2\pi}} e^{-\frac{t^2}{2}}$ the standard Gaussian function distribution so $\int_{-\infty}^{\infty} f(t) dt = \int_{-\infty}^{\infty} |f(t)| dt = 1$ and its signal energy is finite $\int_{-\infty}^{\infty} |f(t)|^2 dt = \frac{1}{2\sqrt{\pi}} \approx 0.282 \ll \infty$ . Also, following “our” notation, the non-unitary Fourier Transform for the angular frequency $\omega$ of $\mathbb{F_t}\left\{ e^{-a t^2}\right\}(\omega) = \sqrt{\frac{\pi}{a}} e^{\frac{-\omega^2}{4a}}\,$ , so for this $f(t)$ we have $F(j\omega)=e^{-\frac{\omega^2}{2}}$ (the property that the Fourier Transform of Gaussians is also Gaussian - unbound domain in time and frequency, vanishing at infinity in both). Then, the following is true: $$ \begin{equation}\begin{split} M_{std.gauss} & = & \max_{t}\left\{\left|{\frac{d f(t)}{dt}}\right|\right\} = \frac{1}{\sqrt{2\pi e}}\text{ on } t^* = \pm 1 & \approx 0.24197 \\ & \leq & {\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega  = \frac{1}{\pi}  & \approx 0.31831 \\ & \leq & \max_{\omega}\left\{\left|{\omega F(j\omega)}\right|\right\}  = \frac{1}{\sqrt{e}} \text{ on } \omega^* = \pm 1  & \approx 0.60653 \\ & \leq & V(f) = \int_{-\infty}^{\infty} \left| \frac{df(t)}{dt} \right| dt  = \sqrt{\frac{2}{\pi}} & \approx 0.79788 \end{split}\end{equation}$$ Is interesting to note that both bounds of Eq. 2 and Eq. 7  have worked better than $V(f)$ . Also note that the bound Eq. 2 is “much tighter” than the bound of Eq. 7, so it’s happened what was commented on (!) . Is interesting to think that the Gaussian is the function that maximizes the Uncertainty Principle, so no other one-variable function with the same energy (since is a one parameter function), can be more concentrated in both time and frequency domains at once ([ 9 ] and [ 10 ]), so my intuition says that time-limited functions (which can be thought as convolution on the frequency domain of a standard function with a ""sinc function""), are going to be even more spread in the frequencies, so it is going to be less likely to found a higher peak for $|\omega F(j\omega)|$ that the ones is achieved by the Gaussian function with the same energy. But anyway, in this example it can be seen that this ""conjecture"" is not a total nonsense. Classic functions examples_____________________ Here I review the simplest cases of traditional functions which its Fourier transforms are tabulated in [1] and in Wikipedia [4]. Knowing beforehand they don't fulfill my requirements, it will be a logic start I think, since many people had worked with them before. The following notation is used from now on: "" $E°$ "" is used for each signal energy (definition on each table), $\Pi(t) = 1, |t|\leq \frac{1}{2}$ is the standard rectangular function ( Unitbox(t) in Wolphram-Alpha ), $\delta(t)$ is the Dirac's delta distribution ( Diracdelta(t) in Wolphram-Alpha ), $\theta(t)=1, t \geq 0$ is the standard step function ( Unitstep(t) in Wolphram-Alpha ), $\Lambda (t)=1-|t|, |t|\leq 1$ is the standard triangular function ( Unittriangle(t) in Wolphram-Alpha ), $H_1(t)=2\cdot t\,$ is the Hermite polynomial of the first kind, which fulfill the equation $H_n(t) = (-1)^n\,e^{t^2}\frac{d^n}{dt^n}(e^{-t^2})$ ( HermiteH(1,t) in Wolphram-Alpha ), and $\mathscr{C}=0.91596559\cdots\,$ is the Catalan's constant. I have let with (*) the results I believe have questionable accuracy. $$ 
\begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|} 
\hline 
f(t) & \text{dom}(f(t)) & F(j\omega)=\mathbb{F}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_t |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V(f) = \int_{-\infty}^{\infty} |f'(t)|dt & \max_t |f(t)| & ||f||_1 = \int_{-\infty}^{\infty} |f(t)|dt & E° = \int_{-\infty}^{\infty} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline
\Pi (t) &  [-\frac{1}{2}; \frac{1}{2}] & \text{sinc}(\frac{\omega}{2}) & (-\infty; \infty) & \infty^* & \infty^* & 2 & 0^* & 1 & 1 & 1 & 1 \\ \hdashline
\text{sinc}(\frac{t}{2}) &  (-\infty; \infty) & 2\pi \cdot \Pi (\omega) & [-\frac{1}{2}; \frac{1}{2}] & 0.218 & \frac{1}{4} = 0.25 & \pi = 3.1416 & \infty & 1 & \infty & 2\pi = 6.2831 & 2.5066 \\ \hdashline
\text{sinc}^2(\frac{t}{2}) &  (-\infty; \infty) & \Lambda (\omega) & [-1; 1] & 0.27 & \frac{1}{3} = 0.33 & \frac{\pi}{2} = 1.57079  & \infty & 1 & 2\pi = 6.2831 & \frac{4\pi}{3} = 4.18879 & 2.046 \\ \hdashline
\Lambda (t) &  [-1; 1] & \text{sinc}^2(\frac{\omega}{2}) & (-\infty; \infty) & undefined & \infty & 1.44922 & 2^* & 1 & 1 & \frac{2}{3} = 0.66 & 0.816 \\ \hdashline
e^{-t} \cdot \theta (t) &  [0; \infty) & \frac{1}{(1+j\omega)} & (-\infty; \infty) & 1^* & \infty & 1^* & 1^* & 1 & 1 & \frac{1}{2} = 0.5 & 0.707 \\ \hdashline
t\cdot  e^{-t} \cdot \theta (t)&  [0; \infty) & \frac{1}{(1+j\omega)^2} & (-\infty; \infty) & 1^* & \infty & \frac{1}{2} = 0.5 & \frac{2}{e}^* = 0.735759^* & \frac{1}{e} = 0.3678 & 1 & \frac{1}{4} = 0.25 & \frac{1}{2} = 0.5 \\ \hdashline
\frac{1}{\sqrt{2\pi}}e^{-\frac{t^2}{2}} &  (-\infty; \infty) & e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & \frac{1}{\sqrt{2 e \pi}} = 0.24 & \frac{1}{\pi} =  0.31831 & \frac{1}{\sqrt{e}} = 0.606 & \sqrt{\frac{2}{\pi}} = 0.797 & \frac{1}{\sqrt{2\pi}} = 0.39 & 1 & \frac{1}{2\sqrt{\pi}} = 0.282 & 0.531 \\ \hdashline
\frac{1}{\sqrt{\pi}}e^{-j\frac{t^2}{2}} &  (-\infty; \infty) & (1-j)\cdot e^{j\frac{\omega^2}{2}} & (-\infty; \infty) & \infty & \infty & \infty & \infty & \frac{1}{\sqrt{\pi}} = 0.564 & \infty & \infty & \infty \\ \hdashline
e^{-|t|} &  (-\infty; \infty) & \frac{2}{(1+\omega^2)} & (-\infty; \infty) & 1^* & \infty & 1 & \infty^* & 1 & 2 & 1 & 1 \\ \hdashline
e^{-\frac{t^2}{2}}H_1(t) &  (-\infty; \infty) & -j\omega \cdot 2\sqrt{2\pi}\cdot e^{-\frac{\omega^2}{2}} & (-\infty; \infty) & 2 & 2 & \frac{4\sqrt{2\pi}}{e} = 3.68 & \frac{8}{\sqrt{e}} = 4.8522 & \frac{2}{\sqrt{e}} = 1.213 & 4 & 2\sqrt{\pi} = 3.5449 & 1.883 \\ \hdashline
\text{sech}(t) & (-\infty; \infty) & \pi \cdot \text{sech}(\frac{\pi \omega}{2}) & (-\infty; \infty) & \frac{1}{2} = 0.5 & \frac{8 \mathscr{C}}{\pi^2} = 0.74245 & 1.32549 & 2 & 1 & \pi = 3.1416 & 2 & 1.414 \\ \hline
\end{array}
$$ Unfortunately, the upper bound of Eq. 2 diverges on many examples (I wasn´t expecting it to work for these examples anyway), but for the cases where it results to be finite, it shows to be much better than the bound given by Eq. 5. On the other hand, the upper bound of Eq. 7 results to be finite on much more examples, and also lower than the bound of Eq. 5, but much closer to it. Also, for the example $f(t) = t\,e^{-t}\theta(t)$ it shows to be lower than the maximum rate of change. This is why I am asking for which conditions this bound have to fulfill to become a valid upper bound (I don't expect that this bound is going to be valid for every possible function, but it could be an alternative when the bound of Eq. 2 doesn't converges). Also note that for the same case of $f(t) = t\,e^{-t}\,\theta(t)$ the maximum rate of change results to be higher than the Total Variation $V(f)$ , going against my intuition at least, so maybe the validity of the bound of Eq. 5 is not universal (could be related to the Fourier transform definition used in [2] being different from the one I am using here, or also a numerical issue). Bounded domain functions examples________________ Here I review some simple cases of time limited signals. I have tried to find simple cases of continuous functions: signals that starts from $0$ and rises/decline “slowly” ( $f(t)=f’(t)=0$ ), others that starts “sharply”, other that starts from a point different than $0$ , odd and even signals, signals with a discontinuity, the same signal with different compact domain, positive signals, signals with flat-top, etc. (some signals which I could found a simple Fourier transform to work with). Unfortunately there is no case of a proper “smooth function” since I don’t found any simple “bump function” $\in C_c^\infty$ with a simple Fourier transform, both in “closed form” (I left the question here , and already test all the functions of here on Wolfram-Alpha with negative results). I extend the previous table notation with these: $J_1(t)$ is the Bessel function of the first kind of order 1 ( BesselJ(1,t) in Wolphram-Alpha ), $\text{Si}(t)$ is the “Sine integral” function ( SinIntegral(t) in Wolphram-Alpha ) Also, since for time limited functions the domain is restricted to $t_0 \leq t \leq t_F$ , the definitions for the Fourier transform changes to $F(j \omega) = \int_{t_0 }^{t_F} f(t) e^{-j \omega t} dt$ , and also all the time domain integrals change its integration limits correspondingly (see headers of the table 2). Be attentive of this, since the Fourier transform of the functions are different with other integration limits. Also related, as explained in point (5) for avoiding the problem at the domain ""edges"", I use $\max_{t_0 < t < t_F} |f'(t)|$ without including the boundaries. Again, I have let with (*) the results I believe have questionable accuracy. $$ 
\begin{array}{|c:c|c:c|c|c:c:c|c:c:c:c|} 
\hline 
f(t) & \text{dom}(f) = [a\,;\,b] & F(j\omega)=\mathbb{F}_{[a\,;\,b]}\{f(t)\}(\omega) & \text{dom}(F(j\omega)) & \max_{a < t < b} |f'(t)| & \frac{1}{2 \pi} \int_{-\infty}^{\infty} |j\omega F(j\omega)|d\omega & \max_\omega |j\omega F(j \omega)| & V_a^b(f) = \int_{a}^{b} |f'(t)|dt & \max_{a\leq t \leq b} |f(t)| & ||f||_1 = \int_{a}^{b} |f(t)|dt & E° = \int_{a}^{b} |f(t)|^2 dt & ||f||_2 = \sqrt{E°} \\  \hline
\sqrt{1-t^2} & [-1; 1] & \pi \cdot \frac{J_1(\omega)}{\omega} & (-\infty; \infty) & \infty & \infty^* & 1.82798 & 2 & 1 & \frac{\pi}{2} = 1.57079 & \frac{4}{3} = 1.33 & 1.1547 \\ \hdashline
\sin(\frac{t\pi}{2}) & [-1; 1] & -j\frac{8\,\omega\cos(\omega)}{(\pi^2-4\,\omega^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.75144 & 2 & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hdashline
\sin^2(\frac{t\pi}{2}) & [-1; 1] & \frac{(\pi^2-2\,\omega^2)\sin(\omega)}{(\pi^2\,\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 2.8929 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline
\cos^2(\frac{t\pi}{2}) &  [0; 1] & j\frac{(\pi^2(1-e^{-j\omega})-2\,\omega^2)}{2\,\omega\,(\omega^2-\pi^2)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & \infty^* & 1.4562 & 1 & 1 & \frac{1}{2} = 0.5 & \frac{3}{8} = 0.375 & 0.612 \\ \hdashline
\cos^2(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2\sin(\omega)}{(\pi^2\omega-\omega^3)} & (-\infty; \infty) & \frac{\pi}{2} = 1.57079 & 2.28547^* & 1.63641 & 2 & 1 & 1 & \frac{3}{4} = 0.75 & 0.866 \\ \hdashline
\frac{(1+\cos(t\pi))^2}{4} & [-1; 1] & \frac{3\,\pi^4\sin(\omega)}{(\omega^5-5\pi^2\omega^3+4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 2.61265^* & 1.58242 & 2 & 1 & \frac{3}{4} = 0.75 & \frac{35}{64} = 0.546875 & 0.7395 \\ \hdashline
\sin(\frac{t\pi}{2})\cos^2(\frac{t\pi}{2}) & [-1; 1] & j\frac{16\, \pi^2\, \omega \cos(\omega)}{(16\, \omega^4-40\, \pi^2 \omega^2+9\,\pi^4)} & (-\infty; \infty) & 1.5708 & 1.93647^* & 1.23244 & 1.5396^* & \frac{2}{3\sqrt{3}} = 0.3849 & \frac{4}{3\sqrt{\pi}} = 0.42441 & \frac{1}{8} = 0.125 & 0.354 \\ \hdashline
\text{sinc}(t\pi)\cos(\frac{t\pi}{2}) & [-1; 1] & \frac{1}{2\pi}\left(\text{Si}(\frac{\pi}{2}-\omega)+\text{Si}(\frac{3\pi}{2}-\omega)+\text{Si}(\frac{\pi}{2}+\omega)+\text{Si}(\frac{3\pi}{2}+\omega)\right) & (-\infty; \infty) & 1.62897 & \infty^* & 1.61724 & 2 & 1 & \frac{2\,\text{Si}(\pi)}{8} = 1.17898 & \frac{2\,\text{Si}(2\pi)}{\pi} = 0.9028 & 0.9501 \\ \hdashline
1-\sin^4(\frac{t\pi}{2}) & [-1; 1] & \frac{\pi^2(5\pi^2 - 2\,\omega^2) \sin(\omega)}{(\omega^5 - 5\pi^2\omega^3 + 4\pi^4\omega)} & (-\infty; \infty) & \frac{3\sqrt{3}\pi}{8} = 2.0405 & 3.01547^* & 1.8225 & 2 & 1 & \frac{5}{4} = 1.25 & \frac{67}{64} =  1.04688 & 1.023 \\ \hdashline
\sin(|t|\pi) & [-1; 1] & \frac{2\pi\,(1+\cos(\omega))}{(\pi^2-\omega^2)} & (-\infty; \infty) & \pi = 3.1416 & \infty^* & 2.90769 & 4^* & 1 & \frac{4}{\pi} =1.2732 & 1 & 1 \\ \hline
\end{array}
$$ Against my expectation, the time limited functions behave much worst I intended, but because they are following my intuition: it decays slowly than the Gaussian function, but so slowly that the integral bound of Eq. 2 diverges in many cases, and also so spread, that the peak given by the bound of Eq. 7 is lower than the maximum rate of change. In many cases, the maximum rate of change results higher than the Total Variation $V_a^b(f)$ , against Eq. 5. The positive side, when the integral of Eq. 2 converges, it results to be a proper bound of the maximum rate of change, even when it was higher than the Total Variation. Unfortunately, all the results for the bound of Eq. 2 have “*” since its validity is questionable, and its value were obtained by numerical approximation through Nintegrate in Wolfram-Alpha, so I will review another bound to ""double check"".
Using Hölder’s inequality, it can be stated that for two functions $f(t)$ and $g(t)$ the following is true: $$\int_{-\infty}^\infty |f(t) \cdot g(t)|\,dt \leq \int_{-\infty}^\infty |f(t)|\,dt \cdot \sup_t |g(t)|$$ I am going to applied it to the bound of Eq. 2 to avoid the multiplication: $$\frac{1}{2 \pi} \int_{-\infty}^\infty |\omega \, g(\omega) \cdot \frac{F(j \omega)}{g(\omega)}|\,d\omega \leq \frac{1}{2 \pi} \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega \cdot \sup_\omega |\frac{F(j \omega)}{g(\omega)}| \texttt{     (Eq. 8)}$$ So, I tested some functions $g(\omega)$ that makes the integral of the right side converge, and then see if they have a supremum when tested against the function $f(t) = \cos^2(\frac{t\pi}{2}),\,|t| \leq 1\,$ : $$\begin{array}{|c|c:c|} \hline 
g(\omega) & \int_{-\infty}^\infty |\omega\, g(\omega)|\,d\omega &  \texttt{(Eq. 8)} \\ \hline 
e^{-\sqrt{|w|}} & 24 & 12.1623 \\ \hline 
\end{array} $$ Here is really interesting that having a ""clear result"" for $g(\omega) = e^{-\sqrt{|w|}}$ proves that the results of table 2, maybe numerical inaccurate, are still valid (at list for the chosen $f(t)$ ), so there exists time-limited functions for which the integral of Eq. 2 converges. But not only this, also it proves something maybe obvious: There exists TIME-LIMITED functions (with unbound domain on the frequencies), which have a maximum rate of change bounded , (for functions with bounded domain in the frequencies, the results is known and presented in [14]) but better, seen the same problem for a different “not so obvious angle”, I believe it could be reinterpreted as: There exists some “ $\text{mysterious conditions}\,\mathbb{X}$ ” that makes that some time-limited signals with unlimited bandwidth will have a bounded maximum rate of change , conditions I am trying to figure out . Additionally, there other issue I found reviewing the results of table 2: for every function a tried with values at the “edges” of its domain different from zero, the integral of Eq. 2 diverges. This is confusing for me, especially for the functions $f(t) = \cos^2(\frac{t\pi}{2})$ when changing the domain from $[-1,\,1]$ to $[0,\,1]$ : technically there is no differences in the achieved maximum slopes of the curves, however for the reduced domain one, the integral of Eq. 2 diverges. I think it could be related to the phenomena I am avoiding according point (5), but somehow it manifests on the frequency domain. I tried to solve it by changing the function to $f(t) = \theta(-t)+\cos^2(\frac{t\pi}{2}),\, 0 \leq t \leq 1\,$ so no disruptive changes happen to the function, but as I was expecting its Fourier Transform in $[0,\,1]$ was the same to the previous functions (it was adding just a zero-measure point, so it doesn’t change the integral). I have tried many other different functions and combinations on $[-1,\,1]$ with $f(-1) \neq 0$ , and I couldn’t find anyone where the integral of Eq. 2 converges. It doesn’t mean that its maximum rate of change was unbounded (Eq. 2 is an upper bound), but its seems as a new conjecture: for a time-limited function $f(t)$ to have a finite ${\frac{1}{2\pi}} \int_{-\infty}^{\infty}\left|\omega F(j\omega)\right| d\omega < \infty $ it must have value zero at its domain boundaries $f(t_0) = f(t_F) = 0$ (don’t meaning this, it need to be a bump function $\in C_c^\infty$ , which it’s much more restrictive). I left this into another question in here . Continues in answers section...","['fourier-analysis', 'real-analysis', 'upper-lower-bounds', 'derivatives', 'piecewise-continuity']"
4269074,Maximum distance between mid-point of chord of ellipse,"Let $E$ be the ellipse $\frac{x^2}{16}+\frac{y^2}{9}=1$ . For any three distinct points $P,Q$ and $Q′$ on $E$ , let $M(P, Q)$ be the mid-point of the line segment joining $P$ and $Q$ , and $M(P, Q′)$ be the mid-point of the line segment joining $P$ and $Q′$ . Then the maximum possible value of the distance between $M(P,Q)$ and $M(P,Q′)$ , as $P,Q$ and $Q ′$ vary on $E$ , is ___ . My approach is as follow Let $P(4cos\alpha,3sin\alpha)$ , $Q(4cos\theta_1,3sin\theta_1)$ , $Q′(4cos\theta_2,3sin\theta_2)$ $M(P, Q)=(\frac{4(cos\theta_1+cos\theta_2)}{2},)$ Let the mid point be represented as $M\left( {P,Q} \right) = \left( {\frac{{4\left( {\cos \alpha  + \cos {\theta _1}} \right)}}{2},\frac{{3\left( {\sin \alpha  + \sin {\theta _1}} \right)}}{2}} \right);M\left( {P,Q'} \right) = \left( {\frac{{4\left( {\cos \alpha  + \cos {\theta _2}} \right)}}{2},\frac{{3\left( {\sin \alpha  + \sin {\theta _2}} \right)}}{2}} \right)$ $\frac{1}{4}\sqrt {{{\left( {4\cos \alpha  + 4\cos {\theta _1} - \left( {4\cos \alpha  + 4\cos {\theta _2}} \right)} \right)}^2} + {{\left( {3\sin \alpha  + 3\sin {\theta _1} - \left( {3\sin \alpha  + 3\sin {\theta _2}} \right)} \right)}^2}} $ $\frac{1}{4}\sqrt {16\left( {{{\cos }^2}{\theta _1} + {{\cos }^2}{\theta _2} - 2\cos {\theta _1}\cos {\theta _2}} \right) + 9\left( {{{\sin }^2}{\theta _1} + {{\sin }^2}{\theta _2} - 2\sin {\theta _1}\sin {\theta _2}} \right)} $ $\frac{1}{4}\sqrt {16{{\cos }^2}{\theta _1} + 9{{\sin }^2}{\theta _1} + 16{{\cos }^2}{\theta _2} + 9{{\sin }^2}{\theta _2} - 32\cos {\theta _1}\cos {\theta _2} - 18\sin {\theta _1}\sin {\theta _2}} $ $\frac{1}{4}\sqrt {18 + 7\left( {{{\cos }^2}{\theta _1} + {{\cos }^2}{\theta _2}} \right) - 14\cos {\theta _1}\cos {\theta _2} - 18\left( {\cos \left( {{\theta _1} - {\theta _2}} \right)} \right)} $ ${\cos ^2}{\theta _1} = \frac{{1 + \cos 2{\theta _1}}}{2};{\cos ^2}{\theta _2} = \frac{{1 + \cos 2{\theta _2}}}{2}$ ${\cos ^2}{\theta _1} + {\cos ^2}{\theta _2} = 1 + \frac{{2\cos \left( {{\theta _1} + {\theta _2}} \right)\cos \left( {{\theta _1} - {\theta _2}} \right)}}{2} = 1 + \cos \left( {{\theta _1} + {\theta _2}} \right)\cos \left( {{\theta _1} - {\theta _2}} \right)$ $\frac{1}{4}\sqrt {18 + 7\left( {1 + \cos \left( {{\theta _1} + {\theta _2}} \right)\cos \left( {{\theta _1} - {\theta _2}} \right)} \right) - 7\left( {\cos \left( {{\theta _1} + {\theta _2}} \right) + \cos \left( {{\theta _1} - {\theta _2}} \right)} \right) - 18\left( {\cos \left( {{\theta _1} - {\theta _2}} \right)} \right)} $ $\frac{1}{4}\sqrt {25 + 7\cos \left( {{\theta _1} + {\theta _2}} \right)\left( {\cos \left( {{\theta _1} - {\theta _2}} \right) - 1} \right) - 25\left( {\cos \left( {{\theta _1} - {\theta _2}} \right)} \right)} $ How do we proceed from here , we get the official answer 4 when $\theta_1=0$ & $\theta_2=\pi$","['trigonometry', 'conic-sections']"
4269084,How to calculate sectional curvature?,"I'm some new here, sorry if my question is not correct. I need to calculate the sectional curvature of a Riemannian manifold. I found this formula in several books. $$K(\sigma,p)=\frac{R(X,Y,Y,X)}{g(X,X)g(Y,Y)-g(X,Y)}$$ My problem is that none of them do the detailed accounts, and it frustrates me because most that I have been able to read have a different notation. I am quite confused, I would like to know if anyone knows any book where I can find some text that makes a summary of the formulas (from defining the metric, Christoffel symbols , Riemannian tensor, etc.) and applies them in an example to understand. And if someone would be so kind to do it here, I would be very grateful, it does not matter if it is a simple example such as the sphere or hyperbolic space or a surface of revolution I don't know... I just want to see the technique to be able to understand and apply it.","['curvature', 'tensors', 'riemannian-geometry', 'differential-geometry']"
4269104,How these circles are congruent?,"Here is a problem involving curvilinear incircles and mixtilinear incircles. Let a triangle $\triangle$$ABC$ have circumcircle $\gamma$ .It's A-Excircle tangency point at side $BC$ is $D$ Let $\gamma_1$ be the circle tangent to $AD$ , $BD$ , $\gamma$ also $\gamma_2$ is the circle tangent to $AD$ , $CD$ ,and $\gamma$ .prove that $\gamma_1$ and $\gamma_2$ are congruent. I have tried to prove the converse of the problem. I drew two congruent circles on the same side of $BC$ , $\gamma_1$ and $\gamma_2$ tangent to $\gamma$ and $BC$ .Let the tangency points of $\gamma$ and $\gamma_1$ are $\alpha_1$ and the tangency point of $\gamma_2$ and $\gamma$ be $\alpha_2$ .Let the tangents of $\gamma$ at $\alpha_1$ and $\alpha_2$ intersect at point $M$ .Invert around the circle centered at $M$ and orthogonal to $\gamma$ .Then I got stuck. I have thought of another approach .according to Thebault's theorem, $P$ , $I$ , $Q$ are collinear where $P$ , $Q$ , $I$ are the centers of $\gamma_1$ , $\gamma_2$ and the incircle of $\triangle$ ABC. Then I tried to prove the congruency of the incircle and $\gamma_1$ also The congruency of the incircle and $\gamma_2$ .Also note that, $\angle$$BAD$ = $\angle$$CAK$ .where $K$ is the tangency point of $\gamma$ and the A-mixtilinear circle of $\triangle$$ABC$ . I have tried two-approach but failed to go any further. Can someone help me?","['contest-math', 'circles', 'geometry', 'sangaku', 'recreational-mathematics']"
4269106,How do you represent an axis as a vector?,"I am Alice. In a question I am doing, it requires me to find the directional derivative of function $f$ at $(2, 3, 4)$ , in the direction of positive $z$ axis. So I trying to figure out, how to represent an axis as a vector in general. In case, I want the positive $z$ , how can I do that? $z = (0, 0, 1)$ Would this suffice? Why? I don't understand. Thanks all.","['multivariable-calculus', 'vectors']"
4269107,"Prove $(x^y+y^x)\left(\frac{1}{x}+\frac{1}{y}\right)\ge 4$ where $x,y>0$.","Without loss of generality, we can assume $x\ge y$ . If $x\ge y \ge 1$ , then $$(x^y+y^x)\left(\frac{1}{x}+\frac{1}{y}\right)\ge
   (x+y)\left(\frac{1}{x}+\frac{1}{y}\right)\ge4.$$ If $1\ge x\ge y>0$ , then $$x^y=\frac{1}{\left(\frac{1}{x}\right)^{y}}=\frac{1}{\left(1+\frac{1-x}{x}\right)^y}\ge
   \frac{1}{1+y\left(\frac{1-x}{x}\right)}=\frac{x}{x+y-xy},$$ and
similarily $$y^x\ge\frac{y}{x+y-xy}.$$ Hence $$(x^y+y^x)\left(\frac{1}{x}+\frac{1}{y}\right)\ge \frac{\left(\frac{1}{x}+\frac{1}{y}\right)^2}{\frac{1}{x}+\frac{1}{y}+1}\ge 4.$$ If $x\ge 1 \ge y>0$ , how to go on?","['optimization', 'calculus', 'functions', 'inequality']"
4269126,Which cardinality has the given set?,"Find : Cardinality of $|\left\{ x \in \mathbb R : \space \text{the only digits in the fractional part of $x$ are } 2,3 \right\}|.$ Some examples : $1.22222222...$ $1.333333333...$ $4.232322223....$ My solution : I can look at the fraction part as $\left\{2,3\right\}^\mathbb N$ There are $\aleph_0$ numbers in the  integral part  of number. Then there are $|\left\{2,3\right\}^\mathbb N|$ = $|\left\{2,3\right\}|^{|\mathbb N|}=2^{\aleph_0}=\mathfrak c$ Then $\mathfrak c \cdot \aleph_0 = \mathfrak c.$ Excuse my bad English!",['elementary-set-theory']
4269177,barycentric coordinates of the foot of an altitude in a tetrahedron,"Given three sides $a=BC$ , $b=CA$ , $c=AB$ of a triangle $ABC$ . The foot $H$ on the segment $BC$ of the altitude $AH$ can be computed via (its proof is easy, using the Law of Cosine, for example) $$H=\dfrac{1}{a^2+c^2-b^2}B+\dfrac{1}{a^2+b^2-c^2}C.\tag{1}$$ In other words, the barycentric coordinate of $H$ with respect to $BC$ is $$\left(\dfrac{1}{a^2+c^2-b^2}:\dfrac{1}{a^2+b^2-c^2}\right).\tag{2}$$ Note that there is no $A$ in the above formulas.
As a consequence, we have another way of construction a triangle when 3 sides is given: take $C=(0,0)$ , $B=(a,0)$ , then take $H$ using that barycentric formula, and $A$ can be obtained by the Pythagorean theorem. Now construction a tetrahedron knowing $6$ sides leads to the following question. Question: Knowing three sides $a=BC$ , $b=CA$ , $c=AB$ of the base $ABC$ ` and three edges $a'=DA$ , $b'=DB$ , $c'=DC$ , how can we calculate the foot $H$ on the base $ABC$ of the altitude $DH$ without using apex $D$ ? I guess there is natural formula for 3-dimensional analog. However, I have not found the answer for that question. Numerical checking show that the formula $$H=\dfrac{1}{S_b^2+S_c^2-S_a^2}A+\dfrac{1}{S_c^2+S_a^2-S_b^2}B+\dfrac{1}{S_a^2+S_b^2-S_c^2}C$$ does not hold, where $S_b=S_{\Delta DCA}$ , $S_c=S_{\Delta DAB}$ , $S_a=S_{\Delta DBC}$ are areas of lateral faces (that can be calculated via $a$ , $b$ , $c$ , $a'$ , $b'$ , $c'$ ).
Thanks for any helps!","['triangles', 'geometry', 'barycentric-coordinates']"
4269189,Is there a connection between the analytical properties of complex analytic functions and the complex numbers being complete algebraically?,I will first have to apologize that this will be a very fuzzy question. At this time I have no better way to formulate it. I am willing to reformulate it when I can better pinpoint what it is that gives me this idea. Is there a connection between the analytic properties of complex valued functions and the complex numbers being complete algebraically? Meromorphic functions we can write as $$f(z) = \frac{\prod_k (z-h_k)}{\prod_k (z-p_k)}$$ We can show that any expression of the form  (thanks to the fund theorem of algebra) $$\frac{1}{f(z)}+\frac{1}{g(z)}=\frac{h(z)}{f(z)g(z)}$$ Where $z\to h(z)$ is ensured to be factorable. Does this have any connection to  the overly nice properties of complex analytic functions?,"['complex-analysis', 'abstract-algebra', 'meromorphic-functions', 'soft-question', 'complex-numbers']"
4269259,"Proving that, in any $\triangle ABC$, $a(1-\cos^2C)=c(\cos A \cos C + \cos B)$.","Question is as follows: Prove that in any $\triangle ABC$ , $$a(1-\cos^2C)=c(\cos A \cos C + \cos B)$$ I began with Sine rule, i.e. $$\frac{a}{\sin A}=\frac{b}{\sin B}=\frac{c}{\sin C}=k(say)$$ then $$\frac{a}{c}=\frac{k\sin A}{k\sin C}$$ $$=\frac{2\sin A \sin C}{2\sin ^2C}$$ $$=\frac{\cos (A-C) - \cos (A+C)}{2(1-\cos ^2C)}$$ $$\frac{\cos (A-C) + \cos (B)}{2(1-\cos ^2C)}$$ Final step is due to $\cos (A+C)=\cos (180^o-B)=-\cos B$ .
My hope was to rearrange $\frac{a}{c}$$\space$ to obtain result but I have been unable to do so. $\space$ I may well have made an error but if anyone can put me on the right track I would be extremely grateful. Many thanks in advance.","['euclidean-geometry', 'trigonometry', 'triangles']"
4269263,Enlarging a probability space to get independence,"Am reading Lemma 5.2 from this paper by Dvoretzky: Let $X$ be a real random variable on a probability space $(\Omega,\mathcal A,P)$ satisfying $|X|\leq 1$ ,  let $\mathcal F=\sigma(X)$ , and let $\mathcal G$ be a sub- $\sigma$ -algebra of $\mathcal A$ . Then $$E\Big[\big|E[X|\mathcal G]-E[X]\big|\Big]\leq 4\alpha(\mathcal F,\mathcal G) \quad\quad\quad (1)$$ where $\alpha(\mathcal F,\mathcal G)=\sup_{F\in\mathcal F, G\in\mathcal G} \big|P(F\cap G)-P(F)P(G)\big|$ . At some point in the proof the author writes: ""Let $\tilde{X}$ be a random variable with the same distribution as $X$ and independent of $\mathcal G$ (if need be the probability space can be enlarged in order to carry such a random variable)."" I don't see how to achieve this result, and how to do it without changing the quantities that appear in $(1)$ . Any ideas on how to proceed? Thanks a lot for your help. EDIT: I tried to write down the details below. Any comment is very appreciated.","['measure-theory', 'independence', 'conditional-expectation', 'probability-theory', 'probability']"
4269307,How powerful is Cayley's theorem?,So the Cayley's theorem gives a subgroup $H$ of $S_n$ for a $G$ such that $G$ is isomorphic to $H$ . So $S_n$ behaves like a Universal set to $G.$ Is there  a smaller universal object for all groups of size $n$ ?,"['symmetric-groups', 'group-theory', 'abstract-algebra', 'finite-groups']"
4269314,Characterize all solutions of the matrix equation $AX=B$ in terms of the SVD $A = U\Sigma V^T$.,"Let $A\in\mathbb R^{m\times n}$ , $B\in\mathbb R^{m\times k}$ and suppose $A$ has an SVD. Assuming $\mathcal R(B) \subseteq \mathcal R(A),$ characterize all solutions of the matrix linear equation $$AX = B$$ in terms of the SVD of $A$ . Attempt: We know that SVD of $A$ and $A^+$ are $A=U\Sigma V^T$ and $A^+ = V\Sigma^+U^T$ , respectively. Thus, all solutions of the matrix equation $$AX=B$$ are of the form $$X = (V\Sigma^+U^T)B +(I- (V\Sigma^+U^T)(U\Sigma V^T))Y.$$ To verify, simply calculate $AX = B$ with our new $X$ . Observe, $$
\begin{equation}\begin{split}
AX = (U\Sigma V^T)X &= (U\Sigma V^T)\bigg((V\Sigma^+U^T)B +\big(I- (V\Sigma^+U^T)(U\Sigma V^T)\big)Y\bigg) \\
&= (U\Sigma V^T)V\Sigma^+U^TB +(U\Sigma V^T)(I- V\Sigma^+\Sigma V^T) Y \\
&= U\Sigma \Sigma^+U^TB +(U\Sigma V^T)(\underbrace{I- VIV^T}_{0}) Y \\
&= U\Sigma \Sigma^+U^TB + 0\\
&= B.
\end{split}\end{equation}
$$ Is this what the question (which seemed trivial) was asking for? Feedback is welcomed! Note that the characterization of all solutions of $AX=B$ not using the SVD is of the form $$X = A^+B +(I- A^+A)Y$$ for $Y\in\mathbb R^{n\times k}$ . NOTE: ' $^+$ ' denotes Moore-Penrose pseudo-inverse.","['svd', 'vector-spaces', 'matrices', 'pseudoinverse', 'linear-algebra']"
4269319,Conditions for $P(X_1 + \cdots X_{n+1} = 0) < P(X_1 + \cdots X_n = 0) $ to hold for all $n$?,"Let $X_i$ be iid random discrete variables with pmf $f$ . We may restrict ourselve to pmfs with finite even support: $f \in G_N$ ( $ f[k] > 0 \implies |k| \le N$ ) or perhaps $f \in G^{+}_N$ ( $ f[k] > 0 \iff |k| \le N$ ) . I wonder if there is some characterization on $f$ (perhaps sufficient or necesary conditions) for this property to hold: $$P(X_1 + \cdots X_{n+1} = 0)  < P(X_1 + \cdots X_n = 0)  \, \forall n\ge 1$$ Alternatively, letting $f^{(n)}$ denote the $n-$ self convolution of $f$ , the above is equivalent to $f^{(n+1)}[0]  < f^{(n)}[0]  $ In by this question it's shown that $f \in G^{+}_1$ is not enough, but $g=f^{(2)}$ is.","['convolution', 'probability']"
4269345,How do I know what is under the sum symbol?,"I'm recently trying to understand maths more deeply, and I've always struggled with more complicated equations featuring multiple sum symbols. Particularly, I never can tell with exactity what is under the influence of a sum symbol. I usually determine this using context. Is there a way to explicitly state what is and isn't still under the sum influence? For example, in the inclusion-exclusion principle from Set theory: $$\biggl|\bigcup_{i=1}^n A_i\biggr|=\sum_{i=1}^n\left|A_i\right|-\sum_{i < j}\left|A_i\cap A_j\right|\>\>+\sum_{i < j < k}\left|A_i\cap A_j\cap A_k\right|-\ \cdots\ + \left(-1\right)^{n+1} \left|A_1\cap\cdots\cap A_n\right|$$ Is the second sum under the first one? Is the third sum under the second one? Is it at the same time still under the first one? Is this somehow implicitly indicated in this equation? Is there any explicit way of indicating this?","['elementary-set-theory', 'summation', 'notation']"
4269437,How to Evaluate $\int_{0}^{1}\frac{\tan^{-1}(x)}{1+x^4}dx$,"Is there a closed form for $$\int_{0}^{1}\frac{\tan^{-1}(x)}{1+x^4}dx \approx 0.349446 $$ It may also be represented as $$ \sum_{n=1}^{\infty}\sum_{k=1}^{\infty} \frac{(-1)^n \,(-1)^k}{(2n-1)(2n+4k-4)} $$ Using Mathematica the result is $$ \frac{\pi \ln(1+\sqrt{2})}{8\sqrt{2}} -\frac{\tanh^{-1}(\sqrt{2})\ln(2)}{8\sqrt{2}} + \frac{\phi(2,2,1/2)}{16}+\frac{i}{4\sqrt{2}}\left(\,\operatorname{Li}_{2}\,(i\sqrt2-i)-\operatorname{Li}_{2}\,(i-i\sqrt{2})\,\right)$$ where $\phi(z,s,a)$ is the Lerch transcendent https://mathworld.wolfram.com/LerchTranscendent.html $Li_n(x) $ is a polylogarithm https://en.wikipedia.org/wiki/Polylogarithm This result was highly unsatisfactory to me as it was not what i would expect based on the below integrals which have a nice closed form : $$ \int_{0}^{1} \frac{x^3 \tan^{-1}(x)}{1+x^4} dx = \frac{C}{2} - \frac{\pi \ln(1+\sqrt{2})}{8} $$ $$ \int_{0}^{1} \frac{x \tan^{-1}(x)}{1+x^4} dx = \frac{\pi^2}{32} - \frac{\ln(1+\sqrt{2})^2}{8} $$ Where $C$ is Catalan's Constant I tried to use integral techniques (by hand) to see if I could arrive at a different closed form hopefully without the special functions but was not successful. Q = Is there a closed form for the above integral? $\vdash$ Could the above closed form be simplified to get rid of the special functions? *** If duplicate question please do direct me to it **** Thank you very much for your help and time.","['integration', 'definite-integrals', 'sequences-and-series']"
4269440,Why do so few authors package up the definition of a limit into a function?,"Occasionally, you will literally see people arguing for nonstandard analysis purely to unnest the quantifiers in the definition of a limit. By unnesting, I mean avoiding an exists quantifier that is required to be nested inside the scope of the for all epsilon quantifier, and instead having only foralls that can be made implicit and constraints on the corresponding variables. NSA does this by just constraining $N$ to be infinitely large or $x$ to be infinitely close to $x_0$ . But there's a much simpler way to clean things up so that the quantifiers are no longer nested. Just define functions: A sequence $a_n$ in a metric space is convergent with limit L if there is a function $N : \mathbf{R^+} \to \mathbf{N}$ such that $d(a_n,L) < \varepsilon$ for all $\varepsilon > 0$ and $n > N(\varepsilon)$ . A sequence $a_n$ in a metric space is a Cauchy series if there is a function $N : \mathbf{R^+} \to \mathbf{N}$ such that $d(a_n,a_m) < \varepsilon$ for all $\varepsilon > 0$ and $n,m > N(\varepsilon)$ . A function $f : X \to Y$ between metric spaces has the limit $L = \lim_{x \to x_0} f(x)$ if there is a function $\delta : \mathbf{R^+} \to \mathbf{R^+}$ so that $d(f(x),L) < \varepsilon$ for any $\varepsilon > 0$ and $d(x,x_0) < \delta(\varepsilon)$ . which lets you write the definitions with only foralls that can be floated out to top level so that you just have one inequality between top level variables implying the other. Furthermore, actually requiring that this function be defined (occasionally called the modulus of convergence), is both very convenient in epsilon delta proofs, and actually necessary to have for anything related to numerics/computability/constructive analysis or if you generally care about ""how many terms do I need"" questions. So this made me curious. Is there any fundamental reason why you would want to write the definition of a limit in terms of nested quantifiers? I'd say tradition, but afaik first order logic is quite a bit newer than the definition of a limit. How did the 19th century analysts state the definition?","['constructive-mathematics', 'limits', 'education', 'real-analysis']"
4269452,Proving that $\Big(\sum_{k=1}^\infty a_k\Big)^p\leq c \sum_{k=1}^\infty \beta^ka_k^p.$,"Let $p>0$ and $\beta>1$ then there exists a constant $c=c(p,\beta)$ such for every sequence $(a_k)_k$ with $a_k\geq0$ we have $$\Big(\sum_{k=1}^\infty a_k\Big)^p\leq c \sum_{k=1}^\infty \beta^ka_k^p$$ The case $p\leq 1$ is obvious here since we have $(a_1+a_2)^p\leq a_1^p+a_2^p$ and by induction one easily arrives at $$\Big(\sum_{k=1}^\infty a_k\Big)^p\leq  \sum_{k=1}^\infty a_k^p\leq \sum_{k=1}^\infty \beta^ka_k^p$$ How to prove the case $p>1$ ?","['analysis', 'real-analysis', 'sequences-and-series', 'inequality', 'convex-analysis']"
4269464,Find the sequence $a_k$ for generating function $\left(\frac{1-x^3}{1-x}\right)^n$,We know that $\frac{1}{(1-x)^n} = \sum_{k=0}^\infty \binom{k+n-1}{n-1} x^k$ I also worked out $(1-x^3)^n$ using the binomial theorem and got $(1-x^3)^n = \sum_{i=0}^\infty (-1)^i \binom{n}{n-i} x^{3i}$ I'm not sure what to do with these to get $a_k$ from $\sum_{k=0}^\infty a_k x^k$ or if these are even what I need to solve the problem Any help is appreciated,"['generating-functions', 'discrete-mathematics', 'sequences-and-series']"
4269467,"Does there exist a poset with exactly 5 maximal chains of size 2, 3, 4, 5, 6, respectively and 2 maximal elements?","I came across this question today Does there exist a poset with exactly 5 maximal chains of size 2, 3,
4, 5, 6, respectively and 2 maximal elements? I dont't know how to begin with this question? Can someone please provide some hints?","['combinatorics', 'discrete-mathematics']"
4269499,Hartshorne problem II.8.7,"Again I'm stuck on a problem in Hartshorne. The situation is as follows: Let $X=\text{Spec}(A)$ be an affine, non-singular scheme which is finite of some field $k$ . Let also $\mathcal{F}$ be a coherent sheaf on $X$ . Let $X'$ be an infinitesimal extension of $X$ by $\mathcal{F}$ . This means $X'$ has a sheaf of ideals $\mathcal{J}$ such that $\mathcal{J^2}=0$ and there is an isomorphism $(X',\mathcal{O}_{X'}/\mathcal{J})\cong (X,\mathcal{O}_X)$ under which $\mathcal{J}$ (which is a $\mathcal{O}_{X'}/\mathcal{J}$ module) corresponds to $\mathcal{F}$ . We have to show that the extension is trivial meaning $\mathcal{O}_{X'}=\mathcal{O}_X\oplus \mathcal{F}$ . My idea was to try and show that $X'$ is affine. If it is we could apply global sections functor to the exact sequence $$0\to \mathcal{J}\to \mathcal{O}_{X'}\to i_*\mathcal{O}_X\to 0$$ of quasi-cogerent $\mathcal{O}_{X'}$ -modules (where $i:X\to X'$ is a homeomorphism) to get an exact sequence $$0\to J\to B\to A\to 0$$ of $B$ -modules
where $J=\Gamma(\mathcal{J})$ and $B=\Gamma(\mathcal{O}_{X'})$ . We could then use the infinitesimal lifting property from the previous problem to see that $B$ is the trivial extension of $A$ and $J$ . If anyone has an idea on how to show $X'$ is affine or want to share another approach to this problem I would be very grateful as always=)","['quasicoherent-sheaves', 'algebraic-geometry', 'sheaf-theory']"
4269520,Are there other functions relating various algebraic curves?,"So I have come across questions asking whether there are functions analogous to the circular(trigonometric) and hyperbolic functions, for parabolas and ellipses.
My question is whether there are functions related to various algebraic curves, not limited to the conic sections, for example: a function for lets say $ x^4 + y^4 = 1$ This is the question I am talking about. Are there parabolic and elliptical functions analogous to the circular and hyperbolic functions $\sinh$, $\cosh$, and $\tanh$? PS: I am a high-school student.","['algebraic-geometry', 'trigonometry', 'functions']"
4269528,"a value of an infinite series, how to obtain this result [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question I'm reading some books about series: convergence/divergence and methods of computing sums. Here is a problem stated by myself: what method allows to compute $$
\sum_{n=0}^{\infty}\left(\frac{1}{8n+1}-\frac{1}{8n+3}\right)
$$ I know some series have nonelementary value, but this one has. The value can be written with known constants, as WolframAlpha says. The sum does not telescope. Also differentiating $\sum_{n=0}^{\infty}\left(\frac{1}{8n+1}-\frac{1}{8n+3}\right)x^{8n+1 \text{ or } 3}$ gives nothing.","['convergence-divergence', 'sequences-and-series']"
4269537,"Properly arguing for basic limit laws: We have to do it backwards? I mean, we're not sure the limits exist...","Ok dumb questions given all the questions I've asked before on this account, but here goes: Example/Question 1 : When we evaluate things like $\lim_{x \to 0} [2x^2 + 5x]$ , is it actually improper to say like $\lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x$ $ = 2 \lim_{x \to 0} x^2 + 5\lim_{x \to 0} x$ $ = 2 (0) + 5 (0) = 0+0 = 0$ ? Context : This seems to be how it is done in, say, Stewart Calculus . See the Stewart Calculus limit laws . The limit laws can be used ASSUMING certain limits involved exist in the 1st place. So for example, I don't see how can we possibly say $$\lim_{x \to 0} [2x^2 + 5x] = \lim_{x \to 0} 2x^2 + \lim_{x \to 0} 5x$$ when we haven't established that both $\lim_{x \to 0} 2x^2$ and $\lim_{x \to 0} 5x$ exist What I think we should do is that the above kind of argument is scratch work and then the proper argument is as follows (similar to the $\varepsilon-\delta$ thing where we argue backwards from $\varepsilon$ to $\delta$ as scratch and then write the formal proof forwards from $\delta$ to $\varepsilon$ ): $\lim_{x \to 0} [2x^2 + 5x]$ exists as the sum of the following limits, if the following limits exist: $\lim_{x \to 0} 2x^2$ , $\lim_{x \to 0} 5x$ . $\lim_{x \to 0} 2x^2$ exists as 2 times the following limit, if the following limit exists: $\lim_{x \to 0} x^2$ $\lim_{x \to 0} 5x$ exists as 5 times the following limit, if the following limit exists: $\lim_{x \to 0} x$ $\lim_{x \to 0} x^2 = 0$ $\lim_{x \to 0} x = 0$ By (5) and (3), $\lim_{x \to 0} 5x$ exists and is equal to $5(0)=0$ By (4) and (2), $\lim_{x \to 0} 2x^2$ exists and is equal to $2(0)=0$ By (1), (6) and (7), $\lim_{x \to 0} [2x^2 + 5x]$ exists and is equal to $0+0=0$ . This seems very weird, unnatural, etc. For some reason ever since elementary calculus this is not what is being done. Yet, I think this should be the case otherwise we may fall into traps like $\lim_{x \to 0} \frac{x}{1} \frac{1}{x} = \lim_{x \to 0} \frac{x}{1} \lim_{x \to 0} \frac{1}{x} = (1)$ (does not exist) = does not exist. I think I fell for this kind of trap here . Please explain what's going on. Example/Question 2 : (a real multivariable example. I think there's an easy way to do this in single real, but I can't think of an example right now.) Here , I am trying to argue that $\lim_{(x,y) \to (0,0)}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2})$ doesn't exist because $\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2})$ doesn't exist because $\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ and then because $\lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ doesn't exist. Actually, how is it even sensible to do this entire long list of limit equalities $$\lim_{\substack{(x,y) \to (0,0) \\ y=0}}e^{\frac{y}{x^2+y^2}}\cos(\frac{x}{x^2+y^2}) = (...) \text{long list of limit equalities} (...) = \lim_{\substack{x \to 0}}\cos(\frac{1}{x})$$ when we're not even sure that the limits exist? Guess : Perhaps there's some implicit reductio ad absurdum here like 'suppose on the contrary that this limit exists. Then this limit equals (...) that limit. But that limit doesn't exist! Contradiction.' Example/Question 3 : Actually now I want to ask about $\lim_{\substack{x \to 0}}\cos(\frac{1}{x})$ , but I'm afraid the post will become too broad (if it isn't already)... Update : Asked here . Maybe related: Limit laws when not both limits exist Why use limit laws to verify continuity instead of direct substitution? Is it enough to show that $\lim_{x\rightarrow 0}\cos(1/x)$ doesn't exist to show that $\lim_{x \rightarrow0}(2x\sin(1/x)-\cos(1/x))$ doesn't exist? Why does this limit exist? Product of limits.","['limits', 'calculus']"
4269594,how to use union and intersection symbol to denote the limit [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question For any $x \in X$ , $f_n(x) \to \infty$ as $n \to \infty$ . Is this equivalent to saying $\cap_{K=1}^{\infty} \cup_{N=1}^{\infty} \cap_{n=N}^{\infty} 
 \{x: f_n(x)\geq K\}$ ? Why? My understanding: for any a positive integer K, we can find N, such that when n>N, then $f_n(x)>K$ ?",['analysis']
4269602,"Prove $\int_{0}^{\infty} \psi^{(2)} (1+x) \ln (x) \, dx = \zeta'(2) + \zeta(2)$","I'm looking for alternate methods of proving the following result: $$\int_{0}^{\infty} \psi^{(2)} (1+x) \ln (x) \, dx = \frac{\pi^2}{6} \left( \gamma + \ln (2\pi)-12 \ln A +1\right) = \zeta'(2) + \zeta(2)$$ Where $\psi$ is the polygamma function, $A$ is the Glaisher-Kinkelin constant and $\gamma$ is the Euler-Mascheroni constant. Here is how I originally proved it: Begin with the known result $$\int_{0}^{\infty} \frac{x \ln x}{e^{2 \pi x}-1} \, dx = \frac{1}{2} \zeta'(-1)=\frac{1}{24}-\frac{1}{2} \ln A$$ then use the following well-known property of the Laplace transform $$\int_{0}^{\infty} f(x) g(x) \, dx = \int_{0}^{\infty} \left( \mathcal{L} f\right) (y) \left( \mathcal{L}^{-1} g \right) (y) \, dy$$ Let $f(x) = \frac{x^2}{e^{2 \pi x} - 1}$ and $g(x) = \frac{\ln x}{x}$ then: $$\left(\mathcal{L} f\right) (y) = -\frac{1}{8 \pi^3} \psi^{(2)} \left(1+\frac{y}{2\pi}\right)$$ $$\left(\mathcal{L}^{-1} g\right)(y) = -\gamma - \ln (y)$$ $$\implies \frac{1}{24} - \frac{1}{2} \ln A = \frac{1}{8\pi^3}\int_{0}^{\infty} \gamma \, \psi^{(2)} \left(1+\frac{y}{2\pi}\right) \, dy + \frac{1}{8\pi^3} \int_{0}^{\infty} \psi^{(2)} \left(1+\frac{y}{2\pi}\right) \ln (y) \, dy$$ Proceed with the substitution $y \mapsto 2\pi x \implies dy = 2 \pi\,dx$ $$\implies \frac{1+\gamma}{24} - \frac{1}{2} \ln A = \frac{1}{4\pi^2} \int_{0}^{\infty} \psi^{(2)} \left(1+x\right) \ln (2 \pi x) \, dx$$ Now separate $\ln (2\pi x) = \ln (2 \pi) + \ln (x)$ and the result quickly follows. I was wondering if there are any alternative methods that don't rely on the initial $\frac{1}{2} \zeta'(-1)$ result. Motivation: I've been experimenting with the following identity: $$\int_{0}^{\infty} \frac{x^{2n-1}}{e^{2\pi x}-1}\, dx = \frac{(-1)^{n-1} B_{2n}}{4n}\,\text{ for } n \in \mathbb{N}$$ I noticed that if one takes the partial derivative with respect to $n$ and then proceeds to take the limit as $n \to 1$ , then we arrive at our $-\frac{1}{2} \zeta'(-1)$ result. This means it is possible for one to define the 'derivative' of the Bernoulli numbers at $n=1$ . More precisely: $$B_{2}^{\prime} = \frac{1}{4} - 2 \ln A$$ is this result already known, and if so, are any other Bernoulli derivatives definable in a similar way using the Glaisher-Kinkelin constant? (I know it is possible to do it with Glaisher-Kinkelin-like constants such as the Bendersky-Adamchik constants due to their similar connection to hyperfactorials, $\zeta'(-2)$ and $\zeta(3)$ for example). I'm not sure what meaning one can give to the 'derivative' of the Bernoulli numbers, but figure 2 of this paper seems to roughly agree with my result.","['integration', 'improper-integrals', 'analysis', 'analytic-number-theory', 'calculus']"
4269628,What is the integration contour used to compute the Fourier transform of $(1- e^{-x})^{-1}$?,"I'm reading ""Modular Groups of Quantum Fields
in Thermal States"" by Borchers and Yngvason, and on page 19 I find: Let's call $x=\beta p \in \mathbb{R}$ , so I'm interested in the Fourier transform of $(1-e^{-x})^{-1}$ . I have tried a contour that avoids the origin with a small arc (in the upper-half plane)  and closing with a large arc also in the upper half-plane, but the contribution from the small arc does not vanish, and this contour would also have infinite contributions from the residues at $2 \pi i n$ , with $n$ a positive integer. I have also tried with a rectangular contour, of width $2L$ and height $2\pi$ , with its lower horizontal segment an $\epsilon$ distance below the real axis (so to avoid the pole at the origin). The integrals along the horizontal segments can be related, and the integral along the left vertical segment goes to zero for $L\rightarrow \infty$ , but the right vertical segment still contributes to the contour integral. So, any suggestions on other possible contours?","['complex-analysis', 'fourier-transform']"
4269629,When is $\int\frac{dx}{\sqrt{a-bx^n-x^2}}$ solvable in terms of elementary functions and why?,"The integral in the question appears in the solution to the orbit of a particle subjected to a central force. It is written in Goldstein's Classical mechanics that the solution is possible in terms of circular trigonometric functions without providing any reason. The wiki page https://en.wikipedia.org/wiki/Exact_solutions_of_classical_central-force_problems refers to Whittaker which says that the expression under the square root must be quadratic at most.
I don't understand why it should be quadratic at most, i.e., why other values of n would make the integral stop having solutions with elementary functions only, what's the guarantee that no other integral value of n is possible? Any help/reference will be appreciated. Thanks in advance.","['integration', 'analysis', 'calculus', 'elementary-functions', 'indefinite-integrals']"
4269638,"Given any $10$ people in a room, prove that among them there are either $3$ that know each other or $4$ that do not know each other.","Given any $10$ people in a room, prove that among them there are either $3$ that know each other or $4$ that do not know each other. (Assume that if A knows B, then B knows A.) Could the pigeonhole principle be used for this? If so, how? I have a feeling it can be used but im not sure where to apply it just yet.","['pigeonhole-principle', 'combinatorics', 'extremal-combinatorics', 'ramsey-theory']"
4269639,How many matrices do you need to generate a dense subset of $U(n)$?,"In quantum computing, people talk about universal gates , which are a finite collection of matrices $\{U_1,\dots,U_k\}$ that generate a dense subset of $U(n)$ . The wiki gives various examples of collections of $\sim \log(n)$ operators that generate $U(n)$ . In the physics literature however, there is a focus on considering unitary matrices that only act on a finite (usually one, two, or three) qubits, so I wouldn't be surprised if you could actually generate $U(n)$ with less (maybe even $O(1)$ ?) number of operators. Is there an example of this, or a proof that this is impossible?","['matrices', 'linear-algebra', 'lie-groups', 'quantum-computation']"
4269684,Singleton in $\sigma$-algebra on a subset of $\mathbb{N}$,"I'm very new to set theory. Given the set $$\mathcal{L}=\left\{\{1,2,3\},\{4,5,6\},\{7,8,9\},\{10,11,12\}\dots\right\}$$ Where $\mathcal{L}$ is on the form $\{𝑘,𝑘+1,𝑘+2\}$ , $𝑘\in\mathbb{N}$ and $3|(𝑘+2)$ . Show that $\{1\}\notin\sigma(\mathcal{L})$ . My only argumentation is that the subsets of the set $\mathcal{L}$ are disjoint given the condition that $𝑘\in\mathbb{N}$ and $3|(𝑘+2)$ , and so any intersection is empty.","['elementary-set-theory', 'measure-theory']"
4269707,General case for sine integral: $ I = \displaystyle\int_{0}^{\infty}\frac{\sin^{2n+1}{x}}{x}dx $ where $n \in \Bbb N $ [duplicate],"This question already has answers here : $\int_{0}^{\infty}\frac{\sin^{2n+1}(x)}{x} \mathrm {d}x$ Evaluate Integral (5 answers) Closed 2 years ago . This integral was a exercise in a calculus book called ""Advanced Calculus Explored"" I have tried many different techniques and the closest one i got to an answer was using feynamn's technique . I have taken calc 1-3 and some other lower level math classes and a proof class but this integral has stumped me for months(working on it here and there). No elementary technique has got me close to  a solution. $ I = \displaystyle\int_{0}^{\infty}\frac{\sin^{2n+1}{x}}{x}dx    $ where $n \in \ N $ The approach I found the most success with was $I(a) = \displaystyle\int_{0}^{\infty}\frac{\sin^{2n+1}{x}}{x}e^{-ax}dx \Rightarrow I'(a) = -\displaystyle\int_{0}^{\infty}\sin^{2n+1}{x}e^{-ax}dx$ with initial condition $ \displaystyle\lim_{a \to \infty}I(a) = 0$ . After integration by parts I have got it down to $I'(a) = -\frac{2n(2n+1)}{a^2}(I'(a) +\displaystyle\int_{0}^{\infty}\sin^{2n-1}{x}e^{-ax}dx)+ (2n+1)I'(a)dx$ $I'(a) = \frac{2n+1}{a^2+2n+1}\displaystyle\int_{0}^{\infty}\sin^{2n-1}{x}e^{-ax}dx$ I am unsure how to proceed from here to get  a closed form for $I$ .","['integration', 'multivariable-calculus', 'special-functions']"
4269708,"Why Cantor diagonalization theorem is failed to prove $S$ is countable, Where $S$ is set of finite subset of $\mathbb{N}$?","I have an given set $S$ where $S=$ set of finite subsets of $\mathbb{N}.$ We need to prove $S$ is countably infinite. My approach: I need to prove there is one-to-one correspondence between $S$ and ${\mathbb{N}}.$ Suppose $S =
\{\{1,2,3\}, \{1,3,4,5\},\{4,5\}, \{\emptyset\},.............\}
=\{f_1,f_2,f_3,f_4..............\}$ where $f_i\subseteq\mathbb{N}.$ $\mathbb{N}=\{1,2,3,4......
..............\}$ Now $1$ map to $f_1,$$2$ map to $f_2,$ $3$ map to $f_3..........$ and so on. Now apply Cantor diagonalisation theorem $f'=\{2,3.............\}$ where $2$ comes from $f_2$ because $2\notin f_2,$$3$ comes from $f_3$ because $3\notin f_3...........$ and so on, $f'$ is different from $f_i.$ $f'$ isn't covered in bijection. $f:\mathbb{N}\to S$ isn't bijection. So $S$ is uncountable. Where did I wrong don't understand?","['elementary-set-theory', 'natural-numbers']"
4269722,Extremally disconnected spaces and the axiom of choice,"Is ZF consistent with ""any compact Hausdorff extremally disconnected topological space is finite?"" (Motivation: it is a theorem that a compact Hausdorff extremally disconnected space is a retract of a Stone-Cech compactification of a discrete set. Now Stone-Cech compactifications depend on choice, and I believe (but am not sure) it's consistent with ZF that there exists a discrete space without a Stone-Cech compactification.)","['axiom-of-choice', 'general-topology', 'logic', 'set-theory']"
4269740,"Understanding n-ary Cartesian product, understanding formula","While I understand the cartesian product I'm having trouble reading its set definition. What I'm having trouble with is the $x_i$ part and my misunderstanding is that there is the variable $i$ that I assumed could be only one value at a time: $X_i$ represents all the sets from $X_1$ to $X_n$ , $x_i$ represents the elements of those sets, if $i = 1$ then $x_i$ is a member of $X_1$ and further more is the first element of $X_1$ , when $i = 2$ $x_i$ is the second member of $X_2$ . I would have used two variables, one $i$ for $x_i$ and $j$ for the set. This is not correct so I would like some help translating this. I know the cartesian product is all the $n$ -tuple possibilities taking every elements of every sets but I can't translate the formula above to that definition. This formula for two sets' cartesian product makes much more sense to me:",['elementary-set-theory']
4269743,Evaluating the integral of the Cantor function,"Define the Cantor set $\mathcal{C}:=[0,1]\setminus\bigcup_{n=1}^{\infty}G_n$ , where $G_1=(\frac{1}{3}, \frac{2}{3})$ and $G_n$ for $n>1$ is
the union of the middle-third open intervals in the intervals of $[0, 1]\setminus (\bigcup_{j=1}^{n-1}G_j)$ . It is easy to see that $\mathcal{C}$ is also the set of numbers in $[0, 1]$ that have a base $3$ representation containing only $0$ s and $2$ s. Define the Cantor function $\Lambda:[0,1]\to [0,1]$ as follows: • If $x\in\mathcal{C}$ , then $\Lambda(x)$ is computed from the unique base $3$ representation of $x$ containing only $0$ s and $2$ s by replacing each $2$ by $1$ and interpreting the resulting string as a base $2$ number. • If $x\in [0,1]\setminus\mathcal{C}$ , then $\Lambda(x)$ is computed from a base $3$ representation of $x$ by truncating after the first $1$ , replacing each $2$ before the first $1$ by $1$ , and interpreting the resulting string as a base $2$ number. $\fbox{$\text{We want to compute }\int_{0}^{1}\Lambda(x).$}$ what follows is what I have come up with to find the value of this integral; it is not completely rigorous although it could be made rigorous by induction but I reckon it would be very lengthy so I would like to know: (a) if what I have done is correct and (b) if there is another (perhaps shorter) and more rigorous proof of this result, thanks Since the Cantor set contains no interval with more than one element $\int_{\mathcal{C}}\Lambda=0$ so the only non-zero contribution to the value of the integral comes from the (constant) values that $\Lambda$ has on each middle-third open interval in $\bigcup_{n=1}^{\infty}G_n$ .
We have $$G_1 =(\frac{1}{3},\frac{2}{3}), G_2 =(\frac{1}{9},\frac{2}{9})\cup (\frac{7}{9},\frac{8}{9}), G_3 =(\frac{1}{27},\frac{2}{27})\cup (\frac{7}{27},\frac{8}{27})\cup (\frac{19}{27},\frac{20}{27})\cup (\frac{25}{27},\frac{26}{27}),$$ $$G_4 =(\frac{1}{81},\frac{2}{81})\cup (\frac{7}{81},\frac{8}{81})\cup (\frac{19}{81},\frac{20}{81})\cup (\frac{25}{81},\frac{26}{81})\cup (\frac{55}{81},\frac{56}{81})\cup (\frac{61}{81},\frac{62}{81})\cup (\frac{73}{81},\frac{74}{81})\cup (\frac{79}{81},\frac{80}{81})\dots$$ i.e. for each $n\geq 1$ we have $2^{n-1}$ disjoint open intervals, each of length $\frac{1}{3^n}$ , which make up each $G_n$ .
The $G_n$ s, in ternary expansion, look like: $$G_1 =(0.1,0.2), G_2 =(0.01,0.02)\cup (0.21,0.22),$$ $$ G_3 =(0.001,0.002)\cup (0.021,0.022)\cup (0.201,0.202)\cup (0.221,0.222),$$ $$G_4 =(0.0001,0.0002)\cup (0.0021,0.0022)\cup (0.0201,0.0202)\cup (0.0221,0.0222)\cup (0.2001,0.2002)\cup (0.2021,0.2022)\cup (0.2201,0.2202)\cup (0.2221,0.2222)\dots$$ and by identifying each open interval in each $G_n$ by its leftmost endpoint we have: $$G_1\to 0.1,\ G_2\to 0.01, 0.21,\ G_3\to 0.001, 0.021, 0.201, 0.221,$$ $$G_4\to 0.0001, 0.0021, 0.0201, 0.0221, 0.2001, 0.2021, 0.2201, 0.2221,\dots$$ so we see that we can find (the ternary expansion) of the left endpoints of all the intervals in $G_{n+1}$ by taking those of $G_n$ and substituting $01$ or $21$ to the last digit so each interval generates two other intervals: one to the left of it (the $01$ one) and one to the right of it (the $21$ one). So, $$\Lambda(G_1)=\{0.1_2\}=\{\frac{1}{2}\}, \Lambda(G_2)=\{0.01_2,0.11_2\}=\{\frac{1}{4},\frac{3}{4}\},$$ $$ \Lambda(G_3)=\{0.001_2,0.011_2,0.101_2,0.111_2\}=\{\frac{1}{8},\frac{3}{8},\frac{5}{8},\frac{7}{8}\},$$ $$\Lambda(G_4)=\{0.0001_2,0.0011_2,0.0101_2,0.0111_2,0.1001_2,0.1011_2,0.1101_2,0.1111_2\}=\{\frac{1}{16},\frac{3}{16},\frac{5}{16},\frac{7}{16},\frac{9}{16},\frac{11}{16},\frac{13}{16},\frac{15}{16}\},\dots$$ hence we have that $G_n=\{\frac{1}{2^n},\frac{3}{2^n},\dots,\frac{2^n-1}{2^n}\}$ so $$\int_{0}^{1}\Lambda=\int_{\bigcup_{n=1}^{\infty} G_n}\Lambda=\frac{1}{3}\cdot\frac{1}{2}+\frac{1}{9}(\frac{1}{4}+\frac{3}{4})+\frac{1}{27}(\frac{1}{8}+\frac{3}{8}+\frac{5}{8}+\frac{7}{8})+\frac{1}{81}(\frac{1}{16}+\frac{3}{16}+\frac{5}{16}+\frac{7}{16}+\frac{9}{16}+\frac{11}{16}+\frac{13}{16}+\frac{15}{16})+\dots +\frac{1}{3^n}(\frac{1}{2^n}+\dots+\frac{2^n-1}{2^n})=\frac{1}{6}+\frac{4}{36}+\frac{16}{216}+\frac{64}{1296}+\dots=\sum_{n=1}^{\infty}\frac{4^{n-1}}{6^n}=\frac{1}{4}\sum_{n=1}^{\infty}(\frac{4}{6})^n=\frac{1}{4}\sum_{n=1}^{\infty}(\frac{2}{3})^n=\frac{1}{4}\cdot\frac{\frac{2}{3}}{1-\frac{2}{3}}=\frac{1}{2}.$$","['integration', 'measure-theory', 'cantor-set', 'real-analysis', 'solution-verification']"
4269796,Can you move a limit into an exponent? [duplicate],"This question already has an answer here : Why is it justified to move the limit into the exponent? (1 answer) Closed 2 years ago . I’m attempting to solve a limit problem, and my current solution requires moving a limit inside the exponent. Symbolically, I’m attempting the following: $$\lim_{x\rightarrow c}e^{\frac{f(x)}{g(x)}}=e^{\lim_{x\rightarrow c}\frac{f(x)}{g(x)}}$$ At this point the various functions are such that I can show that the limit of $\frac{f(x)}{g(x)}$ is $0$ by L’Hôspital’s Rule, and I can conclude that the limit of the overall expression is therefore $1$ . But is this true? I know I can pull terms in and out of limits so long as those terms don’t rely on the variable with respect to which the limit is being evaluated, but can one move the limit inside the function and evaluate it this way? If yes, how do you prove that; if no, why?",['limits']
4269797,Is this a known result on graph products?,"Consider two undirected graphs $G=(V,E)$ and $H=(I,F)$ .
Denote by $\mathcal N_G(v)$ (resp., $\mathcal N_{H}(i)$ ) the first neighborhood of a node $v\in V$ (resp., $i\in I$ ), including $v$ (resp., $i$ ).
For each pair of vertices $(v, i)\in V\times I$ , take a weight $w(v, i)>0$ and assume that for any $u\in V$ , $\sum_{j\in I} w(u, j) \le 1$ . Question: Is there a known (tight!) upper bound of the following quantity? $$Q = \sum_{v\in V}\sum_{i\in I}\frac{w(v,i)}{1-\prod_{u\in\mathcal N_{G}(v)}\left(1-\sum_{j\in\mathcal N_{H}(i)}w(u,j)\right)}$$ My thoughts If $F$ is a clique, then $$Q = \sum_{v\in V}\frac{\sum_{i\in I} w(v,i)}{1-\prod_{u\in\mathcal N_{G}(v)}\left(1-\sum_{j\in I}w(u,j)\right)} \le \frac{\alpha_G + W}{1-e^{-1}}$$ where $\alpha_G$ is the independence number of $G$ and $W = \sum_{v\in V}\sum_{i\in I} w(v,i)$ , for a known inequality (see, e.g., Lemma 3 here ). This leads me to believe that a good bound on $Q$ should be a function of three things: 1-2) the independence numbers $\alpha_G$ and $\alpha_H$ of $G$ and $H$ respectively (note that the latter is $1$ in the previous case where $H$ is a clique) and 3) the sum $W$ of all weights. In particular, this implies that a tight bound should not depend on the cardinalities of $V$ and $I$ , because the special case above indicates that if both graphs are well-connected and the weights are small, $Q$ is small no matter how large $V$ and $I$ are. However, all the inequalities that I can derive easily involve $|V|$ and/or $|I|$ , and are therefore loose. Any even partial answers or ideas that can lead me towards something could be awarded.","['summation', 'graph-theory', 'number-theory', 'elementary-number-theory', 'combinatorics']"
4269843,How to show that $n^{-1}S_n \rightarrow 0$ with probability 1?,"Suppose that $X_1,X_2,....$ are independent and uniformly bounded and $E[X_n]=0$ . Use the first Borel Cantelli Lemma and Chebyshev's inequality, prove that $n^{-1}S_n \rightarrow 0$ with probability 1. I know that convergence in probability 1 implies convergence in probability. This means I need to show that $P[|n^{-1}S_n| \geq \epsilon~ i.o]=0$ . By Chebyshev inequality, $P[|S_n| \geq n\epsilon]\leq \frac{E[|S_n|]}{n \epsilon}$ . I am little confused how to use first Borel Cantelli Lemma here. Can anyone suggest some hints?","['measure-theory', 'probability-theory']"
4269846,When does the Mayer-Vietoris sequence hold for affine algebraic varieties?,"Let $U$ and $V$ be affine algebraic varieties in $\mathbb{R}^k$ or $\mathbb{C}^k$ . When is the Mayer-Vietoris sequence exact for $U \cup V$ and $U \cap V$ ? Intuitively, it feels as though the intersection may be ""nice"" enough so that $U$ and $V$ may be thickened into open sets in a way that will not change the homology of $U \cup V$ and $U \cap V$ . Are there any references that one could suggest?","['algebraic-geometry', 'homology-cohomology', 'algebraic-topology']"
4269852,Jacobian matrix of $\mathbb R^3$ functions involving unit vector,"Jacobian matrix of $\mathbb R^3$ functions involving unit vector I'm learning differentiation of vector-valued functions in my analysis class now and I'm a bit stuck in the following question: Given $f:$ $\mathbb{R^3}\setminus \{0,0,0\} \to \mathbb{R^3}$ , $f(x)=x/||x||,$ for all $x \neq (0,0,0).$ Find ${(Df(p))}$ for $p
> \neq (0,0,0)$ , and hence prove that $||p|| {(Df(p))}^2 =  {(Df(p))}$ . My attempt:
I know that ${(Df(p))}$ will be a $3 \times 3$ matrix represented by the gradient vectors of the 3 coordinate functions. While solving for $||p|| {(Df(p))}^2$ , the only way I thought of is to do the matrix multiplication of ${(Df(p))}$ to itself multiplied by the norm of $p$ to get ${(Df(p))}$ which is a bit tedious and seems not to get me anywhere to  what I need to derive. Could anyone give me some directions on how I could approach the question (probably some definitions that I've missed?). Thanks...","['jacobian', 'derivatives', 'real-analysis']"
4269915,Find a closed form of the generating function,"I need to find the closed form of the generating function $a(z)$ given $(n+1)a_{n+1} = 3a_n + 1$ and $a_0 = 0$ . My attempt: Using $a_0 = 0$ the first few terms can be found. $a_1 = 1$ , $a_2 = 2$ , $a_3 = \frac{7}{3}$ , $a_4 = 2$ . \begin{align*}
(n+1)a_{n+1} &= 3a_n + 1\\
\sum_{n\geq0}(n+1)a_{n+1}z^n &= \sum_{n\geq0}3a_nz^n + \sum_{n\geq0}z^n\\
D(a(z)) &= a(z) + \frac{1}{1-z}\\
\end{align*} Here is a differential equation. I used an integrating factor of $e^{-3z}$ which gives a solution of $a(z) = e^{3z}\int e^{-3z}\frac{1}{1-z}dz$ . I'm not sure how to integrate this. It looks to me that this is too complicated as a solution. I'm not sure if i am doing this correctly. Any help will be appreciated. Thanks.","['ordinary-differential-equations', 'generating-functions']"
4269951,Convergence in probability implies almost sure convergence for some subsequence,"An alternate proof to this question requires below theorem. Let $X$ be a random variable and $\left\{X_{n}\right\}$ be a sequence of random variables on the probability space $(\Omega, \mathcal{F}, \mathbb{P})$ . $X_{n} \rightarrow X$ in probability if $\mathbb{P}\left(\left|X_{n}-X\right|>\varepsilon\right) \rightarrow 0$ as $n \rightarrow \infty$ for every $\varepsilon>0$ . $X_{n} \rightarrow X$ a.s. if $\mathbb{P}\left(\left\{\omega \in \Omega \mid X_{n}(\omega) \rightarrow X(\omega)\right\}\right)=1$ . Prove that $X_{n} \rightarrow X$ in probability implies there exists a subsequence $(X_{n_k})_k$ such that $X_{n_k} \rightarrow X$ a.s. as $k \to \infty$ . Could you check if my understanding is correct? Let's characterize such $(X_{n_k})_k$ . Let $f$ be a strictly decreasing function such that $\lim_{m \to \infty} f(m)=0$ . We have $$
\begin{aligned}
& X_{n_k} \to X \text{ a.s. as } k \to \infty \\
\iff & \mathbb P \left ( \{ \omega \mid X_{n_k} (\omega) \to X (\omega) \} \right ) = 1 \\
\iff & \mathbb P \left ( \bigcap_{m \in \mathbb N^*} \bigcup_{M \in \mathbb N} \bigcap_{n_k \ge M} \left \{ |X_{n_k} - X| \le f(m) \right \} \right ) = 1 \\
\iff & \mathbb P \left ( \bigcap_{m \in \mathbb N^*} \bigcup_{M \in \mathbb N} \bigcap_{k \ge M} \left \{ |X_{n_k} - X| \le f(m) \right \} \right ) = 1, \quad k \to n_k \text{ is strictly increasing} \\
\iff & \mathbb P \left ( \bigcup_{m \in \mathbb N^*} \bigcap_{M \in \mathbb N} \bigcup_{k \ge M} \left \{ |X_{n_k} - X| > f(m) \right \} \right ) = 0 \\
\iff & \mathbb P \left ( \bigcap_{M \in \mathbb N} \bigcup_{k \ge M} \left \{ |X_{n_k} - X| > f(m) \right \} \right ) = 0, \quad m \in \mathbb N^* \\
\iff & \mathbb P \left ( \limsup_k \left \{ |X_{n_k} - X| > f(m) \right \} \right ) = 0, \quad m \in \mathbb N^*.
\end{aligned}
$$ By Borel-Cantelli lemma , it suffices to choose $(n_k)$ and $f$ such that $$\sum_{k} \mathbb P ( \left \{ |X_{n_k} - X| > f(m) \right \} )  < \infty, \quad m \in \mathbb N^*.$$ The problem is to choose $n_k$ such that the inequality holds for all $m$ . The idea is to fix the partition $g(0) > g(1) > \cdots > 0$ of $\mathbb R_+$ and control the quantity $\mathbb P ( \left \{ |X_{n_k} - X| > g(i) \right \} )$ . If $f(m) \ge g (i_0)$ for some $i_0 \in \mathbb N$ , then $$\mathbb P ( \left \{ |X_{n_k} - X| > f(m)  \right \} ) \le \mathbb P ( \left \{ |X_{n_k} - X| > g (i_0) \right \} )$$ and thus $$\sum_{k} \mathbb P ( \left \{ |X_{n_k} - X| > f(m)  \right \} ) \le \sum_{k} \mathbb P ( \left \{ |X_{n_k} - X| > g (i_0) \right \} ).$$ To do so, we want to make $\sum_{k \ge K} \mathbb P ( \left \{ |X_{n_k} - X| > g (i_0) \right \} ) < \infty$ for some $K$ big enough. We expect something like $$\sum_{k \ge K} \mathbb P ( \left \{ |X_{n_k} - X| > g(k) \right \} ) < \infty$$ and $g(k) \le g (i_0)$ for all $k \ge K$ . We know that $\sum_{k} 2^{-k} < \infty$ , so we take $g: k \to 2^{-k}$ and $n_k$ such that $$\mathbb P ( \left \{ |X_{n_k} - X| > 2^{-k} \right \} ) < 2^{-k}.$$ Of course, the existence of such $n_k$ is guaranteed by the convergence in probability of $(X_k)$ to $X$ . This completes the proof.","['convergence-divergence', 'solution-verification', 'probability-theory']"
4269961,Superposition of two cosine terms,"This is an example problem from my textbook. I read through its solution, but was confused by the following step. $$x=\frac{\omega^{2}A\cos(\omega t-\delta)}{\sqrt{(\omega_{0}^{2}-\omega^{2})^{2}+\omega^{2}\gamma^{2}}}+A\cos\omega t$$ The textbook proceeds to say that, quote Since $x$ is a superposition of two cosine terms in $\omega t$ , we can write it as $$x=C(\omega)\cos(\omega t-\alpha)$$ , where $$[C(\omega)]^2=\frac{A^{2}(\omega_{0}^{4}+\omega^{2}\gamma^{2})}{(\omega_{0}^{2}-\omega^{2})^{2}+\omega^{2}\gamma^{2}}$$ I am confused how it gets there",['trigonometry']
4270028,"Verify that $x = −(I − A^{\dagger}A)c$ optimises $ \frac{1}{2}\|x\|^2 + \langle c,x \rangle$ under constraints","Under the constraint that $Ax = 0$ and $A$ is full row rank, how would you get the solution to the optimisation problem $$\text{min } \frac{1}{2}\|x\|^2 + \langle c,x \rangle$$ My text says that $x = −(I − A^{\dagger}A)c$ is a unique solution but I am unsure as to how to verify it as I am unsure what a feasible direction is under this constraint. $A^{\dagger}$ is the Moore Penrose inverse","['nonlinear-optimization', 'linear-algebra', 'numerical-linear-algebra', 'optimization', 'constraints']"
4270082,Integer eigenvalues in Matlab,I am trying to write a Matlab program which decides if a given (integer) matrix A has integer eigenvalues and if this is the case calculates the eigenvalues and their multiplicities. Any ideas how to get started?,"['matrices', 'matlab', 'eigenvalues-eigenvectors']"
4270098,Counterexample to Kunneth Formula,"I'm reading Example 9.14 of Bott&Tu's Differential Forms in Algebraic Topology, which gave a counter example to the Kunneth formula when the assumption is not satisfied by constructing manifolds $M$ and $F$ such that $H^0(M\times F)$ is naturally isomorphic to $\mathbb{R}^{\mathbb{Z}\times\mathbb{Z}}$ while $H^0(M)\otimes H^0(F)$ is naturally isomrophic to $\mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}$ . I can see that these two are not naturally isomorphic via the explicit mapping in Kunneth formula. But if one ommit the explicit mapping in the statement of Kunneth formula, then a counterexample in degree $0$ needs $H^0(M\times F) \not\cong H^0(M)\otimes H^0(F)$ as real vector spaces. So I wish to figure out if $\mathbb{R}^{\mathbb{Z}\times\mathbb{Z}}$ is isomorphic to $\mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}$ as vector spaces. The counterexample would appear to be more perfect to me if they are not isomorphic. Two vector spaces are isomorphic if and only if their basis have a same cardinality. Since there is a natural embedding $\mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}\hookrightarrow \mathbb{R}^{\mathbb{Z}\times\mathbb{Z}}$ , the cardinality of basis of $\mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}$ is no larger than $\mathbb{R}^{\mathbb{Z}\times\mathbb{Z}}$ . On the other hand, we have $\mathbb{R}^\mathbb{Z\times Z}\cong \mathbb{R}^\mathbb{Z}$ by re-indexing $\mathbb{Z}\times \mathbb{Z}\cong\mathbb{Z}$ , and then $\mathbb{R}^\mathbb{Z}\hookrightarrow \mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}:v\mapsto v\otimes 1$ is an embedding, concluding that $\mathbb{R}^{\mathbb{Z\times Z}}\cong \mathbb{R}^{\mathbb{Z}}\otimes \mathbb{R}^{\mathbb{Z}}$ . Is my above argument correct? If so, is there any other counterexample to Kunneth formula, with $H^*(M\times F)\not\cong H^*(M)\otimes H^*(F)$ as real vector spaces in some degree? Thanks in advance.","['differential-forms', 'de-rham-cohomology', 'algebraic-topology', 'differential-geometry']"
4270105,Prove a matrix is idempotent using algebra,"I'd like to prove that this matrix is idempotent using a more algebraic proof for matrices with a similar definition to A, rather than deriving its eigenvalues or calculating $A^2$ . $A=$ $\begin{bmatrix}0.5&0&-0.5\\0&1&0\\ -0.5&0&0.5\end{bmatrix}$ I know that A is symmetric (so $A'=A$ ) and positive semi-definite, so how could I use these properties to prove idempotency?","['matrices', 'linear-algebra', 'idempotents', 'symmetric-matrices']"
4270151,"Why does 3blue1brown use the ""around a point"" to describe a derivative?","In this article (which includes a link to the video version of the article as well), Grant Sanderson aka 3blue1brown describes a derivative. He says at the end of the passage headed ""The Paradox"" , ""Since change in an instant still makes no sense, rather than interpreting the slope of this tangent line as an “instantaneous rate of change”, an alternate notion is to think of it as the best constant approximation for rate of change around a point"". However, I'll be the devil's advocate and disagree with him. I particularly take issue with his usage of the word ""around a point"". I think that the slope of the tangent line is the slope of the curve at that exact point, not around that point. I'll present two quotations in favor of my case: To partially quote myself from my most recent question , Let us consider 2 different points $A$ & $B$ of the above graph. Now, if we find the slope of the secant line $AB$ , it'll be an approximation of the slope of $A$ . If we pick a point that is closer to $A$ than $B$ , $C$ , the slope of $AC$ will be a better approximation of $A$ 's slope. Now, if we know what the value is that the slopes of the secant lines are approaching as the points are getting closer and closer to $A$ , we will be able to find the best approximation and the most correct answer of the slope of that point: the approached value. It is the best approximation because we know that the approximations are getting better and better as the approximations are getting closer and closer to the approached value, so the approached value is the most accurate approximation, and it is the slope of the curve at that exact point , not around that point. We can calculate this approached value by taking the limit: $$f'(x)=\lim_{h\to 0}\frac{f(x+h)-f(x)}{h}$$ $f'(x)$ is the approached valued, and it is known as the derivative of $f$ at $x$ . To quote @Javier's comment to this question , ""Maybe you should try not thinking of limits as movement, because then, as you say, you never ""get there"". Rather, when you see a limit like the derivative, imagine that there is a number that you cannot calculate, but that you can approximate with arbitrarily high precision. This is not some fuzzy thinking that cheats by evading the concept of instantaneous rate of change; if you can approximate a number arbitrarily well, then you know exactly what it is, even if you cannot calculate it ""directly"". With this point of view, the limit is not a process that will never end. Instead, it's an indirect way of specifying (without ambiguity) a number that you couldn't otherwise calculate. Maybe this will help."" In short, I think a derivative is the slope of a graph/curve at an exact point, not near that point or around that point. I think 3blue1brown's reasoning can lead to problems. In the passage titled ""The Paradox at Time Zero"" , he essentially argues that at time zero, the car is not static even though the derivative gives us $0$ at that point. He says, ""For smaller and smaller nudges in time, this ratio of the change in distance over change in time approaches $0$ , though in this case it never actually hits it"". However, I'd argue that as we can see that as our approximations approach $0$ we are getting more accurate. As we get arbitrarily close to zero, our approximations become arbitrarily accurate. So, we can understand that the most accurate approximation is the approached value as our approximations are getting better and better as we are getting closer and closer to the approached value. See @Javier's comment above. Questions: Am I correct or is 3blue1brown correct?","['calculus', 'soft-question', 'derivatives']"
4270153,How do you find the global maximum or minimum of an unbounded function?,"Take for example: $f(x) = \frac{x^4}{4} - 2x^3 + \frac{11x^2}{2}-6x + 2$ , for all $x \in R$ The critical points are 1,2, and 3. 1 and 3 are local minimums and 2 is a local maximum from 2nd derivative test. I can draw a graph and look at the intervals in between and on the sides of the critical points to deduce the graph shape: I found out that x = 2 is a global min and that there is no global max. The problem with this method is that the more critical points there are the more I have to crunch numbers in my calculator to see how the graph is shaped around critical points. Is there another way to reach the same conclusion that avoids this graphical way of doing things? For instance could I take the limit and deduce something from that, or maybe a more rigorous way?","['maxima-minima', 'calculus', 'derivatives']"
4270198,Conceptual misunderstanding of law $\mathbb{P}^{\nu}$ of a stochastic process,"What I understand : Giving Markov transition kernels $(q_t)_{t\ge0}$ and an initial distribution $\nu$ entirely specifies the law of a stochastic process. Indeed If $X^{\nu}$ and $Y^{\nu}$ are two processes such with same transition kernels $(q_t)_{t\ge0}$ and initial distribution $\nu$ , in other words $\nu(A)=P(X^{\nu}_0\in A)=P(Y^{\nu}_0\in A)$ then they have the same law, i.e. $\operatorname{law}(X^{\nu})=\operatorname{law}(Y^{\nu})$ which means that for every $A\subset S^T$ , we have $$\mathbb{P}\{\omega : (t\mapsto X^{\nu}_t(\omega))\in A\} = \mathbb{P}\{\omega : (t\mapsto Y^{\nu}_t(\omega))\in A\}.$$ Now we can define a probability on $S^T$ by $\mathbb{P}^{\nu}:=\operatorname{law}(X^{\nu})$ and in particular we call $\mathbb{P^{\delta_x}}=:\mathbb{P}^x$ . Then things get weird What I don't understand : 1) Rather than considering one Markov process $X^{\nu}$ for every initial
distribution $\nu$ , we may introduce a single canonical process $X$ ,
deﬁned as the identity mapping on the path space $(S^T,\mathcal{B}(S)^T)$ , equipped with the diﬀerent probability
measures $\mathbb{P}^{\nu}$ . I am not sure how these two ways of thinking are equivalent. Mostly, I struggle with these kind of phrasing : Denote by $\mathbb{P}^x$ the probability measure such that $B = (B_t)_{t≥0}$ is
a Brownian motion started at $x\in \mathbb{R}$ . Is $B$ in this case just the process $B:S^T\to S^T, B : \omega\mapsto \omega$ ? But then it wouldn't necessarily be Brownian no? Or does this mean that we construct a Markov process that is a Brownian motion by specifying for example transition kernels $q_{s,t}(z,A):=\mathbb{E}[\mathbb{1}_A(z+B_t-B_s)]$ and with initial distribution $\delta_x$ and then we call $\mathbb{P}^x$ the law of this Brownian motion? Wouldn't it make more sense in that case to say ""Denote by $\mathbb{P}^x$ the law of $B = (B_t)_{t≥0}$ ""? Addendum I think I more or less understand this now. The main calculation that I had difficulty to see is that given $X$ the identity process we have \begin{align}
P(X^{\nu}\in A)=&P(X\circ X^{\nu}\in A)\\=&P((X\circ X^{\nu})^{-1}(A))\\=&P((X^{\nu})^{-1}\circ X^{-1}(A))\\=&P((X^{\nu})^{-1}(X^{-1}(A)))\\=&P^{\nu}(X^{-1}(A))\\=&P^{\nu}(X\in A)
\end{align} So say we construct from $(q_t)_{t\ge0}$ and $\nu$ a Markov process/Brownian motion or something. Then the law of the ""identity process"" $X(\omega)=\omega$ for $\omega\in S^T$ will be the law of the specific Markov process/Brownian motion under the probability $P^{\nu}$ . reference : Markov process and transition semigroup","['probability-distributions', 'probability-theory']"
4270230,Problem of the limits related to a sequence given by relation $a_{n+1}=a_n+a_{n}^{-\frac{1}{k}}$,"Here's the problem: $k>1,k\in\mathbb{N}.$ Given $\ a_0>0,\forall n\in \mathbb{N},a_{n+1}=a_n+a_{n}^{-\frac{1}{k}}.$ Figure out $\lim\limits_{n\to \infty}\frac{(a_n)^{k+1}}{n^k}.$ I have no idea of dealing with the relation.It makes sense to say $a_n$ grows rather slowly,every time by $-\frac{1}{k}$ times of itself.Thus $k+1$ times makes $a_n$ grows like what $n^k$ does.But how to make it to the ground?Are there any techniques to deal with such relations?Anything about it would be highly appreciated.","['limits', 'sequences-and-series', 'analysis', 'real-analysis']"
4270239,Prove that ${f(1);f(2);f(3)...;f(3^{k})}$ is a complete set of residues mod $3^{k}$,"Let a function $ f : \Bbb N^* \to \Bbb N^* $ ( $\mathbb N^*$ is the set of positive integers) $f(1) = 1 $ ; $f(n+1) = f(n) +2^{f(n)}$ for all positive integers $n$ . Prove that ${f(1),f(2),f(3),\dots,f(3^{k})}$ is a complete set of residues mod $3^{k}$ (where $k$ is a positive interger). $f(n+1) = f(n) +2^{f(n)}$ $\Rightarrow f(n+1) =f(n-1) + 2^{f(n-1)}+2^{f(n)} = ... = 1 + 2^{f(1)} + 2^{f(2)} +...+ 2^{f(n)}$ Suppose there exist $2$ positive integers $a, b$ satisfying $ 1 \le a < b \le 3^{k} $ : $ f(a) \equiv f(b) \pmod{ 3^{k}} $ $\Rightarrow 1 + 2^{f(1)} + 2^{f(2)} +...+ 2^{f(a-1)} \equiv 1 + 2^{f(1)} + 2^{f(2)} +...+ 2^{f(b-1)} \pmod {3^{k}}$ $\Rightarrow 2^{f(a)} + 2^{f(a+1)} + ... + 2^{f(b-1)} \equiv \pmod {3^{k}}$ It is easy to see that $2$ is the primitive root mod $3^{k}$ and $f(n) \equiv 1 \pmod 2 $ $\Rightarrow 2^1 ; 2^2 ;...; 2^{ 2.3^{(k-1)} } $ reduced set of residues
mod $3^{k} $ So if $ f(a) \equiv f(b)$ (mod $3^{k}) \Rightarrow f(a) \equiv f(b)$ (mod $3^{k-1})$ $\Rightarrow f(a) \equiv f(b)$ (mod $2.3^{k-1})$ $\Rightarrow 2^{f(a)} \equiv 2^{f(b)}$ (mod $3^{k})$ $\Rightarrow f(a) + 2^{f(a)} \equiv f(b)+2^{f(b)}$ (mod $3^{k})$ $\Rightarrow f(a+1) \equiv f(b+1)$ (mod $3^{k})$ $\Rightarrow$ if $f(a) \equiv f(b)$ (mod $3^{k})$ then $f(a+s) \equiv f(b+s) $ (mod $3^{k}) (b+s \le 3^{k} )$ I want to point out the absurdity in the above argument but have no idea, I hope to get help from everyone. Thanks very much ! (This is a problem from a long time ago, I guarantee.)","['number-theory', 'functions', 'elementary-number-theory']"
4270331,"If instantaneous rates of change aren't that rigorous, how correct is the usage of instantaneous rates of change (like velocity) by physicists?","According to this answer , instantaneous rates of change are more intuitive than they are rigorous. I tend to agree with that answer because, in the Wikipedia article on differential calculus , they aren't defining the derivative to be the slope at a particular point. They define it as, ""The derivative of a function at a chosen input value describes the rate of change of the function near that input value."" Although this isn't wrong, the definition has been written rather safely, and I think that was intentional. They didn't define it as the slope of the graph at a particular point. It is only in the explanations section of the Derivative wiki article that they did that: ""The derivative of a function y = f(x) of a variable x is a measure of the rate at which the value y of the function changes with respect to the change of the variable x. It is called the derivative of f with respect to x. If x and y are real numbers, and if the graph of f is plotted against x, derivative is the slope of this graph at each point. "" So, are physicists using terms like ""instantaneous velocity"" merely from an intuitive standpoint? What is the physical significance of instantaneous rates of change?","['derivatives', 'calculus', 'limits', 'soft-question', 'mathematical-physics']"
4270424,Multivariate Residue Theorem?,"Is there an extension of the residue theorem to multivariate complex functions? Say you have a function of $n$ complex variables $s_{n}$ and you wish to integrate it over some region in $\mathbb{C}^{n}$. Can you exploit the singularities of the function as you would in the single variable case to evaluate the integral?
For example 
\begin{equation}G=\int_{\Omega} d^{n}s \frac{1}{\sum_{i} s_{i}^{2}}\end{equation}
This is singular for the $n$ roots of the equation $\sum_{i}s_{i}^{2}=0$ but I can't think of a way to extend the residue theorem to such a case. I guess you could go to 'polar' coordinates and it may simplify this one, but what about more complicated functions where this isn't possible?
I'm sorry if the question is ill-defined.","['complex-analysis', 'several-complex-variables', 'reference-request']"
4270485,Trace is a multiple of determinant in congruence subgroups of Modular Group,"An element of the congruence subgroup of order $n$ of the modular group looks like this: $M'= \begin{pmatrix} 
An+1 & Bn \\
Cn & Dn+1 
\end{pmatrix}$ with $A, B, C, D, n \in \mathbb{Z},$ and $n>0$ . If you use the property that the det( $M')=1$ , we get the following (assuming $n>0$ ): $(AD-BC)n = A + D$ If we let $M=\begin{pmatrix} 
A & B \\
C & D 
\end{pmatrix}$ , then the identity says that $n\cdot$ det $(M)=A+D=$ Tr( $M$ ). Does this ring any bells for anyone? This identity is quite curious to me, although I have very little intuition for what it means. Perhaps someone can enlighten me.","['number-theory', 'geometry', 'abstract-algebra', 'linear-algebra', 'group-theory']"
4270496,Find Interior Angles of Irregular Symmetrical Polygon,"Apologies if this is has an obvious answer, but I've been stuck on this for a bit now. I've been trying to figure out how to make a symmetrical polygon with a base of m length, with n additional sides of s length using just those values, and I've gotten stuck. I'm not looking for an answer per se (although it would be appreciated). I'm looking for a next step that I might have overlooked. This shape has some given values and rules. n is the number of sides excluding the base, and must be at least 2 m can be any length between $0$ and $n \cdot s$ The shape is symmetrical, the line of symmetry is perpendicular to the base and $\frac{m}{2}$ from either endpoint of the base. In practice, s , n , and m are known values Here is a picture of what I am describing: In this example I've built the shape in reverse. I set $n=5$ , $s=3$ , and $\theta = 110 ^\circ$ . I've also found a relationship between the angles using the angle sum of a polygon: $$ \theta = 180 - \frac{2\zeta}{n-1}, \zeta= \frac{(180 - \theta)(n-1)}{2}$$ Using that (and Geogebra) I found that $\zeta = 140 ^\circ$ . But ideally, I would like to calculate $\theta$ from n , s , and m without setting $\theta$ beforehand. I'm looking for an equation for $\theta$ and $\zeta$ . I know their values relative to each other, but I haven't been able to figure out how to properly find these angles, aside from making triangles out of the entire thing, which I'm still unsure on where to start using just those values. Any help would be appreciated.","['euclidean-geometry', 'angle', 'geometry', 'polygons', 'trigonometry']"
4270561,Can these Haskell operations be presented as piecewise mathematical functions?,"I am researching the idea from O'Donnell,Hall and Page that Haskell serves both as a formal, mathematical notation, and as a
practical and powerful programming language See also the The Haskell Road to Logic, Maths and Programming by
Jan van Eijck. Given the Haskell code in Listing 1, is it possible to represent the two operations $\mathsf{eq}$ and $\mathsf{plus}$ as piecewise mathematical functions? Listing 1 -- Peano numbers
    data Nat = Zero | Suc Nat deriving Show 
    -- Equality
    eq :: Nat -> Nat -> Bool
    eq Zero Zero = True
    eq (Suc x) (Suc y) = eq x y
    eq _ _ = False
    
    plus :: Nat -> Nat -> Nat
    plus x Zero =  x
    plus x (Suc y) = Suc (plus x y) My assumption is that patterns in Haskell function definitions are like mathematical piecewise functions. The arguments on left hand side of each Haskell equation, when compared to some value , represents the condition for each mathematical case.  Here are my attempts: \begin{align*}
\forall x,y \in \mathbb{N}~ \colon \mathsf{eq}(x,y) = \begin{cases}
True & \text{if $x=Zero \land y=Zero$}, \\
\mathsf{eq}(x',y') & \text{if $ x = \mathsf{Suc}(x') \land y = \mathsf{Suc}(y')$}, \\
False & \text{otherwise}.  \\
\end{cases}
\end{align*} \begin{align*}
\forall x,y \in \mathbb{N} ~\colon \mathsf{plus}(x,y) = \begin{cases}
x & \text{if $y=Zero$}, \\
\mathsf{Suc}(\mathsf{plus}(x,y'))  & \text{if $y = \mathsf{Suc}(y')$}. \\
\end{cases}
\end{align*} Are these attempts reasonable? Are there alternative mathematical notations for these recursive function definitions?","['notation', 'programming', 'functions', 'recursion']"
4270576,Integrating $\int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx$,"I found the following integral and wanted to know if there is a nice closed form solution in terms of elementary or some special functions (Polylogarithm, Clausen, etc). $$\displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx$$ I know that the integral converges numerically to $\approx 0.403926$ Here is my try using integration by parts: Let $$ du = \frac{\arctan(x)}{x^2} \Longrightarrow u = -\frac{1}{2}\ln(1+x^2) + \ln(x) - \frac{\arctan(x)}{x} $$ $$ v = \arctan(x^2) \Longrightarrow dv = \frac{2x}{x^4+1}$$ Hence $$\displaystyle \int_{0}^{1} \frac{\arctan(x)\arctan(x^2)}{x^2} dx \stackrel{IBP}{=} -\frac{\pi^2}{16} - \frac{1}{8}\pi \ln(2)  -2\underbrace{\int_{0}^{1} \frac{x\ln(x)}{x^4+1} dx}_{I_{1}} + 2\underbrace{\int_{0}^{1}\frac{\arctan(x)}{x^4+1}dx}_{I_{2}} + \underbrace{\int_{0}^{1} \frac{x\ln(1+x^2)}{x^4+1}dx}_{I_{3}} $$ Can be proven that $$I_{1} = -\frac{C}{4}$$ where $C$ is the Catalan constant and $$I_{3} = \frac{1}{16} \pi \ln(2) $$ but I'm stuck with $I_{2}$ Another way could be: Define $$\Psi(a) =  \int_{0}^{1} \frac{\arctan(ax)\arctan(x^2)}{x^2} dx$$ Hence $$\Psi'(a) = \int_{0}^{1} \frac{\arctan(x^2)}{x(a^2x^2+1)}dx = \frac{1}{2}\int_{0}^{1} \frac{\arctan(w)}{w(a^2w+1)}dx$$ Using integration by parts, we have: $$du = \frac{\arctan(w)}{1+a^2w} \Longrightarrow u= \frac{1}{1+a^4}\ln\left( \frac{1+a^2w}{\sqrt{1+w^2}} \right) - \frac{a^2-w}{(1+a^4)(1+a^2w)} \arctan(w)  $$ $$ v = \frac{1}{w} \Longrightarrow dv = \ln(w) $$ However, this path seems even more rugged that the other. One last hint could be the following integral: $$\int_{0}^{1} \frac{\arctan(x) \arctan(x^3)}{x} dx = \frac{7}{72}\zeta(3) + \frac{\pi}{3}C - \frac{5\pi}{12}\operatorname{Cl}_{2}\left(\frac{2\pi}{3} \right)$$ where $\operatorname{Cl}_{2}$ is the Clausen function of order 2. However, I do not know the proof of this result either.","['integration', 'definite-integrals', 'real-analysis', 'calculus', 'closed-form']"
4270588,Estimating $\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx$ for large $N$,"In a paper I am reading, it is claimed without proof that for every $\varepsilon > 0$ and $N > N_0(\varepsilon)$ $$
\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx < N^{2 + \varepsilon}
$$ I do not quite see how this follows. The Cauchy-Schwarz inequality shows that $$
\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4 \le N^2\left(\sum_{n=1}^{N} \cos^2(n^2x) \right)^2 \le N^3\left(\sum_{n=1}^{N} \cos^4(n^2x) \right)
$$ and hence $$
\int_0^{2\pi}\left(\sum_{n=1}^{N} \cos (n^2x) \right)^4\ dx \le N^3\int_0^{2\pi}\sum_{n=1}^{N} \cos^4(n^2x) = \frac{3}{4}\pi N^4,
$$ but this is evidently not good enough. Any help proving the claim is appreciated.","['calculus', 'integral-inequality', 'analysis']"
4270632,"How many four digit numbers are such that reading from right to left , each digit is greater than the digit at previous position?","How many four digit numbers are such that reading from right to left , each digit is greater than the digit at previous position? My approach: We want a 4 digit number of type abcd where a>b>c>d According to me answer should be 10c4 , as any 4 numbers chosen will have only 1 arrangement in which a>b>c>d, but the answer given in the book is 9c4, please help me identify my mistake",['combinatorics']
4270668,Prove (by definition) that $\lim _{z\to 1+i}\left(\frac{1}{z^2+1}\right)=\frac{1}{2i+1}$,"How can I take this expression $$\left(\frac{1}{z^2+1}\right)-\frac{1}{2i+1}$$ to the form $z-(1+i)$ ? I must use the definition of limit to prove this result. That is, use $\delta$ and $\epsilon$ . What I have done so far is to raise the definition: $$\lim _{z\to 1+i}\left(\frac{1}{z^2+1}\right)=\frac{1}{2i+1}\Leftrightarrow[\forall\epsilon>0,\exists\delta>0/\forall z\in\mathbb{C}:(0<|z-(1+i)|<\delta\Rightarrow|\left(\frac{1}{z^2+1}\right)-\frac{1}{2i+1}|<\epsilon)]$$ I must prove: $$\left|\left(\frac{1}{z^2+1}\right)-\frac{1}{2i+1}\right|<\epsilon$$ as long as $0<|z-(1+i)|<\delta$ We start from \begin{align}
\left|\left(\frac{1}{z^2+1}\right)-\frac{1}{2i+1}\right|&=\left|\frac{1+2i-(z^2+1)}{(z^2+1)(2i+1)}\right|\\
&=\left|\frac{2i-z^2}{(z^2+1)(2i+1)}\right|\\
&=\left|\frac{(2i-z^2)(1-2i)}{(z^2+1)(2i+1)(1-2i)}\right|\\
&=\left|\frac{2i+4+i(2z^2+2)}{5(z^2+1)}\right|.
\end{align} But I still don't get the expression I need.","['complex-analysis', 'limits', 'epsilon-delta']"
4270671,Prove that a certain Group is Abelian,"Let $G$ be a group with following two properties: for all $a, b \in G$ we have $(ab)^2=(ba)^2$ every element $a \in G$ of order $2$ , ie $a^2=e_G$ , is already the neutral element $a=e_G$ question: is this group already Abelian? what I tried: we can derive a nice identity of commuators $[a,b]:=aba^{-1}b^{-1}$ : $$  aba^{-1}b^{-1} = b^{-1}a^{-1}ababa^{-1}b^{-1}= b^{-1}a^{-1}(ab)^2a^{-1}b^{-1}=  b^{-1}a^{-1}(ba)^2a^{-1}b^{-1} = b^{-1}a^{-1}ba $$ Therefore $[a,b]=[b^{-1},a^{-1}]$ and we have to show that $[a,b]=e_G$ .","['group-theory', 'abelian-groups']"
4270693,Show that $\text{rank}(A-B) = \text{rank}(A) - 1 \iff B = \frac{Axy^{T}A}{y^{T}Ax}$ where $\text{rank}(B)=1$.,"Let $m,n \in \mathbb{N}$ and $A,B \in \mathbb{R}^{m\times n}$ with $\text{rank}(B) = 1$ . Show that $\text{rank}(A-B) = \text{rank}(A) - 1$ if and only if there exist vectors $x \in \mathbb{R}^n$ and $y \in \mathbb{R}^m$ such that $y^{T}Ax \neq 0$ and $B = \frac{Axy^{T}A}{y^{T}Ax}$ . I don't really know how to start this problem. I know that $y^{T}Ax$ is a scalar and that $B$ may be written as an outer product of two vectors $u^{T}v$ . Other than that, I am pretty lost. I imagine that alternative characterizations of rank and the rank-nullity theorem might come in handy at some point.","['matrices', 'matrix-rank', 'linear-algebra']"
4270715,Is every square matrix a sum of a diagonal matrix and a nilpotent matrix,"The question is in the title. I am working with complex matrices. This is true for $2\times 2$ matrices, but becomes complicated already for $3\times 3$ matrices if we try to brute force it. By brute force, I mean taking an arbitrary $3\times 3$ matrix and imposing conditions on the diagonal entries so that the matrix is nilpotent. A follow up question for matrices which can be expressed as a sum of diagonal and nilpotent matrices is whether there are infinitely many ways of doing this? This feels unlikely because the final answer seems to depend on solving $n$ independent polynomial equations in $n$ variables, but I might be wrong.","['matrices', 'linear-algebra']"
4270742,How many triples of mutually conjugate linear maps are there?,"I’m looking for finite-dimensional complex vector spaces $V$ with three invertible linear maps $r,s,t:V\to V$ , satisfying the following “mutual-conjugation” condition: $rsr^{-1}=t$ $sts^{-1}=r$ $trt^{-1}=s$ For lack of a better term, I’m calling them conj-representations .
In order to avoid redundancies, say: Two conj-reps $V, V’$ are isomorphic if there is a linear isomorphism $f$ between them respecting the actions of the three maps (i.e. $fr=r’f$ ). A conj-rep is irreducible if it has no subconj-rep (i.e. the three maps do not restrict to a proper subspace). Question: how many isomorphism classes of irreducible conj-reps are there? Example: take $V=\mathrm{Span}(e_1,e_2,e_3)$ and: $r$ permutes $e_1 \leftrightarrow e_2$ and fixes $e_3$ . $s$ permutes $e_2 \leftrightarrow e_3$ and fixes $e_1$ . $t$ permutes $e_3 \leftrightarrow e_1$ and fixes $e_2$ . Note that this is not irreducible, as it has subconj-reps $\mathrm{Span}(e_1+e_2+e_3)$ and $\mathrm{Span}(e_1-e_2,e_2-e_3)$ . Another $1$ -dimensional example: $r, s, t$ all act as $-1$ . I can prove that if all three maps are involutions, then these are all the conj-reps. I’m looking for examples where the maps are not involutions. Also, it’s not hard to show all $1$ -dimensional ones must be of the form $r=s=t=\lambda \mathrm{Id}$ . In general, you can always scale a conj-rep, so I mean up to scalars.","['representation-theory', 'matrices', 'abstract-algebra', 'linear-algebra', 'nonassociative-algebras']"
4270808,The set of countable subsets is countable,Is it always true that a family of countable sets is always countable? If I prove that every $A\in B$ is countable is it enough to prove $B$ is countable?,"['elementary-set-theory', 'cardinals']"
4270828,Mathematically: are magnetic field lines (almost) closed loops?,"Question: Let $X:[0, 1] \rightarrow \mathbb{R}^3$ be a smooth curve, and $\mathbf{B}$ be the vector field in $\mathbb{R}^3$ defined as $$ \mathbf{B}(y) = \int_{\mathbb{R}^3} \frac{y-X(t)}{\|y-X(t)\|^3} \times X'(t) \ dt.$$ Considering the flow of $\mathbf{B}$ , here are some possibilities for its orbits: singleton (where $\mathbf{B} = 0$ ) closed loop (periodic orbit) Not closed, but limits (including point at infinity) as $t \rightarrow \infty$ and $t \rightarrow -\infty$ both exist. Can there exist other possibilites? (e.g. orbit which asymptotically approaches to a closed loop) Details: By Biot-Savart law, $\mathbf{B}$ is the magnetic field generated by current through the curve $X$ (I must have missed some constants), and the flow of $\mathbf{B}$ is called 'magnetic field lines.' I could found affirmative answers like 'Yes, because there is no magnetic monopole', but I desire to find mathematical proof/counterexample.","['mathematical-physics', 'ordinary-differential-equations', 'differential-geometry']"
4270862,Showing $ 2\sqrt{\frac{x+3}{x}}+8\sqrt{\frac{x+1}{x}}-\ln\left(\frac{(x+1)^{3/2}(x+3)}{(x-1)^{5/2}}\right)\geq 10 $ for $x\geq7$,"Suppose that $x\geq 7$ . I would like to show that $$ 2\sqrt{\frac{x+3}{x}} + 8\sqrt{\frac{x+1}{x}}-\ln\left(\frac{(x+1)^{3/2}(x+3)}{(x-1)^{5/2}}\right)\geq 10 $$ I rewrote the inequality as $$ 2\sqrt{\frac{x+3}{x}} +8\sqrt{\frac{x+1}{x}}-\ln\left(\frac{\left(\frac{x+1}{x}\right)^{3/2}\left(\frac{x+3}{x}\right)}{\left(\frac{x-1}{x}\right)^{5/2}}\right)\geq 10, $$ which is equivalent to $$  2\sqrt{\frac{x+3}{x}} -2\ln\left( \sqrt{\frac{x+3}{x}}\right)+8\sqrt{\frac{x+1}{x}}-3\ln\left( \sqrt{\frac{x+1}{x}}\right) \geq 10-5\ln\left(\sqrt{\frac{x}{x-1}} \right). $$ I am not sure if rewriting it in the last way helps. Wolfram Alpha shows me that the inequality is true, but I cannot figure out why. Any help is greatly appreciated. Thank you.","['inequality', 'logarithms', 'calculus', 'radicals', 'algebra-precalculus']"
4271063,"$\frac{\partial f}{\partial t}-\frac{k}{r^2} \left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f}{\partial r} \right)-2f\right)+g(r,t)=0$","Consider the following partial differential equation $$
\frac{\partial f (r,t)}{\partial t} - \frac{k}{r^2} 
\left( \frac{\partial}{\partial r} \left( r^ \, \frac{\partial f (r,t)}{\partial r} \right) - 2f(r,t) \right)
+ \frac{f(r=1, t)}{r^2}  = 0\, , 
$$ subject to the boundary conditions $$
\left. \frac{\partial f}{\partial r} \right|_{r=1} = 0\, , \qquad f(r=\infty, t) = 0 \, , 
$$ together with the initial condition $f(r, t=0) = \epsilon$ .
Here, $k$ and $\epsilon$ are positive real numbers, with $|\epsilon| \ll 1$ To search an analytical solution of this PDE, i used the method of separation of variables.
Accordingly, the solution is expressed as $f(r,t) = F(r)G(t)$ .
Using this approach, it can be shown that $G(t)$ satisfies the following differential equation $$
G'(t) + \operatorname{const.} G(t) = 0 \, .
$$ The latter admit the exponential function as a solution.
The problem is that, by considering an decaying exponential, one would obtain a vanishing solution in the long-time limit as $t \to \infty$ .
This is in contradiction with the numerical solution where, for instance, $f(r=1,t)$ reached a steady/plateau value in the limit $t \to \infty$ .
Does this mean that one cannot employ the method of separation of variables in this problem?","['integration', 'ordinary-differential-equations', 'real-analysis', 'partial-differential-equations', 'nonlinear-dynamics']"
4271082,What is the definition of a cusp of a plane curve and how to find them?,"I am pretty confused about what the definition of a cusp of a plane curve  is (let's say that the curve admits the parametrization $\gamma(t)=(f(t), g(t))$ ). On Wikipedia I found two possible definitions: one that says that it is a singular point (that is a point where $\gamma'(t)=0$ ) satisfying some condition on a directional derivative that I do not understand and one that says that it is a point where the curve is not differentiable. This confuses me because I was looking at this post How to make a sharp 5-pointed astroid in parametric coordinates? and from the comments I gather that the following curve $\gamma(\varphi)=(\frac{n - 1}{n}\cos(\varphi) + \frac{1}{n}\cos\left( (n-1) \varphi \right), \frac{n - 1}{n}\sin(\varphi) - \frac{1}{n}\sin\left( (n-1) \varphi \right)$ has $n$ cusps. So I think that cusps are indeed singular points, since this curve is everywhere differentiable. However, what other conditions should I impose for a cusp and how would I show that this particular curve has $n$ cusps?","['parametric', 'geometry', 'differential-geometry']"
4271127,Every hausdorff space has a non-hausdorff quotient.,"I know some Hausdorff spaces can have non-hausdorff quotient spaces. For example real line with double origin. I am wondering whether this is the case for all Hausdorff spaces. I tried some simpler examples like for a given Hausdorff space I tried to write a quotient map to a finite set with indiscrete topology but couldn't manage to find such map for an arbitrary Hausdorff space either. At this point, I have no clue about how to approach showing whether a Hausdorff space without a non-Hausdorff space exits or not. So in short, the question is written in the title. Does every Hausdorff space have a non-Hausdorff quotient space?","['general-topology', 'separation-axioms']"
4271138,"Is $x^n-\sum_{i=0}^{n-1}x^i$ irreducible in $\mathbb{Z}[x]$, for all $n$?","Let the sequence of polynomials $p_n$ from $\mathbb{Z}[x]$ be defined recursively as $$p_n(x)= xp_{n-1}(x)-1$$ with initial term $p_0(x)=1$ . Then $$p_n(x)= x^n-\sum_{i=0}^{n-1}x^i $$ Question 1: is it true that $p_n(x)$ is always irreducible in $\mathbb{Z}[x]$ ? The usual Eisenstein's criterion can not be applied directly here. I also know that in order to prove the irreducibility over $\mathbb{Z}[x]$ it would suffice to find a prime $q$ for which $p_n(x)$ is irreducible in $\mathbb{Z}/q \mathbb{Z} [x]$ . In $\mathbb{Z}/q \mathbb{Z} [x]$ one may write $$p_n(x)= x^n+\sum_{i=0}^{n-1}(q-1)x^i .$$ Question 2: Can we also use Eisenstein criterion to prove irreducibility in $\mathbb{Z}/q \mathbb{Z} [x]$ ? If yes, then the answer to question 1 would be yes as well, as it would suffice to choose $q=3$ since $2^2$ does not divide $3$ .  Is this correct?","['irreducible-polynomials', 'number-theory', 'polynomials', 'elementary-number-theory']"
4271159,A condition that implies $f_* \mathcal{O}_X \cong \mathcal{O}_S$,"I am reading the lecture note of Dori Bejleri about Picard schemes: https://people.math.harvard.edu/~bejleri/teaching/math259xfa19/math259x_lecture12.pdf In Example 12.8, I don't understand why the smooth and irreducible generic fiber implies that $f:X\rightarrow S$ is a universal algebraic fiber space, i.e. $f_* \mathcal{O}_X \xrightarrow{\sim} \mathcal{O}_S$ holds universally. I have tried to find some conditions to be a universal algebraic fiber space, but I only find a condition in the notes of Picard schemes of Kleiman (Exercise 3.11, when $f:X \rightarrow S$ is proper and flat and its geometric fibers are reduced and connected): https://arxiv.org/abs/math/0504020 Can anyone help me to explain the example of Bejleri? Are there any other conditions implying this universal isomorphism?","['algebraic-geometry', 'picard-scheme']"
4271189,Doubt regarding splitting of the limit $ \lim\limits_{x \to 0} \dfrac{\sin^2(3x)}{x^2} $,"I have a simple question about the logic of the limits of trig functions. For example, let's say you have the following question: $ \lim\limits_{x \to 0} \dfrac{\sin^2(3x)}{x^2} $ What's preventing me from doing this: $ \lim\limits_{x \to 0} \sin(3x)\dfrac{\sin(3x)}{x^2} $ and then simplifying it to this: $ \lim\limits_{x \to 0} \sin(3x) \times  \lim\limits_{x \to 0} \dfrac{\sin(3x)}{x^2} $ and finally this: $0 \times \lim\limits_{x \to 0} \dfrac{\sin(3x)}{x^2}$ Thanks for the help.","['limits', 'calculus']"
4271237,How do I prove the following equality? $(C \cup (A^C \cup(B−A))^C)^C=(A−A)^C \cap((C^C \cap A)−(C^C \cap B))\cup C^C$,"Let the sets A, B, C ⊆ U be such that A ⊆ C. Prove the following equality by means of the laws of set algebra: $$\biggl(C \cup \Bigl(A^C \cup (B-A)\Bigr)^C\biggr)^C = (A-A)^C \cap \Bigl((C^C \cap A)-(C^C \cap B)\Bigr) \cup C^C$$ This is what I have done: $$\biggl(C \cup \Bigl(A^C \cup (B-A)\Bigr)^C\biggr)^C = (A-A)^C \cap \Bigl((C^C \cap A)-(C^C \cap B)\Bigr) \cup C^C$$ $$ \varnothing^C \cap \bigl((C^C \cap A) - (C^C \cap B)\bigr) \cup C^C    \text{ by Set Difference Law} $$ $$ U \cap \bigl((C^C \cap A) - (C^C \cap B)\bigr) \cup C^C       \text{ by Complement Law} $$ $$ \bigl((C^C \cap A) - (C^C \cap B)\bigr) \cup C^C              \text{ by Identity Law} $$ $$ \bigl((C^C \cap A) \cap (C^C \cap B)^C \bigr) \cup C^C        \text{ by Set Difference Law} $$ $$ \bigl((C^C \cap A) \cap C \cup B^C \bigr) \cup C^C            \text{ by De Morgan Law} $$ So, as you can see, I am pretty confused about how to go about solving this. Does anyone have any ideas? By the way, I would appreciate if you could state which theorems you applied to reach your answer. P.S. In case that you think that something that I wrote sounds somewhat weird, I translated this from Spanish.",['elementary-set-theory']
4271253,How to show this $\phi$ mixing inequality?,"Let $(\Omega,\mathcal A,P)$ be a probability space and $\mathcal F$ a sub- $\sigma$ -algebra of $\mathcal A$ . Am trying to show that $$ |P(A|\mathcal F)-P(A)|\leq \phi(\mathcal A,\mathcal F) \quad \quad P\text{-almost surely}$$ for all $A\in \mathcal A$ , where $\phi(\mathcal A,\mathcal F)=\sup_{A\in \mathcal A, F\in \mathcal F, P(F)>0} \bigg|P(A|F)-P(A)\bigg|$ . Seems true but I don't see where to start. Any ideas on how to proceed?","['conditional-probability', 'conditional-expectation', 'probability-theory', 'probability', 'mixing']"
4271259,Why Do We Care About Hölder Continuity?,"I have often encountered Hölder continuity in books on analysis, but the books I've read tend to pass over Hölder functions quickly, without developing applications. While the definition seems natural enough, it's not clear to me what we actually gain from knowing that a function is $\alpha$ -Hölder continuous, for some $\alpha<1$ . I have some guesses, but they are just guesses: do $\alpha$ -Hölder conditions give rise to useful weak solution concepts in PDEs? Are there important results that apply only to $\alpha$ -Hölder functions, for some fixed $\alpha$ ? For $\alpha=1$ (Lipschitz continuity) the answer to both of these questions seems to be yes, but I know nothing for lower values of $\alpha$ . I'd be interested in answers that describe specific applications, as well as answers that give more of a ''big picture''.","['real-analysis', 'functional-analysis', 'partial-differential-equations', 'holder-spaces', 'dynamical-systems']"
4271306,On the submodule of a direct sum of two modules,"Let $R$ be a ring and $M_1, M_2$ be two $R$ -modules (nice properties can be assumed, such as finitely generated, artinian, etc). Let $U$ be a submodule of $M_1\oplus M_2$ . Let $\pi_2: M_1\oplus M_2\to M_2$ be the canonical projection. Then it is clear that the sequence of $R$ -modules $$0\to U\cap M_1 \to U \to \pi_2 U \to 0$$ is exact. Though it is clear that $U$ does not necessarily coincide with $U\cap M_1\oplus \pi_2U$ , I am wondering if we always have $U$ isomorphic to $U\cap M_1 \oplus \pi_2 U$ , as I have some troubles finding counter-examples. Any comments will be appreciated.","['abstract-algebra', 'examples-counterexamples', 'modules']"
4271309,How did Dana Scott show in ZF- that sets are hereditarily ext-invariant if they are ext-invariant and only have hereditarily ext-invariant members?,"From Dana Scott: More on the axiom of extensionality, in Essays on the foundations of mathematics, dedicated to A. A. Fraenkel on his seventieth anniversary, edited by Y. Bar-
Hillel, E. I. J. Poznanski, M. O. Rabin, and A. Robinson for The Hebrew University of Jerusalem, Magnes Press, Jerusalem 1961, pp. 115–131. On page 118, Scott introduces the following three definitions: $a\overset{=}{.}b\leftrightarrow\forall x(x\in a\leftrightarrow x\in b)$ $\mathbb{I}(a)\leftrightarrow\forall x,y(x\in a\wedge x\overset{=}{.}y\to y\in a)$ $\mathbb{H}(a)\leftrightarrow\mathbb{I}(a)\wedge\exists x(a\subseteq x\wedge \forall y(y\in x\to \mathbb{I}(y)\wedge y\subseteq x))$ A set $a$ is ext-invariant just if $\mathbb{I}(a)$ , and hereditarily ext-invariant just if $\mathbb{H}(a)$ . Axiom of extensionality is (I), finite union is (II), infinite union (III), power set (IV), infinity (V), subsets (VI), replacement (VII). $ZF^{\not =}$ is (II) to (VII). On page 130 Scott writes: ""What would happen if (VII) [replacement, in $ZF^{\not =}$ ] were replaced by (VII') $\forall x,y,z[\Phi(x,y)\wedge \Phi(x,z)\to x\overset{=}{.}y]\to\exists w\forall y[y\in w\leftrightarrow\exists x[x\in a\wedge\Phi(x,y)]] \ldots ?$ The theory based on (II) to (VI) and (VII') is certainly intermediate between $ZF^{\not =}$ and $ZF$ . It is in fact much stronger than $ZF^{\not =}$ , for it is possible to prove in this theory the statement $\mathbb{I}(a)\wedge\forall x(x\in a\to\mathbb{H}(x))\to \mathbb{H}(a)$ Using this formula together with (VII') the method of Section 2 can be applied directly to show that $ZF$ is indeed interpretable in this new theory."" Let $ZF-$ be Scott's system based upon (II)-(VI) and (VII'). How did Scott prove his statement in $ZF-$ ? Update: The satisfactory x, $a\subseteq x$ , can be constructed as the $\overset{=}{.}$ -adjusted transitive closure of $a$ .","['elementary-set-theory', 'logic', 'set-theory']"
4271314,What is the proof for variance of triangular distribution?,"In Wikipedia, the formula for the variance of the triangular distribution is given here . However, I don't know how to find it. I have tried a brute force method but the formula is quite complicated (polynomial of degree 5 in a, b, c) and I can't simplify it (I tried manually and with Xcas). The following part is edited thanks to @Imaosome remark: I came to this question with the following problem:
Say $X$ and $Y$ are independent random variables with uniform distribution between $0$ and $1$ .
I want to study $Z = X-Y$ . With convolution , I find that the distribution is triangular, centered in $0$ with extremities $-1$ and $1$ (the proof is also available in this pdf here ). With Wikipedia notations, it gives $a=-1, b=1, c=0$ . And in this case, we sum $2$ independent variables therefore the variance shoud be $Var(X)+Var(-Y) = Var(X)+Var(Y)=2 Var(X)$ . The variance of $X$ is $1/12$ (see for instance formula here ). Therefore the variance of $Z$ is $Var(Z) = 2 * 1/12 = 1/6$ . If I come back to Wikipedia formula, I find: $$
\frac{a^2+b^2+c^2-ab-ac-bc}{18}=\frac{1+1+0+1-0-0}{18}=1/6.
$$ This shows, at least in that particular case that the formula is correct. But I would like to try and prove Wikipedia result. Once again, I know that it should be possible to prove it by integration but I did not succeed and I hope somebody has a simple way to get this formula. Here are the details. By definition, we want to compute: \begin{align*}
\sigma^2 &= \int_a^c \frac{2(x-a)}{(b-a)(c-a)} \left( x - \frac{a+b+c}{3} \right)^2 dx + \int_c^b \frac{2(b-x)}{(b-a)(b-c)} \left( x - \frac{a+b+c}{3} \right)^2 dx \\
&= \frac{2}{(b-a)(c-a)} \left[ \frac{1}{3} (x-a) \left( x - \frac{a+b+c}{3} \right)^3 - \frac{1}{12} \left( x - \frac{a+b+c}{3} \right)^4 \right]_a^c \\
& ~~~~~~ \frac{2}{(b-a)(b-c)} \left[ \frac{1}{3} (b-x) \left( x - \frac{a+b+c}{3} \right)^3 - \frac{1}{12} \left( x - \frac{a+b+c}{3} \right)^4 \right]_c^b
\end{align*} From there, one can see that terms in $\left( x - \frac{a+b+c}{3}\right)$ cancel out leaving us with: \begin{align*}
\sigma^2 &= \frac{2}{12(b-a)(c-a)} \left( - \left( \frac{2c-a-b}{3} \right)^4 + \left( \frac{2a-b-c}{3} \right)^4 \right) \\
& ~~~~~~ \frac{2}{12(b-a)(b-c)} \left( - \left( \frac{2b-a-c}{3} \right)^4 + \left( \frac{2c-a-b}{3} \right)^4 \right) \\
&= \frac{((c-a)-(b-c))^5 +(b-c)((b-a)+(c-a))^4 - (c-a)((b-a)+(b-c))^4}{ 2 \times 3^5 (b-a)(b-c)(c-a) }
\end{align*} And from this point, I am stuck. After all, maybe the last line is not helping much. I don't know.","['statistics', 'uniform-distribution', 'variance']"
4271348,If $f\left(0\right)=1$ and $f'\left(x\right)>3f\left(x\right)\ $ $∀\ x\ \ge0$ then prove that $f\left(x\right)\ge e^{3x}\ ∀\ x\ \ge0$,"Q: $f\left(x\right)$ is a continuous and differentiable function defined in $[0,\infty)$ . If $f\left(0\right)=1$ and $f'\left(x\right)>3f\left(x\right)\ $ $∀\ x\ \ge0$ then prove that $f\left(x\right)\ge e^{3x}\ ∀\ x\ \ge0$ Approach: Given $$f'\left(x\right)>3f\left(x\right)\ $$ Let $f\left(x\right)=t$ , $$\to\frac{dt}{dx}>3t\ \to\frac{dt}{t}>3dx$$ since $t\ (or f(x)) >0$ , equality sign doesn't remain same. $$\ln t>3x+C$$ Put $x=0\ \to f\left(0\right)=1$ ,
We get, $$\ln1>0+C$$ $$0>0+C$$ What do I do now, how shall I proceed further, where am I wrong? Also, $t\ (or f(x)) >0$ because $f\left(0\right)=1$ and $f'\left(x\right)>3f\left(x\right)\ $ so $f'\left(x\right)$ is always positive so function only takes positive values. Need help","['calculus', 'ordinary-differential-equations', 'real-analysis']"
4271395,"Use class algebra to prove that if $A⊆B$ and $C=B−A$, then $A=B−C$.","In A Book Of Set Theory by Charles C. Pinter, in Exercise 1.3 Question 11, that question is asked. I have solved it without class algebra. However, using class algebra, I have gotten stuck at a certain step. I’ve tried introducing the union with the empty set and replacing it with A and it’s complement as well as the other letters, but all approaches I take seem to leave me with a lingering letter. Any pointers in the right direction would be appreciated.",['elementary-set-theory']
4271433,"Is there a slick argument to prove that, for $n>6$, $2(n-2)!=2^kk!(n-2k)! \implies k=1$?","In a book, this implication is used to prove that every automorphism of $S_n$ is interior if $n \neq 6$ by comparing the cardinality of the centralizer of a transposition and the centralizer of the image of said transposition by an automorphism (which has to be a product of $k$ transpositions due to it being of order $2$ ). The equality $k=1$ shows an automorphism of $S_n$ for $n\neq 6$ sends transpositions on transpositions and permits to conclude that it is interior. However, it is not obvious to me that $k=1$ ... My first idea is obviously to try to compare the exponent of $2$ in the prime decomposition, but this doesn't help too much $$1+\sum_{\ell=1}^\infty\left\lfloor \frac{(n-2)}{2^\ell}\right\rfloor = k+\sum_{\ell=1}^\infty\left\lfloor \frac{k}{2^\ell}\right\rfloor +\sum_{\ell=1}^\infty\left\lfloor\frac{n-2k}{2^\ell}\right\rfloor$$ Is there a slick way to go about it without using a computer? The only thing that's obvious to me is that if $k>1$ , then one side grows much larger than the other side for $n$ large enough, so if $n$ is large enough then $k=1$ , but I don't see how ""large enough"" means $n>6$ . It is obvious that $k$ is odd so if $k>1$ then $k \geq 3$ so it is intuitive that the ""large enough"" is not so large, but I'd like an easy argument to prove that $n>6$ shall suffice.","['elementary-number-theory', 'combinatorics']"
4271475,"Do the left and right ""parts"" of the matrix used when performing Gaussian Elimination have names?","Given a matrix used to perform Gaussian Elimination like this: $$
\begin{bmatrix}
1 & -3 & | & 1 \\
2 & -7 & | & 3
\end{bmatrix}
$$ Which would be derived from a system of equations like this: $$
\begin{cases}
x - 3y = 1 \\
2x - 7y = 3
\end{cases} 
$$ Do the different ""sides"", i.e. the parts that come from the right hand side of the equation and the part that comes from the left hand side of the equation, respectively, have names?","['matrices', 'gaussian-elimination', 'terminology']"
4271491,Left ideals of $\mathrm{End}_{k}(V)$,"I'm trying to solve the following problem: $\DeclareMathOperator{\End}{End}$ Let V be a vector space over a field $K$ with $\dim_k V =n$ and consider the ring $R = \End_k(V)$ . If $U$ is a subspace define $I_U = \{ f \in R : U \subset \ker f\}$ . Show that $I_U$ is a left ideal of $R$ and that every left ideal has this form. Showing $I_U$ is left ideal is straightforward. I'm having trouble with the second part. I started taking $I$ left ideal of $R$ . And define $m = \min \{\dim(\ker f) : f \in I\}$ and $U = \ker f $ where $f$ is any element of $I$ with $\dim(\ker f) = m$ . Then I want to show that $I=I_U$ . If $g \in I_U$ I need to construct $h$ such that $g= h \circ f$ . However, I'm having trouble defining $h$ .
For the other inclusion, I don't know where to start.","['ring-theory', 'abstract-algebra', 'linear-algebra', 'ideals']"
4271513,Number of compositions with sum N [duplicate],"This question already has answers here : number of ordered partitions of integer (4 answers) Closed 2 years ago . I was wondering if any of you folks could help me with this combinatorial problem. I need to show the exact number of compositions (c1, c2 , c3, ...) that have a sum of N is $2^{N-1}$ .
For example, for $n=4$ we have $C_N = \{(4) , (3,1), (2,2), (1,3), (2,1,1), (1,2,1), (1,1,2), (1,1,1,1)\}$ with $|C_N| = 2^{4-1} = 8$",['combinatorics']
4271537,What is the probability of certain numbers to be included in a drawn sample?,"Say I have 10 ping pong balls, these balls are labeled 1 through 10. I pick 4 with replacement. What are the chances that the 4 balls I picked are the numbers 3, 5, 7, and 9? I have calculated the possibilities of choosing 4 from these 10, I believe there are 10,000 possibilities. I did this by 10^4.
I also know that each number has 1/10 chance of getting chosen with replacement. But I am not sure where to go from here. Any help would be greatly appreciated!","['permutations', 'statistics', 'combinations', 'factorial', 'probability']"
4271552,An elegant proof of monotone convergence theorem,"I have found an elegant proof of monotone convergence theorem here . I represent the proof below. It's so simple that I'm afraid if I miss some subtle detail. Could you please check if my understanding is fine? Let $( X_{n} )$ be a sequence of non-negative random variables such that $X_n \nearrow X$ a.s. Then $\mathbb{E}\left(X_{n}\right) \nearrow \mathbb{E}\left( X \right)$ . Proof 1: For all $n$ , there exists a sequence $(X_{n,k})_k$ of non-negative simple random variables such that $X_{n, k} \nearrow X_n$ a.s. as $k \to \infty$ . Let $$Y_{k} = \max_{n \le k} X_{n, k}, \quad k \in \mathbb N.$$ Clearly, we have $X_{n,k} \le Y_k \le X_k \le X$ a.s. for all $n \le k$ $(\star)$ . $\mathbb E (Y_k) \le \mathbb E (X_k) \le \mathbb E (X)$ for all $k$ $(\star \star)$ . $(Y_k)$ is a non-decreasing sequence of simple random variables. Let $Y=\lim_{k \to \infty} Y_k$ . Then $\mathbb E (Y) := \lim_{k \to \infty} \mathbb E (Y_k)$ by definition. Taking the limit $k \to \infty$ in $(\star)$ , we get $$\lim_{k \to \infty} X_{n, k} \le \lim_{k \to \infty} Y_k \le \lim_{k \to \infty} X_k \le X \text{ a.s.}, \quad n \in \mathbb N,$$ and consequently $$X_{n} \le Y \le X \text{ a.s.}, \quad n \in \mathbb N \quad (\star \star \star).$$ Taking the limit $n \to \infty$ in $(\star\star\star)$ , we get $X=Y$ a.s. and thus $\mathbb E(X) = \mathbb E(Y)$ . Taking the limit $n \to \infty$ in $(\star\star)$ , we get $\mathbb E(Y) \le \lim_{k \to \infty} \mathbb E (X_k) \le \mathbb E (X)$ . This completes the proof. Proof 2: Clearly, $\mathbb{E}\left(X_{n}\right) \le \mathbb{E}(X)$ for all $n$ and thus $$\lim_n \mathbb{E}\left(X_{n}\right) \le \mathbb{E}(X).$$ Take $\lambda >1$ and a non-negative simple random variable $Y$ such that $Y \le X$ . Define $A_n = \{\omega \mid \lambda X_n(\omega) \ge Y\}$ . It follows from $X_{n} \nearrow X$ that $A_{n} \nearrow A:= \bigcup A_n$ . Let's show that $\mathbb P (A) = 1$ . Given $\omega$ such that $X_n (\omega) \to X (\omega)$ as $n \to \infty$ , If $X(\omega) = 0$ , then $X_n (\omega) = Y (\omega) =0$ and thus $\omega \in A_n$ for all $n$ . If $X(\omega) > 0$ , then $\varepsilon := \lambda X (\omega) - Y (\omega) > 0$ and there exists $m$ such that $X (\omega) - X_m (\omega)\le \varepsilon$ . Then $\lambda X_m(\omega) \ge \lambda X(\omega) - \varepsilon = Y (\omega)$ . As such, $\omega \in A_m$ . We also have $\lambda X_n \ge Y 1_{A_n}$ and thus $\lambda \mathbb E(X_n) \ge \mathbb E (Y 1_{A_n})$ for all $n$ . As such, $$\lambda \lim_n \mathbb E(X_n) \ge \lim_n \mathbb E (Y 1_{A_n})  \overset{(\star)}{=}  \mathbb E (Y \lim_n 1_{A_n})  = \mathbb E(Y 1_A) \overset{(\star\star)}{=} \mathbb E(Y),$$ where $(\star)$ is because $Y, 1_{A_n}$ and hence $Y 1_{A_n}$ are simple. $(\star\star)$ is because $\mathbb P (A)=1$ . Take the limit $\lambda \searrow 1$ , we obtain $$\lim_n \mathbb E(X_n) \ge \mathbb E(Y).$$ This inequality holds for any a non-negative simple random variable $Y$ such that $Y \le X$ . Then $$\lim_n \mathbb E(X_n) \ge \mathbb E (X).$$ This completes the proof. Update: After a while, I have found that these two proofs also hold in a  more general setting of measure theory. We just need to replace $\mathbb E [X]$ by $\int f \mathrm d \mu$ .","['measure-theory', 'solution-verification', 'convergence-divergence', 'probability-theory']"
4271563,Doubt about the statement of the Fundamental Theorem of Algebra.,"I have a doubt, not about the proof, but about the nomenclature of a theorem. I'm researching some interesting applications of the Inverse Function Theorem and found a version of the Fundamental Theorem of Algebra, the proof of which applies that theorem. The version is as follows: Fundamental Theorem of Algebra: If $p:\mathbb{C} \to \mathbb{C}$ is the nonconstant polynomial defined by $$p(z) = a_0 + a_1z + \ldots + a_nz^n,$$ where $a_n \neq 0$ and $n \geq 1$ , then $p$ is surjective. In particular, there exists $z_0 \in \mathbb{C}$ such that $p(z_0) = 0$ . I found this version a bit ""strange"" as it looks weaker than the following: Fundamental Theorem of Algebra: If $p:\mathbb{C} \to \mathbb{C}$ is a nonconstant complex polynomial defined by $$p(z) = a_0 + a_1z + \ldots + a_nz^n,$$ where $a_n \neq 0$ and $n \geq 1$ , then the equation $p(z) = 0$ has $n$ solutions, not necessarily distinct. Is the first version I listed correct? Are they equivalent? To me they don't seem to be, the first seems to be weaker than the second. Taking advantage of the question, I would like to ask for suggestions for the application of this theorem (of Inverse Function in the case).","['calculus', 'abstract-algebra', 'complex-numbers', 'analysis']"
4271581,"How to understand lindeberg CLT, compare with classical CLT","For Lindeberg CLT. Let $\{X_{nj},j=1,2,...k_n\}$ be independent r.v.s with $0<\sigma^2_n=Var(\sum_{j=1}^{k_n}X_{nj})<\infty$ , $n=1,2,...$ and $k_n\rightarrow\infty$ as $n\rightarrow\infty$ . If $$\sum_{j=1}^{k_n}E[(X_{nj}-EX_{nj})^2I_{\{|X_{nj}-EX_{nj}|>\epsilon\sigma_n\}}]=o(\sigma^2_n)$$ for any $\epsilon>0$ . (Lindeberg condition) Then, $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ How to understand this Lindeberg condition? If we consider iid case, with $E(X_{n1})=\mu, Var(X_{n1})=\sigma^2$ , the lindeberg condition becomes: $$\frac{\sum_{j=1}^{k_n}E[(X_{nj}-\mu)^2I_{\{|\frac{X_{nj}-\mu}{\sigma}|>\epsilon\sqrt{k_n}\}}]}{k_n\sigma^2}\rightarrow 0$$ as $k_n\rightarrow\infty$ , so $$\frac{1}{k_n}\sum_{j=1}^{k_n}E[(\frac{X_{nj}-\mu}{\sigma})^2I_{\{(\frac{X_{nj}-\mu}{\sigma})^2>\epsilon^2k_n\}}\rightarrow 0$$ as $k_n\rightarrow\infty$ , so Let $Z_{nj}=\frac{X_{nj}-\mu}{\sigma}$ $$\frac{1}{k_n}\sum_{j=1}^{k_n}E[(Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ , so $$E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ .(is this true?) Since we have $E(X_{nj})=\mu, Var(X_{nj})=\sigma^2$ , $E(Z_{nj})=0, Var(Z_{nj})=1$ . So $E(Z_{nj})^2=E(Z_{nj})^2+Var(Z_{nj})=1<\infty$ . (Is this correct?). From $E(Z_{nj}^2)<\infty$ .
We can already derive $$E[Z_{nj}^2I_{\{Z_{nj}^2>\epsilon^2k_n\}}]\rightarrow 0$$ as $k_n\rightarrow\infty$ , by dominated convergence theorem see prove For non-negative r.x. $X$ , $E(X)<\infty$ iff $E(XI_{X>x})\rightarrow 0$ as $x\rightarrow\infty$ , (is this true?). Then how this Lindeberg condition be helpful? Are there any mistakes above? Or does it means that for the iid case, Lindeberg's condition always holds? From Lindeberg CLT's result $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ For iid case, this result becomes $$\frac{\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})}{\sqrt{k_nVar(X_{n1})}}\rightarrow_dN(0,1)$$ $$\frac{1}{k_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{n1})\rightarrow_dN(0,Var(X_{n1})),$$ which is the same as classical CLT's result. For $\{X_{nj}, j=1,2,...,k_n;n=1,2,3,...\}$ is triangle arrow, since $k_n\le n$ , and $k_n\rightarrow\infty$ as $n\rightarrow\infty$ . How this ""triangle"" condition is used in ""iid"" case?, since this result means that for every r.v. line $\{X_{nj},j=1,2,...k_n\}$ , CLT holds For the classical CLT, we have $$\sqrt{n}(\bar{X}-EX)\to_d N(0,Var(X_1)),$$ Here $\sqrt{n}$ plays a role of converges speed (rate) (Is my understanding correct?) whice is so called "" $\sqrt{n}_consistency$ "" in statistics If so, what's the converges speed in $$\frac{1}{\sigma_n}\sum_{j=1}^{k_n}(X_{nj}-EX_{nj})\rightarrow_dN(0,1)$$ ? Is the converging speed $\frac{1}{\sigma_n}$ or $\frac{1}{\sqrt{k_n}}$ ? Can a convergence rate depend on $Var(X_i)$ ? (I personally don't think so).
If not,  what's the converge rate?","['statistics', 'central-limit-theorem', 'probability-theory', 'probability']"
4271593,"100-level discrete maths, induction problem, prove $n^2 \ge 2n + 1$","I've just run into this problem, and was able to go as far, and understand the induction step up to the bolded section. The last part I found in the back of my book, italicized, I can't understand. Use induction to prove: $n^2\geq 2n + 1$ for all $n\in \mathbb{N}$ and $n\geq 3$. Base: $3^2\geq 6+1$. Induction Hypothesis: Assume that $k^2\geq 2k+1$ for $k\geq 3$. Induction Step: $(k + 1)^2=k^2+2k+1\geq 2k+1 + 2k + 1 = 2(k + 1) + 2k \geq 2(k + 1) + 1$ I went as far as $(k + 1)^2 \geq 2(k+1) + 1$? Any help, much appreciated","['induction', 'discrete-mathematics']"
4271629,"Compute derivative of $k(t)=\int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$","I'm trying to compute the derivative of $k(t) = \int^{\infty}_{-\infty} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$ . I've already showed that it exists, so here's what I've thought of so far:
letting $k_n(t) = \int^n_{-n} \frac{\sin{tx^2}}{1+x^4}\,\textrm dx$ , and using the $\int^\infty_{-\infty} \frac{\textrm dx}{1+x^4}$ bound (and symmetry to write $2 \int_0^\infty$ ), I can show the sequence converges uniformly. Now, letting $h(\delta, t, x) = \frac{\sin{tx^2} - \sin{(t+\delta)x^2}}{\delta} - x^2 \cos{tx^2}$ , by considering we are in $[-n, n]$ , and using uniform continuity and MVT, we get $h(\delta, t, x) < \epsilon$ for sufficiently small delta for a fixed $t$ with $x \in[-n, n]$ . Thus, since we are in a finite interval, we have $k_n^\prime (t) = \int^n_{-n} \frac{x^2 \cos{tx^2}}{1+x^4}\,\textrm dx$ . I think I could then finish it using the bound $\int^\infty_{-\infty} \frac{\textrm dx}{x^2}$ to show the derivatives converge uniformly and then apply the uniform convergence result on derivatives. I would like to ask if my reasoning is correct and I can finish it this way or if there are any mistakes along it. Any help is appreciated!","['real-analysis', 'calculus', 'solution-verification', 'derivatives', 'leibniz-integral-rule']"
4271648,Doubt in one step of the proof.,"I'm dubious in step one proof. The reference I'm following claims to use the Implicit Function Theorem, but I haven't seen where it was used. Lagrange Multipliers Theorem: Let $f:U\to\mathbb{R}$ a differential function in a open $U \subset \mathbb{R}^{m+n}$ and $M = \varphi^{-1}(c)$ a orietable surface in $U$ , inverse image of regular value $c \in \mathbb{R}^n$ by a aplication $\varphi:U \to \mathbb{R}^n$ , of class $C^1$ . A point $p \in M$ is critic of restriction $f|M$ iff there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p),$$ where $\varphi_1, \ldots, \varphi_n: U \to \mathbb{R}$ are the coordinates of $\varphi$ . Proof: Assume the notation of the statement. Since $c$ is a regular value of $\varphi$ , then for each $x \in M$ , the vectors $\nabla \varphi_1 (x) ,\ldots, \nabla \varphi_n(x)$ are linearly independent and so this form a basis of vector space normal in $M$ in the point $x$ , denoted by $(T_xM)^{\bot}$ . In this case, if $f:U \to \mathbb{R}$ a differentiable function, then $p \in M$ is a critic point of the $f|M$ iff the vector $\nabla f(p)$ is a linear combination of vectors $\varphi_i (p)$ , ie,  there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p).$$ I can't figure out where he used the implicit function theorem, I believe it's to show that those vectors form the basis, but I'm not convinced of that. I looked at the case in one variable, it seems to be in that step. Thank you in advance for the tips. Next I did a proof trying to explain where the theorem was used. Proof : If $x \in M$ , then by $M = \varphi^{-1}(c)$ , it follows that $\varphi(x) = c$ and so, since $c$ is a regular value of $\varphi=(\varphi_1, \ldots, \varphi_n):U \to \mathbb{R}^n$ , then $\nabla \varphi_i(x) \neq 0$ , for all $i = 1 ,\ldots, n$ . Thus, by the implicit function theorem, there exists a open $Z \subset U$ with $x \in Z$ such that $\varphi^{-1}(c) \cap Z$ is the graphic of function $\xi:V \to \mathbb{R}^n$ , o class $C^k$ in a open $V \subset \mathbb{R}^n$ . Thus, considering the open $V$ , we have that the set $\{\nabla \varphi_1 (x), \ldots, \nabla \varphi_n(x)\}$ is  linearly independent and so this form a basis of vector space normal in $M$ in the point $x$ , denoted by $(T_xM)^{\bot}$ . In this case, if $f:U \to \mathbb{R}$ a differentiable function, then $p \in M$ is a critic point of the $f|M$ iff the vector $\nabla f(p)$ is a linear combination of vectors $\varphi_i (p)$ , ie,  there exists $\lambda_1, \ldots, \lambda_n$ such that $$\nabla f(p) = \lambda_1 \cdot \nabla \varphi_1 (p) + \ldots + \lambda_n \cdot \nabla \varphi_n(p).$$ Is this second version correct? I'm not convinced it's right. In short, my question is: at what point is the Implicit Function Theorem being used. Taking the question, I would like to ask what would be another interesting application of this theorem in the context of analysis in $\mathbb{R}^n$ .","['lagrange-multiplier', 'analysis', 'calculus', 'solution-verification', 'implicit-function-theorem']"
4271672,"Given numbers $1$ to $3n$, construct $n$ equations of the form $a + b = c$ or $a \times b = c$ such that each number is used exactly once.","Given numbers $1$ to $3n$ , construct $n$ equations of the form $a + b = c$ or $a \times b = c$ such that each number is used exactly once. For example: n=1 => 1+2=3
n=2 => 1+4=5, 2x3=6
n=3 => 4+5=9, 1+7=8, 2x3=6 The question is, does a solution exist for every n? I tried writing a basic program and it becomes too slow after n = 14. Here are the solutions I have so far: 1 ['1+2=3']
2 ['2*3=6', '1+4=5']
3 ['4+5=9', '1+7=8', '2*3=6']
4 ['3+6=9', '1+10=11', '4+8=12', '2+5=7']
5 ['2+8=10', '3+6=9', '1+13=14', '5+7=12', '11+4=15']
6 ['3*5=15', '2+8=10', '4+14=18', '6+11=17', '7+9=16', '1+12=13']
7 ['6+12=18', '3*5=15', '7+10=17', '1+20=21', '4+9=13', '2+14=16', '8+11=19']
8 ['8+14=22', '6+12=18', '7+10=17', '2+19=21', '1+15=16', '11+13=24', '4+5=9', '3+20=23']
9 ['6+19=25', '8+14=22', '4+13=17', '2+18=20', '1+26=27', '3+7=10', '9+15=24', '5+16=21', '11+12=23']
10 ['6+19=25', '14+15=29', '11+17=28', '4+26=30', '2+18=20', '1+21=22', '3*9=27', '8+16=24', '5+7=12', '10+13=23']
11 ['10+23=33', '6+19=25', '14+15=29', '11+17=28', '4+26=30', '2+18=20', '5+27=32', '1+12=13', '9+22=31', '3*7=21', '16+8=24']
12 ['10+23=33', '3+29=32', '6+19=25', '15+21=36', '11+17=28', '8+14=22', '4+16=20', '7+27=34', '2*12=24', '1+30=31', '5+13=18', '9+26=35']
13 ['10+23=33', '3+29=32', '7+30=37', '6+19=25', '5+34=39', '15+21=36', '11+17=28', '18+20=38', '4+31=35', '1+26=27', '9+13=22', '8+16=24', '2+12=14']
14 ['10+23=33', '4+37=41', '3+29=32', '9+25=34', '15+21=36', '11+17=28', '8+14=22', '6+24=30', '13+27=40', '5*7=35', '2+18=20', '1+38=39', '12+19=31', '16+26=42'] Here's the code for the program: def main(n):
    r = set(range(1, n*3+1))
    print(n, solve(n, r, []))

def solve(n, lst, solution):
    if not lst:
        if len(solution) != n:
            return False
        return solution
    cs = set(combinations(lst, 3))
    for c in cs:
        valid_solution = valid(c)
        if valid_solution:
            new_solution = solution + [valid_solution]
            result = solve(n, set(lst) - set(c), new_solution)
            if result:
                return result
    return False

def valid(lst):
    a = lst[0]
    b = lst[1]
    c = lst[2]
    if a + b == c:
        return ""%s+%s=%s"" % (a, b, c)
    if a * b == c:
        return ""%s*%s=%s"" % (a, b, c)
    return False ```","['number-theory', 'arithmetic', 'combinatorics']"
4271770,Understanding the proof of the Implicit Mapping Theorem,"I am following Advanced Calculus of Several Variables by C.H. Edwards, Jr. I failed to build the logic of the theorem III- $3.4$ stated below, Theorem $3.4$ : Let the mapping $G: \mathscr{R}^{m+n} \rightarrow \mathscr{R}^{n}$ be $\mathscr{C}^{1}$ in a neighborhood of the point $(a,b)$ where $G(a,b)=0$ . If the partial derivative matrix $D_{2} G(a, b)$ is nonsingular,  then there exists a neighborhood $U$ of $a$ in $\mathscr{R}^{m}$ , a neighborhood $W$ of $(a, b)$ in $\mathscr{R}^{m+n}$ , and a $\mathscr{C}^{1}$ mapping $h: U \rightarrow \mathscr{R}^{n}$ , such that $y=h(x)$ solves the equation $G(x, y)=0$ in $W$ . In particular, the implicity defined mapping $h$ is the limit of the sequence of successive approximations defined inductively by, $$
\begin{aligned}
&\qquad h_{0}(\mathbf{x})=\mathbf{b}, \quad h_{k+1}(\mathbf{x})=h_{k}(\mathbf{x})-D_{2} G(\mathbf{a}, \mathbf{b})^{-1} G\left(\mathbf{x}, h_{k}(\mathbf{x})\right)
\end{aligned}
$$ for $\mathbf{x} \in U$ . Theorem $3.3$ : Suppose that the mapping $f:\mathscr{R}^n\rightarrow\mathscr{R}^n$ is $\mathscr{C}^1$ in a neighborhood $W$ of the point $a$ , with the matrix $f'(a)\neq 0$ then $f$ is locally invertible - there exist neighborhoods $U\subset W$ of $a$ and $V$ of $b=f(a)$ and a one-to-one $\mathscr{C}^1$ mapping $g:V\rightarrow W$ such that $$g(f(x))=x\quad\text{for } x \in U,$$ $$f(g(y))=y\quad\text{for } y \in W$$ In particular, the local inverse $g$ is the limit of the sequence $\{g_k\}_{k=0}^\infty$ of successive approximations defined inductively by $$g_0(y)=a,\quad g_{k+1}(y)=g_k(y)-f'(a)^{-1}[f(g_k(y))-y]$$ Question $1$ : What I understand, inverse function theorem use implicit function theorem to guarantee there exist a relationship (function) of $y$ in term of $x$ (not explicitly). But the iterative form doesn't make sense to me. Like ""Why applying inverse Jocobian $(f'(a)^{-1})$ on $[f(g_k(y))-y]$ we get better and better approximation of $g(y)?$ "". Because What I know is, ""Jacobian approximate $f$ locally by a linear transformation"". Then what information encoded in the $f'(a)^{-1}$ for theorem $3.3$ and $D_{2} G(\mathbf{a}, \mathbf{b})^{-1}$ for theorem $3.4$ ? Question $2$ : What's the main difference/motivation/intuition between these two theorems? Maybe I am asking too many question for a single thread but as they are related to each other and pointed to understand only a single theorem, that's why I am put them all together. It will be great help if anyone explain those question.","['multivariable-calculus', 'inverse-function-theorem', 'implicit-function-theorem']"
