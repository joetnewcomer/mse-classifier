question_id,title,body,tags
4203739,Binomial/combinatorics summation proof question,"Given $\sum_{k=0}^{2r} (-1)^k \binom{n}{k}\binom{n}{2r-k} = (-1)^r\binom{n}{r}$ for $0≤r≤\frac12n$ How do I show that $\sum_{k=0}^{r} (-1)^k \binom{n}{k}\binom{n}{2r-k} = \frac12(-1)^r\binom{n}{r}[1+\binom{n}{r}]$ for $0≤r≤\frac12n$ For context, the first statement is derived from considering the coefficient of $x^{2r}$ in the statement $(1-x)^n(1+x)^n=(1-x^2)^n$ where $2r≤n$","['summation', 'binomial-coefficients', 'combinatorics', 'binomial-theorem']"
4203759,Applying am-gm on an infinite series expansion of $e^x$,"We know, $$ e^x-1 = \sum_{i=1}^{\infty} \frac{x^i}{i!}$$ Assume $x>0$ and use am-gm on the equation: $$ e^x -1\geq \lim_{n \to \infty}n\left( \frac{x^{ \frac{n(n+1)}{2} } }{\prod_{i=1}^n (i!)}\right)^{1/n}$$ What would be the lower bound on RHS for a given $x$ ? Would it be possible to evaluate the limit in terms of $n$ ? p.s: I came up with this myself, I am not sure how to start since we have product of factorial on the right side but for those who want to try my question, Stirling's approximation may be helpful","['limits', 'taylor-expansion', 'number-theory']"
4203806,What's more likely: $7$-digit number with no $1$'s or at least one $1$ among its digits?,"A $7$ -digit number is chosen at random. Which is more likely: the number has no $1$ 's among its digits or the number has at least one $1$ among its digits? Here's how I did it: The question is asking whether $8(9)^6$ (the number of those with no $1$ 's among its digits) or $9(10)^6 - 8(9)^6$ (the number of those with at least one $1$ among its digits). Some tedious multiplying shows that $8(9)^6 = 4241528 < 4500000$ , which demonstrates that $9(10^6) - 8(9)^6$ i.e. the number having at least one $1$ 's among its digits is more likely. However, I am wondering if there is a slicker way to get the answer without having to do any tedious multipication.","['combinatorics', 'probability']"
4203826,Quasicoherent sheaves over manfiolds,"For any locally ringed space $\left(X,O_X\right)$ , a quasicoherent sheaf is a sheaf of $O_X$ -modules which are locally the quotient of free modules. Considering a manifold (Haussdorf, second-countable) as a locally ringed space, what are the quasi-coherent sheaves? Is there a good characterisation (some Serre-Swan type generalisation)? What are some examples which are not vector bundles? The answers to this question only talks about coherent sheaves and why they are not interesting. I am interested in the answer for quasicoherent sheaves.","['quasicoherent-sheaves', 'smooth-manifolds', 'algebraic-geometry', 'differential-topology', 'differential-geometry']"
4203830,"Annette, Babette, Colette playing in a $16$ person single elimination tournament","Annette, Babette, Colette, and $13$ other girls are playing in a $16$ -player, single-elimination tennis tournament. The $16$ players are placed at random in the first column of the bracket shown in the figure to play $8$ games in Round $1$ . The winners of very match are then written into the bracket and play in Round $2$ , and so on. The tournament proceeds until only one player remains. Given that Annette is the best player of the $16$ in the tournament, Babette is $2$ nd best, and Colette is $3$ rd best, and that the best player always wins each match, find the probability that: (a) Annette wins the tournament. (b) Babette is the runner-up of the tournament (she gets to the finals, but loses). (c) Colette is the runner-up of the tournament. For (a), I ended up getting $1$ since Annette defeats everyone in her way. For (b), I ended up getting ${8\over{15}}$ since if Annette's position is fixed on one half of the brackets in Round $1$ , then there's 15 positions remaining and Babette has to occupy one of the $8$ positions in the other half of the brackets in Round $1$ , hence ${8\over{15}}$ . For (c), we need Annette and Babette to be on one half of the brackets in Round $1$ , and Colette to be on the other half. Fix Annette's position, then there's a ${7\over{15}}$ chance of Babette being on Annette's half, and then a ${8\over{14}}$ chance of Colette being on the other half, so the probability that Colette is the runner-up of the tournament is ${7\over{15}}{8\over{14}} = {4\over{15}}$ . Is this correct?",['probability']
4203845,Any closed subset $S$ of a compact space $T$ is compact.,"I have a lemma which i'm trying to prove: Any closed subset $S$ of a compact space $T$ is compact. My proof: Let $\mathcal{U}$ be a an open cover of $T$ Since $S\subset T \subset \mathcal{U}$ , it follows that $S\subset \mathcal{U}$ , which is a collection of open sets which union contains $S$ and that is precisely the definition of a cover. Since this cover has a finite subcover, it follows that $S$ is compact. Would this be correct?","['general-topology', 'compactness']"
4203876,Understanding Matousek's proof of Equiangular lines,"In Miniature 9 in 33 Miniatures by Matousek, he proofs that: The largest number of equiangular lines in $\mathbb R^3$ is 6, and in general, there cannot be more than $\binom{d+1}{2}$ equiangular lines in $\mathbb R^d$ . The proof starts with: Let us consider a configuration of $n$ lines, where each pair has the same angle $\vartheta \in (0, \pi/2]$ . Let $v_i$ be a unit vector in the direction of the $i$ th line (we choose one of the two possible orientations of $v_i$ arbitrarily). The condition of equal angles is equivalent to $$ |\langle v_i, v_j\rangle| = \cos \vartheta, \quad \text{for all } i\neq j.$$ Let us regard $v_i$ as a column vector, or a $d\times 1$ matrix. Then $v_i^Tv_j$ is the scalar product $\langle v_i, v_j \rangle$ . On the other hand, $v_iv_j^T$ is a $d\times d$ matrix. We show that the matrices $v_iv_i^T, i = 1,2,\dots, n$ are linearly independent. And I do not see how showing the independence of these matrices shows that we can not have more than the claimed number of equiangular lines? Thanks!","['geometry', 'matrices', 'linear-algebra', 'combinatorics', 'dimension-theory-algebra']"
4203878,is $x=y^2$ a function?,"A function is a relation that associates each element of one domain to one and exactly one element in it's co-domain. This is the definition of a function I know. But if I take the function $x=y^2$ this condition is not satisfied.
For $x=1$ I get $y=1$ and $y=-1$ . Does this $x=y^2$ is not a function? and is this same as $y=\sqrt x$ and is $y=\sqrt x$ not a function too?","['calculus', 'functions']"
4203881,Algebraic torus over $\mathbb C$ and line bundle on its classifying space,"Let $G=(\mathbb C^\times)^n$ the algebraic torus, I am trying to understand the isomorphism between its character group and the second cohomology of its classifying space $M(G)\cong H^2(BG,\mathbb Z)$ . The construction is as follows, given a character $\rho\in M(G)$ , we have an associated 1 dimensional representation $\mathbb C_\rho$ , when taken as $G$ -space, we can form the space $$(\mathbb C_\rho)_G=\mathbb C_\rho\times_G E_G$$ Which is a line bundle over $BG$ , the the negative of its first Chern class defines a map $M(G)\to H^2(BG,\mathbb Z)$ . I can show that this is indeed an isomorphism by explicitly writing down a set of generators. I realized there is another construction. An element $\rho\in M(G)=\operatorname{Hom}((\mathbb C^\times)^n,\mathbb C^\times)$ induces a map $BG\to B\mathbb C^\times$ , whose homotopy class can be regarded as element of $[BG,B\mathbb C^\times]=H^2(BG,\mathbb Z)$ . It seems natural that the two maps $M(G)\to H^2(BG,\mathbb Z)$ should be the same. Is it true? If it is then how to prove it? I cannot even show that the second construction is an isomorphism, in particular I don't know how to explicitly describe the induced map between the classifying spaces. Thanks for your help.","['classifying-spaces', 'complex-geometry', 'algebraic-geometry', 'algebraic-topology']"
4203889,Infinite series of matrix product,"If $t$ is real and positive and matrix $A$ and $B$ are such that $A = \begin{pmatrix} \dfrac{t^2+1}{t} & 0 & 0 \\ 0 & t & 0 \\ 0 & 0 & 25 \end{pmatrix}$ and $B=\begin{pmatrix} \dfrac{2t}{t^2+1} & 0 & 0 \\ 0 & \dfrac{3}{t} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}$ and $X=(AB)^{-1} + (AB)^{-2} + (AB)^{-3} + \cdots\infty, Y=X^{-1}$ then which option is correct? $1.$ $\text{det}(Y)=10$ $2.$ $X\cdot \text{adj(adj}(Y)=8I$ $3.$ $\text{det}(Y)=20$ $4.$ $X\cdot \text{adj(adj}(Y)=5I$ My attempt: I managed to calculate some of the important matrices, but am stuck in a part where I have to find the determinant of a matrix that has some elements tending to infinity. As $t>0$ $$AB= \begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix}$$ The inverse of a diagonal matrix is another diagonal matrix that's diagonal elements are reciprocals of the diagonal elements of the original matrix, so $$(AB)^{-1}= \begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, (AB)^{-2}=\begin{pmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 5 \end{pmatrix}, (AB)^{-3}=\begin{pmatrix} \dfrac{1}{2} & 0 & 0 \\ 0 & \dfrac{1}{3} & 0 \\ 0 & 0 & \dfrac{1}{5} \end{pmatrix}, \cdots$$ My problem is, adding these matrices results in a matrix that's diagonal elements tend to infinity, as $$X=\begin{pmatrix} 2+1/2+2+1/2+\cdots \infty & 0 & 0 \\ 0 & 3+1/3+3+1/3+\cdots \infty \\ 0 & 0 & 5+1/5+5+1/5+\cdots \infty\end{pmatrix} = \begin{pmatrix} S_2 & 0 & 0 \\ 0 & S_3 & 0\\ 0 & 0 & S_5 \end{pmatrix}$$ And $$\text{adj}(X) = \begin{pmatrix} S_3S_5 & 0 & 0 \\ 0 & S_2S_5 & 0\\ 0 & 0 & S_2S_3 \end{pmatrix}$$ I know that $Y=\dfrac{\text{adj}(X)}{|X|}$ but I can't figure out how to calculate $|X|$ . The determinant of a diagonal matrix is simply the product of the diagonal elements, but how to calculate this value when the diagonal elements tend to infinity?","['matrices', 'linear-algebra', 'sequences-and-series']"
4203911,Proving on the equation $\frac1{x+1}+\frac1{2(x+2)}+\frac1{3(x+3)}+\cdots+\frac1{n(x+n)}=1$,"Consider the equation: \begin{gather*}
\frac{1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + \cdots + \frac{1} {n(x+n)} = 1.\tag1
\end{gather*} Prove that: For every $n \geq 2$ , equation $(1)$ always has a unique positive solution $x_n$ . The sequence $(x_n)$ has a finite limit with $n \to \infty+$ . Find that limit. Here's all I did: $\frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)} = 1.$ $\Leftrightarrow 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) = n! (x+1)(x+2)...(x+n) $ Consider the polynomial $P(x) = 2.3...n(x+1)(x+2)...(x+n) + 1.3...n(x+1)(x+3)...n + ... + 1.2...(n-1)(x+1)(x+2)...(x+n-1) - n! (x+1)(x+2)...(x+n) $ Consider the highest coefficient of the polynomial: $ -n! < 0 $ Consider the lowest coefficient of the polynomial: $ (2.3...n)^2+(1.3...n)^2+...(1.2...(n-1))^2  - n! = (1.3...n)^2+...(1.2...(n-1))^2  > 0$ Let $x_1,x_2,x_3....$ be the solutions of $ P(x) $ Assume that equation (1) has no positive solution. According to Viette's theorem: $x_1 x_2 x_3....x_n = (-1)^n \frac{a_0}{a_n} $ $\frac {a_0}{a_n} < 0 \Rightarrow $ if $n$ is an odd number , then $x_1 x_2 x_3....x_n < 0$ , but $(-1)^n \frac{a_0}{a_n} >0 \Rightarrow $ (nonsense) . Same with $n $ being an even number. So the polynomial must have at least one integer root. Suppose the polynomial has at least 2 positive roots $x$ and $y$ . $\frac {1}{y+1} + \frac{1}{2(y+2)} + \frac{1}{3(y+3)} + .... + \frac {1} {n(y+n)} =\frac {1}{x+1} + \frac{1}{2(x+2)} + \frac{1}{3(x+3)} + .... + \frac {1} {n(x+n)}  .$ $\Rightarrow (y-x)( \frac {1}{(x+1)(y+1)}+ \frac {1}{2(x+2)(y+2)} +...+\frac {1}{n(x+n)(y+n)} ) = 0 $ $\Rightarrow x=y \Rightarrow$ (no sense) So for every $n \geq 2$ , equation $( 1 )$ always has a unique positive solution $x_n$ That's all I did , and I have no idea what to do next , I want to establish a relationship between $x_n$ but for $n \geq 3$ the polynomial's solution is a "" bad "" solution . "" so it's very difficult to build. Hope to get help from everyone. Thanks very much .","['limits', 'sequences-and-series']"
4203954,"Prove that if $a,b,c$ are sides of a triangle and $s$ is the semi-perimeter, then $a^2+b^2+c^2 \geq \dfrac{36}{35}\bigg(s^2 + \dfrac{abc}{s}\bigg)$","Prove that, if $a,b,c$ are the sides of a triangle and $s$ is the semi-perimeter, then $a^2+b^2+c^2 \geq \dfrac{36}{35}\bigg(s^2 + \dfrac{abc}{s}\bigg)$ . What I Tried :- Nothing special really came in my mind. I did not find a way to use Triangle Inequality. What I did was, by AM-GM :- $$a^2 + b^2 + c^2 \geq \frac{36}{35}\bigg(s^2 + \dfrac{abc}{s}\bigg) > \frac{72}{35}(\sqrt{sabc}).$$ But I couldn't proceed from this. Another Idea I had was :- $a^2 + b^2 + c^2 = (a + b + c)^2 - 2(ab + bc + ca)$ . $\rightarrow a^2 + b^2 + c^2 = 4s^2 - 2(ab + bc + ca).$ But I did not know how to use this here, and would make the calculations a bit messy, especially of the $\dfrac{36}{35}$ part present there. Can Anyone Help me? Thank You.","['triangle-inequality', 'geometry', 'geometric-inequalities', 'inequality', 'problem-solving']"
4204011,Proving $\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2}$,"Can someone help with this sum? Prove $$S=\lim_{n\rightarrow\infty}\sum_{k=1}^n (-1)^{k-1}{n\choose k}\frac{k}{2^k-1}=\frac{1}{\ln2}$$ I have tried to break down the $\frac{k}{2^k-1}=\sum_{j=1}^\infty \frac{k}{2^{kj}}$ and tried writing it as $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\sum_{k=1}^n (-1)^{k-1}{n\choose k}k\left(\frac{1}{2^{j}}\right)^k,$$ then using the binomial summation as such, note $\left(x=\frac{1}{2^j}\right)$ $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{1}{2^j}\frac{\mathrm{d}}{\mathrm{d}x}\left(\sum_{k=1}^n (-1)^{k-1}{n\choose k}x^{k}\right)=\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n(1-x)^{n-1}}{2^j}.$$ Giving us the final sum to be $$\lim_{n\rightarrow\infty}\sum_{j=1}^\infty\frac{n\left(1-\frac{1}{2^j}\right)^{n-1}}{2^j},$$ the limit of which seems to be $0$ . Which is not what we want, can someone tell me where my mistakes lie, and also point towards the solution or present it themselves. Thanks in Advance.","['sequences-and-series', 'real-analysis']"
4204015,How to form a set from 4 sets with the given property?,"Hi, this question is related to Combinatorics. If there is anything I can improve with my question, please let me know. Problem: Let us suppose  we have $4$ sets $S_1,S_2,S_3,S_4$ having $a,b,c,d$ elements. All elements in each $S_i$ are distinct for $1\leq i\leq 4$ .
We need to form a set $S$ from  the elements of $S_i;1\leq i\leq 4$ with the following property: $S$ must contain $a-1$ elements of $S_1$ , $b-1$ elements of $S_2$ , $c-1$ elements of $S_3$ and $d-1$ elements of $S_4$ . Find the number of ways to form $S$ if $S$ has $a+b+c+d-3$ elements. $a+b+c+d-2$ elements. My try: Method 1: Note that $a+b+c+d-3=(a-1)+(b-1)+(c-1)+(d-1)+1$ .
We first choose $a-1$ elements from $S_1$ in $\binom{a}{a-1}=a$ ways,then choose $b-1$ elements from $S_2$ in $\binom{b}{b-1}=b$ ways,
then choose $c-1$ elements from $S_3$ in $\binom{c}{c-1}=c$ ways
and finally choose $d-1$ elements from $S_4$ in $\binom{d}{d-1}=d$ ways.
We now need to choose $1$ remaining  element of $S$ from $\bigcup_{i=1}^4 S_i$ .
Now $1$ remaining  element of $S$ from $\bigcup_{i=1}^4 S_i$ can be chosen in $\binom{4}{1}=4$ ways.
So total number of ways= $4\times a\times b\times c\times d$ . Method 2: We choose $a$ elements from $S_1$ , $b-1$ from $S_2$ , $c-1$ from $S_3$ , $d-1$ from $S_4$ .
Note that $a$ elements can be chosen from $S_1$ in $\binom{a}{a}=1$ way,then choose $b-1$ elements from $S_2$ in $\binom{b}{b-1}=b$ ways,
then choose $c-1$ elements from $S_3$ in $\binom{c}{c-1}=c$ ways
and finally choose $d-1$ elements from $S_4$ in $\binom{d}{d-1}=d$ way.
Thus it becomes $b\times c\times d$ ways. Similarly  we can choose $a-1$ elements from $S_1$ , $b$ from $S_2$ , $c-1$ from $S_3$ , $d-1$ from $S_4$ .
Thus it becomes $a\times c\times d$ ways. Similarly  we can choose $a-1$ elements from $S_1$ , $b-1$ from $S_2$ , $c$ from $S_3$ , $d-1$ from $S_4$ . Thus it becomes $a\times b\times d$ ways. So total ways= $bcd+acd+abd+abc.$ I have 3 questions: Why are the answers in two methods coming different? I feel Method 2 gives the correct answer but the method is very lengthy and cumbersome and can't be extended. Is it possible to write it in a short and elegant way? How to do the second part of the question where $S$ must have $(a-1)+(b-1)+(c-1)+(d-1)+2$ elements? How to extend Method 1 or Method 2 here? I am looking for some help from all the experts out here. Waiting for your responses.","['elementary-set-theory', 'combinations', 'combinatorics']"
4204029,Distinguishing two frieze patterns with symmetry group cyclic,"Among the seven frieze patterns, there are two, whose symmetry group is infinite cyclic. For example, one may see first two in picture below (Ref: Gallian's book on algebra, Chapter on Frieze Groups and Crystallographic Groups) or in the link here . The two frieze patterns with cyclic symmetry group are obtained as follows: (1) Walk only by left foot in a straight line; then the pattern of foot-print is a frieze pattern. (2) Walk as usual in a straight line; then the pattern of left-right foot prints is a frieze patterns. Question: What is correct geometric way to distinguish these patterns? These patters are generated by repeated application of generator of symmetry group to a single foot-print (in positive and negative directions).
For first one, it is just translation. For the second pattern, we repeatedly apply glide reflection. Since, these patterns are one dimensional, so I was unable to compare translation and glide-reflection because they are different as motions in ""2-dimensional plane"" (orientation preserving-reversing).","['group-theory', 'abstract-algebra', 'symmetry']"
4204047,Can orthogonal matrix with positive diagonal have -1 in its spectrum?,"Let $O\in \mathbb{R}^{n\times n}$ be an orthogonal matrix, i.e. $O^tO=I=OO^t$ . Suppose its diagonal entries $\{O_{jj}\}_{j\in \{1,...,n\}}$ are (strictly) positive. Can $-1$ then be included in the spectrum of $O$ ? Note that if the diagonal is required to be non-negative in stead of positive, then $$\begin{pmatrix}
0&-1 & 0\\
0 & 0 & -1\\
-1 & 0& 0
\end{pmatrix}$$ provides a counterexample, since it has a non-negative diagonal yet includes -1 in its spectrum. Apart from that, a straightforward calculation like (suppose, seeking a contradiction, that $v\in \mathbb{R}^n$ is a normalized eigenvector with eigenvalue -1) $$-1=\langle v,-v\rangle=\langle v,Ov\rangle \geq \sum_{j=1}^n\left(|O_{jj}|v_j^2 - \left|\sum_{k\neq j}O_{jk}v_jv_k\right|\right)> -\left(\sum_{j=1}^n\sum_{k\neq j}O_{jk}^2\right)^{1/2}\geq-n^{1/2}$$ doesn't suffice to resolve the problem (because it didn't result in the desired contradiction). Likewise, writing out $\langle e_j,Ov\rangle = -\langle e_j,v\rangle$ doesn't seem to yield anything. I am aware that my statement, if true, would imply for odd $n$ that a positive diagonal would imply $\det O=1$ . Also, note that there is a related 'hybrid' open question: what if the diagonal of $O$ is non-negative and non-zero? EDIT: there are also counterexamples to the hybrid problem just mentioned: take $$\begin{pmatrix}
\sin(\theta) & \cos(\theta) & 0\\
0 & 0 & -1\\
\cos(\theta) & -\sin(\theta)& 0
\end{pmatrix}$$ for an angle $\theta \in (0,\pi)$ . (to quicker analyse examples in odd $n$ , like $n=3$ here, it helps to prove the auxiliary lemma $\det O = -1 \Rightarrow -1 \in \sigma(O)$ )","['orthogonal-matrices', 'inequality', 'linear-algebra']"
4204063,"Need help with identifying dependent variable in change of variables, non-independent variables problem(s)","I understand the idea behind non-independent variables and have been able to solve problems of this form: we have a function $f$ , and a constraint equation that relates the variables of $f$ to each other. e.g. $w=x^2+y^2+z^2$ , where $z=x^2+y^2$ . To solve for $(∂w/∂z)_y$ we do chain rule: $(∂w/∂z)_y$ = $2x(∂x/∂z)_y + 2z$ . We then solve for $(∂x/∂z)_y$ and get the answer we want: $1+2z$ . But, I am attempting to do this problem and have been quite stuck. $$w=u^3-uv^2, u = xy, v = u + x$$ Find $(∂w/∂u)_x$ I'm stuck here because there are a lot more variables than I'm used to. Also, which is the dependent variable? I think this is quite similar to an example brought up in class that I didn't understand. $$f(x, y) = x + y, ∂f/∂x = 1$$ Change of variables: $$x = u, y = u + v$$ Then, $$f = 2u + v, ∂f/∂u = 2$$ $$x = u \text{, but } ∂f/∂x = ∂f/∂u$$ I think the problem I'm having with both examples is that I don't know what the dependent variable is. Is it x,y,u, or v? If someone can explain both examples, I'd appreciate it. Thank you.",['multivariable-calculus']
4204071,On $\mathrm{\sum_{x\in\Bbb Z}sech(x)=3.142242…}$,"Inspired by This question , I started to wonder about simpler series. I have seen similar questions to the following, but none had this special case explicitly. It is related to the q-digamma function . The answer is not the final two terms though as seen here . Source for partial sum formula. Integral representation of sech source . Second source for integral representation partial sum. $$\mathrm{S_{sh}\mathop=^{def}\sum_{x\in\Bbb Z}sech(x)= \sum_{x\in\Bbb Z} \frac{2cosh(x)}{cosh(2x)+1}=1+2\sum_{x=1}^{\infty}\frac{2}{e^x+e^{-x}}=1+2i\lim_{n\to\infty} ψ_e\left(n-\frac{i\pi}2+1\right)-ψ_e\left(n-\frac{i\pi}2+1\right)-ψ_e\left(\frac{i\pi}2\right)+ ψ_e\left(-\frac{i\pi}2\right)}$$ Integral representation: $$\mathrm{S_{sh}=\sum_{x\in\Bbb Z}\frac2\pi\int_0^\infty \frac{t^{\frac{2x i}{\pi}}}{t^2+1}dt=1+\frac1\pi\lim_{b\to\infty} \int_0^\infty\frac{1}{t^2+1}\sum_{x=0}^bt^\frac{2xi}\pi dt= 1+\frac1\pi\lim_{b\to\infty} \int_0^\infty\frac{t^\frac{2i(b+1)}{\pi}-1}{\left(t^\frac{2i}{\pi}-1\right)(t^2+1)}dt=3.14224265993564633914314598537…>\pi=3.1415..}$$ Jacobi theta function about page. Alternate forms . After a change of functions, @Gary found that the constant has the closed form of: $$\mathrm{S_{sh}=\vartheta_3^2\left(\frac1e\right)}$$ Also from @Gary. As a bonus, this hyperbolic cosecant version has other forms, but this is the simplest closed form. The q-digamma term is about $\pi$ : $$\mathrm{\sum_{x\ge1}csch(x)=-ln\left(e^2-1\right)-ψ_{e^2}\left(\frac12\right)= 1.284423027303676524572857579841…}$$ Another interesting thing is the integral expression. How do I evaluate S in closed form as it most likely can be expressed in terms of the q-digamma function above? Also, how would one even derive the sum using the q digamma function? Please correct me and give me feedback!","['summation', 'hyperbolic-functions', 'closed-form', 'q-series', 'trigonometry']"
4204087,"The stability of a gradient flow (discrete JKO scheme, proximal point)","Define a free energy functional on the space of probability densities (on $\mathbb{R}^d$ , denoted $\mathcal{P}(\mathbb{R}^n)$ ) $$E(\rho):=\int_{\mathbb{R}^d} f(x) \rho(x) dx+\int_{\mathbb{R}^d} \rho(x) \log \rho(x) dx $$ for some uniformly convex, Lipschitz, non-negative $f: \mathbb{R}^n\to \mathbb{R}$ . Consider the following discrete scheme of a Wasserstein gradient flow (coined JKO scheme) : Fix a time step $h$ , given a density with finite second moment $\rho^0$ such that $E(\rho^0)<\infty$ iteratively define $$\rho^{n+1}:=\text{argmin}_{\rho \in \mathcal{P}(\mathbb{R}^n)} \frac{1}{2h}W_2^2(\rho^n,\rho)+E(\rho).$$ Now for fixed $h$ consider the sequence $\{\rho^n\}_{n\ge 0}$ . My question: does anyone know of any results relating to the convergence of $\rho^n$ as $n \to \infty$ or, more likely, the existence of an accumulation point of $\{\rho^n\}_{n\ge 0}$ ? Note - we are fixing the time step hence this is different question than that originally answered in this famous paper . I have been looking for results on the stability of the scheme and I can't find any (but im sure there must be).","['gradient-flows', 'stationary-processes', 'reference-request', 'gradient-descent', 'probability-theory']"
4204088,"""Parametric"" versus ""Non-parametric"" hypersurface","What exactly are the meanings of the terms ""parametric hypersurface"" and ""non-parametric hypersurface""? My initial guess was ""parametric"" referred to a hypersurface that is a global graph (e.g. the graph of a parabola in $\mathbb{R}^{2}$ or the graph of the tangent function in $(-\pi/2,\pi/2) \times \mathbb{R}$ ) --- graphs are explicitly ""parametrized"" ---, whereas ""non-parametric"" referred to hypersurfaces that might not be graphs everywhere, like the circle --- which do not have a ""canonical parametrization.""  However, I fear it's the exact opposite --- and I don't understand the reasoning behind the terminology. The terminology seems to be taken for granted in differential geometry papers (e.g. this and this ).","['differential-topology', 'soft-question', 'differential-geometry']"
4204106,"Value of $\,x\,,\,$ for $\,\cos x>0\,.$","In my book the value of $\,x\,$ for $\,\cos x>0\,$ is given as : $$\bigcup_{n=-\infty}^\infty \left(\left(2n-\frac12\right)\pi, \left(2n+1\right)\frac\pi2\right)$$ How did they get the domain for this ?","['trigonometry', 'functions']"
4204108,How to show $|\log | x||^{2 \varepsilon} \cdot|x|^{-2}$ is in $L_{\mathrm{loc}}^{r}$ for every $r<n / 2$,"Let $\Omega=\mathbb R^{n}$ . I wanted to show that $|\log | x||^{2 \varepsilon} \cdot|x|^{-2}$ is in $L_{\mathrm{loc}}^{r}$ for every $r<n / 2$ . Only problem is with compact set including point $0$ .
I thought to use polar coordinates. Consider $B=\overline{B(0,R)}$ and $n\alpha(n)=$ surface area of unit sphere. $$\begin{align}\int_B|\log | x||^{(n-\delta) \varepsilon} \cdot|x|^{-(n-\delta)}dx&=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{-(n-\delta)}\varrho^{n-1}d\varrho\\
&=n\alpha(n)\int_0^R|\log \varrho|^{(n-\delta) \varepsilon} \cdot \varrho^{\delta-1}d\varrho
\end{align}$$ How to show that last integral is finite for any $\delta$ such that $0\le\delta\le n$ ? Any help will be appreciated.","['multivariable-calculus', 'calculus', 'lp-spaces', 'functional-analysis', 'partial-differential-equations']"
4204115,Pointwise equality of codimension-zero immersions,"Suppose $f,g: U \subset \mathbb{R}^n \to \mathbb{R}^n$ are smooth immersions from a closed and bounded domain $U$ such that $f|_{\partial U} = g|_{\partial U}$ .  If $f$ and $g$ induce the same Riemannian metric on $U$ (i.e. $\langle df, df \rangle = \langle dg, dg \rangle$ ), does it follow  that $f = g$ pointwise? I suspect the answer is yes, and I have a sketch of a proof for $n=2$ .  In particular, the hypotheses imply that (at least locally), we have $(g \circ f^{-1})^*\delta = \delta$ where $\delta$ is the Euclidean metric on $\mathbb{R}^2$ .  Then, $g\circ f^{-1}$ is identity on the boundary, and to show that it is also identity on the interior we can consider any Euclidean triangle in $f(U)$ with two vertices fixed on (some convex part of) the boundary and the third in the interior.  This triangle has two vertices and all edge lengths preserved under $g\circ f^{-1}$ , therefore its third vertex is preserved also.  If it is not clear enough from this, I think starting close to the boundary and ''bootstrapping'' the same procedure toward the interior of the domain should allow it to be used anywhere, but I have not thought carefully about it yet. I know that codimension-zero immersions are quite rigid as a general rule, but so far I cannot come up with a proof nor a counterexample for this statement.  Do you know of any arguments or references which might answer this question?","['isometry', 'riemannian-geometry', 'differential-geometry']"
4204119,Join of two preorders and of two equivalence relations,"I'm sorry for the silly doubt. What is the join of two preorders? And of two equivalence relations? The meet is given by intersections. But in general the union of two preorders (resp. equivalence relations) fails to be a preorder (resp. equivalence relations). In the case of equivalence relation, is it the transitive closure of their union?","['elementary-set-theory', 'order-theory', 'lattice-orders']"
4204133,"Combinatorial design related to scheduling group activities (everyone tries every activity, no pair is together twice)","Trying to solve this problem led me to consider the following generalization. Let $g$ and $p$ be positive integers. Imagine that you own $g$ distinct board games, where each game requires exactly $p$ people to play. You plan to host a game night, and invite exactly $p\times g$ friends over to play your games. You plan to do this over the course of $g$ rounds, where in each round, the group divides itself into $g$ groups of $p$ people, and each group plays a game. The goal is to do this in such that way that Everyone plays every game exactly once, and No pair of people play together more than once. This is equivalent to finding a $g\times (gp)$ matrix such that each column is a permutation of $\{1,\dots,g\}$ , each number in $\{1,\dots,g\}$ appears $p$ times in each row, and where any two columns agree in at most one place (each column is a player, each row is a round, the entry says which game that player plays in that round). When $p=1$ , this is just a Latin square of order $g$ . My question is, for what values of $g$ and $p$ is this possible ? Also, I am curious if anyone has seen this problem in the literature. This certainly bears a resemblance to the social golfer problem. Both this and the SGP involve dividing $pg$ people into $g$ groups of $p$ over the course of several rounds, such that no pair of people play together twice. The difference is that here, people are playing distinct games instead of splitting into unlabeled groups, so constraint $(1)$ does not make sense in the context of SGP and makes this problem distinct. This is possible in the trivial cases where $g=1$ or $p=1$ . Besides these, I know that $$g\ge p+1$$ is a necessary condition for a schedule to exist. Consider the $p$ people who play Monopoly (say) in the first round. In the next round, they must all play different games, so there must be at least $p$ other games besides Monopoly. I suspect that this is also a sufficient condition, since I have only found positive results in this regime: You can succeed with $p=2$ , as long as $g$ is odd.  For each $x,y\in \{0,1,\dots,g-1\}$ , persons numbered $2x$ and $2y+1$ will play game number $x+y\pmod g$ during round number $x-y\pmod g$ . It is also possible when $(g,p)=(4,3)$ and $(5,4)$ . I found the schedule for the former case by hand, and for the latter case by computer. However, I cannot see how the schedule I found would generalize. Here is the solution for $(g,p)=(4,3)$ : $$
\begin{array}{|cccccccccccc|}\hline
1&1&1&2&2&2&3&3&3&4&4&4\\
2&3&4&1&3&4&1&2&4&1&2&3\\
3&4&2&4&1&3&2&4&1&3&1&2\\
4&2&3&3&4&1&4&1&2&2&3&1\\\hline
\end{array} 
$$ Therefore, I further ask is it indeed true that a schedule exists if and only if $g\ge p+1$ ?","['combinatorial-designs', 'combinatorics', 'latin-square']"
4204136,What is the relationship between covering designs (La Jolla Covering Repository) and linear block codes?,"In many papers I have been reading, bioengineers are utilizing covering sets from Dan Gordon's La Jolla Covering Repository to design custom Modified Hamming Codes (MHD).
These MHDs have custom codeword lengths and desired Hamming weights, that I see some have been using to create codewords outside the (7,4) Hamming code taught in textbooks. I'm trying to better understand the link between covering designs and linear block codes. Namely, I'm not understanding how (v, k, t) covering design inform the property of a linear block code (ie. what's the Hamming distance? rate? error-detection capacity? error-correcting capacity)? Why can covering designs be used to make a linear block at all? For clarity, a (v, k, t) covering design is defined below: A (v,k,t)-covering design is a collection of k-element subsets, called blocks, of {1,2,…,v}, such that any t-element subset is contained in at least one block.","['combinatorial-designs', 'coding-theory', 'combinatorics', 'information-theory']"
4204189,Geodesic convexity and the 2nd fundamental form,"Let $(M,g)$ be a Riemannian manifold, $\Omega\subset M$ be a closed set with smooth boundary $\partial\Omega$ and $\nu$ be the unit normal of $\partial\Omega$ pointing into $\Omega$. $\Omega$ is said to be geodesically convex iff
$\forall x_0, x_1\in\Omega$ $\exists c:[0,1]\stackrel{\text{geodesic}}\to(M,g)$ s.t. $c(0)=x_0, c(1)=x_1$, $c([0,1])\subset\Omega$, $\mathrm{Length}[c]=d_g(x_0,x_1)$. Suppose $\Omega$ is geodesically convex. Then... [Q.1] Does it hold that the 2nd fundamental form of $\partial\Omega$ toward $\nu$ is nonnegative definite at each point on $\partial\Omega$? [Q.2] Let $\psi_r(x):=\mathrm{exp}^g_x [r\nu(x)]\in N$ $(x\in\partial\Omega)$. Then for small $|r|$, $\psi_r$ is an embedding. Here, does it hold that the inner 2nd fundamental form of $\psi_r$ is nonnegative definite at each point on $\partial\Omega$ when $r>0$ and is sufficinetly small? Thank you.","['manifolds', 'geometry', 'riemannian-geometry', 'differential-geometry']"
4204191,"Let $U=X+Y$, $V=X-Y$, while $X,Y\sim U[0,1]$ and independent. Prove or disprove..","Let $U=X+Y$ , $V=X-Y$ , while $X,Y\sim U[0,1]$ and independent. Prove or disprove: $(U,V)$ has a uniform distribution on some area in the plane. $U$ and $V+1$ are distributed the same (sorry if the translation is bad, would be happy to know how it's usually written). $U,V$ are independent. $U,V$ are (uncoordinated - not sure of the translation), but what it means is $Cov(U,V)=0$ My work: For first statement: Intuitively this is true, but I wanted to find the CDF: $F_{U,V}(u,v)=P(X+Y \le u, X-Y \le v)=P(Y \le u-X)P(X \le v+Y)=(u-X)(v+Y)$ whenever $u,v\le 1$ . I'm confused if what I did is correct and would love to hear feedback. For second statement: $P(V+1 \le v)=P(V \le v-1)=0$ $P(U \le u) = P(X+Y \le u)=P(X \le u-Y)=$ .. I'm a little stuck here, what does it mean that $X$ is less than $u-Y$ since $Y$ could be anything, this is giving me some problems. For third statement: I need to  either prove that $F_UF_V=F_{U,V}$ or disprove it. My intuition says that they're dependent, since they both depend on $X,Y$ . $F_U(u)F_V(v)=P(X+Y \le u , X-Y \le v) $ , again I'm struggling with calculating these, How do I reach $X,Y$ or stuff that I know how to deal with, without complicating myself? The last one was not hard, all I've done is $Cov(X+Y,X-Y)=Var(X)-Var(Y)=0$ . Any help and feedback is really appreciated, thanks in advance.","['independence', 'uniform-distribution', 'probability']"
4204311,"If $x^3 + \frac{1}{x^3} = 2\sqrt{5}$, then find $x^2 + \frac{1}{x^2}$.","Question :  If $x$ is a real number satisfying $x^3 + \frac{1}{x^3} = 2\sqrt{5}$ , determine the exact value of $x^2 + \frac{1}{x^2}$ . My partial solution : $(x + \frac{1}{x})^3 = x^3 + 3x + \frac{3}{x} + \frac{1}{x^3}.$ We know that $x^3 + \frac{1}{x^3} = 2\sqrt5 \Longrightarrow (x + \frac{1}{x})^3 = 2\sqrt5 + 3x + \frac{3}{x}.$ So, $(x + \frac{1}{x})^3 = 2\sqrt5 + 3(x + \frac{1}{x})$ . Let $y = x + \frac{1}{x}$ . Then, $y^3 = 2\sqrt5 + 3y$ . We want to solve for $x^2 + \frac{1}{x^2}$ . $(x + \frac{1}{x})^2 = x^2 + \frac{1}{x^2} + 2$ . So, we want to solve for $y^2 - 2$ . But, I'm confused about how to solve the equation $y^3 = 2\sqrt5 + 3y$ , should I check random values? Any help is appreciated, thanks in advance! (Thanks for the suggestions in the comments :)","['algebra-precalculus', 'solution-verification']"
4204313,Relatively minimal elliptic surfaces which are not minimal,"A complex surface is called minimal if it contains no $(-1)$ -curves, while an elliptic surface $\pi : X \to C$ is called relatively minimal if the fibers of $\pi$ contain no $(-1)$ -curves. It follows that there could be elliptic surfaces which are relatively minimal but not minimal. Such surfaces exist. Example: Choose two conics in $\mathbb{CP}^2$ which intersect at nine points $p_1, \dots, p_9$ . Then there is a pencil of cubics through these nine points, and this defines a map $\mathbb{CP}^2\setminus\{p_1, \dots, p_9\} \to \mathbb{CP}^1$ (given $p_1, \dots, p_9$ and a tenth point, there is a unique cubic in the pencil which passes through all ten points). As cubics in $\mathbb{CP}^2$ are elliptic curves by the degree-genus formula, blowing up $\mathbb{CP}^2$ at $p_1, \dots, p_9$ gives rise to an elliptic surface $\pi : \operatorname{Bl}_{p_1,\dots, p_9}(\mathbb{CP}^2) \to \mathbb{CP}^1$ . Note that $\operatorname{Bl}_{p_1,\dots, p_9}(\mathbb{CP}^2)$ is not minimal, but $\pi : \operatorname{Bl}_{p_1,\dots, p_9}(\mathbb{CP}^2) \to \mathbb{CP}^1$ is relatively minimal (the fibers of $\pi$ intersect each $(-1)$ -curve once). I have seen it claimed that these are the only examples. How does one show that a non-minimal, relatively minimal elliptic surface is a blowup of $\mathbb{CP}^2$ at nine points? Suppose $X$ is the blowup of $Y$ at $k > 0$ points where $Y$ is minimal. If $\pi : X \to C$ is a relatively minimal elliptic surface, then $c_1(X)^2 = 0$ , so $c_1(Y)^2 = k > 0$ . It follows from the classification of surfaces that $Y$ must be rational, and hence biholomorphic to either $\mathbb{CP}^2$ (in which case $k = 9$ ) or a Hirzebruch surface $\Sigma_n$ with $n \neq 1$ (in which case $k = 8$ ). As $\Sigma_1$ is biholomorphic to the blowup of $\mathbb{CP}^2$ at a point, we can combine the previous two possibilities to conclude that $X$ is a blowup of a Hirzebruch surface $\Sigma_n$ at $8$ points. How can I show that $n$ must be $1$ ?","['complex-geometry', 'complex-manifolds', 'algebraic-geometry', '4-manifolds']"
4204322,Paradoxical sums of non-integrable measurable functions,"Are there reals $\alpha_1,\alpha_2$ and Lebesgue-measurable functions $f_1,f_2:\mathbb R/\mathbb Z\to\mathbb R$ such that $$f_1(x)+f_2(x)>f_1(x+\alpha_1)+f_2(x+\alpha_2)$$ for almost all $x\in\mathbb R/\mathbb Z$ ? Here $\mathbb R/\mathbb Z$ has its usual Lebesgue measure coming from the unit interval. This would be a functional relative of paradoxical decompositions, where moving sets around can make them bigger. The famous example is the Banach-Tarski decomposition of the 2-sphere, which can be achieved with Baire measurable pieces [1]. Feel free to use more functions if it helps with a positive answer, or to add $1$ to the right-hand-side if it helps with a negative answer. Neither of the offsets $\alpha_1,\alpha_2$ can be rational. If $\alpha_i$ is a rational $p/q,$ we can sum (*) over $x=z,z+\tfrac1q,\dots,z+\tfrac{p-1}q$ to eliminate $f_i.$ Then $\alpha_i$ is irrelevant, so we can replace it by any irrational and apply the following argument. Neither $f_1$ nor $f_2$ can be integrable. To show this it suffices to consider the case that $f_2$ is integrable and $\alpha_1\not\in\mathbb Q.$ We can then find an integrable function $g$ satisfying $\int g>0$ and $$f_1(x)+f_2(x)> g(x)+f_1(x+\alpha_1)+f_2(x+\alpha_2)$$ for almost all $x,$ for example $g(x)=\min(1,f_1(x)+f_2(x)-f_1(x+\alpha_1)-f_2(x+\alpha_2))/2.$ Averaging over $x=z,z+\alpha_1,\dots,z+(n-1)\alpha_1$ and cancelling and rearranging gives $$\frac1n (f_1(z)-f_1(z+n\alpha_1))>\frac1n \sum_{k=0}^{n-1}(g(z+k\alpha_1)+f_2(z+\alpha_2+k\alpha_1)-f_2(z+k\alpha_1)).\tag{*}$$ Pick $C$ such that $\{x:f_1(x)>C\}$ has positive measure. By the pointwise ergodic theorem $f_1(z+n\alpha_1)>C$ for infinitely many integers $n\geq 0,$ for almost all $z.$ So $$\liminf_{n\to\infty}\frac1n (f_1(z)-f_1(z+n\alpha_1))\leq 0$$ for almost all $z.$ By the pointwise ergodic theorem again, the right-hand-side of (*) tends to $\int g>0$ for almost all $z\in\mathbb R/\mathbb Z.$ This is a contradiction. [1] Marks, Andrew; Unger, Spencer , Baire measurable paradoxical decompositions via matchings , Adv. Math. 289, 397-410 (2016). ZBL1335.54035 .","['measure-theory', 'ergodic-theory']"
4204357,"Prove or disprove: $(I^2,\mathcal{B},\mu+\nu)$ is localizable.","Definition 1 If $(X_i,\mathcal{S}_i,\mu_i)$ are measure spaces for all $i$ in some index set $I$ , where the sets $X_i$ are disjoint, the direct sum of these measure spaces is defined by taking $X=\bigcup_iX_i$ , letting $\mathcal{S}:=\{A\subset X:A\cap X_i\in\mathcal{S}_i\;\mathrm{for}\;\mathrm{all}\;i\}$ , and $\mu(A):=\sum_i\mu_i(A\cap X_i)$ for each $A\in\mathcal{S}$ . Definition 2 A measure space is called localizable iff it can be written as a direct sum of finite measure spaces. Proposition (a) Any $\sigma$ -finite measure space is localizable. (b) Any direct sum of $\sigma$ -finite measure spaces is localizable. Problem Consider the unit square $I^2$ with Borel $\sigma$ -algebra. For each $x\in I:=[0,1]$ let $I_x$ be the vertical interval $\{(x,y):0\leq y\leq1\}$ . Let $\mu$ be the measure on $I^2$ given by the direct sum of the one-dimensional Lebesgue measures on each $I_x$ . Likewise, let $J_y:=\{(x,y):0\leq x\leq1\}$ and let $\nu$ be the measure on $I^2$ given by the direct sum of the one-dimensional Lebesgue measures on each $J_y$ . Let $\mathcal{B}$ be the collection of sets measurable for both direct sums $\mu$ and $\nu$ in $I^2$ . Prove or disprove: $(I^2,\mathcal{B},\mu+\nu)$ is localizable. Hint: Assuming the continuum hypothesis, the following problem applies to Lebesgue measure: Product measure on two uncountable well-ordered sets My question: It seems to me that the problem given in the hint can be used as a counterexample. But I don't understand how that problem is related to this problem via continuum hypothesis.","['set-theory', 'general-topology', 'measure-theory', 'real-analysis']"
4204362,"Is the $n^\text{th}$ derivative of some function, where $n$ is a matrix, ever defined?","For the function $f(x) = x^a$ , the $n^\text{th}$ derivative $\frac{d^n}{dx^n}x^a = \frac{a!}{(a-n)!}x^{a-n}$ . This can be extended to non integer values of $n$ thanks to the gamma function. I recently red that factorials of matrices are a thing, using the gamma function. Also, one can raise a scalar to a matrix power if it is treated as a Maclaurin series. So, if in the above equation $n,a\in\mathbb{R}^{p\times q}$ , can we treat it as mathematically correct? More generally, can fractional calculus be extended so that we have matrix-th derivatives or even tensor-th derivatives?","['matrices', 'derivatives', 'fractional-calculus']"
4204378,How do I know if basic algebra works?,"This may be a very silly question, but I've been wanting to ask it for a while so here goes. I've been reviewing basic high school math in preparation for analysis, and I find myself questioning a lot of things - especially basic algebra - to the point I'm going a little mad. For instance, polar coordinates can be related to rectangular coordinates like so: $x=r \cos\theta$ , $y=r \sin\theta$ . So, $x^2+y^2=1$ can be rewritten as $r^2=1$ , so $r=1$ . And surely enough, it's the same graph. But why does this work? I know it algebraically makes sense but it just feels a bit odd to me. I guess a broader version of this question is: I've been taught that if something is algebraically derived/true, it must produce the right answer. But how do I know that other than saying, ""if you broke zero algebraic rules, it holds."" The manipulation of these variables is just odd to me. Maybe this feeling will pass. Another example: $$\iint 1-x^2-y^2\,dx\,dy = \iint 1-r^2\,rdr\,d\theta $$ This algebraically holds since $x^2+y^2=r^2$ but how do I know it works other than saying ""algebra works""? I'm definitely overthinking this but it keeps nagging me.","['multivariable-calculus', 'algebra-precalculus']"
4204390,Morphism of schemes such that the image contains every generic point of an irreducible component,"Let $f: X \rightarrow Y$ be a morphism of schemes.  When the image $f(X)$ is dense in $Y$ , we say that $f$ is dominant. No doubt, dominant morphisms pop up all over the place and have an important role in birational geometry.  But a lot of the time it seems to me like the property we are really interested in is that $f(X)$ contains all the generic points of irreducible components of $Y$ .  See, for example, the stacks project entry on Dominant Morphisms Stacks 01RI , which is entirely devoted to the question of when these properties coincide.  In geometric situations, like when our schemes have finitely many irreducible components, or when our morphisms are quasi-compact, the concepts coincide.  But in general, dominance is a much weaker property. Let's for now denote by $(*)$ the property of a morphism that its image contains all the generic points of the irreducible components of the target (aka codimension $0$ points aka maximal points). A couple questions: (1) Does $(*)$ have an agreed upon name in the literature? Is it interesting in its own right? More/less useful than dominance in general scheme theory? (2) Is there a historical reason for the prevailing definition of dominance? Some thoughts: in EGA dominance is introduced along with other purely topological properties of morphisms, like being open, closed, surjective.  Meanwhile $(*)$ can be expressed in purely topological terms, but if we even want compositions of $(*)$ morphisms to be stable under composition then we need to restrict to spectral spaces.  So in some sense dominance makes sense as a purely topological property but $(*)$ heavily depends on respecting structure sheaves.","['general-topology', 'algebraic-geometry', 'schemes', 'commutative-algebra']"
4204419,Solving a Solvable Polynomial by Radicals (Effectively),"I'm trying to actually write some code (in sage ) to take a polynomial $f$ with solvable galois group and compute its roots as nested radicals. Right now I'm just trying to get cyclic extensions to work, as the solvable case should follow by iterating this process, but it seems the approach I'm taking (from Gaal's Classical Galois Theory with Examples ) is ineffective in practice (I tried it on $x^5 + x^4 - 4x^3 - 3x^2 + 3x + 1$ , and it took $30$ hours on my desktop, and ended up running out of memory before it could finish). The current algorithm is: Let $\alpha_0, \ldots, \alpha_{n-1}$ be the roots of $f$ . Let $\sigma$ generate $\text{gal}(f / \mathbb{Q})$ Look at the vectors $v_i = \alpha_0 + \zeta^i \alpha_1 + \zeta^{2i} \alpha_2 + \ldots + \zeta^{(n-1)i} \alpha_{n-1}$ . There are $n$ of these ( $i = 0, \ldots, n-1$ ), and each is an eigenvector of $\sigma$ with eigenvalue $\zeta^i$ (where $\zeta$ is an $n$ th root of unity). That means $v_i^n$ is fixed by $\sigma$ for each $i$ , and thus lies in $\mathbb{Q}$ . Now the coefficients of the polynomial $\psi = (Y - v_0^n)(Y-v_1^n) \cdots (Y-v_{n-1}^n)$ are symmetric in the $\alpha_i$ , and so we can write them in terms of the elementary symmetric polynomials in the $\alpha_i$ . That is, we can write the coefficients of $\psi$ in terms of the coefficients of $f$ . Factor $\psi$ over $\mathbb{Q}$ as $(Y - c_0)(Y - c_1) \cdots (Y - c_{n-1})$ . This tells us each $v_i = \sqrt[n]{c_i}$ . Recover the $\alpha_i$ as weighted averages of the $v_i$ . For instance, $\alpha_0 = \frac{1}{n} \left ( v_0 + v_1 + \ldots + v_{n-1} \right )$ . This is all quite effective, but step $5$ is not in practice. In particular, for the above polynomial of degree $5$ , I cannot actually compute the coefficients of $\psi$ . Does anybody know if this is how people solve this problem in practice? Are there any implementations that people know of which might be more efficient? I've also checked some textbooks on computational number theory, but none that I've seen have included an algorithm for this. Edit: If you want to see the exact code I'm running (be warned, it's kind of brittle right now) you can find it here: https://pastebin.com/8qeduT5m Thanks in advance ^_^","['galois-theory', 'number-theory', 'computational-algebra', 'computational-number-theory']"
4204432,Evaluating $\sum_{n=1}^{\infty} 1/\phi(n)^2$,Wolfram Alpha gives $$\sum_{n=1}^{10000} 1/\phi(n)^2\approx 3.3901989747265619591157$$ and a graph of partial sums indicates fairly clearly this converges: . It's well-known that the sum of the inverse squares $\sum_{n=1}^{\infty} 1/n^2=\pi^2/6$ . Is there a closed form for $\sum_{n=1}^{\infty} 1/\phi(n)^2$ ? (Note: $\phi$ refers to Euler's totient function),"['number-theory', 'sequences-and-series']"
4204437,How to Derive this Formula for Expected n-Way Collisions?,"I learned from the first answer to https://crypto.stackexchange.com/questions/24660/the-effect-of-truncated-hash-on-entropy (by formidable contributor fgrieu) that when $N$ uniform random selections are made from a finite set of $N$ elements, the probability that any particular element will be selected exactly $j$ times $\displaystyle\approx\frac 1{e\;j!}$ , so the expected number of elements selected $j$ times $\displaystyle\approx\frac N{e\;j!}$ . The approximation is very rough for small $N$ and seems to converge gradually.  For sufficiently large $N$ (certainly by $10^9$ ) it works very well. Contributor fgrieu even included the results of computational experiments for $N = 2^{35}$ , confirming that this formula is very accurate for small $j$ , and ""breaks down"" only when $j$ is near its maximum. The author wrote that ""we can establish"" this expression ""by counting of the possibilities."" I've been trying to derive the formula myself, and searched for a derivation online, so far without success. I think I grasp the presence of fundamental constant $e$ ; for example, a classic gambler's problem asks ""if the probability of winning a play is $\frac 1 n$ , what is the probability of making $n$ plays without winning?""  The probability of losing each play is $1 - \frac 1 n$ , so the probability of losing $n$ in a row is $\left( 1 - \frac 1 n \right)^n$ , and $$\lim\limits_{n \to \infty} \left( 1 - \frac 1 n \right)^n = \frac 1 e$$ giving the asymptotic proportion of zero-win outcomes. This also exactly accords with fgrieu's formula for the case $j=0$ . I've made no progress, however, justifying the formula's divisor $j!$ .  I thought about $j!$ as the number of distinct orderings of selections of a given element in the sequence of $N$ selections, but didn't see that leading anywhere.  I enumerated every possible selection sequence for some tiny cases with very small $N$ , looking for patterns in those sequences resulting in $j$ -way collisions, without seeing the light.  [Exhaustive enumeration is a desperate exercise of size $N^N$ ; its appeal was that under the premise of uniform random selection every sequence is equally probable, so counting sequences with particular outcomes is a direct measure of probability.] I've pondered the implicit recurrence that for $j > 0$ , the probability a particular element will be selected exactly $j$ times equals $\frac 1 j $ times the probability it will be selected $j-1$ times ... but I have yet to find an explanation of why this must be so. My intuition is that the derivation is simple, and I'm missing something that's right in front of me.  My thanks to any and all who will help me see it.","['statistics', 'combinatorics', 'probability']"
4204440,Showing a set is residual,"Let $D = [c,d]\times [c,d]\subseteq \mathbb{R}^2$ and let $A$ be the set of all closed subsets of $D$ . For $a \in D$ and $B\in A,$ define $d(a,B) := \min\{d(a,b) | b\in B\},$ where the $d$ inside the min is the Euclidean metric on $\mathbb{R}^2$ . For $B,C \in A,$ let $d_A(B,C) = \max\{\max_{b\in B} d(b,C), \max_{c\in C} d(c,B)\}.$ For a metric space $X$ , define an isolated point $x\in X$ to be a point such that $\exists r > 0$ so that $B^*(x, r) \cap X = \emptyset,$ where $B^*(x,r) = B(x,r)\backslash \{x\}$ and $B(x,r)$ is the open ball centered at $x$ of radius $r$ . Prove that the set $G := \{B\in A: B\text{ has no isolated points}\}$ is residual in $(A,d_A)$ by showing that for $k\geq 1,$ the set $F_k := \{B \in A : \exists b \in B, \overline{B}(b, \frac{1}k)\cap B = \{b\}\}$ is closed and nowhere dense. Here, $\overline{B}(b, \frac{1}k)$ is the closed ball of radius $\frac{1}k$ centered at $b$ . If one shows that the sets $F_k$ are closed and nowhere dense, then since $G^C = \cup_{n\geq 1} F_n$ (which is not hard to show), $G^C$ is first category. However, I'm really struggling to show that the sets $F_k$ are nowhere dense. I tried taking an element $C \in F_k^C$ and finding an $r > 0$ so that $B(C, r) \subseteq F_k^C, B(C,r) := \{B \in A : d_A(B,C) < r\}.$ I think $r$ should depend on $\sup_{a,b \in C} d(a,b)$ and on $\frac{1}k,$ but I don't know how to define $r$ to make this work. Assume a working r is chosen (I know it exists, but I'm not sure what it is). Let $D \in B(C,r).$ I tried seeing if I could choose r (independent of $D$ of course but possibly dependent on $C$ ) to get a contradiction if I assumed that $D \in F_k.$ But I wasn't able to do so. In particular, if I assume $D \in F_k$ and choose $b \in D$ so that $\overline{B}(b, \frac{1}n)\cap D = \{b\},$ my issue is that $b$ might very well be in $C$ . Alternatively, I tried showing that every sequence with elements in $F_k$ that converges in $A$ also converges in $F_k,$ but I wasn't able to show this either. If I can show the sets are closed, then to show they're nowhere dense it suffices to show that they have empty interiors. But given an element $C \in F_k,$ I'm not sure how to find for every $r > 0$ a set $B$ so that $d_A(B,C) < r$ but $B\not\in F_k.$ I know some things that might be useful: the EVT on compact metric spaces, the fact that for a compact metric space, for all sets $K$ of closed sets in $X$ , if every finite subset of $K$ has nonempty intersection, so does $K$ . Also, every compact metric space is complete and totally bounded. As a side note, I know I've posted this question before with the exact same background information. However, the requirements for this question are quite different from the requirements in the previous question I posted so I think an answer to this question will be quite a bit different to an answer to the previous question.","['metric-spaces', 'real-analysis', 'calculus', 'elementary-set-theory', 'general-topology']"
4204449,How do you prove that $\frac{10^{n}-1}{10^{n}-2}=1+\sum_{i=0}^{\infty}2^{i}\times10^{-n(i+1)}$,"I was just wondering around and I've noticed that $\frac{9\ldots9}{9\ldots8}$ ratio, lets say $\frac{99}{98}$ is beautiful: $1.010204081632$ . Basically, this is powers of two separated by as many zeros as you have nines in denominator. I'm not that good in math to understand the underlying principle, I'm just curious why this happens and what powers of twos has to do with $\frac{99}{98}$ .","['limits', 'sequences-and-series', 'real-analysis']"
4204478,Invertible sheaf on a scheme is coherent,"If we define invertible sheaf as a locally free sheaf of rank 1, which is the most common definition. I saw it is true that an invertible sheaf must be quasi-coherent. But why is it also coherent? Update: My concerns are related to the fact that $\mathcal{O}_X$ , the structure sheaf, can be NOT coherent if it is not locally-Noetherian. See Is locally free sheaf of finite rank coherent? please.","['algebraic-geometry', 'coherent-sheaves', 'sheaf-theory']"
4204545,Two eigenvalues and an eigenvector walk into a bar...,"Suppose I have the transformation $T(v) = Av = \lambda v$ . If two of the eigenvalues are $\lambda_1$ and $\lambda_2$ where $\lambda_1=-\lambda_2$ , is there a way to quickly find the eigenvector(s) for $\lambda_2$ if I know the eigenvector(s) for $\lambda_1$ ? I ask this because attempting to find the kernel of larger matrices with irrational numbers and writing down each step is time consuming. I noticed a subtle relationship between the eigenvectors for eigenvalues of opposite signs in that the entries are the same with the exception of a negative sign somewhere. Any assistance is much appreciated!","['matrices', 'linear-algebra', 'eigenvalues-eigenvectors']"
4204561,The form of an $n \times n$ unitary matrix,"I recently came across this fact that any $ 2 \times 2 $ unitary matrix can be expressed as $$ \begin{bmatrix} w & z \\ -\overline{z}e^{i\theta} & \overline{w}e^{i\theta}\end{bmatrix}$$ for some $\theta \in \mathbb{R}$ and $w,z \in \mathbb{C}$ . I wanted to ask if there is any similar general form for $n \times n$ unitary matrices too.","['matrices', 'unitary-matrices', 'linear-algebra']"
4204567,Counting tetrahedrons inside a dodecahedron,"The regular dodecahedron has $20$ vertices, and $12$ faces which are (congruent) regular pentagons; three pentagons meet at a vertex. Fixing a vertex $v$ of dodecahedron, we count the other vertices, whose minimum distance from $v$ is $3$ . One can see the Schlegel diagram of the dodecahedron here . There are exactly $6$ vertices at distance $3$ from $v$ : call them $x,x',y,y',z,z'$ . With figure below (Schlegel diagram), we can see that we can partition these six vertices into two sets, such that vertices in each set are at distance $3$ from each other. Thus, from the vertex $v$ , we produced $2$ tetrahedrons, with one vertex common. But, I was unable to see from this construction , why do we get only $10$ regular tetrahedrons inside regular dodecahedron? Can anybody explain it? (Note: I know some other way - going via cubes, and then tetrahedrons; but, here I am trying to understand it from the Schlegel diagram, for better understanding of the diagram and explanation.) why this question came? There are $20$ vertices in Dodecahedron, and tetrahedron has $4$ vertices, so if we want to get $10$ tetrahedrons, I was unable to see how the repetition of vertices is coming in tetrahedrons while distributing vertices of dodecahedron.","['group-theory', 'geometry']"
4204585,How do I work out the value of $\theta$ from the identity $\sin(\theta)=\sin(180°-\theta)$?,"Using the identity $\sin(\theta)=\sin(180°-\theta)$ , I obtained some contradictory results. If we let $\theta=70°$ , then we get that \begin{align}
\sin(70°) &= \sin(180° - 70°) = 0.9396926208 \\[5pt]
\sin^{-1}(\sin(70°)) &= \sin^{-1}(\sin(180° - 70°)) \\[5pt]
70° &= 180° - 70° \\[5pt]
140° &= 180° \, .\\
\end{align} The identity also seems to imply that $\theta=90°$ : \begin{align}
\sin^{-1}(\sin(\theta))&=\sin^{-1}(\sin(180°-\theta)) \\[5pt]
\theta&=180°-\theta \\[5pt]
\theta&=90°
\end{align} Both of these results seem mistaken, but I don't know where I went wrong.","['algebra-precalculus', 'trigonometry']"
4204612,Hartshorne Exercise II. 3.3 (c) for locally of finite type?,"$\DeclareMathOperator{\Spec}{Spec}$ Exercise II. 3.3 (c) in Hartshorne's Algebraic geometry is Show also if $f: X \to Y$ is of finite type, then for every affine subset $V = \Spec(B) \subset Y$ , and for every open affine subset $U = \Spec A \subset f^{-1}(V)$ , $A$ is a finitely generated $B$ -algebra. My proof is to cover $f^{-1}(V)$ by finitely many open affines $\Spec A_i$ , such that each $A_i$ is a finitely generated $B$ -algebra. Then we can cover $U$ by finitely many open affines $D(a), a \in A$ , which are both standard opens with respect to $\Spec A$ and one of the $\Spec A_i$ (see this lemma ). Then each $D(a)$ is finitely generated over one of the $A_i$ , and hence also finitely generated over $B$ . Then this property asserts that $A$ is indeed finitely generated over $B$ . However, I wonder if we really need $f$ to be of finite type, or if $f$ being locally of finite type is sufficient? Because we only need that we can cover $U$ by finitely many of the $D(a)$ , which is certainly true in either case, because $U$ is quasi-compact. Is my reasoning correct?","['schemes', 'algebraic-geometry', 'solution-verification', 'commutative-algebra']"
4204635,Find the number of solutions of the equation $e^z = 2z+1$ in the open unit disc $\{z \in \Bbb C : |z| < 1\}$,"Find the number of solutions of the equation $e^z = 2z+1$ in the open unit disc $\{z \in \Bbb C : |z| < 1\}$ . My Attempt: Let $f(z) = e^z-2z-1$ . Let $A \subseteq \{z \in \Bbb C : |z| < 1\}$ be the solution set. For solution,we will put $f(z) = 0$ . Since $f(0) = 0$ so $0 \in A$ . Therefore $A \neq \varnothing$ . Also let $z = x+iy$ then $f(z) = 0$ gives $$e^{x+iy}-2(x+iy)-1 = 0 \implies e^xe^{iy}-(2x+1)-i(2y) =0\\
\implies e^x(\cos y + i\sin y) = (2x+1) + i(2y)$$ On comparing real and imaginary parts on both sides, we get $$e^x\cos y = 2x + 1,\ e^x\sin y = 2y.$$ On dividing, we get $\tan y = \dfrac{2y}{2x+1}$ . I have no general formula. Please help me.","['complex-analysis', 'roots']"
4204650,Number of real roots of polynomial and intermediate value theorem,"Let $p:\mathbb{R}\rightarrow\mathbb{R}$ be a polynomial function with real coefficients satisfying \begin{align}
p(x_1)<0, p(x_2)>0, p(x_3)<0,\ldots \text{(sign flips in alternating manner)}
\end{align} for $x_1<x_2<\ldots<x_n$ . Even without polynomial assumption, by virtue of intermediate value theorem (IVT), it is apparent that there are at least $n-1$ distinct real roots—precisely, at least one root in each interval $(x_k,x_{k+1}), k=1,\ldots,n-1$ . Replacing all strict inequalities with non-strict counterparts, that is, \begin{align}
p(x_1)\leq0, p(x_2)\geq0, p(x_3)\leq0,\ldots \text{(sign flips in alternating manner)},
\end{align} we can again guarantee that there are at least $n-1$ real roots in $[x_1,x_n]$ when taking multiplicity into account. Let us commence (somehow dirty) derivation with strong induction on $n$ . Base case: The base case $n=2$ is trivial: if either $p(x_1)$ and $p(x_2)$ is zero, we are done; otherwise, applying IVT works out. Induction step: Assume that the claim holds for $n<m$ and consider the case $n=m$ . Let us divide and conquer the problem. i) $p(x_1)=0$ : $x_1$ is a root, and there are at least $m-2$ roots in $[x_2,x_m]$ from induction hypothesis. ii) $p(x_1)<0$ , $p(x_2)>0$ : There is at least one root in $(x_1,x_2)$ by IVT, and there are at least $m-2$ roots in $[x_2,x_m]$ from induction hypothesis. iii) $p(x_1)<0$ , $p(x_2)=0$ : We further divide into three cases: iii-a) $p'(x_2)=0$ : $x_2$ is a root with multiplicity 2, and there are at least $m-3$ roots in $[x_3,x_m]$ from induction hypothesis. iii-b) $p'(x_2)>0$ : We can find $x_2'\in(x_2,x_3)$ such that $p(x_2')>0$ . Applying the inductive hypothesis to $x_2',x_3,\ldots,x_m$ , there are at least $m-2$ roots in $[x_2',x_m]$ . Together with $x_2$ , there are at least $m-1$ roots in $[x_2,x_m]$ . iii-c) $p'(x_2)<0$ : We can find $x_2'\in(x_1,x_2)$ such that $p(x_2')>0$ . Therefore, there is at least one root in $(x_1,x_2')$ by IVT. Together with $m-2$ roots in $[x_2,x_m]$ where the existence is guaranteed by induction hypothesis, we have $m-1$ roots in $[x_1,x_m]$ . I reckon that the statement of the claim is intuitive whereas my proof described above is unnecessarily complicated. Hence, my question is twofold: Is there any simple and elegant proof of this claim? Is there any name referring to this claim? Any comments would be much appreciated.",['calculus']
4204652,"Reference for time-derivatives of measure-valued ""processes""","The notion of the weak derivative of a process of probability measures $(\mu_t)_{t\geq 0}$ in optimal transport theory seems to be so basic that, e.g., in ""Optimal Transport, old and new"" the author hardly bothers to even define it. Apparently it's somehow defined by ""duality"" with smooth functions, maybe somewhat like $\partial_t \int \phi \,d\mu = \int \phi'\,d(\partial_t \mu)$ for test functions $\phi$ . For some more context, it appears for example in the conservation of mass formula (which I cannot explain any further, unfortunately): $$\partial_t \mu + \nabla \cdot (\mu \xi) = 0.$$ Where can I read about the definition of $\partial_t \mu$ , what conditions are necessary and perhaps some explicit examples? (As an aside I also wonder what $\mu \xi$ could mean for some ""velocity field"" (vector field?) $\xi$ ).","['optimal-transport', 'derivatives', 'probability-theory', 'reference-request']"
4204658,How do I apply some extended sketching techniques?,"One of the exam papers I had asked me to sketch $\cos{\theta}=\frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha$ with $\theta$ and $\alpha$ as the axises and for $0<\alpha<2\pi$ . I've tried converting $\frac{1}{\sqrt2}\sin\alpha+\frac{1}{2\sqrt2}\cos\alpha$ into a single trig term which was $\sqrt{\frac{5}{8}}\cos(\alpha-\arctan(2))$ , using this I've found the maxima to be at $\alpha=\arctan(2)$ and $\alpha=\arctan(2)+\pi$ for the domain but that's as far as I can go. What else do I need to find to sketch this graph?","['trigonometry', 'graphing-functions', 'derivatives']"
4204702,Calculating the probability of winning a game with random numbers.,"The game This is a one player game. $n$ numbers are generated uniformly randomly from $0$ to $1$ . These numbers are then written on $n$ tiles. One number per tile.  You may only look at one tile. You must then guess which tile has the largest number on it. Quick example The numbers $0.782$ , $0.432$ and $0.882$ are generated randomly. Each wrote respectively on tiles $1, 2$ and $3$ . You peek at tile $1$ and see $0.782$ and guess this as the largest tile. You are wrong. You loose. Solution for n=2 Naturally in this simple case one peaks at any tile and if it greater than $\frac{1}{2} $ you guess that tile and if it is less you then guess the other tile. Let P denote the random variable that is the value on the tile you peak. $\mathbb{P}[$ You win ] = $\int_{0}^1 \mathbb{P}[$ You win | $P = p ] f(p) 
  dp $ $f(p) =1 $ clearly. Hence splitting the integral over zero to one half and one half to one yields three quarters. My question General formula for the Probability of winning for a given $n>2$ I want to know when one should ""stick"". I know the expected value of the maximum of $k$ tiles is $\frac{k}{k+1}$ . Should we only stick if our peaked tile is greater than that? Intuitively I feel like no. Is there some recursion we could use? Thanks :)","['statistics', 'expected-value', 'game-theory', 'probability', 'random-variables']"
4204707,Multi-keyhole contour integral with branch cut,"How to construct a contour to calculate complex line integral $$\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}\arctan\left(\frac{1}{z}\right)\arctan\left(\frac{1}{s-z}\right)\,\mathrm{d}z$$ This integral is derived from the Laplace transform of the square of the Sinc function. $$\mathscr{L}[f^2(t);s]=\frac{1}{2\pi i}\int_{c-i\infty}^{c+i\infty}F\left(z\right)F\left(s-z\right)\,\mathrm{d}z$$ As shown in the expression above, Wikipedia gives the Laplace transform formula of function product as. I want to verify this formula by taking function as the Sinc function. I've found six singularities $z=\pm\,\!i,0,s,s\pm\,\!i$ so far, but I don't know how to construct the contour. I already know how to calculate $\displaystyle\int_{0}^{+\infty}\!\left(\frac{\sin(x)}{x}\right)^{\!\!2}\,e^{-sx}\,dx$ through the parametric integral, but now I don't know how to construct the contour and use the residue theorem to calculate the arctangent complex integral.","['complex-analysis', 'contour-integration', 'laplace-transform']"
4204722,Decomposition of generalized Einstein condition into irreducible representations of orthogonal group,"I struggle with making sense of the claim in Chapter 16.2 of Arthur Besse's book ""Einstein manifolds"". Let $\nabla$ be an affine connection on the manifold $\mathcal{M}$ . We want to decompose the covariant derivative of the Ricci tensor $\nabla Ric$ into the irreducible representations of the orthogonal group. Since we assume the connection to be Levi-Civita, the tensor $\nabla Ric$ is a section of a vector bundle $H \subset T^*\mathcal{M} \otimes S^2 \mathcal{M}$ , the fibre of which consists of tensors symmetric in its last two indices and obeying the Bianchi identity: $$
2 \sum \xi(X_i, X_i,X) = \sum \xi(X,X_i, X_i)
$$ For the purposes of the classification, the author introduces a number of natural vector bundle homomorphisms, and only two of them are relevant to my question, the contraction on the first two indices $$
\gamma(\xi)(X) = \sum \xi(X_i, X_i, X)
$$ and the construction of rank-3 tensor (a section of $H$ ) out of a covector $$
\varphi(\zeta)(X,Y,Z) = (X,Y) \zeta(Z) + (X,Z) \zeta(Y) + 2n (n-2)^{-1} (Y,Z) \zeta(X)
$$ where $X,Y,Z\in T_x \mathcal{M}$ , $\zeta\in T^*_x \mathcal{M}$ , and $n = \dim \mathcal{M}$ . I cannot understand the author's argument that $Q=Im\,\varphi$ and $\Gamma = H \cap Ker\, \gamma$ are orthogonal complements in $H$ : $Q \oplus \Gamma = H$ . First, consider $\gamma \circ \varphi = \frac{2 n}{n-2} Id_{T^*M}$ . This clearly implies that $Q \cap \Gamma = 0$ .
Next, the author claims that since $$
(\varphi(\zeta),\xi) \, =^? \frac{7n-6}{n-2}(\gamma(\xi), \zeta)
$$ it follows that $Q \cup \Gamma = H$ . I understand neither the logic of the last claim nor the coefficient on the RHS. Should not it be $\dfrac{6n-4}{n-2}$ ?",['differential-geometry']
4204724,Proof of Sampling/Importance Resampling (Weighted Bootstrap) technique,"From Casella Berger exercise 5.65: Let us have $X \sim f$ . Then, assume we produce $m$ i.i.d. random variables $Y_1,...,Y_m$ from another distribution $g$ . Let us have $$q_i = \frac{\frac{f(Y_i)}{g(Y_i)}}{\sum_{j = 1}^{m}\frac{f(Y_j)}{g(Y_j)}}$$ Now, we generate random variables $X^\star$ from the discrete distribution of $P(X^\star = Y_i) = q_i$ . This technique seems to be called ""Sampling/Importance Resampling (SIR) / weighted bootstrap"". I need to show that $X_1^\star, X_2^\star, ..., X_r^\star,..$ are approximately a sample from $f$ . The textbook hint given says: ""Show that $P(X^\star \leq x) = \sum_{i = 1}^m q_iI(Y_i \leq x)$ . From there use WLLN."" The hint itself seems wrong - we are not supposed to have indicator random variables $I(Y_i \leq x)$ in the definition of $P(X^\star \leq x)$ . The solution manual on the internet is wrong. The issue in my reasoning: $$P(X^\star \leq x) = \sum_{i = 1}^m P(X^\star \leq x | X^\star = Y_i)P(X^\star = Y_i) = \sum_{i = 1}^m P(Y_i \leq x)q_i = P(Y_i \leq x)\sum_{i = 1}^m q_i$$ This does not look anything like the required equality in the hint. We arrived at the conclusion that $F_{X^\star}(x) = G_Y(x)$ . Which is back to the definition... How can this be shown, preferably using WLLN?","['bootstrap-sampling', 'probability-theory', 'probability', 'sampling']"
4204786,Begin with the unit ball $\mathbb{B}^n$ and identify antipodal points of its boundary sphere.,"I want to look at the picture of this. This is a question from M.A. Armstrong and we need to show that it is homeomorphic to the unit sphere $\mathbb{S}^n$ in $\mathbb{Е}^{n + 1}$ and partition it into subsets which contain exactly two points, the points being antipodal (at opposite ends of a diameter). I have the intuition that the geometry is : I will have to essentially identify the points $\{x,-x\}$ such that $||x|| = 1$ and if $||x|| \ne 1$ . I was thinking of the fact that the equator will be something of a mobius strip (which is obtained by attaching $\{x,-x\} $ ) and the union of two discs. Can someone explain me(with a diagram if possible ) where I am going wrong?","['general-topology', 'analysis']"
4204788,"$|f''(x)|\leq 1$, $\exists\ x_1\neq x_2, \in [0,1]$ such that $f(x_1)=f(x_2)=0$. Show that $|f(x)|\leq 1, \forall\ x\in [0,1].$ [closed]","Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question $|f''(x)|\leq 1$ , $\exists\ x_1\neq x_2, \in [0,1]$ such that $f(x_1)=f(x_2)=0$ . Show that $|f(x)|\leq 1, \forall\ x\in [0,1].$ If $x_1=0, x_2=1$ , then Taylor expansion would help. But here we only know that $x_1\neq x_2$ ! What this helps?","['calculus', 'derivatives', 'taylor-expansion']"
4204789,Find all function $f(n)$ satisfying $f(n)^2 = n f(f(n))$,"Find all functions $f:\mathbb N^*\to \mathbb N^*$ satisfying: $f$ is a strictly increasing function, and $f(n)^2 = n f(f(n))$ , $\forall n\in\mathbb N^*$ . Suppose we have $f(a) = f(b).$ Then $$\begin{align}
f(a)^2 = f(b)^2 &\Rightarrow af(f(a)) = bf(f(b)) \\
&\Rightarrow a = b
\end{align}
$$ because $f(f(a)) = f(f(b)) \neq 0$ . A function $f:\mathbb N^*\to \mathbb N^*$ satisfying $f$ is a strictly increasing function $\Rightarrow f(k) \geq k$ . It is easy to see that $ f(x) = kx$ satisfies the equation. But are there any other solutions? I have no idea anymore. Hope to get help from everyone. Thanks very much.","['functional-equations', 'algebra-precalculus', 'functions']"
4204832,Help showing $y \cdot f(x+1) \leq \frac{C_0(p) \cdot y \cdot f(y)}{2} + \frac{x \cdot f(x)}{2}$,"I was reading through this paper and noticed a seemingly simple algebraic lemma (Lemma 2 on page 6) for which a proof is not given. I cannot seem to prove the fact myself, so I was hoping to turn to the math commmunity for help. The lemma is as follows: Let $f(x)$ be a polynomial in $x$ of degree $p$ with non-negative coefficients. Then, for any $x,y \geq 0$ , \begin{align*}
y \cdot f(x+1) \leq \frac{C_0(p) \cdot y \cdot f(y)}{2} + \frac{x \cdot f(x)}{2}
\end{align*} where $C_0(p) = p^{p(1 - o(1))}$ . I don't really understand what this $C_0(p)$ function represents and would appreciate any help in proving this lemma!","['game-theory', 'proof-explanation', 'functions', 'polynomials']"
4204974,On the integral of Frobenius norm of Jacobian,"Let $g : S \to \mathbb{R}^d$ be injective and smooth where $S \subseteq \mathbb{R}^k$ is a $k$ -dim regular smooth compact manifold with $d \geq k$ . I'de considering the term $$
A = \int_S \lVert J_g(x) \rVert_F \, dx
$$ where $J_g$ is Jacobian matrix of $g$ and $\lVert \cdot \rVert_F$ is Frobenius norm. If $S$ is an one-dimensional interval on $\mathbb{R}$ , then $A$ is a the length of the curve given by $g(S)$ . I have following questions: What would be a physical meaning of $A$ ? Does increasing the $k$ -dimensional area of $g(S)$ increases $A$ , and vice versa? If 2 is true, how to simply prove? Hopefully my question is not so stupid one.","['jacobian', 'differential-geometry']"
4204984,Algebraic Geometry + Sheaves Reference for Beginner,"I do not know very much (in fact, almost nothing) about algebraic geometry but have some knowledge in commutative algebra (e.g. taken some graduate-level courses in the theory of rings, modules, and Galois theory). I have recently worked through Sheaves in Geometry and Logic because I became interested in topos theory on logical and foundational grounds. I was interested in some of their examples of the Zariski site, but not having any knowledge in Algebraic Geometry, I felt that I missed the full power of topos theory as applied in this way (and it also took me far longer to fill in the details of the examples than it presumably would if I had been familiar with the subject). Also, it's a branch of math which I haven't gotten around to learning that sounds really interesting for its own sake. What would be a good text for learning algebraic geometry which fully leverages category theory, sheaves, and topos theory from the beginning? This question is similar to some others on this site, but other questions focus on either (1) looking the applications of topos theory to algebraic geometry, from the perspective of someone who already knows some of both, (2) learning algebraic geometry only as it applies to understanding specific examples in topos theory, or (3) learning enough topos theory to apply it to algebraic geometry. None of these match my particular situation. I don't mind difficult, dense, or abstract texts. But I would rather not go on a wild nCatLab/Wikipedia chase to find introductory theorems in algebraic geometry that are casually referenced in the book, so I do require that it be reasonably introductory for someone with my knowledge. Thanks!","['self-learning', 'topos-theory', 'algebraic-geometry', 'reference-request']"
4204993,Separation of variables of 2nd order PDE yields 1st order ODEs,"Consider the telegraph or Klein-Gordon equation on a rectangle*, $$
\begin{align}
\left(\frac{\partial^2}{\partial x^2}-\frac{\partial^2}{\partial y^2}\right)\psi(x,y)=\gamma^2\psi(x,y),
\end{align}
$$ with $\gamma$ some arbitrary (maybe complex) constant . Suppose that the necessary initial conditions are given ( $\psi$ or its $x$ derivative at $x_1$ and $x_2$ if $x$ varies between $[x_1,x_2]$ , and $\psi(x,t_0)$ and $\partial\psi(x,t_0)/\partial t$ at some arbitrary $t_0$ ), then it is convenient to separate variables in the form $\psi=A(x)B(y)$ , which yields the ODEs $$
\begin{align}
\frac{\mathrm{d}^2A}{\mathrm{d}x^2}=-\alpha^2A\;\;\;,\;\;\frac{\mathrm{d}^2B}{\mathrm{d}y^2}=-(\gamma^2+\alpha^2)B,
\end{align}
$$ for $\alpha$ some (maybe complex) constant. These are second order equations and then the problem is essentially solved thanks to Sturm-Liouville: along $x$ and $y$ lines we have complete bases that can span generic initial/boundary data. Now we rotate the coordinates: $$
\begin{align}
u=x+y\;\;\;,\;\;v=x-y
\end{align}
$$ and observe that the equation becomes $$
\begin{align}
\frac{\partial}{\partial v}\frac{\partial}{\partial u}\psi(u,v)=\frac{\gamma^2}{2}\psi(u,v),
\end{align}
$$ and after separating variables as $\psi=U(u)V(v)$ we have that $$
\begin{align}
\frac{\mathrm{d}\log U}{\mathrm{d} u}\frac{\mathrm{d}\log V}{\mathrm{d} v}=\frac{\gamma^2}{2}.
\end{align}
$$ There are several ways to split the constants (e.g. $\frac{\mathrm{d}\log U}{\mathrm{d} u}=\beta,\;\frac{\mathrm{d}\log V}{\mathrm{d} v}=\frac{\gamma^2}{2\beta}$ , with $\beta$ an arbitrary constant), but the point is that Now we have 1st rather than 2nd order equations Then, is it true that the eigenfunctions form a basis that can span arbitrary initial/boundary data (specified on the $(u,v)$ plane)? I suspect the answer is no because the family of real exponentials are not complete. Moreover, is there any deep reason for the separated ODEs to become 1st order equations after the rotation ? *The rectangle is just to illustrate the problem, but something similar may happen on several different domains.","['sturm-liouville', 'functional-analysis', 'mathematical-physics', 'partial-differential-equations']"
4204997,Solve $(3x+2)y''+7y'=0$,"Solve $(3x+2)y''+7y'=0$ My first idea was to reduce into an euler-cauchy equation with $z = y' \rightarrow (3x+2)z' + 7z =0$ Let $e^t = (3x+2)$ then $z' = 3e^{-t}\frac{dz}{dt}$ . The previous equation can be written as : $$e^t\cdot3e^{-t} \frac{dz}{dt} + 7z = 0 \rightarrow 3z' + 7z=0$$ Now this is a separable differential equation: $z=C_2e^{\frac{-7}{3}t} + C_1$ . Now $t = \ln(3x+2)$ $$z = C_2e^{\frac{-7}{3}\ln(3x+2)} + C_1=C_2(3x+2)^{\frac{-7}{3}}+C_1$$ Since $z = y'$ we get another equation $y' = C_2(3x+2)^{\frac{-7}{3}}+C_1 \rightarrow y= C_3(3x+2)^{-4/3} + C_2x + C_4$ Thus, since we only need one solution $C_4 = 0$ , and re-writing we can get: $$y = C_1(3x+2)^{-4/3} + C_2x$$ Is this a valid justification?","['solution-verification', 'ordinary-differential-equations']"
4205019,"What is the structure, if any, on the projective space over a field?","In my algebraic geometry course, we study projective spaces. We jumped right in with the definition and then gave some examples. I think I understand the definition itself, but I'm really struggling to get why it matters at all. The definition we have is the standard one: The n-dimensional projective space over a field K, $\mathbb{P}^n_k$ , is the set of all one-dimensional subspaces of $K^{n+1}$ . I get that that as a set, $\mathbb{P}^n_k$ is just all the lines through the origin in the vector space $K^{n+1}$ . We considered this specific example: $\mathbb{P}^1_\mathbb{R}$ . This is the set of all lines in $\mathbb{R}^2$ through the origin. However, we 'identified' this set with the circle; we showed that every line through the origin intersects the top half of a hemisphere in $\mathbb{R}^2$ , except for the horizontal line, which intersects two antipodal points. So we say by identifying these points together, the space is essentially, the circle. My confusion is the following: In the example of $\mathbb{P}^1_\mathbb{R}$ , we say it's a circle. The only reason I see we did this, is because there was a bijection of lines through the origin, to points (antipodal points) on the circle. But I don't get why we say this, and what this circle represents? For example, if by 'identify with' we mean merely 'is in bijection with', I can identify $\mathbb{P}^1_\mathbb{R}$ with any set of equal cardinality. But why is this useful? For example, in algebra, we're not typically interested in bijections, if they don't also preserve the algebraic structure. A bijection that is not an isomorphism is generally not as enlightening as a bijection that respects the ring structure for example. So in the case of projective spaces, what is the 'geometric structure' that $\mathbb{P}^1_\mathbb{R}$ comes endowed with? As far as I can tell, the $\mathbb{P}^1_\mathbb{R}$ is just a set of equivalence classes that can be put in bijection with a circle; but why is this interesting?  Am I completely misunderstanding?","['algebraic-geometry', 'projective-space']"
4205040,"How to prove that the limit of $f(x,y)=\dfrac{x^2 \sin(y) + y^2 \sin(x)}{x^2+y^2}$ is $0$ as $(x,y)$ approaches $(0,0)$?","I need to show that $\displaystyle\lim_{(x,y)\rightarrow(0,0)}\dfrac{x^2 \sin(y) + y^2 \sin(x)}{x^2+y^2}$ exists. I know that it is equal to zero. Until now, all I know how to do is to prove using the $\epsilon-\delta$ definition. I tried to prove it by the following way: Let $\delta>0$ be a real number such that $0<\sqrt{x^2+y^2}<\delta$ . We have $0< x^2+y^2<\delta^2$ . We know that $\left\vert x^2\sin(y)+y^2\sin(x)\right\vert\leq\left\vert x^2\sin(y)\right\vert+\left\vert y^2\sin(x)\right\vert=x^2\left\vert\sin(y)\right\vert+y^2\left\vert\sin(x)\right\vert\leq x^2+y^2$ . Thus $$0<\left\vert x^2\sin(y)+y^2\sin(x)\right\vert\leq x^2+y^2<\delta^2$$ and dividing the inequality by $x^2+y^2>0$ we have $$ 0<\dfrac{\left\vert x^2\sin(y)+y^2\sin(x)\right\vert}{x^2+y^2}<1<\dfrac{\delta^2}{x^2+y^2}. $$ I thought that this could be useful because $\dfrac{\left\vert x^2\sin(y)+y^2\sin(x)\right\vert}{x^2+y^2}=\left\vert\dfrac{x^2\sin(y)+y^2\sin(x)}{x^2+y^2}\right\vert$ , but I don't know how to proceed.","['multivariable-calculus', 'epsilon-delta']"
4205041,"If $\Phi\geq 0$ is non-decreasing, does $\int_1^\infty \frac{\Phi(x)}{x^2}\,dx=\infty$ imply $\int_0^\infty e^{-\Phi(x)}\,dx<\infty$?","Q. Suppose $\Phi : [0, \infty) \to [0, \infty)$ is a non-decreasing function. If $$ \int_{1}^{\infty} \frac{\Phi(x)}{x^2} \, \mathrm{d}x = \infty, $$ then does the inequality $$ \int_{0}^{\infty} e^{-\Phi(x)} \, \mathrm{d}x < \infty $$ hold? This question is motivated by this posting , but it became a statement of independent interest to me. I suspect that the statement is true, based on a naive observation that a higher growth rate of $\Phi$ is in favor of both conditions and a slower growth rate is against them. One obvious observation is that $\Phi(x)$ must tend to $\infty$ as $x \to \infty$ . But, to be honest, I have no clue as to where I should start. So I would like to invite you to this strange yet interesting problem! Edit. The user @Sungjin Kim showed that the answer is false by providing a family of counter-examples. Here is a simplification of his answer: For each strictly increasing sequence $(b_k)_{k=1}^{\infty}$ in $[1, \infty)$ , let $$ \Phi(t) = \sum_{k \mathop{:} b_k \leq t} b_k = \sum_{k=1}^{\infty} b_k \mathbf{1}_{[b_k, \infty)}(t). $$ Then it follows that $$ \int_{1}^{\infty} \frac{\Phi(t)}{t^2} \, \mathrm{d}t
= \sum_{k=1}^{\infty} b_k \int_{b_k}^{\infty} \frac{\mathrm{d}t}{t^2}
= \sum_{k=1}^{\infty} 1
= \infty. $$ On the other hand, since $\Phi(t) = \sum_{l=1}^{k} b_l$ for each $t \in [b_k, b_{k+1})$ , \begin{align*}
\int_{0}^{\infty} e^{-\Phi(t)} \, \mathrm{d}t
\geq \sum_{k=1}^{\infty} \int_{b_k}^{b_{k+1}} e^{-\Phi(t)} \, \mathrm{d}t
= \sum_{k=1}^{\infty} (b_{k+1} - b_k) e^{-\sum_{l=1}^{k} b_l}.
\end{align*} So by choosing $(b_{k+1})$ so that it satisfies $(b_{k+1} - b_k) e^{-\sum_{l=1}^{k} b_l} \geq 1$ for each $k$ , we have $$ \int_{0}^{\infty} e^{-\Phi(t)} \, \mathrm{d}t = \infty. $$","['integration', 'analysis']"
4205067,Relationship between $f\left(\bigcap\limits_{i \in I} A_i\right)$ and $\bigcap\limits_{i \in I} f(A_i)$.,"I am trying to investigate and ultimately prove the relationship between $f\left(\bigcap\limits_{i \in I} A_i\right)$ and $\bigcap\limits_{i \in I} f(A_i)$ . The first thing I was able to find is that in general, regardless of the specification of $f$ , we have $f\left(\bigcap\limits_{i \in I} A_i\right) \subset \bigcap\limits_{i \in I} f(A_i)$ . Proof. Let $y \in f\left(\bigcap\limits_{i \in I} A_i\right)$ . Then for each $i \in I$ and corresponding $A_i \in \{A_i\}_{i \in I}$ , there exists $a_i \in A_i$ such that $f(a_i) = y$ . So for each $i \in I$ , $y \in f(A_i)$ , so $y \in \bigcap\limits_{i \in I} f(A_i)$ . Now, if $f$ is injective, then there exists a unique such $a_i$ , call it $x$ , so that $x \in A_i$ for all $i$ , i.e., $x \in \bigcap\limits_{i \in I} A_i$ , so if $y = f(x)$ , $y \in f\left(\bigcap\limits_{i \in I} A_i\right)$ . That gives the opposite inclusion provided that $f$ is injective, but I am not able to prove -- and am not sure if it's possible -- that equality holds if and only if $f$ is injective. In other words, why must this $a_i$ be unique? I could have $f(x) = y$ for some $x \in \bigcap\limits_{i \in I} A_i$ and $f(t) = y$ for some $t \neq x$ . This depends on the domain of the function, of course, since this does have to hold for all $y$ . So I'm having trouble proving the reverse direction, mostly because I don't fully believe that injectivity is a necessary condition, but surely a sufficient condition.","['elementary-set-theory', 'solution-verification']"
4205091,"Showing $F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h)$, not with Taylor series.","The book I am studying, has this question, show $F(x+h)=F(x)+hF'(x)+\frac{h^2}{2}F''(x)+h^2\phi (h)$ where $\phi(h) \to 0$ as $h\to 0$ . It is noted that this is a Taylor expansion, but the method suggested was to use $F(x+h)-F(x)=\int_x^{x+h}F'(y)dy$ and $F'(y)=F'(x)+(y-x)F''(x)+(y-x)\psi(y-x)$ where $\psi(h) \to 0$ as $h\to 0$ . I tried it and am having trouble at the last step. \begin{align*} F(x+h)-F(x)&=\int_x^{x+h}F'(y)dy\\ &= \int_x^{x+h}F'(x)+(y-x)F''(x)+(y-x)\psi(y-x)dy\\ &=hF'(x)+\frac{h^2}{2}F''(x)+\int_x^{x+h}(y-x)\psi(y-x)dy\\&=hF'(x)+\frac{h^2}{2}F''(x)+\int_0^{h}k\psi(k)dk  \end{align*} How do I finish it off, i.e. how do I show $\int_0^{h}k\psi(k)dk=h^2\phi (h)$ . Thanks","['derivatives', 'taylor-expansion']"
4205211,How does group multiplication interact with this intersection of subgroups?,"I'm working through the book Algebra (Revised Third Edition) by Serge Lang. I'm having trouble simplifying terms in one of the exercises (Ch. 1 Ex. 8b). The question goes as follows: Let $G$ be a group and let $H,~H'$ be subgroups. By a double coset of $H,~H'$ one means a subset of $G$ of the form $HxH'$ . Let $\{c\}$ be a family of representatives for the double cosets. For each $a\in G$ denote by $[a]H'$ the conjugate $aH'a^{-1}$ of $H'$ . For each $c$ we have a decomposition into ordinary cosets $$H=\bigcup_{x_c} x_c(H\cap [c]H'),$$ where $\{x_c\}$ is a family of elements of $H$ , depending on $c$ . Show that the elements $\{x_cc\}$ form a family of left coset
representatives for $H'$ in $G$ ; that is, $$G = \bigcup_{c}\bigcup_{x_c}x_ccH',$$ and the union is disjoint. From question 8a, I already know that $$\begin{align*}
G&=\bigsqcup_{c}HcH'\\
 &=\bigsqcup_{c}\big(\bigcup_{x_c}x_c(H\cap(cH'c^{-1}))\big)cH'\\
 &=\bigsqcup_{c}\bigcup_{x_c}x_c(H\cap(cH'c^{-1}))cH'
\end{align*}$$ I know that $Hc=H$ , but I'm stuck: I'm not sure how $x_c$ and $cH'$ interact/distribute with the intersection $H\cap(cHc^{-1})$ and for what reason. Can I get a hint?","['elementary-set-theory', 'group-theory']"
4205238,Insurance statistics book,"Am am looking for a book on insurance statistics. The problem is that there are a lot of googleable variants that it is difficult to decide which one is good and which ones are better in good ones. This is why I am asking here for sugestion. To be short I am looking for a good reference which, if it is possible, describes mathematical tools for borh life insurance and non-life insurance. It would be great if the reference was about statistics and about the liability side of the balance sheet as well as about finance and the asset side of the balance sheet. My math background is intermediate but if the level needed for a prospective reference is indicated, references for various math background will be appreciated. EDIT(27.08): I am not going to pass any exam to work in insurance.
I am asking for text book on mathematical techniques in insurance. And I am expecting for references based on personal preferences for writing style, approach.","['risk-assessment', 'statistical-inference', 'statistics', 'reference-request', 'probability-theory']"
4205241,Green's Theorem Concepts: Circulation in R2,"I am trying understand how circulation density arises in Green's theorem and I'd like to know if my line of thinking is on the right track. Here it goes :). Idea We know that if we have a vector field $\vec F=P\hat x+Q\hat y\in\mathbb{C}^1$ on an open region $D$ containing the simple region $R$ whose boundary is $\Gamma$ , then we can “cut” $\Gamma$ into $K$ positively-oriented rectangular paths such that $\Gamma=\bigcup_{k=1}^K\Gamma_k$ . Consequently, $$\oint_\Gamma \vec F\cdot d\vec r=\sum_{k=1}^K \oint_{\Gamma_k}\vec F\cdot d\vec r $$ which says the the macroscopic circulation of $\vec F$ along the curve $\Gamma$ is equal to the sum of microscopic circulations over the region $R$ enclosed by $\Gamma$ . Moreover, we know that the circulation along one of these rectangles is $$\oint_{\Gamma_k}\vec F\cdot d\vec r=\bigg(\frac{Q(x+\Delta x,\;y)-Q(x,y)}{\Delta x}-\frac{P(x,\;y+\Delta y)-P(x,y)}{\Delta y}\bigg)\;\Delta x\Delta y$$ (for proof, see Green's Theorem in the Plane: Circulation Density ). Hence, the circulation density, or curl, of $\vec F$ at any point $(x,y)$ in $R$ is given by $$\text{curl}\vec F(x,y)=\lim_{\Delta A\rightarrow 0}\frac{1}{\Delta A}\oint_\Gamma \vec F \cdot d\vec r=Q_x-P_y,$$ which is a scalar function. Now, if we let $\sigma (x,y) = Q_x-P_y$ and integrate over the $R$ , we obtain $$\sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y $$ hence, $$\lim_{n,m\rightarrow\infty}\sum_{i=1}^n\sum_{j=1}^m\sigma(x^*_{ij},y^*_{ij})\Delta x\Delta y =\iint_R \sigma(x,y) dxdy=\iint_R (Q_x-P_y )dA$$ whose RHS can be related back to the line integral $\oint_\Gamma\vec F\cdot d\vec r$ giving us what we know as Green's Theorem.","['greens-theorem', 'multivariable-calculus', 'vector-analysis', 'partial-derivative', 'line-integrals']"
4205252,How many integer matrices have spectral radius bounded by a constant?,"How many integer matrices with the spectral radius bounded by a fixed constant are there? Without any restrictions the answer is infinitely many. Indeed, $nJ_0$ , where $J_0$ is the Jordan cell with the eigenvalue $0$ and $n$ is any integer, all have spectral radius $0$ . On the other hand, for symmetric matrices the spectral radius is equal to the spectral norm so there are only finitely symmetric integer matrices with bounded spectral radius. What about other conditions that rule out Jordan cells and other nilpotents? For example, matrices with strictly positive entries, invertible or diagonalizable matrices. I suspect that there are infinitely many invertible or diagonalizable ones, but cannot think of any general construction. The motivation comes from trying to constructively generate large collections of invertible ""random"" integer matrices whose spectral radius stays between $1$ and $2$ (it cannot be less than $1$ ), so that their powers do not explode too quickly. This comes up in encryption. EDIT: For integer matrices with strictly positive entries this is answered in Does small Perron-Frobenius eigenvalue imply small entries for integral matrices? on MathOverflow There are finitely many because the sum of all entries is bounded by the square of the spectral radius.","['matrices', 'spectral-radius', 'linear-algebra']"
4205313,Total average of averages not same as the average of total values,"I am having a strange problem while computing the overall percentage. Let me demonstrate my problem using an example. Assume that I am receiving multiple batches of apple from the vendor and the number of good apple received in each batch are computed to display as percentage as below: If I find the average of the last column (Good Apple Percentage), I get $\frac{(98.5+98+99)}{3} = 98.5 $ % However, if I find the total no of apples and total number of bad apples first (column 2 and 3), Total No of Apples $= 200+100+300 = 600$ Total No of Bad Apples $= 3+2+3= 8$ Then compute the total percentage, $\frac{600-8}{600}= 98.67$ % Why is the total average of the average not the same as the average of total values? There is no rounding up or rounding down necessary for individual percentages (Column 4), so shouldn't the two percentage be the same? What am I missing?","['average', 'rounding-error', 'statistics', 'means']"
4205358,Problems mainly based on a composite function $f(f(x))={f(x)}^3$,"For $f(x)$ which is differentiable over all $\mathbb{R}$ , $f(x)$ meets four conditions (a) $f(4) < 1 $ (b) For all $\mathbb{R}$ , $f(f(x))={f(x)}^3$ (c) if $p < q$ , $0\le f(p) \le f(q)$ (d).  if $p < q$ , $f'(p) + \cfrac{3}{4}p \le f'(q) + \cfrac{3}{4}q$ for all $f(x)$ that meets above conditions, $M(x)$ is the function where $f(4)$ become largest number. (maximum) Find the value of $$\int_{-2}^{2} M(x) dx$$ Source: A Korean hs problem I was struggling to understand what (b) means, and I couldn't figure it out from the start. (detailed questions after some comments) I was mainly curious about the condition (b). Simply thought, $f(x)$ is $0$ , $1$ or $x^3$ But it is not likely, because of the condition (a) I think the problem wanted me to think about the domain and range of $f(x)$ But at that point, I wasn't able to move forward. I don't know how to connect that thought with other conditions.","['integration', 'functional-equations', 'ordinary-differential-equations']"
4205364,Determine Maximum-Likelihood-Estimator without knowing the observed value,"Consider the following distribution with Lebesgue density $$f_{\theta}(x) = \begin{cases} \theta x^{-\theta-1}, \quad \text{if } 1 < x < \infty \\ 0, \quad \text{else} \end{cases},$$ for $\theta \in [a,b]$ with $1 < a < b < \infty$ . Let $r > 1$ be a real number and the observed value $x_1$ be greater than $r$ . However, the value of $x_1$ is unknown. Determine the MLE for $\theta$ . As we don't know the exact value of $x_1$ the classic approach cannot be done, so I determined the probability of a random variable (with the distribution above) being greater than r and that is $r^{-\theta}$ . If we consider this to be the Likelihood function, then the MLE for $\theta$ is going to be the minimum value of the interval for $\theta$ and that means $a$ , as $r^{-\theta}$ is decreasing in $\theta$ . Is this correct or is there another approach which has to be considered?","['statistics', 'maximum-likelihood']"
4205378,Partial derivative of a derivative of a function with respect to itself,"Similar post, but opposite conclusion? I have a function $x(t)$ and its derivative as a function of other variables $\frac{dx}{dt}(x(t))$ (a differential equation?) and I want to find $\frac{\partial }{\partial x}\frac{dx}{dt}$ . The linked post above says this is zero, and I can see where they are coming from swapping the order of the derivatives. However, I just don't believe it, I have likely misunderstood their point. For instance, say $\frac{dx}{dt}=x^2$ . This is easily solvable, e.g. $x=\frac{-1}{t}$ . But $\frac{\partial}{\partial x}\frac{dx}{dt}=2x$ which is not zero in general. Yet $\frac{\partial}{\partial x}\frac{d}{dt}x=\frac{d}{dt}\frac{\partial}{\partial x}x=\frac{d}{dt}1=0$ ? What goes wrong here? EDIT: is it because $x$ is a function of $t$ ? So we cant swap them?","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
4205392,Homeomorphism from the suspension of $S^{n-1}$ to $S^n$,"Let $S(X) = (X \times [-1,1])/R$ denote the suspension of $X$ , where the classes of $R$ are $X \times \{1\}, X \times \{-1\}$ and all singletons $\{a\}$ , where $a \in X \times (-1,1)$ . Show that $S(S^{n-1})$ is homeomorphic to $S^n$ . (Don’t confuse $S$ and $S^n$ , $S$ is the suspension and $S^n$ is the unit sphere). So I want to find an cont bijection with continuous inverse $f:S(S^{n-1}) \to S^n$ and since every output of $f$ must be of length one I’m thinking about $f(x) = \frac{x}{\|x\|}$ which would satisfy the conditions of homeomorphism, but I’m not sure if this works? Also should is it neccessarily defined in all of $S(S^{n-1})$ ? I don’t think $S(S^{n-1})$ contains $\vec{0}$ ?",['general-topology']
4205396,How to solve $\tan^2x + \tan^2{2x} + \cot^2{3x} =1$ [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 2 years ago . Improve this question How to solve $\tan^2x + \tan^2{2x} + \cot^2{3x} =1$ ? I've tried to use some formulas and I've written that but it didn't make much easier. The derivative is  ugly too. Mayby there is a simple way?",['trigonometry']
4205407,Donsker and Varadhan inequality proof without absolute continuity assumption,"I've been attempting to understand the proof of the Donsker-Varadhan dual form of the Kullback-Liebler divergence, as defined by $$
\operatorname{KL}(\mu \| \lambda)
= \begin{cases}
\int_X \log\left(\frac{d\mu}{d\lambda}\right) \, d\mu, & \text{if $\mu \ll \lambda$ and $\log\left(\frac{d\mu}{d\lambda}\right) \in L^1(\mu)$,} \\
\infty, & \text{otherwise.}
\end{cases}
$$ with Donsker-Varadhan dual form $$
\operatorname{KL}(\mu \| \lambda)
= \sup_{\Phi \in \mathcal{C}} \left(\int_X \Phi \, d\mu - \log\int_X \exp(\Phi) \, d\lambda\right).
$$ Many of the steps in the proof are helpfully outlined here: Reconciling Donsker-Varadhan definition of KL divergence with the ""usual"" definition , and I can follow along readily. However, a crucial first step is establishing that (for any function $\Phi$ ) $$\tag{1}\label{ineq}
\operatorname{KL}(\mu\|\lambda)\ge \left\{\int \Phi d\mu-\log\int e^{\Phi}d\lambda\right\},$$ said to be an immediate consequence of Jensen's inequality. I can prove this easily in the case when $\mu \ll \lambda$ and $\lambda \ll \mu$ : $$ \operatorname{KL}(\mu\|\lambda) - \int \Phi d\mu = \int \left[ -\log\left(\frac{e^{\Phi}}{d\mu / d\lambda}\right) \right] d\mu \ge -\log \int \frac{e^{\Phi}}{d\mu / d\lambda} d\mu = -\log\int\exp(\Phi)d\lambda.$$ However, this last step appears to crucially rely on the existence of $d\lambda/d\mu$ and thus that $\lambda \ll \mu$ , which isn't assumed by the overall theorem. Where I have been able to find proofs of the above in the machine learning literature, this assumption seems to be implicitly made, but I don't believe it is necessary and it is very restrictive. My question is: how can we prove \ref{ineq} without assuming $\lambda \ll \mu$ ?","['machine-learning', 'measure-theory', 'probability-theory', 'information-theory']"
4205433,What is the geometrical importance of the Euler Line?,"What is the geometrical importance of the Euler Line (ie, the line through the centroid, orthocenter, and circumcenter (and other points) of a non-equilateral triangle)? What is meant by importance is that --- What role it plays in mathematics... Why is the Euler Line worthy of attention Why is it valuable in geometry Why should  some more special points also lie on this magical Euler line – the Exeter point, the nine-point centre, and the Schiffler point For example: $(a+b)^2$ Though it is very small compared to the vast field of mathematics, we all know how important it is and it would be a nightmare without its existence. I hope that I was able to make clear what I wanted to ask. If there is any confusion please ask😊. Anyone who sees this question, feel free to share what you think about this. And upvote it if you are curious about it too but don't know the answer.","['euclidean-geometry', 'triangles', 'triangle-centres', 'geometry']"
4205435,A question on proving that $e$ is irrational,"This is a four part question on proving that $e$ is irrational that I've attempted i)  Show that $\int_0^1{x^{\alpha}e^x}dx<\frac{3}{\alpha+1}$ , you may assume $e<3$ ii) Prove using induction that there exists, for all $n=0,1,2,3,\dots$ , integers $a_n$ and $b_n$ such that $\int_0^1{x^{n}e^x}dx=a_n+b_ne$ iii) Suppose that $r$ is a positive rational in the form of $\frac{p}q$ where $p$ and $q$ are both positive integers. Show that, for all integers $a$ and $b$ , that either $|a+br|=0$ or $|a+br|≥\frac1q$ . iv) Prove that $e$ is irrational. I have shown parts i) and ii). Can someone please explain how to do iii) and and double check the following proof for iv) which I think is faulty. Assume that $e$ is rational, i.e $e=\frac{p}{q}$ where $p,q\in\mathbb{Z}$ and where the HCF of $p$ and $q$ is 1. Using the result from part ii) and iii) $\int_0^1{x^{n}e^x}dx=0$ or $\int_0^1{x^{n}e^x}dx≥\frac1q$ . For $\int_0^1{x^{n}e^x}dx=0$ , this statement is false as $x^ne^x>0$ for all $0≤x≤1$ , therefore there is always area under the curve $f(x)=x^ne^x$ between the limits of integration. For $\int_0^1{x^{n}e^x}dx≥\frac1q$ , using the result in part i), $\frac{3}{n+1}>\frac1q$ Hence $q>\frac{n+1}{3}$ . Since in part ii), the statement was shown to be true for all $n$ then $q>\frac{n+1}{3}$ should be true for all $n=0,1,2,\dots$ . However, when $n\to\infty$ that means $q>\infty$ , which is a contradiction since $q\in\mathbb{Z}$ . Therefore there exists no $e$ such that $e=\frac{p}{q}$ where $p,q\in\mathbb{Z}$ , hence $e$ is irrational by proof by contradiction.","['number-theory', 'irrational-numbers', 'proof-writing', 'solution-verification']"
4205469,$O$ is the circumcenter of non-right $\triangle ABC$. $\frac{|AB \cdot CO|}{|AC \cdot BO|} = \frac{|AB \cdot BO|}{|AC \cdot CO|} = 3$. Find $\tan A$,"Problem: $O$ is the circumcenter of $\triangle ABC$ , which is not a right triangle. $$\frac{| AB \cdot CO |}{|AC \cdot BO|} = \frac{|AB \cdot BO|}{|AC \cdot CO|} = 3$$ . Find $\tan A$ . Here $AB \cdot CO$ represents the dot product of vector $\overrightarrow{AB}$ and $\overrightarrow{CO}$ My thoughts: WLOG let the radius of the excircle be $1$ . Let $AB = c$ , $BC = a$ , $AC 
= b$ . Then $AB \cdot CO = c \cdot 1 \cdot \cos( 90^{\circ} + \angle B- \angle A ) = -c \cdot \sin( \angle B- \angle A)$ $AC \cdot BO = -b \cdot \sin( \angle C- \angle A)$ $AB \cdot BO = c \sin C$ $AC \cdot CO = b \sin B$ But none of these look very trivial to solve..","['contest-math', 'algebra-precalculus', 'trigonometry', 'plane-geometry']"
4205494,Computing $\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx$.,"I am interested in $$ \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n) dx = \underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx, $$ where $q\in(0,1)$ . Numerical computation suggests that it is 0. My approach so far has been to express the second factor of $f(x,n)$ as a Gauss hypergeometric function. $$\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = {}_1F_0\left((1-n);(1-q)x^{\frac{1}{1-n}}\right)$$ $$ \int x^\frac{1}{1-n}{}_1F_{0}\left((1-n);(1-q)x^\frac{1}{1-n}\right)dx = {}_2F_{1}\left((1-n);(2-n);(3-n);(1-q)x^\frac{1}{1-n}\right) $$ I would appreciate any hint, how to compute the limit. Thank you. Edit: Based on the suggestions below, I apply the Dominated Convergence theorem: $$\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 f(x,n)dx = \int_{0}^1 \underset{n\rightarrow+\infty}{\lim}f(x,n)dx. $$ $\underset{n\rightarrow+\infty}{\lim} x^{\frac{1}{1-n}} = 1$ and $\underset{n\rightarrow+\infty}{\lim} \left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1} = 0$ . Thus, I conclude that $$\underset{n\rightarrow+\infty}{\lim} \int_{(1-q)^{n-1}}^1 x^{\frac{1}{1-n}}\left(1-(1-q)x^\frac{1}{1-n}\right)^{n-1}dx = 0.$$","['integration', 'limits', 'calculus', 'hypergeometric-function']"
4205504,How to tell $\overline{Z(f)\cap V}=V$?,"Say $V$ is an irreducible variety in $\mathbb{C}^n$ of dimension $d$ . I can view $\mathbb{C}^n$ as $\mathbb{R}^{2n}$ and inside $\mathbb{R}^{2n}$ , $V$ is an irreducible (real) variety of dimension $2d$ . If I pick a polynomial $f\in\mathbb{R}[x_1,\dots,x_{2n}]$ , there are 2 options: $\overline{Z(f)\cap V}=V$ (this also includes the option $V\subset Z(f)$ ) $\dim\overline{Z(f)\cap V}<\dim V$ . Here, $\overline{Z(f)\cap V}$ denotes the Zariski closure inside $\mathbb{C}^n$ . I guess that for a generic $f$ , we have $\overline{Z(f)\cap V}=V$ since the intersection has dimension $2d-1$ (as a real variety) so its closure has dimension at least $d$ . I want to know for which polynomials $f$ the second option applies. Say I restrict to all polynomials of degree at most $k$ . Do the polynomials satisfying the second option form a variety inside $P_k=\{f\mid \deg(f)\leq k\}$ ? If yes, what are the defining polynomials of this variety? As an example, I consider $Z(x-y)\subset\mathbb{C}^2$ . Viewed as a real variety, I have $V=Z(x_1-y_1,x_2-y_2)\subset\mathbb{R}^4$ . If $f$ is a real polynomial in variables $x_1,x_2,y_1,y_2$ , I have $\overline{Z(f)\cap V}\neq V$ if and only if $Z(f)\cap V$ is finite. And this happens if and only if $f(x_1,x_2,x_1,x_2)$ has only finitely many zeros in $\mathbb{R}^2$ . For example $f$ might be of the form $$
f = x_1^2+x_2^2+y_1^2+y_2^2
$$ so $Z(f)\cap V=\{0\}$ .",['algebraic-geometry']
4205530,why $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n?$ Rudin RCA- Lusin's theorem,"I have some  confusion regarding Rudin RCA book, page number $55$ Lusin's  theorem  : Suppose $f$ is a complex   measurable  function on $X$ . $\mu(A) < \infty$ , $f(x)= 0$ if $x \notin A $ and $\epsilon >0$ .The  there  exists a $g \in C_c(X)$ such that $ \mu(\{x : f(x) \neq g(x)\} <\epsilon$ In the theorem of the proof it  is written that By Urysohn's lemma ,there are function $h_n$ such that $K_n\prec h_n \prec V_n.$ Define $$g(x) =\sum_{n=1}^{\infty}2^{-n}h_n(x) $$ for all $ x \in X$ This series converge uniformly on X , so $g$ is contnious.Also ,the support of $g$ lie in $\bar V$ . since $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n$ My confusion : why $2^{-n}h_n(x)=t_n(x)$ except in $V_n- K_n?$ why not $2^{-n}h_n(x)=t_n(x)$ except in $X- V_n?$ My thinking : $h_n(x)= 2^nt_n(x)=\begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \notin V_n  \end{cases} = \begin{cases} 1 \ \text{if  x} \in V_n \\ 0 \ \text{if  x} \in  X- V_n  \end{cases}$ So  I think it should be $2^{-n}h_n(x)=t_n(x)$ except in $X- V_n$","['measure-theory', 'lebesgue-measure', 'analysis', 'measurable-sets', 'measurable-functions']"
4205538,A question regarding Differential Equation,"$f'(x) = 1+f(x) \implies \frac{df(x)}{1+f(x)} = dx$ Now on integrating both sides we get $f(x) = e^{x+c} -1$ So $f(x) = e^{x+c}  -1$ for all $x \in \mathbb R$ This is done in my text book. But I can not understand why they did not care about $1+f(x) $ being $0$ . I would rather prefer to do the following. Let's supose $1+f(x) \neq 0$ when $x\in A$ . So we can say $f(x) = e^{x+c}  -1$ when $x \in A$ by using the previous method. And $f(x) = -1$ when $x \in \mathbb R -A$ Now as $\mathbb R$ is a connected set and $f(x)$ is continuous , we can say either $A= \mathbb R$ or $B= \mathbb R$ . Can anyone please tell me if I have gone wrong anywhere?","['ordinary-differential-equations', 'real-analysis']"
4205637,Unifying theorem for the turning number and the Gauss-Bonnet theorem,The statement that the turning number of a planar curve is a multiple of $2\pi$ and the Gauss-Bonnet theorem are similar in that they both state that the integral of a curvature measure depends on the topology but not the shape of the manifold. Is there a theorem that unifies the two statements that yields the former for $n=1$ and the latter for $n=2$ ? What does it say for $n=3$ ?,"['surfaces', 'curves', 'curvature', 'manifolds', 'differential-geometry']"
4205663,Can a critical point found by Lagrange multipliers method be a saddle point?,I am trying to find the extrema of a function on a particular set. I used the Lagrange multiplier method and found 2 points. Now I want to classify those points. Should I just put them in the function and determine which one is maximum and which one is minimum according to the function value at those 2 points? Or is there a possibility that one of them is actually a saddle point?,"['multivariable-calculus', 'lagrange-multiplier']"
4205705,Galois group of $x^5-5x^3+4x+1 \in \mathbb{Q}[x] $,"Can the Galois group of $x^5-5x^3+4x+1 \in \mathbb{Q}[x] $ be isomorphic to $S_5$ ? I know that it has five real roots, then I think that it is impossible. I think that this Galois group cannot contain a 2-cycle. Any idea about how I can prove this?","['field-theory', 'galois-theory', 'extension-field', 'galois-extensions', 'group-theory']"
4205708,Divergence is coordinate independent,"I am working on a project about Spectral Geometry. One of the main goals of the project is to be able to define the Laplacian on a Riemannian Manifold. As such, Let $(M,g)$ be a Riemannian Manifold, then the Laplacian is given by $\triangle_g= -\operatorname{div}_g\circ \operatorname{grad}_g$ . Given a vector field X on M, we define the divergence as being $$d(\iota_X \omega)=(\operatorname{div}_g X) \omega,$$ where $\omega$ is the volume form and $\iota_X \omega(Y_1, ..., Y_{n-1}) = \omega (X, Y_1, ..., Y_{n-1})$ . I have already shown that given coordinates $x_1,...,x_n$ then we can write the divergence of $X=\sum_{i=1}^n X_i \frac{d}{dx_i}$ as: $$\frac{1}{\sqrt{\det g}}\sum_{i=1}^n\frac{\partial}{\partial x_i}(X_i\sqrt{\det g })$$ I can clearly see that this definition $d(\iota_X \omega)=(\operatorname{div}_g X) \omega$ is independent of the choice of coordinates system. However, if instead of been given that first definition I was only given the last one, how can I prove the independence? I have tried this: Let $x_i$ and $y_i$ be two charts of $M$ . Let $g$ be the metric of $M$ with respect to the coordinates $x_i$ and $\widetilde{g}$ the one with respect to $y_i$ . Then thinking in terms of matrices we have that $\widetilde{g}=J^t g J$ , where $J$ is the jacobian associated with the change of variables from $x_i$ to $y_i$ so we get that: $$\sqrt{\det(\widetilde{g})}=\sqrt{\det (g)} \times \det (J)$$ Let $X=\sum_{i=1}^n X_i \frac{d}{dx_i}$ be a vector field in $x_i$ coordinates then it can be written as $\sum_{i,a=1}^n X_i \frac{dy_a}{dx_i}\frac{d}{dy_a}$ so let $ X_i \frac{dy_a}{dx_i}= Y_i$ .Then , \begin{align*}
    \frac{1}{\sqrt{\det \widetilde{g}}}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det \widetilde{g}})&=\frac{1}{\sqrt{\det g} \det J}\sum_{i=1}^n \frac{\partial}{\partial y_i}(Y_i\sqrt{\det g} \det(J))\\
    &=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\frac{\partial y_a}{\partial x_i}\sqrt{\det g}\det(J))\\
&=\frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n \det(J)\frac{\partial y_a}{\partial x_i} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(X_i\sqrt{\det g})+ \frac{1}{\sqrt{\det g} \det J}\sum_{i,a=1}^n X_i\sqrt{\det(g)} \frac{\partial x_a}{\partial y_i}\frac{\partial}{\partial x_a}(\frac{\partial y_a}{\partial x_i}\det J)\\
\end{align*} Now I am stuck and not sure how to proceed. On the left side, I have what I want but on the right side, I don't know how to proceed. Any ideas?","['laplacian', 'divergence-operator', 'riemannian-geometry', 'differential-geometry']"
4205730,If $P(x)$ be a real polynomial such that $\lvert P(x)\rvert$ is also a polynomial then what can we conclude about $P(x)$?,"My thinking is this:-
If it is a constant polynomial then it is trivial. If it has degree $\geq 1$ . Then the polynomial must be differentiable. Now for it to be differentiable it should lie totally on the upper half of the $xy-$ plane or the lower half . Well it can have roots,but we will consider to lie on the upper half or lower half regardless. Considering geometrically, if the curve had a portion in the lower half and a portion on the upper half...then the function $\lvert P(x)\rvert$ would become non-differentiable at the root on account of it having ""sharp edges"" there.
Is my thinking correct? Did I miss anything? Can I conclude anything else?.
Any help is appreciated.","['derivatives', 'polynomials', 'real-analysis']"
4205733,Are all derivations of a matrix algebra inner derivations?,"Let $\mathcal{A}$ be a subalgebra of $M_n(\mathbb{R}).$ Let $$d:\mathcal{A} \to \mathcal{A}$$ be a linear map satisfying the Leibnitz rule $$d(XY)=d(X)\cdot Y-X\cdot d(Y)$$ for all $X,Y \in \mathcal{A}$ Is it true that $$d(X)=AX-XA$$ for some $A \in M_n(\mathbb{R})?$ In case $\mathcal{A}=M_n(\mathbb{R})$ the statement is well known to be true.","['matrices', 'linear-algebra', 'lie-algebras']"
4205737,Why and when can I just peacefully substitute into $\tan^{-1}x+\tan^{-1}y=\tan^{-1}\frac{x+y}{1-xy}$ without checking range conditions?,This is an example question in my book: To solve for $x$ : $$\tan^{-1}\frac{x-1}{x+2}+\tan^{-1}\frac{x+1}{x+2}=\frac{\pi}{4}$$ and it is solved by a direct formula given by $$\tan^{-1}x+\tan^{-1}y=\tan^{-1}  \left(\frac{x+y}{1-xy}\right) $$ when $xy<1$ . The book uses this formula without checking conditions many many times and it is mildly infuriating. It is alright when checking the condition is simple but in this case finding the range of $\dfrac{x^2-1}{(x+2)^2}$ is a little time consuming and has the square root of 21 in it. Well my question is: Why and when can I just peacefully substitute the formula without worrying about the condition as the book leisurely does so?,"['algebra-precalculus', 'trigonometry']"
4205765,On a better solution for $\large {\int_{-x}^x\frac{dx}{\sqrt {1-x^2} e^{bx^2+ax}}}$,"In this user’s question I was able to find out the following. It is for a bigger physics research problem having to do with comparing a Wigner function corresponding to a state of light for the user named @Jayanth Jayakumar. Although, I know almost nothing on this subject, I still find the result very interesting. In case you do not believe the results, here is graphical proof for each step. I would like to support both of us, but I am more interested in finding out a way to significantly simplify the result below: $$\mathrm{\int_{-x}^x\frac{dx}{\sqrt {1-x^2} e^{bx^2+ax}}=\quad \frac12 \sum_{n=0}^\infty\sum_{m=0}^n\frac{b^ma^{n-m}sgn\left(x^{m+n+1}\right)\left[(-1)^m+(-1)^n\right]B_{x^2}\left(\frac{m+n+1}{2},\frac12\right)}{m!(n-m)!}=\quad \sum_{n=0}^\infty\sum_{m=0}^n\frac{b^ma^{n-m}k^{m+n+1}\left[(-1)^m+(-1)^n\right]\,_2F_1\left(\frac12,\frac{m+n+1}{2}, \frac{m+n+3}{2},k^2\right)}{m!(n-m)!(m+n+1)}=\quad \sum_{n=0}^\infty\sum_{m=0}^n\frac{b^{2m}a^{2(n-m)}sgn\left(x^{2(m+n)+1}\right)B_{x^2}\left(\frac{2(m+n)+1}{2},\frac12\right)}{(2m)!(2(n-m))!-}\quad -\sum_{n=0}^\infty\sum_{m=0}^n\frac{b^{2m+1}a^{2(n-m)}sgn\left(x^{2(m+n)+3}\right)B_{x^2}\left(\frac{2(m+n)+3}{2},\frac12\right)}{(2m+1)!(2(n-m))!}, }$$ For the full area under the function, set $x=\pm1$ : $$\mathrm{\int_{-1}^1 \frac{e^{-bx^2-ax}}{\sqrt{1-x^2}}dx=\quad\frac{\sqrt\pi}{2}\sum_{n=0}^\infty\sum_{m=0}^n\frac{b^ma^{n-m}sgn\left(x^{m+n+1}\right)[(-1)^m+(-1)^n]}{m!(n-m)!} \left(\frac{m+n-1}2\right)^{\left(-\frac12\right)},B_1\left(\frac{m+n+1}2,\frac12\right)= B\left(\frac{m+n+1}2,\frac12\right)= \frac{\sqrt\pi\left(\frac{m+n-1}2\right)!}{\left(\frac{m+n}2\right)!}= \sqrt\pi\left(\frac{m+n -1}2\right)^{\left(-\frac12\right)}}$$ This problem uses a double series , the link has a way to turn it into a single sum, Incomplete Beta function properties , hypergeomtric function values , falling factorial $x^{(y)}$ and the Wigner function . An exact solution is needed, so how can one simplify or evaluate this series? I said before, that the special case was a goal, but now the main goal is simplifying the general case or integrating another way. Please correct me and give me feedback! Here is a single sum solution inspired by @Harry Peter and @James Arathoon. It uses the Lower Incomplete Fox-Wright function and Central Binomial Coefficient : $$\displaystyle\begin{align}\displaystyle\int_{-t}^t\frac{dx}{\sqrt {1-t^2} e^{bt^2+at}}=\sum _{n=0}^{\infty } \frac{(2 n)! }{2^{2 n} (n!)^2}\sum _{m=0}^{\infty } \frac{a^{2 m} }{(2 m)!} b^{-\big(m+n+\frac{1}{2}\big)} \gamma\left(m+n+\frac{1}{2},bt^2\right)=\\ b^{-\frac12}\sum _{n=0}^{\infty } \binom {2n}n (4b)^{-n}\,_2Ψ^{(\gamma)}_1\left[\,^{\left(1,n+\frac12,bt^2\right),(1,1)}_{\quad \quad (1,2)}\ \frac{a^2}b\right]\end{align}$$ which is super close to this would be closed form of equation $2.27$ in this Korea Science article . If the coefficients of the factorials of $n$ were the same in the denominator and numerator, also for $m$ , then we would technically have a closed form.","['integration', 'summation', 'beta-function', 'algebra-precalculus', 'mathematical-physics']"
4205788,Question about projection onto subspaces,"Suppose I have an $n\times n$ (complex) matrix $Q$ viewed as a linear transformation from $\mathbb{C}^{n}$ to $\mathbb{C}^{n}$ . I'm struggling to understand the difference between the following two scenarios. For simplicity, let us consider $n = 4$ . Scenario 1: Let $S$ be a subspace of $\mathbb{C}^{4}$ of dimension $d=2$ . Let us think of $S$ as the subspace generated by two eigenvectors $v_{1}$ and $v_{2}$ of $Q$ .One can construct a projection matrix $P_{S}$ . According to this post , this projection should be given by: $$P_{S} = A(A^{T}A)^{-1}A^{T}$$ where $A = [v_{1} \hspace{0.1cm} v_{2}]$ the a $4\times 2$ matrix. This is a $4\times 4$ matrix . Scenario 2: Let us write $\mathbb{C}^{4} = S \oplus S^{\perp}$ . Since $Q$ is a linear operator on $\mathbb{C}^{4}$ , it might be possible to decompose $Q = Q_{1}\oplus Q_{2}$ so that, if $u = u_{1}+u_{2}$ and $u_{1}\in S$ and $u_{2} \in S^{\perp}$ : $$Qu = Q_{1}u_{1}\oplus Q_{2}u_{2}$$ Each $Q_{i}$ should be a $2\times 2$ matrix. Here is my problem. I would expect that $Q_{1} = QP_{S}$ , since $Q_{1}$ should be the restriction of $Q$ to $S$ . However, as I stressed before, both $Q$ and $P_{S}$ are $4\times 4$ matrices. So, where is the gap? If $Q_{1}$ is not $QP_{S}$ , how can I obtain $Q_{1}$ ? And finally, why is $P_{S}$ a $4\times 4$ matrix if it projects into a subspace of dimension $2$ ? (I would expect it to be a $2\times 4$ matrix instead).","['matrices', 'matrix-calculus', 'linear-algebra', 'matrix-decomposition']"
4205797,"If $K=\mathbb{Z} / p\mathbb{Z}$ and $L$ is the splitting field of $\prod_{i=1} ^{p-1} (t^2 -i)$ for odd prime $p$, find the Galois group","If $K=\mathbb{Z} /p \mathbb{Z}$ and $L$ is the splitting field of $\prod_{i=1} ^{p-1} (t^2 -i)$ for odd prime $p$ , find the Galois group. My proof: Since $\left| \{ i^2 \mid i=1,...,p-1 \} \right| = \frac{p-1}{2}$ ,  there is some $i_0 \in \{1,...,p-1\}$ which is not a square in $\mathbb{Z}/p \mathbb{Z}$ So $ t^2-i_0$ is irreducible over $K=$ $\mathbb{Z}/ p\mathbb{Z}.$ A splitting field of $ t^2-i_0$ over $\mathbb{Z}/ p\mathbb{Z}$ is $\mathbb{F}_{p^2}$ Since any irreducible polynomial of degree 2 over $\mathbb{Z}/p \mathbb{Z}$ splits in $\mathbb{F}_{p^2}$ , $t^2 -i$ splits in $\mathbb{F}_{p^2}$ for every $i \in \{1,...,p-1\}$ Hence $L$ is $\mathbb{F_{p^2}}$ and its galois group $ {\rm Gal}(L/K)=\langle{\rm frob}_p\rangle = \mathbb{Z}/2 \mathbb{Z}$ where ${\rm frob}_p$ is a Frobenius map for $p$ I am not sure if my proof is right. Please feel free to leave any comments here, if the proof is not right, or you have simpler proof.","['galois-theory', 'group-theory', 'solution-verification']"
4205813,"In general, for a graph of solutions to a differential equation is there a 3D graph s.t. these curves are its level curves? Then, what's this graph?","I've noticed that the solution curves to a differential equation look like the level curves to some 3D graph. E.g., for the system of equations: $$\begin{cases}x_1' = \dfrac{1}{10}x_1 \\[1mm]
x_2' = -\dfrac{1}{2}x_2\end{cases}$$ with e.g. a solution given by a parametrized curve: $(k_1e^{\frac{1}{10}t}, k_2e^{-\frac{1}{2}t})$ , with some of the solution curves plotted: It seems to me like there should be some three dimensional graph whose level curves would be precisely these solution curves. Is there any way to prove this in general, or to find a general way to find what is this 3D graph? Any reading recommendation is also appreciated!","['parametrization', '3d', 'ordinary-differential-equations', 'reference-request']"
4205841,"Why can the divergence be defined equivalently as ${\rm div}(X)(x)=\sum_j \langle\nabla_j X,e_j\rangle_x$ and $d(i_X dV)=({\rm div}X)dV$?","Let $M$ be an oriented Riemannian manifold with volume form $dV$ , and let $X$ be a smooth vector field on $M$ . Recall that the divergence of $X$ is characterized by the formula $d(i_X dV)=(\text{div}X) dV $ (where $i_X$ is the interior multiplication by $X$ .) In p.115 of the book Spin Geometry (Lawson, Michelson), it is written that, at $x\in M$ , $\text{div}(X)(x)=\sum_j \langle \nabla_{e_j} X,e_j\rangle_x$ , where $e_1,\dots,e_n$ is an orthonormal frame of $TM$ . Are these two definitions equivalent? I can't see why..","['divergence-operator', 'riemannian-geometry', 'differential-geometry']"
4205868,What is $p$-value? [duplicate],"This question already has answers here : Understanding the P-value (5 answers) Closed 2 years ago . Tell me about this $p$ -value in relatively simple but not too simple terms. I've been reading many academic publications of late and come across it quite a few times without having a clear idea what it means. It's something about probability, isn't it? If it's less than, I believe, $0.05$ , then it's, whatever it is, not a simple coincidence but a real correlation, right?","['p-value', 'probability']"
4205910,Must the norm of a character be an integer?,"I am working on a problem and I was very tempted to take for granted that for any complex character $\chi$ of a finite group $G$ , the norm $$\langle \chi, \chi \rangle = \frac{1}{|G|}\sum_{g\in G} \chi(g)\chi( g^{-1})$$ Is an integer. I have pondered this for awhile and I don't see a reason why it is necessarily true, but nontheless I have never seen any inner product of characters be anything but an integer. My initial thought was that any character is the integral sum of irreducible characters, so the norm will be a sum of square integers, but that isn't sufficient Am I missing something obvious here? If this is not true, can someone provide an explicit example for my repertoire of examples?","['characters', 'abstract-algebra', 'representation-theory']"
4205914,Find the number of real roots $x$ such that $\frac{x}{x+4} = \frac{5[x]-7}{7[x]-5}$,"Here $[.]$ represents the greatest integer function Let $x=[x] + \{x\}= t+u$ Here t is GIF and u is fractional part function $$\frac{t+u}{t+u+4} = \frac{5t-7}{7t-5}$$ $$u=-\frac{2t^2 -18t +28}{9t+2}$$ So now $$0\le u<1$$ Solving the first inequality gives $$t\in (-\infty,-\frac 29)\cup [2,7]$$ The second one gives $$t>-\frac 29$$ So clearly $[x] \in [2,7]$ So $$x\in [2,8)$$ This is giving infinite real roots but we need a finite number. Also only 2 and 7 seem to satisfy the problem. What is wrong with this solution?",['functions']
