question_id,title,body,tags
3987396,"Determining all triples $(a,b,c)$ of positive integers that are sides of a triangle inscribed in a circle of diameter $6.25$","An Olympiad Geometry question Determine all triples $(a, b, c)$ of positive integers which are the lengths of the sides of a triangle inscribed in a circle of diameter $6.25$ units. So, in this, basically diameter = 2R = $\frac{a}{sinA}$ and so on. Or we also have R = $\frac{abc}{4\Delta}$ . But I don't understand how this may help in this problem. Any hint/ solution is appreciated, Thanks!","['contest-math', 'trigonometry', 'geometry']"
3987446,There are 100 towns in some country and each two towns are connected by a one-way road.,"There are 100 towns in some country and each two towns are connected by a one-way
road.
we are to Prove that one can change the direction of at most one road so that after that
each town will be reachable from any other one. please help with the solution, thanks.","['graph-theory', 'combinatorics', 'discrete-mathematics', 'permutations']"
3987477,Prove that limit doesn't exists at all,"I have a function $f(x)=\frac{1}{x}+\sin(\frac{1}{x})$ which I have proved that $\lim_{x \to0^+}f(x)=\infty$ , and I need to prove that $\lim_{x \to0^+}f'(x) \ne -\infty$ . I have got that $f'(x)=-\frac{1}{x^2}-\frac{\cos(\frac{1}{x})}{x^2}$ I am trying to prove that: $$\lim_{x \to 0^+}-\frac{1}{x^2}-\frac{\cos(\frac{1}{x})}{x^2}$$ Does not equal to $-\infty$ (or to prove that it doesn't exists at all). I have tried to assume by contradiction that $$\lim_{x \to 0^+}-\frac{1}{x^2}-\frac{\cos(\frac{1}{x})}{x^2}=-\infty$$ So there exists $\delta_{1}>0$ such that for every $0<x<\delta_{1}$ we say that $$f(x)<\square $$ We choose $\delta=\delta_{1}$ , and let $0<x<\delta$ so: $$f(x)=-\frac{1}{x^2}-\frac{\cos(\frac{1}{x})}{x^2}=-\frac{1}{x^2}(1+\cos(\frac{1}{x}))\le -\frac{2}{x^2}<\frac{2}{\delta^2}=k$$ I do not know what to choose instead of $\square$ , and if what I did is correct. Appreciate your help! I have also tried to think about the continuity of $f$ , and saying that it is not bounded so it can't be continuous, so the limit doesn't exists. Thanks a lot!","['proof-writing', 'calculus', 'solution-verification', 'limits', 'derivatives']"
3987479,Finding convolution of two functions,"A common engineering notational convention is: wikipedia ${\displaystyle f(x)*g(x)\,:=\underbrace {\int_{0}^{x}f(\tau )g(x-\tau )\,d\tau } _{(f*g)(x)}.}$ I want to write the following expression as the convolution of two functions. In the other words;
what is the function $g(x)$ in the definition for the integral below: \begin{equation}
\frac{(\rho+1)^{1-\alpha}}{\Gamma(\alpha)} \int_{a}^{x}\left(x^{\rho+1}-\tau^{\rho+1}\right)^{\alpha-1} \tau^{\rho} f(\tau) \,d \tau
\end{equation} where $\alpha$ and $\rho \neq-1$ are real numbers and $x > a$ , $\Gamma$ is gamma function . My Try: Let \begin{align}
k&=\tau^{\rho+1}\\
dk&=(\rho+1)\tau^{\rho}\,d \tau \implies \frac{dk}{(\rho+1)}=\tau^{\rho}\,d \tau.
\end{align} Substituting the last equality to the integral in the question, we have \begin{equation}
\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{a^{\rho}}^{x^{\rho}}\left(x^{\rho+1}-k\right)^{\alpha-1} f(k^{1/(\rho+1)})\, dk
\end{equation} And then? P.S. If the last expression was \begin{equation}
\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} \int_{0}^{x}\left(x-k\right)^{\alpha-1} f(k)\, dk
\end{equation} $g(x)$ would be as follows: $$g(x)=\frac{(\rho+1)^{-\alpha}}{\Gamma(\alpha)} x^{\alpha-1}.$$","['integration', 'convolution', 'analysis', 'complex-analysis', 'mathematical-physics']"
3987490,Partition the remaining rectangle into equal parts.,There’s a rectangle. A small rectangle is cut from the bigger rectangle ( not necessarily from the center). How will you partition the original rectangle after removing the cut such that the remaining areas of both the partitions are the same? Here I was thinking to partition it by extending the diagonal line of the cut part. Is it correct and Is it the best approach ?,"['rectangles', 'puzzle', 'area', 'geometry']"
3987547,A transformation for independent variables to be dependent,"I ran into a confusing question. If two variables are independent, maybe they will be dependent after linear transformation. How it can happen? Is it possible for independent variables? What is the operation that it makes variables to be dependent? In my opinion the transformation that maps all variables to a point is true for this fact. What is wrong with my answer? The mean of Variables are features of data.","['independence', 'probability']"
3987578,Help with: $\tan2θ+ 2\tanθ= 3\cotθ$ for $0\le θ\le 180$,This is a trigonometry question I have asked everyone I know and no one seems to be able to solve it . Any hits or steps on how to get to the answer I appreciate it! Thank you!,['trigonometry']
3987631,Proving $1^k+2^k+ \dots + n^k = O(n^{k+1})$,"I have to prove that : $1^k+2^k+ \dots + n^k = O(n^{k+1})$ , where $k \in \mathbb{Z}^+$ . I followed a technique different from the book , and I want to check if my way is correct. I approach the problem using Induction. Let $P(n)$ be the statement "" $1^k+2^k+ \dots + n^k = \sum_{i = 1}^ni^k =  O(n^{k+1})$ "". Induction basis: I will show that $P(1)$ is true. That equals: $1^k = O(1^{k+1})$ . In other words, I have to prove that $\lim_{x\to \infty}\frac{1^x}{1^{x+1}} = L \geq 0$ . $\lim_{x\to \infty}\frac{1^x}{1^{x+1}} = \lim_{x\to \infty}\frac{1^x}{1^{x}1^1} = 1 \in \mathbb{R}$ so the Induction basis is proved . Induction step: Let $P(l)$ be true, for some $l>1$ . I will prove that $P(l) \rightarrow P(l+1)$ . As $P(l)$ is true, that means that $\sum_{i=1}^{l} = O(l^{k+1})$ . Let's prove that $P(l+1)$ is true. I want to prove that $\sum_{i=1}^{l+1} = O((l+1)^{k+1})$ is true, so: $1^k + 2^k + \dots + l^k + (l+1)^k = O((l+1)^{k+1})$ I have to prove that: $\lim_{x \to \infty}\frac{ 1^x + 2^x + \dots + l^x + (l+1)^x  }{(l+1)^{x+1}} = L \geq 0$ $
\begin{align}
\\
\lim_{x \to \infty}\frac{ 1^x + 2^x + \dots + l^x + (l+1)^x  }{(l+1)^{x+1}} 
 &= \lim_{x \to \infty}\frac{ 1^x + 2^x + \dots + l^x + (l+1)^x  }{(l+1)^x \times (l+1)} \\
& = \frac{1}{l+1} \lim_{x \to \infty} \frac{ 1^x + 2^x + \dots + l^x + (l+1)^x  }{(l+1)^{x}} \\
& = \frac{1}{l+1} [\lim_{x \to \infty} (\frac{1}{l+1})^x+ (\frac{2}{l+1})^x + \dots (\frac{l}{l+1})^x + 1 ]
\end{align}$ . As $l > 1 \rightarrow l+1 > 2$ the above limit equals $0 + 0 + \dots + 0 + 1 = 1$ . So $\frac{1}{1+l} \in \mathbb{R}$ , and thus the induction step is proven , and the proof is concluded . $\blacksquare$ Is this way correct?","['solution-verification', 'discrete-mathematics']"
3987641,Find the derivative of $f(x)=\text{sin}(x^2)\text{ln}(x)$.,"I have the following task: Find the derivative and state your answer in the simplest form: $$f(x) =\text{sin}(x^2)\text{ln}(x)$$ Here's my attempt: \begin{align}
f'(x)&=\frac{d}{dx}\left(\sin \left(x^2\right)\right)\ln \left(x\right)+\frac{d}{dx}\left(\ln \left(x\right)\right)\sin \left(x^2\right) \\
&= \cos \left(x^2\right)\cdot \:2x\ln \left(x\right)+\frac{1}{x}\sin \left(x^2\right) \\
&= 2x\cos \left(x^2\right)\ln \left(x\right)+\frac{\sin \left(x^2\right)}{x}
\end{align} I'm pretty sure that the answer is correct but I'm not sure if this is the simplest form. From a glance it does look like it can't be simplified anymore but obviously I could be wrong.","['calculus', 'derivatives', 'real-analysis']"
3987672,How many $n^{th}$ powers do I need to add to get a positive density of natural numbers?,"Given a natural number $n$ , I do see that density of the set $S = \{x_{1}^{n} + x_{2}^{n} + ... + x_{n-1}^{n} : x_{1}, ... , x_{n-1} \in\mathbb{N} \}$ is clearly zero due to simple counting reasons. This is because $d(S) \leq \lim_{n \rightarrow\infty} \frac{{n \choose 1} + { n \choose 2} + ... + {n \choose n-1}}{n^{n}} = 0 $ since the numerator is a polynomial of degree $n-1$ . My intuition says that the density of set $T = \{x_{1}^{n} + x_{2}^{n} + ... + x_{n-1}^{n}+x_{n}^{n} : x_{1}, ... , x_{n} \in\mathbb{N} \}$ should be positive. However, all I could show that $d(S) \leq \frac{1}{n!}$ . Is my intuition correct that $d(T) > 0$ ? I cannot seem to even show that the density (i.e. the limit in the definition) exists. I would be happy if I could show that $\overline{d}(T) = \limsup_{k\rightarrow\infty} \frac{|T \cap \{1, ... , k\} |}{k} > 0$ .","['analytic-number-theory', 'number-theory', 'elementary-number-theory']"
3987687,"Sheaf cohomology of the structure sheaf of the grasmannian G(2,7)","How can I calculate $H^i(Gr(2,7),\mathcal{O}_{Gr(2,7)})$ over the base field $\mathbb{C}$ ? Or in general $H^i(Gr(k,n),\mathcal{O}_{Gr(k,n)})$ ? The first idea came to my mind is using Plücker embedding $i:Gr(2,7)\hookrightarrow\mathbb{P}^{20}$ and the long exact sequence associated to the ideal sheaf exact sequence $$0\rightarrow\mathcal{I}_{Gr(2,7)}\rightarrow\mathcal{O}_{\mathbb{P}^{20}}\rightarrow{i_*}\mathcal{O}_{Gr(2,7)}\rightarrow{0}\,.$$ But I don't know how to calculate $H^i(Gr(2,7),\mathcal{I}_{Gr(2,7)})$ either.","['sheaf-cohomology', 'complex-geometry', 'grassmannian', 'algebraic-geometry', 'sheaf-theory']"
3987710,Non-unique solutions for ODE with boundary conditions at infinity,"Ello, I am looking for solutions to equations such as $$ U ^ \prime (y) - y^{\prime \prime} + \beta y^{\textit{IV}} = 0$$ where $\beta >0$ is a constant and $U(x)$ is a function whose Taylor expansion at the origin equals $ \alpha x^2 + \dots$ for $\alpha >0$ and such that $\lim_{x \to \infty} = -\infty$ .
A prototypical example would be the quartic double-well, $$ U(x) = x^2 -x^4$$ . There is a trivial solution $y(x) = 0$ . On intuitive grounds (please see physical background below), as well as noting that the origin is a hyperbolic point for a dynamical system governed by the related system of equations, another solution is expected as well, with boundary conditions $$ \begin{cases} \lim_{x \pm \infty} y(x) = 0  \\ 
\lim_{x \pm \infty} y^ \prime (x) = 0 \end{cases}$$ This fact does not contradict the unicity theorem, as far as I understand, as the latter deals with boundary conditions on the real line, I believe. Now to my problem, what numerical scheme could be used to get the non-trivial solution? Anything I could think of will pick the trivial solution branch only.
Further, I would be grateful for theory references on the possibility of multiple solutions for boundary conditions at infinity. Things would be different if $\beta$ were equal to $0$ .
In that case it is easy to determine two boundary conditions (also clarified in the physical background section below) for $x = 0$ , and any numerical method will handle it easily. Physical Background The ODE is the Euler-Lagrange equation for the functional $$ \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y) + \frac{\beta}{2} y^{\prime \prime 2} \mathrm{d}x $$ representing the energy of a stretchable beam in the potential $U$ . The first term represents the stretching energy, the second the potential energy, the third the bending energy.
Given this physical picture, the trivial solution represents a beam lying in the potential energy ""valley"" at the origin, un-stretched and straight. The non-trivial solution represents a beam lying on the valley as before, then deforming over the potential energy ""bump"", kept in the equilibrium by the pull from the descending potential energy on the other side, and going back to the valley at the origin. If $\beta = 0$ the non-trivial solution is amenable to a simple solution.
The energy functional reduces in such case to $$ \int _{- \infty} ^{\infty } \frac{1}{2} y^{\prime 2} + U(y)  \mathrm{d}x $$ which could be interpreted as the action of a particle rolling in a potential energy $-U(x)$ (with $x$ now representing time). The turning point is then easily found by solving the equation $ U(x_1) = 0$ : that is, the particle starts from $x = 0$ , rolls down and up until $x_1$ , and rolls back in a homoclinic orbit.
This gives two boundary conditions $$ \begin{cases}  y(0) = x_1  \\ 
 y^ \prime (0) = 0 \end{cases}$$ and these allow any numerical scheme to pick the non-trivial branch. If $\beta \neq 0$ , this simple reasoning fails for the full fourth-order ODE (I also thought about converting the fourth-order ODE to a system of equations, to attempt a similar interpretation for a system of four particles, but did not get anywhere yet, apart confirming that a second non-trivial solution exists). I am hence left with the boundary conditions a infinity, and cannot get any solution other than the trivial one. To sum up, I would like to be pointed towards references for the non-unicity of solutions when boundary conditions at infinity are applied, and to understand how to numerically solve similar problems. The ODE has a physical interpretation and I was unsure whether the question were more apt for Physics StackExchange, or even SciComp StackExchange. I would certainly re-route if advised, I thought to start here as ultimately the question is primarily on the maths, thanks a lot.","['ordinary-differential-equations', 'euler-lagrange-equation', 'hamilton-equations', 'numerical-methods', 'dynamical-systems']"
3987718,Finding a function which is not differentiable at $x_{0}$,"Let $L \in \mathbb{R}$ and let $f$ be a function that is differentiable on a deleted neighborhood of $x_{0} \in \mathbb{R}$ such that $\lim_{x \to x_{0}}f'(x)=L$ . Find a function satisfying the above, and such that $f$ is not differentiable at $x_{0}$ . -- So I think that I do not completlely understand, when a function is indeed differentiable at $x_{0}$ and when it's not, and why in both cases I can still find its $f'$ ? I will appreciate some explanation about that. Moreover, I thought of $f(x)=x^x$ or $f(x)=\ln(x^x)$ . If I understand it correctly, than both my $f$ 's does not differentiable at $x_{0}=0$ , because: $f'(0)=\lim_{x \to 0}\frac{x^x-0^0}{x-0}$ which is undefined? or $f'(0)=\lim_{x \to 0}\frac{\ln(x^x)-\ln(0^0)}{x-0}$ which is undefined? Thanks a lot!","['limits', 'calculus', 'functions', 'derivatives']"
3987756,Evaluating $\int_{0}^{\pi} \frac{1}{\sqrt{u^2+2u\cos x +1}} \mathrm{d}x$,"Does the integral $$\int_0^{\pi} \frac{1}{\sqrt{u^2+2u\cos x +1}} \text d x \hspace{30pt} (u \le 1) $$ has a closed form? If it has, how do we evaluate it? I was solving a physics problem which I have asked on physics SE as well ( here ), and this integral popped out. How do I solve it?","['integration', 'calculus', 'elliptic-integrals', 'definite-integrals']"
3987784,"Calabi-Yau conditions for a threefold in the Grassmannian $Gr(2,7)$","I am trying to show a complete intersection $X$ in the grassmannian $G(2,7)$ is a Calabi-Yau in the strict sense. By that I mean $\omega_X\cong\mathcal{O}_X$ and $h^i(\mathcal{O}_X)=0$ for $i=1,2$ . The description of $X$ is given by $X=Gr(2,7)\cap{H}^7$ under the Plücker embedding $i:Gr(2,7)\hookrightarrow\mathbb{P}^{20}$ where $H^7$ is a generic intersection of $7$ hyperplanes in $\mathbb{P}^{20}$ . I showed the part $\omega_X\cong\mathcal{O}_X$ by using adjunction formula and the canonical sheaf $\omega_{G(2,7)}$ of the grassmannian $G(2,7)$ . For the second part I think I know $h^1(\mathcal{O}_X)=h^2(\mathcal{O}_X)$ by Serre duality. How can I show $h^1(\mathcal{O}_X)=0$ ?","['sheaf-cohomology', 'kahler-manifolds', 'complex-geometry', 'algebraic-geometry', 'sheaf-theory']"
3987848,"A Straightforward optimization problem, but without calculus","I am helping a middle schooler out with the following Art of Problem Solving competition problem: An ant travels from the point $A (0,-63)$ to the point $B (0,74)$ as follows. It first crawls straight to $(x,0)$ with $x \ge 0$ , moving at a constant speed of $\sqrt{2}$ units per second. It is then instantly teleported to the point $(x,x)$ . Finally, it heads directly to $B$ at 2 units per second. What value of $x$ should the ant choose to minimize the time it takes to travel from $A$ to $B$ ? A straightforward but somewhat tedious solution involves using calculus to optimize the time taken as a function of x (with this method we get $\approx 23.3$ ). However, the student I am helping has not been introduced to calculus, and I was beating my head against a wall trying to find a clever way to solve this with simpler methods like algebra with quadratics or geometry. For instance, The function of x which represents the time of the ants travel is: $$T = \frac{\sqrt{x^2 + 63^2}}{\sqrt{2}} + \frac{\sqrt{x^2 + (74-x)^2}}{2}$$ I thought that, instead of minimizing $T$ w.r.t $x$ , I could minimize $T^2$ , which I would then be able to algebraically massage into the form of a quadratic equation. Simple knowledge of the properties of these equations would allow for a location of the minimum. This approach stalled out due to the cross term that results from the RHS. Any help would be appreciated","['optimization', 'algebra-precalculus', 'geometry']"
3987874,Question about finite/infinite intersection,"$A= \bigcap_{i \in \mathbb{N}} (-1+\frac{1}{i}, \frac{1}{i})$ $B= \bigcap_{i=1}^{n} (-1+\frac{1}{i}, \frac{1}{i})$ with $n<\infty$ In case of $A,$ I think the intersection should be empty. but I am unsure about $B$ the first few intervals would look like this $i=1: (0,1)$ $i=2: (-1/2,1/2)$ $i=3: (-2/3, 1/3)$ $\vdots$ $i=n: (\frac{1-n}{n}, \frac{1}{n})$ $n<\infty$ so the value on the left would get smaller and 0 would be included in all intervals, but how about the right side?, the last value $\frac{1}{n}$ would be the smallest of all, could I just say the finite intersection would be $B=(0,\frac{1}{n})$ ?","['elementary-set-theory', 'real-analysis']"
3987893,Compute integral with correlated brownian motions,"Compute the expectation and variance of the integral $\int_0^t B_s \, dW_s$ where $B_s$ and $W_s$ are correlated Brownian motions with correlation given by $d[B,W]_s=\rho \, ds$ . I thought it is best to write the brownian motion $W_s$ as $W_s = \rho \, dB_s + \sqrt{1-\rho^2} \, dZ_s$ where $Z_s$ is independent of $B_s$ . This allows me to write $$\int_0^t B_s \, dW_s = \int_0^t B_s (\rho \, dB_s + \sqrt{1-\rho^2} \, dZ_s) = \rho\int_0^t B_s \, dB_s+ \sqrt{1-\rho^2} \int_0^t B_s \, dZ_s $$ The first integral on the right hand side reduces to $$\int_0^t B_s \, dB_s = \frac{1}{2}(B_t^2 - t)  $$ Now, how to handle the second expectation on RHS? What does that integral mean exactly? Thanks.","['expected-value', 'brownian-motion', 'probability-theory', 'stochastic-calculus']"
3987902,What is wrong with this procedure-writing Angular momentum operator in spherical coordinates,"I am trying to write the first component of the quantum angular momentum operator $L_1$ in spherical coordinates. $$L_1=x_2p_3-x_3p_2=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})=$$ The defining equations of the spherical coordinates are: $x_1=r \sin\theta \cos\varphi,x_2=r\sin\theta\sin\varphi, x_3=r\cos\theta\tag{1}$ Then using the chain rule: \begin{align}
L_1
&=-i\hbar (x_2\dfrac{\partial}{\partial x_3}-x_3\dfrac{\partial}{\partial x_2})\\
&=-i\hbar \Big[r\sin\theta\sin\varphi\left(
\dfrac{\partial r}{\partial x_3}\dfrac{\partial}{\partial r}
+\dfrac{\partial \theta}{\partial x_3}\dfrac{\partial}{\partial \theta}
+\dfrac{\partial \varphi}{\partial x_3}\dfrac{\partial}{\partial \varphi}\right)\\
&\qquad\qquad\; -r\cos\theta\ \left(
\dfrac{\partial r}{\partial x_2}\dfrac{\partial}{\partial r}
+\dfrac{\partial \theta}{\partial x_2}\dfrac{\partial}{\partial \theta}
+\dfrac{\partial \varphi}{\partial x_2}\dfrac{\partial}{\partial \varphi}\right)
\Big] \tag{2}
\end{align} Using (1): $\dfrac {\partial r}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial r}}=\dfrac {1}{\cos \theta}$ $\dfrac {\partial \theta}{\partial x_3}=\dfrac {1}{\frac{\partial x_3}{\partial \theta}}=\dfrac {-1}{r \sin \theta}$ $\dfrac {\partial \varphi}{\partial x_3}=0$ $\dfrac {\partial r}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial r}}=\dfrac {1}{\sin \theta \sin \varphi}$ $\dfrac {\partial \theta}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \theta}}=\dfrac {1}{r \cos \theta \sin \varphi}$ $\dfrac {\partial \varphi}{\partial x_2}=\dfrac {1}{\frac{\partial x_2}{\partial \varphi}}=\dfrac {1}{r \sin \theta \cos \varphi}$ Plugging these results in (2): $$L_1=-i\hbar [r\tan\theta\sin\varphi
\dfrac{\partial}{\partial r}
-\sin \varphi\dfrac{\partial}{\partial \theta}
-r \dfrac{\cot\theta}{\sin \varphi}\dfrac{\partial}{\partial r}
-\dfrac{1}{\sin \varphi}\dfrac{\partial}{\partial \theta}
-\dfrac{\cot \theta}{ \cos \varphi}\dfrac{\partial}{\partial \varphi}
] \tag{3}$$ But the result should be $$L_1=+i\hbar [\sin \varphi\dfrac{\partial}{\partial \theta}
+\cos \varphi \cot \theta \dfrac{\partial}{\partial \varphi}
] \tag{4}$$ What am I doing wrong?","['partial-derivative', 'multivariable-calculus', 'quantum-mechanics']"
3987912,Regular value of the determinant map,"Let $\text{det}: M_n(\mathbb{R})\rightarrow \mathbb{R}$ be the determinant map from $n\times n$ matrices to $\mathbb{R}$ , then how can I show that 1 is a regular value of this map? I am specifically having trouble to calculate the derivative of this map on matrices.","['determinant', 'smooth-manifolds', 'lie-groups', 'differential-geometry']"
3987919,$\arccos(1/2)$ products,"I'm finding points of intersection between two functions, $y = \tan(x)$ and $y = 2\sin(x)$ , which are bound between $-\pi/3$ and $\pi/3$ . Solving for the intersections points, I end up performing $x = \arccos\left(\frac12\right)$ I know this produces: $x = \pi/3$ , but should it rather produce $x = \pm\pi/3$ ? This seems to make sense as visually the two functions intersect at both $-\pi/3$ and $\pi/3$ , but I want to verify if this is correct. I'm usually on Stack Overflow. Let me know if any notation here is wrong. Additional apologies if this question is very low level, I haven't worked with trig in a while.",['trigonometry']
3988024,Show the inclusion $\mathcal{B}(\mathbb R)^{I}\supseteq Q$,"Let $I$ be an uncountable set and consider the product sigma algebra on $\mathcal{B}(\mathbb R)^{I}$ I want to show that $\mathcal{B}(\mathbb R)^{I}=\{ \{f \in \mathbb R^{I}\lvert (f(t_{1}),f(t_{2}),...)\in A\}\lvert (t_{n})_{n\in \mathbb N}\subseteq I,\; A \in \mathcal{B}(\mathbb R)^{(t_{n})_{n \in \mathbb N}}\}=: Q$ I have already shown the inclusion $\mathcal{B}(\mathbb R)^{I}\subseteq Q$ by using the fact that $Q$ is a sigma-algebra and the preimages of projections of measurable sets indeed lie in $Q$ Apparently the direction $\mathcal{B}(\mathbb R)^{I}\supseteq Q$ is the easier one but I am lost on how to go about it. Attempt: Assuming that I have $M:=\{f \in \mathbb R^{I}\lvert (f(t_{1}),f(t_{2}),...)\in A\}$ such that $(t_{n})_{n\in \mathbb N}\subseteq I,\; A \in \mathcal{B}(\mathbb R)^{(t_{n})_{n \in \mathbb N}}$ . Then if $A$ is of the form $A=\prod\limits_{n=1}^{\infty}A_{t_{i}}$ for $A_{t_{i}}\in \mathcal{B}(\mathbb R)$ clearly: $M:=\mathbb R^{I\setminus (t_{n})_{n \in \mathbb N}}\times A= \bigcap\limits_{n=1}^{\infty}p_{t_{j}}^{-1}(A_{t_{j}})\in \mathcal{B}(\mathbb R)^{I}$ . But my issue is that not every $A \in \mathcal{B}(\mathbb R)^{(t_{n})_{n \in \mathbb N}}$ is of the form $A=\prod\limits_{n=1}^{\infty}A_{t_{i}}$ . But I know that the set $$\mathcal{C}:=\{ \prod\limits_{n=1}^{\infty}A_{t_{i}}\lvert A_{t_{i}}\in \mathcal{B}(\mathbb R)\; \forall i \in \mathbb N\} $$ is a generator of $\mathcal{B}(\mathbb R)^{(t_{n})_{n \in \mathbb N}}$ . Is there any way I could use this generator to extend the result, like one usually does with the monotone class theorem?","['measure-theory', 'real-analysis', 'borel-sets', 'general-topology', 'probability-theory']"
3988062,Calculate expected value programmed Y,"Let Random Variable Y be determined by the following algorithm: Initially Y = 0; While Y < 1, random number (uniformly distributed in-between [0;1]) is being returned. In Python code would look like this: Y = 0
while Y < 1:
    Y += random() The task is to find Y expected value.
What I've managed to do - just sampling with 1e8 iterations, getting a result of somewhere near 1.39. However the task is to find it using maths. Do you have any ideas?","['expected-value', 'probability-theory', 'random-variables']"
3988147,Cardinality of equivalence classes in the positive braid semigroup?,"A presentation for the braid group is: $$B_n = \{ s_1,...,s_{n-1} | s_is_{i+1}s_i=s_{i+1}s_is_{i+1},\ \text{ } s_is_j = s_js_i \text{ for } |i-j| \geq 2\}$$ As a set, the positive braid semigroup $B_n^+$ is then all the words in $B_n$ such that no negative exponents appear on any of the generators, and the binary operation is the restriction of the operation in $B_n$ . Elements $b \in B_n^+$ are equivalence classes, with $b \cong c$ if the braid word that represents $b$ differs from the braid word representing $c$ by finite many applications of the relations. I am trying to understand, given an element $b \in B_n^+$ , what is its cardinality as an equivalence class as an element of $B_n^+$ ? For example, let $b=(1,2,3,1,2,3)$ . Then I'm pretty sure, by brute force, I've calculated that there are $7$ different braid words that can represent $b$ : $(1,2,3,1,2,3)$ $(1,2,1,3,2,3)$ $(2,1,2,3,2,3)$ $(2,1,3,2,3,3)$ $(2,3,1,2,3,3)$ $(1,2,1,2,3,2)$ $(2,1,2,2,3,2)$ Does anyone know how I could figure this out in a more sophisitcated way? I know word problems such as this have been well studied. Also, is there a systemic way to determine when two braid words are equivalent? Thank you.","['combinatorics-on-words', 'group-theory', 'abstract-algebra', 'semigroups']"
3988168,Evaluate $\sum\limits_{r=1}^\infty(-1)^{r+1}\frac{\cos(2r-1)x}{2r-1}$,"I would like to know how to evaluate $$\sum\limits_{r=1}^\infty(-1)^{r+1}\frac{\cos(2r-1)x}{2r-1}$$ There are a couple of issues I have with this. Firstly, depending on the value of $x$ , it seems, at least numerically, that this value is always $$\pm\sqrt{\frac{\sqrt5-1}{2}}$$ If this is true, how would I prove this? Secondly, I tried using complex numbers to evalaute this, as shown briefly below: $$C=\sum\limits_{r=1}^\infty(-1)^{r+1}\frac{\cos(2r-1)x}{2r-1}$$ $$S=\sum\limits_{r=1}^\infty(-1)^{r+1}\frac{\sin(2r-1)x}{2r-1}$$ $$\implies C+iS=\arctan e^{ix}$$ on using the power series of $\arctan x$ . Differentiating yields a completely imaginary number : $$\frac{i}{2}\sec x$$ which proves that the value of $C$ is a constant. However, this gives me no information whatsoever on the value of $C$ . Thank you for your help.","['summation', 'calculus', 'solution-verification', 'trigonometry', 'algebra-precalculus']"
3988183,Evaluating $\lim_{x\to\infty}(\sqrt{x^2+1} - x)(x+1) $,The question is $$\lim_{x\to\infty}(\sqrt{x^2+1} - x)(x+1) $$ I know the answer is $\frac{1}{2}$ and I found it using this equality : $$(\sqrt{x^2+1} - x)(x+1) = \frac{x+1}{\sqrt{x^2+1} + x}$$ But is there any other way to solve this? Any hints would be appreciated.,"['limits', 'calculus']"
3988210,what are all functions with $x>1$ and $y>1$ $\rho$ that follows $\rho(xy)=\frac{1}{\frac{y}{\rho(x)}+\frac{x}{\rho(y)}}$ and is continuous,What are all functions with $x>1$ and $y>1$ $\rho$ that follows $$\rho(xy)=\frac{1}{\frac{y}{\rho(x)}+\frac{x}{\rho(y)}}$$ and is continuous If this doesn't have any solutions then prove no such solution exists. I would like to understand how to solve a problem like this,"['real-numbers', 'continuity', 'functions']"
3988215,surjective group homomorphism between $\mathbb{Z}^2$ and $\mathbb{Z}_{30}$,"Let $H = \langle (6,2), (3,6)\rangle$ , which is a subgroup of $\mathbb{Z}^2$ (denoted $H\leq \mathbb{Z}^2$ ). Show that $|\mathbb{Z}^2/H| = 30$ and that $\mathbb{Z}^2/ H$ is cyclic, and then find a surjective group homomorphism $\phi : \mathbb{Z}^2 \to \mathbb{Z}_{30}$ with $Ker(\phi) = H.$ I think that $\mathbb{Z}^2/ H = \{(r,s) + H : 0\leq r < 15, 0\leq s < 2\} =: T$ and $T = \langle (2,1) + H\rangle$ . Indeed $(2,1) + H \in T$ and $T\leq \mathbb{Z}^2/H$ so $\langle (2,1) + H\rangle \subseteq T.$ Also, every element in $T$ is a multiple of $(2,1) + H$ so $T\subseteq \langle (2,1) + H\rangle$ (one can show this using the fact that $(6,2), (15,0)\in H$ ). Also, by repeated use of the division algorithm, one can show that every coset is in $T.$ To show they're distinct one can obtain a contradiction from assuming $(r_1, s_1) + H = (r_2, s_2) + H$ if $(r_1, s_1)\neq (r_2, s_2) $ . One can show $s_1 = s_2$ and if $r_1 \neq r_2$ then they must differ by a multiple of $15.$ Also, $\phi : \mathbb{Z}^2/ H \to \mathbb{Z}_{30}, \phi((r,s) + H) = s \cdot 15 + r$ is a group isomorphism, which shows $\mathbb{Z}^2/ H\cong Z_{30}$ and hence $|\mathbb{Z}^2/ H| = 30$ . However, I’m not sure how to find a surjective group homomorphism $f: \mathbb{Z}^2 \to \mathbb{Z}_{30}$ with $Ker(\phi) = H.$ I tried determining the values of $f(1,0)$ and $f(0,1)$ but apparently I seem to get a contradiction.","['group-homomorphism', 'group-theory', 'abstract-algebra']"
3988265,Differential Equation Solution By Power Series,"Solve $(1 + x)y' = py;\ \ \ y(0) = 1$ , where $p$ is an arbitrary constant. First I plugged in the guess $y = \sum_{n = 0}^\infty a_n x^n$ : $(1 + x)(\sum_{n = 0}^\infty a_n x^n)' = p\sum_{n = 0}^\infty a_n x^n$ Then I expanded the derivative and multiplication: $\sum_{n = 0}^\infty n a_n x^{n - 1} + \sum_{n = 0}^\infty n a_n x^n = p\sum_{n = 0}^\infty a_n x^n$ Then I shifted the left index (the first term yielding $0$ allows the lower bound to remain $0$ ) and algebraically combined the summations: $\sum_{n = 0}^\infty (n + 1)a_{n + 1} x^n + (n - p)a_n x^n = 0$ This leads to the following recurrence relation: $a_{n + 1} = \frac{p - n}{n + 1}a_n$ Thus for various values of $n$ : $a_1 = p a_0$ , $a_2 = \frac{p(p - 1)}{2}a_0$ , $a_3 = \frac{p(p - 1)(p - 2)}{6} a_0$ , etc. So applying definitions for the exponential taylor series and falling factorial, the guessed solution would be: $y = \sum_{n = 0}^\infty \frac{p! a_0 x^n}{n! (p - n)!} = \sum_{n = 0}^\infty a_0 e^x p^{\underline n}$ Solving the initial value problem: $1 = \sum_{n = 0}^\infty a_0 e^0 p^{\underline n} \implies a_0 = \frac{1}{\sum_{n = 0}^\infty p^{\underline n}}$ My final solution is: $y = \frac{\sum_{n = 0}^\infty e^x p^{\underline n}}{\sum_{n = 0}^\infty p^{\underline n}}$ However, the answer is supposed to be $y = (1 + x)^p$ .  Are these identical, or did I make an error somewhere?","['factorial', 'ordinary-differential-equations', 'solution-verification', 'sequences-and-series', 'power-series']"
3988274,"Evaluate $ \int_0^{2π} e^{\cos (x)}\cos(\sin x) \, \mathrm{d}x $ [duplicate]","This question already has answers here : How to evaluate $\int_{0}^{2\pi}e^{\cos \theta}\cos( \sin \theta) d\theta$? (6 answers) Closed 2 years ago . I have again a doubt regarding an exercise of differentiation under the integral sign. In this case, it concerns the integral: $$ \int_0^{2π} e^{\cos(x)}\cos(\sin x) \, \mathrm{d}x $$ I tried the substitution: $f(a,b) =  \int_0^{2π} e^{acos(x)}\cos(b\sin x) \, \mathrm{d}x  $ So then: $\frac {\partial f} {\partial a} = \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x  $ and: $\frac {\partial f} {\partial b} = -\int_0^{2π} e^{a\cos(x)}\sin(b\sin x)\sin x \, \mathrm{d}x $ Then what i did was try to integrate $\frac{\partial f}{\partial b}$ by parts, making: $dv = -e^{a\cos x}\sin x\, \mathrm{d}x$ so: $v = \frac{1}{a}e^{a\cos x}$ and: $u = \sin(b\sin x)$ so: $du = \cos(b\sin x)b\cos x$ Then we got that: $\frac{\partial f}{\partial b} = - \frac{b}{a} \int_0^{2π} e^{a\cos(x)}\cos(b\sin x)\cos x \, \mathrm{d}x = -\frac{b}{a} \frac{\partial f}{\partial a} $ (Because evaluating $uv$ from $0$ to $2π$ gives us $0$ ) So we get the partial differential equation: $\frac{1}{b} \frac{\partial f}{\partial b} = - \frac{1}{a} \frac{\partial f}{\partial a}$ I solved this by separation of variables, making $f(a,b) = A(a)B(b)$ . The end result was: $f(a,b) = Ce^{\frac{h^2}{2}(a^2 - b^2)}$ Where $C$ is an integration constant and $h$ comes from solving the ODE associated with each $A$ and $B$ . So, my problem begins with this, because I am not sure of my result, this is mostly because of the $h$ , should I pick an specific value? Or my result is just wrong? If this is the case, where did I made the mistake? Because evaluating $f(0,0)$ we get: $f(0,0) = C = 2π$ , and so: $f(1,1) = 2π$ , that is the correct result evaluating in wolfram alfa. I just want to know if there is a way in wich I should get the value.","['integration', 'definite-integrals', 'ordinary-differential-equations', 'partial-differential-equations']"
3988288,Infection spread on a torus chessboard,"In one of his books, Peter Winkler includes the following problem: A disease is spreading on a $n\times n$ chessboard as follows: if a healthy cell is neighboring at least 2 infected cells, it becomes infected. Using the property that the perimeter of the infected area never increases, it’s easy to prove that it’s impossible to infect the entire chessboard with fewer than $n$ infected cells. If the chessboard is a torus, the result no longer holds (verified on some instances of $n$ ). It seems that $n-1$ is the smallest number of infected cells required to infect the chessboard. The perimeter argument can’t be used here. Does anyone know any other ‘invariant’ that can be used? Note : two cells are neighbors if they share one side.","['puzzle', 'cellular-automata', 'combinatorics', 'upper-lower-bounds', 'combinatorial-game-theory']"
3988296,Value of $i^\sqrt3$,Find all values of $i^\sqrt3$ . I am trying to apply de Moivre's formula here but cannot find a way to do so. I am not sure if i am approaching this wrong.,"['complex-analysis', 'exponentiation', 'complex-numbers']"
3988321,Need help in understanding (a part of) the proof of John's Theorem,"This is the theorem statement: John's Theorem: Each convex body $K$ contains a unique ellipsoid of maximal volume. This ellipsoid is $B^n_2$ (Euclidean ball of unit radius) iff: $B^n_2 \subset K$ and (for some $m$ ), there are Euclidean unit vectors $(u_i)_1^m$ on the boundary of $K$ and positive numbers $(c_i)_1^m$ satisfying \begin{equation}
    \sum_{i=1}^m c_i u_i = 0
\end{equation} and \begin{equation}
    \sum_{i=1}^m c_i \langle x,u_i\rangle^2 = \|x\|^2 \text{ for each }x\in\mathbb{R}^n
\end{equation} Note that we're only working with centrally symmetric convex bodies for the rest of this post, in which case the second condition implies the first, i.e. the latter is redundant. What part I'm specifically concerned with: Suppose $B_2^n$ is an ellipsoid of largest volume in $K$ . We want to show that there is a sequence of contact points $(u_i)$ and positive weights $(c_i)$ with $$\frac{1}{n}I_n = \frac{1}{n}\sum c_i \ u_i\otimes u_i$$ The proof begins: Equating traces on both sides of the equation, we know that if this is possible , then $$\sum \frac{c_i}{n} = 1$$ So our aim is to show that the matrix $I_n/n$ can be written as a convex combination of (a finite number of) matrices of the form $u \otimes u$ , where each $u$ is a contact point. Since the space of matrices is finite-dimensional, the problem is simply to show that $I_n /n$ belongs to the convex hull of the set of all such rank-one matrices, $$T = \{u \otimes u : u \text{ is a contact point}\}$$ How does the space of matrices being finite-dimensional help? The definition of $T$ seems slightly off. Perhaps the author meant $T = \text{conv}\{u \otimes u : u \text{ is a contact point}\}$ , i.e. the convex hull of all matrices of the form $uu^T$ ? Being a contact point between $B^n_2$ and $K$ , $u\in\partial K$ and $\|u\| = 1$ . We shall aim to get a contradiction by showing that if $I_n/n$ is not in T, we can
perturb the unit ball slightly to get a new ellipsoid in $K$ of larger volume than
the unit ball. Suppose that $I_n/n$ is not in $T$ . Apply the separation theorem in the space of
matrices to get a linear functional $φ$ (on this space) with the property that $$φ\left(\frac{I_n}{n}\right) < φ(u\otimes u)$$ for each contact point $u$ . Observe that $φ$ can be represented by an $n \times n$ matrix $H = (h_{jk})$ , so that, for any matrix $A = (a_ {jk})$ , $$φ(A) = \sum_{jk}h_{jk}a_{jk}$$ How did we come up with this linear functional? It makes intuitive sense, but I want to know exactly how we used the separation theorem as stated here. I saw this coming - since we are working in the space of matrices, we had to define an inner product similar to the one for $\mathbb{R}^n$ - hence we just chose the element-wise product and applied the separating hyperplane theorem? We didn't have any other option for the inner product though, right? Since all the matrices $u \otimes u$ and $I_n /n$ are symmetric, we may assume the same
for $H$ . Moreover, since these matrices all have the same trace, namely $1$ , the inequality $φ(I_n /n) < φ(u \otimes u)$ will remain unchanged if we add a constant to
each diagonal entry of $H$ . So we may assume that the trace of $H$ is $0$ : but this says precisely that $φ(I_n) = 0$ . Hence, unless the identity has the representation we want , we have found a
symmetric matrix $H$ with zero trace for which $$\sum_{jk}h_{jk}(u\otimes u)_{jk} > 0$$ for every contact point $u$ . We shall use this $H$ to build a bigger ellipsoid inside $K$ .
Now, for each vector $u$ , $$\sum_{jk}h_{jk}(u\otimes u)_{jk} = u^THu$$ What does the author mean by ""the representation we want""? For sufficiently small $δ > 0$ , the set $$E_δ = \{x ∈ \mathbb{R}^n : x^T (I_n + δH)x ≤ 1\}$$ is an ellipsoid and as $δ$ tends to $0$ these ellipsoids approach $B_2^n$ . If $u$ is one of
the original contact points, then $$u^T (I_n + δH)u = 1 + δu^T Hu > 1$$ so $u$ does not belong to $E_δ$ . Since the boundary of $K$ is compact (and the function $x \mapsto x^T Hx$ is continuous) $E_δ$ will not contain any other point of $∂K$ as long as $δ$ is sufficiently small. Thus, for such $δ$ , the ellipsoid $E_δ$ is strictly inside $K$ and some slightly expanded ellipsoid is inside $K$ . It remains to check that each $E_δ$ has volume at least that of $B_2^n$ . If we denote
by $(μ_j)$ the eigenvalues of the symmetric matrix $I_n + δH$ , the volume of $E_δ$ is $v_n/\prod\mu_j$ so the problem is to show that, for each $δ$ , we have $\prod μ_j ≤ 1$ . What
we know is that $\sum μ_j$ is the trace of $I_n + δH$ , which is $n$ , since the trace of $H$ is $0$ . So the AM/GM inequality again gives $$\prod \mu_j^{1/n} \le \frac{1}{n}\sum \mu_j \le 1$$ as required. $v_n$ denotes the volume of $B^n_2$ - how did we write the volume of the ellipsoid $E_\delta$ in terms of the eigenvalues of $I_n + \delta H$ ? We assumed $I_n/n\notin T$ , in order to get a contradiction. Where is the contradiction ? I know this is a long post! Thanks a lot for reading this far. Since this is a long one, and understanding the proof requires several clarifications, I have decided to award a bounty to an answer that helps with all (or most) of my questions. Thanks again - I'd appreciate any help!","['proof-explanation', 'convex-geometry', 'geometry', 'linear-algebra', 'convex-analysis']"
3988341,How to prove the existence of exactly one root of a function in between two consecutive roots of another function,"I have the following question before me: $f$ and $g$ are two functions derivable in $(a,b)$ such that $f(x) g'(x)-f'(x)g(x)>0$ for all $x$ in $(a,b)$ then prove that there lies exactly one root of $g(x)=0$ in between two consecutive roots of $f(x)=0$ . I assumed the existence of two roots of $g(x)=0$ in between two consecutive roots of $f(x)=0$ in order to reach a contradiction.
Here is how I attempted: Let $c$ and $d$ be the two consecutive roots of $f(x)=0$ . Now for all $x$ in between $c$ and $d$ , $f(x)$ is non-zero. Consider the function $g(x)/f(x)$ in the interval $(c,d)$ . The derivative of this function is $ (f(x)g'(x)-g(x)f'(x))/((f(x))^2$ . Due to the given condition, this derivative is always positive which implies that the function is strictly increasing in nature and thus can't have equal values at any two different points in its domain. Now I consider the existence of two consecutive roots of $g(x)=0$ in $(c,d)$ which makes the value of the function $g(x)/f(x)$ $0$ at two different points which is impermissible. Thus I am able to arrive at a contradiction. From here I can conclude that there lies at most one root of $g(x)=0$ in between two consecutive roots of $f(x)=0$ . But the question asks me to show the existence of exactly one such root. How do I proceed from here to prove this? Please suggest.","['rolles-theorem', 'derivatives', 'real-analysis']"
3988438,"Find $f:[0, 1]\to \mathbb R$ that maximizes $I(f)-J(f)$, where $I(f)=\int_0^1 {x^2 f(x)dx}$, $J(f)=\int_0^1{x\left(f(x)\right)^2 dx}$","I've never seen this kind of problems - finding a function with almost no conditions which maximizes the integral - so I'm asking for a hint. The problem is as follows. Find a continuous function $f:[0, 1]\to \mathbb R$ that maximizes $I(f)-J(f)$ , where $$I(f)=\int_0^1 {x^2 f(x)dx},\ \ \ \ \ \  J(f)=\int_0^1{x\left(f(x)\right)^2 dx}$$ I tried to select candidates of types of functions that can possibly make the given integration maximum. Assuming that $f$ is a function that $\exists I(f), J(f)$ , I considered a function $g(x):=xf(x)\{x-f(x)\}$ , but still don't have a idea. Any helps will be very appreciated. Thanks.","['integration', 'maxima-minima', 'calculus', 'functions', 'functional-analysis']"
3988489,A certain passage in Carathéodory's book concerning analytic continuation,"The fragment below is from the book 'Theory of functions of a complex variable', by Carathéodory. The situation is this. We have an analytic function $f(z)$ in some region $G_z\subset\mathbb{C}$ . Then $G_w=f(G_z)$ is a region in the $w$ -plane. We assume that $G_w^*$ is a simply-connected subregion of $G_w$ that does not contain points $w=f(z)$ such that $f'(z)=0$ . We know,
by the inverse function theorem, that $f$ has a local inverse around such points. But then
Carathéodory claims (highlighted below) that we can start at any such point $w_0$ and continue analytically such a local inverse along any ""polygonal train"" -- (i.e any polygonal path) starting from $w_0=f(z_0)$ (as long as we stay inside $G_w^*$ ). He then deduces -- by the monodromy theorem -- that $f$ has a single-valued analytic inverse defined on the whole of $G_w^*$ . Carathédory's argument says this: If $f$ is an analytic function defined on a region $\Omega$ such that $f'(z)\neq 0$ for every $z\in\Omega$ , then $f$ has a single-valued analytic inverse defined in any simply connected subregion of $f(\Omega)$ . As far as I know, there are examples of functions defined in simply connected regions that cannot be analytically continued. So my question is this: -- Is the statement above true? How does Carathédory know that we can extend a local inverse analytically along every polygonal path that lies in $G_w^*$ ? Added In order to clarify the question, here is the final comment Carathéodory
makes:",['complex-analysis']
3988492,Finding the Riemann $\zeta$ function by adelic integration,"I am referring to Tao's blog post about Tate's thesis. Introduce the adeles $\mathbb A$ of $\mathbb Q$ and the adelic Mellin transform $$Z(s) = \int_{\mathbb A^\times} = g(x) |x|^s d^\times x.$$ Here, $g = \prod_v g_v$ is a product over places $v$ of $\mathbb Q$ and $g_v$ is a self-dual function on $\mathbb Q_v$ , more precisely the Gaussian $g(x) = \exp(-\pi x^2)$ for the real place and the characteristic function of integers $\mathbb 1_{\mathcal O_p}$ at finite places $p$ . I would like to justify that $$Z(x) = \pi^{-s/2} \Gamma(s/2) \zeta(s), $$ as in the blog's equation (24), directly from properties of the adeles (and not the Euler product of $\zeta$ ). In particular, Tao emphasizes that we can use an explicit fundamental domain for $\mathbb A^\times / \mathbb Q^\times$ , viz. $$\mathbb A^\times = \bigsqcup_{k \in \mathbb Q^\times} k \left( J := \mathbb R_+ \prod_p \mathbb Z_p^\times \right).$$ Cutting by classes modulo $\mathbb Q^\times$ , we get $$Z(s) = \int_{\mathbb A^\times / \mathbb Q^\times} \left( \sum_{k \in \mathbb Q^\times} g(kx) \right) |x|^s d^\times x$$ where we recognize the adelic analogue of the theta function used in the derivation of the classical functional equation. However, I do not understand how using the decomposition above we would find another way to recover $Z(s) = \pi^{-s/2} \Gamma(s/2) \zeta(s)$ .","['integration', 'number-theory', 'adeles']"
3988498,What is the necessary condition on $f$ such that the DE $dy/dx=f(y)$ has a solution?,"In my book, it is written that we shall assume throughout the discussion that follows that $f$ and its derivative $f'$ are continuous functions of $y$ on some interval $I$ . I am guessing that this assumption is made in order that the autonomous DE $dy/dx=f(y)$ has a solution. However, I feel like the continuity of $f$ is enough to guarantee that the DE has a solution since this would imply that $dy/dx$ is continuous on some $I$ . What am I missing?","['continuity', 'derivatives', 'ordinary-differential-equations']"
3988508,"there exists a $c>0$ such that every $3$-regular bipartite graph is a $(2n,3,c)$-expander","Definition : A graph $G = (V, E)$ is called an $(n, d, c)$ -expander if it has $n$ vertices, the maximum degree of a vertex is $d$ , and, for every set of vertices $W \subset V$ of cardinality $|W| \le n∕2$ , the inequality $|N(W)| \ge c|W|$ holds, where $N(W)$ denotes the set of all vertices in $V \backslash W$ adjacent to some vertex in $W$ By considering a random bipartite three-regular graph on $2n$ vertices obtained by picking three random permutations between the two color classes, prove that there is a $c > 0$ such that for every n there exists a $(2n, 3, c)$ -expander. This is question 9.6.1 from The Probabilistic Method and I am tempted to argue that for any $c\le 1$ the graph is trivially an expander. In the question we are asked to consider the 3-regular bipartite graph obtained by taking the edge union of three permutations on $[n]$ . A permutation is basically a perfect matching, therefore this construction contains a perfect matching and by Hall's theorem, every subset $W\subset[n]$ satistifes $|N(W)|\ge|W|\ge c|W|$ , in particular for sets $|W|\le 1/2$ . (it is actually even simpler than that, no need for Hall's theorem: by 3-regularity $|N(W)|<|W|$ would imply $3|N(W)|<3|W|$ which in english means $\#\{\text{outedges of } |N(W)|\} < \#\{\text{outedges of } |W|\}$ which is obviously false). But then these hints from this website point to a completely different approach. I would be interested in additional hints on how to compute the probability that $N(W)<(1+\epsilon)|W|$ for a given $W\subset[n]$ as they say in the hints. And of course I'd like to know if taking $c\le 1$ is OK...","['graph-theory', 'probabilistic-method', 'combinatorics', 'bipartite-graphs']"
3988514,Hartshorne II.7.10: Is there any error in my proof that every Zariski-locally trivial $\Bbb P^n$-bundle is relative Proj of a locally free sheaf?,"In Hartshorne's exercise II.7.10 part (c), we're asked to do the following, where $X$ is a noetherian scheme: $\renewcommand{\PP}{\Bbb P}\renewcommand{\cE}{\mathcal{E}}\renewcommand{\cO}{\mathcal{O}}\renewcommand{\cL}{\mathcal{L}}\renewcommand{\Spec}{\operatorname{Spec}}$ Assume that $X$ is regular, and show that every $\PP^n$ -bundle $P$ over $X$ is isomorphic to $\PP(\cE)$ for some locally free sheaf $\cE$ on $X$ .
[ Hint : Let $U\subset X$ be an open set such that $\pi^{-1}(U)\cong U\times\PP^n$ , and let $\cL_0$ be the invertible sheaf $\cO(1)$ on $U\times\PP^n$ .
Show that $\cL_0$ extends to an invertible sheaf $\cL$ on $P$ .
Then show that $\pi_*\cL=\cE$ is a locally free sheaf on $X$ and that $P\cong \PP(\cE)$ .]
Can you weaken the hypothesis ' $X$ regular'? Here's Hartshorne's definition of a projective bundle from earlier in the question: By analogy with the definition of a vector bundle (Ex. 5.18), define the notion of a projective $n$ - space bundle over $X$ , as a scheme $P$ with a morphism $\pi:P\to X$ such that $P$ is locally isomorphic to $U\times \PP^n$ , $U\subset X$ open, and the transition automorphisms on $\Spec A\times\PP^n$ are given by $A$ -linear automorphisms of the homogeneous coordinate ring $A[x_0,\cdots,x_n]$ (e.g., $x'_i=\sum a_{ij}x_j$ , $a_{ij}\in A$ ). I do not understand the hint about extending $\cL_0$ to an invertible sheaf on $P$ . My attempt so far seems to sort of blow by this, and I'm not confident it's correct - if it were true, it would seem that I can completely remove the regularity condition, which seems a little drastic (I would expect that we at least need something like regular in codimension one or locally factorial). Here's what I have so far: Take $X$ connected (else go component-by-component). Cover $X$ by affine schemes $U_i$ so that $\pi^{-1}(U_i)\cong U_i\times\PP^n$ , by the definition of a $\PP^n$ -bundle. Let $\cL_i=\cO(1)$ on $U_i\times\PP^n$ : then $(\pi|_{U_i\times\PP^n})_*\cL_i$ is a free module of rank $n+1$ on each $U_i$ . For any $\Spec A$ which lies in both $U_i$ and $U_j$ , we get that $(\pi|_{\Spec A\times\PP^n})_*\cL_i\cong(\pi|_{\Spec A\times\PP^n})_*\cL_j$ by the $A$ -linear automorphism restricted to the degree-one piece of the homogeneous coordinate ring. We can then use these isomorphisms to glue the $(\pi|_{U_i\times\PP^n})_*\cL_i$ in to a locally free sheaf $\cE$ on $X$ . Then $\PP(\cE)\cong P$ as the gluing data for $\PP(\cE)$ is exactly the gluing data coming from $P$ . Like I said, I'm worried I'm making an error (especially because this problem is starred, and my solution seems too short). Can anyone point out where this error might be, or assuage my concerns?","['algebraic-geometry', 'solution-verification']"
3988543,"Finding MLE of $\mu_1,\mu_2,\Sigma$","Let $X_1,...,X_{n_1}$ be an i.i.d. sample from $N_p(\mu_1,\Sigma)$ and let $Y_1,...,Y_{n_2}$ be an independent sample from $N_p(\mu_2,\Sigma)$ , for some $\mu_1,\mu_2 \in \mathbb{R}^p$ and some invertible, $p\times p$ positive definite matrix $\Sigma$ . I'd like to find the likelihood function $L(\mu_1,\mu_2,\Sigma)$ of the commbined sample: In my book, the likelihood function of $X_1,...,X_n \sim N_p(\mu,\Sigma)$ is given by $$\frac{1}{(2\pi)^{np/2}\text{det}(\Sigma)^{n/2}}\exp\biggl(-1/2\bigl(\sum^n_{i=1}(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\bigr)\biggr)$$ So, $$L(\mu_1,\mu_2,\Sigma)=$$ $$\frac{1}{(2\pi)^{p(n_1+n_2)/2}\text{det}(\Sigma)^{\frac{n_1+n_2}{2}}}\exp\biggl(-1/2\sum^{n_1}_{i=1}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)-1/2\sum^{n_2}_{i=1}(y_i-\mu_2)^T\Sigma^{-1}(y_i-\mu_2)\biggr)$$ And so now I would like to find the MLE for $\mu_1,\mu_2,\Sigma$ : Set $$
\bar{x}:=\frac{1}{n_1}\sum_{i=1}^{n_1}x_{i} \quad\text{and}\quad S_x:=\sum_{i=1}^{n_1}(x_{i}-\bar{x})(x_{i}-\bar{x})^{\top},
$$ and, similarly, $\bar{y}$ and $S_y$ . Now I take the log of $\mathcal{L}$ : \begin{align}
\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)&=-\frac{(n_1+n_2)p}{2}\ln(2\pi)+\frac{(n_1+n_2)}{2}\ln|\Sigma^{-1}| \\
&\quad-\frac{1}{2}\operatorname{tr}(\Sigma^{-1}(S_x+S_y)) \\
&\quad-\frac{n_1}{2}(\bar{x}-\mu_1)^{\top}\Sigma^{-1}(\bar{x}-\mu_1)-\frac{n_2}{2}(\bar{y}-\mu_2)^{\top}\Sigma^{-1}(\bar{y}-\mu_2)
\end{align} To find MLE for $\mu_1$ , I take the derivate of $\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)$ w.r.t $\mu_1$ : $$\frac{d}{d\mu_1}\ln\mathcal{L}(\mu_1,\mu_2,\Sigma)=0$$ $$\frac{n_1}{2}(\bar{x}-\mu_1)^T\bigl((\Sigma^{-1})^T+\Sigma^{-1}\bigr)\frac{d}{d\mu_1}(\bar{x}-\mu_1)=0$$ $$-\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1})=0$$ $$\iff\bar{x}=\mu_1$$ So MLE for $\mu_1=\bar{x}$ . So, $$-\frac{n_1}{2}(\bar{x}-\mu_1)^T(2\Sigma^{-1})$$ is maximized when $\mu_1=\bar{x}$ since $\Sigma^{-1}$ is positive definite? So, I'm not sure whether I'm doing this correct. Could someone please correct me if possible?","['statistics', 'maximum-likelihood']"
3988555,Generalisation of an expression.,"I have been solving matrices questions lately and this pattern keeps showing up: Given matrix $ P = \begin{bmatrix}
1 & 0 & 0\\
4 & 1 & 0\\
16 & 4 & 1\\
\end{bmatrix}$ and $I$ be identity matrix of order 3. If $[Q] = [q_{ij}]$ is a matrix such that $P^{50}-Q= I$ then $\frac{q_{31}+q_{32}}{q_{21}} = ?$ While solving I found this pattern for a general matrix of the type: $$L = \begin{bmatrix}
1 & 0 & 0\\
a & 1 & 0\\
b & a & 1\\ 
\end{bmatrix}$$ As $$L^n = \begin{bmatrix}
1 & 0 & 0\\
a×n & 1 & 0\\
K & a×n & 1\\
\end{bmatrix}$$ Where $K = (L_{31})_{{n-1}}+a^2×(n-1) + b$ If I have to figure out $L_{50}$ using this algorithm, I need to find $L_{49}, L_{48}$ and so on which basically defeats the purpose. How do you reiterate the value of $L_{n-1}$ so that I can use directly the value of $b$ to calculate $L_{31}$ of any power of matrix?","['matrix-equations', 'pattern-recognition', 'sequences-and-series']"
3988562,Question on $\mathbb E_{X_{t_{n-2}}}[1_{B_{n-1}}(X_{t_{n-1}-t_{n-2}})\mathbb E_{X_{t_{n-1}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]]$,"I have a question on these lecture notes: http://page.math.tu-berlin.de/~scheutzow/WT3main.pdf Page 46-47 Lemma 4.15 In this proof, we use, amongst other things that: $\mathbb E_{x}[1_{B_{n-1}}(X_{t_{n-1}})\mathbb E_{X_{t_{n-1}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]\lvert \mathcal{F}_{t_{n-2}}]=\mathbb E_{X_{t_{n-2}}}[1_{B_{n-1}}(X_{t_{n-1}-t_{n-2}})\mathbb E_{X_{t_{n-1}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]](*)$ which comes from the Markov Property which states for any and bounded measurable function $f$ we have: $\mathbb E_{x}[f((X_{t+h})_{t\geq 0})\lvert \mathcal{F}_{h}]=\mathbb E_{X_{h}}[f((X_{t})_{t\geq 0})]$ But my issue in $(*)$ would be that in our case the function $f$ is indeed: $f(X_{t_{n-1}})=1_{B_{n-1}}(X_{t_{n-1}})\mathbb E_{X_{t_{n-1}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]$ and thus $\mathbb E_{x}[f(X_{t_{n-1}})\lvert \mathcal{F}_{t_{n-2}}]=\mathbb E_{X_{t_{n-2}}}[f(X_{t_{n-1}-t_{n-2}})]=\mathbb E_{X_{t_{n-2}}}[1_{B_{n-1}}(X_{t_{n-1}-t_{n-2}})\mathbb E_{X_{t_{n-1}-t_{n-2}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]]$ Note the difference in this computation compared to $(*)$ . My question is rather why would $\mathbb E_{X_{t_{n-1}}}[1_{B_{n}}(X_{t_{n}-t_{n-1}})]$ remain the same when evaluated under $\mathcal{F}_{t_{n-2}}$ even though it is a function of $f(X_{t_{n-1}})$ . Is this a mistake or am I simply missing something?","['stochastic-calculus', 'markov-chains', 'stochastic-processes', 'probability-theory', 'probability']"
3988565,Prove or disprove that the matrix is invertible,"Let $P$ be a $n \times n $ matrix with integer entries. Let $q$ be a non integer and $Q = P + qI$ where $I$ is identity matrix. Prove or disprove that $Q$ is invertible. This is easy if $P$ is a $2 \times 2$ matrix. In this case it is easy to see that determinant of $Q$ is non zero. Let $P$ be a matrix \begin{pmatrix}
a & b \\
c & d \\
\end{pmatrix} then we see that $\det Q = (a + q)(d + q) - bc$ . If $q$ is an irrational then the determinant is obviously non zero. If $q$ is a rational then let $q = \frac mn$ where gcd of $m$ and $n$ is $1$ . Now it is easy to see that if $$
(an+m)(dn+m) = n^2 bc
$$ then any prime which divides $n$ doesn't divide $m$ which implies that it doesn't divide LHS as well. So determinant of $Q$ is non zero so we conclude that $Q$ is invertible. Now how to prove for higher dimension matrices?","['matrices', 'number-theory', 'inverse']"
3988566,"If $(X,d)$ is a compact metric space then is $A$ a closed subset of $K$?","Since $(X,d)$ is compact it is totally bounded so we will have a finite set $A_n$ such that $\displaystyle\bigcup_{x \in A_n} B_d\left(x,\frac{1}{n}\right)$ covers $(X,d)$ . Now we take $A$ as the union of these $A_n$ . Is $A$ closed?","['metric-spaces', 'analysis', 'compactness']"
3988591,Two-index Hajek-Renyi type inequality,"Consider a sequence of mean-zero independent random variables $\{X_i\}_{i=1}^\infty$ and the partial sum $S(n)=\sum_{i=1}^n X_i$ . The Hajek-Renyi inequality provides a upper bound of the normed partial sum: $$\mathbb{P}\left(\max_{n\le t \le m} c_t \lvert S(t) \rvert \ge \epsilon \right) \le \frac{1}{\epsilon^2} \left(c_n^2 \sum_{k=1}^n D_k^2 + \sum_{k=n+1}^m c_k^2 D_k^2\right),$$ where $D_k^2 = \mathrm{Var}(X_k)$ and $\{c_k\}$ is a positive non-increasing sequence. If we choose $c_k = \frac{1}{\sqrt{k}}$ and $D_k^2 = \sigma^2$ , $$\mathbb{P}\left(\max_{n\le t \le m} \frac{1}{\sqrt{t}} \lvert S(t) \rvert \ge \epsilon \right) \le \frac{1}{\epsilon^2} \left(\sigma^2 + \sum_{k=n+1}^m  \sigma^2/k\right) \approx \frac{1}{\epsilon^2} \left(\sigma^2 + (\log(m) - \log(n)) \sigma^2\right) \le \frac{[1+\log(m)] \sigma^2}{\epsilon^2}.$$ I am now considering a similar setting where the maximum is taken at two indexes. Is the following probability $$\mathbb{P}\left(\max_{n\le t < k \le m} \frac{1}{\sqrt{k - t}} \lvert S(k) - S(t) \rvert \ge \epsilon \right),$$ also preserves a $\log(m)$ -ordered upper bound? Or more generally, with general sequence $c_k$ , I am interested in the upper bound of $$\mathbb{P}\left(\max_{n\le t < k \le m} c_{k - t} \lvert S(k) - S(t) \rvert \ge \epsilon \right).$$ Update: Solution draft I have made some effort to this question for $c_k=1/\sqrt{k}$ . Firstly we may reformulate the problem as $$\mathbb{P}\left(\max_{1\le t < k \le m} \frac{1}{\sqrt{k - t}} \lvert S(k) - S(t) \rvert \ge \epsilon \right) = \mathrm{O}(\log(m)).$$ Then divide the maximand into two parts: $$\max_{1\le t < k \le m} \frac{1}{\sqrt{k - t}} \lvert S(k) - S(t) \rvert \le \max_{a_m < t + a_m < k \le m} \frac{1}{\sqrt{k - t}} \lvert S(k) - S(t) \rvert + \max_{1\le t < k \le m \& k \le t + a_m} \frac{1}{\sqrt{k - t}} \lvert S(k) - S(t) \rvert.$$ For sufficient large $a_m$ , e.g. $a_m = \mathrm{O}(\log(m))$ , we can treat the first part as gaussian (by Berry-Essen bound). But what about the second part?","['probability-limit-theorems', 'probability-theory', 'integral-inequality']"
3988613,Computing the distance between vector and its projection on a random subspace,"Let $V\in\mathbb{R}^{n\times m}$ , where $n>m$ , be a random matrix of standard normal gaussians.
Given an arbitrary vector $y\in\mathbb{R}^n$ , I need to understand the distance $||y-p_V(y)||^2$ , where $p_V$ is the projection onto the range of $V$ , meaning $p_V = V(V^TV)^{-1}V^T$ as a matrix.
Through a slightly different lens, I would like to understand the distance of $y$ from $R(V)$ . Given that $V$ is random, this makes the above distance a random variable whose distribution (or at least expected value) is important to my research. This feels like a not-so-novel problem but I couldn't find a solution on my own. How can I approach this? Thanks in advance!","['projection', 'linear-algebra', 'random-matrices', 'projection-matrices', 'probability']"
3988622,Universal set as a collection of objects,"The universal set $U$ can be defined as $U = \{x : x=x\}$ , which is equivalent to $\forall x[x \in U \leftrightarrow x = x]$ . Since $x=x$ is a tautology, then we infer $\forall x \in U$ . This definition tells us that every free variable in the scope of the universal quantifier is an element of $U$ . Since $x \in U$ means $\{x\} \subseteq U$ , then $\forall x \in U$ could be rewritten as $\forall x[\{x\} \subseteq U ]$ or $\forall X[X \subseteq U]$ . Therefore, every set $X$ is a subset of the universal set $U$ . However, I've seen in some proofs and commentaries cases which define the universal set as a finite collection of objects, for example $U = \{ x_0, x_1\}$ . This formula is equivalent to $\forall x[x \in U \leftrightarrow x \in \{x_0, x_1\}]$ and to $\forall x[x \in U \leftrightarrow x = x_0 \lor x=x_1\}]$ . Compatibilization problems with the classical definition $\forall x [x \in U \leftrightarrow x=x]$ arise when we substitute $x$ by a free variable that is not $x_0$ and $x_1$ . For example, $x_2 \in U \leftrightarrow x_2=x_0 \lor x_2=x_1$ returns $x_2 \notin U$ , while $x_2 \in U \leftrightarrow x_2=x_2$ is equivalent to $x_2 \in U$ . Thus, we have an apparent contradiction between the standard definition of universal set and its definition as a finite collection of objects. I was inclined to think that my mistake would lie on the impossibility of universally instantiating the formula $\forall x[x \in U \leftrightarrow x = x_0 \lor x=x_1\}]$ with a variable $x_2$ that is not in the universal set $\{ x_0, x_1\}$ . To formalize such notion, we would have to explicit that the domain of this formula itself is ""limited"" to the defined universal set, thus considering it equivalent to $\forall x \in U[x \in U \leftrightarrow x = x_0 \lor x=x_1\}]$ . Therefore, we would have by universal instantiation $x_2 \in U \rightarrow [x_2 \in U \leftrightarrow x_2=x_0 \lor x_2=x_1]$ , which is a tautology, since $x_2 \notin U$ . But then again, from the formula $\forall x[x \in U \leftrightarrow x=x]$ , we would conclude $x_2 \in U$ , so the contradiction remains. Is the universal set not definable as a collection of finite objects?","['elementary-set-theory', 'quantifiers', 'logic']"
3988629,"The PDF of $X$ is $\frac{1+\theta x}{2}$ for $x\in [-1,1], \theta \in[-1,1]$. Is the Method of Moments estimator of $\theta$ consistent?","We know that the method of moments estimator, $\hat{\theta}_n(\textbf{X})$ , is consistent if $$\lim_{n\to \infty}\mathbb{P}(|\hat{\theta}_n(\textbf{X})-\theta|>\epsilon)=0$$ . We know that $\mathbb{E}(X;\theta)=\frac{1}{2}\int_{-1}^1(x+\theta x^2) dx=\frac{\theta}{3}$ , so $\hat{\theta}_n(\textbf{X})=3\bar{X}$ . Therefore $\lim_{n\to \infty}\mathbb{P}(|\hat{\theta}_n(\textbf{X})-\theta|>\epsilon)= \lim _{n\to \infty}\mathbb{P}(|3\bar{X}-\theta|>\epsilon) $ . At this point I'm not sure how to proceed? I've tried using Chebyshev's inequality but this doesn't lead anywhere. I know that if a sequence in random variables converges in distribution to a constant then that is equivalent to  a sequence in random variables converging in probability to that constant. Now $\lim_{n\to \infty}\mathbb{P}(\hat{\theta}_n(\textbf{X})\leq x)= \lim_{n\to \infty}\mathbb{P}(3\bar{X}\leq x)$ and I don't think this equals $\mathbb{P}(\theta\leq x)$ ? So would I be right in saying the method of moments estimator is not consistent?","['statistical-inference', 'statistics', 'probability-distributions', 'probability-theory']"
3988698,Categorical description of matrix similarity,"I am wondering if there is a nice categorical description of matrix similarity, i.e. the equivalence relation on matrices given by $A\sim B \iff A=QBQ^{-1}$ , for some invertible $Q$ . In particular, I am considering the matrices as linear maps from a vector space into itself, which in turn forms a category $\mathrm{End}(V)$ with one object $V$ and arrows linear maps. We can then form the functor category $\mathrm{End}(V)^{\mathbb2},$ where $\mathbb2$ is the category with $2$ objects and $1$ arrow between them, i.e. this is the 2-category of linear maps and pairs of maps between them making the following commute: \begin{array}{ccccccccc} V & \xrightarrow{A} & V \\
\downarrow &   &  \downarrow \\
V & \xrightarrow{B} & V \end{array} So similarity between $A$ and $B$ is stronger than isomorphism in this category, as $(C,D):A\rightarrow B$ is an isomorphism if and only if $C$ and $D$ are linear isomorphisms, whereas similarity would require $C=D$ . Do similar matrices in this category obey some characterising or otherwise interesting property, or is isomorphism the strongest we can get?","['universal-property', 'linear-algebra', 'category-theory']"
3988719,How many subgraphs can be formed from a graph having $e$ sides?,"A couple of days ago I was asked this question, and with no prior experience of Graph Theory I attempted to come up with a solution , $H(\alpha, \beta)$ is a subgraph of $G(V, E)$ if and only if $\alpha \subseteq V$ and $\beta \subseteq E$ . $e$ as the edge count of graph $G$ represents the cardinality of the edge set of $G$ , i.e. $E$ . Since a graph must have at least one vertex but can contain $0$ edges, $\phi$ can represent an edge set as well. Hence, the total number of subgraphs of $G$ is the cardinality of $P(E)$ where $P$ is the power set of a given set. $$|P(E)| = 2^e$$ $$\therefore \text{the number of subgraphs is } 2^e$$ If this solution is not correct, please let me know of any pointers or where in the concept I went wrong here, I'm soon going to start Graph Theory and I haven't found any solution to this current problem anywhere (Web search and Discrete mathematics and its applications by Rosen) yet.","['graph-theory', 'solution-verification', 'discrete-mathematics']"
3988732,"A matrix with all entries being positive, each row has the same sum.","A matrix $A$ with all entries being positive, each row has the same sum, that is, $\sum_j a_{ij}=c>0$ . Show that up to constant multiplier, the eigenvector $(1,\cdots,1)'$ is the unique eigenvector with all entries being positive. Let $x=(x_1,\cdots,x_n)'$ being the eigenvector of $A$ , with eigenvalue $\mu>0$ , $x_i$ being not identically the same, How to derive a contradiction?","['linear-algebra', 'eigenvalues-eigenvectors']"
3988744,Find the Maximum Trigonometric polynomial coefficient $A_{k}$,"Let $n,k$ be given positive integers and $n\ge k$ . Let $A_i, i=1, 2, \cdots, n$ be given real numbers. If for all real numbers $x$ we have $$A_{1}\cos{x}+A_{2}\cos{(2x)}+\cdots+A_{n}\cos{(nx)}\le 1$$ Find the maximum value of $A_{k}$ . I don't know if this question has been studied If $n=2$ it is easy to solve it.","['trigonometric-series', 'algebra-precalculus', 'trigonometry', 'inequality']"
3988766,Ultralimits vs limits of subsequences,"Let $(x_n)_{n \geq 1} \subset \mathbb{R}$ be a sequence of real numbers, and let $\omega \subset \mathcal{P}(\mathbb{N})$ be a non-principal ultrafilter. We say that $x$ is an ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ if for all $\varepsilon > 0$ we have $\{ n \geq 1 : |x - x_n| < \varepsilon \} \in \omega$ . Suppose that there exists a set $\{ n_k : k \geq 1 \} \in \omega$ such that the subsequence $(x_{n_k})_{k \geq 1}$ has converges to $x$ (in the standard sense). Then it follows that $x$ is the ultralimit $(x_n)_{n \geq 1}$ along $\omega$ , since $\omega$ is non-principal and so taking away finitely many elements from $(n_k)_{k \geq 1}$ we are still in $\omega$ . Conversely, if $x$ is the ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ , we can find a subsequence $(x_{n_k})_{k \geq 1}$ converging to $x$ , for instance we can choose inductively $n_k > n_{k-1}$ such that $n_k \in \{ n \geq 1 : |x_n - x| < 1/k \}$ . Although this subsequence is constructed using sets in $\omega$ , it is not clear to me that the resulting set $\{ n_k : k \geq 1 \}$ should be in $\omega$ . So my question is: if $x$ is the ultralimit of $(x_n)_{n \geq 1}$ along $\omega$ , can we always choose a subsequence $(x_{n_k})_{k \geq 1}$ converging to $x$ so that $\{ n_k : k \geq 1 \} \in \omega$ ?","['filters', 'limits', 'functional-analysis', 'sequences-and-series']"
3988770,Number of roots of the equation $f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0$,"I have the following question before me:
Find the number of roots of the equation $f(x)= \int_0^x (t-1)(t-2)(t-3)(t-4)dt =0$ in the interval $[0,5]$ . $0$ is clearly one of the roots. But how can I find other roots, if any? I tried evaluating the integral and came up with a fifth degree polynomial in $x$ having no constant term. The equation seemed quite daunting to me. How can I get the roots quicker? Please suggest.","['integration', 'definite-integrals', 'derivatives']"
3988775,"Complex Analysis: If f is an odd function, find an expression for f.","Complex Analysis Suppose that $f$ is a holomorphic function in $\mathbb{C}\setminus\{0\}$ and satisfies $$\vert f(z) \vert \leq \vert z \vert ^{2} +\frac{1}{\vert z \vert ^{2}}$$ If f is an odd function, find an expression for f. I thought about calculating the Laurent Serie of f, but I can't calculate Laurent's series explicitly (there is no way to calculate the coefficients), Dai thought about estimating, using the invariance of the circle to know which one cancels out. I cannot develop this question",['complex-analysis']
3988781,Real numbers $a$ and $b$ satisfy $a^3+b^3-6ab=-11$. Prove that $-\frac{7}{3}<a+b<-2$,Real numbers $a$ and $b$ satisfy $a^3+b^3-6ab=-11$ . Prove that $$-\frac{7}{3}<a+b<-2$$ I have shown that $a+b<-2$ . My approach: $-3=8-11=a^3+b^3-6ab+2^3=\frac{1}{2}(a+b+2)((a-b)^2+(a-2)^2+(b-2)^2)$ . From this we must have that $a+b<-2$ . Please give some idea/hint for the other part.,"['algebra-precalculus', 'polynomials', 'inequality']"
3988808,cardinality of a set of finite sequences of natural numbers [duplicate],"This question already has answers here : $Seq (\mathbb{N})$ of all finite sequences of elements of $\mathbb{N}$ is countable. (2 answers) Closed 3 years ago . I recently got into set theory and i was wondering what is the cardinality of a set of all finite sequences of natural numbers?
I know that it is N for natural numbers and 2^N is for real numbers but how can i prove it?","['set-theory', 'discrete-mathematics']"
3988811,Characterizing (stationary) points by the number of valleys one can descent into,"In non-convex optimizing of more than 2 times differentiable $f: \mathbb{R}^2 \mapsto \mathbb{R}$ we can encounter saddle points that have multiple valley one could descent into. At $(0,0)$ there are two direction of descent: (0,-1) and (0,1). However the number of valley one could of descent can be larger than the number of dimensions as the the monkey saddle illustrates: I want to distinguish descending into different valleys and from the number of directions as descent. If we look at a Unit Box of the Rastrigin function we see that the local maxima at (?.5,?.5) can descent in any direction but the axis aligned directions lead into saddle points, if we ignores those we have non connected open sets corresponding to each attractor of a ""not getting stuck in saddle points"" optimizers (Newton method, sufficiently perturbed gradient descent, ...). What is the name for the concept that would assign $(0,0)$ of the normal saddle a 2, the monkey saddle at (0,0) a 3 and any point (?.5, ?.5) a 4 on the Rastrigin function? Context: I am motivated to investigate this property over non stationary points. However i need a name for it first to see if there is existing literature. Any point along the x-axis of normal saddle point could be assigned a two, any point of the (strictly) positive x-axis of the monkey saddle could be assigned a 2 too. Any point of with only one component ending in .5 on the Rastrigin function would be a 2 too. I suspect that for an $f: \mathbb{R}^n \mapsto \mathbb{R}$ there exist a $\mathbb{R}^{n-1}$ dimensional (non connected) pseudo(?) manifold the contains all points that have more than two valleys next to them. All points which have exactly two neighboring valleys are part of n-dimensional hyper surfaces which divide different basins of attractions of local optima. Such surfaces might meet with other surfaces to from ""edges"" connecting the surfaces. I hope that finding that partioning structure can be used to accelerate global optimization of non-convex functions. A related but more Greedy (descents and forks but doesn't ascend) approach called "" Ridge Rider "" is already known in the literature and used to find more diverse solutions to optimization problems.","['reference-request', 'multivariable-calculus', 'non-convex-optimization', 'perturbation-theory', 'stationary-point']"
3988820,Calculate the following complex integral,"I try to calculate the following integral: $$\oint_{\{\vert z\vert=3\}}\frac{e^z}{(z-1)(z-2i)}$$ The integral encloses 2 poles in $1$ at $2i$ . Using the homotopy we see that expect the two circle of radius $\epsilon$ around both pole, the rest is cancelled out. Now my idea is to try to split the integral into something like $$\oint_{\{\vert z\vert=3\}}f(z)dz+\oint_{\{\vert z\vert=3\}}f(z)dz$$ and use the cauchy integral formular but I go stucked... EDIT: If I write now $$
\frac{1}{(z-1)(z-2i)}=\frac{A}{z-1}+\frac{B}{z-2i}\ .
$$ as in the answer and I solve it. Then we get $A=-B$ and $A=-\frac{1}{2i-1}$ . Okay now we get $$A\oint_{\{\vert z\vert=3\}}\frac{e^z}{z-1}-A\oint_{\{\vert z\vert=3\}}\frac{e^z}{z-2i}$$ By the Cauchy Formular this is equal to: $$2\pi iA(e^1+e^{-2i})$$ Is this right?",['complex-analysis']
3988884,On the asymptotic bound for $\arg\zeta(s)$ on the critical line,"I am currently trying to prove $$
N(T)={T\over2\pi}\log{T\over2\pi}-{T\over2\pi}+\mathcal O(\log T)
$$ in which $N(T)$ denotes the number of $\zeta$ 's nontrivial zeros with imaginary part between $(0,T]$ . Currently, using symmetric properties of $\xi(s)$ , I am able to obtain $$
N(T)={T\over2\pi}\log{T\over2\pi}-{T\over2\pi}+\frac78+\frac1\pi\arg\zeta\left(\frac12+iT\right)+\mathcal O\left(\frac1T\right)
$$ Apparently, the remaining job is to show that the argument of $\zeta$ on the critical line is of logarithmic growth, and I become stuck on interpreting the meaning of $\arg\zeta$ . According to H. M. Edwards' Riemann's zeta function , this argument is bounded by the number of zeros of $\Re\zeta(s)$ on a certain curve (section 6.7 of his book), and I wonder if anybody could provide a more intuitive and clear explanation on that. Thank you!","['complex-analysis', 'riemann-zeta', 'analytic-number-theory', 'asymptotics']"
3988889,Right/Left inverse mapping [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question Can someone clearly explain the difference between right and left inverse mapping, if $f: X \to Y$ and $g: Y \to X$ in set theory? I've read the definitions, but I still have no awareness of what is it, their difference, and how if differs from just inverse mapping.","['elementary-set-theory', 'functions']"
3988893,"$T(x_1,x_2,x_3,x_4,\ldots)=\left(x_2,\frac{x_3}{2},\frac{x_4}{3},\frac{x_5}{4},\ldots\right)$, spectral radius and spectrum","Define a pair of operators $S\colon\ell^2 \rightarrow \ell^2$ and $L\colon\ell^2 \rightarrow \ell^2$ (of sequences of complex or real numbers) as follows: $S(x_1,x_2,x_3,x_4,\ldots)=\left(x_1,\frac{x_2}{2},\frac{x_3}{3},\frac{x_4}{4},\ldots\right)$ $L(x_1,x_2,x_3,x_4,\ldots)= (x_2,x_3,x_4,\ldots)$ Let $T=S \circ L$ , so $T(x_1,x_2,x_3,x_4,\ldots)=\left(x_2,\frac{x_3}{2},\frac{x_4}{3},\frac{x_5}{4},\ldots\right)$ and $T^n(x_i)_{i\in\Bbb N}=\left(\frac{(i-1)!}{(i+n-1)!}x_{i+n}\right)_{i\in\Bbb N}$ if I'm not wrong. I know that $S$ is compact and $L$ is continuous, therefore T is compact. I know the norm of $T$ is $1$ . How do I calculate the norm of $T^n$ , and the spectral radius and the spectrum of $T$ ? Thank you in advance. My attempt for the norm of $T^n$ : I've tried to find a constant $K$ such that $\|T^n(x)\| \leq K \|x\|\ $ and an $x_0\in\ell^p$ such that $\|x_0\|=1$ , $\|T^n(x_0)\|=K$ , what would prove that $\|T^n\|=K$ . That worked for $n=1$ but I don't know how to do it for the general case. Easier alternatives are welcome.","['normed-spaces', 'spectral-radius', 'lp-spaces', 'functional-analysis', 'spectral-theory']"
3988923,How do I prove the set of the equivalence classes of $R$ has the same cardinality as the set of all finite sets of primes? [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed 3 years ago . Improve this question I have been stuck on this for a while now. Given the equivalence relation $R$ over $\mathbb{Z^+}: aRb \leftrightarrow \exists q \in \mathbb{Q}(\frac{a}{b} = q^2) $ how does one prove that the set of its equivalence classes $B$ has the same cardinality as the set of all finite sets of primes $C$ . What I am looking for, is some kind of proof that $B$ is a countable set. I tried to come up with a mapping function between the two, compare them both to something else but everything was without any meaningful result. Any help is greatly appretiated.","['equivalence-relations', 'relations', 'discrete-mathematics']"
3988978,Show that $\frac{46!}{48}+1$ is not a power of 47,"The original exercise was: Prove that there is no non-negative integers such that $m!+48=48(m+1)^n$ I could type something, but I've stopped at the moment I had to prove that that $\frac{46!}{48}+1$ is not a power of 47.Could anyone help me? I'll post my solution below: Assume that there is such $m,n$ , This, we must have that $48 \mid m!$ , where $m$ must be at least 6. Note that $\frac{6!+48}{48} = 46 \neq 7^n$ and so, $m\geq 7$ . By the same way, we have that $m \neq 7$ and we find $m \geq 8$ . Note that, in this case, $6 \mid \frac{m!}{48}$ , i.e, $$1=6 \wedge \frac{m!}{48}+1 = 1 = 6 \wedge (m+1)^n$$ and, so, $6 \text { and } m+1$ are coprimes.
If we had $m+1$ compound, then, there is a prime $p >3$ , since both $2 \text{ and } 3$ do not divide $m+1$ , such that $p \mid m+1$ . By the other hand, once $48=2^4.3$ , we have that $p \mid m!$ , but $p \nmid m!+48$ , which is an absurd, since $p \mid m+1$ . Then, $m+1$ is prime and, by Wilson's Theorem, we have that $m+1 \mid m!+1$ . Using this last argument with $m+1 \mid m!+48$ , we reach that $m+1 \mid 47 \Rightarrow m=46$ .
Now, remains to be shown that $\frac{46!}{48}+1$ is not a power of $47$","['number-theory', 'prime-numbers']"
3989016,Show that $\left\lceil x-\frac{1}{2} \right\rceil$ is the closest integer to the number $x$,"Show that $\left\lceil x-\frac{1}{2} \right\rceil$ is the closest integer to the number $x$ , except when $x$ is midway between two integers $n$ and $n+1$ , when it is the smaller of these two integers. We want to prove 2 cases here: When $x$ is midway between the two integers, $\left\lceil x-\frac{1}{2} \right\rceil$ is equal to the smaller of the two integers. When $x$ is not midway between the two integers, $\left\lceil x-\frac{1}{2} \right\rceil$ is equal to the closest integer to the number $x$ . If we let $x$ be midway between the two integers $n$ and $n+1$ , then $$x = n + \frac{(n+1)-n}{2} = n+ \frac{1}{2}$$ Now, substitute back in $x$ in $\left\lceil x-\frac{1}{2} \right\rceil$ , $$\left\lceil n+ \frac{1}{2} - \frac{1}{2}\right\rceil  = \left\lfloor n \right\rfloor$$ The problem is why the ceiling function was changed to the floor function?","['ceiling-and-floor-functions', 'discrete-mathematics']"
3989024,"Prove that $(K,\delta)$ is a compact metric space.","Let $(X, d)$ be a compact metric space. For $x ∈ X $ and $\epsilon > 0$ , define { $B_{\epsilon}(x) := {y ∈
X | d(x, y) < \epsilon}$ }. For $C ⊆ X$ and $\epsilon > 0$ , define $B_{\epsilon}(C) := ∪_{x∈C}B_{\epsilon}(x)$ . Let K be the
set of non-empty compact subsets of X. For $C, C_0 ∈ K$ , define $δ(C, C_0) = $ inf{ $\epsilon| C ⊆ B_{\epsilon}(C_0)$ and $C_0 ⊆ B_{\epsilon}(C)$ }. Assuming that it forms a metric space show that $(K, δ) $ is a compact metric space. My attempt: The metric space $(X,d)$ is compact so it is totally bounded.
Let $B=$ { $C_k$ } be a sequence of compact metric spaces in $K$ . We need to show that the infinite set has a limit point. $(X, d)$ is totally bounded so let $A_1$ ={ $x_{1,1},x_{2, 1},....,x_{k_1,1}$ } such that ${\cup {B_d(x_i,1) }} $ covers X for $i= (1,1),..,(k_1,1)$ and ${\cup {B_d(x_i,\frac{1}{2}) }} $ covers X for $i= (1,2),..,(k_2,2)$ ,so $A_2= x_{1,2},x_{2,2},...,x_{k_2,2}$ So we can generate a new sequence of compact metric space $A_1,A_2,...,A_n$ as mentioned above.Now,let { $\cup {(A_i)}$ }= $A$ .We take the closure $cl(A)$ which is the closed subset of $(X,d)$ .Hence it is compact so it is in $(K,\delta)$ . $(B_{\delta}(cl(A) \cap B)  /cl(A))\ne \phi$ for all $\epsilon>0$ is what we have to show to prove that $cl(A)$ is the limit point of the sequence { ${C_k}$ } to show that the sequence has bolzano weistrass property. We pick an $\epsilon >0$ and proceed as , Now, $C_i \subset B_{\frac{1}{N_1}}(cl(A))$ where $\frac{1}{N_1} < \frac{\epsilon}{2}$ and $C_i \subset B_{\epsilon}(cl(A))$ where $C_i$ is any compact subset in the sequence { $C_k$ } and $C_i$ can be chosen to be different from $cl(A)$ . Now, let $x \in Cl(A)$ then $x \in X$ and $x \in \cup B_d(x_i,\frac{1}{N_1})$ where $i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1)$ then $d(x,x_i) < \frac{1}{N_1}$ and let $x_n \in C_i$ then $d(x_i,x_n) < \frac{1}{N_1}$ so $d(x,x_n) < \epsilon$ and the other case should also follow when $ x = x_i $ where $i=(1,N_1),(2,N_1),(3,N_1),...,(k_{N_1},N_1)$ .
So $cl(A) \subset B_{\epsilon}(C_i)$ . Then $\delta(C_i,cl(A)) < \epsilon$ which allows me to conclude that $cl(A)$ is the limit point of the above mentioned seqeunce (as $\epsilon$ can be chosen arbitrarily).So by bolzano property I can claim the compactness. Is my attempt ok? I have tried hard so that the notations are understandable. Thanks in advance.","['metric-spaces', 'analysis', 'compactness']"
3989061,"The languages $A, B \in \Sigma^*$ are two semi-decidable but not decidable languages. Show that $A \cup B$ is semi-decidable.","The languages $A, B \in \Sigma^*$ are two semi-decidable but not decidable languages. Show that $A \cup B$ is semi-decidable. If a language is semi-decidable then it is also recursively enumerable. And vice-verse. Therefore, if I can show that $A \cup B$ is recursively enumerable, then it is also semi-decidable. A recursively enumerable language must be total and computable. Here is my attempt: We define the following sets: $$f_A(\mathbb{N}) = \{f_A(0), f_A(1),f_A(2),...\}$$ $$f_B(\mathbb{N}) = \{f_B(0), f_B(1),f_B(2),...\}$$ $$f_{A \cup B}(\mathbb{N}) = A \cup B = ...$$ The problem here could be that the number of words in $A$ might be infinite, therefore you would never actually start enumerating $B$ . The solution would thus be to alternatively enumerate elements from both sets, like the following: $$f_{A \cup B}(\mathbb{N}) = A \cup B = \{f_A(0), f_B(0),f_A(1),f_B(1),...\}$$ A more formal definition: $$ f_{A \cup B}(n) = \begin{cases} 
          f_A\left({\frac{n}{2}}\right) & n \ \text{is even,} \\
          f_B\left({\frac{n-1}{2}}\right) & n \ \text{is uneven}
       \end{cases}
    $$ Here, $n$ is the position of the words in the set $A \cup B$ , starting from $n=0$ . Now we must show that $f_{A \cup B}$ is a total and computable function. The function is computable because: We know that determining whether a number is even or odd is computable. This means that $\frac{n}{2}$ and $\frac{n-1}{2}$ are also computable. $A$ and $B$ are semi-decidable and therefore, $f_A$ and $f_B$ are also computable because there exists $f_A(\mathbb{N})$ and $f_B(\mathbb{N})$ that recursively enumerate $A$ and $B$ . Therefore, $f_{A \cup B} $ is computable.  The function is total because: There exists  a lower and upper  bound. $f_A$ and $f_B$ are total. Therefore, we have shown that $f_{A \cup B}$ is computable and total thus is recursively enumerable. It follows from the equivalence that $f_{A \cup B}$ is semi-decidable. I'm a little unsure if my explanation for totality is adequate enough for this. How can I improve it?","['computability', 'logic', 'functions', 'solution-verification', 'computer-science']"
3989124,If $u$ is a solution of $F(x) \cdot \nabla u = h(u) $ and $F(x) \cdot v(x)>0 \forall x \in \partial U$ then $u\equiv 0$,"Let $U\subseteq \mathbb{R}^n$ open and bounded; $0 \in U $ and $\partial U$ is smooth. Let $u \in C^1(\bar{U})$ be a solution of the equation $$F(x) \cdot \nabla u = h(u),$$ with $F:\bar{U} \to \mathbb{R}^n$ continuous satisfying $F(x)\cdot v(x)>0, \forall x \in \partial U$ , where $v(x)$ is the normal unit vector exterior to $\partial U$ . If $h$ is a decreasing function and $h(0) = 0$ then $u \equiv 0$ . I have to solve this (yes, it is my homework, I signed up to some classes and I don't have the prerequisites and so I'm lost in almost everything I have to do. Please help me learn . I might need to know something very basic but I'm missing that piece of information). Now, given thet $\partial U$ is smooth I can use the divergence theorem on this. So far I have this: $$\int_U \nabla\cdot F(x) dx = \int_{\partial U} F(x) \cdot v(x) dx >0,$$ As $h$ is decreasing and $h(0)=0$ I know $h(x)\leq 0 \forall x \in \mathbb{R}$ and so $$0\geq\int_U h(u(x))dx = \int_U F(x)\cdot \nabla u(x) dx = \int_U \nabla\cdot [F(x)u(x)]dx - \int_U [\nabla\cdot F(x)]u(x)dx$$ So $$\int_U \nabla \cdot [F(x)u(x)]dx \leq \int_U[\nabla\cdot F(x)]u(x)dx$$ How do I carry on? I get that I should find something on the lines of: ""on one hand, on $\partial U$ we have that $F(x)\cdot v(x)$ is always positive, on the other $h(u(x))$ is never positive, so $u$ must be $0$ everyhere"". But I don't know how to do so. Thanks in advance. Tips provided by the professor today (haven't tried it yet, but I'll post it here already in case someone wants to give it a try as well). This is just my recollection of what he said in class, it is in no way formally written: The function u is $C^1(\bar{U})$ and therefore has a maximum (and minimum) value somewhere in $\bar{U}$ . Now we have two cases, either the maximum (and minimum) is inside $U$ or is in the boundary $\partial U$ . We'll talk about the maximum but the result follows similarly for the minimum. If u has it's maximum in $x \in U$ then we have that, at the maximum the gradient of $u$ is $0$ . and so $$h(u(x)) = F(x)\cdot \nabla u(x)  = F(x) \cdot 0 =0$$ as the function $h$ is decreasing, and $h(0) = 0$ , we have that $u(x)$ must be $0$ , as it is the only place where $h$ assumes the value $0$ . and so the maximum of the function $u(x)=0$ . Similarly we find that it's minimum is also $0$ , so $u \equiv 0$ . If on the other hand the maximum(and minimum) is on the boundary, then we have a problem. The professor sugested using ""Lagrange multiplier"" to solve this part. I have no clue what a lagrange multiplier is, but I'll read and after I learn it I believe I'll be able to solve the rest.","['calculus', 'partial-differential-equations']"
3989152,Is there a way to find the sum of an infinite series (not geometric),I want to calculate $$\sum_{k=0}^\infty\binom{k+3}k(0.2)^k$$ to get the exact value of it. I have excel and other tools to help me so it is fine if it is computationally expensive. Is there a clear and repeatable way to solve this infinite series? Thank you. This is my first post and be sure to give me some suggestions as well.,['sequences-and-series']
3989201,"How many sequences we can make from 4 digits, such that they don't have more than one zero.","My attempt: I tried to count the number of ways that the four digits don't have one zero, which is $9^4$ , then count the number of ways that they have exactly one zero, I got confused a little on how to count them, but I split this into cases where the zero appears on the first digit, I have $9^3$ ways, and shifted it to the left one by one and added all of them up and got $4*9^3$ . so my answer is $9^4+4*9^3$ Would appreciate if someone can approve my work, or if there's other easier ways to solve this. Thanks in advance to everyone.","['combinatorics', 'discrete-mathematics']"
3989212,Uniqueness of the maximal independent sigma algebra?,"Given a $\sigma$ -algebra $\mathcal{F}$ and a sub- $\sigma$ -algebra $\mathcal{H}\subset\mathcal{F}$ , by Zorn's lemma, we can show that if there exists a nontrival sub- $\sigma$ -algebra independent of $\mathcal{H}$ , then there exists a maximal (nontrivial) sub- $\sigma$ -algebra independent of $\mathcal{H}$ . My question is:
Is this maximal sub- $\sigma$ -algebra unique? If not, can you give a counterexample? For a special case, given a random variable $X$ on a standard probability space, are all random variables independent of $X$ a measurable function of some random variable $Y$ , namely $f(Y)$ ? My proof to show existence of maximal sub- $\sigma$ -algebra independent of $\mathcal{H}$ : Consider the set $A=\{\mathcal{G}\subset\mathcal{F}\mid \mathcal{G} \text{  independent of  }\mathcal{H} \}$ and it's nonempty. Then it's partially ordered with "" $\subset$ "", every chain clearly has an upper bound which is the union of all elements in the chain. Thus by Zorn's lemma we claim the existence of the maximal element which is nontrivial. PS: I know in many cases, there might not even exist a nontrivial independent $\sigma$ -algebra , but here let's talk about the case when there does exist.","['measure-theory', 'probability-theory', 'statistics']"
3989219,Prove that $x/(1 + \frac{2}{\pi} \cdot x) < \arctan(x)$ $\forall x > 0$,"The first thing that I tried to do is to differentiate both functions and try to see if there establishes inequality that we want (considering that they are equal when $x = 0$ ). This attempt failed because firstly it really is true but after that we get opposing inequality. Also, I have noticed that $\lim \frac{x}{1 + x \cdot \frac{2}{\pi}} = \lim \arctan(x) = \frac{\pi}{2}$ as $x \rightarrow + \infty$ but it didn't lead me to solution. I also tried to apply Taylor's formula but it didn't help much either. So, what are available ways to solve this problem?","['calculus', 'derivatives', 'inequality']"
3989248,Radius of curvature for a plane curve,"Given the plane curve $(x(t),y(t)) = (2 \cos t, \sin t)$ , the task is to find the radius of curvature at $(0,1)$ . (The given point corresponds to time $t=\pi/2$ .) The radius $R$ is given by $1/\kappa$ where $\kappa$ is the curvature, $$ \kappa = \frac{|v \times a |}{ |v|^3}, $$ with $v$ and $a$ being the velocity and acceleration vectors, respectively. Taking the first and second derivative, one can obtain $v=(-2\sin t, \cos t)$ and $a=(-2\cos t, -\sin t)$ . At the given point, $v = (-2,0)$ and $a=( 0,-1)$ , so that $v\times a = (0,0,2)$ and $$ \kappa = \frac{2}{8} = \frac{1}{4} \Rightarrow R = 4.$$ However, my solution manual says that $R = 2$ with no technical explanation. Can you help me figure out what's wrong?","['calculus', 'solution-verification', 'differential-geometry']"
3989255,Does AM-GM Follow From the Convexity of Some Function,"The AM-GM for $n = 2$ is $$\frac{x+y}{2} \ge (xy)^{1/2}$$ This is very easy to prove with algebra. However, I am wondering if there is a proof using convexity. That is, can we find a convex function $f:\mathbb{R} \to \mathbb{R}$ so that for some $t = t(x, y)$ between $0$ and $1$ we have $$tf(x) + (1-t)f(y) = \frac {x+y}{2}$$ and $$f(tx + (1-t)y) = (xy)^{1/2}$$ Is there such a proof?","['a.m.-g.m.-inequality', 'inequality', 'analysis']"
3989271,"Let $G$ be an abelian group s.t $H$ is a subgroup of index $n$. Prove that $\forall g \in G, g^n \in H$.","Let $G$ be an abelian group s.t $H$ is a subgroup of index $n$ . Prove that $\forall g \in G, g^n \in H$ . I'm trying to split into some cases, where both cases that $ g \in H$ and case where ${\rm ord}(g)\mid n$ are clear.","['group-theory', 'abelian-groups']"
3989289,How can I generate cyclic whole number?,"My goal was to create a function that would generate number from 1->N->1. 1 2 3 4 3 2 1. Easiest approach I was going for was create a function where using cosine i'd generate integers, and where x -> 2N. So if N is 4, then x would go upto 2x4 = 8. 1 2 3 4 5 6 7 8 then output of ƒ would be 1 2 3 4 4 3 2 1 or 1 2 3 4 3 2 1 either would have been great.","['algebra-precalculus', 'trigonometry', 'algorithms']"
3989321,"If $x_1, x_2$ are roots of $a\cos x+b\sin x+c=0$ for $x_1+x_2\ne 2k\pi$ show that $\sin(x_1+x_2)=\frac{2ab}{a^2+b^2}.$","If $x_1, x_2$ are roots of $a\cos x+b\sin x+c=0$ for $x_1+x_2≠2kπ$ show that $\sin(x_1+x_2)=\frac{2ab}{a^2+b^2}$ What I've done till now: $$a\cos x_1+b\sin x+c-(a\cos x_2+b\sin x_2+c)=0$$ $a\cos x_1+b\sin x_1-a\cos x_2-b\sin x_2=0$ $a(\cos x_1-\cos x_2)+b(\sin x_1-\sin x_2)=0$ $a(-2\sin(\frac{x_1+x_2}{2})\sin(\frac{x_1-x_2}{2}))+b(2\sin(\frac{x_1-x_2}{2})\cos(\frac{x_1+x_2}{2}))=0$ $-a\sin(\frac{x_1+x_2}{2})+b(\frac{\cos x_1+x_2}{2})=0$ That's it. Does anyone know how to solve this?",['trigonometry']
3989334,Closed form for Sum of Tangents with Angles in Arithmetic Progression,"The formulae that can be used to evaluate series of sines and cosines of angles in arithmetic progressions are well known: $$\sum_{k=0}^{n-1}\cos (a+k d) =\frac{\sin( \frac{nd}{2})}{\sin ( \frac{d}{2} )}  \cos \biggl( \frac{ 2 a + (n-1)d}{2}\biggr)$$ $$\sum_{k=0}^{n-1}\sin (a+kd) =\frac{\sin( \frac{nd}{2})}{\sin ( \frac{d}{2} )} \sin\biggl( \frac{2  a + (n-1)d}{2}\biggr)$$ and these can be derived quite easily through use of Euler's relation and De Moivre's theorem or using the product to sum formulae to form a telescoping series. Question. In the case of $\tan$ however, what closed form formula is there for $$\sum_{k=0}^{n-1}\tan (a+kd)$$ and how would we derive it? I know of no product to sum identities or other similar ones that could usefully be applied here. Thanks for your help.","['summation', 'trigonometry', 'closed-form', 'sequences-and-series', 'algebra-precalculus']"
3989348,Any ideas on how to solve this integral $\sin ax / (x^2+b^2)^2$?,"I've been trying to find the value of the following integral using the residue theorem : $$\int_0^\infty \frac{\sin ax\ dx}{(x^2+b^2)^2}$$ So basically the idea that I had is to transform this integral and to make it in the numerator as $e^{iaz} = \cos az\ + i\sin az\ $ and thus $$\mathrm{Im}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},
 = \int_0^\infty \frac{\sin az\ dz}{(z^2+b^2)^2}$$ However in class we've only established the residue formula from $-\infty$ to $+\infty$ , and when I try to integrate it that way (as in from $-\infty$ to $+\infty$ ) I obtain $0$ obviously because the function is odd. I'd like to add that I worked on a similair integral $\int_0^\infty \frac{\cos ax\ dx}{(x^2+b^2)^2}$ , using the poles $+ib$ and $-ib$ , hence $(x^2+b^2)^2$ $=$ $(x+ib)^2(x-ib)^2$ and then calculating the $Res( \frac{\cos az\ dz}{(z^2+b^2)^2}, +ib)$ by using this : $$\mathrm{Re}\, \int_0^\infty \frac{e^{iaz}dz}{(z^2+b^2)^2},
 = \int_0^\infty \frac{\cos az\ dz}{(z^2+b^2)^2}$$ I'm trying to do a similar thing with the sin function. calculating the cos integral was way easier due to the fact that the integrated is even. Any help is welcome to direct me in the right path. Also apologies since english isn't my first language!","['integration', 'complex-analysis', 'residue-calculus', 'complex-integration', 'derivatives']"
3989405,"A subset of the vector space of all real sequences with finitely-many non-zero elements. Boundedness, closedness, compactness and completeness.","Let $X$ be the space of all real sequences with finitely many nonzero elements, equipped with the supremum norm. That is, for each $x= (x_k)_{k \in \mathbb N}$ we have: $$ \|x \| = \sup \{ |x_k| k\in \mathbb N \}.$$ Consider the subset $V$ of $X$ defined by: $$ V:= \{ x= (x_k)_{k \in \mathbb N} \in X | \sum_{k=1}^\infty |x_k| \leq 1 \}.$$ I wish to answer the following practice exam questions: Is $V$ bounded in $X$ ? Is $V$ closed in $X$ ? Is $V$ compact in $X$ ? Is $V$ complete in $X$ ? Consider $x=(x_n)_{n \in \mathbb N}\in V$ , we then have that for each $n\in \mathbb N$ we may write: $$ |x_n| \leq \sum_{k=1}^\infty|x_k| \leq 1 $$ Each term is nonnegative and therefore the entire sum is certainly more than a single term. We thus find that $1$ is an upper bound for each element of the sequence hence certainly it is larger than or equal to the least upper bound: $$ \| x\| \leq 1.  $$ We conclude that $V$ is bounded. I think this is not true as there are sequences in $V$ that leave the space (and $X$ ). Consider $x^{(n)}$ given by: $$x^1= (\frac{1}{2}, 0, 0, \dots) $$ $$x^2=(\frac{1}{2}, \frac{1}{4}, 0, \dots) $$ $$x^3=(\frac{1}{2}, \frac{1}{4}, \frac{1}{8},0, \dots) $$ Which might be described as $(x^n_k)_{k \in \mathbb N}$ with $x^n_k=\frac{1}{2^{k}}$ for $k\leq n$ and $0$ otherwise ( $0 \not \in \mathbb N$ ). Each of the elements of this sequence lives in $X$ and since the geometric series converges to $1$ also in $V$ . So $$ \sum_{k=1}^\infty |x^k_n| \leq 1 .$$ However, we run into a problem since  its limit is $x=(x_k)_{k\in \mathbb N}$ with $x_n=\frac{1}{2^n}$ , which does not have finitely many nonzero elements hence it is not in $X$ and then certainly not in $V$ . Compact normed vector spaces are closed, since the space is not closed, it cannot be compact. The space cannot be complete as the example in (2) is a Cauchy-sequence, but the sequence does not converge to a limit in $V$ . In a complete space every Cauchy sequence is convergent (with limit in the space). Did I do this okay?","['solution-verification', 'functional-analysis']"
3989406,How to integrate $\int_0^{\infty} \frac{x}{e^x+1} dx$,"I want to integrate it explicitly. I looked up if this had an exact solution, and I got that it is $\frac{\pi^2}{12}$ here: https://www.integral-calculator.com/#expr=x%2F%28e%5Ex%2B1%29&lbound=0&ubound=inf I thought it could be solved using geometric series, so I tried to turn my integral into something similar: $$\int_0^{\infty} \frac{x}{e^x+1} dx= \int_0^{\infty} \frac{x}{e^x+1} \frac{e^x-1}{e^x-1} dx=\int_0^{\infty} \frac{x e^x }{e^{2x}-1}dx -\int_0^{\infty} \frac{x}{e^{2x}-1} dx= \int_0^{\infty} \frac{x e^x }{e^{2x}-1}dx -\frac{1}{4}\int_0^{\infty}  \frac{x}{e^{x}-1} dx=\int_0^{\infty} \frac{x e^x}{e^{2x}-1} - \frac{1}{4}\frac{\pi^2}{6}$$ Where the second term is easy to solve with geometric series, and it's also defined with the Riemann zeta: $$\int_0^{\infty} \frac{x}{e^x-1}=\zeta(2)=\frac{\pi^2}{6}$$ I think I can use geometric series also with the other integral, I've tried to solve it this way and I got: $$\int_0^{\infty}\frac{xe^x}{e^x-1}=\frac{1}{4}\sum_{n=1}^{\infty}\frac{1}{\left(n-\frac{1}{2}\right)^2} $$ And I don't know how to obtain a value from this infinite sum, although if I'm not wrong it should be $\pi^2/2$ . Anyone knows how to do this? Also, I think I might be looping the loop and there's some easier way to proceed from the beggining, so I'd appreciate another point of view to solve this. Thanks.","['integration', 'riemann-zeta', 'definite-integrals', 'geometric-series']"
3989412,Finding the middle of n points,"Sorry for my possible bad English, I have a problem that I spent a bit of time on and I have been blocked on it for a couple of hours, I'll try to translate it as best as I can: Given an integer $n\ge3$ , and given $A_1,\ldots,A_n$ points on a plane, under what condition(s) can we find points $P_1,\ldots,P_n$ such that $A_1$ is the midpoint of $[P_1,P_2]$ , $A_2$ is the midpoint of $[P_2,P_3]$ , ... and $A_n$ is the midpoint of $[P_n,P_1]$ ? Please tell me if I wasn't clear enough in my translation, and thank you very much in advance!",['geometry']
3989438,Sign of the first derivative,"I am confused by the statement that a read, that says $f'(x_0)>0$ does not imply that $f$ is increasing in an open interval around $x_0$ . But the book also mentions that if $f(x)$ is differentiable at $x_0$ and $f'(x_0)>0$ then there is $h>0$ such that for all $x_1,x_2\in(x_0-h,x_0+h)$ if $x_1<x_0<x_2$ then $f(x_1)<f(x_0)<f(x_2)$ . I feel the statement contradicts one another. Any explanations will be appreciated.","['calculus', 'real-analysis']"
3989446,General definition for $k$-dependence of a family of sub-$\sigma$-algebra,"If $(\Omega,\mathcal{F},\mathbb{P})$ is a probability space, is there a general definition for the "" $k$ -dependence"" of an arbitrary family $(\mathcal{F}_i)_{i \in I}$ of sub- $\sigma$ -algebra of $\mathcal{F}$ ? Already know the definition for a sequence and that $k=0$ corresponds to the independence. How can we define it for an arbitrary family?","['measure-theory', 'independence', 'probability-theory']"
3989452,Unexpected result on the number of permutations with a restriction.,"Let $p=(p_1,p_2,\dots,p_n)$ be a weak composition of a positive integer number $n$ into $n$ non-negative integer parts and let $k_i$ be the count of the part $i$ ( $i=0,1,2,\dots$ ) in the composition. A permutation $\tilde p$ of $p$ is allowed if its partial sums obey the inequality: $$
\forall m (1\le m\le n):\ s_m\equiv\sum_{j=1}^m \tilde p_j\ge m.
$$ The simple but rather unexpected result valid by numerical evidence  is that the number of allowed permutations is $$
\frac{n!}{\displaystyle(k_0+1)!\prod_{i>0}k_i!}.\tag1
$$ This means that the number is exactly $n+1$ times less than the number of all permutations of the composition supplemented by an additional 0. This simplicity suggests that there should be almost obvious
explanation of the fact but I could not find it. Any hint is appreciated.","['permutations', 'integer-partitions', 'combinatorics', 'catalan-numbers']"
3989455,Prove that $ | \mathcal {P} (\mathbb N) | \le | \mathbb R | $.,"Define $\Phi: \mathcal {P} (\mathbb N) \to \mathbb R$ by $$
\Phi (S) = \sum_ {k \in S} 10 ^ {- k}.
$$ How to prove that $\Phi$ is injective? I assumed that $\Phi(S) = \sum_ {k \in S} 10 ^ {-k} = \sum_ {k \in S'} 10 ^ {- k} = \Phi(S')$ and tried deriving that $S = S'$ , but could not make this approach work.","['elementary-set-theory', 'cardinals', 'functions']"
3989468,How to read $\frac{d^2y}{dx^2}$,Is it correct to say 'dee squared y over dee x squared'? I am just trying to make sure because I have to teach this to someone. I learnt that $f''(x)$ is pronounced 'f double prime of x' but I couldn't find how to pronounce Leibniz notation. Thanks!,"['notation', 'calculus', 'derivatives']"
3989511,Two different approach on integration on manifolds (Chain and sub-manifolds),"I noticed that there are two different approaches on developing the general stokes theorem.
In particular, Rudin did this on Chains (PMA) and Munkres defined the integration on manifolds by using partition of unity (Analysis on Manifolds) As a beginner on this subject, I don't really appreciate the difference between those two approaches. So what are some advantages and disadvantages between those two methods? Which one is more easier? Which one is more general?","['differential-forms', 'smooth-manifolds', 'differential-geometry']"
3989526,L'Hospital's rule doesn't converge for a function with square root,"I was trying to find the limit of a function of the form $\frac{x}{\sqrt{(x+a)(x+b)}}$ . When I apply L'Hospital's rule and differentiate the numerator and denominator, after simplification I end up with the same form as I started with: $$L = \lim_{x \to \infty} \frac{x}{\sqrt{(x+a)(x+b)}} \\
= \lim_{x \to \infty} \frac{1}{\frac{1}{2} \frac{(2x+a+b)}{\sqrt{(x+a)(x+b)}}} = \frac{2\sqrt{(x+a)(x+b)}}{2x+a+b} = \frac{\frac{2x+a+b}{\sqrt{(x+a)(x+b)}}}{2} \\
= \lim_{x \to \infty} \frac{x}{\sqrt{(x+a)(x+b)}} \textrm{(because $\lim_{x \to \infty} \frac{a+b}{\sqrt{...}}$ is zero)}$$ I noticed if I solve it by computing the limit of the squared value, the solution is easy: $$L^2 = \lim_{x \to \infty} \frac{x^2}{(x+a)(x+b)} \\
L^2 = \lim_{x \to \infty} \frac{2x}{2x+b+c} = 1 \\
L = \sqrt{1} = 1$$ Before I found this solution, I was searching online for different ways of evaluating limits and applying L'Hospital's rule, but none of the resources I came across seem to cover this case. Am I missing another straightforward way of solving this (whether using L'Hospital's or not)? When does L'Hospital's rule fail to converge, and what does it mean when it does?","['limits', 'derivatives']"
3989534,How can we calculate $\int_{r\in \mathbb{S}^2}\ {\rm Area}\ \Delta p_0q_0r\ d{\rm Area}_r$,"Define a function $f : \mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2\rightarrow \mathbb{R}$ by $f(p,q,r)$ to be a area of the geodesic triangle $pqr$ in the unit sphere $\mathbb{S}^2$ . (Here the area of the triangle $pqr$ is $\angle p + \angle q +\angle r -\pi$ ). Question 1 : Here how can we find $$ \int_{\mathbb{S}^2\times \mathbb{S}^2\times \mathbb{S}^2 } \ f(p,q,r) d{\rm Vol}(p,q,r) $$ Question 2 : Furthermore, how can we find the following $$\int_{\mathbb{S}^2 } \ f(p_0,q_0,r) d{\rm Vol}(r) $$ Question 3 : Consider the triangle $\Delta pqr$ where the order of $p,\ q,\ r$ is
positively oriented. And define $$A = \frac{ q\times p }{|
 q\times p| },\ B = \frac{r\times q}{|r\times q|},\ C =
 \frac{p\times r}{|p\times r|} $$ Note that $\angle \ (A,B) =\pi-\angle p $ so that $$ \angle (A,B) +
\angle (B,C)+\angle (A,C) = 2\pi - {\rm Area}\ \Delta pqr $$ Hence ${\rm perim}\ \Delta ABC$ , the sum of all side lengths in $\Delta ABC$ , is equal to $2\pi - {\rm Area}\ \Delta pqr$ . Hence we want to calculate $$ \int_{(A,B,C)\in \mathbb{S}^2
 \times\mathbb{S}^2\times
\mathbb{S}^2}\ {\rm perim}\ \Delta ABC \ d{\rm Vol}_{ (A,B,C) }$$","['euclidean-geometry', 'multivariable-calculus', 'spherical-geometry']"
3989547,Let $G$ a group show and $x\in G$ show that $x^{n+m}=x^nx^m$,"Let $G$ a group show and $x\in G$ show that $x^{n+m}=x^nx^m$ I take an arbitrary $n\in \mathbb{Z}$ and first I did induction on $m\geq 0$ If $m=0$ then $x^{n+0}=x^n=x^ne=x^nx^0$ If $m>0$ we assume its valid for m, $x^{n+m}=x^nx^m$ then $x^nx^{m+1}=x^n(x^mx)=(x^nx^m)x=x^{n+m}x=x^{n+m+1}$ so its valid for all $m\geq0$ now if $-m<0$ we assume $x^{n-m}=x^nx^{-m}$ then $x^{x-m-1}=x^{n-m}x^{-1}=x^nx^{-m}x^{-1}=x^nx^{-m-1}$ is right?","['group-theory', 'solution-verification']"
3989566,Adjoint of the differential in Morse-Novikov cohomology,"Let $M^n$ be a smooth manifold and we can define the Morse-Novikov cohomology group $H^k (M^n,\theta)$ with co-boundary differential operator $d_\theta (w) = dw + \theta \wedge w$ where $0\neq[\theta]∈ H_{dR}^1(M^n)$ . This cohomology shares many properties with the ordinary de Rham cohomology and only depend on $[\theta]$ . As it has been shown that the Hodge theory works nicely on this cohomology by defining the adjoint operator $d_\theta^*$ ( I suppose it defines as usual adjoint $\delta$ of de Rham operator $d$ ) and defining the corresponding Laplacian of these operators. I have the following question: It seems to me that the Stokes’ theorem does not work well with this operator $d_\theta$ because we will have an extra  term which is $\int_M[\theta \wedge w]$ and this implies that the Green formula will not work (i.e. $d_\theta^*$ will no longer be adjoint of $d_\theta$ ). Am I right?","['adjoint-operators', 'homology-cohomology', 'de-rham-cohomology', 'differential-geometry']"
3989603,Notation used in Russell's paradox,"I am slightly confused by the notation used in Russell's paradox. I am following this text . I understand that $\phi (x)$ is this boolean function, which outputs either True or False . I understand that $R$ is the set of all $x$ which has the image True when subjected to $\phi$ . This is denoted by $$R = \{x:\phi (x)\}$$ Great. I am good so far. Now, the author defines $\phi(x)$ such that $x \in x$ . I don't understand this notation. How can an object belong to itself? Shouldn't it be ' $=$ ' instead of ' $\in$ '? Then the author goes ahead and defines $R$ is a set that contains all $x$ such that $x$ does not belong in $x$ , or using symbols, $$R = \{ x: \, \sim\phi(x)\}$$ I don't understand this $x\in x$ notation used. I believe this very argument is used to explain the paradox. Could someone explain this?","['elementary-set-theory', 'paradoxes', 'logic']"
3989622,Exterior power of lie algebra representation in coordinates,"Suppose I have a representation $V = \mathbb{C}^3$ of a group $G$ where elements are of the form $M = \begin{bmatrix}m_1^1 & m_1^2 & m_1^3 \\ m_2^1 & m_2^2 & m_2^3 \\m_3^1 & m_3^2 & m_3^3 \end{bmatrix}$ in some basis $v_1,v_2,v_3$ . If I wish to compute the representation $\Lambda^2 V \subset V \otimes V$ , I could look at the action $g \cdot (v \wedge u) := (gv) \wedge (gu)$ and see that I will get a matrix $N$ of minors of $M$ , i.e. $n_{i,j} = $ minor obtained by removing $i$ th row and $j$ th column. My question is as follows: In the case of a representation of a Lie algebra $\mathfrak{g}$ , is there an analogous interpretation when considering an exterior power of the representation? As an example, using the Lie algebra action $g(v \wedge u) := (gv) \wedge u + v \wedge (gv)$ , with basis $v_1 \wedge v_2$ , $v_1 \wedge v_3$ , and $v_2 \wedge v_3$ , I obtain the following matrix for $\begin{bmatrix}m_1^1+m_2^2 & m_2^3 & -m_1^3 \\ m_3^2 & m_1^1+m_3^3 & m_1^2 \\-m_3^1 & m_2^1 & m_2^2 + m_3^3 \end{bmatrix}$ . Any insight would be appreciated!","['representation-theory', 'linear-algebra', 'lie-algebras']"
3989636,How to perform vertical line test algebraically?,"I understand how a function should have one value mapped to a single value in the range. Thus, when graphing the function I understand how a vertical line can determine if a point or multiple points intersect the vertical line. This is visual. However, how can we perform the same test algebraically across a domain of the function? I am learning calculus from the start again since college, so in case my question is naive or obvious I apologize.","['functions', 'graphing-functions']"
3989662,Solve $(1+x) d y-y d x=0$,"Here is the worked example by the author, $$
\int \frac{d y}{y}=\int \frac{d x}{1+x}
$$ $$
\ln |y|=\ln |1+x|+c_{1}
$$ $$
|y|=e^{\ln |1+x|+c_{1}}=e^{\ln |1+x|} \cdot e^{c_{1}}=|1+x| e^{c_{1}}
$$ $$
y=\pm e^{c_{1}}(1+x)
$$ Relabeling $\pm e^{c_{1}}$ as $c$ then gives $y=c(1+x)$ . My question: Shouldn't you say $c=\pm e^{c_1},0$ because $y=0$ is a constant solution? I guess that when $x=-1,y=0$ so I thought maybe that covers the solution $y=0$ , but $(1+x) d y-y d x=0$ can be rearranged to $\frac{d y}{d x}=\frac{y}{1+x}$ and this implies that $x \neq-1$ , so the constant solution $y=0$ cannot be achieved from $y=c(1+x)$ where $c=\pm e^{c_1}$ , is what I think.. Thanks!",['ordinary-differential-equations']
3989691,"Find constants $a$, $b$, $c$, $d$ and $e$ such that $\cos4x=a\sin^4x+b\sin^3x+c\sin^2x+d\sin x+e$ for all angles $x$","Basically, write $\cos4x$ as a polynomial in $\sin x$ . I've tried the double angles theorem and $\cos2x=\cos^2x-\sin^2x$ . I'm still having trouble right now though. Please help! Thanks!",['trigonometry']
3989699,How to derive the kth coefficient standard error?,"Given a multiple regression with the usual assumptions satisfied, with $X \in R^{n \times p}$ $$ y = X \beta + e $$ I know that the estimated variance is given by $\sigma^2 (X^TX)^{-1}$ . But what I want to know is the estimated variance for the $k^{th}$ coefficient out of this whole covariance matrix. I've seen from the resource A (cited below) that this value is the $k^{th}$ diagonal term of the $\sigma^2 (X^TX)^{-1}$ . But more interestingly (AND THIS IS WHAT I WANT TO PROVE), this value is: $$ \dfrac{\sigma^2}{(1-R^2_k) \sum_{i=1}^n (x_{ik} - \bar{x_k})^2 } $$ Here, $x_k$ is the $k^{th}$ column of $X$ . $R^2_k$ is the $R^2$ from regressing $x_k$ on $X_{(k)}$ (= $X$ after taking $k^{th}$ column out). Here is my (failed) attempt at this derivation: Using resource B (cited below), the $k^{th}$ diagonal value of $\sigma^2 (X^TX)^{-1}$ is: $$\sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1}$$ Since $X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k$ can be seen as projection of $x_k$ onto column space of $X_{(k)}$ , we can say: $$ X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k = \hat{x_k} $$ , which is the regression prediction after regressing $x_k$ on $X_{(k)}$ . So our diagonal value simplifies to: $$ \sigma^2 [x_k^Tx_k - x_k^T X_{(k)} (X_{(k)}^TX_{(k)})^{-1}X_{(k)}^T x_k ] ^ {-1}= \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1}$$ Now, since (I will omit subscript $i$ so $x_k = x_{ik}$ ) $$ R^2_k = 1 - \dfrac{ \sum (x_k - \hat{x_k} )^2 }{ \sum (x_k - \bar{x_k} )^2 } $$ $$ \dfrac{\sigma^2}{(1-R^2_k) \sum (x_{k} - \bar{x_k})^2 } $$ will simplify to: $$ \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 } $$ So in summary, we want to show that: $$ \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 }$$ But then, $$ \sigma^2 [ x_k^Tx_k - x_k^T \hat{x_k} ] ^ {-1} = \dfrac{\sigma^2}{\sum (x_k)^2 - \sum x_k \hat{x_k} } $$ where as, $$ \dfrac{\sigma^2}{ \sum (x_{k} - \hat{x_k})^2 } = \dfrac{\sigma^2}{\sum (x_k)^2 - 2\sum x_k \hat{x_k} + \sum x_k^2 } $$ Could someone please help me find where I am making a mistake, and if this is not the right approach, help me derive it correctly? Resource A: http://people.stern.nyu.edu/wgreene/MathStat/GreeneChapter4.pdf (page 40) Resource B: About the diagonal entries of an inverse matrix","['regression', 'standard-error', 'statistics']"
3989739,$\sqrt{x^2+12y}+\sqrt{y^2+12x}=33$ subject to $x+y=23$,"Solve the system of equations: $\sqrt{x^2+12y}+\sqrt{y^2+12x}=33$ , $x+y=23$ The obvious way to solve it is by substituting for one variable. However I was looking for a more clever solution and went ahead and plotted two graphs. The first graph looks pretty weird so please help as to how to proceed with this graphically or an easier algebraic method.
Thanks :)","['algebra-precalculus', 'graphing-functions', 'radicals']"
