question_id,title,body,tags
3189993,Do 50% of people die on either side of an average life expectancy?,"I am no statistician so this may come across as very naive, but I've been trying to get my head around how to interpret life expectancy data. Say you have a population of $n$ and a range of possible ages from 0– $m$ . Am I right in thinking that given an average life expectancy of $x$ years, roughly 50% of the population must fall on either side of that line (ignoring those who fall directly on it)? As such, given an average life expectancy of 75, a maximum age of 120 and a population of 10,000, 5,000 (ish) would be expected to die between the ages of 0 and 74 and 5,000 (ish) between 76 and 110? And that that second half will be more densely packed than the first half (as it covers 34 years rather than 74 with the same number of people)? Finally, if the above is broadly correct (and if we assume for simplicity's sake that each age has an equal likelihood of fatality), does that mean you are more likely to die on the left side of the average until you make it halfway between 0 and $x$ , whereupon you are equally as likely to die on either side of the average, and each subsequent year survived increases your likelihood of outliving the average? My thinking being that if you have a $\frac{1}{120}$ chance of dying each year (0.83%), each year of the left side is $\frac{1}{74}$ (1.35%) whereas each year on the right side is $\frac{1}{34}$ , or ~ $\frac{2}{74}$ (2.94%).","['actuarial-science', 'statistics']"
3190012,Find the number of degree 1 vertices in terms of n and d,"Fix an integer $d>1$ . Let $G$ be a tree with $n$ vertices, and every vertex can have either degree $1$ or $d$ . Find the number of degree $1$ vertices in terms of $d$ and $n$ . 
I've been working on this problem and I feel like I almost have the answer. I worked out that if $d=2$ , the graph will just be a line with vertices and $2$ leaves, so there will be $2$ vertices with $\deg(1)$ , no matter how many vertices there actually are. If $d=3$ , $n$ starts to matter. Using the fact that the sum of all the $\deg(v)$ of a graph is $2|E|$ and that in a tree $|E|=|V|-1$ I worked out that the sum of $\deg(v)=2n-1$ , and that this sum mod $d$ will be the amount of degree $1$ vertices. ( Since $d+d+d+\dots+1+1+1+1$ mod $d$ will be the sum of the $1$ 's unless the sum is greater than $d$ , then it will wrap around). I'm having a hard time putting this all together into a simple equation. I don't want just the solution, I want to understand how to find the equation as well.","['trees', 'graph-theory', 'eulerian-path', 'combinatorics', 'discrete-mathematics']"
3190047,Semisimple linear algebraic group,"Let $G$ be a linear reductive algebraic over an algebraically closed field of characteristic $0$ . I know that there is a surjective map with finite Kernel $$G' \times T \to G$$ where $G'$ is semisimple and $T$ is a torus. When I talk about semisimple/simply connected groups, I refer to their characterization based on their root system as in Springer book ""Linear algebraic group"". I would like to prove that for every semisimple algebraic groups there is a finite covering $$G'' \to G'$$ such that $G''$ is  simply connected, but I really do not know how to do. Also ,if it is possible, I would like to see a reference for the proof of the fact that over $\mathbb{C}$ a simply connected group is simply connected in the standard topological sense.","['algebraic-groups', 'algebraic-geometry', 'representation-theory', 'lie-groups']"
3190070,"Is this proof of $H\le G, [G:H] =2 \implies a^2\in H \forall a\in G$ correct?","If $H$ is a subgroup of a group $G$ and the index (number of right cosets) of $H$ is $2$ , then $a^2 \in H$ for all $a\in G$ . My attempt: if $a\in H$ then $a^2\in H$ directly. If $a\notin H$ and $a^2\notin H$ then $a(a^{-1})^{-1}  = a^2 \notin H$ then $G = Ha \cup Ha^{-1}$ (the union being disjoint). But $e\notin Ha$ because $e=a^{-1}a$ and $a^{-1}\notin H$ . Also $e\notin Ha^{-1}$ because $e = aa^{-1}$ and $a\notin H$ . Then we have a contradiction. So $a^2$ must be in $H$ in both cases. I think it's ok but I dont feel the arguments with the identity $e$ are right and don't see how to justify them more rigorously. Thanks in advance","['group-theory', 'abstract-algebra', 'proof-writing']"
3190075,Show that there is no simple group of order $3393$,"Show that there is no simple group of order $3393$ The hint I was given was to look at Sylow $3$ -subgroups. I know that $3393 = 3^2 \times 13 \times 29$ and that $G$ must contain a subgroup of order $3^2$ , say $H$ . But how can I proceed further with this. The proof I managed to come up with was this. Attempted Proof: Let $G$ be a simple group of order $3393$ . Sylow theory tells us that the number of Sylow $29$ subgroups of $G$ , $n_{29}$ is either $1$ or $117$ and that the number of Sylow $13$ subgroups of $G$ , $n_{13}$ is either $1$ or 260 $261$ . Since $G$ is simple we must have $n_{13} = 261$ and $n_{29} = 117$ . Let $\operatorname{Syl}_{29}(G) = \{H_i \ | 1 \leq i \leq 117\}$ , since $H_i \cap H_j \leq H_i$ for any $i \neq j$ we must have $H_i \cap H_j = \{1_G\}$ , for otherwise we'd end up with $H_i \cap H_j$ . This shows that we have at least $117 \times 28 = 3276$ elements of order $29$ in $G$ . Applying the same reasoning to $\operatorname{Syl}_{13}(G)$ we see that $G$ must also contain $261 \times 12 = 3132$ elements of order $13$ . Thus $|G| \geq 3132 + 3276 > 3393 = |G|$ a contradiction. $\square$ Are there any errors in my proof? If not, how can I use the hint to prove this?","['group-theory', 'proof-verification', 'finite-groups']"
3190078,Birth/Death Processes with constant departure rate,"So, I'm looking at a Birth/Death process $Z$ with an arrival rate of $\frac{1}{n+1}$ and a departure rate of $1$ . I'm trying to show that this process is positive recurrent and find the stationary distribution. My understanding of positive recurrent is that if there is a non-zero probability that the markov chain will not return to a state. Additionally, it is a class property, and there only seems to be one communication class as you can access every state from any other state. Since the departure rate is greater than 0, isn't there always a probability that the markov chain will return to any initial starting state? So, if we take state 1, there is always a non-zero probability that the markov chain will return back to state 1 as the departure rate is nonzero? I'm a bit confused on how to answer the question (and show that this chain is positive recurrent) since I don't agree with the assumption the question is making (that this process is positive recurrent). I'd appreciate any help! Thank you so much!","['stochastic-processes', 'statistics', 'probability']"
3190126,Finding particular solutions of $y^{(4)} + 2y'' + y = x \sin x$,"We have a non homogeneous ODE $$y^{(4)} + 2y'' + y = x \sin x$$ with characteristic equation I get $(l^2+1)^2 = 0$ so $l = -i ,i$ and so the answer of homogeneous ODE is a linear combination of $\sin x , \cos x , x \sin x , x\cos x$ . For finding the Particular solution first I assumed $y_p = (Ax+B)(C\sin x + D \cos x)$ and it didn't work. Then $y_p = x(Ax+B)(C\sin x + D \cos x)$ and it didn't work. At last, $y_p = x^2(Ax+B)(C\sin x + D \cos x)$ worked and the answer was $-1/24 x^3 \sin x -1/8 x^2 \cos x$ worked but it took a lot of time to find that the other two don't work. I want to know is there any way to guess the leading $x^n$ term and not testing different situations? (In this case $n=2$ ) I know it can be solved with a way involving Wronskian and Cramer's rule (but that way needs a 4x4 determinant which takes time to calculate) but I want to solve with undetermined coefficients rule so I want to find a better way for guessing the answer format.","['derivatives', 'ordinary-differential-equations']"
3190151,How can we show that $\left\{x\mapsto\prod_{i=1}^kf_i(x_i):f_i\in C_0(E_i)\right\}$ is convergence determining on $\times_{i=1}^ kE_i$?,"Let $E$ be a complete locally compact separable metric space and $\mu,\mu_n$ be probability measures on $\mathcal B(E)$ . We say $\mu_n\to\mu$ weakly if $$\int f\:{\rm d}\mu_n\to\int f\:{\rm d}\mu\tag1$$ for all bounded continuous $f:E\to\mathbb R$ (write $f\in C_b(E)$ ) and we say $\mu_n\to\mu$ vaguely if $(1)$ for all compactly supported continuous $f:E\to\mathbb R$ (write $f\in C_c(E)$ ). By the Portmanteau theorem, $\mu_n\to\mu$ weakly if and only if $\mu_n\to\mu$ vaguely and $\mu(E)=\lim_{n\to\infty}\mu_n(E)$ . Now, we say that $M\subseteq C_b(E)$ is convergence determining if the assertion that $(1)$ holds for all $f\in M$ implies $\mu_n\to\mu$ weakly. By Proposition 4.4 of Chapter 3 in the book of Ethier/Kurtz, $C_c(E)$ is convergence determining. But that means (by our definition) $\mu_n\to\mu$ vaguely. Question 1 : Am I missing something or does that mean that in the Portmanteau theorem we are able to conclude $\mu_n\to\mu$ vaguely and $\mu(E)=\lim_{n\to\infty}\mu_n(E)$ from $\mu_n\to\mu$ weakly, while we only need to know $\mu_n\to\mu$ vaguely in order to conclude $\mu_n\to\mu$ weakly? Now assume we have complete locally compact separable metric spaces $E_1,\ldots,E_k$ . I would like to show that $$\tilde M:=\left\{\prod_{i=1}^k(f_i\circ\pi_i):f_i\in C_0(E_i)\right\},$$ where $\pi_i:\times_{j=1}^dE_j\to E_i$ is the projection onto the $i$ th coordinate) is convergence determining (on $\times_{i=1}^kE_i$ ). Again in Ethier/Kurtz, Proposition 4.6 of Chapter 3, we find the claim that if $M_i\subseteq C_b(E_i)$ is convergence determining (on $E_i$ ), then $$M:=\left\{\prod_{i=1}^k(f_i\circ\pi_i):f_i\in M_i\cup\color{red}{\left\{1\right\}}\right\}$$ is convergence determining. I would clearly take $M_i=C_0(E_i)$ (which is convergence determining by the former result and since $C_c(E_i)$ is dense in $C_0(E_i)$ by definition). However, Ethier/Kurtz allow $f_i$ to be the constant function $1$ (which is clearly not in $C_0(E_i)$ , unless $E_i$ is compact). They state Proposition 4.6 in the more general case of a countable product, but I don't think that this is the reason why we need to allow $f_i\equiv 1$ . On the other hand, taking $k=1$ yields a result weaker than what we assume beforehand (we start with $M_1$ being convergence determining and conclude $M_1\cup\left\{1\right\}$ is convergence determining). Question 2 : What am I missing here? Is my $\tilde M$ convergence determining?","['weak-convergence', 'probability-theory', 'functional-analysis', 'measure-theory']"
3190190,Computing $\int_0^{\infty} e^{iax}/(x^2+1)dx$,"I am having trouble solving this integral with complex analysis $$\int\limits^{\infty }_{0}\frac{e^{iax}}{x^{2} +1} dx$$ I have tried two different contours; those being contour 2 contour 1 with both contours, I got the answer $\int\limits ^{\infty }_{0}\frac{e^{iax}}{x^{2} +1} dx=\frac{\pi }{2e^{a}}$ But according to wolfram alpha's approximation, this is wrong. Wolfram alpha had a real and imaginary part to the answer. So I'm confused. Can someone show the process for solving this with contour integration?",['integration']
3190245,Why does a pointed surface minus a countable set of points contain a curve?,"Let $S$ be a surface over $\mathbb{C}$ and let $s_1,\ldots, s_n$ be closed points of $S$ .  We consider this data as fixed. It is not hard to see that there is a curve passing through $s_1,\ldots,s_n$ . I am wondering why the following is true: For any (countable) sequence of points $x_1, x_ 2, \ldots$ in $S(\mathbb{C})\setminus \{s_1,\ldots,s_n\}$ there is an algebraic curve $C\subset S$ with $\{s_1,\ldots,s_n\}\subset C$ and $C\cap \{x_1,x_2,\ldots \} = \emptyset$ (i.e., $C\subset S-\{x_1,\ldots\}$ ). I asked a similar question on MO a while ago (see https://mathoverflow.net/questions/322084/does-there-exist-a-curve-which-avoids-a-given-countable-union-of-small-subsets ) but I'm now wondering about a possible generalization of the answer given there.","['algebraic-curves', 'complex-geometry', 'algebraic-geometry', 'curves']"
3190258,Show that every locally compact Hausdorff space is completely regular,"It would be very appreciated if someone could review my solution. Thanks! Problem: Show that every locally compact Hausdorff space is completely regular. Proof: Let $X$ be a locally compact Hausdorff space. One point sets are closed in $X$ since $X$ is Hausdorff. Then there is a space $Y$ such that $Y$ is the one point compactification of $X$ where $Y$ is compact Hausdorff. Let $x_0 \in X$ and $B \subset X$ such that $x_0 \notin B$ and $B$ is closed in $X$ . Then the open set $U = X - B$ contains $x_0$ . Then since $U$ is open in $X$ it is also open in $Y$ by construction of the one point compactification Munkres gives. Then $C = Y - U$ is a closed set in $Y$ that contains $B$ . Then since Y is compact Hausdorff it is also normal. Hence since the set $\{x_0\}$ is closed since Y is Hausdorff, by the Urysohn lemma we can define a continuous function $f: Y \to [a,b]$ such that $f(x_0) = a$ and $f(x) = b$ for every $x \in C$ . But since $f$ is continuous we can take the continuous function $g$ on a subspace of the domain of $f$ . Namely, $g: X \to [a,b]$ since $X$ is a subspace of $Y$ . Hence we have a continuous function $g$ that maps $g(x_0) = a$ and $g(B) = b$ , since $B \subset C$ . We can replace $[a,b]$ by $[0,1]$ to satisfy the definition of completely regular. Hence $X$ is completely regular.","['separation-axioms', 'general-topology', 'solution-verification', 'compactness']"
3190287,prime numbers and expressing non-prime numbers,My textbook says if $b$ is a non-prime number then it can be expressed as a product of prime numbers. But if $1$ isn't prime how it can be expressed as a product of prime numbers?,"['number-theory', 'elementary-number-theory', 'prime-numbers']"
3190288,Proof - Graph Connectedness,"Let $n\ge2$ be an integer. Let $G$ be a graph with $n+1$ vertices and
  more than ${n \choose 2}$ edges. Show that $G$ is connected. Here's my attempted solution to the above problem: Induction on $n$ : Base case: $n=2$ So $n+1=3$ vertices. $\vert E\vert \gt {2 \choose 2} = 2$ so at least 3 edges, so connected. Inductive step: Assume $G$ connected for graph with $n+1$ vertices and $\vert E\vert \gt {n \choose 2}$ for all $n\ge2$ . Define $G\prime$ graph with $n+2$ vertices and $\vert E\vert \gt {n+1 \choose 2}$ . Notice that $G\prime$ forms the complete graph $K_{n+1}$ with $n+1$ vertices and exactly ${n+1 \choose 2}$ edges (which is connected by IA). Now assume for contradiction that $G\prime$ is disconnected. Since we need to use more edges than ${n+1 \choose 2}$ that are used for $K_{n+1}$ , there is nowhere to else to add the edge back in other than between the complete graph and the vertex not included in it. We then obtain one connected component or a connected graph $G\prime$ which contradicts initial assumption. All graphs $G$ connected for $n\ge2$ . (Q.E.D) Is this proof correct? Any tips, corrections, help would be greatly appreciated. EDIT: Removed use of removed vertex in proof. Instead, I rely solely on complete graph.","['graph-theory', 'induction', 'discrete-mathematics', 'connectedness']"
3190322,Uniqueness of Hahn-Banach extensions for $c_0 \subseteq \ell^\infty$,"Let $c_0$ be the space of real sequences converging to zero with supremum norm. $c_0$ is a (closed) subspace of $\ell^\infty$ , the space of bounded real sequences. A $f \in {c_0}^*$ corresponds to a $\hat{f} \in \ell^1$ via $$ f(x) = \sum_{k=0}^\infty \hat{f_k} \, x_k \tag{1} ~,$$ and $\|f\| = \|\hat{f}\|_1$ . It it obvious that map $F \in (\ell^\infty)^*$ defined again by $(1)$ is a Hahn-Banach extension of $f$ , meaning that $F|_{c_0} = f$ and $\|F\| = \|f\|$ . I have read that in this case the extension $F$ is unique. Why?","['banach-spaces', 'normed-spaces', 'functional-analysis', 'hahn-banach-theorem']"
3190347,Will the product of a real valued matrix and its transpose always have a real eigenvector?,"I'm trying to solve this really interesting problem, taken from ""Berkeley Problmes in Mathematics"" by Souza and Silva https://www.amazon.com/Berkeley-Problems-Mathematics-Problem-Books/dp/0387204296 Trying to make headway, I've noticed that if the last two statements are correct, then $M^T(Mu)=\sigma^2 u$ , so in other words, there exists an eigenvector $u$ with the eigenvalue $\sigma^2$ .
Therefore, I have two questions, and I would dearly love a proof or counter-example: Is it true that $M^TM$ must have at least a (real) eigenvector? I strongly suspect this $\sigma$ they talk of is the operator norm of $M$ . Is this true? Is this a dead-end way to try to solve this problem. Should I try something else? Of course, everything is real-valued here.","['multivariable-calculus', 'linear-algebra', 'linear-transformations']"
3190366,Simplifying $\prod_{k=3}^{n-1}\cos\left(\frac{\pi}{k}\right)$,"I am looking to simplify the following, without the use of capital Pi notation: $$\prod_{k=3}^{n-1}\cos\left(\frac{\pi}{k}\right)$$ Which is meant to produce the sequence: $\left[1,\ \frac{1}{2},\ \frac{1}{2}\frac{\sqrt{2}}{2},\frac{1}{2}\frac{\sqrt{2}}{2}\frac{1+\sqrt{5}}{4},\frac{1}{2}\frac{\sqrt{2}}{2}\frac{1+\sqrt{5}}{4}\frac{\sqrt{3}}{2}...\right]$ . I have seen identities of a similar structure, such as: $$\prod_{k=1}^{n-1}\sin\left(\frac{k\pi}{n}\right) = \frac{n}{2^{n-1}},\qquad or\qquad \prod_{k=1}^{n-1}\cos\left(\frac{k\pi}{n}\right) = \frac{\sin(\frac{\pi n}{2})}{2^{n-1}}$$ But, I am versed neither in the proofs of these identities, nor the properties of the $\Pi$ notation, so I've had a lot of difficulty trying to simplify this on my own. Dealing with $k$ in the denominator instead of the numerator (like in the two aforementioned identities) is something that I am evidently unequipped to deal with on my own. Thank you to anyone willing to help me out!","['infinite-product', 'trigonometry', 'geometry', 'sequences-and-series']"
3190400,Proving ${\lim\limits_ {n\to\infty}}\frac{6n^3+5n-1}{2n^3+2n+8} = 3$,"I'm trying to show that $\exists \,\varepsilon >0\mid\forall n>N\in\mathbb{N}$ such that: $$\left|\frac{6n^3+5n-1}{2n^3+2n+8}-3\right| < \varepsilon$$ Let's take $\varepsilon = 1/2$ : $$\left|\frac{6n^3+5n-1}{2n^3+2n+8}-3\right| = \left|\frac{6n^3+5n-1 - 6n^3 -6n -24}{2n^3+2n+8}\right| < \left|\frac{-n -25}{2n^3+2n+8}\right| = \left|-\left(\frac{n +25}{2n^3+2n+8}\right)\right|$$ Since $|-x| = |x|$ : $$\left|-\left(\frac{n +25}{2n^3+2n+8}\right)\right| = \left|\frac{n +25}{2n^3+2n+8}\right|<\varepsilon $$ This is the part which I have trouble with. Here, I always end up with my minimum-required index to be smaller than zero, which seems peculiar to me: $$\left|\frac{n +25}{2n^3+2n+8}\right| < |n+25| = n+ 25 < \varepsilon  = 1/2$$ From here, I will get that my $N$ will be less than zero,  which means that all the elements of the sequence are inside of my given epsilon environment, but I know that's not true since $a_1 = 5/6 < 3 - \varepsilon $ , so what I did do wrong? Is it because I completely removed the denominator? If so, why does that break the inequality?","['real-analysis', 'sequences-and-series', 'limits', 'convergence-divergence', 'infinitesimals']"
3190451,Prove that shock wave is weak solution of Burgers' equation (Riemann problem),"In math modeling studies, I need to prove that $$u(x,t)=\begin{cases}u_l\qquad x<st\\ u_r\qquad x>st\end{cases}$$ where $$s=(u_l+u_r)/2$$ is a weak solution for the Riemann problem of Burgers' equation $u_t+uu_x=0$ with the Riemann data $$u(x,0)=\begin{cases}u_l\qquad x<0\\
u_r\qquad x>0\end{cases}$$ Integrating with a test function $\phi\in C^1_0$ , I got $$
\int_0^\infty\int_{-\infty}^{\infty}\left[u\phi_t+\frac{u^2}{2}\phi_x\right]dx \ dt=\dfrac{u^2_l-u_r^2}{2}\int_0^\infty \phi(st,t)dt-\int_{-\infty}^{\infty} \phi(x,0)u(x,0)dx.
$$ How can I cancel $\displaystyle \dfrac{u^2_l-u_r^2}{2}\int_0^\infty \phi(st,t)dt$ ? Many thanks for a help. This is Exercise 3.4 p. 29 of the book Numerical Methods for Conservation Laws by R.J. LeVeque (Birkäuser, 1992).","['integration', 'hyperbolic-equations', 'mathematical-modeling', 'partial-differential-equations']"
3190474,Find the coefficient of $x^{32}$ in $(x^3 +x^4 +x^5 +x^6 +x^7)^7$,"I don't understand the explanation in the book and why the final answer looks the way it does. I know I am supposed to factor and it will equal to $(x^3(1+x+\cdots+x^4))^7$ . But after that, I am really confused about what happens. Can someone explain this problem step by step. Thank you!","['combinatorics', 'polynomials']"
3190509,Why is this proof of a congruence relation valid?,"The following question comes from the 2012 Singapore Mathematical Olympiad (Open Section), Round 2. Let $p$ be an odd prime. Show that $$1^{p-2}+2^{p-2}+3^{p-2}+\dots+\left(\frac{p-1}2\right)^{p-2}=\frac{2-2^p}{p}\pmod p.$$ After trying and failing to find a proof, I went to look at the official solution, which is as follows. Note that for every $k=1,2,\dots,(p-1)/2$ , it is true that $$\frac{2k}p\binom{p}{2k}=\frac{(p-1)(p-2)\dotsm(p-2k+1)}{(2k-1)!}=-1\pmod{p}.$$ Therefore, the LHS is $$\sum_{k=1}^{(p-1)/2}k^{p-2}=-\sum_{k=1}^{(p-1)/2}k^{p-2}\frac{2k}{p}\binom{p}{2k}=-\frac2p\sum_{k=1}^{(p-1)/2}k^{p-1}\binom{p}{2k}=-\frac2p\sum_{k=1}^{(p-1)/2}\binom{p}{2k}.$$ The summation in the last equality counts the number of even-sized nonempty subsets of a $p$ -element set, which is $2^{p-1}-1$ . I have a problem with the proof, in the last equality. I know that the proof goes on the idea that $k^{p-1}=1$ for every $k\neq 0$ (Fermat's Little Theorem), but doesn't that mean that we have to work inside $\mathbb Z/p\mathbb Z$ , which would make the $-\frac2p$ a division by $0$ ? To illustrate, suppose we claimed that $$\frac{2-2^p}p=\frac2p(1-2^{p-1})=\frac2p(1-1)=0\pmod p.\tag{1}$$ This is of course absurd, because we really have a $0/0$ in the last step, so we are dividing by $0$ . Indeed we can see (for example) that $(2-2^3)/3=-2$ , which is not divisible by $3$ . So when we are working inside $\mathbb Z/p\mathbb Z$ , it is not possible to divide by $p=0$ . How, then, is the reasoning in $(1)$ fallacious, but that in the proof correct? What am I missing here?","['contest-math', 'number-theory', 'elementary-number-theory']"
3190518,Find $\lim_{ n \to \infty} \frac{e^{-n}}{\sqrt{n}} \sum_{k=0}^\infty \frac{\sqrt{k+ n }}{k!} (n+a)^k$,"I am trying to find the following limit: \begin{align}
\lim_{ n \to \infty} \frac{e^{-n}}{\sqrt{n}}  \sum_{k=0}^\infty  \frac{\sqrt{k+ n }}{k!} (n+a)^k
\end{align} for some fixed $a>0$ . Things that tired. 
We can come up with the following bound: \begin{align}
\lim_{ n \to \infty} \frac{e^{-n}}{\sqrt{n}}  \sum_{k=0}^\infty  \frac{\sqrt{k+ n }}{k!} (n+a)^k \le  \lim_{ n \to \infty} \frac{e^{-n}}{\sqrt{n}}  \sum_{k=0}^\infty \left(  \frac{\sqrt{k }}{k!} (n+a)^k + \frac{\sqrt{ n }}{k!} (n+a)^k\right)\\
 \le  \lim_{ n \to \infty} \frac{e^{-n}}{\sqrt{n}}     \sum_{k=0}^\infty \frac{\sqrt{k }}{k!} (n+a)^k + e^{a}
\end{align}","['limits', 'sequences-and-series']"
3190546,Computing $\phi(\frac32)$ where $\phi$ is an automorphism of $\mathbb Q[\sqrt2]$ such that $\phi(1)=1$ and $\phi(\sqrt2)=\sqrt2$,"This question is a followup to this question about Field Automorphisms of $\mathbb{Q}[\sqrt{2}]$ . Since $\mathbb{Q}[\sqrt{2}]$ is a vector space over $\mathbb{Q}$ with basis $\{1, \sqrt{2}\}$ , I naively understand why it is the case that automorphisms $\phi$ of $\mathbb{Q}[\sqrt{2}]$ are determined wholly by the image of $1$ and $\sqrt{2}$ . My problem is using this fact explicitly. For example, suppose I consider the automorphism $\phi$ such that $\phi(1) = 1$ and $\phi(\sqrt{2}) = \sqrt{2}$ , and I want to compute the value of $\phi\left(\frac{3}{2}\right)$ . I can do the following: $$ \phi\left(\frac{3}{2}\right) = \phi(3) \phi\left(\frac{1}{2}\right) = [\phi(1) + \phi(1) + \phi(1)] \phi\left(\frac{1}{2}\right) = 3\phi\left(\frac{1}{2}\right).$$ I am unsure how to proceed from here. I would assume that it is true that $$\phi\left(\frac{1}{1 + 1}\right) = \frac{\phi(1)}{\phi(1) + \phi(1)} = \frac{1}{2},$$ but I don't know what property of ring isomorphisms would allow me to do this.","['field-theory', 'ring-theory', 'abstract-algebra', 'galois-theory']"
3190589,Understanding Ceva's Theorem,"In Ceva's Theorem, I understand that $\dfrac{A_{\triangle PXB}}{A_{\triangle PXC}}=\dfrac{BX}{CX}=\dfrac{A_{\triangle BXA}}{A_{\triangle CXA}}$ . I would like clarification in understanding the following step which states: $$\frac{A_{\triangle APB}}{A_{\triangle APC}}=\frac{A_{\triangle AXB} - A_{\triangle PXB}}{A_{\triangle AXC}-A_{\triangle PXC}}=\frac{BX}{CX}$$ How does the subtraction of the two areas make it so that the new triangles, even though they do not share those sides, are still proportional to $\frac{BX}{CX}$ ?","['proof-explanation', 'triangles', 'geometry']"
3190687,Actuarial practice exam: Find number of exponential data values above threshold given MLE.,"I was going thorough an actuarial exam and came across a problem that I can't figure out. Here is the problem as stated on the practice exam: You are given: $\bullet$ An insurance product with a per loss limit of 200 covers losses from an exponential distribution with parameter $\theta$ . $\bullet$ Based on the following table, the maximum likelihood estimate of $\theta$ is 168. $$\begin{array}{|c | c | c |}\hline
\text{Size of loss} & \text{Number of claims} & \text{Sum of losses} \\ \hline
\text{Less than  } 200 & 1,114 & 142,752\\ \hline
\text{At least  } 200 & N & 200N \\ \hline
\end{array}$$ Calculate $N$ for the table above. The answer is: $N$ is less than 250. My question: How can we know for sure that $N$ is less than 250? It can be thought of as a random variable, couldn't it? It could be that there is a single loss above 200 but very large so that the overall average is still 168, yes? It seems that we are supposed to assume that the losses above 200 are exponential with mean 168 above 200 by the memoryless property and thus $$\frac{142752+368N}{1114+N}=168$$ gives $N=222$ , which aligns well with the official answer. But with the given information, it could be that the first 1,114 data points are all 128.14 and then we have a single data point of 44568. Of course this is an event of negligible probability, but nonetheless it is theoretically positive. What am I missing?","['actuarial-science', 'statistics', 'probability']"
3190690,What does $F'$ and $F''$ mean?,"I'm trying to learn what a Taylor series is, This is the equation I'm looking at and I know 0 calculus. I have been told that $F'(x)$ is a derivative but what does $F''(x)$ mean?","['notation', 'calculus', 'functions', 'taylor-expansion', 'derivatives']"
3190724,Kahler differentials of a Hopf Algebra,"Let $A$ be a $k$ -Hopf Algebra over some ring $k$ , with augmentation ideal $J_A=$ ker $(\epsilon:A\rightarrow k)$ I would like to prove that the module of Khaler Differentials $\Omega_A$ of $A$ over $k$ is isomorphic to the tensor product $A\otimes_k J/J^2$ . I found this theorem in W.C. Waterhouse Intro to Aff. Group Schemes (11.3 pag 85), but I don't get his proof. Related question: Kernel of an algebra map and module of Kahler Differentials","['group-schemes', 'algebraic-geometry', 'commutative-algebra', 'hopf-algebras']"
3190727,Question regarding partially ordered sets and the subset relation,"Could someone look through my attempt at proving the following problem please? Let $(A, \preceq)$ be a POSET.For each $x,y \in A$ ,let $P_x=\{a \in A: a \preceq x\}$ . Let $F$ be the family of sets defined by $F=\{P_x:x\in A\}$ .Therefore $(F,\subseteq)$ is a POSET. Prove $x \preceq y \iff P_x \subseteq P_y$ for all $x,y \in A$ . Attempt : Let $(A, \preceq)$ be a POSET. Let $x,y \in A$ , $P_x=\{a \in A: a \preceq x\}$ and $F$ be the family of sets defined by $F=\{P_x:x \in A\}$ with $(F,\subseteq)$ being a POSET. $(\Rightarrow)$ part of the proof Assume that $x \preceq y$ and suppose for contradiction that $P_x \nsubseteq P_y$ . By definition, there must exist some $x \in A$ such that $x \in P_x$ and $x \notin P_y$ . Since $\preceq$ is a partial order, then it is reflexive and hence $x \preceq x$ and consequently $x \in P_x$ . With $x \preceq y$ by assumption then $x \in P_y$ Therefore we have the following contradiction: $(x \in P_x \text{ and } x \in P_y)$ on the one hand and $(x \in P_x \text{ and } x \notin P_y)$ on the other. $(\Leftarrow)$ part of the proof Assume $P_x \subseteq P_y$ .Since $\preceq$ is a partial order, therefore it is reflexive and $x \preceq x$ and also $x \in P_x$ . Since $x \preceq x$ and $P_x \subseteq P_y$ then $x \in P_y$ . With $P_y=\{a \in A: a \preceq y\}$ and $x \in P_y$ then $x \preceq y$ as required. Thank you","['elementary-set-theory', 'proof-explanation', 'proof-writing', 'proof-verification']"
3190772,Mean of means and standard deviation,"I have a set of data with their means $\mu_1,\mu_2,\ldots\mu_n$ and standard deviations $\sigma_1,\sigma_2,\ldots,\sigma_n$ . These values refer to repetitions of the same experiment. How do I calculate the ""mean mean"" (average?) and mean standard deviation summarizing all different experiments $\mu_{tot},\sigma_{tot}$ ? Basically I have $X_1\sim N(\mu_1,\sigma^2_1), X_2\sim N(\mu_2,\sigma^2_2),\ldots,X_n\sim N(\mu_n,\sigma^2_n)$ and $Z=\frac{X_1+X_2+\ldots+X_n}{n}$ . The question is: $Z\sim N(?,?)$","['statistics', 'standard-deviation', 'means']"
3190829,Bayes' Theorem with Multiple Tests,"I'm having difficulties with this problem: Suppose you have an entire city afflicted with four distinct and exclusive diseases and a laboratory is assigned to test which disease each citizen has. The reliability of these tests are as follows: Disease A = 72.7% Disease B = 81.1% Disease C = 75.2% Disease D = 80.1% The percentage of the population of people afflicted is as follows: P(B1) = 18.1% (Disease A) P(B2) = 31.9% (Disease B) P(B3) = 18.9% (Disease C) P(B4) = 31.1% (Disease D) If a random person were to selected from the entire population and then tested positive for disease A, what is the probability that they actually have disease A? I think that the problem is asking for P(B1|A1), so I used this formula: P(B1|A) = P(A|B1)P(B1) / ( P(A|B1)P(B1) + P(A|B2)P(B2) + P(A|B3)P(B3) + P(A|B4)P(B4) ) These are the values that I am sure of: P(A|B1) = .727 , because that is the chance of a true positive result of disease A being detected P(B1) to P(B4) = the population listed above, corresponding to A, B, C and D. The problem is now, I don't know what values to put inside P(A|B2) to P(A|B4) Do I put in just the rate of false positive (.273)? Or do I use the corresponding tests for disease B, C and D (.811, .752, .801, respectively)? Or am I missing something here?","['discrete-mathematics', 'bayes-theorem', 'probability']"
3190908,What is the value of $\frac11+\frac13-\frac15-\frac17+\frac19+\frac1{11}-\dots$?,"The series $\sum_{k=1}^{\infty }\frac{(-1)^{k+1}}{2k-1}=\frac{1}{1}-\frac{1}{3}+\frac{1}{5}-\frac{1}{7}+\dots$ converges to $\frac{\pi}{4}$ . Here, the sign alternates every term. The series $\displaystyle\sum_{k=1}^{\infty }{(-1)^{\left(k^{2} + k + 2\right)/2} \over 2k-1}=\frac{1}{1}+\frac{1}{3}-\frac{1}{5}-\frac{1}{7}+\dots$ also converges. Here, the sign alternates every two terms. What is the convergence value, explicitly, of the second series? The first summation is noted above, because it might be a useful information to evaluate the second summation.","['summation', 'integers', 'pi', 'sequences-and-series']"
3190922,"Why are the trig functions versine, haversine, exsecant, etc, rarely used in modern mathematics?","I was browsing through a Wikipedia article about the trigonometric identities, when I came across something that caught my attention, namely forgotten trigonometric functions. The versine (arguably the most basic of the functions), coversine, haversine and exsecant formulas had once been utilised for navigational purposes, prior to GPS tracking systems. However, recently, they have become less common in modern mathematics and beyond. Why is that? Here is a link to a PDF file describing all of these now-obsolete trig functions: The Forgotten Trigonometric Functions, or
How Trigonometry was used in the Ancient Art of Navigation (Before GPS!)","['spherical-trigonometry', 'trigonometry', 'math-history']"
3190934,Integral of a differential 2-form in a parametrized surface,"I want to calculate the integral $\mathop{\iint}_{\phi}xdx\land dy+ydy\land dz+z dz\land dx$ considering $\left\{ \phi\left(u,v\right):=\left(u+v,uv,u^{2}-v^{2}\right):u,v\in\left[0,1\right]\right\} $ And the Stokes Theorem says that  integral of a differential form ω over the boundary of some orientable manifold Ω is equal to the integral of its exterior derivative dω over the whole of Ω. I know that $d\left(Fdx+Gdy+Hdz\right)=\left(G_{x}-F_{y}\right)dx\land dy+\left(H_{y}-G_{z}\right)dy\land dz+\left(F_{z}-H_{x}\right)dz\land dx$ How should I do that? I only find examples of vector fields and I don't know if this should be converted to something similar to a vector field.",['multivariable-calculus']
3190944,"Is the differential at a regular point, a vector space isomorphism of tangent spaces, also a diffeomorphism of tangent spaces as manifolds?","Note: My question is not ""If $f$ is a diffeomorphism, then is the differential $D_qf$ an isomorphism?"" My book is From Calculus to Cohomology by Ib Madsen and Jørgen Tornehave. I didn't study much of the definitions or theorems in the book, if they were already found in An Introduction to Manifolds by Loring W. Tu. I mostly assume they're the same until there is evidence otherwise. In Chapter 11, Madsen and Tornehave define ""local index"", which looks to me like just a different way to say sign of the determinant of the Jacobian matrix that represents the differential (See Tu Proposition 8.11 ; Tu Section 23.3 ; Madsen and Tornehave Lemma 10.1 ; Madsen and Tornehave Lemma 10.3 ; Wikipedia Degree of a continuous mapping , specifically this ). Now, for a regular point $q \in f^{-1}(p)$ for a regular value $p$ that is in the image of $f$ (For a regular value $p$ that isn't in the image of $f$ , I'm sure there are neat vacuous arguments that I'm gonna skip), it says the local index is defined as $1$ if $D_qf$ preserves orientation and $-1$ otherwise. I was surprised to see orientation-preserving as an adjective for an isomorphism of vector spaces because I'm used to seeing orientation-preserving as an adjective for diffeomorphisms of manifolds. However, $T_pN^n \cong \mathbb R^n$ (vector space isomorphic), so I guess tangent spaces of manifolds are manifolds as well, assuming the image of an oriented manifold under a vector space isomorphism is also an oriented manifold or something. ( This question seems to confirm that tangent spaces of manifolds are manifolds, although I think the definition in the question is the same as the one in Madsen and Tornehave but different from the one in Tu). Actually, upon a second reading of the answer of Alex Mathers to that question, I think I have an answer to my question: Any vector space isomorphism, of tangent spaces of manifolds or any other vector spaces, turns out to be a homeomorphism. While my question is diffeomorphism, it turns out John M. Lee's Example 1.24 , which was pointed out by Alex Mathers, shows that any isomorphism of finite real vector spaces is a diffeomorphism as well. Rather than analyzing the example, I'm going to try a different proof.) I think that $D_qf$ , or $f_{*, q}$ in Tu's notation, is a diffeomorphism of the tangent spaces as manifolds because: $D_qf$ is surjective either by definition of $q$ being a regular point (Tu Definition 8.22 ) or by $q \in f^{-1}(p)$ and definition of $p$ being regular value of $f$ that is in the image of $f$ (Madsen and Tornehave Chapter 11 ). $D_qf$ is a homomorphism of tangent spaces (almost immediately from definition, but anyway, this follows from Tu Exercise 8.3 ). $D_qf$ is injective, by this , because of (1), (2) and that the dimensions of $T_qN$ and $TpM$ are finite and equal. $D_qf$ is a local diffeomorphism of manifolds if and only if for each $X_q \in T_qN$ , the (double) differential $D_{X_q}(D_qf): T_{X_q}(T_qN) \to T_{D_qf(X_q)}(T_pM)$ is an isomorphism of (double) tangent spaces, by the Inverse Function Theorem for manifolds (specifically by Tu Remark 8.12 , which gives a ""coordinate-free description"" for Tu Inverse Function Theorem for manifolds (Tu Theorem 6.26) ) $D_qf$ is a diffeomorphism of manifolds if and only if $D_qf$ is a bijective local diffeomorphism of manifolds (at each $X_q \in T_qN$ ) by this . $D_qf$ is an isomorphism of tangent spaces by (1), (2) and (3). Every $D_{X_q}(D_qf)$ is identical to $D_qf$ itself, by Tu Problem 8.2 (also found in this question and this question ), because of (2). Every $D_{X_q}(D_qf)$ is an isomorphism of tangent spaces because of (6) and (7). $D_qf$ is a local diffeomorphism of manifolds (at each $X_q \in T_qN$ ) by (4) and (8). $D_qf$ is a diffeomorphism of manifolds by (1), (3), (5), and (9).","['proof-verification', 'vector-spaces', 'linear-algebra', 'manifolds', 'differential-geometry']"
3190952,Requirement of Lyapunov Stability in Asymptotic Stability,"In my Differential Equations course, we defined the equilibrium point $x_0$ of a dynamical system $\frac{dx}{dt} = f(x(t))$ (for $f$ defined on an open subset of $\mathbb R^n$ , say $\mathbb R^n$ itself) to be stable if it is: Lyapunov Stable There is an $\epsilon$ ball around $x_0$ such that the solutions $\varphi$ of this differential equation with initial conditions in this ball satisfy $\lim_{t \to \infty} \varphi(t) = x_0$ . I am trying to find an example of the case where the property (2) holds while the point $x_0$ is not Lyapunov stable. After some searching, I ran across Homoclinic Bifurcation , which is intuitively how I would expect Lyapunov Stability to fail, but have been unable to find examples of Homoclinic Bifurcation where property (2) holds as well. Any help would be appreciated.","['ordinary-differential-equations', 'bifurcation', 'stability-in-odes', 'stability-theory', 'dynamical-systems']"
3190961,How is the Laplace Transform a Change of basis?,"This question is primarily based on the following answer's way of reasoning, https://math.stackexchange.com/a/2156002/525644 If you want to write a new answer to the question; ""How is the Laplace Transform a Change of basis?"" Please do. In jnez71's answer, he concludes that laplace transform is a change of basis, Now lets look at that mysterious laplace transform. $$\mathscr{L}(f(x)) = \int_{-\infty}^\infty e^{-sx}f(x) \, dx$$ Imagine all possible values of $e^{-sx}$ in a big matrix $^1$ , where each
  row corresponds to plugging in a specific $s$ and each column
  corresponds to plugging in a specific $x$ . (This matrix is orthonormal
  if $s=i\omega$ , i.e. the Fourier transform). If you select some $s$ ,
  you are plucking out a specific value of the function that resulted
  from the multiplication of this matrix with the vector $f(x)$ , a
  function we call $F(s):=\mathscr{L}(f(x))$ . Specifically, $$F(s=3) =
> f(x) \cdot e^{-3x}$$ (where that dot is an inner product, not ordinary multiplication). We
  say that $F(s)$ is just $f(x)$ expressed on a basis of exponential
  functions $^2$ . Choosing a specific value of $s=s_1$ is picking out the
  value of $f(x)$ in the $e^{-s_1x}$ direction . The entire $e^{-sx}$ can be viewed as the change of basis matrix. Wait, what basis were we on before if we're on the exponentials now?
  The dirac
  deltas . Take an
  inner product of some function with a dirac delta and notice how you
  get back that function at the action point of the dirac delta $^3$ . This is
  sometimes called the sifting
  theorem , but it
  should be clear that if we can project a vector (via inner product)
  and just get back some component of that vector, that component was how much the vector had in the direction we projected it onto. Questions: 1. Can someone literally write the matrix he's referring to? 2. Please elucidate on ""how"" and where exactly does this change of basis occurs?(even though this information exists in his answer I couldn't get a good grasp) 3. Take an
  inner product of some function with a dirac delta and notice how you
  get back that function at the action point of the dirac delta How does this imply that previously we were on the dirac delta function's basis?","['eigenvalues-eigenvectors', 'laplace-transform', 'change-of-basis', 'linear-algebra', 'eigenfunctions']"
3191024,What are mathematically definable/ useful properties of ternary relations? ( How to define for example symmetry or transitivity?),"My question may seem gratuitous. Here are my presuppositions. What makes a relation mathematically interesting is that one can define abstract properties of this relation , such as reflexivity, symmetry, transitivity. These three are, it seems to me, the basic ones. So, knowing that ternary relations exist and are sometimes used  in mathematics  ( for example the relation of colinearity in geometry), I presuppose that abstract properties can be defined for these relations that make them interesting. So I began to ask myself how to define these properties for ternary relations. And I started  with reflexivity, symmetry and transitivity. But maybe I did not take the right way. What other properties are interesting for ternary relations? As an attenpt to define reflexivity for a ternary relation , I would say that R  is reflexive iff ( for all a, the triple ( a,a,a) belongs to R ). But I cannot manage to find a plausible definition of symmetry or of transitivity for ternary relations. Symmetry requires the notion of , so to say, "" converse"" n-tuple. The "" converse"" couple of (a,b) is (b,a). What would be the "" converse"" triple of 
(a,b,c)? Should one  say ( a, c, b)? ( c, a ,b) ? If I analyse (a,b,c) as ( (a,b), c); may I say that ( c, (a,b) ) is its "" converse"" triple? The problem, is, apparently, that if ( (a,b), c) ( with a, b and c elements of some set A) belongs to a relation R, this relation is from A² to A . But the pair ( c, (a,b)) is not a possible element of a relation from A² to A. So the second pair cannot belong to the same relation as the first one . But this is required to define symmetry: a pair and it's ""converse"" pair have to belong to the same relation in order this relation to be called symmetric. I think the same problem would occur with transitivity.",['elementary-set-theory']
3191033,Why is this example not antisymmetric?,"R = {(a,a),(a,b),(b,a),(b,b),(b,c),(c,b),(c,c)} I know that to be anti-symmetric aRb and bRa, which this example has, but this example also means that a does not equal b (which anti-symmetry needs to be true). How, in this example, does a not equal b even though it has aRb and bRa. This video tells me that aRb and bRa means a=b. But I also heard that antisymmetry needs there to be no edges (on a directed graph) going both ways (in other words, one-way) as shown here . As you may have already worked out, I am very confused, and I just hope my question hasn't confused you. Any help would be much appreciated.","['relations', 'discrete-mathematics']"
3191066,Transforming an arbitrary quadrilateral to a unit square,"In this answer from Pedro Gimeno he proposed the following transformation to map the points of any arbitrary quadrilateral to the unit square $$\pmatrix{x'\\y'} =
> \pmatrix{u_x&v_x&w_x\\u_y&v_y&w_y}\pmatrix{x\\y\\xy}$$ $$x'=u_xx+v_xy+w_xxy\\ y'=u_yx+v_yy+w_yxy$$ It transforms the unit square in a way controlled by the vectors $u=(u_x,u_y), v=(v_x,v_y), w=(w_x,w_y)$ as follows: Geometric interpretation of u, v, w (sorry, I can't post images) However, I'd like to do the opposite. I need to integrate on an arbitrary quadrilateral and I'd like to define my integral on the unit square and then map the coordinates of the unit square on the original quadrilateral . The matrix Pedro proposed, however, is not invertible and I'm not sure a pseudo-inverse matrix is what I'm looking for. How can I obtain the opposite transformation?","['analytic-geometry', 'multivariable-calculus']"
3191165,How to prove $f$ is $C^\infty$,"Suppose $f:U \subseteq \mathbb{R}^2 \rightarrow \mathbb{R}$ is  continous  and $$(x^2+y^4)f(x,y)+(f(x,y))^3=1 \: \text{for all} \: (x,y) \in U. $$ Prove $f$ is $C^\infty$ . This kind of exercise is new to me and I don't really have any idea how to derive that the derivative exist 
infinitely and it's continous.","['functional-equations', 'multivariable-calculus', 'real-analysis']"
3191254,Find $\lim_{t\to 1^{-}}(1-t)\sum_{r = 1}^\infty \frac{t^r}{t^r+1}$,"$$\lim_{t\to 1^{-}}(1-t)\sum_{r = 1}^\infty \frac{t^r}{t^r+1}$$ Note: I am a high school student and this problem appeared in my test. So, please try to use methods to solve this problem at a high school level :) My Attempt: I have honestly no idea how to approach this problem. I first tried to simplify the summation but didn't find any pattern. By looking at the options given to me ( which were all in ln's and e's )I do get a feel that we may have to integrate at some point. Though I am not sure. Any help would be appreciated.","['limits', 'summation']"
3191266,Intersection of a nested sequence of subgroups,"Let $G$ be a f.g. torsion-free nilpotent group and $(H_n)_{n\geq 1}$ a nested sequences of subgroups of finite index with trivial intersection. Question: Is it true that $\displaystyle\bigcap_{n\geq 1} H_nK=K$ for every subgroup $K<G$ ? This question is not from homework or textbook, it is just a question from my mind, I don't know how difficult it is. I have checked several books on nilpotent groups and still was not able to get an answer. In general, it would be interesting to know for which classes of groups the answer is positive.","['nilpotent-groups', 'group-theory']"
3191297,Combinatorial proof $\sum_i^{\lfloor{n/2}\rfloor} (-1)^i {n-i\choose i} 2^{n-2i} = n+1$,"Give a combinatorial proof (double counting) that $\sum_i^{\lfloor{n/2}\rfloor} (-1)^i {n-i\choose i} 2^{n-2i} = n+1$ There was a hint that maybe $n$ bit binary numbers without 01 may help. (eg. 1001, 10000110, 1010101 are invalid) I can prove that count of $n$ bit binary numbers without 01 is n+1. Because they are one of these: $00...00$ $10....00$ $110...00$ ... $11...11$ But I don't know how to prove LHS is equal to this. I think it maybe uses inclusion exclusion because the first term of sum is count of all $n$ bit binary numbers but I don't know what should I say for other terms.","['combinations', 'combinatorics', 'combinatorial-proofs']"
3191340,"If the characters of two representations are equal, what additional information do we need to conclude they're isomorphic?","Let $(\pi,V)$ be a finite dimensional complex representation of a group $G$ .  If $G$ is finite, then $\pi$ is determined up to isomorphism by its character $\chi_{\pi}$ . What about the case where $G$ is not necessarily finite?  I doubt it's still true that $\pi$ is determined by its character, even up to semisimplification. If $\pi_1, \pi_2$ are two semisimple representations of $G$ with the same character, what other properties would $\pi_1$ and $\pi_2$ have to share to conclude they are isomorphic?  For example, suppose $H$ is a subgroup of finite index of $G$ , and we have not only $\chi_{\pi_1} = \chi_{\pi_2}$ , but also $\pi_1|H \cong \pi_2|H$ . The reason I'm asking is I have a semisimple representation $\pi$ of a group $G$ which I believe is isomorphic to some induced representation $\operatorname{Ind}_H^G \sigma$ for a semisimple representation $\sigma$ of $H$ .  I know how to directly construct an isomorphism between these guys, but it's very tedious.  But it's easy for me to show that their characters are equal and that their restrictions to $H$ are isomorphic.","['representation-theory', 'group-theory', 'linear-algebra']"
3191348,Markov Chain upper bound on the probability of hitting time,"I encountered the following problem. $\{x_t\}$ : Markov chain in discrete time; $\Omega$ : a finite state space s.t. $|\Omega|=n<\infty$ ; $\tau_w\equiv\min\{t\ge 0\,|\,x_t=w\}$ , $w\in\Omega$ (first hitting time). Prove: For each $T\ge 1$ and every $x,y\in \Omega$ $$\Pr(\tau_y=T|x_0=x)\le\frac nT.$$ Effort: This is a surprising result as the setup is terribly general. I proceeded by induction. Using recursive characterization, we have $$\Pr(\tau_y=T+1|x_0=x)=\sum_{z\ne y}\Pr(\tau_y=T|x_0=z)p(x,z),$$ where $p(x,z)=\Pr(x_{t+1}=x|x_t=z)$ is the transition probability. For each Markov chain, using the inductive hypothesis, we can easily confirm that this holds for $T$ sufficiently large. So far I cannot see how to show this for a general $T\ge n+1$ . Any hints or comments will be highly appreciated!","['stochastic-processes', 'probability-theory', 'markov-chains']"
3191498,"Does a sequence $a_n$ converge if $|a_n-\frac{1}{n} \sum_{i=1}^n a_i| \to 0$, as $n \to \infty$?","Let $\{a_n\}$ be a real sequence. If $\lim\limits_{n\to \infty} \left|a_n - \frac{1}{n}\sum\limits_{i=1}^n a_i \right|= 0$ , do we have $\lim\limits_{n\to \infty} a_n$ convergent? This is somehow an inverse version of Cesaro mean convergence, which says that if $\lim\limits_{n\to \infty} a_n = L$ , then $\lim\limits_{n\to \infty} \frac{1}{n}\sum\limits_{i=1}^n a_i = L$ . See On cesaro convergence: If $ x_n \to x $ then $ z_n = \frac{x_1 + \dots +x_n}{n} \to x $","['convergence-divergence', 'sequences-and-series']"
3191513,Does digit $6$ always lead to $\ 25921=161^2\ $?,"Consider prime numbers with the property that the product of the factorials of the digits plus $1$ is a perfect square, for example the prime $$30241$$ leads to the square $$3!\cdot 0!\cdot 2!\cdot 4!\cdot 1!+1=17^2$$ If the prime number starts with digit $6$ , the only square seems to be $\ 25921=161^2\ $ which appears if the prime contains of one digit $6$ , two digits $3$ and the rest digits $0$ or $1$ . Is another square possible, when the prime starts with digit $6$ ? To this, we would need factorials below $10!$ , at least one of them $6!$ , such that their product plus $1$ is a perfect square , other than $6!\cdot 3!\cdot 3!+1$","['factorial', 'number-theory', 'elementary-number-theory', 'square-numbers', 'prime-numbers']"
3191570,Are there trilinear inner products?,"Is there such a thing as a ""trilinear inner product""? The definition of an inner product is: Let $H$ be a vector space over $\mathbb{K}\in \{\mathbb{R,C}\}$ . An inner product is a map $\langle \cdot|\cdot\rangle: H^2 \to \mathbb{K}$ such that for all $x,y,z \in H$ and $\lambda \in \mathbb{K}$ the following properties hold: Bilinearity: $\langle x+\lambda y | z\rangle = \langle x|z\rangle + \lambda \langle y|z\rangle $ Complex conjugacy: $\overline{\langle y | x \rangle} = \langle x | y \rangle$ Positive definiteness: $||x||^2:=\langle x | x \rangle$ > 0 if $x \neq 0$ Can this be modified to have a trilinear map $\langle \cdot |\cdot| \cdot \rangle : H^3 \to \mathbb{K}$ ? Would it for example be possible to make $L^3$ into a ""trilinear inner product space"" like $L^2$ is a ""bilinear inner product space""? What is so special about the number $2$ in this context? Of course $2$ is the only number that is conjugate to itself in the sense that $\frac{1}{2}+\frac{1}{2}$ , so there would be no nice identification of this trilinear inner product space with its dual. I guess that there is no useful notion because the complex conjugacy can't be modified to get a trilinear inner product: $\mathbb{C}$ is a field extension of degree $2$ of $\mathbb{R}$ , but there is no field extension of degree $3$ of the reals this inner product could be defined over. What if the ""trilinear space"" is solely defined over $\mathbb{R}$ ?","['inner-products', 'multilinear-algebra', 'linear-algebra', 'functional-analysis']"
3191645,Uniqueness of spanning tree on a grid.,"When I was at the Graduate Student Combinatorics Conference earlier this month, someone introduced me to a puzzle game called Noodles! . The game starts with a collection of ""pipes"" on a grid (centered on each vertex), clicking on a piece rotates it $90^\circ$ , and a piece can be rotated any number of times. The goal is to turn the final configuration of pipes into a spanning tree (of the grid graph), as shown in the screenshots below. Example Question We left the conference with an unsolved question:
Are solutions to this puzzle always unique? Or is it possible to come up with a starting configuration (on any size grid) that has multiple trees as solutions? (The prevailing guess is that solutions are unique, but nobody could manage to prove it.)","['graph-theory', 'trees', 'puzzle', 'combinatorics']"
3191721,Minimum number of dominoes on an $n \times n$ chessboard to prevent placement of another domino.,"OEIS sequence A280984 (based on this Math Stack Exchange question ) describes the minimum number of dominoes on an $n \times n$ chessboard to prevent placement of another domino. The sequence begins: 0, 2, 3, 6, 9, 12, 17, 22, 27, 34, 41, 48, 57, 66, 75 Fifteen terms are known, and a few folks have conjectured that $$
   A280984(n) = \left\lceil \frac {n^2}3 \right\rceil \text{ for } n > 1.
$$ Andrey Zabolotskiy has a proof that (1) this is a lower bound , and (2) this conjecture holds when $n$ is a multiple of $3$ . In order to prove the conjecture it is sufficient to give a tiling strategy for the $(3n+1)\times(3n + 1)$ and $(3n+2)\times(3n+2)$ boards. Example of $3n \times 3n$ boards. To achieve this bound for the $3n \times 3n$ boards you can just do the naive placement, but for the other boards, it's not clear which placement strategy to use. (Notice that the second and third examples are not optimal: the last two boards have $35$ tiles and $42$ tiles respectively, but there exist $34$ and $41$ tile placements.) Question Is there simple strategy for the $(3n+1) \times (3n+1)$ and/or the $(3n+2) \times (3n+2)$ boards that attains the lower bound? Or is the conjecture false?","['graph-theory', 'chessboard', 'puzzle', 'combinatorics']"
3191739,Meromorphic continuation of the multifactorial,"The double factorial $z!!$ , like the normal factorial function, can be extended to the complex plane using $$ z!! = 2^{z/2} \left(\frac{\pi}{2}\right)^{(\cos\pi z-1)/4} \left(\frac{z}{2}\right)! $$ which is a slightly nicer-looking variant from Wolfram Mathworld and is what you get when you type in ""double factorial"" in Wolfram Alpha. (NB $z! = \Gamma(z+1)$ just for consistency here.) The function can take complex numbers, and satisfies the empty product , $0!! = 1$ as well as the double factorial for negative numbers using the identity $$ x!! = x(x-2)!! \implies x!! = \frac{(x+2)!!}{x+2} $$ thus the poles at even negative numbers and the intermediate values $(-1)!! = 1$ , $(-3)!! = -1$ , $(-5)!! = 1/3$ , etc. My question is that is there a similar extension for the triple factorial and further multifactorials? I know there are extensions for the general multifactorial, but they don't cover all the premises raised here. For example, this function is one possible extension: $$ z\overset{\alpha}{\overbrace{!\cdots !}} = \alpha^{(z-1)/\alpha}\frac{(z/\alpha)!}{(1/\alpha)!}$$ but here $0!\cdots ! \neq 1$ . Has there been found a meromorphic function $z\overset{\alpha}{\overbrace{!\cdots !}}, \alpha\gt 0$ (probably in the form $f(z)(z/\alpha)!$ ) to the multifactorial such that they agree with the following...? $$z\overset{\alpha}{\overbrace{!\cdots !}} = \prod_{k=0}^{\left\lfloor\frac{z-1}{\alpha}\right\rfloor}(z-\alpha k)\qquad z\in\mathbb{N} \tag{1} $$ $$ \left.\begin{aligned} z\overset{\alpha}{\overbrace{!\cdots !}} = \frac{(z+\alpha) \overset{\alpha}{\overbrace{!\cdots !}}}{z+\alpha} \\ \implies (-n\alpha)\overset{\alpha}{\overbrace{!\cdots !}} = \pm\infty \end{aligned} \ \right\} \ n\in\mathbb{N}\backslash 0 \tag{2} $$ $$ 0\overset{\alpha}{\overbrace{!\cdots !}}  =1 \tag{3}$$","['factorial', 'analytic-continuation', 'complex-analysis', 'gamma-function', 'meromorphic-functions']"
3191751,Help on proof of $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$ [duplicate],"This question already has answers here : Combinatorial proof of summation of $\sum\limits_{k = 0}^n {n \choose k}^2= {2n \choose n}$ (9 answers) Closed 5 years ago . The proof is required to be made through the binomial theorem. I will expose the demonstration I was tought, and forward my questions after exposing it. You'll see question marks like this one (?-n) on points I don't quite understand, where $n$ is the numeration of the mark. This are the doubts I have about the demonstration, the which I hope someone can clarify. Prove that $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$ . We will use the following equality, and call it $P$ : $(1+x)^n(1+x)^n=(1+x)^{2n}$ (?-1) The result will be proved finding the $x^n$ coefficient of both terms of this equality (?-2) . According to the binomial theorem, the left-hand side of this equation is the product of two factors, both equal to $\binom{n}{0}1+\binom{n}{1}x+...+\binom{n}{r}x^r+...+\binom{n}{n}x^n$ When both factors multiply, a term on $x^n$ is obtained when a term of the first factor has some $x^i$ and the term of the second factor has some $x^{n-i}$ . Therefor the coefficients of $x^n$ are $\binom{n}{0}\binom{n}{n}+\binom{n}{1}\binom{n}{n-1}+\binom{n}{2}\binom{n}{n-2}+...\binom{n}{n}\binom{n}{0}$ . Since $\binom{n}{n-r}=\binom{n}{r}$ , the previous summation is equal to $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2$ . So the left hand side of the equation we are asked to proove is a coefficient of $x^n$ . When we expand the right-hand side of the equation $P$ , we find that $\binom{2n}{n}$ is a coefficient of $x^n$ . Therefore (?-3) the left-hand side of the equation we were asked to prove is in deed equal to $\binom{2n}{n}$ . In conclussion, $\binom{n}{0}^2 + \binom{n}{1}^2 + ... + \binom{n}{n}^2 = \binom{2n}{n}$ . This was all the demonstration. My doubt one (?-1) goes about where the heck does this equation come from? How would I know what equation to come up with if requested to prove a different equality? Doubt two (?-2) goes about why would the solution of the first equation would have anything to do with finding the $x^n$ coefficients of the one I just made up (see doubt one). Doubt three (?-3) goes about why demonstrating that $a$ is a coefficient of $x^n$ on the left hand side of the equation I made up, and that $b$ is a coefficient of $x^n$ on the right-hand side of this equation as well, would prove my original equation, the one I was supposed to prove on the first place? I know there are many doubts here, I hope you guys can help me. Sorry for the long post, it's a long demonstration.","['binomial-coefficients', 'binomial-theorem', 'discrete-mathematics']"
3191887,Countability of a denumerable union of countable sets,"I am trying to prove the following denumerable $$\bigcup_ {n \in \Bbb N} \left\{\frac{n}{2^k} : k \in \Bbb N\right\}$$ I am supposed to use theorems from my book to prove this denumerable. There is a theorem in my book that states ""If $\mathscr A$ is a denumerable family of countable sets, then $\bigcup_ {A \in \mathscr A}A$ is countable"". I am really hung up on the ""syntax"" in proving this. It is clear to be that the number of sets is countably infinite. ans that each individual sets for a fixed in is countable (also denmumerable). It seems like ""drawing"" the Cantor diagonalization for n/k would work, but doesn't really feel like a formal proof.  But creating a bijection seems trivial. Would I define $f: A_n \times \Bbb N$ given by $f(x)=A_x$ and prove this a bijection (surjective and 1-1)? What would this even look like? Next I assume I'd do something similar for a set for a fixed $n \in \Bbb N$ . I.e. show a bijection from $\Bbb N$ to $\frac{n}{2^k}$ . Like $f(k)=\frac{n}{2^k}$ ? Is the best way to do this? What would these functions look like? I feel like the first one would be super trivial to show is a bijection. Any help appreciated.","['elementary-set-theory', 'functions']"
3191894,"If a multivariate function has a gradient at $x$, then it is always differentiable at $x$, right?","By being differentiable, I mean 
a function of several real variables $f: \mathbb{R^m}\rightarrow \mathbb{R}$ is said to be differentiable at a point $x_0$ if there exists a linear map $J: \mathbb{R}^m → \mathbb{R}$ such that $\lim_{\mathbf{h}\to \mathbf{0}} \frac{\|\mathbf{f}(\mathbf{x_0}+\mathbf{h}) - \mathbf{f}(\mathbf{x_0}) - \mathbf{J}\mathbf{(h)}\|}{\| \mathbf{h} \|} = 0.$ By having a gradient at $x_0$ , I mean there exists a vector denoted as $\nabla f$ such that its dot product with any unit vector $v$ is the directional derivative of $f$ along $v$ at the point $x_0$ . That is, $$\nabla f \cdot v = D_v f(x_0)$$ We assume all the directional derivative of $f$ at $x_0$ exists, that being said $f$ may or may not be differentiable at $x_0$ , if in addition we assume the gradient exists at $x_0$ , then $f$ should be differentiable at $x_0$ , right? I think this is simple or well known question, I can visualize it in the following way: if $f$ has a gradient at $x_0$ then all its directional derivative should lie in the same plane defined by $\nabla f$ , therefore it should be differentiable using the definition above.  But surprisingly I couldn't find any rigorous proof or material discussing this. Is this too simple or too well-known?","['real-analysis', 'multivariable-calculus', 'calculus', 'vector-analysis', 'derivatives']"
3191916,"Prove that if two graphs, $G$ and $\overline{G}$, are isomorphic, the number of nodes cannot be twice an odd number.","Having a really hard time going about proving this. First, Graph $G$ is constructed by having $n$ nodes and joining some pairs of distinct nodes with at most one line. Second, Graph $\overline{G}$ is constructed by having the same $n$ nodes as Graph $G$ but instead joining two nodes with a line if and only if they were not joined in $G$ . From what I gathered, Graphs $G$ and $\overline{G}$ can look like below if $n=4$ : How would I go about proving that if Graphs $G$ and $\overline{G}$ are isomorphic then $n$ cannot be twice an odd number (6, 10, 14 ...)?","['graph-theory', 'graph-isomorphism', 'discrete-mathematics']"
3191954,"Rudin Principles of Mathematical Analysis Chapter 10, Exercise 8","I'm working on exercises of chapter 10 in Baby Rudin. I refer to R. Cooke's solutions manual to Baby Rudin while I'm solving those exercises.( https://minds.wisconsin.edu/handle/1793/67009 ) But I think there is a wrong solution for Chap 10, exercise 8. Baby Rudin, chap 10, ex 8 the wrong part of a solution for chap 10, ex 8 Using Theorem 10.9 in Baby Rudin, which is about change of variables on a multiple integral, I think we should represent a integrand on the right side with a mapping T, not an inverse of T. Could you guys check if I'm right or that solution is right?","['analysis', 'real-analysis', 'multivariable-calculus', 'change-of-variable', 'multiple-integral']"
3192014,find the maximum value of $xy + yz +zx$ [duplicate],This question already has answers here : maximum value of $xy+yz+zx.$ given $x+2y+z=4$ (3 answers) Closed 5 years ago . find the maximum value of $$xy + yz +zx$$ given that $x+2y+z=4$ my attempt : $(x+y+z)^2=x^2+y^2+z^2+2(xy+yz+zx) $ or $2S=2(xy+zx+zy)=(x+y+z)^2 -x^2-y^2-z^2=(4-y)^2-x^2-y^2-z^2$ $2S=-x^2-z^2-8y+16=-x^2-z^2+4x+4z$ from the the above we can say due to symmetry maximum value occurs at $x=z$ hence $S=-x^2+4x$ whose maximum is 4 is this right or/and is there a better way ??,"['algebraic-number-theory', 'algebra-precalculus']"
3192025,"If $\vec u$ is orthogonal to both $\vec v $ and $\vec w$, and $\vec u$ not equal to $0$, prove $\vec u$ is not in the span of $ \vec v$ and $\vec w$.","QN: If u is orthogonal to both v and w , and u ≠ 0, argue that u is not in the span of v and w . Where I am at: I get stuck when it comes to solving my augmented matrix with Gauss Jordan Elimination. I also tried formulating the following steps to solve the problem. Create instances of u , v and w that pertain to the question. My visualisation in Geogebra can be viewed here: https://ggbm.at/b6xvwhpa Set u = a v + b w = u (where a and b are constants) Disprove (2) However, I could not get past step 1. Any pointers would be greatly appreciated.","['matrices', 'linear-algebra']"
3192074,Concerning Hartshorne's proof of the Vanishing Theorem of Grothendieck (Hartshorne III 2.7),"At certain point of the proof of the theorem in Hartshorne's book, he mentioned the following in which I don't understand: In Step 3, he said we can reduce our consideration to a sheaf $\mathscr{F}$ of abelian group which is generated by a single section over certain open set $U\subset X$ ( $X$ is irreducible topological space). Then in that case $\mathscr{F}$ $\textbf{is a quotient of the sheaf}$ $\textbf{Z}_U$ (where $\textbf{Z}$ is a constant sheaf on $X$ ). Question: I couldn't see how the statement is true. Assume the things mentioned in (1) is correct, then we will be able to write an exact sequence of $\mathscr{F}$ as $$0\to\mathscr{R}\to\textbf{Z}_U\to\mathscr{F}\to 0$$ where $\mathscr{R}$ is of course, the kernel of the map from $\textbf{Z}_U$ to $\mathscr{F}$ . Then for any $x\in U$ , $\mathscr{R}_x$ is a subgroup of $\textbf{Z}$ and we have certain $d\in\textbf{Z}$ which is the least positive integer which occurs in any of the groups $\mathscr{R}_x$ , no problem. But the problem comes when he said there exist another open subset $V\subseteq U$ such that $\mathscr{R}|_V\cong d\cdot\textbf{Z}|_V$ as a subsheaf of $\textbf{Z}|_V$ . Question: Explicitly why there is such an existence? Thank you in advance and please let me know if there is any unclear point in my question.","['exact-sequence', 'algebraic-geometry', 'sheaf-cohomology', 'sheaf-theory']"
3192104,Particular solution for $D^2 y + 4D y + 4y =18 \cosh x$?,"Set $$y_p = K \cosh x~, \quad y_p' = K \sinh x~, \quad y_p''=K\cosh x$$ and substitute these functions into the original equation, then: $$K\cosh x+4K\sinh x+4K\cosh x=18\cosh x$$ the coefficient of $\cosh x$ would be $5K=18$ , the coefficient of $\sinh x$ is $4K=0$ . (the contradiction would come out ??) How could I get the particular solution $Y_p$ ?","['homogeneous-equation', 'ordinary-differential-equations']"
3192113,"Low-degree polynomial $T\in\mathbb F[x,y]$ with $T(P(z),Q(z))=0$","Given polynomials $P,Q\in\mathbb F[z]$ over a finite field $\mathbb F$ , one can find a non-zero polynomial $T\in\mathbb F[x,y]$ such that $T(P(z),Q(z))=0$ for any $z\in\mathbb F$ . Is there a way to control the degree of $T$ ? Can one guarantee that there exists $T$ with, say, $\deg T\le 2(\deg P+\deg Q)$ , or something of this sort?","['finite-fields', 'algebraic-geometry', 'polynomials']"
3192205,Power sets : Do the relations between P(A) and P(B) always mirror the relations between the sets A and B?,"If I am correct it is true that: (1) "" $P(A)$ is included in $P(B)$ "" implies "" $A$ is included in $B$ "". (2) "" $P(A) = P(B)$ "" implies "" $A = B$ "". Might I conclude from this that the power sets of two sets always have the same relations as these two sets have with one another? Are there classical counterexamples to this (hasty) generalization? I can think of this as a counterexample : The fact that $A$ and $B$ are disjoint does NOT imply that $P(A)$ and $P(B)$ are disjoint. Attenpt to prove (1) using the theorem : "" $\{ x \}$ belongs to $P(S)$ "" $\Longleftrightarrow$ "" $x$ belongs to $S$ . Let's admit that : $P(A)$ is included in $P(B)$ . Now, suppose (in view of refutation) that $A$ is not included in $B$ . It means that there exists an x such that x belongs to $A$ but not to $B$ . And consequently that there is an $x$ such that $\{ x \}$ belongs to $P(A)$ but not to $P(B)$ . If this were true, there would be a set $S$ such that $S$ belongs to $P(A)$ but not to $P(B)$ . This contradicts our hypothesis according to which $P(A)$ is included in $P(B)$ . Conclusion: "" $P(A)$ is included in $P(B)$ "" implies "" $A$ is included in $B$ "".",['elementary-set-theory']
3192271,Function $F$ is surjective if and only if $F$ is $1-1$ [duplicate],"This question already has answers here : Surjectivity of $f:S\to S$ implies injectivity for finite $S$, and conversely (3 answers) Closed 5 years ago . While I was working on proofs of functions, the following claim occurred to me that I think it is correct but I could not prove it. Please note that the claim may not be correct since it is just my hypothesis. So my hypothesis is: Let $A$ be a finite set and $F$ , a function $F:A\rightarrow A$ . $F$ is surjective if and only if $F$ is injective. Here is how I tried to prove it: $\rightarrow$ Let $X$ , $Y$ , $Z$ and assume $F(X)=F(Y)$ . We need to show that $X=Y$ to prove that $F$ is $1-1$ . $F(X)$ and $F(Y)\in A$ and $F$ is surjective then $\text{Img}(F)=A$ . $A$ is a finite set then $\text{Img}(F)$ is a finite set. how can I continue the proof? $\leftarrow$ We need to show that $A=\text{Img}(F)$ to show that $F$ is surjective but we know that $\text{Img}(F)\subseteq A$ so we need to show $A\subseteq \text{Img}(F)$ . Let $x\in A$ . We need to show $x\in\text{Img}(F)$ . $A=\text{Dom}(F)$ and $x\in\text{Dom}(F)$ and $F$ is function so there exists $y$ such that $F(x)=y$ . And here also I got stuck. Any ideas how to continue the proof if it is right..
I know that I did not use that $A$ is a finite set and $F$ is injective in one way because I do not see where I can use in my proof.","['elementary-set-theory', 'proof-writing', 'proof-verification']"
3192310,Drawing without replacement: why is the order of draw irrelevant?,"I am trying to wrap my head around this problem: Daniel randomly chooses balls from the group of $6$ red and $4$ green. What is the probability that he picks $2$ red and $3$ green if balls are drawn without replacement. What I remember from my college days that the probability is found by this formula: $$P(A)=\frac{\binom{6}{2}\binom{4}{3}}{\binom{10}{5}}=\frac{5}{21}$$ Is this correct? I am trying to understand why this works. Wouldn't probability depend on the order of balls drawn as the number of balls is changing after each draw? I get how we obtain numerator and denominator, I just feel that the probability should be dependent on the order. For example, the probability to pick red first is $\frac{6}{10}$ so the probability for the second draw becomes $\frac{5}{9}$ for red and $\frac{4}{9}$ for green. But if the first picked ball is green, the probability for the second draw becomes $\frac{6}{9}$ for red and $\frac{3}{9}$ for green. What am I missing?","['probability-theory', 'probability']"
3192328,Calculating $\lim_{n\to\infty}\left(\frac{\sin(2\sqrt 1)}{n\sqrt 1\cos\sqrt 1} +\cdots+\frac{\sin(2\sqrt n)}{n\sqrt n\cos\sqrt n}\right)$,"Using the trigonometric identity of $\sin 2\alpha = 2\sin \alpha \cos \alpha$ , I rewrote the expression to: $$\lim_{n\to\infty}\left(\frac{\sin(2\sqrt 1)}{n\sqrt 1\cos\sqrt 1} + \cdots+\frac{\sin(2\sqrt n)}{n\sqrt n\cos\sqrt n}\right) = \lim_{n\to\infty}\left(\frac{2\sin(\sqrt 1)}{n\sqrt 1} +\cdots+\frac{2\sin(\sqrt n)}{n\sqrt n}\right)$$ I then tried using the squeeze theorem to get the limit, but I can't get an upper sequence that converges to zero, and the best I could get is a sequence that converges to two: $$\frac{2\sin(\sqrt 1)}{n\sqrt 1} + \cdots+\frac{2\sin(\sqrt n)}{n\sqrt n} \leq \frac{2}{n\sqrt 1} + \cdots+\frac{2}{n\sqrt n} \leq \frac{2}{n}\cdot n \longrightarrow 2$$ What am I missing?","['real-analysis', 'sequences-and-series', 'limits', 'trigonometry', 'infinitesimals']"
3192356,$\cos(n\vartheta)=\frac{a_n}{3^n}$,"I want to show, if i know that $\cos(\vartheta)=\frac{1}{3}$ than $\cos(n\vartheta)=\frac{a_n}{3^n}$ for $n\in \mathbb{N}$ , where $a_n \in \mathbb{Z}$ , $3 \nmid a_n $ My approach was to do it by induction.
For n=1 it is clear.
Than i used $\cos(\alpha+\beta)=\cos(\alpha)\cos(\beta)-\sin(\alpha)\sin(\beta)$ with $\alpha=n\vartheta$ and $\beta=\pm \vartheta$ .
I added the results and got $\cos((n+1)\vartheta)=2\cos(\vartheta)\cos(n\vartheta)-\cos((n-1)\vartheta)$ . I get then $\cos((n+1)\vartheta)=2\frac{1}{3}\frac{a_n}{3^n}-\cos((n-1)\vartheta) = \frac{2a_n-\cos((n-1)\vartheta)3^{n+1}}{3^{n+1}}$ I don't know how to argue that $2a_n-\cos((n-1)\vartheta)3^{n+1} \in \mathbb{Z}$ and that $3 \nmid 2a_n-\cos((n-1)\vartheta)3^{n+1}$ . Thanks in advance","['calculus', 'proof-writing', 'algebra-precalculus']"
3192477,Prove that if $n \in \omega$ then $n \notin n$,"Prove that if $n \in \omega$ then $n \notin n$ . I'm trying to do it by induction. Consider $S=\{n \in \omega : n \notin n\}$ $0 \in S$ : $\emptyset \notin\emptyset $ $i\in S \Rightarrow s(i) \in S$ : $i \notin i \Rightarrow \{i\} \notin \{i\}$ (if $\{i\} \in \{i\} \text{ then } i=\{i\}, \text{ therefore } i \in i$ ). I want show now that $\{i\} \notin\{i\} \Rightarrow s(i)=\{i\} \cup i \notin s(i)=\{i\} \cup i$ . Suppose by absurdity that $\{i\} \cup i \in\{i\} \cup i$ then: or $\{i\} \cup i \in \{i\} \Rightarrow \{i\}\cup i = i \text{ then } i \in i$ , contradition. or $\{i\} \cup i \in i$ . I do not know to get in a contradition. I do not have the axim of regularity.","['elementary-set-theory', 'induction']"
3192482,Every quasi-invariant measures is in an invariant measure class (Zimmer),"I'm reading ""Ergodic Theory and Semisimple Groups"" by Zimmer and at the very beginning of Chapter $2$ (pp. $8$ ) the author claims that An action with quasi-invariant measure can be thought of as an action with an invariant measure class. I interpreted this vague statement in  the following way: Every quasi-invariant measure is in the same measure class with an invariant measure. Question1: is this statement true? I don't see how to prove this fact. Question2: If question1 has a negative answer, how should such a statement be understood? Here the author assumes the group $G$ be locally compact second countable, the action on a standard Borel space $S$ (i.e. Borel isomorphic to a Borel subset of a Polish space) be Borel (i.e. measurable).
Moreover, a $\sigma$ -finite measure $\mu$ is said to be quasi-invariant under the action of $G$ iff for all $A\subseteq S$ , $g\in G$ we have $\mu(Ag)=0\iff\mu(A)=0$ .
It is invariant iff $\mu(Ag)=\mu(A)$ for all $A$ , $g$ .
Finally, two measures are said to be in the same measure class iff they have the same null sets. About my background: I have attended a basic measure theory course mostly focused on the real case. Whenever possible, a good reference that covers these topics is appreciated. Thank you in advance for your help.","['measure-theory', 'ergodic-theory', 'topological-groups', 'polish-spaces']"
3192489,"If $f_n:\Omega\to\Omega$ are homeomorphisms of a planar domain $\Omega$ such that $f_n\to f$, $f_n^{-1}\to g$ in $L^1$, is $f=g^{-1}$?","In Marchioro and Pulvirenti's book Mathematical Theory of Incompressible Nonviscous Fluids , the proof of global well-posedness of the 2D Euler equation in a bounded domain $\Omega\subset\mathbb R^2$ works by approximating the flow $\Phi^n_t(x)$ of the fluid given initial vorticity $\omega_0\in L^\infty(\Omega)$ , and showing that $\Phi^n\to\Phi$ for some $\Phi$ , with convergence in $L^\infty([0,T];L^1(\Omega))$ , and similar arguments show convergence $(\Phi^n)^{-1}\to\tilde\Phi$ for some $\tilde\Phi$ . Implicitly, the authors assume that $\Phi_t^{-1}=\tilde \Phi_t$ for all $t$ , but I don't see why this must hold. If it is of any use, I have found that the $\Phi^n$ and their inverses must all be incompressible flows, which are differentiable in time, and $C^{0,s}$ in space for some $s>0$ .","['fluid-dynamics', 'differential-geometry', 'real-analysis']"
3192540,Is a HNN extension of a virtually torsion-free group virtually torsion-free?,"Let $G=\langle X\ |\ R\rangle$ be a (finitely presented) virtually torsion-free group.  Let $H,K<G$ be isomorphic (finite index) subgroups of $G$ and let $\varphi:H\rightarrow K$ be an isomorphism. Define the HNN extension $\Gamma$ of $G$ and $\varphi$ in the usual way, i.e. $\Gamma=\langle G,t\ |\ tht^{-1}=\varphi(h)\ \forall h\in H\rangle$ . Is $\Gamma$ virtually torsion-free? My thought is that if $T$ is a finite index torsion-free subgroup of $G$ and if $H$ and $K$ are finite index we should be able to look at the intersection of each of them with $T$ .  So the group $\langle T,t\ |\ tht^{-1}=\varphi(h)\ \forall h\in T\cap H\rangle$ would be a finite index torsion-free subgroup of $\Gamma$ . Also, can we say anything about the smallest index of a torsion-free subgroup?  For example if $G$ contains a torsion-free subgroup of index $k$ , does $\Gamma$ contain a torsion-free subgroup of index $k$ ?  Or is the index bounded by some function of $k$ ?","['combinatorial-group-theory', 'geometric-group-theory', 'group-theory', 'group-presentation']"
3192561,"How to create a function that returns original inputs from generated output, with no prior knowledge of inputs?","I'm looking for a way to convert 4 numbers into 1 number, then convert that 1 number back into the original inputs with no knowledge of said inputs. 4 Numbers Convert into 1 number (summing/multiplying/anything) Use the result to generate the original 4 numbers. Example: Four Original numbers:
a = 2;
b = 5;
c = 10;
d = 1; Combined number:
x = 2 + 5 + 10 + 1
x = 18 One function or 4 functions to generate original numbers only using x:
f(x) -->
a = 2;
b = 5;
c = 10;
d = 1; (9 numbers would also be appreciated if this problem is possible)",['functions']
3192578,Existence of non-constant holomorphic map between two given compact Riemann surfaces,"Given two compact Riemann surfaces $X,Y$ , can we always find a non-constant holomorphic map from $X$ to $Y$ ? In particular, when $Y$ is a elliptic curve, does that map exist? Michael Albanese has given a negative answer for the case that when $X$ is $\mathbb{C}P^1$ , but I still want to know if it's true for the cases that $X$ has genus $>0$ ?","['complex-geometry', 'complex-manifolds', 'algebraic-geometry', 'riemann-surfaces']"
3192613,Special case of Bertrand Paradox or just a mistake?,"I've been working on a question and it seems I have  obtained a paradoxical answer. Odds are I've just committed a mistake somewhere, however, I will elucidate the question and my solution just in case anyone is interested. I want to know what is the average distance between two points on a circle of radius 1 where we consider only the boundary points. My attempt is as follows: Consider a segment of the diameter x which is uniformly distributed between 0 and 2. Then you can calculate the distance between the points (2,0) and the point determined by x just by elementary geometry as this picture shows: Here in the picture, the green segment is the geometric mean and the orange one is the distance whose distribution we want to know.
Just by calculating the expected value, we obtain: $E\left(\sqrt{(4-2X)}\right) = \int_{0}^{2} \sqrt{(4-2x)}\cdot\frac{1}{2} dx = 1.333.... = \frac{4}{3}$ Where $\sqrt{(4-2x)}$ is the transformation of the random variable and $\frac{1}{2}$ is the pdf of a uniform distribution $[0,2]$ . Also, if we derive the pdf of the transformation we obtain the same result: $y = \sqrt{(4-2x)} , x = 2- \frac{y^2}{2}, \mid\frac{d}{dy}x\mid = y$ $g(y)=f(x)\cdot\mid\frac{d}{dy}x\mid = \frac{1}{2}\cdot y$ $E(Y)= \int_{0}^{2}y\cdot\frac{1}{2}\cdot y dy = 1.333.... = \frac{4}{3} $ I have seen a different approach somewhere else where the distribution of the angle is considered as a uniform distribution between 0 and $\pi$ and the final result was: $1.27... = \frac{4}{\pi}$ That's pretty much the problem I found. Maybe I just did it wrong in some step but it all makes sense to me. I know this is not exactly what we refer as Bertrand paradox but it just suggests something like that because both problems handle with segments in circumference and maybe my result is wrong because it does not hold for rotations of the circle or something like that (I read a little bit about the Bertrand's Paradox). That's pretty much it. Also sorry for my bad English and maybe I'm also wrong in something pretty elemental since I've just started learning about probability theory. It's also my first post so I will try to improve my exposition and LateX use in the following ones.","['probability-theory', 'geometric-probability', 'random-variables']"
3192617,Directed set and partially/totally ordered sets.,"I am barely new to order theory and this motivates if the question is trivial. I understood the definitions of preorder, partially and totally ordered sets and well ordered sets. 
In particular there is a nice hierarchy among the last three and I think that for a newcomer these are the most natural enviroments to think of, as one is used to the real line and power sets. Moreover these three capture the ""natural"" idea of order I may think of, at different levels. On the other hand directed set are less intuitive to me. Clearly do not fit into the hierarchy since they do not libk with partially ordered sets, but totally ordered sets are directed, even if the converse does not hold. Hence I ask: in which sense directed sets generalize totally ordered sets? Which is the intuition behind them? What aspects of order they capture and what motivates the name directed?","['elementary-set-theory', 'order-theory', 'well-orders']"
3192622,Sheaf of a Closed Subset,"I’ve been given the following definition: Let $(X,\mathcal{O}_X)$ be a ringed space which is locally isomorphic to an affine algebraic variety, and $Y\subseteq X$ be closed. Then for an open $V\subseteq Y$ , set $$\begin{align*}
\mathcal{O}_{0,Y}(V)=\{f:V\to k\mid{}&\exists U\subseteq X\text{ open such that }U\cap Y=V\\
&\text{and } g\in\mathcal{O}_X(U)\text{ such that } g\vert_V=f\}
\end{align*}$$ This defines a presheaf $\mathcal{O}_{0,Y}$ on $Y$ , but not, in general, a sheaf. However I’m struggling to come up with an example where this fails to be a sheaf. I thought I'd found a counterexample with $X=\mathbb{C}^2$ , $Y=V(xy)$ , $U=D(x)\cap Y$ and $V=D(y)\cap Y$ . Then $U\cap V=\varnothing$ , and so if $\mathcal{O}_{0,Y}$ were a sheaf, then we would be able to glue to make a function on $U\cup V$ which is say $1$ on $U$ and $-1$ on $V$ . I can show that we can't get such a function from gluing two functions on $D(x)$ and $D(y)$ , but we can take $\frac{x+y}{x-y}$ on $D(x-y)$ to give the required function. Then it isn't enough to just check the 'obvious' open cover, and I haven't yet been able to find a counterexample which works for every one. Any help would be much appreciated.","['affine-varieties', 'algebraic-geometry', 'affine-geometry', 'sheaf-theory']"
3192635,How to compute this improper integral?,"Let $n\geq1$ be an integer and let $$I_n=\int\limits_{0}^{\infty}\dfrac{\arctan x}{(1+x^2)^n} \,\mathrm dx$$ Prove that $$\sum\limits_{n=1}^{\infty}\dfrac{I_n}{n}=\dfrac{\pi^2}{6} \tag{1}$$ $$\int\limits_{0}^{\infty} \arctan x\cdot\ln\left(1+\frac{1}{x^2}\right) \,\mathrm d x=\dfrac{\pi^2}{6} \tag{2}$$ I have an idea that dominated convergence theorem may help here. But I am not getting the proper way.","['integration', 'calculus', 'improper-integrals', 'sequences-and-series']"
3192637,Simplifying $\sin\frac{11\pi}{12}\sin\frac{29\pi}{12}-\cos\frac{13\pi}{12}\cos\frac{41\pi}{12}$. Why do I get the wrong answer?,"Can someone explain why I get wrong answer in simplifying this expression? $$\sin\frac{11\pi}{12}\sin\frac{29\pi}{12}-\cos\frac{13\pi}{12}\cos\frac{41\pi}{12}$$ If we rewrite the expression with new angles, $$\frac{29\pi}{12}\to\frac{5\pi}{12} \quad\text{and}\quad \frac{13\pi}{12}\to\frac{11\pi}{12}$$ we don't change the value of the expression. But, if we now use sine of sum of angles, we get $\frac{1}{2}$ instead of $0$ . Why does this happen? Do angles need to be in the same quadrant for the formula to work?",['trigonometry']
3192644,Variant of the Strong Law of Large Numbers,"Let $X_1,X_2,\ldots$ be a i.i.d. sequence of random variables with uniform distribution on $[0,1]$ , with $X_n: \Omega \to \mathbf{R}$ for each $n$ . Question. Is it true that $$
\mathrm{Pr}\left(\left\{\omega \in \Omega: \lim_{n\to \infty}\frac{\sum_{1\le i\le j \le n}{\bf{1}}_{(-1/n,1/n)}{(X_i(\omega)-X_j(\omega))}}{n}=2\right\}\right)=1\,\,\,?
$$ Here ${\bf{1}}_A(z)$ is the characteristic function of $A$ , that is, it is $1$ if $z \in A$ and $0$ otherwise.","['law-of-large-numbers', 'probability-limit-theorems', 'uniform-distribution', 'probability-theory']"
3192647,"Understanding HNN extensions: intuition, examples, exercises.","What is an HNN extension? What would be some elementary, intuitive examples of them and what exercises involving them would you suggest? The Wikipedia definition is easiest to get to, since neither indexes of Magnus et al. nor Johnson's ""Presentation $\color{red}{s}$ of Groups (Old Version)"" indicate where they are. Here it is for convenience: Let $G$ be a group with presentation $G=\langle S\mid R\rangle$ , and let $\alpha: H\to K$ be an isomorphism between two subgroups of $G$ . Let $t$ be a symbol not in $S$ , and define $$G\ast_\alpha=\langle S, t\mid R, tht^{-1}=\alpha(h)\forall h\in H\rangle.$$ The group $G\ast_\alpha$ is called the HNN extension of $G$ relative to $\alpha$ . The original group $G$ is called the base group for the construction, while the subgroups $H$ and $K$ are the associated subgroups . The new generator $t$ is called the stable letter . The definition in Lyndon & Schupp's ""Combinatorial Group Theory"" , despite the concept being mentioned a few times prior to it, is on page 179. It's very similar: Let $G$ be a group, and let $A$ and $B$ be subgroups of $G$ with $\phi: A\to B$ an isomorphism. The HNN extension of $G$ relative to $A$ and $B$ and $\phi$ is the group $$G^\ast=\langle G, t\mid t^{-1}at=\phi(a), a\in A\rangle.$$ The group $G$ is called the base of $G^\ast$ , $t$ is called the stable letter , and $A$ and $B$ are called the associated subgroups . The definition in Baumslag's ""Topics in Combinatorial Group Theory"" , page 66, reads Definition 2: Let $$B=\langle X\mid R\rangle$$ be a group given by a presentation and suppose $U$ and $V$ are subgroups of $B$ equipped with an isomorphism $$\tau: U\stackrel{\sim}{\longrightarrow} V.$$ Then we term $$E=\langle X, t\mid R\cup\{ tut^{-1}=u\tau\}\rangle\quad(\text{where }t\notin X)$$ an HNN extension of $B$ with stable letter $t$ , associated subgroups $U$ and $V$ and associating isomorphism $\tau$ . These are all well & good. I can see how they are equivalent quite readily. The definition in Stillwell's ""Classic Topology and Combinatorial Group Theory (Second Edition)"" is on page 286 and is much different. I shan't copy it down, for it's quite lengthy, but, again, I can sort of see what it's getting at. There are three exercises on HNN extensions starting on that page too, now that I've looked at the dreaded topology book, so I'll give them a go now. If you have anything more to add to help me and others understand this important concept, please do so as a comment or even an answer. I still don't get the intuition behind'm and I know not of any tangible examples yet. Please help :)","['combinatorial-group-theory', 'intuition', 'group-theory', 'examples-counterexamples']"
3192694,"Which notation provides "" a limit does not exist""?","I was told by my teacher that it's not common to use a logical symbol like  ∄ to show a limit nonexistence. But i was kind of unsure due to the exact definition of this notation which is ""does not exist"".
So I'm looking for an authenticated source to prove my opinion. I was wondering if you know any:)","['notation', 'limits']"
3192728,What is the topology associated with the algebras for the ultrafilter monad?,"It is easy to find references stating that the category of compact Hausdorff spaces $\mathbf{CompHaus}$ is equivalent to the category of algebras for the ultrafilter monad, $\mathbf{\beta Alg}$ . After doing some digging, the $\mathbf{CompHaus}\to \mathbf{\beta Alg}$ half of the equivalence is simple enough, but I haven't been able to find a description of the $\mathbf{\beta Alg}\to \mathbf{CompHaus}$ half of this equivalence. I have tried to work it out, but I have little experience with topological spaces and am not sure what the associated topology ought to look like. I'm wondering if anyone has a good reference that describes the $\mathbf{\beta Alg}\to \mathbf{CompHaus}$ half of the equivalence, or can describe it here.","['general-topology', 'category-theory']"
3192777,Sequence such that $\lim\limits_{n\to \infty} \frac{x_1^2+x_2^2+...+x_n^2}{n}=0$,"Let $(x_n) _{n\ge 1}$ be a sequence of real numbers such that $\lim\limits_{n\to \infty} \frac{x_1^2+x_2^2+...+x_n^2}{n}=0$ . Prove that $\lim\limits_{n\to \infty} \frac{x_1+x_2+...+x_n}{n}=0$ . I used the definition of the limit to conclude that $\exists N\in \mathbb{N} $ such that $|\frac{x_1^2+x_2^2+...+x_n^2}{n}|<\frac{1}{n^2}$ , $\forall n\ge N$ . Hence, we get that $|x_1^2+x_2^2+...+x_n^2|<\frac{1}{n}$ . Now here comes the part where I am not really sure. I think that this implies that $\sum_{n=1}^{\infty}x_n^2=0$ , and as a result $x_n\to 0$ ,which solves the problem because if we use the Stolz-Cesaro lemma we get that $\lim\limits_{n\to \infty} \frac{x_1+x_2+...+x_n}{n}=\lim\limits_{n\to \infty} x_n=0$ .","['sequences-and-series', 'real-analysis']"
3192845,"For $\langle Tx, x \rangle \geq \|x\|^2$, prove a solution exists for $Tx = y$.","Edit I've posted this a couple other times, so I now plan on deleting those, and just using this one. Here's the original problem: $H$ is a real Hilbert space. Let $T: H\longrightarrow H$ be a bounded linear operator on $H$ such that the inner product satisfies $$\langle Tx, x \rangle \geq \|x\|^2 \tag{A}$$ Prove that a solution for the equation $Tx = y$ exists for all $y \in H$ . I've been looking at this problem and studying orthogonal complements and the direct sum theorem for a few days now, to no avail, so I suspect there's something very fundamental I don't understand here. Below I'm posting W. Zhan's proof that I don't understand, and a couple of starting points of my own. W. Zhan's, as mentioned below, isn't working for me, so I'm starting a bounty. Thank you. Proof given by W. Zhan Note that adjoint of a bounded operator always exists by Riesz Represenation Theorem. So we have $$ \langle Tx,x \rangle = \langle x, T^*x \rangle  \ge ||x||^2$$ So $T^*$ is injective. We also have $im T^\perp =  \ker T^*$ . Hence, $im T = H$ . I'm not sure what I'm not understanding. I do understand So $T^*$ is injective. and also We also have $im T^\perp =  \ker T^*$ but not how those two facts yield the conclusion. Here's a couple starts I've had on my own Proof (alternate start 1) Let $Tx = 0$ , then $$0 = \langle Tx, x\rangle \geq \|x\|^2 \geq 0$$ so $\|x \| = 0$ , but by definition of the norm, only if $x = 0$ . So $Tx = 0 \iff x = 0$ , therefore $T$ is injective. This means $T$ has an inverse linear operator $T^{-1}$ . Now, let $y \in \overline{TH}$ . By definition, $y$ is a limit point of $TH$ , so there exists a sequence $(x_n)$ in $H$ such that $(Tx_n) \longrightarrow y$ . However, $x_n = T^{-1}Tx_n$ , which implies $$\lim_{n\to \infty}x_n = \lim_{n\to \infty} T^{-1}Tx_n  = T^{-1}\lim_{n \to \infty}Tx_n = T^{-1}y$$ ... here I think I'm in trouble because I don't know if I can actually show $T^{-1}y$ exits, which I think is equivalent to showint $T^{-1}$ is bounded. Proof (alternate start 2) Here, as W. Zhan pointed out, it's easy to see that $(TH)^\perp = \{0\}$ , which implies that $TH$ is dense in $H$ . In other words, $\overline{TH} = H$ . So all that remains is to show that $TH$ is closed. But this is similar to the argument in my first attempt. Discussion I understand that $T^*$ is injective—because it is a linear operator and $Tx = 0 \implies x = 0$ . In fact $T$ is as well. So I believe another option is to show surjectivity of $T$ , but I can see how to do that. Would somebody mind proving the original proposition in a detailed way?","['hilbert-spaces', 'inner-products', 'functional-analysis', 'real-analysis']"
3192894,"Evaluate the definite integral $\int^{\infty }_{0}\frac{x \,dx}{e^{x} -1}$ using contour integration","My friend and I have been trying weeks to evaluate the integral $$\int^{\infty }_{0}\frac{x \,dx}{e^{x} -1} .$$ We have together tried 23 contours, and all have failed. We already know how to solve this with infinite sums (i.e., using the zeta function and the Basel problem), but we can't figure out how to solve it using contour integration methods. We already know the answer is $\frac{\pi^{2}}{6}$ .","['integration', 'improper-integrals', 'definite-integrals', 'calculus', 'contour-integration']"
3192898,"Prove $\exists\theta\in(0,1)$ s.t. $\Delta f=\frac{\partial f}{\partial x}\Delta x+\frac{\partial f}{\partial y}\Delta y$","Let $f(x,y)\in C^1$ in $\mathbb{R^2}$ and let $(x_0+\Delta x,y_0+\Delta y)$ and $(x_0,y_0)$ be points in $\mathbb{R^2}$ . Prove that $\exists\theta\in(0,1)$ such that: $$f(x_0+\Delta x,y_0+\Delta y)-f(x_0,y_0)=\\\frac{\partial f}{\partial x}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta x+\frac{\partial f}{\partial y}(x_0+\theta\Delta x,y_0+\theta\Delta y)\Delta y$$ At first glance, this seemed like the differentiability definition, but I tried to make the connection and unfortunately failed. I guess that MVT hides here, but I don't see how to rigorously reach it. Thanks! Note : I found a solution to this problem online (not here) but I couldn't understand it, so I'd really appreciate a somewhat detailed solution.","['partial-derivative', 'multivariable-calculus', 'calculus', 'derivatives']"
3192899,Find function $f(x)$ satisfying $\int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0$,"I am looking for a non-trivial function $f(x)\in L_2(0,\infty)$ independent of the parameter $n$ (a natural number) satisfying the following integral equation: $$\displaystyle\int_{0}^{\infty} \frac{f(x)}{1+e^{nx}}dx=0$$ or prove that $f(x)=0$ is the only solution. The similar question is here but there are no parameters in the integral and the answer is based upon hit and trial method. I want to know what would be the nice approach to tackle this problem. EDIT: Such a function may exist, here is an example due to Stieltjes. A function $f(x) = \exp(-x^{1/4}) \sin x^{1/4}$ satisfies $\int_0^{\infty} f(x) x^n dx = 0$ for all integers $n \ge 0$ . We use the substitution $x=u^4$ to write $I_n = \int_0^{\infty} f(x) x^n dx = 4 \int_0^{\infty} e^{-u} \sin(u) u^{4n+3} du$ ;
then integrate by parts four times (differentiating the power of $u$ , and integrating the rest) to show that $I_n$ is proportional to $I_{n-1}$ ,
and finally check that $I_0=0$ .(the Edit copied from here )","['integration', 'functional-analysis', 'analysis', 'real-analysis']"
3192969,Proving every group of order 2673 has a non-trivial proper normal subgroup,"If $|G|=2673=3^511$ then let $S \in Syl_3(G)$ . Then the number of conjugates of $S$ , $n_s \equiv 1$ mod $3$ and $n_s | 11$ . But the only divisors of 11 are 1 and 11, and therefore $n_s=1$ so the $S$ is the unique sylow-3 subgroup of G and hence is normal. Is this correct?","['group-theory', 'abstract-algebra', 'sylow-theory']"
3193095,Are there $k$-rational points that are not closed point?,"I know for a scheme $X$ locally of finite type over a field $k$ , $k$ -rational points are closed ponits. If we remove the assumption that $X$ is locally of finite type over $k$ , are there some $k$ -rational points which are not closed point?",['algebraic-geometry']
3193123,"Examining properties of the relation $R = \{(S_1,S_2) \mid |S_1| < |S_2|, S_1,S_2 \subseteq S\}$ where $S$ is finite","The Problem: Given a finite set $S$ , let the relation $$R = \{(S_1, S_2) \mid |S_1| < |S_2|, S_1, S_2 ⊆ S\}$$ Show
whether or not $R$ is reflexive, symmetric, antisymmetric or transitive. I'm shaky on how to approach this problem. Any help would be greatly appreciated. I think that it's antisymmetric only. Suppose $S = \{1, 2, 3, 4, 5\}$ . Let $S_1 = \{1, 2\}$ and $S_2 = \{3, 4, 5\}$ . $S_1$ and $S_2$ are subsets of $S$ , and $|S_1| < |S_2|$ . Then $R = \{(1, 3), (1, 4), (1, 5), (2, 3), (2, 4), (2, 5)\}.$ $R$ is not reflexive because $S_1$ will never equal $S_2$ and a reflexive relation must act on a set, not two different sets. $R$ is clearly not symmetric, nor is it transitive.","['relations', 'solution-verification', 'discrete-mathematics']"
3193131,"Continuous function on $[0,1]$ such that its zeros form a nowhere dense set of positive measure?","I know few facts, if $f : [0,1] \to \mathbb{R}$ is continuous, $Z(f) \triangleq f^{-1}(\{0\})$ is closed, there are continuous functions whose zeros are nowhere dense, there are nowhere dense sets of positive measure. From these facts, I cannot conclude that if $f:[0,1] \to \mathbb{R}$ is continuous and its zeros $Z(f)$ form a nowhere dense set, this set $Z(f)$ is of null measure. Can we prove that it is indeed the case, or exhibit a counter-example?","['continuity', 'measure-theory', 'roots']"
3193159,Product of Mrówka space and one point compactification discrete space.,"I was reading an article and I have some troubles to understand it. First, the required definition to understand the problem: Let $\mathcal{U}\subseteq \{A\subseteq\omega: |A|=\aleph_0 \}$ . We say that $\mathcal{U}$ is an almost disjoint family if for all $A,B\in\mathcal{U}$ such that $A\neq B$ we have that $|A\cap B|<\aleph_0$ The proof that I was reading is the next: The key part of the proof is the fact that $A$ is a closed subset of $X\times Y$ . But I can't see that $A$ is closed only by the construction of the topology of $X\times Y$ . In fact, I think that we need a lot of cases to prove that fact because if we take $(a,b)\in (X\times Y)\setminus A$ then $b=d^{*}$ . $a=r_\alpha$ and $b=d_\beta$ with $\alpha\neq\beta$ . Here probably we have two subcases because $\alpha<\beta$ or $\beta<\alpha$ . $a\in\omega$ and $b=d_\alpha$ for some $\alpha<\mathfrak{c}$ $a\in\omega$ and $b=d^*$ . Are they all cases? Or am I forgetting some? I don't know if my thoughts are correct. Can you help me to complete the proof? I really appreciate any help you can provide me.","['proof-explanation', 'general-topology']"
3193169,Point spectrum of an integral operator,"Let we have $$Tu(x) = \cfrac{1}{x}\int_0^x u(y)dy$$ so that $u \in L^2(0,1)$ . How can I show that $(0,2) \subset \sigma_p(T)$ and $T$ is not compact?","['integral-operators', 'spectral-theory', 'functional-analysis']"
3193260,Why limits give us the exact value of the slope of the tangent line?,"Limits tell us how functions behave at $x\to a$ , not how they behave at $x = a$ . However, in limits we plug $x = a$ as an approximation of $x\to a$ , so: why the limits give us the exact value of slope of the tangent line, despite being just an approximation of what happend around $x$ ?","['limits', 'tangent-line', 'slope']"
3193288,Solution of second ordinary equation,"i have the following question. Let $\phi_1$ and $\phi_2$ fundamental system solutions on an interval $I$ for the second order equation $$
y''+a(x)y= 0.
$$ Prove that there exists fundamental system solutions $\{y_1,y_2\}$ such that the Wronksian $W[y_1,y_2]$ satisfies $W[y_1,y_2]=1$ . So, I know that $\{y_1,y_2\}$ is a system of fundamental solution means that any solution $y$ of edo is written: $y(x)= c_1 y_1(x)+ c_2 y_2(x)$ where $c_1$ and $c_2$ arbitrary contacts. Then $W[y_1,y_2]= y_1(x)y'_2(x)-y_2(x)y_1'(x)$ . But I don't know how to resolve the question and what's utility of $\phi_1$ and $\phi_2$ . Thank's in advance to the help.","['partial-differential-equations', 'ordinary-differential-equations', 'real-analysis']"
3193315,Zeros of $ f''$,"Let $ f : \mathbb{R} \to \mathbb{R} $ be a $C^2$ function such that $$ \lim_{x \to \pm \infty}{f(x)} = 0 $$ Prove that $f''$ has at least two zeros. Assume $f$ is not a constant. Than $f$ must have a stationary point, $a$ . Assume it's a max point. Than $f''$ must be negative in a neighborhood of that point. Now let's prove that $f''$ has at least one zero in $[-\infty, a ]$ ... From this my proof gets really messy...",['analysis']
3193336,Weitzenböck identity for $TM$-valued differential forms,"Let $M$ be a Riemannian manifold, and let $\nabla$ denote its Levi-Civita connection. We have two second order differential operators $\Gamma(T^*M \otimes TM) \to \Gamma(T^*M \otimes TM)$ : The Bochner Laplacian $\Delta_B=\nabla^* \nabla$ , and the ""Hodge"" Laplacian $\Delta_H=\delta_{\nabla} d_{\nabla}+d_{\nabla}\delta_{\nabla}$ , where here $d_{\nabla}: \Omega^1(M,TM) \to \Omega^2(M,TM)$ is the covariant exterior derivative associated with the Levi-Civita connection $\nabla$ , and $\delta_{\nabla}$ is its adjoint. Is there some kind of Weitzenböck formula connecting these two Laplacians? something like $\Delta_H-\Delta_B=C(R^{\nabla})$ , where $R^{\nabla}$ is the curvature tensor of $\nabla$ ? I know that for real-valued one-forms, the classical Weitzenböck formula is $\Delta_H-\Delta_B=\text{Ric}$ , where "" $\text{Ric}$ "" denotes the Ricci curvature of the manifold. Can we do something similar here?","['riemannian-geometry', 'curvature', 'laplacian', 'differential-forms', 'differential-geometry']"
3193339,Quadrilaterals with equal sides,"$AC = BD$ $EC = ED$ $AF = FB$ Angle CAF = 70 deg Angle DBF = 60 deg We are looking for angle EFA. I have found through Geogebra that the required angle is 85 deg.
Any ideas how to prove it? I am not so familiar with Geometry :(",['geometry']
3193377,"Rewrite $(\det A)^{1/n}=\min\left\{\frac{\mathrm{tr}(AC)}{n}:C \in \Bbb{C}^{n×n},C>0,\det C=1\right\}$ in terms of $\frac{\rm{tr}(CAC)}{n}$","Given $$
(\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(AC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label1\tag1
$$ Question Show that the formula can be rewritten as $$
(\det A)^{1/n} = \min \left\{\frac{\operatorname{tr}(CAC)}{n} : C \in {\Bbb C}^{n \times n}, C > 0, \det C = 1\right\}. \label{2}\tag{2}
$$ Then, show by an example that the formula \eqref{1} is false if $A$ is singular. My approach , would love to get your opinions: The determinant and the trace are two quite different beasts, little relation can be found among them. If the matrix is not only symmetric (hermitic) but also positive semi-definite , then its eigenvalues are real and non-negative. 
Hence, given the properties ${\rm tr}(AC)=\sum \lambda_c$ and ${\rm det}(A)=\prod \lambda_C$ , and recalling the AM GM inequality, we get the following (probably not very useful) inequality: $$\frac{\operatorname{tr}(AC)}{n} \ge {\det}(A)^{1/n}.$$ (equality holds iff $M = \lambda I$ for some $\lambda \ge  0$ ) My Solution: If $\det C = 1$ , could I say that $C$ is either an unity matrix or identity matrix, and hence it won't have any other max. or min. occurrences, thus the formula can be re-expressed as stated above? Could I say that $AC = A = $ matrix with all zeros and just eigenvalues on the diagonal $\lambda_1 ,..., \lambda_i,..., \lambda_n$ , then $$
\frac{\lambda_1 +...+\lambda_n}{n} \ge (\lambda_1 \cdot ... \cdot \lambda_n)^{1/n}$$ and $$
\frac{\lambda_1 +...+\lambda_n}{(\lambda_1 \cdot ... \cdot \lambda_n)^{1/n}} \ge n\:?
$$ Then, I say that if $A$ is singular then $\det A = 0$ thus one of the $\lambda_i = 0$ , therefore the product of all $\lambda_i = 0$ , thus, could I say that $\frac{0}{0} \ge n$ (although it's undefined), therefore, it contradicts the assumption and the formula doesn't hold for a singular matrix? What I'm struggling with: where I bolded ""could I say that"" - I'm not sure that it's correct and would love to know your opinion.","['trace', 'determinant', 'linear-algebra']"
3193487,"Is there a bounded connected set $X$ such that for all point $b$ there exists $r > 0$ such that $X \setminus O(b, r)$ is disconnect？","I find a question by myself, and I do not know if it is an interesting question. Let $X \subseteq \mathbb{R}^n$ be a bounded connected set. And I define a ""bad point"" $b \in \mathbb{R}^n$ with respect to $X$ if there exists $r > 0$ such that $X \setminus O(b, r)$ is not connect, where $O(b, r)$ is a ball with center point $b$ and radius $r$ . Let $B(X)$ be a set of all ""bad point"" with respect to $X$ . Strong question: Is there a bounded connected set $X$ such that $B(X) = \mathbb{R}^n$ ? Weak question: Is there a bounded connected set $X$ such that $X \subseteq B(X)$ ? I have no idea how to solve my question. I give some examples of my definition. (1) If $X$ is a ball or spherical surface, then $B(X) = \varnothing$ . (2) If $X = \{(x,0, \ldots, 0) \mid x \in [0,1] \}$ is a close line segment, then $$B(X) = \{(x,a_{1}, \ldots, a_{n - 1}) \mid x \in (0,1), a_{i} \in \mathbb{R} \}$$ (3) If $X = \{(x,0, \ldots, 0) \mid x \in (0,1) \}$ is a open line segment, then $$B(X) = \{(x,a_{1}, \ldots, a_{n - 1}) \mid x \in (0,1), a_{i} \in \mathbb{R} \}$$","['general-topology', 'geometry']"
3193511,How to prove this simple property for two sets?,"We are given two vectors, $a = (a_1,\dots,a_n)$ and $b = (b_1,\dots,b_n)$ such that $0 \le a_i=b_i \le \varDelta$ for $i=1,\dots,n$ . We want to modify each of these vectors in an iterative procedure such that in each iteration one component of each vector is increased. However, we do this for each vector differently. Let $0 \le \delta \le \varDelta$ be a given number. 
For vector $a$ , we select the minimum component , say $a_k$ , and increase it by $\min(\delta,\varDelta-a_k)$ . For vector $b$ , we select an arbitrary component , say $b_k$ , and increase it by $\min(\delta,\varDelta-b_k)$ . Let $a^\ell$ and $b^\ell$ represent the modified vectors at iteration $\ell$ . At the beginning, iteration $0$ , we have $a^0 = b^0=a=b$ . It is quite clear that $$
\min \{a^\ell_1,\dots,a^\ell_n\} \ge \min  \{b^\ell_1,\dots,b^\ell_n\}
$$ for any iteration $\ell$ . However, I have difficulty proving this simple logical fact, mathematically! I tried induction, proved it for $\ell=1$ , but could not succeed proving the next steps. Given that our statement is logically trivial, there should be some easy way to prove this. Any ideas? If that helps, all parameters are non-negative integers.","['elementary-set-theory', 'proof-writing', 'supremum-and-infimum', 'alternative-proof']"
3193543,How does one prove such an equation?,The problem occurred to me while I was trying to solve a problem in planimetry using analytic geometry. for $b$ between $-\frac{1}2$ and $1$ : $\sqrt{2+\sqrt{3-3b^2}+b} = \sqrt{2-2b}+ \sqrt{2-\sqrt{3-3b^2}+b}$,['functions']
