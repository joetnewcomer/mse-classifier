question_id,title,body,tags
2347220,What two input functions satisify an output between 0 and 1? [closed],"Closed . This question needs to be more focused . It is not currently accepting answers. Want to improve this question? Update the question so it focuses on one problem only by editing this post . Closed 6 years ago . Improve this question What are common functions that take two input variables and make the output between 0 and 1? Question is as simple as that, two inputs and one output, output needs to stay between 0 and 1!",['functions']
2347271,"If $ax^2+bx+c = 0$ and $bx^2+cx + a = 0$ have a common root and $a\neq 0$, then find $\frac{a^3+b^3+c^3}{abc}$","If $ax^2+bx+c = 0$ and $bx^2+cx + a = 0$ have a common root and $a\neq 0$, then find $$\frac{a^3+b^3+c^3}{abc}$$ I tried that for both equations to have a common root, the expression on left hand sides must be equal, ie $$ax^2+bx+c = bx^2+cx + a$$ for this we must have $x=1$ (i cannot prove this, but it appears to be true). Also both of these must be equal to $0$, so we have: $$a+b+c=0$$ So using this we say 
$$\frac{a^3+b^3+c^3}{abc} = \frac{a^3+b^3+c^3-3abc+3abc}{abc}=\frac{(a+b+c)(...)}{abc}+3$$ So we get the answer as $3$. How do we say that $x =1$ is the commmon root? Thanks!","['polynomials', 'complex-numbers', 'algebra-precalculus', 'fractions', 'quadratics']"
2347363,derivative under integral intuition,"let $f(x,y)$ continuous in $[a,b] \times [c,d]$ and $\frac{\partial f}{\partial y} $ continuous in $[a,b] \times [c,d]$, define $$F(y) = \int_a^b f(x,y)dx$$
  then $$F'(y) = \int_a^b \frac{\partial f}{\partial y}(x,y) dx$$ Can you explain the intuition behind this rule, in terms of geometry for example, or how did Leibniz  came up with this rule?","['intuition', 'integration', 'partial-derivative']"
2347366,Prove the inequality using AM-GM only.,"By considering ""Arithmetic mean $\geq $ Geometric mean"" prove the trigonometric inequality: 
$$\sin A + \sin B + \sin C \leq \frac{3\sqrt{3}}{2}. $$
where $A+B+C=180°$. My try: By using transformation formulae, I proved that
$$\sin A + \sin B + \sin C = 4\cos\left(\frac{A}{2}\right)\cos\left(\frac{B}{2}\right)\cos\left(\frac{C}{2}\right)=y(let)$$
Next using AM-GM inequality
$$\cos\left(\frac{A}{2}\right)+\cos\left(\frac{B}{2}\right)+\cos\left(\frac{C}{2}\right)\geq 3 \left(\frac{y}{4}\right)^{\frac{1}{3}}.$$
I'm unable to proceed further. Please help me.","['inequality', 'a.m.-g.m.-inequality', 'trigonometry']"
2347380,Reconciling measure theory change of variables with u-substitution,"In measure theory we learn that
$$
\int_\Omega g \circ f d\mu = \int_{f(\Omega)}g d(\mu \circ f^{-1})
$$
where $(\Omega, \mathscr F, \mu)$ is a measure space and $f$ and $g$ are measurable. Now in calculus we have that
$$
\int_{\phi(a)}^{\phi(b)} h(x)dx = \int_a^b h(\phi(t))\phi'(t)dt
$$
for a suitable substitution $x = \phi(t)$. From this substitution we have $dx = \phi'(t)dt$ with $\phi'(t)$ being the Jacobian, but I want to understand this in terms of the measure theoretic formulation. Clearly I'll have $g = h$ and $f = \phi$, so this ought to be equivalent to
$$
\int_{[a,b]} h \circ \phi d\mu \stackrel ?= \int_{\phi([a,b])} h \ d(\mu \circ \phi^{-1}).
$$ Let $\lambda$ be the Lebesgue measure. It seems fair to assume that $\mu \ll \lambda$ so the Radon-Nikodym derivative $\frac{d\mu}{d\lambda}$ exists. This means that 
$$
\int_{[a,b]}h \circ \phi d\mu = \int_{[a,b]} h \circ \phi \frac{d\mu}{d\lambda}d\lambda
$$
so we have $\phi' = d\mu / d\lambda$? Now how do we reconcile this with the $\mu \circ \phi^{-1}?$","['substitution', 'integration', 'measure-theory', 'calculus']"
2347404,Some negative property of Riemann curvature,"I am trying to consider a question about the following,
suppose $T$ is any symmetric trace-free 2-tensor, what conditions on curvature are sufficient for
$$T^{ij}T^{kl}R_{ikjl}\leq 0\,?$$
My definition of curvature is
$$R(X,Y)=\nabla_Y\nabla_X-\nabla_X\nabla_Y+\nabla_{[X,Y]}$$
$$R_{ikjl}=R(e_i,e_k,e_j,e_l)=\langle R(e_i,e_j)e_k,e_l\rangle $$
Note that when $R_{ikjl}=g_{ij}g_{kl}-g_{il}g_{kj}$, we have $T^{ij}T^{kl}R_{ikjl}=-T^{ij}T_{ij}\leq 0$. Does there exists a more general type of manifold has this negative property?","['riemannian-geometry', 'differential-geometry']"
2347436,A question on Grothendieck's paper On the de Rham cohomology of algebraic varieties,"In the paper, http://www.numdam.org/article/PMIHES_1966__29__95_0.pdf Grothendieck proves the isomorphism between algebraic de Rham cohomology and Betti cohomology, i.e. for a smooth quasi-projective variety $X$ over $\mathbb{C}$, we have
\begin{equation}
H^i_{\text{dR}}(X/\mathbb{C}) \simeq H^i(X^{\text{an}},\mathbb{C})
\end{equation}
The algebraic de Rham cohomology $H^i_{\text{dR}}(X/\mathbb{C})$ is the hypercohomology of the complex $\Omega^*_{X}$ of algebraic forms, while $H^i(X^{\text{an}},\mathbb{C})$ could also be computed as the hypercohomology of the holomorphic forms $\Omega^{h,*}_{X^{an}}$. Naively, it seems straightforward that a direct application of Serre's GAGA to the two complexes of forms would imply this isomorphism, but I remember someone mentions that this naive argument does not work, but why? References are sincerely appreciated!","['arithmetic-geometry', 'complex-geometry', 'algebraic-geometry']"
2347437,Continuations in mathematics: nice examples?,"I wondered whether continuations, used in computer science, occur as natural and interesting mathematical structures, perhaps as algebraic (in the theory of monoids?), model-theoretic or type theoretic structures, of some kind. Continuations as I understand them are monads (which in turn are monoids). More precisely, suppose that our monad maps types of some kind to other types. Let $\alpha, \beta$ be types and $\rightarrow$ be a mapping between types, and let $a : \alpha \hspace{0.2cm}$ (or $b: \beta)$ indicate that $a\hspace{0.2cm}$ (or $b$) is an expression of type $\alpha \hspace{0.2cm}$ (or $\beta)$. Then a continuation monad is a structure $\thinspace(\mathbb{M}, \eta, ⋆)\thinspace$, with $\eta$ the unit and ⋆  the binary operation of the monoid) such that: $$\mathbb{M} \thinspace α = (α → ω) → ω, \hspace{1cm} ∀α$$
$$η(a) = λc. c(a) : \mathbb{M} \thinspace α \hspace{1cm} ∀a : α $$
$$m ⋆ k = λc. m (λa. k(a)(c)): \mathbb{M}\thinspace β \hspace{1cm} ∀m : \mathbb{M}\thinspace α, k : α → \mathbb{M}\thinspace β . $$ Continuation monads have been used to effect a mapping that is available in full second order logic (discussed in two previous questions on this site: Principal ultrafilters and The existence of a function between the individuals of the domain and the set of all subsets of the domain in SOL ) from an individual in  a domain to the principle ultrafilter containing that individual (see for example https://arxiv.org/abs/cs/0205026 ). In type theoretic terms we thus map an individual of type $e$ (the type of individuals) to something of type $(e → t) → t$ (the type of sets of sets), matching the original treatment of English quantification by Montague (1974). However, I wondered whether the particular type of monad that continuations exemplify occurs in mathematical structures that mathematicians study. Perhaps there are interesting structures, for example involving ultrafilters that are examples of continuations. The following link discusses the relation between continuations and the Yoneda embedding: https://reperiendi.wordpress.com/2007/12/19/the-continuation-passing-transform-and-the-yoneda-embedding/ However, I would be particularly interested in examples of mathematical structures that act like continuations in fields such as algebra (perhaps in the theory of monoids?), set theory or model theory (and outside of category theory).","['category-theory', 'abstract-algebra', 'model-theory', 'monads']"
2347467,Nonlinear ODE of second order with Boundary Conditions defined.,"The problem is: $y''(x)-a\cdot(y(x))^2=0, a>0$ S.t. $ y(0)=b,
\lim_{x\to\infty } y'(x)=0$ That problem results from a catalyst which has a chemical reaction of second order occuring within it - the book Transport Phenomena of Bird at. al. contains that question. Someone can give me a tip how to proceed to solve that nonlinear ODE? That is the first time that I found this kind of problem.","['ordinary-differential-equations', 'calculus']"
2347488,Conjugations in the comparison isomorphisms between Betti cohomology and algebraic de Rham cohomology,"For a smooth projective variety $X$ defined over $k$ which admits a real embedding $\sigma:k \rightarrow \mathbb{C}$, its Betti cohomology is defined by
\begin{equation}
H^*_{B,\sigma}(X):=H^*(X \times_{k,\sigma}\mathbb{C}(\mathbb{C}),\mathbb{Q}) \,,
\end{equation}
where $X \times_{k,\sigma}\mathbb{C}(\mathbb{C})$ is the complex valued points of $X \times_{k,\sigma}\mathbb{C}$. Since the embedding is real, complex conjugation acts on the points of $X \times_{k,\sigma}\mathbb{C}(\mathbb{C})$, which induces an involution $F_{\infty}$ on $H^*_{B,\sigma}(X)$. The etale cohomology is defined by
\begin{equation}
H^*_{et}(X)_{\ell}:=H^*(X \times_k \bar{k},\mathbb{Q}_{\ell}) \,.
\end{equation}
There is a standard comparison isomorphism, $I_{\ell,\bar{\sigma}}$
\begin{equation}
I_{\ell,\bar{\sigma}}:H^*_{B,\sigma}(X) \otimes_{\mathbb{Q}} \mathbb{Q}_{\ell} \simeq H^*_{et}(X)_{\ell}
\end{equation}
which depends on the choice of an extension of $\sigma$ to $\bar{\sigma}:\bar{k} \rightarrow \mathbb{C}$. From lots of references, under this isomorphism, the involution $F_{\infty} \otimes 1$ corresponds to the automorphism $\bar{\sigma}^*(c) \in \text{Gal}(\bar{k}/k)$ (which acts on etale cohomology) where $c$ is complex conjugation which acts on $\bar{k}$ through the embedding. Could anyone explain the ideas in the proof of this comparison isomorphism and the correspondence of the two involutions? Or give some references?","['complex-geometry', 'algebraic-geometry', 'reference-request', 'arithmetic-geometry', 'proof-explanation']"
2347492,"To Show that $\exp(tX)\exp(tY)= \exp(t(X+Y)) + (1/2)t^2[X, Y] +o(t^3)$","Problem 20-9 in Lee's Introduction to Smooth Manifolds (2nd Ed.) asks to prove that for a Lie group $G$, and for $X, Y\in \text{Lie}(G)=\mathfrak g$, there is $\epsilon>0$ and a smooth map $Z:(-\epsilon, \epsilon)\to \mathfrak g$ such that
  $$(\exp tX)(\exp tY) = \exp\left(t(X+Y) + \frac{1}{2}t^2[X, Y] + t^3 Z(t)\right)$$ What we want to show is that the first and second term in the Taylor expansion of $\exp^{-1}\left((\exp tX)(\exp tY)\right)$ are $t(X+Y)$ and $\frac{1}{2}t^2[X, Y]$ respectively. Calculating the first term is easy since we just need to differentiate $\exp^{-1}((\exp tX)(\exp tY))$ with respect to $t$ at $t=0$ once, and this gives the desired result. But for the second term we need to take the second derivative at which I am stuck.","['differential-geometry', 'differential-topology', 'lie-groups']"
2347522,How to recognize a sum as a Riemann Sum [duplicate],"This question already has answers here : Infinite Series $1+\frac12-\frac23+\frac14+\frac15-\frac26+\cdots$ (5 answers) Closed 6 years ago . Evaluate $$\frac{1}{1}+\frac{1}{2}-\frac{2}{3}+\frac{1}{4}+\frac{1}{5}-\frac{2}{6}+\frac{1}{7}+\frac{1}{8}-\frac{2}{9}+\cdots+\frac{1}{3n+1}+\frac{1}{3n+2}-\frac{2}{3n+3}+\cdots$$ answer choices: a) $\ln 2$ b) $\ln 3$ c) $e^2$ d) $\dfrac 9 {25}$ Looking at the answer choices, I am almost certain that we need to rearrange the sum into a Riemann sum, but I am stuck on how to do so. Also are there any general techniques to recognize/rearrange summations such as these into Riemann sums? This problem is from a competition, so I would need to solve these problems quickly. Thanks!",['calculus']
2347534,"The norm of $(Af)(x)=\int_0^x f(t)\,dt$","For $f \in L^1[0,1]$ let $A:L^1[0,1] \rightarrow L^1[0,1]$ be given by
  $$(Af)(x)=\int_0^x f(t)\,dt, \quad0\le x \le 1.$$
  Show that $A$ is a continuous linear operator and calculate its norm. I showed continuity, equivalently boundedness in this way: 
$$\|Af\|=\left\|\int_0^xf(t)\,dt\right\|=\int_0^1\left| \int_0^xf(t) \, dt \right| \,dx \le\int_0^1\int_0^x|f(t)| \, dt \, dx \le\int_0^1\|f\| \, dx=\|f\| $$ 
hence the operator is bounded with a constant $C=1$ and thus $\|A\|\le1$. However, I'm struggling with showing that indeed $\|A\|=1$. I was advised to use $$f_n = n 1_{[0,{1 \over n}]}$$ but I have two questions. Firstly, how do I come up with such a function and what do I use it for?","['functional-analysis', 'normed-spaces', 'operator-theory']"
2347614,Boosting randomized algorithms with Hoeffding's inequality,"I am working on Ex. 2.2.8 out of Roman Vershinyn's High Dimensional Probability book: https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf The problem asks: Imagine we have an algorithm for solving a decision problem. Suppose the algorithm makes the decision at random and returns the correct answer with probability $1/2 + \delta$ for some $\delta > 0$. To improve the performance, we run the algorithm $N$ times and take the majority vote. Show that, for any $\epsilon \in (0,1)$, the answer is correct with probability $1 - \epsilon$ as long as $N \geq 2 \delta^{-2} \ln (\epsilon^{-1})$. The hint was to set $X_i$ to the indicator function on the event that the algorithm guessed incorrectly and apply Hoeffding's inequality. However, in this section he gives variations of Hoeffding's inequality, one for symmetric bernoulli random variables (along with its two sided inequality) and for general bounded random variables (which is asked to be proven in the previous exercise). Ultimately, I think I want to show that $$\mathbb{P}\left\{\text{event that the algorithm guessed incorrectly a majority of the time}\right\} \leq \epsilon$$ as long as $N \geq 2\delta^{-2}\ln(\epsilon^{-1})$ but I am not sure how to formulate this properly with the Hoeffding inequalities introduced since $X_i$ is not symmetric. Would anyone be able to provide a hint as to how to proceed?","['statistics', 'probability']"
2347634,Solving a first order differential equation involving $\sin(x/y)$,"I am trying to solve the differential equation
$$
y'\left(x\right) - \frac{y\left(x\right)}{2x} =
x\sin\left(x \over y\left(x\right)\right)
$$
I think it is separable variable differential equations. I tried to substitute: 
$$z=\frac{x}{y}$$
and $$y'=\frac {z-z'x}{z^2}$$
$$\frac {z-z'x}{z^2}-\frac{1}{2z}=x\sin(z)$$ multiply by $z^2$ $$z-z'x -\frac{z}{2}=z^2x\sin(z)$$ And now I have no idea how to manipulate this.",['ordinary-differential-equations']
2347692,How to prove that limit doesn't exist using epsilon-delta definition?,"It is easy to prove the limit exists, all we have to show is there exists a relationship between $\delta$ and $\epsilon$. But how are we supposed to prove limit doesn't exists? The problem is when we are proving for a limit we already know what the limit is and with that, algebra is all that's needed. Please show through an example (you may show that $\lim_{x\rightarrow0} \frac{1}{x}$ doesn't exist) If possible please use the explanation scheme that is used by this answer https://math.stackexchange.com/a/66552/335742",['limits']
2347696,Hadamard product of matrices,"I've encountered a notion of Hadamard product, if $A=[a_{ij}],B=[b_{ij}]$ are $n\times n$ matrices then their Hadamard product is $A\circ B=[a_{ij}\ b_{ij}]$. My question is what does it represent, as standard matrix multiplication represents composition of linear maps? I didn't find too much about it safe for definition and basic properties.","['matrices', 'linear-algebra']"
2347727,Coordinates/charts for the space of k-tuples of orthonormal vectors (a.k.a. Stiefel manifold),"Is there a reasonably natural way to create an atlas (with coordinate charts) for the space of $k$-tuples of orthonormal vectors in $\mathbb{R}^n$? (Obviously $k \le n$.) The dimension of the desired space is
$$\sum_{i=1}^k n - i = k(n - \tfrac{1}{2}k - \tfrac{1}{2})$$
if I am not mistaken. Naïve approach Choose a random matrix and do Gram-Schmidt orthonormalization (or possibly a more numerically stable alternative, like Householder transforms). Then figure out how to fix $n^2 - k(n - \tfrac{1}{2}k - \tfrac{1}{2})$ parameters to get a dense open subset, and find different ways to do this so that the open sets cover the entire space. I would accept an answer based on the naïve approach, but I hope something better (""more natural"") comes along. Notes I said all $k$-tuples, but really I'm okay with just the connected component containing $(\mathbf{e}_1, \dotsc, \mathbf{e}_k)$. What I'm actually hoping to do is plug this into a computer. So computational aspects matter too, although as a former mathematician my first concern is for naturality.","['differential-geometry', 'parametrization', 'linear-algebra', 'differential-topology']"
2347753,"How do I recognize when my approach is over-counting, and how to avoid it?","I'm wondering how to avoid over counting and recognize when I'm over counting. Here are some examples in which I over-count. Example one : How many hands of $5$ cards are there were no two have the same number. Here is my approach: Let the #: no two have the same number $=N$. Work with the complement ""Two have the same number"". $N={52 \choose 5}$-Two have same number To get the same number twice. First choose a number $2-10$ for a total of $9$ choices (I don't count an ace as a number). Then choose $2$ cards out of $4$ with that number. Then choose $3$ cards from the remaining $50$. $${52 \choose 5}-9{4 \choose 2}{50 \choose 3}$$ I realize this over-counts as it counts 2 heart, 2 spades, 2 diamonds, 3 hearts, 4 spades different from 2 diamond, 2 spades, 2 spades, 3 hearts, 4 spades. This is quite troubling for me because I don't know how to recognize when I over count nor do I know how to fix this. Example two : How many hands of $13$ contain at least $3$ cards from every suit. Pick $3$ cards from each suit: ${13 \choose 3}$. Then pick a card from the reaming $52-12=40$: ${40 \choose 1}$. That gives ${13 \choose 3}^4(40)$.",['combinatorics']
2347755,Galois group of $ X^6-2tX^3+1 $ over $ \mathbb{Q}(t) $,"As in the question, I am asked to determine the Galois group of $$f(X)= X^6-2tX^3+1 \in \mathbb{Q}(t)[X] $$ over $  \mathbb{Q}(t) $. First, I should prove that $ f $ is irreducible over $ \mathbb{Q}(t) $ and to do this I thought of the poynomial $ g(X)=f(X^\frac{1}{3})=X^2-2tX+1 $ since $ g(X-1)=X^2-2(t+1)X+2(t+1) $ is Eisenstein with respect to the prime element $ 2(t+1) $ of $ \mathbb{Q}[t] $ but I am not sure if this is enough to conclude that $ f $ is irreducible. Furthermore, we have that the splitting field $ L $ of $ f $ over $ \mathbb{Q}(t) $ is $ \mathbb{Q}(\alpha, \omega) $ where $ \alpha $ is a root of $ f $ and $ \omega $ is a primitive third root of unity. Starting from here, I would like to compute the Galois group of $ f $. I would appreciate any help regarding both questions. Thank you!","['irreducible-polynomials', 'abstract-algebra', 'galois-theory', 'splitting-field']"
2347756,Definition of Conditional Expectation and its Uniqueness,"there are many questions on Mathexchange, e.g. Definition of conditional expectation , discussing the definition of conditional expectation. In most cases the random variable which is later named ""conditional expectation"" is assumed to be a measurable $\Omega\rightarrow\mathbb R$ function. In some lecture notes (that are maybe too advanced for me) conditional expectation is defined to be a $\Omega\rightarrow\mathbb R^n$ measurable function -- but imposes in the definition that this function is a.e. unqiue. For me it feels that we do not need a.e. uniqueness in the definition but that it can be proven (I failed though). So how is uniqueness of conditional expectation proven in $\mathbb R^n$ or even more general, in Banach spaces? Edit: Interestingly, this five-year-old question receives some attention now. The conditional expectation $Z$ of a random variable $X:\Omega\rightarrow S$ given a sub- $\sigma$ -algebra $\mathcal G$ satisfies $$\int_G Z\,\mathrm dP = \int_G X\,\mathrm dP$$ for all $G\in\mathcal G$ . The common notation is $\mathbb E[X\mid\mathcal G] := Z$ . It can be shown in the case $S = \mathbb R$ that if such a random variable exists, it is unique. The proof can be found e.g. here: Proof that conditional expectation is defined uniquely almost everywhere? This proof does not work for $S = \mathbb R^n$ , or more general $S = $ some Banach space. The question is: how can I prove uniqueness in these cases?","['probability-theory', 'conditional-expectation']"
2347757,Borel Sets and Homeomorphism ($ \mathbb C$ and $\mathbb R^2$),"I understand that that the Borel sigma algebra on the product topology of second countable metric spaces is the same as the sigma algebra generated from the product of their individual sigma algebras. In particular, $\mathscr B (\mathbb R^2) = \mathscr B (\mathbb R) \otimes \mathscr B (\mathbb R) $ Secondly, with their standard topologies,  $ \mathbb C$ is isometrically homeomorphic to $\mathbb R^2$. What is the relationship between $\mathscr B (\mathbb R^2) $ and $\mathscr B (\mathbb C) $ ? One reference I have says they are the same, but I think some form of isomorphism is more likely. And, knowing that a function from a product Borel space (to another Borel space) is measurable if and only if its projections are measurable, how would one use that rigorously to show the same for the real and imaginary parts of a complex valued function ? Update: the last part I think I have solved. Just use the continuity of the homeomorphism and the fact that compositions of continuous and Borel measurable functions are Borel measurable.","['borel-sets', 'general-topology', 'measure-theory']"
2347794,How many sequences of subsets are there such that $T_1 \subseteq T_2 \subseteq \cdots\subseteq T_k$?,"Problem: Given a positive integer $k$ and a set $S$ with $|S| = n$, how many sequences $(T_1, T_2, ... , T_k)$ of subsets $T_i$ of $S$ are there such that $T_1 \subseteq T_2 \subseteq \cdots\subseteq T_k$? Hint: Suppose $x \in S$. If I tell you $x \in T_5$ for one particular sequence of subsets, what do you know about $x$’s inclusion in the other subsets? My thought (or lack thereof) process: I don't think I understand what the question is asking. What exactly is in set $S$? And are we asked to count the number of times $T_i$ appears in each sequence? If so, I think we need to use the binomial theorem or some deviation of it to count the number.","['combinatorics', 'elementary-set-theory', 'discrete-mathematics']"
2347807,Derivative as difference quotient with tricky limits due to square roots,"I have a function that I want to find the derivative of using the difference quotient definition of a derivative. The function is: $$f(x)=\frac{\sqrt{x}}{x+1}$$ therefore, using the difference quotient definition, we have: $$f'(x)=\lim_{h\to0}\frac{1}{h}\left(\frac{\sqrt{x+h}}{x+h+1}-\frac{\sqrt{x}}{x+1}\right)$$ this is equal to: $$\lim_{h\to0}\frac{1}{h}\left(\frac{(x+1)\sqrt{x+h}-(x+h+1)\sqrt{x}}{(x+1)(x+h+1)}\right)$$ I tried using (a+b)(a-b) = a^2 - b^2 to get rid of the square roots in the numerator, but the denominator gets pretty huge, so I'm not sure if this is a wise path to proceed down. At this point I get stuck with the algebra. I understand that this is a simple derivative to take using the quotient rule, but I'm trying to practice taking limits, and to learn useful algebra tricks. A step by step computation would be helpful. Many thanks","['algebra-precalculus', 'recreational-mathematics', 'derivatives', 'limits']"
2347814,Is there a way to get all the permutation braids of a braid group?,"Geometrically, it's easy to ""draw"" the permutation braids, but I was wondering if there was an algorithm to write down all the permutation braids in terms of the Artin generators. I had a few ideas, but none of them seem quite feasible or efficient. First, I considered the fundamental braid $\Delta_n \in B_n$ and I thought permutation braids must be formed from the fundamental braid by removing some Artin generators. So by going through all choices and checking whether they are permutation braids, we can get the set of permutation braids. Second, because $B_{n-1}$ embeds into $B_n$, the permutation braids of $B_n$ are the permutation braids of $B_{n-1}$ and the permutation braids containing the last generator $\sigma_{n-1}.$ So if we have the set of permutation braids of $B_{n-1},$ we can try to insert one or more $\sigma_{n-1}$ to get the other permutation braids of $B_n$. Thirdly, I know there is a bijection between the permutation braids of $B_n$ and the symmetric group $S_n$. But it's difficult to explicitly write the permutation braid in terms of the generators. One method I thought of is: if we number the braids $1$ through $n$ and we have a permutation $\rho$, then first send the $n$th braid to position $\rho(n)$ which can be done by $\sigma_{n-1}\sigma_{n-2}\cdots \sigma_{\rho(n)}$ and etc. However, this method doesn't work always because if we have $\sigma_1\sigma_2 \in B_3$ corresponding to the permutation $(1 2 3),$ we can't write that as a permutation braid starting with $\sigma_2.$ These are the ideas that I had, but the first two seem much too inefficient while the third one doesn't seem to even work. Since permutation braids seem crucial to the theory of braid cryptography, I was thinking there must be some efficient method to write them out, but I couldn't find them in literature. Help would be much appreciated.","['permutations', 'abstract-algebra', 'braid-groups', 'group-theory']"
2347824,Solving Telescoping Series,"$$\ \sum_{n=5}^\infty \frac{8}{n^2-1} $$ I tried the following: $$\ \sum_{n=5}^\infty \frac{8}{n^2-1} = \sum_{n=5}^\infty \frac{8}{n-1} - \frac{8}{n} =$$  $$\left(2-\frac{8}{5}\right) + \left(\frac{8}{5} - \frac{8}{6}\right) + \left(\frac{8}{6} - \frac{8}{7}\right) + \cdots + \left(-\frac{8}{n}\right)$$ Terms cancelled each other out, therefore we are left with:
$$ \ (2 - \frac{8}{n}) $$ I could think the series converges to 2 since: 
$$\lim_{n \to \infty} \left(2-\frac 8 n \right) = 2$$ However, the correct answer is $$ \frac{9}{5} $$ what am I doing wrong?","['telescopic-series', 'sequences-and-series', 'convergence-divergence', 'limits']"
2347840,Limit of a function at $x = 0$,Let $$ f(x) = \frac{e^{2\pi x}}{(1 - e^{2 \pi x})^2} - \frac{1}{(2\pi x)^2}$$ Can someone provide a formula for $$ \lim_{x \to 0} \frac{d^k f(x)}{dx^k}$$ or at least for $\frac{d^k f(x)}{dx^k}$ ? Everything I've tried so far was L'Hospital but after few derivatives I've gave up and used Wolfram Alpha to test my results ... I've also been thinking about using the Cauchy Integral formula since $f(z)$ is holomorphic (I think) on $\mathbb{C}\setminus \{ i\cdot n| n\in \mathbb{Z}\}$ and can be continued analytically to $\mathbb{C}$ using Riemann theorem on removable singularities ... Can someone help?,"['derivatives', 'real-analysis', 'limits', 'cauchy-integral-formula', 'complex-analysis']"
2347843,Prove $\lim_{x \to 1} \frac{x^b - 1}{x - 1} = b$,"Prove that $$
  \lim_{x \to 1} \frac{x^b - 1}{x - 1} = b
$$ (No L'Hospital's rule, or series) I'm not sure how to go about this. I have that $x^b = e^{b \ln(x)}$, which gives $$
  \lim_{x \to 1} \frac{x^b - 1}{x - 1} = 
  \lim_{x \to 1} \frac{e^{b \ln(x)}- 1}{x - 1}
$$ But this doesn't (seem to) do much. I also have that $1 - \frac{1}{x} < \ln(x) < x - 1$.","['real-analysis', 'calculus', 'limits']"
2347851,Find the orthogonal complement of the subspace of diagonal matrices,"In $\mathbb{C^{3\times3}}$ , with the inner product of matrices defined as $$\langle A,B\rangle = \operatorname{tr}(A^*B)$$ find the orthogonal complement of the subspace of diagonal matrices. Then, considering the following matrices $\in\mathbb{C^{3\times3}}$ $$A=\begin{pmatrix} a_{11}&0&0\\0&a_{22}&0\\0&0&a_{33}\end{pmatrix}$$ and $$B=\begin{pmatrix} b_{11}&0&0\\0&b_{22}&0\\0&0&b_{33}\end{pmatrix}$$ I concluded by the statement and the definition of orthogonal complement: $$\langle A,B\rangle=\operatorname{tr}\begin{pmatrix}
\overline{a_{11}}b_{11}&0&0\\0&\overline{a_{22}}  b_{22}&0\\0&0&\overline{a_{22}}b_{33}\end{pmatrix}=0$$ After, I get the trace of the $3$ -by- $3$ square matrix of the inner product $\langle A,B\rangle$ $$\overline{a_{11}}  b_{11}+ \overline{a_{22}}b_{22}+ \overline{a_{33}}b_{33}=0$$ I'm stuck from here, how can I do to get the orthogonal complement?","['matrices', 'orthogonality', 'linear-algebra', 'inner-products']"
2347887,"for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution","for the cauchy problem , determine unique solution ,or no solution , or infinitely many solution $u_x-6u_y=y$ with the date $u(x,y)=e^x$  on the line $y=-6x+2$ My attemp t: given $u_x-6u_y=y$ then $\frac{dx}{1}=\frac{dy}{-6}=\frac{du}{u}$ there fore the general solution  $\phi(6x+y, y^2+12u)=0$ and hence implicit  equation $y^2+12u=F(6x+y)$ but i cant go for further","['cauchy-problem', 'ordinary-differential-equations', 'partial-differential-equations']"
2347902,Total Derivatives and Total Differential,"I am confused between total derivatives and total differential. What is  the difference between total derivatives and 
total differential?",['derivatives']
2347915,I would like help identifying the rigorous classification of this 'surface' geometry based on my interpretation of 3D models.,"I want to try and identify a geometric structure I thought up while doing some weird stuff with making things walk on the surface of a 3D model and trying to incorporate backface culling into the surface geometry itself. See, in computer graphics each side of a polygon or triangle are considered separate entities and so I specifically desired to capture this within the geometry I constructed. Below, I will describe the different properties I know of to see if anyone can identify it as anything previously studied. The Structure Itself Let us define a special triangular mesh. Let's just call it a ""half-triangle mesh"" since I don't know what else to call it. In this context we define a half-triangle mesh to be a collection of half-triangles and we define a half-triangle to be an ordered triplet of points. These points technically form the vertices of Euclidean triangle in space. Now comes the somewhat weird part. We can say that a half-triangle only has one side. If we look at it from a geodesic perspective and a physics perspective, from the side in space where the points are in clockwise order, there is nothing on that side. Literal emptiness. The geodesics will behave as if that triangle isn't there. However, from the other side, the half-triangle does exist and geodesics extending onto that triangle will behave as if it is there. Think of it like a one way window. Geodesics Now I did technically say a bit about the geodesics, but let me be more rigorous. When triangles facing the right direction form a surface like in a triangular mesh or this image geodesics behave like you would expect. In fact, two half-triangles with the same three points but facing in opposite directions form a regular triangle. The unusual case is if we had a shape like in the T-surface within the below thing I found on google images. If all the protrusions are formed by normal triangles, then under my system the blue line is a geodesic/straight line. Whereas if the surface was formed by half-triangles and the unseen backsides did not have any half-triangles, then by extending the green lines according to the allowed rule set forth in Euclid's second postulate which states ""lines may be extended infinitely in either direction"" we get the following: I hope the image makes sense. Basically, when there is no side from where the line is coming from, the line just ignores it and passes by it (hence the action of the blue line). Furthermore because there is no backside for the red line to wrap around onto, it just ends. These are just two examples. Note, that this also means that by extending backwards along the blue line, the blue line can essentially grow a perpendicular segment that is part of the line. This essentially means that the lines in this system can split and branch wherever a lone half-triangle intersects a plane on the side that exists. I feel that this is in particular an important property of this geometric system. Let's presume that this concept can be extended to 'surfaces' in general. Not just polygons. I dare not attempt it myself, but I'm sure the polygonal case is clear enough for people to get an idea of how that might extend. What sort of 'surface' geometry am I doing it and what is it classified under?","['surfaces', 'computational-geometry', 'geometry', '3d', 'paradoxes']"
2347971,Number of equilateral triangles in triangular grid covered by a circle with radius 1,"Here's a problem I saw: How many equilateral triangles (with length $1$ cm) on a triangle grid can be covered with a circle centered at a point with radius $r$ cm? At first I thought the answer is just $r^2\times 6$, but then I found out it's only for small $r$. For example: Here the grey part is the answer, but then if the circle gets bigger, the orange part might cover some triangles, which makes the answer different. Can anyone help me with it? I want a formula for at least $1\leq r\leq 100000$. Thanks.",['geometry']
2347998,Solve the integral equation of convolution (Abel),"Solve the integral equation, $$I = \int_{0}^{t} \frac{f(\tau)}{\sqrt{t - \tau}}d\tau = \sqrt{2g}T$$ where $T, g$ are constants. Find $f(t)$ I see that $$I = f(t) * g(t)$$ where $f(t)$ we need to find and $g(t) = \frac{1}{\sqrt{t}}$ Using the convolution theorem, we see that: $$\sqrt{2g}T = \mathcal{L}^{-1}\{ \mathcal{L}(f) \mathcal{L}(g) \} $$ Let $F(s) = \mathcal{L}(f)$ and we know $\mathcal{L}(g) = \frac{\sqrt{\pi}}{2s^{3/2}}$ therefore, $$\sqrt{2g}T = \frac{\sqrt{\pi}}{2}\mathcal{L}^{-1} \{\frac{F(s)}{s^{3/2}}\}$$ Now, I got stuck. Can someone provide some help?","['laplace-transform', 'ordinary-differential-equations', 'calculus']"
2348011,Analytic continuation commuting with series,"Suppose $f_1,f_2,...$ are entire functions, and there is an open subset $U \subseteq \mathbb{C}$ such that the series $F(z) = \sum_{n=1}^{\infty} f_n(z)$ converges normally on $U$. Also suppose that $F$ can be analytically continued to an entire function. I have a situation where all $f_n$ vanish at some point $z_0$, but unfortunately $z_0 \notin U.$ Can we still say that $F(z_0) = 0$? I would guess not, since it feels like bending the rules of analytic continuation in a way that shouldn't be allowed. But I didn't think of a counterexample.","['complex-analysis', 'analytic-continuation', 'convergence-divergence', 'analyticity', 'sequences-and-series']"
2348022,"Riccati D.E., vertical asymptotes","For the D.E. 
  $$y'=x^2+y^2$$
  show that the solution with $y(0) = 0$ has a vertical asymptote at some point $x_0$. Try to find upper and lower bounds for $x_0$: $$y'=x^2+y^2$$
$$x\in \left [ a,b \right ]$$
$$b> a> 0$$
$$a^2+y^2\leq x^2+y^2\leq b^2+y^2$$
$$a^2+y^2\leq y'\leq b^2+y^2$$
$$y'\geq a^2+y^2$$
$$\frac{y}{a^2+y^2}\geq 1$$
$$\int \frac{dy}{a^2+y^2}\geq \int dx=x+c$$
$$\frac{1}{a}\arctan \frac{y}{a}\geq x+c$$
$$\arctan \frac{y}{a}\geq a(x+c)$$
$$\frac{y}{a}\geq\tan a(x+c)$$
$$y\geq a\tan a(x+c)$$
$$a(x+c)\simeq \frac{\pi}{2}$$ But where to from here?",['ordinary-differential-equations']
2348031,Constructing a smooth bump function on a manifold,"In "" Loring W. Tu, An introduction to manifolds "" the following question exists: Let $q$ be a point of an $n$-dimensional manifold $M$ and $U$ any neighborhood of $q$. Construct a smooth bump function at $q$ supported in $U$. I answered that question but I want to make sure. Here is my answer: Let $q$ be arbitrary of $M$ that is contained in a neighborhood $U\subset M$. Then, there exists a coordinate chart $(V,\phi)$ in the maximal atlas of $M$ such that $q\in V \subset U$. In particular, there exists a smooth bump function $\rho:\mathbb{R}^n \to \mathbb{R}$ at $\phi(q)$ supported in $\phi(V)$ that is identically $1$ in a neighborhood $B_r(\phi(q)) \subset \phi(V)$, say, of $\phi(q)$. Define a map $f:M\to \mathbb{R}$ by 
$$
f(p) = 
\begin{cases}
\rho\bigl(\phi(p)  \bigr), &\text{$p\in V$}, \\
0, &\text{$p\not\in V$}.
\end{cases}
$$
Being the composite of two smooth function, $f$ is smooth on $V$ and hence on the whole manifold $M$. If $p\in \phi^{-1}\bigl( B_r(\phi(q)) \bigr)$, then $\phi(p) \in B_r\bigl( \phi(q) \bigr)$ and therefore, by the construction of $\rho$, $\rho\bigl( \phi(p) \bigr)=1$. That is, $f \equiv 1$ on the neighborhood $\phi^{-1}\bigl( B_r(\phi(q)) \bigr)$ of $q$. Clearly, by the definition of $f$, $supp\, f \subset V \subset U$. Hence, $f$ is a smooth bump function at $q$ supported in $U$. Can anyone please revise my proof ?. I appreciate your help.
Thanks in advance. Note: A smooth bump function $f:M\to \mathbb{R}$ at a point $q\in M$ supported in $U\subset M$ is a non-negative smooth function such that $f\equiv 1$ on a neighborhood $V_q \subset U$ of $q$ and that $supp\, f \subset U$.",['differential-geometry']
2348060,Introductory text on Hopf algebras,"Is there an introductory (but as complete as possible) text on Hopf algebras like Abe or Sweedler but working when possible over arbitrary commutative rings instead fields? ""Like Abe or Sweedler"" means that I prefer to avoid texts focused in cohomology applications, usually working in the graded setting, etc. (Milnor-Moore, May-Ponto). Also, I don't mind if it deals only or mainly with commutative Hopf algebras.","['algebraic-geometry', 'abstract-algebra', 'noncommutative-algebra', 'ring-theory', 'hopf-algebras']"
2348103,Parallel rectangles covering circle problem,"Problem:
Given a circle that is just barely covered by a parallel collection of adjacent rectangular boards that have length at least as much as the diameter of the circle, prove that the circle cannot be covered by the same boards if they are arranged in a non parallel configuration. This is problem 11c from the chapter 19 appendix (integration) of Spivak's calculus 4th ed. I've thought of a few ideas, but I am not sure how to proceed with them and make progress. I was considering looking at an extreme scenario, with an arbitrarily high number of rectangles which are very long. If I can prove this scenario fails, then any other one must too, since one large piece is just multiple smaller pieces and increasing the length of a piece cannot decrease coverage. I was also considering looking at total area each rectangle covers minus the overlapped area with another rectangle. I was thinking about working with an expression for the total covered area, and somehow showing that only a parallel arrangement can make the total equal to the circle's area. I am stuck at the moment and any help or insights would be greatly appreciated.","['recreational-mathematics', 'integration', 'calculus', 'geometry']"
2348115,Rigorous Proof of Leibniz's Rule for Complex,"Let $f(t,z):[a,b] \times D \rightarrow \mathbb{C}$ , $D \subseteq \mathbb{C}$ open, a continuous function analytic in $D$ for all $t \in [a,b]$ . Also, $\frac{\partial f }{\partial z} (t,z) : [a,b] \times D \rightarrow \mathbb{C}$ is continuous. Then $g(z) = \int_a^b f(t,z) \, dt $ is analytic on $D$ with $$g'(z) = \int_a^b \frac{\partial f}{\partial z} (t,z) \, dt. $$ (P97, Complex Analysis , Freitag) The author did not provide a proof, and said it is ""direct"" from the real case. I am really interested in how one writes this out completely. (probably in a much more neater way than mine's). My attempt: (Correct?) $g:D \rightarrow \mathbb{C}$ is analytic in $D$ iff $g$ is frechet differentiable at $z$ (in the reals) and $g$ satisfies the Cauchy Riemann equations, i.e, $\frac{\partial g }{ \partial x} = \frac{1}{i} \frac{\partial g} {\partial y}$ . It is sufficient if we show the partial derivatives are continuous for frechet differnetiability. Let $f(t,z) = u(t,z) + iv(t,z)$ , then, $g(z) = \int_a^b u(t,z) \, dt + i \int_a^b v(t,z) \, dt $ and it suffices to show $$ \frac{\partial}{\partial x} \int_a^b u(t,z) \, dt = \frac{\partial}{\partial y} \int_a^b v(t,z) \, dt , \quad \frac{\partial}{\partial y} \int_a^b u(t,z) \,dt = - \frac{\partial }{\partial x} \int_a^b v(t,z) \, dt $$ Fix $z_0 \in D$ and  consider $i: [a,b] \times [c,d] \rightarrow [a,b] \times D$ given by $(t,x) \mapsto (t,z_0+x)$ which defines continuous function $U = u \circ i :[a,b] \times [c,d] \rightarrow \mathbb{C}.$ We may wlog assume $[c,d] = [-1,1]$ as $D$ is open. We have $\frac{\partial }{\partial x} U (t,0) = \frac{\partial}{\partial x}u (t,z_0),$ and $\frac{\partial}{\partial x} U (t,s) = \mathfrak{R} \frac{\partial f}{\partial z} (t,z_0+s)$ is continuous on $[a,b] \times [-1,1]$ as $\frac{\partial f}{\partial z}$ exists and is continuous on $[a,b] \times D$ . So $\frac{\partial U}{\partial x}$ is bounded by some constant $C$ on $[a,b] \times [-1,1]$ . Set $g_n (t) := \frac{U(t,x_n) - U(t,0)}{x_n }$ for any real seqence $x_n \rightarrow 0$ .  Existence of partial derivative implies $\lim_{n \rightarrow \infty} g_n(t) = \frac{\partial}{ \partial x} U(t,0).$ $g_n:[a,b] \rightarrow \mathbb{C}$ is a continuous function; by MVT, exists some $x' $ between $x_n$ and $0$ such that $$ |g_n(t)| \le \Big| \frac{\partial}{\partial x} U(t,x') \Big| \le C \chi_{[a,b]}$$ Applying DCT to the sequence of measurable functions $\{g_n(t)\}$ , \begin{align*} \lim_{n \rightarrow \infty} \int_{[a,b]} \frac{U(t,x_n) -U(t,0)}{x_n} \, dt & = \lim_{n \rightarrow \infty} \int_{[a,b]} g_n(t) \, dt  \\ 
& = \int_{[a,b]} \lim_{n \rightarrow \infty} g_n(t) \, dt \\
&  = \int_{[a,b]} \frac{\partial }{\partial x} U(t,0) \, dt  
\end{align*} As this holds for any sequence $x_n \rightarrow 0$ , the function $G(x):= \int_{[a,b]} \frac{U(t,x) - U(t,0)}{x}\, dt$ ,  is sequentially convergent at $0$ , hence continuous at $0$ , with $$  \lim_{x \rightarrow 0} G(x) =  \int_{[a,b]} \frac{\partial }{\partial x} U(t,0) \, dt  $$ By definition, the above equality is equivalent to, $$ \frac{ \partial }{\partial x}  \Big| _{z_0} \int_a^b u(t,z) \, dt = \int_a^b \frac{\partial }{\partial x} \Big| _{z_0} u(t,z) \,dt . $$ We obtain, by the same argument, an equation for the partials with respect to $y$ . As $f$ is holomorphic, we have $$ \frac{\partial u}{\partial x}(t,z_0) = \frac{\partial v}{\partial y}(t,z_0) . $$ As the choice $z_0 \in D$ was arbitrary, Cauchy Riemann Equations are satisfied on whole of $D$ , and $g$ is holomorphic on whole of $D$ .","['complex-analysis', 'integration', 'measure-theory']"
2348119,When is a function a permutation of the integers?,"When is a function a permutation of the integers? In his 2011 paper on the Collatz conjecture here Lagarias writes; > Collatz’s  original  function, which is a permutation of the integers... But when is a function over the integers a permutation of the integers?  My understanding would be that a permutation must be a bijection from a domain onto itself because only this will reposition every domain element uniquely in the range. But if we consider the Collatz function exactly a sixth of the integers are mapped to by two distinct elements; namely every even number equivalent to $1\mod 3$, which is is mapped to by both $3x+1$ and $x/2$ e.g.the number 16. This doesn't seem to be a permutation because $16$ and other numbers will appear multiple times in the range. Where am I (or Lagarias) going wrong?  If it's his mistake, what do you suppose he meant by this?","['permutations', 'collatz-conjecture', 'functions']"
2348183,Examples of trigonometric substitutions for solving equations,"I was working through a booklet of Olympiad-style problems when I came across a method which used the substitution $x = \cos \alpha$ to solve $x = \sqrt{2 + \sqrt{2-\sqrt{2+x}}}$. The solution works out nicely using the half angle formula. Are there any other good examples of such equations, where a trigonometric substitution and an identity can reduce a problem like this so effectively?",['trigonometry']
2348229,Validity and Equivalence of two definitions of the real exponential function,"The Problem : We state the following two definitions of the real exponential function from the Pr$\infty$fWiki page . We're interested in showing that the two definitions are valid $($i.e. the defining sequence/series does converge to a unique real number$)$ and that the two definitions are equivalent . I'm stuck at a couple of points $($which are described in highlighted lines$)$. Any help would be much appreciated. Thank you! Definition $1$. The exponential function can be defined as the following limit of a sequence $$\exp x := \lim_{n \to \infty} \left({1 + \frac x n}\right)^n$$ Definition $2$. The exponential function can be defined as a power series $$\exp x := \sum_{n = 0}^\infty \frac {x^n} {n!}$$ My Progress and two places where I'm stuck : Essentially the solution consists of three parts,  namely validity of definition $1$ , validity of definition $2$ and equivalence of the two definitions . Validity of Definition $1$ . I'm stuck here! Can we show that the sequence $(a_n)$ given by $a_n = \left(1+\frac{x}{n}\right)^n$ converges for every $x \in \mathbb{R} ??$ Is it eventually monotone and bounded $??$ Validity of Definition $2$ . The radius of convergence of the power series is $$r=\lim_{n \to \infty}\left| \frac{\frac{1}{n!}}{\frac{1}{(n+1)!}} \right|=\lim_{n \to \infty}\left| \frac{(n+1)!}{n!} \right|=\lim_{n \to \infty}(n+1)=+\infty$$
Thus the infinite series in the right-hand side of Definition $2$ converges to a unique real number for all $x \in \mathbb{R}$. Hence the definition is well-defined . Equivalence of Definition $1$ and Definition $2$ . There's this proof of Definition $1$ $\implies$ Definition $2$ in the Pr$\infty$fWiki page, but it's kind of under construction and I'm not really convinced by it. So I decided to try to take my own shot at it. For all $n \in \mathbb{N} \cup \{0\},$ let 
$$T_n=\left(1+\frac{x}{n} \right)^n, ~S_n=\sum_{k=0}^n \frac{x^k}{k!}$$
We have to show that $\lim_{n \to \infty} T_n = \lim_{n \to \infty} S_n$ Now,
\begin{align}
T_n &= \left(1+\frac{x}{n}\right)^n\\
&= 1+n\cdot\frac{x}{n}+\frac{n(n-1)}{2!}\cdot\frac{x^2}{n^2}+\cdots +\frac{n(n-1)\cdots 1}{n!}\cdot\frac{x^n}{n^n}\\
&= 1+x+\left(1-\frac{1}{n}\right)\cdot \frac{x^2}{2!}+\cdots +\left(1-\frac{1}{n}\right)\cdots \left(1-\frac{n-1}{n}\right)\cdot \frac{x^n}{n!}
\end{align} Clearly, $$S_n-T_n=\left\{1-\left(1-\frac{1}{n}\right)\right\}\frac{x^2}{2!}+\cdots +\left\{1-\left(1-\frac{1}{n}\right)\cdots \left(1-\frac{n-1}{n}\right)\right\}\cdot \frac{x^n}{n!}\geq 0$$ I'm stuck at this point. Can we show that $S_n-T_n \leq B_n$ such that $B_n \to 0$ as $n \to \infty ??$","['real-analysis', 'limits', 'exponential-function', 'convergence-divergence', 'sequences-and-series']"
2348233,Reconciling the 'column' and the 'row' pictures for matrices,"I understand the column picture (namely considering a matrix equation $Ax=b$ as a vector equation with each column of the matrix $A$ scalarly multiplied by each coefficient of the vector $x$ to give the vector $b$) and the row picture (intersection of the planes, or hyperplanes/lines according to the dimensions). But since these are equivalent, there has to be some way to show this equivalency ? What I mean is, the vectors in the column picture must somehow directly correspond to the planes in the row picture, and the procedures we follow in either of the pictures must be equivalent. Is there an easy way to visualise this equivalency ? I do not mean equivalence, in terms of determining the solutions, that is, to determine if the planes are parallel based on the column vectors. What I mean is to get to a system/visualisation, which can connect the two pictures. The row and the column pictures are as mentioned in the Introduction to Linear Algebra by Gilbert Strang.","['matrices', 'matrix-equations', 'linear-algebra', 'systems-of-equations']"
2348240,Existence of finitely additive non-trivial measure on $\mathcal{P}(\mathbb{R})$.,"There exists no map $\mu: \mathcal{P}(\mathbb{R})\rightarrow [0,\infty]$ satisfying: (1) $\mu ((a,b])=b-a$ for all $a,b\in\mathbb{R}$ (this is what I mean by non-trivial in the title), (2) Translation invariance, (3) Countable additivity. We call one such map a measure on $\mathcal{P}(\mathbb{R})$. A reason for its non-existence lies in the properties of the Vitali set $V$, that imply that $V$ cannot belong in any $\sigma$-algebra on which a measure satisfying (1), (2) and (3) exists. As a relaxation of (3) consider (3)* Finite additivity. My question is whether a map $\mu: \mathcal{P}(\mathbb{R})\rightarrow [0,\infty]$ satisfying (1), (2) and (3)* exists. Let's call this a finitely additive measure on $\mathcal{P}(\mathbb{R})$. If one such map $\mu$ existed then the Vitali set $V$ would have to satisfy $\mu(V)=0$. A non-exitence proof here could be something along the lines of the Banach Tarski Paradox, but in $\mathbb{R}$. Bonus question: What about a map $\mu: \mathcal{P}(\mathbb{R})\rightarrow [0,\infty]$ satisfying (1) and (3)? Finally, I wonder what's so great about countable additivity that someone decided that all measures should satisfy it. Related results: If two measures $\mu_1$ and $\mu_2$ agree on $\mathcal{A}\subseteq \mathcal{P}(X)$, where $\mathcal{A}$ is closed under finite intersections (a $\pi$-system) and satisfy that $\mu_1(X)=\mu_2(X)<\infty$, then $\mu_1=\mu_2$ on the $\sigma$-algebra generated by $\mathcal{A}$ (by the $\pi-\lambda$ Theorem ). For example two finite measures that agree on all open sets also agree on all Borel sets. [Generalization of the above] Suppose two measures $\mu_1$ and $\mu_2$ agree on $\mathcal{A}\subseteq \mathcal{P}(X)$, where $\mathcal{A}$ is closed under finite intersections (a $\pi$-system). Moreover suppose there exists a countable nested subfamily of $\mathcal{B}\subseteq \mathcal{A}$ of sets that cover $X$ and have finite $\mu_1$-(and $\mu_2$-) measure. Then $\mu_1=\mu_2$ in the $\sigma$-algebra generated by $\mathcal{A}$ (again using the $\pi - \lambda$ Theorem). So for example the Lebesgue measure in all Borel sets in $\mathbb{R}^n$ is uniquely determined by its values on boxes/intervals. Up to a multiplicative constant, Lebesgue measure is the only translation-invariant measure on the Borel sets that puts finite measure on the unit interval. This can be generalised to higher dimensions considering the unit box $[0,1]^n$ instead of the unit interval. You can find proofs here using, you guessed it, the $\pi - \lambda$ Theorem. The proof using the equivalent Monotone Class Theorem also seems quite straightforward here. Let $\mu_1$ and $\mu_2$ be $\sigma$-finite measures on a measurable space $(X,\mathcal{S})$, $\mathcal{A}$ an algebra which generates $\mathcal{S}$ and suppose that, for each $A\in\mathcal{A}$, $\mu_1(A)=\mu_2(A)$. Then $\mu_1(B)=\mu_2(B)$ for each $B\in\mathcal{S}$ (still haven't seen a proof for this). In relation to the bonus question, by the second (or third) related result any measure $\mu$ satisfying (1) is equal to the Borel measure on all Borel sets. And also, if all Lebesgue measurable sets are $\mu$-measurable, equal to the Lebesgue measure on said sets, since the completion of a measure space is unique. So the question really has to do with whether the Lebesgue measure can be extended to a measure on all of $\mathcal{P}(\mathbb{R})$ (see t.b.'s comment here ).","['lebesgue-measure', 'measure-theory']"
2348293,Calculate length of photographed object given known width,"Sorry, I feel like this should be simple, but I'm stumped and I've searched everywhere.  Is it possible to calculate the length of a photographed object in the attached scenario if you know the width and assume it is a perfect rectangle?","['angle', 'rectangles', 'geometry']"
2348332,How to 'diagonalise' this special $(N+1)\times(N)$ matrix (described in the text)?,"Here is a special $(N+1)\times N$ matrix: $$A=\begin{pmatrix}a_1&a_2&a_3&\ldots&a_N\\b_1&0&0&\ldots&0\\0&b_2&0&\ldots&0\\0&0&b_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&b_N\end{pmatrix}_{(N+1)\times N}$$ Where the matrix elements $a_i$, $b_i$ are all real and positive. Does it exist two orthogonal matrices $U_{(N+1)\times(N+1)}$ and $V_{N\times N}$, satisfying $$UAV=\begin{pmatrix}0&0&0&\ldots&0\\\xi_1&0&0&\ldots&0\\0&\xi_2&0&\ldots&0\\0&0&\xi_3&\ldots&0\\\vdots&\vdots&\vdots&\ddots&0\\0&0&0&\ldots&\xi_N\end{pmatrix}_{(N+1)\times N}$$? For the case $a_i\ll b_i$, I can find the answer (within the accuracy of $\frac{a_i}{b_i}$); but for the general case, I have no idea. I am not sure if this problem is a kind of typical exercise in the linear algebra textbook. Thanks for everyone who help me to solve this problem or give me some hints.","['matrices', 'diagonalization', 'linear-algebra']"
2348336,"Let $a,b,c$ be roots of $x^3+px+r=0$. Find the cubic whose roots are $(a-b)^2$,$ (b-c)^2$ and $(c-a)^2$","Question Let $a,b,c$ be roots of $x^3+px+r=0$. Then find the cubic whose roots are $(a-b)^2, (b-c)^2$ and $ (c-a)^2$ Attempt I have tried using Vieta's formulas to compute coefficients of the sought cubic. For sum of roots we have $$\sum_{cyc}(a-b)^2 = 2\left(\sum_{cyc} a^2-\sum_{cyc}{ab}\right)\\
= 2\left(\sum_{cyc} a\right)^2-6\left(\sum_{cyc} ab\right)\\
= -6p$$ This is coefficient of $x^2$ in the sought cubic. But now computing coefficient of $x^2$ requires simplifying factors like $(a-b)^2\cdot(b-c)^2$ which becomes very lengthy. Is there a shorter way around? Thanks!","['algebra-precalculus', 'cubics', 'polynomials']"
2348339,Pseudoinverse as a submatrix of matrix inverse,"Suppose I have a device that can compute 2x2 (complex) matrix inverses. (For now, assume only invertible matrices, $A$, are ever provided as input): $A\triangleq \begin{bmatrix}
        a_{11} & a_{12}\\
        a_{21} & a_{22}\\
        \end{bmatrix}$ $A^{-1}= \frac{1}{(a_{11}a_{22}-a_{12}a_{21})}\begin{bmatrix}
        a_{22} & -a_{12}\\
        -a_{21} & a_{11}\\
        \end{bmatrix}$ I can use the same device to compute the pseudoinverse of a 2x1 vector (where $^H$ denotes complex conjugate transpose): $\underline{b}^+=\frac{1}{(\underline{b}^H\underline{b})}\underline{b}^H$ For example, I can achieve this by inputting the following matrix to my matrix inversion device (where $^*$ denotes complex conjugate): $B= \begin{bmatrix}
        b_{1} & -b^*_2\\
        b_{2} & b^*_1\\
        \end{bmatrix}$ Since the inverse is: $B^{-1}= \frac{1}{(\underline{b}^H\underline{b})}\begin{bmatrix}
        b^*_1 & b^*_2\\
        -b_2 & b_1\\
        \end{bmatrix}$ and the first row of the result is $\underline{b}^+$. My question is whether this is generalisable to larger matrices. For example, if I have a 3x3 matrix inversion device, can I arrange inputs such that I can read off a 3x2 matrix pseudoinverse? Or have I just stumbled upon the only special case?","['matrices', 'pseudoinverse', 'inverse']"
2348343,Concentration of two independent sub-Gaussian random variables,"Suppose $X$ and $Y$ are independent sub-Gaussian random variables with 0 mean and $\sigma^2$ sub-Gaussian parameter. More specifically, $\mathbb E[\exp(a^T X)]\leq \exp\{\|a\|_2^2\sigma^2/2\}$ for all $a$, and the same holds for $Y$ as well. I wish to upper bound the tail probability
$$
\Pr\left[|X^T Y|>t\right]
$$
using $\sigma^2$ and dimension $n$ (that is, both $X$ and $Y$ are $d$-dimensional random variables). How can I achieve this?  $X^T Y$ does not seem to be either sub-Gaussian or sub-exponential.","['probability', 'concentration-of-measure']"
2348373,Blowing up $x^2=y^5$ at the origin -- why two blow-ups are needed?,"Consider $$f(x,y)=x^2-y^5 \in k[x,y].$$ For simplicity assume $\operatorname{char} k=0$. Then $C=(f=0)\subset \mathbb{A}^2$ has a singularity at $(0,0)$. I was told that the singularity at $(0,0)$ needs two blow-ups, however, according to my calculations only one is enough. I am almost certain that something is horribly wrong in what follows, but I cannot quite put my finger on it. The blow-up is given by 
$$
\operatorname{Bl}_{(0,0)}\mathbb{A}^2:=\{((a,b)\in \mathbb{A}^2, [l]\in \mathbb{P}^1): (a,b)\in l\}\subset \mathbb{A}^2\times \mathbb{P}^1
$$
If I take the co-ordinates on $\mathbb{P}^1$ to be $X$ and $Y$, then the blow-up is cut out by the equation 
$$
xY=Xy.
$$
Hence, the pre-image of $C$ is cut out by 
$$
x^2-y^5=0,
$$
$$
xY=Xy.
$$
Now I want to show that this pre-image is nonsingular. This is done locally on the two standard charts. If $X\neq 0$ then $X=1$ and we have 
$$
x^2=y^5
$$
$$
xY=y
$$
Call these equations $f$ and $g$. To show that the resulting curve is nonsingular, we need to compute the Jacobian, which is 
$$
J=\begin{pmatrix} 2x & -5y^4 & 0 \\
                Y & 1 & x \end{pmatrix}
$$
I claim that the rank of $J$ is 2. Indeed, look at the row rank. Suppose there is $\lambda\in \mathbb{C}^*$ such that
$$
\lambda Y=2x,
$$
$$
\lambda =-5y^4,
$$
$$
\lambda x=0.
$$
This at once implies $x=0$, so $y=0$ and $\lambda=0$, so the rows are always linearly independent. There is a similar calculation involving the chart $Y\neq 0$ with the same conclusion. Where am I wrong?","['algebraic-geometry', 'blowup']"
2348394,Lagrange multipliers with inequality constraints.,"Find the maximum of $f(x,y,z)=(x+y+z)^3$ , in $\mathbb{R}^3$ with the following constraints: $x \ge 0,\ 3x+2y+z=1,\ z\ge x^2+y^2$ I know how to work with lagrange multipliers when the constraints are equalities(defining $h=f-\lambda_1 g_1-\lambda _2 g_2-...-\lambda_kg_k$ and solving $h=0$ .But what am I suppose to do when the constraints are inequalities?","['inequality', 'optimization', 'calculus', 'multivariable-calculus', 'lagrange-multiplier']"
2348402,A funtion and its fourier transformation cannot both be compactly supported unless f=0 [duplicate],"This question already has an answer here : A function and its Fourier transform cannot both be compactly supported (1 answer) Closed 7 years ago . Problem : Suppose that $f$ is continuous on $\mathbb{R}$. Show that $f$ and $\hat f$ cannot both be compactly supported unless $f=0$. Hint : Assume $f$ is supported in [0,1/2]. Expand $f$ in a Fourier series in the interval [-,1], and note that as a result, f is a trigonometric polynomial. I proved that f is trigonometric polynomial by using hint.
But, I don't know how to prove function's fourier transform cannot compactly supported function. 
Can I get some hints?",['fourier-analysis']
2348433,Proof limit and integral of sequence of continuous functions interchangeable,"I want to proof the following theorem. Let $f_n: \Omega \subset \mathbb{R} \to \mathbb{R} $ be a sequence of continuous functions, $ [a,b] \subset \Omega \,$ , $f_n \to f $ uniformly convergent on $\Omega \,$ , $f$ on $[a,b]$ integrable. $$ \lim_{n \to \infty} \int_a^b (f_n(x)) \,dx = \int_a^b (\lim_{n \to \infty} f_n(x)) \, dx$$ (Note that integrability is already given in this version of the theorem.) I’m not really sure how to show this at all. What must be proven, so that I can interchange limits and Integrals?","['functions', 'calculus', 'limits']"
2348438,How to obtain the sum of the following series? $\sum_{n=1}^\infty{\frac{n^2}{2^n}}$ [duplicate],"This question already has answers here : Calculate the sum of infinite series with general term $\frac{n^2}{2^n}$. [duplicate] (2 answers) Closed 6 years ago . It seems that I'm missing something about this. 
First of all, the series is convergent: $\lim_{n\rightarrow\infty}\frac{2^{-n-1} (n+1)^2}{2^{-n} n^2}=\frac{1}{2}$ (ratio test) What I tried to do is to find a limit of a partial sum $\lim_{n\rightarrow\infty}S_n$ as follows: $S_n=\frac{\frac{1}{6} n (n+1) (2 n+1)}{\frac{1-\left(\frac{1}{2}\right)^n}{2 \left(1-\frac{1}{2}\right)}}$. Still, the limit is $\infty$ and I'm clearly doing something wrong.",['sequences-and-series']
2348444,Prove that $\sum_{x=0}^{n}(-1)^x\binom{n}{n-x} (n+1-x)^n=n!$,"I figure out these thing when ""playing"" with numbers: $$3^2-2.2^2+1^2=2=2!$$ $$4^3-3.3^3+3.2^3-1^3=6=3!$$ $$5^4-4.4^4+6.3^4-4.2^4+1^4=24=4!$$ So I go to the conjecture that: $$\binom{n}{n}(n+1)^n-\binom{n}{n-1}n^n+\binom{n}{n-2}(n-1)^n-...=n!$$ or $$\sum_{x=0}^{n}(-1)^x\binom{n}{n-x} (n+1-x)^n=n!$$ Now, how can I prove this conjecture? I've tried a lot, but still couldn't have any idea.","['algebra-precalculus', 'sequences-and-series']"
2348466,Sphere to ellipsoid affine transformation matrix,"I am trying to find the minimum bounding box of an ellipsoid. In my search, I found this answer and also some other nice descriptions like this one to the problem. I am not a mathematician (I need this in an program for my own field of astronomy), I wanted to consult you on the affine transformation that is necessary to define an ellipsiod to use in those solutions. Let's say we define the orientation of the ellipsoid from its major axis (the largest axis of the ellipsoid). Assuming the 3 axes of the ellipsoid to be on the three coordinates with lengths of $a$ , $b$ and $c$ along each axis (with $a\ge{b}$ and $a\ge{c}$ ), then only a single affine transformation (to a sphere) is necessary: $$ \left[\matrix{a&0&0&0 \\ 0&b&0&0 \\ 0&0&c&0 \\ 0&0&0&1}\right] $$ Now, if we first rotate the major axis by $\theta$ from the first axis towards the second axis, and then rotate it by $\phi$ from the (rotated) first axis towards the third axis, the combined affine transformation becomes: $$ \left[\matrix{a&0&0&0 \\ 0&b&0&0 \\ 0&0&c&0 \\ 0&0&0&1}\right]    
   \left[\matrix{\cos\theta & -\sin\theta&0&0 \\ \sin\theta&\cos\theta&0&0 \\ 0&0&1&0 \\ 0&0&0&1}\right] 
   \left[\matrix{\cos\phi&0&-\sin\phi&0 \\ 0&1&0&0\\ \sin\phi&0&\cos\phi&0 \\ 0&0&0&1}\right]$$ Is the multiplied matrix (from left to right) the correct affine transformation that must go into the equations in the links above?","['affine-geometry', 'ellipsoids', 'geometry']"
2348499,How does one find a polynomial approximation of a non-analytic function?,"I have a function $$f(x) =  \left\{
     \begin{array}{lr}
       0 &  0 \leq x < 1/3\\
       q(x) &  1/3 \leq x <  2/3 \\
       1 &  2/3\leq x \leq 1
     \end{array}
   \right.\\ $$
where $q(x)$ is some analytic function that interpolates between the points $x=1/3, 2/3$ and matches the first derivative at these points (say a spline fit). I want to approximate this with a polynomial (as Weierstrauss' theorem says I can) and want to understand how the error falls off as I increase the degree of the approximating polynomial, $p_n(x)$, where the error is defined as $\epsilon_n = \sup|f(x)-p_n(x)|$ . My initial idea was to choose set of orthogonal polynomials (say Legendre polynomials, $l_m(x)$) and then try express $f(x) \approx \sum_{m=1}^n a_m l_m(x)$. The error can then be bounded by the part of the series that has been cut off, and we should be able to bound this. However, $f(x)$ is not analytic and hence there I have no idea if extracting the $a_m$ is possible in the conventional way of $$a_m = \int_0^1 f(x)l_m(x) dx $$. is this is a valid method of extracting the coefficients $a_m$ given $f(x)$ is not analytic? If not, is there a way to go about it? More generally, is there a better way of find out how the error scales with the degree of the approximating polynomial?","['approximation-theory', 'real-analysis', 'analysis', 'approximation']"
2348500,Prove that the conditional probability $P_B(A)=\frac{P(A\cap B)}{P(B)}$ is a probability,"$$\text{Prove that the conditional probability } P_B(A)=\frac{P(A\cap
B)}{P(B)} \text{ is a probability.}$$ I wasn't sure what I need to show, so I checked that in abook and it's saying that something is a probability if these three axioms are satisfied: $\forall \text{ A} \in \epsilon$ we have that $0\leq P(A)\leq1$ $P(\Omega)=1$ $P(\bigcup _{i=1}^{\infty}A_i)= \sum_{i=1}^{\infty}P(A_i)$ So if this is the correct approach I already have troubles at showing 1. :s I tried: $P(B)=P(A \cap B)+P(B \setminus A) \Leftrightarrow P(A \cap B) = P(B)-P(B \setminus A)$ We already see that $P(A \cap B) \geq 0$ and $P(B) \geq 0$ and $P(A \cap B) \geq P(B)$, thus the fraction will be $\geq 0$ But I don't know how could 2. be shown and especially 3.? Did I do 1. correctly?","['probability-theory', 'probability']"
2348503,How to solve $n^2+1 \mid 2^n+1$ for positive integer $n$?,"How to solve following divisibility relation: $$n^2+1 \mid 2^n+1$$ for positive integer $n$? You might have seen the similar IMO problem. Find all positive integers $n$ such that $\frac{2^n+1}{n^2}$ is an integer. But I am sure that the extra $1$ has made this problem difficult. $n$ must be even. Let $p$ be the smallest prime divisor of $n^2+1$, then $$p \mid \gcd \left(2^{p-1}-1, 2^{2n}-1 \right)=4^{\gcd \left((p-1)/2, n \right)}-1$$ Can we say something about $\gcd \left((p-1)/2, n \right)$? Any ideas?","['number-theory', 'diophantine-equations', 'elementary-number-theory']"
2348509,BMO2 1992 - Triangle Circumscribed in Circle,"The circumcircle of triangle $ABC$ has radius $R$ satisfying $$AB^2+AC^2=BC^2-R^2$$ Prove that the angles in the triangle are uniquely determined, and state the values for the angles. So far I have used the Extended Law of Sines and the fact that $\alpha+\beta+\gamma=180$ to show that $\sin{\beta}\sin{\gamma}\cos{\alpha}=-\frac{1}{8}$ (where $\alpha$, $\beta$, $\gamma$ are the angles opposite $BC$, $CA$, $AB$), but I cannot seem to get any further.","['circles', 'trigonometry', 'triangles', 'geometry', 'contest-math']"
2348514,Automorphisms of algebraic groups and maximal tori,"Let $G$ be a linear algebraic group over a field $k$, and $T\subset G$ be a maximal torus. Assume that $f$ is an automorphism of $G$ as an abstract group . Is it true that $f$ maps $T$ to some maximal torus?","['algebraic-groups', 'group-theory', 'algebraic-geometry']"
2348522,Why Does $\mathbf{Set}$ Have Equalizers for All Pairs of Arrows?,"From pg. 113 of Categories for the Working Mathematician : Problem: This seems to imply that $\mathbf{Set}$ has equalizers for all pairs of arrows. But how could this be so? Consider $A = \{0\}$ and $B=\{1,2\}$ with $f,g: A \rightarrow B$ s.t. $f(0) = 1$ and $g(0) = 2$. Then there couldn't be an $e: E \rightarrow A$ s.t. $fe = ge$ for any set $E$. Doesn't this then mean that $f$ and $g$ don't have an equalizer?","['category-theory', 'elementary-set-theory']"
2348529,Prove that $x\left(1+2x\sin\left(\frac{1}{x}\right)\right)$ is differentiable,"I am having difficulties proving that $f:\mathbb{R}\to \mathbb{R}$ , $x\mapsto\begin{cases}x\left(1+2x\sin\left(\frac{1}{x}\right)\right),&\text{if }x\neq0\\0,&\text{if }x=0\end{cases}$ is differentiable with $f'(0) > 0$ . Also show that $f$ is not strictly monotonically increasing in neighbourhood of point $0$ . So I must show that $(-\varepsilon, \varepsilon)$ for every $\varepsilon > 0$ has an interval in which $f$ is strictly monotonically decreasing and can use the properties that $\sin'(x) = \cos(x)$ , $\cos'(x) = -\sin(x)$ and $\sin(2\pi n) = 0$ , $\cos(2 \pi n) = 1$ . I can't seem to figure it out. Any help would be highly appreciated.","['functional-analysis', 'real-analysis', 'ordinary-differential-equations', 'monotone-functions']"
2348606,Solve $(n+1)f(n+1)=(n+2)f(n)+1$,"I solved a problem here , but I could not solve the functional equation, $(n+1)f(n+1)=(n+2)f(n)+1$ without induction at that time. So here is a way to solve the problem. I want to know if their is another way to do it, which is shorter than what I show here not involving induction: Problem: $f(1)=1$ and $(n+1)f(n+1)=(n+2)f(n)+1$ , $n\in \mathbb{N}$ . Find $f(n)$ . Solution: Let $g(n)=\frac{f(n)}{n}$ . Then $$(n+1)^2g(n+1)=n(n+2)g(n)+1\\\Rightarrow (n+1)^2[g(n+1)-1]=n(n+2)[g(n)-1]\\\Rightarrow [g(n+1)-1]=\frac{n(n+2)}{(n+1)^2}[g(n)-1]=\frac{n(n+2)(n-1)(n+1)}{(n+1)^2n^2}[g(n-1)-1]\\=\dots =\frac{n!(n+2)!}{2!(n+1)!(n+1)!}[g(1)-1]=\frac{(n+2)}{2(n+1)}[g(1)-1]=0\\\text{(since as $n\to\infty$,  $\frac{n+2}{n+1}\rightarrow 1$)}(*)$$ Hence $g(n)=1\Rightarrow f(n)=n\space\space\space \blacksquare$ Also I need to know if can say $(*)$ , as well.","['functional-equations', 'discrete-mathematics']"
2348610,Prove that the nth derivative of a function has two real solutions,"I have the function $f:\mathbb{R} \rightarrow \mathbb{R} f(x)=e^x(x^2-5x+7)$. I need to prove that $\forall n \in \mathbb{N}^*$, the equation $f^{(n)}$ has two real solutions. Where $f^{(n)}$ is the nth derivate of the function. My idea is that this should be proved by induction but Im not sure.","['derivatives', 'real-analysis', 'functions']"
2348652,Simplifying the Solution to the Cubic,"I am trying to solve the cubic.  I currently have that, for $ax^3+bx^2+cx+d=0$, a substitution to make this monic.  Dividing by $a$ gives $$x^3+Bx^2+Cx+D=0$$ where $B=\frac{b}{a}, C=\frac{c}{a}, D=\frac{d}{a}$.  Then, with the substitution $x=y-\frac{B}{3}$, I got $$y^3+\left(C-\frac{B^2}{3}\right)y+\left(D-\frac{BC}{3}+\frac{2B^3}{27}\right)=0$$ Thus, to make things simpler, i made the substitution $p=C-\frac{B^2}{3}$ and $q=D-\frac{BC}{3}+\frac{2B^3}{27}$ we have the ""depressed cubic"" $$y^3+py+q=0$$ Now, using the identity, $$(m+n)^3=3mn(m+n)+(m^3+n^3)$$ we let $y=m+n$.  This then translates to $p=-3mn,$ and $q=-(m^3+n^3)$ and gives us a system of equations in $m$ and $n$.  Solving for $n$ gives $n=-\frac{p}{3m}$ and back substituting yields $$q=-m^3+\frac{p}{3m}\qquad \Rightarrow \qquad m^6+qm^3-\frac{p^3}{27}=0$$ and now we can solve the quadratic for $m$; $$m=\sqrt[3]{\frac{-q\pm\sqrt{q^2+\frac{4p^2}{27}}}{2}}$$ and then that means, by back substitution $$n=-\frac{p}{3\sqrt[3]{\frac{-q\pm\sqrt{q^2+\frac{4p^2}{27}}}{2}}}$$ So, I think I am almost here, because now, $$y=m+n=\sqrt[3]{\frac{-q\pm\sqrt{q^2+\frac{4p^2}{27}}}{2}}-\frac{p}{3\sqrt[3]{\frac{-q\pm\sqrt{q^2+\frac{4p^2}{27}}}{2}}}$$ But how can I simplify this expression?  I know I can back substitute for the original $a,b,c,d$ and solve for $x$.  But this sum looks complicated and my attepts to simplify the sum have not worked.","['algebra-precalculus', 'cubics']"
2348685,What is equivalent that logical function?,"(A+B) => (B XOR C) => is Implication Options: BC+!A+!B+!C !B!C+ABC B!C+A+!BC !BC+!ABC BC+A!B!C ! is NOT Please tell me how it is solved. 
I do not think that this requires knowledge of discrete mathematics.
But I did not find any simple options","['boolean-algebra', 'propositional-calculus', 'logic', 'discrete-mathematics']"
2348723,Are these two graphs (trees) isomorphic?,"I would say they are not isomorphic since the node with degree 4 is in two different positions, any suggestion?","['graph-theory', 'discrete-mathematics']"
2348779,Connecting $1997$ points in the plane- what am I missing?,"I know I am missing something in this problem, but I don't know what: Let $P_1, P_2, ..., P_{1997}$ be distinct points in the plane. Connect the points with the line segments $P_1P_2, P_2P_3, P_3P_4, ..., P_{1996}P_{1997}, P_{1997}P_{1}$. Can one draw a line that passes through the interior of every one of these segments? According to how I understand the problem, the solution is easy. It's clearly not possible to have one line pass through every line segment in this picture.","['problem-solving', 'recreational-mathematics', 'plane-geometry', 'geometry']"
2348811,Expected Number of Single Socks when Matching Socks,"Whenever I go through the big pile of socks that just went through the laundry, and have to find the matching pairs, I usually do this like I am a simple automaton: I randomly pick a sock, and see if it matches any of the single socks I picked out earlier and that haven't found a match yet. If there is a match, I will fold the two socks together and put them in the 'done' pile, otherwise I will add the single sock to the 'no match yet' pile of single socks, and pick out another random sock. So, as I was doing this last night, I started thinking about this, and figured that the following would be true: The 'no match yet' pile can be expected to slowly grow, up to some point somewhere in the 'middle' of the process, after which the pile will gradually shrink, and eventually go down back to $0$. In fact, my intuition is that the expected number of loose socks as a function of the number of socks picked so far, is a symmetric function, with the maximum being when I have picked half of the socks. So, my questions are: With $n$ pairs of socks, what is the expected number of loose socks that are in my 'no match yet' pile after having picked $k$ socks? Is it true that this function is a symmetric function, and that the maximum is for $k=n$? (if so, I figure there must be a conceptual way of looking at the problem that makes this immediately clear, without using any formulas ... what is that way? Is it just that I can think of reversing the process?) Of course, this is all assuming there are $n$ pairs of socks total, and that there are no single socks in the original pile, and while this is something that never seems to apply to the pile of socks coming through my actual laundry, let's assume for the sake of mathematical simplicity that there really just are $n$ pairs of socks.",['probability']
2348814,Relation between left and right invariant vector fields.,"What I'm trying to show: Let $Y$ be a vector field on a Lie group $G$. If $G$ is connected and $[X,Y]=0$ for all left invariant vector field $X$, then $Y$ is right invariant. I thought I could prove it using only the relations between left and right invariant vector fields, but I failed. I realized I wasn't using the connectedness of $G$. I'm having difficulty in understanding what role plays the connectedness of $G$.","['differential-geometry', 'lie-groups']"
2348817,"How to calculate $\iint_D{} (x^2 + y^2) \, dxdy$ with $D = \{(\frac{x}{2})^2 + (\frac{y}{3})^2 \leq 1\}$?","I'm asked to calculate 
  $$\iint_D{} (x^2 + y^2)\, dxdy,\quad D = \left\{\left(\frac{x}{2}\right)^2 + \left(\frac{y}{3}\right)^2 \leq 1\right\}.$$ My attempt: Set 
$$\frac{x}{2} = r\cos{\theta},\quad\frac{y}{3} = r\sin{\theta}$$
which nets the functional matrix i $6r$. $$\iint_D{(4r^2\cos^2{\theta} + 9r^2\sin^2{\theta})6r \, drd\theta}$$
$$\iint_D{24r^3 + 30r^3\sin^2{\theta} \, drd\theta}$$
$$\int^{2\pi}_0\int^1_0{24r^3 + 30r^3\sin^2{\theta} \,drd\theta}$$
$$\int^{2\pi}_0{6 + 10\sin^2{\theta} \, d\theta}$$
$$\left[6\theta + \frac{10}{2}(\theta - \sin{\theta}\cos{\theta})\right]^{2\pi}_0$$ which equals to $22\pi$. However, the answer is supposed to be $\frac{39\pi}{2}$. What am I doing wrong / missing?","['multivariable-calculus', 'definite-integrals', 'analysis']"
2348832,Is $\sin z$ a square of a holomorphic function?,"Is there a holomorphic function $f$ satisfying $(f(z))^2=\sin(z)$ on a nbhd of 0? I know that for these kind of problems it is common to use a logarithm, in this case $g(z):=\exp(L(\sin(z)/2))$ where $L$ is the logarithm. I assume that such a $g$ cannot exist because of the zeros of $\sin z$.","['complex-analysis', 'functions']"
2348843,What is a population minimizer?,"I am reading into statistics in combination with machine learning and I came across the expression ""population minimizer"". I found no good explanation on what a population minimizer exactly means, so what exactly is it?",['statistics']
2348864,1985 Putnam B4 Integration in Rectangular Coordinates,"Let C be the circle radius 1, center the origin. A point P is chosen at random on the circumference of C, and another point Q is chosen at random in the interior of C. What is the probability that the rectangle with diagonal PQ, and sides parallel to the x-axis and y-axis, lies entirely inside (or on) C? The answer is $4/\pi^2$ and is gotten by integrating with polar coordinates like so: Given P, let R be the rectangle inscribed in C with sides parallel to the axes. Then the rectangle with diagonal PQ lies entirely inside C iff Q lies inside R. Let O be the center of C. Let the diameter of C parallel to the y-axis make an angle θ with OP. Then the area of R is 4 sin θ cos θ. So the required probability is $2/\pi\int_0^{\pi/2} (4\sin\theta\cos\theta)/\pi\,d\theta = 4/\pi^2\int_0^{\pi/2}\sin 2\theta\,d\theta = 4/\pi^2$. When I was trying to solve this I got the first part but tried to integrate in rectangular coordinates:
Let $P=(x,y)$ then the area of the rectangle is $4x\sqrt{1-x^2}$ and integrating 
$4\int_0^14x\sqrt{1-x^2}/\pi$ doesn't give $4/\pi^2$. I then have 3 questions: 1) Why does the integral in the solution have a $2/\pi$ coefficient? 2) Why does the integral in the solution go only to $\pi/2$, I thought its limits would be $2\pi$ or $4$ times the given boudaries. 3) How would you properly do the integral in rectangular coordinates?","['contest-math', 'integration', 'definite-integrals']"
2348898,Equation for tangent line for $f(x) = 1/\sqrt{x}$ at $x=a$,"first time math stack-exchange-er here. I'm self-teaching single variable calculus using MIT's free online courses and I think I found a typo in the homework solution set (problem 1C-4 part d).  I'm not confident enough in my own abilities to know for sure if this is a mistake vs. my poor math skills. Could anyone tell me if the following equation is correct? The problem is as follows:
Write an equation for the tangent line for the following functions: $$f(x) = \frac1{\sqrt{x}}\ \text{ at } x=a$$ I did the following.  First I found the derivative of $f(x)$:
$$f'(x) = -\frac12 x^{-\frac32}\ $$ Then I plugged in a to get $f(a)$ and $f'(a)$ and used the point-slope method to find the equation for the tangent line:
$$y-a^{-\frac12} = -\frac12 a^{-\frac32}(x-a)\ $$ Which I then simplified to:
$$y=-\frac12a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ However, the solution set says the answer is:
$$y=-a^{-\frac32}x  + \frac32 a^{-\frac12}\ $$ Is the solution set correct?  If so, where did the $-\frac12$ go? Thank you!","['algebra-precalculus', 'tangent-line', 'calculus']"
2348902,"What is meant by ""expand f[x,y]=xy in powers of x-1 and y-1""?","This question refers to C.H. Edwards, Jr.'s Advanced Calculus of Several Variables, Chapter II-7, Example 2. Suppose we want to expand $f[x,y]=xy$ in powers of $x-1$ and $y-1$ .  Of course the result will be $xy=1+(x-1)+(y-1)+(x-1)(y-1)$ , but let us obtain this result by calculating the second degree Taylor polynomial $P_2[\mathbf{h}]$ of $f[x,y]$ at $\mathbf{a}=\{1,1\}$ with $\mathbf{h}=\{h_1,h_2\}=\{x-1,y-1\}$ . ... Establishing the equivalence is a matter of simple algebra, but ""expand $f[x,y]=xy$ in powers of $x-1$ and $y-1$ "" has no clear meaning to me.  Edwards obviously does not mean to use a Taylor polynomial, because he wants to show the above result is equivalent to the Taylor polynomial. What exactly does it mean to expand a function in powers of some given pattern?  I'm assuming this is something I should have learned in high school.",['algebra-precalculus']
2348927,Expectation of the number of points inside a foursquare of a rectangle,"Consider a rectangle (black one) in the following image. Lets take four random points uniformly on each border then connecting the points one after another (red lines) to get a foursquare  inside the rectangle. If we put a set of random points ($n$ points) uniformly  inside the rectangle , I would like to know what is the mathematical expectation of the number of points that are inside the red area? Since the position of red points are random, I really can't solve this problem. The probability that each point falls in the red area, is the area of red_line divided by area of rectangle. Since the area it self is a random process, so we need to calculate the expectation of the area of the red line. Thanks in advance.","['expectation', 'random', 'probability', 'random-variables']"
2348933,An application of Hurwitz theorem from Conway's Complex Analysis,"Suppose that $f_{n}$ is a sequence in $H(G)$, $f$ is a non-constant function and $f_{n}\to f$ in $H(G)$. Let $a\in G$ and $f(a)=\alpha$. Show that there is a sequence $a_{n}$ in $G$, such that (i)$a=lim_{n \to \infty} a_{n}$ (ii) $f_{n}(a_{n})=\alpha$ for sufficiently large $n$. I am stuck with this problem and I have no idea. It seem it is an application of Hurwitz's theorem but I am not able to apply it! Thanks in advance!","['analyticity', 'complex-analysis', 'holomorphic-functions']"
2348991,Prove that $\pi^2/8 = 1 + 1/3^2 + 1/5^2 + 1/7^2 + \cdots$,"Attempt: I found the Fourier series for $f(x) = \begin{cases} 0,& -\pi < x < 0 \\ x/2,& 0 < x < \pi \end{cases}$ a) $a_0 = \frac{1}{2\pi}\int_0^{\pi} r\,dr = \pi/4$ $a_n =  \frac{1}{2\pi}\int_0^r \frac{r\cos(nr)}{2}dr = \frac{(-1)^n - 1}{2\pi n^2}$ $b_n =  \frac{1}{2\pi}\int_0^r r\sin(nr)\,dr = \frac{(-1)^n + 1}{2n}$ $f(x) = \frac{\pi}{8} - \sum_n [\frac{((-1)^n - 1)\cos(nx)}{2\pi n^2} + \frac{((-1)^n + 1)\sin(nx)}{2n}]$ The prof asked us to use this Fourier series to prove that $\pi^2/8 = 1+1/3^2+1/5^2+1/7^2+\cdots$. How do I do this?","['fourier-series', 'sequences-and-series', 'partial-differential-equations']"
2349001,Why are critical points called critical?,"For a function $y = f(x)$, a number $x_0$ is called $\textit{critical}$ if either $f'(x_0) = 0$ or $f'(x)$ does not exist. Sometimes the term $\textit{stationary}$ is used, but it is by far less popular. My question is Why is the word ""critical"" used in this case as terminology? What makes $x_0$ critical if $f'(x_0) = 0$? Of course the tangent line being horizontal or not being able to draw a tangent line gives local minimums and maximums. So why are maximums and minimums ""critical""? It seems that ""stationary"" is more appropriate. So I am puzzled as to why the latter is less popular.","['derivatives', 'terminology']"
2349040,$2^n$ base $5$ contains more than ones and zeros?,"This math problem was posted on The Nineteenth Byte over at Programming Puzzles and Code Golf . It states this: Prove that for all integers $n > 0, 2^n$ will contain a digit other than $1$ and $0$ in base $5$. TNB came up with these conditions for a counterexample: $2^n \text{ mod } 5 = 1$ ; $2^n \text{ mod } 10 = 6$ ; and $n \text{ mod } 4 = 0$","['number-theory', 'modular-arithmetic']"
2349088,Convert vector differential equation's order,"Here is a 2nd order vector differential equation: $$\overrightarrow{Y}''= \begin{pmatrix}a & b \\c & d \end{pmatrix} \overrightarrow{Y}$$ Don't work it out, but write it as a vector differential eqn in $1$st order in higher dimensions. I am not sure where to begin. How does one convert from 2nd to 1st order? Hints are appreciated.","['laplace-transform', 'calculus', 'ordinary-differential-equations', 'linear-algebra', 'analysis']"
2349091,strong mixing implies weak mixing,"We say that a measure-preserving transformation $T: X\to X$ of a probability space $(X, \mathscr{B}, \mu)$ is weak mixing if, for all $A, B\in \mathscr{B}$, 
$$\lim_{n\to \infty} \frac{1}{n} \sum_{j=0}^{n-1} |\mu(T^{-j}A \cap B)-\mu(A)\mu(B)|=0$$ and strong mixing if, for all $A, B\in \mathscr{B}$, 
$$\lim_{n\to \infty} \mu(T^{-n} A\cap B)=\mu(A)\mu(B).$$ In my notes, it simply states that strong mixing clearly implies weak mixing, however, I can't show this. How may I prove this obvious result?","['dynamical-systems', 'ergodic-theory', 'sequences-and-series', 'analysis']"
2349130,Arranging identical balls,"Suppose you have four pairs of balls, each pair being identical in colour ( say $\color{#00f}{blue}$, $\color{#f00}{red}$, $\color{#06b121}{green}$ and $\color{#d4ac0d}{yellow}$ ). How many ways can they be arranged so that no two identical balls are next to each other ?. I have an inkling that inclusion/exclusion principle is needed, but I am not completely certain. Also, is there a technique to solve this one out without using inclusion/exclusion principle ?.","['permutations', 'combinatorics']"
2349164,Exercise 1.12 in Tao's nonlinear dispersive equation,"Suppose $F$ is locally Lipschitz and has at most $x\log(x)$ growth i.e.
$$\|F(u)\|_D\leq (1+\|u\|_D)\log(2+\|u\|_D).$$
Does solution to the Cauchy problem of ODE
$$\partial_tu(t)=F(u)$$
(with some initial datum) exist classically for all time or is it possible to blow up?",['ordinary-differential-equations']
2349180,Is $A^TA$ positive semi-definite for any real matrix $A$?,The question is written in title. I read a theorem saying: Suppose $A\in \mathbb R^{n\times n}$ is symmetric. Then the following are equivalent. $A$ is positive semidefinite. Eigenvalues of $A$ are all non-negative. $A$ can be factored as $A=G^TG$ where $G$ is an $p\times n$ matrix for some $p$. The reason why I am asking is that I have a matrix $X$ and get some negative eigenvalues of $X^TX$ using either MatLab or R.,['linear-algebra']
2349205,Predicting growth with differential equations,"Look at the differential equation $y'[x] = y[x] (2 + \sin[x]^2 - y[x])$ with $y[0] = 1$. How do you know before you do any plotting that as $x$ advances from $0$, the plot of the solution $y[x]$ goes up near $x$'s for which $2 + \sin[x]^2 > y[x]$ and the plot of the solution $y[x]$ goes down near $x$'s for which $2 + \sin[x]^2 < y[x]$ ? Why do you expect that the crests and dips of the plot of $y[x]$ are located at places where the $y[x]$ plot crosses the plot of $2 + \sin[x]^2$ ? My answer: The plot of $y[x]$ goes up near $x$'s for which $2+\sin[x]^2 > y[x]$ because if $2+\sin[x]^2 > y[x]$ then the expression in the parenthese will evaluate to a positive number times a positive number on the outside which results in a positve $y'[x]$ meaning it is growing. The plot of the solution $y[x]$ goes down near $x$'s for which $2+\sin[x]^2<y[x]$ because then the expression in the parentheses will evaluate to a negative number times a positive number on the outside which results in a negative $y'[x]$ meaning it is decreasing. We know that $y[x]$ cannot be a negative number because $2+\sin[x]^2$ never goes below 2. Meaning that whenever $y[x]$ crosses $2+\sin[x]^2$ it will hit a crest or dip because $2+\sin[x]^2$ is where $y[x]$ is not growing or decreasing. I feel like I am missing the big picture and my answer is wrong here. What is the correct way to approach this problem?","['derivatives', 'ordinary-differential-equations', 'calculus']"
2349229,What do we know about the reciprocal busy beaver series?,"I just read these excellent lecture notes by Scott Aaronson, and I found the second homework problem at the end to be incredibly thought-provoking (this course was offered over ten years ago, so I think it's now safe to discuss the homework online): Let BB(n), or the ""nth Busy Beaver number,"" be the maximum number of steps that an n-state Turing machine can make on an initially blank tape before halting. (Here the maximum is over all n-state Turing machines that eventually halt.) Prove that BB(n) grows faster than any computable function. Let S = 1/BB(1) + 1/BB(2) + 1/BB(3) + ...
  Is S a computable real number? In other words, is there an algorithm that, given as input a positive integer k, outputs a rational number S' such that |S-S'|<1/k? I understand question #1 - it's #2 I'm wondering about. Clearly the series converges, since the sequence $1/BB(n)$ falls off much faster than $1/n$ (to put it mildly...). I suspect that $S$ in uncomputable like Chaitin's constant . (Although in some vague sense S seems to me to be more ""natural,"" because it does not rely on a specific choice of prefix-free universal computable function ""programming language"" - so perhaps it's more analytically tractable?) Am I correct? Also, is there anything at all that we can say about $S$ quantitatively? (Beyond the trivial result that it's greater than $1/4 + 1/6 + 1/13 = 77/156 = 0.494...$ based on the known values of $BB(2)$, $BB(3)$, and $BB(4)$.) Edit: The solution is on pg. 43 of Quantum Computing Since Democritus . The answer is that $S$ is uncomputable and the reasoning is similar to mercio's, except instead of comparing $\mathrm{BB}(n)$ to a specific exponential sequence, there's just the vague sentence Since $1/\mathrm{BB}(n+1)$, $1/\mathrm{BB}(n+2)$, and so on are so much smaller than $1/\mathrm{BB}(n)$, any upper bound on $1/S_n$ immediately yields an upper bound on $\mathrm{BB}(n)$ as well. What is a precise upper bound?","['computability', 'sequences-and-series', 'computer-science']"
2349233,Number of ways to permute a string of all numbers 1-n such that the string can be separated into two ascending strings,"The problem is to find how many permutations of length n consisting of all numbers 1-n are able to be separated into two ascending strings. One example would be 13724856, which could be separated into '1378' and '2456'. I had listed out all cases for n=3 and n=4, for which I got 5 and 14 permutations respectively, so I'm thinking it follows the Catalan sequence but I've not been able to prove it yet. One thought was to try and separate it into two smaller cases to try and get the summation form of the Catalan sequence but that didn't lead me anywhere. Something else I thought of trying was to find the permutations of n-k and k strings, then use nCr(n-k, k) to calculate the number of ways, but some strings can be sorted multiple ways. Then I tried a bijective proof with another Catalan problem where you have to order n '(' and n ')' with the number of ')' never exceeding the number of '(' at any point, but it doesn't seem to follow the same pattern as this so I'm at a wall. Maybe these don't follow the Catalan sequence but I'm fairly certain that they do. For anyone wanting the n=3 and n=4 cases: 123, 132, 213, 231, 312 1234, 1243, 1324, 1342, 1423, 2134, 2143, 2341, 2314, 2413, 3412, 3142, 3124, 4123 preemptive thanks for your help and sorry if the formatting is wonky (mobile!)","['permutations', 'combinatorics', 'catalan-numbers']"
2349241,Show that $f(\xi + o_{\Bbb P}(1))=f(\xi) + o_{\Bbb P}(1)$,"$f:\Bbb R\to\Bbb R$ is continuous, $\xi:\Omega\to\Bbb R$ a random variable, and $o_{\Bbb P}(1)$, in analogue to Landau's little-o notation, denotes any sequences of random variables on $\Omega$ that converges to $0$ in probability (implicitly indexed by $n$). Then show that 
  $$f(\xi + o_{\Bbb P}(1))=f(\xi) + o_{\Bbb P}(1).$$ In other words, we want to show $f(\xi + o_{\Bbb P}(1))-f(\xi)\to 0$ in probability as $n\to\infty$. So I investigate the probability $P(|f(\xi + o_{\Bbb P}(1))-f(\xi)|>\epsilon)$. Now if $f$ is uniformly continuous, we are easily done since we can find a uniform $\delta$ such that $[|f(\xi + o_{\Bbb P}(1))-f(\xi)|>\epsilon]$ is included in $[|o_{\Bbb P}(1)|>\delta]$. But what if the continuity is not uniform?","['continuity', 'real-analysis', 'random-variables', 'probability-theory']"
2349289,"If $\int_{\Omega} u(x) ~\text{div} \phi(x) ~\text{d}x =0$ for all $\phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$, then $u=\text{constant}$?","I want to take the weak gradient operator $$ \begin{aligned} \nabla: L^2(\Omega) &\to W^{-1,2}(\Omega,\mathbb{R}^d) \\ \langle \nabla u, \phi \rangle_{W^{-1,2},W_0^{1,2}}&:=(u,\text{div}\phi)_{L^2}=\int_\Omega u(x) \,\text{div}\phi(x) ~\text{d}x \end{aligned} $$
for all $\phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$. Assume $\nabla u=0$. Show that $u$ is constant. I know that if I interpret $u$ as a tempered distribution and take the gradient as a distributional derivative it should be possible to conclude $u$ is constant. But I'd like to take the definition of the weak gradient operator above. We have
$$ \int_\Omega u(x) \,  \text{div}\phi(x) ~dx=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d).$$ If $d=1$ I could conclude $u=\text{const}$ by the fundamental lemma of calculus of variations . But in general
$$ \int_\Omega u(x) \,  (\partial_{1}\phi_1(x)+...+\partial_d \phi_d(x)) ~d(x_1,...,x_d)=0 \text{ for all } \phi \in W_0^{1,2}(\Omega,\mathbb{R}^d)$$
and I don't know if there is a similar theorem.","['functional-analysis', 'sobolev-spaces', 'operator-theory', 'derivatives']"
2349297,"Tensor product $L^2([0,1],\mathbb R)\otimes L^2([0,1],\mathbb R)$","I am trying to get a good understanding of what a tensor product is and I am trying to understand one particular example. Suppose that $H=L^2([0,1],\mathbb R)$ and take the inner-product
$$
\langle f,g\rangle=\int_0^1f(x)g(x)dx
$$
defined for each $f,g\in H$ as a bilinear map from $H\times H$ to $\mathbb R$. I want to investigate the tensor product $H\otimes H$. First of all, as far as I understand, there exists a universal bilinear map $\varphi:H\times H\to H\otimes H$ and then there exists a linear map $l:H\otimes H\to\mathbb R$ such that $\langle\cdot,\cdot\rangle=l\circ \varphi$, i.e. $\langle f,g\rangle=l(\varphi(f,g))=l(f\otimes g)$ for each $f,g\in H$. My questions are as follow. As far as I understand (see also here ), a tensor $f\otimes g$ is defined as $(f\otimes g)(x,y)=f(x)g(y)$ for each $f,g\in H$ and $x,y\in[0,1]$ . How can we deduce that tensors $f\otimes g$ look like this? How can we come up with this expression? The bilinear map $\varphi$ is given by $\varphi(f,g)=f\otimes g$. $H\otimes H$ also contains elements that are finite linear combinations of the tensors $f\otimes g$. So the map $\varphi$ is not necessarily surjective, the image of $H\times H$ is smaller than $H\otimes H$, right? The tensor product $H\otimes H$ is actually isomorphic to the space $L^2([0,1]^2,\mathbb R)$, is that correct? How can we find the linear map $l$ that maps tensors $f\otimes g$ to $\langle f,g\rangle$ for each $f,g\in H$? Thanks a lot for your help!","['tensor-products', 'hilbert-spaces', 'multilinear-algebra', 'functional-analysis', 'vector-spaces']"
2349301,Why write $\mathbb R\times \mathbb R^3\to\mathbb R^3$ instead of $\mathbb R^4\to\mathbb R^3$?,"I often see this notation for a vector field
$$\mathbf A:\mathbb R\times \mathbb R^3\to\mathbb R^3$$ But isn't $\mathbb R\times \mathbb R^3=\mathbb R^4$, right? So, is there any advantages using $\mathbb R\times \mathbb R^3\rightarrow\mathbb R^3$ instead of $\mathbb{R}^4\rightarrow \mathbb{R}^3$?","['multivariable-calculus', 'vector-analysis']"
2349310,How do I show convergence in the 2-adics?,"How do I show convergence in the 2-adics in general, but particularly for the series given below? If, to discriminate between convergence in different $p$-adics I define $\infty_p=\lim_{n\to\infty} p^n$ then it would seem reasonable to state $\infty_2=0_2$ in the 2-adics.  I'm unsure of however, whether other series are convergent in the 2-adics such as whether $\infty_3=0_2$ I have an infinite sum which is divergent in the integers but in the 2-adics it converges to $\infty_2=0$. The case I have in mind is; show that for every $x_0\in3\mathbb{N}_{>0}$: $$f(x_0)=\lim_{n\to\infty} \left(3^n\left(x_m-2^{v(x_0)}\right)+\sum_{k=0}^n3^{n-k}2^{v(x_{k})} \right)=0_2$$ where $v(\cdot)$ is the (additive) 2-adic valuation $v:\Bbb Q_2^\times\to\Bbb Z$ and $x_{m+1}=2x_m+v(x_m$). It actually converges to $2^r$ for some $r$ in finite steps for all $x_m$ and lands on every alternate power of $2$ thereafter. How might one typically identify convergence in such a series?  Straight off the bat I can see that it's sufficient to show its value converges to the inverse of its 2-adic norm and therefore if $x$ converges to $0_2$, then $\frac{x}{2^{v(x)}}=1$ I can also see that this problem is equivalent to stating that for any given $x_0$, for sufficiently large $r$, there exists a series $x_0+x_{1}+\ldots$ in which $x_{m+1}=2x'_m+2^{v(x)}$ which sums to either $2^r$ or $2^{r+1}$ where $x'_m$ denotes the partial sum to $x_m$.  I mention this partly to highlight the alternating nature of the series in that for sufficiently high $2^r$ it lands on alternate powers of $2$ and not on every power of $2$. I'm not necessarily asking you to solve this problem (although that would be welcome!) as it's clearly a challenging one but guidance as to a typical method would be appreciated.  There is much I am unsure of.","['convergence-divergence', 'analysis', 'p-adic-number-theory']"
2349322,"the determinant of the $n \times n$ matrix $A = (\alpha_v^u)$, where $\alpha_v^u = 1- \delta_v^u,$ is equal to $(n-1)(-1)^{n-1}$","In the book of Linear Algebra by Werner Greub, at page $111$, question $2$, Prove that the determinant of the $n \times n$ matrix $A = (\alpha_v^u)$, where $$\alpha_v^u = 1- \delta_v^u,$$ is equal to
  $(n-1)(-1)^{n-1}$. If we consider $A$ as the matrix of the map $\phi : E \to E, (dim E = n)$, with respect to the basis $e_v$ , we can say that $$\phi (e_v) = (\sum_u e_u) - e_v$$, so by definition $$\Delta_\phi(e_1,..., e_n)= \Delta(\phi(e_1), ..., \phi (e_n)) = \Delta (\sum_{u \not = 1} e_u, ..., \sum_{u \not = n} e_u) = det \phi$$, where $\Delta$ is a nonzero determinant function and $\Delta (e_1, ..., e_n) = 1$, and from that the the only contribution will come from the derangement of $e_v$s, but the number of derangements is huge compare to the $(n-1)(-1)^{n-1}$, so how can continue from that ? I would appreciate help. Note: In here , there is a answer to my question, and its link is given, but I would specifically like to learn how to continue from the point that I have arrived because, for example, if I tried to solve this very same question after a month, I will again use a method similar to this one. Edit: I'm particularly looking for a proof that continues from where I left. Note 2: After 3 months that I have first faced with this question, I have tried to solve it again, and used the same method as the my first attempt above, and stuck in a similar point in the answer given to this question.","['matrices', 'linear-algebra', 'linear-transformations', 'determinant']"
2349347,Eigenvalues of a matrix with binomial entries,"I am trying to determine the eigenvalues of the following matrix: $$M_{ij} = 4^{-j}\binom{2j}{i}$$ where it is understood that the binomial coefficient $\binom{m}{k}$ is zero if $k<0$ or $k>m$. Here $i,j$ go from $0$ to $N$, therefore the matrix is $(N+1)\times(N+1)$. If an exact expression is not available, I would content myself with approximations valid for large $N$. Moreover, I am mostly interested in the largest positive eigenvalue and its corresponding eigenvector.","['matrices', 'eigenvalues-eigenvectors', 'binomial-coefficients']"
2349351,Second Mean Value Theorem of Integral Proof [closed],"Closed. This question does not meet Mathematics Stack Exchange guidelines . It is not currently accepting answers. Please provide additional context , which ideally explains why the question is relevant to you and our  community. Some forms of context include: background and motivation, relevant definitions, source, possible strategies, your current progress, why the question is interesting or important, etc. Closed last year . Improve this question This is from wikipedia on MVT. EDIT: Now that user Marsan has given a proof, why does the interval need not contain $a$? $H$ is continuous at $[a,b]$. What happens if $ \int G \phi \, dt = 0$?","['real-analysis', 'integration', 'definite-integrals']"
2349372,Prove that $\alpha_{m}=\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}}$ is irrational for all $m\geq 1$.,"I came across this question and went through the proof that $e$ is irrational with a few minor tweaks. I hope someone can have a look through it and hopefully check it or tidy it up. Thanks. Proof : To begin, note that for $m\geq 1$:
$$\alpha_{m} \,=\, 1 + \sum_{n=2}^{\infty}\frac{1}{(n!)^{m}} \,>\, 1 $$
and 
$$\alpha_{m} \,=\, -1 + \sum_{n=0}^{\infty}\frac{1}{(n!)^{m}} \,<\, -1 + \sum_{n=0}^{\infty}\frac{1}{n!} \,=\, -1 + e \,<\, 2 .$$
Thus $1<\alpha_{m}<2$ for all $m\geq 1$ and so $\alpha_{m}\notin\mathbb{Z}$. Now assume (for contradiction) that $\alpha_{m}\in\mathbb{Q}$: $$\exists\, p,q\in\mathbb{N} \quad\text{with}\quad q\,>\,1\quad : \quad \alpha_{m}=\frac{p}{q}.$$
Since $p,q\in\mathbb{N}$ and $m\geq 1$: 
$$(q!)^{m}\alpha_{m} \;=\; (q!)^{m}\cdot \frac{p}{q} \;=\; q!\,(q!)^{m-1}\cdot \frac{p}{q} \;=\;  (q-1)!(q!)^{m-1}p$$
and hence $(q!)^{m}\alpha_{m}\in\mathbb{Z}$. Now
$$\begin{align}
(q!)^{m}\alpha_{m} &\;=\; (q!)^{m}\sum_{n=1}^{\infty}\frac{1}{(n!)^{m}} \;=\;
 (q!)^{m}\left(\sum_{n=1}^{q}\frac{1}{(n!)^{m}} + \sum_{n=q+1}^{\infty}\frac{1}{(n!)^{m}}  \right)  \\[0.2cm]
&\;=\; \sum_{n=1}^{q}\left(\frac{q!}{(n!)}\right)^{m} + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m}
\end{align}$$
for some $N\in\mathbb{Z}$, since $n!|q!$ for each $n\leq q$. In particular, since the series in the last term is positive, we have the bound:
$$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{(n!)}\right)^{m}.$$
Considering the sum:
$$\begin{align}
\sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} &\;=\; \left(\frac{q!}{(q+1)!}\right)^{m} + \left(\frac{q!}{(q+2)!}\right)^{m} + \left(\frac{q!}{(q+3)!}\right)^{m} + \cdots \\[0.2cm]
&\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}} + \frac{1}{(q+1)^{m}(q+2)^{m}(q+3)^{m}}+ \cdots \\[0.2cm]
&\;<\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}} + \frac{1}{(q+1)^{m}(q+1)^{m}(q+1)^{m}} + \cdots \\[0.2cm]
&\;=\; \frac{1}{(q+1)^{m}} + \frac{1}{(q+1)^{2m}} + \frac{1}{(q+1)^{3m}} + \cdots.
\end{align}$$
This is a geometric series with ratio $0<\frac{1}{(q+1)^{m}}<1$ for all $m\geq 1$. Hence
$$\sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;=\; \frac{\frac{1}{(q+1)^{m}}}{1-\frac{1}{(q+1)^{m}}} \;=\; \frac{1}{(q+1)^{m}-1} \;<\; 1$$
for all $m\geq 1$ since $(q+1)^{m}>1$. Thus we have arrived at:
$$N \;<\; (q!)^{m}\alpha_{m} \;=\; N + \sum_{n=q+1}^{\infty}\left(\frac{q!}{n!}\right)^{m} \;<\; N + 1.$$
But no integer exists in the interval $(N,N+1)$ and hence we have a contradiction; our assumption must be false and $\alpha_{m}\notin\mathbb{Q} \; \blacksquare$.","['real-analysis', 'sequences-and-series', 'proof-verification']"
